/home/gridsan/nchutisilp/.conda/envs/nnunet/bin/python
wandb: Tracking run with wandb version 0.16.6
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.

############################
INFO: You are using the old nnU-Net default plans. We have updated our recommendations. Please consider using those instead! Read more here: https://github.com/MIC-DKFZ/nnUNet/blob/master/documentation/resenc_presets.md
############################

Using device: cuda:0

#######################################################################
Please cite the following paper when using nnU-Net:
Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.
#######################################################################

2024-05-06 18:28:04.795230: Using torch.compile...
/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
################### Loading pretrained weights from file  /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_results/Dataset300_Lumen_and_Wall_OCT/nnUNetTrainer__nnUNetPreprocessPlans__2d/fold_all/checkpoint_best.pth ###################
Below is the list of overlapping blocks in pretrained model and nnUNet architecture:
encoder.stages.0.0.convs.0.conv.weight shape torch.Size([32, 1, 3, 3])
encoder.stages.0.0.convs.0.conv.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.norm.weight shape torch.Size([32])
encoder.stages.0.0.convs.0.norm.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.0.weight shape torch.Size([32, 1, 3, 3])
encoder.stages.0.0.convs.0.all_modules.0.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.1.weight shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.1.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.conv.weight shape torch.Size([32, 32, 3, 3])
encoder.stages.0.0.convs.1.conv.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.norm.weight shape torch.Size([32])
encoder.stages.0.0.convs.1.norm.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3])
encoder.stages.0.0.convs.1.all_modules.0.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.1.weight shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.1.bias shape torch.Size([32])
encoder.stages.1.0.convs.0.conv.weight shape torch.Size([64, 32, 3, 3])
encoder.stages.1.0.convs.0.conv.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.norm.weight shape torch.Size([64])
encoder.stages.1.0.convs.0.norm.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.0.weight shape torch.Size([64, 32, 3, 3])
encoder.stages.1.0.convs.0.all_modules.0.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.1.weight shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.1.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.conv.weight shape torch.Size([64, 64, 3, 3])
encoder.stages.1.0.convs.1.conv.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.norm.weight shape torch.Size([64])
encoder.stages.1.0.convs.1.norm.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3])
encoder.stages.1.0.convs.1.all_modules.0.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.1.weight shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.1.bias shape torch.Size([64])
encoder.stages.2.0.convs.0.conv.weight shape torch.Size([128, 64, 3, 3])
encoder.stages.2.0.convs.0.conv.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.norm.weight shape torch.Size([128])
encoder.stages.2.0.convs.0.norm.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.0.weight shape torch.Size([128, 64, 3, 3])
encoder.stages.2.0.convs.0.all_modules.0.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.1.weight shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.1.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.conv.weight shape torch.Size([128, 128, 3, 3])
encoder.stages.2.0.convs.1.conv.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.norm.weight shape torch.Size([128])
encoder.stages.2.0.convs.1.norm.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3])
encoder.stages.2.0.convs.1.all_modules.0.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.1.weight shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.1.bias shape torch.Size([128])
encoder.stages.3.0.convs.0.conv.weight shape torch.Size([256, 128, 3, 3])
encoder.stages.3.0.convs.0.conv.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.norm.weight shape torch.Size([256])
encoder.stages.3.0.convs.0.norm.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.0.weight shape torch.Size([256, 128, 3, 3])
encoder.stages.3.0.convs.0.all_modules.0.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.1.weight shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.1.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.conv.weight shape torch.Size([256, 256, 3, 3])
encoder.stages.3.0.convs.1.conv.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.norm.weight shape torch.Size([256])
encoder.stages.3.0.convs.1.norm.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3])
encoder.stages.3.0.convs.1.all_modules.0.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.1.weight shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.1.bias shape torch.Size([256])
encoder.stages.4.0.convs.0.conv.weight shape torch.Size([512, 256, 3, 3])
encoder.stages.4.0.convs.0.conv.bias shape torch.Size([512])
encoder.stages.4.0.convs.0.norm.weight shape torch.Size([512])
encoder.stages.4.0.convs.0.norm.bias shape torch.Size([512])
encoder.stages.4.0.convs.0.all_modules.0.weight shape torch.Size([512, 256, 3, 3])
encoder.stages.4.0.convs.0.all_modules.0.bias shape torch.Size([512])
encoder.stages.4.0.convs.0.all_modules.1.weight shape torch.Size([512])
encoder.stages.4.0.convs.0.all_modules.1.bias shape torch.Size([512])
encoder.stages.4.0.convs.1.conv.weight shape torch.Size([512, 512, 3, 3])
encoder.stages.4.0.convs.1.conv.bias shape torch.Size([512])
encoder.stages.4.0.convs.1.norm.weight shape torch.Size([512])
encoder.stages.4.0.convs.1.norm.bias shape torch.Size([512])
encoder.stages.4.0.convs.1.all_modules.0.weight shape torch.Size([512, 512, 3, 3])
encoder.stages.4.0.convs.1.all_modules.0.bias shape torch.Size([512])
encoder.stages.4.0.convs.1.all_modules.1.weight shape torch.Size([512])
encoder.stages.4.0.convs.1.all_modules.1.bias shape torch.Size([512])
encoder.stages.5.0.convs.0.conv.weight shape torch.Size([512, 512, 3, 3])
encoder.stages.5.0.convs.0.conv.bias shape torch.Size([512])
encoder.stages.5.0.convs.0.norm.weight shape torch.Size([512])
encoder.stages.5.0.convs.0.norm.bias shape torch.Size([512])
encoder.stages.5.0.convs.0.all_modules.0.weight shape torch.Size([512, 512, 3, 3])
encoder.stages.5.0.convs.0.all_modules.0.bias shape torch.Size([512])
encoder.stages.5.0.convs.0.all_modules.1.weight shape torch.Size([512])
encoder.stages.5.0.convs.0.all_modules.1.bias shape torch.Size([512])
encoder.stages.5.0.convs.1.conv.weight shape torch.Size([512, 512, 3, 3])
encoder.stages.5.0.convs.1.conv.bias shape torch.Size([512])
encoder.stages.5.0.convs.1.norm.weight shape torch.Size([512])
encoder.stages.5.0.convs.1.norm.bias shape torch.Size([512])
encoder.stages.5.0.convs.1.all_modules.0.weight shape torch.Size([512, 512, 3, 3])
encoder.stages.5.0.convs.1.all_modules.0.bias shape torch.Size([512])
encoder.stages.5.0.convs.1.all_modules.1.weight shape torch.Size([512])
encoder.stages.5.0.convs.1.all_modules.1.bias shape torch.Size([512])
encoder.stages.6.0.convs.0.conv.weight shape torch.Size([512, 512, 3, 3])
encoder.stages.6.0.convs.0.conv.bias shape torch.Size([512])
encoder.stages.6.0.convs.0.norm.weight shape torch.Size([512])
encoder.stages.6.0.convs.0.norm.bias shape torch.Size([512])
encoder.stages.6.0.convs.0.all_modules.0.weight shape torch.Size([512, 512, 3, 3])
encoder.stages.6.0.convs.0.all_modules.0.bias shape torch.Size([512])
encoder.stages.6.0.convs.0.all_modules.1.weight shape torch.Size([512])
encoder.stages.6.0.convs.0.all_modules.1.bias shape torch.Size([512])
encoder.stages.6.0.convs.1.conv.weight shape torch.Size([512, 512, 3, 3])
encoder.stages.6.0.convs.1.conv.bias shape torch.Size([512])
encoder.stages.6.0.convs.1.norm.weight shape torch.Size([512])
encoder.stages.6.0.convs.1.norm.bias shape torch.Size([512])
encoder.stages.6.0.convs.1.all_modules.0.weight shape torch.Size([512, 512, 3, 3])
encoder.stages.6.0.convs.1.all_modules.0.bias shape torch.Size([512])
encoder.stages.6.0.convs.1.all_modules.1.weight shape torch.Size([512])
encoder.stages.6.0.convs.1.all_modules.1.bias shape torch.Size([512])
encoder.stages.7.0.convs.0.conv.weight shape torch.Size([512, 512, 3, 3])
encoder.stages.7.0.convs.0.conv.bias shape torch.Size([512])
encoder.stages.7.0.convs.0.norm.weight shape torch.Size([512])
encoder.stages.7.0.convs.0.norm.bias shape torch.Size([512])
encoder.stages.7.0.convs.0.all_modules.0.weight shape torch.Size([512, 512, 3, 3])
encoder.stages.7.0.convs.0.all_modules.0.bias shape torch.Size([512])
encoder.stages.7.0.convs.0.all_modules.1.weight shape torch.Size([512])
encoder.stages.7.0.convs.0.all_modules.1.bias shape torch.Size([512])
encoder.stages.7.0.convs.1.conv.weight shape torch.Size([512, 512, 3, 3])
encoder.stages.7.0.convs.1.conv.bias shape torch.Size([512])
encoder.stages.7.0.convs.1.norm.weight shape torch.Size([512])
encoder.stages.7.0.convs.1.norm.bias shape torch.Size([512])
encoder.stages.7.0.convs.1.all_modules.0.weight shape torch.Size([512, 512, 3, 3])
encoder.stages.7.0.convs.1.all_modules.0.bias shape torch.Size([512])
encoder.stages.7.0.convs.1.all_modules.1.weight shape torch.Size([512])
encoder.stages.7.0.convs.1.all_modules.1.bias shape torch.Size([512])
decoder.encoder.stages.0.0.convs.0.conv.weight shape torch.Size([32, 1, 3, 3])
decoder.encoder.stages.0.0.convs.0.conv.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.norm.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.norm.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.0.weight shape torch.Size([32, 1, 3, 3])
decoder.encoder.stages.0.0.convs.0.all_modules.0.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.1.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.1.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.conv.weight shape torch.Size([32, 32, 3, 3])
decoder.encoder.stages.0.0.convs.1.conv.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.norm.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.norm.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3])
decoder.encoder.stages.0.0.convs.1.all_modules.0.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.1.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.1.bias shape torch.Size([32])
decoder.encoder.stages.1.0.convs.0.conv.weight shape torch.Size([64, 32, 3, 3])
decoder.encoder.stages.1.0.convs.0.conv.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.norm.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.norm.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.0.weight shape torch.Size([64, 32, 3, 3])
decoder.encoder.stages.1.0.convs.0.all_modules.0.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.1.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.1.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.conv.weight shape torch.Size([64, 64, 3, 3])
decoder.encoder.stages.1.0.convs.1.conv.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.norm.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.norm.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3])
decoder.encoder.stages.1.0.convs.1.all_modules.0.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.1.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.1.bias shape torch.Size([64])
decoder.encoder.stages.2.0.convs.0.conv.weight shape torch.Size([128, 64, 3, 3])
decoder.encoder.stages.2.0.convs.0.conv.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.norm.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.norm.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.0.weight shape torch.Size([128, 64, 3, 3])
decoder.encoder.stages.2.0.convs.0.all_modules.0.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.1.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.1.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.conv.weight shape torch.Size([128, 128, 3, 3])
decoder.encoder.stages.2.0.convs.1.conv.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.norm.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.norm.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3])
decoder.encoder.stages.2.0.convs.1.all_modules.0.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.1.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.1.bias shape torch.Size([128])
decoder.encoder.stages.3.0.convs.0.conv.weight shape torch.Size([256, 128, 3, 3])
decoder.encoder.stages.3.0.convs.0.conv.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.norm.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.norm.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.0.weight shape torch.Size([256, 128, 3, 3])
decoder.encoder.stages.3.0.convs.0.all_modules.0.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.1.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.1.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.conv.weight shape torch.Size([256, 256, 3, 3])
decoder.encoder.stages.3.0.convs.1.conv.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.norm.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.norm.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3])
decoder.encoder.stages.3.0.convs.1.all_modules.0.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.1.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.1.bias shape torch.Size([256])
decoder.encoder.stages.4.0.convs.0.conv.weight shape torch.Size([512, 256, 3, 3])
decoder.encoder.stages.4.0.convs.0.conv.bias shape torch.Size([512])
decoder.encoder.stages.4.0.convs.0.norm.weight shape torch.Size([512])
decoder.encoder.stages.4.0.convs.0.norm.bias shape torch.Size([512])
decoder.encoder.stages.4.0.convs.0.all_modules.0.weight shape torch.Size([512, 256, 3, 3])
decoder.encoder.stages.4.0.convs.0.all_modules.0.bias shape torch.Size([512])
decoder.encoder.stages.4.0.convs.0.all_modules.1.weight shape torch.Size([512])
decoder.encoder.stages.4.0.convs.0.all_modules.1.bias shape torch.Size([512])
decoder.encoder.stages.4.0.convs.1.conv.weight shape torch.Size([512, 512, 3, 3])
decoder.encoder.stages.4.0.convs.1.conv.bias shape torch.Size([512])
decoder.encoder.stages.4.0.convs.1.norm.weight shape torch.Size([512])
decoder.encoder.stages.4.0.convs.1.norm.bias shape torch.Size([512])
decoder.encoder.stages.4.0.convs.1.all_modules.0.weight shape torch.Size([512, 512, 3, 3])
decoder.encoder.stages.4.0.convs.1.all_modules.0.bias shape torch.Size([512])
decoder.encoder.stages.4.0.convs.1.all_modules.1.weight shape torch.Size([512])
decoder.encoder.stages.4.0.convs.1.all_modules.1.bias shape torch.Size([512])
decoder.encoder.stages.5.0.convs.0.conv.weight shape torch.Size([512, 512, 3, 3])
decoder.encoder.stages.5.0.convs.0.conv.bias shape torch.Size([512])
decoder.encoder.stages.5.0.convs.0.norm.weight shape torch.Size([512])
decoder.encoder.stages.5.0.convs.0.norm.bias shape torch.Size([512])
decoder.encoder.stages.5.0.convs.0.all_modules.0.weight shape torch.Size([512, 512, 3, 3])
decoder.encoder.stages.5.0.convs.0.all_modules.0.bias shape torch.Size([512])
decoder.encoder.stages.5.0.convs.0.all_modules.1.weight shape torch.Size([512])
decoder.encoder.stages.5.0.convs.0.all_modules.1.bias shape torch.Size([512])
decoder.encoder.stages.5.0.convs.1.conv.weight shape torch.Size([512, 512, 3, 3])
decoder.encoder.stages.5.0.convs.1.conv.bias shape torch.Size([512])
decoder.encoder.stages.5.0.convs.1.norm.weight shape torch.Size([512])
decoder.encoder.stages.5.0.convs.1.norm.bias shape torch.Size([512])
decoder.encoder.stages.5.0.convs.1.all_modules.0.weight shape torch.Size([512, 512, 3, 3])
decoder.encoder.stages.5.0.convs.1.all_modules.0.bias shape torch.Size([512])
decoder.encoder.stages.5.0.convs.1.all_modules.1.weight shape torch.Size([512])
decoder.encoder.stages.5.0.convs.1.all_modules.1.bias shape torch.Size([512])
decoder.encoder.stages.6.0.convs.0.conv.weight shape torch.Size([512, 512, 3, 3])
decoder.encoder.stages.6.0.convs.0.conv.bias shape torch.Size([512])
decoder.encoder.stages.6.0.convs.0.norm.weight shape torch.Size([512])
decoder.encoder.stages.6.0.convs.0.norm.bias shape torch.Size([512])
decoder.encoder.stages.6.0.convs.0.all_modules.0.weight shape torch.Size([512, 512, 3, 3])
decoder.encoder.stages.6.0.convs.0.all_modules.0.bias shape torch.Size([512])
decoder.encoder.stages.6.0.convs.0.all_modules.1.weight shape torch.Size([512])
decoder.encoder.stages.6.0.convs.0.all_modules.1.bias shape torch.Size([512])
decoder.encoder.stages.6.0.convs.1.conv.weight shape torch.Size([512, 512, 3, 3])
decoder.encoder.stages.6.0.convs.1.conv.bias shape torch.Size([512])
decoder.encoder.stages.6.0.convs.1.norm.weight shape torch.Size([512])
decoder.encoder.stages.6.0.convs.1.norm.bias shape torch.Size([512])
decoder.encoder.stages.6.0.convs.1.all_modules.0.weight shape torch.Size([512, 512, 3, 3])
decoder.encoder.stages.6.0.convs.1.all_modules.0.bias shape torch.Size([512])
decoder.encoder.stages.6.0.convs.1.all_modules.1.weight shape torch.Size([512])
decoder.encoder.stages.6.0.convs.1.all_modules.1.bias shape torch.Size([512])
decoder.encoder.stages.7.0.convs.0.conv.weight shape torch.Size([512, 512, 3, 3])
decoder.encoder.stages.7.0.convs.0.conv.bias shape torch.Size([512])
decoder.encoder.stages.7.0.convs.0.norm.weight shape torch.Size([512])
decoder.encoder.stages.7.0.convs.0.norm.bias shape torch.Size([512])
decoder.encoder.stages.7.0.convs.0.all_modules.0.weight shape torch.Size([512, 512, 3, 3])
decoder.encoder.stages.7.0.convs.0.all_modules.0.bias shape torch.Size([512])
decoder.encoder.stages.7.0.convs.0.all_modules.1.weight shape torch.Size([512])
decoder.encoder.stages.7.0.convs.0.all_modules.1.bias shape torch.Size([512])
decoder.encoder.stages.7.0.convs.1.conv.weight shape torch.Size([512, 512, 3, 3])
decoder.encoder.stages.7.0.convs.1.conv.bias shape torch.Size([512])
decoder.encoder.stages.7.0.convs.1.norm.weight shape torch.Size([512])
decoder.encoder.stages.7.0.convs.1.norm.bias shape torch.Size([512])
decoder.encoder.stages.7.0.convs.1.all_modules.0.weight shape torch.Size([512, 512, 3, 3])
decoder.encoder.stages.7.0.convs.1.all_modules.0.bias shape torch.Size([512])
decoder.encoder.stages.7.0.convs.1.all_modules.1.weight shape torch.Size([512])
decoder.encoder.stages.7.0.convs.1.all_modules.1.bias shape torch.Size([512])
decoder.stages.0.convs.0.conv.weight shape torch.Size([512, 1024, 3, 3])
decoder.stages.0.convs.0.conv.bias shape torch.Size([512])
decoder.stages.0.convs.0.norm.weight shape torch.Size([512])
decoder.stages.0.convs.0.norm.bias shape torch.Size([512])
decoder.stages.0.convs.0.all_modules.0.weight shape torch.Size([512, 1024, 3, 3])
decoder.stages.0.convs.0.all_modules.0.bias shape torch.Size([512])
decoder.stages.0.convs.0.all_modules.1.weight shape torch.Size([512])
decoder.stages.0.convs.0.all_modules.1.bias shape torch.Size([512])
decoder.stages.0.convs.1.conv.weight shape torch.Size([512, 512, 3, 3])
decoder.stages.0.convs.1.conv.bias shape torch.Size([512])
decoder.stages.0.convs.1.norm.weight shape torch.Size([512])
decoder.stages.0.convs.1.norm.bias shape torch.Size([512])
decoder.stages.0.convs.1.all_modules.0.weight shape torch.Size([512, 512, 3, 3])
decoder.stages.0.convs.1.all_modules.0.bias shape torch.Size([512])
decoder.stages.0.convs.1.all_modules.1.weight shape torch.Size([512])
decoder.stages.0.convs.1.all_modules.1.bias shape torch.Size([512])
decoder.stages.1.convs.0.conv.weight shape torch.Size([512, 1024, 3, 3])
decoder.stages.1.convs.0.conv.bias shape torch.Size([512])
decoder.stages.1.convs.0.norm.weight shape torch.Size([512])
decoder.stages.1.convs.0.norm.bias shape torch.Size([512])
decoder.stages.1.convs.0.all_modules.0.weight shape torch.Size([512, 1024, 3, 3])
decoder.stages.1.convs.0.all_modules.0.bias shape torch.Size([512])
decoder.stages.1.convs.0.all_modules.1.weight shape torch.Size([512])
decoder.stages.1.convs.0.all_modules.1.bias shape torch.Size([512])
decoder.stages.1.convs.1.conv.weight shape torch.Size([512, 512, 3, 3])
decoder.stages.1.convs.1.conv.bias shape torch.Size([512])
decoder.stages.1.convs.1.norm.weight shape torch.Size([512])
decoder.stages.1.convs.1.norm.bias shape torch.Size([512])
decoder.stages.1.convs.1.all_modules.0.weight shape torch.Size([512, 512, 3, 3])
decoder.stages.1.convs.1.all_modules.0.bias shape torch.Size([512])
decoder.stages.1.convs.1.all_modules.1.weight shape torch.Size([512])
decoder.stages.1.convs.1.all_modules.1.bias shape torch.Size([512])
decoder.stages.2.convs.0.conv.weight shape torch.Size([512, 1024, 3, 3])
decoder.stages.2.convs.0.conv.bias shape torch.Size([512])
decoder.stages.2.convs.0.norm.weight shape torch.Size([512])
decoder.stages.2.convs.0.norm.bias shape torch.Size([512])
decoder.stages.2.convs.0.all_modules.0.weight shape torch.Size([512, 1024, 3, 3])
decoder.stages.2.convs.0.all_modules.0.bias shape torch.Size([512])
decoder.stages.2.convs.0.all_modules.1.weight shape torch.Size([512])
decoder.stages.2.convs.0.all_modules.1.bias shape torch.Size([512])
decoder.stages.2.convs.1.conv.weight shape torch.Size([512, 512, 3, 3])
decoder.stages.2.convs.1.conv.bias shape torch.Size([512])
decoder.stages.2.convs.1.norm.weight shape torch.Size([512])
decoder.stages.2.convs.1.norm.bias shape torch.Size([512])
decoder.stages.2.convs.1.all_modules.0.weight shape torch.Size([512, 512, 3, 3])
decoder.stages.2.convs.1.all_modules.0.bias shape torch.Size([512])
decoder.stages.2.convs.1.all_modules.1.weight shape torch.Size([512])
decoder.stages.2.convs.1.all_modules.1.bias shape torch.Size([512])
decoder.stages.3.convs.0.conv.weight shape torch.Size([256, 512, 3, 3])
decoder.stages.3.convs.0.conv.bias shape torch.Size([256])
decoder.stages.3.convs.0.norm.weight shape torch.Size([256])
decoder.stages.3.convs.0.norm.bias shape torch.Size([256])
decoder.stages.3.convs.0.all_modules.0.weight shape torch.Size([256, 512, 3, 3])
decoder.stages.3.convs.0.all_modules.0.bias shape torch.Size([256])
decoder.stages.3.convs.0.all_modules.1.weight shape torch.Size([256])
decoder.stages.3.convs.0.all_modules.1.bias shape torch.Size([256])
decoder.stages.3.convs.1.conv.weight shape torch.Size([256, 256, 3, 3])
decoder.stages.3.convs.1.conv.bias shape torch.Size([256])
decoder.stages.3.convs.1.norm.weight shape torch.Size([256])
decoder.stages.3.convs.1.norm.bias shape torch.Size([256])
decoder.stages.3.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3])
decoder.stages.3.convs.1.all_modules.0.bias shape torch.Size([256])
decoder.stages.3.convs.1.all_modules.1.weight shape torch.Size([256])
decoder.stages.3.convs.1.all_modules.1.bias shape torch.Size([256])
decoder.stages.4.convs.0.conv.weight shape torch.Size([128, 256, 3, 3])
decoder.stages.4.convs.0.conv.bias shape torch.Size([128])
decoder.stages.4.convs.0.norm.weight shape torch.Size([128])
decoder.stages.4.convs.0.norm.bias shape torch.Size([128])
decoder.stages.4.convs.0.all_modules.0.weight shape torch.Size([128, 256, 3, 3])
decoder.stages.4.convs.0.all_modules.0.bias shape torch.Size([128])
decoder.stages.4.convs.0.all_modules.1.weight shape torch.Size([128])
decoder.stages.4.convs.0.all_modules.1.bias shape torch.Size([128])
decoder.stages.4.convs.1.conv.weight shape torch.Size([128, 128, 3, 3])
decoder.stages.4.convs.1.conv.bias shape torch.Size([128])
decoder.stages.4.convs.1.norm.weight shape torch.Size([128])
decoder.stages.4.convs.1.norm.bias shape torch.Size([128])
decoder.stages.4.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3])
decoder.stages.4.convs.1.all_modules.0.bias shape torch.Size([128])
decoder.stages.4.convs.1.all_modules.1.weight shape torch.Size([128])
decoder.stages.4.convs.1.all_modules.1.bias shape torch.Size([128])
decoder.stages.5.convs.0.conv.weight shape torch.Size([64, 128, 3, 3])
decoder.stages.5.convs.0.conv.bias shape torch.Size([64])
decoder.stages.5.convs.0.norm.weight shape torch.Size([64])
decoder.stages.5.convs.0.norm.bias shape torch.Size([64])
decoder.stages.5.convs.0.all_modules.0.weight shape torch.Size([64, 128, 3, 3])
decoder.stages.5.convs.0.all_modules.0.bias shape torch.Size([64])
decoder.stages.5.convs.0.all_modules.1.weight shape torch.Size([64])
decoder.stages.5.convs.0.all_modules.1.bias shape torch.Size([64])
decoder.stages.5.convs.1.conv.weight shape torch.Size([64, 64, 3, 3])
decoder.stages.5.convs.1.conv.bias shape torch.Size([64])
decoder.stages.5.convs.1.norm.weight shape torch.Size([64])
decoder.stages.5.convs.1.norm.bias shape torch.Size([64])
decoder.stages.5.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3])
decoder.stages.5.convs.1.all_modules.0.bias shape torch.Size([64])
decoder.stages.5.convs.1.all_modules.1.weight shape torch.Size([64])
decoder.stages.5.convs.1.all_modules.1.bias shape torch.Size([64])
decoder.stages.6.convs.0.conv.weight shape torch.Size([32, 64, 3, 3])
decoder.stages.6.convs.0.conv.bias shape torch.Size([32])
decoder.stages.6.convs.0.norm.weight shape torch.Size([32])
decoder.stages.6.convs.0.norm.bias shape torch.Size([32])
decoder.stages.6.convs.0.all_modules.0.weight shape torch.Size([32, 64, 3, 3])
decoder.stages.6.convs.0.all_modules.0.bias shape torch.Size([32])
decoder.stages.6.convs.0.all_modules.1.weight shape torch.Size([32])
decoder.stages.6.convs.0.all_modules.1.bias shape torch.Size([32])
decoder.stages.6.convs.1.conv.weight shape torch.Size([32, 32, 3, 3])
decoder.stages.6.convs.1.conv.bias shape torch.Size([32])
decoder.stages.6.convs.1.norm.weight shape torch.Size([32])
decoder.stages.6.convs.1.norm.bias shape torch.Size([32])
decoder.stages.6.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3])
decoder.stages.6.convs.1.all_modules.0.bias shape torch.Size([32])
decoder.stages.6.convs.1.all_modules.1.weight shape torch.Size([32])
decoder.stages.6.convs.1.all_modules.1.bias shape torch.Size([32])
decoder.transpconvs.0.weight shape torch.Size([512, 512, 2, 2])
decoder.transpconvs.0.bias shape torch.Size([512])
decoder.transpconvs.1.weight shape torch.Size([512, 512, 2, 2])
decoder.transpconvs.1.bias shape torch.Size([512])
decoder.transpconvs.2.weight shape torch.Size([512, 512, 2, 2])
decoder.transpconvs.2.bias shape torch.Size([512])
decoder.transpconvs.3.weight shape torch.Size([512, 256, 2, 2])
decoder.transpconvs.3.bias shape torch.Size([256])
decoder.transpconvs.4.weight shape torch.Size([256, 128, 2, 2])
decoder.transpconvs.4.bias shape torch.Size([128])
decoder.transpconvs.5.weight shape torch.Size([128, 64, 2, 2])
decoder.transpconvs.5.bias shape torch.Size([64])
decoder.transpconvs.6.weight shape torch.Size([64, 32, 2, 2])
decoder.transpconvs.6.bias shape torch.Size([32])
################### Done ###################
2024-05-06 18:28:12.409893: do_dummy_2d_data_aug: False
2024-05-06 18:28:12.413000: Using splits from existing split file: /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset302_Calcium_OCTv2/splits_final.json
2024-05-06 18:28:12.416548: The split file contains 3 splits.
2024-05-06 18:28:12.418184: Desired fold for training: 0
2024-05-06 18:28:12.419563: This split has 4 training and 2 validation cases.
using pin_memory on device 0
using pin_memory on device 0

This is the configuration used by this training:
Configuration name: 2d
 {'data_identifier': 'nnUNetPlans_2d', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 12, 'patch_size': [512, 512], 'median_image_size_in_voxels': [498.0, 498.0], 'spacing': [1.0, 1.0], 'normalization_schemes': ['ZScoreNormalization'], 'use_mask_for_norm': [False], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'architecture': {'network_class_name': 'dynamic_network_architectures.architectures.unet.PlainConvUNet', 'arch_kwargs': {'n_stages': 8, 'features_per_stage': [32, 64, 128, 256, 512, 512, 512, 512], 'conv_op': 'torch.nn.modules.conv.Conv2d', 'kernel_sizes': [[3, 3], [3, 3], [3, 3], [3, 3], [3, 3], [3, 3], [3, 3], [3, 3]], 'strides': [[1, 1], [2, 2], [2, 2], [2, 2], [2, 2], [2, 2], [2, 2], [2, 2]], 'n_conv_per_stage': [2, 2, 2, 2, 2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2, 2, 2, 2, 2], 'conv_bias': True, 'norm_op': 'torch.nn.modules.instancenorm.InstanceNorm2d', 'norm_op_kwargs': {'eps': 1e-05, 'affine': True}, 'dropout_op': None, 'dropout_op_kwargs': None, 'nonlin': 'torch.nn.LeakyReLU', 'nonlin_kwargs': {'inplace': True}, 'deep_supervision': True}, '_kw_requires_import': ['conv_op', 'norm_op', 'dropout_op', 'nonlin']}, 'batch_dice': True} 

These are the global plan.json settings:
 {'dataset_name': 'Dataset302_Calcium_OCTv2', 'plans_name': 'nnUNetPlans', 'original_median_spacing_after_transp': [1.0, 1.0, 1.0], 'original_median_shape_after_transp': [375, 498, 498], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 0.4977976381778717, 'mean': 0.13332499563694, 'median': 0.08871842920780182, 'min': 0.0, 'percentile_00_5': 0.014754901640117168, 'percentile_99_5': 0.4977976381778717, 'std': 0.12022686749696732}}} 

2024-05-06 18:28:15.676702: unpacking dataset...
2024-05-06 18:28:21.384580: unpacking done...
2024-05-06 18:28:21.410950: Unable to plot network architecture: nnUNet_compile is enabled!
2024-05-06 18:28:21.497123: 
2024-05-06 18:28:21.498923: Epoch 0
2024-05-06 18:28:21.500672: Current learning rate: 0.01
2024-05-06 18:31:53.083617: Validation loss improved from 1000.00000 to -0.59334! Patience: 0/50
2024-05-06 18:31:53.112990: train_loss -0.537
2024-05-06 18:31:53.129243: val_loss -0.5933
2024-05-06 18:31:53.130428: Pseudo dice [0.7052]
2024-05-06 18:31:53.131456: Epoch time: 211.59 s
2024-05-06 18:31:53.132371: Yayy! New best EMA pseudo Dice: 0.7052
2024-05-06 18:31:55.555011: 
2024-05-06 18:31:55.557089: Epoch 1
2024-05-06 18:31:55.558401: Current learning rate: 0.00999
2024-05-06 18:32:41.783395: Validation loss improved from -0.59334 to -0.61099! Patience: 0/50
2024-05-06 18:32:41.785673: train_loss -0.6857
2024-05-06 18:32:41.786912: val_loss -0.611
2024-05-06 18:32:41.788081: Pseudo dice [0.7068]
2024-05-06 18:32:41.789253: Epoch time: 46.23 s
2024-05-06 18:32:41.790225: Yayy! New best EMA pseudo Dice: 0.7054
2024-05-06 18:32:43.514028: 
2024-05-06 18:32:43.516050: Epoch 2
2024-05-06 18:32:43.517082: Current learning rate: 0.00998
2024-05-06 18:33:29.725181: Validation loss improved from -0.61099 to -0.64976! Patience: 0/50
2024-05-06 18:33:29.727324: train_loss -0.7239
2024-05-06 18:33:29.728507: val_loss -0.6498
2024-05-06 18:33:29.729541: Pseudo dice [0.7397]
2024-05-06 18:33:29.731076: Epoch time: 46.21 s
2024-05-06 18:33:29.732334: Yayy! New best EMA pseudo Dice: 0.7088
2024-05-06 18:33:31.516499: 
2024-05-06 18:33:31.519099: Epoch 3
2024-05-06 18:33:31.520758: Current learning rate: 0.00997
2024-05-06 18:34:17.741802: Validation loss improved from -0.64976 to -0.65272! Patience: 0/50
2024-05-06 18:34:17.743426: train_loss -0.7525
2024-05-06 18:34:17.744435: val_loss -0.6527
2024-05-06 18:34:17.745373: Pseudo dice [0.7441]
2024-05-06 18:34:17.746343: Epoch time: 46.23 s
2024-05-06 18:34:17.747207: Yayy! New best EMA pseudo Dice: 0.7124
2024-05-06 18:34:19.474445: 
2024-05-06 18:34:19.476425: Epoch 4
2024-05-06 18:34:19.478057: Current learning rate: 0.00996
2024-05-06 18:35:05.688990: Validation loss improved from -0.65272 to -0.65714! Patience: 0/50
2024-05-06 18:35:05.691524: train_loss -0.7605
2024-05-06 18:35:05.693373: val_loss -0.6571
2024-05-06 18:35:05.694766: Pseudo dice [0.7501]
2024-05-06 18:35:05.696067: Epoch time: 46.22 s
2024-05-06 18:35:06.121472: Yayy! New best EMA pseudo Dice: 0.7161
2024-05-06 18:35:07.871105: 
2024-05-06 18:35:07.874042: Epoch 5
2024-05-06 18:35:07.875417: Current learning rate: 0.00995
2024-05-06 18:35:54.151339: Validation loss improved from -0.65714 to -0.66511! Patience: 0/50
2024-05-06 18:35:54.154006: train_loss -0.7718
2024-05-06 18:35:54.155176: val_loss -0.6651
2024-05-06 18:35:54.156350: Pseudo dice [0.7521]
2024-05-06 18:35:54.157404: Epoch time: 46.28 s
2024-05-06 18:35:54.158412: Yayy! New best EMA pseudo Dice: 0.7197
2024-05-06 18:35:55.825811: 
2024-05-06 18:35:55.827476: Epoch 6
2024-05-06 18:35:55.828960: Current learning rate: 0.00995
2024-05-06 18:36:42.031662: Validation loss did not improve from -0.66511. Patience: 1/50
2024-05-06 18:36:42.033577: train_loss -0.788
2024-05-06 18:36:42.034701: val_loss -0.6516
2024-05-06 18:36:42.035768: Pseudo dice [0.7403]
2024-05-06 18:36:42.036884: Epoch time: 46.21 s
2024-05-06 18:36:42.037884: Yayy! New best EMA pseudo Dice: 0.7218
2024-05-06 18:36:43.784231: 
2024-05-06 18:36:43.786056: Epoch 7
2024-05-06 18:36:43.787277: Current learning rate: 0.00994
2024-05-06 18:37:29.984064: Validation loss did not improve from -0.66511. Patience: 2/50
2024-05-06 18:37:29.986125: train_loss -0.7905
2024-05-06 18:37:29.987906: val_loss -0.6607
2024-05-06 18:37:29.989548: Pseudo dice [0.75]
2024-05-06 18:37:29.990772: Epoch time: 46.2 s
2024-05-06 18:37:29.991800: Yayy! New best EMA pseudo Dice: 0.7246
2024-05-06 18:37:31.737390: 
2024-05-06 18:37:31.739805: Epoch 8
2024-05-06 18:37:31.741282: Current learning rate: 0.00993
2024-05-06 18:38:18.033013: Validation loss improved from -0.66511 to -0.69976! Patience: 2/50
2024-05-06 18:38:18.036241: train_loss -0.7975
2024-05-06 18:38:18.037749: val_loss -0.6998
2024-05-06 18:38:18.038768: Pseudo dice [0.7816]
2024-05-06 18:38:18.039897: Epoch time: 46.3 s
2024-05-06 18:38:18.040897: Yayy! New best EMA pseudo Dice: 0.7303
2024-05-06 18:38:20.218122: 
2024-05-06 18:38:20.221223: Epoch 9
2024-05-06 18:38:20.222587: Current learning rate: 0.00992
2024-05-06 18:39:06.556683: Validation loss did not improve from -0.69976. Patience: 1/50
2024-05-06 18:39:06.559821: train_loss -0.8007
2024-05-06 18:39:06.561081: val_loss -0.6932
2024-05-06 18:39:06.562216: Pseudo dice [0.7705]
2024-05-06 18:39:06.563561: Epoch time: 46.34 s
2024-05-06 18:39:07.051219: Yayy! New best EMA pseudo Dice: 0.7343
2024-05-06 18:39:08.750717: 
2024-05-06 18:39:08.754766: Epoch 10
2024-05-06 18:39:08.756187: Current learning rate: 0.00991
2024-05-06 18:39:55.023479: Validation loss did not improve from -0.69976. Patience: 2/50
2024-05-06 18:39:55.025366: train_loss -0.8089
2024-05-06 18:39:55.026686: val_loss -0.6692
2024-05-06 18:39:55.027728: Pseudo dice [0.7536]
2024-05-06 18:39:55.028793: Epoch time: 46.28 s
2024-05-06 18:39:55.029654: Yayy! New best EMA pseudo Dice: 0.7363
2024-05-06 18:39:56.757904: 
2024-05-06 18:39:56.759657: Epoch 11
2024-05-06 18:39:56.760814: Current learning rate: 0.0099
2024-05-06 18:40:42.942225: Validation loss did not improve from -0.69976. Patience: 3/50
2024-05-06 18:40:42.944024: train_loss -0.8143
2024-05-06 18:40:42.945228: val_loss -0.6993
2024-05-06 18:40:42.946350: Pseudo dice [0.7774]
2024-05-06 18:40:42.947473: Epoch time: 46.19 s
2024-05-06 18:40:42.948562: Yayy! New best EMA pseudo Dice: 0.7404
2024-05-06 18:40:44.678391: 
2024-05-06 18:40:44.680313: Epoch 12
2024-05-06 18:40:44.681737: Current learning rate: 0.00989
2024-05-06 18:41:30.769328: Validation loss did not improve from -0.69976. Patience: 4/50
2024-05-06 18:41:30.772063: train_loss -0.8148
2024-05-06 18:41:30.773284: val_loss -0.6873
2024-05-06 18:41:30.774513: Pseudo dice [0.768]
2024-05-06 18:41:30.775609: Epoch time: 46.1 s
2024-05-06 18:41:30.780707: Yayy! New best EMA pseudo Dice: 0.7431
2024-05-06 18:41:32.514202: 
2024-05-06 18:41:32.516046: Epoch 13
2024-05-06 18:41:32.517334: Current learning rate: 0.00988
2024-05-06 18:42:18.623075: Validation loss did not improve from -0.69976. Patience: 5/50
2024-05-06 18:42:18.625067: train_loss -0.8133
2024-05-06 18:42:18.626356: val_loss -0.6996
2024-05-06 18:42:18.627778: Pseudo dice [0.7743]
2024-05-06 18:42:18.628959: Epoch time: 46.11 s
2024-05-06 18:42:18.630128: Yayy! New best EMA pseudo Dice: 0.7463
2024-05-06 18:42:20.401783: 
2024-05-06 18:42:20.404530: Epoch 14
2024-05-06 18:42:20.406034: Current learning rate: 0.00987
2024-05-06 18:43:06.583906: Validation loss did not improve from -0.69976. Patience: 6/50
2024-05-06 18:43:06.586024: train_loss -0.8201
2024-05-06 18:43:06.587577: val_loss -0.6893
2024-05-06 18:43:06.588995: Pseudo dice [0.7699]
2024-05-06 18:43:06.590229: Epoch time: 46.19 s
2024-05-06 18:43:07.077059: Yayy! New best EMA pseudo Dice: 0.7486
2024-05-06 18:43:08.824938: 
2024-05-06 18:43:08.826911: Epoch 15
2024-05-06 18:43:08.828396: Current learning rate: 0.00986
2024-05-06 18:43:54.942411: Validation loss improved from -0.69976 to -0.70456! Patience: 6/50
2024-05-06 18:43:54.944470: train_loss -0.8255
2024-05-06 18:43:54.945526: val_loss -0.7046
2024-05-06 18:43:54.946493: Pseudo dice [0.7838]
2024-05-06 18:43:54.947451: Epoch time: 46.12 s
2024-05-06 18:43:54.948380: Yayy! New best EMA pseudo Dice: 0.7521
2024-05-06 18:43:56.704195: 
2024-05-06 18:43:56.706006: Epoch 16
2024-05-06 18:43:56.706926: Current learning rate: 0.00986
2024-05-06 18:44:42.885310: Validation loss did not improve from -0.70456. Patience: 1/50
2024-05-06 18:44:42.887187: train_loss -0.8243
2024-05-06 18:44:42.888505: val_loss -0.691
2024-05-06 18:44:42.889545: Pseudo dice [0.7746]
2024-05-06 18:44:42.890664: Epoch time: 46.18 s
2024-05-06 18:44:42.891774: Yayy! New best EMA pseudo Dice: 0.7544
2024-05-06 18:44:44.691009: 
2024-05-06 18:44:44.693623: Epoch 17
2024-05-06 18:44:44.694932: Current learning rate: 0.00985
2024-05-06 18:45:30.865853: Validation loss did not improve from -0.70456. Patience: 2/50
2024-05-06 18:45:30.868524: train_loss -0.8261
2024-05-06 18:45:30.869719: val_loss -0.6345
2024-05-06 18:45:30.870985: Pseudo dice [0.7355]
2024-05-06 18:45:30.872036: Epoch time: 46.18 s
2024-05-06 18:45:32.181316: 
2024-05-06 18:45:32.184828: Epoch 18
2024-05-06 18:45:32.186921: Current learning rate: 0.00984
2024-05-06 18:46:18.862911: Validation loss did not improve from -0.70456. Patience: 3/50
2024-05-06 18:46:18.865529: train_loss -0.8245
2024-05-06 18:46:18.866906: val_loss -0.6864
2024-05-06 18:46:18.868003: Pseudo dice [0.7689]
2024-05-06 18:46:18.869006: Epoch time: 46.69 s
2024-05-06 18:46:20.167941: 
2024-05-06 18:46:20.169823: Epoch 19
2024-05-06 18:46:20.170879: Current learning rate: 0.00983
2024-05-06 18:47:06.348893: Validation loss did not improve from -0.70456. Patience: 4/50
2024-05-06 18:47:06.351914: train_loss -0.8271
2024-05-06 18:47:06.353595: val_loss -0.6995
2024-05-06 18:47:06.354920: Pseudo dice [0.7786]
2024-05-06 18:47:06.356297: Epoch time: 46.19 s
2024-05-06 18:47:06.897498: Yayy! New best EMA pseudo Dice: 0.7566
2024-05-06 18:47:09.074888: 
2024-05-06 18:47:09.077094: Epoch 20
2024-05-06 18:47:09.079039: Current learning rate: 0.00982
2024-05-06 18:47:55.305058: Validation loss did not improve from -0.70456. Patience: 5/50
2024-05-06 18:47:55.307063: train_loss -0.8297
2024-05-06 18:47:55.308252: val_loss -0.686
2024-05-06 18:47:55.309521: Pseudo dice [0.7647]
2024-05-06 18:47:55.310912: Epoch time: 46.23 s
2024-05-06 18:47:55.312291: Yayy! New best EMA pseudo Dice: 0.7574
2024-05-06 18:47:57.159964: 
2024-05-06 18:47:57.161835: Epoch 21
2024-05-06 18:47:57.163149: Current learning rate: 0.00981
2024-05-06 18:48:43.990773: Validation loss did not improve from -0.70456. Patience: 6/50
2024-05-06 18:48:43.992933: train_loss -0.8317
2024-05-06 18:48:43.994048: val_loss -0.6853
2024-05-06 18:48:43.995186: Pseudo dice [0.7696]
2024-05-06 18:48:43.996397: Epoch time: 46.83 s
2024-05-06 18:48:43.997362: Yayy! New best EMA pseudo Dice: 0.7586
2024-05-06 18:48:45.803592: 
2024-05-06 18:48:45.805737: Epoch 22
2024-05-06 18:48:45.807131: Current learning rate: 0.0098
2024-05-06 18:49:32.062969: Validation loss did not improve from -0.70456. Patience: 7/50
2024-05-06 18:49:32.064999: train_loss -0.8339
2024-05-06 18:49:32.066417: val_loss -0.7022
2024-05-06 18:49:32.067614: Pseudo dice [0.7791]
2024-05-06 18:49:32.068903: Epoch time: 46.26 s
2024-05-06 18:49:32.070170: Yayy! New best EMA pseudo Dice: 0.7607
2024-05-06 18:49:33.869871: 
2024-05-06 18:49:33.872666: Epoch 23
2024-05-06 18:49:33.874369: Current learning rate: 0.00979
2024-05-06 18:50:20.164156: Validation loss did not improve from -0.70456. Patience: 8/50
2024-05-06 18:50:20.167702: train_loss -0.824
2024-05-06 18:50:20.169199: val_loss -0.6902
2024-05-06 18:50:20.170922: Pseudo dice [0.7703]
2024-05-06 18:50:20.172140: Epoch time: 46.3 s
2024-05-06 18:50:20.173224: Yayy! New best EMA pseudo Dice: 0.7616
2024-05-06 18:50:21.967926: 
2024-05-06 18:50:21.969851: Epoch 24
2024-05-06 18:50:21.971927: Current learning rate: 0.00978
2024-05-06 18:51:08.296032: Validation loss did not improve from -0.70456. Patience: 9/50
2024-05-06 18:51:08.298724: train_loss -0.8362
2024-05-06 18:51:08.300103: val_loss -0.6989
2024-05-06 18:51:08.301498: Pseudo dice [0.7782]
2024-05-06 18:51:08.302764: Epoch time: 46.33 s
2024-05-06 18:51:08.800813: Yayy! New best EMA pseudo Dice: 0.7633
2024-05-06 18:51:10.563046: 
2024-05-06 18:51:10.564846: Epoch 25
2024-05-06 18:51:10.566070: Current learning rate: 0.00977
2024-05-06 18:51:56.830572: Validation loss did not improve from -0.70456. Patience: 10/50
2024-05-06 18:51:56.832529: train_loss -0.8433
2024-05-06 18:51:56.834367: val_loss -0.6758
2024-05-06 18:51:56.835716: Pseudo dice [0.758]
2024-05-06 18:51:56.837047: Epoch time: 46.27 s
2024-05-06 18:51:58.105552: 
2024-05-06 18:51:58.107884: Epoch 26
2024-05-06 18:51:58.109780: Current learning rate: 0.00977
2024-05-06 18:52:44.403773: Validation loss did not improve from -0.70456. Patience: 11/50
2024-05-06 18:52:44.405776: train_loss -0.8454
2024-05-06 18:52:44.407372: val_loss -0.6771
2024-05-06 18:52:44.408854: Pseudo dice [0.7607]
2024-05-06 18:52:44.409884: Epoch time: 46.3 s
2024-05-06 18:52:45.671746: 
2024-05-06 18:52:45.674237: Epoch 27
2024-05-06 18:52:45.675516: Current learning rate: 0.00976
2024-05-06 18:53:31.960643: Validation loss did not improve from -0.70456. Patience: 12/50
2024-05-06 18:53:31.962330: train_loss -0.8456
2024-05-06 18:53:31.963514: val_loss -0.6788
2024-05-06 18:53:31.964563: Pseudo dice [0.7693]
2024-05-06 18:53:31.965622: Epoch time: 46.29 s
2024-05-06 18:53:33.249253: 
2024-05-06 18:53:33.251855: Epoch 28
2024-05-06 18:53:33.253672: Current learning rate: 0.00975
2024-05-06 18:54:19.539128: Validation loss improved from -0.70456 to -0.70771! Patience: 12/50
2024-05-06 18:54:19.541038: train_loss -0.8475
2024-05-06 18:54:19.542306: val_loss -0.7077
2024-05-06 18:54:19.543371: Pseudo dice [0.7858]
2024-05-06 18:54:19.544392: Epoch time: 46.29 s
2024-05-06 18:54:19.545300: Yayy! New best EMA pseudo Dice: 0.7655
2024-05-06 18:54:21.331551: 
2024-05-06 18:54:21.333467: Epoch 29
2024-05-06 18:54:21.334825: Current learning rate: 0.00974
2024-05-06 18:55:07.626765: Validation loss did not improve from -0.70771. Patience: 1/50
2024-05-06 18:55:07.628295: train_loss -0.8477
2024-05-06 18:55:07.629285: val_loss -0.6959
2024-05-06 18:55:07.630187: Pseudo dice [0.7769]
2024-05-06 18:55:07.631294: Epoch time: 46.3 s
2024-05-06 18:55:08.142854: Yayy! New best EMA pseudo Dice: 0.7666
2024-05-06 18:55:09.911838: 
2024-05-06 18:55:09.914556: Epoch 30
2024-05-06 18:55:09.915560: Current learning rate: 0.00973
2024-05-06 18:55:56.201175: Validation loss improved from -0.70771 to -0.71484! Patience: 1/50
2024-05-06 18:55:56.203365: train_loss -0.8466
2024-05-06 18:55:56.204587: val_loss -0.7148
2024-05-06 18:55:56.205499: Pseudo dice [0.7917]
2024-05-06 18:55:56.206554: Epoch time: 46.29 s
2024-05-06 18:55:56.208703: Yayy! New best EMA pseudo Dice: 0.7691
2024-05-06 18:55:58.027682: 
2024-05-06 18:55:58.030309: Epoch 31
2024-05-06 18:55:58.031983: Current learning rate: 0.00972
2024-05-06 18:56:44.315118: Validation loss did not improve from -0.71484. Patience: 1/50
2024-05-06 18:56:44.316747: train_loss -0.848
2024-05-06 18:56:44.317823: val_loss -0.7128
2024-05-06 18:56:44.318899: Pseudo dice [0.7883]
2024-05-06 18:56:44.319967: Epoch time: 46.29 s
2024-05-06 18:56:44.320933: Yayy! New best EMA pseudo Dice: 0.771
2024-05-06 18:56:46.515295: 
2024-05-06 18:56:46.517663: Epoch 32
2024-05-06 18:56:46.519114: Current learning rate: 0.00971
2024-05-06 18:57:32.818078: Validation loss did not improve from -0.71484. Patience: 2/50
2024-05-06 18:57:32.819689: train_loss -0.851
2024-05-06 18:57:32.820757: val_loss -0.7056
2024-05-06 18:57:32.822030: Pseudo dice [0.7769]
2024-05-06 18:57:32.823232: Epoch time: 46.31 s
2024-05-06 18:57:32.824377: Yayy! New best EMA pseudo Dice: 0.7716
2024-05-06 18:57:34.623495: 
2024-05-06 18:57:34.626031: Epoch 33
2024-05-06 18:57:34.627968: Current learning rate: 0.0097
2024-05-06 18:58:20.950477: Validation loss did not improve from -0.71484. Patience: 3/50
2024-05-06 18:58:20.952159: train_loss -0.8557
2024-05-06 18:58:20.953116: val_loss -0.67
2024-05-06 18:58:20.954043: Pseudo dice [0.763]
2024-05-06 18:58:20.955158: Epoch time: 46.33 s
2024-05-06 18:58:22.282283: 
2024-05-06 18:58:22.283814: Epoch 34
2024-05-06 18:58:22.285459: Current learning rate: 0.00969
2024-05-06 18:59:08.624180: Validation loss did not improve from -0.71484. Patience: 4/50
2024-05-06 18:59:08.625737: train_loss -0.8552
2024-05-06 18:59:08.626882: val_loss -0.6651
2024-05-06 18:59:08.627942: Pseudo dice [0.7561]
2024-05-06 18:59:08.629142: Epoch time: 46.35 s
2024-05-06 18:59:10.456804: 
2024-05-06 18:59:10.458856: Epoch 35
2024-05-06 18:59:10.459994: Current learning rate: 0.00968
2024-05-06 18:59:56.741419: Validation loss did not improve from -0.71484. Patience: 5/50
2024-05-06 18:59:56.743340: train_loss -0.8498
2024-05-06 18:59:56.744518: val_loss -0.7099
2024-05-06 18:59:56.745666: Pseudo dice [0.785]
2024-05-06 18:59:56.746828: Epoch time: 46.29 s
2024-05-06 18:59:58.068603: 
2024-05-06 18:59:58.070894: Epoch 36
2024-05-06 18:59:58.072854: Current learning rate: 0.00968
2024-05-06 19:00:44.390142: Validation loss did not improve from -0.71484. Patience: 6/50
2024-05-06 19:00:44.392547: train_loss -0.854
2024-05-06 19:00:44.393780: val_loss -0.6824
2024-05-06 19:00:44.394941: Pseudo dice [0.7701]
2024-05-06 19:00:44.396063: Epoch time: 46.33 s
2024-05-06 19:00:45.723033: 
2024-05-06 19:00:45.726091: Epoch 37
2024-05-06 19:00:45.727569: Current learning rate: 0.00967
2024-05-06 19:01:32.113627: Validation loss did not improve from -0.71484. Patience: 7/50
2024-05-06 19:01:32.115299: train_loss -0.8547
2024-05-06 19:01:32.116495: val_loss -0.698
2024-05-06 19:01:32.117666: Pseudo dice [0.7776]
2024-05-06 19:01:32.118803: Epoch time: 46.39 s
2024-05-06 19:01:33.471165: 
2024-05-06 19:01:33.472614: Epoch 38
2024-05-06 19:01:33.473797: Current learning rate: 0.00966
2024-05-06 19:02:19.773984: Validation loss did not improve from -0.71484. Patience: 8/50
2024-05-06 19:02:19.776313: train_loss -0.8543
2024-05-06 19:02:19.777497: val_loss -0.6748
2024-05-06 19:02:19.778609: Pseudo dice [0.7651]
2024-05-06 19:02:19.779726: Epoch time: 46.31 s
2024-05-06 19:02:21.097013: 
2024-05-06 19:02:21.099018: Epoch 39
2024-05-06 19:02:21.100197: Current learning rate: 0.00965
2024-05-06 19:03:07.403260: Validation loss did not improve from -0.71484. Patience: 9/50
2024-05-06 19:03:07.405044: train_loss -0.8546
2024-05-06 19:03:07.406419: val_loss -0.7024
2024-05-06 19:03:07.407820: Pseudo dice [0.7824]
2024-05-06 19:03:07.408998: Epoch time: 46.31 s
2024-05-06 19:03:07.925214: Yayy! New best EMA pseudo Dice: 0.772
2024-05-06 19:03:09.733469: 
2024-05-06 19:03:09.735464: Epoch 40
2024-05-06 19:03:09.736814: Current learning rate: 0.00964
2024-05-06 19:03:56.100467: Validation loss did not improve from -0.71484. Patience: 10/50
2024-05-06 19:03:56.102041: train_loss -0.8603
2024-05-06 19:03:56.103087: val_loss -0.6695
2024-05-06 19:03:56.104108: Pseudo dice [0.7614]
2024-05-06 19:03:56.105154: Epoch time: 46.37 s
2024-05-06 19:03:57.457261: 
2024-05-06 19:03:57.458985: Epoch 41
2024-05-06 19:03:57.460013: Current learning rate: 0.00963
2024-05-06 19:04:43.807627: Validation loss did not improve from -0.71484. Patience: 11/50
2024-05-06 19:04:43.810222: train_loss -0.8552
2024-05-06 19:04:43.811503: val_loss -0.6873
2024-05-06 19:04:43.812898: Pseudo dice [0.7721]
2024-05-06 19:04:43.814277: Epoch time: 46.35 s
2024-05-06 19:04:45.059007: 
2024-05-06 19:04:45.061035: Epoch 42
2024-05-06 19:04:45.062641: Current learning rate: 0.00962
2024-05-06 19:05:31.397451: Validation loss did not improve from -0.71484. Patience: 12/50
2024-05-06 19:05:31.399376: train_loss -0.8588
2024-05-06 19:05:31.400483: val_loss -0.7007
2024-05-06 19:05:31.401618: Pseudo dice [0.779]
2024-05-06 19:05:31.402653: Epoch time: 46.34 s
2024-05-06 19:05:33.048703: 
2024-05-06 19:05:33.051248: Epoch 43
2024-05-06 19:05:33.052616: Current learning rate: 0.00961
2024-05-06 19:06:19.379928: Validation loss did not improve from -0.71484. Patience: 13/50
2024-05-06 19:06:19.381559: train_loss -0.8575
2024-05-06 19:06:19.382781: val_loss -0.7136
2024-05-06 19:06:19.384062: Pseudo dice [0.7899]
2024-05-06 19:06:19.385194: Epoch time: 46.33 s
2024-05-06 19:06:19.386225: Yayy! New best EMA pseudo Dice: 0.7737
2024-05-06 19:06:21.560432: 
2024-05-06 19:06:21.562447: Epoch 44
2024-05-06 19:06:21.564175: Current learning rate: 0.0096
2024-05-06 19:07:07.869430: Validation loss did not improve from -0.71484. Patience: 14/50
2024-05-06 19:07:07.870814: train_loss -0.8684
2024-05-06 19:07:07.871861: val_loss -0.7072
2024-05-06 19:07:07.872873: Pseudo dice [0.785]
2024-05-06 19:07:07.873943: Epoch time: 46.31 s
2024-05-06 19:07:08.391047: Yayy! New best EMA pseudo Dice: 0.7748
2024-05-06 19:07:10.160733: 
2024-05-06 19:07:10.162399: Epoch 45
2024-05-06 19:07:10.163514: Current learning rate: 0.00959
2024-05-06 19:07:56.479097: Validation loss did not improve from -0.71484. Patience: 15/50
2024-05-06 19:07:56.480757: train_loss -0.8615
2024-05-06 19:07:56.482014: val_loss -0.6839
2024-05-06 19:07:56.483097: Pseudo dice [0.7702]
2024-05-06 19:07:56.484106: Epoch time: 46.32 s
2024-05-06 19:07:57.710696: 
2024-05-06 19:07:57.712511: Epoch 46
2024-05-06 19:07:57.713798: Current learning rate: 0.00959
2024-05-06 19:08:44.041073: Validation loss did not improve from -0.71484. Patience: 16/50
2024-05-06 19:08:44.042848: train_loss -0.8638
2024-05-06 19:08:44.043854: val_loss -0.7072
2024-05-06 19:08:44.044850: Pseudo dice [0.7858]
2024-05-06 19:08:44.045797: Epoch time: 46.33 s
2024-05-06 19:08:44.046836: Yayy! New best EMA pseudo Dice: 0.7755
2024-05-06 19:08:45.827739: 
2024-05-06 19:08:45.829866: Epoch 47
2024-05-06 19:08:45.831550: Current learning rate: 0.00958
2024-05-06 19:09:32.207303: Validation loss did not improve from -0.71484. Patience: 17/50
2024-05-06 19:09:32.209266: train_loss -0.8629
2024-05-06 19:09:32.210519: val_loss -0.6819
2024-05-06 19:09:32.211697: Pseudo dice [0.7687]
2024-05-06 19:09:32.212880: Epoch time: 46.38 s
2024-05-06 19:09:33.449191: 
2024-05-06 19:09:33.451116: Epoch 48
2024-05-06 19:09:33.452284: Current learning rate: 0.00957
2024-05-06 19:10:19.768785: Validation loss improved from -0.71484 to -0.71718! Patience: 17/50
2024-05-06 19:10:19.770430: train_loss -0.8699
2024-05-06 19:10:19.771643: val_loss -0.7172
2024-05-06 19:10:19.772708: Pseudo dice [0.7952]
2024-05-06 19:10:19.773638: Epoch time: 46.32 s
2024-05-06 19:10:19.774527: Yayy! New best EMA pseudo Dice: 0.7768
2024-05-06 19:10:21.567144: 
2024-05-06 19:10:21.568547: Epoch 49
2024-05-06 19:10:21.569740: Current learning rate: 0.00956
2024-05-06 19:11:07.893652: Validation loss did not improve from -0.71718. Patience: 1/50
2024-05-06 19:11:07.895790: train_loss -0.8629
2024-05-06 19:11:07.897166: val_loss -0.7106
2024-05-06 19:11:07.898193: Pseudo dice [0.7914]
2024-05-06 19:11:07.899383: Epoch time: 46.33 s
2024-05-06 19:11:08.453485: Yayy! New best EMA pseudo Dice: 0.7783
2024-05-06 19:11:10.260204: 
2024-05-06 19:11:10.262537: Epoch 50
2024-05-06 19:11:10.264010: Current learning rate: 0.00955
2024-05-06 19:11:56.597630: Validation loss did not improve from -0.71718. Patience: 2/50
2024-05-06 19:11:56.599334: train_loss -0.8696
2024-05-06 19:11:56.600518: val_loss -0.6949
2024-05-06 19:11:56.601466: Pseudo dice [0.7779]
2024-05-06 19:11:56.602512: Epoch time: 46.34 s
2024-05-06 19:11:57.840402: 
2024-05-06 19:11:57.842543: Epoch 51
2024-05-06 19:11:57.844237: Current learning rate: 0.00954
2024-05-06 19:12:44.197543: Validation loss did not improve from -0.71718. Patience: 3/50
2024-05-06 19:12:44.199795: train_loss -0.8663
2024-05-06 19:12:44.201339: val_loss -0.6955
2024-05-06 19:12:44.202673: Pseudo dice [0.7766]
2024-05-06 19:12:44.203979: Epoch time: 46.36 s
2024-05-06 19:12:45.469532: 
2024-05-06 19:12:45.472191: Epoch 52
2024-05-06 19:12:45.473686: Current learning rate: 0.00953
2024-05-06 19:13:31.787001: Validation loss did not improve from -0.71718. Patience: 4/50
2024-05-06 19:13:31.789227: train_loss -0.8687
2024-05-06 19:13:31.790467: val_loss -0.671
2024-05-06 19:13:31.791790: Pseudo dice [0.7622]
2024-05-06 19:13:31.793142: Epoch time: 46.32 s
2024-05-06 19:13:33.051719: 
2024-05-06 19:13:33.053907: Epoch 53
2024-05-06 19:13:33.055537: Current learning rate: 0.00952
2024-05-06 19:14:19.386006: Validation loss did not improve from -0.71718. Patience: 5/50
2024-05-06 19:14:19.387696: train_loss -0.8675
2024-05-06 19:14:19.388723: val_loss -0.7165
2024-05-06 19:14:19.389788: Pseudo dice [0.7902]
2024-05-06 19:14:19.390889: Epoch time: 46.34 s
2024-05-06 19:14:20.657721: 
2024-05-06 19:14:20.660033: Epoch 54
2024-05-06 19:14:20.661740: Current learning rate: 0.00951
2024-05-06 19:15:06.991731: Validation loss did not improve from -0.71718. Patience: 6/50
2024-05-06 19:15:06.993535: train_loss -0.8702
2024-05-06 19:15:06.994925: val_loss -0.7064
2024-05-06 19:15:06.995930: Pseudo dice [0.7876]
2024-05-06 19:15:06.996878: Epoch time: 46.34 s
2024-05-06 19:15:07.524435: Yayy! New best EMA pseudo Dice: 0.7788
2024-05-06 19:15:10.017669: 
2024-05-06 19:15:10.019586: Epoch 55
2024-05-06 19:15:10.020605: Current learning rate: 0.0095
2024-05-06 19:15:56.324232: Validation loss did not improve from -0.71718. Patience: 7/50
2024-05-06 19:15:56.325673: train_loss -0.8741
2024-05-06 19:15:56.326905: val_loss -0.7093
2024-05-06 19:15:56.327915: Pseudo dice [0.7897]
2024-05-06 19:15:56.328889: Epoch time: 46.31 s
2024-05-06 19:15:56.329789: Yayy! New best EMA pseudo Dice: 0.7799
2024-05-06 19:15:58.105639: 
2024-05-06 19:15:58.107422: Epoch 56
2024-05-06 19:15:58.108444: Current learning rate: 0.00949
2024-05-06 19:16:44.405386: Validation loss did not improve from -0.71718. Patience: 8/50
2024-05-06 19:16:44.406847: train_loss -0.8689
2024-05-06 19:16:44.408432: val_loss -0.6696
2024-05-06 19:16:44.409342: Pseudo dice [0.7626]
2024-05-06 19:16:44.410346: Epoch time: 46.3 s
2024-05-06 19:16:45.678372: 
2024-05-06 19:16:45.680159: Epoch 57
2024-05-06 19:16:45.681669: Current learning rate: 0.00949
2024-05-06 19:17:32.044038: Validation loss did not improve from -0.71718. Patience: 9/50
2024-05-06 19:17:32.046158: train_loss -0.8696
2024-05-06 19:17:32.047391: val_loss -0.7064
2024-05-06 19:17:32.048506: Pseudo dice [0.7925]
2024-05-06 19:17:32.049628: Epoch time: 46.37 s
2024-05-06 19:17:33.345881: 
2024-05-06 19:17:33.348285: Epoch 58
2024-05-06 19:17:33.349429: Current learning rate: 0.00948
2024-05-06 19:18:19.730011: Validation loss did not improve from -0.71718. Patience: 10/50
2024-05-06 19:18:19.731970: train_loss -0.8716
2024-05-06 19:18:19.733067: val_loss -0.7022
2024-05-06 19:18:19.734012: Pseudo dice [0.7874]
2024-05-06 19:18:19.735234: Epoch time: 46.39 s
2024-05-06 19:18:19.736326: Yayy! New best EMA pseudo Dice: 0.7804
2024-05-06 19:18:21.575919: 
2024-05-06 19:18:21.578171: Epoch 59
2024-05-06 19:18:21.579571: Current learning rate: 0.00947
2024-05-06 19:19:07.970170: Validation loss did not improve from -0.71718. Patience: 11/50
2024-05-06 19:19:07.972104: train_loss -0.8702
2024-05-06 19:19:07.973409: val_loss -0.6756
2024-05-06 19:19:07.974638: Pseudo dice [0.7708]
2024-05-06 19:19:07.975951: Epoch time: 46.4 s
2024-05-06 19:19:09.756755: 
2024-05-06 19:19:09.759332: Epoch 60
2024-05-06 19:19:09.760553: Current learning rate: 0.00946
2024-05-06 19:19:56.095479: Validation loss did not improve from -0.71718. Patience: 12/50
2024-05-06 19:19:56.097779: train_loss -0.8721
2024-05-06 19:19:56.099953: val_loss -0.665
2024-05-06 19:19:56.101603: Pseudo dice [0.761]
2024-05-06 19:19:56.102891: Epoch time: 46.34 s
2024-05-06 19:19:57.403701: 
2024-05-06 19:19:57.405614: Epoch 61
2024-05-06 19:19:57.407214: Current learning rate: 0.00945
2024-05-06 19:20:44.433947: Validation loss did not improve from -0.71718. Patience: 13/50
2024-05-06 19:20:44.435627: train_loss -0.8761
2024-05-06 19:20:44.436856: val_loss -0.6804
2024-05-06 19:20:44.437963: Pseudo dice [0.7715]
2024-05-06 19:20:44.439090: Epoch time: 47.03 s
2024-05-06 19:20:45.744017: 
2024-05-06 19:20:45.745677: Epoch 62
2024-05-06 19:20:45.746835: Current learning rate: 0.00944
2024-05-06 19:21:32.051948: Validation loss did not improve from -0.71718. Patience: 14/50
2024-05-06 19:21:32.054091: train_loss -0.872
2024-05-06 19:21:32.055509: val_loss -0.6574
2024-05-06 19:21:32.056764: Pseudo dice [0.7603]
2024-05-06 19:21:32.058243: Epoch time: 46.31 s
2024-05-06 19:21:33.340793: 
2024-05-06 19:21:33.343325: Epoch 63
2024-05-06 19:21:33.344755: Current learning rate: 0.00943
2024-05-06 19:22:19.655162: Validation loss did not improve from -0.71718. Patience: 15/50
2024-05-06 19:22:19.656847: train_loss -0.8766
2024-05-06 19:22:19.657996: val_loss -0.6855
2024-05-06 19:22:19.658927: Pseudo dice [0.7771]
2024-05-06 19:22:19.660077: Epoch time: 46.32 s
2024-05-06 19:22:20.962081: 
2024-05-06 19:22:20.964031: Epoch 64
2024-05-06 19:22:20.965107: Current learning rate: 0.00942
2024-05-06 19:23:07.287040: Validation loss did not improve from -0.71718. Patience: 16/50
2024-05-06 19:23:07.288962: train_loss -0.8746
2024-05-06 19:23:07.290448: val_loss -0.7061
2024-05-06 19:23:07.291538: Pseudo dice [0.786]
2024-05-06 19:23:07.292492: Epoch time: 46.33 s
2024-05-06 19:23:09.114537: 
2024-05-06 19:23:09.116277: Epoch 65
2024-05-06 19:23:09.117473: Current learning rate: 0.00941
2024-05-06 19:23:55.409528: Validation loss did not improve from -0.71718. Patience: 17/50
2024-05-06 19:23:55.412394: train_loss -0.8745
2024-05-06 19:23:55.413662: val_loss -0.6962
2024-05-06 19:23:55.414704: Pseudo dice [0.7789]
2024-05-06 19:23:55.415711: Epoch time: 46.3 s
2024-05-06 19:23:56.695629: 
2024-05-06 19:23:56.699366: Epoch 66
2024-05-06 19:23:56.701278: Current learning rate: 0.0094
2024-05-06 19:24:42.981223: Validation loss did not improve from -0.71718. Patience: 18/50
2024-05-06 19:24:42.984111: train_loss -0.8749
2024-05-06 19:24:42.985443: val_loss -0.6721
2024-05-06 19:24:42.986766: Pseudo dice [0.7664]
2024-05-06 19:24:42.987878: Epoch time: 46.29 s
2024-05-06 19:24:44.673850: 
2024-05-06 19:24:44.676900: Epoch 67
2024-05-06 19:24:44.677971: Current learning rate: 0.00939
2024-05-06 19:25:30.952156: Validation loss did not improve from -0.71718. Patience: 19/50
2024-05-06 19:25:30.954471: train_loss -0.8769
2024-05-06 19:25:30.955483: val_loss -0.6566
2024-05-06 19:25:30.956581: Pseudo dice [0.7559]
2024-05-06 19:25:30.957664: Epoch time: 46.28 s
2024-05-06 19:25:32.262416: 
2024-05-06 19:25:32.268237: Epoch 68
2024-05-06 19:25:32.269789: Current learning rate: 0.00939
2024-05-06 19:26:18.574152: Validation loss did not improve from -0.71718. Patience: 20/50
2024-05-06 19:26:18.576696: train_loss -0.8719
2024-05-06 19:26:18.577668: val_loss -0.6865
2024-05-06 19:26:18.578588: Pseudo dice [0.7775]
2024-05-06 19:26:18.579517: Epoch time: 46.32 s
2024-05-06 19:26:19.903102: 
2024-05-06 19:26:19.906590: Epoch 69
2024-05-06 19:26:19.908067: Current learning rate: 0.00938
2024-05-06 19:27:06.209967: Validation loss did not improve from -0.71718. Patience: 21/50
2024-05-06 19:27:06.212815: train_loss -0.8738
2024-05-06 19:27:06.213982: val_loss -0.686
2024-05-06 19:27:06.215039: Pseudo dice [0.7695]
2024-05-06 19:27:06.216091: Epoch time: 46.31 s
2024-05-06 19:27:08.036082: 
2024-05-06 19:27:08.039548: Epoch 70
2024-05-06 19:27:08.041153: Current learning rate: 0.00937
2024-05-06 19:27:54.345076: Validation loss did not improve from -0.71718. Patience: 22/50
2024-05-06 19:27:54.347338: train_loss -0.8778
2024-05-06 19:27:54.348408: val_loss -0.6962
2024-05-06 19:27:54.349340: Pseudo dice [0.7799]
2024-05-06 19:27:54.350214: Epoch time: 46.31 s
2024-05-06 19:27:55.654208: 
2024-05-06 19:27:55.657550: Epoch 71
2024-05-06 19:27:55.658681: Current learning rate: 0.00936
2024-05-06 19:28:41.996159: Validation loss did not improve from -0.71718. Patience: 23/50
2024-05-06 19:28:41.999373: train_loss -0.8791
2024-05-06 19:28:42.000643: val_loss -0.6597
2024-05-06 19:28:42.001612: Pseudo dice [0.7544]
2024-05-06 19:28:42.002837: Epoch time: 46.35 s
2024-05-06 19:28:43.329092: 
2024-05-06 19:28:43.332257: Epoch 72
2024-05-06 19:28:43.333541: Current learning rate: 0.00935
2024-05-06 19:29:29.665083: Validation loss did not improve from -0.71718. Patience: 24/50
2024-05-06 19:29:29.668079: train_loss -0.8826
2024-05-06 19:29:29.669079: val_loss -0.6745
2024-05-06 19:29:29.670127: Pseudo dice [0.7724]
2024-05-06 19:29:29.671089: Epoch time: 46.34 s
2024-05-06 19:29:31.025306: 
2024-05-06 19:29:31.028435: Epoch 73
2024-05-06 19:29:31.030622: Current learning rate: 0.00934
2024-05-06 19:30:17.569689: Validation loss did not improve from -0.71718. Patience: 25/50
2024-05-06 19:30:17.596527: train_loss -0.8788
2024-05-06 19:30:17.597887: val_loss -0.6834
2024-05-06 19:30:17.598965: Pseudo dice [0.7767]
2024-05-06 19:30:17.600170: Epoch time: 46.57 s
2024-05-06 19:30:18.899309: 
2024-05-06 19:30:18.903143: Epoch 74
2024-05-06 19:30:18.905346: Current learning rate: 0.00933
2024-05-06 19:31:05.232430: Validation loss did not improve from -0.71718. Patience: 26/50
2024-05-06 19:31:05.235706: train_loss -0.8818
2024-05-06 19:31:05.237179: val_loss -0.6989
2024-05-06 19:31:05.238423: Pseudo dice [0.7806]
2024-05-06 19:31:05.239633: Epoch time: 46.34 s
2024-05-06 19:31:07.106232: 
2024-05-06 19:31:07.109248: Epoch 75
2024-05-06 19:31:07.110739: Current learning rate: 0.00932
2024-05-06 19:31:53.488382: Validation loss did not improve from -0.71718. Patience: 27/50
2024-05-06 19:31:53.491018: train_loss -0.8848
2024-05-06 19:31:53.492203: val_loss -0.6941
2024-05-06 19:31:53.493083: Pseudo dice [0.7762]
2024-05-06 19:31:53.494146: Epoch time: 46.39 s
2024-05-06 19:31:54.831770: 
2024-05-06 19:31:54.835027: Epoch 76
2024-05-06 19:31:54.836339: Current learning rate: 0.00931
2024-05-06 19:32:41.171504: Validation loss did not improve from -0.71718. Patience: 28/50
2024-05-06 19:32:41.174345: train_loss -0.8816
2024-05-06 19:32:41.175796: val_loss -0.6858
2024-05-06 19:32:41.176898: Pseudo dice [0.7757]
2024-05-06 19:32:41.178042: Epoch time: 46.34 s
2024-05-06 19:32:42.513118: 
2024-05-06 19:32:42.516423: Epoch 77
2024-05-06 19:32:42.518333: Current learning rate: 0.0093
2024-05-06 19:33:28.975568: Validation loss did not improve from -0.71718. Patience: 29/50
2024-05-06 19:33:29.050792: train_loss -0.886
2024-05-06 19:33:29.052312: val_loss -0.6937
2024-05-06 19:33:29.053483: Pseudo dice [0.7775]
2024-05-06 19:33:29.054990: Epoch time: 46.54 s
2024-05-06 19:33:30.398728: 
2024-05-06 19:33:30.402545: Epoch 78
2024-05-06 19:33:30.404229: Current learning rate: 0.0093
2024-05-06 19:34:16.814939: Validation loss did not improve from -0.71718. Patience: 30/50
2024-05-06 19:34:16.817231: train_loss -0.8843
2024-05-06 19:34:16.818296: val_loss -0.6712
2024-05-06 19:34:16.819375: Pseudo dice [0.7662]
2024-05-06 19:34:16.820473: Epoch time: 46.42 s
2024-05-06 19:34:18.771456: 
2024-05-06 19:34:18.775201: Epoch 79
2024-05-06 19:34:18.776697: Current learning rate: 0.00929
2024-05-06 19:35:05.283968: Validation loss did not improve from -0.71718. Patience: 31/50
2024-05-06 19:35:05.286499: train_loss -0.8806
2024-05-06 19:35:05.287680: val_loss -0.6878
2024-05-06 19:35:05.288639: Pseudo dice [0.7725]
2024-05-06 19:35:05.289573: Epoch time: 46.52 s
2024-05-06 19:35:07.204057: 
2024-05-06 19:35:07.208239: Epoch 80
2024-05-06 19:35:07.210216: Current learning rate: 0.00928
2024-05-06 19:35:53.527363: Validation loss did not improve from -0.71718. Patience: 32/50
2024-05-06 19:35:53.530705: train_loss -0.8858
2024-05-06 19:35:53.532067: val_loss -0.6916
2024-05-06 19:35:53.533458: Pseudo dice [0.7811]
2024-05-06 19:35:53.535017: Epoch time: 46.33 s
2024-05-06 19:35:54.892711: 
2024-05-06 19:35:54.896269: Epoch 81
2024-05-06 19:35:54.897904: Current learning rate: 0.00927
2024-05-06 19:36:41.804225: Validation loss did not improve from -0.71718. Patience: 33/50
2024-05-06 19:36:41.806891: train_loss -0.8827
2024-05-06 19:36:41.819331: val_loss -0.7081
2024-05-06 19:36:41.820570: Pseudo dice [0.7872]
2024-05-06 19:36:41.821811: Epoch time: 46.92 s
2024-05-06 19:36:43.152822: 
2024-05-06 19:36:43.156086: Epoch 82
2024-05-06 19:36:43.157268: Current learning rate: 0.00926
2024-05-06 19:37:29.863931: Validation loss did not improve from -0.71718. Patience: 34/50
2024-05-06 19:37:29.895385: train_loss -0.8799
2024-05-06 19:37:29.896950: val_loss -0.7034
2024-05-06 19:37:29.898153: Pseudo dice [0.7865]
2024-05-06 19:37:29.899329: Epoch time: 46.72 s
2024-05-06 19:37:31.166047: 
2024-05-06 19:37:31.169838: Epoch 83
2024-05-06 19:37:31.171489: Current learning rate: 0.00925
2024-05-06 19:38:17.475207: Validation loss did not improve from -0.71718. Patience: 35/50
2024-05-06 19:38:17.477637: train_loss -0.8848
2024-05-06 19:38:17.478787: val_loss -0.6737
2024-05-06 19:38:17.479728: Pseudo dice [0.7667]
2024-05-06 19:38:17.480668: Epoch time: 46.31 s
2024-05-06 19:38:18.767205: 
2024-05-06 19:38:18.770926: Epoch 84
2024-05-06 19:38:18.772275: Current learning rate: 0.00924
2024-05-06 19:39:05.095804: Validation loss did not improve from -0.71718. Patience: 36/50
2024-05-06 19:39:05.099332: train_loss -0.884
2024-05-06 19:39:05.100692: val_loss -0.6731
2024-05-06 19:39:05.101853: Pseudo dice [0.7681]
2024-05-06 19:39:05.102822: Epoch time: 46.33 s
2024-05-06 19:39:07.162134: 
2024-05-06 19:39:07.165724: Epoch 85
2024-05-06 19:39:07.167588: Current learning rate: 0.00923
2024-05-06 19:39:53.470607: Validation loss did not improve from -0.71718. Patience: 37/50
2024-05-06 19:39:53.472986: train_loss -0.8832
2024-05-06 19:39:53.474102: val_loss -0.6923
2024-05-06 19:39:53.534886: Pseudo dice [0.7791]
2024-05-06 19:39:53.536361: Epoch time: 46.31 s
2024-05-06 19:39:54.798739: 
2024-05-06 19:39:54.802581: Epoch 86
2024-05-06 19:39:54.804657: Current learning rate: 0.00922
2024-05-06 19:40:41.146741: Validation loss did not improve from -0.71718. Patience: 38/50
2024-05-06 19:40:41.149367: train_loss -0.8768
2024-05-06 19:40:41.150429: val_loss -0.6573
2024-05-06 19:40:41.151447: Pseudo dice [0.766]
2024-05-06 19:40:41.152523: Epoch time: 46.35 s
2024-05-06 19:40:42.422257: 
2024-05-06 19:40:42.427281: Epoch 87
2024-05-06 19:40:42.428864: Current learning rate: 0.00921
2024-05-06 19:41:28.762396: Validation loss did not improve from -0.71718. Patience: 39/50
2024-05-06 19:41:28.765074: train_loss -0.8743
2024-05-06 19:41:28.766115: val_loss -0.6823
2024-05-06 19:41:28.767218: Pseudo dice [0.7711]
2024-05-06 19:41:28.768262: Epoch time: 46.34 s
2024-05-06 19:41:30.038722: 
2024-05-06 19:41:30.042306: Epoch 88
2024-05-06 19:41:30.043710: Current learning rate: 0.0092
2024-05-06 19:42:16.371692: Validation loss did not improve from -0.71718. Patience: 40/50
2024-05-06 19:42:16.375021: train_loss -0.876
2024-05-06 19:42:16.376118: val_loss -0.6713
2024-05-06 19:42:16.377130: Pseudo dice [0.7669]
2024-05-06 19:42:16.378028: Epoch time: 46.34 s
2024-05-06 19:42:17.661472: 
2024-05-06 19:42:17.665508: Epoch 89
2024-05-06 19:42:17.666930: Current learning rate: 0.0092
2024-05-06 19:43:03.993816: Validation loss did not improve from -0.71718. Patience: 41/50
2024-05-06 19:43:03.996413: train_loss -0.8799
2024-05-06 19:43:03.997470: val_loss -0.6944
2024-05-06 19:43:03.998321: Pseudo dice [0.7847]
2024-05-06 19:43:03.999250: Epoch time: 46.34 s
2024-05-06 19:43:05.790243: 
2024-05-06 19:43:05.793943: Epoch 90
2024-05-06 19:43:05.795011: Current learning rate: 0.00919
2024-05-06 19:43:52.082417: Validation loss did not improve from -0.71718. Patience: 42/50
2024-05-06 19:43:52.084951: train_loss -0.884
2024-05-06 19:43:52.086051: val_loss -0.6754
2024-05-06 19:43:52.087105: Pseudo dice [0.7716]
2024-05-06 19:43:52.088355: Epoch time: 46.3 s
2024-05-06 19:43:54.196219: 
2024-05-06 19:43:54.200169: Epoch 91
2024-05-06 19:43:54.202120: Current learning rate: 0.00918
2024-05-06 19:44:40.519166: Validation loss did not improve from -0.71718. Patience: 43/50
2024-05-06 19:44:40.521360: train_loss -0.886
2024-05-06 19:44:40.522337: val_loss -0.657
2024-05-06 19:44:40.523277: Pseudo dice [0.7604]
2024-05-06 19:44:40.524220: Epoch time: 46.33 s
2024-05-06 19:44:41.742463: 
2024-05-06 19:44:41.745889: Epoch 92
2024-05-06 19:44:41.747264: Current learning rate: 0.00917
2024-05-06 19:45:28.057874: Validation loss did not improve from -0.71718. Patience: 44/50
2024-05-06 19:45:28.060344: train_loss -0.8781
2024-05-06 19:45:28.061409: val_loss -0.6593
2024-05-06 19:45:28.062441: Pseudo dice [0.754]
2024-05-06 19:45:28.063442: Epoch time: 46.32 s
2024-05-06 19:45:29.321463: 
2024-05-06 19:45:29.324754: Epoch 93
2024-05-06 19:45:29.325715: Current learning rate: 0.00916
2024-05-06 19:46:15.681679: Validation loss did not improve from -0.71718. Patience: 45/50
2024-05-06 19:46:15.684083: train_loss -0.8824
2024-05-06 19:46:15.685098: val_loss -0.6965
2024-05-06 19:46:15.686116: Pseudo dice [0.7843]
2024-05-06 19:46:15.687232: Epoch time: 46.36 s
2024-05-06 19:46:16.937572: 
2024-05-06 19:46:16.940603: Epoch 94
2024-05-06 19:46:16.942235: Current learning rate: 0.00915
2024-05-06 19:47:03.307564: Validation loss did not improve from -0.71718. Patience: 46/50
2024-05-06 19:47:03.310447: train_loss -0.8859
2024-05-06 19:47:03.311979: val_loss -0.6745
2024-05-06 19:47:03.313145: Pseudo dice [0.7694]
2024-05-06 19:47:03.314372: Epoch time: 46.37 s
2024-05-06 19:47:05.103436: 
2024-05-06 19:47:05.106986: Epoch 95
2024-05-06 19:47:05.109031: Current learning rate: 0.00914
2024-05-06 19:47:51.462875: Validation loss did not improve from -0.71718. Patience: 47/50
2024-05-06 19:47:51.465409: train_loss -0.8867
2024-05-06 19:47:51.466504: val_loss -0.6813
2024-05-06 19:47:51.467413: Pseudo dice [0.7726]
2024-05-06 19:47:51.468310: Epoch time: 46.36 s
2024-05-06 19:47:52.711711: 
2024-05-06 19:47:52.714943: Epoch 96
2024-05-06 19:47:52.716210: Current learning rate: 0.00913
2024-05-06 19:48:39.067489: Validation loss did not improve from -0.71718. Patience: 48/50
2024-05-06 19:48:39.070251: train_loss -0.8903
2024-05-06 19:48:39.071541: val_loss -0.6977
2024-05-06 19:48:39.072703: Pseudo dice [0.7871]
2024-05-06 19:48:39.073934: Epoch time: 46.36 s
2024-05-06 19:48:40.340513: 
2024-05-06 19:48:40.344235: Epoch 97
2024-05-06 19:48:40.345517: Current learning rate: 0.00912
2024-05-06 19:49:26.720175: Validation loss did not improve from -0.71718. Patience: 49/50
2024-05-06 19:49:26.722892: train_loss -0.8884
2024-05-06 19:49:26.723997: val_loss -0.6642
2024-05-06 19:49:26.724989: Pseudo dice [0.7611]
2024-05-06 19:49:26.726057: Epoch time: 46.38 s
2024-05-06 19:49:28.023015: 
2024-05-06 19:49:28.026429: Epoch 98
2024-05-06 19:49:28.028005: Current learning rate: 0.00911
2024-05-06 19:50:14.574006: Validation loss did not improve from -0.71718. Patience: 50/50
2024-05-06 19:50:14.576338: train_loss -0.8901
2024-05-06 19:50:14.577580: val_loss -0.6891
2024-05-06 19:50:14.578712: Pseudo dice [0.777]
2024-05-06 19:50:14.579788: Epoch time: 46.56 s
2024-05-06 19:50:15.876158: Patience reached. Stopping training.
2024-05-06 19:50:16.646451: Training done.
2024-05-06 19:50:17.077045: Using splits from existing split file: /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset302_Calcium_OCTv2/splits_final.json
2024-05-06 19:50:17.079784: The split file contains 3 splits.
2024-05-06 19:50:17.081173: Desired fold for training: 0
2024-05-06 19:50:17.082442: This split has 4 training and 2 validation cases.
2024-05-06 19:50:17.083825: predicting 101-019
2024-05-06 19:50:17.101971: 101-019, shape torch.Size([1, 375, 498, 498]), rank 0
2024-05-06 19:50:51.581640: predicting 704-003
2024-05-06 19:50:51.609163: 704-003, shape torch.Size([1, 375, 498, 498]), rank 0
2024-05-06 19:51:44.712845: Validation complete
2024-05-06 19:51:44.717827: Mean Validation Dice:  0.7626523853410878
wandb: 
wandb: Run history:
wandb:            ema_fg_dice ▁▁▂▃▄▅▅▅▆▆▆▆▇▇▇▇▇▇▇████████▇▇▇▇▇▇██▇▇▇▇▇
wandb:   epoch_end_timestamps ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb: epoch_start_timestamps ▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███
wandb:                    lrs ████▇▇▇▇▇▆▆▆▆▆▆▅▅▅▅▅▄▄▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▁▁▁
wandb:           mean_fg_dice ▁▄▅▅▅▆▇▃▆▇▅▆█▇▇▇▆▇▆▆▇▆██▅▅▇▅▇▆▇▇▇█▇▆▆▅▆▇
wandb:           train_losses █▄▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:             val_losses █▅▄▄▄▃▂▆▃▂▃▃▁▂▁▂▄▂▃▃▂▄▁▁▄▄▂▄▂▃▂▂▂▂▂▃▃▄▃▂
wandb: 
wandb: Run summary:
wandb:            ema_fg_dice 0.77276
wandb:   epoch_end_timestamps 1715039414.57617
wandb: epoch_start_timestamps 1715039368.02109
wandb:                    lrs 0.00911
wandb:           mean_fg_dice 0.77696
wandb:           train_losses -0.89013
wandb:             val_losses -0.68907
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_results/Dataset302_Calcium_OCTv2/nnUNetTrainer__nnUNetPlans__2d/fold_0/wandb/offline-run-20240506_182800-d2yuj5gh
wandb: Find logs at: /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_results/Dataset302_Calcium_OCTv2/nnUNetTrainer__nnUNetPlans__2d/fold_0/wandb/offline-run-20240506_182800-d2yuj5gh/logs
WARNING:urllib3.connectionpool:Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7feb7aa481f0>: Failed to establish a new connection: [Errno 101] Network is unreachable')': /api/4504800232407040/envelope/
WARNING:urllib3.connectionpool:Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7feb75b214c0>: Failed to establish a new connection: [Errno 101] Network is unreachable')': /api/4504800232407040/envelope/
WARNING:urllib3.connectionpool:Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7fea9b280190>: Failed to establish a new connection: [Errno 101] Network is unreachable')': /api/4504800232407040/envelope/
WARNING:urllib3.connectionpool:Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7feb45ac2580>: Failed to establish a new connection: [Errno 101] Network is unreachable')': /api/4504800232407040/envelope/
WARNING:urllib3.connectionpool:Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7feb45a632b0>: Failed to establish a new connection: [Errno 101] Network is unreachable')': /api/4504800232407040/envelope/
WARNING:urllib3.connectionpool:Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7fea925ace50>: Failed to establish a new connection: [Errno 101] Network is unreachable')': /api/4504800232407040/envelope/
