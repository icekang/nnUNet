/home/gridsan/nchutisilp/.conda/envs/nnunet/bin/python
Starting training with CONFIG=3d_32x160x128_b10, DATASET_ID=307, TRAINER=nnUNetTrainer

############################
INFO: You are using the old nnU-Net default plans. We have updated our recommendations. Please consider using those instead! Read more here: https://github.com/MIC-DKFZ/nnUNet/blob/master/documentation/resenc_presets.md
############################

Using device: cuda:0

#######################################################################
Please cite the following paper when using nnU-Net:
Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.
#######################################################################

2024-12-08 16:39:31.680551: Using torch.compile...

############################
INFO: You are using the old nnU-Net default plans. We have updated our recommendations. Please consider using those instead! Read more here: https://github.com/MIC-DKFZ/nnUNet/blob/master/documentation/resenc_presets.md
############################

Using device: cuda:0

#######################################################################
Please cite the following paper when using nnU-Net:
Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.
#######################################################################

2024-12-08 16:39:31.680016: Using torch.compile...
/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
################### Loading pretrained weights from file  /home/gridsan/nchutisilp/datasets/ModelGenesisOutputs/ModelGenesisNNUNetPretrainingV2_noNorm_correct_orientation/Converted_nnUNet_Genesis_OCT_Best.pt ###################
Below is the list of overlapping blocks in pretrained model and nnUNet architecture:
encoder.stages.0.0.convs.0.conv.weight shape torch.Size([32, 1, 3, 3, 3])
encoder.stages.0.0.convs.0.conv.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.norm.weight shape torch.Size([32])
encoder.stages.0.0.convs.0.norm.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.0.weight shape torch.Size([32, 1, 3, 3, 3])
encoder.stages.0.0.convs.0.all_modules.0.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.1.weight shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.1.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
encoder.stages.0.0.convs.1.conv.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.norm.weight shape torch.Size([32])
encoder.stages.0.0.convs.1.norm.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
encoder.stages.0.0.convs.1.all_modules.0.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.1.weight shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.1.bias shape torch.Size([32])
encoder.stages.1.0.convs.0.conv.weight shape torch.Size([64, 32, 3, 3, 3])
encoder.stages.1.0.convs.0.conv.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.norm.weight shape torch.Size([64])
encoder.stages.1.0.convs.0.norm.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.0.weight shape torch.Size([64, 32, 3, 3, 3])
encoder.stages.1.0.convs.0.all_modules.0.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.1.weight shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.1.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
encoder.stages.1.0.convs.1.conv.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.norm.weight shape torch.Size([64])
encoder.stages.1.0.convs.1.norm.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
encoder.stages.1.0.convs.1.all_modules.0.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.1.weight shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.1.bias shape torch.Size([64])
encoder.stages.2.0.convs.0.conv.weight shape torch.Size([128, 64, 3, 3, 3])
encoder.stages.2.0.convs.0.conv.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.norm.weight shape torch.Size([128])
encoder.stages.2.0.convs.0.norm.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.0.weight shape torch.Size([128, 64, 3, 3, 3])
encoder.stages.2.0.convs.0.all_modules.0.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.1.weight shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.1.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
encoder.stages.2.0.convs.1.conv.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.norm.weight shape torch.Size([128])
encoder.stages.2.0.convs.1.norm.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
encoder.stages.2.0.convs.1.all_modules.0.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.1.weight shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.1.bias shape torch.Size([128])
encoder.stages.3.0.convs.0.conv.weight shape torch.Size([256, 128, 3, 3, 3])
encoder.stages.3.0.convs.0.conv.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.norm.weight shape torch.Size([256])
encoder.stages.3.0.convs.0.norm.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.0.weight shape torch.Size([256, 128, 3, 3, 3])
encoder.stages.3.0.convs.0.all_modules.0.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.1.weight shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.1.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
encoder.stages.3.0.convs.1.conv.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.norm.weight shape torch.Size([256])
encoder.stages.3.0.convs.1.norm.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
encoder.stages.3.0.convs.1.all_modules.0.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.1.weight shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.1.bias shape torch.Size([256])
encoder.stages.4.0.convs.0.conv.weight shape torch.Size([320, 256, 3, 3, 3])
encoder.stages.4.0.convs.0.conv.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.norm.weight shape torch.Size([320])
encoder.stages.4.0.convs.0.norm.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.0.weight shape torch.Size([320, 256, 3, 3, 3])
encoder.stages.4.0.convs.0.all_modules.0.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.1.weight shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.1.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.4.0.convs.1.conv.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.norm.weight shape torch.Size([320])
encoder.stages.4.0.convs.1.norm.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.4.0.convs.1.all_modules.0.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.1.weight shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.1.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.0.conv.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.norm.weight shape torch.Size([320])
encoder.stages.5.0.convs.0.norm.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.0.all_modules.0.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.1.weight shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.1.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.1.conv.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.norm.weight shape torch.Size([320])
encoder.stages.5.0.convs.1.norm.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.1.all_modules.0.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.1.weight shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.0.0.convs.0.conv.weight shape torch.Size([32, 1, 3, 3, 3])
decoder.encoder.stages.0.0.convs.0.conv.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.norm.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.norm.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.0.weight shape torch.Size([32, 1, 3, 3, 3])
decoder.encoder.stages.0.0.convs.0.all_modules.0.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.1.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.1.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.encoder.stages.0.0.convs.1.conv.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.norm.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.norm.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.encoder.stages.0.0.convs.1.all_modules.0.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.1.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.1.bias shape torch.Size([32])
decoder.encoder.stages.1.0.convs.0.conv.weight shape torch.Size([64, 32, 3, 3, 3])
decoder.encoder.stages.1.0.convs.0.conv.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.norm.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.norm.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.0.weight shape torch.Size([64, 32, 3, 3, 3])
decoder.encoder.stages.1.0.convs.0.all_modules.0.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.1.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.1.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.encoder.stages.1.0.convs.1.conv.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.norm.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.norm.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.encoder.stages.1.0.convs.1.all_modules.0.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.1.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.1.bias shape torch.Size([64])
decoder.encoder.stages.2.0.convs.0.conv.weight shape torch.Size([128, 64, 3, 3, 3])
decoder.encoder.stages.2.0.convs.0.conv.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.norm.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.norm.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.0.weight shape torch.Size([128, 64, 3, 3, 3])
decoder.encoder.stages.2.0.convs.0.all_modules.0.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.1.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.1.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.encoder.stages.2.0.convs.1.conv.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.norm.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.norm.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.encoder.stages.2.0.convs.1.all_modules.0.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.1.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.1.bias shape torch.Size([128])
decoder.encoder.stages.3.0.convs.0.conv.weight shape torch.Size([256, 128, 3, 3, 3])
decoder.encoder.stages.3.0.convs.0.conv.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.norm.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.norm.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.0.weight shape torch.Size([256, 128, 3, 3, 3])
decoder.encoder.stages.3.0.convs.0.all_modules.0.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.1.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.1.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.encoder.stages.3.0.convs.1.conv.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.norm.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.norm.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.encoder.stages.3.0.convs.1.all_modules.0.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.1.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.1.bias shape torch.Size([256])
decoder.encoder.stages.4.0.convs.0.conv.weight shape torch.Size([320, 256, 3, 3, 3])
decoder.encoder.stages.4.0.convs.0.conv.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.norm.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.norm.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.0.weight shape torch.Size([320, 256, 3, 3, 3])
decoder.encoder.stages.4.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.4.0.convs.1.conv.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.norm.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.norm.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.4.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.0.conv.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.norm.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.norm.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.1.conv.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.norm.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.norm.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.stages.0.convs.0.conv.weight shape torch.Size([320, 640, 3, 3, 3])
decoder.stages.0.convs.0.conv.bias shape torch.Size([320])
decoder.stages.0.convs.0.norm.weight shape torch.Size([320])
decoder.stages.0.convs.0.norm.bias shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.0.weight shape torch.Size([320, 640, 3, 3, 3])
decoder.stages.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.stages.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.stages.0.convs.1.conv.bias shape torch.Size([320])
decoder.stages.0.convs.1.norm.weight shape torch.Size([320])
decoder.stages.0.convs.1.norm.bias shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.stages.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.stages.1.convs.0.conv.weight shape torch.Size([256, 512, 3, 3, 3])
decoder.stages.1.convs.0.conv.bias shape torch.Size([256])
decoder.stages.1.convs.0.norm.weight shape torch.Size([256])
decoder.stages.1.convs.0.norm.bias shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.0.weight shape torch.Size([256, 512, 3, 3, 3])
decoder.stages.1.convs.0.all_modules.0.bias shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.1.weight shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.1.bias shape torch.Size([256])
decoder.stages.1.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.stages.1.convs.1.conv.bias shape torch.Size([256])
decoder.stages.1.convs.1.norm.weight shape torch.Size([256])
decoder.stages.1.convs.1.norm.bias shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.stages.1.convs.1.all_modules.0.bias shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.1.weight shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.1.bias shape torch.Size([256])
decoder.stages.2.convs.0.conv.weight shape torch.Size([128, 256, 3, 3, 3])
decoder.stages.2.convs.0.conv.bias shape torch.Size([128])
decoder.stages.2.convs.0.norm.weight shape torch.Size([128])
decoder.stages.2.convs.0.norm.bias shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.0.weight shape torch.Size([128, 256, 3, 3, 3])
decoder.stages.2.convs.0.all_modules.0.bias shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.1.weight shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.1.bias shape torch.Size([128])
decoder.stages.2.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.stages.2.convs.1.conv.bias shape torch.Size([128])
decoder.stages.2.convs.1.norm.weight shape torch.Size([128])
decoder.stages.2.convs.1.norm.bias shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.stages.2.convs.1.all_modules.0.bias shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.1.weight shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.1.bias shape torch.Size([128])
decoder.stages.3.convs.0.conv.weight shape torch.Size([64, 128, 3, 3, 3])
decoder.stages.3.convs.0.conv.bias shape torch.Size([64])
decoder.stages.3.convs.0.norm.weight shape torch.Size([64])
decoder.stages.3.convs.0.norm.bias shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.0.weight shape torch.Size([64, 128, 3, 3, 3])
decoder.stages.3.convs.0.all_modules.0.bias shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.1.weight shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.1.bias shape torch.Size([64])
decoder.stages.3.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.stages.3.convs.1.conv.bias shape torch.Size([64])
decoder.stages.3.convs.1.norm.weight shape torch.Size([64])
decoder.stages.3.convs.1.norm.bias shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.stages.3.convs.1.all_modules.0.bias shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.1.weight shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.1.bias shape torch.Size([64])
decoder.stages.4.convs.0.conv.weight shape torch.Size([32, 64, 3, 3, 3])
decoder.stages.4.convs.0.conv.bias shape torch.Size([32])
decoder.stages.4.convs.0.norm.weight shape torch.Size([32])
decoder.stages.4.convs.0.norm.bias shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.0.weight shape torch.Size([32, 64, 3, 3, 3])
decoder.stages.4.convs.0.all_modules.0.bias shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.1.weight shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.1.bias shape torch.Size([32])
decoder.stages.4.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.stages.4.convs.1.conv.bias shape torch.Size([32])
decoder.stages.4.convs.1.norm.weight shape torch.Size([32])
decoder.stages.4.convs.1.norm.bias shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.stages.4.convs.1.all_modules.0.bias shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.1.weight shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.1.bias shape torch.Size([32])
decoder.transpconvs.0.weight shape torch.Size([320, 320, 1, 2, 2])
decoder.transpconvs.0.bias shape torch.Size([320])
decoder.transpconvs.1.weight shape torch.Size([320, 256, 2, 2, 2])
decoder.transpconvs.1.bias shape torch.Size([256])
decoder.transpconvs.2.weight shape torch.Size([256, 128, 2, 2, 2])
decoder.transpconvs.2.bias shape torch.Size([128])
decoder.transpconvs.3.weight shape torch.Size([128, 64, 2, 2, 2])
decoder.transpconvs.3.bias shape torch.Size([64])
decoder.transpconvs.4.weight shape torch.Size([64, 32, 2, 2, 2])
decoder.transpconvs.4.bias shape torch.Size([32])
################### Done ###################
2024-12-08 16:39:34.841496: do_dummy_2d_data_aug: True
2024-12-08 16:39:34.843773: Using splits from existing split file: /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset307_Sohee_Calcium_OCT_CrossValidation/splits_final.json
2024-12-08 16:39:34.845570: The split file contains 5 splits.
2024-12-08 16:39:34.846564: Desired fold for training: 1
2024-12-08 16:39:34.847424: This split has 6 training and 2 validation cases.
################### Loading pretrained weights from file  /home/gridsan/nchutisilp/datasets/ModelGenesisOutputs/ModelGenesisNNUNetPretrainingV2_noNorm_correct_orientation/Converted_nnUNet_Genesis_OCT_Best.pt ###################
Below is the list of overlapping blocks in pretrained model and nnUNet architecture:
encoder.stages.0.0.convs.0.conv.weight shape torch.Size([32, 1, 3, 3, 3])
encoder.stages.0.0.convs.0.conv.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.norm.weight shape torch.Size([32])
encoder.stages.0.0.convs.0.norm.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.0.weight shape torch.Size([32, 1, 3, 3, 3])
encoder.stages.0.0.convs.0.all_modules.0.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.1.weight shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.1.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
encoder.stages.0.0.convs.1.conv.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.norm.weight shape torch.Size([32])
encoder.stages.0.0.convs.1.norm.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
encoder.stages.0.0.convs.1.all_modules.0.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.1.weight shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.1.bias shape torch.Size([32])
encoder.stages.1.0.convs.0.conv.weight shape torch.Size([64, 32, 3, 3, 3])
encoder.stages.1.0.convs.0.conv.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.norm.weight shape torch.Size([64])
encoder.stages.1.0.convs.0.norm.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.0.weight shape torch.Size([64, 32, 3, 3, 3])
encoder.stages.1.0.convs.0.all_modules.0.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.1.weight shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.1.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
encoder.stages.1.0.convs.1.conv.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.norm.weight shape torch.Size([64])
encoder.stages.1.0.convs.1.norm.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
encoder.stages.1.0.convs.1.all_modules.0.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.1.weight shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.1.bias shape torch.Size([64])
encoder.stages.2.0.convs.0.conv.weight shape torch.Size([128, 64, 3, 3, 3])
encoder.stages.2.0.convs.0.conv.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.norm.weight shape torch.Size([128])
encoder.stages.2.0.convs.0.norm.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.0.weight shape torch.Size([128, 64, 3, 3, 3])
encoder.stages.2.0.convs.0.all_modules.0.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.1.weight shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.1.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
encoder.stages.2.0.convs.1.conv.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.norm.weight shape torch.Size([128])
encoder.stages.2.0.convs.1.norm.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
encoder.stages.2.0.convs.1.all_modules.0.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.1.weight shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.1.bias shape torch.Size([128])
encoder.stages.3.0.convs.0.conv.weight shape torch.Size([256, 128, 3, 3, 3])
encoder.stages.3.0.convs.0.conv.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.norm.weight shape torch.Size([256])
encoder.stages.3.0.convs.0.norm.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.0.weight shape torch.Size([256, 128, 3, 3, 3])
encoder.stages.3.0.convs.0.all_modules.0.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.1.weight shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.1.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
encoder.stages.3.0.convs.1.conv.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.norm.weight shape torch.Size([256])
encoder.stages.3.0.convs.1.norm.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
encoder.stages.3.0.convs.1.all_modules.0.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.1.weight shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.1.bias shape torch.Size([256])
encoder.stages.4.0.convs.0.conv.weight shape torch.Size([320, 256, 3, 3, 3])
encoder.stages.4.0.convs.0.conv.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.norm.weight shape torch.Size([320])
encoder.stages.4.0.convs.0.norm.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.0.weight shape torch.Size([320, 256, 3, 3, 3])
encoder.stages.4.0.convs.0.all_modules.0.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.1.weight shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.1.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.4.0.convs.1.conv.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.norm.weight shape torch.Size([320])
encoder.stages.4.0.convs.1.norm.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.4.0.convs.1.all_modules.0.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.1.weight shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.1.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.0.conv.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.norm.weight shape torch.Size([320])
encoder.stages.5.0.convs.0.norm.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.0.all_modules.0.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.1.weight shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.1.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.1.conv.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.norm.weight shape torch.Size([320])
encoder.stages.5.0.convs.1.norm.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.1.all_modules.0.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.1.weight shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.0.0.convs.0.conv.weight shape torch.Size([32, 1, 3, 3, 3])
decoder.encoder.stages.0.0.convs.0.conv.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.norm.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.norm.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.0.weight shape torch.Size([32, 1, 3, 3, 3])
decoder.encoder.stages.0.0.convs.0.all_modules.0.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.1.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.1.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.encoder.stages.0.0.convs.1.conv.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.norm.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.norm.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.encoder.stages.0.0.convs.1.all_modules.0.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.1.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.1.bias shape torch.Size([32])
decoder.encoder.stages.1.0.convs.0.conv.weight shape torch.Size([64, 32, 3, 3, 3])
decoder.encoder.stages.1.0.convs.0.conv.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.norm.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.norm.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.0.weight shape torch.Size([64, 32, 3, 3, 3])
decoder.encoder.stages.1.0.convs.0.all_modules.0.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.1.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.1.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.encoder.stages.1.0.convs.1.conv.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.norm.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.norm.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.encoder.stages.1.0.convs.1.all_modules.0.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.1.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.1.bias shape torch.Size([64])
decoder.encoder.stages.2.0.convs.0.conv.weight shape torch.Size([128, 64, 3, 3, 3])
decoder.encoder.stages.2.0.convs.0.conv.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.norm.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.norm.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.0.weight shape torch.Size([128, 64, 3, 3, 3])
decoder.encoder.stages.2.0.convs.0.all_modules.0.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.1.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.1.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.encoder.stages.2.0.convs.1.conv.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.norm.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.norm.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.encoder.stages.2.0.convs.1.all_modules.0.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.1.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.1.bias shape torch.Size([128])
decoder.encoder.stages.3.0.convs.0.conv.weight shape torch.Size([256, 128, 3, 3, 3])
decoder.encoder.stages.3.0.convs.0.conv.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.norm.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.norm.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.0.weight shape torch.Size([256, 128, 3, 3, 3])
decoder.encoder.stages.3.0.convs.0.all_modules.0.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.1.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.1.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.encoder.stages.3.0.convs.1.conv.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.norm.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.norm.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.encoder.stages.3.0.convs.1.all_modules.0.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.1.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.1.bias shape torch.Size([256])
decoder.encoder.stages.4.0.convs.0.conv.weight shape torch.Size([320, 256, 3, 3, 3])
decoder.encoder.stages.4.0.convs.0.conv.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.norm.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.norm.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.0.weight shape torch.Size([320, 256, 3, 3, 3])
decoder.encoder.stages.4.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.4.0.convs.1.conv.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.norm.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.norm.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.4.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.0.conv.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.norm.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.norm.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.1.conv.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.norm.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.norm.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.stages.0.convs.0.conv.weight shape torch.Size([320, 640, 3, 3, 3])
decoder.stages.0.convs.0.conv.bias shape torch.Size([320])
decoder.stages.0.convs.0.norm.weight shape torch.Size([320])
decoder.stages.0.convs.0.norm.bias shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.0.weight shape torch.Size([320, 640, 3, 3, 3])
decoder.stages.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.stages.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.stages.0.convs.1.conv.bias shape torch.Size([320])
decoder.stages.0.convs.1.norm.weight shape torch.Size([320])
decoder.stages.0.convs.1.norm.bias shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.stages.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.stages.1.convs.0.conv.weight shape torch.Size([256, 512, 3, 3, 3])
decoder.stages.1.convs.0.conv.bias shape torch.Size([256])
decoder.stages.1.convs.0.norm.weight shape torch.Size([256])
decoder.stages.1.convs.0.norm.bias shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.0.weight shape torch.Size([256, 512, 3, 3, 3])
decoder.stages.1.convs.0.all_modules.0.bias shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.1.weight shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.1.bias shape torch.Size([256])
decoder.stages.1.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.stages.1.convs.1.conv.bias shape torch.Size([256])
decoder.stages.1.convs.1.norm.weight shape torch.Size([256])
decoder.stages.1.convs.1.norm.bias shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.stages.1.convs.1.all_modules.0.bias shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.1.weight shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.1.bias shape torch.Size([256])
decoder.stages.2.convs.0.conv.weight shape torch.Size([128, 256, 3, 3, 3])
decoder.stages.2.convs.0.conv.bias shape torch.Size([128])
decoder.stages.2.convs.0.norm.weight shape torch.Size([128])
decoder.stages.2.convs.0.norm.bias shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.0.weight shape torch.Size([128, 256, 3, 3, 3])
decoder.stages.2.convs.0.all_modules.0.bias shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.1.weight shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.1.bias shape torch.Size([128])
decoder.stages.2.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.stages.2.convs.1.conv.bias shape torch.Size([128])
decoder.stages.2.convs.1.norm.weight shape torch.Size([128])
decoder.stages.2.convs.1.norm.bias shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.stages.2.convs.1.all_modules.0.bias shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.1.weight shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.1.bias shape torch.Size([128])
decoder.stages.3.convs.0.conv.weight shape torch.Size([64, 128, 3, 3, 3])
decoder.stages.3.convs.0.conv.bias shape torch.Size([64])
decoder.stages.3.convs.0.norm.weight shape torch.Size([64])
decoder.stages.3.convs.0.norm.bias shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.0.weight shape torch.Size([64, 128, 3, 3, 3])
decoder.stages.3.convs.0.all_modules.0.bias shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.1.weight shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.1.bias shape torch.Size([64])
decoder.stages.3.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.stages.3.convs.1.conv.bias shape torch.Size([64])
decoder.stages.3.convs.1.norm.weight shape torch.Size([64])
decoder.stages.3.convs.1.norm.bias shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.stages.3.convs.1.all_modules.0.bias shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.1.weight shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.1.bias shape torch.Size([64])
decoder.stages.4.convs.0.conv.weight shape torch.Size([32, 64, 3, 3, 3])
decoder.stages.4.convs.0.conv.bias shape torch.Size([32])
decoder.stages.4.convs.0.norm.weight shape torch.Size([32])
decoder.stages.4.convs.0.norm.bias shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.0.weight shape torch.Size([32, 64, 3, 3, 3])
decoder.stages.4.convs.0.all_modules.0.bias shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.1.weight shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.1.bias shape torch.Size([32])
decoder.stages.4.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.stages.4.convs.1.conv.bias shape torch.Size([32])
decoder.stages.4.convs.1.norm.weight shape torch.Size([32])
decoder.stages.4.convs.1.norm.bias shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.stages.4.convs.1.all_modules.0.bias shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.1.weight shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.1.bias shape torch.Size([32])
decoder.transpconvs.0.weight shape torch.Size([320, 320, 1, 2, 2])
decoder.transpconvs.0.bias shape torch.Size([320])
decoder.transpconvs.1.weight shape torch.Size([320, 256, 2, 2, 2])
decoder.transpconvs.1.bias shape torch.Size([256])
decoder.transpconvs.2.weight shape torch.Size([256, 128, 2, 2, 2])
decoder.transpconvs.2.bias shape torch.Size([128])
decoder.transpconvs.3.weight shape torch.Size([128, 64, 2, 2, 2])
decoder.transpconvs.3.bias shape torch.Size([64])
decoder.transpconvs.4.weight shape torch.Size([64, 32, 2, 2, 2])
decoder.transpconvs.4.bias shape torch.Size([32])
################### Done ###################
2024-12-08 16:39:34.874440: do_dummy_2d_data_aug: True
2024-12-08 16:39:34.876467: Using splits from existing split file: /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset307_Sohee_Calcium_OCT_CrossValidation/splits_final.json
2024-12-08 16:39:34.878249: The split file contains 5 splits.
2024-12-08 16:39:34.879329: Desired fold for training: 0
2024-12-08 16:39:34.880183: This split has 6 training and 2 validation cases.
using pin_memory on device 0
using pin_memory on device 0

This is the configuration used by this training:
Configuration name: 3d_32x160x128_b10
 {'data_identifier': 'nnUNetPlans_3d_fullres', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 10, 'patch_size': [32, 160, 128], 'median_image_size_in_voxels': [375.0, 498.0, 498.0], 'spacing': [1.0, 1.0, 1.0], 'normalization_schemes': ['ZScoreNormalization'], 'use_mask_for_norm': [False], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'architecture': {'network_class_name': 'dynamic_network_architectures.architectures.unet.PlainConvUNet', 'arch_kwargs': {'n_stages': 6, 'features_per_stage': [32, 64, 128, 256, 320, 320], 'conv_op': 'torch.nn.modules.conv.Conv3d', 'kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]], 'strides': [[1, 1, 1], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [1, 2, 2]], 'n_conv_per_stage': [2, 2, 2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2, 2, 2], 'conv_bias': True, 'norm_op': 'torch.nn.modules.instancenorm.InstanceNorm3d', 'norm_op_kwargs': {'eps': 1e-05, 'affine': True}, 'dropout_op': None, 'dropout_op_kwargs': None, 'nonlin': 'torch.nn.LeakyReLU', 'nonlin_kwargs': {'inplace': True}, 'deep_supervision': True}, '_kw_requires_import': ['conv_op', 'norm_op', 'dropout_op', 'nonlin']}, 'batch_dice': True, 'inherits_from': '3d_fullres'} 

These are the global plan.json settings:
 {'dataset_name': 'Dataset307_Sohee_Calcium_OCT_CrossValidation', 'plans_name': 'nnUNetPlans', 'original_median_spacing_after_transp': [1.0, 1.0, 1.0], 'original_median_shape_after_transp': [375, 498, 498], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 0.4977976381778717, 'mean': 0.13507525622844696, 'median': 0.09599608182907104, 'min': 0.0, 'percentile_00_5': 0.014754901640117168, 'percentile_99_5': 0.4977976381778717, 'std': 0.12025152146816254}}} 

2024-12-08 16:39:37.033045: unpacking dataset...
using pin_memory on device 0
using pin_memory on device 0

This is the configuration used by this training:
Configuration name: 3d_32x160x128_b10
 {'data_identifier': 'nnUNetPlans_3d_fullres', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 10, 'patch_size': [32, 160, 128], 'median_image_size_in_voxels': [375.0, 498.0, 498.0], 'spacing': [1.0, 1.0, 1.0], 'normalization_schemes': ['ZScoreNormalization'], 'use_mask_for_norm': [False], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'architecture': {'network_class_name': 'dynamic_network_architectures.architectures.unet.PlainConvUNet', 'arch_kwargs': {'n_stages': 6, 'features_per_stage': [32, 64, 128, 256, 320, 320], 'conv_op': 'torch.nn.modules.conv.Conv3d', 'kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]], 'strides': [[1, 1, 1], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [1, 2, 2]], 'n_conv_per_stage': [2, 2, 2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2, 2, 2], 'conv_bias': True, 'norm_op': 'torch.nn.modules.instancenorm.InstanceNorm3d', 'norm_op_kwargs': {'eps': 1e-05, 'affine': True}, 'dropout_op': None, 'dropout_op_kwargs': None, 'nonlin': 'torch.nn.LeakyReLU', 'nonlin_kwargs': {'inplace': True}, 'deep_supervision': True}, '_kw_requires_import': ['conv_op', 'norm_op', 'dropout_op', 'nonlin']}, 'batch_dice': True, 'inherits_from': '3d_fullres'} 

These are the global plan.json settings:
 {'dataset_name': 'Dataset307_Sohee_Calcium_OCT_CrossValidation', 'plans_name': 'nnUNetPlans', 'original_median_spacing_after_transp': [1.0, 1.0, 1.0], 'original_median_shape_after_transp': [375, 498, 498], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 0.4977976381778717, 'mean': 0.13507525622844696, 'median': 0.09599608182907104, 'min': 0.0, 'percentile_00_5': 0.014754901640117168, 'percentile_99_5': 0.4977976381778717, 'std': 0.12025152146816254}}} 

2024-12-08 16:39:37.766295: unpacking dataset...
2024-12-08 16:39:41.762624: unpacking done...
2024-12-08 16:39:41.769858: Unable to plot network architecture: nnUNet_compile is enabled!
2024-12-08 16:39:41.890848: 
2024-12-08 16:39:41.892373: Epoch 0
2024-12-08 16:39:41.893502: Current learning rate: 0.01
2024-12-08 16:42:11.894263: Validation loss improved from 1000.00000 to -0.19502! Patience: 0/50
2024-12-08 16:42:11.895950: train_loss -0.1352
2024-12-08 16:42:11.897091: val_loss -0.195
2024-12-08 16:42:11.897740: Pseudo dice [0.5667]
2024-12-08 16:42:11.898446: Epoch time: 150.01 s
2024-12-08 16:42:11.899223: Yayy! New best EMA pseudo Dice: 0.5667
2024-12-08 16:42:13.356891: 
2024-12-08 16:42:13.358603: Epoch 1
2024-12-08 16:42:13.359477: Current learning rate: 0.00999
2024-12-08 16:43:39.695844: Validation loss improved from -0.19502 to -0.24352! Patience: 0/50
2024-12-08 16:43:39.697020: train_loss -0.2555
2024-12-08 16:43:39.697838: val_loss -0.2435
2024-12-08 16:43:39.698472: Pseudo dice [0.6043]
2024-12-08 16:43:39.699168: Epoch time: 86.34 s
2024-12-08 16:43:39.700027: Yayy! New best EMA pseudo Dice: 0.5705
2024-12-08 16:43:41.283321: 
2024-12-08 16:43:41.284945: Epoch 2
2024-12-08 16:43:41.286059: Current learning rate: 0.00998
2024-12-08 16:45:08.229424: Validation loss improved from -0.24352 to -0.33135! Patience: 0/50
2024-12-08 16:45:08.230598: train_loss -0.3151
2024-12-08 16:45:08.231828: val_loss -0.3313
2024-12-08 16:45:08.232596: Pseudo dice [0.6622]
2024-12-08 16:45:08.233507: Epoch time: 86.95 s
2024-12-08 16:45:08.234403: Yayy! New best EMA pseudo Dice: 0.5796
2024-12-08 16:45:09.890924: 
2024-12-08 16:45:09.891707: Epoch 3
2024-12-08 16:45:09.892459: Current learning rate: 0.00997
2024-12-08 16:46:37.170976: Validation loss improved from -0.33135 to -0.33519! Patience: 0/50
2024-12-08 16:46:37.172068: train_loss -0.3647
2024-12-08 16:46:37.172974: val_loss -0.3352
2024-12-08 16:46:37.173823: Pseudo dice [0.6426]
2024-12-08 16:46:37.174622: Epoch time: 87.28 s
2024-12-08 16:46:37.175488: Yayy! New best EMA pseudo Dice: 0.5859
2024-12-08 16:46:38.787165: 
2024-12-08 16:46:38.788875: Epoch 4
2024-12-08 16:46:38.789686: Current learning rate: 0.00996
2024-12-08 16:48:06.189628: Validation loss did not improve from -0.33519. Patience: 1/50
2024-12-08 16:48:06.190878: train_loss -0.3849
2024-12-08 16:48:06.191844: val_loss -0.3322
2024-12-08 16:48:06.192614: Pseudo dice [0.6406]
2024-12-08 16:48:06.193522: Epoch time: 87.4 s
2024-12-08 16:48:06.499578: Yayy! New best EMA pseudo Dice: 0.5914
2024-12-08 16:48:08.139585: 
2024-12-08 16:48:08.141196: Epoch 5
2024-12-08 16:48:08.141902: Current learning rate: 0.00995
2024-12-08 16:49:35.518208: Validation loss improved from -0.33519 to -0.36676! Patience: 1/50
2024-12-08 16:49:35.518960: train_loss -0.4161
2024-12-08 16:49:35.519829: val_loss -0.3668
2024-12-08 16:49:35.520597: Pseudo dice [0.6759]
2024-12-08 16:49:35.521443: Epoch time: 87.38 s
2024-12-08 16:49:35.522186: Yayy! New best EMA pseudo Dice: 0.5998
2024-12-08 16:49:37.065972: 
2024-12-08 16:49:37.067584: Epoch 6
2024-12-08 16:49:37.068441: Current learning rate: 0.00995
2024-12-08 16:51:04.101339: Validation loss improved from -0.36676 to -0.40449! Patience: 0/50
2024-12-08 16:51:04.102212: train_loss -0.4533
2024-12-08 16:51:04.103285: val_loss -0.4045
2024-12-08 16:51:04.104188: Pseudo dice [0.6937]
2024-12-08 16:51:04.105094: Epoch time: 87.04 s
2024-12-08 16:51:04.105959: Yayy! New best EMA pseudo Dice: 0.6092
2024-12-08 16:51:05.707608: 
2024-12-08 16:51:05.709260: Epoch 7
2024-12-08 16:51:05.710037: Current learning rate: 0.00994
2024-12-08 16:52:32.939225: Validation loss did not improve from -0.40449. Patience: 1/50
2024-12-08 16:52:32.940036: train_loss -0.4565
2024-12-08 16:52:32.941004: val_loss -0.3797
2024-12-08 16:52:32.941727: Pseudo dice [0.6794]
2024-12-08 16:52:32.942786: Epoch time: 87.23 s
2024-12-08 16:52:32.943726: Yayy! New best EMA pseudo Dice: 0.6162
2024-12-08 16:52:34.884866: 
2024-12-08 16:52:34.886081: Epoch 8
2024-12-08 16:52:34.887069: Current learning rate: 0.00993
2024-12-08 16:54:02.682071: Validation loss did not improve from -0.40449. Patience: 2/50
2024-12-08 16:54:02.683359: train_loss -0.4622
2024-12-08 16:54:02.684487: val_loss -0.4031
2024-12-08 16:54:02.685534: Pseudo dice [0.6844]
2024-12-08 16:54:02.686579: Epoch time: 87.8 s
2024-12-08 16:54:02.687493: Yayy! New best EMA pseudo Dice: 0.6231
2024-12-08 16:54:04.342044: 
2024-12-08 16:54:04.344106: Epoch 9
2024-12-08 16:54:04.345456: Current learning rate: 0.00992
2024-12-08 16:55:31.819754: Validation loss improved from -0.40449 to -0.43378! Patience: 2/50
2024-12-08 16:55:31.820628: train_loss -0.479
2024-12-08 16:55:31.821619: val_loss -0.4338
2024-12-08 16:55:31.822367: Pseudo dice [0.7162]
2024-12-08 16:55:31.823307: Epoch time: 87.48 s
2024-12-08 16:55:32.156175: Yayy! New best EMA pseudo Dice: 0.6324
2024-12-08 16:55:33.702664: 
2024-12-08 16:55:33.703927: Epoch 10
2024-12-08 16:55:33.704687: Current learning rate: 0.00991
2024-12-08 16:57:01.179934: Validation loss did not improve from -0.43378. Patience: 1/50
2024-12-08 16:57:01.180923: train_loss -0.4902
2024-12-08 16:57:01.181790: val_loss -0.4266
2024-12-08 16:57:01.182625: Pseudo dice [0.7093]
2024-12-08 16:57:01.183575: Epoch time: 87.48 s
2024-12-08 16:57:01.184416: Yayy! New best EMA pseudo Dice: 0.6401
2024-12-08 16:57:02.754640: 
2024-12-08 16:57:02.756320: Epoch 11
2024-12-08 16:57:02.757259: Current learning rate: 0.0099
2024-12-08 16:58:30.283153: Validation loss improved from -0.43378 to -0.47001! Patience: 1/50
2024-12-08 16:58:30.284404: train_loss -0.4995
2024-12-08 16:58:30.285424: val_loss -0.47
2024-12-08 16:58:30.286261: Pseudo dice [0.727]
2024-12-08 16:58:30.287208: Epoch time: 87.53 s
2024-12-08 16:58:30.288121: Yayy! New best EMA pseudo Dice: 0.6488
2024-12-08 16:58:31.837052: 
2024-12-08 16:58:31.838275: Epoch 12
2024-12-08 16:58:31.839387: Current learning rate: 0.00989
2024-12-08 16:59:59.283978: Validation loss did not improve from -0.47001. Patience: 1/50
2024-12-08 16:59:59.284971: train_loss -0.4937
2024-12-08 16:59:59.285909: val_loss -0.4563
2024-12-08 16:59:59.286718: Pseudo dice [0.7238]
2024-12-08 16:59:59.287374: Epoch time: 87.45 s
2024-12-08 16:59:59.288212: Yayy! New best EMA pseudo Dice: 0.6563
2024-12-08 17:00:00.860686: 
2024-12-08 17:00:00.861968: Epoch 13
2024-12-08 17:00:00.862725: Current learning rate: 0.00988
2024-12-08 17:01:28.217546: Validation loss did not improve from -0.47001. Patience: 2/50
2024-12-08 17:01:28.218432: train_loss -0.5092
2024-12-08 17:01:28.219468: val_loss -0.4694
2024-12-08 17:01:28.220283: Pseudo dice [0.7281]
2024-12-08 17:01:28.221107: Epoch time: 87.36 s
2024-12-08 17:01:28.221823: Yayy! New best EMA pseudo Dice: 0.6635
2024-12-08 17:01:29.802182: 
2024-12-08 17:01:29.803580: Epoch 14
2024-12-08 17:01:29.804347: Current learning rate: 0.00987
2024-12-08 17:02:57.219717: Validation loss improved from -0.47001 to -0.48606! Patience: 2/50
2024-12-08 17:02:57.220819: train_loss -0.5034
2024-12-08 17:02:57.221749: val_loss -0.4861
2024-12-08 17:02:57.222479: Pseudo dice [0.7488]
2024-12-08 17:02:57.223199: Epoch time: 87.42 s
2024-12-08 17:02:57.566100: Yayy! New best EMA pseudo Dice: 0.672
2024-12-08 17:02:59.173036: 
2024-12-08 17:02:59.174847: Epoch 15
2024-12-08 17:02:59.175795: Current learning rate: 0.00986
2024-12-08 17:04:26.462881: Validation loss did not improve from -0.48606. Patience: 1/50
2024-12-08 17:04:26.463987: train_loss -0.5303
2024-12-08 17:04:26.465162: val_loss -0.4773
2024-12-08 17:04:26.466241: Pseudo dice [0.7332]
2024-12-08 17:04:26.467208: Epoch time: 87.29 s
2024-12-08 17:04:26.468108: Yayy! New best EMA pseudo Dice: 0.6781
2024-12-08 17:04:28.095926: 
2024-12-08 17:04:28.097363: Epoch 16
2024-12-08 17:04:28.098186: Current learning rate: 0.00986
2024-12-08 17:05:55.823345: Validation loss did not improve from -0.48606. Patience: 2/50
2024-12-08 17:05:55.824379: train_loss -0.5348
2024-12-08 17:05:55.825849: val_loss -0.4617
2024-12-08 17:05:55.826925: Pseudo dice [0.7288]
2024-12-08 17:05:55.827828: Epoch time: 87.73 s
2024-12-08 17:05:55.828841: Yayy! New best EMA pseudo Dice: 0.6832
2024-12-08 17:05:57.457566: 
2024-12-08 17:05:57.459286: Epoch 17
2024-12-08 17:05:57.460060: Current learning rate: 0.00985
2024-12-08 17:07:25.220176: Validation loss did not improve from -0.48606. Patience: 3/50
2024-12-08 17:07:25.221160: train_loss -0.5319
2024-12-08 17:07:25.222361: val_loss -0.4603
2024-12-08 17:07:25.223398: Pseudo dice [0.7212]
2024-12-08 17:07:25.224450: Epoch time: 87.76 s
2024-12-08 17:07:25.225299: Yayy! New best EMA pseudo Dice: 0.687
2024-12-08 17:07:26.860697: 
2024-12-08 17:07:26.862477: Epoch 18
2024-12-08 17:07:26.863366: Current learning rate: 0.00984
2024-12-08 17:08:54.710505: Validation loss improved from -0.48606 to -0.48812! Patience: 3/50
2024-12-08 17:08:54.711420: train_loss -0.5387
2024-12-08 17:08:54.712280: val_loss -0.4881
2024-12-08 17:08:54.712961: Pseudo dice [0.7415]
2024-12-08 17:08:54.713621: Epoch time: 87.85 s
2024-12-08 17:08:54.714468: Yayy! New best EMA pseudo Dice: 0.6924
2024-12-08 17:08:57.263105: 
2024-12-08 17:08:57.265024: Epoch 19
2024-12-08 17:08:57.266147: Current learning rate: 0.00983
2024-12-08 17:10:25.160293: Validation loss improved from -0.48812 to -0.50567! Patience: 0/50
2024-12-08 17:10:25.161645: train_loss -0.5437
2024-12-08 17:10:25.162720: val_loss -0.5057
2024-12-08 17:10:25.163599: Pseudo dice [0.7483]
2024-12-08 17:10:25.164452: Epoch time: 87.9 s
2024-12-08 17:10:25.603370: Yayy! New best EMA pseudo Dice: 0.698
2024-12-08 17:10:27.247209: 
2024-12-08 17:10:27.248366: Epoch 20
2024-12-08 17:10:27.249111: Current learning rate: 0.00982
2024-12-08 17:11:55.141929: Validation loss did not improve from -0.50567. Patience: 1/50
2024-12-08 17:11:55.143210: train_loss -0.5587
2024-12-08 17:11:55.144074: val_loss -0.4936
2024-12-08 17:11:55.144962: Pseudo dice [0.7485]
2024-12-08 17:11:55.145774: Epoch time: 87.9 s
2024-12-08 17:11:55.146477: Yayy! New best EMA pseudo Dice: 0.7031
2024-12-08 17:11:56.903749: 
2024-12-08 17:11:56.904849: Epoch 21
2024-12-08 17:11:56.905633: Current learning rate: 0.00981
2024-12-08 17:13:24.781776: Validation loss did not improve from -0.50567. Patience: 2/50
2024-12-08 17:13:24.782645: train_loss -0.5624
2024-12-08 17:13:24.783396: val_loss -0.4941
2024-12-08 17:13:24.784015: Pseudo dice [0.754]
2024-12-08 17:13:24.784718: Epoch time: 87.88 s
2024-12-08 17:13:24.785380: Yayy! New best EMA pseudo Dice: 0.7082
2024-12-08 17:13:26.307121: 
2024-12-08 17:13:26.308661: Epoch 22
2024-12-08 17:13:26.309428: Current learning rate: 0.0098
2024-12-08 17:14:54.094857: Validation loss did not improve from -0.50567. Patience: 3/50
2024-12-08 17:14:54.096094: train_loss -0.5626
2024-12-08 17:14:54.096993: val_loss -0.4835
2024-12-08 17:14:54.097745: Pseudo dice [0.7397]
2024-12-08 17:14:54.098547: Epoch time: 87.79 s
2024-12-08 17:14:54.099226: Yayy! New best EMA pseudo Dice: 0.7113
2024-12-08 17:14:55.637224: 
2024-12-08 17:14:55.638644: Epoch 23
2024-12-08 17:14:55.639488: Current learning rate: 0.00979
2024-12-08 17:16:23.235210: Validation loss improved from -0.50567 to -0.51367! Patience: 3/50
2024-12-08 17:16:23.236383: train_loss -0.5714
2024-12-08 17:16:23.237357: val_loss -0.5137
2024-12-08 17:16:23.238188: Pseudo dice [0.7592]
2024-12-08 17:16:23.239097: Epoch time: 87.6 s
2024-12-08 17:16:23.239853: Yayy! New best EMA pseudo Dice: 0.7161
2024-12-08 17:16:24.746882: 
2024-12-08 17:16:24.748434: Epoch 24
2024-12-08 17:16:24.749687: Current learning rate: 0.00978
2024-12-08 17:17:52.528166: Validation loss did not improve from -0.51367. Patience: 1/50
2024-12-08 17:17:52.528956: train_loss -0.5687
2024-12-08 17:17:52.529955: val_loss -0.4763
2024-12-08 17:17:52.530840: Pseudo dice [0.7294]
2024-12-08 17:17:52.531744: Epoch time: 87.78 s
2024-12-08 17:17:52.871902: Yayy! New best EMA pseudo Dice: 0.7174
2024-12-08 17:17:54.409633: 
2024-12-08 17:17:54.411267: Epoch 25
2024-12-08 17:17:54.412105: Current learning rate: 0.00977
2024-12-08 17:19:22.126105: Validation loss improved from -0.51367 to -0.51382! Patience: 1/50
2024-12-08 17:19:22.127268: train_loss -0.5783
2024-12-08 17:19:22.128274: val_loss -0.5138
2024-12-08 17:19:22.129070: Pseudo dice [0.7574]
2024-12-08 17:19:22.129859: Epoch time: 87.72 s
2024-12-08 17:19:22.130515: Yayy! New best EMA pseudo Dice: 0.7214
2024-12-08 17:19:23.654642: 
2024-12-08 17:19:23.655999: Epoch 26
2024-12-08 17:19:23.656720: Current learning rate: 0.00977
2024-12-08 17:20:51.214204: Validation loss did not improve from -0.51382. Patience: 1/50
2024-12-08 17:20:51.215089: train_loss -0.5807
2024-12-08 17:20:51.216079: val_loss -0.4837
2024-12-08 17:20:51.217045: Pseudo dice [0.7422]
2024-12-08 17:20:51.217889: Epoch time: 87.56 s
2024-12-08 17:20:51.218758: Yayy! New best EMA pseudo Dice: 0.7235
2024-12-08 17:20:52.777072: 
2024-12-08 17:20:52.778989: Epoch 27
2024-12-08 17:20:52.780183: Current learning rate: 0.00976
2024-12-08 17:22:20.118325: Validation loss improved from -0.51382 to -0.52613! Patience: 1/50
2024-12-08 17:22:20.119151: train_loss -0.5868
2024-12-08 17:22:20.120046: val_loss -0.5261
2024-12-08 17:22:20.120808: Pseudo dice [0.7619]
2024-12-08 17:22:20.121563: Epoch time: 87.34 s
2024-12-08 17:22:20.122335: Yayy! New best EMA pseudo Dice: 0.7273
2024-12-08 17:22:21.668842: 
2024-12-08 17:22:21.669876: Epoch 28
2024-12-08 17:22:21.670635: Current learning rate: 0.00975
2024-12-08 17:23:48.883985: Validation loss did not improve from -0.52613. Patience: 1/50
2024-12-08 17:23:48.884909: train_loss -0.5842
2024-12-08 17:23:48.885936: val_loss -0.4651
2024-12-08 17:23:48.886741: Pseudo dice [0.7399]
2024-12-08 17:23:48.887492: Epoch time: 87.22 s
2024-12-08 17:23:48.888189: Yayy! New best EMA pseudo Dice: 0.7286
2024-12-08 17:23:50.798502: 
2024-12-08 17:23:50.799935: Epoch 29
2024-12-08 17:23:50.800735: Current learning rate: 0.00974
2024-12-08 17:25:18.191336: Validation loss did not improve from -0.52613. Patience: 2/50
2024-12-08 17:25:18.192323: train_loss -0.5878
2024-12-08 17:25:18.193168: val_loss -0.5237
2024-12-08 17:25:18.193820: Pseudo dice [0.7609]
2024-12-08 17:25:18.194634: Epoch time: 87.39 s
2024-12-08 17:25:18.552236: Yayy! New best EMA pseudo Dice: 0.7318
2024-12-08 17:25:20.165903: 
2024-12-08 17:25:20.167527: Epoch 30
2024-12-08 17:25:20.168327: Current learning rate: 0.00973
2024-12-08 17:26:47.356873: Validation loss did not improve from -0.52613. Patience: 3/50
2024-12-08 17:26:47.357887: train_loss -0.5994
2024-12-08 17:26:47.358791: val_loss -0.5023
2024-12-08 17:26:47.359605: Pseudo dice [0.7454]
2024-12-08 17:26:47.360552: Epoch time: 87.19 s
2024-12-08 17:26:47.361344: Yayy! New best EMA pseudo Dice: 0.7332
2024-12-08 17:26:48.967413: 
2024-12-08 17:26:48.968750: Epoch 31
2024-12-08 17:26:48.969584: Current learning rate: 0.00972
2024-12-08 17:28:16.141905: Validation loss did not improve from -0.52613. Patience: 4/50
2024-12-08 17:28:16.142857: train_loss -0.607
2024-12-08 17:28:16.143894: val_loss -0.5202
2024-12-08 17:28:16.144828: Pseudo dice [0.7502]
2024-12-08 17:28:16.145751: Epoch time: 87.18 s
2024-12-08 17:28:16.146542: Yayy! New best EMA pseudo Dice: 0.7349
2024-12-08 17:28:17.736177: 
2024-12-08 17:28:17.737804: Epoch 32
2024-12-08 17:28:17.738741: Current learning rate: 0.00971
2024-12-08 17:29:45.129078: Validation loss improved from -0.52613 to -0.54926! Patience: 4/50
2024-12-08 17:29:45.129853: train_loss -0.603
2024-12-08 17:29:45.130881: val_loss -0.5493
2024-12-08 17:29:45.131708: Pseudo dice [0.7758]
2024-12-08 17:29:45.132486: Epoch time: 87.39 s
2024-12-08 17:29:45.133193: Yayy! New best EMA pseudo Dice: 0.739
2024-12-08 17:29:46.705604: 
2024-12-08 17:29:46.706989: Epoch 33
2024-12-08 17:29:46.707866: Current learning rate: 0.0097
2024-12-08 17:31:13.954357: Validation loss did not improve from -0.54926. Patience: 1/50
2024-12-08 17:31:13.955343: train_loss -0.6008
2024-12-08 17:31:13.956452: val_loss -0.5455
2024-12-08 17:31:13.957223: Pseudo dice [0.7735]
2024-12-08 17:31:13.957973: Epoch time: 87.25 s
2024-12-08 17:31:13.958680: Yayy! New best EMA pseudo Dice: 0.7424
2024-12-08 17:31:15.510409: 
2024-12-08 17:31:15.512010: Epoch 34
2024-12-08 17:31:15.512971: Current learning rate: 0.00969
2024-12-08 17:32:43.041550: Validation loss did not improve from -0.54926. Patience: 2/50
2024-12-08 17:32:43.042711: train_loss -0.6138
2024-12-08 17:32:43.043581: val_loss -0.5228
2024-12-08 17:32:43.044265: Pseudo dice [0.7616]
2024-12-08 17:32:43.044983: Epoch time: 87.53 s
2024-12-08 17:32:43.426258: Yayy! New best EMA pseudo Dice: 0.7443
2024-12-08 17:32:45.019015: 
2024-12-08 17:32:45.020593: Epoch 35
2024-12-08 17:32:45.021593: Current learning rate: 0.00968
2024-12-08 17:34:12.730405: Validation loss did not improve from -0.54926. Patience: 3/50
2024-12-08 17:34:12.731394: train_loss -0.6117
2024-12-08 17:34:12.732174: val_loss -0.5354
2024-12-08 17:34:12.732864: Pseudo dice [0.7757]
2024-12-08 17:34:12.733550: Epoch time: 87.71 s
2024-12-08 17:34:12.734259: Yayy! New best EMA pseudo Dice: 0.7475
2024-12-08 17:34:14.391487: 
2024-12-08 17:34:14.392908: Epoch 36
2024-12-08 17:34:14.393611: Current learning rate: 0.00968
2024-12-08 17:35:41.821910: Validation loss did not improve from -0.54926. Patience: 4/50
2024-12-08 17:35:41.823246: train_loss -0.6219
2024-12-08 17:35:41.824114: val_loss -0.5333
2024-12-08 17:35:41.824858: Pseudo dice [0.7701]
2024-12-08 17:35:41.825577: Epoch time: 87.43 s
2024-12-08 17:35:41.826200: Yayy! New best EMA pseudo Dice: 0.7497
2024-12-08 17:35:43.444404: 
2024-12-08 17:35:43.446398: Epoch 37
2024-12-08 17:35:43.447169: Current learning rate: 0.00967
2024-12-08 17:37:10.733224: Validation loss did not improve from -0.54926. Patience: 5/50
2024-12-08 17:37:10.734453: train_loss -0.6194
2024-12-08 17:37:10.735749: val_loss -0.3599
2024-12-08 17:37:10.736578: Pseudo dice [0.664]
2024-12-08 17:37:10.737414: Epoch time: 87.29 s
2024-12-08 17:37:11.974662: 
2024-12-08 17:37:11.976242: Epoch 38
2024-12-08 17:37:11.977081: Current learning rate: 0.00966
2024-12-08 17:38:39.204516: Validation loss did not improve from -0.54926. Patience: 6/50
2024-12-08 17:38:39.205380: train_loss -0.6154
2024-12-08 17:38:39.206418: val_loss -0.4784
2024-12-08 17:38:39.207319: Pseudo dice [0.7435]
2024-12-08 17:38:39.208464: Epoch time: 87.23 s
2024-12-08 17:38:40.463225: 
2024-12-08 17:38:40.464479: Epoch 39
2024-12-08 17:38:40.465256: Current learning rate: 0.00965
2024-12-08 17:40:07.792972: Validation loss did not improve from -0.54926. Patience: 7/50
2024-12-08 17:40:07.793933: train_loss -0.6186
2024-12-08 17:40:07.794850: val_loss -0.4933
2024-12-08 17:40:07.795661: Pseudo dice [0.7488]
2024-12-08 17:40:07.796467: Epoch time: 87.33 s
2024-12-08 17:40:09.880702: 
2024-12-08 17:40:09.882210: Epoch 40
2024-12-08 17:40:09.883232: Current learning rate: 0.00964
2024-12-08 17:41:36.934651: Validation loss did not improve from -0.54926. Patience: 8/50
2024-12-08 17:41:36.936146: train_loss -0.6203
2024-12-08 17:41:36.937219: val_loss -0.4911
2024-12-08 17:41:36.938103: Pseudo dice [0.7431]
2024-12-08 17:41:36.938981: Epoch time: 87.06 s
2024-12-08 17:41:38.210240: 
2024-12-08 17:41:38.211876: Epoch 41
2024-12-08 17:41:38.212699: Current learning rate: 0.00963
2024-12-08 17:43:05.590224: Validation loss did not improve from -0.54926. Patience: 9/50
2024-12-08 17:43:05.591513: train_loss -0.6289
2024-12-08 17:43:05.592660: val_loss -0.5406
2024-12-08 17:43:05.593601: Pseudo dice [0.7658]
2024-12-08 17:43:05.594482: Epoch time: 87.38 s
2024-12-08 17:43:06.780902: 
2024-12-08 17:43:06.782897: Epoch 42
2024-12-08 17:43:06.783758: Current learning rate: 0.00962
2024-12-08 17:44:33.999647: Validation loss did not improve from -0.54926. Patience: 10/50
2024-12-08 17:44:34.001007: train_loss -0.6373
2024-12-08 17:44:34.002326: val_loss -0.5021
2024-12-08 17:44:34.003092: Pseudo dice [0.7533]
2024-12-08 17:44:34.003805: Epoch time: 87.22 s
2024-12-08 17:44:35.180265: 
2024-12-08 17:44:35.181820: Epoch 43
2024-12-08 17:44:35.182606: Current learning rate: 0.00961
2024-12-08 17:46:02.623198: Validation loss did not improve from -0.54926. Patience: 11/50
2024-12-08 17:46:02.624199: train_loss -0.631
2024-12-08 17:46:02.624990: val_loss -0.5029
2024-12-08 17:46:02.625832: Pseudo dice [0.7464]
2024-12-08 17:46:02.626681: Epoch time: 87.44 s
2024-12-08 17:46:03.874938: 
2024-12-08 17:46:03.876334: Epoch 44
2024-12-08 17:46:03.877168: Current learning rate: 0.0096
2024-12-08 17:47:31.372316: Validation loss did not improve from -0.54926. Patience: 12/50
2024-12-08 17:47:31.373721: train_loss -0.6372
2024-12-08 17:47:31.374859: val_loss -0.5426
2024-12-08 17:47:31.375623: Pseudo dice [0.7727]
2024-12-08 17:47:31.376475: Epoch time: 87.5 s
2024-12-08 17:47:32.876758: 
2024-12-08 17:47:32.878195: Epoch 45
2024-12-08 17:47:32.879119: Current learning rate: 0.00959
2024-12-08 17:49:00.469083: Validation loss did not improve from -0.54926. Patience: 13/50
2024-12-08 17:49:00.470019: train_loss -0.6383
2024-12-08 17:49:00.471027: val_loss -0.4905
2024-12-08 17:49:00.471863: Pseudo dice [0.7451]
2024-12-08 17:49:00.472760: Epoch time: 87.59 s
2024-12-08 17:49:01.698801: 
2024-12-08 17:49:01.700396: Epoch 46
2024-12-08 17:49:01.701222: Current learning rate: 0.00959
2024-12-08 17:50:29.163457: Validation loss improved from -0.54926 to -0.55354! Patience: 13/50
2024-12-08 17:50:29.164783: train_loss -0.6406
2024-12-08 17:50:29.165961: val_loss -0.5535
2024-12-08 17:50:29.166747: Pseudo dice [0.7815]
2024-12-08 17:50:29.167523: Epoch time: 87.47 s
2024-12-08 17:50:29.168295: Yayy! New best EMA pseudo Dice: 0.7513
2024-12-08 17:50:30.676077: 
2024-12-08 17:50:30.678478: Epoch 47
2024-12-08 17:50:30.679334: Current learning rate: 0.00958
2024-12-08 17:51:58.132141: Validation loss did not improve from -0.55354. Patience: 1/50
2024-12-08 17:51:58.133166: train_loss -0.6469
2024-12-08 17:51:58.134100: val_loss -0.5402
2024-12-08 17:51:58.134898: Pseudo dice [0.7766]
2024-12-08 17:51:58.135834: Epoch time: 87.46 s
2024-12-08 17:51:58.136749: Yayy! New best EMA pseudo Dice: 0.7538
2024-12-08 17:51:59.673486: 
2024-12-08 17:51:59.675030: Epoch 48
2024-12-08 17:51:59.676149: Current learning rate: 0.00957
2024-12-08 17:53:27.220610: Validation loss did not improve from -0.55354. Patience: 2/50
2024-12-08 17:53:27.221742: train_loss -0.6463
2024-12-08 17:53:27.222667: val_loss -0.5218
2024-12-08 17:53:27.223430: Pseudo dice [0.7475]
2024-12-08 17:53:27.224168: Epoch time: 87.55 s
2024-12-08 17:53:28.472176: 
2024-12-08 17:53:28.473350: Epoch 49
2024-12-08 17:53:28.474131: Current learning rate: 0.00956
2024-12-08 17:54:55.997101: Validation loss improved from -0.55354 to -0.55484! Patience: 2/50
2024-12-08 17:54:55.998072: train_loss -0.6508
2024-12-08 17:54:55.998899: val_loss -0.5548
2024-12-08 17:54:55.999593: Pseudo dice [0.7797]
2024-12-08 17:54:56.000294: Epoch time: 87.53 s
2024-12-08 17:54:56.390469: Yayy! New best EMA pseudo Dice: 0.7558
2024-12-08 17:54:57.970894: 
2024-12-08 17:54:57.972161: Epoch 50
2024-12-08 17:54:57.972846: Current learning rate: 0.00955
2024-12-08 17:56:25.405340: Validation loss did not improve from -0.55484. Patience: 1/50
2024-12-08 17:56:25.406538: train_loss -0.6537
2024-12-08 17:56:25.407639: val_loss -0.5213
2024-12-08 17:56:25.408584: Pseudo dice [0.7655]
2024-12-08 17:56:25.409498: Epoch time: 87.44 s
2024-12-08 17:56:25.410174: Yayy! New best EMA pseudo Dice: 0.7568
2024-12-08 17:56:27.420165: 
2024-12-08 17:56:27.421849: Epoch 51
2024-12-08 17:56:27.422713: Current learning rate: 0.00954
2024-12-08 17:57:54.843772: Validation loss did not improve from -0.55484. Patience: 2/50
2024-12-08 17:57:54.844946: train_loss -0.6512
2024-12-08 17:57:54.846080: val_loss -0.4858
2024-12-08 17:57:54.847130: Pseudo dice [0.7404]
2024-12-08 17:57:54.847935: Epoch time: 87.43 s
2024-12-08 17:57:56.101789: 
2024-12-08 17:57:56.103154: Epoch 52
2024-12-08 17:57:56.104203: Current learning rate: 0.00953
2024-12-08 17:59:23.699819: Validation loss did not improve from -0.55484. Patience: 3/50
2024-12-08 17:59:23.700823: train_loss -0.6551
2024-12-08 17:59:23.701874: val_loss -0.5421
2024-12-08 17:59:23.702850: Pseudo dice [0.7676]
2024-12-08 17:59:23.703648: Epoch time: 87.6 s
2024-12-08 17:59:24.961885: 
2024-12-08 17:59:24.963858: Epoch 53
2024-12-08 17:59:24.964640: Current learning rate: 0.00952
2024-12-08 18:00:52.606789: Validation loss did not improve from -0.55484. Patience: 4/50
2024-12-08 18:00:52.608063: train_loss -0.6616
2024-12-08 18:00:52.609200: val_loss -0.5058
2024-12-08 18:00:52.610030: Pseudo dice [0.752]
2024-12-08 18:00:52.610954: Epoch time: 87.65 s
2024-12-08 18:00:53.855978: 
2024-12-08 18:00:53.858218: Epoch 54
2024-12-08 18:00:53.859259: Current learning rate: 0.00951
2024-12-08 18:02:21.401192: Validation loss did not improve from -0.55484. Patience: 5/50
2024-12-08 18:02:21.402223: train_loss -0.6537
2024-12-08 18:02:21.403302: val_loss -0.5336
2024-12-08 18:02:21.404453: Pseudo dice [0.7661]
2024-12-08 18:02:21.405479: Epoch time: 87.55 s
2024-12-08 18:02:21.764057: Yayy! New best EMA pseudo Dice: 0.757
2024-12-08 18:02:23.300716: 
2024-12-08 18:02:23.301513: Epoch 55
2024-12-08 18:02:23.302229: Current learning rate: 0.0095
2024-12-08 18:03:50.780167: Validation loss did not improve from -0.55484. Patience: 6/50
2024-12-08 18:03:50.781104: train_loss -0.6565
2024-12-08 18:03:50.781963: val_loss -0.4991
2024-12-08 18:03:50.782872: Pseudo dice [0.7487]
2024-12-08 18:03:50.783600: Epoch time: 87.48 s
2024-12-08 18:03:52.016879: 
2024-12-08 18:03:52.018641: Epoch 56
2024-12-08 18:03:52.019545: Current learning rate: 0.00949
2024-12-08 18:05:19.551715: Validation loss did not improve from -0.55484. Patience: 7/50
2024-12-08 18:05:19.552652: train_loss -0.658
2024-12-08 18:05:19.553546: val_loss -0.5293
2024-12-08 18:05:19.554368: Pseudo dice [0.7656]
2024-12-08 18:05:19.555127: Epoch time: 87.54 s
2024-12-08 18:05:19.555923: Yayy! New best EMA pseudo Dice: 0.7571
2024-12-08 18:05:21.108607: 
2024-12-08 18:05:21.110089: Epoch 57
2024-12-08 18:05:21.110986: Current learning rate: 0.00949
2024-12-08 18:06:48.817622: Validation loss did not improve from -0.55484. Patience: 8/50
2024-12-08 18:06:48.818868: train_loss -0.6621
2024-12-08 18:06:48.819720: val_loss -0.515
2024-12-08 18:06:48.820518: Pseudo dice [0.765]
2024-12-08 18:06:48.821462: Epoch time: 87.71 s
2024-12-08 18:06:48.822126: Yayy! New best EMA pseudo Dice: 0.7579
2024-12-08 18:06:50.375845: 
2024-12-08 18:06:50.377769: Epoch 58
2024-12-08 18:06:50.378599: Current learning rate: 0.00948
2024-12-08 18:08:17.947432: Validation loss did not improve from -0.55484. Patience: 9/50
2024-12-08 18:08:17.948305: train_loss -0.6654
2024-12-08 18:08:17.949277: val_loss -0.5368
2024-12-08 18:08:17.950168: Pseudo dice [0.7721]
2024-12-08 18:08:17.951078: Epoch time: 87.57 s
2024-12-08 18:08:17.951960: Yayy! New best EMA pseudo Dice: 0.7593
2024-12-08 18:08:19.476057: 
2024-12-08 18:08:19.477723: Epoch 59
2024-12-08 18:08:19.478683: Current learning rate: 0.00947
2024-12-08 18:09:47.100098: Validation loss did not improve from -0.55484. Patience: 10/50
2024-12-08 18:09:47.101194: train_loss -0.674
2024-12-08 18:09:47.103414: val_loss -0.4767
2024-12-08 18:09:47.104288: Pseudo dice [0.7515]
2024-12-08 18:09:47.106290: Epoch time: 87.63 s
2024-12-08 18:09:48.651729: 
2024-12-08 18:09:48.653089: Epoch 60
2024-12-08 18:09:48.653952: Current learning rate: 0.00946
2024-12-08 18:11:16.439085: Validation loss did not improve from -0.55484. Patience: 11/50
2024-12-08 18:11:16.440083: train_loss -0.6722
2024-12-08 18:11:16.441088: val_loss -0.5331
2024-12-08 18:11:16.441940: Pseudo dice [0.7713]
2024-12-08 18:11:16.442810: Epoch time: 87.79 s
2024-12-08 18:11:16.443566: Yayy! New best EMA pseudo Dice: 0.7598
2024-12-08 18:11:18.025092: 
2024-12-08 18:11:18.026134: Epoch 61
2024-12-08 18:11:18.026833: Current learning rate: 0.00945
2024-12-08 18:12:45.779965: Validation loss did not improve from -0.55484. Patience: 12/50
2024-12-08 18:12:45.780806: train_loss -0.6765
2024-12-08 18:12:45.781792: val_loss -0.5279
2024-12-08 18:12:45.782583: Pseudo dice [0.7599]
2024-12-08 18:12:45.783261: Epoch time: 87.76 s
2024-12-08 18:12:45.783988: Yayy! New best EMA pseudo Dice: 0.7598
2024-12-08 18:12:47.776652: 
2024-12-08 18:12:47.778414: Epoch 62
2024-12-08 18:12:47.779318: Current learning rate: 0.00944
2024-12-08 18:14:15.500731: Validation loss did not improve from -0.55484. Patience: 13/50
2024-12-08 18:14:15.501550: train_loss -0.6761
2024-12-08 18:14:15.502422: val_loss -0.496
2024-12-08 18:14:15.503334: Pseudo dice [0.7442]
2024-12-08 18:14:15.504097: Epoch time: 87.73 s
2024-12-08 18:14:16.766121: 
2024-12-08 18:14:17.062371: Epoch 63
2024-12-08 18:14:17.063717: Current learning rate: 0.00943
2024-12-08 18:15:44.776589: Validation loss did not improve from -0.55484. Patience: 14/50
2024-12-08 18:15:44.777940: train_loss -0.6814
2024-12-08 18:15:44.778927: val_loss -0.5402
2024-12-08 18:15:44.779573: Pseudo dice [0.7749]
2024-12-08 18:15:44.780319: Epoch time: 88.01 s
2024-12-08 18:15:44.781021: Yayy! New best EMA pseudo Dice: 0.7599
2024-12-08 18:15:46.405467: 
2024-12-08 18:15:46.407215: Epoch 64
2024-12-08 18:15:46.408134: Current learning rate: 0.00942
2024-12-08 18:17:14.048938: Validation loss did not improve from -0.55484. Patience: 15/50
2024-12-08 18:17:14.050140: train_loss -0.6779
2024-12-08 18:17:14.051377: val_loss -0.514
2024-12-08 18:17:14.052389: Pseudo dice [0.758]
2024-12-08 18:17:14.053278: Epoch time: 87.65 s
2024-12-08 18:17:15.652442: 
2024-12-08 18:17:15.654250: Epoch 65
2024-12-08 18:17:15.655015: Current learning rate: 0.00941
2024-12-08 18:18:43.274800: Validation loss did not improve from -0.55484. Patience: 16/50
2024-12-08 18:18:43.275657: train_loss -0.6841
2024-12-08 18:18:43.276628: val_loss -0.5387
2024-12-08 18:18:43.277379: Pseudo dice [0.7672]
2024-12-08 18:18:43.278154: Epoch time: 87.62 s
2024-12-08 18:18:43.278964: Yayy! New best EMA pseudo Dice: 0.7605
2024-12-08 18:18:44.905059: 
2024-12-08 18:18:44.906726: Epoch 66
2024-12-08 18:18:44.907607: Current learning rate: 0.0094
2024-12-08 18:20:12.730134: Validation loss did not improve from -0.55484. Patience: 17/50
2024-12-08 18:20:12.731127: train_loss -0.6816
2024-12-08 18:20:12.732124: val_loss -0.4646
2024-12-08 18:20:12.733068: Pseudo dice [0.7285]
2024-12-08 18:20:12.734020: Epoch time: 87.83 s
2024-12-08 18:20:13.964773: 
2024-12-08 18:20:13.966169: Epoch 67
2024-12-08 18:20:13.966913: Current learning rate: 0.00939
2024-12-08 18:21:41.844640: Validation loss improved from -0.55484 to -0.55571! Patience: 17/50
2024-12-08 18:21:41.845898: train_loss -0.6749
2024-12-08 18:21:41.846859: val_loss -0.5557
2024-12-08 18:21:41.847702: Pseudo dice [0.7816]
2024-12-08 18:21:41.848681: Epoch time: 87.88 s
2024-12-08 18:21:43.129309: 
2024-12-08 18:21:43.130855: Epoch 68
2024-12-08 18:21:43.131659: Current learning rate: 0.00939
2024-12-08 18:23:10.942359: Validation loss did not improve from -0.55571. Patience: 1/50
2024-12-08 18:23:10.944154: train_loss -0.6751
2024-12-08 18:23:10.945304: val_loss -0.5236
2024-12-08 18:23:10.945942: Pseudo dice [0.7558]
2024-12-08 18:23:10.946655: Epoch time: 87.82 s
2024-12-08 18:23:12.215963: 
2024-12-08 18:23:12.217819: Epoch 69
2024-12-08 18:23:12.218617: Current learning rate: 0.00938
2024-12-08 18:24:40.010008: Validation loss did not improve from -0.55571. Patience: 2/50
2024-12-08 18:24:40.010908: train_loss -0.6776
2024-12-08 18:24:40.011893: val_loss -0.5144
2024-12-08 18:24:40.012558: Pseudo dice [0.7563]
2024-12-08 18:24:40.013319: Epoch time: 87.8 s
2024-12-08 18:24:41.590293: 
2024-12-08 18:24:41.591588: Epoch 70
2024-12-08 18:24:41.592395: Current learning rate: 0.00937
2024-12-08 18:26:09.361075: Validation loss did not improve from -0.55571. Patience: 3/50
2024-12-08 18:26:09.362009: train_loss -0.6768
2024-12-08 18:26:09.362870: val_loss -0.4608
2024-12-08 18:26:09.363611: Pseudo dice [0.7229]
2024-12-08 18:26:09.364419: Epoch time: 87.77 s
2024-12-08 18:26:10.698864: 
2024-12-08 18:26:10.700428: Epoch 71
2024-12-08 18:26:10.701263: Current learning rate: 0.00936
2024-12-08 18:27:38.436334: Validation loss did not improve from -0.55571. Patience: 4/50
2024-12-08 18:27:38.437314: train_loss -0.686
2024-12-08 18:27:38.438129: val_loss -0.5141
2024-12-08 18:27:38.438888: Pseudo dice [0.7599]
2024-12-08 18:27:38.439702: Epoch time: 87.74 s
2024-12-08 18:27:39.749155: 
2024-12-08 18:27:39.750634: Epoch 72
2024-12-08 18:27:39.751461: Current learning rate: 0.00935
2024-12-08 18:29:07.326840: Validation loss did not improve from -0.55571. Patience: 5/50
2024-12-08 18:29:07.328135: train_loss -0.6866
2024-12-08 18:29:07.329156: val_loss -0.4905
2024-12-08 18:29:07.329870: Pseudo dice [0.7438]
2024-12-08 18:29:07.330672: Epoch time: 87.58 s
2024-12-08 18:29:09.027662: 
2024-12-08 18:29:09.029144: Epoch 73
2024-12-08 18:29:09.029795: Current learning rate: 0.00934
2024-12-08 18:30:36.596405: Validation loss did not improve from -0.55571. Patience: 6/50
2024-12-08 18:30:36.597672: train_loss -0.6926
2024-12-08 18:30:36.599052: val_loss -0.5464
2024-12-08 18:30:36.599974: Pseudo dice [0.7758]
2024-12-08 18:30:36.600929: Epoch time: 87.57 s
2024-12-08 18:30:37.899165: 
2024-12-08 18:30:37.900714: Epoch 74
2024-12-08 18:30:37.901927: Current learning rate: 0.00933
2024-12-08 18:32:05.359959: Validation loss did not improve from -0.55571. Patience: 7/50
2024-12-08 18:32:05.361464: train_loss -0.6904
2024-12-08 18:32:05.362883: val_loss -0.5506
2024-12-08 18:32:05.363700: Pseudo dice [0.7684]
2024-12-08 18:32:05.364444: Epoch time: 87.46 s
2024-12-08 18:32:06.960269: 
2024-12-08 18:32:06.961917: Epoch 75
2024-12-08 18:32:06.962687: Current learning rate: 0.00932
2024-12-08 18:33:34.510128: Validation loss did not improve from -0.55571. Patience: 8/50
2024-12-08 18:33:34.511237: train_loss -0.6948
2024-12-08 18:33:34.512685: val_loss -0.4876
2024-12-08 18:33:34.513464: Pseudo dice [0.7505]
2024-12-08 18:33:34.514292: Epoch time: 87.55 s
2024-12-08 18:33:35.855183: 
2024-12-08 18:33:35.856317: Epoch 76
2024-12-08 18:33:35.857188: Current learning rate: 0.00931
2024-12-08 18:35:03.401206: Validation loss did not improve from -0.55571. Patience: 9/50
2024-12-08 18:35:03.402663: train_loss -0.6941
2024-12-08 18:35:03.403868: val_loss -0.5013
2024-12-08 18:35:03.404775: Pseudo dice [0.7471]
2024-12-08 18:35:03.405777: Epoch time: 87.55 s
2024-12-08 18:35:04.745116: 
2024-12-08 18:35:04.746869: Epoch 77
2024-12-08 18:35:04.747668: Current learning rate: 0.0093
2024-12-08 18:36:32.274744: Validation loss did not improve from -0.55571. Patience: 10/50
2024-12-08 18:36:32.275875: train_loss -0.6991
2024-12-08 18:36:32.276785: val_loss -0.5386
2024-12-08 18:36:32.277588: Pseudo dice [0.7705]
2024-12-08 18:36:32.278370: Epoch time: 87.53 s
2024-12-08 18:36:33.620632: 
2024-12-08 18:36:33.622220: Epoch 78
2024-12-08 18:36:33.623195: Current learning rate: 0.0093
2024-12-08 18:38:01.357887: Validation loss improved from -0.55571 to -0.56099! Patience: 10/50
2024-12-08 18:38:01.358972: train_loss -0.6988
2024-12-08 18:38:01.359859: val_loss -0.561
2024-12-08 18:38:01.360610: Pseudo dice [0.7839]
2024-12-08 18:38:01.361470: Epoch time: 87.74 s
2024-12-08 18:38:02.723414: 
2024-12-08 18:38:02.725315: Epoch 79
2024-12-08 18:38:02.726495: Current learning rate: 0.00929
2024-12-08 18:39:30.600124: Validation loss did not improve from -0.56099. Patience: 1/50
2024-12-08 18:39:30.601363: train_loss -0.7048
2024-12-08 18:39:30.602597: val_loss -0.5292
2024-12-08 18:39:30.603548: Pseudo dice [0.7609]
2024-12-08 18:39:30.604660: Epoch time: 87.88 s
2024-12-08 18:39:32.218194: 
2024-12-08 18:39:32.219637: Epoch 80
2024-12-08 18:39:32.220528: Current learning rate: 0.00928
2024-12-08 18:41:00.092881: Validation loss did not improve from -0.56099. Patience: 2/50
2024-12-08 18:41:00.094209: train_loss -0.6955
2024-12-08 18:41:00.095131: val_loss -0.4705
2024-12-08 18:41:00.095911: Pseudo dice [0.7408]
2024-12-08 18:41:00.096746: Epoch time: 87.88 s
2024-12-08 18:41:01.381195: 
2024-12-08 18:41:01.382665: Epoch 81
2024-12-08 18:41:01.383404: Current learning rate: 0.00927
2024-12-08 18:42:29.253068: Validation loss did not improve from -0.56099. Patience: 3/50
2024-12-08 18:42:29.253890: train_loss -0.7016
2024-12-08 18:42:29.254827: val_loss -0.522
2024-12-08 18:42:29.255466: Pseudo dice [0.7589]
2024-12-08 18:42:29.256165: Epoch time: 87.87 s
2024-12-08 18:42:30.496954: 
2024-12-08 18:42:30.498281: Epoch 82
2024-12-08 18:42:30.499053: Current learning rate: 0.00926
2024-12-08 18:43:58.392790: Validation loss improved from -0.56099 to -0.57175! Patience: 3/50
2024-12-08 18:43:58.394057: train_loss -0.6984
2024-12-08 18:43:58.395319: val_loss -0.5718
2024-12-08 18:43:58.396127: Pseudo dice [0.7838]
2024-12-08 18:43:58.397085: Epoch time: 87.9 s
2024-12-08 18:43:58.397918: Yayy! New best EMA pseudo Dice: 0.761
2024-12-08 18:43:59.969936: 
2024-12-08 18:43:59.971302: Epoch 83
2024-12-08 18:43:59.972231: Current learning rate: 0.00925
2024-12-08 18:45:27.817087: Validation loss did not improve from -0.57175. Patience: 1/50
2024-12-08 18:45:27.818087: train_loss -0.7033
2024-12-08 18:45:27.819291: val_loss -0.4366
2024-12-08 18:45:27.820059: Pseudo dice [0.7124]
2024-12-08 18:45:27.820826: Epoch time: 87.85 s
2024-12-08 18:45:29.391362: 
2024-12-08 18:45:29.393120: Epoch 84
2024-12-08 18:45:29.394099: Current learning rate: 0.00924
2024-12-08 18:46:57.224257: Validation loss did not improve from -0.57175. Patience: 2/50
2024-12-08 18:46:57.231908: train_loss -0.6989
2024-12-08 18:46:57.234040: val_loss -0.5165
2024-12-08 18:46:57.234921: Pseudo dice [0.756]
2024-12-08 18:46:57.235780: Epoch time: 87.84 s
2024-12-08 18:46:58.883563: 
2024-12-08 18:46:58.885323: Epoch 85
2024-12-08 18:46:58.886455: Current learning rate: 0.00923
2024-12-08 18:48:26.754524: Validation loss did not improve from -0.57175. Patience: 3/50
2024-12-08 18:48:26.755576: train_loss -0.7091
2024-12-08 18:48:26.756863: val_loss -0.5368
2024-12-08 18:48:26.757598: Pseudo dice [0.7699]
2024-12-08 18:48:26.758520: Epoch time: 87.87 s
2024-12-08 18:48:27.952441: 
2024-12-08 18:48:27.953880: Epoch 86
2024-12-08 18:48:27.954722: Current learning rate: 0.00922
2024-12-08 18:49:55.746535: Validation loss did not improve from -0.57175. Patience: 4/50
2024-12-08 18:49:55.747738: train_loss -0.7096
2024-12-08 18:49:55.748521: val_loss -0.4913
2024-12-08 18:49:55.749317: Pseudo dice [0.7494]
2024-12-08 18:49:55.750070: Epoch time: 87.8 s
2024-12-08 18:49:56.938072: 
2024-12-08 18:49:56.939497: Epoch 87
2024-12-08 18:49:56.940324: Current learning rate: 0.00921
2024-12-08 18:51:24.605245: Validation loss did not improve from -0.57175. Patience: 5/50
2024-12-08 18:51:24.606227: train_loss -0.7072
2024-12-08 18:51:24.607180: val_loss -0.5181
2024-12-08 18:51:24.608000: Pseudo dice [0.7631]
2024-12-08 18:51:24.608887: Epoch time: 87.67 s
2024-12-08 18:51:25.820940: 
2024-12-08 18:51:25.822421: Epoch 88
2024-12-08 18:51:25.823241: Current learning rate: 0.0092
2024-12-08 18:52:53.644894: Validation loss did not improve from -0.57175. Patience: 6/50
2024-12-08 18:52:53.645976: train_loss -0.7107
2024-12-08 18:52:53.646933: val_loss -0.5022
2024-12-08 18:52:53.647599: Pseudo dice [0.7552]
2024-12-08 18:52:53.648347: Epoch time: 87.83 s
2024-12-08 18:52:54.903125: 
2024-12-08 18:52:54.904788: Epoch 89
2024-12-08 18:52:54.905580: Current learning rate: 0.0092
2024-12-08 18:54:22.729674: Validation loss did not improve from -0.57175. Patience: 7/50
2024-12-08 18:54:22.730697: train_loss -0.7164
2024-12-08 18:54:22.731626: val_loss -0.4909
2024-12-08 18:54:22.732418: Pseudo dice [0.7487]
2024-12-08 18:54:22.733051: Epoch time: 87.83 s
2024-12-08 18:54:24.303642: 
2024-12-08 18:54:24.305456: Epoch 90
2024-12-08 18:54:24.306280: Current learning rate: 0.00919
2024-12-08 18:55:52.107854: Validation loss did not improve from -0.57175. Patience: 8/50
2024-12-08 18:55:52.109402: train_loss -0.7151
2024-12-08 18:55:52.110308: val_loss -0.5061
2024-12-08 18:55:52.111171: Pseudo dice [0.7582]
2024-12-08 18:55:52.112096: Epoch time: 87.81 s
2024-12-08 18:55:53.300396: 
2024-12-08 18:55:53.301944: Epoch 91
2024-12-08 18:55:53.302939: Current learning rate: 0.00918
2024-12-08 18:57:21.253143: Validation loss did not improve from -0.57175. Patience: 9/50
2024-12-08 18:57:21.254149: train_loss -0.7152
2024-12-08 18:57:21.255323: val_loss -0.5229
2024-12-08 18:57:21.256149: Pseudo dice [0.7698]
2024-12-08 18:57:21.256859: Epoch time: 87.96 s
2024-12-08 18:57:22.487608: 
2024-12-08 18:57:22.489137: Epoch 92
2024-12-08 18:57:22.489882: Current learning rate: 0.00917
2024-12-08 18:58:50.411604: Validation loss did not improve from -0.57175. Patience: 10/50
2024-12-08 18:58:50.412940: train_loss -0.7175
2024-12-08 18:58:50.414182: val_loss -0.5282
2024-12-08 18:58:50.415105: Pseudo dice [0.7708]
2024-12-08 18:58:50.415963: Epoch time: 87.93 s
2024-12-08 18:58:51.670590: 
2024-12-08 18:58:51.671952: Epoch 93
2024-12-08 18:58:51.672908: Current learning rate: 0.00916
2024-12-08 19:00:19.775050: Validation loss did not improve from -0.57175. Patience: 11/50
2024-12-08 19:00:19.775927: train_loss -0.7147
2024-12-08 19:00:19.776675: val_loss -0.553
2024-12-08 19:00:19.777436: Pseudo dice [0.7738]
2024-12-08 19:00:19.778326: Epoch time: 88.11 s
2024-12-08 19:00:20.949044: 
2024-12-08 19:00:20.950412: Epoch 94
2024-12-08 19:00:20.951251: Current learning rate: 0.00915
2024-12-08 19:01:49.050712: Validation loss did not improve from -0.57175. Patience: 12/50
2024-12-08 19:01:49.051908: train_loss -0.7192
2024-12-08 19:01:49.052771: val_loss -0.5214
2024-12-08 19:01:49.053555: Pseudo dice [0.7628]
2024-12-08 19:01:49.054407: Epoch time: 88.1 s
2024-12-08 19:01:50.961066: 
2024-12-08 19:01:50.962692: Epoch 95
2024-12-08 19:01:50.963617: Current learning rate: 0.00914
2024-12-08 19:03:18.916804: Validation loss did not improve from -0.57175. Patience: 13/50
2024-12-08 19:03:18.917951: train_loss -0.718
2024-12-08 19:03:18.919168: val_loss -0.5144
2024-12-08 19:03:18.919930: Pseudo dice [0.763]
2024-12-08 19:03:18.920772: Epoch time: 87.96 s
2024-12-08 19:03:18.921605: Yayy! New best EMA pseudo Dice: 0.761
2024-12-08 19:03:20.483032: 
2024-12-08 19:03:20.484689: Epoch 96
2024-12-08 19:03:20.485570: Current learning rate: 0.00913
2024-12-08 19:04:48.291547: Validation loss did not improve from -0.57175. Patience: 14/50
2024-12-08 19:04:48.292755: train_loss -0.7067
2024-12-08 19:04:48.293663: val_loss -0.5506
2024-12-08 19:04:48.294338: Pseudo dice [0.7803]
2024-12-08 19:04:48.295114: Epoch time: 87.81 s
2024-12-08 19:04:48.295848: Yayy! New best EMA pseudo Dice: 0.7629
2024-12-08 19:04:49.847443: 
2024-12-08 19:04:49.849116: Epoch 97
2024-12-08 19:04:49.849987: Current learning rate: 0.00912
2024-12-08 19:06:17.800915: Validation loss did not improve from -0.57175. Patience: 15/50
2024-12-08 19:06:17.802044: train_loss -0.7142
2024-12-08 19:06:17.803221: val_loss -0.4919
2024-12-08 19:06:17.804214: Pseudo dice [0.7406]
2024-12-08 19:06:17.805051: Epoch time: 87.96 s
2024-12-08 19:06:19.043515: 
2024-12-08 19:06:19.045325: Epoch 98
2024-12-08 19:06:19.046360: Current learning rate: 0.00911
2024-12-08 19:07:47.048950: Validation loss did not improve from -0.57175. Patience: 16/50
2024-12-08 19:07:47.050126: train_loss -0.7157
2024-12-08 19:07:47.051085: val_loss -0.5244
2024-12-08 19:07:47.051893: Pseudo dice [0.7699]
2024-12-08 19:07:47.052552: Epoch time: 88.01 s
2024-12-08 19:07:48.277414: 
2024-12-08 19:07:48.279220: Epoch 99
2024-12-08 19:07:48.279995: Current learning rate: 0.0091
2024-12-08 19:09:16.083523: Validation loss did not improve from -0.57175. Patience: 17/50
2024-12-08 19:09:16.084662: train_loss -0.7218
2024-12-08 19:09:16.085848: val_loss -0.4506
2024-12-08 19:09:16.087043: Pseudo dice [0.7224]
2024-12-08 19:09:16.087993: Epoch time: 87.81 s
2024-12-08 19:09:17.641642: 
2024-12-08 19:09:17.643241: Epoch 100
2024-12-08 19:09:17.643996: Current learning rate: 0.0091
2024-12-08 19:10:45.604690: Validation loss did not improve from -0.57175. Patience: 18/50
2024-12-08 19:10:45.605803: train_loss -0.7204
2024-12-08 19:10:45.606555: val_loss -0.4947
2024-12-08 19:10:45.607258: Pseudo dice [0.7588]
2024-12-08 19:10:45.607917: Epoch time: 87.97 s
2024-12-08 19:10:46.825204: 
2024-12-08 19:10:46.826691: Epoch 101
2024-12-08 19:10:46.827441: Current learning rate: 0.00909
2024-12-08 19:12:14.677921: Validation loss did not improve from -0.57175. Patience: 19/50
2024-12-08 19:12:14.679179: train_loss -0.7201
2024-12-08 19:12:14.680094: val_loss -0.4782
2024-12-08 19:12:14.680816: Pseudo dice [0.7414]
2024-12-08 19:12:14.681505: Epoch time: 87.86 s
2024-12-08 19:12:15.889096: 
2024-12-08 19:12:15.890389: Epoch 102
2024-12-08 19:12:15.891304: Current learning rate: 0.00908
2024-12-08 19:13:43.773987: Validation loss did not improve from -0.57175. Patience: 20/50
2024-12-08 19:13:43.774840: train_loss -0.7245
2024-12-08 19:13:43.775708: val_loss -0.4994
2024-12-08 19:13:43.776674: Pseudo dice [0.762]
2024-12-08 19:13:43.777603: Epoch time: 87.89 s
2024-12-08 19:13:45.045596: 
2024-12-08 19:13:45.047364: Epoch 103
2024-12-08 19:13:45.048203: Current learning rate: 0.00907
2024-12-08 19:15:13.177925: Validation loss did not improve from -0.57175. Patience: 21/50
2024-12-08 19:15:13.179009: train_loss -0.7315
2024-12-08 19:15:13.179975: val_loss -0.4985
2024-12-08 19:15:13.180709: Pseudo dice [0.7619]
2024-12-08 19:15:13.181401: Epoch time: 88.13 s
2024-12-08 19:15:14.393878: 
2024-12-08 19:15:14.395307: Epoch 104
2024-12-08 19:15:14.396166: Current learning rate: 0.00906
2024-12-08 19:16:42.520190: Validation loss did not improve from -0.57175. Patience: 22/50
2024-12-08 19:16:42.521415: train_loss -0.7251
2024-12-08 19:16:42.522290: val_loss -0.4956
2024-12-08 19:16:42.523094: Pseudo dice [0.7641]
2024-12-08 19:16:42.523962: Epoch time: 88.13 s
2024-12-08 19:16:44.068795: 
2024-12-08 19:16:44.070164: Epoch 105
2024-12-08 19:16:44.070886: Current learning rate: 0.00905
2024-12-08 19:18:12.079662: Validation loss did not improve from -0.57175. Patience: 23/50
2024-12-08 19:18:12.080864: train_loss -0.7236
2024-12-08 19:18:12.081662: val_loss -0.4842
2024-12-08 19:18:12.082358: Pseudo dice [0.747]
2024-12-08 19:18:12.083205: Epoch time: 88.01 s
2024-12-08 19:18:13.281965: 
2024-12-08 19:18:13.283375: Epoch 106
2024-12-08 19:18:13.284186: Current learning rate: 0.00904
2024-12-08 19:19:41.503406: Validation loss did not improve from -0.57175. Patience: 24/50
2024-12-08 19:19:41.504646: train_loss -0.7326
2024-12-08 19:19:41.505632: val_loss -0.4934
2024-12-08 19:19:41.506260: Pseudo dice [0.7515]
2024-12-08 19:19:41.506970: Epoch time: 88.22 s
2024-12-08 19:19:42.725128: 
2024-12-08 19:19:42.726673: Epoch 107
2024-12-08 19:19:42.727409: Current learning rate: 0.00903
2024-12-08 19:21:10.624484: Validation loss did not improve from -0.57175. Patience: 25/50
2024-12-08 19:21:10.625677: train_loss -0.7244
2024-12-08 19:21:10.626722: val_loss -0.5089
2024-12-08 19:21:10.627655: Pseudo dice [0.7624]
2024-12-08 19:21:10.628347: Epoch time: 87.9 s
2024-12-08 19:21:11.832469: 
2024-12-08 19:21:11.834223: Epoch 108
2024-12-08 19:21:11.834987: Current learning rate: 0.00902
2024-12-08 19:22:39.577803: Validation loss did not improve from -0.57175. Patience: 26/50
2024-12-08 19:22:39.578829: train_loss -0.7305
2024-12-08 19:22:39.579713: val_loss -0.4011
2024-12-08 19:22:39.580439: Pseudo dice [0.7104]
2024-12-08 19:22:39.581246: Epoch time: 87.75 s
2024-12-08 19:22:40.784736: 
2024-12-08 19:22:40.786565: Epoch 109
2024-12-08 19:22:40.787502: Current learning rate: 0.00901
2024-12-08 19:24:08.572710: Validation loss did not improve from -0.57175. Patience: 27/50
2024-12-08 19:24:08.573916: train_loss -0.7216
2024-12-08 19:24:08.574732: val_loss -0.4723
2024-12-08 19:24:08.575525: Pseudo dice [0.741]
2024-12-08 19:24:08.576308: Epoch time: 87.79 s
2024-12-08 19:24:10.213036: 
2024-12-08 19:24:10.214329: Epoch 110
2024-12-08 19:24:10.215235: Current learning rate: 0.009
2024-12-08 19:25:37.900490: Validation loss did not improve from -0.57175. Patience: 28/50
2024-12-08 19:25:37.901802: train_loss -0.7265
2024-12-08 19:25:37.902828: val_loss -0.5094
2024-12-08 19:25:37.903486: Pseudo dice [0.7622]
2024-12-08 19:25:37.904165: Epoch time: 87.69 s
2024-12-08 19:25:39.160287: 
2024-12-08 19:25:39.161863: Epoch 111
2024-12-08 19:25:39.162565: Current learning rate: 0.009
2024-12-08 19:27:06.790204: Validation loss did not improve from -0.57175. Patience: 29/50
2024-12-08 19:27:06.790962: train_loss -0.7328
2024-12-08 19:27:06.791816: val_loss -0.5048
2024-12-08 19:27:06.792646: Pseudo dice [0.7587]
2024-12-08 19:27:06.793453: Epoch time: 87.63 s
2024-12-08 19:27:08.045679: 
2024-12-08 19:27:08.046659: Epoch 112
2024-12-08 19:27:08.047337: Current learning rate: 0.00899
2024-12-08 19:28:35.775837: Validation loss did not improve from -0.57175. Patience: 30/50
2024-12-08 19:28:35.777027: train_loss -0.7348
2024-12-08 19:28:35.778325: val_loss -0.5104
2024-12-08 19:28:35.779062: Pseudo dice [0.7528]
2024-12-08 19:28:35.779825: Epoch time: 87.73 s
2024-12-08 19:28:36.982677: 
2024-12-08 19:28:36.983939: Epoch 113
2024-12-08 19:28:36.984684: Current learning rate: 0.00898
2024-12-08 19:30:04.848124: Validation loss did not improve from -0.57175. Patience: 31/50
2024-12-08 19:30:04.849418: train_loss -0.7362
2024-12-08 19:30:04.850223: val_loss -0.4773
2024-12-08 19:30:04.851124: Pseudo dice [0.7412]
2024-12-08 19:30:04.852010: Epoch time: 87.87 s
2024-12-08 19:30:06.066421: 
2024-12-08 19:30:06.067828: Epoch 114
2024-12-08 19:30:06.068514: Current learning rate: 0.00897
2024-12-08 19:31:34.032547: Validation loss did not improve from -0.57175. Patience: 32/50
2024-12-08 19:31:34.033865: train_loss -0.7393
2024-12-08 19:31:34.034983: val_loss -0.5075
2024-12-08 19:31:34.035871: Pseudo dice [0.7617]
2024-12-08 19:31:34.036635: Epoch time: 87.97 s
2024-12-08 19:31:35.664984: 
2024-12-08 19:31:35.667048: Epoch 115
2024-12-08 19:31:35.668023: Current learning rate: 0.00896
2024-12-08 19:33:03.562325: Validation loss did not improve from -0.57175. Patience: 33/50
2024-12-08 19:33:03.563602: train_loss -0.7398
2024-12-08 19:33:03.564627: val_loss -0.5505
2024-12-08 19:33:03.565358: Pseudo dice [0.7832]
2024-12-08 19:33:03.566064: Epoch time: 87.9 s
2024-12-08 19:33:04.793945: 
2024-12-08 19:33:04.795414: Epoch 116
2024-12-08 19:33:04.796249: Current learning rate: 0.00895
2024-12-08 19:34:32.669067: Validation loss did not improve from -0.57175. Patience: 34/50
2024-12-08 19:34:32.670089: train_loss -0.7409
2024-12-08 19:34:32.671085: val_loss -0.4834
2024-12-08 19:34:32.671924: Pseudo dice [0.7461]
2024-12-08 19:34:32.672703: Epoch time: 87.88 s
2024-12-08 19:34:33.943256: 
2024-12-08 19:34:33.944965: Epoch 117
2024-12-08 19:34:33.945810: Current learning rate: 0.00894
2024-12-08 19:36:01.946260: Validation loss did not improve from -0.57175. Patience: 35/50
2024-12-08 19:36:01.947428: train_loss -0.7344
2024-12-08 19:36:01.948307: val_loss -0.5527
2024-12-08 19:36:01.949195: Pseudo dice [0.7825]
2024-12-08 19:36:01.950013: Epoch time: 88.01 s
2024-12-08 19:36:03.525083: 
2024-12-08 19:36:03.526814: Epoch 118
2024-12-08 19:36:03.527584: Current learning rate: 0.00893
2024-12-08 19:37:31.335065: Validation loss did not improve from -0.57175. Patience: 36/50
2024-12-08 19:37:31.336474: train_loss -0.738
2024-12-08 19:37:31.337710: val_loss -0.5198
2024-12-08 19:37:31.338710: Pseudo dice [0.7615]
2024-12-08 19:37:31.339636: Epoch time: 87.81 s
2024-12-08 19:37:32.578388: 
2024-12-08 19:37:32.579748: Epoch 119
2024-12-08 19:37:32.580610: Current learning rate: 0.00892
2024-12-08 19:39:00.445558: Validation loss did not improve from -0.57175. Patience: 37/50
2024-12-08 19:39:00.446988: train_loss -0.7427
2024-12-08 19:39:00.447935: val_loss -0.5103
2024-12-08 19:39:00.448676: Pseudo dice [0.7627]
2024-12-08 19:39:00.449410: Epoch time: 87.87 s
2024-12-08 19:39:02.062272: 
2024-12-08 19:39:02.063832: Epoch 120
2024-12-08 19:39:02.064557: Current learning rate: 0.00891
2024-12-08 19:40:30.007543: Validation loss did not improve from -0.57175. Patience: 38/50
2024-12-08 19:40:30.008852: train_loss -0.7422
2024-12-08 19:40:30.009734: val_loss -0.5092
2024-12-08 19:40:30.010398: Pseudo dice [0.7614]
2024-12-08 19:40:30.011143: Epoch time: 87.95 s
2024-12-08 19:40:31.282995: 
2024-12-08 19:40:31.284569: Epoch 121
2024-12-08 19:40:31.285281: Current learning rate: 0.0089
2024-12-08 19:41:59.338211: Validation loss did not improve from -0.57175. Patience: 39/50
2024-12-08 19:41:59.339325: train_loss -0.7399
2024-12-08 19:41:59.340178: val_loss -0.5157
2024-12-08 19:41:59.341154: Pseudo dice [0.761]
2024-12-08 19:41:59.341985: Epoch time: 88.06 s
2024-12-08 19:42:00.570927: 
2024-12-08 19:42:00.572685: Epoch 122
2024-12-08 19:42:00.573509: Current learning rate: 0.00889
2024-12-08 19:43:28.714766: Validation loss did not improve from -0.57175. Patience: 40/50
2024-12-08 19:43:28.716014: train_loss -0.7413
2024-12-08 19:43:28.717046: val_loss -0.5256
2024-12-08 19:43:28.717893: Pseudo dice [0.757]
2024-12-08 19:43:28.718616: Epoch time: 88.15 s
2024-12-08 19:43:29.947417: 
2024-12-08 19:43:29.949245: Epoch 123
2024-12-08 19:43:29.950175: Current learning rate: 0.00889
2024-12-08 19:44:57.909744: Validation loss did not improve from -0.57175. Patience: 41/50
2024-12-08 19:44:57.910510: train_loss -0.7385
2024-12-08 19:44:57.911657: val_loss -0.4955
2024-12-08 19:44:57.912668: Pseudo dice [0.7577]
2024-12-08 19:44:57.913516: Epoch time: 87.96 s
2024-12-08 19:44:59.134293: 
2024-12-08 19:44:59.135907: Epoch 124
2024-12-08 19:44:59.136886: Current learning rate: 0.00888
2024-12-08 19:46:26.956168: Validation loss did not improve from -0.57175. Patience: 42/50
2024-12-08 19:46:26.957531: train_loss -0.7422
2024-12-08 19:46:26.958706: val_loss -0.5111
2024-12-08 19:46:26.959620: Pseudo dice [0.7678]
2024-12-08 19:46:26.960664: Epoch time: 87.82 s
2024-12-08 19:46:28.664958: 
2024-12-08 19:46:28.666660: Epoch 125
2024-12-08 19:46:28.667764: Current learning rate: 0.00887
2024-12-08 19:47:56.418967: Validation loss did not improve from -0.57175. Patience: 43/50
2024-12-08 19:47:56.419948: train_loss -0.7431
2024-12-08 19:47:56.420776: val_loss -0.4878
2024-12-08 19:47:56.421582: Pseudo dice [0.7494]
2024-12-08 19:47:56.422403: Epoch time: 87.76 s
2024-12-08 19:47:57.705631: 
2024-12-08 19:47:57.707701: Epoch 126
2024-12-08 19:47:57.708652: Current learning rate: 0.00886
2024-12-08 19:49:25.510346: Validation loss did not improve from -0.57175. Patience: 44/50
2024-12-08 19:49:25.511179: train_loss -0.7383
2024-12-08 19:49:25.512190: val_loss -0.4915
2024-12-08 19:49:25.513048: Pseudo dice [0.7531]
2024-12-08 19:49:25.513883: Epoch time: 87.81 s
2024-12-08 19:49:26.749743: 
2024-12-08 19:49:26.751303: Epoch 127
2024-12-08 19:49:26.752029: Current learning rate: 0.00885
2024-12-08 19:50:54.334169: Validation loss did not improve from -0.57175. Patience: 45/50
2024-12-08 19:50:54.335126: train_loss -0.7412
2024-12-08 19:50:54.335936: val_loss -0.4976
2024-12-08 19:50:54.336735: Pseudo dice [0.7513]
2024-12-08 19:50:54.337789: Epoch time: 87.59 s
2024-12-08 19:50:55.618534: 
2024-12-08 19:50:55.619841: Epoch 128
2024-12-08 19:50:55.620645: Current learning rate: 0.00884
2024-12-08 19:52:24.315264: Validation loss did not improve from -0.57175. Patience: 46/50
2024-12-08 19:52:24.320608: train_loss -0.741
2024-12-08 19:52:24.322887: val_loss -0.4833
2024-12-08 19:52:24.323830: Pseudo dice [0.7521]
2024-12-08 19:52:24.325544: Epoch time: 88.7 s
2024-12-08 19:52:26.108237: 
2024-12-08 19:52:26.109779: Epoch 129
2024-12-08 19:52:26.110955: Current learning rate: 0.00883
2024-12-08 19:53:53.758534: Validation loss did not improve from -0.57175. Patience: 47/50
2024-12-08 19:53:53.759492: train_loss -0.7404
2024-12-08 19:53:53.760319: val_loss -0.494
2024-12-08 19:53:53.761255: Pseudo dice [0.7555]
2024-12-08 19:53:53.762040: Epoch time: 87.65 s
2024-12-08 19:53:55.441668: 
2024-12-08 19:53:55.443406: Epoch 130
2024-12-08 19:53:55.444378: Current learning rate: 0.00882
2024-12-08 19:55:23.139126: Validation loss did not improve from -0.57175. Patience: 48/50
2024-12-08 19:55:23.140099: train_loss -0.7435
2024-12-08 19:55:23.140879: val_loss -0.4452
2024-12-08 19:55:23.141744: Pseudo dice [0.7317]
2024-12-08 19:55:23.142432: Epoch time: 87.7 s
2024-12-08 19:55:24.392306: 
2024-12-08 19:55:24.394067: Epoch 131
2024-12-08 19:55:24.395054: Current learning rate: 0.00881
2024-12-08 19:56:52.366221: Validation loss did not improve from -0.57175. Patience: 49/50
2024-12-08 19:56:52.367349: train_loss -0.7314
2024-12-08 19:56:52.368321: val_loss -0.5471
2024-12-08 19:56:52.369097: Pseudo dice [0.787]
2024-12-08 19:56:52.369809: Epoch time: 87.98 s
2024-12-08 19:56:53.653440: 
2024-12-08 19:56:53.655461: Epoch 132
2024-12-08 19:56:53.656346: Current learning rate: 0.0088
2024-12-08 19:58:21.472625: Validation loss did not improve from -0.57175. Patience: 50/50
2024-12-08 19:58:21.473619: train_loss -0.7342
2024-12-08 19:58:21.474391: val_loss -0.5414
2024-12-08 19:58:21.475167: Pseudo dice [0.7764]
2024-12-08 19:58:21.475879: Epoch time: 87.82 s
2024-12-08 19:58:22.703452: Patience reached. Stopping training.
2024-12-08 19:58:23.245281: Training done.
2024-12-08 19:58:23.469491: Using splits from existing split file: /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset307_Sohee_Calcium_OCT_CrossValidation/splits_final.json
2024-12-08 19:58:23.484336: The split file contains 5 splits.
2024-12-08 19:58:23.485343: Desired fold for training: 0
2024-12-08 19:58:23.486125: This split has 6 training and 2 validation cases.
2024-12-08 19:58:23.487119: predicting 106-002
2024-12-08 19:58:23.499622: 106-002, shape torch.Size([1, 539, 498, 498]), rank 0
2024-12-08 20:00:44.562797: predicting 706-005
2024-12-08 20:00:44.596374: 706-005, shape torch.Size([1, 375, 498, 498]), rank 0
2024-12-08 20:02:35.274827: Validation complete
2024-12-08 20:02:35.275444: Mean Validation Dice:  0.7751648846759343
2024-12-08 16:39:41.268828: unpacking done...
2024-12-08 16:39:41.278437: Unable to plot network architecture: nnUNet_compile is enabled!
2024-12-08 16:39:41.319046: 
2024-12-08 16:39:41.320298: Epoch 0
2024-12-08 16:39:41.321487: Current learning rate: 0.01
2024-12-08 16:42:11.979432: Validation loss improved from 1000.00000 to -0.17409! Patience: 0/50
2024-12-08 16:42:11.981015: train_loss -0.1374
2024-12-08 16:42:11.982186: val_loss -0.1741
2024-12-08 16:42:11.982933: Pseudo dice [0.5186]
2024-12-08 16:42:11.983999: Epoch time: 150.66 s
2024-12-08 16:42:11.985169: Yayy! New best EMA pseudo Dice: 0.5186
2024-12-08 16:42:13.435062: 
2024-12-08 16:42:13.436869: Epoch 1
2024-12-08 16:42:13.437829: Current learning rate: 0.00999
2024-12-08 16:43:41.801476: Validation loss improved from -0.17409 to -0.22391! Patience: 0/50
2024-12-08 16:43:41.802636: train_loss -0.2897
2024-12-08 16:43:41.803961: val_loss -0.2239
2024-12-08 16:43:41.804880: Pseudo dice [0.5568]
2024-12-08 16:43:41.805746: Epoch time: 88.37 s
2024-12-08 16:43:41.806642: Yayy! New best EMA pseudo Dice: 0.5224
2024-12-08 16:43:43.421242: 
2024-12-08 16:43:43.422636: Epoch 2
2024-12-08 16:43:43.423387: Current learning rate: 0.00998
2024-12-08 16:45:12.437068: Validation loss improved from -0.22391 to -0.25560! Patience: 0/50
2024-12-08 16:45:12.438138: train_loss -0.3475
2024-12-08 16:45:12.439240: val_loss -0.2556
2024-12-08 16:45:12.440140: Pseudo dice [0.5772]
2024-12-08 16:45:12.441031: Epoch time: 89.02 s
2024-12-08 16:45:12.441895: Yayy! New best EMA pseudo Dice: 0.5279
2024-12-08 16:45:14.095578: 
2024-12-08 16:45:14.097006: Epoch 3
2024-12-08 16:45:14.097738: Current learning rate: 0.00997
2024-12-08 16:46:43.430148: Validation loss improved from -0.25560 to -0.34946! Patience: 0/50
2024-12-08 16:46:43.431237: train_loss -0.3681
2024-12-08 16:46:43.432156: val_loss -0.3495
2024-12-08 16:46:43.433022: Pseudo dice [0.6192]
2024-12-08 16:46:43.433755: Epoch time: 89.34 s
2024-12-08 16:46:43.434675: Yayy! New best EMA pseudo Dice: 0.537
2024-12-08 16:46:45.059979: 
2024-12-08 16:46:45.061416: Epoch 4
2024-12-08 16:46:45.062279: Current learning rate: 0.00996
2024-12-08 16:48:12.419598: Validation loss improved from -0.34946 to -0.36653! Patience: 0/50
2024-12-08 16:48:12.420603: train_loss -0.4113
2024-12-08 16:48:12.421605: val_loss -0.3665
2024-12-08 16:48:12.422258: Pseudo dice [0.6393]
2024-12-08 16:48:12.423004: Epoch time: 87.36 s
2024-12-08 16:48:12.716858: Yayy! New best EMA pseudo Dice: 0.5472
2024-12-08 16:48:14.335194: 
2024-12-08 16:48:14.337271: Epoch 5
2024-12-08 16:48:14.338299: Current learning rate: 0.00995
2024-12-08 16:49:41.572482: Validation loss improved from -0.36653 to -0.40772! Patience: 0/50
2024-12-08 16:49:41.573688: train_loss -0.422
2024-12-08 16:49:41.574674: val_loss -0.4077
2024-12-08 16:49:41.575486: Pseudo dice [0.6636]
2024-12-08 16:49:41.576294: Epoch time: 87.24 s
2024-12-08 16:49:41.577062: Yayy! New best EMA pseudo Dice: 0.5589
2024-12-08 16:49:43.116812: 
2024-12-08 16:49:43.118494: Epoch 6
2024-12-08 16:49:43.119482: Current learning rate: 0.00995
2024-12-08 16:51:10.460814: Validation loss improved from -0.40772 to -0.41599! Patience: 0/50
2024-12-08 16:51:10.461863: train_loss -0.4429
2024-12-08 16:51:10.463365: val_loss -0.416
2024-12-08 16:51:10.464320: Pseudo dice [0.6734]
2024-12-08 16:51:10.465087: Epoch time: 87.35 s
2024-12-08 16:51:10.466035: Yayy! New best EMA pseudo Dice: 0.5703
2024-12-08 16:51:12.079652: 
2024-12-08 16:51:12.081051: Epoch 7
2024-12-08 16:51:12.081899: Current learning rate: 0.00994
2024-12-08 16:52:39.262419: Validation loss did not improve from -0.41599. Patience: 1/50
2024-12-08 16:52:39.263524: train_loss -0.4634
2024-12-08 16:52:39.264415: val_loss -0.4101
2024-12-08 16:52:39.265179: Pseudo dice [0.6566]
2024-12-08 16:52:39.265906: Epoch time: 87.18 s
2024-12-08 16:52:39.266586: Yayy! New best EMA pseudo Dice: 0.5789
2024-12-08 16:52:40.863973: 
2024-12-08 16:52:40.864972: Epoch 8
2024-12-08 16:52:40.865705: Current learning rate: 0.00993
2024-12-08 16:54:08.521908: Validation loss did not improve from -0.41599. Patience: 2/50
2024-12-08 16:54:08.523003: train_loss -0.4723
2024-12-08 16:54:08.523873: val_loss -0.3815
2024-12-08 16:54:08.524755: Pseudo dice [0.6311]
2024-12-08 16:54:08.525543: Epoch time: 87.66 s
2024-12-08 16:54:08.526310: Yayy! New best EMA pseudo Dice: 0.5842
2024-12-08 16:54:10.742367: 
2024-12-08 16:54:10.744016: Epoch 9
2024-12-08 16:54:10.744955: Current learning rate: 0.00992
2024-12-08 16:55:39.990316: Validation loss improved from -0.41599 to -0.42453! Patience: 2/50
2024-12-08 16:55:39.991455: train_loss -0.4778
2024-12-08 16:55:39.992470: val_loss -0.4245
2024-12-08 16:55:39.993398: Pseudo dice [0.6645]
2024-12-08 16:55:39.994152: Epoch time: 89.25 s
2024-12-08 16:55:40.322634: Yayy! New best EMA pseudo Dice: 0.5922
2024-12-08 16:55:41.874334: 
2024-12-08 16:55:41.875946: Epoch 10
2024-12-08 16:55:41.876715: Current learning rate: 0.00991
2024-12-08 16:57:11.166275: Validation loss improved from -0.42453 to -0.45072! Patience: 0/50
2024-12-08 16:57:11.167560: train_loss -0.5092
2024-12-08 16:57:11.168532: val_loss -0.4507
2024-12-08 16:57:11.169505: Pseudo dice [0.6811]
2024-12-08 16:57:11.170386: Epoch time: 89.29 s
2024-12-08 16:57:11.171067: Yayy! New best EMA pseudo Dice: 0.6011
2024-12-08 16:57:12.713402: 
2024-12-08 16:57:12.714426: Epoch 11
2024-12-08 16:57:12.715239: Current learning rate: 0.0099
2024-12-08 16:58:42.060788: Validation loss did not improve from -0.45072. Patience: 1/50
2024-12-08 16:58:42.061657: train_loss -0.5153
2024-12-08 16:58:42.062572: val_loss -0.3946
2024-12-08 16:58:42.063277: Pseudo dice [0.6615]
2024-12-08 16:58:42.063942: Epoch time: 89.35 s
2024-12-08 16:58:42.064543: Yayy! New best EMA pseudo Dice: 0.6071
2024-12-08 16:58:43.621899: 
2024-12-08 16:58:43.623508: Epoch 12
2024-12-08 16:58:43.624279: Current learning rate: 0.00989
2024-12-08 17:00:12.897317: Validation loss did not improve from -0.45072. Patience: 2/50
2024-12-08 17:00:12.898309: train_loss -0.5143
2024-12-08 17:00:12.899172: val_loss -0.4376
2024-12-08 17:00:12.900017: Pseudo dice [0.6783]
2024-12-08 17:00:12.900921: Epoch time: 89.28 s
2024-12-08 17:00:12.901603: Yayy! New best EMA pseudo Dice: 0.6142
2024-12-08 17:00:14.516744: 
2024-12-08 17:00:14.518338: Epoch 13
2024-12-08 17:00:14.519032: Current learning rate: 0.00988
2024-12-08 17:01:43.836253: Validation loss improved from -0.45072 to -0.46581! Patience: 2/50
2024-12-08 17:01:43.837539: train_loss -0.5253
2024-12-08 17:01:43.838477: val_loss -0.4658
2024-12-08 17:01:43.839310: Pseudo dice [0.6967]
2024-12-08 17:01:43.840072: Epoch time: 89.32 s
2024-12-08 17:01:43.840882: Yayy! New best EMA pseudo Dice: 0.6225
2024-12-08 17:01:45.477407: 
2024-12-08 17:01:45.478868: Epoch 14
2024-12-08 17:01:45.479670: Current learning rate: 0.00987
2024-12-08 17:03:14.796989: Validation loss did not improve from -0.46581. Patience: 1/50
2024-12-08 17:03:14.798115: train_loss -0.5271
2024-12-08 17:03:14.799263: val_loss -0.4428
2024-12-08 17:03:14.800197: Pseudo dice [0.6832]
2024-12-08 17:03:14.801249: Epoch time: 89.32 s
2024-12-08 17:03:15.132958: Yayy! New best EMA pseudo Dice: 0.6286
2024-12-08 17:03:16.715678: 
2024-12-08 17:03:16.717334: Epoch 15
2024-12-08 17:03:16.718136: Current learning rate: 0.00986
2024-12-08 17:04:45.877486: Validation loss did not improve from -0.46581. Patience: 2/50
2024-12-08 17:04:45.878720: train_loss -0.5481
2024-12-08 17:04:45.879790: val_loss -0.4594
2024-12-08 17:04:45.880710: Pseudo dice [0.686]
2024-12-08 17:04:45.881557: Epoch time: 89.16 s
2024-12-08 17:04:45.882363: Yayy! New best EMA pseudo Dice: 0.6343
2024-12-08 17:04:47.481792: 
2024-12-08 17:04:47.483692: Epoch 16
2024-12-08 17:04:47.484560: Current learning rate: 0.00986
2024-12-08 17:06:17.017333: Validation loss did not improve from -0.46581. Patience: 3/50
2024-12-08 17:06:17.018637: train_loss -0.5418
2024-12-08 17:06:17.019607: val_loss -0.4229
2024-12-08 17:06:17.020406: Pseudo dice [0.6608]
2024-12-08 17:06:17.021209: Epoch time: 89.54 s
2024-12-08 17:06:17.022025: Yayy! New best EMA pseudo Dice: 0.637
2024-12-08 17:06:18.630479: 
2024-12-08 17:06:18.632165: Epoch 17
2024-12-08 17:06:18.633065: Current learning rate: 0.00985
2024-12-08 17:07:48.205858: Validation loss did not improve from -0.46581. Patience: 4/50
2024-12-08 17:07:48.207091: train_loss -0.5378
2024-12-08 17:07:48.208113: val_loss -0.4514
2024-12-08 17:07:48.208960: Pseudo dice [0.6784]
2024-12-08 17:07:48.209676: Epoch time: 89.58 s
2024-12-08 17:07:48.210413: Yayy! New best EMA pseudo Dice: 0.6411
2024-12-08 17:07:49.840156: 
2024-12-08 17:07:49.841623: Epoch 18
2024-12-08 17:07:49.842403: Current learning rate: 0.00984
2024-12-08 17:09:19.510979: Validation loss improved from -0.46581 to -0.46712! Patience: 4/50
2024-12-08 17:09:19.511801: train_loss -0.5524
2024-12-08 17:09:19.512579: val_loss -0.4671
2024-12-08 17:09:19.513342: Pseudo dice [0.6923]
2024-12-08 17:09:19.514025: Epoch time: 89.67 s
2024-12-08 17:09:19.514735: Yayy! New best EMA pseudo Dice: 0.6462
2024-12-08 17:09:21.135791: 
2024-12-08 17:09:21.137291: Epoch 19
2024-12-08 17:09:21.138104: Current learning rate: 0.00983
2024-12-08 17:10:50.670935: Validation loss improved from -0.46712 to -0.47099! Patience: 0/50
2024-12-08 17:10:50.672074: train_loss -0.5633
2024-12-08 17:10:50.672963: val_loss -0.471
2024-12-08 17:10:50.673775: Pseudo dice [0.699]
2024-12-08 17:10:50.674565: Epoch time: 89.54 s
2024-12-08 17:10:51.325241: Yayy! New best EMA pseudo Dice: 0.6515
2024-12-08 17:10:52.913103: 
2024-12-08 17:10:52.914556: Epoch 20
2024-12-08 17:10:52.915276: Current learning rate: 0.00982
2024-12-08 17:12:20.631585: Validation loss improved from -0.47099 to -0.50187! Patience: 0/50
2024-12-08 17:12:20.632700: train_loss -0.5656
2024-12-08 17:12:20.633716: val_loss -0.5019
2024-12-08 17:12:20.634509: Pseudo dice [0.7132]
2024-12-08 17:12:20.635254: Epoch time: 87.72 s
2024-12-08 17:12:20.635959: Yayy! New best EMA pseudo Dice: 0.6577
2024-12-08 17:12:22.262897: 
2024-12-08 17:12:22.264064: Epoch 21
2024-12-08 17:12:22.264785: Current learning rate: 0.00981
2024-12-08 17:13:49.935694: Validation loss did not improve from -0.50187. Patience: 1/50
2024-12-08 17:13:49.936437: train_loss -0.5679
2024-12-08 17:13:49.937290: val_loss -0.4821
2024-12-08 17:13:49.938268: Pseudo dice [0.7062]
2024-12-08 17:13:49.939119: Epoch time: 87.67 s
2024-12-08 17:13:49.939863: Yayy! New best EMA pseudo Dice: 0.6625
2024-12-08 17:13:51.470516: 
2024-12-08 17:13:51.472178: Epoch 22
2024-12-08 17:13:51.473078: Current learning rate: 0.0098
2024-12-08 17:15:18.892667: Validation loss did not improve from -0.50187. Patience: 2/50
2024-12-08 17:15:18.893886: train_loss -0.5793
2024-12-08 17:15:18.894743: val_loss -0.5006
2024-12-08 17:15:18.895460: Pseudo dice [0.716]
2024-12-08 17:15:18.896115: Epoch time: 87.42 s
2024-12-08 17:15:18.896705: Yayy! New best EMA pseudo Dice: 0.6679
2024-12-08 17:15:20.452650: 
2024-12-08 17:15:20.454289: Epoch 23
2024-12-08 17:15:20.455147: Current learning rate: 0.00979
2024-12-08 17:16:47.891191: Validation loss did not improve from -0.50187. Patience: 3/50
2024-12-08 17:16:47.892549: train_loss -0.5814
2024-12-08 17:16:47.893982: val_loss -0.4444
2024-12-08 17:16:47.894822: Pseudo dice [0.6887]
2024-12-08 17:16:47.895751: Epoch time: 87.44 s
2024-12-08 17:16:47.896467: Yayy! New best EMA pseudo Dice: 0.6699
2024-12-08 17:16:49.454237: 
2024-12-08 17:16:49.455828: Epoch 24
2024-12-08 17:16:49.456830: Current learning rate: 0.00978
2024-12-08 17:18:19.063861: Validation loss did not improve from -0.50187. Patience: 4/50
2024-12-08 17:18:19.064847: train_loss -0.5866
2024-12-08 17:18:19.065633: val_loss -0.4685
2024-12-08 17:18:19.066390: Pseudo dice [0.7023]
2024-12-08 17:18:19.067119: Epoch time: 89.61 s
2024-12-08 17:18:19.424650: Yayy! New best EMA pseudo Dice: 0.6732
2024-12-08 17:18:20.961392: 
2024-12-08 17:18:20.963378: Epoch 25
2024-12-08 17:18:20.964184: Current learning rate: 0.00977
2024-12-08 17:19:50.388346: Validation loss did not improve from -0.50187. Patience: 5/50
2024-12-08 17:19:50.389399: train_loss -0.5839
2024-12-08 17:19:50.390256: val_loss -0.4974
2024-12-08 17:19:50.391060: Pseudo dice [0.7072]
2024-12-08 17:19:50.391824: Epoch time: 89.43 s
2024-12-08 17:19:50.392607: Yayy! New best EMA pseudo Dice: 0.6766
2024-12-08 17:19:51.965771: 
2024-12-08 17:19:51.967514: Epoch 26
2024-12-08 17:19:51.968434: Current learning rate: 0.00977
2024-12-08 17:21:21.537545: Validation loss did not improve from -0.50187. Patience: 6/50
2024-12-08 17:21:21.538697: train_loss -0.5926
2024-12-08 17:21:21.539897: val_loss -0.4674
2024-12-08 17:21:21.540717: Pseudo dice [0.6935]
2024-12-08 17:21:21.541670: Epoch time: 89.57 s
2024-12-08 17:21:21.542572: Yayy! New best EMA pseudo Dice: 0.6783
2024-12-08 17:21:23.095274: 
2024-12-08 17:21:23.096859: Epoch 27
2024-12-08 17:21:23.097709: Current learning rate: 0.00976
2024-12-08 17:22:52.415601: Validation loss improved from -0.50187 to -0.52539! Patience: 6/50
2024-12-08 17:22:52.416706: train_loss -0.6089
2024-12-08 17:22:52.417619: val_loss -0.5254
2024-12-08 17:22:52.418388: Pseudo dice [0.7298]
2024-12-08 17:22:52.419071: Epoch time: 89.32 s
2024-12-08 17:22:52.419856: Yayy! New best EMA pseudo Dice: 0.6834
2024-12-08 17:22:53.942592: 
2024-12-08 17:22:53.943785: Epoch 28
2024-12-08 17:22:53.944602: Current learning rate: 0.00975
2024-12-08 17:24:23.323835: Validation loss did not improve from -0.52539. Patience: 1/50
2024-12-08 17:24:23.324702: train_loss -0.6069
2024-12-08 17:24:23.325540: val_loss -0.4935
2024-12-08 17:24:23.326433: Pseudo dice [0.7109]
2024-12-08 17:24:23.327088: Epoch time: 89.38 s
2024-12-08 17:24:23.327679: Yayy! New best EMA pseudo Dice: 0.6862
2024-12-08 17:24:24.887958: 
2024-12-08 17:24:24.889653: Epoch 29
2024-12-08 17:24:24.890418: Current learning rate: 0.00974
2024-12-08 17:25:54.338080: Validation loss did not improve from -0.52539. Patience: 2/50
2024-12-08 17:25:54.339231: train_loss -0.6099
2024-12-08 17:25:54.340125: val_loss -0.509
2024-12-08 17:25:54.340883: Pseudo dice [0.7177]
2024-12-08 17:25:54.341552: Epoch time: 89.45 s
2024-12-08 17:25:54.702137: Yayy! New best EMA pseudo Dice: 0.6893
2024-12-08 17:25:56.572928: 
2024-12-08 17:25:56.573916: Epoch 30
2024-12-08 17:25:56.574694: Current learning rate: 0.00973
2024-12-08 17:27:25.829662: Validation loss did not improve from -0.52539. Patience: 3/50
2024-12-08 17:27:25.830640: train_loss -0.6049
2024-12-08 17:27:25.831825: val_loss -0.4959
2024-12-08 17:27:25.832932: Pseudo dice [0.7239]
2024-12-08 17:27:25.833851: Epoch time: 89.26 s
2024-12-08 17:27:25.834802: Yayy! New best EMA pseudo Dice: 0.6928
2024-12-08 17:27:27.489188: 
2024-12-08 17:27:27.490475: Epoch 31
2024-12-08 17:27:27.491278: Current learning rate: 0.00972
2024-12-08 17:28:56.552603: Validation loss did not improve from -0.52539. Patience: 4/50
2024-12-08 17:28:56.554027: train_loss -0.6118
2024-12-08 17:28:56.555255: val_loss -0.4943
2024-12-08 17:28:56.555964: Pseudo dice [0.7038]
2024-12-08 17:28:56.556754: Epoch time: 89.07 s
2024-12-08 17:28:56.557458: Yayy! New best EMA pseudo Dice: 0.6939
2024-12-08 17:28:58.122998: 
2024-12-08 17:28:58.124549: Epoch 32
2024-12-08 17:28:58.125396: Current learning rate: 0.00971
2024-12-08 17:30:27.141478: Validation loss did not improve from -0.52539. Patience: 5/50
2024-12-08 17:30:27.142660: train_loss -0.6121
2024-12-08 17:30:27.143664: val_loss -0.5029
2024-12-08 17:30:27.144476: Pseudo dice [0.7137]
2024-12-08 17:30:27.145422: Epoch time: 89.02 s
2024-12-08 17:30:27.146255: Yayy! New best EMA pseudo Dice: 0.6959
2024-12-08 17:30:28.746238: 
2024-12-08 17:30:28.747559: Epoch 33
2024-12-08 17:30:28.748415: Current learning rate: 0.0097
2024-12-08 17:31:57.882054: Validation loss did not improve from -0.52539. Patience: 6/50
2024-12-08 17:31:57.883322: train_loss -0.6218
2024-12-08 17:31:57.884191: val_loss -0.4713
2024-12-08 17:31:57.884906: Pseudo dice [0.6967]
2024-12-08 17:31:57.885709: Epoch time: 89.14 s
2024-12-08 17:31:57.886392: Yayy! New best EMA pseudo Dice: 0.696
2024-12-08 17:31:59.473892: 
2024-12-08 17:31:59.475261: Epoch 34
2024-12-08 17:31:59.476081: Current learning rate: 0.00969
2024-12-08 17:33:28.625057: Validation loss did not improve from -0.52539. Patience: 7/50
2024-12-08 17:33:28.626320: train_loss -0.6285
2024-12-08 17:33:28.627226: val_loss -0.5095
2024-12-08 17:33:28.628175: Pseudo dice [0.7236]
2024-12-08 17:33:28.628859: Epoch time: 89.15 s
2024-12-08 17:33:28.988261: Yayy! New best EMA pseudo Dice: 0.6987
2024-12-08 17:33:30.582723: 
2024-12-08 17:33:30.584561: Epoch 35
2024-12-08 17:33:30.585685: Current learning rate: 0.00968
2024-12-08 17:34:59.703307: Validation loss did not improve from -0.52539. Patience: 8/50
2024-12-08 17:34:59.704313: train_loss -0.6223
2024-12-08 17:34:59.705299: val_loss -0.5189
2024-12-08 17:34:59.706034: Pseudo dice [0.7238]
2024-12-08 17:34:59.706790: Epoch time: 89.12 s
2024-12-08 17:34:59.707602: Yayy! New best EMA pseudo Dice: 0.7012
2024-12-08 17:35:01.306831: 
2024-12-08 17:35:01.308623: Epoch 36
2024-12-08 17:35:01.309679: Current learning rate: 0.00968
2024-12-08 17:36:30.358918: Validation loss did not improve from -0.52539. Patience: 9/50
2024-12-08 17:36:30.360065: train_loss -0.6224
2024-12-08 17:36:30.361409: val_loss -0.4802
2024-12-08 17:36:30.362502: Pseudo dice [0.6981]
2024-12-08 17:36:30.363462: Epoch time: 89.05 s
2024-12-08 17:36:31.672510: 
2024-12-08 17:36:31.674420: Epoch 37
2024-12-08 17:36:31.675510: Current learning rate: 0.00967
2024-12-08 17:38:00.668787: Validation loss did not improve from -0.52539. Patience: 10/50
2024-12-08 17:38:00.669787: train_loss -0.6301
2024-12-08 17:38:00.670723: val_loss -0.5206
2024-12-08 17:38:00.671496: Pseudo dice [0.7282]
2024-12-08 17:38:00.672339: Epoch time: 89.0 s
2024-12-08 17:38:00.673039: Yayy! New best EMA pseudo Dice: 0.7036
2024-12-08 17:38:02.248734: 
2024-12-08 17:38:02.249642: Epoch 38
2024-12-08 17:38:02.250387: Current learning rate: 0.00966
2024-12-08 17:39:31.487523: Validation loss did not improve from -0.52539. Patience: 11/50
2024-12-08 17:39:31.488741: train_loss -0.6277
2024-12-08 17:39:31.489674: val_loss -0.5196
2024-12-08 17:39:31.490468: Pseudo dice [0.7225]
2024-12-08 17:39:31.491284: Epoch time: 89.24 s
2024-12-08 17:39:31.492008: Yayy! New best EMA pseudo Dice: 0.7055
2024-12-08 17:39:33.083108: 
2024-12-08 17:39:33.084627: Epoch 39
2024-12-08 17:39:33.085416: Current learning rate: 0.00965
2024-12-08 17:41:02.418096: Validation loss improved from -0.52539 to -0.53406! Patience: 11/50
2024-12-08 17:41:02.419187: train_loss -0.6232
2024-12-08 17:41:02.420033: val_loss -0.5341
2024-12-08 17:41:02.420718: Pseudo dice [0.7272]
2024-12-08 17:41:02.421452: Epoch time: 89.34 s
2024-12-08 17:41:02.764564: Yayy! New best EMA pseudo Dice: 0.7077
2024-12-08 17:41:04.367537: 
2024-12-08 17:41:04.369033: Epoch 40
2024-12-08 17:41:04.369810: Current learning rate: 0.00964
2024-12-08 17:42:33.748599: Validation loss did not improve from -0.53406. Patience: 1/50
2024-12-08 17:42:33.749569: train_loss -0.6312
2024-12-08 17:42:33.750481: val_loss -0.5152
2024-12-08 17:42:33.751224: Pseudo dice [0.7278]
2024-12-08 17:42:33.752155: Epoch time: 89.38 s
2024-12-08 17:42:33.752911: Yayy! New best EMA pseudo Dice: 0.7097
2024-12-08 17:42:35.653929: 
2024-12-08 17:42:35.655602: Epoch 41
2024-12-08 17:42:35.656614: Current learning rate: 0.00963
2024-12-08 17:44:04.762928: Validation loss did not improve from -0.53406. Patience: 2/50
2024-12-08 17:44:04.763918: train_loss -0.6192
2024-12-08 17:44:04.764753: val_loss -0.515
2024-12-08 17:44:04.765730: Pseudo dice [0.7158]
2024-12-08 17:44:04.766600: Epoch time: 89.11 s
2024-12-08 17:44:04.767290: Yayy! New best EMA pseudo Dice: 0.7103
2024-12-08 17:44:06.333047: 
2024-12-08 17:44:06.334688: Epoch 42
2024-12-08 17:44:06.335400: Current learning rate: 0.00962
2024-12-08 17:45:35.489322: Validation loss did not improve from -0.53406. Patience: 3/50
2024-12-08 17:45:35.490512: train_loss -0.6343
2024-12-08 17:45:35.491569: val_loss -0.5317
2024-12-08 17:45:35.492462: Pseudo dice [0.7376]
2024-12-08 17:45:35.493337: Epoch time: 89.16 s
2024-12-08 17:45:35.494148: Yayy! New best EMA pseudo Dice: 0.713
2024-12-08 17:45:37.124223: 
2024-12-08 17:45:37.125507: Epoch 43
2024-12-08 17:45:37.126456: Current learning rate: 0.00961
2024-12-08 17:47:06.074245: Validation loss improved from -0.53406 to -0.54731! Patience: 3/50
2024-12-08 17:47:06.075309: train_loss -0.6396
2024-12-08 17:47:06.076337: val_loss -0.5473
2024-12-08 17:47:06.077234: Pseudo dice [0.7415]
2024-12-08 17:47:06.078088: Epoch time: 88.95 s
2024-12-08 17:47:06.079022: Yayy! New best EMA pseudo Dice: 0.7159
2024-12-08 17:47:07.663992: 
2024-12-08 17:47:07.665221: Epoch 44
2024-12-08 17:47:07.665971: Current learning rate: 0.0096
2024-12-08 17:48:36.614177: Validation loss did not improve from -0.54731. Patience: 1/50
2024-12-08 17:48:36.615313: train_loss -0.6522
2024-12-08 17:48:36.616404: val_loss -0.533
2024-12-08 17:48:36.617210: Pseudo dice [0.7353]
2024-12-08 17:48:36.617933: Epoch time: 88.95 s
2024-12-08 17:48:36.947871: Yayy! New best EMA pseudo Dice: 0.7178
2024-12-08 17:48:38.509431: 
2024-12-08 17:48:38.511257: Epoch 45
2024-12-08 17:48:38.512207: Current learning rate: 0.00959
2024-12-08 17:50:07.391576: Validation loss did not improve from -0.54731. Patience: 2/50
2024-12-08 17:50:07.392860: train_loss -0.6498
2024-12-08 17:50:07.393847: val_loss -0.5299
2024-12-08 17:50:07.394637: Pseudo dice [0.7266]
2024-12-08 17:50:07.395401: Epoch time: 88.88 s
2024-12-08 17:50:07.396202: Yayy! New best EMA pseudo Dice: 0.7187
2024-12-08 17:50:08.964610: 
2024-12-08 17:50:08.966170: Epoch 46
2024-12-08 17:50:08.966933: Current learning rate: 0.00959
2024-12-08 17:51:37.917354: Validation loss did not improve from -0.54731. Patience: 3/50
2024-12-08 17:51:37.918681: train_loss -0.6511
2024-12-08 17:51:37.919441: val_loss -0.5189
2024-12-08 17:51:37.920232: Pseudo dice [0.7273]
2024-12-08 17:51:37.920907: Epoch time: 88.95 s
2024-12-08 17:51:37.921590: Yayy! New best EMA pseudo Dice: 0.7196
2024-12-08 17:51:39.485030: 
2024-12-08 17:51:39.486228: Epoch 47
2024-12-08 17:51:39.486958: Current learning rate: 0.00958
2024-12-08 17:53:08.469517: Validation loss did not improve from -0.54731. Patience: 4/50
2024-12-08 17:53:08.470853: train_loss -0.6523
2024-12-08 17:53:08.472188: val_loss -0.529
2024-12-08 17:53:08.473262: Pseudo dice [0.7368]
2024-12-08 17:53:08.474244: Epoch time: 88.99 s
2024-12-08 17:53:08.475026: Yayy! New best EMA pseudo Dice: 0.7213
2024-12-08 17:53:10.068979: 
2024-12-08 17:53:10.070344: Epoch 48
2024-12-08 17:53:10.071208: Current learning rate: 0.00957
2024-12-08 17:54:38.914096: Validation loss did not improve from -0.54731. Patience: 5/50
2024-12-08 17:54:38.915390: train_loss -0.6495
2024-12-08 17:54:38.916834: val_loss -0.5381
2024-12-08 17:54:38.917637: Pseudo dice [0.7425]
2024-12-08 17:54:38.918498: Epoch time: 88.85 s
2024-12-08 17:54:38.919162: Yayy! New best EMA pseudo Dice: 0.7234
2024-12-08 17:54:40.432714: 
2024-12-08 17:54:40.434033: Epoch 49
2024-12-08 17:54:40.434784: Current learning rate: 0.00956
2024-12-08 17:56:09.678143: Validation loss did not improve from -0.54731. Patience: 6/50
2024-12-08 17:56:09.679165: train_loss -0.6566
2024-12-08 17:56:09.680189: val_loss -0.5051
2024-12-08 17:56:09.681080: Pseudo dice [0.7126]
2024-12-08 17:56:09.681845: Epoch time: 89.25 s
2024-12-08 17:56:11.262117: 
2024-12-08 17:56:11.263594: Epoch 50
2024-12-08 17:56:11.264315: Current learning rate: 0.00955
2024-12-08 17:57:40.406725: Validation loss did not improve from -0.54731. Patience: 7/50
2024-12-08 17:57:40.407840: train_loss -0.6603
2024-12-08 17:57:40.408700: val_loss -0.5243
2024-12-08 17:57:40.409380: Pseudo dice [0.7322]
2024-12-08 17:57:40.410041: Epoch time: 89.15 s
2024-12-08 17:57:41.933395: 
2024-12-08 17:57:41.935286: Epoch 51
2024-12-08 17:57:41.936111: Current learning rate: 0.00954
2024-12-08 17:59:11.093322: Validation loss improved from -0.54731 to -0.55734! Patience: 7/50
2024-12-08 17:59:11.094443: train_loss -0.6516
2024-12-08 17:59:11.095565: val_loss -0.5573
2024-12-08 17:59:11.096319: Pseudo dice [0.7497]
2024-12-08 17:59:11.097111: Epoch time: 89.16 s
2024-12-08 17:59:11.097797: Yayy! New best EMA pseudo Dice: 0.726
2024-12-08 17:59:12.665253: 
2024-12-08 17:59:12.667031: Epoch 52
2024-12-08 17:59:12.667884: Current learning rate: 0.00953
2024-12-08 18:00:41.842695: Validation loss did not improve from -0.55734. Patience: 1/50
2024-12-08 18:00:41.843640: train_loss -0.6584
2024-12-08 18:00:41.844448: val_loss -0.5296
2024-12-08 18:00:41.845249: Pseudo dice [0.7329]
2024-12-08 18:00:41.845959: Epoch time: 89.18 s
2024-12-08 18:00:41.846589: Yayy! New best EMA pseudo Dice: 0.7267
2024-12-08 18:00:43.397426: 
2024-12-08 18:00:43.399231: Epoch 53
2024-12-08 18:00:43.400002: Current learning rate: 0.00952
2024-12-08 18:02:12.644525: Validation loss did not improve from -0.55734. Patience: 2/50
2024-12-08 18:02:12.645907: train_loss -0.6595
2024-12-08 18:02:12.646715: val_loss -0.5077
2024-12-08 18:02:12.647372: Pseudo dice [0.724]
2024-12-08 18:02:12.648096: Epoch time: 89.25 s
2024-12-08 18:02:13.884630: 
2024-12-08 18:02:13.886331: Epoch 54
2024-12-08 18:02:13.887265: Current learning rate: 0.00951
2024-12-08 18:03:43.095100: Validation loss improved from -0.55734 to -0.56644! Patience: 2/50
2024-12-08 18:03:43.096354: train_loss -0.6617
2024-12-08 18:03:43.097301: val_loss -0.5664
2024-12-08 18:03:43.097978: Pseudo dice [0.7537]
2024-12-08 18:03:43.098749: Epoch time: 89.21 s
2024-12-08 18:03:43.454384: Yayy! New best EMA pseudo Dice: 0.7291
2024-12-08 18:03:45.125301: 
2024-12-08 18:03:45.127179: Epoch 55
2024-12-08 18:03:45.128067: Current learning rate: 0.0095
2024-12-08 18:05:14.421767: Validation loss did not improve from -0.56644. Patience: 1/50
2024-12-08 18:05:14.422763: train_loss -0.6712
2024-12-08 18:05:14.423688: val_loss -0.5295
2024-12-08 18:05:14.424393: Pseudo dice [0.7272]
2024-12-08 18:05:14.425045: Epoch time: 89.3 s
2024-12-08 18:05:15.709731: 
2024-12-08 18:05:15.711023: Epoch 56
2024-12-08 18:05:15.711696: Current learning rate: 0.00949
2024-12-08 18:06:45.073220: Validation loss did not improve from -0.56644. Patience: 2/50
2024-12-08 18:06:45.074029: train_loss -0.6678
2024-12-08 18:06:45.074858: val_loss -0.5426
2024-12-08 18:06:45.075605: Pseudo dice [0.7399]
2024-12-08 18:06:45.076238: Epoch time: 89.37 s
2024-12-08 18:06:45.076864: Yayy! New best EMA pseudo Dice: 0.73
2024-12-08 18:06:46.674830: 
2024-12-08 18:06:46.676574: Epoch 57
2024-12-08 18:06:46.677670: Current learning rate: 0.00949
2024-12-08 18:08:16.186979: Validation loss did not improve from -0.56644. Patience: 3/50
2024-12-08 18:08:16.188177: train_loss -0.655
2024-12-08 18:08:16.189268: val_loss -0.5241
2024-12-08 18:08:16.190203: Pseudo dice [0.7288]
2024-12-08 18:08:16.191259: Epoch time: 89.51 s
2024-12-08 18:08:17.393713: 
2024-12-08 18:08:17.395347: Epoch 58
2024-12-08 18:08:17.396432: Current learning rate: 0.00948
2024-12-08 18:09:46.879993: Validation loss did not improve from -0.56644. Patience: 4/50
2024-12-08 18:09:46.881022: train_loss -0.6643
2024-12-08 18:09:46.882128: val_loss -0.5486
2024-12-08 18:09:46.883142: Pseudo dice [0.745]
2024-12-08 18:09:46.883919: Epoch time: 89.49 s
2024-12-08 18:09:46.884864: Yayy! New best EMA pseudo Dice: 0.7314
2024-12-08 18:09:48.464139: 
2024-12-08 18:09:48.465559: Epoch 59
2024-12-08 18:09:48.466421: Current learning rate: 0.00947
2024-12-08 18:11:18.012371: Validation loss did not improve from -0.56644. Patience: 5/50
2024-12-08 18:11:18.013150: train_loss -0.6722
2024-12-08 18:11:18.014186: val_loss -0.5189
2024-12-08 18:11:18.015069: Pseudo dice [0.7264]
2024-12-08 18:11:18.015752: Epoch time: 89.55 s
2024-12-08 18:11:19.567734: 
2024-12-08 18:11:19.569465: Epoch 60
2024-12-08 18:11:19.570196: Current learning rate: 0.00946
2024-12-08 18:12:49.048413: Validation loss did not improve from -0.56644. Patience: 6/50
2024-12-08 18:12:49.049489: train_loss -0.67
2024-12-08 18:12:49.050459: val_loss -0.5331
2024-12-08 18:12:49.051348: Pseudo dice [0.7358]
2024-12-08 18:12:49.052212: Epoch time: 89.48 s
2024-12-08 18:12:50.306478: 
2024-12-08 18:12:50.308389: Epoch 61
2024-12-08 18:12:50.309284: Current learning rate: 0.00945
2024-12-08 18:14:19.880229: Validation loss did not improve from -0.56644. Patience: 7/50
2024-12-08 18:14:19.881336: train_loss -0.6695
2024-12-08 18:14:19.882186: val_loss -0.5431
2024-12-08 18:14:19.883025: Pseudo dice [0.7408]
2024-12-08 18:14:19.883817: Epoch time: 89.58 s
2024-12-08 18:14:19.884728: Yayy! New best EMA pseudo Dice: 0.7323
2024-12-08 18:14:21.472082: 
2024-12-08 18:14:21.473433: Epoch 62
2024-12-08 18:14:21.474455: Current learning rate: 0.00944
2024-12-08 18:15:51.005380: Validation loss did not improve from -0.56644. Patience: 8/50
2024-12-08 18:15:51.006561: train_loss -0.6726
2024-12-08 18:15:51.007539: val_loss -0.5413
2024-12-08 18:15:51.008273: Pseudo dice [0.7464]
2024-12-08 18:15:51.009079: Epoch time: 89.54 s
2024-12-08 18:15:51.009753: Yayy! New best EMA pseudo Dice: 0.7338
2024-12-08 18:15:52.954244: 
2024-12-08 18:15:52.956229: Epoch 63
2024-12-08 18:15:52.957237: Current learning rate: 0.00943
2024-12-08 18:17:22.456035: Validation loss did not improve from -0.56644. Patience: 9/50
2024-12-08 18:17:22.457274: train_loss -0.6706
2024-12-08 18:17:22.458624: val_loss -0.5555
2024-12-08 18:17:22.459498: Pseudo dice [0.7492]
2024-12-08 18:17:22.460352: Epoch time: 89.5 s
2024-12-08 18:17:22.461263: Yayy! New best EMA pseudo Dice: 0.7353
2024-12-08 18:17:24.064680: 
2024-12-08 18:17:24.066508: Epoch 64
2024-12-08 18:17:24.067492: Current learning rate: 0.00942
2024-12-08 18:18:53.579274: Validation loss did not improve from -0.56644. Patience: 10/50
2024-12-08 18:18:53.580441: train_loss -0.6712
2024-12-08 18:18:53.581423: val_loss -0.5467
2024-12-08 18:18:53.582187: Pseudo dice [0.7418]
2024-12-08 18:18:53.583034: Epoch time: 89.52 s
2024-12-08 18:18:53.923698: Yayy! New best EMA pseudo Dice: 0.7359
2024-12-08 18:18:55.458119: 
2024-12-08 18:18:55.459712: Epoch 65
2024-12-08 18:18:55.460508: Current learning rate: 0.00941
2024-12-08 18:20:24.952919: Validation loss did not improve from -0.56644. Patience: 11/50
2024-12-08 18:20:24.954115: train_loss -0.6808
2024-12-08 18:20:24.954965: val_loss -0.5446
2024-12-08 18:20:24.955733: Pseudo dice [0.7457]
2024-12-08 18:20:24.956592: Epoch time: 89.5 s
2024-12-08 18:20:24.957372: Yayy! New best EMA pseudo Dice: 0.7369
2024-12-08 18:20:26.564141: 
2024-12-08 18:20:26.565847: Epoch 66
2024-12-08 18:20:26.566733: Current learning rate: 0.0094
2024-12-08 18:21:56.161178: Validation loss did not improve from -0.56644. Patience: 12/50
2024-12-08 18:21:56.162574: train_loss -0.6826
2024-12-08 18:21:56.163676: val_loss -0.5299
2024-12-08 18:21:56.164419: Pseudo dice [0.7306]
2024-12-08 18:21:56.165184: Epoch time: 89.6 s
2024-12-08 18:21:57.389869: 
2024-12-08 18:21:57.391422: Epoch 67
2024-12-08 18:21:57.392177: Current learning rate: 0.00939
2024-12-08 18:23:27.025517: Validation loss did not improve from -0.56644. Patience: 13/50
2024-12-08 18:23:27.026495: train_loss -0.6712
2024-12-08 18:23:27.027811: val_loss -0.5131
2024-12-08 18:23:27.028858: Pseudo dice [0.7219]
2024-12-08 18:23:27.030016: Epoch time: 89.64 s
2024-12-08 18:23:28.327793: 
2024-12-08 18:23:28.329708: Epoch 68
2024-12-08 18:23:28.330683: Current learning rate: 0.00939
2024-12-08 18:24:57.880644: Validation loss did not improve from -0.56644. Patience: 14/50
2024-12-08 18:24:57.881762: train_loss -0.6799
2024-12-08 18:24:57.882604: val_loss -0.5408
2024-12-08 18:24:57.883515: Pseudo dice [0.7428]
2024-12-08 18:24:57.884289: Epoch time: 89.56 s
2024-12-08 18:24:59.125994: 
2024-12-08 18:24:59.127649: Epoch 69
2024-12-08 18:24:59.128501: Current learning rate: 0.00938
2024-12-08 18:26:28.584063: Validation loss did not improve from -0.56644. Patience: 15/50
2024-12-08 18:26:28.585054: train_loss -0.6902
2024-12-08 18:26:28.585984: val_loss -0.5493
2024-12-08 18:26:28.586942: Pseudo dice [0.7458]
2024-12-08 18:26:28.587649: Epoch time: 89.46 s
2024-12-08 18:26:30.229303: 
2024-12-08 18:26:30.230648: Epoch 70
2024-12-08 18:26:30.231406: Current learning rate: 0.00937
2024-12-08 18:27:59.616241: Validation loss did not improve from -0.56644. Patience: 16/50
2024-12-08 18:27:59.617246: train_loss -0.6873
2024-12-08 18:27:59.618098: val_loss -0.5511
2024-12-08 18:27:59.619116: Pseudo dice [0.7441]
2024-12-08 18:27:59.620016: Epoch time: 89.39 s
2024-12-08 18:27:59.620929: Yayy! New best EMA pseudo Dice: 0.7374
2024-12-08 18:28:01.235491: 
2024-12-08 18:28:01.237077: Epoch 71
2024-12-08 18:28:01.237923: Current learning rate: 0.00936
2024-12-08 18:29:30.505083: Validation loss did not improve from -0.56644. Patience: 17/50
2024-12-08 18:29:30.506440: train_loss -0.6904
2024-12-08 18:29:30.507870: val_loss -0.5357
2024-12-08 18:29:30.508724: Pseudo dice [0.7402]
2024-12-08 18:29:30.509679: Epoch time: 89.27 s
2024-12-08 18:29:30.510493: Yayy! New best EMA pseudo Dice: 0.7377
2024-12-08 18:29:32.130448: 
2024-12-08 18:29:32.132013: Epoch 72
2024-12-08 18:29:32.132932: Current learning rate: 0.00935
2024-12-08 18:31:01.532403: Validation loss did not improve from -0.56644. Patience: 18/50
2024-12-08 18:31:01.533700: train_loss -0.6936
2024-12-08 18:31:01.534672: val_loss -0.5393
2024-12-08 18:31:01.535389: Pseudo dice [0.7359]
2024-12-08 18:31:01.536163: Epoch time: 89.4 s
2024-12-08 18:31:03.135377: 
2024-12-08 18:31:03.137216: Epoch 73
2024-12-08 18:31:03.138006: Current learning rate: 0.00934
2024-12-08 18:32:32.655254: Validation loss did not improve from -0.56644. Patience: 19/50
2024-12-08 18:32:32.656527: train_loss -0.6969
2024-12-08 18:32:32.657444: val_loss -0.556
2024-12-08 18:32:32.658216: Pseudo dice [0.7494]
2024-12-08 18:32:32.658871: Epoch time: 89.52 s
2024-12-08 18:32:32.659594: Yayy! New best EMA pseudo Dice: 0.7387
2024-12-08 18:32:34.295060: 
2024-12-08 18:32:34.296736: Epoch 74
2024-12-08 18:32:34.297471: Current learning rate: 0.00933
2024-12-08 18:34:03.851913: Validation loss did not improve from -0.56644. Patience: 20/50
2024-12-08 18:34:03.852876: train_loss -0.6985
2024-12-08 18:34:03.853793: val_loss -0.5305
2024-12-08 18:34:03.854668: Pseudo dice [0.7369]
2024-12-08 18:34:03.855519: Epoch time: 89.56 s
2024-12-08 18:34:05.480778: 
2024-12-08 18:34:05.482490: Epoch 75
2024-12-08 18:34:05.483424: Current learning rate: 0.00932
2024-12-08 18:35:35.060755: Validation loss did not improve from -0.56644. Patience: 21/50
2024-12-08 18:35:35.061868: train_loss -0.6993
2024-12-08 18:35:35.062771: val_loss -0.5442
2024-12-08 18:35:35.063543: Pseudo dice [0.7448]
2024-12-08 18:35:35.064294: Epoch time: 89.58 s
2024-12-08 18:35:35.064903: Yayy! New best EMA pseudo Dice: 0.7391
2024-12-08 18:35:36.649332: 
2024-12-08 18:35:36.650429: Epoch 76
2024-12-08 18:35:36.651212: Current learning rate: 0.00931
2024-12-08 18:37:06.116324: Validation loss did not improve from -0.56644. Patience: 22/50
2024-12-08 18:37:06.117322: train_loss -0.6984
2024-12-08 18:37:06.118590: val_loss -0.5188
2024-12-08 18:37:06.119605: Pseudo dice [0.7351]
2024-12-08 18:37:06.120284: Epoch time: 89.47 s
2024-12-08 18:37:07.384465: 
2024-12-08 18:37:07.385998: Epoch 77
2024-12-08 18:37:07.386878: Current learning rate: 0.0093
2024-12-08 18:38:36.823998: Validation loss did not improve from -0.56644. Patience: 23/50
2024-12-08 18:38:36.825039: train_loss -0.6879
2024-12-08 18:38:36.826041: val_loss -0.5301
2024-12-08 18:38:36.826910: Pseudo dice [0.7384]
2024-12-08 18:38:36.827775: Epoch time: 89.44 s
2024-12-08 18:38:38.062565: 
2024-12-08 18:38:38.064425: Epoch 78
2024-12-08 18:38:38.065344: Current learning rate: 0.0093
2024-12-08 18:40:07.528001: Validation loss did not improve from -0.56644. Patience: 24/50
2024-12-08 18:40:07.529265: train_loss -0.6944
2024-12-08 18:40:07.530228: val_loss -0.5395
2024-12-08 18:40:07.531127: Pseudo dice [0.7443]
2024-12-08 18:40:07.531839: Epoch time: 89.47 s
2024-12-08 18:40:07.532569: Yayy! New best EMA pseudo Dice: 0.7393
2024-12-08 18:40:09.151842: 
2024-12-08 18:40:09.153370: Epoch 79
2024-12-08 18:40:09.154151: Current learning rate: 0.00929
2024-12-08 18:41:38.659536: Validation loss did not improve from -0.56644. Patience: 25/50
2024-12-08 18:41:38.660779: train_loss -0.6916
2024-12-08 18:41:38.661821: val_loss -0.539
2024-12-08 18:41:38.662476: Pseudo dice [0.7423]
2024-12-08 18:41:38.663152: Epoch time: 89.51 s
2024-12-08 18:41:39.013853: Yayy! New best EMA pseudo Dice: 0.7396
2024-12-08 18:41:40.640135: 
2024-12-08 18:41:40.641643: Epoch 80
2024-12-08 18:41:40.642461: Current learning rate: 0.00928
2024-12-08 18:43:10.119220: Validation loss did not improve from -0.56644. Patience: 26/50
2024-12-08 18:43:10.120614: train_loss -0.6893
2024-12-08 18:43:10.121711: val_loss -0.5508
2024-12-08 18:43:10.122416: Pseudo dice [0.7409]
2024-12-08 18:43:10.123165: Epoch time: 89.48 s
2024-12-08 18:43:10.123849: Yayy! New best EMA pseudo Dice: 0.7397
2024-12-08 18:43:11.755522: 
2024-12-08 18:43:11.757178: Epoch 81
2024-12-08 18:43:11.757974: Current learning rate: 0.00927
2024-12-08 18:44:41.193843: Validation loss did not improve from -0.56644. Patience: 27/50
2024-12-08 18:44:41.195087: train_loss -0.7002
2024-12-08 18:44:41.196012: val_loss -0.5179
2024-12-08 18:44:41.196867: Pseudo dice [0.7314]
2024-12-08 18:44:41.197593: Epoch time: 89.44 s
2024-12-08 18:44:42.507396: 
2024-12-08 18:44:42.509313: Epoch 82
2024-12-08 18:44:42.510235: Current learning rate: 0.00926
2024-12-08 18:46:11.944813: Validation loss did not improve from -0.56644. Patience: 28/50
2024-12-08 18:46:11.945903: train_loss -0.6998
2024-12-08 18:46:11.946809: val_loss -0.5398
2024-12-08 18:46:11.947808: Pseudo dice [0.7393]
2024-12-08 18:46:11.948696: Epoch time: 89.44 s
2024-12-08 18:46:13.154290: 
2024-12-08 18:46:13.156192: Epoch 83
2024-12-08 18:46:13.157262: Current learning rate: 0.00925
2024-12-08 18:47:42.678349: Validation loss did not improve from -0.56644. Patience: 29/50
2024-12-08 18:47:42.683738: train_loss -0.6989
2024-12-08 18:47:42.686741: val_loss -0.5405
2024-12-08 18:47:42.687638: Pseudo dice [0.737]
2024-12-08 18:47:42.689306: Epoch time: 89.53 s
2024-12-08 18:47:44.305403: 
2024-12-08 18:47:44.306787: Epoch 84
2024-12-08 18:47:44.307476: Current learning rate: 0.00924
2024-12-08 18:49:13.894696: Validation loss did not improve from -0.56644. Patience: 30/50
2024-12-08 18:49:13.895872: train_loss -0.7082
2024-12-08 18:49:13.897163: val_loss -0.5382
2024-12-08 18:49:13.897946: Pseudo dice [0.7404]
2024-12-08 18:49:13.898668: Epoch time: 89.59 s
2024-12-08 18:49:15.415688: 
2024-12-08 18:49:15.417395: Epoch 85
2024-12-08 18:49:15.418190: Current learning rate: 0.00923
2024-12-08 18:50:45.176053: Validation loss did not improve from -0.56644. Patience: 31/50
2024-12-08 18:50:45.177161: train_loss -0.7103
2024-12-08 18:50:45.178136: val_loss -0.5471
2024-12-08 18:50:45.179074: Pseudo dice [0.7436]
2024-12-08 18:50:45.179756: Epoch time: 89.76 s
2024-12-08 18:50:46.384156: 
2024-12-08 18:50:46.385600: Epoch 86
2024-12-08 18:50:46.386444: Current learning rate: 0.00922
2024-12-08 18:52:16.222612: Validation loss did not improve from -0.56644. Patience: 32/50
2024-12-08 18:52:16.223482: train_loss -0.707
2024-12-08 18:52:16.224615: val_loss -0.5221
2024-12-08 18:52:16.225692: Pseudo dice [0.733]
2024-12-08 18:52:16.226670: Epoch time: 89.84 s
2024-12-08 18:52:17.428597: 
2024-12-08 18:52:17.430339: Epoch 87
2024-12-08 18:52:17.431506: Current learning rate: 0.00921
2024-12-08 18:53:47.051080: Validation loss did not improve from -0.56644. Patience: 33/50
2024-12-08 18:53:47.052325: train_loss -0.7098
2024-12-08 18:53:47.053298: val_loss -0.5422
2024-12-08 18:53:47.054126: Pseudo dice [0.7379]
2024-12-08 18:53:47.054859: Epoch time: 89.62 s
2024-12-08 18:53:48.264854: 
2024-12-08 18:53:48.266678: Epoch 88
2024-12-08 18:53:48.267609: Current learning rate: 0.0092
2024-12-08 18:55:18.004319: Validation loss improved from -0.56644 to -0.56956! Patience: 33/50
2024-12-08 18:55:18.005244: train_loss -0.7111
2024-12-08 18:55:18.006204: val_loss -0.5696
2024-12-08 18:55:18.006872: Pseudo dice [0.7582]
2024-12-08 18:55:18.007578: Epoch time: 89.74 s
2024-12-08 18:55:18.008313: Yayy! New best EMA pseudo Dice: 0.7406
2024-12-08 18:55:19.497919: 
2024-12-08 18:55:19.499368: Epoch 89
2024-12-08 18:55:19.500149: Current learning rate: 0.0092
2024-12-08 18:56:49.219077: Validation loss did not improve from -0.56956. Patience: 1/50
2024-12-08 18:56:49.219939: train_loss -0.7073
2024-12-08 18:56:49.220890: val_loss -0.5244
2024-12-08 18:56:49.221728: Pseudo dice [0.7235]
2024-12-08 18:56:49.222574: Epoch time: 89.72 s
2024-12-08 18:56:50.755590: 
2024-12-08 18:56:50.757098: Epoch 90
2024-12-08 18:56:50.758024: Current learning rate: 0.00919
2024-12-08 18:58:20.500619: Validation loss did not improve from -0.56956. Patience: 2/50
2024-12-08 18:58:20.501855: train_loss -0.7135
2024-12-08 18:58:20.502957: val_loss -0.5488
2024-12-08 18:58:20.503849: Pseudo dice [0.7549]
2024-12-08 18:58:20.504640: Epoch time: 89.75 s
2024-12-08 18:58:21.787738: 
2024-12-08 18:58:21.789176: Epoch 91
2024-12-08 18:58:21.790000: Current learning rate: 0.00918
2024-12-08 18:59:51.621606: Validation loss did not improve from -0.56956. Patience: 3/50
2024-12-08 18:59:51.622702: train_loss -0.6964
2024-12-08 18:59:51.623603: val_loss -0.5614
2024-12-08 18:59:51.624486: Pseudo dice [0.7518]
2024-12-08 18:59:51.625176: Epoch time: 89.84 s
2024-12-08 18:59:51.625963: Yayy! New best EMA pseudo Dice: 0.7416
2024-12-08 18:59:53.156462: 
2024-12-08 18:59:53.158137: Epoch 92
2024-12-08 18:59:53.158943: Current learning rate: 0.00917
2024-12-08 19:01:22.811194: Validation loss did not improve from -0.56956. Patience: 4/50
2024-12-08 19:01:22.811958: train_loss -0.7077
2024-12-08 19:01:22.812724: val_loss -0.5577
2024-12-08 19:01:22.813394: Pseudo dice [0.7523]
2024-12-08 19:01:22.814096: Epoch time: 89.66 s
2024-12-08 19:01:22.814830: Yayy! New best EMA pseudo Dice: 0.7427
2024-12-08 19:01:24.313079: 
2024-12-08 19:01:24.313982: Epoch 93
2024-12-08 19:01:24.314893: Current learning rate: 0.00916
2024-12-08 19:02:53.966513: Validation loss did not improve from -0.56956. Patience: 5/50
2024-12-08 19:02:53.967422: train_loss -0.7139
2024-12-08 19:02:53.968168: val_loss -0.5387
2024-12-08 19:02:53.968982: Pseudo dice [0.7366]
2024-12-08 19:02:53.969754: Epoch time: 89.66 s
2024-12-08 19:02:55.117520: 
2024-12-08 19:02:55.119154: Epoch 94
2024-12-08 19:02:55.120260: Current learning rate: 0.00915
2024-12-08 19:04:24.834432: Validation loss did not improve from -0.56956. Patience: 6/50
2024-12-08 19:04:24.835598: train_loss -0.7148
2024-12-08 19:04:24.836733: val_loss -0.5435
2024-12-08 19:04:24.837463: Pseudo dice [0.7442]
2024-12-08 19:04:24.838262: Epoch time: 89.72 s
2024-12-08 19:04:26.699122: 
2024-12-08 19:04:26.700029: Epoch 95
2024-12-08 19:04:26.700753: Current learning rate: 0.00914
2024-12-08 19:05:56.513499: Validation loss did not improve from -0.56956. Patience: 7/50
2024-12-08 19:05:56.514710: train_loss -0.7192
2024-12-08 19:05:56.515564: val_loss -0.5663
2024-12-08 19:05:56.516409: Pseudo dice [0.7542]
2024-12-08 19:05:56.517125: Epoch time: 89.82 s
2024-12-08 19:05:56.517847: Yayy! New best EMA pseudo Dice: 0.7435
2024-12-08 19:05:58.079260: 
2024-12-08 19:05:58.080547: Epoch 96
2024-12-08 19:05:58.081316: Current learning rate: 0.00913
2024-12-08 19:07:27.741502: Validation loss did not improve from -0.56956. Patience: 8/50
2024-12-08 19:07:27.742318: train_loss -0.7189
2024-12-08 19:07:27.743565: val_loss -0.5552
2024-12-08 19:07:27.744534: Pseudo dice [0.7546]
2024-12-08 19:07:27.745384: Epoch time: 89.66 s
2024-12-08 19:07:27.746241: Yayy! New best EMA pseudo Dice: 0.7446
2024-12-08 19:07:29.338232: 
2024-12-08 19:07:29.339285: Epoch 97
2024-12-08 19:07:29.340173: Current learning rate: 0.00912
2024-12-08 19:08:58.932245: Validation loss did not improve from -0.56956. Patience: 9/50
2024-12-08 19:08:58.933547: train_loss -0.7181
2024-12-08 19:08:58.934498: val_loss -0.5516
2024-12-08 19:08:58.935169: Pseudo dice [0.7524]
2024-12-08 19:08:58.936221: Epoch time: 89.6 s
2024-12-08 19:08:58.937018: Yayy! New best EMA pseudo Dice: 0.7454
2024-12-08 19:09:00.531191: 
2024-12-08 19:09:00.533175: Epoch 98
2024-12-08 19:09:00.534198: Current learning rate: 0.00911
2024-12-08 19:10:30.227659: Validation loss did not improve from -0.56956. Patience: 10/50
2024-12-08 19:10:30.228869: train_loss -0.7163
2024-12-08 19:10:30.229867: val_loss -0.5678
2024-12-08 19:10:30.230766: Pseudo dice [0.7527]
2024-12-08 19:10:30.231572: Epoch time: 89.7 s
2024-12-08 19:10:30.232313: Yayy! New best EMA pseudo Dice: 0.7461
2024-12-08 19:10:31.784239: 
2024-12-08 19:10:31.785845: Epoch 99
2024-12-08 19:10:31.786688: Current learning rate: 0.0091
2024-12-08 19:12:01.442986: Validation loss did not improve from -0.56956. Patience: 11/50
2024-12-08 19:12:01.444365: train_loss -0.7204
2024-12-08 19:12:01.445318: val_loss -0.5349
2024-12-08 19:12:01.446108: Pseudo dice [0.7419]
2024-12-08 19:12:01.446911: Epoch time: 89.66 s
2024-12-08 19:12:02.998856: 
2024-12-08 19:12:03.000222: Epoch 100
2024-12-08 19:12:03.001282: Current learning rate: 0.0091
2024-12-08 19:13:32.717856: Validation loss did not improve from -0.56956. Patience: 12/50
2024-12-08 19:13:32.718743: train_loss -0.7213
2024-12-08 19:13:32.719641: val_loss -0.5604
2024-12-08 19:13:32.720291: Pseudo dice [0.7493]
2024-12-08 19:13:32.721059: Epoch time: 89.72 s
2024-12-08 19:13:33.912628: 
2024-12-08 19:13:33.914455: Epoch 101
2024-12-08 19:13:33.915267: Current learning rate: 0.00909
2024-12-08 19:15:03.782509: Validation loss did not improve from -0.56956. Patience: 13/50
2024-12-08 19:15:03.783501: train_loss -0.721
2024-12-08 19:15:03.784357: val_loss -0.5431
2024-12-08 19:15:03.785126: Pseudo dice [0.7467]
2024-12-08 19:15:03.785860: Epoch time: 89.87 s
2024-12-08 19:15:03.786698: Yayy! New best EMA pseudo Dice: 0.7461
2024-12-08 19:15:05.328290: 
2024-12-08 19:15:05.329897: Epoch 102
2024-12-08 19:15:05.330760: Current learning rate: 0.00908
2024-12-08 19:16:35.030681: Validation loss did not improve from -0.56956. Patience: 14/50
2024-12-08 19:16:35.032018: train_loss -0.7213
2024-12-08 19:16:35.033005: val_loss -0.5183
2024-12-08 19:16:35.033836: Pseudo dice [0.7298]
2024-12-08 19:16:35.034615: Epoch time: 89.7 s
2024-12-08 19:16:36.237143: 
2024-12-08 19:16:36.238958: Epoch 103
2024-12-08 19:16:36.239934: Current learning rate: 0.00907
2024-12-08 19:18:06.206287: Validation loss improved from -0.56956 to -0.58342! Patience: 14/50
2024-12-08 19:18:06.207376: train_loss -0.7149
2024-12-08 19:18:06.208643: val_loss -0.5834
2024-12-08 19:18:06.209576: Pseudo dice [0.7662]
2024-12-08 19:18:06.210453: Epoch time: 89.97 s
2024-12-08 19:18:06.211296: Yayy! New best EMA pseudo Dice: 0.7467
2024-12-08 19:18:07.796981: 
2024-12-08 19:18:07.798720: Epoch 104
2024-12-08 19:18:07.799748: Current learning rate: 0.00906
2024-12-08 19:19:37.693779: Validation loss did not improve from -0.58342. Patience: 1/50
2024-12-08 19:19:37.694936: train_loss -0.7208
2024-12-08 19:19:37.695690: val_loss -0.5284
2024-12-08 19:19:37.696428: Pseudo dice [0.7356]
2024-12-08 19:19:37.697101: Epoch time: 89.9 s
2024-12-08 19:19:39.323505: 
2024-12-08 19:19:39.324822: Epoch 105
2024-12-08 19:19:39.325571: Current learning rate: 0.00905
2024-12-08 19:21:08.863571: Validation loss did not improve from -0.58342. Patience: 2/50
2024-12-08 19:21:08.864642: train_loss -0.7231
2024-12-08 19:21:08.865978: val_loss -0.5456
2024-12-08 19:21:08.866973: Pseudo dice [0.7384]
2024-12-08 19:21:08.867869: Epoch time: 89.54 s
2024-12-08 19:21:10.523014: 
2024-12-08 19:21:10.523995: Epoch 106
2024-12-08 19:21:10.524755: Current learning rate: 0.00904
2024-12-08 19:22:39.905645: Validation loss did not improve from -0.58342. Patience: 3/50
2024-12-08 19:22:39.906795: train_loss -0.7206
2024-12-08 19:22:39.907844: val_loss -0.538
2024-12-08 19:22:39.908712: Pseudo dice [0.7397]
2024-12-08 19:22:39.909415: Epoch time: 89.38 s
2024-12-08 19:22:41.101869: 
2024-12-08 19:22:41.103724: Epoch 107
2024-12-08 19:22:41.104684: Current learning rate: 0.00903
2024-12-08 19:24:10.252481: Validation loss did not improve from -0.58342. Patience: 4/50
2024-12-08 19:24:10.253657: train_loss -0.7181
2024-12-08 19:24:10.254439: val_loss -0.5152
2024-12-08 19:24:10.255120: Pseudo dice [0.7359]
2024-12-08 19:24:10.255779: Epoch time: 89.15 s
2024-12-08 19:24:11.476190: 
2024-12-08 19:24:11.477716: Epoch 108
2024-12-08 19:24:11.478821: Current learning rate: 0.00902
2024-12-08 19:25:40.754567: Validation loss did not improve from -0.58342. Patience: 5/50
2024-12-08 19:25:40.755774: train_loss -0.7285
2024-12-08 19:25:40.757406: val_loss -0.5573
2024-12-08 19:25:40.758576: Pseudo dice [0.7557]
2024-12-08 19:25:40.759495: Epoch time: 89.28 s
2024-12-08 19:25:42.048315: 
2024-12-08 19:25:42.049889: Epoch 109
2024-12-08 19:25:42.050801: Current learning rate: 0.00901
2024-12-08 19:27:11.171951: Validation loss did not improve from -0.58342. Patience: 6/50
2024-12-08 19:27:11.172668: train_loss -0.7234
2024-12-08 19:27:11.173609: val_loss -0.5488
2024-12-08 19:27:11.174480: Pseudo dice [0.7532]
2024-12-08 19:27:11.175281: Epoch time: 89.13 s
2024-12-08 19:27:12.750441: 
2024-12-08 19:27:12.751967: Epoch 110
2024-12-08 19:27:12.752742: Current learning rate: 0.009
2024-12-08 19:28:41.932956: Validation loss did not improve from -0.58342. Patience: 7/50
2024-12-08 19:28:41.934213: train_loss -0.7198
2024-12-08 19:28:41.935188: val_loss -0.5526
2024-12-08 19:28:41.935940: Pseudo dice [0.7454]
2024-12-08 19:28:41.936635: Epoch time: 89.18 s
2024-12-08 19:28:43.140128: 
2024-12-08 19:28:43.141762: Epoch 111
2024-12-08 19:28:43.142655: Current learning rate: 0.009
2024-12-08 19:30:12.204719: Validation loss did not improve from -0.58342. Patience: 8/50
2024-12-08 19:30:12.205905: train_loss -0.7273
2024-12-08 19:30:12.206804: val_loss -0.5462
2024-12-08 19:30:12.207402: Pseudo dice [0.7415]
2024-12-08 19:30:12.207997: Epoch time: 89.07 s
2024-12-08 19:30:13.437924: 
2024-12-08 19:30:13.439287: Epoch 112
2024-12-08 19:30:13.440113: Current learning rate: 0.00899
2024-12-08 19:31:42.480666: Validation loss did not improve from -0.58342. Patience: 9/50
2024-12-08 19:31:42.481851: train_loss -0.7335
2024-12-08 19:31:42.482683: val_loss -0.5733
2024-12-08 19:31:42.483452: Pseudo dice [0.7535]
2024-12-08 19:31:42.484211: Epoch time: 89.04 s
2024-12-08 19:31:43.719504: 
2024-12-08 19:31:43.721102: Epoch 113
2024-12-08 19:31:43.722082: Current learning rate: 0.00898
2024-12-08 19:33:12.775607: Validation loss did not improve from -0.58342. Patience: 10/50
2024-12-08 19:33:12.776361: train_loss -0.7359
2024-12-08 19:33:12.777279: val_loss -0.49
2024-12-08 19:33:12.777953: Pseudo dice [0.7269]
2024-12-08 19:33:12.778654: Epoch time: 89.06 s
2024-12-08 19:33:14.288270: 
2024-12-08 19:33:14.290205: Epoch 114
2024-12-08 19:33:14.291152: Current learning rate: 0.00897
2024-12-08 19:34:43.537004: Validation loss did not improve from -0.58342. Patience: 11/50
2024-12-08 19:34:43.538028: train_loss -0.7311
2024-12-08 19:34:43.538986: val_loss -0.5128
2024-12-08 19:34:43.539681: Pseudo dice [0.7242]
2024-12-08 19:34:43.540465: Epoch time: 89.25 s
2024-12-08 19:34:45.076554: 
2024-12-08 19:34:45.078368: Epoch 115
2024-12-08 19:34:45.079476: Current learning rate: 0.00896
2024-12-08 19:36:14.525556: Validation loss did not improve from -0.58342. Patience: 12/50
2024-12-08 19:36:14.526553: train_loss -0.7334
2024-12-08 19:36:14.527358: val_loss -0.5608
2024-12-08 19:36:14.528117: Pseudo dice [0.7575]
2024-12-08 19:36:14.529133: Epoch time: 89.45 s
2024-12-08 19:36:15.745684: 
2024-12-08 19:36:15.746953: Epoch 116
2024-12-08 19:36:15.747730: Current learning rate: 0.00895
2024-12-08 19:37:45.073761: Validation loss did not improve from -0.58342. Patience: 13/50
2024-12-08 19:37:45.075079: train_loss -0.7358
2024-12-08 19:37:45.076091: val_loss -0.5373
2024-12-08 19:37:45.076825: Pseudo dice [0.7426]
2024-12-08 19:37:45.077763: Epoch time: 89.33 s
2024-12-08 19:37:46.418259: 
2024-12-08 19:37:46.419984: Epoch 117
2024-12-08 19:37:46.420957: Current learning rate: 0.00894
2024-12-08 19:39:15.644081: Validation loss did not improve from -0.58342. Patience: 14/50
2024-12-08 19:39:15.645057: train_loss -0.7262
2024-12-08 19:39:15.646028: val_loss -0.5359
2024-12-08 19:39:15.647225: Pseudo dice [0.7449]
2024-12-08 19:39:15.648285: Epoch time: 89.23 s
2024-12-08 19:39:17.302427: 
2024-12-08 19:39:17.304167: Epoch 118
2024-12-08 19:39:17.305265: Current learning rate: 0.00893
2024-12-08 19:40:46.553684: Validation loss did not improve from -0.58342. Patience: 15/50
2024-12-08 19:40:46.554984: train_loss -0.729
2024-12-08 19:40:46.555912: val_loss -0.5629
2024-12-08 19:40:46.556734: Pseudo dice [0.7508]
2024-12-08 19:40:46.557538: Epoch time: 89.25 s
2024-12-08 19:40:47.794525: 
2024-12-08 19:40:47.796077: Epoch 119
2024-12-08 19:40:47.796860: Current learning rate: 0.00892
2024-12-08 19:42:17.188144: Validation loss improved from -0.58342 to -0.58735! Patience: 15/50
2024-12-08 19:42:17.189243: train_loss -0.7321
2024-12-08 19:42:17.190178: val_loss -0.5873
2024-12-08 19:42:17.190992: Pseudo dice [0.7655]
2024-12-08 19:42:17.191820: Epoch time: 89.4 s
2024-12-08 19:42:18.831732: 
2024-12-08 19:42:18.833325: Epoch 120
2024-12-08 19:42:18.834212: Current learning rate: 0.00891
2024-12-08 19:43:48.157936: Validation loss did not improve from -0.58735. Patience: 1/50
2024-12-08 19:43:48.159204: train_loss -0.7429
2024-12-08 19:43:48.160020: val_loss -0.5369
2024-12-08 19:43:48.160722: Pseudo dice [0.7416]
2024-12-08 19:43:48.161813: Epoch time: 89.33 s
2024-12-08 19:43:49.401397: 
2024-12-08 19:43:49.403233: Epoch 121
2024-12-08 19:43:49.404115: Current learning rate: 0.0089
2024-12-08 19:45:18.872684: Validation loss did not improve from -0.58735. Patience: 2/50
2024-12-08 19:45:18.873799: train_loss -0.738
2024-12-08 19:45:18.874760: val_loss -0.5528
2024-12-08 19:45:18.875655: Pseudo dice [0.7498]
2024-12-08 19:45:18.876489: Epoch time: 89.47 s
2024-12-08 19:45:20.159914: 
2024-12-08 19:45:20.161691: Epoch 122
2024-12-08 19:45:20.162402: Current learning rate: 0.00889
2024-12-08 19:46:49.469708: Validation loss did not improve from -0.58735. Patience: 3/50
2024-12-08 19:46:49.470710: train_loss -0.7382
2024-12-08 19:46:49.471607: val_loss -0.555
2024-12-08 19:46:49.472386: Pseudo dice [0.7464]
2024-12-08 19:46:49.473167: Epoch time: 89.31 s
2024-12-08 19:46:50.696965: 
2024-12-08 19:46:50.698583: Epoch 123
2024-12-08 19:46:50.699451: Current learning rate: 0.00889
2024-12-08 19:48:19.910806: Validation loss did not improve from -0.58735. Patience: 4/50
2024-12-08 19:48:19.912042: train_loss -0.7412
2024-12-08 19:48:19.913017: val_loss -0.5574
2024-12-08 19:48:19.913764: Pseudo dice [0.7504]
2024-12-08 19:48:19.914428: Epoch time: 89.22 s
2024-12-08 19:48:19.915037: Yayy! New best EMA pseudo Dice: 0.7468
2024-12-08 19:48:21.793538: 
2024-12-08 19:48:21.795296: Epoch 124
2024-12-08 19:48:21.796070: Current learning rate: 0.00888
2024-12-08 19:49:50.831614: Validation loss did not improve from -0.58735. Patience: 5/50
2024-12-08 19:49:50.832850: train_loss -0.7427
2024-12-08 19:49:50.833763: val_loss -0.5411
2024-12-08 19:49:50.834550: Pseudo dice [0.7518]
2024-12-08 19:49:50.835269: Epoch time: 89.04 s
2024-12-08 19:49:51.202981: Yayy! New best EMA pseudo Dice: 0.7473
2024-12-08 19:49:52.783649: 
2024-12-08 19:49:52.785356: Epoch 125
2024-12-08 19:49:52.786153: Current learning rate: 0.00887
2024-12-08 19:51:22.091069: Validation loss did not improve from -0.58735. Patience: 6/50
2024-12-08 19:51:22.092347: train_loss -0.7404
2024-12-08 19:51:22.093256: val_loss -0.5417
2024-12-08 19:51:22.094187: Pseudo dice [0.7406]
2024-12-08 19:51:22.094881: Epoch time: 89.31 s
2024-12-08 19:51:23.322401: 
2024-12-08 19:51:23.324510: Epoch 126
2024-12-08 19:51:23.325443: Current learning rate: 0.00886
2024-12-08 19:52:52.964592: Validation loss did not improve from -0.58735. Patience: 7/50
2024-12-08 19:52:52.967279: train_loss -0.7367
2024-12-08 19:52:52.968675: val_loss -0.5709
2024-12-08 19:52:52.969419: Pseudo dice [0.7581]
2024-12-08 19:52:52.970202: Epoch time: 89.65 s
2024-12-08 19:52:52.970912: Yayy! New best EMA pseudo Dice: 0.7478
2024-12-08 19:52:54.584662: 
2024-12-08 19:52:54.586477: Epoch 127
2024-12-08 19:52:54.587520: Current learning rate: 0.00885
2024-12-08 19:54:24.165836: Validation loss did not improve from -0.58735. Patience: 8/50
2024-12-08 19:54:24.167676: train_loss -0.7338
2024-12-08 19:54:24.168930: val_loss -0.5776
2024-12-08 19:54:24.169703: Pseudo dice [0.7608]
2024-12-08 19:54:24.170646: Epoch time: 89.59 s
2024-12-08 19:54:24.171336: Yayy! New best EMA pseudo Dice: 0.7491
2024-12-08 19:54:26.315029: 
2024-12-08 19:54:26.316654: Epoch 128
2024-12-08 19:54:26.317348: Current learning rate: 0.00884
2024-12-08 19:55:55.586255: Validation loss did not improve from -0.58735. Patience: 9/50
2024-12-08 19:55:55.588021: train_loss -0.7297
2024-12-08 19:55:55.589533: val_loss -0.5202
2024-12-08 19:55:55.590444: Pseudo dice [0.7343]
2024-12-08 19:55:55.591162: Epoch time: 89.27 s
2024-12-08 19:55:56.909427: 
2024-12-08 19:55:56.910520: Epoch 129
2024-12-08 19:55:56.911182: Current learning rate: 0.00883
2024-12-08 19:57:26.340734: Validation loss did not improve from -0.58735. Patience: 10/50
2024-12-08 19:57:26.341541: train_loss -0.7182
2024-12-08 19:57:26.342513: val_loss -0.5637
2024-12-08 19:57:26.343324: Pseudo dice [0.7552]
2024-12-08 19:57:26.344032: Epoch time: 89.43 s
2024-12-08 19:57:27.936388: 
2024-12-08 19:57:27.938074: Epoch 130
2024-12-08 19:57:27.938874: Current learning rate: 0.00882
2024-12-08 19:58:57.425114: Validation loss did not improve from -0.58735. Patience: 11/50
2024-12-08 19:58:57.426072: train_loss -0.7254
2024-12-08 19:58:57.426876: val_loss -0.5484
2024-12-08 19:58:57.427608: Pseudo dice [0.743]
2024-12-08 19:58:57.428315: Epoch time: 89.49 s
2024-12-08 19:58:58.704005: 
2024-12-08 19:58:58.705402: Epoch 131
2024-12-08 19:58:58.706117: Current learning rate: 0.00881
2024-12-08 20:00:27.851082: Validation loss did not improve from -0.58735. Patience: 12/50
2024-12-08 20:00:27.852427: train_loss -0.7364
2024-12-08 20:00:27.853381: val_loss -0.5251
2024-12-08 20:00:27.854368: Pseudo dice [0.7387]
2024-12-08 20:00:27.855441: Epoch time: 89.15 s
2024-12-08 20:00:29.064425: 
2024-12-08 20:00:29.066285: Epoch 132
2024-12-08 20:00:29.067631: Current learning rate: 0.0088
2024-12-08 20:01:58.431149: Validation loss did not improve from -0.58735. Patience: 13/50
2024-12-08 20:01:58.432575: train_loss -0.7341
2024-12-08 20:01:58.433914: val_loss -0.5784
2024-12-08 20:01:58.435173: Pseudo dice [0.7634]
2024-12-08 20:01:58.436605: Epoch time: 89.37 s
2024-12-08 20:01:59.651218: 
2024-12-08 20:01:59.653440: Epoch 133
2024-12-08 20:01:59.654536: Current learning rate: 0.00879
2024-12-08 20:03:29.046205: Validation loss did not improve from -0.58735. Patience: 14/50
2024-12-08 20:03:29.047468: train_loss -0.7425
2024-12-08 20:03:29.048525: val_loss -0.5692
2024-12-08 20:03:29.049599: Pseudo dice [0.7541]
2024-12-08 20:03:29.050247: Epoch time: 89.4 s
2024-12-08 20:03:29.051007: Yayy! New best EMA pseudo Dice: 0.7491
2024-12-08 20:03:30.577238: 
2024-12-08 20:03:30.579379: Epoch 134
2024-12-08 20:03:30.580622: Current learning rate: 0.00879
2024-12-08 20:04:59.971981: Validation loss did not improve from -0.58735. Patience: 15/50
2024-12-08 20:04:59.973148: train_loss -0.7354
2024-12-08 20:04:59.974501: val_loss -0.549
2024-12-08 20:04:59.975777: Pseudo dice [0.7523]
2024-12-08 20:04:59.977093: Epoch time: 89.4 s
2024-12-08 20:05:00.333001: Yayy! New best EMA pseudo Dice: 0.7494
2024-12-08 20:05:01.879224: 
2024-12-08 20:05:01.881053: Epoch 135
2024-12-08 20:05:01.882267: Current learning rate: 0.00878
2024-12-08 20:06:31.387781: Validation loss did not improve from -0.58735. Patience: 16/50
2024-12-08 20:06:31.388584: train_loss -0.7418
2024-12-08 20:06:31.389272: val_loss -0.5342
2024-12-08 20:06:31.389993: Pseudo dice [0.7362]
2024-12-08 20:06:31.390810: Epoch time: 89.51 s
2024-12-08 20:06:32.598160: 
2024-12-08 20:06:32.600290: Epoch 136
2024-12-08 20:06:32.601382: Current learning rate: 0.00877
2024-12-08 20:08:02.195907: Validation loss did not improve from -0.58735. Patience: 17/50
2024-12-08 20:08:02.196981: train_loss -0.7478
2024-12-08 20:08:02.197949: val_loss -0.5436
2024-12-08 20:08:02.198623: Pseudo dice [0.7385]
2024-12-08 20:08:02.199352: Epoch time: 89.6 s
2024-12-08 20:08:03.441272: 
2024-12-08 20:08:03.443424: Epoch 137
2024-12-08 20:08:03.444593: Current learning rate: 0.00876
2024-12-08 20:09:33.211900: Validation loss did not improve from -0.58735. Patience: 18/50
2024-12-08 20:09:33.213119: train_loss -0.7436
2024-12-08 20:09:33.214240: val_loss -0.5352
2024-12-08 20:09:33.214971: Pseudo dice [0.7407]
2024-12-08 20:09:33.215707: Epoch time: 89.77 s
2024-12-08 20:09:34.409540: 
2024-12-08 20:09:34.411710: Epoch 138
2024-12-08 20:09:34.413152: Current learning rate: 0.00875
2024-12-08 20:11:04.353365: Validation loss did not improve from -0.58735. Patience: 19/50
2024-12-08 20:11:04.355085: train_loss -0.7474
2024-12-08 20:11:04.356354: val_loss -0.5716
2024-12-08 20:11:04.357198: Pseudo dice [0.7657]
2024-12-08 20:11:04.358227: Epoch time: 89.95 s
2024-12-08 20:11:06.036076: 
2024-12-08 20:11:06.037810: Epoch 139
2024-12-08 20:11:06.039096: Current learning rate: 0.00874
2024-12-08 20:12:35.804643: Validation loss did not improve from -0.58735. Patience: 20/50
2024-12-08 20:12:35.806214: train_loss -0.7461
2024-12-08 20:12:35.807345: val_loss -0.5374
2024-12-08 20:12:35.808271: Pseudo dice [0.7435]
2024-12-08 20:12:35.809053: Epoch time: 89.77 s
2024-12-08 20:12:37.337349: 
2024-12-08 20:12:37.339227: Epoch 140
2024-12-08 20:12:37.340317: Current learning rate: 0.00873
2024-12-08 20:14:07.100062: Validation loss did not improve from -0.58735. Patience: 21/50
2024-12-08 20:14:07.101045: train_loss -0.745
2024-12-08 20:14:07.101999: val_loss -0.5621
2024-12-08 20:14:07.102720: Pseudo dice [0.7506]
2024-12-08 20:14:07.103895: Epoch time: 89.76 s
2024-12-08 20:14:08.324424: 
2024-12-08 20:14:08.326629: Epoch 141
2024-12-08 20:14:08.327522: Current learning rate: 0.00872
2024-12-08 20:15:38.073813: Validation loss did not improve from -0.58735. Patience: 22/50
2024-12-08 20:15:38.075090: train_loss -0.7434
2024-12-08 20:15:38.076530: val_loss -0.5641
2024-12-08 20:15:38.077960: Pseudo dice [0.7588]
2024-12-08 20:15:38.079316: Epoch time: 89.75 s
2024-12-08 20:15:39.292962: 
2024-12-08 20:15:39.295062: Epoch 142
2024-12-08 20:15:39.296517: Current learning rate: 0.00871
2024-12-08 20:17:08.844606: Validation loss did not improve from -0.58735. Patience: 23/50
2024-12-08 20:17:08.845550: train_loss -0.744
2024-12-08 20:17:08.846258: val_loss -0.5133
2024-12-08 20:17:08.847309: Pseudo dice [0.7328]
2024-12-08 20:17:08.847969: Epoch time: 89.55 s
2024-12-08 20:17:10.064142: 
2024-12-08 20:17:10.065852: Epoch 143
2024-12-08 20:17:10.066507: Current learning rate: 0.0087
2024-12-08 20:18:39.729186: Validation loss did not improve from -0.58735. Patience: 24/50
2024-12-08 20:18:39.730412: train_loss -0.7458
2024-12-08 20:18:39.731903: val_loss -0.5653
2024-12-08 20:18:39.733149: Pseudo dice [0.7655]
2024-12-08 20:18:39.734109: Epoch time: 89.67 s
2024-12-08 20:18:40.958968: 
2024-12-08 20:18:40.960845: Epoch 144
2024-12-08 20:18:40.962012: Current learning rate: 0.00869
2024-12-08 20:20:10.990742: Validation loss did not improve from -0.58735. Patience: 25/50
2024-12-08 20:20:10.991966: train_loss -0.7477
2024-12-08 20:20:10.993275: val_loss -0.5504
2024-12-08 20:20:10.994582: Pseudo dice [0.7484]
2024-12-08 20:20:10.995890: Epoch time: 90.03 s
2024-12-08 20:20:12.568237: 
2024-12-08 20:20:12.569854: Epoch 145
2024-12-08 20:20:12.570609: Current learning rate: 0.00868
2024-12-08 20:21:42.488790: Validation loss did not improve from -0.58735. Patience: 26/50
2024-12-08 20:21:42.490058: train_loss -0.7463
2024-12-08 20:21:42.490957: val_loss -0.5503
2024-12-08 20:21:42.492100: Pseudo dice [0.7508]
2024-12-08 20:21:42.493068: Epoch time: 89.92 s
2024-12-08 20:21:42.493995: Yayy! New best EMA pseudo Dice: 0.7495
2024-12-08 20:21:44.068918: 
2024-12-08 20:21:44.070319: Epoch 146
2024-12-08 20:21:44.071010: Current learning rate: 0.00868
2024-12-08 20:23:13.839293: Validation loss did not improve from -0.58735. Patience: 27/50
2024-12-08 20:23:13.840836: train_loss -0.7521
2024-12-08 20:23:13.842262: val_loss -0.5218
2024-12-08 20:23:13.843454: Pseudo dice [0.732]
2024-12-08 20:23:13.844178: Epoch time: 89.77 s
2024-12-08 20:23:15.059833: 
2024-12-08 20:23:15.062081: Epoch 147
2024-12-08 20:23:15.063076: Current learning rate: 0.00867
2024-12-08 20:24:44.761804: Validation loss did not improve from -0.58735. Patience: 28/50
2024-12-08 20:24:44.762697: train_loss -0.7563
2024-12-08 20:24:44.763680: val_loss -0.5456
2024-12-08 20:24:44.764595: Pseudo dice [0.7513]
2024-12-08 20:24:44.765517: Epoch time: 89.7 s
2024-12-08 20:24:45.979961: 
2024-12-08 20:24:45.982053: Epoch 148
2024-12-08 20:24:45.983299: Current learning rate: 0.00866
2024-12-08 20:26:15.673290: Validation loss did not improve from -0.58735. Patience: 29/50
2024-12-08 20:26:15.674671: train_loss -0.7497
2024-12-08 20:26:15.675716: val_loss -0.5423
2024-12-08 20:26:15.676413: Pseudo dice [0.7459]
2024-12-08 20:26:15.677072: Epoch time: 89.7 s
2024-12-08 20:26:16.887185: 
2024-12-08 20:26:16.889069: Epoch 149
2024-12-08 20:26:16.890176: Current learning rate: 0.00865
2024-12-08 20:27:46.691354: Validation loss did not improve from -0.58735. Patience: 30/50
2024-12-08 20:27:46.692849: train_loss -0.7526
2024-12-08 20:27:46.693930: val_loss -0.5525
2024-12-08 20:27:46.694839: Pseudo dice [0.7606]
2024-12-08 20:27:46.696056: Epoch time: 89.81 s
2024-12-08 20:27:48.604437: 
2024-12-08 20:27:48.606728: Epoch 150
2024-12-08 20:27:48.607556: Current learning rate: 0.00864
2024-12-08 20:29:18.102236: Validation loss did not improve from -0.58735. Patience: 31/50
2024-12-08 20:29:18.103398: train_loss -0.7502
2024-12-08 20:29:18.104637: val_loss -0.5791
2024-12-08 20:29:18.105905: Pseudo dice [0.7581]
2024-12-08 20:29:18.107281: Epoch time: 89.5 s
2024-12-08 20:29:18.108379: Yayy! New best EMA pseudo Dice: 0.75
2024-12-08 20:29:19.705312: 
2024-12-08 20:29:19.706785: Epoch 151
2024-12-08 20:29:19.707674: Current learning rate: 0.00863
2024-12-08 20:30:49.269104: Validation loss did not improve from -0.58735. Patience: 32/50
2024-12-08 20:30:49.270323: train_loss -0.7493
2024-12-08 20:30:49.271280: val_loss -0.5248
2024-12-08 20:30:49.272075: Pseudo dice [0.7427]
2024-12-08 20:30:49.272979: Epoch time: 89.57 s
2024-12-08 20:30:50.560159: 
2024-12-08 20:30:50.562182: Epoch 152
2024-12-08 20:30:50.563704: Current learning rate: 0.00862
2024-12-08 20:32:20.176633: Validation loss did not improve from -0.58735. Patience: 33/50
2024-12-08 20:32:20.178230: train_loss -0.7487
2024-12-08 20:32:20.179224: val_loss -0.5448
2024-12-08 20:32:20.180320: Pseudo dice [0.745]
2024-12-08 20:32:20.181327: Epoch time: 89.62 s
2024-12-08 20:32:21.408281: 
2024-12-08 20:32:21.410350: Epoch 153
2024-12-08 20:32:21.411468: Current learning rate: 0.00861
2024-12-08 20:33:50.890418: Validation loss did not improve from -0.58735. Patience: 34/50
2024-12-08 20:33:50.891935: train_loss -0.7409
2024-12-08 20:33:50.893257: val_loss -0.5244
2024-12-08 20:33:50.894302: Pseudo dice [0.7346]
2024-12-08 20:33:50.895583: Epoch time: 89.48 s
2024-12-08 20:33:52.152494: 
2024-12-08 20:33:52.154249: Epoch 154
2024-12-08 20:33:52.155112: Current learning rate: 0.0086
2024-12-08 20:35:21.572609: Validation loss did not improve from -0.58735. Patience: 35/50
2024-12-08 20:35:21.573633: train_loss -0.7519
2024-12-08 20:35:21.574508: val_loss -0.5348
2024-12-08 20:35:21.575459: Pseudo dice [0.7378]
2024-12-08 20:35:21.576281: Epoch time: 89.42 s
2024-12-08 20:35:23.143285: 
2024-12-08 20:35:23.145479: Epoch 155
2024-12-08 20:35:23.147048: Current learning rate: 0.00859
2024-12-08 20:36:52.734738: Validation loss did not improve from -0.58735. Patience: 36/50
2024-12-08 20:36:52.736059: train_loss -0.7489
2024-12-08 20:36:52.737219: val_loss -0.5812
2024-12-08 20:36:52.738085: Pseudo dice [0.7579]
2024-12-08 20:36:52.738846: Epoch time: 89.59 s
2024-12-08 20:36:54.000830: 
2024-12-08 20:36:54.002621: Epoch 156
2024-12-08 20:36:54.003821: Current learning rate: 0.00858
2024-12-08 20:38:23.346330: Validation loss did not improve from -0.58735. Patience: 37/50
2024-12-08 20:38:23.347130: train_loss -0.7585
2024-12-08 20:38:23.348074: val_loss -0.5261
2024-12-08 20:38:23.348672: Pseudo dice [0.7476]
2024-12-08 20:38:23.349648: Epoch time: 89.35 s
2024-12-08 20:38:24.601405: 
2024-12-08 20:38:24.603565: Epoch 157
2024-12-08 20:38:24.604694: Current learning rate: 0.00858
2024-12-08 20:39:53.945770: Validation loss did not improve from -0.58735. Patience: 38/50
2024-12-08 20:39:53.947092: train_loss -0.7503
2024-12-08 20:39:53.947916: val_loss -0.5448
2024-12-08 20:39:53.948681: Pseudo dice [0.7517]
2024-12-08 20:39:53.949436: Epoch time: 89.35 s
2024-12-08 20:39:55.185169: 
2024-12-08 20:39:55.186445: Epoch 158
2024-12-08 20:39:55.187551: Current learning rate: 0.00857
2024-12-08 20:41:24.806305: Validation loss improved from -0.58735 to -0.60100! Patience: 38/50
2024-12-08 20:41:24.807639: train_loss -0.7565
2024-12-08 20:41:24.808794: val_loss -0.601
2024-12-08 20:41:24.809632: Pseudo dice [0.7733]
2024-12-08 20:41:24.810448: Epoch time: 89.62 s
2024-12-08 20:41:24.811279: Yayy! New best EMA pseudo Dice: 0.7505
2024-12-08 20:41:26.397484: 
2024-12-08 20:41:26.398959: Epoch 159
2024-12-08 20:41:26.399812: Current learning rate: 0.00856
2024-12-08 20:42:56.086546: Validation loss did not improve from -0.60100. Patience: 1/50
2024-12-08 20:42:56.087654: train_loss -0.7544
2024-12-08 20:42:56.088697: val_loss -0.5524
2024-12-08 20:42:56.089521: Pseudo dice [0.7512]
2024-12-08 20:42:56.090372: Epoch time: 89.69 s
2024-12-08 20:42:56.439177: Yayy! New best EMA pseudo Dice: 0.7506
2024-12-08 20:42:58.334958: 
2024-12-08 20:42:58.336779: Epoch 160
2024-12-08 20:42:58.338249: Current learning rate: 0.00855
2024-12-08 20:44:27.997861: Validation loss did not improve from -0.60100. Patience: 2/50
2024-12-08 20:44:27.999205: train_loss -0.7533
2024-12-08 20:44:28.000362: val_loss -0.5146
2024-12-08 20:44:28.001352: Pseudo dice [0.7319]
2024-12-08 20:44:28.002563: Epoch time: 89.67 s
2024-12-08 20:44:29.251800: 
2024-12-08 20:44:29.253906: Epoch 161
2024-12-08 20:44:29.255195: Current learning rate: 0.00854
2024-12-08 20:45:58.883838: Validation loss did not improve from -0.60100. Patience: 3/50
2024-12-08 20:45:58.885076: train_loss -0.7598
2024-12-08 20:45:58.886507: val_loss -0.5509
2024-12-08 20:45:58.887732: Pseudo dice [0.7542]
2024-12-08 20:45:58.888462: Epoch time: 89.63 s
2024-12-08 20:46:00.126823: 
2024-12-08 20:46:00.128988: Epoch 162
2024-12-08 20:46:00.129973: Current learning rate: 0.00853
2024-12-08 20:47:29.683561: Validation loss did not improve from -0.60100. Patience: 4/50
2024-12-08 20:47:29.684555: train_loss -0.7578
2024-12-08 20:47:29.685510: val_loss -0.5702
2024-12-08 20:47:29.686396: Pseudo dice [0.7517]
2024-12-08 20:47:29.687395: Epoch time: 89.56 s
2024-12-08 20:47:30.968979: 
2024-12-08 20:47:30.970400: Epoch 163
2024-12-08 20:47:30.971430: Current learning rate: 0.00852
2024-12-08 20:49:00.402612: Validation loss did not improve from -0.60100. Patience: 5/50
2024-12-08 20:49:00.403837: train_loss -0.764
2024-12-08 20:49:00.405250: val_loss -0.5448
2024-12-08 20:49:00.406277: Pseudo dice [0.7502]
2024-12-08 20:49:00.407065: Epoch time: 89.44 s
2024-12-08 20:49:01.656736: 
2024-12-08 20:49:01.658561: Epoch 164
2024-12-08 20:49:01.659892: Current learning rate: 0.00851
2024-12-08 20:50:31.198755: Validation loss did not improve from -0.60100. Patience: 6/50
2024-12-08 20:50:31.199560: train_loss -0.7549
2024-12-08 20:50:31.201068: val_loss -0.5315
2024-12-08 20:50:31.202518: Pseudo dice [0.7396]
2024-12-08 20:50:31.203683: Epoch time: 89.54 s
2024-12-08 20:50:32.745661: 
2024-12-08 20:50:32.747778: Epoch 165
2024-12-08 20:50:32.748995: Current learning rate: 0.0085
2024-12-08 20:52:02.186960: Validation loss did not improve from -0.60100. Patience: 7/50
2024-12-08 20:52:02.187838: train_loss -0.7485
2024-12-08 20:52:02.188752: val_loss -0.5132
2024-12-08 20:52:02.189415: Pseudo dice [0.739]
2024-12-08 20:52:02.190018: Epoch time: 89.44 s
2024-12-08 20:52:03.382050: 
2024-12-08 20:52:03.384874: Epoch 166
2024-12-08 20:52:03.385987: Current learning rate: 0.00849
2024-12-08 20:53:33.150897: Validation loss did not improve from -0.60100. Patience: 8/50
2024-12-08 20:53:33.152054: train_loss -0.7309
2024-12-08 20:53:33.152933: val_loss -0.5647
2024-12-08 20:53:33.153706: Pseudo dice [0.7533]
2024-12-08 20:53:33.154655: Epoch time: 89.77 s
2024-12-08 20:53:34.363904: 
2024-12-08 20:53:34.365826: Epoch 167
2024-12-08 20:53:34.367230: Current learning rate: 0.00848
2024-12-08 20:55:03.990106: Validation loss did not improve from -0.60100. Patience: 9/50
2024-12-08 20:55:03.991286: train_loss -0.7519
2024-12-08 20:55:03.992344: val_loss -0.5582
2024-12-08 20:55:03.993118: Pseudo dice [0.7537]
2024-12-08 20:55:03.993837: Epoch time: 89.63 s
2024-12-08 20:55:05.224527: 
2024-12-08 20:55:05.226307: Epoch 168
2024-12-08 20:55:05.227026: Current learning rate: 0.00847
2024-12-08 20:56:34.603256: Validation loss did not improve from -0.60100. Patience: 10/50
2024-12-08 20:56:34.604810: train_loss -0.7563
2024-12-08 20:56:34.605745: val_loss -0.5495
2024-12-08 20:56:34.606278: Pseudo dice [0.7505]
2024-12-08 20:56:34.607282: Epoch time: 89.38 s
2024-12-08 20:56:35.839529: 
2024-12-08 20:56:35.841379: Epoch 169
2024-12-08 20:56:35.842534: Current learning rate: 0.00847
2024-12-08 20:58:05.231013: Validation loss did not improve from -0.60100. Patience: 11/50
2024-12-08 20:58:05.232160: train_loss -0.7601
2024-12-08 20:58:05.233143: val_loss -0.533
2024-12-08 20:58:05.234064: Pseudo dice [0.7369]
2024-12-08 20:58:05.235017: Epoch time: 89.39 s
2024-12-08 20:58:07.179140: 
2024-12-08 20:58:07.180748: Epoch 170
2024-12-08 20:58:07.181900: Current learning rate: 0.00846
2024-12-08 20:59:36.455759: Validation loss did not improve from -0.60100. Patience: 12/50
2024-12-08 20:59:36.457113: train_loss -0.7682
2024-12-08 20:59:36.458840: val_loss -0.536
2024-12-08 20:59:36.460179: Pseudo dice [0.7471]
2024-12-08 20:59:36.461262: Epoch time: 89.28 s
2024-12-08 20:59:37.673700: 
2024-12-08 20:59:37.676283: Epoch 171
2024-12-08 20:59:37.677630: Current learning rate: 0.00845
2024-12-08 21:01:06.902275: Validation loss did not improve from -0.60100. Patience: 13/50
2024-12-08 21:01:06.903766: train_loss -0.7648
2024-12-08 21:01:06.904736: val_loss -0.5423
2024-12-08 21:01:06.905755: Pseudo dice [0.7504]
2024-12-08 21:01:06.906532: Epoch time: 89.23 s
2024-12-08 21:01:08.131549: 
2024-12-08 21:01:08.133867: Epoch 172
2024-12-08 21:01:08.135282: Current learning rate: 0.00844
2024-12-08 21:02:37.684965: Validation loss did not improve from -0.60100. Patience: 14/50
2024-12-08 21:02:37.686542: train_loss -0.7541
2024-12-08 21:02:37.687936: val_loss -0.5679
2024-12-08 21:02:37.688780: Pseudo dice [0.7615]
2024-12-08 21:02:37.689799: Epoch time: 89.56 s
2024-12-08 21:02:38.932804: 
2024-12-08 21:02:38.935019: Epoch 173
2024-12-08 21:02:38.935978: Current learning rate: 0.00843
2024-12-08 21:04:08.623242: Validation loss did not improve from -0.60100. Patience: 15/50
2024-12-08 21:04:08.637683: train_loss -0.7526
2024-12-08 21:04:08.639450: val_loss -0.5448
2024-12-08 21:04:08.640760: Pseudo dice [0.7501]
2024-12-08 21:04:08.641851: Epoch time: 89.71 s
2024-12-08 21:04:09.897657: 
2024-12-08 21:04:09.899273: Epoch 174
2024-12-08 21:04:09.900165: Current learning rate: 0.00842
2024-12-08 21:05:39.415873: Validation loss did not improve from -0.60100. Patience: 16/50
2024-12-08 21:05:39.417039: train_loss -0.7537
2024-12-08 21:05:39.418046: val_loss -0.5442
2024-12-08 21:05:39.418785: Pseudo dice [0.7432]
2024-12-08 21:05:39.419539: Epoch time: 89.52 s
2024-12-08 21:05:41.008703: 
2024-12-08 21:05:41.009938: Epoch 175
2024-12-08 21:05:41.010998: Current learning rate: 0.00841
2024-12-08 21:07:10.646513: Validation loss did not improve from -0.60100. Patience: 17/50
2024-12-08 21:07:10.647533: train_loss -0.7599
2024-12-08 21:07:10.648555: val_loss -0.5498
2024-12-08 21:07:10.649729: Pseudo dice [0.7499]
2024-12-08 21:07:10.650624: Epoch time: 89.64 s
2024-12-08 21:07:11.895294: 
2024-12-08 21:07:11.897015: Epoch 176
2024-12-08 21:07:11.897891: Current learning rate: 0.0084
2024-12-08 21:08:42.189824: Validation loss did not improve from -0.60100. Patience: 18/50
2024-12-08 21:08:42.209627: train_loss -0.7592
2024-12-08 21:08:42.212099: val_loss -0.5421
2024-12-08 21:08:42.212930: Pseudo dice [0.7448]
2024-12-08 21:08:42.214536: Epoch time: 90.31 s
2024-12-08 21:08:43.640492: 
2024-12-08 21:08:43.642110: Epoch 177
2024-12-08 21:08:43.642955: Current learning rate: 0.00839
2024-12-08 21:10:13.360284: Validation loss did not improve from -0.60100. Patience: 19/50
2024-12-08 21:10:13.361676: train_loss -0.7624
2024-12-08 21:10:13.362831: val_loss -0.552
2024-12-08 21:10:13.363448: Pseudo dice [0.7511]
2024-12-08 21:10:13.364153: Epoch time: 89.72 s
2024-12-08 21:10:14.595193: 
2024-12-08 21:10:14.596854: Epoch 178
2024-12-08 21:10:14.598021: Current learning rate: 0.00838
2024-12-08 21:11:44.408735: Validation loss did not improve from -0.60100. Patience: 20/50
2024-12-08 21:11:44.409743: train_loss -0.765
2024-12-08 21:11:44.410743: val_loss -0.5602
2024-12-08 21:11:44.411673: Pseudo dice [0.7446]
2024-12-08 21:11:44.412478: Epoch time: 89.82 s
2024-12-08 21:11:45.650759: 
2024-12-08 21:11:45.653836: Epoch 179
2024-12-08 21:11:45.655467: Current learning rate: 0.00837
2024-12-08 21:13:15.376759: Validation loss did not improve from -0.60100. Patience: 21/50
2024-12-08 21:13:15.378297: train_loss -0.7644
2024-12-08 21:13:15.379983: val_loss -0.5814
2024-12-08 21:13:15.381085: Pseudo dice [0.7674]
2024-12-08 21:13:15.382086: Epoch time: 89.73 s
2024-12-08 21:13:17.133785: 
2024-12-08 21:13:17.135447: Epoch 180
2024-12-08 21:13:17.136482: Current learning rate: 0.00836
2024-12-08 21:14:46.588837: Validation loss did not improve from -0.60100. Patience: 22/50
2024-12-08 21:14:46.590338: train_loss -0.7667
2024-12-08 21:14:46.591616: val_loss -0.5724
2024-12-08 21:14:46.592684: Pseudo dice [0.7653]
2024-12-08 21:14:46.593686: Epoch time: 89.46 s
2024-12-08 21:14:46.594534: Yayy! New best EMA pseudo Dice: 0.7517
2024-12-08 21:14:49.149966: 
2024-12-08 21:14:49.152101: Epoch 181
2024-12-08 21:14:49.153269: Current learning rate: 0.00836
2024-12-08 21:16:18.631437: Validation loss did not improve from -0.60100. Patience: 23/50
2024-12-08 21:16:18.632914: train_loss -0.7691
2024-12-08 21:16:18.634459: val_loss -0.5474
2024-12-08 21:16:18.635476: Pseudo dice [0.7494]
2024-12-08 21:16:18.636561: Epoch time: 89.48 s
2024-12-08 21:16:19.882647: 
2024-12-08 21:16:19.884950: Epoch 182
2024-12-08 21:16:19.885942: Current learning rate: 0.00835
2024-12-08 21:17:49.467999: Validation loss did not improve from -0.60100. Patience: 24/50
2024-12-08 21:17:49.469256: train_loss -0.7737
2024-12-08 21:17:49.470262: val_loss -0.5558
2024-12-08 21:17:49.471251: Pseudo dice [0.7523]
2024-12-08 21:17:49.472090: Epoch time: 89.59 s
2024-12-08 21:17:50.697104: 
2024-12-08 21:17:50.699138: Epoch 183
2024-12-08 21:17:50.700501: Current learning rate: 0.00834
2024-12-08 21:19:20.223052: Validation loss did not improve from -0.60100. Patience: 25/50
2024-12-08 21:19:20.224416: train_loss -0.7686
2024-12-08 21:19:20.225365: val_loss -0.5143
2024-12-08 21:19:20.226539: Pseudo dice [0.7299]
2024-12-08 21:19:20.227738: Epoch time: 89.53 s
2024-12-08 21:19:21.463327: 
2024-12-08 21:19:21.465043: Epoch 184
2024-12-08 21:19:21.466124: Current learning rate: 0.00833
2024-12-08 21:20:51.013297: Validation loss did not improve from -0.60100. Patience: 26/50
2024-12-08 21:20:51.014432: train_loss -0.7645
2024-12-08 21:20:51.015568: val_loss -0.5229
2024-12-08 21:20:51.016202: Pseudo dice [0.7376]
2024-12-08 21:20:51.016824: Epoch time: 89.55 s
2024-12-08 21:20:52.601307: 
2024-12-08 21:20:52.603312: Epoch 185
2024-12-08 21:20:52.604448: Current learning rate: 0.00832
2024-12-08 21:22:21.928062: Validation loss did not improve from -0.60100. Patience: 27/50
2024-12-08 21:22:21.929582: train_loss -0.766
2024-12-08 21:22:21.931479: val_loss -0.55
2024-12-08 21:22:21.932234: Pseudo dice [0.7485]
2024-12-08 21:22:21.933449: Epoch time: 89.33 s
2024-12-08 21:22:23.133006: 
2024-12-08 21:22:23.134984: Epoch 186
2024-12-08 21:22:23.136184: Current learning rate: 0.00831
2024-12-08 21:23:52.469394: Validation loss did not improve from -0.60100. Patience: 28/50
2024-12-08 21:23:52.471138: train_loss -0.7702
2024-12-08 21:23:52.472833: val_loss -0.5491
2024-12-08 21:23:52.474079: Pseudo dice [0.7493]
2024-12-08 21:23:52.475296: Epoch time: 89.34 s
2024-12-08 21:23:53.714692: 
2024-12-08 21:23:53.717028: Epoch 187
2024-12-08 21:23:53.717946: Current learning rate: 0.0083
2024-12-08 21:25:23.110910: Validation loss did not improve from -0.60100. Patience: 29/50
2024-12-08 21:25:23.112325: train_loss -0.7669
2024-12-08 21:25:23.114091: val_loss -0.5424
2024-12-08 21:25:23.114923: Pseudo dice [0.7413]
2024-12-08 21:25:23.115698: Epoch time: 89.4 s
2024-12-08 21:25:24.346192: 
2024-12-08 21:25:24.348068: Epoch 188
2024-12-08 21:25:24.348999: Current learning rate: 0.00829
2024-12-08 21:26:53.928901: Validation loss did not improve from -0.60100. Patience: 30/50
2024-12-08 21:26:53.930534: train_loss -0.7634
2024-12-08 21:26:53.932460: val_loss -0.5546
2024-12-08 21:26:53.933804: Pseudo dice [0.7504]
2024-12-08 21:26:53.935006: Epoch time: 89.59 s
2024-12-08 21:26:55.172344: 
2024-12-08 21:26:55.174547: Epoch 189
2024-12-08 21:26:55.175920: Current learning rate: 0.00828
2024-12-08 21:28:24.786337: Validation loss did not improve from -0.60100. Patience: 31/50
2024-12-08 21:28:24.787796: train_loss -0.7631
2024-12-08 21:28:24.789513: val_loss -0.5407
2024-12-08 21:28:24.790739: Pseudo dice [0.7432]
2024-12-08 21:28:24.791626: Epoch time: 89.62 s
2024-12-08 21:28:26.384493: 
2024-12-08 21:28:26.386362: Epoch 190
2024-12-08 21:28:26.387524: Current learning rate: 0.00827
2024-12-08 21:29:55.964102: Validation loss did not improve from -0.60100. Patience: 32/50
2024-12-08 21:29:55.965662: train_loss -0.7704
2024-12-08 21:29:55.966770: val_loss -0.544
2024-12-08 21:29:55.967680: Pseudo dice [0.7461]
2024-12-08 21:29:55.968577: Epoch time: 89.58 s
2024-12-08 21:29:57.259805: 
2024-12-08 21:29:57.261605: Epoch 191
2024-12-08 21:29:57.262584: Current learning rate: 0.00826
2024-12-08 21:31:26.797637: Validation loss did not improve from -0.60100. Patience: 33/50
2024-12-08 21:31:26.798427: train_loss -0.7677
2024-12-08 21:31:26.799333: val_loss -0.5403
2024-12-08 21:31:26.800261: Pseudo dice [0.7481]
2024-12-08 21:31:26.801499: Epoch time: 89.54 s
2024-12-08 21:31:28.419466: 
2024-12-08 21:31:28.421290: Epoch 192
2024-12-08 21:31:28.422006: Current learning rate: 0.00825
2024-12-08 21:32:57.863726: Validation loss did not improve from -0.60100. Patience: 34/50
2024-12-08 21:32:57.864534: train_loss -0.7715
2024-12-08 21:32:57.865795: val_loss -0.547
2024-12-08 21:32:57.867130: Pseudo dice [0.7502]
2024-12-08 21:32:57.868083: Epoch time: 89.45 s
2024-12-08 21:32:59.116582: 
2024-12-08 21:32:59.118613: Epoch 193
2024-12-08 21:32:59.119881: Current learning rate: 0.00824
2024-12-08 21:34:28.372913: Validation loss did not improve from -0.60100. Patience: 35/50
2024-12-08 21:34:28.373927: train_loss -0.7753
2024-12-08 21:34:28.375140: val_loss -0.5258
2024-12-08 21:34:28.375853: Pseudo dice [0.7414]
2024-12-08 21:34:28.376654: Epoch time: 89.26 s
2024-12-08 21:34:29.633773: 
2024-12-08 21:34:29.635487: Epoch 194
2024-12-08 21:34:29.636338: Current learning rate: 0.00824
2024-12-08 21:35:59.037591: Validation loss did not improve from -0.60100. Patience: 36/50
2024-12-08 21:35:59.038812: train_loss -0.7735
2024-12-08 21:35:59.039685: val_loss -0.5347
2024-12-08 21:35:59.040370: Pseudo dice [0.742]
2024-12-08 21:35:59.040953: Epoch time: 89.41 s
2024-12-08 21:36:00.637869: 
2024-12-08 21:36:00.639731: Epoch 195
2024-12-08 21:36:00.640821: Current learning rate: 0.00823
2024-12-08 21:37:30.050877: Validation loss did not improve from -0.60100. Patience: 37/50
2024-12-08 21:37:30.052341: train_loss -0.7637
2024-12-08 21:37:30.053271: val_loss -0.5595
2024-12-08 21:37:30.054122: Pseudo dice [0.7542]
2024-12-08 21:37:30.055060: Epoch time: 89.42 s
2024-12-08 21:37:31.270374: 
2024-12-08 21:37:31.272043: Epoch 196
2024-12-08 21:37:31.273332: Current learning rate: 0.00822
2024-12-08 21:39:00.753277: Validation loss did not improve from -0.60100. Patience: 38/50
2024-12-08 21:39:00.754414: train_loss -0.7603
2024-12-08 21:39:00.755331: val_loss -0.5606
2024-12-08 21:39:00.756031: Pseudo dice [0.751]
2024-12-08 21:39:00.756909: Epoch time: 89.49 s
2024-12-08 21:39:02.008804: 
2024-12-08 21:39:02.010649: Epoch 197
2024-12-08 21:39:02.011687: Current learning rate: 0.00821
2024-12-08 21:40:31.544451: Validation loss did not improve from -0.60100. Patience: 39/50
2024-12-08 21:40:31.545333: train_loss -0.7662
2024-12-08 21:40:31.546331: val_loss -0.5496
2024-12-08 21:40:31.547232: Pseudo dice [0.7534]
2024-12-08 21:40:31.547926: Epoch time: 89.54 s
2024-12-08 21:40:32.801682: 
2024-12-08 21:40:32.803147: Epoch 198
2024-12-08 21:40:32.804319: Current learning rate: 0.0082
2024-12-08 21:42:02.497056: Validation loss did not improve from -0.60100. Patience: 40/50
2024-12-08 21:42:02.498061: train_loss -0.7651
2024-12-08 21:42:02.499597: val_loss -0.5528
2024-12-08 21:42:02.500219: Pseudo dice [0.7453]
2024-12-08 21:42:02.500836: Epoch time: 89.7 s
2024-12-08 21:42:03.749605: 
2024-12-08 21:42:03.751466: Epoch 199
2024-12-08 21:42:03.752259: Current learning rate: 0.00819
2024-12-08 21:43:33.406533: Validation loss did not improve from -0.60100. Patience: 41/50
2024-12-08 21:43:33.407470: train_loss -0.7657
2024-12-08 21:43:33.408357: val_loss -0.5306
2024-12-08 21:43:33.409114: Pseudo dice [0.7481]
2024-12-08 21:43:33.410094: Epoch time: 89.66 s
2024-12-08 21:43:35.025130: 
2024-12-08 21:43:35.027068: Epoch 200
2024-12-08 21:43:35.028383: Current learning rate: 0.00818
2024-12-08 21:45:04.343869: Validation loss did not improve from -0.60100. Patience: 42/50
2024-12-08 21:45:04.344979: train_loss -0.7687
2024-12-08 21:45:04.346214: val_loss -0.5511
2024-12-08 21:45:04.347070: Pseudo dice [0.7597]
2024-12-08 21:45:04.347815: Epoch time: 89.32 s
2024-12-08 21:45:05.590482: 
2024-12-08 21:45:05.592253: Epoch 201
2024-12-08 21:45:05.593271: Current learning rate: 0.00817
2024-12-08 21:46:34.659682: Validation loss did not improve from -0.60100. Patience: 43/50
2024-12-08 21:46:34.660916: train_loss -0.7665
2024-12-08 21:46:34.661841: val_loss -0.5574
2024-12-08 21:46:34.662632: Pseudo dice [0.7612]
2024-12-08 21:46:34.663285: Epoch time: 89.07 s
2024-12-08 21:46:36.233049: 
2024-12-08 21:46:36.234566: Epoch 202
2024-12-08 21:46:36.235273: Current learning rate: 0.00816
2024-12-08 21:48:05.216512: Validation loss did not improve from -0.60100. Patience: 44/50
2024-12-08 21:48:05.217777: train_loss -0.768
2024-12-08 21:48:05.219277: val_loss -0.5621
2024-12-08 21:48:05.220300: Pseudo dice [0.7592]
2024-12-08 21:48:05.221210: Epoch time: 88.99 s
2024-12-08 21:48:06.482185: 
2024-12-08 21:48:06.484632: Epoch 203
2024-12-08 21:48:06.485889: Current learning rate: 0.00815
2024-12-08 21:49:35.465817: Validation loss did not improve from -0.60100. Patience: 45/50
2024-12-08 21:49:35.467148: train_loss -0.7746
2024-12-08 21:49:35.468638: val_loss -0.531
2024-12-08 21:49:35.469728: Pseudo dice [0.7429]
2024-12-08 21:49:35.470601: Epoch time: 88.99 s
2024-12-08 21:49:36.724045: 
2024-12-08 21:49:36.726187: Epoch 204
2024-12-08 21:49:36.727340: Current learning rate: 0.00814
2024-12-08 21:51:05.676828: Validation loss did not improve from -0.60100. Patience: 46/50
2024-12-08 21:51:05.678377: train_loss -0.7713
2024-12-08 21:51:05.679398: val_loss -0.5859
2024-12-08 21:51:05.680485: Pseudo dice [0.7677]
2024-12-08 21:51:05.681556: Epoch time: 88.96 s
2024-12-08 21:51:06.037937: Yayy! New best EMA pseudo Dice: 0.7521
2024-12-08 21:51:07.638680: 
2024-12-08 21:51:07.640608: Epoch 205
2024-12-08 21:51:07.641641: Current learning rate: 0.00813
2024-12-08 21:52:36.570400: Validation loss did not improve from -0.60100. Patience: 47/50
2024-12-08 21:52:36.571478: train_loss -0.7729
2024-12-08 21:52:36.572534: val_loss -0.523
2024-12-08 21:52:36.573272: Pseudo dice [0.7425]
2024-12-08 21:52:36.573956: Epoch time: 88.93 s
2024-12-08 21:52:37.798465: 
2024-12-08 21:52:37.800457: Epoch 206
2024-12-08 21:52:37.801479: Current learning rate: 0.00813
2024-12-08 21:54:06.814117: Validation loss did not improve from -0.60100. Patience: 48/50
2024-12-08 21:54:06.815063: train_loss -0.7718
2024-12-08 21:54:06.815728: val_loss -0.5158
2024-12-08 21:54:06.816731: Pseudo dice [0.7399]
2024-12-08 21:54:06.817473: Epoch time: 89.02 s
2024-12-08 21:54:08.001616: 
2024-12-08 21:54:08.004482: Epoch 207
2024-12-08 21:54:08.005803: Current learning rate: 0.00812
2024-12-08 21:55:37.139940: Validation loss did not improve from -0.60100. Patience: 49/50
2024-12-08 21:55:37.141205: train_loss -0.7701
2024-12-08 21:55:37.143227: val_loss -0.5481
2024-12-08 21:55:37.144108: Pseudo dice [0.7423]
2024-12-08 21:55:37.145014: Epoch time: 89.14 s
2024-12-08 21:55:38.324156: 
2024-12-08 21:55:38.326579: Epoch 208
2024-12-08 21:55:38.328167: Current learning rate: 0.00811
2024-12-08 21:57:07.698658: Validation loss did not improve from -0.60100. Patience: 50/50
2024-12-08 21:57:07.699549: train_loss -0.7695
2024-12-08 21:57:07.700938: val_loss -0.542
2024-12-08 21:57:07.701951: Pseudo dice [0.7344]
2024-12-08 21:57:07.702914: Epoch time: 89.38 s
2024-12-08 21:57:08.888426: Patience reached. Stopping training.
2024-12-08 21:57:09.359240: Training done.
2024-12-08 21:57:09.626733: Using splits from existing split file: /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset307_Sohee_Calcium_OCT_CrossValidation/splits_final.json
2024-12-08 21:57:09.631922: The split file contains 5 splits.
2024-12-08 21:57:09.633313: Desired fold for training: 1
2024-12-08 21:57:09.634440: This split has 6 training and 2 validation cases.
2024-12-08 21:57:09.635514: predicting 101-019
2024-12-08 21:57:09.643558: 101-019, shape torch.Size([1, 375, 498, 498]), rank 0
2024-12-08 21:58:50.183977: predicting 401-004
2024-12-08 21:58:50.214895: 401-004, shape torch.Size([1, 375, 498, 498]), rank 0
2024-12-08 22:00:40.830850: Validation complete
2024-12-08 22:00:40.831862: Mean Validation Dice:  0.7336797877922078

############################
INFO: You are using the old nnU-Net default plans. We have updated our recommendations. Please consider using those instead! Read more here: https://github.com/MIC-DKFZ/nnUNet/blob/master/documentation/resenc_presets.md
############################

Using device: cuda:0

#######################################################################
Please cite the following paper when using nnU-Net:
Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.
#######################################################################

2024-12-08 22:00:51.798025: Using torch.compile...

############################
INFO: You are using the old nnU-Net default plans. We have updated our recommendations. Please consider using those instead! Read more here: https://github.com/MIC-DKFZ/nnUNet/blob/master/documentation/resenc_presets.md
############################

Using device: cuda:0

#######################################################################
Please cite the following paper when using nnU-Net:
Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.
#######################################################################

2024-12-08 22:00:51.801460: Using torch.compile...
/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
################### Loading pretrained weights from file  /home/gridsan/nchutisilp/datasets/ModelGenesisOutputs/ModelGenesisNNUNetPretrainingV2_noNorm_correct_orientation/Converted_nnUNet_Genesis_OCT_Best.pt ###################
Below is the list of overlapping blocks in pretrained model and nnUNet architecture:
encoder.stages.0.0.convs.0.conv.weight shape torch.Size([32, 1, 3, 3, 3])
encoder.stages.0.0.convs.0.conv.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.norm.weight shape torch.Size([32])
encoder.stages.0.0.convs.0.norm.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.0.weight shape torch.Size([32, 1, 3, 3, 3])
encoder.stages.0.0.convs.0.all_modules.0.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.1.weight shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.1.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
encoder.stages.0.0.convs.1.conv.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.norm.weight shape torch.Size([32])
encoder.stages.0.0.convs.1.norm.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
encoder.stages.0.0.convs.1.all_modules.0.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.1.weight shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.1.bias shape torch.Size([32])
encoder.stages.1.0.convs.0.conv.weight shape torch.Size([64, 32, 3, 3, 3])
encoder.stages.1.0.convs.0.conv.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.norm.weight shape torch.Size([64])
encoder.stages.1.0.convs.0.norm.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.0.weight shape torch.Size([64, 32, 3, 3, 3])
encoder.stages.1.0.convs.0.all_modules.0.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.1.weight shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.1.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
encoder.stages.1.0.convs.1.conv.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.norm.weight shape torch.Size([64])
encoder.stages.1.0.convs.1.norm.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
encoder.stages.1.0.convs.1.all_modules.0.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.1.weight shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.1.bias shape torch.Size([64])
encoder.stages.2.0.convs.0.conv.weight shape torch.Size([128, 64, 3, 3, 3])
encoder.stages.2.0.convs.0.conv.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.norm.weight shape torch.Size([128])
encoder.stages.2.0.convs.0.norm.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.0.weight shape torch.Size([128, 64, 3, 3, 3])
encoder.stages.2.0.convs.0.all_modules.0.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.1.weight shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.1.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
encoder.stages.2.0.convs.1.conv.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.norm.weight shape torch.Size([128])
encoder.stages.2.0.convs.1.norm.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
encoder.stages.2.0.convs.1.all_modules.0.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.1.weight shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.1.bias shape torch.Size([128])
encoder.stages.3.0.convs.0.conv.weight shape torch.Size([256, 128, 3, 3, 3])
encoder.stages.3.0.convs.0.conv.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.norm.weight shape torch.Size([256])
encoder.stages.3.0.convs.0.norm.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.0.weight shape torch.Size([256, 128, 3, 3, 3])
encoder.stages.3.0.convs.0.all_modules.0.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.1.weight shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.1.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
encoder.stages.3.0.convs.1.conv.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.norm.weight shape torch.Size([256])
encoder.stages.3.0.convs.1.norm.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
encoder.stages.3.0.convs.1.all_modules.0.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.1.weight shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.1.bias shape torch.Size([256])
encoder.stages.4.0.convs.0.conv.weight shape torch.Size([320, 256, 3, 3, 3])
encoder.stages.4.0.convs.0.conv.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.norm.weight shape torch.Size([320])
encoder.stages.4.0.convs.0.norm.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.0.weight shape torch.Size([320, 256, 3, 3, 3])
encoder.stages.4.0.convs.0.all_modules.0.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.1.weight shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.1.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.4.0.convs.1.conv.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.norm.weight shape torch.Size([320])
encoder.stages.4.0.convs.1.norm.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.4.0.convs.1.all_modules.0.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.1.weight shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.1.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.0.conv.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.norm.weight shape torch.Size([320])
encoder.stages.5.0.convs.0.norm.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.0.all_modules.0.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.1.weight shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.1.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.1.conv.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.norm.weight shape torch.Size([320])
encoder.stages.5.0.convs.1.norm.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.1.all_modules.0.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.1.weight shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.0.0.convs.0.conv.weight shape torch.Size([32, 1, 3, 3, 3])
decoder.encoder.stages.0.0.convs.0.conv.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.norm.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.norm.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.0.weight shape torch.Size([32, 1, 3, 3, 3])
decoder.encoder.stages.0.0.convs.0.all_modules.0.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.1.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.1.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.encoder.stages.0.0.convs.1.conv.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.norm.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.norm.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.encoder.stages.0.0.convs.1.all_modules.0.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.1.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.1.bias shape torch.Size([32])
decoder.encoder.stages.1.0.convs.0.conv.weight shape torch.Size([64, 32, 3, 3, 3])
decoder.encoder.stages.1.0.convs.0.conv.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.norm.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.norm.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.0.weight shape torch.Size([64, 32, 3, 3, 3])
decoder.encoder.stages.1.0.convs.0.all_modules.0.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.1.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.1.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.encoder.stages.1.0.convs.1.conv.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.norm.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.norm.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.encoder.stages.1.0.convs.1.all_modules.0.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.1.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.1.bias shape torch.Size([64])
decoder.encoder.stages.2.0.convs.0.conv.weight shape torch.Size([128, 64, 3, 3, 3])
decoder.encoder.stages.2.0.convs.0.conv.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.norm.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.norm.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.0.weight shape torch.Size([128, 64, 3, 3, 3])
decoder.encoder.stages.2.0.convs.0.all_modules.0.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.1.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.1.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.encoder.stages.2.0.convs.1.conv.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.norm.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.norm.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.encoder.stages.2.0.convs.1.all_modules.0.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.1.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.1.bias shape torch.Size([128])
decoder.encoder.stages.3.0.convs.0.conv.weight shape torch.Size([256, 128, 3, 3, 3])
decoder.encoder.stages.3.0.convs.0.conv.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.norm.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.norm.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.0.weight shape torch.Size([256, 128, 3, 3, 3])
decoder.encoder.stages.3.0.convs.0.all_modules.0.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.1.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.1.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.encoder.stages.3.0.convs.1.conv.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.norm.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.norm.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.encoder.stages.3.0.convs.1.all_modules.0.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.1.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.1.bias shape torch.Size([256])
decoder.encoder.stages.4.0.convs.0.conv.weight shape torch.Size([320, 256, 3, 3, 3])
decoder.encoder.stages.4.0.convs.0.conv.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.norm.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.norm.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.0.weight shape torch.Size([320, 256, 3, 3, 3])
decoder.encoder.stages.4.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.4.0.convs.1.conv.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.norm.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.norm.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.4.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.0.conv.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.norm.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.norm.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.1.conv.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.norm.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.norm.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.stages.0.convs.0.conv.weight shape torch.Size([320, 640, 3, 3, 3])
decoder.stages.0.convs.0.conv.bias shape torch.Size([320])
decoder.stages.0.convs.0.norm.weight shape torch.Size([320])
decoder.stages.0.convs.0.norm.bias shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.0.weight shape torch.Size([320, 640, 3, 3, 3])
decoder.stages.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.stages.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.stages.0.convs.1.conv.bias shape torch.Size([320])
decoder.stages.0.convs.1.norm.weight shape torch.Size([320])
decoder.stages.0.convs.1.norm.bias shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.stages.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.stages.1.convs.0.conv.weight shape torch.Size([256, 512, 3, 3, 3])
decoder.stages.1.convs.0.conv.bias shape torch.Size([256])
decoder.stages.1.convs.0.norm.weight shape torch.Size([256])
decoder.stages.1.convs.0.norm.bias shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.0.weight shape torch.Size([256, 512, 3, 3, 3])
decoder.stages.1.convs.0.all_modules.0.bias shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.1.weight shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.1.bias shape torch.Size([256])
decoder.stages.1.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.stages.1.convs.1.conv.bias shape torch.Size([256])
decoder.stages.1.convs.1.norm.weight shape torch.Size([256])
decoder.stages.1.convs.1.norm.bias shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.stages.1.convs.1.all_modules.0.bias shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.1.weight shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.1.bias shape torch.Size([256])
decoder.stages.2.convs.0.conv.weight shape torch.Size([128, 256, 3, 3, 3])
decoder.stages.2.convs.0.conv.bias shape torch.Size([128])
decoder.stages.2.convs.0.norm.weight shape torch.Size([128])
decoder.stages.2.convs.0.norm.bias shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.0.weight shape torch.Size([128, 256, 3, 3, 3])
decoder.stages.2.convs.0.all_modules.0.bias shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.1.weight shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.1.bias shape torch.Size([128])
decoder.stages.2.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.stages.2.convs.1.conv.bias shape torch.Size([128])
decoder.stages.2.convs.1.norm.weight shape torch.Size([128])
decoder.stages.2.convs.1.norm.bias shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.stages.2.convs.1.all_modules.0.bias shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.1.weight shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.1.bias shape torch.Size([128])
decoder.stages.3.convs.0.conv.weight shape torch.Size([64, 128, 3, 3, 3])
decoder.stages.3.convs.0.conv.bias shape torch.Size([64])
decoder.stages.3.convs.0.norm.weight shape torch.Size([64])
decoder.stages.3.convs.0.norm.bias shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.0.weight shape torch.Size([64, 128, 3, 3, 3])
decoder.stages.3.convs.0.all_modules.0.bias shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.1.weight shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.1.bias shape torch.Size([64])
decoder.stages.3.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.stages.3.convs.1.conv.bias shape torch.Size([64])
decoder.stages.3.convs.1.norm.weight shape torch.Size([64])
decoder.stages.3.convs.1.norm.bias shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.stages.3.convs.1.all_modules.0.bias shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.1.weight shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.1.bias shape torch.Size([64])
decoder.stages.4.convs.0.conv.weight shape torch.Size([32, 64, 3, 3, 3])
decoder.stages.4.convs.0.conv.bias shape torch.Size([32])
decoder.stages.4.convs.0.norm.weight shape torch.Size([32])
decoder.stages.4.convs.0.norm.bias shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.0.weight shape torch.Size([32, 64, 3, 3, 3])
decoder.stages.4.convs.0.all_modules.0.bias shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.1.weight shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.1.bias shape torch.Size([32])
decoder.stages.4.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.stages.4.convs.1.conv.bias shape torch.Size([32])
decoder.stages.4.convs.1.norm.weight shape torch.Size([32])
decoder.stages.4.convs.1.norm.bias shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.stages.4.convs.1.all_modules.0.bias shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.1.weight shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.1.bias shape torch.Size([32])
decoder.transpconvs.0.weight shape torch.Size([320, 320, 1, 2, 2])
decoder.transpconvs.0.bias shape torch.Size([320])
decoder.transpconvs.1.weight shape torch.Size([320, 256, 2, 2, 2])
decoder.transpconvs.1.bias shape torch.Size([256])
decoder.transpconvs.2.weight shape torch.Size([256, 128, 2, 2, 2])
decoder.transpconvs.2.bias shape torch.Size([128])
decoder.transpconvs.3.weight shape torch.Size([128, 64, 2, 2, 2])
decoder.transpconvs.3.bias shape torch.Size([64])
decoder.transpconvs.4.weight shape torch.Size([64, 32, 2, 2, 2])
decoder.transpconvs.4.bias shape torch.Size([32])
################### Done ###################
2024-12-08 22:00:59.784610: do_dummy_2d_data_aug: True
2024-12-08 22:00:59.788141: Using splits from existing split file: /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset307_Sohee_Calcium_OCT_CrossValidation/splits_final.json
2024-12-08 22:00:59.789971: The split file contains 5 splits.
2024-12-08 22:00:59.791052: Desired fold for training: 4
2024-12-08 22:00:59.791947: This split has 7 training and 1 validation cases.
################### Loading pretrained weights from file  /home/gridsan/nchutisilp/datasets/ModelGenesisOutputs/ModelGenesisNNUNetPretrainingV2_noNorm_correct_orientation/Converted_nnUNet_Genesis_OCT_Best.pt ###################
Below is the list of overlapping blocks in pretrained model and nnUNet architecture:
encoder.stages.0.0.convs.0.conv.weight shape torch.Size([32, 1, 3, 3, 3])
encoder.stages.0.0.convs.0.conv.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.norm.weight shape torch.Size([32])
encoder.stages.0.0.convs.0.norm.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.0.weight shape torch.Size([32, 1, 3, 3, 3])
encoder.stages.0.0.convs.0.all_modules.0.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.1.weight shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.1.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
encoder.stages.0.0.convs.1.conv.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.norm.weight shape torch.Size([32])
encoder.stages.0.0.convs.1.norm.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
encoder.stages.0.0.convs.1.all_modules.0.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.1.weight shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.1.bias shape torch.Size([32])
encoder.stages.1.0.convs.0.conv.weight shape torch.Size([64, 32, 3, 3, 3])
encoder.stages.1.0.convs.0.conv.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.norm.weight shape torch.Size([64])
encoder.stages.1.0.convs.0.norm.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.0.weight shape torch.Size([64, 32, 3, 3, 3])
encoder.stages.1.0.convs.0.all_modules.0.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.1.weight shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.1.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
encoder.stages.1.0.convs.1.conv.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.norm.weight shape torch.Size([64])
encoder.stages.1.0.convs.1.norm.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
encoder.stages.1.0.convs.1.all_modules.0.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.1.weight shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.1.bias shape torch.Size([64])
encoder.stages.2.0.convs.0.conv.weight shape torch.Size([128, 64, 3, 3, 3])
encoder.stages.2.0.convs.0.conv.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.norm.weight shape torch.Size([128])
encoder.stages.2.0.convs.0.norm.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.0.weight shape torch.Size([128, 64, 3, 3, 3])
encoder.stages.2.0.convs.0.all_modules.0.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.1.weight shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.1.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
encoder.stages.2.0.convs.1.conv.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.norm.weight shape torch.Size([128])
encoder.stages.2.0.convs.1.norm.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
encoder.stages.2.0.convs.1.all_modules.0.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.1.weight shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.1.bias shape torch.Size([128])
encoder.stages.3.0.convs.0.conv.weight shape torch.Size([256, 128, 3, 3, 3])
encoder.stages.3.0.convs.0.conv.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.norm.weight shape torch.Size([256])
encoder.stages.3.0.convs.0.norm.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.0.weight shape torch.Size([256, 128, 3, 3, 3])
encoder.stages.3.0.convs.0.all_modules.0.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.1.weight shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.1.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
encoder.stages.3.0.convs.1.conv.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.norm.weight shape torch.Size([256])
encoder.stages.3.0.convs.1.norm.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
encoder.stages.3.0.convs.1.all_modules.0.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.1.weight shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.1.bias shape torch.Size([256])
encoder.stages.4.0.convs.0.conv.weight shape torch.Size([320, 256, 3, 3, 3])
encoder.stages.4.0.convs.0.conv.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.norm.weight shape torch.Size([320])
encoder.stages.4.0.convs.0.norm.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.0.weight shape torch.Size([320, 256, 3, 3, 3])
encoder.stages.4.0.convs.0.all_modules.0.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.1.weight shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.1.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.4.0.convs.1.conv.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.norm.weight shape torch.Size([320])
encoder.stages.4.0.convs.1.norm.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.4.0.convs.1.all_modules.0.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.1.weight shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.1.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.0.conv.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.norm.weight shape torch.Size([320])
encoder.stages.5.0.convs.0.norm.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.0.all_modules.0.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.1.weight shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.1.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.1.conv.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.norm.weight shape torch.Size([320])
encoder.stages.5.0.convs.1.norm.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.1.all_modules.0.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.1.weight shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.0.0.convs.0.conv.weight shape torch.Size([32, 1, 3, 3, 3])
decoder.encoder.stages.0.0.convs.0.conv.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.norm.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.norm.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.0.weight shape torch.Size([32, 1, 3, 3, 3])
decoder.encoder.stages.0.0.convs.0.all_modules.0.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.1.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.1.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.encoder.stages.0.0.convs.1.conv.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.norm.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.norm.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.encoder.stages.0.0.convs.1.all_modules.0.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.1.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.1.bias shape torch.Size([32])
decoder.encoder.stages.1.0.convs.0.conv.weight shape torch.Size([64, 32, 3, 3, 3])
decoder.encoder.stages.1.0.convs.0.conv.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.norm.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.norm.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.0.weight shape torch.Size([64, 32, 3, 3, 3])
decoder.encoder.stages.1.0.convs.0.all_modules.0.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.1.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.1.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.encoder.stages.1.0.convs.1.conv.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.norm.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.norm.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.encoder.stages.1.0.convs.1.all_modules.0.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.1.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.1.bias shape torch.Size([64])
decoder.encoder.stages.2.0.convs.0.conv.weight shape torch.Size([128, 64, 3, 3, 3])
decoder.encoder.stages.2.0.convs.0.conv.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.norm.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.norm.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.0.weight shape torch.Size([128, 64, 3, 3, 3])
decoder.encoder.stages.2.0.convs.0.all_modules.0.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.1.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.1.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.encoder.stages.2.0.convs.1.conv.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.norm.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.norm.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.encoder.stages.2.0.convs.1.all_modules.0.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.1.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.1.bias shape torch.Size([128])
decoder.encoder.stages.3.0.convs.0.conv.weight shape torch.Size([256, 128, 3, 3, 3])
decoder.encoder.stages.3.0.convs.0.conv.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.norm.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.norm.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.0.weight shape torch.Size([256, 128, 3, 3, 3])
decoder.encoder.stages.3.0.convs.0.all_modules.0.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.1.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.1.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.encoder.stages.3.0.convs.1.conv.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.norm.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.norm.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.encoder.stages.3.0.convs.1.all_modules.0.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.1.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.1.bias shape torch.Size([256])
decoder.encoder.stages.4.0.convs.0.conv.weight shape torch.Size([320, 256, 3, 3, 3])
decoder.encoder.stages.4.0.convs.0.conv.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.norm.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.norm.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.0.weight shape torch.Size([320, 256, 3, 3, 3])
decoder.encoder.stages.4.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.4.0.convs.1.conv.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.norm.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.norm.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.4.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.0.conv.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.norm.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.norm.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.1.conv.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.norm.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.norm.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.stages.0.convs.0.conv.weight shape torch.Size([320, 640, 3, 3, 3])
decoder.stages.0.convs.0.conv.bias shape torch.Size([320])
decoder.stages.0.convs.0.norm.weight shape torch.Size([320])
decoder.stages.0.convs.0.norm.bias shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.0.weight shape torch.Size([320, 640, 3, 3, 3])
decoder.stages.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.stages.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.stages.0.convs.1.conv.bias shape torch.Size([320])
decoder.stages.0.convs.1.norm.weight shape torch.Size([320])
decoder.stages.0.convs.1.norm.bias shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.stages.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.stages.1.convs.0.conv.weight shape torch.Size([256, 512, 3, 3, 3])
decoder.stages.1.convs.0.conv.bias shape torch.Size([256])
decoder.stages.1.convs.0.norm.weight shape torch.Size([256])
decoder.stages.1.convs.0.norm.bias shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.0.weight shape torch.Size([256, 512, 3, 3, 3])
decoder.stages.1.convs.0.all_modules.0.bias shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.1.weight shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.1.bias shape torch.Size([256])
decoder.stages.1.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.stages.1.convs.1.conv.bias shape torch.Size([256])
decoder.stages.1.convs.1.norm.weight shape torch.Size([256])
decoder.stages.1.convs.1.norm.bias shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.stages.1.convs.1.all_modules.0.bias shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.1.weight shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.1.bias shape torch.Size([256])
decoder.stages.2.convs.0.conv.weight shape torch.Size([128, 256, 3, 3, 3])
decoder.stages.2.convs.0.conv.bias shape torch.Size([128])
decoder.stages.2.convs.0.norm.weight shape torch.Size([128])
decoder.stages.2.convs.0.norm.bias shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.0.weight shape torch.Size([128, 256, 3, 3, 3])
decoder.stages.2.convs.0.all_modules.0.bias shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.1.weight shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.1.bias shape torch.Size([128])
decoder.stages.2.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.stages.2.convs.1.conv.bias shape torch.Size([128])
decoder.stages.2.convs.1.norm.weight shape torch.Size([128])
decoder.stages.2.convs.1.norm.bias shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.stages.2.convs.1.all_modules.0.bias shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.1.weight shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.1.bias shape torch.Size([128])
decoder.stages.3.convs.0.conv.weight shape torch.Size([64, 128, 3, 3, 3])
decoder.stages.3.convs.0.conv.bias shape torch.Size([64])
decoder.stages.3.convs.0.norm.weight shape torch.Size([64])
decoder.stages.3.convs.0.norm.bias shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.0.weight shape torch.Size([64, 128, 3, 3, 3])
decoder.stages.3.convs.0.all_modules.0.bias shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.1.weight shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.1.bias shape torch.Size([64])
decoder.stages.3.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.stages.3.convs.1.conv.bias shape torch.Size([64])
decoder.stages.3.convs.1.norm.weight shape torch.Size([64])
decoder.stages.3.convs.1.norm.bias shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.stages.3.convs.1.all_modules.0.bias shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.1.weight shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.1.bias shape torch.Size([64])
decoder.stages.4.convs.0.conv.weight shape torch.Size([32, 64, 3, 3, 3])
decoder.stages.4.convs.0.conv.bias shape torch.Size([32])
decoder.stages.4.convs.0.norm.weight shape torch.Size([32])
decoder.stages.4.convs.0.norm.bias shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.0.weight shape torch.Size([32, 64, 3, 3, 3])
decoder.stages.4.convs.0.all_modules.0.bias shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.1.weight shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.1.bias shape torch.Size([32])
decoder.stages.4.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.stages.4.convs.1.conv.bias shape torch.Size([32])
decoder.stages.4.convs.1.norm.weight shape torch.Size([32])
decoder.stages.4.convs.1.norm.bias shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.stages.4.convs.1.all_modules.0.bias shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.1.weight shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.1.bias shape torch.Size([32])
decoder.transpconvs.0.weight shape torch.Size([320, 320, 1, 2, 2])
decoder.transpconvs.0.bias shape torch.Size([320])
decoder.transpconvs.1.weight shape torch.Size([320, 256, 2, 2, 2])
decoder.transpconvs.1.bias shape torch.Size([256])
decoder.transpconvs.2.weight shape torch.Size([256, 128, 2, 2, 2])
decoder.transpconvs.2.bias shape torch.Size([128])
decoder.transpconvs.3.weight shape torch.Size([128, 64, 2, 2, 2])
decoder.transpconvs.3.bias shape torch.Size([64])
decoder.transpconvs.4.weight shape torch.Size([64, 32, 2, 2, 2])
decoder.transpconvs.4.bias shape torch.Size([32])
################### Done ###################
2024-12-08 22:00:59.784649: do_dummy_2d_data_aug: True
2024-12-08 22:00:59.787845: Using splits from existing split file: /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset307_Sohee_Calcium_OCT_CrossValidation/splits_final.json
2024-12-08 22:00:59.790248: The split file contains 5 splits.
2024-12-08 22:00:59.791423: Desired fold for training: 3
2024-12-08 22:00:59.792238: This split has 7 training and 1 validation cases.
using pin_memory on device 0
using pin_memory on device 0
using pin_memory on device 0

This is the configuration used by this training:
Configuration name: 3d_32x160x128_b10
 {'data_identifier': 'nnUNetPlans_3d_fullres', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 10, 'patch_size': [32, 160, 128], 'median_image_size_in_voxels': [375.0, 498.0, 498.0], 'spacing': [1.0, 1.0, 1.0], 'normalization_schemes': ['ZScoreNormalization'], 'use_mask_for_norm': [False], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'architecture': {'network_class_name': 'dynamic_network_architectures.architectures.unet.PlainConvUNet', 'arch_kwargs': {'n_stages': 6, 'features_per_stage': [32, 64, 128, 256, 320, 320], 'conv_op': 'torch.nn.modules.conv.Conv3d', 'kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]], 'strides': [[1, 1, 1], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [1, 2, 2]], 'n_conv_per_stage': [2, 2, 2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2, 2, 2], 'conv_bias': True, 'norm_op': 'torch.nn.modules.instancenorm.InstanceNorm3d', 'norm_op_kwargs': {'eps': 1e-05, 'affine': True}, 'dropout_op': None, 'dropout_op_kwargs': None, 'nonlin': 'torch.nn.LeakyReLU', 'nonlin_kwargs': {'inplace': True}, 'deep_supervision': True}, '_kw_requires_import': ['conv_op', 'norm_op', 'dropout_op', 'nonlin']}, 'batch_dice': True, 'inherits_from': '3d_fullres'} 

These are the global plan.json settings:
 {'dataset_name': 'Dataset307_Sohee_Calcium_OCT_CrossValidation', 'plans_name': 'nnUNetPlans', 'original_median_spacing_after_transp': [1.0, 1.0, 1.0], 'original_median_shape_after_transp': [375, 498, 498], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 0.4977976381778717, 'mean': 0.13507525622844696, 'median': 0.09599608182907104, 'min': 0.0, 'percentile_00_5': 0.014754901640117168, 'percentile_99_5': 0.4977976381778717, 'std': 0.12025152146816254}}} 

2024-12-08 22:01:03.021598: unpacking dataset...
using pin_memory on device 0

This is the configuration used by this training:
Configuration name: 3d_32x160x128_b10
 {'data_identifier': 'nnUNetPlans_3d_fullres', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 10, 'patch_size': [32, 160, 128], 'median_image_size_in_voxels': [375.0, 498.0, 498.0], 'spacing': [1.0, 1.0, 1.0], 'normalization_schemes': ['ZScoreNormalization'], 'use_mask_for_norm': [False], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'architecture': {'network_class_name': 'dynamic_network_architectures.architectures.unet.PlainConvUNet', 'arch_kwargs': {'n_stages': 6, 'features_per_stage': [32, 64, 128, 256, 320, 320], 'conv_op': 'torch.nn.modules.conv.Conv3d', 'kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]], 'strides': [[1, 1, 1], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [1, 2, 2]], 'n_conv_per_stage': [2, 2, 2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2, 2, 2], 'conv_bias': True, 'norm_op': 'torch.nn.modules.instancenorm.InstanceNorm3d', 'norm_op_kwargs': {'eps': 1e-05, 'affine': True}, 'dropout_op': None, 'dropout_op_kwargs': None, 'nonlin': 'torch.nn.LeakyReLU', 'nonlin_kwargs': {'inplace': True}, 'deep_supervision': True}, '_kw_requires_import': ['conv_op', 'norm_op', 'dropout_op', 'nonlin']}, 'batch_dice': True, 'inherits_from': '3d_fullres'} 

These are the global plan.json settings:
 {'dataset_name': 'Dataset307_Sohee_Calcium_OCT_CrossValidation', 'plans_name': 'nnUNetPlans', 'original_median_spacing_after_transp': [1.0, 1.0, 1.0], 'original_median_shape_after_transp': [375, 498, 498], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 0.4977976381778717, 'mean': 0.13507525622844696, 'median': 0.09599608182907104, 'min': 0.0, 'percentile_00_5': 0.014754901640117168, 'percentile_99_5': 0.4977976381778717, 'std': 0.12025152146816254}}} 

2024-12-08 22:01:03.313215: unpacking dataset...
2024-12-08 22:01:07.393626: unpacking done...
2024-12-08 22:01:07.611270: Unable to plot network architecture: nnUNet_compile is enabled!
2024-12-08 22:01:07.706731: 
2024-12-08 22:01:07.708245: Epoch 0
2024-12-08 22:01:07.710565: Current learning rate: 0.01
2024-12-08 22:03:33.144058: Validation loss improved from 1000.00000 to -0.20106! Patience: 0/50
2024-12-08 22:03:33.145564: train_loss -0.0982
2024-12-08 22:03:33.146597: val_loss -0.2011
2024-12-08 22:03:33.147462: Pseudo dice [0.5666]
2024-12-08 22:03:33.148198: Epoch time: 145.44 s
2024-12-08 22:03:33.148958: Yayy! New best EMA pseudo Dice: 0.5666
2024-12-08 22:03:34.821232: 
2024-12-08 22:03:34.823164: Epoch 1
2024-12-08 22:03:34.824328: Current learning rate: 0.00999
2024-12-08 22:05:01.764184: Validation loss did not improve from -0.20106. Patience: 1/50
2024-12-08 22:05:01.765347: train_loss -0.2939
2024-12-08 22:05:01.766249: val_loss -0.0791
2024-12-08 22:05:01.766949: Pseudo dice [0.413]
2024-12-08 22:05:01.767590: Epoch time: 86.95 s
2024-12-08 22:05:03.021012: 
2024-12-08 22:05:03.022573: Epoch 2
2024-12-08 22:05:03.023486: Current learning rate: 0.00998
2024-12-08 22:06:30.598665: Validation loss improved from -0.20106 to -0.28657! Patience: 1/50
2024-12-08 22:06:30.599637: train_loss -0.3579
2024-12-08 22:06:30.600344: val_loss -0.2866
2024-12-08 22:06:30.601013: Pseudo dice [0.6163]
2024-12-08 22:06:30.601755: Epoch time: 87.58 s
2024-12-08 22:06:32.009044: 
2024-12-08 22:06:32.010411: Epoch 3
2024-12-08 22:06:32.011340: Current learning rate: 0.00997
2024-12-08 22:07:59.552678: Validation loss did not improve from -0.28657. Patience: 1/50
2024-12-08 22:07:59.553865: train_loss -0.3934
2024-12-08 22:07:59.554733: val_loss -0.2709
2024-12-08 22:07:59.555466: Pseudo dice [0.564]
2024-12-08 22:07:59.556087: Epoch time: 87.55 s
2024-12-08 22:08:00.886734: 
2024-12-08 22:08:00.888070: Epoch 4
2024-12-08 22:08:00.888844: Current learning rate: 0.00996
2024-12-08 22:09:28.547195: Validation loss improved from -0.28657 to -0.32685! Patience: 1/50
2024-12-08 22:09:28.548369: train_loss -0.4285
2024-12-08 22:09:28.549545: val_loss -0.3269
2024-12-08 22:09:28.550525: Pseudo dice [0.6429]
2024-12-08 22:09:28.551532: Epoch time: 87.66 s
2024-12-08 22:09:28.898104: Yayy! New best EMA pseudo Dice: 0.5668
2024-12-08 22:09:30.597270: 
2024-12-08 22:09:30.598569: Epoch 5
2024-12-08 22:09:30.599249: Current learning rate: 0.00995
2024-12-08 22:10:58.212879: Validation loss improved from -0.32685 to -0.34658! Patience: 0/50
2024-12-08 22:10:58.214334: train_loss -0.4451
2024-12-08 22:10:58.215559: val_loss -0.3466
2024-12-08 22:10:58.216602: Pseudo dice [0.6434]
2024-12-08 22:10:58.217700: Epoch time: 87.62 s
2024-12-08 22:10:58.218729: Yayy! New best EMA pseudo Dice: 0.5745
2024-12-08 22:10:59.903767: 
2024-12-08 22:10:59.905225: Epoch 6
2024-12-08 22:10:59.906335: Current learning rate: 0.00995
2024-12-08 22:12:27.292289: Validation loss did not improve from -0.34658. Patience: 1/50
2024-12-08 22:12:27.293406: train_loss -0.4792
2024-12-08 22:12:27.294258: val_loss -0.3365
2024-12-08 22:12:27.295022: Pseudo dice [0.6417]
2024-12-08 22:12:27.295888: Epoch time: 87.39 s
2024-12-08 22:12:27.296521: Yayy! New best EMA pseudo Dice: 0.5812
2024-12-08 22:12:28.971298: 
2024-12-08 22:12:28.972821: Epoch 7
2024-12-08 22:12:28.973815: Current learning rate: 0.00994
2024-12-08 22:13:56.420482: Validation loss did not improve from -0.34658. Patience: 2/50
2024-12-08 22:13:56.421732: train_loss -0.4945
2024-12-08 22:13:56.422796: val_loss -0.2765
2024-12-08 22:13:56.423610: Pseudo dice [0.6099]
2024-12-08 22:13:56.424226: Epoch time: 87.45 s
2024-12-08 22:13:56.425014: Yayy! New best EMA pseudo Dice: 0.5841
2024-12-08 22:13:58.152044: 
2024-12-08 22:13:58.153541: Epoch 8
2024-12-08 22:13:58.154419: Current learning rate: 0.00993
2024-12-08 22:15:26.009016: Validation loss did not improve from -0.34658. Patience: 3/50
2024-12-08 22:15:26.010206: train_loss -0.4925
2024-12-08 22:15:26.011608: val_loss -0.2684
2024-12-08 22:15:26.012693: Pseudo dice [0.5775]
2024-12-08 22:15:26.013608: Epoch time: 87.86 s
2024-12-08 22:15:27.971916: 
2024-12-08 22:15:27.973649: Epoch 9
2024-12-08 22:15:27.974562: Current learning rate: 0.00992
2024-12-08 22:16:55.905246: Validation loss improved from -0.34658 to -0.37907! Patience: 3/50
2024-12-08 22:16:55.906412: train_loss -0.5072
2024-12-08 22:16:55.907320: val_loss -0.3791
2024-12-08 22:16:55.908056: Pseudo dice [0.6606]
2024-12-08 22:16:55.908960: Epoch time: 87.94 s
2024-12-08 22:16:56.301055: Yayy! New best EMA pseudo Dice: 0.5911
2024-12-08 22:16:57.900238: 
2024-12-08 22:16:57.901954: Epoch 10
2024-12-08 22:16:57.902776: Current learning rate: 0.00991
2024-12-08 22:18:25.758348: Validation loss did not improve from -0.37907. Patience: 1/50
2024-12-08 22:18:25.759539: train_loss -0.5161
2024-12-08 22:18:25.760501: val_loss -0.295
2024-12-08 22:18:25.761206: Pseudo dice [0.6076]
2024-12-08 22:18:25.761990: Epoch time: 87.86 s
2024-12-08 22:18:25.762736: Yayy! New best EMA pseudo Dice: 0.5928
2024-12-08 22:18:27.383804: 
2024-12-08 22:18:27.385561: Epoch 11
2024-12-08 22:18:27.386470: Current learning rate: 0.0099
2024-12-08 22:19:55.331633: Validation loss improved from -0.37907 to -0.45198! Patience: 1/50
2024-12-08 22:19:55.332634: train_loss -0.517
2024-12-08 22:19:55.333821: val_loss -0.452
2024-12-08 22:19:55.334851: Pseudo dice [0.7033]
2024-12-08 22:19:55.335712: Epoch time: 87.95 s
2024-12-08 22:19:55.336677: Yayy! New best EMA pseudo Dice: 0.6038
2024-12-08 22:19:56.990483: 
2024-12-08 22:19:56.992408: Epoch 12
2024-12-08 22:19:56.993409: Current learning rate: 0.00989
2024-12-08 22:21:24.858388: Validation loss did not improve from -0.45198. Patience: 1/50
2024-12-08 22:21:24.859109: train_loss -0.5282
2024-12-08 22:21:24.860242: val_loss -0.3675
2024-12-08 22:21:24.861551: Pseudo dice [0.6429]
2024-12-08 22:21:24.862639: Epoch time: 87.87 s
2024-12-08 22:21:24.863699: Yayy! New best EMA pseudo Dice: 0.6077
2024-12-08 22:21:26.560564: 
2024-12-08 22:21:26.562231: Epoch 13
2024-12-08 22:21:26.563256: Current learning rate: 0.00988
2024-12-08 22:22:54.419641: Validation loss did not improve from -0.45198. Patience: 2/50
2024-12-08 22:22:54.420979: train_loss -0.5364
2024-12-08 22:22:54.422247: val_loss -0.3689
2024-12-08 22:22:54.423220: Pseudo dice [0.661]
2024-12-08 22:22:54.423895: Epoch time: 87.86 s
2024-12-08 22:22:54.424667: Yayy! New best EMA pseudo Dice: 0.6131
2024-12-08 22:22:56.095009: 
2024-12-08 22:22:56.096990: Epoch 14
2024-12-08 22:22:56.097917: Current learning rate: 0.00987
2024-12-08 22:24:24.161867: Validation loss did not improve from -0.45198. Patience: 3/50
2024-12-08 22:24:24.163327: train_loss -0.5195
2024-12-08 22:24:24.164285: val_loss -0.2881
2024-12-08 22:24:24.165053: Pseudo dice [0.6064]
2024-12-08 22:24:24.165682: Epoch time: 88.07 s
2024-12-08 22:24:25.834567: 
2024-12-08 22:24:25.836393: Epoch 15
2024-12-08 22:24:25.837121: Current learning rate: 0.00986
2024-12-08 22:25:53.821081: Validation loss did not improve from -0.45198. Patience: 4/50
2024-12-08 22:25:53.822629: train_loss -0.5446
2024-12-08 22:25:53.823900: val_loss -0.3249
2024-12-08 22:25:53.824776: Pseudo dice [0.5907]
2024-12-08 22:25:53.825629: Epoch time: 87.99 s
2024-12-08 22:25:55.157198: 
2024-12-08 22:25:55.158876: Epoch 16
2024-12-08 22:25:55.159539: Current learning rate: 0.00986
2024-12-08 22:27:23.496882: Validation loss did not improve from -0.45198. Patience: 5/50
2024-12-08 22:27:23.498404: train_loss -0.5464
2024-12-08 22:27:23.499771: val_loss -0.2539
2024-12-08 22:27:23.500798: Pseudo dice [0.5682]
2024-12-08 22:27:23.501891: Epoch time: 88.34 s
2024-12-08 22:27:24.844073: 
2024-12-08 22:27:24.845636: Epoch 17
2024-12-08 22:27:24.846507: Current learning rate: 0.00985
2024-12-08 22:28:53.305046: Validation loss did not improve from -0.45198. Patience: 6/50
2024-12-08 22:28:53.306285: train_loss -0.5609
2024-12-08 22:28:53.307057: val_loss -0.4238
2024-12-08 22:28:53.307931: Pseudo dice [0.6925]
2024-12-08 22:28:53.308635: Epoch time: 88.46 s
2024-12-08 22:28:53.309289: Yayy! New best EMA pseudo Dice: 0.6147
2024-12-08 22:28:55.023225: 
2024-12-08 22:28:55.024741: Epoch 18
2024-12-08 22:28:55.025648: Current learning rate: 0.00984
2024-12-08 22:30:23.442640: Validation loss improved from -0.45198 to -0.48247! Patience: 6/50
2024-12-08 22:30:23.443866: train_loss -0.5688
2024-12-08 22:30:23.444954: val_loss -0.4825
2024-12-08 22:30:23.445627: Pseudo dice [0.7177]
2024-12-08 22:30:23.446255: Epoch time: 88.42 s
2024-12-08 22:30:23.446861: Yayy! New best EMA pseudo Dice: 0.625
2024-12-08 22:30:25.531261: 
2024-12-08 22:30:25.532911: Epoch 19
2024-12-08 22:30:25.533608: Current learning rate: 0.00983
2024-12-08 22:31:53.849391: Validation loss did not improve from -0.48247. Patience: 1/50
2024-12-08 22:31:53.850799: train_loss -0.5805
2024-12-08 22:31:53.852067: val_loss -0.3281
2024-12-08 22:31:53.852940: Pseudo dice [0.6476]
2024-12-08 22:31:53.853744: Epoch time: 88.32 s
2024-12-08 22:31:54.227319: Yayy! New best EMA pseudo Dice: 0.6272
2024-12-08 22:31:55.916503: 
2024-12-08 22:31:55.918400: Epoch 20
2024-12-08 22:31:55.919251: Current learning rate: 0.00982
2024-12-08 22:33:24.235092: Validation loss did not improve from -0.48247. Patience: 2/50
2024-12-08 22:33:24.236444: train_loss -0.5699
2024-12-08 22:33:24.237468: val_loss -0.4091
2024-12-08 22:33:24.238517: Pseudo dice [0.691]
2024-12-08 22:33:24.239401: Epoch time: 88.32 s
2024-12-08 22:33:24.240306: Yayy! New best EMA pseudo Dice: 0.6336
2024-12-08 22:33:25.950033: 
2024-12-08 22:33:25.951661: Epoch 21
2024-12-08 22:33:25.952656: Current learning rate: 0.00981
2024-12-08 22:34:54.212310: Validation loss did not improve from -0.48247. Patience: 3/50
2024-12-08 22:34:54.213521: train_loss -0.5868
2024-12-08 22:34:54.214309: val_loss -0.414
2024-12-08 22:34:54.214926: Pseudo dice [0.6785]
2024-12-08 22:34:54.215754: Epoch time: 88.26 s
2024-12-08 22:34:54.216516: Yayy! New best EMA pseudo Dice: 0.6381
2024-12-08 22:34:55.816925: 
2024-12-08 22:34:55.818485: Epoch 22
2024-12-08 22:34:55.819248: Current learning rate: 0.0098
2024-12-08 22:36:24.022176: Validation loss did not improve from -0.48247. Patience: 4/50
2024-12-08 22:36:24.023478: train_loss -0.5889
2024-12-08 22:36:24.024633: val_loss -0.3108
2024-12-08 22:36:24.025541: Pseudo dice [0.6062]
2024-12-08 22:36:24.026463: Epoch time: 88.21 s
2024-12-08 22:36:25.328468: 
2024-12-08 22:36:25.329758: Epoch 23
2024-12-08 22:36:25.330701: Current learning rate: 0.00979
2024-12-08 22:37:53.561426: Validation loss did not improve from -0.48247. Patience: 5/50
2024-12-08 22:37:53.562897: train_loss -0.5855
2024-12-08 22:37:53.563758: val_loss -0.4056
2024-12-08 22:37:53.564405: Pseudo dice [0.6908]
2024-12-08 22:37:53.565247: Epoch time: 88.24 s
2024-12-08 22:37:53.565920: Yayy! New best EMA pseudo Dice: 0.6405
2024-12-08 22:37:55.191576: 
2024-12-08 22:37:55.193083: Epoch 24
2024-12-08 22:37:55.193765: Current learning rate: 0.00978
2024-12-08 22:39:23.738917: Validation loss did not improve from -0.48247. Patience: 6/50
2024-12-08 22:39:23.739996: train_loss -0.5773
2024-12-08 22:39:23.740951: val_loss -0.3854
2024-12-08 22:39:23.741727: Pseudo dice [0.631]
2024-12-08 22:39:23.742425: Epoch time: 88.55 s
2024-12-08 22:39:25.415272: 
2024-12-08 22:39:25.416981: Epoch 25
2024-12-08 22:39:25.417966: Current learning rate: 0.00977
2024-12-08 22:40:53.870326: Validation loss did not improve from -0.48247. Patience: 7/50
2024-12-08 22:40:53.871418: train_loss -0.5985
2024-12-08 22:40:53.872416: val_loss -0.346
2024-12-08 22:40:53.873021: Pseudo dice [0.6204]
2024-12-08 22:40:53.873658: Epoch time: 88.46 s
2024-12-08 22:40:55.153664: 
2024-12-08 22:40:55.154896: Epoch 26
2024-12-08 22:40:55.155578: Current learning rate: 0.00977
2024-12-08 22:42:23.716452: Validation loss did not improve from -0.48247. Patience: 8/50
2024-12-08 22:42:23.717309: train_loss -0.5879
2024-12-08 22:42:23.718319: val_loss -0.4219
2024-12-08 22:42:23.719051: Pseudo dice [0.6583]
2024-12-08 22:42:23.719697: Epoch time: 88.56 s
2024-12-08 22:42:24.970243: 
2024-12-08 22:42:24.972089: Epoch 27
2024-12-08 22:42:24.973110: Current learning rate: 0.00976
2024-12-08 22:43:53.490850: Validation loss did not improve from -0.48247. Patience: 9/50
2024-12-08 22:43:53.491877: train_loss -0.609
2024-12-08 22:43:53.492766: val_loss -0.4136
2024-12-08 22:43:53.493501: Pseudo dice [0.6797]
2024-12-08 22:43:53.494312: Epoch time: 88.52 s
2024-12-08 22:43:53.494999: Yayy! New best EMA pseudo Dice: 0.6437
2024-12-08 22:43:55.141390: 
2024-12-08 22:43:55.142791: Epoch 28
2024-12-08 22:43:55.143582: Current learning rate: 0.00975
2024-12-08 22:45:23.667540: Validation loss did not improve from -0.48247. Patience: 10/50
2024-12-08 22:45:23.668810: train_loss -0.6047
2024-12-08 22:45:23.669750: val_loss -0.2069
2024-12-08 22:45:23.670453: Pseudo dice [0.5439]
2024-12-08 22:45:23.671355: Epoch time: 88.53 s
2024-12-08 22:45:24.932769: 
2024-12-08 22:45:24.934301: Epoch 29
2024-12-08 22:45:24.935106: Current learning rate: 0.00974
2024-12-08 22:46:53.359229: Validation loss did not improve from -0.48247. Patience: 11/50
2024-12-08 22:46:53.360208: train_loss -0.6112
2024-12-08 22:46:53.361119: val_loss -0.3576
2024-12-08 22:46:53.361927: Pseudo dice [0.6484]
2024-12-08 22:46:53.362539: Epoch time: 88.43 s
2024-12-08 22:46:55.385458: 
2024-12-08 22:46:55.386941: Epoch 30
2024-12-08 22:46:55.387577: Current learning rate: 0.00973
2024-12-08 22:48:23.785237: Validation loss did not improve from -0.48247. Patience: 12/50
2024-12-08 22:48:23.786476: train_loss -0.6023
2024-12-08 22:48:23.787422: val_loss -0.394
2024-12-08 22:48:23.788177: Pseudo dice [0.6581]
2024-12-08 22:48:23.788891: Epoch time: 88.4 s
2024-12-08 22:48:25.078904: 
2024-12-08 22:48:25.080530: Epoch 31
2024-12-08 22:48:25.081589: Current learning rate: 0.00972
2024-12-08 22:49:53.548295: Validation loss did not improve from -0.48247. Patience: 13/50
2024-12-08 22:49:53.549549: train_loss -0.6124
2024-12-08 22:49:53.550460: val_loss -0.4328
2024-12-08 22:49:53.551419: Pseudo dice [0.6776]
2024-12-08 22:49:53.552210: Epoch time: 88.47 s
2024-12-08 22:49:54.893235: 
2024-12-08 22:49:54.895026: Epoch 32
2024-12-08 22:49:54.895751: Current learning rate: 0.00971
2024-12-08 22:51:23.467853: Validation loss improved from -0.48247 to -0.48419! Patience: 13/50
2024-12-08 22:51:23.468849: train_loss -0.6162
2024-12-08 22:51:23.469641: val_loss -0.4842
2024-12-08 22:51:23.470404: Pseudo dice [0.7259]
2024-12-08 22:51:23.471254: Epoch time: 88.58 s
2024-12-08 22:51:23.472028: Yayy! New best EMA pseudo Dice: 0.6499
2024-12-08 22:51:25.149119: 
2024-12-08 22:51:25.150541: Epoch 33
2024-12-08 22:51:25.151357: Current learning rate: 0.0097
2024-12-08 22:52:53.570526: Validation loss did not improve from -0.48419. Patience: 1/50
2024-12-08 22:52:53.571738: train_loss -0.6134
2024-12-08 22:52:53.572780: val_loss -0.3384
2024-12-08 22:52:53.573809: Pseudo dice [0.6326]
2024-12-08 22:52:53.574655: Epoch time: 88.42 s
2024-12-08 22:52:54.872094: 
2024-12-08 22:52:54.873930: Epoch 34
2024-12-08 22:52:54.874855: Current learning rate: 0.00969
2024-12-08 22:54:23.277009: Validation loss did not improve from -0.48419. Patience: 2/50
2024-12-08 22:54:23.278245: train_loss -0.6262
2024-12-08 22:54:23.279410: val_loss -0.3958
2024-12-08 22:54:23.280350: Pseudo dice [0.6652]
2024-12-08 22:54:23.281224: Epoch time: 88.41 s
2024-12-08 22:54:24.972558: 
2024-12-08 22:54:24.974217: Epoch 35
2024-12-08 22:54:24.975311: Current learning rate: 0.00968
2024-12-08 22:55:53.341830: Validation loss did not improve from -0.48419. Patience: 3/50
2024-12-08 22:55:53.343154: train_loss -0.6205
2024-12-08 22:55:53.344162: val_loss -0.4536
2024-12-08 22:55:53.344728: Pseudo dice [0.7049]
2024-12-08 22:55:53.345347: Epoch time: 88.37 s
2024-12-08 22:55:53.346064: Yayy! New best EMA pseudo Dice: 0.6554
2024-12-08 22:55:54.998255: 
2024-12-08 22:55:55.000073: Epoch 36
2024-12-08 22:55:55.001022: Current learning rate: 0.00968
2024-12-08 22:57:23.330396: Validation loss did not improve from -0.48419. Patience: 4/50
2024-12-08 22:57:23.331766: train_loss -0.6267
2024-12-08 22:57:23.332799: val_loss -0.2572
2024-12-08 22:57:23.333749: Pseudo dice [0.5654]
2024-12-08 22:57:23.334565: Epoch time: 88.33 s
2024-12-08 22:57:24.709731: 
2024-12-08 22:57:24.711838: Epoch 37
2024-12-08 22:57:24.712834: Current learning rate: 0.00967
2024-12-08 22:58:53.097814: Validation loss did not improve from -0.48419. Patience: 5/50
2024-12-08 22:58:53.098629: train_loss -0.6236
2024-12-08 22:58:53.099414: val_loss -0.4818
2024-12-08 22:58:53.100175: Pseudo dice [0.7228]
2024-12-08 22:58:53.100907: Epoch time: 88.39 s
2024-12-08 22:58:54.466144: 
2024-12-08 22:58:54.467825: Epoch 38
2024-12-08 22:58:54.468750: Current learning rate: 0.00966
2024-12-08 23:00:22.915973: Validation loss did not improve from -0.48419. Patience: 6/50
2024-12-08 23:00:22.917435: train_loss -0.6204
2024-12-08 23:00:22.918740: val_loss -0.449
2024-12-08 23:00:22.919740: Pseudo dice [0.7074]
2024-12-08 23:00:22.920606: Epoch time: 88.45 s
2024-12-08 23:00:22.921578: Yayy! New best EMA pseudo Dice: 0.6594
2024-12-08 23:00:24.614778: 
2024-12-08 23:00:24.616526: Epoch 39
2024-12-08 23:00:24.617463: Current learning rate: 0.00965
2024-12-08 23:01:53.021125: Validation loss did not improve from -0.48419. Patience: 7/50
2024-12-08 23:01:53.022447: train_loss -0.6376
2024-12-08 23:01:53.023631: val_loss -0.4276
2024-12-08 23:01:53.024348: Pseudo dice [0.693]
2024-12-08 23:01:53.025141: Epoch time: 88.41 s
2024-12-08 23:01:53.409596: Yayy! New best EMA pseudo Dice: 0.6627
2024-12-08 23:01:55.088917: 
2024-12-08 23:01:55.090485: Epoch 40
2024-12-08 23:01:55.091357: Current learning rate: 0.00964
2024-12-08 23:03:23.441045: Validation loss did not improve from -0.48419. Patience: 8/50
2024-12-08 23:03:23.442317: train_loss -0.6298
2024-12-08 23:03:23.443550: val_loss -0.4767
2024-12-08 23:03:23.444319: Pseudo dice [0.7124]
2024-12-08 23:03:23.445013: Epoch time: 88.35 s
2024-12-08 23:03:23.445766: Yayy! New best EMA pseudo Dice: 0.6677
2024-12-08 23:03:25.545578: 
2024-12-08 23:03:25.547249: Epoch 41
2024-12-08 23:03:25.548185: Current learning rate: 0.00963
2024-12-08 23:04:54.092288: Validation loss did not improve from -0.48419. Patience: 9/50
2024-12-08 23:04:54.093434: train_loss -0.6324
2024-12-08 23:04:54.094394: val_loss -0.2335
2024-12-08 23:04:54.095316: Pseudo dice [0.5854]
2024-12-08 23:04:54.096096: Epoch time: 88.55 s
2024-12-08 23:04:55.370337: 
2024-12-08 23:04:55.372092: Epoch 42
2024-12-08 23:04:55.373142: Current learning rate: 0.00962
2024-12-08 23:06:23.613131: Validation loss did not improve from -0.48419. Patience: 10/50
2024-12-08 23:06:23.623481: train_loss -0.6508
2024-12-08 23:06:23.624297: val_loss -0.4064
2024-12-08 23:06:23.625063: Pseudo dice [0.6642]
2024-12-08 23:06:23.625826: Epoch time: 88.25 s
2024-12-08 23:06:24.916577: 
2024-12-08 23:06:24.918414: Epoch 43
2024-12-08 23:06:24.919178: Current learning rate: 0.00961
2024-12-08 23:07:53.280219: Validation loss did not improve from -0.48419. Patience: 11/50
2024-12-08 23:07:53.281082: train_loss -0.6429
2024-12-08 23:07:53.282270: val_loss -0.3076
2024-12-08 23:07:53.283107: Pseudo dice [0.6213]
2024-12-08 23:07:53.283925: Epoch time: 88.37 s
2024-12-08 23:07:54.565668: 
2024-12-08 23:07:54.567418: Epoch 44
2024-12-08 23:07:54.568303: Current learning rate: 0.0096
2024-12-08 23:09:22.938055: Validation loss did not improve from -0.48419. Patience: 12/50
2024-12-08 23:09:22.939320: train_loss -0.6514
2024-12-08 23:09:22.940201: val_loss -0.176
2024-12-08 23:09:22.940886: Pseudo dice [0.5188]
2024-12-08 23:09:22.941652: Epoch time: 88.37 s
2024-12-08 23:09:24.878088: 
2024-12-08 23:09:24.879812: Epoch 45
2024-12-08 23:09:24.880689: Current learning rate: 0.00959
2024-12-08 23:10:53.209745: Validation loss did not improve from -0.48419. Patience: 13/50
2024-12-08 23:10:53.211104: train_loss -0.6343
2024-12-08 23:10:53.212294: val_loss -0.3935
2024-12-08 23:10:53.213111: Pseudo dice [0.6701]
2024-12-08 23:10:53.213908: Epoch time: 88.33 s
2024-12-08 23:10:54.477156: 
2024-12-08 23:10:54.478956: Epoch 46
2024-12-08 23:10:54.479661: Current learning rate: 0.00959
2024-12-08 23:12:22.798194: Validation loss did not improve from -0.48419. Patience: 14/50
2024-12-08 23:12:22.799172: train_loss -0.6539
2024-12-08 23:12:22.800030: val_loss -0.4535
2024-12-08 23:12:22.800960: Pseudo dice [0.6909]
2024-12-08 23:12:22.801848: Epoch time: 88.32 s
2024-12-08 23:12:24.077144: 
2024-12-08 23:12:24.079017: Epoch 47
2024-12-08 23:12:24.079900: Current learning rate: 0.00958
2024-12-08 23:13:52.411281: Validation loss did not improve from -0.48419. Patience: 15/50
2024-12-08 23:13:52.412904: train_loss -0.6532
2024-12-08 23:13:52.413801: val_loss -0.4009
2024-12-08 23:13:52.414597: Pseudo dice [0.6683]
2024-12-08 23:13:52.415322: Epoch time: 88.34 s
2024-12-08 23:13:53.637679: 
2024-12-08 23:13:53.639280: Epoch 48
2024-12-08 23:13:53.640046: Current learning rate: 0.00957
2024-12-08 23:15:21.997033: Validation loss did not improve from -0.48419. Patience: 16/50
2024-12-08 23:15:21.998559: train_loss -0.6447
2024-12-08 23:15:21.999372: val_loss -0.3555
2024-12-08 23:15:21.999981: Pseudo dice [0.6427]
2024-12-08 23:15:22.000589: Epoch time: 88.36 s
2024-12-08 23:15:23.290148: 
2024-12-08 23:15:23.291778: Epoch 49
2024-12-08 23:15:23.292540: Current learning rate: 0.00956
2024-12-08 23:16:51.537329: Validation loss did not improve from -0.48419. Patience: 17/50
2024-12-08 23:16:51.538558: train_loss -0.6453
2024-12-08 23:16:51.539442: val_loss -0.358
2024-12-08 23:16:51.540181: Pseudo dice [0.6428]
2024-12-08 23:16:51.540839: Epoch time: 88.25 s
2024-12-08 23:16:53.252372: 
2024-12-08 23:16:53.253767: Epoch 50
2024-12-08 23:16:53.254489: Current learning rate: 0.00955
2024-12-08 23:18:21.722853: Validation loss did not improve from -0.48419. Patience: 18/50
2024-12-08 23:18:21.724137: train_loss -0.6571
2024-12-08 23:18:21.725253: val_loss -0.3097
2024-12-08 23:18:21.726190: Pseudo dice [0.6236]
2024-12-08 23:18:21.727074: Epoch time: 88.47 s
2024-12-08 23:18:23.034882: 
2024-12-08 23:18:23.036556: Epoch 51
2024-12-08 23:18:23.037719: Current learning rate: 0.00954
2024-12-08 23:19:51.420549: Validation loss did not improve from -0.48419. Patience: 19/50
2024-12-08 23:19:51.422001: train_loss -0.6532
2024-12-08 23:19:51.423121: val_loss -0.4181
2024-12-08 23:19:51.423780: Pseudo dice [0.6719]
2024-12-08 23:19:51.424411: Epoch time: 88.39 s
2024-12-08 23:19:53.079582: 
2024-12-08 23:19:53.081202: Epoch 52
2024-12-08 23:19:53.081916: Current learning rate: 0.00953
2024-12-08 23:21:21.312136: Validation loss did not improve from -0.48419. Patience: 20/50
2024-12-08 23:21:21.313614: train_loss -0.6539
2024-12-08 23:21:21.314760: val_loss -0.3878
2024-12-08 23:21:21.315672: Pseudo dice [0.672]
2024-12-08 23:21:21.316585: Epoch time: 88.23 s
2024-12-08 23:21:22.601630: 
2024-12-08 23:21:22.603463: Epoch 53
2024-12-08 23:21:22.604484: Current learning rate: 0.00952
2024-12-08 23:22:50.925268: Validation loss did not improve from -0.48419. Patience: 21/50
2024-12-08 23:22:50.926644: train_loss -0.6528
2024-12-08 23:22:50.927925: val_loss -0.4135
2024-12-08 23:22:50.928999: Pseudo dice [0.685]
2024-12-08 23:22:50.930111: Epoch time: 88.33 s
2024-12-08 23:22:52.178646: 
2024-12-08 23:22:52.180060: Epoch 54
2024-12-08 23:22:52.180948: Current learning rate: 0.00951
2024-12-08 23:24:20.490860: Validation loss did not improve from -0.48419. Patience: 22/50
2024-12-08 23:24:20.491911: train_loss -0.6654
2024-12-08 23:24:20.492814: val_loss -0.3633
2024-12-08 23:24:20.493539: Pseudo dice [0.6598]
2024-12-08 23:24:20.494227: Epoch time: 88.31 s
2024-12-08 23:24:22.179313: 
2024-12-08 23:24:22.180552: Epoch 55
2024-12-08 23:24:22.181295: Current learning rate: 0.0095
2024-12-08 23:25:50.532809: Validation loss did not improve from -0.48419. Patience: 23/50
2024-12-08 23:25:50.534157: train_loss -0.6647
2024-12-08 23:25:50.535013: val_loss -0.3412
2024-12-08 23:25:50.535595: Pseudo dice [0.6453]
2024-12-08 23:25:50.536348: Epoch time: 88.36 s
2024-12-08 23:25:51.791908: 
2024-12-08 23:25:51.793353: Epoch 56
2024-12-08 23:25:51.794044: Current learning rate: 0.00949
2024-12-08 23:27:20.301009: Validation loss did not improve from -0.48419. Patience: 24/50
2024-12-08 23:27:20.302325: train_loss -0.661
2024-12-08 23:27:20.303362: val_loss -0.472
2024-12-08 23:27:20.304196: Pseudo dice [0.7124]
2024-12-08 23:27:20.305132: Epoch time: 88.51 s
2024-12-08 23:27:21.583438: 
2024-12-08 23:27:21.584811: Epoch 57
2024-12-08 23:27:21.585766: Current learning rate: 0.00949
2024-12-08 23:28:50.178088: Validation loss did not improve from -0.48419. Patience: 25/50
2024-12-08 23:28:50.179314: train_loss -0.6697
2024-12-08 23:28:50.180255: val_loss -0.3507
2024-12-08 23:28:50.180901: Pseudo dice [0.6446]
2024-12-08 23:28:50.181570: Epoch time: 88.6 s
2024-12-08 23:28:51.518665: 
2024-12-08 23:28:51.519973: Epoch 58
2024-12-08 23:28:51.520732: Current learning rate: 0.00948
2024-12-08 23:30:20.292741: Validation loss did not improve from -0.48419. Patience: 26/50
2024-12-08 23:30:20.293886: train_loss -0.6626
2024-12-08 23:30:20.294984: val_loss -0.4546
2024-12-08 23:30:20.296022: Pseudo dice [0.6847]
2024-12-08 23:30:20.297027: Epoch time: 88.78 s
2024-12-08 23:30:21.591926: 
2024-12-08 23:30:21.593374: Epoch 59
2024-12-08 23:30:21.594435: Current learning rate: 0.00947
2024-12-08 23:31:50.242438: Validation loss did not improve from -0.48419. Patience: 27/50
2024-12-08 23:31:50.243603: train_loss -0.6507
2024-12-08 23:31:50.244379: val_loss -0.3529
2024-12-08 23:31:50.245200: Pseudo dice [0.6537]
2024-12-08 23:31:50.246012: Epoch time: 88.65 s
2024-12-08 23:31:51.939287: 
2024-12-08 23:31:51.940411: Epoch 60
2024-12-08 23:31:51.941138: Current learning rate: 0.00946
2024-12-08 23:33:20.516502: Validation loss did not improve from -0.48419. Patience: 28/50
2024-12-08 23:33:20.517841: train_loss -0.6665
2024-12-08 23:33:20.518589: val_loss -0.3677
2024-12-08 23:33:20.519346: Pseudo dice [0.6442]
2024-12-08 23:33:20.520044: Epoch time: 88.58 s
2024-12-08 23:33:21.815132: 
2024-12-08 23:33:21.816824: Epoch 61
2024-12-08 23:33:21.817659: Current learning rate: 0.00945
2024-12-08 23:34:50.414059: Validation loss did not improve from -0.48419. Patience: 29/50
2024-12-08 23:34:50.415129: train_loss -0.6714
2024-12-08 23:34:50.416036: val_loss -0.4613
2024-12-08 23:34:50.416871: Pseudo dice [0.7165]
2024-12-08 23:34:50.417653: Epoch time: 88.6 s
2024-12-08 23:34:52.107015: 
2024-12-08 23:34:52.108748: Epoch 62
2024-12-08 23:34:52.109748: Current learning rate: 0.00944
2024-12-08 23:36:20.652875: Validation loss did not improve from -0.48419. Patience: 30/50
2024-12-08 23:36:20.654187: train_loss -0.6761
2024-12-08 23:36:20.655151: val_loss -0.4386
2024-12-08 23:36:20.655930: Pseudo dice [0.699]
2024-12-08 23:36:20.656646: Epoch time: 88.55 s
2024-12-08 23:36:20.657377: Yayy! New best EMA pseudo Dice: 0.6682
2024-12-08 23:36:22.333916: 
2024-12-08 23:36:22.335592: Epoch 63
2024-12-08 23:36:22.336510: Current learning rate: 0.00943
2024-12-08 23:37:50.945831: Validation loss did not improve from -0.48419. Patience: 31/50
2024-12-08 23:37:50.946788: train_loss -0.6752
2024-12-08 23:37:50.947747: val_loss -0.4493
2024-12-08 23:37:50.948475: Pseudo dice [0.7091]
2024-12-08 23:37:50.949198: Epoch time: 88.61 s
2024-12-08 23:37:50.949852: Yayy! New best EMA pseudo Dice: 0.6723
2024-12-08 23:37:52.616359: 
2024-12-08 23:37:52.618095: Epoch 64
2024-12-08 23:37:52.618883: Current learning rate: 0.00942
2024-12-08 23:39:21.088049: Validation loss did not improve from -0.48419. Patience: 32/50
2024-12-08 23:39:21.089293: train_loss -0.6838
2024-12-08 23:39:21.090193: val_loss -0.335
2024-12-08 23:39:21.091017: Pseudo dice [0.6485]
2024-12-08 23:39:21.091684: Epoch time: 88.47 s
2024-12-08 23:39:22.758913: 
2024-12-08 23:39:22.760277: Epoch 65
2024-12-08 23:39:22.761132: Current learning rate: 0.00941
2024-12-08 23:40:51.331291: Validation loss did not improve from -0.48419. Patience: 33/50
2024-12-08 23:40:51.332438: train_loss -0.6791
2024-12-08 23:40:51.333397: val_loss -0.4116
2024-12-08 23:40:51.334051: Pseudo dice [0.6713]
2024-12-08 23:40:51.334761: Epoch time: 88.57 s
2024-12-08 23:40:52.656713: 
2024-12-08 23:40:52.658239: Epoch 66
2024-12-08 23:40:52.659203: Current learning rate: 0.0094
2024-12-08 23:42:21.341634: Validation loss did not improve from -0.48419. Patience: 34/50
2024-12-08 23:42:21.342575: train_loss -0.683
2024-12-08 23:42:21.343463: val_loss -0.402
2024-12-08 23:42:21.344122: Pseudo dice [0.6833]
2024-12-08 23:42:21.344869: Epoch time: 88.69 s
2024-12-08 23:42:22.637045: 
2024-12-08 23:42:22.638887: Epoch 67
2024-12-08 23:42:22.639784: Current learning rate: 0.00939
2024-12-08 23:43:51.363665: Validation loss did not improve from -0.48419. Patience: 35/50
2024-12-08 23:43:51.364864: train_loss -0.678
2024-12-08 23:43:51.366026: val_loss -0.3555
2024-12-08 23:43:51.367056: Pseudo dice [0.6374]
2024-12-08 23:43:51.368115: Epoch time: 88.73 s
2024-12-08 23:43:52.674613: 
2024-12-08 23:43:52.676218: Epoch 68
2024-12-08 23:43:52.677322: Current learning rate: 0.00939
2024-12-08 23:45:21.289886: Validation loss did not improve from -0.48419. Patience: 36/50
2024-12-08 23:45:21.290881: train_loss -0.6744
2024-12-08 23:45:21.291797: val_loss -0.3419
2024-12-08 23:45:21.292446: Pseudo dice [0.6342]
2024-12-08 23:45:21.293174: Epoch time: 88.62 s
2024-12-08 23:45:22.658348: 
2024-12-08 23:45:22.659910: Epoch 69
2024-12-08 23:45:22.660756: Current learning rate: 0.00938
2024-12-08 23:46:51.267308: Validation loss did not improve from -0.48419. Patience: 37/50
2024-12-08 23:46:51.268041: train_loss -0.6875
2024-12-08 23:46:51.268782: val_loss -0.4388
2024-12-08 23:46:51.269529: Pseudo dice [0.6964]
2024-12-08 23:46:51.270196: Epoch time: 88.61 s
2024-12-08 23:46:52.980167: 
2024-12-08 23:46:52.981421: Epoch 70
2024-12-08 23:46:52.982266: Current learning rate: 0.00937
2024-12-08 23:48:21.499424: Validation loss did not improve from -0.48419. Patience: 38/50
2024-12-08 23:48:21.500665: train_loss -0.6813
2024-12-08 23:48:21.501496: val_loss -0.3375
2024-12-08 23:48:21.502150: Pseudo dice [0.6391]
2024-12-08 23:48:21.502862: Epoch time: 88.52 s
2024-12-08 23:48:22.810778: 
2024-12-08 23:48:22.812287: Epoch 71
2024-12-08 23:48:22.813076: Current learning rate: 0.00936
2024-12-08 23:49:51.393687: Validation loss did not improve from -0.48419. Patience: 39/50
2024-12-08 23:49:51.394752: train_loss -0.6792
2024-12-08 23:49:51.395670: val_loss -0.3707
2024-12-08 23:49:51.396413: Pseudo dice [0.6614]
2024-12-08 23:49:51.397213: Epoch time: 88.58 s
2024-12-08 23:49:52.683776: 
2024-12-08 23:49:52.685420: Epoch 72
2024-12-08 23:49:52.686212: Current learning rate: 0.00935
2024-12-08 23:51:21.228985: Validation loss did not improve from -0.48419. Patience: 40/50
2024-12-08 23:51:21.230241: train_loss -0.6814
2024-12-08 23:51:21.231255: val_loss -0.4448
2024-12-08 23:51:21.231940: Pseudo dice [0.6951]
2024-12-08 23:51:21.232680: Epoch time: 88.55 s
2024-12-08 23:51:22.926161: 
2024-12-08 23:51:22.927289: Epoch 73
2024-12-08 23:51:22.928165: Current learning rate: 0.00934
2024-12-08 23:52:51.521873: Validation loss did not improve from -0.48419. Patience: 41/50
2024-12-08 23:52:51.523022: train_loss -0.6879
2024-12-08 23:52:51.523916: val_loss -0.382
2024-12-08 23:52:51.524650: Pseudo dice [0.6505]
2024-12-08 23:52:51.525273: Epoch time: 88.6 s
2024-12-08 23:52:52.854703: 
2024-12-08 23:52:52.856356: Epoch 74
2024-12-08 23:52:52.857175: Current learning rate: 0.00933
2024-12-08 23:54:21.495632: Validation loss did not improve from -0.48419. Patience: 42/50
2024-12-08 23:54:21.496828: train_loss -0.6801
2024-12-08 23:54:21.497679: val_loss -0.331
2024-12-08 23:54:21.498615: Pseudo dice [0.639]
2024-12-08 23:54:21.499427: Epoch time: 88.64 s
2024-12-08 23:54:23.194953: 
2024-12-08 23:54:23.196881: Epoch 75
2024-12-08 23:54:23.197861: Current learning rate: 0.00932
2024-12-08 23:55:51.882898: Validation loss did not improve from -0.48419. Patience: 43/50
2024-12-08 23:55:51.883920: train_loss -0.6846
2024-12-08 23:55:51.884807: val_loss -0.3691
2024-12-08 23:55:51.885534: Pseudo dice [0.6578]
2024-12-08 23:55:51.886153: Epoch time: 88.69 s
2024-12-08 23:55:53.218371: 
2024-12-08 23:55:53.219830: Epoch 76
2024-12-08 23:55:53.220532: Current learning rate: 0.00931
2024-12-08 23:57:22.083961: Validation loss did not improve from -0.48419. Patience: 44/50
2024-12-08 23:57:22.085287: train_loss -0.6849
2024-12-08 23:57:22.086423: val_loss -0.39
2024-12-08 23:57:22.087375: Pseudo dice [0.6574]
2024-12-08 23:57:22.088012: Epoch time: 88.87 s
2024-12-08 23:57:23.435065: 
2024-12-08 23:57:23.436501: Epoch 77
2024-12-08 23:57:23.437227: Current learning rate: 0.0093
2024-12-08 23:58:52.549460: Validation loss did not improve from -0.48419. Patience: 45/50
2024-12-08 23:58:52.550378: train_loss -0.6848
2024-12-08 23:58:52.551249: val_loss -0.4302
2024-12-08 23:58:52.552073: Pseudo dice [0.6835]
2024-12-08 23:58:52.553010: Epoch time: 89.12 s
2024-12-08 23:58:53.910814: 
2024-12-08 23:58:53.912434: Epoch 78
2024-12-08 23:58:53.913457: Current learning rate: 0.0093
2024-12-09 00:00:23.085617: Validation loss did not improve from -0.48419. Patience: 46/50
2024-12-09 00:00:23.086770: train_loss -0.6919
2024-12-09 00:00:23.087701: val_loss -0.3921
2024-12-09 00:00:23.088544: Pseudo dice [0.6709]
2024-12-09 00:00:23.089379: Epoch time: 89.18 s
2024-12-09 00:00:24.474538: 
2024-12-09 00:00:24.475957: Epoch 79
2024-12-09 00:00:24.476866: Current learning rate: 0.00929
2024-12-09 00:01:53.631387: Validation loss did not improve from -0.48419. Patience: 47/50
2024-12-09 00:01:53.632493: train_loss -0.6956
2024-12-09 00:01:53.633685: val_loss -0.449
2024-12-09 00:01:53.634460: Pseudo dice [0.711]
2024-12-09 00:01:53.635287: Epoch time: 89.16 s
2024-12-09 00:01:55.368513: 
2024-12-09 00:01:55.370105: Epoch 80
2024-12-09 00:01:55.370778: Current learning rate: 0.00928
2024-12-09 00:03:24.835827: Validation loss did not improve from -0.48419. Patience: 48/50
2024-12-09 00:03:24.837002: train_loss -0.6952
2024-12-09 00:03:24.838054: val_loss -0.4275
2024-12-09 00:03:24.839065: Pseudo dice [0.6932]
2024-12-09 00:03:24.840019: Epoch time: 89.47 s
2024-12-09 00:03:26.216756: 
2024-12-09 00:03:26.218000: Epoch 81
2024-12-09 00:03:26.218785: Current learning rate: 0.00927
2024-12-09 00:04:55.617074: Validation loss did not improve from -0.48419. Patience: 49/50
2024-12-09 00:04:55.618289: train_loss -0.7012
2024-12-09 00:04:55.619245: val_loss -0.353
2024-12-09 00:04:55.619949: Pseudo dice [0.6378]
2024-12-09 00:04:55.620755: Epoch time: 89.4 s
2024-12-09 00:04:57.023156: 
2024-12-09 00:04:57.024823: Epoch 82
2024-12-09 00:04:57.025633: Current learning rate: 0.00926
2024-12-09 00:06:26.078120: Validation loss did not improve from -0.48419. Patience: 50/50
2024-12-09 00:06:26.079072: train_loss -0.6896
2024-12-09 00:06:26.079919: val_loss -0.424
2024-12-09 00:06:26.080658: Pseudo dice [0.6929]
2024-12-09 00:06:26.081422: Epoch time: 89.06 s
2024-12-09 00:06:27.762277: Patience reached. Stopping training.
2024-12-09 00:06:28.156223: Training done.
2024-12-09 00:06:28.473417: Using splits from existing split file: /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset307_Sohee_Calcium_OCT_CrossValidation/splits_final.json
2024-12-09 00:06:28.494465: The split file contains 5 splits.
2024-12-09 00:06:28.495639: Desired fold for training: 3
2024-12-09 00:06:28.496443: This split has 7 training and 1 validation cases.
2024-12-09 00:06:28.497352: predicting 701-013
2024-12-09 00:06:28.507004: 701-013, shape torch.Size([1, 375, 498, 498]), rank 0
2024-12-09 00:08:44.059915: Validation complete
2024-12-09 00:08:44.060624: Mean Validation Dice:  0.6842270602590166
2024-12-08 22:01:07.622306: unpacking done...
2024-12-08 22:01:07.631569: Unable to plot network architecture: nnUNet_compile is enabled!
2024-12-08 22:01:07.706131: 
2024-12-08 22:01:07.707895: Epoch 0
2024-12-08 22:01:07.710214: Current learning rate: 0.01
2024-12-08 22:03:33.488883: Validation loss improved from 1000.00000 to -0.27484! Patience: 0/50
2024-12-08 22:03:33.490414: train_loss -0.1303
2024-12-08 22:03:33.492129: val_loss -0.2748
2024-12-08 22:03:33.493438: Pseudo dice [0.5739]
2024-12-08 22:03:33.494811: Epoch time: 145.79 s
2024-12-08 22:03:33.495902: Yayy! New best EMA pseudo Dice: 0.5739
2024-12-08 22:03:34.904341: 
2024-12-08 22:03:34.905877: Epoch 1
2024-12-08 22:03:34.907089: Current learning rate: 0.00999
2024-12-08 22:05:02.714152: Validation loss improved from -0.27484 to -0.27870! Patience: 0/50
2024-12-08 22:05:02.715816: train_loss -0.2534
2024-12-08 22:05:02.717357: val_loss -0.2787
2024-12-08 22:05:02.718339: Pseudo dice [0.5615]
2024-12-08 22:05:02.719207: Epoch time: 87.81 s
2024-12-08 22:05:03.978342: 
2024-12-08 22:05:03.980026: Epoch 2
2024-12-08 22:05:03.980901: Current learning rate: 0.00998
2024-12-08 22:06:32.324745: Validation loss improved from -0.27870 to -0.32859! Patience: 0/50
2024-12-08 22:06:32.326264: train_loss -0.3329
2024-12-08 22:06:32.327676: val_loss -0.3286
2024-12-08 22:06:32.328479: Pseudo dice [0.6253]
2024-12-08 22:06:32.329307: Epoch time: 88.35 s
2024-12-08 22:06:32.330102: Yayy! New best EMA pseudo Dice: 0.5779
2024-12-08 22:06:34.022263: 
2024-12-08 22:06:34.023995: Epoch 3
2024-12-08 22:06:34.024786: Current learning rate: 0.00997
2024-12-08 22:08:02.380764: Validation loss improved from -0.32859 to -0.34104! Patience: 0/50
2024-12-08 22:08:02.382094: train_loss -0.3784
2024-12-08 22:08:02.383125: val_loss -0.341
2024-12-08 22:08:02.383995: Pseudo dice [0.6072]
2024-12-08 22:08:02.384791: Epoch time: 88.36 s
2024-12-08 22:08:02.385519: Yayy! New best EMA pseudo Dice: 0.5808
2024-12-08 22:08:04.003855: 
2024-12-08 22:08:04.005731: Epoch 4
2024-12-08 22:08:04.006585: Current learning rate: 0.00996
2024-12-08 22:09:32.218588: Validation loss did not improve from -0.34104. Patience: 1/50
2024-12-08 22:09:32.219663: train_loss -0.4132
2024-12-08 22:09:32.220652: val_loss -0.3191
2024-12-08 22:09:32.221501: Pseudo dice [0.6505]
2024-12-08 22:09:32.222399: Epoch time: 88.22 s
2024-12-08 22:09:32.576235: Yayy! New best EMA pseudo Dice: 0.5878
2024-12-08 22:09:34.222301: 
2024-12-08 22:09:34.224743: Epoch 5
2024-12-08 22:09:34.225735: Current learning rate: 0.00995
2024-12-08 22:11:02.587347: Validation loss improved from -0.34104 to -0.40772! Patience: 1/50
2024-12-08 22:11:02.588547: train_loss -0.434
2024-12-08 22:11:02.589732: val_loss -0.4077
2024-12-08 22:11:02.590933: Pseudo dice [0.6766]
2024-12-08 22:11:02.592185: Epoch time: 88.37 s
2024-12-08 22:11:02.593373: Yayy! New best EMA pseudo Dice: 0.5967
2024-12-08 22:11:04.256584: 
2024-12-08 22:11:04.258577: Epoch 6
2024-12-08 22:11:04.259421: Current learning rate: 0.00995
2024-12-08 22:12:32.403844: Validation loss did not improve from -0.40772. Patience: 1/50
2024-12-08 22:12:32.404908: train_loss -0.474
2024-12-08 22:12:32.406073: val_loss -0.3822
2024-12-08 22:12:32.407077: Pseudo dice [0.6564]
2024-12-08 22:12:32.407996: Epoch time: 88.15 s
2024-12-08 22:12:32.408844: Yayy! New best EMA pseudo Dice: 0.6027
2024-12-08 22:12:34.072579: 
2024-12-08 22:12:34.073972: Epoch 7
2024-12-08 22:12:34.074761: Current learning rate: 0.00994
2024-12-08 22:14:02.132031: Validation loss did not improve from -0.40772. Patience: 2/50
2024-12-08 22:14:02.132896: train_loss -0.4789
2024-12-08 22:14:02.133917: val_loss -0.3946
2024-12-08 22:14:02.134853: Pseudo dice [0.6627]
2024-12-08 22:14:02.135677: Epoch time: 88.06 s
2024-12-08 22:14:02.136617: Yayy! New best EMA pseudo Dice: 0.6087
2024-12-08 22:14:03.804769: 
2024-12-08 22:14:03.806447: Epoch 8
2024-12-08 22:14:03.807473: Current learning rate: 0.00993
2024-12-08 22:15:32.661036: Validation loss improved from -0.40772 to -0.44328! Patience: 2/50
2024-12-08 22:15:32.662235: train_loss -0.491
2024-12-08 22:15:32.663429: val_loss -0.4433
2024-12-08 22:15:32.664385: Pseudo dice [0.6913]
2024-12-08 22:15:32.665390: Epoch time: 88.86 s
2024-12-08 22:15:32.666274: Yayy! New best EMA pseudo Dice: 0.6169
2024-12-08 22:15:34.372906: 
2024-12-08 22:15:34.374633: Epoch 9
2024-12-08 22:15:34.375416: Current learning rate: 0.00992
2024-12-08 22:17:02.868080: Validation loss improved from -0.44328 to -0.45626! Patience: 0/50
2024-12-08 22:17:02.869468: train_loss -0.5009
2024-12-08 22:17:02.870450: val_loss -0.4563
2024-12-08 22:17:02.871193: Pseudo dice [0.6879]
2024-12-08 22:17:02.871886: Epoch time: 88.5 s
2024-12-08 22:17:03.231485: Yayy! New best EMA pseudo Dice: 0.624
2024-12-08 22:17:04.849597: 
2024-12-08 22:17:04.851022: Epoch 10
2024-12-08 22:17:04.851905: Current learning rate: 0.00991
2024-12-08 22:18:33.303137: Validation loss did not improve from -0.45626. Patience: 1/50
2024-12-08 22:18:33.304458: train_loss -0.5057
2024-12-08 22:18:33.305426: val_loss -0.4031
2024-12-08 22:18:33.306279: Pseudo dice [0.6706]
2024-12-08 22:18:33.307124: Epoch time: 88.46 s
2024-12-08 22:18:33.307936: Yayy! New best EMA pseudo Dice: 0.6287
2024-12-08 22:18:34.898578: 
2024-12-08 22:18:34.900030: Epoch 11
2024-12-08 22:18:34.900881: Current learning rate: 0.0099
2024-12-08 22:20:03.489753: Validation loss did not improve from -0.45626. Patience: 2/50
2024-12-08 22:20:03.490679: train_loss -0.5128
2024-12-08 22:20:03.491653: val_loss -0.4026
2024-12-08 22:20:03.492357: Pseudo dice [0.6739]
2024-12-08 22:20:03.493249: Epoch time: 88.59 s
2024-12-08 22:20:03.493935: Yayy! New best EMA pseudo Dice: 0.6332
2024-12-08 22:20:05.094989: 
2024-12-08 22:20:05.096470: Epoch 12
2024-12-08 22:20:05.097289: Current learning rate: 0.00989
2024-12-08 22:21:33.675548: Validation loss did not improve from -0.45626. Patience: 3/50
2024-12-08 22:21:33.676934: train_loss -0.5055
2024-12-08 22:21:33.678280: val_loss -0.4017
2024-12-08 22:21:33.679461: Pseudo dice [0.6683]
2024-12-08 22:21:33.680566: Epoch time: 88.58 s
2024-12-08 22:21:33.681677: Yayy! New best EMA pseudo Dice: 0.6367
2024-12-08 22:21:35.309656: 
2024-12-08 22:21:35.311778: Epoch 13
2024-12-08 22:21:35.312885: Current learning rate: 0.00988
2024-12-08 22:23:03.718076: Validation loss did not improve from -0.45626. Patience: 4/50
2024-12-08 22:23:03.719307: train_loss -0.5011
2024-12-08 22:23:03.720425: val_loss -0.4066
2024-12-08 22:23:03.721460: Pseudo dice [0.6725]
2024-12-08 22:23:03.722399: Epoch time: 88.41 s
2024-12-08 22:23:03.723114: Yayy! New best EMA pseudo Dice: 0.6403
2024-12-08 22:23:05.372670: 
2024-12-08 22:23:05.374349: Epoch 14
2024-12-08 22:23:05.375105: Current learning rate: 0.00987
2024-12-08 22:24:33.866660: Validation loss did not improve from -0.45626. Patience: 5/50
2024-12-08 22:24:33.867971: train_loss -0.5427
2024-12-08 22:24:33.869298: val_loss -0.429
2024-12-08 22:24:33.870110: Pseudo dice [0.6831]
2024-12-08 22:24:33.870951: Epoch time: 88.5 s
2024-12-08 22:24:34.241865: Yayy! New best EMA pseudo Dice: 0.6446
2024-12-08 22:24:35.891776: 
2024-12-08 22:24:35.893265: Epoch 15
2024-12-08 22:24:35.894010: Current learning rate: 0.00986
2024-12-08 22:26:04.490964: Validation loss did not improve from -0.45626. Patience: 6/50
2024-12-08 22:26:04.491683: train_loss -0.5398
2024-12-08 22:26:04.492608: val_loss -0.4524
2024-12-08 22:26:04.493492: Pseudo dice [0.695]
2024-12-08 22:26:04.494351: Epoch time: 88.6 s
2024-12-08 22:26:04.495224: Yayy! New best EMA pseudo Dice: 0.6496
2024-12-08 22:26:06.086241: 
2024-12-08 22:26:06.088015: Epoch 16
2024-12-08 22:26:06.089169: Current learning rate: 0.00986
2024-12-08 22:27:35.087084: Validation loss did not improve from -0.45626. Patience: 7/50
2024-12-08 22:27:35.088345: train_loss -0.5438
2024-12-08 22:27:35.089303: val_loss -0.4555
2024-12-08 22:27:35.090081: Pseudo dice [0.6849]
2024-12-08 22:27:35.090934: Epoch time: 89.0 s
2024-12-08 22:27:35.091741: Yayy! New best EMA pseudo Dice: 0.6532
2024-12-08 22:27:36.731859: 
2024-12-08 22:27:36.733651: Epoch 17
2024-12-08 22:27:36.734500: Current learning rate: 0.00985
2024-12-08 22:29:05.861706: Validation loss improved from -0.45626 to -0.47130! Patience: 7/50
2024-12-08 22:29:05.863057: train_loss -0.5563
2024-12-08 22:29:05.864268: val_loss -0.4713
2024-12-08 22:29:05.865202: Pseudo dice [0.7116]
2024-12-08 22:29:05.866159: Epoch time: 89.13 s
2024-12-08 22:29:05.867072: Yayy! New best EMA pseudo Dice: 0.659
2024-12-08 22:29:07.499074: 
2024-12-08 22:29:07.500542: Epoch 18
2024-12-08 22:29:07.501493: Current learning rate: 0.00984
2024-12-08 22:30:36.615527: Validation loss did not improve from -0.47130. Patience: 1/50
2024-12-08 22:30:36.616710: train_loss -0.5651
2024-12-08 22:30:36.617679: val_loss -0.4131
2024-12-08 22:30:36.618376: Pseudo dice [0.6821]
2024-12-08 22:30:36.619097: Epoch time: 89.12 s
2024-12-08 22:30:36.619745: Yayy! New best EMA pseudo Dice: 0.6613
2024-12-08 22:30:38.662736: 
2024-12-08 22:30:38.664171: Epoch 19
2024-12-08 22:30:38.664890: Current learning rate: 0.00983
2024-12-08 22:32:07.625250: Validation loss improved from -0.47130 to -0.50159! Patience: 1/50
2024-12-08 22:32:07.626332: train_loss -0.5559
2024-12-08 22:32:07.627394: val_loss -0.5016
2024-12-08 22:32:07.628161: Pseudo dice [0.7202]
2024-12-08 22:32:07.628896: Epoch time: 88.96 s
2024-12-08 22:32:08.014565: Yayy! New best EMA pseudo Dice: 0.6672
2024-12-08 22:32:09.703120: 
2024-12-08 22:32:09.704649: Epoch 20
2024-12-08 22:32:09.705591: Current learning rate: 0.00982
2024-12-08 22:33:38.688903: Validation loss did not improve from -0.50159. Patience: 1/50
2024-12-08 22:33:38.689908: train_loss -0.5593
2024-12-08 22:33:38.690853: val_loss -0.4743
2024-12-08 22:33:38.691653: Pseudo dice [0.7007]
2024-12-08 22:33:38.692523: Epoch time: 88.99 s
2024-12-08 22:33:38.693359: Yayy! New best EMA pseudo Dice: 0.6705
2024-12-08 22:33:40.341759: 
2024-12-08 22:33:40.343666: Epoch 21
2024-12-08 22:33:40.344459: Current learning rate: 0.00981
2024-12-08 22:35:09.185189: Validation loss did not improve from -0.50159. Patience: 2/50
2024-12-08 22:35:09.186692: train_loss -0.5689
2024-12-08 22:35:09.187939: val_loss -0.4835
2024-12-08 22:35:09.189063: Pseudo dice [0.7167]
2024-12-08 22:35:09.189929: Epoch time: 88.85 s
2024-12-08 22:35:09.190809: Yayy! New best EMA pseudo Dice: 0.6752
2024-12-08 22:35:10.757184: 
2024-12-08 22:35:10.758840: Epoch 22
2024-12-08 22:35:10.759811: Current learning rate: 0.0098
2024-12-08 22:36:39.467846: Validation loss did not improve from -0.50159. Patience: 3/50
2024-12-08 22:36:39.468949: train_loss -0.5823
2024-12-08 22:36:39.469801: val_loss -0.4658
2024-12-08 22:36:39.470581: Pseudo dice [0.7084]
2024-12-08 22:36:39.471408: Epoch time: 88.71 s
2024-12-08 22:36:39.472036: Yayy! New best EMA pseudo Dice: 0.6785
2024-12-08 22:36:41.072153: 
2024-12-08 22:36:41.073952: Epoch 23
2024-12-08 22:36:41.074696: Current learning rate: 0.00979
2024-12-08 22:38:09.888914: Validation loss did not improve from -0.50159. Patience: 4/50
2024-12-08 22:38:09.889994: train_loss -0.5914
2024-12-08 22:38:09.891042: val_loss -0.4596
2024-12-08 22:38:09.892087: Pseudo dice [0.7022]
2024-12-08 22:38:09.893311: Epoch time: 88.82 s
2024-12-08 22:38:09.894346: Yayy! New best EMA pseudo Dice: 0.6809
2024-12-08 22:38:11.465284: 
2024-12-08 22:38:11.467205: Epoch 24
2024-12-08 22:38:11.468169: Current learning rate: 0.00978
2024-12-08 22:39:40.531768: Validation loss did not improve from -0.50159. Patience: 5/50
2024-12-08 22:39:40.532746: train_loss -0.5939
2024-12-08 22:39:40.533491: val_loss -0.4715
2024-12-08 22:39:40.534121: Pseudo dice [0.7132]
2024-12-08 22:39:40.534806: Epoch time: 89.07 s
2024-12-08 22:39:40.899641: Yayy! New best EMA pseudo Dice: 0.6841
2024-12-08 22:39:42.465996: 
2024-12-08 22:39:42.467111: Epoch 25
2024-12-08 22:39:42.467908: Current learning rate: 0.00977
2024-12-08 22:41:11.517172: Validation loss did not improve from -0.50159. Patience: 6/50
2024-12-08 22:41:11.518348: train_loss -0.6019
2024-12-08 22:41:11.519328: val_loss -0.4199
2024-12-08 22:41:11.520009: Pseudo dice [0.6823]
2024-12-08 22:41:11.520889: Epoch time: 89.05 s
2024-12-08 22:41:12.761100: 
2024-12-08 22:41:12.762572: Epoch 26
2024-12-08 22:41:12.763363: Current learning rate: 0.00977
2024-12-08 22:42:41.958622: Validation loss did not improve from -0.50159. Patience: 7/50
2024-12-08 22:42:41.959615: train_loss -0.5942
2024-12-08 22:42:41.960558: val_loss -0.4543
2024-12-08 22:42:41.961357: Pseudo dice [0.6889]
2024-12-08 22:42:41.962005: Epoch time: 89.2 s
2024-12-08 22:42:41.962745: Yayy! New best EMA pseudo Dice: 0.6844
2024-12-08 22:42:43.566178: 
2024-12-08 22:42:43.567997: Epoch 27
2024-12-08 22:42:43.568887: Current learning rate: 0.00976
2024-12-08 22:44:12.793580: Validation loss did not improve from -0.50159. Patience: 8/50
2024-12-08 22:44:12.794791: train_loss -0.6009
2024-12-08 22:44:12.795841: val_loss -0.4455
2024-12-08 22:44:12.796759: Pseudo dice [0.6913]
2024-12-08 22:44:12.797583: Epoch time: 89.23 s
2024-12-08 22:44:12.798502: Yayy! New best EMA pseudo Dice: 0.6851
2024-12-08 22:44:14.387910: 
2024-12-08 22:44:14.389459: Epoch 28
2024-12-08 22:44:14.390434: Current learning rate: 0.00975
2024-12-08 22:45:43.545974: Validation loss did not improve from -0.50159. Patience: 9/50
2024-12-08 22:45:43.547389: train_loss -0.6061
2024-12-08 22:45:43.548279: val_loss -0.4215
2024-12-08 22:45:43.549043: Pseudo dice [0.6809]
2024-12-08 22:45:43.549758: Epoch time: 89.16 s
2024-12-08 22:45:45.187167: 
2024-12-08 22:45:45.189099: Epoch 29
2024-12-08 22:45:45.190044: Current learning rate: 0.00974
2024-12-08 22:47:14.307230: Validation loss did not improve from -0.50159. Patience: 10/50
2024-12-08 22:47:14.308909: train_loss -0.6149
2024-12-08 22:47:14.310002: val_loss -0.4991
2024-12-08 22:47:14.310994: Pseudo dice [0.7274]
2024-12-08 22:47:14.312108: Epoch time: 89.12 s
2024-12-08 22:47:14.683657: Yayy! New best EMA pseudo Dice: 0.689
2024-12-08 22:47:16.238080: 
2024-12-08 22:47:16.239664: Epoch 30
2024-12-08 22:47:16.240484: Current learning rate: 0.00973
2024-12-08 22:48:45.231715: Validation loss did not improve from -0.50159. Patience: 11/50
2024-12-08 22:48:45.233007: train_loss -0.6132
2024-12-08 22:48:45.233873: val_loss -0.4753
2024-12-08 22:48:45.234578: Pseudo dice [0.7097]
2024-12-08 22:48:45.235463: Epoch time: 89.0 s
2024-12-08 22:48:45.236199: Yayy! New best EMA pseudo Dice: 0.691
2024-12-08 22:48:46.870193: 
2024-12-08 22:48:46.871922: Epoch 31
2024-12-08 22:48:46.872858: Current learning rate: 0.00972
2024-12-08 22:50:16.012653: Validation loss did not improve from -0.50159. Patience: 12/50
2024-12-08 22:50:16.013944: train_loss -0.5969
2024-12-08 22:50:16.014821: val_loss -0.4657
2024-12-08 22:50:16.015628: Pseudo dice [0.7056]
2024-12-08 22:50:16.016417: Epoch time: 89.14 s
2024-12-08 22:50:16.017047: Yayy! New best EMA pseudo Dice: 0.6925
2024-12-08 22:50:17.637089: 
2024-12-08 22:50:17.638621: Epoch 32
2024-12-08 22:50:17.639449: Current learning rate: 0.00971
2024-12-08 22:51:46.563114: Validation loss did not improve from -0.50159. Patience: 13/50
2024-12-08 22:51:46.564235: train_loss -0.6161
2024-12-08 22:51:46.565479: val_loss -0.4546
2024-12-08 22:51:46.566543: Pseudo dice [0.7074]
2024-12-08 22:51:46.567424: Epoch time: 88.93 s
2024-12-08 22:51:46.568390: Yayy! New best EMA pseudo Dice: 0.694
2024-12-08 22:51:48.221334: 
2024-12-08 22:51:48.222999: Epoch 33
2024-12-08 22:51:48.223804: Current learning rate: 0.0097
2024-12-08 22:53:17.279252: Validation loss did not improve from -0.50159. Patience: 14/50
2024-12-08 22:53:17.280383: train_loss -0.6216
2024-12-08 22:53:17.281393: val_loss -0.4808
2024-12-08 22:53:17.282268: Pseudo dice [0.7119]
2024-12-08 22:53:17.283060: Epoch time: 89.06 s
2024-12-08 22:53:17.283771: Yayy! New best EMA pseudo Dice: 0.6958
2024-12-08 22:53:18.939847: 
2024-12-08 22:53:18.941385: Epoch 34
2024-12-08 22:53:18.942280: Current learning rate: 0.00969
2024-12-08 22:54:47.957759: Validation loss improved from -0.50159 to -0.51011! Patience: 14/50
2024-12-08 22:54:47.959140: train_loss -0.6317
2024-12-08 22:54:47.960313: val_loss -0.5101
2024-12-08 22:54:47.961244: Pseudo dice [0.7331]
2024-12-08 22:54:47.962086: Epoch time: 89.02 s
2024-12-08 22:54:48.333587: Yayy! New best EMA pseudo Dice: 0.6995
2024-12-08 22:54:49.970689: 
2024-12-08 22:54:49.972407: Epoch 35
2024-12-08 22:54:49.973277: Current learning rate: 0.00968
2024-12-08 22:56:18.828283: Validation loss did not improve from -0.51011. Patience: 1/50
2024-12-08 22:56:18.829643: train_loss -0.639
2024-12-08 22:56:18.830483: val_loss -0.5068
2024-12-08 22:56:18.831312: Pseudo dice [0.7245]
2024-12-08 22:56:18.832148: Epoch time: 88.86 s
2024-12-08 22:56:18.832954: Yayy! New best EMA pseudo Dice: 0.702
2024-12-08 22:56:20.463455: 
2024-12-08 22:56:20.464731: Epoch 36
2024-12-08 22:56:20.465621: Current learning rate: 0.00968
2024-12-08 22:57:49.407554: Validation loss did not improve from -0.51011. Patience: 2/50
2024-12-08 22:57:49.408882: train_loss -0.6402
2024-12-08 22:57:49.409921: val_loss -0.4797
2024-12-08 22:57:49.410874: Pseudo dice [0.7145]
2024-12-08 22:57:49.411647: Epoch time: 88.95 s
2024-12-08 22:57:49.412354: Yayy! New best EMA pseudo Dice: 0.7033
2024-12-08 22:57:51.052812: 
2024-12-08 22:57:51.054282: Epoch 37
2024-12-08 22:57:51.055115: Current learning rate: 0.00967
2024-12-08 22:59:20.024286: Validation loss did not improve from -0.51011. Patience: 3/50
2024-12-08 22:59:20.025536: train_loss -0.6314
2024-12-08 22:59:20.026708: val_loss -0.4714
2024-12-08 22:59:20.027551: Pseudo dice [0.7136]
2024-12-08 22:59:20.028378: Epoch time: 88.97 s
2024-12-08 22:59:20.029338: Yayy! New best EMA pseudo Dice: 0.7043
2024-12-08 22:59:21.658409: 
2024-12-08 22:59:21.660271: Epoch 38
2024-12-08 22:59:21.661390: Current learning rate: 0.00966
2024-12-08 23:00:50.647596: Validation loss improved from -0.51011 to -0.51085! Patience: 3/50
2024-12-08 23:00:50.648744: train_loss -0.6274
2024-12-08 23:00:50.649561: val_loss -0.5108
2024-12-08 23:00:50.650354: Pseudo dice [0.7332]
2024-12-08 23:00:50.651219: Epoch time: 88.99 s
2024-12-08 23:00:50.652142: Yayy! New best EMA pseudo Dice: 0.7072
2024-12-08 23:00:52.587310: 
2024-12-08 23:00:52.588865: Epoch 39
2024-12-08 23:00:52.589967: Current learning rate: 0.00965
2024-12-08 23:02:21.631434: Validation loss did not improve from -0.51085. Patience: 1/50
2024-12-08 23:02:21.632427: train_loss -0.6302
2024-12-08 23:02:21.633530: val_loss -0.4848
2024-12-08 23:02:21.634492: Pseudo dice [0.7151]
2024-12-08 23:02:21.635390: Epoch time: 89.05 s
2024-12-08 23:02:22.018675: Yayy! New best EMA pseudo Dice: 0.708
2024-12-08 23:02:23.687452: 
2024-12-08 23:02:23.689245: Epoch 40
2024-12-08 23:02:23.690189: Current learning rate: 0.00964
2024-12-08 23:03:53.064981: Validation loss did not improve from -0.51085. Patience: 2/50
2024-12-08 23:03:53.066151: train_loss -0.6253
2024-12-08 23:03:53.067215: val_loss -0.4832
2024-12-08 23:03:53.068057: Pseudo dice [0.7212]
2024-12-08 23:03:53.069020: Epoch time: 89.38 s
2024-12-08 23:03:53.069717: Yayy! New best EMA pseudo Dice: 0.7093
2024-12-08 23:03:54.739966: 
2024-12-08 23:03:54.741847: Epoch 41
2024-12-08 23:03:54.742780: Current learning rate: 0.00963
2024-12-08 23:05:23.920039: Validation loss did not improve from -0.51085. Patience: 3/50
2024-12-08 23:05:23.921147: train_loss -0.6353
2024-12-08 23:05:23.921933: val_loss -0.4471
2024-12-08 23:05:23.922705: Pseudo dice [0.7011]
2024-12-08 23:05:23.923383: Epoch time: 89.18 s
2024-12-08 23:05:25.139428: 
2024-12-08 23:05:25.141137: Epoch 42
2024-12-08 23:05:25.141851: Current learning rate: 0.00962
2024-12-08 23:06:54.645304: Validation loss did not improve from -0.51085. Patience: 4/50
2024-12-08 23:06:54.650067: train_loss -0.6416
2024-12-08 23:06:54.688744: val_loss -0.4831
2024-12-08 23:06:54.690214: Pseudo dice [0.705]
2024-12-08 23:06:54.692029: Epoch time: 89.51 s
2024-12-08 23:06:55.975081: 
2024-12-08 23:06:55.976906: Epoch 43
2024-12-08 23:06:55.978336: Current learning rate: 0.00961
2024-12-08 23:08:25.276626: Validation loss did not improve from -0.51085. Patience: 5/50
2024-12-08 23:08:25.277437: train_loss -0.6476
2024-12-08 23:08:25.278717: val_loss -0.442
2024-12-08 23:08:25.279994: Pseudo dice [0.6959]
2024-12-08 23:08:25.280983: Epoch time: 89.3 s
2024-12-08 23:08:26.510198: 
2024-12-08 23:08:26.511708: Epoch 44
2024-12-08 23:08:26.512729: Current learning rate: 0.0096
2024-12-08 23:09:55.712470: Validation loss did not improve from -0.51085. Patience: 6/50
2024-12-08 23:09:55.714409: train_loss -0.6512
2024-12-08 23:09:55.715997: val_loss -0.4684
2024-12-08 23:09:55.716848: Pseudo dice [0.7104]
2024-12-08 23:09:55.717611: Epoch time: 89.21 s
2024-12-08 23:09:57.289438: 
2024-12-08 23:09:57.290869: Epoch 45
2024-12-08 23:09:57.291613: Current learning rate: 0.00959
2024-12-08 23:11:26.411026: Validation loss did not improve from -0.51085. Patience: 7/50
2024-12-08 23:11:26.412267: train_loss -0.6457
2024-12-08 23:11:26.413145: val_loss -0.4893
2024-12-08 23:11:26.414058: Pseudo dice [0.7173]
2024-12-08 23:11:26.414815: Epoch time: 89.12 s
2024-12-08 23:11:27.630120: 
2024-12-08 23:11:27.631613: Epoch 46
2024-12-08 23:11:27.632370: Current learning rate: 0.00959
2024-12-08 23:12:56.857925: Validation loss did not improve from -0.51085. Patience: 8/50
2024-12-08 23:12:56.858920: train_loss -0.6489
2024-12-08 23:12:56.860171: val_loss -0.4705
2024-12-08 23:12:56.860949: Pseudo dice [0.7077]
2024-12-08 23:12:56.861831: Epoch time: 89.23 s
2024-12-08 23:12:58.088736: 
2024-12-08 23:12:58.090315: Epoch 47
2024-12-08 23:12:58.091219: Current learning rate: 0.00958
2024-12-08 23:14:27.361679: Validation loss did not improve from -0.51085. Patience: 9/50
2024-12-08 23:14:27.362956: train_loss -0.6427
2024-12-08 23:14:27.363750: val_loss -0.4218
2024-12-08 23:14:27.364467: Pseudo dice [0.6899]
2024-12-08 23:14:27.365329: Epoch time: 89.28 s
2024-12-08 23:14:28.557929: 
2024-12-08 23:14:28.559383: Epoch 48
2024-12-08 23:14:28.560130: Current learning rate: 0.00957
2024-12-08 23:15:57.986006: Validation loss did not improve from -0.51085. Patience: 10/50
2024-12-08 23:15:57.987082: train_loss -0.645
2024-12-08 23:15:57.988017: val_loss -0.4472
2024-12-08 23:15:57.988925: Pseudo dice [0.6977]
2024-12-08 23:15:57.989650: Epoch time: 89.43 s
2024-12-08 23:15:59.233345: 
2024-12-08 23:15:59.234708: Epoch 49
2024-12-08 23:15:59.235563: Current learning rate: 0.00956
2024-12-08 23:17:28.333371: Validation loss did not improve from -0.51085. Patience: 11/50
2024-12-08 23:17:28.334609: train_loss -0.6477
2024-12-08 23:17:28.335624: val_loss -0.4854
2024-12-08 23:17:28.336327: Pseudo dice [0.7137]
2024-12-08 23:17:28.337163: Epoch time: 89.1 s
2024-12-08 23:17:30.651467: 
2024-12-08 23:17:30.653320: Epoch 50
2024-12-08 23:17:30.654148: Current learning rate: 0.00955
2024-12-08 23:18:59.323200: Validation loss did not improve from -0.51085. Patience: 12/50
2024-12-08 23:18:59.323907: train_loss -0.6607
2024-12-08 23:18:59.324861: val_loss -0.5027
2024-12-08 23:18:59.325806: Pseudo dice [0.7369]
2024-12-08 23:18:59.326638: Epoch time: 88.67 s
2024-12-08 23:18:59.327573: Yayy! New best EMA pseudo Dice: 0.7094
2024-12-08 23:19:00.883937: 
2024-12-08 23:19:00.885583: Epoch 51
2024-12-08 23:19:00.886751: Current learning rate: 0.00954
2024-12-08 23:20:29.397339: Validation loss did not improve from -0.51085. Patience: 13/50
2024-12-08 23:20:29.398561: train_loss -0.6619
2024-12-08 23:20:29.399583: val_loss -0.4857
2024-12-08 23:20:29.400394: Pseudo dice [0.7131]
2024-12-08 23:20:29.401178: Epoch time: 88.52 s
2024-12-08 23:20:29.401915: Yayy! New best EMA pseudo Dice: 0.7097
2024-12-08 23:20:31.024374: 
2024-12-08 23:20:31.025960: Epoch 52
2024-12-08 23:20:31.026921: Current learning rate: 0.00953
2024-12-08 23:21:59.621954: Validation loss did not improve from -0.51085. Patience: 14/50
2024-12-08 23:21:59.622844: train_loss -0.6479
2024-12-08 23:21:59.623673: val_loss -0.4568
2024-12-08 23:21:59.624482: Pseudo dice [0.703]
2024-12-08 23:21:59.625318: Epoch time: 88.6 s
2024-12-08 23:22:00.900894: 
2024-12-08 23:22:00.902424: Epoch 53
2024-12-08 23:22:00.903120: Current learning rate: 0.00952
2024-12-08 23:23:29.464168: Validation loss did not improve from -0.51085. Patience: 15/50
2024-12-08 23:23:29.465522: train_loss -0.6602
2024-12-08 23:23:29.466708: val_loss -0.4886
2024-12-08 23:23:29.467656: Pseudo dice [0.7168]
2024-12-08 23:23:29.468553: Epoch time: 88.57 s
2024-12-08 23:23:29.469244: Yayy! New best EMA pseudo Dice: 0.7098
2024-12-08 23:23:31.080062: 
2024-12-08 23:23:31.081530: Epoch 54
2024-12-08 23:23:31.082272: Current learning rate: 0.00951
2024-12-08 23:24:59.726635: Validation loss did not improve from -0.51085. Patience: 16/50
2024-12-08 23:24:59.727514: train_loss -0.6638
2024-12-08 23:24:59.728486: val_loss -0.4922
2024-12-08 23:24:59.729215: Pseudo dice [0.7216]
2024-12-08 23:24:59.730020: Epoch time: 88.65 s
2024-12-08 23:25:00.106620: Yayy! New best EMA pseudo Dice: 0.711
2024-12-08 23:25:01.693253: 
2024-12-08 23:25:01.695065: Epoch 55
2024-12-08 23:25:01.695992: Current learning rate: 0.0095
2024-12-08 23:26:30.382957: Validation loss did not improve from -0.51085. Patience: 17/50
2024-12-08 23:26:30.384406: train_loss -0.6668
2024-12-08 23:26:30.385345: val_loss -0.4566
2024-12-08 23:26:30.385967: Pseudo dice [0.7074]
2024-12-08 23:26:30.386650: Epoch time: 88.69 s
2024-12-08 23:26:31.661361: 
2024-12-08 23:26:31.663108: Epoch 56
2024-12-08 23:26:31.663941: Current learning rate: 0.00949
2024-12-08 23:28:00.419044: Validation loss did not improve from -0.51085. Patience: 18/50
2024-12-08 23:28:00.420292: train_loss -0.6677
2024-12-08 23:28:00.421419: val_loss -0.4503
2024-12-08 23:28:00.422199: Pseudo dice [0.7029]
2024-12-08 23:28:00.422972: Epoch time: 88.76 s
2024-12-08 23:28:01.697340: 
2024-12-08 23:28:01.698710: Epoch 57
2024-12-08 23:28:01.699467: Current learning rate: 0.00949
2024-12-08 23:29:30.410927: Validation loss did not improve from -0.51085. Patience: 19/50
2024-12-08 23:29:30.412007: train_loss -0.6685
2024-12-08 23:29:30.412844: val_loss -0.4281
2024-12-08 23:29:30.413611: Pseudo dice [0.6921]
2024-12-08 23:29:30.414298: Epoch time: 88.72 s
2024-12-08 23:29:31.715677: 
2024-12-08 23:29:31.717381: Epoch 58
2024-12-08 23:29:31.718551: Current learning rate: 0.00948
2024-12-08 23:31:00.657563: Validation loss did not improve from -0.51085. Patience: 20/50
2024-12-08 23:31:00.658674: train_loss -0.6716
2024-12-08 23:31:00.659688: val_loss -0.454
2024-12-08 23:31:00.660721: Pseudo dice [0.7013]
2024-12-08 23:31:00.661665: Epoch time: 88.94 s
2024-12-08 23:31:01.896768: 
2024-12-08 23:31:01.898405: Epoch 59
2024-12-08 23:31:01.899314: Current learning rate: 0.00947
2024-12-08 23:32:31.032838: Validation loss did not improve from -0.51085. Patience: 21/50
2024-12-08 23:32:31.034051: train_loss -0.6699
2024-12-08 23:32:31.035178: val_loss -0.485
2024-12-08 23:32:31.036225: Pseudo dice [0.7197]
2024-12-08 23:32:31.037192: Epoch time: 89.14 s
2024-12-08 23:32:32.657919: 
2024-12-08 23:32:32.659305: Epoch 60
2024-12-08 23:32:32.660304: Current learning rate: 0.00946
2024-12-08 23:34:01.747484: Validation loss did not improve from -0.51085. Patience: 22/50
2024-12-08 23:34:01.748499: train_loss -0.6718
2024-12-08 23:34:01.749442: val_loss -0.504
2024-12-08 23:34:01.750151: Pseudo dice [0.7106]
2024-12-08 23:34:01.750895: Epoch time: 89.09 s
2024-12-08 23:34:03.378219: 
2024-12-08 23:34:03.380072: Epoch 61
2024-12-08 23:34:03.381089: Current learning rate: 0.00945
2024-12-08 23:35:32.551642: Validation loss did not improve from -0.51085. Patience: 23/50
2024-12-08 23:35:32.552736: train_loss -0.6744
2024-12-08 23:35:32.553704: val_loss -0.479
2024-12-08 23:35:32.554552: Pseudo dice [0.7168]
2024-12-08 23:35:32.555560: Epoch time: 89.18 s
2024-12-08 23:35:33.854605: 
2024-12-08 23:35:33.856385: Epoch 62
2024-12-08 23:35:33.857207: Current learning rate: 0.00944
2024-12-08 23:37:02.999319: Validation loss did not improve from -0.51085. Patience: 24/50
2024-12-08 23:37:03.000372: train_loss -0.6763
2024-12-08 23:37:03.001428: val_loss -0.5092
2024-12-08 23:37:03.002457: Pseudo dice [0.735]
2024-12-08 23:37:03.003431: Epoch time: 89.15 s
2024-12-08 23:37:03.004351: Yayy! New best EMA pseudo Dice: 0.7122
2024-12-08 23:37:04.606004: 
2024-12-08 23:37:04.607709: Epoch 63
2024-12-08 23:37:04.609042: Current learning rate: 0.00943
2024-12-08 23:38:33.676343: Validation loss improved from -0.51085 to -0.51437! Patience: 24/50
2024-12-08 23:38:33.677506: train_loss -0.6781
2024-12-08 23:38:33.678344: val_loss -0.5144
2024-12-08 23:38:33.679201: Pseudo dice [0.7363]
2024-12-08 23:38:33.679994: Epoch time: 89.07 s
2024-12-08 23:38:33.680777: Yayy! New best EMA pseudo Dice: 0.7146
2024-12-08 23:38:35.277541: 
2024-12-08 23:38:35.279392: Epoch 64
2024-12-08 23:38:35.280991: Current learning rate: 0.00942
2024-12-08 23:40:04.362799: Validation loss did not improve from -0.51437. Patience: 1/50
2024-12-08 23:40:04.364111: train_loss -0.6808
2024-12-08 23:40:04.365194: val_loss -0.4995
2024-12-08 23:40:04.366075: Pseudo dice [0.7221]
2024-12-08 23:40:04.367066: Epoch time: 89.09 s
2024-12-08 23:40:04.735321: Yayy! New best EMA pseudo Dice: 0.7153
2024-12-08 23:40:06.340131: 
2024-12-08 23:40:06.342258: Epoch 65
2024-12-08 23:40:06.343383: Current learning rate: 0.00941
2024-12-08 23:41:35.380317: Validation loss did not improve from -0.51437. Patience: 2/50
2024-12-08 23:41:35.381200: train_loss -0.6821
2024-12-08 23:41:35.381992: val_loss -0.4847
2024-12-08 23:41:35.382613: Pseudo dice [0.7163]
2024-12-08 23:41:35.383381: Epoch time: 89.04 s
2024-12-08 23:41:35.384167: Yayy! New best EMA pseudo Dice: 0.7154
2024-12-08 23:41:37.009213: 
2024-12-08 23:41:37.010806: Epoch 66
2024-12-08 23:41:37.011648: Current learning rate: 0.0094
2024-12-08 23:43:06.268790: Validation loss did not improve from -0.51437. Patience: 3/50
2024-12-08 23:43:06.269799: train_loss -0.6824
2024-12-08 23:43:06.270874: val_loss -0.4997
2024-12-08 23:43:06.271862: Pseudo dice [0.7178]
2024-12-08 23:43:06.272789: Epoch time: 89.26 s
2024-12-08 23:43:06.273827: Yayy! New best EMA pseudo Dice: 0.7157
2024-12-08 23:43:07.896052: 
2024-12-08 23:43:07.897607: Epoch 67
2024-12-08 23:43:07.898637: Current learning rate: 0.00939
2024-12-08 23:44:37.256823: Validation loss did not improve from -0.51437. Patience: 4/50
2024-12-08 23:44:37.257774: train_loss -0.6862
2024-12-08 23:44:37.258817: val_loss -0.4472
2024-12-08 23:44:37.259518: Pseudo dice [0.6911]
2024-12-08 23:44:37.260184: Epoch time: 89.36 s
2024-12-08 23:44:38.569211: 
2024-12-08 23:44:38.571014: Epoch 68
2024-12-08 23:44:38.571732: Current learning rate: 0.00939
2024-12-08 23:46:07.921832: Validation loss improved from -0.51437 to -0.51964! Patience: 4/50
2024-12-08 23:46:07.922850: train_loss -0.6867
2024-12-08 23:46:07.923873: val_loss -0.5196
2024-12-08 23:46:07.924569: Pseudo dice [0.7364]
2024-12-08 23:46:07.925277: Epoch time: 89.35 s
2024-12-08 23:46:09.260217: 
2024-12-08 23:46:09.262245: Epoch 69
2024-12-08 23:46:09.263338: Current learning rate: 0.00938
2024-12-08 23:47:38.373162: Validation loss did not improve from -0.51964. Patience: 1/50
2024-12-08 23:47:38.374267: train_loss -0.6857
2024-12-08 23:47:38.375205: val_loss -0.5019
2024-12-08 23:47:38.375963: Pseudo dice [0.7268]
2024-12-08 23:47:38.376836: Epoch time: 89.12 s
2024-12-08 23:47:38.747023: Yayy! New best EMA pseudo Dice: 0.7167
2024-12-08 23:47:40.355855: 
2024-12-08 23:47:40.357490: Epoch 70
2024-12-08 23:47:40.358637: Current learning rate: 0.00937
2024-12-08 23:49:09.589555: Validation loss did not improve from -0.51964. Patience: 2/50
2024-12-08 23:49:09.590631: train_loss -0.6714
2024-12-08 23:49:09.591756: val_loss -0.426
2024-12-08 23:49:09.592776: Pseudo dice [0.6928]
2024-12-08 23:49:09.593771: Epoch time: 89.24 s
2024-12-08 23:49:10.848262: 
2024-12-08 23:49:10.850022: Epoch 71
2024-12-08 23:49:10.851206: Current learning rate: 0.00936
2024-12-08 23:50:39.929603: Validation loss did not improve from -0.51964. Patience: 3/50
2024-12-08 23:50:39.931046: train_loss -0.6734
2024-12-08 23:50:39.932566: val_loss -0.4697
2024-12-08 23:50:39.933401: Pseudo dice [0.7058]
2024-12-08 23:50:39.934136: Epoch time: 89.08 s
2024-12-08 23:50:41.619055: 
2024-12-08 23:50:41.620735: Epoch 72
2024-12-08 23:50:41.621667: Current learning rate: 0.00935
2024-12-08 23:52:10.903108: Validation loss did not improve from -0.51964. Patience: 4/50
2024-12-08 23:52:10.904293: train_loss -0.6783
2024-12-08 23:52:10.905423: val_loss -0.4885
2024-12-08 23:52:10.906353: Pseudo dice [0.7201]
2024-12-08 23:52:10.907194: Epoch time: 89.29 s
2024-12-08 23:52:12.185392: 
2024-12-08 23:52:12.187536: Epoch 73
2024-12-08 23:52:12.188715: Current learning rate: 0.00934
2024-12-08 23:53:41.476420: Validation loss did not improve from -0.51964. Patience: 5/50
2024-12-08 23:53:41.477380: train_loss -0.6846
2024-12-08 23:53:41.478313: val_loss -0.513
2024-12-08 23:53:41.479180: Pseudo dice [0.7175]
2024-12-08 23:53:41.480096: Epoch time: 89.29 s
2024-12-08 23:53:42.743427: 
2024-12-08 23:53:42.745266: Epoch 74
2024-12-08 23:53:42.746195: Current learning rate: 0.00933
2024-12-08 23:55:11.979386: Validation loss did not improve from -0.51964. Patience: 6/50
2024-12-08 23:55:11.980192: train_loss -0.6851
2024-12-08 23:55:11.981215: val_loss -0.4676
2024-12-08 23:55:11.982052: Pseudo dice [0.7011]
2024-12-08 23:55:11.982717: Epoch time: 89.24 s
2024-12-08 23:55:13.650884: 
2024-12-08 23:55:13.652488: Epoch 75
2024-12-08 23:55:13.653383: Current learning rate: 0.00932
2024-12-08 23:56:42.978557: Validation loss did not improve from -0.51964. Patience: 7/50
2024-12-08 23:56:42.979606: train_loss -0.6796
2024-12-08 23:56:42.980808: val_loss -0.4909
2024-12-08 23:56:42.981647: Pseudo dice [0.7167]
2024-12-08 23:56:42.982501: Epoch time: 89.33 s
2024-12-08 23:56:44.225086: 
2024-12-08 23:56:44.226788: Epoch 76
2024-12-08 23:56:44.227945: Current learning rate: 0.00931
2024-12-08 23:58:13.646897: Validation loss improved from -0.51964 to -0.53570! Patience: 7/50
2024-12-08 23:58:13.648263: train_loss -0.6952
2024-12-08 23:58:13.649308: val_loss -0.5357
2024-12-08 23:58:13.649948: Pseudo dice [0.74]
2024-12-08 23:58:13.650657: Epoch time: 89.42 s
2024-12-08 23:58:14.938823: 
2024-12-08 23:58:14.940590: Epoch 77
2024-12-08 23:58:14.941504: Current learning rate: 0.0093
2024-12-08 23:59:43.094788: Validation loss did not improve from -0.53570. Patience: 1/50
2024-12-08 23:59:43.096016: train_loss -0.697
2024-12-08 23:59:43.097278: val_loss -0.3851
2024-12-08 23:59:43.098165: Pseudo dice [0.671]
2024-12-08 23:59:43.098879: Epoch time: 88.16 s
2024-12-08 23:59:44.392767: 
2024-12-08 23:59:44.394363: Epoch 78
2024-12-08 23:59:44.395123: Current learning rate: 0.0093
2024-12-09 00:01:12.758077: Validation loss did not improve from -0.53570. Patience: 2/50
2024-12-09 00:01:12.759544: train_loss -0.6949
2024-12-09 00:01:12.760496: val_loss -0.5167
2024-12-09 00:01:12.761303: Pseudo dice [0.7363]
2024-12-09 00:01:12.762175: Epoch time: 88.37 s
2024-12-09 00:01:14.048716: 
2024-12-09 00:01:14.050526: Epoch 79
2024-12-09 00:01:14.051774: Current learning rate: 0.00929
2024-12-09 00:02:42.252851: Validation loss did not improve from -0.53570. Patience: 3/50
2024-12-09 00:02:42.254003: train_loss -0.6988
2024-12-09 00:02:42.255120: val_loss -0.4939
2024-12-09 00:02:42.255897: Pseudo dice [0.7208]
2024-12-09 00:02:42.256696: Epoch time: 88.21 s
2024-12-09 00:02:43.977582: 
2024-12-09 00:02:43.979310: Epoch 80
2024-12-09 00:02:43.980386: Current learning rate: 0.00928
2024-12-09 00:04:12.299665: Validation loss did not improve from -0.53570. Patience: 4/50
2024-12-09 00:04:12.300866: train_loss -0.7028
2024-12-09 00:04:12.301923: val_loss -0.466
2024-12-09 00:04:12.302923: Pseudo dice [0.7003]
2024-12-09 00:04:12.303692: Epoch time: 88.32 s
2024-12-09 00:04:13.655508: 
2024-12-09 00:04:13.656766: Epoch 81
2024-12-09 00:04:13.657497: Current learning rate: 0.00927
2024-12-09 00:05:41.737603: Validation loss did not improve from -0.53570. Patience: 5/50
2024-12-09 00:05:41.738770: train_loss -0.6982
2024-12-09 00:05:41.739721: val_loss -0.4819
2024-12-09 00:05:41.740491: Pseudo dice [0.7085]
2024-12-09 00:05:41.741388: Epoch time: 88.08 s
2024-12-09 00:05:43.456806: 
2024-12-09 00:05:43.458675: Epoch 82
2024-12-09 00:05:43.459444: Current learning rate: 0.00926
2024-12-09 00:07:11.010319: Validation loss did not improve from -0.53570. Patience: 6/50
2024-12-09 00:07:11.011293: train_loss -0.7106
2024-12-09 00:07:11.012341: val_loss -0.4816
2024-12-09 00:07:11.013076: Pseudo dice [0.7149]
2024-12-09 00:07:11.013928: Epoch time: 87.56 s
2024-12-09 00:07:12.200681: 
2024-12-09 00:07:12.201838: Epoch 83
2024-12-09 00:07:12.202520: Current learning rate: 0.00925
2024-12-09 00:08:39.045138: Validation loss did not improve from -0.53570. Patience: 7/50
2024-12-09 00:08:39.046117: train_loss -0.7055
2024-12-09 00:08:39.047201: val_loss -0.5003
2024-12-09 00:08:39.047903: Pseudo dice [0.7218]
2024-12-09 00:08:39.048553: Epoch time: 86.85 s
2024-12-09 00:08:40.256875: 
2024-12-09 00:08:40.257900: Epoch 84
2024-12-09 00:08:40.258687: Current learning rate: 0.00924
2024-12-09 00:10:09.219342: Validation loss did not improve from -0.53570. Patience: 8/50
2024-12-09 00:10:09.220512: train_loss -0.7042
2024-12-09 00:10:09.221598: val_loss -0.4717
2024-12-09 00:10:09.222803: Pseudo dice [0.7076]
2024-12-09 00:10:09.223758: Epoch time: 88.96 s
2024-12-09 00:10:10.734189: 
2024-12-09 00:10:10.736227: Epoch 85
2024-12-09 00:10:10.737417: Current learning rate: 0.00923
2024-12-09 00:11:39.483309: Validation loss did not improve from -0.53570. Patience: 9/50
2024-12-09 00:11:39.485039: train_loss -0.7047
2024-12-09 00:11:39.486214: val_loss -0.5172
2024-12-09 00:11:39.487257: Pseudo dice [0.7329]
2024-12-09 00:11:39.488175: Epoch time: 88.75 s
2024-12-09 00:11:40.666089: 
2024-12-09 00:11:40.668078: Epoch 86
2024-12-09 00:11:40.669181: Current learning rate: 0.00922
2024-12-09 00:13:09.275730: Validation loss did not improve from -0.53570. Patience: 10/50
2024-12-09 00:13:09.277067: train_loss -0.7003
2024-12-09 00:13:09.277868: val_loss -0.5065
2024-12-09 00:13:09.278989: Pseudo dice [0.7296]
2024-12-09 00:13:09.279763: Epoch time: 88.61 s
2024-12-09 00:13:09.280378: Yayy! New best EMA pseudo Dice: 0.7167
2024-12-09 00:13:10.845380: 
2024-12-09 00:13:10.847316: Epoch 87
2024-12-09 00:13:10.848441: Current learning rate: 0.00921
2024-12-09 00:14:39.585235: Validation loss did not improve from -0.53570. Patience: 11/50
2024-12-09 00:14:39.586642: train_loss -0.7142
2024-12-09 00:14:39.588145: val_loss -0.4564
2024-12-09 00:14:39.589234: Pseudo dice [0.7004]
2024-12-09 00:14:39.590098: Epoch time: 88.74 s
2024-12-09 00:14:40.764018: 
2024-12-09 00:14:40.766303: Epoch 88
2024-12-09 00:14:40.767356: Current learning rate: 0.0092
2024-12-09 00:16:09.329193: Validation loss did not improve from -0.53570. Patience: 12/50
2024-12-09 00:16:09.330372: train_loss -0.7014
2024-12-09 00:16:09.331645: val_loss -0.5242
2024-12-09 00:16:09.332731: Pseudo dice [0.7387]
2024-12-09 00:16:09.333934: Epoch time: 88.57 s
2024-12-09 00:16:09.334798: Yayy! New best EMA pseudo Dice: 0.7174
2024-12-09 00:16:10.857581: 
2024-12-09 00:16:10.859880: Epoch 89
2024-12-09 00:16:10.861179: Current learning rate: 0.0092
2024-12-09 00:17:39.332551: Validation loss did not improve from -0.53570. Patience: 13/50
2024-12-09 00:17:39.333824: train_loss -0.7055
2024-12-09 00:17:39.335038: val_loss -0.4734
2024-12-09 00:17:39.336215: Pseudo dice [0.7208]
2024-12-09 00:17:39.336873: Epoch time: 88.48 s
2024-12-09 00:17:39.696604: Yayy! New best EMA pseudo Dice: 0.7177
2024-12-09 00:17:41.258858: 
2024-12-09 00:17:41.261356: Epoch 90
2024-12-09 00:17:41.262488: Current learning rate: 0.00919
2024-12-09 00:19:09.816811: Validation loss did not improve from -0.53570. Patience: 14/50
2024-12-09 00:19:09.818118: train_loss -0.7104
2024-12-09 00:19:09.819103: val_loss -0.4476
2024-12-09 00:19:09.820050: Pseudo dice [0.7084]
2024-12-09 00:19:09.820954: Epoch time: 88.56 s
2024-12-09 00:19:11.025741: 
2024-12-09 00:19:11.027663: Epoch 91
2024-12-09 00:19:11.028869: Current learning rate: 0.00918
2024-12-09 00:20:39.505639: Validation loss did not improve from -0.53570. Patience: 15/50
2024-12-09 00:20:39.506826: train_loss -0.707
2024-12-09 00:20:39.507903: val_loss -0.5136
2024-12-09 00:20:39.508893: Pseudo dice [0.7248]
2024-12-09 00:20:39.509673: Epoch time: 88.48 s
2024-12-09 00:20:40.669118: 
2024-12-09 00:20:40.671106: Epoch 92
2024-12-09 00:20:40.672177: Current learning rate: 0.00917
2024-12-09 00:22:09.093458: Validation loss did not improve from -0.53570. Patience: 16/50
2024-12-09 00:22:09.094858: train_loss -0.709
2024-12-09 00:22:09.095896: val_loss -0.4739
2024-12-09 00:22:09.096625: Pseudo dice [0.7165]
2024-12-09 00:22:09.097364: Epoch time: 88.43 s
2024-12-09 00:22:10.799023: 
2024-12-09 00:22:10.801059: Epoch 93
2024-12-09 00:22:10.801939: Current learning rate: 0.00916
2024-12-09 00:23:39.193749: Validation loss did not improve from -0.53570. Patience: 17/50
2024-12-09 00:23:39.195026: train_loss -0.7129
2024-12-09 00:23:39.196198: val_loss -0.4723
2024-12-09 00:23:39.197178: Pseudo dice [0.7081]
2024-12-09 00:23:39.198163: Epoch time: 88.4 s
2024-12-09 00:23:40.354369: 
2024-12-09 00:23:40.356610: Epoch 94
2024-12-09 00:23:40.357866: Current learning rate: 0.00915
2024-12-09 00:25:08.644771: Validation loss did not improve from -0.53570. Patience: 18/50
2024-12-09 00:25:08.646261: train_loss -0.7161
2024-12-09 00:25:08.647537: val_loss -0.5172
2024-12-09 00:25:08.648447: Pseudo dice [0.7325]
2024-12-09 00:25:08.649468: Epoch time: 88.29 s
2024-12-09 00:25:08.996900: Yayy! New best EMA pseudo Dice: 0.7181
2024-12-09 00:25:10.511294: 
2024-12-09 00:25:10.513678: Epoch 95
2024-12-09 00:25:10.514811: Current learning rate: 0.00914
2024-12-09 00:26:38.766609: Validation loss did not improve from -0.53570. Patience: 19/50
2024-12-09 00:26:38.767831: train_loss -0.7203
2024-12-09 00:26:38.768723: val_loss -0.485
2024-12-09 00:26:38.769503: Pseudo dice [0.708]
2024-12-09 00:26:38.770779: Epoch time: 88.26 s
2024-12-09 00:26:39.937396: 
2024-12-09 00:26:39.939530: Epoch 96
2024-12-09 00:26:39.940714: Current learning rate: 0.00913
2024-12-09 00:28:08.455525: Validation loss did not improve from -0.53570. Patience: 20/50
2024-12-09 00:28:08.456831: train_loss -0.7186
2024-12-09 00:28:08.457932: val_loss -0.5137
2024-12-09 00:28:08.458992: Pseudo dice [0.7361]
2024-12-09 00:28:08.459714: Epoch time: 88.52 s
2024-12-09 00:28:08.460677: Yayy! New best EMA pseudo Dice: 0.719
2024-12-09 00:28:09.960492: 
2024-12-09 00:28:09.962611: Epoch 97
2024-12-09 00:28:09.963789: Current learning rate: 0.00912
2024-12-09 00:29:38.433345: Validation loss did not improve from -0.53570. Patience: 21/50
2024-12-09 00:29:38.434233: train_loss -0.7227
2024-12-09 00:29:38.435533: val_loss -0.527
2024-12-09 00:29:38.436334: Pseudo dice [0.7384]
2024-12-09 00:29:38.437228: Epoch time: 88.47 s
2024-12-09 00:29:38.438064: Yayy! New best EMA pseudo Dice: 0.721
2024-12-09 00:29:39.954964: 
2024-12-09 00:29:39.956442: Epoch 98
2024-12-09 00:29:39.957127: Current learning rate: 0.00911
2024-12-09 00:31:08.460551: Validation loss did not improve from -0.53570. Patience: 22/50
2024-12-09 00:31:08.461689: train_loss -0.7212
2024-12-09 00:31:08.462682: val_loss -0.4675
2024-12-09 00:31:08.463419: Pseudo dice [0.7002]
2024-12-09 00:31:08.464109: Epoch time: 88.51 s
2024-12-09 00:31:09.640705: 
2024-12-09 00:31:09.642949: Epoch 99
2024-12-09 00:31:09.644497: Current learning rate: 0.0091
2024-12-09 00:32:38.359521: Validation loss did not improve from -0.53570. Patience: 23/50
2024-12-09 00:32:38.360804: train_loss -0.7191
2024-12-09 00:32:38.362439: val_loss -0.4906
2024-12-09 00:32:38.363213: Pseudo dice [0.7292]
2024-12-09 00:32:38.364129: Epoch time: 88.72 s
2024-12-09 00:32:39.849085: 
2024-12-09 00:32:39.850616: Epoch 100
2024-12-09 00:32:39.851939: Current learning rate: 0.0091
2024-12-09 00:34:08.472734: Validation loss did not improve from -0.53570. Patience: 24/50
2024-12-09 00:34:08.474147: train_loss -0.721
2024-12-09 00:34:08.475533: val_loss -0.4979
2024-12-09 00:34:08.476405: Pseudo dice [0.7184]
2024-12-09 00:34:08.477316: Epoch time: 88.63 s
2024-12-09 00:34:09.651030: 
2024-12-09 00:34:09.652677: Epoch 101
2024-12-09 00:34:09.653631: Current learning rate: 0.00909
2024-12-09 00:35:38.140698: Validation loss did not improve from -0.53570. Patience: 25/50
2024-12-09 00:35:38.141869: train_loss -0.7245
2024-12-09 00:35:38.142997: val_loss -0.4862
2024-12-09 00:35:38.143928: Pseudo dice [0.7207]
2024-12-09 00:35:38.144719: Epoch time: 88.49 s
2024-12-09 00:35:39.288480: 
2024-12-09 00:35:39.290148: Epoch 102
2024-12-09 00:35:39.291203: Current learning rate: 0.00908
2024-12-09 00:37:07.912275: Validation loss did not improve from -0.53570. Patience: 26/50
2024-12-09 00:37:07.913455: train_loss -0.7242
2024-12-09 00:37:07.914663: val_loss -0.4888
2024-12-09 00:37:07.915719: Pseudo dice [0.7229]
2024-12-09 00:37:07.916677: Epoch time: 88.63 s
2024-12-09 00:37:09.059039: 
2024-12-09 00:37:09.060790: Epoch 103
2024-12-09 00:37:09.062044: Current learning rate: 0.00907
2024-12-09 00:38:37.553416: Validation loss did not improve from -0.53570. Patience: 27/50
2024-12-09 00:38:37.554835: train_loss -0.7233
2024-12-09 00:38:37.555709: val_loss -0.4901
2024-12-09 00:38:37.556374: Pseudo dice [0.7207]
2024-12-09 00:38:37.557190: Epoch time: 88.5 s
2024-12-09 00:38:39.074609: 
2024-12-09 00:38:39.076926: Epoch 104
2024-12-09 00:38:39.077945: Current learning rate: 0.00906
2024-12-09 00:40:07.457094: Validation loss did not improve from -0.53570. Patience: 28/50
2024-12-09 00:40:07.458424: train_loss -0.7145
2024-12-09 00:40:07.459376: val_loss -0.4703
2024-12-09 00:40:07.460328: Pseudo dice [0.7073]
2024-12-09 00:40:07.461075: Epoch time: 88.39 s
2024-12-09 00:40:08.979294: 
2024-12-09 00:40:08.980726: Epoch 105
2024-12-09 00:40:08.981919: Current learning rate: 0.00905
2024-12-09 00:41:37.494448: Validation loss did not improve from -0.53570. Patience: 29/50
2024-12-09 00:41:37.495526: train_loss -0.7182
2024-12-09 00:41:37.496650: val_loss -0.52
2024-12-09 00:41:37.497589: Pseudo dice [0.7227]
2024-12-09 00:41:37.498628: Epoch time: 88.52 s
2024-12-09 00:41:38.679323: 
2024-12-09 00:41:38.681401: Epoch 106
2024-12-09 00:41:38.682562: Current learning rate: 0.00904
2024-12-09 00:43:07.168750: Validation loss did not improve from -0.53570. Patience: 30/50
2024-12-09 00:43:07.169718: train_loss -0.7099
2024-12-09 00:43:07.170565: val_loss -0.489
2024-12-09 00:43:07.171231: Pseudo dice [0.724]
2024-12-09 00:43:07.171978: Epoch time: 88.49 s
2024-12-09 00:43:08.356899: 
2024-12-09 00:43:08.359073: Epoch 107
2024-12-09 00:43:08.360314: Current learning rate: 0.00903
2024-12-09 00:44:36.728033: Validation loss did not improve from -0.53570. Patience: 31/50
2024-12-09 00:44:36.729046: train_loss -0.7204
2024-12-09 00:44:36.730218: val_loss -0.4855
2024-12-09 00:44:36.731167: Pseudo dice [0.7044]
2024-12-09 00:44:36.732133: Epoch time: 88.37 s
2024-12-09 00:44:37.894859: 
2024-12-09 00:44:37.896797: Epoch 108
2024-12-09 00:44:37.897864: Current learning rate: 0.00902
2024-12-09 00:46:06.393198: Validation loss did not improve from -0.53570. Patience: 32/50
2024-12-09 00:46:06.394537: train_loss -0.7231
2024-12-09 00:46:06.395828: val_loss -0.5137
2024-12-09 00:46:06.396867: Pseudo dice [0.7286]
2024-12-09 00:46:06.397883: Epoch time: 88.5 s
2024-12-09 00:46:07.580796: 
2024-12-09 00:46:07.583075: Epoch 109
2024-12-09 00:46:07.584315: Current learning rate: 0.00901
2024-12-09 00:47:35.897863: Validation loss did not improve from -0.53570. Patience: 33/50
2024-12-09 00:47:35.899176: train_loss -0.7213
2024-12-09 00:47:35.900393: val_loss -0.5168
2024-12-09 00:47:35.901366: Pseudo dice [0.7364]
2024-12-09 00:47:35.902206: Epoch time: 88.32 s
2024-12-09 00:47:36.243555: Yayy! New best EMA pseudo Dice: 0.721
2024-12-09 00:47:37.737808: 
2024-12-09 00:47:37.739721: Epoch 110
2024-12-09 00:47:37.740821: Current learning rate: 0.009
2024-12-09 00:49:05.839550: Validation loss did not improve from -0.53570. Patience: 34/50
2024-12-09 00:49:05.841000: train_loss -0.7208
2024-12-09 00:49:05.842488: val_loss -0.4557
2024-12-09 00:49:05.843322: Pseudo dice [0.6958]
2024-12-09 00:49:05.844158: Epoch time: 88.1 s
2024-12-09 00:49:06.976500: 
2024-12-09 00:49:06.978635: Epoch 111
2024-12-09 00:49:06.979406: Current learning rate: 0.009
2024-12-09 00:50:35.511861: Validation loss did not improve from -0.53570. Patience: 35/50
2024-12-09 00:50:35.513273: train_loss -0.7206
2024-12-09 00:50:35.514787: val_loss -0.4992
2024-12-09 00:50:35.515669: Pseudo dice [0.7111]
2024-12-09 00:50:35.516695: Epoch time: 88.54 s
2024-12-09 00:50:36.683532: 
2024-12-09 00:50:36.685587: Epoch 112
2024-12-09 00:50:36.686978: Current learning rate: 0.00899
2024-12-09 00:52:05.229363: Validation loss did not improve from -0.53570. Patience: 36/50
2024-12-09 00:52:05.230796: train_loss -0.7251
2024-12-09 00:52:05.232229: val_loss -0.4865
2024-12-09 00:52:05.233203: Pseudo dice [0.716]
2024-12-09 00:52:05.234224: Epoch time: 88.55 s
2024-12-09 00:52:06.413750: 
2024-12-09 00:52:06.415938: Epoch 113
2024-12-09 00:52:06.416976: Current learning rate: 0.00898
2024-12-09 00:53:34.907514: Validation loss improved from -0.53570 to -0.54065! Patience: 36/50
2024-12-09 00:53:34.908783: train_loss -0.7269
2024-12-09 00:53:34.909688: val_loss -0.5407
2024-12-09 00:53:34.910505: Pseudo dice [0.7424]
2024-12-09 00:53:34.911166: Epoch time: 88.5 s
2024-12-09 00:53:36.080532: 
2024-12-09 00:53:36.082680: Epoch 114
2024-12-09 00:53:36.084150: Current learning rate: 0.00897
2024-12-09 00:55:04.693957: Validation loss did not improve from -0.54065. Patience: 1/50
2024-12-09 00:55:04.695479: train_loss -0.7288
2024-12-09 00:55:04.696851: val_loss -0.5114
2024-12-09 00:55:04.697652: Pseudo dice [0.7315]
2024-12-09 00:55:04.698555: Epoch time: 88.62 s
2024-12-09 00:55:05.057495: Yayy! New best EMA pseudo Dice: 0.7212
2024-12-09 00:55:06.913286: 
2024-12-09 00:55:06.915735: Epoch 115
2024-12-09 00:55:06.917100: Current learning rate: 0.00896
2024-12-09 00:56:35.483861: Validation loss did not improve from -0.54065. Patience: 2/50
2024-12-09 00:56:35.485320: train_loss -0.7209
2024-12-09 00:56:35.486742: val_loss -0.4723
2024-12-09 00:56:35.487874: Pseudo dice [0.7118]
2024-12-09 00:56:35.489153: Epoch time: 88.57 s
2024-12-09 00:56:36.689523: 
2024-12-09 00:56:36.691647: Epoch 116
2024-12-09 00:56:36.693130: Current learning rate: 0.00895
2024-12-09 00:58:04.948618: Validation loss did not improve from -0.54065. Patience: 3/50
2024-12-09 00:58:04.949971: train_loss -0.7367
2024-12-09 00:58:04.951105: val_loss -0.5353
2024-12-09 00:58:04.952045: Pseudo dice [0.7385]
2024-12-09 00:58:04.952902: Epoch time: 88.26 s
2024-12-09 00:58:04.953834: Yayy! New best EMA pseudo Dice: 0.7221
2024-12-09 00:58:06.501738: 
2024-12-09 00:58:06.504230: Epoch 117
2024-12-09 00:58:06.505363: Current learning rate: 0.00894
2024-12-09 00:59:34.962532: Validation loss did not improve from -0.54065. Patience: 4/50
2024-12-09 00:59:34.963986: train_loss -0.7262
2024-12-09 00:59:34.965694: val_loss -0.4604
2024-12-09 00:59:34.966838: Pseudo dice [0.6966]
2024-12-09 00:59:34.967759: Epoch time: 88.46 s
2024-12-09 00:59:36.201971: 
2024-12-09 00:59:36.203950: Epoch 118
2024-12-09 00:59:36.205169: Current learning rate: 0.00893
2024-12-09 01:01:04.341320: Validation loss did not improve from -0.54065. Patience: 5/50
2024-12-09 01:01:04.342803: train_loss -0.7289
2024-12-09 01:01:04.344276: val_loss -0.496
2024-12-09 01:01:04.345204: Pseudo dice [0.7138]
2024-12-09 01:01:04.346248: Epoch time: 88.14 s
2024-12-09 01:01:05.585023: 
2024-12-09 01:01:05.586846: Epoch 119
2024-12-09 01:01:05.587946: Current learning rate: 0.00892
2024-12-09 01:02:33.691454: Validation loss did not improve from -0.54065. Patience: 6/50
2024-12-09 01:02:33.692623: train_loss -0.7337
2024-12-09 01:02:33.694017: val_loss -0.5144
2024-12-09 01:02:33.695003: Pseudo dice [0.7329]
2024-12-09 01:02:33.696011: Epoch time: 88.11 s
2024-12-09 01:02:35.194440: 
2024-12-09 01:02:35.196367: Epoch 120
2024-12-09 01:02:35.197720: Current learning rate: 0.00891
2024-12-09 01:04:03.229775: Validation loss did not improve from -0.54065. Patience: 7/50
2024-12-09 01:04:03.230965: train_loss -0.7377
2024-12-09 01:04:03.232437: val_loss -0.5362
2024-12-09 01:04:03.233549: Pseudo dice [0.7429]
2024-12-09 01:04:03.234447: Epoch time: 88.04 s
2024-12-09 01:04:03.235297: Yayy! New best EMA pseudo Dice: 0.7226
2024-12-09 01:04:04.774088: 
2024-12-09 01:04:04.776212: Epoch 121
2024-12-09 01:04:04.777322: Current learning rate: 0.0089
2024-12-09 01:05:32.839025: Validation loss did not improve from -0.54065. Patience: 8/50
2024-12-09 01:05:32.840300: train_loss -0.7243
2024-12-09 01:05:32.841781: val_loss -0.4841
2024-12-09 01:05:32.842721: Pseudo dice [0.7049]
2024-12-09 01:05:32.843558: Epoch time: 88.07 s
2024-12-09 01:05:34.035449: 
2024-12-09 01:05:34.037626: Epoch 122
2024-12-09 01:05:34.038784: Current learning rate: 0.00889
2024-12-09 01:07:02.078187: Validation loss did not improve from -0.54065. Patience: 9/50
2024-12-09 01:07:02.079503: train_loss -0.733
2024-12-09 01:07:02.080826: val_loss -0.4921
2024-12-09 01:07:02.082112: Pseudo dice [0.7152]
2024-12-09 01:07:02.083096: Epoch time: 88.04 s
2024-12-09 01:07:03.290826: 
2024-12-09 01:07:03.293363: Epoch 123
2024-12-09 01:07:03.294720: Current learning rate: 0.00889
2024-12-09 01:08:31.414021: Validation loss did not improve from -0.54065. Patience: 10/50
2024-12-09 01:08:31.415270: train_loss -0.733
2024-12-09 01:08:31.416468: val_loss -0.5006
2024-12-09 01:08:31.417995: Pseudo dice [0.7209]
2024-12-09 01:08:31.419005: Epoch time: 88.13 s
2024-12-09 01:08:32.618620: 
2024-12-09 01:08:32.620827: Epoch 124
2024-12-09 01:08:32.622212: Current learning rate: 0.00888
2024-12-09 01:10:00.645476: Validation loss did not improve from -0.54065. Patience: 11/50
2024-12-09 01:10:00.646905: train_loss -0.727
2024-12-09 01:10:00.648102: val_loss -0.5244
2024-12-09 01:10:00.649034: Pseudo dice [0.7316]
2024-12-09 01:10:00.649928: Epoch time: 88.03 s
2024-12-09 01:10:02.185825: 
2024-12-09 01:10:02.187765: Epoch 125
2024-12-09 01:10:02.189170: Current learning rate: 0.00887
2024-12-09 01:11:30.436570: Validation loss did not improve from -0.54065. Patience: 12/50
2024-12-09 01:11:30.437868: train_loss -0.7385
2024-12-09 01:11:30.438932: val_loss -0.4996
2024-12-09 01:11:30.440186: Pseudo dice [0.7295]
2024-12-09 01:11:30.441247: Epoch time: 88.25 s
2024-12-09 01:11:31.650024: 
2024-12-09 01:11:31.651733: Epoch 126
2024-12-09 01:11:31.652728: Current learning rate: 0.00886
2024-12-09 01:13:00.189388: Validation loss did not improve from -0.54065. Patience: 13/50
2024-12-09 01:13:00.190617: train_loss -0.742
2024-12-09 01:13:00.191604: val_loss -0.4704
2024-12-09 01:13:00.192446: Pseudo dice [0.7023]
2024-12-09 01:13:00.193312: Epoch time: 88.54 s
2024-12-09 01:13:01.407865: 
2024-12-09 01:13:01.410014: Epoch 127
2024-12-09 01:13:01.411164: Current learning rate: 0.00885
2024-12-09 01:14:30.299849: Validation loss did not improve from -0.54065. Patience: 14/50
2024-12-09 01:14:30.305996: train_loss -0.742
2024-12-09 01:14:30.309760: val_loss -0.4691
2024-12-09 01:14:30.311243: Pseudo dice [0.7107]
2024-12-09 01:14:30.313010: Epoch time: 88.9 s
2024-12-09 01:14:31.593926: 
2024-12-09 01:14:31.595937: Epoch 128
2024-12-09 01:14:31.596961: Current learning rate: 0.00884
2024-12-09 01:16:00.070054: Validation loss did not improve from -0.54065. Patience: 15/50
2024-12-09 01:16:00.071037: train_loss -0.7396
2024-12-09 01:16:00.072232: val_loss -0.5161
2024-12-09 01:16:00.073148: Pseudo dice [0.7344]
2024-12-09 01:16:00.073945: Epoch time: 88.48 s
2024-12-09 01:16:01.276748: 
2024-12-09 01:16:01.279019: Epoch 129
2024-12-09 01:16:01.279943: Current learning rate: 0.00883
2024-12-09 01:17:29.876967: Validation loss did not improve from -0.54065. Patience: 16/50
2024-12-09 01:17:29.878702: train_loss -0.7382
2024-12-09 01:17:29.880005: val_loss -0.5009
2024-12-09 01:17:29.880821: Pseudo dice [0.7318]
2024-12-09 01:17:29.881664: Epoch time: 88.6 s
2024-12-09 01:17:31.385406: 
2024-12-09 01:17:31.387657: Epoch 130
2024-12-09 01:17:31.388937: Current learning rate: 0.00882
2024-12-09 01:18:59.848882: Validation loss did not improve from -0.54065. Patience: 17/50
2024-12-09 01:18:59.850006: train_loss -0.7426
2024-12-09 01:18:59.851019: val_loss -0.5114
2024-12-09 01:18:59.852006: Pseudo dice [0.7327]
2024-12-09 01:18:59.852955: Epoch time: 88.47 s
2024-12-09 01:18:59.853934: Yayy! New best EMA pseudo Dice: 0.723
2024-12-09 01:19:01.374033: 
2024-12-09 01:19:01.376343: Epoch 131
2024-12-09 01:19:01.377728: Current learning rate: 0.00881
2024-12-09 01:20:29.663731: Validation loss did not improve from -0.54065. Patience: 18/50
2024-12-09 01:20:29.664995: train_loss -0.7456
2024-12-09 01:20:29.666270: val_loss -0.507
2024-12-09 01:20:29.667290: Pseudo dice [0.7285]
2024-12-09 01:20:29.668244: Epoch time: 88.29 s
2024-12-09 01:20:29.669297: Yayy! New best EMA pseudo Dice: 0.7236
2024-12-09 01:20:31.202500: 
2024-12-09 01:20:31.205713: Epoch 132
2024-12-09 01:20:31.207610: Current learning rate: 0.0088
2024-12-09 01:21:59.526810: Validation loss did not improve from -0.54065. Patience: 19/50
2024-12-09 01:21:59.528182: train_loss -0.7517
2024-12-09 01:21:59.529477: val_loss -0.4957
2024-12-09 01:21:59.530467: Pseudo dice [0.7164]
2024-12-09 01:21:59.531602: Epoch time: 88.33 s
2024-12-09 01:22:00.711147: 
2024-12-09 01:22:00.713418: Epoch 133
2024-12-09 01:22:00.714483: Current learning rate: 0.00879
2024-12-09 01:23:29.203037: Validation loss did not improve from -0.54065. Patience: 20/50
2024-12-09 01:23:29.204304: train_loss -0.7446
2024-12-09 01:23:29.205431: val_loss -0.4993
2024-12-09 01:23:29.206569: Pseudo dice [0.723]
2024-12-09 01:23:29.207673: Epoch time: 88.49 s
2024-12-09 01:23:30.404763: 
2024-12-09 01:23:30.406953: Epoch 134
2024-12-09 01:23:30.408399: Current learning rate: 0.00879
2024-12-09 01:24:59.044773: Validation loss did not improve from -0.54065. Patience: 21/50
2024-12-09 01:24:59.045950: train_loss -0.7415
2024-12-09 01:24:59.047360: val_loss -0.4869
2024-12-09 01:24:59.048327: Pseudo dice [0.7168]
2024-12-09 01:24:59.049373: Epoch time: 88.64 s
2024-12-09 01:25:00.620605: 
2024-12-09 01:25:00.622941: Epoch 135
2024-12-09 01:25:00.624150: Current learning rate: 0.00878
2024-12-09 01:26:29.369422: Validation loss did not improve from -0.54065. Patience: 22/50
2024-12-09 01:26:29.370388: train_loss -0.7435
2024-12-09 01:26:29.371604: val_loss -0.4861
2024-12-09 01:26:29.372428: Pseudo dice [0.7151]
2024-12-09 01:26:29.373217: Epoch time: 88.75 s
2024-12-09 01:26:31.495918: 
2024-12-09 01:26:31.497764: Epoch 136
2024-12-09 01:26:31.499025: Current learning rate: 0.00877
2024-12-09 01:28:00.116525: Validation loss did not improve from -0.54065. Patience: 23/50
2024-12-09 01:28:00.117737: train_loss -0.7396
2024-12-09 01:28:00.118715: val_loss -0.5108
2024-12-09 01:28:00.119499: Pseudo dice [0.7273]
2024-12-09 01:28:00.120359: Epoch time: 88.62 s
2024-12-09 01:28:01.350578: 
2024-12-09 01:28:01.352907: Epoch 137
2024-12-09 01:28:01.354146: Current learning rate: 0.00876
2024-12-09 01:29:29.702735: Validation loss did not improve from -0.54065. Patience: 24/50
2024-12-09 01:29:29.703894: train_loss -0.7387
2024-12-09 01:29:29.705156: val_loss -0.4906
2024-12-09 01:29:29.706048: Pseudo dice [0.7159]
2024-12-09 01:29:29.707066: Epoch time: 88.35 s
2024-12-09 01:29:30.941701: 
2024-12-09 01:29:30.943916: Epoch 138
2024-12-09 01:29:30.944878: Current learning rate: 0.00875
2024-12-09 01:30:59.415913: Validation loss did not improve from -0.54065. Patience: 25/50
2024-12-09 01:30:59.416916: train_loss -0.7423
2024-12-09 01:30:59.417846: val_loss -0.4807
2024-12-09 01:30:59.418545: Pseudo dice [0.718]
2024-12-09 01:30:59.419340: Epoch time: 88.48 s
2024-12-09 01:31:00.642597: 
2024-12-09 01:31:00.645504: Epoch 139
2024-12-09 01:31:00.646582: Current learning rate: 0.00874
2024-12-09 01:32:29.167710: Validation loss did not improve from -0.54065. Patience: 26/50
2024-12-09 01:32:29.169018: train_loss -0.7347
2024-12-09 01:32:29.170489: val_loss -0.4988
2024-12-09 01:32:29.171375: Pseudo dice [0.7224]
2024-12-09 01:32:29.172364: Epoch time: 88.53 s
2024-12-09 01:32:30.748297: 
2024-12-09 01:32:30.750181: Epoch 140
2024-12-09 01:32:30.751376: Current learning rate: 0.00873
2024-12-09 01:33:59.107642: Validation loss did not improve from -0.54065. Patience: 27/50
2024-12-09 01:33:59.108949: train_loss -0.7397
2024-12-09 01:33:59.110095: val_loss -0.4736
2024-12-09 01:33:59.111392: Pseudo dice [0.7218]
2024-12-09 01:33:59.112682: Epoch time: 88.36 s
2024-12-09 01:34:00.386248: 
2024-12-09 01:34:00.388985: Epoch 141
2024-12-09 01:34:00.390095: Current learning rate: 0.00872
2024-12-09 01:35:28.951083: Validation loss did not improve from -0.54065. Patience: 28/50
2024-12-09 01:35:28.952757: train_loss -0.7424
2024-12-09 01:35:28.954285: val_loss -0.5294
2024-12-09 01:35:28.955387: Pseudo dice [0.7311]
2024-12-09 01:35:28.956528: Epoch time: 88.57 s
2024-12-09 01:35:30.200231: 
2024-12-09 01:35:30.202243: Epoch 142
2024-12-09 01:35:30.203648: Current learning rate: 0.00871
2024-12-09 01:36:58.414232: Validation loss did not improve from -0.54065. Patience: 29/50
2024-12-09 01:36:58.415238: train_loss -0.7376
2024-12-09 01:36:58.416354: val_loss -0.4939
2024-12-09 01:36:58.417253: Pseudo dice [0.716]
2024-12-09 01:36:58.418236: Epoch time: 88.22 s
2024-12-09 01:36:59.628209: 
2024-12-09 01:36:59.630462: Epoch 143
2024-12-09 01:36:59.631810: Current learning rate: 0.0087
2024-12-09 01:38:27.802818: Validation loss did not improve from -0.54065. Patience: 30/50
2024-12-09 01:38:27.804293: train_loss -0.7283
2024-12-09 01:38:27.805526: val_loss -0.4521
2024-12-09 01:38:27.806559: Pseudo dice [0.6987]
2024-12-09 01:38:27.807447: Epoch time: 88.18 s
2024-12-09 01:38:29.045887: 
2024-12-09 01:38:29.048530: Epoch 144
2024-12-09 01:38:29.049564: Current learning rate: 0.00869
2024-12-09 01:39:57.366247: Validation loss did not improve from -0.54065. Patience: 31/50
2024-12-09 01:39:57.367576: train_loss -0.733
2024-12-09 01:39:57.368607: val_loss -0.5234
2024-12-09 01:39:57.369528: Pseudo dice [0.7363]
2024-12-09 01:39:57.370281: Epoch time: 88.32 s
2024-12-09 01:39:58.943537: 
2024-12-09 01:39:58.945641: Epoch 145
2024-12-09 01:39:58.947067: Current learning rate: 0.00868
2024-12-09 01:41:27.273501: Validation loss did not improve from -0.54065. Patience: 32/50
2024-12-09 01:41:27.274746: train_loss -0.7335
2024-12-09 01:41:27.276099: val_loss -0.5075
2024-12-09 01:41:27.277083: Pseudo dice [0.7304]
2024-12-09 01:41:27.278177: Epoch time: 88.33 s
2024-12-09 01:41:28.502750: 
2024-12-09 01:41:28.504707: Epoch 146
2024-12-09 01:41:28.505658: Current learning rate: 0.00868
2024-12-09 01:42:56.933650: Validation loss did not improve from -0.54065. Patience: 33/50
2024-12-09 01:42:56.934852: train_loss -0.7424
2024-12-09 01:42:56.936275: val_loss -0.4638
2024-12-09 01:42:56.937169: Pseudo dice [0.7122]
2024-12-09 01:42:56.937891: Epoch time: 88.43 s
2024-12-09 01:42:58.160591: 
2024-12-09 01:42:58.162562: Epoch 147
2024-12-09 01:42:58.163934: Current learning rate: 0.00867
2024-12-09 01:44:26.410658: Validation loss did not improve from -0.54065. Patience: 34/50
2024-12-09 01:44:26.412156: train_loss -0.7433
2024-12-09 01:44:26.413547: val_loss -0.4983
2024-12-09 01:44:26.414645: Pseudo dice [0.7204]
2024-12-09 01:44:26.415927: Epoch time: 88.25 s
2024-12-09 01:44:27.631354: 
2024-12-09 01:44:27.633666: Epoch 148
2024-12-09 01:44:27.635168: Current learning rate: 0.00866
2024-12-09 01:45:55.772405: Validation loss did not improve from -0.54065. Patience: 35/50
2024-12-09 01:45:55.773765: train_loss -0.7431
2024-12-09 01:45:55.774937: val_loss -0.5169
2024-12-09 01:45:55.775834: Pseudo dice [0.7272]
2024-12-09 01:45:55.776814: Epoch time: 88.14 s
2024-12-09 01:45:57.012518: 
2024-12-09 01:45:57.014927: Epoch 149
2024-12-09 01:45:57.015845: Current learning rate: 0.00865
2024-12-09 01:47:25.097890: Validation loss did not improve from -0.54065. Patience: 36/50
2024-12-09 01:47:25.099266: train_loss -0.7493
2024-12-09 01:47:25.100453: val_loss -0.471
2024-12-09 01:47:25.101156: Pseudo dice [0.7142]
2024-12-09 01:47:25.102545: Epoch time: 88.09 s
2024-12-09 01:47:26.639384: 
2024-12-09 01:47:26.641441: Epoch 150
2024-12-09 01:47:26.642586: Current learning rate: 0.00864
2024-12-09 01:48:55.209933: Validation loss did not improve from -0.54065. Patience: 37/50
2024-12-09 01:48:55.211428: train_loss -0.7474
2024-12-09 01:48:55.212392: val_loss -0.494
2024-12-09 01:48:55.213602: Pseudo dice [0.7229]
2024-12-09 01:48:55.214471: Epoch time: 88.57 s
2024-12-09 01:48:56.504547: 
2024-12-09 01:48:56.506818: Epoch 151
2024-12-09 01:48:56.507812: Current learning rate: 0.00863
2024-12-09 01:50:25.043131: Validation loss did not improve from -0.54065. Patience: 38/50
2024-12-09 01:50:25.046003: train_loss -0.7529
2024-12-09 01:50:25.047114: val_loss -0.4709
2024-12-09 01:50:25.047811: Pseudo dice [0.7206]
2024-12-09 01:50:25.048748: Epoch time: 88.54 s
2024-12-09 01:50:26.352666: 
2024-12-09 01:50:26.354981: Epoch 152
2024-12-09 01:50:26.356110: Current learning rate: 0.00862
2024-12-09 01:51:54.682646: Validation loss did not improve from -0.54065. Patience: 39/50
2024-12-09 01:51:54.683850: train_loss -0.7464
2024-12-09 01:51:54.684576: val_loss -0.4853
2024-12-09 01:51:54.685215: Pseudo dice [0.7276]
2024-12-09 01:51:54.686272: Epoch time: 88.33 s
2024-12-09 01:51:55.929733: 
2024-12-09 01:51:55.932236: Epoch 153
2024-12-09 01:51:55.933879: Current learning rate: 0.00861
2024-12-09 01:53:24.384600: Validation loss did not improve from -0.54065. Patience: 40/50
2024-12-09 01:53:24.386726: train_loss -0.7507
2024-12-09 01:53:24.388053: val_loss -0.5082
2024-12-09 01:53:24.388774: Pseudo dice [0.7333]
2024-12-09 01:53:24.389532: Epoch time: 88.46 s
2024-12-09 01:53:25.680970: 
2024-12-09 01:53:25.683037: Epoch 154
2024-12-09 01:53:25.684043: Current learning rate: 0.0086
2024-12-09 01:54:54.092060: Validation loss did not improve from -0.54065. Patience: 41/50
2024-12-09 01:54:54.093285: train_loss -0.7543
2024-12-09 01:54:54.094546: val_loss -0.5072
2024-12-09 01:54:54.095654: Pseudo dice [0.7223]
2024-12-09 01:54:54.096939: Epoch time: 88.41 s
2024-12-09 01:54:55.686022: 
2024-12-09 01:54:55.687570: Epoch 155
2024-12-09 01:54:55.688495: Current learning rate: 0.00859
2024-12-09 01:56:24.068004: Validation loss did not improve from -0.54065. Patience: 42/50
2024-12-09 01:56:24.069414: train_loss -0.7503
2024-12-09 01:56:24.070750: val_loss -0.4542
2024-12-09 01:56:24.072155: Pseudo dice [0.6995]
2024-12-09 01:56:24.073263: Epoch time: 88.38 s
2024-12-09 01:56:25.303294: 
2024-12-09 01:56:25.304860: Epoch 156
2024-12-09 01:56:25.306164: Current learning rate: 0.00858
2024-12-09 01:57:53.681717: Validation loss did not improve from -0.54065. Patience: 43/50
2024-12-09 01:57:53.682696: train_loss -0.7516
2024-12-09 01:57:53.683729: val_loss -0.4822
2024-12-09 01:57:53.684673: Pseudo dice [0.7237]
2024-12-09 01:57:53.685442: Epoch time: 88.38 s
2024-12-09 01:57:55.230175: 
2024-12-09 01:57:55.232542: Epoch 157
2024-12-09 01:57:55.233682: Current learning rate: 0.00858
2024-12-09 01:59:23.703660: Validation loss did not improve from -0.54065. Patience: 44/50
2024-12-09 01:59:23.705162: train_loss -0.7347
2024-12-09 01:59:23.706347: val_loss -0.4837
2024-12-09 01:59:23.707153: Pseudo dice [0.7258]
2024-12-09 01:59:23.708003: Epoch time: 88.48 s
2024-12-09 01:59:24.936155: 
2024-12-09 01:59:24.937841: Epoch 158
2024-12-09 01:59:24.938946: Current learning rate: 0.00857
2024-12-09 02:00:54.488104: Validation loss did not improve from -0.54065. Patience: 45/50
2024-12-09 02:00:54.490237: train_loss -0.7347
2024-12-09 02:00:54.491416: val_loss -0.4894
2024-12-09 02:00:54.492450: Pseudo dice [0.7213]
2024-12-09 02:00:54.493407: Epoch time: 89.56 s
2024-12-09 02:00:55.762731: 
2024-12-09 02:00:55.765000: Epoch 159
2024-12-09 02:00:55.765781: Current learning rate: 0.00856
2024-12-09 02:02:24.453617: Validation loss did not improve from -0.54065. Patience: 46/50
2024-12-09 02:02:24.454770: train_loss -0.7433
2024-12-09 02:02:24.455677: val_loss -0.5068
2024-12-09 02:02:24.456304: Pseudo dice [0.7272]
2024-12-09 02:02:24.456928: Epoch time: 88.69 s
2024-12-09 02:02:26.034407: 
2024-12-09 02:02:26.036836: Epoch 160
2024-12-09 02:02:26.038059: Current learning rate: 0.00855
2024-12-09 02:03:54.729754: Validation loss did not improve from -0.54065. Patience: 47/50
2024-12-09 02:03:54.730738: train_loss -0.7479
2024-12-09 02:03:54.731733: val_loss -0.4988
2024-12-09 02:03:54.732696: Pseudo dice [0.7279]
2024-12-09 02:03:54.733780: Epoch time: 88.7 s
2024-12-09 02:03:55.997900: 
2024-12-09 02:03:56.000154: Epoch 161
2024-12-09 02:03:56.001265: Current learning rate: 0.00854
2024-12-09 02:05:24.702910: Validation loss did not improve from -0.54065. Patience: 48/50
2024-12-09 02:05:24.704089: train_loss -0.7516
2024-12-09 02:05:24.705452: val_loss -0.4959
2024-12-09 02:05:24.706180: Pseudo dice [0.724]
2024-12-09 02:05:24.706936: Epoch time: 88.71 s
2024-12-09 02:05:25.962129: 
2024-12-09 02:05:25.963959: Epoch 162
2024-12-09 02:05:25.965094: Current learning rate: 0.00853
2024-12-09 02:06:54.588958: Validation loss did not improve from -0.54065. Patience: 49/50
2024-12-09 02:06:54.589968: train_loss -0.7484
2024-12-09 02:06:54.591465: val_loss -0.4952
2024-12-09 02:06:54.592353: Pseudo dice [0.7297]
2024-12-09 02:06:54.593692: Epoch time: 88.63 s
2024-12-09 02:06:55.831935: 
2024-12-09 02:06:55.833802: Epoch 163
2024-12-09 02:06:55.835108: Current learning rate: 0.00852
2024-12-09 02:08:24.346286: Validation loss did not improve from -0.54065. Patience: 50/50
2024-12-09 02:08:24.347672: train_loss -0.7512
2024-12-09 02:08:24.349160: val_loss -0.5179
2024-12-09 02:08:24.350114: Pseudo dice [0.7344]
2024-12-09 02:08:24.350918: Epoch time: 88.52 s
2024-12-09 02:08:24.351840: Yayy! New best EMA pseudo Dice: 0.7244
2024-12-09 02:08:25.901224: Patience reached. Stopping training.
2024-12-09 02:08:26.245616: Training done.
2024-12-09 02:08:26.389240: Using splits from existing split file: /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset307_Sohee_Calcium_OCT_CrossValidation/splits_final.json
2024-12-09 02:08:26.392003: The split file contains 5 splits.
2024-12-09 02:08:26.392691: Desired fold for training: 4
2024-12-09 02:08:26.393309: This split has 7 training and 1 validation cases.
2024-12-09 02:08:26.394269: predicting 101-045
2024-12-09 02:08:26.404423: 101-045, shape torch.Size([1, 375, 498, 498]), rank 0
2024-12-09 02:10:29.115498: Validation complete
2024-12-09 02:10:29.116415: Mean Validation Dice:  0.7251261269096222
mv: cannot stat '/home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_results/Dataset307_Sohee_Calcium_OCT_CrossValidation/nnUNetTrainer__nnUNetPlans__3d_32x160x128_b10/fold_2': No such file or directory

############################
INFO: You are using the old nnU-Net default plans. We have updated our recommendations. Please consider using those instead! Read more here: https://github.com/MIC-DKFZ/nnUNet/blob/master/documentation/resenc_presets.md
############################

Using device: cuda:0

#######################################################################
Please cite the following paper when using nnU-Net:
Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.
#######################################################################

2024-12-09 02:10:36.775267: Using torch.compile...
/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
################### Loading pretrained weights from file  /home/gridsan/nchutisilp/datasets/ModelGenesisOutputs/ModelGenesisNNUNetPretrainingV2_noNorm_correct_orientation/Converted_nnUNet_Genesis_OCT_Best.pt ###################
Below is the list of overlapping blocks in pretrained model and nnUNet architecture:
encoder.stages.0.0.convs.0.conv.weight shape torch.Size([32, 1, 3, 3, 3])
encoder.stages.0.0.convs.0.conv.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.norm.weight shape torch.Size([32])
encoder.stages.0.0.convs.0.norm.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.0.weight shape torch.Size([32, 1, 3, 3, 3])
encoder.stages.0.0.convs.0.all_modules.0.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.1.weight shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.1.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
encoder.stages.0.0.convs.1.conv.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.norm.weight shape torch.Size([32])
encoder.stages.0.0.convs.1.norm.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
encoder.stages.0.0.convs.1.all_modules.0.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.1.weight shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.1.bias shape torch.Size([32])
encoder.stages.1.0.convs.0.conv.weight shape torch.Size([64, 32, 3, 3, 3])
encoder.stages.1.0.convs.0.conv.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.norm.weight shape torch.Size([64])
encoder.stages.1.0.convs.0.norm.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.0.weight shape torch.Size([64, 32, 3, 3, 3])
encoder.stages.1.0.convs.0.all_modules.0.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.1.weight shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.1.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
encoder.stages.1.0.convs.1.conv.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.norm.weight shape torch.Size([64])
encoder.stages.1.0.convs.1.norm.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
encoder.stages.1.0.convs.1.all_modules.0.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.1.weight shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.1.bias shape torch.Size([64])
encoder.stages.2.0.convs.0.conv.weight shape torch.Size([128, 64, 3, 3, 3])
encoder.stages.2.0.convs.0.conv.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.norm.weight shape torch.Size([128])
encoder.stages.2.0.convs.0.norm.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.0.weight shape torch.Size([128, 64, 3, 3, 3])
encoder.stages.2.0.convs.0.all_modules.0.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.1.weight shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.1.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
encoder.stages.2.0.convs.1.conv.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.norm.weight shape torch.Size([128])
encoder.stages.2.0.convs.1.norm.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
encoder.stages.2.0.convs.1.all_modules.0.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.1.weight shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.1.bias shape torch.Size([128])
encoder.stages.3.0.convs.0.conv.weight shape torch.Size([256, 128, 3, 3, 3])
encoder.stages.3.0.convs.0.conv.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.norm.weight shape torch.Size([256])
encoder.stages.3.0.convs.0.norm.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.0.weight shape torch.Size([256, 128, 3, 3, 3])
encoder.stages.3.0.convs.0.all_modules.0.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.1.weight shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.1.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
encoder.stages.3.0.convs.1.conv.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.norm.weight shape torch.Size([256])
encoder.stages.3.0.convs.1.norm.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
encoder.stages.3.0.convs.1.all_modules.0.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.1.weight shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.1.bias shape torch.Size([256])
encoder.stages.4.0.convs.0.conv.weight shape torch.Size([320, 256, 3, 3, 3])
encoder.stages.4.0.convs.0.conv.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.norm.weight shape torch.Size([320])
encoder.stages.4.0.convs.0.norm.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.0.weight shape torch.Size([320, 256, 3, 3, 3])
encoder.stages.4.0.convs.0.all_modules.0.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.1.weight shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.1.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.4.0.convs.1.conv.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.norm.weight shape torch.Size([320])
encoder.stages.4.0.convs.1.norm.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.4.0.convs.1.all_modules.0.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.1.weight shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.1.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.0.conv.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.norm.weight shape torch.Size([320])
encoder.stages.5.0.convs.0.norm.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.0.all_modules.0.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.1.weight shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.1.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.1.conv.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.norm.weight shape torch.Size([320])
encoder.stages.5.0.convs.1.norm.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.1.all_modules.0.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.1.weight shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.0.0.convs.0.conv.weight shape torch.Size([32, 1, 3, 3, 3])
decoder.encoder.stages.0.0.convs.0.conv.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.norm.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.norm.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.0.weight shape torch.Size([32, 1, 3, 3, 3])
decoder.encoder.stages.0.0.convs.0.all_modules.0.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.1.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.1.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.encoder.stages.0.0.convs.1.conv.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.norm.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.norm.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.encoder.stages.0.0.convs.1.all_modules.0.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.1.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.1.bias shape torch.Size([32])
decoder.encoder.stages.1.0.convs.0.conv.weight shape torch.Size([64, 32, 3, 3, 3])
decoder.encoder.stages.1.0.convs.0.conv.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.norm.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.norm.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.0.weight shape torch.Size([64, 32, 3, 3, 3])
decoder.encoder.stages.1.0.convs.0.all_modules.0.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.1.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.1.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.encoder.stages.1.0.convs.1.conv.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.norm.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.norm.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.encoder.stages.1.0.convs.1.all_modules.0.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.1.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.1.bias shape torch.Size([64])
decoder.encoder.stages.2.0.convs.0.conv.weight shape torch.Size([128, 64, 3, 3, 3])
decoder.encoder.stages.2.0.convs.0.conv.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.norm.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.norm.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.0.weight shape torch.Size([128, 64, 3, 3, 3])
decoder.encoder.stages.2.0.convs.0.all_modules.0.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.1.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.1.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.encoder.stages.2.0.convs.1.conv.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.norm.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.norm.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.encoder.stages.2.0.convs.1.all_modules.0.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.1.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.1.bias shape torch.Size([128])
decoder.encoder.stages.3.0.convs.0.conv.weight shape torch.Size([256, 128, 3, 3, 3])
decoder.encoder.stages.3.0.convs.0.conv.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.norm.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.norm.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.0.weight shape torch.Size([256, 128, 3, 3, 3])
decoder.encoder.stages.3.0.convs.0.all_modules.0.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.1.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.1.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.encoder.stages.3.0.convs.1.conv.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.norm.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.norm.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.encoder.stages.3.0.convs.1.all_modules.0.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.1.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.1.bias shape torch.Size([256])
decoder.encoder.stages.4.0.convs.0.conv.weight shape torch.Size([320, 256, 3, 3, 3])
decoder.encoder.stages.4.0.convs.0.conv.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.norm.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.norm.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.0.weight shape torch.Size([320, 256, 3, 3, 3])
decoder.encoder.stages.4.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.4.0.convs.1.conv.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.norm.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.norm.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.4.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.0.conv.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.norm.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.norm.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.1.conv.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.norm.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.norm.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.stages.0.convs.0.conv.weight shape torch.Size([320, 640, 3, 3, 3])
decoder.stages.0.convs.0.conv.bias shape torch.Size([320])
decoder.stages.0.convs.0.norm.weight shape torch.Size([320])
decoder.stages.0.convs.0.norm.bias shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.0.weight shape torch.Size([320, 640, 3, 3, 3])
decoder.stages.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.stages.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.stages.0.convs.1.conv.bias shape torch.Size([320])
decoder.stages.0.convs.1.norm.weight shape torch.Size([320])
decoder.stages.0.convs.1.norm.bias shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.stages.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.stages.1.convs.0.conv.weight shape torch.Size([256, 512, 3, 3, 3])
decoder.stages.1.convs.0.conv.bias shape torch.Size([256])
decoder.stages.1.convs.0.norm.weight shape torch.Size([256])
decoder.stages.1.convs.0.norm.bias shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.0.weight shape torch.Size([256, 512, 3, 3, 3])
decoder.stages.1.convs.0.all_modules.0.bias shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.1.weight shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.1.bias shape torch.Size([256])
decoder.stages.1.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.stages.1.convs.1.conv.bias shape torch.Size([256])
decoder.stages.1.convs.1.norm.weight shape torch.Size([256])
decoder.stages.1.convs.1.norm.bias shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.stages.1.convs.1.all_modules.0.bias shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.1.weight shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.1.bias shape torch.Size([256])
decoder.stages.2.convs.0.conv.weight shape torch.Size([128, 256, 3, 3, 3])
decoder.stages.2.convs.0.conv.bias shape torch.Size([128])
decoder.stages.2.convs.0.norm.weight shape torch.Size([128])
decoder.stages.2.convs.0.norm.bias shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.0.weight shape torch.Size([128, 256, 3, 3, 3])
decoder.stages.2.convs.0.all_modules.0.bias shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.1.weight shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.1.bias shape torch.Size([128])
decoder.stages.2.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.stages.2.convs.1.conv.bias shape torch.Size([128])
decoder.stages.2.convs.1.norm.weight shape torch.Size([128])
decoder.stages.2.convs.1.norm.bias shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.stages.2.convs.1.all_modules.0.bias shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.1.weight shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.1.bias shape torch.Size([128])
decoder.stages.3.convs.0.conv.weight shape torch.Size([64, 128, 3, 3, 3])
decoder.stages.3.convs.0.conv.bias shape torch.Size([64])
decoder.stages.3.convs.0.norm.weight shape torch.Size([64])
decoder.stages.3.convs.0.norm.bias shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.0.weight shape torch.Size([64, 128, 3, 3, 3])
decoder.stages.3.convs.0.all_modules.0.bias shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.1.weight shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.1.bias shape torch.Size([64])
decoder.stages.3.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.stages.3.convs.1.conv.bias shape torch.Size([64])
decoder.stages.3.convs.1.norm.weight shape torch.Size([64])
decoder.stages.3.convs.1.norm.bias shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.stages.3.convs.1.all_modules.0.bias shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.1.weight shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.1.bias shape torch.Size([64])
decoder.stages.4.convs.0.conv.weight shape torch.Size([32, 64, 3, 3, 3])
decoder.stages.4.convs.0.conv.bias shape torch.Size([32])
decoder.stages.4.convs.0.norm.weight shape torch.Size([32])
decoder.stages.4.convs.0.norm.bias shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.0.weight shape torch.Size([32, 64, 3, 3, 3])
decoder.stages.4.convs.0.all_modules.0.bias shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.1.weight shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.1.bias shape torch.Size([32])
decoder.stages.4.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.stages.4.convs.1.conv.bias shape torch.Size([32])
decoder.stages.4.convs.1.norm.weight shape torch.Size([32])
decoder.stages.4.convs.1.norm.bias shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.stages.4.convs.1.all_modules.0.bias shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.1.weight shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.1.bias shape torch.Size([32])
decoder.transpconvs.0.weight shape torch.Size([320, 320, 1, 2, 2])
decoder.transpconvs.0.bias shape torch.Size([320])
decoder.transpconvs.1.weight shape torch.Size([320, 256, 2, 2, 2])
decoder.transpconvs.1.bias shape torch.Size([256])
decoder.transpconvs.2.weight shape torch.Size([256, 128, 2, 2, 2])
decoder.transpconvs.2.bias shape torch.Size([128])
decoder.transpconvs.3.weight shape torch.Size([128, 64, 2, 2, 2])
decoder.transpconvs.3.bias shape torch.Size([64])
decoder.transpconvs.4.weight shape torch.Size([64, 32, 2, 2, 2])
decoder.transpconvs.4.bias shape torch.Size([32])
################### Done ###################
2024-12-09 02:10:41.160872: do_dummy_2d_data_aug: True
2024-12-09 02:10:41.164291: Using splits from existing split file: /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset307_Sohee_Calcium_OCT_CrossValidation/splits_final.json
2024-12-09 02:10:41.167046: The split file contains 5 splits.
2024-12-09 02:10:41.168349: Desired fold for training: 4
2024-12-09 02:10:41.169506: This split has 7 training and 1 validation cases.
using pin_memory on device 0
using pin_memory on device 0

This is the configuration used by this training:
Configuration name: 3d_32x160x128_b10
 {'data_identifier': 'nnUNetPlans_3d_fullres', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 10, 'patch_size': [32, 160, 128], 'median_image_size_in_voxels': [375.0, 498.0, 498.0], 'spacing': [1.0, 1.0, 1.0], 'normalization_schemes': ['ZScoreNormalization'], 'use_mask_for_norm': [False], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'architecture': {'network_class_name': 'dynamic_network_architectures.architectures.unet.PlainConvUNet', 'arch_kwargs': {'n_stages': 6, 'features_per_stage': [32, 64, 128, 256, 320, 320], 'conv_op': 'torch.nn.modules.conv.Conv3d', 'kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]], 'strides': [[1, 1, 1], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [1, 2, 2]], 'n_conv_per_stage': [2, 2, 2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2, 2, 2], 'conv_bias': True, 'norm_op': 'torch.nn.modules.instancenorm.InstanceNorm3d', 'norm_op_kwargs': {'eps': 1e-05, 'affine': True}, 'dropout_op': None, 'dropout_op_kwargs': None, 'nonlin': 'torch.nn.LeakyReLU', 'nonlin_kwargs': {'inplace': True}, 'deep_supervision': True}, '_kw_requires_import': ['conv_op', 'norm_op', 'dropout_op', 'nonlin']}, 'batch_dice': True, 'inherits_from': '3d_fullres'} 

These are the global plan.json settings:
 {'dataset_name': 'Dataset307_Sohee_Calcium_OCT_CrossValidation', 'plans_name': 'nnUNetPlans', 'original_median_spacing_after_transp': [1.0, 1.0, 1.0], 'original_median_shape_after_transp': [375, 498, 498], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 0.4977976381778717, 'mean': 0.13507525622844696, 'median': 0.09599608182907104, 'min': 0.0, 'percentile_00_5': 0.014754901640117168, 'percentile_99_5': 0.4977976381778717, 'std': 0.12025152146816254}}} 

2024-12-09 02:10:43.342010: unpacking dataset...
2024-12-09 02:10:47.664876: unpacking done...
2024-12-09 02:10:47.706154: Unable to plot network architecture: nnUNet_compile is enabled!
2024-12-09 02:10:47.759149: 
2024-12-09 02:10:47.761145: Epoch 0
2024-12-09 02:10:47.762460: Current learning rate: 0.01
2024-12-09 02:12:57.107703: Validation loss improved from 1000.00000 to -0.26055! Patience: 0/50
2024-12-09 02:12:57.109172: train_loss -0.13
2024-12-09 02:12:57.110545: val_loss -0.2605
2024-12-09 02:12:57.111739: Pseudo dice [0.5767]
2024-12-09 02:12:57.112867: Epoch time: 129.35 s
2024-12-09 02:12:57.113837: Yayy! New best EMA pseudo Dice: 0.5767
2024-12-09 02:12:58.549014: 
2024-12-09 02:12:58.550672: Epoch 1
2024-12-09 02:12:58.551748: Current learning rate: 0.00999
2024-12-09 02:14:24.524057: Validation loss improved from -0.26055 to -0.31427! Patience: 0/50
2024-12-09 02:14:24.525449: train_loss -0.2961
2024-12-09 02:14:24.526546: val_loss -0.3143
2024-12-09 02:14:24.527473: Pseudo dice [0.6236]
2024-12-09 02:14:24.528343: Epoch time: 85.98 s
2024-12-09 02:14:24.529197: Yayy! New best EMA pseudo Dice: 0.5814
2024-12-09 02:14:26.003244: 
2024-12-09 02:14:26.005681: Epoch 2
2024-12-09 02:14:26.006739: Current learning rate: 0.00998
2024-12-09 02:15:51.837610: Validation loss did not improve from -0.31427. Patience: 1/50
2024-12-09 02:15:51.839348: train_loss -0.3464
2024-12-09 02:15:51.840542: val_loss -0.3032
2024-12-09 02:15:51.841453: Pseudo dice [0.6061]
2024-12-09 02:15:51.842650: Epoch time: 85.84 s
2024-12-09 02:15:51.843925: Yayy! New best EMA pseudo Dice: 0.5839
2024-12-09 02:15:53.388421: 
2024-12-09 02:15:53.390250: Epoch 3
2024-12-09 02:15:53.391407: Current learning rate: 0.00997
2024-12-09 02:17:19.253334: Validation loss improved from -0.31427 to -0.34059! Patience: 1/50
2024-12-09 02:17:19.254704: train_loss -0.3913
2024-12-09 02:17:19.255988: val_loss -0.3406
2024-12-09 02:17:19.256888: Pseudo dice [0.628]
2024-12-09 02:17:19.257815: Epoch time: 85.87 s
2024-12-09 02:17:19.258625: Yayy! New best EMA pseudo Dice: 0.5883
2024-12-09 02:17:20.787045: 
2024-12-09 02:17:20.789467: Epoch 4
2024-12-09 02:17:20.790704: Current learning rate: 0.00996
2024-12-09 02:18:46.720046: Validation loss improved from -0.34059 to -0.38484! Patience: 0/50
2024-12-09 02:18:46.721531: train_loss -0.415
2024-12-09 02:18:46.723125: val_loss -0.3848
2024-12-09 02:18:46.724334: Pseudo dice [0.6401]
2024-12-09 02:18:46.725806: Epoch time: 85.94 s
2024-12-09 02:18:47.018517: Yayy! New best EMA pseudo Dice: 0.5935
2024-12-09 02:18:48.584639: 
2024-12-09 02:18:48.586418: Epoch 5
2024-12-09 02:18:48.587427: Current learning rate: 0.00995
2024-12-09 02:20:14.548398: Validation loss improved from -0.38484 to -0.40671! Patience: 0/50
2024-12-09 02:20:14.549934: train_loss -0.4098
2024-12-09 02:20:14.550980: val_loss -0.4067
2024-12-09 02:20:14.551718: Pseudo dice [0.6539]
2024-12-09 02:20:14.552592: Epoch time: 85.97 s
2024-12-09 02:20:14.553637: Yayy! New best EMA pseudo Dice: 0.5995
2024-12-09 02:20:16.057849: 
2024-12-09 02:20:16.060009: Epoch 6
2024-12-09 02:20:16.061124: Current learning rate: 0.00995
2024-12-09 02:21:41.910935: Validation loss did not improve from -0.40671. Patience: 1/50
2024-12-09 02:21:41.912286: train_loss -0.4562
2024-12-09 02:21:41.913348: val_loss -0.398
2024-12-09 02:21:41.914184: Pseudo dice [0.6652]
2024-12-09 02:21:41.914977: Epoch time: 85.86 s
2024-12-09 02:21:41.915735: Yayy! New best EMA pseudo Dice: 0.6061
2024-12-09 02:21:43.426331: 
2024-12-09 02:21:43.428776: Epoch 7
2024-12-09 02:21:43.430375: Current learning rate: 0.00994
2024-12-09 02:23:09.337589: Validation loss did not improve from -0.40671. Patience: 2/50
2024-12-09 02:23:09.338847: train_loss -0.4557
2024-12-09 02:23:09.339761: val_loss -0.3548
2024-12-09 02:23:09.340897: Pseudo dice [0.6502]
2024-12-09 02:23:09.341694: Epoch time: 85.91 s
2024-12-09 02:23:09.342555: Yayy! New best EMA pseudo Dice: 0.6105
2024-12-09 02:23:11.206330: 
2024-12-09 02:23:11.208497: Epoch 8
2024-12-09 02:23:11.209616: Current learning rate: 0.00993
2024-12-09 02:24:37.236656: Validation loss improved from -0.40671 to -0.41976! Patience: 2/50
2024-12-09 02:24:37.237882: train_loss -0.4803
2024-12-09 02:24:37.238698: val_loss -0.4198
2024-12-09 02:24:37.239476: Pseudo dice [0.6634]
2024-12-09 02:24:37.240137: Epoch time: 86.03 s
2024-12-09 02:24:37.240877: Yayy! New best EMA pseudo Dice: 0.6158
2024-12-09 02:24:38.762325: 
2024-12-09 02:24:38.764810: Epoch 9
2024-12-09 02:24:38.765694: Current learning rate: 0.00992
2024-12-09 02:26:04.639387: Validation loss improved from -0.41976 to -0.43971! Patience: 0/50
2024-12-09 02:26:04.640530: train_loss -0.4841
2024-12-09 02:26:04.641443: val_loss -0.4397
2024-12-09 02:26:04.642278: Pseudo dice [0.678]
2024-12-09 02:26:04.643062: Epoch time: 85.88 s
2024-12-09 02:26:04.973867: Yayy! New best EMA pseudo Dice: 0.622
2024-12-09 02:26:06.381834: 
2024-12-09 02:26:06.384433: Epoch 10
2024-12-09 02:26:06.385508: Current learning rate: 0.00991
2024-12-09 02:27:32.252742: Validation loss did not improve from -0.43971. Patience: 1/50
2024-12-09 02:27:32.254129: train_loss -0.497
2024-12-09 02:27:32.255436: val_loss -0.4189
2024-12-09 02:27:32.256567: Pseudo dice [0.6827]
2024-12-09 02:27:32.257366: Epoch time: 85.87 s
2024-12-09 02:27:32.258435: Yayy! New best EMA pseudo Dice: 0.6281
2024-12-09 02:27:33.747406: 
2024-12-09 02:27:33.749466: Epoch 11
2024-12-09 02:27:33.750889: Current learning rate: 0.0099
2024-12-09 02:28:59.732657: Validation loss did not improve from -0.43971. Patience: 2/50
2024-12-09 02:28:59.733845: train_loss -0.5102
2024-12-09 02:28:59.734863: val_loss -0.4142
2024-12-09 02:28:59.735773: Pseudo dice [0.6685]
2024-12-09 02:28:59.736544: Epoch time: 85.99 s
2024-12-09 02:28:59.737309: Yayy! New best EMA pseudo Dice: 0.6321
2024-12-09 02:29:01.220455: 
2024-12-09 02:29:01.222241: Epoch 12
2024-12-09 02:29:01.223046: Current learning rate: 0.00989
2024-12-09 02:30:27.145631: Validation loss did not improve from -0.43971. Patience: 3/50
2024-12-09 02:30:27.147296: train_loss -0.5177
2024-12-09 02:30:27.148910: val_loss -0.436
2024-12-09 02:30:27.149934: Pseudo dice [0.6876]
2024-12-09 02:30:27.151086: Epoch time: 85.93 s
2024-12-09 02:30:27.151924: Yayy! New best EMA pseudo Dice: 0.6377
2024-12-09 02:30:28.644422: 
2024-12-09 02:30:28.646259: Epoch 13
2024-12-09 02:30:28.647372: Current learning rate: 0.00988
2024-12-09 02:31:54.529957: Validation loss did not improve from -0.43971. Patience: 4/50
2024-12-09 02:31:54.531161: train_loss -0.5324
2024-12-09 02:31:54.532372: val_loss -0.4301
2024-12-09 02:31:54.533331: Pseudo dice [0.6827]
2024-12-09 02:31:54.534296: Epoch time: 85.89 s
2024-12-09 02:31:54.535091: Yayy! New best EMA pseudo Dice: 0.6422
2024-12-09 02:31:56.040673: 
2024-12-09 02:31:56.043077: Epoch 14
2024-12-09 02:31:56.044399: Current learning rate: 0.00987
2024-12-09 02:33:21.958264: Validation loss improved from -0.43971 to -0.46266! Patience: 4/50
2024-12-09 02:33:21.959675: train_loss -0.5315
2024-12-09 02:33:21.960840: val_loss -0.4627
2024-12-09 02:33:21.961881: Pseudo dice [0.6964]
2024-12-09 02:33:21.963334: Epoch time: 85.92 s
2024-12-09 02:33:22.304501: Yayy! New best EMA pseudo Dice: 0.6476
2024-12-09 02:33:23.821692: 
2024-12-09 02:33:23.823780: Epoch 15
2024-12-09 02:33:23.825008: Current learning rate: 0.00986
2024-12-09 02:34:49.812510: Validation loss did not improve from -0.46266. Patience: 1/50
2024-12-09 02:34:49.813735: train_loss -0.5454
2024-12-09 02:34:49.814656: val_loss -0.4615
2024-12-09 02:34:49.815681: Pseudo dice [0.7036]
2024-12-09 02:34:49.816700: Epoch time: 85.99 s
2024-12-09 02:34:49.817694: Yayy! New best EMA pseudo Dice: 0.6532
2024-12-09 02:34:51.338099: 
2024-12-09 02:34:51.339931: Epoch 16
2024-12-09 02:34:51.341280: Current learning rate: 0.00986
2024-12-09 02:36:17.170719: Validation loss did not improve from -0.46266. Patience: 2/50
2024-12-09 02:36:17.171747: train_loss -0.554
2024-12-09 02:36:17.172490: val_loss -0.4389
2024-12-09 02:36:17.173220: Pseudo dice [0.6893]
2024-12-09 02:36:17.174080: Epoch time: 85.83 s
2024-12-09 02:36:17.174923: Yayy! New best EMA pseudo Dice: 0.6568
2024-12-09 02:36:18.694809: 
2024-12-09 02:36:18.697137: Epoch 17
2024-12-09 02:36:18.698384: Current learning rate: 0.00985
2024-12-09 02:37:44.579809: Validation loss did not improve from -0.46266. Patience: 3/50
2024-12-09 02:37:44.581142: train_loss -0.5509
2024-12-09 02:37:44.582503: val_loss -0.416
2024-12-09 02:37:44.583494: Pseudo dice [0.6707]
2024-12-09 02:37:44.584702: Epoch time: 85.89 s
2024-12-09 02:37:44.585796: Yayy! New best EMA pseudo Dice: 0.6582
2024-12-09 02:37:46.119354: 
2024-12-09 02:37:46.121452: Epoch 18
2024-12-09 02:37:46.122934: Current learning rate: 0.00984
2024-12-09 02:39:12.092943: Validation loss improved from -0.46266 to -0.48368! Patience: 3/50
2024-12-09 02:39:12.094053: train_loss -0.5438
2024-12-09 02:39:12.095166: val_loss -0.4837
2024-12-09 02:39:12.095952: Pseudo dice [0.7057]
2024-12-09 02:39:12.096779: Epoch time: 85.98 s
2024-12-09 02:39:12.097472: Yayy! New best EMA pseudo Dice: 0.6629
2024-12-09 02:39:13.952794: 
2024-12-09 02:39:13.954981: Epoch 19
2024-12-09 02:39:13.956031: Current learning rate: 0.00983
2024-12-09 02:40:39.796005: Validation loss did not improve from -0.48368. Patience: 1/50
2024-12-09 02:40:39.797447: train_loss -0.5635
2024-12-09 02:40:39.798620: val_loss -0.3938
2024-12-09 02:40:39.799413: Pseudo dice [0.67]
2024-12-09 02:40:39.800100: Epoch time: 85.85 s
2024-12-09 02:40:40.142120: Yayy! New best EMA pseudo Dice: 0.6636
2024-12-09 02:40:41.642934: 
2024-12-09 02:40:41.645178: Epoch 20
2024-12-09 02:40:41.646267: Current learning rate: 0.00982
2024-12-09 02:42:07.533505: Validation loss did not improve from -0.48368. Patience: 2/50
2024-12-09 02:42:07.535173: train_loss -0.5689
2024-12-09 02:42:07.536507: val_loss -0.4531
2024-12-09 02:42:07.537863: Pseudo dice [0.7055]
2024-12-09 02:42:07.538655: Epoch time: 85.89 s
2024-12-09 02:42:07.539527: Yayy! New best EMA pseudo Dice: 0.6678
2024-12-09 02:42:09.069799: 
2024-12-09 02:42:09.071553: Epoch 21
2024-12-09 02:42:09.072454: Current learning rate: 0.00981
2024-12-09 02:43:35.038899: Validation loss did not improve from -0.48368. Patience: 3/50
2024-12-09 02:43:35.040117: train_loss -0.5534
2024-12-09 02:43:35.041365: val_loss -0.48
2024-12-09 02:43:35.042453: Pseudo dice [0.7053]
2024-12-09 02:43:35.043545: Epoch time: 85.97 s
2024-12-09 02:43:35.044602: Yayy! New best EMA pseudo Dice: 0.6716
2024-12-09 02:43:36.519562: 
2024-12-09 02:43:36.521510: Epoch 22
2024-12-09 02:43:36.522795: Current learning rate: 0.0098
2024-12-09 02:45:02.405930: Validation loss improved from -0.48368 to -0.50548! Patience: 3/50
2024-12-09 02:45:02.407240: train_loss -0.5778
2024-12-09 02:45:02.408886: val_loss -0.5055
2024-12-09 02:45:02.409968: Pseudo dice [0.7217]
2024-12-09 02:45:02.410994: Epoch time: 85.89 s
2024-12-09 02:45:02.411985: Yayy! New best EMA pseudo Dice: 0.6766
2024-12-09 02:45:03.877718: 
2024-12-09 02:45:03.880119: Epoch 23
2024-12-09 02:45:03.881606: Current learning rate: 0.00979
2024-12-09 02:46:29.760013: Validation loss did not improve from -0.50548. Patience: 1/50
2024-12-09 02:46:29.761431: train_loss -0.5897
2024-12-09 02:46:29.762787: val_loss -0.4544
2024-12-09 02:46:29.763947: Pseudo dice [0.7026]
2024-12-09 02:46:29.765269: Epoch time: 85.88 s
2024-12-09 02:46:29.766251: Yayy! New best EMA pseudo Dice: 0.6792
2024-12-09 02:46:31.219521: 
2024-12-09 02:46:31.221687: Epoch 24
2024-12-09 02:46:31.222888: Current learning rate: 0.00978
2024-12-09 02:47:57.096217: Validation loss did not improve from -0.50548. Patience: 2/50
2024-12-09 02:47:57.097319: train_loss -0.5909
2024-12-09 02:47:57.098658: val_loss -0.4658
2024-12-09 02:47:57.099388: Pseudo dice [0.7023]
2024-12-09 02:47:57.100188: Epoch time: 85.88 s
2024-12-09 02:47:57.437668: Yayy! New best EMA pseudo Dice: 0.6815
2024-12-09 02:47:58.926451: 
2024-12-09 02:47:58.928631: Epoch 25
2024-12-09 02:47:58.929984: Current learning rate: 0.00977
2024-12-09 02:49:24.872447: Validation loss did not improve from -0.50548. Patience: 3/50
2024-12-09 02:49:24.873732: train_loss -0.5783
2024-12-09 02:49:24.874503: val_loss -0.4633
2024-12-09 02:49:24.875197: Pseudo dice [0.706]
2024-12-09 02:49:24.875986: Epoch time: 85.95 s
2024-12-09 02:49:24.876725: Yayy! New best EMA pseudo Dice: 0.6839
2024-12-09 02:49:26.348522: 
2024-12-09 02:49:26.350750: Epoch 26
2024-12-09 02:49:26.351735: Current learning rate: 0.00977
2024-12-09 02:50:52.168075: Validation loss did not improve from -0.50548. Patience: 4/50
2024-12-09 02:50:52.169329: train_loss -0.5992
2024-12-09 02:50:52.170096: val_loss -0.482
2024-12-09 02:50:52.171047: Pseudo dice [0.7027]
2024-12-09 02:50:52.172215: Epoch time: 85.82 s
2024-12-09 02:50:52.173000: Yayy! New best EMA pseudo Dice: 0.6858
2024-12-09 02:50:53.633942: 
2024-12-09 02:50:53.635959: Epoch 27
2024-12-09 02:50:53.637081: Current learning rate: 0.00976
2024-12-09 02:52:19.548949: Validation loss did not improve from -0.50548. Patience: 5/50
2024-12-09 02:52:19.549995: train_loss -0.6034
2024-12-09 02:52:19.551174: val_loss -0.4268
2024-12-09 02:52:19.552007: Pseudo dice [0.6917]
2024-12-09 02:52:19.552814: Epoch time: 85.92 s
2024-12-09 02:52:19.553707: Yayy! New best EMA pseudo Dice: 0.6864
2024-12-09 02:52:21.056761: 
2024-12-09 02:52:21.058972: Epoch 28
2024-12-09 02:52:21.060113: Current learning rate: 0.00975
2024-12-09 02:53:47.056031: Validation loss did not improve from -0.50548. Patience: 6/50
2024-12-09 02:53:47.057204: train_loss -0.6027
2024-12-09 02:53:47.058082: val_loss -0.4319
2024-12-09 02:53:47.059067: Pseudo dice [0.6901]
2024-12-09 02:53:47.059829: Epoch time: 86.0 s
2024-12-09 02:53:47.060697: Yayy! New best EMA pseudo Dice: 0.6868
2024-12-09 02:53:48.514630: 
2024-12-09 02:53:48.516525: Epoch 29
2024-12-09 02:53:48.517859: Current learning rate: 0.00974
2024-12-09 02:55:14.355746: Validation loss did not improve from -0.50548. Patience: 7/50
2024-12-09 02:55:14.356707: train_loss -0.5969
2024-12-09 02:55:14.357864: val_loss -0.4207
2024-12-09 02:55:14.358948: Pseudo dice [0.682]
2024-12-09 02:55:14.359885: Epoch time: 85.84 s
2024-12-09 02:55:16.146528: 
2024-12-09 02:55:16.148537: Epoch 30
2024-12-09 02:55:16.149577: Current learning rate: 0.00973
2024-12-09 02:56:42.077162: Validation loss did not improve from -0.50548. Patience: 8/50
2024-12-09 02:56:42.078567: train_loss -0.5958
2024-12-09 02:56:42.079963: val_loss -0.4563
2024-12-09 02:56:42.081173: Pseudo dice [0.708]
2024-12-09 02:56:42.082140: Epoch time: 85.93 s
2024-12-09 02:56:42.083353: Yayy! New best EMA pseudo Dice: 0.6885
2024-12-09 02:56:43.640732: 
2024-12-09 02:56:43.643127: Epoch 31
2024-12-09 02:56:43.644438: Current learning rate: 0.00972
2024-12-09 02:58:09.594021: Validation loss did not improve from -0.50548. Patience: 9/50
2024-12-09 02:58:09.595226: train_loss -0.6158
2024-12-09 02:58:09.596273: val_loss -0.4644
2024-12-09 02:58:09.597081: Pseudo dice [0.6978]
2024-12-09 02:58:09.597955: Epoch time: 85.96 s
2024-12-09 02:58:09.598694: Yayy! New best EMA pseudo Dice: 0.6894
2024-12-09 02:58:11.075974: 
2024-12-09 02:58:11.078251: Epoch 32
2024-12-09 02:58:11.079628: Current learning rate: 0.00971
2024-12-09 02:59:36.992945: Validation loss improved from -0.50548 to -0.51186! Patience: 9/50
2024-12-09 02:59:36.993866: train_loss -0.6153
2024-12-09 02:59:36.994995: val_loss -0.5119
2024-12-09 02:59:36.995969: Pseudo dice [0.7213]
2024-12-09 02:59:36.996888: Epoch time: 85.92 s
2024-12-09 02:59:36.997627: Yayy! New best EMA pseudo Dice: 0.6926
2024-12-09 02:59:38.475113: 
2024-12-09 02:59:38.477162: Epoch 33
2024-12-09 02:59:38.478330: Current learning rate: 0.0097
2024-12-09 03:01:04.291041: Validation loss did not improve from -0.51186. Patience: 1/50
2024-12-09 03:01:04.292221: train_loss -0.6163
2024-12-09 03:01:04.293555: val_loss -0.4759
2024-12-09 03:01:04.294564: Pseudo dice [0.6903]
2024-12-09 03:01:04.295454: Epoch time: 85.82 s
2024-12-09 03:01:05.471699: 
2024-12-09 03:01:05.473695: Epoch 34
2024-12-09 03:01:05.474745: Current learning rate: 0.00969
2024-12-09 03:02:31.380738: Validation loss did not improve from -0.51186. Patience: 2/50
2024-12-09 03:02:31.382033: train_loss -0.6176
2024-12-09 03:02:31.383219: val_loss -0.4896
2024-12-09 03:02:31.384032: Pseudo dice [0.7069]
2024-12-09 03:02:31.385045: Epoch time: 85.91 s
2024-12-09 03:02:31.725081: Yayy! New best EMA pseudo Dice: 0.6938
2024-12-09 03:02:33.251544: 
2024-12-09 03:02:33.253874: Epoch 35
2024-12-09 03:02:33.255125: Current learning rate: 0.00968
2024-12-09 03:03:59.217490: Validation loss did not improve from -0.51186. Patience: 3/50
2024-12-09 03:03:59.218928: train_loss -0.6239
2024-12-09 03:03:59.220067: val_loss -0.4964
2024-12-09 03:03:59.220963: Pseudo dice [0.7238]
2024-12-09 03:03:59.221762: Epoch time: 85.97 s
2024-12-09 03:03:59.222640: Yayy! New best EMA pseudo Dice: 0.6968
2024-12-09 03:04:00.759880: 
2024-12-09 03:04:00.762218: Epoch 36
2024-12-09 03:04:00.763178: Current learning rate: 0.00968
2024-12-09 03:05:26.601832: Validation loss did not improve from -0.51186. Patience: 4/50
2024-12-09 03:05:26.603198: train_loss -0.6204
2024-12-09 03:05:26.604351: val_loss -0.5094
2024-12-09 03:05:26.605476: Pseudo dice [0.7293]
2024-12-09 03:05:26.606660: Epoch time: 85.84 s
2024-12-09 03:05:26.607746: Yayy! New best EMA pseudo Dice: 0.7001
2024-12-09 03:05:28.131964: 
2024-12-09 03:05:28.134337: Epoch 37
2024-12-09 03:05:28.135679: Current learning rate: 0.00967
2024-12-09 03:06:54.019884: Validation loss did not improve from -0.51186. Patience: 5/50
2024-12-09 03:06:54.021270: train_loss -0.6272
2024-12-09 03:06:54.022935: val_loss -0.4884
2024-12-09 03:06:54.024164: Pseudo dice [0.7034]
2024-12-09 03:06:54.025383: Epoch time: 85.89 s
2024-12-09 03:06:54.026449: Yayy! New best EMA pseudo Dice: 0.7004
2024-12-09 03:06:55.548555: 
2024-12-09 03:06:55.550403: Epoch 38
2024-12-09 03:06:55.551562: Current learning rate: 0.00966
2024-12-09 03:08:21.551819: Validation loss did not improve from -0.51186. Patience: 6/50
2024-12-09 03:08:21.553108: train_loss -0.6347
2024-12-09 03:08:21.554536: val_loss -0.4827
2024-12-09 03:08:21.555514: Pseudo dice [0.7125]
2024-12-09 03:08:21.556435: Epoch time: 86.01 s
2024-12-09 03:08:21.557626: Yayy! New best EMA pseudo Dice: 0.7016
2024-12-09 03:08:23.099950: 
2024-12-09 03:08:23.102258: Epoch 39
2024-12-09 03:08:23.103367: Current learning rate: 0.00965
2024-12-09 03:09:48.957622: Validation loss did not improve from -0.51186. Patience: 7/50
2024-12-09 03:09:48.958777: train_loss -0.6385
2024-12-09 03:09:48.960437: val_loss -0.4884
2024-12-09 03:09:48.961449: Pseudo dice [0.7179]
2024-12-09 03:09:48.962481: Epoch time: 85.86 s
2024-12-09 03:09:49.332319: Yayy! New best EMA pseudo Dice: 0.7032
2024-12-09 03:09:51.227937: 
2024-12-09 03:09:51.229941: Epoch 40
2024-12-09 03:09:51.231084: Current learning rate: 0.00964
2024-12-09 03:11:17.123752: Validation loss did not improve from -0.51186. Patience: 8/50
2024-12-09 03:11:17.125531: train_loss -0.6324
2024-12-09 03:11:17.127166: val_loss -0.5036
2024-12-09 03:11:17.128236: Pseudo dice [0.7187]
2024-12-09 03:11:17.129199: Epoch time: 85.9 s
2024-12-09 03:11:17.129886: Yayy! New best EMA pseudo Dice: 0.7048
2024-12-09 03:11:18.623724: 
2024-12-09 03:11:18.625830: Epoch 41
2024-12-09 03:11:18.627075: Current learning rate: 0.00963
2024-12-09 03:12:44.532841: Validation loss did not improve from -0.51186. Patience: 9/50
2024-12-09 03:12:44.534114: train_loss -0.6354
2024-12-09 03:12:44.535047: val_loss -0.4577
2024-12-09 03:12:44.535776: Pseudo dice [0.7085]
2024-12-09 03:12:44.536503: Epoch time: 85.91 s
2024-12-09 03:12:44.537281: Yayy! New best EMA pseudo Dice: 0.7051
2024-12-09 03:12:45.979767: 
2024-12-09 03:12:45.981202: Epoch 42
2024-12-09 03:12:45.982363: Current learning rate: 0.00962
2024-12-09 03:14:11.901751: Validation loss did not improve from -0.51186. Patience: 10/50
2024-12-09 03:14:11.905292: train_loss -0.6467
2024-12-09 03:14:11.906653: val_loss -0.4978
2024-12-09 03:14:11.907435: Pseudo dice [0.7123]
2024-12-09 03:14:11.908210: Epoch time: 85.93 s
2024-12-09 03:14:11.909143: Yayy! New best EMA pseudo Dice: 0.7059
2024-12-09 03:14:13.377752: 
2024-12-09 03:14:13.379947: Epoch 43
2024-12-09 03:14:13.381168: Current learning rate: 0.00961
2024-12-09 03:15:39.222056: Validation loss did not improve from -0.51186. Patience: 11/50
2024-12-09 03:15:39.223576: train_loss -0.6422
2024-12-09 03:15:39.224497: val_loss -0.4641
2024-12-09 03:15:39.225402: Pseudo dice [0.7023]
2024-12-09 03:15:39.226406: Epoch time: 85.85 s
2024-12-09 03:15:40.372829: 
2024-12-09 03:15:40.374702: Epoch 44
2024-12-09 03:15:40.375797: Current learning rate: 0.0096
2024-12-09 03:17:06.345670: Validation loss did not improve from -0.51186. Patience: 12/50
2024-12-09 03:17:06.348653: train_loss -0.6514
2024-12-09 03:17:06.352571: val_loss -0.4655
2024-12-09 03:17:06.353621: Pseudo dice [0.7025]
2024-12-09 03:17:06.354924: Epoch time: 85.98 s
2024-12-09 03:17:07.913494: 
2024-12-09 03:17:07.915068: Epoch 45
2024-12-09 03:17:07.915878: Current learning rate: 0.00959
2024-12-09 03:18:33.886334: Validation loss did not improve from -0.51186. Patience: 13/50
2024-12-09 03:18:33.887972: train_loss -0.6451
2024-12-09 03:18:33.889209: val_loss -0.4839
2024-12-09 03:18:33.889996: Pseudo dice [0.722]
2024-12-09 03:18:33.890886: Epoch time: 85.98 s
2024-12-09 03:18:33.891763: Yayy! New best EMA pseudo Dice: 0.7069
2024-12-09 03:18:35.320688: 
2024-12-09 03:18:35.322936: Epoch 46
2024-12-09 03:18:35.323993: Current learning rate: 0.00959
2024-12-09 03:20:01.187908: Validation loss did not improve from -0.51186. Patience: 14/50
2024-12-09 03:20:01.189218: train_loss -0.6437
2024-12-09 03:20:01.190546: val_loss -0.4421
2024-12-09 03:20:01.191318: Pseudo dice [0.6929]
2024-12-09 03:20:01.192040: Epoch time: 85.87 s
2024-12-09 03:20:02.325735: 
2024-12-09 03:20:02.327812: Epoch 47
2024-12-09 03:20:02.329112: Current learning rate: 0.00958
2024-12-09 03:21:28.214819: Validation loss did not improve from -0.51186. Patience: 15/50
2024-12-09 03:21:28.216236: train_loss -0.6464
2024-12-09 03:21:28.217465: val_loss -0.5019
2024-12-09 03:21:28.218425: Pseudo dice [0.7161]
2024-12-09 03:21:28.219226: Epoch time: 85.89 s
2024-12-09 03:21:29.324540: 
2024-12-09 03:21:29.326401: Epoch 48
2024-12-09 03:21:29.327789: Current learning rate: 0.00957
2024-12-09 03:22:55.307137: Validation loss improved from -0.51186 to -0.52662! Patience: 15/50
2024-12-09 03:22:55.308281: train_loss -0.6557
2024-12-09 03:22:55.309397: val_loss -0.5266
2024-12-09 03:22:55.310324: Pseudo dice [0.7408]
2024-12-09 03:22:55.311204: Epoch time: 85.98 s
2024-12-09 03:22:55.312217: Yayy! New best EMA pseudo Dice: 0.71
2024-12-09 03:22:56.728297: 
2024-12-09 03:22:56.730409: Epoch 49
2024-12-09 03:22:56.731584: Current learning rate: 0.00956
2024-12-09 03:24:22.585647: Validation loss did not improve from -0.52662. Patience: 1/50
2024-12-09 03:24:22.587066: train_loss -0.6545
2024-12-09 03:24:22.588490: val_loss -0.4848
2024-12-09 03:24:22.589717: Pseudo dice [0.7101]
2024-12-09 03:24:22.591082: Epoch time: 85.86 s
2024-12-09 03:24:22.928607: Yayy! New best EMA pseudo Dice: 0.71
2024-12-09 03:24:24.397165: 
2024-12-09 03:24:24.399445: Epoch 50
2024-12-09 03:24:24.400896: Current learning rate: 0.00955
2024-12-09 03:25:50.271690: Validation loss did not improve from -0.52662. Patience: 2/50
2024-12-09 03:25:50.273164: train_loss -0.6535
2024-12-09 03:25:50.274380: val_loss -0.4849
2024-12-09 03:25:50.275409: Pseudo dice [0.7187]
2024-12-09 03:25:50.276277: Epoch time: 85.88 s
2024-12-09 03:25:50.277408: Yayy! New best EMA pseudo Dice: 0.7109
2024-12-09 03:25:52.197173: 
2024-12-09 03:25:52.199344: Epoch 51
2024-12-09 03:25:52.200416: Current learning rate: 0.00954
2024-12-09 03:27:18.108015: Validation loss did not improve from -0.52662. Patience: 3/50
2024-12-09 03:27:18.109359: train_loss -0.6537
2024-12-09 03:27:18.110404: val_loss -0.4694
2024-12-09 03:27:18.111513: Pseudo dice [0.7017]
2024-12-09 03:27:18.112680: Epoch time: 85.91 s
2024-12-09 03:27:19.277552: 
2024-12-09 03:27:19.279965: Epoch 52
2024-12-09 03:27:19.281389: Current learning rate: 0.00953
2024-12-09 03:28:45.239281: Validation loss did not improve from -0.52662. Patience: 4/50
2024-12-09 03:28:45.240576: train_loss -0.6608
2024-12-09 03:28:45.242001: val_loss -0.5087
2024-12-09 03:28:45.243050: Pseudo dice [0.7283]
2024-12-09 03:28:45.244209: Epoch time: 85.96 s
2024-12-09 03:28:45.245217: Yayy! New best EMA pseudo Dice: 0.7118
2024-12-09 03:28:46.742036: 
2024-12-09 03:28:46.743929: Epoch 53
2024-12-09 03:28:46.745287: Current learning rate: 0.00952
2024-12-09 03:30:12.585913: Validation loss did not improve from -0.52662. Patience: 5/50
2024-12-09 03:30:12.587164: train_loss -0.6728
2024-12-09 03:30:12.588151: val_loss -0.5193
2024-12-09 03:30:12.589218: Pseudo dice [0.7275]
2024-12-09 03:30:12.590216: Epoch time: 85.85 s
2024-12-09 03:30:12.591038: Yayy! New best EMA pseudo Dice: 0.7133
2024-12-09 03:30:14.088990: 
2024-12-09 03:30:14.090927: Epoch 54
2024-12-09 03:30:14.092057: Current learning rate: 0.00951
2024-12-09 03:31:39.983077: Validation loss did not improve from -0.52662. Patience: 6/50
2024-12-09 03:31:39.984263: train_loss -0.6666
2024-12-09 03:31:39.985444: val_loss -0.4885
2024-12-09 03:31:39.986173: Pseudo dice [0.7296]
2024-12-09 03:31:39.986954: Epoch time: 85.9 s
2024-12-09 03:31:40.340068: Yayy! New best EMA pseudo Dice: 0.715
2024-12-09 03:31:41.847662: 
2024-12-09 03:31:41.849703: Epoch 55
2024-12-09 03:31:41.851069: Current learning rate: 0.0095
2024-12-09 03:33:07.859498: Validation loss did not improve from -0.52662. Patience: 7/50
2024-12-09 03:33:07.860888: train_loss -0.6691
2024-12-09 03:33:07.861953: val_loss -0.5029
2024-12-09 03:33:07.862740: Pseudo dice [0.7098]
2024-12-09 03:33:07.863520: Epoch time: 86.01 s
2024-12-09 03:33:09.013438: 
2024-12-09 03:33:09.015393: Epoch 56
2024-12-09 03:33:09.016286: Current learning rate: 0.00949
2024-12-09 03:34:34.957056: Validation loss did not improve from -0.52662. Patience: 8/50
2024-12-09 03:34:34.958192: train_loss -0.664
2024-12-09 03:34:34.959518: val_loss -0.4811
2024-12-09 03:34:34.960783: Pseudo dice [0.7054]
2024-12-09 03:34:34.962188: Epoch time: 85.95 s
2024-12-09 03:34:36.193204: 
2024-12-09 03:34:36.195146: Epoch 57
2024-12-09 03:34:36.196327: Current learning rate: 0.00949
2024-12-09 03:36:02.096989: Validation loss did not improve from -0.52662. Patience: 9/50
2024-12-09 03:36:02.098471: train_loss -0.6644
2024-12-09 03:36:02.099641: val_loss -0.4914
2024-12-09 03:36:02.100499: Pseudo dice [0.7141]
2024-12-09 03:36:02.101452: Epoch time: 85.91 s
2024-12-09 03:36:03.267131: 
2024-12-09 03:36:03.269500: Epoch 58
2024-12-09 03:36:03.270582: Current learning rate: 0.00948
2024-12-09 03:37:29.231191: Validation loss did not improve from -0.52662. Patience: 10/50
2024-12-09 03:37:29.232462: train_loss -0.6678
2024-12-09 03:37:29.233822: val_loss -0.5211
2024-12-09 03:37:29.234565: Pseudo dice [0.7304]
2024-12-09 03:37:29.235454: Epoch time: 85.97 s
2024-12-09 03:37:29.236138: Yayy! New best EMA pseudo Dice: 0.7153
2024-12-09 03:37:30.741037: 
2024-12-09 03:37:30.743137: Epoch 59
2024-12-09 03:37:30.744231: Current learning rate: 0.00947
2024-12-09 03:38:56.636303: Validation loss did not improve from -0.52662. Patience: 11/50
2024-12-09 03:38:56.637122: train_loss -0.672
2024-12-09 03:38:56.638263: val_loss -0.491
2024-12-09 03:38:56.639047: Pseudo dice [0.7081]
2024-12-09 03:38:56.639774: Epoch time: 85.9 s
2024-12-09 03:38:58.140195: 
2024-12-09 03:38:58.142903: Epoch 60
2024-12-09 03:38:58.144189: Current learning rate: 0.00946
2024-12-09 03:40:24.029937: Validation loss did not improve from -0.52662. Patience: 12/50
2024-12-09 03:40:24.031124: train_loss -0.6726
2024-12-09 03:40:24.032107: val_loss -0.4957
2024-12-09 03:40:24.032980: Pseudo dice [0.7206]
2024-12-09 03:40:24.033728: Epoch time: 85.89 s
2024-12-09 03:40:25.169541: 
2024-12-09 03:40:25.171342: Epoch 61
2024-12-09 03:40:25.172656: Current learning rate: 0.00945
2024-12-09 03:41:51.091944: Validation loss did not improve from -0.52662. Patience: 13/50
2024-12-09 03:41:51.093345: train_loss -0.6765
2024-12-09 03:41:51.094469: val_loss -0.4664
2024-12-09 03:41:51.095393: Pseudo dice [0.7042]
2024-12-09 03:41:51.096092: Epoch time: 85.92 s
2024-12-09 03:41:52.563414: 
2024-12-09 03:41:52.565269: Epoch 62
2024-12-09 03:41:52.566432: Current learning rate: 0.00944
2024-12-09 03:43:18.576199: Validation loss did not improve from -0.52662. Patience: 14/50
2024-12-09 03:43:18.577582: train_loss -0.6663
2024-12-09 03:43:18.578880: val_loss -0.4758
2024-12-09 03:43:18.579616: Pseudo dice [0.7161]
2024-12-09 03:43:18.580409: Epoch time: 86.02 s
2024-12-09 03:43:19.770280: 
2024-12-09 03:43:19.772506: Epoch 63
2024-12-09 03:43:19.773430: Current learning rate: 0.00943
2024-12-09 03:44:45.652870: Validation loss did not improve from -0.52662. Patience: 15/50
2024-12-09 03:44:45.654115: train_loss -0.6745
2024-12-09 03:44:45.655377: val_loss -0.4678
2024-12-09 03:44:45.656362: Pseudo dice [0.714]
2024-12-09 03:44:45.657546: Epoch time: 85.89 s
2024-12-09 03:44:46.820333: 
2024-12-09 03:44:46.822863: Epoch 64
2024-12-09 03:44:46.824124: Current learning rate: 0.00942
2024-12-09 03:46:12.716569: Validation loss did not improve from -0.52662. Patience: 16/50
2024-12-09 03:46:12.717924: train_loss -0.678
2024-12-09 03:46:12.719463: val_loss -0.4951
2024-12-09 03:46:12.720509: Pseudo dice [0.7212]
2024-12-09 03:46:12.721441: Epoch time: 85.9 s
2024-12-09 03:46:14.246713: 
2024-12-09 03:46:14.248894: Epoch 65
2024-12-09 03:46:14.250200: Current learning rate: 0.00941
2024-12-09 03:47:40.241961: Validation loss did not improve from -0.52662. Patience: 17/50
2024-12-09 03:47:40.243191: train_loss -0.6791
2024-12-09 03:47:40.244212: val_loss -0.4917
2024-12-09 03:47:40.245115: Pseudo dice [0.7228]
2024-12-09 03:47:40.246416: Epoch time: 86.0 s
2024-12-09 03:47:40.247644: Yayy! New best EMA pseudo Dice: 0.7157
2024-12-09 03:47:41.766766: 
2024-12-09 03:47:41.768971: Epoch 66
2024-12-09 03:47:41.770279: Current learning rate: 0.0094
2024-12-09 03:49:07.598478: Validation loss did not improve from -0.52662. Patience: 18/50
2024-12-09 03:49:07.599412: train_loss -0.6838
2024-12-09 03:49:07.600531: val_loss -0.4829
2024-12-09 03:49:07.601446: Pseudo dice [0.7134]
2024-12-09 03:49:07.602301: Epoch time: 85.83 s
2024-12-09 03:49:08.774385: 
2024-12-09 03:49:08.776593: Epoch 67
2024-12-09 03:49:08.777612: Current learning rate: 0.00939
2024-12-09 03:50:34.650023: Validation loss did not improve from -0.52662. Patience: 19/50
2024-12-09 03:50:34.651495: train_loss -0.6867
2024-12-09 03:50:34.652813: val_loss -0.478
2024-12-09 03:50:34.653579: Pseudo dice [0.7222]
2024-12-09 03:50:34.654229: Epoch time: 85.88 s
2024-12-09 03:50:34.655017: Yayy! New best EMA pseudo Dice: 0.7162
2024-12-09 03:50:36.199820: 
2024-12-09 03:50:36.202312: Epoch 68
2024-12-09 03:50:36.203362: Current learning rate: 0.00939
2024-12-09 03:52:02.129048: Validation loss did not improve from -0.52662. Patience: 20/50
2024-12-09 03:52:02.130035: train_loss -0.6858
2024-12-09 03:52:02.131336: val_loss -0.4688
2024-12-09 03:52:02.132443: Pseudo dice [0.7077]
2024-12-09 03:52:02.133515: Epoch time: 85.93 s
2024-12-09 03:52:03.337132: 
2024-12-09 03:52:03.339148: Epoch 69
2024-12-09 03:52:03.340091: Current learning rate: 0.00938
2024-12-09 03:53:29.288359: Validation loss did not improve from -0.52662. Patience: 21/50
2024-12-09 03:53:29.289464: train_loss -0.6847
2024-12-09 03:53:29.290547: val_loss -0.4846
2024-12-09 03:53:29.291508: Pseudo dice [0.7157]
2024-12-09 03:53:29.292467: Epoch time: 85.95 s
2024-12-09 03:53:30.826636: 
2024-12-09 03:53:30.829288: Epoch 70
2024-12-09 03:53:30.830961: Current learning rate: 0.00937
2024-12-09 03:54:56.694995: Validation loss did not improve from -0.52662. Patience: 22/50
2024-12-09 03:54:56.696292: train_loss -0.6917
2024-12-09 03:54:56.697418: val_loss -0.517
2024-12-09 03:54:56.698391: Pseudo dice [0.7298]
2024-12-09 03:54:56.699134: Epoch time: 85.87 s
2024-12-09 03:54:56.699894: Yayy! New best EMA pseudo Dice: 0.7168
2024-12-09 03:54:58.216948: 
2024-12-09 03:54:58.218966: Epoch 71
2024-12-09 03:54:58.219810: Current learning rate: 0.00936
2024-12-09 03:56:24.084144: Validation loss did not improve from -0.52662. Patience: 23/50
2024-12-09 03:56:24.085772: train_loss -0.6868
2024-12-09 03:56:24.087425: val_loss -0.4999
2024-12-09 03:56:24.088484: Pseudo dice [0.7229]
2024-12-09 03:56:24.089438: Epoch time: 85.87 s
2024-12-09 03:56:24.090591: Yayy! New best EMA pseudo Dice: 0.7174
2024-12-09 03:56:25.636403: 
2024-12-09 03:56:25.638475: Epoch 72
2024-12-09 03:56:25.639769: Current learning rate: 0.00935
2024-12-09 03:57:51.605665: Validation loss did not improve from -0.52662. Patience: 24/50
2024-12-09 03:57:51.607402: train_loss -0.6945
2024-12-09 03:57:51.609267: val_loss -0.5117
2024-12-09 03:57:51.610345: Pseudo dice [0.7196]
2024-12-09 03:57:51.611645: Epoch time: 85.97 s
2024-12-09 03:57:51.612624: Yayy! New best EMA pseudo Dice: 0.7176
2024-12-09 03:57:53.474193: 
2024-12-09 03:57:53.476677: Epoch 73
2024-12-09 03:57:53.477822: Current learning rate: 0.00934
2024-12-09 03:59:19.322284: Validation loss did not improve from -0.52662. Patience: 25/50
2024-12-09 03:59:19.323463: train_loss -0.7021
2024-12-09 03:59:19.324287: val_loss -0.5153
2024-12-09 03:59:19.325114: Pseudo dice [0.732]
2024-12-09 03:59:19.326183: Epoch time: 85.85 s
2024-12-09 03:59:19.327137: Yayy! New best EMA pseudo Dice: 0.7191
2024-12-09 03:59:20.870098: 
2024-12-09 03:59:20.872002: Epoch 74
2024-12-09 03:59:20.873248: Current learning rate: 0.00933
2024-12-09 04:00:46.744581: Validation loss did not improve from -0.52662. Patience: 26/50
2024-12-09 04:00:46.746056: train_loss -0.6915
2024-12-09 04:00:46.747963: val_loss -0.5014
2024-12-09 04:00:46.749045: Pseudo dice [0.7274]
2024-12-09 04:00:46.750286: Epoch time: 85.88 s
2024-12-09 04:00:47.093525: Yayy! New best EMA pseudo Dice: 0.7199
2024-12-09 04:00:48.596361: 
2024-12-09 04:00:48.598437: Epoch 75
2024-12-09 04:00:48.599516: Current learning rate: 0.00932
2024-12-09 04:02:14.603325: Validation loss did not improve from -0.52662. Patience: 27/50
2024-12-09 04:02:14.604621: train_loss -0.6962
2024-12-09 04:02:14.605744: val_loss -0.502
2024-12-09 04:02:14.606513: Pseudo dice [0.7247]
2024-12-09 04:02:14.607519: Epoch time: 86.01 s
2024-12-09 04:02:14.608357: Yayy! New best EMA pseudo Dice: 0.7204
2024-12-09 04:02:16.155868: 
2024-12-09 04:02:16.158206: Epoch 76
2024-12-09 04:02:16.159596: Current learning rate: 0.00931
2024-12-09 04:03:42.010577: Validation loss did not improve from -0.52662. Patience: 28/50
2024-12-09 04:03:42.012151: train_loss -0.6885
2024-12-09 04:03:42.013796: val_loss -0.4665
2024-12-09 04:03:42.014776: Pseudo dice [0.7112]
2024-12-09 04:03:42.015968: Epoch time: 85.86 s
2024-12-09 04:03:43.221536: 
2024-12-09 04:03:43.224170: Epoch 77
2024-12-09 04:03:43.225378: Current learning rate: 0.0093
2024-12-09 04:05:09.086172: Validation loss did not improve from -0.52662. Patience: 29/50
2024-12-09 04:05:09.087425: train_loss -0.688
2024-12-09 04:05:09.088466: val_loss -0.4913
2024-12-09 04:05:09.089833: Pseudo dice [0.7285]
2024-12-09 04:05:09.090837: Epoch time: 85.87 s
2024-12-09 04:05:10.325566: 
2024-12-09 04:05:10.327562: Epoch 78
2024-12-09 04:05:10.328939: Current learning rate: 0.0093
2024-12-09 04:06:36.249159: Validation loss did not improve from -0.52662. Patience: 30/50
2024-12-09 04:06:36.250371: train_loss -0.6872
2024-12-09 04:06:36.251908: val_loss -0.4503
2024-12-09 04:06:36.253040: Pseudo dice [0.703]
2024-12-09 04:06:36.253823: Epoch time: 85.93 s
2024-12-09 04:06:37.451386: 
2024-12-09 04:06:37.453457: Epoch 79
2024-12-09 04:06:37.454873: Current learning rate: 0.00929
2024-12-09 04:08:03.396398: Validation loss did not improve from -0.52662. Patience: 31/50
2024-12-09 04:08:03.397524: train_loss -0.6945
2024-12-09 04:08:03.398493: val_loss -0.5
2024-12-09 04:08:03.399595: Pseudo dice [0.7192]
2024-12-09 04:08:03.400763: Epoch time: 85.95 s
2024-12-09 04:08:04.963657: 
2024-12-09 04:08:04.965738: Epoch 80
2024-12-09 04:08:04.967132: Current learning rate: 0.00928
2024-12-09 04:09:30.829450: Validation loss did not improve from -0.52662. Patience: 32/50
2024-12-09 04:09:30.830519: train_loss -0.6979
2024-12-09 04:09:30.831595: val_loss -0.5038
2024-12-09 04:09:30.832622: Pseudo dice [0.7267]
2024-12-09 04:09:30.833599: Epoch time: 85.87 s
2024-12-09 04:09:32.017873: 
2024-12-09 04:09:32.020243: Epoch 81
2024-12-09 04:09:32.021477: Current learning rate: 0.00927
2024-12-09 04:10:57.930409: Validation loss did not improve from -0.52662. Patience: 33/50
2024-12-09 04:10:57.932144: train_loss -0.7017
2024-12-09 04:10:57.933345: val_loss -0.5071
2024-12-09 04:10:57.934370: Pseudo dice [0.7314]
2024-12-09 04:10:57.935174: Epoch time: 85.92 s
2024-12-09 04:10:57.936307: Yayy! New best EMA pseudo Dice: 0.7207
2024-12-09 04:10:59.469270: 
2024-12-09 04:10:59.471839: Epoch 82
2024-12-09 04:10:59.473617: Current learning rate: 0.00926
2024-12-09 04:12:25.465439: Validation loss did not improve from -0.52662. Patience: 34/50
2024-12-09 04:12:25.466483: train_loss -0.6964
2024-12-09 04:12:25.467865: val_loss -0.5091
2024-12-09 04:12:25.469042: Pseudo dice [0.7303]
2024-12-09 04:12:25.469989: Epoch time: 86.0 s
2024-12-09 04:12:25.471061: Yayy! New best EMA pseudo Dice: 0.7216
2024-12-09 04:12:27.300239: 
2024-12-09 04:12:27.303015: Epoch 83
2024-12-09 04:12:27.304456: Current learning rate: 0.00925
2024-12-09 04:13:53.169039: Validation loss did not improve from -0.52662. Patience: 35/50
2024-12-09 04:13:53.170257: train_loss -0.7036
2024-12-09 04:13:53.171401: val_loss -0.4826
2024-12-09 04:13:53.172149: Pseudo dice [0.7203]
2024-12-09 04:13:53.173283: Epoch time: 85.87 s
2024-12-09 04:13:54.313547: 
2024-12-09 04:13:54.315598: Epoch 84
2024-12-09 04:13:54.316336: Current learning rate: 0.00924
2024-12-09 04:15:20.198354: Validation loss did not improve from -0.52662. Patience: 36/50
2024-12-09 04:15:20.199862: train_loss -0.7077
2024-12-09 04:15:20.201161: val_loss -0.517
2024-12-09 04:15:20.202213: Pseudo dice [0.7359]
2024-12-09 04:15:20.203176: Epoch time: 85.89 s
2024-12-09 04:15:20.573886: Yayy! New best EMA pseudo Dice: 0.723
2024-12-09 04:15:22.061999: 
2024-12-09 04:15:22.063980: Epoch 85
2024-12-09 04:15:22.065365: Current learning rate: 0.00923
2024-12-09 04:16:48.042107: Validation loss did not improve from -0.52662. Patience: 37/50
2024-12-09 04:16:48.043380: train_loss -0.7103
2024-12-09 04:16:48.044164: val_loss -0.4844
2024-12-09 04:16:48.045363: Pseudo dice [0.7224]
2024-12-09 04:16:48.046252: Epoch time: 85.98 s
2024-12-09 04:16:49.270007: 
2024-12-09 04:16:49.272587: Epoch 86
2024-12-09 04:16:49.274400: Current learning rate: 0.00922
2024-12-09 04:18:15.176627: Validation loss did not improve from -0.52662. Patience: 38/50
2024-12-09 04:18:15.177820: train_loss -0.71
2024-12-09 04:18:15.179195: val_loss -0.5031
2024-12-09 04:18:15.180209: Pseudo dice [0.7225]
2024-12-09 04:18:15.181377: Epoch time: 85.91 s
2024-12-09 04:18:16.342895: 
2024-12-09 04:18:16.345040: Epoch 87
2024-12-09 04:18:16.346241: Current learning rate: 0.00921
2024-12-09 04:19:42.320380: Validation loss did not improve from -0.52662. Patience: 39/50
2024-12-09 04:19:42.325911: train_loss -0.7016
2024-12-09 04:19:42.327628: val_loss -0.4793
2024-12-09 04:19:42.328633: Pseudo dice [0.7204]
2024-12-09 04:19:42.329512: Epoch time: 85.98 s
2024-12-09 04:19:43.539760: 
2024-12-09 04:19:43.542037: Epoch 88
2024-12-09 04:19:43.543599: Current learning rate: 0.0092
2024-12-09 04:21:09.452793: Validation loss did not improve from -0.52662. Patience: 40/50
2024-12-09 04:21:09.454885: train_loss -0.7081
2024-12-09 04:21:09.456761: val_loss -0.5021
2024-12-09 04:21:09.457867: Pseudo dice [0.7268]
2024-12-09 04:21:09.459422: Epoch time: 85.92 s
2024-12-09 04:21:09.460950: Yayy! New best EMA pseudo Dice: 0.723
2024-12-09 04:21:10.984520: 
2024-12-09 04:21:10.986362: Epoch 89
2024-12-09 04:21:10.987804: Current learning rate: 0.0092
2024-12-09 04:22:36.969541: Validation loss did not improve from -0.52662. Patience: 41/50
2024-12-09 04:22:36.970964: train_loss -0.7096
2024-12-09 04:22:36.973677: val_loss -0.4996
2024-12-09 04:22:36.974778: Pseudo dice [0.719]
2024-12-09 04:22:36.976160: Epoch time: 85.99 s
2024-12-09 04:22:38.468396: 
2024-12-09 04:22:38.470605: Epoch 90
2024-12-09 04:22:38.471881: Current learning rate: 0.00919
2024-12-09 04:24:04.336802: Validation loss did not improve from -0.52662. Patience: 42/50
2024-12-09 04:24:04.338125: train_loss -0.6987
2024-12-09 04:24:04.339508: val_loss -0.4764
2024-12-09 04:24:04.340524: Pseudo dice [0.7137]
2024-12-09 04:24:04.341926: Epoch time: 85.87 s
2024-12-09 04:24:05.527937: 
2024-12-09 04:24:05.530433: Epoch 91
2024-12-09 04:24:05.531414: Current learning rate: 0.00918
2024-12-09 04:25:31.423514: Validation loss did not improve from -0.52662. Patience: 43/50
2024-12-09 04:25:31.424985: train_loss -0.7102
2024-12-09 04:25:31.426251: val_loss -0.501
2024-12-09 04:25:31.427098: Pseudo dice [0.7315]
2024-12-09 04:25:31.428200: Epoch time: 85.9 s
2024-12-09 04:25:32.549145: 
2024-12-09 04:25:32.551083: Epoch 92
2024-12-09 04:25:32.552428: Current learning rate: 0.00917
2024-12-09 04:26:58.568478: Validation loss did not improve from -0.52662. Patience: 44/50
2024-12-09 04:26:58.569716: train_loss -0.7172
2024-12-09 04:26:58.570976: val_loss -0.4877
2024-12-09 04:26:58.571675: Pseudo dice [0.7196]
2024-12-09 04:26:58.572719: Epoch time: 86.02 s
2024-12-09 04:26:59.708345: 
2024-12-09 04:26:59.710420: Epoch 93
2024-12-09 04:26:59.711497: Current learning rate: 0.00916
2024-12-09 04:28:25.549795: Validation loss did not improve from -0.52662. Patience: 45/50
2024-12-09 04:28:25.551090: train_loss -0.7048
2024-12-09 04:28:25.552430: val_loss -0.5047
2024-12-09 04:28:25.553279: Pseudo dice [0.7232]
2024-12-09 04:28:25.554125: Epoch time: 85.84 s
2024-12-09 04:28:27.020144: 
2024-12-09 04:28:27.022415: Epoch 94
2024-12-09 04:28:27.023891: Current learning rate: 0.00915
2024-12-09 04:29:52.916555: Validation loss did not improve from -0.52662. Patience: 46/50
2024-12-09 04:29:52.917696: train_loss -0.7141
2024-12-09 04:29:52.918876: val_loss -0.4607
2024-12-09 04:29:52.919804: Pseudo dice [0.71]
2024-12-09 04:29:52.920627: Epoch time: 85.9 s
2024-12-09 04:29:54.371660: 
2024-12-09 04:29:54.373478: Epoch 95
2024-12-09 04:29:54.374750: Current learning rate: 0.00914
2024-12-09 04:31:20.317327: Validation loss did not improve from -0.52662. Patience: 47/50
2024-12-09 04:31:20.320637: train_loss -0.7145
2024-12-09 04:31:20.321958: val_loss -0.5191
2024-12-09 04:31:20.323172: Pseudo dice [0.7229]
2024-12-09 04:31:20.324421: Epoch time: 85.95 s
2024-12-09 04:31:21.483914: 
2024-12-09 04:31:21.486204: Epoch 96
2024-12-09 04:31:21.487410: Current learning rate: 0.00913
2024-12-09 04:32:47.408747: Validation loss did not improve from -0.52662. Patience: 48/50
2024-12-09 04:32:47.409678: train_loss -0.7129
2024-12-09 04:32:47.411024: val_loss -0.514
2024-12-09 04:32:47.411885: Pseudo dice [0.7255]
2024-12-09 04:32:47.412802: Epoch time: 85.93 s
2024-12-09 04:32:48.510998: 
2024-12-09 04:32:48.513191: Epoch 97
2024-12-09 04:32:48.514329: Current learning rate: 0.00912
2024-12-09 04:34:14.353880: Validation loss did not improve from -0.52662. Patience: 49/50
2024-12-09 04:34:14.355094: train_loss -0.7114
2024-12-09 04:34:14.356653: val_loss -0.4919
2024-12-09 04:34:14.357645: Pseudo dice [0.7152]
2024-12-09 04:34:14.358742: Epoch time: 85.85 s
2024-12-09 04:34:15.508660: 
2024-12-09 04:34:15.510821: Epoch 98
2024-12-09 04:34:15.511912: Current learning rate: 0.00911
2024-12-09 04:35:41.413364: Validation loss did not improve from -0.52662. Patience: 50/50
2024-12-09 04:35:41.414351: train_loss -0.7156
2024-12-09 04:35:41.415523: val_loss -0.4949
2024-12-09 04:35:41.416491: Pseudo dice [0.7358]
2024-12-09 04:35:41.417624: Epoch time: 85.91 s
2024-12-09 04:35:42.553880: Patience reached. Stopping training.
2024-12-09 04:35:42.954533: Training done.
2024-12-09 04:35:43.123899: Using splits from existing split file: /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset307_Sohee_Calcium_OCT_CrossValidation/splits_final.json
2024-12-09 04:35:43.129664: The split file contains 5 splits.
2024-12-09 04:35:43.131166: Desired fold for training: 4
2024-12-09 04:35:43.131887: This split has 7 training and 1 validation cases.
2024-12-09 04:35:43.133263: predicting 101-045
2024-12-09 04:35:43.149601: 101-045, shape torch.Size([1, 375, 498, 498]), rank 0
2024-12-09 04:37:43.948527: Validation complete
2024-12-09 04:37:43.949973: Mean Validation Dice:  0.7059905717526602
