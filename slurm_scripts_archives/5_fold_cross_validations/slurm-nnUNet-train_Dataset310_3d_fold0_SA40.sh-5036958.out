/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/bin/python
Starting training with CONFIG=3d_32x160x128_b10, DATASET_ID=310, TRAINER=nnUNetTrainerScaleAnalysis40
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/nnunetv2/training/nnUNetTrainer/nnUNetTrainer.py:169: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  self.grad_scaler = GradScaler() if self.device.type == 'cuda' else None

############################
INFO: You are using the old nnU-Net default plans. We have updated our recommendations. Please consider using those instead! Read more here: https://github.com/MIC-DKFZ/nnUNet/blob/master/documentation/resenc_presets.md
############################

Using device: cuda:0

#######################################################################
Please cite the following paper when using nnU-Net:
Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.
#######################################################################

2025-10-05 08:43:43.232073: do_dummy_2d_data_aug: True
2025-10-05 08:43:43.232765: Using splits from existing split file: /nfs/erelab001/shared/Computational_Group/Naravich/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset310_nnInteractive_Calcium_OCT_CrossValidation/splits_final_40.json
2025-10-05 08:43:43.233438: The split file contains 5 splits.
2025-10-05 08:43:43.233721: Desired fold for training: 0
2025-10-05 08:43:43.233991: This split has 3 training and 5 validation cases.
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/fft_conv_pytorch/fft_conv.py:139: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:306.)
  output = output[crop_slices].contiguous()
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/fft_conv_pytorch/fft_conv.py:139: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:306.)
  output = output[crop_slices].contiguous()
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/fft_conv_pytorch/fft_conv.py:139: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:306.)
  output = output[crop_slices].contiguous()
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/fft_conv_pytorch/fft_conv.py:139: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:306.)
  output = output[crop_slices].contiguous()
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/fft_conv_pytorch/fft_conv.py:139: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:306.)
  output = output[crop_slices].contiguous()
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/fft_conv_pytorch/fft_conv.py:139: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:306.)
  output = output[crop_slices].contiguous()
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/fft_conv_pytorch/fft_conv.py:139: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:306.)
  output = output[crop_slices].contiguous()
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/fft_conv_pytorch/fft_conv.py:139: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:306.)
  output = output[crop_slices].contiguous()
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/fft_conv_pytorch/fft_conv.py:139: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:306.)
  output = output[crop_slices].contiguous()
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/fft_conv_pytorch/fft_conv.py:139: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:306.)
  output = output[crop_slices].contiguous()
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/fft_conv_pytorch/fft_conv.py:139: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:306.)
  output = output[crop_slices].contiguous()
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/fft_conv_pytorch/fft_conv.py:139: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:306.)
  output = output[crop_slices].contiguous()
using pin_memory on device 0
using pin_memory on device 0
2025-10-05 08:43:50.150735: Using torch.compile...

This is the configuration used by this training:
Configuration name: 3d_32x160x128_b10
 {'data_identifier': 'nnUNetPlans_3d_fullres', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 10, 'patch_size': [32, 160, 128], 'median_image_size_in_voxels': [375.0, 498.0, 498.0], 'spacing': [1.0, 1.0, 1.0], 'normalization_schemes': ['ZScoreNormalization'], 'use_mask_for_norm': [False], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'architecture': {'network_class_name': 'dynamic_network_architectures.architectures.unet.PlainConvUNet', 'arch_kwargs': {'n_stages': 6, 'features_per_stage': [32, 64, 128, 256, 320, 320], 'conv_op': 'torch.nn.modules.conv.Conv3d', 'kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]], 'strides': [[1, 1, 1], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [1, 2, 2]], 'n_conv_per_stage': [2, 2, 2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2, 2, 2], 'conv_bias': True, 'norm_op': 'torch.nn.modules.instancenorm.InstanceNorm3d', 'norm_op_kwargs': {'eps': 1e-05, 'affine': True}, 'dropout_op': None, 'dropout_op_kwargs': None, 'nonlin': 'torch.nn.LeakyReLU', 'nonlin_kwargs': {'inplace': True}, 'deep_supervision': True}, '_kw_requires_import': ['conv_op', 'norm_op', 'dropout_op', 'nonlin']}, 'batch_dice': True, 'inherits_from': '3d_fullres'} 

These are the global plan.json settings:
 {'dataset_name': 'Dataset310_nnInteractive_Calcium_OCT_CrossValidation', 'plans_name': 'nnUNetPlans', 'original_median_spacing_after_transp': [1.0, 1.0, 1.0], 'original_median_shape_after_transp': [375, 498, 498], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 0.4977976381778717, 'mean': 0.1394134759902954, 'median': 0.09849607944488525, 'min': 0.0, 'percentile_00_5': 0.015305490233004093, 'percentile_99_5': 0.4977976381778717, 'std': 0.121165432035923}}} 

2025-10-05 08:43:56.171413: unpacking dataset...
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/torch/_inductor/lowering.py:7095: UserWarning: 
Online softmax is disabled on the fly since Inductor decides to
split the reduction. Cut an issue to PyTorch if this is an
important use case and you want to speed it up with online
softmax.

  warnings.warn(
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/torch/_inductor/lowering.py:7095: UserWarning: 
Online softmax is disabled on the fly since Inductor decides to
split the reduction. Cut an issue to PyTorch if this is an
important use case and you want to speed it up with online
softmax.

  warnings.warn(
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/torch/_inductor/lowering.py:7095: UserWarning: 
Online softmax is disabled on the fly since Inductor decides to
split the reduction. Cut an issue to PyTorch if this is an
important use case and you want to speed it up with online
softmax.

  warnings.warn(
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/torch/_inductor/lowering.py:7095: UserWarning: 
Online softmax is disabled on the fly since Inductor decides to
split the reduction. Cut an issue to PyTorch if this is an
important use case and you want to speed it up with online
softmax.

  warnings.warn(
2025-10-05 08:44:00.435690: unpacking done...
2025-10-05 08:44:00.454134: Unable to plot network architecture: nnUNet_compile is enabled!
2025-10-05 08:44:00.461350: 
2025-10-05 08:44:00.461513: Epoch 0
2025-10-05 08:44:00.461720: Current learning rate: 0.01
2025-10-05 08:45:22.690226: Validation loss improved from 1000.00000 to -0.10144! Patience: 0/50
2025-10-05 08:45:22.690887: train_loss -0.0581
2025-10-05 08:45:22.691086: val_loss -0.1014
2025-10-05 08:45:22.691254: Pseudo dice [np.float32(0.5478)]
2025-10-05 08:45:22.691440: Epoch time: 82.23 s
2025-10-05 08:45:22.691599: Yayy! New best EMA pseudo Dice: 0.5478000044822693
2025-10-05 08:45:24.258839: 
2025-10-05 08:45:24.259250: Epoch 1
2025-10-05 08:45:24.259483: Current learning rate: 0.00994
2025-10-05 08:46:10.198010: Validation loss improved from -0.10144 to -0.17765! Patience: 0/50
2025-10-05 08:46:10.198539: train_loss -0.2328
2025-10-05 08:46:10.198701: val_loss -0.1777
2025-10-05 08:46:10.198858: Pseudo dice [np.float32(0.5671)]
2025-10-05 08:46:10.199034: Epoch time: 45.94 s
2025-10-05 08:46:10.199168: Yayy! New best EMA pseudo Dice: 0.5497000217437744
2025-10-05 08:46:11.476282: 
2025-10-05 08:46:11.476700: Epoch 2
2025-10-05 08:46:11.476925: Current learning rate: 0.00988
2025-10-05 08:46:57.628029: Validation loss improved from -0.17765 to -0.18603! Patience: 0/50
2025-10-05 08:46:57.628764: train_loss -0.3021
2025-10-05 08:46:57.629242: val_loss -0.186
2025-10-05 08:46:57.629640: Pseudo dice [np.float32(0.5844)]
2025-10-05 08:46:57.630117: Epoch time: 46.15 s
2025-10-05 08:46:57.630527: Yayy! New best EMA pseudo Dice: 0.5532000064849854
2025-10-05 08:46:58.805873: 
2025-10-05 08:46:58.806217: Epoch 3
2025-10-05 08:46:58.806422: Current learning rate: 0.00982
2025-10-05 08:47:44.882819: Validation loss did not improve from -0.18603. Patience: 1/50
2025-10-05 08:47:44.883351: train_loss -0.3751
2025-10-05 08:47:44.883541: val_loss -0.168
2025-10-05 08:47:44.883705: Pseudo dice [np.float32(0.5177)]
2025-10-05 08:47:44.883875: Epoch time: 46.08 s
2025-10-05 08:47:45.524898: 
2025-10-05 08:47:45.525242: Epoch 4
2025-10-05 08:47:45.525456: Current learning rate: 0.00976
2025-10-05 08:48:31.602924: Validation loss improved from -0.18603 to -0.21629! Patience: 1/50
2025-10-05 08:48:31.603520: train_loss -0.4053
2025-10-05 08:48:31.603769: val_loss -0.2163
2025-10-05 08:48:31.603942: Pseudo dice [np.float32(0.5971)]
2025-10-05 08:48:31.604109: Epoch time: 46.08 s
2025-10-05 08:48:32.030661: Yayy! New best EMA pseudo Dice: 0.5544000267982483
2025-10-05 08:48:33.203878: 
2025-10-05 08:48:33.204211: Epoch 5
2025-10-05 08:48:33.204407: Current learning rate: 0.0097
2025-10-05 08:49:19.324919: Validation loss did not improve from -0.21629. Patience: 1/50
2025-10-05 08:49:19.325320: train_loss -0.4178
2025-10-05 08:49:19.325514: val_loss -0.1321
2025-10-05 08:49:19.325705: Pseudo dice [np.float32(0.5251)]
2025-10-05 08:49:19.325861: Epoch time: 46.12 s
2025-10-05 08:49:19.963411: 
2025-10-05 08:49:19.964988: Epoch 6
2025-10-05 08:49:19.965282: Current learning rate: 0.00964
2025-10-05 08:50:06.050143: Validation loss improved from -0.21629 to -0.27677! Patience: 1/50
2025-10-05 08:50:06.050980: train_loss -0.4601
2025-10-05 08:50:06.051152: val_loss -0.2768
2025-10-05 08:50:06.051353: Pseudo dice [np.float32(0.6361)]
2025-10-05 08:50:06.051527: Epoch time: 46.09 s
2025-10-05 08:50:06.051766: Yayy! New best EMA pseudo Dice: 0.5598999857902527
2025-10-05 08:50:07.147236: 
2025-10-05 08:50:07.147688: Epoch 7
2025-10-05 08:50:07.147965: Current learning rate: 0.00958
2025-10-05 08:50:53.239992: Validation loss did not improve from -0.27677. Patience: 1/50
2025-10-05 08:50:53.240480: train_loss -0.4619
2025-10-05 08:50:53.240726: val_loss -0.2173
2025-10-05 08:50:53.240917: Pseudo dice [np.float32(0.6126)]
2025-10-05 08:50:53.241170: Epoch time: 46.09 s
2025-10-05 08:50:53.241304: Yayy! New best EMA pseudo Dice: 0.5651999711990356
2025-10-05 08:50:54.351680: 
2025-10-05 08:50:54.352046: Epoch 8
2025-10-05 08:50:54.352381: Current learning rate: 0.00952
2025-10-05 08:51:40.488360: Validation loss did not improve from -0.27677. Patience: 2/50
2025-10-05 08:51:40.489263: train_loss -0.4918
2025-10-05 08:51:40.489648: val_loss -0.2537
2025-10-05 08:51:40.489843: Pseudo dice [np.float32(0.5995)]
2025-10-05 08:51:40.490057: Epoch time: 46.14 s
2025-10-05 08:51:40.490412: Yayy! New best EMA pseudo Dice: 0.5685999989509583
2025-10-05 08:51:41.567411: 
2025-10-05 08:51:41.567842: Epoch 9
2025-10-05 08:51:41.568155: Current learning rate: 0.00946
2025-10-05 08:52:27.634460: Validation loss did not improve from -0.27677. Patience: 3/50
2025-10-05 08:52:27.634994: train_loss -0.5079
2025-10-05 08:52:27.635283: val_loss -0.2672
2025-10-05 08:52:27.635540: Pseudo dice [np.float32(0.6148)]
2025-10-05 08:52:27.635814: Epoch time: 46.07 s
2025-10-05 08:52:28.070274: Yayy! New best EMA pseudo Dice: 0.573199987411499
2025-10-05 08:52:29.172436: 
2025-10-05 08:52:29.172969: Epoch 10
2025-10-05 08:52:29.173174: Current learning rate: 0.0094
2025-10-05 08:53:15.224421: Validation loss did not improve from -0.27677. Patience: 4/50
2025-10-05 08:53:15.225116: train_loss -0.5155
2025-10-05 08:53:15.225272: val_loss -0.1606
2025-10-05 08:53:15.225404: Pseudo dice [np.float32(0.5378)]
2025-10-05 08:53:15.225548: Epoch time: 46.05 s
2025-10-05 08:53:15.847760: 
2025-10-05 08:53:15.848132: Epoch 11
2025-10-05 08:53:15.848354: Current learning rate: 0.00934
2025-10-05 08:54:01.905700: Validation loss did not improve from -0.27677. Patience: 5/50
2025-10-05 08:54:01.906122: train_loss -0.5337
2025-10-05 08:54:01.906303: val_loss -0.2189
2025-10-05 08:54:01.906428: Pseudo dice [np.float32(0.5847)]
2025-10-05 08:54:01.906574: Epoch time: 46.06 s
2025-10-05 08:54:02.966448: 
2025-10-05 08:54:02.974689: Epoch 12
2025-10-05 08:54:02.983561: Current learning rate: 0.00928
2025-10-05 08:54:49.220180: Validation loss improved from -0.27677 to -0.30034! Patience: 5/50
2025-10-05 08:54:49.220868: train_loss -0.5391
2025-10-05 08:54:49.221081: val_loss -0.3003
2025-10-05 08:54:49.221303: Pseudo dice [np.float32(0.6645)]
2025-10-05 08:54:49.221485: Epoch time: 46.26 s
2025-10-05 08:54:49.221642: Yayy! New best EMA pseudo Dice: 0.5805000066757202
2025-10-05 08:54:50.358535: 
2025-10-05 08:54:50.358902: Epoch 13
2025-10-05 08:54:50.359174: Current learning rate: 0.00922
2025-10-05 08:55:36.440695: Validation loss did not improve from -0.30034. Patience: 1/50
2025-10-05 08:55:36.441180: train_loss -0.5502
2025-10-05 08:55:36.441439: val_loss -0.294
2025-10-05 08:55:36.441676: Pseudo dice [np.float32(0.6487)]
2025-10-05 08:55:36.441954: Epoch time: 46.08 s
2025-10-05 08:55:36.442084: Yayy! New best EMA pseudo Dice: 0.5873000025749207
2025-10-05 08:55:37.528365: 
2025-10-05 08:55:37.528678: Epoch 14
2025-10-05 08:55:37.528893: Current learning rate: 0.00916
2025-10-05 08:56:23.588177: Validation loss did not improve from -0.30034. Patience: 2/50
2025-10-05 08:56:23.588945: train_loss -0.5672
2025-10-05 08:56:23.589134: val_loss -0.2776
2025-10-05 08:56:23.589311: Pseudo dice [np.float32(0.6366)]
2025-10-05 08:56:23.589530: Epoch time: 46.06 s
2025-10-05 08:56:24.012519: Yayy! New best EMA pseudo Dice: 0.5922999978065491
2025-10-05 08:56:25.044862: 
2025-10-05 08:56:25.045244: Epoch 15
2025-10-05 08:56:25.045511: Current learning rate: 0.0091
2025-10-05 08:57:11.176596: Validation loss did not improve from -0.30034. Patience: 3/50
2025-10-05 08:57:11.177172: train_loss -0.5685
2025-10-05 08:57:11.177448: val_loss -0.1728
2025-10-05 08:57:11.177715: Pseudo dice [np.float32(0.5758)]
2025-10-05 08:57:11.178011: Epoch time: 46.13 s
2025-10-05 08:57:11.820307: 
2025-10-05 08:57:11.820700: Epoch 16
2025-10-05 08:57:11.820943: Current learning rate: 0.00903
2025-10-05 08:57:57.973002: Validation loss did not improve from -0.30034. Patience: 4/50
2025-10-05 08:57:57.973567: train_loss -0.5807
2025-10-05 08:57:57.973870: val_loss -0.2347
2025-10-05 08:57:57.974036: Pseudo dice [np.float32(0.6058)]
2025-10-05 08:57:57.974211: Epoch time: 46.15 s
2025-10-05 08:57:58.621366: 
2025-10-05 08:57:58.631243: Epoch 17
2025-10-05 08:57:58.640051: Current learning rate: 0.00897
2025-10-05 08:58:44.801070: Validation loss did not improve from -0.30034. Patience: 5/50
2025-10-05 08:58:44.801728: train_loss -0.594
2025-10-05 08:58:44.802141: val_loss -0.2695
2025-10-05 08:58:44.802503: Pseudo dice [np.float32(0.6298)]
2025-10-05 08:58:44.802793: Epoch time: 46.18 s
2025-10-05 08:58:44.802985: Yayy! New best EMA pseudo Dice: 0.5958999991416931
2025-10-05 08:58:45.868285: 
2025-10-05 08:58:45.868837: Epoch 18
2025-10-05 08:58:45.869339: Current learning rate: 0.00891
2025-10-05 08:59:31.984735: Validation loss did not improve from -0.30034. Patience: 6/50
2025-10-05 08:59:31.985409: train_loss -0.5831
2025-10-05 08:59:31.985596: val_loss -0.2254
2025-10-05 08:59:31.985742: Pseudo dice [np.float32(0.5844)]
2025-10-05 08:59:31.985882: Epoch time: 46.12 s
2025-10-05 08:59:32.635669: 
2025-10-05 08:59:32.642971: Epoch 19
2025-10-05 08:59:32.651669: Current learning rate: 0.00885
2025-10-05 09:00:18.841394: Validation loss did not improve from -0.30034. Patience: 7/50
2025-10-05 09:00:18.841904: train_loss -0.6142
2025-10-05 09:00:18.842089: val_loss -0.2698
2025-10-05 09:00:18.842239: Pseudo dice [np.float32(0.644)]
2025-10-05 09:00:18.842379: Epoch time: 46.21 s
2025-10-05 09:00:19.265069: Yayy! New best EMA pseudo Dice: 0.5996999740600586
2025-10-05 09:00:20.364942: 
2025-10-05 09:00:20.365313: Epoch 20
2025-10-05 09:00:20.365566: Current learning rate: 0.00879
2025-10-05 09:01:06.567270: Validation loss did not improve from -0.30034. Patience: 8/50
2025-10-05 09:01:06.568625: train_loss -0.6117
2025-10-05 09:01:06.569014: val_loss -0.2046
2025-10-05 09:01:06.569419: Pseudo dice [np.float32(0.5923)]
2025-10-05 09:01:06.569819: Epoch time: 46.2 s
2025-10-05 09:01:07.233601: 
2025-10-05 09:01:07.241280: Epoch 21
2025-10-05 09:01:07.249735: Current learning rate: 0.00873
2025-10-05 09:01:53.423119: Validation loss did not improve from -0.30034. Patience: 9/50
2025-10-05 09:01:53.423552: train_loss -0.6291
2025-10-05 09:01:53.423701: val_loss -0.2405
2025-10-05 09:01:53.423901: Pseudo dice [np.float32(0.6072)]
2025-10-05 09:01:53.424081: Epoch time: 46.19 s
2025-10-05 09:01:53.424229: Yayy! New best EMA pseudo Dice: 0.5997999906539917
2025-10-05 09:01:54.495431: 
2025-10-05 09:01:54.505283: Epoch 22
2025-10-05 09:01:54.518984: Current learning rate: 0.00867
2025-10-05 09:02:40.689495: Validation loss did not improve from -0.30034. Patience: 10/50
2025-10-05 09:02:40.690762: train_loss -0.6255
2025-10-05 09:02:40.691126: val_loss -0.2727
2025-10-05 09:02:40.691536: Pseudo dice [np.float32(0.6413)]
2025-10-05 09:02:40.691966: Epoch time: 46.2 s
2025-10-05 09:02:40.692345: Yayy! New best EMA pseudo Dice: 0.6039000153541565
2025-10-05 09:02:41.758207: 
2025-10-05 09:02:41.758734: Epoch 23
2025-10-05 09:02:41.759151: Current learning rate: 0.00861
2025-10-05 09:03:27.970406: Validation loss did not improve from -0.30034. Patience: 11/50
2025-10-05 09:03:27.970852: train_loss -0.6396
2025-10-05 09:03:27.971048: val_loss -0.2393
2025-10-05 09:03:27.971225: Pseudo dice [np.float32(0.6233)]
2025-10-05 09:03:27.971420: Epoch time: 46.21 s
2025-10-05 09:03:27.971585: Yayy! New best EMA pseudo Dice: 0.60589998960495
2025-10-05 09:03:29.035791: 
2025-10-05 09:03:29.036159: Epoch 24
2025-10-05 09:03:29.036391: Current learning rate: 0.00855
2025-10-05 09:04:15.205421: Validation loss did not improve from -0.30034. Patience: 12/50
2025-10-05 09:04:15.206006: train_loss -0.6499
2025-10-05 09:04:15.206176: val_loss -0.2896
2025-10-05 09:04:15.206347: Pseudo dice [np.float32(0.6309)]
2025-10-05 09:04:15.206542: Epoch time: 46.17 s
2025-10-05 09:04:15.825612: Yayy! New best EMA pseudo Dice: 0.6083999872207642
2025-10-05 09:04:17.115371: 
2025-10-05 09:04:17.115801: Epoch 25
2025-10-05 09:04:17.116153: Current learning rate: 0.00849
2025-10-05 09:05:03.433965: Validation loss did not improve from -0.30034. Patience: 13/50
2025-10-05 09:05:03.434432: train_loss -0.6493
2025-10-05 09:05:03.434602: val_loss -0.2403
2025-10-05 09:05:03.434748: Pseudo dice [np.float32(0.6103)]
2025-10-05 09:05:03.434897: Epoch time: 46.32 s
2025-10-05 09:05:03.435038: Yayy! New best EMA pseudo Dice: 0.6086000204086304
2025-10-05 09:05:04.508967: 
2025-10-05 09:05:04.509318: Epoch 26
2025-10-05 09:05:04.509592: Current learning rate: 0.00843
2025-10-05 09:05:50.664461: Validation loss did not improve from -0.30034. Patience: 14/50
2025-10-05 09:05:50.665025: train_loss -0.6637
2025-10-05 09:05:50.665213: val_loss -0.2627
2025-10-05 09:05:50.665361: Pseudo dice [np.float32(0.6241)]
2025-10-05 09:05:50.665533: Epoch time: 46.16 s
2025-10-05 09:05:50.665695: Yayy! New best EMA pseudo Dice: 0.6100999712944031
2025-10-05 09:05:52.112193: 
2025-10-05 09:05:52.112746: Epoch 27
2025-10-05 09:05:52.113131: Current learning rate: 0.00836
2025-10-05 09:06:38.420634: Validation loss did not improve from -0.30034. Patience: 15/50
2025-10-05 09:06:38.421009: train_loss -0.6559
2025-10-05 09:06:38.421156: val_loss -0.2477
2025-10-05 09:06:38.421309: Pseudo dice [np.float32(0.6102)]
2025-10-05 09:06:38.421493: Epoch time: 46.31 s
2025-10-05 09:06:38.421667: Yayy! New best EMA pseudo Dice: 0.6100999712944031
2025-10-05 09:06:39.489552: 
2025-10-05 09:06:39.489867: Epoch 28
2025-10-05 09:06:39.490053: Current learning rate: 0.0083
2025-10-05 09:07:25.668128: Validation loss did not improve from -0.30034. Patience: 16/50
2025-10-05 09:07:25.669137: train_loss -0.6642
2025-10-05 09:07:25.669542: val_loss -0.1395
2025-10-05 09:07:25.669900: Pseudo dice [np.float32(0.5795)]
2025-10-05 09:07:25.670368: Epoch time: 46.18 s
2025-10-05 09:07:26.301765: 
2025-10-05 09:07:26.302150: Epoch 29
2025-10-05 09:07:26.302449: Current learning rate: 0.00824
2025-10-05 09:08:12.490510: Validation loss did not improve from -0.30034. Patience: 17/50
2025-10-05 09:08:12.490938: train_loss -0.6808
2025-10-05 09:08:12.491181: val_loss -0.2393
2025-10-05 09:08:12.491636: Pseudo dice [np.float32(0.619)]
2025-10-05 09:08:12.492034: Epoch time: 46.19 s
2025-10-05 09:08:13.588202: 
2025-10-05 09:08:13.588632: Epoch 30
2025-10-05 09:08:13.588981: Current learning rate: 0.00818
2025-10-05 09:08:59.796828: Validation loss did not improve from -0.30034. Patience: 18/50
2025-10-05 09:08:59.797542: train_loss -0.6772
2025-10-05 09:08:59.797721: val_loss -0.2153
2025-10-05 09:08:59.797875: Pseudo dice [np.float32(0.5855)]
2025-10-05 09:08:59.798142: Epoch time: 46.21 s
2025-10-05 09:09:00.436943: 
2025-10-05 09:09:00.437346: Epoch 31
2025-10-05 09:09:00.437582: Current learning rate: 0.00812
2025-10-05 09:09:46.587640: Validation loss did not improve from -0.30034. Patience: 19/50
2025-10-05 09:09:46.588109: train_loss -0.6886
2025-10-05 09:09:46.588335: val_loss -0.2917
2025-10-05 09:09:46.588539: Pseudo dice [np.float32(0.6543)]
2025-10-05 09:09:46.588713: Epoch time: 46.15 s
2025-10-05 09:09:46.588945: Yayy! New best EMA pseudo Dice: 0.61080002784729
2025-10-05 09:09:47.668386: 
2025-10-05 09:09:47.668804: Epoch 32
2025-10-05 09:09:47.669090: Current learning rate: 0.00806
2025-10-05 09:10:33.819999: Validation loss did not improve from -0.30034. Patience: 20/50
2025-10-05 09:10:33.820714: train_loss -0.6954
2025-10-05 09:10:33.820971: val_loss -0.19
2025-10-05 09:10:33.821281: Pseudo dice [np.float32(0.5874)]
2025-10-05 09:10:33.821537: Epoch time: 46.15 s
2025-10-05 09:10:34.468017: 
2025-10-05 09:10:34.468405: Epoch 33
2025-10-05 09:10:34.468622: Current learning rate: 0.008
2025-10-05 09:11:20.652651: Validation loss did not improve from -0.30034. Patience: 21/50
2025-10-05 09:11:20.653205: train_loss -0.7122
2025-10-05 09:11:20.653475: val_loss -0.1993
2025-10-05 09:11:20.653702: Pseudo dice [np.float32(0.6094)]
2025-10-05 09:11:20.653927: Epoch time: 46.19 s
2025-10-05 09:11:21.312244: 
2025-10-05 09:11:21.312621: Epoch 34
2025-10-05 09:11:21.312841: Current learning rate: 0.00793
2025-10-05 09:12:07.511341: Validation loss did not improve from -0.30034. Patience: 22/50
2025-10-05 09:12:07.511956: train_loss -0.7067
2025-10-05 09:12:07.512281: val_loss -0.2633
2025-10-05 09:12:07.512530: Pseudo dice [np.float32(0.6313)]
2025-10-05 09:12:07.512809: Epoch time: 46.2 s
2025-10-05 09:12:07.936362: Yayy! New best EMA pseudo Dice: 0.61080002784729
2025-10-05 09:12:09.001838: 
2025-10-05 09:12:09.002185: Epoch 35
2025-10-05 09:12:09.002389: Current learning rate: 0.00787
2025-10-05 09:12:55.183553: Validation loss did not improve from -0.30034. Patience: 23/50
2025-10-05 09:12:55.184052: train_loss -0.7142
2025-10-05 09:12:55.184239: val_loss -0.2113
2025-10-05 09:12:55.184422: Pseudo dice [np.float32(0.6097)]
2025-10-05 09:12:55.184640: Epoch time: 46.18 s
2025-10-05 09:12:55.824686: 
2025-10-05 09:12:55.825041: Epoch 36
2025-10-05 09:12:55.825264: Current learning rate: 0.00781
2025-10-05 09:13:42.047042: Validation loss did not improve from -0.30034. Patience: 24/50
2025-10-05 09:13:42.047704: train_loss -0.7231
2025-10-05 09:13:42.047863: val_loss -0.2408
2025-10-05 09:13:42.047991: Pseudo dice [np.float32(0.6557)]
2025-10-05 09:13:42.048152: Epoch time: 46.22 s
2025-10-05 09:13:42.048287: Yayy! New best EMA pseudo Dice: 0.6151999831199646
2025-10-05 09:13:43.107539: 
2025-10-05 09:13:43.107913: Epoch 37
2025-10-05 09:13:43.108119: Current learning rate: 0.00775
2025-10-05 09:14:29.276439: Validation loss did not improve from -0.30034. Patience: 25/50
2025-10-05 09:14:29.276911: train_loss -0.7224
2025-10-05 09:14:29.277069: val_loss -0.1464
2025-10-05 09:14:29.277202: Pseudo dice [np.float32(0.5816)]
2025-10-05 09:14:29.277437: Epoch time: 46.17 s
2025-10-05 09:14:29.925210: 
2025-10-05 09:14:29.925588: Epoch 38
2025-10-05 09:14:29.925796: Current learning rate: 0.00769
2025-10-05 09:15:16.128043: Validation loss did not improve from -0.30034. Patience: 26/50
2025-10-05 09:15:16.128784: train_loss -0.727
2025-10-05 09:15:16.128958: val_loss -0.2484
2025-10-05 09:15:16.129105: Pseudo dice [np.float32(0.6414)]
2025-10-05 09:15:16.129260: Epoch time: 46.2 s
2025-10-05 09:15:16.775878: 
2025-10-05 09:15:16.776196: Epoch 39
2025-10-05 09:15:16.776415: Current learning rate: 0.00763
2025-10-05 09:16:02.999449: Validation loss did not improve from -0.30034. Patience: 27/50
2025-10-05 09:16:02.999926: train_loss -0.7345
2025-10-05 09:16:03.000119: val_loss -0.2514
2025-10-05 09:16:03.000295: Pseudo dice [np.float32(0.6349)]
2025-10-05 09:16:03.000473: Epoch time: 46.22 s
2025-10-05 09:16:03.432498: Yayy! New best EMA pseudo Dice: 0.6168000102043152
2025-10-05 09:16:04.490721: 
2025-10-05 09:16:04.491032: Epoch 40
2025-10-05 09:16:04.491347: Current learning rate: 0.00756
2025-10-05 09:16:50.661421: Validation loss did not improve from -0.30034. Patience: 28/50
2025-10-05 09:16:50.662017: train_loss -0.7419
2025-10-05 09:16:50.662235: val_loss -0.2346
2025-10-05 09:16:50.662482: Pseudo dice [np.float32(0.6175)]
2025-10-05 09:16:50.662653: Epoch time: 46.17 s
2025-10-05 09:16:50.662859: Yayy! New best EMA pseudo Dice: 0.6169000267982483
2025-10-05 09:16:51.748735: 
2025-10-05 09:16:51.749117: Epoch 41
2025-10-05 09:16:51.749318: Current learning rate: 0.0075
2025-10-05 09:17:37.928500: Validation loss did not improve from -0.30034. Patience: 29/50
2025-10-05 09:17:37.929071: train_loss -0.7467
2025-10-05 09:17:37.929286: val_loss -0.1447
2025-10-05 09:17:37.929516: Pseudo dice [np.float32(0.5989)]
2025-10-05 09:17:37.929707: Epoch time: 46.18 s
2025-10-05 09:17:38.996999: 
2025-10-05 09:17:38.997301: Epoch 42
2025-10-05 09:17:38.997566: Current learning rate: 0.00744
2025-10-05 09:18:25.251226: Validation loss did not improve from -0.30034. Patience: 30/50
2025-10-05 09:18:25.252345: train_loss -0.7562
2025-10-05 09:18:25.252661: val_loss -0.2399
2025-10-05 09:18:25.253344: Pseudo dice [np.float32(0.6256)]
2025-10-05 09:18:25.253606: Epoch time: 46.26 s
2025-10-05 09:18:25.887201: 
2025-10-05 09:18:25.887583: Epoch 43
2025-10-05 09:18:25.887807: Current learning rate: 0.00738
2025-10-05 09:19:12.143951: Validation loss did not improve from -0.30034. Patience: 31/50
2025-10-05 09:19:12.144588: train_loss -0.7421
2025-10-05 09:19:12.145001: val_loss -0.1925
2025-10-05 09:19:12.145345: Pseudo dice [np.float32(0.6084)]
2025-10-05 09:19:12.145728: Epoch time: 46.26 s
2025-10-05 09:19:12.779957: 
2025-10-05 09:19:12.780356: Epoch 44
2025-10-05 09:19:12.780611: Current learning rate: 0.00732
2025-10-05 09:19:59.012110: Validation loss did not improve from -0.30034. Patience: 32/50
2025-10-05 09:19:59.012636: train_loss -0.7568
2025-10-05 09:19:59.012800: val_loss -0.1238
2025-10-05 09:19:59.012965: Pseudo dice [np.float32(0.553)]
2025-10-05 09:19:59.013193: Epoch time: 46.23 s
2025-10-05 09:20:00.054327: 
2025-10-05 09:20:00.054670: Epoch 45
2025-10-05 09:20:00.054880: Current learning rate: 0.00725
2025-10-05 09:20:46.194057: Validation loss did not improve from -0.30034. Patience: 33/50
2025-10-05 09:20:46.194523: train_loss -0.7708
2025-10-05 09:20:46.194705: val_loss -0.1834
2025-10-05 09:20:46.194872: Pseudo dice [np.float32(0.5987)]
2025-10-05 09:20:46.195013: Epoch time: 46.14 s
2025-10-05 09:20:46.826902: 
2025-10-05 09:20:46.827254: Epoch 46
2025-10-05 09:20:46.827464: Current learning rate: 0.00719
2025-10-05 09:21:32.962722: Validation loss did not improve from -0.30034. Patience: 34/50
2025-10-05 09:21:32.963497: train_loss -0.7693
2025-10-05 09:21:32.963803: val_loss -0.2417
2025-10-05 09:21:32.964002: Pseudo dice [np.float32(0.6287)]
2025-10-05 09:21:32.964261: Epoch time: 46.14 s
2025-10-05 09:21:33.604216: 
2025-10-05 09:21:33.604735: Epoch 47
2025-10-05 09:21:33.605128: Current learning rate: 0.00713
2025-10-05 09:22:19.765720: Validation loss did not improve from -0.30034. Patience: 35/50
2025-10-05 09:22:19.766326: train_loss -0.7703
2025-10-05 09:22:19.766512: val_loss -0.2232
2025-10-05 09:22:19.766671: Pseudo dice [np.float32(0.6197)]
2025-10-05 09:22:19.766826: Epoch time: 46.16 s
2025-10-05 09:22:20.406703: 
2025-10-05 09:22:20.407185: Epoch 48
2025-10-05 09:22:20.407512: Current learning rate: 0.00707
2025-10-05 09:23:06.581095: Validation loss did not improve from -0.30034. Patience: 36/50
2025-10-05 09:23:06.581883: train_loss -0.7796
2025-10-05 09:23:06.582184: val_loss -0.2197
2025-10-05 09:23:06.582393: Pseudo dice [np.float32(0.6299)]
2025-10-05 09:23:06.582599: Epoch time: 46.18 s
2025-10-05 09:23:07.231503: 
2025-10-05 09:23:07.231835: Epoch 49
2025-10-05 09:23:07.232024: Current learning rate: 0.007
2025-10-05 09:23:53.388264: Validation loss did not improve from -0.30034. Patience: 37/50
2025-10-05 09:23:53.389037: train_loss -0.784
2025-10-05 09:23:53.389392: val_loss -0.2229
2025-10-05 09:23:53.389736: Pseudo dice [np.float32(0.6377)]
2025-10-05 09:23:53.390013: Epoch time: 46.16 s
2025-10-05 09:23:54.605766: 
2025-10-05 09:23:54.606223: Epoch 50
2025-10-05 09:23:54.606568: Current learning rate: 0.00694
2025-10-05 09:24:40.832456: Validation loss did not improve from -0.30034. Patience: 38/50
2025-10-05 09:24:40.833104: train_loss -0.7802
2025-10-05 09:24:40.833313: val_loss -0.2113
2025-10-05 09:24:40.833464: Pseudo dice [np.float32(0.6382)]
2025-10-05 09:24:40.833635: Epoch time: 46.23 s
2025-10-05 09:24:40.833790: Yayy! New best EMA pseudo Dice: 0.6176999807357788
2025-10-05 09:24:41.911462: 
2025-10-05 09:24:41.911789: Epoch 51
2025-10-05 09:24:41.912007: Current learning rate: 0.00688
2025-10-05 09:25:28.076451: Validation loss did not improve from -0.30034. Patience: 39/50
2025-10-05 09:25:28.076955: train_loss -0.7829
2025-10-05 09:25:28.077179: val_loss -0.1766
2025-10-05 09:25:28.077368: Pseudo dice [np.float32(0.6019)]
2025-10-05 09:25:28.077557: Epoch time: 46.17 s
2025-10-05 09:25:28.734698: 
2025-10-05 09:25:28.735067: Epoch 52
2025-10-05 09:25:28.735300: Current learning rate: 0.00682
2025-10-05 09:26:14.903119: Validation loss did not improve from -0.30034. Patience: 40/50
2025-10-05 09:26:14.904057: train_loss -0.7882
2025-10-05 09:26:14.904422: val_loss -0.2318
2025-10-05 09:26:14.904749: Pseudo dice [np.float32(0.6437)]
2025-10-05 09:26:14.905194: Epoch time: 46.17 s
2025-10-05 09:26:14.905539: Yayy! New best EMA pseudo Dice: 0.6189000010490417
2025-10-05 09:26:15.999761: 
2025-10-05 09:26:16.000046: Epoch 53
2025-10-05 09:26:16.000402: Current learning rate: 0.00675
2025-10-05 09:27:02.165573: Validation loss did not improve from -0.30034. Patience: 41/50
2025-10-05 09:27:02.166135: train_loss -0.7847
2025-10-05 09:27:02.166426: val_loss -0.2376
2025-10-05 09:27:02.166909: Pseudo dice [np.float32(0.6345)]
2025-10-05 09:27:02.167329: Epoch time: 46.17 s
2025-10-05 09:27:02.167569: Yayy! New best EMA pseudo Dice: 0.6205000281333923
2025-10-05 09:27:03.235690: 
2025-10-05 09:27:03.236066: Epoch 54
2025-10-05 09:27:03.236338: Current learning rate: 0.00669
2025-10-05 09:27:49.420034: Validation loss did not improve from -0.30034. Patience: 42/50
2025-10-05 09:27:49.420665: train_loss -0.7842
2025-10-05 09:27:49.420971: val_loss -0.177
2025-10-05 09:27:49.421226: Pseudo dice [np.float32(0.6123)]
2025-10-05 09:27:49.421641: Epoch time: 46.19 s
2025-10-05 09:27:50.492685: 
2025-10-05 09:27:50.492958: Epoch 55
2025-10-05 09:27:50.493169: Current learning rate: 0.00663
2025-10-05 09:28:36.621270: Validation loss did not improve from -0.30034. Patience: 43/50
2025-10-05 09:28:36.622252: train_loss -0.785
2025-10-05 09:28:36.622478: val_loss -0.2416
2025-10-05 09:28:36.622638: Pseudo dice [np.float32(0.6403)]
2025-10-05 09:28:36.622925: Epoch time: 46.13 s
2025-10-05 09:28:36.623286: Yayy! New best EMA pseudo Dice: 0.6216999888420105
2025-10-05 09:28:37.751269: 
2025-10-05 09:28:37.751734: Epoch 56
2025-10-05 09:28:37.752118: Current learning rate: 0.00657
2025-10-05 09:29:23.949898: Validation loss did not improve from -0.30034. Patience: 44/50
2025-10-05 09:29:23.950545: train_loss -0.7964
2025-10-05 09:29:23.950831: val_loss -0.1681
2025-10-05 09:29:23.951062: Pseudo dice [np.float32(0.5911)]
2025-10-05 09:29:23.951224: Epoch time: 46.2 s
2025-10-05 09:29:24.607051: 
2025-10-05 09:29:24.607404: Epoch 57
2025-10-05 09:29:24.607618: Current learning rate: 0.0065
2025-10-05 09:30:10.801774: Validation loss did not improve from -0.30034. Patience: 45/50
2025-10-05 09:30:10.802257: train_loss -0.8018
2025-10-05 09:30:10.802423: val_loss -0.1092
2025-10-05 09:30:10.802624: Pseudo dice [np.float32(0.5733)]
2025-10-05 09:30:10.802804: Epoch time: 46.2 s
2025-10-05 09:30:11.927208: 
2025-10-05 09:30:11.927660: Epoch 58
2025-10-05 09:30:11.927968: Current learning rate: 0.00644
2025-10-05 09:30:58.313768: Validation loss did not improve from -0.30034. Patience: 46/50
2025-10-05 09:30:58.314319: train_loss -0.8116
2025-10-05 09:30:58.314478: val_loss -0.1583
2025-10-05 09:30:58.314607: Pseudo dice [np.float32(0.604)]
2025-10-05 09:30:58.314748: Epoch time: 46.39 s
2025-10-05 09:30:58.962105: 
2025-10-05 09:30:58.962790: Epoch 59
2025-10-05 09:30:58.963251: Current learning rate: 0.00638
2025-10-05 09:31:45.196594: Validation loss did not improve from -0.30034. Patience: 47/50
2025-10-05 09:31:45.197120: train_loss -0.811
2025-10-05 09:31:45.197324: val_loss -0.2077
2025-10-05 09:31:45.197519: Pseudo dice [np.float32(0.6257)]
2025-10-05 09:31:45.197764: Epoch time: 46.24 s
2025-10-05 09:31:46.274546: 
2025-10-05 09:31:46.274984: Epoch 60
2025-10-05 09:31:46.275278: Current learning rate: 0.00631
2025-10-05 09:32:32.435124: Validation loss did not improve from -0.30034. Patience: 48/50
2025-10-05 09:32:32.435849: train_loss -0.8133
2025-10-05 09:32:32.436175: val_loss -0.1398
2025-10-05 09:32:32.436490: Pseudo dice [np.float32(0.62)]
2025-10-05 09:32:32.436820: Epoch time: 46.16 s
2025-10-05 09:32:33.077593: 
2025-10-05 09:32:33.077898: Epoch 61
2025-10-05 09:32:33.078257: Current learning rate: 0.00625
2025-10-05 09:33:19.345824: Validation loss did not improve from -0.30034. Patience: 49/50
2025-10-05 09:33:19.346289: train_loss -0.8062
2025-10-05 09:33:19.346510: val_loss -0.0924
2025-10-05 09:33:19.346788: Pseudo dice [np.float32(0.6019)]
2025-10-05 09:33:19.347053: Epoch time: 46.27 s
2025-10-05 09:33:19.996978: 
2025-10-05 09:33:19.997345: Epoch 62
2025-10-05 09:33:19.997595: Current learning rate: 0.00619
2025-10-05 09:34:06.176762: Validation loss did not improve from -0.30034. Patience: 50/50
2025-10-05 09:34:06.177488: train_loss -0.8123
2025-10-05 09:34:06.177654: val_loss -0.1863
2025-10-05 09:34:06.177786: Pseudo dice [np.float32(0.619)]
2025-10-05 09:34:06.177974: Epoch time: 46.18 s
2025-10-05 09:34:06.827036: 
2025-10-05 09:34:06.827352: Epoch 63
2025-10-05 09:34:06.827564: Current learning rate: 0.00612
2025-10-05 09:34:52.994041: Validation loss did not improve from -0.30034. Patience: 51/50
2025-10-05 09:34:52.994577: train_loss -0.8143
2025-10-05 09:34:52.994747: val_loss -0.1673
2025-10-05 09:34:52.994883: Pseudo dice [np.float32(0.6221)]
2025-10-05 09:34:52.995063: Epoch time: 46.17 s
2025-10-05 09:34:53.652704: 
2025-10-05 09:34:53.653276: Epoch 64
2025-10-05 09:34:53.653688: Current learning rate: 0.00606
2025-10-05 09:35:39.787057: Validation loss did not improve from -0.30034. Patience: 52/50
2025-10-05 09:35:39.787638: train_loss -0.8191
2025-10-05 09:35:39.787791: val_loss -0.0831
2025-10-05 09:35:39.787941: Pseudo dice [np.float32(0.5909)]
2025-10-05 09:35:39.788096: Epoch time: 46.14 s
2025-10-05 09:35:40.867003: 
2025-10-05 09:35:40.867332: Epoch 65
2025-10-05 09:35:40.867516: Current learning rate: 0.006
2025-10-05 09:36:27.196848: Validation loss did not improve from -0.30034. Patience: 53/50
2025-10-05 09:36:27.197364: train_loss -0.824
2025-10-05 09:36:27.197522: val_loss -0.2217
2025-10-05 09:36:27.197652: Pseudo dice [np.float32(0.6455)]
2025-10-05 09:36:27.197810: Epoch time: 46.33 s
2025-10-05 09:36:27.848194: 
2025-10-05 09:36:27.848473: Epoch 66
2025-10-05 09:36:27.848653: Current learning rate: 0.00593
2025-10-05 09:37:14.144404: Validation loss did not improve from -0.30034. Patience: 54/50
2025-10-05 09:37:14.145071: train_loss -0.8212
2025-10-05 09:37:14.145310: val_loss -0.1731
2025-10-05 09:37:14.145458: Pseudo dice [np.float32(0.6354)]
2025-10-05 09:37:14.145625: Epoch time: 46.3 s
2025-10-05 09:37:14.806093: 
2025-10-05 09:37:14.806485: Epoch 67
2025-10-05 09:37:14.806714: Current learning rate: 0.00587
2025-10-05 09:38:01.122769: Validation loss did not improve from -0.30034. Patience: 55/50
2025-10-05 09:38:01.123434: train_loss -0.8277
2025-10-05 09:38:01.123681: val_loss -0.1072
2025-10-05 09:38:01.123851: Pseudo dice [np.float32(0.598)]
2025-10-05 09:38:01.124068: Epoch time: 46.32 s
2025-10-05 09:38:01.785928: 
2025-10-05 09:38:01.786211: Epoch 68
2025-10-05 09:38:01.786542: Current learning rate: 0.00581
2025-10-05 09:38:48.073897: Validation loss did not improve from -0.30034. Patience: 56/50
2025-10-05 09:38:48.074547: train_loss -0.8272
2025-10-05 09:38:48.074767: val_loss -0.1382
2025-10-05 09:38:48.074933: Pseudo dice [np.float32(0.6083)]
2025-10-05 09:38:48.075201: Epoch time: 46.29 s
2025-10-05 09:38:48.730794: 
2025-10-05 09:38:48.731243: Epoch 69
2025-10-05 09:38:48.731618: Current learning rate: 0.00574
2025-10-05 09:39:34.887399: Validation loss did not improve from -0.30034. Patience: 57/50
2025-10-05 09:39:34.887895: train_loss -0.8343
2025-10-05 09:39:34.888118: val_loss -0.1968
2025-10-05 09:39:34.888335: Pseudo dice [np.float32(0.653)]
2025-10-05 09:39:34.888562: Epoch time: 46.16 s
2025-10-05 09:39:36.040612: 
2025-10-05 09:39:36.040902: Epoch 70
2025-10-05 09:39:36.041156: Current learning rate: 0.00568
2025-10-05 09:40:22.258574: Validation loss did not improve from -0.30034. Patience: 58/50
2025-10-05 09:40:22.259214: train_loss -0.8322
2025-10-05 09:40:22.259384: val_loss -0.1056
2025-10-05 09:40:22.259561: Pseudo dice [np.float32(0.5901)]
2025-10-05 09:40:22.259751: Epoch time: 46.22 s
2025-10-05 09:40:22.915201: 
2025-10-05 09:40:22.915534: Epoch 71
2025-10-05 09:40:22.915753: Current learning rate: 0.00562
2025-10-05 09:41:09.013024: Validation loss did not improve from -0.30034. Patience: 59/50
2025-10-05 09:41:09.013643: train_loss -0.8302
2025-10-05 09:41:09.014010: val_loss -0.1655
2025-10-05 09:41:09.014451: Pseudo dice [np.float32(0.6087)]
2025-10-05 09:41:09.014810: Epoch time: 46.1 s
2025-10-05 09:41:09.668440: 
2025-10-05 09:41:09.668795: Epoch 72
2025-10-05 09:41:09.668983: Current learning rate: 0.00555
2025-10-05 09:41:55.779627: Validation loss did not improve from -0.30034. Patience: 60/50
2025-10-05 09:41:55.780174: train_loss -0.8294
2025-10-05 09:41:55.780365: val_loss -0.1222
2025-10-05 09:41:55.780538: Pseudo dice [np.float32(0.6134)]
2025-10-05 09:41:55.780683: Epoch time: 46.11 s
2025-10-05 09:41:56.913481: 
2025-10-05 09:41:56.913840: Epoch 73
2025-10-05 09:41:56.914057: Current learning rate: 0.00549
2025-10-05 09:42:43.045148: Validation loss did not improve from -0.30034. Patience: 61/50
2025-10-05 09:42:43.045649: train_loss -0.8318
2025-10-05 09:42:43.045866: val_loss -0.1376
2025-10-05 09:42:43.046021: Pseudo dice [np.float32(0.6096)]
2025-10-05 09:42:43.046186: Epoch time: 46.13 s
2025-10-05 09:42:43.708249: 
2025-10-05 09:42:43.708531: Epoch 74
2025-10-05 09:42:43.708712: Current learning rate: 0.00542
2025-10-05 09:43:29.830564: Validation loss did not improve from -0.30034. Patience: 62/50
2025-10-05 09:43:29.831379: train_loss -0.8429
2025-10-05 09:43:29.831829: val_loss -0.1502
2025-10-05 09:43:29.832344: Pseudo dice [np.float32(0.6184)]
2025-10-05 09:43:29.832597: Epoch time: 46.12 s
2025-10-05 09:43:30.972616: 
2025-10-05 09:43:30.972893: Epoch 75
2025-10-05 09:43:30.973184: Current learning rate: 0.00536
2025-10-05 09:44:17.127833: Validation loss did not improve from -0.30034. Patience: 63/50
2025-10-05 09:44:17.128409: train_loss -0.8393
2025-10-05 09:44:17.128603: val_loss -0.147
2025-10-05 09:44:17.128739: Pseudo dice [np.float32(0.6138)]
2025-10-05 09:44:17.128892: Epoch time: 46.16 s
2025-10-05 09:44:17.781759: 
2025-10-05 09:44:17.782042: Epoch 76
2025-10-05 09:44:17.782217: Current learning rate: 0.00529
2025-10-05 09:45:03.954416: Validation loss did not improve from -0.30034. Patience: 64/50
2025-10-05 09:45:03.954994: train_loss -0.8377
2025-10-05 09:45:03.955224: val_loss -0.1223
2025-10-05 09:45:03.955436: Pseudo dice [np.float32(0.5919)]
2025-10-05 09:45:03.955639: Epoch time: 46.17 s
2025-10-05 09:45:04.601620: 
2025-10-05 09:45:04.601929: Epoch 77
2025-10-05 09:45:04.602146: Current learning rate: 0.00523
2025-10-05 09:45:50.758453: Validation loss did not improve from -0.30034. Patience: 65/50
2025-10-05 09:45:50.758880: train_loss -0.8344
2025-10-05 09:45:50.759069: val_loss -0.2291
2025-10-05 09:45:50.759246: Pseudo dice [np.float32(0.6598)]
2025-10-05 09:45:50.759426: Epoch time: 46.16 s
2025-10-05 09:45:51.409841: 
2025-10-05 09:45:51.410146: Epoch 78
2025-10-05 09:45:51.410343: Current learning rate: 0.00517
2025-10-05 09:46:37.559911: Validation loss did not improve from -0.30034. Patience: 66/50
2025-10-05 09:46:37.560538: train_loss -0.8419
2025-10-05 09:46:37.560785: val_loss -0.0904
2025-10-05 09:46:37.561051: Pseudo dice [np.float32(0.5938)]
2025-10-05 09:46:37.561348: Epoch time: 46.15 s
2025-10-05 09:46:38.207984: 
2025-10-05 09:46:38.208340: Epoch 79
2025-10-05 09:46:38.208650: Current learning rate: 0.0051
2025-10-05 09:47:24.368695: Validation loss did not improve from -0.30034. Patience: 67/50
2025-10-05 09:47:24.369132: train_loss -0.8459
2025-10-05 09:47:24.369322: val_loss -0.0913
2025-10-05 09:47:24.369473: Pseudo dice [np.float32(0.5929)]
2025-10-05 09:47:24.369645: Epoch time: 46.16 s
2025-10-05 09:47:25.500471: 
2025-10-05 09:47:25.500810: Epoch 80
2025-10-05 09:47:25.501091: Current learning rate: 0.00504
2025-10-05 09:48:11.666846: Validation loss did not improve from -0.30034. Patience: 68/50
2025-10-05 09:48:11.667519: train_loss -0.847
2025-10-05 09:48:11.667741: val_loss -0.1005
2025-10-05 09:48:11.667907: Pseudo dice [np.float32(0.5928)]
2025-10-05 09:48:11.668125: Epoch time: 46.17 s
2025-10-05 09:48:12.334689: 
2025-10-05 09:48:12.335073: Epoch 81
2025-10-05 09:48:12.335377: Current learning rate: 0.00497
2025-10-05 09:48:58.482059: Validation loss did not improve from -0.30034. Patience: 69/50
2025-10-05 09:48:58.482598: train_loss -0.8515
2025-10-05 09:48:58.482803: val_loss -0.1257
2025-10-05 09:48:58.482941: Pseudo dice [np.float32(0.6187)]
2025-10-05 09:48:58.483103: Epoch time: 46.15 s
2025-10-05 09:48:59.144872: 
2025-10-05 09:48:59.145214: Epoch 82
2025-10-05 09:48:59.145414: Current learning rate: 0.00491
2025-10-05 09:49:45.341085: Validation loss did not improve from -0.30034. Patience: 70/50
2025-10-05 09:49:45.341862: train_loss -0.8514
2025-10-05 09:49:45.342256: val_loss -0.1674
2025-10-05 09:49:45.342505: Pseudo dice [np.float32(0.6429)]
2025-10-05 09:49:45.342755: Epoch time: 46.2 s
2025-10-05 09:49:46.001832: 
2025-10-05 09:49:46.002204: Epoch 83
2025-10-05 09:49:46.002436: Current learning rate: 0.00484
2025-10-05 09:50:32.148171: Validation loss did not improve from -0.30034. Patience: 71/50
2025-10-05 09:50:32.148650: train_loss -0.8547
2025-10-05 09:50:32.148874: val_loss -0.095
2025-10-05 09:50:32.149053: Pseudo dice [np.float32(0.6009)]
2025-10-05 09:50:32.149296: Epoch time: 46.15 s
2025-10-05 09:50:32.786749: 
2025-10-05 09:50:32.787133: Epoch 84
2025-10-05 09:50:32.787368: Current learning rate: 0.00478
2025-10-05 09:51:18.994993: Validation loss did not improve from -0.30034. Patience: 72/50
2025-10-05 09:51:18.996071: train_loss -0.8539
2025-10-05 09:51:18.996598: val_loss -0.1817
2025-10-05 09:51:18.996998: Pseudo dice [np.float32(0.6455)]
2025-10-05 09:51:18.997476: Epoch time: 46.21 s
2025-10-05 09:51:20.096905: 
2025-10-05 09:51:20.097417: Epoch 85
2025-10-05 09:51:20.097760: Current learning rate: 0.00471
2025-10-05 09:52:06.336658: Validation loss did not improve from -0.30034. Patience: 73/50
2025-10-05 09:52:06.337332: train_loss -0.8546
2025-10-05 09:52:06.337827: val_loss -0.0754
2025-10-05 09:52:06.338223: Pseudo dice [np.float32(0.5828)]
2025-10-05 09:52:06.338516: Epoch time: 46.24 s
2025-10-05 09:52:06.979671: 
2025-10-05 09:52:06.980108: Epoch 86
2025-10-05 09:52:06.980367: Current learning rate: 0.00465
2025-10-05 09:52:53.237139: Validation loss did not improve from -0.30034. Patience: 74/50
2025-10-05 09:52:53.238100: train_loss -0.8561
2025-10-05 09:52:53.238430: val_loss -0.1224
2025-10-05 09:52:53.238842: Pseudo dice [np.float32(0.6229)]
2025-10-05 09:52:53.239244: Epoch time: 46.26 s
2025-10-05 09:52:53.887243: 
2025-10-05 09:52:53.887865: Epoch 87
2025-10-05 09:52:53.888361: Current learning rate: 0.00458
2025-10-05 09:53:40.051411: Validation loss did not improve from -0.30034. Patience: 75/50
2025-10-05 09:53:40.052029: train_loss -0.86
2025-10-05 09:53:40.052189: val_loss -0.1579
2025-10-05 09:53:40.052404: Pseudo dice [np.float32(0.6504)]
2025-10-05 09:53:40.052564: Epoch time: 46.17 s
2025-10-05 09:53:41.173001: 
2025-10-05 09:53:41.173496: Epoch 88
2025-10-05 09:53:41.173784: Current learning rate: 0.00452
2025-10-05 09:54:27.328036: Validation loss did not improve from -0.30034. Patience: 76/50
2025-10-05 09:54:27.328672: train_loss -0.8625
2025-10-05 09:54:27.328895: val_loss -0.0935
2025-10-05 09:54:27.329120: Pseudo dice [np.float32(0.5892)]
2025-10-05 09:54:27.329345: Epoch time: 46.16 s
2025-10-05 09:54:27.981347: 
2025-10-05 09:54:27.981638: Epoch 89
2025-10-05 09:54:27.981895: Current learning rate: 0.00445
2025-10-05 09:55:14.140099: Validation loss did not improve from -0.30034. Patience: 77/50
2025-10-05 09:55:14.140590: train_loss -0.8633
2025-10-05 09:55:14.140788: val_loss -0.1089
2025-10-05 09:55:14.140945: Pseudo dice [np.float32(0.6258)]
2025-10-05 09:55:14.141186: Epoch time: 46.16 s
2025-10-05 09:55:15.258583: 
2025-10-05 09:55:15.259065: Epoch 90
2025-10-05 09:55:15.259363: Current learning rate: 0.00438
2025-10-05 09:56:01.576408: Validation loss did not improve from -0.30034. Patience: 78/50
2025-10-05 09:56:01.576909: train_loss -0.8638
2025-10-05 09:56:01.577126: val_loss -0.1359
2025-10-05 09:56:01.577303: Pseudo dice [np.float32(0.631)]
2025-10-05 09:56:01.577548: Epoch time: 46.32 s
2025-10-05 09:56:02.220115: 
2025-10-05 09:56:02.220700: Epoch 91
2025-10-05 09:56:02.221104: Current learning rate: 0.00432
2025-10-05 09:56:48.389211: Validation loss did not improve from -0.30034. Patience: 79/50
2025-10-05 09:56:48.389940: train_loss -0.8644
2025-10-05 09:56:48.390334: val_loss -0.1083
2025-10-05 09:56:48.390711: Pseudo dice [np.float32(0.6164)]
2025-10-05 09:56:48.391099: Epoch time: 46.17 s
2025-10-05 09:56:49.032994: 
2025-10-05 09:56:49.033311: Epoch 92
2025-10-05 09:56:49.033502: Current learning rate: 0.00425
2025-10-05 09:57:35.201630: Validation loss did not improve from -0.30034. Patience: 80/50
2025-10-05 09:57:35.202168: train_loss -0.867
2025-10-05 09:57:35.202413: val_loss -0.1778
2025-10-05 09:57:35.202784: Pseudo dice [np.float32(0.6393)]
2025-10-05 09:57:35.202965: Epoch time: 46.17 s
2025-10-05 09:57:35.845973: 
2025-10-05 09:57:35.846272: Epoch 93
2025-10-05 09:57:35.846485: Current learning rate: 0.00419
2025-10-05 09:58:22.101062: Validation loss did not improve from -0.30034. Patience: 81/50
2025-10-05 09:58:22.101503: train_loss -0.866
2025-10-05 09:58:22.101658: val_loss -0.1037
2025-10-05 09:58:22.101815: Pseudo dice [np.float32(0.5972)]
2025-10-05 09:58:22.102057: Epoch time: 46.26 s
2025-10-05 09:58:22.748330: 
2025-10-05 09:58:22.748641: Epoch 94
2025-10-05 09:58:22.748835: Current learning rate: 0.00412
2025-10-05 09:59:08.953946: Validation loss did not improve from -0.30034. Patience: 82/50
2025-10-05 09:59:08.954601: train_loss -0.8684
2025-10-05 09:59:08.954762: val_loss -0.1182
2025-10-05 09:59:08.954939: Pseudo dice [np.float32(0.629)]
2025-10-05 09:59:08.955085: Epoch time: 46.21 s
2025-10-05 09:59:10.060574: 
2025-10-05 09:59:10.060955: Epoch 95
2025-10-05 09:59:10.061141: Current learning rate: 0.00405
2025-10-05 09:59:56.256046: Validation loss did not improve from -0.30034. Patience: 83/50
2025-10-05 09:59:56.256538: train_loss -0.869
2025-10-05 09:59:56.256753: val_loss -0.0033
2025-10-05 09:59:56.257011: Pseudo dice [np.float32(0.568)]
2025-10-05 09:59:56.257199: Epoch time: 46.2 s
2025-10-05 09:59:56.890426: 
2025-10-05 09:59:56.890719: Epoch 96
2025-10-05 09:59:56.890881: Current learning rate: 0.00399
2025-10-05 10:00:43.112515: Validation loss did not improve from -0.30034. Patience: 84/50
2025-10-05 10:00:43.113066: train_loss -0.8719
2025-10-05 10:00:43.113323: val_loss -0.103
2025-10-05 10:00:43.113598: Pseudo dice [np.float32(0.6138)]
2025-10-05 10:00:43.113868: Epoch time: 46.22 s
2025-10-05 10:00:43.760028: 
2025-10-05 10:00:43.760371: Epoch 97
2025-10-05 10:00:43.760736: Current learning rate: 0.00392
2025-10-05 10:01:29.967235: Validation loss did not improve from -0.30034. Patience: 85/50
2025-10-05 10:01:29.967799: train_loss -0.875
2025-10-05 10:01:29.968054: val_loss -0.1272
2025-10-05 10:01:29.968350: Pseudo dice [np.float32(0.6308)]
2025-10-05 10:01:29.968609: Epoch time: 46.21 s
2025-10-05 10:01:30.615471: 
2025-10-05 10:01:30.615773: Epoch 98
2025-10-05 10:01:30.616115: Current learning rate: 0.00385
2025-10-05 10:02:16.810232: Validation loss did not improve from -0.30034. Patience: 86/50
2025-10-05 10:02:16.810775: train_loss -0.8726
2025-10-05 10:02:16.810966: val_loss -0.0889
2025-10-05 10:02:16.811126: Pseudo dice [np.float32(0.5996)]
2025-10-05 10:02:16.811340: Epoch time: 46.2 s
2025-10-05 10:02:17.456201: 
2025-10-05 10:02:17.456552: Epoch 99
2025-10-05 10:02:17.456769: Current learning rate: 0.00379
2025-10-05 10:03:03.709140: Validation loss did not improve from -0.30034. Patience: 87/50
2025-10-05 10:03:03.709860: train_loss -0.8723
2025-10-05 10:03:03.710267: val_loss -0.0709
2025-10-05 10:03:03.710689: Pseudo dice [np.float32(0.6172)]
2025-10-05 10:03:03.711079: Epoch time: 46.25 s
2025-10-05 10:03:04.833563: 
2025-10-05 10:03:04.833997: Epoch 100
2025-10-05 10:03:04.834292: Current learning rate: 0.00372
2025-10-05 10:03:51.036270: Validation loss did not improve from -0.30034. Patience: 88/50
2025-10-05 10:03:51.036795: train_loss -0.8743
2025-10-05 10:03:51.037024: val_loss -0.1779
2025-10-05 10:03:51.037198: Pseudo dice [np.float32(0.6533)]
2025-10-05 10:03:51.037415: Epoch time: 46.2 s
2025-10-05 10:03:51.680681: 
2025-10-05 10:03:51.681105: Epoch 101
2025-10-05 10:03:51.681336: Current learning rate: 0.00365
2025-10-05 10:04:37.894408: Validation loss did not improve from -0.30034. Patience: 89/50
2025-10-05 10:04:37.894905: train_loss -0.8761
2025-10-05 10:04:37.895077: val_loss -0.0631
2025-10-05 10:04:37.895217: Pseudo dice [np.float32(0.5996)]
2025-10-05 10:04:37.895454: Epoch time: 46.21 s
2025-10-05 10:04:38.544549: 
2025-10-05 10:04:38.544905: Epoch 102
2025-10-05 10:04:38.545146: Current learning rate: 0.00359
2025-10-05 10:05:24.705275: Validation loss did not improve from -0.30034. Patience: 90/50
2025-10-05 10:05:24.705829: train_loss -0.8801
2025-10-05 10:05:24.705991: val_loss -0.1152
2025-10-05 10:05:24.706120: Pseudo dice [np.float32(0.6284)]
2025-10-05 10:05:24.706300: Epoch time: 46.16 s
2025-10-05 10:05:25.346370: 
2025-10-05 10:05:25.346613: Epoch 103
2025-10-05 10:05:25.346813: Current learning rate: 0.00352
2025-10-05 10:06:11.585722: Validation loss did not improve from -0.30034. Patience: 91/50
2025-10-05 10:06:11.586605: train_loss -0.8793
2025-10-05 10:06:11.587240: val_loss -0.0408
2025-10-05 10:06:11.587878: Pseudo dice [np.float32(0.5793)]
2025-10-05 10:06:11.588605: Epoch time: 46.24 s
2025-10-05 10:06:12.665001: 
2025-10-05 10:06:12.665454: Epoch 104
2025-10-05 10:06:12.665862: Current learning rate: 0.00345
2025-10-05 10:06:58.939613: Validation loss did not improve from -0.30034. Patience: 92/50
2025-10-05 10:06:58.940282: train_loss -0.8824
2025-10-05 10:06:58.940571: val_loss -0.1215
2025-10-05 10:06:58.940828: Pseudo dice [np.float32(0.6458)]
2025-10-05 10:06:58.941103: Epoch time: 46.28 s
2025-10-05 10:07:00.049066: 
2025-10-05 10:07:00.049371: Epoch 105
2025-10-05 10:07:00.049563: Current learning rate: 0.00338
2025-10-05 10:07:46.279577: Validation loss did not improve from -0.30034. Patience: 93/50
2025-10-05 10:07:46.280347: train_loss -0.8812
2025-10-05 10:07:46.280813: val_loss -0.1026
2025-10-05 10:07:46.281314: Pseudo dice [np.float32(0.6153)]
2025-10-05 10:07:46.281801: Epoch time: 46.23 s
2025-10-05 10:07:46.945766: 
2025-10-05 10:07:46.946238: Epoch 106
2025-10-05 10:07:46.946660: Current learning rate: 0.00332
2025-10-05 10:08:33.212022: Validation loss did not improve from -0.30034. Patience: 94/50
2025-10-05 10:08:33.212745: train_loss -0.8801
2025-10-05 10:08:33.212961: val_loss -0.0608
2025-10-05 10:08:33.213113: Pseudo dice [np.float32(0.6032)]
2025-10-05 10:08:33.213290: Epoch time: 46.27 s
2025-10-05 10:08:33.862520: 
2025-10-05 10:08:33.863109: Epoch 107
2025-10-05 10:08:33.863556: Current learning rate: 0.00325
2025-10-05 10:09:20.249654: Validation loss did not improve from -0.30034. Patience: 95/50
2025-10-05 10:09:20.250387: train_loss -0.877
2025-10-05 10:09:20.250789: val_loss -0.0629
2025-10-05 10:09:20.251119: Pseudo dice [np.float32(0.5979)]
2025-10-05 10:09:20.251454: Epoch time: 46.39 s
2025-10-05 10:09:20.911367: 
2025-10-05 10:09:20.911951: Epoch 108
2025-10-05 10:09:20.912435: Current learning rate: 0.00318
2025-10-05 10:10:07.289751: Validation loss did not improve from -0.30034. Patience: 96/50
2025-10-05 10:10:07.290302: train_loss -0.8777
2025-10-05 10:10:07.290472: val_loss -0.1187
2025-10-05 10:10:07.290668: Pseudo dice [np.float32(0.6419)]
2025-10-05 10:10:07.290869: Epoch time: 46.38 s
2025-10-05 10:10:07.935582: 
2025-10-05 10:10:07.935976: Epoch 109
2025-10-05 10:10:07.936227: Current learning rate: 0.00311
2025-10-05 10:10:54.358752: Validation loss did not improve from -0.30034. Patience: 97/50
2025-10-05 10:10:54.359226: train_loss -0.8822
2025-10-05 10:10:54.359421: val_loss -0.1397
2025-10-05 10:10:54.359597: Pseudo dice [np.float32(0.6405)]
2025-10-05 10:10:54.359792: Epoch time: 46.42 s
2025-10-05 10:10:55.516039: 
2025-10-05 10:10:55.516470: Epoch 110
2025-10-05 10:10:55.517000: Current learning rate: 0.00304
2025-10-05 10:11:41.854084: Validation loss did not improve from -0.30034. Patience: 98/50
2025-10-05 10:11:41.854577: train_loss -0.8816
2025-10-05 10:11:41.854753: val_loss -0.1041
2025-10-05 10:11:41.854916: Pseudo dice [np.float32(0.6282)]
2025-10-05 10:11:41.855088: Epoch time: 46.34 s
2025-10-05 10:11:42.499589: 
2025-10-05 10:11:42.499968: Epoch 111
2025-10-05 10:11:42.500158: Current learning rate: 0.00297
2025-10-05 10:12:28.801852: Validation loss did not improve from -0.30034. Patience: 99/50
2025-10-05 10:12:28.802468: train_loss -0.8851
2025-10-05 10:12:28.802818: val_loss -0.1038
2025-10-05 10:12:28.803106: Pseudo dice [np.float32(0.6156)]
2025-10-05 10:12:28.803494: Epoch time: 46.3 s
2025-10-05 10:12:29.446066: 
2025-10-05 10:12:29.446367: Epoch 112
2025-10-05 10:12:29.446549: Current learning rate: 0.00291
2025-10-05 10:13:15.588009: Validation loss did not improve from -0.30034. Patience: 100/50
2025-10-05 10:13:15.589087: train_loss -0.8859
2025-10-05 10:13:15.589407: val_loss -0.1127
2025-10-05 10:13:15.589691: Pseudo dice [np.float32(0.6108)]
2025-10-05 10:13:15.589998: Epoch time: 46.14 s
2025-10-05 10:13:16.237582: 
2025-10-05 10:13:16.238050: Epoch 113
2025-10-05 10:13:16.238581: Current learning rate: 0.00284
2025-10-05 10:14:02.370319: Validation loss did not improve from -0.30034. Patience: 101/50
2025-10-05 10:14:02.370995: train_loss -0.8878
2025-10-05 10:14:02.371512: val_loss -0.1336
2025-10-05 10:14:02.371896: Pseudo dice [np.float32(0.6495)]
2025-10-05 10:14:02.372608: Epoch time: 46.13 s
2025-10-05 10:14:03.011133: 
2025-10-05 10:14:03.011396: Epoch 114
2025-10-05 10:14:03.011551: Current learning rate: 0.00277
2025-10-05 10:14:49.215217: Validation loss did not improve from -0.30034. Patience: 102/50
2025-10-05 10:14:49.215729: train_loss -0.886
2025-10-05 10:14:49.215915: val_loss -0.1045
2025-10-05 10:14:49.216043: Pseudo dice [np.float32(0.6383)]
2025-10-05 10:14:49.216191: Epoch time: 46.21 s
2025-10-05 10:14:49.653324: Yayy! New best EMA pseudo Dice: 0.623199999332428
2025-10-05 10:14:50.728928: 
2025-10-05 10:14:50.729306: Epoch 115
2025-10-05 10:14:50.729524: Current learning rate: 0.0027
2025-10-05 10:15:36.836707: Validation loss did not improve from -0.30034. Patience: 103/50
2025-10-05 10:15:36.837095: train_loss -0.8891
2025-10-05 10:15:36.837275: val_loss -0.0537
2025-10-05 10:15:36.837481: Pseudo dice [np.float32(0.6224)]
2025-10-05 10:15:36.837653: Epoch time: 46.11 s
2025-10-05 10:15:37.487788: 
2025-10-05 10:15:37.488100: Epoch 116
2025-10-05 10:15:37.488314: Current learning rate: 0.00263
2025-10-05 10:16:23.620448: Validation loss did not improve from -0.30034. Patience: 104/50
2025-10-05 10:16:23.621075: train_loss -0.8917
2025-10-05 10:16:23.621292: val_loss -0.1174
2025-10-05 10:16:23.621479: Pseudo dice [np.float32(0.6412)]
2025-10-05 10:16:23.621648: Epoch time: 46.13 s
2025-10-05 10:16:23.621818: Yayy! New best EMA pseudo Dice: 0.625
2025-10-05 10:16:24.882602: 
2025-10-05 10:16:24.883175: Epoch 117
2025-10-05 10:16:24.883627: Current learning rate: 0.00256
2025-10-05 10:17:11.050301: Validation loss did not improve from -0.30034. Patience: 105/50
2025-10-05 10:17:11.050888: train_loss -0.8903
2025-10-05 10:17:11.051301: val_loss -0.1226
2025-10-05 10:17:11.051630: Pseudo dice [np.float32(0.6247)]
2025-10-05 10:17:11.051934: Epoch time: 46.17 s
2025-10-05 10:17:11.713778: 
2025-10-05 10:17:11.714125: Epoch 118
2025-10-05 10:17:11.714343: Current learning rate: 0.00249
2025-10-05 10:17:57.827926: Validation loss did not improve from -0.30034. Patience: 106/50
2025-10-05 10:17:57.828988: train_loss -0.892
2025-10-05 10:17:57.829483: val_loss -0.076
2025-10-05 10:17:57.829856: Pseudo dice [np.float32(0.6212)]
2025-10-05 10:17:57.830214: Epoch time: 46.12 s
2025-10-05 10:17:58.956638: 
2025-10-05 10:17:58.957011: Epoch 119
2025-10-05 10:17:58.957218: Current learning rate: 0.00242
2025-10-05 10:18:45.091648: Validation loss did not improve from -0.30034. Patience: 107/50
2025-10-05 10:18:45.092124: train_loss -0.8937
2025-10-05 10:18:45.092314: val_loss -0.1329
2025-10-05 10:18:45.092468: Pseudo dice [np.float32(0.6429)]
2025-10-05 10:18:45.092684: Epoch time: 46.14 s
2025-10-05 10:18:45.533458: Yayy! New best EMA pseudo Dice: 0.6263999938964844
2025-10-05 10:18:46.635217: 
2025-10-05 10:18:46.635612: Epoch 120
2025-10-05 10:18:46.635818: Current learning rate: 0.00235
2025-10-05 10:19:32.931642: Validation loss did not improve from -0.30034. Patience: 108/50
2025-10-05 10:19:32.932394: train_loss -0.8919
2025-10-05 10:19:32.932667: val_loss 0.0029
2025-10-05 10:19:32.932932: Pseudo dice [np.float32(0.5806)]
2025-10-05 10:19:32.933109: Epoch time: 46.3 s
2025-10-05 10:19:33.587730: 
2025-10-05 10:19:33.588099: Epoch 121
2025-10-05 10:19:33.588296: Current learning rate: 0.00228
2025-10-05 10:20:19.880013: Validation loss did not improve from -0.30034. Patience: 109/50
2025-10-05 10:20:19.880518: train_loss -0.8935
2025-10-05 10:20:19.880719: val_loss 0.0115
2025-10-05 10:20:19.880883: Pseudo dice [np.float32(0.5813)]
2025-10-05 10:20:19.881046: Epoch time: 46.29 s
2025-10-05 10:20:20.544239: 
2025-10-05 10:20:20.544545: Epoch 122
2025-10-05 10:20:20.544730: Current learning rate: 0.00221
2025-10-05 10:21:06.905208: Validation loss did not improve from -0.30034. Patience: 110/50
2025-10-05 10:21:06.905902: train_loss -0.8934
2025-10-05 10:21:06.906121: val_loss -0.0878
2025-10-05 10:21:06.906301: Pseudo dice [np.float32(0.62)]
2025-10-05 10:21:06.906472: Epoch time: 46.36 s
2025-10-05 10:21:07.566048: 
2025-10-05 10:21:07.566369: Epoch 123
2025-10-05 10:21:07.566584: Current learning rate: 0.00214
2025-10-05 10:21:53.825208: Validation loss did not improve from -0.30034. Patience: 111/50
2025-10-05 10:21:53.825885: train_loss -0.8972
2025-10-05 10:21:53.826345: val_loss -0.0199
2025-10-05 10:21:53.826735: Pseudo dice [np.float32(0.606)]
2025-10-05 10:21:53.827097: Epoch time: 46.26 s
2025-10-05 10:21:54.492484: 
2025-10-05 10:21:54.492747: Epoch 124
2025-10-05 10:21:54.492948: Current learning rate: 0.00207
2025-10-05 10:22:40.705755: Validation loss did not improve from -0.30034. Patience: 112/50
2025-10-05 10:22:40.706267: train_loss -0.8969
2025-10-05 10:22:40.706417: val_loss -0.0707
2025-10-05 10:22:40.706557: Pseudo dice [np.float32(0.6211)]
2025-10-05 10:22:40.706695: Epoch time: 46.21 s
2025-10-05 10:22:41.797113: 
2025-10-05 10:22:41.797504: Epoch 125
2025-10-05 10:22:41.797764: Current learning rate: 0.00199
2025-10-05 10:23:27.982744: Validation loss did not improve from -0.30034. Patience: 113/50
2025-10-05 10:23:27.983136: train_loss -0.8958
2025-10-05 10:23:27.983293: val_loss -0.0496
2025-10-05 10:23:27.983452: Pseudo dice [np.float32(0.635)]
2025-10-05 10:23:27.983597: Epoch time: 46.19 s
2025-10-05 10:23:28.637954: 
2025-10-05 10:23:28.638276: Epoch 126
2025-10-05 10:23:28.638477: Current learning rate: 0.00192
2025-10-05 10:24:14.888994: Validation loss did not improve from -0.30034. Patience: 114/50
2025-10-05 10:24:14.889767: train_loss -0.8984
2025-10-05 10:24:14.890018: val_loss -0.0749
2025-10-05 10:24:14.890398: Pseudo dice [np.float32(0.6193)]
2025-10-05 10:24:14.890647: Epoch time: 46.25 s
2025-10-05 10:24:15.557926: 
2025-10-05 10:24:15.558299: Epoch 127
2025-10-05 10:24:15.558618: Current learning rate: 0.00185
2025-10-05 10:25:01.817198: Validation loss did not improve from -0.30034. Patience: 115/50
2025-10-05 10:25:01.817731: train_loss -0.8972
2025-10-05 10:25:01.818032: val_loss 0.0044
2025-10-05 10:25:01.818315: Pseudo dice [np.float32(0.5825)]
2025-10-05 10:25:01.818600: Epoch time: 46.26 s
2025-10-05 10:25:02.475843: 
2025-10-05 10:25:02.476209: Epoch 128
2025-10-05 10:25:02.476542: Current learning rate: 0.00178
2025-10-05 10:25:48.851951: Validation loss did not improve from -0.30034. Patience: 116/50
2025-10-05 10:25:48.852898: train_loss -0.8994
2025-10-05 10:25:48.853100: val_loss -0.1114
2025-10-05 10:25:48.853333: Pseudo dice [np.float32(0.6331)]
2025-10-05 10:25:48.853559: Epoch time: 46.38 s
2025-10-05 10:25:49.498995: 
2025-10-05 10:25:49.499397: Epoch 129
2025-10-05 10:25:49.499622: Current learning rate: 0.0017
2025-10-05 10:26:35.693687: Validation loss did not improve from -0.30034. Patience: 117/50
2025-10-05 10:26:35.694116: train_loss -0.8999
2025-10-05 10:26:35.694284: val_loss -0.0531
2025-10-05 10:26:35.694481: Pseudo dice [np.float32(0.604)]
2025-10-05 10:26:35.694636: Epoch time: 46.2 s
2025-10-05 10:26:36.807362: 
2025-10-05 10:26:36.807737: Epoch 130
2025-10-05 10:26:36.808001: Current learning rate: 0.00163
2025-10-05 10:27:22.974302: Validation loss did not improve from -0.30034. Patience: 118/50
2025-10-05 10:27:22.974858: train_loss -0.9009
2025-10-05 10:27:22.975040: val_loss -0.0885
2025-10-05 10:27:22.975189: Pseudo dice [np.float32(0.6079)]
2025-10-05 10:27:22.975327: Epoch time: 46.17 s
2025-10-05 10:27:23.617970: 
2025-10-05 10:27:23.618284: Epoch 131
2025-10-05 10:27:23.618496: Current learning rate: 0.00156
2025-10-05 10:28:09.824526: Validation loss did not improve from -0.30034. Patience: 119/50
2025-10-05 10:28:09.824908: train_loss -0.9009
2025-10-05 10:28:09.825248: val_loss -0.0115
2025-10-05 10:28:09.825383: Pseudo dice [np.float32(0.6067)]
2025-10-05 10:28:09.825531: Epoch time: 46.21 s
2025-10-05 10:28:10.461940: 
2025-10-05 10:28:10.462222: Epoch 132
2025-10-05 10:28:10.462420: Current learning rate: 0.00148
2025-10-05 10:28:56.746783: Validation loss did not improve from -0.30034. Patience: 120/50
2025-10-05 10:28:56.747570: train_loss -0.9039
2025-10-05 10:28:56.747824: val_loss -0.0029
2025-10-05 10:28:56.748048: Pseudo dice [np.float32(0.6076)]
2025-10-05 10:28:56.748346: Epoch time: 46.29 s
2025-10-05 10:28:57.402609: 
2025-10-05 10:28:57.403197: Epoch 133
2025-10-05 10:28:57.403480: Current learning rate: 0.00141
2025-10-05 10:29:43.627227: Validation loss did not improve from -0.30034. Patience: 121/50
2025-10-05 10:29:43.627688: train_loss -0.9013
2025-10-05 10:29:43.627916: val_loss -0.0399
2025-10-05 10:29:43.628067: Pseudo dice [np.float32(0.5949)]
2025-10-05 10:29:43.628217: Epoch time: 46.23 s
2025-10-05 10:29:44.765005: 
2025-10-05 10:29:44.765487: Epoch 134
2025-10-05 10:29:44.765833: Current learning rate: 0.00133
2025-10-05 10:30:31.064062: Validation loss did not improve from -0.30034. Patience: 122/50
2025-10-05 10:30:31.065151: train_loss -0.9023
2025-10-05 10:30:31.065677: val_loss -0.0648
2025-10-05 10:30:31.066113: Pseudo dice [np.float32(0.621)]
2025-10-05 10:30:31.066574: Epoch time: 46.3 s
2025-10-05 10:30:32.277008: 
2025-10-05 10:30:32.277359: Epoch 135
2025-10-05 10:30:32.277597: Current learning rate: 0.00126
2025-10-05 10:31:18.531813: Validation loss did not improve from -0.30034. Patience: 123/50
2025-10-05 10:31:18.532624: train_loss -0.9044
2025-10-05 10:31:18.533111: val_loss -0.0209
2025-10-05 10:31:18.533541: Pseudo dice [np.float32(0.6031)]
2025-10-05 10:31:18.533969: Epoch time: 46.26 s
2025-10-05 10:31:19.189855: 
2025-10-05 10:31:19.190450: Epoch 136
2025-10-05 10:31:19.190851: Current learning rate: 0.00118
2025-10-05 10:32:05.503319: Validation loss did not improve from -0.30034. Patience: 124/50
2025-10-05 10:32:05.504436: train_loss -0.9044
2025-10-05 10:32:05.504905: val_loss 0.0135
2025-10-05 10:32:05.505291: Pseudo dice [np.float32(0.6094)]
2025-10-05 10:32:05.505672: Epoch time: 46.32 s
2025-10-05 10:32:06.168519: 
2025-10-05 10:32:06.168879: Epoch 137
2025-10-05 10:32:06.169244: Current learning rate: 0.00111
2025-10-05 10:32:52.453156: Validation loss did not improve from -0.30034. Patience: 125/50
2025-10-05 10:32:52.453643: train_loss -0.9035
2025-10-05 10:32:52.453903: val_loss -0.0656
2025-10-05 10:32:52.454176: Pseudo dice [np.float32(0.6144)]
2025-10-05 10:32:52.454451: Epoch time: 46.29 s
2025-10-05 10:32:53.110088: 
2025-10-05 10:32:53.110528: Epoch 138
2025-10-05 10:32:53.110868: Current learning rate: 0.00103
2025-10-05 10:33:39.269151: Validation loss did not improve from -0.30034. Patience: 126/50
2025-10-05 10:33:39.269779: train_loss -0.9045
2025-10-05 10:33:39.269966: val_loss 0.0184
2025-10-05 10:33:39.270114: Pseudo dice [np.float32(0.5927)]
2025-10-05 10:33:39.270280: Epoch time: 46.16 s
2025-10-05 10:33:39.924156: 
2025-10-05 10:33:39.924545: Epoch 139
2025-10-05 10:33:39.924767: Current learning rate: 0.00095
2025-10-05 10:34:26.062753: Validation loss did not improve from -0.30034. Patience: 127/50
2025-10-05 10:34:26.063122: train_loss -0.9044
2025-10-05 10:34:26.063294: val_loss -0.0665
2025-10-05 10:34:26.063427: Pseudo dice [np.float32(0.6243)]
2025-10-05 10:34:26.063640: Epoch time: 46.14 s
2025-10-05 10:34:27.159113: 
2025-10-05 10:34:27.159425: Epoch 140
2025-10-05 10:34:27.159638: Current learning rate: 0.00087
2025-10-05 10:35:13.397855: Validation loss did not improve from -0.30034. Patience: 128/50
2025-10-05 10:35:13.399108: train_loss -0.9054
2025-10-05 10:35:13.399769: val_loss -0.0228
2025-10-05 10:35:13.400341: Pseudo dice [np.float32(0.6304)]
2025-10-05 10:35:13.400822: Epoch time: 46.24 s
2025-10-05 10:35:14.053549: 
2025-10-05 10:35:14.053967: Epoch 141
2025-10-05 10:35:14.054303: Current learning rate: 0.00079
2025-10-05 10:36:00.319305: Validation loss did not improve from -0.30034. Patience: 129/50
2025-10-05 10:36:00.319852: train_loss -0.9085
2025-10-05 10:36:00.320083: val_loss -0.0294
2025-10-05 10:36:00.320251: Pseudo dice [np.float32(0.6161)]
2025-10-05 10:36:00.320420: Epoch time: 46.27 s
2025-10-05 10:36:00.979290: 
2025-10-05 10:36:00.979632: Epoch 142
2025-10-05 10:36:00.979865: Current learning rate: 0.00071
2025-10-05 10:36:47.148112: Validation loss did not improve from -0.30034. Patience: 130/50
2025-10-05 10:36:47.148729: train_loss -0.9058
2025-10-05 10:36:47.148889: val_loss -0.0487
2025-10-05 10:36:47.149044: Pseudo dice [np.float32(0.6173)]
2025-10-05 10:36:47.149219: Epoch time: 46.17 s
2025-10-05 10:36:47.806356: 
2025-10-05 10:36:47.806731: Epoch 143
2025-10-05 10:36:47.806913: Current learning rate: 0.00063
2025-10-05 10:37:33.972449: Validation loss did not improve from -0.30034. Patience: 131/50
2025-10-05 10:37:33.972978: train_loss -0.9096
2025-10-05 10:37:33.973228: val_loss -0.0735
2025-10-05 10:37:33.973402: Pseudo dice [np.float32(0.6187)]
2025-10-05 10:37:33.973592: Epoch time: 46.17 s
2025-10-05 10:37:34.642948: 
2025-10-05 10:37:34.643241: Epoch 144
2025-10-05 10:37:34.643446: Current learning rate: 0.00055
2025-10-05 10:38:20.785187: Validation loss did not improve from -0.30034. Patience: 132/50
2025-10-05 10:38:20.785960: train_loss -0.9099
2025-10-05 10:38:20.786313: val_loss 0.008
2025-10-05 10:38:20.786608: Pseudo dice [np.float32(0.597)]
2025-10-05 10:38:20.786842: Epoch time: 46.14 s
2025-10-05 10:38:21.872984: 
2025-10-05 10:38:21.873332: Epoch 145
2025-10-05 10:38:21.873555: Current learning rate: 0.00047
2025-10-05 10:39:08.130085: Validation loss did not improve from -0.30034. Patience: 133/50
2025-10-05 10:39:08.130729: train_loss -0.9099
2025-10-05 10:39:08.131042: val_loss -0.0626
2025-10-05 10:39:08.131356: Pseudo dice [np.float32(0.62)]
2025-10-05 10:39:08.131663: Epoch time: 46.26 s
2025-10-05 10:39:08.794489: 
2025-10-05 10:39:08.794876: Epoch 146
2025-10-05 10:39:08.795214: Current learning rate: 0.00038
2025-10-05 10:39:55.014600: Validation loss did not improve from -0.30034. Patience: 134/50
2025-10-05 10:39:55.015639: train_loss -0.9089
2025-10-05 10:39:55.016140: val_loss -0.0084
2025-10-05 10:39:55.016700: Pseudo dice [np.float32(0.6198)]
2025-10-05 10:39:55.017252: Epoch time: 46.22 s
2025-10-05 10:39:55.667709: 
2025-10-05 10:39:55.668034: Epoch 147
2025-10-05 10:39:55.668321: Current learning rate: 0.0003
2025-10-05 10:40:41.840179: Validation loss did not improve from -0.30034. Patience: 135/50
2025-10-05 10:40:41.840612: train_loss -0.9103
2025-10-05 10:40:41.840822: val_loss -0.0634
2025-10-05 10:40:41.841002: Pseudo dice [np.float32(0.6222)]
2025-10-05 10:40:41.841248: Epoch time: 46.17 s
2025-10-05 10:40:42.498370: 
2025-10-05 10:40:42.498966: Epoch 148
2025-10-05 10:40:42.499569: Current learning rate: 0.00021
2025-10-05 10:41:28.653335: Validation loss did not improve from -0.30034. Patience: 136/50
2025-10-05 10:41:28.653854: train_loss -0.9117
2025-10-05 10:41:28.654102: val_loss -0.0234
2025-10-05 10:41:28.654293: Pseudo dice [np.float32(0.6143)]
2025-10-05 10:41:28.654475: Epoch time: 46.16 s
2025-10-05 10:41:29.304063: 
2025-10-05 10:41:29.304426: Epoch 149
2025-10-05 10:41:29.304613: Current learning rate: 0.00011
2025-10-05 10:42:15.425036: Validation loss did not improve from -0.30034. Patience: 137/50
2025-10-05 10:42:15.425539: train_loss -0.909
2025-10-05 10:42:15.425773: val_loss -0.0144
2025-10-05 10:42:15.425928: Pseudo dice [np.float32(0.606)]
2025-10-05 10:42:15.426150: Epoch time: 46.12 s
2025-10-05 10:42:17.001539: Training done.
2025-10-05 10:42:17.015406: Using splits from existing split file: /nfs/erelab001/shared/Computational_Group/Naravich/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset310_nnInteractive_Calcium_OCT_CrossValidation/splits_final_40.json
2025-10-05 10:42:17.016048: The split file contains 5 splits.
2025-10-05 10:42:17.017010: Desired fold for training: 0
2025-10-05 10:42:17.017532: This split has 3 training and 5 validation cases.
2025-10-05 10:42:17.018631: predicting 101-045
2025-10-05 10:42:17.022281: 101-045, shape torch.Size([1, 375, 498, 498]), rank 0
2025-10-05 10:42:58.859897: predicting 106-002
2025-10-05 10:42:58.870128: 106-002, shape torch.Size([1, 539, 498, 498]), rank 0
2025-10-05 10:43:48.604567: predicting 701-013
2025-10-05 10:43:48.617970: 701-013, shape torch.Size([1, 375, 498, 498]), rank 0
2025-10-05 10:44:23.786116: predicting 704-003
2025-10-05 10:44:23.796282: 704-003, shape torch.Size([1, 375, 498, 498]), rank 0
2025-10-05 10:44:58.887983: predicting 706-005
2025-10-05 10:44:58.898603: 706-005, shape torch.Size([1, 375, 498, 498]), rank 0
2025-10-05 10:45:47.294842: Validation complete
2025-10-05 10:45:47.295105: Mean Validation Dice:  0.6213812943002163
Finished training fold 0 saving to /nfs/erelab001/shared/Computational_Group/Naravich/datasets/nnUNet_Datasets/nnUNet_results/Dataset310_nnInteractive_Calcium_OCT_CrossValidation/nnUNetTrainerScaleAnalysis40__nnUNetPlans__3d_32x160x128_b10/fold_0_No_Pretrained
