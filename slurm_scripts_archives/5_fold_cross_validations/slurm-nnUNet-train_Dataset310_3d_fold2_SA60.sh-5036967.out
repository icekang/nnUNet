/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/bin/python
Starting training with CONFIG=3d_32x160x128_b10, DATASET_ID=310, TRAINER=nnUNetTrainerScaleAnalysis60
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/nnunetv2/training/nnUNetTrainer/nnUNetTrainer.py:169: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  self.grad_scaler = GradScaler() if self.device.type == 'cuda' else None

############################
INFO: You are using the old nnU-Net default plans. We have updated our recommendations. Please consider using those instead! Read more here: https://github.com/MIC-DKFZ/nnUNet/blob/master/documentation/resenc_presets.md
############################

Using device: cuda:0

#######################################################################
Please cite the following paper when using nnU-Net:
Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.
#######################################################################

2025-10-05 18:59:41.763809: do_dummy_2d_data_aug: True
2025-10-05 18:59:41.764122: Using splits from existing split file: /nfs/erelab001/shared/Computational_Group/Naravich/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset310_nnInteractive_Calcium_OCT_CrossValidation/splits_final_60.json
2025-10-05 18:59:41.764405: The split file contains 5 splits.
2025-10-05 18:59:41.764496: Desired fold for training: 2
2025-10-05 18:59:41.764595: This split has 4 training and 4 validation cases.
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/fft_conv_pytorch/fft_conv.py:139: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:306.)
  output = output[crop_slices].contiguous()
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/fft_conv_pytorch/fft_conv.py:139: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:306.)
  output = output[crop_slices].contiguous()
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/fft_conv_pytorch/fft_conv.py:139: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:306.)
  output = output[crop_slices].contiguous()
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/fft_conv_pytorch/fft_conv.py:139: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:306.)
  output = output[crop_slices].contiguous()
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/fft_conv_pytorch/fft_conv.py:139: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:306.)
  output = output[crop_slices].contiguous()
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/fft_conv_pytorch/fft_conv.py:139: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:306.)
  output = output[crop_slices].contiguous()
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/fft_conv_pytorch/fft_conv.py:139: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:306.)
  output = output[crop_slices].contiguous()
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/fft_conv_pytorch/fft_conv.py:139: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:306.)
  output = output[crop_slices].contiguous()
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/fft_conv_pytorch/fft_conv.py:139: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:306.)
  output = output[crop_slices].contiguous()
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/fft_conv_pytorch/fft_conv.py:139: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:306.)
  output = output[crop_slices].contiguous()
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/fft_conv_pytorch/fft_conv.py:139: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:306.)
  output = output[crop_slices].contiguous()
using pin_memory on device 0
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/fft_conv_pytorch/fft_conv.py:139: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:306.)
  output = output[crop_slices].contiguous()
using pin_memory on device 0
2025-10-05 18:59:46.970957: Using torch.compile...

This is the configuration used by this training:
Configuration name: 3d_32x160x128_b10
 {'data_identifier': 'nnUNetPlans_3d_fullres', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 10, 'patch_size': [32, 160, 128], 'median_image_size_in_voxels': [375.0, 498.0, 498.0], 'spacing': [1.0, 1.0, 1.0], 'normalization_schemes': ['ZScoreNormalization'], 'use_mask_for_norm': [False], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'architecture': {'network_class_name': 'dynamic_network_architectures.architectures.unet.PlainConvUNet', 'arch_kwargs': {'n_stages': 6, 'features_per_stage': [32, 64, 128, 256, 320, 320], 'conv_op': 'torch.nn.modules.conv.Conv3d', 'kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]], 'strides': [[1, 1, 1], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [1, 2, 2]], 'n_conv_per_stage': [2, 2, 2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2, 2, 2], 'conv_bias': True, 'norm_op': 'torch.nn.modules.instancenorm.InstanceNorm3d', 'norm_op_kwargs': {'eps': 1e-05, 'affine': True}, 'dropout_op': None, 'dropout_op_kwargs': None, 'nonlin': 'torch.nn.LeakyReLU', 'nonlin_kwargs': {'inplace': True}, 'deep_supervision': True}, '_kw_requires_import': ['conv_op', 'norm_op', 'dropout_op', 'nonlin']}, 'batch_dice': True, 'inherits_from': '3d_fullres'} 

These are the global plan.json settings:
 {'dataset_name': 'Dataset310_nnInteractive_Calcium_OCT_CrossValidation', 'plans_name': 'nnUNetPlans', 'original_median_spacing_after_transp': [1.0, 1.0, 1.0], 'original_median_shape_after_transp': [375, 498, 498], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 0.4977976381778717, 'mean': 0.1394134759902954, 'median': 0.09849607944488525, 'min': 0.0, 'percentile_00_5': 0.015305490233004093, 'percentile_99_5': 0.4977976381778717, 'std': 0.121165432035923}}} 

2025-10-05 18:59:48.515142: unpacking dataset...
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/torch/_inductor/lowering.py:7095: UserWarning: 
Online softmax is disabled on the fly since Inductor decides to
split the reduction. Cut an issue to PyTorch if this is an
important use case and you want to speed it up with online
softmax.

  warnings.warn(
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/torch/_inductor/lowering.py:7095: UserWarning: 
Online softmax is disabled on the fly since Inductor decides to
split the reduction. Cut an issue to PyTorch if this is an
important use case and you want to speed it up with online
softmax.

  warnings.warn(
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/torch/_inductor/lowering.py:7095: UserWarning: 
Online softmax is disabled on the fly since Inductor decides to
split the reduction. Cut an issue to PyTorch if this is an
important use case and you want to speed it up with online
softmax.

  warnings.warn(
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/torch/_inductor/lowering.py:7095: UserWarning: 
Online softmax is disabled on the fly since Inductor decides to
split the reduction. Cut an issue to PyTorch if this is an
important use case and you want to speed it up with online
softmax.

  warnings.warn(
2025-10-05 18:59:52.588023: unpacking done...
2025-10-05 18:59:52.590353: Unable to plot network architecture: nnUNet_compile is enabled!
2025-10-05 18:59:52.596568: 
2025-10-05 18:59:52.596733: Epoch 0
2025-10-05 18:59:52.596945: Current learning rate: 0.01
2025-10-05 19:01:11.257804: Validation loss improved from 1000.00000 to -0.20258! Patience: 0/50
2025-10-05 19:01:11.258320: train_loss -0.1319
2025-10-05 19:01:11.258480: val_loss -0.2026
2025-10-05 19:01:11.258595: Pseudo dice [np.float32(0.5456)]
2025-10-05 19:01:11.258746: Epoch time: 78.66 s
2025-10-05 19:01:11.258857: Yayy! New best EMA pseudo Dice: 0.5455999970436096
2025-10-05 19:01:12.186205: 
2025-10-05 19:01:12.186496: Epoch 1
2025-10-05 19:01:12.186722: Current learning rate: 0.00994
2025-10-05 19:01:57.892483: Validation loss improved from -0.20258 to -0.24091! Patience: 0/50
2025-10-05 19:01:57.892989: train_loss -0.2436
2025-10-05 19:01:57.893124: val_loss -0.2409
2025-10-05 19:01:57.893254: Pseudo dice [np.float32(0.5737)]
2025-10-05 19:01:57.893375: Epoch time: 45.71 s
2025-10-05 19:01:57.893480: Yayy! New best EMA pseudo Dice: 0.5483999848365784
2025-10-05 19:01:58.960465: 
2025-10-05 19:01:58.960757: Epoch 2
2025-10-05 19:01:58.960923: Current learning rate: 0.00988
2025-10-05 19:02:44.736918: Validation loss improved from -0.24091 to -0.27648! Patience: 0/50
2025-10-05 19:02:44.737418: train_loss -0.2707
2025-10-05 19:02:44.737571: val_loss -0.2765
2025-10-05 19:02:44.737685: Pseudo dice [np.float32(0.5902)]
2025-10-05 19:02:44.737842: Epoch time: 45.78 s
2025-10-05 19:02:44.737951: Yayy! New best EMA pseudo Dice: 0.5525000095367432
2025-10-05 19:02:45.871340: 
2025-10-05 19:02:45.871624: Epoch 3
2025-10-05 19:02:45.871805: Current learning rate: 0.00982
2025-10-05 19:03:31.712282: Validation loss did not improve from -0.27648. Patience: 1/50
2025-10-05 19:03:31.712693: train_loss -0.3377
2025-10-05 19:03:31.712827: val_loss -0.1882
2025-10-05 19:03:31.713016: Pseudo dice [np.float32(0.5608)]
2025-10-05 19:03:31.713273: Epoch time: 45.84 s
2025-10-05 19:03:31.713454: Yayy! New best EMA pseudo Dice: 0.5533999800682068
2025-10-05 19:03:32.774361: 
2025-10-05 19:03:32.774615: Epoch 4
2025-10-05 19:03:32.774852: Current learning rate: 0.00976
2025-10-05 19:04:18.631378: Validation loss improved from -0.27648 to -0.32415! Patience: 1/50
2025-10-05 19:04:18.631979: train_loss -0.3541
2025-10-05 19:04:18.632131: val_loss -0.3241
2025-10-05 19:04:18.632386: Pseudo dice [np.float32(0.6273)]
2025-10-05 19:04:18.632638: Epoch time: 45.86 s
2025-10-05 19:04:19.031861: Yayy! New best EMA pseudo Dice: 0.5608000159263611
2025-10-05 19:04:20.102334: 
2025-10-05 19:04:20.102613: Epoch 5
2025-10-05 19:04:20.102858: Current learning rate: 0.0097
2025-10-05 19:05:05.927914: Validation loss improved from -0.32415 to -0.36120! Patience: 0/50
2025-10-05 19:05:05.928363: train_loss -0.381
2025-10-05 19:05:05.928610: val_loss -0.3612
2025-10-05 19:05:05.928771: Pseudo dice [np.float32(0.6354)]
2025-10-05 19:05:05.928923: Epoch time: 45.83 s
2025-10-05 19:05:05.929052: Yayy! New best EMA pseudo Dice: 0.5681999921798706
2025-10-05 19:05:06.992853: 
2025-10-05 19:05:06.993138: Epoch 6
2025-10-05 19:05:06.993352: Current learning rate: 0.00964
2025-10-05 19:05:52.796314: Validation loss did not improve from -0.36120. Patience: 1/50
2025-10-05 19:05:52.796901: train_loss -0.4027
2025-10-05 19:05:52.797065: val_loss -0.3552
2025-10-05 19:05:52.797225: Pseudo dice [np.float32(0.6324)]
2025-10-05 19:05:52.797361: Epoch time: 45.8 s
2025-10-05 19:05:52.797483: Yayy! New best EMA pseudo Dice: 0.5745999813079834
2025-10-05 19:05:53.860896: 
2025-10-05 19:05:53.861211: Epoch 7
2025-10-05 19:05:53.861461: Current learning rate: 0.00958
2025-10-05 19:06:39.691846: Validation loss did not improve from -0.36120. Patience: 2/50
2025-10-05 19:06:39.692295: train_loss -0.4402
2025-10-05 19:06:39.692443: val_loss -0.3453
2025-10-05 19:06:39.692574: Pseudo dice [np.float32(0.6282)]
2025-10-05 19:06:39.692749: Epoch time: 45.83 s
2025-10-05 19:06:39.692857: Yayy! New best EMA pseudo Dice: 0.5799999833106995
2025-10-05 19:06:40.753118: 
2025-10-05 19:06:40.753411: Epoch 8
2025-10-05 19:06:40.753592: Current learning rate: 0.00952
2025-10-05 19:07:26.615071: Validation loss improved from -0.36120 to -0.37730! Patience: 2/50
2025-10-05 19:07:26.615643: train_loss -0.4487
2025-10-05 19:07:26.615885: val_loss -0.3773
2025-10-05 19:07:26.616033: Pseudo dice [np.float32(0.6427)]
2025-10-05 19:07:26.616264: Epoch time: 45.86 s
2025-10-05 19:07:26.616381: Yayy! New best EMA pseudo Dice: 0.5863000154495239
2025-10-05 19:07:27.702016: 
2025-10-05 19:07:27.702288: Epoch 9
2025-10-05 19:07:27.702441: Current learning rate: 0.00946
2025-10-05 19:08:13.598117: Validation loss did not improve from -0.37730. Patience: 1/50
2025-10-05 19:08:13.598618: train_loss -0.4575
2025-10-05 19:08:13.598981: val_loss -0.3643
2025-10-05 19:08:13.599318: Pseudo dice [np.float32(0.6441)]
2025-10-05 19:08:13.599637: Epoch time: 45.9 s
2025-10-05 19:08:14.036328: Yayy! New best EMA pseudo Dice: 0.5921000242233276
2025-10-05 19:08:15.076585: 
2025-10-05 19:08:15.076847: Epoch 10
2025-10-05 19:08:15.077091: Current learning rate: 0.0094
2025-10-05 19:09:00.921387: Validation loss improved from -0.37730 to -0.40261! Patience: 1/50
2025-10-05 19:09:00.921946: train_loss -0.4757
2025-10-05 19:09:00.922104: val_loss -0.4026
2025-10-05 19:09:00.922275: Pseudo dice [np.float32(0.6497)]
2025-10-05 19:09:00.922424: Epoch time: 45.85 s
2025-10-05 19:09:00.922538: Yayy! New best EMA pseudo Dice: 0.5978000164031982
2025-10-05 19:09:01.980123: 
2025-10-05 19:09:01.980392: Epoch 11
2025-10-05 19:09:01.980572: Current learning rate: 0.00934
2025-10-05 19:09:47.879823: Validation loss improved from -0.40261 to -0.40465! Patience: 0/50
2025-10-05 19:09:47.880210: train_loss -0.4801
2025-10-05 19:09:47.880438: val_loss -0.4047
2025-10-05 19:09:47.880696: Pseudo dice [np.float32(0.6577)]
2025-10-05 19:09:47.881030: Epoch time: 45.9 s
2025-10-05 19:09:47.881326: Yayy! New best EMA pseudo Dice: 0.6037999987602234
2025-10-05 19:09:49.269428: 
2025-10-05 19:09:49.269860: Epoch 12
2025-10-05 19:09:49.270252: Current learning rate: 0.00928
2025-10-05 19:10:35.117784: Validation loss did not improve from -0.40465. Patience: 1/50
2025-10-05 19:10:35.118307: train_loss -0.4955
2025-10-05 19:10:35.118505: val_loss -0.4031
2025-10-05 19:10:35.118612: Pseudo dice [np.float32(0.6685)]
2025-10-05 19:10:35.118736: Epoch time: 45.85 s
2025-10-05 19:10:35.118904: Yayy! New best EMA pseudo Dice: 0.6103000044822693
2025-10-05 19:10:36.178412: 
2025-10-05 19:10:36.178647: Epoch 13
2025-10-05 19:10:36.178814: Current learning rate: 0.00922
2025-10-05 19:11:22.007360: Validation loss did not improve from -0.40465. Patience: 2/50
2025-10-05 19:11:22.007838: train_loss -0.5079
2025-10-05 19:11:22.008178: val_loss -0.3934
2025-10-05 19:11:22.008351: Pseudo dice [np.float32(0.6506)]
2025-10-05 19:11:22.008636: Epoch time: 45.83 s
2025-10-05 19:11:22.008892: Yayy! New best EMA pseudo Dice: 0.614300012588501
2025-10-05 19:11:23.080259: 
2025-10-05 19:11:23.080621: Epoch 14
2025-10-05 19:11:23.080909: Current learning rate: 0.00916
2025-10-05 19:12:08.944774: Validation loss improved from -0.40465 to -0.40786! Patience: 2/50
2025-10-05 19:12:08.945695: train_loss -0.5022
2025-10-05 19:12:08.945972: val_loss -0.4079
2025-10-05 19:12:08.946301: Pseudo dice [np.float32(0.6585)]
2025-10-05 19:12:08.946540: Epoch time: 45.87 s
2025-10-05 19:12:09.403871: Yayy! New best EMA pseudo Dice: 0.6187000274658203
2025-10-05 19:12:10.452326: 
2025-10-05 19:12:10.452692: Epoch 15
2025-10-05 19:12:10.453051: Current learning rate: 0.0091
2025-10-05 19:12:56.283068: Validation loss improved from -0.40786 to -0.42293! Patience: 0/50
2025-10-05 19:12:56.283464: train_loss -0.509
2025-10-05 19:12:56.283638: val_loss -0.4229
2025-10-05 19:12:56.283784: Pseudo dice [np.float32(0.67)]
2025-10-05 19:12:56.283934: Epoch time: 45.83 s
2025-10-05 19:12:56.284079: Yayy! New best EMA pseudo Dice: 0.6238999962806702
2025-10-05 19:12:57.370024: 
2025-10-05 19:12:57.370272: Epoch 16
2025-10-05 19:12:57.370473: Current learning rate: 0.00903
2025-10-05 19:13:43.269078: Validation loss did not improve from -0.42293. Patience: 1/50
2025-10-05 19:13:43.269685: train_loss -0.521
2025-10-05 19:13:43.269848: val_loss -0.3939
2025-10-05 19:13:43.269974: Pseudo dice [np.float32(0.6607)]
2025-10-05 19:13:43.270100: Epoch time: 45.9 s
2025-10-05 19:13:43.270253: Yayy! New best EMA pseudo Dice: 0.6274999976158142
2025-10-05 19:13:44.394569: 
2025-10-05 19:13:44.394914: Epoch 17
2025-10-05 19:13:44.395112: Current learning rate: 0.00897
2025-10-05 19:14:30.299701: Validation loss did not improve from -0.42293. Patience: 2/50
2025-10-05 19:14:30.300057: train_loss -0.5392
2025-10-05 19:14:30.300214: val_loss -0.3858
2025-10-05 19:14:30.300344: Pseudo dice [np.float32(0.6427)]
2025-10-05 19:14:30.300469: Epoch time: 45.91 s
2025-10-05 19:14:30.300579: Yayy! New best EMA pseudo Dice: 0.6291000247001648
2025-10-05 19:14:31.416695: 
2025-10-05 19:14:31.416979: Epoch 18
2025-10-05 19:14:31.417110: Current learning rate: 0.00891
2025-10-05 19:15:17.311341: Validation loss improved from -0.42293 to -0.43067! Patience: 2/50
2025-10-05 19:15:17.311844: train_loss -0.5354
2025-10-05 19:15:17.311980: val_loss -0.4307
2025-10-05 19:15:17.312111: Pseudo dice [np.float32(0.6802)]
2025-10-05 19:15:17.312265: Epoch time: 45.9 s
2025-10-05 19:15:17.312401: Yayy! New best EMA pseudo Dice: 0.6341999769210815
2025-10-05 19:15:18.408776: 
2025-10-05 19:15:18.409080: Epoch 19
2025-10-05 19:15:18.409324: Current learning rate: 0.00885
2025-10-05 19:16:04.302415: Validation loss did not improve from -0.43067. Patience: 1/50
2025-10-05 19:16:04.302775: train_loss -0.5521
2025-10-05 19:16:04.302914: val_loss -0.3774
2025-10-05 19:16:04.303043: Pseudo dice [np.float32(0.6359)]
2025-10-05 19:16:04.303211: Epoch time: 45.89 s
2025-10-05 19:16:04.747487: Yayy! New best EMA pseudo Dice: 0.6342999935150146
2025-10-05 19:16:05.831327: 
2025-10-05 19:16:05.831580: Epoch 20
2025-10-05 19:16:05.831788: Current learning rate: 0.00879
2025-10-05 19:16:51.710502: Validation loss did not improve from -0.43067. Patience: 2/50
2025-10-05 19:16:51.711075: train_loss -0.557
2025-10-05 19:16:51.711221: val_loss -0.4167
2025-10-05 19:16:51.711332: Pseudo dice [np.float32(0.6679)]
2025-10-05 19:16:51.711453: Epoch time: 45.88 s
2025-10-05 19:16:51.711621: Yayy! New best EMA pseudo Dice: 0.6377000212669373
2025-10-05 19:16:52.812668: 
2025-10-05 19:16:52.813163: Epoch 21
2025-10-05 19:16:52.813518: Current learning rate: 0.00873
2025-10-05 19:17:38.693041: Validation loss did not improve from -0.43067. Patience: 3/50
2025-10-05 19:17:38.693445: train_loss -0.5661
2025-10-05 19:17:38.693633: val_loss -0.4214
2025-10-05 19:17:38.693751: Pseudo dice [np.float32(0.677)]
2025-10-05 19:17:38.693903: Epoch time: 45.88 s
2025-10-05 19:17:38.694027: Yayy! New best EMA pseudo Dice: 0.6416000127792358
2025-10-05 19:17:39.762184: 
2025-10-05 19:17:39.762428: Epoch 22
2025-10-05 19:17:39.762599: Current learning rate: 0.00867
2025-10-05 19:18:25.663522: Validation loss did not improve from -0.43067. Patience: 4/50
2025-10-05 19:18:25.664071: train_loss -0.5727
2025-10-05 19:18:25.664253: val_loss -0.4142
2025-10-05 19:18:25.664373: Pseudo dice [np.float32(0.6594)]
2025-10-05 19:18:25.664512: Epoch time: 45.9 s
2025-10-05 19:18:25.664659: Yayy! New best EMA pseudo Dice: 0.6434000134468079
2025-10-05 19:18:26.748900: 
2025-10-05 19:18:26.749173: Epoch 23
2025-10-05 19:18:26.749374: Current learning rate: 0.00861
2025-10-05 19:19:12.681477: Validation loss improved from -0.43067 to -0.44816! Patience: 4/50
2025-10-05 19:19:12.681977: train_loss -0.5826
2025-10-05 19:19:12.682229: val_loss -0.4482
2025-10-05 19:19:12.682468: Pseudo dice [np.float32(0.6838)]
2025-10-05 19:19:12.682695: Epoch time: 45.93 s
2025-10-05 19:19:12.682908: Yayy! New best EMA pseudo Dice: 0.6474999785423279
2025-10-05 19:19:13.747175: 
2025-10-05 19:19:13.747417: Epoch 24
2025-10-05 19:19:13.747557: Current learning rate: 0.00855
2025-10-05 19:19:59.665798: Validation loss did not improve from -0.44816. Patience: 1/50
2025-10-05 19:19:59.666514: train_loss -0.5869
2025-10-05 19:19:59.666712: val_loss -0.4389
2025-10-05 19:19:59.666888: Pseudo dice [np.float32(0.6851)]
2025-10-05 19:19:59.667107: Epoch time: 45.92 s
2025-10-05 19:20:00.088766: Yayy! New best EMA pseudo Dice: 0.651199996471405
2025-10-05 19:20:01.141986: 
2025-10-05 19:20:01.142186: Epoch 25
2025-10-05 19:20:01.142332: Current learning rate: 0.00849
2025-10-05 19:20:47.028365: Validation loss did not improve from -0.44816. Patience: 2/50
2025-10-05 19:20:47.028794: train_loss -0.5734
2025-10-05 19:20:47.029005: val_loss -0.4357
2025-10-05 19:20:47.029144: Pseudo dice [np.float32(0.6955)]
2025-10-05 19:20:47.029299: Epoch time: 45.89 s
2025-10-05 19:20:47.029412: Yayy! New best EMA pseudo Dice: 0.6556000113487244
2025-10-05 19:20:48.093577: 
2025-10-05 19:20:48.094057: Epoch 26
2025-10-05 19:20:48.094419: Current learning rate: 0.00843
2025-10-05 19:21:33.981471: Validation loss did not improve from -0.44816. Patience: 3/50
2025-10-05 19:21:33.982077: train_loss -0.5718
2025-10-05 19:21:33.982222: val_loss -0.4407
2025-10-05 19:21:33.982395: Pseudo dice [np.float32(0.689)]
2025-10-05 19:21:33.982517: Epoch time: 45.89 s
2025-10-05 19:21:33.982659: Yayy! New best EMA pseudo Dice: 0.6589999794960022
2025-10-05 19:21:35.407181: 
2025-10-05 19:21:35.407459: Epoch 27
2025-10-05 19:21:35.407623: Current learning rate: 0.00836
2025-10-05 19:22:21.248762: Validation loss did not improve from -0.44816. Patience: 4/50
2025-10-05 19:22:21.249283: train_loss -0.5953
2025-10-05 19:22:21.249527: val_loss -0.378
2025-10-05 19:22:21.249685: Pseudo dice [np.float32(0.65)]
2025-10-05 19:22:21.249835: Epoch time: 45.84 s
2025-10-05 19:22:21.896028: 
2025-10-05 19:22:21.896323: Epoch 28
2025-10-05 19:22:21.896470: Current learning rate: 0.0083
2025-10-05 19:23:07.808769: Validation loss did not improve from -0.44816. Patience: 5/50
2025-10-05 19:23:07.809529: train_loss -0.5903
2025-10-05 19:23:07.809665: val_loss -0.4155
2025-10-05 19:23:07.809841: Pseudo dice [np.float32(0.6644)]
2025-10-05 19:23:07.810035: Epoch time: 45.91 s
2025-10-05 19:23:08.435526: 
2025-10-05 19:23:08.435795: Epoch 29
2025-10-05 19:23:08.436005: Current learning rate: 0.00824
2025-10-05 19:23:54.351415: Validation loss did not improve from -0.44816. Patience: 6/50
2025-10-05 19:23:54.351864: train_loss -0.5958
2025-10-05 19:23:54.352020: val_loss -0.444
2025-10-05 19:23:54.352150: Pseudo dice [np.float32(0.6871)]
2025-10-05 19:23:54.352297: Epoch time: 45.92 s
2025-10-05 19:23:54.789688: Yayy! New best EMA pseudo Dice: 0.6615999937057495
2025-10-05 19:23:55.825612: 
2025-10-05 19:23:55.826165: Epoch 30
2025-10-05 19:23:55.826572: Current learning rate: 0.00818
2025-10-05 19:24:41.717143: Validation loss did not improve from -0.44816. Patience: 7/50
2025-10-05 19:24:41.717847: train_loss -0.6099
2025-10-05 19:24:41.718111: val_loss -0.4189
2025-10-05 19:24:41.718269: Pseudo dice [np.float32(0.6602)]
2025-10-05 19:24:41.718511: Epoch time: 45.89 s
2025-10-05 19:24:42.350125: 
2025-10-05 19:24:42.350372: Epoch 31
2025-10-05 19:24:42.350568: Current learning rate: 0.00812
2025-10-05 19:25:28.241195: Validation loss did not improve from -0.44816. Patience: 8/50
2025-10-05 19:25:28.241589: train_loss -0.603
2025-10-05 19:25:28.241731: val_loss -0.4279
2025-10-05 19:25:28.241858: Pseudo dice [np.float32(0.6713)]
2025-10-05 19:25:28.241987: Epoch time: 45.89 s
2025-10-05 19:25:28.242117: Yayy! New best EMA pseudo Dice: 0.6624000072479248
2025-10-05 19:25:29.300823: 
2025-10-05 19:25:29.301109: Epoch 32
2025-10-05 19:25:29.301263: Current learning rate: 0.00806
2025-10-05 19:26:15.199520: Validation loss improved from -0.44816 to -0.45623! Patience: 8/50
2025-10-05 19:26:15.200167: train_loss -0.5995
2025-10-05 19:26:15.200329: val_loss -0.4562
2025-10-05 19:26:15.200487: Pseudo dice [np.float32(0.6994)]
2025-10-05 19:26:15.200621: Epoch time: 45.9 s
2025-10-05 19:26:15.200747: Yayy! New best EMA pseudo Dice: 0.666100025177002
2025-10-05 19:26:16.293339: 
2025-10-05 19:26:16.293647: Epoch 33
2025-10-05 19:26:16.293824: Current learning rate: 0.008
2025-10-05 19:27:02.229187: Validation loss did not improve from -0.45623. Patience: 1/50
2025-10-05 19:27:02.229743: train_loss -0.6165
2025-10-05 19:27:02.229891: val_loss -0.4401
2025-10-05 19:27:02.230031: Pseudo dice [np.float32(0.6732)]
2025-10-05 19:27:02.230182: Epoch time: 45.94 s
2025-10-05 19:27:02.230318: Yayy! New best EMA pseudo Dice: 0.6668000221252441
2025-10-05 19:27:03.320969: 
2025-10-05 19:27:03.321211: Epoch 34
2025-10-05 19:27:03.321392: Current learning rate: 0.00793
2025-10-05 19:27:49.309831: Validation loss improved from -0.45623 to -0.46363! Patience: 1/50
2025-10-05 19:27:49.310597: train_loss -0.6293
2025-10-05 19:27:49.310814: val_loss -0.4636
2025-10-05 19:27:49.311014: Pseudo dice [np.float32(0.7072)]
2025-10-05 19:27:49.311296: Epoch time: 45.99 s
2025-10-05 19:27:49.748978: Yayy! New best EMA pseudo Dice: 0.670799970626831
2025-10-05 19:27:50.818570: 
2025-10-05 19:27:50.818795: Epoch 35
2025-10-05 19:27:50.818966: Current learning rate: 0.00787
2025-10-05 19:28:36.755805: Validation loss did not improve from -0.46363. Patience: 1/50
2025-10-05 19:28:36.756237: train_loss -0.6328
2025-10-05 19:28:36.756402: val_loss -0.4635
2025-10-05 19:28:36.756510: Pseudo dice [np.float32(0.6994)]
2025-10-05 19:28:36.756629: Epoch time: 45.94 s
2025-10-05 19:28:36.756732: Yayy! New best EMA pseudo Dice: 0.6736999750137329
2025-10-05 19:28:37.838642: 
2025-10-05 19:28:37.838955: Epoch 36
2025-10-05 19:28:37.839120: Current learning rate: 0.00781
2025-10-05 19:29:23.815327: Validation loss did not improve from -0.46363. Patience: 2/50
2025-10-05 19:29:23.815957: train_loss -0.6273
2025-10-05 19:29:23.816129: val_loss -0.3971
2025-10-05 19:29:23.816274: Pseudo dice [np.float32(0.6721)]
2025-10-05 19:29:23.816400: Epoch time: 45.98 s
2025-10-05 19:29:24.443617: 
2025-10-05 19:29:24.443889: Epoch 37
2025-10-05 19:29:24.444028: Current learning rate: 0.00775
2025-10-05 19:30:10.373387: Validation loss did not improve from -0.46363. Patience: 3/50
2025-10-05 19:30:10.373717: train_loss -0.6349
2025-10-05 19:30:10.373880: val_loss -0.4174
2025-10-05 19:30:10.374025: Pseudo dice [np.float32(0.6787)]
2025-10-05 19:30:10.374177: Epoch time: 45.93 s
2025-10-05 19:30:10.374342: Yayy! New best EMA pseudo Dice: 0.6740999817848206
2025-10-05 19:30:11.462760: 
2025-10-05 19:30:11.462974: Epoch 38
2025-10-05 19:30:11.463139: Current learning rate: 0.00769
2025-10-05 19:30:57.386400: Validation loss did not improve from -0.46363. Patience: 4/50
2025-10-05 19:30:57.386938: train_loss -0.6315
2025-10-05 19:30:57.387082: val_loss -0.4231
2025-10-05 19:30:57.387223: Pseudo dice [np.float32(0.6819)]
2025-10-05 19:30:57.387368: Epoch time: 45.92 s
2025-10-05 19:30:57.387508: Yayy! New best EMA pseudo Dice: 0.6747999787330627
2025-10-05 19:30:58.474221: 
2025-10-05 19:30:58.474563: Epoch 39
2025-10-05 19:30:58.474788: Current learning rate: 0.00763
2025-10-05 19:31:44.370188: Validation loss did not improve from -0.46363. Patience: 5/50
2025-10-05 19:31:44.370611: train_loss -0.6397
2025-10-05 19:31:44.370757: val_loss -0.4396
2025-10-05 19:31:44.370890: Pseudo dice [np.float32(0.6941)]
2025-10-05 19:31:44.371036: Epoch time: 45.9 s
2025-10-05 19:31:44.807917: Yayy! New best EMA pseudo Dice: 0.676800012588501
2025-10-05 19:31:45.863467: 
2025-10-05 19:31:45.863735: Epoch 40
2025-10-05 19:31:45.863904: Current learning rate: 0.00756
2025-10-05 19:32:31.770598: Validation loss did not improve from -0.46363. Patience: 6/50
2025-10-05 19:32:31.771213: train_loss -0.6545
2025-10-05 19:32:31.771367: val_loss -0.3912
2025-10-05 19:32:31.771496: Pseudo dice [np.float32(0.649)]
2025-10-05 19:32:31.771625: Epoch time: 45.91 s
2025-10-05 19:32:32.402883: 
2025-10-05 19:32:32.403206: Epoch 41
2025-10-05 19:32:32.403384: Current learning rate: 0.0075
2025-10-05 19:33:18.361723: Validation loss improved from -0.46363 to -0.46575! Patience: 6/50
2025-10-05 19:33:18.362176: train_loss -0.6459
2025-10-05 19:33:18.362352: val_loss -0.4658
2025-10-05 19:33:18.362497: Pseudo dice [np.float32(0.7034)]
2025-10-05 19:33:18.362664: Epoch time: 45.96 s
2025-10-05 19:33:18.362808: Yayy! New best EMA pseudo Dice: 0.6769000291824341
2025-10-05 19:33:19.442720: 
2025-10-05 19:33:19.443151: Epoch 42
2025-10-05 19:33:19.443337: Current learning rate: 0.00744
2025-10-05 19:34:05.368666: Validation loss improved from -0.46575 to -0.50692! Patience: 0/50
2025-10-05 19:34:05.369226: train_loss -0.6673
2025-10-05 19:34:05.369393: val_loss -0.5069
2025-10-05 19:34:05.369524: Pseudo dice [np.float32(0.7267)]
2025-10-05 19:34:05.369642: Epoch time: 45.93 s
2025-10-05 19:34:05.369742: Yayy! New best EMA pseudo Dice: 0.6819000244140625
2025-10-05 19:34:06.788557: 
2025-10-05 19:34:06.788862: Epoch 43
2025-10-05 19:34:06.789034: Current learning rate: 0.00738
2025-10-05 19:34:52.726644: Validation loss did not improve from -0.50692. Patience: 1/50
2025-10-05 19:34:52.727087: train_loss -0.6642
2025-10-05 19:34:52.727260: val_loss -0.4305
2025-10-05 19:34:52.727395: Pseudo dice [np.float32(0.6949)]
2025-10-05 19:34:52.727536: Epoch time: 45.94 s
2025-10-05 19:34:52.727659: Yayy! New best EMA pseudo Dice: 0.6832000017166138
2025-10-05 19:34:53.798341: 
2025-10-05 19:34:53.798662: Epoch 44
2025-10-05 19:34:53.798852: Current learning rate: 0.00732
2025-10-05 19:35:39.695271: Validation loss did not improve from -0.50692. Patience: 2/50
2025-10-05 19:35:39.695877: train_loss -0.6772
2025-10-05 19:35:39.696049: val_loss -0.4384
2025-10-05 19:35:39.696167: Pseudo dice [np.float32(0.6874)]
2025-10-05 19:35:39.696299: Epoch time: 45.9 s
2025-10-05 19:35:40.142640: Yayy! New best EMA pseudo Dice: 0.6836000084877014
2025-10-05 19:35:41.182322: 
2025-10-05 19:35:41.182572: Epoch 45
2025-10-05 19:35:41.182730: Current learning rate: 0.00725
2025-10-05 19:36:27.054423: Validation loss did not improve from -0.50692. Patience: 3/50
2025-10-05 19:36:27.054820: train_loss -0.6705
2025-10-05 19:36:27.055006: val_loss -0.4599
2025-10-05 19:36:27.055153: Pseudo dice [np.float32(0.6951)]
2025-10-05 19:36:27.055284: Epoch time: 45.87 s
2025-10-05 19:36:27.055390: Yayy! New best EMA pseudo Dice: 0.6848000288009644
2025-10-05 19:36:28.106138: 
2025-10-05 19:36:28.106449: Epoch 46
2025-10-05 19:36:28.106622: Current learning rate: 0.00719
2025-10-05 19:37:13.989842: Validation loss did not improve from -0.50692. Patience: 4/50
2025-10-05 19:37:13.990398: train_loss -0.6733
2025-10-05 19:37:13.990540: val_loss -0.4064
2025-10-05 19:37:13.990662: Pseudo dice [np.float32(0.6768)]
2025-10-05 19:37:13.990785: Epoch time: 45.88 s
2025-10-05 19:37:14.605385: 
2025-10-05 19:37:14.605690: Epoch 47
2025-10-05 19:37:14.605867: Current learning rate: 0.00713
2025-10-05 19:38:00.514646: Validation loss did not improve from -0.50692. Patience: 5/50
2025-10-05 19:38:00.514953: train_loss -0.6793
2025-10-05 19:38:00.515118: val_loss -0.4419
2025-10-05 19:38:00.515265: Pseudo dice [np.float32(0.6904)]
2025-10-05 19:38:00.515436: Epoch time: 45.91 s
2025-10-05 19:38:01.129394: 
2025-10-05 19:38:01.129705: Epoch 48
2025-10-05 19:38:01.129895: Current learning rate: 0.00707
2025-10-05 19:38:46.978323: Validation loss did not improve from -0.50692. Patience: 6/50
2025-10-05 19:38:46.979022: train_loss -0.6916
2025-10-05 19:38:46.979219: val_loss -0.4643
2025-10-05 19:38:46.979379: Pseudo dice [np.float32(0.7085)]
2025-10-05 19:38:46.979556: Epoch time: 45.85 s
2025-10-05 19:38:46.979733: Yayy! New best EMA pseudo Dice: 0.6869999766349792
2025-10-05 19:38:48.040548: 
2025-10-05 19:38:48.040815: Epoch 49
2025-10-05 19:38:48.041007: Current learning rate: 0.007
2025-10-05 19:39:33.915835: Validation loss did not improve from -0.50692. Patience: 7/50
2025-10-05 19:39:33.916236: train_loss -0.6923
2025-10-05 19:39:33.916415: val_loss -0.4369
2025-10-05 19:39:33.916553: Pseudo dice [np.float32(0.6834)]
2025-10-05 19:39:33.916707: Epoch time: 45.88 s
2025-10-05 19:39:34.963392: 
2025-10-05 19:39:34.963696: Epoch 50
2025-10-05 19:39:34.963871: Current learning rate: 0.00694
2025-10-05 19:40:20.853664: Validation loss did not improve from -0.50692. Patience: 8/50
2025-10-05 19:40:20.854276: train_loss -0.6927
2025-10-05 19:40:20.854447: val_loss -0.434
2025-10-05 19:40:20.854609: Pseudo dice [np.float32(0.6854)]
2025-10-05 19:40:20.854788: Epoch time: 45.89 s
2025-10-05 19:40:21.477615: 
2025-10-05 19:40:21.477860: Epoch 51
2025-10-05 19:40:21.478091: Current learning rate: 0.00688
2025-10-05 19:41:07.363629: Validation loss did not improve from -0.50692. Patience: 9/50
2025-10-05 19:41:07.364060: train_loss -0.7004
2025-10-05 19:41:07.364196: val_loss -0.4306
2025-10-05 19:41:07.364336: Pseudo dice [np.float32(0.6945)]
2025-10-05 19:41:07.364452: Epoch time: 45.89 s
2025-10-05 19:41:07.364552: Yayy! New best EMA pseudo Dice: 0.6873000264167786
2025-10-05 19:41:08.422393: 
2025-10-05 19:41:08.422694: Epoch 52
2025-10-05 19:41:08.422858: Current learning rate: 0.00682
2025-10-05 19:41:54.291543: Validation loss did not improve from -0.50692. Patience: 10/50
2025-10-05 19:41:54.292181: train_loss -0.706
2025-10-05 19:41:54.292447: val_loss -0.4722
2025-10-05 19:41:54.292644: Pseudo dice [np.float32(0.7016)]
2025-10-05 19:41:54.292871: Epoch time: 45.87 s
2025-10-05 19:41:54.293034: Yayy! New best EMA pseudo Dice: 0.6887000203132629
2025-10-05 19:41:55.359793: 
2025-10-05 19:41:55.360012: Epoch 53
2025-10-05 19:41:55.360218: Current learning rate: 0.00675
2025-10-05 19:42:41.177092: Validation loss did not improve from -0.50692. Patience: 11/50
2025-10-05 19:42:41.177517: train_loss -0.7137
2025-10-05 19:42:41.177673: val_loss -0.4454
2025-10-05 19:42:41.177779: Pseudo dice [np.float32(0.686)]
2025-10-05 19:42:41.177898: Epoch time: 45.82 s
2025-10-05 19:42:41.796216: 
2025-10-05 19:42:41.796512: Epoch 54
2025-10-05 19:42:41.796679: Current learning rate: 0.00669
2025-10-05 19:43:27.698799: Validation loss did not improve from -0.50692. Patience: 12/50
2025-10-05 19:43:27.699383: train_loss -0.7078
2025-10-05 19:43:27.699520: val_loss -0.4274
2025-10-05 19:43:27.699647: Pseudo dice [np.float32(0.691)]
2025-10-05 19:43:27.699765: Epoch time: 45.9 s
2025-10-05 19:43:28.752332: 
2025-10-05 19:43:28.752606: Epoch 55
2025-10-05 19:43:28.752900: Current learning rate: 0.00663
2025-10-05 19:44:14.646404: Validation loss did not improve from -0.50692. Patience: 13/50
2025-10-05 19:44:14.646903: train_loss -0.7114
2025-10-05 19:44:14.647196: val_loss -0.4494
2025-10-05 19:44:14.647438: Pseudo dice [np.float32(0.708)]
2025-10-05 19:44:14.647641: Epoch time: 45.9 s
2025-10-05 19:44:14.647806: Yayy! New best EMA pseudo Dice: 0.6905999779701233
2025-10-05 19:44:15.722454: 
2025-10-05 19:44:15.722763: Epoch 56
2025-10-05 19:44:15.722929: Current learning rate: 0.00657
2025-10-05 19:45:01.610750: Validation loss did not improve from -0.50692. Patience: 14/50
2025-10-05 19:45:01.611345: train_loss -0.7079
2025-10-05 19:45:01.611523: val_loss -0.4342
2025-10-05 19:45:01.611696: Pseudo dice [np.float32(0.6876)]
2025-10-05 19:45:01.611854: Epoch time: 45.89 s
2025-10-05 19:45:02.238737: 
2025-10-05 19:45:02.239021: Epoch 57
2025-10-05 19:45:02.239198: Current learning rate: 0.0065
2025-10-05 19:45:48.190431: Validation loss did not improve from -0.50692. Patience: 15/50
2025-10-05 19:45:48.190860: train_loss -0.7092
2025-10-05 19:45:48.191061: val_loss -0.4291
2025-10-05 19:45:48.191210: Pseudo dice [np.float32(0.6869)]
2025-10-05 19:45:48.191353: Epoch time: 45.95 s
2025-10-05 19:45:48.813594: 
2025-10-05 19:45:48.813836: Epoch 58
2025-10-05 19:45:48.814008: Current learning rate: 0.00644
2025-10-05 19:46:34.734408: Validation loss did not improve from -0.50692. Patience: 16/50
2025-10-05 19:46:34.735024: train_loss -0.7124
2025-10-05 19:46:34.735168: val_loss -0.4579
2025-10-05 19:46:34.735287: Pseudo dice [np.float32(0.6959)]
2025-10-05 19:46:34.735440: Epoch time: 45.92 s
2025-10-05 19:46:35.724906: 
2025-10-05 19:46:35.725170: Epoch 59
2025-10-05 19:46:35.725349: Current learning rate: 0.00638
2025-10-05 19:47:21.610813: Validation loss did not improve from -0.50692. Patience: 17/50
2025-10-05 19:47:21.611257: train_loss -0.7256
2025-10-05 19:47:21.611635: val_loss -0.4147
2025-10-05 19:47:21.611776: Pseudo dice [np.float32(0.6793)]
2025-10-05 19:47:21.611909: Epoch time: 45.89 s
2025-10-05 19:47:22.665762: 
2025-10-05 19:47:22.666041: Epoch 60
2025-10-05 19:47:22.666243: Current learning rate: 0.00631
2025-10-05 19:48:08.490966: Validation loss did not improve from -0.50692. Patience: 18/50
2025-10-05 19:48:08.491463: train_loss -0.7343
2025-10-05 19:48:08.491634: val_loss -0.4558
2025-10-05 19:48:08.491759: Pseudo dice [np.float32(0.705)]
2025-10-05 19:48:08.491877: Epoch time: 45.83 s
2025-10-05 19:48:08.492011: Yayy! New best EMA pseudo Dice: 0.6909999847412109
2025-10-05 19:48:09.562819: 
2025-10-05 19:48:09.563048: Epoch 61
2025-10-05 19:48:09.563180: Current learning rate: 0.00625
2025-10-05 19:48:55.414864: Validation loss did not improve from -0.50692. Patience: 19/50
2025-10-05 19:48:55.415205: train_loss -0.7278
2025-10-05 19:48:55.415349: val_loss -0.4081
2025-10-05 19:48:55.415455: Pseudo dice [np.float32(0.6767)]
2025-10-05 19:48:55.415579: Epoch time: 45.85 s
2025-10-05 19:48:56.045091: 
2025-10-05 19:48:56.045294: Epoch 62
2025-10-05 19:48:56.045450: Current learning rate: 0.00619
2025-10-05 19:49:41.918674: Validation loss did not improve from -0.50692. Patience: 20/50
2025-10-05 19:49:41.919190: train_loss -0.7322
2025-10-05 19:49:41.919325: val_loss -0.4309
2025-10-05 19:49:41.919433: Pseudo dice [np.float32(0.69)]
2025-10-05 19:49:41.919554: Epoch time: 45.87 s
2025-10-05 19:49:42.551317: 
2025-10-05 19:49:42.551620: Epoch 63
2025-10-05 19:49:42.551767: Current learning rate: 0.00612
2025-10-05 19:50:28.431425: Validation loss did not improve from -0.50692. Patience: 21/50
2025-10-05 19:50:28.431879: train_loss -0.7247
2025-10-05 19:50:28.432051: val_loss -0.3878
2025-10-05 19:50:28.432183: Pseudo dice [np.float32(0.6558)]
2025-10-05 19:50:28.432386: Epoch time: 45.88 s
2025-10-05 19:50:29.065466: 
2025-10-05 19:50:29.065711: Epoch 64
2025-10-05 19:50:29.065845: Current learning rate: 0.00606
2025-10-05 19:51:14.931172: Validation loss did not improve from -0.50692. Patience: 22/50
2025-10-05 19:51:14.931789: train_loss -0.7429
2025-10-05 19:51:14.931989: val_loss -0.4084
2025-10-05 19:51:14.932164: Pseudo dice [np.float32(0.6676)]
2025-10-05 19:51:14.932343: Epoch time: 45.87 s
2025-10-05 19:51:15.968809: 
2025-10-05 19:51:15.969061: Epoch 65
2025-10-05 19:51:15.969237: Current learning rate: 0.006
2025-10-05 19:52:01.849964: Validation loss did not improve from -0.50692. Patience: 23/50
2025-10-05 19:52:01.850650: train_loss -0.743
2025-10-05 19:52:01.851055: val_loss -0.447
2025-10-05 19:52:01.851359: Pseudo dice [np.float32(0.7048)]
2025-10-05 19:52:01.851666: Epoch time: 45.88 s
2025-10-05 19:52:02.484220: 
2025-10-05 19:52:02.484419: Epoch 66
2025-10-05 19:52:02.484579: Current learning rate: 0.00593
2025-10-05 19:52:48.385040: Validation loss did not improve from -0.50692. Patience: 24/50
2025-10-05 19:52:48.385794: train_loss -0.7401
2025-10-05 19:52:48.386056: val_loss -0.41
2025-10-05 19:52:48.386272: Pseudo dice [np.float32(0.687)]
2025-10-05 19:52:48.386516: Epoch time: 45.9 s
2025-10-05 19:52:49.016343: 
2025-10-05 19:52:49.016747: Epoch 67
2025-10-05 19:52:49.017021: Current learning rate: 0.00587
2025-10-05 19:53:34.929949: Validation loss did not improve from -0.50692. Patience: 25/50
2025-10-05 19:53:34.930398: train_loss -0.7497
2025-10-05 19:53:34.930580: val_loss -0.4587
2025-10-05 19:53:34.930753: Pseudo dice [np.float32(0.7157)]
2025-10-05 19:53:34.930892: Epoch time: 45.91 s
2025-10-05 19:53:35.561029: 
2025-10-05 19:53:35.561283: Epoch 68
2025-10-05 19:53:35.561442: Current learning rate: 0.00581
2025-10-05 19:54:21.454141: Validation loss did not improve from -0.50692. Patience: 26/50
2025-10-05 19:54:21.454664: train_loss -0.7475
2025-10-05 19:54:21.454792: val_loss -0.4545
2025-10-05 19:54:21.454950: Pseudo dice [np.float32(0.7089)]
2025-10-05 19:54:21.455090: Epoch time: 45.89 s
2025-10-05 19:54:21.455248: Yayy! New best EMA pseudo Dice: 0.6912999749183655
2025-10-05 19:54:22.519249: 
2025-10-05 19:54:22.519560: Epoch 69
2025-10-05 19:54:22.519766: Current learning rate: 0.00574
2025-10-05 19:55:08.442075: Validation loss did not improve from -0.50692. Patience: 27/50
2025-10-05 19:55:08.442460: train_loss -0.7482
2025-10-05 19:55:08.442685: val_loss -0.4129
2025-10-05 19:55:08.442796: Pseudo dice [np.float32(0.6922)]
2025-10-05 19:55:08.442945: Epoch time: 45.92 s
2025-10-05 19:55:08.896020: Yayy! New best EMA pseudo Dice: 0.6913999915122986
2025-10-05 19:55:09.947227: 
2025-10-05 19:55:09.947449: Epoch 70
2025-10-05 19:55:09.947673: Current learning rate: 0.00568
2025-10-05 19:55:55.810141: Validation loss did not improve from -0.50692. Patience: 28/50
2025-10-05 19:55:55.810591: train_loss -0.7531
2025-10-05 19:55:55.810735: val_loss -0.4314
2025-10-05 19:55:55.810875: Pseudo dice [np.float32(0.7026)]
2025-10-05 19:55:55.811007: Epoch time: 45.86 s
2025-10-05 19:55:55.811113: Yayy! New best EMA pseudo Dice: 0.6924999952316284
2025-10-05 19:55:56.887743: 
2025-10-05 19:55:56.888075: Epoch 71
2025-10-05 19:55:56.888299: Current learning rate: 0.00562
2025-10-05 19:56:42.797721: Validation loss did not improve from -0.50692. Patience: 29/50
2025-10-05 19:56:42.798157: train_loss -0.755
2025-10-05 19:56:42.798319: val_loss -0.4372
2025-10-05 19:56:42.798451: Pseudo dice [np.float32(0.6951)]
2025-10-05 19:56:42.798591: Epoch time: 45.91 s
2025-10-05 19:56:42.798705: Yayy! New best EMA pseudo Dice: 0.692799985408783
2025-10-05 19:56:43.856850: 
2025-10-05 19:56:43.857065: Epoch 72
2025-10-05 19:56:43.857204: Current learning rate: 0.00555
2025-10-05 19:57:29.751498: Validation loss did not improve from -0.50692. Patience: 30/50
2025-10-05 19:57:29.752066: train_loss -0.7572
2025-10-05 19:57:29.752214: val_loss -0.4656
2025-10-05 19:57:29.752323: Pseudo dice [np.float32(0.7141)]
2025-10-05 19:57:29.752439: Epoch time: 45.9 s
2025-10-05 19:57:29.752547: Yayy! New best EMA pseudo Dice: 0.6948999762535095
2025-10-05 19:57:30.815700: 
2025-10-05 19:57:30.815946: Epoch 73
2025-10-05 19:57:30.816088: Current learning rate: 0.00549
2025-10-05 19:58:17.089675: Validation loss did not improve from -0.50692. Patience: 31/50
2025-10-05 19:58:17.090302: train_loss -0.7641
2025-10-05 19:58:17.090763: val_loss -0.418
2025-10-05 19:58:17.091139: Pseudo dice [np.float32(0.702)]
2025-10-05 19:58:17.091520: Epoch time: 46.28 s
2025-10-05 19:58:17.091888: Yayy! New best EMA pseudo Dice: 0.6955999732017517
2025-10-05 19:58:18.166052: 
2025-10-05 19:58:18.166390: Epoch 74
2025-10-05 19:58:18.166631: Current learning rate: 0.00542
2025-10-05 19:59:04.049947: Validation loss did not improve from -0.50692. Patience: 32/50
2025-10-05 19:59:04.050658: train_loss -0.7672
2025-10-05 19:59:04.050884: val_loss -0.4447
2025-10-05 19:59:04.051106: Pseudo dice [np.float32(0.7037)]
2025-10-05 19:59:04.051333: Epoch time: 45.89 s
2025-10-05 19:59:04.504566: Yayy! New best EMA pseudo Dice: 0.696399986743927
2025-10-05 19:59:05.550593: 
2025-10-05 19:59:05.550941: Epoch 75
2025-10-05 19:59:05.551186: Current learning rate: 0.00536
2025-10-05 19:59:51.463729: Validation loss did not improve from -0.50692. Patience: 33/50
2025-10-05 19:59:51.464207: train_loss -0.7693
2025-10-05 19:59:51.464394: val_loss -0.4442
2025-10-05 19:59:51.464527: Pseudo dice [np.float32(0.7061)]
2025-10-05 19:59:51.464655: Epoch time: 45.91 s
2025-10-05 19:59:51.464767: Yayy! New best EMA pseudo Dice: 0.6973999738693237
2025-10-05 19:59:52.528252: 
2025-10-05 19:59:52.528549: Epoch 76
2025-10-05 19:59:52.528725: Current learning rate: 0.00529
2025-10-05 20:00:38.467178: Validation loss did not improve from -0.50692. Patience: 34/50
2025-10-05 20:00:38.467771: train_loss -0.7738
2025-10-05 20:00:38.467945: val_loss -0.4374
2025-10-05 20:00:38.468058: Pseudo dice [np.float32(0.702)]
2025-10-05 20:00:38.468186: Epoch time: 45.94 s
2025-10-05 20:00:38.468303: Yayy! New best EMA pseudo Dice: 0.6978999972343445
2025-10-05 20:00:39.523107: 
2025-10-05 20:00:39.523430: Epoch 77
2025-10-05 20:00:39.523571: Current learning rate: 0.00523
2025-10-05 20:01:25.460490: Validation loss did not improve from -0.50692. Patience: 35/50
2025-10-05 20:01:25.460837: train_loss -0.7738
2025-10-05 20:01:25.461031: val_loss -0.4376
2025-10-05 20:01:25.461161: Pseudo dice [np.float32(0.7041)]
2025-10-05 20:01:25.461306: Epoch time: 45.94 s
2025-10-05 20:01:25.461449: Yayy! New best EMA pseudo Dice: 0.6984999775886536
2025-10-05 20:01:26.547188: 
2025-10-05 20:01:26.547486: Epoch 78
2025-10-05 20:01:26.547698: Current learning rate: 0.00517
2025-10-05 20:02:12.409661: Validation loss did not improve from -0.50692. Patience: 36/50
2025-10-05 20:02:12.410287: train_loss -0.7787
2025-10-05 20:02:12.410427: val_loss -0.4606
2025-10-05 20:02:12.410592: Pseudo dice [np.float32(0.7148)]
2025-10-05 20:02:12.410777: Epoch time: 45.86 s
2025-10-05 20:02:12.410926: Yayy! New best EMA pseudo Dice: 0.7001000046730042
2025-10-05 20:02:13.494848: 
2025-10-05 20:02:13.495094: Epoch 79
2025-10-05 20:02:13.495232: Current learning rate: 0.0051
2025-10-05 20:02:59.425561: Validation loss did not improve from -0.50692. Patience: 37/50
2025-10-05 20:02:59.426111: train_loss -0.7794
2025-10-05 20:02:59.426377: val_loss -0.4282
2025-10-05 20:02:59.426559: Pseudo dice [np.float32(0.6976)]
2025-10-05 20:02:59.426710: Epoch time: 45.93 s
2025-10-05 20:03:00.519109: 
2025-10-05 20:03:00.519370: Epoch 80
2025-10-05 20:03:00.519504: Current learning rate: 0.00504
2025-10-05 20:03:46.444232: Validation loss did not improve from -0.50692. Patience: 38/50
2025-10-05 20:03:46.444727: train_loss -0.7811
2025-10-05 20:03:46.444918: val_loss -0.4195
2025-10-05 20:03:46.445031: Pseudo dice [np.float32(0.6826)]
2025-10-05 20:03:46.445150: Epoch time: 45.93 s
2025-10-05 20:03:47.084402: 
2025-10-05 20:03:47.084712: Epoch 81
2025-10-05 20:03:47.084874: Current learning rate: 0.00497
2025-10-05 20:04:33.042484: Validation loss did not improve from -0.50692. Patience: 39/50
2025-10-05 20:04:33.042918: train_loss -0.7808
2025-10-05 20:04:33.043210: val_loss -0.411
2025-10-05 20:04:33.043413: Pseudo dice [np.float32(0.6814)]
2025-10-05 20:04:33.043576: Epoch time: 45.96 s
2025-10-05 20:04:33.688263: 
2025-10-05 20:04:33.688476: Epoch 82
2025-10-05 20:04:33.688631: Current learning rate: 0.00491
2025-10-05 20:05:19.575264: Validation loss did not improve from -0.50692. Patience: 40/50
2025-10-05 20:05:19.575890: train_loss -0.7879
2025-10-05 20:05:19.576040: val_loss -0.422
2025-10-05 20:05:19.576174: Pseudo dice [np.float32(0.7064)]
2025-10-05 20:05:19.576322: Epoch time: 45.89 s
2025-10-05 20:05:20.197038: 
2025-10-05 20:05:20.197352: Epoch 83
2025-10-05 20:05:20.197520: Current learning rate: 0.00484
2025-10-05 20:06:06.114240: Validation loss did not improve from -0.50692. Patience: 41/50
2025-10-05 20:06:06.114621: train_loss -0.7834
2025-10-05 20:06:06.114808: val_loss -0.4024
2025-10-05 20:06:06.114968: Pseudo dice [np.float32(0.6858)]
2025-10-05 20:06:06.115124: Epoch time: 45.92 s
2025-10-05 20:06:06.735112: 
2025-10-05 20:06:06.735318: Epoch 84
2025-10-05 20:06:06.735490: Current learning rate: 0.00478
2025-10-05 20:06:52.615528: Validation loss did not improve from -0.50692. Patience: 42/50
2025-10-05 20:06:52.616078: train_loss -0.7885
2025-10-05 20:06:52.616224: val_loss -0.4301
2025-10-05 20:06:52.616353: Pseudo dice [np.float32(0.6957)]
2025-10-05 20:06:52.616492: Epoch time: 45.88 s
2025-10-05 20:06:53.683298: 
2025-10-05 20:06:53.683635: Epoch 85
2025-10-05 20:06:53.683867: Current learning rate: 0.00471
2025-10-05 20:07:39.583512: Validation loss did not improve from -0.50692. Patience: 43/50
2025-10-05 20:07:39.583888: train_loss -0.784
2025-10-05 20:07:39.584101: val_loss -0.4246
2025-10-05 20:07:39.584221: Pseudo dice [np.float32(0.6998)]
2025-10-05 20:07:39.584576: Epoch time: 45.9 s
2025-10-05 20:07:40.205425: 
2025-10-05 20:07:40.205740: Epoch 86
2025-10-05 20:07:40.205942: Current learning rate: 0.00465
2025-10-05 20:08:26.108012: Validation loss did not improve from -0.50692. Patience: 44/50
2025-10-05 20:08:26.108555: train_loss -0.7843
2025-10-05 20:08:26.108701: val_loss -0.4313
2025-10-05 20:08:26.108812: Pseudo dice [np.float32(0.7049)]
2025-10-05 20:08:26.108982: Epoch time: 45.9 s
2025-10-05 20:08:26.731246: 
2025-10-05 20:08:26.731539: Epoch 87
2025-10-05 20:08:26.731703: Current learning rate: 0.00458
2025-10-05 20:09:12.616251: Validation loss did not improve from -0.50692. Patience: 45/50
2025-10-05 20:09:12.616719: train_loss -0.7941
2025-10-05 20:09:12.616948: val_loss -0.4243
2025-10-05 20:09:12.617113: Pseudo dice [np.float32(0.6826)]
2025-10-05 20:09:12.617269: Epoch time: 45.89 s
2025-10-05 20:09:13.580778: 
2025-10-05 20:09:13.581127: Epoch 88
2025-10-05 20:09:13.581262: Current learning rate: 0.00452
2025-10-05 20:09:59.506759: Validation loss did not improve from -0.50692. Patience: 46/50
2025-10-05 20:09:59.507220: train_loss -0.7993
2025-10-05 20:09:59.507375: val_loss -0.407
2025-10-05 20:09:59.507501: Pseudo dice [np.float32(0.6858)]
2025-10-05 20:09:59.507640: Epoch time: 45.93 s
2025-10-05 20:10:00.127534: 
2025-10-05 20:10:00.127847: Epoch 89
2025-10-05 20:10:00.127987: Current learning rate: 0.00445
2025-10-05 20:10:46.015941: Validation loss did not improve from -0.50692. Patience: 47/50
2025-10-05 20:10:46.016464: train_loss -0.7963
2025-10-05 20:10:46.016709: val_loss -0.4229
2025-10-05 20:10:46.016914: Pseudo dice [np.float32(0.6919)]
2025-10-05 20:10:46.017115: Epoch time: 45.89 s
2025-10-05 20:10:47.097289: 
2025-10-05 20:10:47.097555: Epoch 90
2025-10-05 20:10:47.097726: Current learning rate: 0.00438
2025-10-05 20:11:32.994939: Validation loss did not improve from -0.50692. Patience: 48/50
2025-10-05 20:11:32.995672: train_loss -0.7994
2025-10-05 20:11:32.995855: val_loss -0.3861
2025-10-05 20:11:32.996043: Pseudo dice [np.float32(0.6831)]
2025-10-05 20:11:32.996261: Epoch time: 45.9 s
2025-10-05 20:11:33.618621: 
2025-10-05 20:11:33.618862: Epoch 91
2025-10-05 20:11:33.619076: Current learning rate: 0.00432
2025-10-05 20:12:19.462750: Validation loss did not improve from -0.50692. Patience: 49/50
2025-10-05 20:12:19.463169: train_loss -0.8026
2025-10-05 20:12:19.463321: val_loss -0.4265
2025-10-05 20:12:19.463464: Pseudo dice [np.float32(0.6956)]
2025-10-05 20:12:19.463586: Epoch time: 45.85 s
2025-10-05 20:12:20.085558: 
2025-10-05 20:12:20.085825: Epoch 92
2025-10-05 20:12:20.085965: Current learning rate: 0.00425
2025-10-05 20:13:05.959897: Validation loss did not improve from -0.50692. Patience: 50/50
2025-10-05 20:13:05.960970: train_loss -0.8038
2025-10-05 20:13:05.961293: val_loss -0.3983
2025-10-05 20:13:05.961490: Pseudo dice [np.float32(0.6871)]
2025-10-05 20:13:05.961624: Epoch time: 45.88 s
2025-10-05 20:13:06.589185: 
2025-10-05 20:13:06.589593: Epoch 93
2025-10-05 20:13:06.589936: Current learning rate: 0.00419
2025-10-05 20:13:52.499506: Validation loss did not improve from -0.50692. Patience: 51/50
2025-10-05 20:13:52.499878: train_loss -0.7978
2025-10-05 20:13:52.500040: val_loss -0.4475
2025-10-05 20:13:52.500190: Pseudo dice [np.float32(0.7185)]
2025-10-05 20:13:52.500334: Epoch time: 45.91 s
2025-10-05 20:13:53.126559: 
2025-10-05 20:13:53.126886: Epoch 94
2025-10-05 20:13:53.127039: Current learning rate: 0.00412
2025-10-05 20:14:39.041876: Validation loss did not improve from -0.50692. Patience: 52/50
2025-10-05 20:14:39.042444: train_loss -0.8119
2025-10-05 20:14:39.042605: val_loss -0.4183
2025-10-05 20:14:39.042742: Pseudo dice [np.float32(0.7048)]
2025-10-05 20:14:39.042884: Epoch time: 45.92 s
2025-10-05 20:14:40.109924: 
2025-10-05 20:14:40.110176: Epoch 95
2025-10-05 20:14:40.110420: Current learning rate: 0.00405
2025-10-05 20:15:26.048383: Validation loss did not improve from -0.50692. Patience: 53/50
2025-10-05 20:15:26.049316: train_loss -0.8084
2025-10-05 20:15:26.049851: val_loss -0.4127
2025-10-05 20:15:26.050601: Pseudo dice [np.float32(0.6894)]
2025-10-05 20:15:26.050904: Epoch time: 45.94 s
2025-10-05 20:15:26.683189: 
2025-10-05 20:15:26.683444: Epoch 96
2025-10-05 20:15:26.683614: Current learning rate: 0.00399
2025-10-05 20:16:12.595448: Validation loss did not improve from -0.50692. Patience: 54/50
2025-10-05 20:16:12.596090: train_loss -0.8118
2025-10-05 20:16:12.596273: val_loss -0.4442
2025-10-05 20:16:12.596427: Pseudo dice [np.float32(0.7035)]
2025-10-05 20:16:12.596548: Epoch time: 45.91 s
2025-10-05 20:16:13.222492: 
2025-10-05 20:16:13.222730: Epoch 97
2025-10-05 20:16:13.222886: Current learning rate: 0.00392
2025-10-05 20:16:59.147533: Validation loss did not improve from -0.50692. Patience: 55/50
2025-10-05 20:16:59.147830: train_loss -0.8146
2025-10-05 20:16:59.147963: val_loss -0.4401
2025-10-05 20:16:59.148101: Pseudo dice [np.float32(0.7108)]
2025-10-05 20:16:59.148326: Epoch time: 45.93 s
2025-10-05 20:16:59.776380: 
2025-10-05 20:16:59.776614: Epoch 98
2025-10-05 20:16:59.776777: Current learning rate: 0.00385
2025-10-05 20:17:45.715397: Validation loss did not improve from -0.50692. Patience: 56/50
2025-10-05 20:17:45.715955: train_loss -0.817
2025-10-05 20:17:45.716086: val_loss -0.4097
2025-10-05 20:17:45.716223: Pseudo dice [np.float32(0.6918)]
2025-10-05 20:17:45.716343: Epoch time: 45.94 s
2025-10-05 20:17:46.341706: 
2025-10-05 20:17:46.341950: Epoch 99
2025-10-05 20:17:46.342089: Current learning rate: 0.00379
2025-10-05 20:18:32.275477: Validation loss did not improve from -0.50692. Patience: 57/50
2025-10-05 20:18:32.275911: train_loss -0.8211
2025-10-05 20:18:32.276078: val_loss -0.4246
2025-10-05 20:18:32.276186: Pseudo dice [np.float32(0.7032)]
2025-10-05 20:18:32.276314: Epoch time: 45.93 s
2025-10-05 20:18:33.339735: 
2025-10-05 20:18:33.340034: Epoch 100
2025-10-05 20:18:33.340211: Current learning rate: 0.00372
2025-10-05 20:19:19.261858: Validation loss did not improve from -0.50692. Patience: 58/50
2025-10-05 20:19:19.262443: train_loss -0.8188
2025-10-05 20:19:19.262618: val_loss -0.3903
2025-10-05 20:19:19.262749: Pseudo dice [np.float32(0.6813)]
2025-10-05 20:19:19.262892: Epoch time: 45.92 s
2025-10-05 20:19:19.891711: 
2025-10-05 20:19:19.891916: Epoch 101
2025-10-05 20:19:19.892081: Current learning rate: 0.00365
2025-10-05 20:20:05.825581: Validation loss did not improve from -0.50692. Patience: 59/50
2025-10-05 20:20:05.826008: train_loss -0.8209
2025-10-05 20:20:05.826186: val_loss -0.3743
2025-10-05 20:20:05.826324: Pseudo dice [np.float32(0.6761)]
2025-10-05 20:20:05.826507: Epoch time: 45.93 s
2025-10-05 20:20:06.455901: 
2025-10-05 20:20:06.456115: Epoch 102
2025-10-05 20:20:06.456282: Current learning rate: 0.00359
2025-10-05 20:20:52.389479: Validation loss did not improve from -0.50692. Patience: 60/50
2025-10-05 20:20:52.390227: train_loss -0.8205
2025-10-05 20:20:52.390472: val_loss -0.4128
2025-10-05 20:20:52.390683: Pseudo dice [np.float32(0.6883)]
2025-10-05 20:20:52.390891: Epoch time: 45.93 s
2025-10-05 20:20:53.017743: 
2025-10-05 20:20:53.018078: Epoch 103
2025-10-05 20:20:53.018301: Current learning rate: 0.00352
2025-10-05 20:21:38.937067: Validation loss did not improve from -0.50692. Patience: 61/50
2025-10-05 20:21:38.937527: train_loss -0.8234
2025-10-05 20:21:38.937803: val_loss -0.4377
2025-10-05 20:21:38.938032: Pseudo dice [np.float32(0.704)]
2025-10-05 20:21:38.938303: Epoch time: 45.92 s
2025-10-05 20:21:39.908849: 
2025-10-05 20:21:39.909163: Epoch 104
2025-10-05 20:21:39.909350: Current learning rate: 0.00345
2025-10-05 20:22:25.835761: Validation loss did not improve from -0.50692. Patience: 62/50
2025-10-05 20:22:25.836333: train_loss -0.8238
2025-10-05 20:22:25.836474: val_loss -0.407
2025-10-05 20:22:25.836617: Pseudo dice [np.float32(0.6961)]
2025-10-05 20:22:25.836743: Epoch time: 45.93 s
2025-10-05 20:22:26.902027: 
2025-10-05 20:22:26.902311: Epoch 105
2025-10-05 20:22:26.902480: Current learning rate: 0.00338
2025-10-05 20:23:12.806073: Validation loss did not improve from -0.50692. Patience: 63/50
2025-10-05 20:23:12.806591: train_loss -0.8236
2025-10-05 20:23:12.806817: val_loss -0.4259
2025-10-05 20:23:12.807094: Pseudo dice [np.float32(0.7036)]
2025-10-05 20:23:12.807303: Epoch time: 45.91 s
2025-10-05 20:23:13.434470: 
2025-10-05 20:23:13.434751: Epoch 106
2025-10-05 20:23:13.434989: Current learning rate: 0.00332
2025-10-05 20:23:59.294758: Validation loss did not improve from -0.50692. Patience: 64/50
2025-10-05 20:23:59.295167: train_loss -0.8284
2025-10-05 20:23:59.295320: val_loss -0.4138
2025-10-05 20:23:59.295453: Pseudo dice [np.float32(0.7041)]
2025-10-05 20:23:59.295587: Epoch time: 45.86 s
2025-10-05 20:23:59.925972: 
2025-10-05 20:23:59.926281: Epoch 107
2025-10-05 20:23:59.926507: Current learning rate: 0.00325
2025-10-05 20:24:45.832464: Validation loss did not improve from -0.50692. Patience: 65/50
2025-10-05 20:24:45.832866: train_loss -0.8238
2025-10-05 20:24:45.833025: val_loss -0.4307
2025-10-05 20:24:45.833135: Pseudo dice [np.float32(0.7122)]
2025-10-05 20:24:45.833258: Epoch time: 45.91 s
2025-10-05 20:24:46.462594: 
2025-10-05 20:24:46.462886: Epoch 108
2025-10-05 20:24:46.463048: Current learning rate: 0.00318
2025-10-05 20:25:32.354735: Validation loss did not improve from -0.50692. Patience: 66/50
2025-10-05 20:25:32.355283: train_loss -0.8223
2025-10-05 20:25:32.355514: val_loss -0.4322
2025-10-05 20:25:32.355623: Pseudo dice [np.float32(0.7031)]
2025-10-05 20:25:32.355741: Epoch time: 45.89 s
2025-10-05 20:25:32.986692: 
2025-10-05 20:25:32.986951: Epoch 109
2025-10-05 20:25:32.987100: Current learning rate: 0.00311
2025-10-05 20:26:18.927048: Validation loss did not improve from -0.50692. Patience: 67/50
2025-10-05 20:26:18.927462: train_loss -0.8277
2025-10-05 20:26:18.927638: val_loss -0.4052
2025-10-05 20:26:18.927788: Pseudo dice [np.float32(0.7053)]
2025-10-05 20:26:18.927931: Epoch time: 45.94 s
2025-10-05 20:26:20.000633: 
2025-10-05 20:26:20.000837: Epoch 110
2025-10-05 20:26:20.000972: Current learning rate: 0.00304
2025-10-05 20:27:05.864616: Validation loss did not improve from -0.50692. Patience: 68/50
2025-10-05 20:27:05.865205: train_loss -0.8289
2025-10-05 20:27:05.865461: val_loss -0.3906
2025-10-05 20:27:05.865609: Pseudo dice [np.float32(0.6908)]
2025-10-05 20:27:05.865757: Epoch time: 45.87 s
2025-10-05 20:27:06.497305: 
2025-10-05 20:27:06.497512: Epoch 111
2025-10-05 20:27:06.497706: Current learning rate: 0.00297
2025-10-05 20:27:52.443996: Validation loss did not improve from -0.50692. Patience: 69/50
2025-10-05 20:27:52.444333: train_loss -0.8289
2025-10-05 20:27:52.444498: val_loss -0.4258
2025-10-05 20:27:52.444665: Pseudo dice [np.float32(0.6991)]
2025-10-05 20:27:52.444817: Epoch time: 45.95 s
2025-10-05 20:27:53.075734: 
2025-10-05 20:27:53.075965: Epoch 112
2025-10-05 20:27:53.076108: Current learning rate: 0.00291
2025-10-05 20:28:38.968771: Validation loss did not improve from -0.50692. Patience: 70/50
2025-10-05 20:28:38.969234: train_loss -0.8317
2025-10-05 20:28:38.969420: val_loss -0.4055
2025-10-05 20:28:38.969537: Pseudo dice [np.float32(0.6954)]
2025-10-05 20:28:38.969676: Epoch time: 45.89 s
2025-10-05 20:28:39.600498: 
2025-10-05 20:28:39.600808: Epoch 113
2025-10-05 20:28:39.600999: Current learning rate: 0.00284
2025-10-05 20:29:25.501104: Validation loss did not improve from -0.50692. Patience: 71/50
2025-10-05 20:29:25.501526: train_loss -0.8343
2025-10-05 20:29:25.501669: val_loss -0.3993
2025-10-05 20:29:25.501797: Pseudo dice [np.float32(0.7081)]
2025-10-05 20:29:25.501937: Epoch time: 45.9 s
2025-10-05 20:29:26.129253: 
2025-10-05 20:29:26.129496: Epoch 114
2025-10-05 20:29:26.129666: Current learning rate: 0.00277
2025-10-05 20:30:12.050832: Validation loss did not improve from -0.50692. Patience: 72/50
2025-10-05 20:30:12.051628: train_loss -0.8288
2025-10-05 20:30:12.051838: val_loss -0.4062
2025-10-05 20:30:12.052033: Pseudo dice [np.float32(0.6958)]
2025-10-05 20:30:12.052241: Epoch time: 45.92 s
2025-10-05 20:30:13.124310: 
2025-10-05 20:30:13.124567: Epoch 115
2025-10-05 20:30:13.124752: Current learning rate: 0.0027
2025-10-05 20:30:59.045130: Validation loss did not improve from -0.50692. Patience: 73/50
2025-10-05 20:30:59.045550: train_loss -0.832
2025-10-05 20:30:59.045732: val_loss -0.4189
2025-10-05 20:30:59.045843: Pseudo dice [np.float32(0.6978)]
2025-10-05 20:30:59.045997: Epoch time: 45.92 s
2025-10-05 20:30:59.680893: 
2025-10-05 20:30:59.681192: Epoch 116
2025-10-05 20:30:59.681387: Current learning rate: 0.00263
2025-10-05 20:31:45.601067: Validation loss did not improve from -0.50692. Patience: 74/50
2025-10-05 20:31:45.601676: train_loss -0.8353
2025-10-05 20:31:45.601809: val_loss -0.4104
2025-10-05 20:31:45.601915: Pseudo dice [np.float32(0.6919)]
2025-10-05 20:31:45.602034: Epoch time: 45.92 s
2025-10-05 20:31:46.234280: 
2025-10-05 20:31:46.234508: Epoch 117
2025-10-05 20:31:46.234642: Current learning rate: 0.00256
2025-10-05 20:32:32.191568: Validation loss did not improve from -0.50692. Patience: 75/50
2025-10-05 20:32:32.191969: train_loss -0.8398
2025-10-05 20:32:32.192123: val_loss -0.3827
2025-10-05 20:32:32.192237: Pseudo dice [np.float32(0.686)]
2025-10-05 20:32:32.192387: Epoch time: 45.96 s
2025-10-05 20:32:32.824229: 
2025-10-05 20:32:32.824425: Epoch 118
2025-10-05 20:32:32.824564: Current learning rate: 0.00249
2025-10-05 20:33:18.739489: Validation loss did not improve from -0.50692. Patience: 76/50
2025-10-05 20:33:18.740220: train_loss -0.8413
2025-10-05 20:33:18.740497: val_loss -0.413
2025-10-05 20:33:18.740670: Pseudo dice [np.float32(0.6904)]
2025-10-05 20:33:18.740847: Epoch time: 45.92 s
2025-10-05 20:33:19.722562: 
2025-10-05 20:33:19.722866: Epoch 119
2025-10-05 20:33:19.723029: Current learning rate: 0.00242
2025-10-05 20:34:05.693486: Validation loss did not improve from -0.50692. Patience: 77/50
2025-10-05 20:34:05.693909: train_loss -0.8448
2025-10-05 20:34:05.694059: val_loss -0.4099
2025-10-05 20:34:05.694214: Pseudo dice [np.float32(0.7024)]
2025-10-05 20:34:05.694433: Epoch time: 45.97 s
2025-10-05 20:34:06.776650: 
2025-10-05 20:34:06.776980: Epoch 120
2025-10-05 20:34:06.777134: Current learning rate: 0.00235
2025-10-05 20:34:52.673974: Validation loss did not improve from -0.50692. Patience: 78/50
2025-10-05 20:34:52.674580: train_loss -0.8451
2025-10-05 20:34:52.674782: val_loss -0.3992
2025-10-05 20:34:52.674920: Pseudo dice [np.float32(0.6969)]
2025-10-05 20:34:52.675087: Epoch time: 45.9 s
2025-10-05 20:34:53.313321: 
2025-10-05 20:34:53.313601: Epoch 121
2025-10-05 20:34:53.313770: Current learning rate: 0.00228
2025-10-05 20:35:39.215041: Validation loss did not improve from -0.50692. Patience: 79/50
2025-10-05 20:35:39.215488: train_loss -0.8414
2025-10-05 20:35:39.215641: val_loss -0.3965
2025-10-05 20:35:39.215775: Pseudo dice [np.float32(0.7041)]
2025-10-05 20:35:39.215924: Epoch time: 45.9 s
2025-10-05 20:35:39.852329: 
2025-10-05 20:35:39.852656: Epoch 122
2025-10-05 20:35:39.852823: Current learning rate: 0.00221
2025-10-05 20:36:25.727959: Validation loss did not improve from -0.50692. Patience: 80/50
2025-10-05 20:36:25.728561: train_loss -0.845
2025-10-05 20:36:25.728730: val_loss -0.4144
2025-10-05 20:36:25.728848: Pseudo dice [np.float32(0.7022)]
2025-10-05 20:36:25.728988: Epoch time: 45.88 s
2025-10-05 20:36:26.367882: 
2025-10-05 20:36:26.368128: Epoch 123
2025-10-05 20:36:26.368273: Current learning rate: 0.00214
2025-10-05 20:37:12.207475: Validation loss did not improve from -0.50692. Patience: 81/50
2025-10-05 20:37:12.207847: train_loss -0.8476
2025-10-05 20:37:12.207985: val_loss -0.3958
2025-10-05 20:37:12.208093: Pseudo dice [np.float32(0.7012)]
2025-10-05 20:37:12.208228: Epoch time: 45.84 s
2025-10-05 20:37:12.843896: 
2025-10-05 20:37:12.844125: Epoch 124
2025-10-05 20:37:12.844300: Current learning rate: 0.00207
2025-10-05 20:37:58.739639: Validation loss did not improve from -0.50692. Patience: 82/50
2025-10-05 20:37:58.740227: train_loss -0.8476
2025-10-05 20:37:58.740426: val_loss -0.4054
2025-10-05 20:37:58.740585: Pseudo dice [np.float32(0.6923)]
2025-10-05 20:37:58.740746: Epoch time: 45.9 s
2025-10-05 20:37:59.847124: 
2025-10-05 20:37:59.847409: Epoch 125
2025-10-05 20:37:59.847586: Current learning rate: 0.00199
2025-10-05 20:38:45.758948: Validation loss did not improve from -0.50692. Patience: 83/50
2025-10-05 20:38:45.759395: train_loss -0.8477
2025-10-05 20:38:45.759538: val_loss -0.374
2025-10-05 20:38:45.759657: Pseudo dice [np.float32(0.6801)]
2025-10-05 20:38:45.759775: Epoch time: 45.91 s
2025-10-05 20:38:46.394132: 
2025-10-05 20:38:46.394420: Epoch 126
2025-10-05 20:38:46.394562: Current learning rate: 0.00192
2025-10-05 20:39:32.272870: Validation loss did not improve from -0.50692. Patience: 84/50
2025-10-05 20:39:32.273426: train_loss -0.8463
2025-10-05 20:39:32.273555: val_loss -0.4282
2025-10-05 20:39:32.273744: Pseudo dice [np.float32(0.7154)]
2025-10-05 20:39:32.273863: Epoch time: 45.88 s
2025-10-05 20:39:32.909099: 
2025-10-05 20:39:32.909366: Epoch 127
2025-10-05 20:39:32.909524: Current learning rate: 0.00185
2025-10-05 20:40:18.792278: Validation loss did not improve from -0.50692. Patience: 85/50
2025-10-05 20:40:18.792687: train_loss -0.8571
2025-10-05 20:40:18.792847: val_loss -0.4126
2025-10-05 20:40:18.792953: Pseudo dice [np.float32(0.7091)]
2025-10-05 20:40:18.793071: Epoch time: 45.88 s
2025-10-05 20:40:19.433666: 
2025-10-05 20:40:19.434026: Epoch 128
2025-10-05 20:40:19.434340: Current learning rate: 0.00178
2025-10-05 20:41:05.322920: Validation loss did not improve from -0.50692. Patience: 86/50
2025-10-05 20:41:05.323513: train_loss -0.8499
2025-10-05 20:41:05.323647: val_loss -0.4029
2025-10-05 20:41:05.323756: Pseudo dice [np.float32(0.7007)]
2025-10-05 20:41:05.323906: Epoch time: 45.89 s
2025-10-05 20:41:05.956951: 
2025-10-05 20:41:05.957250: Epoch 129
2025-10-05 20:41:05.957422: Current learning rate: 0.0017
2025-10-05 20:41:51.783311: Validation loss did not improve from -0.50692. Patience: 87/50
2025-10-05 20:41:51.783718: train_loss -0.8517
2025-10-05 20:41:51.783883: val_loss -0.4054
2025-10-05 20:41:51.784030: Pseudo dice [np.float32(0.6985)]
2025-10-05 20:41:51.784314: Epoch time: 45.83 s
2025-10-05 20:41:52.839463: 
2025-10-05 20:41:52.839739: Epoch 130
2025-10-05 20:41:52.839915: Current learning rate: 0.00163
2025-10-05 20:42:38.735616: Validation loss did not improve from -0.50692. Patience: 88/50
2025-10-05 20:42:38.736249: train_loss -0.8559
2025-10-05 20:42:38.736400: val_loss -0.3694
2025-10-05 20:42:38.736540: Pseudo dice [np.float32(0.6848)]
2025-10-05 20:42:38.736670: Epoch time: 45.9 s
2025-10-05 20:42:39.365862: 
2025-10-05 20:42:39.366178: Epoch 131
2025-10-05 20:42:39.366401: Current learning rate: 0.00156
2025-10-05 20:43:25.252364: Validation loss did not improve from -0.50692. Patience: 89/50
2025-10-05 20:43:25.252802: train_loss -0.8557
2025-10-05 20:43:25.252978: val_loss -0.4221
2025-10-05 20:43:25.253105: Pseudo dice [np.float32(0.7057)]
2025-10-05 20:43:25.253255: Epoch time: 45.89 s
2025-10-05 20:43:25.880325: 
2025-10-05 20:43:25.880687: Epoch 132
2025-10-05 20:43:25.880871: Current learning rate: 0.00148
2025-10-05 20:44:11.736018: Validation loss did not improve from -0.50692. Patience: 90/50
2025-10-05 20:44:11.736556: train_loss -0.8559
2025-10-05 20:44:11.736783: val_loss -0.3728
2025-10-05 20:44:11.736931: Pseudo dice [np.float32(0.69)]
2025-10-05 20:44:11.737087: Epoch time: 45.86 s
2025-10-05 20:44:12.366853: 
2025-10-05 20:44:12.367157: Epoch 133
2025-10-05 20:44:12.367368: Current learning rate: 0.00141
2025-10-05 20:44:58.309972: Validation loss did not improve from -0.50692. Patience: 91/50
2025-10-05 20:44:58.310403: train_loss -0.8574
2025-10-05 20:44:58.310542: val_loss -0.3873
2025-10-05 20:44:58.310704: Pseudo dice [np.float32(0.6913)]
2025-10-05 20:44:58.310862: Epoch time: 45.94 s
2025-10-05 20:44:59.290591: 
2025-10-05 20:44:59.290809: Epoch 134
2025-10-05 20:44:59.291016: Current learning rate: 0.00133
2025-10-05 20:45:45.186042: Validation loss did not improve from -0.50692. Patience: 92/50
2025-10-05 20:45:45.186585: train_loss -0.8564
2025-10-05 20:45:45.186768: val_loss -0.3857
2025-10-05 20:45:45.186893: Pseudo dice [np.float32(0.7039)]
2025-10-05 20:45:45.187061: Epoch time: 45.9 s
2025-10-05 20:45:46.270516: 
2025-10-05 20:45:46.270855: Epoch 135
2025-10-05 20:45:46.271061: Current learning rate: 0.00126
2025-10-05 20:46:32.224219: Validation loss did not improve from -0.50692. Patience: 93/50
2025-10-05 20:46:32.224590: train_loss -0.8563
2025-10-05 20:46:32.224726: val_loss -0.4041
2025-10-05 20:46:32.224839: Pseudo dice [np.float32(0.7078)]
2025-10-05 20:46:32.224976: Epoch time: 45.95 s
2025-10-05 20:46:32.859787: 
2025-10-05 20:46:32.859996: Epoch 136
2025-10-05 20:46:32.860126: Current learning rate: 0.00118
2025-10-05 20:47:18.706075: Validation loss did not improve from -0.50692. Patience: 94/50
2025-10-05 20:47:18.706636: train_loss -0.8572
2025-10-05 20:47:18.706828: val_loss -0.4003
2025-10-05 20:47:18.706961: Pseudo dice [np.float32(0.6999)]
2025-10-05 20:47:18.707100: Epoch time: 45.85 s
2025-10-05 20:47:19.342267: 
2025-10-05 20:47:19.342539: Epoch 137
2025-10-05 20:47:19.342718: Current learning rate: 0.00111
2025-10-05 20:48:05.195972: Validation loss did not improve from -0.50692. Patience: 95/50
2025-10-05 20:48:05.196359: train_loss -0.8594
2025-10-05 20:48:05.196514: val_loss -0.3764
2025-10-05 20:48:05.196621: Pseudo dice [np.float32(0.702)]
2025-10-05 20:48:05.196793: Epoch time: 45.85 s
2025-10-05 20:48:05.832123: 
2025-10-05 20:48:05.832367: Epoch 138
2025-10-05 20:48:05.832514: Current learning rate: 0.00103
2025-10-05 20:48:51.720383: Validation loss did not improve from -0.50692. Patience: 96/50
2025-10-05 20:48:51.720900: train_loss -0.862
2025-10-05 20:48:51.721066: val_loss -0.4267
2025-10-05 20:48:51.721175: Pseudo dice [np.float32(0.7112)]
2025-10-05 20:48:51.721293: Epoch time: 45.89 s
2025-10-05 20:48:51.721400: Yayy! New best EMA pseudo Dice: 0.7003999948501587
2025-10-05 20:48:52.790517: 
2025-10-05 20:48:52.790824: Epoch 139
2025-10-05 20:48:52.791035: Current learning rate: 0.00095
2025-10-05 20:49:38.751648: Validation loss did not improve from -0.50692. Patience: 97/50
2025-10-05 20:49:38.752041: train_loss -0.8605
2025-10-05 20:49:38.752199: val_loss -0.4082
2025-10-05 20:49:38.752355: Pseudo dice [np.float32(0.7106)]
2025-10-05 20:49:38.752482: Epoch time: 45.96 s
2025-10-05 20:49:39.195317: Yayy! New best EMA pseudo Dice: 0.7013999819755554
2025-10-05 20:49:40.254841: 
2025-10-05 20:49:40.255127: Epoch 140
2025-10-05 20:49:40.255336: Current learning rate: 0.00087
2025-10-05 20:50:26.187278: Validation loss did not improve from -0.50692. Patience: 98/50
2025-10-05 20:50:26.187955: train_loss -0.8617
2025-10-05 20:50:26.188098: val_loss -0.3971
2025-10-05 20:50:26.188246: Pseudo dice [np.float32(0.7146)]
2025-10-05 20:50:26.188370: Epoch time: 45.93 s
2025-10-05 20:50:26.188477: Yayy! New best EMA pseudo Dice: 0.7027000188827515
2025-10-05 20:50:27.293962: 
2025-10-05 20:50:27.294205: Epoch 141
2025-10-05 20:50:27.294393: Current learning rate: 0.00079
2025-10-05 20:51:13.201363: Validation loss did not improve from -0.50692. Patience: 99/50
2025-10-05 20:51:13.201716: train_loss -0.865
2025-10-05 20:51:13.201878: val_loss -0.3882
2025-10-05 20:51:13.202036: Pseudo dice [np.float32(0.7045)]
2025-10-05 20:51:13.202178: Epoch time: 45.91 s
2025-10-05 20:51:13.202310: Yayy! New best EMA pseudo Dice: 0.7028999924659729
2025-10-05 20:51:14.285280: 
2025-10-05 20:51:14.285508: Epoch 142
2025-10-05 20:51:14.285648: Current learning rate: 0.00071
2025-10-05 20:52:00.215794: Validation loss did not improve from -0.50692. Patience: 100/50
2025-10-05 20:52:00.216428: train_loss -0.8607
2025-10-05 20:52:00.216719: val_loss -0.3768
2025-10-05 20:52:00.216943: Pseudo dice [np.float32(0.6897)]
2025-10-05 20:52:00.217184: Epoch time: 45.93 s
2025-10-05 20:52:00.857120: 
2025-10-05 20:52:00.857379: Epoch 143
2025-10-05 20:52:00.857524: Current learning rate: 0.00063
2025-10-05 20:52:46.765993: Validation loss did not improve from -0.50692. Patience: 101/50
2025-10-05 20:52:46.766349: train_loss -0.8644
2025-10-05 20:52:46.766492: val_loss -0.4075
2025-10-05 20:52:46.766609: Pseudo dice [np.float32(0.6991)]
2025-10-05 20:52:46.766761: Epoch time: 45.91 s
2025-10-05 20:52:47.401741: 
2025-10-05 20:52:47.401961: Epoch 144
2025-10-05 20:52:47.402136: Current learning rate: 0.00055
2025-10-05 20:53:33.248737: Validation loss did not improve from -0.50692. Patience: 102/50
2025-10-05 20:53:33.249303: train_loss -0.8604
2025-10-05 20:53:33.249481: val_loss -0.3955
2025-10-05 20:53:33.249700: Pseudo dice [np.float32(0.6982)]
2025-10-05 20:53:33.249840: Epoch time: 45.85 s
2025-10-05 20:53:34.342270: 
2025-10-05 20:53:34.342583: Epoch 145
2025-10-05 20:53:34.342746: Current learning rate: 0.00047
2025-10-05 20:54:20.254542: Validation loss did not improve from -0.50692. Patience: 103/50
2025-10-05 20:54:20.254980: train_loss -0.8631
2025-10-05 20:54:20.255193: val_loss -0.3971
2025-10-05 20:54:20.255368: Pseudo dice [np.float32(0.7027)]
2025-10-05 20:54:20.255485: Epoch time: 45.91 s
2025-10-05 20:54:20.898771: 
2025-10-05 20:54:20.898991: Epoch 146
2025-10-05 20:54:20.899138: Current learning rate: 0.00038
2025-10-05 20:55:06.787640: Validation loss did not improve from -0.50692. Patience: 104/50
2025-10-05 20:55:06.788601: train_loss -0.8693
2025-10-05 20:55:06.788911: val_loss -0.3872
2025-10-05 20:55:06.789180: Pseudo dice [np.float32(0.7018)]
2025-10-05 20:55:06.789459: Epoch time: 45.89 s
2025-10-05 20:55:07.431218: 
2025-10-05 20:55:07.431461: Epoch 147
2025-10-05 20:55:07.431618: Current learning rate: 0.0003
2025-10-05 20:55:53.310719: Validation loss did not improve from -0.50692. Patience: 105/50
2025-10-05 20:55:53.311123: train_loss -0.8685
2025-10-05 20:55:53.311267: val_loss -0.3661
2025-10-05 20:55:53.311395: Pseudo dice [np.float32(0.6888)]
2025-10-05 20:55:53.311540: Epoch time: 45.88 s
2025-10-05 20:55:53.950992: 
2025-10-05 20:55:53.951293: Epoch 148
2025-10-05 20:55:53.951431: Current learning rate: 0.00021
2025-10-05 20:56:39.844477: Validation loss did not improve from -0.50692. Patience: 106/50
2025-10-05 20:56:39.845075: train_loss -0.8662
2025-10-05 20:56:39.845218: val_loss -0.4058
2025-10-05 20:56:39.845348: Pseudo dice [np.float32(0.7138)]
2025-10-05 20:56:39.845502: Epoch time: 45.89 s
2025-10-05 20:56:40.840816: 
2025-10-05 20:56:40.841099: Epoch 149
2025-10-05 20:56:40.841287: Current learning rate: 0.00011
2025-10-05 20:57:26.791181: Validation loss did not improve from -0.50692. Patience: 107/50
2025-10-05 20:57:26.791626: train_loss -0.8644
2025-10-05 20:57:26.791782: val_loss -0.3918
2025-10-05 20:57:26.791915: Pseudo dice [np.float32(0.6939)]
2025-10-05 20:57:26.792048: Epoch time: 45.95 s
2025-10-05 20:57:27.884491: Training done.
2025-10-05 20:57:27.924142: Using splits from existing split file: /nfs/erelab001/shared/Computational_Group/Naravich/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset310_nnInteractive_Calcium_OCT_CrossValidation/splits_final_60.json
2025-10-05 20:57:27.925023: The split file contains 5 splits.
2025-10-05 20:57:27.925447: Desired fold for training: 2
2025-10-05 20:57:27.925747: This split has 4 training and 4 validation cases.
2025-10-05 20:57:27.926605: predicting 101-044
2025-10-05 20:57:27.931797: 101-044, shape torch.Size([1, 404, 498, 498]), rank 0
2025-10-05 20:58:16.757762: predicting 101-045
2025-10-05 20:58:16.769997: 101-045, shape torch.Size([1, 375, 498, 498]), rank 0
2025-10-05 20:58:50.532532: predicting 704-003
2025-10-05 20:58:50.542499: 704-003, shape torch.Size([1, 375, 498, 498]), rank 0
2025-10-05 20:59:24.312729: predicting 706-005
2025-10-05 20:59:24.322636: 706-005, shape torch.Size([1, 375, 498, 498]), rank 0
2025-10-05 21:00:10.394432: Validation complete
2025-10-05 21:00:10.394677: Mean Validation Dice:  0.6880527022607268
Finished training fold 2 saving to /nfs/erelab001/shared/Computational_Group/Naravich/datasets/nnUNet_Datasets/nnUNet_results/Dataset310_nnInteractive_Calcium_OCT_CrossValidation/nnUNetTrainerScaleAnalysis60__nnUNetPlans__3d_32x160x128_b10/fold_2_No_Pretrained
