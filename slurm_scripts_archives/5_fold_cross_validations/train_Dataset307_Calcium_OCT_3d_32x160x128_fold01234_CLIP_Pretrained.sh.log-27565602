
CommandNotFoundError: Your shell has not been properly configured to use 'conda activate'.
To initialize your shell, run

    $ conda init <SHELL_NAME>

Currently supported shells are:
  - bash
  - fish
  - tcsh
  - xonsh
  - zsh
  - powershell

See 'conda init --help' for more information and options.

IMPORTANT: You may need to close and restart your shell after running 'conda init'.


/home/gridsan/nchutisilp/.conda/envs/nnunet/bin/python
Starting training with CONFIG=3d_32x160x128_b10, DATASET_ID=307, TRAINER=nnUNetTrainer

############################
INFO: You are using the old nnU-Net default plans. We have updated our recommendations. Please consider using those instead! Read more here: https://github.com/MIC-DKFZ/nnUNet/blob/master/documentation/resenc_presets.md
############################

Using device: cuda:0

#######################################################################
Please cite the following paper when using nnU-Net:
Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.
#######################################################################

2024-12-06 19:58:08.258692: Using torch.compile...

############################
INFO: You are using the old nnU-Net default plans. We have updated our recommendations. Please consider using those instead! Read more here: https://github.com/MIC-DKFZ/nnUNet/blob/master/documentation/resenc_presets.md
############################

Using device: cuda:0

#######################################################################
Please cite the following paper when using nnU-Net:
Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.
#######################################################################

2024-12-06 19:58:08.258654: Using torch.compile...
/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
################### Loading pretrained weights from file  /home/gridsan/nchutisilp/projects/OpenAI-CLIP/clip_pretrained_nnUNet.pt ###################
Below is the list of overlapping blocks in pretrained model and nnUNet architecture:
encoder.stages.0.0.convs.0.conv.weight shape torch.Size([32, 1, 3, 3, 3])
encoder.stages.0.0.convs.0.conv.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.norm.weight shape torch.Size([32])
encoder.stages.0.0.convs.0.norm.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.0.weight shape torch.Size([32, 1, 3, 3, 3])
encoder.stages.0.0.convs.0.all_modules.0.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.1.weight shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.1.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
encoder.stages.0.0.convs.1.conv.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.norm.weight shape torch.Size([32])
encoder.stages.0.0.convs.1.norm.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
encoder.stages.0.0.convs.1.all_modules.0.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.1.weight shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.1.bias shape torch.Size([32])
encoder.stages.1.0.convs.0.conv.weight shape torch.Size([64, 32, 3, 3, 3])
encoder.stages.1.0.convs.0.conv.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.norm.weight shape torch.Size([64])
encoder.stages.1.0.convs.0.norm.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.0.weight shape torch.Size([64, 32, 3, 3, 3])
encoder.stages.1.0.convs.0.all_modules.0.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.1.weight shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.1.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
encoder.stages.1.0.convs.1.conv.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.norm.weight shape torch.Size([64])
encoder.stages.1.0.convs.1.norm.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
encoder.stages.1.0.convs.1.all_modules.0.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.1.weight shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.1.bias shape torch.Size([64])
encoder.stages.2.0.convs.0.conv.weight shape torch.Size([128, 64, 3, 3, 3])
encoder.stages.2.0.convs.0.conv.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.norm.weight shape torch.Size([128])
encoder.stages.2.0.convs.0.norm.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.0.weight shape torch.Size([128, 64, 3, 3, 3])
encoder.stages.2.0.convs.0.all_modules.0.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.1.weight shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.1.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
encoder.stages.2.0.convs.1.conv.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.norm.weight shape torch.Size([128])
encoder.stages.2.0.convs.1.norm.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
encoder.stages.2.0.convs.1.all_modules.0.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.1.weight shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.1.bias shape torch.Size([128])
encoder.stages.3.0.convs.0.conv.weight shape torch.Size([256, 128, 3, 3, 3])
encoder.stages.3.0.convs.0.conv.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.norm.weight shape torch.Size([256])
encoder.stages.3.0.convs.0.norm.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.0.weight shape torch.Size([256, 128, 3, 3, 3])
encoder.stages.3.0.convs.0.all_modules.0.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.1.weight shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.1.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
encoder.stages.3.0.convs.1.conv.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.norm.weight shape torch.Size([256])
encoder.stages.3.0.convs.1.norm.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
encoder.stages.3.0.convs.1.all_modules.0.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.1.weight shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.1.bias shape torch.Size([256])
encoder.stages.4.0.convs.0.conv.weight shape torch.Size([320, 256, 3, 3, 3])
encoder.stages.4.0.convs.0.conv.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.norm.weight shape torch.Size([320])
encoder.stages.4.0.convs.0.norm.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.0.weight shape torch.Size([320, 256, 3, 3, 3])
encoder.stages.4.0.convs.0.all_modules.0.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.1.weight shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.1.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.4.0.convs.1.conv.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.norm.weight shape torch.Size([320])
encoder.stages.4.0.convs.1.norm.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.4.0.convs.1.all_modules.0.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.1.weight shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.1.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.0.conv.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.norm.weight shape torch.Size([320])
encoder.stages.5.0.convs.0.norm.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.0.all_modules.0.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.1.weight shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.1.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.1.conv.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.norm.weight shape torch.Size([320])
encoder.stages.5.0.convs.1.norm.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.1.all_modules.0.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.1.weight shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.0.0.convs.0.conv.weight shape torch.Size([32, 1, 3, 3, 3])
decoder.encoder.stages.0.0.convs.0.conv.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.norm.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.norm.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.0.weight shape torch.Size([32, 1, 3, 3, 3])
decoder.encoder.stages.0.0.convs.0.all_modules.0.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.1.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.1.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.encoder.stages.0.0.convs.1.conv.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.norm.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.norm.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.encoder.stages.0.0.convs.1.all_modules.0.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.1.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.1.bias shape torch.Size([32])
decoder.encoder.stages.1.0.convs.0.conv.weight shape torch.Size([64, 32, 3, 3, 3])
decoder.encoder.stages.1.0.convs.0.conv.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.norm.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.norm.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.0.weight shape torch.Size([64, 32, 3, 3, 3])
decoder.encoder.stages.1.0.convs.0.all_modules.0.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.1.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.1.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.encoder.stages.1.0.convs.1.conv.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.norm.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.norm.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.encoder.stages.1.0.convs.1.all_modules.0.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.1.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.1.bias shape torch.Size([64])
decoder.encoder.stages.2.0.convs.0.conv.weight shape torch.Size([128, 64, 3, 3, 3])
decoder.encoder.stages.2.0.convs.0.conv.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.norm.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.norm.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.0.weight shape torch.Size([128, 64, 3, 3, 3])
decoder.encoder.stages.2.0.convs.0.all_modules.0.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.1.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.1.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.encoder.stages.2.0.convs.1.conv.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.norm.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.norm.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.encoder.stages.2.0.convs.1.all_modules.0.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.1.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.1.bias shape torch.Size([128])
decoder.encoder.stages.3.0.convs.0.conv.weight shape torch.Size([256, 128, 3, 3, 3])
decoder.encoder.stages.3.0.convs.0.conv.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.norm.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.norm.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.0.weight shape torch.Size([256, 128, 3, 3, 3])
decoder.encoder.stages.3.0.convs.0.all_modules.0.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.1.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.1.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.encoder.stages.3.0.convs.1.conv.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.norm.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.norm.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.encoder.stages.3.0.convs.1.all_modules.0.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.1.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.1.bias shape torch.Size([256])
decoder.encoder.stages.4.0.convs.0.conv.weight shape torch.Size([320, 256, 3, 3, 3])
decoder.encoder.stages.4.0.convs.0.conv.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.norm.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.norm.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.0.weight shape torch.Size([320, 256, 3, 3, 3])
decoder.encoder.stages.4.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.4.0.convs.1.conv.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.norm.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.norm.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.4.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.0.conv.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.norm.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.norm.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.1.conv.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.norm.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.norm.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.stages.0.convs.0.conv.weight shape torch.Size([320, 640, 3, 3, 3])
decoder.stages.0.convs.0.conv.bias shape torch.Size([320])
decoder.stages.0.convs.0.norm.weight shape torch.Size([320])
decoder.stages.0.convs.0.norm.bias shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.0.weight shape torch.Size([320, 640, 3, 3, 3])
decoder.stages.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.stages.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.stages.0.convs.1.conv.bias shape torch.Size([320])
decoder.stages.0.convs.1.norm.weight shape torch.Size([320])
decoder.stages.0.convs.1.norm.bias shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.stages.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.stages.1.convs.0.conv.weight shape torch.Size([256, 512, 3, 3, 3])
decoder.stages.1.convs.0.conv.bias shape torch.Size([256])
decoder.stages.1.convs.0.norm.weight shape torch.Size([256])
decoder.stages.1.convs.0.norm.bias shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.0.weight shape torch.Size([256, 512, 3, 3, 3])
decoder.stages.1.convs.0.all_modules.0.bias shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.1.weight shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.1.bias shape torch.Size([256])
decoder.stages.1.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.stages.1.convs.1.conv.bias shape torch.Size([256])
decoder.stages.1.convs.1.norm.weight shape torch.Size([256])
decoder.stages.1.convs.1.norm.bias shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.stages.1.convs.1.all_modules.0.bias shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.1.weight shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.1.bias shape torch.Size([256])
decoder.stages.2.convs.0.conv.weight shape torch.Size([128, 256, 3, 3, 3])
decoder.stages.2.convs.0.conv.bias shape torch.Size([128])
decoder.stages.2.convs.0.norm.weight shape torch.Size([128])
decoder.stages.2.convs.0.norm.bias shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.0.weight shape torch.Size([128, 256, 3, 3, 3])
decoder.stages.2.convs.0.all_modules.0.bias shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.1.weight shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.1.bias shape torch.Size([128])
decoder.stages.2.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.stages.2.convs.1.conv.bias shape torch.Size([128])
decoder.stages.2.convs.1.norm.weight shape torch.Size([128])
decoder.stages.2.convs.1.norm.bias shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.stages.2.convs.1.all_modules.0.bias shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.1.weight shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.1.bias shape torch.Size([128])
decoder.stages.3.convs.0.conv.weight shape torch.Size([64, 128, 3, 3, 3])
decoder.stages.3.convs.0.conv.bias shape torch.Size([64])
decoder.stages.3.convs.0.norm.weight shape torch.Size([64])
decoder.stages.3.convs.0.norm.bias shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.0.weight shape torch.Size([64, 128, 3, 3, 3])
decoder.stages.3.convs.0.all_modules.0.bias shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.1.weight shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.1.bias shape torch.Size([64])
decoder.stages.3.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.stages.3.convs.1.conv.bias shape torch.Size([64])
decoder.stages.3.convs.1.norm.weight shape torch.Size([64])
decoder.stages.3.convs.1.norm.bias shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.stages.3.convs.1.all_modules.0.bias shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.1.weight shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.1.bias shape torch.Size([64])
decoder.stages.4.convs.0.conv.weight shape torch.Size([32, 64, 3, 3, 3])
decoder.stages.4.convs.0.conv.bias shape torch.Size([32])
decoder.stages.4.convs.0.norm.weight shape torch.Size([32])
decoder.stages.4.convs.0.norm.bias shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.0.weight shape torch.Size([32, 64, 3, 3, 3])
decoder.stages.4.convs.0.all_modules.0.bias shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.1.weight shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.1.bias shape torch.Size([32])
decoder.stages.4.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.stages.4.convs.1.conv.bias shape torch.Size([32])
decoder.stages.4.convs.1.norm.weight shape torch.Size([32])
decoder.stages.4.convs.1.norm.bias shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.stages.4.convs.1.all_modules.0.bias shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.1.weight shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.1.bias shape torch.Size([32])
decoder.transpconvs.0.weight shape torch.Size([320, 320, 1, 2, 2])
decoder.transpconvs.0.bias shape torch.Size([320])
decoder.transpconvs.1.weight shape torch.Size([320, 256, 2, 2, 2])
decoder.transpconvs.1.bias shape torch.Size([256])
decoder.transpconvs.2.weight shape torch.Size([256, 128, 2, 2, 2])
decoder.transpconvs.2.bias shape torch.Size([128])
decoder.transpconvs.3.weight shape torch.Size([128, 64, 2, 2, 2])
decoder.transpconvs.3.bias shape torch.Size([64])
decoder.transpconvs.4.weight shape torch.Size([64, 32, 2, 2, 2])
decoder.transpconvs.4.bias shape torch.Size([32])
################### Done ###################
2024-12-06 19:58:12.406416: do_dummy_2d_data_aug: True
2024-12-06 19:58:12.409471: Using splits from existing split file: /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset307_Sohee_Calcium_OCT_CrossValidation/splits_final.json
2024-12-06 19:58:12.411867: The split file contains 5 splits.
2024-12-06 19:58:12.412968: Desired fold for training: 1
2024-12-06 19:58:12.414065: This split has 6 training and 2 validation cases.
################### Loading pretrained weights from file  /home/gridsan/nchutisilp/projects/OpenAI-CLIP/clip_pretrained_nnUNet.pt ###################
Below is the list of overlapping blocks in pretrained model and nnUNet architecture:
encoder.stages.0.0.convs.0.conv.weight shape torch.Size([32, 1, 3, 3, 3])
encoder.stages.0.0.convs.0.conv.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.norm.weight shape torch.Size([32])
encoder.stages.0.0.convs.0.norm.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.0.weight shape torch.Size([32, 1, 3, 3, 3])
encoder.stages.0.0.convs.0.all_modules.0.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.1.weight shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.1.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
encoder.stages.0.0.convs.1.conv.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.norm.weight shape torch.Size([32])
encoder.stages.0.0.convs.1.norm.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
encoder.stages.0.0.convs.1.all_modules.0.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.1.weight shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.1.bias shape torch.Size([32])
encoder.stages.1.0.convs.0.conv.weight shape torch.Size([64, 32, 3, 3, 3])
encoder.stages.1.0.convs.0.conv.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.norm.weight shape torch.Size([64])
encoder.stages.1.0.convs.0.norm.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.0.weight shape torch.Size([64, 32, 3, 3, 3])
encoder.stages.1.0.convs.0.all_modules.0.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.1.weight shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.1.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
encoder.stages.1.0.convs.1.conv.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.norm.weight shape torch.Size([64])
encoder.stages.1.0.convs.1.norm.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
encoder.stages.1.0.convs.1.all_modules.0.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.1.weight shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.1.bias shape torch.Size([64])
encoder.stages.2.0.convs.0.conv.weight shape torch.Size([128, 64, 3, 3, 3])
encoder.stages.2.0.convs.0.conv.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.norm.weight shape torch.Size([128])
encoder.stages.2.0.convs.0.norm.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.0.weight shape torch.Size([128, 64, 3, 3, 3])
encoder.stages.2.0.convs.0.all_modules.0.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.1.weight shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.1.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
encoder.stages.2.0.convs.1.conv.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.norm.weight shape torch.Size([128])
encoder.stages.2.0.convs.1.norm.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
encoder.stages.2.0.convs.1.all_modules.0.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.1.weight shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.1.bias shape torch.Size([128])
encoder.stages.3.0.convs.0.conv.weight shape torch.Size([256, 128, 3, 3, 3])
encoder.stages.3.0.convs.0.conv.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.norm.weight shape torch.Size([256])
encoder.stages.3.0.convs.0.norm.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.0.weight shape torch.Size([256, 128, 3, 3, 3])
encoder.stages.3.0.convs.0.all_modules.0.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.1.weight shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.1.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
encoder.stages.3.0.convs.1.conv.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.norm.weight shape torch.Size([256])
encoder.stages.3.0.convs.1.norm.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
encoder.stages.3.0.convs.1.all_modules.0.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.1.weight shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.1.bias shape torch.Size([256])
encoder.stages.4.0.convs.0.conv.weight shape torch.Size([320, 256, 3, 3, 3])
encoder.stages.4.0.convs.0.conv.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.norm.weight shape torch.Size([320])
encoder.stages.4.0.convs.0.norm.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.0.weight shape torch.Size([320, 256, 3, 3, 3])
encoder.stages.4.0.convs.0.all_modules.0.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.1.weight shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.1.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.4.0.convs.1.conv.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.norm.weight shape torch.Size([320])
encoder.stages.4.0.convs.1.norm.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.4.0.convs.1.all_modules.0.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.1.weight shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.1.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.0.conv.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.norm.weight shape torch.Size([320])
encoder.stages.5.0.convs.0.norm.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.0.all_modules.0.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.1.weight shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.1.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.1.conv.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.norm.weight shape torch.Size([320])
encoder.stages.5.0.convs.1.norm.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.1.all_modules.0.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.1.weight shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.0.0.convs.0.conv.weight shape torch.Size([32, 1, 3, 3, 3])
decoder.encoder.stages.0.0.convs.0.conv.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.norm.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.norm.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.0.weight shape torch.Size([32, 1, 3, 3, 3])
decoder.encoder.stages.0.0.convs.0.all_modules.0.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.1.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.1.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.encoder.stages.0.0.convs.1.conv.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.norm.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.norm.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.encoder.stages.0.0.convs.1.all_modules.0.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.1.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.1.bias shape torch.Size([32])
decoder.encoder.stages.1.0.convs.0.conv.weight shape torch.Size([64, 32, 3, 3, 3])
decoder.encoder.stages.1.0.convs.0.conv.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.norm.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.norm.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.0.weight shape torch.Size([64, 32, 3, 3, 3])
decoder.encoder.stages.1.0.convs.0.all_modules.0.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.1.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.1.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.encoder.stages.1.0.convs.1.conv.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.norm.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.norm.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.encoder.stages.1.0.convs.1.all_modules.0.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.1.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.1.bias shape torch.Size([64])
decoder.encoder.stages.2.0.convs.0.conv.weight shape torch.Size([128, 64, 3, 3, 3])
decoder.encoder.stages.2.0.convs.0.conv.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.norm.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.norm.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.0.weight shape torch.Size([128, 64, 3, 3, 3])
decoder.encoder.stages.2.0.convs.0.all_modules.0.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.1.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.1.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.encoder.stages.2.0.convs.1.conv.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.norm.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.norm.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.encoder.stages.2.0.convs.1.all_modules.0.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.1.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.1.bias shape torch.Size([128])
decoder.encoder.stages.3.0.convs.0.conv.weight shape torch.Size([256, 128, 3, 3, 3])
decoder.encoder.stages.3.0.convs.0.conv.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.norm.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.norm.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.0.weight shape torch.Size([256, 128, 3, 3, 3])
decoder.encoder.stages.3.0.convs.0.all_modules.0.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.1.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.1.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.encoder.stages.3.0.convs.1.conv.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.norm.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.norm.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.encoder.stages.3.0.convs.1.all_modules.0.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.1.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.1.bias shape torch.Size([256])
decoder.encoder.stages.4.0.convs.0.conv.weight shape torch.Size([320, 256, 3, 3, 3])
decoder.encoder.stages.4.0.convs.0.conv.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.norm.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.norm.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.0.weight shape torch.Size([320, 256, 3, 3, 3])
decoder.encoder.stages.4.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.4.0.convs.1.conv.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.norm.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.norm.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.4.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.0.conv.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.norm.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.norm.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.1.conv.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.norm.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.norm.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.stages.0.convs.0.conv.weight shape torch.Size([320, 640, 3, 3, 3])
decoder.stages.0.convs.0.conv.bias shape torch.Size([320])
decoder.stages.0.convs.0.norm.weight shape torch.Size([320])
decoder.stages.0.convs.0.norm.bias shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.0.weight shape torch.Size([320, 640, 3, 3, 3])
decoder.stages.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.stages.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.stages.0.convs.1.conv.bias shape torch.Size([320])
decoder.stages.0.convs.1.norm.weight shape torch.Size([320])
decoder.stages.0.convs.1.norm.bias shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.stages.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.stages.1.convs.0.conv.weight shape torch.Size([256, 512, 3, 3, 3])
decoder.stages.1.convs.0.conv.bias shape torch.Size([256])
decoder.stages.1.convs.0.norm.weight shape torch.Size([256])
decoder.stages.1.convs.0.norm.bias shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.0.weight shape torch.Size([256, 512, 3, 3, 3])
decoder.stages.1.convs.0.all_modules.0.bias shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.1.weight shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.1.bias shape torch.Size([256])
decoder.stages.1.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.stages.1.convs.1.conv.bias shape torch.Size([256])
decoder.stages.1.convs.1.norm.weight shape torch.Size([256])
decoder.stages.1.convs.1.norm.bias shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.stages.1.convs.1.all_modules.0.bias shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.1.weight shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.1.bias shape torch.Size([256])
decoder.stages.2.convs.0.conv.weight shape torch.Size([128, 256, 3, 3, 3])
decoder.stages.2.convs.0.conv.bias shape torch.Size([128])
decoder.stages.2.convs.0.norm.weight shape torch.Size([128])
decoder.stages.2.convs.0.norm.bias shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.0.weight shape torch.Size([128, 256, 3, 3, 3])
decoder.stages.2.convs.0.all_modules.0.bias shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.1.weight shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.1.bias shape torch.Size([128])
decoder.stages.2.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.stages.2.convs.1.conv.bias shape torch.Size([128])
decoder.stages.2.convs.1.norm.weight shape torch.Size([128])
decoder.stages.2.convs.1.norm.bias shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.stages.2.convs.1.all_modules.0.bias shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.1.weight shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.1.bias shape torch.Size([128])
decoder.stages.3.convs.0.conv.weight shape torch.Size([64, 128, 3, 3, 3])
decoder.stages.3.convs.0.conv.bias shape torch.Size([64])
decoder.stages.3.convs.0.norm.weight shape torch.Size([64])
decoder.stages.3.convs.0.norm.bias shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.0.weight shape torch.Size([64, 128, 3, 3, 3])
decoder.stages.3.convs.0.all_modules.0.bias shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.1.weight shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.1.bias shape torch.Size([64])
decoder.stages.3.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.stages.3.convs.1.conv.bias shape torch.Size([64])
decoder.stages.3.convs.1.norm.weight shape torch.Size([64])
decoder.stages.3.convs.1.norm.bias shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.stages.3.convs.1.all_modules.0.bias shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.1.weight shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.1.bias shape torch.Size([64])
decoder.stages.4.convs.0.conv.weight shape torch.Size([32, 64, 3, 3, 3])
decoder.stages.4.convs.0.conv.bias shape torch.Size([32])
decoder.stages.4.convs.0.norm.weight shape torch.Size([32])
decoder.stages.4.convs.0.norm.bias shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.0.weight shape torch.Size([32, 64, 3, 3, 3])
decoder.stages.4.convs.0.all_modules.0.bias shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.1.weight shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.1.bias shape torch.Size([32])
decoder.stages.4.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.stages.4.convs.1.conv.bias shape torch.Size([32])
decoder.stages.4.convs.1.norm.weight shape torch.Size([32])
decoder.stages.4.convs.1.norm.bias shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.stages.4.convs.1.all_modules.0.bias shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.1.weight shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.1.bias shape torch.Size([32])
decoder.transpconvs.0.weight shape torch.Size([320, 320, 1, 2, 2])
decoder.transpconvs.0.bias shape torch.Size([320])
decoder.transpconvs.1.weight shape torch.Size([320, 256, 2, 2, 2])
decoder.transpconvs.1.bias shape torch.Size([256])
decoder.transpconvs.2.weight shape torch.Size([256, 128, 2, 2, 2])
decoder.transpconvs.2.bias shape torch.Size([128])
decoder.transpconvs.3.weight shape torch.Size([128, 64, 2, 2, 2])
decoder.transpconvs.3.bias shape torch.Size([64])
decoder.transpconvs.4.weight shape torch.Size([64, 32, 2, 2, 2])
decoder.transpconvs.4.bias shape torch.Size([32])
################### Done ###################
2024-12-06 19:58:12.406417: do_dummy_2d_data_aug: True
2024-12-06 19:58:12.409475: Using splits from existing split file: /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset307_Sohee_Calcium_OCT_CrossValidation/splits_final.json
2024-12-06 19:58:12.411984: The split file contains 5 splits.
2024-12-06 19:58:12.413001: Desired fold for training: 0
2024-12-06 19:58:12.414166: This split has 6 training and 2 validation cases.
using pin_memory on device 0
using pin_memory on device 0

This is the configuration used by this training:
Configuration name: 3d_32x160x128_b10
 {'data_identifier': 'nnUNetPlans_3d_fullres', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 10, 'patch_size': [32, 160, 128], 'median_image_size_in_voxels': [375.0, 498.0, 498.0], 'spacing': [1.0, 1.0, 1.0], 'normalization_schemes': ['ZScoreNormalization'], 'use_mask_for_norm': [False], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'architecture': {'network_class_name': 'dynamic_network_architectures.architectures.unet.PlainConvUNet', 'arch_kwargs': {'n_stages': 6, 'features_per_stage': [32, 64, 128, 256, 320, 320], 'conv_op': 'torch.nn.modules.conv.Conv3d', 'kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]], 'strides': [[1, 1, 1], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [1, 2, 2]], 'n_conv_per_stage': [2, 2, 2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2, 2, 2], 'conv_bias': True, 'norm_op': 'torch.nn.modules.instancenorm.InstanceNorm3d', 'norm_op_kwargs': {'eps': 1e-05, 'affine': True}, 'dropout_op': None, 'dropout_op_kwargs': None, 'nonlin': 'torch.nn.LeakyReLU', 'nonlin_kwargs': {'inplace': True}, 'deep_supervision': True}, '_kw_requires_import': ['conv_op', 'norm_op', 'dropout_op', 'nonlin']}, 'batch_dice': True, 'inherits_from': '3d_fullres'} 

These are the global plan.json settings:
 {'dataset_name': 'Dataset307_Sohee_Calcium_OCT_CrossValidation', 'plans_name': 'nnUNetPlans', 'original_median_spacing_after_transp': [1.0, 1.0, 1.0], 'original_median_shape_after_transp': [375, 498, 498], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 0.4977976381778717, 'mean': 0.13507525622844696, 'median': 0.09599608182907104, 'min': 0.0, 'percentile_00_5': 0.014754901640117168, 'percentile_99_5': 0.4977976381778717, 'std': 0.12025152146816254}}} 

2024-12-06 19:58:15.206077: unpacking dataset...
using pin_memory on device 0
using pin_memory on device 0

This is the configuration used by this training:
Configuration name: 3d_32x160x128_b10
 {'data_identifier': 'nnUNetPlans_3d_fullres', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 10, 'patch_size': [32, 160, 128], 'median_image_size_in_voxels': [375.0, 498.0, 498.0], 'spacing': [1.0, 1.0, 1.0], 'normalization_schemes': ['ZScoreNormalization'], 'use_mask_for_norm': [False], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'architecture': {'network_class_name': 'dynamic_network_architectures.architectures.unet.PlainConvUNet', 'arch_kwargs': {'n_stages': 6, 'features_per_stage': [32, 64, 128, 256, 320, 320], 'conv_op': 'torch.nn.modules.conv.Conv3d', 'kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]], 'strides': [[1, 1, 1], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [1, 2, 2]], 'n_conv_per_stage': [2, 2, 2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2, 2, 2], 'conv_bias': True, 'norm_op': 'torch.nn.modules.instancenorm.InstanceNorm3d', 'norm_op_kwargs': {'eps': 1e-05, 'affine': True}, 'dropout_op': None, 'dropout_op_kwargs': None, 'nonlin': 'torch.nn.LeakyReLU', 'nonlin_kwargs': {'inplace': True}, 'deep_supervision': True}, '_kw_requires_import': ['conv_op', 'norm_op', 'dropout_op', 'nonlin']}, 'batch_dice': True, 'inherits_from': '3d_fullres'} 

These are the global plan.json settings:
 {'dataset_name': 'Dataset307_Sohee_Calcium_OCT_CrossValidation', 'plans_name': 'nnUNetPlans', 'original_median_spacing_after_transp': [1.0, 1.0, 1.0], 'original_median_shape_after_transp': [375, 498, 498], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 0.4977976381778717, 'mean': 0.13507525622844696, 'median': 0.09599608182907104, 'min': 0.0, 'percentile_00_5': 0.014754901640117168, 'percentile_99_5': 0.4977976381778717, 'std': 0.12025152146816254}}} 

2024-12-06 19:58:17.351280: unpacking dataset...
2024-12-06 19:58:21.345545: unpacking done...
2024-12-06 19:58:21.436342: Unable to plot network architecture: nnUNet_compile is enabled!
2024-12-06 19:58:21.490340: 
2024-12-06 19:58:21.491904: Epoch 0
2024-12-06 19:58:21.492802: Current learning rate: 0.01
2024-12-06 20:01:01.702586: Validation loss improved from 1000.00000 to -0.18836! Patience: 0/50
2024-12-06 20:01:01.704112: train_loss -0.0861
2024-12-06 20:01:01.705085: val_loss -0.1884
2024-12-06 20:01:01.705835: Pseudo dice [0.5582]
2024-12-06 20:01:01.706672: Epoch time: 160.22 s
2024-12-06 20:01:01.707412: Yayy! New best EMA pseudo Dice: 0.5582
2024-12-06 20:01:03.294107: 
2024-12-06 20:01:03.295506: Epoch 1
2024-12-06 20:01:03.296309: Current learning rate: 0.00999
2024-12-06 20:02:37.373735: Validation loss improved from -0.18836 to -0.26342! Patience: 0/50
2024-12-06 20:02:37.374496: train_loss -0.2424
2024-12-06 20:02:37.375200: val_loss -0.2634
2024-12-06 20:02:37.375828: Pseudo dice [0.6034]
2024-12-06 20:02:37.376523: Epoch time: 94.08 s
2024-12-06 20:02:37.377299: Yayy! New best EMA pseudo Dice: 0.5627
2024-12-06 20:02:39.214307: 
2024-12-06 20:02:39.215261: Epoch 2
2024-12-06 20:02:39.215991: Current learning rate: 0.00998
2024-12-06 20:04:16.048371: Validation loss did not improve from -0.26342. Patience: 1/50
2024-12-06 20:04:16.049389: train_loss -0.3137
2024-12-06 20:04:16.050400: val_loss -0.2633
2024-12-06 20:04:16.051255: Pseudo dice [0.6141]
2024-12-06 20:04:16.052000: Epoch time: 96.84 s
2024-12-06 20:04:16.052784: Yayy! New best EMA pseudo Dice: 0.5679
2024-12-06 20:04:17.871615: 
2024-12-06 20:04:17.872955: Epoch 3
2024-12-06 20:04:17.874077: Current learning rate: 0.00997
2024-12-06 20:05:55.674961: Validation loss improved from -0.26342 to -0.34761! Patience: 1/50
2024-12-06 20:05:55.675974: train_loss -0.3382
2024-12-06 20:05:55.676757: val_loss -0.3476
2024-12-06 20:05:55.677391: Pseudo dice [0.6554]
2024-12-06 20:05:55.678053: Epoch time: 97.81 s
2024-12-06 20:05:55.678806: Yayy! New best EMA pseudo Dice: 0.5766
2024-12-06 20:05:57.440106: 
2024-12-06 20:05:57.441489: Epoch 4
2024-12-06 20:05:57.442244: Current learning rate: 0.00996
2024-12-06 20:07:34.933676: Validation loss improved from -0.34761 to -0.36288! Patience: 0/50
2024-12-06 20:07:34.934821: train_loss -0.3731
2024-12-06 20:07:34.935747: val_loss -0.3629
2024-12-06 20:07:34.936453: Pseudo dice [0.6733]
2024-12-06 20:07:34.937174: Epoch time: 97.5 s
2024-12-06 20:07:35.349926: Yayy! New best EMA pseudo Dice: 0.5863
2024-12-06 20:07:37.205947: 
2024-12-06 20:07:37.207278: Epoch 5
2024-12-06 20:07:37.208118: Current learning rate: 0.00995
2024-12-06 20:09:16.546518: Validation loss improved from -0.36288 to -0.37812! Patience: 0/50
2024-12-06 20:09:16.547537: train_loss -0.3976
2024-12-06 20:09:16.548539: val_loss -0.3781
2024-12-06 20:09:16.549338: Pseudo dice [0.6789]
2024-12-06 20:09:16.550273: Epoch time: 99.34 s
2024-12-06 20:09:16.551222: Yayy! New best EMA pseudo Dice: 0.5955
2024-12-06 20:09:18.436911: 
2024-12-06 20:09:18.438593: Epoch 6
2024-12-06 20:09:18.439584: Current learning rate: 0.00995
2024-12-06 20:10:58.547159: Validation loss improved from -0.37812 to -0.40503! Patience: 0/50
2024-12-06 20:10:58.548327: train_loss -0.4271
2024-12-06 20:10:58.549344: val_loss -0.405
2024-12-06 20:10:58.550096: Pseudo dice [0.7012]
2024-12-06 20:10:58.550938: Epoch time: 100.11 s
2024-12-06 20:10:58.551774: Yayy! New best EMA pseudo Dice: 0.6061
2024-12-06 20:11:00.432926: 
2024-12-06 20:11:00.434433: Epoch 7
2024-12-06 20:11:00.435414: Current learning rate: 0.00994
2024-12-06 20:12:40.358682: Validation loss did not improve from -0.40503. Patience: 1/50
2024-12-06 20:12:40.359753: train_loss -0.4393
2024-12-06 20:12:40.360643: val_loss -0.3798
2024-12-06 20:12:40.361473: Pseudo dice [0.6818]
2024-12-06 20:12:40.362320: Epoch time: 99.93 s
2024-12-06 20:12:40.363037: Yayy! New best EMA pseudo Dice: 0.6137
2024-12-06 20:12:42.247285: 
2024-12-06 20:12:42.248730: Epoch 8
2024-12-06 20:12:42.249526: Current learning rate: 0.00993
2024-12-06 20:14:25.454753: Validation loss improved from -0.40503 to -0.41696! Patience: 1/50
2024-12-06 20:14:25.455725: train_loss -0.4328
2024-12-06 20:14:25.456579: val_loss -0.417
2024-12-06 20:14:25.457309: Pseudo dice [0.7098]
2024-12-06 20:14:25.458052: Epoch time: 103.21 s
2024-12-06 20:14:25.458825: Yayy! New best EMA pseudo Dice: 0.6233
2024-12-06 20:14:27.938242: 
2024-12-06 20:14:27.939503: Epoch 9
2024-12-06 20:14:27.940456: Current learning rate: 0.00992
2024-12-06 20:16:10.846241: Validation loss did not improve from -0.41696. Patience: 1/50
2024-12-06 20:16:10.847134: train_loss -0.4542
2024-12-06 20:16:10.847962: val_loss -0.3611
2024-12-06 20:16:10.848769: Pseudo dice [0.6751]
2024-12-06 20:16:10.849625: Epoch time: 102.91 s
2024-12-06 20:16:11.270013: Yayy! New best EMA pseudo Dice: 0.6285
2024-12-06 20:16:13.075670: 
2024-12-06 20:16:13.077038: Epoch 10
2024-12-06 20:16:13.077832: Current learning rate: 0.00991
2024-12-06 20:17:53.635433: Validation loss improved from -0.41696 to -0.42901! Patience: 1/50
2024-12-06 20:17:53.636565: train_loss -0.4828
2024-12-06 20:17:53.637509: val_loss -0.429
2024-12-06 20:17:53.638287: Pseudo dice [0.7037]
2024-12-06 20:17:53.638987: Epoch time: 100.56 s
2024-12-06 20:17:53.639661: Yayy! New best EMA pseudo Dice: 0.636
2024-12-06 20:17:55.593863: 
2024-12-06 20:17:55.595270: Epoch 11
2024-12-06 20:17:55.596116: Current learning rate: 0.0099
2024-12-06 20:19:40.930383: Validation loss did not improve from -0.42901. Patience: 1/50
2024-12-06 20:19:40.931482: train_loss -0.4657
2024-12-06 20:19:40.932383: val_loss -0.4197
2024-12-06 20:19:40.933317: Pseudo dice [0.7048]
2024-12-06 20:19:40.934145: Epoch time: 105.34 s
2024-12-06 20:19:40.935038: Yayy! New best EMA pseudo Dice: 0.6429
2024-12-06 20:19:42.775001: 
2024-12-06 20:19:42.776403: Epoch 12
2024-12-06 20:19:42.777316: Current learning rate: 0.00989
2024-12-06 20:21:28.679090: Validation loss did not improve from -0.42901. Patience: 2/50
2024-12-06 20:21:28.680283: train_loss -0.476
2024-12-06 20:21:28.681400: val_loss -0.4242
2024-12-06 20:21:28.682425: Pseudo dice [0.7067]
2024-12-06 20:21:28.683390: Epoch time: 105.91 s
2024-12-06 20:21:28.684520: Yayy! New best EMA pseudo Dice: 0.6492
2024-12-06 20:21:30.649274: 
2024-12-06 20:21:30.650755: Epoch 13
2024-12-06 20:21:30.651821: Current learning rate: 0.00988
2024-12-06 20:23:13.903647: Validation loss improved from -0.42901 to -0.45550! Patience: 2/50
2024-12-06 20:23:13.904786: train_loss -0.4993
2024-12-06 20:23:13.905701: val_loss -0.4555
2024-12-06 20:23:13.906528: Pseudo dice [0.7306]
2024-12-06 20:23:13.907422: Epoch time: 103.26 s
2024-12-06 20:23:13.908397: Yayy! New best EMA pseudo Dice: 0.6574
2024-12-06 20:23:15.784598: 
2024-12-06 20:23:15.785996: Epoch 14
2024-12-06 20:23:15.786798: Current learning rate: 0.00987
2024-12-06 20:25:02.825511: Validation loss did not improve from -0.45550. Patience: 1/50
2024-12-06 20:25:02.826543: train_loss -0.5024
2024-12-06 20:25:02.827437: val_loss -0.4496
2024-12-06 20:25:02.828196: Pseudo dice [0.7342]
2024-12-06 20:25:02.828984: Epoch time: 107.04 s
2024-12-06 20:25:03.259129: Yayy! New best EMA pseudo Dice: 0.6651
2024-12-06 20:25:05.119461: 
2024-12-06 20:25:05.120902: Epoch 15
2024-12-06 20:25:05.121642: Current learning rate: 0.00986
2024-12-06 20:26:42.970620: Validation loss improved from -0.45550 to -0.47136! Patience: 1/50
2024-12-06 20:26:42.971777: train_loss -0.5193
2024-12-06 20:26:42.972915: val_loss -0.4714
2024-12-06 20:26:42.973857: Pseudo dice [0.7319]
2024-12-06 20:26:42.974893: Epoch time: 97.85 s
2024-12-06 20:26:42.975817: Yayy! New best EMA pseudo Dice: 0.6717
2024-12-06 20:26:44.902794: 
2024-12-06 20:26:44.907467: Epoch 16
2024-12-06 20:26:44.909123: Current learning rate: 0.00986
2024-12-06 20:28:23.170941: Validation loss did not improve from -0.47136. Patience: 1/50
2024-12-06 20:28:23.172526: train_loss -0.5214
2024-12-06 20:28:23.173620: val_loss -0.4283
2024-12-06 20:28:23.174353: Pseudo dice [0.7113]
2024-12-06 20:28:23.175109: Epoch time: 98.27 s
2024-12-06 20:28:23.175900: Yayy! New best EMA pseudo Dice: 0.6757
2024-12-06 20:28:25.007381: 
2024-12-06 20:28:25.009468: Epoch 17
2024-12-06 20:28:25.010729: Current learning rate: 0.00985
2024-12-06 20:30:02.811868: Validation loss improved from -0.47136 to -0.49238! Patience: 1/50
2024-12-06 20:30:02.813138: train_loss -0.5342
2024-12-06 20:30:02.814283: val_loss -0.4924
2024-12-06 20:30:02.815206: Pseudo dice [0.7407]
2024-12-06 20:30:02.816156: Epoch time: 97.81 s
2024-12-06 20:30:02.817006: Yayy! New best EMA pseudo Dice: 0.6822
2024-12-06 20:30:05.251096: 
2024-12-06 20:30:05.253137: Epoch 18
2024-12-06 20:30:05.254096: Current learning rate: 0.00984
2024-12-06 20:31:44.345805: failed to log:  (<class 'FileNotFoundError'>, FileNotFoundError(2, 'No such file or directory'), <traceback object at 0x7f81583857c0>)
2024-12-06 20:31:44.345805: failed to log:  (<class 'FileNotFoundError'>, FileNotFoundError(2, 'No such file or directory'), <traceback object at 0x7f81583857c0>)
2024-12-06 20:31:44.345805: failed to log:  (<class 'FileNotFoundError'>, FileNotFoundError(2, 'No such file or directory'), <traceback object at 0x7f81583857c0>)
2024-12-06 20:31:44.345805: failed to log:  (<class 'FileNotFoundError'>, FileNotFoundError(2, 'No such file or directory'), <traceback object at 0x7f81583857c0>)
2024-12-06 20:31:44.345805: failed to log:  (<class 'FileNotFoundError'>, FileNotFoundError(2, 'No such file or directory'), <traceback object at 0x7f81583857c0>)
2024-12-06 20:31:44.345805: Validation loss did not improve from -0.49238. Patience: 1/50
2024-12-06 20:31:46.849998: failed to log:  (<class 'FileNotFoundError'>, FileNotFoundError(2, 'No such file or directory'), <traceback object at 0x7f81583857c0>)
2024-12-06 20:31:46.849998: failed to log:  (<class 'FileNotFoundError'>, FileNotFoundError(2, 'No such file or directory'), <traceback object at 0x7f81583857c0>)
2024-12-06 20:31:46.849998: failed to log:  (<class 'FileNotFoundError'>, FileNotFoundError(2, 'No such file or directory'), <traceback object at 0x7f81583857c0>)
2024-12-06 20:31:46.849998: failed to log:  (<class 'FileNotFoundError'>, FileNotFoundError(2, 'No such file or directory'), <traceback object at 0x7f81583857c0>)
2024-12-06 20:31:46.849998: failed to log:  (<class 'FileNotFoundError'>, FileNotFoundError(2, 'No such file or directory'), <traceback object at 0x7f81583857c0>)
2024-12-06 20:31:46.849998: train_loss -0.5331
2024-12-06 20:31:49.353299: failed to log:  (<class 'FileNotFoundError'>, FileNotFoundError(2, 'No such file or directory'), <traceback object at 0x7f8190a3b340>)
2024-12-06 20:31:49.353299: failed to log:  (<class 'FileNotFoundError'>, FileNotFoundError(2, 'No such file or directory'), <traceback object at 0x7f8190a3b340>)
2024-12-06 20:31:49.353299: failed to log:  (<class 'FileNotFoundError'>, FileNotFoundError(2, 'No such file or directory'), <traceback object at 0x7f8190a3b340>)
2024-12-06 20:31:49.353299: failed to log:  (<class 'FileNotFoundError'>, FileNotFoundError(2, 'No such file or directory'), <traceback object at 0x7f8190a3b340>)
2024-12-06 20:31:49.353299: failed to log:  (<class 'FileNotFoundError'>, FileNotFoundError(2, 'No such file or directory'), <traceback object at 0x7f8190a3b340>)
2024-12-06 20:31:49.353299: val_loss -0.4575
2024-12-06 20:31:51.856754: failed to log:  (<class 'FileNotFoundError'>, FileNotFoundError(2, 'No such file or directory'), <traceback object at 0x7f8130ec34c0>)
2024-12-06 20:31:51.856754: failed to log:  (<class 'FileNotFoundError'>, FileNotFoundError(2, 'No such file or directory'), <traceback object at 0x7f8130ec34c0>)
2024-12-06 20:31:51.856754: failed to log:  (<class 'FileNotFoundError'>, FileNotFoundError(2, 'No such file or directory'), <traceback object at 0x7f8130ec34c0>)
2024-12-06 20:31:51.856754: failed to log:  (<class 'FileNotFoundError'>, FileNotFoundError(2, 'No such file or directory'), <traceback object at 0x7f8130ec34c0>)
2024-12-06 20:31:51.856754: failed to log:  (<class 'FileNotFoundError'>, FileNotFoundError(2, 'No such file or directory'), <traceback object at 0x7f8130ec34c0>)
2024-12-06 20:31:51.856754: Pseudo dice [0.7235]
2024-12-06 20:31:54.359747: failed to log:  (<class 'FileNotFoundError'>, FileNotFoundError(2, 'No such file or directory'), <traceback object at 0x7f81583c6840>)
2024-12-06 20:31:54.359747: failed to log:  (<class 'FileNotFoundError'>, FileNotFoundError(2, 'No such file or directory'), <traceback object at 0x7f81583c6840>)
2024-12-06 20:31:54.359747: failed to log:  (<class 'FileNotFoundError'>, FileNotFoundError(2, 'No such file or directory'), <traceback object at 0x7f81583c6840>)
2024-12-06 20:31:54.359747: failed to log:  (<class 'FileNotFoundError'>, FileNotFoundError(2, 'No such file or directory'), <traceback object at 0x7f81583c6840>)
2024-12-06 20:31:54.359747: failed to log:  (<class 'FileNotFoundError'>, FileNotFoundError(2, 'No such file or directory'), <traceback object at 0x7f81583c6840>)
2024-12-06 20:31:54.359747: Epoch time: 101.6 s
2024-12-06 20:31:56.863089: failed to log:  (<class 'FileNotFoundError'>, FileNotFoundError(2, 'No such file or directory'), <traceback object at 0x7f8130cd2140>)
2024-12-06 20:31:56.863089: failed to log:  (<class 'FileNotFoundError'>, FileNotFoundError(2, 'No such file or directory'), <traceback object at 0x7f8130cd2140>)
2024-12-06 20:31:56.863089: failed to log:  (<class 'FileNotFoundError'>, FileNotFoundError(2, 'No such file or directory'), <traceback object at 0x7f8130cd2140>)
2024-12-06 20:31:56.863089: failed to log:  (<class 'FileNotFoundError'>, FileNotFoundError(2, 'No such file or directory'), <traceback object at 0x7f8130cd2140>)
2024-12-06 20:31:56.863089: failed to log:  (<class 'FileNotFoundError'>, FileNotFoundError(2, 'No such file or directory'), <traceback object at 0x7f8130cd2140>)
2024-12-06 20:31:56.863089: Yayy! New best EMA pseudo Dice: 0.6863
Traceback (most recent call last):
  File "/home/gridsan/nchutisilp/.conda/envs/nnunet/bin/nnUNetv2_train", line 8, in <module>
    sys.exit(run_training_entry())
  File "/home/gridsan/nchutisilp/projects/nnUNet/nnunetv2/run/run_training.py", line 275, in run_training_entry
    run_training(args.dataset_name_or_id, args.configuration, args.fold, args.tr, args.p, args.pretrained_weights,
  File "/home/gridsan/nchutisilp/projects/nnUNet/nnunetv2/run/run_training.py", line 211, in run_training
    nnunet_trainer.run_training()
  File "/home/gridsan/nchutisilp/projects/nnUNet/nnunetv2/training/nnUNetTrainer/nnUNetTrainer.py", line 1411, in run_training
    self.on_epoch_end()
  File "/home/gridsan/nchutisilp/projects/nnUNet/nnunetv2/training/nnUNetTrainer/nnUNetTrainer.py", line 1171, in on_epoch_end
    self.save_checkpoint(join(self.output_folder, 'checkpoint_best.pth'))
  File "/home/gridsan/nchutisilp/projects/nnUNet/nnunetv2/training/nnUNetTrainer/nnUNetTrainer.py", line 1201, in save_checkpoint
    torch.save(checkpoint, filename)
  File "/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/serialization.py", line 627, in save
    with _open_zipfile_writer(f) as opened_zipfile:
  File "/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/serialization.py", line 501, in _open_zipfile_writer
    return container(name_or_buffer)
  File "/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/serialization.py", line 472, in __init__
    super().__init__(torch._C.PyTorchFileWriter(self.name))
RuntimeError: Parent directory /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_results/Dataset307_Sohee_Calcium_OCT_CrossValidation/nnUNetTrainer__nnUNetPlans__3d_32x160x128_b10/fold_0 does not exist.
Exception in thread Thread-3:
Traceback (most recent call last):
  File "/home/gridsan/nchutisilp/.conda/envs/nnunet/lib/python3.9/threading.py", line 980, in _bootstrap_inner
Exception in thread Thread-2:
Traceback (most recent call last):
  File "/home/gridsan/nchutisilp/.conda/envs/nnunet/lib/python3.9/threading.py", line 980, in _bootstrap_inner
    self.run()
  File "/home/gridsan/nchutisilp/.conda/envs/nnunet/lib/python3.9/threading.py", line 917, in run
2024-12-06 19:58:21.788078: unpacking done...
2024-12-06 19:58:21.795712: Unable to plot network architecture: nnUNet_compile is enabled!
2024-12-06 19:58:21.825354: 
2024-12-06 19:58:21.826839: Epoch 0
2024-12-06 19:58:21.827719: Current learning rate: 0.01
2024-12-06 20:01:00.944310: Validation loss improved from 1000.00000 to -0.03838! Patience: 0/50
2024-12-06 20:01:00.945529: train_loss -0.0801
2024-12-06 20:01:00.946904: val_loss -0.0384
2024-12-06 20:01:00.947622: Pseudo dice [0.4214]
2024-12-06 20:01:00.948364: Epoch time: 159.12 s
2024-12-06 20:01:00.948987: Yayy! New best EMA pseudo Dice: 0.4214
2024-12-06 20:01:02.613372: 
2024-12-06 20:01:02.614643: Epoch 1
2024-12-06 20:01:02.615413: Current learning rate: 0.00999
2024-12-06 20:02:40.869856: Validation loss improved from -0.03838 to -0.12723! Patience: 0/50
2024-12-06 20:02:40.870838: train_loss -0.2344
2024-12-06 20:02:40.871665: val_loss -0.1272
2024-12-06 20:02:40.872376: Pseudo dice [0.4867]
2024-12-06 20:02:40.873148: Epoch time: 98.26 s
2024-12-06 20:02:40.873843: Yayy! New best EMA pseudo Dice: 0.4279
2024-12-06 20:02:42.651875: 
2024-12-06 20:02:42.653131: Epoch 2
2024-12-06 20:02:42.653985: Current learning rate: 0.00998
2024-12-06 20:04:23.073737: Validation loss improved from -0.12723 to -0.18185! Patience: 0/50
2024-12-06 20:04:23.074831: train_loss -0.2698
2024-12-06 20:04:23.075669: val_loss -0.1819
2024-12-06 20:04:23.076511: Pseudo dice [0.5233]
2024-12-06 20:04:23.077328: Epoch time: 100.42 s
2024-12-06 20:04:23.078128: Yayy! New best EMA pseudo Dice: 0.4375
2024-12-06 20:04:24.938026: 
2024-12-06 20:04:24.939314: Epoch 3
2024-12-06 20:04:24.940165: Current learning rate: 0.00997
2024-12-06 20:06:08.876011: Validation loss improved from -0.18185 to -0.22566! Patience: 0/50
2024-12-06 20:06:08.877121: train_loss -0.3165
2024-12-06 20:06:08.877984: val_loss -0.2257
2024-12-06 20:06:08.878856: Pseudo dice [0.5373]
2024-12-06 20:06:08.879706: Epoch time: 103.94 s
2024-12-06 20:06:08.880523: Yayy! New best EMA pseudo Dice: 0.4474
2024-12-06 20:06:10.710349: 
2024-12-06 20:06:10.711659: Epoch 4
2024-12-06 20:06:10.712443: Current learning rate: 0.00996
2024-12-06 20:07:55.913811: Validation loss improved from -0.22566 to -0.24490! Patience: 0/50
2024-12-06 20:07:55.914876: train_loss -0.355
2024-12-06 20:07:55.915615: val_loss -0.2449
2024-12-06 20:07:55.916329: Pseudo dice [0.562]
2024-12-06 20:07:55.917098: Epoch time: 105.21 s
2024-12-06 20:07:56.283944: Yayy! New best EMA pseudo Dice: 0.4589
2024-12-06 20:07:58.182109: 
2024-12-06 20:07:58.183405: Epoch 5
2024-12-06 20:07:58.184124: Current learning rate: 0.00995
2024-12-06 20:09:41.391399: Validation loss improved from -0.24490 to -0.27628! Patience: 0/50
2024-12-06 20:09:41.392357: train_loss -0.3763
2024-12-06 20:09:41.393529: val_loss -0.2763
2024-12-06 20:09:41.394419: Pseudo dice [0.5755]
2024-12-06 20:09:41.395405: Epoch time: 103.21 s
2024-12-06 20:09:41.396322: Yayy! New best EMA pseudo Dice: 0.4705
2024-12-06 20:09:43.179726: 
2024-12-06 20:09:43.180957: Epoch 6
2024-12-06 20:09:43.181709: Current learning rate: 0.00995
2024-12-06 20:11:15.946270: Validation loss improved from -0.27628 to -0.29877! Patience: 0/50
2024-12-06 20:11:15.947089: train_loss -0.3912
2024-12-06 20:11:15.947945: val_loss -0.2988
2024-12-06 20:11:15.948714: Pseudo dice [0.607]
2024-12-06 20:11:15.949537: Epoch time: 92.77 s
2024-12-06 20:11:15.950294: Yayy! New best EMA pseudo Dice: 0.4842
2024-12-06 20:11:17.787354: 
2024-12-06 20:11:17.788642: Epoch 7
2024-12-06 20:11:17.789475: Current learning rate: 0.00994
2024-12-06 20:13:05.036765: Validation loss improved from -0.29877 to -0.36110! Patience: 0/50
2024-12-06 20:13:05.037806: train_loss -0.4008
2024-12-06 20:13:05.038712: val_loss -0.3611
2024-12-06 20:13:05.039406: Pseudo dice [0.6361]
2024-12-06 20:13:05.040223: Epoch time: 107.25 s
2024-12-06 20:13:05.041029: Yayy! New best EMA pseudo Dice: 0.4994
2024-12-06 20:13:07.474328: 
2024-12-06 20:13:07.475868: Epoch 8
2024-12-06 20:13:07.476640: Current learning rate: 0.00993
2024-12-06 20:14:44.031852: Validation loss did not improve from -0.36110. Patience: 1/50
2024-12-06 20:14:44.033535: train_loss -0.4182
2024-12-06 20:14:44.034795: val_loss -0.3404
2024-12-06 20:14:44.035568: Pseudo dice [0.6267]
2024-12-06 20:14:44.036441: Epoch time: 96.56 s
2024-12-06 20:14:44.037164: Yayy! New best EMA pseudo Dice: 0.5121
2024-12-06 20:14:45.945914: 
2024-12-06 20:14:45.947365: Epoch 9
2024-12-06 20:14:45.948211: Current learning rate: 0.00992
2024-12-06 20:16:32.031644: Validation loss did not improve from -0.36110. Patience: 2/50
2024-12-06 20:16:32.032456: train_loss -0.4399
2024-12-06 20:16:32.033277: val_loss -0.3454
2024-12-06 20:16:32.034024: Pseudo dice [0.6207]
2024-12-06 20:16:32.034764: Epoch time: 106.09 s
2024-12-06 20:16:32.453825: Yayy! New best EMA pseudo Dice: 0.523
2024-12-06 20:16:34.328782: 
2024-12-06 20:16:34.330339: Epoch 10
2024-12-06 20:16:34.331288: Current learning rate: 0.00991
2024-12-06 20:18:16.866584: Validation loss improved from -0.36110 to -0.42055! Patience: 2/50
2024-12-06 20:18:16.867659: train_loss -0.4365
2024-12-06 20:18:16.868567: val_loss -0.4205
2024-12-06 20:18:16.869431: Pseudo dice [0.6634]
2024-12-06 20:18:16.870242: Epoch time: 102.54 s
2024-12-06 20:18:16.871033: Yayy! New best EMA pseudo Dice: 0.537
2024-12-06 20:18:18.682976: 
2024-12-06 20:18:18.684519: Epoch 11
2024-12-06 20:18:18.685627: Current learning rate: 0.0099
2024-12-06 20:20:07.157298: Validation loss did not improve from -0.42055. Patience: 1/50
2024-12-06 20:20:07.158181: train_loss -0.4554
2024-12-06 20:20:07.159014: val_loss -0.4166
2024-12-06 20:20:07.159665: Pseudo dice [0.6628]
2024-12-06 20:20:07.160503: Epoch time: 108.48 s
2024-12-06 20:20:07.161227: Yayy! New best EMA pseudo Dice: 0.5496
2024-12-06 20:20:08.940192: 
2024-12-06 20:20:08.941531: Epoch 12
2024-12-06 20:20:08.942297: Current learning rate: 0.00989
2024-12-06 20:21:56.588109: Validation loss did not improve from -0.42055. Patience: 2/50
2024-12-06 20:21:56.589322: train_loss -0.4628
2024-12-06 20:21:56.590229: val_loss -0.3992
2024-12-06 20:21:56.590962: Pseudo dice [0.6551]
2024-12-06 20:21:56.591854: Epoch time: 107.65 s
2024-12-06 20:21:56.592794: Yayy! New best EMA pseudo Dice: 0.5601
2024-12-06 20:21:58.404419: 
2024-12-06 20:21:58.407386: Epoch 13
2024-12-06 20:21:58.408225: Current learning rate: 0.00988
2024-12-06 20:23:45.566215: Validation loss did not improve from -0.42055. Patience: 3/50
2024-12-06 20:23:45.567294: train_loss -0.4787
2024-12-06 20:23:45.568095: val_loss -0.4045
2024-12-06 20:23:45.568815: Pseudo dice [0.6651]
2024-12-06 20:23:45.569604: Epoch time: 107.16 s
2024-12-06 20:23:45.570354: Yayy! New best EMA pseudo Dice: 0.5706
2024-12-06 20:23:47.385375: 
2024-12-06 20:23:47.386585: Epoch 14
2024-12-06 20:23:47.387329: Current learning rate: 0.00987
2024-12-06 20:25:33.634183: Validation loss did not improve from -0.42055. Patience: 4/50
2024-12-06 20:25:33.635181: train_loss -0.4969
2024-12-06 20:25:33.636022: val_loss -0.3937
2024-12-06 20:25:33.636722: Pseudo dice [0.6482]
2024-12-06 20:25:33.637432: Epoch time: 106.25 s
2024-12-06 20:25:34.056820: Yayy! New best EMA pseudo Dice: 0.5784
2024-12-06 20:25:35.964084: 
2024-12-06 20:25:35.965361: Epoch 15
2024-12-06 20:25:35.966187: Current learning rate: 0.00986
2024-12-06 20:27:10.874636: Validation loss improved from -0.42055 to -0.44428! Patience: 4/50
2024-12-06 20:27:10.875774: train_loss -0.4998
2024-12-06 20:27:10.876810: val_loss -0.4443
2024-12-06 20:27:10.877453: Pseudo dice [0.6772]
2024-12-06 20:27:10.878276: Epoch time: 94.91 s
2024-12-06 20:27:10.878945: Yayy! New best EMA pseudo Dice: 0.5883
2024-12-06 20:27:12.689683: 
2024-12-06 20:27:12.691246: Epoch 16
2024-12-06 20:27:12.692097: Current learning rate: 0.00986
2024-12-06 20:28:47.678929: Validation loss improved from -0.44428 to -0.44590! Patience: 0/50
2024-12-06 20:28:47.680082: train_loss -0.5056
2024-12-06 20:28:47.681071: val_loss -0.4459
2024-12-06 20:28:47.681773: Pseudo dice [0.6818]
2024-12-06 20:28:47.682478: Epoch time: 94.99 s
2024-12-06 20:28:47.683143: Yayy! New best EMA pseudo Dice: 0.5976
2024-12-06 20:28:49.509663: 
2024-12-06 20:28:49.511302: Epoch 17
2024-12-06 20:28:49.512132: Current learning rate: 0.00985
2024-12-06 20:30:25.068954: Validation loss improved from -0.44590 to -0.45269! Patience: 0/50
2024-12-06 20:30:25.070215: train_loss -0.5266
2024-12-06 20:30:25.071074: val_loss -0.4527
2024-12-06 20:30:25.071844: Pseudo dice [0.6955]
2024-12-06 20:30:25.072565: Epoch time: 95.56 s
2024-12-06 20:30:25.073222: Yayy! New best EMA pseudo Dice: 0.6074
2024-12-06 20:30:26.827796: 
2024-12-06 20:30:26.829248: Epoch 18
2024-12-06 20:30:26.829920: Current learning rate: 0.00984
2024-12-06 20:31:58.520995: failed to log:  (<class 'FileNotFoundError'>, FileNotFoundError(2, 'No such file or directory'), <traceback object at 0x7f8188ba9540>)
2024-12-06 20:31:58.520995: failed to log:  (<class 'FileNotFoundError'>, FileNotFoundError(2, 'No such file or directory'), <traceback object at 0x7f8188ba9540>)
2024-12-06 20:31:58.520995: failed to log:  (<class 'FileNotFoundError'>, FileNotFoundError(2, 'No such file or directory'), <traceback object at 0x7f8188ba9540>)
2024-12-06 20:31:58.520995: failed to log:  (<class 'FileNotFoundError'>, FileNotFoundError(2, 'No such file or directory'), <traceback object at 0x7f8188ba9540>)
2024-12-06 20:31:58.520995: failed to log:  (<class 'FileNotFoundError'>, FileNotFoundError(2, 'No such file or directory'), <traceback object at 0x7f8188ba9540>)
2024-12-06 20:31:58.520995: Validation loss did not improve from -0.45269. Patience: 1/50
2024-12-06 20:32:01.025441: failed to log:  (<class 'FileNotFoundError'>, FileNotFoundError(2, 'No such file or directory'), <traceback object at 0x7f81c0375480>)
2024-12-06 20:32:01.025441: failed to log:  (<class 'FileNotFoundError'>, FileNotFoundError(2, 'No such file or directory'), <traceback object at 0x7f81c0375480>)
2024-12-06 20:32:01.025441: failed to log:  (<class 'FileNotFoundError'>, FileNotFoundError(2, 'No such file or directory'), <traceback object at 0x7f81c0375480>)
2024-12-06 20:32:01.025441: failed to log:  (<class 'FileNotFoundError'>, FileNotFoundError(2, 'No such file or directory'), <traceback object at 0x7f81c0375480>)
2024-12-06 20:32:01.025441: failed to log:  (<class 'FileNotFoundError'>, FileNotFoundError(2, 'No such file or directory'), <traceback object at 0x7f81c0375480>)
2024-12-06 20:32:01.025441: train_loss -0.5312
2024-12-06 20:32:03.529200: failed to log:  (<class 'FileNotFoundError'>, FileNotFoundError(2, 'No such file or directory'), <traceback object at 0x7f8188ba9540>)
2024-12-06 20:32:03.529200: failed to log:  (<class 'FileNotFoundError'>, FileNotFoundError(2, 'No such file or directory'), <traceback object at 0x7f8188ba9540>)
2024-12-06 20:32:03.529200: failed to log:  (<class 'FileNotFoundError'>, FileNotFoundError(2, 'No such file or directory'), <traceback object at 0x7f8188ba9540>)
2024-12-06 20:32:03.529200: failed to log:  (<class 'FileNotFoundError'>, FileNotFoundError(2, 'No such file or directory'), <traceback object at 0x7f8188ba9540>)
2024-12-06 20:32:03.529200: failed to log:  (<class 'FileNotFoundError'>, FileNotFoundError(2, 'No such file or directory'), <traceback object at 0x7f8188ba9540>)
2024-12-06 20:32:03.529200: val_loss -0.4312
2024-12-06 20:32:06.032904: failed to log:  (<class 'FileNotFoundError'>, FileNotFoundError(2, 'No such file or directory'), <traceback object at 0x7f81901bbcc0>)
2024-12-06 20:32:06.032904: failed to log:  (<class 'FileNotFoundError'>, FileNotFoundError(2, 'No such file or directory'), <traceback object at 0x7f81901bbcc0>)
2024-12-06 20:32:06.032904: failed to log:  (<class 'FileNotFoundError'>, FileNotFoundError(2, 'No such file or directory'), <traceback object at 0x7f81901bbcc0>)
2024-12-06 20:32:06.032904: failed to log:  (<class 'FileNotFoundError'>, FileNotFoundError(2, 'No such file or directory'), <traceback object at 0x7f81901bbcc0>)
2024-12-06 20:32:06.032904: failed to log:  (<class 'FileNotFoundError'>, FileNotFoundError(2, 'No such file or directory'), <traceback object at 0x7f81901bbcc0>)
2024-12-06 20:32:06.032904: Pseudo dice [0.6647]
2024-12-06 20:32:08.536691: failed to log:  (<class 'FileNotFoundError'>, FileNotFoundError(2, 'No such file or directory'), <traceback object at 0x7f81c07fb440>)
2024-12-06 20:32:08.536691: failed to log:  (<class 'FileNotFoundError'>, FileNotFoundError(2, 'No such file or directory'), <traceback object at 0x7f81c07fb440>)
2024-12-06 20:32:08.536691: failed to log:  (<class 'FileNotFoundError'>, FileNotFoundError(2, 'No such file or directory'), <traceback object at 0x7f81c07fb440>)
2024-12-06 20:32:08.536691: failed to log:  (<class 'FileNotFoundError'>, FileNotFoundError(2, 'No such file or directory'), <traceback object at 0x7f81c07fb440>)
2024-12-06 20:32:08.536691: failed to log:  (<class 'FileNotFoundError'>, FileNotFoundError(2, 'No such file or directory'), <traceback object at 0x7f81c07fb440>)
2024-12-06 20:32:08.536691: Epoch time: 94.2 s
2024-12-06 20:32:11.040454: failed to log:  (<class 'FileNotFoundError'>, FileNotFoundError(2, 'No such file or directory'), <traceback object at 0x7f81c0650280>)
2024-12-06 20:32:11.040454: failed to log:  (<class 'FileNotFoundError'>, FileNotFoundError(2, 'No such file or directory'), <traceback object at 0x7f81c0650280>)
2024-12-06 20:32:11.040454: failed to log:  (<class 'FileNotFoundError'>, FileNotFoundError(2, 'No such file or directory'), <traceback object at 0x7f81c0650280>)
2024-12-06 20:32:11.040454: failed to log:  (<class 'FileNotFoundError'>, FileNotFoundError(2, 'No such file or directory'), <traceback object at 0x7f81c0650280>)
2024-12-06 20:32:11.040454: failed to log:  (<class 'FileNotFoundError'>, FileNotFoundError(2, 'No such file or directory'), <traceback object at 0x7f81c0650280>)
2024-12-06 20:32:11.040454: Yayy! New best EMA pseudo Dice: 0.6131
Traceback (most recent call last):
  File "/home/gridsan/nchutisilp/.conda/envs/nnunet/bin/nnUNetv2_train", line 8, in <module>
    sys.exit(run_training_entry())
  File "/home/gridsan/nchutisilp/projects/nnUNet/nnunetv2/run/run_training.py", line 275, in run_training_entry
    run_training(args.dataset_name_or_id, args.configuration, args.fold, args.tr, args.p, args.pretrained_weights,
  File "/home/gridsan/nchutisilp/projects/nnUNet/nnunetv2/run/run_training.py", line 211, in run_training
    nnunet_trainer.run_training()
  File "/home/gridsan/nchutisilp/projects/nnUNet/nnunetv2/training/nnUNetTrainer/nnUNetTrainer.py", line 1411, in run_training
    self.on_epoch_end()
  File "/home/gridsan/nchutisilp/projects/nnUNet/nnunetv2/training/nnUNetTrainer/nnUNetTrainer.py", line 1171, in on_epoch_end
    self.save_checkpoint(join(self.output_folder, 'checkpoint_best.pth'))
  File "/home/gridsan/nchutisilp/projects/nnUNet/nnunetv2/training/nnUNetTrainer/nnUNetTrainer.py", line 1201, in save_checkpoint
    torch.save(checkpoint, filename)
  File "/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/serialization.py", line 627, in save
    with _open_zipfile_writer(f) as opened_zipfile:
  File "/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/serialization.py", line 501, in _open_zipfile_writer
    return container(name_or_buffer)
  File "/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/serialization.py", line 472, in __init__
    super().__init__(torch._C.PyTorchFileWriter(self.name))
RuntimeError: Parent directory /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_results/Dataset307_Sohee_Calcium_OCT_CrossValidation/nnUNetTrainer__nnUNetPlans__3d_32x160x128_b10/fold_1 does not exist.
Exception in thread Thread-2:
Traceback (most recent call last):
  File "/home/gridsan/nchutisilp/.conda/envs/nnunet/lib/python3.9/threading.py", line 980, in _bootstrap_inner
    self.run()
  File "/home/gridsan/nchutisilp/.conda/envs/nnunet/lib/python3.9/threading.py", line 917, in run
    self._target(*self._args, **self._kwargs)
  File "/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/batchgenerators/dataloading/nondet_multi_threaded_augmenter.py", line 125, in results_loop
Exception in thread Thread-3:
Traceback (most recent call last):
  File "/home/gridsan/nchutisilp/.conda/envs/nnunet/lib/python3.9/threading.py", line 980, in _bootstrap_inner
    raise e
  File "/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/batchgenerators/dataloading/nondet_multi_threaded_augmenter.py", line 103, in results_loop
    self.run()
  File "/home/gridsan/nchutisilp/.conda/envs/nnunet/lib/python3.9/threading.py", line 917, in run
    self._target(*self._args, **self._kwargs)
  File "/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/batchgenerators/dataloading/nondet_multi_threaded_augmenter.py", line 125, in results_loop
    raise RuntimeError("One or more background workers are no longer alive. Exiting. Please check the "
RuntimeError: One or more background workers are no longer alive. Exiting. Please check the print statements above for the actual error message
    raise e
  File "/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/batchgenerators/dataloading/nondet_multi_threaded_augmenter.py", line 103, in results_loop
    raise RuntimeError("One or more background workers are no longer alive. Exiting. Please check the "
RuntimeError: One or more background workers are no longer alive. Exiting. Please check the print statements above for the actual error message
mv: cannot stat '/home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_results/Dataset307_Sohee_Calcium_OCT_CrossValidation/nnUNetTrainer__nnUNetPlans__3d_32x160x128_b10/fold_0': No such file or directory
mv: cannot stat '/home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_results/Dataset307_Sohee_Calcium_OCT_CrossValidation/nnUNetTrainer__nnUNetPlans__3d_32x160x128_b10/fold_1': No such file or directory

############################
INFO: You are using the old nnU-Net default plans. We have updated our recommendations. Please consider using those instead! Read more here: https://github.com/MIC-DKFZ/nnUNet/blob/master/documentation/resenc_presets.md
############################

Using device: cuda:0

#######################################################################
Please cite the following paper when using nnU-Net:
Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.
#######################################################################

2024-12-06 20:32:19.277088: Using torch.compile...

############################
INFO: You are using the old nnU-Net default plans. We have updated our recommendations. Please consider using those instead! Read more here: https://github.com/MIC-DKFZ/nnUNet/blob/master/documentation/resenc_presets.md
############################

Using device: cuda:0

#######################################################################
Please cite the following paper when using nnU-Net:
Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.
#######################################################################

2024-12-06 20:32:19.276911: Using torch.compile...
/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
################### Loading pretrained weights from file  /home/gridsan/nchutisilp/projects/OpenAI-CLIP/clip_pretrained_nnUNet.pt ###################
Below is the list of overlapping blocks in pretrained model and nnUNet architecture:
encoder.stages.0.0.convs.0.conv.weight shape torch.Size([32, 1, 3, 3, 3])
encoder.stages.0.0.convs.0.conv.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.norm.weight shape torch.Size([32])
encoder.stages.0.0.convs.0.norm.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.0.weight shape torch.Size([32, 1, 3, 3, 3])
encoder.stages.0.0.convs.0.all_modules.0.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.1.weight shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.1.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
encoder.stages.0.0.convs.1.conv.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.norm.weight shape torch.Size([32])
encoder.stages.0.0.convs.1.norm.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
encoder.stages.0.0.convs.1.all_modules.0.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.1.weight shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.1.bias shape torch.Size([32])
encoder.stages.1.0.convs.0.conv.weight shape torch.Size([64, 32, 3, 3, 3])
encoder.stages.1.0.convs.0.conv.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.norm.weight shape torch.Size([64])
encoder.stages.1.0.convs.0.norm.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.0.weight shape torch.Size([64, 32, 3, 3, 3])
encoder.stages.1.0.convs.0.all_modules.0.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.1.weight shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.1.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
encoder.stages.1.0.convs.1.conv.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.norm.weight shape torch.Size([64])
encoder.stages.1.0.convs.1.norm.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
encoder.stages.1.0.convs.1.all_modules.0.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.1.weight shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.1.bias shape torch.Size([64])
encoder.stages.2.0.convs.0.conv.weight shape torch.Size([128, 64, 3, 3, 3])
encoder.stages.2.0.convs.0.conv.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.norm.weight shape torch.Size([128])
encoder.stages.2.0.convs.0.norm.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.0.weight shape torch.Size([128, 64, 3, 3, 3])
encoder.stages.2.0.convs.0.all_modules.0.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.1.weight shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.1.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
encoder.stages.2.0.convs.1.conv.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.norm.weight shape torch.Size([128])
encoder.stages.2.0.convs.1.norm.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
encoder.stages.2.0.convs.1.all_modules.0.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.1.weight shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.1.bias shape torch.Size([128])
encoder.stages.3.0.convs.0.conv.weight shape torch.Size([256, 128, 3, 3, 3])
encoder.stages.3.0.convs.0.conv.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.norm.weight shape torch.Size([256])
encoder.stages.3.0.convs.0.norm.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.0.weight shape torch.Size([256, 128, 3, 3, 3])
encoder.stages.3.0.convs.0.all_modules.0.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.1.weight shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.1.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
encoder.stages.3.0.convs.1.conv.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.norm.weight shape torch.Size([256])
encoder.stages.3.0.convs.1.norm.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
encoder.stages.3.0.convs.1.all_modules.0.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.1.weight shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.1.bias shape torch.Size([256])
encoder.stages.4.0.convs.0.conv.weight shape torch.Size([320, 256, 3, 3, 3])
encoder.stages.4.0.convs.0.conv.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.norm.weight shape torch.Size([320])
encoder.stages.4.0.convs.0.norm.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.0.weight shape torch.Size([320, 256, 3, 3, 3])
encoder.stages.4.0.convs.0.all_modules.0.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.1.weight shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.1.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.4.0.convs.1.conv.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.norm.weight shape torch.Size([320])
encoder.stages.4.0.convs.1.norm.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.4.0.convs.1.all_modules.0.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.1.weight shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.1.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.0.conv.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.norm.weight shape torch.Size([320])
encoder.stages.5.0.convs.0.norm.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.0.all_modules.0.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.1.weight shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.1.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.1.conv.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.norm.weight shape torch.Size([320])
encoder.stages.5.0.convs.1.norm.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.1.all_modules.0.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.1.weight shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.0.0.convs.0.conv.weight shape torch.Size([32, 1, 3, 3, 3])
decoder.encoder.stages.0.0.convs.0.conv.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.norm.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.norm.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.0.weight shape torch.Size([32, 1, 3, 3, 3])
decoder.encoder.stages.0.0.convs.0.all_modules.0.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.1.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.1.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.encoder.stages.0.0.convs.1.conv.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.norm.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.norm.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.encoder.stages.0.0.convs.1.all_modules.0.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.1.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.1.bias shape torch.Size([32])
decoder.encoder.stages.1.0.convs.0.conv.weight shape torch.Size([64, 32, 3, 3, 3])
decoder.encoder.stages.1.0.convs.0.conv.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.norm.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.norm.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.0.weight shape torch.Size([64, 32, 3, 3, 3])
decoder.encoder.stages.1.0.convs.0.all_modules.0.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.1.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.1.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.encoder.stages.1.0.convs.1.conv.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.norm.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.norm.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.encoder.stages.1.0.convs.1.all_modules.0.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.1.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.1.bias shape torch.Size([64])
decoder.encoder.stages.2.0.convs.0.conv.weight shape torch.Size([128, 64, 3, 3, 3])
decoder.encoder.stages.2.0.convs.0.conv.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.norm.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.norm.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.0.weight shape torch.Size([128, 64, 3, 3, 3])
decoder.encoder.stages.2.0.convs.0.all_modules.0.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.1.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.1.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.encoder.stages.2.0.convs.1.conv.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.norm.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.norm.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.encoder.stages.2.0.convs.1.all_modules.0.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.1.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.1.bias shape torch.Size([128])
decoder.encoder.stages.3.0.convs.0.conv.weight shape torch.Size([256, 128, 3, 3, 3])
decoder.encoder.stages.3.0.convs.0.conv.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.norm.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.norm.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.0.weight shape torch.Size([256, 128, 3, 3, 3])
decoder.encoder.stages.3.0.convs.0.all_modules.0.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.1.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.1.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.encoder.stages.3.0.convs.1.conv.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.norm.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.norm.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.encoder.stages.3.0.convs.1.all_modules.0.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.1.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.1.bias shape torch.Size([256])
decoder.encoder.stages.4.0.convs.0.conv.weight shape torch.Size([320, 256, 3, 3, 3])
decoder.encoder.stages.4.0.convs.0.conv.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.norm.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.norm.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.0.weight shape torch.Size([320, 256, 3, 3, 3])
decoder.encoder.stages.4.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.4.0.convs.1.conv.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.norm.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.norm.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.4.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.0.conv.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.norm.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.norm.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.1.conv.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.norm.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.norm.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.stages.0.convs.0.conv.weight shape torch.Size([320, 640, 3, 3, 3])
decoder.stages.0.convs.0.conv.bias shape torch.Size([320])
decoder.stages.0.convs.0.norm.weight shape torch.Size([320])
decoder.stages.0.convs.0.norm.bias shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.0.weight shape torch.Size([320, 640, 3, 3, 3])
decoder.stages.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.stages.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.stages.0.convs.1.conv.bias shape torch.Size([320])
decoder.stages.0.convs.1.norm.weight shape torch.Size([320])
decoder.stages.0.convs.1.norm.bias shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.stages.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.stages.1.convs.0.conv.weight shape torch.Size([256, 512, 3, 3, 3])
decoder.stages.1.convs.0.conv.bias shape torch.Size([256])
decoder.stages.1.convs.0.norm.weight shape torch.Size([256])
decoder.stages.1.convs.0.norm.bias shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.0.weight shape torch.Size([256, 512, 3, 3, 3])
decoder.stages.1.convs.0.all_modules.0.bias shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.1.weight shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.1.bias shape torch.Size([256])
decoder.stages.1.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.stages.1.convs.1.conv.bias shape torch.Size([256])
decoder.stages.1.convs.1.norm.weight shape torch.Size([256])
decoder.stages.1.convs.1.norm.bias shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.stages.1.convs.1.all_modules.0.bias shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.1.weight shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.1.bias shape torch.Size([256])
decoder.stages.2.convs.0.conv.weight shape torch.Size([128, 256, 3, 3, 3])
decoder.stages.2.convs.0.conv.bias shape torch.Size([128])
decoder.stages.2.convs.0.norm.weight shape torch.Size([128])
decoder.stages.2.convs.0.norm.bias shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.0.weight shape torch.Size([128, 256, 3, 3, 3])
decoder.stages.2.convs.0.all_modules.0.bias shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.1.weight shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.1.bias shape torch.Size([128])
decoder.stages.2.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.stages.2.convs.1.conv.bias shape torch.Size([128])
decoder.stages.2.convs.1.norm.weight shape torch.Size([128])
decoder.stages.2.convs.1.norm.bias shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.stages.2.convs.1.all_modules.0.bias shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.1.weight shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.1.bias shape torch.Size([128])
decoder.stages.3.convs.0.conv.weight shape torch.Size([64, 128, 3, 3, 3])
decoder.stages.3.convs.0.conv.bias shape torch.Size([64])
decoder.stages.3.convs.0.norm.weight shape torch.Size([64])
decoder.stages.3.convs.0.norm.bias shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.0.weight shape torch.Size([64, 128, 3, 3, 3])
decoder.stages.3.convs.0.all_modules.0.bias shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.1.weight shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.1.bias shape torch.Size([64])
decoder.stages.3.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.stages.3.convs.1.conv.bias shape torch.Size([64])
decoder.stages.3.convs.1.norm.weight shape torch.Size([64])
decoder.stages.3.convs.1.norm.bias shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.stages.3.convs.1.all_modules.0.bias shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.1.weight shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.1.bias shape torch.Size([64])
decoder.stages.4.convs.0.conv.weight shape torch.Size([32, 64, 3, 3, 3])
decoder.stages.4.convs.0.conv.bias shape torch.Size([32])
decoder.stages.4.convs.0.norm.weight shape torch.Size([32])
decoder.stages.4.convs.0.norm.bias shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.0.weight shape torch.Size([32, 64, 3, 3, 3])
decoder.stages.4.convs.0.all_modules.0.bias shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.1.weight shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.1.bias shape torch.Size([32])
decoder.stages.4.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.stages.4.convs.1.conv.bias shape torch.Size([32])
decoder.stages.4.convs.1.norm.weight shape torch.Size([32])
decoder.stages.4.convs.1.norm.bias shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.stages.4.convs.1.all_modules.0.bias shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.1.weight shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.1.bias shape torch.Size([32])
decoder.transpconvs.0.weight shape torch.Size([320, 320, 1, 2, 2])
decoder.transpconvs.0.bias shape torch.Size([320])
decoder.transpconvs.1.weight shape torch.Size([320, 256, 2, 2, 2])
decoder.transpconvs.1.bias shape torch.Size([256])
decoder.transpconvs.2.weight shape torch.Size([256, 128, 2, 2, 2])
decoder.transpconvs.2.bias shape torch.Size([128])
decoder.transpconvs.3.weight shape torch.Size([128, 64, 2, 2, 2])
decoder.transpconvs.3.bias shape torch.Size([64])
decoder.transpconvs.4.weight shape torch.Size([64, 32, 2, 2, 2])
decoder.transpconvs.4.bias shape torch.Size([32])
################### Done ###################
2024-12-06 20:32:20.096706: do_dummy_2d_data_aug: True
2024-12-06 20:32:20.099164: Using splits from existing split file: /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset307_Sohee_Calcium_OCT_CrossValidation/splits_final.json
2024-12-06 20:32:20.100856: The split file contains 5 splits.
2024-12-06 20:32:20.101835: Desired fold for training: 3
2024-12-06 20:32:20.102791: This split has 7 training and 1 validation cases.
################### Loading pretrained weights from file  /home/gridsan/nchutisilp/projects/OpenAI-CLIP/clip_pretrained_nnUNet.pt ###################
Below is the list of overlapping blocks in pretrained model and nnUNet architecture:
encoder.stages.0.0.convs.0.conv.weight shape torch.Size([32, 1, 3, 3, 3])
encoder.stages.0.0.convs.0.conv.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.norm.weight shape torch.Size([32])
encoder.stages.0.0.convs.0.norm.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.0.weight shape torch.Size([32, 1, 3, 3, 3])
encoder.stages.0.0.convs.0.all_modules.0.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.1.weight shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.1.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
encoder.stages.0.0.convs.1.conv.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.norm.weight shape torch.Size([32])
encoder.stages.0.0.convs.1.norm.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
encoder.stages.0.0.convs.1.all_modules.0.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.1.weight shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.1.bias shape torch.Size([32])
encoder.stages.1.0.convs.0.conv.weight shape torch.Size([64, 32, 3, 3, 3])
encoder.stages.1.0.convs.0.conv.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.norm.weight shape torch.Size([64])
encoder.stages.1.0.convs.0.norm.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.0.weight shape torch.Size([64, 32, 3, 3, 3])
encoder.stages.1.0.convs.0.all_modules.0.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.1.weight shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.1.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
encoder.stages.1.0.convs.1.conv.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.norm.weight shape torch.Size([64])
encoder.stages.1.0.convs.1.norm.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
encoder.stages.1.0.convs.1.all_modules.0.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.1.weight shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.1.bias shape torch.Size([64])
encoder.stages.2.0.convs.0.conv.weight shape torch.Size([128, 64, 3, 3, 3])
encoder.stages.2.0.convs.0.conv.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.norm.weight shape torch.Size([128])
encoder.stages.2.0.convs.0.norm.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.0.weight shape torch.Size([128, 64, 3, 3, 3])
encoder.stages.2.0.convs.0.all_modules.0.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.1.weight shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.1.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
encoder.stages.2.0.convs.1.conv.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.norm.weight shape torch.Size([128])
encoder.stages.2.0.convs.1.norm.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
encoder.stages.2.0.convs.1.all_modules.0.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.1.weight shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.1.bias shape torch.Size([128])
encoder.stages.3.0.convs.0.conv.weight shape torch.Size([256, 128, 3, 3, 3])
encoder.stages.3.0.convs.0.conv.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.norm.weight shape torch.Size([256])
encoder.stages.3.0.convs.0.norm.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.0.weight shape torch.Size([256, 128, 3, 3, 3])
encoder.stages.3.0.convs.0.all_modules.0.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.1.weight shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.1.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
encoder.stages.3.0.convs.1.conv.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.norm.weight shape torch.Size([256])
encoder.stages.3.0.convs.1.norm.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
encoder.stages.3.0.convs.1.all_modules.0.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.1.weight shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.1.bias shape torch.Size([256])
encoder.stages.4.0.convs.0.conv.weight shape torch.Size([320, 256, 3, 3, 3])
encoder.stages.4.0.convs.0.conv.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.norm.weight shape torch.Size([320])
encoder.stages.4.0.convs.0.norm.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.0.weight shape torch.Size([320, 256, 3, 3, 3])
encoder.stages.4.0.convs.0.all_modules.0.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.1.weight shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.1.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.4.0.convs.1.conv.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.norm.weight shape torch.Size([320])
encoder.stages.4.0.convs.1.norm.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.4.0.convs.1.all_modules.0.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.1.weight shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.1.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.0.conv.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.norm.weight shape torch.Size([320])
encoder.stages.5.0.convs.0.norm.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.0.all_modules.0.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.1.weight shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.1.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.1.conv.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.norm.weight shape torch.Size([320])
encoder.stages.5.0.convs.1.norm.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.1.all_modules.0.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.1.weight shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.0.0.convs.0.conv.weight shape torch.Size([32, 1, 3, 3, 3])
decoder.encoder.stages.0.0.convs.0.conv.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.norm.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.norm.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.0.weight shape torch.Size([32, 1, 3, 3, 3])
decoder.encoder.stages.0.0.convs.0.all_modules.0.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.1.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.1.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.encoder.stages.0.0.convs.1.conv.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.norm.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.norm.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.encoder.stages.0.0.convs.1.all_modules.0.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.1.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.1.bias shape torch.Size([32])
decoder.encoder.stages.1.0.convs.0.conv.weight shape torch.Size([64, 32, 3, 3, 3])
decoder.encoder.stages.1.0.convs.0.conv.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.norm.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.norm.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.0.weight shape torch.Size([64, 32, 3, 3, 3])
decoder.encoder.stages.1.0.convs.0.all_modules.0.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.1.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.1.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.encoder.stages.1.0.convs.1.conv.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.norm.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.norm.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.encoder.stages.1.0.convs.1.all_modules.0.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.1.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.1.bias shape torch.Size([64])
decoder.encoder.stages.2.0.convs.0.conv.weight shape torch.Size([128, 64, 3, 3, 3])
decoder.encoder.stages.2.0.convs.0.conv.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.norm.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.norm.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.0.weight shape torch.Size([128, 64, 3, 3, 3])
decoder.encoder.stages.2.0.convs.0.all_modules.0.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.1.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.1.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.encoder.stages.2.0.convs.1.conv.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.norm.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.norm.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.encoder.stages.2.0.convs.1.all_modules.0.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.1.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.1.bias shape torch.Size([128])
decoder.encoder.stages.3.0.convs.0.conv.weight shape torch.Size([256, 128, 3, 3, 3])
decoder.encoder.stages.3.0.convs.0.conv.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.norm.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.norm.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.0.weight shape torch.Size([256, 128, 3, 3, 3])
decoder.encoder.stages.3.0.convs.0.all_modules.0.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.1.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.1.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.encoder.stages.3.0.convs.1.conv.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.norm.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.norm.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.encoder.stages.3.0.convs.1.all_modules.0.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.1.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.1.bias shape torch.Size([256])
decoder.encoder.stages.4.0.convs.0.conv.weight shape torch.Size([320, 256, 3, 3, 3])
decoder.encoder.stages.4.0.convs.0.conv.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.norm.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.norm.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.0.weight shape torch.Size([320, 256, 3, 3, 3])
decoder.encoder.stages.4.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.4.0.convs.1.conv.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.norm.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.norm.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.4.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.0.conv.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.norm.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.norm.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.1.conv.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.norm.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.norm.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.stages.0.convs.0.conv.weight shape torch.Size([320, 640, 3, 3, 3])
decoder.stages.0.convs.0.conv.bias shape torch.Size([320])
decoder.stages.0.convs.0.norm.weight shape torch.Size([320])
decoder.stages.0.convs.0.norm.bias shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.0.weight shape torch.Size([320, 640, 3, 3, 3])
decoder.stages.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.stages.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.stages.0.convs.1.conv.bias shape torch.Size([320])
decoder.stages.0.convs.1.norm.weight shape torch.Size([320])
decoder.stages.0.convs.1.norm.bias shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.stages.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.stages.1.convs.0.conv.weight shape torch.Size([256, 512, 3, 3, 3])
decoder.stages.1.convs.0.conv.bias shape torch.Size([256])
decoder.stages.1.convs.0.norm.weight shape torch.Size([256])
decoder.stages.1.convs.0.norm.bias shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.0.weight shape torch.Size([256, 512, 3, 3, 3])
decoder.stages.1.convs.0.all_modules.0.bias shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.1.weight shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.1.bias shape torch.Size([256])
decoder.stages.1.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.stages.1.convs.1.conv.bias shape torch.Size([256])
decoder.stages.1.convs.1.norm.weight shape torch.Size([256])
decoder.stages.1.convs.1.norm.bias shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.stages.1.convs.1.all_modules.0.bias shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.1.weight shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.1.bias shape torch.Size([256])
decoder.stages.2.convs.0.conv.weight shape torch.Size([128, 256, 3, 3, 3])
decoder.stages.2.convs.0.conv.bias shape torch.Size([128])
decoder.stages.2.convs.0.norm.weight shape torch.Size([128])
decoder.stages.2.convs.0.norm.bias shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.0.weight shape torch.Size([128, 256, 3, 3, 3])
decoder.stages.2.convs.0.all_modules.0.bias shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.1.weight shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.1.bias shape torch.Size([128])
decoder.stages.2.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.stages.2.convs.1.conv.bias shape torch.Size([128])
decoder.stages.2.convs.1.norm.weight shape torch.Size([128])
decoder.stages.2.convs.1.norm.bias shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.stages.2.convs.1.all_modules.0.bias shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.1.weight shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.1.bias shape torch.Size([128])
decoder.stages.3.convs.0.conv.weight shape torch.Size([64, 128, 3, 3, 3])
decoder.stages.3.convs.0.conv.bias shape torch.Size([64])
decoder.stages.3.convs.0.norm.weight shape torch.Size([64])
decoder.stages.3.convs.0.norm.bias shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.0.weight shape torch.Size([64, 128, 3, 3, 3])
decoder.stages.3.convs.0.all_modules.0.bias shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.1.weight shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.1.bias shape torch.Size([64])
decoder.stages.3.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.stages.3.convs.1.conv.bias shape torch.Size([64])
decoder.stages.3.convs.1.norm.weight shape torch.Size([64])
decoder.stages.3.convs.1.norm.bias shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.stages.3.convs.1.all_modules.0.bias shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.1.weight shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.1.bias shape torch.Size([64])
decoder.stages.4.convs.0.conv.weight shape torch.Size([32, 64, 3, 3, 3])
decoder.stages.4.convs.0.conv.bias shape torch.Size([32])
decoder.stages.4.convs.0.norm.weight shape torch.Size([32])
decoder.stages.4.convs.0.norm.bias shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.0.weight shape torch.Size([32, 64, 3, 3, 3])
decoder.stages.4.convs.0.all_modules.0.bias shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.1.weight shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.1.bias shape torch.Size([32])
decoder.stages.4.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.stages.4.convs.1.conv.bias shape torch.Size([32])
decoder.stages.4.convs.1.norm.weight shape torch.Size([32])
decoder.stages.4.convs.1.norm.bias shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.stages.4.convs.1.all_modules.0.bias shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.1.weight shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.1.bias shape torch.Size([32])
decoder.transpconvs.0.weight shape torch.Size([320, 320, 1, 2, 2])
decoder.transpconvs.0.bias shape torch.Size([320])
decoder.transpconvs.1.weight shape torch.Size([320, 256, 2, 2, 2])
decoder.transpconvs.1.bias shape torch.Size([256])
decoder.transpconvs.2.weight shape torch.Size([256, 128, 2, 2, 2])
decoder.transpconvs.2.bias shape torch.Size([128])
decoder.transpconvs.3.weight shape torch.Size([128, 64, 2, 2, 2])
decoder.transpconvs.3.bias shape torch.Size([64])
decoder.transpconvs.4.weight shape torch.Size([64, 32, 2, 2, 2])
decoder.transpconvs.4.bias shape torch.Size([32])
################### Done ###################
2024-12-06 20:32:20.150943: do_dummy_2d_data_aug: True
2024-12-06 20:32:20.153025: Using splits from existing split file: /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset307_Sohee_Calcium_OCT_CrossValidation/splits_final.json
2024-12-06 20:32:20.155203: The split file contains 5 splits.
2024-12-06 20:32:20.156301: Desired fold for training: 4
2024-12-06 20:32:20.157371: This split has 7 training and 1 validation cases.
using pin_memory on device 0
using pin_memory on device 0
using pin_memory on device 0

This is the configuration used by this training:
Configuration name: 3d_32x160x128_b10
 {'data_identifier': 'nnUNetPlans_3d_fullres', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 10, 'patch_size': [32, 160, 128], 'median_image_size_in_voxels': [375.0, 498.0, 498.0], 'spacing': [1.0, 1.0, 1.0], 'normalization_schemes': ['ZScoreNormalization'], 'use_mask_for_norm': [False], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'architecture': {'network_class_name': 'dynamic_network_architectures.architectures.unet.PlainConvUNet', 'arch_kwargs': {'n_stages': 6, 'features_per_stage': [32, 64, 128, 256, 320, 320], 'conv_op': 'torch.nn.modules.conv.Conv3d', 'kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]], 'strides': [[1, 1, 1], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [1, 2, 2]], 'n_conv_per_stage': [2, 2, 2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2, 2, 2], 'conv_bias': True, 'norm_op': 'torch.nn.modules.instancenorm.InstanceNorm3d', 'norm_op_kwargs': {'eps': 1e-05, 'affine': True}, 'dropout_op': None, 'dropout_op_kwargs': None, 'nonlin': 'torch.nn.LeakyReLU', 'nonlin_kwargs': {'inplace': True}, 'deep_supervision': True}, '_kw_requires_import': ['conv_op', 'norm_op', 'dropout_op', 'nonlin']}, 'batch_dice': True, 'inherits_from': '3d_fullres'} 

These are the global plan.json settings:
 {'dataset_name': 'Dataset307_Sohee_Calcium_OCT_CrossValidation', 'plans_name': 'nnUNetPlans', 'original_median_spacing_after_transp': [1.0, 1.0, 1.0], 'original_median_shape_after_transp': [375, 498, 498], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 0.4977976381778717, 'mean': 0.13507525622844696, 'median': 0.09599608182907104, 'min': 0.0, 'percentile_00_5': 0.014754901640117168, 'percentile_99_5': 0.4977976381778717, 'std': 0.12025152146816254}}} 

2024-12-06 20:32:28.165542: unpacking dataset...
using pin_memory on device 0

This is the configuration used by this training:
Configuration name: 3d_32x160x128_b10
 {'data_identifier': 'nnUNetPlans_3d_fullres', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 10, 'patch_size': [32, 160, 128], 'median_image_size_in_voxels': [375.0, 498.0, 498.0], 'spacing': [1.0, 1.0, 1.0], 'normalization_schemes': ['ZScoreNormalization'], 'use_mask_for_norm': [False], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'architecture': {'network_class_name': 'dynamic_network_architectures.architectures.unet.PlainConvUNet', 'arch_kwargs': {'n_stages': 6, 'features_per_stage': [32, 64, 128, 256, 320, 320], 'conv_op': 'torch.nn.modules.conv.Conv3d', 'kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]], 'strides': [[1, 1, 1], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [1, 2, 2]], 'n_conv_per_stage': [2, 2, 2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2, 2, 2], 'conv_bias': True, 'norm_op': 'torch.nn.modules.instancenorm.InstanceNorm3d', 'norm_op_kwargs': {'eps': 1e-05, 'affine': True}, 'dropout_op': None, 'dropout_op_kwargs': None, 'nonlin': 'torch.nn.LeakyReLU', 'nonlin_kwargs': {'inplace': True}, 'deep_supervision': True}, '_kw_requires_import': ['conv_op', 'norm_op', 'dropout_op', 'nonlin']}, 'batch_dice': True, 'inherits_from': '3d_fullres'} 

These are the global plan.json settings:
 {'dataset_name': 'Dataset307_Sohee_Calcium_OCT_CrossValidation', 'plans_name': 'nnUNetPlans', 'original_median_spacing_after_transp': [1.0, 1.0, 1.0], 'original_median_shape_after_transp': [375, 498, 498], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 0.4977976381778717, 'mean': 0.13507525622844696, 'median': 0.09599608182907104, 'min': 0.0, 'percentile_00_5': 0.014754901640117168, 'percentile_99_5': 0.4977976381778717, 'std': 0.12025152146816254}}} 

2024-12-06 20:32:28.330271: unpacking dataset...
2024-12-06 20:32:32.644132: unpacking done...
2024-12-06 20:32:32.809914: Unable to plot network architecture: nnUNet_compile is enabled!
2024-12-06 20:32:32.858517: 
2024-12-06 20:32:32.859702: Epoch 0
2024-12-06 20:32:32.861104: Current learning rate: 0.01
2024-12-06 20:34:53.809745: Validation loss improved from 1000.00000 to -0.16367! Patience: 0/50
2024-12-06 20:34:53.811837: train_loss -0.093
2024-12-06 20:34:53.812974: val_loss -0.1637
2024-12-06 20:34:53.813789: Pseudo dice [0.5261]
2024-12-06 20:34:53.814870: Epoch time: 140.96 s
2024-12-06 20:34:53.815604: Yayy! New best EMA pseudo Dice: 0.5261
2024-12-06 20:34:55.459358: 
2024-12-06 20:34:55.460775: Epoch 1
2024-12-06 20:34:55.461686: Current learning rate: 0.00999
2024-12-06 20:36:35.413450: Validation loss improved from -0.16367 to -0.21363! Patience: 0/50
2024-12-06 20:36:35.414575: train_loss -0.2338
2024-12-06 20:36:35.415690: val_loss -0.2136
2024-12-06 20:36:35.416685: Pseudo dice [0.5455]
2024-12-06 20:36:35.417609: Epoch time: 99.96 s
2024-12-06 20:36:35.418572: Yayy! New best EMA pseudo Dice: 0.528
2024-12-06 20:36:37.193308: 
2024-12-06 20:36:37.194872: Epoch 2
2024-12-06 20:36:37.195591: Current learning rate: 0.00998
2024-12-06 20:38:16.895854: Validation loss did not improve from -0.21363. Patience: 1/50
2024-12-06 20:38:16.897032: train_loss -0.2912
2024-12-06 20:38:16.897975: val_loss -0.1162
2024-12-06 20:38:16.898631: Pseudo dice [0.4955]
2024-12-06 20:38:16.899427: Epoch time: 99.71 s
2024-12-06 20:38:18.316073: 
2024-12-06 20:38:18.317254: Epoch 3
2024-12-06 20:38:18.317971: Current learning rate: 0.00997
2024-12-06 20:39:58.655529: Validation loss improved from -0.21363 to -0.24805! Patience: 1/50
2024-12-06 20:39:58.657662: train_loss -0.3214
2024-12-06 20:39:58.658974: val_loss -0.248
2024-12-06 20:39:58.659733: Pseudo dice [0.5791]
2024-12-06 20:39:58.660365: Epoch time: 100.34 s
2024-12-06 20:39:58.661170: Yayy! New best EMA pseudo Dice: 0.5302
2024-12-06 20:40:00.390360: 
2024-12-06 20:40:00.391732: Epoch 4
2024-12-06 20:40:00.392506: Current learning rate: 0.00996
2024-12-06 20:41:40.065277: Validation loss improved from -0.24805 to -0.25107! Patience: 0/50
2024-12-06 20:41:40.066591: train_loss -0.3397
2024-12-06 20:41:40.067506: val_loss -0.2511
2024-12-06 20:41:40.068209: Pseudo dice [0.5726]
2024-12-06 20:41:40.068860: Epoch time: 99.68 s
2024-12-06 20:41:40.696233: Yayy! New best EMA pseudo Dice: 0.5344
2024-12-06 20:41:42.464894: 
2024-12-06 20:41:42.466409: Epoch 5
2024-12-06 20:41:42.467254: Current learning rate: 0.00995
2024-12-06 20:43:21.521851: Validation loss improved from -0.25107 to -0.32108! Patience: 0/50
2024-12-06 20:43:21.523141: train_loss -0.3769
2024-12-06 20:43:21.524237: val_loss -0.3211
2024-12-06 20:43:21.525104: Pseudo dice [0.6175]
2024-12-06 20:43:21.525944: Epoch time: 99.06 s
2024-12-06 20:43:21.526826: Yayy! New best EMA pseudo Dice: 0.5427
2024-12-06 20:43:23.236477: 
2024-12-06 20:43:23.238053: Epoch 6
2024-12-06 20:43:23.238996: Current learning rate: 0.00995
2024-12-06 20:45:03.382059: Validation loss did not improve from -0.32108. Patience: 1/50
2024-12-06 20:45:03.383356: train_loss -0.3853
2024-12-06 20:45:03.384687: val_loss -0.3022
2024-12-06 20:45:03.385475: Pseudo dice [0.5922]
2024-12-06 20:45:03.386281: Epoch time: 100.15 s
2024-12-06 20:45:03.387057: Yayy! New best EMA pseudo Dice: 0.5477
2024-12-06 20:45:05.145650: 
2024-12-06 20:45:05.147268: Epoch 7
2024-12-06 20:45:05.148064: Current learning rate: 0.00994
2024-12-06 20:46:44.367553: Validation loss improved from -0.32108 to -0.32704! Patience: 1/50
2024-12-06 20:46:44.368713: train_loss -0.3938
2024-12-06 20:46:44.369764: val_loss -0.327
2024-12-06 20:46:44.370687: Pseudo dice [0.6225]
2024-12-06 20:46:44.371450: Epoch time: 99.22 s
2024-12-06 20:46:44.372248: Yayy! New best EMA pseudo Dice: 0.5552
2024-12-06 20:46:47.044060: 
2024-12-06 20:46:47.045425: Epoch 8
2024-12-06 20:46:47.046371: Current learning rate: 0.00993
2024-12-06 20:48:27.205580: Validation loss improved from -0.32704 to -0.38540! Patience: 0/50
2024-12-06 20:48:27.206886: train_loss -0.4366
2024-12-06 20:48:27.207952: val_loss -0.3854
2024-12-06 20:48:27.208988: Pseudo dice [0.6661]
2024-12-06 20:48:27.209983: Epoch time: 100.16 s
2024-12-06 20:48:27.210655: Yayy! New best EMA pseudo Dice: 0.5663
2024-12-06 20:48:29.020585: 
2024-12-06 20:48:29.022170: Epoch 9
2024-12-06 20:48:29.023027: Current learning rate: 0.00992
2024-12-06 20:50:09.342018: Validation loss did not improve from -0.38540. Patience: 1/50
2024-12-06 20:50:09.342789: train_loss -0.4463
2024-12-06 20:50:09.343533: val_loss -0.2736
2024-12-06 20:50:09.344134: Pseudo dice [0.5828]
2024-12-06 20:50:09.344830: Epoch time: 100.32 s
2024-12-06 20:50:09.741930: Yayy! New best EMA pseudo Dice: 0.5679
2024-12-06 20:50:11.459842: 
2024-12-06 20:50:11.461162: Epoch 10
2024-12-06 20:50:11.461837: Current learning rate: 0.00991
2024-12-06 20:51:51.984265: Validation loss did not improve from -0.38540. Patience: 2/50
2024-12-06 20:51:51.985450: train_loss -0.4675
2024-12-06 20:51:51.986430: val_loss -0.3289
2024-12-06 20:51:51.987079: Pseudo dice [0.6164]
2024-12-06 20:51:51.987772: Epoch time: 100.53 s
2024-12-06 20:51:51.988444: Yayy! New best EMA pseudo Dice: 0.5728
2024-12-06 20:51:53.690461: 
2024-12-06 20:51:53.692154: Epoch 11
2024-12-06 20:51:53.692958: Current learning rate: 0.0099
2024-12-06 20:53:33.854599: Validation loss did not improve from -0.38540. Patience: 3/50
2024-12-06 20:53:33.855744: train_loss -0.4857
2024-12-06 20:53:33.856886: val_loss -0.3426
2024-12-06 20:53:33.857638: Pseudo dice [0.6545]
2024-12-06 20:53:33.858428: Epoch time: 100.17 s
2024-12-06 20:53:33.859227: Yayy! New best EMA pseudo Dice: 0.5809
2024-12-06 20:53:35.600610: 
2024-12-06 20:53:35.603209: Epoch 12
2024-12-06 20:53:35.604039: Current learning rate: 0.00989
2024-12-06 20:55:15.391766: Validation loss did not improve from -0.38540. Patience: 4/50
2024-12-06 20:55:15.393079: train_loss -0.5007
2024-12-06 20:55:15.394053: val_loss -0.3711
2024-12-06 20:55:15.394819: Pseudo dice [0.6409]
2024-12-06 20:55:15.395580: Epoch time: 99.79 s
2024-12-06 20:55:15.396335: Yayy! New best EMA pseudo Dice: 0.5869
2024-12-06 20:55:17.166021: 
2024-12-06 20:55:17.167659: Epoch 13
2024-12-06 20:55:17.168519: Current learning rate: 0.00988
2024-12-06 20:56:56.965495: Validation loss did not improve from -0.38540. Patience: 5/50
2024-12-06 20:56:56.966850: train_loss -0.4954
2024-12-06 20:56:56.967817: val_loss -0.2561
2024-12-06 20:56:56.968673: Pseudo dice [0.5636]
2024-12-06 20:56:56.969385: Epoch time: 99.8 s
2024-12-06 20:56:58.314543: 
2024-12-06 20:56:58.316097: Epoch 14
2024-12-06 20:56:58.317058: Current learning rate: 0.00987
2024-12-06 20:58:37.722832: Validation loss improved from -0.38540 to -0.41609! Patience: 5/50
2024-12-06 20:58:37.723675: train_loss -0.5116
2024-12-06 20:58:37.724525: val_loss -0.4161
2024-12-06 20:58:37.725238: Pseudo dice [0.6766]
2024-12-06 20:58:37.725898: Epoch time: 99.41 s
2024-12-06 20:58:38.119132: Yayy! New best EMA pseudo Dice: 0.5938
2024-12-06 20:58:39.885232: 
2024-12-06 20:58:39.886285: Epoch 15
2024-12-06 20:58:39.887036: Current learning rate: 0.00986
2024-12-06 21:00:19.890158: Validation loss did not improve from -0.41609. Patience: 1/50
2024-12-06 21:00:19.891414: train_loss -0.5131
2024-12-06 21:00:19.892572: val_loss -0.353
2024-12-06 21:00:19.893568: Pseudo dice [0.6442]
2024-12-06 21:00:19.894370: Epoch time: 100.01 s
2024-12-06 21:00:19.895272: Yayy! New best EMA pseudo Dice: 0.5988
2024-12-06 21:00:21.632378: 
2024-12-06 21:00:21.633953: Epoch 16
2024-12-06 21:00:21.634907: Current learning rate: 0.00986
2024-12-06 21:02:01.511083: Validation loss did not improve from -0.41609. Patience: 2/50
2024-12-06 21:02:01.512248: train_loss -0.5227
2024-12-06 21:02:01.513226: val_loss -0.3841
2024-12-06 21:02:01.513993: Pseudo dice [0.6582]
2024-12-06 21:02:01.514753: Epoch time: 99.88 s
2024-12-06 21:02:01.515461: Yayy! New best EMA pseudo Dice: 0.6048
2024-12-06 21:02:03.312489: 
2024-12-06 21:02:03.314220: Epoch 17
2024-12-06 21:02:03.315054: Current learning rate: 0.00985
2024-12-06 21:03:43.853043: Validation loss improved from -0.41609 to -0.43705! Patience: 2/50
2024-12-06 21:03:43.854323: train_loss -0.5155
2024-12-06 21:03:43.855312: val_loss -0.437
2024-12-06 21:03:43.856096: Pseudo dice [0.6898]
2024-12-06 21:03:43.856805: Epoch time: 100.54 s
2024-12-06 21:03:43.857478: Yayy! New best EMA pseudo Dice: 0.6133
2024-12-06 21:03:45.669035: 
2024-12-06 21:03:45.670548: Epoch 18
2024-12-06 21:03:45.671239: Current learning rate: 0.00984
2024-12-06 21:05:25.537684: Validation loss did not improve from -0.43705. Patience: 1/50
2024-12-06 21:05:25.538688: train_loss -0.5329
2024-12-06 21:05:25.539538: val_loss -0.2817
2024-12-06 21:05:25.540226: Pseudo dice [0.6036]
2024-12-06 21:05:25.541121: Epoch time: 99.87 s
2024-12-06 21:05:27.575877: 
2024-12-06 21:05:27.577684: Epoch 19
2024-12-06 21:05:27.578629: Current learning rate: 0.00983
2024-12-06 21:07:06.845023: Validation loss did not improve from -0.43705. Patience: 2/50
2024-12-06 21:07:06.846158: train_loss -0.5424
2024-12-06 21:07:06.847019: val_loss -0.3034
2024-12-06 21:07:06.847758: Pseudo dice [0.6214]
2024-12-06 21:07:06.848465: Epoch time: 99.27 s
2024-12-06 21:07:08.597447: 
2024-12-06 21:07:08.598884: Epoch 20
2024-12-06 21:07:08.599725: Current learning rate: 0.00982
2024-12-06 21:08:47.986154: Validation loss did not improve from -0.43705. Patience: 3/50
2024-12-06 21:08:47.987424: train_loss -0.5376
2024-12-06 21:08:47.988224: val_loss -0.2308
2024-12-06 21:08:47.988932: Pseudo dice [0.5772]
2024-12-06 21:08:47.989700: Epoch time: 99.39 s
2024-12-06 21:08:49.385975: 
2024-12-06 21:08:49.387503: Epoch 21
2024-12-06 21:08:49.388527: Current learning rate: 0.00981
2024-12-06 21:10:28.621252: Validation loss did not improve from -0.43705. Patience: 4/50
2024-12-06 21:10:28.622670: train_loss -0.5466
2024-12-06 21:10:28.623918: val_loss -0.3825
2024-12-06 21:10:28.624796: Pseudo dice [0.6638]
2024-12-06 21:10:28.625565: Epoch time: 99.24 s
2024-12-06 21:10:28.626223: Yayy! New best EMA pseudo Dice: 0.615
2024-12-06 21:10:30.310988: 
2024-12-06 21:10:30.312580: Epoch 22
2024-12-06 21:10:30.313544: Current learning rate: 0.0098
2024-12-06 21:12:10.718634: Validation loss did not improve from -0.43705. Patience: 5/50
2024-12-06 21:12:10.719976: train_loss -0.5388
2024-12-06 21:12:10.720822: val_loss -0.3836
2024-12-06 21:12:10.721622: Pseudo dice [0.6648]
2024-12-06 21:12:10.722516: Epoch time: 100.41 s
2024-12-06 21:12:10.723410: Yayy! New best EMA pseudo Dice: 0.62
2024-12-06 21:12:12.410871: 
2024-12-06 21:12:12.412497: Epoch 23
2024-12-06 21:12:12.413364: Current learning rate: 0.00979
2024-12-06 21:13:52.188965: Validation loss did not improve from -0.43705. Patience: 6/50
2024-12-06 21:13:52.190196: train_loss -0.5472
2024-12-06 21:13:52.191113: val_loss -0.3913
2024-12-06 21:13:52.191970: Pseudo dice [0.6646]
2024-12-06 21:13:52.192710: Epoch time: 99.78 s
2024-12-06 21:13:52.193381: Yayy! New best EMA pseudo Dice: 0.6245
2024-12-06 21:13:53.850679: 
2024-12-06 21:13:53.852345: Epoch 24
2024-12-06 21:13:53.853302: Current learning rate: 0.00978
2024-12-06 21:15:33.420761: Validation loss did not improve from -0.43705. Patience: 7/50
2024-12-06 21:15:33.422064: train_loss -0.5468
2024-12-06 21:15:33.423398: val_loss -0.2516
2024-12-06 21:15:33.424670: Pseudo dice [0.5829]
2024-12-06 21:15:33.425569: Epoch time: 99.57 s
2024-12-06 21:15:35.127529: 
2024-12-06 21:15:35.129052: Epoch 25
2024-12-06 21:15:35.129933: Current learning rate: 0.00977
2024-12-06 21:17:14.084410: Validation loss did not improve from -0.43705. Patience: 8/50
2024-12-06 21:17:14.085478: train_loss -0.5517
2024-12-06 21:17:14.086354: val_loss -0.335
2024-12-06 21:17:14.087080: Pseudo dice [0.6264]
2024-12-06 21:17:14.087877: Epoch time: 98.96 s
2024-12-06 21:17:15.397533: 
2024-12-06 21:17:15.399128: Epoch 26
2024-12-06 21:17:15.399856: Current learning rate: 0.00977
2024-12-06 21:18:55.098245: Validation loss did not improve from -0.43705. Patience: 9/50
2024-12-06 21:18:55.099295: train_loss -0.568
2024-12-06 21:18:55.100314: val_loss -0.3665
2024-12-06 21:18:55.101218: Pseudo dice [0.6466]
2024-12-06 21:18:55.102231: Epoch time: 99.7 s
2024-12-06 21:18:56.399178: 
2024-12-06 21:18:56.400672: Epoch 27
2024-12-06 21:18:56.401707: Current learning rate: 0.00976
2024-12-06 21:20:36.512776: Validation loss did not improve from -0.43705. Patience: 10/50
2024-12-06 21:20:36.513971: train_loss -0.5873
2024-12-06 21:20:36.515007: val_loss -0.2459
2024-12-06 21:20:36.515858: Pseudo dice [0.5791]
2024-12-06 21:20:36.516640: Epoch time: 100.12 s
2024-12-06 21:20:37.822420: 
2024-12-06 21:20:37.824235: Epoch 28
2024-12-06 21:20:37.825121: Current learning rate: 0.00975
2024-12-06 21:22:17.543880: Validation loss did not improve from -0.43705. Patience: 11/50
2024-12-06 21:22:17.545024: train_loss -0.5729
2024-12-06 21:22:17.545909: val_loss -0.3891
2024-12-06 21:22:17.546529: Pseudo dice [0.6769]
2024-12-06 21:22:17.547171: Epoch time: 99.72 s
2024-12-06 21:22:17.547809: Yayy! New best EMA pseudo Dice: 0.6248
2024-12-06 21:22:19.227416: 
2024-12-06 21:22:19.229062: Epoch 29
2024-12-06 21:22:19.229796: Current learning rate: 0.00974
2024-12-06 21:23:59.375880: Validation loss did not improve from -0.43705. Patience: 12/50
2024-12-06 21:23:59.377321: train_loss -0.5793
2024-12-06 21:23:59.378251: val_loss -0.3312
2024-12-06 21:23:59.378950: Pseudo dice [0.6371]
2024-12-06 21:23:59.379611: Epoch time: 100.15 s
2024-12-06 21:23:59.798388: Yayy! New best EMA pseudo Dice: 0.6261
2024-12-06 21:24:01.971778: 
2024-12-06 21:24:01.973527: Epoch 30
2024-12-06 21:24:01.974496: Current learning rate: 0.00973
2024-12-06 21:25:42.538408: Validation loss did not improve from -0.43705. Patience: 13/50
2024-12-06 21:25:42.539481: train_loss -0.5848
2024-12-06 21:25:42.540353: val_loss -0.2981
2024-12-06 21:25:42.541150: Pseudo dice [0.6142]
2024-12-06 21:25:42.542003: Epoch time: 100.57 s
2024-12-06 21:25:43.889029: 
2024-12-06 21:25:43.890596: Epoch 31
2024-12-06 21:25:43.891600: Current learning rate: 0.00972
2024-12-06 21:27:25.519535: Validation loss did not improve from -0.43705. Patience: 14/50
2024-12-06 21:27:25.520760: train_loss -0.5761
2024-12-06 21:27:25.521629: val_loss -0.2354
2024-12-06 21:27:25.522538: Pseudo dice [0.5768]
2024-12-06 21:27:25.523300: Epoch time: 101.63 s
2024-12-06 21:27:26.877128: 
2024-12-06 21:27:26.878564: Epoch 32
2024-12-06 21:27:26.879460: Current learning rate: 0.00971
2024-12-06 21:29:07.470228: Validation loss did not improve from -0.43705. Patience: 15/50
2024-12-06 21:29:07.471364: train_loss -0.5849
2024-12-06 21:29:07.472290: val_loss -0.3123
2024-12-06 21:29:07.473101: Pseudo dice [0.6225]
2024-12-06 21:29:07.473846: Epoch time: 100.6 s
2024-12-06 21:29:08.824876: 
2024-12-06 21:29:08.826127: Epoch 33
2024-12-06 21:29:08.827015: Current learning rate: 0.0097
2024-12-06 21:30:48.911332: Validation loss improved from -0.43705 to -0.45405! Patience: 15/50
2024-12-06 21:30:48.912620: train_loss -0.5947
2024-12-06 21:30:48.913650: val_loss -0.454
2024-12-06 21:30:48.914532: Pseudo dice [0.7132]
2024-12-06 21:30:48.915314: Epoch time: 100.09 s
2024-12-06 21:30:48.916177: Yayy! New best EMA pseudo Dice: 0.6296
2024-12-06 21:30:50.678746: 
2024-12-06 21:30:50.680158: Epoch 34
2024-12-06 21:30:50.681089: Current learning rate: 0.00969
2024-12-06 21:32:21.623547: Validation loss did not improve from -0.45405. Patience: 1/50
2024-12-06 21:32:21.624885: train_loss -0.5875
2024-12-06 21:32:21.625671: val_loss -0.3453
2024-12-06 21:32:21.626544: Pseudo dice [0.6406]
2024-12-06 21:32:21.627242: Epoch time: 90.95 s
2024-12-06 21:32:22.000790: Yayy! New best EMA pseudo Dice: 0.6307
2024-12-06 21:32:23.626495: 
2024-12-06 21:32:23.628858: Epoch 35
2024-12-06 21:32:23.629764: Current learning rate: 0.00968
2024-12-06 21:33:54.717393: Validation loss did not improve from -0.45405. Patience: 2/50
2024-12-06 21:33:54.718427: train_loss -0.5955
2024-12-06 21:33:54.719290: val_loss -0.3575
2024-12-06 21:33:54.720106: Pseudo dice [0.6477]
2024-12-06 21:33:54.720863: Epoch time: 91.09 s
2024-12-06 21:33:54.721619: Yayy! New best EMA pseudo Dice: 0.6324
2024-12-06 21:33:56.368131: 
2024-12-06 21:33:56.370296: Epoch 36
2024-12-06 21:33:56.371119: Current learning rate: 0.00968
2024-12-06 21:35:27.541520: Validation loss did not improve from -0.45405. Patience: 3/50
2024-12-06 21:35:27.542377: train_loss -0.5907
2024-12-06 21:35:27.543177: val_loss -0.3218
2024-12-06 21:35:27.543926: Pseudo dice [0.6199]
2024-12-06 21:35:27.544709: Epoch time: 91.18 s
2024-12-06 21:35:28.820276: 
2024-12-06 21:35:28.822055: Epoch 37
2024-12-06 21:35:28.822881: Current learning rate: 0.00967
2024-12-06 21:36:59.835454: Validation loss did not improve from -0.45405. Patience: 4/50
2024-12-06 21:36:59.836509: train_loss -0.6058
2024-12-06 21:36:59.837381: val_loss -0.3101
2024-12-06 21:36:59.838157: Pseudo dice [0.6242]
2024-12-06 21:36:59.838871: Epoch time: 91.02 s
2024-12-06 21:37:01.128198: 
2024-12-06 21:37:01.129426: Epoch 38
2024-12-06 21:37:01.130329: Current learning rate: 0.00966
2024-12-06 21:38:32.528526: Validation loss did not improve from -0.45405. Patience: 5/50
2024-12-06 21:38:32.530394: train_loss -0.6051
2024-12-06 21:38:32.533358: val_loss -0.2962
2024-12-06 21:38:32.534262: Pseudo dice [0.6074]
2024-12-06 21:38:32.535540: Epoch time: 91.4 s
2024-12-06 21:38:34.243208: 
2024-12-06 21:38:34.244778: Epoch 39
2024-12-06 21:38:34.245662: Current learning rate: 0.00965
2024-12-06 21:40:05.182166: Validation loss did not improve from -0.45405. Patience: 6/50
2024-12-06 21:40:05.183529: train_loss -0.615
2024-12-06 21:40:05.184520: val_loss -0.3326
2024-12-06 21:40:05.185220: Pseudo dice [0.6486]
2024-12-06 21:40:05.185853: Epoch time: 90.94 s
2024-12-06 21:40:07.294134: 
2024-12-06 21:40:07.296414: Epoch 40
2024-12-06 21:40:07.297635: Current learning rate: 0.00964
2024-12-06 21:41:38.343924: Validation loss did not improve from -0.45405. Patience: 7/50
2024-12-06 21:41:38.345141: train_loss -0.6057
2024-12-06 21:41:38.346185: val_loss -0.3467
2024-12-06 21:41:38.346936: Pseudo dice [0.6533]
2024-12-06 21:41:38.347566: Epoch time: 91.05 s
2024-12-06 21:41:38.348246: Yayy! New best EMA pseudo Dice: 0.6325
2024-12-06 21:41:40.034832: 
2024-12-06 21:41:40.036768: Epoch 41
2024-12-06 21:41:40.037828: Current learning rate: 0.00963
2024-12-06 21:43:10.811635: Validation loss did not improve from -0.45405. Patience: 8/50
2024-12-06 21:43:10.812385: train_loss -0.6113
2024-12-06 21:43:10.813400: val_loss -0.3582
2024-12-06 21:43:10.814190: Pseudo dice [0.6512]
2024-12-06 21:43:10.814965: Epoch time: 90.78 s
2024-12-06 21:43:10.815804: Yayy! New best EMA pseudo Dice: 0.6344
2024-12-06 21:43:12.444243: 
2024-12-06 21:43:12.445907: Epoch 42
2024-12-06 21:43:12.447221: Current learning rate: 0.00962
2024-12-06 21:44:43.447923: Validation loss did not improve from -0.45405. Patience: 9/50
2024-12-06 21:44:43.448894: train_loss -0.6164
2024-12-06 21:44:43.449795: val_loss -0.3985
2024-12-06 21:44:43.450531: Pseudo dice [0.6785]
2024-12-06 21:44:43.451322: Epoch time: 91.01 s
2024-12-06 21:44:43.452032: Yayy! New best EMA pseudo Dice: 0.6388
2024-12-06 21:44:45.043470: 
2024-12-06 21:44:45.045788: Epoch 43
2024-12-06 21:44:45.046937: Current learning rate: 0.00961
2024-12-06 21:46:15.982104: Validation loss did not improve from -0.45405. Patience: 10/50
2024-12-06 21:46:15.983204: train_loss -0.6324
2024-12-06 21:46:15.984115: val_loss -0.3258
2024-12-06 21:46:15.984777: Pseudo dice [0.6403]
2024-12-06 21:46:15.985435: Epoch time: 90.94 s
2024-12-06 21:46:15.986183: Yayy! New best EMA pseudo Dice: 0.6389
2024-12-06 21:46:17.529221: 
2024-12-06 21:46:17.530811: Epoch 44
2024-12-06 21:46:17.531516: Current learning rate: 0.0096
2024-12-06 21:47:48.347123: Validation loss did not improve from -0.45405. Patience: 11/50
2024-12-06 21:47:48.348123: train_loss -0.6175
2024-12-06 21:47:48.349051: val_loss -0.3537
2024-12-06 21:47:48.349727: Pseudo dice [0.6457]
2024-12-06 21:47:48.350422: Epoch time: 90.82 s
2024-12-06 21:47:48.746360: Yayy! New best EMA pseudo Dice: 0.6396
2024-12-06 21:47:50.327495: 
2024-12-06 21:47:50.329416: Epoch 45
2024-12-06 21:47:50.330088: Current learning rate: 0.00959
2024-12-06 21:49:21.289996: Validation loss did not improve from -0.45405. Patience: 12/50
2024-12-06 21:49:21.291172: train_loss -0.6178
2024-12-06 21:49:21.292081: val_loss -0.2897
2024-12-06 21:49:21.292899: Pseudo dice [0.6255]
2024-12-06 21:49:21.293589: Epoch time: 90.96 s
2024-12-06 21:49:22.509911: 
2024-12-06 21:49:22.512042: Epoch 46
2024-12-06 21:49:22.512807: Current learning rate: 0.00959
2024-12-06 21:50:53.510159: Validation loss did not improve from -0.45405. Patience: 13/50
2024-12-06 21:50:53.511193: train_loss -0.624
2024-12-06 21:50:53.511923: val_loss -0.2279
2024-12-06 21:50:53.512575: Pseudo dice [0.581]
2024-12-06 21:50:53.513257: Epoch time: 91.0 s
2024-12-06 21:50:54.718989: 
2024-12-06 21:50:54.720680: Epoch 47
2024-12-06 21:50:54.721701: Current learning rate: 0.00958
2024-12-06 21:52:25.714519: Validation loss did not improve from -0.45405. Patience: 14/50
2024-12-06 21:52:25.715509: train_loss -0.6216
2024-12-06 21:52:25.716559: val_loss -0.4252
2024-12-06 21:52:25.717550: Pseudo dice [0.6906]
2024-12-06 21:52:25.718325: Epoch time: 91.0 s
2024-12-06 21:52:26.895893: 
2024-12-06 21:52:26.897752: Epoch 48
2024-12-06 21:52:26.898831: Current learning rate: 0.00957
2024-12-06 21:53:57.784156: Validation loss did not improve from -0.45405. Patience: 15/50
2024-12-06 21:53:57.785317: train_loss -0.6246
2024-12-06 21:53:57.786477: val_loss -0.4204
2024-12-06 21:53:57.787258: Pseudo dice [0.6932]
2024-12-06 21:53:57.788230: Epoch time: 90.89 s
2024-12-06 21:53:57.789203: Yayy! New best EMA pseudo Dice: 0.6438
2024-12-06 21:53:59.410022: 
2024-12-06 21:53:59.411798: Epoch 49
2024-12-06 21:53:59.413189: Current learning rate: 0.00956
2024-12-06 21:55:30.255773: Validation loss did not improve from -0.45405. Patience: 16/50
2024-12-06 21:55:30.256655: train_loss -0.6347
2024-12-06 21:55:30.257495: val_loss -0.3224
2024-12-06 21:55:30.258149: Pseudo dice [0.6491]
2024-12-06 21:55:30.258861: Epoch time: 90.85 s
2024-12-06 21:55:30.647179: Yayy! New best EMA pseudo Dice: 0.6443
2024-12-06 21:55:32.280511: 
2024-12-06 21:55:32.281804: Epoch 50
2024-12-06 21:55:32.282543: Current learning rate: 0.00955
2024-12-06 21:57:03.222270: Validation loss did not improve from -0.45405. Patience: 17/50
2024-12-06 21:57:03.223208: train_loss -0.6382
2024-12-06 21:57:03.224174: val_loss -0.3178
2024-12-06 21:57:03.224964: Pseudo dice [0.6254]
2024-12-06 21:57:03.225652: Epoch time: 90.94 s
2024-12-06 21:57:04.823615: 
2024-12-06 21:57:04.825355: Epoch 51
2024-12-06 21:57:04.826231: Current learning rate: 0.00954
2024-12-06 21:58:35.651466: Validation loss did not improve from -0.45405. Patience: 18/50
2024-12-06 21:58:35.652483: train_loss -0.639
2024-12-06 21:58:35.653416: val_loss -0.4017
2024-12-06 21:58:35.654066: Pseudo dice [0.6951]
2024-12-06 21:58:35.654844: Epoch time: 90.83 s
2024-12-06 21:58:35.655436: Yayy! New best EMA pseudo Dice: 0.6477
2024-12-06 21:58:37.258991: 
2024-12-06 21:58:37.260948: Epoch 52
2024-12-06 21:58:37.261716: Current learning rate: 0.00953
2024-12-06 22:00:08.202897: Validation loss did not improve from -0.45405. Patience: 19/50
2024-12-06 22:00:08.203791: train_loss -0.6331
2024-12-06 22:00:08.204840: val_loss -0.2609
2024-12-06 22:00:08.205741: Pseudo dice [0.5947]
2024-12-06 22:00:08.206709: Epoch time: 90.95 s
2024-12-06 22:00:09.433656: 
2024-12-06 22:00:09.435248: Epoch 53
2024-12-06 22:00:09.436259: Current learning rate: 0.00952
2024-12-06 22:01:40.183862: Validation loss did not improve from -0.45405. Patience: 20/50
2024-12-06 22:01:40.184937: train_loss -0.6445
2024-12-06 22:01:40.185919: val_loss -0.365
2024-12-06 22:01:40.186670: Pseudo dice [0.6658]
2024-12-06 22:01:40.187413: Epoch time: 90.75 s
2024-12-06 22:01:41.420934: 
2024-12-06 22:01:41.422420: Epoch 54
2024-12-06 22:01:41.423219: Current learning rate: 0.00951
2024-12-06 22:03:12.291356: Validation loss did not improve from -0.45405. Patience: 21/50
2024-12-06 22:03:12.292740: train_loss -0.6417
2024-12-06 22:03:12.293652: val_loss -0.3145
2024-12-06 22:03:12.294376: Pseudo dice [0.6283]
2024-12-06 22:03:12.295077: Epoch time: 90.87 s
2024-12-06 22:03:13.931413: 
2024-12-06 22:03:13.933359: Epoch 55
2024-12-06 22:03:13.934442: Current learning rate: 0.0095
2024-12-06 22:04:44.756612: Validation loss did not improve from -0.45405. Patience: 22/50
2024-12-06 22:04:44.757854: train_loss -0.6405
2024-12-06 22:04:44.758886: val_loss -0.4269
2024-12-06 22:04:44.759855: Pseudo dice [0.6847]
2024-12-06 22:04:44.760713: Epoch time: 90.83 s
2024-12-06 22:04:46.001299: 
2024-12-06 22:04:46.002771: Epoch 56
2024-12-06 22:04:46.004330: Current learning rate: 0.00949
2024-12-06 22:06:16.956653: Validation loss did not improve from -0.45405. Patience: 23/50
2024-12-06 22:06:16.957765: train_loss -0.637
2024-12-06 22:06:16.958764: val_loss -0.3569
2024-12-06 22:06:16.959412: Pseudo dice [0.657]
2024-12-06 22:06:16.960191: Epoch time: 90.96 s
2024-12-06 22:06:16.960946: Yayy! New best EMA pseudo Dice: 0.6482
2024-12-06 22:06:18.650442: 
2024-12-06 22:06:18.651999: Epoch 57
2024-12-06 22:06:18.653055: Current learning rate: 0.00949
2024-12-06 22:07:49.884066: Validation loss did not improve from -0.45405. Patience: 24/50
2024-12-06 22:07:49.885340: train_loss -0.6347
2024-12-06 22:07:49.886529: val_loss -0.2941
2024-12-06 22:07:49.887243: Pseudo dice [0.6264]
2024-12-06 22:07:49.887941: Epoch time: 91.24 s
2024-12-06 22:07:51.120310: 
2024-12-06 22:07:51.122179: Epoch 58
2024-12-06 22:07:51.123156: Current learning rate: 0.00948
2024-12-06 22:09:22.681593: Validation loss did not improve from -0.45405. Patience: 25/50
2024-12-06 22:09:22.682548: train_loss -0.6524
2024-12-06 22:09:22.683546: val_loss -0.3682
2024-12-06 22:09:22.684356: Pseudo dice [0.6661]
2024-12-06 22:09:22.685109: Epoch time: 91.56 s
2024-12-06 22:09:23.953168: 
2024-12-06 22:09:23.954588: Epoch 59
2024-12-06 22:09:23.955520: Current learning rate: 0.00947
2024-12-06 22:10:54.919201: Validation loss did not improve from -0.45405. Patience: 26/50
2024-12-06 22:10:54.920098: train_loss -0.6486
2024-12-06 22:10:54.921177: val_loss -0.321
2024-12-06 22:10:54.922187: Pseudo dice [0.624]
2024-12-06 22:10:54.923227: Epoch time: 90.97 s
2024-12-06 22:10:56.545290: 
2024-12-06 22:10:56.547124: Epoch 60
2024-12-06 22:10:56.548044: Current learning rate: 0.00946
2024-12-06 22:12:27.448703: Validation loss did not improve from -0.45405. Patience: 27/50
2024-12-06 22:12:27.449727: train_loss -0.6591
2024-12-06 22:12:27.450657: val_loss -0.4028
2024-12-06 22:12:27.451399: Pseudo dice [0.6786]
2024-12-06 22:12:27.452229: Epoch time: 90.91 s
2024-12-06 22:12:27.452978: Yayy! New best EMA pseudo Dice: 0.6489
2024-12-06 22:12:29.057592: 
2024-12-06 22:12:29.059158: Epoch 61
2024-12-06 22:12:29.059924: Current learning rate: 0.00945
2024-12-06 22:13:59.959435: Validation loss did not improve from -0.45405. Patience: 28/50
2024-12-06 22:13:59.960674: train_loss -0.657
2024-12-06 22:13:59.961739: val_loss -0.332
2024-12-06 22:13:59.962797: Pseudo dice [0.6433]
2024-12-06 22:13:59.963597: Epoch time: 90.9 s
2024-12-06 22:14:01.544071: 
2024-12-06 22:14:01.545837: Epoch 62
2024-12-06 22:14:01.546814: Current learning rate: 0.00944
2024-12-06 22:15:32.342859: Validation loss did not improve from -0.45405. Patience: 29/50
2024-12-06 22:15:32.344025: train_loss -0.6561
2024-12-06 22:15:32.345016: val_loss -0.3728
2024-12-06 22:15:32.345858: Pseudo dice [0.6634]
2024-12-06 22:15:32.346809: Epoch time: 90.8 s
2024-12-06 22:15:32.347690: Yayy! New best EMA pseudo Dice: 0.6499
2024-12-06 22:15:33.958247: 
2024-12-06 22:15:33.960513: Epoch 63
2024-12-06 22:15:33.961633: Current learning rate: 0.00943
2024-12-06 22:17:04.962754: Validation loss did not improve from -0.45405. Patience: 30/50
2024-12-06 22:17:04.963802: train_loss -0.6484
2024-12-06 22:17:04.965031: val_loss -0.162
2024-12-06 22:17:04.966074: Pseudo dice [0.5321]
2024-12-06 22:17:04.967134: Epoch time: 91.01 s
2024-12-06 22:17:06.197971: 
2024-12-06 22:17:06.199737: Epoch 64
2024-12-06 22:17:06.200921: Current learning rate: 0.00942
2024-12-06 22:18:37.028811: Validation loss did not improve from -0.45405. Patience: 31/50
2024-12-06 22:18:37.029997: train_loss -0.6606
2024-12-06 22:18:37.030989: val_loss -0.3855
2024-12-06 22:18:37.031719: Pseudo dice [0.666]
2024-12-06 22:18:37.032356: Epoch time: 90.83 s
2024-12-06 22:18:38.642579: 
2024-12-06 22:18:38.644379: Epoch 65
2024-12-06 22:18:38.645157: Current learning rate: 0.00941
2024-12-06 22:20:09.546366: Validation loss did not improve from -0.45405. Patience: 32/50
2024-12-06 22:20:09.547166: train_loss -0.6653
2024-12-06 22:20:09.547903: val_loss -0.3512
2024-12-06 22:20:09.548482: Pseudo dice [0.6418]
2024-12-06 22:20:09.549143: Epoch time: 90.91 s
2024-12-06 22:20:10.778233: 
2024-12-06 22:20:10.780206: Epoch 66
2024-12-06 22:20:10.781274: Current learning rate: 0.0094
2024-12-06 22:21:41.689976: Validation loss did not improve from -0.45405. Patience: 33/50
2024-12-06 22:21:41.690845: train_loss -0.6661
2024-12-06 22:21:41.691798: val_loss -0.2883
2024-12-06 22:21:41.692563: Pseudo dice [0.6308]
2024-12-06 22:21:41.693464: Epoch time: 90.91 s
2024-12-06 22:21:42.955776: 
2024-12-06 22:21:42.957524: Epoch 67
2024-12-06 22:21:42.958342: Current learning rate: 0.00939
2024-12-06 22:23:13.862793: Validation loss improved from -0.45405 to -0.45643! Patience: 33/50
2024-12-06 22:23:13.863881: train_loss -0.6668
2024-12-06 22:23:13.864973: val_loss -0.4564
2024-12-06 22:23:13.865916: Pseudo dice [0.7105]
2024-12-06 22:23:13.866710: Epoch time: 90.91 s
2024-12-06 22:23:15.170569: 
2024-12-06 22:23:15.171932: Epoch 68
2024-12-06 22:23:15.172922: Current learning rate: 0.00939
2024-12-06 22:24:46.099434: Validation loss improved from -0.45643 to -0.50750! Patience: 0/50
2024-12-06 22:24:46.100561: train_loss -0.6674
2024-12-06 22:24:46.101798: val_loss -0.5075
2024-12-06 22:24:46.102742: Pseudo dice [0.7356]
2024-12-06 22:24:46.103647: Epoch time: 90.93 s
2024-12-06 22:24:46.105022: Yayy! New best EMA pseudo Dice: 0.6559
2024-12-06 22:24:47.750321: 
2024-12-06 22:24:47.751962: Epoch 69
2024-12-06 22:24:47.753054: Current learning rate: 0.00938
2024-12-06 22:26:18.598722: Validation loss did not improve from -0.50750. Patience: 1/50
2024-12-06 22:26:18.599627: train_loss -0.6635
2024-12-06 22:26:18.600634: val_loss -0.3302
2024-12-06 22:26:18.601406: Pseudo dice [0.6385]
2024-12-06 22:26:18.602138: Epoch time: 90.85 s
2024-12-06 22:26:20.241233: 
2024-12-06 22:26:20.243148: Epoch 70
2024-12-06 22:26:20.244315: Current learning rate: 0.00937
2024-12-06 22:27:51.038049: Validation loss did not improve from -0.50750. Patience: 2/50
2024-12-06 22:27:51.039350: train_loss -0.6642
2024-12-06 22:27:51.040166: val_loss -0.3802
2024-12-06 22:27:51.040847: Pseudo dice [0.6746]
2024-12-06 22:27:51.041678: Epoch time: 90.8 s
2024-12-06 22:27:51.042392: Yayy! New best EMA pseudo Dice: 0.6562
2024-12-06 22:27:52.699878: 
2024-12-06 22:27:52.701642: Epoch 71
2024-12-06 22:27:52.702479: Current learning rate: 0.00936
2024-12-06 22:29:23.495315: Validation loss did not improve from -0.50750. Patience: 3/50
2024-12-06 22:29:23.496487: train_loss -0.6715
2024-12-06 22:29:23.497478: val_loss -0.4171
2024-12-06 22:29:23.498275: Pseudo dice [0.6899]
2024-12-06 22:29:23.499072: Epoch time: 90.8 s
2024-12-06 22:29:23.499970: Yayy! New best EMA pseudo Dice: 0.6596
2024-12-06 22:29:25.562164: 
2024-12-06 22:29:25.564344: Epoch 72
2024-12-06 22:29:25.565217: Current learning rate: 0.00935
2024-12-06 22:30:56.469436: Validation loss did not improve from -0.50750. Patience: 4/50
2024-12-06 22:30:56.470476: train_loss -0.6621
2024-12-06 22:30:56.471301: val_loss -0.4524
2024-12-06 22:30:56.472030: Pseudo dice [0.7063]
2024-12-06 22:30:56.472635: Epoch time: 90.91 s
2024-12-06 22:30:56.473214: Yayy! New best EMA pseudo Dice: 0.6642
2024-12-06 22:30:58.161633: 
2024-12-06 22:30:58.163553: Epoch 73
2024-12-06 22:30:58.164454: Current learning rate: 0.00934
2024-12-06 22:32:29.168907: Validation loss did not improve from -0.50750. Patience: 5/50
2024-12-06 22:32:29.170010: train_loss -0.6759
2024-12-06 22:32:29.170785: val_loss -0.377
2024-12-06 22:32:29.171669: Pseudo dice [0.65]
2024-12-06 22:32:29.172355: Epoch time: 91.01 s
2024-12-06 22:32:30.472377: 
2024-12-06 22:32:30.474595: Epoch 74
2024-12-06 22:32:30.475476: Current learning rate: 0.00933
2024-12-06 22:34:01.142633: Validation loss did not improve from -0.50750. Patience: 6/50
2024-12-06 22:34:01.143742: train_loss -0.6718
2024-12-06 22:34:01.144709: val_loss -0.3289
2024-12-06 22:34:01.145562: Pseudo dice [0.6399]
2024-12-06 22:34:01.146255: Epoch time: 90.67 s
2024-12-06 22:34:02.815701: 
2024-12-06 22:34:02.818326: Epoch 75
2024-12-06 22:34:02.819441: Current learning rate: 0.00932
2024-12-06 22:35:33.711692: Validation loss did not improve from -0.50750. Patience: 7/50
2024-12-06 22:35:33.712723: train_loss -0.6686
2024-12-06 22:35:33.713770: val_loss -0.351
2024-12-06 22:35:33.714671: Pseudo dice [0.6393]
2024-12-06 22:35:33.715383: Epoch time: 90.9 s
2024-12-06 22:35:35.034542: 
2024-12-06 22:35:35.036350: Epoch 76
2024-12-06 22:35:35.037206: Current learning rate: 0.00931
2024-12-06 22:37:05.796631: Validation loss did not improve from -0.50750. Patience: 8/50
2024-12-06 22:37:05.797530: train_loss -0.6614
2024-12-06 22:37:05.798498: val_loss -0.3328
2024-12-06 22:37:05.799312: Pseudo dice [0.6454]
2024-12-06 22:37:05.800049: Epoch time: 90.76 s
2024-12-06 22:37:07.138672: 
2024-12-06 22:37:07.140454: Epoch 77
2024-12-06 22:37:07.141701: Current learning rate: 0.0093
2024-12-06 22:38:38.169451: Validation loss did not improve from -0.50750. Patience: 9/50
2024-12-06 22:38:38.170549: train_loss -0.6492
2024-12-06 22:38:38.171588: val_loss -0.2632
2024-12-06 22:38:38.172698: Pseudo dice [0.5871]
2024-12-06 22:38:38.173760: Epoch time: 91.03 s
2024-12-06 22:38:39.461090: 
2024-12-06 22:38:39.463148: Epoch 78
2024-12-06 22:38:39.464073: Current learning rate: 0.0093
2024-12-06 22:40:10.547387: Validation loss did not improve from -0.50750. Patience: 10/50
2024-12-06 22:40:10.548552: train_loss -0.6586
2024-12-06 22:40:10.549949: val_loss -0.3621
2024-12-06 22:40:10.550872: Pseudo dice [0.6627]
2024-12-06 22:40:10.551637: Epoch time: 91.09 s
2024-12-06 22:40:11.889223: 
2024-12-06 22:40:11.891511: Epoch 79
2024-12-06 22:40:11.892611: Current learning rate: 0.00929
2024-12-06 22:41:43.116510: Validation loss did not improve from -0.50750. Patience: 11/50
2024-12-06 22:41:43.117526: train_loss -0.665
2024-12-06 22:41:43.118262: val_loss -0.4186
2024-12-06 22:41:43.118940: Pseudo dice [0.6934]
2024-12-06 22:41:43.119597: Epoch time: 91.23 s
2024-12-06 22:41:44.794356: 
2024-12-06 22:41:44.796205: Epoch 80
2024-12-06 22:41:44.797009: Current learning rate: 0.00928
2024-12-06 22:43:16.167440: Validation loss did not improve from -0.50750. Patience: 12/50
2024-12-06 22:43:16.168866: train_loss -0.677
2024-12-06 22:43:16.169798: val_loss -0.3696
2024-12-06 22:43:16.170659: Pseudo dice [0.67]
2024-12-06 22:43:16.171379: Epoch time: 91.38 s
2024-12-06 22:43:17.444102: 
2024-12-06 22:43:17.445958: Epoch 81
2024-12-06 22:43:17.446855: Current learning rate: 0.00927
2024-12-06 22:44:48.088769: Validation loss did not improve from -0.50750. Patience: 13/50
2024-12-06 22:44:48.089750: train_loss -0.6822
2024-12-06 22:44:48.090547: val_loss -0.3722
2024-12-06 22:44:48.091288: Pseudo dice [0.6722]
2024-12-06 22:44:48.092116: Epoch time: 90.65 s
2024-12-06 22:44:49.411415: 
2024-12-06 22:44:49.413088: Epoch 82
2024-12-06 22:44:49.413912: Current learning rate: 0.00926
2024-12-06 22:46:19.874662: Validation loss did not improve from -0.50750. Patience: 14/50
2024-12-06 22:46:19.875594: train_loss -0.6715
2024-12-06 22:46:19.876585: val_loss -0.3721
2024-12-06 22:46:19.877342: Pseudo dice [0.6673]
2024-12-06 22:46:19.878052: Epoch time: 90.47 s
2024-12-06 22:46:21.444903: 
2024-12-06 22:46:21.446524: Epoch 83
2024-12-06 22:46:21.447331: Current learning rate: 0.00925
2024-12-06 22:47:52.053496: Validation loss did not improve from -0.50750. Patience: 15/50
2024-12-06 22:47:52.054579: train_loss -0.6731
2024-12-06 22:47:52.055452: val_loss -0.3107
2024-12-06 22:47:52.056222: Pseudo dice [0.6312]
2024-12-06 22:47:52.056967: Epoch time: 90.61 s
2024-12-06 22:47:53.271497: 
2024-12-06 22:47:53.273214: Epoch 84
2024-12-06 22:47:53.273974: Current learning rate: 0.00924
2024-12-06 22:49:23.501009: Validation loss did not improve from -0.50750. Patience: 16/50
2024-12-06 22:49:23.502140: train_loss -0.6857
2024-12-06 22:49:23.503210: val_loss -0.3534
2024-12-06 22:49:23.504159: Pseudo dice [0.6456]
2024-12-06 22:49:23.505146: Epoch time: 90.23 s
2024-12-06 22:49:25.107786: 
2024-12-06 22:49:25.109275: Epoch 85
2024-12-06 22:49:25.110161: Current learning rate: 0.00923
2024-12-06 22:50:55.666077: Validation loss did not improve from -0.50750. Patience: 17/50
2024-12-06 22:50:55.667114: train_loss -0.6849
2024-12-06 22:50:55.667938: val_loss -0.3584
2024-12-06 22:50:55.668690: Pseudo dice [0.6566]
2024-12-06 22:50:55.669541: Epoch time: 90.56 s
2024-12-06 22:50:56.902675: 
2024-12-06 22:50:56.904635: Epoch 86
2024-12-06 22:50:56.905348: Current learning rate: 0.00922
2024-12-06 22:52:27.500310: Validation loss did not improve from -0.50750. Patience: 18/50
2024-12-06 22:52:27.501138: train_loss -0.6964
2024-12-06 22:52:27.501961: val_loss -0.3649
2024-12-06 22:52:27.502670: Pseudo dice [0.6433]
2024-12-06 22:52:27.503329: Epoch time: 90.6 s
2024-12-06 22:52:28.704362: 
2024-12-06 22:52:28.705784: Epoch 87
2024-12-06 22:52:28.706535: Current learning rate: 0.00921
2024-12-06 22:53:59.434055: Validation loss did not improve from -0.50750. Patience: 19/50
2024-12-06 22:53:59.435209: train_loss -0.6914
2024-12-06 22:53:59.436158: val_loss -0.4082
2024-12-06 22:53:59.437076: Pseudo dice [0.6755]
2024-12-06 22:53:59.437719: Epoch time: 90.73 s
2024-12-06 22:54:00.671291: 
2024-12-06 22:54:00.673004: Epoch 88
2024-12-06 22:54:00.674270: Current learning rate: 0.0092
2024-12-06 22:55:30.733864: Validation loss did not improve from -0.50750. Patience: 20/50
2024-12-06 22:55:30.735096: train_loss -0.6917
2024-12-06 22:55:30.735964: val_loss -0.3148
2024-12-06 22:55:30.736615: Pseudo dice [0.6203]
2024-12-06 22:55:30.737243: Epoch time: 90.06 s
2024-12-06 22:55:31.992865: 
2024-12-06 22:55:31.994703: Epoch 89
2024-12-06 22:55:31.995534: Current learning rate: 0.0092
2024-12-06 22:57:02.771289: Validation loss did not improve from -0.50750. Patience: 21/50
2024-12-06 22:57:02.772221: train_loss -0.6767
2024-12-06 22:57:02.773201: val_loss -0.3865
2024-12-06 22:57:02.774213: Pseudo dice [0.6719]
2024-12-06 22:57:02.775199: Epoch time: 90.78 s
2024-12-06 22:57:04.369727: 
2024-12-06 22:57:04.371603: Epoch 90
2024-12-06 22:57:04.372521: Current learning rate: 0.00919
2024-12-06 22:58:35.297523: Validation loss did not improve from -0.50750. Patience: 22/50
2024-12-06 22:58:35.298673: train_loss -0.6836
2024-12-06 22:58:35.299531: val_loss -0.2621
2024-12-06 22:58:35.300270: Pseudo dice [0.6184]
2024-12-06 22:58:35.300904: Epoch time: 90.93 s
2024-12-06 22:58:36.555382: 
2024-12-06 22:58:36.556358: Epoch 91
2024-12-06 22:58:36.557104: Current learning rate: 0.00918
2024-12-06 23:00:07.368888: Validation loss did not improve from -0.50750. Patience: 23/50
2024-12-06 23:00:07.369669: train_loss -0.6887
2024-12-06 23:00:07.370552: val_loss -0.3903
2024-12-06 23:00:07.371349: Pseudo dice [0.6907]
2024-12-06 23:00:07.372048: Epoch time: 90.82 s
2024-12-06 23:00:08.586957: 
2024-12-06 23:00:08.588863: Epoch 92
2024-12-06 23:00:08.589646: Current learning rate: 0.00917
2024-12-06 23:01:39.534178: Validation loss did not improve from -0.50750. Patience: 24/50
2024-12-06 23:01:39.535185: train_loss -0.6963
2024-12-06 23:01:39.535962: val_loss -0.2693
2024-12-06 23:01:39.536766: Pseudo dice [0.6163]
2024-12-06 23:01:39.537563: Epoch time: 90.95 s
2024-12-06 23:01:41.066048: 
2024-12-06 23:01:41.067798: Epoch 93
2024-12-06 23:01:41.068707: Current learning rate: 0.00916
2024-12-06 23:03:11.847225: Validation loss did not improve from -0.50750. Patience: 25/50
2024-12-06 23:03:11.848401: train_loss -0.697
2024-12-06 23:03:11.849219: val_loss -0.4252
2024-12-06 23:03:11.849851: Pseudo dice [0.6823]
2024-12-06 23:03:11.850529: Epoch time: 90.78 s
2024-12-06 23:03:13.098093: 
2024-12-06 23:03:13.099202: Epoch 94
2024-12-06 23:03:13.100178: Current learning rate: 0.00915
2024-12-06 23:04:44.181232: Validation loss did not improve from -0.50750. Patience: 26/50
2024-12-06 23:04:44.182160: train_loss -0.6889
2024-12-06 23:04:44.183112: val_loss -0.4008
2024-12-06 23:04:44.183820: Pseudo dice [0.6738]
2024-12-06 23:04:44.184532: Epoch time: 91.09 s
2024-12-06 23:04:45.830217: 
2024-12-06 23:04:45.832021: Epoch 95
2024-12-06 23:04:45.832740: Current learning rate: 0.00914
2024-12-06 23:06:16.714792: Validation loss did not improve from -0.50750. Patience: 27/50
2024-12-06 23:06:16.715741: train_loss -0.697
2024-12-06 23:06:16.716488: val_loss -0.2906
2024-12-06 23:06:16.717109: Pseudo dice [0.6293]
2024-12-06 23:06:16.717761: Epoch time: 90.89 s
2024-12-06 23:06:17.933588: 
2024-12-06 23:06:17.935071: Epoch 96
2024-12-06 23:06:17.935849: Current learning rate: 0.00913
2024-12-06 23:07:48.787145: Validation loss did not improve from -0.50750. Patience: 28/50
2024-12-06 23:07:48.788418: train_loss -0.693
2024-12-06 23:07:48.789373: val_loss -0.4146
2024-12-06 23:07:48.790090: Pseudo dice [0.6926]
2024-12-06 23:07:48.790752: Epoch time: 90.86 s
2024-12-06 23:07:50.019376: 
2024-12-06 23:07:50.021322: Epoch 97
2024-12-06 23:07:50.022074: Current learning rate: 0.00912
2024-12-06 23:09:20.962074: Validation loss did not improve from -0.50750. Patience: 29/50
2024-12-06 23:09:20.963249: train_loss -0.7015
2024-12-06 23:09:20.964278: val_loss -0.4015
2024-12-06 23:09:20.965277: Pseudo dice [0.6895]
2024-12-06 23:09:20.966099: Epoch time: 90.95 s
2024-12-06 23:09:22.212731: 
2024-12-06 23:09:22.214265: Epoch 98
2024-12-06 23:09:22.215254: Current learning rate: 0.00911
2024-12-06 23:10:53.253824: Validation loss did not improve from -0.50750. Patience: 30/50
2024-12-06 23:10:53.254889: train_loss -0.7028
2024-12-06 23:10:53.255584: val_loss -0.3917
2024-12-06 23:10:53.256200: Pseudo dice [0.6712]
2024-12-06 23:10:53.256889: Epoch time: 91.04 s
2024-12-06 23:10:54.537354: 
2024-12-06 23:10:54.538841: Epoch 99
2024-12-06 23:10:54.539764: Current learning rate: 0.0091
2024-12-06 23:12:25.373436: Validation loss did not improve from -0.50750. Patience: 31/50
2024-12-06 23:12:25.374396: train_loss -0.6994
2024-12-06 23:12:25.375132: val_loss -0.2707
2024-12-06 23:12:25.375724: Pseudo dice [0.6103]
2024-12-06 23:12:25.376358: Epoch time: 90.84 s
2024-12-06 23:12:27.030648: 
2024-12-06 23:12:27.032036: Epoch 100
2024-12-06 23:12:27.033481: Current learning rate: 0.0091
2024-12-06 23:13:58.102849: Validation loss did not improve from -0.50750. Patience: 32/50
2024-12-06 23:13:58.108087: train_loss -0.7009
2024-12-06 23:13:58.110996: val_loss -0.3624
2024-12-06 23:13:58.112098: Pseudo dice [0.6505]
2024-12-06 23:13:58.113681: Epoch time: 91.08 s
2024-12-06 23:13:59.751856: 
2024-12-06 23:13:59.753629: Epoch 101
2024-12-06 23:13:59.754601: Current learning rate: 0.00909
2024-12-06 23:15:30.507631: Validation loss did not improve from -0.50750. Patience: 33/50
2024-12-06 23:15:30.508881: train_loss -0.7092
2024-12-06 23:15:30.509826: val_loss -0.3591
2024-12-06 23:15:30.510557: Pseudo dice [0.665]
2024-12-06 23:15:30.511382: Epoch time: 90.76 s
2024-12-06 23:15:31.739670: 
2024-12-06 23:15:31.741258: Epoch 102
2024-12-06 23:15:31.742245: Current learning rate: 0.00908
2024-12-06 23:17:02.619803: Validation loss did not improve from -0.50750. Patience: 34/50
2024-12-06 23:17:02.620944: train_loss -0.6994
2024-12-06 23:17:02.621971: val_loss -0.3388
2024-12-06 23:17:02.622744: Pseudo dice [0.6475]
2024-12-06 23:17:02.623544: Epoch time: 90.88 s
2024-12-06 23:17:03.858789: 
2024-12-06 23:17:03.860492: Epoch 103
2024-12-06 23:17:03.861380: Current learning rate: 0.00907
2024-12-06 23:18:34.814623: Validation loss did not improve from -0.50750. Patience: 35/50
2024-12-06 23:18:34.815705: train_loss -0.7038
2024-12-06 23:18:34.816903: val_loss -0.3313
2024-12-06 23:18:34.817688: Pseudo dice [0.6402]
2024-12-06 23:18:34.818487: Epoch time: 90.96 s
2024-12-06 23:18:36.071525: 
2024-12-06 23:18:36.073397: Epoch 104
2024-12-06 23:18:36.074350: Current learning rate: 0.00906
2024-12-06 23:20:06.857765: Validation loss did not improve from -0.50750. Patience: 36/50
2024-12-06 23:20:06.858739: train_loss -0.707
2024-12-06 23:20:06.859692: val_loss -0.2986
2024-12-06 23:20:06.860402: Pseudo dice [0.6229]
2024-12-06 23:20:06.861168: Epoch time: 90.79 s
2024-12-06 23:20:09.083434: 
2024-12-06 23:20:09.084880: Epoch 105
2024-12-06 23:20:09.085616: Current learning rate: 0.00905
2024-12-06 23:21:40.047532: Validation loss did not improve from -0.50750. Patience: 37/50
2024-12-06 23:21:40.048305: train_loss -0.6945
2024-12-06 23:21:40.049108: val_loss -0.4645
2024-12-06 23:21:40.050030: Pseudo dice [0.7171]
2024-12-06 23:21:40.050921: Epoch time: 90.97 s
2024-12-06 23:21:41.292356: 
2024-12-06 23:21:41.293756: Epoch 106
2024-12-06 23:21:41.294677: Current learning rate: 0.00904
2024-12-06 23:23:12.114669: Validation loss did not improve from -0.50750. Patience: 38/50
2024-12-06 23:23:12.115825: train_loss -0.6976
2024-12-06 23:23:12.116675: val_loss -0.4054
2024-12-06 23:23:12.117571: Pseudo dice [0.6897]
2024-12-06 23:23:12.118214: Epoch time: 90.82 s
2024-12-06 23:23:13.374666: 
2024-12-06 23:23:13.375758: Epoch 107
2024-12-06 23:23:13.376469: Current learning rate: 0.00903
2024-12-06 23:24:44.482191: Validation loss did not improve from -0.50750. Patience: 39/50
2024-12-06 23:24:44.483428: train_loss -0.7058
2024-12-06 23:24:44.484483: val_loss -0.3076
2024-12-06 23:24:44.485280: Pseudo dice [0.6173]
2024-12-06 23:24:44.485969: Epoch time: 91.11 s
2024-12-06 23:24:45.764804: 
2024-12-06 23:24:45.766378: Epoch 108
2024-12-06 23:24:45.767333: Current learning rate: 0.00902
2024-12-06 23:26:16.874273: Validation loss did not improve from -0.50750. Patience: 40/50
2024-12-06 23:26:16.875546: train_loss -0.6963
2024-12-06 23:26:16.876279: val_loss -0.4393
2024-12-06 23:26:16.876954: Pseudo dice [0.6988]
2024-12-06 23:26:16.877750: Epoch time: 91.11 s
2024-12-06 23:26:18.118713: 
2024-12-06 23:26:18.120427: Epoch 109
2024-12-06 23:26:18.121550: Current learning rate: 0.00901
2024-12-06 23:27:49.119904: Validation loss did not improve from -0.50750. Patience: 41/50
2024-12-06 23:27:49.121257: train_loss -0.7057
2024-12-06 23:27:49.122157: val_loss -0.4147
2024-12-06 23:27:49.122915: Pseudo dice [0.703]
2024-12-06 23:27:49.123731: Epoch time: 91.0 s
2024-12-06 23:27:49.518212: Yayy! New best EMA pseudo Dice: 0.6651
2024-12-06 23:27:51.089746: 
2024-12-06 23:27:51.091362: Epoch 110
2024-12-06 23:27:51.092237: Current learning rate: 0.009
2024-12-06 23:29:21.999607: Validation loss did not improve from -0.50750. Patience: 42/50
2024-12-06 23:29:22.000866: train_loss -0.706
2024-12-06 23:29:22.001587: val_loss -0.4086
2024-12-06 23:29:22.002249: Pseudo dice [0.6945]
2024-12-06 23:29:22.002914: Epoch time: 90.91 s
2024-12-06 23:29:22.003646: Yayy! New best EMA pseudo Dice: 0.668
2024-12-06 23:29:23.650317: 
2024-12-06 23:29:23.651736: Epoch 111
2024-12-06 23:29:23.652584: Current learning rate: 0.009
2024-12-06 23:30:54.657170: Validation loss did not improve from -0.50750. Patience: 43/50
2024-12-06 23:30:54.658336: train_loss -0.7091
2024-12-06 23:30:54.659191: val_loss -0.415
2024-12-06 23:30:54.659817: Pseudo dice [0.6809]
2024-12-06 23:30:54.660422: Epoch time: 91.01 s
2024-12-06 23:30:54.661070: Yayy! New best EMA pseudo Dice: 0.6693
2024-12-06 23:30:56.271977: 
2024-12-06 23:30:56.273676: Epoch 112
2024-12-06 23:30:56.274815: Current learning rate: 0.00899
2024-12-06 23:32:27.285534: Validation loss did not improve from -0.50750. Patience: 44/50
2024-12-06 23:32:27.286756: train_loss -0.7053
2024-12-06 23:32:27.287646: val_loss -0.312
2024-12-06 23:32:27.288553: Pseudo dice [0.6373]
2024-12-06 23:32:27.289377: Epoch time: 91.02 s
2024-12-06 23:32:28.506842: 
2024-12-06 23:32:28.508463: Epoch 113
2024-12-06 23:32:28.509344: Current learning rate: 0.00898
2024-12-06 23:33:59.365575: Validation loss did not improve from -0.50750. Patience: 45/50
2024-12-06 23:33:59.366714: train_loss -0.7111
2024-12-06 23:33:59.367681: val_loss -0.1598
2024-12-06 23:33:59.368675: Pseudo dice [0.5586]
2024-12-06 23:33:59.369360: Epoch time: 90.86 s
2024-12-06 23:34:00.601914: 
2024-12-06 23:34:00.603756: Epoch 114
2024-12-06 23:34:00.604682: Current learning rate: 0.00897
2024-12-06 23:35:31.469062: Validation loss did not improve from -0.50750. Patience: 46/50
2024-12-06 23:35:31.470061: train_loss -0.7112
2024-12-06 23:35:31.471346: val_loss -0.3536
2024-12-06 23:35:31.472203: Pseudo dice [0.6542]
2024-12-06 23:35:31.473106: Epoch time: 90.87 s
2024-12-06 23:35:33.117083: 
2024-12-06 23:35:33.118475: Epoch 115
2024-12-06 23:35:33.119318: Current learning rate: 0.00896
2024-12-06 23:37:03.824407: Validation loss did not improve from -0.50750. Patience: 47/50
2024-12-06 23:37:03.825449: train_loss -0.7155
2024-12-06 23:37:03.826325: val_loss -0.3518
2024-12-06 23:37:03.827010: Pseudo dice [0.6657]
2024-12-06 23:37:03.827741: Epoch time: 90.71 s
2024-12-06 23:37:05.422195: 
2024-12-06 23:37:05.423676: Epoch 116
2024-12-06 23:37:05.424427: Current learning rate: 0.00895
2024-12-06 23:38:36.396918: Validation loss did not improve from -0.50750. Patience: 48/50
2024-12-06 23:38:36.397919: train_loss -0.7184
2024-12-06 23:38:36.398746: val_loss -0.3853
2024-12-06 23:38:36.399481: Pseudo dice [0.6741]
2024-12-06 23:38:36.400303: Epoch time: 90.98 s
2024-12-06 23:38:37.670092: 
2024-12-06 23:38:37.671849: Epoch 117
2024-12-06 23:38:37.672736: Current learning rate: 0.00894
2024-12-06 23:40:08.675167: Validation loss did not improve from -0.50750. Patience: 49/50
2024-12-06 23:40:08.675992: train_loss -0.717
2024-12-06 23:40:08.677092: val_loss -0.3579
2024-12-06 23:40:08.678071: Pseudo dice [0.6702]
2024-12-06 23:40:08.678823: Epoch time: 91.01 s
2024-12-06 23:40:09.969445: 
2024-12-06 23:40:09.971035: Epoch 118
2024-12-06 23:40:09.972019: Current learning rate: 0.00893
2024-12-06 23:41:40.794322: Validation loss did not improve from -0.50750. Patience: 50/50
2024-12-06 23:41:40.795369: train_loss -0.7225
2024-12-06 23:41:40.796288: val_loss -0.4148
2024-12-06 23:41:40.797007: Pseudo dice [0.6951]
2024-12-06 23:41:40.797679: Epoch time: 90.83 s
2024-12-06 23:41:42.075353: Patience reached. Stopping training.
2024-12-06 23:41:42.526808: Training done.
2024-12-06 23:41:43.099318: Using splits from existing split file: /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset307_Sohee_Calcium_OCT_CrossValidation/splits_final.json
2024-12-06 23:41:43.154323: The split file contains 5 splits.
2024-12-06 23:41:43.155627: Desired fold for training: 3
2024-12-06 23:41:43.156490: This split has 7 training and 1 validation cases.
2024-12-06 23:41:43.157596: predicting 701-013
2024-12-06 23:41:43.219717: 701-013, shape torch.Size([1, 375, 498, 498]), rank 0
2024-12-06 23:45:46.947674: Validation complete
2024-12-06 23:45:46.948716: Mean Validation Dice:  0.6854307399944675
2024-12-06 20:32:32.845128: unpacking done...
2024-12-06 20:32:32.865119: Unable to plot network architecture: nnUNet_compile is enabled!
2024-12-06 20:32:32.930463: 
2024-12-06 20:32:32.931461: Epoch 0
2024-12-06 20:32:32.932710: Current learning rate: 0.01
2024-12-06 20:34:51.188463: Validation loss improved from 1000.00000 to -0.24482! Patience: 0/50
2024-12-06 20:34:51.189789: train_loss -0.0961
2024-12-06 20:34:51.191035: val_loss -0.2448
2024-12-06 20:34:51.192230: Pseudo dice [0.5753]
2024-12-06 20:34:51.193388: Epoch time: 138.26 s
2024-12-06 20:34:51.194519: Yayy! New best EMA pseudo Dice: 0.5753
2024-12-06 20:34:52.779966: 
2024-12-06 20:34:52.781532: Epoch 1
2024-12-06 20:34:52.782694: Current learning rate: 0.00999
2024-12-06 20:36:30.138283: Validation loss improved from -0.24482 to -0.28763! Patience: 0/50
2024-12-06 20:36:30.139430: train_loss -0.2409
2024-12-06 20:36:30.140649: val_loss -0.2876
2024-12-06 20:36:30.141598: Pseudo dice [0.5959]
2024-12-06 20:36:30.142612: Epoch time: 97.36 s
2024-12-06 20:36:30.143597: Yayy! New best EMA pseudo Dice: 0.5773
2024-12-06 20:36:31.830822: 
2024-12-06 20:36:31.832442: Epoch 2
2024-12-06 20:36:31.833492: Current learning rate: 0.00998
2024-12-06 20:38:10.006531: Validation loss did not improve from -0.28763. Patience: 1/50
2024-12-06 20:38:10.007582: train_loss -0.283
2024-12-06 20:38:10.008474: val_loss -0.2741
2024-12-06 20:38:10.009289: Pseudo dice [0.5928]
2024-12-06 20:38:10.010109: Epoch time: 98.18 s
2024-12-06 20:38:10.010929: Yayy! New best EMA pseudo Dice: 0.5789
2024-12-06 20:38:11.789107: 
2024-12-06 20:38:11.790599: Epoch 3
2024-12-06 20:38:11.791401: Current learning rate: 0.00997
2024-12-06 20:39:50.254595: Validation loss improved from -0.28763 to -0.29755! Patience: 1/50
2024-12-06 20:39:50.255880: train_loss -0.3227
2024-12-06 20:39:50.257095: val_loss -0.2975
2024-12-06 20:39:50.257861: Pseudo dice [0.6205]
2024-12-06 20:39:50.258577: Epoch time: 98.47 s
2024-12-06 20:39:50.259243: Yayy! New best EMA pseudo Dice: 0.583
2024-12-06 20:39:51.999371: 
2024-12-06 20:39:52.001207: Epoch 4
2024-12-06 20:39:52.002238: Current learning rate: 0.00996
2024-12-06 20:41:29.889231: Validation loss did not improve from -0.29755. Patience: 1/50
2024-12-06 20:41:29.890441: train_loss -0.3404
2024-12-06 20:41:29.891346: val_loss -0.2702
2024-12-06 20:41:29.892160: Pseudo dice [0.5906]
2024-12-06 20:41:29.892950: Epoch time: 97.89 s
2024-12-06 20:41:30.266275: Yayy! New best EMA pseudo Dice: 0.5838
2024-12-06 20:41:32.019407: 
2024-12-06 20:41:32.020986: Epoch 5
2024-12-06 20:41:32.021966: Current learning rate: 0.00995
2024-12-06 20:43:09.935665: Validation loss improved from -0.29755 to -0.37345! Patience: 1/50
2024-12-06 20:43:09.937103: train_loss -0.3745
2024-12-06 20:43:09.938257: val_loss -0.3735
2024-12-06 20:43:09.939154: Pseudo dice [0.649]
2024-12-06 20:43:09.940105: Epoch time: 97.92 s
2024-12-06 20:43:09.940900: Yayy! New best EMA pseudo Dice: 0.5903
2024-12-06 20:43:11.636521: 
2024-12-06 20:43:11.638268: Epoch 6
2024-12-06 20:43:11.639055: Current learning rate: 0.00995
2024-12-06 20:44:50.060769: Validation loss improved from -0.37345 to -0.39763! Patience: 0/50
2024-12-06 20:44:50.062054: train_loss -0.4284
2024-12-06 20:44:50.062946: val_loss -0.3976
2024-12-06 20:44:50.063762: Pseudo dice [0.6624]
2024-12-06 20:44:50.064454: Epoch time: 98.43 s
2024-12-06 20:44:50.065392: Yayy! New best EMA pseudo Dice: 0.5975
2024-12-06 20:44:51.717038: 
2024-12-06 20:44:51.718778: Epoch 7
2024-12-06 20:44:51.719669: Current learning rate: 0.00994
2024-12-06 20:46:29.575052: Validation loss did not improve from -0.39763. Patience: 1/50
2024-12-06 20:46:29.576020: train_loss -0.4159
2024-12-06 20:46:29.576908: val_loss -0.39
2024-12-06 20:46:29.577690: Pseudo dice [0.6535]
2024-12-06 20:46:29.578411: Epoch time: 97.86 s
2024-12-06 20:46:29.579257: Yayy! New best EMA pseudo Dice: 0.6031
2024-12-06 20:46:31.715776: 
2024-12-06 20:46:31.717465: Epoch 8
2024-12-06 20:46:31.718399: Current learning rate: 0.00993
2024-12-06 20:48:10.597613: Validation loss did not improve from -0.39763. Patience: 2/50
2024-12-06 20:48:10.598700: train_loss -0.4382
2024-12-06 20:48:10.599706: val_loss -0.3734
2024-12-06 20:48:10.600778: Pseudo dice [0.6339]
2024-12-06 20:48:10.601683: Epoch time: 98.88 s
2024-12-06 20:48:10.602574: Yayy! New best EMA pseudo Dice: 0.6062
2024-12-06 20:48:12.348439: 
2024-12-06 20:48:12.350202: Epoch 9
2024-12-06 20:48:12.351164: Current learning rate: 0.00992
2024-12-06 20:49:50.481758: Validation loss did not improve from -0.39763. Patience: 3/50
2024-12-06 20:49:50.482559: train_loss -0.4428
2024-12-06 20:49:50.483681: val_loss -0.392
2024-12-06 20:49:50.484332: Pseudo dice [0.668]
2024-12-06 20:49:50.485107: Epoch time: 98.14 s
2024-12-06 20:49:50.870246: Yayy! New best EMA pseudo Dice: 0.6124
2024-12-06 20:49:52.505202: 
2024-12-06 20:49:52.506553: Epoch 10
2024-12-06 20:49:52.507606: Current learning rate: 0.00991
2024-12-06 20:51:30.134946: Validation loss improved from -0.39763 to -0.42538! Patience: 3/50
2024-12-06 20:51:30.135720: train_loss -0.4592
2024-12-06 20:51:30.136705: val_loss -0.4254
2024-12-06 20:51:30.137795: Pseudo dice [0.6654]
2024-12-06 20:51:30.138984: Epoch time: 97.63 s
2024-12-06 20:51:30.140044: Yayy! New best EMA pseudo Dice: 0.6177
2024-12-06 20:51:31.796740: 
2024-12-06 20:51:31.798553: Epoch 11
2024-12-06 20:51:31.799670: Current learning rate: 0.0099
2024-12-06 20:53:10.875752: Validation loss did not improve from -0.42538. Patience: 1/50
2024-12-06 20:53:10.876897: train_loss -0.4685
2024-12-06 20:53:10.877982: val_loss -0.4142
2024-12-06 20:53:10.878809: Pseudo dice [0.6707]
2024-12-06 20:53:10.879561: Epoch time: 99.08 s
2024-12-06 20:53:10.880237: Yayy! New best EMA pseudo Dice: 0.623
2024-12-06 20:53:12.548683: 
2024-12-06 20:53:12.550371: Epoch 12
2024-12-06 20:53:12.551210: Current learning rate: 0.00989
2024-12-06 20:54:51.039618: Validation loss did not improve from -0.42538. Patience: 2/50
2024-12-06 20:54:51.040872: train_loss -0.4863
2024-12-06 20:54:51.041999: val_loss -0.425
2024-12-06 20:54:51.042827: Pseudo dice [0.6725]
2024-12-06 20:54:51.043535: Epoch time: 98.49 s
2024-12-06 20:54:51.044290: Yayy! New best EMA pseudo Dice: 0.6279
2024-12-06 20:54:52.710378: 
2024-12-06 20:54:52.711889: Epoch 13
2024-12-06 20:54:52.712837: Current learning rate: 0.00988
2024-12-06 20:56:30.945393: Validation loss improved from -0.42538 to -0.43367! Patience: 2/50
2024-12-06 20:56:30.946610: train_loss -0.4891
2024-12-06 20:56:30.947677: val_loss -0.4337
2024-12-06 20:56:30.948531: Pseudo dice [0.6894]
2024-12-06 20:56:30.949315: Epoch time: 98.24 s
2024-12-06 20:56:30.950017: Yayy! New best EMA pseudo Dice: 0.6341
2024-12-06 20:56:32.646709: 
2024-12-06 20:56:32.648407: Epoch 14
2024-12-06 20:56:32.649477: Current learning rate: 0.00987
2024-12-06 20:58:11.454379: Validation loss improved from -0.43367 to -0.44232! Patience: 0/50
2024-12-06 20:58:11.455613: train_loss -0.4826
2024-12-06 20:58:11.456641: val_loss -0.4423
2024-12-06 20:58:11.457554: Pseudo dice [0.6875]
2024-12-06 20:58:11.458602: Epoch time: 98.81 s
2024-12-06 20:58:11.852498: Yayy! New best EMA pseudo Dice: 0.6394
2024-12-06 20:58:13.587332: 
2024-12-06 20:58:13.589174: Epoch 15
2024-12-06 20:58:13.590155: Current learning rate: 0.00986
2024-12-06 20:59:50.663064: Validation loss improved from -0.44232 to -0.44909! Patience: 0/50
2024-12-06 20:59:50.664299: train_loss -0.501
2024-12-06 20:59:50.665322: val_loss -0.4491
2024-12-06 20:59:50.666080: Pseudo dice [0.694]
2024-12-06 20:59:50.666805: Epoch time: 97.08 s
2024-12-06 20:59:50.667590: Yayy! New best EMA pseudo Dice: 0.6449
2024-12-06 20:59:52.402792: 
2024-12-06 20:59:52.404497: Epoch 16
2024-12-06 20:59:52.405239: Current learning rate: 0.00986
2024-12-06 21:01:31.403109: Validation loss improved from -0.44909 to -0.47026! Patience: 0/50
2024-12-06 21:01:31.404485: train_loss -0.5066
2024-12-06 21:01:31.405609: val_loss -0.4703
2024-12-06 21:01:31.406704: Pseudo dice [0.7055]
2024-12-06 21:01:31.407461: Epoch time: 99.0 s
2024-12-06 21:01:31.408173: Yayy! New best EMA pseudo Dice: 0.6509
2024-12-06 21:01:33.191395: 
2024-12-06 21:01:33.193298: Epoch 17
2024-12-06 21:01:33.194257: Current learning rate: 0.00985
2024-12-06 21:03:10.757531: Validation loss did not improve from -0.47026. Patience: 1/50
2024-12-06 21:03:10.758500: train_loss -0.5233
2024-12-06 21:03:10.759617: val_loss -0.4371
2024-12-06 21:03:10.760719: Pseudo dice [0.6913]
2024-12-06 21:03:10.761706: Epoch time: 97.57 s
2024-12-06 21:03:10.762604: Yayy! New best EMA pseudo Dice: 0.655
2024-12-06 21:03:12.491561: 
2024-12-06 21:03:12.493399: Epoch 18
2024-12-06 21:03:12.494222: Current learning rate: 0.00984
2024-12-06 21:04:50.155739: Validation loss did not improve from -0.47026. Patience: 2/50
2024-12-06 21:04:50.156731: train_loss -0.5313
2024-12-06 21:04:50.157632: val_loss -0.447
2024-12-06 21:04:50.158519: Pseudo dice [0.6983]
2024-12-06 21:04:50.159314: Epoch time: 97.67 s
2024-12-06 21:04:50.160150: Yayy! New best EMA pseudo Dice: 0.6593
2024-12-06 21:04:52.229615: 
2024-12-06 21:04:52.230806: Epoch 19
2024-12-06 21:04:52.231540: Current learning rate: 0.00983
2024-12-06 21:06:29.378255: Validation loss did not improve from -0.47026. Patience: 3/50
2024-12-06 21:06:29.379200: train_loss -0.5426
2024-12-06 21:06:29.380056: val_loss -0.4465
2024-12-06 21:06:29.380832: Pseudo dice [0.6932]
2024-12-06 21:06:29.381588: Epoch time: 97.15 s
2024-12-06 21:06:29.764544: Yayy! New best EMA pseudo Dice: 0.6627
2024-12-06 21:06:31.464897: 
2024-12-06 21:06:31.466475: Epoch 20
2024-12-06 21:06:31.467552: Current learning rate: 0.00982
2024-12-06 21:08:08.425215: Validation loss did not improve from -0.47026. Patience: 4/50
2024-12-06 21:08:08.426474: train_loss -0.5413
2024-12-06 21:08:08.427753: val_loss -0.4555
2024-12-06 21:08:08.428735: Pseudo dice [0.7018]
2024-12-06 21:08:08.429725: Epoch time: 96.96 s
2024-12-06 21:08:08.430476: Yayy! New best EMA pseudo Dice: 0.6666
2024-12-06 21:08:10.184767: 
2024-12-06 21:08:10.186216: Epoch 21
2024-12-06 21:08:10.187201: Current learning rate: 0.00981
2024-12-06 21:09:48.460925: Validation loss did not improve from -0.47026. Patience: 5/50
2024-12-06 21:09:48.462006: train_loss -0.5425
2024-12-06 21:09:48.463073: val_loss -0.4327
2024-12-06 21:09:48.464154: Pseudo dice [0.6838]
2024-12-06 21:09:48.465157: Epoch time: 98.28 s
2024-12-06 21:09:48.466058: Yayy! New best EMA pseudo Dice: 0.6683
2024-12-06 21:09:50.111441: 
2024-12-06 21:09:50.112663: Epoch 22
2024-12-06 21:09:50.113595: Current learning rate: 0.0098
2024-12-06 21:11:27.598239: Validation loss improved from -0.47026 to -0.48122! Patience: 5/50
2024-12-06 21:11:27.599500: train_loss -0.5423
2024-12-06 21:11:27.600482: val_loss -0.4812
2024-12-06 21:11:27.601274: Pseudo dice [0.7115]
2024-12-06 21:11:27.602015: Epoch time: 97.49 s
2024-12-06 21:11:27.602740: Yayy! New best EMA pseudo Dice: 0.6726
2024-12-06 21:11:29.404493: 
2024-12-06 21:11:29.407027: Epoch 23
2024-12-06 21:11:29.408317: Current learning rate: 0.00979
2024-12-06 21:13:06.859761: Validation loss did not improve from -0.48122. Patience: 1/50
2024-12-06 21:13:06.861013: train_loss -0.5542
2024-12-06 21:13:06.862194: val_loss -0.4611
2024-12-06 21:13:06.863136: Pseudo dice [0.6988]
2024-12-06 21:13:06.863877: Epoch time: 97.46 s
2024-12-06 21:13:06.864668: Yayy! New best EMA pseudo Dice: 0.6753
2024-12-06 21:13:08.597070: 
2024-12-06 21:13:08.598927: Epoch 24
2024-12-06 21:13:08.599815: Current learning rate: 0.00978
2024-12-06 21:14:46.633735: Validation loss did not improve from -0.48122. Patience: 2/50
2024-12-06 21:14:46.634796: train_loss -0.5571
2024-12-06 21:14:46.635731: val_loss -0.4138
2024-12-06 21:14:46.636482: Pseudo dice [0.679]
2024-12-06 21:14:46.637192: Epoch time: 98.04 s
2024-12-06 21:14:47.041617: Yayy! New best EMA pseudo Dice: 0.6756
2024-12-06 21:14:48.745352: 
2024-12-06 21:14:48.747202: Epoch 25
2024-12-06 21:14:48.747994: Current learning rate: 0.00977
2024-12-06 21:16:26.840474: Validation loss did not improve from -0.48122. Patience: 3/50
2024-12-06 21:16:26.842448: train_loss -0.5564
2024-12-06 21:16:26.843662: val_loss -0.4789
2024-12-06 21:16:26.844368: Pseudo dice [0.7126]
2024-12-06 21:16:26.845304: Epoch time: 98.1 s
2024-12-06 21:16:26.846108: Yayy! New best EMA pseudo Dice: 0.6793
2024-12-06 21:16:28.555032: 
2024-12-06 21:16:28.556294: Epoch 26
2024-12-06 21:16:28.557185: Current learning rate: 0.00977
2024-12-06 21:18:06.038659: Validation loss did not improve from -0.48122. Patience: 4/50
2024-12-06 21:18:06.039735: train_loss -0.5628
2024-12-06 21:18:06.040581: val_loss -0.473
2024-12-06 21:18:06.041312: Pseudo dice [0.7098]
2024-12-06 21:18:06.042143: Epoch time: 97.49 s
2024-12-06 21:18:06.042874: Yayy! New best EMA pseudo Dice: 0.6824
2024-12-06 21:18:07.758594: 
2024-12-06 21:18:07.760164: Epoch 27
2024-12-06 21:18:07.760983: Current learning rate: 0.00976
2024-12-06 21:19:45.939842: Validation loss did not improve from -0.48122. Patience: 5/50
2024-12-06 21:19:45.940809: train_loss -0.573
2024-12-06 21:19:45.941635: val_loss -0.4792
2024-12-06 21:19:45.942307: Pseudo dice [0.7076]
2024-12-06 21:19:45.943132: Epoch time: 98.18 s
2024-12-06 21:19:45.943857: Yayy! New best EMA pseudo Dice: 0.6849
2024-12-06 21:19:47.614294: 
2024-12-06 21:19:47.615583: Epoch 28
2024-12-06 21:19:47.616380: Current learning rate: 0.00975
2024-12-06 21:21:26.282535: Validation loss did not improve from -0.48122. Patience: 6/50
2024-12-06 21:21:26.283903: train_loss -0.5662
2024-12-06 21:21:26.284790: val_loss -0.4772
2024-12-06 21:21:26.285530: Pseudo dice [0.7139]
2024-12-06 21:21:26.286407: Epoch time: 98.67 s
2024-12-06 21:21:26.287117: Yayy! New best EMA pseudo Dice: 0.6878
2024-12-06 21:21:28.335437: 
2024-12-06 21:21:28.337046: Epoch 29
2024-12-06 21:21:28.337854: Current learning rate: 0.00974
2024-12-06 21:23:06.046107: Validation loss did not improve from -0.48122. Patience: 7/50
2024-12-06 21:23:06.047422: train_loss -0.5756
2024-12-06 21:23:06.048663: val_loss -0.4593
2024-12-06 21:23:06.049375: Pseudo dice [0.7033]
2024-12-06 21:23:06.050080: Epoch time: 97.71 s
2024-12-06 21:23:06.445098: Yayy! New best EMA pseudo Dice: 0.6894
2024-12-06 21:23:08.171588: 
2024-12-06 21:23:08.173401: Epoch 30
2024-12-06 21:23:08.174210: Current learning rate: 0.00973
2024-12-06 21:24:45.516466: Validation loss did not improve from -0.48122. Patience: 8/50
2024-12-06 21:24:45.517828: train_loss -0.5718
2024-12-06 21:24:45.518713: val_loss -0.4691
2024-12-06 21:24:45.519385: Pseudo dice [0.7066]
2024-12-06 21:24:45.520032: Epoch time: 97.35 s
2024-12-06 21:24:45.520754: Yayy! New best EMA pseudo Dice: 0.6911
2024-12-06 21:24:47.219504: 
2024-12-06 21:24:47.221346: Epoch 31
2024-12-06 21:24:47.222272: Current learning rate: 0.00972
2024-12-06 21:26:24.992346: Validation loss did not improve from -0.48122. Patience: 9/50
2024-12-06 21:26:24.993571: train_loss -0.5816
2024-12-06 21:26:24.994898: val_loss -0.4685
2024-12-06 21:26:24.995931: Pseudo dice [0.6901]
2024-12-06 21:26:24.996892: Epoch time: 97.78 s
2024-12-06 21:26:26.309055: 
2024-12-06 21:26:26.310851: Epoch 32
2024-12-06 21:26:26.311851: Current learning rate: 0.00971
2024-12-06 21:28:03.712442: Validation loss did not improve from -0.48122. Patience: 10/50
2024-12-06 21:28:03.713578: train_loss -0.5768
2024-12-06 21:28:03.714424: val_loss -0.4541
2024-12-06 21:28:03.715176: Pseudo dice [0.6925]
2024-12-06 21:28:03.715955: Epoch time: 97.41 s
2024-12-06 21:28:03.716663: Yayy! New best EMA pseudo Dice: 0.6911
2024-12-06 21:28:05.421710: 
2024-12-06 21:28:05.423262: Epoch 33
2024-12-06 21:28:05.424135: Current learning rate: 0.0097
2024-12-06 21:29:42.480866: Validation loss improved from -0.48122 to -0.49811! Patience: 10/50
2024-12-06 21:29:42.481910: train_loss -0.5992
2024-12-06 21:29:42.482859: val_loss -0.4981
2024-12-06 21:29:42.483644: Pseudo dice [0.7191]
2024-12-06 21:29:42.484371: Epoch time: 97.06 s
2024-12-06 21:29:42.485063: Yayy! New best EMA pseudo Dice: 0.6939
2024-12-06 21:29:44.236833: 
2024-12-06 21:29:44.238568: Epoch 34
2024-12-06 21:29:44.239473: Current learning rate: 0.00969
2024-12-06 21:31:13.099422: Validation loss improved from -0.49811 to -0.51375! Patience: 0/50
2024-12-06 21:31:13.100806: train_loss -0.5921
2024-12-06 21:31:13.101943: val_loss -0.5138
2024-12-06 21:31:13.102763: Pseudo dice [0.7255]
2024-12-06 21:31:13.103440: Epoch time: 88.86 s
2024-12-06 21:31:13.496201: Yayy! New best EMA pseudo Dice: 0.6971
2024-12-06 21:31:15.287855: 
2024-12-06 21:31:15.289468: Epoch 35
2024-12-06 21:31:15.290361: Current learning rate: 0.00968
2024-12-06 21:32:43.929878: Validation loss did not improve from -0.51375. Patience: 1/50
2024-12-06 21:32:43.930964: train_loss -0.6014
2024-12-06 21:32:43.932113: val_loss -0.4753
2024-12-06 21:32:43.932760: Pseudo dice [0.6932]
2024-12-06 21:32:43.933416: Epoch time: 88.64 s
2024-12-06 21:32:45.206904: 
2024-12-06 21:32:45.208654: Epoch 36
2024-12-06 21:32:45.209836: Current learning rate: 0.00968
2024-12-06 21:34:13.373978: Validation loss did not improve from -0.51375. Patience: 2/50
2024-12-06 21:34:13.374916: train_loss -0.5911
2024-12-06 21:34:13.375688: val_loss -0.4804
2024-12-06 21:34:13.376501: Pseudo dice [0.7]
2024-12-06 21:34:13.377183: Epoch time: 88.17 s
2024-12-06 21:34:14.671163: 
2024-12-06 21:34:14.672634: Epoch 37
2024-12-06 21:34:14.673637: Current learning rate: 0.00967
2024-12-06 21:35:42.907799: Validation loss did not improve from -0.51375. Patience: 3/50
2024-12-06 21:35:42.909126: train_loss -0.6059
2024-12-06 21:35:42.910053: val_loss -0.4829
2024-12-06 21:35:42.910781: Pseudo dice [0.7057]
2024-12-06 21:35:42.911548: Epoch time: 88.24 s
2024-12-06 21:35:42.912196: Yayy! New best EMA pseudo Dice: 0.6979
2024-12-06 21:35:44.552210: 
2024-12-06 21:35:44.553175: Epoch 38
2024-12-06 21:35:44.553974: Current learning rate: 0.00966
2024-12-06 21:37:13.948229: Validation loss did not improve from -0.51375. Patience: 4/50
2024-12-06 21:37:13.949114: train_loss -0.6071
2024-12-06 21:37:13.949837: val_loss -0.424
2024-12-06 21:37:13.950472: Pseudo dice [0.6874]
2024-12-06 21:37:13.951212: Epoch time: 89.4 s
2024-12-06 21:37:15.503916: 
2024-12-06 21:37:15.505790: Epoch 39
2024-12-06 21:37:15.506900: Current learning rate: 0.00965
2024-12-06 21:38:44.227726: Validation loss did not improve from -0.51375. Patience: 5/50
2024-12-06 21:38:44.229088: train_loss -0.5917
2024-12-06 21:38:44.230000: val_loss -0.4552
2024-12-06 21:38:44.230766: Pseudo dice [0.6958]
2024-12-06 21:38:44.231576: Epoch time: 88.73 s
2024-12-06 21:38:46.026747: 
2024-12-06 21:38:46.028446: Epoch 40
2024-12-06 21:38:46.029719: Current learning rate: 0.00964
2024-12-06 21:40:14.431343: Validation loss did not improve from -0.51375. Patience: 6/50
2024-12-06 21:40:14.432522: train_loss -0.6074
2024-12-06 21:40:14.433565: val_loss -0.5005
2024-12-06 21:40:14.434584: Pseudo dice [0.7244]
2024-12-06 21:40:14.435473: Epoch time: 88.41 s
2024-12-06 21:40:14.436374: Yayy! New best EMA pseudo Dice: 0.6995
2024-12-06 21:40:16.110655: 
2024-12-06 21:40:16.112523: Epoch 41
2024-12-06 21:40:16.113301: Current learning rate: 0.00963
2024-12-06 21:41:44.538497: Validation loss did not improve from -0.51375. Patience: 7/50
2024-12-06 21:41:44.539333: train_loss -0.6132
2024-12-06 21:41:44.540195: val_loss -0.4671
2024-12-06 21:41:44.540996: Pseudo dice [0.6988]
2024-12-06 21:41:44.541733: Epoch time: 88.43 s
2024-12-06 21:41:45.733931: 
2024-12-06 21:41:45.735458: Epoch 42
2024-12-06 21:41:45.736505: Current learning rate: 0.00962
2024-12-06 21:43:14.677578: Validation loss did not improve from -0.51375. Patience: 8/50
2024-12-06 21:43:14.678597: train_loss -0.6147
2024-12-06 21:43:14.679726: val_loss -0.5077
2024-12-06 21:43:14.680806: Pseudo dice [0.7262]
2024-12-06 21:43:14.681947: Epoch time: 88.95 s
2024-12-06 21:43:14.682765: Yayy! New best EMA pseudo Dice: 0.7021
2024-12-06 21:43:16.259201: 
2024-12-06 21:43:16.260813: Epoch 43
2024-12-06 21:43:16.261834: Current learning rate: 0.00961
2024-12-06 21:44:44.644179: Validation loss did not improve from -0.51375. Patience: 9/50
2024-12-06 21:44:44.645094: train_loss -0.6254
2024-12-06 21:44:44.646118: val_loss -0.5107
2024-12-06 21:44:44.646825: Pseudo dice [0.7241]
2024-12-06 21:44:44.647528: Epoch time: 88.39 s
2024-12-06 21:44:44.648500: Yayy! New best EMA pseudo Dice: 0.7043
2024-12-06 21:44:46.227615: 
2024-12-06 21:44:46.229048: Epoch 44
2024-12-06 21:44:46.230172: Current learning rate: 0.0096
2024-12-06 21:46:14.538566: Validation loss improved from -0.51375 to -0.53156! Patience: 9/50
2024-12-06 21:46:14.539685: train_loss -0.6266
2024-12-06 21:46:14.540752: val_loss -0.5316
2024-12-06 21:46:14.541610: Pseudo dice [0.732]
2024-12-06 21:46:14.542342: Epoch time: 88.31 s
2024-12-06 21:46:14.922708: Yayy! New best EMA pseudo Dice: 0.7071
2024-12-06 21:46:16.533695: 
2024-12-06 21:46:16.535154: Epoch 45
2024-12-06 21:46:16.536520: Current learning rate: 0.00959
2024-12-06 21:47:44.613289: Validation loss did not improve from -0.53156. Patience: 1/50
2024-12-06 21:47:44.614505: train_loss -0.6236
2024-12-06 21:47:44.615594: val_loss -0.4603
2024-12-06 21:47:44.616420: Pseudo dice [0.7113]
2024-12-06 21:47:44.617218: Epoch time: 88.08 s
2024-12-06 21:47:44.618040: Yayy! New best EMA pseudo Dice: 0.7075
2024-12-06 21:47:46.277697: 
2024-12-06 21:47:46.279268: Epoch 46
2024-12-06 21:47:46.280149: Current learning rate: 0.00959
2024-12-06 21:49:14.627944: Validation loss did not improve from -0.53156. Patience: 2/50
2024-12-06 21:49:14.629220: train_loss -0.6215
2024-12-06 21:49:14.630374: val_loss -0.4995
2024-12-06 21:49:14.631382: Pseudo dice [0.7191]
2024-12-06 21:49:14.632350: Epoch time: 88.35 s
2024-12-06 21:49:14.633316: Yayy! New best EMA pseudo Dice: 0.7087
2024-12-06 21:49:16.206390: 
2024-12-06 21:49:16.207850: Epoch 47
2024-12-06 21:49:16.209305: Current learning rate: 0.00958
2024-12-06 21:50:44.683783: Validation loss did not improve from -0.53156. Patience: 3/50
2024-12-06 21:50:44.684935: train_loss -0.6328
2024-12-06 21:50:44.685905: val_loss -0.5177
2024-12-06 21:50:44.686697: Pseudo dice [0.7301]
2024-12-06 21:50:44.687663: Epoch time: 88.48 s
2024-12-06 21:50:44.688621: Yayy! New best EMA pseudo Dice: 0.7108
2024-12-06 21:50:46.262064: 
2024-12-06 21:50:46.263743: Epoch 48
2024-12-06 21:50:46.264840: Current learning rate: 0.00957
2024-12-06 21:52:14.775938: Validation loss did not improve from -0.53156. Patience: 4/50
2024-12-06 21:52:14.777041: train_loss -0.6303
2024-12-06 21:52:14.777998: val_loss -0.4896
2024-12-06 21:52:14.778851: Pseudo dice [0.7172]
2024-12-06 21:52:14.779604: Epoch time: 88.52 s
2024-12-06 21:52:14.780386: Yayy! New best EMA pseudo Dice: 0.7114
2024-12-06 21:52:16.356000: 
2024-12-06 21:52:16.357728: Epoch 49
2024-12-06 21:52:16.358695: Current learning rate: 0.00956
2024-12-06 21:53:44.670631: Validation loss did not improve from -0.53156. Patience: 5/50
2024-12-06 21:53:44.671809: train_loss -0.636
2024-12-06 21:53:44.672667: val_loss -0.4986
2024-12-06 21:53:44.673675: Pseudo dice [0.7165]
2024-12-06 21:53:44.674585: Epoch time: 88.32 s
2024-12-06 21:53:45.054563: Yayy! New best EMA pseudo Dice: 0.7119
2024-12-06 21:53:46.983321: 
2024-12-06 21:53:46.984796: Epoch 50
2024-12-06 21:53:46.985802: Current learning rate: 0.00955
2024-12-06 21:55:15.044209: Validation loss did not improve from -0.53156. Patience: 6/50
2024-12-06 21:55:15.045211: train_loss -0.6355
2024-12-06 21:55:15.046008: val_loss -0.5099
2024-12-06 21:55:15.046769: Pseudo dice [0.7338]
2024-12-06 21:55:15.047437: Epoch time: 88.06 s
2024-12-06 21:55:15.048189: Yayy! New best EMA pseudo Dice: 0.7141
2024-12-06 21:55:16.621585: 
2024-12-06 21:55:16.623279: Epoch 51
2024-12-06 21:55:16.624244: Current learning rate: 0.00954
2024-12-06 21:56:44.782997: Validation loss did not improve from -0.53156. Patience: 7/50
2024-12-06 21:56:44.784209: train_loss -0.6317
2024-12-06 21:56:44.785273: val_loss -0.4891
2024-12-06 21:56:44.785984: Pseudo dice [0.7212]
2024-12-06 21:56:44.786777: Epoch time: 88.16 s
2024-12-06 21:56:44.787581: Yayy! New best EMA pseudo Dice: 0.7148
2024-12-06 21:56:46.329199: 
2024-12-06 21:56:46.330726: Epoch 52
2024-12-06 21:56:46.331683: Current learning rate: 0.00953
2024-12-06 21:58:14.397088: Validation loss did not improve from -0.53156. Patience: 8/50
2024-12-06 21:58:14.398099: train_loss -0.6415
2024-12-06 21:58:14.399003: val_loss -0.5009
2024-12-06 21:58:14.399836: Pseudo dice [0.7211]
2024-12-06 21:58:14.400704: Epoch time: 88.07 s
2024-12-06 21:58:14.401453: Yayy! New best EMA pseudo Dice: 0.7155
2024-12-06 21:58:15.951435: 
2024-12-06 21:58:15.953096: Epoch 53
2024-12-06 21:58:15.953981: Current learning rate: 0.00952
2024-12-06 21:59:44.180229: Validation loss did not improve from -0.53156. Patience: 9/50
2024-12-06 21:59:44.181361: train_loss -0.6341
2024-12-06 21:59:44.182122: val_loss -0.5221
2024-12-06 21:59:44.182778: Pseudo dice [0.7302]
2024-12-06 21:59:44.183485: Epoch time: 88.23 s
2024-12-06 21:59:44.184100: Yayy! New best EMA pseudo Dice: 0.7169
2024-12-06 21:59:45.758806: 
2024-12-06 21:59:45.760682: Epoch 54
2024-12-06 21:59:45.761500: Current learning rate: 0.00951
2024-12-06 22:01:13.502853: Validation loss did not improve from -0.53156. Patience: 10/50
2024-12-06 22:01:13.503940: train_loss -0.6425
2024-12-06 22:01:13.504864: val_loss -0.4843
2024-12-06 22:01:13.505644: Pseudo dice [0.7041]
2024-12-06 22:01:13.506350: Epoch time: 87.75 s
2024-12-06 22:01:15.125991: 
2024-12-06 22:01:15.127717: Epoch 55
2024-12-06 22:01:15.128794: Current learning rate: 0.0095
2024-12-06 22:02:43.208474: Validation loss did not improve from -0.53156. Patience: 11/50
2024-12-06 22:02:43.209646: train_loss -0.645
2024-12-06 22:02:43.210696: val_loss -0.5023
2024-12-06 22:02:43.211485: Pseudo dice [0.7224]
2024-12-06 22:02:43.212200: Epoch time: 88.08 s
2024-12-06 22:02:44.430854: 
2024-12-06 22:02:44.432347: Epoch 56
2024-12-06 22:02:44.433413: Current learning rate: 0.00949
2024-12-06 22:04:12.290867: Validation loss did not improve from -0.53156. Patience: 12/50
2024-12-06 22:04:12.292089: train_loss -0.6456
2024-12-06 22:04:12.292918: val_loss -0.5049
2024-12-06 22:04:12.293823: Pseudo dice [0.7219]
2024-12-06 22:04:12.294595: Epoch time: 87.86 s
2024-12-06 22:04:13.530230: 
2024-12-06 22:04:13.531907: Epoch 57
2024-12-06 22:04:13.532759: Current learning rate: 0.00949
2024-12-06 22:05:41.640225: Validation loss did not improve from -0.53156. Patience: 13/50
2024-12-06 22:05:41.641139: train_loss -0.6553
2024-12-06 22:05:41.642184: val_loss -0.4693
2024-12-06 22:05:41.643217: Pseudo dice [0.7054]
2024-12-06 22:05:41.644201: Epoch time: 88.11 s
2024-12-06 22:05:42.857676: 
2024-12-06 22:05:42.859096: Epoch 58
2024-12-06 22:05:42.860052: Current learning rate: 0.00948
2024-12-06 22:07:11.406714: Validation loss did not improve from -0.53156. Patience: 14/50
2024-12-06 22:07:11.407888: train_loss -0.6554
2024-12-06 22:07:11.408694: val_loss -0.4094
2024-12-06 22:07:11.409482: Pseudo dice [0.6804]
2024-12-06 22:07:11.410345: Epoch time: 88.55 s
2024-12-06 22:07:12.641092: 
2024-12-06 22:07:12.643115: Epoch 59
2024-12-06 22:07:12.643902: Current learning rate: 0.00947
2024-12-06 22:08:42.012581: Validation loss did not improve from -0.53156. Patience: 15/50
2024-12-06 22:08:42.107535: train_loss -0.6389
2024-12-06 22:08:42.110723: val_loss -0.4843
2024-12-06 22:08:42.111743: Pseudo dice [0.7085]
2024-12-06 22:08:42.120383: Epoch time: 89.45 s
2024-12-06 22:08:44.194906: 
2024-12-06 22:08:44.196629: Epoch 60
2024-12-06 22:08:44.197579: Current learning rate: 0.00946
2024-12-06 22:10:12.069511: Validation loss improved from -0.53156 to -0.53285! Patience: 15/50
2024-12-06 22:10:12.070821: train_loss -0.6498
2024-12-06 22:10:12.071856: val_loss -0.5329
2024-12-06 22:10:12.072666: Pseudo dice [0.7265]
2024-12-06 22:10:12.073660: Epoch time: 87.88 s
2024-12-06 22:10:14.717628: 
2024-12-06 22:10:14.719450: Epoch 61
2024-12-06 22:10:14.720209: Current learning rate: 0.00945
2024-12-06 22:11:42.744374: Validation loss did not improve from -0.53285. Patience: 1/50
2024-12-06 22:11:42.745628: train_loss -0.6466
2024-12-06 22:11:42.746758: val_loss -0.4862
2024-12-06 22:11:42.747627: Pseudo dice [0.7137]
2024-12-06 22:11:42.748316: Epoch time: 88.03 s
2024-12-06 22:11:43.996774: 
2024-12-06 22:11:43.998555: Epoch 62
2024-12-06 22:11:43.999449: Current learning rate: 0.00944
2024-12-06 22:13:12.183329: Validation loss did not improve from -0.53285. Patience: 2/50
2024-12-06 22:13:12.184353: train_loss -0.6502
2024-12-06 22:13:12.185192: val_loss -0.4875
2024-12-06 22:13:12.185952: Pseudo dice [0.7122]
2024-12-06 22:13:12.186778: Epoch time: 88.19 s
2024-12-06 22:13:13.418426: 
2024-12-06 22:13:13.419984: Epoch 63
2024-12-06 22:13:13.420799: Current learning rate: 0.00943
2024-12-06 22:14:41.381007: Validation loss did not improve from -0.53285. Patience: 3/50
2024-12-06 22:14:41.382061: train_loss -0.6581
2024-12-06 22:14:41.383025: val_loss -0.5171
2024-12-06 22:14:41.383787: Pseudo dice [0.7277]
2024-12-06 22:14:41.384552: Epoch time: 87.96 s
2024-12-06 22:14:42.627368: 
2024-12-06 22:14:42.628451: Epoch 64
2024-12-06 22:14:42.629191: Current learning rate: 0.00942
2024-12-06 22:16:10.891820: Validation loss did not improve from -0.53285. Patience: 4/50
2024-12-06 22:16:10.892918: train_loss -0.6565
2024-12-06 22:16:10.893847: val_loss -0.4921
2024-12-06 22:16:10.894720: Pseudo dice [0.7128]
2024-12-06 22:16:10.895473: Epoch time: 88.27 s
2024-12-06 22:16:12.524966: 
2024-12-06 22:16:12.526001: Epoch 65
2024-12-06 22:16:12.526746: Current learning rate: 0.00941
2024-12-06 22:17:40.484454: Validation loss did not improve from -0.53285. Patience: 5/50
2024-12-06 22:17:40.485337: train_loss -0.6612
2024-12-06 22:17:40.486080: val_loss -0.4963
2024-12-06 22:17:40.486956: Pseudo dice [0.7165]
2024-12-06 22:17:40.487767: Epoch time: 87.96 s
2024-12-06 22:17:41.723050: 
2024-12-06 22:17:41.724872: Epoch 66
2024-12-06 22:17:41.726190: Current learning rate: 0.0094
2024-12-06 22:19:09.744527: Validation loss did not improve from -0.53285. Patience: 6/50
2024-12-06 22:19:09.745346: train_loss -0.6527
2024-12-06 22:19:09.746327: val_loss -0.5081
2024-12-06 22:19:09.747181: Pseudo dice [0.7286]
2024-12-06 22:19:09.748105: Epoch time: 88.02 s
2024-12-06 22:19:11.025392: 
2024-12-06 22:19:11.027246: Epoch 67
2024-12-06 22:19:11.028374: Current learning rate: 0.00939
2024-12-06 22:20:39.276306: Validation loss did not improve from -0.53285. Patience: 7/50
2024-12-06 22:20:39.277214: train_loss -0.6615
2024-12-06 22:20:39.278156: val_loss -0.4915
2024-12-06 22:20:39.278908: Pseudo dice [0.7121]
2024-12-06 22:20:39.279873: Epoch time: 88.25 s
2024-12-06 22:20:40.539458: 
2024-12-06 22:20:40.540710: Epoch 68
2024-12-06 22:20:40.541672: Current learning rate: 0.00939
2024-12-06 22:22:08.835644: Validation loss did not improve from -0.53285. Patience: 8/50
2024-12-06 22:22:08.836625: train_loss -0.669
2024-12-06 22:22:08.837546: val_loss -0.505
2024-12-06 22:22:08.838415: Pseudo dice [0.7224]
2024-12-06 22:22:08.839236: Epoch time: 88.3 s
2024-12-06 22:22:10.117800: 
2024-12-06 22:22:10.119324: Epoch 69
2024-12-06 22:22:10.120546: Current learning rate: 0.00938
2024-12-06 22:23:38.365227: Validation loss did not improve from -0.53285. Patience: 9/50
2024-12-06 22:23:38.366401: train_loss -0.6687
2024-12-06 22:23:38.367473: val_loss -0.4546
2024-12-06 22:23:38.368178: Pseudo dice [0.7099]
2024-12-06 22:23:38.368949: Epoch time: 88.25 s
2024-12-06 22:23:40.008577: 
2024-12-06 22:23:40.010206: Epoch 70
2024-12-06 22:23:40.011018: Current learning rate: 0.00937
2024-12-06 22:25:08.330687: Validation loss did not improve from -0.53285. Patience: 10/50
2024-12-06 22:25:08.331897: train_loss -0.6661
2024-12-06 22:25:08.333134: val_loss -0.459
2024-12-06 22:25:08.334094: Pseudo dice [0.6967]
2024-12-06 22:25:08.335068: Epoch time: 88.32 s
2024-12-06 22:25:09.916726: 
2024-12-06 22:25:09.918772: Epoch 71
2024-12-06 22:25:09.919799: Current learning rate: 0.00936
2024-12-06 22:26:38.113946: Validation loss did not improve from -0.53285. Patience: 11/50
2024-12-06 22:26:38.115061: train_loss -0.6665
2024-12-06 22:26:38.116052: val_loss -0.4812
2024-12-06 22:26:38.116997: Pseudo dice [0.7124]
2024-12-06 22:26:38.117879: Epoch time: 88.2 s
2024-12-06 22:26:39.355007: 
2024-12-06 22:26:39.356730: Epoch 72
2024-12-06 22:26:39.357666: Current learning rate: 0.00935
2024-12-06 22:28:07.607562: Validation loss did not improve from -0.53285. Patience: 12/50
2024-12-06 22:28:07.608637: train_loss -0.671
2024-12-06 22:28:07.609924: val_loss -0.4921
2024-12-06 22:28:07.610837: Pseudo dice [0.7205]
2024-12-06 22:28:07.611864: Epoch time: 88.25 s
2024-12-06 22:28:08.854394: 
2024-12-06 22:28:08.855997: Epoch 73
2024-12-06 22:28:08.856816: Current learning rate: 0.00934
2024-12-06 22:29:37.205260: Validation loss did not improve from -0.53285. Patience: 13/50
2024-12-06 22:29:37.206259: train_loss -0.673
2024-12-06 22:29:37.207217: val_loss -0.499
2024-12-06 22:29:37.207957: Pseudo dice [0.725]
2024-12-06 22:29:37.208849: Epoch time: 88.35 s
2024-12-06 22:29:38.500582: 
2024-12-06 22:29:38.502491: Epoch 74
2024-12-06 22:29:38.503491: Current learning rate: 0.00933
2024-12-06 22:31:06.880780: Validation loss did not improve from -0.53285. Patience: 14/50
2024-12-06 22:31:06.881811: train_loss -0.6839
2024-12-06 22:31:06.882778: val_loss -0.4839
2024-12-06 22:31:06.883559: Pseudo dice [0.7146]
2024-12-06 22:31:06.884244: Epoch time: 88.38 s
2024-12-06 22:31:08.547905: 
2024-12-06 22:31:08.549753: Epoch 75
2024-12-06 22:31:08.550508: Current learning rate: 0.00932
2024-12-06 22:32:37.027782: Validation loss did not improve from -0.53285. Patience: 15/50
2024-12-06 22:32:37.028929: train_loss -0.6814
2024-12-06 22:32:37.029764: val_loss -0.5173
2024-12-06 22:32:37.030781: Pseudo dice [0.7353]
2024-12-06 22:32:37.031632: Epoch time: 88.48 s
2024-12-06 22:32:37.032512: Yayy! New best EMA pseudo Dice: 0.7173
2024-12-06 22:32:38.645487: 
2024-12-06 22:32:38.647186: Epoch 76
2024-12-06 22:32:38.648240: Current learning rate: 0.00931
2024-12-06 22:34:07.053255: Validation loss did not improve from -0.53285. Patience: 16/50
2024-12-06 22:34:07.054447: train_loss -0.6787
2024-12-06 22:34:07.055341: val_loss -0.5031
2024-12-06 22:34:07.056164: Pseudo dice [0.718]
2024-12-06 22:34:07.056904: Epoch time: 88.41 s
2024-12-06 22:34:07.057659: Yayy! New best EMA pseudo Dice: 0.7174
2024-12-06 22:34:08.707474: 
2024-12-06 22:34:08.708908: Epoch 77
2024-12-06 22:34:08.709822: Current learning rate: 0.0093
2024-12-06 22:35:37.059749: Validation loss did not improve from -0.53285. Patience: 17/50
2024-12-06 22:35:37.060863: train_loss -0.6808
2024-12-06 22:35:37.062038: val_loss -0.5108
2024-12-06 22:35:37.062872: Pseudo dice [0.7366]
2024-12-06 22:35:37.063696: Epoch time: 88.35 s
2024-12-06 22:35:37.064409: Yayy! New best EMA pseudo Dice: 0.7193
2024-12-06 22:35:38.759514: 
2024-12-06 22:35:38.761628: Epoch 78
2024-12-06 22:35:38.762821: Current learning rate: 0.0093
2024-12-06 22:37:06.974505: Validation loss did not improve from -0.53285. Patience: 18/50
2024-12-06 22:37:06.975527: train_loss -0.6796
2024-12-06 22:37:06.976285: val_loss -0.5048
2024-12-06 22:37:06.977061: Pseudo dice [0.7207]
2024-12-06 22:37:06.977873: Epoch time: 88.22 s
2024-12-06 22:37:06.978650: Yayy! New best EMA pseudo Dice: 0.7195
2024-12-06 22:37:08.673692: 
2024-12-06 22:37:08.675118: Epoch 79
2024-12-06 22:37:08.675893: Current learning rate: 0.00929
2024-12-06 22:38:37.017829: Validation loss did not improve from -0.53285. Patience: 19/50
2024-12-06 22:38:37.019004: train_loss -0.6791
2024-12-06 22:38:37.020490: val_loss -0.4647
2024-12-06 22:38:37.021873: Pseudo dice [0.7034]
2024-12-06 22:38:37.023144: Epoch time: 88.35 s
2024-12-06 22:38:38.705604: 
2024-12-06 22:38:38.707531: Epoch 80
2024-12-06 22:38:38.708936: Current learning rate: 0.00928
2024-12-06 22:40:07.123138: Validation loss did not improve from -0.53285. Patience: 20/50
2024-12-06 22:40:07.124163: train_loss -0.675
2024-12-06 22:40:07.125316: val_loss -0.487
2024-12-06 22:40:07.126347: Pseudo dice [0.7089]
2024-12-06 22:40:07.127224: Epoch time: 88.42 s
2024-12-06 22:40:08.836103: 
2024-12-06 22:40:08.837980: Epoch 81
2024-12-06 22:40:08.839053: Current learning rate: 0.00927
2024-12-06 22:41:37.368427: Validation loss did not improve from -0.53285. Patience: 21/50
2024-12-06 22:41:37.395946: train_loss -0.6727
2024-12-06 22:41:37.397000: val_loss -0.4807
2024-12-06 22:41:37.397872: Pseudo dice [0.7142]
2024-12-06 22:41:37.398543: Epoch time: 88.56 s
2024-12-06 22:41:38.760524: 
2024-12-06 22:41:38.761882: Epoch 82
2024-12-06 22:41:38.762580: Current learning rate: 0.00926
2024-12-06 22:43:07.524981: Validation loss did not improve from -0.53285. Patience: 22/50
2024-12-06 22:43:07.526196: train_loss -0.6747
2024-12-06 22:43:07.527436: val_loss -0.5066
2024-12-06 22:43:07.528496: Pseudo dice [0.7229]
2024-12-06 22:43:07.529572: Epoch time: 88.77 s
2024-12-06 22:43:08.753483: 
2024-12-06 22:43:08.755518: Epoch 83
2024-12-06 22:43:08.756670: Current learning rate: 0.00925
2024-12-06 22:44:37.053159: Validation loss did not improve from -0.53285. Patience: 23/50
2024-12-06 22:44:37.054544: train_loss -0.6844
2024-12-06 22:44:37.055467: val_loss -0.5228
2024-12-06 22:44:37.056237: Pseudo dice [0.723]
2024-12-06 22:44:37.057078: Epoch time: 88.3 s
2024-12-06 22:44:38.275589: 
2024-12-06 22:44:38.277541: Epoch 84
2024-12-06 22:44:38.278480: Current learning rate: 0.00924
2024-12-06 22:46:06.513890: Validation loss did not improve from -0.53285. Patience: 24/50
2024-12-06 22:46:06.514872: train_loss -0.6893
2024-12-06 22:46:06.515785: val_loss -0.512
2024-12-06 22:46:06.516650: Pseudo dice [0.7258]
2024-12-06 22:46:06.517392: Epoch time: 88.24 s
2024-12-06 22:46:08.114876: 
2024-12-06 22:46:08.116453: Epoch 85
2024-12-06 22:46:08.117800: Current learning rate: 0.00923
2024-12-06 22:47:36.617773: Validation loss did not improve from -0.53285. Patience: 25/50
2024-12-06 22:47:36.618946: train_loss -0.6908
2024-12-06 22:47:36.620324: val_loss -0.5127
2024-12-06 22:47:36.621050: Pseudo dice [0.7302]
2024-12-06 22:47:36.621836: Epoch time: 88.51 s
2024-12-06 22:47:36.622481: Yayy! New best EMA pseudo Dice: 0.7198
2024-12-06 22:47:38.174037: 
2024-12-06 22:47:38.175947: Epoch 86
2024-12-06 22:47:38.177122: Current learning rate: 0.00922
2024-12-06 22:49:06.273894: Validation loss did not improve from -0.53285. Patience: 26/50
2024-12-06 22:49:06.275196: train_loss -0.691
2024-12-06 22:49:06.276163: val_loss -0.4966
2024-12-06 22:49:06.276855: Pseudo dice [0.7216]
2024-12-06 22:49:06.277639: Epoch time: 88.1 s
2024-12-06 22:49:06.278304: Yayy! New best EMA pseudo Dice: 0.72
2024-12-06 22:49:07.820702: 
2024-12-06 22:49:07.821754: Epoch 87
2024-12-06 22:49:07.822414: Current learning rate: 0.00921
2024-12-06 22:50:35.754580: Validation loss did not improve from -0.53285. Patience: 27/50
2024-12-06 22:50:35.755515: train_loss -0.6905
2024-12-06 22:50:35.756492: val_loss -0.5252
2024-12-06 22:50:35.757140: Pseudo dice [0.7322]
2024-12-06 22:50:35.757995: Epoch time: 87.94 s
2024-12-06 22:50:35.758631: Yayy! New best EMA pseudo Dice: 0.7212
2024-12-06 22:50:37.333987: 
2024-12-06 22:50:37.335700: Epoch 88
2024-12-06 22:50:37.336787: Current learning rate: 0.0092
2024-12-06 22:52:05.123830: Validation loss did not improve from -0.53285. Patience: 28/50
2024-12-06 22:52:05.124968: train_loss -0.6914
2024-12-06 22:52:05.125968: val_loss -0.4884
2024-12-06 22:52:05.126715: Pseudo dice [0.6994]
2024-12-06 22:52:05.127780: Epoch time: 87.79 s
2024-12-06 22:52:06.354716: 
2024-12-06 22:52:06.356158: Epoch 89
2024-12-06 22:52:06.357113: Current learning rate: 0.0092
2024-12-06 22:53:34.636649: Validation loss did not improve from -0.53285. Patience: 29/50
2024-12-06 22:53:34.637932: train_loss -0.6994
2024-12-06 22:53:34.638979: val_loss -0.5117
2024-12-06 22:53:34.639792: Pseudo dice [0.734]
2024-12-06 22:53:34.640612: Epoch time: 88.28 s
2024-12-06 22:53:36.215901: 
2024-12-06 22:53:36.217828: Epoch 90
2024-12-06 22:53:36.219063: Current learning rate: 0.00919
2024-12-06 22:55:03.827759: Validation loss did not improve from -0.53285. Patience: 30/50
2024-12-06 22:55:03.828525: train_loss -0.6903
2024-12-06 22:55:03.829506: val_loss -0.4941
2024-12-06 22:55:03.830385: Pseudo dice [0.7212]
2024-12-06 22:55:03.831204: Epoch time: 87.61 s
2024-12-06 22:55:05.369086: 
2024-12-06 22:55:05.371295: Epoch 91
2024-12-06 22:55:05.372184: Current learning rate: 0.00918
2024-12-06 22:56:33.382982: Validation loss improved from -0.53285 to -0.53693! Patience: 30/50
2024-12-06 22:56:33.384115: train_loss -0.6938
2024-12-06 22:56:33.385338: val_loss -0.5369
2024-12-06 22:56:33.386425: Pseudo dice [0.7417]
2024-12-06 22:56:33.387402: Epoch time: 88.02 s
2024-12-06 22:56:33.388424: Yayy! New best EMA pseudo Dice: 0.7227
2024-12-06 22:56:35.048246: 
2024-12-06 22:56:35.049634: Epoch 92
2024-12-06 22:56:35.050581: Current learning rate: 0.00917
2024-12-06 22:58:03.215361: Validation loss did not improve from -0.53693. Patience: 1/50
2024-12-06 22:58:03.216667: train_loss -0.6792
2024-12-06 22:58:03.218019: val_loss -0.4619
2024-12-06 22:58:03.219012: Pseudo dice [0.6899]
2024-12-06 22:58:03.220003: Epoch time: 88.17 s
2024-12-06 22:58:04.406327: 
2024-12-06 22:58:04.408117: Epoch 93
2024-12-06 22:58:04.408995: Current learning rate: 0.00916
2024-12-06 22:59:32.401695: Validation loss did not improve from -0.53693. Patience: 2/50
2024-12-06 22:59:32.402788: train_loss -0.6894
2024-12-06 22:59:32.403566: val_loss -0.4961
2024-12-06 22:59:32.404298: Pseudo dice [0.7036]
2024-12-06 22:59:32.405090: Epoch time: 88.0 s
2024-12-06 22:59:33.597346: 
2024-12-06 22:59:33.599253: Epoch 94
2024-12-06 22:59:33.600112: Current learning rate: 0.00915
2024-12-06 23:01:01.830729: Validation loss did not improve from -0.53693. Patience: 3/50
2024-12-06 23:01:01.831887: train_loss -0.6941
2024-12-06 23:01:01.832765: val_loss -0.5205
2024-12-06 23:01:01.833610: Pseudo dice [0.7345]
2024-12-06 23:01:01.834342: Epoch time: 88.24 s
2024-12-06 23:01:03.466750: 
2024-12-06 23:01:03.468525: Epoch 95
2024-12-06 23:01:03.469318: Current learning rate: 0.00914
2024-12-06 23:02:31.667266: Validation loss did not improve from -0.53693. Patience: 4/50
2024-12-06 23:02:31.668388: train_loss -0.6955
2024-12-06 23:02:31.669267: val_loss -0.5356
2024-12-06 23:02:31.669989: Pseudo dice [0.7427]
2024-12-06 23:02:31.670823: Epoch time: 88.2 s
2024-12-06 23:02:32.849841: 
2024-12-06 23:02:32.851511: Epoch 96
2024-12-06 23:02:32.852538: Current learning rate: 0.00913
2024-12-06 23:04:01.167252: Validation loss did not improve from -0.53693. Patience: 5/50
2024-12-06 23:04:01.168299: train_loss -0.6906
2024-12-06 23:04:01.169211: val_loss -0.4848
2024-12-06 23:04:01.170011: Pseudo dice [0.7019]
2024-12-06 23:04:01.170718: Epoch time: 88.32 s
2024-12-06 23:04:02.398792: 
2024-12-06 23:04:02.400483: Epoch 97
2024-12-06 23:04:02.401350: Current learning rate: 0.00912
2024-12-06 23:05:30.634675: Validation loss did not improve from -0.53693. Patience: 6/50
2024-12-06 23:05:30.635859: train_loss -0.703
2024-12-06 23:05:30.637199: val_loss -0.4733
2024-12-06 23:05:30.638171: Pseudo dice [0.7224]
2024-12-06 23:05:30.639342: Epoch time: 88.24 s
2024-12-06 23:05:31.834056: 
2024-12-06 23:05:31.835675: Epoch 98
2024-12-06 23:05:31.836758: Current learning rate: 0.00911
2024-12-06 23:07:00.038411: Validation loss did not improve from -0.53693. Patience: 7/50
2024-12-06 23:07:00.039261: train_loss -0.6969
2024-12-06 23:07:00.040127: val_loss -0.4983
2024-12-06 23:07:00.041009: Pseudo dice [0.7238]
2024-12-06 23:07:00.041744: Epoch time: 88.21 s
2024-12-06 23:07:01.240651: 
2024-12-06 23:07:01.242655: Epoch 99
2024-12-06 23:07:01.243815: Current learning rate: 0.0091
2024-12-06 23:08:29.061091: Validation loss did not improve from -0.53693. Patience: 8/50
2024-12-06 23:08:29.062324: train_loss -0.6977
2024-12-06 23:08:29.063156: val_loss -0.4814
2024-12-06 23:08:29.063850: Pseudo dice [0.7294]
2024-12-06 23:08:29.064730: Epoch time: 87.82 s
2024-12-06 23:08:30.625956: 
2024-12-06 23:08:30.628180: Epoch 100
2024-12-06 23:08:30.629388: Current learning rate: 0.0091
2024-12-06 23:09:58.733283: Validation loss did not improve from -0.53693. Patience: 9/50
2024-12-06 23:09:58.734179: train_loss -0.6982
2024-12-06 23:09:58.734911: val_loss -0.4957
2024-12-06 23:09:58.735612: Pseudo dice [0.7169]
2024-12-06 23:09:58.736286: Epoch time: 88.11 s
2024-12-06 23:09:59.962987: 
2024-12-06 23:09:59.965000: Epoch 101
2024-12-06 23:09:59.965874: Current learning rate: 0.00909
2024-12-06 23:11:27.899685: Validation loss did not improve from -0.53693. Patience: 10/50
2024-12-06 23:11:27.900418: train_loss -0.7103
2024-12-06 23:11:27.901304: val_loss -0.5157
2024-12-06 23:11:27.902138: Pseudo dice [0.7183]
2024-12-06 23:11:27.902778: Epoch time: 87.94 s
2024-12-06 23:11:29.114534: 
2024-12-06 23:11:29.116187: Epoch 102
2024-12-06 23:11:29.116947: Current learning rate: 0.00908
2024-12-06 23:12:57.044329: Validation loss did not improve from -0.53693. Patience: 11/50
2024-12-06 23:12:57.045368: train_loss -0.713
2024-12-06 23:12:57.046370: val_loss -0.4938
2024-12-06 23:12:57.047142: Pseudo dice [0.7175]
2024-12-06 23:12:57.047986: Epoch time: 87.93 s
2024-12-06 23:12:58.658717: 
2024-12-06 23:12:58.660398: Epoch 103
2024-12-06 23:12:58.661549: Current learning rate: 0.00907
2024-12-06 23:14:26.997596: Validation loss did not improve from -0.53693. Patience: 12/50
2024-12-06 23:14:27.000005: train_loss -0.7133
2024-12-06 23:14:27.001180: val_loss -0.4857
2024-12-06 23:14:27.002253: Pseudo dice [0.7053]
2024-12-06 23:14:27.003221: Epoch time: 88.34 s
2024-12-06 23:14:28.241963: 
2024-12-06 23:14:28.243964: Epoch 104
2024-12-06 23:14:28.245349: Current learning rate: 0.00906
2024-12-06 23:15:56.274761: Validation loss did not improve from -0.53693. Patience: 13/50
2024-12-06 23:15:56.275861: train_loss -0.7145
2024-12-06 23:15:56.276904: val_loss -0.5365
2024-12-06 23:15:56.277813: Pseudo dice [0.7356]
2024-12-06 23:15:56.278575: Epoch time: 88.03 s
2024-12-06 23:15:57.829671: 
2024-12-06 23:15:57.831611: Epoch 105
2024-12-06 23:15:57.832615: Current learning rate: 0.00905
2024-12-06 23:17:25.828805: Validation loss improved from -0.53693 to -0.54000! Patience: 13/50
2024-12-06 23:17:25.829975: train_loss -0.7108
2024-12-06 23:17:25.831383: val_loss -0.54
2024-12-06 23:17:25.832433: Pseudo dice [0.7482]
2024-12-06 23:17:25.833427: Epoch time: 88.0 s
2024-12-06 23:17:25.834344: Yayy! New best EMA pseudo Dice: 0.7233
2024-12-06 23:17:27.418125: 
2024-12-06 23:17:27.420109: Epoch 106
2024-12-06 23:17:27.421026: Current learning rate: 0.00904
2024-12-06 23:18:55.646267: Validation loss did not improve from -0.54000. Patience: 1/50
2024-12-06 23:18:55.647611: train_loss -0.7109
2024-12-06 23:18:55.648493: val_loss -0.484
2024-12-06 23:18:55.649281: Pseudo dice [0.7216]
2024-12-06 23:18:55.650139: Epoch time: 88.23 s
2024-12-06 23:18:56.879406: 
2024-12-06 23:18:56.881241: Epoch 107
2024-12-06 23:18:56.882126: Current learning rate: 0.00903
2024-12-06 23:20:25.210323: Validation loss did not improve from -0.54000. Patience: 2/50
2024-12-06 23:20:25.211087: train_loss -0.7159
2024-12-06 23:20:25.211999: val_loss -0.5222
2024-12-06 23:20:25.212772: Pseudo dice [0.7357]
2024-12-06 23:20:25.213539: Epoch time: 88.33 s
2024-12-06 23:20:25.214336: Yayy! New best EMA pseudo Dice: 0.7244
2024-12-06 23:20:26.792832: 
2024-12-06 23:20:26.794346: Epoch 108
2024-12-06 23:20:26.795266: Current learning rate: 0.00902
2024-12-06 23:21:55.259562: Validation loss did not improve from -0.54000. Patience: 3/50
2024-12-06 23:21:55.260798: train_loss -0.712
2024-12-06 23:21:55.262109: val_loss -0.5004
2024-12-06 23:21:55.263283: Pseudo dice [0.7278]
2024-12-06 23:21:55.264390: Epoch time: 88.47 s
2024-12-06 23:21:55.265406: Yayy! New best EMA pseudo Dice: 0.7247
2024-12-06 23:21:56.867264: 
2024-12-06 23:21:56.869371: Epoch 109
2024-12-06 23:21:56.870416: Current learning rate: 0.00901
2024-12-06 23:23:25.112941: Validation loss did not improve from -0.54000. Patience: 4/50
2024-12-06 23:23:25.113809: train_loss -0.7101
2024-12-06 23:23:25.114721: val_loss -0.5038
2024-12-06 23:23:25.115605: Pseudo dice [0.7223]
2024-12-06 23:23:25.116465: Epoch time: 88.25 s
2024-12-06 23:23:26.708474: 
2024-12-06 23:23:26.710201: Epoch 110
2024-12-06 23:23:26.711347: Current learning rate: 0.009
2024-12-06 23:24:55.089126: Validation loss did not improve from -0.54000. Patience: 5/50
2024-12-06 23:24:55.090081: train_loss -0.7075
2024-12-06 23:24:55.091069: val_loss -0.4913
2024-12-06 23:24:55.091819: Pseudo dice [0.7182]
2024-12-06 23:24:55.092674: Epoch time: 88.38 s
2024-12-06 23:24:56.331698: 
2024-12-06 23:24:56.333979: Epoch 111
2024-12-06 23:24:56.335135: Current learning rate: 0.009
2024-12-06 23:26:24.559910: Validation loss did not improve from -0.54000. Patience: 6/50
2024-12-06 23:26:24.561332: train_loss -0.7171
2024-12-06 23:26:24.562739: val_loss -0.4618
2024-12-06 23:26:24.563626: Pseudo dice [0.7064]
2024-12-06 23:26:24.564587: Epoch time: 88.23 s
2024-12-06 23:26:25.772088: 
2024-12-06 23:26:25.773811: Epoch 112
2024-12-06 23:26:25.775020: Current learning rate: 0.00899
2024-12-06 23:27:53.883000: Validation loss did not improve from -0.54000. Patience: 7/50
2024-12-06 23:27:53.883942: train_loss -0.7197
2024-12-06 23:27:53.884701: val_loss -0.4802
2024-12-06 23:27:53.885511: Pseudo dice [0.7137]
2024-12-06 23:27:53.886246: Epoch time: 88.11 s
2024-12-06 23:27:55.426729: 
2024-12-06 23:27:55.428386: Epoch 113
2024-12-06 23:27:55.429521: Current learning rate: 0.00898
2024-12-06 23:29:23.812727: Validation loss did not improve from -0.54000. Patience: 8/50
2024-12-06 23:29:23.813669: train_loss -0.7125
2024-12-06 23:29:23.814949: val_loss -0.5179
2024-12-06 23:29:23.815795: Pseudo dice [0.7317]
2024-12-06 23:29:23.816479: Epoch time: 88.39 s
2024-12-06 23:29:25.032549: 
2024-12-06 23:29:25.034245: Epoch 114
2024-12-06 23:29:25.035163: Current learning rate: 0.00897
2024-12-06 23:30:53.319342: Validation loss did not improve from -0.54000. Patience: 9/50
2024-12-06 23:30:53.320334: train_loss -0.7162
2024-12-06 23:30:53.321217: val_loss -0.523
2024-12-06 23:30:53.322182: Pseudo dice [0.7295]
2024-12-06 23:30:53.322811: Epoch time: 88.29 s
2024-12-06 23:30:54.993382: 
2024-12-06 23:30:54.994998: Epoch 115
2024-12-06 23:30:54.995768: Current learning rate: 0.00896
2024-12-06 23:32:23.422302: Validation loss did not improve from -0.54000. Patience: 10/50
2024-12-06 23:32:23.423415: train_loss -0.7247
2024-12-06 23:32:23.424348: val_loss -0.4748
2024-12-06 23:32:23.425139: Pseudo dice [0.7189]
2024-12-06 23:32:23.425911: Epoch time: 88.43 s
2024-12-06 23:32:24.684820: 
2024-12-06 23:32:24.685930: Epoch 116
2024-12-06 23:32:24.686791: Current learning rate: 0.00895
2024-12-06 23:33:52.876699: Validation loss did not improve from -0.54000. Patience: 11/50
2024-12-06 23:33:52.877638: train_loss -0.7249
2024-12-06 23:33:52.878716: val_loss -0.4787
2024-12-06 23:33:52.879699: Pseudo dice [0.7031]
2024-12-06 23:33:52.880655: Epoch time: 88.19 s
2024-12-06 23:33:54.156848: 
2024-12-06 23:33:54.158394: Epoch 117
2024-12-06 23:33:54.159645: Current learning rate: 0.00894
2024-12-06 23:35:22.465745: Validation loss did not improve from -0.54000. Patience: 12/50
2024-12-06 23:35:22.466970: train_loss -0.725
2024-12-06 23:35:22.467916: val_loss -0.4876
2024-12-06 23:35:22.468648: Pseudo dice [0.7238]
2024-12-06 23:35:22.469584: Epoch time: 88.31 s
2024-12-06 23:35:23.718886: 
2024-12-06 23:35:23.720881: Epoch 118
2024-12-06 23:35:23.721890: Current learning rate: 0.00893
2024-12-06 23:36:51.794276: Validation loss did not improve from -0.54000. Patience: 13/50
2024-12-06 23:36:51.795158: train_loss -0.7255
2024-12-06 23:36:51.796088: val_loss -0.4876
2024-12-06 23:36:51.796858: Pseudo dice [0.7196]
2024-12-06 23:36:51.797580: Epoch time: 88.08 s
2024-12-06 23:36:53.053495: 
2024-12-06 23:36:53.055617: Epoch 119
2024-12-06 23:36:53.056565: Current learning rate: 0.00892
2024-12-06 23:38:21.421403: Validation loss did not improve from -0.54000. Patience: 14/50
2024-12-06 23:38:21.422718: train_loss -0.7275
2024-12-06 23:38:21.423732: val_loss -0.5081
2024-12-06 23:38:21.424855: Pseudo dice [0.7271]
2024-12-06 23:38:21.425820: Epoch time: 88.37 s
2024-12-06 23:38:23.024112: 
2024-12-06 23:38:23.025958: Epoch 120
2024-12-06 23:38:23.027013: Current learning rate: 0.00891
2024-12-06 23:39:51.514304: Validation loss did not improve from -0.54000. Patience: 15/50
2024-12-06 23:39:51.515368: train_loss -0.7231
2024-12-06 23:39:51.516259: val_loss -0.5106
2024-12-06 23:39:51.516990: Pseudo dice [0.7286]
2024-12-06 23:39:51.517756: Epoch time: 88.49 s
2024-12-06 23:39:52.739979: 
2024-12-06 23:39:52.741803: Epoch 121
2024-12-06 23:39:52.742938: Current learning rate: 0.0089
2024-12-06 23:41:20.956752: Validation loss did not improve from -0.54000. Patience: 16/50
2024-12-06 23:41:20.957930: train_loss -0.7254
2024-12-06 23:41:20.959069: val_loss -0.5101
2024-12-06 23:41:20.960101: Pseudo dice [0.7294]
2024-12-06 23:41:20.961026: Epoch time: 88.22 s
2024-12-06 23:41:22.279191: 
2024-12-06 23:41:22.280941: Epoch 122
2024-12-06 23:41:22.282157: Current learning rate: 0.00889
2024-12-06 23:42:51.788919: Validation loss did not improve from -0.54000. Patience: 17/50
2024-12-06 23:42:51.790252: train_loss -0.7321
2024-12-06 23:42:51.791698: val_loss -0.5092
2024-12-06 23:42:51.792615: Pseudo dice [0.7318]
2024-12-06 23:42:51.793437: Epoch time: 89.51 s
2024-12-06 23:42:53.020291: 
2024-12-06 23:42:53.021917: Epoch 123
2024-12-06 23:42:53.022734: Current learning rate: 0.00889
2024-12-06 23:44:23.046646: Validation loss did not improve from -0.54000. Patience: 18/50
2024-12-06 23:44:23.048191: train_loss -0.7265
2024-12-06 23:44:23.049929: val_loss -0.4901
2024-12-06 23:44:23.051154: Pseudo dice [0.7226]
2024-12-06 23:44:23.052218: Epoch time: 90.03 s
2024-12-06 23:44:24.599784: 
2024-12-06 23:44:24.601524: Epoch 124
2024-12-06 23:44:24.602646: Current learning rate: 0.00888
2024-12-06 23:45:54.043648: Validation loss did not improve from -0.54000. Patience: 19/50
2024-12-06 23:45:54.044808: train_loss -0.7332
2024-12-06 23:45:54.045946: val_loss -0.5355
2024-12-06 23:45:54.046934: Pseudo dice [0.743]
2024-12-06 23:45:54.047956: Epoch time: 89.45 s
2024-12-06 23:45:54.449232: Yayy! New best EMA pseudo Dice: 0.7256
2024-12-06 23:45:56.021275: 
2024-12-06 23:45:56.023220: Epoch 125
2024-12-06 23:45:56.024723: Current learning rate: 0.00887
2024-12-06 23:47:25.568786: Validation loss did not improve from -0.54000. Patience: 20/50
2024-12-06 23:47:25.569983: train_loss -0.7231
2024-12-06 23:47:25.570935: val_loss -0.4661
2024-12-06 23:47:25.571642: Pseudo dice [0.7037]
2024-12-06 23:47:25.572453: Epoch time: 89.55 s
2024-12-06 23:47:26.794841: 
2024-12-06 23:47:26.797024: Epoch 126
2024-12-06 23:47:26.797999: Current learning rate: 0.00886
2024-12-06 23:48:56.069465: Validation loss did not improve from -0.54000. Patience: 21/50
2024-12-06 23:48:56.071438: train_loss -0.7277
2024-12-06 23:48:56.073084: val_loss -0.5061
2024-12-06 23:48:56.074085: Pseudo dice [0.7256]
2024-12-06 23:48:56.075155: Epoch time: 89.28 s
2024-12-06 23:48:57.358335: 
2024-12-06 23:48:57.360520: Epoch 127
2024-12-06 23:48:57.361635: Current learning rate: 0.00885
2024-12-06 23:50:26.645107: Validation loss did not improve from -0.54000. Patience: 22/50
2024-12-06 23:50:26.646115: train_loss -0.724
2024-12-06 23:50:26.646939: val_loss -0.4885
2024-12-06 23:50:26.647674: Pseudo dice [0.7142]
2024-12-06 23:50:26.648464: Epoch time: 89.29 s
2024-12-06 23:50:27.842834: 
2024-12-06 23:50:27.844743: Epoch 128
2024-12-06 23:50:27.845971: Current learning rate: 0.00884
2024-12-06 23:51:56.860279: Validation loss did not improve from -0.54000. Patience: 23/50
2024-12-06 23:51:56.861290: train_loss -0.7267
2024-12-06 23:51:56.862517: val_loss -0.4834
2024-12-06 23:51:56.863703: Pseudo dice [0.7161]
2024-12-06 23:51:56.864824: Epoch time: 89.02 s
2024-12-06 23:51:58.045276: 
2024-12-06 23:51:58.046766: Epoch 129
2024-12-06 23:51:58.047904: Current learning rate: 0.00883
2024-12-06 23:53:27.329137: Validation loss did not improve from -0.54000. Patience: 24/50
2024-12-06 23:53:27.330415: train_loss -0.722
2024-12-06 23:53:27.331639: val_loss -0.4883
2024-12-06 23:53:27.332728: Pseudo dice [0.7156]
2024-12-06 23:53:27.333666: Epoch time: 89.29 s
2024-12-06 23:53:28.897737: 
2024-12-06 23:53:28.899846: Epoch 130
2024-12-06 23:53:28.900880: Current learning rate: 0.00882
2024-12-06 23:54:57.977130: Validation loss did not improve from -0.54000. Patience: 25/50
2024-12-06 23:54:57.978015: train_loss -0.7255
2024-12-06 23:54:57.979084: val_loss -0.5195
2024-12-06 23:54:57.979800: Pseudo dice [0.7245]
2024-12-06 23:54:57.980858: Epoch time: 89.08 s
2024-12-06 23:54:59.200498: 
2024-12-06 23:54:59.202570: Epoch 131
2024-12-06 23:54:59.203635: Current learning rate: 0.00881
2024-12-06 23:56:28.243672: Validation loss did not improve from -0.54000. Patience: 26/50
2024-12-06 23:56:28.245035: train_loss -0.7257
2024-12-06 23:56:28.246524: val_loss -0.4639
2024-12-06 23:56:28.247741: Pseudo dice [0.7061]
2024-12-06 23:56:28.248701: Epoch time: 89.05 s
2024-12-06 23:56:29.447554: 
2024-12-06 23:56:29.449653: Epoch 132
2024-12-06 23:56:29.450800: Current learning rate: 0.0088
2024-12-06 23:57:58.482478: Validation loss did not improve from -0.54000. Patience: 27/50
2024-12-06 23:57:58.483803: train_loss -0.7227
2024-12-06 23:57:58.485013: val_loss -0.5202
2024-12-06 23:57:58.485800: Pseudo dice [0.7457]
2024-12-06 23:57:58.486709: Epoch time: 89.04 s
2024-12-06 23:57:59.694899: 
2024-12-06 23:57:59.696523: Epoch 133
2024-12-06 23:57:59.697890: Current learning rate: 0.00879
2024-12-06 23:59:28.780946: Validation loss did not improve from -0.54000. Patience: 28/50
2024-12-06 23:59:28.782326: train_loss -0.7272
2024-12-06 23:59:28.783484: val_loss -0.4831
2024-12-06 23:59:28.784269: Pseudo dice [0.7172]
2024-12-06 23:59:28.784978: Epoch time: 89.09 s
2024-12-06 23:59:29.942824: 
2024-12-06 23:59:29.945272: Epoch 134
2024-12-06 23:59:29.946694: Current learning rate: 0.00879
2024-12-07 00:00:59.214563: Validation loss did not improve from -0.54000. Patience: 29/50
2024-12-07 00:00:59.215476: train_loss -0.7263
2024-12-07 00:00:59.216446: val_loss -0.4848
2024-12-07 00:00:59.217118: Pseudo dice [0.7077]
2024-12-07 00:00:59.217948: Epoch time: 89.27 s
2024-12-07 00:01:01.096844: 
2024-12-07 00:01:01.099148: Epoch 135
2024-12-07 00:01:01.100405: Current learning rate: 0.00878
2024-12-07 00:02:30.062520: Validation loss did not improve from -0.54000. Patience: 30/50
2024-12-07 00:02:30.063809: train_loss -0.726
2024-12-07 00:02:30.065352: val_loss -0.5153
2024-12-07 00:02:30.066369: Pseudo dice [0.7344]
2024-12-07 00:02:30.067630: Epoch time: 88.97 s
2024-12-07 00:02:31.288774: 
2024-12-07 00:02:31.290752: Epoch 136
2024-12-07 00:02:31.291587: Current learning rate: 0.00877
2024-12-07 00:04:00.748634: Validation loss did not improve from -0.54000. Patience: 31/50
2024-12-07 00:04:00.749931: train_loss -0.7295
2024-12-07 00:04:00.750895: val_loss -0.5067
2024-12-07 00:04:00.751644: Pseudo dice [0.7181]
2024-12-07 00:04:00.752326: Epoch time: 89.46 s
2024-12-07 00:04:01.987064: 
2024-12-07 00:04:01.989318: Epoch 137
2024-12-07 00:04:01.990487: Current learning rate: 0.00876
2024-12-07 00:05:31.674659: Validation loss improved from -0.54000 to -0.54247! Patience: 31/50
2024-12-07 00:05:31.675959: train_loss -0.738
2024-12-07 00:05:31.676897: val_loss -0.5425
2024-12-07 00:05:31.678064: Pseudo dice [0.741]
2024-12-07 00:05:31.679362: Epoch time: 89.69 s
2024-12-07 00:05:32.888684: 
2024-12-07 00:05:32.891465: Epoch 138
2024-12-07 00:05:32.893510: Current learning rate: 0.00875
2024-12-07 00:07:02.058799: Validation loss did not improve from -0.54247. Patience: 1/50
2024-12-07 00:07:02.060242: train_loss -0.7371
2024-12-07 00:07:02.061877: val_loss -0.5175
2024-12-07 00:07:02.063028: Pseudo dice [0.7378]
2024-12-07 00:07:02.064174: Epoch time: 89.17 s
2024-12-07 00:07:03.268616: 
2024-12-07 00:07:03.270928: Epoch 139
2024-12-07 00:07:03.272104: Current learning rate: 0.00874
2024-12-07 00:08:32.623938: Validation loss did not improve from -0.54247. Patience: 2/50
2024-12-07 00:08:32.625038: train_loss -0.7411
2024-12-07 00:08:32.626079: val_loss -0.4641
2024-12-07 00:08:32.626885: Pseudo dice [0.7093]
2024-12-07 00:08:32.627755: Epoch time: 89.36 s
2024-12-07 00:08:34.179795: 
2024-12-07 00:08:34.181556: Epoch 140
2024-12-07 00:08:34.182629: Current learning rate: 0.00873
2024-12-07 00:10:03.966554: Validation loss did not improve from -0.54247. Patience: 3/50
2024-12-07 00:10:03.967978: train_loss -0.7422
2024-12-07 00:10:03.969271: val_loss -0.5081
2024-12-07 00:10:03.970263: Pseudo dice [0.7278]
2024-12-07 00:10:03.970928: Epoch time: 89.79 s
2024-12-07 00:10:05.148674: 
2024-12-07 00:10:05.151012: Epoch 141
2024-12-07 00:10:05.152416: Current learning rate: 0.00872
2024-12-07 00:11:34.190012: Validation loss did not improve from -0.54247. Patience: 4/50
2024-12-07 00:11:34.191503: train_loss -0.7368
2024-12-07 00:11:34.192763: val_loss -0.5189
2024-12-07 00:11:34.193630: Pseudo dice [0.7345]
2024-12-07 00:11:34.194411: Epoch time: 89.04 s
2024-12-07 00:11:35.397808: 
2024-12-07 00:11:35.399742: Epoch 142
2024-12-07 00:11:35.400980: Current learning rate: 0.00871
2024-12-07 00:13:04.930294: Validation loss did not improve from -0.54247. Patience: 5/50
2024-12-07 00:13:04.931790: train_loss -0.7409
2024-12-07 00:13:04.932862: val_loss -0.4492
2024-12-07 00:13:04.933652: Pseudo dice [0.6942]
2024-12-07 00:13:04.934460: Epoch time: 89.53 s
2024-12-07 00:13:06.152386: 
2024-12-07 00:13:06.154372: Epoch 143
2024-12-07 00:13:06.155229: Current learning rate: 0.0087
2024-12-07 00:14:35.694591: Validation loss did not improve from -0.54247. Patience: 6/50
2024-12-07 00:14:35.695747: train_loss -0.7407
2024-12-07 00:14:35.697077: val_loss -0.5203
2024-12-07 00:14:35.697918: Pseudo dice [0.7337]
2024-12-07 00:14:35.698848: Epoch time: 89.54 s
2024-12-07 00:14:36.877886: 
2024-12-07 00:14:36.880060: Epoch 144
2024-12-07 00:14:36.881577: Current learning rate: 0.00869
2024-12-07 00:16:06.608662: Validation loss did not improve from -0.54247. Patience: 7/50
2024-12-07 00:16:06.609965: train_loss -0.7399
2024-12-07 00:16:06.611232: val_loss -0.495
2024-12-07 00:16:06.611959: Pseudo dice [0.7199]
2024-12-07 00:16:06.612864: Epoch time: 89.73 s
2024-12-07 00:16:08.466730: 
2024-12-07 00:16:08.468813: Epoch 145
2024-12-07 00:16:08.469953: Current learning rate: 0.00868
2024-12-07 00:17:37.795869: Validation loss did not improve from -0.54247. Patience: 8/50
2024-12-07 00:17:37.797312: train_loss -0.7294
2024-12-07 00:17:37.798647: val_loss -0.4824
2024-12-07 00:17:37.799483: Pseudo dice [0.7132]
2024-12-07 00:17:37.800550: Epoch time: 89.33 s
2024-12-07 00:17:39.016459: 
2024-12-07 00:17:39.018870: Epoch 146
2024-12-07 00:17:39.020550: Current learning rate: 0.00868
2024-12-07 00:19:08.507489: Validation loss did not improve from -0.54247. Patience: 9/50
2024-12-07 00:19:08.508976: train_loss -0.7357
2024-12-07 00:19:08.510174: val_loss -0.5182
2024-12-07 00:19:08.511377: Pseudo dice [0.7262]
2024-12-07 00:19:08.512588: Epoch time: 89.49 s
2024-12-07 00:19:09.721516: 
2024-12-07 00:19:09.723717: Epoch 147
2024-12-07 00:19:09.724791: Current learning rate: 0.00867
2024-12-07 00:20:38.970891: Validation loss did not improve from -0.54247. Patience: 10/50
2024-12-07 00:20:38.972678: train_loss -0.7405
2024-12-07 00:20:38.974481: val_loss -0.5144
2024-12-07 00:20:38.975766: Pseudo dice [0.73]
2024-12-07 00:20:38.976987: Epoch time: 89.25 s
2024-12-07 00:20:40.214025: 
2024-12-07 00:20:40.216025: Epoch 148
2024-12-07 00:20:40.216990: Current learning rate: 0.00866
2024-12-07 00:22:09.596646: Validation loss did not improve from -0.54247. Patience: 11/50
2024-12-07 00:22:09.598068: train_loss -0.7459
2024-12-07 00:22:09.599453: val_loss -0.5268
2024-12-07 00:22:09.600520: Pseudo dice [0.7359]
2024-12-07 00:22:09.601610: Epoch time: 89.39 s
2024-12-07 00:22:10.820069: 
2024-12-07 00:22:10.822115: Epoch 149
2024-12-07 00:22:10.822976: Current learning rate: 0.00865
2024-12-07 00:23:40.159419: Validation loss did not improve from -0.54247. Patience: 12/50
2024-12-07 00:23:40.160506: train_loss -0.7391
2024-12-07 00:23:40.161699: val_loss -0.4918
2024-12-07 00:23:40.162765: Pseudo dice [0.7288]
2024-12-07 00:23:40.163685: Epoch time: 89.34 s
2024-12-07 00:23:41.688172: 
2024-12-07 00:23:41.690353: Epoch 150
2024-12-07 00:23:41.691381: Current learning rate: 0.00864
2024-12-07 00:25:11.295150: Validation loss did not improve from -0.54247. Patience: 13/50
2024-12-07 00:25:11.296375: train_loss -0.7419
2024-12-07 00:25:11.297563: val_loss -0.5235
2024-12-07 00:25:11.298442: Pseudo dice [0.7414]
2024-12-07 00:25:11.299064: Epoch time: 89.61 s
2024-12-07 00:25:11.299709: Yayy! New best EMA pseudo Dice: 0.7264
2024-12-07 00:25:12.875447: 
2024-12-07 00:25:12.877595: Epoch 151
2024-12-07 00:25:12.878415: Current learning rate: 0.00863
2024-12-07 00:26:42.274775: Validation loss did not improve from -0.54247. Patience: 14/50
2024-12-07 00:26:42.275816: train_loss -0.7498
2024-12-07 00:26:42.277045: val_loss -0.4593
2024-12-07 00:26:42.277945: Pseudo dice [0.7132]
2024-12-07 00:26:42.278947: Epoch time: 89.4 s
2024-12-07 00:26:43.501351: 
2024-12-07 00:26:43.503310: Epoch 152
2024-12-07 00:26:43.504498: Current learning rate: 0.00862
2024-12-07 00:28:12.824711: Validation loss did not improve from -0.54247. Patience: 15/50
2024-12-07 00:28:12.825840: train_loss -0.7438
2024-12-07 00:28:12.826765: val_loss -0.5089
2024-12-07 00:28:12.827724: Pseudo dice [0.734]
2024-12-07 00:28:12.828465: Epoch time: 89.33 s
2024-12-07 00:28:14.024860: 
2024-12-07 00:28:14.026206: Epoch 153
2024-12-07 00:28:14.027215: Current learning rate: 0.00861
2024-12-07 00:29:43.565119: Validation loss did not improve from -0.54247. Patience: 16/50
2024-12-07 00:29:43.566197: train_loss -0.7411
2024-12-07 00:29:43.567362: val_loss -0.5328
2024-12-07 00:29:43.568286: Pseudo dice [0.7471]
2024-12-07 00:29:43.569210: Epoch time: 89.54 s
2024-12-07 00:29:43.570055: Yayy! New best EMA pseudo Dice: 0.7281
2024-12-07 00:29:45.171871: 
2024-12-07 00:29:45.173759: Epoch 154
2024-12-07 00:29:45.174890: Current learning rate: 0.0086
2024-12-07 00:31:14.802131: Validation loss did not improve from -0.54247. Patience: 17/50
2024-12-07 00:31:14.803846: train_loss -0.7424
2024-12-07 00:31:14.805225: val_loss -0.4761
2024-12-07 00:31:14.806392: Pseudo dice [0.7082]
2024-12-07 00:31:14.807359: Epoch time: 89.63 s
2024-12-07 00:31:16.752169: 
2024-12-07 00:31:16.753944: Epoch 155
2024-12-07 00:31:16.755082: Current learning rate: 0.00859
2024-12-07 00:32:46.494981: Validation loss did not improve from -0.54247. Patience: 18/50
2024-12-07 00:32:46.496451: train_loss -0.7429
2024-12-07 00:32:46.498086: val_loss -0.5125
2024-12-07 00:32:46.499122: Pseudo dice [0.7239]
2024-12-07 00:32:46.500111: Epoch time: 89.75 s
2024-12-07 00:32:47.677386: 
2024-12-07 00:32:47.679486: Epoch 156
2024-12-07 00:32:47.681127: Current learning rate: 0.00858
2024-12-07 00:34:17.476234: Validation loss did not improve from -0.54247. Patience: 19/50
2024-12-07 00:34:17.477435: train_loss -0.7399
2024-12-07 00:34:17.478319: val_loss -0.465
2024-12-07 00:34:17.479125: Pseudo dice [0.7198]
2024-12-07 00:34:17.479795: Epoch time: 89.8 s
2024-12-07 00:34:18.731349: 
2024-12-07 00:34:18.733299: Epoch 157
2024-12-07 00:34:18.734576: Current learning rate: 0.00858
2024-12-07 00:35:48.160418: Validation loss did not improve from -0.54247. Patience: 20/50
2024-12-07 00:35:48.161510: train_loss -0.742
2024-12-07 00:35:48.162832: val_loss -0.5099
2024-12-07 00:35:48.163609: Pseudo dice [0.7299]
2024-12-07 00:35:48.164660: Epoch time: 89.43 s
2024-12-07 00:35:49.391655: 
2024-12-07 00:35:49.393639: Epoch 158
2024-12-07 00:35:49.394734: Current learning rate: 0.00857
2024-12-07 00:37:18.900828: Validation loss did not improve from -0.54247. Patience: 21/50
2024-12-07 00:37:18.902102: train_loss -0.7466
2024-12-07 00:37:18.902968: val_loss -0.4848
2024-12-07 00:37:18.903780: Pseudo dice [0.7132]
2024-12-07 00:37:18.904740: Epoch time: 89.51 s
2024-12-07 00:37:20.155570: 
2024-12-07 00:37:20.157805: Epoch 159
2024-12-07 00:37:20.158739: Current learning rate: 0.00856
2024-12-07 00:38:49.543415: Validation loss did not improve from -0.54247. Patience: 22/50
2024-12-07 00:38:49.544756: train_loss -0.7328
2024-12-07 00:38:49.545720: val_loss -0.4798
2024-12-07 00:38:49.546710: Pseudo dice [0.7132]
2024-12-07 00:38:49.547707: Epoch time: 89.39 s
2024-12-07 00:38:51.163023: 
2024-12-07 00:38:51.165000: Epoch 160
2024-12-07 00:38:51.166221: Current learning rate: 0.00855
2024-12-07 00:40:20.676732: Validation loss did not improve from -0.54247. Patience: 23/50
2024-12-07 00:40:20.678131: train_loss -0.7339
2024-12-07 00:40:20.679491: val_loss -0.511
2024-12-07 00:40:20.680670: Pseudo dice [0.7274]
2024-12-07 00:40:20.681728: Epoch time: 89.52 s
2024-12-07 00:40:21.881155: 
2024-12-07 00:40:21.883018: Epoch 161
2024-12-07 00:40:21.884298: Current learning rate: 0.00854
2024-12-07 00:41:51.543267: Validation loss did not improve from -0.54247. Patience: 24/50
2024-12-07 00:41:51.544445: train_loss -0.7348
2024-12-07 00:41:51.545548: val_loss -0.5285
2024-12-07 00:41:51.546402: Pseudo dice [0.7413]
2024-12-07 00:41:51.547220: Epoch time: 89.66 s
2024-12-07 00:41:52.760048: 
2024-12-07 00:41:52.761947: Epoch 162
2024-12-07 00:41:52.763388: Current learning rate: 0.00853
2024-12-07 00:43:22.314223: Validation loss did not improve from -0.54247. Patience: 25/50
2024-12-07 00:43:22.315299: train_loss -0.735
2024-12-07 00:43:22.316315: val_loss -0.5206
2024-12-07 00:43:22.317379: Pseudo dice [0.7351]
2024-12-07 00:43:22.318381: Epoch time: 89.56 s
2024-12-07 00:43:23.548931: 
2024-12-07 00:43:23.550977: Epoch 163
2024-12-07 00:43:23.552150: Current learning rate: 0.00852
2024-12-07 00:44:52.872660: Validation loss did not improve from -0.54247. Patience: 26/50
2024-12-07 00:44:52.874054: train_loss -0.7457
2024-12-07 00:44:52.875411: val_loss -0.455
2024-12-07 00:44:52.876338: Pseudo dice [0.7122]
2024-12-07 00:44:52.877387: Epoch time: 89.33 s
2024-12-07 00:44:54.097070: 
2024-12-07 00:44:54.099459: Epoch 164
2024-12-07 00:44:54.101068: Current learning rate: 0.00851
2024-12-07 00:46:26.965084: Validation loss did not improve from -0.54247. Patience: 27/50
2024-12-07 00:46:26.966491: train_loss -0.7388
2024-12-07 00:46:26.968227: val_loss -0.5017
2024-12-07 00:46:26.969097: Pseudo dice [0.7344]
2024-12-07 00:46:26.969829: Epoch time: 92.87 s
2024-12-07 00:46:28.526100: 
2024-12-07 00:46:28.528621: Epoch 165
2024-12-07 00:46:28.530031: Current learning rate: 0.0085
2024-12-07 00:47:59.157642: Validation loss did not improve from -0.54247. Patience: 28/50
2024-12-07 00:47:59.159126: train_loss -0.7409
2024-12-07 00:47:59.160283: val_loss -0.514
2024-12-07 00:47:59.161132: Pseudo dice [0.7338]
2024-12-07 00:47:59.162029: Epoch time: 90.63 s
2024-12-07 00:48:00.700943: 
2024-12-07 00:48:00.703779: Epoch 166
2024-12-07 00:48:00.705094: Current learning rate: 0.00849
2024-12-07 00:49:30.068550: Validation loss did not improve from -0.54247. Patience: 29/50
2024-12-07 00:49:30.069781: train_loss -0.7437
2024-12-07 00:49:30.071357: val_loss -0.4675
2024-12-07 00:49:30.072433: Pseudo dice [0.7158]
2024-12-07 00:49:30.073409: Epoch time: 89.37 s
2024-12-07 00:49:31.293451: 
2024-12-07 00:49:31.295403: Epoch 167
2024-12-07 00:49:31.297052: Current learning rate: 0.00848
2024-12-07 00:51:01.976060: Validation loss did not improve from -0.54247. Patience: 30/50
2024-12-07 00:51:01.992719: train_loss -0.7431
2024-12-07 00:51:01.994525: val_loss -0.4943
2024-12-07 00:51:01.995378: Pseudo dice [0.7171]
2024-12-07 00:51:01.996404: Epoch time: 90.7 s
2024-12-07 00:51:03.497005: 
2024-12-07 00:51:03.498968: Epoch 168
2024-12-07 00:51:03.500386: Current learning rate: 0.00847
2024-12-07 00:52:32.854062: Validation loss did not improve from -0.54247. Patience: 31/50
2024-12-07 00:52:32.855185: train_loss -0.7397
2024-12-07 00:52:32.856230: val_loss -0.4986
2024-12-07 00:52:32.857321: Pseudo dice [0.7319]
2024-12-07 00:52:32.858034: Epoch time: 89.36 s
2024-12-07 00:52:34.083731: 
2024-12-07 00:52:34.085837: Epoch 169
2024-12-07 00:52:34.086771: Current learning rate: 0.00847
2024-12-07 00:54:03.554246: Validation loss did not improve from -0.54247. Patience: 32/50
2024-12-07 00:54:03.555570: train_loss -0.7493
2024-12-07 00:54:03.556540: val_loss -0.5077
2024-12-07 00:54:03.557691: Pseudo dice [0.7243]
2024-12-07 00:54:03.558655: Epoch time: 89.47 s
2024-12-07 00:54:05.220622: 
2024-12-07 00:54:05.222733: Epoch 170
2024-12-07 00:54:05.223781: Current learning rate: 0.00846
2024-12-07 00:55:34.601064: Validation loss did not improve from -0.54247. Patience: 33/50
2024-12-07 00:55:34.602223: train_loss -0.756
2024-12-07 00:55:34.603100: val_loss -0.5219
2024-12-07 00:55:34.603904: Pseudo dice [0.7386]
2024-12-07 00:55:34.604694: Epoch time: 89.38 s
2024-12-07 00:55:35.817872: 
2024-12-07 00:55:35.819945: Epoch 171
2024-12-07 00:55:35.821526: Current learning rate: 0.00845
2024-12-07 00:57:05.384961: Validation loss did not improve from -0.54247. Patience: 34/50
2024-12-07 00:57:05.386287: train_loss -0.7491
2024-12-07 00:57:05.387403: val_loss -0.4471
2024-12-07 00:57:05.388688: Pseudo dice [0.6972]
2024-12-07 00:57:05.389364: Epoch time: 89.57 s
2024-12-07 00:57:06.634683: 
2024-12-07 00:57:06.636816: Epoch 172
2024-12-07 00:57:06.637700: Current learning rate: 0.00844
2024-12-07 00:58:35.911453: Validation loss did not improve from -0.54247. Patience: 35/50
2024-12-07 00:58:35.912699: train_loss -0.7487
2024-12-07 00:58:35.913935: val_loss -0.4718
2024-12-07 00:58:35.915047: Pseudo dice [0.7125]
2024-12-07 00:58:35.916059: Epoch time: 89.28 s
2024-12-07 00:58:37.109499: 
2024-12-07 00:58:37.112461: Epoch 173
2024-12-07 00:58:37.114038: Current learning rate: 0.00843
2024-12-07 01:00:06.607502: Validation loss did not improve from -0.54247. Patience: 36/50
2024-12-07 01:00:06.608689: train_loss -0.7512
2024-12-07 01:00:06.610016: val_loss -0.5097
2024-12-07 01:00:06.611016: Pseudo dice [0.7254]
2024-12-07 01:00:06.612055: Epoch time: 89.5 s
2024-12-07 01:00:07.836303: 
2024-12-07 01:00:07.838450: Epoch 174
2024-12-07 01:00:07.839766: Current learning rate: 0.00842
2024-12-07 01:01:37.228296: Validation loss did not improve from -0.54247. Patience: 37/50
2024-12-07 01:01:37.229871: train_loss -0.7581
2024-12-07 01:01:37.231601: val_loss -0.489
2024-12-07 01:01:37.233073: Pseudo dice [0.7172]
2024-12-07 01:01:37.234248: Epoch time: 89.39 s
2024-12-07 01:01:38.800188: 
2024-12-07 01:01:38.802409: Epoch 175
2024-12-07 01:01:38.804291: Current learning rate: 0.00841
2024-12-07 01:03:08.472418: Validation loss did not improve from -0.54247. Patience: 38/50
2024-12-07 01:03:08.473572: train_loss -0.7536
2024-12-07 01:03:08.474891: val_loss -0.52
2024-12-07 01:03:08.475782: Pseudo dice [0.7403]
2024-12-07 01:03:08.476665: Epoch time: 89.67 s
2024-12-07 01:03:10.781853: 
2024-12-07 01:03:10.783704: Epoch 176
2024-12-07 01:03:10.785055: Current learning rate: 0.0084
2024-12-07 01:04:40.393261: Validation loss did not improve from -0.54247. Patience: 39/50
2024-12-07 01:04:40.394836: train_loss -0.754
2024-12-07 01:04:40.395864: val_loss -0.5365
2024-12-07 01:04:40.396912: Pseudo dice [0.7458]
2024-12-07 01:04:40.397952: Epoch time: 89.61 s
2024-12-07 01:04:41.620272: 
2024-12-07 01:04:41.622488: Epoch 177
2024-12-07 01:04:41.623929: Current learning rate: 0.00839
2024-12-07 01:06:11.649935: Validation loss did not improve from -0.54247. Patience: 40/50
2024-12-07 01:06:11.651396: train_loss -0.7537
2024-12-07 01:06:11.653296: val_loss -0.5332
2024-12-07 01:06:11.654509: Pseudo dice [0.7365]
2024-12-07 01:06:11.655465: Epoch time: 90.03 s
2024-12-07 01:06:12.919386: 
2024-12-07 01:06:12.921692: Epoch 178
2024-12-07 01:06:12.922796: Current learning rate: 0.00838
2024-12-07 01:07:42.018861: Validation loss did not improve from -0.54247. Patience: 41/50
2024-12-07 01:07:42.020097: train_loss -0.7567
2024-12-07 01:07:42.020933: val_loss -0.5221
2024-12-07 01:07:42.021965: Pseudo dice [0.7371]
2024-12-07 01:07:42.023193: Epoch time: 89.1 s
2024-12-07 01:07:42.024276: Yayy! New best EMA pseudo Dice: 0.7283
2024-12-07 01:07:43.616837: 
2024-12-07 01:07:43.618953: Epoch 179
2024-12-07 01:07:43.620163: Current learning rate: 0.00837
2024-12-07 01:09:13.313336: Validation loss did not improve from -0.54247. Patience: 42/50
2024-12-07 01:09:13.314682: train_loss -0.7568
2024-12-07 01:09:13.315800: val_loss -0.5022
2024-12-07 01:09:13.316801: Pseudo dice [0.7212]
2024-12-07 01:09:13.317728: Epoch time: 89.7 s
2024-12-07 01:09:14.914020: 
2024-12-07 01:09:14.916033: Epoch 180
2024-12-07 01:09:14.917079: Current learning rate: 0.00836
2024-12-07 01:10:44.157842: Validation loss did not improve from -0.54247. Patience: 43/50
2024-12-07 01:10:44.158851: train_loss -0.7557
2024-12-07 01:10:44.159847: val_loss -0.5229
2024-12-07 01:10:44.160969: Pseudo dice [0.7312]
2024-12-07 01:10:44.162276: Epoch time: 89.25 s
2024-12-07 01:10:45.353958: 
2024-12-07 01:10:45.356133: Epoch 181
2024-12-07 01:10:45.357333: Current learning rate: 0.00836
2024-12-07 01:12:14.776276: Validation loss did not improve from -0.54247. Patience: 44/50
2024-12-07 01:12:14.777470: train_loss -0.749
2024-12-07 01:12:14.778542: val_loss -0.4776
2024-12-07 01:12:14.779516: Pseudo dice [0.7128]
2024-12-07 01:12:14.780478: Epoch time: 89.43 s
2024-12-07 01:12:15.977472: 
2024-12-07 01:12:15.979633: Epoch 182
2024-12-07 01:12:15.981022: Current learning rate: 0.00835
2024-12-07 01:13:45.494997: Validation loss did not improve from -0.54247. Patience: 45/50
2024-12-07 01:13:45.496115: train_loss -0.7592
2024-12-07 01:13:45.497344: val_loss -0.5058
2024-12-07 01:13:45.498288: Pseudo dice [0.7318]
2024-12-07 01:13:45.499287: Epoch time: 89.52 s
2024-12-07 01:13:46.721840: 
2024-12-07 01:13:46.723944: Epoch 183
2024-12-07 01:13:46.724767: Current learning rate: 0.00834
2024-12-07 01:15:16.178757: Validation loss did not improve from -0.54247. Patience: 46/50
2024-12-07 01:15:16.180060: train_loss -0.7605
2024-12-07 01:15:16.181505: val_loss -0.4988
2024-12-07 01:15:16.182728: Pseudo dice [0.7204]
2024-12-07 01:15:16.183876: Epoch time: 89.46 s
2024-12-07 01:15:17.390835: 
2024-12-07 01:15:17.393120: Epoch 184
2024-12-07 01:15:17.394519: Current learning rate: 0.00833
2024-12-07 01:16:46.390450: Validation loss did not improve from -0.54247. Patience: 47/50
2024-12-07 01:16:46.391999: train_loss -0.753
2024-12-07 01:16:46.393190: val_loss -0.4708
2024-12-07 01:16:46.394150: Pseudo dice [0.7124]
2024-12-07 01:16:46.395104: Epoch time: 89.0 s
2024-12-07 01:16:47.950218: 
2024-12-07 01:16:47.952134: Epoch 185
2024-12-07 01:16:47.953071: Current learning rate: 0.00832
2024-12-07 01:18:16.949676: Validation loss did not improve from -0.54247. Patience: 48/50
2024-12-07 01:18:16.950976: train_loss -0.7539
2024-12-07 01:18:16.952276: val_loss -0.5188
2024-12-07 01:18:16.952990: Pseudo dice [0.734]
2024-12-07 01:18:16.953810: Epoch time: 89.0 s
2024-12-07 01:18:18.160212: 
2024-12-07 01:18:18.162525: Epoch 186
2024-12-07 01:18:18.163825: Current learning rate: 0.00831
2024-12-07 01:19:47.319586: Validation loss did not improve from -0.54247. Patience: 49/50
2024-12-07 01:19:47.320836: train_loss -0.7476
2024-12-07 01:19:47.321805: val_loss -0.4784
2024-12-07 01:19:47.322553: Pseudo dice [0.7226]
2024-12-07 01:19:47.323218: Epoch time: 89.16 s
2024-12-07 01:19:48.869576: 
2024-12-07 01:19:48.871465: Epoch 187
2024-12-07 01:19:48.872552: Current learning rate: 0.0083
2024-12-07 01:21:17.923618: Validation loss did not improve from -0.54247. Patience: 50/50
2024-12-07 01:21:17.924951: train_loss -0.7546
2024-12-07 01:21:17.926388: val_loss -0.5072
2024-12-07 01:21:17.927238: Pseudo dice [0.7236]
2024-12-07 01:21:17.928291: Epoch time: 89.06 s
2024-12-07 01:21:19.128355: Patience reached. Stopping training.
2024-12-07 01:21:19.571087: Training done.
2024-12-07 01:21:19.768358: Using splits from existing split file: /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset307_Sohee_Calcium_OCT_CrossValidation/splits_final.json
2024-12-07 01:21:19.772605: The split file contains 5 splits.
2024-12-07 01:21:19.774597: Desired fold for training: 4
2024-12-07 01:21:19.776405: This split has 7 training and 1 validation cases.
2024-12-07 01:21:19.777643: predicting 101-045
2024-12-07 01:21:19.793713: 101-045, shape torch.Size([1, 375, 498, 498]), rank 0
2024-12-07 01:23:24.488431: Validation complete
2024-12-07 01:23:24.489570: Mean Validation Dice:  0.7282557215553579
