/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/bin/python
Starting training with CONFIG=3d_32x160x128_b10, DATASET_ID=307, TRAINER=nnUNetTrainerScaleAnalysis80
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/nnunetv2/training/nnUNetTrainer/nnUNetTrainer.py:169: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  self.grad_scaler = GradScaler() if self.device.type == 'cuda' else None

############################
INFO: You are using the old nnU-Net default plans. We have updated our recommendations. Please consider using those instead! Read more here: https://github.com/MIC-DKFZ/nnUNet/blob/master/documentation/resenc_presets.md
############################

Using device: cuda:0

#######################################################################
Please cite the following paper when using nnU-Net:
Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.
#######################################################################

2025-10-15 07:29:37.926158: Using torch.compile...
################### Loading pretrained weights from file  /nfs/erelab001/shared/Computational_Group/Naravich/datasets/nnUNet_Datasets/../ModelGenesisOutputs/ModelGenesisNNUNetPretrainingV2_noNorm_correct_orientation/Converted_nnUNet_Genesis_OCT_Best.pt ###################
Below is the list of overlapping blocks in pretrained model and nnUNet architecture:
encoder.stages.0.0.convs.0.conv.weight shape torch.Size([32, 1, 3, 3, 3])
encoder.stages.0.0.convs.0.conv.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.norm.weight shape torch.Size([32])
encoder.stages.0.0.convs.0.norm.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.0.weight shape torch.Size([32, 1, 3, 3, 3])
encoder.stages.0.0.convs.0.all_modules.0.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.1.weight shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.1.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
encoder.stages.0.0.convs.1.conv.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.norm.weight shape torch.Size([32])
encoder.stages.0.0.convs.1.norm.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
encoder.stages.0.0.convs.1.all_modules.0.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.1.weight shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.1.bias shape torch.Size([32])
encoder.stages.1.0.convs.0.conv.weight shape torch.Size([64, 32, 3, 3, 3])
encoder.stages.1.0.convs.0.conv.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.norm.weight shape torch.Size([64])
encoder.stages.1.0.convs.0.norm.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.0.weight shape torch.Size([64, 32, 3, 3, 3])
encoder.stages.1.0.convs.0.all_modules.0.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.1.weight shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.1.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
encoder.stages.1.0.convs.1.conv.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.norm.weight shape torch.Size([64])
encoder.stages.1.0.convs.1.norm.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
encoder.stages.1.0.convs.1.all_modules.0.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.1.weight shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.1.bias shape torch.Size([64])
encoder.stages.2.0.convs.0.conv.weight shape torch.Size([128, 64, 3, 3, 3])
encoder.stages.2.0.convs.0.conv.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.norm.weight shape torch.Size([128])
encoder.stages.2.0.convs.0.norm.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.0.weight shape torch.Size([128, 64, 3, 3, 3])
encoder.stages.2.0.convs.0.all_modules.0.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.1.weight shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.1.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
encoder.stages.2.0.convs.1.conv.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.norm.weight shape torch.Size([128])
encoder.stages.2.0.convs.1.norm.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
encoder.stages.2.0.convs.1.all_modules.0.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.1.weight shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.1.bias shape torch.Size([128])
encoder.stages.3.0.convs.0.conv.weight shape torch.Size([256, 128, 3, 3, 3])
encoder.stages.3.0.convs.0.conv.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.norm.weight shape torch.Size([256])
encoder.stages.3.0.convs.0.norm.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.0.weight shape torch.Size([256, 128, 3, 3, 3])
encoder.stages.3.0.convs.0.all_modules.0.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.1.weight shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.1.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
encoder.stages.3.0.convs.1.conv.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.norm.weight shape torch.Size([256])
encoder.stages.3.0.convs.1.norm.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
encoder.stages.3.0.convs.1.all_modules.0.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.1.weight shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.1.bias shape torch.Size([256])
encoder.stages.4.0.convs.0.conv.weight shape torch.Size([320, 256, 3, 3, 3])
encoder.stages.4.0.convs.0.conv.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.norm.weight shape torch.Size([320])
encoder.stages.4.0.convs.0.norm.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.0.weight shape torch.Size([320, 256, 3, 3, 3])
encoder.stages.4.0.convs.0.all_modules.0.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.1.weight shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.1.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.4.0.convs.1.conv.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.norm.weight shape torch.Size([320])
encoder.stages.4.0.convs.1.norm.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.4.0.convs.1.all_modules.0.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.1.weight shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.1.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.0.conv.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.norm.weight shape torch.Size([320])
encoder.stages.5.0.convs.0.norm.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.0.all_modules.0.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.1.weight shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.1.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.1.conv.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.norm.weight shape torch.Size([320])
encoder.stages.5.0.convs.1.norm.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.1.all_modules.0.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.1.weight shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.0.0.convs.0.conv.weight shape torch.Size([32, 1, 3, 3, 3])
decoder.encoder.stages.0.0.convs.0.conv.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.norm.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.norm.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.0.weight shape torch.Size([32, 1, 3, 3, 3])
decoder.encoder.stages.0.0.convs.0.all_modules.0.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.1.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.1.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.encoder.stages.0.0.convs.1.conv.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.norm.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.norm.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.encoder.stages.0.0.convs.1.all_modules.0.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.1.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.1.bias shape torch.Size([32])
decoder.encoder.stages.1.0.convs.0.conv.weight shape torch.Size([64, 32, 3, 3, 3])
decoder.encoder.stages.1.0.convs.0.conv.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.norm.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.norm.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.0.weight shape torch.Size([64, 32, 3, 3, 3])
decoder.encoder.stages.1.0.convs.0.all_modules.0.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.1.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.1.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.encoder.stages.1.0.convs.1.conv.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.norm.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.norm.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.encoder.stages.1.0.convs.1.all_modules.0.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.1.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.1.bias shape torch.Size([64])
decoder.encoder.stages.2.0.convs.0.conv.weight shape torch.Size([128, 64, 3, 3, 3])
decoder.encoder.stages.2.0.convs.0.conv.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.norm.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.norm.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.0.weight shape torch.Size([128, 64, 3, 3, 3])
decoder.encoder.stages.2.0.convs.0.all_modules.0.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.1.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.1.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.encoder.stages.2.0.convs.1.conv.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.norm.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.norm.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.encoder.stages.2.0.convs.1.all_modules.0.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.1.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.1.bias shape torch.Size([128])
decoder.encoder.stages.3.0.convs.0.conv.weight shape torch.Size([256, 128, 3, 3, 3])
decoder.encoder.stages.3.0.convs.0.conv.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.norm.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.norm.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.0.weight shape torch.Size([256, 128, 3, 3, 3])
decoder.encoder.stages.3.0.convs.0.all_modules.0.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.1.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.1.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.encoder.stages.3.0.convs.1.conv.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.norm.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.norm.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.encoder.stages.3.0.convs.1.all_modules.0.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.1.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.1.bias shape torch.Size([256])
decoder.encoder.stages.4.0.convs.0.conv.weight shape torch.Size([320, 256, 3, 3, 3])
decoder.encoder.stages.4.0.convs.0.conv.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.norm.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.norm.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.0.weight shape torch.Size([320, 256, 3, 3, 3])
decoder.encoder.stages.4.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.4.0.convs.1.conv.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.norm.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.norm.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.4.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.0.conv.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.norm.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.norm.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.1.conv.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.norm.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.norm.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.stages.0.convs.0.conv.weight shape torch.Size([320, 640, 3, 3, 3])
decoder.stages.0.convs.0.conv.bias shape torch.Size([320])
decoder.stages.0.convs.0.norm.weight shape torch.Size([320])
decoder.stages.0.convs.0.norm.bias shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.0.weight shape torch.Size([320, 640, 3, 3, 3])
decoder.stages.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.stages.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.stages.0.convs.1.conv.bias shape torch.Size([320])
decoder.stages.0.convs.1.norm.weight shape torch.Size([320])
decoder.stages.0.convs.1.norm.bias shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.stages.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.stages.1.convs.0.conv.weight shape torch.Size([256, 512, 3, 3, 3])
decoder.stages.1.convs.0.conv.bias shape torch.Size([256])
decoder.stages.1.convs.0.norm.weight shape torch.Size([256])
decoder.stages.1.convs.0.norm.bias shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.0.weight shape torch.Size([256, 512, 3, 3, 3])
decoder.stages.1.convs.0.all_modules.0.bias shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.1.weight shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.1.bias shape torch.Size([256])
decoder.stages.1.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.stages.1.convs.1.conv.bias shape torch.Size([256])
decoder.stages.1.convs.1.norm.weight shape torch.Size([256])
decoder.stages.1.convs.1.norm.bias shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.stages.1.convs.1.all_modules.0.bias shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.1.weight shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.1.bias shape torch.Size([256])
decoder.stages.2.convs.0.conv.weight shape torch.Size([128, 256, 3, 3, 3])
decoder.stages.2.convs.0.conv.bias shape torch.Size([128])
decoder.stages.2.convs.0.norm.weight shape torch.Size([128])
decoder.stages.2.convs.0.norm.bias shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.0.weight shape torch.Size([128, 256, 3, 3, 3])
decoder.stages.2.convs.0.all_modules.0.bias shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.1.weight shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.1.bias shape torch.Size([128])
decoder.stages.2.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.stages.2.convs.1.conv.bias shape torch.Size([128])
decoder.stages.2.convs.1.norm.weight shape torch.Size([128])
decoder.stages.2.convs.1.norm.bias shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.stages.2.convs.1.all_modules.0.bias shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.1.weight shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.1.bias shape torch.Size([128])
decoder.stages.3.convs.0.conv.weight shape torch.Size([64, 128, 3, 3, 3])
decoder.stages.3.convs.0.conv.bias shape torch.Size([64])
decoder.stages.3.convs.0.norm.weight shape torch.Size([64])
decoder.stages.3.convs.0.norm.bias shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.0.weight shape torch.Size([64, 128, 3, 3, 3])
decoder.stages.3.convs.0.all_modules.0.bias shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.1.weight shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.1.bias shape torch.Size([64])
decoder.stages.3.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.stages.3.convs.1.conv.bias shape torch.Size([64])
decoder.stages.3.convs.1.norm.weight shape torch.Size([64])
decoder.stages.3.convs.1.norm.bias shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.stages.3.convs.1.all_modules.0.bias shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.1.weight shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.1.bias shape torch.Size([64])
decoder.stages.4.convs.0.conv.weight shape torch.Size([32, 64, 3, 3, 3])
decoder.stages.4.convs.0.conv.bias shape torch.Size([32])
decoder.stages.4.convs.0.norm.weight shape torch.Size([32])
decoder.stages.4.convs.0.norm.bias shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.0.weight shape torch.Size([32, 64, 3, 3, 3])
decoder.stages.4.convs.0.all_modules.0.bias shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.1.weight shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.1.bias shape torch.Size([32])
decoder.stages.4.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.stages.4.convs.1.conv.bias shape torch.Size([32])
decoder.stages.4.convs.1.norm.weight shape torch.Size([32])
decoder.stages.4.convs.1.norm.bias shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.stages.4.convs.1.all_modules.0.bias shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.1.weight shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.1.bias shape torch.Size([32])
decoder.transpconvs.0.weight shape torch.Size([320, 320, 1, 2, 2])
decoder.transpconvs.0.bias shape torch.Size([320])
decoder.transpconvs.1.weight shape torch.Size([320, 256, 2, 2, 2])
decoder.transpconvs.1.bias shape torch.Size([256])
decoder.transpconvs.2.weight shape torch.Size([256, 128, 2, 2, 2])
decoder.transpconvs.2.bias shape torch.Size([128])
decoder.transpconvs.3.weight shape torch.Size([128, 64, 2, 2, 2])
decoder.transpconvs.3.bias shape torch.Size([64])
decoder.transpconvs.4.weight shape torch.Size([64, 32, 2, 2, 2])
decoder.transpconvs.4.bias shape torch.Size([32])
################### Done ###################
2025-10-15 07:29:39.243479: do_dummy_2d_data_aug: True
2025-10-15 07:29:39.244028: Using splits from existing split file: /nfs/erelab001/shared/Computational_Group/Naravich/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset307_Sohee_Calcium_OCT_CrossValidation/splits_final_80.json
2025-10-15 07:29:39.244495: The split file contains 5 splits.
2025-10-15 07:29:39.244639: Desired fold for training: 2
2025-10-15 07:29:39.244840: This split has 6 training and 3 validation cases.
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/fft_conv_pytorch/fft_conv.py:139: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:306.)
  output = output[crop_slices].contiguous()
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/fft_conv_pytorch/fft_conv.py:139: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:306.)
  output = output[crop_slices].contiguous()
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/fft_conv_pytorch/fft_conv.py:139: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:306.)
  output = output[crop_slices].contiguous()
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/fft_conv_pytorch/fft_conv.py:139: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:306.)
  output = output[crop_slices].contiguous()
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/fft_conv_pytorch/fft_conv.py:139: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:306.)
  output = output[crop_slices].contiguous()
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/fft_conv_pytorch/fft_conv.py:139: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:306.)
  output = output[crop_slices].contiguous()
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/fft_conv_pytorch/fft_conv.py:139: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:306.)
  output = output[crop_slices].contiguous()
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/fft_conv_pytorch/fft_conv.py:139: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:306.)
  output = output[crop_slices].contiguous()
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/fft_conv_pytorch/fft_conv.py:139: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:306.)
  output = output[crop_slices].contiguous()
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/fft_conv_pytorch/fft_conv.py:139: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:306.)
  output = output[crop_slices].contiguous()
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/fft_conv_pytorch/fft_conv.py:139: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:306.)
  output = output[crop_slices].contiguous()
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/fft_conv_pytorch/fft_conv.py:139: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:306.)
  output = output[crop_slices].contiguous()
using pin_memory on device 0
using pin_memory on device 0

This is the configuration used by this training:
Configuration name: 3d_32x160x128_b10
 {'data_identifier': 'nnUNetPlans_3d_fullres', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 10, 'patch_size': [32, 160, 128], 'median_image_size_in_voxels': [375.0, 498.0, 498.0], 'spacing': [1.0, 1.0, 1.0], 'normalization_schemes': ['ZScoreNormalization'], 'use_mask_for_norm': [False], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'architecture': {'network_class_name': 'dynamic_network_architectures.architectures.unet.PlainConvUNet', 'arch_kwargs': {'n_stages': 6, 'features_per_stage': [32, 64, 128, 256, 320, 320], 'conv_op': 'torch.nn.modules.conv.Conv3d', 'kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]], 'strides': [[1, 1, 1], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [1, 2, 2]], 'n_conv_per_stage': [2, 2, 2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2, 2, 2], 'conv_bias': True, 'norm_op': 'torch.nn.modules.instancenorm.InstanceNorm3d', 'norm_op_kwargs': {'eps': 1e-05, 'affine': True}, 'dropout_op': None, 'dropout_op_kwargs': None, 'nonlin': 'torch.nn.LeakyReLU', 'nonlin_kwargs': {'inplace': True}, 'deep_supervision': True}, '_kw_requires_import': ['conv_op', 'norm_op', 'dropout_op', 'nonlin']}, 'batch_dice': True, 'inherits_from': '3d_fullres'} 

These are the global plan.json settings:
 {'dataset_name': 'Dataset307_Sohee_Calcium_OCT_CrossValidation', 'plans_name': 'nnUNetPlans', 'original_median_spacing_after_transp': [1.0, 1.0, 1.0], 'original_median_shape_after_transp': [375, 498, 498], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 0.4977976381778717, 'mean': 0.13507525622844696, 'median': 0.09599608182907104, 'min': 0.0, 'percentile_00_5': 0.014754901640117168, 'percentile_99_5': 0.4977976381778717, 'std': 0.12025152146816254}}} 

2025-10-15 07:29:42.188210: unpacking dataset...
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/torch/_inductor/lowering.py:7095: UserWarning: 
Online softmax is disabled on the fly since Inductor decides to
split the reduction. Cut an issue to PyTorch if this is an
important use case and you want to speed it up with online
softmax.

  warnings.warn(
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/torch/_inductor/lowering.py:7095: UserWarning: 
Online softmax is disabled on the fly since Inductor decides to
split the reduction. Cut an issue to PyTorch if this is an
important use case and you want to speed it up with online
softmax.

  warnings.warn(
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/torch/_inductor/lowering.py:7095: UserWarning: 
Online softmax is disabled on the fly since Inductor decides to
split the reduction. Cut an issue to PyTorch if this is an
important use case and you want to speed it up with online
softmax.

  warnings.warn(
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/torch/_inductor/lowering.py:7095: UserWarning: 
Online softmax is disabled on the fly since Inductor decides to
split the reduction. Cut an issue to PyTorch if this is an
important use case and you want to speed it up with online
softmax.

  warnings.warn(
2025-10-15 07:29:47.760869: unpacking done...
2025-10-15 07:29:47.763073: Unable to plot network architecture: nnUNet_compile is enabled!
2025-10-15 07:29:47.767831: 
2025-10-15 07:29:47.768047: Epoch 0
2025-10-15 07:29:47.768331: Current learning rate: 0.01
2025-10-15 07:31:07.180797: Validation loss improved from 1000.00000 to -0.19727! Patience: 0/50
2025-10-15 07:31:07.181421: train_loss -0.1131
2025-10-15 07:31:07.181617: val_loss -0.1973
2025-10-15 07:31:07.181798: Pseudo dice [np.float32(0.5542)]
2025-10-15 07:31:07.181990: Epoch time: 79.41 s
2025-10-15 07:31:07.182157: Yayy! New best EMA pseudo Dice: 0.5541999936103821
2025-10-15 07:31:08.097445: 
2025-10-15 07:31:08.097777: Epoch 1
2025-10-15 07:31:08.097962: Current learning rate: 0.00994
2025-10-15 07:31:54.032122: Validation loss improved from -0.19727 to -0.28544! Patience: 0/50
2025-10-15 07:31:54.032756: train_loss -0.2919
2025-10-15 07:31:54.032936: val_loss -0.2854
2025-10-15 07:31:54.033159: Pseudo dice [np.float32(0.5964)]
2025-10-15 07:31:54.033307: Epoch time: 45.94 s
2025-10-15 07:31:54.033518: Yayy! New best EMA pseudo Dice: 0.5583999752998352
2025-10-15 07:31:55.087115: 
2025-10-15 07:31:55.087357: Epoch 2
2025-10-15 07:31:55.087625: Current learning rate: 0.00988
2025-10-15 07:32:41.043682: Validation loss improved from -0.28544 to -0.31465! Patience: 0/50
2025-10-15 07:32:41.044298: train_loss -0.3589
2025-10-15 07:32:41.044499: val_loss -0.3147
2025-10-15 07:32:41.044635: Pseudo dice [np.float32(0.6109)]
2025-10-15 07:32:41.044770: Epoch time: 45.96 s
2025-10-15 07:32:41.044900: Yayy! New best EMA pseudo Dice: 0.5637000203132629
2025-10-15 07:32:42.137841: 
2025-10-15 07:32:42.138211: Epoch 3
2025-10-15 07:32:42.138391: Current learning rate: 0.00982
2025-10-15 07:33:28.084383: Validation loss did not improve from -0.31465. Patience: 1/50
2025-10-15 07:33:28.085013: train_loss -0.3925
2025-10-15 07:33:28.085443: val_loss -0.3113
2025-10-15 07:33:28.085818: Pseudo dice [np.float32(0.6222)]
2025-10-15 07:33:28.086198: Epoch time: 45.95 s
2025-10-15 07:33:28.086562: Yayy! New best EMA pseudo Dice: 0.5695000290870667
2025-10-15 07:33:29.149252: 
2025-10-15 07:33:29.149571: Epoch 4
2025-10-15 07:33:29.149764: Current learning rate: 0.00976
2025-10-15 07:34:15.104850: Validation loss improved from -0.31465 to -0.35422! Patience: 1/50
2025-10-15 07:34:15.105550: train_loss -0.4208
2025-10-15 07:34:15.105804: val_loss -0.3542
2025-10-15 07:34:15.106077: Pseudo dice [np.float32(0.6308)]
2025-10-15 07:34:15.106433: Epoch time: 45.96 s
2025-10-15 07:34:15.515924: Yayy! New best EMA pseudo Dice: 0.5756999850273132
2025-10-15 07:34:16.573785: 
2025-10-15 07:34:16.574169: Epoch 5
2025-10-15 07:34:16.574448: Current learning rate: 0.0097
2025-10-15 07:35:02.495383: Validation loss improved from -0.35422 to -0.36957! Patience: 0/50
2025-10-15 07:35:02.495802: train_loss -0.4276
2025-10-15 07:35:02.496018: val_loss -0.3696
2025-10-15 07:35:02.496215: Pseudo dice [np.float32(0.6419)]
2025-10-15 07:35:02.496486: Epoch time: 45.92 s
2025-10-15 07:35:02.496720: Yayy! New best EMA pseudo Dice: 0.5823000073432922
2025-10-15 07:35:03.560269: 
2025-10-15 07:35:03.560570: Epoch 6
2025-10-15 07:35:03.560773: Current learning rate: 0.00964
2025-10-15 07:35:49.483369: Validation loss did not improve from -0.36957. Patience: 1/50
2025-10-15 07:35:49.484134: train_loss -0.4603
2025-10-15 07:35:49.484309: val_loss -0.3528
2025-10-15 07:35:49.484444: Pseudo dice [np.float32(0.6405)]
2025-10-15 07:35:49.484614: Epoch time: 45.92 s
2025-10-15 07:35:49.484748: Yayy! New best EMA pseudo Dice: 0.588100016117096
2025-10-15 07:35:50.560520: 
2025-10-15 07:35:50.560863: Epoch 7
2025-10-15 07:35:50.561087: Current learning rate: 0.00958
2025-10-15 07:36:36.451018: Validation loss did not improve from -0.36957. Patience: 2/50
2025-10-15 07:36:36.451519: train_loss -0.4729
2025-10-15 07:36:36.451751: val_loss -0.3586
2025-10-15 07:36:36.451961: Pseudo dice [np.float32(0.6441)]
2025-10-15 07:36:36.452177: Epoch time: 45.89 s
2025-10-15 07:36:36.452374: Yayy! New best EMA pseudo Dice: 0.5936999917030334
2025-10-15 07:36:37.520548: 
2025-10-15 07:36:37.521236: Epoch 8
2025-10-15 07:36:37.521674: Current learning rate: 0.00952
2025-10-15 07:37:23.490766: Validation loss improved from -0.36957 to -0.41409! Patience: 2/50
2025-10-15 07:37:23.491616: train_loss -0.4946
2025-10-15 07:37:23.491872: val_loss -0.4141
2025-10-15 07:37:23.492129: Pseudo dice [np.float32(0.6687)]
2025-10-15 07:37:23.492360: Epoch time: 45.97 s
2025-10-15 07:37:23.492577: Yayy! New best EMA pseudo Dice: 0.6011999845504761
2025-10-15 07:37:24.565127: 
2025-10-15 07:37:24.565505: Epoch 9
2025-10-15 07:37:24.565762: Current learning rate: 0.00946
2025-10-15 07:38:10.521945: Validation loss did not improve from -0.41409. Patience: 1/50
2025-10-15 07:38:10.522593: train_loss -0.5162
2025-10-15 07:38:10.522972: val_loss -0.3576
2025-10-15 07:38:10.523360: Pseudo dice [np.float32(0.6293)]
2025-10-15 07:38:10.523716: Epoch time: 45.96 s
2025-10-15 07:38:10.971848: Yayy! New best EMA pseudo Dice: 0.6039999723434448
2025-10-15 07:38:12.035854: 
2025-10-15 07:38:12.036475: Epoch 10
2025-10-15 07:38:12.036941: Current learning rate: 0.0094
2025-10-15 07:38:57.938446: Validation loss did not improve from -0.41409. Patience: 2/50
2025-10-15 07:38:57.939796: train_loss -0.5071
2025-10-15 07:38:57.940207: val_loss -0.3767
2025-10-15 07:38:57.940590: Pseudo dice [np.float32(0.6523)]
2025-10-15 07:38:57.940991: Epoch time: 45.9 s
2025-10-15 07:38:57.941458: Yayy! New best EMA pseudo Dice: 0.6087999939918518
2025-10-15 07:38:59.032080: 
2025-10-15 07:38:59.032630: Epoch 11
2025-10-15 07:38:59.033055: Current learning rate: 0.00934
2025-10-15 07:39:44.984907: Validation loss did not improve from -0.41409. Patience: 3/50
2025-10-15 07:39:44.985398: train_loss -0.531
2025-10-15 07:39:44.985577: val_loss -0.3528
2025-10-15 07:39:44.985722: Pseudo dice [np.float32(0.6496)]
2025-10-15 07:39:44.985881: Epoch time: 45.95 s
2025-10-15 07:39:44.986014: Yayy! New best EMA pseudo Dice: 0.6129000186920166
2025-10-15 07:39:46.048986: 
2025-10-15 07:39:46.049287: Epoch 12
2025-10-15 07:39:46.049495: Current learning rate: 0.00928
2025-10-15 07:40:31.992716: Validation loss improved from -0.41409 to -0.42325! Patience: 3/50
2025-10-15 07:40:31.993381: train_loss -0.5298
2025-10-15 07:40:31.993596: val_loss -0.4232
2025-10-15 07:40:31.993761: Pseudo dice [np.float32(0.6694)]
2025-10-15 07:40:31.993933: Epoch time: 45.95 s
2025-10-15 07:40:31.994099: Yayy! New best EMA pseudo Dice: 0.6186000108718872
2025-10-15 07:40:33.623613: 
2025-10-15 07:40:33.624022: Epoch 13
2025-10-15 07:40:33.624326: Current learning rate: 0.00922
2025-10-15 07:41:19.693445: Validation loss improved from -0.42325 to -0.43783! Patience: 0/50
2025-10-15 07:41:19.693902: train_loss -0.5473
2025-10-15 07:41:19.694090: val_loss -0.4378
2025-10-15 07:41:19.694271: Pseudo dice [np.float32(0.6937)]
2025-10-15 07:41:19.694428: Epoch time: 46.07 s
2025-10-15 07:41:19.694600: Yayy! New best EMA pseudo Dice: 0.6261000037193298
2025-10-15 07:41:20.795809: 
2025-10-15 07:41:20.796118: Epoch 14
2025-10-15 07:41:20.796318: Current learning rate: 0.00916
2025-10-15 07:42:06.723228: Validation loss did not improve from -0.43783. Patience: 1/50
2025-10-15 07:42:06.723828: train_loss -0.564
2025-10-15 07:42:06.724022: val_loss -0.4081
2025-10-15 07:42:06.724200: Pseudo dice [np.float32(0.6725)]
2025-10-15 07:42:06.724446: Epoch time: 45.93 s
2025-10-15 07:42:07.158260: Yayy! New best EMA pseudo Dice: 0.6306999921798706
2025-10-15 07:42:08.212082: 
2025-10-15 07:42:08.212373: Epoch 15
2025-10-15 07:42:08.212625: Current learning rate: 0.0091
2025-10-15 07:42:54.189770: Validation loss improved from -0.43783 to -0.44790! Patience: 1/50
2025-10-15 07:42:54.190247: train_loss -0.5555
2025-10-15 07:42:54.190430: val_loss -0.4479
2025-10-15 07:42:54.190573: Pseudo dice [np.float32(0.681)]
2025-10-15 07:42:54.190746: Epoch time: 45.98 s
2025-10-15 07:42:54.190895: Yayy! New best EMA pseudo Dice: 0.6358000040054321
2025-10-15 07:42:55.268165: 
2025-10-15 07:42:55.268508: Epoch 16
2025-10-15 07:42:55.268698: Current learning rate: 0.00903
2025-10-15 07:43:41.328749: Validation loss did not improve from -0.44790. Patience: 1/50
2025-10-15 07:43:41.329490: train_loss -0.5737
2025-10-15 07:43:41.329705: val_loss -0.4363
2025-10-15 07:43:41.329856: Pseudo dice [np.float32(0.6803)]
2025-10-15 07:43:41.330005: Epoch time: 46.06 s
2025-10-15 07:43:41.330137: Yayy! New best EMA pseudo Dice: 0.6402000188827515
2025-10-15 07:43:42.442396: 
2025-10-15 07:43:42.442768: Epoch 17
2025-10-15 07:43:42.443025: Current learning rate: 0.00897
2025-10-15 07:44:28.502999: Validation loss did not improve from -0.44790. Patience: 2/50
2025-10-15 07:44:28.503427: train_loss -0.5714
2025-10-15 07:44:28.503604: val_loss -0.4476
2025-10-15 07:44:28.503730: Pseudo dice [np.float32(0.6814)]
2025-10-15 07:44:28.503871: Epoch time: 46.06 s
2025-10-15 07:44:28.503992: Yayy! New best EMA pseudo Dice: 0.6442999839782715
2025-10-15 07:44:29.601243: 
2025-10-15 07:44:29.601532: Epoch 18
2025-10-15 07:44:29.601742: Current learning rate: 0.00891
2025-10-15 07:45:15.627304: Validation loss improved from -0.44790 to -0.45875! Patience: 2/50
2025-10-15 07:45:15.627950: train_loss -0.5794
2025-10-15 07:45:15.628143: val_loss -0.4587
2025-10-15 07:45:15.628291: Pseudo dice [np.float32(0.6927)]
2025-10-15 07:45:15.628449: Epoch time: 46.03 s
2025-10-15 07:45:15.628611: Yayy! New best EMA pseudo Dice: 0.6492000222206116
2025-10-15 07:45:16.719067: 
2025-10-15 07:45:16.719386: Epoch 19
2025-10-15 07:45:16.719587: Current learning rate: 0.00885
2025-10-15 07:46:02.746371: Validation loss did not improve from -0.45875. Patience: 1/50
2025-10-15 07:46:02.746890: train_loss -0.5753
2025-10-15 07:46:02.747081: val_loss -0.4539
2025-10-15 07:46:02.747205: Pseudo dice [np.float32(0.6885)]
2025-10-15 07:46:02.747381: Epoch time: 46.03 s
2025-10-15 07:46:03.190558: Yayy! New best EMA pseudo Dice: 0.6531000137329102
2025-10-15 07:46:04.274480: 
2025-10-15 07:46:04.274767: Epoch 20
2025-10-15 07:46:04.274949: Current learning rate: 0.00879
2025-10-15 07:46:50.279545: Validation loss improved from -0.45875 to -0.45969! Patience: 1/50
2025-10-15 07:46:50.280176: train_loss -0.5832
2025-10-15 07:46:50.280329: val_loss -0.4597
2025-10-15 07:46:50.280455: Pseudo dice [np.float32(0.7031)]
2025-10-15 07:46:50.280607: Epoch time: 46.01 s
2025-10-15 07:46:50.280756: Yayy! New best EMA pseudo Dice: 0.6581000089645386
2025-10-15 07:46:51.376068: 
2025-10-15 07:46:51.376501: Epoch 21
2025-10-15 07:46:51.376771: Current learning rate: 0.00873
2025-10-15 07:47:37.389792: Validation loss did not improve from -0.45969. Patience: 1/50
2025-10-15 07:47:37.390291: train_loss -0.5894
2025-10-15 07:47:37.390481: val_loss -0.42
2025-10-15 07:47:37.390621: Pseudo dice [np.float32(0.6739)]
2025-10-15 07:47:37.390789: Epoch time: 46.01 s
2025-10-15 07:47:37.390945: Yayy! New best EMA pseudo Dice: 0.6596999764442444
2025-10-15 07:47:38.457214: 
2025-10-15 07:47:38.457611: Epoch 22
2025-10-15 07:47:38.457792: Current learning rate: 0.00867
2025-10-15 07:48:24.478780: Validation loss improved from -0.45969 to -0.48766! Patience: 1/50
2025-10-15 07:48:24.479529: train_loss -0.5967
2025-10-15 07:48:24.479779: val_loss -0.4877
2025-10-15 07:48:24.480050: Pseudo dice [np.float32(0.7066)]
2025-10-15 07:48:24.480294: Epoch time: 46.02 s
2025-10-15 07:48:24.480510: Yayy! New best EMA pseudo Dice: 0.6643999814987183
2025-10-15 07:48:25.545771: 
2025-10-15 07:48:25.546152: Epoch 23
2025-10-15 07:48:25.546509: Current learning rate: 0.00861
2025-10-15 07:49:11.549948: Validation loss improved from -0.48766 to -0.48805! Patience: 0/50
2025-10-15 07:49:11.550322: train_loss -0.6056
2025-10-15 07:49:11.550546: val_loss -0.4881
2025-10-15 07:49:11.550823: Pseudo dice [np.float32(0.707)]
2025-10-15 07:49:11.550963: Epoch time: 46.01 s
2025-10-15 07:49:11.551211: Yayy! New best EMA pseudo Dice: 0.6686000227928162
2025-10-15 07:49:12.613204: 
2025-10-15 07:49:12.613572: Epoch 24
2025-10-15 07:49:12.613891: Current learning rate: 0.00855
2025-10-15 07:49:58.616773: Validation loss improved from -0.48805 to -0.49422! Patience: 0/50
2025-10-15 07:49:58.617509: train_loss -0.6113
2025-10-15 07:49:58.617738: val_loss -0.4942
2025-10-15 07:49:58.617940: Pseudo dice [np.float32(0.7108)]
2025-10-15 07:49:58.618143: Epoch time: 46.01 s
2025-10-15 07:49:59.066116: Yayy! New best EMA pseudo Dice: 0.6729000210762024
2025-10-15 07:50:00.143199: 
2025-10-15 07:50:00.143506: Epoch 25
2025-10-15 07:50:00.143724: Current learning rate: 0.00849
2025-10-15 07:50:46.141919: Validation loss did not improve from -0.49422. Patience: 1/50
2025-10-15 07:50:46.142357: train_loss -0.6136
2025-10-15 07:50:46.142531: val_loss -0.4664
2025-10-15 07:50:46.142661: Pseudo dice [np.float32(0.708)]
2025-10-15 07:50:46.142815: Epoch time: 46.0 s
2025-10-15 07:50:46.142958: Yayy! New best EMA pseudo Dice: 0.6764000058174133
2025-10-15 07:50:47.236227: 
2025-10-15 07:50:47.236535: Epoch 26
2025-10-15 07:50:47.236713: Current learning rate: 0.00843
2025-10-15 07:51:33.257559: Validation loss did not improve from -0.49422. Patience: 2/50
2025-10-15 07:51:33.258178: train_loss -0.6245
2025-10-15 07:51:33.258342: val_loss -0.4732
2025-10-15 07:51:33.258465: Pseudo dice [np.float32(0.7018)]
2025-10-15 07:51:33.258605: Epoch time: 46.02 s
2025-10-15 07:51:33.258724: Yayy! New best EMA pseudo Dice: 0.6789000034332275
2025-10-15 07:51:34.359097: 
2025-10-15 07:51:34.359412: Epoch 27
2025-10-15 07:51:34.359609: Current learning rate: 0.00836
2025-10-15 07:52:20.416593: Validation loss improved from -0.49422 to -0.50797! Patience: 2/50
2025-10-15 07:52:20.417018: train_loss -0.6227
2025-10-15 07:52:20.417171: val_loss -0.508
2025-10-15 07:52:20.417297: Pseudo dice [np.float32(0.7056)]
2025-10-15 07:52:20.417440: Epoch time: 46.06 s
2025-10-15 07:52:20.417582: Yayy! New best EMA pseudo Dice: 0.6815999746322632
2025-10-15 07:52:21.992253: 
2025-10-15 07:52:21.992527: Epoch 28
2025-10-15 07:52:21.992705: Current learning rate: 0.0083
2025-10-15 07:53:08.096314: Validation loss did not improve from -0.50797. Patience: 1/50
2025-10-15 07:53:08.096895: train_loss -0.6316
2025-10-15 07:53:08.097059: val_loss -0.4818
2025-10-15 07:53:08.097186: Pseudo dice [np.float32(0.7055)]
2025-10-15 07:53:08.097321: Epoch time: 46.11 s
2025-10-15 07:53:08.097440: Yayy! New best EMA pseudo Dice: 0.6840000152587891
2025-10-15 07:53:09.175710: 
2025-10-15 07:53:09.176014: Epoch 29
2025-10-15 07:53:09.176259: Current learning rate: 0.00824
2025-10-15 07:53:55.253626: Validation loss did not improve from -0.50797. Patience: 2/50
2025-10-15 07:53:55.254091: train_loss -0.6211
2025-10-15 07:53:55.254262: val_loss -0.4418
2025-10-15 07:53:55.254397: Pseudo dice [np.float32(0.6888)]
2025-10-15 07:53:55.254551: Epoch time: 46.08 s
2025-10-15 07:53:55.689528: Yayy! New best EMA pseudo Dice: 0.6844000220298767
2025-10-15 07:53:56.750086: 
2025-10-15 07:53:56.750406: Epoch 30
2025-10-15 07:53:56.750685: Current learning rate: 0.00818
2025-10-15 07:54:42.787070: Validation loss improved from -0.50797 to -0.50884! Patience: 2/50
2025-10-15 07:54:42.787728: train_loss -0.6296
2025-10-15 07:54:42.787957: val_loss -0.5088
2025-10-15 07:54:42.788158: Pseudo dice [np.float32(0.7259)]
2025-10-15 07:54:42.788358: Epoch time: 46.04 s
2025-10-15 07:54:42.788523: Yayy! New best EMA pseudo Dice: 0.6886000037193298
2025-10-15 07:54:43.878004: 
2025-10-15 07:54:43.878332: Epoch 31
2025-10-15 07:54:43.878515: Current learning rate: 0.00812
2025-10-15 07:55:29.964727: Validation loss did not improve from -0.50884. Patience: 1/50
2025-10-15 07:55:29.965316: train_loss -0.6335
2025-10-15 07:55:29.965588: val_loss -0.4591
2025-10-15 07:55:29.965788: Pseudo dice [np.float32(0.6993)]
2025-10-15 07:55:29.965996: Epoch time: 46.09 s
2025-10-15 07:55:29.966142: Yayy! New best EMA pseudo Dice: 0.6897000074386597
2025-10-15 07:55:31.052571: 
2025-10-15 07:55:31.052976: Epoch 32
2025-10-15 07:55:31.053197: Current learning rate: 0.00806
2025-10-15 07:56:17.175278: Validation loss did not improve from -0.50884. Patience: 2/50
2025-10-15 07:56:17.176062: train_loss -0.6396
2025-10-15 07:56:17.176449: val_loss -0.497
2025-10-15 07:56:17.176783: Pseudo dice [np.float32(0.7093)]
2025-10-15 07:56:17.177082: Epoch time: 46.12 s
2025-10-15 07:56:17.177596: Yayy! New best EMA pseudo Dice: 0.6916000247001648
2025-10-15 07:56:18.264695: 
2025-10-15 07:56:18.265013: Epoch 33
2025-10-15 07:56:18.265314: Current learning rate: 0.008
2025-10-15 07:57:04.316561: Validation loss did not improve from -0.50884. Patience: 3/50
2025-10-15 07:57:04.317101: train_loss -0.6428
2025-10-15 07:57:04.317308: val_loss -0.4748
2025-10-15 07:57:04.317444: Pseudo dice [np.float32(0.7112)]
2025-10-15 07:57:04.317603: Epoch time: 46.05 s
2025-10-15 07:57:04.317724: Yayy! New best EMA pseudo Dice: 0.6935999989509583
2025-10-15 07:57:05.407395: 
2025-10-15 07:57:05.407732: Epoch 34
2025-10-15 07:57:05.407999: Current learning rate: 0.00793
2025-10-15 07:57:51.448123: Validation loss improved from -0.50884 to -0.53653! Patience: 3/50
2025-10-15 07:57:51.448779: train_loss -0.6405
2025-10-15 07:57:51.448934: val_loss -0.5365
2025-10-15 07:57:51.449090: Pseudo dice [np.float32(0.7378)]
2025-10-15 07:57:51.449228: Epoch time: 46.04 s
2025-10-15 07:57:51.891191: Yayy! New best EMA pseudo Dice: 0.6980000138282776
2025-10-15 07:57:52.964525: 
2025-10-15 07:57:52.964895: Epoch 35
2025-10-15 07:57:52.965108: Current learning rate: 0.00787
2025-10-15 07:58:39.042718: Validation loss did not improve from -0.53653. Patience: 1/50
2025-10-15 07:58:39.043140: train_loss -0.6506
2025-10-15 07:58:39.043339: val_loss -0.4807
2025-10-15 07:58:39.043491: Pseudo dice [np.float32(0.7131)]
2025-10-15 07:58:39.043646: Epoch time: 46.08 s
2025-10-15 07:58:39.043785: Yayy! New best EMA pseudo Dice: 0.6995000243186951
2025-10-15 07:58:40.131846: 
2025-10-15 07:58:40.132177: Epoch 36
2025-10-15 07:58:40.132360: Current learning rate: 0.00781
2025-10-15 07:59:26.195040: Validation loss did not improve from -0.53653. Patience: 2/50
2025-10-15 07:59:26.195738: train_loss -0.6416
2025-10-15 07:59:26.195910: val_loss -0.5084
2025-10-15 07:59:26.196068: Pseudo dice [np.float32(0.7241)]
2025-10-15 07:59:26.196219: Epoch time: 46.06 s
2025-10-15 07:59:26.196339: Yayy! New best EMA pseudo Dice: 0.7020000219345093
2025-10-15 07:59:27.269467: 
2025-10-15 07:59:27.269794: Epoch 37
2025-10-15 07:59:27.269999: Current learning rate: 0.00775
2025-10-15 08:00:13.365018: Validation loss did not improve from -0.53653. Patience: 3/50
2025-10-15 08:00:13.365438: train_loss -0.6454
2025-10-15 08:00:13.365631: val_loss -0.4827
2025-10-15 08:00:13.365769: Pseudo dice [np.float32(0.6957)]
2025-10-15 08:00:13.365915: Epoch time: 46.1 s
2025-10-15 08:00:14.003892: 
2025-10-15 08:00:14.004221: Epoch 38
2025-10-15 08:00:14.004426: Current learning rate: 0.00769
2025-10-15 08:01:00.049438: Validation loss did not improve from -0.53653. Patience: 4/50
2025-10-15 08:01:00.050367: train_loss -0.6467
2025-10-15 08:01:00.050648: val_loss -0.5254
2025-10-15 08:01:00.050851: Pseudo dice [np.float32(0.7294)]
2025-10-15 08:01:00.051109: Epoch time: 46.05 s
2025-10-15 08:01:00.051396: Yayy! New best EMA pseudo Dice: 0.704200029373169
2025-10-15 08:01:01.138200: 
2025-10-15 08:01:01.138567: Epoch 39
2025-10-15 08:01:01.138801: Current learning rate: 0.00763
2025-10-15 08:01:47.171224: Validation loss did not improve from -0.53653. Patience: 5/50
2025-10-15 08:01:47.171625: train_loss -0.6577
2025-10-15 08:01:47.171792: val_loss -0.4939
2025-10-15 08:01:47.171949: Pseudo dice [np.float32(0.7203)]
2025-10-15 08:01:47.172107: Epoch time: 46.03 s
2025-10-15 08:01:47.624907: Yayy! New best EMA pseudo Dice: 0.7057999968528748
2025-10-15 08:01:48.699229: 
2025-10-15 08:01:48.699635: Epoch 40
2025-10-15 08:01:48.699900: Current learning rate: 0.00756
2025-10-15 08:02:34.801255: Validation loss did not improve from -0.53653. Patience: 6/50
2025-10-15 08:02:34.802445: train_loss -0.6601
2025-10-15 08:02:34.802856: val_loss -0.5072
2025-10-15 08:02:34.803201: Pseudo dice [np.float32(0.7165)]
2025-10-15 08:02:34.803550: Epoch time: 46.1 s
2025-10-15 08:02:34.803881: Yayy! New best EMA pseudo Dice: 0.7067999839782715
2025-10-15 08:02:35.893684: 
2025-10-15 08:02:35.894279: Epoch 41
2025-10-15 08:02:35.894796: Current learning rate: 0.0075
2025-10-15 08:03:21.979687: Validation loss did not improve from -0.53653. Patience: 7/50
2025-10-15 08:03:21.980147: train_loss -0.6561
2025-10-15 08:03:21.980344: val_loss -0.4759
2025-10-15 08:03:21.980513: Pseudo dice [np.float32(0.7131)]
2025-10-15 08:03:21.980715: Epoch time: 46.09 s
2025-10-15 08:03:21.980845: Yayy! New best EMA pseudo Dice: 0.7074999809265137
2025-10-15 08:03:23.057940: 
2025-10-15 08:03:23.058204: Epoch 42
2025-10-15 08:03:23.058385: Current learning rate: 0.00744
2025-10-15 08:04:09.060972: Validation loss did not improve from -0.53653. Patience: 8/50
2025-10-15 08:04:09.061564: train_loss -0.6575
2025-10-15 08:04:09.061742: val_loss -0.5106
2025-10-15 08:04:09.061866: Pseudo dice [np.float32(0.7249)]
2025-10-15 08:04:09.062044: Epoch time: 46.0 s
2025-10-15 08:04:09.062184: Yayy! New best EMA pseudo Dice: 0.7092000246047974
2025-10-15 08:04:10.614711: 
2025-10-15 08:04:10.615082: Epoch 43
2025-10-15 08:04:10.615292: Current learning rate: 0.00738
2025-10-15 08:04:56.632468: Validation loss did not improve from -0.53653. Patience: 9/50
2025-10-15 08:04:56.632897: train_loss -0.6656
2025-10-15 08:04:56.633052: val_loss -0.5345
2025-10-15 08:04:56.633195: Pseudo dice [np.float32(0.7369)]
2025-10-15 08:04:56.633385: Epoch time: 46.02 s
2025-10-15 08:04:56.633519: Yayy! New best EMA pseudo Dice: 0.7120000123977661
2025-10-15 08:04:57.714134: 
2025-10-15 08:04:57.714443: Epoch 44
2025-10-15 08:04:57.714626: Current learning rate: 0.00732
2025-10-15 08:05:43.716059: Validation loss improved from -0.53653 to -0.54066! Patience: 9/50
2025-10-15 08:05:43.716710: train_loss -0.664
2025-10-15 08:05:43.716859: val_loss -0.5407
2025-10-15 08:05:43.716990: Pseudo dice [np.float32(0.7327)]
2025-10-15 08:05:43.717202: Epoch time: 46.0 s
2025-10-15 08:05:44.161309: Yayy! New best EMA pseudo Dice: 0.7141000032424927
2025-10-15 08:05:45.226459: 
2025-10-15 08:05:45.226807: Epoch 45
2025-10-15 08:05:45.227058: Current learning rate: 0.00725
2025-10-15 08:06:31.328536: Validation loss did not improve from -0.54066. Patience: 1/50
2025-10-15 08:06:31.329065: train_loss -0.6599
2025-10-15 08:06:31.329285: val_loss -0.5074
2025-10-15 08:06:31.329493: Pseudo dice [np.float32(0.7298)]
2025-10-15 08:06:31.329871: Epoch time: 46.1 s
2025-10-15 08:06:31.330083: Yayy! New best EMA pseudo Dice: 0.7156000137329102
2025-10-15 08:06:32.410032: 
2025-10-15 08:06:32.410316: Epoch 46
2025-10-15 08:06:32.410537: Current learning rate: 0.00719
2025-10-15 08:07:18.496985: Validation loss did not improve from -0.54066. Patience: 2/50
2025-10-15 08:07:18.497700: train_loss -0.6691
2025-10-15 08:07:18.497875: val_loss -0.5084
2025-10-15 08:07:18.498011: Pseudo dice [np.float32(0.7297)]
2025-10-15 08:07:18.498189: Epoch time: 46.09 s
2025-10-15 08:07:18.498346: Yayy! New best EMA pseudo Dice: 0.7170000076293945
2025-10-15 08:07:19.573556: 
2025-10-15 08:07:19.573899: Epoch 47
2025-10-15 08:07:19.574105: Current learning rate: 0.00713
2025-10-15 08:08:05.604563: Validation loss did not improve from -0.54066. Patience: 3/50
2025-10-15 08:08:05.605038: train_loss -0.6692
2025-10-15 08:08:05.605222: val_loss -0.5309
2025-10-15 08:08:05.605354: Pseudo dice [np.float32(0.7418)]
2025-10-15 08:08:05.605532: Epoch time: 46.03 s
2025-10-15 08:08:05.605685: Yayy! New best EMA pseudo Dice: 0.7195000052452087
2025-10-15 08:08:06.670761: 
2025-10-15 08:08:06.671362: Epoch 48
2025-10-15 08:08:06.671667: Current learning rate: 0.00707
2025-10-15 08:08:52.727141: Validation loss did not improve from -0.54066. Patience: 4/50
2025-10-15 08:08:52.727766: train_loss -0.6727
2025-10-15 08:08:52.727933: val_loss -0.4811
2025-10-15 08:08:52.728095: Pseudo dice [np.float32(0.7132)]
2025-10-15 08:08:52.728245: Epoch time: 46.06 s
2025-10-15 08:08:53.368554: 
2025-10-15 08:08:53.368861: Epoch 49
2025-10-15 08:08:53.369083: Current learning rate: 0.007
2025-10-15 08:09:39.366905: Validation loss did not improve from -0.54066. Patience: 5/50
2025-10-15 08:09:39.367477: train_loss -0.6808
2025-10-15 08:09:39.367776: val_loss -0.5273
2025-10-15 08:09:39.368116: Pseudo dice [np.float32(0.7339)]
2025-10-15 08:09:39.368606: Epoch time: 46.0 s
2025-10-15 08:09:39.820180: Yayy! New best EMA pseudo Dice: 0.7203999757766724
2025-10-15 08:09:40.886501: 
2025-10-15 08:09:40.886861: Epoch 50
2025-10-15 08:09:40.887053: Current learning rate: 0.00694
2025-10-15 08:10:26.902156: Validation loss did not improve from -0.54066. Patience: 6/50
2025-10-15 08:10:26.902703: train_loss -0.682
2025-10-15 08:10:26.902873: val_loss -0.5283
2025-10-15 08:10:26.903065: Pseudo dice [np.float32(0.7307)]
2025-10-15 08:10:26.903235: Epoch time: 46.02 s
2025-10-15 08:10:26.903355: Yayy! New best EMA pseudo Dice: 0.7214000225067139
2025-10-15 08:10:27.986594: 
2025-10-15 08:10:27.986936: Epoch 51
2025-10-15 08:10:27.987167: Current learning rate: 0.00688
2025-10-15 08:11:13.963555: Validation loss did not improve from -0.54066. Patience: 7/50
2025-10-15 08:11:13.963963: train_loss -0.6874
2025-10-15 08:11:13.964135: val_loss -0.4905
2025-10-15 08:11:13.964272: Pseudo dice [np.float32(0.7126)]
2025-10-15 08:11:13.964410: Epoch time: 45.98 s
2025-10-15 08:11:14.597712: 
2025-10-15 08:11:14.598078: Epoch 52
2025-10-15 08:11:14.598256: Current learning rate: 0.00682
2025-10-15 08:12:00.564806: Validation loss did not improve from -0.54066. Patience: 8/50
2025-10-15 08:12:00.566153: train_loss -0.6806
2025-10-15 08:12:00.566616: val_loss -0.5305
2025-10-15 08:12:00.566984: Pseudo dice [np.float32(0.7328)]
2025-10-15 08:12:00.567349: Epoch time: 45.97 s
2025-10-15 08:12:00.567686: Yayy! New best EMA pseudo Dice: 0.7218000292778015
2025-10-15 08:12:01.652364: 
2025-10-15 08:12:01.652651: Epoch 53
2025-10-15 08:12:01.652851: Current learning rate: 0.00675
2025-10-15 08:12:47.635241: Validation loss did not improve from -0.54066. Patience: 9/50
2025-10-15 08:12:47.635741: train_loss -0.6877
2025-10-15 08:12:47.635912: val_loss -0.5114
2025-10-15 08:12:47.636076: Pseudo dice [np.float32(0.7319)]
2025-10-15 08:12:47.636213: Epoch time: 45.98 s
2025-10-15 08:12:47.636337: Yayy! New best EMA pseudo Dice: 0.7228000164031982
2025-10-15 08:12:48.754389: 
2025-10-15 08:12:48.754707: Epoch 54
2025-10-15 08:12:48.754935: Current learning rate: 0.00669
2025-10-15 08:13:34.763759: Validation loss did not improve from -0.54066. Patience: 10/50
2025-10-15 08:13:34.764362: train_loss -0.6956
2025-10-15 08:13:34.764525: val_loss -0.5032
2025-10-15 08:13:34.764680: Pseudo dice [np.float32(0.7199)]
2025-10-15 08:13:34.764826: Epoch time: 46.01 s
2025-10-15 08:13:35.844713: 
2025-10-15 08:13:35.845074: Epoch 55
2025-10-15 08:13:35.845264: Current learning rate: 0.00663
2025-10-15 08:14:21.867968: Validation loss did not improve from -0.54066. Patience: 11/50
2025-10-15 08:14:21.868516: train_loss -0.6862
2025-10-15 08:14:21.868854: val_loss -0.4767
2025-10-15 08:14:21.869139: Pseudo dice [np.float32(0.7144)]
2025-10-15 08:14:21.869352: Epoch time: 46.02 s
2025-10-15 08:14:22.520647: 
2025-10-15 08:14:22.521030: Epoch 56
2025-10-15 08:14:22.521240: Current learning rate: 0.00657
2025-10-15 08:15:08.563014: Validation loss did not improve from -0.54066. Patience: 12/50
2025-10-15 08:15:08.563693: train_loss -0.6863
2025-10-15 08:15:08.563855: val_loss -0.488
2025-10-15 08:15:08.564010: Pseudo dice [np.float32(0.7271)]
2025-10-15 08:15:08.564170: Epoch time: 46.04 s
2025-10-15 08:15:09.202424: 
2025-10-15 08:15:09.202745: Epoch 57
2025-10-15 08:15:09.202956: Current learning rate: 0.0065
2025-10-15 08:15:55.267454: Validation loss did not improve from -0.54066. Patience: 13/50
2025-10-15 08:15:55.267842: train_loss -0.6948
2025-10-15 08:15:55.268025: val_loss -0.5306
2025-10-15 08:15:55.268172: Pseudo dice [np.float32(0.7407)]
2025-10-15 08:15:55.268317: Epoch time: 46.07 s
2025-10-15 08:15:55.268495: Yayy! New best EMA pseudo Dice: 0.7240999937057495
2025-10-15 08:15:56.865809: 
2025-10-15 08:15:56.866565: Epoch 58
2025-10-15 08:15:56.867711: Current learning rate: 0.00644
2025-10-15 08:16:42.922746: Validation loss did not improve from -0.54066. Patience: 14/50
2025-10-15 08:16:42.923347: train_loss -0.6919
2025-10-15 08:16:42.923522: val_loss -0.4945
2025-10-15 08:16:42.923651: Pseudo dice [np.float32(0.7256)]
2025-10-15 08:16:42.923844: Epoch time: 46.06 s
2025-10-15 08:16:42.923997: Yayy! New best EMA pseudo Dice: 0.7242000102996826
2025-10-15 08:16:44.010266: 
2025-10-15 08:16:44.010536: Epoch 59
2025-10-15 08:16:44.010728: Current learning rate: 0.00638
2025-10-15 08:17:30.091393: Validation loss did not improve from -0.54066. Patience: 15/50
2025-10-15 08:17:30.091888: train_loss -0.6933
2025-10-15 08:17:30.092074: val_loss -0.5374
2025-10-15 08:17:30.092245: Pseudo dice [np.float32(0.7415)]
2025-10-15 08:17:30.092402: Epoch time: 46.08 s
2025-10-15 08:17:30.552005: Yayy! New best EMA pseudo Dice: 0.7260000109672546
2025-10-15 08:17:31.614464: 
2025-10-15 08:17:31.614971: Epoch 60
2025-10-15 08:17:31.615364: Current learning rate: 0.00631
2025-10-15 08:18:17.638465: Validation loss improved from -0.54066 to -0.55105! Patience: 15/50
2025-10-15 08:18:17.639129: train_loss -0.6954
2025-10-15 08:18:17.639324: val_loss -0.5511
2025-10-15 08:18:17.639449: Pseudo dice [np.float32(0.7478)]
2025-10-15 08:18:17.639590: Epoch time: 46.03 s
2025-10-15 08:18:17.639714: Yayy! New best EMA pseudo Dice: 0.7281000018119812
2025-10-15 08:18:18.720296: 
2025-10-15 08:18:18.720586: Epoch 61
2025-10-15 08:18:18.720847: Current learning rate: 0.00625
2025-10-15 08:19:04.749990: Validation loss did not improve from -0.55105. Patience: 1/50
2025-10-15 08:19:04.750533: train_loss -0.7043
2025-10-15 08:19:04.750809: val_loss -0.5075
2025-10-15 08:19:04.751084: Pseudo dice [np.float32(0.7289)]
2025-10-15 08:19:04.751347: Epoch time: 46.03 s
2025-10-15 08:19:04.751546: Yayy! New best EMA pseudo Dice: 0.7282000184059143
2025-10-15 08:19:05.854790: 
2025-10-15 08:19:05.855119: Epoch 62
2025-10-15 08:19:05.855302: Current learning rate: 0.00619
2025-10-15 08:19:51.880748: Validation loss did not improve from -0.55105. Patience: 2/50
2025-10-15 08:19:51.881366: train_loss -0.6997
2025-10-15 08:19:51.881558: val_loss -0.4574
2025-10-15 08:19:51.881733: Pseudo dice [np.float32(0.6946)]
2025-10-15 08:19:51.881937: Epoch time: 46.03 s
2025-10-15 08:19:52.530620: 
2025-10-15 08:19:52.530903: Epoch 63
2025-10-15 08:19:52.531120: Current learning rate: 0.00612
2025-10-15 08:20:38.629944: Validation loss did not improve from -0.55105. Patience: 3/50
2025-10-15 08:20:38.630383: train_loss -0.6967
2025-10-15 08:20:38.630549: val_loss -0.5343
2025-10-15 08:20:38.630700: Pseudo dice [np.float32(0.7463)]
2025-10-15 08:20:38.630866: Epoch time: 46.1 s
2025-10-15 08:20:39.276502: 
2025-10-15 08:20:39.276890: Epoch 64
2025-10-15 08:20:39.277089: Current learning rate: 0.00606
2025-10-15 08:21:25.404169: Validation loss did not improve from -0.55105. Patience: 4/50
2025-10-15 08:21:25.404781: train_loss -0.7037
2025-10-15 08:21:25.404944: val_loss -0.5176
2025-10-15 08:21:25.405094: Pseudo dice [np.float32(0.7354)]
2025-10-15 08:21:25.405242: Epoch time: 46.13 s
2025-10-15 08:21:26.503689: 
2025-10-15 08:21:26.504004: Epoch 65
2025-10-15 08:21:26.504247: Current learning rate: 0.006
2025-10-15 08:22:12.573776: Validation loss did not improve from -0.55105. Patience: 5/50
2025-10-15 08:22:12.574268: train_loss -0.7047
2025-10-15 08:22:12.574462: val_loss -0.5073
2025-10-15 08:22:12.574616: Pseudo dice [np.float32(0.7251)]
2025-10-15 08:22:12.574761: Epoch time: 46.07 s
2025-10-15 08:22:13.224096: 
2025-10-15 08:22:13.224370: Epoch 66
2025-10-15 08:22:13.224572: Current learning rate: 0.00593
2025-10-15 08:22:59.291671: Validation loss did not improve from -0.55105. Patience: 6/50
2025-10-15 08:22:59.292377: train_loss -0.7108
2025-10-15 08:22:59.292572: val_loss -0.4836
2025-10-15 08:22:59.292727: Pseudo dice [np.float32(0.713)]
2025-10-15 08:22:59.292873: Epoch time: 46.07 s
2025-10-15 08:22:59.943291: 
2025-10-15 08:22:59.943624: Epoch 67
2025-10-15 08:22:59.943810: Current learning rate: 0.00587
2025-10-15 08:23:45.979687: Validation loss did not improve from -0.55105. Patience: 7/50
2025-10-15 08:23:45.980091: train_loss -0.7155
2025-10-15 08:23:45.980285: val_loss -0.5328
2025-10-15 08:23:45.980468: Pseudo dice [np.float32(0.7407)]
2025-10-15 08:23:45.980841: Epoch time: 46.04 s
2025-10-15 08:23:46.621338: 
2025-10-15 08:23:46.621698: Epoch 68
2025-10-15 08:23:46.621889: Current learning rate: 0.00581
2025-10-15 08:24:32.620093: Validation loss did not improve from -0.55105. Patience: 8/50
2025-10-15 08:24:32.620613: train_loss -0.7149
2025-10-15 08:24:32.620775: val_loss -0.5215
2025-10-15 08:24:32.620911: Pseudo dice [np.float32(0.7286)]
2025-10-15 08:24:32.621060: Epoch time: 46.0 s
2025-10-15 08:24:33.254705: 
2025-10-15 08:24:33.254914: Epoch 69
2025-10-15 08:24:33.255074: Current learning rate: 0.00574
2025-10-15 08:25:19.158123: Validation loss did not improve from -0.55105. Patience: 9/50
2025-10-15 08:25:19.158479: train_loss -0.714
2025-10-15 08:25:19.158638: val_loss -0.5358
2025-10-15 08:25:19.158806: Pseudo dice [np.float32(0.7408)]
2025-10-15 08:25:19.159025: Epoch time: 45.9 s
2025-10-15 08:25:19.608819: Yayy! New best EMA pseudo Dice: 0.7289999723434448
2025-10-15 08:25:20.656827: 
2025-10-15 08:25:20.657151: Epoch 70
2025-10-15 08:25:20.657335: Current learning rate: 0.00568
2025-10-15 08:26:06.569407: Validation loss did not improve from -0.55105. Patience: 10/50
2025-10-15 08:26:06.570038: train_loss -0.718
2025-10-15 08:26:06.570209: val_loss -0.5372
2025-10-15 08:26:06.570373: Pseudo dice [np.float32(0.7392)]
2025-10-15 08:26:06.570553: Epoch time: 45.91 s
2025-10-15 08:26:06.570697: Yayy! New best EMA pseudo Dice: 0.7300000190734863
2025-10-15 08:26:07.633774: 
2025-10-15 08:26:07.634267: Epoch 71
2025-10-15 08:26:07.634429: Current learning rate: 0.00562
2025-10-15 08:26:53.578465: Validation loss did not improve from -0.55105. Patience: 11/50
2025-10-15 08:26:53.578868: train_loss -0.7206
2025-10-15 08:26:53.579019: val_loss -0.504
2025-10-15 08:26:53.579143: Pseudo dice [np.float32(0.7216)]
2025-10-15 08:26:53.579313: Epoch time: 45.95 s
2025-10-15 08:26:54.213489: 
2025-10-15 08:26:54.213783: Epoch 72
2025-10-15 08:26:54.213983: Current learning rate: 0.00555
2025-10-15 08:27:40.180261: Validation loss did not improve from -0.55105. Patience: 12/50
2025-10-15 08:27:40.180910: train_loss -0.7174
2025-10-15 08:27:40.181075: val_loss -0.524
2025-10-15 08:27:40.181240: Pseudo dice [np.float32(0.7313)]
2025-10-15 08:27:40.181402: Epoch time: 45.97 s
2025-10-15 08:27:41.174158: 
2025-10-15 08:27:41.174491: Epoch 73
2025-10-15 08:27:41.174673: Current learning rate: 0.00549
2025-10-15 08:28:27.138419: Validation loss did not improve from -0.55105. Patience: 13/50
2025-10-15 08:28:27.138839: train_loss -0.7211
2025-10-15 08:28:27.139034: val_loss -0.5321
2025-10-15 08:28:27.139197: Pseudo dice [np.float32(0.7385)]
2025-10-15 08:28:27.139370: Epoch time: 45.97 s
2025-10-15 08:28:27.139538: Yayy! New best EMA pseudo Dice: 0.7303000092506409
2025-10-15 08:28:28.214530: 
2025-10-15 08:28:28.214837: Epoch 74
2025-10-15 08:28:28.215063: Current learning rate: 0.00542
2025-10-15 08:29:14.132117: Validation loss improved from -0.55105 to -0.56661! Patience: 13/50
2025-10-15 08:29:14.132839: train_loss -0.7227
2025-10-15 08:29:14.133016: val_loss -0.5666
2025-10-15 08:29:14.133161: Pseudo dice [np.float32(0.7568)]
2025-10-15 08:29:14.133308: Epoch time: 45.92 s
2025-10-15 08:29:14.589460: Yayy! New best EMA pseudo Dice: 0.7329000234603882
2025-10-15 08:29:15.628895: 
2025-10-15 08:29:15.629260: Epoch 75
2025-10-15 08:29:15.629512: Current learning rate: 0.00536
2025-10-15 08:30:01.637012: Validation loss did not improve from -0.56661. Patience: 1/50
2025-10-15 08:30:01.637506: train_loss -0.7285
2025-10-15 08:30:01.637696: val_loss -0.5168
2025-10-15 08:30:01.637883: Pseudo dice [np.float32(0.7342)]
2025-10-15 08:30:01.638043: Epoch time: 46.01 s
2025-10-15 08:30:01.638188: Yayy! New best EMA pseudo Dice: 0.7330999970436096
2025-10-15 08:30:02.699314: 
2025-10-15 08:30:02.699781: Epoch 76
2025-10-15 08:30:02.699990: Current learning rate: 0.00529
2025-10-15 08:30:48.679366: Validation loss did not improve from -0.56661. Patience: 2/50
2025-10-15 08:30:48.680099: train_loss -0.729
2025-10-15 08:30:48.680281: val_loss -0.5217
2025-10-15 08:30:48.680401: Pseudo dice [np.float32(0.7342)]
2025-10-15 08:30:48.680556: Epoch time: 45.98 s
2025-10-15 08:30:48.680681: Yayy! New best EMA pseudo Dice: 0.7332000136375427
2025-10-15 08:30:49.733394: 
2025-10-15 08:30:49.733717: Epoch 77
2025-10-15 08:30:49.733868: Current learning rate: 0.00523
2025-10-15 08:31:35.755420: Validation loss did not improve from -0.56661. Patience: 3/50
2025-10-15 08:31:35.755826: train_loss -0.7316
2025-10-15 08:31:35.755975: val_loss -0.5245
2025-10-15 08:31:35.756140: Pseudo dice [np.float32(0.7386)]
2025-10-15 08:31:35.756327: Epoch time: 46.02 s
2025-10-15 08:31:35.756455: Yayy! New best EMA pseudo Dice: 0.7336999773979187
2025-10-15 08:31:36.829101: 
2025-10-15 08:31:36.829428: Epoch 78
2025-10-15 08:31:36.829623: Current learning rate: 0.00517
2025-10-15 08:32:22.819444: Validation loss did not improve from -0.56661. Patience: 4/50
2025-10-15 08:32:22.819989: train_loss -0.731
2025-10-15 08:32:22.820145: val_loss -0.5626
2025-10-15 08:32:22.820268: Pseudo dice [np.float32(0.7543)]
2025-10-15 08:32:22.820401: Epoch time: 45.99 s
2025-10-15 08:32:22.820539: Yayy! New best EMA pseudo Dice: 0.73580002784729
2025-10-15 08:32:23.904336: 
2025-10-15 08:32:23.904660: Epoch 79
2025-10-15 08:32:23.904827: Current learning rate: 0.0051
2025-10-15 08:33:09.913789: Validation loss did not improve from -0.56661. Patience: 5/50
2025-10-15 08:33:09.914140: train_loss -0.728
2025-10-15 08:33:09.914353: val_loss -0.5428
2025-10-15 08:33:09.914485: Pseudo dice [np.float32(0.7381)]
2025-10-15 08:33:09.914647: Epoch time: 46.01 s
2025-10-15 08:33:10.368248: Yayy! New best EMA pseudo Dice: 0.7360000014305115
2025-10-15 08:33:11.441631: 
2025-10-15 08:33:11.441945: Epoch 80
2025-10-15 08:33:11.442148: Current learning rate: 0.00504
2025-10-15 08:33:57.469554: Validation loss did not improve from -0.56661. Patience: 6/50
2025-10-15 08:33:57.470274: train_loss -0.7322
2025-10-15 08:33:57.470439: val_loss -0.5393
2025-10-15 08:33:57.470577: Pseudo dice [np.float32(0.7383)]
2025-10-15 08:33:57.470719: Epoch time: 46.03 s
2025-10-15 08:33:57.470870: Yayy! New best EMA pseudo Dice: 0.7361999750137329
2025-10-15 08:33:58.554379: 
2025-10-15 08:33:58.554701: Epoch 81
2025-10-15 08:33:58.554906: Current learning rate: 0.00497
2025-10-15 08:34:44.597235: Validation loss did not improve from -0.56661. Patience: 7/50
2025-10-15 08:34:44.597680: train_loss -0.7261
2025-10-15 08:34:44.597869: val_loss -0.4838
2025-10-15 08:34:44.598042: Pseudo dice [np.float32(0.7151)]
2025-10-15 08:34:44.598243: Epoch time: 46.04 s
2025-10-15 08:34:45.241192: 
2025-10-15 08:34:45.241496: Epoch 82
2025-10-15 08:34:45.241683: Current learning rate: 0.00491
2025-10-15 08:35:31.291540: Validation loss did not improve from -0.56661. Patience: 8/50
2025-10-15 08:35:31.292202: train_loss -0.7296
2025-10-15 08:35:31.292394: val_loss -0.5193
2025-10-15 08:35:31.292548: Pseudo dice [np.float32(0.7298)]
2025-10-15 08:35:31.292725: Epoch time: 46.05 s
2025-10-15 08:35:31.917951: 
2025-10-15 08:35:31.918265: Epoch 83
2025-10-15 08:35:31.918453: Current learning rate: 0.00484
2025-10-15 08:36:17.940869: Validation loss did not improve from -0.56661. Patience: 9/50
2025-10-15 08:36:17.941335: train_loss -0.7318
2025-10-15 08:36:17.941750: val_loss -0.5047
2025-10-15 08:36:17.941925: Pseudo dice [np.float32(0.7277)]
2025-10-15 08:36:17.942121: Epoch time: 46.02 s
2025-10-15 08:36:18.564398: 
2025-10-15 08:36:18.564721: Epoch 84
2025-10-15 08:36:18.564947: Current learning rate: 0.00478
2025-10-15 08:37:04.522382: Validation loss did not improve from -0.56661. Patience: 10/50
2025-10-15 08:37:04.522967: train_loss -0.7357
2025-10-15 08:37:04.523197: val_loss -0.4944
2025-10-15 08:37:04.523353: Pseudo dice [np.float32(0.7223)]
2025-10-15 08:37:04.523535: Epoch time: 45.96 s
2025-10-15 08:37:05.582123: 
2025-10-15 08:37:05.582465: Epoch 85
2025-10-15 08:37:05.582665: Current learning rate: 0.00471
2025-10-15 08:37:51.529981: Validation loss improved from -0.56661 to -0.57923! Patience: 10/50
2025-10-15 08:37:51.530362: train_loss -0.7348
2025-10-15 08:37:51.530717: val_loss -0.5792
2025-10-15 08:37:51.530873: Pseudo dice [np.float32(0.7618)]
2025-10-15 08:37:51.531006: Epoch time: 45.95 s
2025-10-15 08:37:52.147077: 
2025-10-15 08:37:52.147298: Epoch 86
2025-10-15 08:37:52.147488: Current learning rate: 0.00465
2025-10-15 08:38:38.147867: Validation loss did not improve from -0.57923. Patience: 1/50
2025-10-15 08:38:38.148872: train_loss -0.7402
2025-10-15 08:38:38.149223: val_loss -0.5181
2025-10-15 08:38:38.149534: Pseudo dice [np.float32(0.7344)]
2025-10-15 08:38:38.149841: Epoch time: 46.0 s
2025-10-15 08:38:38.775154: 
2025-10-15 08:38:38.775459: Epoch 87
2025-10-15 08:38:38.775690: Current learning rate: 0.00458
2025-10-15 08:39:24.823021: Validation loss did not improve from -0.57923. Patience: 2/50
2025-10-15 08:39:24.823458: train_loss -0.7377
2025-10-15 08:39:24.823642: val_loss -0.5421
2025-10-15 08:39:24.823822: Pseudo dice [np.float32(0.7444)]
2025-10-15 08:39:24.823968: Epoch time: 46.05 s
2025-10-15 08:39:25.832544: 
2025-10-15 08:39:25.832898: Epoch 88
2025-10-15 08:39:25.833088: Current learning rate: 0.00452
2025-10-15 08:40:11.812884: Validation loss did not improve from -0.57923. Patience: 3/50
2025-10-15 08:40:11.813555: train_loss -0.7467
2025-10-15 08:40:11.813712: val_loss -0.5522
2025-10-15 08:40:11.813831: Pseudo dice [np.float32(0.7523)]
2025-10-15 08:40:11.813966: Epoch time: 45.98 s
2025-10-15 08:40:11.814141: Yayy! New best EMA pseudo Dice: 0.737500011920929
2025-10-15 08:40:12.877539: 
2025-10-15 08:40:12.877894: Epoch 89
2025-10-15 08:40:12.878057: Current learning rate: 0.00445
2025-10-15 08:40:58.844557: Validation loss did not improve from -0.57923. Patience: 4/50
2025-10-15 08:40:58.845014: train_loss -0.7478
2025-10-15 08:40:58.845232: val_loss -0.5428
2025-10-15 08:40:58.845365: Pseudo dice [np.float32(0.7444)]
2025-10-15 08:40:58.845540: Epoch time: 45.97 s
2025-10-15 08:40:59.287137: Yayy! New best EMA pseudo Dice: 0.7382000088691711
2025-10-15 08:41:00.321904: 
2025-10-15 08:41:00.322202: Epoch 90
2025-10-15 08:41:00.322386: Current learning rate: 0.00438
2025-10-15 08:41:46.287245: Validation loss did not improve from -0.57923. Patience: 5/50
2025-10-15 08:41:46.287868: train_loss -0.7452
2025-10-15 08:41:46.288051: val_loss -0.552
2025-10-15 08:41:46.288200: Pseudo dice [np.float32(0.7526)]
2025-10-15 08:41:46.288379: Epoch time: 45.97 s
2025-10-15 08:41:46.288549: Yayy! New best EMA pseudo Dice: 0.7396000027656555
2025-10-15 08:41:47.339295: 
2025-10-15 08:41:47.339835: Epoch 91
2025-10-15 08:41:47.340234: Current learning rate: 0.00432
2025-10-15 08:42:33.345678: Validation loss did not improve from -0.57923. Patience: 6/50
2025-10-15 08:42:33.346128: train_loss -0.7447
2025-10-15 08:42:33.346380: val_loss -0.5115
2025-10-15 08:42:33.346549: Pseudo dice [np.float32(0.7315)]
2025-10-15 08:42:33.346750: Epoch time: 46.01 s
2025-10-15 08:42:33.961936: 
2025-10-15 08:42:33.962256: Epoch 92
2025-10-15 08:42:33.962473: Current learning rate: 0.00425
2025-10-15 08:43:19.900603: Validation loss did not improve from -0.57923. Patience: 7/50
2025-10-15 08:43:19.901230: train_loss -0.7429
2025-10-15 08:43:19.901452: val_loss -0.5342
2025-10-15 08:43:19.901582: Pseudo dice [np.float32(0.7446)]
2025-10-15 08:43:19.901733: Epoch time: 45.94 s
2025-10-15 08:43:20.524429: 
2025-10-15 08:43:20.524720: Epoch 93
2025-10-15 08:43:20.524933: Current learning rate: 0.00419
2025-10-15 08:44:06.443152: Validation loss did not improve from -0.57923. Patience: 8/50
2025-10-15 08:44:06.443619: train_loss -0.7436
2025-10-15 08:44:06.443807: val_loss -0.512
2025-10-15 08:44:06.443944: Pseudo dice [np.float32(0.7367)]
2025-10-15 08:44:06.444117: Epoch time: 45.92 s
2025-10-15 08:44:07.063482: 
2025-10-15 08:44:07.063769: Epoch 94
2025-10-15 08:44:07.063951: Current learning rate: 0.00412
2025-10-15 08:44:53.042949: Validation loss did not improve from -0.57923. Patience: 9/50
2025-10-15 08:44:53.043619: train_loss -0.7474
2025-10-15 08:44:53.043771: val_loss -0.535
2025-10-15 08:44:53.043890: Pseudo dice [np.float32(0.7403)]
2025-10-15 08:44:53.044019: Epoch time: 45.98 s
2025-10-15 08:44:54.123578: 
2025-10-15 08:44:54.123916: Epoch 95
2025-10-15 08:44:54.124124: Current learning rate: 0.00405
2025-10-15 08:45:40.073672: Validation loss did not improve from -0.57923. Patience: 10/50
2025-10-15 08:45:40.074254: train_loss -0.7499
2025-10-15 08:45:40.074439: val_loss -0.5294
2025-10-15 08:45:40.074583: Pseudo dice [np.float32(0.7483)]
2025-10-15 08:45:40.074735: Epoch time: 45.95 s
2025-10-15 08:45:40.074858: Yayy! New best EMA pseudo Dice: 0.7401999831199646
2025-10-15 08:45:41.122403: 
2025-10-15 08:45:41.122768: Epoch 96
2025-10-15 08:45:41.122958: Current learning rate: 0.00399
2025-10-15 08:46:27.098001: Validation loss did not improve from -0.57923. Patience: 11/50
2025-10-15 08:46:27.098617: train_loss -0.7495
2025-10-15 08:46:27.098779: val_loss -0.5306
2025-10-15 08:46:27.098950: Pseudo dice [np.float32(0.7362)]
2025-10-15 08:46:27.099083: Epoch time: 45.98 s
2025-10-15 08:46:27.727033: 
2025-10-15 08:46:27.727336: Epoch 97
2025-10-15 08:46:27.727585: Current learning rate: 0.00392
2025-10-15 08:47:13.720206: Validation loss did not improve from -0.57923. Patience: 12/50
2025-10-15 08:47:13.720637: train_loss -0.7513
2025-10-15 08:47:13.720789: val_loss -0.5273
2025-10-15 08:47:13.720982: Pseudo dice [np.float32(0.739)]
2025-10-15 08:47:13.721185: Epoch time: 45.99 s
2025-10-15 08:47:14.343621: 
2025-10-15 08:47:14.343950: Epoch 98
2025-10-15 08:47:14.344225: Current learning rate: 0.00385
2025-10-15 08:48:00.389153: Validation loss did not improve from -0.57923. Patience: 13/50
2025-10-15 08:48:00.389964: train_loss -0.751
2025-10-15 08:48:00.390304: val_loss -0.5417
2025-10-15 08:48:00.390580: Pseudo dice [np.float32(0.7447)]
2025-10-15 08:48:00.390916: Epoch time: 46.05 s
2025-10-15 08:48:00.391240: Yayy! New best EMA pseudo Dice: 0.7401999831199646
2025-10-15 08:48:01.452545: 
2025-10-15 08:48:01.452823: Epoch 99
2025-10-15 08:48:01.453025: Current learning rate: 0.00379
2025-10-15 08:48:47.431590: Validation loss did not improve from -0.57923. Patience: 14/50
2025-10-15 08:48:47.432089: train_loss -0.7539
2025-10-15 08:48:47.432283: val_loss -0.5157
2025-10-15 08:48:47.432466: Pseudo dice [np.float32(0.727)]
2025-10-15 08:48:47.432662: Epoch time: 45.98 s
2025-10-15 08:48:48.494315: 
2025-10-15 08:48:48.494738: Epoch 100
2025-10-15 08:48:48.494990: Current learning rate: 0.00372
2025-10-15 08:49:34.475393: Validation loss did not improve from -0.57923. Patience: 15/50
2025-10-15 08:49:34.476053: train_loss -0.7552
2025-10-15 08:49:34.476204: val_loss -0.5239
2025-10-15 08:49:34.476337: Pseudo dice [np.float32(0.7373)]
2025-10-15 08:49:34.476477: Epoch time: 45.98 s
2025-10-15 08:49:35.099200: 
2025-10-15 08:49:35.099549: Epoch 101
2025-10-15 08:49:35.099764: Current learning rate: 0.00365
2025-10-15 08:50:21.086878: Validation loss did not improve from -0.57923. Patience: 16/50
2025-10-15 08:50:21.087405: train_loss -0.7534
2025-10-15 08:50:21.087692: val_loss -0.5411
2025-10-15 08:50:21.087903: Pseudo dice [np.float32(0.7397)]
2025-10-15 08:50:21.088168: Epoch time: 45.99 s
2025-10-15 08:50:21.715889: 
2025-10-15 08:50:21.716227: Epoch 102
2025-10-15 08:50:21.716437: Current learning rate: 0.00359
2025-10-15 08:51:07.772688: Validation loss did not improve from -0.57923. Patience: 17/50
2025-10-15 08:51:07.773304: train_loss -0.7529
2025-10-15 08:51:07.773517: val_loss -0.5219
2025-10-15 08:51:07.773666: Pseudo dice [np.float32(0.7326)]
2025-10-15 08:51:07.773824: Epoch time: 46.06 s
2025-10-15 08:51:08.400224: 
2025-10-15 08:51:08.400525: Epoch 103
2025-10-15 08:51:08.400721: Current learning rate: 0.00352
2025-10-15 08:51:54.477578: Validation loss did not improve from -0.57923. Patience: 18/50
2025-10-15 08:51:54.478143: train_loss -0.7572
2025-10-15 08:51:54.478547: val_loss -0.5373
2025-10-15 08:51:54.478881: Pseudo dice [np.float32(0.734)]
2025-10-15 08:51:54.479225: Epoch time: 46.08 s
2025-10-15 08:51:55.102723: 
2025-10-15 08:51:55.102961: Epoch 104
2025-10-15 08:51:55.103137: Current learning rate: 0.00345
2025-10-15 08:52:41.152780: Validation loss did not improve from -0.57923. Patience: 19/50
2025-10-15 08:52:41.153608: train_loss -0.7639
2025-10-15 08:52:41.153870: val_loss -0.5263
2025-10-15 08:52:41.153995: Pseudo dice [np.float32(0.7377)]
2025-10-15 08:52:41.154145: Epoch time: 46.05 s
2025-10-15 08:52:42.622370: 
2025-10-15 08:52:42.622772: Epoch 105
2025-10-15 08:52:42.622986: Current learning rate: 0.00338
2025-10-15 08:53:28.596376: Validation loss did not improve from -0.57923. Patience: 20/50
2025-10-15 08:53:28.596760: train_loss -0.7585
2025-10-15 08:53:28.596910: val_loss -0.5701
2025-10-15 08:53:28.597061: Pseudo dice [np.float32(0.7549)]
2025-10-15 08:53:28.597225: Epoch time: 45.98 s
2025-10-15 08:53:29.223756: 
2025-10-15 08:53:29.224019: Epoch 106
2025-10-15 08:53:29.224278: Current learning rate: 0.00332
2025-10-15 08:54:15.140779: Validation loss did not improve from -0.57923. Patience: 21/50
2025-10-15 08:54:15.141406: train_loss -0.7651
2025-10-15 08:54:15.141561: val_loss -0.5648
2025-10-15 08:54:15.141718: Pseudo dice [np.float32(0.7602)]
2025-10-15 08:54:15.141865: Epoch time: 45.92 s
2025-10-15 08:54:15.141998: Yayy! New best EMA pseudo Dice: 0.7415000200271606
2025-10-15 08:54:16.229574: 
2025-10-15 08:54:16.229869: Epoch 107
2025-10-15 08:54:16.230061: Current learning rate: 0.00325
2025-10-15 08:55:02.170783: Validation loss did not improve from -0.57923. Patience: 22/50
2025-10-15 08:55:02.171237: train_loss -0.7617
2025-10-15 08:55:02.171412: val_loss -0.5412
2025-10-15 08:55:02.171623: Pseudo dice [np.float32(0.7429)]
2025-10-15 08:55:02.171781: Epoch time: 45.94 s
2025-10-15 08:55:02.171938: Yayy! New best EMA pseudo Dice: 0.7416999936103821
2025-10-15 08:55:03.246060: 
2025-10-15 08:55:03.246455: Epoch 108
2025-10-15 08:55:03.246679: Current learning rate: 0.00318
2025-10-15 08:55:49.167902: Validation loss did not improve from -0.57923. Patience: 23/50
2025-10-15 08:55:49.168704: train_loss -0.7635
2025-10-15 08:55:49.168977: val_loss -0.5346
2025-10-15 08:55:49.169274: Pseudo dice [np.float32(0.7436)]
2025-10-15 08:55:49.169546: Epoch time: 45.92 s
2025-10-15 08:55:49.169794: Yayy! New best EMA pseudo Dice: 0.7419000267982483
2025-10-15 08:55:50.258673: 
2025-10-15 08:55:50.258966: Epoch 109
2025-10-15 08:55:50.259173: Current learning rate: 0.00311
2025-10-15 08:56:36.210648: Validation loss did not improve from -0.57923. Patience: 24/50
2025-10-15 08:56:36.211169: train_loss -0.7623
2025-10-15 08:56:36.211451: val_loss -0.5259
2025-10-15 08:56:36.211639: Pseudo dice [np.float32(0.7394)]
2025-10-15 08:56:36.211859: Epoch time: 45.95 s
2025-10-15 08:56:37.283667: 
2025-10-15 08:56:37.283948: Epoch 110
2025-10-15 08:56:37.284159: Current learning rate: 0.00304
2025-10-15 08:57:23.216277: Validation loss did not improve from -0.57923. Patience: 25/50
2025-10-15 08:57:23.216924: train_loss -0.7617
2025-10-15 08:57:23.217167: val_loss -0.5546
2025-10-15 08:57:23.217312: Pseudo dice [np.float32(0.7515)]
2025-10-15 08:57:23.217464: Epoch time: 45.93 s
2025-10-15 08:57:23.217596: Yayy! New best EMA pseudo Dice: 0.7426000237464905
2025-10-15 08:57:24.286119: 
2025-10-15 08:57:24.286439: Epoch 111
2025-10-15 08:57:24.286652: Current learning rate: 0.00297
2025-10-15 08:58:10.175552: Validation loss did not improve from -0.57923. Patience: 26/50
2025-10-15 08:58:10.176035: train_loss -0.7654
2025-10-15 08:58:10.176218: val_loss -0.5287
2025-10-15 08:58:10.176354: Pseudo dice [np.float32(0.7468)]
2025-10-15 08:58:10.176513: Epoch time: 45.89 s
2025-10-15 08:58:10.176669: Yayy! New best EMA pseudo Dice: 0.7429999709129333
2025-10-15 08:58:11.250570: 
2025-10-15 08:58:11.250966: Epoch 112
2025-10-15 08:58:11.251154: Current learning rate: 0.00291
2025-10-15 08:58:57.246768: Validation loss did not improve from -0.57923. Patience: 27/50
2025-10-15 08:58:57.247441: train_loss -0.7661
2025-10-15 08:58:57.247652: val_loss -0.5433
2025-10-15 08:58:57.247864: Pseudo dice [np.float32(0.7375)]
2025-10-15 08:58:57.248004: Epoch time: 46.0 s
2025-10-15 08:58:57.876956: 
2025-10-15 08:58:57.877290: Epoch 113
2025-10-15 08:58:57.877478: Current learning rate: 0.00284
2025-10-15 08:59:43.812263: Validation loss improved from -0.57923 to -0.58053! Patience: 27/50
2025-10-15 08:59:43.812673: train_loss -0.768
2025-10-15 08:59:43.812888: val_loss -0.5805
2025-10-15 08:59:43.813028: Pseudo dice [np.float32(0.7639)]
2025-10-15 08:59:43.813187: Epoch time: 45.94 s
2025-10-15 08:59:43.813321: Yayy! New best EMA pseudo Dice: 0.7445999979972839
2025-10-15 08:59:44.901875: 
2025-10-15 08:59:44.902165: Epoch 114
2025-10-15 08:59:44.902399: Current learning rate: 0.00277
2025-10-15 09:00:30.861386: Validation loss did not improve from -0.58053. Patience: 1/50
2025-10-15 09:00:30.862031: train_loss -0.7652
2025-10-15 09:00:30.862209: val_loss -0.541
2025-10-15 09:00:30.862336: Pseudo dice [np.float32(0.7422)]
2025-10-15 09:00:30.862477: Epoch time: 45.96 s
2025-10-15 09:00:31.947383: 
2025-10-15 09:00:31.947709: Epoch 115
2025-10-15 09:00:31.947918: Current learning rate: 0.0027
2025-10-15 09:01:18.032464: Validation loss did not improve from -0.58053. Patience: 2/50
2025-10-15 09:01:18.032830: train_loss -0.7676
2025-10-15 09:01:18.033012: val_loss -0.5409
2025-10-15 09:01:18.033171: Pseudo dice [np.float32(0.7375)]
2025-10-15 09:01:18.033348: Epoch time: 46.09 s
2025-10-15 09:01:18.671527: 
2025-10-15 09:01:18.671854: Epoch 116
2025-10-15 09:01:18.672033: Current learning rate: 0.00263
2025-10-15 09:02:04.791498: Validation loss did not improve from -0.58053. Patience: 3/50
2025-10-15 09:02:04.792304: train_loss -0.7679
2025-10-15 09:02:04.792565: val_loss -0.5601
2025-10-15 09:02:04.792800: Pseudo dice [np.float32(0.7525)]
2025-10-15 09:02:04.793059: Epoch time: 46.12 s
2025-10-15 09:02:05.427018: 
2025-10-15 09:02:05.427298: Epoch 117
2025-10-15 09:02:05.427527: Current learning rate: 0.00256
2025-10-15 09:02:51.365605: Validation loss did not improve from -0.58053. Patience: 4/50
2025-10-15 09:02:51.365987: train_loss -0.767
2025-10-15 09:02:51.366178: val_loss -0.545
2025-10-15 09:02:51.366311: Pseudo dice [np.float32(0.7512)]
2025-10-15 09:02:51.366458: Epoch time: 45.94 s
2025-10-15 09:02:51.366610: Yayy! New best EMA pseudo Dice: 0.745199978351593
2025-10-15 09:02:52.447638: 
2025-10-15 09:02:52.447919: Epoch 118
2025-10-15 09:02:52.448144: Current learning rate: 0.00249
2025-10-15 09:03:38.450304: Validation loss did not improve from -0.58053. Patience: 5/50
2025-10-15 09:03:38.451023: train_loss -0.7677
2025-10-15 09:03:38.451229: val_loss -0.5336
2025-10-15 09:03:38.451380: Pseudo dice [np.float32(0.735)]
2025-10-15 09:03:38.451539: Epoch time: 46.0 s
2025-10-15 09:03:39.090182: 
2025-10-15 09:03:39.090466: Epoch 119
2025-10-15 09:03:39.090651: Current learning rate: 0.00242
2025-10-15 09:04:25.122190: Validation loss did not improve from -0.58053. Patience: 6/50
2025-10-15 09:04:25.122645: train_loss -0.7705
2025-10-15 09:04:25.122822: val_loss -0.5261
2025-10-15 09:04:25.122969: Pseudo dice [np.float32(0.7408)]
2025-10-15 09:04:25.123130: Epoch time: 46.03 s
2025-10-15 09:04:26.604324: 
2025-10-15 09:04:26.604696: Epoch 120
2025-10-15 09:04:26.604918: Current learning rate: 0.00235
2025-10-15 09:05:12.523336: Validation loss did not improve from -0.58053. Patience: 7/50
2025-10-15 09:05:12.523891: train_loss -0.7691
2025-10-15 09:05:12.524098: val_loss -0.5485
2025-10-15 09:05:12.524310: Pseudo dice [np.float32(0.7512)]
2025-10-15 09:05:12.524449: Epoch time: 45.92 s
2025-10-15 09:05:13.162929: 
2025-10-15 09:05:13.163220: Epoch 121
2025-10-15 09:05:13.163402: Current learning rate: 0.00228
2025-10-15 09:05:59.084574: Validation loss did not improve from -0.58053. Patience: 8/50
2025-10-15 09:05:59.085063: train_loss -0.772
2025-10-15 09:05:59.085306: val_loss -0.5581
2025-10-15 09:05:59.085436: Pseudo dice [np.float32(0.7554)]
2025-10-15 09:05:59.085577: Epoch time: 45.92 s
2025-10-15 09:05:59.085704: Yayy! New best EMA pseudo Dice: 0.7457000017166138
2025-10-15 09:06:00.182411: 
2025-10-15 09:06:00.182698: Epoch 122
2025-10-15 09:06:00.182872: Current learning rate: 0.00221
2025-10-15 09:06:46.109788: Validation loss did not improve from -0.58053. Patience: 9/50
2025-10-15 09:06:46.110713: train_loss -0.7701
2025-10-15 09:06:46.110985: val_loss -0.5526
2025-10-15 09:06:46.111185: Pseudo dice [np.float32(0.7482)]
2025-10-15 09:06:46.111351: Epoch time: 45.93 s
2025-10-15 09:06:46.111499: Yayy! New best EMA pseudo Dice: 0.7458999752998352
2025-10-15 09:06:47.195117: 
2025-10-15 09:06:47.195432: Epoch 123
2025-10-15 09:06:47.195610: Current learning rate: 0.00214
2025-10-15 09:07:33.089046: Validation loss did not improve from -0.58053. Patience: 10/50
2025-10-15 09:07:33.089480: train_loss -0.775
2025-10-15 09:07:33.089689: val_loss -0.5591
2025-10-15 09:07:33.089846: Pseudo dice [np.float32(0.7554)]
2025-10-15 09:07:33.090026: Epoch time: 45.9 s
2025-10-15 09:07:33.090222: Yayy! New best EMA pseudo Dice: 0.7469000220298767
2025-10-15 09:07:34.183185: 
2025-10-15 09:07:34.183492: Epoch 124
2025-10-15 09:07:34.183660: Current learning rate: 0.00207
2025-10-15 09:08:20.104028: Validation loss improved from -0.58053 to -0.59120! Patience: 10/50
2025-10-15 09:08:20.104593: train_loss -0.7761
2025-10-15 09:08:20.104755: val_loss -0.5912
2025-10-15 09:08:20.104911: Pseudo dice [np.float32(0.77)]
2025-10-15 09:08:20.105087: Epoch time: 45.92 s
2025-10-15 09:08:20.565255: Yayy! New best EMA pseudo Dice: 0.7491999864578247
2025-10-15 09:08:21.642971: 
2025-10-15 09:08:21.643243: Epoch 125
2025-10-15 09:08:21.643493: Current learning rate: 0.00199
2025-10-15 09:09:07.612507: Validation loss did not improve from -0.59120. Patience: 1/50
2025-10-15 09:09:07.612922: train_loss -0.7735
2025-10-15 09:09:07.613154: val_loss -0.5439
2025-10-15 09:09:07.613425: Pseudo dice [np.float32(0.7455)]
2025-10-15 09:09:07.613714: Epoch time: 45.97 s
2025-10-15 09:09:08.250039: 
2025-10-15 09:09:08.250302: Epoch 126
2025-10-15 09:09:08.250626: Current learning rate: 0.00192
2025-10-15 09:09:54.242655: Validation loss did not improve from -0.59120. Patience: 2/50
2025-10-15 09:09:54.243495: train_loss -0.7764
2025-10-15 09:09:54.243718: val_loss -0.5301
2025-10-15 09:09:54.243940: Pseudo dice [np.float32(0.7421)]
2025-10-15 09:09:54.244161: Epoch time: 45.99 s
2025-10-15 09:09:54.879297: 
2025-10-15 09:09:54.879613: Epoch 127
2025-10-15 09:09:54.879858: Current learning rate: 0.00185
2025-10-15 09:10:40.848771: Validation loss did not improve from -0.59120. Patience: 3/50
2025-10-15 09:10:40.849245: train_loss -0.7774
2025-10-15 09:10:40.849423: val_loss -0.5412
2025-10-15 09:10:40.849559: Pseudo dice [np.float32(0.7511)]
2025-10-15 09:10:40.849704: Epoch time: 45.97 s
2025-10-15 09:10:41.487549: 
2025-10-15 09:10:41.487791: Epoch 128
2025-10-15 09:10:41.488028: Current learning rate: 0.00178
2025-10-15 09:11:27.479799: Validation loss did not improve from -0.59120. Patience: 4/50
2025-10-15 09:11:27.480434: train_loss -0.777
2025-10-15 09:11:27.480653: val_loss -0.5598
2025-10-15 09:11:27.480826: Pseudo dice [np.float32(0.7536)]
2025-10-15 09:11:27.481065: Epoch time: 45.99 s
2025-10-15 09:11:28.111530: 
2025-10-15 09:11:28.111863: Epoch 129
2025-10-15 09:11:28.112088: Current learning rate: 0.0017
2025-10-15 09:12:14.070270: Validation loss did not improve from -0.59120. Patience: 5/50
2025-10-15 09:12:14.070776: train_loss -0.7793
2025-10-15 09:12:14.071018: val_loss -0.5513
2025-10-15 09:12:14.071306: Pseudo dice [np.float32(0.7587)]
2025-10-15 09:12:14.071478: Epoch time: 45.96 s
2025-10-15 09:12:14.534665: Yayy! New best EMA pseudo Dice: 0.7498999834060669
2025-10-15 09:12:15.602263: 
2025-10-15 09:12:15.602618: Epoch 130
2025-10-15 09:12:15.602883: Current learning rate: 0.00163
2025-10-15 09:13:01.590682: Validation loss did not improve from -0.59120. Patience: 6/50
2025-10-15 09:13:01.591729: train_loss -0.7727
2025-10-15 09:13:01.592160: val_loss -0.5717
2025-10-15 09:13:01.592495: Pseudo dice [np.float32(0.7624)]
2025-10-15 09:13:01.592833: Epoch time: 45.99 s
2025-10-15 09:13:01.593104: Yayy! New best EMA pseudo Dice: 0.7512000203132629
2025-10-15 09:13:02.668686: 
2025-10-15 09:13:02.668948: Epoch 131
2025-10-15 09:13:02.669139: Current learning rate: 0.00156
2025-10-15 09:13:48.646498: Validation loss did not improve from -0.59120. Patience: 7/50
2025-10-15 09:13:48.647052: train_loss -0.7798
2025-10-15 09:13:48.647254: val_loss -0.5531
2025-10-15 09:13:48.647392: Pseudo dice [np.float32(0.7503)]
2025-10-15 09:13:48.647609: Epoch time: 45.98 s
2025-10-15 09:13:49.279313: 
2025-10-15 09:13:49.279686: Epoch 132
2025-10-15 09:13:49.279903: Current learning rate: 0.00148
2025-10-15 09:14:35.278967: Validation loss did not improve from -0.59120. Patience: 8/50
2025-10-15 09:14:35.279638: train_loss -0.7798
2025-10-15 09:14:35.279811: val_loss -0.5679
2025-10-15 09:14:35.279954: Pseudo dice [np.float32(0.7651)]
2025-10-15 09:14:35.280142: Epoch time: 46.0 s
2025-10-15 09:14:35.280265: Yayy! New best EMA pseudo Dice: 0.7524999976158142
2025-10-15 09:14:36.359804: 
2025-10-15 09:14:36.360059: Epoch 133
2025-10-15 09:14:36.360315: Current learning rate: 0.00141
2025-10-15 09:15:22.350255: Validation loss did not improve from -0.59120. Patience: 9/50
2025-10-15 09:15:22.350671: train_loss -0.78
2025-10-15 09:15:22.350862: val_loss -0.53
2025-10-15 09:15:22.351149: Pseudo dice [np.float32(0.7404)]
2025-10-15 09:15:22.351311: Epoch time: 45.99 s
2025-10-15 09:15:22.985832: 
2025-10-15 09:15:22.986130: Epoch 134
2025-10-15 09:15:22.986304: Current learning rate: 0.00133
2025-10-15 09:16:09.037862: Validation loss did not improve from -0.59120. Patience: 10/50
2025-10-15 09:16:09.038607: train_loss -0.7792
2025-10-15 09:16:09.038880: val_loss -0.5565
2025-10-15 09:16:09.039051: Pseudo dice [np.float32(0.7526)]
2025-10-15 09:16:09.039277: Epoch time: 46.05 s
2025-10-15 09:16:10.514596: 
2025-10-15 09:16:10.515039: Epoch 135
2025-10-15 09:16:10.515272: Current learning rate: 0.00126
2025-10-15 09:16:56.486472: Validation loss did not improve from -0.59120. Patience: 11/50
2025-10-15 09:16:56.486968: train_loss -0.7783
2025-10-15 09:16:56.487241: val_loss -0.5631
2025-10-15 09:16:56.487476: Pseudo dice [np.float32(0.7544)]
2025-10-15 09:16:56.487761: Epoch time: 45.97 s
2025-10-15 09:16:57.127243: 
2025-10-15 09:16:57.127495: Epoch 136
2025-10-15 09:16:57.127680: Current learning rate: 0.00118
2025-10-15 09:17:43.038852: Validation loss did not improve from -0.59120. Patience: 12/50
2025-10-15 09:17:43.039713: train_loss -0.7844
2025-10-15 09:17:43.039929: val_loss -0.5518
2025-10-15 09:17:43.040195: Pseudo dice [np.float32(0.7561)]
2025-10-15 09:17:43.040518: Epoch time: 45.91 s
2025-10-15 09:17:43.680600: 
2025-10-15 09:17:43.681019: Epoch 137
2025-10-15 09:17:43.681252: Current learning rate: 0.00111
2025-10-15 09:18:29.633368: Validation loss did not improve from -0.59120. Patience: 13/50
2025-10-15 09:18:29.633908: train_loss -0.7839
2025-10-15 09:18:29.634226: val_loss -0.54
2025-10-15 09:18:29.634506: Pseudo dice [np.float32(0.7457)]
2025-10-15 09:18:29.634777: Epoch time: 45.95 s
2025-10-15 09:18:30.270467: 
2025-10-15 09:18:30.270945: Epoch 138
2025-10-15 09:18:30.271264: Current learning rate: 0.00103
2025-10-15 09:19:16.173154: Validation loss did not improve from -0.59120. Patience: 14/50
2025-10-15 09:19:16.173689: train_loss -0.7857
2025-10-15 09:19:16.173838: val_loss -0.5617
2025-10-15 09:19:16.174078: Pseudo dice [np.float32(0.7576)]
2025-10-15 09:19:16.174251: Epoch time: 45.9 s
2025-10-15 09:19:16.808647: 
2025-10-15 09:19:16.808972: Epoch 139
2025-10-15 09:19:16.809234: Current learning rate: 0.00095
2025-10-15 09:20:02.775068: Validation loss did not improve from -0.59120. Patience: 15/50
2025-10-15 09:20:02.775476: train_loss -0.786
2025-10-15 09:20:02.775636: val_loss -0.5714
2025-10-15 09:20:02.775773: Pseudo dice [np.float32(0.7661)]
2025-10-15 09:20:02.775905: Epoch time: 45.97 s
2025-10-15 09:20:03.240595: Yayy! New best EMA pseudo Dice: 0.7534999847412109
2025-10-15 09:20:04.329475: 
2025-10-15 09:20:04.329811: Epoch 140
2025-10-15 09:20:04.330010: Current learning rate: 0.00087
2025-10-15 09:20:50.233979: Validation loss did not improve from -0.59120. Patience: 16/50
2025-10-15 09:20:50.234576: train_loss -0.7849
2025-10-15 09:20:50.234733: val_loss -0.5511
2025-10-15 09:20:50.234852: Pseudo dice [np.float32(0.7455)]
2025-10-15 09:20:50.235202: Epoch time: 45.91 s
2025-10-15 09:20:50.873859: 
2025-10-15 09:20:50.874124: Epoch 141
2025-10-15 09:20:50.874352: Current learning rate: 0.00079
2025-10-15 09:21:36.796261: Validation loss did not improve from -0.59120. Patience: 17/50
2025-10-15 09:21:36.796715: train_loss -0.7831
2025-10-15 09:21:36.796891: val_loss -0.5664
2025-10-15 09:21:36.797089: Pseudo dice [np.float32(0.7589)]
2025-10-15 09:21:36.797253: Epoch time: 45.92 s
2025-10-15 09:21:37.433535: 
2025-10-15 09:21:37.433836: Epoch 142
2025-10-15 09:21:37.434005: Current learning rate: 0.00071
2025-10-15 09:22:23.341048: Validation loss did not improve from -0.59120. Patience: 18/50
2025-10-15 09:22:23.341689: train_loss -0.7865
2025-10-15 09:22:23.341834: val_loss -0.5566
2025-10-15 09:22:23.341950: Pseudo dice [np.float32(0.7555)]
2025-10-15 09:22:23.342095: Epoch time: 45.91 s
2025-10-15 09:22:23.342233: Yayy! New best EMA pseudo Dice: 0.7534999847412109
2025-10-15 09:22:24.433770: 
2025-10-15 09:22:24.434095: Epoch 143
2025-10-15 09:22:24.434287: Current learning rate: 0.00063
2025-10-15 09:23:10.374583: Validation loss did not improve from -0.59120. Patience: 19/50
2025-10-15 09:23:10.375097: train_loss -0.7872
2025-10-15 09:23:10.375368: val_loss -0.5363
2025-10-15 09:23:10.375624: Pseudo dice [np.float32(0.7428)]
2025-10-15 09:23:10.375890: Epoch time: 45.94 s
2025-10-15 09:23:11.042008: 
2025-10-15 09:23:11.042337: Epoch 144
2025-10-15 09:23:11.042524: Current learning rate: 0.00055
2025-10-15 09:23:56.964766: Validation loss did not improve from -0.59120. Patience: 20/50
2025-10-15 09:23:56.965443: train_loss -0.7895
2025-10-15 09:23:56.965626: val_loss -0.5582
2025-10-15 09:23:56.965768: Pseudo dice [np.float32(0.7526)]
2025-10-15 09:23:56.965922: Epoch time: 45.92 s
2025-10-15 09:23:58.109526: 
2025-10-15 09:23:58.109797: Epoch 145
2025-10-15 09:23:58.110007: Current learning rate: 0.00047
2025-10-15 09:24:44.047155: Validation loss did not improve from -0.59120. Patience: 21/50
2025-10-15 09:24:44.047656: train_loss -0.788
2025-10-15 09:24:44.047885: val_loss -0.5419
2025-10-15 09:24:44.048079: Pseudo dice [np.float32(0.7495)]
2025-10-15 09:24:44.048341: Epoch time: 45.94 s
2025-10-15 09:24:44.710462: 
2025-10-15 09:24:44.710802: Epoch 146
2025-10-15 09:24:44.711002: Current learning rate: 0.00038
2025-10-15 09:25:30.720519: Validation loss did not improve from -0.59120. Patience: 22/50
2025-10-15 09:25:30.721114: train_loss -0.7893
2025-10-15 09:25:30.721316: val_loss -0.5588
2025-10-15 09:25:30.721559: Pseudo dice [np.float32(0.7571)]
2025-10-15 09:25:30.721791: Epoch time: 46.01 s
2025-10-15 09:25:31.357951: 
2025-10-15 09:25:31.358352: Epoch 147
2025-10-15 09:25:31.358654: Current learning rate: 0.0003
2025-10-15 09:26:17.325751: Validation loss did not improve from -0.59120. Patience: 23/50
2025-10-15 09:26:17.326169: train_loss -0.7873
2025-10-15 09:26:17.326359: val_loss -0.5682
2025-10-15 09:26:17.326527: Pseudo dice [np.float32(0.7566)]
2025-10-15 09:26:17.326681: Epoch time: 45.97 s
2025-10-15 09:26:17.967825: 
2025-10-15 09:26:17.968132: Epoch 148
2025-10-15 09:26:17.968344: Current learning rate: 0.00021
2025-10-15 09:27:03.945192: Validation loss did not improve from -0.59120. Patience: 24/50
2025-10-15 09:27:03.945742: train_loss -0.788
2025-10-15 09:27:03.945890: val_loss -0.5401
2025-10-15 09:27:03.946068: Pseudo dice [np.float32(0.7463)]
2025-10-15 09:27:03.946260: Epoch time: 45.98 s
2025-10-15 09:27:04.588609: 
2025-10-15 09:27:04.588877: Epoch 149
2025-10-15 09:27:04.589042: Current learning rate: 0.00011
2025-10-15 09:27:50.590259: Validation loss did not improve from -0.59120. Patience: 25/50
2025-10-15 09:27:50.590644: train_loss -0.7882
2025-10-15 09:27:50.590794: val_loss -0.5589
2025-10-15 09:27:50.590914: Pseudo dice [np.float32(0.755)]
2025-10-15 09:27:50.591063: Epoch time: 46.0 s
2025-10-15 09:27:51.749796: Training done.
2025-10-15 09:27:51.780925: Using splits from existing split file: /nfs/erelab001/shared/Computational_Group/Naravich/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset307_Sohee_Calcium_OCT_CrossValidation/splits_final_80.json
2025-10-15 09:27:51.781328: The split file contains 5 splits.
2025-10-15 09:27:51.781523: Desired fold for training: 2
2025-10-15 09:27:51.781794: This split has 6 training and 3 validation cases.
2025-10-15 09:27:51.782231: predicting 101-045
2025-10-15 09:27:51.785237: 101-045, shape torch.Size([1, 375, 498, 498]), rank 0
2025-10-15 09:28:38.647643: predicting 401-004
2025-10-15 09:28:38.655376: 401-004, shape torch.Size([1, 375, 498, 498]), rank 0
2025-10-15 09:29:12.376078: predicting 704-003
2025-10-15 09:29:12.382923: 704-003, shape torch.Size([1, 375, 498, 498]), rank 0
2025-10-15 09:29:58.716638: Validation complete
2025-10-15 09:29:58.716910: Mean Validation Dice:  0.7386082302844391
Finished training fold 2 saving to /nfs/erelab001/shared/Computational_Group/Naravich/datasets/nnUNet_Datasets/nnUNet_results/Dataset307_Sohee_Calcium_OCT_CrossValidation/nnUNetTrainerScaleAnalysis80__nnUNetPlans__3d_32x160x128_b10/fold_2_Genesis_Pretrained
