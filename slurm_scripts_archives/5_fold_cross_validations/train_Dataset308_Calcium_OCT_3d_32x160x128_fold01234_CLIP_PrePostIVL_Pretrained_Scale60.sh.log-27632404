/home/gridsan/nchutisilp/.conda/envs/nnunet/bin/python
Starting training with CONFIG=3d_32x160x128_b10, DATASET_ID=308, TRAINER=nnUNetTrainerScaleAnalysis60

############################
INFO: You are using the old nnU-Net default plans. We have updated our recommendations. Please consider using those instead! Read more here: https://github.com/MIC-DKFZ/nnUNet/blob/master/documentation/resenc_presets.md
############################

Using device: cuda:0

#######################################################################
Please cite the following paper when using nnU-Net:
Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.
#######################################################################

2024-12-19 19:40:11.804619: Using torch.compile...

############################
INFO: You are using the old nnU-Net default plans. We have updated our recommendations. Please consider using those instead! Read more here: https://github.com/MIC-DKFZ/nnUNet/blob/master/documentation/resenc_presets.md
############################

Using device: cuda:0

#######################################################################
Please cite the following paper when using nnU-Net:
Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.
#######################################################################

2024-12-19 19:40:11.801348: Using torch.compile...
/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
################### Loading pretrained weights from file  /home/gridsan/nchutisilp/projects/OpenAI-CLIP/clip_pretrained_nnUNet.pt ###################
Below is the list of overlapping blocks in pretrained model and nnUNet architecture:
encoder.stages.0.0.convs.0.conv.weight shape torch.Size([32, 1, 3, 3, 3])
encoder.stages.0.0.convs.0.conv.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.norm.weight shape torch.Size([32])
encoder.stages.0.0.convs.0.norm.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.0.weight shape torch.Size([32, 1, 3, 3, 3])
encoder.stages.0.0.convs.0.all_modules.0.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.1.weight shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.1.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
encoder.stages.0.0.convs.1.conv.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.norm.weight shape torch.Size([32])
encoder.stages.0.0.convs.1.norm.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
encoder.stages.0.0.convs.1.all_modules.0.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.1.weight shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.1.bias shape torch.Size([32])
encoder.stages.1.0.convs.0.conv.weight shape torch.Size([64, 32, 3, 3, 3])
encoder.stages.1.0.convs.0.conv.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.norm.weight shape torch.Size([64])
encoder.stages.1.0.convs.0.norm.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.0.weight shape torch.Size([64, 32, 3, 3, 3])
encoder.stages.1.0.convs.0.all_modules.0.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.1.weight shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.1.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
encoder.stages.1.0.convs.1.conv.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.norm.weight shape torch.Size([64])
encoder.stages.1.0.convs.1.norm.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
encoder.stages.1.0.convs.1.all_modules.0.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.1.weight shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.1.bias shape torch.Size([64])
encoder.stages.2.0.convs.0.conv.weight shape torch.Size([128, 64, 3, 3, 3])
encoder.stages.2.0.convs.0.conv.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.norm.weight shape torch.Size([128])
encoder.stages.2.0.convs.0.norm.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.0.weight shape torch.Size([128, 64, 3, 3, 3])
encoder.stages.2.0.convs.0.all_modules.0.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.1.weight shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.1.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
encoder.stages.2.0.convs.1.conv.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.norm.weight shape torch.Size([128])
encoder.stages.2.0.convs.1.norm.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
encoder.stages.2.0.convs.1.all_modules.0.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.1.weight shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.1.bias shape torch.Size([128])
encoder.stages.3.0.convs.0.conv.weight shape torch.Size([256, 128, 3, 3, 3])
encoder.stages.3.0.convs.0.conv.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.norm.weight shape torch.Size([256])
encoder.stages.3.0.convs.0.norm.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.0.weight shape torch.Size([256, 128, 3, 3, 3])
encoder.stages.3.0.convs.0.all_modules.0.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.1.weight shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.1.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
encoder.stages.3.0.convs.1.conv.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.norm.weight shape torch.Size([256])
encoder.stages.3.0.convs.1.norm.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
encoder.stages.3.0.convs.1.all_modules.0.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.1.weight shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.1.bias shape torch.Size([256])
encoder.stages.4.0.convs.0.conv.weight shape torch.Size([320, 256, 3, 3, 3])
encoder.stages.4.0.convs.0.conv.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.norm.weight shape torch.Size([320])
encoder.stages.4.0.convs.0.norm.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.0.weight shape torch.Size([320, 256, 3, 3, 3])
encoder.stages.4.0.convs.0.all_modules.0.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.1.weight shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.1.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.4.0.convs.1.conv.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.norm.weight shape torch.Size([320])
encoder.stages.4.0.convs.1.norm.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.4.0.convs.1.all_modules.0.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.1.weight shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.1.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.0.conv.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.norm.weight shape torch.Size([320])
encoder.stages.5.0.convs.0.norm.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.0.all_modules.0.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.1.weight shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.1.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.1.conv.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.norm.weight shape torch.Size([320])
encoder.stages.5.0.convs.1.norm.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.1.all_modules.0.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.1.weight shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.0.0.convs.0.conv.weight shape torch.Size([32, 1, 3, 3, 3])
decoder.encoder.stages.0.0.convs.0.conv.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.norm.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.norm.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.0.weight shape torch.Size([32, 1, 3, 3, 3])
decoder.encoder.stages.0.0.convs.0.all_modules.0.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.1.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.1.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.encoder.stages.0.0.convs.1.conv.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.norm.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.norm.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.encoder.stages.0.0.convs.1.all_modules.0.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.1.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.1.bias shape torch.Size([32])
decoder.encoder.stages.1.0.convs.0.conv.weight shape torch.Size([64, 32, 3, 3, 3])
decoder.encoder.stages.1.0.convs.0.conv.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.norm.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.norm.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.0.weight shape torch.Size([64, 32, 3, 3, 3])
decoder.encoder.stages.1.0.convs.0.all_modules.0.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.1.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.1.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.encoder.stages.1.0.convs.1.conv.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.norm.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.norm.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.encoder.stages.1.0.convs.1.all_modules.0.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.1.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.1.bias shape torch.Size([64])
decoder.encoder.stages.2.0.convs.0.conv.weight shape torch.Size([128, 64, 3, 3, 3])
decoder.encoder.stages.2.0.convs.0.conv.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.norm.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.norm.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.0.weight shape torch.Size([128, 64, 3, 3, 3])
decoder.encoder.stages.2.0.convs.0.all_modules.0.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.1.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.1.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.encoder.stages.2.0.convs.1.conv.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.norm.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.norm.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.encoder.stages.2.0.convs.1.all_modules.0.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.1.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.1.bias shape torch.Size([128])
decoder.encoder.stages.3.0.convs.0.conv.weight shape torch.Size([256, 128, 3, 3, 3])
decoder.encoder.stages.3.0.convs.0.conv.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.norm.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.norm.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.0.weight shape torch.Size([256, 128, 3, 3, 3])
decoder.encoder.stages.3.0.convs.0.all_modules.0.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.1.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.1.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.encoder.stages.3.0.convs.1.conv.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.norm.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.norm.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.encoder.stages.3.0.convs.1.all_modules.0.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.1.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.1.bias shape torch.Size([256])
decoder.encoder.stages.4.0.convs.0.conv.weight shape torch.Size([320, 256, 3, 3, 3])
decoder.encoder.stages.4.0.convs.0.conv.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.norm.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.norm.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.0.weight shape torch.Size([320, 256, 3, 3, 3])
decoder.encoder.stages.4.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.4.0.convs.1.conv.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.norm.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.norm.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.4.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.0.conv.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.norm.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.norm.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.1.conv.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.norm.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.norm.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.stages.0.convs.0.conv.weight shape torch.Size([320, 640, 3, 3, 3])
decoder.stages.0.convs.0.conv.bias shape torch.Size([320])
decoder.stages.0.convs.0.norm.weight shape torch.Size([320])
decoder.stages.0.convs.0.norm.bias shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.0.weight shape torch.Size([320, 640, 3, 3, 3])
decoder.stages.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.stages.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.stages.0.convs.1.conv.bias shape torch.Size([320])
decoder.stages.0.convs.1.norm.weight shape torch.Size([320])
decoder.stages.0.convs.1.norm.bias shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.stages.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.stages.1.convs.0.conv.weight shape torch.Size([256, 512, 3, 3, 3])
decoder.stages.1.convs.0.conv.bias shape torch.Size([256])
decoder.stages.1.convs.0.norm.weight shape torch.Size([256])
decoder.stages.1.convs.0.norm.bias shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.0.weight shape torch.Size([256, 512, 3, 3, 3])
decoder.stages.1.convs.0.all_modules.0.bias shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.1.weight shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.1.bias shape torch.Size([256])
decoder.stages.1.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.stages.1.convs.1.conv.bias shape torch.Size([256])
decoder.stages.1.convs.1.norm.weight shape torch.Size([256])
decoder.stages.1.convs.1.norm.bias shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.stages.1.convs.1.all_modules.0.bias shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.1.weight shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.1.bias shape torch.Size([256])
decoder.stages.2.convs.0.conv.weight shape torch.Size([128, 256, 3, 3, 3])
decoder.stages.2.convs.0.conv.bias shape torch.Size([128])
decoder.stages.2.convs.0.norm.weight shape torch.Size([128])
decoder.stages.2.convs.0.norm.bias shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.0.weight shape torch.Size([128, 256, 3, 3, 3])
decoder.stages.2.convs.0.all_modules.0.bias shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.1.weight shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.1.bias shape torch.Size([128])
decoder.stages.2.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.stages.2.convs.1.conv.bias shape torch.Size([128])
decoder.stages.2.convs.1.norm.weight shape torch.Size([128])
decoder.stages.2.convs.1.norm.bias shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.stages.2.convs.1.all_modules.0.bias shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.1.weight shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.1.bias shape torch.Size([128])
decoder.stages.3.convs.0.conv.weight shape torch.Size([64, 128, 3, 3, 3])
decoder.stages.3.convs.0.conv.bias shape torch.Size([64])
decoder.stages.3.convs.0.norm.weight shape torch.Size([64])
decoder.stages.3.convs.0.norm.bias shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.0.weight shape torch.Size([64, 128, 3, 3, 3])
decoder.stages.3.convs.0.all_modules.0.bias shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.1.weight shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.1.bias shape torch.Size([64])
decoder.stages.3.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.stages.3.convs.1.conv.bias shape torch.Size([64])
decoder.stages.3.convs.1.norm.weight shape torch.Size([64])
decoder.stages.3.convs.1.norm.bias shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.stages.3.convs.1.all_modules.0.bias shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.1.weight shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.1.bias shape torch.Size([64])
decoder.stages.4.convs.0.conv.weight shape torch.Size([32, 64, 3, 3, 3])
decoder.stages.4.convs.0.conv.bias shape torch.Size([32])
decoder.stages.4.convs.0.norm.weight shape torch.Size([32])
decoder.stages.4.convs.0.norm.bias shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.0.weight shape torch.Size([32, 64, 3, 3, 3])
decoder.stages.4.convs.0.all_modules.0.bias shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.1.weight shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.1.bias shape torch.Size([32])
decoder.stages.4.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.stages.4.convs.1.conv.bias shape torch.Size([32])
decoder.stages.4.convs.1.norm.weight shape torch.Size([32])
decoder.stages.4.convs.1.norm.bias shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.stages.4.convs.1.all_modules.0.bias shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.1.weight shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.1.bias shape torch.Size([32])
decoder.transpconvs.0.weight shape torch.Size([320, 320, 1, 2, 2])
decoder.transpconvs.0.bias shape torch.Size([320])
decoder.transpconvs.1.weight shape torch.Size([320, 256, 2, 2, 2])
decoder.transpconvs.1.bias shape torch.Size([256])
decoder.transpconvs.2.weight shape torch.Size([256, 128, 2, 2, 2])
decoder.transpconvs.2.bias shape torch.Size([128])
decoder.transpconvs.3.weight shape torch.Size([128, 64, 2, 2, 2])
decoder.transpconvs.3.bias shape torch.Size([64])
decoder.transpconvs.4.weight shape torch.Size([64, 32, 2, 2, 2])
decoder.transpconvs.4.bias shape torch.Size([32])
################### Done ###################
2024-12-19 19:40:22.603763: do_dummy_2d_data_aug: True
2024-12-19 19:40:22.633169: Using splits from existing split file: /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset308_Sohee_Calcium_OCT_CrossValidation/splits_final_60.json
2024-12-19 19:40:22.642974: The split file contains 5 splits.
2024-12-19 19:40:22.644974: Desired fold for training: 1
2024-12-19 19:40:22.646067: This split has 4 training and 5 validation cases.
################### Loading pretrained weights from file  /home/gridsan/nchutisilp/projects/OpenAI-CLIP/clip_pretrained_nnUNet.pt ###################
Below is the list of overlapping blocks in pretrained model and nnUNet architecture:
encoder.stages.0.0.convs.0.conv.weight shape torch.Size([32, 1, 3, 3, 3])
encoder.stages.0.0.convs.0.conv.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.norm.weight shape torch.Size([32])
encoder.stages.0.0.convs.0.norm.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.0.weight shape torch.Size([32, 1, 3, 3, 3])
encoder.stages.0.0.convs.0.all_modules.0.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.1.weight shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.1.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
encoder.stages.0.0.convs.1.conv.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.norm.weight shape torch.Size([32])
encoder.stages.0.0.convs.1.norm.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
encoder.stages.0.0.convs.1.all_modules.0.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.1.weight shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.1.bias shape torch.Size([32])
encoder.stages.1.0.convs.0.conv.weight shape torch.Size([64, 32, 3, 3, 3])
encoder.stages.1.0.convs.0.conv.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.norm.weight shape torch.Size([64])
encoder.stages.1.0.convs.0.norm.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.0.weight shape torch.Size([64, 32, 3, 3, 3])
encoder.stages.1.0.convs.0.all_modules.0.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.1.weight shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.1.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
encoder.stages.1.0.convs.1.conv.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.norm.weight shape torch.Size([64])
encoder.stages.1.0.convs.1.norm.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
encoder.stages.1.0.convs.1.all_modules.0.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.1.weight shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.1.bias shape torch.Size([64])
encoder.stages.2.0.convs.0.conv.weight shape torch.Size([128, 64, 3, 3, 3])
encoder.stages.2.0.convs.0.conv.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.norm.weight shape torch.Size([128])
encoder.stages.2.0.convs.0.norm.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.0.weight shape torch.Size([128, 64, 3, 3, 3])
encoder.stages.2.0.convs.0.all_modules.0.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.1.weight shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.1.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
encoder.stages.2.0.convs.1.conv.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.norm.weight shape torch.Size([128])
encoder.stages.2.0.convs.1.norm.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
encoder.stages.2.0.convs.1.all_modules.0.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.1.weight shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.1.bias shape torch.Size([128])
encoder.stages.3.0.convs.0.conv.weight shape torch.Size([256, 128, 3, 3, 3])
encoder.stages.3.0.convs.0.conv.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.norm.weight shape torch.Size([256])
encoder.stages.3.0.convs.0.norm.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.0.weight shape torch.Size([256, 128, 3, 3, 3])
encoder.stages.3.0.convs.0.all_modules.0.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.1.weight shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.1.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
encoder.stages.3.0.convs.1.conv.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.norm.weight shape torch.Size([256])
encoder.stages.3.0.convs.1.norm.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
encoder.stages.3.0.convs.1.all_modules.0.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.1.weight shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.1.bias shape torch.Size([256])
encoder.stages.4.0.convs.0.conv.weight shape torch.Size([320, 256, 3, 3, 3])
encoder.stages.4.0.convs.0.conv.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.norm.weight shape torch.Size([320])
encoder.stages.4.0.convs.0.norm.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.0.weight shape torch.Size([320, 256, 3, 3, 3])
encoder.stages.4.0.convs.0.all_modules.0.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.1.weight shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.1.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.4.0.convs.1.conv.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.norm.weight shape torch.Size([320])
encoder.stages.4.0.convs.1.norm.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.4.0.convs.1.all_modules.0.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.1.weight shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.1.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.0.conv.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.norm.weight shape torch.Size([320])
encoder.stages.5.0.convs.0.norm.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.0.all_modules.0.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.1.weight shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.1.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.1.conv.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.norm.weight shape torch.Size([320])
encoder.stages.5.0.convs.1.norm.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.1.all_modules.0.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.1.weight shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.0.0.convs.0.conv.weight shape torch.Size([32, 1, 3, 3, 3])
decoder.encoder.stages.0.0.convs.0.conv.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.norm.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.norm.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.0.weight shape torch.Size([32, 1, 3, 3, 3])
decoder.encoder.stages.0.0.convs.0.all_modules.0.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.1.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.1.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.encoder.stages.0.0.convs.1.conv.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.norm.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.norm.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.encoder.stages.0.0.convs.1.all_modules.0.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.1.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.1.bias shape torch.Size([32])
decoder.encoder.stages.1.0.convs.0.conv.weight shape torch.Size([64, 32, 3, 3, 3])
decoder.encoder.stages.1.0.convs.0.conv.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.norm.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.norm.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.0.weight shape torch.Size([64, 32, 3, 3, 3])
decoder.encoder.stages.1.0.convs.0.all_modules.0.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.1.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.1.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.encoder.stages.1.0.convs.1.conv.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.norm.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.norm.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.encoder.stages.1.0.convs.1.all_modules.0.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.1.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.1.bias shape torch.Size([64])
decoder.encoder.stages.2.0.convs.0.conv.weight shape torch.Size([128, 64, 3, 3, 3])
decoder.encoder.stages.2.0.convs.0.conv.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.norm.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.norm.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.0.weight shape torch.Size([128, 64, 3, 3, 3])
decoder.encoder.stages.2.0.convs.0.all_modules.0.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.1.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.1.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.encoder.stages.2.0.convs.1.conv.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.norm.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.norm.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.encoder.stages.2.0.convs.1.all_modules.0.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.1.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.1.bias shape torch.Size([128])
decoder.encoder.stages.3.0.convs.0.conv.weight shape torch.Size([256, 128, 3, 3, 3])
decoder.encoder.stages.3.0.convs.0.conv.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.norm.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.norm.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.0.weight shape torch.Size([256, 128, 3, 3, 3])
decoder.encoder.stages.3.0.convs.0.all_modules.0.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.1.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.1.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.encoder.stages.3.0.convs.1.conv.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.norm.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.norm.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.encoder.stages.3.0.convs.1.all_modules.0.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.1.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.1.bias shape torch.Size([256])
decoder.encoder.stages.4.0.convs.0.conv.weight shape torch.Size([320, 256, 3, 3, 3])
decoder.encoder.stages.4.0.convs.0.conv.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.norm.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.norm.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.0.weight shape torch.Size([320, 256, 3, 3, 3])
decoder.encoder.stages.4.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.4.0.convs.1.conv.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.norm.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.norm.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.4.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.0.conv.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.norm.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.norm.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.1.conv.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.norm.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.norm.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.stages.0.convs.0.conv.weight shape torch.Size([320, 640, 3, 3, 3])
decoder.stages.0.convs.0.conv.bias shape torch.Size([320])
decoder.stages.0.convs.0.norm.weight shape torch.Size([320])
decoder.stages.0.convs.0.norm.bias shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.0.weight shape torch.Size([320, 640, 3, 3, 3])
decoder.stages.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.stages.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.stages.0.convs.1.conv.bias shape torch.Size([320])
decoder.stages.0.convs.1.norm.weight shape torch.Size([320])
decoder.stages.0.convs.1.norm.bias shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.stages.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.stages.1.convs.0.conv.weight shape torch.Size([256, 512, 3, 3, 3])
decoder.stages.1.convs.0.conv.bias shape torch.Size([256])
decoder.stages.1.convs.0.norm.weight shape torch.Size([256])
decoder.stages.1.convs.0.norm.bias shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.0.weight shape torch.Size([256, 512, 3, 3, 3])
decoder.stages.1.convs.0.all_modules.0.bias shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.1.weight shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.1.bias shape torch.Size([256])
decoder.stages.1.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.stages.1.convs.1.conv.bias shape torch.Size([256])
decoder.stages.1.convs.1.norm.weight shape torch.Size([256])
decoder.stages.1.convs.1.norm.bias shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.stages.1.convs.1.all_modules.0.bias shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.1.weight shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.1.bias shape torch.Size([256])
decoder.stages.2.convs.0.conv.weight shape torch.Size([128, 256, 3, 3, 3])
decoder.stages.2.convs.0.conv.bias shape torch.Size([128])
decoder.stages.2.convs.0.norm.weight shape torch.Size([128])
decoder.stages.2.convs.0.norm.bias shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.0.weight shape torch.Size([128, 256, 3, 3, 3])
decoder.stages.2.convs.0.all_modules.0.bias shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.1.weight shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.1.bias shape torch.Size([128])
decoder.stages.2.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.stages.2.convs.1.conv.bias shape torch.Size([128])
decoder.stages.2.convs.1.norm.weight shape torch.Size([128])
decoder.stages.2.convs.1.norm.bias shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.stages.2.convs.1.all_modules.0.bias shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.1.weight shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.1.bias shape torch.Size([128])
decoder.stages.3.convs.0.conv.weight shape torch.Size([64, 128, 3, 3, 3])
decoder.stages.3.convs.0.conv.bias shape torch.Size([64])
decoder.stages.3.convs.0.norm.weight shape torch.Size([64])
decoder.stages.3.convs.0.norm.bias shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.0.weight shape torch.Size([64, 128, 3, 3, 3])
decoder.stages.3.convs.0.all_modules.0.bias shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.1.weight shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.1.bias shape torch.Size([64])
decoder.stages.3.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.stages.3.convs.1.conv.bias shape torch.Size([64])
decoder.stages.3.convs.1.norm.weight shape torch.Size([64])
decoder.stages.3.convs.1.norm.bias shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.stages.3.convs.1.all_modules.0.bias shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.1.weight shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.1.bias shape torch.Size([64])
decoder.stages.4.convs.0.conv.weight shape torch.Size([32, 64, 3, 3, 3])
decoder.stages.4.convs.0.conv.bias shape torch.Size([32])
decoder.stages.4.convs.0.norm.weight shape torch.Size([32])
decoder.stages.4.convs.0.norm.bias shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.0.weight shape torch.Size([32, 64, 3, 3, 3])
decoder.stages.4.convs.0.all_modules.0.bias shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.1.weight shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.1.bias shape torch.Size([32])
decoder.stages.4.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.stages.4.convs.1.conv.bias shape torch.Size([32])
decoder.stages.4.convs.1.norm.weight shape torch.Size([32])
decoder.stages.4.convs.1.norm.bias shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.stages.4.convs.1.all_modules.0.bias shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.1.weight shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.1.bias shape torch.Size([32])
decoder.transpconvs.0.weight shape torch.Size([320, 320, 1, 2, 2])
decoder.transpconvs.0.bias shape torch.Size([320])
decoder.transpconvs.1.weight shape torch.Size([320, 256, 2, 2, 2])
decoder.transpconvs.1.bias shape torch.Size([256])
decoder.transpconvs.2.weight shape torch.Size([256, 128, 2, 2, 2])
decoder.transpconvs.2.bias shape torch.Size([128])
decoder.transpconvs.3.weight shape torch.Size([128, 64, 2, 2, 2])
decoder.transpconvs.3.bias shape torch.Size([64])
decoder.transpconvs.4.weight shape torch.Size([64, 32, 2, 2, 2])
decoder.transpconvs.4.bias shape torch.Size([32])
################### Done ###################
2024-12-19 19:40:22.603762: do_dummy_2d_data_aug: True
2024-12-19 19:40:22.633183: Using splits from existing split file: /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset308_Sohee_Calcium_OCT_CrossValidation/splits_final_60.json
2024-12-19 19:40:22.643636: The split file contains 5 splits.
2024-12-19 19:40:22.645260: Desired fold for training: 0
2024-12-19 19:40:22.646769: This split has 4 training and 4 validation cases.
using pin_memory on device 0
using pin_memory on device 0
using pin_memory on device 0

This is the configuration used by this training:
Configuration name: 3d_32x160x128_b10
 {'data_identifier': 'nnUNetPlans_3d_fullres', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 10, 'patch_size': [32, 160, 128], 'median_image_size_in_voxels': [375.0, 498.0, 498.0], 'spacing': [1.0, 1.0, 1.0], 'normalization_schemes': ['ZScoreNormalization'], 'use_mask_for_norm': [False], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'architecture': {'network_class_name': 'dynamic_network_architectures.architectures.unet.PlainConvUNet', 'arch_kwargs': {'n_stages': 6, 'features_per_stage': [32, 64, 128, 256, 320, 320], 'conv_op': 'torch.nn.modules.conv.Conv3d', 'kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]], 'strides': [[1, 1, 1], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [1, 2, 2]], 'n_conv_per_stage': [2, 2, 2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2, 2, 2], 'conv_bias': True, 'norm_op': 'torch.nn.modules.instancenorm.InstanceNorm3d', 'norm_op_kwargs': {'eps': 1e-05, 'affine': True}, 'dropout_op': None, 'dropout_op_kwargs': None, 'nonlin': 'torch.nn.LeakyReLU', 'nonlin_kwargs': {'inplace': True}, 'deep_supervision': True}, '_kw_requires_import': ['conv_op', 'norm_op', 'dropout_op', 'nonlin']}, 'batch_dice': True, 'inherits_from': '3d_fullres'} 

These are the global plan.json settings:
 {'dataset_name': 'Dataset308_Sohee_Calcium_OCT_CrossValidation', 'plans_name': 'nnUNetPlans', 'original_median_spacing_after_transp': [1.0, 1.0, 1.0], 'original_median_shape_after_transp': [375, 498, 498], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 0.4977976381778717, 'mean': 0.13507525622844696, 'median': 0.09599608182907104, 'min': 0.0, 'percentile_00_5': 0.014754901640117168, 'percentile_99_5': 0.4977976381778717, 'std': 0.12025152146816254}}} 

2024-12-19 19:40:51.401855: unpacking dataset...
using pin_memory on device 0

This is the configuration used by this training:
Configuration name: 3d_32x160x128_b10
 {'data_identifier': 'nnUNetPlans_3d_fullres', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 10, 'patch_size': [32, 160, 128], 'median_image_size_in_voxels': [375.0, 498.0, 498.0], 'spacing': [1.0, 1.0, 1.0], 'normalization_schemes': ['ZScoreNormalization'], 'use_mask_for_norm': [False], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'architecture': {'network_class_name': 'dynamic_network_architectures.architectures.unet.PlainConvUNet', 'arch_kwargs': {'n_stages': 6, 'features_per_stage': [32, 64, 128, 256, 320, 320], 'conv_op': 'torch.nn.modules.conv.Conv3d', 'kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]], 'strides': [[1, 1, 1], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [1, 2, 2]], 'n_conv_per_stage': [2, 2, 2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2, 2, 2], 'conv_bias': True, 'norm_op': 'torch.nn.modules.instancenorm.InstanceNorm3d', 'norm_op_kwargs': {'eps': 1e-05, 'affine': True}, 'dropout_op': None, 'dropout_op_kwargs': None, 'nonlin': 'torch.nn.LeakyReLU', 'nonlin_kwargs': {'inplace': True}, 'deep_supervision': True}, '_kw_requires_import': ['conv_op', 'norm_op', 'dropout_op', 'nonlin']}, 'batch_dice': True, 'inherits_from': '3d_fullres'} 

These are the global plan.json settings:
 {'dataset_name': 'Dataset308_Sohee_Calcium_OCT_CrossValidation', 'plans_name': 'nnUNetPlans', 'original_median_spacing_after_transp': [1.0, 1.0, 1.0], 'original_median_shape_after_transp': [375, 498, 498], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 0.4977976381778717, 'mean': 0.13507525622844696, 'median': 0.09599608182907104, 'min': 0.0, 'percentile_00_5': 0.014754901640117168, 'percentile_99_5': 0.4977976381778717, 'std': 0.12025152146816254}}} 

2024-12-19 19:40:53.784006: unpacking dataset...
2024-12-19 19:40:57.338316: unpacking done...
2024-12-19 19:40:57.539908: Unable to plot network architecture: nnUNet_compile is enabled!
2024-12-19 19:40:57.834603: 
2024-12-19 19:40:57.835656: Epoch 0
2024-12-19 19:40:57.836614: Current learning rate: 0.01
2024-12-19 19:48:40.073014: Validation loss improved from 1000.00000 to -0.24566! Patience: 0/50
2024-12-19 19:48:40.074579: train_loss -0.0696
2024-12-19 19:48:40.075657: val_loss -0.2457
2024-12-19 19:48:40.076468: Pseudo dice [0.5781]
2024-12-19 19:48:40.077316: Epoch time: 462.24 s
2024-12-19 19:48:40.078046: Yayy! New best EMA pseudo Dice: 0.5781
2024-12-19 19:48:41.920239: 
2024-12-19 19:48:41.921428: Epoch 1
2024-12-19 19:48:41.922210: Current learning rate: 0.00994
2024-12-19 19:55:09.273760: Validation loss improved from -0.24566 to -0.25945! Patience: 0/50
2024-12-19 19:55:09.274826: train_loss -0.2356
2024-12-19 19:55:09.275580: val_loss -0.2595
2024-12-19 19:55:09.276206: Pseudo dice [0.564]
2024-12-19 19:55:09.276958: Epoch time: 387.36 s
2024-12-19 19:55:10.648930: 
2024-12-19 19:55:10.650092: Epoch 2
2024-12-19 19:55:10.651167: Current learning rate: 0.00988
2024-12-19 20:01:41.807159: Validation loss did not improve from -0.25945. Patience: 1/50
2024-12-19 20:01:41.808300: train_loss -0.2975
2024-12-19 20:01:41.809688: val_loss -0.2182
2024-12-19 20:01:41.810769: Pseudo dice [0.5233]
2024-12-19 20:01:41.811851: Epoch time: 391.16 s
2024-12-19 20:01:43.176213: 
2024-12-19 20:01:43.177527: Epoch 3
2024-12-19 20:01:43.178426: Current learning rate: 0.00982
2024-12-19 20:08:08.882951: Validation loss improved from -0.25945 to -0.31224! Patience: 1/50
2024-12-19 20:08:08.883599: train_loss -0.3253
2024-12-19 20:08:08.884381: val_loss -0.3122
2024-12-19 20:08:08.885094: Pseudo dice [0.6035]
2024-12-19 20:08:08.885858: Epoch time: 385.71 s
2024-12-19 20:08:10.250342: 
2024-12-19 20:08:10.251440: Epoch 4
2024-12-19 20:08:10.252324: Current learning rate: 0.00976
2024-12-19 20:14:42.883754: Validation loss improved from -0.31224 to -0.33840! Patience: 0/50
2024-12-19 20:14:42.884706: train_loss -0.3734
2024-12-19 20:14:42.885494: val_loss -0.3384
2024-12-19 20:14:42.886161: Pseudo dice [0.6285]
2024-12-19 20:14:42.886980: Epoch time: 392.64 s
2024-12-19 20:14:43.264505: Yayy! New best EMA pseudo Dice: 0.58
2024-12-19 20:14:45.100367: 
2024-12-19 20:14:45.101675: Epoch 5
2024-12-19 20:14:45.102364: Current learning rate: 0.0097
2024-12-19 20:21:08.184945: Validation loss improved from -0.33840 to -0.34925! Patience: 0/50
2024-12-19 20:21:08.185921: train_loss -0.3983
2024-12-19 20:21:08.186975: val_loss -0.3492
2024-12-19 20:21:08.187992: Pseudo dice [0.6304]
2024-12-19 20:21:08.189017: Epoch time: 383.09 s
2024-12-19 20:21:08.189875: Yayy! New best EMA pseudo Dice: 0.585
2024-12-19 20:21:10.116061: 
2024-12-19 20:21:10.118750: Epoch 6
2024-12-19 20:21:10.120004: Current learning rate: 0.00964
2024-12-19 20:27:26.267805: Validation loss improved from -0.34925 to -0.35834! Patience: 0/50
2024-12-19 20:27:26.268932: train_loss -0.4159
2024-12-19 20:27:26.270231: val_loss -0.3583
2024-12-19 20:27:26.271268: Pseudo dice [0.6417]
2024-12-19 20:27:26.272186: Epoch time: 376.16 s
2024-12-19 20:27:26.273195: Yayy! New best EMA pseudo Dice: 0.5907
2024-12-19 20:27:28.160822: 
2024-12-19 20:27:28.162090: Epoch 7
2024-12-19 20:27:28.163048: Current learning rate: 0.00958
2024-12-19 20:33:16.189904: Validation loss improved from -0.35834 to -0.37861! Patience: 0/50
2024-12-19 20:33:16.190979: train_loss -0.4423
2024-12-19 20:33:16.192113: val_loss -0.3786
2024-12-19 20:33:16.193146: Pseudo dice [0.652]
2024-12-19 20:33:16.194053: Epoch time: 348.03 s
2024-12-19 20:33:16.194890: Yayy! New best EMA pseudo Dice: 0.5968
2024-12-19 20:33:18.657856: 
2024-12-19 20:33:18.659325: Epoch 8
2024-12-19 20:33:18.660515: Current learning rate: 0.00952
2024-12-19 20:39:32.041708: Validation loss improved from -0.37861 to -0.39608! Patience: 0/50
2024-12-19 20:39:32.042696: train_loss -0.468
2024-12-19 20:39:32.043435: val_loss -0.3961
2024-12-19 20:39:32.044165: Pseudo dice [0.6608]
2024-12-19 20:39:32.044901: Epoch time: 373.39 s
2024-12-19 20:39:32.045621: Yayy! New best EMA pseudo Dice: 0.6032
2024-12-19 20:39:34.117746: 
2024-12-19 20:39:34.119078: Epoch 9
2024-12-19 20:39:34.120090: Current learning rate: 0.00946
2024-12-19 20:45:43.951440: Validation loss did not improve from -0.39608. Patience: 1/50
2024-12-19 20:45:43.952516: train_loss -0.4792
2024-12-19 20:45:43.953373: val_loss -0.3527
2024-12-19 20:45:43.954153: Pseudo dice [0.6342]
2024-12-19 20:45:43.955030: Epoch time: 369.84 s
2024-12-19 20:45:44.501595: Yayy! New best EMA pseudo Dice: 0.6063
2024-12-19 20:45:46.384703: 
2024-12-19 20:45:46.385765: Epoch 10
2024-12-19 20:45:46.386568: Current learning rate: 0.0094
2024-12-19 20:52:05.677381: Validation loss improved from -0.39608 to -0.41991! Patience: 1/50
2024-12-19 20:52:05.678425: train_loss -0.5024
2024-12-19 20:52:05.679352: val_loss -0.4199
2024-12-19 20:52:05.680217: Pseudo dice [0.6705]
2024-12-19 20:52:05.681189: Epoch time: 379.3 s
2024-12-19 20:52:05.682111: Yayy! New best EMA pseudo Dice: 0.6127
2024-12-19 20:52:07.435887: 
2024-12-19 20:52:07.437154: Epoch 11
2024-12-19 20:52:07.438056: Current learning rate: 0.00934
2024-12-19 20:58:24.459886: Validation loss did not improve from -0.41991. Patience: 1/50
2024-12-19 20:58:24.460567: train_loss -0.5171
2024-12-19 20:58:24.461305: val_loss -0.4088
2024-12-19 20:58:24.462023: Pseudo dice [0.6762]
2024-12-19 20:58:24.462697: Epoch time: 377.03 s
2024-12-19 20:58:24.463410: Yayy! New best EMA pseudo Dice: 0.6191
2024-12-19 20:58:26.284569: 
2024-12-19 20:58:26.285561: Epoch 12
2024-12-19 20:58:26.286318: Current learning rate: 0.00928
2024-12-19 21:05:08.605874: Validation loss improved from -0.41991 to -0.44482! Patience: 1/50
2024-12-19 21:05:08.606783: train_loss -0.5209
2024-12-19 21:05:08.607657: val_loss -0.4448
2024-12-19 21:05:08.608508: Pseudo dice [0.6889]
2024-12-19 21:05:08.609235: Epoch time: 402.32 s
2024-12-19 21:05:08.610013: Yayy! New best EMA pseudo Dice: 0.6261
2024-12-19 21:05:10.444848: 
2024-12-19 21:05:10.446353: Epoch 13
2024-12-19 21:05:10.447216: Current learning rate: 0.00922
2024-12-19 21:11:40.514411: Validation loss improved from -0.44482 to -0.45097! Patience: 0/50
2024-12-19 21:11:40.515450: train_loss -0.5335
2024-12-19 21:11:40.516290: val_loss -0.451
2024-12-19 21:11:40.516968: Pseudo dice [0.682]
2024-12-19 21:11:40.517699: Epoch time: 390.07 s
2024-12-19 21:11:40.518469: Yayy! New best EMA pseudo Dice: 0.6317
2024-12-19 21:11:42.355829: 
2024-12-19 21:11:42.357042: Epoch 14
2024-12-19 21:11:42.358011: Current learning rate: 0.00916
2024-12-19 21:18:19.853944: Validation loss did not improve from -0.45097. Patience: 1/50
2024-12-19 21:18:19.854703: train_loss -0.5421
2024-12-19 21:18:19.855765: val_loss -0.442
2024-12-19 21:18:19.856779: Pseudo dice [0.6961]
2024-12-19 21:18:19.857866: Epoch time: 397.5 s
2024-12-19 21:18:20.279813: Yayy! New best EMA pseudo Dice: 0.6381
2024-12-19 21:18:22.073733: 
2024-12-19 21:18:22.074667: Epoch 15
2024-12-19 21:18:22.075414: Current learning rate: 0.0091
2024-12-19 21:24:45.452293: Validation loss did not improve from -0.45097. Patience: 2/50
2024-12-19 21:24:45.453058: train_loss -0.5453
2024-12-19 21:24:45.453858: val_loss -0.4019
2024-12-19 21:24:45.454581: Pseudo dice [0.659]
2024-12-19 21:24:45.455354: Epoch time: 383.38 s
2024-12-19 21:24:45.456050: Yayy! New best EMA pseudo Dice: 0.6402
2024-12-19 21:24:47.306172: 
2024-12-19 21:24:47.307162: Epoch 16
2024-12-19 21:24:47.307973: Current learning rate: 0.00903
2024-12-19 21:31:14.745319: Validation loss did not improve from -0.45097. Patience: 3/50
2024-12-19 21:31:14.746384: train_loss -0.5638
2024-12-19 21:31:14.747447: val_loss -0.4165
2024-12-19 21:31:14.748484: Pseudo dice [0.6671]
2024-12-19 21:31:14.749502: Epoch time: 387.44 s
2024-12-19 21:31:14.750558: Yayy! New best EMA pseudo Dice: 0.6429
2024-12-19 21:31:16.628832: 
2024-12-19 21:31:16.629894: Epoch 17
2024-12-19 21:31:16.630968: Current learning rate: 0.00897
2024-12-19 21:38:20.306376: Validation loss improved from -0.45097 to -0.45630! Patience: 3/50
2024-12-19 21:38:20.307539: train_loss -0.565
2024-12-19 21:38:20.308601: val_loss -0.4563
2024-12-19 21:38:20.309450: Pseudo dice [0.6998]
2024-12-19 21:38:20.310295: Epoch time: 423.68 s
2024-12-19 21:38:20.311121: Yayy! New best EMA pseudo Dice: 0.6486
2024-12-19 21:38:22.717302: 
2024-12-19 21:38:22.718575: Epoch 18
2024-12-19 21:38:22.719432: Current learning rate: 0.00891
2024-12-19 21:46:11.733627: Validation loss did not improve from -0.45630. Patience: 1/50
2024-12-19 21:46:11.734406: train_loss -0.5622
2024-12-19 21:46:11.735230: val_loss -0.4281
2024-12-19 21:46:11.736062: Pseudo dice [0.6839]
2024-12-19 21:46:11.736916: Epoch time: 469.02 s
2024-12-19 21:46:11.737665: Yayy! New best EMA pseudo Dice: 0.6521
2024-12-19 21:46:13.595383: 
2024-12-19 21:46:13.596499: Epoch 19
2024-12-19 21:46:13.597306: Current learning rate: 0.00885
2024-12-19 21:53:58.322465: Validation loss did not improve from -0.45630. Patience: 2/50
2024-12-19 21:53:58.323766: train_loss -0.5673
2024-12-19 21:53:58.324731: val_loss -0.4191
2024-12-19 21:53:58.325578: Pseudo dice [0.6655]
2024-12-19 21:53:58.326497: Epoch time: 464.73 s
2024-12-19 21:53:58.737543: Yayy! New best EMA pseudo Dice: 0.6535
2024-12-19 21:54:00.590076: 
2024-12-19 21:54:00.591452: Epoch 20
2024-12-19 21:54:00.592322: Current learning rate: 0.00879
2024-12-19 22:02:04.308312: Validation loss did not improve from -0.45630. Patience: 3/50
2024-12-19 22:02:04.309435: train_loss -0.5839
2024-12-19 22:02:04.312552: val_loss -0.4491
2024-12-19 22:02:04.313470: Pseudo dice [0.6904]
2024-12-19 22:02:04.314629: Epoch time: 483.72 s
2024-12-19 22:02:04.315478: Yayy! New best EMA pseudo Dice: 0.6571
2024-12-19 22:02:06.223204: 
2024-12-19 22:02:06.224591: Epoch 21
2024-12-19 22:02:06.225442: Current learning rate: 0.00873
2024-12-19 22:10:31.622780: Validation loss did not improve from -0.45630. Patience: 4/50
2024-12-19 22:10:31.623796: train_loss -0.5863
2024-12-19 22:10:31.624776: val_loss -0.3898
2024-12-19 22:10:31.625537: Pseudo dice [0.6587]
2024-12-19 22:10:31.626321: Epoch time: 505.4 s
2024-12-19 22:10:31.627204: Yayy! New best EMA pseudo Dice: 0.6573
2024-12-19 22:10:33.477677: 
2024-12-19 22:10:33.478959: Epoch 22
2024-12-19 22:10:33.479699: Current learning rate: 0.00867
2024-12-19 22:19:01.038838: Validation loss did not improve from -0.45630. Patience: 5/50
2024-12-19 22:19:01.039924: train_loss -0.592
2024-12-19 22:19:01.040718: val_loss -0.4035
2024-12-19 22:19:01.041574: Pseudo dice [0.6678]
2024-12-19 22:19:01.042269: Epoch time: 507.56 s
2024-12-19 22:19:01.042958: Yayy! New best EMA pseudo Dice: 0.6583
2024-12-19 22:19:02.781100: 
2024-12-19 22:19:02.782389: Epoch 23
2024-12-19 22:19:02.783196: Current learning rate: 0.00861
2024-12-19 22:27:06.397384: Validation loss improved from -0.45630 to -0.47153! Patience: 5/50
2024-12-19 22:27:06.398497: train_loss -0.5899
2024-12-19 22:27:06.399507: val_loss -0.4715
2024-12-19 22:27:06.400432: Pseudo dice [0.7049]
2024-12-19 22:27:06.401265: Epoch time: 483.62 s
2024-12-19 22:27:06.402024: Yayy! New best EMA pseudo Dice: 0.663
2024-12-19 22:27:08.164498: 
2024-12-19 22:27:08.166098: Epoch 24
2024-12-19 22:27:08.166990: Current learning rate: 0.00855
2024-12-19 22:35:12.117843: Validation loss did not improve from -0.47153. Patience: 1/50
2024-12-19 22:35:12.118674: train_loss -0.5875
2024-12-19 22:35:12.119707: val_loss -0.4489
2024-12-19 22:35:12.120780: Pseudo dice [0.6966]
2024-12-19 22:35:12.121781: Epoch time: 483.96 s
2024-12-19 22:35:12.598682: Yayy! New best EMA pseudo Dice: 0.6664
2024-12-19 22:35:14.372498: 
2024-12-19 22:35:14.373888: Epoch 25
2024-12-19 22:35:14.374749: Current learning rate: 0.00849
2024-12-19 22:42:17.825370: Validation loss did not improve from -0.47153. Patience: 2/50
2024-12-19 22:42:17.826287: train_loss -0.609
2024-12-19 22:42:17.827197: val_loss -0.4593
2024-12-19 22:42:17.828046: Pseudo dice [0.6961]
2024-12-19 22:42:17.828821: Epoch time: 423.46 s
2024-12-19 22:42:17.829679: Yayy! New best EMA pseudo Dice: 0.6693
2024-12-19 22:42:19.629743: 
2024-12-19 22:42:19.631191: Epoch 26
2024-12-19 22:42:19.632097: Current learning rate: 0.00843
2024-12-19 22:50:29.117616: Validation loss improved from -0.47153 to -0.47709! Patience: 2/50
2024-12-19 22:50:29.118726: train_loss -0.6093
2024-12-19 22:50:29.119499: val_loss -0.4771
2024-12-19 22:50:29.120420: Pseudo dice [0.7181]
2024-12-19 22:50:29.121172: Epoch time: 489.49 s
2024-12-19 22:50:29.122193: Yayy! New best EMA pseudo Dice: 0.6742
2024-12-19 22:50:31.012004: 
2024-12-19 22:50:31.013512: Epoch 27
2024-12-19 22:50:31.014520: Current learning rate: 0.00836
2024-12-19 22:58:25.641232: Validation loss did not improve from -0.47709. Patience: 1/50
2024-12-19 22:58:25.662182: train_loss -0.6195
2024-12-19 22:58:25.663607: val_loss -0.4667
2024-12-19 22:58:25.664387: Pseudo dice [0.7116]
2024-12-19 22:58:25.665382: Epoch time: 474.63 s
2024-12-19 22:58:25.666354: Yayy! New best EMA pseudo Dice: 0.6779
2024-12-19 22:58:27.553188: 
2024-12-19 22:58:27.554588: Epoch 28
2024-12-19 22:58:27.555386: Current learning rate: 0.0083
2024-12-19 23:06:33.410545: Validation loss improved from -0.47709 to -0.48022! Patience: 1/50
2024-12-19 23:06:33.411753: train_loss -0.6182
2024-12-19 23:06:33.414237: val_loss -0.4802
2024-12-19 23:06:33.415141: Pseudo dice [0.7122]
2024-12-19 23:06:33.416148: Epoch time: 485.86 s
2024-12-19 23:06:33.416872: Yayy! New best EMA pseudo Dice: 0.6814
2024-12-19 23:06:35.994921: 
2024-12-19 23:06:35.996204: Epoch 29
2024-12-19 23:06:35.996938: Current learning rate: 0.00824
2024-12-19 23:14:15.717454: Validation loss did not improve from -0.48022. Patience: 1/50
2024-12-19 23:14:15.718503: train_loss -0.6228
2024-12-19 23:14:15.719361: val_loss -0.4124
2024-12-19 23:14:15.720074: Pseudo dice [0.687]
2024-12-19 23:14:15.720903: Epoch time: 459.72 s
2024-12-19 23:14:16.115358: Yayy! New best EMA pseudo Dice: 0.6819
2024-12-19 23:14:17.964163: 
2024-12-19 23:14:17.965676: Epoch 30
2024-12-19 23:14:17.966569: Current learning rate: 0.00818
2024-12-19 23:21:28.305655: Validation loss did not improve from -0.48022. Patience: 2/50
2024-12-19 23:21:28.306562: train_loss -0.6263
2024-12-19 23:21:28.307408: val_loss -0.4584
2024-12-19 23:21:28.308132: Pseudo dice [0.7039]
2024-12-19 23:21:28.308880: Epoch time: 430.34 s
2024-12-19 23:21:28.309633: Yayy! New best EMA pseudo Dice: 0.6841
2024-12-19 23:21:30.104338: 
2024-12-19 23:21:30.105840: Epoch 31
2024-12-19 23:21:30.106765: Current learning rate: 0.00812
2024-12-19 23:28:42.665627: Validation loss did not improve from -0.48022. Patience: 3/50
2024-12-19 23:28:42.666657: train_loss -0.6316
2024-12-19 23:28:42.667421: val_loss -0.4376
2024-12-19 23:28:42.668197: Pseudo dice [0.6846]
2024-12-19 23:28:42.668991: Epoch time: 432.56 s
2024-12-19 23:28:42.669791: Yayy! New best EMA pseudo Dice: 0.6842
2024-12-19 23:28:44.590246: 
2024-12-19 23:28:44.591736: Epoch 32
2024-12-19 23:28:44.592878: Current learning rate: 0.00806
2024-12-19 23:36:12.448251: Validation loss did not improve from -0.48022. Patience: 4/50
2024-12-19 23:36:12.449222: train_loss -0.6264
2024-12-19 23:36:12.450016: val_loss -0.4018
2024-12-19 23:36:12.450658: Pseudo dice [0.6668]
2024-12-19 23:36:12.451319: Epoch time: 447.86 s
2024-12-19 23:36:13.952102: 
2024-12-19 23:36:13.953538: Epoch 33
2024-12-19 23:36:13.954386: Current learning rate: 0.008
2024-12-19 23:43:39.903352: Validation loss did not improve from -0.48022. Patience: 5/50
2024-12-19 23:43:39.904320: train_loss -0.6358
2024-12-19 23:43:39.905424: val_loss -0.444
2024-12-19 23:43:39.906282: Pseudo dice [0.6952]
2024-12-19 23:43:39.907289: Epoch time: 445.95 s
2024-12-19 23:43:41.347739: 
2024-12-19 23:43:41.349343: Epoch 34
2024-12-19 23:43:41.350453: Current learning rate: 0.00793
2024-12-19 23:50:48.543415: Validation loss did not improve from -0.48022. Patience: 6/50
2024-12-19 23:50:48.544229: train_loss -0.6416
2024-12-19 23:50:48.545294: val_loss -0.4548
2024-12-19 23:50:48.546288: Pseudo dice [0.6953]
2024-12-19 23:50:48.547364: Epoch time: 427.2 s
2024-12-19 23:50:49.066089: Yayy! New best EMA pseudo Dice: 0.6849
2024-12-19 23:50:50.949044: 
2024-12-19 23:50:50.950645: Epoch 35
2024-12-19 23:50:50.951820: Current learning rate: 0.00787
2024-12-19 23:57:34.318218: Validation loss did not improve from -0.48022. Patience: 7/50
2024-12-19 23:57:34.319205: train_loss -0.6447
2024-12-19 23:57:34.320041: val_loss -0.4755
2024-12-19 23:57:34.320859: Pseudo dice [0.7017]
2024-12-19 23:57:34.321603: Epoch time: 403.37 s
2024-12-19 23:57:34.322253: Yayy! New best EMA pseudo Dice: 0.6865
2024-12-19 23:57:36.153522: 
2024-12-19 23:57:36.154721: Epoch 36
2024-12-19 23:57:36.155536: Current learning rate: 0.00781
2024-12-20 00:04:08.932061: Validation loss did not improve from -0.48022. Patience: 8/50
2024-12-20 00:04:08.935884: train_loss -0.6353
2024-12-20 00:04:08.937280: val_loss -0.4367
2024-12-20 00:04:08.938068: Pseudo dice [0.6936]
2024-12-20 00:04:08.939095: Epoch time: 392.78 s
2024-12-20 00:04:08.940202: Yayy! New best EMA pseudo Dice: 0.6872
2024-12-20 00:04:10.943857: 
2024-12-20 00:04:10.945343: Epoch 37
2024-12-20 00:04:10.946135: Current learning rate: 0.00775
2024-12-20 00:11:55.531395: Validation loss did not improve from -0.48022. Patience: 9/50
2024-12-20 00:11:55.532472: train_loss -0.6505
2024-12-20 00:11:55.534327: val_loss -0.4619
2024-12-20 00:11:55.535140: Pseudo dice [0.7077]
2024-12-20 00:11:55.536353: Epoch time: 464.59 s
2024-12-20 00:11:55.537075: Yayy! New best EMA pseudo Dice: 0.6893
2024-12-20 00:11:57.328904: 
2024-12-20 00:11:57.330379: Epoch 38
2024-12-20 00:11:57.331208: Current learning rate: 0.00769
2024-12-20 00:19:34.049665: Validation loss did not improve from -0.48022. Patience: 10/50
2024-12-20 00:19:34.050777: train_loss -0.6647
2024-12-20 00:19:34.051644: val_loss -0.4676
2024-12-20 00:19:34.052408: Pseudo dice [0.6996]
2024-12-20 00:19:34.053173: Epoch time: 456.72 s
2024-12-20 00:19:34.053969: Yayy! New best EMA pseudo Dice: 0.6903
2024-12-20 00:19:36.271103: 
2024-12-20 00:19:36.272682: Epoch 39
2024-12-20 00:19:36.273798: Current learning rate: 0.00763
2024-12-20 00:27:10.886647: Validation loss improved from -0.48022 to -0.48680! Patience: 10/50
2024-12-20 00:27:10.887592: train_loss -0.6637
2024-12-20 00:27:10.888541: val_loss -0.4868
2024-12-20 00:27:10.889570: Pseudo dice [0.7172]
2024-12-20 00:27:10.890475: Epoch time: 454.62 s
2024-12-20 00:27:11.334322: Yayy! New best EMA pseudo Dice: 0.693
2024-12-20 00:27:13.272313: 
2024-12-20 00:27:13.273883: Epoch 40
2024-12-20 00:27:13.274914: Current learning rate: 0.00756
2024-12-20 00:34:53.069834: Validation loss did not improve from -0.48680. Patience: 1/50
2024-12-20 00:34:53.071666: train_loss -0.6577
2024-12-20 00:34:53.072576: val_loss -0.4477
2024-12-20 00:34:53.073326: Pseudo dice [0.6934]
2024-12-20 00:34:53.074169: Epoch time: 459.8 s
2024-12-20 00:34:53.074964: Yayy! New best EMA pseudo Dice: 0.6931
2024-12-20 00:34:55.113728: 
2024-12-20 00:34:55.115136: Epoch 41
2024-12-20 00:34:55.116212: Current learning rate: 0.0075
2024-12-20 00:42:39.772757: Validation loss improved from -0.48680 to -0.49604! Patience: 1/50
2024-12-20 00:42:39.773840: train_loss -0.6622
2024-12-20 00:42:39.774714: val_loss -0.496
2024-12-20 00:42:39.775438: Pseudo dice [0.7167]
2024-12-20 00:42:39.776259: Epoch time: 464.66 s
2024-12-20 00:42:39.777159: Yayy! New best EMA pseudo Dice: 0.6954
2024-12-20 00:42:41.556663: 
2024-12-20 00:42:41.558162: Epoch 42
2024-12-20 00:42:41.559308: Current learning rate: 0.00744
2024-12-20 00:50:20.489288: Validation loss did not improve from -0.49604. Patience: 1/50
2024-12-20 00:50:20.490358: train_loss -0.6544
2024-12-20 00:50:20.491395: val_loss -0.4082
2024-12-20 00:50:20.492165: Pseudo dice [0.6757]
2024-12-20 00:50:20.492910: Epoch time: 458.93 s
2024-12-20 00:50:21.874873: 
2024-12-20 00:50:21.876260: Epoch 43
2024-12-20 00:50:21.877078: Current learning rate: 0.00738
2024-12-20 00:58:02.768394: Validation loss improved from -0.49604 to -0.50184! Patience: 1/50
2024-12-20 00:58:02.769609: train_loss -0.6596
2024-12-20 00:58:02.770678: val_loss -0.5018
2024-12-20 00:58:02.771583: Pseudo dice [0.722]
2024-12-20 00:58:02.772394: Epoch time: 460.9 s
2024-12-20 00:58:02.773176: Yayy! New best EMA pseudo Dice: 0.6963
2024-12-20 00:58:04.681583: 
2024-12-20 00:58:04.682973: Epoch 44
2024-12-20 00:58:04.683706: Current learning rate: 0.00732
2024-12-20 01:06:00.656984: Validation loss did not improve from -0.50184. Patience: 1/50
2024-12-20 01:06:00.659136: train_loss -0.6639
2024-12-20 01:06:00.659869: val_loss -0.4897
2024-12-20 01:06:00.660495: Pseudo dice [0.7128]
2024-12-20 01:06:00.661138: Epoch time: 475.98 s
2024-12-20 01:06:01.050209: Yayy! New best EMA pseudo Dice: 0.698
2024-12-20 01:06:02.808854: 
2024-12-20 01:06:02.809801: Epoch 45
2024-12-20 01:06:02.810597: Current learning rate: 0.00725
2024-12-20 01:13:50.118412: Validation loss did not improve from -0.50184. Patience: 2/50
2024-12-20 01:13:50.120862: train_loss -0.6721
2024-12-20 01:13:50.121824: val_loss -0.4773
2024-12-20 01:13:50.122568: Pseudo dice [0.716]
2024-12-20 01:13:50.123291: Epoch time: 467.31 s
2024-12-20 01:13:50.123998: Yayy! New best EMA pseudo Dice: 0.6998
2024-12-20 01:13:52.003804: 
2024-12-20 01:13:52.004770: Epoch 46
2024-12-20 01:13:52.005639: Current learning rate: 0.00719
2024-12-20 01:21:25.522305: Validation loss did not improve from -0.50184. Patience: 3/50
2024-12-20 01:21:25.523116: train_loss -0.6769
2024-12-20 01:21:25.524696: val_loss -0.4604
2024-12-20 01:21:25.525309: Pseudo dice [0.7086]
2024-12-20 01:21:25.526119: Epoch time: 453.52 s
2024-12-20 01:21:25.526852: Yayy! New best EMA pseudo Dice: 0.7006
2024-12-20 01:21:27.366896: 
2024-12-20 01:21:27.367968: Epoch 47
2024-12-20 01:21:27.368819: Current learning rate: 0.00713
2024-12-20 01:29:22.186748: Validation loss did not improve from -0.50184. Patience: 4/50
2024-12-20 01:29:22.187825: train_loss -0.6805
2024-12-20 01:29:22.188788: val_loss -0.4866
2024-12-20 01:29:22.189761: Pseudo dice [0.718]
2024-12-20 01:29:22.190633: Epoch time: 474.82 s
2024-12-20 01:29:22.191532: Yayy! New best EMA pseudo Dice: 0.7024
2024-12-20 01:29:23.972116: 
2024-12-20 01:29:23.973423: Epoch 48
2024-12-20 01:29:23.974172: Current learning rate: 0.00707
2024-12-20 01:37:20.848272: Validation loss did not improve from -0.50184. Patience: 5/50
2024-12-20 01:37:20.848908: train_loss -0.6849
2024-12-20 01:37:20.849717: val_loss -0.4628
2024-12-20 01:37:20.850374: Pseudo dice [0.7019]
2024-12-20 01:37:20.851024: Epoch time: 476.88 s
2024-12-20 01:37:22.361668: 
2024-12-20 01:37:22.362663: Epoch 49
2024-12-20 01:37:22.363474: Current learning rate: 0.007
2024-12-20 01:45:09.636598: Validation loss did not improve from -0.50184. Patience: 6/50
2024-12-20 01:45:09.637258: train_loss -0.6875
2024-12-20 01:45:09.638201: val_loss -0.4756
2024-12-20 01:45:09.639039: Pseudo dice [0.7121]
2024-12-20 01:45:09.639925: Epoch time: 467.28 s
2024-12-20 01:45:10.126608: Yayy! New best EMA pseudo Dice: 0.7033
2024-12-20 01:45:12.366744: 
2024-12-20 01:45:12.368003: Epoch 50
2024-12-20 01:45:12.368816: Current learning rate: 0.00694
2024-12-20 01:52:07.854861: Validation loss did not improve from -0.50184. Patience: 7/50
2024-12-20 01:52:07.855651: train_loss -0.6829
2024-12-20 01:52:07.856560: val_loss -0.4424
2024-12-20 01:52:07.857305: Pseudo dice [0.6908]
2024-12-20 01:52:07.858163: Epoch time: 415.49 s
2024-12-20 01:52:09.280361: 
2024-12-20 01:52:09.281833: Epoch 51
2024-12-20 01:52:09.282860: Current learning rate: 0.00688
2024-12-20 01:59:37.554710: Validation loss did not improve from -0.50184. Patience: 8/50
2024-12-20 01:59:37.555421: train_loss -0.6868
2024-12-20 01:59:37.556272: val_loss -0.4784
2024-12-20 01:59:37.556987: Pseudo dice [0.7078]
2024-12-20 01:59:37.557791: Epoch time: 448.28 s
2024-12-20 01:59:39.017949: 
2024-12-20 01:59:39.019838: Epoch 52
2024-12-20 01:59:39.020904: Current learning rate: 0.00682
2024-12-20 02:07:05.363668: Validation loss did not improve from -0.50184. Patience: 9/50
2024-12-20 02:07:05.364735: train_loss -0.6925
2024-12-20 02:07:05.365540: val_loss -0.4571
2024-12-20 02:07:05.366412: Pseudo dice [0.7082]
2024-12-20 02:07:05.367195: Epoch time: 446.35 s
2024-12-20 02:07:06.799423: 
2024-12-20 02:07:06.801071: Epoch 53
2024-12-20 02:07:06.801989: Current learning rate: 0.00675
2024-12-20 02:13:57.916518: Validation loss improved from -0.50184 to -0.50929! Patience: 9/50
2024-12-20 02:13:57.936050: train_loss -0.7003
2024-12-20 02:13:57.937644: val_loss -0.5093
2024-12-20 02:13:57.938496: Pseudo dice [0.7297]
2024-12-20 02:13:57.939487: Epoch time: 411.12 s
2024-12-20 02:13:57.940532: Yayy! New best EMA pseudo Dice: 0.7058
2024-12-20 02:13:59.861396: 
2024-12-20 02:13:59.862831: Epoch 54
2024-12-20 02:13:59.863558: Current learning rate: 0.00669
2024-12-20 02:21:27.217289: Validation loss did not improve from -0.50929. Patience: 1/50
2024-12-20 02:21:27.219253: train_loss -0.7027
2024-12-20 02:21:27.222208: val_loss -0.4821
2024-12-20 02:21:27.223056: Pseudo dice [0.7166]
2024-12-20 02:21:27.224311: Epoch time: 447.36 s
2024-12-20 02:21:27.579513: Yayy! New best EMA pseudo Dice: 0.7069
2024-12-20 02:21:29.429824: 
2024-12-20 02:21:29.431297: Epoch 55
2024-12-20 02:21:29.432367: Current learning rate: 0.00663
2024-12-20 02:28:55.547577: Validation loss did not improve from -0.50929. Patience: 2/50
2024-12-20 02:28:55.548641: train_loss -0.7048
2024-12-20 02:28:55.549716: val_loss -0.4725
2024-12-20 02:28:55.550838: Pseudo dice [0.7109]
2024-12-20 02:28:55.551824: Epoch time: 446.12 s
2024-12-20 02:28:55.552739: Yayy! New best EMA pseudo Dice: 0.7073
2024-12-20 02:28:57.318932: 
2024-12-20 02:28:57.320457: Epoch 56
2024-12-20 02:28:57.321521: Current learning rate: 0.00657
2024-12-20 02:36:38.193206: Validation loss did not improve from -0.50929. Patience: 3/50
2024-12-20 02:36:38.194268: train_loss -0.699
2024-12-20 02:36:38.195130: val_loss -0.4782
2024-12-20 02:36:38.195920: Pseudo dice [0.7196]
2024-12-20 02:36:38.196671: Epoch time: 460.88 s
2024-12-20 02:36:38.197553: Yayy! New best EMA pseudo Dice: 0.7085
2024-12-20 02:36:40.021253: 
2024-12-20 02:36:40.022643: Epoch 57
2024-12-20 02:36:40.023576: Current learning rate: 0.0065
2024-12-20 02:44:20.043425: Validation loss did not improve from -0.50929. Patience: 4/50
2024-12-20 02:44:20.044488: train_loss -0.6922
2024-12-20 02:44:20.045499: val_loss -0.4712
2024-12-20 02:44:20.046415: Pseudo dice [0.7128]
2024-12-20 02:44:20.047396: Epoch time: 460.02 s
2024-12-20 02:44:20.048257: Yayy! New best EMA pseudo Dice: 0.709
2024-12-20 02:44:21.846962: 
2024-12-20 02:44:21.848340: Epoch 58
2024-12-20 02:44:21.849178: Current learning rate: 0.00644
2024-12-20 02:51:41.153485: Validation loss did not improve from -0.50929. Patience: 5/50
2024-12-20 02:51:41.154543: train_loss -0.6935
2024-12-20 02:51:41.155624: val_loss -0.4848
2024-12-20 02:51:41.156464: Pseudo dice [0.7126]
2024-12-20 02:51:41.157331: Epoch time: 439.31 s
2024-12-20 02:51:41.158182: Yayy! New best EMA pseudo Dice: 0.7093
2024-12-20 02:51:43.036207: 
2024-12-20 02:51:43.037664: Epoch 59
2024-12-20 02:51:43.038565: Current learning rate: 0.00638
2024-12-20 02:59:19.141490: Validation loss did not improve from -0.50929. Patience: 6/50
2024-12-20 02:59:19.142566: train_loss -0.696
2024-12-20 02:59:19.143420: val_loss -0.4557
2024-12-20 02:59:19.144183: Pseudo dice [0.707]
2024-12-20 02:59:19.145098: Epoch time: 456.11 s
2024-12-20 02:59:21.037054: 
2024-12-20 02:59:21.038460: Epoch 60
2024-12-20 02:59:21.039267: Current learning rate: 0.00631
2024-12-20 03:06:51.940484: Validation loss did not improve from -0.50929. Patience: 7/50
2024-12-20 03:06:51.941285: train_loss -0.7012
2024-12-20 03:06:51.942070: val_loss -0.465
2024-12-20 03:06:51.942813: Pseudo dice [0.7043]
2024-12-20 03:06:51.943579: Epoch time: 450.91 s
2024-12-20 03:06:53.777948: 
2024-12-20 03:06:53.779343: Epoch 61
2024-12-20 03:06:53.780141: Current learning rate: 0.00625
2024-12-20 03:14:25.946943: Validation loss did not improve from -0.50929. Patience: 8/50
2024-12-20 03:14:25.947945: train_loss -0.7128
2024-12-20 03:14:25.948731: val_loss -0.4852
2024-12-20 03:14:25.949497: Pseudo dice [0.7173]
2024-12-20 03:14:25.950183: Epoch time: 452.17 s
2024-12-20 03:14:25.950941: Yayy! New best EMA pseudo Dice: 0.7095
2024-12-20 03:14:27.899796: 
2024-12-20 03:14:27.901346: Epoch 62
2024-12-20 03:14:27.902322: Current learning rate: 0.00619
2024-12-20 03:22:03.785779: Validation loss did not improve from -0.50929. Patience: 9/50
2024-12-20 03:22:03.787765: train_loss -0.7112
2024-12-20 03:22:03.788977: val_loss -0.4529
2024-12-20 03:22:03.789697: Pseudo dice [0.6992]
2024-12-20 03:22:03.790757: Epoch time: 455.89 s
2024-12-20 03:22:05.235818: 
2024-12-20 03:22:05.237266: Epoch 63
2024-12-20 03:22:05.238050: Current learning rate: 0.00612
2024-12-20 03:29:29.073246: Validation loss did not improve from -0.50929. Patience: 10/50
2024-12-20 03:29:29.074310: train_loss -0.7143
2024-12-20 03:29:29.075198: val_loss -0.4772
2024-12-20 03:29:29.076123: Pseudo dice [0.7089]
2024-12-20 03:29:29.076872: Epoch time: 443.84 s
2024-12-20 03:29:30.514001: 
2024-12-20 03:29:30.515456: Epoch 64
2024-12-20 03:29:30.516327: Current learning rate: 0.00606
2024-12-20 03:36:37.730251: Validation loss improved from -0.50929 to -0.51419! Patience: 10/50
2024-12-20 03:36:37.731420: train_loss -0.7077
2024-12-20 03:36:37.732487: val_loss -0.5142
2024-12-20 03:36:37.733491: Pseudo dice [0.7444]
2024-12-20 03:36:37.734415: Epoch time: 427.22 s
2024-12-20 03:36:38.164668: Yayy! New best EMA pseudo Dice: 0.7121
2024-12-20 03:36:40.065305: 
2024-12-20 03:36:40.066949: Epoch 65
2024-12-20 03:36:40.067924: Current learning rate: 0.006
2024-12-20 03:44:08.908073: Validation loss did not improve from -0.51419. Patience: 1/50
2024-12-20 03:44:08.909038: train_loss -0.715
2024-12-20 03:44:08.910017: val_loss -0.4757
2024-12-20 03:44:08.910812: Pseudo dice [0.7092]
2024-12-20 03:44:08.911509: Epoch time: 448.85 s
2024-12-20 03:44:10.332403: 
2024-12-20 03:44:10.333794: Epoch 66
2024-12-20 03:44:10.334641: Current learning rate: 0.00593
2024-12-20 03:51:31.317800: Validation loss did not improve from -0.51419. Patience: 2/50
2024-12-20 03:51:31.318861: train_loss -0.7168
2024-12-20 03:51:31.319920: val_loss -0.4572
2024-12-20 03:51:31.320752: Pseudo dice [0.703]
2024-12-20 03:51:31.321644: Epoch time: 440.99 s
2024-12-20 03:51:32.806297: 
2024-12-20 03:51:32.807782: Epoch 67
2024-12-20 03:51:32.808994: Current learning rate: 0.00587
2024-12-20 03:59:02.228660: Validation loss did not improve from -0.51419. Patience: 3/50
2024-12-20 03:59:02.229633: train_loss -0.7208
2024-12-20 03:59:02.230516: val_loss -0.4434
2024-12-20 03:59:02.231231: Pseudo dice [0.6913]
2024-12-20 03:59:02.232036: Epoch time: 449.42 s
2024-12-20 03:59:03.798657: 
2024-12-20 03:59:03.799773: Epoch 68
2024-12-20 03:59:03.800565: Current learning rate: 0.00581
2024-12-20 04:06:41.442874: Validation loss did not improve from -0.51419. Patience: 4/50
2024-12-20 04:06:41.445389: train_loss -0.7271
2024-12-20 04:06:41.447177: val_loss -0.442
2024-12-20 04:06:41.448181: Pseudo dice [0.691]
2024-12-20 04:06:41.449225: Epoch time: 457.65 s
2024-12-20 04:06:42.861912: 
2024-12-20 04:06:42.863516: Epoch 69
2024-12-20 04:06:42.864610: Current learning rate: 0.00574
2024-12-20 04:13:58.648764: Validation loss did not improve from -0.51419. Patience: 5/50
2024-12-20 04:13:58.649691: train_loss -0.7303
2024-12-20 04:13:58.650574: val_loss -0.475
2024-12-20 04:13:58.651447: Pseudo dice [0.718]
2024-12-20 04:13:58.652303: Epoch time: 435.79 s
2024-12-20 04:14:00.511149: 
2024-12-20 04:14:00.512620: Epoch 70
2024-12-20 04:14:00.513645: Current learning rate: 0.00568
2024-12-20 04:21:42.127075: Validation loss did not improve from -0.51419. Patience: 6/50
2024-12-20 04:21:42.130479: train_loss -0.7285
2024-12-20 04:21:42.131916: val_loss -0.4616
2024-12-20 04:21:42.132711: Pseudo dice [0.7025]
2024-12-20 04:21:42.133479: Epoch time: 461.62 s
2024-12-20 04:21:43.661519: 
2024-12-20 04:21:43.662759: Epoch 71
2024-12-20 04:21:43.663630: Current learning rate: 0.00562
2024-12-20 04:29:10.426962: Validation loss did not improve from -0.51419. Patience: 7/50
2024-12-20 04:29:10.428511: train_loss -0.7291
2024-12-20 04:29:10.429273: val_loss -0.44
2024-12-20 04:29:10.430065: Pseudo dice [0.6964]
2024-12-20 04:29:10.431040: Epoch time: 446.77 s
2024-12-20 04:29:12.593011: 
2024-12-20 04:29:12.594248: Epoch 72
2024-12-20 04:29:12.594935: Current learning rate: 0.00555
2024-12-20 04:37:17.323834: Validation loss improved from -0.51419 to -0.52474! Patience: 7/50
2024-12-20 04:37:17.324891: train_loss -0.7354
2024-12-20 04:37:17.325825: val_loss -0.5247
2024-12-20 04:37:17.326648: Pseudo dice [0.7396]
2024-12-20 04:37:17.327459: Epoch time: 484.73 s
2024-12-20 04:37:18.757233: 
2024-12-20 04:37:18.758815: Epoch 73
2024-12-20 04:37:18.759880: Current learning rate: 0.00549
2024-12-20 04:45:02.519888: Validation loss did not improve from -0.52474. Patience: 1/50
2024-12-20 04:45:02.520891: train_loss -0.7288
2024-12-20 04:45:02.521689: val_loss -0.4495
2024-12-20 04:45:02.522448: Pseudo dice [0.7069]
2024-12-20 04:45:02.523188: Epoch time: 463.76 s
2024-12-20 04:45:04.090849: 
2024-12-20 04:45:04.092214: Epoch 74
2024-12-20 04:45:04.093155: Current learning rate: 0.00542
2024-12-20 04:52:22.827091: Validation loss did not improve from -0.52474. Patience: 2/50
2024-12-20 04:52:22.828017: train_loss -0.7265
2024-12-20 04:52:22.828900: val_loss -0.5187
2024-12-20 04:52:22.829698: Pseudo dice [0.7394]
2024-12-20 04:52:22.830444: Epoch time: 438.74 s
2024-12-20 04:52:23.367250: Yayy! New best EMA pseudo Dice: 0.7125
2024-12-20 04:52:25.292511: 
2024-12-20 04:52:25.293856: Epoch 75
2024-12-20 04:52:25.294656: Current learning rate: 0.00536
2024-12-20 04:59:50.151600: Validation loss did not improve from -0.52474. Patience: 3/50
2024-12-20 04:59:50.152771: train_loss -0.7325
2024-12-20 04:59:50.153720: val_loss -0.4985
2024-12-20 04:59:50.155162: Pseudo dice [0.7294]
2024-12-20 04:59:50.155961: Epoch time: 444.86 s
2024-12-20 04:59:50.156689: Yayy! New best EMA pseudo Dice: 0.7142
2024-12-20 04:59:52.128245: 
2024-12-20 04:59:52.129790: Epoch 76
2024-12-20 04:59:52.130771: Current learning rate: 0.00529
2024-12-20 05:07:34.047898: Validation loss did not improve from -0.52474. Patience: 4/50
2024-12-20 05:07:34.048959: train_loss -0.7333
2024-12-20 05:07:34.049862: val_loss -0.4449
2024-12-20 05:07:34.050802: Pseudo dice [0.6908]
2024-12-20 05:07:34.051705: Epoch time: 461.92 s
2024-12-20 05:07:35.437679: 
2024-12-20 05:07:35.439041: Epoch 77
2024-12-20 05:07:35.440151: Current learning rate: 0.00523
2024-12-20 05:15:07.674800: Validation loss did not improve from -0.52474. Patience: 5/50
2024-12-20 05:15:07.675864: train_loss -0.7324
2024-12-20 05:15:07.676688: val_loss -0.4869
2024-12-20 05:15:07.677421: Pseudo dice [0.7227]
2024-12-20 05:15:07.678176: Epoch time: 452.24 s
2024-12-20 05:15:09.219720: 
2024-12-20 05:15:09.221127: Epoch 78
2024-12-20 05:15:09.221997: Current learning rate: 0.00517
2024-12-20 05:22:49.270766: Validation loss did not improve from -0.52474. Patience: 6/50
2024-12-20 05:22:49.271823: train_loss -0.7406
2024-12-20 05:22:49.272619: val_loss -0.4729
2024-12-20 05:22:49.273384: Pseudo dice [0.7118]
2024-12-20 05:22:49.274148: Epoch time: 460.05 s
2024-12-20 05:22:50.718790: 
2024-12-20 05:22:50.720140: Epoch 79
2024-12-20 05:22:50.720910: Current learning rate: 0.0051
2024-12-20 05:30:18.583753: Validation loss did not improve from -0.52474. Patience: 7/50
2024-12-20 05:30:18.585141: train_loss -0.7355
2024-12-20 05:30:18.585927: val_loss -0.4913
2024-12-20 05:30:18.586622: Pseudo dice [0.7252]
2024-12-20 05:30:18.587575: Epoch time: 447.87 s
2024-12-20 05:30:20.522008: 
2024-12-20 05:30:20.523299: Epoch 80
2024-12-20 05:30:20.524108: Current learning rate: 0.00504
2024-12-20 05:37:45.350164: Validation loss did not improve from -0.52474. Patience: 8/50
2024-12-20 05:37:45.351486: train_loss -0.7378
2024-12-20 05:37:45.352442: val_loss -0.4738
2024-12-20 05:37:45.353288: Pseudo dice [0.7193]
2024-12-20 05:37:45.354023: Epoch time: 444.83 s
2024-12-20 05:37:45.354781: Yayy! New best EMA pseudo Dice: 0.7146
2024-12-20 05:37:47.211396: 
2024-12-20 05:37:47.212801: Epoch 81
2024-12-20 05:37:47.213657: Current learning rate: 0.00497
2024-12-20 05:44:51.937766: Validation loss did not improve from -0.52474. Patience: 9/50
2024-12-20 05:44:51.938848: train_loss -0.7405
2024-12-20 05:44:51.939764: val_loss -0.4562
2024-12-20 05:44:51.940540: Pseudo dice [0.7061]
2024-12-20 05:44:51.941303: Epoch time: 424.73 s
2024-12-20 05:44:53.413715: 
2024-12-20 05:44:53.415102: Epoch 82
2024-12-20 05:44:53.415920: Current learning rate: 0.00491
2024-12-20 05:51:57.337488: Validation loss did not improve from -0.52474. Patience: 10/50
2024-12-20 05:51:57.338517: train_loss -0.7402
2024-12-20 05:51:57.339634: val_loss -0.468
2024-12-20 05:51:57.340563: Pseudo dice [0.701]
2024-12-20 05:51:57.341460: Epoch time: 423.93 s
2024-12-20 05:51:59.176911: 
2024-12-20 05:51:59.178206: Epoch 83
2024-12-20 05:51:59.179082: Current learning rate: 0.00484
2024-12-20 05:58:54.801148: Validation loss did not improve from -0.52474. Patience: 11/50
2024-12-20 05:58:54.802070: train_loss -0.7391
2024-12-20 05:58:54.802804: val_loss -0.4479
2024-12-20 05:58:54.803586: Pseudo dice [0.6979]
2024-12-20 05:58:54.804355: Epoch time: 415.63 s
2024-12-20 05:58:56.208190: 
2024-12-20 05:58:56.209472: Epoch 84
2024-12-20 05:58:56.210259: Current learning rate: 0.00478
2024-12-20 06:05:59.940640: Validation loss did not improve from -0.52474. Patience: 12/50
2024-12-20 06:05:59.941474: train_loss -0.7457
2024-12-20 06:05:59.942382: val_loss -0.5043
2024-12-20 06:05:59.943063: Pseudo dice [0.7323]
2024-12-20 06:05:59.943795: Epoch time: 423.73 s
2024-12-20 06:06:01.731886: 
2024-12-20 06:06:01.733078: Epoch 85
2024-12-20 06:06:01.733855: Current learning rate: 0.00471
2024-12-20 06:13:13.606975: Validation loss did not improve from -0.52474. Patience: 13/50
2024-12-20 06:13:13.608032: train_loss -0.7491
2024-12-20 06:13:13.608964: val_loss -0.4715
2024-12-20 06:13:13.609786: Pseudo dice [0.7267]
2024-12-20 06:13:13.610667: Epoch time: 431.88 s
2024-12-20 06:13:14.984942: 
2024-12-20 06:13:14.986325: Epoch 86
2024-12-20 06:13:14.987249: Current learning rate: 0.00465
2024-12-20 06:20:18.515387: Validation loss did not improve from -0.52474. Patience: 14/50
2024-12-20 06:20:18.516412: train_loss -0.7447
2024-12-20 06:20:18.517392: val_loss -0.4706
2024-12-20 06:20:18.518152: Pseudo dice [0.7124]
2024-12-20 06:20:18.519067: Epoch time: 423.53 s
2024-12-20 06:20:19.968696: 
2024-12-20 06:20:19.970138: Epoch 87
2024-12-20 06:20:19.970986: Current learning rate: 0.00458
2024-12-20 06:28:02.552897: Validation loss did not improve from -0.52474. Patience: 15/50
2024-12-20 06:28:02.553509: train_loss -0.7472
2024-12-20 06:28:02.554245: val_loss -0.4483
2024-12-20 06:28:02.554893: Pseudo dice [0.7036]
2024-12-20 06:28:02.555546: Epoch time: 462.59 s
2024-12-20 06:28:03.918417: 
2024-12-20 06:28:03.919895: Epoch 88
2024-12-20 06:28:03.920686: Current learning rate: 0.00452
2024-12-20 06:35:21.665700: Validation loss did not improve from -0.52474. Patience: 16/50
2024-12-20 06:35:21.670385: train_loss -0.7533
2024-12-20 06:35:21.671944: val_loss -0.4387
2024-12-20 06:35:21.672754: Pseudo dice [0.7021]
2024-12-20 06:35:21.673950: Epoch time: 437.75 s
2024-12-20 06:35:23.072219: 
2024-12-20 06:35:23.073666: Epoch 89
2024-12-20 06:35:23.074718: Current learning rate: 0.00445
2024-12-20 06:42:30.025161: Validation loss did not improve from -0.52474. Patience: 17/50
2024-12-20 06:42:30.026125: train_loss -0.7548
2024-12-20 06:42:30.026869: val_loss -0.4555
2024-12-20 06:42:30.027616: Pseudo dice [0.7113]
2024-12-20 06:42:30.028392: Epoch time: 426.96 s
2024-12-20 06:42:31.844148: 
2024-12-20 06:42:31.845674: Epoch 90
2024-12-20 06:42:31.846751: Current learning rate: 0.00438
2024-12-20 06:50:01.945962: Validation loss did not improve from -0.52474. Patience: 18/50
2024-12-20 06:50:01.947017: train_loss -0.7589
2024-12-20 06:50:01.947957: val_loss -0.4464
2024-12-20 06:50:01.948707: Pseudo dice [0.6936]
2024-12-20 06:50:01.949412: Epoch time: 450.1 s
2024-12-20 06:50:03.290330: 
2024-12-20 06:50:03.291700: Epoch 91
2024-12-20 06:50:03.292449: Current learning rate: 0.00432
2024-12-20 06:57:01.759562: Validation loss did not improve from -0.52474. Patience: 19/50
2024-12-20 06:57:01.760648: train_loss -0.7552
2024-12-20 06:57:01.761645: val_loss -0.4564
2024-12-20 06:57:01.762539: Pseudo dice [0.7056]
2024-12-20 06:57:01.763364: Epoch time: 418.47 s
2024-12-20 06:57:03.149720: 
2024-12-20 06:57:03.151142: Epoch 92
2024-12-20 06:57:03.152140: Current learning rate: 0.00425
2024-12-20 07:03:52.885061: Validation loss did not improve from -0.52474. Patience: 20/50
2024-12-20 07:03:52.886206: train_loss -0.7575
2024-12-20 07:03:52.887300: val_loss -0.4929
2024-12-20 07:03:52.888114: Pseudo dice [0.7205]
2024-12-20 07:03:52.889006: Epoch time: 409.74 s
2024-12-20 07:03:54.437329: 
2024-12-20 07:03:54.438613: Epoch 93
2024-12-20 07:03:54.439605: Current learning rate: 0.00419
2024-12-20 07:11:24.419660: Validation loss did not improve from -0.52474. Patience: 21/50
2024-12-20 07:11:24.420687: train_loss -0.7588
2024-12-20 07:11:24.421718: val_loss -0.4896
2024-12-20 07:11:24.422462: Pseudo dice [0.7231]
2024-12-20 07:11:24.423278: Epoch time: 449.98 s
2024-12-20 07:11:26.845044: 
2024-12-20 07:11:26.846572: Epoch 94
2024-12-20 07:11:26.847533: Current learning rate: 0.00412
2024-12-20 07:18:18.747540: Validation loss did not improve from -0.52474. Patience: 22/50
2024-12-20 07:18:18.748720: train_loss -0.7583
2024-12-20 07:18:18.749500: val_loss -0.4776
2024-12-20 07:18:18.750266: Pseudo dice [0.717]
2024-12-20 07:18:18.751000: Epoch time: 411.9 s
2024-12-20 07:18:20.595001: 
2024-12-20 07:18:20.596651: Epoch 95
2024-12-20 07:18:20.597496: Current learning rate: 0.00405
2024-12-20 07:25:08.014260: Validation loss did not improve from -0.52474. Patience: 23/50
2024-12-20 07:25:08.015264: train_loss -0.7559
2024-12-20 07:25:08.016049: val_loss -0.476
2024-12-20 07:25:08.016791: Pseudo dice [0.7152]
2024-12-20 07:25:08.017492: Epoch time: 407.42 s
2024-12-20 07:25:09.390289: 
2024-12-20 07:25:09.391822: Epoch 96
2024-12-20 07:25:09.392576: Current learning rate: 0.00399
2024-12-20 07:32:39.162318: Validation loss did not improve from -0.52474. Patience: 24/50
2024-12-20 07:32:39.163374: train_loss -0.7607
2024-12-20 07:32:39.164259: val_loss -0.4765
2024-12-20 07:32:39.165289: Pseudo dice [0.7303]
2024-12-20 07:32:39.166064: Epoch time: 449.78 s
2024-12-20 07:32:40.584000: 
2024-12-20 07:32:40.585171: Epoch 97
2024-12-20 07:32:40.586005: Current learning rate: 0.00392
2024-12-20 07:39:57.721805: Validation loss did not improve from -0.52474. Patience: 25/50
2024-12-20 07:39:57.723375: train_loss -0.762
2024-12-20 07:39:57.724325: val_loss -0.469
2024-12-20 07:39:57.725219: Pseudo dice [0.7131]
2024-12-20 07:39:57.726105: Epoch time: 437.14 s
2024-12-20 07:39:59.177156: 
2024-12-20 07:39:59.178420: Epoch 98
2024-12-20 07:39:59.179339: Current learning rate: 0.00385
2024-12-20 07:46:59.000395: Validation loss did not improve from -0.52474. Patience: 26/50
2024-12-20 07:46:59.001427: train_loss -0.7617
2024-12-20 07:46:59.002295: val_loss -0.4877
2024-12-20 07:46:59.003118: Pseudo dice [0.7316]
2024-12-20 07:46:59.003867: Epoch time: 419.83 s
2024-12-20 07:46:59.004744: Yayy! New best EMA pseudo Dice: 0.7161
2024-12-20 07:47:00.890574: 
2024-12-20 07:47:00.892062: Epoch 99
2024-12-20 07:47:00.892990: Current learning rate: 0.00379
2024-12-20 07:54:34.053005: Validation loss did not improve from -0.52474. Patience: 27/50
2024-12-20 07:54:34.054234: train_loss -0.7629
2024-12-20 07:54:34.055094: val_loss -0.4794
2024-12-20 07:54:34.055885: Pseudo dice [0.7264]
2024-12-20 07:54:34.056593: Epoch time: 453.17 s
2024-12-20 07:54:34.432184: Yayy! New best EMA pseudo Dice: 0.7172
2024-12-20 07:54:36.249537: 
2024-12-20 07:54:36.251008: Epoch 100
2024-12-20 07:54:36.251909: Current learning rate: 0.00372
2024-12-20 08:01:55.382741: Validation loss did not improve from -0.52474. Patience: 28/50
2024-12-20 08:01:55.383921: train_loss -0.7635
2024-12-20 08:01:55.384806: val_loss -0.4952
2024-12-20 08:01:55.385560: Pseudo dice [0.7168]
2024-12-20 08:01:55.386247: Epoch time: 439.14 s
2024-12-20 08:01:56.870579: 
2024-12-20 08:01:56.871614: Epoch 101
2024-12-20 08:01:56.872427: Current learning rate: 0.00365
2024-12-20 08:09:10.088957: Validation loss did not improve from -0.52474. Patience: 29/50
2024-12-20 08:09:10.089901: train_loss -0.7616
2024-12-20 08:09:10.090829: val_loss -0.4043
2024-12-20 08:09:10.091559: Pseudo dice [0.6809]
2024-12-20 08:09:10.092347: Epoch time: 433.22 s
2024-12-20 08:09:11.489832: 
2024-12-20 08:09:11.491313: Epoch 102
2024-12-20 08:09:11.492176: Current learning rate: 0.00359
2024-12-20 08:16:44.668764: Validation loss did not improve from -0.52474. Patience: 30/50
2024-12-20 08:16:44.669839: train_loss -0.7653
2024-12-20 08:16:44.670655: val_loss -0.4473
2024-12-20 08:16:44.671411: Pseudo dice [0.7114]
2024-12-20 08:16:44.672218: Epoch time: 453.18 s
2024-12-20 08:16:46.094103: 
2024-12-20 08:16:46.095437: Epoch 103
2024-12-20 08:16:46.096139: Current learning rate: 0.00352
2024-12-20 08:24:01.541000: Validation loss did not improve from -0.52474. Patience: 31/50
2024-12-20 08:24:01.542485: train_loss -0.7696
2024-12-20 08:24:01.543301: val_loss -0.4794
2024-12-20 08:24:01.544110: Pseudo dice [0.7212]
2024-12-20 08:24:01.544900: Epoch time: 435.45 s
2024-12-20 08:24:02.999352: 
2024-12-20 08:24:03.000785: Epoch 104
2024-12-20 08:24:03.001569: Current learning rate: 0.00345
2024-12-20 08:31:33.023176: Validation loss did not improve from -0.52474. Patience: 32/50
2024-12-20 08:31:33.024021: train_loss -0.7689
2024-12-20 08:31:33.024833: val_loss -0.4441
2024-12-20 08:31:33.025531: Pseudo dice [0.6973]
2024-12-20 08:31:33.026270: Epoch time: 450.03 s
2024-12-20 08:31:35.508340: 
2024-12-20 08:31:35.509717: Epoch 105
2024-12-20 08:31:35.510445: Current learning rate: 0.00338
2024-12-20 08:39:02.993511: Validation loss did not improve from -0.52474. Patience: 33/50
2024-12-20 08:39:02.994264: train_loss -0.7706
2024-12-20 08:39:02.995068: val_loss -0.4633
2024-12-20 08:39:02.995852: Pseudo dice [0.7084]
2024-12-20 08:39:02.996649: Epoch time: 447.49 s
2024-12-20 08:39:04.445353: 
2024-12-20 08:39:04.446560: Epoch 106
2024-12-20 08:39:04.447485: Current learning rate: 0.00332
2024-12-20 08:46:25.609119: Validation loss did not improve from -0.52474. Patience: 34/50
2024-12-20 08:46:25.615096: train_loss -0.7669
2024-12-20 08:46:25.616210: val_loss -0.4751
2024-12-20 08:46:25.617060: Pseudo dice [0.7125]
2024-12-20 08:46:25.618330: Epoch time: 441.17 s
2024-12-20 08:46:27.104881: 
2024-12-20 08:46:27.106113: Epoch 107
2024-12-20 08:46:27.106867: Current learning rate: 0.00325
2024-12-20 08:54:14.443133: Validation loss did not improve from -0.52474. Patience: 35/50
2024-12-20 08:54:14.444112: train_loss -0.7707
2024-12-20 08:54:14.444998: val_loss -0.4841
2024-12-20 08:54:14.445807: Pseudo dice [0.7208]
2024-12-20 08:54:14.446598: Epoch time: 467.34 s
2024-12-20 08:54:15.929873: 
2024-12-20 08:54:15.931223: Epoch 108
2024-12-20 08:54:15.932110: Current learning rate: 0.00318
2024-12-20 09:01:54.940410: Validation loss did not improve from -0.52474. Patience: 36/50
2024-12-20 09:01:54.941749: train_loss -0.7719
2024-12-20 09:01:54.942718: val_loss -0.4861
2024-12-20 09:01:54.943601: Pseudo dice [0.7265]
2024-12-20 09:01:54.944374: Epoch time: 459.01 s
2024-12-20 09:01:56.357696: 
2024-12-20 09:01:56.359004: Epoch 109
2024-12-20 09:01:56.359730: Current learning rate: 0.00311
2024-12-20 09:09:43.288523: Validation loss did not improve from -0.52474. Patience: 37/50
2024-12-20 09:09:43.289697: train_loss -0.7704
2024-12-20 09:09:43.290435: val_loss -0.4825
2024-12-20 09:09:43.291153: Pseudo dice [0.7242]
2024-12-20 09:09:43.291802: Epoch time: 466.93 s
2024-12-20 09:09:45.154008: 
2024-12-20 09:09:45.155396: Epoch 110
2024-12-20 09:09:45.156392: Current learning rate: 0.00304
2024-12-20 09:17:23.994526: Validation loss did not improve from -0.52474. Patience: 38/50
2024-12-20 09:17:23.995619: train_loss -0.7747
2024-12-20 09:17:23.996480: val_loss -0.4741
2024-12-20 09:17:23.997231: Pseudo dice [0.7182]
2024-12-20 09:17:23.997973: Epoch time: 458.84 s
2024-12-20 09:17:25.380648: 
2024-12-20 09:17:25.381887: Epoch 111
2024-12-20 09:17:25.382638: Current learning rate: 0.00297
2024-12-20 09:24:34.864274: Validation loss did not improve from -0.52474. Patience: 39/50
2024-12-20 09:24:34.865453: train_loss -0.7776
2024-12-20 09:24:34.866738: val_loss -0.4255
2024-12-20 09:24:34.867737: Pseudo dice [0.6968]
2024-12-20 09:24:34.868840: Epoch time: 429.49 s
2024-12-20 09:24:36.254033: 
2024-12-20 09:24:36.255494: Epoch 112
2024-12-20 09:24:36.256693: Current learning rate: 0.00291
2024-12-20 09:31:57.391877: Validation loss did not improve from -0.52474. Patience: 40/50
2024-12-20 09:31:57.392922: train_loss -0.7742
2024-12-20 09:31:57.393841: val_loss -0.472
2024-12-20 09:31:57.394718: Pseudo dice [0.71]
2024-12-20 09:31:57.395511: Epoch time: 441.14 s
2024-12-20 09:31:58.869690: 
2024-12-20 09:31:58.871058: Epoch 113
2024-12-20 09:31:58.871845: Current learning rate: 0.00284
2024-12-20 09:39:23.415372: Validation loss did not improve from -0.52474. Patience: 41/50
2024-12-20 09:39:23.416445: train_loss -0.7769
2024-12-20 09:39:23.417315: val_loss -0.4893
2024-12-20 09:39:23.418065: Pseudo dice [0.7274]
2024-12-20 09:39:23.418849: Epoch time: 444.55 s
2024-12-20 09:39:24.900965: 
2024-12-20 09:39:24.902515: Epoch 114
2024-12-20 09:39:24.903677: Current learning rate: 0.00277
2024-12-20 09:46:15.444209: Validation loss did not improve from -0.52474. Patience: 42/50
2024-12-20 09:46:15.447256: train_loss -0.7757
2024-12-20 09:46:15.448495: val_loss -0.4472
2024-12-20 09:46:15.449215: Pseudo dice [0.7008]
2024-12-20 09:46:15.450122: Epoch time: 410.55 s
2024-12-20 09:46:17.323901: 
2024-12-20 09:46:17.325603: Epoch 115
2024-12-20 09:46:17.326467: Current learning rate: 0.0027
2024-12-20 09:53:27.146026: Validation loss did not improve from -0.52474. Patience: 43/50
2024-12-20 09:53:27.147248: train_loss -0.7779
2024-12-20 09:53:27.148239: val_loss -0.4593
2024-12-20 09:53:27.149119: Pseudo dice [0.709]
2024-12-20 09:53:27.149854: Epoch time: 429.82 s
2024-12-20 09:53:29.226458: 
2024-12-20 09:53:29.227686: Epoch 116
2024-12-20 09:53:29.228660: Current learning rate: 0.00263
2024-12-20 10:00:37.465377: Validation loss did not improve from -0.52474. Patience: 44/50
2024-12-20 10:00:37.466438: train_loss -0.7781
2024-12-20 10:00:37.467467: val_loss -0.4767
2024-12-20 10:00:37.468118: Pseudo dice [0.7303]
2024-12-20 10:00:37.468868: Epoch time: 428.24 s
2024-12-20 10:00:38.900715: 
2024-12-20 10:00:38.902142: Epoch 117
2024-12-20 10:00:38.902959: Current learning rate: 0.00256
2024-12-20 10:07:31.303135: Validation loss did not improve from -0.52474. Patience: 45/50
2024-12-20 10:07:31.304237: train_loss -0.7761
2024-12-20 10:07:31.305252: val_loss -0.4296
2024-12-20 10:07:31.306235: Pseudo dice [0.6919]
2024-12-20 10:07:31.307139: Epoch time: 412.4 s
2024-12-20 10:07:32.768178: 
2024-12-20 10:07:32.769695: Epoch 118
2024-12-20 10:07:32.770739: Current learning rate: 0.00249
2024-12-20 10:14:20.427192: Validation loss did not improve from -0.52474. Patience: 46/50
2024-12-20 10:14:20.428272: train_loss -0.7788
2024-12-20 10:14:20.429220: val_loss -0.447
2024-12-20 10:14:20.430005: Pseudo dice [0.7056]
2024-12-20 10:14:20.430785: Epoch time: 407.66 s
2024-12-20 10:14:21.884788: 
2024-12-20 10:14:21.886195: Epoch 119
2024-12-20 10:14:21.886945: Current learning rate: 0.00242
2024-12-20 10:21:59.592067: Validation loss did not improve from -0.52474. Patience: 47/50
2024-12-20 10:21:59.593680: train_loss -0.7807
2024-12-20 10:21:59.594973: val_loss -0.5057
2024-12-20 10:21:59.595751: Pseudo dice [0.7302]
2024-12-20 10:21:59.596615: Epoch time: 457.71 s
2024-12-20 10:22:01.513342: 
2024-12-20 10:22:01.514757: Epoch 120
2024-12-20 10:22:01.515636: Current learning rate: 0.00235
2024-12-20 10:29:25.832888: Validation loss did not improve from -0.52474. Patience: 48/50
2024-12-20 10:29:25.834012: train_loss -0.7777
2024-12-20 10:29:25.834972: val_loss -0.4653
2024-12-20 10:29:25.835725: Pseudo dice [0.7193]
2024-12-20 10:29:25.836461: Epoch time: 444.32 s
2024-12-20 10:29:27.402515: 
2024-12-20 10:29:27.404000: Epoch 121
2024-12-20 10:29:27.404906: Current learning rate: 0.00228
2024-12-20 10:36:33.949136: Validation loss did not improve from -0.52474. Patience: 49/50
2024-12-20 10:36:33.950185: train_loss -0.7801
2024-12-20 10:36:33.951651: val_loss -0.4635
2024-12-20 10:36:33.952515: Pseudo dice [0.7028]
2024-12-20 10:36:33.953321: Epoch time: 426.55 s
2024-12-20 10:36:35.435969: 
2024-12-20 10:36:35.437335: Epoch 122
2024-12-20 10:36:35.438179: Current learning rate: 0.00221
2024-12-20 10:44:10.912841: Validation loss did not improve from -0.52474. Patience: 50/50
2024-12-20 10:44:10.913831: train_loss -0.7793
2024-12-20 10:44:10.914584: val_loss -0.5151
2024-12-20 10:44:10.915226: Pseudo dice [0.7367]
2024-12-20 10:44:10.916025: Epoch time: 455.48 s
2024-12-20 10:44:12.391117: 
2024-12-20 10:44:12.392116: Epoch 123
2024-12-20 10:44:12.392865: Current learning rate: 0.00214
2024-12-20 10:52:00.388147: Validation loss did not improve from -0.52474. Patience: 51/50
2024-12-20 10:52:00.390697: train_loss -0.7815
2024-12-20 10:52:00.391553: val_loss -0.4902
2024-12-20 10:52:00.392276: Pseudo dice [0.7331]
2024-12-20 10:52:00.392951: Epoch time: 468.0 s
2024-12-20 10:52:01.839345: 
2024-12-20 10:52:01.840962: Epoch 124
2024-12-20 10:52:01.841988: Current learning rate: 0.00207
2024-12-20 10:59:11.767887: Validation loss did not improve from -0.52474. Patience: 52/50
2024-12-20 10:59:11.769000: train_loss -0.7875
2024-12-20 10:59:11.769962: val_loss -0.4378
2024-12-20 10:59:11.770775: Pseudo dice [0.6906]
2024-12-20 10:59:11.771639: Epoch time: 429.93 s
2024-12-20 10:59:13.648030: 
2024-12-20 10:59:13.649523: Epoch 125
2024-12-20 10:59:13.650702: Current learning rate: 0.00199
2024-12-20 11:06:38.379626: Validation loss did not improve from -0.52474. Patience: 53/50
2024-12-20 11:06:38.380558: train_loss -0.7837
2024-12-20 11:06:38.381452: val_loss -0.4414
2024-12-20 11:06:38.382263: Pseudo dice [0.6966]
2024-12-20 11:06:38.382982: Epoch time: 444.73 s
2024-12-20 11:06:39.816492: 
2024-12-20 11:06:39.817916: Epoch 126
2024-12-20 11:06:39.819101: Current learning rate: 0.00192
2024-12-20 11:13:53.627498: Validation loss did not improve from -0.52474. Patience: 54/50
2024-12-20 11:13:53.628553: train_loss -0.7827
2024-12-20 11:13:53.629576: val_loss -0.4606
2024-12-20 11:13:53.630479: Pseudo dice [0.7133]
2024-12-20 11:13:53.631391: Epoch time: 433.81 s
2024-12-20 11:13:56.210341: 
2024-12-20 11:13:56.211913: Epoch 127
2024-12-20 11:13:56.212914: Current learning rate: 0.00185
2024-12-20 11:20:58.430439: Validation loss did not improve from -0.52474. Patience: 55/50
2024-12-20 11:20:58.431502: train_loss -0.7841
2024-12-20 11:20:58.432365: val_loss -0.4721
2024-12-20 11:20:58.433180: Pseudo dice [0.7087]
2024-12-20 11:20:58.434016: Epoch time: 422.22 s
2024-12-20 11:20:59.944215: 
2024-12-20 11:20:59.945598: Epoch 128
2024-12-20 11:20:59.946535: Current learning rate: 0.00178
2024-12-20 11:28:24.810334: Validation loss did not improve from -0.52474. Patience: 56/50
2024-12-20 11:28:24.825072: train_loss -0.7856
2024-12-20 11:28:24.826223: val_loss -0.4665
2024-12-20 11:28:24.827039: Pseudo dice [0.7206]
2024-12-20 11:28:24.827896: Epoch time: 444.88 s
2024-12-20 11:28:26.310496: 
2024-12-20 11:28:26.311807: Epoch 129
2024-12-20 11:28:26.312713: Current learning rate: 0.0017
2024-12-20 11:35:49.233134: Validation loss did not improve from -0.52474. Patience: 57/50
2024-12-20 11:35:49.233886: train_loss -0.786
2024-12-20 11:35:49.234739: val_loss -0.5009
2024-12-20 11:35:49.235636: Pseudo dice [0.7318]
2024-12-20 11:35:49.236553: Epoch time: 442.92 s
2024-12-20 11:35:51.146451: 
2024-12-20 11:35:51.147974: Epoch 130
2024-12-20 11:35:51.148860: Current learning rate: 0.00163
2024-12-20 11:42:17.997977: Validation loss did not improve from -0.52474. Patience: 58/50
2024-12-20 11:42:17.999748: train_loss -0.7862
2024-12-20 11:42:18.002218: val_loss -0.4701
2024-12-20 11:42:18.003113: Pseudo dice [0.7264]
2024-12-20 11:42:18.004218: Epoch time: 386.85 s
2024-12-20 11:42:19.446443: 
2024-12-20 11:42:19.447861: Epoch 131
2024-12-20 11:42:19.448735: Current learning rate: 0.00156
2024-12-20 11:49:52.147423: Validation loss did not improve from -0.52474. Patience: 59/50
2024-12-20 11:49:52.148506: train_loss -0.7848
2024-12-20 11:49:52.149625: val_loss -0.4568
2024-12-20 11:49:52.150415: Pseudo dice [0.7142]
2024-12-20 11:49:52.151228: Epoch time: 452.7 s
2024-12-20 11:49:53.639047: 
2024-12-20 11:49:53.640185: Epoch 132
2024-12-20 11:49:53.641060: Current learning rate: 0.00148
2024-12-20 11:57:30.384988: Validation loss did not improve from -0.52474. Patience: 60/50
2024-12-20 11:57:30.385993: train_loss -0.7879
2024-12-20 11:57:30.386847: val_loss -0.4672
2024-12-20 11:57:30.387563: Pseudo dice [0.7167]
2024-12-20 11:57:30.388237: Epoch time: 456.75 s
2024-12-20 11:57:31.864819: 
2024-12-20 11:57:31.866118: Epoch 133
2024-12-20 11:57:31.866961: Current learning rate: 0.00141
2024-12-20 12:04:06.449975: Validation loss did not improve from -0.52474. Patience: 61/50
2024-12-20 12:04:06.451109: train_loss -0.7875
2024-12-20 12:04:06.451907: val_loss -0.4426
2024-12-20 12:04:06.452585: Pseudo dice [0.7094]
2024-12-20 12:04:06.453314: Epoch time: 394.59 s
2024-12-20 12:04:07.887665: 
2024-12-20 12:04:07.889044: Epoch 134
2024-12-20 12:04:07.889835: Current learning rate: 0.00133
2024-12-20 12:11:50.257138: Validation loss did not improve from -0.52474. Patience: 62/50
2024-12-20 12:11:50.258390: train_loss -0.788
2024-12-20 12:11:50.259281: val_loss -0.495
2024-12-20 12:11:50.260000: Pseudo dice [0.7337]
2024-12-20 12:11:50.260812: Epoch time: 462.37 s
2024-12-20 12:11:50.883759: Yayy! New best EMA pseudo Dice: 0.7172
2024-12-20 12:11:52.852877: 
2024-12-20 12:11:52.854194: Epoch 135
2024-12-20 12:11:52.855007: Current learning rate: 0.00126
2024-12-20 12:19:42.218387: Validation loss did not improve from -0.52474. Patience: 63/50
2024-12-20 12:19:42.219319: train_loss -0.7889
2024-12-20 12:19:42.220314: val_loss -0.438
2024-12-20 12:19:42.221207: Pseudo dice [0.6996]
2024-12-20 12:19:42.222215: Epoch time: 469.37 s
2024-12-20 12:19:43.755877: 
2024-12-20 12:19:43.757526: Epoch 136
2024-12-20 12:19:43.758570: Current learning rate: 0.00118
2024-12-20 12:26:42.753587: Validation loss did not improve from -0.52474. Patience: 64/50
2024-12-20 12:26:42.754645: train_loss -0.7897
2024-12-20 12:26:42.755633: val_loss -0.4869
2024-12-20 12:26:42.756663: Pseudo dice [0.7247]
2024-12-20 12:26:42.757663: Epoch time: 419.0 s
2024-12-20 12:26:44.240184: 
2024-12-20 12:26:44.241715: Epoch 137
2024-12-20 12:26:44.242604: Current learning rate: 0.00111
2024-12-20 12:34:20.450720: Validation loss did not improve from -0.52474. Patience: 65/50
2024-12-20 12:34:20.451714: train_loss -0.7896
2024-12-20 12:34:20.452662: val_loss -0.4831
2024-12-20 12:34:20.453482: Pseudo dice [0.7294]
2024-12-20 12:34:20.454283: Epoch time: 456.21 s
2024-12-20 12:34:20.454979: Yayy! New best EMA pseudo Dice: 0.7177
2024-12-20 12:34:22.671422: 
2024-12-20 12:34:22.672782: Epoch 138
2024-12-20 12:34:22.673665: Current learning rate: 0.00103
2024-12-20 12:41:47.505497: Validation loss did not improve from -0.52474. Patience: 66/50
2024-12-20 12:41:47.506562: train_loss -0.7855
2024-12-20 12:41:47.507470: val_loss -0.4456
2024-12-20 12:41:47.508223: Pseudo dice [0.7072]
2024-12-20 12:41:47.509248: Epoch time: 444.84 s
2024-12-20 12:41:48.967697: 
2024-12-20 12:41:48.969264: Epoch 139
2024-12-20 12:41:48.970271: Current learning rate: 0.00095
2024-12-20 12:48:43.529137: Validation loss did not improve from -0.52474. Patience: 67/50
2024-12-20 12:48:43.530191: train_loss -0.7931
2024-12-20 12:48:43.531080: val_loss -0.4728
2024-12-20 12:48:43.531757: Pseudo dice [0.7116]
2024-12-20 12:48:43.532532: Epoch time: 414.56 s
2024-12-20 12:48:45.411144: 
2024-12-20 12:48:45.412102: Epoch 140
2024-12-20 12:48:45.412883: Current learning rate: 0.00087
2024-12-20 12:56:42.280275: Validation loss did not improve from -0.52474. Patience: 68/50
2024-12-20 12:56:42.280999: train_loss -0.7897
2024-12-20 12:56:42.281822: val_loss -0.4715
2024-12-20 12:56:42.282584: Pseudo dice [0.7227]
2024-12-20 12:56:42.283324: Epoch time: 476.87 s
2024-12-20 12:56:43.735026: 
2024-12-20 12:56:43.736281: Epoch 141
2024-12-20 12:56:43.737094: Current learning rate: 0.00079
2024-12-20 13:04:55.111497: Validation loss did not improve from -0.52474. Patience: 69/50
2024-12-20 13:04:55.167076: train_loss -0.7909
2024-12-20 13:04:55.168407: val_loss -0.4767
2024-12-20 13:04:55.169160: Pseudo dice [0.7168]
2024-12-20 13:04:55.170003: Epoch time: 491.42 s
2024-12-20 13:04:56.708589: 
2024-12-20 13:04:56.710113: Epoch 142
2024-12-20 13:04:56.711184: Current learning rate: 0.00071
2024-12-20 13:11:48.026736: Validation loss did not improve from -0.52474. Patience: 70/50
2024-12-20 13:11:48.027878: train_loss -0.7902
2024-12-20 13:11:48.028709: val_loss -0.4421
2024-12-20 13:11:48.029579: Pseudo dice [0.6933]
2024-12-20 13:11:48.030404: Epoch time: 411.32 s
2024-12-20 13:11:49.487327: 
2024-12-20 13:11:49.488559: Epoch 143
2024-12-20 13:11:49.489274: Current learning rate: 0.00063
2024-12-20 13:19:21.752318: Validation loss did not improve from -0.52474. Patience: 71/50
2024-12-20 13:19:21.753401: train_loss -0.7919
2024-12-20 13:19:21.754313: val_loss -0.4815
2024-12-20 13:19:21.755154: Pseudo dice [0.729]
2024-12-20 13:19:21.755902: Epoch time: 452.27 s
2024-12-20 13:19:23.260756: 
2024-12-20 13:19:23.262094: Epoch 144
2024-12-20 13:19:23.263113: Current learning rate: 0.00055
2024-12-20 13:26:38.136070: Validation loss did not improve from -0.52474. Patience: 72/50
2024-12-20 13:26:38.137012: train_loss -0.7912
2024-12-20 13:26:38.137757: val_loss -0.4899
2024-12-20 13:26:38.138565: Pseudo dice [0.7272]
2024-12-20 13:26:38.139417: Epoch time: 434.88 s
2024-12-20 13:26:40.280396: 
2024-12-20 13:26:40.281681: Epoch 145
2024-12-20 13:26:40.282413: Current learning rate: 0.00047
2024-12-20 13:33:52.400601: Validation loss did not improve from -0.52474. Patience: 73/50
2024-12-20 13:33:52.401483: train_loss -0.7936
2024-12-20 13:33:52.402314: val_loss -0.4888
2024-12-20 13:33:52.403090: Pseudo dice [0.7245]
2024-12-20 13:33:52.403858: Epoch time: 432.12 s
2024-12-20 13:33:52.404690: Yayy! New best EMA pseudo Dice: 0.7178
2024-12-20 13:33:54.376892: 
2024-12-20 13:33:54.378294: Epoch 146
2024-12-20 13:33:54.379069: Current learning rate: 0.00038
2024-12-20 13:42:35.588073: Validation loss did not improve from -0.52474. Patience: 74/50
2024-12-20 13:42:35.589719: train_loss -0.7941
2024-12-20 13:42:35.590695: val_loss -0.4851
2024-12-20 13:42:35.591452: Pseudo dice [0.7277]
2024-12-20 13:42:35.592210: Epoch time: 521.21 s
2024-12-20 13:42:35.592970: Yayy! New best EMA pseudo Dice: 0.7188
2024-12-20 13:42:37.526160: 
2024-12-20 13:42:37.528023: Epoch 147
2024-12-20 13:42:37.529102: Current learning rate: 0.0003
2024-12-20 13:50:16.026698: Validation loss did not improve from -0.52474. Patience: 75/50
2024-12-20 13:50:16.028219: train_loss -0.7924
2024-12-20 13:50:16.030396: val_loss -0.4384
2024-12-20 13:50:16.031323: Pseudo dice [0.704]
2024-12-20 13:50:16.032878: Epoch time: 458.5 s
2024-12-20 13:50:17.473503: 
2024-12-20 13:50:17.474834: Epoch 148
2024-12-20 13:50:17.475642: Current learning rate: 0.00021
2024-12-20 13:57:47.251994: Validation loss did not improve from -0.52474. Patience: 76/50
2024-12-20 13:57:47.253004: train_loss -0.7959
2024-12-20 13:57:47.253845: val_loss -0.4495
2024-12-20 13:57:47.254705: Pseudo dice [0.7088]
2024-12-20 13:57:47.255587: Epoch time: 449.78 s
2024-12-20 13:57:49.265971: 
2024-12-20 13:57:49.267454: Epoch 149
2024-12-20 13:57:49.268192: Current learning rate: 0.00011
2024-12-20 14:05:03.149009: Validation loss did not improve from -0.52474. Patience: 77/50
2024-12-20 14:05:03.149796: train_loss -0.7944
2024-12-20 14:05:03.150524: val_loss -0.5168
2024-12-20 14:05:03.151154: Pseudo dice [0.7394]
2024-12-20 14:05:03.151797: Epoch time: 433.88 s
2024-12-20 14:05:04.987875: Training done.
2024-12-20 14:05:05.177212: Using splits from existing split file: /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset308_Sohee_Calcium_OCT_CrossValidation/splits_final_60.json
2024-12-20 14:05:05.193898: The split file contains 5 splits.
2024-12-20 14:05:05.195098: Desired fold for training: 0
2024-12-20 14:05:05.196164: This split has 4 training and 4 validation cases.
2024-12-20 14:05:05.197501: predicting 101-045
2024-12-20 14:05:05.269013: 101-045, shape torch.Size([1, 375, 498, 498]), rank 0
2024-12-20 14:07:01.425779: predicting 701-013
2024-12-20 14:07:01.472159: 701-013, shape torch.Size([1, 375, 498, 498]), rank 0
2024-12-20 14:09:29.285039: predicting 704-003
2024-12-20 14:09:29.303165: 704-003, shape torch.Size([1, 375, 498, 498]), rank 0
2024-12-20 14:11:24.872034: predicting 706-005
2024-12-20 14:11:24.886614: 706-005, shape torch.Size([1, 375, 498, 498]), rank 0
2024-12-20 14:13:47.410104: Validation complete
2024-12-20 14:13:47.411089: Mean Validation Dice:  0.7153441849667507
2024-12-19 19:40:58.382164: unpacking done...
2024-12-19 19:40:58.390134: Unable to plot network architecture: nnUNet_compile is enabled!
2024-12-19 19:40:58.420789: 
2024-12-19 19:40:58.421886: Epoch 0
2024-12-19 19:40:58.422794: Current learning rate: 0.01
2024-12-19 19:48:55.049188: Validation loss improved from 1000.00000 to -0.12450! Patience: 0/50
2024-12-19 19:48:55.049836: train_loss -0.0736
2024-12-19 19:48:55.050645: val_loss -0.1245
2024-12-19 19:48:55.051405: Pseudo dice [0.4578]
2024-12-19 19:48:55.052294: Epoch time: 476.63 s
2024-12-19 19:48:55.052978: Yayy! New best EMA pseudo Dice: 0.4578
2024-12-19 19:48:56.571469: 
2024-12-19 19:48:56.572768: Epoch 1
2024-12-19 19:48:56.573604: Current learning rate: 0.00994
2024-12-19 19:55:24.332901: Validation loss improved from -0.12450 to -0.22173! Patience: 0/50
2024-12-19 19:55:24.333542: train_loss -0.2301
2024-12-19 19:55:24.334301: val_loss -0.2217
2024-12-19 19:55:24.335132: Pseudo dice [0.5452]
2024-12-19 19:55:24.335974: Epoch time: 387.76 s
2024-12-19 19:55:24.336837: Yayy! New best EMA pseudo Dice: 0.4666
2024-12-19 19:55:26.110967: 
2024-12-19 19:55:26.112207: Epoch 2
2024-12-19 19:55:26.112992: Current learning rate: 0.00988
2024-12-19 20:01:53.466891: Validation loss improved from -0.22173 to -0.26903! Patience: 0/50
2024-12-19 20:01:53.467837: train_loss -0.3182
2024-12-19 20:01:53.468854: val_loss -0.269
2024-12-19 20:01:53.469667: Pseudo dice [0.5951]
2024-12-19 20:01:53.470541: Epoch time: 387.36 s
2024-12-19 20:01:53.471388: Yayy! New best EMA pseudo Dice: 0.4794
2024-12-19 20:01:55.342103: 
2024-12-19 20:01:55.343327: Epoch 3
2024-12-19 20:01:55.344331: Current learning rate: 0.00982
2024-12-19 20:08:55.632540: Validation loss improved from -0.26903 to -0.29891! Patience: 0/50
2024-12-19 20:08:55.633338: train_loss -0.3254
2024-12-19 20:08:55.634302: val_loss -0.2989
2024-12-19 20:08:55.635133: Pseudo dice [0.5993]
2024-12-19 20:08:55.635988: Epoch time: 420.29 s
2024-12-19 20:08:55.636870: Yayy! New best EMA pseudo Dice: 0.4914
2024-12-19 20:08:57.437981: 
2024-12-19 20:08:57.439313: Epoch 4
2024-12-19 20:08:57.440446: Current learning rate: 0.00976
2024-12-19 20:16:05.636434: Validation loss did not improve from -0.29891. Patience: 1/50
2024-12-19 20:16:05.637432: train_loss -0.3563
2024-12-19 20:16:05.638336: val_loss -0.255
2024-12-19 20:16:05.639046: Pseudo dice [0.5838]
2024-12-19 20:16:05.639758: Epoch time: 428.2 s
2024-12-19 20:16:06.030500: Yayy! New best EMA pseudo Dice: 0.5007
2024-12-19 20:16:07.819373: 
2024-12-19 20:16:07.820565: Epoch 5
2024-12-19 20:16:07.821390: Current learning rate: 0.0097
2024-12-19 20:22:57.083933: Validation loss improved from -0.29891 to -0.30791! Patience: 1/50
2024-12-19 20:22:57.085035: train_loss -0.3703
2024-12-19 20:22:57.085917: val_loss -0.3079
2024-12-19 20:22:57.086576: Pseudo dice [0.6103]
2024-12-19 20:22:57.087223: Epoch time: 409.27 s
2024-12-19 20:22:57.087992: Yayy! New best EMA pseudo Dice: 0.5116
2024-12-19 20:22:58.848301: 
2024-12-19 20:22:58.849645: Epoch 6
2024-12-19 20:22:58.850469: Current learning rate: 0.00964
2024-12-19 20:29:45.281961: Validation loss did not improve from -0.30791. Patience: 1/50
2024-12-19 20:29:45.282775: train_loss -0.4235
2024-12-19 20:29:45.283584: val_loss -0.3018
2024-12-19 20:29:45.284507: Pseudo dice [0.6206]
2024-12-19 20:29:45.285316: Epoch time: 406.44 s
2024-12-19 20:29:45.286120: Yayy! New best EMA pseudo Dice: 0.5225
2024-12-19 20:29:47.125053: 
2024-12-19 20:29:47.125972: Epoch 7
2024-12-19 20:29:47.126778: Current learning rate: 0.00958
2024-12-19 20:36:48.637701: Validation loss improved from -0.30791 to -0.37562! Patience: 1/50
2024-12-19 20:36:48.638670: train_loss -0.4437
2024-12-19 20:36:48.639533: val_loss -0.3756
2024-12-19 20:36:48.640165: Pseudo dice [0.6601]
2024-12-19 20:36:48.640846: Epoch time: 421.51 s
2024-12-19 20:36:48.641502: Yayy! New best EMA pseudo Dice: 0.5363
2024-12-19 20:36:50.751437: 
2024-12-19 20:36:50.752689: Epoch 8
2024-12-19 20:36:50.753451: Current learning rate: 0.00952
2024-12-19 20:43:53.145953: Validation loss did not improve from -0.37562. Patience: 1/50
2024-12-19 20:43:53.146995: train_loss -0.4641
2024-12-19 20:43:53.147792: val_loss -0.3371
2024-12-19 20:43:53.148469: Pseudo dice [0.6392]
2024-12-19 20:43:53.149232: Epoch time: 422.4 s
2024-12-19 20:43:53.149912: Yayy! New best EMA pseudo Dice: 0.5466
2024-12-19 20:43:55.068342: 
2024-12-19 20:43:55.069699: Epoch 9
2024-12-19 20:43:55.070449: Current learning rate: 0.00946
2024-12-19 20:51:07.599055: Validation loss did not improve from -0.37562. Patience: 2/50
2024-12-19 20:51:07.601546: train_loss -0.4781
2024-12-19 20:51:07.602851: val_loss -0.3297
2024-12-19 20:51:07.603648: Pseudo dice [0.6249]
2024-12-19 20:51:07.604567: Epoch time: 432.53 s
2024-12-19 20:51:08.007883: Yayy! New best EMA pseudo Dice: 0.5544
2024-12-19 20:51:09.756730: 
2024-12-19 20:51:09.758047: Epoch 10
2024-12-19 20:51:09.758806: Current learning rate: 0.0094
2024-12-19 20:58:05.345907: Validation loss improved from -0.37562 to -0.39505! Patience: 2/50
2024-12-19 20:58:05.347381: train_loss -0.4843
2024-12-19 20:58:05.348791: val_loss -0.3951
2024-12-19 20:58:05.349471: Pseudo dice [0.6647]
2024-12-19 20:58:05.350340: Epoch time: 415.59 s
2024-12-19 20:58:05.351100: Yayy! New best EMA pseudo Dice: 0.5654
2024-12-19 20:58:07.047598: 
2024-12-19 20:58:07.048581: Epoch 11
2024-12-19 20:58:07.049607: Current learning rate: 0.00934
2024-12-19 21:05:22.639347: Validation loss did not improve from -0.39505. Patience: 1/50
2024-12-19 21:05:22.640216: train_loss -0.4981
2024-12-19 21:05:22.640922: val_loss -0.3445
2024-12-19 21:05:22.641595: Pseudo dice [0.6413]
2024-12-19 21:05:22.642278: Epoch time: 435.59 s
2024-12-19 21:05:22.642921: Yayy! New best EMA pseudo Dice: 0.573
2024-12-19 21:05:24.374356: 
2024-12-19 21:05:24.375462: Epoch 12
2024-12-19 21:05:24.376230: Current learning rate: 0.00928
2024-12-19 21:12:26.655362: Validation loss did not improve from -0.39505. Patience: 2/50
2024-12-19 21:12:26.656481: train_loss -0.517
2024-12-19 21:12:26.657400: val_loss -0.3197
2024-12-19 21:12:26.658208: Pseudo dice [0.6258]
2024-12-19 21:12:26.658892: Epoch time: 422.28 s
2024-12-19 21:12:26.659632: Yayy! New best EMA pseudo Dice: 0.5783
2024-12-19 21:12:28.380519: 
2024-12-19 21:12:28.381768: Epoch 13
2024-12-19 21:12:28.382756: Current learning rate: 0.00922
2024-12-19 21:19:55.412928: Validation loss improved from -0.39505 to -0.40495! Patience: 2/50
2024-12-19 21:19:55.413844: train_loss -0.5201
2024-12-19 21:19:55.414732: val_loss -0.405
2024-12-19 21:19:55.415435: Pseudo dice [0.6662]
2024-12-19 21:19:55.416254: Epoch time: 447.03 s
2024-12-19 21:19:55.416921: Yayy! New best EMA pseudo Dice: 0.5871
2024-12-19 21:19:57.217651: 
2024-12-19 21:19:57.218925: Epoch 14
2024-12-19 21:19:57.219640: Current learning rate: 0.00916
2024-12-19 21:27:01.174164: Validation loss did not improve from -0.40495. Patience: 1/50
2024-12-19 21:27:01.174815: train_loss -0.5285
2024-12-19 21:27:01.175560: val_loss -0.3663
2024-12-19 21:27:01.176140: Pseudo dice [0.6574]
2024-12-19 21:27:01.176802: Epoch time: 423.96 s
2024-12-19 21:27:01.606236: Yayy! New best EMA pseudo Dice: 0.5941
2024-12-19 21:27:03.364360: 
2024-12-19 21:27:03.365345: Epoch 15
2024-12-19 21:27:03.366276: Current learning rate: 0.0091
2024-12-19 21:34:20.958003: Validation loss did not improve from -0.40495. Patience: 2/50
2024-12-19 21:34:20.958809: train_loss -0.54
2024-12-19 21:34:20.959482: val_loss -0.378
2024-12-19 21:34:20.960178: Pseudo dice [0.6554]
2024-12-19 21:34:20.960881: Epoch time: 437.6 s
2024-12-19 21:34:20.961596: Yayy! New best EMA pseudo Dice: 0.6003
2024-12-19 21:34:22.755445: 
2024-12-19 21:34:22.756463: Epoch 16
2024-12-19 21:34:22.757244: Current learning rate: 0.00903
2024-12-19 21:44:05.164575: Validation loss did not improve from -0.40495. Patience: 3/50
2024-12-19 21:44:05.165580: train_loss -0.5484
2024-12-19 21:44:05.166421: val_loss -0.3761
2024-12-19 21:44:05.167175: Pseudo dice [0.648]
2024-12-19 21:44:05.168032: Epoch time: 582.41 s
2024-12-19 21:44:05.168802: Yayy! New best EMA pseudo Dice: 0.605
2024-12-19 21:44:07.182159: 
2024-12-19 21:44:07.183229: Epoch 17
2024-12-19 21:44:07.184015: Current learning rate: 0.00897
2024-12-19 21:53:33.857991: Validation loss did not improve from -0.40495. Patience: 4/50
2024-12-19 21:53:33.898173: train_loss -0.5731
2024-12-19 21:53:33.899163: val_loss -0.4025
2024-12-19 21:53:33.899852: Pseudo dice [0.6717]
2024-12-19 21:53:33.900926: Epoch time: 566.68 s
2024-12-19 21:53:33.901723: Yayy! New best EMA pseudo Dice: 0.6117
2024-12-19 21:53:35.782559: 
2024-12-19 21:53:35.783964: Epoch 18
2024-12-19 21:53:35.784751: Current learning rate: 0.00891
2024-12-19 22:03:01.050956: Validation loss did not improve from -0.40495. Patience: 5/50
2024-12-19 22:03:01.052007: train_loss -0.5795
2024-12-19 22:03:01.052850: val_loss -0.348
2024-12-19 22:03:01.053507: Pseudo dice [0.6493]
2024-12-19 22:03:01.054202: Epoch time: 565.27 s
2024-12-19 22:03:01.054935: Yayy! New best EMA pseudo Dice: 0.6155
2024-12-19 22:03:03.735532: 
2024-12-19 22:03:03.736837: Epoch 19
2024-12-19 22:03:03.737691: Current learning rate: 0.00885
2024-12-19 22:12:27.876864: Validation loss did not improve from -0.40495. Patience: 6/50
2024-12-19 22:12:27.877518: train_loss -0.5764
2024-12-19 22:12:27.878428: val_loss -0.4049
2024-12-19 22:12:27.879122: Pseudo dice [0.6683]
2024-12-19 22:12:27.879795: Epoch time: 564.14 s
2024-12-19 22:12:28.234514: Yayy! New best EMA pseudo Dice: 0.6207
2024-12-19 22:12:30.074038: 
2024-12-19 22:12:30.075363: Epoch 20
2024-12-19 22:12:30.076315: Current learning rate: 0.00879
2024-12-19 22:21:54.406114: Validation loss did not improve from -0.40495. Patience: 7/50
2024-12-19 22:21:54.408195: train_loss -0.5799
2024-12-19 22:21:54.409178: val_loss -0.3978
2024-12-19 22:21:54.410162: Pseudo dice [0.6714]
2024-12-19 22:21:54.411172: Epoch time: 564.34 s
2024-12-19 22:21:54.412196: Yayy! New best EMA pseudo Dice: 0.6258
2024-12-19 22:21:56.442813: 
2024-12-19 22:21:56.443728: Epoch 21
2024-12-19 22:21:56.444642: Current learning rate: 0.00873
2024-12-19 22:31:20.384573: Validation loss improved from -0.40495 to -0.42952! Patience: 7/50
2024-12-19 22:31:20.385542: train_loss -0.5939
2024-12-19 22:31:20.386511: val_loss -0.4295
2024-12-19 22:31:20.387382: Pseudo dice [0.6877]
2024-12-19 22:31:20.388361: Epoch time: 563.94 s
2024-12-19 22:31:20.389342: Yayy! New best EMA pseudo Dice: 0.632
2024-12-19 22:31:22.161218: 
2024-12-19 22:31:22.163268: Epoch 22
2024-12-19 22:31:22.164136: Current learning rate: 0.00867
2024-12-19 22:40:43.221785: Validation loss improved from -0.42952 to -0.45561! Patience: 0/50
2024-12-19 22:40:43.223400: train_loss -0.5998
2024-12-19 22:40:43.224490: val_loss -0.4556
2024-12-19 22:40:43.225243: Pseudo dice [0.6927]
2024-12-19 22:40:43.226040: Epoch time: 561.06 s
2024-12-19 22:40:43.226748: Yayy! New best EMA pseudo Dice: 0.6381
2024-12-19 22:40:44.986466: 
2024-12-19 22:40:44.988019: Epoch 23
2024-12-19 22:40:44.988734: Current learning rate: 0.00861
2024-12-19 22:49:51.879634: Validation loss did not improve from -0.45561. Patience: 1/50
2024-12-19 22:49:51.880618: train_loss -0.6013
2024-12-19 22:49:51.881453: val_loss -0.4244
2024-12-19 22:49:51.882168: Pseudo dice [0.69]
2024-12-19 22:49:51.882883: Epoch time: 546.9 s
2024-12-19 22:49:51.883659: Yayy! New best EMA pseudo Dice: 0.6433
2024-12-19 22:49:53.601571: 
2024-12-19 22:49:53.602993: Epoch 24
2024-12-19 22:49:53.603810: Current learning rate: 0.00855
2024-12-19 22:58:53.008530: Validation loss did not improve from -0.45561. Patience: 2/50
2024-12-19 22:58:53.009352: train_loss -0.6129
2024-12-19 22:58:53.010241: val_loss -0.3551
2024-12-19 22:58:53.011114: Pseudo dice [0.6427]
2024-12-19 22:58:53.011892: Epoch time: 539.41 s
2024-12-19 22:58:54.742142: 
2024-12-19 22:58:54.743009: Epoch 25
2024-12-19 22:58:54.743718: Current learning rate: 0.00849
2024-12-19 23:08:19.873867: Validation loss did not improve from -0.45561. Patience: 3/50
2024-12-19 23:08:19.875784: train_loss -0.6231
2024-12-19 23:08:19.876681: val_loss -0.4224
2024-12-19 23:08:19.877444: Pseudo dice [0.6937]
2024-12-19 23:08:19.878275: Epoch time: 565.13 s
2024-12-19 23:08:19.879172: Yayy! New best EMA pseudo Dice: 0.6483
2024-12-19 23:08:21.611767: 
2024-12-19 23:08:21.612874: Epoch 26
2024-12-19 23:08:21.613811: Current learning rate: 0.00843
2024-12-19 23:16:52.954697: Validation loss did not improve from -0.45561. Patience: 4/50
2024-12-19 23:16:52.955321: train_loss -0.6292
2024-12-19 23:16:52.956162: val_loss -0.3658
2024-12-19 23:16:52.956943: Pseudo dice [0.6444]
2024-12-19 23:16:52.957665: Epoch time: 511.34 s
2024-12-19 23:16:54.303370: 
2024-12-19 23:16:54.304880: Epoch 27
2024-12-19 23:16:54.305726: Current learning rate: 0.00836
2024-12-19 23:25:23.413998: Validation loss did not improve from -0.45561. Patience: 5/50
2024-12-19 23:25:23.414909: train_loss -0.632
2024-12-19 23:25:23.415974: val_loss -0.4207
2024-12-19 23:25:23.416973: Pseudo dice [0.6745]
2024-12-19 23:25:23.417964: Epoch time: 509.11 s
2024-12-19 23:25:23.418811: Yayy! New best EMA pseudo Dice: 0.6505
2024-12-19 23:25:25.305073: 
2024-12-19 23:25:25.306429: Epoch 28
2024-12-19 23:25:25.307361: Current learning rate: 0.0083
2024-12-19 23:33:44.288194: Validation loss improved from -0.45561 to -0.45770! Patience: 5/50
2024-12-19 23:33:44.289147: train_loss -0.633
2024-12-19 23:33:44.290016: val_loss -0.4577
2024-12-19 23:33:44.290729: Pseudo dice [0.7154]
2024-12-19 23:33:44.291569: Epoch time: 498.99 s
2024-12-19 23:33:44.292284: Yayy! New best EMA pseudo Dice: 0.657
2024-12-19 23:33:46.089814: 
2024-12-19 23:33:46.091079: Epoch 29
2024-12-19 23:33:46.092113: Current learning rate: 0.00824
2024-12-19 23:42:26.576145: Validation loss did not improve from -0.45770. Patience: 1/50
2024-12-19 23:42:26.577297: train_loss -0.6409
2024-12-19 23:42:26.578290: val_loss -0.4076
2024-12-19 23:42:26.579111: Pseudo dice [0.6694]
2024-12-19 23:42:26.579921: Epoch time: 520.49 s
2024-12-19 23:42:27.015439: Yayy! New best EMA pseudo Dice: 0.6583
2024-12-19 23:42:29.230010: 
2024-12-19 23:42:29.231425: Epoch 30
2024-12-19 23:42:29.232365: Current learning rate: 0.00818
2024-12-19 23:50:57.990297: Validation loss did not improve from -0.45770. Patience: 2/50
2024-12-19 23:50:57.991266: train_loss -0.6494
2024-12-19 23:50:57.992266: val_loss -0.4506
2024-12-19 23:50:57.993271: Pseudo dice [0.7171]
2024-12-19 23:50:57.994248: Epoch time: 508.76 s
2024-12-19 23:50:57.995153: Yayy! New best EMA pseudo Dice: 0.6641
2024-12-19 23:50:59.801328: 
2024-12-19 23:50:59.802701: Epoch 31
2024-12-19 23:50:59.803485: Current learning rate: 0.00812
2024-12-19 23:58:43.444028: Validation loss did not improve from -0.45770. Patience: 3/50
2024-12-19 23:58:43.445068: train_loss -0.643
2024-12-19 23:58:43.446071: val_loss -0.4397
2024-12-19 23:58:43.446882: Pseudo dice [0.7012]
2024-12-19 23:58:43.447755: Epoch time: 463.65 s
2024-12-19 23:58:43.448449: Yayy! New best EMA pseudo Dice: 0.6679
2024-12-19 23:58:45.322580: 
2024-12-19 23:58:45.323449: Epoch 32
2024-12-19 23:58:45.324111: Current learning rate: 0.00806
2024-12-20 00:07:55.908981: Validation loss did not improve from -0.45770. Patience: 4/50
2024-12-20 00:07:55.909865: train_loss -0.6496
2024-12-20 00:07:55.910528: val_loss -0.4464
2024-12-20 00:07:55.911153: Pseudo dice [0.7045]
2024-12-20 00:07:55.911883: Epoch time: 550.59 s
2024-12-20 00:07:55.912571: Yayy! New best EMA pseudo Dice: 0.6715
2024-12-20 00:07:57.748100: 
2024-12-20 00:07:57.749032: Epoch 33
2024-12-20 00:07:57.749747: Current learning rate: 0.008
2024-12-20 00:17:00.283307: Validation loss did not improve from -0.45770. Patience: 5/50
2024-12-20 00:17:00.284432: train_loss -0.6567
2024-12-20 00:17:00.285699: val_loss -0.4348
2024-12-20 00:17:00.286609: Pseudo dice [0.698]
2024-12-20 00:17:00.287742: Epoch time: 542.54 s
2024-12-20 00:17:00.288821: Yayy! New best EMA pseudo Dice: 0.6742
2024-12-20 00:17:02.063877: 
2024-12-20 00:17:02.065759: Epoch 34
2024-12-20 00:17:02.066899: Current learning rate: 0.00793
2024-12-20 00:26:00.529590: Validation loss did not improve from -0.45770. Patience: 6/50
2024-12-20 00:26:00.530796: train_loss -0.6639
2024-12-20 00:26:00.531734: val_loss -0.3941
2024-12-20 00:26:00.532732: Pseudo dice [0.6617]
2024-12-20 00:26:00.533542: Epoch time: 538.47 s
2024-12-20 00:26:02.481631: 
2024-12-20 00:26:02.483841: Epoch 35
2024-12-20 00:26:02.485264: Current learning rate: 0.00787
2024-12-20 00:35:07.923865: Validation loss did not improve from -0.45770. Patience: 7/50
2024-12-20 00:35:07.926092: train_loss -0.6594
2024-12-20 00:35:07.927102: val_loss -0.4377
2024-12-20 00:35:07.927857: Pseudo dice [0.6994]
2024-12-20 00:35:07.928647: Epoch time: 545.45 s
2024-12-20 00:35:07.929489: Yayy! New best EMA pseudo Dice: 0.6756
2024-12-20 00:35:09.836376: 
2024-12-20 00:35:09.837551: Epoch 36
2024-12-20 00:35:09.838336: Current learning rate: 0.00781
2024-12-20 00:43:55.715845: Validation loss did not improve from -0.45770. Patience: 8/50
2024-12-20 00:43:55.716634: train_loss -0.6726
2024-12-20 00:43:55.717452: val_loss -0.4431
2024-12-20 00:43:55.718247: Pseudo dice [0.7094]
2024-12-20 00:43:55.719053: Epoch time: 525.88 s
2024-12-20 00:43:55.719849: Yayy! New best EMA pseudo Dice: 0.679
2024-12-20 00:43:57.640665: 
2024-12-20 00:43:57.641884: Epoch 37
2024-12-20 00:43:57.642758: Current learning rate: 0.00775
2024-12-20 00:53:31.035357: Validation loss did not improve from -0.45770. Patience: 9/50
2024-12-20 00:53:31.036535: train_loss -0.6737
2024-12-20 00:53:31.037469: val_loss -0.4361
2024-12-20 00:53:31.038213: Pseudo dice [0.6966]
2024-12-20 00:53:31.039010: Epoch time: 573.4 s
2024-12-20 00:53:31.039690: Yayy! New best EMA pseudo Dice: 0.6807
2024-12-20 00:53:33.067218: 
2024-12-20 00:53:33.068865: Epoch 38
2024-12-20 00:53:33.070005: Current learning rate: 0.00769
2024-12-20 01:03:18.606570: Validation loss did not improve from -0.45770. Patience: 10/50
2024-12-20 01:03:18.607645: train_loss -0.6727
2024-12-20 01:03:18.608539: val_loss -0.3704
2024-12-20 01:03:18.609389: Pseudo dice [0.6533]
2024-12-20 01:03:18.610206: Epoch time: 585.54 s
2024-12-20 01:03:20.062298: 
2024-12-20 01:03:20.063866: Epoch 39
2024-12-20 01:03:20.064862: Current learning rate: 0.00763
2024-12-20 01:12:45.126716: Validation loss did not improve from -0.45770. Patience: 11/50
2024-12-20 01:12:45.128525: train_loss -0.6861
2024-12-20 01:12:45.129646: val_loss -0.4442
2024-12-20 01:12:45.130494: Pseudo dice [0.7092]
2024-12-20 01:12:45.131405: Epoch time: 565.07 s
2024-12-20 01:12:45.539354: Yayy! New best EMA pseudo Dice: 0.6811
2024-12-20 01:12:48.391572: 
2024-12-20 01:12:48.392959: Epoch 40
2024-12-20 01:12:48.393732: Current learning rate: 0.00756
2024-12-20 01:21:52.132636: Validation loss improved from -0.45770 to -0.47738! Patience: 11/50
2024-12-20 01:21:52.133451: train_loss -0.69
2024-12-20 01:21:52.134402: val_loss -0.4774
2024-12-20 01:21:52.135341: Pseudo dice [0.7158]
2024-12-20 01:21:52.136209: Epoch time: 543.74 s
2024-12-20 01:21:52.137141: Yayy! New best EMA pseudo Dice: 0.6846
2024-12-20 01:21:54.063382: 
2024-12-20 01:21:54.064308: Epoch 41
2024-12-20 01:21:54.065160: Current learning rate: 0.0075
2024-12-20 01:31:24.259231: Validation loss did not improve from -0.47738. Patience: 1/50
2024-12-20 01:31:24.260287: train_loss -0.6875
2024-12-20 01:31:24.261129: val_loss -0.4125
2024-12-20 01:31:24.261841: Pseudo dice [0.6917]
2024-12-20 01:31:24.262480: Epoch time: 570.2 s
2024-12-20 01:31:24.263212: Yayy! New best EMA pseudo Dice: 0.6853
2024-12-20 01:31:26.160737: 
2024-12-20 01:31:26.162323: Epoch 42
2024-12-20 01:31:26.163265: Current learning rate: 0.00744
2024-12-20 01:40:56.685461: Validation loss did not improve from -0.47738. Patience: 2/50
2024-12-20 01:40:56.686521: train_loss -0.6901
2024-12-20 01:40:56.687415: val_loss -0.3948
2024-12-20 01:40:56.688228: Pseudo dice [0.6758]
2024-12-20 01:40:56.688947: Epoch time: 570.53 s
2024-12-20 01:40:58.073447: 
2024-12-20 01:40:58.075315: Epoch 43
2024-12-20 01:40:58.076133: Current learning rate: 0.00738
2024-12-20 01:49:42.037936: Validation loss did not improve from -0.47738. Patience: 3/50
2024-12-20 01:49:42.039070: train_loss -0.7005
2024-12-20 01:49:42.040112: val_loss -0.4534
2024-12-20 01:49:42.040880: Pseudo dice [0.7082]
2024-12-20 01:49:42.041637: Epoch time: 523.97 s
2024-12-20 01:49:42.042439: Yayy! New best EMA pseudo Dice: 0.6867
2024-12-20 01:49:43.842003: 
2024-12-20 01:49:43.843586: Epoch 44
2024-12-20 01:49:43.844574: Current learning rate: 0.00732
2024-12-20 01:58:15.243656: Validation loss did not improve from -0.47738. Patience: 4/50
2024-12-20 01:58:15.244926: train_loss -0.703
2024-12-20 01:58:15.245897: val_loss -0.4009
2024-12-20 01:58:15.246779: Pseudo dice [0.6835]
2024-12-20 01:58:15.247684: Epoch time: 511.4 s
2024-12-20 01:58:17.257482: 
2024-12-20 01:58:17.258802: Epoch 45
2024-12-20 01:58:17.259794: Current learning rate: 0.00725
2024-12-20 02:06:48.105886: Validation loss did not improve from -0.47738. Patience: 5/50
2024-12-20 02:06:48.106953: train_loss -0.707
2024-12-20 02:06:48.107788: val_loss -0.4376
2024-12-20 02:06:48.108437: Pseudo dice [0.6964]
2024-12-20 02:06:48.109226: Epoch time: 510.85 s
2024-12-20 02:06:48.109930: Yayy! New best EMA pseudo Dice: 0.6874
2024-12-20 02:06:49.849135: 
2024-12-20 02:06:49.850500: Epoch 46
2024-12-20 02:06:49.851256: Current learning rate: 0.00719
2024-12-20 02:14:51.771217: Validation loss did not improve from -0.47738. Patience: 6/50
2024-12-20 02:14:51.772571: train_loss -0.712
2024-12-20 02:14:51.773563: val_loss -0.4218
2024-12-20 02:14:51.774362: Pseudo dice [0.6962]
2024-12-20 02:14:51.775163: Epoch time: 481.92 s
2024-12-20 02:14:51.775859: Yayy! New best EMA pseudo Dice: 0.6883
2024-12-20 02:14:53.535638: 
2024-12-20 02:14:53.536980: Epoch 47
2024-12-20 02:14:53.537733: Current learning rate: 0.00713
2024-12-20 02:23:18.602225: Validation loss did not improve from -0.47738. Patience: 7/50
2024-12-20 02:23:18.604190: train_loss -0.7144
2024-12-20 02:23:18.605051: val_loss -0.4397
2024-12-20 02:23:18.605824: Pseudo dice [0.6989]
2024-12-20 02:23:18.606673: Epoch time: 505.07 s
2024-12-20 02:23:18.607637: Yayy! New best EMA pseudo Dice: 0.6893
2024-12-20 02:23:20.478025: 
2024-12-20 02:23:20.479525: Epoch 48
2024-12-20 02:23:20.480415: Current learning rate: 0.00707
2024-12-20 02:32:04.556728: Validation loss did not improve from -0.47738. Patience: 8/50
2024-12-20 02:32:04.557809: train_loss -0.7193
2024-12-20 02:32:04.558737: val_loss -0.4455
2024-12-20 02:32:04.559762: Pseudo dice [0.7091]
2024-12-20 02:32:04.560623: Epoch time: 524.08 s
2024-12-20 02:32:04.561512: Yayy! New best EMA pseudo Dice: 0.6913
2024-12-20 02:32:06.497644: 
2024-12-20 02:32:06.499125: Epoch 49
2024-12-20 02:32:06.500210: Current learning rate: 0.007
2024-12-20 02:40:43.987902: Validation loss did not improve from -0.47738. Patience: 9/50
2024-12-20 02:40:43.989168: train_loss -0.7084
2024-12-20 02:40:43.990023: val_loss -0.4236
2024-12-20 02:40:43.991026: Pseudo dice [0.6886]
2024-12-20 02:40:43.991849: Epoch time: 517.49 s
2024-12-20 02:40:45.850280: 
2024-12-20 02:40:45.851837: Epoch 50
2024-12-20 02:40:45.852667: Current learning rate: 0.00694
2024-12-20 02:49:39.066754: Validation loss did not improve from -0.47738. Patience: 10/50
2024-12-20 02:49:39.067475: train_loss -0.7228
2024-12-20 02:49:39.068277: val_loss -0.4615
2024-12-20 02:49:39.069046: Pseudo dice [0.7076]
2024-12-20 02:49:39.069800: Epoch time: 533.22 s
2024-12-20 02:49:39.070533: Yayy! New best EMA pseudo Dice: 0.6927
2024-12-20 02:49:40.942828: 
2024-12-20 02:49:40.943739: Epoch 51
2024-12-20 02:49:40.944459: Current learning rate: 0.00688
2024-12-20 02:58:23.349351: Validation loss did not improve from -0.47738. Patience: 11/50
2024-12-20 02:58:23.350503: train_loss -0.7146
2024-12-20 02:58:23.351392: val_loss -0.4386
2024-12-20 02:58:23.352187: Pseudo dice [0.7041]
2024-12-20 02:58:23.352947: Epoch time: 522.41 s
2024-12-20 02:58:23.353694: Yayy! New best EMA pseudo Dice: 0.6938
2024-12-20 02:58:26.151662: 
2024-12-20 02:58:26.153041: Epoch 52
2024-12-20 02:58:26.154115: Current learning rate: 0.00682
2024-12-20 03:07:16.528946: Validation loss did not improve from -0.47738. Patience: 12/50
2024-12-20 03:07:16.530187: train_loss -0.7081
2024-12-20 03:07:16.531013: val_loss -0.3974
2024-12-20 03:07:16.531865: Pseudo dice [0.6877]
2024-12-20 03:07:16.532853: Epoch time: 530.38 s
2024-12-20 03:07:17.929905: 
2024-12-20 03:07:17.931448: Epoch 53
2024-12-20 03:07:17.932530: Current learning rate: 0.00675
2024-12-20 03:16:15.425899: Validation loss did not improve from -0.47738. Patience: 13/50
2024-12-20 03:16:15.432384: train_loss -0.7228
2024-12-20 03:16:15.433723: val_loss -0.4174
2024-12-20 03:16:15.434502: Pseudo dice [0.6907]
2024-12-20 03:16:15.435282: Epoch time: 537.5 s
2024-12-20 03:16:16.940627: 
2024-12-20 03:16:16.941975: Epoch 54
2024-12-20 03:16:16.942786: Current learning rate: 0.00669
2024-12-20 03:24:59.024893: Validation loss did not improve from -0.47738. Patience: 14/50
2024-12-20 03:24:59.026128: train_loss -0.722
2024-12-20 03:24:59.028040: val_loss -0.4312
2024-12-20 03:24:59.028881: Pseudo dice [0.7009]
2024-12-20 03:24:59.029854: Epoch time: 522.09 s
2024-12-20 03:25:00.835159: 
2024-12-20 03:25:00.836571: Epoch 55
2024-12-20 03:25:00.837381: Current learning rate: 0.00663
2024-12-20 03:33:49.716825: Validation loss did not improve from -0.47738. Patience: 15/50
2024-12-20 03:33:49.717878: train_loss -0.7269
2024-12-20 03:33:49.718764: val_loss -0.4187
2024-12-20 03:33:49.719627: Pseudo dice [0.6961]
2024-12-20 03:33:49.720447: Epoch time: 528.88 s
2024-12-20 03:33:49.721230: Yayy! New best EMA pseudo Dice: 0.694
2024-12-20 03:33:51.568676: 
2024-12-20 03:33:51.570211: Epoch 56
2024-12-20 03:33:51.571129: Current learning rate: 0.00657
2024-12-20 03:42:41.627601: Validation loss did not improve from -0.47738. Patience: 16/50
2024-12-20 03:42:41.628642: train_loss -0.7174
2024-12-20 03:42:41.629435: val_loss -0.3714
2024-12-20 03:42:41.630124: Pseudo dice [0.6696]
2024-12-20 03:42:41.630841: Epoch time: 530.06 s
2024-12-20 03:42:43.127150: 
2024-12-20 03:42:43.128384: Epoch 57
2024-12-20 03:42:43.129297: Current learning rate: 0.0065
2024-12-20 03:51:23.731647: Validation loss did not improve from -0.47738. Patience: 17/50
2024-12-20 03:51:23.732651: train_loss -0.7271
2024-12-20 03:51:23.733439: val_loss -0.3678
2024-12-20 03:51:23.734133: Pseudo dice [0.6768]
2024-12-20 03:51:23.734942: Epoch time: 520.61 s
2024-12-20 03:51:25.149114: 
2024-12-20 03:51:25.150770: Epoch 58
2024-12-20 03:51:25.151611: Current learning rate: 0.00644
2024-12-20 03:59:54.049938: Validation loss did not improve from -0.47738. Patience: 18/50
2024-12-20 03:59:54.051072: train_loss -0.7287
2024-12-20 03:59:54.052176: val_loss -0.4361
2024-12-20 03:59:54.053066: Pseudo dice [0.7056]
2024-12-20 03:59:54.053964: Epoch time: 508.9 s
2024-12-20 03:59:55.569844: 
2024-12-20 03:59:55.571303: Epoch 59
2024-12-20 03:59:55.572242: Current learning rate: 0.00638
2024-12-20 04:08:31.043557: Validation loss did not improve from -0.47738. Patience: 19/50
2024-12-20 04:08:31.044706: train_loss -0.734
2024-12-20 04:08:31.045737: val_loss -0.4622
2024-12-20 04:08:31.046444: Pseudo dice [0.7195]
2024-12-20 04:08:31.047200: Epoch time: 515.48 s
2024-12-20 04:08:31.532019: Yayy! New best EMA pseudo Dice: 0.6944
2024-12-20 04:08:33.499853: 
2024-12-20 04:08:33.501201: Epoch 60
2024-12-20 04:08:33.502034: Current learning rate: 0.00631
2024-12-20 04:17:07.848949: Validation loss did not improve from -0.47738. Patience: 20/50
2024-12-20 04:17:07.849984: train_loss -0.7322
2024-12-20 04:17:07.850790: val_loss -0.4304
2024-12-20 04:17:07.851435: Pseudo dice [0.7069]
2024-12-20 04:17:07.852146: Epoch time: 514.35 s
2024-12-20 04:17:07.852880: Yayy! New best EMA pseudo Dice: 0.6957
2024-12-20 04:17:09.893119: 
2024-12-20 04:17:09.894527: Epoch 61
2024-12-20 04:17:09.895447: Current learning rate: 0.00625
2024-12-20 04:25:42.715093: Validation loss did not improve from -0.47738. Patience: 21/50
2024-12-20 04:25:42.716700: train_loss -0.7359
2024-12-20 04:25:42.717754: val_loss -0.4344
2024-12-20 04:25:42.718710: Pseudo dice [0.7106]
2024-12-20 04:25:42.719874: Epoch time: 512.83 s
2024-12-20 04:25:42.720825: Yayy! New best EMA pseudo Dice: 0.6972
2024-12-20 04:25:45.331592: 
2024-12-20 04:25:45.333006: Epoch 62
2024-12-20 04:25:45.333841: Current learning rate: 0.00619
2024-12-20 04:35:18.062224: Validation loss did not improve from -0.47738. Patience: 22/50
2024-12-20 04:35:18.063362: train_loss -0.7465
2024-12-20 04:35:18.064072: val_loss -0.4378
2024-12-20 04:35:18.064697: Pseudo dice [0.7052]
2024-12-20 04:35:18.065329: Epoch time: 572.73 s
2024-12-20 04:35:18.066007: Yayy! New best EMA pseudo Dice: 0.698
2024-12-20 04:35:20.140469: 
2024-12-20 04:35:20.141994: Epoch 63
2024-12-20 04:35:20.142821: Current learning rate: 0.00612
2024-12-20 04:44:16.255155: Validation loss did not improve from -0.47738. Patience: 23/50
2024-12-20 04:44:16.256175: train_loss -0.7419
2024-12-20 04:44:16.257070: val_loss -0.4109
2024-12-20 04:44:16.257823: Pseudo dice [0.6959]
2024-12-20 04:44:16.258674: Epoch time: 536.12 s
2024-12-20 04:44:17.692746: 
2024-12-20 04:44:17.694205: Epoch 64
2024-12-20 04:44:17.695130: Current learning rate: 0.00606
2024-12-20 04:52:41.746789: Validation loss did not improve from -0.47738. Patience: 24/50
2024-12-20 04:52:41.747776: train_loss -0.746
2024-12-20 04:52:41.748677: val_loss -0.4683
2024-12-20 04:52:41.749482: Pseudo dice [0.7209]
2024-12-20 04:52:41.750335: Epoch time: 504.06 s
2024-12-20 04:52:42.260788: Yayy! New best EMA pseudo Dice: 0.7001
2024-12-20 04:52:44.204441: 
2024-12-20 04:52:44.205799: Epoch 65
2024-12-20 04:52:44.206585: Current learning rate: 0.006
2024-12-20 05:01:34.259320: Validation loss did not improve from -0.47738. Patience: 25/50
2024-12-20 05:01:34.260330: train_loss -0.7488
2024-12-20 05:01:34.261049: val_loss -0.3985
2024-12-20 05:01:34.261917: Pseudo dice [0.6846]
2024-12-20 05:01:34.262783: Epoch time: 530.06 s
2024-12-20 05:01:35.730995: 
2024-12-20 05:01:35.733732: Epoch 66
2024-12-20 05:01:35.735097: Current learning rate: 0.00593
2024-12-20 05:10:00.836661: Validation loss did not improve from -0.47738. Patience: 26/50
2024-12-20 05:10:00.837569: train_loss -0.7453
2024-12-20 05:10:00.839297: val_loss -0.3619
2024-12-20 05:10:00.840356: Pseudo dice [0.6668]
2024-12-20 05:10:00.841248: Epoch time: 505.11 s
2024-12-20 05:10:02.237676: 
2024-12-20 05:10:02.239033: Epoch 67
2024-12-20 05:10:02.239817: Current learning rate: 0.00587
2024-12-20 05:18:43.071627: Validation loss did not improve from -0.47738. Patience: 27/50
2024-12-20 05:18:43.072582: train_loss -0.7419
2024-12-20 05:18:43.073425: val_loss -0.4281
2024-12-20 05:18:43.074157: Pseudo dice [0.6942]
2024-12-20 05:18:43.074795: Epoch time: 520.84 s
2024-12-20 05:18:44.511269: 
2024-12-20 05:18:44.512691: Epoch 68
2024-12-20 05:18:44.513484: Current learning rate: 0.00581
2024-12-20 05:27:30.035645: Validation loss did not improve from -0.47738. Patience: 28/50
2024-12-20 05:27:30.039630: train_loss -0.7501
2024-12-20 05:27:30.041045: val_loss -0.4299
2024-12-20 05:27:30.041744: Pseudo dice [0.7058]
2024-12-20 05:27:30.042565: Epoch time: 525.53 s
2024-12-20 05:27:31.657449: 
2024-12-20 05:27:31.659778: Epoch 69
2024-12-20 05:27:31.661208: Current learning rate: 0.00574
2024-12-20 05:35:58.484007: Validation loss did not improve from -0.47738. Patience: 29/50
2024-12-20 05:35:58.486129: train_loss -0.748
2024-12-20 05:35:58.487117: val_loss -0.4327
2024-12-20 05:35:58.488038: Pseudo dice [0.7069]
2024-12-20 05:35:58.488853: Epoch time: 506.83 s
2024-12-20 05:36:00.442381: 
2024-12-20 05:36:00.443790: Epoch 70
2024-12-20 05:36:00.444725: Current learning rate: 0.00568
2024-12-20 05:44:36.434495: Validation loss did not improve from -0.47738. Patience: 30/50
2024-12-20 05:44:36.435674: train_loss -0.7527
2024-12-20 05:44:36.436493: val_loss -0.4422
2024-12-20 05:44:36.437264: Pseudo dice [0.708]
2024-12-20 05:44:36.438021: Epoch time: 515.99 s
2024-12-20 05:44:37.920532: 
2024-12-20 05:44:37.922892: Epoch 71
2024-12-20 05:44:37.923734: Current learning rate: 0.00562
2024-12-20 05:53:31.535924: Validation loss did not improve from -0.47738. Patience: 31/50
2024-12-20 05:53:31.536951: train_loss -0.7509
2024-12-20 05:53:31.537938: val_loss -0.4579
2024-12-20 05:53:31.538711: Pseudo dice [0.7177]
2024-12-20 05:53:31.539507: Epoch time: 533.62 s
2024-12-20 05:53:31.540441: Yayy! New best EMA pseudo Dice: 0.7003
2024-12-20 05:53:33.383618: 
2024-12-20 05:53:33.385142: Epoch 72
2024-12-20 05:53:33.386065: Current learning rate: 0.00555
2024-12-20 06:01:45.192842: Validation loss did not improve from -0.47738. Patience: 32/50
2024-12-20 06:01:45.193698: train_loss -0.7546
2024-12-20 06:01:45.195024: val_loss -0.42
2024-12-20 06:01:45.196008: Pseudo dice [0.6999]
2024-12-20 06:01:45.196887: Epoch time: 491.81 s
2024-12-20 06:01:47.054725: 
2024-12-20 06:01:47.056147: Epoch 73
2024-12-20 06:01:47.057072: Current learning rate: 0.00549
2024-12-20 06:11:01.709075: Validation loss did not improve from -0.47738. Patience: 33/50
2024-12-20 06:11:01.710138: train_loss -0.7532
2024-12-20 06:11:01.711019: val_loss -0.4033
2024-12-20 06:11:01.711737: Pseudo dice [0.6883]
2024-12-20 06:11:01.712431: Epoch time: 554.66 s
2024-12-20 06:11:03.138354: 
2024-12-20 06:11:03.139785: Epoch 74
2024-12-20 06:11:03.140530: Current learning rate: 0.00542
2024-12-20 06:19:27.635956: Validation loss did not improve from -0.47738. Patience: 34/50
2024-12-20 06:19:27.636970: train_loss -0.7632
2024-12-20 06:19:27.638994: val_loss -0.4638
2024-12-20 06:19:27.639976: Pseudo dice [0.7198]
2024-12-20 06:19:27.640908: Epoch time: 504.5 s
2024-12-20 06:19:28.094945: Yayy! New best EMA pseudo Dice: 0.7012
2024-12-20 06:19:29.953126: 
2024-12-20 06:19:29.954448: Epoch 75
2024-12-20 06:19:29.955292: Current learning rate: 0.00536
2024-12-20 06:28:10.410546: Validation loss did not improve from -0.47738. Patience: 35/50
2024-12-20 06:28:10.411725: train_loss -0.759
2024-12-20 06:28:10.412460: val_loss -0.4449
2024-12-20 06:28:10.413155: Pseudo dice [0.7007]
2024-12-20 06:28:10.413857: Epoch time: 520.46 s
2024-12-20 06:28:11.891104: 
2024-12-20 06:28:11.892502: Epoch 76
2024-12-20 06:28:11.893383: Current learning rate: 0.00529
2024-12-20 06:36:57.174500: Validation loss did not improve from -0.47738. Patience: 36/50
2024-12-20 06:36:57.176301: train_loss -0.7645
2024-12-20 06:36:57.177147: val_loss -0.4096
2024-12-20 06:36:57.177925: Pseudo dice [0.6877]
2024-12-20 06:36:57.178745: Epoch time: 525.29 s
2024-12-20 06:36:58.574819: 
2024-12-20 06:36:58.576178: Epoch 77
2024-12-20 06:36:58.577133: Current learning rate: 0.00523
2024-12-20 06:45:23.994067: Validation loss did not improve from -0.47738. Patience: 37/50
2024-12-20 06:45:23.995682: train_loss -0.7654
2024-12-20 06:45:23.996745: val_loss -0.413
2024-12-20 06:45:23.997578: Pseudo dice [0.6973]
2024-12-20 06:45:23.998405: Epoch time: 505.42 s
2024-12-20 06:45:25.412851: 
2024-12-20 06:45:25.414173: Epoch 78
2024-12-20 06:45:25.414966: Current learning rate: 0.00517
2024-12-20 06:54:23.977526: Validation loss did not improve from -0.47738. Patience: 38/50
2024-12-20 06:54:23.978468: train_loss -0.7638
2024-12-20 06:54:23.979246: val_loss -0.35
2024-12-20 06:54:23.979871: Pseudo dice [0.6627]
2024-12-20 06:54:23.980529: Epoch time: 538.57 s
2024-12-20 06:54:25.394526: 
2024-12-20 06:54:25.395698: Epoch 79
2024-12-20 06:54:25.396409: Current learning rate: 0.0051
2024-12-20 07:03:00.673312: Validation loss did not improve from -0.47738. Patience: 39/50
2024-12-20 07:03:00.674256: train_loss -0.7672
2024-12-20 07:03:00.675180: val_loss -0.4376
2024-12-20 07:03:00.675975: Pseudo dice [0.7146]
2024-12-20 07:03:00.676794: Epoch time: 515.28 s
2024-12-20 07:03:02.552185: 
2024-12-20 07:03:02.553610: Epoch 80
2024-12-20 07:03:02.554439: Current learning rate: 0.00504
2024-12-20 07:11:54.694678: Validation loss did not improve from -0.47738. Patience: 40/50
2024-12-20 07:11:54.695725: train_loss -0.7665
2024-12-20 07:11:54.696641: val_loss -0.433
2024-12-20 07:11:54.697394: Pseudo dice [0.7155]
2024-12-20 07:11:54.698157: Epoch time: 532.14 s
2024-12-20 07:11:56.167259: 
2024-12-20 07:11:56.168601: Epoch 81
2024-12-20 07:11:56.169474: Current learning rate: 0.00497
2024-12-20 07:20:41.499319: Validation loss did not improve from -0.47738. Patience: 41/50
2024-12-20 07:20:41.500774: train_loss -0.7724
2024-12-20 07:20:41.502379: val_loss -0.4302
2024-12-20 07:20:41.503204: Pseudo dice [0.7106]
2024-12-20 07:20:41.504288: Epoch time: 525.33 s
2024-12-20 07:20:43.100279: 
2024-12-20 07:20:43.102135: Epoch 82
2024-12-20 07:20:43.103065: Current learning rate: 0.00491
2024-12-20 07:29:08.031516: Validation loss did not improve from -0.47738. Patience: 42/50
2024-12-20 07:29:08.032556: train_loss -0.7728
2024-12-20 07:29:08.033532: val_loss -0.436
2024-12-20 07:29:08.034321: Pseudo dice [0.6967]
2024-12-20 07:29:08.035181: Epoch time: 504.93 s
2024-12-20 07:29:09.476375: 
2024-12-20 07:29:09.477821: Epoch 83
2024-12-20 07:29:09.478638: Current learning rate: 0.00484
2024-12-20 07:38:00.607823: Validation loss did not improve from -0.47738. Patience: 43/50
2024-12-20 07:38:00.610878: train_loss -0.7728
2024-12-20 07:38:00.612142: val_loss -0.4124
2024-12-20 07:38:00.613042: Pseudo dice [0.702]
2024-12-20 07:38:00.614103: Epoch time: 531.14 s
2024-12-20 07:38:02.746490: 
2024-12-20 07:38:02.748167: Epoch 84
2024-12-20 07:38:02.749318: Current learning rate: 0.00478
2024-12-20 07:46:18.132360: Validation loss did not improve from -0.47738. Patience: 44/50
2024-12-20 07:46:18.134373: train_loss -0.7742
2024-12-20 07:46:18.135411: val_loss -0.4088
2024-12-20 07:46:18.136201: Pseudo dice [0.6999]
2024-12-20 07:46:18.136985: Epoch time: 495.39 s
2024-12-20 07:46:20.080527: 
2024-12-20 07:46:20.081883: Epoch 85
2024-12-20 07:46:20.082777: Current learning rate: 0.00471
2024-12-20 07:55:15.322626: Validation loss did not improve from -0.47738. Patience: 45/50
2024-12-20 07:55:15.323656: train_loss -0.7776
2024-12-20 07:55:15.324637: val_loss -0.4065
2024-12-20 07:55:15.325650: Pseudo dice [0.6963]
2024-12-20 07:55:15.326667: Epoch time: 535.24 s
2024-12-20 07:55:16.768296: 
2024-12-20 07:55:16.769634: Epoch 86
2024-12-20 07:55:16.770403: Current learning rate: 0.00465
2024-12-20 08:03:50.023815: Validation loss did not improve from -0.47738. Patience: 46/50
2024-12-20 08:03:50.024889: train_loss -0.7781
2024-12-20 08:03:50.025965: val_loss -0.4349
2024-12-20 08:03:50.026660: Pseudo dice [0.7061]
2024-12-20 08:03:50.027468: Epoch time: 513.26 s
2024-12-20 08:03:51.530470: 
2024-12-20 08:03:51.531838: Epoch 87
2024-12-20 08:03:51.532657: Current learning rate: 0.00458
2024-12-20 08:12:35.298545: Validation loss did not improve from -0.47738. Patience: 47/50
2024-12-20 08:12:35.299649: train_loss -0.7743
2024-12-20 08:12:35.300595: val_loss -0.3692
2024-12-20 08:12:35.301341: Pseudo dice [0.6729]
2024-12-20 08:12:35.302075: Epoch time: 523.77 s
2024-12-20 08:12:36.751848: 
2024-12-20 08:12:36.753487: Epoch 88
2024-12-20 08:12:36.754580: Current learning rate: 0.00452
2024-12-20 08:21:26.875476: Validation loss did not improve from -0.47738. Patience: 48/50
2024-12-20 08:21:26.877474: train_loss -0.7748
2024-12-20 08:21:26.878778: val_loss -0.4284
2024-12-20 08:21:26.879682: Pseudo dice [0.7062]
2024-12-20 08:21:26.880693: Epoch time: 530.13 s
2024-12-20 08:21:28.270817: 
2024-12-20 08:21:28.272408: Epoch 89
2024-12-20 08:21:28.273525: Current learning rate: 0.00445
2024-12-20 08:29:46.895132: Validation loss did not improve from -0.47738. Patience: 49/50
2024-12-20 08:29:46.896263: train_loss -0.7748
2024-12-20 08:29:46.897965: val_loss -0.4205
2024-12-20 08:29:46.898996: Pseudo dice [0.7067]
2024-12-20 08:29:46.900150: Epoch time: 498.63 s
2024-12-20 08:29:48.773227: 
2024-12-20 08:29:48.774883: Epoch 90
2024-12-20 08:29:48.775925: Current learning rate: 0.00438
2024-12-20 08:38:47.351194: Validation loss did not improve from -0.47738. Patience: 50/50
2024-12-20 08:38:47.352306: train_loss -0.7749
2024-12-20 08:38:47.353142: val_loss -0.4003
2024-12-20 08:38:47.353885: Pseudo dice [0.6894]
2024-12-20 08:38:47.354625: Epoch time: 538.58 s
2024-12-20 08:38:48.714415: 
2024-12-20 08:38:48.715635: Epoch 91
2024-12-20 08:38:48.716439: Current learning rate: 0.00432
2024-12-20 08:47:24.814683: Validation loss did not improve from -0.47738. Patience: 51/50
2024-12-20 08:47:24.815979: train_loss -0.7771
2024-12-20 08:47:24.817147: val_loss -0.4227
2024-12-20 08:47:24.818068: Pseudo dice [0.6963]
2024-12-20 08:47:24.819313: Epoch time: 516.1 s
2024-12-20 08:47:26.325532: 
2024-12-20 08:47:26.326761: Epoch 92
2024-12-20 08:47:26.327588: Current learning rate: 0.00425
2024-12-20 08:56:11.236639: Validation loss did not improve from -0.47738. Patience: 52/50
2024-12-20 08:56:11.237680: train_loss -0.7796
2024-12-20 08:56:11.238596: val_loss -0.4387
2024-12-20 08:56:11.239491: Pseudo dice [0.7176]
2024-12-20 08:56:11.240448: Epoch time: 524.91 s
2024-12-20 08:56:12.706064: 
2024-12-20 08:56:12.707829: Epoch 93
2024-12-20 08:56:12.709014: Current learning rate: 0.00419
2024-12-20 09:05:24.745430: Validation loss did not improve from -0.47738. Patience: 53/50
2024-12-20 09:05:24.746565: train_loss -0.7875
2024-12-20 09:05:24.747485: val_loss -0.4397
2024-12-20 09:05:24.748130: Pseudo dice [0.7243]
2024-12-20 09:05:24.748826: Epoch time: 552.04 s
2024-12-20 09:05:24.749549: Yayy! New best EMA pseudo Dice: 0.7026
2024-12-20 09:05:26.627077: 
2024-12-20 09:05:26.628590: Epoch 94
2024-12-20 09:05:26.629787: Current learning rate: 0.00412
2024-12-20 09:14:03.908987: Validation loss did not improve from -0.47738. Patience: 54/50
2024-12-20 09:14:03.910207: train_loss -0.7845
2024-12-20 09:14:03.911106: val_loss -0.4081
2024-12-20 09:14:03.911944: Pseudo dice [0.6983]
2024-12-20 09:14:03.912799: Epoch time: 517.28 s
2024-12-20 09:14:06.303943: 
2024-12-20 09:14:06.305293: Epoch 95
2024-12-20 09:14:06.306058: Current learning rate: 0.00405
2024-12-20 09:22:47.950810: Validation loss did not improve from -0.47738. Patience: 55/50
2024-12-20 09:22:47.951880: train_loss -0.7844
2024-12-20 09:22:47.952699: val_loss -0.4213
2024-12-20 09:22:47.953354: Pseudo dice [0.7042]
2024-12-20 09:22:47.954034: Epoch time: 521.65 s
2024-12-20 09:22:49.357188: 
2024-12-20 09:22:49.358696: Epoch 96
2024-12-20 09:22:49.359468: Current learning rate: 0.00399
2024-12-20 09:31:50.044268: Validation loss did not improve from -0.47738. Patience: 56/50
2024-12-20 09:31:50.045885: train_loss -0.7866
2024-12-20 09:31:50.048142: val_loss -0.3872
2024-12-20 09:31:50.049026: Pseudo dice [0.6912]
2024-12-20 09:31:50.050132: Epoch time: 540.69 s
2024-12-20 09:31:51.458854: 
2024-12-20 09:31:51.460448: Epoch 97
2024-12-20 09:31:51.461473: Current learning rate: 0.00392
2024-12-20 09:40:31.157162: Validation loss did not improve from -0.47738. Patience: 57/50
2024-12-20 09:40:31.158849: train_loss -0.7903
2024-12-20 09:40:31.160088: val_loss -0.3724
2024-12-20 09:40:31.160957: Pseudo dice [0.6808]
2024-12-20 09:40:31.161807: Epoch time: 519.7 s
2024-12-20 09:40:32.599405: 
2024-12-20 09:40:32.600827: Epoch 98
2024-12-20 09:40:32.601790: Current learning rate: 0.00385
2024-12-20 09:49:41.840546: Validation loss did not improve from -0.47738. Patience: 58/50
2024-12-20 09:49:41.841950: train_loss -0.7874
2024-12-20 09:49:41.842772: val_loss -0.3908
2024-12-20 09:49:41.843545: Pseudo dice [0.6846]
2024-12-20 09:49:41.844593: Epoch time: 549.24 s
2024-12-20 09:49:43.280262: 
2024-12-20 09:49:43.282682: Epoch 99
2024-12-20 09:49:43.284378: Current learning rate: 0.00379
2024-12-20 09:58:23.971436: Validation loss did not improve from -0.47738. Patience: 59/50
2024-12-20 09:58:23.972908: train_loss -0.7887
2024-12-20 09:58:23.974162: val_loss -0.4226
2024-12-20 09:58:23.975080: Pseudo dice [0.7163]
2024-12-20 09:58:23.976240: Epoch time: 520.7 s
2024-12-20 09:58:25.792167: 
2024-12-20 09:58:25.793628: Epoch 100
2024-12-20 09:58:25.794580: Current learning rate: 0.00372
2024-12-20 10:07:26.175112: Validation loss did not improve from -0.47738. Patience: 60/50
2024-12-20 10:07:26.175853: train_loss -0.7882
2024-12-20 10:07:26.176602: val_loss -0.437
2024-12-20 10:07:26.177363: Pseudo dice [0.7118]
2024-12-20 10:07:26.178226: Epoch time: 540.38 s
2024-12-20 10:07:27.598640: 
2024-12-20 10:07:27.600013: Epoch 101
2024-12-20 10:07:27.600926: Current learning rate: 0.00365
2024-12-20 10:16:34.246903: Validation loss did not improve from -0.47738. Patience: 61/50
2024-12-20 10:16:34.247807: train_loss -0.7863
2024-12-20 10:16:34.248605: val_loss -0.4263
2024-12-20 10:16:34.249362: Pseudo dice [0.7159]
2024-12-20 10:16:34.250206: Epoch time: 546.65 s
2024-12-20 10:16:35.659151: 
2024-12-20 10:16:35.660321: Epoch 102
2024-12-20 10:16:35.661036: Current learning rate: 0.00359
2024-12-20 10:25:57.579191: Validation loss did not improve from -0.47738. Patience: 62/50
2024-12-20 10:25:57.580236: train_loss -0.7932
2024-12-20 10:25:57.580996: val_loss -0.4195
2024-12-20 10:25:57.581662: Pseudo dice [0.7038]
2024-12-20 10:25:57.582307: Epoch time: 561.92 s
2024-12-20 10:25:58.942685: 
2024-12-20 10:25:58.944033: Epoch 103
2024-12-20 10:25:58.944738: Current learning rate: 0.00352
2024-12-20 10:34:29.505163: Validation loss did not improve from -0.47738. Patience: 63/50
2024-12-20 10:34:29.506547: train_loss -0.7947
2024-12-20 10:34:29.507474: val_loss -0.413
2024-12-20 10:34:29.508128: Pseudo dice [0.6982]
2024-12-20 10:34:29.508774: Epoch time: 510.56 s
2024-12-20 10:34:30.904469: 
2024-12-20 10:34:30.905366: Epoch 104
2024-12-20 10:34:30.906178: Current learning rate: 0.00345
2024-12-20 10:43:43.224679: Validation loss did not improve from -0.47738. Patience: 64/50
2024-12-20 10:43:43.225500: train_loss -0.7931
2024-12-20 10:43:43.226323: val_loss -0.4238
2024-12-20 10:43:43.227024: Pseudo dice [0.7131]
2024-12-20 10:43:43.227689: Epoch time: 552.32 s
2024-12-20 10:43:43.679857: Yayy! New best EMA pseudo Dice: 0.7031
2024-12-20 10:43:45.528116: 
2024-12-20 10:43:45.529555: Epoch 105
2024-12-20 10:43:45.530335: Current learning rate: 0.00338
2024-12-20 10:53:17.773791: Validation loss did not improve from -0.47738. Patience: 65/50
2024-12-20 10:53:17.775478: train_loss -0.7926
2024-12-20 10:53:17.776440: val_loss -0.4266
2024-12-20 10:53:17.777267: Pseudo dice [0.7056]
2024-12-20 10:53:17.778374: Epoch time: 572.25 s
2024-12-20 10:53:17.779246: Yayy! New best EMA pseudo Dice: 0.7034
2024-12-20 10:53:20.618435: 
2024-12-20 10:53:20.619907: Epoch 106
2024-12-20 10:53:20.620707: Current learning rate: 0.00332
2024-12-20 11:02:30.524113: Validation loss did not improve from -0.47738. Patience: 66/50
2024-12-20 11:02:30.525394: train_loss -0.7968
2024-12-20 11:02:30.526210: val_loss -0.369
2024-12-20 11:02:30.527021: Pseudo dice [0.6872]
2024-12-20 11:02:30.527711: Epoch time: 549.91 s
2024-12-20 11:02:31.925470: 
2024-12-20 11:02:31.926870: Epoch 107
2024-12-20 11:02:31.927603: Current learning rate: 0.00325
2024-12-20 11:11:47.177753: Validation loss did not improve from -0.47738. Patience: 67/50
2024-12-20 11:11:47.178727: train_loss -0.7961
2024-12-20 11:11:47.179644: val_loss -0.4212
2024-12-20 11:11:47.180472: Pseudo dice [0.7117]
2024-12-20 11:11:47.181282: Epoch time: 555.25 s
2024-12-20 11:11:48.573469: 
2024-12-20 11:11:48.574798: Epoch 108
2024-12-20 11:11:48.575640: Current learning rate: 0.00318
2024-12-20 11:20:12.066924: Validation loss did not improve from -0.47738. Patience: 68/50
2024-12-20 11:20:12.067874: train_loss -0.7987
2024-12-20 11:20:12.068724: val_loss -0.4113
2024-12-20 11:20:12.069410: Pseudo dice [0.7054]
2024-12-20 11:20:12.070171: Epoch time: 503.5 s
2024-12-20 11:20:13.523114: 
2024-12-20 11:20:13.524286: Epoch 109
2024-12-20 11:20:13.525000: Current learning rate: 0.00311
2024-12-20 11:28:56.522583: Validation loss did not improve from -0.47738. Patience: 69/50
2024-12-20 11:28:56.523406: train_loss -0.7982
2024-12-20 11:28:56.524147: val_loss -0.4198
2024-12-20 11:28:56.524983: Pseudo dice [0.6964]
2024-12-20 11:28:56.525800: Epoch time: 523.0 s
2024-12-20 11:28:58.416655: 
2024-12-20 11:28:58.417985: Epoch 110
2024-12-20 11:28:58.418893: Current learning rate: 0.00304
2024-12-20 11:37:49.807305: Validation loss did not improve from -0.47738. Patience: 70/50
2024-12-20 11:37:49.808071: train_loss -0.7996
2024-12-20 11:37:49.808816: val_loss -0.4503
2024-12-20 11:37:49.809489: Pseudo dice [0.7127]
2024-12-20 11:37:49.810241: Epoch time: 531.39 s
2024-12-20 11:37:49.810913: Yayy! New best EMA pseudo Dice: 0.7034
2024-12-20 11:37:51.752488: 
2024-12-20 11:37:51.753818: Epoch 111
2024-12-20 11:37:51.754617: Current learning rate: 0.00297
2024-12-20 11:46:36.160413: Validation loss improved from -0.47738 to -0.47768! Patience: 70/50
2024-12-20 11:46:36.161204: train_loss -0.7977
2024-12-20 11:46:36.161916: val_loss -0.4777
2024-12-20 11:46:36.162575: Pseudo dice [0.7347]
2024-12-20 11:46:36.163249: Epoch time: 524.41 s
2024-12-20 11:46:36.164025: Yayy! New best EMA pseudo Dice: 0.7065
2024-12-20 11:46:38.002167: 
2024-12-20 11:46:38.003033: Epoch 112
2024-12-20 11:46:38.003824: Current learning rate: 0.00291
2024-12-20 11:55:42.278945: Validation loss did not improve from -0.47768. Patience: 1/50
2024-12-20 11:55:42.281285: train_loss -0.7981
2024-12-20 11:55:42.282248: val_loss -0.4423
2024-12-20 11:55:42.283164: Pseudo dice [0.7092]
2024-12-20 11:55:42.284037: Epoch time: 544.28 s
2024-12-20 11:55:42.284925: Yayy! New best EMA pseudo Dice: 0.7068
2024-12-20 11:55:44.113683: 
2024-12-20 11:55:44.115264: Epoch 113
2024-12-20 11:55:44.116366: Current learning rate: 0.00284
2024-12-20 12:04:02.268304: Validation loss did not improve from -0.47768. Patience: 2/50
2024-12-20 12:04:02.269808: train_loss -0.8014
2024-12-20 12:04:02.270641: val_loss -0.4294
2024-12-20 12:04:02.271243: Pseudo dice [0.7109]
2024-12-20 12:04:02.272011: Epoch time: 498.16 s
2024-12-20 12:04:02.272882: Yayy! New best EMA pseudo Dice: 0.7072
2024-12-20 12:04:04.287375: 
2024-12-20 12:04:04.288740: Epoch 114
2024-12-20 12:04:04.289503: Current learning rate: 0.00277
2024-12-20 12:13:03.076636: Validation loss did not improve from -0.47768. Patience: 3/50
2024-12-20 12:13:03.077765: train_loss -0.8011
2024-12-20 12:13:03.078551: val_loss -0.4183
2024-12-20 12:13:03.079267: Pseudo dice [0.7034]
2024-12-20 12:13:03.080027: Epoch time: 538.79 s
2024-12-20 12:13:05.018995: 
2024-12-20 12:13:05.020441: Epoch 115
2024-12-20 12:13:05.021487: Current learning rate: 0.0027
2024-12-20 12:22:12.498849: Validation loss did not improve from -0.47768. Patience: 4/50
2024-12-20 12:22:12.500648: train_loss -0.8037
2024-12-20 12:22:12.501587: val_loss -0.4482
2024-12-20 12:22:12.502417: Pseudo dice [0.7262]
2024-12-20 12:22:12.503110: Epoch time: 547.48 s
2024-12-20 12:22:12.503764: Yayy! New best EMA pseudo Dice: 0.7088
2024-12-20 12:22:17.199462: 
2024-12-20 12:22:17.200892: Epoch 116
2024-12-20 12:22:17.201731: Current learning rate: 0.00263
2024-12-20 12:31:02.573801: Validation loss did not improve from -0.47768. Patience: 5/50
2024-12-20 12:31:02.605243: train_loss -0.8036
2024-12-20 12:31:02.606550: val_loss -0.4316
2024-12-20 12:31:02.607269: Pseudo dice [0.7106]
2024-12-20 12:31:02.607884: Epoch time: 525.41 s
2024-12-20 12:31:02.608706: Yayy! New best EMA pseudo Dice: 0.7089
2024-12-20 12:31:04.563080: 
2024-12-20 12:31:04.564398: Epoch 117
2024-12-20 12:31:04.565125: Current learning rate: 0.00256
2024-12-20 12:40:06.029549: Validation loss did not improve from -0.47768. Patience: 6/50
2024-12-20 12:40:06.030650: train_loss -0.8004
2024-12-20 12:40:06.031394: val_loss -0.4445
2024-12-20 12:40:06.032134: Pseudo dice [0.7066]
2024-12-20 12:40:06.032794: Epoch time: 541.47 s
2024-12-20 12:40:07.434155: 
2024-12-20 12:40:07.435436: Epoch 118
2024-12-20 12:40:07.436316: Current learning rate: 0.00249
2024-12-20 12:48:35.475054: Validation loss did not improve from -0.47768. Patience: 7/50
2024-12-20 12:48:35.476203: train_loss -0.8041
2024-12-20 12:48:35.477943: val_loss -0.3742
2024-12-20 12:48:35.478639: Pseudo dice [0.6732]
2024-12-20 12:48:35.479397: Epoch time: 508.04 s
2024-12-20 12:48:36.895592: 
2024-12-20 12:48:36.897086: Epoch 119
2024-12-20 12:48:36.897931: Current learning rate: 0.00242
2024-12-20 12:58:08.757225: Validation loss did not improve from -0.47768. Patience: 8/50
2024-12-20 12:58:08.758141: train_loss -0.804
2024-12-20 12:58:08.758933: val_loss -0.4124
2024-12-20 12:58:08.759576: Pseudo dice [0.7018]
2024-12-20 12:58:08.760189: Epoch time: 571.86 s
2024-12-20 12:58:10.628567: 
2024-12-20 12:58:10.629947: Epoch 120
2024-12-20 12:58:10.630752: Current learning rate: 0.00235
2024-12-20 13:08:06.040332: Validation loss did not improve from -0.47768. Patience: 9/50
2024-12-20 13:08:06.041324: train_loss -0.8015
2024-12-20 13:08:06.042050: val_loss -0.4012
2024-12-20 13:08:06.042686: Pseudo dice [0.7032]
2024-12-20 13:08:06.043449: Epoch time: 595.41 s
2024-12-20 13:08:07.519394: 
2024-12-20 13:08:07.520679: Epoch 121
2024-12-20 13:08:07.521335: Current learning rate: 0.00228
2024-12-20 13:16:38.147794: Validation loss did not improve from -0.47768. Patience: 10/50
2024-12-20 13:16:38.148858: train_loss -0.8071
2024-12-20 13:16:38.149740: val_loss -0.3967
2024-12-20 13:16:38.150527: Pseudo dice [0.6887]
2024-12-20 13:16:38.151301: Epoch time: 510.63 s
2024-12-20 13:16:39.576869: 
2024-12-20 13:16:39.578199: Epoch 122
2024-12-20 13:16:39.579121: Current learning rate: 0.00221
2024-12-20 13:25:46.944028: Validation loss did not improve from -0.47768. Patience: 11/50
2024-12-20 13:25:46.945185: train_loss -0.8054
2024-12-20 13:25:46.946169: val_loss -0.4582
2024-12-20 13:25:46.946934: Pseudo dice [0.7255]
2024-12-20 13:25:46.947729: Epoch time: 547.37 s
2024-12-20 13:25:48.386206: 
2024-12-20 13:25:48.387617: Epoch 123
2024-12-20 13:25:48.388617: Current learning rate: 0.00214
2024-12-20 13:34:11.217839: Validation loss did not improve from -0.47768. Patience: 12/50
2024-12-20 13:34:11.220015: train_loss -0.8091
2024-12-20 13:34:11.221305: val_loss -0.4274
2024-12-20 13:34:11.222283: Pseudo dice [0.7142]
2024-12-20 13:34:11.223224: Epoch time: 502.83 s
2024-12-20 13:34:12.684581: 
2024-12-20 13:34:12.685797: Epoch 124
2024-12-20 13:34:12.686648: Current learning rate: 0.00207
2024-12-20 13:44:10.708349: Validation loss did not improve from -0.47768. Patience: 13/50
2024-12-20 13:44:10.709430: train_loss -0.8089
2024-12-20 13:44:10.710202: val_loss -0.4528
2024-12-20 13:44:10.710952: Pseudo dice [0.7246]
2024-12-20 13:44:10.711581: Epoch time: 598.03 s
2024-12-20 13:44:12.791386: 
2024-12-20 13:44:12.792755: Epoch 125
2024-12-20 13:44:12.793594: Current learning rate: 0.00199
2024-12-20 13:52:44.263074: Validation loss did not improve from -0.47768. Patience: 14/50
2024-12-20 13:52:44.263936: train_loss -0.8083
2024-12-20 13:52:44.264893: val_loss -0.424
2024-12-20 13:52:44.265618: Pseudo dice [0.713]
2024-12-20 13:52:44.266353: Epoch time: 511.47 s
2024-12-20 13:52:45.669044: 
2024-12-20 13:52:45.670087: Epoch 126
2024-12-20 13:52:45.670887: Current learning rate: 0.00192
2024-12-20 14:00:59.239349: Validation loss did not improve from -0.47768. Patience: 15/50
2024-12-20 14:00:59.240408: train_loss -0.8101
2024-12-20 14:00:59.241482: val_loss -0.414
2024-12-20 14:00:59.242301: Pseudo dice [0.7074]
2024-12-20 14:00:59.243156: Epoch time: 493.57 s
2024-12-20 14:01:01.031366: 
2024-12-20 14:01:01.033088: Epoch 127
2024-12-20 14:01:01.034119: Current learning rate: 0.00185
2024-12-20 14:09:39.804729: Validation loss did not improve from -0.47768. Patience: 16/50
2024-12-20 14:09:39.805800: train_loss -0.8105
2024-12-20 14:09:39.806721: val_loss -0.4414
2024-12-20 14:09:39.807544: Pseudo dice [0.7174]
2024-12-20 14:09:39.808361: Epoch time: 518.78 s
2024-12-20 14:09:39.809090: Yayy! New best EMA pseudo Dice: 0.7093
2024-12-20 14:09:41.678938: 
2024-12-20 14:09:41.680710: Epoch 128
2024-12-20 14:09:41.681456: Current learning rate: 0.00178
2024-12-20 14:17:28.020806: Validation loss did not improve from -0.47768. Patience: 17/50
2024-12-20 14:17:28.023378: train_loss -0.8137
2024-12-20 14:17:28.024738: val_loss -0.4088
2024-12-20 14:17:28.025424: Pseudo dice [0.7006]
2024-12-20 14:17:28.026148: Epoch time: 466.35 s
2024-12-20 14:17:29.573665: 
2024-12-20 14:17:29.575132: Epoch 129
2024-12-20 14:17:29.575934: Current learning rate: 0.0017
2024-12-20 14:25:49.999130: Validation loss did not improve from -0.47768. Patience: 18/50
2024-12-20 14:25:50.000096: train_loss -0.8113
2024-12-20 14:25:50.001013: val_loss -0.4232
2024-12-20 14:25:50.001964: Pseudo dice [0.707]
2024-12-20 14:25:50.002751: Epoch time: 500.43 s
2024-12-20 14:25:52.023529: 
2024-12-20 14:25:52.025030: Epoch 130
2024-12-20 14:25:52.026133: Current learning rate: 0.00163
2024-12-20 14:34:20.008082: Validation loss did not improve from -0.47768. Patience: 19/50
2024-12-20 14:34:20.009092: train_loss -0.8122
2024-12-20 14:34:20.009987: val_loss -0.4131
2024-12-20 14:34:20.011094: Pseudo dice [0.6988]
2024-12-20 14:34:20.011980: Epoch time: 507.99 s
2024-12-20 14:34:21.430961: 
2024-12-20 14:34:21.432578: Epoch 131
2024-12-20 14:34:21.433691: Current learning rate: 0.00156
2024-12-20 14:42:53.794740: Validation loss did not improve from -0.47768. Patience: 20/50
2024-12-20 14:42:53.795762: train_loss -0.8121
2024-12-20 14:42:53.796596: val_loss -0.4166
2024-12-20 14:42:53.797232: Pseudo dice [0.7111]
2024-12-20 14:42:53.797962: Epoch time: 512.37 s
2024-12-20 14:42:55.351747: 
2024-12-20 14:42:55.353916: Epoch 132
2024-12-20 14:42:55.354717: Current learning rate: 0.00148
2024-12-20 14:51:01.474326: Validation loss did not improve from -0.47768. Patience: 21/50
2024-12-20 14:51:01.475390: train_loss -0.8098
2024-12-20 14:51:01.476241: val_loss -0.4453
2024-12-20 14:51:01.477018: Pseudo dice [0.7234]
2024-12-20 14:51:01.477720: Epoch time: 486.13 s
2024-12-20 14:51:02.912171: 
2024-12-20 14:51:02.913516: Epoch 133
2024-12-20 14:51:02.914370: Current learning rate: 0.00141
2024-12-20 14:58:56.627280: Validation loss did not improve from -0.47768. Patience: 22/50
2024-12-20 14:58:56.628001: train_loss -0.8138
2024-12-20 14:58:56.628711: val_loss -0.4219
2024-12-20 14:58:56.629322: Pseudo dice [0.7091]
2024-12-20 14:58:56.629947: Epoch time: 473.72 s
2024-12-20 14:58:58.078779: 
2024-12-20 14:58:58.080546: Epoch 134
2024-12-20 14:58:58.081509: Current learning rate: 0.00133
2024-12-20 15:07:16.823092: Validation loss did not improve from -0.47768. Patience: 23/50
2024-12-20 15:07:16.824121: train_loss -0.8108
2024-12-20 15:07:16.824960: val_loss -0.458
2024-12-20 15:07:16.825648: Pseudo dice [0.729]
2024-12-20 15:07:16.826449: Epoch time: 498.75 s
2024-12-20 15:07:17.279985: Yayy! New best EMA pseudo Dice: 0.7112
2024-12-20 15:07:19.018714: 
2024-12-20 15:07:19.019883: Epoch 135
2024-12-20 15:07:19.020644: Current learning rate: 0.00126
2024-12-20 15:15:20.314095: Validation loss did not improve from -0.47768. Patience: 24/50
2024-12-20 15:15:20.315087: train_loss -0.8143
2024-12-20 15:15:20.315864: val_loss -0.4264
2024-12-20 15:15:20.316566: Pseudo dice [0.7119]
2024-12-20 15:15:20.317471: Epoch time: 481.3 s
2024-12-20 15:15:20.318283: Yayy! New best EMA pseudo Dice: 0.7113
2024-12-20 15:15:22.098240: 
2024-12-20 15:15:22.099576: Epoch 136
2024-12-20 15:15:22.100255: Current learning rate: 0.00118
2024-12-20 15:23:49.132406: Validation loss did not improve from -0.47768. Patience: 25/50
2024-12-20 15:23:49.136050: train_loss -0.8154
2024-12-20 15:23:49.138055: val_loss -0.4038
2024-12-20 15:23:49.138981: Pseudo dice [0.6996]
2024-12-20 15:23:49.139924: Epoch time: 507.04 s
2024-12-20 15:23:50.561361: 
2024-12-20 15:23:50.562556: Epoch 137
2024-12-20 15:23:50.563292: Current learning rate: 0.00111
2024-12-20 15:32:04.601061: Validation loss did not improve from -0.47768. Patience: 26/50
2024-12-20 15:32:04.602122: train_loss -0.8121
2024-12-20 15:32:04.603004: val_loss -0.4086
2024-12-20 15:32:04.603783: Pseudo dice [0.7037]
2024-12-20 15:32:04.604915: Epoch time: 494.04 s
2024-12-20 15:32:07.436591: 
2024-12-20 15:32:07.438256: Epoch 138
2024-12-20 15:32:07.439136: Current learning rate: 0.00103
2024-12-20 15:40:23.018643: Validation loss did not improve from -0.47768. Patience: 27/50
2024-12-20 15:40:23.019518: train_loss -0.8161
2024-12-20 15:40:23.020266: val_loss -0.4271
2024-12-20 15:40:23.021041: Pseudo dice [0.7159]
2024-12-20 15:40:23.021880: Epoch time: 495.58 s
2024-12-20 15:40:24.430576: 
2024-12-20 15:40:24.431558: Epoch 139
2024-12-20 15:40:24.432222: Current learning rate: 0.00095
2024-12-20 15:48:09.752216: Validation loss did not improve from -0.47768. Patience: 28/50
2024-12-20 15:48:09.753121: train_loss -0.8149
2024-12-20 15:48:09.754521: val_loss -0.4452
2024-12-20 15:48:09.755314: Pseudo dice [0.7139]
2024-12-20 15:48:09.756052: Epoch time: 465.32 s
2024-12-20 15:48:11.530490: 
2024-12-20 15:48:11.531675: Epoch 140
2024-12-20 15:48:11.532491: Current learning rate: 0.00087
2024-12-20 15:56:08.860135: Validation loss did not improve from -0.47768. Patience: 29/50
2024-12-20 15:56:08.861026: train_loss -0.8165
2024-12-20 15:56:08.861855: val_loss -0.4414
2024-12-20 15:56:08.862669: Pseudo dice [0.7179]
2024-12-20 15:56:08.863418: Epoch time: 477.33 s
2024-12-20 15:56:10.238690: 
2024-12-20 15:56:10.240477: Epoch 141
2024-12-20 15:56:10.241352: Current learning rate: 0.00079
2024-12-20 16:04:13.797841: Validation loss did not improve from -0.47768. Patience: 30/50
2024-12-20 16:04:13.798819: train_loss -0.813
2024-12-20 16:04:13.799589: val_loss -0.4066
2024-12-20 16:04:13.800188: Pseudo dice [0.6975]
2024-12-20 16:04:13.800871: Epoch time: 483.56 s
2024-12-20 16:04:15.210859: 
2024-12-20 16:04:15.212444: Epoch 142
2024-12-20 16:04:15.213341: Current learning rate: 0.00071
2024-12-20 16:10:33.667876: Validation loss did not improve from -0.47768. Patience: 31/50
2024-12-20 16:10:33.668752: train_loss -0.8142
2024-12-20 16:10:33.669493: val_loss -0.4164
2024-12-20 16:10:33.670245: Pseudo dice [0.7032]
2024-12-20 16:10:33.671042: Epoch time: 378.46 s
2024-12-20 16:10:35.049802: 
2024-12-20 16:10:35.051100: Epoch 143
2024-12-20 16:10:35.051848: Current learning rate: 0.00063
2024-12-20 16:17:01.020088: Validation loss did not improve from -0.47768. Patience: 32/50
2024-12-20 16:17:01.021974: train_loss -0.8161
2024-12-20 16:17:01.023612: val_loss -0.4073
2024-12-20 16:17:01.024555: Pseudo dice [0.7068]
2024-12-20 16:17:01.025622: Epoch time: 385.97 s
2024-12-20 16:17:02.440234: 
2024-12-20 16:17:02.441746: Epoch 144
2024-12-20 16:17:02.442864: Current learning rate: 0.00055
2024-12-20 16:23:12.784120: Validation loss did not improve from -0.47768. Patience: 33/50
2024-12-20 16:23:12.784783: train_loss -0.8157
2024-12-20 16:23:12.785531: val_loss -0.4262
2024-12-20 16:23:12.786182: Pseudo dice [0.705]
2024-12-20 16:23:12.786753: Epoch time: 370.35 s
2024-12-20 16:23:14.558368: 
2024-12-20 16:23:14.559646: Epoch 145
2024-12-20 16:23:14.560370: Current learning rate: 0.00047
2024-12-20 16:28:06.197200: Validation loss did not improve from -0.47768. Patience: 34/50
2024-12-20 16:28:06.199051: train_loss -0.8166
2024-12-20 16:28:06.200419: val_loss -0.4512
2024-12-20 16:28:06.201312: Pseudo dice [0.7295]
2024-12-20 16:28:06.202454: Epoch time: 291.64 s
2024-12-20 16:28:07.604633: 
2024-12-20 16:28:07.605992: Epoch 146
2024-12-20 16:28:07.606824: Current learning rate: 0.00038
2024-12-20 16:33:38.512554: Validation loss did not improve from -0.47768. Patience: 35/50
2024-12-20 16:33:38.514830: train_loss -0.816
2024-12-20 16:33:38.515661: val_loss -0.4357
2024-12-20 16:33:38.516478: Pseudo dice [0.7126]
2024-12-20 16:33:38.517189: Epoch time: 330.91 s
2024-12-20 16:33:39.964114: 
2024-12-20 16:33:39.965535: Epoch 147
2024-12-20 16:33:39.966427: Current learning rate: 0.0003
2024-12-20 16:39:16.111397: Validation loss did not improve from -0.47768. Patience: 36/50
2024-12-20 16:39:16.114216: train_loss -0.8209
2024-12-20 16:39:16.115769: val_loss -0.4033
2024-12-20 16:39:16.116602: Pseudo dice [0.716]
2024-12-20 16:39:16.117340: Epoch time: 336.15 s
2024-12-20 16:39:16.117913: Yayy! New best EMA pseudo Dice: 0.7114
2024-12-20 16:39:17.967595: 
2024-12-20 16:39:17.968741: Epoch 148
2024-12-20 16:39:17.969424: Current learning rate: 0.00021
2024-12-20 16:44:08.307597: Validation loss did not improve from -0.47768. Patience: 37/50
2024-12-20 16:44:08.308387: train_loss -0.8207
2024-12-20 16:44:08.309191: val_loss -0.3968
2024-12-20 16:44:08.310002: Pseudo dice [0.7054]
2024-12-20 16:44:08.310695: Epoch time: 290.34 s
2024-12-20 16:44:10.205067: 
2024-12-20 16:44:10.206114: Epoch 149
2024-12-20 16:44:10.206881: Current learning rate: 0.00011
2024-12-20 16:50:00.663798: Validation loss did not improve from -0.47768. Patience: 38/50
2024-12-20 16:50:00.664574: train_loss -0.8167
2024-12-20 16:50:00.665332: val_loss -0.4624
2024-12-20 16:50:00.666110: Pseudo dice [0.7205]
2024-12-20 16:50:00.666909: Epoch time: 350.46 s
2024-12-20 16:50:00.667883: Yayy! New best EMA pseudo Dice: 0.7117
2024-12-20 16:50:02.885866: Training done.
2024-12-20 16:50:03.074855: Using splits from existing split file: /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset308_Sohee_Calcium_OCT_CrossValidation/splits_final_60.json
2024-12-20 16:50:03.089136: The split file contains 5 splits.
2024-12-20 16:50:03.090386: Desired fold for training: 1
2024-12-20 16:50:03.091509: This split has 4 training and 5 validation cases.
2024-12-20 16:50:03.092474: predicting 101-019
2024-12-20 16:50:03.107033: 101-019, shape torch.Size([1, 375, 498, 498]), rank 0
2024-12-20 16:52:23.463598: predicting 101-045
2024-12-20 16:52:23.483703: 101-045, shape torch.Size([1, 375, 498, 498]), rank 0
2024-12-20 16:54:39.429479: predicting 106-002
2024-12-20 16:54:39.450349: 106-002, shape torch.Size([1, 539, 498, 498]), rank 0
2024-12-20 16:57:52.414278: predicting 704-003
2024-12-20 16:57:52.428605: 704-003, shape torch.Size([1, 375, 498, 498]), rank 0
2024-12-20 16:59:53.502259: predicting 706-005
2024-12-20 16:59:53.515671: 706-005, shape torch.Size([1, 375, 498, 498]), rank 0
2024-12-20 17:02:22.502908: Validation complete
2024-12-20 17:02:22.503554: Mean Validation Dice:  0.7071197454731211

############################
INFO: You are using the old nnU-Net default plans. We have updated our recommendations. Please consider using those instead! Read more here: https://github.com/MIC-DKFZ/nnUNet/blob/master/documentation/resenc_presets.md
############################

Using device: cuda:0

#######################################################################
Please cite the following paper when using nnU-Net:
Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.
#######################################################################

2024-12-20 17:02:29.829379: Using torch.compile...

############################
INFO: You are using the old nnU-Net default plans. We have updated our recommendations. Please consider using those instead! Read more here: https://github.com/MIC-DKFZ/nnUNet/blob/master/documentation/resenc_presets.md
############################

Using device: cuda:0

#######################################################################
Please cite the following paper when using nnU-Net:
Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.
#######################################################################

2024-12-20 17:02:29.825691: Using torch.compile...
/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
################### Loading pretrained weights from file  /home/gridsan/nchutisilp/projects/OpenAI-CLIP/clip_pretrained_nnUNet.pt ###################
Below is the list of overlapping blocks in pretrained model and nnUNet architecture:
encoder.stages.0.0.convs.0.conv.weight shape torch.Size([32, 1, 3, 3, 3])
encoder.stages.0.0.convs.0.conv.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.norm.weight shape torch.Size([32])
encoder.stages.0.0.convs.0.norm.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.0.weight shape torch.Size([32, 1, 3, 3, 3])
encoder.stages.0.0.convs.0.all_modules.0.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.1.weight shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.1.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
encoder.stages.0.0.convs.1.conv.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.norm.weight shape torch.Size([32])
encoder.stages.0.0.convs.1.norm.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
encoder.stages.0.0.convs.1.all_modules.0.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.1.weight shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.1.bias shape torch.Size([32])
encoder.stages.1.0.convs.0.conv.weight shape torch.Size([64, 32, 3, 3, 3])
encoder.stages.1.0.convs.0.conv.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.norm.weight shape torch.Size([64])
encoder.stages.1.0.convs.0.norm.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.0.weight shape torch.Size([64, 32, 3, 3, 3])
encoder.stages.1.0.convs.0.all_modules.0.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.1.weight shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.1.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
encoder.stages.1.0.convs.1.conv.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.norm.weight shape torch.Size([64])
encoder.stages.1.0.convs.1.norm.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
encoder.stages.1.0.convs.1.all_modules.0.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.1.weight shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.1.bias shape torch.Size([64])
encoder.stages.2.0.convs.0.conv.weight shape torch.Size([128, 64, 3, 3, 3])
encoder.stages.2.0.convs.0.conv.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.norm.weight shape torch.Size([128])
encoder.stages.2.0.convs.0.norm.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.0.weight shape torch.Size([128, 64, 3, 3, 3])
encoder.stages.2.0.convs.0.all_modules.0.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.1.weight shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.1.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
encoder.stages.2.0.convs.1.conv.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.norm.weight shape torch.Size([128])
encoder.stages.2.0.convs.1.norm.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
encoder.stages.2.0.convs.1.all_modules.0.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.1.weight shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.1.bias shape torch.Size([128])
encoder.stages.3.0.convs.0.conv.weight shape torch.Size([256, 128, 3, 3, 3])
encoder.stages.3.0.convs.0.conv.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.norm.weight shape torch.Size([256])
encoder.stages.3.0.convs.0.norm.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.0.weight shape torch.Size([256, 128, 3, 3, 3])
encoder.stages.3.0.convs.0.all_modules.0.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.1.weight shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.1.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
encoder.stages.3.0.convs.1.conv.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.norm.weight shape torch.Size([256])
encoder.stages.3.0.convs.1.norm.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
encoder.stages.3.0.convs.1.all_modules.0.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.1.weight shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.1.bias shape torch.Size([256])
encoder.stages.4.0.convs.0.conv.weight shape torch.Size([320, 256, 3, 3, 3])
encoder.stages.4.0.convs.0.conv.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.norm.weight shape torch.Size([320])
encoder.stages.4.0.convs.0.norm.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.0.weight shape torch.Size([320, 256, 3, 3, 3])
encoder.stages.4.0.convs.0.all_modules.0.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.1.weight shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.1.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.4.0.convs.1.conv.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.norm.weight shape torch.Size([320])
encoder.stages.4.0.convs.1.norm.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.4.0.convs.1.all_modules.0.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.1.weight shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.1.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.0.conv.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.norm.weight shape torch.Size([320])
encoder.stages.5.0.convs.0.norm.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.0.all_modules.0.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.1.weight shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.1.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.1.conv.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.norm.weight shape torch.Size([320])
encoder.stages.5.0.convs.1.norm.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.1.all_modules.0.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.1.weight shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.0.0.convs.0.conv.weight shape torch.Size([32, 1, 3, 3, 3])
decoder.encoder.stages.0.0.convs.0.conv.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.norm.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.norm.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.0.weight shape torch.Size([32, 1, 3, 3, 3])
decoder.encoder.stages.0.0.convs.0.all_modules.0.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.1.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.1.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.encoder.stages.0.0.convs.1.conv.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.norm.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.norm.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.encoder.stages.0.0.convs.1.all_modules.0.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.1.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.1.bias shape torch.Size([32])
decoder.encoder.stages.1.0.convs.0.conv.weight shape torch.Size([64, 32, 3, 3, 3])
decoder.encoder.stages.1.0.convs.0.conv.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.norm.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.norm.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.0.weight shape torch.Size([64, 32, 3, 3, 3])
decoder.encoder.stages.1.0.convs.0.all_modules.0.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.1.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.1.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.encoder.stages.1.0.convs.1.conv.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.norm.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.norm.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.encoder.stages.1.0.convs.1.all_modules.0.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.1.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.1.bias shape torch.Size([64])
decoder.encoder.stages.2.0.convs.0.conv.weight shape torch.Size([128, 64, 3, 3, 3])
decoder.encoder.stages.2.0.convs.0.conv.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.norm.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.norm.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.0.weight shape torch.Size([128, 64, 3, 3, 3])
decoder.encoder.stages.2.0.convs.0.all_modules.0.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.1.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.1.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.encoder.stages.2.0.convs.1.conv.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.norm.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.norm.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.encoder.stages.2.0.convs.1.all_modules.0.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.1.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.1.bias shape torch.Size([128])
decoder.encoder.stages.3.0.convs.0.conv.weight shape torch.Size([256, 128, 3, 3, 3])
decoder.encoder.stages.3.0.convs.0.conv.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.norm.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.norm.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.0.weight shape torch.Size([256, 128, 3, 3, 3])
decoder.encoder.stages.3.0.convs.0.all_modules.0.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.1.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.1.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.encoder.stages.3.0.convs.1.conv.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.norm.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.norm.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.encoder.stages.3.0.convs.1.all_modules.0.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.1.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.1.bias shape torch.Size([256])
decoder.encoder.stages.4.0.convs.0.conv.weight shape torch.Size([320, 256, 3, 3, 3])
decoder.encoder.stages.4.0.convs.0.conv.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.norm.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.norm.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.0.weight shape torch.Size([320, 256, 3, 3, 3])
decoder.encoder.stages.4.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.4.0.convs.1.conv.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.norm.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.norm.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.4.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.0.conv.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.norm.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.norm.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.1.conv.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.norm.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.norm.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.stages.0.convs.0.conv.weight shape torch.Size([320, 640, 3, 3, 3])
decoder.stages.0.convs.0.conv.bias shape torch.Size([320])
decoder.stages.0.convs.0.norm.weight shape torch.Size([320])
decoder.stages.0.convs.0.norm.bias shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.0.weight shape torch.Size([320, 640, 3, 3, 3])
decoder.stages.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.stages.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.stages.0.convs.1.conv.bias shape torch.Size([320])
decoder.stages.0.convs.1.norm.weight shape torch.Size([320])
decoder.stages.0.convs.1.norm.bias shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.stages.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.stages.1.convs.0.conv.weight shape torch.Size([256, 512, 3, 3, 3])
decoder.stages.1.convs.0.conv.bias shape torch.Size([256])
decoder.stages.1.convs.0.norm.weight shape torch.Size([256])
decoder.stages.1.convs.0.norm.bias shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.0.weight shape torch.Size([256, 512, 3, 3, 3])
decoder.stages.1.convs.0.all_modules.0.bias shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.1.weight shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.1.bias shape torch.Size([256])
decoder.stages.1.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.stages.1.convs.1.conv.bias shape torch.Size([256])
decoder.stages.1.convs.1.norm.weight shape torch.Size([256])
decoder.stages.1.convs.1.norm.bias shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.stages.1.convs.1.all_modules.0.bias shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.1.weight shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.1.bias shape torch.Size([256])
decoder.stages.2.convs.0.conv.weight shape torch.Size([128, 256, 3, 3, 3])
decoder.stages.2.convs.0.conv.bias shape torch.Size([128])
decoder.stages.2.convs.0.norm.weight shape torch.Size([128])
decoder.stages.2.convs.0.norm.bias shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.0.weight shape torch.Size([128, 256, 3, 3, 3])
decoder.stages.2.convs.0.all_modules.0.bias shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.1.weight shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.1.bias shape torch.Size([128])
decoder.stages.2.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.stages.2.convs.1.conv.bias shape torch.Size([128])
decoder.stages.2.convs.1.norm.weight shape torch.Size([128])
decoder.stages.2.convs.1.norm.bias shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.stages.2.convs.1.all_modules.0.bias shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.1.weight shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.1.bias shape torch.Size([128])
decoder.stages.3.convs.0.conv.weight shape torch.Size([64, 128, 3, 3, 3])
decoder.stages.3.convs.0.conv.bias shape torch.Size([64])
decoder.stages.3.convs.0.norm.weight shape torch.Size([64])
decoder.stages.3.convs.0.norm.bias shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.0.weight shape torch.Size([64, 128, 3, 3, 3])
decoder.stages.3.convs.0.all_modules.0.bias shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.1.weight shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.1.bias shape torch.Size([64])
decoder.stages.3.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.stages.3.convs.1.conv.bias shape torch.Size([64])
decoder.stages.3.convs.1.norm.weight shape torch.Size([64])
decoder.stages.3.convs.1.norm.bias shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.stages.3.convs.1.all_modules.0.bias shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.1.weight shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.1.bias shape torch.Size([64])
decoder.stages.4.convs.0.conv.weight shape torch.Size([32, 64, 3, 3, 3])
decoder.stages.4.convs.0.conv.bias shape torch.Size([32])
decoder.stages.4.convs.0.norm.weight shape torch.Size([32])
decoder.stages.4.convs.0.norm.bias shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.0.weight shape torch.Size([32, 64, 3, 3, 3])
decoder.stages.4.convs.0.all_modules.0.bias shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.1.weight shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.1.bias shape torch.Size([32])
decoder.stages.4.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.stages.4.convs.1.conv.bias shape torch.Size([32])
decoder.stages.4.convs.1.norm.weight shape torch.Size([32])
decoder.stages.4.convs.1.norm.bias shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.stages.4.convs.1.all_modules.0.bias shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.1.weight shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.1.bias shape torch.Size([32])
decoder.transpconvs.0.weight shape torch.Size([320, 320, 1, 2, 2])
decoder.transpconvs.0.bias shape torch.Size([320])
decoder.transpconvs.1.weight shape torch.Size([320, 256, 2, 2, 2])
decoder.transpconvs.1.bias shape torch.Size([256])
decoder.transpconvs.2.weight shape torch.Size([256, 128, 2, 2, 2])
decoder.transpconvs.2.bias shape torch.Size([128])
decoder.transpconvs.3.weight shape torch.Size([128, 64, 2, 2, 2])
decoder.transpconvs.3.bias shape torch.Size([64])
decoder.transpconvs.4.weight shape torch.Size([64, 32, 2, 2, 2])
decoder.transpconvs.4.bias shape torch.Size([32])
################### Done ###################
2024-12-20 17:02:33.190548: do_dummy_2d_data_aug: True
2024-12-20 17:02:33.191442: Using splits from existing split file: /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset308_Sohee_Calcium_OCT_CrossValidation/splits_final_60.json
2024-12-20 17:02:33.192753: The split file contains 5 splits.
2024-12-20 17:02:33.193600: Desired fold for training: 2
2024-12-20 17:02:33.194546: This split has 4 training and 4 validation cases.
################### Loading pretrained weights from file  /home/gridsan/nchutisilp/projects/OpenAI-CLIP/clip_pretrained_nnUNet.pt ###################
Below is the list of overlapping blocks in pretrained model and nnUNet architecture:
encoder.stages.0.0.convs.0.conv.weight shape torch.Size([32, 1, 3, 3, 3])
encoder.stages.0.0.convs.0.conv.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.norm.weight shape torch.Size([32])
encoder.stages.0.0.convs.0.norm.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.0.weight shape torch.Size([32, 1, 3, 3, 3])
encoder.stages.0.0.convs.0.all_modules.0.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.1.weight shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.1.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
encoder.stages.0.0.convs.1.conv.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.norm.weight shape torch.Size([32])
encoder.stages.0.0.convs.1.norm.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
encoder.stages.0.0.convs.1.all_modules.0.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.1.weight shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.1.bias shape torch.Size([32])
encoder.stages.1.0.convs.0.conv.weight shape torch.Size([64, 32, 3, 3, 3])
encoder.stages.1.0.convs.0.conv.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.norm.weight shape torch.Size([64])
encoder.stages.1.0.convs.0.norm.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.0.weight shape torch.Size([64, 32, 3, 3, 3])
encoder.stages.1.0.convs.0.all_modules.0.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.1.weight shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.1.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
encoder.stages.1.0.convs.1.conv.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.norm.weight shape torch.Size([64])
encoder.stages.1.0.convs.1.norm.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
encoder.stages.1.0.convs.1.all_modules.0.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.1.weight shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.1.bias shape torch.Size([64])
encoder.stages.2.0.convs.0.conv.weight shape torch.Size([128, 64, 3, 3, 3])
encoder.stages.2.0.convs.0.conv.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.norm.weight shape torch.Size([128])
encoder.stages.2.0.convs.0.norm.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.0.weight shape torch.Size([128, 64, 3, 3, 3])
encoder.stages.2.0.convs.0.all_modules.0.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.1.weight shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.1.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
encoder.stages.2.0.convs.1.conv.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.norm.weight shape torch.Size([128])
encoder.stages.2.0.convs.1.norm.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
encoder.stages.2.0.convs.1.all_modules.0.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.1.weight shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.1.bias shape torch.Size([128])
encoder.stages.3.0.convs.0.conv.weight shape torch.Size([256, 128, 3, 3, 3])
encoder.stages.3.0.convs.0.conv.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.norm.weight shape torch.Size([256])
encoder.stages.3.0.convs.0.norm.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.0.weight shape torch.Size([256, 128, 3, 3, 3])
encoder.stages.3.0.convs.0.all_modules.0.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.1.weight shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.1.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
encoder.stages.3.0.convs.1.conv.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.norm.weight shape torch.Size([256])
encoder.stages.3.0.convs.1.norm.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
encoder.stages.3.0.convs.1.all_modules.0.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.1.weight shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.1.bias shape torch.Size([256])
encoder.stages.4.0.convs.0.conv.weight shape torch.Size([320, 256, 3, 3, 3])
encoder.stages.4.0.convs.0.conv.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.norm.weight shape torch.Size([320])
encoder.stages.4.0.convs.0.norm.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.0.weight shape torch.Size([320, 256, 3, 3, 3])
encoder.stages.4.0.convs.0.all_modules.0.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.1.weight shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.1.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.4.0.convs.1.conv.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.norm.weight shape torch.Size([320])
encoder.stages.4.0.convs.1.norm.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.4.0.convs.1.all_modules.0.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.1.weight shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.1.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.0.conv.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.norm.weight shape torch.Size([320])
encoder.stages.5.0.convs.0.norm.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.0.all_modules.0.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.1.weight shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.1.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.1.conv.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.norm.weight shape torch.Size([320])
encoder.stages.5.0.convs.1.norm.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.1.all_modules.0.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.1.weight shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.0.0.convs.0.conv.weight shape torch.Size([32, 1, 3, 3, 3])
decoder.encoder.stages.0.0.convs.0.conv.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.norm.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.norm.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.0.weight shape torch.Size([32, 1, 3, 3, 3])
decoder.encoder.stages.0.0.convs.0.all_modules.0.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.1.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.1.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.encoder.stages.0.0.convs.1.conv.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.norm.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.norm.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.encoder.stages.0.0.convs.1.all_modules.0.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.1.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.1.bias shape torch.Size([32])
decoder.encoder.stages.1.0.convs.0.conv.weight shape torch.Size([64, 32, 3, 3, 3])
decoder.encoder.stages.1.0.convs.0.conv.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.norm.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.norm.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.0.weight shape torch.Size([64, 32, 3, 3, 3])
decoder.encoder.stages.1.0.convs.0.all_modules.0.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.1.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.1.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.encoder.stages.1.0.convs.1.conv.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.norm.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.norm.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.encoder.stages.1.0.convs.1.all_modules.0.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.1.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.1.bias shape torch.Size([64])
decoder.encoder.stages.2.0.convs.0.conv.weight shape torch.Size([128, 64, 3, 3, 3])
decoder.encoder.stages.2.0.convs.0.conv.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.norm.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.norm.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.0.weight shape torch.Size([128, 64, 3, 3, 3])
decoder.encoder.stages.2.0.convs.0.all_modules.0.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.1.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.1.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.encoder.stages.2.0.convs.1.conv.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.norm.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.norm.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.encoder.stages.2.0.convs.1.all_modules.0.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.1.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.1.bias shape torch.Size([128])
decoder.encoder.stages.3.0.convs.0.conv.weight shape torch.Size([256, 128, 3, 3, 3])
decoder.encoder.stages.3.0.convs.0.conv.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.norm.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.norm.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.0.weight shape torch.Size([256, 128, 3, 3, 3])
decoder.encoder.stages.3.0.convs.0.all_modules.0.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.1.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.1.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.encoder.stages.3.0.convs.1.conv.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.norm.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.norm.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.encoder.stages.3.0.convs.1.all_modules.0.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.1.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.1.bias shape torch.Size([256])
decoder.encoder.stages.4.0.convs.0.conv.weight shape torch.Size([320, 256, 3, 3, 3])
decoder.encoder.stages.4.0.convs.0.conv.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.norm.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.norm.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.0.weight shape torch.Size([320, 256, 3, 3, 3])
decoder.encoder.stages.4.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.4.0.convs.1.conv.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.norm.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.norm.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.4.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.0.conv.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.norm.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.norm.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.1.conv.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.norm.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.norm.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.stages.0.convs.0.conv.weight shape torch.Size([320, 640, 3, 3, 3])
decoder.stages.0.convs.0.conv.bias shape torch.Size([320])
decoder.stages.0.convs.0.norm.weight shape torch.Size([320])
decoder.stages.0.convs.0.norm.bias shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.0.weight shape torch.Size([320, 640, 3, 3, 3])
decoder.stages.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.stages.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.stages.0.convs.1.conv.bias shape torch.Size([320])
decoder.stages.0.convs.1.norm.weight shape torch.Size([320])
decoder.stages.0.convs.1.norm.bias shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.stages.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.stages.1.convs.0.conv.weight shape torch.Size([256, 512, 3, 3, 3])
decoder.stages.1.convs.0.conv.bias shape torch.Size([256])
decoder.stages.1.convs.0.norm.weight shape torch.Size([256])
decoder.stages.1.convs.0.norm.bias shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.0.weight shape torch.Size([256, 512, 3, 3, 3])
decoder.stages.1.convs.0.all_modules.0.bias shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.1.weight shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.1.bias shape torch.Size([256])
decoder.stages.1.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.stages.1.convs.1.conv.bias shape torch.Size([256])
decoder.stages.1.convs.1.norm.weight shape torch.Size([256])
decoder.stages.1.convs.1.norm.bias shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.stages.1.convs.1.all_modules.0.bias shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.1.weight shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.1.bias shape torch.Size([256])
decoder.stages.2.convs.0.conv.weight shape torch.Size([128, 256, 3, 3, 3])
decoder.stages.2.convs.0.conv.bias shape torch.Size([128])
decoder.stages.2.convs.0.norm.weight shape torch.Size([128])
decoder.stages.2.convs.0.norm.bias shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.0.weight shape torch.Size([128, 256, 3, 3, 3])
decoder.stages.2.convs.0.all_modules.0.bias shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.1.weight shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.1.bias shape torch.Size([128])
decoder.stages.2.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.stages.2.convs.1.conv.bias shape torch.Size([128])
decoder.stages.2.convs.1.norm.weight shape torch.Size([128])
decoder.stages.2.convs.1.norm.bias shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.stages.2.convs.1.all_modules.0.bias shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.1.weight shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.1.bias shape torch.Size([128])
decoder.stages.3.convs.0.conv.weight shape torch.Size([64, 128, 3, 3, 3])
decoder.stages.3.convs.0.conv.bias shape torch.Size([64])
decoder.stages.3.convs.0.norm.weight shape torch.Size([64])
decoder.stages.3.convs.0.norm.bias shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.0.weight shape torch.Size([64, 128, 3, 3, 3])
decoder.stages.3.convs.0.all_modules.0.bias shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.1.weight shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.1.bias shape torch.Size([64])
decoder.stages.3.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.stages.3.convs.1.conv.bias shape torch.Size([64])
decoder.stages.3.convs.1.norm.weight shape torch.Size([64])
decoder.stages.3.convs.1.norm.bias shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.stages.3.convs.1.all_modules.0.bias shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.1.weight shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.1.bias shape torch.Size([64])
decoder.stages.4.convs.0.conv.weight shape torch.Size([32, 64, 3, 3, 3])
decoder.stages.4.convs.0.conv.bias shape torch.Size([32])
decoder.stages.4.convs.0.norm.weight shape torch.Size([32])
decoder.stages.4.convs.0.norm.bias shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.0.weight shape torch.Size([32, 64, 3, 3, 3])
decoder.stages.4.convs.0.all_modules.0.bias shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.1.weight shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.1.bias shape torch.Size([32])
decoder.stages.4.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.stages.4.convs.1.conv.bias shape torch.Size([32])
decoder.stages.4.convs.1.norm.weight shape torch.Size([32])
decoder.stages.4.convs.1.norm.bias shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.stages.4.convs.1.all_modules.0.bias shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.1.weight shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.1.bias shape torch.Size([32])
decoder.transpconvs.0.weight shape torch.Size([320, 320, 1, 2, 2])
decoder.transpconvs.0.bias shape torch.Size([320])
decoder.transpconvs.1.weight shape torch.Size([320, 256, 2, 2, 2])
decoder.transpconvs.1.bias shape torch.Size([256])
decoder.transpconvs.2.weight shape torch.Size([256, 128, 2, 2, 2])
decoder.transpconvs.2.bias shape torch.Size([128])
decoder.transpconvs.3.weight shape torch.Size([128, 64, 2, 2, 2])
decoder.transpconvs.3.bias shape torch.Size([64])
decoder.transpconvs.4.weight shape torch.Size([64, 32, 2, 2, 2])
decoder.transpconvs.4.bias shape torch.Size([32])
################### Done ###################
2024-12-20 17:02:33.183657: do_dummy_2d_data_aug: True
2024-12-20 17:02:33.185544: Using splits from existing split file: /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset308_Sohee_Calcium_OCT_CrossValidation/splits_final_60.json
2024-12-20 17:02:33.187960: The split file contains 5 splits.
2024-12-20 17:02:33.188794: Desired fold for training: 3
2024-12-20 17:02:33.189435: This split has 4 training and 5 validation cases.
using pin_memory on device 0
using pin_memory on device 0
using pin_memory on device 0

This is the configuration used by this training:
Configuration name: 3d_32x160x128_b10
 {'data_identifier': 'nnUNetPlans_3d_fullres', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 10, 'patch_size': [32, 160, 128], 'median_image_size_in_voxels': [375.0, 498.0, 498.0], 'spacing': [1.0, 1.0, 1.0], 'normalization_schemes': ['ZScoreNormalization'], 'use_mask_for_norm': [False], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'architecture': {'network_class_name': 'dynamic_network_architectures.architectures.unet.PlainConvUNet', 'arch_kwargs': {'n_stages': 6, 'features_per_stage': [32, 64, 128, 256, 320, 320], 'conv_op': 'torch.nn.modules.conv.Conv3d', 'kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]], 'strides': [[1, 1, 1], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [1, 2, 2]], 'n_conv_per_stage': [2, 2, 2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2, 2, 2], 'conv_bias': True, 'norm_op': 'torch.nn.modules.instancenorm.InstanceNorm3d', 'norm_op_kwargs': {'eps': 1e-05, 'affine': True}, 'dropout_op': None, 'dropout_op_kwargs': None, 'nonlin': 'torch.nn.LeakyReLU', 'nonlin_kwargs': {'inplace': True}, 'deep_supervision': True}, '_kw_requires_import': ['conv_op', 'norm_op', 'dropout_op', 'nonlin']}, 'batch_dice': True, 'inherits_from': '3d_fullres'} 

These are the global plan.json settings:
 {'dataset_name': 'Dataset308_Sohee_Calcium_OCT_CrossValidation', 'plans_name': 'nnUNetPlans', 'original_median_spacing_after_transp': [1.0, 1.0, 1.0], 'original_median_shape_after_transp': [375, 498, 498], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 0.4977976381778717, 'mean': 0.13507525622844696, 'median': 0.09599608182907104, 'min': 0.0, 'percentile_00_5': 0.014754901640117168, 'percentile_99_5': 0.4977976381778717, 'std': 0.12025152146816254}}} 

2024-12-20 17:03:01.601071: unpacking dataset...
using pin_memory on device 0

This is the configuration used by this training:
Configuration name: 3d_32x160x128_b10
 {'data_identifier': 'nnUNetPlans_3d_fullres', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 10, 'patch_size': [32, 160, 128], 'median_image_size_in_voxels': [375.0, 498.0, 498.0], 'spacing': [1.0, 1.0, 1.0], 'normalization_schemes': ['ZScoreNormalization'], 'use_mask_for_norm': [False], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'architecture': {'network_class_name': 'dynamic_network_architectures.architectures.unet.PlainConvUNet', 'arch_kwargs': {'n_stages': 6, 'features_per_stage': [32, 64, 128, 256, 320, 320], 'conv_op': 'torch.nn.modules.conv.Conv3d', 'kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]], 'strides': [[1, 1, 1], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [1, 2, 2]], 'n_conv_per_stage': [2, 2, 2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2, 2, 2], 'conv_bias': True, 'norm_op': 'torch.nn.modules.instancenorm.InstanceNorm3d', 'norm_op_kwargs': {'eps': 1e-05, 'affine': True}, 'dropout_op': None, 'dropout_op_kwargs': None, 'nonlin': 'torch.nn.LeakyReLU', 'nonlin_kwargs': {'inplace': True}, 'deep_supervision': True}, '_kw_requires_import': ['conv_op', 'norm_op', 'dropout_op', 'nonlin']}, 'batch_dice': True, 'inherits_from': '3d_fullres'} 

These are the global plan.json settings:
 {'dataset_name': 'Dataset308_Sohee_Calcium_OCT_CrossValidation', 'plans_name': 'nnUNetPlans', 'original_median_spacing_after_transp': [1.0, 1.0, 1.0], 'original_median_shape_after_transp': [375, 498, 498], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 0.4977976381778717, 'mean': 0.13507525622844696, 'median': 0.09599608182907104, 'min': 0.0, 'percentile_00_5': 0.014754901640117168, 'percentile_99_5': 0.4977976381778717, 'std': 0.12025152146816254}}} 

2024-12-20 17:03:03.402571: unpacking dataset...
2024-12-20 17:03:06.310316: unpacking done...
2024-12-20 17:03:06.733571: Unable to plot network architecture: nnUNet_compile is enabled!
2024-12-20 17:03:06.783693: 
2024-12-20 17:03:06.784654: Epoch 0
2024-12-20 17:03:06.785611: Current learning rate: 0.01
2024-12-20 17:03:07.952476: unpacking done...
2024-12-20 17:03:07.958968: Unable to plot network architecture: nnUNet_compile is enabled!
2024-12-20 17:03:07.995300: 
2024-12-20 17:03:07.996222: Epoch 0
2024-12-20 17:03:07.997147: Current learning rate: 0.01
concurrent.futures.process._RemoteTraceback: 
"""
Traceback (most recent call last):
  File "/home/gridsan/nchutisilp/.conda/envs/nnunet/lib/python3.9/concurrent/futures/process.py", line 246, in _process_worker
    r = call_item.fn(*call_item.args, **call_item.kwargs)
  File "/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/_inductor/codecache.py", line 2492, in _worker_compile
    kernel = TritonCodeCache.load(kernel_name, source_code)
  File "/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/_inductor/codecache.py", line 2205, in load
    mod = PyCodeCache.load(source_code)
  File "/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/_inductor/codecache.py", line 2137, in load
    return cls.load_by_key_path(key, path, linemap, attrs)
  File "/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/_inductor/codecache.py", line 2160, in load_by_key_path
    exec(code, mod.__dict__, mod.__dict__)
  File "/state/partition1/slurm_tmp/27632404.4294967291.0/torchinductor_nchutisilp/ad/cadzaybh4uknn7zkalumm5yaigcp3wp34ziefbua6uhu7gggoboy.py", line 12, in <module>
    @triton_heuristics.pointwise(
  File "/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/_inductor/triton_heuristics.py", line 1188, in pointwise
    return cached_autotune(
  File "/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/_inductor/triton_heuristics.py", line 883, in cached_autotune
    best_config = json.loads(fd.read())
  File "/home/gridsan/nchutisilp/.conda/envs/nnunet/lib/python3.9/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
  File "/home/gridsan/nchutisilp/.conda/envs/nnunet/lib/python3.9/json/decoder.py", line 340, in decode
    raise JSONDecodeError("Extra data", s, end)
json.decoder.JSONDecodeError: Extra data: line 1 column 161 (char 160)
"""

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/gridsan/nchutisilp/.conda/envs/nnunet/bin/nnUNetv2_train", line 8, in <module>
concurrent.futures.process._RemoteTraceback: 
"""
Traceback (most recent call last):
  File "/home/gridsan/nchutisilp/.conda/envs/nnunet/lib/python3.9/concurrent/futures/process.py", line 246, in _process_worker
    r = call_item.fn(*call_item.args, **call_item.kwargs)
  File "/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/_inductor/codecache.py", line 2492, in _worker_compile
    kernel = TritonCodeCache.load(kernel_name, source_code)
  File "/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/_inductor/codecache.py", line 2205, in load
    mod = PyCodeCache.load(source_code)
  File "/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/_inductor/codecache.py", line 2137, in load
    return cls.load_by_key_path(key, path, linemap, attrs)
  File "/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/_inductor/codecache.py", line 2160, in load_by_key_path
    exec(code, mod.__dict__, mod.__dict__)
  File "/state/partition1/slurm_tmp/27632404.4294967291.0/torchinductor_nchutisilp/ad/cadzaybh4uknn7zkalumm5yaigcp3wp34ziefbua6uhu7gggoboy.py", line 12, in <module>
    @triton_heuristics.pointwise(
  File "/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/_inductor/triton_heuristics.py", line 1188, in pointwise
    return cached_autotune(
  File "/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/_inductor/triton_heuristics.py", line 883, in cached_autotune
    best_config = json.loads(fd.read())
  File "/home/gridsan/nchutisilp/.conda/envs/nnunet/lib/python3.9/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
  File "/home/gridsan/nchutisilp/.conda/envs/nnunet/lib/python3.9/json/decoder.py", line 340, in decode
    raise JSONDecodeError("Extra data", s, end)
json.decoder.JSONDecodeError: Extra data: line 1 column 161 (char 160)
"""

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/gridsan/nchutisilp/.conda/envs/nnunet/bin/nnUNetv2_train", line 8, in <module>
    sys.exit(run_training_entry())
  File "/home/gridsan/nchutisilp/projects/nnUNet/nnunetv2/run/run_training.py", line 275, in run_training_entry
    sys.exit(run_training_entry())
  File "/home/gridsan/nchutisilp/projects/nnUNet/nnunetv2/run/run_training.py", line 275, in run_training_entry
    run_training(args.dataset_name_or_id, args.configuration, args.fold, args.tr, args.p, args.pretrained_weights,
  File "/home/gridsan/nchutisilp/projects/nnUNet/nnunetv2/run/run_training.py", line 211, in run_training
    run_training(args.dataset_name_or_id, args.configuration, args.fold, args.tr, args.p, args.pretrained_weights,
  File "/home/gridsan/nchutisilp/projects/nnUNet/nnunetv2/run/run_training.py", line 211, in run_training
    nnunet_trainer.run_training()
  File "/home/gridsan/nchutisilp/projects/nnUNet/nnunetv2/training/nnUNetTrainer/nnUNetTrainer.py", line 1399, in run_training
    nnunet_trainer.run_training()
  File "/home/gridsan/nchutisilp/projects/nnUNet/nnunetv2/training/nnUNetTrainer/nnUNetTrainer.py", line 1399, in run_training
    train_outputs.append(self.train_step(next(self.dataloader_train)))
  File "/home/gridsan/nchutisilp/projects/nnUNet/nnunetv2/training/nnUNetTrainer/nnUNetTrainer.py", line 1014, in train_step
    self.grad_scaler.scale(l).backward()
  File "/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/_tensor.py", line 525, in backward
    train_outputs.append(self.train_step(next(self.dataloader_train)))
  File "/home/gridsan/nchutisilp/projects/nnUNet/nnunetv2/training/nnUNetTrainer/nnUNetTrainer.py", line 1014, in train_step
    self.grad_scaler.scale(l).backward()
  File "/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/_tensor.py", line 525, in backward
    torch.autograd.backward(
  File "/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/autograd/__init__.py", line 267, in backward
    torch.autograd.backward(
  File "/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/autograd/__init__.py", line 267, in backward
    _engine_run_backward(
  File "/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/autograd/graph.py", line 744, in _engine_run_backward
    _engine_run_backward(
  File "/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/autograd/graph.py", line 744, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
  File "/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/autograd/function.py", line 301, in apply
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
  File "/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/autograd/function.py", line 301, in apply
    return user_fn(self, *args)
  File "/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py", line 882, in backward
    return user_fn(self, *args)
  File "/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py", line 882, in backward
    out = call_compiled_backward()
  File "/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py", line 827, in call_compiled_backward
    out = call_compiled_backward()
  File "/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py", line 827, in call_compiled_backward
    CompiledFunction.compiled_bw = aot_config.bw_compiler(
    CompiledFunction.compiled_bw = aot_config.bw_compiler(
  File "/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/_dynamo/backends/common.py", line 36, in _wrapped_bw_compiler
  File "/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/_dynamo/backends/common.py", line 36, in _wrapped_bw_compiler
    return disable(disable(bw_compiler)(*args, **kwargs))
  File "/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/_dynamo/eval_frame.py", line 451, in _fn
    return disable(disable(bw_compiler)(*args, **kwargs))
  File "/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/_dynamo/eval_frame.py", line 451, in _fn
    return fn(*args, **kwargs)
  File "/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/_dynamo/external_utils.py", line 36, in inner
    return fn(*args, **kwargs)
  File "/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/_dynamo/external_utils.py", line 36, in inner
    return fn(*args, **kwargs)
  File "/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/_dynamo/utils.py", line 262, in time_wrapper
    return fn(*args, **kwargs)
  File "/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/_dynamo/utils.py", line 262, in time_wrapper
    r = func(*args, **kwargs)
  File "/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/_inductor/compile_fx.py", line 1293, in bw_compiler
    r = func(*args, **kwargs)
  File "/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/_inductor/compile_fx.py", line 1293, in bw_compiler
    return inner_compile(
  File "/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/_dynamo/repro/after_aot.py", line 83, in debug_wrapper
    inner_compiled_fn = compiler_fn(gm, example_inputs)
  File "/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/_inductor/debug.py", line 304, in inner
    return fn(*args, **kwargs)
  File "/home/gridsan/nchutisilp/.conda/envs/nnunet/lib/python3.9/contextlib.py", line 79, in inner
    return inner_compile(
  File "/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/_dynamo/repro/after_aot.py", line 83, in debug_wrapper
    return func(*args, **kwds)
  File "/home/gridsan/nchutisilp/.conda/envs/nnunet/lib/python3.9/contextlib.py", line 79, in inner
    inner_compiled_fn = compiler_fn(gm, example_inputs)
  File "/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/_inductor/debug.py", line 304, in inner
    return func(*args, **kwds)
  File "/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/_dynamo/utils.py", line 262, in time_wrapper
    return fn(*args, **kwargs)
  File "/home/gridsan/nchutisilp/.conda/envs/nnunet/lib/python3.9/contextlib.py", line 79, in inner
    return func(*args, **kwds)
  File "/home/gridsan/nchutisilp/.conda/envs/nnunet/lib/python3.9/contextlib.py", line 79, in inner
    return func(*args, **kwds)
    r = func(*args, **kwargs)
  File "/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/_dynamo/utils.py", line 262, in time_wrapper
  File "/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/_inductor/compile_fx.py", line 438, in compile_fx_inner
    r = func(*args, **kwargs)
  File "/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/_inductor/compile_fx.py", line 438, in compile_fx_inner
    compiled_graph = fx_codegen_and_compile(
  File "/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/_inductor/compile_fx.py", line 714, in fx_codegen_and_compile
    compiled_graph = fx_codegen_and_compile(
  File "/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/_inductor/compile_fx.py", line 714, in fx_codegen_and_compile
    compiled_fn = graph.compile_to_fn()
    compiled_fn = graph.compile_to_fn()
  File "/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/_inductor/graph.py", line 1307, in compile_to_fn
  File "/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/_inductor/graph.py", line 1307, in compile_to_fn
    return self.compile_to_module().call
    return self.compile_to_module().call
  File "/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/_dynamo/utils.py", line 262, in time_wrapper
  File "/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/_dynamo/utils.py", line 262, in time_wrapper
    r = func(*args, **kwargs)
    r = func(*args, **kwargs)
  File "/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/_inductor/graph.py", line 1254, in compile_to_module
  File "/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/_inductor/graph.py", line 1254, in compile_to_module
    mod = PyCodeCache.load_by_key_path(
  File "/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/_inductor/codecache.py", line 2160, in load_by_key_path
    mod = PyCodeCache.load_by_key_path(
  File "/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/_inductor/codecache.py", line 2160, in load_by_key_path
    exec(code, mod.__dict__, mod.__dict__)
  File "/state/partition1/slurm_tmp/27632404.4294967291.0/torchinductor_nchutisilp/zx/czxley6cqpz3r5erwspwfdbl357ncss4a5ujvbuxtvjf4cnyyfug.py", line 3651, in <module>
    exec(code, mod.__dict__, mod.__dict__)
  File "/state/partition1/slurm_tmp/27632404.4294967291.0/torchinductor_nchutisilp/zx/czxley6cqpz3r5erwspwfdbl357ncss4a5ujvbuxtvjf4cnyyfug.py", line 3651, in <module>
    async_compile.wait(globals())
    async_compile.wait(globals())
  File "/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/_inductor/codecache.py", line 2715, in wait
  File "/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/_inductor/codecache.py", line 2715, in wait
    scope[key] = result.result()
    scope[key] = result.result()
  File "/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/_inductor/codecache.py", line 2522, in result
  File "/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/_inductor/codecache.py", line 2522, in result
    self.future.result()
  File "/home/gridsan/nchutisilp/.conda/envs/nnunet/lib/python3.9/concurrent/futures/_base.py", line 439, in result
    self.future.result()
  File "/home/gridsan/nchutisilp/.conda/envs/nnunet/lib/python3.9/concurrent/futures/_base.py", line 439, in result
    return self.__get_result()
  File "/home/gridsan/nchutisilp/.conda/envs/nnunet/lib/python3.9/concurrent/futures/_base.py", line 391, in __get_result
    return self.__get_result()
  File "/home/gridsan/nchutisilp/.conda/envs/nnunet/lib/python3.9/concurrent/futures/_base.py", line 391, in __get_result
    raise self._exception
json.decoder.JSONDecodeError: Extra data: line 1 column 161 (char 160)
    raise self._exception
json.decoder.JSONDecodeError: Extra data: line 1 column 161 (char 160)
Exception in thread Thread-3:
Traceback (most recent call last):
  File "/home/gridsan/nchutisilp/.conda/envs/nnunet/lib/python3.9/threading.py", line 980, in _bootstrap_inner
Exception in thread Thread-3:
Traceback (most recent call last):
  File "/home/gridsan/nchutisilp/.conda/envs/nnunet/lib/python3.9/threading.py", line 980, in _bootstrap_inner
Exception in thread Thread-2:
Traceback (most recent call last):
  File "/home/gridsan/nchutisilp/.conda/envs/nnunet/lib/python3.9/threading.py", line 980, in _bootstrap_inner
    self.run()
  File "/home/gridsan/nchutisilp/.conda/envs/nnunet/lib/python3.9/threading.py", line 917, in run
    self.run()
    self.run()
  File "/home/gridsan/nchutisilp/.conda/envs/nnunet/lib/python3.9/threading.py", line 917, in run
  File "/home/gridsan/nchutisilp/.conda/envs/nnunet/lib/python3.9/threading.py", line 917, in run
    self._target(*self._args, **self._kwargs)
  File "/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/batchgenerators/dataloading/nondet_multi_threaded_augmenter.py", line 125, in results_loop
    self._target(*self._args, **self._kwargs)
  File "/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/batchgenerators/dataloading/nondet_multi_threaded_augmenter.py", line 125, in results_loop
    self._target(*self._args, **self._kwargs)
  File "/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/batchgenerators/dataloading/nondet_multi_threaded_augmenter.py", line 125, in results_loop
    raise e
  File "/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/batchgenerators/dataloading/nondet_multi_threaded_augmenter.py", line 103, in results_loop
    raise RuntimeError("One or more background workers are no longer alive. Exiting. Please check the "
RuntimeError: One or more background workers are no longer alive. Exiting. Please check the print statements above for the actual error message
    raise e
  File "/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/batchgenerators/dataloading/nondet_multi_threaded_augmenter.py", line 103, in results_loop
    raise e
  File "/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/batchgenerators/dataloading/nondet_multi_threaded_augmenter.py", line 103, in results_loop
    raise RuntimeError("One or more background workers are no longer alive. Exiting. Please check the "
    raise RuntimeError("One or more background workers are no longer alive. Exiting. Please check the "
RuntimeError: One or more background workers are no longer alive. Exiting. Please check the print statements above for the actual error message
RuntimeError: One or more background workers are no longer alive. Exiting. Please check the print statements above for the actual error message
slurmstepd-d-8-9-2: error: *** JOB 27632404 ON d-8-9-2 CANCELLED AT 2024-12-23T23:40:07 DUE TO TIME LIMIT ***
