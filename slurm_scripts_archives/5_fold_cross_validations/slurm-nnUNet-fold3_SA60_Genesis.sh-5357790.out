/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/bin/python
Starting training with CONFIG=3d_32x160x128_b10, DATASET_ID=307, TRAINER=nnUNetTrainerScaleAnalysis60
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/nnunetv2/training/nnUNetTrainer/nnUNetTrainer.py:169: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  self.grad_scaler = GradScaler() if self.device.type == 'cuda' else None

############################
INFO: You are using the old nnU-Net default plans. We have updated our recommendations. Please consider using those instead! Read more here: https://github.com/MIC-DKFZ/nnUNet/blob/master/documentation/resenc_presets.md
############################

Using device: cuda:0

#######################################################################
Please cite the following paper when using nnU-Net:
Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.
#######################################################################

2025-10-15 05:25:02.602423: Using torch.compile...
################### Loading pretrained weights from file  /nfs/erelab001/shared/Computational_Group/Naravich/datasets/nnUNet_Datasets/../ModelGenesisOutputs/ModelGenesisNNUNetPretrainingV2_noNorm_correct_orientation/Converted_nnUNet_Genesis_OCT_Best.pt ###################
Below is the list of overlapping blocks in pretrained model and nnUNet architecture:
encoder.stages.0.0.convs.0.conv.weight shape torch.Size([32, 1, 3, 3, 3])
encoder.stages.0.0.convs.0.conv.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.norm.weight shape torch.Size([32])
encoder.stages.0.0.convs.0.norm.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.0.weight shape torch.Size([32, 1, 3, 3, 3])
encoder.stages.0.0.convs.0.all_modules.0.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.1.weight shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.1.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
encoder.stages.0.0.convs.1.conv.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.norm.weight shape torch.Size([32])
encoder.stages.0.0.convs.1.norm.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
encoder.stages.0.0.convs.1.all_modules.0.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.1.weight shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.1.bias shape torch.Size([32])
encoder.stages.1.0.convs.0.conv.weight shape torch.Size([64, 32, 3, 3, 3])
encoder.stages.1.0.convs.0.conv.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.norm.weight shape torch.Size([64])
encoder.stages.1.0.convs.0.norm.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.0.weight shape torch.Size([64, 32, 3, 3, 3])
encoder.stages.1.0.convs.0.all_modules.0.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.1.weight shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.1.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
encoder.stages.1.0.convs.1.conv.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.norm.weight shape torch.Size([64])
encoder.stages.1.0.convs.1.norm.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
encoder.stages.1.0.convs.1.all_modules.0.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.1.weight shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.1.bias shape torch.Size([64])
encoder.stages.2.0.convs.0.conv.weight shape torch.Size([128, 64, 3, 3, 3])
encoder.stages.2.0.convs.0.conv.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.norm.weight shape torch.Size([128])
encoder.stages.2.0.convs.0.norm.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.0.weight shape torch.Size([128, 64, 3, 3, 3])
encoder.stages.2.0.convs.0.all_modules.0.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.1.weight shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.1.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
encoder.stages.2.0.convs.1.conv.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.norm.weight shape torch.Size([128])
encoder.stages.2.0.convs.1.norm.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
encoder.stages.2.0.convs.1.all_modules.0.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.1.weight shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.1.bias shape torch.Size([128])
encoder.stages.3.0.convs.0.conv.weight shape torch.Size([256, 128, 3, 3, 3])
encoder.stages.3.0.convs.0.conv.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.norm.weight shape torch.Size([256])
encoder.stages.3.0.convs.0.norm.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.0.weight shape torch.Size([256, 128, 3, 3, 3])
encoder.stages.3.0.convs.0.all_modules.0.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.1.weight shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.1.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
encoder.stages.3.0.convs.1.conv.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.norm.weight shape torch.Size([256])
encoder.stages.3.0.convs.1.norm.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
encoder.stages.3.0.convs.1.all_modules.0.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.1.weight shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.1.bias shape torch.Size([256])
encoder.stages.4.0.convs.0.conv.weight shape torch.Size([320, 256, 3, 3, 3])
encoder.stages.4.0.convs.0.conv.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.norm.weight shape torch.Size([320])
encoder.stages.4.0.convs.0.norm.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.0.weight shape torch.Size([320, 256, 3, 3, 3])
encoder.stages.4.0.convs.0.all_modules.0.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.1.weight shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.1.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.4.0.convs.1.conv.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.norm.weight shape torch.Size([320])
encoder.stages.4.0.convs.1.norm.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.4.0.convs.1.all_modules.0.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.1.weight shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.1.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.0.conv.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.norm.weight shape torch.Size([320])
encoder.stages.5.0.convs.0.norm.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.0.all_modules.0.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.1.weight shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.1.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.1.conv.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.norm.weight shape torch.Size([320])
encoder.stages.5.0.convs.1.norm.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.1.all_modules.0.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.1.weight shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.0.0.convs.0.conv.weight shape torch.Size([32, 1, 3, 3, 3])
decoder.encoder.stages.0.0.convs.0.conv.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.norm.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.norm.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.0.weight shape torch.Size([32, 1, 3, 3, 3])
decoder.encoder.stages.0.0.convs.0.all_modules.0.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.1.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.1.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.encoder.stages.0.0.convs.1.conv.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.norm.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.norm.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.encoder.stages.0.0.convs.1.all_modules.0.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.1.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.1.bias shape torch.Size([32])
decoder.encoder.stages.1.0.convs.0.conv.weight shape torch.Size([64, 32, 3, 3, 3])
decoder.encoder.stages.1.0.convs.0.conv.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.norm.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.norm.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.0.weight shape torch.Size([64, 32, 3, 3, 3])
decoder.encoder.stages.1.0.convs.0.all_modules.0.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.1.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.1.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.encoder.stages.1.0.convs.1.conv.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.norm.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.norm.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.encoder.stages.1.0.convs.1.all_modules.0.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.1.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.1.bias shape torch.Size([64])
decoder.encoder.stages.2.0.convs.0.conv.weight shape torch.Size([128, 64, 3, 3, 3])
decoder.encoder.stages.2.0.convs.0.conv.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.norm.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.norm.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.0.weight shape torch.Size([128, 64, 3, 3, 3])
decoder.encoder.stages.2.0.convs.0.all_modules.0.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.1.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.1.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.encoder.stages.2.0.convs.1.conv.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.norm.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.norm.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.encoder.stages.2.0.convs.1.all_modules.0.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.1.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.1.bias shape torch.Size([128])
decoder.encoder.stages.3.0.convs.0.conv.weight shape torch.Size([256, 128, 3, 3, 3])
decoder.encoder.stages.3.0.convs.0.conv.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.norm.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.norm.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.0.weight shape torch.Size([256, 128, 3, 3, 3])
decoder.encoder.stages.3.0.convs.0.all_modules.0.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.1.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.1.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.encoder.stages.3.0.convs.1.conv.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.norm.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.norm.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.encoder.stages.3.0.convs.1.all_modules.0.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.1.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.1.bias shape torch.Size([256])
decoder.encoder.stages.4.0.convs.0.conv.weight shape torch.Size([320, 256, 3, 3, 3])
decoder.encoder.stages.4.0.convs.0.conv.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.norm.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.norm.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.0.weight shape torch.Size([320, 256, 3, 3, 3])
decoder.encoder.stages.4.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.4.0.convs.1.conv.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.norm.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.norm.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.4.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.0.conv.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.norm.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.norm.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.1.conv.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.norm.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.norm.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.stages.0.convs.0.conv.weight shape torch.Size([320, 640, 3, 3, 3])
decoder.stages.0.convs.0.conv.bias shape torch.Size([320])
decoder.stages.0.convs.0.norm.weight shape torch.Size([320])
decoder.stages.0.convs.0.norm.bias shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.0.weight shape torch.Size([320, 640, 3, 3, 3])
decoder.stages.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.stages.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.stages.0.convs.1.conv.bias shape torch.Size([320])
decoder.stages.0.convs.1.norm.weight shape torch.Size([320])
decoder.stages.0.convs.1.norm.bias shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.stages.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.stages.1.convs.0.conv.weight shape torch.Size([256, 512, 3, 3, 3])
decoder.stages.1.convs.0.conv.bias shape torch.Size([256])
decoder.stages.1.convs.0.norm.weight shape torch.Size([256])
decoder.stages.1.convs.0.norm.bias shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.0.weight shape torch.Size([256, 512, 3, 3, 3])
decoder.stages.1.convs.0.all_modules.0.bias shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.1.weight shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.1.bias shape torch.Size([256])
decoder.stages.1.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.stages.1.convs.1.conv.bias shape torch.Size([256])
decoder.stages.1.convs.1.norm.weight shape torch.Size([256])
decoder.stages.1.convs.1.norm.bias shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.stages.1.convs.1.all_modules.0.bias shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.1.weight shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.1.bias shape torch.Size([256])
decoder.stages.2.convs.0.conv.weight shape torch.Size([128, 256, 3, 3, 3])
decoder.stages.2.convs.0.conv.bias shape torch.Size([128])
decoder.stages.2.convs.0.norm.weight shape torch.Size([128])
decoder.stages.2.convs.0.norm.bias shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.0.weight shape torch.Size([128, 256, 3, 3, 3])
decoder.stages.2.convs.0.all_modules.0.bias shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.1.weight shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.1.bias shape torch.Size([128])
decoder.stages.2.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.stages.2.convs.1.conv.bias shape torch.Size([128])
decoder.stages.2.convs.1.norm.weight shape torch.Size([128])
decoder.stages.2.convs.1.norm.bias shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.stages.2.convs.1.all_modules.0.bias shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.1.weight shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.1.bias shape torch.Size([128])
decoder.stages.3.convs.0.conv.weight shape torch.Size([64, 128, 3, 3, 3])
decoder.stages.3.convs.0.conv.bias shape torch.Size([64])
decoder.stages.3.convs.0.norm.weight shape torch.Size([64])
decoder.stages.3.convs.0.norm.bias shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.0.weight shape torch.Size([64, 128, 3, 3, 3])
decoder.stages.3.convs.0.all_modules.0.bias shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.1.weight shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.1.bias shape torch.Size([64])
decoder.stages.3.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.stages.3.convs.1.conv.bias shape torch.Size([64])
decoder.stages.3.convs.1.norm.weight shape torch.Size([64])
decoder.stages.3.convs.1.norm.bias shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.stages.3.convs.1.all_modules.0.bias shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.1.weight shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.1.bias shape torch.Size([64])
decoder.stages.4.convs.0.conv.weight shape torch.Size([32, 64, 3, 3, 3])
decoder.stages.4.convs.0.conv.bias shape torch.Size([32])
decoder.stages.4.convs.0.norm.weight shape torch.Size([32])
decoder.stages.4.convs.0.norm.bias shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.0.weight shape torch.Size([32, 64, 3, 3, 3])
decoder.stages.4.convs.0.all_modules.0.bias shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.1.weight shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.1.bias shape torch.Size([32])
decoder.stages.4.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.stages.4.convs.1.conv.bias shape torch.Size([32])
decoder.stages.4.convs.1.norm.weight shape torch.Size([32])
decoder.stages.4.convs.1.norm.bias shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.stages.4.convs.1.all_modules.0.bias shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.1.weight shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.1.bias shape torch.Size([32])
decoder.transpconvs.0.weight shape torch.Size([320, 320, 1, 2, 2])
decoder.transpconvs.0.bias shape torch.Size([320])
decoder.transpconvs.1.weight shape torch.Size([320, 256, 2, 2, 2])
decoder.transpconvs.1.bias shape torch.Size([256])
decoder.transpconvs.2.weight shape torch.Size([256, 128, 2, 2, 2])
decoder.transpconvs.2.bias shape torch.Size([128])
decoder.transpconvs.3.weight shape torch.Size([128, 64, 2, 2, 2])
decoder.transpconvs.3.bias shape torch.Size([64])
decoder.transpconvs.4.weight shape torch.Size([64, 32, 2, 2, 2])
decoder.transpconvs.4.bias shape torch.Size([32])
################### Done ###################
2025-10-15 05:25:03.871823: do_dummy_2d_data_aug: True
2025-10-15 05:25:03.872385: Using splits from existing split file: /nfs/erelab001/shared/Computational_Group/Naravich/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset307_Sohee_Calcium_OCT_CrossValidation/splits_final_60.json
2025-10-15 05:25:03.872742: The split file contains 5 splits.
2025-10-15 05:25:03.872908: Desired fold for training: 3
2025-10-15 05:25:03.873089: This split has 4 training and 5 validation cases.
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/fft_conv_pytorch/fft_conv.py:139: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:306.)
  output = output[crop_slices].contiguous()
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/fft_conv_pytorch/fft_conv.py:139: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:306.)
  output = output[crop_slices].contiguous()
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/fft_conv_pytorch/fft_conv.py:139: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:306.)
  output = output[crop_slices].contiguous()
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/fft_conv_pytorch/fft_conv.py:139: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:306.)
  output = output[crop_slices].contiguous()
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/fft_conv_pytorch/fft_conv.py:139: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:306.)
  output = output[crop_slices].contiguous()
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/fft_conv_pytorch/fft_conv.py:139: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:306.)
  output = output[crop_slices].contiguous()
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/fft_conv_pytorch/fft_conv.py:139: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:306.)
  output = output[crop_slices].contiguous()
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/fft_conv_pytorch/fft_conv.py:139: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:306.)
  output = output[crop_slices].contiguous()
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/fft_conv_pytorch/fft_conv.py:139: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:306.)
  output = output[crop_slices].contiguous()
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/fft_conv_pytorch/fft_conv.py:139: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:306.)
  output = output[crop_slices].contiguous()
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/fft_conv_pytorch/fft_conv.py:139: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:306.)
  output = output[crop_slices].contiguous()
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/fft_conv_pytorch/fft_conv.py:139: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:306.)
  output = output[crop_slices].contiguous()
using pin_memory on device 0
using pin_memory on device 0

This is the configuration used by this training:
Configuration name: 3d_32x160x128_b10
 {'data_identifier': 'nnUNetPlans_3d_fullres', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 10, 'patch_size': [32, 160, 128], 'median_image_size_in_voxels': [375.0, 498.0, 498.0], 'spacing': [1.0, 1.0, 1.0], 'normalization_schemes': ['ZScoreNormalization'], 'use_mask_for_norm': [False], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'architecture': {'network_class_name': 'dynamic_network_architectures.architectures.unet.PlainConvUNet', 'arch_kwargs': {'n_stages': 6, 'features_per_stage': [32, 64, 128, 256, 320, 320], 'conv_op': 'torch.nn.modules.conv.Conv3d', 'kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]], 'strides': [[1, 1, 1], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [1, 2, 2]], 'n_conv_per_stage': [2, 2, 2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2, 2, 2], 'conv_bias': True, 'norm_op': 'torch.nn.modules.instancenorm.InstanceNorm3d', 'norm_op_kwargs': {'eps': 1e-05, 'affine': True}, 'dropout_op': None, 'dropout_op_kwargs': None, 'nonlin': 'torch.nn.LeakyReLU', 'nonlin_kwargs': {'inplace': True}, 'deep_supervision': True}, '_kw_requires_import': ['conv_op', 'norm_op', 'dropout_op', 'nonlin']}, 'batch_dice': True, 'inherits_from': '3d_fullres'} 

These are the global plan.json settings:
 {'dataset_name': 'Dataset307_Sohee_Calcium_OCT_CrossValidation', 'plans_name': 'nnUNetPlans', 'original_median_spacing_after_transp': [1.0, 1.0, 1.0], 'original_median_shape_after_transp': [375, 498, 498], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 0.4977976381778717, 'mean': 0.13507525622844696, 'median': 0.09599608182907104, 'min': 0.0, 'percentile_00_5': 0.014754901640117168, 'percentile_99_5': 0.4977976381778717, 'std': 0.12025152146816254}}} 

2025-10-15 05:25:06.897069: unpacking dataset...
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/torch/_inductor/lowering.py:7095: UserWarning: 
Online softmax is disabled on the fly since Inductor decides to
split the reduction. Cut an issue to PyTorch if this is an
important use case and you want to speed it up with online
softmax.

  warnings.warn(
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/torch/_inductor/lowering.py:7095: UserWarning: 
Online softmax is disabled on the fly since Inductor decides to
split the reduction. Cut an issue to PyTorch if this is an
important use case and you want to speed it up with online
softmax.

  warnings.warn(
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/torch/_inductor/lowering.py:7095: UserWarning: 
Online softmax is disabled on the fly since Inductor decides to
split the reduction. Cut an issue to PyTorch if this is an
important use case and you want to speed it up with online
softmax.

  warnings.warn(
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/torch/_inductor/lowering.py:7095: UserWarning: 
Online softmax is disabled on the fly since Inductor decides to
split the reduction. Cut an issue to PyTorch if this is an
important use case and you want to speed it up with online
softmax.

  warnings.warn(
2025-10-15 05:25:12.540764: unpacking done...
2025-10-15 05:25:12.542791: Unable to plot network architecture: nnUNet_compile is enabled!
2025-10-15 05:25:12.547617: 
2025-10-15 05:25:12.547765: Epoch 0
2025-10-15 05:25:12.547959: Current learning rate: 0.01
2025-10-15 05:26:33.272262: Validation loss improved from 1000.00000 to -0.25308! Patience: 0/50
2025-10-15 05:26:33.272909: train_loss -0.1724
2025-10-15 05:26:33.273079: val_loss -0.2531
2025-10-15 05:26:33.273238: Pseudo dice [np.float32(0.553)]
2025-10-15 05:26:33.273371: Epoch time: 80.73 s
2025-10-15 05:26:33.273487: Yayy! New best EMA pseudo Dice: 0.5529999732971191
2025-10-15 05:26:34.215679: 
2025-10-15 05:26:34.215990: Epoch 1
2025-10-15 05:26:34.216179: Current learning rate: 0.00994
2025-10-15 05:27:20.719813: Validation loss improved from -0.25308 to -0.28900! Patience: 0/50
2025-10-15 05:27:20.720521: train_loss -0.3438
2025-10-15 05:27:20.720750: val_loss -0.289
2025-10-15 05:27:20.720980: Pseudo dice [np.float32(0.5992)]
2025-10-15 05:27:20.721196: Epoch time: 46.51 s
2025-10-15 05:27:20.721324: Yayy! New best EMA pseudo Dice: 0.5576000213623047
2025-10-15 05:27:21.764022: 
2025-10-15 05:27:21.764275: Epoch 2
2025-10-15 05:27:21.764454: Current learning rate: 0.00988
2025-10-15 05:28:08.233851: Validation loss improved from -0.28900 to -0.31696! Patience: 0/50
2025-10-15 05:28:08.234531: train_loss -0.3805
2025-10-15 05:28:08.234690: val_loss -0.317
2025-10-15 05:28:08.234796: Pseudo dice [np.float32(0.6264)]
2025-10-15 05:28:08.235029: Epoch time: 46.47 s
2025-10-15 05:28:08.235142: Yayy! New best EMA pseudo Dice: 0.5644999742507935
2025-10-15 05:28:09.342148: 
2025-10-15 05:28:09.342455: Epoch 3
2025-10-15 05:28:09.342669: Current learning rate: 0.00982
2025-10-15 05:28:55.738584: Validation loss improved from -0.31696 to -0.32075! Patience: 0/50
2025-10-15 05:28:55.739004: train_loss -0.4159
2025-10-15 05:28:55.739192: val_loss -0.3207
2025-10-15 05:28:55.739315: Pseudo dice [np.float32(0.6326)]
2025-10-15 05:28:55.739475: Epoch time: 46.4 s
2025-10-15 05:28:55.739607: Yayy! New best EMA pseudo Dice: 0.5713000297546387
2025-10-15 05:28:56.801811: 
2025-10-15 05:28:56.802047: Epoch 4
2025-10-15 05:28:56.802255: Current learning rate: 0.00976
2025-10-15 05:29:43.188203: Validation loss did not improve from -0.32075. Patience: 1/50
2025-10-15 05:29:43.188806: train_loss -0.4584
2025-10-15 05:29:43.188993: val_loss -0.303
2025-10-15 05:29:43.189131: Pseudo dice [np.float32(0.6256)]
2025-10-15 05:29:43.189294: Epoch time: 46.39 s
2025-10-15 05:29:43.603959: Yayy! New best EMA pseudo Dice: 0.57669997215271
2025-10-15 05:29:44.703464: 
2025-10-15 05:29:44.703732: Epoch 5
2025-10-15 05:29:44.703956: Current learning rate: 0.0097
2025-10-15 05:30:31.175775: Validation loss improved from -0.32075 to -0.34036! Patience: 1/50
2025-10-15 05:30:31.176283: train_loss -0.4844
2025-10-15 05:30:31.176439: val_loss -0.3404
2025-10-15 05:30:31.176573: Pseudo dice [np.float32(0.6477)]
2025-10-15 05:30:31.176724: Epoch time: 46.47 s
2025-10-15 05:30:31.176877: Yayy! New best EMA pseudo Dice: 0.5838000178337097
2025-10-15 05:30:32.273077: 
2025-10-15 05:30:32.273440: Epoch 6
2025-10-15 05:30:32.273708: Current learning rate: 0.00964
2025-10-15 05:31:18.783191: Validation loss improved from -0.34036 to -0.35572! Patience: 0/50
2025-10-15 05:31:18.783745: train_loss -0.5004
2025-10-15 05:31:18.783895: val_loss -0.3557
2025-10-15 05:31:18.784030: Pseudo dice [np.float32(0.6576)]
2025-10-15 05:31:18.784157: Epoch time: 46.51 s
2025-10-15 05:31:18.784269: Yayy! New best EMA pseudo Dice: 0.5911999940872192
2025-10-15 05:31:19.858814: 
2025-10-15 05:31:19.859099: Epoch 7
2025-10-15 05:31:19.859404: Current learning rate: 0.00958
2025-10-15 05:32:06.362263: Validation loss did not improve from -0.35572. Patience: 1/50
2025-10-15 05:32:06.362900: train_loss -0.5274
2025-10-15 05:32:06.363081: val_loss -0.3277
2025-10-15 05:32:06.363237: Pseudo dice [np.float32(0.6536)]
2025-10-15 05:32:06.363380: Epoch time: 46.5 s
2025-10-15 05:32:06.363495: Yayy! New best EMA pseudo Dice: 0.5975000262260437
2025-10-15 05:32:07.477158: 
2025-10-15 05:32:07.477470: Epoch 8
2025-10-15 05:32:07.477662: Current learning rate: 0.00952
2025-10-15 05:32:53.888000: Validation loss improved from -0.35572 to -0.35844! Patience: 1/50
2025-10-15 05:32:53.889345: train_loss -0.5325
2025-10-15 05:32:53.889809: val_loss -0.3584
2025-10-15 05:32:53.890166: Pseudo dice [np.float32(0.6671)]
2025-10-15 05:32:53.890612: Epoch time: 46.41 s
2025-10-15 05:32:53.891090: Yayy! New best EMA pseudo Dice: 0.6043999791145325
2025-10-15 05:32:55.054162: 
2025-10-15 05:32:55.054533: Epoch 9
2025-10-15 05:32:55.054773: Current learning rate: 0.00946
2025-10-15 05:33:41.510605: Validation loss improved from -0.35844 to -0.39699! Patience: 0/50
2025-10-15 05:33:41.511435: train_loss -0.5467
2025-10-15 05:33:41.511693: val_loss -0.397
2025-10-15 05:33:41.511845: Pseudo dice [np.float32(0.686)]
2025-10-15 05:33:41.512054: Epoch time: 46.46 s
2025-10-15 05:33:41.968857: Yayy! New best EMA pseudo Dice: 0.6126000285148621
2025-10-15 05:33:43.026956: 
2025-10-15 05:33:43.027176: Epoch 10
2025-10-15 05:33:43.027401: Current learning rate: 0.0094
2025-10-15 05:34:29.590919: Validation loss did not improve from -0.39699. Patience: 1/50
2025-10-15 05:34:29.591456: train_loss -0.5503
2025-10-15 05:34:29.591614: val_loss -0.3806
2025-10-15 05:34:29.591795: Pseudo dice [np.float32(0.6711)]
2025-10-15 05:34:29.591986: Epoch time: 46.57 s
2025-10-15 05:34:29.592115: Yayy! New best EMA pseudo Dice: 0.618399977684021
2025-10-15 05:34:30.731952: 
2025-10-15 05:34:30.732232: Epoch 11
2025-10-15 05:34:30.732422: Current learning rate: 0.00934
2025-10-15 05:35:17.268803: Validation loss improved from -0.39699 to -0.42225! Patience: 1/50
2025-10-15 05:35:17.269300: train_loss -0.5507
2025-10-15 05:35:17.269454: val_loss -0.4223
2025-10-15 05:35:17.269562: Pseudo dice [np.float32(0.6916)]
2025-10-15 05:35:17.269704: Epoch time: 46.54 s
2025-10-15 05:35:17.269829: Yayy! New best EMA pseudo Dice: 0.6256999969482422
2025-10-15 05:35:18.733741: 
2025-10-15 05:35:18.734035: Epoch 12
2025-10-15 05:35:18.734256: Current learning rate: 0.00928
2025-10-15 05:36:05.278066: Validation loss did not improve from -0.42225. Patience: 1/50
2025-10-15 05:36:05.278723: train_loss -0.5696
2025-10-15 05:36:05.278947: val_loss -0.4187
2025-10-15 05:36:05.279117: Pseudo dice [np.float32(0.6773)]
2025-10-15 05:36:05.279260: Epoch time: 46.55 s
2025-10-15 05:36:05.279441: Yayy! New best EMA pseudo Dice: 0.6309000253677368
2025-10-15 05:36:06.408427: 
2025-10-15 05:36:06.408824: Epoch 13
2025-10-15 05:36:06.409106: Current learning rate: 0.00922
2025-10-15 05:36:52.879270: Validation loss improved from -0.42225 to -0.42929! Patience: 1/50
2025-10-15 05:36:52.879874: train_loss -0.5732
2025-10-15 05:36:52.880120: val_loss -0.4293
2025-10-15 05:36:52.880290: Pseudo dice [np.float32(0.6918)]
2025-10-15 05:36:52.880465: Epoch time: 46.47 s
2025-10-15 05:36:52.880701: Yayy! New best EMA pseudo Dice: 0.6370000243186951
2025-10-15 05:36:53.973059: 
2025-10-15 05:36:53.973425: Epoch 14
2025-10-15 05:36:53.973647: Current learning rate: 0.00916
2025-10-15 05:37:40.446472: Validation loss did not improve from -0.42929. Patience: 1/50
2025-10-15 05:37:40.447097: train_loss -0.5876
2025-10-15 05:37:40.447294: val_loss -0.4278
2025-10-15 05:37:40.447443: Pseudo dice [np.float32(0.6966)]
2025-10-15 05:37:40.447602: Epoch time: 46.47 s
2025-10-15 05:37:40.877888: Yayy! New best EMA pseudo Dice: 0.6428999900817871
2025-10-15 05:37:41.951168: 
2025-10-15 05:37:41.951596: Epoch 15
2025-10-15 05:37:41.951900: Current learning rate: 0.0091
2025-10-15 05:38:28.385825: Validation loss did not improve from -0.42929. Patience: 2/50
2025-10-15 05:38:28.386318: train_loss -0.5952
2025-10-15 05:38:28.386476: val_loss -0.4021
2025-10-15 05:38:28.386594: Pseudo dice [np.float32(0.6852)]
2025-10-15 05:38:28.386739: Epoch time: 46.44 s
2025-10-15 05:38:28.386903: Yayy! New best EMA pseudo Dice: 0.6471999883651733
2025-10-15 05:38:29.454070: 
2025-10-15 05:38:29.454318: Epoch 16
2025-10-15 05:38:29.454507: Current learning rate: 0.00903
2025-10-15 05:39:15.910555: Validation loss did not improve from -0.42929. Patience: 3/50
2025-10-15 05:39:15.911430: train_loss -0.5966
2025-10-15 05:39:15.911664: val_loss -0.3768
2025-10-15 05:39:15.911852: Pseudo dice [np.float32(0.6701)]
2025-10-15 05:39:15.912068: Epoch time: 46.46 s
2025-10-15 05:39:15.912275: Yayy! New best EMA pseudo Dice: 0.6495000123977661
2025-10-15 05:39:17.011276: 
2025-10-15 05:39:17.011639: Epoch 17
2025-10-15 05:39:17.011846: Current learning rate: 0.00897
2025-10-15 05:40:03.690256: Validation loss did not improve from -0.42929. Patience: 4/50
2025-10-15 05:40:03.690765: train_loss -0.6121
2025-10-15 05:40:03.690917: val_loss -0.4288
2025-10-15 05:40:03.691043: Pseudo dice [np.float32(0.6971)]
2025-10-15 05:40:03.691168: Epoch time: 46.68 s
2025-10-15 05:40:03.691272: Yayy! New best EMA pseudo Dice: 0.65420001745224
2025-10-15 05:40:04.773330: 
2025-10-15 05:40:04.773698: Epoch 18
2025-10-15 05:40:04.773911: Current learning rate: 0.00891
2025-10-15 05:40:51.319816: Validation loss improved from -0.42929 to -0.43307! Patience: 4/50
2025-10-15 05:40:51.320496: train_loss -0.6129
2025-10-15 05:40:51.320631: val_loss -0.4331
2025-10-15 05:40:51.320752: Pseudo dice [np.float32(0.7048)]
2025-10-15 05:40:51.320880: Epoch time: 46.55 s
2025-10-15 05:40:51.321005: Yayy! New best EMA pseudo Dice: 0.6593000292778015
2025-10-15 05:40:52.409208: 
2025-10-15 05:40:52.409593: Epoch 19
2025-10-15 05:40:52.409816: Current learning rate: 0.00885
2025-10-15 05:41:38.873758: Validation loss did not improve from -0.43307. Patience: 1/50
2025-10-15 05:41:38.874236: train_loss -0.6261
2025-10-15 05:41:38.874388: val_loss -0.4194
2025-10-15 05:41:38.874499: Pseudo dice [np.float32(0.6893)]
2025-10-15 05:41:38.874632: Epoch time: 46.47 s
2025-10-15 05:41:39.318804: Yayy! New best EMA pseudo Dice: 0.6622999906539917
2025-10-15 05:41:40.404346: 
2025-10-15 05:41:40.404567: Epoch 20
2025-10-15 05:41:40.404752: Current learning rate: 0.00879
2025-10-15 05:42:26.863826: Validation loss did not improve from -0.43307. Patience: 2/50
2025-10-15 05:42:26.864292: train_loss -0.6365
2025-10-15 05:42:26.864422: val_loss -0.426
2025-10-15 05:42:26.864553: Pseudo dice [np.float32(0.6979)]
2025-10-15 05:42:26.864678: Epoch time: 46.46 s
2025-10-15 05:42:26.864805: Yayy! New best EMA pseudo Dice: 0.6657999753952026
2025-10-15 05:42:27.948098: 
2025-10-15 05:42:27.948443: Epoch 21
2025-10-15 05:42:27.948657: Current learning rate: 0.00873
2025-10-15 05:43:14.409172: Validation loss did not improve from -0.43307. Patience: 3/50
2025-10-15 05:43:14.409669: train_loss -0.6363
2025-10-15 05:43:14.409836: val_loss -0.3916
2025-10-15 05:43:14.410027: Pseudo dice [np.float32(0.685)]
2025-10-15 05:43:14.410159: Epoch time: 46.46 s
2025-10-15 05:43:14.410298: Yayy! New best EMA pseudo Dice: 0.6678000092506409
2025-10-15 05:43:15.461145: 
2025-10-15 05:43:15.461462: Epoch 22
2025-10-15 05:43:15.461643: Current learning rate: 0.00867
2025-10-15 05:44:02.017049: Validation loss improved from -0.43307 to -0.44187! Patience: 3/50
2025-10-15 05:44:02.017690: train_loss -0.6304
2025-10-15 05:44:02.017859: val_loss -0.4419
2025-10-15 05:44:02.018024: Pseudo dice [np.float32(0.7058)]
2025-10-15 05:44:02.018189: Epoch time: 46.56 s
2025-10-15 05:44:02.018320: Yayy! New best EMA pseudo Dice: 0.6715999841690063
2025-10-15 05:44:03.108631: 
2025-10-15 05:44:03.108972: Epoch 23
2025-10-15 05:44:03.109168: Current learning rate: 0.00861
2025-10-15 05:44:49.744567: Validation loss improved from -0.44187 to -0.44331! Patience: 0/50
2025-10-15 05:44:49.745484: train_loss -0.6439
2025-10-15 05:44:49.745912: val_loss -0.4433
2025-10-15 05:44:49.746221: Pseudo dice [np.float32(0.7047)]
2025-10-15 05:44:49.746534: Epoch time: 46.64 s
2025-10-15 05:44:49.746821: Yayy! New best EMA pseudo Dice: 0.6748999953269958
2025-10-15 05:44:50.819006: 
2025-10-15 05:44:50.819282: Epoch 24
2025-10-15 05:44:50.819537: Current learning rate: 0.00855
2025-10-15 05:45:37.375632: Validation loss did not improve from -0.44331. Patience: 1/50
2025-10-15 05:45:37.376178: train_loss -0.6498
2025-10-15 05:45:37.376321: val_loss -0.3879
2025-10-15 05:45:37.376525: Pseudo dice [np.float32(0.6864)]
2025-10-15 05:45:37.376661: Epoch time: 46.56 s
2025-10-15 05:45:37.820991: Yayy! New best EMA pseudo Dice: 0.6759999990463257
2025-10-15 05:45:38.882555: 
2025-10-15 05:45:38.882889: Epoch 25
2025-10-15 05:45:38.883105: Current learning rate: 0.00849
2025-10-15 05:46:25.458726: Validation loss did not improve from -0.44331. Patience: 2/50
2025-10-15 05:46:25.459278: train_loss -0.6566
2025-10-15 05:46:25.459415: val_loss -0.4283
2025-10-15 05:46:25.459537: Pseudo dice [np.float32(0.6971)]
2025-10-15 05:46:25.459658: Epoch time: 46.58 s
2025-10-15 05:46:25.459762: Yayy! New best EMA pseudo Dice: 0.6780999898910522
2025-10-15 05:46:26.522698: 
2025-10-15 05:46:26.523060: Epoch 26
2025-10-15 05:46:26.523272: Current learning rate: 0.00843
2025-10-15 05:47:13.057767: Validation loss did not improve from -0.44331. Patience: 3/50
2025-10-15 05:47:13.058353: train_loss -0.6608
2025-10-15 05:47:13.058500: val_loss -0.3797
2025-10-15 05:47:13.058606: Pseudo dice [np.float32(0.686)]
2025-10-15 05:47:13.058807: Epoch time: 46.54 s
2025-10-15 05:47:13.058927: Yayy! New best EMA pseudo Dice: 0.6789000034332275
2025-10-15 05:47:14.531776: 
2025-10-15 05:47:14.532161: Epoch 27
2025-10-15 05:47:14.532410: Current learning rate: 0.00836
2025-10-15 05:48:01.026690: Validation loss improved from -0.44331 to -0.46531! Patience: 3/50
2025-10-15 05:48:01.027164: train_loss -0.67
2025-10-15 05:48:01.027373: val_loss -0.4653
2025-10-15 05:48:01.027488: Pseudo dice [np.float32(0.7101)]
2025-10-15 05:48:01.027638: Epoch time: 46.5 s
2025-10-15 05:48:01.027745: Yayy! New best EMA pseudo Dice: 0.6819999814033508
2025-10-15 05:48:02.092190: 
2025-10-15 05:48:02.092503: Epoch 28
2025-10-15 05:48:02.092685: Current learning rate: 0.0083
2025-10-15 05:48:48.640656: Validation loss did not improve from -0.46531. Patience: 1/50
2025-10-15 05:48:48.641692: train_loss -0.6578
2025-10-15 05:48:48.641943: val_loss -0.4256
2025-10-15 05:48:48.642113: Pseudo dice [np.float32(0.6992)]
2025-10-15 05:48:48.642274: Epoch time: 46.55 s
2025-10-15 05:48:48.642383: Yayy! New best EMA pseudo Dice: 0.6837999820709229
2025-10-15 05:48:49.736393: 
2025-10-15 05:48:49.736761: Epoch 29
2025-10-15 05:48:49.737031: Current learning rate: 0.00824
2025-10-15 05:49:36.350568: Validation loss did not improve from -0.46531. Patience: 2/50
2025-10-15 05:49:36.351106: train_loss -0.6658
2025-10-15 05:49:36.351290: val_loss -0.4222
2025-10-15 05:49:36.351415: Pseudo dice [np.float32(0.7053)]
2025-10-15 05:49:36.351583: Epoch time: 46.62 s
2025-10-15 05:49:36.781734: Yayy! New best EMA pseudo Dice: 0.6858999729156494
2025-10-15 05:49:37.879475: 
2025-10-15 05:49:37.879798: Epoch 30
2025-10-15 05:49:37.880079: Current learning rate: 0.00818
2025-10-15 05:50:24.448790: Validation loss did not improve from -0.46531. Patience: 3/50
2025-10-15 05:50:24.449402: train_loss -0.6759
2025-10-15 05:50:24.449579: val_loss -0.4343
2025-10-15 05:50:24.449751: Pseudo dice [np.float32(0.6985)]
2025-10-15 05:50:24.449896: Epoch time: 46.57 s
2025-10-15 05:50:24.450058: Yayy! New best EMA pseudo Dice: 0.6872000098228455
2025-10-15 05:50:25.544602: 
2025-10-15 05:50:25.544936: Epoch 31
2025-10-15 05:50:25.545170: Current learning rate: 0.00812
2025-10-15 05:51:12.039402: Validation loss did not improve from -0.46531. Patience: 4/50
2025-10-15 05:51:12.039987: train_loss -0.6765
2025-10-15 05:51:12.040231: val_loss -0.4118
2025-10-15 05:51:12.040358: Pseudo dice [np.float32(0.7012)]
2025-10-15 05:51:12.040494: Epoch time: 46.5 s
2025-10-15 05:51:12.040611: Yayy! New best EMA pseudo Dice: 0.6886000037193298
2025-10-15 05:51:13.122668: 
2025-10-15 05:51:13.123210: Epoch 32
2025-10-15 05:51:13.123712: Current learning rate: 0.00806
2025-10-15 05:51:59.738925: Validation loss did not improve from -0.46531. Patience: 5/50
2025-10-15 05:51:59.739542: train_loss -0.6855
2025-10-15 05:51:59.739709: val_loss -0.4224
2025-10-15 05:51:59.739909: Pseudo dice [np.float32(0.7007)]
2025-10-15 05:51:59.740053: Epoch time: 46.62 s
2025-10-15 05:51:59.740354: Yayy! New best EMA pseudo Dice: 0.6898000240325928
2025-10-15 05:52:00.859818: 
2025-10-15 05:52:00.860160: Epoch 33
2025-10-15 05:52:00.860367: Current learning rate: 0.008
2025-10-15 05:52:47.404293: Validation loss did not improve from -0.46531. Patience: 6/50
2025-10-15 05:52:47.404762: train_loss -0.6847
2025-10-15 05:52:47.405015: val_loss -0.4456
2025-10-15 05:52:47.405141: Pseudo dice [np.float32(0.7082)]
2025-10-15 05:52:47.405274: Epoch time: 46.55 s
2025-10-15 05:52:47.405379: Yayy! New best EMA pseudo Dice: 0.6916000247001648
2025-10-15 05:52:48.489619: 
2025-10-15 05:52:48.489949: Epoch 34
2025-10-15 05:52:48.490140: Current learning rate: 0.00793
2025-10-15 05:53:35.124638: Validation loss did not improve from -0.46531. Patience: 7/50
2025-10-15 05:53:35.125211: train_loss -0.6835
2025-10-15 05:53:35.125382: val_loss -0.4496
2025-10-15 05:53:35.125551: Pseudo dice [np.float32(0.722)]
2025-10-15 05:53:35.125713: Epoch time: 46.64 s
2025-10-15 05:53:35.592184: Yayy! New best EMA pseudo Dice: 0.6947000026702881
2025-10-15 05:53:36.683066: 
2025-10-15 05:53:36.683356: Epoch 35
2025-10-15 05:53:36.683536: Current learning rate: 0.00787
2025-10-15 05:54:23.276637: Validation loss did not improve from -0.46531. Patience: 8/50
2025-10-15 05:54:23.277491: train_loss -0.6872
2025-10-15 05:54:23.278054: val_loss -0.4253
2025-10-15 05:54:23.278379: Pseudo dice [np.float32(0.703)]
2025-10-15 05:54:23.278653: Epoch time: 46.6 s
2025-10-15 05:54:23.278958: Yayy! New best EMA pseudo Dice: 0.6955000162124634
2025-10-15 05:54:24.411635: 
2025-10-15 05:54:24.412112: Epoch 36
2025-10-15 05:54:24.412463: Current learning rate: 0.00781
2025-10-15 05:55:11.070636: Validation loss did not improve from -0.46531. Patience: 9/50
2025-10-15 05:55:11.071491: train_loss -0.6832
2025-10-15 05:55:11.071650: val_loss -0.4081
2025-10-15 05:55:11.071845: Pseudo dice [np.float32(0.6971)]
2025-10-15 05:55:11.072001: Epoch time: 46.66 s
2025-10-15 05:55:11.072178: Yayy! New best EMA pseudo Dice: 0.6956999897956848
2025-10-15 05:55:12.175172: 
2025-10-15 05:55:12.175582: Epoch 37
2025-10-15 05:55:12.175822: Current learning rate: 0.00775
2025-10-15 05:55:58.733177: Validation loss did not improve from -0.46531. Patience: 10/50
2025-10-15 05:55:58.733733: train_loss -0.6936
2025-10-15 05:55:58.733873: val_loss -0.432
2025-10-15 05:55:58.734036: Pseudo dice [np.float32(0.6983)]
2025-10-15 05:55:58.734195: Epoch time: 46.56 s
2025-10-15 05:55:58.734419: Yayy! New best EMA pseudo Dice: 0.695900022983551
2025-10-15 05:55:59.853516: 
2025-10-15 05:55:59.854052: Epoch 38
2025-10-15 05:55:59.854485: Current learning rate: 0.00769
2025-10-15 05:56:46.370998: Validation loss did not improve from -0.46531. Patience: 11/50
2025-10-15 05:56:46.371587: train_loss -0.6992
2025-10-15 05:56:46.371792: val_loss -0.4491
2025-10-15 05:56:46.371949: Pseudo dice [np.float32(0.7201)]
2025-10-15 05:56:46.372134: Epoch time: 46.52 s
2025-10-15 05:56:46.372258: Yayy! New best EMA pseudo Dice: 0.6983000040054321
2025-10-15 05:56:47.462985: 
2025-10-15 05:56:47.463222: Epoch 39
2025-10-15 05:56:47.463487: Current learning rate: 0.00763
2025-10-15 05:57:33.971440: Validation loss did not improve from -0.46531. Patience: 12/50
2025-10-15 05:57:33.972172: train_loss -0.7083
2025-10-15 05:57:33.972343: val_loss -0.4478
2025-10-15 05:57:33.972614: Pseudo dice [np.float32(0.7184)]
2025-10-15 05:57:33.972829: Epoch time: 46.51 s
2025-10-15 05:57:34.428948: Yayy! New best EMA pseudo Dice: 0.7002999782562256
2025-10-15 05:57:35.526369: 
2025-10-15 05:57:35.526733: Epoch 40
2025-10-15 05:57:35.526969: Current learning rate: 0.00756
2025-10-15 05:58:22.069248: Validation loss did not improve from -0.46531. Patience: 13/50
2025-10-15 05:58:22.070179: train_loss -0.7084
2025-10-15 05:58:22.070519: val_loss -0.4295
2025-10-15 05:58:22.070831: Pseudo dice [np.float32(0.7032)]
2025-10-15 05:58:22.071178: Epoch time: 46.54 s
2025-10-15 05:58:22.071461: Yayy! New best EMA pseudo Dice: 0.7006000280380249
2025-10-15 05:58:23.219130: 
2025-10-15 05:58:23.219462: Epoch 41
2025-10-15 05:58:23.219683: Current learning rate: 0.0075
2025-10-15 05:59:09.771748: Validation loss did not improve from -0.46531. Patience: 14/50
2025-10-15 05:59:09.772290: train_loss -0.7073
2025-10-15 05:59:09.772441: val_loss -0.435
2025-10-15 05:59:09.772575: Pseudo dice [np.float32(0.702)]
2025-10-15 05:59:09.772706: Epoch time: 46.55 s
2025-10-15 05:59:09.772816: Yayy! New best EMA pseudo Dice: 0.7008000016212463
2025-10-15 05:59:11.382021: 
2025-10-15 05:59:11.382310: Epoch 42
2025-10-15 05:59:11.382574: Current learning rate: 0.00744
2025-10-15 05:59:57.910432: Validation loss did not improve from -0.46531. Patience: 15/50
2025-10-15 05:59:57.911006: train_loss -0.7158
2025-10-15 05:59:57.911219: val_loss -0.4571
2025-10-15 05:59:57.911371: Pseudo dice [np.float32(0.7222)]
2025-10-15 05:59:57.911500: Epoch time: 46.53 s
2025-10-15 05:59:57.911637: Yayy! New best EMA pseudo Dice: 0.7028999924659729
2025-10-15 05:59:58.999358: 
2025-10-15 05:59:58.999672: Epoch 43
2025-10-15 05:59:58.999869: Current learning rate: 0.00738
2025-10-15 06:00:45.541205: Validation loss did not improve from -0.46531. Patience: 16/50
2025-10-15 06:00:45.541890: train_loss -0.714
2025-10-15 06:00:45.542068: val_loss -0.4241
2025-10-15 06:00:45.542224: Pseudo dice [np.float32(0.7018)]
2025-10-15 06:00:45.542426: Epoch time: 46.54 s
2025-10-15 06:00:46.161054: 
2025-10-15 06:00:46.161393: Epoch 44
2025-10-15 06:00:46.161596: Current learning rate: 0.00732
2025-10-15 06:01:32.756529: Validation loss did not improve from -0.46531. Patience: 17/50
2025-10-15 06:01:32.757190: train_loss -0.7144
2025-10-15 06:01:32.757331: val_loss -0.4569
2025-10-15 06:01:32.757443: Pseudo dice [np.float32(0.7211)]
2025-10-15 06:01:32.757572: Epoch time: 46.6 s
2025-10-15 06:01:33.197596: Yayy! New best EMA pseudo Dice: 0.7045999765396118
2025-10-15 06:01:34.292598: 
2025-10-15 06:01:34.292905: Epoch 45
2025-10-15 06:01:34.293180: Current learning rate: 0.00725
2025-10-15 06:02:20.978752: Validation loss did not improve from -0.46531. Patience: 18/50
2025-10-15 06:02:20.979471: train_loss -0.7213
2025-10-15 06:02:20.979812: val_loss -0.3991
2025-10-15 06:02:20.979994: Pseudo dice [np.float32(0.7052)]
2025-10-15 06:02:20.980196: Epoch time: 46.69 s
2025-10-15 06:02:20.980368: Yayy! New best EMA pseudo Dice: 0.7046999931335449
2025-10-15 06:02:22.091692: 
2025-10-15 06:02:22.091962: Epoch 46
2025-10-15 06:02:22.092178: Current learning rate: 0.00719
2025-10-15 06:03:08.749489: Validation loss did not improve from -0.46531. Patience: 19/50
2025-10-15 06:03:08.750147: train_loss -0.7228
2025-10-15 06:03:08.750478: val_loss -0.4062
2025-10-15 06:03:08.750718: Pseudo dice [np.float32(0.7085)]
2025-10-15 06:03:08.751021: Epoch time: 46.66 s
2025-10-15 06:03:08.751222: Yayy! New best EMA pseudo Dice: 0.7050999999046326
2025-10-15 06:03:09.831367: 
2025-10-15 06:03:09.831736: Epoch 47
2025-10-15 06:03:09.831958: Current learning rate: 0.00713
2025-10-15 06:03:56.433129: Validation loss did not improve from -0.46531. Patience: 20/50
2025-10-15 06:03:56.433754: train_loss -0.722
2025-10-15 06:03:56.433996: val_loss -0.4088
2025-10-15 06:03:56.434230: Pseudo dice [np.float32(0.7003)]
2025-10-15 06:03:56.434483: Epoch time: 46.6 s
2025-10-15 06:03:57.075545: 
2025-10-15 06:03:57.075866: Epoch 48
2025-10-15 06:03:57.076141: Current learning rate: 0.00707
2025-10-15 06:04:43.727566: Validation loss did not improve from -0.46531. Patience: 21/50
2025-10-15 06:04:43.728271: train_loss -0.7274
2025-10-15 06:04:43.728444: val_loss -0.4114
2025-10-15 06:04:43.728563: Pseudo dice [np.float32(0.7115)]
2025-10-15 06:04:43.728726: Epoch time: 46.65 s
2025-10-15 06:04:43.728840: Yayy! New best EMA pseudo Dice: 0.705299973487854
2025-10-15 06:04:44.815277: 
2025-10-15 06:04:44.815702: Epoch 49
2025-10-15 06:04:44.816020: Current learning rate: 0.007
2025-10-15 06:05:31.392763: Validation loss did not improve from -0.46531. Patience: 22/50
2025-10-15 06:05:31.393406: train_loss -0.7351
2025-10-15 06:05:31.393614: val_loss -0.4101
2025-10-15 06:05:31.393770: Pseudo dice [np.float32(0.7055)]
2025-10-15 06:05:31.394083: Epoch time: 46.58 s
2025-10-15 06:05:31.839023: Yayy! New best EMA pseudo Dice: 0.705299973487854
2025-10-15 06:05:32.942222: 
2025-10-15 06:05:32.942615: Epoch 50
2025-10-15 06:05:32.942879: Current learning rate: 0.00694
2025-10-15 06:06:19.520043: Validation loss did not improve from -0.46531. Patience: 23/50
2025-10-15 06:06:19.520829: train_loss -0.731
2025-10-15 06:06:19.520970: val_loss -0.4049
2025-10-15 06:06:19.521112: Pseudo dice [np.float32(0.7042)]
2025-10-15 06:06:19.521336: Epoch time: 46.58 s
2025-10-15 06:06:20.160352: 
2025-10-15 06:06:20.160696: Epoch 51
2025-10-15 06:06:20.161086: Current learning rate: 0.00688
2025-10-15 06:07:06.756103: Validation loss did not improve from -0.46531. Patience: 24/50
2025-10-15 06:07:06.756613: train_loss -0.7292
2025-10-15 06:07:06.756747: val_loss -0.4373
2025-10-15 06:07:06.756856: Pseudo dice [np.float32(0.7194)]
2025-10-15 06:07:06.756983: Epoch time: 46.6 s
2025-10-15 06:07:06.757152: Yayy! New best EMA pseudo Dice: 0.70660001039505
2025-10-15 06:07:07.819409: 
2025-10-15 06:07:07.819661: Epoch 52
2025-10-15 06:07:07.819868: Current learning rate: 0.00682
2025-10-15 06:07:54.378514: Validation loss did not improve from -0.46531. Patience: 25/50
2025-10-15 06:07:54.379209: train_loss -0.7303
2025-10-15 06:07:54.379411: val_loss -0.4325
2025-10-15 06:07:54.379530: Pseudo dice [np.float32(0.712)]
2025-10-15 06:07:54.379685: Epoch time: 46.56 s
2025-10-15 06:07:54.379818: Yayy! New best EMA pseudo Dice: 0.7071999907493591
2025-10-15 06:07:55.476631: 
2025-10-15 06:07:55.476917: Epoch 53
2025-10-15 06:07:55.477127: Current learning rate: 0.00675
2025-10-15 06:08:42.134783: Validation loss did not improve from -0.46531. Patience: 26/50
2025-10-15 06:08:42.135321: train_loss -0.7309
2025-10-15 06:08:42.135480: val_loss -0.4289
2025-10-15 06:08:42.135595: Pseudo dice [np.float32(0.7113)]
2025-10-15 06:08:42.135756: Epoch time: 46.66 s
2025-10-15 06:08:42.135866: Yayy! New best EMA pseudo Dice: 0.7075999975204468
2025-10-15 06:08:43.220452: 
2025-10-15 06:08:43.220812: Epoch 54
2025-10-15 06:08:43.221107: Current learning rate: 0.00669
2025-10-15 06:09:29.756194: Validation loss did not improve from -0.46531. Patience: 27/50
2025-10-15 06:09:29.756866: train_loss -0.7361
2025-10-15 06:09:29.757077: val_loss -0.4454
2025-10-15 06:09:29.757255: Pseudo dice [np.float32(0.7201)]
2025-10-15 06:09:29.757483: Epoch time: 46.54 s
2025-10-15 06:09:30.224072: Yayy! New best EMA pseudo Dice: 0.7088000178337097
2025-10-15 06:09:31.291046: 
2025-10-15 06:09:31.291439: Epoch 55
2025-10-15 06:09:31.291664: Current learning rate: 0.00663
2025-10-15 06:10:17.836038: Validation loss did not improve from -0.46531. Patience: 28/50
2025-10-15 06:10:17.836615: train_loss -0.7383
2025-10-15 06:10:17.836847: val_loss -0.4152
2025-10-15 06:10:17.836981: Pseudo dice [np.float32(0.7197)]
2025-10-15 06:10:17.837145: Epoch time: 46.55 s
2025-10-15 06:10:17.837262: Yayy! New best EMA pseudo Dice: 0.7099000215530396
2025-10-15 06:10:18.931842: 
2025-10-15 06:10:18.932193: Epoch 56
2025-10-15 06:10:18.932418: Current learning rate: 0.00657
2025-10-15 06:11:05.500541: Validation loss did not improve from -0.46531. Patience: 29/50
2025-10-15 06:11:05.501190: train_loss -0.7375
2025-10-15 06:11:05.501379: val_loss -0.425
2025-10-15 06:11:05.501521: Pseudo dice [np.float32(0.7157)]
2025-10-15 06:11:05.501661: Epoch time: 46.57 s
2025-10-15 06:11:05.501785: Yayy! New best EMA pseudo Dice: 0.7105000019073486
2025-10-15 06:11:06.592883: 
2025-10-15 06:11:06.593248: Epoch 57
2025-10-15 06:11:06.593469: Current learning rate: 0.0065
2025-10-15 06:11:53.204001: Validation loss did not improve from -0.46531. Patience: 30/50
2025-10-15 06:11:53.204626: train_loss -0.7396
2025-10-15 06:11:53.204784: val_loss -0.4462
2025-10-15 06:11:53.204960: Pseudo dice [np.float32(0.7153)]
2025-10-15 06:11:53.205141: Epoch time: 46.61 s
2025-10-15 06:11:53.205265: Yayy! New best EMA pseudo Dice: 0.7110000252723694
2025-10-15 06:11:54.807837: 
2025-10-15 06:11:54.808193: Epoch 58
2025-10-15 06:11:54.808450: Current learning rate: 0.00644
2025-10-15 06:12:41.373946: Validation loss did not improve from -0.46531. Patience: 31/50
2025-10-15 06:12:41.374627: train_loss -0.7434
2025-10-15 06:12:41.374783: val_loss -0.4375
2025-10-15 06:12:41.374894: Pseudo dice [np.float32(0.7165)]
2025-10-15 06:12:41.375037: Epoch time: 46.57 s
2025-10-15 06:12:41.375140: Yayy! New best EMA pseudo Dice: 0.7114999890327454
2025-10-15 06:12:42.469511: 
2025-10-15 06:12:42.469822: Epoch 59
2025-10-15 06:12:42.470046: Current learning rate: 0.00638
2025-10-15 06:13:29.096098: Validation loss did not improve from -0.46531. Patience: 32/50
2025-10-15 06:13:29.096763: train_loss -0.7452
2025-10-15 06:13:29.097055: val_loss -0.3995
2025-10-15 06:13:29.097248: Pseudo dice [np.float32(0.693)]
2025-10-15 06:13:29.097432: Epoch time: 46.63 s
2025-10-15 06:13:30.202032: 
2025-10-15 06:13:30.202424: Epoch 60
2025-10-15 06:13:30.202688: Current learning rate: 0.00631
2025-10-15 06:14:16.853991: Validation loss did not improve from -0.46531. Patience: 33/50
2025-10-15 06:14:16.854571: train_loss -0.7512
2025-10-15 06:14:16.854715: val_loss -0.4352
2025-10-15 06:14:16.854818: Pseudo dice [np.float32(0.7123)]
2025-10-15 06:14:16.854979: Epoch time: 46.65 s
2025-10-15 06:14:17.494557: 
2025-10-15 06:14:17.494912: Epoch 61
2025-10-15 06:14:17.495102: Current learning rate: 0.00625
2025-10-15 06:15:04.070131: Validation loss did not improve from -0.46531. Patience: 34/50
2025-10-15 06:15:04.070898: train_loss -0.7537
2025-10-15 06:15:04.071248: val_loss -0.4608
2025-10-15 06:15:04.071540: Pseudo dice [np.float32(0.7226)]
2025-10-15 06:15:04.071856: Epoch time: 46.58 s
2025-10-15 06:15:04.713543: 
2025-10-15 06:15:04.713829: Epoch 62
2025-10-15 06:15:04.714027: Current learning rate: 0.00619
2025-10-15 06:15:51.270364: Validation loss did not improve from -0.46531. Patience: 35/50
2025-10-15 06:15:51.271411: train_loss -0.7491
2025-10-15 06:15:51.271816: val_loss -0.382
2025-10-15 06:15:51.272134: Pseudo dice [np.float32(0.6955)]
2025-10-15 06:15:51.272463: Epoch time: 46.56 s
2025-10-15 06:15:51.920559: 
2025-10-15 06:15:51.920809: Epoch 63
2025-10-15 06:15:51.921007: Current learning rate: 0.00612
2025-10-15 06:16:38.430978: Validation loss did not improve from -0.46531. Patience: 36/50
2025-10-15 06:16:38.431481: train_loss -0.7483
2025-10-15 06:16:38.431625: val_loss -0.4536
2025-10-15 06:16:38.431776: Pseudo dice [np.float32(0.7169)]
2025-10-15 06:16:38.431900: Epoch time: 46.51 s
2025-10-15 06:16:39.079051: 
2025-10-15 06:16:39.079409: Epoch 64
2025-10-15 06:16:39.079628: Current learning rate: 0.00606
2025-10-15 06:17:25.610013: Validation loss did not improve from -0.46531. Patience: 37/50
2025-10-15 06:17:25.610594: train_loss -0.7557
2025-10-15 06:17:25.610749: val_loss -0.4442
2025-10-15 06:17:25.610890: Pseudo dice [np.float32(0.718)]
2025-10-15 06:17:25.611046: Epoch time: 46.53 s
2025-10-15 06:17:26.724279: 
2025-10-15 06:17:26.724680: Epoch 65
2025-10-15 06:17:26.724957: Current learning rate: 0.006
2025-10-15 06:18:13.366503: Validation loss did not improve from -0.46531. Patience: 38/50
2025-10-15 06:18:13.367057: train_loss -0.7556
2025-10-15 06:18:13.367236: val_loss -0.4164
2025-10-15 06:18:13.367383: Pseudo dice [np.float32(0.7212)]
2025-10-15 06:18:13.367638: Epoch time: 46.64 s
2025-10-15 06:18:13.367804: Yayy! New best EMA pseudo Dice: 0.7121000289916992
2025-10-15 06:18:14.523368: 
2025-10-15 06:18:14.523712: Epoch 66
2025-10-15 06:18:14.523970: Current learning rate: 0.00593
2025-10-15 06:19:01.121571: Validation loss did not improve from -0.46531. Patience: 39/50
2025-10-15 06:19:01.122072: train_loss -0.7594
2025-10-15 06:19:01.122234: val_loss -0.4495
2025-10-15 06:19:01.122416: Pseudo dice [np.float32(0.729)]
2025-10-15 06:19:01.122596: Epoch time: 46.6 s
2025-10-15 06:19:01.122724: Yayy! New best EMA pseudo Dice: 0.7138000130653381
2025-10-15 06:19:02.230768: 
2025-10-15 06:19:02.231093: Epoch 67
2025-10-15 06:19:02.231347: Current learning rate: 0.00587
2025-10-15 06:19:48.792724: Validation loss did not improve from -0.46531. Patience: 40/50
2025-10-15 06:19:48.793669: train_loss -0.7636
2025-10-15 06:19:48.793978: val_loss -0.4421
2025-10-15 06:19:48.794274: Pseudo dice [np.float32(0.7171)]
2025-10-15 06:19:48.794608: Epoch time: 46.56 s
2025-10-15 06:19:48.794900: Yayy! New best EMA pseudo Dice: 0.7141000032424927
2025-10-15 06:19:49.918024: 
2025-10-15 06:19:49.918373: Epoch 68
2025-10-15 06:19:49.918556: Current learning rate: 0.00581
2025-10-15 06:20:36.468469: Validation loss did not improve from -0.46531. Patience: 41/50
2025-10-15 06:20:36.469084: train_loss -0.7613
2025-10-15 06:20:36.469215: val_loss -0.4182
2025-10-15 06:20:36.469344: Pseudo dice [np.float32(0.7084)]
2025-10-15 06:20:36.469503: Epoch time: 46.55 s
2025-10-15 06:20:37.105384: 
2025-10-15 06:20:37.105672: Epoch 69
2025-10-15 06:20:37.105883: Current learning rate: 0.00574
2025-10-15 06:21:23.665942: Validation loss did not improve from -0.46531. Patience: 42/50
2025-10-15 06:21:23.666563: train_loss -0.7638
2025-10-15 06:21:23.666713: val_loss -0.4254
2025-10-15 06:21:23.666896: Pseudo dice [np.float32(0.7064)]
2025-10-15 06:21:23.667107: Epoch time: 46.56 s
2025-10-15 06:21:24.767123: 
2025-10-15 06:21:24.767476: Epoch 70
2025-10-15 06:21:24.767725: Current learning rate: 0.00568
2025-10-15 06:22:11.435663: Validation loss did not improve from -0.46531. Patience: 43/50
2025-10-15 06:22:11.436306: train_loss -0.7625
2025-10-15 06:22:11.436612: val_loss -0.4137
2025-10-15 06:22:11.436917: Pseudo dice [np.float32(0.7101)]
2025-10-15 06:22:11.437168: Epoch time: 46.67 s
2025-10-15 06:22:12.086750: 
2025-10-15 06:22:12.087051: Epoch 71
2025-10-15 06:22:12.087255: Current learning rate: 0.00562
2025-10-15 06:22:58.584696: Validation loss did not improve from -0.46531. Patience: 44/50
2025-10-15 06:22:58.585150: train_loss -0.7699
2025-10-15 06:22:58.585289: val_loss -0.4391
2025-10-15 06:22:58.585500: Pseudo dice [np.float32(0.7092)]
2025-10-15 06:22:58.585687: Epoch time: 46.5 s
2025-10-15 06:22:59.222171: 
2025-10-15 06:22:59.222664: Epoch 72
2025-10-15 06:22:59.223064: Current learning rate: 0.00555
2025-10-15 06:23:45.898962: Validation loss did not improve from -0.46531. Patience: 45/50
2025-10-15 06:23:45.899589: train_loss -0.7691
2025-10-15 06:23:45.899869: val_loss -0.4347
2025-10-15 06:23:45.900066: Pseudo dice [np.float32(0.7219)]
2025-10-15 06:23:45.900231: Epoch time: 46.68 s
2025-10-15 06:23:46.543441: 
2025-10-15 06:23:46.543784: Epoch 73
2025-10-15 06:23:46.544074: Current learning rate: 0.00549
2025-10-15 06:24:33.249497: Validation loss did not improve from -0.46531. Patience: 46/50
2025-10-15 06:24:33.250192: train_loss -0.7665
2025-10-15 06:24:33.250433: val_loss -0.4443
2025-10-15 06:24:33.250659: Pseudo dice [np.float32(0.7184)]
2025-10-15 06:24:33.250880: Epoch time: 46.71 s
2025-10-15 06:24:34.419818: 
2025-10-15 06:24:34.420273: Epoch 74
2025-10-15 06:24:34.420577: Current learning rate: 0.00542
2025-10-15 06:25:21.094583: Validation loss did not improve from -0.46531. Patience: 47/50
2025-10-15 06:25:21.095151: train_loss -0.7639
2025-10-15 06:25:21.095298: val_loss -0.4434
2025-10-15 06:25:21.095414: Pseudo dice [np.float32(0.7103)]
2025-10-15 06:25:21.095556: Epoch time: 46.68 s
2025-10-15 06:25:22.183677: 
2025-10-15 06:25:22.184015: Epoch 75
2025-10-15 06:25:22.184208: Current learning rate: 0.00536
2025-10-15 06:26:08.703364: Validation loss did not improve from -0.46531. Patience: 48/50
2025-10-15 06:26:08.703869: train_loss -0.7672
2025-10-15 06:26:08.704074: val_loss -0.4217
2025-10-15 06:26:08.704214: Pseudo dice [np.float32(0.7174)]
2025-10-15 06:26:08.704370: Epoch time: 46.52 s
2025-10-15 06:26:09.345596: 
2025-10-15 06:26:09.345948: Epoch 76
2025-10-15 06:26:09.346168: Current learning rate: 0.00529
2025-10-15 06:26:55.879169: Validation loss did not improve from -0.46531. Patience: 49/50
2025-10-15 06:26:55.879780: train_loss -0.7674
2025-10-15 06:26:55.879929: val_loss -0.4201
2025-10-15 06:26:55.880081: Pseudo dice [np.float32(0.715)]
2025-10-15 06:26:55.880208: Epoch time: 46.53 s
2025-10-15 06:26:56.512440: 
2025-10-15 06:26:56.512784: Epoch 77
2025-10-15 06:26:56.512980: Current learning rate: 0.00523
2025-10-15 06:27:43.053934: Validation loss did not improve from -0.46531. Patience: 50/50
2025-10-15 06:27:43.054443: train_loss -0.7708
2025-10-15 06:27:43.054601: val_loss -0.4269
2025-10-15 06:27:43.054765: Pseudo dice [np.float32(0.7283)]
2025-10-15 06:27:43.054901: Epoch time: 46.54 s
2025-10-15 06:27:43.055012: Yayy! New best EMA pseudo Dice: 0.7153000235557556
2025-10-15 06:27:44.192878: 
2025-10-15 06:27:44.193165: Epoch 78
2025-10-15 06:27:44.193365: Current learning rate: 0.00517
2025-10-15 06:28:30.823405: Validation loss did not improve from -0.46531. Patience: 51/50
2025-10-15 06:28:30.823999: train_loss -0.7738
2025-10-15 06:28:30.824177: val_loss -0.4088
2025-10-15 06:28:30.824286: Pseudo dice [np.float32(0.7052)]
2025-10-15 06:28:30.824430: Epoch time: 46.63 s
2025-10-15 06:28:31.480085: 
2025-10-15 06:28:31.480447: Epoch 79
2025-10-15 06:28:31.480638: Current learning rate: 0.0051
2025-10-15 06:29:17.979457: Validation loss did not improve from -0.46531. Patience: 52/50
2025-10-15 06:29:17.979877: train_loss -0.7711
2025-10-15 06:29:17.980060: val_loss -0.4395
2025-10-15 06:29:17.980179: Pseudo dice [np.float32(0.723)]
2025-10-15 06:29:17.980352: Epoch time: 46.5 s
2025-10-15 06:29:19.072507: 
2025-10-15 06:29:19.072858: Epoch 80
2025-10-15 06:29:19.073076: Current learning rate: 0.00504
2025-10-15 06:30:05.687052: Validation loss did not improve from -0.46531. Patience: 53/50
2025-10-15 06:30:05.687666: train_loss -0.7723
2025-10-15 06:30:05.687912: val_loss -0.4237
2025-10-15 06:30:05.688054: Pseudo dice [np.float32(0.7169)]
2025-10-15 06:30:05.688177: Epoch time: 46.62 s
2025-10-15 06:30:05.688288: Yayy! New best EMA pseudo Dice: 0.715399980545044
2025-10-15 06:30:06.789289: 
2025-10-15 06:30:06.789700: Epoch 81
2025-10-15 06:30:06.789996: Current learning rate: 0.00497
2025-10-15 06:30:53.440023: Validation loss did not improve from -0.46531. Patience: 54/50
2025-10-15 06:30:53.440504: train_loss -0.7762
2025-10-15 06:30:53.440670: val_loss -0.4511
2025-10-15 06:30:53.440862: Pseudo dice [np.float32(0.7265)]
2025-10-15 06:30:53.441014: Epoch time: 46.65 s
2025-10-15 06:30:53.441137: Yayy! New best EMA pseudo Dice: 0.7164999842643738
2025-10-15 06:30:54.535297: 
2025-10-15 06:30:54.535816: Epoch 82
2025-10-15 06:30:54.536144: Current learning rate: 0.00491
2025-10-15 06:31:41.160836: Validation loss did not improve from -0.46531. Patience: 55/50
2025-10-15 06:31:41.161433: train_loss -0.7778
2025-10-15 06:31:41.161596: val_loss -0.4329
2025-10-15 06:31:41.161776: Pseudo dice [np.float32(0.7216)]
2025-10-15 06:31:41.161941: Epoch time: 46.63 s
2025-10-15 06:31:41.162098: Yayy! New best EMA pseudo Dice: 0.7170000076293945
2025-10-15 06:31:42.252657: 
2025-10-15 06:31:42.252954: Epoch 83
2025-10-15 06:31:42.253213: Current learning rate: 0.00484
2025-10-15 06:32:28.860935: Validation loss did not improve from -0.46531. Patience: 56/50
2025-10-15 06:32:28.861515: train_loss -0.7767
2025-10-15 06:32:28.861665: val_loss -0.4319
2025-10-15 06:32:28.861791: Pseudo dice [np.float32(0.7203)]
2025-10-15 06:32:28.861935: Epoch time: 46.61 s
2025-10-15 06:32:28.862104: Yayy! New best EMA pseudo Dice: 0.7172999978065491
2025-10-15 06:32:29.960912: 
2025-10-15 06:32:29.961229: Epoch 84
2025-10-15 06:32:29.961490: Current learning rate: 0.00478
2025-10-15 06:33:16.544203: Validation loss did not improve from -0.46531. Patience: 57/50
2025-10-15 06:33:16.544981: train_loss -0.7812
2025-10-15 06:33:16.545383: val_loss -0.4335
2025-10-15 06:33:16.545681: Pseudo dice [np.float32(0.7161)]
2025-10-15 06:33:16.545994: Epoch time: 46.58 s
2025-10-15 06:33:17.611311: 
2025-10-15 06:33:17.611757: Epoch 85
2025-10-15 06:33:17.612041: Current learning rate: 0.00471
2025-10-15 06:34:04.132314: Validation loss did not improve from -0.46531. Patience: 58/50
2025-10-15 06:34:04.132841: train_loss -0.7778
2025-10-15 06:34:04.133002: val_loss -0.4393
2025-10-15 06:34:04.133184: Pseudo dice [np.float32(0.7217)]
2025-10-15 06:34:04.133309: Epoch time: 46.52 s
2025-10-15 06:34:04.133414: Yayy! New best EMA pseudo Dice: 0.7175999879837036
2025-10-15 06:34:05.215282: 
2025-10-15 06:34:05.215778: Epoch 86
2025-10-15 06:34:05.216153: Current learning rate: 0.00465
2025-10-15 06:34:51.745904: Validation loss did not improve from -0.46531. Patience: 59/50
2025-10-15 06:34:51.746589: train_loss -0.7805
2025-10-15 06:34:51.746801: val_loss -0.4314
2025-10-15 06:34:51.747010: Pseudo dice [np.float32(0.7093)]
2025-10-15 06:34:51.747260: Epoch time: 46.53 s
2025-10-15 06:34:52.381422: 
2025-10-15 06:34:52.381936: Epoch 87
2025-10-15 06:34:52.382309: Current learning rate: 0.00458
2025-10-15 06:35:38.868097: Validation loss did not improve from -0.46531. Patience: 60/50
2025-10-15 06:35:38.868619: train_loss -0.7814
2025-10-15 06:35:38.868773: val_loss -0.414
2025-10-15 06:35:38.868945: Pseudo dice [np.float32(0.7121)]
2025-10-15 06:35:38.869103: Epoch time: 46.49 s
2025-10-15 06:35:39.499287: 
2025-10-15 06:35:39.499624: Epoch 88
2025-10-15 06:35:39.499822: Current learning rate: 0.00452
2025-10-15 06:36:26.011033: Validation loss did not improve from -0.46531. Patience: 61/50
2025-10-15 06:36:26.011670: train_loss -0.7838
2025-10-15 06:36:26.011855: val_loss -0.4335
2025-10-15 06:36:26.011991: Pseudo dice [np.float32(0.7142)]
2025-10-15 06:36:26.012135: Epoch time: 46.51 s
2025-10-15 06:36:27.162143: 
2025-10-15 06:36:27.162501: Epoch 89
2025-10-15 06:36:27.162711: Current learning rate: 0.00445
2025-10-15 06:37:13.760293: Validation loss did not improve from -0.46531. Patience: 62/50
2025-10-15 06:37:13.760919: train_loss -0.7837
2025-10-15 06:37:13.761141: val_loss -0.4183
2025-10-15 06:37:13.761303: Pseudo dice [np.float32(0.7189)]
2025-10-15 06:37:13.761508: Epoch time: 46.6 s
2025-10-15 06:37:14.859039: 
2025-10-15 06:37:14.859408: Epoch 90
2025-10-15 06:37:14.859646: Current learning rate: 0.00438
2025-10-15 06:38:01.437726: Validation loss did not improve from -0.46531. Patience: 63/50
2025-10-15 06:38:01.438472: train_loss -0.7843
2025-10-15 06:38:01.438866: val_loss -0.4089
2025-10-15 06:38:01.439162: Pseudo dice [np.float32(0.7186)]
2025-10-15 06:38:01.439521: Epoch time: 46.58 s
2025-10-15 06:38:02.079294: 
2025-10-15 06:38:02.079831: Epoch 91
2025-10-15 06:38:02.080196: Current learning rate: 0.00432
2025-10-15 06:38:48.623754: Validation loss did not improve from -0.46531. Patience: 64/50
2025-10-15 06:38:48.624188: train_loss -0.7828
2025-10-15 06:38:48.624329: val_loss -0.4318
2025-10-15 06:38:48.624465: Pseudo dice [np.float32(0.7081)]
2025-10-15 06:38:48.624619: Epoch time: 46.55 s
2025-10-15 06:38:49.256938: 
2025-10-15 06:38:49.257217: Epoch 92
2025-10-15 06:38:49.257402: Current learning rate: 0.00425
2025-10-15 06:39:35.808238: Validation loss did not improve from -0.46531. Patience: 65/50
2025-10-15 06:39:35.809226: train_loss -0.7851
2025-10-15 06:39:35.809510: val_loss -0.4012
2025-10-15 06:39:35.809806: Pseudo dice [np.float32(0.7014)]
2025-10-15 06:39:35.810084: Epoch time: 46.55 s
2025-10-15 06:39:36.444437: 
2025-10-15 06:39:36.444702: Epoch 93
2025-10-15 06:39:36.444915: Current learning rate: 0.00419
2025-10-15 06:40:23.061860: Validation loss did not improve from -0.46531. Patience: 66/50
2025-10-15 06:40:23.062623: train_loss -0.7842
2025-10-15 06:40:23.062974: val_loss -0.4418
2025-10-15 06:40:23.063261: Pseudo dice [np.float32(0.7184)]
2025-10-15 06:40:23.063556: Epoch time: 46.62 s
2025-10-15 06:40:23.696834: 
2025-10-15 06:40:23.697065: Epoch 94
2025-10-15 06:40:23.697247: Current learning rate: 0.00412
2025-10-15 06:41:10.201334: Validation loss did not improve from -0.46531. Patience: 67/50
2025-10-15 06:41:10.201892: train_loss -0.784
2025-10-15 06:41:10.202063: val_loss -0.4236
2025-10-15 06:41:10.202206: Pseudo dice [np.float32(0.7086)]
2025-10-15 06:41:10.202357: Epoch time: 46.51 s
2025-10-15 06:41:11.334780: 
2025-10-15 06:41:11.335151: Epoch 95
2025-10-15 06:41:11.335517: Current learning rate: 0.00405
2025-10-15 06:41:57.923007: Validation loss did not improve from -0.46531. Patience: 68/50
2025-10-15 06:41:57.923416: train_loss -0.7853
2025-10-15 06:41:57.923575: val_loss -0.4307
2025-10-15 06:41:57.923706: Pseudo dice [np.float32(0.7228)]
2025-10-15 06:41:57.923874: Epoch time: 46.59 s
2025-10-15 06:41:58.551310: 
2025-10-15 06:41:58.551568: Epoch 96
2025-10-15 06:41:58.551747: Current learning rate: 0.00399
2025-10-15 06:42:45.139814: Validation loss did not improve from -0.46531. Patience: 69/50
2025-10-15 06:42:45.140299: train_loss -0.7906
2025-10-15 06:42:45.140434: val_loss -0.432
2025-10-15 06:42:45.140562: Pseudo dice [np.float32(0.7202)]
2025-10-15 06:42:45.140685: Epoch time: 46.59 s
2025-10-15 06:42:45.774800: 
2025-10-15 06:42:45.775131: Epoch 97
2025-10-15 06:42:45.775336: Current learning rate: 0.00392
2025-10-15 06:43:32.257566: Validation loss did not improve from -0.46531. Patience: 70/50
2025-10-15 06:43:32.258138: train_loss -0.7878
2025-10-15 06:43:32.258279: val_loss -0.4227
2025-10-15 06:43:32.258399: Pseudo dice [np.float32(0.7207)]
2025-10-15 06:43:32.258574: Epoch time: 46.48 s
2025-10-15 06:43:32.891103: 
2025-10-15 06:43:32.891374: Epoch 98
2025-10-15 06:43:32.891556: Current learning rate: 0.00385
2025-10-15 06:44:19.377210: Validation loss did not improve from -0.46531. Patience: 71/50
2025-10-15 06:44:19.377749: train_loss -0.79
2025-10-15 06:44:19.377936: val_loss -0.4378
2025-10-15 06:44:19.378056: Pseudo dice [np.float32(0.7311)]
2025-10-15 06:44:19.378190: Epoch time: 46.49 s
2025-10-15 06:44:20.011352: 
2025-10-15 06:44:20.011673: Epoch 99
2025-10-15 06:44:20.011924: Current learning rate: 0.00379
2025-10-15 06:45:06.580879: Validation loss did not improve from -0.46531. Patience: 72/50
2025-10-15 06:45:06.581531: train_loss -0.7919
2025-10-15 06:45:06.581748: val_loss -0.4604
2025-10-15 06:45:06.581960: Pseudo dice [np.float32(0.7273)]
2025-10-15 06:45:06.582183: Epoch time: 46.57 s
2025-10-15 06:45:07.013809: Yayy! New best EMA pseudo Dice: 0.718500018119812
2025-10-15 06:45:08.110749: 
2025-10-15 06:45:08.111156: Epoch 100
2025-10-15 06:45:08.111415: Current learning rate: 0.00372
2025-10-15 06:45:54.759058: Validation loss did not improve from -0.46531. Patience: 73/50
2025-10-15 06:45:54.760162: train_loss -0.7914
2025-10-15 06:45:54.760515: val_loss -0.3957
2025-10-15 06:45:54.760870: Pseudo dice [np.float32(0.7095)]
2025-10-15 06:45:54.761168: Epoch time: 46.65 s
2025-10-15 06:45:55.411277: 
2025-10-15 06:45:55.411606: Epoch 101
2025-10-15 06:45:55.411803: Current learning rate: 0.00365
2025-10-15 06:46:42.081227: Validation loss did not improve from -0.46531. Patience: 74/50
2025-10-15 06:46:42.081770: train_loss -0.7906
2025-10-15 06:46:42.082119: val_loss -0.4352
2025-10-15 06:46:42.082281: Pseudo dice [np.float32(0.72)]
2025-10-15 06:46:42.082546: Epoch time: 46.67 s
2025-10-15 06:46:42.729349: 
2025-10-15 06:46:42.729770: Epoch 102
2025-10-15 06:46:42.730013: Current learning rate: 0.00359
2025-10-15 06:47:29.351585: Validation loss did not improve from -0.46531. Patience: 75/50
2025-10-15 06:47:29.352222: train_loss -0.7917
2025-10-15 06:47:29.352368: val_loss -0.4347
2025-10-15 06:47:29.352479: Pseudo dice [np.float32(0.725)]
2025-10-15 06:47:29.352650: Epoch time: 46.62 s
2025-10-15 06:47:29.352806: Yayy! New best EMA pseudo Dice: 0.7185999751091003
2025-10-15 06:47:30.457414: 
2025-10-15 06:47:30.457706: Epoch 103
2025-10-15 06:47:30.457934: Current learning rate: 0.00352
2025-10-15 06:48:17.165638: Validation loss did not improve from -0.46531. Patience: 76/50
2025-10-15 06:48:17.166487: train_loss -0.7938
2025-10-15 06:48:17.166817: val_loss -0.4041
2025-10-15 06:48:17.167174: Pseudo dice [np.float32(0.7127)]
2025-10-15 06:48:17.167508: Epoch time: 46.71 s
2025-10-15 06:48:17.810379: 
2025-10-15 06:48:17.810884: Epoch 104
2025-10-15 06:48:17.811222: Current learning rate: 0.00345
2025-10-15 06:49:04.591876: Validation loss did not improve from -0.46531. Patience: 77/50
2025-10-15 06:49:04.592728: train_loss -0.7932
2025-10-15 06:49:04.592901: val_loss -0.4158
2025-10-15 06:49:04.593095: Pseudo dice [np.float32(0.709)]
2025-10-15 06:49:04.593300: Epoch time: 46.78 s
2025-10-15 06:49:06.252100: 
2025-10-15 06:49:06.252395: Epoch 105
2025-10-15 06:49:06.252617: Current learning rate: 0.00338
2025-10-15 06:49:52.806351: Validation loss did not improve from -0.46531. Patience: 78/50
2025-10-15 06:49:52.806937: train_loss -0.7926
2025-10-15 06:49:52.807080: val_loss -0.4185
2025-10-15 06:49:52.807190: Pseudo dice [np.float32(0.7118)]
2025-10-15 06:49:52.807338: Epoch time: 46.56 s
2025-10-15 06:49:53.453987: 
2025-10-15 06:49:53.454247: Epoch 106
2025-10-15 06:49:53.454422: Current learning rate: 0.00332
2025-10-15 06:50:40.062284: Validation loss did not improve from -0.46531. Patience: 79/50
2025-10-15 06:50:40.063014: train_loss -0.7944
2025-10-15 06:50:40.063275: val_loss -0.4321
2025-10-15 06:50:40.063463: Pseudo dice [np.float32(0.7095)]
2025-10-15 06:50:40.063672: Epoch time: 46.61 s
2025-10-15 06:50:40.708512: 
2025-10-15 06:50:40.708906: Epoch 107
2025-10-15 06:50:40.709260: Current learning rate: 0.00325
2025-10-15 06:51:27.333983: Validation loss did not improve from -0.46531. Patience: 80/50
2025-10-15 06:51:27.334462: train_loss -0.7938
2025-10-15 06:51:27.334643: val_loss -0.4348
2025-10-15 06:51:27.334824: Pseudo dice [np.float32(0.7209)]
2025-10-15 06:51:27.334981: Epoch time: 46.63 s
2025-10-15 06:51:27.971333: 
2025-10-15 06:51:27.971610: Epoch 108
2025-10-15 06:51:27.971798: Current learning rate: 0.00318
2025-10-15 06:52:14.625158: Validation loss did not improve from -0.46531. Patience: 81/50
2025-10-15 06:52:14.626001: train_loss -0.7986
2025-10-15 06:52:14.626350: val_loss -0.3954
2025-10-15 06:52:14.626607: Pseudo dice [np.float32(0.7098)]
2025-10-15 06:52:14.626795: Epoch time: 46.66 s
2025-10-15 06:52:15.271830: 
2025-10-15 06:52:15.272208: Epoch 109
2025-10-15 06:52:15.272593: Current learning rate: 0.00311
2025-10-15 06:53:01.841998: Validation loss did not improve from -0.46531. Patience: 82/50
2025-10-15 06:53:01.842576: train_loss -0.797
2025-10-15 06:53:01.842737: val_loss -0.3901
2025-10-15 06:53:01.842894: Pseudo dice [np.float32(0.7141)]
2025-10-15 06:53:01.843102: Epoch time: 46.57 s
2025-10-15 06:53:02.941321: 
2025-10-15 06:53:02.941761: Epoch 110
2025-10-15 06:53:02.942094: Current learning rate: 0.00304
2025-10-15 06:53:49.454365: Validation loss did not improve from -0.46531. Patience: 83/50
2025-10-15 06:53:49.455026: train_loss -0.7997
2025-10-15 06:53:49.455206: val_loss -0.4124
2025-10-15 06:53:49.455355: Pseudo dice [np.float32(0.7107)]
2025-10-15 06:53:49.455481: Epoch time: 46.51 s
2025-10-15 06:53:50.090580: 
2025-10-15 06:53:50.090905: Epoch 111
2025-10-15 06:53:50.091120: Current learning rate: 0.00297
2025-10-15 06:54:36.552096: Validation loss did not improve from -0.46531. Patience: 84/50
2025-10-15 06:54:36.552598: train_loss -0.8003
2025-10-15 06:54:36.552813: val_loss -0.4556
2025-10-15 06:54:36.552947: Pseudo dice [np.float32(0.7226)]
2025-10-15 06:54:36.553121: Epoch time: 46.46 s
2025-10-15 06:54:37.197175: 
2025-10-15 06:54:37.197405: Epoch 112
2025-10-15 06:54:37.197594: Current learning rate: 0.00291
2025-10-15 06:55:23.755854: Validation loss did not improve from -0.46531. Patience: 85/50
2025-10-15 06:55:23.756476: train_loss -0.7994
2025-10-15 06:55:23.756680: val_loss -0.4161
2025-10-15 06:55:23.756840: Pseudo dice [np.float32(0.7159)]
2025-10-15 06:55:23.757019: Epoch time: 46.56 s
2025-10-15 06:55:24.392571: 
2025-10-15 06:55:24.392912: Epoch 113
2025-10-15 06:55:24.393135: Current learning rate: 0.00284
2025-10-15 06:56:11.135228: Validation loss did not improve from -0.46531. Patience: 86/50
2025-10-15 06:56:11.135831: train_loss -0.8
2025-10-15 06:56:11.136037: val_loss -0.4129
2025-10-15 06:56:11.136204: Pseudo dice [np.float32(0.7079)]
2025-10-15 06:56:11.136338: Epoch time: 46.74 s
2025-10-15 06:56:11.778363: 
2025-10-15 06:56:11.778657: Epoch 114
2025-10-15 06:56:11.778848: Current learning rate: 0.00277
2025-10-15 06:56:58.417067: Validation loss did not improve from -0.46531. Patience: 87/50
2025-10-15 06:56:58.417624: train_loss -0.8005
2025-10-15 06:56:58.417817: val_loss -0.415
2025-10-15 06:56:58.417969: Pseudo dice [np.float32(0.7194)]
2025-10-15 06:56:58.418130: Epoch time: 46.64 s
2025-10-15 06:56:59.513360: 
2025-10-15 06:56:59.513711: Epoch 115
2025-10-15 06:56:59.513928: Current learning rate: 0.0027
2025-10-15 06:57:46.112750: Validation loss did not improve from -0.46531. Patience: 88/50
2025-10-15 06:57:46.113165: train_loss -0.8016
2025-10-15 06:57:46.113298: val_loss -0.4387
2025-10-15 06:57:46.113407: Pseudo dice [np.float32(0.7259)]
2025-10-15 06:57:46.113556: Epoch time: 46.6 s
2025-10-15 06:57:46.754427: 
2025-10-15 06:57:46.754643: Epoch 116
2025-10-15 06:57:46.754844: Current learning rate: 0.00263
2025-10-15 06:58:33.371302: Validation loss did not improve from -0.46531. Patience: 89/50
2025-10-15 06:58:33.371865: train_loss -0.8033
2025-10-15 06:58:33.372003: val_loss -0.4355
2025-10-15 06:58:33.372120: Pseudo dice [np.float32(0.7289)]
2025-10-15 06:58:33.372247: Epoch time: 46.62 s
2025-10-15 06:58:34.010390: 
2025-10-15 06:58:34.010725: Epoch 117
2025-10-15 06:58:34.010933: Current learning rate: 0.00256
2025-10-15 06:59:20.465726: Validation loss did not improve from -0.46531. Patience: 90/50
2025-10-15 06:59:20.466189: train_loss -0.8022
2025-10-15 06:59:20.466345: val_loss -0.4423
2025-10-15 06:59:20.466472: Pseudo dice [np.float32(0.7258)]
2025-10-15 06:59:20.466632: Epoch time: 46.46 s
2025-10-15 06:59:21.104623: 
2025-10-15 06:59:21.104959: Epoch 118
2025-10-15 06:59:21.105153: Current learning rate: 0.00249
2025-10-15 07:00:07.632104: Validation loss did not improve from -0.46531. Patience: 91/50
2025-10-15 07:00:07.632731: train_loss -0.8021
2025-10-15 07:00:07.632887: val_loss -0.3955
2025-10-15 07:00:07.633017: Pseudo dice [np.float32(0.7094)]
2025-10-15 07:00:07.633158: Epoch time: 46.53 s
2025-10-15 07:00:08.276857: 
2025-10-15 07:00:08.277205: Epoch 119
2025-10-15 07:00:08.277396: Current learning rate: 0.00242
2025-10-15 07:00:54.921094: Validation loss did not improve from -0.46531. Patience: 92/50
2025-10-15 07:00:54.921662: train_loss -0.8041
2025-10-15 07:00:54.921820: val_loss -0.4189
2025-10-15 07:00:54.921967: Pseudo dice [np.float32(0.7155)]
2025-10-15 07:00:54.922170: Epoch time: 46.65 s
2025-10-15 07:00:56.562576: 
2025-10-15 07:00:56.563085: Epoch 120
2025-10-15 07:00:56.563472: Current learning rate: 0.00235
2025-10-15 07:01:43.144248: Validation loss did not improve from -0.46531. Patience: 93/50
2025-10-15 07:01:43.144859: train_loss -0.807
2025-10-15 07:01:43.145011: val_loss -0.444
2025-10-15 07:01:43.145199: Pseudo dice [np.float32(0.7286)]
2025-10-15 07:01:43.145334: Epoch time: 46.58 s
2025-10-15 07:01:43.794477: 
2025-10-15 07:01:43.794836: Epoch 121
2025-10-15 07:01:43.795028: Current learning rate: 0.00228
2025-10-15 07:02:30.365302: Validation loss did not improve from -0.46531. Patience: 94/50
2025-10-15 07:02:30.365811: train_loss -0.8067
2025-10-15 07:02:30.365957: val_loss -0.4055
2025-10-15 07:02:30.366084: Pseudo dice [np.float32(0.7116)]
2025-10-15 07:02:30.366216: Epoch time: 46.57 s
2025-10-15 07:02:31.009324: 
2025-10-15 07:02:31.009907: Epoch 122
2025-10-15 07:02:31.010108: Current learning rate: 0.00221
2025-10-15 07:03:17.540975: Validation loss did not improve from -0.46531. Patience: 95/50
2025-10-15 07:03:17.541498: train_loss -0.8061
2025-10-15 07:03:17.541686: val_loss -0.4176
2025-10-15 07:03:17.541860: Pseudo dice [np.float32(0.717)]
2025-10-15 07:03:17.542038: Epoch time: 46.53 s
2025-10-15 07:03:18.184189: 
2025-10-15 07:03:18.184552: Epoch 123
2025-10-15 07:03:18.184759: Current learning rate: 0.00214
2025-10-15 07:04:04.793229: Validation loss did not improve from -0.46531. Patience: 96/50
2025-10-15 07:04:04.793724: train_loss -0.8086
2025-10-15 07:04:04.793904: val_loss -0.4181
2025-10-15 07:04:04.794069: Pseudo dice [np.float32(0.7203)]
2025-10-15 07:04:04.794222: Epoch time: 46.61 s
2025-10-15 07:04:05.440506: 
2025-10-15 07:04:05.440836: Epoch 124
2025-10-15 07:04:05.441066: Current learning rate: 0.00207
2025-10-15 07:04:52.124398: Validation loss did not improve from -0.46531. Patience: 97/50
2025-10-15 07:04:52.125179: train_loss -0.8067
2025-10-15 07:04:52.125432: val_loss -0.4461
2025-10-15 07:04:52.125640: Pseudo dice [np.float32(0.7265)]
2025-10-15 07:04:52.126033: Epoch time: 46.69 s
2025-10-15 07:04:52.565622: Yayy! New best EMA pseudo Dice: 0.7189000248908997
2025-10-15 07:04:53.669478: 
2025-10-15 07:04:53.669731: Epoch 125
2025-10-15 07:04:53.669912: Current learning rate: 0.00199
2025-10-15 07:05:40.254282: Validation loss did not improve from -0.46531. Patience: 98/50
2025-10-15 07:05:40.254813: train_loss -0.8067
2025-10-15 07:05:40.254964: val_loss -0.4381
2025-10-15 07:05:40.255076: Pseudo dice [np.float32(0.7245)]
2025-10-15 07:05:40.255198: Epoch time: 46.59 s
2025-10-15 07:05:40.255336: Yayy! New best EMA pseudo Dice: 0.7193999886512756
2025-10-15 07:05:41.366825: 
2025-10-15 07:05:41.367099: Epoch 126
2025-10-15 07:05:41.367280: Current learning rate: 0.00192
2025-10-15 07:06:27.984569: Validation loss did not improve from -0.46531. Patience: 99/50
2025-10-15 07:06:27.985204: train_loss -0.8096
2025-10-15 07:06:27.985480: val_loss -0.4277
2025-10-15 07:06:27.985699: Pseudo dice [np.float32(0.714)]
2025-10-15 07:06:27.985878: Epoch time: 46.62 s
2025-10-15 07:06:28.636063: 
2025-10-15 07:06:28.636322: Epoch 127
2025-10-15 07:06:28.636560: Current learning rate: 0.00185
2025-10-15 07:07:15.117535: Validation loss did not improve from -0.46531. Patience: 100/50
2025-10-15 07:07:15.118012: train_loss -0.808
2025-10-15 07:07:15.118154: val_loss -0.441
2025-10-15 07:07:15.118270: Pseudo dice [np.float32(0.7229)]
2025-10-15 07:07:15.118395: Epoch time: 46.48 s
2025-10-15 07:07:15.759387: 
2025-10-15 07:07:15.759725: Epoch 128
2025-10-15 07:07:15.759972: Current learning rate: 0.00178
2025-10-15 07:08:02.267555: Validation loss did not improve from -0.46531. Patience: 101/50
2025-10-15 07:08:02.268108: train_loss -0.8071
2025-10-15 07:08:02.268303: val_loss -0.4551
2025-10-15 07:08:02.268419: Pseudo dice [np.float32(0.7375)]
2025-10-15 07:08:02.268550: Epoch time: 46.51 s
2025-10-15 07:08:02.268732: Yayy! New best EMA pseudo Dice: 0.7210999727249146
2025-10-15 07:08:03.379838: 
2025-10-15 07:08:03.380262: Epoch 129
2025-10-15 07:08:03.380475: Current learning rate: 0.0017
2025-10-15 07:08:49.953534: Validation loss did not improve from -0.46531. Patience: 102/50
2025-10-15 07:08:49.954097: train_loss -0.8095
2025-10-15 07:08:49.954244: val_loss -0.4104
2025-10-15 07:08:49.954387: Pseudo dice [np.float32(0.7022)]
2025-10-15 07:08:49.954553: Epoch time: 46.57 s
2025-10-15 07:08:51.046096: 
2025-10-15 07:08:51.046427: Epoch 130
2025-10-15 07:08:51.046724: Current learning rate: 0.00163
2025-10-15 07:09:37.578369: Validation loss did not improve from -0.46531. Patience: 103/50
2025-10-15 07:09:37.579235: train_loss -0.8108
2025-10-15 07:09:37.579595: val_loss -0.428
2025-10-15 07:09:37.579915: Pseudo dice [np.float32(0.7226)]
2025-10-15 07:09:37.580346: Epoch time: 46.53 s
2025-10-15 07:09:38.225116: 
2025-10-15 07:09:38.225482: Epoch 131
2025-10-15 07:09:38.225728: Current learning rate: 0.00156
2025-10-15 07:10:24.722645: Validation loss did not improve from -0.46531. Patience: 104/50
2025-10-15 07:10:24.723113: train_loss -0.8112
2025-10-15 07:10:24.723294: val_loss -0.4372
2025-10-15 07:10:24.723420: Pseudo dice [np.float32(0.7181)]
2025-10-15 07:10:24.723725: Epoch time: 46.5 s
2025-10-15 07:10:25.353815: 
2025-10-15 07:10:25.354146: Epoch 132
2025-10-15 07:10:25.354422: Current learning rate: 0.00148
2025-10-15 07:11:11.755305: Validation loss did not improve from -0.46531. Patience: 105/50
2025-10-15 07:11:11.756083: train_loss -0.8115
2025-10-15 07:11:11.756442: val_loss -0.4103
2025-10-15 07:11:11.756649: Pseudo dice [np.float32(0.713)]
2025-10-15 07:11:11.756816: Epoch time: 46.4 s
2025-10-15 07:11:12.393009: 
2025-10-15 07:11:12.393385: Epoch 133
2025-10-15 07:11:12.393573: Current learning rate: 0.00141
2025-10-15 07:11:58.827304: Validation loss did not improve from -0.46531. Patience: 106/50
2025-10-15 07:11:58.827782: train_loss -0.8136
2025-10-15 07:11:58.827950: val_loss -0.4138
2025-10-15 07:11:58.828071: Pseudo dice [np.float32(0.7115)]
2025-10-15 07:11:58.828202: Epoch time: 46.44 s
2025-10-15 07:11:59.462013: 
2025-10-15 07:11:59.462347: Epoch 134
2025-10-15 07:11:59.462523: Current learning rate: 0.00133
2025-10-15 07:12:45.939242: Validation loss did not improve from -0.46531. Patience: 107/50
2025-10-15 07:12:45.939876: train_loss -0.8118
2025-10-15 07:12:45.940078: val_loss -0.4395
2025-10-15 07:12:45.940209: Pseudo dice [np.float32(0.7203)]
2025-10-15 07:12:45.940370: Epoch time: 46.48 s
2025-10-15 07:12:47.542519: 
2025-10-15 07:12:47.542924: Epoch 135
2025-10-15 07:12:47.543236: Current learning rate: 0.00126
2025-10-15 07:13:34.165190: Validation loss did not improve from -0.46531. Patience: 108/50
2025-10-15 07:13:34.165757: train_loss -0.8119
2025-10-15 07:13:34.165897: val_loss -0.4415
2025-10-15 07:13:34.166020: Pseudo dice [np.float32(0.7329)]
2025-10-15 07:13:34.166155: Epoch time: 46.62 s
2025-10-15 07:13:34.829537: 
2025-10-15 07:13:34.829900: Epoch 136
2025-10-15 07:13:34.830107: Current learning rate: 0.00118
2025-10-15 07:14:21.449953: Validation loss did not improve from -0.46531. Patience: 109/50
2025-10-15 07:14:21.450673: train_loss -0.8142
2025-10-15 07:14:21.450819: val_loss -0.426
2025-10-15 07:14:21.451011: Pseudo dice [np.float32(0.7178)]
2025-10-15 07:14:21.451171: Epoch time: 46.62 s
2025-10-15 07:14:22.100050: 
2025-10-15 07:14:22.100520: Epoch 137
2025-10-15 07:14:22.100809: Current learning rate: 0.00111
2025-10-15 07:15:08.687488: Validation loss did not improve from -0.46531. Patience: 110/50
2025-10-15 07:15:08.688276: train_loss -0.8135
2025-10-15 07:15:08.688782: val_loss -0.4163
2025-10-15 07:15:08.689173: Pseudo dice [np.float32(0.7097)]
2025-10-15 07:15:08.689432: Epoch time: 46.59 s
2025-10-15 07:15:09.357746: 
2025-10-15 07:15:09.358024: Epoch 138
2025-10-15 07:15:09.358213: Current learning rate: 0.00103
2025-10-15 07:15:55.975854: Validation loss did not improve from -0.46531. Patience: 111/50
2025-10-15 07:15:55.976608: train_loss -0.8148
2025-10-15 07:15:55.976843: val_loss -0.4498
2025-10-15 07:15:55.977043: Pseudo dice [np.float32(0.7288)]
2025-10-15 07:15:55.977347: Epoch time: 46.62 s
2025-10-15 07:15:56.632702: 
2025-10-15 07:15:56.633135: Epoch 139
2025-10-15 07:15:56.633421: Current learning rate: 0.00095
2025-10-15 07:16:43.259065: Validation loss did not improve from -0.46531. Patience: 112/50
2025-10-15 07:16:43.259573: train_loss -0.8167
2025-10-15 07:16:43.259744: val_loss -0.4186
2025-10-15 07:16:43.259896: Pseudo dice [np.float32(0.7153)]
2025-10-15 07:16:43.260074: Epoch time: 46.63 s
2025-10-15 07:16:44.354080: 
2025-10-15 07:16:44.354332: Epoch 140
2025-10-15 07:16:44.354557: Current learning rate: 0.00087
2025-10-15 07:17:30.935363: Validation loss did not improve from -0.46531. Patience: 113/50
2025-10-15 07:17:30.936117: train_loss -0.8132
2025-10-15 07:17:30.936361: val_loss -0.4223
2025-10-15 07:17:30.936520: Pseudo dice [np.float32(0.7109)]
2025-10-15 07:17:30.936751: Epoch time: 46.58 s
2025-10-15 07:17:31.596839: 
2025-10-15 07:17:31.597220: Epoch 141
2025-10-15 07:17:31.597414: Current learning rate: 0.00079
2025-10-15 07:18:18.173111: Validation loss did not improve from -0.46531. Patience: 114/50
2025-10-15 07:18:18.173756: train_loss -0.8128
2025-10-15 07:18:18.174124: val_loss -0.4357
2025-10-15 07:18:18.174471: Pseudo dice [np.float32(0.7206)]
2025-10-15 07:18:18.174705: Epoch time: 46.58 s
2025-10-15 07:18:18.827033: 
2025-10-15 07:18:18.827547: Epoch 142
2025-10-15 07:18:18.827854: Current learning rate: 0.00071
2025-10-15 07:19:05.457433: Validation loss did not improve from -0.46531. Patience: 115/50
2025-10-15 07:19:05.457996: train_loss -0.812
2025-10-15 07:19:05.458139: val_loss -0.4281
2025-10-15 07:19:05.458266: Pseudo dice [np.float32(0.7121)]
2025-10-15 07:19:05.458442: Epoch time: 46.63 s
2025-10-15 07:19:06.100470: 
2025-10-15 07:19:06.100825: Epoch 143
2025-10-15 07:19:06.101056: Current learning rate: 0.00063
2025-10-15 07:19:52.683188: Validation loss did not improve from -0.46531. Patience: 116/50
2025-10-15 07:19:52.683635: train_loss -0.8132
2025-10-15 07:19:52.683766: val_loss -0.3959
2025-10-15 07:19:52.683892: Pseudo dice [np.float32(0.718)]
2025-10-15 07:19:52.684182: Epoch time: 46.58 s
2025-10-15 07:19:53.326606: 
2025-10-15 07:19:53.326839: Epoch 144
2025-10-15 07:19:53.327021: Current learning rate: 0.00055
2025-10-15 07:20:39.902733: Validation loss did not improve from -0.46531. Patience: 117/50
2025-10-15 07:20:39.903490: train_loss -0.8168
2025-10-15 07:20:39.903701: val_loss -0.4306
2025-10-15 07:20:39.903826: Pseudo dice [np.float32(0.7181)]
2025-10-15 07:20:39.904008: Epoch time: 46.58 s
2025-10-15 07:20:40.999248: 
2025-10-15 07:20:40.999574: Epoch 145
2025-10-15 07:20:40.999787: Current learning rate: 0.00047
2025-10-15 07:21:27.479841: Validation loss did not improve from -0.46531. Patience: 118/50
2025-10-15 07:21:27.480390: train_loss -0.8144
2025-10-15 07:21:27.480558: val_loss -0.4146
2025-10-15 07:21:27.480706: Pseudo dice [np.float32(0.7155)]
2025-10-15 07:21:27.480866: Epoch time: 46.48 s
2025-10-15 07:21:28.139027: 
2025-10-15 07:21:28.139406: Epoch 146
2025-10-15 07:21:28.139617: Current learning rate: 0.00038
2025-10-15 07:22:14.636033: Validation loss did not improve from -0.46531. Patience: 119/50
2025-10-15 07:22:14.636616: train_loss -0.8185
2025-10-15 07:22:14.636792: val_loss -0.4122
2025-10-15 07:22:14.636926: Pseudo dice [np.float32(0.719)]
2025-10-15 07:22:14.637071: Epoch time: 46.5 s
2025-10-15 07:22:15.280834: 
2025-10-15 07:22:15.281171: Epoch 147
2025-10-15 07:22:15.281393: Current learning rate: 0.0003
2025-10-15 07:23:01.764856: Validation loss did not improve from -0.46531. Patience: 120/50
2025-10-15 07:23:01.765543: train_loss -0.8182
2025-10-15 07:23:01.765800: val_loss -0.4324
2025-10-15 07:23:01.765952: Pseudo dice [np.float32(0.7131)]
2025-10-15 07:23:01.766156: Epoch time: 46.49 s
2025-10-15 07:23:02.421437: 
2025-10-15 07:23:02.421700: Epoch 148
2025-10-15 07:23:02.421951: Current learning rate: 0.00021
2025-10-15 07:23:48.871405: Validation loss did not improve from -0.46531. Patience: 121/50
2025-10-15 07:23:48.872021: train_loss -0.815
2025-10-15 07:23:48.872200: val_loss -0.4111
2025-10-15 07:23:48.872338: Pseudo dice [np.float32(0.7139)]
2025-10-15 07:23:48.872463: Epoch time: 46.45 s
2025-10-15 07:23:49.515583: 
2025-10-15 07:23:49.515907: Epoch 149
2025-10-15 07:23:49.516098: Current learning rate: 0.00011
2025-10-15 07:24:35.973030: Validation loss did not improve from -0.46531. Patience: 122/50
2025-10-15 07:24:35.973497: train_loss -0.8169
2025-10-15 07:24:35.973731: val_loss -0.4214
2025-10-15 07:24:35.973855: Pseudo dice [np.float32(0.7175)]
2025-10-15 07:24:35.974014: Epoch time: 46.46 s
2025-10-15 07:24:37.503674: Training done.
2025-10-15 07:24:37.549288: Using splits from existing split file: /nfs/erelab001/shared/Computational_Group/Naravich/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset307_Sohee_Calcium_OCT_CrossValidation/splits_final_60.json
2025-10-15 07:24:37.549913: The split file contains 5 splits.
2025-10-15 07:24:37.550279: Desired fold for training: 3
2025-10-15 07:24:37.550617: This split has 4 training and 5 validation cases.
2025-10-15 07:24:37.551054: predicting 101-045
2025-10-15 07:24:37.555871: 101-045, shape torch.Size([1, 375, 498, 498]), rank 0
2025-10-15 07:25:24.952040: predicting 106-002
2025-10-15 07:25:24.962034: 106-002, shape torch.Size([1, 539, 498, 498]), rank 0
2025-10-15 07:26:14.336989: predicting 401-004
2025-10-15 07:26:14.347260: 401-004, shape torch.Size([1, 375, 498, 498]), rank 0
2025-10-15 07:26:49.126238: predicting 704-003
2025-10-15 07:26:49.137207: 704-003, shape torch.Size([1, 375, 498, 498]), rank 0
2025-10-15 07:27:23.886227: predicting 706-005
2025-10-15 07:27:23.898034: 706-005, shape torch.Size([1, 375, 498, 498]), rank 0
2025-10-15 07:28:12.191729: Validation complete
2025-10-15 07:28:12.192083: Mean Validation Dice:  0.6958932969574037
Finished training fold 3 saving to /nfs/erelab001/shared/Computational_Group/Naravich/datasets/nnUNet_Datasets/nnUNet_results/Dataset307_Sohee_Calcium_OCT_CrossValidation/nnUNetTrainerScaleAnalysis60__nnUNetPlans__3d_32x160x128_b10/fold_3_Genesis_Pretrained
