/home/gridsan/nchutisilp/.conda/envs/nnunet/bin/python
Starting training with CONFIG=3d_32x160x128_b10, DATASET_ID=308, TRAINER=nnUNetTrainerScaleAnalysis40

############################
INFO: You are using the old nnU-Net default plans. We have updated our recommendations. Please consider using those instead! Read more here: https://github.com/MIC-DKFZ/nnUNet/blob/master/documentation/resenc_presets.md
############################

Using device: cuda:0

#######################################################################
Please cite the following paper when using nnU-Net:
Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.
#######################################################################

2024-12-19 21:31:55.236286: Using torch.compile...

############################
INFO: You are using the old nnU-Net default plans. We have updated our recommendations. Please consider using those instead! Read more here: https://github.com/MIC-DKFZ/nnUNet/blob/master/documentation/resenc_presets.md
############################

Using device: cuda:0

#######################################################################
Please cite the following paper when using nnU-Net:
Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.
#######################################################################

2024-12-19 21:31:55.238691: Using torch.compile...
/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
################### Loading pretrained weights from file  /home/gridsan/nchutisilp/projects/OpenAI-CLIP/clip_pretrained_nnUNet.pt ###################
Below is the list of overlapping blocks in pretrained model and nnUNet architecture:
encoder.stages.0.0.convs.0.conv.weight shape torch.Size([32, 1, 3, 3, 3])
encoder.stages.0.0.convs.0.conv.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.norm.weight shape torch.Size([32])
encoder.stages.0.0.convs.0.norm.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.0.weight shape torch.Size([32, 1, 3, 3, 3])
encoder.stages.0.0.convs.0.all_modules.0.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.1.weight shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.1.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
encoder.stages.0.0.convs.1.conv.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.norm.weight shape torch.Size([32])
encoder.stages.0.0.convs.1.norm.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
encoder.stages.0.0.convs.1.all_modules.0.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.1.weight shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.1.bias shape torch.Size([32])
encoder.stages.1.0.convs.0.conv.weight shape torch.Size([64, 32, 3, 3, 3])
encoder.stages.1.0.convs.0.conv.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.norm.weight shape torch.Size([64])
encoder.stages.1.0.convs.0.norm.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.0.weight shape torch.Size([64, 32, 3, 3, 3])
encoder.stages.1.0.convs.0.all_modules.0.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.1.weight shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.1.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
encoder.stages.1.0.convs.1.conv.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.norm.weight shape torch.Size([64])
encoder.stages.1.0.convs.1.norm.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
encoder.stages.1.0.convs.1.all_modules.0.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.1.weight shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.1.bias shape torch.Size([64])
encoder.stages.2.0.convs.0.conv.weight shape torch.Size([128, 64, 3, 3, 3])
encoder.stages.2.0.convs.0.conv.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.norm.weight shape torch.Size([128])
encoder.stages.2.0.convs.0.norm.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.0.weight shape torch.Size([128, 64, 3, 3, 3])
encoder.stages.2.0.convs.0.all_modules.0.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.1.weight shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.1.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
encoder.stages.2.0.convs.1.conv.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.norm.weight shape torch.Size([128])
encoder.stages.2.0.convs.1.norm.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
encoder.stages.2.0.convs.1.all_modules.0.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.1.weight shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.1.bias shape torch.Size([128])
encoder.stages.3.0.convs.0.conv.weight shape torch.Size([256, 128, 3, 3, 3])
encoder.stages.3.0.convs.0.conv.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.norm.weight shape torch.Size([256])
encoder.stages.3.0.convs.0.norm.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.0.weight shape torch.Size([256, 128, 3, 3, 3])
encoder.stages.3.0.convs.0.all_modules.0.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.1.weight shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.1.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
encoder.stages.3.0.convs.1.conv.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.norm.weight shape torch.Size([256])
encoder.stages.3.0.convs.1.norm.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
encoder.stages.3.0.convs.1.all_modules.0.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.1.weight shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.1.bias shape torch.Size([256])
encoder.stages.4.0.convs.0.conv.weight shape torch.Size([320, 256, 3, 3, 3])
encoder.stages.4.0.convs.0.conv.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.norm.weight shape torch.Size([320])
encoder.stages.4.0.convs.0.norm.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.0.weight shape torch.Size([320, 256, 3, 3, 3])
encoder.stages.4.0.convs.0.all_modules.0.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.1.weight shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.1.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.4.0.convs.1.conv.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.norm.weight shape torch.Size([320])
encoder.stages.4.0.convs.1.norm.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.4.0.convs.1.all_modules.0.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.1.weight shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.1.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.0.conv.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.norm.weight shape torch.Size([320])
encoder.stages.5.0.convs.0.norm.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.0.all_modules.0.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.1.weight shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.1.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.1.conv.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.norm.weight shape torch.Size([320])
encoder.stages.5.0.convs.1.norm.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.1.all_modules.0.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.1.weight shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.0.0.convs.0.conv.weight shape torch.Size([32, 1, 3, 3, 3])
decoder.encoder.stages.0.0.convs.0.conv.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.norm.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.norm.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.0.weight shape torch.Size([32, 1, 3, 3, 3])
decoder.encoder.stages.0.0.convs.0.all_modules.0.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.1.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.1.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.encoder.stages.0.0.convs.1.conv.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.norm.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.norm.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.encoder.stages.0.0.convs.1.all_modules.0.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.1.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.1.bias shape torch.Size([32])
decoder.encoder.stages.1.0.convs.0.conv.weight shape torch.Size([64, 32, 3, 3, 3])
decoder.encoder.stages.1.0.convs.0.conv.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.norm.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.norm.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.0.weight shape torch.Size([64, 32, 3, 3, 3])
decoder.encoder.stages.1.0.convs.0.all_modules.0.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.1.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.1.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.encoder.stages.1.0.convs.1.conv.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.norm.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.norm.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.encoder.stages.1.0.convs.1.all_modules.0.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.1.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.1.bias shape torch.Size([64])
decoder.encoder.stages.2.0.convs.0.conv.weight shape torch.Size([128, 64, 3, 3, 3])
decoder.encoder.stages.2.0.convs.0.conv.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.norm.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.norm.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.0.weight shape torch.Size([128, 64, 3, 3, 3])
decoder.encoder.stages.2.0.convs.0.all_modules.0.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.1.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.1.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.encoder.stages.2.0.convs.1.conv.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.norm.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.norm.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.encoder.stages.2.0.convs.1.all_modules.0.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.1.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.1.bias shape torch.Size([128])
decoder.encoder.stages.3.0.convs.0.conv.weight shape torch.Size([256, 128, 3, 3, 3])
decoder.encoder.stages.3.0.convs.0.conv.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.norm.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.norm.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.0.weight shape torch.Size([256, 128, 3, 3, 3])
decoder.encoder.stages.3.0.convs.0.all_modules.0.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.1.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.1.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.encoder.stages.3.0.convs.1.conv.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.norm.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.norm.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.encoder.stages.3.0.convs.1.all_modules.0.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.1.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.1.bias shape torch.Size([256])
decoder.encoder.stages.4.0.convs.0.conv.weight shape torch.Size([320, 256, 3, 3, 3])
decoder.encoder.stages.4.0.convs.0.conv.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.norm.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.norm.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.0.weight shape torch.Size([320, 256, 3, 3, 3])
decoder.encoder.stages.4.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.4.0.convs.1.conv.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.norm.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.norm.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.4.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.0.conv.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.norm.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.norm.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.1.conv.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.norm.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.norm.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.stages.0.convs.0.conv.weight shape torch.Size([320, 640, 3, 3, 3])
decoder.stages.0.convs.0.conv.bias shape torch.Size([320])
decoder.stages.0.convs.0.norm.weight shape torch.Size([320])
decoder.stages.0.convs.0.norm.bias shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.0.weight shape torch.Size([320, 640, 3, 3, 3])
decoder.stages.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.stages.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.stages.0.convs.1.conv.bias shape torch.Size([320])
decoder.stages.0.convs.1.norm.weight shape torch.Size([320])
decoder.stages.0.convs.1.norm.bias shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.stages.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.stages.1.convs.0.conv.weight shape torch.Size([256, 512, 3, 3, 3])
decoder.stages.1.convs.0.conv.bias shape torch.Size([256])
decoder.stages.1.convs.0.norm.weight shape torch.Size([256])
decoder.stages.1.convs.0.norm.bias shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.0.weight shape torch.Size([256, 512, 3, 3, 3])
decoder.stages.1.convs.0.all_modules.0.bias shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.1.weight shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.1.bias shape torch.Size([256])
decoder.stages.1.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.stages.1.convs.1.conv.bias shape torch.Size([256])
decoder.stages.1.convs.1.norm.weight shape torch.Size([256])
decoder.stages.1.convs.1.norm.bias shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.stages.1.convs.1.all_modules.0.bias shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.1.weight shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.1.bias shape torch.Size([256])
decoder.stages.2.convs.0.conv.weight shape torch.Size([128, 256, 3, 3, 3])
decoder.stages.2.convs.0.conv.bias shape torch.Size([128])
decoder.stages.2.convs.0.norm.weight shape torch.Size([128])
decoder.stages.2.convs.0.norm.bias shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.0.weight shape torch.Size([128, 256, 3, 3, 3])
decoder.stages.2.convs.0.all_modules.0.bias shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.1.weight shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.1.bias shape torch.Size([128])
decoder.stages.2.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.stages.2.convs.1.conv.bias shape torch.Size([128])
decoder.stages.2.convs.1.norm.weight shape torch.Size([128])
decoder.stages.2.convs.1.norm.bias shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.stages.2.convs.1.all_modules.0.bias shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.1.weight shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.1.bias shape torch.Size([128])
decoder.stages.3.convs.0.conv.weight shape torch.Size([64, 128, 3, 3, 3])
decoder.stages.3.convs.0.conv.bias shape torch.Size([64])
decoder.stages.3.convs.0.norm.weight shape torch.Size([64])
decoder.stages.3.convs.0.norm.bias shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.0.weight shape torch.Size([64, 128, 3, 3, 3])
decoder.stages.3.convs.0.all_modules.0.bias shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.1.weight shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.1.bias shape torch.Size([64])
decoder.stages.3.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.stages.3.convs.1.conv.bias shape torch.Size([64])
decoder.stages.3.convs.1.norm.weight shape torch.Size([64])
decoder.stages.3.convs.1.norm.bias shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.stages.3.convs.1.all_modules.0.bias shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.1.weight shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.1.bias shape torch.Size([64])
decoder.stages.4.convs.0.conv.weight shape torch.Size([32, 64, 3, 3, 3])
decoder.stages.4.convs.0.conv.bias shape torch.Size([32])
decoder.stages.4.convs.0.norm.weight shape torch.Size([32])
decoder.stages.4.convs.0.norm.bias shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.0.weight shape torch.Size([32, 64, 3, 3, 3])
decoder.stages.4.convs.0.all_modules.0.bias shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.1.weight shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.1.bias shape torch.Size([32])
decoder.stages.4.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.stages.4.convs.1.conv.bias shape torch.Size([32])
decoder.stages.4.convs.1.norm.weight shape torch.Size([32])
decoder.stages.4.convs.1.norm.bias shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.stages.4.convs.1.all_modules.0.bias shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.1.weight shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.1.bias shape torch.Size([32])
decoder.transpconvs.0.weight shape torch.Size([320, 320, 1, 2, 2])
decoder.transpconvs.0.bias shape torch.Size([320])
decoder.transpconvs.1.weight shape torch.Size([320, 256, 2, 2, 2])
decoder.transpconvs.1.bias shape torch.Size([256])
decoder.transpconvs.2.weight shape torch.Size([256, 128, 2, 2, 2])
decoder.transpconvs.2.bias shape torch.Size([128])
decoder.transpconvs.3.weight shape torch.Size([128, 64, 2, 2, 2])
decoder.transpconvs.3.bias shape torch.Size([64])
decoder.transpconvs.4.weight shape torch.Size([64, 32, 2, 2, 2])
decoder.transpconvs.4.bias shape torch.Size([32])
################### Done ###################
2024-12-19 21:32:04.522946: do_dummy_2d_data_aug: True
2024-12-19 21:32:04.542190: Using splits from existing split file: /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset308_Sohee_Calcium_OCT_CrossValidation/splits_final_40.json
2024-12-19 21:32:04.552300: The split file contains 5 splits.
2024-12-19 21:32:04.554032: Desired fold for training: 1
2024-12-19 21:32:04.555126: This split has 3 training and 6 validation cases.
################### Loading pretrained weights from file  /home/gridsan/nchutisilp/projects/OpenAI-CLIP/clip_pretrained_nnUNet.pt ###################
Below is the list of overlapping blocks in pretrained model and nnUNet architecture:
encoder.stages.0.0.convs.0.conv.weight shape torch.Size([32, 1, 3, 3, 3])
encoder.stages.0.0.convs.0.conv.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.norm.weight shape torch.Size([32])
encoder.stages.0.0.convs.0.norm.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.0.weight shape torch.Size([32, 1, 3, 3, 3])
encoder.stages.0.0.convs.0.all_modules.0.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.1.weight shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.1.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
encoder.stages.0.0.convs.1.conv.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.norm.weight shape torch.Size([32])
encoder.stages.0.0.convs.1.norm.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
encoder.stages.0.0.convs.1.all_modules.0.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.1.weight shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.1.bias shape torch.Size([32])
encoder.stages.1.0.convs.0.conv.weight shape torch.Size([64, 32, 3, 3, 3])
encoder.stages.1.0.convs.0.conv.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.norm.weight shape torch.Size([64])
encoder.stages.1.0.convs.0.norm.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.0.weight shape torch.Size([64, 32, 3, 3, 3])
encoder.stages.1.0.convs.0.all_modules.0.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.1.weight shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.1.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
encoder.stages.1.0.convs.1.conv.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.norm.weight shape torch.Size([64])
encoder.stages.1.0.convs.1.norm.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
encoder.stages.1.0.convs.1.all_modules.0.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.1.weight shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.1.bias shape torch.Size([64])
encoder.stages.2.0.convs.0.conv.weight shape torch.Size([128, 64, 3, 3, 3])
encoder.stages.2.0.convs.0.conv.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.norm.weight shape torch.Size([128])
encoder.stages.2.0.convs.0.norm.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.0.weight shape torch.Size([128, 64, 3, 3, 3])
encoder.stages.2.0.convs.0.all_modules.0.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.1.weight shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.1.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
encoder.stages.2.0.convs.1.conv.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.norm.weight shape torch.Size([128])
encoder.stages.2.0.convs.1.norm.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
encoder.stages.2.0.convs.1.all_modules.0.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.1.weight shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.1.bias shape torch.Size([128])
encoder.stages.3.0.convs.0.conv.weight shape torch.Size([256, 128, 3, 3, 3])
encoder.stages.3.0.convs.0.conv.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.norm.weight shape torch.Size([256])
encoder.stages.3.0.convs.0.norm.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.0.weight shape torch.Size([256, 128, 3, 3, 3])
encoder.stages.3.0.convs.0.all_modules.0.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.1.weight shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.1.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
encoder.stages.3.0.convs.1.conv.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.norm.weight shape torch.Size([256])
encoder.stages.3.0.convs.1.norm.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
encoder.stages.3.0.convs.1.all_modules.0.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.1.weight shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.1.bias shape torch.Size([256])
encoder.stages.4.0.convs.0.conv.weight shape torch.Size([320, 256, 3, 3, 3])
encoder.stages.4.0.convs.0.conv.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.norm.weight shape torch.Size([320])
encoder.stages.4.0.convs.0.norm.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.0.weight shape torch.Size([320, 256, 3, 3, 3])
encoder.stages.4.0.convs.0.all_modules.0.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.1.weight shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.1.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.4.0.convs.1.conv.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.norm.weight shape torch.Size([320])
encoder.stages.4.0.convs.1.norm.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.4.0.convs.1.all_modules.0.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.1.weight shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.1.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.0.conv.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.norm.weight shape torch.Size([320])
encoder.stages.5.0.convs.0.norm.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.0.all_modules.0.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.1.weight shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.1.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.1.conv.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.norm.weight shape torch.Size([320])
encoder.stages.5.0.convs.1.norm.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.1.all_modules.0.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.1.weight shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.0.0.convs.0.conv.weight shape torch.Size([32, 1, 3, 3, 3])
decoder.encoder.stages.0.0.convs.0.conv.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.norm.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.norm.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.0.weight shape torch.Size([32, 1, 3, 3, 3])
decoder.encoder.stages.0.0.convs.0.all_modules.0.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.1.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.1.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.encoder.stages.0.0.convs.1.conv.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.norm.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.norm.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.encoder.stages.0.0.convs.1.all_modules.0.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.1.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.1.bias shape torch.Size([32])
decoder.encoder.stages.1.0.convs.0.conv.weight shape torch.Size([64, 32, 3, 3, 3])
decoder.encoder.stages.1.0.convs.0.conv.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.norm.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.norm.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.0.weight shape torch.Size([64, 32, 3, 3, 3])
decoder.encoder.stages.1.0.convs.0.all_modules.0.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.1.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.1.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.encoder.stages.1.0.convs.1.conv.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.norm.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.norm.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.encoder.stages.1.0.convs.1.all_modules.0.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.1.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.1.bias shape torch.Size([64])
decoder.encoder.stages.2.0.convs.0.conv.weight shape torch.Size([128, 64, 3, 3, 3])
decoder.encoder.stages.2.0.convs.0.conv.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.norm.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.norm.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.0.weight shape torch.Size([128, 64, 3, 3, 3])
decoder.encoder.stages.2.0.convs.0.all_modules.0.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.1.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.1.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.encoder.stages.2.0.convs.1.conv.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.norm.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.norm.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.encoder.stages.2.0.convs.1.all_modules.0.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.1.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.1.bias shape torch.Size([128])
decoder.encoder.stages.3.0.convs.0.conv.weight shape torch.Size([256, 128, 3, 3, 3])
decoder.encoder.stages.3.0.convs.0.conv.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.norm.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.norm.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.0.weight shape torch.Size([256, 128, 3, 3, 3])
decoder.encoder.stages.3.0.convs.0.all_modules.0.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.1.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.1.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.encoder.stages.3.0.convs.1.conv.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.norm.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.norm.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.encoder.stages.3.0.convs.1.all_modules.0.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.1.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.1.bias shape torch.Size([256])
decoder.encoder.stages.4.0.convs.0.conv.weight shape torch.Size([320, 256, 3, 3, 3])
decoder.encoder.stages.4.0.convs.0.conv.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.norm.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.norm.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.0.weight shape torch.Size([320, 256, 3, 3, 3])
decoder.encoder.stages.4.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.4.0.convs.1.conv.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.norm.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.norm.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.4.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.0.conv.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.norm.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.norm.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.1.conv.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.norm.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.norm.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.stages.0.convs.0.conv.weight shape torch.Size([320, 640, 3, 3, 3])
decoder.stages.0.convs.0.conv.bias shape torch.Size([320])
decoder.stages.0.convs.0.norm.weight shape torch.Size([320])
decoder.stages.0.convs.0.norm.bias shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.0.weight shape torch.Size([320, 640, 3, 3, 3])
decoder.stages.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.stages.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.stages.0.convs.1.conv.bias shape torch.Size([320])
decoder.stages.0.convs.1.norm.weight shape torch.Size([320])
decoder.stages.0.convs.1.norm.bias shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.stages.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.stages.1.convs.0.conv.weight shape torch.Size([256, 512, 3, 3, 3])
decoder.stages.1.convs.0.conv.bias shape torch.Size([256])
decoder.stages.1.convs.0.norm.weight shape torch.Size([256])
decoder.stages.1.convs.0.norm.bias shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.0.weight shape torch.Size([256, 512, 3, 3, 3])
decoder.stages.1.convs.0.all_modules.0.bias shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.1.weight shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.1.bias shape torch.Size([256])
decoder.stages.1.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.stages.1.convs.1.conv.bias shape torch.Size([256])
decoder.stages.1.convs.1.norm.weight shape torch.Size([256])
decoder.stages.1.convs.1.norm.bias shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.stages.1.convs.1.all_modules.0.bias shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.1.weight shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.1.bias shape torch.Size([256])
decoder.stages.2.convs.0.conv.weight shape torch.Size([128, 256, 3, 3, 3])
decoder.stages.2.convs.0.conv.bias shape torch.Size([128])
decoder.stages.2.convs.0.norm.weight shape torch.Size([128])
decoder.stages.2.convs.0.norm.bias shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.0.weight shape torch.Size([128, 256, 3, 3, 3])
decoder.stages.2.convs.0.all_modules.0.bias shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.1.weight shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.1.bias shape torch.Size([128])
decoder.stages.2.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.stages.2.convs.1.conv.bias shape torch.Size([128])
decoder.stages.2.convs.1.norm.weight shape torch.Size([128])
decoder.stages.2.convs.1.norm.bias shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.stages.2.convs.1.all_modules.0.bias shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.1.weight shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.1.bias shape torch.Size([128])
decoder.stages.3.convs.0.conv.weight shape torch.Size([64, 128, 3, 3, 3])
decoder.stages.3.convs.0.conv.bias shape torch.Size([64])
decoder.stages.3.convs.0.norm.weight shape torch.Size([64])
decoder.stages.3.convs.0.norm.bias shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.0.weight shape torch.Size([64, 128, 3, 3, 3])
decoder.stages.3.convs.0.all_modules.0.bias shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.1.weight shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.1.bias shape torch.Size([64])
decoder.stages.3.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.stages.3.convs.1.conv.bias shape torch.Size([64])
decoder.stages.3.convs.1.norm.weight shape torch.Size([64])
decoder.stages.3.convs.1.norm.bias shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.stages.3.convs.1.all_modules.0.bias shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.1.weight shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.1.bias shape torch.Size([64])
decoder.stages.4.convs.0.conv.weight shape torch.Size([32, 64, 3, 3, 3])
decoder.stages.4.convs.0.conv.bias shape torch.Size([32])
decoder.stages.4.convs.0.norm.weight shape torch.Size([32])
decoder.stages.4.convs.0.norm.bias shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.0.weight shape torch.Size([32, 64, 3, 3, 3])
decoder.stages.4.convs.0.all_modules.0.bias shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.1.weight shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.1.bias shape torch.Size([32])
decoder.stages.4.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.stages.4.convs.1.conv.bias shape torch.Size([32])
decoder.stages.4.convs.1.norm.weight shape torch.Size([32])
decoder.stages.4.convs.1.norm.bias shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.stages.4.convs.1.all_modules.0.bias shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.1.weight shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.1.bias shape torch.Size([32])
decoder.transpconvs.0.weight shape torch.Size([320, 320, 1, 2, 2])
decoder.transpconvs.0.bias shape torch.Size([320])
decoder.transpconvs.1.weight shape torch.Size([320, 256, 2, 2, 2])
decoder.transpconvs.1.bias shape torch.Size([256])
decoder.transpconvs.2.weight shape torch.Size([256, 128, 2, 2, 2])
decoder.transpconvs.2.bias shape torch.Size([128])
decoder.transpconvs.3.weight shape torch.Size([128, 64, 2, 2, 2])
decoder.transpconvs.3.bias shape torch.Size([64])
decoder.transpconvs.4.weight shape torch.Size([64, 32, 2, 2, 2])
decoder.transpconvs.4.bias shape torch.Size([32])
################### Done ###################
2024-12-19 21:32:04.510432: do_dummy_2d_data_aug: True
2024-12-19 21:32:04.542154: Using splits from existing split file: /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset308_Sohee_Calcium_OCT_CrossValidation/splits_final_40.json
2024-12-19 21:32:04.552830: The split file contains 5 splits.
2024-12-19 21:32:04.554327: Desired fold for training: 0
2024-12-19 21:32:04.555457: This split has 3 training and 5 validation cases.
using pin_memory on device 0
using pin_memory on device 0
using pin_memory on device 0

This is the configuration used by this training:
Configuration name: 3d_32x160x128_b10
 {'data_identifier': 'nnUNetPlans_3d_fullres', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 10, 'patch_size': [32, 160, 128], 'median_image_size_in_voxels': [375.0, 498.0, 498.0], 'spacing': [1.0, 1.0, 1.0], 'normalization_schemes': ['ZScoreNormalization'], 'use_mask_for_norm': [False], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'architecture': {'network_class_name': 'dynamic_network_architectures.architectures.unet.PlainConvUNet', 'arch_kwargs': {'n_stages': 6, 'features_per_stage': [32, 64, 128, 256, 320, 320], 'conv_op': 'torch.nn.modules.conv.Conv3d', 'kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]], 'strides': [[1, 1, 1], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [1, 2, 2]], 'n_conv_per_stage': [2, 2, 2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2, 2, 2], 'conv_bias': True, 'norm_op': 'torch.nn.modules.instancenorm.InstanceNorm3d', 'norm_op_kwargs': {'eps': 1e-05, 'affine': True}, 'dropout_op': None, 'dropout_op_kwargs': None, 'nonlin': 'torch.nn.LeakyReLU', 'nonlin_kwargs': {'inplace': True}, 'deep_supervision': True}, '_kw_requires_import': ['conv_op', 'norm_op', 'dropout_op', 'nonlin']}, 'batch_dice': True, 'inherits_from': '3d_fullres'} 

These are the global plan.json settings:
 {'dataset_name': 'Dataset308_Sohee_Calcium_OCT_CrossValidation', 'plans_name': 'nnUNetPlans', 'original_median_spacing_after_transp': [1.0, 1.0, 1.0], 'original_median_shape_after_transp': [375, 498, 498], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 0.4977976381778717, 'mean': 0.13507525622844696, 'median': 0.09599608182907104, 'min': 0.0, 'percentile_00_5': 0.014754901640117168, 'percentile_99_5': 0.4977976381778717, 'std': 0.12025152146816254}}} 

2024-12-19 21:32:35.958806: unpacking dataset...
using pin_memory on device 0

This is the configuration used by this training:
Configuration name: 3d_32x160x128_b10
 {'data_identifier': 'nnUNetPlans_3d_fullres', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 10, 'patch_size': [32, 160, 128], 'median_image_size_in_voxels': [375.0, 498.0, 498.0], 'spacing': [1.0, 1.0, 1.0], 'normalization_schemes': ['ZScoreNormalization'], 'use_mask_for_norm': [False], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'architecture': {'network_class_name': 'dynamic_network_architectures.architectures.unet.PlainConvUNet', 'arch_kwargs': {'n_stages': 6, 'features_per_stage': [32, 64, 128, 256, 320, 320], 'conv_op': 'torch.nn.modules.conv.Conv3d', 'kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]], 'strides': [[1, 1, 1], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [1, 2, 2]], 'n_conv_per_stage': [2, 2, 2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2, 2, 2], 'conv_bias': True, 'norm_op': 'torch.nn.modules.instancenorm.InstanceNorm3d', 'norm_op_kwargs': {'eps': 1e-05, 'affine': True}, 'dropout_op': None, 'dropout_op_kwargs': None, 'nonlin': 'torch.nn.LeakyReLU', 'nonlin_kwargs': {'inplace': True}, 'deep_supervision': True}, '_kw_requires_import': ['conv_op', 'norm_op', 'dropout_op', 'nonlin']}, 'batch_dice': True, 'inherits_from': '3d_fullres'} 

These are the global plan.json settings:
 {'dataset_name': 'Dataset308_Sohee_Calcium_OCT_CrossValidation', 'plans_name': 'nnUNetPlans', 'original_median_spacing_after_transp': [1.0, 1.0, 1.0], 'original_median_shape_after_transp': [375, 498, 498], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 0.4977976381778717, 'mean': 0.13507525622844696, 'median': 0.09599608182907104, 'min': 0.0, 'percentile_00_5': 0.014754901640117168, 'percentile_99_5': 0.4977976381778717, 'std': 0.12025152146816254}}} 

2024-12-19 21:32:38.056662: unpacking dataset...
2024-12-19 21:32:42.482135: unpacking done...
2024-12-19 21:32:42.489988: Unable to plot network architecture: nnUNet_compile is enabled!
2024-12-19 21:32:42.556671: 
2024-12-19 21:32:42.557924: Epoch 0
2024-12-19 21:32:42.559029: Current learning rate: 0.01
2024-12-19 21:42:54.199396: Validation loss improved from 1000.00000 to -0.19171! Patience: 0/50
2024-12-19 21:42:54.200362: train_loss -0.0634
2024-12-19 21:42:54.201281: val_loss -0.1917
2024-12-19 21:42:54.202153: Pseudo dice [0.5352]
2024-12-19 21:42:54.203055: Epoch time: 611.65 s
2024-12-19 21:42:54.204094: Yayy! New best EMA pseudo Dice: 0.5352
2024-12-19 21:42:57.372874: 
2024-12-19 21:42:57.374061: Epoch 1
2024-12-19 21:42:57.374927: Current learning rate: 0.00994
2024-12-19 21:51:41.082301: Validation loss improved from -0.19171 to -0.25922! Patience: 0/50
2024-12-19 21:51:41.083221: train_loss -0.2332
2024-12-19 21:51:41.083992: val_loss -0.2592
2024-12-19 21:51:41.084668: Pseudo dice [0.5683]
2024-12-19 21:51:41.085399: Epoch time: 523.71 s
2024-12-19 21:51:41.086163: Yayy! New best EMA pseudo Dice: 0.5385
2024-12-19 21:51:42.972312: 
2024-12-19 21:51:42.973554: Epoch 2
2024-12-19 21:51:42.974254: Current learning rate: 0.00988
2024-12-19 22:00:16.338273: Validation loss improved from -0.25922 to -0.29378! Patience: 0/50
2024-12-19 22:00:16.339168: train_loss -0.2884
2024-12-19 22:00:16.339847: val_loss -0.2938
2024-12-19 22:00:16.340631: Pseudo dice [0.6132]
2024-12-19 22:00:16.341277: Epoch time: 513.37 s
2024-12-19 22:00:16.341888: Yayy! New best EMA pseudo Dice: 0.546
2024-12-19 22:00:18.210225: 
2024-12-19 22:00:18.211491: Epoch 3
2024-12-19 22:00:18.212205: Current learning rate: 0.00982
2024-12-19 22:09:30.890596: Validation loss improved from -0.29378 to -0.33025! Patience: 0/50
2024-12-19 22:09:30.891648: train_loss -0.3443
2024-12-19 22:09:30.892437: val_loss -0.3303
2024-12-19 22:09:30.893167: Pseudo dice [0.6355]
2024-12-19 22:09:30.893864: Epoch time: 552.68 s
2024-12-19 22:09:30.894517: Yayy! New best EMA pseudo Dice: 0.5549
2024-12-19 22:09:32.800262: 
2024-12-19 22:09:32.801582: Epoch 4
2024-12-19 22:09:32.802364: Current learning rate: 0.00976
2024-12-19 22:18:26.438747: Validation loss did not improve from -0.33025. Patience: 1/50
2024-12-19 22:18:26.439738: train_loss -0.373
2024-12-19 22:18:26.440640: val_loss -0.3145
2024-12-19 22:18:26.441473: Pseudo dice [0.6113]
2024-12-19 22:18:26.442219: Epoch time: 533.64 s
2024-12-19 22:18:26.837034: Yayy! New best EMA pseudo Dice: 0.5606
2024-12-19 22:18:28.697130: 
2024-12-19 22:18:28.698202: Epoch 5
2024-12-19 22:18:28.698950: Current learning rate: 0.0097
2024-12-19 22:27:20.505544: Validation loss did not improve from -0.33025. Patience: 2/50
2024-12-19 22:27:20.506558: train_loss -0.3994
2024-12-19 22:27:20.507459: val_loss -0.3008
2024-12-19 22:27:20.508150: Pseudo dice [0.596]
2024-12-19 22:27:20.508876: Epoch time: 531.81 s
2024-12-19 22:27:20.509759: Yayy! New best EMA pseudo Dice: 0.5641
2024-12-19 22:27:22.372504: 
2024-12-19 22:27:22.373897: Epoch 6
2024-12-19 22:27:22.374806: Current learning rate: 0.00964
2024-12-19 22:36:22.888843: Validation loss improved from -0.33025 to -0.36467! Patience: 2/50
2024-12-19 22:36:22.889727: train_loss -0.4372
2024-12-19 22:36:22.890556: val_loss -0.3647
2024-12-19 22:36:22.891197: Pseudo dice [0.6569]
2024-12-19 22:36:22.891859: Epoch time: 540.52 s
2024-12-19 22:36:22.892769: Yayy! New best EMA pseudo Dice: 0.5734
2024-12-19 22:36:24.735070: 
2024-12-19 22:36:24.736420: Epoch 7
2024-12-19 22:36:24.737133: Current learning rate: 0.00958
2024-12-19 22:45:19.200179: Validation loss did not improve from -0.36467. Patience: 1/50
2024-12-19 22:45:19.201040: train_loss -0.4686
2024-12-19 22:45:19.201801: val_loss -0.3556
2024-12-19 22:45:19.202610: Pseudo dice [0.6482]
2024-12-19 22:45:19.203383: Epoch time: 534.47 s
2024-12-19 22:45:19.204044: Yayy! New best EMA pseudo Dice: 0.5809
2024-12-19 22:45:21.615227: 
2024-12-19 22:45:21.616068: Epoch 8
2024-12-19 22:45:21.616805: Current learning rate: 0.00952
2024-12-19 22:54:10.384162: Validation loss did not improve from -0.36467. Patience: 2/50
2024-12-19 22:54:10.385061: train_loss -0.4788
2024-12-19 22:54:10.385943: val_loss -0.347
2024-12-19 22:54:10.386593: Pseudo dice [0.6434]
2024-12-19 22:54:10.387259: Epoch time: 528.77 s
2024-12-19 22:54:10.387926: Yayy! New best EMA pseudo Dice: 0.5871
2024-12-19 22:54:12.251358: 
2024-12-19 22:54:12.252695: Epoch 9
2024-12-19 22:54:12.253400: Current learning rate: 0.00946
2024-12-19 23:02:59.948403: Validation loss did not improve from -0.36467. Patience: 3/50
2024-12-19 23:02:59.949461: train_loss -0.4993
2024-12-19 23:02:59.950314: val_loss -0.3487
2024-12-19 23:02:59.950951: Pseudo dice [0.6408]
2024-12-19 23:02:59.951590: Epoch time: 527.7 s
2024-12-19 23:03:00.356560: Yayy! New best EMA pseudo Dice: 0.5925
2024-12-19 23:03:02.269437: 
2024-12-19 23:03:02.270834: Epoch 10
2024-12-19 23:03:02.271693: Current learning rate: 0.0094
2024-12-19 23:11:38.134895: Validation loss did not improve from -0.36467. Patience: 4/50
2024-12-19 23:11:38.135929: train_loss -0.5213
2024-12-19 23:11:38.136831: val_loss -0.3626
2024-12-19 23:11:38.137552: Pseudo dice [0.6562]
2024-12-19 23:11:38.138284: Epoch time: 515.87 s
2024-12-19 23:11:38.138992: Yayy! New best EMA pseudo Dice: 0.5988
2024-12-19 23:11:39.916956: 
2024-12-19 23:11:39.918129: Epoch 11
2024-12-19 23:11:39.918968: Current learning rate: 0.00934
2024-12-19 23:19:32.797648: Validation loss improved from -0.36467 to -0.38033! Patience: 4/50
2024-12-19 23:19:32.798395: train_loss -0.5255
2024-12-19 23:19:32.799324: val_loss -0.3803
2024-12-19 23:19:32.800295: Pseudo dice [0.6601]
2024-12-19 23:19:32.801098: Epoch time: 472.88 s
2024-12-19 23:19:32.801837: Yayy! New best EMA pseudo Dice: 0.605
2024-12-19 23:19:34.666017: 
2024-12-19 23:19:34.667204: Epoch 12
2024-12-19 23:19:34.667973: Current learning rate: 0.00928
2024-12-19 23:27:13.243782: Validation loss improved from -0.38033 to -0.41746! Patience: 0/50
2024-12-19 23:27:13.244836: train_loss -0.5451
2024-12-19 23:27:13.246063: val_loss -0.4175
2024-12-19 23:27:13.247025: Pseudo dice [0.6832]
2024-12-19 23:27:13.247918: Epoch time: 458.58 s
2024-12-19 23:27:13.248698: Yayy! New best EMA pseudo Dice: 0.6128
2024-12-19 23:27:15.098651: 
2024-12-19 23:27:15.099756: Epoch 13
2024-12-19 23:27:15.100497: Current learning rate: 0.00922
2024-12-19 23:35:11.112467: Validation loss did not improve from -0.41746. Patience: 1/50
2024-12-19 23:35:11.130779: train_loss -0.5433
2024-12-19 23:35:11.132215: val_loss -0.3265
2024-12-19 23:35:11.132878: Pseudo dice [0.6366]
2024-12-19 23:35:11.133707: Epoch time: 476.03 s
2024-12-19 23:35:11.134351: Yayy! New best EMA pseudo Dice: 0.6152
2024-12-19 23:35:13.079554: 
2024-12-19 23:35:13.080712: Epoch 14
2024-12-19 23:35:13.081410: Current learning rate: 0.00916
2024-12-19 23:43:07.573276: Validation loss did not improve from -0.41746. Patience: 2/50
2024-12-19 23:43:07.575118: train_loss -0.5645
2024-12-19 23:43:07.576173: val_loss -0.3002
2024-12-19 23:43:07.576827: Pseudo dice [0.6288]
2024-12-19 23:43:07.577842: Epoch time: 474.5 s
2024-12-19 23:43:07.998528: Yayy! New best EMA pseudo Dice: 0.6165
2024-12-19 23:43:09.848563: 
2024-12-19 23:43:09.849904: Epoch 15
2024-12-19 23:43:09.850805: Current learning rate: 0.0091
2024-12-19 23:51:09.147942: Validation loss did not improve from -0.41746. Patience: 3/50
2024-12-19 23:51:09.149552: train_loss -0.5746
2024-12-19 23:51:09.150417: val_loss -0.3783
2024-12-19 23:51:09.151214: Pseudo dice [0.6663]
2024-12-19 23:51:09.152026: Epoch time: 479.3 s
2024-12-19 23:51:09.152642: Yayy! New best EMA pseudo Dice: 0.6215
2024-12-19 23:51:11.188614: 
2024-12-19 23:51:11.189774: Epoch 16
2024-12-19 23:51:11.190475: Current learning rate: 0.00903
2024-12-19 23:58:43.383881: Validation loss did not improve from -0.41746. Patience: 4/50
2024-12-19 23:58:43.385028: train_loss -0.5838
2024-12-19 23:58:43.385797: val_loss -0.3481
2024-12-19 23:58:43.386549: Pseudo dice [0.6394]
2024-12-19 23:58:43.387399: Epoch time: 452.2 s
2024-12-19 23:58:43.388089: Yayy! New best EMA pseudo Dice: 0.6233
2024-12-19 23:58:45.351447: 
2024-12-19 23:58:45.352892: Epoch 17
2024-12-19 23:58:45.353696: Current learning rate: 0.00897
2024-12-20 00:06:43.251821: Validation loss did not improve from -0.41746. Patience: 5/50
2024-12-20 00:06:43.252760: train_loss -0.5875
2024-12-20 00:06:43.254001: val_loss -0.3477
2024-12-20 00:06:43.254650: Pseudo dice [0.6465]
2024-12-20 00:06:43.255341: Epoch time: 477.9 s
2024-12-20 00:06:43.255969: Yayy! New best EMA pseudo Dice: 0.6256
2024-12-20 00:06:45.211088: 
2024-12-20 00:06:45.212260: Epoch 18
2024-12-20 00:06:45.213102: Current learning rate: 0.00891
2024-12-20 00:14:50.257504: Validation loss did not improve from -0.41746. Patience: 6/50
2024-12-20 00:14:50.258546: train_loss -0.592
2024-12-20 00:14:50.259596: val_loss -0.2882
2024-12-20 00:14:50.260602: Pseudo dice [0.635]
2024-12-20 00:14:50.261601: Epoch time: 485.05 s
2024-12-20 00:14:50.262550: Yayy! New best EMA pseudo Dice: 0.6266
2024-12-20 00:14:52.716769: 
2024-12-20 00:14:52.717976: Epoch 19
2024-12-20 00:14:52.718816: Current learning rate: 0.00885
2024-12-20 00:23:04.099194: Validation loss did not improve from -0.41746. Patience: 7/50
2024-12-20 00:23:04.100214: train_loss -0.5999
2024-12-20 00:23:04.100999: val_loss -0.3934
2024-12-20 00:23:04.101709: Pseudo dice [0.6744]
2024-12-20 00:23:04.102761: Epoch time: 491.38 s
2024-12-20 00:23:04.522309: Yayy! New best EMA pseudo Dice: 0.6313
2024-12-20 00:23:06.587203: 
2024-12-20 00:23:06.588411: Epoch 20
2024-12-20 00:23:06.589309: Current learning rate: 0.00879
2024-12-20 00:30:57.675911: Validation loss improved from -0.41746 to -0.43605! Patience: 7/50
2024-12-20 00:30:57.676760: train_loss -0.6081
2024-12-20 00:30:57.677532: val_loss -0.436
2024-12-20 00:30:57.678270: Pseudo dice [0.688]
2024-12-20 00:30:57.678914: Epoch time: 471.09 s
2024-12-20 00:30:57.679539: Yayy! New best EMA pseudo Dice: 0.637
2024-12-20 00:30:59.591492: 
2024-12-20 00:30:59.592453: Epoch 21
2024-12-20 00:30:59.593256: Current learning rate: 0.00873
2024-12-20 00:38:47.049937: Validation loss did not improve from -0.43605. Patience: 1/50
2024-12-20 00:38:47.051916: train_loss -0.6267
2024-12-20 00:38:47.052718: val_loss -0.3931
2024-12-20 00:38:47.053378: Pseudo dice [0.6701]
2024-12-20 00:38:47.054077: Epoch time: 467.46 s
2024-12-20 00:38:47.054888: Yayy! New best EMA pseudo Dice: 0.6403
2024-12-20 00:38:48.885396: 
2024-12-20 00:38:48.886451: Epoch 22
2024-12-20 00:38:48.887176: Current learning rate: 0.00867
2024-12-20 00:46:51.974063: Validation loss did not improve from -0.43605. Patience: 2/50
2024-12-20 00:46:51.975118: train_loss -0.6343
2024-12-20 00:46:51.976115: val_loss -0.4213
2024-12-20 00:46:51.976869: Pseudo dice [0.6802]
2024-12-20 00:46:51.977602: Epoch time: 483.09 s
2024-12-20 00:46:51.978310: Yayy! New best EMA pseudo Dice: 0.6443
2024-12-20 00:46:53.903219: 
2024-12-20 00:46:53.904368: Epoch 23
2024-12-20 00:46:53.905178: Current learning rate: 0.00861
2024-12-20 00:54:58.098074: Validation loss did not improve from -0.43605. Patience: 3/50
2024-12-20 00:54:58.099184: train_loss -0.6344
2024-12-20 00:54:58.099989: val_loss -0.2862
2024-12-20 00:54:58.100754: Pseudo dice [0.6332]
2024-12-20 00:54:58.101514: Epoch time: 484.2 s
2024-12-20 00:54:59.518910: 
2024-12-20 00:54:59.520651: Epoch 24
2024-12-20 00:54:59.522305: Current learning rate: 0.00855
2024-12-20 01:02:33.914840: Validation loss did not improve from -0.43605. Patience: 4/50
2024-12-20 01:02:33.915969: train_loss -0.6435
2024-12-20 01:02:33.916713: val_loss -0.4348
2024-12-20 01:02:33.917506: Pseudo dice [0.6927]
2024-12-20 01:02:33.918113: Epoch time: 454.4 s
2024-12-20 01:02:34.425318: Yayy! New best EMA pseudo Dice: 0.6481
2024-12-20 01:02:36.352973: 
2024-12-20 01:02:36.354089: Epoch 25
2024-12-20 01:02:36.354878: Current learning rate: 0.00849
2024-12-20 01:10:22.427328: Validation loss did not improve from -0.43605. Patience: 5/50
2024-12-20 01:10:22.428385: train_loss -0.6487
2024-12-20 01:10:22.429211: val_loss -0.368
2024-12-20 01:10:22.430069: Pseudo dice [0.659]
2024-12-20 01:10:22.430940: Epoch time: 466.08 s
2024-12-20 01:10:22.431684: Yayy! New best EMA pseudo Dice: 0.6492
2024-12-20 01:10:24.407763: 
2024-12-20 01:10:24.408895: Epoch 26
2024-12-20 01:10:24.409690: Current learning rate: 0.00843
2024-12-20 01:18:28.570788: Validation loss did not improve from -0.43605. Patience: 6/50
2024-12-20 01:18:28.571848: train_loss -0.6524
2024-12-20 01:18:28.572747: val_loss -0.3897
2024-12-20 01:18:28.573486: Pseudo dice [0.6719]
2024-12-20 01:18:28.574204: Epoch time: 484.17 s
2024-12-20 01:18:28.574862: Yayy! New best EMA pseudo Dice: 0.6515
2024-12-20 01:18:30.478481: 
2024-12-20 01:18:30.479850: Epoch 27
2024-12-20 01:18:30.480623: Current learning rate: 0.00836
2024-12-20 01:26:52.036705: Validation loss did not improve from -0.43605. Patience: 7/50
2024-12-20 01:26:52.038411: train_loss -0.6577
2024-12-20 01:26:52.039389: val_loss -0.3994
2024-12-20 01:26:52.040191: Pseudo dice [0.6697]
2024-12-20 01:26:52.040873: Epoch time: 501.56 s
2024-12-20 01:26:52.041557: Yayy! New best EMA pseudo Dice: 0.6533
2024-12-20 01:26:54.050809: 
2024-12-20 01:26:54.052092: Epoch 28
2024-12-20 01:26:54.052762: Current learning rate: 0.0083
2024-12-20 01:34:37.555983: Validation loss did not improve from -0.43605. Patience: 8/50
2024-12-20 01:34:37.557157: train_loss -0.6607
2024-12-20 01:34:37.558359: val_loss -0.4111
2024-12-20 01:34:37.559257: Pseudo dice [0.6926]
2024-12-20 01:34:37.559975: Epoch time: 463.51 s
2024-12-20 01:34:37.560812: Yayy! New best EMA pseudo Dice: 0.6573
2024-12-20 01:34:39.663050: 
2024-12-20 01:34:39.664440: Epoch 29
2024-12-20 01:34:39.665532: Current learning rate: 0.00824
2024-12-20 01:42:45.203791: Validation loss improved from -0.43605 to -0.44781! Patience: 8/50
2024-12-20 01:42:45.205968: train_loss -0.6727
2024-12-20 01:42:45.207158: val_loss -0.4478
2024-12-20 01:42:45.208012: Pseudo dice [0.7085]
2024-12-20 01:42:45.208650: Epoch time: 485.54 s
2024-12-20 01:42:45.814094: Yayy! New best EMA pseudo Dice: 0.6624
2024-12-20 01:42:49.378433: 
2024-12-20 01:42:49.379906: Epoch 30
2024-12-20 01:42:49.380754: Current learning rate: 0.00818
2024-12-20 01:50:27.589998: Validation loss did not improve from -0.44781. Patience: 1/50
2024-12-20 01:50:27.591000: train_loss -0.6765
2024-12-20 01:50:27.591996: val_loss -0.3785
2024-12-20 01:50:27.592973: Pseudo dice [0.6661]
2024-12-20 01:50:27.593777: Epoch time: 458.21 s
2024-12-20 01:50:27.594514: Yayy! New best EMA pseudo Dice: 0.6627
2024-12-20 01:50:29.433598: 
2024-12-20 01:50:29.434934: Epoch 31
2024-12-20 01:50:29.435759: Current learning rate: 0.00812
2024-12-20 01:58:02.297829: Validation loss did not improve from -0.44781. Patience: 2/50
2024-12-20 01:58:02.299052: train_loss -0.6753
2024-12-20 01:58:02.300220: val_loss -0.4169
2024-12-20 01:58:02.301121: Pseudo dice [0.6789]
2024-12-20 01:58:02.302012: Epoch time: 452.87 s
2024-12-20 01:58:02.302943: Yayy! New best EMA pseudo Dice: 0.6644
2024-12-20 01:58:04.171996: 
2024-12-20 01:58:04.173539: Epoch 32
2024-12-20 01:58:04.174554: Current learning rate: 0.00806
2024-12-20 02:05:35.652722: Validation loss did not improve from -0.44781. Patience: 3/50
2024-12-20 02:05:35.653953: train_loss -0.6752
2024-12-20 02:05:35.654880: val_loss -0.399
2024-12-20 02:05:35.655770: Pseudo dice [0.685]
2024-12-20 02:05:35.656658: Epoch time: 451.48 s
2024-12-20 02:05:35.657510: Yayy! New best EMA pseudo Dice: 0.6664
2024-12-20 02:05:37.576183: 
2024-12-20 02:05:37.577589: Epoch 33
2024-12-20 02:05:37.578446: Current learning rate: 0.008
2024-12-20 02:13:03.111128: Validation loss did not improve from -0.44781. Patience: 4/50
2024-12-20 02:13:03.112253: train_loss -0.6804
2024-12-20 02:13:03.113126: val_loss -0.3508
2024-12-20 02:13:03.114007: Pseudo dice [0.65]
2024-12-20 02:13:03.114824: Epoch time: 445.54 s
2024-12-20 02:13:04.658911: 
2024-12-20 02:13:04.660249: Epoch 34
2024-12-20 02:13:04.661303: Current learning rate: 0.00793
2024-12-20 02:20:37.854534: Validation loss did not improve from -0.44781. Patience: 5/50
2024-12-20 02:20:37.855657: train_loss -0.6859
2024-12-20 02:20:37.856627: val_loss -0.4401
2024-12-20 02:20:37.857498: Pseudo dice [0.7035]
2024-12-20 02:20:37.858621: Epoch time: 453.2 s
2024-12-20 02:20:38.263592: Yayy! New best EMA pseudo Dice: 0.6687
2024-12-20 02:20:40.177520: 
2024-12-20 02:20:40.178881: Epoch 35
2024-12-20 02:20:40.179825: Current learning rate: 0.00787
2024-12-20 02:27:57.597258: Validation loss did not improve from -0.44781. Patience: 6/50
2024-12-20 02:27:57.598255: train_loss -0.6862
2024-12-20 02:27:57.599227: val_loss -0.3764
2024-12-20 02:27:57.600038: Pseudo dice [0.6745]
2024-12-20 02:27:57.600811: Epoch time: 437.42 s
2024-12-20 02:27:57.601678: Yayy! New best EMA pseudo Dice: 0.6692
2024-12-20 02:27:59.512815: 
2024-12-20 02:27:59.514031: Epoch 36
2024-12-20 02:27:59.514823: Current learning rate: 0.00781
2024-12-20 02:35:21.608449: Validation loss did not improve from -0.44781. Patience: 7/50
2024-12-20 02:35:21.609536: train_loss -0.6997
2024-12-20 02:35:21.610588: val_loss -0.329
2024-12-20 02:35:21.611612: Pseudo dice [0.6521]
2024-12-20 02:35:21.612570: Epoch time: 442.1 s
2024-12-20 02:35:23.089360: 
2024-12-20 02:35:23.090402: Epoch 37
2024-12-20 02:35:23.091568: Current learning rate: 0.00775
2024-12-20 02:43:00.724273: Validation loss did not improve from -0.44781. Patience: 8/50
2024-12-20 02:43:00.725126: train_loss -0.7014
2024-12-20 02:43:00.725922: val_loss -0.3757
2024-12-20 02:43:00.726657: Pseudo dice [0.668]
2024-12-20 02:43:00.727398: Epoch time: 457.64 s
2024-12-20 02:43:02.191041: 
2024-12-20 02:43:02.192199: Epoch 38
2024-12-20 02:43:02.192948: Current learning rate: 0.00769
2024-12-20 02:50:30.138209: Validation loss did not improve from -0.44781. Patience: 9/50
2024-12-20 02:50:30.140708: train_loss -0.7014
2024-12-20 02:50:30.141519: val_loss -0.4029
2024-12-20 02:50:30.142230: Pseudo dice [0.6769]
2024-12-20 02:50:30.142929: Epoch time: 447.95 s
2024-12-20 02:50:31.593447: 
2024-12-20 02:50:31.594617: Epoch 39
2024-12-20 02:50:31.595327: Current learning rate: 0.00763
2024-12-20 02:58:17.478192: Validation loss did not improve from -0.44781. Patience: 10/50
2024-12-20 02:58:17.480817: train_loss -0.7039
2024-12-20 02:58:17.481881: val_loss -0.4019
2024-12-20 02:58:17.482729: Pseudo dice [0.6997]
2024-12-20 02:58:17.483717: Epoch time: 465.89 s
2024-12-20 02:58:17.917030: Yayy! New best EMA pseudo Dice: 0.6716
2024-12-20 02:58:20.574333: 
2024-12-20 02:58:20.575740: Epoch 40
2024-12-20 02:58:20.576697: Current learning rate: 0.00756
2024-12-20 03:05:52.909683: Validation loss did not improve from -0.44781. Patience: 11/50
2024-12-20 03:05:52.910429: train_loss -0.7121
2024-12-20 03:05:52.911191: val_loss -0.4375
2024-12-20 03:05:52.911818: Pseudo dice [0.7061]
2024-12-20 03:05:52.912467: Epoch time: 452.34 s
2024-12-20 03:05:52.913126: Yayy! New best EMA pseudo Dice: 0.6751
2024-12-20 03:05:54.865282: 
2024-12-20 03:05:54.866191: Epoch 41
2024-12-20 03:05:54.866896: Current learning rate: 0.0075
2024-12-20 03:13:15.334421: Validation loss did not improve from -0.44781. Patience: 12/50
2024-12-20 03:13:15.335558: train_loss -0.7067
2024-12-20 03:13:15.336447: val_loss -0.394
2024-12-20 03:13:15.337317: Pseudo dice [0.6807]
2024-12-20 03:13:15.338141: Epoch time: 440.47 s
2024-12-20 03:13:15.338979: Yayy! New best EMA pseudo Dice: 0.6756
2024-12-20 03:13:17.237749: 
2024-12-20 03:13:17.239221: Epoch 42
2024-12-20 03:13:17.240562: Current learning rate: 0.00744
2024-12-20 03:21:00.247621: Validation loss did not improve from -0.44781. Patience: 13/50
2024-12-20 03:21:00.248569: train_loss -0.7023
2024-12-20 03:21:00.249260: val_loss -0.3891
2024-12-20 03:21:00.249984: Pseudo dice [0.6736]
2024-12-20 03:21:00.250705: Epoch time: 463.01 s
2024-12-20 03:21:01.693122: 
2024-12-20 03:21:01.694272: Epoch 43
2024-12-20 03:21:01.694986: Current learning rate: 0.00738
2024-12-20 03:28:09.805933: Validation loss did not improve from -0.44781. Patience: 14/50
2024-12-20 03:28:09.806970: train_loss -0.7118
2024-12-20 03:28:09.807786: val_loss -0.4307
2024-12-20 03:28:09.808511: Pseudo dice [0.7065]
2024-12-20 03:28:09.809300: Epoch time: 428.12 s
2024-12-20 03:28:09.809998: Yayy! New best EMA pseudo Dice: 0.6785
2024-12-20 03:28:11.718432: 
2024-12-20 03:28:11.719744: Epoch 44
2024-12-20 03:28:11.721150: Current learning rate: 0.00732
2024-12-20 03:35:22.291651: Validation loss did not improve from -0.44781. Patience: 15/50
2024-12-20 03:35:22.292737: train_loss -0.7239
2024-12-20 03:35:22.293587: val_loss -0.3892
2024-12-20 03:35:22.294361: Pseudo dice [0.6846]
2024-12-20 03:35:22.295077: Epoch time: 430.58 s
2024-12-20 03:35:22.774674: Yayy! New best EMA pseudo Dice: 0.6791
2024-12-20 03:35:24.689454: 
2024-12-20 03:35:24.690608: Epoch 45
2024-12-20 03:35:24.691312: Current learning rate: 0.00725
2024-12-20 03:43:12.365555: Validation loss did not improve from -0.44781. Patience: 16/50
2024-12-20 03:43:12.366381: train_loss -0.7274
2024-12-20 03:43:12.367060: val_loss -0.3805
2024-12-20 03:43:12.367693: Pseudo dice [0.6742]
2024-12-20 03:43:12.368330: Epoch time: 467.68 s
2024-12-20 03:43:13.830597: 
2024-12-20 03:43:13.831863: Epoch 46
2024-12-20 03:43:13.832585: Current learning rate: 0.00719
2024-12-20 03:50:43.117925: Validation loss did not improve from -0.44781. Patience: 17/50
2024-12-20 03:50:43.118922: train_loss -0.7228
2024-12-20 03:50:43.119921: val_loss -0.3504
2024-12-20 03:50:43.120824: Pseudo dice [0.6629]
2024-12-20 03:50:43.121727: Epoch time: 449.29 s
2024-12-20 03:50:44.509026: 
2024-12-20 03:50:44.510319: Epoch 47
2024-12-20 03:50:44.511191: Current learning rate: 0.00713
2024-12-20 03:58:02.435650: Validation loss did not improve from -0.44781. Patience: 18/50
2024-12-20 03:58:02.436705: train_loss -0.7159
2024-12-20 03:58:02.437414: val_loss -0.3958
2024-12-20 03:58:02.438184: Pseudo dice [0.6866]
2024-12-20 03:58:02.438972: Epoch time: 437.93 s
2024-12-20 03:58:03.852102: 
2024-12-20 03:58:03.853361: Epoch 48
2024-12-20 03:58:03.854055: Current learning rate: 0.00707
2024-12-20 04:05:35.022010: Validation loss did not improve from -0.44781. Patience: 19/50
2024-12-20 04:05:35.023968: train_loss -0.7235
2024-12-20 04:05:35.025770: val_loss -0.3691
2024-12-20 04:05:35.026756: Pseudo dice [0.6698]
2024-12-20 04:05:35.027682: Epoch time: 451.17 s
2024-12-20 04:05:36.512559: 
2024-12-20 04:05:36.513946: Epoch 49
2024-12-20 04:05:36.514715: Current learning rate: 0.007
2024-12-20 04:12:58.978686: Validation loss did not improve from -0.44781. Patience: 20/50
2024-12-20 04:12:58.979817: train_loss -0.728
2024-12-20 04:12:58.980592: val_loss -0.353
2024-12-20 04:12:58.981249: Pseudo dice [0.6742]
2024-12-20 04:12:58.981881: Epoch time: 442.47 s
2024-12-20 04:13:00.905632: 
2024-12-20 04:13:00.906811: Epoch 50
2024-12-20 04:13:00.907646: Current learning rate: 0.00694
2024-12-20 04:20:27.042421: Validation loss did not improve from -0.44781. Patience: 21/50
2024-12-20 04:20:27.043501: train_loss -0.7333
2024-12-20 04:20:27.044309: val_loss -0.407
2024-12-20 04:20:27.044986: Pseudo dice [0.6972]
2024-12-20 04:20:27.045710: Epoch time: 446.14 s
2024-12-20 04:20:29.559831: 
2024-12-20 04:20:29.561039: Epoch 51
2024-12-20 04:20:29.561917: Current learning rate: 0.00688
2024-12-20 04:27:56.231176: Validation loss improved from -0.44781 to -0.44938! Patience: 21/50
2024-12-20 04:27:56.232053: train_loss -0.738
2024-12-20 04:27:56.232967: val_loss -0.4494
2024-12-20 04:27:56.233760: Pseudo dice [0.7076]
2024-12-20 04:27:56.234486: Epoch time: 446.67 s
2024-12-20 04:27:56.235176: Yayy! New best EMA pseudo Dice: 0.6818
2024-12-20 04:27:58.369326: 
2024-12-20 04:27:58.370605: Epoch 52
2024-12-20 04:27:58.371436: Current learning rate: 0.00682
2024-12-20 04:36:10.473991: Validation loss improved from -0.44938 to -0.45362! Patience: 0/50
2024-12-20 04:36:10.475199: train_loss -0.7413
2024-12-20 04:36:10.475998: val_loss -0.4536
2024-12-20 04:36:10.476693: Pseudo dice [0.7183]
2024-12-20 04:36:10.477526: Epoch time: 492.11 s
2024-12-20 04:36:10.478276: Yayy! New best EMA pseudo Dice: 0.6855
2024-12-20 04:36:12.565615: 
2024-12-20 04:36:12.566592: Epoch 53
2024-12-20 04:36:12.567383: Current learning rate: 0.00675
2024-12-20 04:44:06.122943: Validation loss did not improve from -0.45362. Patience: 1/50
2024-12-20 04:44:06.124288: train_loss -0.7388
2024-12-20 04:44:06.125170: val_loss -0.4015
2024-12-20 04:44:06.125995: Pseudo dice [0.6851]
2024-12-20 04:44:06.126680: Epoch time: 473.56 s
2024-12-20 04:44:07.618605: 
2024-12-20 04:44:07.619727: Epoch 54
2024-12-20 04:44:07.620625: Current learning rate: 0.00669
2024-12-20 04:51:36.301846: Validation loss did not improve from -0.45362. Patience: 2/50
2024-12-20 04:51:36.302530: train_loss -0.7401
2024-12-20 04:51:36.303506: val_loss -0.4154
2024-12-20 04:51:36.304407: Pseudo dice [0.6996]
2024-12-20 04:51:36.305206: Epoch time: 448.69 s
2024-12-20 04:51:36.784954: Yayy! New best EMA pseudo Dice: 0.6868
2024-12-20 04:51:38.723627: 
2024-12-20 04:51:38.724686: Epoch 55
2024-12-20 04:51:38.725448: Current learning rate: 0.00663
2024-12-20 04:59:25.300464: Validation loss did not improve from -0.45362. Patience: 3/50
2024-12-20 04:59:25.302319: train_loss -0.7444
2024-12-20 04:59:25.303169: val_loss -0.3367
2024-12-20 04:59:25.303928: Pseudo dice [0.6455]
2024-12-20 04:59:25.304660: Epoch time: 466.58 s
2024-12-20 04:59:26.740880: 
2024-12-20 04:59:26.742095: Epoch 56
2024-12-20 04:59:26.742872: Current learning rate: 0.00657
2024-12-20 05:06:55.430848: Validation loss did not improve from -0.45362. Patience: 4/50
2024-12-20 05:06:55.431711: train_loss -0.748
2024-12-20 05:06:55.432560: val_loss -0.3234
2024-12-20 05:06:55.433239: Pseudo dice [0.6594]
2024-12-20 05:06:55.433908: Epoch time: 448.69 s
2024-12-20 05:06:56.906040: 
2024-12-20 05:06:56.907206: Epoch 57
2024-12-20 05:06:56.907903: Current learning rate: 0.0065
2024-12-20 05:14:34.743121: Validation loss did not improve from -0.45362. Patience: 5/50
2024-12-20 05:14:34.744125: train_loss -0.75
2024-12-20 05:14:34.745096: val_loss -0.3906
2024-12-20 05:14:34.745857: Pseudo dice [0.6872]
2024-12-20 05:14:34.746628: Epoch time: 457.84 s
2024-12-20 05:14:36.198402: 
2024-12-20 05:14:36.199452: Epoch 58
2024-12-20 05:14:36.200181: Current learning rate: 0.00644
2024-12-20 05:22:25.579302: Validation loss did not improve from -0.45362. Patience: 6/50
2024-12-20 05:22:25.580289: train_loss -0.7503
2024-12-20 05:22:25.581293: val_loss -0.3494
2024-12-20 05:22:25.582161: Pseudo dice [0.6764]
2024-12-20 05:22:25.583112: Epoch time: 469.38 s
2024-12-20 05:22:27.051414: 
2024-12-20 05:22:27.052732: Epoch 59
2024-12-20 05:22:27.053705: Current learning rate: 0.00638
2024-12-20 05:30:09.195833: Validation loss did not improve from -0.45362. Patience: 7/50
2024-12-20 05:30:09.197352: train_loss -0.7589
2024-12-20 05:30:09.198099: val_loss -0.314
2024-12-20 05:30:09.198708: Pseudo dice [0.6605]
2024-12-20 05:30:09.199501: Epoch time: 462.15 s
2024-12-20 05:30:11.136257: 
2024-12-20 05:30:11.137286: Epoch 60
2024-12-20 05:30:11.137941: Current learning rate: 0.00631
2024-12-20 05:38:02.652885: Validation loss did not improve from -0.45362. Patience: 8/50
2024-12-20 05:38:02.654184: train_loss -0.7465
2024-12-20 05:38:02.655164: val_loss -0.3359
2024-12-20 05:38:02.655911: Pseudo dice [0.6595]
2024-12-20 05:38:02.656759: Epoch time: 471.52 s
2024-12-20 05:38:04.223777: 
2024-12-20 05:38:04.225200: Epoch 61
2024-12-20 05:38:04.226378: Current learning rate: 0.00625
2024-12-20 05:45:45.640260: Validation loss did not improve from -0.45362. Patience: 9/50
2024-12-20 05:45:45.641265: train_loss -0.7485
2024-12-20 05:45:45.641984: val_loss -0.3646
2024-12-20 05:45:45.642681: Pseudo dice [0.6704]
2024-12-20 05:45:45.643504: Epoch time: 461.42 s
2024-12-20 05:45:48.141752: 
2024-12-20 05:45:48.143081: Epoch 62
2024-12-20 05:45:48.143882: Current learning rate: 0.00619
2024-12-20 05:54:00.407320: Validation loss did not improve from -0.45362. Patience: 10/50
2024-12-20 05:54:00.408248: train_loss -0.7574
2024-12-20 05:54:00.409242: val_loss -0.3779
2024-12-20 05:54:00.410015: Pseudo dice [0.685]
2024-12-20 05:54:00.410791: Epoch time: 492.27 s
2024-12-20 05:54:01.882253: 
2024-12-20 05:54:01.883553: Epoch 63
2024-12-20 05:54:01.884304: Current learning rate: 0.00612
2024-12-20 06:01:45.031551: Validation loss improved from -0.45362 to -0.45491! Patience: 10/50
2024-12-20 06:01:45.032511: train_loss -0.7556
2024-12-20 06:01:45.033267: val_loss -0.4549
2024-12-20 06:01:45.033957: Pseudo dice [0.7222]
2024-12-20 06:01:45.034729: Epoch time: 463.15 s
2024-12-20 06:01:46.465980: 
2024-12-20 06:01:46.467281: Epoch 64
2024-12-20 06:01:46.467984: Current learning rate: 0.00606
2024-12-20 06:08:56.485847: Validation loss did not improve from -0.45491. Patience: 1/50
2024-12-20 06:08:56.486809: train_loss -0.7563
2024-12-20 06:08:56.487602: val_loss -0.3706
2024-12-20 06:08:56.488240: Pseudo dice [0.6763]
2024-12-20 06:08:56.488896: Epoch time: 430.02 s
2024-12-20 06:08:58.532790: 
2024-12-20 06:08:58.534182: Epoch 65
2024-12-20 06:08:58.535083: Current learning rate: 0.006
2024-12-20 06:16:29.173198: Validation loss did not improve from -0.45491. Patience: 2/50
2024-12-20 06:16:29.174665: train_loss -0.7544
2024-12-20 06:16:29.176629: val_loss -0.4024
2024-12-20 06:16:29.177624: Pseudo dice [0.7052]
2024-12-20 06:16:29.178832: Epoch time: 450.64 s
2024-12-20 06:16:30.661403: 
2024-12-20 06:16:30.662959: Epoch 66
2024-12-20 06:16:30.664126: Current learning rate: 0.00593
2024-12-20 06:24:04.737487: Validation loss did not improve from -0.45491. Patience: 3/50
2024-12-20 06:24:04.738515: train_loss -0.7653
2024-12-20 06:24:04.739374: val_loss -0.3647
2024-12-20 06:24:04.740020: Pseudo dice [0.6802]
2024-12-20 06:24:04.740612: Epoch time: 454.08 s
2024-12-20 06:24:06.287500: 
2024-12-20 06:24:06.288726: Epoch 67
2024-12-20 06:24:06.289751: Current learning rate: 0.00587
2024-12-20 06:30:55.668813: Validation loss did not improve from -0.45491. Patience: 4/50
2024-12-20 06:30:55.669924: train_loss -0.7645
2024-12-20 06:30:55.671061: val_loss -0.433
2024-12-20 06:30:55.672148: Pseudo dice [0.705]
2024-12-20 06:30:55.673131: Epoch time: 409.38 s
2024-12-20 06:30:57.160219: 
2024-12-20 06:30:57.161521: Epoch 68
2024-12-20 06:30:57.162507: Current learning rate: 0.00581
2024-12-20 06:38:42.047844: Validation loss did not improve from -0.45491. Patience: 5/50
2024-12-20 06:38:42.048846: train_loss -0.7691
2024-12-20 06:38:42.049653: val_loss -0.3591
2024-12-20 06:38:42.050518: Pseudo dice [0.6699]
2024-12-20 06:38:42.051301: Epoch time: 464.89 s
2024-12-20 06:38:43.559181: 
2024-12-20 06:38:43.560350: Epoch 69
2024-12-20 06:38:43.561111: Current learning rate: 0.00574
2024-12-20 06:45:49.835440: Validation loss did not improve from -0.45491. Patience: 6/50
2024-12-20 06:45:49.836326: train_loss -0.7678
2024-12-20 06:45:49.837076: val_loss -0.4045
2024-12-20 06:45:49.837800: Pseudo dice [0.6967]
2024-12-20 06:45:49.838423: Epoch time: 426.28 s
2024-12-20 06:45:51.786338: 
2024-12-20 06:45:51.787596: Epoch 70
2024-12-20 06:45:51.788411: Current learning rate: 0.00568
2024-12-20 06:53:10.582605: Validation loss did not improve from -0.45491. Patience: 7/50
2024-12-20 06:53:10.611585: train_loss -0.7694
2024-12-20 06:53:10.612507: val_loss -0.3716
2024-12-20 06:53:10.613276: Pseudo dice [0.6821]
2024-12-20 06:53:10.613877: Epoch time: 438.83 s
2024-12-20 06:53:12.285751: 
2024-12-20 06:53:12.286975: Epoch 71
2024-12-20 06:53:12.287791: Current learning rate: 0.00562
2024-12-20 07:00:40.927423: Validation loss did not improve from -0.45491. Patience: 8/50
2024-12-20 07:00:40.928320: train_loss -0.7667
2024-12-20 07:00:40.929026: val_loss -0.368
2024-12-20 07:00:40.929620: Pseudo dice [0.6782]
2024-12-20 07:00:40.930224: Epoch time: 448.64 s
2024-12-20 07:00:42.823355: 
2024-12-20 07:00:42.825263: Epoch 72
2024-12-20 07:00:42.826010: Current learning rate: 0.00555
2024-12-20 07:08:04.234908: Validation loss did not improve from -0.45491. Patience: 9/50
2024-12-20 07:08:04.237342: train_loss -0.7685
2024-12-20 07:08:04.238224: val_loss -0.3678
2024-12-20 07:08:04.238991: Pseudo dice [0.6911]
2024-12-20 07:08:04.239697: Epoch time: 441.42 s
2024-12-20 07:08:05.796260: 
2024-12-20 07:08:05.797353: Epoch 73
2024-12-20 07:08:05.798160: Current learning rate: 0.00549
2024-12-20 07:15:18.051095: Validation loss did not improve from -0.45491. Patience: 10/50
2024-12-20 07:15:18.052026: train_loss -0.772
2024-12-20 07:15:18.052788: val_loss -0.3773
2024-12-20 07:15:18.053427: Pseudo dice [0.6828]
2024-12-20 07:15:18.054049: Epoch time: 432.26 s
2024-12-20 07:15:19.511478: 
2024-12-20 07:15:19.512594: Epoch 74
2024-12-20 07:15:19.513249: Current learning rate: 0.00542
2024-12-20 07:22:58.282178: Validation loss did not improve from -0.45491. Patience: 11/50
2024-12-20 07:22:58.283063: train_loss -0.7709
2024-12-20 07:22:58.283853: val_loss -0.3759
2024-12-20 07:22:58.284474: Pseudo dice [0.6864]
2024-12-20 07:22:58.285256: Epoch time: 458.77 s
2024-12-20 07:23:00.282801: 
2024-12-20 07:23:00.284025: Epoch 75
2024-12-20 07:23:00.284942: Current learning rate: 0.00536
2024-12-20 07:30:34.998398: Validation loss did not improve from -0.45491. Patience: 12/50
2024-12-20 07:30:35.000444: train_loss -0.7757
2024-12-20 07:30:35.001431: val_loss -0.3586
2024-12-20 07:30:35.002229: Pseudo dice [0.6697]
2024-12-20 07:30:35.002941: Epoch time: 454.72 s
2024-12-20 07:30:36.650520: 
2024-12-20 07:30:36.651664: Epoch 76
2024-12-20 07:30:36.652550: Current learning rate: 0.00529
2024-12-20 07:37:57.624874: Validation loss did not improve from -0.45491. Patience: 13/50
2024-12-20 07:37:57.626240: train_loss -0.7737
2024-12-20 07:37:57.627532: val_loss -0.4107
2024-12-20 07:37:57.628577: Pseudo dice [0.704]
2024-12-20 07:37:57.629435: Epoch time: 440.98 s
2024-12-20 07:37:59.264708: 
2024-12-20 07:37:59.265950: Epoch 77
2024-12-20 07:37:59.267046: Current learning rate: 0.00523
2024-12-20 07:45:24.284284: Validation loss did not improve from -0.45491. Patience: 14/50
2024-12-20 07:45:24.285326: train_loss -0.7732
2024-12-20 07:45:24.286235: val_loss -0.371
2024-12-20 07:45:24.286887: Pseudo dice [0.6859]
2024-12-20 07:45:24.287682: Epoch time: 445.02 s
2024-12-20 07:45:25.772054: 
2024-12-20 07:45:25.773430: Epoch 78
2024-12-20 07:45:25.774380: Current learning rate: 0.00517
2024-12-20 07:52:51.111001: Validation loss did not improve from -0.45491. Patience: 15/50
2024-12-20 07:52:51.112087: train_loss -0.7677
2024-12-20 07:52:51.112867: val_loss -0.4187
2024-12-20 07:52:51.113493: Pseudo dice [0.7157]
2024-12-20 07:52:51.114128: Epoch time: 445.34 s
2024-12-20 07:52:51.114754: Yayy! New best EMA pseudo Dice: 0.6884
2024-12-20 07:52:53.224648: 
2024-12-20 07:52:53.225842: Epoch 79
2024-12-20 07:52:53.226707: Current learning rate: 0.0051
2024-12-20 08:00:07.382218: Validation loss did not improve from -0.45491. Patience: 16/50
2024-12-20 08:00:07.383200: train_loss -0.7777
2024-12-20 08:00:07.383977: val_loss -0.3658
2024-12-20 08:00:07.384707: Pseudo dice [0.6705]
2024-12-20 08:00:07.385429: Epoch time: 434.16 s
2024-12-20 08:00:09.344332: 
2024-12-20 08:00:09.345407: Epoch 80
2024-12-20 08:00:09.346138: Current learning rate: 0.00504
2024-12-20 08:07:38.133801: Validation loss did not improve from -0.45491. Patience: 17/50
2024-12-20 08:07:38.135517: train_loss -0.7796
2024-12-20 08:07:38.136466: val_loss -0.3821
2024-12-20 08:07:38.137357: Pseudo dice [0.6954]
2024-12-20 08:07:38.138245: Epoch time: 448.79 s
2024-12-20 08:07:39.621729: 
2024-12-20 08:07:39.623138: Epoch 81
2024-12-20 08:07:39.624081: Current learning rate: 0.00497
2024-12-20 08:15:27.188590: Validation loss did not improve from -0.45491. Patience: 18/50
2024-12-20 08:15:27.189534: train_loss -0.7757
2024-12-20 08:15:27.190271: val_loss -0.4035
2024-12-20 08:15:27.190948: Pseudo dice [0.7016]
2024-12-20 08:15:27.191686: Epoch time: 467.57 s
2024-12-20 08:15:27.192287: Yayy! New best EMA pseudo Dice: 0.6889
2024-12-20 08:15:29.112541: 
2024-12-20 08:15:29.113893: Epoch 82
2024-12-20 08:15:29.114632: Current learning rate: 0.00491
2024-12-20 08:22:52.601337: Validation loss did not improve from -0.45491. Patience: 19/50
2024-12-20 08:22:52.602356: train_loss -0.7796
2024-12-20 08:22:52.603157: val_loss -0.3763
2024-12-20 08:22:52.603912: Pseudo dice [0.6864]
2024-12-20 08:22:52.604648: Epoch time: 443.49 s
2024-12-20 08:22:54.527217: 
2024-12-20 08:22:54.528508: Epoch 83
2024-12-20 08:22:54.529296: Current learning rate: 0.00484
2024-12-20 08:30:41.785228: Validation loss did not improve from -0.45491. Patience: 20/50
2024-12-20 08:30:41.786698: train_loss -0.7809
2024-12-20 08:30:41.788081: val_loss -0.3754
2024-12-20 08:30:41.788795: Pseudo dice [0.6912]
2024-12-20 08:30:41.789787: Epoch time: 467.26 s
2024-12-20 08:30:41.790657: Yayy! New best EMA pseudo Dice: 0.6889
2024-12-20 08:30:43.655496: 
2024-12-20 08:30:43.656742: Epoch 84
2024-12-20 08:30:43.657707: Current learning rate: 0.00478
2024-12-20 08:38:25.390000: Validation loss did not improve from -0.45491. Patience: 21/50
2024-12-20 08:38:25.391041: train_loss -0.7801
2024-12-20 08:38:25.391717: val_loss -0.3656
2024-12-20 08:38:25.392354: Pseudo dice [0.6788]
2024-12-20 08:38:25.392987: Epoch time: 461.74 s
2024-12-20 08:38:27.217918: 
2024-12-20 08:38:27.219779: Epoch 85
2024-12-20 08:38:27.220933: Current learning rate: 0.00471
2024-12-20 08:45:51.427689: Validation loss did not improve from -0.45491. Patience: 22/50
2024-12-20 08:45:51.428670: train_loss -0.7845
2024-12-20 08:45:51.429494: val_loss -0.3815
2024-12-20 08:45:51.430237: Pseudo dice [0.6863]
2024-12-20 08:45:51.431036: Epoch time: 444.21 s
2024-12-20 08:45:52.805562: 
2024-12-20 08:45:52.806725: Epoch 86
2024-12-20 08:45:52.807459: Current learning rate: 0.00465
2024-12-20 08:53:39.214678: Validation loss did not improve from -0.45491. Patience: 23/50
2024-12-20 08:53:39.215520: train_loss -0.7872
2024-12-20 08:53:39.216148: val_loss -0.3734
2024-12-20 08:53:39.216870: Pseudo dice [0.6834]
2024-12-20 08:53:39.217505: Epoch time: 466.41 s
2024-12-20 08:53:40.642314: 
2024-12-20 08:53:40.643226: Epoch 87
2024-12-20 08:53:40.643906: Current learning rate: 0.00458
2024-12-20 09:01:36.378225: Validation loss did not improve from -0.45491. Patience: 24/50
2024-12-20 09:01:36.379147: train_loss -0.7835
2024-12-20 09:01:36.380011: val_loss -0.3902
2024-12-20 09:01:36.380779: Pseudo dice [0.6901]
2024-12-20 09:01:36.381552: Epoch time: 475.74 s
2024-12-20 09:01:37.747262: 
2024-12-20 09:01:37.748292: Epoch 88
2024-12-20 09:01:37.749058: Current learning rate: 0.00452
2024-12-20 09:09:37.318987: Validation loss did not improve from -0.45491. Patience: 25/50
2024-12-20 09:09:37.319807: train_loss -0.7856
2024-12-20 09:09:37.320577: val_loss -0.4322
2024-12-20 09:09:37.321290: Pseudo dice [0.7086]
2024-12-20 09:09:37.322130: Epoch time: 479.57 s
2024-12-20 09:09:37.322856: Yayy! New best EMA pseudo Dice: 0.6897
2024-12-20 09:09:39.051459: 
2024-12-20 09:09:39.052704: Epoch 89
2024-12-20 09:09:39.053416: Current learning rate: 0.00445
2024-12-20 09:17:45.332263: Validation loss did not improve from -0.45491. Patience: 26/50
2024-12-20 09:17:45.333775: train_loss -0.7898
2024-12-20 09:17:45.334437: val_loss -0.3785
2024-12-20 09:17:45.335088: Pseudo dice [0.6983]
2024-12-20 09:17:45.335799: Epoch time: 486.28 s
2024-12-20 09:17:45.776527: Yayy! New best EMA pseudo Dice: 0.6905
2024-12-20 09:17:47.719252: 
2024-12-20 09:17:47.720879: Epoch 90
2024-12-20 09:17:47.721993: Current learning rate: 0.00438
2024-12-20 09:25:24.514897: Validation loss did not improve from -0.45491. Patience: 27/50
2024-12-20 09:25:24.516844: train_loss -0.791
2024-12-20 09:25:24.517701: val_loss -0.3665
2024-12-20 09:25:24.518318: Pseudo dice [0.6838]
2024-12-20 09:25:24.519096: Epoch time: 456.8 s
2024-12-20 09:25:26.044593: 
2024-12-20 09:25:26.045797: Epoch 91
2024-12-20 09:25:26.046528: Current learning rate: 0.00432
2024-12-20 09:32:55.568374: Validation loss did not improve from -0.45491. Patience: 28/50
2024-12-20 09:32:55.569568: train_loss -0.7909
2024-12-20 09:32:55.570524: val_loss -0.4447
2024-12-20 09:32:55.571296: Pseudo dice [0.7228]
2024-12-20 09:32:55.572056: Epoch time: 449.53 s
2024-12-20 09:32:55.572834: Yayy! New best EMA pseudo Dice: 0.6932
2024-12-20 09:32:57.442725: 
2024-12-20 09:32:57.443812: Epoch 92
2024-12-20 09:32:57.444705: Current learning rate: 0.00425
2024-12-20 09:40:32.397900: Validation loss did not improve from -0.45491. Patience: 29/50
2024-12-20 09:40:32.398780: train_loss -0.7883
2024-12-20 09:40:32.399826: val_loss -0.3852
2024-12-20 09:40:32.400671: Pseudo dice [0.7002]
2024-12-20 09:40:32.401352: Epoch time: 454.96 s
2024-12-20 09:40:32.402134: Yayy! New best EMA pseudo Dice: 0.6939
2024-12-20 09:40:34.191844: 
2024-12-20 09:40:34.193159: Epoch 93
2024-12-20 09:40:34.194008: Current learning rate: 0.00419
2024-12-20 09:48:27.525505: Validation loss did not improve from -0.45491. Patience: 30/50
2024-12-20 09:48:27.526328: train_loss -0.793
2024-12-20 09:48:27.527235: val_loss -0.4083
2024-12-20 09:48:27.528234: Pseudo dice [0.6981]
2024-12-20 09:48:27.529124: Epoch time: 473.34 s
2024-12-20 09:48:27.530028: Yayy! New best EMA pseudo Dice: 0.6943
2024-12-20 09:48:29.893332: 
2024-12-20 09:48:29.895278: Epoch 94
2024-12-20 09:48:29.896298: Current learning rate: 0.00412
2024-12-20 09:55:38.684476: Validation loss did not improve from -0.45491. Patience: 31/50
2024-12-20 09:55:38.685476: train_loss -0.7938
2024-12-20 09:55:38.686495: val_loss -0.4204
2024-12-20 09:55:38.687432: Pseudo dice [0.7018]
2024-12-20 09:55:38.688571: Epoch time: 428.79 s
2024-12-20 09:55:39.068180: Yayy! New best EMA pseudo Dice: 0.695
2024-12-20 09:55:40.979824: 
2024-12-20 09:55:40.981310: Epoch 95
2024-12-20 09:55:40.982381: Current learning rate: 0.00405
2024-12-20 10:03:10.275119: Validation loss did not improve from -0.45491. Patience: 32/50
2024-12-20 10:03:10.276044: train_loss -0.796
2024-12-20 10:03:10.276900: val_loss -0.3897
2024-12-20 10:03:10.277748: Pseudo dice [0.6914]
2024-12-20 10:03:10.278427: Epoch time: 449.3 s
2024-12-20 10:03:11.704889: 
2024-12-20 10:03:11.706110: Epoch 96
2024-12-20 10:03:11.706842: Current learning rate: 0.00399
2024-12-20 10:10:56.913517: Validation loss did not improve from -0.45491. Patience: 33/50
2024-12-20 10:10:56.914488: train_loss -0.7975
2024-12-20 10:10:56.915318: val_loss -0.3567
2024-12-20 10:10:56.916116: Pseudo dice [0.6725]
2024-12-20 10:10:56.916747: Epoch time: 465.21 s
2024-12-20 10:10:58.428268: 
2024-12-20 10:10:58.429480: Epoch 97
2024-12-20 10:10:58.430135: Current learning rate: 0.00392
2024-12-20 10:18:11.804749: Validation loss did not improve from -0.45491. Patience: 34/50
2024-12-20 10:18:11.805655: train_loss -0.7987
2024-12-20 10:18:11.806585: val_loss -0.3041
2024-12-20 10:18:11.807342: Pseudo dice [0.6664]
2024-12-20 10:18:11.808006: Epoch time: 433.38 s
2024-12-20 10:18:13.221263: 
2024-12-20 10:18:13.222636: Epoch 98
2024-12-20 10:18:13.223596: Current learning rate: 0.00385
2024-12-20 10:25:19.701202: Validation loss did not improve from -0.45491. Patience: 35/50
2024-12-20 10:25:19.702189: train_loss -0.7934
2024-12-20 10:25:19.702972: val_loss -0.3614
2024-12-20 10:25:19.703671: Pseudo dice [0.6906]
2024-12-20 10:25:19.704520: Epoch time: 426.48 s
2024-12-20 10:25:21.181874: 
2024-12-20 10:25:21.183241: Epoch 99
2024-12-20 10:25:21.184028: Current learning rate: 0.00379
2024-12-20 10:32:54.589298: Validation loss did not improve from -0.45491. Patience: 36/50
2024-12-20 10:32:54.591173: train_loss -0.799
2024-12-20 10:32:54.592224: val_loss -0.3653
2024-12-20 10:32:54.592959: Pseudo dice [0.6753]
2024-12-20 10:32:54.593726: Epoch time: 453.41 s
2024-12-20 10:32:56.509398: 
2024-12-20 10:32:56.510669: Epoch 100
2024-12-20 10:32:56.511678: Current learning rate: 0.00372
2024-12-20 10:40:16.005560: Validation loss did not improve from -0.45491. Patience: 37/50
2024-12-20 10:40:16.007237: train_loss -0.7984
2024-12-20 10:40:16.008792: val_loss -0.3833
2024-12-20 10:40:16.009663: Pseudo dice [0.6916]
2024-12-20 10:40:16.010925: Epoch time: 439.5 s
2024-12-20 10:40:17.501891: 
2024-12-20 10:40:17.503157: Epoch 101
2024-12-20 10:40:17.504188: Current learning rate: 0.00365
2024-12-20 10:46:08.926129: Validation loss did not improve from -0.45491. Patience: 38/50
2024-12-20 10:46:08.927275: train_loss -0.7986
2024-12-20 10:46:08.928115: val_loss -0.3847
2024-12-20 10:46:08.928862: Pseudo dice [0.6919]
2024-12-20 10:46:08.929512: Epoch time: 351.43 s
2024-12-20 10:46:10.343540: 
2024-12-20 10:46:10.344751: Epoch 102
2024-12-20 10:46:10.345383: Current learning rate: 0.00359
2024-12-20 10:54:25.381734: Validation loss did not improve from -0.45491. Patience: 39/50
2024-12-20 10:54:25.382509: train_loss -0.7976
2024-12-20 10:54:25.383219: val_loss -0.3684
2024-12-20 10:54:25.383842: Pseudo dice [0.6814]
2024-12-20 10:54:25.384577: Epoch time: 495.04 s
2024-12-20 10:54:26.920165: 
2024-12-20 10:54:26.921008: Epoch 103
2024-12-20 10:54:26.921807: Current learning rate: 0.00352
2024-12-20 11:01:58.707228: Validation loss did not improve from -0.45491. Patience: 40/50
2024-12-20 11:01:58.708205: train_loss -0.8012
2024-12-20 11:01:58.708966: val_loss -0.4185
2024-12-20 11:01:58.709702: Pseudo dice [0.7139]
2024-12-20 11:01:58.710441: Epoch time: 451.79 s
2024-12-20 11:02:00.287836: 
2024-12-20 11:02:00.288996: Epoch 104
2024-12-20 11:02:00.289920: Current learning rate: 0.00345
2024-12-20 11:08:59.585651: Validation loss did not improve from -0.45491. Patience: 41/50
2024-12-20 11:08:59.586616: train_loss -0.8009
2024-12-20 11:08:59.587467: val_loss -0.3419
2024-12-20 11:08:59.588180: Pseudo dice [0.6702]
2024-12-20 11:08:59.588809: Epoch time: 419.3 s
2024-12-20 11:09:02.004728: 
2024-12-20 11:09:02.005800: Epoch 105
2024-12-20 11:09:02.006452: Current learning rate: 0.00338
2024-12-20 11:16:43.498008: Validation loss did not improve from -0.45491. Patience: 42/50
2024-12-20 11:16:43.498946: train_loss -0.801
2024-12-20 11:16:43.499657: val_loss -0.3971
2024-12-20 11:16:43.500335: Pseudo dice [0.7026]
2024-12-20 11:16:43.501019: Epoch time: 461.5 s
2024-12-20 11:16:44.960400: 
2024-12-20 11:16:44.961439: Epoch 106
2024-12-20 11:16:44.962125: Current learning rate: 0.00332
2024-12-20 11:24:23.183100: Validation loss did not improve from -0.45491. Patience: 43/50
2024-12-20 11:24:23.183924: train_loss -0.7996
2024-12-20 11:24:23.184577: val_loss -0.3518
2024-12-20 11:24:23.185158: Pseudo dice [0.6855]
2024-12-20 11:24:23.185784: Epoch time: 458.22 s
2024-12-20 11:24:24.931478: 
2024-12-20 11:24:24.932826: Epoch 107
2024-12-20 11:24:24.933560: Current learning rate: 0.00325
2024-12-20 11:31:31.747890: Validation loss did not improve from -0.45491. Patience: 44/50
2024-12-20 11:31:31.750113: train_loss -0.8005
2024-12-20 11:31:31.751256: val_loss -0.346
2024-12-20 11:31:31.752182: Pseudo dice [0.6803]
2024-12-20 11:31:31.753155: Epoch time: 426.82 s
2024-12-20 11:31:33.338001: 
2024-12-20 11:31:33.339443: Epoch 108
2024-12-20 11:31:33.340589: Current learning rate: 0.00318
2024-12-20 11:39:23.476563: Validation loss did not improve from -0.45491. Patience: 45/50
2024-12-20 11:39:23.478323: train_loss -0.8065
2024-12-20 11:39:23.480031: val_loss -0.3864
2024-12-20 11:39:23.480953: Pseudo dice [0.6938]
2024-12-20 11:39:23.482046: Epoch time: 470.14 s
2024-12-20 11:39:25.181592: 
2024-12-20 11:39:25.183055: Epoch 109
2024-12-20 11:39:25.183857: Current learning rate: 0.00311
2024-12-20 11:46:56.532132: Validation loss did not improve from -0.45491. Patience: 46/50
2024-12-20 11:46:56.533944: train_loss -0.8045
2024-12-20 11:46:56.534829: val_loss -0.3886
2024-12-20 11:46:56.535674: Pseudo dice [0.6983]
2024-12-20 11:46:56.536605: Epoch time: 451.35 s
2024-12-20 11:46:58.533159: 
2024-12-20 11:46:58.534162: Epoch 110
2024-12-20 11:46:58.535066: Current learning rate: 0.00304
2024-12-20 11:54:29.509378: Validation loss did not improve from -0.45491. Patience: 47/50
2024-12-20 11:54:29.510427: train_loss -0.8049
2024-12-20 11:54:29.511241: val_loss -0.403
2024-12-20 11:54:29.512033: Pseudo dice [0.7012]
2024-12-20 11:54:29.512904: Epoch time: 450.98 s
2024-12-20 11:54:30.985985: 
2024-12-20 11:54:30.987346: Epoch 111
2024-12-20 11:54:30.988147: Current learning rate: 0.00297
2024-12-20 12:01:57.040379: Validation loss did not improve from -0.45491. Patience: 48/50
2024-12-20 12:01:57.041419: train_loss -0.8074
2024-12-20 12:01:57.042295: val_loss -0.3518
2024-12-20 12:01:57.043032: Pseudo dice [0.6805]
2024-12-20 12:01:57.043845: Epoch time: 446.06 s
2024-12-20 12:01:58.504219: 
2024-12-20 12:01:58.505491: Epoch 112
2024-12-20 12:01:58.506323: Current learning rate: 0.00291
2024-12-20 12:09:13.370658: Validation loss did not improve from -0.45491. Patience: 49/50
2024-12-20 12:09:13.371583: train_loss -0.807
2024-12-20 12:09:13.372275: val_loss -0.3864
2024-12-20 12:09:13.373047: Pseudo dice [0.6941]
2024-12-20 12:09:13.373780: Epoch time: 434.87 s
2024-12-20 12:09:14.800575: 
2024-12-20 12:09:14.801830: Epoch 113
2024-12-20 12:09:14.802816: Current learning rate: 0.00284
2024-12-20 12:16:16.677622: Validation loss did not improve from -0.45491. Patience: 50/50
2024-12-20 12:16:16.678923: train_loss -0.8084
2024-12-20 12:16:16.679885: val_loss -0.3361
2024-12-20 12:16:16.680768: Pseudo dice [0.678]
2024-12-20 12:16:16.681796: Epoch time: 421.88 s
2024-12-20 12:16:18.132603: 
2024-12-20 12:16:18.134039: Epoch 114
2024-12-20 12:16:18.134894: Current learning rate: 0.00277
2024-12-20 12:23:25.422176: Validation loss did not improve from -0.45491. Patience: 51/50
2024-12-20 12:23:25.423161: train_loss -0.8084
2024-12-20 12:23:25.423883: val_loss -0.3737
2024-12-20 12:23:25.424555: Pseudo dice [0.6853]
2024-12-20 12:23:25.425249: Epoch time: 427.29 s
2024-12-20 12:23:27.362286: 
2024-12-20 12:23:27.363424: Epoch 115
2024-12-20 12:23:27.364300: Current learning rate: 0.0027
2024-12-20 12:31:00.555027: Validation loss did not improve from -0.45491. Patience: 52/50
2024-12-20 12:31:00.555841: train_loss -0.8088
2024-12-20 12:31:00.556712: val_loss -0.3748
2024-12-20 12:31:00.557347: Pseudo dice [0.6885]
2024-12-20 12:31:00.557991: Epoch time: 453.19 s
2024-12-20 12:31:02.488813: 
2024-12-20 12:31:02.490027: Epoch 116
2024-12-20 12:31:02.490719: Current learning rate: 0.00263
2024-12-20 12:38:20.316643: Validation loss did not improve from -0.45491. Patience: 53/50
2024-12-20 12:38:20.318766: train_loss -0.8104
2024-12-20 12:38:20.319617: val_loss -0.3614
2024-12-20 12:38:20.320266: Pseudo dice [0.6858]
2024-12-20 12:38:20.320993: Epoch time: 437.83 s
2024-12-20 12:38:21.783475: 
2024-12-20 12:38:21.784508: Epoch 117
2024-12-20 12:38:21.785274: Current learning rate: 0.00256
2024-12-20 12:45:46.691776: Validation loss did not improve from -0.45491. Patience: 54/50
2024-12-20 12:45:46.693244: train_loss -0.8088
2024-12-20 12:45:46.694938: val_loss -0.3966
2024-12-20 12:45:46.695640: Pseudo dice [0.6957]
2024-12-20 12:45:46.696674: Epoch time: 444.91 s
2024-12-20 12:45:48.122334: 
2024-12-20 12:45:48.123626: Epoch 118
2024-12-20 12:45:48.124449: Current learning rate: 0.00249
2024-12-20 12:53:36.343287: Validation loss did not improve from -0.45491. Patience: 55/50
2024-12-20 12:53:36.344133: train_loss -0.8109
2024-12-20 12:53:36.344885: val_loss -0.3907
2024-12-20 12:53:36.345521: Pseudo dice [0.7012]
2024-12-20 12:53:36.347399: Epoch time: 468.22 s
2024-12-20 12:53:37.789590: 
2024-12-20 12:53:37.790771: Epoch 119
2024-12-20 12:53:37.791472: Current learning rate: 0.00242
2024-12-20 13:01:14.669466: Validation loss did not improve from -0.45491. Patience: 56/50
2024-12-20 13:01:14.670371: train_loss -0.8103
2024-12-20 13:01:14.671214: val_loss -0.386
2024-12-20 13:01:14.672010: Pseudo dice [0.695]
2024-12-20 13:01:14.672853: Epoch time: 456.88 s
2024-12-20 13:01:16.737259: 
2024-12-20 13:01:16.738621: Epoch 120
2024-12-20 13:01:16.739743: Current learning rate: 0.00235
2024-12-20 13:09:42.463555: Validation loss did not improve from -0.45491. Patience: 57/50
2024-12-20 13:09:42.464403: train_loss -0.8106
2024-12-20 13:09:42.465437: val_loss -0.3802
2024-12-20 13:09:42.466468: Pseudo dice [0.6932]
2024-12-20 13:09:42.467335: Epoch time: 505.73 s
2024-12-20 13:09:44.027150: 
2024-12-20 13:09:44.028316: Epoch 121
2024-12-20 13:09:44.029119: Current learning rate: 0.00228
2024-12-20 13:17:27.329644: Validation loss did not improve from -0.45491. Patience: 58/50
2024-12-20 13:17:27.330335: train_loss -0.8106
2024-12-20 13:17:27.331381: val_loss -0.3509
2024-12-20 13:17:27.332170: Pseudo dice [0.6761]
2024-12-20 13:17:27.333013: Epoch time: 463.3 s
2024-12-20 13:17:28.754619: 
2024-12-20 13:17:28.755878: Epoch 122
2024-12-20 13:17:28.756807: Current learning rate: 0.00221
2024-12-20 13:25:15.864787: Validation loss did not improve from -0.45491. Patience: 59/50
2024-12-20 13:25:15.865655: train_loss -0.8118
2024-12-20 13:25:15.866681: val_loss -0.3865
2024-12-20 13:25:15.867592: Pseudo dice [0.6988]
2024-12-20 13:25:15.868565: Epoch time: 467.11 s
2024-12-20 13:25:17.337938: 
2024-12-20 13:25:17.339054: Epoch 123
2024-12-20 13:25:17.339850: Current learning rate: 0.00214
2024-12-20 13:32:45.898018: Validation loss did not improve from -0.45491. Patience: 60/50
2024-12-20 13:32:45.899688: train_loss -0.8144
2024-12-20 13:32:45.900495: val_loss -0.3756
2024-12-20 13:32:45.901215: Pseudo dice [0.6873]
2024-12-20 13:32:45.902055: Epoch time: 448.56 s
2024-12-20 13:32:47.341599: 
2024-12-20 13:32:47.342522: Epoch 124
2024-12-20 13:32:47.343346: Current learning rate: 0.00207
2024-12-20 13:41:28.273578: Validation loss did not improve from -0.45491. Patience: 61/50
2024-12-20 13:41:28.275701: train_loss -0.8118
2024-12-20 13:41:28.276509: val_loss -0.3893
2024-12-20 13:41:28.277212: Pseudo dice [0.6932]
2024-12-20 13:41:28.277885: Epoch time: 520.94 s
2024-12-20 13:41:30.643823: 
2024-12-20 13:41:30.645780: Epoch 125
2024-12-20 13:41:30.646801: Current learning rate: 0.00199
2024-12-20 13:49:00.809857: Validation loss did not improve from -0.45491. Patience: 62/50
2024-12-20 13:49:00.811387: train_loss -0.8128
2024-12-20 13:49:00.812291: val_loss -0.3591
2024-12-20 13:49:00.813128: Pseudo dice [0.6853]
2024-12-20 13:49:00.814162: Epoch time: 450.17 s
2024-12-20 13:49:02.317933: 
2024-12-20 13:49:02.319034: Epoch 126
2024-12-20 13:49:02.319650: Current learning rate: 0.00192
2024-12-20 13:56:24.360813: Validation loss did not improve from -0.45491. Patience: 63/50
2024-12-20 13:56:24.361865: train_loss -0.8135
2024-12-20 13:56:24.362689: val_loss -0.3983
2024-12-20 13:56:24.363492: Pseudo dice [0.7057]
2024-12-20 13:56:24.364179: Epoch time: 442.05 s
2024-12-20 13:56:27.912736: 
2024-12-20 13:56:27.914037: Epoch 127
2024-12-20 13:56:27.914773: Current learning rate: 0.00185
2024-12-20 14:04:10.138934: Validation loss did not improve from -0.45491. Patience: 64/50
2024-12-20 14:04:10.139961: train_loss -0.8138
2024-12-20 14:04:10.140889: val_loss -0.3509
2024-12-20 14:04:10.141587: Pseudo dice [0.6827]
2024-12-20 14:04:10.142280: Epoch time: 462.23 s
2024-12-20 14:04:11.599663: 
2024-12-20 14:04:11.600626: Epoch 128
2024-12-20 14:04:11.601343: Current learning rate: 0.00178
2024-12-20 14:09:57.663908: Validation loss did not improve from -0.45491. Patience: 65/50
2024-12-20 14:09:57.664707: train_loss -0.8168
2024-12-20 14:09:57.665635: val_loss -0.3746
2024-12-20 14:09:57.666355: Pseudo dice [0.6889]
2024-12-20 14:09:57.667155: Epoch time: 346.07 s
2024-12-20 14:09:59.148535: 
2024-12-20 14:09:59.149570: Epoch 129
2024-12-20 14:09:59.150308: Current learning rate: 0.0017
2024-12-20 14:15:25.810555: Validation loss did not improve from -0.45491. Patience: 66/50
2024-12-20 14:15:25.812208: train_loss -0.8172
2024-12-20 14:15:25.813679: val_loss -0.388
2024-12-20 14:15:25.814358: Pseudo dice [0.7014]
2024-12-20 14:15:25.815023: Epoch time: 326.67 s
2024-12-20 14:15:27.646058: 
2024-12-20 14:15:27.646958: Epoch 130
2024-12-20 14:15:27.647603: Current learning rate: 0.00163
2024-12-20 14:20:52.831476: Validation loss did not improve from -0.45491. Patience: 67/50
2024-12-20 14:20:52.832596: train_loss -0.8171
2024-12-20 14:20:52.833540: val_loss -0.3443
2024-12-20 14:20:52.834355: Pseudo dice [0.6809]
2024-12-20 14:20:52.835204: Epoch time: 325.19 s
2024-12-20 14:20:54.325052: 
2024-12-20 14:20:54.326082: Epoch 131
2024-12-20 14:20:54.326906: Current learning rate: 0.00156
2024-12-20 14:26:14.455245: Validation loss did not improve from -0.45491. Patience: 68/50
2024-12-20 14:26:14.456095: train_loss -0.8158
2024-12-20 14:26:14.456997: val_loss -0.3424
2024-12-20 14:26:14.457661: Pseudo dice [0.6928]
2024-12-20 14:26:14.458347: Epoch time: 320.13 s
2024-12-20 14:26:15.951878: 
2024-12-20 14:26:15.953125: Epoch 132
2024-12-20 14:26:15.953966: Current learning rate: 0.00148
2024-12-20 14:31:48.068743: Validation loss did not improve from -0.45491. Patience: 69/50
2024-12-20 14:31:48.069947: train_loss -0.8156
2024-12-20 14:31:48.070952: val_loss -0.404
2024-12-20 14:31:48.071688: Pseudo dice [0.7069]
2024-12-20 14:31:48.072445: Epoch time: 332.12 s
2024-12-20 14:31:49.531088: 
2024-12-20 14:31:49.532114: Epoch 133
2024-12-20 14:31:49.532824: Current learning rate: 0.00141
2024-12-20 14:36:41.746302: Validation loss did not improve from -0.45491. Patience: 70/50
2024-12-20 14:36:41.747310: train_loss -0.8205
2024-12-20 14:36:41.748153: val_loss -0.3575
2024-12-20 14:36:41.748919: Pseudo dice [0.6981]
2024-12-20 14:36:41.749625: Epoch time: 292.22 s
2024-12-20 14:36:43.233109: 
2024-12-20 14:36:43.234391: Epoch 134
2024-12-20 14:36:43.235347: Current learning rate: 0.00133
2024-12-20 14:42:16.004568: Validation loss did not improve from -0.45491. Patience: 71/50
2024-12-20 14:42:16.005627: train_loss -0.818
2024-12-20 14:42:16.006530: val_loss -0.3868
2024-12-20 14:42:16.007356: Pseudo dice [0.6884]
2024-12-20 14:42:16.008145: Epoch time: 332.77 s
2024-12-20 14:42:18.085879: 
2024-12-20 14:42:18.086912: Epoch 135
2024-12-20 14:42:18.087709: Current learning rate: 0.00126
2024-12-20 14:47:40.729142: Validation loss did not improve from -0.45491. Patience: 72/50
2024-12-20 14:47:40.730156: train_loss -0.8186
2024-12-20 14:47:40.730940: val_loss -0.386
2024-12-20 14:47:40.731632: Pseudo dice [0.6995]
2024-12-20 14:47:40.732323: Epoch time: 322.65 s
2024-12-20 14:47:42.269794: 
2024-12-20 14:47:42.271068: Epoch 136
2024-12-20 14:47:42.271911: Current learning rate: 0.00118
2024-12-20 14:52:50.507237: Validation loss did not improve from -0.45491. Patience: 73/50
2024-12-20 14:52:50.509156: train_loss -0.8188
2024-12-20 14:52:50.510204: val_loss -0.3734
2024-12-20 14:52:50.510849: Pseudo dice [0.6811]
2024-12-20 14:52:50.511455: Epoch time: 308.24 s
2024-12-20 14:52:52.573091: 
2024-12-20 14:52:52.574387: Epoch 137
2024-12-20 14:52:52.575251: Current learning rate: 0.00111
2024-12-20 14:58:14.041673: Validation loss did not improve from -0.45491. Patience: 74/50
2024-12-20 14:58:14.043383: train_loss -0.8189
2024-12-20 14:58:14.044405: val_loss -0.3917
2024-12-20 14:58:14.045336: Pseudo dice [0.6947]
2024-12-20 14:58:14.046597: Epoch time: 321.47 s
2024-12-20 14:58:15.668708: 
2024-12-20 14:58:15.669776: Epoch 138
2024-12-20 14:58:15.670738: Current learning rate: 0.00103
2024-12-20 15:03:18.605160: Validation loss did not improve from -0.45491. Patience: 75/50
2024-12-20 15:03:18.606578: train_loss -0.8209
2024-12-20 15:03:18.607337: val_loss -0.356
2024-12-20 15:03:18.608113: Pseudo dice [0.681]
2024-12-20 15:03:18.608910: Epoch time: 302.94 s
2024-12-20 15:03:20.102042: 
2024-12-20 15:03:20.103178: Epoch 139
2024-12-20 15:03:20.103863: Current learning rate: 0.00095
2024-12-20 15:08:38.871024: Validation loss did not improve from -0.45491. Patience: 76/50
2024-12-20 15:08:38.871986: train_loss -0.819
2024-12-20 15:08:38.872960: val_loss -0.3995
2024-12-20 15:08:38.873719: Pseudo dice [0.6981]
2024-12-20 15:08:38.874459: Epoch time: 318.77 s
2024-12-20 15:08:40.802782: 
2024-12-20 15:08:40.804079: Epoch 140
2024-12-20 15:08:40.804778: Current learning rate: 0.00087
2024-12-20 15:14:11.043179: Validation loss did not improve from -0.45491. Patience: 77/50
2024-12-20 15:14:11.044100: train_loss -0.8188
2024-12-20 15:14:11.045130: val_loss -0.3734
2024-12-20 15:14:11.045986: Pseudo dice [0.6888]
2024-12-20 15:14:11.046869: Epoch time: 330.24 s
2024-12-20 15:14:12.550637: 
2024-12-20 15:14:12.551841: Epoch 141
2024-12-20 15:14:12.552703: Current learning rate: 0.00079
2024-12-20 15:19:20.520294: Validation loss did not improve from -0.45491. Patience: 78/50
2024-12-20 15:19:20.521220: train_loss -0.8193
2024-12-20 15:19:20.522082: val_loss -0.3874
2024-12-20 15:19:20.522810: Pseudo dice [0.7004]
2024-12-20 15:19:20.523615: Epoch time: 307.97 s
2024-12-20 15:19:22.021799: 
2024-12-20 15:19:22.022851: Epoch 142
2024-12-20 15:19:22.023633: Current learning rate: 0.00071
2024-12-20 15:24:50.215654: Validation loss did not improve from -0.45491. Patience: 79/50
2024-12-20 15:24:50.216509: train_loss -0.824
2024-12-20 15:24:50.217200: val_loss -0.3855
2024-12-20 15:24:50.217861: Pseudo dice [0.7061]
2024-12-20 15:24:50.218565: Epoch time: 328.2 s
2024-12-20 15:24:51.691865: 
2024-12-20 15:24:51.692993: Epoch 143
2024-12-20 15:24:51.693770: Current learning rate: 0.00063
2024-12-20 15:30:19.057721: Validation loss did not improve from -0.45491. Patience: 80/50
2024-12-20 15:30:19.058842: train_loss -0.8225
2024-12-20 15:30:19.059638: val_loss -0.3952
2024-12-20 15:30:19.060288: Pseudo dice [0.6998]
2024-12-20 15:30:19.060976: Epoch time: 327.37 s
2024-12-20 15:30:20.564960: 
2024-12-20 15:30:20.566133: Epoch 144
2024-12-20 15:30:20.566962: Current learning rate: 0.00055
2024-12-20 15:35:45.007565: Validation loss did not improve from -0.45491. Patience: 81/50
2024-12-20 15:35:45.008448: train_loss -0.8218
2024-12-20 15:35:45.009243: val_loss -0.349
2024-12-20 15:35:45.010018: Pseudo dice [0.6858]
2024-12-20 15:35:45.010739: Epoch time: 324.44 s
2024-12-20 15:35:47.036294: 
2024-12-20 15:35:47.037584: Epoch 145
2024-12-20 15:35:47.038377: Current learning rate: 0.00047
2024-12-20 15:41:19.163235: Validation loss did not improve from -0.45491. Patience: 82/50
2024-12-20 15:41:19.164199: train_loss -0.8209
2024-12-20 15:41:19.164906: val_loss -0.3881
2024-12-20 15:41:19.165604: Pseudo dice [0.7046]
2024-12-20 15:41:19.166292: Epoch time: 332.13 s
2024-12-20 15:41:20.740611: 
2024-12-20 15:41:20.741710: Epoch 146
2024-12-20 15:41:20.742544: Current learning rate: 0.00038
2024-12-20 15:46:51.249767: Validation loss did not improve from -0.45491. Patience: 83/50
2024-12-20 15:46:51.251240: train_loss -0.821
2024-12-20 15:46:51.252045: val_loss -0.3644
2024-12-20 15:46:51.252742: Pseudo dice [0.6798]
2024-12-20 15:46:51.253497: Epoch time: 330.51 s
2024-12-20 15:46:52.937894: 
2024-12-20 15:46:52.939302: Epoch 147
2024-12-20 15:46:52.940200: Current learning rate: 0.0003
2024-12-20 15:52:13.545262: Validation loss did not improve from -0.45491. Patience: 84/50
2024-12-20 15:52:13.546256: train_loss -0.8216
2024-12-20 15:52:13.547028: val_loss -0.3473
2024-12-20 15:52:13.547745: Pseudo dice [0.6829]
2024-12-20 15:52:13.548346: Epoch time: 320.61 s
2024-12-20 15:52:15.706490: 
2024-12-20 15:52:15.707949: Epoch 148
2024-12-20 15:52:15.708668: Current learning rate: 0.00021
2024-12-20 15:57:39.086996: Validation loss did not improve from -0.45491. Patience: 85/50
2024-12-20 15:57:39.087766: train_loss -0.8228
2024-12-20 15:57:39.088413: val_loss -0.3731
2024-12-20 15:57:39.088996: Pseudo dice [0.6875]
2024-12-20 15:57:39.089606: Epoch time: 323.38 s
2024-12-20 15:57:40.557387: 
2024-12-20 15:57:40.558631: Epoch 149
2024-12-20 15:57:40.559296: Current learning rate: 0.00011
2024-12-20 16:03:11.477045: Validation loss did not improve from -0.45491. Patience: 86/50
2024-12-20 16:03:11.479489: train_loss -0.8239
2024-12-20 16:03:11.480940: val_loss -0.339
2024-12-20 16:03:11.481811: Pseudo dice [0.68]
2024-12-20 16:03:11.482737: Epoch time: 330.92 s
2024-12-20 16:03:13.434979: Training done.
2024-12-20 16:03:13.833235: Using splits from existing split file: /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset308_Sohee_Calcium_OCT_CrossValidation/splits_final_40.json
2024-12-20 16:03:13.839001: The split file contains 5 splits.
2024-12-20 16:03:13.839877: Desired fold for training: 0
2024-12-20 16:03:13.840666: This split has 3 training and 5 validation cases.
2024-12-20 16:03:13.841659: predicting 101-045
2024-12-20 16:03:13.946274: 101-045, shape torch.Size([1, 375, 498, 498]), rank 0
2024-12-20 16:05:35.868398: predicting 106-002
2024-12-20 16:05:35.888339: 106-002, shape torch.Size([1, 539, 498, 498]), rank 0
2024-12-20 16:08:23.922007: predicting 701-013
2024-12-20 16:08:23.961753: 701-013, shape torch.Size([1, 375, 498, 498]), rank 0
2024-12-20 16:10:30.634265: predicting 704-003
2024-12-20 16:10:30.648414: 704-003, shape torch.Size([1, 375, 498, 498]), rank 0
2024-12-20 16:12:29.313504: predicting 706-005
2024-12-20 16:12:29.328913: 706-005, shape torch.Size([1, 375, 498, 498]), rank 0
2024-12-20 16:14:53.411066: Validation complete
2024-12-20 16:14:53.411541: Mean Validation Dice:  0.692017365462586
2024-12-19 21:32:40.495335: unpacking done...
2024-12-19 21:32:40.832563: Unable to plot network architecture: nnUNet_compile is enabled!
2024-12-19 21:32:40.923053: 
2024-12-19 21:32:40.924362: Epoch 0
2024-12-19 21:32:40.925672: Current learning rate: 0.01
2024-12-19 21:43:28.303676: Validation loss improved from 1000.00000 to -0.18769! Patience: 0/50
2024-12-19 21:43:28.304666: train_loss -0.0882
2024-12-19 21:43:28.305478: val_loss -0.1877
2024-12-19 21:43:28.306193: Pseudo dice [0.5389]
2024-12-19 21:43:28.306973: Epoch time: 647.38 s
2024-12-19 21:43:28.307672: Yayy! New best EMA pseudo Dice: 0.5389
2024-12-19 21:43:29.999235: 
2024-12-19 21:43:30.000368: Epoch 1
2024-12-19 21:43:30.001133: Current learning rate: 0.00994
2024-12-19 21:53:00.517883: Validation loss improved from -0.18769 to -0.27436! Patience: 0/50
2024-12-19 21:53:00.518864: train_loss -0.2358
2024-12-19 21:53:00.519675: val_loss -0.2744
2024-12-19 21:53:00.520394: Pseudo dice [0.595]
2024-12-19 21:53:00.521155: Epoch time: 570.52 s
2024-12-19 21:53:00.521798: Yayy! New best EMA pseudo Dice: 0.5445
2024-12-19 21:53:02.425804: 
2024-12-19 21:53:02.427097: Epoch 2
2024-12-19 21:53:02.427775: Current learning rate: 0.00988
2024-12-19 22:02:04.576857: Validation loss improved from -0.27436 to -0.32848! Patience: 0/50
2024-12-19 22:02:04.577905: train_loss -0.3084
2024-12-19 22:02:04.578882: val_loss -0.3285
2024-12-19 22:02:04.579660: Pseudo dice [0.6247]
2024-12-19 22:02:04.580580: Epoch time: 542.15 s
2024-12-19 22:02:04.581353: Yayy! New best EMA pseudo Dice: 0.5526
2024-12-19 22:02:06.431425: 
2024-12-19 22:02:06.432524: Epoch 3
2024-12-19 22:02:06.433457: Current learning rate: 0.00982
2024-12-19 22:11:11.173330: Validation loss did not improve from -0.32848. Patience: 1/50
2024-12-19 22:11:11.174205: train_loss -0.3604
2024-12-19 22:11:11.174900: val_loss -0.3268
2024-12-19 22:11:11.175688: Pseudo dice [0.6192]
2024-12-19 22:11:11.176656: Epoch time: 544.74 s
2024-12-19 22:11:11.177540: Yayy! New best EMA pseudo Dice: 0.5592
2024-12-19 22:11:13.019920: 
2024-12-19 22:11:13.021008: Epoch 4
2024-12-19 22:11:13.022041: Current learning rate: 0.00976
2024-12-19 22:20:23.135645: Validation loss did not improve from -0.32848. Patience: 2/50
2024-12-19 22:20:23.136557: train_loss -0.3305
2024-12-19 22:20:23.137461: val_loss -0.2863
2024-12-19 22:20:23.138328: Pseudo dice [0.5818]
2024-12-19 22:20:23.139304: Epoch time: 550.12 s
2024-12-19 22:20:23.560523: Yayy! New best EMA pseudo Dice: 0.5615
2024-12-19 22:20:25.414848: 
2024-12-19 22:20:25.416036: Epoch 5
2024-12-19 22:20:25.416907: Current learning rate: 0.0097
2024-12-19 22:29:22.275055: Validation loss improved from -0.32848 to -0.36109! Patience: 2/50
2024-12-19 22:29:22.275820: train_loss -0.3899
2024-12-19 22:29:22.276581: val_loss -0.3611
2024-12-19 22:29:22.277297: Pseudo dice [0.6334]
2024-12-19 22:29:22.278063: Epoch time: 536.86 s
2024-12-19 22:29:22.278826: Yayy! New best EMA pseudo Dice: 0.5687
2024-12-19 22:29:24.130436: 
2024-12-19 22:29:24.132109: Epoch 6
2024-12-19 22:29:24.132923: Current learning rate: 0.00964
2024-12-19 22:38:43.019050: Validation loss did not improve from -0.36109. Patience: 1/50
2024-12-19 22:38:43.037589: train_loss -0.4323
2024-12-19 22:38:43.039306: val_loss -0.3407
2024-12-19 22:38:43.039999: Pseudo dice [0.6172]
2024-12-19 22:38:43.040762: Epoch time: 558.91 s
2024-12-19 22:38:43.041378: Yayy! New best EMA pseudo Dice: 0.5735
2024-12-19 22:38:44.881145: 
2024-12-19 22:38:44.882352: Epoch 7
2024-12-19 22:38:44.883028: Current learning rate: 0.00958
2024-12-19 22:47:54.902224: Validation loss did not improve from -0.36109. Patience: 2/50
2024-12-19 22:47:54.902818: train_loss -0.4418
2024-12-19 22:47:54.903613: val_loss -0.3418
2024-12-19 22:47:54.904387: Pseudo dice [0.6346]
2024-12-19 22:47:54.905133: Epoch time: 550.02 s
2024-12-19 22:47:54.905834: Yayy! New best EMA pseudo Dice: 0.5796
2024-12-19 22:47:57.118465: 
2024-12-19 22:47:57.119526: Epoch 8
2024-12-19 22:47:57.120528: Current learning rate: 0.00952
2024-12-19 22:57:06.301354: Validation loss did not improve from -0.36109. Patience: 3/50
2024-12-19 22:57:06.302104: train_loss -0.4543
2024-12-19 22:57:06.302941: val_loss -0.3582
2024-12-19 22:57:06.303648: Pseudo dice [0.6445]
2024-12-19 22:57:06.304394: Epoch time: 549.19 s
2024-12-19 22:57:06.305115: Yayy! New best EMA pseudo Dice: 0.5861
2024-12-19 22:57:08.090652: 
2024-12-19 22:57:08.091736: Epoch 9
2024-12-19 22:57:08.092497: Current learning rate: 0.00946
2024-12-19 23:06:28.951517: Validation loss improved from -0.36109 to -0.39182! Patience: 3/50
2024-12-19 23:06:28.952318: train_loss -0.4602
2024-12-19 23:06:28.953137: val_loss -0.3918
2024-12-19 23:06:28.953988: Pseudo dice [0.6704]
2024-12-19 23:06:28.954715: Epoch time: 560.86 s
2024-12-19 23:06:29.338020: Yayy! New best EMA pseudo Dice: 0.5945
2024-12-19 23:06:31.066482: 
2024-12-19 23:06:31.067693: Epoch 10
2024-12-19 23:06:31.068560: Current learning rate: 0.0094
2024-12-19 23:14:43.420876: Validation loss improved from -0.39182 to -0.43220! Patience: 0/50
2024-12-19 23:14:43.421573: train_loss -0.4945
2024-12-19 23:14:43.422292: val_loss -0.4322
2024-12-19 23:14:43.423073: Pseudo dice [0.6823]
2024-12-19 23:14:43.423714: Epoch time: 492.36 s
2024-12-19 23:14:43.424413: Yayy! New best EMA pseudo Dice: 0.6033
2024-12-19 23:14:45.208530: 
2024-12-19 23:14:45.209352: Epoch 11
2024-12-19 23:14:45.209977: Current learning rate: 0.00934
2024-12-19 23:23:22.828554: Validation loss did not improve from -0.43220. Patience: 1/50
2024-12-19 23:23:22.829542: train_loss -0.519
2024-12-19 23:23:22.830441: val_loss -0.3908
2024-12-19 23:23:22.831184: Pseudo dice [0.6552]
2024-12-19 23:23:22.831965: Epoch time: 517.62 s
2024-12-19 23:23:22.832714: Yayy! New best EMA pseudo Dice: 0.6085
2024-12-19 23:23:24.632633: 
2024-12-19 23:23:24.633842: Epoch 12
2024-12-19 23:23:24.634672: Current learning rate: 0.00928
2024-12-19 23:32:00.468162: Validation loss improved from -0.43220 to -0.43546! Patience: 1/50
2024-12-19 23:32:00.469156: train_loss -0.5355
2024-12-19 23:32:00.470099: val_loss -0.4355
2024-12-19 23:32:00.470812: Pseudo dice [0.685]
2024-12-19 23:32:00.471508: Epoch time: 515.84 s
2024-12-19 23:32:00.472293: Yayy! New best EMA pseudo Dice: 0.6162
2024-12-19 23:32:02.388301: 
2024-12-19 23:32:02.389466: Epoch 13
2024-12-19 23:32:02.390184: Current learning rate: 0.00922
2024-12-19 23:40:39.201420: Validation loss did not improve from -0.43546. Patience: 1/50
2024-12-19 23:40:39.202433: train_loss -0.5451
2024-12-19 23:40:39.203145: val_loss -0.406
2024-12-19 23:40:39.203803: Pseudo dice [0.6823]
2024-12-19 23:40:39.204474: Epoch time: 516.82 s
2024-12-19 23:40:39.205205: Yayy! New best EMA pseudo Dice: 0.6228
2024-12-19 23:40:41.131111: 
2024-12-19 23:40:41.132499: Epoch 14
2024-12-19 23:40:41.133467: Current learning rate: 0.00916
2024-12-19 23:49:17.259010: Validation loss did not improve from -0.43546. Patience: 2/50
2024-12-19 23:49:17.259976: train_loss -0.5606
2024-12-19 23:49:17.261543: val_loss -0.3973
2024-12-19 23:49:17.262372: Pseudo dice [0.6828]
2024-12-19 23:49:17.263101: Epoch time: 516.13 s
2024-12-19 23:49:17.624479: Yayy! New best EMA pseudo Dice: 0.6288
2024-12-19 23:49:19.389273: 
2024-12-19 23:49:19.390393: Epoch 15
2024-12-19 23:49:19.391048: Current learning rate: 0.0091
2024-12-19 23:57:22.372406: Validation loss did not improve from -0.43546. Patience: 3/50
2024-12-19 23:57:22.373329: train_loss -0.5792
2024-12-19 23:57:22.374031: val_loss -0.3596
2024-12-19 23:57:22.374802: Pseudo dice [0.6479]
2024-12-19 23:57:22.375499: Epoch time: 482.99 s
2024-12-19 23:57:22.376090: Yayy! New best EMA pseudo Dice: 0.6307
2024-12-19 23:57:24.218146: 
2024-12-19 23:57:24.219102: Epoch 16
2024-12-19 23:57:24.219872: Current learning rate: 0.00903
2024-12-20 00:06:06.027077: Validation loss did not improve from -0.43546. Patience: 4/50
2024-12-20 00:06:06.027908: train_loss -0.5824
2024-12-20 00:06:06.028773: val_loss -0.4144
2024-12-20 00:06:06.029522: Pseudo dice [0.6796]
2024-12-20 00:06:06.030100: Epoch time: 521.81 s
2024-12-20 00:06:06.030688: Yayy! New best EMA pseudo Dice: 0.6356
2024-12-20 00:06:07.854195: 
2024-12-20 00:06:07.855245: Epoch 17
2024-12-20 00:06:07.855917: Current learning rate: 0.00897
2024-12-20 00:14:55.393641: Validation loss did not improve from -0.43546. Patience: 5/50
2024-12-20 00:14:55.394408: train_loss -0.5839
2024-12-20 00:14:55.395131: val_loss -0.4181
2024-12-20 00:14:55.395946: Pseudo dice [0.6864]
2024-12-20 00:14:55.396682: Epoch time: 527.54 s
2024-12-20 00:14:55.397340: Yayy! New best EMA pseudo Dice: 0.6407
2024-12-20 00:14:57.188860: 
2024-12-20 00:14:57.190199: Epoch 18
2024-12-20 00:14:57.191227: Current learning rate: 0.00891
2024-12-20 00:24:10.003785: Validation loss did not improve from -0.43546. Patience: 6/50
2024-12-20 00:24:10.004544: train_loss -0.6023
2024-12-20 00:24:10.005298: val_loss -0.373
2024-12-20 00:24:10.005912: Pseudo dice [0.6473]
2024-12-20 00:24:10.006590: Epoch time: 552.82 s
2024-12-20 00:24:10.007644: Yayy! New best EMA pseudo Dice: 0.6413
2024-12-20 00:24:12.199731: 
2024-12-20 00:24:12.201114: Epoch 19
2024-12-20 00:24:12.201890: Current learning rate: 0.00885
2024-12-20 00:33:14.739709: Validation loss did not improve from -0.43546. Patience: 7/50
2024-12-20 00:33:14.740554: train_loss -0.5988
2024-12-20 00:33:14.741277: val_loss -0.4054
2024-12-20 00:33:14.742000: Pseudo dice [0.6779]
2024-12-20 00:33:14.742639: Epoch time: 542.54 s
2024-12-20 00:33:15.143428: Yayy! New best EMA pseudo Dice: 0.645
2024-12-20 00:33:16.926226: 
2024-12-20 00:33:16.927263: Epoch 20
2024-12-20 00:33:16.927907: Current learning rate: 0.00879
2024-12-20 00:42:28.035704: Validation loss did not improve from -0.43546. Patience: 8/50
2024-12-20 00:42:28.036615: train_loss -0.6081
2024-12-20 00:42:28.037325: val_loss -0.4035
2024-12-20 00:42:28.038072: Pseudo dice [0.6857]
2024-12-20 00:42:28.038860: Epoch time: 551.11 s
2024-12-20 00:42:28.039439: Yayy! New best EMA pseudo Dice: 0.6491
2024-12-20 00:42:29.884104: 
2024-12-20 00:42:29.885446: Epoch 21
2024-12-20 00:42:29.886174: Current learning rate: 0.00873
2024-12-20 00:51:51.959507: Validation loss did not improve from -0.43546. Patience: 9/50
2024-12-20 00:51:51.979479: train_loss -0.616
2024-12-20 00:51:51.981379: val_loss -0.4151
2024-12-20 00:51:51.982309: Pseudo dice [0.683]
2024-12-20 00:51:51.983402: Epoch time: 562.08 s
2024-12-20 00:51:51.984476: Yayy! New best EMA pseudo Dice: 0.6525
2024-12-20 00:51:53.740714: 
2024-12-20 00:51:53.741808: Epoch 22
2024-12-20 00:51:53.742616: Current learning rate: 0.00867
2024-12-20 01:01:02.839120: Validation loss improved from -0.43546 to -0.44079! Patience: 9/50
2024-12-20 01:01:02.840014: train_loss -0.6331
2024-12-20 01:01:02.840879: val_loss -0.4408
2024-12-20 01:01:02.841690: Pseudo dice [0.704]
2024-12-20 01:01:02.842486: Epoch time: 549.1 s
2024-12-20 01:01:02.843254: Yayy! New best EMA pseudo Dice: 0.6576
2024-12-20 01:01:04.672670: 
2024-12-20 01:01:04.673699: Epoch 23
2024-12-20 01:01:04.674453: Current learning rate: 0.00861
2024-12-20 01:10:11.757158: Validation loss did not improve from -0.44079. Patience: 1/50
2024-12-20 01:10:11.758004: train_loss -0.6389
2024-12-20 01:10:11.758843: val_loss -0.3999
2024-12-20 01:10:11.759560: Pseudo dice [0.673]
2024-12-20 01:10:11.760300: Epoch time: 547.09 s
2024-12-20 01:10:11.760987: Yayy! New best EMA pseudo Dice: 0.6592
2024-12-20 01:10:13.467101: 
2024-12-20 01:10:13.468351: Epoch 24
2024-12-20 01:10:13.469089: Current learning rate: 0.00855
2024-12-20 01:19:20.960081: Validation loss improved from -0.44079 to -0.46158! Patience: 1/50
2024-12-20 01:19:20.960936: train_loss -0.6479
2024-12-20 01:19:20.961602: val_loss -0.4616
2024-12-20 01:19:20.962289: Pseudo dice [0.7141]
2024-12-20 01:19:20.963006: Epoch time: 547.5 s
2024-12-20 01:19:21.354486: Yayy! New best EMA pseudo Dice: 0.6646
2024-12-20 01:19:23.111637: 
2024-12-20 01:19:23.112906: Epoch 25
2024-12-20 01:19:23.113695: Current learning rate: 0.00849
2024-12-20 01:28:53.174019: Validation loss did not improve from -0.46158. Patience: 1/50
2024-12-20 01:28:53.174909: train_loss -0.6548
2024-12-20 01:28:53.175575: val_loss -0.3996
2024-12-20 01:28:53.176213: Pseudo dice [0.6674]
2024-12-20 01:28:53.176893: Epoch time: 570.06 s
2024-12-20 01:28:53.177515: Yayy! New best EMA pseudo Dice: 0.6649
2024-12-20 01:28:54.961955: 
2024-12-20 01:28:54.963003: Epoch 26
2024-12-20 01:28:54.963590: Current learning rate: 0.00843
2024-12-20 01:38:29.444301: Validation loss did not improve from -0.46158. Patience: 2/50
2024-12-20 01:38:29.445115: train_loss -0.6653
2024-12-20 01:38:29.445914: val_loss -0.4136
2024-12-20 01:38:29.446784: Pseudo dice [0.6794]
2024-12-20 01:38:29.447598: Epoch time: 574.48 s
2024-12-20 01:38:29.448451: Yayy! New best EMA pseudo Dice: 0.6664
2024-12-20 01:38:31.263506: 
2024-12-20 01:38:31.264598: Epoch 27
2024-12-20 01:38:31.265397: Current learning rate: 0.00836
2024-12-20 01:47:57.095933: Validation loss did not improve from -0.46158. Patience: 3/50
2024-12-20 01:47:57.096789: train_loss -0.6696
2024-12-20 01:47:57.097746: val_loss -0.3977
2024-12-20 01:47:57.098700: Pseudo dice [0.6713]
2024-12-20 01:47:57.099713: Epoch time: 565.83 s
2024-12-20 01:47:57.100664: Yayy! New best EMA pseudo Dice: 0.6669
2024-12-20 01:47:58.837723: 
2024-12-20 01:47:58.839082: Epoch 28
2024-12-20 01:47:58.840060: Current learning rate: 0.0083
2024-12-20 01:56:43.673388: Validation loss did not improve from -0.46158. Patience: 4/50
2024-12-20 01:56:43.675631: train_loss -0.6782
2024-12-20 01:56:43.677185: val_loss -0.4242
2024-12-20 01:56:43.677881: Pseudo dice [0.6976]
2024-12-20 01:56:43.678822: Epoch time: 524.84 s
2024-12-20 01:56:43.679557: Yayy! New best EMA pseudo Dice: 0.6699
2024-12-20 01:56:45.824356: 
2024-12-20 01:56:45.825613: Epoch 29
2024-12-20 01:56:45.826258: Current learning rate: 0.00824
2024-12-20 02:05:16.960860: Validation loss did not improve from -0.46158. Patience: 5/50
2024-12-20 02:05:16.961828: train_loss -0.6824
2024-12-20 02:05:16.962489: val_loss -0.4475
2024-12-20 02:05:16.963100: Pseudo dice [0.6992]
2024-12-20 02:05:16.963943: Epoch time: 511.14 s
2024-12-20 02:05:17.348783: Yayy! New best EMA pseudo Dice: 0.6729
2024-12-20 02:05:19.124064: 
2024-12-20 02:05:19.125098: Epoch 30
2024-12-20 02:05:19.125872: Current learning rate: 0.00818
2024-12-20 02:14:01.551508: Validation loss did not improve from -0.46158. Patience: 6/50
2024-12-20 02:14:01.552614: train_loss -0.6813
2024-12-20 02:14:01.553645: val_loss -0.3665
2024-12-20 02:14:01.554461: Pseudo dice [0.6619]
2024-12-20 02:14:01.555249: Epoch time: 522.43 s
2024-12-20 02:14:02.934321: 
2024-12-20 02:14:02.935638: Epoch 31
2024-12-20 02:14:02.936496: Current learning rate: 0.00812
2024-12-20 02:22:05.779285: Validation loss did not improve from -0.46158. Patience: 7/50
2024-12-20 02:22:05.780212: train_loss -0.686
2024-12-20 02:22:05.780906: val_loss -0.3477
2024-12-20 02:22:05.781503: Pseudo dice [0.6425]
2024-12-20 02:22:05.782118: Epoch time: 482.85 s
2024-12-20 02:22:07.141019: 
2024-12-20 02:22:07.142066: Epoch 32
2024-12-20 02:22:07.142655: Current learning rate: 0.00806
2024-12-20 02:30:24.401584: Validation loss did not improve from -0.46158. Patience: 8/50
2024-12-20 02:30:24.402615: train_loss -0.694
2024-12-20 02:30:24.403327: val_loss -0.4203
2024-12-20 02:30:24.403934: Pseudo dice [0.6823]
2024-12-20 02:30:24.404550: Epoch time: 497.26 s
2024-12-20 02:30:25.811042: 
2024-12-20 02:30:25.812296: Epoch 33
2024-12-20 02:30:25.813059: Current learning rate: 0.008
2024-12-20 02:39:00.156245: Validation loss did not improve from -0.46158. Patience: 9/50
2024-12-20 02:39:00.157228: train_loss -0.6974
2024-12-20 02:39:00.157976: val_loss -0.4157
2024-12-20 02:39:00.158559: Pseudo dice [0.7034]
2024-12-20 02:39:00.159186: Epoch time: 514.35 s
2024-12-20 02:39:00.159968: Yayy! New best EMA pseudo Dice: 0.6735
2024-12-20 02:39:01.992362: 
2024-12-20 02:39:01.993245: Epoch 34
2024-12-20 02:39:01.994017: Current learning rate: 0.00793
2024-12-20 02:47:38.182909: Validation loss did not improve from -0.46158. Patience: 10/50
2024-12-20 02:47:38.183896: train_loss -0.699
2024-12-20 02:47:38.184649: val_loss -0.4047
2024-12-20 02:47:38.185296: Pseudo dice [0.6976]
2024-12-20 02:47:38.185974: Epoch time: 516.19 s
2024-12-20 02:47:38.659109: Yayy! New best EMA pseudo Dice: 0.6759
2024-12-20 02:47:40.499733: 
2024-12-20 02:47:40.500692: Epoch 35
2024-12-20 02:47:40.501292: Current learning rate: 0.00787
2024-12-20 02:56:11.910602: Validation loss did not improve from -0.46158. Patience: 11/50
2024-12-20 02:56:11.911577: train_loss -0.6988
2024-12-20 02:56:11.912371: val_loss -0.3768
2024-12-20 02:56:11.913012: Pseudo dice [0.6725]
2024-12-20 02:56:11.913624: Epoch time: 511.41 s
2024-12-20 02:56:13.300906: 
2024-12-20 02:56:13.301943: Epoch 36
2024-12-20 02:56:13.302600: Current learning rate: 0.00781
2024-12-20 03:04:28.104349: Validation loss did not improve from -0.46158. Patience: 12/50
2024-12-20 03:04:28.105247: train_loss -0.7053
2024-12-20 03:04:28.106557: val_loss -0.4232
2024-12-20 03:04:28.107222: Pseudo dice [0.697]
2024-12-20 03:04:28.107989: Epoch time: 494.81 s
2024-12-20 03:04:28.108634: Yayy! New best EMA pseudo Dice: 0.6777
2024-12-20 03:04:29.884492: 
2024-12-20 03:04:29.885416: Epoch 37
2024-12-20 03:04:29.886109: Current learning rate: 0.00775
2024-12-20 03:12:46.332084: Validation loss did not improve from -0.46158. Patience: 13/50
2024-12-20 03:12:46.333039: train_loss -0.7097
2024-12-20 03:12:46.333811: val_loss -0.3491
2024-12-20 03:12:46.334537: Pseudo dice [0.6493]
2024-12-20 03:12:46.335418: Epoch time: 496.45 s
2024-12-20 03:12:47.957981: 
2024-12-20 03:12:47.960248: Epoch 38
2024-12-20 03:12:47.961020: Current learning rate: 0.00769
2024-12-20 03:21:22.533988: Validation loss did not improve from -0.46158. Patience: 14/50
2024-12-20 03:21:22.534763: train_loss -0.7231
2024-12-20 03:21:22.535651: val_loss -0.4332
2024-12-20 03:21:22.536484: Pseudo dice [0.7099]
2024-12-20 03:21:22.537314: Epoch time: 514.58 s
2024-12-20 03:21:22.537975: Yayy! New best EMA pseudo Dice: 0.6784
2024-12-20 03:21:25.005448: 
2024-12-20 03:21:25.006824: Epoch 39
2024-12-20 03:21:25.007562: Current learning rate: 0.00763
2024-12-20 03:29:30.737640: Validation loss did not improve from -0.46158. Patience: 15/50
2024-12-20 03:29:30.738639: train_loss -0.7249
2024-12-20 03:29:30.739321: val_loss -0.3485
2024-12-20 03:29:30.739971: Pseudo dice [0.6618]
2024-12-20 03:29:30.740644: Epoch time: 485.73 s
2024-12-20 03:29:32.621113: 
2024-12-20 03:29:32.622459: Epoch 40
2024-12-20 03:29:32.623260: Current learning rate: 0.00756
2024-12-20 03:37:45.000830: Validation loss did not improve from -0.46158. Patience: 16/50
2024-12-20 03:37:45.001858: train_loss -0.7195
2024-12-20 03:37:45.002808: val_loss -0.4133
2024-12-20 03:37:45.003585: Pseudo dice [0.6976]
2024-12-20 03:37:45.004381: Epoch time: 492.38 s
2024-12-20 03:37:45.005136: Yayy! New best EMA pseudo Dice: 0.6788
2024-12-20 03:37:47.040891: 
2024-12-20 03:37:47.042264: Epoch 41
2024-12-20 03:37:47.043023: Current learning rate: 0.0075
2024-12-20 03:46:16.617427: Validation loss did not improve from -0.46158. Patience: 17/50
2024-12-20 03:46:16.618425: train_loss -0.7273
2024-12-20 03:46:16.619180: val_loss -0.4231
2024-12-20 03:46:16.619815: Pseudo dice [0.6984]
2024-12-20 03:46:16.620480: Epoch time: 509.58 s
2024-12-20 03:46:16.621239: Yayy! New best EMA pseudo Dice: 0.6808
2024-12-20 03:46:18.397426: 
2024-12-20 03:46:18.398631: Epoch 42
2024-12-20 03:46:18.399338: Current learning rate: 0.00744
2024-12-20 03:54:47.220007: Validation loss did not improve from -0.46158. Patience: 18/50
2024-12-20 03:54:47.222490: train_loss -0.7261
2024-12-20 03:54:47.223839: val_loss -0.361
2024-12-20 03:54:47.224803: Pseudo dice [0.6713]
2024-12-20 03:54:47.225851: Epoch time: 508.83 s
2024-12-20 03:54:48.596523: 
2024-12-20 03:54:48.597923: Epoch 43
2024-12-20 03:54:48.598746: Current learning rate: 0.00738
2024-12-20 04:03:16.061974: Validation loss did not improve from -0.46158. Patience: 19/50
2024-12-20 04:03:16.064411: train_loss -0.7265
2024-12-20 04:03:16.065835: val_loss -0.4507
2024-12-20 04:03:16.066830: Pseudo dice [0.7138]
2024-12-20 04:03:16.067830: Epoch time: 507.47 s
2024-12-20 04:03:16.068785: Yayy! New best EMA pseudo Dice: 0.6832
2024-12-20 04:03:17.840342: 
2024-12-20 04:03:17.841618: Epoch 44
2024-12-20 04:03:17.842417: Current learning rate: 0.00732
2024-12-20 04:11:47.990203: Validation loss did not improve from -0.46158. Patience: 20/50
2024-12-20 04:11:47.991601: train_loss -0.7396
2024-12-20 04:11:47.992856: val_loss -0.4284
2024-12-20 04:11:47.993748: Pseudo dice [0.6927]
2024-12-20 04:11:47.994649: Epoch time: 510.15 s
2024-12-20 04:11:48.361981: Yayy! New best EMA pseudo Dice: 0.6842
2024-12-20 04:11:50.125199: 
2024-12-20 04:11:50.126233: Epoch 45
2024-12-20 04:11:50.127010: Current learning rate: 0.00725
2024-12-20 04:20:10.799731: Validation loss did not improve from -0.46158. Patience: 21/50
2024-12-20 04:20:10.800662: train_loss -0.7474
2024-12-20 04:20:10.801429: val_loss -0.3832
2024-12-20 04:20:10.802126: Pseudo dice [0.6712]
2024-12-20 04:20:10.802795: Epoch time: 500.68 s
2024-12-20 04:20:12.136359: 
2024-12-20 04:20:12.138733: Epoch 46
2024-12-20 04:20:12.139520: Current learning rate: 0.00719
2024-12-20 04:28:48.468007: Validation loss did not improve from -0.46158. Patience: 22/50
2024-12-20 04:28:48.468867: train_loss -0.7437
2024-12-20 04:28:48.469556: val_loss -0.4166
2024-12-20 04:28:48.470337: Pseudo dice [0.7015]
2024-12-20 04:28:48.471102: Epoch time: 516.33 s
2024-12-20 04:28:48.471909: Yayy! New best EMA pseudo Dice: 0.6847
2024-12-20 04:28:50.550298: 
2024-12-20 04:28:50.551451: Epoch 47
2024-12-20 04:28:50.552246: Current learning rate: 0.00713
2024-12-20 04:37:27.746357: Validation loss did not improve from -0.46158. Patience: 23/50
2024-12-20 04:37:27.747336: train_loss -0.7455
2024-12-20 04:37:27.748323: val_loss -0.334
2024-12-20 04:37:27.749323: Pseudo dice [0.6536]
2024-12-20 04:37:27.750321: Epoch time: 517.2 s
2024-12-20 04:37:29.147160: 
2024-12-20 04:37:29.148587: Epoch 48
2024-12-20 04:37:29.149578: Current learning rate: 0.00707
2024-12-20 04:45:52.386333: Validation loss did not improve from -0.46158. Patience: 24/50
2024-12-20 04:45:52.387317: train_loss -0.7415
2024-12-20 04:45:52.388045: val_loss -0.419
2024-12-20 04:45:52.388666: Pseudo dice [0.6947]
2024-12-20 04:45:52.389230: Epoch time: 503.24 s
2024-12-20 04:45:53.735327: 
2024-12-20 04:45:53.736704: Epoch 49
2024-12-20 04:45:53.737487: Current learning rate: 0.007
2024-12-20 04:54:23.723566: Validation loss did not improve from -0.46158. Patience: 25/50
2024-12-20 04:54:23.724531: train_loss -0.7453
2024-12-20 04:54:23.725281: val_loss -0.4334
2024-12-20 04:54:23.726090: Pseudo dice [0.7076]
2024-12-20 04:54:23.726858: Epoch time: 509.99 s
2024-12-20 04:54:24.604924: Yayy! New best EMA pseudo Dice: 0.6854
2024-12-20 04:54:26.405038: 
2024-12-20 04:54:26.406225: Epoch 50
2024-12-20 04:54:26.406966: Current learning rate: 0.00694
2024-12-20 05:02:53.881710: Validation loss did not improve from -0.46158. Patience: 26/50
2024-12-20 05:02:53.882654: train_loss -0.7553
2024-12-20 05:02:53.883333: val_loss -0.3798
2024-12-20 05:02:53.883971: Pseudo dice [0.6744]
2024-12-20 05:02:53.884569: Epoch time: 507.48 s
2024-12-20 05:02:55.264845: 
2024-12-20 05:02:55.265774: Epoch 51
2024-12-20 05:02:55.266439: Current learning rate: 0.00688
2024-12-20 05:11:24.102329: Validation loss did not improve from -0.46158. Patience: 27/50
2024-12-20 05:11:24.122198: train_loss -0.765
2024-12-20 05:11:24.124126: val_loss -0.4263
2024-12-20 05:11:24.125124: Pseudo dice [0.7014]
2024-12-20 05:11:24.126527: Epoch time: 508.86 s
2024-12-20 05:11:24.127588: Yayy! New best EMA pseudo Dice: 0.686
2024-12-20 05:11:25.933724: 
2024-12-20 05:11:25.934764: Epoch 52
2024-12-20 05:11:25.935456: Current learning rate: 0.00682
2024-12-20 05:20:02.919823: Validation loss did not improve from -0.46158. Patience: 28/50
2024-12-20 05:20:02.920883: train_loss -0.7586
2024-12-20 05:20:02.921732: val_loss -0.4038
2024-12-20 05:20:02.922539: Pseudo dice [0.6868]
2024-12-20 05:20:02.923220: Epoch time: 516.99 s
2024-12-20 05:20:02.923888: Yayy! New best EMA pseudo Dice: 0.6861
2024-12-20 05:20:04.727526: 
2024-12-20 05:20:04.728694: Epoch 53
2024-12-20 05:20:04.729414: Current learning rate: 0.00675
2024-12-20 05:28:44.416558: Validation loss did not improve from -0.46158. Patience: 29/50
2024-12-20 05:28:44.417545: train_loss -0.7659
2024-12-20 05:28:44.418764: val_loss -0.4559
2024-12-20 05:28:44.419907: Pseudo dice [0.7141]
2024-12-20 05:28:44.421056: Epoch time: 519.69 s
2024-12-20 05:28:44.422124: Yayy! New best EMA pseudo Dice: 0.6889
2024-12-20 05:28:46.359576: 
2024-12-20 05:28:46.360669: Epoch 54
2024-12-20 05:28:46.361691: Current learning rate: 0.00669
2024-12-20 05:37:33.062691: Validation loss did not improve from -0.46158. Patience: 30/50
2024-12-20 05:37:33.063574: train_loss -0.7661
2024-12-20 05:37:33.064244: val_loss -0.3658
2024-12-20 05:37:33.064847: Pseudo dice [0.6679]
2024-12-20 05:37:33.065484: Epoch time: 526.71 s
2024-12-20 05:37:34.899964: 
2024-12-20 05:37:34.901354: Epoch 55
2024-12-20 05:37:34.902055: Current learning rate: 0.00663
2024-12-20 05:45:58.980147: Validation loss did not improve from -0.46158. Patience: 31/50
2024-12-20 05:45:58.981019: train_loss -0.7634
2024-12-20 05:45:58.981851: val_loss -0.3404
2024-12-20 05:45:58.982585: Pseudo dice [0.6556]
2024-12-20 05:45:58.983300: Epoch time: 504.08 s
2024-12-20 05:46:00.383069: 
2024-12-20 05:46:00.384404: Epoch 56
2024-12-20 05:46:00.385403: Current learning rate: 0.00657
2024-12-20 05:54:42.563257: Validation loss did not improve from -0.46158. Patience: 32/50
2024-12-20 05:54:42.564309: train_loss -0.7639
2024-12-20 05:54:42.565170: val_loss -0.4106
2024-12-20 05:54:42.566004: Pseudo dice [0.6987]
2024-12-20 05:54:42.566701: Epoch time: 522.18 s
2024-12-20 05:54:43.941195: 
2024-12-20 05:54:43.942514: Epoch 57
2024-12-20 05:54:43.943276: Current learning rate: 0.0065
2024-12-20 06:03:11.265745: Validation loss did not improve from -0.46158. Patience: 33/50
2024-12-20 06:03:11.267803: train_loss -0.7655
2024-12-20 06:03:11.268454: val_loss -0.3589
2024-12-20 06:03:11.269116: Pseudo dice [0.6664]
2024-12-20 06:03:11.269763: Epoch time: 507.33 s
2024-12-20 06:03:12.640783: 
2024-12-20 06:03:12.642117: Epoch 58
2024-12-20 06:03:12.642862: Current learning rate: 0.00644
2024-12-20 06:12:04.539278: Validation loss did not improve from -0.46158. Patience: 34/50
2024-12-20 06:12:04.540882: train_loss -0.7677
2024-12-20 06:12:04.541882: val_loss -0.3764
2024-12-20 06:12:04.542592: Pseudo dice [0.6907]
2024-12-20 06:12:04.543284: Epoch time: 531.9 s
2024-12-20 06:12:05.938884: 
2024-12-20 06:12:05.940028: Epoch 59
2024-12-20 06:12:05.940688: Current learning rate: 0.00638
2024-12-20 06:21:01.002410: Validation loss did not improve from -0.46158. Patience: 35/50
2024-12-20 06:21:01.003693: train_loss -0.7706
2024-12-20 06:21:01.004708: val_loss -0.3969
2024-12-20 06:21:01.005496: Pseudo dice [0.6935]
2024-12-20 06:21:01.006389: Epoch time: 535.07 s
2024-12-20 06:21:03.307649: 
2024-12-20 06:21:03.308843: Epoch 60
2024-12-20 06:21:03.309890: Current learning rate: 0.00631
2024-12-20 06:29:28.429402: Validation loss did not improve from -0.46158. Patience: 36/50
2024-12-20 06:29:28.430337: train_loss -0.7712
2024-12-20 06:29:28.431085: val_loss -0.3106
2024-12-20 06:29:28.431794: Pseudo dice [0.6296]
2024-12-20 06:29:28.432582: Epoch time: 505.12 s
2024-12-20 06:29:29.851818: 
2024-12-20 06:29:29.853161: Epoch 61
2024-12-20 06:29:29.854085: Current learning rate: 0.00625
2024-12-20 06:38:09.439043: Validation loss did not improve from -0.46158. Patience: 37/50
2024-12-20 06:38:09.439949: train_loss -0.774
2024-12-20 06:38:09.440670: val_loss -0.3799
2024-12-20 06:38:09.441389: Pseudo dice [0.6917]
2024-12-20 06:38:09.442010: Epoch time: 519.59 s
2024-12-20 06:38:10.921269: 
2024-12-20 06:38:10.922483: Epoch 62
2024-12-20 06:38:10.923195: Current learning rate: 0.00619
2024-12-20 06:46:29.126062: Validation loss did not improve from -0.46158. Patience: 38/50
2024-12-20 06:46:29.127135: train_loss -0.765
2024-12-20 06:46:29.128160: val_loss -0.3942
2024-12-20 06:46:29.128896: Pseudo dice [0.6909]
2024-12-20 06:46:29.129707: Epoch time: 498.21 s
2024-12-20 06:46:30.641813: 
2024-12-20 06:46:30.643088: Epoch 63
2024-12-20 06:46:30.643943: Current learning rate: 0.00612
2024-12-20 06:55:14.823220: Validation loss did not improve from -0.46158. Patience: 39/50
2024-12-20 06:55:14.825230: train_loss -0.7591
2024-12-20 06:55:14.826289: val_loss -0.3679
2024-12-20 06:55:14.827022: Pseudo dice [0.6663]
2024-12-20 06:55:14.827790: Epoch time: 524.18 s
2024-12-20 06:55:16.315753: 
2024-12-20 06:55:16.317365: Epoch 64
2024-12-20 06:55:16.318928: Current learning rate: 0.00606
2024-12-20 07:04:01.947939: Validation loss did not improve from -0.46158. Patience: 40/50
2024-12-20 07:04:01.948811: train_loss -0.7698
2024-12-20 07:04:01.949834: val_loss -0.4272
2024-12-20 07:04:01.950758: Pseudo dice [0.7051]
2024-12-20 07:04:01.951713: Epoch time: 525.64 s
2024-12-20 07:04:03.800028: 
2024-12-20 07:04:03.801428: Epoch 65
2024-12-20 07:04:03.802344: Current learning rate: 0.006
2024-12-20 07:12:50.969877: Validation loss did not improve from -0.46158. Patience: 41/50
2024-12-20 07:12:50.970754: train_loss -0.7756
2024-12-20 07:12:50.971530: val_loss -0.3546
2024-12-20 07:12:50.972230: Pseudo dice [0.6626]
2024-12-20 07:12:50.973043: Epoch time: 527.17 s
2024-12-20 07:12:52.426185: 
2024-12-20 07:12:52.427198: Epoch 66
2024-12-20 07:12:52.427936: Current learning rate: 0.00593
2024-12-20 07:21:38.219916: Validation loss did not improve from -0.46158. Patience: 42/50
2024-12-20 07:21:38.285636: train_loss -0.7815
2024-12-20 07:21:38.287241: val_loss -0.3535
2024-12-20 07:21:38.288072: Pseudo dice [0.6565]
2024-12-20 07:21:38.288969: Epoch time: 525.86 s
2024-12-20 07:21:39.746417: 
2024-12-20 07:21:39.747394: Epoch 67
2024-12-20 07:21:39.748088: Current learning rate: 0.00587
2024-12-20 07:30:17.146141: Validation loss did not improve from -0.46158. Patience: 43/50
2024-12-20 07:30:17.147120: train_loss -0.7753
2024-12-20 07:30:17.147931: val_loss -0.4467
2024-12-20 07:30:17.148659: Pseudo dice [0.712]
2024-12-20 07:30:17.149289: Epoch time: 517.4 s
2024-12-20 07:30:18.568105: 
2024-12-20 07:30:18.569493: Epoch 68
2024-12-20 07:30:18.570227: Current learning rate: 0.00581
2024-12-20 07:39:18.530070: Validation loss did not improve from -0.46158. Patience: 44/50
2024-12-20 07:39:18.531025: train_loss -0.7773
2024-12-20 07:39:18.531855: val_loss -0.4358
2024-12-20 07:39:18.532698: Pseudo dice [0.7023]
2024-12-20 07:39:18.533469: Epoch time: 539.96 s
2024-12-20 07:39:19.963916: 
2024-12-20 07:39:19.965057: Epoch 69
2024-12-20 07:39:19.965828: Current learning rate: 0.00574
2024-12-20 07:47:55.157640: Validation loss did not improve from -0.46158. Patience: 45/50
2024-12-20 07:47:55.158376: train_loss -0.7834
2024-12-20 07:47:55.159049: val_loss -0.3971
2024-12-20 07:47:55.159632: Pseudo dice [0.6917]
2024-12-20 07:47:55.160265: Epoch time: 515.2 s
2024-12-20 07:47:56.984663: 
2024-12-20 07:47:56.986186: Epoch 70
2024-12-20 07:47:56.987021: Current learning rate: 0.00568
2024-12-20 07:56:40.724854: Validation loss did not improve from -0.46158. Patience: 46/50
2024-12-20 07:56:40.725853: train_loss -0.7886
2024-12-20 07:56:40.726708: val_loss -0.386
2024-12-20 07:56:40.727569: Pseudo dice [0.6883]
2024-12-20 07:56:40.728320: Epoch time: 523.74 s
2024-12-20 07:56:42.806356: 
2024-12-20 07:56:42.807589: Epoch 71
2024-12-20 07:56:42.808372: Current learning rate: 0.00562
2024-12-20 08:05:08.513731: Validation loss did not improve from -0.46158. Patience: 47/50
2024-12-20 08:05:08.514697: train_loss -0.7916
2024-12-20 08:05:08.515408: val_loss -0.4589
2024-12-20 08:05:08.516174: Pseudo dice [0.7203]
2024-12-20 08:05:08.517037: Epoch time: 505.71 s
2024-12-20 08:05:09.980967: 
2024-12-20 08:05:09.982037: Epoch 72
2024-12-20 08:05:09.982731: Current learning rate: 0.00555
2024-12-20 08:13:45.103570: Validation loss did not improve from -0.46158. Patience: 48/50
2024-12-20 08:13:45.106271: train_loss -0.7887
2024-12-20 08:13:45.106987: val_loss -0.4378
2024-12-20 08:13:45.107664: Pseudo dice [0.7176]
2024-12-20 08:13:45.108426: Epoch time: 515.13 s
2024-12-20 08:13:45.108981: Yayy! New best EMA pseudo Dice: 0.6913
2024-12-20 08:13:47.038582: 
2024-12-20 08:13:47.039753: Epoch 73
2024-12-20 08:13:47.040551: Current learning rate: 0.00549
2024-12-20 08:22:22.717862: Validation loss did not improve from -0.46158. Patience: 49/50
2024-12-20 08:22:22.719741: train_loss -0.7892
2024-12-20 08:22:22.720747: val_loss -0.4437
2024-12-20 08:22:22.721399: Pseudo dice [0.7102]
2024-12-20 08:22:22.722124: Epoch time: 515.68 s
2024-12-20 08:22:22.722887: Yayy! New best EMA pseudo Dice: 0.6932
2024-12-20 08:22:24.563499: 
2024-12-20 08:22:24.564734: Epoch 74
2024-12-20 08:22:24.565431: Current learning rate: 0.00542
2024-12-20 08:31:03.885084: Validation loss did not improve from -0.46158. Patience: 50/50
2024-12-20 08:31:03.886167: train_loss -0.7917
2024-12-20 08:31:03.887063: val_loss -0.4021
2024-12-20 08:31:03.887805: Pseudo dice [0.6997]
2024-12-20 08:31:03.888618: Epoch time: 519.32 s
2024-12-20 08:31:04.275791: Yayy! New best EMA pseudo Dice: 0.6939
2024-12-20 08:31:06.126906: 
2024-12-20 08:31:06.128154: Epoch 75
2024-12-20 08:31:06.128980: Current learning rate: 0.00536
2024-12-20 08:39:34.884514: Validation loss did not improve from -0.46158. Patience: 51/50
2024-12-20 08:39:34.885521: train_loss -0.7926
2024-12-20 08:39:34.886345: val_loss -0.3823
2024-12-20 08:39:34.887132: Pseudo dice [0.6802]
2024-12-20 08:39:34.887821: Epoch time: 508.76 s
2024-12-20 08:39:36.420857: 
2024-12-20 08:39:36.422021: Epoch 76
2024-12-20 08:39:36.422906: Current learning rate: 0.00529
2024-12-20 08:48:08.013795: Validation loss did not improve from -0.46158. Patience: 52/50
2024-12-20 08:48:08.014592: train_loss -0.7976
2024-12-20 08:48:08.015563: val_loss -0.3835
2024-12-20 08:48:08.016462: Pseudo dice [0.6742]
2024-12-20 08:48:08.017458: Epoch time: 511.6 s
2024-12-20 08:48:09.713283: 
2024-12-20 08:48:09.715022: Epoch 77
2024-12-20 08:48:09.716017: Current learning rate: 0.00523
2024-12-20 08:56:51.385410: Validation loss did not improve from -0.46158. Patience: 53/50
2024-12-20 08:56:51.386311: train_loss -0.7993
2024-12-20 08:56:51.387081: val_loss -0.4514
2024-12-20 08:56:51.387877: Pseudo dice [0.7141]
2024-12-20 08:56:51.388671: Epoch time: 521.67 s
2024-12-20 08:56:52.908034: 
2024-12-20 08:56:52.909203: Epoch 78
2024-12-20 08:56:52.910022: Current learning rate: 0.00517
2024-12-20 09:06:10.848941: Validation loss did not improve from -0.46158. Patience: 54/50
2024-12-20 09:06:10.849961: train_loss -0.8006
2024-12-20 09:06:10.850800: val_loss -0.3377
2024-12-20 09:06:10.851493: Pseudo dice [0.6588]
2024-12-20 09:06:10.852189: Epoch time: 557.94 s
2024-12-20 09:06:12.354378: 
2024-12-20 09:06:12.355581: Epoch 79
2024-12-20 09:06:12.356387: Current learning rate: 0.0051
2024-12-20 09:14:56.690919: Validation loss did not improve from -0.46158. Patience: 55/50
2024-12-20 09:14:56.692546: train_loss -0.7979
2024-12-20 09:14:56.693450: val_loss -0.3977
2024-12-20 09:14:56.694144: Pseudo dice [0.6975]
2024-12-20 09:14:56.694842: Epoch time: 524.34 s
2024-12-20 09:14:58.676734: 
2024-12-20 09:14:58.678085: Epoch 80
2024-12-20 09:14:58.679143: Current learning rate: 0.00504
2024-12-20 09:23:46.300447: Validation loss did not improve from -0.46158. Patience: 56/50
2024-12-20 09:23:46.302285: train_loss -0.7983
2024-12-20 09:23:46.303043: val_loss -0.3687
2024-12-20 09:23:46.303648: Pseudo dice [0.6722]
2024-12-20 09:23:46.304268: Epoch time: 527.63 s
2024-12-20 09:23:48.285451: 
2024-12-20 09:23:48.286990: Epoch 81
2024-12-20 09:23:48.287782: Current learning rate: 0.00497
2024-12-20 09:32:28.349260: Validation loss did not improve from -0.46158. Patience: 57/50
2024-12-20 09:32:28.351298: train_loss -0.7982
2024-12-20 09:32:28.352781: val_loss -0.4002
2024-12-20 09:32:28.353675: Pseudo dice [0.71]
2024-12-20 09:32:28.354664: Epoch time: 520.07 s
2024-12-20 09:32:29.914336: 
2024-12-20 09:32:29.916120: Epoch 82
2024-12-20 09:32:29.917210: Current learning rate: 0.00491
2024-12-20 09:41:00.942321: Validation loss did not improve from -0.46158. Patience: 58/50
2024-12-20 09:41:00.943264: train_loss -0.8003
2024-12-20 09:41:00.943997: val_loss -0.421
2024-12-20 09:41:00.944687: Pseudo dice [0.7002]
2024-12-20 09:41:00.945357: Epoch time: 511.03 s
2024-12-20 09:41:02.330634: 
2024-12-20 09:41:02.332243: Epoch 83
2024-12-20 09:41:02.333114: Current learning rate: 0.00484
2024-12-20 09:49:54.041500: Validation loss did not improve from -0.46158. Patience: 59/50
2024-12-20 09:49:54.043121: train_loss -0.8006
2024-12-20 09:49:54.044079: val_loss -0.378
2024-12-20 09:49:54.044904: Pseudo dice [0.6864]
2024-12-20 09:49:54.045770: Epoch time: 531.71 s
2024-12-20 09:49:55.478114: 
2024-12-20 09:49:55.479934: Epoch 84
2024-12-20 09:49:55.481026: Current learning rate: 0.00478
2024-12-20 09:58:44.843245: Validation loss did not improve from -0.46158. Patience: 60/50
2024-12-20 09:58:44.844896: train_loss -0.8025
2024-12-20 09:58:44.845978: val_loss -0.3496
2024-12-20 09:58:44.846681: Pseudo dice [0.6738]
2024-12-20 09:58:44.847487: Epoch time: 529.37 s
2024-12-20 09:58:46.584348: 
2024-12-20 09:58:46.585923: Epoch 85
2024-12-20 09:58:46.586788: Current learning rate: 0.00471
2024-12-20 10:07:34.306286: Validation loss did not improve from -0.46158. Patience: 61/50
2024-12-20 10:07:34.307313: train_loss -0.804
2024-12-20 10:07:34.308165: val_loss -0.3458
2024-12-20 10:07:34.308847: Pseudo dice [0.6758]
2024-12-20 10:07:34.309579: Epoch time: 527.72 s
2024-12-20 10:07:35.668135: 
2024-12-20 10:07:35.669702: Epoch 86
2024-12-20 10:07:35.670497: Current learning rate: 0.00465
2024-12-20 10:16:32.260213: Validation loss did not improve from -0.46158. Patience: 62/50
2024-12-20 10:16:32.261158: train_loss -0.8069
2024-12-20 10:16:32.261904: val_loss -0.4209
2024-12-20 10:16:32.262550: Pseudo dice [0.7088]
2024-12-20 10:16:32.263121: Epoch time: 536.59 s
2024-12-20 10:16:33.628132: 
2024-12-20 10:16:33.629868: Epoch 87
2024-12-20 10:16:33.630862: Current learning rate: 0.00458
2024-12-20 10:25:07.166107: Validation loss did not improve from -0.46158. Patience: 63/50
2024-12-20 10:25:07.168640: train_loss -0.8074
2024-12-20 10:25:07.169699: val_loss -0.4245
2024-12-20 10:25:07.170374: Pseudo dice [0.7053]
2024-12-20 10:25:07.171022: Epoch time: 513.54 s
2024-12-20 10:25:08.575745: 
2024-12-20 10:25:08.577534: Epoch 88
2024-12-20 10:25:08.578428: Current learning rate: 0.00452
2024-12-20 10:33:31.316053: Validation loss did not improve from -0.46158. Patience: 64/50
2024-12-20 10:33:31.317007: train_loss -0.8069
2024-12-20 10:33:31.317799: val_loss -0.4285
2024-12-20 10:33:31.318497: Pseudo dice [0.7041]
2024-12-20 10:33:31.319209: Epoch time: 502.74 s
2024-12-20 10:33:32.700155: 
2024-12-20 10:33:32.701470: Epoch 89
2024-12-20 10:33:32.702314: Current learning rate: 0.00445
2024-12-20 10:41:45.550501: Validation loss did not improve from -0.46158. Patience: 65/50
2024-12-20 10:41:45.551559: train_loss -0.8055
2024-12-20 10:41:45.552420: val_loss -0.3809
2024-12-20 10:41:45.553270: Pseudo dice [0.6861]
2024-12-20 10:41:45.554194: Epoch time: 492.85 s
2024-12-20 10:41:47.306129: 
2024-12-20 10:41:47.307188: Epoch 90
2024-12-20 10:41:47.307988: Current learning rate: 0.00438
2024-12-20 10:50:15.832736: Validation loss did not improve from -0.46158. Patience: 66/50
2024-12-20 10:50:15.833363: train_loss -0.8106
2024-12-20 10:50:15.833996: val_loss -0.395
2024-12-20 10:50:15.834607: Pseudo dice [0.6876]
2024-12-20 10:50:15.835215: Epoch time: 508.53 s
2024-12-20 10:50:17.197022: 
2024-12-20 10:50:17.198107: Epoch 91
2024-12-20 10:50:17.198781: Current learning rate: 0.00432
2024-12-20 10:58:48.141447: Validation loss did not improve from -0.46158. Patience: 67/50
2024-12-20 10:58:48.142329: train_loss -0.8057
2024-12-20 10:58:48.143079: val_loss -0.4582
2024-12-20 10:58:48.143767: Pseudo dice [0.7194]
2024-12-20 10:58:48.144560: Epoch time: 510.95 s
2024-12-20 10:58:48.145234: Yayy! New best EMA pseudo Dice: 0.6945
2024-12-20 10:58:51.016161: 
2024-12-20 10:58:51.017480: Epoch 92
2024-12-20 10:58:51.018181: Current learning rate: 0.00425
2024-12-20 11:07:13.132282: Validation loss did not improve from -0.46158. Patience: 68/50
2024-12-20 11:07:13.133199: train_loss -0.8101
2024-12-20 11:07:13.133975: val_loss -0.3958
2024-12-20 11:07:13.134661: Pseudo dice [0.6978]
2024-12-20 11:07:13.135614: Epoch time: 502.12 s
2024-12-20 11:07:13.136386: Yayy! New best EMA pseudo Dice: 0.6948
2024-12-20 11:07:14.937027: 
2024-12-20 11:07:14.938107: Epoch 93
2024-12-20 11:07:14.938996: Current learning rate: 0.00419
2024-12-20 11:15:47.848721: Validation loss did not improve from -0.46158. Patience: 69/50
2024-12-20 11:15:47.849774: train_loss -0.8129
2024-12-20 11:15:47.850511: val_loss -0.3977
2024-12-20 11:15:47.851190: Pseudo dice [0.6976]
2024-12-20 11:15:47.851973: Epoch time: 512.91 s
2024-12-20 11:15:47.852700: Yayy! New best EMA pseudo Dice: 0.6951
2024-12-20 11:15:49.673136: 
2024-12-20 11:15:49.674467: Epoch 94
2024-12-20 11:15:49.675147: Current learning rate: 0.00412
2024-12-20 11:24:38.126004: Validation loss did not improve from -0.46158. Patience: 70/50
2024-12-20 11:24:38.127023: train_loss -0.8149
2024-12-20 11:24:38.127871: val_loss -0.3412
2024-12-20 11:24:38.128622: Pseudo dice [0.6727]
2024-12-20 11:24:38.129229: Epoch time: 528.46 s
2024-12-20 11:24:40.047563: 
2024-12-20 11:24:40.048924: Epoch 95
2024-12-20 11:24:40.049763: Current learning rate: 0.00405
2024-12-20 11:33:13.736050: Validation loss did not improve from -0.46158. Patience: 71/50
2024-12-20 11:33:13.745362: train_loss -0.8139
2024-12-20 11:33:13.746526: val_loss -0.3792
2024-12-20 11:33:13.747231: Pseudo dice [0.6925]
2024-12-20 11:33:13.748047: Epoch time: 513.7 s
2024-12-20 11:33:15.127495: 
2024-12-20 11:33:15.128876: Epoch 96
2024-12-20 11:33:15.129676: Current learning rate: 0.00399
2024-12-20 11:41:47.909420: Validation loss did not improve from -0.46158. Patience: 72/50
2024-12-20 11:41:47.910743: train_loss -0.8147
2024-12-20 11:41:47.911609: val_loss -0.3703
2024-12-20 11:41:47.912340: Pseudo dice [0.6868]
2024-12-20 11:41:47.913030: Epoch time: 512.78 s
2024-12-20 11:41:49.302982: 
2024-12-20 11:41:49.304376: Epoch 97
2024-12-20 11:41:49.305116: Current learning rate: 0.00392
2024-12-20 11:49:47.721388: Validation loss did not improve from -0.46158. Patience: 73/50
2024-12-20 11:49:47.722595: train_loss -0.8152
2024-12-20 11:49:47.723385: val_loss -0.344
2024-12-20 11:49:47.724147: Pseudo dice [0.6746]
2024-12-20 11:49:47.724884: Epoch time: 478.42 s
2024-12-20 11:49:49.119923: 
2024-12-20 11:49:49.121076: Epoch 98
2024-12-20 11:49:49.121992: Current learning rate: 0.00385
2024-12-20 11:58:17.142544: Validation loss did not improve from -0.46158. Patience: 74/50
2024-12-20 11:58:17.143368: train_loss -0.8155
2024-12-20 11:58:17.144098: val_loss -0.3984
2024-12-20 11:58:17.144763: Pseudo dice [0.6947]
2024-12-20 11:58:17.145417: Epoch time: 508.02 s
2024-12-20 11:58:18.536354: 
2024-12-20 11:58:18.537498: Epoch 99
2024-12-20 11:58:18.538362: Current learning rate: 0.00379
2024-12-20 12:07:20.147510: Validation loss did not improve from -0.46158. Patience: 75/50
2024-12-20 12:07:20.148365: train_loss -0.8175
2024-12-20 12:07:20.149124: val_loss -0.3536
2024-12-20 12:07:20.149820: Pseudo dice [0.6737]
2024-12-20 12:07:20.150648: Epoch time: 541.61 s
2024-12-20 12:07:21.981947: 
2024-12-20 12:07:21.982979: Epoch 100
2024-12-20 12:07:21.983695: Current learning rate: 0.00372
2024-12-20 12:15:37.756055: Validation loss did not improve from -0.46158. Patience: 76/50
2024-12-20 12:15:37.756988: train_loss -0.8153
2024-12-20 12:15:37.757862: val_loss -0.4351
2024-12-20 12:15:37.758638: Pseudo dice [0.7056]
2024-12-20 12:15:37.759255: Epoch time: 495.78 s
2024-12-20 12:15:39.174485: 
2024-12-20 12:15:39.175605: Epoch 101
2024-12-20 12:15:39.176251: Current learning rate: 0.00365
2024-12-20 12:24:07.552982: Validation loss did not improve from -0.46158. Patience: 77/50
2024-12-20 12:24:07.554218: train_loss -0.8189
2024-12-20 12:24:07.555201: val_loss -0.3904
2024-12-20 12:24:07.556003: Pseudo dice [0.7011]
2024-12-20 12:24:07.556833: Epoch time: 508.38 s
2024-12-20 12:24:09.023370: 
2024-12-20 12:24:09.024553: Epoch 102
2024-12-20 12:24:09.025293: Current learning rate: 0.00359
2024-12-20 12:32:35.311972: Validation loss did not improve from -0.46158. Patience: 78/50
2024-12-20 12:32:35.312844: train_loss -0.8189
2024-12-20 12:32:35.313744: val_loss -0.3899
2024-12-20 12:32:35.314688: Pseudo dice [0.684]
2024-12-20 12:32:35.315651: Epoch time: 506.29 s
2024-12-20 12:32:37.101152: 
2024-12-20 12:32:37.102963: Epoch 103
2024-12-20 12:32:37.103880: Current learning rate: 0.00352
2024-12-20 12:41:13.434927: Validation loss did not improve from -0.46158. Patience: 79/50
2024-12-20 12:41:13.435751: train_loss -0.8191
2024-12-20 12:41:13.436907: val_loss -0.3368
2024-12-20 12:41:13.437919: Pseudo dice [0.6624]
2024-12-20 12:41:13.439001: Epoch time: 516.34 s
2024-12-20 12:41:14.872532: 
2024-12-20 12:41:14.874044: Epoch 104
2024-12-20 12:41:14.875211: Current learning rate: 0.00345
2024-12-20 12:50:11.273015: Validation loss did not improve from -0.46158. Patience: 80/50
2024-12-20 12:50:11.274386: train_loss -0.8222
2024-12-20 12:50:11.275083: val_loss -0.3785
2024-12-20 12:50:11.275693: Pseudo dice [0.6912]
2024-12-20 12:50:11.276319: Epoch time: 536.4 s
2024-12-20 12:50:13.081653: 
2024-12-20 12:50:13.082774: Epoch 105
2024-12-20 12:50:13.083439: Current learning rate: 0.00338
2024-12-20 12:59:11.233289: Validation loss did not improve from -0.46158. Patience: 81/50
2024-12-20 12:59:11.234434: train_loss -0.8206
2024-12-20 12:59:11.235561: val_loss -0.3361
2024-12-20 12:59:11.236447: Pseudo dice [0.6722]
2024-12-20 12:59:11.237354: Epoch time: 538.15 s
2024-12-20 12:59:12.930970: 
2024-12-20 12:59:12.932239: Epoch 106
2024-12-20 12:59:12.932874: Current learning rate: 0.00332
2024-12-20 13:09:16.809377: Validation loss did not improve from -0.46158. Patience: 82/50
2024-12-20 13:09:16.810169: train_loss -0.8212
2024-12-20 13:09:16.810956: val_loss -0.3951
2024-12-20 13:09:16.811705: Pseudo dice [0.6949]
2024-12-20 13:09:16.812362: Epoch time: 603.88 s
2024-12-20 13:09:18.263452: 
2024-12-20 13:09:18.264520: Epoch 107
2024-12-20 13:09:18.265243: Current learning rate: 0.00325
2024-12-20 13:17:52.349188: Validation loss did not improve from -0.46158. Patience: 83/50
2024-12-20 13:17:52.350071: train_loss -0.8257
2024-12-20 13:17:52.350951: val_loss -0.3581
2024-12-20 13:17:52.351867: Pseudo dice [0.6785]
2024-12-20 13:17:52.352738: Epoch time: 514.09 s
2024-12-20 13:17:53.772691: 
2024-12-20 13:17:53.773800: Epoch 108
2024-12-20 13:17:53.774697: Current learning rate: 0.00318
2024-12-20 13:26:31.126819: Validation loss did not improve from -0.46158. Patience: 84/50
2024-12-20 13:26:31.127635: train_loss -0.8239
2024-12-20 13:26:31.128335: val_loss -0.4245
2024-12-20 13:26:31.129092: Pseudo dice [0.709]
2024-12-20 13:26:31.129829: Epoch time: 517.36 s
2024-12-20 13:26:32.571332: 
2024-12-20 13:26:32.572354: Epoch 109
2024-12-20 13:26:32.573029: Current learning rate: 0.00311
2024-12-20 13:35:35.306547: Validation loss did not improve from -0.46158. Patience: 85/50
2024-12-20 13:35:35.326533: train_loss -0.8223
2024-12-20 13:35:35.327298: val_loss -0.3416
2024-12-20 13:35:35.327912: Pseudo dice [0.6694]
2024-12-20 13:35:35.328623: Epoch time: 542.76 s
2024-12-20 13:35:37.298542: 
2024-12-20 13:35:37.299609: Epoch 110
2024-12-20 13:35:37.300246: Current learning rate: 0.00304
2024-12-20 13:45:24.239325: Validation loss did not improve from -0.46158. Patience: 86/50
2024-12-20 13:45:24.240216: train_loss -0.8222
2024-12-20 13:45:24.241063: val_loss -0.3507
2024-12-20 13:45:24.241741: Pseudo dice [0.674]
2024-12-20 13:45:24.242407: Epoch time: 586.94 s
2024-12-20 13:45:25.665587: 
2024-12-20 13:45:25.666743: Epoch 111
2024-12-20 13:45:25.667613: Current learning rate: 0.00297
2024-12-20 13:54:17.377196: Validation loss did not improve from -0.46158. Patience: 87/50
2024-12-20 13:54:17.378606: train_loss -0.8211
2024-12-20 13:54:17.380172: val_loss -0.3354
2024-12-20 13:54:17.380869: Pseudo dice [0.6726]
2024-12-20 13:54:17.381585: Epoch time: 531.71 s
2024-12-20 13:54:18.810192: 
2024-12-20 13:54:18.811134: Epoch 112
2024-12-20 13:54:18.811793: Current learning rate: 0.00291
2024-12-20 14:02:34.096183: Validation loss did not improve from -0.46158. Patience: 88/50
2024-12-20 14:02:34.097055: train_loss -0.8259
2024-12-20 14:02:34.097867: val_loss -0.3795
2024-12-20 14:02:34.098479: Pseudo dice [0.6869]
2024-12-20 14:02:34.099131: Epoch time: 495.29 s
2024-12-20 14:02:35.503960: 
2024-12-20 14:02:35.504783: Epoch 113
2024-12-20 14:02:35.505382: Current learning rate: 0.00284
2024-12-20 14:09:37.062121: Validation loss did not improve from -0.46158. Patience: 89/50
2024-12-20 14:09:37.062773: train_loss -0.8253
2024-12-20 14:09:37.063451: val_loss -0.3938
2024-12-20 14:09:37.064076: Pseudo dice [0.6942]
2024-12-20 14:09:37.064773: Epoch time: 421.56 s
2024-12-20 14:09:38.839994: 
2024-12-20 14:09:38.841072: Epoch 114
2024-12-20 14:09:38.841853: Current learning rate: 0.00277
2024-12-20 14:15:42.106210: Validation loss did not improve from -0.46158. Patience: 90/50
2024-12-20 14:15:42.107684: train_loss -0.8252
2024-12-20 14:15:42.108735: val_loss -0.4011
2024-12-20 14:15:42.109448: Pseudo dice [0.6983]
2024-12-20 14:15:42.110249: Epoch time: 363.27 s
2024-12-20 14:15:43.905055: 
2024-12-20 14:15:43.906950: Epoch 115
2024-12-20 14:15:43.907732: Current learning rate: 0.0027
2024-12-20 14:21:31.453608: Validation loss did not improve from -0.46158. Patience: 91/50
2024-12-20 14:21:31.454579: train_loss -0.8286
2024-12-20 14:21:31.455508: val_loss -0.3975
2024-12-20 14:21:31.456380: Pseudo dice [0.7007]
2024-12-20 14:21:31.457299: Epoch time: 347.55 s
2024-12-20 14:21:32.908824: 
2024-12-20 14:21:32.910256: Epoch 116
2024-12-20 14:21:32.911274: Current learning rate: 0.00263
2024-12-20 14:27:12.733598: Validation loss did not improve from -0.46158. Patience: 92/50
2024-12-20 14:27:12.734642: train_loss -0.8269
2024-12-20 14:27:12.735513: val_loss -0.3411
2024-12-20 14:27:12.736381: Pseudo dice [0.6758]
2024-12-20 14:27:12.737204: Epoch time: 339.83 s
2024-12-20 14:27:14.182774: 
2024-12-20 14:27:14.183988: Epoch 117
2024-12-20 14:27:14.184960: Current learning rate: 0.00256
2024-12-20 14:33:26.669979: Validation loss did not improve from -0.46158. Patience: 93/50
2024-12-20 14:33:26.670773: train_loss -0.8308
2024-12-20 14:33:26.671496: val_loss -0.3719
2024-12-20 14:33:26.672116: Pseudo dice [0.6892]
2024-12-20 14:33:26.672736: Epoch time: 372.49 s
2024-12-20 14:33:28.104656: 
2024-12-20 14:33:28.105872: Epoch 118
2024-12-20 14:33:28.106653: Current learning rate: 0.00249
2024-12-20 14:39:02.665423: Validation loss did not improve from -0.46158. Patience: 94/50
2024-12-20 14:39:02.667277: train_loss -0.8294
2024-12-20 14:39:02.668242: val_loss -0.3325
2024-12-20 14:39:02.669036: Pseudo dice [0.6609]
2024-12-20 14:39:02.669752: Epoch time: 334.56 s
2024-12-20 14:39:04.121889: 
2024-12-20 14:39:04.123032: Epoch 119
2024-12-20 14:39:04.123762: Current learning rate: 0.00242
2024-12-20 14:45:18.620134: Validation loss did not improve from -0.46158. Patience: 95/50
2024-12-20 14:45:18.627829: train_loss -0.8297
2024-12-20 14:45:18.628730: val_loss -0.4157
2024-12-20 14:45:18.629437: Pseudo dice [0.7073]
2024-12-20 14:45:18.630157: Epoch time: 374.5 s
2024-12-20 14:45:20.531133: 
2024-12-20 14:45:20.532191: Epoch 120
2024-12-20 14:45:20.532883: Current learning rate: 0.00235
2024-12-20 14:51:47.472248: Validation loss did not improve from -0.46158. Patience: 96/50
2024-12-20 14:51:47.473191: train_loss -0.8309
2024-12-20 14:51:47.473972: val_loss -0.3934
2024-12-20 14:51:47.474631: Pseudo dice [0.6937]
2024-12-20 14:51:47.475441: Epoch time: 386.94 s
2024-12-20 14:51:48.931848: 
2024-12-20 14:51:48.933280: Epoch 121
2024-12-20 14:51:48.934112: Current learning rate: 0.00228
2024-12-20 14:58:00.723957: Validation loss did not improve from -0.46158. Patience: 97/50
2024-12-20 14:58:00.725679: train_loss -0.83
2024-12-20 14:58:00.727429: val_loss -0.3911
2024-12-20 14:58:00.728134: Pseudo dice [0.6947]
2024-12-20 14:58:00.729112: Epoch time: 371.8 s
2024-12-20 14:58:02.258788: 
2024-12-20 14:58:02.260017: Epoch 122
2024-12-20 14:58:02.261000: Current learning rate: 0.00221
2024-12-20 15:03:21.076243: Validation loss did not improve from -0.46158. Patience: 98/50
2024-12-20 15:03:21.077210: train_loss -0.8327
2024-12-20 15:03:21.077940: val_loss -0.3943
2024-12-20 15:03:21.078541: Pseudo dice [0.7026]
2024-12-20 15:03:21.079205: Epoch time: 318.82 s
2024-12-20 15:03:22.605210: 
2024-12-20 15:03:22.606449: Epoch 123
2024-12-20 15:03:22.607175: Current learning rate: 0.00214
2024-12-20 15:08:56.167401: Validation loss did not improve from -0.46158. Patience: 99/50
2024-12-20 15:08:56.168441: train_loss -0.8322
2024-12-20 15:08:56.169177: val_loss -0.3547
2024-12-20 15:08:56.169835: Pseudo dice [0.685]
2024-12-20 15:08:56.170568: Epoch time: 333.56 s
2024-12-20 15:08:57.617776: 
2024-12-20 15:08:57.618858: Epoch 124
2024-12-20 15:08:57.619645: Current learning rate: 0.00207
2024-12-20 15:15:17.312697: Validation loss did not improve from -0.46158. Patience: 100/50
2024-12-20 15:15:17.313574: train_loss -0.8326
2024-12-20 15:15:17.314285: val_loss -0.4195
2024-12-20 15:15:17.314878: Pseudo dice [0.7001]
2024-12-20 15:15:17.315469: Epoch time: 379.7 s
2024-12-20 15:15:19.705762: 
2024-12-20 15:15:19.707006: Epoch 125
2024-12-20 15:15:19.707662: Current learning rate: 0.00199
2024-12-20 15:21:30.048265: Validation loss did not improve from -0.46158. Patience: 101/50
2024-12-20 15:21:30.049017: train_loss -0.8334
2024-12-20 15:21:30.049729: val_loss -0.3819
2024-12-20 15:21:30.050380: Pseudo dice [0.6939]
2024-12-20 15:21:30.051140: Epoch time: 370.34 s
2024-12-20 15:21:31.533498: 
2024-12-20 15:21:31.534833: Epoch 126
2024-12-20 15:21:31.535522: Current learning rate: 0.00192
2024-12-20 15:27:22.047262: Validation loss did not improve from -0.46158. Patience: 102/50
2024-12-20 15:27:22.048038: train_loss -0.8333
2024-12-20 15:27:22.048695: val_loss -0.3788
2024-12-20 15:27:22.049332: Pseudo dice [0.6933]
2024-12-20 15:27:22.049935: Epoch time: 350.52 s
2024-12-20 15:27:23.526512: 
2024-12-20 15:27:23.527618: Epoch 127
2024-12-20 15:27:23.528314: Current learning rate: 0.00185
2024-12-20 15:33:12.129098: Validation loss did not improve from -0.46158. Patience: 103/50
2024-12-20 15:33:12.130041: train_loss -0.834
2024-12-20 15:33:12.130846: val_loss -0.3815
2024-12-20 15:33:12.131470: Pseudo dice [0.6933]
2024-12-20 15:33:12.132048: Epoch time: 348.6 s
2024-12-20 15:33:13.561976: 
2024-12-20 15:33:13.563258: Epoch 128
2024-12-20 15:33:13.563873: Current learning rate: 0.00178
2024-12-20 15:39:32.753289: Validation loss did not improve from -0.46158. Patience: 104/50
2024-12-20 15:39:32.754156: train_loss -0.8352
2024-12-20 15:39:32.754939: val_loss -0.3796
2024-12-20 15:39:32.755642: Pseudo dice [0.702]
2024-12-20 15:39:32.756417: Epoch time: 379.19 s
2024-12-20 15:39:34.217357: 
2024-12-20 15:39:34.218539: Epoch 129
2024-12-20 15:39:34.219256: Current learning rate: 0.0017
2024-12-20 15:44:53.062223: Validation loss did not improve from -0.46158. Patience: 105/50
2024-12-20 15:44:53.062985: train_loss -0.8358
2024-12-20 15:44:53.063910: val_loss -0.3353
2024-12-20 15:44:53.064622: Pseudo dice [0.6846]
2024-12-20 15:44:53.065335: Epoch time: 318.85 s
2024-12-20 15:44:54.919316: 
2024-12-20 15:44:54.920397: Epoch 130
2024-12-20 15:44:54.921159: Current learning rate: 0.00163
2024-12-20 15:51:15.881374: Validation loss did not improve from -0.46158. Patience: 106/50
2024-12-20 15:51:15.883759: train_loss -0.8369
2024-12-20 15:51:15.884635: val_loss -0.3472
2024-12-20 15:51:15.885382: Pseudo dice [0.6872]
2024-12-20 15:51:15.886100: Epoch time: 380.97 s
2024-12-20 15:51:17.337539: 
2024-12-20 15:51:17.338792: Epoch 131
2024-12-20 15:51:17.339573: Current learning rate: 0.00156
2024-12-20 15:57:17.582631: Validation loss did not improve from -0.46158. Patience: 107/50
2024-12-20 15:57:17.583767: train_loss -0.8345
2024-12-20 15:57:17.584742: val_loss -0.3837
2024-12-20 15:57:17.585458: Pseudo dice [0.7067]
2024-12-20 15:57:17.586233: Epoch time: 360.25 s
2024-12-20 15:57:19.018443: 
2024-12-20 15:57:19.019362: Epoch 132
2024-12-20 15:57:19.020072: Current learning rate: 0.00148
2024-12-20 16:03:41.240396: Validation loss did not improve from -0.46158. Patience: 108/50
2024-12-20 16:03:41.241358: train_loss -0.8391
2024-12-20 16:03:41.242077: val_loss -0.3765
2024-12-20 16:03:41.242894: Pseudo dice [0.6905]
2024-12-20 16:03:41.243616: Epoch time: 382.22 s
2024-12-20 16:03:42.658308: 
2024-12-20 16:03:42.659363: Epoch 133
2024-12-20 16:03:42.660030: Current learning rate: 0.00141
2024-12-20 16:09:33.964685: Validation loss did not improve from -0.46158. Patience: 109/50
2024-12-20 16:09:33.965591: train_loss -0.8362
2024-12-20 16:09:33.966276: val_loss -0.3573
2024-12-20 16:09:33.966867: Pseudo dice [0.6743]
2024-12-20 16:09:33.967472: Epoch time: 351.31 s
2024-12-20 16:09:35.363634: 
2024-12-20 16:09:35.364624: Epoch 134
2024-12-20 16:09:35.365244: Current learning rate: 0.00133
2024-12-20 16:15:21.980329: Validation loss did not improve from -0.46158. Patience: 110/50
2024-12-20 16:15:21.981202: train_loss -0.8377
2024-12-20 16:15:21.982165: val_loss -0.3913
2024-12-20 16:15:21.982839: Pseudo dice [0.7036]
2024-12-20 16:15:21.983500: Epoch time: 346.62 s
2024-12-20 16:15:23.761624: 
2024-12-20 16:15:23.762722: Epoch 135
2024-12-20 16:15:23.763418: Current learning rate: 0.00126
2024-12-20 16:20:30.723067: Validation loss did not improve from -0.46158. Patience: 111/50
2024-12-20 16:20:30.724089: train_loss -0.8387
2024-12-20 16:20:30.724857: val_loss -0.3435
2024-12-20 16:20:30.725519: Pseudo dice [0.6742]
2024-12-20 16:20:30.726309: Epoch time: 306.96 s
2024-12-20 16:20:32.483881: 
2024-12-20 16:20:32.485157: Epoch 136
2024-12-20 16:20:32.486039: Current learning rate: 0.00118
2024-12-20 16:26:08.508491: Validation loss did not improve from -0.46158. Patience: 112/50
2024-12-20 16:26:08.509492: train_loss -0.8377
2024-12-20 16:26:08.510394: val_loss -0.3424
2024-12-20 16:26:08.511117: Pseudo dice [0.6695]
2024-12-20 16:26:08.511847: Epoch time: 336.03 s
2024-12-20 16:26:10.123518: 
2024-12-20 16:26:10.124701: Epoch 137
2024-12-20 16:26:10.125439: Current learning rate: 0.00111
2024-12-20 16:31:07.591717: Validation loss did not improve from -0.46158. Patience: 113/50
2024-12-20 16:31:07.592940: train_loss -0.8382
2024-12-20 16:31:07.593978: val_loss -0.3466
2024-12-20 16:31:07.594831: Pseudo dice [0.6791]
2024-12-20 16:31:07.595408: Epoch time: 297.47 s
2024-12-20 16:31:09.052909: 
2024-12-20 16:31:09.053979: Epoch 138
2024-12-20 16:31:09.054746: Current learning rate: 0.00103
2024-12-20 16:35:23.330475: Validation loss did not improve from -0.46158. Patience: 114/50
2024-12-20 16:35:23.331244: train_loss -0.8381
2024-12-20 16:35:23.332087: val_loss -0.3573
2024-12-20 16:35:23.332659: Pseudo dice [0.6945]
2024-12-20 16:35:23.333452: Epoch time: 254.28 s
2024-12-20 16:35:24.758152: 
2024-12-20 16:35:24.759245: Epoch 139
2024-12-20 16:35:24.759867: Current learning rate: 0.00095
2024-12-20 16:40:22.995726: Validation loss did not improve from -0.46158. Patience: 115/50
2024-12-20 16:40:22.996541: train_loss -0.8377
2024-12-20 16:40:22.997172: val_loss -0.3506
2024-12-20 16:40:22.997757: Pseudo dice [0.6863]
2024-12-20 16:40:22.998383: Epoch time: 298.24 s
2024-12-20 16:40:24.813740: 
2024-12-20 16:40:24.814633: Epoch 140
2024-12-20 16:40:24.815388: Current learning rate: 0.00087
2024-12-20 16:44:52.435328: Validation loss did not improve from -0.46158. Patience: 116/50
2024-12-20 16:44:52.436241: train_loss -0.8393
2024-12-20 16:44:52.436908: val_loss -0.385
2024-12-20 16:44:52.437497: Pseudo dice [0.6895]
2024-12-20 16:44:52.438127: Epoch time: 267.62 s
2024-12-20 16:44:53.862267: 
2024-12-20 16:44:53.863420: Epoch 141
2024-12-20 16:44:53.864120: Current learning rate: 0.00079
2024-12-20 16:50:31.810939: Validation loss did not improve from -0.46158. Patience: 117/50
2024-12-20 16:50:31.811582: train_loss -0.8401
2024-12-20 16:50:31.812236: val_loss -0.3935
2024-12-20 16:50:31.812990: Pseudo dice [0.7106]
2024-12-20 16:50:31.813581: Epoch time: 337.95 s
2024-12-20 16:50:33.286556: 
2024-12-20 16:50:33.287755: Epoch 142
2024-12-20 16:50:33.288514: Current learning rate: 0.00071
2024-12-20 16:53:23.030762: Validation loss did not improve from -0.46158. Patience: 118/50
2024-12-20 16:53:23.031650: train_loss -0.8382
2024-12-20 16:53:23.032476: val_loss -0.3914
2024-12-20 16:53:23.033301: Pseudo dice [0.6871]
2024-12-20 16:53:23.034290: Epoch time: 169.75 s
2024-12-20 16:53:24.501812: 
2024-12-20 16:53:24.502781: Epoch 143
2024-12-20 16:53:24.503545: Current learning rate: 0.00063
2024-12-20 16:56:18.466625: Validation loss did not improve from -0.46158. Patience: 119/50
2024-12-20 16:56:18.467618: train_loss -0.8416
2024-12-20 16:56:18.468290: val_loss -0.4178
2024-12-20 16:56:18.468869: Pseudo dice [0.7209]
2024-12-20 16:56:18.469480: Epoch time: 173.97 s
2024-12-20 16:56:19.897008: 
2024-12-20 16:56:19.898096: Epoch 144
2024-12-20 16:56:19.898742: Current learning rate: 0.00055
2024-12-20 16:59:03.033597: Validation loss did not improve from -0.46158. Patience: 120/50
2024-12-20 16:59:03.034333: train_loss -0.84
2024-12-20 16:59:03.035251: val_loss -0.356
2024-12-20 16:59:03.036003: Pseudo dice [0.6877]
2024-12-20 16:59:03.036680: Epoch time: 163.14 s
2024-12-20 16:59:04.903413: 
2024-12-20 16:59:04.904755: Epoch 145
2024-12-20 16:59:04.905511: Current learning rate: 0.00047
2024-12-20 17:02:12.454798: Validation loss did not improve from -0.46158. Patience: 121/50
2024-12-20 17:02:12.455756: train_loss -0.8403
2024-12-20 17:02:12.456435: val_loss -0.3268
2024-12-20 17:02:12.457060: Pseudo dice [0.6707]
2024-12-20 17:02:12.457620: Epoch time: 187.55 s
2024-12-20 17:02:14.240546: 
2024-12-20 17:02:14.241998: Epoch 146
2024-12-20 17:02:14.243517: Current learning rate: 0.00038
2024-12-20 17:05:53.623019: Validation loss did not improve from -0.46158. Patience: 122/50
2024-12-20 17:05:53.624136: train_loss -0.8421
2024-12-20 17:05:53.625000: val_loss -0.3689
2024-12-20 17:05:53.626097: Pseudo dice [0.6879]
2024-12-20 17:05:53.627199: Epoch time: 219.38 s
2024-12-20 17:05:55.076509: 
2024-12-20 17:05:55.077674: Epoch 147
2024-12-20 17:05:55.078375: Current learning rate: 0.0003
2024-12-20 17:08:18.400555: Validation loss did not improve from -0.46158. Patience: 123/50
2024-12-20 17:08:18.401524: train_loss -0.8397
2024-12-20 17:08:18.402374: val_loss -0.3649
2024-12-20 17:08:18.403095: Pseudo dice [0.6877]
2024-12-20 17:08:18.403752: Epoch time: 143.33 s
2024-12-20 17:08:19.855113: 
2024-12-20 17:08:19.856389: Epoch 148
2024-12-20 17:08:19.857318: Current learning rate: 0.00021
2024-12-20 17:11:23.164887: Validation loss did not improve from -0.46158. Patience: 124/50
2024-12-20 17:11:23.165860: train_loss -0.8432
2024-12-20 17:11:23.166858: val_loss -0.3845
2024-12-20 17:11:23.167627: Pseudo dice [0.698]
2024-12-20 17:11:23.168473: Epoch time: 183.31 s
2024-12-20 17:11:24.664314: 
2024-12-20 17:11:24.665450: Epoch 149
2024-12-20 17:11:24.666240: Current learning rate: 0.00011
2024-12-20 17:14:33.182437: Validation loss did not improve from -0.46158. Patience: 125/50
2024-12-20 17:14:33.184212: train_loss -0.8423
2024-12-20 17:14:33.185191: val_loss -0.3875
2024-12-20 17:14:33.186095: Pseudo dice [0.6981]
2024-12-20 17:14:33.186829: Epoch time: 188.52 s
2024-12-20 17:14:35.121967: Training done.
2024-12-20 17:14:35.272580: Using splits from existing split file: /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset308_Sohee_Calcium_OCT_CrossValidation/splits_final_40.json
2024-12-20 17:14:35.287850: The split file contains 5 splits.
2024-12-20 17:14:35.288607: Desired fold for training: 1
2024-12-20 17:14:35.289181: This split has 3 training and 6 validation cases.
2024-12-20 17:14:35.289964: predicting 101-019
2024-12-20 17:14:35.307793: 101-019, shape torch.Size([1, 375, 498, 498]), rank 0
2024-12-20 17:16:57.605461: predicting 101-044
2024-12-20 17:16:57.620251: 101-044, shape torch.Size([1, 404, 498, 498]), rank 0
2024-12-20 17:19:17.875241: predicting 101-045
2024-12-20 17:19:17.890076: 101-045, shape torch.Size([1, 375, 498, 498]), rank 0
2024-12-20 17:21:20.389157: predicting 106-002
2024-12-20 17:21:20.414108: 106-002, shape torch.Size([1, 539, 498, 498]), rank 0
2024-12-20 17:24:31.265788: predicting 704-003
2024-12-20 17:24:31.385496: 704-003, shape torch.Size([1, 375, 498, 498]), rank 0
2024-12-20 17:26:29.448828: predicting 706-005
2024-12-20 17:26:29.471775: 706-005, shape torch.Size([1, 375, 498, 498]), rank 0
2024-12-20 17:28:53.532740: Validation complete
2024-12-20 17:28:53.533497: Mean Validation Dice:  0.6970857847363771

############################
INFO: You are using the old nnU-Net default plans. We have updated our recommendations. Please consider using those instead! Read more here: https://github.com/MIC-DKFZ/nnUNet/blob/master/documentation/resenc_presets.md
############################

Using device: cuda:0

#######################################################################
Please cite the following paper when using nnU-Net:
Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.
#######################################################################

2024-12-20 17:29:00.259047: Using torch.compile...

############################
INFO: You are using the old nnU-Net default plans. We have updated our recommendations. Please consider using those instead! Read more here: https://github.com/MIC-DKFZ/nnUNet/blob/master/documentation/resenc_presets.md
############################

Using device: cuda:0

#######################################################################
Please cite the following paper when using nnU-Net:
Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.
#######################################################################

2024-12-20 17:29:00.260684: Using torch.compile...
/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
################### Loading pretrained weights from file  /home/gridsan/nchutisilp/projects/OpenAI-CLIP/clip_pretrained_nnUNet.pt ###################
Below is the list of overlapping blocks in pretrained model and nnUNet architecture:
encoder.stages.0.0.convs.0.conv.weight shape torch.Size([32, 1, 3, 3, 3])
encoder.stages.0.0.convs.0.conv.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.norm.weight shape torch.Size([32])
encoder.stages.0.0.convs.0.norm.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.0.weight shape torch.Size([32, 1, 3, 3, 3])
encoder.stages.0.0.convs.0.all_modules.0.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.1.weight shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.1.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
encoder.stages.0.0.convs.1.conv.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.norm.weight shape torch.Size([32])
encoder.stages.0.0.convs.1.norm.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
encoder.stages.0.0.convs.1.all_modules.0.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.1.weight shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.1.bias shape torch.Size([32])
encoder.stages.1.0.convs.0.conv.weight shape torch.Size([64, 32, 3, 3, 3])
encoder.stages.1.0.convs.0.conv.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.norm.weight shape torch.Size([64])
encoder.stages.1.0.convs.0.norm.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.0.weight shape torch.Size([64, 32, 3, 3, 3])
encoder.stages.1.0.convs.0.all_modules.0.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.1.weight shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.1.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
encoder.stages.1.0.convs.1.conv.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.norm.weight shape torch.Size([64])
encoder.stages.1.0.convs.1.norm.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
encoder.stages.1.0.convs.1.all_modules.0.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.1.weight shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.1.bias shape torch.Size([64])
encoder.stages.2.0.convs.0.conv.weight shape torch.Size([128, 64, 3, 3, 3])
encoder.stages.2.0.convs.0.conv.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.norm.weight shape torch.Size([128])
encoder.stages.2.0.convs.0.norm.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.0.weight shape torch.Size([128, 64, 3, 3, 3])
encoder.stages.2.0.convs.0.all_modules.0.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.1.weight shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.1.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
encoder.stages.2.0.convs.1.conv.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.norm.weight shape torch.Size([128])
encoder.stages.2.0.convs.1.norm.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
encoder.stages.2.0.convs.1.all_modules.0.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.1.weight shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.1.bias shape torch.Size([128])
encoder.stages.3.0.convs.0.conv.weight shape torch.Size([256, 128, 3, 3, 3])
encoder.stages.3.0.convs.0.conv.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.norm.weight shape torch.Size([256])
encoder.stages.3.0.convs.0.norm.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.0.weight shape torch.Size([256, 128, 3, 3, 3])
encoder.stages.3.0.convs.0.all_modules.0.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.1.weight shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.1.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
encoder.stages.3.0.convs.1.conv.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.norm.weight shape torch.Size([256])
encoder.stages.3.0.convs.1.norm.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
encoder.stages.3.0.convs.1.all_modules.0.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.1.weight shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.1.bias shape torch.Size([256])
encoder.stages.4.0.convs.0.conv.weight shape torch.Size([320, 256, 3, 3, 3])
encoder.stages.4.0.convs.0.conv.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.norm.weight shape torch.Size([320])
encoder.stages.4.0.convs.0.norm.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.0.weight shape torch.Size([320, 256, 3, 3, 3])
encoder.stages.4.0.convs.0.all_modules.0.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.1.weight shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.1.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.4.0.convs.1.conv.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.norm.weight shape torch.Size([320])
encoder.stages.4.0.convs.1.norm.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.4.0.convs.1.all_modules.0.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.1.weight shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.1.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.0.conv.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.norm.weight shape torch.Size([320])
encoder.stages.5.0.convs.0.norm.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.0.all_modules.0.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.1.weight shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.1.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.1.conv.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.norm.weight shape torch.Size([320])
encoder.stages.5.0.convs.1.norm.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.1.all_modules.0.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.1.weight shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.0.0.convs.0.conv.weight shape torch.Size([32, 1, 3, 3, 3])
decoder.encoder.stages.0.0.convs.0.conv.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.norm.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.norm.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.0.weight shape torch.Size([32, 1, 3, 3, 3])
decoder.encoder.stages.0.0.convs.0.all_modules.0.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.1.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.1.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.encoder.stages.0.0.convs.1.conv.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.norm.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.norm.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.encoder.stages.0.0.convs.1.all_modules.0.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.1.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.1.bias shape torch.Size([32])
decoder.encoder.stages.1.0.convs.0.conv.weight shape torch.Size([64, 32, 3, 3, 3])
decoder.encoder.stages.1.0.convs.0.conv.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.norm.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.norm.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.0.weight shape torch.Size([64, 32, 3, 3, 3])
decoder.encoder.stages.1.0.convs.0.all_modules.0.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.1.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.1.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.encoder.stages.1.0.convs.1.conv.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.norm.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.norm.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.encoder.stages.1.0.convs.1.all_modules.0.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.1.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.1.bias shape torch.Size([64])
decoder.encoder.stages.2.0.convs.0.conv.weight shape torch.Size([128, 64, 3, 3, 3])
decoder.encoder.stages.2.0.convs.0.conv.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.norm.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.norm.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.0.weight shape torch.Size([128, 64, 3, 3, 3])
decoder.encoder.stages.2.0.convs.0.all_modules.0.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.1.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.1.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.encoder.stages.2.0.convs.1.conv.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.norm.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.norm.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.encoder.stages.2.0.convs.1.all_modules.0.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.1.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.1.bias shape torch.Size([128])
decoder.encoder.stages.3.0.convs.0.conv.weight shape torch.Size([256, 128, 3, 3, 3])
decoder.encoder.stages.3.0.convs.0.conv.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.norm.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.norm.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.0.weight shape torch.Size([256, 128, 3, 3, 3])
decoder.encoder.stages.3.0.convs.0.all_modules.0.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.1.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.1.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.encoder.stages.3.0.convs.1.conv.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.norm.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.norm.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.encoder.stages.3.0.convs.1.all_modules.0.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.1.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.1.bias shape torch.Size([256])
decoder.encoder.stages.4.0.convs.0.conv.weight shape torch.Size([320, 256, 3, 3, 3])
decoder.encoder.stages.4.0.convs.0.conv.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.norm.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.norm.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.0.weight shape torch.Size([320, 256, 3, 3, 3])
decoder.encoder.stages.4.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.4.0.convs.1.conv.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.norm.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.norm.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.4.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.0.conv.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.norm.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.norm.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.1.conv.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.norm.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.norm.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.stages.0.convs.0.conv.weight shape torch.Size([320, 640, 3, 3, 3])
decoder.stages.0.convs.0.conv.bias shape torch.Size([320])
decoder.stages.0.convs.0.norm.weight shape torch.Size([320])
decoder.stages.0.convs.0.norm.bias shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.0.weight shape torch.Size([320, 640, 3, 3, 3])
decoder.stages.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.stages.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.stages.0.convs.1.conv.bias shape torch.Size([320])
decoder.stages.0.convs.1.norm.weight shape torch.Size([320])
decoder.stages.0.convs.1.norm.bias shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.stages.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.stages.1.convs.0.conv.weight shape torch.Size([256, 512, 3, 3, 3])
decoder.stages.1.convs.0.conv.bias shape torch.Size([256])
decoder.stages.1.convs.0.norm.weight shape torch.Size([256])
decoder.stages.1.convs.0.norm.bias shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.0.weight shape torch.Size([256, 512, 3, 3, 3])
decoder.stages.1.convs.0.all_modules.0.bias shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.1.weight shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.1.bias shape torch.Size([256])
decoder.stages.1.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.stages.1.convs.1.conv.bias shape torch.Size([256])
decoder.stages.1.convs.1.norm.weight shape torch.Size([256])
decoder.stages.1.convs.1.norm.bias shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.stages.1.convs.1.all_modules.0.bias shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.1.weight shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.1.bias shape torch.Size([256])
decoder.stages.2.convs.0.conv.weight shape torch.Size([128, 256, 3, 3, 3])
decoder.stages.2.convs.0.conv.bias shape torch.Size([128])
decoder.stages.2.convs.0.norm.weight shape torch.Size([128])
decoder.stages.2.convs.0.norm.bias shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.0.weight shape torch.Size([128, 256, 3, 3, 3])
decoder.stages.2.convs.0.all_modules.0.bias shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.1.weight shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.1.bias shape torch.Size([128])
decoder.stages.2.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.stages.2.convs.1.conv.bias shape torch.Size([128])
decoder.stages.2.convs.1.norm.weight shape torch.Size([128])
decoder.stages.2.convs.1.norm.bias shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.stages.2.convs.1.all_modules.0.bias shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.1.weight shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.1.bias shape torch.Size([128])
decoder.stages.3.convs.0.conv.weight shape torch.Size([64, 128, 3, 3, 3])
decoder.stages.3.convs.0.conv.bias shape torch.Size([64])
decoder.stages.3.convs.0.norm.weight shape torch.Size([64])
decoder.stages.3.convs.0.norm.bias shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.0.weight shape torch.Size([64, 128, 3, 3, 3])
decoder.stages.3.convs.0.all_modules.0.bias shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.1.weight shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.1.bias shape torch.Size([64])
decoder.stages.3.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.stages.3.convs.1.conv.bias shape torch.Size([64])
decoder.stages.3.convs.1.norm.weight shape torch.Size([64])
decoder.stages.3.convs.1.norm.bias shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.stages.3.convs.1.all_modules.0.bias shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.1.weight shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.1.bias shape torch.Size([64])
decoder.stages.4.convs.0.conv.weight shape torch.Size([32, 64, 3, 3, 3])
decoder.stages.4.convs.0.conv.bias shape torch.Size([32])
decoder.stages.4.convs.0.norm.weight shape torch.Size([32])
decoder.stages.4.convs.0.norm.bias shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.0.weight shape torch.Size([32, 64, 3, 3, 3])
decoder.stages.4.convs.0.all_modules.0.bias shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.1.weight shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.1.bias shape torch.Size([32])
decoder.stages.4.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.stages.4.convs.1.conv.bias shape torch.Size([32])
decoder.stages.4.convs.1.norm.weight shape torch.Size([32])
decoder.stages.4.convs.1.norm.bias shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.stages.4.convs.1.all_modules.0.bias shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.1.weight shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.1.bias shape torch.Size([32])
decoder.transpconvs.0.weight shape torch.Size([320, 320, 1, 2, 2])
decoder.transpconvs.0.bias shape torch.Size([320])
decoder.transpconvs.1.weight shape torch.Size([320, 256, 2, 2, 2])
decoder.transpconvs.1.bias shape torch.Size([256])
decoder.transpconvs.2.weight shape torch.Size([256, 128, 2, 2, 2])
decoder.transpconvs.2.bias shape torch.Size([128])
decoder.transpconvs.3.weight shape torch.Size([128, 64, 2, 2, 2])
decoder.transpconvs.3.bias shape torch.Size([64])
decoder.transpconvs.4.weight shape torch.Size([64, 32, 2, 2, 2])
decoder.transpconvs.4.bias shape torch.Size([32])
################### Done ###################
2024-12-20 17:29:02.622288: do_dummy_2d_data_aug: True
2024-12-20 17:29:02.623219: Using splits from existing split file: /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset308_Sohee_Calcium_OCT_CrossValidation/splits_final_40.json
2024-12-20 17:29:02.625163: The split file contains 5 splits.
2024-12-20 17:29:02.626585: Desired fold for training: 3
2024-12-20 17:29:02.627555: This split has 3 training and 6 validation cases.
################### Loading pretrained weights from file  /home/gridsan/nchutisilp/projects/OpenAI-CLIP/clip_pretrained_nnUNet.pt ###################
Below is the list of overlapping blocks in pretrained model and nnUNet architecture:
encoder.stages.0.0.convs.0.conv.weight shape torch.Size([32, 1, 3, 3, 3])
encoder.stages.0.0.convs.0.conv.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.norm.weight shape torch.Size([32])
encoder.stages.0.0.convs.0.norm.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.0.weight shape torch.Size([32, 1, 3, 3, 3])
encoder.stages.0.0.convs.0.all_modules.0.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.1.weight shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.1.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
encoder.stages.0.0.convs.1.conv.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.norm.weight shape torch.Size([32])
encoder.stages.0.0.convs.1.norm.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
encoder.stages.0.0.convs.1.all_modules.0.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.1.weight shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.1.bias shape torch.Size([32])
encoder.stages.1.0.convs.0.conv.weight shape torch.Size([64, 32, 3, 3, 3])
encoder.stages.1.0.convs.0.conv.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.norm.weight shape torch.Size([64])
encoder.stages.1.0.convs.0.norm.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.0.weight shape torch.Size([64, 32, 3, 3, 3])
encoder.stages.1.0.convs.0.all_modules.0.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.1.weight shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.1.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
encoder.stages.1.0.convs.1.conv.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.norm.weight shape torch.Size([64])
encoder.stages.1.0.convs.1.norm.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
encoder.stages.1.0.convs.1.all_modules.0.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.1.weight shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.1.bias shape torch.Size([64])
encoder.stages.2.0.convs.0.conv.weight shape torch.Size([128, 64, 3, 3, 3])
encoder.stages.2.0.convs.0.conv.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.norm.weight shape torch.Size([128])
encoder.stages.2.0.convs.0.norm.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.0.weight shape torch.Size([128, 64, 3, 3, 3])
encoder.stages.2.0.convs.0.all_modules.0.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.1.weight shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.1.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
encoder.stages.2.0.convs.1.conv.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.norm.weight shape torch.Size([128])
encoder.stages.2.0.convs.1.norm.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
encoder.stages.2.0.convs.1.all_modules.0.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.1.weight shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.1.bias shape torch.Size([128])
encoder.stages.3.0.convs.0.conv.weight shape torch.Size([256, 128, 3, 3, 3])
encoder.stages.3.0.convs.0.conv.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.norm.weight shape torch.Size([256])
encoder.stages.3.0.convs.0.norm.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.0.weight shape torch.Size([256, 128, 3, 3, 3])
encoder.stages.3.0.convs.0.all_modules.0.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.1.weight shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.1.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
encoder.stages.3.0.convs.1.conv.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.norm.weight shape torch.Size([256])
encoder.stages.3.0.convs.1.norm.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
encoder.stages.3.0.convs.1.all_modules.0.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.1.weight shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.1.bias shape torch.Size([256])
encoder.stages.4.0.convs.0.conv.weight shape torch.Size([320, 256, 3, 3, 3])
encoder.stages.4.0.convs.0.conv.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.norm.weight shape torch.Size([320])
encoder.stages.4.0.convs.0.norm.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.0.weight shape torch.Size([320, 256, 3, 3, 3])
encoder.stages.4.0.convs.0.all_modules.0.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.1.weight shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.1.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.4.0.convs.1.conv.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.norm.weight shape torch.Size([320])
encoder.stages.4.0.convs.1.norm.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.4.0.convs.1.all_modules.0.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.1.weight shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.1.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.0.conv.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.norm.weight shape torch.Size([320])
encoder.stages.5.0.convs.0.norm.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.0.all_modules.0.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.1.weight shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.1.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.1.conv.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.norm.weight shape torch.Size([320])
encoder.stages.5.0.convs.1.norm.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.1.all_modules.0.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.1.weight shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.0.0.convs.0.conv.weight shape torch.Size([32, 1, 3, 3, 3])
decoder.encoder.stages.0.0.convs.0.conv.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.norm.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.norm.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.0.weight shape torch.Size([32, 1, 3, 3, 3])
decoder.encoder.stages.0.0.convs.0.all_modules.0.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.1.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.1.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.encoder.stages.0.0.convs.1.conv.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.norm.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.norm.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.encoder.stages.0.0.convs.1.all_modules.0.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.1.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.1.bias shape torch.Size([32])
decoder.encoder.stages.1.0.convs.0.conv.weight shape torch.Size([64, 32, 3, 3, 3])
decoder.encoder.stages.1.0.convs.0.conv.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.norm.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.norm.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.0.weight shape torch.Size([64, 32, 3, 3, 3])
decoder.encoder.stages.1.0.convs.0.all_modules.0.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.1.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.1.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.encoder.stages.1.0.convs.1.conv.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.norm.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.norm.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.encoder.stages.1.0.convs.1.all_modules.0.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.1.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.1.bias shape torch.Size([64])
decoder.encoder.stages.2.0.convs.0.conv.weight shape torch.Size([128, 64, 3, 3, 3])
decoder.encoder.stages.2.0.convs.0.conv.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.norm.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.norm.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.0.weight shape torch.Size([128, 64, 3, 3, 3])
decoder.encoder.stages.2.0.convs.0.all_modules.0.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.1.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.1.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.encoder.stages.2.0.convs.1.conv.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.norm.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.norm.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.encoder.stages.2.0.convs.1.all_modules.0.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.1.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.1.bias shape torch.Size([128])
decoder.encoder.stages.3.0.convs.0.conv.weight shape torch.Size([256, 128, 3, 3, 3])
decoder.encoder.stages.3.0.convs.0.conv.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.norm.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.norm.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.0.weight shape torch.Size([256, 128, 3, 3, 3])
decoder.encoder.stages.3.0.convs.0.all_modules.0.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.1.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.1.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.encoder.stages.3.0.convs.1.conv.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.norm.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.norm.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.encoder.stages.3.0.convs.1.all_modules.0.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.1.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.1.bias shape torch.Size([256])
decoder.encoder.stages.4.0.convs.0.conv.weight shape torch.Size([320, 256, 3, 3, 3])
decoder.encoder.stages.4.0.convs.0.conv.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.norm.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.norm.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.0.weight shape torch.Size([320, 256, 3, 3, 3])
decoder.encoder.stages.4.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.4.0.convs.1.conv.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.norm.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.norm.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.4.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.0.conv.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.norm.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.norm.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.1.conv.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.norm.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.norm.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.stages.0.convs.0.conv.weight shape torch.Size([320, 640, 3, 3, 3])
decoder.stages.0.convs.0.conv.bias shape torch.Size([320])
decoder.stages.0.convs.0.norm.weight shape torch.Size([320])
decoder.stages.0.convs.0.norm.bias shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.0.weight shape torch.Size([320, 640, 3, 3, 3])
decoder.stages.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.stages.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.stages.0.convs.1.conv.bias shape torch.Size([320])
decoder.stages.0.convs.1.norm.weight shape torch.Size([320])
decoder.stages.0.convs.1.norm.bias shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.stages.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.stages.1.convs.0.conv.weight shape torch.Size([256, 512, 3, 3, 3])
decoder.stages.1.convs.0.conv.bias shape torch.Size([256])
decoder.stages.1.convs.0.norm.weight shape torch.Size([256])
decoder.stages.1.convs.0.norm.bias shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.0.weight shape torch.Size([256, 512, 3, 3, 3])
decoder.stages.1.convs.0.all_modules.0.bias shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.1.weight shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.1.bias shape torch.Size([256])
decoder.stages.1.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.stages.1.convs.1.conv.bias shape torch.Size([256])
decoder.stages.1.convs.1.norm.weight shape torch.Size([256])
decoder.stages.1.convs.1.norm.bias shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.stages.1.convs.1.all_modules.0.bias shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.1.weight shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.1.bias shape torch.Size([256])
decoder.stages.2.convs.0.conv.weight shape torch.Size([128, 256, 3, 3, 3])
decoder.stages.2.convs.0.conv.bias shape torch.Size([128])
decoder.stages.2.convs.0.norm.weight shape torch.Size([128])
decoder.stages.2.convs.0.norm.bias shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.0.weight shape torch.Size([128, 256, 3, 3, 3])
decoder.stages.2.convs.0.all_modules.0.bias shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.1.weight shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.1.bias shape torch.Size([128])
decoder.stages.2.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.stages.2.convs.1.conv.bias shape torch.Size([128])
decoder.stages.2.convs.1.norm.weight shape torch.Size([128])
decoder.stages.2.convs.1.norm.bias shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.stages.2.convs.1.all_modules.0.bias shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.1.weight shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.1.bias shape torch.Size([128])
decoder.stages.3.convs.0.conv.weight shape torch.Size([64, 128, 3, 3, 3])
decoder.stages.3.convs.0.conv.bias shape torch.Size([64])
decoder.stages.3.convs.0.norm.weight shape torch.Size([64])
decoder.stages.3.convs.0.norm.bias shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.0.weight shape torch.Size([64, 128, 3, 3, 3])
decoder.stages.3.convs.0.all_modules.0.bias shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.1.weight shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.1.bias shape torch.Size([64])
decoder.stages.3.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.stages.3.convs.1.conv.bias shape torch.Size([64])
decoder.stages.3.convs.1.norm.weight shape torch.Size([64])
decoder.stages.3.convs.1.norm.bias shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.stages.3.convs.1.all_modules.0.bias shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.1.weight shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.1.bias shape torch.Size([64])
decoder.stages.4.convs.0.conv.weight shape torch.Size([32, 64, 3, 3, 3])
decoder.stages.4.convs.0.conv.bias shape torch.Size([32])
decoder.stages.4.convs.0.norm.weight shape torch.Size([32])
decoder.stages.4.convs.0.norm.bias shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.0.weight shape torch.Size([32, 64, 3, 3, 3])
decoder.stages.4.convs.0.all_modules.0.bias shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.1.weight shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.1.bias shape torch.Size([32])
decoder.stages.4.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.stages.4.convs.1.conv.bias shape torch.Size([32])
decoder.stages.4.convs.1.norm.weight shape torch.Size([32])
decoder.stages.4.convs.1.norm.bias shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.stages.4.convs.1.all_modules.0.bias shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.1.weight shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.1.bias shape torch.Size([32])
decoder.transpconvs.0.weight shape torch.Size([320, 320, 1, 2, 2])
decoder.transpconvs.0.bias shape torch.Size([320])
decoder.transpconvs.1.weight shape torch.Size([320, 256, 2, 2, 2])
decoder.transpconvs.1.bias shape torch.Size([256])
decoder.transpconvs.2.weight shape torch.Size([256, 128, 2, 2, 2])
decoder.transpconvs.2.bias shape torch.Size([128])
decoder.transpconvs.3.weight shape torch.Size([128, 64, 2, 2, 2])
decoder.transpconvs.3.bias shape torch.Size([64])
decoder.transpconvs.4.weight shape torch.Size([64, 32, 2, 2, 2])
decoder.transpconvs.4.bias shape torch.Size([32])
################### Done ###################
2024-12-20 17:29:02.618843: do_dummy_2d_data_aug: True
2024-12-20 17:29:02.620732: Using splits from existing split file: /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset308_Sohee_Calcium_OCT_CrossValidation/splits_final_40.json
2024-12-20 17:29:02.622984: The split file contains 5 splits.
2024-12-20 17:29:02.624107: Desired fold for training: 2
2024-12-20 17:29:02.625163: This split has 3 training and 5 validation cases.
using pin_memory on device 0
using pin_memory on device 0
using pin_memory on device 0

This is the configuration used by this training:
Configuration name: 3d_32x160x128_b10
 {'data_identifier': 'nnUNetPlans_3d_fullres', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 10, 'patch_size': [32, 160, 128], 'median_image_size_in_voxels': [375.0, 498.0, 498.0], 'spacing': [1.0, 1.0, 1.0], 'normalization_schemes': ['ZScoreNormalization'], 'use_mask_for_norm': [False], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'architecture': {'network_class_name': 'dynamic_network_architectures.architectures.unet.PlainConvUNet', 'arch_kwargs': {'n_stages': 6, 'features_per_stage': [32, 64, 128, 256, 320, 320], 'conv_op': 'torch.nn.modules.conv.Conv3d', 'kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]], 'strides': [[1, 1, 1], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [1, 2, 2]], 'n_conv_per_stage': [2, 2, 2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2, 2, 2], 'conv_bias': True, 'norm_op': 'torch.nn.modules.instancenorm.InstanceNorm3d', 'norm_op_kwargs': {'eps': 1e-05, 'affine': True}, 'dropout_op': None, 'dropout_op_kwargs': None, 'nonlin': 'torch.nn.LeakyReLU', 'nonlin_kwargs': {'inplace': True}, 'deep_supervision': True}, '_kw_requires_import': ['conv_op', 'norm_op', 'dropout_op', 'nonlin']}, 'batch_dice': True, 'inherits_from': '3d_fullres'} 

These are the global plan.json settings:
 {'dataset_name': 'Dataset308_Sohee_Calcium_OCT_CrossValidation', 'plans_name': 'nnUNetPlans', 'original_median_spacing_after_transp': [1.0, 1.0, 1.0], 'original_median_shape_after_transp': [375, 498, 498], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 0.4977976381778717, 'mean': 0.13507525622844696, 'median': 0.09599608182907104, 'min': 0.0, 'percentile_00_5': 0.014754901640117168, 'percentile_99_5': 0.4977976381778717, 'std': 0.12025152146816254}}} 

2024-12-20 17:29:27.490184: unpacking dataset...
using pin_memory on device 0

This is the configuration used by this training:
Configuration name: 3d_32x160x128_b10
 {'data_identifier': 'nnUNetPlans_3d_fullres', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 10, 'patch_size': [32, 160, 128], 'median_image_size_in_voxels': [375.0, 498.0, 498.0], 'spacing': [1.0, 1.0, 1.0], 'normalization_schemes': ['ZScoreNormalization'], 'use_mask_for_norm': [False], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'architecture': {'network_class_name': 'dynamic_network_architectures.architectures.unet.PlainConvUNet', 'arch_kwargs': {'n_stages': 6, 'features_per_stage': [32, 64, 128, 256, 320, 320], 'conv_op': 'torch.nn.modules.conv.Conv3d', 'kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]], 'strides': [[1, 1, 1], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [1, 2, 2]], 'n_conv_per_stage': [2, 2, 2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2, 2, 2], 'conv_bias': True, 'norm_op': 'torch.nn.modules.instancenorm.InstanceNorm3d', 'norm_op_kwargs': {'eps': 1e-05, 'affine': True}, 'dropout_op': None, 'dropout_op_kwargs': None, 'nonlin': 'torch.nn.LeakyReLU', 'nonlin_kwargs': {'inplace': True}, 'deep_supervision': True}, '_kw_requires_import': ['conv_op', 'norm_op', 'dropout_op', 'nonlin']}, 'batch_dice': True, 'inherits_from': '3d_fullres'} 

These are the global plan.json settings:
 {'dataset_name': 'Dataset308_Sohee_Calcium_OCT_CrossValidation', 'plans_name': 'nnUNetPlans', 'original_median_spacing_after_transp': [1.0, 1.0, 1.0], 'original_median_shape_after_transp': [375, 498, 498], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 0.4977976381778717, 'mean': 0.13507525622844696, 'median': 0.09599608182907104, 'min': 0.0, 'percentile_00_5': 0.014754901640117168, 'percentile_99_5': 0.4977976381778717, 'std': 0.12025152146816254}}} 

2024-12-20 17:29:27.556400: unpacking dataset...
2024-12-20 17:29:32.414500: unpacking done...
2024-12-20 17:29:32.651830: Unable to plot network architecture: nnUNet_compile is enabled!
2024-12-20 17:29:32.706161: 
2024-12-20 17:29:32.707274: Epoch 0
2024-12-20 17:29:32.708467: Current learning rate: 0.01
2024-12-20 17:34:25.411147: Validation loss improved from 1000.00000 to -0.18813! Patience: 0/50
2024-12-20 17:34:25.412097: train_loss -0.0609
2024-12-20 17:34:25.413020: val_loss -0.1881
2024-12-20 17:34:25.413918: Pseudo dice [0.5333]
2024-12-20 17:34:25.414938: Epoch time: 292.71 s
2024-12-20 17:34:25.415859: Yayy! New best EMA pseudo Dice: 0.5333
2024-12-20 17:34:27.099734: 
2024-12-20 17:34:27.100672: Epoch 1
2024-12-20 17:34:27.101414: Current learning rate: 0.00994
2024-12-20 17:38:11.524298: Validation loss improved from -0.18813 to -0.21281! Patience: 0/50
2024-12-20 17:38:11.525090: train_loss -0.2387
2024-12-20 17:38:11.525787: val_loss -0.2128
2024-12-20 17:38:11.526463: Pseudo dice [0.5343]
2024-12-20 17:38:11.527164: Epoch time: 224.43 s
2024-12-20 17:38:11.527837: Yayy! New best EMA pseudo Dice: 0.5334
2024-12-20 17:38:13.342430: 
2024-12-20 17:38:13.343541: Epoch 2
2024-12-20 17:38:13.344183: Current learning rate: 0.00988
2024-12-20 17:41:59.319132: Validation loss improved from -0.21281 to -0.23726! Patience: 0/50
2024-12-20 17:41:59.324365: train_loss -0.2897
2024-12-20 17:41:59.325696: val_loss -0.2373
2024-12-20 17:41:59.326689: Pseudo dice [0.5772]
2024-12-20 17:41:59.327638: Epoch time: 225.98 s
2024-12-20 17:41:59.328513: Yayy! New best EMA pseudo Dice: 0.5378
2024-12-20 17:42:01.242455: 
2024-12-20 17:42:01.243900: Epoch 3
2024-12-20 17:42:01.244808: Current learning rate: 0.00982
2024-12-20 17:46:01.855761: Validation loss improved from -0.23726 to -0.27844! Patience: 0/50
2024-12-20 17:46:01.856625: train_loss -0.3158
2024-12-20 17:46:01.857413: val_loss -0.2784
2024-12-20 17:46:01.858127: Pseudo dice [0.5923]
2024-12-20 17:46:01.858833: Epoch time: 240.62 s
2024-12-20 17:46:01.859538: Yayy! New best EMA pseudo Dice: 0.5432
2024-12-20 17:46:04.357628: 
2024-12-20 17:46:04.358655: Epoch 4
2024-12-20 17:46:04.359351: Current learning rate: 0.00976
2024-12-20 17:49:48.973099: Validation loss improved from -0.27844 to -0.31291! Patience: 0/50
2024-12-20 17:49:48.973867: train_loss -0.3555
2024-12-20 17:49:48.974617: val_loss -0.3129
2024-12-20 17:49:48.975276: Pseudo dice [0.6128]
2024-12-20 17:49:48.975912: Epoch time: 224.62 s
2024-12-20 17:49:49.349055: Yayy! New best EMA pseudo Dice: 0.5502
2024-12-20 17:49:51.218611: 
2024-12-20 17:49:51.219660: Epoch 5
2024-12-20 17:49:51.220571: Current learning rate: 0.0097
2024-12-20 17:52:28.528397: Validation loss improved from -0.31291 to -0.34558! Patience: 0/50
2024-12-20 17:52:28.529155: train_loss -0.3774
2024-12-20 17:52:28.529930: val_loss -0.3456
2024-12-20 17:52:28.530607: Pseudo dice [0.6457]
2024-12-20 17:52:28.531276: Epoch time: 157.31 s
2024-12-20 17:52:28.532050: Yayy! New best EMA pseudo Dice: 0.5597
2024-12-20 17:52:30.310137: 
2024-12-20 17:52:30.310972: Epoch 6
2024-12-20 17:52:30.311639: Current learning rate: 0.00964
2024-12-20 17:56:29.031969: Validation loss improved from -0.34558 to -0.36305! Patience: 0/50
2024-12-20 17:56:29.032844: train_loss -0.4176
2024-12-20 17:56:29.033690: val_loss -0.3631
2024-12-20 17:56:29.034526: Pseudo dice [0.6294]
2024-12-20 17:56:29.035265: Epoch time: 238.72 s
2024-12-20 17:56:29.036016: Yayy! New best EMA pseudo Dice: 0.5667
2024-12-20 17:56:30.883348: 
2024-12-20 17:56:30.884554: Epoch 7
2024-12-20 17:56:30.885548: Current learning rate: 0.00958
2024-12-20 18:00:53.938032: Validation loss did not improve from -0.36305. Patience: 1/50
2024-12-20 18:00:53.938849: train_loss -0.4565
2024-12-20 18:00:53.939619: val_loss -0.3283
2024-12-20 18:00:53.940240: Pseudo dice [0.6141]
2024-12-20 18:00:53.940958: Epoch time: 263.06 s
2024-12-20 18:00:53.941589: Yayy! New best EMA pseudo Dice: 0.5714
2024-12-20 18:00:56.144186: 
2024-12-20 18:00:56.145197: Epoch 8
2024-12-20 18:00:56.145916: Current learning rate: 0.00952
2024-12-20 18:04:59.132999: Validation loss improved from -0.36305 to -0.36476! Patience: 1/50
2024-12-20 18:04:59.133900: train_loss -0.4648
2024-12-20 18:04:59.134651: val_loss -0.3648
2024-12-20 18:04:59.135429: Pseudo dice [0.649]
2024-12-20 18:04:59.136303: Epoch time: 242.99 s
2024-12-20 18:04:59.137067: Yayy! New best EMA pseudo Dice: 0.5792
2024-12-20 18:05:01.034810: 
2024-12-20 18:05:01.036071: Epoch 9
2024-12-20 18:05:01.036798: Current learning rate: 0.00946
2024-12-20 18:08:58.690204: Validation loss improved from -0.36476 to -0.36592! Patience: 0/50
2024-12-20 18:08:58.691241: train_loss -0.4808
2024-12-20 18:08:58.692347: val_loss -0.3659
2024-12-20 18:08:58.693207: Pseudo dice [0.651]
2024-12-20 18:08:58.694252: Epoch time: 237.66 s
2024-12-20 18:08:59.111483: Yayy! New best EMA pseudo Dice: 0.5864
2024-12-20 18:09:00.902219: 
2024-12-20 18:09:00.903431: Epoch 10
2024-12-20 18:09:00.904292: Current learning rate: 0.0094
2024-12-20 18:12:49.473882: Validation loss improved from -0.36592 to -0.37145! Patience: 0/50
2024-12-20 18:12:49.474924: train_loss -0.487
2024-12-20 18:12:49.475853: val_loss -0.3714
2024-12-20 18:12:49.476772: Pseudo dice [0.6604]
2024-12-20 18:12:49.477659: Epoch time: 228.57 s
2024-12-20 18:12:49.478477: Yayy! New best EMA pseudo Dice: 0.5938
2024-12-20 18:12:51.303255: 
2024-12-20 18:12:51.304532: Epoch 11
2024-12-20 18:12:51.305381: Current learning rate: 0.00934
2024-12-20 18:16:43.457574: Validation loss did not improve from -0.37145. Patience: 1/50
2024-12-20 18:16:43.458580: train_loss -0.5083
2024-12-20 18:16:43.459435: val_loss -0.3711
2024-12-20 18:16:43.460184: Pseudo dice [0.6566]
2024-12-20 18:16:43.460878: Epoch time: 232.16 s
2024-12-20 18:16:43.461561: Yayy! New best EMA pseudo Dice: 0.6001
2024-12-20 18:16:45.317716: 
2024-12-20 18:16:45.318841: Epoch 12
2024-12-20 18:16:45.319623: Current learning rate: 0.00928
2024-12-20 18:20:18.565609: Validation loss did not improve from -0.37145. Patience: 2/50
2024-12-20 18:20:18.566450: train_loss -0.5167
2024-12-20 18:20:18.567216: val_loss -0.3039
2024-12-20 18:20:18.567887: Pseudo dice [0.6136]
2024-12-20 18:20:18.568587: Epoch time: 213.25 s
2024-12-20 18:20:18.569445: Yayy! New best EMA pseudo Dice: 0.6014
2024-12-20 18:20:20.446849: 
2024-12-20 18:20:20.448092: Epoch 13
2024-12-20 18:20:20.448852: Current learning rate: 0.00922
2024-12-20 18:24:16.366470: Validation loss did not improve from -0.37145. Patience: 3/50
2024-12-20 18:24:16.367191: train_loss -0.5253
2024-12-20 18:24:16.367879: val_loss -0.2994
2024-12-20 18:24:16.368561: Pseudo dice [0.6037]
2024-12-20 18:24:16.369253: Epoch time: 235.92 s
2024-12-20 18:24:16.369921: Yayy! New best EMA pseudo Dice: 0.6016
2024-12-20 18:24:18.236730: 
2024-12-20 18:24:18.237901: Epoch 14
2024-12-20 18:24:18.238639: Current learning rate: 0.00916
2024-12-20 18:27:35.386716: Validation loss did not improve from -0.37145. Patience: 4/50
2024-12-20 18:27:35.387582: train_loss -0.5318
2024-12-20 18:27:35.388421: val_loss -0.3419
2024-12-20 18:27:35.389189: Pseudo dice [0.6388]
2024-12-20 18:27:35.389965: Epoch time: 197.15 s
2024-12-20 18:27:35.938664: Yayy! New best EMA pseudo Dice: 0.6054
2024-12-20 18:27:37.722001: 
2024-12-20 18:27:37.722931: Epoch 15
2024-12-20 18:27:37.723734: Current learning rate: 0.0091
2024-12-20 18:31:24.111313: Validation loss did not improve from -0.37145. Patience: 5/50
2024-12-20 18:31:24.112199: train_loss -0.5432
2024-12-20 18:31:24.113089: val_loss -0.3389
2024-12-20 18:31:24.113763: Pseudo dice [0.6337]
2024-12-20 18:31:24.114509: Epoch time: 226.39 s
2024-12-20 18:31:24.115142: Yayy! New best EMA pseudo Dice: 0.6082
2024-12-20 18:31:25.962747: 
2024-12-20 18:31:25.963859: Epoch 16
2024-12-20 18:31:25.964594: Current learning rate: 0.00903
2024-12-20 18:34:35.478020: Validation loss did not improve from -0.37145. Patience: 6/50
2024-12-20 18:34:35.479817: train_loss -0.5505
2024-12-20 18:34:35.480858: val_loss -0.3496
2024-12-20 18:34:35.481868: Pseudo dice [0.6342]
2024-12-20 18:34:35.482770: Epoch time: 189.52 s
2024-12-20 18:34:35.483638: Yayy! New best EMA pseudo Dice: 0.6108
2024-12-20 18:34:37.375976: 
2024-12-20 18:34:37.377280: Epoch 17
2024-12-20 18:34:37.378122: Current learning rate: 0.00897
2024-12-20 18:38:20.094326: Validation loss did not improve from -0.37145. Patience: 7/50
2024-12-20 18:38:20.095327: train_loss -0.5577
2024-12-20 18:38:20.096239: val_loss -0.2656
2024-12-20 18:38:20.096904: Pseudo dice [0.5982]
2024-12-20 18:38:20.097639: Epoch time: 222.72 s
2024-12-20 18:38:21.572444: 
2024-12-20 18:38:21.573726: Epoch 18
2024-12-20 18:38:21.574517: Current learning rate: 0.00891
2024-12-20 18:41:52.256680: Validation loss improved from -0.37145 to -0.37933! Patience: 7/50
2024-12-20 18:41:52.257712: train_loss -0.5694
2024-12-20 18:41:52.258640: val_loss -0.3793
2024-12-20 18:41:52.259541: Pseudo dice [0.6488]
2024-12-20 18:41:52.260492: Epoch time: 210.69 s
2024-12-20 18:41:52.261472: Yayy! New best EMA pseudo Dice: 0.6135
2024-12-20 18:41:54.680647: 
2024-12-20 18:41:54.681931: Epoch 19
2024-12-20 18:41:54.682821: Current learning rate: 0.00885
2024-12-20 18:45:16.013727: Validation loss improved from -0.37933 to -0.44884! Patience: 0/50
2024-12-20 18:45:16.014753: train_loss -0.5773
2024-12-20 18:45:16.015536: val_loss -0.4488
2024-12-20 18:45:16.016304: Pseudo dice [0.6895]
2024-12-20 18:45:16.016968: Epoch time: 201.34 s
2024-12-20 18:45:16.391826: Yayy! New best EMA pseudo Dice: 0.6211
2024-12-20 18:45:18.238478: 
2024-12-20 18:45:18.239653: Epoch 20
2024-12-20 18:45:18.240374: Current learning rate: 0.00879
2024-12-20 18:49:03.965882: Validation loss did not improve from -0.44884. Patience: 1/50
2024-12-20 18:49:03.966927: train_loss -0.5886
2024-12-20 18:49:03.967674: val_loss -0.4189
2024-12-20 18:49:03.968307: Pseudo dice [0.6759]
2024-12-20 18:49:03.968949: Epoch time: 225.73 s
2024-12-20 18:49:03.969694: Yayy! New best EMA pseudo Dice: 0.6266
2024-12-20 18:49:05.947982: 
2024-12-20 18:49:05.949066: Epoch 21
2024-12-20 18:49:05.949949: Current learning rate: 0.00873
2024-12-20 18:53:05.413533: Validation loss did not improve from -0.44884. Patience: 2/50
2024-12-20 18:53:05.414222: train_loss -0.5944
2024-12-20 18:53:05.415012: val_loss -0.4441
2024-12-20 18:53:05.415693: Pseudo dice [0.6919]
2024-12-20 18:53:05.416356: Epoch time: 239.47 s
2024-12-20 18:53:05.416992: Yayy! New best EMA pseudo Dice: 0.6331
2024-12-20 18:53:07.182526: 
2024-12-20 18:53:07.183456: Epoch 22
2024-12-20 18:53:07.184244: Current learning rate: 0.00867
2024-12-20 18:56:10.851449: Validation loss improved from -0.44884 to -0.45394! Patience: 2/50
2024-12-20 18:56:10.852538: train_loss -0.5933
2024-12-20 18:56:10.853417: val_loss -0.4539
2024-12-20 18:56:10.854299: Pseudo dice [0.6993]
2024-12-20 18:56:10.855169: Epoch time: 183.67 s
2024-12-20 18:56:10.856048: Yayy! New best EMA pseudo Dice: 0.6397
2024-12-20 18:56:12.707109: 
2024-12-20 18:56:12.708547: Epoch 23
2024-12-20 18:56:12.709430: Current learning rate: 0.00861
2024-12-20 19:00:00.202786: Validation loss did not improve from -0.45394. Patience: 1/50
2024-12-20 19:00:00.203559: train_loss -0.6055
2024-12-20 19:00:00.204477: val_loss -0.42
2024-12-20 19:00:00.205292: Pseudo dice [0.6867]
2024-12-20 19:00:00.206149: Epoch time: 227.5 s
2024-12-20 19:00:00.206818: Yayy! New best EMA pseudo Dice: 0.6444
2024-12-20 19:00:01.955351: 
2024-12-20 19:00:01.956600: Epoch 24
2024-12-20 19:00:01.957339: Current learning rate: 0.00855
2024-12-20 19:03:08.297168: Validation loss did not improve from -0.45394. Patience: 2/50
2024-12-20 19:03:08.298117: train_loss -0.6086
2024-12-20 19:03:08.299000: val_loss -0.3901
2024-12-20 19:03:08.299726: Pseudo dice [0.6613]
2024-12-20 19:03:08.300597: Epoch time: 186.34 s
2024-12-20 19:03:08.742063: Yayy! New best EMA pseudo Dice: 0.6461
2024-12-20 19:03:10.594735: 
2024-12-20 19:03:10.595972: Epoch 25
2024-12-20 19:03:10.596822: Current learning rate: 0.00849
2024-12-20 19:06:51.637898: Validation loss did not improve from -0.45394. Patience: 3/50
2024-12-20 19:06:51.638845: train_loss -0.6196
2024-12-20 19:06:51.639545: val_loss -0.4306
2024-12-20 19:06:51.640189: Pseudo dice [0.6892]
2024-12-20 19:06:51.640838: Epoch time: 221.05 s
2024-12-20 19:06:51.641480: Yayy! New best EMA pseudo Dice: 0.6504
2024-12-20 19:06:53.582436: 
2024-12-20 19:06:53.583730: Epoch 26
2024-12-20 19:06:53.584556: Current learning rate: 0.00843
2024-12-20 19:10:18.505959: Validation loss did not improve from -0.45394. Patience: 4/50
2024-12-20 19:10:18.507429: train_loss -0.6172
2024-12-20 19:10:18.508373: val_loss -0.4302
2024-12-20 19:10:18.509161: Pseudo dice [0.6739]
2024-12-20 19:10:18.509986: Epoch time: 204.93 s
2024-12-20 19:10:18.510720: Yayy! New best EMA pseudo Dice: 0.6528
2024-12-20 19:10:20.338750: 
2024-12-20 19:10:20.340029: Epoch 27
2024-12-20 19:10:20.340832: Current learning rate: 0.00836
2024-12-20 19:13:44.521993: Validation loss did not improve from -0.45394. Patience: 5/50
2024-12-20 19:13:44.522938: train_loss -0.6298
2024-12-20 19:13:44.523588: val_loss -0.4049
2024-12-20 19:13:44.524205: Pseudo dice [0.6744]
2024-12-20 19:13:44.524796: Epoch time: 204.19 s
2024-12-20 19:13:44.525391: Yayy! New best EMA pseudo Dice: 0.6549
2024-12-20 19:13:46.381950: 
2024-12-20 19:13:46.383269: Epoch 28
2024-12-20 19:13:46.383992: Current learning rate: 0.0083
2024-12-20 19:17:37.506499: Validation loss did not improve from -0.45394. Patience: 6/50
2024-12-20 19:17:37.509622: train_loss -0.6255
2024-12-20 19:17:37.510450: val_loss -0.3803
2024-12-20 19:17:37.511193: Pseudo dice [0.6852]
2024-12-20 19:17:37.511849: Epoch time: 231.13 s
2024-12-20 19:17:37.512503: Yayy! New best EMA pseudo Dice: 0.658
2024-12-20 19:17:39.717149: 
2024-12-20 19:17:39.718508: Epoch 29
2024-12-20 19:17:39.719247: Current learning rate: 0.00824
2024-12-20 19:21:14.569984: Validation loss did not improve from -0.45394. Patience: 7/50
2024-12-20 19:21:14.570875: train_loss -0.6308
2024-12-20 19:21:14.571740: val_loss -0.3265
2024-12-20 19:21:14.572474: Pseudo dice [0.6358]
2024-12-20 19:21:14.573237: Epoch time: 214.86 s
2024-12-20 19:21:16.792940: 
2024-12-20 19:21:16.794679: Epoch 30
2024-12-20 19:21:16.795548: Current learning rate: 0.00818
2024-12-20 19:24:49.346015: Validation loss did not improve from -0.45394. Patience: 8/50
2024-12-20 19:24:49.346969: train_loss -0.6294
2024-12-20 19:24:49.347750: val_loss -0.4154
2024-12-20 19:24:49.348405: Pseudo dice [0.6854]
2024-12-20 19:24:49.349052: Epoch time: 212.56 s
2024-12-20 19:24:49.349673: Yayy! New best EMA pseudo Dice: 0.6587
2024-12-20 19:24:51.201054: 
2024-12-20 19:24:51.201961: Epoch 31
2024-12-20 19:24:51.202677: Current learning rate: 0.00812
2024-12-20 19:28:42.122824: Validation loss did not improve from -0.45394. Patience: 9/50
2024-12-20 19:28:42.123749: train_loss -0.6444
2024-12-20 19:28:42.124509: val_loss -0.42
2024-12-20 19:28:42.125308: Pseudo dice [0.6907]
2024-12-20 19:28:42.126117: Epoch time: 230.92 s
2024-12-20 19:28:42.126869: Yayy! New best EMA pseudo Dice: 0.6619
2024-12-20 19:28:43.940532: 
2024-12-20 19:28:43.941467: Epoch 32
2024-12-20 19:28:43.942268: Current learning rate: 0.00806
2024-12-20 19:32:15.997638: Validation loss did not improve from -0.45394. Patience: 10/50
2024-12-20 19:32:15.998351: train_loss -0.6532
2024-12-20 19:32:15.999227: val_loss -0.422
2024-12-20 19:32:16.000013: Pseudo dice [0.6944]
2024-12-20 19:32:16.001008: Epoch time: 212.06 s
2024-12-20 19:32:16.001802: Yayy! New best EMA pseudo Dice: 0.6652
2024-12-20 19:32:17.813411: 
2024-12-20 19:32:17.814227: Epoch 33
2024-12-20 19:32:17.814902: Current learning rate: 0.008
2024-12-20 19:35:29.550874: Validation loss did not improve from -0.45394. Patience: 11/50
2024-12-20 19:35:29.551890: train_loss -0.6489
2024-12-20 19:35:29.552708: val_loss -0.378
2024-12-20 19:35:29.553349: Pseudo dice [0.6552]
2024-12-20 19:35:29.554043: Epoch time: 191.74 s
2024-12-20 19:35:30.978952: 
2024-12-20 19:35:30.980133: Epoch 34
2024-12-20 19:35:30.980895: Current learning rate: 0.00793
2024-12-20 19:39:05.746649: Validation loss did not improve from -0.45394. Patience: 12/50
2024-12-20 19:39:05.747756: train_loss -0.6522
2024-12-20 19:39:05.748739: val_loss -0.4495
2024-12-20 19:39:05.749362: Pseudo dice [0.7071]
2024-12-20 19:39:05.750178: Epoch time: 214.77 s
2024-12-20 19:39:06.198781: Yayy! New best EMA pseudo Dice: 0.6685
2024-12-20 19:39:08.107237: 
2024-12-20 19:39:08.108559: Epoch 35
2024-12-20 19:39:08.109494: Current learning rate: 0.00787
2024-12-20 19:42:23.369414: Validation loss did not improve from -0.45394. Patience: 13/50
2024-12-20 19:42:23.394793: train_loss -0.6561
2024-12-20 19:42:23.395531: val_loss -0.3675
2024-12-20 19:42:23.396238: Pseudo dice [0.665]
2024-12-20 19:42:23.396945: Epoch time: 195.29 s
2024-12-20 19:42:24.939309: 
2024-12-20 19:42:24.940668: Epoch 36
2024-12-20 19:42:24.941492: Current learning rate: 0.00781
2024-12-20 19:46:24.598700: Validation loss did not improve from -0.45394. Patience: 14/50
2024-12-20 19:46:24.600182: train_loss -0.6559
2024-12-20 19:46:24.601878: val_loss -0.4168
2024-12-20 19:46:24.602628: Pseudo dice [0.6754]
2024-12-20 19:46:24.603573: Epoch time: 239.66 s
2024-12-20 19:46:24.604265: Yayy! New best EMA pseudo Dice: 0.6688
2024-12-20 19:46:26.642782: 
2024-12-20 19:46:26.643940: Epoch 37
2024-12-20 19:46:26.644965: Current learning rate: 0.00775
2024-12-20 19:50:17.606324: Validation loss did not improve from -0.45394. Patience: 15/50
2024-12-20 19:50:17.607227: train_loss -0.6654
2024-12-20 19:50:17.608054: val_loss -0.444
2024-12-20 19:50:17.608756: Pseudo dice [0.699]
2024-12-20 19:50:17.609483: Epoch time: 230.97 s
2024-12-20 19:50:17.610190: Yayy! New best EMA pseudo Dice: 0.6718
2024-12-20 19:50:19.499130: 
2024-12-20 19:50:19.500389: Epoch 38
2024-12-20 19:50:19.501060: Current learning rate: 0.00769
2024-12-20 19:53:42.434109: Validation loss did not improve from -0.45394. Patience: 16/50
2024-12-20 19:53:42.435138: train_loss -0.6645
2024-12-20 19:53:42.435936: val_loss -0.3901
2024-12-20 19:53:42.436651: Pseudo dice [0.6678]
2024-12-20 19:53:42.437340: Epoch time: 202.94 s
2024-12-20 19:53:44.430087: 
2024-12-20 19:53:44.431339: Epoch 39
2024-12-20 19:53:44.432107: Current learning rate: 0.00763
2024-12-20 19:57:21.460843: Validation loss did not improve from -0.45394. Patience: 17/50
2024-12-20 19:57:21.461717: train_loss -0.6652
2024-12-20 19:57:21.462594: val_loss -0.4184
2024-12-20 19:57:21.463551: Pseudo dice [0.6793]
2024-12-20 19:57:21.464509: Epoch time: 217.03 s
2024-12-20 19:57:21.822692: Yayy! New best EMA pseudo Dice: 0.6722
2024-12-20 19:57:23.748114: 
2024-12-20 19:57:23.749448: Epoch 40
2024-12-20 19:57:23.750399: Current learning rate: 0.00756
2024-12-20 20:01:26.059570: Validation loss did not improve from -0.45394. Patience: 18/50
2024-12-20 20:01:26.060398: train_loss -0.672
2024-12-20 20:01:26.061242: val_loss -0.4334
2024-12-20 20:01:26.062126: Pseudo dice [0.6889]
2024-12-20 20:01:26.063035: Epoch time: 242.31 s
2024-12-20 20:01:26.063902: Yayy! New best EMA pseudo Dice: 0.6739
2024-12-20 20:01:28.057480: 
2024-12-20 20:01:28.058787: Epoch 41
2024-12-20 20:01:28.059647: Current learning rate: 0.0075
2024-12-20 20:05:03.617394: Validation loss did not improve from -0.45394. Patience: 19/50
2024-12-20 20:05:03.618504: train_loss -0.676
2024-12-20 20:05:03.619363: val_loss -0.3799
2024-12-20 20:05:03.620030: Pseudo dice [0.6573]
2024-12-20 20:05:03.620664: Epoch time: 215.56 s
2024-12-20 20:05:05.070098: 
2024-12-20 20:05:05.071146: Epoch 42
2024-12-20 20:05:05.071875: Current learning rate: 0.00744
2024-12-20 20:08:00.502663: Validation loss did not improve from -0.45394. Patience: 20/50
2024-12-20 20:08:00.503740: train_loss -0.6844
2024-12-20 20:08:00.504484: val_loss -0.4068
2024-12-20 20:08:00.505105: Pseudo dice [0.6721]
2024-12-20 20:08:00.505763: Epoch time: 175.44 s
2024-12-20 20:08:01.933977: 
2024-12-20 20:08:01.935044: Epoch 43
2024-12-20 20:08:01.935772: Current learning rate: 0.00738
2024-12-20 20:11:29.642953: Validation loss did not improve from -0.45394. Patience: 21/50
2024-12-20 20:11:29.643929: train_loss -0.6861
2024-12-20 20:11:29.644727: val_loss -0.4405
2024-12-20 20:11:29.645327: Pseudo dice [0.702]
2024-12-20 20:11:29.645934: Epoch time: 207.71 s
2024-12-20 20:11:29.646692: Yayy! New best EMA pseudo Dice: 0.6752
2024-12-20 20:11:31.553229: 
2024-12-20 20:11:31.554647: Epoch 44
2024-12-20 20:11:31.555465: Current learning rate: 0.00732
2024-12-20 20:14:59.074280: Validation loss improved from -0.45394 to -0.46670! Patience: 21/50
2024-12-20 20:14:59.075200: train_loss -0.6915
2024-12-20 20:14:59.076121: val_loss -0.4667
2024-12-20 20:14:59.076982: Pseudo dice [0.7089]
2024-12-20 20:14:59.077814: Epoch time: 207.52 s
2024-12-20 20:14:59.493918: Yayy! New best EMA pseudo Dice: 0.6786
2024-12-20 20:15:01.315889: 
2024-12-20 20:15:01.317185: Epoch 45
2024-12-20 20:15:01.317955: Current learning rate: 0.00725
2024-12-20 20:18:47.674325: Validation loss did not improve from -0.46670. Patience: 1/50
2024-12-20 20:18:47.675482: train_loss -0.6971
2024-12-20 20:18:47.676466: val_loss -0.3695
2024-12-20 20:18:47.677289: Pseudo dice [0.645]
2024-12-20 20:18:47.678161: Epoch time: 226.36 s
2024-12-20 20:18:49.114504: 
2024-12-20 20:18:49.115766: Epoch 46
2024-12-20 20:18:49.116701: Current learning rate: 0.00719
2024-12-20 20:22:38.554024: Validation loss did not improve from -0.46670. Patience: 2/50
2024-12-20 20:22:38.554937: train_loss -0.6935
2024-12-20 20:22:38.555748: val_loss -0.37
2024-12-20 20:22:38.556638: Pseudo dice [0.6584]
2024-12-20 20:22:38.557652: Epoch time: 229.44 s
2024-12-20 20:22:40.008156: 
2024-12-20 20:22:40.009434: Epoch 47
2024-12-20 20:22:40.010241: Current learning rate: 0.00713
2024-12-20 20:26:07.925543: Validation loss did not improve from -0.46670. Patience: 3/50
2024-12-20 20:26:07.926517: train_loss -0.6948
2024-12-20 20:26:07.927389: val_loss -0.3731
2024-12-20 20:26:07.928069: Pseudo dice [0.6715]
2024-12-20 20:26:07.928697: Epoch time: 207.92 s
2024-12-20 20:26:09.344096: 
2024-12-20 20:26:09.345487: Epoch 48
2024-12-20 20:26:09.346341: Current learning rate: 0.00707
2024-12-20 20:30:29.691350: Validation loss did not improve from -0.46670. Patience: 4/50
2024-12-20 20:30:29.692378: train_loss -0.6992
2024-12-20 20:30:29.693290: val_loss -0.4332
2024-12-20 20:30:29.693978: Pseudo dice [0.6948]
2024-12-20 20:30:29.694728: Epoch time: 260.35 s
2024-12-20 20:30:31.125893: 
2024-12-20 20:30:31.127354: Epoch 49
2024-12-20 20:30:31.128186: Current learning rate: 0.007
2024-12-20 20:34:10.092012: Validation loss did not improve from -0.46670. Patience: 5/50
2024-12-20 20:34:10.093117: train_loss -0.7044
2024-12-20 20:34:10.093945: val_loss -0.354
2024-12-20 20:34:10.094723: Pseudo dice [0.6632]
2024-12-20 20:34:10.095607: Epoch time: 218.97 s
2024-12-20 20:34:12.351190: 
2024-12-20 20:34:12.352651: Epoch 50
2024-12-20 20:34:12.353419: Current learning rate: 0.00694
2024-12-20 20:37:58.330168: Validation loss did not improve from -0.46670. Patience: 6/50
2024-12-20 20:37:58.331199: train_loss -0.7063
2024-12-20 20:37:58.331959: val_loss -0.4137
2024-12-20 20:37:58.332625: Pseudo dice [0.6767]
2024-12-20 20:37:58.333326: Epoch time: 225.98 s
2024-12-20 20:37:59.818746: 
2024-12-20 20:37:59.819932: Epoch 51
2024-12-20 20:37:59.820724: Current learning rate: 0.00688
2024-12-20 20:41:34.142078: Validation loss did not improve from -0.46670. Patience: 7/50
2024-12-20 20:41:34.143082: train_loss -0.706
2024-12-20 20:41:34.144197: val_loss -0.413
2024-12-20 20:41:34.144823: Pseudo dice [0.6827]
2024-12-20 20:41:34.145468: Epoch time: 214.33 s
2024-12-20 20:41:35.589755: 
2024-12-20 20:41:35.590931: Epoch 52
2024-12-20 20:41:35.591583: Current learning rate: 0.00682
2024-12-20 20:45:30.857647: Validation loss improved from -0.46670 to -0.46690! Patience: 7/50
2024-12-20 20:45:30.858802: train_loss -0.7141
2024-12-20 20:45:30.859602: val_loss -0.4669
2024-12-20 20:45:30.860366: Pseudo dice [0.7135]
2024-12-20 20:45:30.861061: Epoch time: 235.27 s
2024-12-20 20:45:30.861732: Yayy! New best EMA pseudo Dice: 0.6791
2024-12-20 20:45:32.892831: 
2024-12-20 20:45:32.893932: Epoch 53
2024-12-20 20:45:32.894677: Current learning rate: 0.00675
2024-12-20 20:49:21.492529: Validation loss did not improve from -0.46690. Patience: 1/50
2024-12-20 20:49:21.493498: train_loss -0.7169
2024-12-20 20:49:21.494939: val_loss -0.3897
2024-12-20 20:49:21.495889: Pseudo dice [0.6811]
2024-12-20 20:49:21.496696: Epoch time: 228.6 s
2024-12-20 20:49:21.497368: Yayy! New best EMA pseudo Dice: 0.6793
2024-12-20 20:49:23.330559: 
2024-12-20 20:49:23.331793: Epoch 54
2024-12-20 20:49:23.332575: Current learning rate: 0.00669
2024-12-20 20:52:21.675736: Validation loss did not improve from -0.46690. Patience: 2/50
2024-12-20 20:52:21.677201: train_loss -0.7153
2024-12-20 20:52:21.677872: val_loss -0.3812
2024-12-20 20:52:21.678634: Pseudo dice [0.6802]
2024-12-20 20:52:21.679273: Epoch time: 178.35 s
2024-12-20 20:52:22.094329: Yayy! New best EMA pseudo Dice: 0.6794
2024-12-20 20:52:23.952790: 
2024-12-20 20:52:23.953599: Epoch 55
2024-12-20 20:52:23.954297: Current learning rate: 0.00663
2024-12-20 20:55:44.541252: Validation loss did not improve from -0.46690. Patience: 3/50
2024-12-20 20:55:44.542055: train_loss -0.7212
2024-12-20 20:55:44.543092: val_loss -0.4355
2024-12-20 20:55:44.544133: Pseudo dice [0.7013]
2024-12-20 20:55:44.545119: Epoch time: 200.59 s
2024-12-20 20:55:44.546231: Yayy! New best EMA pseudo Dice: 0.6816
2024-12-20 20:55:46.424089: 
2024-12-20 20:55:46.425726: Epoch 56
2024-12-20 20:55:46.426713: Current learning rate: 0.00657
2024-12-20 20:59:11.971687: Validation loss did not improve from -0.46690. Patience: 4/50
2024-12-20 20:59:11.972806: train_loss -0.7205
2024-12-20 20:59:11.974120: val_loss -0.3697
2024-12-20 20:59:11.975143: Pseudo dice [0.6767]
2024-12-20 20:59:11.975816: Epoch time: 205.55 s
2024-12-20 20:59:13.376478: 
2024-12-20 20:59:13.377569: Epoch 57
2024-12-20 20:59:13.378382: Current learning rate: 0.0065
2024-12-20 21:03:25.621098: Validation loss did not improve from -0.46690. Patience: 5/50
2024-12-20 21:03:25.622003: train_loss -0.7201
2024-12-20 21:03:25.622866: val_loss -0.4662
2024-12-20 21:03:25.623643: Pseudo dice [0.7083]
2024-12-20 21:03:25.624328: Epoch time: 252.25 s
2024-12-20 21:03:25.624973: Yayy! New best EMA pseudo Dice: 0.6838
2024-12-20 21:03:27.505700: 
2024-12-20 21:03:27.506550: Epoch 58
2024-12-20 21:03:27.507256: Current learning rate: 0.00644
2024-12-20 21:07:38.021183: Validation loss improved from -0.46690 to -0.47118! Patience: 5/50
2024-12-20 21:07:38.022263: train_loss -0.721
2024-12-20 21:07:38.023179: val_loss -0.4712
2024-12-20 21:07:38.024042: Pseudo dice [0.7211]
2024-12-20 21:07:38.024790: Epoch time: 250.52 s
2024-12-20 21:07:38.025527: Yayy! New best EMA pseudo Dice: 0.6876
2024-12-20 21:07:39.918392: 
2024-12-20 21:07:39.919712: Epoch 59
2024-12-20 21:07:39.920593: Current learning rate: 0.00638
2024-12-20 21:11:09.561434: Validation loss did not improve from -0.47118. Patience: 1/50
2024-12-20 21:11:09.562329: train_loss -0.7257
2024-12-20 21:11:09.563138: val_loss -0.4217
2024-12-20 21:11:09.563817: Pseudo dice [0.6921]
2024-12-20 21:11:09.564441: Epoch time: 209.65 s
2024-12-20 21:11:09.963274: Yayy! New best EMA pseudo Dice: 0.688
2024-12-20 21:11:11.772619: 
2024-12-20 21:11:11.773593: Epoch 60
2024-12-20 21:11:11.774337: Current learning rate: 0.00631
2024-12-20 21:14:42.916947: Validation loss did not improve from -0.47118. Patience: 2/50
2024-12-20 21:14:42.917775: train_loss -0.7299
2024-12-20 21:14:42.918622: val_loss -0.4194
2024-12-20 21:14:42.919312: Pseudo dice [0.7021]
2024-12-20 21:14:42.920011: Epoch time: 211.15 s
2024-12-20 21:14:42.920739: Yayy! New best EMA pseudo Dice: 0.6894
2024-12-20 21:14:45.491436: 
2024-12-20 21:14:45.492729: Epoch 61
2024-12-20 21:14:45.493762: Current learning rate: 0.00625
2024-12-20 21:19:10.530520: Validation loss did not improve from -0.47118. Patience: 3/50
2024-12-20 21:19:10.531381: train_loss -0.7332
2024-12-20 21:19:10.532075: val_loss -0.4117
2024-12-20 21:19:10.532664: Pseudo dice [0.685]
2024-12-20 21:19:10.533282: Epoch time: 265.04 s
2024-12-20 21:19:11.978915: 
2024-12-20 21:19:11.980496: Epoch 62
2024-12-20 21:19:11.981136: Current learning rate: 0.00619
2024-12-20 21:23:26.433133: Validation loss did not improve from -0.47118. Patience: 4/50
2024-12-20 21:23:26.434363: train_loss -0.7343
2024-12-20 21:23:26.435258: val_loss -0.3834
2024-12-20 21:23:26.436082: Pseudo dice [0.6828]
2024-12-20 21:23:26.437013: Epoch time: 254.46 s
2024-12-20 21:23:27.886970: 
2024-12-20 21:23:27.888376: Epoch 63
2024-12-20 21:23:27.889300: Current learning rate: 0.00612
2024-12-20 21:26:03.320113: Validation loss improved from -0.47118 to -0.47145! Patience: 4/50
2024-12-20 21:26:03.321176: train_loss -0.7336
2024-12-20 21:26:03.322070: val_loss -0.4714
2024-12-20 21:26:03.322893: Pseudo dice [0.7229]
2024-12-20 21:26:03.323758: Epoch time: 155.44 s
2024-12-20 21:26:03.324653: Yayy! New best EMA pseudo Dice: 0.6918
2024-12-20 21:26:05.291585: 
2024-12-20 21:26:05.292493: Epoch 64
2024-12-20 21:26:05.293214: Current learning rate: 0.00606
2024-12-20 21:29:31.973615: Validation loss did not improve from -0.47145. Patience: 1/50
2024-12-20 21:29:31.974422: train_loss -0.7403
2024-12-20 21:29:31.975321: val_loss -0.4284
2024-12-20 21:29:31.976329: Pseudo dice [0.7036]
2024-12-20 21:29:31.977287: Epoch time: 206.68 s
2024-12-20 21:29:32.393602: Yayy! New best EMA pseudo Dice: 0.693
2024-12-20 21:29:34.341683: 
2024-12-20 21:29:34.343064: Epoch 65
2024-12-20 21:29:34.344105: Current learning rate: 0.006
2024-12-20 21:32:43.191669: Validation loss did not improve from -0.47145. Patience: 2/50
2024-12-20 21:32:43.192478: train_loss -0.7413
2024-12-20 21:32:43.193197: val_loss -0.409
2024-12-20 21:32:43.193849: Pseudo dice [0.697]
2024-12-20 21:32:43.194462: Epoch time: 188.85 s
2024-12-20 21:32:43.195248: Yayy! New best EMA pseudo Dice: 0.6934
2024-12-20 21:32:45.097643: 
2024-12-20 21:32:45.098678: Epoch 66
2024-12-20 21:32:45.099440: Current learning rate: 0.00593
2024-12-20 21:36:41.475940: Validation loss did not improve from -0.47145. Patience: 3/50
2024-12-20 21:36:41.476931: train_loss -0.7447
2024-12-20 21:36:41.477735: val_loss -0.4353
2024-12-20 21:36:41.478462: Pseudo dice [0.71]
2024-12-20 21:36:41.479156: Epoch time: 236.38 s
2024-12-20 21:36:41.479884: Yayy! New best EMA pseudo Dice: 0.695
2024-12-20 21:36:43.407632: 
2024-12-20 21:36:43.408897: Epoch 67
2024-12-20 21:36:43.409771: Current learning rate: 0.00587
2024-12-20 21:40:53.262582: Validation loss did not improve from -0.47145. Patience: 4/50
2024-12-20 21:40:53.263579: train_loss -0.7428
2024-12-20 21:40:53.264330: val_loss -0.3924
2024-12-20 21:40:53.264999: Pseudo dice [0.679]
2024-12-20 21:40:53.265689: Epoch time: 249.86 s
2024-12-20 21:40:54.800106: 
2024-12-20 21:40:54.801303: Epoch 68
2024-12-20 21:40:54.801960: Current learning rate: 0.00581
2024-12-20 21:44:33.352629: Validation loss did not improve from -0.47145. Patience: 5/50
2024-12-20 21:44:33.353433: train_loss -0.7415
2024-12-20 21:44:33.354215: val_loss -0.4026
2024-12-20 21:44:33.354990: Pseudo dice [0.6948]
2024-12-20 21:44:33.355749: Epoch time: 218.55 s
2024-12-20 21:44:34.865210: 
2024-12-20 21:44:34.866323: Epoch 69
2024-12-20 21:44:34.867146: Current learning rate: 0.00574
2024-12-20 21:48:15.580192: Validation loss did not improve from -0.47145. Patience: 6/50
2024-12-20 21:48:15.581193: train_loss -0.7488
2024-12-20 21:48:15.582176: val_loss -0.4305
2024-12-20 21:48:15.583152: Pseudo dice [0.709]
2024-12-20 21:48:15.584090: Epoch time: 220.72 s
2024-12-20 21:48:15.997866: Yayy! New best EMA pseudo Dice: 0.6951
2024-12-20 21:48:18.012554: 
2024-12-20 21:48:18.013946: Epoch 70
2024-12-20 21:48:18.014977: Current learning rate: 0.00568
2024-12-20 21:52:13.104496: Validation loss did not improve from -0.47145. Patience: 7/50
2024-12-20 21:52:13.105879: train_loss -0.7512
2024-12-20 21:52:13.106917: val_loss -0.4426
2024-12-20 21:52:13.107795: Pseudo dice [0.7095]
2024-12-20 21:52:13.108688: Epoch time: 235.09 s
2024-12-20 21:52:13.109494: Yayy! New best EMA pseudo Dice: 0.6966
2024-12-20 21:52:15.393029: 
2024-12-20 21:52:15.394108: Epoch 71
2024-12-20 21:52:15.394833: Current learning rate: 0.00562
2024-12-20 21:55:12.786115: Validation loss did not improve from -0.47145. Patience: 8/50
2024-12-20 21:55:12.787292: train_loss -0.7498
2024-12-20 21:55:12.788064: val_loss -0.3123
2024-12-20 21:55:12.788904: Pseudo dice [0.6551]
2024-12-20 21:55:12.789741: Epoch time: 177.4 s
2024-12-20 21:55:14.292259: 
2024-12-20 21:55:14.293606: Epoch 72
2024-12-20 21:55:14.294364: Current learning rate: 0.00555
2024-12-20 21:58:53.042175: Validation loss did not improve from -0.47145. Patience: 9/50
2024-12-20 21:58:53.042954: train_loss -0.7479
2024-12-20 21:58:53.043861: val_loss -0.3953
2024-12-20 21:58:53.044563: Pseudo dice [0.6851]
2024-12-20 21:58:53.045230: Epoch time: 218.75 s
2024-12-20 21:58:54.603189: 
2024-12-20 21:58:54.604431: Epoch 73
2024-12-20 21:58:54.605198: Current learning rate: 0.00549
2024-12-20 22:02:18.377284: Validation loss did not improve from -0.47145. Patience: 10/50
2024-12-20 22:02:18.378437: train_loss -0.7513
2024-12-20 22:02:18.379411: val_loss -0.4251
2024-12-20 22:02:18.380352: Pseudo dice [0.6977]
2024-12-20 22:02:18.381076: Epoch time: 203.78 s
2024-12-20 22:02:19.861798: 
2024-12-20 22:02:19.863188: Epoch 74
2024-12-20 22:02:19.863984: Current learning rate: 0.00542
2024-12-20 22:06:06.222489: Validation loss did not improve from -0.47145. Patience: 11/50
2024-12-20 22:06:06.223411: train_loss -0.7552
2024-12-20 22:06:06.224301: val_loss -0.3387
2024-12-20 22:06:06.225025: Pseudo dice [0.661]
2024-12-20 22:06:06.225861: Epoch time: 226.36 s
2024-12-20 22:06:08.212744: 
2024-12-20 22:06:08.214212: Epoch 75
2024-12-20 22:06:08.215092: Current learning rate: 0.00536
2024-12-20 22:09:48.290418: Validation loss did not improve from -0.47145. Patience: 12/50
2024-12-20 22:09:48.291377: train_loss -0.7527
2024-12-20 22:09:48.292206: val_loss -0.3822
2024-12-20 22:09:48.292901: Pseudo dice [0.6764]
2024-12-20 22:09:48.293553: Epoch time: 220.08 s
2024-12-20 22:09:49.784660: 
2024-12-20 22:09:49.786016: Epoch 76
2024-12-20 22:09:49.786727: Current learning rate: 0.00529
2024-12-20 22:13:15.418834: Validation loss did not improve from -0.47145. Patience: 13/50
2024-12-20 22:13:15.419642: train_loss -0.7551
2024-12-20 22:13:15.420360: val_loss -0.4074
2024-12-20 22:13:15.420981: Pseudo dice [0.6952]
2024-12-20 22:13:15.421606: Epoch time: 205.64 s
2024-12-20 22:13:17.021595: 
2024-12-20 22:13:17.022603: Epoch 77
2024-12-20 22:13:17.023433: Current learning rate: 0.00523
2024-12-20 22:17:04.868546: Validation loss did not improve from -0.47145. Patience: 14/50
2024-12-20 22:17:04.869654: train_loss -0.7561
2024-12-20 22:17:04.870532: val_loss -0.398
2024-12-20 22:17:04.871264: Pseudo dice [0.6865]
2024-12-20 22:17:04.871928: Epoch time: 227.85 s
2024-12-20 22:17:06.389861: 
2024-12-20 22:17:06.390866: Epoch 78
2024-12-20 22:17:06.391573: Current learning rate: 0.00517
2024-12-20 22:21:24.217942: Validation loss did not improve from -0.47145. Patience: 15/50
2024-12-20 22:21:24.218950: train_loss -0.757
2024-12-20 22:21:24.219850: val_loss -0.4541
2024-12-20 22:21:24.220705: Pseudo dice [0.7195]
2024-12-20 22:21:24.221505: Epoch time: 257.83 s
2024-12-20 22:21:25.748711: 
2024-12-20 22:21:25.749930: Epoch 79
2024-12-20 22:21:25.750801: Current learning rate: 0.0051
2024-12-20 22:25:12.224483: Validation loss did not improve from -0.47145. Patience: 16/50
2024-12-20 22:25:12.225387: train_loss -0.7634
2024-12-20 22:25:12.226315: val_loss -0.386
2024-12-20 22:25:12.227126: Pseudo dice [0.6904]
2024-12-20 22:25:12.228028: Epoch time: 226.48 s
2024-12-20 22:25:14.277958: 
2024-12-20 22:25:14.279353: Epoch 80
2024-12-20 22:25:14.280212: Current learning rate: 0.00504
2024-12-20 22:28:00.865076: Validation loss did not improve from -0.47145. Patience: 17/50
2024-12-20 22:28:00.866484: train_loss -0.7624
2024-12-20 22:28:00.867438: val_loss -0.3846
2024-12-20 22:28:00.868073: Pseudo dice [0.6888]
2024-12-20 22:28:00.868733: Epoch time: 166.59 s
2024-12-20 22:28:02.392078: 
2024-12-20 22:28:02.393253: Epoch 81
2024-12-20 22:28:02.393992: Current learning rate: 0.00497
2024-12-20 22:31:35.397927: Validation loss did not improve from -0.47145. Patience: 18/50
2024-12-20 22:31:35.398902: train_loss -0.7621
2024-12-20 22:31:35.399686: val_loss -0.4087
2024-12-20 22:31:35.400391: Pseudo dice [0.6905]
2024-12-20 22:31:35.401166: Epoch time: 213.01 s
2024-12-20 22:31:37.433114: 
2024-12-20 22:31:37.434394: Epoch 82
2024-12-20 22:31:37.435216: Current learning rate: 0.00491
2024-12-20 22:34:56.624033: Validation loss did not improve from -0.47145. Patience: 19/50
2024-12-20 22:34:56.625585: train_loss -0.7663
2024-12-20 22:34:56.626497: val_loss -0.403
2024-12-20 22:34:56.627145: Pseudo dice [0.6848]
2024-12-20 22:34:56.627800: Epoch time: 199.19 s
2024-12-20 22:34:58.033031: 
2024-12-20 22:34:58.034276: Epoch 83
2024-12-20 22:34:58.034985: Current learning rate: 0.00484
2024-12-20 22:38:52.467105: Validation loss did not improve from -0.47145. Patience: 20/50
2024-12-20 22:38:52.467989: train_loss -0.7617
2024-12-20 22:38:52.469104: val_loss -0.3808
2024-12-20 22:38:52.470136: Pseudo dice [0.6896]
2024-12-20 22:38:52.471146: Epoch time: 234.44 s
2024-12-20 22:38:53.884592: 
2024-12-20 22:38:53.885737: Epoch 84
2024-12-20 22:38:53.886838: Current learning rate: 0.00478
2024-12-20 22:42:57.734711: Validation loss did not improve from -0.47145. Patience: 21/50
2024-12-20 22:42:57.735375: train_loss -0.7617
2024-12-20 22:42:57.736145: val_loss -0.3546
2024-12-20 22:42:57.736795: Pseudo dice [0.6802]
2024-12-20 22:42:57.737643: Epoch time: 243.85 s
2024-12-20 22:42:59.655794: 
2024-12-20 22:42:59.657078: Epoch 85
2024-12-20 22:42:59.657838: Current learning rate: 0.00471
2024-12-20 22:46:46.367935: Validation loss did not improve from -0.47145. Patience: 22/50
2024-12-20 22:46:46.369012: train_loss -0.7647
2024-12-20 22:46:46.369925: val_loss -0.3974
2024-12-20 22:46:46.370569: Pseudo dice [0.6938]
2024-12-20 22:46:46.371245: Epoch time: 226.71 s
2024-12-20 22:46:47.851023: 
2024-12-20 22:46:47.852322: Epoch 86
2024-12-20 22:46:47.853298: Current learning rate: 0.00465
2024-12-20 22:50:23.191550: Validation loss did not improve from -0.47145. Patience: 23/50
2024-12-20 22:50:23.192510: train_loss -0.7702
2024-12-20 22:50:23.193435: val_loss -0.3362
2024-12-20 22:50:23.194148: Pseudo dice [0.663]
2024-12-20 22:50:23.194825: Epoch time: 215.34 s
2024-12-20 22:50:24.608439: 
2024-12-20 22:50:24.609557: Epoch 87
2024-12-20 22:50:24.610276: Current learning rate: 0.00458
2024-12-20 22:54:15.948251: Validation loss did not improve from -0.47145. Patience: 24/50
2024-12-20 22:54:15.949441: train_loss -0.7693
2024-12-20 22:54:15.950649: val_loss -0.3805
2024-12-20 22:54:15.951555: Pseudo dice [0.6839]
2024-12-20 22:54:15.952395: Epoch time: 231.34 s
2024-12-20 22:54:17.463058: 
2024-12-20 22:54:17.464500: Epoch 88
2024-12-20 22:54:17.465551: Current learning rate: 0.00452
2024-12-20 22:58:12.834187: Validation loss did not improve from -0.47145. Patience: 25/50
2024-12-20 22:58:12.836951: train_loss -0.7774
2024-12-20 22:58:12.837990: val_loss -0.3419
2024-12-20 22:58:12.838790: Pseudo dice [0.6805]
2024-12-20 22:58:12.839864: Epoch time: 235.38 s
2024-12-20 22:58:14.243335: 
2024-12-20 22:58:14.244541: Epoch 89
2024-12-20 22:58:14.245253: Current learning rate: 0.00445
2024-12-20 23:01:50.565287: Validation loss did not improve from -0.47145. Patience: 26/50
2024-12-20 23:01:50.566522: train_loss -0.7718
2024-12-20 23:01:50.568073: val_loss -0.4479
2024-12-20 23:01:50.568838: Pseudo dice [0.7159]
2024-12-20 23:01:50.569777: Epoch time: 216.32 s
2024-12-20 23:01:52.384190: 
2024-12-20 23:01:52.385563: Epoch 90
2024-12-20 23:01:52.386563: Current learning rate: 0.00438
2024-12-20 23:05:38.888635: Validation loss did not improve from -0.47145. Patience: 27/50
2024-12-20 23:05:38.889621: train_loss -0.7732
2024-12-20 23:05:38.890420: val_loss -0.3881
2024-12-20 23:05:38.891245: Pseudo dice [0.6911]
2024-12-20 23:05:38.891949: Epoch time: 226.51 s
2024-12-20 23:05:40.255198: 
2024-12-20 23:05:40.256713: Epoch 91
2024-12-20 23:05:40.257568: Current learning rate: 0.00432
2024-12-20 23:09:25.453950: Validation loss did not improve from -0.47145. Patience: 28/50
2024-12-20 23:09:25.454860: train_loss -0.7691
2024-12-20 23:09:25.455814: val_loss -0.4516
2024-12-20 23:09:25.456779: Pseudo dice [0.7146]
2024-12-20 23:09:25.457659: Epoch time: 225.2 s
2024-12-20 23:09:26.886554: 
2024-12-20 23:09:26.887627: Epoch 92
2024-12-20 23:09:26.888358: Current learning rate: 0.00425
2024-12-20 23:13:22.878889: Validation loss did not improve from -0.47145. Patience: 29/50
2024-12-20 23:13:22.879713: train_loss -0.7781
2024-12-20 23:13:22.880817: val_loss -0.4165
2024-12-20 23:13:22.881794: Pseudo dice [0.6971]
2024-12-20 23:13:22.882661: Epoch time: 235.99 s
2024-12-20 23:13:24.789207: 
2024-12-20 23:13:24.790344: Epoch 93
2024-12-20 23:13:24.791045: Current learning rate: 0.00419
2024-12-20 23:17:23.243613: Validation loss did not improve from -0.47145. Patience: 30/50
2024-12-20 23:17:23.244543: train_loss -0.7786
2024-12-20 23:17:23.245488: val_loss -0.4238
2024-12-20 23:17:23.246115: Pseudo dice [0.6933]
2024-12-20 23:17:23.246779: Epoch time: 238.46 s
2024-12-20 23:17:24.643186: 
2024-12-20 23:17:24.644245: Epoch 94
2024-12-20 23:17:24.644898: Current learning rate: 0.00412
2024-12-20 23:21:37.176759: Validation loss did not improve from -0.47145. Patience: 31/50
2024-12-20 23:21:37.177715: train_loss -0.7756
2024-12-20 23:21:37.178504: val_loss -0.4521
2024-12-20 23:21:37.179248: Pseudo dice [0.7118]
2024-12-20 23:21:37.179966: Epoch time: 252.54 s
2024-12-20 23:21:39.007689: 
2024-12-20 23:21:39.008868: Epoch 95
2024-12-20 23:21:39.009626: Current learning rate: 0.00405
2024-12-20 23:25:46.951712: Validation loss did not improve from -0.47145. Patience: 32/50
2024-12-20 23:25:46.952573: train_loss -0.7776
2024-12-20 23:25:46.953516: val_loss -0.3969
2024-12-20 23:25:46.954312: Pseudo dice [0.6903]
2024-12-20 23:25:46.955032: Epoch time: 247.95 s
2024-12-20 23:25:48.403952: 
2024-12-20 23:25:48.404823: Epoch 96
2024-12-20 23:25:48.405650: Current learning rate: 0.00399
2024-12-20 23:29:17.153391: Validation loss did not improve from -0.47145. Patience: 33/50
2024-12-20 23:29:17.154380: train_loss -0.7804
2024-12-20 23:29:17.155117: val_loss -0.3692
2024-12-20 23:29:17.155926: Pseudo dice [0.6744]
2024-12-20 23:29:17.156645: Epoch time: 208.75 s
2024-12-20 23:29:18.605119: 
2024-12-20 23:29:18.606412: Epoch 97
2024-12-20 23:29:18.607126: Current learning rate: 0.00392
2024-12-20 23:32:37.850819: Validation loss did not improve from -0.47145. Patience: 34/50
2024-12-20 23:32:37.851653: train_loss -0.7805
2024-12-20 23:32:37.852689: val_loss -0.3884
2024-12-20 23:32:37.853537: Pseudo dice [0.689]
2024-12-20 23:32:37.854435: Epoch time: 199.25 s
2024-12-20 23:32:39.352762: 
2024-12-20 23:32:39.354070: Epoch 98
2024-12-20 23:32:39.355027: Current learning rate: 0.00385
2024-12-20 23:36:07.521414: Validation loss did not improve from -0.47145. Patience: 35/50
2024-12-20 23:36:07.522334: train_loss -0.7817
2024-12-20 23:36:07.523239: val_loss -0.3328
2024-12-20 23:36:07.523846: Pseudo dice [0.6791]
2024-12-20 23:36:07.524503: Epoch time: 208.17 s
2024-12-20 23:36:08.979455: 
2024-12-20 23:36:08.980601: Epoch 99
2024-12-20 23:36:08.981361: Current learning rate: 0.00379
2024-12-20 23:39:28.478520: Validation loss did not improve from -0.47145. Patience: 36/50
2024-12-20 23:39:28.480160: train_loss -0.7821
2024-12-20 23:39:28.480935: val_loss -0.4349
2024-12-20 23:39:28.481595: Pseudo dice [0.7167]
2024-12-20 23:39:28.482495: Epoch time: 199.5 s
2024-12-20 23:39:30.411613: 
2024-12-20 23:39:30.412997: Epoch 100
2024-12-20 23:39:30.413921: Current learning rate: 0.00372
2024-12-20 23:43:24.204139: Validation loss did not improve from -0.47145. Patience: 37/50
2024-12-20 23:43:24.205062: train_loss -0.7824
2024-12-20 23:43:24.206008: val_loss -0.364
2024-12-20 23:43:24.207040: Pseudo dice [0.6795]
2024-12-20 23:43:24.207930: Epoch time: 233.79 s
2024-12-20 23:43:25.699488: 
2024-12-20 23:43:25.700806: Epoch 101
2024-12-20 23:43:25.701571: Current learning rate: 0.00365
2024-12-20 23:46:48.641676: Validation loss did not improve from -0.47145. Patience: 38/50
2024-12-20 23:46:48.642609: train_loss -0.7827
2024-12-20 23:46:48.643499: val_loss -0.4028
2024-12-20 23:46:48.644292: Pseudo dice [0.681]
2024-12-20 23:46:48.645099: Epoch time: 202.94 s
2024-12-20 23:46:50.094843: 
2024-12-20 23:46:50.096222: Epoch 102
2024-12-20 23:46:50.097123: Current learning rate: 0.00359
2024-12-20 23:50:25.777976: Validation loss did not improve from -0.47145. Patience: 39/50
2024-12-20 23:50:25.778871: train_loss -0.7826
2024-12-20 23:50:25.779726: val_loss -0.3614
2024-12-20 23:50:25.780407: Pseudo dice [0.6729]
2024-12-20 23:50:25.781121: Epoch time: 215.69 s
2024-12-20 23:50:27.210566: 
2024-12-20 23:50:27.211900: Epoch 103
2024-12-20 23:50:27.212726: Current learning rate: 0.00352
2024-12-20 23:54:20.216178: Validation loss did not improve from -0.47145. Patience: 40/50
2024-12-20 23:54:20.217119: train_loss -0.7856
2024-12-20 23:54:20.217881: val_loss -0.4012
2024-12-20 23:54:20.218748: Pseudo dice [0.6966]
2024-12-20 23:54:20.219621: Epoch time: 233.01 s
2024-12-20 23:54:22.224322: 
2024-12-20 23:54:22.225298: Epoch 104
2024-12-20 23:54:22.226152: Current learning rate: 0.00345
2024-12-20 23:57:45.736916: Validation loss did not improve from -0.47145. Patience: 41/50
2024-12-20 23:57:45.737752: train_loss -0.7872
2024-12-20 23:57:45.738541: val_loss -0.3975
2024-12-20 23:57:45.739321: Pseudo dice [0.7023]
2024-12-20 23:57:45.740069: Epoch time: 203.51 s
2024-12-20 23:57:47.670933: 
2024-12-20 23:57:47.671952: Epoch 105
2024-12-20 23:57:47.672821: Current learning rate: 0.00338
2024-12-21 00:01:29.267771: Validation loss did not improve from -0.47145. Patience: 42/50
2024-12-21 00:01:29.270774: train_loss -0.7882
2024-12-21 00:01:29.271840: val_loss -0.4223
2024-12-21 00:01:29.272501: Pseudo dice [0.7121]
2024-12-21 00:01:29.273335: Epoch time: 221.6 s
2024-12-21 00:01:30.735890: 
2024-12-21 00:01:30.737147: Epoch 106
2024-12-21 00:01:30.738140: Current learning rate: 0.00332
2024-12-21 00:05:17.933248: Validation loss did not improve from -0.47145. Patience: 43/50
2024-12-21 00:05:17.934027: train_loss -0.7906
2024-12-21 00:05:17.934738: val_loss -0.3833
2024-12-21 00:05:17.935351: Pseudo dice [0.6888]
2024-12-21 00:05:17.936059: Epoch time: 227.2 s
2024-12-21 00:05:19.476112: 
2024-12-21 00:05:19.477210: Epoch 107
2024-12-21 00:05:19.478081: Current learning rate: 0.00325
2024-12-21 00:08:46.817874: Validation loss did not improve from -0.47145. Patience: 44/50
2024-12-21 00:08:46.818702: train_loss -0.788
2024-12-21 00:08:46.819657: val_loss -0.3635
2024-12-21 00:08:46.820652: Pseudo dice [0.6905]
2024-12-21 00:08:46.821483: Epoch time: 207.34 s
2024-12-21 00:08:48.269713: 
2024-12-21 00:08:48.270750: Epoch 108
2024-12-21 00:08:48.271752: Current learning rate: 0.00318
2024-12-21 00:12:34.410104: Validation loss did not improve from -0.47145. Patience: 45/50
2024-12-21 00:12:34.410919: train_loss -0.7915
2024-12-21 00:12:34.411787: val_loss -0.3946
2024-12-21 00:12:34.412509: Pseudo dice [0.6824]
2024-12-21 00:12:34.413247: Epoch time: 226.14 s
2024-12-21 00:12:35.903886: 
2024-12-21 00:12:35.905233: Epoch 109
2024-12-21 00:12:35.906042: Current learning rate: 0.00311
2024-12-21 00:16:27.536893: Validation loss did not improve from -0.47145. Patience: 46/50
2024-12-21 00:16:27.538073: train_loss -0.7889
2024-12-21 00:16:27.538907: val_loss -0.3908
2024-12-21 00:16:27.539742: Pseudo dice [0.6983]
2024-12-21 00:16:27.540529: Epoch time: 231.64 s
2024-12-21 00:16:29.464336: 
2024-12-21 00:16:29.466163: Epoch 110
2024-12-21 00:16:29.467121: Current learning rate: 0.00304
2024-12-21 00:19:53.567066: Validation loss did not improve from -0.47145. Patience: 47/50
2024-12-21 00:19:53.568480: train_loss -0.7911
2024-12-21 00:19:53.569560: val_loss -0.3368
2024-12-21 00:19:53.570483: Pseudo dice [0.6625]
2024-12-21 00:19:53.571446: Epoch time: 204.11 s
2024-12-21 00:19:54.981190: 
2024-12-21 00:19:54.982632: Epoch 111
2024-12-21 00:19:54.983438: Current learning rate: 0.00297
2024-12-21 00:23:33.652392: Validation loss did not improve from -0.47145. Patience: 48/50
2024-12-21 00:23:33.653436: train_loss -0.7915
2024-12-21 00:23:33.654249: val_loss -0.4122
2024-12-21 00:23:33.655054: Pseudo dice [0.7055]
2024-12-21 00:23:33.655774: Epoch time: 218.67 s
2024-12-21 00:23:35.078882: 
2024-12-21 00:23:35.080269: Epoch 112
2024-12-21 00:23:35.081144: Current learning rate: 0.00291
2024-12-21 00:26:48.917328: Validation loss did not improve from -0.47145. Patience: 49/50
2024-12-21 00:26:48.918395: train_loss -0.7897
2024-12-21 00:26:48.919099: val_loss -0.356
2024-12-21 00:26:48.919764: Pseudo dice [0.6878]
2024-12-21 00:26:48.920548: Epoch time: 193.84 s
2024-12-21 00:26:50.382883: 
2024-12-21 00:26:50.384120: Epoch 113
2024-12-21 00:26:50.384805: Current learning rate: 0.00284
2024-12-21 00:30:08.703577: Validation loss did not improve from -0.47145. Patience: 50/50
2024-12-21 00:30:08.704554: train_loss -0.7938
2024-12-21 00:30:08.705558: val_loss -0.343
2024-12-21 00:30:08.706369: Pseudo dice [0.6688]
2024-12-21 00:30:08.707245: Epoch time: 198.32 s
2024-12-21 00:30:10.126570: 
2024-12-21 00:30:10.128049: Epoch 114
2024-12-21 00:30:10.129127: Current learning rate: 0.00277
2024-12-21 00:34:06.982246: Validation loss did not improve from -0.47145. Patience: 51/50
2024-12-21 00:34:06.983229: train_loss -0.7932
2024-12-21 00:34:06.984001: val_loss -0.4387
2024-12-21 00:34:06.984782: Pseudo dice [0.7088]
2024-12-21 00:34:06.985590: Epoch time: 236.86 s
2024-12-21 00:34:09.442732: 
2024-12-21 00:34:09.443992: Epoch 115
2024-12-21 00:34:09.444760: Current learning rate: 0.0027
2024-12-21 00:37:40.860967: Validation loss did not improve from -0.47145. Patience: 52/50
2024-12-21 00:37:40.862595: train_loss -0.7952
2024-12-21 00:37:40.864917: val_loss -0.3706
2024-12-21 00:37:40.865613: Pseudo dice [0.6743]
2024-12-21 00:37:40.866320: Epoch time: 211.42 s
2024-12-21 00:37:42.351538: 
2024-12-21 00:37:42.352698: Epoch 116
2024-12-21 00:37:42.353534: Current learning rate: 0.00263
2024-12-21 00:41:20.217355: Validation loss did not improve from -0.47145. Patience: 53/50
2024-12-21 00:41:20.218370: train_loss -0.794
2024-12-21 00:41:20.219334: val_loss -0.3658
2024-12-21 00:41:20.219959: Pseudo dice [0.6754]
2024-12-21 00:41:20.220607: Epoch time: 217.87 s
2024-12-21 00:41:21.701874: 
2024-12-21 00:41:21.703000: Epoch 117
2024-12-21 00:41:21.703712: Current learning rate: 0.00256
2024-12-21 00:45:12.102261: Validation loss did not improve from -0.47145. Patience: 54/50
2024-12-21 00:45:12.103360: train_loss -0.7978
2024-12-21 00:45:12.104203: val_loss -0.3406
2024-12-21 00:45:12.104885: Pseudo dice [0.6728]
2024-12-21 00:45:12.105620: Epoch time: 230.4 s
2024-12-21 00:45:13.653384: 
2024-12-21 00:45:13.655001: Epoch 118
2024-12-21 00:45:13.656009: Current learning rate: 0.00249
2024-12-21 00:48:43.593756: Validation loss did not improve from -0.47145. Patience: 55/50
2024-12-21 00:48:43.594727: train_loss -0.7962
2024-12-21 00:48:43.595515: val_loss -0.3908
2024-12-21 00:48:43.597430: Pseudo dice [0.6949]
2024-12-21 00:48:43.598264: Epoch time: 209.94 s
2024-12-21 00:48:45.077638: 
2024-12-21 00:48:45.078738: Epoch 119
2024-12-21 00:48:45.079391: Current learning rate: 0.00242
2024-12-21 00:52:24.064926: Validation loss did not improve from -0.47145. Patience: 56/50
2024-12-21 00:52:24.065964: train_loss -0.7969
2024-12-21 00:52:24.066782: val_loss -0.399
2024-12-21 00:52:24.067512: Pseudo dice [0.7054]
2024-12-21 00:52:24.068252: Epoch time: 218.99 s
2024-12-21 00:52:26.110381: 
2024-12-21 00:52:26.111773: Epoch 120
2024-12-21 00:52:26.112616: Current learning rate: 0.00235
2024-12-21 00:56:39.995554: Validation loss did not improve from -0.47145. Patience: 57/50
2024-12-21 00:56:39.996543: train_loss -0.7979
2024-12-21 00:56:39.997491: val_loss -0.41
2024-12-21 00:56:39.998472: Pseudo dice [0.7115]
2024-12-21 00:56:39.999475: Epoch time: 253.89 s
2024-12-21 00:56:41.587205: 
2024-12-21 00:56:41.588953: Epoch 121
2024-12-21 00:56:41.590179: Current learning rate: 0.00228
2024-12-21 01:00:01.730905: Validation loss did not improve from -0.47145. Patience: 58/50
2024-12-21 01:00:01.731891: train_loss -0.7979
2024-12-21 01:00:01.732759: val_loss -0.4065
2024-12-21 01:00:01.733606: Pseudo dice [0.6967]
2024-12-21 01:00:01.734505: Epoch time: 200.15 s
2024-12-21 01:00:03.242382: 
2024-12-21 01:00:03.243515: Epoch 122
2024-12-21 01:00:03.244403: Current learning rate: 0.00221
2024-12-21 01:03:18.608619: Validation loss did not improve from -0.47145. Patience: 59/50
2024-12-21 01:03:18.609582: train_loss -0.8028
2024-12-21 01:03:18.610756: val_loss -0.4205
2024-12-21 01:03:18.611781: Pseudo dice [0.7063]
2024-12-21 01:03:18.612774: Epoch time: 195.37 s
2024-12-21 01:03:20.116265: 
2024-12-21 01:03:20.117499: Epoch 123
2024-12-21 01:03:20.118436: Current learning rate: 0.00214
2024-12-21 01:06:39.949435: Validation loss did not improve from -0.47145. Patience: 60/50
2024-12-21 01:06:39.951942: train_loss -0.7981
2024-12-21 01:06:39.953133: val_loss -0.4128
2024-12-21 01:06:39.953929: Pseudo dice [0.7052]
2024-12-21 01:06:39.954767: Epoch time: 199.84 s
2024-12-21 01:06:41.479825: 
2024-12-21 01:06:41.480953: Epoch 124
2024-12-21 01:06:41.481685: Current learning rate: 0.00207
2024-12-21 01:10:17.765833: Validation loss did not improve from -0.47145. Patience: 61/50
2024-12-21 01:10:17.767564: train_loss -0.8014
2024-12-21 01:10:17.770315: val_loss -0.3621
2024-12-21 01:10:17.771256: Pseudo dice [0.6874]
2024-12-21 01:10:17.772623: Epoch time: 216.29 s
2024-12-21 01:10:20.257080: 
2024-12-21 01:10:20.258944: Epoch 125
2024-12-21 01:10:20.260601: Current learning rate: 0.00199
2024-12-21 01:14:20.224014: Validation loss did not improve from -0.47145. Patience: 62/50
2024-12-21 01:14:20.224799: train_loss -0.8008
2024-12-21 01:14:20.225725: val_loss -0.4002
2024-12-21 01:14:20.226453: Pseudo dice [0.6985]
2024-12-21 01:14:20.227122: Epoch time: 239.97 s
2024-12-21 01:14:21.747913: 
2024-12-21 01:14:21.749388: Epoch 126
2024-12-21 01:14:21.750289: Current learning rate: 0.00192
2024-12-21 01:17:56.645137: Validation loss did not improve from -0.47145. Patience: 63/50
2024-12-21 01:17:56.646502: train_loss -0.8006
2024-12-21 01:17:56.647317: val_loss -0.3608
2024-12-21 01:17:56.647972: Pseudo dice [0.6821]
2024-12-21 01:17:56.648775: Epoch time: 214.9 s
2024-12-21 01:17:58.172651: 
2024-12-21 01:17:58.173922: Epoch 127
2024-12-21 01:17:58.174649: Current learning rate: 0.00185
2024-12-21 01:21:28.373767: Validation loss did not improve from -0.47145. Patience: 64/50
2024-12-21 01:21:28.374782: train_loss -0.8066
2024-12-21 01:21:28.375726: val_loss -0.4257
2024-12-21 01:21:28.376417: Pseudo dice [0.706]
2024-12-21 01:21:28.377058: Epoch time: 210.2 s
2024-12-21 01:21:29.828781: 
2024-12-21 01:21:29.829888: Epoch 128
2024-12-21 01:21:29.830571: Current learning rate: 0.00178
2024-12-21 01:25:27.029867: Validation loss did not improve from -0.47145. Patience: 65/50
2024-12-21 01:25:27.030883: train_loss -0.8048
2024-12-21 01:25:27.031636: val_loss -0.3495
2024-12-21 01:25:27.032279: Pseudo dice [0.671]
2024-12-21 01:25:27.032918: Epoch time: 237.2 s
2024-12-21 01:25:28.437494: 
2024-12-21 01:25:28.438693: Epoch 129
2024-12-21 01:25:28.439317: Current learning rate: 0.0017
2024-12-21 01:29:09.515260: Validation loss did not improve from -0.47145. Patience: 66/50
2024-12-21 01:29:09.516129: train_loss -0.8046
2024-12-21 01:29:09.516914: val_loss -0.4062
2024-12-21 01:29:09.517650: Pseudo dice [0.711]
2024-12-21 01:29:09.518544: Epoch time: 221.08 s
2024-12-21 01:29:11.455216: 
2024-12-21 01:29:11.456702: Epoch 130
2024-12-21 01:29:11.457854: Current learning rate: 0.00163
2024-12-21 01:32:15.982804: Validation loss did not improve from -0.47145. Patience: 67/50
2024-12-21 01:32:15.983718: train_loss -0.8066
2024-12-21 01:32:15.984627: val_loss -0.3669
2024-12-21 01:32:15.985524: Pseudo dice [0.687]
2024-12-21 01:32:15.986334: Epoch time: 184.53 s
2024-12-21 01:32:17.459639: 
2024-12-21 01:32:17.461077: Epoch 131
2024-12-21 01:32:17.461920: Current learning rate: 0.00156
2024-12-21 01:36:01.127538: Validation loss did not improve from -0.47145. Patience: 68/50
2024-12-21 01:36:01.128524: train_loss -0.806
2024-12-21 01:36:01.129422: val_loss -0.3321
2024-12-21 01:36:01.130283: Pseudo dice [0.68]
2024-12-21 01:36:01.131019: Epoch time: 223.67 s
2024-12-21 01:36:02.611665: 
2024-12-21 01:36:02.612845: Epoch 132
2024-12-21 01:36:02.613540: Current learning rate: 0.00148
2024-12-21 01:38:52.813896: Validation loss did not improve from -0.47145. Patience: 69/50
2024-12-21 01:38:52.814812: train_loss -0.8012
2024-12-21 01:38:52.815606: val_loss -0.3509
2024-12-21 01:38:52.816423: Pseudo dice [0.694]
2024-12-21 01:38:52.817248: Epoch time: 170.2 s
2024-12-21 01:38:54.332055: 
2024-12-21 01:38:54.333452: Epoch 133
2024-12-21 01:38:54.334351: Current learning rate: 0.00141
2024-12-21 01:42:48.861046: Validation loss did not improve from -0.47145. Patience: 70/50
2024-12-21 01:42:48.862637: train_loss -0.8085
2024-12-21 01:42:48.863519: val_loss -0.4095
2024-12-21 01:42:48.864235: Pseudo dice [0.6939]
2024-12-21 01:42:48.864998: Epoch time: 234.53 s
2024-12-21 01:42:50.402251: 
2024-12-21 01:42:50.403456: Epoch 134
2024-12-21 01:42:50.404326: Current learning rate: 0.00133
2024-12-21 01:46:41.749504: Validation loss did not improve from -0.47145. Patience: 71/50
2024-12-21 01:46:41.750379: train_loss -0.8074
2024-12-21 01:46:41.751146: val_loss -0.3716
2024-12-21 01:46:41.751938: Pseudo dice [0.692]
2024-12-21 01:46:41.752612: Epoch time: 231.35 s
2024-12-21 01:46:43.722420: 
2024-12-21 01:46:43.723652: Epoch 135
2024-12-21 01:46:43.724571: Current learning rate: 0.00126
2024-12-21 01:49:52.362193: Validation loss did not improve from -0.47145. Patience: 72/50
2024-12-21 01:49:52.362989: train_loss -0.808
2024-12-21 01:49:52.363866: val_loss -0.4304
2024-12-21 01:49:52.364691: Pseudo dice [0.7026]
2024-12-21 01:49:52.365504: Epoch time: 188.64 s
2024-12-21 01:49:54.356740: 
2024-12-21 01:49:54.357800: Epoch 136
2024-12-21 01:49:54.358667: Current learning rate: 0.00118
2024-12-21 01:54:11.668770: Validation loss did not improve from -0.47145. Patience: 73/50
2024-12-21 01:54:11.669758: train_loss -0.8097
2024-12-21 01:54:11.670509: val_loss -0.3862
2024-12-21 01:54:11.671119: Pseudo dice [0.6979]
2024-12-21 01:54:11.671784: Epoch time: 257.31 s
2024-12-21 01:54:13.168254: 
2024-12-21 01:54:13.169548: Epoch 137
2024-12-21 01:54:13.170228: Current learning rate: 0.00111
2024-12-21 01:58:04.435036: Validation loss did not improve from -0.47145. Patience: 74/50
2024-12-21 01:58:04.435717: train_loss -0.8049
2024-12-21 01:58:04.436417: val_loss -0.3718
2024-12-21 01:58:04.437028: Pseudo dice [0.6809]
2024-12-21 01:58:04.437649: Epoch time: 231.27 s
2024-12-21 01:58:05.939014: 
2024-12-21 01:58:05.940059: Epoch 138
2024-12-21 01:58:05.940876: Current learning rate: 0.00103
2024-12-21 02:01:36.545540: Validation loss did not improve from -0.47145. Patience: 75/50
2024-12-21 02:01:36.546604: train_loss -0.8068
2024-12-21 02:01:36.547317: val_loss -0.4141
2024-12-21 02:01:36.548165: Pseudo dice [0.7056]
2024-12-21 02:01:36.548931: Epoch time: 210.61 s
2024-12-21 02:01:38.042403: 
2024-12-21 02:01:38.044239: Epoch 139
2024-12-21 02:01:38.045059: Current learning rate: 0.00095
2024-12-21 02:05:35.217277: Validation loss did not improve from -0.47145. Patience: 76/50
2024-12-21 02:05:35.218266: train_loss -0.808
2024-12-21 02:05:35.219091: val_loss -0.3807
2024-12-21 02:05:35.219710: Pseudo dice [0.6974]
2024-12-21 02:05:35.220299: Epoch time: 237.18 s
2024-12-21 02:05:37.073123: 
2024-12-21 02:05:37.074298: Epoch 140
2024-12-21 02:05:37.075163: Current learning rate: 0.00087
2024-12-21 02:09:42.517329: Validation loss did not improve from -0.47145. Patience: 77/50
2024-12-21 02:09:42.518132: train_loss -0.8091
2024-12-21 02:09:42.519044: val_loss -0.3786
2024-12-21 02:09:42.519774: Pseudo dice [0.6832]
2024-12-21 02:09:42.520541: Epoch time: 245.45 s
2024-12-21 02:09:43.973544: 
2024-12-21 02:09:43.975101: Epoch 141
2024-12-21 02:09:43.976239: Current learning rate: 0.00079
2024-12-21 02:12:49.004171: Validation loss did not improve from -0.47145. Patience: 78/50
2024-12-21 02:12:49.006385: train_loss -0.8095
2024-12-21 02:12:49.007248: val_loss -0.3681
2024-12-21 02:12:49.007940: Pseudo dice [0.6898]
2024-12-21 02:12:49.008604: Epoch time: 185.03 s
2024-12-21 02:12:50.495816: 
2024-12-21 02:12:50.497022: Epoch 142
2024-12-21 02:12:50.497721: Current learning rate: 0.00071
2024-12-21 02:16:38.063812: Validation loss did not improve from -0.47145. Patience: 79/50
2024-12-21 02:16:38.065411: train_loss -0.8106
2024-12-21 02:16:38.067351: val_loss -0.3559
2024-12-21 02:16:38.068419: Pseudo dice [0.6892]
2024-12-21 02:16:38.069956: Epoch time: 227.57 s
2024-12-21 02:16:39.562664: 
2024-12-21 02:16:39.563851: Epoch 143
2024-12-21 02:16:39.564666: Current learning rate: 0.00063
2024-12-21 02:19:46.094413: Validation loss did not improve from -0.47145. Patience: 80/50
2024-12-21 02:19:46.095250: train_loss -0.8102
2024-12-21 02:19:46.096070: val_loss -0.3694
2024-12-21 02:19:46.096688: Pseudo dice [0.6928]
2024-12-21 02:19:46.097297: Epoch time: 186.53 s
2024-12-21 02:19:47.603364: 
2024-12-21 02:19:47.604627: Epoch 144
2024-12-21 02:19:47.605314: Current learning rate: 0.00055
2024-12-21 02:23:36.349609: Validation loss did not improve from -0.47145. Patience: 81/50
2024-12-21 02:23:36.350703: train_loss -0.8115
2024-12-21 02:23:36.351441: val_loss -0.338
2024-12-21 02:23:36.352388: Pseudo dice [0.6711]
2024-12-21 02:23:36.353164: Epoch time: 228.75 s
2024-12-21 02:23:38.390000: 
2024-12-21 02:23:38.391513: Epoch 145
2024-12-21 02:23:38.392246: Current learning rate: 0.00047
2024-12-21 02:27:17.639959: Validation loss did not improve from -0.47145. Patience: 82/50
2024-12-21 02:27:17.641032: train_loss -0.8131
2024-12-21 02:27:17.641752: val_loss -0.3726
2024-12-21 02:27:17.642486: Pseudo dice [0.6866]
2024-12-21 02:27:17.643168: Epoch time: 219.26 s
2024-12-21 02:27:19.138867: 
2024-12-21 02:27:19.140032: Epoch 146
2024-12-21 02:27:19.140878: Current learning rate: 0.00038
2024-12-21 02:30:53.505458: Validation loss did not improve from -0.47145. Patience: 83/50
2024-12-21 02:30:53.506292: train_loss -0.814
2024-12-21 02:30:53.507278: val_loss -0.3216
2024-12-21 02:30:53.508140: Pseudo dice [0.6786]
2024-12-21 02:30:53.508919: Epoch time: 214.37 s
2024-12-21 02:30:56.009680: 
2024-12-21 02:30:56.011126: Epoch 147
2024-12-21 02:30:56.011936: Current learning rate: 0.0003
2024-12-21 02:34:39.574824: Validation loss did not improve from -0.47145. Patience: 84/50
2024-12-21 02:34:39.575823: train_loss -0.8123
2024-12-21 02:34:39.576655: val_loss -0.3717
2024-12-21 02:34:39.577428: Pseudo dice [0.6926]
2024-12-21 02:34:39.578053: Epoch time: 223.57 s
2024-12-21 02:34:41.112804: 
2024-12-21 02:34:41.114133: Epoch 148
2024-12-21 02:34:41.114799: Current learning rate: 0.00021
2024-12-21 02:38:26.040567: Validation loss did not improve from -0.47145. Patience: 85/50
2024-12-21 02:38:26.041382: train_loss -0.8109
2024-12-21 02:38:26.042360: val_loss -0.3754
2024-12-21 02:38:26.043175: Pseudo dice [0.6976]
2024-12-21 02:38:26.044067: Epoch time: 224.93 s
2024-12-21 02:38:27.538061: 
2024-12-21 02:38:27.539297: Epoch 149
2024-12-21 02:38:27.540208: Current learning rate: 0.00011
2024-12-21 02:41:52.649235: Validation loss did not improve from -0.47145. Patience: 86/50
2024-12-21 02:41:52.650200: train_loss -0.8143
2024-12-21 02:41:52.651156: val_loss -0.3541
2024-12-21 02:41:52.651873: Pseudo dice [0.6852]
2024-12-21 02:41:52.652660: Epoch time: 205.11 s
2024-12-21 02:41:54.586731: Training done.
2024-12-20 17:29:32.515208: unpacking done...
2024-12-20 17:29:32.651604: Unable to plot network architecture: nnUNet_compile is enabled!
2024-12-20 17:29:32.700886: 
2024-12-20 17:29:32.702177: Epoch 0
2024-12-20 17:29:32.703223: Current learning rate: 0.01
2024-12-20 17:34:18.961008: Validation loss improved from 1000.00000 to -0.14427! Patience: 0/50
2024-12-20 17:34:18.962321: train_loss -0.0893
2024-12-20 17:34:18.963023: val_loss -0.1443
2024-12-20 17:34:18.963711: Pseudo dice [0.5283]
2024-12-20 17:34:18.964358: Epoch time: 286.26 s
2024-12-20 17:34:18.965071: Yayy! New best EMA pseudo Dice: 0.5283
2024-12-20 17:34:20.622709: 
2024-12-20 17:34:20.623900: Epoch 1
2024-12-20 17:34:20.624886: Current learning rate: 0.00994
2024-12-20 17:38:27.687739: Validation loss did not improve from -0.14427. Patience: 1/50
2024-12-20 17:38:27.688917: train_loss -0.2558
2024-12-20 17:38:27.689714: val_loss -0.1361
2024-12-20 17:38:27.690341: Pseudo dice [0.5219]
2024-12-20 17:38:27.691048: Epoch time: 247.07 s
2024-12-20 17:38:29.080558: 
2024-12-20 17:38:29.081749: Epoch 2
2024-12-20 17:38:29.082402: Current learning rate: 0.00988
2024-12-20 17:42:45.214597: Validation loss improved from -0.14427 to -0.20837! Patience: 1/50
2024-12-20 17:42:45.215548: train_loss -0.3122
2024-12-20 17:42:45.216294: val_loss -0.2084
2024-12-20 17:42:45.216970: Pseudo dice [0.5563]
2024-12-20 17:42:45.217664: Epoch time: 256.14 s
2024-12-20 17:42:45.218231: Yayy! New best EMA pseudo Dice: 0.5305
2024-12-20 17:42:47.043006: 
2024-12-20 17:42:47.044195: Epoch 3
2024-12-20 17:42:47.045015: Current learning rate: 0.00982
2024-12-20 17:46:25.610455: Validation loss improved from -0.20837 to -0.24203! Patience: 0/50
2024-12-20 17:46:25.611156: train_loss -0.3739
2024-12-20 17:46:25.611831: val_loss -0.242
2024-12-20 17:46:25.612468: Pseudo dice [0.5742]
2024-12-20 17:46:25.613137: Epoch time: 218.57 s
2024-12-20 17:46:25.613769: Yayy! New best EMA pseudo Dice: 0.5349
2024-12-20 17:46:32.656927: 
2024-12-20 17:46:32.658133: Epoch 4
2024-12-20 17:46:32.658875: Current learning rate: 0.00976
2024-12-20 17:49:48.863178: Validation loss did not improve from -0.24203. Patience: 1/50
2024-12-20 17:49:48.863950: train_loss -0.4143
2024-12-20 17:49:48.864712: val_loss -0.2179
2024-12-20 17:49:48.865323: Pseudo dice [0.5334]
2024-12-20 17:49:48.865947: Epoch time: 196.21 s
2024-12-20 17:49:50.620127: 
2024-12-20 17:49:50.621006: Epoch 5
2024-12-20 17:49:50.621711: Current learning rate: 0.0097
2024-12-20 17:52:35.203829: Validation loss improved from -0.24203 to -0.27787! Patience: 1/50
2024-12-20 17:52:35.204726: train_loss -0.4396
2024-12-20 17:52:35.205579: val_loss -0.2779
2024-12-20 17:52:35.206360: Pseudo dice [0.617]
2024-12-20 17:52:35.207020: Epoch time: 164.59 s
2024-12-20 17:52:35.207692: Yayy! New best EMA pseudo Dice: 0.543
2024-12-20 17:52:37.109303: 
2024-12-20 17:52:37.110272: Epoch 6
2024-12-20 17:52:37.111125: Current learning rate: 0.00964
2024-12-20 17:56:11.814111: Validation loss improved from -0.27787 to -0.30237! Patience: 0/50
2024-12-20 17:56:11.814935: train_loss -0.4611
2024-12-20 17:56:11.815874: val_loss -0.3024
2024-12-20 17:56:11.816712: Pseudo dice [0.6138]
2024-12-20 17:56:11.817516: Epoch time: 214.71 s
2024-12-20 17:56:11.818438: Yayy! New best EMA pseudo Dice: 0.55
2024-12-20 17:56:13.561145: 
2024-12-20 17:56:13.562536: Epoch 7
2024-12-20 17:56:13.563650: Current learning rate: 0.00958
2024-12-20 18:00:16.406839: Validation loss did not improve from -0.30237. Patience: 1/50
2024-12-20 18:00:16.407973: train_loss -0.48
2024-12-20 18:00:16.408779: val_loss -0.2829
2024-12-20 18:00:16.409542: Pseudo dice [0.6178]
2024-12-20 18:00:16.410322: Epoch time: 242.85 s
2024-12-20 18:00:16.410983: Yayy! New best EMA pseudo Dice: 0.5568
2024-12-20 18:00:18.632078: 
2024-12-20 18:00:18.633118: Epoch 8
2024-12-20 18:00:18.633894: Current learning rate: 0.00952
2024-12-20 18:04:08.350414: Validation loss did not improve from -0.30237. Patience: 2/50
2024-12-20 18:04:08.351275: train_loss -0.504
2024-12-20 18:04:08.351946: val_loss -0.2962
2024-12-20 18:04:08.352527: Pseudo dice [0.6133]
2024-12-20 18:04:08.353147: Epoch time: 229.72 s
2024-12-20 18:04:08.353779: Yayy! New best EMA pseudo Dice: 0.5625
2024-12-20 18:04:10.148011: 
2024-12-20 18:04:10.149177: Epoch 9
2024-12-20 18:04:10.149988: Current learning rate: 0.00946
2024-12-20 18:08:37.289188: Validation loss improved from -0.30237 to -0.35445! Patience: 2/50
2024-12-20 18:08:37.289768: train_loss -0.5022
2024-12-20 18:08:37.290396: val_loss -0.3544
2024-12-20 18:08:37.290972: Pseudo dice [0.6419]
2024-12-20 18:08:37.291589: Epoch time: 267.14 s
2024-12-20 18:08:37.692479: Yayy! New best EMA pseudo Dice: 0.5704
2024-12-20 18:08:39.351191: 
2024-12-20 18:08:39.352021: Epoch 10
2024-12-20 18:08:39.352674: Current learning rate: 0.0094
2024-12-20 18:12:30.348564: Validation loss did not improve from -0.35445. Patience: 1/50
2024-12-20 18:12:30.349498: train_loss -0.5164
2024-12-20 18:12:30.350299: val_loss -0.3368
2024-12-20 18:12:30.350962: Pseudo dice [0.6436]
2024-12-20 18:12:30.351677: Epoch time: 231.0 s
2024-12-20 18:12:30.352452: Yayy! New best EMA pseudo Dice: 0.5777
2024-12-20 18:12:32.057474: 
2024-12-20 18:12:32.058763: Epoch 11
2024-12-20 18:12:32.059419: Current learning rate: 0.00934
2024-12-20 18:16:28.062486: Validation loss did not improve from -0.35445. Patience: 2/50
2024-12-20 18:16:28.063329: train_loss -0.5393
2024-12-20 18:16:28.064010: val_loss -0.3433
2024-12-20 18:16:28.064738: Pseudo dice [0.6443]
2024-12-20 18:16:28.065281: Epoch time: 236.01 s
2024-12-20 18:16:28.065802: Yayy! New best EMA pseudo Dice: 0.5844
2024-12-20 18:16:29.755694: 
2024-12-20 18:16:29.756907: Epoch 12
2024-12-20 18:16:29.757579: Current learning rate: 0.00928
2024-12-20 18:20:02.544803: Validation loss improved from -0.35445 to -0.35547! Patience: 2/50
2024-12-20 18:20:02.545611: train_loss -0.5527
2024-12-20 18:20:02.546438: val_loss -0.3555
2024-12-20 18:20:02.547215: Pseudo dice [0.6567]
2024-12-20 18:20:02.548084: Epoch time: 212.79 s
2024-12-20 18:20:02.549017: Yayy! New best EMA pseudo Dice: 0.5916
2024-12-20 18:20:04.300064: 
2024-12-20 18:20:04.301513: Epoch 13
2024-12-20 18:20:04.302359: Current learning rate: 0.00922
2024-12-20 18:23:55.978629: Validation loss did not improve from -0.35547. Patience: 1/50
2024-12-20 18:23:55.979931: train_loss -0.5601
2024-12-20 18:23:55.980935: val_loss -0.3515
2024-12-20 18:23:55.981859: Pseudo dice [0.6509]
2024-12-20 18:23:55.982766: Epoch time: 231.68 s
2024-12-20 18:23:55.983570: Yayy! New best EMA pseudo Dice: 0.5976
2024-12-20 18:23:57.736564: 
2024-12-20 18:23:57.737603: Epoch 14
2024-12-20 18:23:57.738500: Current learning rate: 0.00916
2024-12-20 18:27:41.099445: Validation loss improved from -0.35547 to -0.37661! Patience: 1/50
2024-12-20 18:27:41.100349: train_loss -0.5658
2024-12-20 18:27:41.101069: val_loss -0.3766
2024-12-20 18:27:41.101729: Pseudo dice [0.663]
2024-12-20 18:27:41.102465: Epoch time: 223.37 s
2024-12-20 18:27:41.508097: Yayy! New best EMA pseudo Dice: 0.6041
2024-12-20 18:27:43.335075: 
2024-12-20 18:27:43.336286: Epoch 15
2024-12-20 18:27:43.337041: Current learning rate: 0.0091
2024-12-20 18:31:12.946518: Validation loss did not improve from -0.37661. Patience: 1/50
2024-12-20 18:31:12.947385: train_loss -0.5734
2024-12-20 18:31:12.948241: val_loss -0.3549
2024-12-20 18:31:12.948877: Pseudo dice [0.6556]
2024-12-20 18:31:12.949554: Epoch time: 209.61 s
2024-12-20 18:31:12.950235: Yayy! New best EMA pseudo Dice: 0.6093
2024-12-20 18:31:14.693080: 
2024-12-20 18:31:14.694185: Epoch 16
2024-12-20 18:31:14.694942: Current learning rate: 0.00903
2024-12-20 18:34:36.129246: Validation loss did not improve from -0.37661. Patience: 2/50
2024-12-20 18:34:36.130002: train_loss -0.588
2024-12-20 18:34:36.130955: val_loss -0.3323
2024-12-20 18:34:36.131985: Pseudo dice [0.636]
2024-12-20 18:34:36.132843: Epoch time: 201.44 s
2024-12-20 18:34:36.133732: Yayy! New best EMA pseudo Dice: 0.6119
2024-12-20 18:34:37.959671: 
2024-12-20 18:34:37.961218: Epoch 17
2024-12-20 18:34:37.962198: Current learning rate: 0.00897
2024-12-20 18:38:17.053878: Validation loss did not improve from -0.37661. Patience: 3/50
2024-12-20 18:38:17.055617: train_loss -0.599
2024-12-20 18:38:17.056727: val_loss -0.3617
2024-12-20 18:38:17.057344: Pseudo dice [0.6595]
2024-12-20 18:38:17.057964: Epoch time: 219.1 s
2024-12-20 18:38:17.058537: Yayy! New best EMA pseudo Dice: 0.6167
2024-12-20 18:38:18.888709: 
2024-12-20 18:38:18.889839: Epoch 18
2024-12-20 18:38:18.890576: Current learning rate: 0.00891
2024-12-20 18:41:49.050120: Validation loss did not improve from -0.37661. Patience: 4/50
2024-12-20 18:41:49.051369: train_loss -0.6091
2024-12-20 18:41:49.052836: val_loss -0.364
2024-12-20 18:41:49.053550: Pseudo dice [0.6599]
2024-12-20 18:41:49.054572: Epoch time: 210.16 s
2024-12-20 18:41:49.055491: Yayy! New best EMA pseudo Dice: 0.621
2024-12-20 18:41:51.477703: 
2024-12-20 18:41:51.478747: Epoch 19
2024-12-20 18:41:51.479439: Current learning rate: 0.00885
2024-12-20 18:45:34.938606: Validation loss did not improve from -0.37661. Patience: 5/50
2024-12-20 18:45:34.939522: train_loss -0.6141
2024-12-20 18:45:34.940281: val_loss -0.374
2024-12-20 18:45:34.941039: Pseudo dice [0.6733]
2024-12-20 18:45:34.941893: Epoch time: 223.46 s
2024-12-20 18:45:35.328434: Yayy! New best EMA pseudo Dice: 0.6262
2024-12-20 18:45:37.260170: 
2024-12-20 18:45:37.261242: Epoch 20
2024-12-20 18:45:37.261958: Current learning rate: 0.00879
2024-12-20 18:49:15.117986: Validation loss improved from -0.37661 to -0.38366! Patience: 5/50
2024-12-20 18:49:15.118777: train_loss -0.6204
2024-12-20 18:49:15.119425: val_loss -0.3837
2024-12-20 18:49:15.120019: Pseudo dice [0.6701]
2024-12-20 18:49:15.120763: Epoch time: 217.86 s
2024-12-20 18:49:15.121345: Yayy! New best EMA pseudo Dice: 0.6306
2024-12-20 18:49:16.986885: 
2024-12-20 18:49:16.987816: Epoch 21
2024-12-20 18:49:16.988502: Current learning rate: 0.00873
2024-12-20 18:53:11.853356: Validation loss did not improve from -0.38366. Patience: 1/50
2024-12-20 18:53:11.854309: train_loss -0.627
2024-12-20 18:53:11.855134: val_loss -0.3346
2024-12-20 18:53:11.855750: Pseudo dice [0.6454]
2024-12-20 18:53:11.856503: Epoch time: 234.87 s
2024-12-20 18:53:11.857262: Yayy! New best EMA pseudo Dice: 0.6321
2024-12-20 18:53:13.611833: 
2024-12-20 18:53:13.612928: Epoch 22
2024-12-20 18:53:13.614008: Current learning rate: 0.00867
2024-12-20 18:56:53.832134: Validation loss improved from -0.38366 to -0.38974! Patience: 1/50
2024-12-20 18:56:53.833031: train_loss -0.6303
2024-12-20 18:56:53.833745: val_loss -0.3897
2024-12-20 18:56:53.834613: Pseudo dice [0.6806]
2024-12-20 18:56:53.835475: Epoch time: 220.22 s
2024-12-20 18:56:53.836295: Yayy! New best EMA pseudo Dice: 0.637
2024-12-20 18:56:55.697902: 
2024-12-20 18:56:55.699345: Epoch 23
2024-12-20 18:56:55.700208: Current learning rate: 0.00861
2024-12-20 19:00:25.598326: Validation loss did not improve from -0.38974. Patience: 1/50
2024-12-20 19:00:25.599294: train_loss -0.6352
2024-12-20 19:00:25.600075: val_loss -0.3637
2024-12-20 19:00:25.600955: Pseudo dice [0.6596]
2024-12-20 19:00:25.601692: Epoch time: 209.9 s
2024-12-20 19:00:25.602415: Yayy! New best EMA pseudo Dice: 0.6392
2024-12-20 19:00:27.386054: 
2024-12-20 19:00:27.387191: Epoch 24
2024-12-20 19:00:27.388025: Current learning rate: 0.00855
2024-12-20 19:03:43.570418: Validation loss did not improve from -0.38974. Patience: 2/50
2024-12-20 19:03:43.571342: train_loss -0.6523
2024-12-20 19:03:43.572185: val_loss -0.377
2024-12-20 19:03:43.572930: Pseudo dice [0.6702]
2024-12-20 19:03:43.573605: Epoch time: 196.19 s
2024-12-20 19:03:43.974802: Yayy! New best EMA pseudo Dice: 0.6423
2024-12-20 19:03:45.736421: 
2024-12-20 19:03:45.737392: Epoch 25
2024-12-20 19:03:45.738247: Current learning rate: 0.00849
2024-12-20 19:07:00.887282: Validation loss did not improve from -0.38974. Patience: 3/50
2024-12-20 19:07:00.888090: train_loss -0.6509
2024-12-20 19:07:00.888840: val_loss -0.3394
2024-12-20 19:07:00.889566: Pseudo dice [0.6436]
2024-12-20 19:07:00.890383: Epoch time: 195.15 s
2024-12-20 19:07:00.891123: Yayy! New best EMA pseudo Dice: 0.6424
2024-12-20 19:07:02.679513: 
2024-12-20 19:07:02.680910: Epoch 26
2024-12-20 19:07:02.681676: Current learning rate: 0.00843
2024-12-20 19:10:30.904014: Validation loss did not improve from -0.38974. Patience: 4/50
2024-12-20 19:10:30.905046: train_loss -0.6569
2024-12-20 19:10:30.905772: val_loss -0.3678
2024-12-20 19:10:30.906463: Pseudo dice [0.661]
2024-12-20 19:10:30.907123: Epoch time: 208.23 s
2024-12-20 19:10:30.907861: Yayy! New best EMA pseudo Dice: 0.6443
2024-12-20 19:10:32.733011: 
2024-12-20 19:10:32.734061: Epoch 27
2024-12-20 19:10:32.734773: Current learning rate: 0.00836
2024-12-20 19:14:06.390229: Validation loss improved from -0.38974 to -0.40396! Patience: 4/50
2024-12-20 19:14:06.391111: train_loss -0.6585
2024-12-20 19:14:06.392086: val_loss -0.404
2024-12-20 19:14:06.392966: Pseudo dice [0.6845]
2024-12-20 19:14:06.393805: Epoch time: 213.66 s
2024-12-20 19:14:06.394569: Yayy! New best EMA pseudo Dice: 0.6483
2024-12-20 19:14:08.175761: 
2024-12-20 19:14:08.177287: Epoch 28
2024-12-20 19:14:08.182552: Current learning rate: 0.0083
2024-12-20 19:17:48.416675: Validation loss did not improve from -0.40396. Patience: 1/50
2024-12-20 19:17:48.417537: train_loss -0.6647
2024-12-20 19:17:48.418352: val_loss -0.3556
2024-12-20 19:17:48.419123: Pseudo dice [0.6661]
2024-12-20 19:17:48.419974: Epoch time: 220.24 s
2024-12-20 19:17:48.420780: Yayy! New best EMA pseudo Dice: 0.6501
2024-12-20 19:17:50.183291: 
2024-12-20 19:17:50.184480: Epoch 29
2024-12-20 19:17:50.185236: Current learning rate: 0.00824
2024-12-20 19:21:25.816655: Validation loss did not improve from -0.40396. Patience: 2/50
2024-12-20 19:21:25.817493: train_loss -0.6786
2024-12-20 19:21:25.818239: val_loss -0.3788
2024-12-20 19:21:25.819128: Pseudo dice [0.6713]
2024-12-20 19:21:25.819869: Epoch time: 215.64 s
2024-12-20 19:21:26.633450: Yayy! New best EMA pseudo Dice: 0.6522
2024-12-20 19:21:28.462908: 
2024-12-20 19:21:28.464196: Epoch 30
2024-12-20 19:21:28.465226: Current learning rate: 0.00818
2024-12-20 19:25:10.231683: Validation loss improved from -0.40396 to -0.40590! Patience: 2/50
2024-12-20 19:25:10.232623: train_loss -0.6791
2024-12-20 19:25:10.233476: val_loss -0.4059
2024-12-20 19:25:10.234166: Pseudo dice [0.6889]
2024-12-20 19:25:10.234758: Epoch time: 221.77 s
2024-12-20 19:25:10.235411: Yayy! New best EMA pseudo Dice: 0.6559
2024-12-20 19:25:12.078432: 
2024-12-20 19:25:12.079574: Epoch 31
2024-12-20 19:25:12.080190: Current learning rate: 0.00812
2024-12-20 19:28:55.356065: Validation loss did not improve from -0.40590. Patience: 1/50
2024-12-20 19:28:55.357013: train_loss -0.6868
2024-12-20 19:28:55.357773: val_loss -0.3813
2024-12-20 19:28:55.369515: Pseudo dice [0.6728]
2024-12-20 19:28:55.370275: Epoch time: 223.28 s
2024-12-20 19:28:55.378430: Yayy! New best EMA pseudo Dice: 0.6576
2024-12-20 19:28:57.216272: 
2024-12-20 19:28:57.217392: Epoch 32
2024-12-20 19:28:57.218231: Current learning rate: 0.00806
2024-12-20 19:32:33.799260: Validation loss did not improve from -0.40590. Patience: 2/50
2024-12-20 19:32:33.800130: train_loss -0.692
2024-12-20 19:32:33.800792: val_loss -0.354
2024-12-20 19:32:33.801472: Pseudo dice [0.664]
2024-12-20 19:32:33.802161: Epoch time: 216.59 s
2024-12-20 19:32:33.802731: Yayy! New best EMA pseudo Dice: 0.6582
2024-12-20 19:32:35.619662: 
2024-12-20 19:32:35.620947: Epoch 33
2024-12-20 19:32:35.621671: Current learning rate: 0.008
2024-12-20 19:35:42.946754: Validation loss did not improve from -0.40590. Patience: 3/50
2024-12-20 19:35:42.947706: train_loss -0.6909
2024-12-20 19:35:42.948480: val_loss -0.3543
2024-12-20 19:35:42.949208: Pseudo dice [0.6567]
2024-12-20 19:35:42.949967: Epoch time: 187.33 s
2024-12-20 19:35:44.391281: 
2024-12-20 19:35:44.392254: Epoch 34
2024-12-20 19:35:44.393047: Current learning rate: 0.00793
2024-12-20 19:39:10.461431: Validation loss did not improve from -0.40590. Patience: 4/50
2024-12-20 19:39:10.462358: train_loss -0.6961
2024-12-20 19:39:10.463386: val_loss -0.3944
2024-12-20 19:39:10.464314: Pseudo dice [0.6828]
2024-12-20 19:39:10.465160: Epoch time: 206.07 s
2024-12-20 19:39:10.958226: Yayy! New best EMA pseudo Dice: 0.6605
2024-12-20 19:39:12.799358: 
2024-12-20 19:39:12.800552: Epoch 35
2024-12-20 19:39:12.801369: Current learning rate: 0.00787
2024-12-20 19:42:43.821877: Validation loss did not improve from -0.40590. Patience: 5/50
2024-12-20 19:42:43.822927: train_loss -0.6948
2024-12-20 19:42:43.823704: val_loss -0.3902
2024-12-20 19:42:43.824367: Pseudo dice [0.672]
2024-12-20 19:42:43.824975: Epoch time: 211.02 s
2024-12-20 19:42:43.825563: Yayy! New best EMA pseudo Dice: 0.6617
2024-12-20 19:42:45.630988: 
2024-12-20 19:42:45.631835: Epoch 36
2024-12-20 19:42:45.632632: Current learning rate: 0.00781
2024-12-20 19:46:26.423851: Validation loss did not improve from -0.40590. Patience: 6/50
2024-12-20 19:46:26.424685: train_loss -0.6995
2024-12-20 19:46:26.425396: val_loss -0.3777
2024-12-20 19:46:26.425990: Pseudo dice [0.6703]
2024-12-20 19:46:26.426628: Epoch time: 220.79 s
2024-12-20 19:46:26.427362: Yayy! New best EMA pseudo Dice: 0.6625
2024-12-20 19:46:28.199060: 
2024-12-20 19:46:28.200270: Epoch 37
2024-12-20 19:46:28.201196: Current learning rate: 0.00775
2024-12-20 19:50:01.798492: Validation loss did not improve from -0.40590. Patience: 7/50
2024-12-20 19:50:01.799140: train_loss -0.7037
2024-12-20 19:50:01.799916: val_loss -0.4005
2024-12-20 19:50:01.800568: Pseudo dice [0.6925]
2024-12-20 19:50:01.801234: Epoch time: 213.6 s
2024-12-20 19:50:01.801872: Yayy! New best EMA pseudo Dice: 0.6655
2024-12-20 19:50:03.574801: 
2024-12-20 19:50:03.575930: Epoch 38
2024-12-20 19:50:03.576673: Current learning rate: 0.00769
2024-12-20 19:53:38.996835: Validation loss did not improve from -0.40590. Patience: 8/50
2024-12-20 19:53:38.997660: train_loss -0.7032
2024-12-20 19:53:38.998514: val_loss -0.3565
2024-12-20 19:53:38.999299: Pseudo dice [0.6721]
2024-12-20 19:53:39.000178: Epoch time: 215.42 s
2024-12-20 19:53:39.000968: Yayy! New best EMA pseudo Dice: 0.6662
2024-12-20 19:53:40.756754: 
2024-12-20 19:53:40.757901: Epoch 39
2024-12-20 19:53:40.758574: Current learning rate: 0.00763
2024-12-20 19:57:18.709520: Validation loss did not improve from -0.40590. Patience: 9/50
2024-12-20 19:57:18.710366: train_loss -0.7076
2024-12-20 19:57:18.711485: val_loss -0.3675
2024-12-20 19:57:18.712485: Pseudo dice [0.68]
2024-12-20 19:57:18.713507: Epoch time: 217.95 s
2024-12-20 19:57:19.092985: Yayy! New best EMA pseudo Dice: 0.6676
2024-12-20 19:57:21.217307: 
2024-12-20 19:57:21.218622: Epoch 40
2024-12-20 19:57:21.219602: Current learning rate: 0.00756
2024-12-20 20:00:58.606076: Validation loss did not improve from -0.40590. Patience: 10/50
2024-12-20 20:00:58.607143: train_loss -0.7204
2024-12-20 20:00:58.607872: val_loss -0.4041
2024-12-20 20:00:58.608629: Pseudo dice [0.6927]
2024-12-20 20:00:58.609308: Epoch time: 217.39 s
2024-12-20 20:00:58.610018: Yayy! New best EMA pseudo Dice: 0.6701
2024-12-20 20:01:00.464531: 
2024-12-20 20:01:00.465867: Epoch 41
2024-12-20 20:01:00.466884: Current learning rate: 0.0075
2024-12-20 20:04:04.375748: Validation loss did not improve from -0.40590. Patience: 11/50
2024-12-20 20:04:04.376719: train_loss -0.7218
2024-12-20 20:04:04.377512: val_loss -0.3382
2024-12-20 20:04:04.378217: Pseudo dice [0.6605]
2024-12-20 20:04:04.378867: Epoch time: 183.91 s
2024-12-20 20:04:05.741995: 
2024-12-20 20:04:05.742958: Epoch 42
2024-12-20 20:04:05.743660: Current learning rate: 0.00744
2024-12-20 20:07:35.636823: Validation loss did not improve from -0.40590. Patience: 12/50
2024-12-20 20:07:35.637569: train_loss -0.7221
2024-12-20 20:07:35.638293: val_loss -0.3827
2024-12-20 20:07:35.639003: Pseudo dice [0.6786]
2024-12-20 20:07:35.639731: Epoch time: 209.9 s
2024-12-20 20:07:37.045536: 
2024-12-20 20:07:37.046703: Epoch 43
2024-12-20 20:07:37.047486: Current learning rate: 0.00738
2024-12-20 20:11:15.583855: Validation loss did not improve from -0.40590. Patience: 13/50
2024-12-20 20:11:15.584520: train_loss -0.7241
2024-12-20 20:11:15.585266: val_loss -0.3779
2024-12-20 20:11:15.585953: Pseudo dice [0.6967]
2024-12-20 20:11:15.586519: Epoch time: 218.54 s
2024-12-20 20:11:15.587075: Yayy! New best EMA pseudo Dice: 0.6727
2024-12-20 20:11:17.344153: 
2024-12-20 20:11:17.345376: Epoch 44
2024-12-20 20:11:17.346220: Current learning rate: 0.00732
2024-12-20 20:15:01.252973: Validation loss did not improve from -0.40590. Patience: 14/50
2024-12-20 20:15:01.253696: train_loss -0.7299
2024-12-20 20:15:01.254428: val_loss -0.3455
2024-12-20 20:15:01.255069: Pseudo dice [0.6644]
2024-12-20 20:15:01.255800: Epoch time: 223.91 s
2024-12-20 20:15:03.013286: 
2024-12-20 20:15:03.014439: Epoch 45
2024-12-20 20:15:03.015214: Current learning rate: 0.00725
2024-12-20 20:18:38.003895: Validation loss improved from -0.40590 to -0.41973! Patience: 14/50
2024-12-20 20:18:38.004791: train_loss -0.7299
2024-12-20 20:18:38.005564: val_loss -0.4197
2024-12-20 20:18:38.006355: Pseudo dice [0.7041]
2024-12-20 20:18:38.007104: Epoch time: 214.99 s
2024-12-20 20:18:38.007951: Yayy! New best EMA pseudo Dice: 0.6751
2024-12-20 20:18:39.710889: 
2024-12-20 20:18:39.712154: Epoch 46
2024-12-20 20:18:39.712808: Current learning rate: 0.00719
2024-12-20 20:22:20.099584: Validation loss did not improve from -0.41973. Patience: 1/50
2024-12-20 20:22:20.100401: train_loss -0.7351
2024-12-20 20:22:20.101223: val_loss -0.3759
2024-12-20 20:22:20.101891: Pseudo dice [0.6776]
2024-12-20 20:22:20.102558: Epoch time: 220.39 s
2024-12-20 20:22:20.103107: Yayy! New best EMA pseudo Dice: 0.6754
2024-12-20 20:22:21.821157: 
2024-12-20 20:22:21.822443: Epoch 47
2024-12-20 20:22:21.823143: Current learning rate: 0.00713
2024-12-20 20:26:01.470875: Validation loss did not improve from -0.41973. Patience: 2/50
2024-12-20 20:26:01.471638: train_loss -0.7373
2024-12-20 20:26:01.472296: val_loss -0.3842
2024-12-20 20:26:01.472922: Pseudo dice [0.6802]
2024-12-20 20:26:01.473616: Epoch time: 219.65 s
2024-12-20 20:26:01.474262: Yayy! New best EMA pseudo Dice: 0.6759
2024-12-20 20:26:03.195043: 
2024-12-20 20:26:03.196422: Epoch 48
2024-12-20 20:26:03.197192: Current learning rate: 0.00707
2024-12-20 20:30:09.489459: Validation loss did not improve from -0.41973. Patience: 3/50
2024-12-20 20:30:09.490294: train_loss -0.7361
2024-12-20 20:30:09.491233: val_loss -0.3827
2024-12-20 20:30:09.491987: Pseudo dice [0.6899]
2024-12-20 20:30:09.492824: Epoch time: 246.3 s
2024-12-20 20:30:09.493477: Yayy! New best EMA pseudo Dice: 0.6773
2024-12-20 20:30:11.220409: 
2024-12-20 20:30:11.221641: Epoch 49
2024-12-20 20:30:11.222413: Current learning rate: 0.007
2024-12-20 20:33:51.000797: Validation loss did not improve from -0.41973. Patience: 4/50
2024-12-20 20:33:51.002051: train_loss -0.7331
2024-12-20 20:33:51.003057: val_loss -0.3564
2024-12-20 20:33:51.004009: Pseudo dice [0.6796]
2024-12-20 20:33:51.004787: Epoch time: 219.78 s
2024-12-20 20:33:51.383217: Yayy! New best EMA pseudo Dice: 0.6775
2024-12-20 20:33:53.455626: 
2024-12-20 20:33:53.456836: Epoch 50
2024-12-20 20:33:53.457646: Current learning rate: 0.00694
2024-12-20 20:37:48.892478: Validation loss did not improve from -0.41973. Patience: 5/50
2024-12-20 20:37:48.893340: train_loss -0.7407
2024-12-20 20:37:48.893969: val_loss -0.3757
2024-12-20 20:37:48.894583: Pseudo dice [0.6804]
2024-12-20 20:37:48.895134: Epoch time: 235.44 s
2024-12-20 20:37:48.895693: Yayy! New best EMA pseudo Dice: 0.6778
2024-12-20 20:37:50.613014: 
2024-12-20 20:37:50.614524: Epoch 51
2024-12-20 20:37:50.615266: Current learning rate: 0.00688
2024-12-20 20:41:40.319846: Validation loss did not improve from -0.41973. Patience: 6/50
2024-12-20 20:41:40.320750: train_loss -0.7404
2024-12-20 20:41:40.321417: val_loss -0.3167
2024-12-20 20:41:40.322223: Pseudo dice [0.653]
2024-12-20 20:41:40.322989: Epoch time: 229.71 s
2024-12-20 20:41:41.708410: 
2024-12-20 20:41:41.709327: Epoch 52
2024-12-20 20:41:41.709966: Current learning rate: 0.00682
2024-12-20 20:45:13.301692: Validation loss did not improve from -0.41973. Patience: 7/50
2024-12-20 20:45:13.305367: train_loss -0.741
2024-12-20 20:45:13.306674: val_loss -0.3192
2024-12-20 20:45:13.307278: Pseudo dice [0.6614]
2024-12-20 20:45:13.308269: Epoch time: 211.6 s
2024-12-20 20:45:14.700523: 
2024-12-20 20:45:14.701760: Epoch 53
2024-12-20 20:45:14.702490: Current learning rate: 0.00675
2024-12-20 20:48:17.600677: Validation loss did not improve from -0.41973. Patience: 8/50
2024-12-20 20:48:17.601577: train_loss -0.7485
2024-12-20 20:48:17.602283: val_loss -0.3882
2024-12-20 20:48:17.602915: Pseudo dice [0.6819]
2024-12-20 20:48:17.603629: Epoch time: 182.9 s
2024-12-20 20:48:18.996145: 
2024-12-20 20:48:18.997172: Epoch 54
2024-12-20 20:48:18.997936: Current learning rate: 0.00669
2024-12-20 20:51:47.485900: Validation loss did not improve from -0.41973. Patience: 9/50
2024-12-20 20:51:47.487128: train_loss -0.755
2024-12-20 20:51:47.487863: val_loss -0.3898
2024-12-20 20:51:47.488497: Pseudo dice [0.6961]
2024-12-20 20:51:47.489390: Epoch time: 208.49 s
2024-12-20 20:51:49.233695: 
2024-12-20 20:51:49.234942: Epoch 55
2024-12-20 20:51:49.235662: Current learning rate: 0.00663
2024-12-20 20:55:30.344660: Validation loss did not improve from -0.41973. Patience: 10/50
2024-12-20 20:55:30.345517: train_loss -0.7496
2024-12-20 20:55:30.346322: val_loss -0.3524
2024-12-20 20:55:30.347014: Pseudo dice [0.6776]
2024-12-20 20:55:30.347961: Epoch time: 221.11 s
2024-12-20 20:55:31.729288: 
2024-12-20 20:55:31.730490: Epoch 56
2024-12-20 20:55:31.731264: Current learning rate: 0.00657
2024-12-20 20:59:25.434632: Validation loss did not improve from -0.41973. Patience: 11/50
2024-12-20 20:59:25.435616: train_loss -0.7545
2024-12-20 20:59:25.436533: val_loss -0.3529
2024-12-20 20:59:25.437299: Pseudo dice [0.6797]
2024-12-20 20:59:25.438126: Epoch time: 233.71 s
2024-12-20 20:59:26.853293: 
2024-12-20 20:59:26.854594: Epoch 57
2024-12-20 20:59:26.855469: Current learning rate: 0.0065
2024-12-20 21:03:20.226866: Validation loss did not improve from -0.41973. Patience: 12/50
2024-12-20 21:03:20.227501: train_loss -0.7569
2024-12-20 21:03:20.228212: val_loss -0.3949
2024-12-20 21:03:20.228876: Pseudo dice [0.6988]
2024-12-20 21:03:20.229559: Epoch time: 233.38 s
2024-12-20 21:03:20.230181: Yayy! New best EMA pseudo Dice: 0.6794
2024-12-20 21:03:22.007217: 
2024-12-20 21:03:22.008426: Epoch 58
2024-12-20 21:03:22.009341: Current learning rate: 0.00644
2024-12-20 21:07:41.827298: Validation loss did not improve from -0.41973. Patience: 13/50
2024-12-20 21:07:41.828112: train_loss -0.7586
2024-12-20 21:07:41.828873: val_loss -0.304
2024-12-20 21:07:41.829522: Pseudo dice [0.6584]
2024-12-20 21:07:41.830129: Epoch time: 259.82 s
2024-12-20 21:07:43.245086: 
2024-12-20 21:07:43.246165: Epoch 59
2024-12-20 21:07:43.246843: Current learning rate: 0.00638
2024-12-20 21:11:28.639225: Validation loss did not improve from -0.41973. Patience: 14/50
2024-12-20 21:11:28.640155: train_loss -0.7598
2024-12-20 21:11:28.641031: val_loss -0.301
2024-12-20 21:11:28.641930: Pseudo dice [0.6518]
2024-12-20 21:11:28.642686: Epoch time: 225.4 s
2024-12-20 21:11:30.507308: 
2024-12-20 21:11:30.508579: Epoch 60
2024-12-20 21:11:30.509427: Current learning rate: 0.00631
2024-12-20 21:15:22.374037: Validation loss did not improve from -0.41973. Patience: 15/50
2024-12-20 21:15:22.375051: train_loss -0.7619
2024-12-20 21:15:22.375793: val_loss -0.3401
2024-12-20 21:15:22.376446: Pseudo dice [0.6807]
2024-12-20 21:15:22.377120: Epoch time: 231.87 s
2024-12-20 21:15:23.805585: 
2024-12-20 21:15:23.806689: Epoch 61
2024-12-20 21:15:23.807546: Current learning rate: 0.00625
2024-12-20 21:19:32.763301: Validation loss did not improve from -0.41973. Patience: 16/50
2024-12-20 21:19:32.764307: train_loss -0.7608
2024-12-20 21:19:32.765171: val_loss -0.3481
2024-12-20 21:19:32.765935: Pseudo dice [0.6663]
2024-12-20 21:19:32.766690: Epoch time: 248.96 s
2024-12-20 21:19:34.179412: 
2024-12-20 21:19:34.180727: Epoch 62
2024-12-20 21:19:34.181853: Current learning rate: 0.00619
2024-12-20 21:23:40.553477: Validation loss did not improve from -0.41973. Patience: 17/50
2024-12-20 21:23:40.554516: train_loss -0.7658
2024-12-20 21:23:40.555476: val_loss -0.357
2024-12-20 21:23:40.556355: Pseudo dice [0.6756]
2024-12-20 21:23:40.557105: Epoch time: 246.38 s
2024-12-20 21:23:42.061512: 
2024-12-20 21:23:42.062600: Epoch 63
2024-12-20 21:23:42.063288: Current learning rate: 0.00612
2024-12-20 21:27:39.680407: Validation loss did not improve from -0.41973. Patience: 18/50
2024-12-20 21:27:39.681424: train_loss -0.7641
2024-12-20 21:27:39.682237: val_loss -0.3688
2024-12-20 21:27:39.683002: Pseudo dice [0.6678]
2024-12-20 21:27:39.683631: Epoch time: 237.62 s
2024-12-20 21:27:41.112248: 
2024-12-20 21:27:41.113427: Epoch 64
2024-12-20 21:27:41.114128: Current learning rate: 0.00606
2024-12-20 21:31:43.277506: Validation loss did not improve from -0.41973. Patience: 19/50
2024-12-20 21:31:43.278358: train_loss -0.7632
2024-12-20 21:31:43.279201: val_loss -0.363
2024-12-20 21:31:43.279841: Pseudo dice [0.6748]
2024-12-20 21:31:43.280630: Epoch time: 242.17 s
2024-12-20 21:31:45.080433: 
2024-12-20 21:31:45.081537: Epoch 65
2024-12-20 21:31:45.082161: Current learning rate: 0.006
2024-12-20 21:35:58.466071: Validation loss did not improve from -0.41973. Patience: 20/50
2024-12-20 21:35:58.467056: train_loss -0.7718
2024-12-20 21:35:58.467833: val_loss -0.2882
2024-12-20 21:35:58.468559: Pseudo dice [0.6455]
2024-12-20 21:35:58.469160: Epoch time: 253.39 s
2024-12-20 21:35:59.902463: 
2024-12-20 21:35:59.903497: Epoch 66
2024-12-20 21:35:59.904147: Current learning rate: 0.00593
2024-12-20 21:39:58.214565: Validation loss did not improve from -0.41973. Patience: 21/50
2024-12-20 21:39:58.215422: train_loss -0.7719
2024-12-20 21:39:58.216166: val_loss -0.3731
2024-12-20 21:39:58.216805: Pseudo dice [0.6805]
2024-12-20 21:39:58.217445: Epoch time: 238.31 s
2024-12-20 21:39:59.605346: 
2024-12-20 21:39:59.606754: Epoch 67
2024-12-20 21:39:59.607599: Current learning rate: 0.00587
2024-12-20 21:43:49.290398: Validation loss did not improve from -0.41973. Patience: 22/50
2024-12-20 21:43:49.291109: train_loss -0.7695
2024-12-20 21:43:49.291942: val_loss -0.3342
2024-12-20 21:43:49.292722: Pseudo dice [0.665]
2024-12-20 21:43:49.293598: Epoch time: 229.69 s
2024-12-20 21:43:50.691494: 
2024-12-20 21:43:50.692713: Epoch 68
2024-12-20 21:43:50.693568: Current learning rate: 0.00581
2024-12-20 21:47:58.303059: Validation loss did not improve from -0.41973. Patience: 23/50
2024-12-20 21:47:58.303923: train_loss -0.7723
2024-12-20 21:47:58.304852: val_loss -0.3809
2024-12-20 21:47:58.305490: Pseudo dice [0.7004]
2024-12-20 21:47:58.306199: Epoch time: 247.61 s
2024-12-20 21:47:59.737706: 
2024-12-20 21:47:59.738785: Epoch 69
2024-12-20 21:47:59.739578: Current learning rate: 0.00574
2024-12-20 21:51:45.161243: Validation loss did not improve from -0.41973. Patience: 24/50
2024-12-20 21:51:45.165013: train_loss -0.7691
2024-12-20 21:51:45.165960: val_loss -0.3705
2024-12-20 21:51:45.166699: Pseudo dice [0.6881]
2024-12-20 21:51:45.167632: Epoch time: 225.43 s
2024-12-20 21:51:47.106675: 
2024-12-20 21:51:47.107569: Epoch 70
2024-12-20 21:51:47.108256: Current learning rate: 0.00568
2024-12-20 21:54:51.713764: Validation loss did not improve from -0.41973. Patience: 25/50
2024-12-20 21:54:51.714993: train_loss -0.7731
2024-12-20 21:54:51.716757: val_loss -0.3469
2024-12-20 21:54:51.717630: Pseudo dice [0.6619]
2024-12-20 21:54:51.718664: Epoch time: 184.61 s
2024-12-20 21:54:53.227328: 
2024-12-20 21:54:53.228399: Epoch 71
2024-12-20 21:54:53.229127: Current learning rate: 0.00562
2024-12-20 21:58:41.532772: Validation loss did not improve from -0.41973. Patience: 26/50
2024-12-20 21:58:41.533719: train_loss -0.7799
2024-12-20 21:58:41.534547: val_loss -0.3621
2024-12-20 21:58:41.535292: Pseudo dice [0.6844]
2024-12-20 21:58:41.535959: Epoch time: 228.31 s
2024-12-20 21:58:43.680166: 
2024-12-20 21:58:43.681440: Epoch 72
2024-12-20 21:58:43.682231: Current learning rate: 0.00555
2024-12-20 22:02:14.930977: Validation loss did not improve from -0.41973. Patience: 27/50
2024-12-20 22:02:14.931929: train_loss -0.7769
2024-12-20 22:02:14.932991: val_loss -0.3398
2024-12-20 22:02:14.933906: Pseudo dice [0.6726]
2024-12-20 22:02:14.935090: Epoch time: 211.25 s
2024-12-20 22:02:16.543808: 
2024-12-20 22:02:16.545250: Epoch 73
2024-12-20 22:02:16.546363: Current learning rate: 0.00549
2024-12-20 22:05:59.476202: Validation loss did not improve from -0.41973. Patience: 28/50
2024-12-20 22:05:59.477273: train_loss -0.7794
2024-12-20 22:05:59.477982: val_loss -0.3418
2024-12-20 22:05:59.478646: Pseudo dice [0.6702]
2024-12-20 22:05:59.479308: Epoch time: 222.93 s
2024-12-20 22:06:00.994344: 
2024-12-20 22:06:00.995401: Epoch 74
2024-12-20 22:06:00.996120: Current learning rate: 0.00542
2024-12-20 22:09:48.778564: Validation loss did not improve from -0.41973. Patience: 29/50
2024-12-20 22:09:48.779454: train_loss -0.7821
2024-12-20 22:09:48.780195: val_loss -0.3237
2024-12-20 22:09:48.780819: Pseudo dice [0.6617]
2024-12-20 22:09:48.781462: Epoch time: 227.79 s
2024-12-20 22:09:50.715585: 
2024-12-20 22:09:50.717045: Epoch 75
2024-12-20 22:09:50.718194: Current learning rate: 0.00536
2024-12-20 22:13:27.802652: Validation loss did not improve from -0.41973. Patience: 30/50
2024-12-20 22:13:27.803664: train_loss -0.7844
2024-12-20 22:13:27.804478: val_loss -0.3457
2024-12-20 22:13:27.805134: Pseudo dice [0.681]
2024-12-20 22:13:27.805840: Epoch time: 217.09 s
2024-12-20 22:13:29.322005: 
2024-12-20 22:13:29.327150: Epoch 76
2024-12-20 22:13:29.327862: Current learning rate: 0.00529
2024-12-20 22:17:14.481993: Validation loss did not improve from -0.41973. Patience: 31/50
2024-12-20 22:17:14.483488: train_loss -0.7824
2024-12-20 22:17:14.484189: val_loss -0.3812
2024-12-20 22:17:14.484851: Pseudo dice [0.6862]
2024-12-20 22:17:14.485521: Epoch time: 225.16 s
2024-12-20 22:17:16.031107: 
2024-12-20 22:17:16.032313: Epoch 77
2024-12-20 22:17:16.033038: Current learning rate: 0.00523
2024-12-20 22:21:06.541739: Validation loss did not improve from -0.41973. Patience: 32/50
2024-12-20 22:21:06.542395: train_loss -0.7864
2024-12-20 22:21:06.543195: val_loss -0.346
2024-12-20 22:21:06.543862: Pseudo dice [0.6753]
2024-12-20 22:21:06.544591: Epoch time: 230.51 s
2024-12-20 22:21:08.000492: 
2024-12-20 22:21:08.001938: Epoch 78
2024-12-20 22:21:08.002813: Current learning rate: 0.00517
2024-12-20 22:24:46.314811: Validation loss did not improve from -0.41973. Patience: 33/50
2024-12-20 22:24:46.316452: train_loss -0.7839
2024-12-20 22:24:46.317258: val_loss -0.387
2024-12-20 22:24:46.317878: Pseudo dice [0.6927]
2024-12-20 22:24:46.318532: Epoch time: 218.32 s
2024-12-20 22:24:47.763696: 
2024-12-20 22:24:47.764703: Epoch 79
2024-12-20 22:24:47.765367: Current learning rate: 0.0051
2024-12-20 22:28:01.693684: Validation loss did not improve from -0.41973. Patience: 34/50
2024-12-20 22:28:01.694558: train_loss -0.7868
2024-12-20 22:28:01.695244: val_loss -0.3864
2024-12-20 22:28:01.695852: Pseudo dice [0.6934]
2024-12-20 22:28:01.696467: Epoch time: 193.93 s
2024-12-20 22:28:03.515686: 
2024-12-20 22:28:03.516862: Epoch 80
2024-12-20 22:28:03.517555: Current learning rate: 0.00504
2024-12-20 22:31:27.783162: Validation loss did not improve from -0.41973. Patience: 35/50
2024-12-20 22:31:27.784062: train_loss -0.7847
2024-12-20 22:31:27.784970: val_loss -0.3614
2024-12-20 22:31:27.785593: Pseudo dice [0.6906]
2024-12-20 22:31:27.786299: Epoch time: 204.27 s
2024-12-20 22:31:27.786946: Yayy! New best EMA pseudo Dice: 0.6798
2024-12-20 22:31:29.668405: 
2024-12-20 22:31:29.669259: Epoch 81
2024-12-20 22:31:29.669956: Current learning rate: 0.00497
2024-12-20 22:35:11.304037: Validation loss did not improve from -0.41973. Patience: 36/50
2024-12-20 22:35:11.304960: train_loss -0.7895
2024-12-20 22:35:11.305739: val_loss -0.3542
2024-12-20 22:35:11.306427: Pseudo dice [0.6905]
2024-12-20 22:35:11.307141: Epoch time: 221.64 s
2024-12-20 22:35:11.307843: Yayy! New best EMA pseudo Dice: 0.6809
2024-12-20 22:35:13.281405: 
2024-12-20 22:35:13.282439: Epoch 82
2024-12-20 22:35:13.283113: Current learning rate: 0.00491
2024-12-20 22:38:58.929662: Validation loss did not improve from -0.41973. Patience: 37/50
2024-12-20 22:38:58.930607: train_loss -0.7891
2024-12-20 22:38:58.931553: val_loss -0.3382
2024-12-20 22:38:58.932322: Pseudo dice [0.6762]
2024-12-20 22:38:58.932956: Epoch time: 225.65 s
2024-12-20 22:39:00.729707: 
2024-12-20 22:39:00.730934: Epoch 83
2024-12-20 22:39:00.731607: Current learning rate: 0.00484
2024-12-20 22:42:48.102398: Validation loss did not improve from -0.41973. Patience: 38/50
2024-12-20 22:42:48.103300: train_loss -0.7911
2024-12-20 22:42:48.104289: val_loss -0.3689
2024-12-20 22:42:48.104932: Pseudo dice [0.69]
2024-12-20 22:42:48.105624: Epoch time: 227.37 s
2024-12-20 22:42:48.106297: Yayy! New best EMA pseudo Dice: 0.6814
2024-12-20 22:42:49.976301: 
2024-12-20 22:42:49.977719: Epoch 84
2024-12-20 22:42:49.978398: Current learning rate: 0.00478
2024-12-20 22:46:41.825702: Validation loss did not improve from -0.41973. Patience: 39/50
2024-12-20 22:46:41.827661: train_loss -0.7939
2024-12-20 22:46:41.828667: val_loss -0.3708
2024-12-20 22:46:41.829361: Pseudo dice [0.6921]
2024-12-20 22:46:41.830192: Epoch time: 231.85 s
2024-12-20 22:46:42.315770: Yayy! New best EMA pseudo Dice: 0.6824
2024-12-20 22:46:44.334770: 
2024-12-20 22:46:44.336101: Epoch 85
2024-12-20 22:46:44.336909: Current learning rate: 0.00471
2024-12-20 22:50:33.648499: Validation loss did not improve from -0.41973. Patience: 40/50
2024-12-20 22:50:33.649461: train_loss -0.7931
2024-12-20 22:50:33.650194: val_loss -0.3318
2024-12-20 22:50:33.650874: Pseudo dice [0.6757]
2024-12-20 22:50:33.651564: Epoch time: 229.32 s
2024-12-20 22:50:35.120558: 
2024-12-20 22:50:35.121563: Epoch 86
2024-12-20 22:50:35.122360: Current learning rate: 0.00465
2024-12-20 22:54:28.014347: Validation loss did not improve from -0.41973. Patience: 41/50
2024-12-20 22:54:28.015236: train_loss -0.7941
2024-12-20 22:54:28.016297: val_loss -0.3551
2024-12-20 22:54:28.017256: Pseudo dice [0.6789]
2024-12-20 22:54:28.018168: Epoch time: 232.9 s
2024-12-20 22:54:29.505304: 
2024-12-20 22:54:29.506214: Epoch 87
2024-12-20 22:54:29.506965: Current learning rate: 0.00458
2024-12-20 22:58:21.320369: Validation loss did not improve from -0.41973. Patience: 42/50
2024-12-20 22:58:21.321439: train_loss -0.7976
2024-12-20 22:58:21.322454: val_loss -0.3685
2024-12-20 22:58:21.323337: Pseudo dice [0.6944]
2024-12-20 22:58:21.324348: Epoch time: 231.82 s
2024-12-20 22:58:21.325376: Yayy! New best EMA pseudo Dice: 0.6828
2024-12-20 22:58:23.288636: 
2024-12-20 22:58:23.289906: Epoch 88
2024-12-20 22:58:23.290689: Current learning rate: 0.00452
2024-12-20 23:02:15.950484: Validation loss did not improve from -0.41973. Patience: 43/50
2024-12-20 23:02:15.951535: train_loss -0.7925
2024-12-20 23:02:15.952549: val_loss -0.3586
2024-12-20 23:02:15.953545: Pseudo dice [0.6867]
2024-12-20 23:02:15.954371: Epoch time: 232.66 s
2024-12-20 23:02:15.955186: Yayy! New best EMA pseudo Dice: 0.6832
2024-12-20 23:02:17.871914: 
2024-12-20 23:02:17.873063: Epoch 89
2024-12-20 23:02:17.873796: Current learning rate: 0.00445
2024-12-20 23:06:08.498683: Validation loss did not improve from -0.41973. Patience: 44/50
2024-12-20 23:06:08.500912: train_loss -0.7945
2024-12-20 23:06:08.501648: val_loss -0.3526
2024-12-20 23:06:08.502230: Pseudo dice [0.6879]
2024-12-20 23:06:08.503057: Epoch time: 230.63 s
2024-12-20 23:06:08.912903: Yayy! New best EMA pseudo Dice: 0.6836
2024-12-20 23:06:10.771350: 
2024-12-20 23:06:10.772617: Epoch 90
2024-12-20 23:06:10.773323: Current learning rate: 0.00438
2024-12-20 23:10:13.418678: Validation loss did not improve from -0.41973. Patience: 45/50
2024-12-20 23:10:13.419560: train_loss -0.7945
2024-12-20 23:10:13.420502: val_loss -0.3752
2024-12-20 23:10:13.421279: Pseudo dice [0.6734]
2024-12-20 23:10:13.422276: Epoch time: 242.65 s
2024-12-20 23:10:14.823902: 
2024-12-20 23:10:14.825191: Epoch 91
2024-12-20 23:10:14.826061: Current learning rate: 0.00432
2024-12-20 23:14:10.260452: Validation loss did not improve from -0.41973. Patience: 46/50
2024-12-20 23:14:10.261288: train_loss -0.7993
2024-12-20 23:14:10.262329: val_loss -0.3268
2024-12-20 23:14:10.263346: Pseudo dice [0.669]
2024-12-20 23:14:10.264654: Epoch time: 235.44 s
2024-12-20 23:14:11.686011: 
2024-12-20 23:14:11.687251: Epoch 92
2024-12-20 23:14:11.688326: Current learning rate: 0.00425
2024-12-20 23:17:54.269835: Validation loss did not improve from -0.41973. Patience: 47/50
2024-12-20 23:17:54.270775: train_loss -0.7993
2024-12-20 23:17:54.271672: val_loss -0.3458
2024-12-20 23:17:54.272301: Pseudo dice [0.6805]
2024-12-20 23:17:54.272906: Epoch time: 222.59 s
2024-12-20 23:17:55.626689: 
2024-12-20 23:17:55.657990: Epoch 93
2024-12-20 23:17:55.659060: Current learning rate: 0.00419
2024-12-20 23:21:45.990751: Validation loss did not improve from -0.41973. Patience: 48/50
2024-12-20 23:21:45.991746: train_loss -0.7986
2024-12-20 23:21:45.992617: val_loss -0.3619
2024-12-20 23:21:45.993487: Pseudo dice [0.6967]
2024-12-20 23:21:45.994361: Epoch time: 230.37 s
2024-12-20 23:21:47.827137: 
2024-12-20 23:21:47.828306: Epoch 94
2024-12-20 23:21:47.829223: Current learning rate: 0.00412
2024-12-20 23:25:42.721204: Validation loss did not improve from -0.41973. Patience: 49/50
2024-12-20 23:25:42.721866: train_loss -0.8012
2024-12-20 23:25:42.722660: val_loss -0.2828
2024-12-20 23:25:42.723315: Pseudo dice [0.6641]
2024-12-20 23:25:42.723963: Epoch time: 234.9 s
2024-12-20 23:25:44.468109: 
2024-12-20 23:25:44.468923: Epoch 95
2024-12-20 23:25:44.469624: Current learning rate: 0.00405
2024-12-20 23:29:08.315224: Validation loss did not improve from -0.41973. Patience: 50/50
2024-12-20 23:29:08.316030: train_loss -0.8022
2024-12-20 23:29:08.316774: val_loss -0.3316
2024-12-20 23:29:08.317458: Pseudo dice [0.6804]
2024-12-20 23:29:08.318165: Epoch time: 203.85 s
2024-12-20 23:29:09.671324: 
2024-12-20 23:29:09.672414: Epoch 96
2024-12-20 23:29:09.673145: Current learning rate: 0.00399
2024-12-20 23:32:35.569937: Validation loss did not improve from -0.41973. Patience: 51/50
2024-12-20 23:32:35.570943: train_loss -0.8025
2024-12-20 23:32:35.571925: val_loss -0.3597
2024-12-20 23:32:35.572721: Pseudo dice [0.6957]
2024-12-20 23:32:35.573484: Epoch time: 205.9 s
2024-12-20 23:32:36.967187: 
2024-12-20 23:32:36.968671: Epoch 97
2024-12-20 23:32:36.969655: Current learning rate: 0.00392
2024-12-20 23:36:05.655021: Validation loss did not improve from -0.41973. Patience: 52/50
2024-12-20 23:36:05.655678: train_loss -0.8039
2024-12-20 23:36:05.656403: val_loss -0.3772
2024-12-20 23:36:05.657035: Pseudo dice [0.7003]
2024-12-20 23:36:05.657596: Epoch time: 208.69 s
2024-12-20 23:36:05.658210: Yayy! New best EMA pseudo Dice: 0.6841
2024-12-20 23:36:07.450283: 
2024-12-20 23:36:07.451383: Epoch 98
2024-12-20 23:36:07.452047: Current learning rate: 0.00385
2024-12-20 23:39:46.686582: Validation loss did not improve from -0.41973. Patience: 53/50
2024-12-20 23:39:46.687642: train_loss -0.8016
2024-12-20 23:39:46.688554: val_loss -0.3556
2024-12-20 23:39:46.689283: Pseudo dice [0.685]
2024-12-20 23:39:46.690087: Epoch time: 219.24 s
2024-12-20 23:39:46.690727: Yayy! New best EMA pseudo Dice: 0.6842
2024-12-20 23:39:48.475140: 
2024-12-20 23:39:48.476403: Epoch 99
2024-12-20 23:39:48.477140: Current learning rate: 0.00379
2024-12-20 23:43:16.194301: Validation loss did not improve from -0.41973. Patience: 54/50
2024-12-20 23:43:16.194967: train_loss -0.8064
2024-12-20 23:43:16.195786: val_loss -0.3748
2024-12-20 23:43:16.196543: Pseudo dice [0.6862]
2024-12-20 23:43:16.197439: Epoch time: 207.72 s
2024-12-20 23:43:16.581665: Yayy! New best EMA pseudo Dice: 0.6844
2024-12-20 23:43:18.337684: 
2024-12-20 23:43:18.338936: Epoch 100
2024-12-20 23:43:18.339795: Current learning rate: 0.00372
2024-12-20 23:46:48.995830: Validation loss did not improve from -0.41973. Patience: 55/50
2024-12-20 23:46:48.996638: train_loss -0.8061
2024-12-20 23:46:48.997533: val_loss -0.3313
2024-12-20 23:46:48.998299: Pseudo dice [0.6818]
2024-12-20 23:46:48.999124: Epoch time: 210.66 s
2024-12-20 23:46:50.384313: 
2024-12-20 23:46:50.385524: Epoch 101
2024-12-20 23:46:50.386459: Current learning rate: 0.00365
2024-12-20 23:50:26.932756: Validation loss did not improve from -0.41973. Patience: 56/50
2024-12-20 23:50:26.933611: train_loss -0.8092
2024-12-20 23:50:26.934437: val_loss -0.3412
2024-12-20 23:50:26.935246: Pseudo dice [0.6754]
2024-12-20 23:50:26.935898: Epoch time: 216.55 s
2024-12-20 23:50:28.434734: 
2024-12-20 23:50:28.435888: Epoch 102
2024-12-20 23:50:28.436773: Current learning rate: 0.00359
2024-12-20 23:54:11.264322: Validation loss did not improve from -0.41973. Patience: 57/50
2024-12-20 23:54:11.265307: train_loss -0.8074
2024-12-20 23:54:11.266164: val_loss -0.3408
2024-12-20 23:54:11.266796: Pseudo dice [0.6809]
2024-12-20 23:54:11.267430: Epoch time: 222.83 s
2024-12-20 23:54:12.622972: 
2024-12-20 23:54:12.624119: Epoch 103
2024-12-20 23:54:12.624821: Current learning rate: 0.00352
2024-12-20 23:57:46.949649: Validation loss did not improve from -0.41973. Patience: 58/50
2024-12-20 23:57:46.950451: train_loss -0.8106
2024-12-20 23:57:46.951286: val_loss -0.3277
2024-12-20 23:57:46.952072: Pseudo dice [0.6803]
2024-12-20 23:57:46.952802: Epoch time: 214.33 s
2024-12-20 23:57:48.692274: 
2024-12-20 23:57:48.693328: Epoch 104
2024-12-20 23:57:48.694128: Current learning rate: 0.00345
2024-12-21 00:01:33.168385: Validation loss did not improve from -0.41973. Patience: 59/50
2024-12-21 00:01:33.169261: train_loss -0.809
2024-12-21 00:01:33.170047: val_loss -0.3511
2024-12-21 00:01:33.170933: Pseudo dice [0.6894]
2024-12-21 00:01:33.171701: Epoch time: 224.48 s
2024-12-21 00:01:34.953178: 
2024-12-21 00:01:34.954091: Epoch 105
2024-12-21 00:01:34.955045: Current learning rate: 0.00338
2024-12-21 00:05:12.811133: Validation loss did not improve from -0.41973. Patience: 60/50
2024-12-21 00:05:12.813586: train_loss -0.811
2024-12-21 00:05:12.815231: val_loss -0.3817
2024-12-21 00:05:12.816090: Pseudo dice [0.69]
2024-12-21 00:05:12.817274: Epoch time: 217.86 s
2024-12-21 00:05:14.288786: 
2024-12-21 00:05:14.290382: Epoch 106
2024-12-21 00:05:14.291457: Current learning rate: 0.00332
2024-12-21 00:08:48.727930: Validation loss did not improve from -0.41973. Patience: 61/50
2024-12-21 00:08:48.728605: train_loss -0.8144
2024-12-21 00:08:48.729335: val_loss -0.32
2024-12-21 00:08:48.730016: Pseudo dice [0.6766]
2024-12-21 00:08:48.730733: Epoch time: 214.44 s
2024-12-21 00:08:50.260135: 
2024-12-21 00:08:50.261163: Epoch 107
2024-12-21 00:08:50.261842: Current learning rate: 0.00325
2024-12-21 00:12:30.041837: Validation loss did not improve from -0.41973. Patience: 62/50
2024-12-21 00:12:30.042746: train_loss -0.8126
2024-12-21 00:12:30.043667: val_loss -0.367
2024-12-21 00:12:30.044349: Pseudo dice [0.7013]
2024-12-21 00:12:30.045186: Epoch time: 219.78 s
2024-12-21 00:12:30.045975: Yayy! New best EMA pseudo Dice: 0.6851
2024-12-21 00:12:31.905206: 
2024-12-21 00:12:31.906130: Epoch 108
2024-12-21 00:12:31.906916: Current learning rate: 0.00318
2024-12-21 00:16:24.030435: Validation loss did not improve from -0.41973. Patience: 63/50
2024-12-21 00:16:24.031247: train_loss -0.8115
2024-12-21 00:16:24.032072: val_loss -0.3584
2024-12-21 00:16:24.032704: Pseudo dice [0.6952]
2024-12-21 00:16:24.033471: Epoch time: 232.13 s
2024-12-21 00:16:24.034056: Yayy! New best EMA pseudo Dice: 0.6861
2024-12-21 00:16:25.834917: 
2024-12-21 00:16:25.836174: Epoch 109
2024-12-21 00:16:25.836908: Current learning rate: 0.00311
2024-12-21 00:20:13.504252: Validation loss did not improve from -0.41973. Patience: 64/50
2024-12-21 00:20:13.505222: train_loss -0.8142
2024-12-21 00:20:13.506163: val_loss -0.35
2024-12-21 00:20:13.507121: Pseudo dice [0.6885]
2024-12-21 00:20:13.508075: Epoch time: 227.67 s
2024-12-21 00:20:13.937923: Yayy! New best EMA pseudo Dice: 0.6864
2024-12-21 00:20:15.797124: 
2024-12-21 00:20:15.798627: Epoch 110
2024-12-21 00:20:15.799691: Current learning rate: 0.00304
2024-12-21 00:23:44.006934: Validation loss did not improve from -0.41973. Patience: 65/50
2024-12-21 00:23:44.007901: train_loss -0.8144
2024-12-21 00:23:44.008734: val_loss -0.3449
2024-12-21 00:23:44.009496: Pseudo dice [0.6849]
2024-12-21 00:23:44.010204: Epoch time: 208.21 s
2024-12-21 00:23:45.457769: 
2024-12-21 00:23:45.459028: Epoch 111
2024-12-21 00:23:45.459751: Current learning rate: 0.00297
2024-12-21 00:27:03.170217: Validation loss did not improve from -0.41973. Patience: 66/50
2024-12-21 00:27:03.171152: train_loss -0.8143
2024-12-21 00:27:03.172022: val_loss -0.2935
2024-12-21 00:27:03.172715: Pseudo dice [0.6753]
2024-12-21 00:27:03.173459: Epoch time: 197.71 s
2024-12-21 00:27:04.673903: 
2024-12-21 00:27:04.675098: Epoch 112
2024-12-21 00:27:04.675860: Current learning rate: 0.00291
2024-12-21 00:30:26.940879: Validation loss did not improve from -0.41973. Patience: 67/50
2024-12-21 00:30:26.941947: train_loss -0.8158
2024-12-21 00:30:26.942918: val_loss -0.3387
2024-12-21 00:30:26.943729: Pseudo dice [0.6856]
2024-12-21 00:30:26.944687: Epoch time: 202.27 s
2024-12-21 00:30:28.428752: 
2024-12-21 00:30:28.430150: Epoch 113
2024-12-21 00:30:28.431085: Current learning rate: 0.00284
2024-12-21 00:34:08.671965: Validation loss did not improve from -0.41973. Patience: 68/50
2024-12-21 00:34:08.672944: train_loss -0.8156
2024-12-21 00:34:08.673776: val_loss -0.3671
2024-12-21 00:34:08.674545: Pseudo dice [0.6952]
2024-12-21 00:34:08.675251: Epoch time: 220.25 s
2024-12-21 00:34:10.142569: 
2024-12-21 00:34:10.143840: Epoch 114
2024-12-21 00:34:10.144648: Current learning rate: 0.00277
2024-12-21 00:37:40.599260: Validation loss did not improve from -0.41973. Patience: 69/50
2024-12-21 00:37:40.600494: train_loss -0.8148
2024-12-21 00:37:40.601360: val_loss -0.3066
2024-12-21 00:37:40.601989: Pseudo dice [0.6695]
2024-12-21 00:37:40.602804: Epoch time: 210.46 s
2024-12-21 00:37:42.962832: 
2024-12-21 00:37:42.963975: Epoch 115
2024-12-21 00:37:42.964699: Current learning rate: 0.0027
2024-12-21 00:41:25.323273: Validation loss did not improve from -0.41973. Patience: 70/50
2024-12-21 00:41:25.323892: train_loss -0.8163
2024-12-21 00:41:25.324594: val_loss -0.3679
2024-12-21 00:41:25.325264: Pseudo dice [0.6944]
2024-12-21 00:41:25.325953: Epoch time: 222.36 s
2024-12-21 00:41:27.098739: 
2024-12-21 00:41:27.099991: Epoch 116
2024-12-21 00:41:27.100756: Current learning rate: 0.00263
2024-12-21 00:45:01.715045: Validation loss did not improve from -0.41973. Patience: 71/50
2024-12-21 00:45:01.716016: train_loss -0.8148
2024-12-21 00:45:01.717722: val_loss -0.3235
2024-12-21 00:45:01.718343: Pseudo dice [0.6839]
2024-12-21 00:45:01.718972: Epoch time: 214.62 s
2024-12-21 00:45:03.218060: 
2024-12-21 00:45:03.219296: Epoch 117
2024-12-21 00:45:03.220056: Current learning rate: 0.00256
2024-12-21 00:48:48.301329: Validation loss did not improve from -0.41973. Patience: 72/50
2024-12-21 00:48:48.302349: train_loss -0.8166
2024-12-21 00:48:48.303075: val_loss -0.3407
2024-12-21 00:48:48.303868: Pseudo dice [0.6842]
2024-12-21 00:48:48.304649: Epoch time: 225.09 s
2024-12-21 00:48:49.772847: 
2024-12-21 00:48:49.773724: Epoch 118
2024-12-21 00:48:49.774432: Current learning rate: 0.00249
2024-12-21 00:52:37.590147: Validation loss did not improve from -0.41973. Patience: 73/50
2024-12-21 00:52:37.591201: train_loss -0.8203
2024-12-21 00:52:37.592136: val_loss -0.3568
2024-12-21 00:52:37.592846: Pseudo dice [0.691]
2024-12-21 00:52:37.593545: Epoch time: 227.82 s
2024-12-21 00:52:39.048943: 
2024-12-21 00:52:39.050390: Epoch 119
2024-12-21 00:52:39.051373: Current learning rate: 0.00242
2024-12-21 00:56:35.216491: Validation loss did not improve from -0.41973. Patience: 74/50
2024-12-21 00:56:35.217314: train_loss -0.8181
2024-12-21 00:56:35.218518: val_loss -0.3355
2024-12-21 00:56:35.219828: Pseudo dice [0.6915]
2024-12-21 00:56:35.220950: Epoch time: 236.17 s
2024-12-21 00:56:35.641717: Yayy! New best EMA pseudo Dice: 0.6864
2024-12-21 00:56:37.563714: 
2024-12-21 00:56:37.565022: Epoch 120
2024-12-21 00:56:37.566308: Current learning rate: 0.00235
2024-12-21 01:00:25.674585: Validation loss did not improve from -0.41973. Patience: 75/50
2024-12-21 01:00:25.675405: train_loss -0.8186
2024-12-21 01:00:25.676367: val_loss -0.3171
2024-12-21 01:00:25.677253: Pseudo dice [0.6766]
2024-12-21 01:00:25.678157: Epoch time: 228.11 s
2024-12-21 01:00:27.185682: 
2024-12-21 01:00:27.186944: Epoch 121
2024-12-21 01:00:27.187827: Current learning rate: 0.00228
2024-12-21 01:03:42.136740: Validation loss did not improve from -0.41973. Patience: 76/50
2024-12-21 01:03:42.137764: train_loss -0.8169
2024-12-21 01:03:42.138599: val_loss -0.3531
2024-12-21 01:03:42.139254: Pseudo dice [0.6923]
2024-12-21 01:03:42.140045: Epoch time: 194.95 s
2024-12-21 01:03:43.687905: 
2024-12-21 01:03:43.689074: Epoch 122
2024-12-21 01:03:43.689771: Current learning rate: 0.00221
2024-12-21 01:07:00.866263: Validation loss did not improve from -0.41973. Patience: 77/50
2024-12-21 01:07:00.867285: train_loss -0.8198
2024-12-21 01:07:00.868019: val_loss -0.3225
2024-12-21 01:07:00.868776: Pseudo dice [0.677]
2024-12-21 01:07:00.869588: Epoch time: 197.18 s
2024-12-21 01:07:02.427251: 
2024-12-21 01:07:02.428528: Epoch 123
2024-12-21 01:07:02.429363: Current learning rate: 0.00214
2024-12-21 01:10:41.286821: Validation loss did not improve from -0.41973. Patience: 78/50
2024-12-21 01:10:41.288002: train_loss -0.8196
2024-12-21 01:10:41.288875: val_loss -0.3427
2024-12-21 01:10:41.289740: Pseudo dice [0.6889]
2024-12-21 01:10:41.290576: Epoch time: 218.86 s
2024-12-21 01:10:42.797909: 
2024-12-21 01:10:42.799134: Epoch 124
2024-12-21 01:10:42.799804: Current learning rate: 0.00207
2024-12-21 01:14:30.969089: Validation loss did not improve from -0.41973. Patience: 79/50
2024-12-21 01:14:30.970046: train_loss -0.821
2024-12-21 01:14:30.970840: val_loss -0.3369
2024-12-21 01:14:30.971516: Pseudo dice [0.6768]
2024-12-21 01:14:30.972187: Epoch time: 228.17 s
2024-12-21 01:14:32.856829: 
2024-12-21 01:14:32.857959: Epoch 125
2024-12-21 01:14:32.858691: Current learning rate: 0.00199
2024-12-21 01:17:59.544332: Validation loss did not improve from -0.41973. Patience: 80/50
2024-12-21 01:17:59.545313: train_loss -0.8218
2024-12-21 01:17:59.545989: val_loss -0.3394
2024-12-21 01:17:59.546635: Pseudo dice [0.6892]
2024-12-21 01:17:59.547270: Epoch time: 206.69 s
2024-12-21 01:18:01.652266: 
2024-12-21 01:18:01.653194: Epoch 126
2024-12-21 01:18:01.653856: Current learning rate: 0.00192
2024-12-21 01:21:50.431292: Validation loss did not improve from -0.41973. Patience: 81/50
2024-12-21 01:21:50.432478: train_loss -0.8219
2024-12-21 01:21:50.433367: val_loss -0.3479
2024-12-21 01:21:50.434060: Pseudo dice [0.6816]
2024-12-21 01:21:50.434756: Epoch time: 228.78 s
2024-12-21 01:21:51.967622: 
2024-12-21 01:21:51.968958: Epoch 127
2024-12-21 01:21:51.969661: Current learning rate: 0.00185
2024-12-21 01:25:32.220296: Validation loss did not improve from -0.41973. Patience: 82/50
2024-12-21 01:25:32.221204: train_loss -0.8241
2024-12-21 01:25:32.222002: val_loss -0.3707
2024-12-21 01:25:32.222722: Pseudo dice [0.7052]
2024-12-21 01:25:32.223404: Epoch time: 220.25 s
2024-12-21 01:25:32.224121: Yayy! New best EMA pseudo Dice: 0.6868
2024-12-21 01:25:34.115305: 
2024-12-21 01:25:34.116575: Epoch 128
2024-12-21 01:25:34.117449: Current learning rate: 0.00178
2024-12-21 01:29:30.125406: Validation loss did not improve from -0.41973. Patience: 83/50
2024-12-21 01:29:30.126451: train_loss -0.823
2024-12-21 01:29:30.127387: val_loss -0.3041
2024-12-21 01:29:30.128265: Pseudo dice [0.6695]
2024-12-21 01:29:30.129142: Epoch time: 236.01 s
2024-12-21 01:29:31.654754: 
2024-12-21 01:29:31.656406: Epoch 129
2024-12-21 01:29:31.657261: Current learning rate: 0.0017
2024-12-21 01:33:21.256684: Validation loss did not improve from -0.41973. Patience: 84/50
2024-12-21 01:33:21.257614: train_loss -0.8237
2024-12-21 01:33:21.258322: val_loss -0.3353
2024-12-21 01:33:21.258954: Pseudo dice [0.6849]
2024-12-21 01:33:21.259961: Epoch time: 229.6 s
2024-12-21 01:33:23.104081: 
2024-12-21 01:33:23.105528: Epoch 130
2024-12-21 01:33:23.106495: Current learning rate: 0.00163
2024-12-21 01:37:16.240848: Validation loss did not improve from -0.41973. Patience: 85/50
2024-12-21 01:37:16.241528: train_loss -0.8265
2024-12-21 01:37:16.242228: val_loss -0.3283
2024-12-21 01:37:16.242872: Pseudo dice [0.6837]
2024-12-21 01:37:16.243722: Epoch time: 233.14 s
2024-12-21 01:37:17.728540: 
2024-12-21 01:37:17.729797: Epoch 131
2024-12-21 01:37:17.730568: Current learning rate: 0.00156
2024-12-21 01:41:21.551116: Validation loss did not improve from -0.41973. Patience: 86/50
2024-12-21 01:41:21.551937: train_loss -0.824
2024-12-21 01:41:21.553223: val_loss -0.292
2024-12-21 01:41:21.554280: Pseudo dice [0.6681]
2024-12-21 01:41:21.555345: Epoch time: 243.82 s
2024-12-21 01:41:23.055096: 
2024-12-21 01:41:23.056731: Epoch 132
2024-12-21 01:41:23.058265: Current learning rate: 0.00148
2024-12-21 01:44:33.590875: Validation loss did not improve from -0.41973. Patience: 87/50
2024-12-21 01:44:33.591903: train_loss -0.8246
2024-12-21 01:44:33.592664: val_loss -0.3346
2024-12-21 01:44:33.593358: Pseudo dice [0.6745]
2024-12-21 01:44:33.594073: Epoch time: 190.54 s
2024-12-21 01:44:35.047480: 
2024-12-21 01:44:35.049034: Epoch 133
2024-12-21 01:44:35.050045: Current learning rate: 0.00141
2024-12-21 01:48:13.394596: Validation loss did not improve from -0.41973. Patience: 88/50
2024-12-21 01:48:13.395585: train_loss -0.8249
2024-12-21 01:48:13.396640: val_loss -0.3217
2024-12-21 01:48:13.397610: Pseudo dice [0.6833]
2024-12-21 01:48:13.398649: Epoch time: 218.35 s
2024-12-21 01:48:14.822082: 
2024-12-21 01:48:14.823284: Epoch 134
2024-12-21 01:48:14.824189: Current learning rate: 0.00133
2024-12-21 01:51:56.373928: Validation loss did not improve from -0.41973. Patience: 89/50
2024-12-21 01:51:56.374839: train_loss -0.8269
2024-12-21 01:51:56.375517: val_loss -0.3332
2024-12-21 01:51:56.376193: Pseudo dice [0.6742]
2024-12-21 01:51:56.376895: Epoch time: 221.55 s
2024-12-21 01:51:58.320697: 
2024-12-21 01:51:58.321936: Epoch 135
2024-12-21 01:51:58.322780: Current learning rate: 0.00126
2024-12-21 01:54:53.615919: Validation loss did not improve from -0.41973. Patience: 90/50
2024-12-21 01:54:53.616843: train_loss -0.8265
2024-12-21 01:54:53.617668: val_loss -0.3328
2024-12-21 01:54:53.618431: Pseudo dice [0.6774]
2024-12-21 01:54:53.619240: Epoch time: 175.3 s
2024-12-21 01:54:55.121407: 
2024-12-21 01:54:55.122567: Epoch 136
2024-12-21 01:54:55.123442: Current learning rate: 0.00118
2024-12-21 01:58:24.385047: Validation loss did not improve from -0.41973. Patience: 91/50
2024-12-21 01:58:24.385780: train_loss -0.8285
2024-12-21 01:58:24.386607: val_loss -0.3091
2024-12-21 01:58:24.387327: Pseudo dice [0.681]
2024-12-21 01:58:24.388085: Epoch time: 209.27 s
2024-12-21 01:58:26.327954: 
2024-12-21 01:58:26.329303: Epoch 137
2024-12-21 01:58:26.330091: Current learning rate: 0.00111
2024-12-21 02:01:56.311953: Validation loss did not improve from -0.41973. Patience: 92/50
2024-12-21 02:01:56.312902: train_loss -0.8272
2024-12-21 02:01:56.313699: val_loss -0.3402
2024-12-21 02:01:56.314440: Pseudo dice [0.6781]
2024-12-21 02:01:56.315195: Epoch time: 209.99 s
2024-12-21 02:01:57.820824: 
2024-12-21 02:01:57.822422: Epoch 138
2024-12-21 02:01:57.823495: Current learning rate: 0.00103
2024-12-21 02:05:44.540493: Validation loss did not improve from -0.41973. Patience: 93/50
2024-12-21 02:05:44.541680: train_loss -0.8264
2024-12-21 02:05:44.542569: val_loss -0.3661
2024-12-21 02:05:44.543399: Pseudo dice [0.6998]
2024-12-21 02:05:44.544239: Epoch time: 226.72 s
2024-12-21 02:05:46.110575: 
2024-12-21 02:05:46.112013: Epoch 139
2024-12-21 02:05:46.112848: Current learning rate: 0.00095
2024-12-21 02:09:46.251599: Validation loss did not improve from -0.41973. Patience: 94/50
2024-12-21 02:09:46.252563: train_loss -0.8287
2024-12-21 02:09:46.253407: val_loss -0.3253
2024-12-21 02:09:46.254214: Pseudo dice [0.6798]
2024-12-21 02:09:46.254968: Epoch time: 240.14 s
2024-12-21 02:09:48.207564: 
2024-12-21 02:09:48.208989: Epoch 140
2024-12-21 02:09:48.209896: Current learning rate: 0.00087
2024-12-21 02:13:37.916389: Validation loss did not improve from -0.41973. Patience: 95/50
2024-12-21 02:13:37.917522: train_loss -0.8318
2024-12-21 02:13:37.918381: val_loss -0.328
2024-12-21 02:13:37.919076: Pseudo dice [0.6782]
2024-12-21 02:13:37.919839: Epoch time: 229.71 s
2024-12-21 02:13:39.376062: 
2024-12-21 02:13:39.378368: Epoch 141
2024-12-21 02:13:39.379176: Current learning rate: 0.00079
2024-12-21 02:17:23.349416: Validation loss did not improve from -0.41973. Patience: 96/50
2024-12-21 02:17:23.350631: train_loss -0.8302
2024-12-21 02:17:23.351462: val_loss -0.3518
2024-12-21 02:17:23.352078: Pseudo dice [0.6926]
2024-12-21 02:17:23.352672: Epoch time: 223.98 s
2024-12-21 02:17:24.824006: 
2024-12-21 02:17:24.825357: Epoch 142
2024-12-21 02:17:24.826160: Current learning rate: 0.00071
2024-12-21 02:21:05.907399: Validation loss did not improve from -0.41973. Patience: 97/50
2024-12-21 02:21:05.908216: train_loss -0.8291
2024-12-21 02:21:05.909098: val_loss -0.3259
2024-12-21 02:21:05.909773: Pseudo dice [0.6768]
2024-12-21 02:21:05.910485: Epoch time: 221.09 s
2024-12-21 02:21:07.336776: 
2024-12-21 02:21:07.337894: Epoch 143
2024-12-21 02:21:07.338591: Current learning rate: 0.00063
2024-12-21 02:24:10.283912: Validation loss did not improve from -0.41973. Patience: 98/50
2024-12-21 02:24:10.284877: train_loss -0.8305
2024-12-21 02:24:10.285581: val_loss -0.3711
2024-12-21 02:24:10.286345: Pseudo dice [0.6952]
2024-12-21 02:24:10.287026: Epoch time: 182.95 s
2024-12-21 02:24:11.722752: 
2024-12-21 02:24:11.724067: Epoch 144
2024-12-21 02:24:11.724936: Current learning rate: 0.00055
2024-12-21 02:27:40.004380: Validation loss did not improve from -0.41973. Patience: 99/50
2024-12-21 02:27:40.005296: train_loss -0.8295
2024-12-21 02:27:40.006037: val_loss -0.2998
2024-12-21 02:27:40.006775: Pseudo dice [0.6756]
2024-12-21 02:27:40.007425: Epoch time: 208.28 s
2024-12-21 02:27:42.014759: 
2024-12-21 02:27:42.016071: Epoch 145
2024-12-21 02:27:42.016802: Current learning rate: 0.00047
2024-12-21 02:31:11.429250: Validation loss did not improve from -0.41973. Patience: 100/50
2024-12-21 02:31:11.429972: train_loss -0.8295
2024-12-21 02:31:11.430753: val_loss -0.3126
2024-12-21 02:31:11.431375: Pseudo dice [0.6783]
2024-12-21 02:31:11.432016: Epoch time: 209.42 s
2024-12-21 02:31:12.868813: 
2024-12-21 02:31:12.870062: Epoch 146
2024-12-21 02:31:12.870749: Current learning rate: 0.00038
2024-12-21 02:34:49.147750: Validation loss did not improve from -0.41973. Patience: 101/50
2024-12-21 02:34:49.148722: train_loss -0.8281
2024-12-21 02:34:49.149511: val_loss -0.3178
2024-12-21 02:34:49.150119: Pseudo dice [0.6826]
2024-12-21 02:34:49.150757: Epoch time: 216.28 s
2024-12-21 02:34:50.634213: 
2024-12-21 02:34:50.635328: Epoch 147
2024-12-21 02:34:50.635987: Current learning rate: 0.0003
2024-12-21 02:38:43.132226: Validation loss did not improve from -0.41973. Patience: 102/50
2024-12-21 02:38:43.133095: train_loss -0.8297
2024-12-21 02:38:43.134184: val_loss -0.3408
2024-12-21 02:38:43.135120: Pseudo dice [0.6948]
2024-12-21 02:38:43.136090: Epoch time: 232.5 s
2024-12-21 02:38:44.991118: 
2024-12-21 02:38:44.992382: Epoch 148
2024-12-21 02:38:44.993174: Current learning rate: 0.00021
2024-12-21 02:42:38.714652: Validation loss did not improve from -0.41973. Patience: 103/50
2024-12-21 02:42:38.715517: train_loss -0.8319
2024-12-21 02:42:38.716421: val_loss -0.3526
2024-12-21 02:42:38.717045: Pseudo dice [0.6981]
2024-12-21 02:42:38.717671: Epoch time: 233.73 s
2024-12-21 02:42:40.108153: 
2024-12-21 02:42:40.109426: Epoch 149
2024-12-21 02:42:40.110127: Current learning rate: 0.00011
2024-12-21 02:45:45.343161: Validation loss did not improve from -0.41973. Patience: 104/50
2024-12-21 02:45:45.343993: train_loss -0.8314
2024-12-21 02:45:45.344731: val_loss -0.3109
2024-12-21 02:45:45.345346: Pseudo dice [0.6809]
2024-12-21 02:45:45.346290: Epoch time: 185.24 s
2024-12-21 02:45:47.131529: Training done.
2024-12-21 02:41:55.334781: Using splits from existing split file: /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset308_Sohee_Calcium_OCT_CrossValidation/splits_final_40.json
2024-12-21 02:41:55.344998: The split file contains 5 splits.
2024-12-21 02:41:55.345815: Desired fold for training: 2
2024-12-21 02:41:55.346420: This split has 3 training and 5 validation cases.
2024-12-21 02:41:55.347228: predicting 101-044
2024-12-21 02:41:55.371123: 101-044, shape torch.Size([1, 404, 498, 498]), rank 0
2024-12-21 02:45:26.727785: predicting 106-002
2024-12-21 02:45:26.762512: 106-002, shape torch.Size([1, 539, 498, 498]), rank 0
2024-12-21 02:48:36.328314: predicting 401-004
2024-12-21 02:48:36.341707: 401-004, shape torch.Size([1, 375, 498, 498]), rank 0
2024-12-21 02:50:48.718231: predicting 701-013
2024-12-21 02:50:48.733086: 701-013, shape torch.Size([1, 375, 498, 498]), rank 0
2024-12-21 02:52:53.299633: predicting 704-003
2024-12-21 02:52:53.315347: 704-003, shape torch.Size([1, 375, 498, 498]), rank 0
2024-12-21 02:55:14.497579: Validation complete
2024-12-21 02:55:14.498223: Mean Validation Dice:  0.6799921576149568
2024-12-21 02:45:47.255890: Using splits from existing split file: /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset308_Sohee_Calcium_OCT_CrossValidation/splits_final_40.json
2024-12-21 02:45:47.258597: The split file contains 5 splits.
2024-12-21 02:45:47.260067: Desired fold for training: 3
2024-12-21 02:45:47.261387: This split has 3 training and 6 validation cases.
2024-12-21 02:45:47.263339: predicting 101-044
2024-12-21 02:45:47.276094: 101-044, shape torch.Size([1, 404, 498, 498]), rank 0
2024-12-21 02:48:39.538713: predicting 101-045
2024-12-21 02:48:39.554740: 101-045, shape torch.Size([1, 375, 498, 498]), rank 0
2024-12-21 02:50:51.568000: predicting 106-002
2024-12-21 02:50:51.646291: 106-002, shape torch.Size([1, 539, 498, 498]), rank 0
2024-12-21 02:54:05.095082: predicting 401-004
2024-12-21 02:54:05.109626: 401-004, shape torch.Size([1, 375, 498, 498]), rank 0
2024-12-21 02:56:27.251534: predicting 704-003
2024-12-21 02:56:27.265277: 704-003, shape torch.Size([1, 375, 498, 498]), rank 0
2024-12-21 02:58:24.500437: predicting 706-005
2024-12-21 02:58:24.524039: 706-005, shape torch.Size([1, 375, 498, 498]), rank 0
2024-12-21 03:00:50.302010: Validation complete
2024-12-21 03:00:50.302838: Mean Validation Dice:  0.6567324472490729

############################
INFO: You are using the old nnU-Net default plans. We have updated our recommendations. Please consider using those instead! Read more here: https://github.com/MIC-DKFZ/nnUNet/blob/master/documentation/resenc_presets.md
############################

Using device: cuda:0

#######################################################################
Please cite the following paper when using nnU-Net:
Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.
#######################################################################

2024-12-21 03:00:58.320631: Using torch.compile...
/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
################### Loading pretrained weights from file  /home/gridsan/nchutisilp/projects/OpenAI-CLIP/clip_pretrained_nnUNet.pt ###################
Below is the list of overlapping blocks in pretrained model and nnUNet architecture:
encoder.stages.0.0.convs.0.conv.weight shape torch.Size([32, 1, 3, 3, 3])
encoder.stages.0.0.convs.0.conv.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.norm.weight shape torch.Size([32])
encoder.stages.0.0.convs.0.norm.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.0.weight shape torch.Size([32, 1, 3, 3, 3])
encoder.stages.0.0.convs.0.all_modules.0.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.1.weight shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.1.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
encoder.stages.0.0.convs.1.conv.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.norm.weight shape torch.Size([32])
encoder.stages.0.0.convs.1.norm.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
encoder.stages.0.0.convs.1.all_modules.0.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.1.weight shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.1.bias shape torch.Size([32])
encoder.stages.1.0.convs.0.conv.weight shape torch.Size([64, 32, 3, 3, 3])
encoder.stages.1.0.convs.0.conv.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.norm.weight shape torch.Size([64])
encoder.stages.1.0.convs.0.norm.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.0.weight shape torch.Size([64, 32, 3, 3, 3])
encoder.stages.1.0.convs.0.all_modules.0.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.1.weight shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.1.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
encoder.stages.1.0.convs.1.conv.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.norm.weight shape torch.Size([64])
encoder.stages.1.0.convs.1.norm.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
encoder.stages.1.0.convs.1.all_modules.0.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.1.weight shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.1.bias shape torch.Size([64])
encoder.stages.2.0.convs.0.conv.weight shape torch.Size([128, 64, 3, 3, 3])
encoder.stages.2.0.convs.0.conv.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.norm.weight shape torch.Size([128])
encoder.stages.2.0.convs.0.norm.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.0.weight shape torch.Size([128, 64, 3, 3, 3])
encoder.stages.2.0.convs.0.all_modules.0.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.1.weight shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.1.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
encoder.stages.2.0.convs.1.conv.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.norm.weight shape torch.Size([128])
encoder.stages.2.0.convs.1.norm.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
encoder.stages.2.0.convs.1.all_modules.0.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.1.weight shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.1.bias shape torch.Size([128])
encoder.stages.3.0.convs.0.conv.weight shape torch.Size([256, 128, 3, 3, 3])
encoder.stages.3.0.convs.0.conv.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.norm.weight shape torch.Size([256])
encoder.stages.3.0.convs.0.norm.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.0.weight shape torch.Size([256, 128, 3, 3, 3])
encoder.stages.3.0.convs.0.all_modules.0.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.1.weight shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.1.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
encoder.stages.3.0.convs.1.conv.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.norm.weight shape torch.Size([256])
encoder.stages.3.0.convs.1.norm.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
encoder.stages.3.0.convs.1.all_modules.0.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.1.weight shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.1.bias shape torch.Size([256])
encoder.stages.4.0.convs.0.conv.weight shape torch.Size([320, 256, 3, 3, 3])
encoder.stages.4.0.convs.0.conv.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.norm.weight shape torch.Size([320])
encoder.stages.4.0.convs.0.norm.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.0.weight shape torch.Size([320, 256, 3, 3, 3])
encoder.stages.4.0.convs.0.all_modules.0.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.1.weight shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.1.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.4.0.convs.1.conv.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.norm.weight shape torch.Size([320])
encoder.stages.4.0.convs.1.norm.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.4.0.convs.1.all_modules.0.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.1.weight shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.1.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.0.conv.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.norm.weight shape torch.Size([320])
encoder.stages.5.0.convs.0.norm.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.0.all_modules.0.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.1.weight shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.1.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.1.conv.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.norm.weight shape torch.Size([320])
encoder.stages.5.0.convs.1.norm.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.1.all_modules.0.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.1.weight shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.0.0.convs.0.conv.weight shape torch.Size([32, 1, 3, 3, 3])
decoder.encoder.stages.0.0.convs.0.conv.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.norm.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.norm.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.0.weight shape torch.Size([32, 1, 3, 3, 3])
decoder.encoder.stages.0.0.convs.0.all_modules.0.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.1.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.1.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.encoder.stages.0.0.convs.1.conv.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.norm.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.norm.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.encoder.stages.0.0.convs.1.all_modules.0.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.1.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.1.bias shape torch.Size([32])
decoder.encoder.stages.1.0.convs.0.conv.weight shape torch.Size([64, 32, 3, 3, 3])
decoder.encoder.stages.1.0.convs.0.conv.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.norm.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.norm.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.0.weight shape torch.Size([64, 32, 3, 3, 3])
decoder.encoder.stages.1.0.convs.0.all_modules.0.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.1.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.1.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.encoder.stages.1.0.convs.1.conv.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.norm.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.norm.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.encoder.stages.1.0.convs.1.all_modules.0.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.1.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.1.bias shape torch.Size([64])
decoder.encoder.stages.2.0.convs.0.conv.weight shape torch.Size([128, 64, 3, 3, 3])
decoder.encoder.stages.2.0.convs.0.conv.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.norm.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.norm.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.0.weight shape torch.Size([128, 64, 3, 3, 3])
decoder.encoder.stages.2.0.convs.0.all_modules.0.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.1.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.1.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.encoder.stages.2.0.convs.1.conv.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.norm.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.norm.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.encoder.stages.2.0.convs.1.all_modules.0.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.1.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.1.bias shape torch.Size([128])
decoder.encoder.stages.3.0.convs.0.conv.weight shape torch.Size([256, 128, 3, 3, 3])
decoder.encoder.stages.3.0.convs.0.conv.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.norm.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.norm.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.0.weight shape torch.Size([256, 128, 3, 3, 3])
decoder.encoder.stages.3.0.convs.0.all_modules.0.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.1.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.1.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.encoder.stages.3.0.convs.1.conv.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.norm.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.norm.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.encoder.stages.3.0.convs.1.all_modules.0.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.1.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.1.bias shape torch.Size([256])
decoder.encoder.stages.4.0.convs.0.conv.weight shape torch.Size([320, 256, 3, 3, 3])
decoder.encoder.stages.4.0.convs.0.conv.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.norm.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.norm.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.0.weight shape torch.Size([320, 256, 3, 3, 3])
decoder.encoder.stages.4.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.4.0.convs.1.conv.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.norm.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.norm.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.4.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.0.conv.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.norm.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.norm.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.1.conv.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.norm.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.norm.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.stages.0.convs.0.conv.weight shape torch.Size([320, 640, 3, 3, 3])
decoder.stages.0.convs.0.conv.bias shape torch.Size([320])
decoder.stages.0.convs.0.norm.weight shape torch.Size([320])
decoder.stages.0.convs.0.norm.bias shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.0.weight shape torch.Size([320, 640, 3, 3, 3])
decoder.stages.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.stages.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.stages.0.convs.1.conv.bias shape torch.Size([320])
decoder.stages.0.convs.1.norm.weight shape torch.Size([320])
decoder.stages.0.convs.1.norm.bias shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.stages.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.stages.1.convs.0.conv.weight shape torch.Size([256, 512, 3, 3, 3])
decoder.stages.1.convs.0.conv.bias shape torch.Size([256])
decoder.stages.1.convs.0.norm.weight shape torch.Size([256])
decoder.stages.1.convs.0.norm.bias shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.0.weight shape torch.Size([256, 512, 3, 3, 3])
decoder.stages.1.convs.0.all_modules.0.bias shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.1.weight shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.1.bias shape torch.Size([256])
decoder.stages.1.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.stages.1.convs.1.conv.bias shape torch.Size([256])
decoder.stages.1.convs.1.norm.weight shape torch.Size([256])
decoder.stages.1.convs.1.norm.bias shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.stages.1.convs.1.all_modules.0.bias shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.1.weight shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.1.bias shape torch.Size([256])
decoder.stages.2.convs.0.conv.weight shape torch.Size([128, 256, 3, 3, 3])
decoder.stages.2.convs.0.conv.bias shape torch.Size([128])
decoder.stages.2.convs.0.norm.weight shape torch.Size([128])
decoder.stages.2.convs.0.norm.bias shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.0.weight shape torch.Size([128, 256, 3, 3, 3])
decoder.stages.2.convs.0.all_modules.0.bias shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.1.weight shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.1.bias shape torch.Size([128])
decoder.stages.2.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.stages.2.convs.1.conv.bias shape torch.Size([128])
decoder.stages.2.convs.1.norm.weight shape torch.Size([128])
decoder.stages.2.convs.1.norm.bias shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.stages.2.convs.1.all_modules.0.bias shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.1.weight shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.1.bias shape torch.Size([128])
decoder.stages.3.convs.0.conv.weight shape torch.Size([64, 128, 3, 3, 3])
decoder.stages.3.convs.0.conv.bias shape torch.Size([64])
decoder.stages.3.convs.0.norm.weight shape torch.Size([64])
decoder.stages.3.convs.0.norm.bias shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.0.weight shape torch.Size([64, 128, 3, 3, 3])
decoder.stages.3.convs.0.all_modules.0.bias shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.1.weight shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.1.bias shape torch.Size([64])
decoder.stages.3.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.stages.3.convs.1.conv.bias shape torch.Size([64])
decoder.stages.3.convs.1.norm.weight shape torch.Size([64])
decoder.stages.3.convs.1.norm.bias shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.stages.3.convs.1.all_modules.0.bias shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.1.weight shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.1.bias shape torch.Size([64])
decoder.stages.4.convs.0.conv.weight shape torch.Size([32, 64, 3, 3, 3])
decoder.stages.4.convs.0.conv.bias shape torch.Size([32])
decoder.stages.4.convs.0.norm.weight shape torch.Size([32])
decoder.stages.4.convs.0.norm.bias shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.0.weight shape torch.Size([32, 64, 3, 3, 3])
decoder.stages.4.convs.0.all_modules.0.bias shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.1.weight shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.1.bias shape torch.Size([32])
decoder.stages.4.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.stages.4.convs.1.conv.bias shape torch.Size([32])
decoder.stages.4.convs.1.norm.weight shape torch.Size([32])
decoder.stages.4.convs.1.norm.bias shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.stages.4.convs.1.all_modules.0.bias shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.1.weight shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.1.bias shape torch.Size([32])
decoder.transpconvs.0.weight shape torch.Size([320, 320, 1, 2, 2])
decoder.transpconvs.0.bias shape torch.Size([320])
decoder.transpconvs.1.weight shape torch.Size([320, 256, 2, 2, 2])
decoder.transpconvs.1.bias shape torch.Size([256])
decoder.transpconvs.2.weight shape torch.Size([256, 128, 2, 2, 2])
decoder.transpconvs.2.bias shape torch.Size([128])
decoder.transpconvs.3.weight shape torch.Size([128, 64, 2, 2, 2])
decoder.transpconvs.3.bias shape torch.Size([64])
decoder.transpconvs.4.weight shape torch.Size([64, 32, 2, 2, 2])
decoder.transpconvs.4.bias shape torch.Size([32])
################### Done ###################
2024-12-21 03:01:18.957606: do_dummy_2d_data_aug: True
2024-12-21 03:01:18.959043: Using splits from existing split file: /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset308_Sohee_Calcium_OCT_CrossValidation/splits_final_40.json
2024-12-21 03:01:18.960860: The split file contains 5 splits.
2024-12-21 03:01:18.961835: Desired fold for training: 4
2024-12-21 03:01:18.962567: This split has 3 training and 5 validation cases.
using pin_memory on device 0
using pin_memory on device 0

This is the configuration used by this training:
Configuration name: 3d_32x160x128_b10
 {'data_identifier': 'nnUNetPlans_3d_fullres', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 10, 'patch_size': [32, 160, 128], 'median_image_size_in_voxels': [375.0, 498.0, 498.0], 'spacing': [1.0, 1.0, 1.0], 'normalization_schemes': ['ZScoreNormalization'], 'use_mask_for_norm': [False], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'architecture': {'network_class_name': 'dynamic_network_architectures.architectures.unet.PlainConvUNet', 'arch_kwargs': {'n_stages': 6, 'features_per_stage': [32, 64, 128, 256, 320, 320], 'conv_op': 'torch.nn.modules.conv.Conv3d', 'kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]], 'strides': [[1, 1, 1], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [1, 2, 2]], 'n_conv_per_stage': [2, 2, 2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2, 2, 2], 'conv_bias': True, 'norm_op': 'torch.nn.modules.instancenorm.InstanceNorm3d', 'norm_op_kwargs': {'eps': 1e-05, 'affine': True}, 'dropout_op': None, 'dropout_op_kwargs': None, 'nonlin': 'torch.nn.LeakyReLU', 'nonlin_kwargs': {'inplace': True}, 'deep_supervision': True}, '_kw_requires_import': ['conv_op', 'norm_op', 'dropout_op', 'nonlin']}, 'batch_dice': True, 'inherits_from': '3d_fullres'} 

These are the global plan.json settings:
 {'dataset_name': 'Dataset308_Sohee_Calcium_OCT_CrossValidation', 'plans_name': 'nnUNetPlans', 'original_median_spacing_after_transp': [1.0, 1.0, 1.0], 'original_median_shape_after_transp': [375, 498, 498], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 0.4977976381778717, 'mean': 0.13507525622844696, 'median': 0.09599608182907104, 'min': 0.0, 'percentile_00_5': 0.014754901640117168, 'percentile_99_5': 0.4977976381778717, 'std': 0.12025152146816254}}} 

2024-12-21 03:01:42.412510: unpacking dataset...
2024-12-21 03:01:46.656563: unpacking done...
2024-12-21 03:01:46.971100: Unable to plot network architecture: nnUNet_compile is enabled!
2024-12-21 03:01:47.070593: 
2024-12-21 03:01:47.071615: Epoch 0
2024-12-21 03:01:47.072427: Current learning rate: 0.01
2024-12-21 03:05:50.160957: Validation loss improved from 1000.00000 to -0.17603! Patience: 0/50
2024-12-21 03:05:50.161908: train_loss -0.1165
2024-12-21 03:05:50.162723: val_loss -0.176
2024-12-21 03:05:50.163324: Pseudo dice [0.5279]
2024-12-21 03:05:50.163881: Epoch time: 243.09 s
2024-12-21 03:05:50.164636: Yayy! New best EMA pseudo Dice: 0.5279
2024-12-21 03:05:52.249043: 
2024-12-21 03:05:52.250302: Epoch 1
2024-12-21 03:05:52.251053: Current learning rate: 0.00994
2024-12-21 03:08:44.254518: Validation loss improved from -0.17603 to -0.19593! Patience: 0/50
2024-12-21 03:08:44.255586: train_loss -0.2575
2024-12-21 03:08:44.256370: val_loss -0.1959
2024-12-21 03:08:44.257097: Pseudo dice [0.5435]
2024-12-21 03:08:44.257812: Epoch time: 172.01 s
2024-12-21 03:08:44.258605: Yayy! New best EMA pseudo Dice: 0.5295
2024-12-21 03:08:46.148261: 
2024-12-21 03:08:46.149170: Epoch 2
2024-12-21 03:08:46.150033: Current learning rate: 0.00988
2024-12-21 03:11:53.660810: Validation loss improved from -0.19593 to -0.25996! Patience: 0/50
2024-12-21 03:11:53.662033: train_loss -0.294
2024-12-21 03:11:53.662869: val_loss -0.26
2024-12-21 03:11:53.663595: Pseudo dice [0.5737]
2024-12-21 03:11:53.664304: Epoch time: 187.52 s
2024-12-21 03:11:53.665032: Yayy! New best EMA pseudo Dice: 0.5339
2024-12-21 03:11:55.564132: 
2024-12-21 03:11:55.565259: Epoch 3
2024-12-21 03:11:55.566118: Current learning rate: 0.00982
2024-12-21 03:15:04.209253: Validation loss improved from -0.25996 to -0.28577! Patience: 0/50
2024-12-21 03:15:04.210400: train_loss -0.3399
2024-12-21 03:15:04.211174: val_loss -0.2858
2024-12-21 03:15:04.211884: Pseudo dice [0.5912]
2024-12-21 03:15:04.212565: Epoch time: 188.65 s
2024-12-21 03:15:04.213261: Yayy! New best EMA pseudo Dice: 0.5396
2024-12-21 03:15:06.058794: 
2024-12-21 03:15:06.059958: Epoch 4
2024-12-21 03:15:06.060782: Current learning rate: 0.00976
2024-12-21 03:18:17.737296: Validation loss improved from -0.28577 to -0.33463! Patience: 0/50
2024-12-21 03:18:17.738344: train_loss -0.3707
2024-12-21 03:18:17.739385: val_loss -0.3346
2024-12-21 03:18:17.740222: Pseudo dice [0.6183]
2024-12-21 03:18:17.741115: Epoch time: 191.68 s
2024-12-21 03:18:18.085471: Yayy! New best EMA pseudo Dice: 0.5475
2024-12-21 03:18:20.000345: 
2024-12-21 03:18:20.001773: Epoch 5
2024-12-21 03:18:20.002714: Current learning rate: 0.0097
2024-12-21 03:21:30.543376: Validation loss improved from -0.33463 to -0.38788! Patience: 0/50
2024-12-21 03:21:30.544181: train_loss -0.3909
2024-12-21 03:21:30.544947: val_loss -0.3879
2024-12-21 03:21:30.545640: Pseudo dice [0.6378]
2024-12-21 03:21:30.546259: Epoch time: 190.55 s
2024-12-21 03:21:30.546940: Yayy! New best EMA pseudo Dice: 0.5565
2024-12-21 03:21:32.285672: 
2024-12-21 03:21:32.286999: Epoch 6
2024-12-21 03:21:32.287794: Current learning rate: 0.00964
2024-12-21 03:24:36.740354: Validation loss did not improve from -0.38788. Patience: 1/50
2024-12-21 03:24:36.741330: train_loss -0.4086
2024-12-21 03:24:36.742146: val_loss -0.2726
2024-12-21 03:24:36.742749: Pseudo dice [0.6023]
2024-12-21 03:24:36.743511: Epoch time: 184.46 s
2024-12-21 03:24:36.744340: Yayy! New best EMA pseudo Dice: 0.5611
2024-12-21 03:24:38.540166: 
2024-12-21 03:24:38.542077: Epoch 7
2024-12-21 03:24:38.543019: Current learning rate: 0.00958
2024-12-21 03:27:51.726788: Validation loss improved from -0.38788 to -0.39545! Patience: 1/50
2024-12-21 03:27:51.728390: train_loss -0.4287
2024-12-21 03:27:51.729254: val_loss -0.3955
2024-12-21 03:27:51.729944: Pseudo dice [0.6504]
2024-12-21 03:27:51.730558: Epoch time: 193.19 s
2024-12-21 03:27:51.731128: Yayy! New best EMA pseudo Dice: 0.57
2024-12-21 03:27:54.054064: 
2024-12-21 03:27:54.055486: Epoch 8
2024-12-21 03:27:54.056216: Current learning rate: 0.00952
2024-12-21 03:31:04.287449: Validation loss did not improve from -0.39545. Patience: 1/50
2024-12-21 03:31:04.288697: train_loss -0.4527
2024-12-21 03:31:04.289432: val_loss -0.3561
2024-12-21 03:31:04.290076: Pseudo dice [0.6237]
2024-12-21 03:31:04.290835: Epoch time: 190.24 s
2024-12-21 03:31:04.291538: Yayy! New best EMA pseudo Dice: 0.5754
2024-12-21 03:31:06.184661: 
2024-12-21 03:31:06.186466: Epoch 9
2024-12-21 03:31:06.187513: Current learning rate: 0.00946
2024-12-21 03:34:13.863716: Validation loss did not improve from -0.39545. Patience: 2/50
2024-12-21 03:34:13.864795: train_loss -0.4633
2024-12-21 03:34:13.866112: val_loss -0.3735
2024-12-21 03:34:13.867050: Pseudo dice [0.6368]
2024-12-21 03:34:13.868056: Epoch time: 187.68 s
2024-12-21 03:34:14.281915: Yayy! New best EMA pseudo Dice: 0.5815
2024-12-21 03:34:16.045027: 
2024-12-21 03:34:16.046319: Epoch 10
2024-12-21 03:34:16.047182: Current learning rate: 0.0094
2024-12-21 03:37:29.494904: Validation loss improved from -0.39545 to -0.40737! Patience: 2/50
2024-12-21 03:37:29.495856: train_loss -0.4786
2024-12-21 03:37:29.496599: val_loss -0.4074
2024-12-21 03:37:29.497261: Pseudo dice [0.6664]
2024-12-21 03:37:29.497936: Epoch time: 193.45 s
2024-12-21 03:37:29.498628: Yayy! New best EMA pseudo Dice: 0.59
2024-12-21 03:37:31.370261: 
2024-12-21 03:37:31.371529: Epoch 11
2024-12-21 03:37:31.372386: Current learning rate: 0.00934
2024-12-21 03:40:41.498293: Validation loss did not improve from -0.40737. Patience: 1/50
2024-12-21 03:40:41.499258: train_loss -0.478
2024-12-21 03:40:41.500036: val_loss -0.405
2024-12-21 03:40:41.500725: Pseudo dice [0.6611]
2024-12-21 03:40:41.501414: Epoch time: 190.13 s
2024-12-21 03:40:41.502069: Yayy! New best EMA pseudo Dice: 0.5971
2024-12-21 03:40:43.337916: 
2024-12-21 03:40:43.339283: Epoch 12
2024-12-21 03:40:43.340079: Current learning rate: 0.00928
2024-12-21 03:43:54.661038: Validation loss improved from -0.40737 to -0.41510! Patience: 1/50
2024-12-21 03:43:54.662056: train_loss -0.5065
2024-12-21 03:43:54.662839: val_loss -0.4151
2024-12-21 03:43:54.663571: Pseudo dice [0.6603]
2024-12-21 03:43:54.664334: Epoch time: 191.33 s
2024-12-21 03:43:54.665045: Yayy! New best EMA pseudo Dice: 0.6034
2024-12-21 03:43:56.518764: 
2024-12-21 03:43:56.519828: Epoch 13
2024-12-21 03:43:56.520914: Current learning rate: 0.00922
2024-12-21 03:47:06.804011: Validation loss improved from -0.41510 to -0.45746! Patience: 0/50
2024-12-21 03:47:06.805062: train_loss -0.5234
2024-12-21 03:47:06.805887: val_loss -0.4575
2024-12-21 03:47:06.806711: Pseudo dice [0.6936]
2024-12-21 03:47:06.807551: Epoch time: 190.29 s
2024-12-21 03:47:06.808405: Yayy! New best EMA pseudo Dice: 0.6125
2024-12-21 03:47:08.608535: 
2024-12-21 03:47:08.609818: Epoch 14
2024-12-21 03:47:08.610711: Current learning rate: 0.00916
2024-12-21 03:50:07.976116: Validation loss did not improve from -0.45746. Patience: 1/50
2024-12-21 03:50:07.977128: train_loss -0.5324
2024-12-21 03:50:07.977925: val_loss -0.4403
2024-12-21 03:50:07.978543: Pseudo dice [0.6923]
2024-12-21 03:50:07.979120: Epoch time: 179.37 s
2024-12-21 03:50:08.396701: Yayy! New best EMA pseudo Dice: 0.6204
2024-12-21 03:50:10.165661: 
2024-12-21 03:50:10.166820: Epoch 15
2024-12-21 03:50:10.167457: Current learning rate: 0.0091
2024-12-21 03:53:12.209894: Validation loss did not improve from -0.45746. Patience: 2/50
2024-12-21 03:53:12.210881: train_loss -0.5428
2024-12-21 03:53:12.212089: val_loss -0.437
2024-12-21 03:53:12.213065: Pseudo dice [0.6818]
2024-12-21 03:53:12.214003: Epoch time: 182.05 s
2024-12-21 03:53:12.215015: Yayy! New best EMA pseudo Dice: 0.6266
2024-12-21 03:53:14.026100: 
2024-12-21 03:53:14.027327: Epoch 16
2024-12-21 03:53:14.028236: Current learning rate: 0.00903
2024-12-21 03:56:09.794011: Validation loss did not improve from -0.45746. Patience: 3/50
2024-12-21 03:56:09.794900: train_loss -0.5406
2024-12-21 03:56:09.795934: val_loss -0.4147
2024-12-21 03:56:09.796891: Pseudo dice [0.6748]
2024-12-21 03:56:09.797664: Epoch time: 175.77 s
2024-12-21 03:56:09.798495: Yayy! New best EMA pseudo Dice: 0.6314
2024-12-21 03:56:11.597072: 
2024-12-21 03:56:11.598871: Epoch 17
2024-12-21 03:56:11.600090: Current learning rate: 0.00897
2024-12-21 03:59:09.096184: Validation loss did not improve from -0.45746. Patience: 4/50
2024-12-21 03:59:09.097169: train_loss -0.5544
2024-12-21 03:59:09.098047: val_loss -0.4308
2024-12-21 03:59:09.098911: Pseudo dice [0.6779]
2024-12-21 03:59:09.099678: Epoch time: 177.5 s
2024-12-21 03:59:09.100349: Yayy! New best EMA pseudo Dice: 0.636
2024-12-21 03:59:10.921831: 
2024-12-21 03:59:10.922976: Epoch 18
2024-12-21 03:59:10.923888: Current learning rate: 0.00891
2024-12-21 04:02:05.824481: Validation loss improved from -0.45746 to -0.45955! Patience: 4/50
2024-12-21 04:02:05.825198: train_loss -0.5537
2024-12-21 04:02:05.825931: val_loss -0.4595
2024-12-21 04:02:05.826651: Pseudo dice [0.6977]
2024-12-21 04:02:05.827316: Epoch time: 174.9 s
2024-12-21 04:02:05.828058: Yayy! New best EMA pseudo Dice: 0.6422
2024-12-21 04:02:08.085488: 
2024-12-21 04:02:08.086593: Epoch 19
2024-12-21 04:02:08.087355: Current learning rate: 0.00885
2024-12-21 04:05:31.305037: Validation loss did not improve from -0.45955. Patience: 1/50
2024-12-21 04:05:31.305958: train_loss -0.5687
2024-12-21 04:05:31.306776: val_loss -0.4362
2024-12-21 04:05:31.307538: Pseudo dice [0.6723]
2024-12-21 04:05:31.308222: Epoch time: 203.22 s
2024-12-21 04:05:31.710858: Yayy! New best EMA pseudo Dice: 0.6452
2024-12-21 04:05:33.506424: 
2024-12-21 04:05:33.507498: Epoch 20
2024-12-21 04:05:33.508343: Current learning rate: 0.00879
2024-12-21 04:13:05.286493: Validation loss improved from -0.45955 to -0.46109! Patience: 1/50
2024-12-21 04:13:05.290293: train_loss -0.571
2024-12-21 04:13:05.292320: val_loss -0.4611
2024-12-21 04:13:05.292997: Pseudo dice [0.7014]
2024-12-21 04:13:05.293999: Epoch time: 451.78 s
2024-12-21 04:13:05.294597: Yayy! New best EMA pseudo Dice: 0.6508
2024-12-21 04:13:07.213960: 
2024-12-21 04:13:07.215133: Epoch 21
2024-12-21 04:13:07.215848: Current learning rate: 0.00873
2024-12-21 04:20:36.776720: Validation loss improved from -0.46109 to -0.46661! Patience: 0/50
2024-12-21 04:20:36.777579: train_loss -0.5861
2024-12-21 04:20:36.778332: val_loss -0.4666
2024-12-21 04:20:36.779057: Pseudo dice [0.704]
2024-12-21 04:20:36.779960: Epoch time: 449.56 s
2024-12-21 04:20:36.780657: Yayy! New best EMA pseudo Dice: 0.6562
2024-12-21 04:20:38.479352: 
2024-12-21 04:20:38.481109: Epoch 22
2024-12-21 04:20:38.482110: Current learning rate: 0.00867
2024-12-21 04:27:58.307106: Validation loss improved from -0.46661 to -0.47275! Patience: 0/50
2024-12-21 04:27:58.308026: train_loss -0.5798
2024-12-21 04:27:58.308759: val_loss -0.4727
2024-12-21 04:27:58.309520: Pseudo dice [0.703]
2024-12-21 04:27:58.310227: Epoch time: 439.83 s
2024-12-21 04:27:58.310944: Yayy! New best EMA pseudo Dice: 0.6608
2024-12-21 04:27:59.979689: 
2024-12-21 04:27:59.980869: Epoch 23
2024-12-21 04:27:59.981543: Current learning rate: 0.00861
2024-12-21 04:35:03.817761: Validation loss improved from -0.47275 to -0.48671! Patience: 0/50
2024-12-21 04:35:03.818753: train_loss -0.5931
2024-12-21 04:35:03.819650: val_loss -0.4867
2024-12-21 04:35:03.820253: Pseudo dice [0.7152]
2024-12-21 04:35:03.820859: Epoch time: 423.84 s
2024-12-21 04:35:03.821529: Yayy! New best EMA pseudo Dice: 0.6663
2024-12-21 04:35:05.615790: 
2024-12-21 04:35:05.617000: Epoch 24
2024-12-21 04:35:05.617739: Current learning rate: 0.00855
2024-12-21 04:42:04.708801: Validation loss did not improve from -0.48671. Patience: 1/50
2024-12-21 04:42:04.709863: train_loss -0.5958
2024-12-21 04:42:04.710589: val_loss -0.4418
2024-12-21 04:42:04.711250: Pseudo dice [0.6961]
2024-12-21 04:42:04.711937: Epoch time: 419.1 s
2024-12-21 04:42:05.155105: Yayy! New best EMA pseudo Dice: 0.6693
2024-12-21 04:42:07.003953: 
2024-12-21 04:42:07.005280: Epoch 25
2024-12-21 04:42:07.006132: Current learning rate: 0.00849
2024-12-21 04:48:58.447083: Validation loss did not improve from -0.48671. Patience: 2/50
2024-12-21 04:48:58.447861: train_loss -0.5947
2024-12-21 04:48:58.448628: val_loss -0.4826
2024-12-21 04:48:58.449304: Pseudo dice [0.7097]
2024-12-21 04:48:58.449957: Epoch time: 411.45 s
2024-12-21 04:48:58.450588: Yayy! New best EMA pseudo Dice: 0.6733
2024-12-21 04:49:00.246599: 
2024-12-21 04:49:00.247821: Epoch 26
2024-12-21 04:49:00.248493: Current learning rate: 0.00843
2024-12-21 04:55:58.122206: Validation loss did not improve from -0.48671. Patience: 3/50
2024-12-21 04:55:58.122892: train_loss -0.6039
2024-12-21 04:55:58.123639: val_loss -0.476
2024-12-21 04:55:58.124388: Pseudo dice [0.7112]
2024-12-21 04:55:58.125071: Epoch time: 417.88 s
2024-12-21 04:55:58.125745: Yayy! New best EMA pseudo Dice: 0.6771
2024-12-21 04:55:59.937589: 
2024-12-21 04:55:59.938900: Epoch 27
2024-12-21 04:55:59.939604: Current learning rate: 0.00836
2024-12-21 05:02:59.090948: Validation loss did not improve from -0.48671. Patience: 4/50
2024-12-21 05:02:59.093379: train_loss -0.6128
2024-12-21 05:02:59.094857: val_loss -0.4513
2024-12-21 05:02:59.095644: Pseudo dice [0.7021]
2024-12-21 05:02:59.096822: Epoch time: 419.16 s
2024-12-21 05:02:59.097683: Yayy! New best EMA pseudo Dice: 0.6796
2024-12-21 05:03:00.923176: 
2024-12-21 05:03:00.924330: Epoch 28
2024-12-21 05:03:00.925014: Current learning rate: 0.0083
2024-12-21 05:10:01.858107: Validation loss did not improve from -0.48671. Patience: 5/50
2024-12-21 05:10:01.859216: train_loss -0.6259
2024-12-21 05:10:01.860013: val_loss -0.4686
2024-12-21 05:10:01.860728: Pseudo dice [0.7055]
2024-12-21 05:10:01.861481: Epoch time: 420.94 s
2024-12-21 05:10:01.862162: Yayy! New best EMA pseudo Dice: 0.6822
2024-12-21 05:10:03.634955: 
2024-12-21 05:10:03.636241: Epoch 29
2024-12-21 05:10:03.637129: Current learning rate: 0.00824
2024-12-21 05:16:58.895465: Validation loss did not improve from -0.48671. Patience: 6/50
2024-12-21 05:16:58.898391: train_loss -0.6226
2024-12-21 05:16:58.899322: val_loss -0.4753
2024-12-21 05:16:58.900111: Pseudo dice [0.7055]
2024-12-21 05:16:58.900879: Epoch time: 415.26 s
2024-12-21 05:16:59.819062: Yayy! New best EMA pseudo Dice: 0.6845
2024-12-21 05:17:01.615250: 
2024-12-21 05:17:01.616714: Epoch 30
2024-12-21 05:17:01.617691: Current learning rate: 0.00818
2024-12-21 05:24:16.381027: Validation loss did not improve from -0.48671. Patience: 7/50
2024-12-21 05:24:16.381815: train_loss -0.6273
2024-12-21 05:24:16.382581: val_loss -0.4588
2024-12-21 05:24:16.383338: Pseudo dice [0.6944]
2024-12-21 05:24:16.384062: Epoch time: 434.77 s
2024-12-21 05:24:16.384808: Yayy! New best EMA pseudo Dice: 0.6855
2024-12-21 05:24:18.272016: 
2024-12-21 05:24:18.273375: Epoch 31
2024-12-21 05:24:18.274090: Current learning rate: 0.00812
2024-12-21 05:31:17.768882: Validation loss did not improve from -0.48671. Patience: 8/50
2024-12-21 05:31:17.769820: train_loss -0.6246
2024-12-21 05:31:17.770708: val_loss -0.4711
2024-12-21 05:31:17.771546: Pseudo dice [0.7013]
2024-12-21 05:31:17.772445: Epoch time: 419.5 s
2024-12-21 05:31:17.773232: Yayy! New best EMA pseudo Dice: 0.6871
2024-12-21 05:31:19.614693: 
2024-12-21 05:31:19.615791: Epoch 32
2024-12-21 05:31:19.616736: Current learning rate: 0.00806
2024-12-21 05:38:05.561329: Validation loss did not improve from -0.48671. Patience: 9/50
2024-12-21 05:38:05.562049: train_loss -0.6347
2024-12-21 05:38:05.562771: val_loss -0.428
2024-12-21 05:38:05.563391: Pseudo dice [0.6818]
2024-12-21 05:38:05.564034: Epoch time: 405.95 s
2024-12-21 05:38:07.035196: 
2024-12-21 05:38:07.036491: Epoch 33
2024-12-21 05:38:07.037158: Current learning rate: 0.008
2024-12-21 05:45:06.007939: Validation loss improved from -0.48671 to -0.48840! Patience: 9/50
2024-12-21 05:45:06.009020: train_loss -0.626
2024-12-21 05:45:06.009823: val_loss -0.4884
2024-12-21 05:45:06.010526: Pseudo dice [0.7084]
2024-12-21 05:45:06.011510: Epoch time: 418.98 s
2024-12-21 05:45:06.012343: Yayy! New best EMA pseudo Dice: 0.6887
2024-12-21 05:45:07.917993: 
2024-12-21 05:45:07.919232: Epoch 34
2024-12-21 05:45:07.919930: Current learning rate: 0.00793
2024-12-21 05:51:56.966815: Validation loss did not improve from -0.48840. Patience: 1/50
2024-12-21 05:51:56.967883: train_loss -0.642
2024-12-21 05:51:56.969083: val_loss -0.4862
2024-12-21 05:51:56.969817: Pseudo dice [0.7169]
2024-12-21 05:51:56.970591: Epoch time: 409.05 s
2024-12-21 05:51:57.429147: Yayy! New best EMA pseudo Dice: 0.6916
2024-12-21 05:51:59.246253: 
2024-12-21 05:51:59.247412: Epoch 35
2024-12-21 05:51:59.248183: Current learning rate: 0.00787
2024-12-21 05:58:50.143161: Validation loss improved from -0.48840 to -0.48986! Patience: 1/50
2024-12-21 05:58:50.144157: train_loss -0.6383
2024-12-21 05:58:50.144928: val_loss -0.4899
2024-12-21 05:58:50.145648: Pseudo dice [0.7154]
2024-12-21 05:58:50.146574: Epoch time: 410.9 s
2024-12-21 05:58:50.147349: Yayy! New best EMA pseudo Dice: 0.6939
2024-12-21 05:58:51.998788: 
2024-12-21 05:58:51.999760: Epoch 36
2024-12-21 05:58:52.000457: Current learning rate: 0.00781
2024-12-21 06:05:48.038530: Validation loss improved from -0.48986 to -0.49169! Patience: 0/50
2024-12-21 06:05:48.040859: train_loss -0.6503
2024-12-21 06:05:48.042576: val_loss -0.4917
2024-12-21 06:05:48.043352: Pseudo dice [0.7214]
2024-12-21 06:05:48.044352: Epoch time: 416.04 s
2024-12-21 06:05:48.045285: Yayy! New best EMA pseudo Dice: 0.6967
2024-12-21 06:05:50.012646: 
2024-12-21 06:05:50.013893: Epoch 37
2024-12-21 06:05:50.014634: Current learning rate: 0.00775
2024-12-21 06:12:53.654000: Validation loss did not improve from -0.49169. Patience: 1/50
2024-12-21 06:12:53.655402: train_loss -0.6552
2024-12-21 06:12:53.656235: val_loss -0.4894
2024-12-21 06:12:53.657061: Pseudo dice [0.7249]
2024-12-21 06:12:53.658038: Epoch time: 423.64 s
2024-12-21 06:12:53.658798: Yayy! New best EMA pseudo Dice: 0.6995
2024-12-21 06:12:55.546544: 
2024-12-21 06:12:55.547823: Epoch 38
2024-12-21 06:12:55.548656: Current learning rate: 0.00769
2024-12-21 06:19:54.008673: Validation loss did not improve from -0.49169. Patience: 2/50
2024-12-21 06:19:54.011253: train_loss -0.6539
2024-12-21 06:19:54.012081: val_loss -0.4819
2024-12-21 06:19:54.012831: Pseudo dice [0.7158]
2024-12-21 06:19:54.013643: Epoch time: 418.47 s
2024-12-21 06:19:54.014479: Yayy! New best EMA pseudo Dice: 0.7011
2024-12-21 06:19:55.867262: 
2024-12-21 06:19:55.868343: Epoch 39
2024-12-21 06:19:55.869018: Current learning rate: 0.00763
2024-12-21 06:26:50.412127: Validation loss did not improve from -0.49169. Patience: 3/50
2024-12-21 06:26:50.413265: train_loss -0.66
2024-12-21 06:26:50.414093: val_loss -0.479
2024-12-21 06:26:50.414956: Pseudo dice [0.7185]
2024-12-21 06:26:50.415663: Epoch time: 414.55 s
2024-12-21 06:26:50.802029: Yayy! New best EMA pseudo Dice: 0.7029
2024-12-21 06:26:53.538507: 
2024-12-21 06:26:53.539731: Epoch 40
2024-12-21 06:26:53.540374: Current learning rate: 0.00756
2024-12-21 06:34:01.495913: Validation loss improved from -0.49169 to -0.52567! Patience: 3/50
2024-12-21 06:34:01.497143: train_loss -0.6502
2024-12-21 06:34:01.497948: val_loss -0.5257
2024-12-21 06:34:01.498636: Pseudo dice [0.7349]
2024-12-21 06:34:01.499331: Epoch time: 427.96 s
2024-12-21 06:34:01.499952: Yayy! New best EMA pseudo Dice: 0.7061
2024-12-21 06:34:03.428228: 
2024-12-21 06:34:03.429270: Epoch 41
2024-12-21 06:34:03.429990: Current learning rate: 0.0075
2024-12-21 06:41:09.049329: Validation loss did not improve from -0.52567. Patience: 1/50
2024-12-21 06:41:09.050399: train_loss -0.6596
2024-12-21 06:41:09.051363: val_loss -0.5238
2024-12-21 06:41:09.052206: Pseudo dice [0.7352]
2024-12-21 06:41:09.053071: Epoch time: 425.62 s
2024-12-21 06:41:09.053821: Yayy! New best EMA pseudo Dice: 0.709
2024-12-21 06:41:10.941390: 
2024-12-21 06:41:10.942753: Epoch 42
2024-12-21 06:41:10.943575: Current learning rate: 0.00744
2024-12-21 06:48:06.192719: Validation loss did not improve from -0.52567. Patience: 2/50
2024-12-21 06:48:06.193700: train_loss -0.6602
2024-12-21 06:48:06.194543: val_loss -0.4811
2024-12-21 06:48:06.195262: Pseudo dice [0.7123]
2024-12-21 06:48:06.195936: Epoch time: 415.25 s
2024-12-21 06:48:06.196571: Yayy! New best EMA pseudo Dice: 0.7093
2024-12-21 06:48:07.975870: 
2024-12-21 06:48:07.977152: Epoch 43
2024-12-21 06:48:07.977816: Current learning rate: 0.00738
2024-12-21 06:55:24.373019: Validation loss did not improve from -0.52567. Patience: 3/50
2024-12-21 06:55:24.374158: train_loss -0.6749
2024-12-21 06:55:24.374974: val_loss -0.5117
2024-12-21 06:55:24.375612: Pseudo dice [0.7229]
2024-12-21 06:55:24.376264: Epoch time: 436.4 s
2024-12-21 06:55:24.377027: Yayy! New best EMA pseudo Dice: 0.7107
2024-12-21 06:55:26.151059: 
2024-12-21 06:55:26.152894: Epoch 44
2024-12-21 06:55:26.153907: Current learning rate: 0.00732
2024-12-21 07:02:03.574099: Validation loss did not improve from -0.52567. Patience: 4/50
2024-12-21 07:02:03.574988: train_loss -0.6727
2024-12-21 07:02:03.575907: val_loss -0.4798
2024-12-21 07:02:03.576686: Pseudo dice [0.7128]
2024-12-21 07:02:03.577483: Epoch time: 397.43 s
2024-12-21 07:02:03.996699: Yayy! New best EMA pseudo Dice: 0.7109
2024-12-21 07:02:05.777300: 
2024-12-21 07:02:05.778612: Epoch 45
2024-12-21 07:02:05.779416: Current learning rate: 0.00725
2024-12-21 07:08:58.165704: Validation loss did not improve from -0.52567. Patience: 5/50
2024-12-21 07:08:58.167987: train_loss -0.6721
2024-12-21 07:08:58.169624: val_loss -0.5019
2024-12-21 07:08:58.170500: Pseudo dice [0.7201]
2024-12-21 07:08:58.171661: Epoch time: 412.39 s
2024-12-21 07:08:58.172714: Yayy! New best EMA pseudo Dice: 0.7118
2024-12-21 07:08:59.982293: 
2024-12-21 07:08:59.983708: Epoch 46
2024-12-21 07:08:59.984684: Current learning rate: 0.00719
2024-12-21 07:16:00.413366: Validation loss did not improve from -0.52567. Patience: 6/50
2024-12-21 07:16:00.418241: train_loss -0.6716
2024-12-21 07:16:00.427068: val_loss -0.4998
2024-12-21 07:16:00.428046: Pseudo dice [0.7267]
2024-12-21 07:16:00.428747: Epoch time: 420.43 s
2024-12-21 07:16:00.429429: Yayy! New best EMA pseudo Dice: 0.7133
2024-12-21 07:16:02.330894: 
2024-12-21 07:16:02.332149: Epoch 47
2024-12-21 07:16:02.332930: Current learning rate: 0.00713
2024-12-21 07:22:54.859227: Validation loss did not improve from -0.52567. Patience: 7/50
2024-12-21 07:22:54.861701: train_loss -0.6706
2024-12-21 07:22:54.862660: val_loss -0.4871
2024-12-21 07:22:54.863361: Pseudo dice [0.7106]
2024-12-21 07:22:54.864096: Epoch time: 412.53 s
2024-12-21 07:22:56.236827: 
2024-12-21 07:22:56.237981: Epoch 48
2024-12-21 07:22:56.238719: Current learning rate: 0.00707
2024-12-21 07:30:03.270566: Validation loss did not improve from -0.52567. Patience: 8/50
2024-12-21 07:30:03.271800: train_loss -0.6824
2024-12-21 07:30:03.272956: val_loss -0.484
2024-12-21 07:30:03.273719: Pseudo dice [0.7141]
2024-12-21 07:30:03.274533: Epoch time: 427.04 s
2024-12-21 07:30:04.628565: 
2024-12-21 07:30:04.629802: Epoch 49
2024-12-21 07:30:04.630537: Current learning rate: 0.007
2024-12-21 07:36:53.949639: Validation loss did not improve from -0.52567. Patience: 9/50
2024-12-21 07:36:53.951096: train_loss -0.6816
2024-12-21 07:36:53.951980: val_loss -0.5011
2024-12-21 07:36:53.952742: Pseudo dice [0.7304]
2024-12-21 07:36:53.953497: Epoch time: 409.32 s
2024-12-21 07:36:54.300243: Yayy! New best EMA pseudo Dice: 0.7149
2024-12-21 07:36:56.082167: 
2024-12-21 07:36:56.083486: Epoch 50
2024-12-21 07:36:56.084300: Current learning rate: 0.00694
2024-12-21 07:43:45.141536: Validation loss did not improve from -0.52567. Patience: 10/50
2024-12-21 07:43:45.142324: train_loss -0.6918
2024-12-21 07:43:45.143141: val_loss -0.5091
2024-12-21 07:43:45.143774: Pseudo dice [0.7232]
2024-12-21 07:43:45.144579: Epoch time: 409.06 s
2024-12-21 07:43:45.145404: Yayy! New best EMA pseudo Dice: 0.7157
2024-12-21 07:43:47.917915: 
2024-12-21 07:43:47.919207: Epoch 51
2024-12-21 07:43:47.920020: Current learning rate: 0.00688
2024-12-21 07:50:43.340023: Validation loss did not improve from -0.52567. Patience: 11/50
2024-12-21 07:50:43.341001: train_loss -0.6911
2024-12-21 07:50:43.341856: val_loss -0.5013
2024-12-21 07:50:43.342591: Pseudo dice [0.7208]
2024-12-21 07:50:43.343364: Epoch time: 415.42 s
2024-12-21 07:50:43.344067: Yayy! New best EMA pseudo Dice: 0.7162
2024-12-21 07:50:45.210075: 
2024-12-21 07:50:45.211433: Epoch 52
2024-12-21 07:50:45.212474: Current learning rate: 0.00682
2024-12-21 07:57:34.708700: Validation loss did not improve from -0.52567. Patience: 12/50
2024-12-21 07:57:34.709428: train_loss -0.6906
2024-12-21 07:57:34.710094: val_loss -0.5062
2024-12-21 07:57:34.710744: Pseudo dice [0.7223]
2024-12-21 07:57:34.711516: Epoch time: 409.5 s
2024-12-21 07:57:34.712101: Yayy! New best EMA pseudo Dice: 0.7168
2024-12-21 07:57:36.572428: 
2024-12-21 07:57:36.573722: Epoch 53
2024-12-21 07:57:36.574383: Current learning rate: 0.00675
2024-12-21 08:04:36.257273: Validation loss did not improve from -0.52567. Patience: 13/50
2024-12-21 08:04:36.257895: train_loss -0.6936
2024-12-21 08:04:36.258702: val_loss -0.5069
2024-12-21 08:04:36.259330: Pseudo dice [0.731]
2024-12-21 08:04:36.259979: Epoch time: 419.69 s
2024-12-21 08:04:36.260625: Yayy! New best EMA pseudo Dice: 0.7182
2024-12-21 08:04:38.061853: 
2024-12-21 08:04:38.062856: Epoch 54
2024-12-21 08:04:38.063603: Current learning rate: 0.00669
2024-12-21 08:11:26.168903: Validation loss did not improve from -0.52567. Patience: 14/50
2024-12-21 08:11:26.175802: train_loss -0.6948
2024-12-21 08:11:26.176945: val_loss -0.4905
2024-12-21 08:11:26.177686: Pseudo dice [0.7154]
2024-12-21 08:11:26.178388: Epoch time: 408.11 s
2024-12-21 08:11:28.073926: 
2024-12-21 08:11:28.075297: Epoch 55
2024-12-21 08:11:28.076016: Current learning rate: 0.00663
2024-12-21 08:18:19.722660: Validation loss did not improve from -0.52567. Patience: 15/50
2024-12-21 08:18:19.724488: train_loss -0.6927
2024-12-21 08:18:19.726699: val_loss -0.4892
2024-12-21 08:18:19.727755: Pseudo dice [0.7085]
2024-12-21 08:18:19.729342: Epoch time: 411.65 s
2024-12-21 08:18:21.129711: 
2024-12-21 08:18:21.130810: Epoch 56
2024-12-21 08:18:21.131841: Current learning rate: 0.00657
2024-12-21 08:25:17.156960: Validation loss improved from -0.52567 to -0.54128! Patience: 15/50
2024-12-21 08:25:17.158004: train_loss -0.7034
2024-12-21 08:25:17.159104: val_loss -0.5413
2024-12-21 08:25:17.159758: Pseudo dice [0.7494]
2024-12-21 08:25:17.160410: Epoch time: 416.03 s
2024-12-21 08:25:17.161038: Yayy! New best EMA pseudo Dice: 0.7202
2024-12-21 08:25:18.990247: 
2024-12-21 08:25:18.991401: Epoch 57
2024-12-21 08:25:18.992182: Current learning rate: 0.0065
2024-12-21 08:32:13.617644: Validation loss did not improve from -0.54128. Patience: 1/50
2024-12-21 08:32:13.620622: train_loss -0.7097
2024-12-21 08:32:13.621641: val_loss -0.52
2024-12-21 08:32:13.622306: Pseudo dice [0.7265]
2024-12-21 08:32:13.623109: Epoch time: 414.63 s
2024-12-21 08:32:13.623931: Yayy! New best EMA pseudo Dice: 0.7209
2024-12-21 08:32:15.412756: 
2024-12-21 08:32:15.414021: Epoch 58
2024-12-21 08:32:15.414780: Current learning rate: 0.00644
2024-12-21 08:39:11.604480: Validation loss did not improve from -0.54128. Patience: 2/50
2024-12-21 08:39:11.605476: train_loss -0.7026
2024-12-21 08:39:11.606288: val_loss -0.5028
2024-12-21 08:39:11.606979: Pseudo dice [0.7229]
2024-12-21 08:39:11.607619: Epoch time: 416.19 s
2024-12-21 08:39:11.608298: Yayy! New best EMA pseudo Dice: 0.7211
2024-12-21 08:39:13.444455: 
2024-12-21 08:39:13.446135: Epoch 59
2024-12-21 08:39:13.447018: Current learning rate: 0.00638
2024-12-21 08:46:11.310984: Validation loss did not improve from -0.54128. Patience: 3/50
2024-12-21 08:46:11.311982: train_loss -0.7006
2024-12-21 08:46:11.313011: val_loss -0.4754
2024-12-21 08:46:11.313909: Pseudo dice [0.7052]
2024-12-21 08:46:11.314749: Epoch time: 417.87 s
2024-12-21 08:46:13.178303: 
2024-12-21 08:46:13.179487: Epoch 60
2024-12-21 08:46:13.180188: Current learning rate: 0.00631
2024-12-21 08:53:01.736108: Validation loss did not improve from -0.54128. Patience: 4/50
2024-12-21 08:53:01.737894: train_loss -0.7143
2024-12-21 08:53:01.739051: val_loss -0.4981
2024-12-21 08:53:01.740024: Pseudo dice [0.7244]
2024-12-21 08:53:01.740863: Epoch time: 408.56 s
2024-12-21 08:53:03.225250: 
2024-12-21 08:53:03.226857: Epoch 61
2024-12-21 08:53:03.227826: Current learning rate: 0.00625
2024-12-21 08:59:59.975434: Validation loss did not improve from -0.54128. Patience: 5/50
2024-12-21 08:59:59.976460: train_loss -0.7126
2024-12-21 08:59:59.977198: val_loss -0.4955
2024-12-21 08:59:59.977858: Pseudo dice [0.7272]
2024-12-21 08:59:59.978444: Epoch time: 416.75 s
2024-12-21 09:00:02.045038: 
2024-12-21 09:00:02.046986: Epoch 62
2024-12-21 09:00:02.048014: Current learning rate: 0.00619
2024-12-21 09:06:53.333943: Validation loss did not improve from -0.54128. Patience: 6/50
2024-12-21 09:06:53.335037: train_loss -0.7161
2024-12-21 09:06:53.336010: val_loss -0.5052
2024-12-21 09:06:53.336907: Pseudo dice [0.7256]
2024-12-21 09:06:53.337845: Epoch time: 411.29 s
2024-12-21 09:06:53.338679: Yayy! New best EMA pseudo Dice: 0.7212
2024-12-21 09:06:55.117922: 
2024-12-21 09:06:55.119438: Epoch 63
2024-12-21 09:06:55.120421: Current learning rate: 0.00612
2024-12-21 09:13:45.700544: Validation loss did not improve from -0.54128. Patience: 7/50
2024-12-21 09:13:45.701355: train_loss -0.713
2024-12-21 09:13:45.702059: val_loss -0.5407
2024-12-21 09:13:45.702823: Pseudo dice [0.7417]
2024-12-21 09:13:45.703420: Epoch time: 410.58 s
2024-12-21 09:13:45.703974: Yayy! New best EMA pseudo Dice: 0.7232
2024-12-21 09:13:47.503034: 
2024-12-21 09:13:47.504221: Epoch 64
2024-12-21 09:13:47.504985: Current learning rate: 0.00606
2024-12-21 09:20:38.279657: Validation loss did not improve from -0.54128. Patience: 8/50
2024-12-21 09:20:38.282083: train_loss -0.716
2024-12-21 09:20:38.283770: val_loss -0.4634
2024-12-21 09:20:38.284533: Pseudo dice [0.7095]
2024-12-21 09:20:38.285633: Epoch time: 410.78 s
2024-12-21 09:20:40.012938: 
2024-12-21 09:20:40.014333: Epoch 65
2024-12-21 09:20:40.015106: Current learning rate: 0.006
2024-12-21 09:27:38.869604: Validation loss did not improve from -0.54128. Patience: 9/50
2024-12-21 09:27:38.871032: train_loss -0.712
2024-12-21 09:27:38.872058: val_loss -0.4872
2024-12-21 09:27:38.872811: Pseudo dice [0.7138]
2024-12-21 09:27:38.873659: Epoch time: 418.86 s
2024-12-21 09:27:40.267581: 
2024-12-21 09:27:40.268984: Epoch 66
2024-12-21 09:27:40.269766: Current learning rate: 0.00593
2024-12-21 09:34:42.636501: Validation loss did not improve from -0.54128. Patience: 10/50
2024-12-21 09:34:42.638902: train_loss -0.7161
2024-12-21 09:34:42.639628: val_loss -0.5133
2024-12-21 09:34:42.640282: Pseudo dice [0.733]
2024-12-21 09:34:42.640999: Epoch time: 422.37 s
2024-12-21 09:34:44.070884: 
2024-12-21 09:34:44.072114: Epoch 67
2024-12-21 09:34:44.072764: Current learning rate: 0.00587
2024-12-21 09:42:16.078602: Validation loss did not improve from -0.54128. Patience: 11/50
2024-12-21 09:42:16.080149: train_loss -0.7186
2024-12-21 09:42:16.081393: val_loss -0.503
2024-12-21 09:42:16.081975: Pseudo dice [0.7245]
2024-12-21 09:42:16.082841: Epoch time: 452.01 s
2024-12-21 09:42:17.521272: 
2024-12-21 09:42:17.522007: Epoch 68
2024-12-21 09:42:17.522640: Current learning rate: 0.00581
2024-12-21 09:49:49.131474: Validation loss did not improve from -0.54128. Patience: 12/50
2024-12-21 09:49:49.132538: train_loss -0.7288
2024-12-21 09:49:49.134064: val_loss -0.5285
2024-12-21 09:49:49.135074: Pseudo dice [0.7384]
2024-12-21 09:49:49.136149: Epoch time: 451.61 s
2024-12-21 09:49:49.137154: Yayy! New best EMA pseudo Dice: 0.7241
2024-12-21 09:49:50.865335: 
2024-12-21 09:49:50.867085: Epoch 69
2024-12-21 09:49:50.868361: Current learning rate: 0.00574
2024-12-21 09:56:53.326748: Validation loss did not improve from -0.54128. Patience: 13/50
2024-12-21 09:56:53.328444: train_loss -0.7289
2024-12-21 09:56:53.329584: val_loss -0.5234
2024-12-21 09:56:53.330554: Pseudo dice [0.7443]
2024-12-21 09:56:53.331417: Epoch time: 422.46 s
2024-12-21 09:56:53.705646: Yayy! New best EMA pseudo Dice: 0.7261
2024-12-21 09:56:55.463341: 
2024-12-21 09:56:55.464665: Epoch 70
2024-12-21 09:56:55.465625: Current learning rate: 0.00568
2024-12-21 10:03:42.119058: Validation loss did not improve from -0.54128. Patience: 14/50
2024-12-21 10:03:42.120001: train_loss -0.7284
2024-12-21 10:03:42.120790: val_loss -0.4908
2024-12-21 10:03:42.121494: Pseudo dice [0.7288]
2024-12-21 10:03:42.122280: Epoch time: 406.66 s
2024-12-21 10:03:42.122949: Yayy! New best EMA pseudo Dice: 0.7263
2024-12-21 10:03:44.009767: 
2024-12-21 10:03:44.010881: Epoch 71
2024-12-21 10:03:44.011558: Current learning rate: 0.00562
2024-12-21 10:10:32.923994: Validation loss did not improve from -0.54128. Patience: 15/50
2024-12-21 10:10:32.925561: train_loss -0.7288
2024-12-21 10:10:32.926678: val_loss -0.4731
2024-12-21 10:10:32.927336: Pseudo dice [0.711]
2024-12-21 10:10:32.928127: Epoch time: 408.92 s
2024-12-21 10:10:34.339285: 
2024-12-21 10:10:34.340591: Epoch 72
2024-12-21 10:10:34.341383: Current learning rate: 0.00555
2024-12-21 10:17:30.462720: Validation loss did not improve from -0.54128. Patience: 16/50
2024-12-21 10:17:30.463762: train_loss -0.7321
2024-12-21 10:17:30.464591: val_loss -0.4899
2024-12-21 10:17:30.465269: Pseudo dice [0.7192]
2024-12-21 10:17:30.465944: Epoch time: 416.13 s
2024-12-21 10:17:32.547212: 
2024-12-21 10:17:32.548304: Epoch 73
2024-12-21 10:17:32.548956: Current learning rate: 0.00549
2024-12-21 10:24:36.041061: Validation loss did not improve from -0.54128. Patience: 17/50
2024-12-21 10:24:36.043145: train_loss -0.7342
2024-12-21 10:24:36.044575: val_loss -0.4955
2024-12-21 10:24:36.045324: Pseudo dice [0.7134]
2024-12-21 10:24:36.046401: Epoch time: 423.5 s
2024-12-21 10:24:37.495869: 
2024-12-21 10:24:37.496667: Epoch 74
2024-12-21 10:24:37.497370: Current learning rate: 0.00542
2024-12-21 10:31:14.960639: Validation loss did not improve from -0.54128. Patience: 18/50
2024-12-21 10:31:14.961794: train_loss -0.7292
2024-12-21 10:31:14.962667: val_loss -0.4977
2024-12-21 10:31:14.963290: Pseudo dice [0.7239]
2024-12-21 10:31:14.964000: Epoch time: 397.47 s
2024-12-21 10:31:16.769000: 
2024-12-21 10:31:16.769854: Epoch 75
2024-12-21 10:31:16.770512: Current learning rate: 0.00536
2024-12-21 10:38:31.725979: Validation loss did not improve from -0.54128. Patience: 19/50
2024-12-21 10:38:31.729347: train_loss -0.7352
2024-12-21 10:38:31.730127: val_loss -0.5137
2024-12-21 10:38:31.730910: Pseudo dice [0.7377]
2024-12-21 10:38:31.731718: Epoch time: 434.96 s
2024-12-21 10:38:33.164004: 
2024-12-21 10:38:33.165407: Epoch 76
2024-12-21 10:38:33.166183: Current learning rate: 0.00529
2024-12-21 10:45:27.267207: Validation loss did not improve from -0.54128. Patience: 20/50
2024-12-21 10:45:27.267948: train_loss -0.7364
2024-12-21 10:45:27.268857: val_loss -0.5185
2024-12-21 10:45:27.269799: Pseudo dice [0.7362]
2024-12-21 10:45:27.270645: Epoch time: 414.11 s
2024-12-21 10:45:28.726746: 
2024-12-21 10:45:28.727714: Epoch 77
2024-12-21 10:45:28.728636: Current learning rate: 0.00523
2024-12-21 10:52:25.788596: Validation loss did not improve from -0.54128. Patience: 21/50
2024-12-21 10:52:25.789421: train_loss -0.7393
2024-12-21 10:52:25.790181: val_loss -0.514
2024-12-21 10:52:25.790931: Pseudo dice [0.731]
2024-12-21 10:52:25.791620: Epoch time: 417.06 s
2024-12-21 10:52:25.792276: Yayy! New best EMA pseudo Dice: 0.7263
2024-12-21 10:52:27.708520: 
2024-12-21 10:52:27.709457: Epoch 78
2024-12-21 10:52:27.710744: Current learning rate: 0.00517
2024-12-21 10:59:34.746491: Validation loss did not improve from -0.54128. Patience: 22/50
2024-12-21 10:59:34.747509: train_loss -0.7442
2024-12-21 10:59:34.748355: val_loss -0.5194
2024-12-21 10:59:34.749091: Pseudo dice [0.7385]
2024-12-21 10:59:34.749787: Epoch time: 427.04 s
2024-12-21 10:59:34.750478: Yayy! New best EMA pseudo Dice: 0.7276
2024-12-21 10:59:36.573519: 
2024-12-21 10:59:36.574374: Epoch 79
2024-12-21 10:59:36.575057: Current learning rate: 0.0051
2024-12-21 11:06:31.078697: Validation loss improved from -0.54128 to -0.54636! Patience: 22/50
2024-12-21 11:06:31.079556: train_loss -0.745
2024-12-21 11:06:31.080233: val_loss -0.5464
2024-12-21 11:06:31.080912: Pseudo dice [0.7497]
2024-12-21 11:06:31.081613: Epoch time: 414.51 s
2024-12-21 11:06:31.478571: Yayy! New best EMA pseudo Dice: 0.7298
2024-12-21 11:06:33.314285: 
2024-12-21 11:06:33.315181: Epoch 80
2024-12-21 11:06:33.315860: Current learning rate: 0.00504
2024-12-21 11:13:26.426339: Validation loss did not improve from -0.54636. Patience: 1/50
2024-12-21 11:13:26.427060: train_loss -0.7426
2024-12-21 11:13:26.427806: val_loss -0.514
2024-12-21 11:13:26.428446: Pseudo dice [0.7366]
2024-12-21 11:13:26.429119: Epoch time: 413.11 s
2024-12-21 11:13:26.429792: Yayy! New best EMA pseudo Dice: 0.7305
2024-12-21 11:13:28.291009: 
2024-12-21 11:13:28.292203: Epoch 81
2024-12-21 11:13:28.292914: Current learning rate: 0.00497
2024-12-21 11:20:35.529460: Validation loss did not improve from -0.54636. Patience: 2/50
2024-12-21 11:20:35.530304: train_loss -0.7426
2024-12-21 11:20:35.531103: val_loss -0.5065
2024-12-21 11:20:35.531801: Pseudo dice [0.7189]
2024-12-21 11:20:35.532668: Epoch time: 427.24 s
2024-12-21 11:20:36.986409: 
2024-12-21 11:20:36.987517: Epoch 82
2024-12-21 11:20:36.988343: Current learning rate: 0.00491
2024-12-21 11:27:51.257533: Validation loss did not improve from -0.54636. Patience: 3/50
2024-12-21 11:27:51.260312: train_loss -0.747
2024-12-21 11:27:51.262637: val_loss -0.5177
2024-12-21 11:27:51.263643: Pseudo dice [0.7353]
2024-12-21 11:27:51.265070: Epoch time: 434.27 s
2024-12-21 11:27:52.661024: 
2024-12-21 11:27:52.662030: Epoch 83
2024-12-21 11:27:52.662761: Current learning rate: 0.00484
2024-12-21 11:34:44.066209: Validation loss did not improve from -0.54636. Patience: 4/50
2024-12-21 11:34:44.067138: train_loss -0.7472
2024-12-21 11:34:44.067896: val_loss -0.5189
2024-12-21 11:34:44.068564: Pseudo dice [0.7364]
2024-12-21 11:34:44.069224: Epoch time: 411.41 s
2024-12-21 11:34:44.070022: Yayy! New best EMA pseudo Dice: 0.7306
2024-12-21 11:34:46.328675: 
2024-12-21 11:34:46.336570: Epoch 84
2024-12-21 11:34:46.337336: Current learning rate: 0.00478
2024-12-21 11:41:46.760431: Validation loss did not improve from -0.54636. Patience: 5/50
2024-12-21 11:41:46.761206: train_loss -0.7475
2024-12-21 11:41:46.761949: val_loss -0.4951
2024-12-21 11:41:46.762667: Pseudo dice [0.734]
2024-12-21 11:41:46.763333: Epoch time: 420.43 s
2024-12-21 11:41:47.132220: Yayy! New best EMA pseudo Dice: 0.7309
2024-12-21 11:41:48.889418: 
2024-12-21 11:41:48.890626: Epoch 85
2024-12-21 11:41:48.891490: Current learning rate: 0.00471
2024-12-21 11:48:47.144310: Validation loss did not improve from -0.54636. Patience: 6/50
2024-12-21 11:48:47.146580: train_loss -0.7448
2024-12-21 11:48:47.147336: val_loss -0.4941
2024-12-21 11:48:47.148001: Pseudo dice [0.7207]
2024-12-21 11:48:47.148689: Epoch time: 418.26 s
2024-12-21 11:48:48.543780: 
2024-12-21 11:48:48.544883: Epoch 86
2024-12-21 11:48:48.545570: Current learning rate: 0.00465
2024-12-21 11:55:55.805590: Validation loss did not improve from -0.54636. Patience: 7/50
2024-12-21 11:55:55.806499: train_loss -0.7522
2024-12-21 11:55:55.807360: val_loss -0.4976
2024-12-21 11:55:55.808120: Pseudo dice [0.7216]
2024-12-21 11:55:55.808855: Epoch time: 427.26 s
2024-12-21 11:55:57.181258: 
2024-12-21 11:55:57.182357: Epoch 87
2024-12-21 11:55:57.183165: Current learning rate: 0.00458
2024-12-21 12:02:44.111184: Validation loss did not improve from -0.54636. Patience: 8/50
2024-12-21 12:02:44.112171: train_loss -0.7522
2024-12-21 12:02:44.113081: val_loss -0.5117
2024-12-21 12:02:44.113987: Pseudo dice [0.731]
2024-12-21 12:02:44.114948: Epoch time: 406.93 s
2024-12-21 12:02:45.470770: 
2024-12-21 12:02:45.471961: Epoch 88
2024-12-21 12:02:45.472841: Current learning rate: 0.00452
2024-12-21 12:10:02.126962: Validation loss did not improve from -0.54636. Patience: 9/50
2024-12-21 12:10:02.127969: train_loss -0.7557
2024-12-21 12:10:02.129053: val_loss -0.4795
2024-12-21 12:10:02.130194: Pseudo dice [0.7112]
2024-12-21 12:10:02.131171: Epoch time: 436.66 s
2024-12-21 12:10:03.541113: 
2024-12-21 12:10:03.542088: Epoch 89
2024-12-21 12:10:03.542831: Current learning rate: 0.00445
2024-12-21 12:16:49.347510: Validation loss did not improve from -0.54636. Patience: 10/50
2024-12-21 12:16:49.348804: train_loss -0.7587
2024-12-21 12:16:49.349611: val_loss -0.5335
2024-12-21 12:16:49.350298: Pseudo dice [0.7442]
2024-12-21 12:16:49.351205: Epoch time: 405.81 s
2024-12-21 12:16:51.220546: 
2024-12-21 12:16:51.221668: Epoch 90
2024-12-21 12:16:51.222518: Current learning rate: 0.00438
2024-12-21 12:23:53.467676: Validation loss did not improve from -0.54636. Patience: 11/50
2024-12-21 12:23:53.468677: train_loss -0.756
2024-12-21 12:23:53.469456: val_loss -0.5078
2024-12-21 12:23:53.470110: Pseudo dice [0.7293]
2024-12-21 12:23:53.470855: Epoch time: 422.25 s
2024-12-21 12:23:54.833286: 
2024-12-21 12:23:54.834515: Epoch 91
2024-12-21 12:23:54.835210: Current learning rate: 0.00432
2024-12-21 12:31:11.719097: Validation loss did not improve from -0.54636. Patience: 12/50
2024-12-21 12:31:11.720036: train_loss -0.7564
2024-12-21 12:31:11.721110: val_loss -0.515
2024-12-21 12:31:11.721712: Pseudo dice [0.7393]
2024-12-21 12:31:11.722320: Epoch time: 436.89 s
2024-12-21 12:31:13.164185: 
2024-12-21 12:31:13.165835: Epoch 92
2024-12-21 12:31:13.167236: Current learning rate: 0.00425
2024-12-21 12:38:21.991360: Validation loss did not improve from -0.54636. Patience: 13/50
2024-12-21 12:38:22.021857: train_loss -0.7575
2024-12-21 12:38:22.023782: val_loss -0.5155
2024-12-21 12:38:22.024861: Pseudo dice [0.7358]
2024-12-21 12:38:22.026276: Epoch time: 428.86 s
2024-12-21 12:38:23.529744: 
2024-12-21 12:38:23.530730: Epoch 93
2024-12-21 12:38:23.531578: Current learning rate: 0.00419
2024-12-21 12:45:08.614349: Validation loss did not improve from -0.54636. Patience: 14/50
2024-12-21 12:45:08.616398: train_loss -0.7608
2024-12-21 12:45:08.617248: val_loss -0.5077
2024-12-21 12:45:08.617945: Pseudo dice [0.734]
2024-12-21 12:45:08.618649: Epoch time: 405.09 s
2024-12-21 12:45:08.619399: Yayy! New best EMA pseudo Dice: 0.7311
2024-12-21 12:45:10.460838: 
2024-12-21 12:45:10.461975: Epoch 94
2024-12-21 12:45:10.462637: Current learning rate: 0.00412
2024-12-21 12:50:30.282984: Validation loss did not improve from -0.54636. Patience: 15/50
2024-12-21 12:50:30.285040: train_loss -0.7559
2024-12-21 12:50:30.285754: val_loss -0.4862
2024-12-21 12:50:30.286416: Pseudo dice [0.7261]
2024-12-21 12:50:30.287033: Epoch time: 319.83 s
2024-12-21 12:50:31.997757: 
2024-12-21 12:50:31.998940: Epoch 95
2024-12-21 12:50:31.999638: Current learning rate: 0.00405
2024-12-21 12:52:07.926827: Validation loss did not improve from -0.54636. Patience: 16/50
2024-12-21 12:52:07.927790: train_loss -0.7612
2024-12-21 12:52:07.928834: val_loss -0.462
2024-12-21 12:52:07.930042: Pseudo dice [0.7161]
2024-12-21 12:52:07.930820: Epoch time: 95.93 s
2024-12-21 12:52:09.799693: 
2024-12-21 12:52:09.801314: Epoch 96
2024-12-21 12:52:09.802135: Current learning rate: 0.00399
2024-12-21 12:53:45.201827: Validation loss did not improve from -0.54636. Patience: 17/50
2024-12-21 12:53:45.202711: train_loss -0.7641
2024-12-21 12:53:45.203546: val_loss -0.4836
2024-12-21 12:53:45.204554: Pseudo dice [0.721]
2024-12-21 12:53:45.205416: Epoch time: 95.4 s
2024-12-21 12:53:46.574753: 
2024-12-21 12:53:46.575913: Epoch 97
2024-12-21 12:53:46.576605: Current learning rate: 0.00392
2024-12-21 12:55:22.398266: Validation loss did not improve from -0.54636. Patience: 18/50
2024-12-21 12:55:22.399372: train_loss -0.7619
2024-12-21 12:55:22.400171: val_loss -0.5289
2024-12-21 12:55:22.400797: Pseudo dice [0.7384]
2024-12-21 12:55:22.401566: Epoch time: 95.83 s
2024-12-21 12:55:23.705204: 
2024-12-21 12:55:23.706851: Epoch 98
2024-12-21 12:55:23.707570: Current learning rate: 0.00385
2024-12-21 12:56:55.720151: Validation loss did not improve from -0.54636. Patience: 19/50
2024-12-21 12:56:55.721055: train_loss -0.7653
2024-12-21 12:56:55.721868: val_loss -0.4977
2024-12-21 12:56:55.722584: Pseudo dice [0.7269]
2024-12-21 12:56:55.723335: Epoch time: 92.02 s
2024-12-21 12:56:57.098916: 
2024-12-21 12:56:57.100104: Epoch 99
2024-12-21 12:56:57.100791: Current learning rate: 0.00379
2024-12-21 12:58:31.725092: Validation loss did not improve from -0.54636. Patience: 20/50
2024-12-21 12:58:31.726094: train_loss -0.7666
2024-12-21 12:58:31.727094: val_loss -0.5092
2024-12-21 12:58:31.727985: Pseudo dice [0.739]
2024-12-21 12:58:31.728763: Epoch time: 94.63 s
2024-12-21 12:58:33.391709: 
2024-12-21 12:58:33.393396: Epoch 100
2024-12-21 12:58:33.394224: Current learning rate: 0.00372
2024-12-21 13:00:08.374875: Validation loss did not improve from -0.54636. Patience: 21/50
2024-12-21 13:00:08.375965: train_loss -0.7694
2024-12-21 13:00:08.376776: val_loss -0.512
2024-12-21 13:00:08.377642: Pseudo dice [0.7413]
2024-12-21 13:00:08.378443: Epoch time: 94.99 s
2024-12-21 13:00:08.379231: Yayy! New best EMA pseudo Dice: 0.7312
2024-12-21 13:00:10.095277: 
2024-12-21 13:00:10.096739: Epoch 101
2024-12-21 13:00:10.097449: Current learning rate: 0.00365
2024-12-21 13:01:45.881177: Validation loss did not improve from -0.54636. Patience: 22/50
2024-12-21 13:01:45.882125: train_loss -0.7668
2024-12-21 13:01:45.883160: val_loss -0.5115
2024-12-21 13:01:45.883815: Pseudo dice [0.7326]
2024-12-21 13:01:45.884580: Epoch time: 95.79 s
2024-12-21 13:01:45.885316: Yayy! New best EMA pseudo Dice: 0.7313
2024-12-21 13:01:47.521082: 
2024-12-21 13:01:47.522349: Epoch 102
2024-12-21 13:01:47.523163: Current learning rate: 0.00359
2024-12-21 13:03:22.857056: Validation loss did not improve from -0.54636. Patience: 23/50
2024-12-21 13:03:22.858106: train_loss -0.7713
2024-12-21 13:03:22.859072: val_loss -0.488
2024-12-21 13:03:22.860008: Pseudo dice [0.7358]
2024-12-21 13:03:22.860867: Epoch time: 95.34 s
2024-12-21 13:03:22.861848: Yayy! New best EMA pseudo Dice: 0.7318
2024-12-21 13:03:24.642433: 
2024-12-21 13:03:24.644142: Epoch 103
2024-12-21 13:03:24.644920: Current learning rate: 0.00352
2024-12-21 13:04:56.204831: Validation loss did not improve from -0.54636. Patience: 24/50
2024-12-21 13:04:56.205622: train_loss -0.7724
2024-12-21 13:04:56.206429: val_loss -0.5117
2024-12-21 13:04:56.207114: Pseudo dice [0.7394]
2024-12-21 13:04:56.207791: Epoch time: 91.56 s
2024-12-21 13:04:56.208439: Yayy! New best EMA pseudo Dice: 0.7325
2024-12-21 13:04:57.917075: 
2024-12-21 13:04:57.918689: Epoch 104
2024-12-21 13:04:57.919583: Current learning rate: 0.00345
2024-12-21 13:06:43.719933: Validation loss did not improve from -0.54636. Patience: 25/50
2024-12-21 13:06:43.720839: train_loss -0.7705
2024-12-21 13:06:43.721713: val_loss -0.4974
2024-12-21 13:06:43.722415: Pseudo dice [0.7339]
2024-12-21 13:06:43.723126: Epoch time: 105.8 s
2024-12-21 13:06:44.124058: Yayy! New best EMA pseudo Dice: 0.7327
2024-12-21 13:06:45.900998: 
2024-12-21 13:06:45.901989: Epoch 105
2024-12-21 13:06:45.902739: Current learning rate: 0.00338
2024-12-21 13:08:21.152690: Validation loss did not improve from -0.54636. Patience: 26/50
2024-12-21 13:08:21.153729: train_loss -0.7702
2024-12-21 13:08:21.154691: val_loss -0.5139
2024-12-21 13:08:21.155553: Pseudo dice [0.7413]
2024-12-21 13:08:21.156441: Epoch time: 95.25 s
2024-12-21 13:08:21.157230: Yayy! New best EMA pseudo Dice: 0.7335
2024-12-21 13:08:22.965994: 
2024-12-21 13:08:22.967132: Epoch 106
2024-12-21 13:08:22.968023: Current learning rate: 0.00332
2024-12-21 13:11:35.506372: Validation loss did not improve from -0.54636. Patience: 27/50
2024-12-21 13:11:35.507436: train_loss -0.7701
2024-12-21 13:11:35.508436: val_loss -0.5007
2024-12-21 13:11:35.509210: Pseudo dice [0.737]
2024-12-21 13:11:35.509976: Epoch time: 192.54 s
2024-12-21 13:11:35.510663: Yayy! New best EMA pseudo Dice: 0.7339
2024-12-21 13:11:37.819913: 
2024-12-21 13:11:37.821079: Epoch 107
2024-12-21 13:11:37.821798: Current learning rate: 0.00325
2024-12-21 13:14:30.029579: Validation loss did not improve from -0.54636. Patience: 28/50
2024-12-21 13:14:30.030665: train_loss -0.7684
2024-12-21 13:14:30.031636: val_loss -0.5247
2024-12-21 13:14:30.032402: Pseudo dice [0.7397]
2024-12-21 13:14:30.033188: Epoch time: 172.21 s
2024-12-21 13:14:30.033933: Yayy! New best EMA pseudo Dice: 0.7345
2024-12-21 13:14:31.779160: 
2024-12-21 13:14:31.780324: Epoch 108
2024-12-21 13:14:31.781109: Current learning rate: 0.00318
2024-12-21 13:17:36.143138: Validation loss did not improve from -0.54636. Patience: 29/50
2024-12-21 13:17:36.144213: train_loss -0.7747
2024-12-21 13:17:36.144924: val_loss -0.4763
2024-12-21 13:17:36.145591: Pseudo dice [0.7294]
2024-12-21 13:17:36.146338: Epoch time: 184.37 s
2024-12-21 13:17:37.632657: 
2024-12-21 13:17:37.633950: Epoch 109
2024-12-21 13:17:37.634731: Current learning rate: 0.00311
2024-12-21 13:20:39.367907: Validation loss did not improve from -0.54636. Patience: 30/50
2024-12-21 13:20:39.369201: train_loss -0.7749
2024-12-21 13:20:39.370217: val_loss -0.4908
2024-12-21 13:20:39.371000: Pseudo dice [0.7358]
2024-12-21 13:20:39.371823: Epoch time: 181.74 s
2024-12-21 13:20:41.177252: 
2024-12-21 13:20:41.178420: Epoch 110
2024-12-21 13:20:41.179297: Current learning rate: 0.00304
2024-12-21 13:24:00.512933: Validation loss did not improve from -0.54636. Patience: 31/50
2024-12-21 13:24:00.514055: train_loss -0.7745
2024-12-21 13:24:00.514898: val_loss -0.51
2024-12-21 13:24:00.515626: Pseudo dice [0.7404]
2024-12-21 13:24:00.516452: Epoch time: 199.34 s
2024-12-21 13:24:00.517123: Yayy! New best EMA pseudo Dice: 0.7348
2024-12-21 13:24:02.375642: 
2024-12-21 13:24:02.376829: Epoch 111
2024-12-21 13:24:02.377645: Current learning rate: 0.00297
2024-12-21 13:26:42.152883: Validation loss did not improve from -0.54636. Patience: 32/50
2024-12-21 13:26:42.153710: train_loss -0.7759
2024-12-21 13:26:42.154775: val_loss -0.5121
2024-12-21 13:26:42.155688: Pseudo dice [0.7327]
2024-12-21 13:26:42.156632: Epoch time: 159.78 s
2024-12-21 13:26:43.487645: 
2024-12-21 13:26:43.488994: Epoch 112
2024-12-21 13:26:43.489973: Current learning rate: 0.00291
2024-12-21 13:30:00.929554: Validation loss did not improve from -0.54636. Patience: 33/50
2024-12-21 13:30:00.930630: train_loss -0.7772
2024-12-21 13:30:00.931784: val_loss -0.5098
2024-12-21 13:30:00.932642: Pseudo dice [0.7383]
2024-12-21 13:30:00.933451: Epoch time: 197.44 s
2024-12-21 13:30:00.934213: Yayy! New best EMA pseudo Dice: 0.7349
2024-12-21 13:30:02.781669: 
2024-12-21 13:30:02.782856: Epoch 113
2024-12-21 13:30:02.783895: Current learning rate: 0.00284
2024-12-21 13:32:39.412033: Validation loss did not improve from -0.54636. Patience: 34/50
2024-12-21 13:32:39.413312: train_loss -0.7777
2024-12-21 13:32:39.414307: val_loss -0.513
2024-12-21 13:32:39.415199: Pseudo dice [0.7407]
2024-12-21 13:32:39.416043: Epoch time: 156.63 s
2024-12-21 13:32:39.416718: Yayy! New best EMA pseudo Dice: 0.7355
2024-12-21 13:32:41.166374: 
2024-12-21 13:32:41.167362: Epoch 114
2024-12-21 13:32:41.168096: Current learning rate: 0.00277
2024-12-21 13:36:18.475661: Validation loss did not improve from -0.54636. Patience: 35/50
2024-12-21 13:36:18.476542: train_loss -0.7815
2024-12-21 13:36:18.477342: val_loss -0.5026
2024-12-21 13:36:18.477962: Pseudo dice [0.7253]
2024-12-21 13:36:18.478589: Epoch time: 217.31 s
2024-12-21 13:36:20.302791: 
2024-12-21 13:36:20.303937: Epoch 115
2024-12-21 13:36:20.304681: Current learning rate: 0.0027
2024-12-21 13:39:41.441782: Validation loss did not improve from -0.54636. Patience: 36/50
2024-12-21 13:39:41.444119: train_loss -0.779
2024-12-21 13:39:41.446034: val_loss -0.5182
2024-12-21 13:39:41.446731: Pseudo dice [0.7392]
2024-12-21 13:39:41.447742: Epoch time: 201.14 s
2024-12-21 13:39:42.950814: 
2024-12-21 13:39:42.952018: Epoch 116
2024-12-21 13:39:42.952807: Current learning rate: 0.00263
2024-12-21 13:42:43.242339: Validation loss did not improve from -0.54636. Patience: 37/50
2024-12-21 13:42:43.243240: train_loss -0.7802
2024-12-21 13:42:43.243991: val_loss -0.4809
2024-12-21 13:42:43.244700: Pseudo dice [0.726]
2024-12-21 13:42:43.245373: Epoch time: 180.29 s
2024-12-21 13:42:44.682099: 
2024-12-21 13:42:44.683350: Epoch 117
2024-12-21 13:42:44.684141: Current learning rate: 0.00256
2024-12-21 13:45:46.397138: Validation loss did not improve from -0.54636. Patience: 38/50
2024-12-21 13:45:46.398211: train_loss -0.7803
2024-12-21 13:45:46.399031: val_loss -0.5221
2024-12-21 13:45:46.399747: Pseudo dice [0.7487]
2024-12-21 13:45:46.400387: Epoch time: 181.72 s
2024-12-21 13:45:46.401008: Yayy! New best EMA pseudo Dice: 0.7355
2024-12-21 13:45:48.730426: 
2024-12-21 13:45:48.731346: Epoch 118
2024-12-21 13:45:48.732196: Current learning rate: 0.00249
2024-12-21 13:48:46.510827: Validation loss did not improve from -0.54636. Patience: 39/50
2024-12-21 13:48:46.511796: train_loss -0.7832
2024-12-21 13:48:46.512637: val_loss -0.4889
2024-12-21 13:48:46.513417: Pseudo dice [0.731]
2024-12-21 13:48:46.514138: Epoch time: 177.78 s
2024-12-21 13:48:47.968935: 
2024-12-21 13:48:47.970081: Epoch 119
2024-12-21 13:48:47.970881: Current learning rate: 0.00242
2024-12-21 13:51:59.742877: Validation loss did not improve from -0.54636. Patience: 40/50
2024-12-21 13:51:59.743686: train_loss -0.7821
2024-12-21 13:51:59.744478: val_loss -0.5111
2024-12-21 13:51:59.745089: Pseudo dice [0.7386]
2024-12-21 13:51:59.745777: Epoch time: 191.78 s
2024-12-21 13:52:01.600854: 
2024-12-21 13:52:01.601960: Epoch 120
2024-12-21 13:52:01.603135: Current learning rate: 0.00235
2024-12-21 13:55:02.222040: Validation loss did not improve from -0.54636. Patience: 41/50
2024-12-21 13:55:02.224136: train_loss -0.7835
2024-12-21 13:55:02.225119: val_loss -0.5079
2024-12-21 13:55:02.226078: Pseudo dice [0.7363]
2024-12-21 13:55:02.226684: Epoch time: 180.62 s
2024-12-21 13:55:03.615732: 
2024-12-21 13:55:03.616758: Epoch 121
2024-12-21 13:55:03.617493: Current learning rate: 0.00228
2024-12-21 13:58:30.659703: Validation loss did not improve from -0.54636. Patience: 42/50
2024-12-21 13:58:30.660562: train_loss -0.7816
2024-12-21 13:58:30.661297: val_loss -0.5123
2024-12-21 13:58:30.662075: Pseudo dice [0.7311]
2024-12-21 13:58:30.662973: Epoch time: 207.05 s
2024-12-21 13:58:32.113774: 
2024-12-21 13:58:32.115211: Epoch 122
2024-12-21 13:58:32.115970: Current learning rate: 0.00221
2024-12-21 14:01:13.351766: Validation loss did not improve from -0.54636. Patience: 43/50
2024-12-21 14:01:13.352782: train_loss -0.7874
2024-12-21 14:01:13.353573: val_loss -0.5168
2024-12-21 14:01:13.354362: Pseudo dice [0.7431]
2024-12-21 14:01:13.355047: Epoch time: 161.24 s
2024-12-21 14:01:13.355795: Yayy! New best EMA pseudo Dice: 0.7359
2024-12-21 14:01:15.147396: 
2024-12-21 14:01:15.148966: Epoch 123
2024-12-21 14:01:15.149813: Current learning rate: 0.00214
2024-12-21 14:04:47.551259: Validation loss did not improve from -0.54636. Patience: 44/50
2024-12-21 14:04:47.552159: train_loss -0.7853
2024-12-21 14:04:47.552830: val_loss -0.5085
2024-12-21 14:04:47.553504: Pseudo dice [0.7424]
2024-12-21 14:04:47.554137: Epoch time: 212.41 s
2024-12-21 14:04:47.554753: Yayy! New best EMA pseudo Dice: 0.7365
2024-12-21 14:04:49.423376: 
2024-12-21 14:04:49.424104: Epoch 124
2024-12-21 14:04:49.424747: Current learning rate: 0.00207
2024-12-21 14:08:05.671563: Validation loss did not improve from -0.54636. Patience: 45/50
2024-12-21 14:08:05.672687: train_loss -0.7873
2024-12-21 14:08:05.673440: val_loss -0.4987
2024-12-21 14:08:05.674220: Pseudo dice [0.7343]
2024-12-21 14:08:05.674855: Epoch time: 196.25 s
2024-12-21 14:08:07.505426: 
2024-12-21 14:08:07.506683: Epoch 125
2024-12-21 14:08:07.507314: Current learning rate: 0.00199
2024-12-21 14:11:02.808816: Validation loss did not improve from -0.54636. Patience: 46/50
2024-12-21 14:11:02.809709: train_loss -0.7852
2024-12-21 14:11:02.810529: val_loss -0.4911
2024-12-21 14:11:02.811312: Pseudo dice [0.7258]
2024-12-21 14:11:02.812057: Epoch time: 175.31 s
2024-12-21 14:11:04.201087: 
2024-12-21 14:11:04.202290: Epoch 126
2024-12-21 14:11:04.203139: Current learning rate: 0.00192
2024-12-21 14:14:23.076620: Validation loss did not improve from -0.54636. Patience: 47/50
2024-12-21 14:14:23.077467: train_loss -0.7898
2024-12-21 14:14:23.078597: val_loss -0.5048
2024-12-21 14:14:23.079860: Pseudo dice [0.7226]
2024-12-21 14:14:23.080987: Epoch time: 198.88 s
2024-12-21 14:14:24.515698: 
2024-12-21 14:14:24.517124: Epoch 127
2024-12-21 14:14:24.518214: Current learning rate: 0.00185
2024-12-21 14:17:26.868366: Validation loss did not improve from -0.54636. Patience: 48/50
2024-12-21 14:17:26.869340: train_loss -0.7896
2024-12-21 14:17:26.870245: val_loss -0.4939
2024-12-21 14:17:26.871040: Pseudo dice [0.7244]
2024-12-21 14:17:26.871698: Epoch time: 182.35 s
2024-12-21 14:17:28.263880: 
2024-12-21 14:17:28.265565: Epoch 128
2024-12-21 14:17:28.266404: Current learning rate: 0.00178
2024-12-21 14:20:52.319925: Validation loss did not improve from -0.54636. Patience: 49/50
2024-12-21 14:20:52.320611: train_loss -0.7911
2024-12-21 14:20:52.321424: val_loss -0.5114
2024-12-21 14:20:52.322145: Pseudo dice [0.7309]
2024-12-21 14:20:52.322866: Epoch time: 204.06 s
2024-12-21 14:20:54.161223: 
2024-12-21 14:20:54.162344: Epoch 129
2024-12-21 14:20:54.163207: Current learning rate: 0.0017
2024-12-21 14:23:49.827216: Validation loss did not improve from -0.54636. Patience: 50/50
2024-12-21 14:23:49.828134: train_loss -0.7922
2024-12-21 14:23:49.829056: val_loss -0.5107
2024-12-21 14:23:49.829798: Pseudo dice [0.7388]
2024-12-21 14:23:49.830530: Epoch time: 175.67 s
2024-12-21 14:23:51.635832: 
2024-12-21 14:23:51.637108: Epoch 130
2024-12-21 14:23:51.638022: Current learning rate: 0.00163
2024-12-21 14:27:35.159043: Validation loss did not improve from -0.54636. Patience: 51/50
2024-12-21 14:27:35.160125: train_loss -0.7898
2024-12-21 14:27:35.161062: val_loss -0.5057
2024-12-21 14:27:35.161702: Pseudo dice [0.7321]
2024-12-21 14:27:35.162465: Epoch time: 223.53 s
2024-12-21 14:27:36.607976: 
2024-12-21 14:27:36.609480: Epoch 131
2024-12-21 14:27:36.610275: Current learning rate: 0.00156
2024-12-21 14:31:03.573015: Validation loss did not improve from -0.54636. Patience: 52/50
2024-12-21 14:31:03.574161: train_loss -0.7891
2024-12-21 14:31:03.574919: val_loss -0.5072
2024-12-21 14:31:03.575620: Pseudo dice [0.7339]
2024-12-21 14:31:03.576400: Epoch time: 206.97 s
2024-12-21 14:31:05.012859: 
2024-12-21 14:31:05.014087: Epoch 132
2024-12-21 14:31:05.014981: Current learning rate: 0.00148
2024-12-21 14:33:59.720885: Validation loss did not improve from -0.54636. Patience: 53/50
2024-12-21 14:33:59.721801: train_loss -0.7924
2024-12-21 14:33:59.722661: val_loss -0.5373
2024-12-21 14:33:59.723438: Pseudo dice [0.7517]
2024-12-21 14:33:59.724177: Epoch time: 174.71 s
2024-12-21 14:34:01.100898: 
2024-12-21 14:34:01.102100: Epoch 133
2024-12-21 14:34:01.102986: Current learning rate: 0.00141
2024-12-21 14:37:15.311998: Validation loss did not improve from -0.54636. Patience: 54/50
2024-12-21 14:37:15.312749: train_loss -0.7897
2024-12-21 14:37:15.313552: val_loss -0.5084
2024-12-21 14:37:15.314212: Pseudo dice [0.7414]
2024-12-21 14:37:15.314961: Epoch time: 194.21 s
2024-12-21 14:37:16.754972: 
2024-12-21 14:37:16.756113: Epoch 134
2024-12-21 14:37:16.756912: Current learning rate: 0.00133
2024-12-21 14:40:23.618059: Validation loss did not improve from -0.54636. Patience: 55/50
2024-12-21 14:40:23.618991: train_loss -0.7927
2024-12-21 14:40:23.619947: val_loss -0.5083
2024-12-21 14:40:23.620909: Pseudo dice [0.7379]
2024-12-21 14:40:23.621584: Epoch time: 186.87 s
2024-12-21 14:40:25.399338: 
2024-12-21 14:40:25.400386: Epoch 135
2024-12-21 14:40:25.401165: Current learning rate: 0.00126
2024-12-21 14:43:47.988975: Validation loss did not improve from -0.54636. Patience: 56/50
2024-12-21 14:43:47.991069: train_loss -0.7885
2024-12-21 14:43:47.992617: val_loss -0.4885
2024-12-21 14:43:47.993308: Pseudo dice [0.7251]
2024-12-21 14:43:47.994316: Epoch time: 202.59 s
2024-12-21 14:43:49.480437: 
2024-12-21 14:43:49.481371: Epoch 136
2024-12-21 14:43:49.482070: Current learning rate: 0.00118
2024-12-21 14:46:44.098660: Validation loss did not improve from -0.54636. Patience: 57/50
2024-12-21 14:46:44.099591: train_loss -0.7955
2024-12-21 14:46:44.100326: val_loss -0.5105
2024-12-21 14:46:44.100964: Pseudo dice [0.7309]
2024-12-21 14:46:44.101605: Epoch time: 174.62 s
2024-12-21 14:46:45.509293: 
2024-12-21 14:46:45.510793: Epoch 137
2024-12-21 14:46:45.511643: Current learning rate: 0.00111
2024-12-21 14:50:28.795291: Validation loss did not improve from -0.54636. Patience: 58/50
2024-12-21 14:50:28.796373: train_loss -0.7928
2024-12-21 14:50:28.797269: val_loss -0.4933
2024-12-21 14:50:28.797991: Pseudo dice [0.7302]
2024-12-21 14:50:28.798729: Epoch time: 223.29 s
2024-12-21 14:50:30.227550: 
2024-12-21 14:50:30.228590: Epoch 138
2024-12-21 14:50:30.229212: Current learning rate: 0.00103
2024-12-21 14:54:06.527572: Validation loss did not improve from -0.54636. Patience: 59/50
2024-12-21 14:54:06.528610: train_loss -0.7932
2024-12-21 14:54:06.529367: val_loss -0.4953
2024-12-21 14:54:06.530074: Pseudo dice [0.7304]
2024-12-21 14:54:06.530745: Epoch time: 216.3 s
2024-12-21 14:54:07.958760: 
2024-12-21 14:54:07.959706: Epoch 139
2024-12-21 14:54:07.960461: Current learning rate: 0.00095
2024-12-21 14:57:07.833652: Validation loss did not improve from -0.54636. Patience: 60/50
2024-12-21 14:57:07.834643: train_loss -0.794
2024-12-21 14:57:07.835341: val_loss -0.5102
2024-12-21 14:57:07.835928: Pseudo dice [0.7351]
2024-12-21 14:57:07.836533: Epoch time: 179.88 s
2024-12-21 14:57:10.515934: 
2024-12-21 14:57:10.517024: Epoch 140
2024-12-21 14:57:10.517746: Current learning rate: 0.00087
2024-12-21 15:00:34.075255: Validation loss did not improve from -0.54636. Patience: 61/50
2024-12-21 15:00:34.077562: train_loss -0.7995
2024-12-21 15:00:34.078365: val_loss -0.5097
2024-12-21 15:00:34.079098: Pseudo dice [0.7363]
2024-12-21 15:00:34.079967: Epoch time: 203.56 s
2024-12-21 15:00:35.548542: 
2024-12-21 15:00:35.549662: Epoch 141
2024-12-21 15:00:35.550563: Current learning rate: 0.00079
2024-12-21 15:03:26.183117: Validation loss did not improve from -0.54636. Patience: 62/50
2024-12-21 15:03:26.183975: train_loss -0.8011
2024-12-21 15:03:26.184741: val_loss -0.4943
2024-12-21 15:03:26.185486: Pseudo dice [0.7272]
2024-12-21 15:03:26.186244: Epoch time: 170.64 s
2024-12-21 15:03:27.597453: 
2024-12-21 15:03:27.598366: Epoch 142
2024-12-21 15:03:27.599140: Current learning rate: 0.00071
2024-12-21 15:06:43.400604: Validation loss did not improve from -0.54636. Patience: 63/50
2024-12-21 15:06:43.401488: train_loss -0.7978
2024-12-21 15:06:43.404798: val_loss -0.5041
2024-12-21 15:06:43.405782: Pseudo dice [0.7319]
2024-12-21 15:06:43.406510: Epoch time: 195.81 s
2024-12-21 15:06:44.892025: 
2024-12-21 15:06:44.893144: Epoch 143
2024-12-21 15:06:44.893961: Current learning rate: 0.00063
2024-12-21 15:09:22.518730: Validation loss did not improve from -0.54636. Patience: 64/50
2024-12-21 15:09:22.519480: train_loss -0.7931
2024-12-21 15:09:22.520312: val_loss -0.5055
2024-12-21 15:09:22.520876: Pseudo dice [0.7381]
2024-12-21 15:09:22.521480: Epoch time: 157.63 s
2024-12-21 15:09:24.031647: 
2024-12-21 15:09:24.032850: Epoch 144
2024-12-21 15:09:24.033669: Current learning rate: 0.00055
2024-12-21 15:12:36.346700: Validation loss did not improve from -0.54636. Patience: 65/50
2024-12-21 15:12:36.347708: train_loss -0.7951
2024-12-21 15:12:36.348891: val_loss -0.5029
2024-12-21 15:12:36.349990: Pseudo dice [0.7402]
2024-12-21 15:12:36.351097: Epoch time: 192.32 s
2024-12-21 15:12:38.299216: 
2024-12-21 15:12:38.300580: Epoch 145
2024-12-21 15:12:38.301774: Current learning rate: 0.00047
2024-12-21 15:15:07.631586: Validation loss did not improve from -0.54636. Patience: 66/50
2024-12-21 15:15:07.632780: train_loss -0.7987
2024-12-21 15:15:07.633739: val_loss -0.5031
2024-12-21 15:15:07.634485: Pseudo dice [0.7337]
2024-12-21 15:15:07.635251: Epoch time: 149.34 s
2024-12-21 15:15:09.057300: 
2024-12-21 15:15:09.058411: Epoch 146
2024-12-21 15:15:09.059352: Current learning rate: 0.00038
2024-12-21 15:18:41.763012: Validation loss did not improve from -0.54636. Patience: 67/50
2024-12-21 15:18:41.764162: train_loss -0.7978
2024-12-21 15:18:41.764946: val_loss -0.5082
2024-12-21 15:18:41.765718: Pseudo dice [0.7387]
2024-12-21 15:18:41.766484: Epoch time: 212.71 s
2024-12-21 15:18:43.279225: 
2024-12-21 15:18:43.280950: Epoch 147
2024-12-21 15:18:43.281755: Current learning rate: 0.0003
2024-12-21 15:21:46.038560: Validation loss did not improve from -0.54636. Patience: 68/50
2024-12-21 15:21:46.039243: train_loss -0.7985
2024-12-21 15:21:46.040274: val_loss -0.5117
2024-12-21 15:21:46.042579: Pseudo dice [0.74]
2024-12-21 15:21:46.043513: Epoch time: 182.76 s
2024-12-21 15:21:47.489878: 
2024-12-21 15:21:47.492129: Epoch 148
2024-12-21 15:21:47.494132: Current learning rate: 0.00021
2024-12-21 15:25:13.492029: Validation loss did not improve from -0.54636. Patience: 69/50
2024-12-21 15:25:13.493155: train_loss -0.8001
2024-12-21 15:25:13.494033: val_loss -0.5329
2024-12-21 15:25:13.494885: Pseudo dice [0.749]
2024-12-21 15:25:13.495565: Epoch time: 206.0 s
2024-12-21 15:25:13.496331: Yayy! New best EMA pseudo Dice: 0.7367
2024-12-21 15:25:15.316005: 
2024-12-21 15:25:15.317487: Epoch 149
2024-12-21 15:25:15.318860: Current learning rate: 0.00011
2024-12-21 15:28:32.353923: Validation loss did not improve from -0.54636. Patience: 70/50
2024-12-21 15:28:32.355030: train_loss -0.7968
2024-12-21 15:28:32.355845: val_loss -0.5207
2024-12-21 15:28:32.356757: Pseudo dice [0.7451]
2024-12-21 15:28:32.357678: Epoch time: 197.04 s
2024-12-21 15:28:32.358394: Yayy! New best EMA pseudo Dice: 0.7375
2024-12-21 15:28:35.284241: Training done.
2024-12-21 15:28:35.517976: Using splits from existing split file: /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset308_Sohee_Calcium_OCT_CrossValidation/splits_final_40.json
2024-12-21 15:28:35.535872: The split file contains 5 splits.
2024-12-21 15:28:35.536884: Desired fold for training: 4
2024-12-21 15:28:35.537644: This split has 3 training and 5 validation cases.
2024-12-21 15:28:35.538603: predicting 101-044
2024-12-21 15:28:35.628411: 101-044, shape torch.Size([1, 404, 498, 498]), rank 0
2024-12-21 15:30:25.951728: predicting 101-045
2024-12-21 15:30:25.991098: 101-045, shape torch.Size([1, 375, 498, 498]), rank 0
2024-12-21 15:32:21.909222: predicting 401-004
2024-12-21 15:32:21.938529: 401-004, shape torch.Size([1, 375, 498, 498]), rank 0
2024-12-21 15:34:20.758423: predicting 704-003
2024-12-21 15:34:20.791272: 704-003, shape torch.Size([1, 375, 498, 498]), rank 0
2024-12-21 15:36:15.587282: predicting 706-005
2024-12-21 15:36:15.612487: 706-005, shape torch.Size([1, 375, 498, 498]), rank 0
2024-12-21 15:38:39.637910: Validation complete
2024-12-21 15:38:39.639015: Mean Validation Dice:  0.721234918093457
