/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/bin/python
Starting training with CONFIG=3d_32x160x128_b10, DATASET_ID=310, TRAINER=nnUNetTrainerScaleAnalysis20
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/nnunetv2/training/nnUNetTrainer/nnUNetTrainer.py:169: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  self.grad_scaler = GradScaler() if self.device.type == 'cuda' else None

############################
INFO: You are using the old nnU-Net default plans. We have updated our recommendations. Please consider using those instead! Read more here: https://github.com/MIC-DKFZ/nnUNet/blob/master/documentation/resenc_presets.md
############################

Using device: cuda:0

#######################################################################
Please cite the following paper when using nnU-Net:
Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.
#######################################################################

2025-10-06 01:10:12.215808: do_dummy_2d_data_aug: True
2025-10-06 01:10:12.216234: Using splits from existing split file: /nfs/erelab001/shared/Computational_Group/Naravich/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset310_nnInteractive_Calcium_OCT_CrossValidation/splits_final_20.json
2025-10-06 01:10:12.216595: The split file contains 5 splits.
2025-10-06 01:10:12.216781: Desired fold for training: 4
2025-10-06 01:10:12.216953: This split has 1 training and 7 validation cases.
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/fft_conv_pytorch/fft_conv.py:139: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:306.)
  output = output[crop_slices].contiguous()
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/fft_conv_pytorch/fft_conv.py:139: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:306.)
  output = output[crop_slices].contiguous()
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/fft_conv_pytorch/fft_conv.py:139: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:306.)
  output = output[crop_slices].contiguous()
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/fft_conv_pytorch/fft_conv.py:139: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:306.)
  output = output[crop_slices].contiguous()
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/fft_conv_pytorch/fft_conv.py:139: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:306.)
  output = output[crop_slices].contiguous()
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/fft_conv_pytorch/fft_conv.py:139: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:306.)
  output = output[crop_slices].contiguous()
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/fft_conv_pytorch/fft_conv.py:139: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:306.)
  output = output[crop_slices].contiguous()
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/fft_conv_pytorch/fft_conv.py:139: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:306.)
  output = output[crop_slices].contiguous()
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/fft_conv_pytorch/fft_conv.py:139: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:306.)
  output = output[crop_slices].contiguous()
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/fft_conv_pytorch/fft_conv.py:139: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:306.)
  output = output[crop_slices].contiguous()
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/fft_conv_pytorch/fft_conv.py:139: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:306.)
  output = output[crop_slices].contiguous()
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/fft_conv_pytorch/fft_conv.py:139: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:306.)
  output = output[crop_slices].contiguous()
using pin_memory on device 0
using pin_memory on device 0
2025-10-06 01:10:16.448941: Using torch.compile...

This is the configuration used by this training:
Configuration name: 3d_32x160x128_b10
 {'data_identifier': 'nnUNetPlans_3d_fullres', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 10, 'patch_size': [32, 160, 128], 'median_image_size_in_voxels': [375.0, 498.0, 498.0], 'spacing': [1.0, 1.0, 1.0], 'normalization_schemes': ['ZScoreNormalization'], 'use_mask_for_norm': [False], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'architecture': {'network_class_name': 'dynamic_network_architectures.architectures.unet.PlainConvUNet', 'arch_kwargs': {'n_stages': 6, 'features_per_stage': [32, 64, 128, 256, 320, 320], 'conv_op': 'torch.nn.modules.conv.Conv3d', 'kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]], 'strides': [[1, 1, 1], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [1, 2, 2]], 'n_conv_per_stage': [2, 2, 2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2, 2, 2], 'conv_bias': True, 'norm_op': 'torch.nn.modules.instancenorm.InstanceNorm3d', 'norm_op_kwargs': {'eps': 1e-05, 'affine': True}, 'dropout_op': None, 'dropout_op_kwargs': None, 'nonlin': 'torch.nn.LeakyReLU', 'nonlin_kwargs': {'inplace': True}, 'deep_supervision': True}, '_kw_requires_import': ['conv_op', 'norm_op', 'dropout_op', 'nonlin']}, 'batch_dice': True, 'inherits_from': '3d_fullres'} 

These are the global plan.json settings:
 {'dataset_name': 'Dataset310_nnInteractive_Calcium_OCT_CrossValidation', 'plans_name': 'nnUNetPlans', 'original_median_spacing_after_transp': [1.0, 1.0, 1.0], 'original_median_shape_after_transp': [375, 498, 498], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 0.4977976381778717, 'mean': 0.1394134759902954, 'median': 0.09849607944488525, 'min': 0.0, 'percentile_00_5': 0.015305490233004093, 'percentile_99_5': 0.4977976381778717, 'std': 0.121165432035923}}} 

2025-10-06 01:10:18.202422: unpacking dataset...
2025-10-06 01:10:22.578080: unpacking done...
2025-10-06 01:10:22.580500: Unable to plot network architecture: nnUNet_compile is enabled!
2025-10-06 01:10:22.585181: 
2025-10-06 01:10:22.585390: Epoch 0
2025-10-06 01:10:22.585602: Current learning rate: 0.01
2025-10-06 01:11:15.201456: Validation loss improved from 1000.00000 to -0.23895! Patience: 0/50
2025-10-06 01:11:15.202109: train_loss -0.1963
2025-10-06 01:11:15.202285: val_loss -0.2389
2025-10-06 01:11:15.202412: Pseudo dice [np.float32(0.5718)]
2025-10-06 01:11:15.202541: Epoch time: 52.62 s
2025-10-06 01:11:15.202657: Yayy! New best EMA pseudo Dice: 0.5717999935150146
2025-10-06 01:11:16.365657: 
2025-10-06 01:11:16.366009: Epoch 1
2025-10-06 01:11:16.366221: Current learning rate: 0.00994
2025-10-06 01:12:02.982189: Validation loss did not improve from -0.23895. Patience: 1/50
2025-10-06 01:12:02.982787: train_loss -0.3933
2025-10-06 01:12:02.982967: val_loss -0.2334
2025-10-06 01:12:02.983113: Pseudo dice [np.float32(0.5577)]
2025-10-06 01:12:02.983239: Epoch time: 46.62 s
2025-10-06 01:12:03.612130: 
2025-10-06 01:12:03.612506: Epoch 2
2025-10-06 01:12:03.612743: Current learning rate: 0.00988
2025-10-06 01:12:50.383963: Validation loss improved from -0.23895 to -0.29181! Patience: 1/50
2025-10-06 01:12:50.384544: train_loss -0.4452
2025-10-06 01:12:50.384717: val_loss -0.2918
2025-10-06 01:12:50.384841: Pseudo dice [np.float32(0.5932)]
2025-10-06 01:12:50.385000: Epoch time: 46.77 s
2025-10-06 01:12:50.385115: Yayy! New best EMA pseudo Dice: 0.572700023651123
2025-10-06 01:12:51.471507: 
2025-10-06 01:12:51.471812: Epoch 3
2025-10-06 01:12:51.472003: Current learning rate: 0.00982
2025-10-06 01:13:38.327757: Validation loss improved from -0.29181 to -0.31802! Patience: 0/50
2025-10-06 01:13:38.328285: train_loss -0.5029
2025-10-06 01:13:38.328459: val_loss -0.318
2025-10-06 01:13:38.328582: Pseudo dice [np.float32(0.6276)]
2025-10-06 01:13:38.328754: Epoch time: 46.86 s
2025-10-06 01:13:38.328873: Yayy! New best EMA pseudo Dice: 0.5781999826431274
2025-10-06 01:13:39.425006: 
2025-10-06 01:13:39.425349: Epoch 4
2025-10-06 01:13:39.425538: Current learning rate: 0.00976
2025-10-06 01:14:26.264994: Validation loss improved from -0.31802 to -0.36393! Patience: 0/50
2025-10-06 01:14:26.265554: train_loss -0.5505
2025-10-06 01:14:26.265693: val_loss -0.3639
2025-10-06 01:14:26.265832: Pseudo dice [np.float32(0.6368)]
2025-10-06 01:14:26.265962: Epoch time: 46.84 s
2025-10-06 01:14:26.637326: Yayy! New best EMA pseudo Dice: 0.5841000080108643
2025-10-06 01:14:27.728009: 
2025-10-06 01:14:27.728315: Epoch 5
2025-10-06 01:14:27.728503: Current learning rate: 0.0097
2025-10-06 01:15:14.613652: Validation loss did not improve from -0.36393. Patience: 1/50
2025-10-06 01:15:14.614087: train_loss -0.5928
2025-10-06 01:15:14.614268: val_loss -0.3341
2025-10-06 01:15:14.614426: Pseudo dice [np.float32(0.6208)]
2025-10-06 01:15:14.614569: Epoch time: 46.89 s
2025-10-06 01:15:14.614691: Yayy! New best EMA pseudo Dice: 0.5877000093460083
2025-10-06 01:15:15.719919: 
2025-10-06 01:15:15.720376: Epoch 6
2025-10-06 01:15:15.720563: Current learning rate: 0.00964
2025-10-06 01:16:02.550479: Validation loss did not improve from -0.36393. Patience: 2/50
2025-10-06 01:16:02.551367: train_loss -0.5816
2025-10-06 01:16:02.551709: val_loss -0.3447
2025-10-06 01:16:02.551959: Pseudo dice [np.float32(0.6358)]
2025-10-06 01:16:02.552250: Epoch time: 46.83 s
2025-10-06 01:16:02.552470: Yayy! New best EMA pseudo Dice: 0.5924999713897705
2025-10-06 01:16:03.675702: 
2025-10-06 01:16:03.675949: Epoch 7
2025-10-06 01:16:03.676195: Current learning rate: 0.00958
2025-10-06 01:16:50.423630: Validation loss improved from -0.36393 to -0.36604! Patience: 2/50
2025-10-06 01:16:50.424214: train_loss -0.6124
2025-10-06 01:16:50.424549: val_loss -0.366
2025-10-06 01:16:50.424762: Pseudo dice [np.float32(0.6359)]
2025-10-06 01:16:50.425016: Epoch time: 46.75 s
2025-10-06 01:16:50.425294: Yayy! New best EMA pseudo Dice: 0.5968999862670898
2025-10-06 01:16:51.512449: 
2025-10-06 01:16:51.512862: Epoch 8
2025-10-06 01:16:51.513113: Current learning rate: 0.00952
2025-10-06 01:17:38.301911: Validation loss did not improve from -0.36604. Patience: 1/50
2025-10-06 01:17:38.302611: train_loss -0.6377
2025-10-06 01:17:38.302799: val_loss -0.337
2025-10-06 01:17:38.302944: Pseudo dice [np.float32(0.6412)]
2025-10-06 01:17:38.303104: Epoch time: 46.79 s
2025-10-06 01:17:38.303222: Yayy! New best EMA pseudo Dice: 0.6013000011444092
2025-10-06 01:17:39.402999: 
2025-10-06 01:17:39.403349: Epoch 9
2025-10-06 01:17:39.403586: Current learning rate: 0.00946
2025-10-06 01:18:26.288393: Validation loss did not improve from -0.36604. Patience: 2/50
2025-10-06 01:18:26.288783: train_loss -0.6524
2025-10-06 01:18:26.288931: val_loss -0.3414
2025-10-06 01:18:26.289077: Pseudo dice [np.float32(0.626)]
2025-10-06 01:18:26.289225: Epoch time: 46.89 s
2025-10-06 01:18:26.721604: Yayy! New best EMA pseudo Dice: 0.6037999987602234
2025-10-06 01:18:27.762597: 
2025-10-06 01:18:27.762990: Epoch 10
2025-10-06 01:18:27.763170: Current learning rate: 0.0094
2025-10-06 01:19:14.669074: Validation loss improved from -0.36604 to -0.38519! Patience: 2/50
2025-10-06 01:19:14.669596: train_loss -0.6573
2025-10-06 01:19:14.669732: val_loss -0.3852
2025-10-06 01:19:14.669861: Pseudo dice [np.float32(0.656)]
2025-10-06 01:19:14.670020: Epoch time: 46.91 s
2025-10-06 01:19:14.670138: Yayy! New best EMA pseudo Dice: 0.609000027179718
2025-10-06 01:19:15.728709: 
2025-10-06 01:19:15.729074: Epoch 11
2025-10-06 01:19:15.729278: Current learning rate: 0.00934
2025-10-06 01:20:02.581948: Validation loss did not improve from -0.38519. Patience: 1/50
2025-10-06 01:20:02.582624: train_loss -0.6758
2025-10-06 01:20:02.582845: val_loss -0.3461
2025-10-06 01:20:02.583027: Pseudo dice [np.float32(0.6432)]
2025-10-06 01:20:02.583224: Epoch time: 46.85 s
2025-10-06 01:20:02.583404: Yayy! New best EMA pseudo Dice: 0.6123999953269958
2025-10-06 01:20:04.131474: 
2025-10-06 01:20:04.131856: Epoch 12
2025-10-06 01:20:04.132068: Current learning rate: 0.00928
2025-10-06 01:20:50.945613: Validation loss did not improve from -0.38519. Patience: 2/50
2025-10-06 01:20:50.946209: train_loss -0.6841
2025-10-06 01:20:50.946347: val_loss -0.3381
2025-10-06 01:20:50.946550: Pseudo dice [np.float32(0.6282)]
2025-10-06 01:20:50.946768: Epoch time: 46.82 s
2025-10-06 01:20:50.946963: Yayy! New best EMA pseudo Dice: 0.6140000224113464
2025-10-06 01:20:52.008601: 
2025-10-06 01:20:52.008953: Epoch 13
2025-10-06 01:20:52.009138: Current learning rate: 0.00922
2025-10-06 01:21:38.793411: Validation loss did not improve from -0.38519. Patience: 3/50
2025-10-06 01:21:38.794197: train_loss -0.701
2025-10-06 01:21:38.794380: val_loss -0.3158
2025-10-06 01:21:38.794536: Pseudo dice [np.float32(0.6357)]
2025-10-06 01:21:38.794744: Epoch time: 46.79 s
2025-10-06 01:21:38.794914: Yayy! New best EMA pseudo Dice: 0.6161999702453613
2025-10-06 01:21:39.869171: 
2025-10-06 01:21:39.869522: Epoch 14
2025-10-06 01:21:39.869742: Current learning rate: 0.00916
2025-10-06 01:22:26.734412: Validation loss did not improve from -0.38519. Patience: 4/50
2025-10-06 01:22:26.735247: train_loss -0.7088
2025-10-06 01:22:26.735490: val_loss -0.3627
2025-10-06 01:22:26.735716: Pseudo dice [np.float32(0.6565)]
2025-10-06 01:22:26.735952: Epoch time: 46.87 s
2025-10-06 01:22:27.164161: Yayy! New best EMA pseudo Dice: 0.620199978351593
2025-10-06 01:22:28.233062: 
2025-10-06 01:22:28.233280: Epoch 15
2025-10-06 01:22:28.233473: Current learning rate: 0.0091
2025-10-06 01:23:15.149052: Validation loss did not improve from -0.38519. Patience: 5/50
2025-10-06 01:23:15.149676: train_loss -0.7113
2025-10-06 01:23:15.149868: val_loss -0.326
2025-10-06 01:23:15.150011: Pseudo dice [np.float32(0.6398)]
2025-10-06 01:23:15.150149: Epoch time: 46.92 s
2025-10-06 01:23:15.150289: Yayy! New best EMA pseudo Dice: 0.6222000122070312
2025-10-06 01:23:16.285055: 
2025-10-06 01:23:16.285508: Epoch 16
2025-10-06 01:23:16.285822: Current learning rate: 0.00903
2025-10-06 01:24:03.175755: Validation loss did not improve from -0.38519. Patience: 6/50
2025-10-06 01:24:03.176533: train_loss -0.7163
2025-10-06 01:24:03.176776: val_loss -0.3472
2025-10-06 01:24:03.176933: Pseudo dice [np.float32(0.6475)]
2025-10-06 01:24:03.177097: Epoch time: 46.89 s
2025-10-06 01:24:03.177277: Yayy! New best EMA pseudo Dice: 0.6247000098228455
2025-10-06 01:24:04.278106: 
2025-10-06 01:24:04.278472: Epoch 17
2025-10-06 01:24:04.278730: Current learning rate: 0.00897
2025-10-06 01:24:51.149327: Validation loss did not improve from -0.38519. Patience: 7/50
2025-10-06 01:24:51.149840: train_loss -0.7318
2025-10-06 01:24:51.149985: val_loss -0.3275
2025-10-06 01:24:51.150097: Pseudo dice [np.float32(0.6573)]
2025-10-06 01:24:51.150237: Epoch time: 46.87 s
2025-10-06 01:24:51.150357: Yayy! New best EMA pseudo Dice: 0.6279000043869019
2025-10-06 01:24:52.230262: 
2025-10-06 01:24:52.230564: Epoch 18
2025-10-06 01:24:52.230820: Current learning rate: 0.00891
2025-10-06 01:25:39.094107: Validation loss did not improve from -0.38519. Patience: 8/50
2025-10-06 01:25:39.094698: train_loss -0.7297
2025-10-06 01:25:39.094869: val_loss -0.2916
2025-10-06 01:25:39.095087: Pseudo dice [np.float32(0.6226)]
2025-10-06 01:25:39.095268: Epoch time: 46.87 s
2025-10-06 01:25:39.737907: 
2025-10-06 01:25:39.738250: Epoch 19
2025-10-06 01:25:39.738428: Current learning rate: 0.00885
2025-10-06 01:26:26.603767: Validation loss did not improve from -0.38519. Patience: 9/50
2025-10-06 01:26:26.604476: train_loss -0.7368
2025-10-06 01:26:26.604805: val_loss -0.346
2025-10-06 01:26:26.605062: Pseudo dice [np.float32(0.6589)]
2025-10-06 01:26:26.605301: Epoch time: 46.87 s
2025-10-06 01:26:27.113504: Yayy! New best EMA pseudo Dice: 0.6305999755859375
2025-10-06 01:26:28.291617: 
2025-10-06 01:26:28.291965: Epoch 20
2025-10-06 01:26:28.292130: Current learning rate: 0.00879
2025-10-06 01:27:15.151899: Validation loss did not improve from -0.38519. Patience: 10/50
2025-10-06 01:27:15.152649: train_loss -0.7393
2025-10-06 01:27:15.152796: val_loss -0.3348
2025-10-06 01:27:15.152969: Pseudo dice [np.float32(0.638)]
2025-10-06 01:27:15.153151: Epoch time: 46.86 s
2025-10-06 01:27:15.153338: Yayy! New best EMA pseudo Dice: 0.6312999725341797
2025-10-06 01:27:16.315548: 
2025-10-06 01:27:16.315937: Epoch 21
2025-10-06 01:27:16.316159: Current learning rate: 0.00873
2025-10-06 01:28:03.158379: Validation loss did not improve from -0.38519. Patience: 11/50
2025-10-06 01:28:03.158770: train_loss -0.742
2025-10-06 01:28:03.159116: val_loss -0.3545
2025-10-06 01:28:03.159380: Pseudo dice [np.float32(0.6588)]
2025-10-06 01:28:03.159614: Epoch time: 46.84 s
2025-10-06 01:28:03.159829: Yayy! New best EMA pseudo Dice: 0.6341000199317932
2025-10-06 01:28:04.216974: 
2025-10-06 01:28:04.217232: Epoch 22
2025-10-06 01:28:04.217469: Current learning rate: 0.00867
2025-10-06 01:28:51.099686: Validation loss improved from -0.38519 to -0.41893! Patience: 11/50
2025-10-06 01:28:51.100279: train_loss -0.7574
2025-10-06 01:28:51.100451: val_loss -0.4189
2025-10-06 01:28:51.100573: Pseudo dice [np.float32(0.6911)]
2025-10-06 01:28:51.100711: Epoch time: 46.88 s
2025-10-06 01:28:51.100842: Yayy! New best EMA pseudo Dice: 0.6398000121116638
2025-10-06 01:28:52.168284: 
2025-10-06 01:28:52.168652: Epoch 23
2025-10-06 01:28:52.168899: Current learning rate: 0.00861
2025-10-06 01:29:39.119317: Validation loss did not improve from -0.41893. Patience: 1/50
2025-10-06 01:29:39.119989: train_loss -0.7585
2025-10-06 01:29:39.120176: val_loss -0.3576
2025-10-06 01:29:39.120328: Pseudo dice [np.float32(0.6538)]
2025-10-06 01:29:39.120494: Epoch time: 46.95 s
2025-10-06 01:29:39.120633: Yayy! New best EMA pseudo Dice: 0.6412000060081482
2025-10-06 01:29:40.565467: 
2025-10-06 01:29:40.565884: Epoch 24
2025-10-06 01:29:40.566102: Current learning rate: 0.00855
2025-10-06 01:30:27.520711: Validation loss did not improve from -0.41893. Patience: 2/50
2025-10-06 01:30:27.521259: train_loss -0.7625
2025-10-06 01:30:27.521402: val_loss -0.3446
2025-10-06 01:30:27.521677: Pseudo dice [np.float32(0.6391)]
2025-10-06 01:30:27.522016: Epoch time: 46.96 s
2025-10-06 01:30:28.655093: 
2025-10-06 01:30:28.655482: Epoch 25
2025-10-06 01:30:28.655760: Current learning rate: 0.00849
2025-10-06 01:31:15.571283: Validation loss did not improve from -0.41893. Patience: 3/50
2025-10-06 01:31:15.571803: train_loss -0.7718
2025-10-06 01:31:15.571973: val_loss -0.376
2025-10-06 01:31:15.572120: Pseudo dice [np.float32(0.6746)]
2025-10-06 01:31:15.572313: Epoch time: 46.92 s
2025-10-06 01:31:15.572487: Yayy! New best EMA pseudo Dice: 0.6442999839782715
2025-10-06 01:31:16.637728: 
2025-10-06 01:31:16.638060: Epoch 26
2025-10-06 01:31:16.638325: Current learning rate: 0.00843
2025-10-06 01:32:03.520940: Validation loss did not improve from -0.41893. Patience: 4/50
2025-10-06 01:32:03.521711: train_loss -0.7826
2025-10-06 01:32:03.522005: val_loss -0.3504
2025-10-06 01:32:03.522233: Pseudo dice [np.float32(0.6728)]
2025-10-06 01:32:03.522476: Epoch time: 46.88 s
2025-10-06 01:32:03.522645: Yayy! New best EMA pseudo Dice: 0.6471999883651733
2025-10-06 01:32:04.622420: 
2025-10-06 01:32:04.622764: Epoch 27
2025-10-06 01:32:04.622958: Current learning rate: 0.00836
2025-10-06 01:32:51.618456: Validation loss did not improve from -0.41893. Patience: 5/50
2025-10-06 01:32:51.619205: train_loss -0.7808
2025-10-06 01:32:51.619566: val_loss -0.3209
2025-10-06 01:32:51.619777: Pseudo dice [np.float32(0.6494)]
2025-10-06 01:32:51.620011: Epoch time: 47.0 s
2025-10-06 01:32:51.620176: Yayy! New best EMA pseudo Dice: 0.6474000215530396
2025-10-06 01:32:52.687316: 
2025-10-06 01:32:52.687665: Epoch 28
2025-10-06 01:32:52.687927: Current learning rate: 0.0083
2025-10-06 01:33:39.559544: Validation loss did not improve from -0.41893. Patience: 6/50
2025-10-06 01:33:39.560069: train_loss -0.7906
2025-10-06 01:33:39.560204: val_loss -0.3419
2025-10-06 01:33:39.560404: Pseudo dice [np.float32(0.6753)]
2025-10-06 01:33:39.560536: Epoch time: 46.87 s
2025-10-06 01:33:39.560677: Yayy! New best EMA pseudo Dice: 0.6502000093460083
2025-10-06 01:33:40.643219: 
2025-10-06 01:33:40.643602: Epoch 29
2025-10-06 01:33:40.643814: Current learning rate: 0.00824
2025-10-06 01:34:27.467949: Validation loss did not improve from -0.41893. Patience: 7/50
2025-10-06 01:34:27.468489: train_loss -0.7958
2025-10-06 01:34:27.468659: val_loss -0.2949
2025-10-06 01:34:27.468806: Pseudo dice [np.float32(0.6353)]
2025-10-06 01:34:27.468964: Epoch time: 46.83 s
2025-10-06 01:34:28.530058: 
2025-10-06 01:34:28.530339: Epoch 30
2025-10-06 01:34:28.530526: Current learning rate: 0.00818
2025-10-06 01:35:15.376759: Validation loss did not improve from -0.41893. Patience: 8/50
2025-10-06 01:35:15.377347: train_loss -0.7957
2025-10-06 01:35:15.377539: val_loss -0.3101
2025-10-06 01:35:15.377689: Pseudo dice [np.float32(0.6393)]
2025-10-06 01:35:15.377892: Epoch time: 46.85 s
2025-10-06 01:35:16.013426: 
2025-10-06 01:35:16.013811: Epoch 31
2025-10-06 01:35:16.014078: Current learning rate: 0.00812
2025-10-06 01:36:02.862809: Validation loss did not improve from -0.41893. Patience: 9/50
2025-10-06 01:36:02.863242: train_loss -0.7973
2025-10-06 01:36:02.863384: val_loss -0.3449
2025-10-06 01:36:02.863497: Pseudo dice [np.float32(0.6625)]
2025-10-06 01:36:02.863628: Epoch time: 46.85 s
2025-10-06 01:36:03.502505: 
2025-10-06 01:36:03.502848: Epoch 32
2025-10-06 01:36:03.503057: Current learning rate: 0.00806
2025-10-06 01:36:50.423248: Validation loss did not improve from -0.41893. Patience: 10/50
2025-10-06 01:36:50.423919: train_loss -0.8034
2025-10-06 01:36:50.424107: val_loss -0.3506
2025-10-06 01:36:50.424253: Pseudo dice [np.float32(0.6631)]
2025-10-06 01:36:50.424387: Epoch time: 46.92 s
2025-10-06 01:36:50.424527: Yayy! New best EMA pseudo Dice: 0.650600016117096
2025-10-06 01:36:51.524076: 
2025-10-06 01:36:51.524426: Epoch 33
2025-10-06 01:36:51.524642: Current learning rate: 0.008
2025-10-06 01:37:38.523992: Validation loss did not improve from -0.41893. Patience: 11/50
2025-10-06 01:37:38.524448: train_loss -0.7978
2025-10-06 01:37:38.524665: val_loss -0.3382
2025-10-06 01:37:38.524832: Pseudo dice [np.float32(0.6483)]
2025-10-06 01:37:38.525003: Epoch time: 47.0 s
2025-10-06 01:37:39.162519: 
2025-10-06 01:37:39.162856: Epoch 34
2025-10-06 01:37:39.163121: Current learning rate: 0.00793
2025-10-06 01:38:26.119000: Validation loss did not improve from -0.41893. Patience: 12/50
2025-10-06 01:38:26.119622: train_loss -0.806
2025-10-06 01:38:26.119779: val_loss -0.3074
2025-10-06 01:38:26.119949: Pseudo dice [np.float32(0.6392)]
2025-10-06 01:38:26.120113: Epoch time: 46.96 s
2025-10-06 01:38:27.200296: 
2025-10-06 01:38:27.200555: Epoch 35
2025-10-06 01:38:27.200740: Current learning rate: 0.00787
2025-10-06 01:39:14.039559: Validation loss did not improve from -0.41893. Patience: 13/50
2025-10-06 01:39:14.040110: train_loss -0.8102
2025-10-06 01:39:14.040246: val_loss -0.2801
2025-10-06 01:39:14.040424: Pseudo dice [np.float32(0.6424)]
2025-10-06 01:39:14.040589: Epoch time: 46.84 s
2025-10-06 01:39:15.036288: 
2025-10-06 01:39:15.036680: Epoch 36
2025-10-06 01:39:15.036906: Current learning rate: 0.00781
2025-10-06 01:40:01.862739: Validation loss did not improve from -0.41893. Patience: 14/50
2025-10-06 01:40:01.863185: train_loss -0.8156
2025-10-06 01:40:01.863325: val_loss -0.3341
2025-10-06 01:40:01.863462: Pseudo dice [np.float32(0.6514)]
2025-10-06 01:40:01.863595: Epoch time: 46.83 s
2025-10-06 01:40:02.500010: 
2025-10-06 01:40:02.500284: Epoch 37
2025-10-06 01:40:02.500475: Current learning rate: 0.00775
2025-10-06 01:40:49.376805: Validation loss did not improve from -0.41893. Patience: 15/50
2025-10-06 01:40:49.377384: train_loss -0.8142
2025-10-06 01:40:49.377561: val_loss -0.3186
2025-10-06 01:40:49.377746: Pseudo dice [np.float32(0.6545)]
2025-10-06 01:40:49.377915: Epoch time: 46.88 s
2025-10-06 01:40:50.009133: 
2025-10-06 01:40:50.009380: Epoch 38
2025-10-06 01:40:50.009547: Current learning rate: 0.00769
2025-10-06 01:41:36.947345: Validation loss did not improve from -0.41893. Patience: 16/50
2025-10-06 01:41:36.947957: train_loss -0.8188
2025-10-06 01:41:36.948136: val_loss -0.2967
2025-10-06 01:41:36.948251: Pseudo dice [np.float32(0.6361)]
2025-10-06 01:41:36.948394: Epoch time: 46.94 s
2025-10-06 01:41:37.582729: 
2025-10-06 01:41:37.583101: Epoch 39
2025-10-06 01:41:37.583324: Current learning rate: 0.00763
2025-10-06 01:42:24.491367: Validation loss did not improve from -0.41893. Patience: 17/50
2025-10-06 01:42:24.491778: train_loss -0.8243
2025-10-06 01:42:24.491966: val_loss -0.299
2025-10-06 01:42:24.492111: Pseudo dice [np.float32(0.6456)]
2025-10-06 01:42:24.492275: Epoch time: 46.91 s
2025-10-06 01:42:25.602149: 
2025-10-06 01:42:25.602578: Epoch 40
2025-10-06 01:42:25.602864: Current learning rate: 0.00756
2025-10-06 01:43:12.469809: Validation loss did not improve from -0.41893. Patience: 18/50
2025-10-06 01:43:12.470477: train_loss -0.8279
2025-10-06 01:43:12.470629: val_loss -0.3311
2025-10-06 01:43:12.470762: Pseudo dice [np.float32(0.6709)]
2025-10-06 01:43:12.470923: Epoch time: 46.87 s
2025-10-06 01:43:13.109866: 
2025-10-06 01:43:13.110187: Epoch 41
2025-10-06 01:43:13.110378: Current learning rate: 0.0075
2025-10-06 01:44:00.009844: Validation loss did not improve from -0.41893. Patience: 19/50
2025-10-06 01:44:00.010388: train_loss -0.8293
2025-10-06 01:44:00.010598: val_loss -0.2849
2025-10-06 01:44:00.010755: Pseudo dice [np.float32(0.6616)]
2025-10-06 01:44:00.010952: Epoch time: 46.9 s
2025-10-06 01:44:00.011167: Yayy! New best EMA pseudo Dice: 0.6513000130653381
2025-10-06 01:44:01.085450: 
2025-10-06 01:44:01.085827: Epoch 42
2025-10-06 01:44:01.086058: Current learning rate: 0.00744
2025-10-06 01:44:48.068053: Validation loss did not improve from -0.41893. Patience: 20/50
2025-10-06 01:44:48.068803: train_loss -0.8319
2025-10-06 01:44:48.069069: val_loss -0.2973
2025-10-06 01:44:48.069295: Pseudo dice [np.float32(0.649)]
2025-10-06 01:44:48.069524: Epoch time: 46.98 s
2025-10-06 01:44:48.696905: 
2025-10-06 01:44:48.697244: Epoch 43
2025-10-06 01:44:48.697492: Current learning rate: 0.00738
2025-10-06 01:45:35.683049: Validation loss did not improve from -0.41893. Patience: 21/50
2025-10-06 01:45:35.683524: train_loss -0.8351
2025-10-06 01:45:35.683704: val_loss -0.2786
2025-10-06 01:45:35.683833: Pseudo dice [np.float32(0.6528)]
2025-10-06 01:45:35.683982: Epoch time: 46.99 s
2025-10-06 01:45:36.314554: 
2025-10-06 01:45:36.314827: Epoch 44
2025-10-06 01:45:36.315026: Current learning rate: 0.00732
2025-10-06 01:46:23.186080: Validation loss did not improve from -0.41893. Patience: 22/50
2025-10-06 01:46:23.186879: train_loss -0.8353
2025-10-06 01:46:23.187055: val_loss -0.2756
2025-10-06 01:46:23.187201: Pseudo dice [np.float32(0.6412)]
2025-10-06 01:46:23.187398: Epoch time: 46.87 s
2025-10-06 01:46:24.237399: 
2025-10-06 01:46:24.237750: Epoch 45
2025-10-06 01:46:24.237946: Current learning rate: 0.00725
2025-10-06 01:47:11.036674: Validation loss did not improve from -0.41893. Patience: 23/50
2025-10-06 01:47:11.037298: train_loss -0.8346
2025-10-06 01:47:11.037479: val_loss -0.2866
2025-10-06 01:47:11.037630: Pseudo dice [np.float32(0.6457)]
2025-10-06 01:47:11.037907: Epoch time: 46.8 s
2025-10-06 01:47:11.667057: 
2025-10-06 01:47:11.667281: Epoch 46
2025-10-06 01:47:11.667493: Current learning rate: 0.00719
2025-10-06 01:47:58.499712: Validation loss did not improve from -0.41893. Patience: 24/50
2025-10-06 01:47:58.500504: train_loss -0.8382
2025-10-06 01:47:58.500781: val_loss -0.3062
2025-10-06 01:47:58.500986: Pseudo dice [np.float32(0.6543)]
2025-10-06 01:47:58.501221: Epoch time: 46.83 s
2025-10-06 01:47:59.333230: 
2025-10-06 01:47:59.333559: Epoch 47
2025-10-06 01:47:59.333746: Current learning rate: 0.00713
2025-10-06 01:48:46.349500: Validation loss did not improve from -0.41893. Patience: 25/50
2025-10-06 01:48:46.350036: train_loss -0.8404
2025-10-06 01:48:46.350190: val_loss -0.3474
2025-10-06 01:48:46.350376: Pseudo dice [np.float32(0.6774)]
2025-10-06 01:48:46.350512: Epoch time: 47.02 s
2025-10-06 01:48:46.350639: Yayy! New best EMA pseudo Dice: 0.652899980545044
2025-10-06 01:48:47.834478: 
2025-10-06 01:48:47.834860: Epoch 48
2025-10-06 01:48:47.835097: Current learning rate: 0.00707
2025-10-06 01:49:34.781697: Validation loss did not improve from -0.41893. Patience: 26/50
2025-10-06 01:49:34.782432: train_loss -0.8415
2025-10-06 01:49:34.782676: val_loss -0.2841
2025-10-06 01:49:34.782892: Pseudo dice [np.float32(0.6587)]
2025-10-06 01:49:34.783125: Epoch time: 46.95 s
2025-10-06 01:49:34.783319: Yayy! New best EMA pseudo Dice: 0.6535000205039978
2025-10-06 01:49:35.868774: 
2025-10-06 01:49:35.869120: Epoch 49
2025-10-06 01:49:35.869285: Current learning rate: 0.007
2025-10-06 01:50:22.683115: Validation loss did not improve from -0.41893. Patience: 27/50
2025-10-06 01:50:22.683825: train_loss -0.8471
2025-10-06 01:50:22.684163: val_loss -0.3028
2025-10-06 01:50:22.684378: Pseudo dice [np.float32(0.6496)]
2025-10-06 01:50:22.684560: Epoch time: 46.82 s
2025-10-06 01:50:23.744219: 
2025-10-06 01:50:23.744537: Epoch 50
2025-10-06 01:50:23.744742: Current learning rate: 0.00694
2025-10-06 01:51:10.535878: Validation loss did not improve from -0.41893. Patience: 28/50
2025-10-06 01:51:10.536581: train_loss -0.8441
2025-10-06 01:51:10.536769: val_loss -0.3347
2025-10-06 01:51:10.536934: Pseudo dice [np.float32(0.6869)]
2025-10-06 01:51:10.537152: Epoch time: 46.79 s
2025-10-06 01:51:10.537303: Yayy! New best EMA pseudo Dice: 0.656499981880188
2025-10-06 01:51:11.608077: 
2025-10-06 01:51:11.608451: Epoch 51
2025-10-06 01:51:11.608702: Current learning rate: 0.00688
2025-10-06 01:51:58.475068: Validation loss did not improve from -0.41893. Patience: 29/50
2025-10-06 01:51:58.475463: train_loss -0.8441
2025-10-06 01:51:58.475622: val_loss -0.3511
2025-10-06 01:51:58.475758: Pseudo dice [np.float32(0.6783)]
2025-10-06 01:51:58.475916: Epoch time: 46.87 s
2025-10-06 01:51:58.476058: Yayy! New best EMA pseudo Dice: 0.6586999893188477
2025-10-06 01:51:59.580424: 
2025-10-06 01:51:59.580797: Epoch 52
2025-10-06 01:51:59.581023: Current learning rate: 0.00682
2025-10-06 01:52:46.502742: Validation loss did not improve from -0.41893. Patience: 30/50
2025-10-06 01:52:46.503452: train_loss -0.8544
2025-10-06 01:52:46.503684: val_loss -0.3042
2025-10-06 01:52:46.504091: Pseudo dice [np.float32(0.6639)]
2025-10-06 01:52:46.504428: Epoch time: 46.92 s
2025-10-06 01:52:46.504626: Yayy! New best EMA pseudo Dice: 0.6592000126838684
2025-10-06 01:52:47.589334: 
2025-10-06 01:52:47.589682: Epoch 53
2025-10-06 01:52:47.589888: Current learning rate: 0.00675
2025-10-06 01:53:34.512967: Validation loss did not improve from -0.41893. Patience: 31/50
2025-10-06 01:53:34.513463: train_loss -0.8541
2025-10-06 01:53:34.513600: val_loss -0.267
2025-10-06 01:53:34.513710: Pseudo dice [np.float32(0.6479)]
2025-10-06 01:53:34.513839: Epoch time: 46.92 s
2025-10-06 01:53:35.137542: 
2025-10-06 01:53:35.137981: Epoch 54
2025-10-06 01:53:35.138327: Current learning rate: 0.00669
2025-10-06 01:54:21.954836: Validation loss did not improve from -0.41893. Patience: 32/50
2025-10-06 01:54:21.955403: train_loss -0.8559
2025-10-06 01:54:21.955528: val_loss -0.254
2025-10-06 01:54:21.955698: Pseudo dice [np.float32(0.6475)]
2025-10-06 01:54:21.955895: Epoch time: 46.82 s
2025-10-06 01:54:23.000059: 
2025-10-06 01:54:23.000325: Epoch 55
2025-10-06 01:54:23.000555: Current learning rate: 0.00663
2025-10-06 01:55:09.776793: Validation loss did not improve from -0.41893. Patience: 33/50
2025-10-06 01:55:09.777319: train_loss -0.8552
2025-10-06 01:55:09.777566: val_loss -0.3137
2025-10-06 01:55:09.777689: Pseudo dice [np.float32(0.6611)]
2025-10-06 01:55:09.777840: Epoch time: 46.78 s
2025-10-06 01:55:10.408396: 
2025-10-06 01:55:10.408638: Epoch 56
2025-10-06 01:55:10.408817: Current learning rate: 0.00657
2025-10-06 01:55:57.283135: Validation loss did not improve from -0.41893. Patience: 34/50
2025-10-06 01:55:57.283890: train_loss -0.8557
2025-10-06 01:55:57.284097: val_loss -0.3266
2025-10-06 01:55:57.284301: Pseudo dice [np.float32(0.676)]
2025-10-06 01:55:57.284504: Epoch time: 46.88 s
2025-10-06 01:55:57.284640: Yayy! New best EMA pseudo Dice: 0.6593000292778015
2025-10-06 01:55:58.374277: 
2025-10-06 01:55:58.374616: Epoch 57
2025-10-06 01:55:58.375099: Current learning rate: 0.0065
2025-10-06 01:56:45.366812: Validation loss did not improve from -0.41893. Patience: 35/50
2025-10-06 01:56:45.367358: train_loss -0.8597
2025-10-06 01:56:45.367551: val_loss -0.3065
2025-10-06 01:56:45.367693: Pseudo dice [np.float32(0.6754)]
2025-10-06 01:56:45.367845: Epoch time: 46.99 s
2025-10-06 01:56:45.367986: Yayy! New best EMA pseudo Dice: 0.6608999967575073
2025-10-06 01:56:46.455144: 
2025-10-06 01:56:46.455496: Epoch 58
2025-10-06 01:56:46.455708: Current learning rate: 0.00644
2025-10-06 01:57:33.427546: Validation loss did not improve from -0.41893. Patience: 36/50
2025-10-06 01:57:33.428129: train_loss -0.8607
2025-10-06 01:57:33.428306: val_loss -0.3413
2025-10-06 01:57:33.428424: Pseudo dice [np.float32(0.6791)]
2025-10-06 01:57:33.428555: Epoch time: 46.97 s
2025-10-06 01:57:33.428688: Yayy! New best EMA pseudo Dice: 0.6626999974250793
2025-10-06 01:57:34.885262: 
2025-10-06 01:57:34.885709: Epoch 59
2025-10-06 01:57:34.885967: Current learning rate: 0.00638
2025-10-06 01:58:21.739352: Validation loss did not improve from -0.41893. Patience: 37/50
2025-10-06 01:58:21.739944: train_loss -0.8602
2025-10-06 01:58:21.740154: val_loss -0.3192
2025-10-06 01:58:21.740309: Pseudo dice [np.float32(0.6812)]
2025-10-06 01:58:21.740438: Epoch time: 46.86 s
2025-10-06 01:58:22.176431: Yayy! New best EMA pseudo Dice: 0.6646000146865845
2025-10-06 01:58:23.255426: 
2025-10-06 01:58:23.255802: Epoch 60
2025-10-06 01:58:23.256017: Current learning rate: 0.00631
2025-10-06 01:59:10.089552: Validation loss did not improve from -0.41893. Patience: 38/50
2025-10-06 01:59:10.090222: train_loss -0.8621
2025-10-06 01:59:10.090459: val_loss -0.2907
2025-10-06 01:59:10.090640: Pseudo dice [np.float32(0.6627)]
2025-10-06 01:59:10.090825: Epoch time: 46.84 s
2025-10-06 01:59:10.733326: 
2025-10-06 01:59:10.733727: Epoch 61
2025-10-06 01:59:10.733946: Current learning rate: 0.00625
2025-10-06 01:59:57.708740: Validation loss did not improve from -0.41893. Patience: 39/50
2025-10-06 01:59:57.709275: train_loss -0.8611
2025-10-06 01:59:57.709462: val_loss -0.318
2025-10-06 01:59:57.709620: Pseudo dice [np.float32(0.6813)]
2025-10-06 01:59:57.709780: Epoch time: 46.98 s
2025-10-06 01:59:57.709900: Yayy! New best EMA pseudo Dice: 0.666100025177002
2025-10-06 01:59:58.858395: 
2025-10-06 01:59:58.858610: Epoch 62
2025-10-06 01:59:58.858838: Current learning rate: 0.00619
2025-10-06 02:00:45.878044: Validation loss did not improve from -0.41893. Patience: 40/50
2025-10-06 02:00:45.878855: train_loss -0.8645
2025-10-06 02:00:45.879053: val_loss -0.309
2025-10-06 02:00:45.879228: Pseudo dice [np.float32(0.6751)]
2025-10-06 02:00:45.879378: Epoch time: 47.02 s
2025-10-06 02:00:45.879509: Yayy! New best EMA pseudo Dice: 0.6669999957084656
2025-10-06 02:00:46.990808: 
2025-10-06 02:00:46.991114: Epoch 63
2025-10-06 02:00:46.991302: Current learning rate: 0.00612
2025-10-06 02:01:33.893947: Validation loss did not improve from -0.41893. Patience: 41/50
2025-10-06 02:01:33.894403: train_loss -0.8672
2025-10-06 02:01:33.894577: val_loss -0.2672
2025-10-06 02:01:33.894779: Pseudo dice [np.float32(0.6514)]
2025-10-06 02:01:33.894966: Epoch time: 46.9 s
2025-10-06 02:01:34.545341: 
2025-10-06 02:01:34.545691: Epoch 64
2025-10-06 02:01:34.545948: Current learning rate: 0.00606
2025-10-06 02:02:21.332690: Validation loss did not improve from -0.41893. Patience: 42/50
2025-10-06 02:02:21.333302: train_loss -0.8667
2025-10-06 02:02:21.333484: val_loss -0.2975
2025-10-06 02:02:21.333624: Pseudo dice [np.float32(0.6592)]
2025-10-06 02:02:21.333780: Epoch time: 46.79 s
2025-10-06 02:02:22.411240: 
2025-10-06 02:02:22.411555: Epoch 65
2025-10-06 02:02:22.411749: Current learning rate: 0.006
2025-10-06 02:03:09.217086: Validation loss did not improve from -0.41893. Patience: 43/50
2025-10-06 02:03:09.217538: train_loss -0.8679
2025-10-06 02:03:09.217694: val_loss -0.3156
2025-10-06 02:03:09.217873: Pseudo dice [np.float32(0.6729)]
2025-10-06 02:03:09.218042: Epoch time: 46.81 s
2025-10-06 02:03:09.859690: 
2025-10-06 02:03:09.860031: Epoch 66
2025-10-06 02:03:09.860211: Current learning rate: 0.00593
2025-10-06 02:03:56.675844: Validation loss did not improve from -0.41893. Patience: 44/50
2025-10-06 02:03:56.676365: train_loss -0.8672
2025-10-06 02:03:56.676574: val_loss -0.2638
2025-10-06 02:03:56.676761: Pseudo dice [np.float32(0.6641)]
2025-10-06 02:03:56.676960: Epoch time: 46.82 s
2025-10-06 02:03:57.327205: 
2025-10-06 02:03:57.327610: Epoch 67
2025-10-06 02:03:57.327896: Current learning rate: 0.00587
2025-10-06 02:04:44.260816: Validation loss did not improve from -0.41893. Patience: 45/50
2025-10-06 02:04:44.261322: train_loss -0.8674
2025-10-06 02:04:44.261502: val_loss -0.2686
2025-10-06 02:04:44.261648: Pseudo dice [np.float32(0.6511)]
2025-10-06 02:04:44.261802: Epoch time: 46.93 s
2025-10-06 02:04:44.904528: 
2025-10-06 02:04:44.904780: Epoch 68
2025-10-06 02:04:44.904976: Current learning rate: 0.00581
2025-10-06 02:05:31.717546: Validation loss did not improve from -0.41893. Patience: 46/50
2025-10-06 02:05:31.718121: train_loss -0.8688
2025-10-06 02:05:31.718278: val_loss -0.329
2025-10-06 02:05:31.718400: Pseudo dice [np.float32(0.6773)]
2025-10-06 02:05:31.718709: Epoch time: 46.81 s
2025-10-06 02:05:32.357213: 
2025-10-06 02:05:32.357518: Epoch 69
2025-10-06 02:05:32.357705: Current learning rate: 0.00574
2025-10-06 02:06:19.181447: Validation loss did not improve from -0.41893. Patience: 47/50
2025-10-06 02:06:19.181951: train_loss -0.871
2025-10-06 02:06:19.182126: val_loss -0.2823
2025-10-06 02:06:19.182274: Pseudo dice [np.float32(0.6754)]
2025-10-06 02:06:19.182424: Epoch time: 46.83 s
2025-10-06 02:06:20.286228: 
2025-10-06 02:06:20.286514: Epoch 70
2025-10-06 02:06:20.286683: Current learning rate: 0.00568
2025-10-06 02:07:07.036149: Validation loss did not improve from -0.41893. Patience: 48/50
2025-10-06 02:07:07.036663: train_loss -0.8716
2025-10-06 02:07:07.036818: val_loss -0.3155
2025-10-06 02:07:07.036959: Pseudo dice [np.float32(0.6766)]
2025-10-06 02:07:07.037091: Epoch time: 46.75 s
2025-10-06 02:07:07.037202: Yayy! New best EMA pseudo Dice: 0.6674000024795532
2025-10-06 02:07:08.493797: 
2025-10-06 02:07:08.494157: Epoch 71
2025-10-06 02:07:08.494340: Current learning rate: 0.00562
2025-10-06 02:07:55.316287: Validation loss did not improve from -0.41893. Patience: 49/50
2025-10-06 02:07:55.316873: train_loss -0.8732
2025-10-06 02:07:55.317009: val_loss -0.3042
2025-10-06 02:07:55.317149: Pseudo dice [np.float32(0.6659)]
2025-10-06 02:07:55.317340: Epoch time: 46.82 s
2025-10-06 02:07:55.966088: 
2025-10-06 02:07:55.966448: Epoch 72
2025-10-06 02:07:55.966644: Current learning rate: 0.00555
2025-10-06 02:08:42.904450: Validation loss did not improve from -0.41893. Patience: 50/50
2025-10-06 02:08:42.905059: train_loss -0.8734
2025-10-06 02:08:42.905216: val_loss -0.2674
2025-10-06 02:08:42.905327: Pseudo dice [np.float32(0.6564)]
2025-10-06 02:08:42.905472: Epoch time: 46.94 s
2025-10-06 02:08:43.558356: 
2025-10-06 02:08:43.558678: Epoch 73
2025-10-06 02:08:43.558931: Current learning rate: 0.00549
2025-10-06 02:09:30.430931: Validation loss did not improve from -0.41893. Patience: 51/50
2025-10-06 02:09:30.431453: train_loss -0.8747
2025-10-06 02:09:30.431618: val_loss -0.3025
2025-10-06 02:09:30.431804: Pseudo dice [np.float32(0.674)]
2025-10-06 02:09:30.432006: Epoch time: 46.87 s
2025-10-06 02:09:31.073519: 
2025-10-06 02:09:31.073807: Epoch 74
2025-10-06 02:09:31.073989: Current learning rate: 0.00542
2025-10-06 02:10:17.811424: Validation loss did not improve from -0.41893. Patience: 52/50
2025-10-06 02:10:17.812275: train_loss -0.8763
2025-10-06 02:10:17.812462: val_loss -0.3279
2025-10-06 02:10:17.812608: Pseudo dice [np.float32(0.678)]
2025-10-06 02:10:17.812824: Epoch time: 46.74 s
2025-10-06 02:10:18.248024: Yayy! New best EMA pseudo Dice: 0.6679999828338623
2025-10-06 02:10:19.313296: 
2025-10-06 02:10:19.313558: Epoch 75
2025-10-06 02:10:19.313833: Current learning rate: 0.00536
2025-10-06 02:11:05.998011: Validation loss did not improve from -0.41893. Patience: 53/50
2025-10-06 02:11:05.998563: train_loss -0.8799
2025-10-06 02:11:05.998857: val_loss -0.2673
2025-10-06 02:11:05.999058: Pseudo dice [np.float32(0.6654)]
2025-10-06 02:11:05.999301: Epoch time: 46.69 s
2025-10-06 02:11:06.646510: 
2025-10-06 02:11:06.646956: Epoch 76
2025-10-06 02:11:06.647258: Current learning rate: 0.00529
2025-10-06 02:11:53.454986: Validation loss did not improve from -0.41893. Patience: 54/50
2025-10-06 02:11:53.455567: train_loss -0.8814
2025-10-06 02:11:53.455779: val_loss -0.2716
2025-10-06 02:11:53.455951: Pseudo dice [np.float32(0.6587)]
2025-10-06 02:11:53.456080: Epoch time: 46.81 s
2025-10-06 02:11:54.102569: 
2025-10-06 02:11:54.102912: Epoch 77
2025-10-06 02:11:54.103127: Current learning rate: 0.00523
2025-10-06 02:12:41.056243: Validation loss did not improve from -0.41893. Patience: 55/50
2025-10-06 02:12:41.056823: train_loss -0.8808
2025-10-06 02:12:41.057021: val_loss -0.2864
2025-10-06 02:12:41.057191: Pseudo dice [np.float32(0.6593)]
2025-10-06 02:12:41.057391: Epoch time: 46.95 s
2025-10-06 02:12:41.714145: 
2025-10-06 02:12:41.714606: Epoch 78
2025-10-06 02:12:41.714871: Current learning rate: 0.00517
2025-10-06 02:13:28.517091: Validation loss did not improve from -0.41893. Patience: 56/50
2025-10-06 02:13:28.517855: train_loss -0.8801
2025-10-06 02:13:28.518109: val_loss -0.3125
2025-10-06 02:13:28.518339: Pseudo dice [np.float32(0.6795)]
2025-10-06 02:13:28.518588: Epoch time: 46.8 s
2025-10-06 02:13:29.172343: 
2025-10-06 02:13:29.172812: Epoch 79
2025-10-06 02:13:29.173072: Current learning rate: 0.0051
2025-10-06 02:14:15.985970: Validation loss did not improve from -0.41893. Patience: 57/50
2025-10-06 02:14:15.986504: train_loss -0.8768
2025-10-06 02:14:15.986664: val_loss -0.3058
2025-10-06 02:14:15.986860: Pseudo dice [np.float32(0.6769)]
2025-10-06 02:14:15.987022: Epoch time: 46.81 s
2025-10-06 02:14:16.421523: Yayy! New best EMA pseudo Dice: 0.66839998960495
2025-10-06 02:14:17.507157: 
2025-10-06 02:14:17.507447: Epoch 80
2025-10-06 02:14:17.507648: Current learning rate: 0.00504
2025-10-06 02:15:04.273709: Validation loss did not improve from -0.41893. Patience: 58/50
2025-10-06 02:15:04.274626: train_loss -0.8766
2025-10-06 02:15:04.275018: val_loss -0.2571
2025-10-06 02:15:04.275244: Pseudo dice [np.float32(0.6714)]
2025-10-06 02:15:04.275448: Epoch time: 46.77 s
2025-10-06 02:15:04.275629: Yayy! New best EMA pseudo Dice: 0.6686999797821045
2025-10-06 02:15:05.378035: 
2025-10-06 02:15:05.378371: Epoch 81
2025-10-06 02:15:05.378620: Current learning rate: 0.00497
2025-10-06 02:15:52.219967: Validation loss did not improve from -0.41893. Patience: 59/50
2025-10-06 02:15:52.220431: train_loss -0.8775
2025-10-06 02:15:52.220590: val_loss -0.282
2025-10-06 02:15:52.220702: Pseudo dice [np.float32(0.6574)]
2025-10-06 02:15:52.220837: Epoch time: 46.84 s
2025-10-06 02:15:53.257473: 
2025-10-06 02:15:53.257838: Epoch 82
2025-10-06 02:15:53.258021: Current learning rate: 0.00491
2025-10-06 02:16:40.184195: Validation loss did not improve from -0.41893. Patience: 60/50
2025-10-06 02:16:40.184756: train_loss -0.8816
2025-10-06 02:16:40.184995: val_loss -0.3066
2025-10-06 02:16:40.185111: Pseudo dice [np.float32(0.6773)]
2025-10-06 02:16:40.185257: Epoch time: 46.93 s
2025-10-06 02:16:40.812034: 
2025-10-06 02:16:40.812394: Epoch 83
2025-10-06 02:16:40.812618: Current learning rate: 0.00484
2025-10-06 02:17:27.613956: Validation loss did not improve from -0.41893. Patience: 61/50
2025-10-06 02:17:27.614511: train_loss -0.8848
2025-10-06 02:17:27.614646: val_loss -0.2881
2025-10-06 02:17:27.614799: Pseudo dice [np.float32(0.6685)]
2025-10-06 02:17:27.615011: Epoch time: 46.8 s
2025-10-06 02:17:28.245623: 
2025-10-06 02:17:28.245955: Epoch 84
2025-10-06 02:17:28.246150: Current learning rate: 0.00478
2025-10-06 02:18:15.093967: Validation loss did not improve from -0.41893. Patience: 62/50
2025-10-06 02:18:15.094568: train_loss -0.8852
2025-10-06 02:18:15.094750: val_loss -0.2982
2025-10-06 02:18:15.094887: Pseudo dice [np.float32(0.6674)]
2025-10-06 02:18:15.095248: Epoch time: 46.85 s
2025-10-06 02:18:16.205100: 
2025-10-06 02:18:16.205514: Epoch 85
2025-10-06 02:18:16.205774: Current learning rate: 0.00471
2025-10-06 02:19:03.052600: Validation loss did not improve from -0.41893. Patience: 63/50
2025-10-06 02:19:03.053144: train_loss -0.8888
2025-10-06 02:19:03.053292: val_loss -0.2587
2025-10-06 02:19:03.053407: Pseudo dice [np.float32(0.6771)]
2025-10-06 02:19:03.053568: Epoch time: 46.85 s
2025-10-06 02:19:03.053709: Yayy! New best EMA pseudo Dice: 0.6693000197410583
2025-10-06 02:19:04.138452: 
2025-10-06 02:19:04.138852: Epoch 86
2025-10-06 02:19:04.139066: Current learning rate: 0.00465
2025-10-06 02:19:51.017488: Validation loss did not improve from -0.41893. Patience: 64/50
2025-10-06 02:19:51.018160: train_loss -0.8886
2025-10-06 02:19:51.018301: val_loss -0.2887
2025-10-06 02:19:51.018416: Pseudo dice [np.float32(0.6791)]
2025-10-06 02:19:51.018564: Epoch time: 46.88 s
2025-10-06 02:19:51.018688: Yayy! New best EMA pseudo Dice: 0.6703000068664551
2025-10-06 02:19:52.114881: 
2025-10-06 02:19:52.115250: Epoch 87
2025-10-06 02:19:52.115434: Current learning rate: 0.00458
2025-10-06 02:20:38.903038: Validation loss did not improve from -0.41893. Patience: 65/50
2025-10-06 02:20:38.903582: train_loss -0.887
2025-10-06 02:20:38.903882: val_loss -0.2411
2025-10-06 02:20:38.904070: Pseudo dice [np.float32(0.6551)]
2025-10-06 02:20:38.904282: Epoch time: 46.79 s
2025-10-06 02:20:39.527009: 
2025-10-06 02:20:39.527311: Epoch 88
2025-10-06 02:20:39.527549: Current learning rate: 0.00452
2025-10-06 02:21:26.292670: Validation loss did not improve from -0.41893. Patience: 66/50
2025-10-06 02:21:26.293230: train_loss -0.8873
2025-10-06 02:21:26.293440: val_loss -0.2382
2025-10-06 02:21:26.293624: Pseudo dice [np.float32(0.6605)]
2025-10-06 02:21:26.293814: Epoch time: 46.77 s
2025-10-06 02:21:26.928185: 
2025-10-06 02:21:26.928555: Epoch 89
2025-10-06 02:21:26.928871: Current learning rate: 0.00445
2025-10-06 02:22:13.701253: Validation loss did not improve from -0.41893. Patience: 67/50
2025-10-06 02:22:13.701905: train_loss -0.888
2025-10-06 02:22:13.702224: val_loss -0.2875
2025-10-06 02:22:13.702451: Pseudo dice [np.float32(0.6817)]
2025-10-06 02:22:13.702656: Epoch time: 46.77 s
2025-10-06 02:22:14.776947: 
2025-10-06 02:22:14.777307: Epoch 90
2025-10-06 02:22:14.777510: Current learning rate: 0.00438
2025-10-06 02:23:01.521487: Validation loss did not improve from -0.41893. Patience: 68/50
2025-10-06 02:23:01.522146: train_loss -0.8888
2025-10-06 02:23:01.522325: val_loss -0.2832
2025-10-06 02:23:01.522476: Pseudo dice [np.float32(0.6677)]
2025-10-06 02:23:01.522617: Epoch time: 46.75 s
2025-10-06 02:23:02.158252: 
2025-10-06 02:23:02.158504: Epoch 91
2025-10-06 02:23:02.158768: Current learning rate: 0.00432
2025-10-06 02:23:48.927519: Validation loss did not improve from -0.41893. Patience: 69/50
2025-10-06 02:23:48.928285: train_loss -0.8897
2025-10-06 02:23:48.928423: val_loss -0.2745
2025-10-06 02:23:48.928637: Pseudo dice [np.float32(0.6688)]
2025-10-06 02:23:48.928921: Epoch time: 46.77 s
2025-10-06 02:23:49.555697: 
2025-10-06 02:23:49.556010: Epoch 92
2025-10-06 02:23:49.556204: Current learning rate: 0.00425
2025-10-06 02:24:36.392931: Validation loss did not improve from -0.41893. Patience: 70/50
2025-10-06 02:24:36.393474: train_loss -0.8897
2025-10-06 02:24:36.393612: val_loss -0.2514
2025-10-06 02:24:36.393777: Pseudo dice [np.float32(0.6615)]
2025-10-06 02:24:36.393937: Epoch time: 46.84 s
2025-10-06 02:24:37.017469: 
2025-10-06 02:24:37.017843: Epoch 93
2025-10-06 02:24:37.018051: Current learning rate: 0.00419
2025-10-06 02:25:23.823413: Validation loss did not improve from -0.41893. Patience: 71/50
2025-10-06 02:25:23.824145: train_loss -0.8904
2025-10-06 02:25:23.824508: val_loss -0.2462
2025-10-06 02:25:23.824841: Pseudo dice [np.float32(0.6515)]
2025-10-06 02:25:23.825181: Epoch time: 46.81 s
2025-10-06 02:25:24.843072: 
2025-10-06 02:25:24.843456: Epoch 94
2025-10-06 02:25:24.843679: Current learning rate: 0.00412
2025-10-06 02:26:11.664352: Validation loss did not improve from -0.41893. Patience: 72/50
2025-10-06 02:26:11.665030: train_loss -0.8933
2025-10-06 02:26:11.665345: val_loss -0.3043
2025-10-06 02:26:11.665606: Pseudo dice [np.float32(0.6823)]
2025-10-06 02:26:11.665882: Epoch time: 46.82 s
2025-10-06 02:26:12.744284: 
2025-10-06 02:26:12.744722: Epoch 95
2025-10-06 02:26:12.744984: Current learning rate: 0.00405
2025-10-06 02:26:59.550937: Validation loss did not improve from -0.41893. Patience: 73/50
2025-10-06 02:26:59.551722: train_loss -0.8931
2025-10-06 02:26:59.552007: val_loss -0.2334
2025-10-06 02:26:59.552254: Pseudo dice [np.float32(0.6581)]
2025-10-06 02:26:59.552574: Epoch time: 46.81 s
2025-10-06 02:27:00.198283: 
2025-10-06 02:27:00.198708: Epoch 96
2025-10-06 02:27:00.199002: Current learning rate: 0.00399
2025-10-06 02:27:47.019812: Validation loss did not improve from -0.41893. Patience: 74/50
2025-10-06 02:27:47.020433: train_loss -0.8952
2025-10-06 02:27:47.020610: val_loss -0.272
2025-10-06 02:27:47.020759: Pseudo dice [np.float32(0.6702)]
2025-10-06 02:27:47.020924: Epoch time: 46.82 s
2025-10-06 02:27:47.658399: 
2025-10-06 02:27:47.658821: Epoch 97
2025-10-06 02:27:47.659049: Current learning rate: 0.00392
2025-10-06 02:28:34.428676: Validation loss did not improve from -0.41893. Patience: 75/50
2025-10-06 02:28:34.429262: train_loss -0.8939
2025-10-06 02:28:34.429484: val_loss -0.1891
2025-10-06 02:28:34.429635: Pseudo dice [np.float32(0.6406)]
2025-10-06 02:28:34.429788: Epoch time: 46.77 s
2025-10-06 02:28:35.074524: 
2025-10-06 02:28:35.074901: Epoch 98
2025-10-06 02:28:35.075121: Current learning rate: 0.00385
2025-10-06 02:29:21.900245: Validation loss did not improve from -0.41893. Patience: 76/50
2025-10-06 02:29:21.901019: train_loss -0.8958
2025-10-06 02:29:21.901336: val_loss -0.293
2025-10-06 02:29:21.901537: Pseudo dice [np.float32(0.6752)]
2025-10-06 02:29:21.901733: Epoch time: 46.83 s
2025-10-06 02:29:22.564321: 
2025-10-06 02:29:22.564691: Epoch 99
2025-10-06 02:29:22.564914: Current learning rate: 0.00379
2025-10-06 02:30:09.391449: Validation loss did not improve from -0.41893. Patience: 77/50
2025-10-06 02:30:09.392032: train_loss -0.8958
2025-10-06 02:30:09.392204: val_loss -0.2989
2025-10-06 02:30:09.392404: Pseudo dice [np.float32(0.6654)]
2025-10-06 02:30:09.392622: Epoch time: 46.83 s
2025-10-06 02:30:10.476200: 
2025-10-06 02:30:10.476518: Epoch 100
2025-10-06 02:30:10.476715: Current learning rate: 0.00372
2025-10-06 02:30:57.235029: Validation loss did not improve from -0.41893. Patience: 78/50
2025-10-06 02:30:57.235560: train_loss -0.896
2025-10-06 02:30:57.235685: val_loss -0.2603
2025-10-06 02:30:57.235835: Pseudo dice [np.float32(0.6557)]
2025-10-06 02:30:57.235973: Epoch time: 46.76 s
2025-10-06 02:30:57.881264: 
2025-10-06 02:30:57.881636: Epoch 101
2025-10-06 02:30:57.881834: Current learning rate: 0.00365
2025-10-06 02:31:44.718664: Validation loss did not improve from -0.41893. Patience: 79/50
2025-10-06 02:31:44.719241: train_loss -0.8968
2025-10-06 02:31:44.719407: val_loss -0.3205
2025-10-06 02:31:44.719552: Pseudo dice [np.float32(0.6803)]
2025-10-06 02:31:44.719727: Epoch time: 46.84 s
2025-10-06 02:31:45.363531: 
2025-10-06 02:31:45.363886: Epoch 102
2025-10-06 02:31:45.364064: Current learning rate: 0.00359
2025-10-06 02:32:32.236871: Validation loss did not improve from -0.41893. Patience: 80/50
2025-10-06 02:32:32.237331: train_loss -0.898
2025-10-06 02:32:32.237482: val_loss -0.3068
2025-10-06 02:32:32.237597: Pseudo dice [np.float32(0.6817)]
2025-10-06 02:32:32.237751: Epoch time: 46.87 s
2025-10-06 02:32:32.871399: 
2025-10-06 02:32:32.871772: Epoch 103
2025-10-06 02:32:32.872061: Current learning rate: 0.00352
2025-10-06 02:33:19.715599: Validation loss did not improve from -0.41893. Patience: 81/50
2025-10-06 02:33:19.716276: train_loss -0.8961
2025-10-06 02:33:19.716425: val_loss -0.2273
2025-10-06 02:33:19.716544: Pseudo dice [np.float32(0.6462)]
2025-10-06 02:33:19.716704: Epoch time: 46.85 s
2025-10-06 02:33:20.360015: 
2025-10-06 02:33:20.360251: Epoch 104
2025-10-06 02:33:20.360428: Current learning rate: 0.00345
2025-10-06 02:34:07.214146: Validation loss did not improve from -0.41893. Patience: 82/50
2025-10-06 02:34:07.214719: train_loss -0.8984
2025-10-06 02:34:07.214867: val_loss -0.2867
2025-10-06 02:34:07.215030: Pseudo dice [np.float32(0.6726)]
2025-10-06 02:34:07.215178: Epoch time: 46.86 s
2025-10-06 02:34:08.309515: 
2025-10-06 02:34:08.309897: Epoch 105
2025-10-06 02:34:08.310143: Current learning rate: 0.00338
2025-10-06 02:34:55.090024: Validation loss did not improve from -0.41893. Patience: 83/50
2025-10-06 02:34:55.090570: train_loss -0.8996
2025-10-06 02:34:55.090750: val_loss -0.2955
2025-10-06 02:34:55.090916: Pseudo dice [np.float32(0.6685)]
2025-10-06 02:34:55.091084: Epoch time: 46.78 s
2025-10-06 02:34:56.109054: 
2025-10-06 02:34:56.109455: Epoch 106
2025-10-06 02:34:56.109635: Current learning rate: 0.00332
2025-10-06 02:35:42.957286: Validation loss did not improve from -0.41893. Patience: 84/50
2025-10-06 02:35:42.957846: train_loss -0.8982
2025-10-06 02:35:42.958053: val_loss -0.2535
2025-10-06 02:35:42.958236: Pseudo dice [np.float32(0.661)]
2025-10-06 02:35:42.958430: Epoch time: 46.85 s
2025-10-06 02:35:43.606348: 
2025-10-06 02:35:43.606702: Epoch 107
2025-10-06 02:35:43.606908: Current learning rate: 0.00325
2025-10-06 02:36:30.484355: Validation loss did not improve from -0.41893. Patience: 85/50
2025-10-06 02:36:30.484921: train_loss -0.8997
2025-10-06 02:36:30.485113: val_loss -0.255
2025-10-06 02:36:30.485286: Pseudo dice [np.float32(0.6712)]
2025-10-06 02:36:30.485438: Epoch time: 46.88 s
2025-10-06 02:36:31.132356: 
2025-10-06 02:36:31.132655: Epoch 108
2025-10-06 02:36:31.132951: Current learning rate: 0.00318
2025-10-06 02:37:18.046288: Validation loss did not improve from -0.41893. Patience: 86/50
2025-10-06 02:37:18.046874: train_loss -0.9032
2025-10-06 02:37:18.047056: val_loss -0.2486
2025-10-06 02:37:18.047167: Pseudo dice [np.float32(0.6809)]
2025-10-06 02:37:18.047354: Epoch time: 46.92 s
2025-10-06 02:37:18.688351: 
2025-10-06 02:37:18.688740: Epoch 109
2025-10-06 02:37:18.688943: Current learning rate: 0.00311
2025-10-06 02:38:05.552650: Validation loss did not improve from -0.41893. Patience: 87/50
2025-10-06 02:38:05.553312: train_loss -0.9007
2025-10-06 02:38:05.553478: val_loss -0.218
2025-10-06 02:38:05.553623: Pseudo dice [np.float32(0.6568)]
2025-10-06 02:38:05.553835: Epoch time: 46.87 s
2025-10-06 02:38:06.655832: 
2025-10-06 02:38:06.656148: Epoch 110
2025-10-06 02:38:06.656341: Current learning rate: 0.00304
2025-10-06 02:38:53.577693: Validation loss did not improve from -0.41893. Patience: 88/50
2025-10-06 02:38:53.578243: train_loss -0.9021
2025-10-06 02:38:53.578389: val_loss -0.2979
2025-10-06 02:38:53.578500: Pseudo dice [np.float32(0.6825)]
2025-10-06 02:38:53.578672: Epoch time: 46.92 s
2025-10-06 02:38:54.221596: 
2025-10-06 02:38:54.221896: Epoch 111
2025-10-06 02:38:54.222070: Current learning rate: 0.00297
2025-10-06 02:39:41.091673: Validation loss did not improve from -0.41893. Patience: 89/50
2025-10-06 02:39:41.092219: train_loss -0.9023
2025-10-06 02:39:41.092393: val_loss -0.2447
2025-10-06 02:39:41.092506: Pseudo dice [np.float32(0.6608)]
2025-10-06 02:39:41.092634: Epoch time: 46.87 s
2025-10-06 02:39:41.732399: 
2025-10-06 02:39:41.732633: Epoch 112
2025-10-06 02:39:41.732804: Current learning rate: 0.00291
2025-10-06 02:40:28.621731: Validation loss did not improve from -0.41893. Patience: 90/50
2025-10-06 02:40:28.622281: train_loss -0.9033
2025-10-06 02:40:28.622447: val_loss -0.2182
2025-10-06 02:40:28.622615: Pseudo dice [np.float32(0.6474)]
2025-10-06 02:40:28.622781: Epoch time: 46.89 s
2025-10-06 02:40:29.267208: 
2025-10-06 02:40:29.267478: Epoch 113
2025-10-06 02:40:29.267704: Current learning rate: 0.00284
2025-10-06 02:41:16.180060: Validation loss did not improve from -0.41893. Patience: 91/50
2025-10-06 02:41:16.180678: train_loss -0.9031
2025-10-06 02:41:16.180877: val_loss -0.2623
2025-10-06 02:41:16.181043: Pseudo dice [np.float32(0.6697)]
2025-10-06 02:41:16.181252: Epoch time: 46.91 s
2025-10-06 02:41:16.830839: 
2025-10-06 02:41:16.831208: Epoch 114
2025-10-06 02:41:16.831448: Current learning rate: 0.00277
2025-10-06 02:42:03.722365: Validation loss did not improve from -0.41893. Patience: 92/50
2025-10-06 02:42:03.722959: train_loss -0.9034
2025-10-06 02:42:03.723148: val_loss -0.2998
2025-10-06 02:42:03.723269: Pseudo dice [np.float32(0.6688)]
2025-10-06 02:42:03.723409: Epoch time: 46.89 s
2025-10-06 02:42:04.832427: 
2025-10-06 02:42:04.832794: Epoch 115
2025-10-06 02:42:04.833018: Current learning rate: 0.0027
2025-10-06 02:42:51.705743: Validation loss did not improve from -0.41893. Patience: 93/50
2025-10-06 02:42:51.706286: train_loss -0.9062
2025-10-06 02:42:51.706488: val_loss -0.239
2025-10-06 02:42:51.706640: Pseudo dice [np.float32(0.6682)]
2025-10-06 02:42:51.706815: Epoch time: 46.87 s
2025-10-06 02:42:52.360483: 
2025-10-06 02:42:52.360845: Epoch 116
2025-10-06 02:42:52.361146: Current learning rate: 0.00263
2025-10-06 02:43:39.177408: Validation loss did not improve from -0.41893. Patience: 94/50
2025-10-06 02:43:39.177953: train_loss -0.905
2025-10-06 02:43:39.178122: val_loss -0.2803
2025-10-06 02:43:39.178239: Pseudo dice [np.float32(0.664)]
2025-10-06 02:43:39.178385: Epoch time: 46.82 s
2025-10-06 02:43:39.822923: 
2025-10-06 02:43:39.823169: Epoch 117
2025-10-06 02:43:39.823411: Current learning rate: 0.00256
2025-10-06 02:44:26.619236: Validation loss did not improve from -0.41893. Patience: 95/50
2025-10-06 02:44:26.619741: train_loss -0.9075
2025-10-06 02:44:26.619925: val_loss -0.262
2025-10-06 02:44:26.620046: Pseudo dice [np.float32(0.6789)]
2025-10-06 02:44:26.620252: Epoch time: 46.8 s
2025-10-06 02:44:27.649208: 
2025-10-06 02:44:27.649430: Epoch 118
2025-10-06 02:44:27.649617: Current learning rate: 0.00249
2025-10-06 02:45:14.555654: Validation loss did not improve from -0.41893. Patience: 96/50
2025-10-06 02:45:14.556206: train_loss -0.9043
2025-10-06 02:45:14.556449: val_loss -0.2731
2025-10-06 02:45:14.556594: Pseudo dice [np.float32(0.6787)]
2025-10-06 02:45:14.556769: Epoch time: 46.91 s
2025-10-06 02:45:15.210312: 
2025-10-06 02:45:15.210531: Epoch 119
2025-10-06 02:45:15.210701: Current learning rate: 0.00242
2025-10-06 02:46:02.126431: Validation loss did not improve from -0.41893. Patience: 97/50
2025-10-06 02:46:02.126865: train_loss -0.905
2025-10-06 02:46:02.126995: val_loss -0.2449
2025-10-06 02:46:02.127135: Pseudo dice [np.float32(0.6683)]
2025-10-06 02:46:02.127264: Epoch time: 46.92 s
2025-10-06 02:46:03.218776: 
2025-10-06 02:46:03.219026: Epoch 120
2025-10-06 02:46:03.219195: Current learning rate: 0.00235
2025-10-06 02:46:50.170818: Validation loss did not improve from -0.41893. Patience: 98/50
2025-10-06 02:46:50.171403: train_loss -0.9056
2025-10-06 02:46:50.171620: val_loss -0.2921
2025-10-06 02:46:50.171764: Pseudo dice [np.float32(0.6858)]
2025-10-06 02:46:50.172042: Epoch time: 46.95 s
2025-10-06 02:46:50.172255: Yayy! New best EMA pseudo Dice: 0.6703000068664551
2025-10-06 02:46:51.284179: 
2025-10-06 02:46:51.284530: Epoch 121
2025-10-06 02:46:51.284793: Current learning rate: 0.00228
2025-10-06 02:47:38.227667: Validation loss did not improve from -0.41893. Patience: 99/50
2025-10-06 02:47:38.228487: train_loss -0.9051
2025-10-06 02:47:38.228751: val_loss -0.2915
2025-10-06 02:47:38.228947: Pseudo dice [np.float32(0.6842)]
2025-10-06 02:47:38.229129: Epoch time: 46.94 s
2025-10-06 02:47:38.229317: Yayy! New best EMA pseudo Dice: 0.6717000007629395
2025-10-06 02:47:39.350046: 
2025-10-06 02:47:39.350367: Epoch 122
2025-10-06 02:47:39.350556: Current learning rate: 0.00221
2025-10-06 02:48:26.255743: Validation loss did not improve from -0.41893. Patience: 100/50
2025-10-06 02:48:26.256369: train_loss -0.9065
2025-10-06 02:48:26.256723: val_loss -0.293
2025-10-06 02:48:26.256979: Pseudo dice [np.float32(0.6842)]
2025-10-06 02:48:26.257262: Epoch time: 46.91 s
2025-10-06 02:48:26.257442: Yayy! New best EMA pseudo Dice: 0.6729999780654907
2025-10-06 02:48:27.434335: 
2025-10-06 02:48:27.434721: Epoch 123
2025-10-06 02:48:27.434982: Current learning rate: 0.00214
2025-10-06 02:49:14.391294: Validation loss did not improve from -0.41893. Patience: 101/50
2025-10-06 02:49:14.391797: train_loss -0.9077
2025-10-06 02:49:14.392004: val_loss -0.2448
2025-10-06 02:49:14.392187: Pseudo dice [np.float32(0.657)]
2025-10-06 02:49:14.392354: Epoch time: 46.96 s
2025-10-06 02:49:15.047116: 
2025-10-06 02:49:15.047473: Epoch 124
2025-10-06 02:49:15.047660: Current learning rate: 0.00207
2025-10-06 02:50:01.959967: Validation loss did not improve from -0.41893. Patience: 102/50
2025-10-06 02:50:01.960464: train_loss -0.9088
2025-10-06 02:50:01.960620: val_loss -0.2495
2025-10-06 02:50:01.960754: Pseudo dice [np.float32(0.659)]
2025-10-06 02:50:01.960972: Epoch time: 46.91 s
2025-10-06 02:50:03.059784: 
2025-10-06 02:50:03.060183: Epoch 125
2025-10-06 02:50:03.060440: Current learning rate: 0.00199
2025-10-06 02:50:49.935592: Validation loss did not improve from -0.41893. Patience: 103/50
2025-10-06 02:50:49.936085: train_loss -0.9067
2025-10-06 02:50:49.936272: val_loss -0.2242
2025-10-06 02:50:49.936418: Pseudo dice [np.float32(0.6625)]
2025-10-06 02:50:49.936639: Epoch time: 46.88 s
2025-10-06 02:50:50.585540: 
2025-10-06 02:50:50.585978: Epoch 126
2025-10-06 02:50:50.586252: Current learning rate: 0.00192
2025-10-06 02:51:37.463703: Validation loss did not improve from -0.41893. Patience: 104/50
2025-10-06 02:51:37.464360: train_loss -0.9065
2025-10-06 02:51:37.464705: val_loss -0.2544
2025-10-06 02:51:37.465054: Pseudo dice [np.float32(0.6599)]
2025-10-06 02:51:37.465275: Epoch time: 46.88 s
2025-10-06 02:51:38.120610: 
2025-10-06 02:51:38.120889: Epoch 127
2025-10-06 02:51:38.121060: Current learning rate: 0.00185
2025-10-06 02:52:24.999477: Validation loss did not improve from -0.41893. Patience: 105/50
2025-10-06 02:52:24.999983: train_loss -0.9069
2025-10-06 02:52:25.000150: val_loss -0.3018
2025-10-06 02:52:25.000291: Pseudo dice [np.float32(0.6693)]
2025-10-06 02:52:25.000445: Epoch time: 46.88 s
2025-10-06 02:52:25.652342: 
2025-10-06 02:52:25.652724: Epoch 128
2025-10-06 02:52:25.652957: Current learning rate: 0.00178
2025-10-06 02:53:12.550279: Validation loss did not improve from -0.41893. Patience: 106/50
2025-10-06 02:53:12.551056: train_loss -0.9083
2025-10-06 02:53:12.551232: val_loss -0.2633
2025-10-06 02:53:12.551344: Pseudo dice [np.float32(0.6604)]
2025-10-06 02:53:12.551473: Epoch time: 46.9 s
2025-10-06 02:53:13.192485: 
2025-10-06 02:53:13.192914: Epoch 129
2025-10-06 02:53:13.193125: Current learning rate: 0.0017
2025-10-06 02:54:00.075719: Validation loss did not improve from -0.41893. Patience: 107/50
2025-10-06 02:54:00.076275: train_loss -0.9082
2025-10-06 02:54:00.076560: val_loss -0.26
2025-10-06 02:54:00.076780: Pseudo dice [np.float32(0.6706)]
2025-10-06 02:54:00.077061: Epoch time: 46.88 s
2025-10-06 02:54:01.582034: 
2025-10-06 02:54:01.582363: Epoch 130
2025-10-06 02:54:01.582612: Current learning rate: 0.00163
2025-10-06 02:54:48.407330: Validation loss did not improve from -0.41893. Patience: 108/50
2025-10-06 02:54:48.407916: train_loss -0.9093
2025-10-06 02:54:48.408120: val_loss -0.2864
2025-10-06 02:54:48.408271: Pseudo dice [np.float32(0.6818)]
2025-10-06 02:54:48.408415: Epoch time: 46.83 s
2025-10-06 02:54:49.050072: 
2025-10-06 02:54:49.050418: Epoch 131
2025-10-06 02:54:49.050604: Current learning rate: 0.00156
2025-10-06 02:55:35.926942: Validation loss did not improve from -0.41893. Patience: 109/50
2025-10-06 02:55:35.927430: train_loss -0.9095
2025-10-06 02:55:35.927610: val_loss -0.2053
2025-10-06 02:55:35.927761: Pseudo dice [np.float32(0.6491)]
2025-10-06 02:55:35.927963: Epoch time: 46.88 s
2025-10-06 02:55:36.560087: 
2025-10-06 02:55:36.560539: Epoch 132
2025-10-06 02:55:36.560757: Current learning rate: 0.00148
2025-10-06 02:56:23.489083: Validation loss did not improve from -0.41893. Patience: 110/50
2025-10-06 02:56:23.489644: train_loss -0.9107
2025-10-06 02:56:23.489853: val_loss -0.2388
2025-10-06 02:56:23.489989: Pseudo dice [np.float32(0.6544)]
2025-10-06 02:56:23.490131: Epoch time: 46.93 s
2025-10-06 02:56:24.137182: 
2025-10-06 02:56:24.137522: Epoch 133
2025-10-06 02:56:24.137742: Current learning rate: 0.00141
2025-10-06 02:57:11.047073: Validation loss did not improve from -0.41893. Patience: 111/50
2025-10-06 02:57:11.047469: train_loss -0.9099
2025-10-06 02:57:11.047691: val_loss -0.2636
2025-10-06 02:57:11.047836: Pseudo dice [np.float32(0.6716)]
2025-10-06 02:57:11.047985: Epoch time: 46.91 s
2025-10-06 02:57:11.693763: 
2025-10-06 02:57:11.694139: Epoch 134
2025-10-06 02:57:11.694346: Current learning rate: 0.00133
2025-10-06 02:57:58.525956: Validation loss did not improve from -0.41893. Patience: 112/50
2025-10-06 02:57:58.526491: train_loss -0.9117
2025-10-06 02:57:58.526629: val_loss -0.2876
2025-10-06 02:57:58.526784: Pseudo dice [np.float32(0.6799)]
2025-10-06 02:57:58.526942: Epoch time: 46.83 s
2025-10-06 02:57:59.630687: 
2025-10-06 02:57:59.630981: Epoch 135
2025-10-06 02:57:59.631148: Current learning rate: 0.00126
2025-10-06 02:58:46.568262: Validation loss did not improve from -0.41893. Patience: 113/50
2025-10-06 02:58:46.568775: train_loss -0.911
2025-10-06 02:58:46.569034: val_loss -0.2271
2025-10-06 02:58:46.569246: Pseudo dice [np.float32(0.6612)]
2025-10-06 02:58:46.569449: Epoch time: 46.94 s
2025-10-06 02:58:47.218494: 
2025-10-06 02:58:47.218821: Epoch 136
2025-10-06 02:58:47.219023: Current learning rate: 0.00118
2025-10-06 02:59:34.088094: Validation loss did not improve from -0.41893. Patience: 114/50
2025-10-06 02:59:34.088608: train_loss -0.9121
2025-10-06 02:59:34.088814: val_loss -0.273
2025-10-06 02:59:34.088959: Pseudo dice [np.float32(0.6781)]
2025-10-06 02:59:34.089102: Epoch time: 46.87 s
2025-10-06 02:59:34.729414: 
2025-10-06 02:59:34.729789: Epoch 137
2025-10-06 02:59:34.730001: Current learning rate: 0.00111
2025-10-06 03:00:21.683288: Validation loss did not improve from -0.41893. Patience: 115/50
2025-10-06 03:00:21.683811: train_loss -0.9114
2025-10-06 03:00:21.684017: val_loss -0.2463
2025-10-06 03:00:21.684194: Pseudo dice [np.float32(0.6642)]
2025-10-06 03:00:21.684400: Epoch time: 46.96 s
2025-10-06 03:00:22.343766: 
2025-10-06 03:00:22.344004: Epoch 138
2025-10-06 03:00:22.344179: Current learning rate: 0.00103
2025-10-06 03:01:09.224184: Validation loss did not improve from -0.41893. Patience: 116/50
2025-10-06 03:01:09.224661: train_loss -0.9135
2025-10-06 03:01:09.224816: val_loss -0.2387
2025-10-06 03:01:09.224971: Pseudo dice [np.float32(0.6472)]
2025-10-06 03:01:09.225120: Epoch time: 46.88 s
2025-10-06 03:01:09.879522: 
2025-10-06 03:01:09.879876: Epoch 139
2025-10-06 03:01:09.880049: Current learning rate: 0.00095
2025-10-06 03:01:56.724468: Validation loss did not improve from -0.41893. Patience: 117/50
2025-10-06 03:01:56.724874: train_loss -0.9114
2025-10-06 03:01:56.725067: val_loss -0.2062
2025-10-06 03:01:56.725206: Pseudo dice [np.float32(0.6582)]
2025-10-06 03:01:56.725379: Epoch time: 46.85 s
2025-10-06 03:01:57.813989: 
2025-10-06 03:01:57.814275: Epoch 140
2025-10-06 03:01:57.814490: Current learning rate: 0.00087
2025-10-06 03:02:44.696303: Validation loss did not improve from -0.41893. Patience: 118/50
2025-10-06 03:02:44.696932: train_loss -0.9114
2025-10-06 03:02:44.697127: val_loss -0.2631
2025-10-06 03:02:44.697299: Pseudo dice [np.float32(0.6795)]
2025-10-06 03:02:44.697477: Epoch time: 46.88 s
2025-10-06 03:02:45.339617: 
2025-10-06 03:02:45.339976: Epoch 141
2025-10-06 03:02:45.340214: Current learning rate: 0.00079
2025-10-06 03:03:32.159101: Validation loss did not improve from -0.41893. Patience: 119/50
2025-10-06 03:03:32.159470: train_loss -0.9123
2025-10-06 03:03:32.159759: val_loss -0.2839
2025-10-06 03:03:32.160058: Pseudo dice [np.float32(0.6792)]
2025-10-06 03:03:32.160504: Epoch time: 46.82 s
2025-10-06 03:03:33.156426: 
2025-10-06 03:03:33.156833: Epoch 142
2025-10-06 03:03:33.157031: Current learning rate: 0.00071
2025-10-06 03:04:20.079196: Validation loss did not improve from -0.41893. Patience: 120/50
2025-10-06 03:04:20.079828: train_loss -0.9119
2025-10-06 03:04:20.080073: val_loss -0.281
2025-10-06 03:04:20.080322: Pseudo dice [np.float32(0.6906)]
2025-10-06 03:04:20.080609: Epoch time: 46.92 s
2025-10-06 03:04:20.740093: 
2025-10-06 03:04:20.740499: Epoch 143
2025-10-06 03:04:20.740712: Current learning rate: 0.00063
2025-10-06 03:05:07.626433: Validation loss did not improve from -0.41893. Patience: 121/50
2025-10-06 03:05:07.626886: train_loss -0.9126
2025-10-06 03:05:07.627071: val_loss -0.2774
2025-10-06 03:05:07.627232: Pseudo dice [np.float32(0.6721)]
2025-10-06 03:05:07.627409: Epoch time: 46.89 s
2025-10-06 03:05:08.276135: 
2025-10-06 03:05:08.276554: Epoch 144
2025-10-06 03:05:08.276767: Current learning rate: 0.00055
2025-10-06 03:05:55.171183: Validation loss did not improve from -0.41893. Patience: 122/50
2025-10-06 03:05:55.171619: train_loss -0.914
2025-10-06 03:05:55.171798: val_loss -0.2361
2025-10-06 03:05:55.171999: Pseudo dice [np.float32(0.6678)]
2025-10-06 03:05:55.172186: Epoch time: 46.9 s
2025-10-06 03:05:56.281663: 
2025-10-06 03:05:56.282254: Epoch 145
2025-10-06 03:05:56.282527: Current learning rate: 0.00047
2025-10-06 03:06:43.134014: Validation loss did not improve from -0.41893. Patience: 123/50
2025-10-06 03:06:43.134540: train_loss -0.9125
2025-10-06 03:06:43.134763: val_loss -0.2779
2025-10-06 03:06:43.134977: Pseudo dice [np.float32(0.6799)]
2025-10-06 03:06:43.135229: Epoch time: 46.85 s
2025-10-06 03:06:43.784409: 
2025-10-06 03:06:43.784750: Epoch 146
2025-10-06 03:06:43.785030: Current learning rate: 0.00038
2025-10-06 03:07:30.696445: Validation loss did not improve from -0.41893. Patience: 124/50
2025-10-06 03:07:30.696980: train_loss -0.9135
2025-10-06 03:07:30.697115: val_loss -0.258
2025-10-06 03:07:30.697243: Pseudo dice [np.float32(0.6637)]
2025-10-06 03:07:30.697374: Epoch time: 46.91 s
2025-10-06 03:07:31.353271: 
2025-10-06 03:07:31.353544: Epoch 147
2025-10-06 03:07:31.353770: Current learning rate: 0.0003
2025-10-06 03:08:18.296729: Validation loss did not improve from -0.41893. Patience: 125/50
2025-10-06 03:08:18.297077: train_loss -0.9139
2025-10-06 03:08:18.297225: val_loss -0.2391
2025-10-06 03:08:18.297401: Pseudo dice [np.float32(0.6684)]
2025-10-06 03:08:18.297597: Epoch time: 46.94 s
2025-10-06 03:08:18.948756: 
2025-10-06 03:08:18.949061: Epoch 148
2025-10-06 03:08:18.949307: Current learning rate: 0.00021
2025-10-06 03:09:05.880213: Validation loss did not improve from -0.41893. Patience: 126/50
2025-10-06 03:09:05.880727: train_loss -0.9137
2025-10-06 03:09:05.880902: val_loss -0.2484
2025-10-06 03:09:05.881059: Pseudo dice [np.float32(0.6728)]
2025-10-06 03:09:05.881185: Epoch time: 46.93 s
2025-10-06 03:09:06.534109: 
2025-10-06 03:09:06.534446: Epoch 149
2025-10-06 03:09:06.534612: Current learning rate: 0.00011
2025-10-06 03:09:53.452039: Validation loss did not improve from -0.41893. Patience: 127/50
2025-10-06 03:09:53.452506: train_loss -0.9132
2025-10-06 03:09:53.452675: val_loss -0.2287
2025-10-06 03:09:53.452888: Pseudo dice [np.float32(0.6521)]
2025-10-06 03:09:53.453042: Epoch time: 46.92 s
2025-10-06 03:09:54.592589: Training done.
2025-10-06 03:09:54.616566: Using splits from existing split file: /nfs/erelab001/shared/Computational_Group/Naravich/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset310_nnInteractive_Calcium_OCT_CrossValidation/splits_final_20.json
2025-10-06 03:09:54.616990: The split file contains 5 splits.
2025-10-06 03:09:54.617440: Desired fold for training: 4
2025-10-06 03:09:54.620178: This split has 1 training and 7 validation cases.
2025-10-06 03:09:54.620577: predicting 101-019
2025-10-06 03:09:54.623445: 101-019, shape torch.Size([1, 375, 498, 498]), rank 0
2025-10-06 03:10:32.555124: predicting 101-044
2025-10-06 03:10:32.567852: 101-044, shape torch.Size([1, 404, 498, 498]), rank 0
2025-10-06 03:11:10.883148: predicting 101-045
2025-10-06 03:11:10.895827: 101-045, shape torch.Size([1, 375, 498, 498]), rank 0
2025-10-06 03:11:45.480561: predicting 401-004
2025-10-06 03:11:45.493868: 401-004, shape torch.Size([1, 375, 498, 498]), rank 0
2025-10-06 03:12:20.137353: predicting 701-013
2025-10-06 03:12:20.156342: 701-013, shape torch.Size([1, 375, 498, 498]), rank 0
2025-10-06 03:12:54.729872: predicting 704-003
2025-10-06 03:12:54.742872: 704-003, shape torch.Size([1, 375, 498, 498]), rank 0
2025-10-06 03:13:29.265030: predicting 706-005
2025-10-06 03:13:29.277997: 706-005, shape torch.Size([1, 375, 498, 498]), rank 0
2025-10-06 03:14:17.511304: Validation complete
2025-10-06 03:14:17.511618: Mean Validation Dice:  0.6402914546182553
Finished training fold 4 saving to /nfs/erelab001/shared/Computational_Group/Naravich/datasets/nnUNet_Datasets/nnUNet_results/Dataset310_nnInteractive_Calcium_OCT_CrossValidation/nnUNetTrainerScaleAnalysis20__nnUNetPlans__3d_32x160x128_b10/fold_4_No_Pretrained
