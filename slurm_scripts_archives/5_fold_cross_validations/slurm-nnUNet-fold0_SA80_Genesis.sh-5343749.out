/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/bin/python
Starting training with CONFIG=3d_32x160x128_b10, DATASET_ID=307, TRAINER=nnUNetTrainerScaleAnalysis80
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/nnunetv2/training/nnUNetTrainer/nnUNetTrainer.py:169: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  self.grad_scaler = GradScaler() if self.device.type == 'cuda' else None

############################
INFO: You are using the old nnU-Net default plans. We have updated our recommendations. Please consider using those instead! Read more here: https://github.com/MIC-DKFZ/nnUNet/blob/master/documentation/resenc_presets.md
############################

Using device: cuda:0

#######################################################################
Please cite the following paper when using nnU-Net:
Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.
#######################################################################

2025-10-13 19:27:28.015390: Using torch.compile...
################### Loading pretrained weights from file  /nfs/erelab001/shared/Computational_Group/Naravich/datasets/nnUNet_Datasets/../ModelGenesisOutputs/ModelGenesisNNUNetPretrainingV2_noNorm_correct_orientation/Converted_nnUNet_Genesis_OCT_Best.pt ###################
Below is the list of overlapping blocks in pretrained model and nnUNet architecture:
encoder.stages.0.0.convs.0.conv.weight shape torch.Size([32, 1, 3, 3, 3])
encoder.stages.0.0.convs.0.conv.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.norm.weight shape torch.Size([32])
encoder.stages.0.0.convs.0.norm.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.0.weight shape torch.Size([32, 1, 3, 3, 3])
encoder.stages.0.0.convs.0.all_modules.0.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.1.weight shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.1.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
encoder.stages.0.0.convs.1.conv.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.norm.weight shape torch.Size([32])
encoder.stages.0.0.convs.1.norm.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
encoder.stages.0.0.convs.1.all_modules.0.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.1.weight shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.1.bias shape torch.Size([32])
encoder.stages.1.0.convs.0.conv.weight shape torch.Size([64, 32, 3, 3, 3])
encoder.stages.1.0.convs.0.conv.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.norm.weight shape torch.Size([64])
encoder.stages.1.0.convs.0.norm.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.0.weight shape torch.Size([64, 32, 3, 3, 3])
encoder.stages.1.0.convs.0.all_modules.0.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.1.weight shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.1.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
encoder.stages.1.0.convs.1.conv.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.norm.weight shape torch.Size([64])
encoder.stages.1.0.convs.1.norm.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
encoder.stages.1.0.convs.1.all_modules.0.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.1.weight shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.1.bias shape torch.Size([64])
encoder.stages.2.0.convs.0.conv.weight shape torch.Size([128, 64, 3, 3, 3])
encoder.stages.2.0.convs.0.conv.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.norm.weight shape torch.Size([128])
encoder.stages.2.0.convs.0.norm.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.0.weight shape torch.Size([128, 64, 3, 3, 3])
encoder.stages.2.0.convs.0.all_modules.0.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.1.weight shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.1.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
encoder.stages.2.0.convs.1.conv.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.norm.weight shape torch.Size([128])
encoder.stages.2.0.convs.1.norm.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
encoder.stages.2.0.convs.1.all_modules.0.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.1.weight shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.1.bias shape torch.Size([128])
encoder.stages.3.0.convs.0.conv.weight shape torch.Size([256, 128, 3, 3, 3])
encoder.stages.3.0.convs.0.conv.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.norm.weight shape torch.Size([256])
encoder.stages.3.0.convs.0.norm.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.0.weight shape torch.Size([256, 128, 3, 3, 3])
encoder.stages.3.0.convs.0.all_modules.0.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.1.weight shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.1.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
encoder.stages.3.0.convs.1.conv.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.norm.weight shape torch.Size([256])
encoder.stages.3.0.convs.1.norm.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
encoder.stages.3.0.convs.1.all_modules.0.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.1.weight shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.1.bias shape torch.Size([256])
encoder.stages.4.0.convs.0.conv.weight shape torch.Size([320, 256, 3, 3, 3])
encoder.stages.4.0.convs.0.conv.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.norm.weight shape torch.Size([320])
encoder.stages.4.0.convs.0.norm.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.0.weight shape torch.Size([320, 256, 3, 3, 3])
encoder.stages.4.0.convs.0.all_modules.0.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.1.weight shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.1.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.4.0.convs.1.conv.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.norm.weight shape torch.Size([320])
encoder.stages.4.0.convs.1.norm.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.4.0.convs.1.all_modules.0.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.1.weight shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.1.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.0.conv.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.norm.weight shape torch.Size([320])
encoder.stages.5.0.convs.0.norm.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.0.all_modules.0.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.1.weight shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.1.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.1.conv.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.norm.weight shape torch.Size([320])
encoder.stages.5.0.convs.1.norm.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.1.all_modules.0.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.1.weight shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.0.0.convs.0.conv.weight shape torch.Size([32, 1, 3, 3, 3])
decoder.encoder.stages.0.0.convs.0.conv.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.norm.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.norm.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.0.weight shape torch.Size([32, 1, 3, 3, 3])
decoder.encoder.stages.0.0.convs.0.all_modules.0.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.1.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.1.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.encoder.stages.0.0.convs.1.conv.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.norm.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.norm.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.encoder.stages.0.0.convs.1.all_modules.0.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.1.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.1.bias shape torch.Size([32])
decoder.encoder.stages.1.0.convs.0.conv.weight shape torch.Size([64, 32, 3, 3, 3])
decoder.encoder.stages.1.0.convs.0.conv.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.norm.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.norm.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.0.weight shape torch.Size([64, 32, 3, 3, 3])
decoder.encoder.stages.1.0.convs.0.all_modules.0.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.1.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.1.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.encoder.stages.1.0.convs.1.conv.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.norm.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.norm.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.encoder.stages.1.0.convs.1.all_modules.0.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.1.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.1.bias shape torch.Size([64])
decoder.encoder.stages.2.0.convs.0.conv.weight shape torch.Size([128, 64, 3, 3, 3])
decoder.encoder.stages.2.0.convs.0.conv.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.norm.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.norm.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.0.weight shape torch.Size([128, 64, 3, 3, 3])
decoder.encoder.stages.2.0.convs.0.all_modules.0.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.1.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.1.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.encoder.stages.2.0.convs.1.conv.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.norm.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.norm.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.encoder.stages.2.0.convs.1.all_modules.0.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.1.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.1.bias shape torch.Size([128])
decoder.encoder.stages.3.0.convs.0.conv.weight shape torch.Size([256, 128, 3, 3, 3])
decoder.encoder.stages.3.0.convs.0.conv.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.norm.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.norm.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.0.weight shape torch.Size([256, 128, 3, 3, 3])
decoder.encoder.stages.3.0.convs.0.all_modules.0.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.1.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.1.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.encoder.stages.3.0.convs.1.conv.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.norm.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.norm.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.encoder.stages.3.0.convs.1.all_modules.0.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.1.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.1.bias shape torch.Size([256])
decoder.encoder.stages.4.0.convs.0.conv.weight shape torch.Size([320, 256, 3, 3, 3])
decoder.encoder.stages.4.0.convs.0.conv.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.norm.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.norm.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.0.weight shape torch.Size([320, 256, 3, 3, 3])
decoder.encoder.stages.4.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.4.0.convs.1.conv.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.norm.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.norm.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.4.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.0.conv.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.norm.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.norm.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.1.conv.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.norm.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.norm.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.stages.0.convs.0.conv.weight shape torch.Size([320, 640, 3, 3, 3])
decoder.stages.0.convs.0.conv.bias shape torch.Size([320])
decoder.stages.0.convs.0.norm.weight shape torch.Size([320])
decoder.stages.0.convs.0.norm.bias shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.0.weight shape torch.Size([320, 640, 3, 3, 3])
decoder.stages.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.stages.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.stages.0.convs.1.conv.bias shape torch.Size([320])
decoder.stages.0.convs.1.norm.weight shape torch.Size([320])
decoder.stages.0.convs.1.norm.bias shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.stages.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.stages.1.convs.0.conv.weight shape torch.Size([256, 512, 3, 3, 3])
decoder.stages.1.convs.0.conv.bias shape torch.Size([256])
decoder.stages.1.convs.0.norm.weight shape torch.Size([256])
decoder.stages.1.convs.0.norm.bias shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.0.weight shape torch.Size([256, 512, 3, 3, 3])
decoder.stages.1.convs.0.all_modules.0.bias shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.1.weight shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.1.bias shape torch.Size([256])
decoder.stages.1.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.stages.1.convs.1.conv.bias shape torch.Size([256])
decoder.stages.1.convs.1.norm.weight shape torch.Size([256])
decoder.stages.1.convs.1.norm.bias shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.stages.1.convs.1.all_modules.0.bias shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.1.weight shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.1.bias shape torch.Size([256])
decoder.stages.2.convs.0.conv.weight shape torch.Size([128, 256, 3, 3, 3])
decoder.stages.2.convs.0.conv.bias shape torch.Size([128])
decoder.stages.2.convs.0.norm.weight shape torch.Size([128])
decoder.stages.2.convs.0.norm.bias shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.0.weight shape torch.Size([128, 256, 3, 3, 3])
decoder.stages.2.convs.0.all_modules.0.bias shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.1.weight shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.1.bias shape torch.Size([128])
decoder.stages.2.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.stages.2.convs.1.conv.bias shape torch.Size([128])
decoder.stages.2.convs.1.norm.weight shape torch.Size([128])
decoder.stages.2.convs.1.norm.bias shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.stages.2.convs.1.all_modules.0.bias shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.1.weight shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.1.bias shape torch.Size([128])
decoder.stages.3.convs.0.conv.weight shape torch.Size([64, 128, 3, 3, 3])
decoder.stages.3.convs.0.conv.bias shape torch.Size([64])
decoder.stages.3.convs.0.norm.weight shape torch.Size([64])
decoder.stages.3.convs.0.norm.bias shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.0.weight shape torch.Size([64, 128, 3, 3, 3])
decoder.stages.3.convs.0.all_modules.0.bias shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.1.weight shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.1.bias shape torch.Size([64])
decoder.stages.3.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.stages.3.convs.1.conv.bias shape torch.Size([64])
decoder.stages.3.convs.1.norm.weight shape torch.Size([64])
decoder.stages.3.convs.1.norm.bias shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.stages.3.convs.1.all_modules.0.bias shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.1.weight shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.1.bias shape torch.Size([64])
decoder.stages.4.convs.0.conv.weight shape torch.Size([32, 64, 3, 3, 3])
decoder.stages.4.convs.0.conv.bias shape torch.Size([32])
decoder.stages.4.convs.0.norm.weight shape torch.Size([32])
decoder.stages.4.convs.0.norm.bias shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.0.weight shape torch.Size([32, 64, 3, 3, 3])
decoder.stages.4.convs.0.all_modules.0.bias shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.1.weight shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.1.bias shape torch.Size([32])
decoder.stages.4.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.stages.4.convs.1.conv.bias shape torch.Size([32])
decoder.stages.4.convs.1.norm.weight shape torch.Size([32])
decoder.stages.4.convs.1.norm.bias shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.stages.4.convs.1.all_modules.0.bias shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.1.weight shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.1.bias shape torch.Size([32])
decoder.transpconvs.0.weight shape torch.Size([320, 320, 1, 2, 2])
decoder.transpconvs.0.bias shape torch.Size([320])
decoder.transpconvs.1.weight shape torch.Size([320, 256, 2, 2, 2])
decoder.transpconvs.1.bias shape torch.Size([256])
decoder.transpconvs.2.weight shape torch.Size([256, 128, 2, 2, 2])
decoder.transpconvs.2.bias shape torch.Size([128])
decoder.transpconvs.3.weight shape torch.Size([128, 64, 2, 2, 2])
decoder.transpconvs.3.bias shape torch.Size([64])
decoder.transpconvs.4.weight shape torch.Size([64, 32, 2, 2, 2])
decoder.transpconvs.4.bias shape torch.Size([32])
################### Done ###################
2025-10-13 19:27:29.286981: do_dummy_2d_data_aug: True
2025-10-13 19:27:29.287492: Using splits from existing split file: /nfs/erelab001/shared/Computational_Group/Naravich/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset307_Sohee_Calcium_OCT_CrossValidation/splits_final_80.json
2025-10-13 19:27:29.288034: The split file contains 5 splits.
2025-10-13 19:27:29.288162: Desired fold for training: 0
2025-10-13 19:27:29.288262: This split has 6 training and 4 validation cases.
using pin_memory on device 0
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/fft_conv_pytorch/fft_conv.py:139: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:306.)
  output = output[crop_slices].contiguous()
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/fft_conv_pytorch/fft_conv.py:139: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:306.)
  output = output[crop_slices].contiguous()
using pin_memory on device 0

This is the configuration used by this training:
Configuration name: 3d_32x160x128_b10
 {'data_identifier': 'nnUNetPlans_3d_fullres', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 10, 'patch_size': [32, 160, 128], 'median_image_size_in_voxels': [375.0, 498.0, 498.0], 'spacing': [1.0, 1.0, 1.0], 'normalization_schemes': ['ZScoreNormalization'], 'use_mask_for_norm': [False], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'architecture': {'network_class_name': 'dynamic_network_architectures.architectures.unet.PlainConvUNet', 'arch_kwargs': {'n_stages': 6, 'features_per_stage': [32, 64, 128, 256, 320, 320], 'conv_op': 'torch.nn.modules.conv.Conv3d', 'kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]], 'strides': [[1, 1, 1], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [1, 2, 2]], 'n_conv_per_stage': [2, 2, 2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2, 2, 2], 'conv_bias': True, 'norm_op': 'torch.nn.modules.instancenorm.InstanceNorm3d', 'norm_op_kwargs': {'eps': 1e-05, 'affine': True}, 'dropout_op': None, 'dropout_op_kwargs': None, 'nonlin': 'torch.nn.LeakyReLU', 'nonlin_kwargs': {'inplace': True}, 'deep_supervision': True}, '_kw_requires_import': ['conv_op', 'norm_op', 'dropout_op', 'nonlin']}, 'batch_dice': True, 'inherits_from': '3d_fullres'} 

These are the global plan.json settings:
 {'dataset_name': 'Dataset307_Sohee_Calcium_OCT_CrossValidation', 'plans_name': 'nnUNetPlans', 'original_median_spacing_after_transp': [1.0, 1.0, 1.0], 'original_median_shape_after_transp': [375, 498, 498], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 0.4977976381778717, 'mean': 0.13507525622844696, 'median': 0.09599608182907104, 'min': 0.0, 'percentile_00_5': 0.014754901640117168, 'percentile_99_5': 0.4977976381778717, 'std': 0.12025152146816254}}} 

2025-10-13 19:27:31.457716: unpacking dataset...
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/fft_conv_pytorch/fft_conv.py:139: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:306.)
  output = output[crop_slices].contiguous()
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/fft_conv_pytorch/fft_conv.py:139: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:306.)
  output = output[crop_slices].contiguous()
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/fft_conv_pytorch/fft_conv.py:139: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:306.)
  output = output[crop_slices].contiguous()
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/fft_conv_pytorch/fft_conv.py:139: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:306.)
  output = output[crop_slices].contiguous()
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/fft_conv_pytorch/fft_conv.py:139: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:306.)
  output = output[crop_slices].contiguous()
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/fft_conv_pytorch/fft_conv.py:139: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:306.)
  output = output[crop_slices].contiguous()
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/fft_conv_pytorch/fft_conv.py:139: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:306.)
  output = output[crop_slices].contiguous()
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/fft_conv_pytorch/fft_conv.py:139: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:306.)
  output = output[crop_slices].contiguous()
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/fft_conv_pytorch/fft_conv.py:139: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:306.)
  output = output[crop_slices].contiguous()
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/fft_conv_pytorch/fft_conv.py:139: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:306.)
  output = output[crop_slices].contiguous()
2025-10-13 19:27:37.748093: unpacking done...
2025-10-13 19:27:37.750436: Unable to plot network architecture: nnUNet_compile is enabled!
2025-10-13 19:27:37.755284: 
2025-10-13 19:27:37.755530: Epoch 0
2025-10-13 19:27:37.755707: Current learning rate: 0.01
2025-10-13 19:28:29.758297: Validation loss improved from 1000.00000 to -0.26932! Patience: 0/50
2025-10-13 19:28:29.758960: train_loss -0.1493
2025-10-13 19:28:29.759204: val_loss -0.2693
2025-10-13 19:28:29.759371: Pseudo dice [np.float32(0.5906)]
2025-10-13 19:28:29.759542: Epoch time: 52.0 s
2025-10-13 19:28:29.759687: Yayy! New best EMA pseudo Dice: 0.5906000137329102
2025-10-13 19:28:30.643527: 
2025-10-13 19:28:30.643866: Epoch 1
2025-10-13 19:28:30.644033: Current learning rate: 0.00994
2025-10-13 19:29:16.734629: Validation loss improved from -0.26932 to -0.32625! Patience: 0/50
2025-10-13 19:29:16.735195: train_loss -0.3239
2025-10-13 19:29:16.735356: val_loss -0.3263
2025-10-13 19:29:16.735485: Pseudo dice [np.float32(0.6032)]
2025-10-13 19:29:16.735680: Epoch time: 46.09 s
2025-10-13 19:29:16.735821: Yayy! New best EMA pseudo Dice: 0.5918999910354614
2025-10-13 19:29:17.766045: 
2025-10-13 19:29:17.766304: Epoch 2
2025-10-13 19:29:17.766463: Current learning rate: 0.00988
2025-10-13 19:30:03.978341: Validation loss improved from -0.32625 to -0.35839! Patience: 0/50
2025-10-13 19:30:03.978965: train_loss -0.4055
2025-10-13 19:30:03.979250: val_loss -0.3584
2025-10-13 19:30:03.979444: Pseudo dice [np.float32(0.6467)]
2025-10-13 19:30:03.979632: Epoch time: 46.21 s
2025-10-13 19:30:03.979810: Yayy! New best EMA pseudo Dice: 0.5972999930381775
2025-10-13 19:30:05.034583: 
2025-10-13 19:30:05.034906: Epoch 3
2025-10-13 19:30:05.035115: Current learning rate: 0.00982
2025-10-13 19:30:51.225344: Validation loss improved from -0.35839 to -0.38784! Patience: 0/50
2025-10-13 19:30:51.225891: train_loss -0.4398
2025-10-13 19:30:51.226063: val_loss -0.3878
2025-10-13 19:30:51.226245: Pseudo dice [np.float32(0.6624)]
2025-10-13 19:30:51.226423: Epoch time: 46.19 s
2025-10-13 19:30:51.226568: Yayy! New best EMA pseudo Dice: 0.6039000153541565
2025-10-13 19:30:52.265866: 
2025-10-13 19:30:52.266155: Epoch 4
2025-10-13 19:30:52.266318: Current learning rate: 0.00976
2025-10-13 19:31:38.412804: Validation loss improved from -0.38784 to -0.40507! Patience: 0/50
2025-10-13 19:31:38.413290: train_loss -0.4694
2025-10-13 19:31:38.413429: val_loss -0.4051
2025-10-13 19:31:38.413565: Pseudo dice [np.float32(0.6679)]
2025-10-13 19:31:38.413703: Epoch time: 46.15 s
2025-10-13 19:31:38.786857: Yayy! New best EMA pseudo Dice: 0.6103000044822693
2025-10-13 19:31:39.827550: 
2025-10-13 19:31:39.827951: Epoch 5
2025-10-13 19:31:39.828334: Current learning rate: 0.0097
2025-10-13 19:32:25.988182: Validation loss improved from -0.40507 to -0.42260! Patience: 0/50
2025-10-13 19:32:25.988709: train_loss -0.4804
2025-10-13 19:32:25.988854: val_loss -0.4226
2025-10-13 19:32:25.988978: Pseudo dice [np.float32(0.676)]
2025-10-13 19:32:25.989135: Epoch time: 46.16 s
2025-10-13 19:32:25.989321: Yayy! New best EMA pseudo Dice: 0.6168000102043152
2025-10-13 19:32:27.005855: 
2025-10-13 19:32:27.006209: Epoch 6
2025-10-13 19:32:27.006426: Current learning rate: 0.00964
2025-10-13 19:33:13.216544: Validation loss did not improve from -0.42260. Patience: 1/50
2025-10-13 19:33:13.217170: train_loss -0.5063
2025-10-13 19:33:13.217409: val_loss -0.3718
2025-10-13 19:33:13.217681: Pseudo dice [np.float32(0.637)]
2025-10-13 19:33:13.217928: Epoch time: 46.21 s
2025-10-13 19:33:13.218154: Yayy! New best EMA pseudo Dice: 0.6187999844551086
2025-10-13 19:33:14.262321: 
2025-10-13 19:33:14.262603: Epoch 7
2025-10-13 19:33:14.262796: Current learning rate: 0.00958
2025-10-13 19:34:00.429833: Validation loss improved from -0.42260 to -0.44829! Patience: 1/50
2025-10-13 19:34:00.430416: train_loss -0.5132
2025-10-13 19:34:00.430796: val_loss -0.4483
2025-10-13 19:34:00.431150: Pseudo dice [np.float32(0.6957)]
2025-10-13 19:34:00.431498: Epoch time: 46.17 s
2025-10-13 19:34:00.431817: Yayy! New best EMA pseudo Dice: 0.6265000104904175
2025-10-13 19:34:01.473277: 
2025-10-13 19:34:01.473686: Epoch 8
2025-10-13 19:34:01.474125: Current learning rate: 0.00952
2025-10-13 19:34:47.701300: Validation loss did not improve from -0.44829. Patience: 1/50
2025-10-13 19:34:47.701769: train_loss -0.5242
2025-10-13 19:34:47.701912: val_loss -0.4268
2025-10-13 19:34:47.702062: Pseudo dice [np.float32(0.6788)]
2025-10-13 19:34:47.702243: Epoch time: 46.23 s
2025-10-13 19:34:47.702396: Yayy! New best EMA pseudo Dice: 0.6317999958992004
2025-10-13 19:34:48.753141: 
2025-10-13 19:34:48.753356: Epoch 9
2025-10-13 19:34:48.753531: Current learning rate: 0.00946
2025-10-13 19:35:34.954319: Validation loss did not improve from -0.44829. Patience: 2/50
2025-10-13 19:35:34.954663: train_loss -0.5525
2025-10-13 19:35:34.954796: val_loss -0.4371
2025-10-13 19:35:34.954905: Pseudo dice [np.float32(0.6959)]
2025-10-13 19:35:34.955090: Epoch time: 46.2 s
2025-10-13 19:35:35.378927: Yayy! New best EMA pseudo Dice: 0.6381999850273132
2025-10-13 19:35:36.405305: 
2025-10-13 19:35:36.405557: Epoch 10
2025-10-13 19:35:36.405784: Current learning rate: 0.0094
2025-10-13 19:36:22.584181: Validation loss improved from -0.44829 to -0.45495! Patience: 2/50
2025-10-13 19:36:22.584647: train_loss -0.5558
2025-10-13 19:36:22.584780: val_loss -0.455
2025-10-13 19:36:22.584899: Pseudo dice [np.float32(0.7057)]
2025-10-13 19:36:22.585033: Epoch time: 46.18 s
2025-10-13 19:36:22.585145: Yayy! New best EMA pseudo Dice: 0.6449000239372253
2025-10-13 19:36:23.629798: 
2025-10-13 19:36:23.630070: Epoch 11
2025-10-13 19:36:23.630228: Current learning rate: 0.00934
2025-10-13 19:37:09.790181: Validation loss improved from -0.45495 to -0.45706! Patience: 0/50
2025-10-13 19:37:09.790650: train_loss -0.5616
2025-10-13 19:37:09.790803: val_loss -0.4571
2025-10-13 19:37:09.791004: Pseudo dice [np.float32(0.693)]
2025-10-13 19:37:09.791208: Epoch time: 46.16 s
2025-10-13 19:37:09.791391: Yayy! New best EMA pseudo Dice: 0.6496999859809875
2025-10-13 19:37:11.083612: 
2025-10-13 19:37:11.084041: Epoch 12
2025-10-13 19:37:11.084244: Current learning rate: 0.00928
2025-10-13 19:37:57.301066: Validation loss improved from -0.45706 to -0.46363! Patience: 0/50
2025-10-13 19:37:57.301675: train_loss -0.5654
2025-10-13 19:37:57.301868: val_loss -0.4636
2025-10-13 19:37:57.302038: Pseudo dice [np.float32(0.7088)]
2025-10-13 19:37:57.302203: Epoch time: 46.22 s
2025-10-13 19:37:57.302400: Yayy! New best EMA pseudo Dice: 0.6556000113487244
2025-10-13 19:37:58.341829: 
2025-10-13 19:37:58.342141: Epoch 13
2025-10-13 19:37:58.342336: Current learning rate: 0.00922
2025-10-13 19:38:44.608287: Validation loss did not improve from -0.46363. Patience: 1/50
2025-10-13 19:38:44.608972: train_loss -0.5886
2025-10-13 19:38:44.609308: val_loss -0.4359
2025-10-13 19:38:44.609582: Pseudo dice [np.float32(0.6813)]
2025-10-13 19:38:44.609885: Epoch time: 46.27 s
2025-10-13 19:38:44.610305: Yayy! New best EMA pseudo Dice: 0.6582000255584717
2025-10-13 19:38:45.643284: 
2025-10-13 19:38:45.643579: Epoch 14
2025-10-13 19:38:45.643800: Current learning rate: 0.00916
2025-10-13 19:39:31.859150: Validation loss did not improve from -0.46363. Patience: 2/50
2025-10-13 19:39:31.859645: train_loss -0.5997
2025-10-13 19:39:31.859796: val_loss -0.4514
2025-10-13 19:39:31.859947: Pseudo dice [np.float32(0.6974)]
2025-10-13 19:39:31.860089: Epoch time: 46.22 s
2025-10-13 19:39:32.268072: Yayy! New best EMA pseudo Dice: 0.6621000170707703
2025-10-13 19:39:33.270594: 
2025-10-13 19:39:33.270857: Epoch 15
2025-10-13 19:39:33.271038: Current learning rate: 0.0091
2025-10-13 19:40:19.474634: Validation loss improved from -0.46363 to -0.48176! Patience: 2/50
2025-10-13 19:40:19.475141: train_loss -0.6031
2025-10-13 19:40:19.475366: val_loss -0.4818
2025-10-13 19:40:19.475605: Pseudo dice [np.float32(0.7065)]
2025-10-13 19:40:19.475854: Epoch time: 46.21 s
2025-10-13 19:40:19.476075: Yayy! New best EMA pseudo Dice: 0.6665999889373779
2025-10-13 19:40:20.496289: 
2025-10-13 19:40:20.496525: Epoch 16
2025-10-13 19:40:20.496695: Current learning rate: 0.00903
2025-10-13 19:41:06.743549: Validation loss improved from -0.48176 to -0.49380! Patience: 0/50
2025-10-13 19:41:06.744303: train_loss -0.6093
2025-10-13 19:41:06.744547: val_loss -0.4938
2025-10-13 19:41:06.744786: Pseudo dice [np.float32(0.7212)]
2025-10-13 19:41:06.745027: Epoch time: 46.25 s
2025-10-13 19:41:06.745239: Yayy! New best EMA pseudo Dice: 0.671999990940094
2025-10-13 19:41:07.773108: 
2025-10-13 19:41:07.773442: Epoch 17
2025-10-13 19:41:07.773600: Current learning rate: 0.00897
2025-10-13 19:41:54.023917: Validation loss did not improve from -0.49380. Patience: 1/50
2025-10-13 19:41:54.024362: train_loss -0.6195
2025-10-13 19:41:54.024509: val_loss -0.4626
2025-10-13 19:41:54.024707: Pseudo dice [np.float32(0.6926)]
2025-10-13 19:41:54.024926: Epoch time: 46.25 s
2025-10-13 19:41:54.025059: Yayy! New best EMA pseudo Dice: 0.6740999817848206
2025-10-13 19:41:55.072439: 
2025-10-13 19:41:55.072701: Epoch 18
2025-10-13 19:41:55.072882: Current learning rate: 0.00891
2025-10-13 19:42:41.300389: Validation loss improved from -0.49380 to -0.51329! Patience: 1/50
2025-10-13 19:42:41.300894: train_loss -0.6194
2025-10-13 19:42:41.301054: val_loss -0.5133
2025-10-13 19:42:41.301192: Pseudo dice [np.float32(0.7325)]
2025-10-13 19:42:41.301337: Epoch time: 46.23 s
2025-10-13 19:42:41.301510: Yayy! New best EMA pseudo Dice: 0.6798999905586243
2025-10-13 19:42:42.341794: 
2025-10-13 19:42:42.342544: Epoch 19
2025-10-13 19:42:42.343138: Current learning rate: 0.00885
2025-10-13 19:43:28.537317: Validation loss did not improve from -0.51329. Patience: 1/50
2025-10-13 19:43:28.537854: train_loss -0.6217
2025-10-13 19:43:28.538009: val_loss -0.5102
2025-10-13 19:43:28.538209: Pseudo dice [np.float32(0.7304)]
2025-10-13 19:43:28.538438: Epoch time: 46.2 s
2025-10-13 19:43:28.953807: Yayy! New best EMA pseudo Dice: 0.6850000023841858
2025-10-13 19:43:29.986862: 
2025-10-13 19:43:29.987360: Epoch 20
2025-10-13 19:43:29.987576: Current learning rate: 0.00879
2025-10-13 19:44:16.221721: Validation loss did not improve from -0.51329. Patience: 2/50
2025-10-13 19:44:16.222218: train_loss -0.6142
2025-10-13 19:44:16.222373: val_loss -0.4766
2025-10-13 19:44:16.222571: Pseudo dice [np.float32(0.7043)]
2025-10-13 19:44:16.222709: Epoch time: 46.24 s
2025-10-13 19:44:16.222819: Yayy! New best EMA pseudo Dice: 0.6869000196456909
2025-10-13 19:44:17.276984: 
2025-10-13 19:44:17.277254: Epoch 21
2025-10-13 19:44:17.277415: Current learning rate: 0.00873
2025-10-13 19:45:03.524699: Validation loss did not improve from -0.51329. Patience: 3/50
2025-10-13 19:45:03.525182: train_loss -0.6271
2025-10-13 19:45:03.525329: val_loss -0.4953
2025-10-13 19:45:03.525483: Pseudo dice [np.float32(0.7252)]
2025-10-13 19:45:03.525624: Epoch time: 46.25 s
2025-10-13 19:45:03.525741: Yayy! New best EMA pseudo Dice: 0.6906999945640564
2025-10-13 19:45:04.559542: 
2025-10-13 19:45:04.559812: Epoch 22
2025-10-13 19:45:04.560002: Current learning rate: 0.00867
2025-10-13 19:45:50.839644: Validation loss did not improve from -0.51329. Patience: 4/50
2025-10-13 19:45:50.840218: train_loss -0.6356
2025-10-13 19:45:50.840427: val_loss -0.4867
2025-10-13 19:45:50.840687: Pseudo dice [np.float32(0.7129)]
2025-10-13 19:45:50.840900: Epoch time: 46.28 s
2025-10-13 19:45:50.841102: Yayy! New best EMA pseudo Dice: 0.6930000185966492
2025-10-13 19:45:52.144064: 
2025-10-13 19:45:52.144448: Epoch 23
2025-10-13 19:45:52.144678: Current learning rate: 0.00861
2025-10-13 19:46:38.402303: Validation loss did not improve from -0.51329. Patience: 5/50
2025-10-13 19:46:38.402777: train_loss -0.6434
2025-10-13 19:46:38.402936: val_loss -0.4974
2025-10-13 19:46:38.403090: Pseudo dice [np.float32(0.7203)]
2025-10-13 19:46:38.403228: Epoch time: 46.26 s
2025-10-13 19:46:38.403368: Yayy! New best EMA pseudo Dice: 0.6956999897956848
2025-10-13 19:46:39.451262: 
2025-10-13 19:46:39.451708: Epoch 24
2025-10-13 19:46:39.451941: Current learning rate: 0.00855
2025-10-13 19:47:25.754960: Validation loss did not improve from -0.51329. Patience: 6/50
2025-10-13 19:47:25.756084: train_loss -0.6455
2025-10-13 19:47:25.756401: val_loss -0.4854
2025-10-13 19:47:25.756716: Pseudo dice [np.float32(0.7086)]
2025-10-13 19:47:25.757015: Epoch time: 46.31 s
2025-10-13 19:47:26.179111: Yayy! New best EMA pseudo Dice: 0.6970000267028809
2025-10-13 19:47:27.192867: 
2025-10-13 19:47:27.193377: Epoch 25
2025-10-13 19:47:27.193869: Current learning rate: 0.00849
2025-10-13 19:48:13.436340: Validation loss did not improve from -0.51329. Patience: 7/50
2025-10-13 19:48:13.436837: train_loss -0.6433
2025-10-13 19:48:13.437083: val_loss -0.4796
2025-10-13 19:48:13.437265: Pseudo dice [np.float32(0.7167)]
2025-10-13 19:48:13.437460: Epoch time: 46.24 s
2025-10-13 19:48:13.437644: Yayy! New best EMA pseudo Dice: 0.6988999843597412
2025-10-13 19:48:14.472844: 
2025-10-13 19:48:14.473040: Epoch 26
2025-10-13 19:48:14.473277: Current learning rate: 0.00843
2025-10-13 19:49:00.709726: Validation loss did not improve from -0.51329. Patience: 8/50
2025-10-13 19:49:00.710170: train_loss -0.6564
2025-10-13 19:49:00.710319: val_loss -0.4712
2025-10-13 19:49:00.710428: Pseudo dice [np.float32(0.6978)]
2025-10-13 19:49:00.710615: Epoch time: 46.24 s
2025-10-13 19:49:01.315531: 
2025-10-13 19:49:01.315788: Epoch 27
2025-10-13 19:49:01.316052: Current learning rate: 0.00836
2025-10-13 19:49:47.591046: Validation loss did not improve from -0.51329. Patience: 9/50
2025-10-13 19:49:47.591472: train_loss -0.66
2025-10-13 19:49:47.591612: val_loss -0.4948
2025-10-13 19:49:47.591791: Pseudo dice [np.float32(0.7135)]
2025-10-13 19:49:47.592081: Epoch time: 46.28 s
2025-10-13 19:49:47.592329: Yayy! New best EMA pseudo Dice: 0.7002999782562256
2025-10-13 19:49:48.629562: 
2025-10-13 19:49:48.629797: Epoch 28
2025-10-13 19:49:48.630013: Current learning rate: 0.0083
2025-10-13 19:50:34.904843: Validation loss did not improve from -0.51329. Patience: 10/50
2025-10-13 19:50:34.905732: train_loss -0.6588
2025-10-13 19:50:34.906050: val_loss -0.4913
2025-10-13 19:50:34.906358: Pseudo dice [np.float32(0.7236)]
2025-10-13 19:50:34.906664: Epoch time: 46.28 s
2025-10-13 19:50:34.906956: Yayy! New best EMA pseudo Dice: 0.7026000022888184
2025-10-13 19:50:35.946174: 
2025-10-13 19:50:35.946475: Epoch 29
2025-10-13 19:50:35.946768: Current learning rate: 0.00824
2025-10-13 19:51:22.178893: Validation loss did not improve from -0.51329. Patience: 11/50
2025-10-13 19:51:22.179397: train_loss -0.6651
2025-10-13 19:51:22.179629: val_loss -0.4771
2025-10-13 19:51:22.179850: Pseudo dice [np.float32(0.7115)]
2025-10-13 19:51:22.180143: Epoch time: 46.23 s
2025-10-13 19:51:22.621069: Yayy! New best EMA pseudo Dice: 0.703499972820282
2025-10-13 19:51:23.654600: 
2025-10-13 19:51:23.655448: Epoch 30
2025-10-13 19:51:23.655855: Current learning rate: 0.00818
2025-10-13 19:52:09.915316: Validation loss did not improve from -0.51329. Patience: 12/50
2025-10-13 19:52:09.915849: train_loss -0.663
2025-10-13 19:52:09.915991: val_loss -0.4727
2025-10-13 19:52:09.916105: Pseudo dice [np.float32(0.7098)]
2025-10-13 19:52:09.916251: Epoch time: 46.26 s
2025-10-13 19:52:09.916373: Yayy! New best EMA pseudo Dice: 0.7041000127792358
2025-10-13 19:52:10.961030: 
2025-10-13 19:52:10.961356: Epoch 31
2025-10-13 19:52:10.961666: Current learning rate: 0.00812
2025-10-13 19:52:57.178974: Validation loss did not improve from -0.51329. Patience: 13/50
2025-10-13 19:52:57.179539: train_loss -0.6704
2025-10-13 19:52:57.179789: val_loss -0.5016
2025-10-13 19:52:57.180018: Pseudo dice [np.float32(0.7215)]
2025-10-13 19:52:57.180242: Epoch time: 46.22 s
2025-10-13 19:52:57.180425: Yayy! New best EMA pseudo Dice: 0.7059000134468079
2025-10-13 19:52:58.218158: 
2025-10-13 19:52:58.218486: Epoch 32
2025-10-13 19:52:58.218710: Current learning rate: 0.00806
2025-10-13 19:53:44.520291: Validation loss did not improve from -0.51329. Patience: 14/50
2025-10-13 19:53:44.520865: train_loss -0.6825
2025-10-13 19:53:44.521040: val_loss -0.4547
2025-10-13 19:53:44.521226: Pseudo dice [np.float32(0.6989)]
2025-10-13 19:53:44.521468: Epoch time: 46.3 s
2025-10-13 19:53:45.141277: 
2025-10-13 19:53:45.141608: Epoch 33
2025-10-13 19:53:45.141969: Current learning rate: 0.008
2025-10-13 19:54:31.449565: Validation loss did not improve from -0.51329. Patience: 15/50
2025-10-13 19:54:31.450060: train_loss -0.6762
2025-10-13 19:54:31.450225: val_loss -0.425
2025-10-13 19:54:31.450342: Pseudo dice [np.float32(0.6843)]
2025-10-13 19:54:31.450473: Epoch time: 46.31 s
2025-10-13 19:54:32.062115: 
2025-10-13 19:54:32.062387: Epoch 34
2025-10-13 19:54:32.062528: Current learning rate: 0.00793
2025-10-13 19:55:18.425502: Validation loss did not improve from -0.51329. Patience: 16/50
2025-10-13 19:55:18.426354: train_loss -0.6838
2025-10-13 19:55:18.426661: val_loss -0.4857
2025-10-13 19:55:18.426906: Pseudo dice [np.float32(0.7117)]
2025-10-13 19:55:18.427160: Epoch time: 46.36 s
2025-10-13 19:55:19.762082: 
2025-10-13 19:55:19.762618: Epoch 35
2025-10-13 19:55:19.762811: Current learning rate: 0.00787
2025-10-13 19:56:06.018212: Validation loss did not improve from -0.51329. Patience: 17/50
2025-10-13 19:56:06.018678: train_loss -0.6854
2025-10-13 19:56:06.018839: val_loss -0.4814
2025-10-13 19:56:06.018954: Pseudo dice [np.float32(0.7202)]
2025-10-13 19:56:06.019108: Epoch time: 46.26 s
2025-10-13 19:56:06.633328: 
2025-10-13 19:56:06.633653: Epoch 36
2025-10-13 19:56:06.633834: Current learning rate: 0.00781
2025-10-13 19:56:52.901551: Validation loss did not improve from -0.51329. Patience: 18/50
2025-10-13 19:56:52.902192: train_loss -0.6909
2025-10-13 19:56:52.902396: val_loss -0.4923
2025-10-13 19:56:52.902588: Pseudo dice [np.float32(0.7282)]
2025-10-13 19:56:52.902787: Epoch time: 46.27 s
2025-10-13 19:56:52.902979: Yayy! New best EMA pseudo Dice: 0.7077999711036682
2025-10-13 19:56:53.966810: 
2025-10-13 19:56:53.967069: Epoch 37
2025-10-13 19:56:53.967235: Current learning rate: 0.00775
2025-10-13 19:57:40.206213: Validation loss did not improve from -0.51329. Patience: 19/50
2025-10-13 19:57:40.206577: train_loss -0.6928
2025-10-13 19:57:40.206728: val_loss -0.4761
2025-10-13 19:57:40.206856: Pseudo dice [np.float32(0.7044)]
2025-10-13 19:57:40.206987: Epoch time: 46.24 s
2025-10-13 19:57:40.822156: 
2025-10-13 19:57:40.822493: Epoch 38
2025-10-13 19:57:40.822679: Current learning rate: 0.00769
2025-10-13 19:58:27.086668: Validation loss did not improve from -0.51329. Patience: 20/50
2025-10-13 19:58:27.087192: train_loss -0.6922
2025-10-13 19:58:27.087346: val_loss -0.4835
2025-10-13 19:58:27.087457: Pseudo dice [np.float32(0.7122)]
2025-10-13 19:58:27.087607: Epoch time: 46.27 s
2025-10-13 19:58:27.087735: Yayy! New best EMA pseudo Dice: 0.7080000042915344
2025-10-13 19:58:28.130804: 
2025-10-13 19:58:28.131076: Epoch 39
2025-10-13 19:58:28.131327: Current learning rate: 0.00763
2025-10-13 19:59:14.388191: Validation loss did not improve from -0.51329. Patience: 21/50
2025-10-13 19:59:14.388702: train_loss -0.6947
2025-10-13 19:59:14.388861: val_loss -0.4695
2025-10-13 19:59:14.388996: Pseudo dice [np.float32(0.7034)]
2025-10-13 19:59:14.389152: Epoch time: 46.26 s
2025-10-13 19:59:15.439094: 
2025-10-13 19:59:15.439439: Epoch 40
2025-10-13 19:59:15.439616: Current learning rate: 0.00756
2025-10-13 20:00:01.689010: Validation loss improved from -0.51329 to -0.54026! Patience: 21/50
2025-10-13 20:00:01.689618: train_loss -0.7062
2025-10-13 20:00:01.689809: val_loss -0.5403
2025-10-13 20:00:01.689938: Pseudo dice [np.float32(0.75)]
2025-10-13 20:00:01.690100: Epoch time: 46.25 s
2025-10-13 20:00:01.690231: Yayy! New best EMA pseudo Dice: 0.7117999792098999
2025-10-13 20:00:02.732866: 
2025-10-13 20:00:02.733304: Epoch 41
2025-10-13 20:00:02.733572: Current learning rate: 0.0075
2025-10-13 20:00:48.984264: Validation loss did not improve from -0.54026. Patience: 1/50
2025-10-13 20:00:48.985052: train_loss -0.6963
2025-10-13 20:00:48.985219: val_loss -0.498
2025-10-13 20:00:48.985333: Pseudo dice [np.float32(0.7331)]
2025-10-13 20:00:48.985559: Epoch time: 46.25 s
2025-10-13 20:00:48.985739: Yayy! New best EMA pseudo Dice: 0.7139000296592712
2025-10-13 20:00:50.007229: 
2025-10-13 20:00:50.007749: Epoch 42
2025-10-13 20:00:50.008157: Current learning rate: 0.00744
2025-10-13 20:01:36.292789: Validation loss did not improve from -0.54026. Patience: 2/50
2025-10-13 20:01:36.293357: train_loss -0.6988
2025-10-13 20:01:36.293512: val_loss -0.5039
2025-10-13 20:01:36.293655: Pseudo dice [np.float32(0.7317)]
2025-10-13 20:01:36.293843: Epoch time: 46.29 s
2025-10-13 20:01:36.294017: Yayy! New best EMA pseudo Dice: 0.7156999707221985
2025-10-13 20:01:37.331543: 
2025-10-13 20:01:37.331981: Epoch 43
2025-10-13 20:01:37.332313: Current learning rate: 0.00738
2025-10-13 20:02:23.601569: Validation loss did not improve from -0.54026. Patience: 3/50
2025-10-13 20:02:23.602151: train_loss -0.7081
2025-10-13 20:02:23.602323: val_loss -0.5277
2025-10-13 20:02:23.602453: Pseudo dice [np.float32(0.7385)]
2025-10-13 20:02:23.602599: Epoch time: 46.27 s
2025-10-13 20:02:23.602723: Yayy! New best EMA pseudo Dice: 0.7179999947547913
2025-10-13 20:02:24.632944: 
2025-10-13 20:02:24.633269: Epoch 44
2025-10-13 20:02:24.633424: Current learning rate: 0.00732
2025-10-13 20:03:10.951499: Validation loss did not improve from -0.54026. Patience: 4/50
2025-10-13 20:03:10.952062: train_loss -0.7063
2025-10-13 20:03:10.952218: val_loss -0.4749
2025-10-13 20:03:10.952342: Pseudo dice [np.float32(0.7059)]
2025-10-13 20:03:10.952463: Epoch time: 46.32 s
2025-10-13 20:03:11.984173: 
2025-10-13 20:03:11.984421: Epoch 45
2025-10-13 20:03:11.984568: Current learning rate: 0.00725
2025-10-13 20:03:58.269301: Validation loss did not improve from -0.54026. Patience: 5/50
2025-10-13 20:03:58.269937: train_loss -0.7115
2025-10-13 20:03:58.270325: val_loss -0.458
2025-10-13 20:03:58.270669: Pseudo dice [np.float32(0.7073)]
2025-10-13 20:03:58.271070: Epoch time: 46.29 s
2025-10-13 20:03:58.869262: 
2025-10-13 20:03:58.869712: Epoch 46
2025-10-13 20:03:58.870067: Current learning rate: 0.00719
2025-10-13 20:04:45.216724: Validation loss did not improve from -0.54026. Patience: 6/50
2025-10-13 20:04:45.217307: train_loss -0.7094
2025-10-13 20:04:45.217470: val_loss -0.5247
2025-10-13 20:04:45.217590: Pseudo dice [np.float32(0.7467)]
2025-10-13 20:04:45.217726: Epoch time: 46.35 s
2025-10-13 20:04:45.217876: Yayy! New best EMA pseudo Dice: 0.7189000248908997
2025-10-13 20:04:46.570552: 
2025-10-13 20:04:46.570867: Epoch 47
2025-10-13 20:04:46.571162: Current learning rate: 0.00713
2025-10-13 20:05:32.849374: Validation loss did not improve from -0.54026. Patience: 7/50
2025-10-13 20:05:32.849810: train_loss -0.7172
2025-10-13 20:05:32.850065: val_loss -0.5009
2025-10-13 20:05:32.850292: Pseudo dice [np.float32(0.7228)]
2025-10-13 20:05:32.850487: Epoch time: 46.28 s
2025-10-13 20:05:32.850683: Yayy! New best EMA pseudo Dice: 0.7192999720573425
2025-10-13 20:05:33.891808: 
2025-10-13 20:05:33.892065: Epoch 48
2025-10-13 20:05:33.892313: Current learning rate: 0.00707
2025-10-13 20:06:20.137116: Validation loss did not improve from -0.54026. Patience: 8/50
2025-10-13 20:06:20.137725: train_loss -0.7171
2025-10-13 20:06:20.137885: val_loss -0.5077
2025-10-13 20:06:20.138002: Pseudo dice [np.float32(0.7333)]
2025-10-13 20:06:20.138168: Epoch time: 46.25 s
2025-10-13 20:06:20.138303: Yayy! New best EMA pseudo Dice: 0.7207000255584717
2025-10-13 20:06:21.182606: 
2025-10-13 20:06:21.182951: Epoch 49
2025-10-13 20:06:21.183256: Current learning rate: 0.007
2025-10-13 20:07:07.406082: Validation loss did not improve from -0.54026. Patience: 9/50
2025-10-13 20:07:07.406628: train_loss -0.72
2025-10-13 20:07:07.406781: val_loss -0.4787
2025-10-13 20:07:07.406897: Pseudo dice [np.float32(0.7107)]
2025-10-13 20:07:07.407051: Epoch time: 46.22 s
2025-10-13 20:07:08.439711: 
2025-10-13 20:07:08.440112: Epoch 50
2025-10-13 20:07:08.440392: Current learning rate: 0.00694
2025-10-13 20:07:54.819066: Validation loss did not improve from -0.54026. Patience: 10/50
2025-10-13 20:07:54.819773: train_loss -0.729
2025-10-13 20:07:54.820045: val_loss -0.4953
2025-10-13 20:07:54.820302: Pseudo dice [np.float32(0.7248)]
2025-10-13 20:07:54.820453: Epoch time: 46.38 s
2025-10-13 20:07:55.427623: 
2025-10-13 20:07:55.427953: Epoch 51
2025-10-13 20:07:55.428361: Current learning rate: 0.00688
2025-10-13 20:08:41.772330: Validation loss did not improve from -0.54026. Patience: 11/50
2025-10-13 20:08:41.772768: train_loss -0.7187
2025-10-13 20:08:41.772960: val_loss -0.4931
2025-10-13 20:08:41.773085: Pseudo dice [np.float32(0.7312)]
2025-10-13 20:08:41.773232: Epoch time: 46.35 s
2025-10-13 20:08:41.773349: Yayy! New best EMA pseudo Dice: 0.7213000059127808
2025-10-13 20:08:42.819877: 
2025-10-13 20:08:42.820186: Epoch 52
2025-10-13 20:08:42.820489: Current learning rate: 0.00682
2025-10-13 20:09:29.179304: Validation loss did not improve from -0.54026. Patience: 12/50
2025-10-13 20:09:29.180019: train_loss -0.728
2025-10-13 20:09:29.180289: val_loss -0.5058
2025-10-13 20:09:29.180536: Pseudo dice [np.float32(0.7267)]
2025-10-13 20:09:29.180779: Epoch time: 46.36 s
2025-10-13 20:09:29.180924: Yayy! New best EMA pseudo Dice: 0.7218000292778015
2025-10-13 20:09:30.214941: 
2025-10-13 20:09:30.215235: Epoch 53
2025-10-13 20:09:30.215395: Current learning rate: 0.00675
2025-10-13 20:10:16.406153: Validation loss did not improve from -0.54026. Patience: 13/50
2025-10-13 20:10:16.406729: train_loss -0.7271
2025-10-13 20:10:16.407047: val_loss -0.4784
2025-10-13 20:10:16.407331: Pseudo dice [np.float32(0.7179)]
2025-10-13 20:10:16.407639: Epoch time: 46.19 s
2025-10-13 20:10:17.014834: 
2025-10-13 20:10:17.015160: Epoch 54
2025-10-13 20:10:17.015340: Current learning rate: 0.00669
2025-10-13 20:11:03.217278: Validation loss did not improve from -0.54026. Patience: 14/50
2025-10-13 20:11:03.217808: train_loss -0.7233
2025-10-13 20:11:03.217932: val_loss -0.4444
2025-10-13 20:11:03.218035: Pseudo dice [np.float32(0.6956)]
2025-10-13 20:11:03.218156: Epoch time: 46.2 s
2025-10-13 20:11:04.267351: 
2025-10-13 20:11:04.267636: Epoch 55
2025-10-13 20:11:04.267857: Current learning rate: 0.00663
2025-10-13 20:11:50.454039: Validation loss did not improve from -0.54026. Patience: 15/50
2025-10-13 20:11:50.454534: train_loss -0.722
2025-10-13 20:11:50.454682: val_loss -0.4608
2025-10-13 20:11:50.454803: Pseudo dice [np.float32(0.6966)]
2025-10-13 20:11:50.454931: Epoch time: 46.19 s
2025-10-13 20:11:51.067927: 
2025-10-13 20:11:51.068268: Epoch 56
2025-10-13 20:11:51.068440: Current learning rate: 0.00657
2025-10-13 20:12:37.233173: Validation loss did not improve from -0.54026. Patience: 16/50
2025-10-13 20:12:37.233799: train_loss -0.7206
2025-10-13 20:12:37.234019: val_loss -0.4906
2025-10-13 20:12:37.234271: Pseudo dice [np.float32(0.7199)]
2025-10-13 20:12:37.234560: Epoch time: 46.17 s
2025-10-13 20:12:37.846561: 
2025-10-13 20:12:37.846920: Epoch 57
2025-10-13 20:12:37.847301: Current learning rate: 0.0065
2025-10-13 20:13:24.052427: Validation loss did not improve from -0.54026. Patience: 17/50
2025-10-13 20:13:24.052892: train_loss -0.7222
2025-10-13 20:13:24.053120: val_loss -0.5223
2025-10-13 20:13:24.053377: Pseudo dice [np.float32(0.744)]
2025-10-13 20:13:24.053622: Epoch time: 46.21 s
2025-10-13 20:13:24.665790: 
2025-10-13 20:13:24.666284: Epoch 58
2025-10-13 20:13:24.666632: Current learning rate: 0.00644
2025-10-13 20:14:10.909628: Validation loss did not improve from -0.54026. Patience: 18/50
2025-10-13 20:14:10.910323: train_loss -0.7338
2025-10-13 20:14:10.910510: val_loss -0.4629
2025-10-13 20:14:10.910664: Pseudo dice [np.float32(0.7032)]
2025-10-13 20:14:10.910789: Epoch time: 46.25 s
2025-10-13 20:14:11.812622: 
2025-10-13 20:14:11.812925: Epoch 59
2025-10-13 20:14:11.813122: Current learning rate: 0.00638
2025-10-13 20:14:58.050692: Validation loss did not improve from -0.54026. Patience: 19/50
2025-10-13 20:14:58.051147: train_loss -0.7367
2025-10-13 20:14:58.051288: val_loss -0.4675
2025-10-13 20:14:58.051408: Pseudo dice [np.float32(0.7123)]
2025-10-13 20:14:58.051568: Epoch time: 46.24 s
2025-10-13 20:14:59.131613: 
2025-10-13 20:14:59.131924: Epoch 60
2025-10-13 20:14:59.132093: Current learning rate: 0.00631
2025-10-13 20:15:45.434640: Validation loss did not improve from -0.54026. Patience: 20/50
2025-10-13 20:15:45.435090: train_loss -0.7386
2025-10-13 20:15:45.435227: val_loss -0.4824
2025-10-13 20:15:45.435342: Pseudo dice [np.float32(0.721)]
2025-10-13 20:15:45.435510: Epoch time: 46.3 s
2025-10-13 20:15:46.053455: 
2025-10-13 20:15:46.053789: Epoch 61
2025-10-13 20:15:46.053977: Current learning rate: 0.00625
2025-10-13 20:16:32.345835: Validation loss did not improve from -0.54026. Patience: 21/50
2025-10-13 20:16:32.346893: train_loss -0.7375
2025-10-13 20:16:32.347299: val_loss -0.4988
2025-10-13 20:16:32.347649: Pseudo dice [np.float32(0.7276)]
2025-10-13 20:16:32.347986: Epoch time: 46.29 s
2025-10-13 20:16:32.969363: 
2025-10-13 20:16:32.969746: Epoch 62
2025-10-13 20:16:32.969932: Current learning rate: 0.00619
2025-10-13 20:17:19.275813: Validation loss did not improve from -0.54026. Patience: 22/50
2025-10-13 20:17:19.276352: train_loss -0.741
2025-10-13 20:17:19.276572: val_loss -0.4753
2025-10-13 20:17:19.276722: Pseudo dice [np.float32(0.7145)]
2025-10-13 20:17:19.276883: Epoch time: 46.31 s
2025-10-13 20:17:19.893805: 
2025-10-13 20:17:19.894071: Epoch 63
2025-10-13 20:17:19.894258: Current learning rate: 0.00612
2025-10-13 20:18:06.163335: Validation loss did not improve from -0.54026. Patience: 23/50
2025-10-13 20:18:06.163884: train_loss -0.7402
2025-10-13 20:18:06.164053: val_loss -0.5055
2025-10-13 20:18:06.164188: Pseudo dice [np.float32(0.7366)]
2025-10-13 20:18:06.164313: Epoch time: 46.27 s
2025-10-13 20:18:06.787008: 
2025-10-13 20:18:06.787389: Epoch 64
2025-10-13 20:18:06.787762: Current learning rate: 0.00606
2025-10-13 20:18:53.067922: Validation loss did not improve from -0.54026. Patience: 24/50
2025-10-13 20:18:53.068521: train_loss -0.7397
2025-10-13 20:18:53.068678: val_loss -0.5069
2025-10-13 20:18:53.068798: Pseudo dice [np.float32(0.738)]
2025-10-13 20:18:53.068941: Epoch time: 46.28 s
2025-10-13 20:18:53.510116: Yayy! New best EMA pseudo Dice: 0.722000002861023
2025-10-13 20:18:54.554824: 
2025-10-13 20:18:54.555262: Epoch 65
2025-10-13 20:18:54.555594: Current learning rate: 0.006
2025-10-13 20:19:40.804277: Validation loss did not improve from -0.54026. Patience: 25/50
2025-10-13 20:19:40.804674: train_loss -0.7395
2025-10-13 20:19:40.804882: val_loss -0.4658
2025-10-13 20:19:40.805115: Pseudo dice [np.float32(0.7023)]
2025-10-13 20:19:40.805405: Epoch time: 46.25 s
2025-10-13 20:19:41.423363: 
2025-10-13 20:19:41.423954: Epoch 66
2025-10-13 20:19:41.424386: Current learning rate: 0.00593
2025-10-13 20:20:27.631191: Validation loss did not improve from -0.54026. Patience: 26/50
2025-10-13 20:20:27.631701: train_loss -0.7471
2025-10-13 20:20:27.631874: val_loss -0.4884
2025-10-13 20:20:27.632016: Pseudo dice [np.float32(0.7144)]
2025-10-13 20:20:27.632170: Epoch time: 46.21 s
2025-10-13 20:20:28.253424: 
2025-10-13 20:20:28.253723: Epoch 67
2025-10-13 20:20:28.253919: Current learning rate: 0.00587
2025-10-13 20:21:14.463008: Validation loss did not improve from -0.54026. Patience: 27/50
2025-10-13 20:21:14.463730: train_loss -0.7492
2025-10-13 20:21:14.463962: val_loss -0.5024
2025-10-13 20:21:14.464185: Pseudo dice [np.float32(0.7326)]
2025-10-13 20:21:14.464394: Epoch time: 46.21 s
2025-10-13 20:21:15.080687: 
2025-10-13 20:21:15.080942: Epoch 68
2025-10-13 20:21:15.081098: Current learning rate: 0.00581
2025-10-13 20:22:01.404886: Validation loss did not improve from -0.54026. Patience: 28/50
2025-10-13 20:22:01.405491: train_loss -0.7499
2025-10-13 20:22:01.405726: val_loss -0.4877
2025-10-13 20:22:01.405959: Pseudo dice [np.float32(0.7327)]
2025-10-13 20:22:01.406256: Epoch time: 46.33 s
2025-10-13 20:22:02.024557: 
2025-10-13 20:22:02.024970: Epoch 69
2025-10-13 20:22:02.025252: Current learning rate: 0.00574
2025-10-13 20:22:48.228056: Validation loss did not improve from -0.54026. Patience: 29/50
2025-10-13 20:22:48.228663: train_loss -0.7516
2025-10-13 20:22:48.228953: val_loss -0.5072
2025-10-13 20:22:48.229209: Pseudo dice [np.float32(0.7368)]
2025-10-13 20:22:48.229531: Epoch time: 46.2 s
2025-10-13 20:22:48.649266: Yayy! New best EMA pseudo Dice: 0.7233999967575073
2025-10-13 20:22:49.686295: 
2025-10-13 20:22:49.686801: Epoch 70
2025-10-13 20:22:49.687021: Current learning rate: 0.00568
2025-10-13 20:23:35.920161: Validation loss did not improve from -0.54026. Patience: 30/50
2025-10-13 20:23:35.920898: train_loss -0.7524
2025-10-13 20:23:35.921043: val_loss -0.4946
2025-10-13 20:23:35.921193: Pseudo dice [np.float32(0.7269)]
2025-10-13 20:23:35.921413: Epoch time: 46.24 s
2025-10-13 20:23:35.921543: Yayy! New best EMA pseudo Dice: 0.723800003528595
2025-10-13 20:23:37.282754: 
2025-10-13 20:23:37.283021: Epoch 71
2025-10-13 20:23:37.283197: Current learning rate: 0.00562
2025-10-13 20:24:23.550783: Validation loss did not improve from -0.54026. Patience: 31/50
2025-10-13 20:24:23.551231: train_loss -0.7548
2025-10-13 20:24:23.551372: val_loss -0.4924
2025-10-13 20:24:23.551482: Pseudo dice [np.float32(0.7274)]
2025-10-13 20:24:23.551608: Epoch time: 46.27 s
2025-10-13 20:24:23.551753: Yayy! New best EMA pseudo Dice: 0.7240999937057495
2025-10-13 20:24:24.609399: 
2025-10-13 20:24:24.609856: Epoch 72
2025-10-13 20:24:24.610212: Current learning rate: 0.00555
2025-10-13 20:25:10.858205: Validation loss did not improve from -0.54026. Patience: 32/50
2025-10-13 20:25:10.858937: train_loss -0.7522
2025-10-13 20:25:10.859233: val_loss -0.475
2025-10-13 20:25:10.859422: Pseudo dice [np.float32(0.7165)]
2025-10-13 20:25:10.859581: Epoch time: 46.25 s
2025-10-13 20:25:11.479305: 
2025-10-13 20:25:11.479671: Epoch 73
2025-10-13 20:25:11.479846: Current learning rate: 0.00549
2025-10-13 20:25:57.694654: Validation loss did not improve from -0.54026. Patience: 33/50
2025-10-13 20:25:57.695356: train_loss -0.7573
2025-10-13 20:25:57.695525: val_loss -0.4947
2025-10-13 20:25:57.695703: Pseudo dice [np.float32(0.7284)]
2025-10-13 20:25:57.695965: Epoch time: 46.22 s
2025-10-13 20:25:58.315123: 
2025-10-13 20:25:58.315634: Epoch 74
2025-10-13 20:25:58.316073: Current learning rate: 0.00542
2025-10-13 20:26:44.588227: Validation loss did not improve from -0.54026. Patience: 34/50
2025-10-13 20:26:44.588822: train_loss -0.7545
2025-10-13 20:26:44.589018: val_loss -0.483
2025-10-13 20:26:44.589172: Pseudo dice [np.float32(0.7291)]
2025-10-13 20:26:44.589331: Epoch time: 46.27 s
2025-10-13 20:26:45.014825: Yayy! New best EMA pseudo Dice: 0.724399983882904
2025-10-13 20:26:46.035995: 
2025-10-13 20:26:46.036339: Epoch 75
2025-10-13 20:26:46.036549: Current learning rate: 0.00536
2025-10-13 20:27:32.252424: Validation loss did not improve from -0.54026. Patience: 35/50
2025-10-13 20:27:32.252936: train_loss -0.7572
2025-10-13 20:27:32.253092: val_loss -0.5055
2025-10-13 20:27:32.253239: Pseudo dice [np.float32(0.7305)]
2025-10-13 20:27:32.253404: Epoch time: 46.22 s
2025-10-13 20:27:32.253549: Yayy! New best EMA pseudo Dice: 0.7250000238418579
2025-10-13 20:27:33.302470: 
2025-10-13 20:27:33.302985: Epoch 76
2025-10-13 20:27:33.303361: Current learning rate: 0.00529
2025-10-13 20:28:19.536209: Validation loss did not improve from -0.54026. Patience: 36/50
2025-10-13 20:28:19.536858: train_loss -0.7625
2025-10-13 20:28:19.537013: val_loss -0.466
2025-10-13 20:28:19.537129: Pseudo dice [np.float32(0.7111)]
2025-10-13 20:28:19.537319: Epoch time: 46.24 s
2025-10-13 20:28:20.154718: 
2025-10-13 20:28:20.155004: Epoch 77
2025-10-13 20:28:20.155199: Current learning rate: 0.00523
2025-10-13 20:29:06.395226: Validation loss did not improve from -0.54026. Patience: 37/50
2025-10-13 20:29:06.395675: train_loss -0.7598
2025-10-13 20:29:06.395829: val_loss -0.4942
2025-10-13 20:29:06.395984: Pseudo dice [np.float32(0.7313)]
2025-10-13 20:29:06.396149: Epoch time: 46.24 s
2025-10-13 20:29:07.024683: 
2025-10-13 20:29:07.024960: Epoch 78
2025-10-13 20:29:07.025132: Current learning rate: 0.00517
2025-10-13 20:29:53.285469: Validation loss did not improve from -0.54026. Patience: 38/50
2025-10-13 20:29:53.285948: train_loss -0.7612
2025-10-13 20:29:53.286089: val_loss -0.5248
2025-10-13 20:29:53.286314: Pseudo dice [np.float32(0.7426)]
2025-10-13 20:29:53.286484: Epoch time: 46.26 s
2025-10-13 20:29:53.286709: Yayy! New best EMA pseudo Dice: 0.7261999845504761
2025-10-13 20:29:54.344016: 
2025-10-13 20:29:54.344272: Epoch 79
2025-10-13 20:29:54.344495: Current learning rate: 0.0051
2025-10-13 20:30:40.581867: Validation loss did not improve from -0.54026. Patience: 39/50
2025-10-13 20:30:40.582369: train_loss -0.756
2025-10-13 20:30:40.582501: val_loss -0.5122
2025-10-13 20:30:40.582612: Pseudo dice [np.float32(0.7389)]
2025-10-13 20:30:40.582779: Epoch time: 46.24 s
2025-10-13 20:30:41.019021: Yayy! New best EMA pseudo Dice: 0.7275000214576721
2025-10-13 20:30:42.091024: 
2025-10-13 20:30:42.091550: Epoch 80
2025-10-13 20:30:42.091928: Current learning rate: 0.00504
2025-10-13 20:31:28.351854: Validation loss did not improve from -0.54026. Patience: 40/50
2025-10-13 20:31:28.352371: train_loss -0.7641
2025-10-13 20:31:28.352525: val_loss -0.4617
2025-10-13 20:31:28.352633: Pseudo dice [np.float32(0.6991)]
2025-10-13 20:31:28.352791: Epoch time: 46.26 s
2025-10-13 20:31:28.978123: 
2025-10-13 20:31:28.978569: Epoch 81
2025-10-13 20:31:28.978826: Current learning rate: 0.00497
2025-10-13 20:32:15.131140: Validation loss did not improve from -0.54026. Patience: 41/50
2025-10-13 20:32:15.131638: train_loss -0.7614
2025-10-13 20:32:15.131799: val_loss -0.4997
2025-10-13 20:32:15.131923: Pseudo dice [np.float32(0.7332)]
2025-10-13 20:32:15.132059: Epoch time: 46.15 s
2025-10-13 20:32:16.054758: 
2025-10-13 20:32:16.055069: Epoch 82
2025-10-13 20:32:16.055231: Current learning rate: 0.00491
2025-10-13 20:33:02.349356: Validation loss did not improve from -0.54026. Patience: 42/50
2025-10-13 20:33:02.350115: train_loss -0.7627
2025-10-13 20:33:02.350294: val_loss -0.4768
2025-10-13 20:33:02.350426: Pseudo dice [np.float32(0.7055)]
2025-10-13 20:33:02.350571: Epoch time: 46.3 s
2025-10-13 20:33:02.955969: 
2025-10-13 20:33:02.956313: Epoch 83
2025-10-13 20:33:02.956498: Current learning rate: 0.00484
2025-10-13 20:33:49.250094: Validation loss did not improve from -0.54026. Patience: 43/50
2025-10-13 20:33:49.250482: train_loss -0.768
2025-10-13 20:33:49.250623: val_loss -0.5091
2025-10-13 20:33:49.250749: Pseudo dice [np.float32(0.7336)]
2025-10-13 20:33:49.250873: Epoch time: 46.3 s
2025-10-13 20:33:49.855367: 
2025-10-13 20:33:49.855713: Epoch 84
2025-10-13 20:33:49.855910: Current learning rate: 0.00478
2025-10-13 20:34:36.168669: Validation loss did not improve from -0.54026. Patience: 44/50
2025-10-13 20:34:36.169255: train_loss -0.7655
2025-10-13 20:34:36.169433: val_loss -0.4869
2025-10-13 20:34:36.169579: Pseudo dice [np.float32(0.7242)]
2025-10-13 20:34:36.169736: Epoch time: 46.31 s
2025-10-13 20:34:37.230555: 
2025-10-13 20:34:37.230908: Epoch 85
2025-10-13 20:34:37.231171: Current learning rate: 0.00471
2025-10-13 20:35:23.484188: Validation loss did not improve from -0.54026. Patience: 45/50
2025-10-13 20:35:23.484702: train_loss -0.7675
2025-10-13 20:35:23.484846: val_loss -0.4805
2025-10-13 20:35:23.484966: Pseudo dice [np.float32(0.7248)]
2025-10-13 20:35:23.485134: Epoch time: 46.25 s
2025-10-13 20:35:24.087708: 
2025-10-13 20:35:24.088039: Epoch 86
2025-10-13 20:35:24.088197: Current learning rate: 0.00465
2025-10-13 20:36:10.367306: Validation loss did not improve from -0.54026. Patience: 46/50
2025-10-13 20:36:10.367860: train_loss -0.7682
2025-10-13 20:36:10.368014: val_loss -0.5035
2025-10-13 20:36:10.368134: Pseudo dice [np.float32(0.7396)]
2025-10-13 20:36:10.368274: Epoch time: 46.28 s
2025-10-13 20:36:10.976963: 
2025-10-13 20:36:10.977437: Epoch 87
2025-10-13 20:36:10.977834: Current learning rate: 0.00458
2025-10-13 20:36:57.278766: Validation loss did not improve from -0.54026. Patience: 47/50
2025-10-13 20:36:57.279315: train_loss -0.7707
2025-10-13 20:36:57.279459: val_loss -0.5103
2025-10-13 20:36:57.279576: Pseudo dice [np.float32(0.737)]
2025-10-13 20:36:57.279799: Epoch time: 46.3 s
2025-10-13 20:36:57.886916: 
2025-10-13 20:36:57.887215: Epoch 88
2025-10-13 20:36:57.887377: Current learning rate: 0.00452
2025-10-13 20:37:44.092039: Validation loss did not improve from -0.54026. Patience: 48/50
2025-10-13 20:37:44.092681: train_loss -0.7681
2025-10-13 20:37:44.093026: val_loss -0.5191
2025-10-13 20:37:44.093171: Pseudo dice [np.float32(0.7455)]
2025-10-13 20:37:44.093322: Epoch time: 46.21 s
2025-10-13 20:37:44.093456: Yayy! New best EMA pseudo Dice: 0.7289999723434448
2025-10-13 20:37:45.135776: 
2025-10-13 20:37:45.136092: Epoch 89
2025-10-13 20:37:45.136270: Current learning rate: 0.00445
2025-10-13 20:38:31.375843: Validation loss did not improve from -0.54026. Patience: 49/50
2025-10-13 20:38:31.376280: train_loss -0.7693
2025-10-13 20:38:31.376450: val_loss -0.4843
2025-10-13 20:38:31.376587: Pseudo dice [np.float32(0.7218)]
2025-10-13 20:38:31.376828: Epoch time: 46.24 s
2025-10-13 20:38:32.422565: 
2025-10-13 20:38:32.422890: Epoch 90
2025-10-13 20:38:32.423069: Current learning rate: 0.00438
2025-10-13 20:39:18.640495: Validation loss did not improve from -0.54026. Patience: 50/50
2025-10-13 20:39:18.641115: train_loss -0.7697
2025-10-13 20:39:18.641258: val_loss -0.5079
2025-10-13 20:39:18.641450: Pseudo dice [np.float32(0.7311)]
2025-10-13 20:39:18.641643: Epoch time: 46.22 s
2025-10-13 20:39:19.248715: 
2025-10-13 20:39:19.248940: Epoch 91
2025-10-13 20:39:19.249106: Current learning rate: 0.00432
2025-10-13 20:40:05.472986: Validation loss did not improve from -0.54026. Patience: 51/50
2025-10-13 20:40:05.473550: train_loss -0.7721
2025-10-13 20:40:05.473683: val_loss -0.4757
2025-10-13 20:40:05.473809: Pseudo dice [np.float32(0.7227)]
2025-10-13 20:40:05.473932: Epoch time: 46.23 s
2025-10-13 20:40:06.083363: 
2025-10-13 20:40:06.083692: Epoch 92
2025-10-13 20:40:06.083854: Current learning rate: 0.00425
2025-10-13 20:40:52.304685: Validation loss did not improve from -0.54026. Patience: 52/50
2025-10-13 20:40:52.305279: train_loss -0.7778
2025-10-13 20:40:52.305462: val_loss -0.4918
2025-10-13 20:40:52.305643: Pseudo dice [np.float32(0.73)]
2025-10-13 20:40:52.305903: Epoch time: 46.22 s
2025-10-13 20:40:52.919001: 
2025-10-13 20:40:52.919315: Epoch 93
2025-10-13 20:40:52.919492: Current learning rate: 0.00419
2025-10-13 20:41:39.155848: Validation loss did not improve from -0.54026. Patience: 53/50
2025-10-13 20:41:39.156326: train_loss -0.7751
2025-10-13 20:41:39.156506: val_loss -0.5181
2025-10-13 20:41:39.156634: Pseudo dice [np.float32(0.7423)]
2025-10-13 20:41:39.156790: Epoch time: 46.24 s
2025-10-13 20:41:39.156904: Yayy! New best EMA pseudo Dice: 0.7296000123023987
2025-10-13 20:41:40.500920: 
2025-10-13 20:41:40.501157: Epoch 94
2025-10-13 20:41:40.501356: Current learning rate: 0.00412
2025-10-13 20:42:26.776201: Validation loss did not improve from -0.54026. Patience: 54/50
2025-10-13 20:42:26.776831: train_loss -0.7764
2025-10-13 20:42:26.776996: val_loss -0.5141
2025-10-13 20:42:26.777141: Pseudo dice [np.float32(0.7352)]
2025-10-13 20:42:26.777303: Epoch time: 46.28 s
2025-10-13 20:42:27.235737: Yayy! New best EMA pseudo Dice: 0.7300999760627747
2025-10-13 20:42:28.288776: 
2025-10-13 20:42:28.289033: Epoch 95
2025-10-13 20:42:28.289190: Current learning rate: 0.00405
2025-10-13 20:43:14.547130: Validation loss did not improve from -0.54026. Patience: 55/50
2025-10-13 20:43:14.547616: train_loss -0.7752
2025-10-13 20:43:14.547887: val_loss -0.5081
2025-10-13 20:43:14.548091: Pseudo dice [np.float32(0.7396)]
2025-10-13 20:43:14.548315: Epoch time: 46.26 s
2025-10-13 20:43:14.548500: Yayy! New best EMA pseudo Dice: 0.7311000227928162
2025-10-13 20:43:15.614169: 
2025-10-13 20:43:15.614512: Epoch 96
2025-10-13 20:43:15.614700: Current learning rate: 0.00399
2025-10-13 20:44:01.866021: Validation loss did not improve from -0.54026. Patience: 56/50
2025-10-13 20:44:01.866553: train_loss -0.7806
2025-10-13 20:44:01.866720: val_loss -0.4791
2025-10-13 20:44:01.866913: Pseudo dice [np.float32(0.7195)]
2025-10-13 20:44:01.867083: Epoch time: 46.25 s
2025-10-13 20:44:02.487160: 
2025-10-13 20:44:02.487658: Epoch 97
2025-10-13 20:44:02.488021: Current learning rate: 0.00392
2025-10-13 20:44:48.720891: Validation loss did not improve from -0.54026. Patience: 57/50
2025-10-13 20:44:48.721359: train_loss -0.7781
2025-10-13 20:44:48.721487: val_loss -0.4446
2025-10-13 20:44:48.721590: Pseudo dice [np.float32(0.7066)]
2025-10-13 20:44:48.721733: Epoch time: 46.23 s
2025-10-13 20:44:49.337316: 
2025-10-13 20:44:49.337987: Epoch 98
2025-10-13 20:44:49.338530: Current learning rate: 0.00385
2025-10-13 20:45:35.559047: Validation loss did not improve from -0.54026. Patience: 58/50
2025-10-13 20:45:35.559952: train_loss -0.7786
2025-10-13 20:45:35.560291: val_loss -0.5109
2025-10-13 20:45:35.560599: Pseudo dice [np.float32(0.7413)]
2025-10-13 20:45:35.560914: Epoch time: 46.22 s
2025-10-13 20:45:36.173867: 
2025-10-13 20:45:36.174373: Epoch 99
2025-10-13 20:45:36.174723: Current learning rate: 0.00379
2025-10-13 20:46:22.454810: Validation loss did not improve from -0.54026. Patience: 59/50
2025-10-13 20:46:22.455211: train_loss -0.7827
2025-10-13 20:46:22.455372: val_loss -0.4838
2025-10-13 20:46:22.455504: Pseudo dice [np.float32(0.7259)]
2025-10-13 20:46:22.455649: Epoch time: 46.28 s
2025-10-13 20:46:23.515935: 
2025-10-13 20:46:23.516175: Epoch 100
2025-10-13 20:46:23.516326: Current learning rate: 0.00372
2025-10-13 20:47:09.739688: Validation loss did not improve from -0.54026. Patience: 60/50
2025-10-13 20:47:09.740142: train_loss -0.7837
2025-10-13 20:47:09.740273: val_loss -0.4823
2025-10-13 20:47:09.740378: Pseudo dice [np.float32(0.7215)]
2025-10-13 20:47:09.740497: Epoch time: 46.22 s
2025-10-13 20:47:10.357744: 
2025-10-13 20:47:10.358006: Epoch 101
2025-10-13 20:47:10.358160: Current learning rate: 0.00365
2025-10-13 20:47:56.541190: Validation loss did not improve from -0.54026. Patience: 61/50
2025-10-13 20:47:56.541773: train_loss -0.7795
2025-10-13 20:47:56.542090: val_loss -0.5118
2025-10-13 20:47:56.542506: Pseudo dice [np.float32(0.7435)]
2025-10-13 20:47:56.543060: Epoch time: 46.18 s
2025-10-13 20:47:57.162765: 
2025-10-13 20:47:57.163023: Epoch 102
2025-10-13 20:47:57.163206: Current learning rate: 0.00359
2025-10-13 20:48:43.431773: Validation loss did not improve from -0.54026. Patience: 62/50
2025-10-13 20:48:43.432240: train_loss -0.7828
2025-10-13 20:48:43.432410: val_loss -0.463
2025-10-13 20:48:43.432617: Pseudo dice [np.float32(0.7169)]
2025-10-13 20:48:43.432852: Epoch time: 46.27 s
2025-10-13 20:48:44.048721: 
2025-10-13 20:48:44.049068: Epoch 103
2025-10-13 20:48:44.049266: Current learning rate: 0.00352
2025-10-13 20:49:30.274687: Validation loss did not improve from -0.54026. Patience: 63/50
2025-10-13 20:49:30.275313: train_loss -0.7845
2025-10-13 20:49:30.275586: val_loss -0.4859
2025-10-13 20:49:30.275836: Pseudo dice [np.float32(0.7257)]
2025-10-13 20:49:30.276111: Epoch time: 46.23 s
2025-10-13 20:49:30.902066: 
2025-10-13 20:49:30.902575: Epoch 104
2025-10-13 20:49:30.902895: Current learning rate: 0.00345
2025-10-13 20:50:17.159271: Validation loss did not improve from -0.54026. Patience: 64/50
2025-10-13 20:50:17.159836: train_loss -0.7805
2025-10-13 20:50:17.160015: val_loss -0.4655
2025-10-13 20:50:17.160187: Pseudo dice [np.float32(0.7178)]
2025-10-13 20:50:17.160336: Epoch time: 46.26 s
2025-10-13 20:50:18.207051: 
2025-10-13 20:50:18.207328: Epoch 105
2025-10-13 20:50:18.207530: Current learning rate: 0.00338
2025-10-13 20:51:04.379185: Validation loss did not improve from -0.54026. Patience: 65/50
2025-10-13 20:51:04.379666: train_loss -0.782
2025-10-13 20:51:04.379821: val_loss -0.4841
2025-10-13 20:51:04.379946: Pseudo dice [np.float32(0.7253)]
2025-10-13 20:51:04.380084: Epoch time: 46.17 s
2025-10-13 20:51:05.000004: 
2025-10-13 20:51:05.000454: Epoch 106
2025-10-13 20:51:05.000732: Current learning rate: 0.00332
2025-10-13 20:51:51.499034: Validation loss did not improve from -0.54026. Patience: 66/50
2025-10-13 20:51:51.499633: train_loss -0.7858
2025-10-13 20:51:51.499814: val_loss -0.4698
2025-10-13 20:51:51.499978: Pseudo dice [np.float32(0.7187)]
2025-10-13 20:51:51.500156: Epoch time: 46.5 s
2025-10-13 20:51:52.121712: 
2025-10-13 20:51:52.121989: Epoch 107
2025-10-13 20:51:52.122151: Current learning rate: 0.00325
2025-10-13 20:52:38.388158: Validation loss did not improve from -0.54026. Patience: 67/50
2025-10-13 20:52:38.388857: train_loss -0.788
2025-10-13 20:52:38.389042: val_loss -0.4978
2025-10-13 20:52:38.389162: Pseudo dice [np.float32(0.7306)]
2025-10-13 20:52:38.389314: Epoch time: 46.27 s
2025-10-13 20:52:39.007410: 
2025-10-13 20:52:39.007610: Epoch 108
2025-10-13 20:52:39.007766: Current learning rate: 0.00318
2025-10-13 20:53:25.207002: Validation loss did not improve from -0.54026. Patience: 68/50
2025-10-13 20:53:25.207540: train_loss -0.7857
2025-10-13 20:53:25.207670: val_loss -0.5035
2025-10-13 20:53:25.207798: Pseudo dice [np.float32(0.7285)]
2025-10-13 20:53:25.208014: Epoch time: 46.2 s
2025-10-13 20:53:25.825475: 
2025-10-13 20:53:25.825790: Epoch 109
2025-10-13 20:53:25.825974: Current learning rate: 0.00311
2025-10-13 20:54:12.008268: Validation loss did not improve from -0.54026. Patience: 69/50
2025-10-13 20:54:12.008920: train_loss -0.7899
2025-10-13 20:54:12.009173: val_loss -0.4917
2025-10-13 20:54:12.009424: Pseudo dice [np.float32(0.7326)]
2025-10-13 20:54:12.009731: Epoch time: 46.18 s
2025-10-13 20:54:13.070275: 
2025-10-13 20:54:13.070620: Epoch 110
2025-10-13 20:54:13.070807: Current learning rate: 0.00304
2025-10-13 20:54:59.314462: Validation loss did not improve from -0.54026. Patience: 70/50
2025-10-13 20:54:59.315169: train_loss -0.7915
2025-10-13 20:54:59.315459: val_loss -0.4914
2025-10-13 20:54:59.315637: Pseudo dice [np.float32(0.7316)]
2025-10-13 20:54:59.315854: Epoch time: 46.25 s
2025-10-13 20:54:59.933458: 
2025-10-13 20:54:59.933926: Epoch 111
2025-10-13 20:54:59.934273: Current learning rate: 0.00297
2025-10-13 20:55:46.143592: Validation loss did not improve from -0.54026. Patience: 71/50
2025-10-13 20:55:46.144066: train_loss -0.7856
2025-10-13 20:55:46.144253: val_loss -0.4832
2025-10-13 20:55:46.144371: Pseudo dice [np.float32(0.7245)]
2025-10-13 20:55:46.144500: Epoch time: 46.21 s
2025-10-13 20:55:46.764000: 
2025-10-13 20:55:46.764264: Epoch 112
2025-10-13 20:55:46.764419: Current learning rate: 0.00291
2025-10-13 20:56:32.950253: Validation loss did not improve from -0.54026. Patience: 72/50
2025-10-13 20:56:32.951461: train_loss -0.7867
2025-10-13 20:56:32.951775: val_loss -0.4385
2025-10-13 20:56:32.952042: Pseudo dice [np.float32(0.6993)]
2025-10-13 20:56:32.952333: Epoch time: 46.19 s
2025-10-13 20:56:33.572211: 
2025-10-13 20:56:33.572459: Epoch 113
2025-10-13 20:56:33.572639: Current learning rate: 0.00284
2025-10-13 20:57:19.778242: Validation loss did not improve from -0.54026. Patience: 73/50
2025-10-13 20:57:19.778690: train_loss -0.7878
2025-10-13 20:57:19.778838: val_loss -0.4893
2025-10-13 20:57:19.778967: Pseudo dice [np.float32(0.7313)]
2025-10-13 20:57:19.779129: Epoch time: 46.21 s
2025-10-13 20:57:20.397266: 
2025-10-13 20:57:20.397514: Epoch 114
2025-10-13 20:57:20.397674: Current learning rate: 0.00277
2025-10-13 20:58:06.602360: Validation loss did not improve from -0.54026. Patience: 74/50
2025-10-13 20:58:06.602929: train_loss -0.7883
2025-10-13 20:58:06.603083: val_loss -0.463
2025-10-13 20:58:06.603256: Pseudo dice [np.float32(0.7134)]
2025-10-13 20:58:06.603393: Epoch time: 46.21 s
2025-10-13 20:58:07.677907: 
2025-10-13 20:58:07.678374: Epoch 115
2025-10-13 20:58:07.678602: Current learning rate: 0.0027
2025-10-13 20:58:53.902224: Validation loss did not improve from -0.54026. Patience: 75/50
2025-10-13 20:58:53.902721: train_loss -0.79
2025-10-13 20:58:53.902922: val_loss -0.4961
2025-10-13 20:58:53.903042: Pseudo dice [np.float32(0.7308)]
2025-10-13 20:58:53.903188: Epoch time: 46.23 s
2025-10-13 20:58:54.531956: 
2025-10-13 20:58:54.532257: Epoch 116
2025-10-13 20:58:54.532420: Current learning rate: 0.00263
2025-10-13 20:59:40.699416: Validation loss did not improve from -0.54026. Patience: 76/50
2025-10-13 20:59:40.700085: train_loss -0.7915
2025-10-13 20:59:40.700250: val_loss -0.4922
2025-10-13 20:59:40.700378: Pseudo dice [np.float32(0.7376)]
2025-10-13 20:59:40.700582: Epoch time: 46.17 s
2025-10-13 20:59:41.328368: 
2025-10-13 20:59:41.328683: Epoch 117
2025-10-13 20:59:41.328911: Current learning rate: 0.00256
2025-10-13 21:00:27.499326: Validation loss did not improve from -0.54026. Patience: 77/50
2025-10-13 21:00:27.499822: train_loss -0.7942
2025-10-13 21:00:27.499982: val_loss -0.5088
2025-10-13 21:00:27.500112: Pseudo dice [np.float32(0.7332)]
2025-10-13 21:00:27.500263: Epoch time: 46.17 s
2025-10-13 21:00:28.425113: 
2025-10-13 21:00:28.425448: Epoch 118
2025-10-13 21:00:28.425601: Current learning rate: 0.00249
2025-10-13 21:01:14.558676: Validation loss did not improve from -0.54026. Patience: 78/50
2025-10-13 21:01:14.559523: train_loss -0.7955
2025-10-13 21:01:14.559682: val_loss -0.448
2025-10-13 21:01:14.559910: Pseudo dice [np.float32(0.7057)]
2025-10-13 21:01:14.560039: Epoch time: 46.14 s
2025-10-13 21:01:15.183297: 
2025-10-13 21:01:15.183623: Epoch 119
2025-10-13 21:01:15.183812: Current learning rate: 0.00242
2025-10-13 21:02:01.327875: Validation loss did not improve from -0.54026. Patience: 79/50
2025-10-13 21:02:01.328334: train_loss -0.7924
2025-10-13 21:02:01.328490: val_loss -0.5048
2025-10-13 21:02:01.328601: Pseudo dice [np.float32(0.7293)]
2025-10-13 21:02:01.328724: Epoch time: 46.15 s
2025-10-13 21:02:02.413368: 
2025-10-13 21:02:02.413623: Epoch 120
2025-10-13 21:02:02.413870: Current learning rate: 0.00235
2025-10-13 21:02:48.581588: Validation loss did not improve from -0.54026. Patience: 80/50
2025-10-13 21:02:48.582241: train_loss -0.795
2025-10-13 21:02:48.582454: val_loss -0.4931
2025-10-13 21:02:48.582616: Pseudo dice [np.float32(0.7324)]
2025-10-13 21:02:48.582842: Epoch time: 46.17 s
2025-10-13 21:02:49.209484: 
2025-10-13 21:02:49.209825: Epoch 121
2025-10-13 21:02:49.210014: Current learning rate: 0.00228
2025-10-13 21:03:35.426020: Validation loss did not improve from -0.54026. Patience: 81/50
2025-10-13 21:03:35.426738: train_loss -0.7911
2025-10-13 21:03:35.427000: val_loss -0.4353
2025-10-13 21:03:35.427197: Pseudo dice [np.float32(0.6995)]
2025-10-13 21:03:35.427456: Epoch time: 46.22 s
2025-10-13 21:03:36.061255: 
2025-10-13 21:03:36.061496: Epoch 122
2025-10-13 21:03:36.061680: Current learning rate: 0.00221
2025-10-13 21:04:22.282583: Validation loss did not improve from -0.54026. Patience: 82/50
2025-10-13 21:04:22.283257: train_loss -0.7932
2025-10-13 21:04:22.283563: val_loss -0.5031
2025-10-13 21:04:22.283837: Pseudo dice [np.float32(0.7361)]
2025-10-13 21:04:22.284123: Epoch time: 46.22 s
2025-10-13 21:04:22.919508: 
2025-10-13 21:04:22.919844: Epoch 123
2025-10-13 21:04:22.920034: Current learning rate: 0.00214
2025-10-13 21:05:09.102967: Validation loss did not improve from -0.54026. Patience: 83/50
2025-10-13 21:05:09.103408: train_loss -0.7935
2025-10-13 21:05:09.103570: val_loss -0.4841
2025-10-13 21:05:09.103688: Pseudo dice [np.float32(0.7284)]
2025-10-13 21:05:09.103824: Epoch time: 46.18 s
2025-10-13 21:05:09.732912: 
2025-10-13 21:05:09.733258: Epoch 124
2025-10-13 21:05:09.733452: Current learning rate: 0.00207
2025-10-13 21:05:55.969372: Validation loss did not improve from -0.54026. Patience: 84/50
2025-10-13 21:05:55.970435: train_loss -0.7934
2025-10-13 21:05:55.970759: val_loss -0.4822
2025-10-13 21:05:55.971083: Pseudo dice [np.float32(0.7185)]
2025-10-13 21:05:55.971334: Epoch time: 46.24 s
2025-10-13 21:05:57.030565: 
2025-10-13 21:05:57.030927: Epoch 125
2025-10-13 21:05:57.031162: Current learning rate: 0.00199
2025-10-13 21:06:43.267200: Validation loss did not improve from -0.54026. Patience: 85/50
2025-10-13 21:06:43.267603: train_loss -0.7942
2025-10-13 21:06:43.267762: val_loss -0.4518
2025-10-13 21:06:43.267904: Pseudo dice [np.float32(0.7033)]
2025-10-13 21:06:43.268043: Epoch time: 46.24 s
2025-10-13 21:06:43.893930: 
2025-10-13 21:06:43.894222: Epoch 126
2025-10-13 21:06:43.894400: Current learning rate: 0.00192
2025-10-13 21:07:30.116011: Validation loss did not improve from -0.54026. Patience: 86/50
2025-10-13 21:07:30.116592: train_loss -0.797
2025-10-13 21:07:30.116748: val_loss -0.4683
2025-10-13 21:07:30.116912: Pseudo dice [np.float32(0.7125)]
2025-10-13 21:07:30.117109: Epoch time: 46.22 s
2025-10-13 21:07:30.740944: 
2025-10-13 21:07:30.741181: Epoch 127
2025-10-13 21:07:30.741526: Current learning rate: 0.00185
2025-10-13 21:08:16.987261: Validation loss did not improve from -0.54026. Patience: 87/50
2025-10-13 21:08:16.987811: train_loss -0.7974
2025-10-13 21:08:16.987980: val_loss -0.5002
2025-10-13 21:08:16.988114: Pseudo dice [np.float32(0.7376)]
2025-10-13 21:08:16.988252: Epoch time: 46.25 s
2025-10-13 21:08:17.613662: 
2025-10-13 21:08:17.613899: Epoch 128
2025-10-13 21:08:17.614063: Current learning rate: 0.00178
2025-10-13 21:09:03.852913: Validation loss did not improve from -0.54026. Patience: 88/50
2025-10-13 21:09:03.853956: train_loss -0.7973
2025-10-13 21:09:03.854298: val_loss -0.5118
2025-10-13 21:09:03.854588: Pseudo dice [np.float32(0.7408)]
2025-10-13 21:09:03.854921: Epoch time: 46.24 s
2025-10-13 21:09:04.473297: 
2025-10-13 21:09:04.473568: Epoch 129
2025-10-13 21:09:04.473717: Current learning rate: 0.0017
2025-10-13 21:09:50.760971: Validation loss did not improve from -0.54026. Patience: 89/50
2025-10-13 21:09:50.761418: train_loss -0.7948
2025-10-13 21:09:50.761576: val_loss -0.5294
2025-10-13 21:09:50.761688: Pseudo dice [np.float32(0.7508)]
2025-10-13 21:09:50.761831: Epoch time: 46.29 s
2025-10-13 21:09:52.142267: 
2025-10-13 21:09:52.142595: Epoch 130
2025-10-13 21:09:52.142762: Current learning rate: 0.00163
2025-10-13 21:10:38.439239: Validation loss did not improve from -0.54026. Patience: 90/50
2025-10-13 21:10:38.440016: train_loss -0.8
2025-10-13 21:10:38.440300: val_loss -0.4999
2025-10-13 21:10:38.440534: Pseudo dice [np.float32(0.734)]
2025-10-13 21:10:38.440799: Epoch time: 46.3 s
2025-10-13 21:10:39.070145: 
2025-10-13 21:10:39.070441: Epoch 131
2025-10-13 21:10:39.070660: Current learning rate: 0.00156
2025-10-13 21:11:25.354687: Validation loss did not improve from -0.54026. Patience: 91/50
2025-10-13 21:11:25.355087: train_loss -0.7954
2025-10-13 21:11:25.355266: val_loss -0.48
2025-10-13 21:11:25.355376: Pseudo dice [np.float32(0.7194)]
2025-10-13 21:11:25.355508: Epoch time: 46.29 s
2025-10-13 21:11:25.975193: 
2025-10-13 21:11:25.975403: Epoch 132
2025-10-13 21:11:25.975583: Current learning rate: 0.00148
2025-10-13 21:12:12.276914: Validation loss did not improve from -0.54026. Patience: 92/50
2025-10-13 21:12:12.277574: train_loss -0.8006
2025-10-13 21:12:12.277874: val_loss -0.4777
2025-10-13 21:12:12.278182: Pseudo dice [np.float32(0.7306)]
2025-10-13 21:12:12.278527: Epoch time: 46.3 s
2025-10-13 21:12:12.899046: 
2025-10-13 21:12:12.899338: Epoch 133
2025-10-13 21:12:12.899525: Current learning rate: 0.00141
2025-10-13 21:12:59.167859: Validation loss did not improve from -0.54026. Patience: 93/50
2025-10-13 21:12:59.168677: train_loss -0.7992
2025-10-13 21:12:59.168960: val_loss -0.4607
2025-10-13 21:12:59.169244: Pseudo dice [np.float32(0.7157)]
2025-10-13 21:12:59.169508: Epoch time: 46.27 s
2025-10-13 21:12:59.795233: 
2025-10-13 21:12:59.795549: Epoch 134
2025-10-13 21:12:59.795768: Current learning rate: 0.00133
2025-10-13 21:13:46.026280: Validation loss did not improve from -0.54026. Patience: 94/50
2025-10-13 21:13:46.026867: train_loss -0.7984
2025-10-13 21:13:46.027056: val_loss -0.5239
2025-10-13 21:13:46.027207: Pseudo dice [np.float32(0.7434)]
2025-10-13 21:13:46.027358: Epoch time: 46.23 s
2025-10-13 21:13:47.107972: 
2025-10-13 21:13:47.108224: Epoch 135
2025-10-13 21:13:47.108417: Current learning rate: 0.00126
2025-10-13 21:14:33.390198: Validation loss did not improve from -0.54026. Patience: 95/50
2025-10-13 21:14:33.390771: train_loss -0.8017
2025-10-13 21:14:33.391047: val_loss -0.4665
2025-10-13 21:14:33.391294: Pseudo dice [np.float32(0.7236)]
2025-10-13 21:14:33.391485: Epoch time: 46.28 s
2025-10-13 21:14:34.022299: 
2025-10-13 21:14:34.022816: Epoch 136
2025-10-13 21:14:34.023211: Current learning rate: 0.00118
2025-10-13 21:15:20.337293: Validation loss did not improve from -0.54026. Patience: 96/50
2025-10-13 21:15:20.337955: train_loss -0.802
2025-10-13 21:15:20.338162: val_loss -0.4921
2025-10-13 21:15:20.338322: Pseudo dice [np.float32(0.725)]
2025-10-13 21:15:20.338460: Epoch time: 46.32 s
2025-10-13 21:15:20.960485: 
2025-10-13 21:15:20.960807: Epoch 137
2025-10-13 21:15:20.960963: Current learning rate: 0.00111
2025-10-13 21:16:07.240258: Validation loss did not improve from -0.54026. Patience: 97/50
2025-10-13 21:16:07.240696: train_loss -0.802
2025-10-13 21:16:07.240845: val_loss -0.4851
2025-10-13 21:16:07.240955: Pseudo dice [np.float32(0.7354)]
2025-10-13 21:16:07.241096: Epoch time: 46.28 s
2025-10-13 21:16:07.869526: 
2025-10-13 21:16:07.869822: Epoch 138
2025-10-13 21:16:07.869977: Current learning rate: 0.00103
2025-10-13 21:16:54.185891: Validation loss did not improve from -0.54026. Patience: 98/50
2025-10-13 21:16:54.186463: train_loss -0.805
2025-10-13 21:16:54.186597: val_loss -0.4906
2025-10-13 21:16:54.186716: Pseudo dice [np.float32(0.7315)]
2025-10-13 21:16:54.186834: Epoch time: 46.32 s
2025-10-13 21:16:54.814751: 
2025-10-13 21:16:54.815050: Epoch 139
2025-10-13 21:16:54.815242: Current learning rate: 0.00095
2025-10-13 21:17:41.108766: Validation loss did not improve from -0.54026. Patience: 99/50
2025-10-13 21:17:41.109253: train_loss -0.8025
2025-10-13 21:17:41.109388: val_loss -0.4885
2025-10-13 21:17:41.109509: Pseudo dice [np.float32(0.7289)]
2025-10-13 21:17:41.109629: Epoch time: 46.3 s
2025-10-13 21:17:42.175918: 
2025-10-13 21:17:42.176283: Epoch 140
2025-10-13 21:17:42.176549: Current learning rate: 0.00087
2025-10-13 21:18:28.403188: Validation loss did not improve from -0.54026. Patience: 100/50
2025-10-13 21:18:28.404029: train_loss -0.8062
2025-10-13 21:18:28.404364: val_loss -0.4886
2025-10-13 21:18:28.404657: Pseudo dice [np.float32(0.7311)]
2025-10-13 21:18:28.404961: Epoch time: 46.23 s
2025-10-13 21:18:29.322713: 
2025-10-13 21:18:29.322972: Epoch 141
2025-10-13 21:18:29.323164: Current learning rate: 0.00079
2025-10-13 21:19:15.587192: Validation loss did not improve from -0.54026. Patience: 101/50
2025-10-13 21:19:15.587588: train_loss -0.8058
2025-10-13 21:19:15.587783: val_loss -0.4974
2025-10-13 21:19:15.588007: Pseudo dice [np.float32(0.7181)]
2025-10-13 21:19:15.588292: Epoch time: 46.27 s
2025-10-13 21:19:16.218279: 
2025-10-13 21:19:16.218603: Epoch 142
2025-10-13 21:19:16.218823: Current learning rate: 0.00071
2025-10-13 21:20:02.654215: Validation loss did not improve from -0.54026. Patience: 102/50
2025-10-13 21:20:02.655073: train_loss -0.8037
2025-10-13 21:20:02.655339: val_loss -0.514
2025-10-13 21:20:02.655529: Pseudo dice [np.float32(0.74)]
2025-10-13 21:20:02.655740: Epoch time: 46.44 s
2025-10-13 21:20:03.279436: 
2025-10-13 21:20:03.279823: Epoch 143
2025-10-13 21:20:03.280048: Current learning rate: 0.00063
2025-10-13 21:20:49.522557: Validation loss did not improve from -0.54026. Patience: 103/50
2025-10-13 21:20:49.523162: train_loss -0.8075
2025-10-13 21:20:49.523482: val_loss -0.4695
2025-10-13 21:20:49.523746: Pseudo dice [np.float32(0.719)]
2025-10-13 21:20:49.524040: Epoch time: 46.24 s
2025-10-13 21:20:50.152409: 
2025-10-13 21:20:50.152791: Epoch 144
2025-10-13 21:20:50.152982: Current learning rate: 0.00055
2025-10-13 21:21:36.426054: Validation loss did not improve from -0.54026. Patience: 104/50
2025-10-13 21:21:36.426646: train_loss -0.8023
2025-10-13 21:21:36.426790: val_loss -0.486
2025-10-13 21:21:36.426964: Pseudo dice [np.float32(0.7298)]
2025-10-13 21:21:36.427123: Epoch time: 46.27 s
2025-10-13 21:21:37.513873: 
2025-10-13 21:21:37.514383: Epoch 145
2025-10-13 21:21:37.514885: Current learning rate: 0.00047
2025-10-13 21:22:23.801479: Validation loss did not improve from -0.54026. Patience: 105/50
2025-10-13 21:22:23.802533: train_loss -0.8054
2025-10-13 21:22:23.802982: val_loss -0.4936
2025-10-13 21:22:23.803370: Pseudo dice [np.float32(0.7311)]
2025-10-13 21:22:23.803779: Epoch time: 46.29 s
2025-10-13 21:22:24.433919: 
2025-10-13 21:22:24.434252: Epoch 146
2025-10-13 21:22:24.434511: Current learning rate: 0.00038
2025-10-13 21:23:10.714427: Validation loss did not improve from -0.54026. Patience: 106/50
2025-10-13 21:23:10.715128: train_loss -0.8053
2025-10-13 21:23:10.715398: val_loss -0.4759
2025-10-13 21:23:10.715623: Pseudo dice [np.float32(0.7251)]
2025-10-13 21:23:10.716060: Epoch time: 46.28 s
2025-10-13 21:23:11.356903: 
2025-10-13 21:23:11.357177: Epoch 147
2025-10-13 21:23:11.357343: Current learning rate: 0.0003
2025-10-13 21:23:57.645979: Validation loss did not improve from -0.54026. Patience: 107/50
2025-10-13 21:23:57.646483: train_loss -0.8068
2025-10-13 21:23:57.646642: val_loss -0.4725
2025-10-13 21:23:57.646778: Pseudo dice [np.float32(0.7161)]
2025-10-13 21:23:57.646997: Epoch time: 46.29 s
2025-10-13 21:23:58.275307: 
2025-10-13 21:23:58.275618: Epoch 148
2025-10-13 21:23:58.275771: Current learning rate: 0.00021
2025-10-13 21:24:44.551297: Validation loss did not improve from -0.54026. Patience: 108/50
2025-10-13 21:24:44.552626: train_loss -0.8046
2025-10-13 21:24:44.552962: val_loss -0.5041
2025-10-13 21:24:44.553337: Pseudo dice [np.float32(0.7412)]
2025-10-13 21:24:44.553747: Epoch time: 46.28 s
2025-10-13 21:24:45.172614: 
2025-10-13 21:24:45.172884: Epoch 149
2025-10-13 21:24:45.173019: Current learning rate: 0.00011
2025-10-13 21:25:31.443578: Validation loss did not improve from -0.54026. Patience: 109/50
2025-10-13 21:25:31.444014: train_loss -0.8051
2025-10-13 21:25:31.444146: val_loss -0.5039
2025-10-13 21:25:31.444265: Pseudo dice [np.float32(0.7338)]
2025-10-13 21:25:31.444391: Epoch time: 46.27 s
2025-10-13 21:25:32.519562: Training done.
2025-10-13 21:25:32.575006: Using splits from existing split file: /nfs/erelab001/shared/Computational_Group/Naravich/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset307_Sohee_Calcium_OCT_CrossValidation/splits_final_80.json
2025-10-13 21:25:32.576292: The split file contains 5 splits.
2025-10-13 21:25:32.577088: Desired fold for training: 0
2025-10-13 21:25:32.578031: This split has 6 training and 4 validation cases.
2025-10-13 21:25:32.578721: predicting 101-045
2025-10-13 21:25:32.582479: 101-045, shape torch.Size([1, 375, 498, 498]), rank 0
2025-10-13 21:26:10.632176: predicting 701-013
2025-10-13 21:26:10.640486: 701-013, shape torch.Size([1, 375, 498, 498]), rank 0
2025-10-13 21:26:45.067121: predicting 704-003
2025-10-13 21:26:45.076132: 704-003, shape torch.Size([1, 375, 498, 498]), rank 0
2025-10-13 21:27:19.430350: predicting 706-005
2025-10-13 21:27:19.439159: 706-005, shape torch.Size([1, 375, 498, 498]), rank 0
2025-10-13 21:28:06.513856: Validation complete
2025-10-13 21:28:06.514155: Mean Validation Dice:  0.7235873661768847
Finished training fold 0 saving to /nfs/erelab001/shared/Computational_Group/Naravich/datasets/nnUNet_Datasets/nnUNet_results/Dataset307_Sohee_Calcium_OCT_CrossValidation/nnUNetTrainerScaleAnalysis80__nnUNetPlans__3d_32x160x128_b10/fold_0_Genesis_Pretrained
