/home/gridsan/nchutisilp/.conda/envs/nnunet/bin/python
Starting training with CONFIG=3d_32x160x128_b10, DATASET_ID=307, TRAINER=nnUNetTrainer

############################
INFO: You are using the old nnU-Net default plans. We have updated our recommendations. Please consider using those instead! Read more here: https://github.com/MIC-DKFZ/nnUNet/blob/master/documentation/resenc_presets.md
############################

Using device: cuda:0

#######################################################################
Please cite the following paper when using nnU-Net:
Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.
#######################################################################

2024-12-09 13:07:10.060883: Using torch.compile...

############################
INFO: You are using the old nnU-Net default plans. We have updated our recommendations. Please consider using those instead! Read more here: https://github.com/MIC-DKFZ/nnUNet/blob/master/documentation/resenc_presets.md
############################

Using device: cuda:0

#######################################################################
Please cite the following paper when using nnU-Net:
Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.
#######################################################################

2024-12-09 13:07:10.061142: Using torch.compile...
/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
################### Loading pretrained weights from file  /home/gridsan/nchutisilp/datasets/ModelGenesisOutputs/ModelGenesisNNUNetPretrainingV2_noNorm_correct_orientation/Converted_nnUNet_Genesis_OCT_Best.pt ###################
Below is the list of overlapping blocks in pretrained model and nnUNet architecture:
encoder.stages.0.0.convs.0.conv.weight shape torch.Size([32, 1, 3, 3, 3])
encoder.stages.0.0.convs.0.conv.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.norm.weight shape torch.Size([32])
encoder.stages.0.0.convs.0.norm.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.0.weight shape torch.Size([32, 1, 3, 3, 3])
encoder.stages.0.0.convs.0.all_modules.0.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.1.weight shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.1.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
encoder.stages.0.0.convs.1.conv.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.norm.weight shape torch.Size([32])
encoder.stages.0.0.convs.1.norm.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
encoder.stages.0.0.convs.1.all_modules.0.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.1.weight shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.1.bias shape torch.Size([32])
encoder.stages.1.0.convs.0.conv.weight shape torch.Size([64, 32, 3, 3, 3])
encoder.stages.1.0.convs.0.conv.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.norm.weight shape torch.Size([64])
encoder.stages.1.0.convs.0.norm.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.0.weight shape torch.Size([64, 32, 3, 3, 3])
encoder.stages.1.0.convs.0.all_modules.0.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.1.weight shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.1.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
encoder.stages.1.0.convs.1.conv.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.norm.weight shape torch.Size([64])
encoder.stages.1.0.convs.1.norm.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
encoder.stages.1.0.convs.1.all_modules.0.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.1.weight shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.1.bias shape torch.Size([64])
encoder.stages.2.0.convs.0.conv.weight shape torch.Size([128, 64, 3, 3, 3])
encoder.stages.2.0.convs.0.conv.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.norm.weight shape torch.Size([128])
encoder.stages.2.0.convs.0.norm.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.0.weight shape torch.Size([128, 64, 3, 3, 3])
encoder.stages.2.0.convs.0.all_modules.0.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.1.weight shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.1.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
encoder.stages.2.0.convs.1.conv.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.norm.weight shape torch.Size([128])
encoder.stages.2.0.convs.1.norm.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
encoder.stages.2.0.convs.1.all_modules.0.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.1.weight shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.1.bias shape torch.Size([128])
encoder.stages.3.0.convs.0.conv.weight shape torch.Size([256, 128, 3, 3, 3])
encoder.stages.3.0.convs.0.conv.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.norm.weight shape torch.Size([256])
encoder.stages.3.0.convs.0.norm.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.0.weight shape torch.Size([256, 128, 3, 3, 3])
encoder.stages.3.0.convs.0.all_modules.0.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.1.weight shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.1.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
encoder.stages.3.0.convs.1.conv.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.norm.weight shape torch.Size([256])
encoder.stages.3.0.convs.1.norm.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
encoder.stages.3.0.convs.1.all_modules.0.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.1.weight shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.1.bias shape torch.Size([256])
encoder.stages.4.0.convs.0.conv.weight shape torch.Size([320, 256, 3, 3, 3])
encoder.stages.4.0.convs.0.conv.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.norm.weight shape torch.Size([320])
encoder.stages.4.0.convs.0.norm.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.0.weight shape torch.Size([320, 256, 3, 3, 3])
encoder.stages.4.0.convs.0.all_modules.0.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.1.weight shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.1.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.4.0.convs.1.conv.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.norm.weight shape torch.Size([320])
encoder.stages.4.0.convs.1.norm.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.4.0.convs.1.all_modules.0.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.1.weight shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.1.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.0.conv.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.norm.weight shape torch.Size([320])
encoder.stages.5.0.convs.0.norm.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.0.all_modules.0.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.1.weight shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.1.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.1.conv.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.norm.weight shape torch.Size([320])
encoder.stages.5.0.convs.1.norm.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.1.all_modules.0.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.1.weight shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.0.0.convs.0.conv.weight shape torch.Size([32, 1, 3, 3, 3])
decoder.encoder.stages.0.0.convs.0.conv.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.norm.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.norm.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.0.weight shape torch.Size([32, 1, 3, 3, 3])
decoder.encoder.stages.0.0.convs.0.all_modules.0.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.1.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.1.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.encoder.stages.0.0.convs.1.conv.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.norm.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.norm.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.encoder.stages.0.0.convs.1.all_modules.0.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.1.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.1.bias shape torch.Size([32])
decoder.encoder.stages.1.0.convs.0.conv.weight shape torch.Size([64, 32, 3, 3, 3])
decoder.encoder.stages.1.0.convs.0.conv.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.norm.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.norm.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.0.weight shape torch.Size([64, 32, 3, 3, 3])
decoder.encoder.stages.1.0.convs.0.all_modules.0.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.1.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.1.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.encoder.stages.1.0.convs.1.conv.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.norm.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.norm.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.encoder.stages.1.0.convs.1.all_modules.0.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.1.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.1.bias shape torch.Size([64])
decoder.encoder.stages.2.0.convs.0.conv.weight shape torch.Size([128, 64, 3, 3, 3])
decoder.encoder.stages.2.0.convs.0.conv.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.norm.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.norm.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.0.weight shape torch.Size([128, 64, 3, 3, 3])
decoder.encoder.stages.2.0.convs.0.all_modules.0.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.1.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.1.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.encoder.stages.2.0.convs.1.conv.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.norm.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.norm.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.encoder.stages.2.0.convs.1.all_modules.0.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.1.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.1.bias shape torch.Size([128])
decoder.encoder.stages.3.0.convs.0.conv.weight shape torch.Size([256, 128, 3, 3, 3])
decoder.encoder.stages.3.0.convs.0.conv.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.norm.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.norm.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.0.weight shape torch.Size([256, 128, 3, 3, 3])
decoder.encoder.stages.3.0.convs.0.all_modules.0.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.1.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.1.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.encoder.stages.3.0.convs.1.conv.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.norm.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.norm.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.encoder.stages.3.0.convs.1.all_modules.0.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.1.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.1.bias shape torch.Size([256])
decoder.encoder.stages.4.0.convs.0.conv.weight shape torch.Size([320, 256, 3, 3, 3])
decoder.encoder.stages.4.0.convs.0.conv.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.norm.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.norm.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.0.weight shape torch.Size([320, 256, 3, 3, 3])
decoder.encoder.stages.4.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.4.0.convs.1.conv.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.norm.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.norm.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.4.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.0.conv.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.norm.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.norm.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.1.conv.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.norm.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.norm.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.stages.0.convs.0.conv.weight shape torch.Size([320, 640, 3, 3, 3])
decoder.stages.0.convs.0.conv.bias shape torch.Size([320])
decoder.stages.0.convs.0.norm.weight shape torch.Size([320])
decoder.stages.0.convs.0.norm.bias shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.0.weight shape torch.Size([320, 640, 3, 3, 3])
decoder.stages.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.stages.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.stages.0.convs.1.conv.bias shape torch.Size([320])
decoder.stages.0.convs.1.norm.weight shape torch.Size([320])
decoder.stages.0.convs.1.norm.bias shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.stages.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.stages.1.convs.0.conv.weight shape torch.Size([256, 512, 3, 3, 3])
decoder.stages.1.convs.0.conv.bias shape torch.Size([256])
decoder.stages.1.convs.0.norm.weight shape torch.Size([256])
decoder.stages.1.convs.0.norm.bias shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.0.weight shape torch.Size([256, 512, 3, 3, 3])
decoder.stages.1.convs.0.all_modules.0.bias shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.1.weight shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.1.bias shape torch.Size([256])
decoder.stages.1.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.stages.1.convs.1.conv.bias shape torch.Size([256])
decoder.stages.1.convs.1.norm.weight shape torch.Size([256])
decoder.stages.1.convs.1.norm.bias shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.stages.1.convs.1.all_modules.0.bias shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.1.weight shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.1.bias shape torch.Size([256])
decoder.stages.2.convs.0.conv.weight shape torch.Size([128, 256, 3, 3, 3])
decoder.stages.2.convs.0.conv.bias shape torch.Size([128])
decoder.stages.2.convs.0.norm.weight shape torch.Size([128])
decoder.stages.2.convs.0.norm.bias shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.0.weight shape torch.Size([128, 256, 3, 3, 3])
decoder.stages.2.convs.0.all_modules.0.bias shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.1.weight shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.1.bias shape torch.Size([128])
decoder.stages.2.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.stages.2.convs.1.conv.bias shape torch.Size([128])
decoder.stages.2.convs.1.norm.weight shape torch.Size([128])
decoder.stages.2.convs.1.norm.bias shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.stages.2.convs.1.all_modules.0.bias shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.1.weight shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.1.bias shape torch.Size([128])
decoder.stages.3.convs.0.conv.weight shape torch.Size([64, 128, 3, 3, 3])
decoder.stages.3.convs.0.conv.bias shape torch.Size([64])
decoder.stages.3.convs.0.norm.weight shape torch.Size([64])
decoder.stages.3.convs.0.norm.bias shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.0.weight shape torch.Size([64, 128, 3, 3, 3])
decoder.stages.3.convs.0.all_modules.0.bias shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.1.weight shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.1.bias shape torch.Size([64])
decoder.stages.3.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.stages.3.convs.1.conv.bias shape torch.Size([64])
decoder.stages.3.convs.1.norm.weight shape torch.Size([64])
decoder.stages.3.convs.1.norm.bias shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.stages.3.convs.1.all_modules.0.bias shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.1.weight shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.1.bias shape torch.Size([64])
decoder.stages.4.convs.0.conv.weight shape torch.Size([32, 64, 3, 3, 3])
decoder.stages.4.convs.0.conv.bias shape torch.Size([32])
decoder.stages.4.convs.0.norm.weight shape torch.Size([32])
decoder.stages.4.convs.0.norm.bias shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.0.weight shape torch.Size([32, 64, 3, 3, 3])
decoder.stages.4.convs.0.all_modules.0.bias shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.1.weight shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.1.bias shape torch.Size([32])
decoder.stages.4.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.stages.4.convs.1.conv.bias shape torch.Size([32])
decoder.stages.4.convs.1.norm.weight shape torch.Size([32])
decoder.stages.4.convs.1.norm.bias shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.stages.4.convs.1.all_modules.0.bias shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.1.weight shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.1.bias shape torch.Size([32])
decoder.transpconvs.0.weight shape torch.Size([320, 320, 1, 2, 2])
decoder.transpconvs.0.bias shape torch.Size([320])
decoder.transpconvs.1.weight shape torch.Size([320, 256, 2, 2, 2])
decoder.transpconvs.1.bias shape torch.Size([256])
decoder.transpconvs.2.weight shape torch.Size([256, 128, 2, 2, 2])
decoder.transpconvs.2.bias shape torch.Size([128])
decoder.transpconvs.3.weight shape torch.Size([128, 64, 2, 2, 2])
decoder.transpconvs.3.bias shape torch.Size([64])
decoder.transpconvs.4.weight shape torch.Size([64, 32, 2, 2, 2])
decoder.transpconvs.4.bias shape torch.Size([32])
################### Done ###################
2024-12-09 13:07:33.484550: do_dummy_2d_data_aug: True
2024-12-09 13:07:33.487946: Using splits from existing split file: /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset307_Sohee_Calcium_OCT_CrossValidation/splits_final.json
2024-12-09 13:07:33.490325: The split file contains 5 splits.
2024-12-09 13:07:33.491570: Desired fold for training: 2
2024-12-09 13:07:33.493180: This split has 6 training and 2 validation cases.
################### Loading pretrained weights from file  /home/gridsan/nchutisilp/datasets/ModelGenesisOutputs/ModelGenesisNNUNetPretrainingV2_noNorm_correct_orientation/Converted_nnUNet_Genesis_OCT_Best.pt ###################
Below is the list of overlapping blocks in pretrained model and nnUNet architecture:
encoder.stages.0.0.convs.0.conv.weight shape torch.Size([32, 1, 3, 3, 3])
encoder.stages.0.0.convs.0.conv.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.norm.weight shape torch.Size([32])
encoder.stages.0.0.convs.0.norm.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.0.weight shape torch.Size([32, 1, 3, 3, 3])
encoder.stages.0.0.convs.0.all_modules.0.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.1.weight shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.1.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
encoder.stages.0.0.convs.1.conv.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.norm.weight shape torch.Size([32])
encoder.stages.0.0.convs.1.norm.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
encoder.stages.0.0.convs.1.all_modules.0.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.1.weight shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.1.bias shape torch.Size([32])
encoder.stages.1.0.convs.0.conv.weight shape torch.Size([64, 32, 3, 3, 3])
encoder.stages.1.0.convs.0.conv.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.norm.weight shape torch.Size([64])
encoder.stages.1.0.convs.0.norm.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.0.weight shape torch.Size([64, 32, 3, 3, 3])
encoder.stages.1.0.convs.0.all_modules.0.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.1.weight shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.1.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
encoder.stages.1.0.convs.1.conv.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.norm.weight shape torch.Size([64])
encoder.stages.1.0.convs.1.norm.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
encoder.stages.1.0.convs.1.all_modules.0.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.1.weight shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.1.bias shape torch.Size([64])
encoder.stages.2.0.convs.0.conv.weight shape torch.Size([128, 64, 3, 3, 3])
encoder.stages.2.0.convs.0.conv.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.norm.weight shape torch.Size([128])
encoder.stages.2.0.convs.0.norm.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.0.weight shape torch.Size([128, 64, 3, 3, 3])
encoder.stages.2.0.convs.0.all_modules.0.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.1.weight shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.1.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
encoder.stages.2.0.convs.1.conv.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.norm.weight shape torch.Size([128])
encoder.stages.2.0.convs.1.norm.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
encoder.stages.2.0.convs.1.all_modules.0.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.1.weight shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.1.bias shape torch.Size([128])
encoder.stages.3.0.convs.0.conv.weight shape torch.Size([256, 128, 3, 3, 3])
encoder.stages.3.0.convs.0.conv.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.norm.weight shape torch.Size([256])
encoder.stages.3.0.convs.0.norm.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.0.weight shape torch.Size([256, 128, 3, 3, 3])
encoder.stages.3.0.convs.0.all_modules.0.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.1.weight shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.1.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
encoder.stages.3.0.convs.1.conv.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.norm.weight shape torch.Size([256])
encoder.stages.3.0.convs.1.norm.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
encoder.stages.3.0.convs.1.all_modules.0.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.1.weight shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.1.bias shape torch.Size([256])
encoder.stages.4.0.convs.0.conv.weight shape torch.Size([320, 256, 3, 3, 3])
encoder.stages.4.0.convs.0.conv.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.norm.weight shape torch.Size([320])
encoder.stages.4.0.convs.0.norm.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.0.weight shape torch.Size([320, 256, 3, 3, 3])
encoder.stages.4.0.convs.0.all_modules.0.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.1.weight shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.1.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.4.0.convs.1.conv.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.norm.weight shape torch.Size([320])
encoder.stages.4.0.convs.1.norm.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.4.0.convs.1.all_modules.0.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.1.weight shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.1.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.0.conv.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.norm.weight shape torch.Size([320])
encoder.stages.5.0.convs.0.norm.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.0.all_modules.0.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.1.weight shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.1.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.1.conv.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.norm.weight shape torch.Size([320])
encoder.stages.5.0.convs.1.norm.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.1.all_modules.0.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.1.weight shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.0.0.convs.0.conv.weight shape torch.Size([32, 1, 3, 3, 3])
decoder.encoder.stages.0.0.convs.0.conv.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.norm.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.norm.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.0.weight shape torch.Size([32, 1, 3, 3, 3])
decoder.encoder.stages.0.0.convs.0.all_modules.0.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.1.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.1.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.encoder.stages.0.0.convs.1.conv.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.norm.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.norm.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.encoder.stages.0.0.convs.1.all_modules.0.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.1.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.1.bias shape torch.Size([32])
decoder.encoder.stages.1.0.convs.0.conv.weight shape torch.Size([64, 32, 3, 3, 3])
decoder.encoder.stages.1.0.convs.0.conv.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.norm.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.norm.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.0.weight shape torch.Size([64, 32, 3, 3, 3])
decoder.encoder.stages.1.0.convs.0.all_modules.0.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.1.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.1.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.encoder.stages.1.0.convs.1.conv.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.norm.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.norm.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.encoder.stages.1.0.convs.1.all_modules.0.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.1.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.1.bias shape torch.Size([64])
decoder.encoder.stages.2.0.convs.0.conv.weight shape torch.Size([128, 64, 3, 3, 3])
decoder.encoder.stages.2.0.convs.0.conv.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.norm.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.norm.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.0.weight shape torch.Size([128, 64, 3, 3, 3])
decoder.encoder.stages.2.0.convs.0.all_modules.0.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.1.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.1.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.encoder.stages.2.0.convs.1.conv.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.norm.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.norm.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.encoder.stages.2.0.convs.1.all_modules.0.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.1.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.1.bias shape torch.Size([128])
decoder.encoder.stages.3.0.convs.0.conv.weight shape torch.Size([256, 128, 3, 3, 3])
decoder.encoder.stages.3.0.convs.0.conv.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.norm.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.norm.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.0.weight shape torch.Size([256, 128, 3, 3, 3])
decoder.encoder.stages.3.0.convs.0.all_modules.0.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.1.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.1.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.encoder.stages.3.0.convs.1.conv.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.norm.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.norm.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.encoder.stages.3.0.convs.1.all_modules.0.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.1.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.1.bias shape torch.Size([256])
decoder.encoder.stages.4.0.convs.0.conv.weight shape torch.Size([320, 256, 3, 3, 3])
decoder.encoder.stages.4.0.convs.0.conv.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.norm.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.norm.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.0.weight shape torch.Size([320, 256, 3, 3, 3])
decoder.encoder.stages.4.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.4.0.convs.1.conv.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.norm.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.norm.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.4.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.0.conv.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.norm.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.norm.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.1.conv.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.norm.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.norm.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.stages.0.convs.0.conv.weight shape torch.Size([320, 640, 3, 3, 3])
decoder.stages.0.convs.0.conv.bias shape torch.Size([320])
decoder.stages.0.convs.0.norm.weight shape torch.Size([320])
decoder.stages.0.convs.0.norm.bias shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.0.weight shape torch.Size([320, 640, 3, 3, 3])
decoder.stages.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.stages.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.stages.0.convs.1.conv.bias shape torch.Size([320])
decoder.stages.0.convs.1.norm.weight shape torch.Size([320])
decoder.stages.0.convs.1.norm.bias shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.stages.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.stages.1.convs.0.conv.weight shape torch.Size([256, 512, 3, 3, 3])
decoder.stages.1.convs.0.conv.bias shape torch.Size([256])
decoder.stages.1.convs.0.norm.weight shape torch.Size([256])
decoder.stages.1.convs.0.norm.bias shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.0.weight shape torch.Size([256, 512, 3, 3, 3])
decoder.stages.1.convs.0.all_modules.0.bias shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.1.weight shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.1.bias shape torch.Size([256])
decoder.stages.1.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.stages.1.convs.1.conv.bias shape torch.Size([256])
decoder.stages.1.convs.1.norm.weight shape torch.Size([256])
decoder.stages.1.convs.1.norm.bias shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.stages.1.convs.1.all_modules.0.bias shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.1.weight shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.1.bias shape torch.Size([256])
decoder.stages.2.convs.0.conv.weight shape torch.Size([128, 256, 3, 3, 3])
decoder.stages.2.convs.0.conv.bias shape torch.Size([128])
decoder.stages.2.convs.0.norm.weight shape torch.Size([128])
decoder.stages.2.convs.0.norm.bias shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.0.weight shape torch.Size([128, 256, 3, 3, 3])
decoder.stages.2.convs.0.all_modules.0.bias shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.1.weight shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.1.bias shape torch.Size([128])
decoder.stages.2.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.stages.2.convs.1.conv.bias shape torch.Size([128])
decoder.stages.2.convs.1.norm.weight shape torch.Size([128])
decoder.stages.2.convs.1.norm.bias shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.stages.2.convs.1.all_modules.0.bias shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.1.weight shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.1.bias shape torch.Size([128])
decoder.stages.3.convs.0.conv.weight shape torch.Size([64, 128, 3, 3, 3])
decoder.stages.3.convs.0.conv.bias shape torch.Size([64])
decoder.stages.3.convs.0.norm.weight shape torch.Size([64])
decoder.stages.3.convs.0.norm.bias shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.0.weight shape torch.Size([64, 128, 3, 3, 3])
decoder.stages.3.convs.0.all_modules.0.bias shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.1.weight shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.1.bias shape torch.Size([64])
decoder.stages.3.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.stages.3.convs.1.conv.bias shape torch.Size([64])
decoder.stages.3.convs.1.norm.weight shape torch.Size([64])
decoder.stages.3.convs.1.norm.bias shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.stages.3.convs.1.all_modules.0.bias shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.1.weight shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.1.bias shape torch.Size([64])
decoder.stages.4.convs.0.conv.weight shape torch.Size([32, 64, 3, 3, 3])
decoder.stages.4.convs.0.conv.bias shape torch.Size([32])
decoder.stages.4.convs.0.norm.weight shape torch.Size([32])
decoder.stages.4.convs.0.norm.bias shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.0.weight shape torch.Size([32, 64, 3, 3, 3])
decoder.stages.4.convs.0.all_modules.0.bias shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.1.weight shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.1.bias shape torch.Size([32])
decoder.stages.4.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.stages.4.convs.1.conv.bias shape torch.Size([32])
decoder.stages.4.convs.1.norm.weight shape torch.Size([32])
decoder.stages.4.convs.1.norm.bias shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.stages.4.convs.1.all_modules.0.bias shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.1.weight shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.1.bias shape torch.Size([32])
decoder.transpconvs.0.weight shape torch.Size([320, 320, 1, 2, 2])
decoder.transpconvs.0.bias shape torch.Size([320])
decoder.transpconvs.1.weight shape torch.Size([320, 256, 2, 2, 2])
decoder.transpconvs.1.bias shape torch.Size([256])
decoder.transpconvs.2.weight shape torch.Size([256, 128, 2, 2, 2])
decoder.transpconvs.2.bias shape torch.Size([128])
decoder.transpconvs.3.weight shape torch.Size([128, 64, 2, 2, 2])
decoder.transpconvs.3.bias shape torch.Size([64])
decoder.transpconvs.4.weight shape torch.Size([64, 32, 2, 2, 2])
decoder.transpconvs.4.bias shape torch.Size([32])
################### Done ###################
2024-12-09 13:07:33.484550: do_dummy_2d_data_aug: True
2024-12-09 13:07:33.487953: Using splits from existing split file: /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset307_Sohee_Calcium_OCT_CrossValidation/splits_final.json
2024-12-09 13:07:33.490563: The split file contains 5 splits.
2024-12-09 13:07:33.491710: Desired fold for training: 3
2024-12-09 13:07:33.493188: This split has 7 training and 1 validation cases.
using pin_memory on device 0
using pin_memory on device 0
using pin_memory on device 0

This is the configuration used by this training:
Configuration name: 3d_32x160x128_b10
 {'data_identifier': 'nnUNetPlans_3d_fullres', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 10, 'patch_size': [32, 160, 128], 'median_image_size_in_voxels': [375.0, 498.0, 498.0], 'spacing': [1.0, 1.0, 1.0], 'normalization_schemes': ['ZScoreNormalization'], 'use_mask_for_norm': [False], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'architecture': {'network_class_name': 'dynamic_network_architectures.architectures.unet.PlainConvUNet', 'arch_kwargs': {'n_stages': 6, 'features_per_stage': [32, 64, 128, 256, 320, 320], 'conv_op': 'torch.nn.modules.conv.Conv3d', 'kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]], 'strides': [[1, 1, 1], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [1, 2, 2]], 'n_conv_per_stage': [2, 2, 2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2, 2, 2], 'conv_bias': True, 'norm_op': 'torch.nn.modules.instancenorm.InstanceNorm3d', 'norm_op_kwargs': {'eps': 1e-05, 'affine': True}, 'dropout_op': None, 'dropout_op_kwargs': None, 'nonlin': 'torch.nn.LeakyReLU', 'nonlin_kwargs': {'inplace': True}, 'deep_supervision': True}, '_kw_requires_import': ['conv_op', 'norm_op', 'dropout_op', 'nonlin']}, 'batch_dice': True, 'inherits_from': '3d_fullres'} 

These are the global plan.json settings:
 {'dataset_name': 'Dataset307_Sohee_Calcium_OCT_CrossValidation', 'plans_name': 'nnUNetPlans', 'original_median_spacing_after_transp': [1.0, 1.0, 1.0], 'original_median_shape_after_transp': [375, 498, 498], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 0.4977976381778717, 'mean': 0.13507525622844696, 'median': 0.09599608182907104, 'min': 0.0, 'percentile_00_5': 0.014754901640117168, 'percentile_99_5': 0.4977976381778717, 'std': 0.12025152146816254}}} 

2024-12-09 13:07:43.635422: unpacking dataset...
using pin_memory on device 0

This is the configuration used by this training:
Configuration name: 3d_32x160x128_b10
 {'data_identifier': 'nnUNetPlans_3d_fullres', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 10, 'patch_size': [32, 160, 128], 'median_image_size_in_voxels': [375.0, 498.0, 498.0], 'spacing': [1.0, 1.0, 1.0], 'normalization_schemes': ['ZScoreNormalization'], 'use_mask_for_norm': [False], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'architecture': {'network_class_name': 'dynamic_network_architectures.architectures.unet.PlainConvUNet', 'arch_kwargs': {'n_stages': 6, 'features_per_stage': [32, 64, 128, 256, 320, 320], 'conv_op': 'torch.nn.modules.conv.Conv3d', 'kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]], 'strides': [[1, 1, 1], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [1, 2, 2]], 'n_conv_per_stage': [2, 2, 2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2, 2, 2], 'conv_bias': True, 'norm_op': 'torch.nn.modules.instancenorm.InstanceNorm3d', 'norm_op_kwargs': {'eps': 1e-05, 'affine': True}, 'dropout_op': None, 'dropout_op_kwargs': None, 'nonlin': 'torch.nn.LeakyReLU', 'nonlin_kwargs': {'inplace': True}, 'deep_supervision': True}, '_kw_requires_import': ['conv_op', 'norm_op', 'dropout_op', 'nonlin']}, 'batch_dice': True, 'inherits_from': '3d_fullres'} 

These are the global plan.json settings:
 {'dataset_name': 'Dataset307_Sohee_Calcium_OCT_CrossValidation', 'plans_name': 'nnUNetPlans', 'original_median_spacing_after_transp': [1.0, 1.0, 1.0], 'original_median_shape_after_transp': [375, 498, 498], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 0.4977976381778717, 'mean': 0.13507525622844696, 'median': 0.09599608182907104, 'min': 0.0, 'percentile_00_5': 0.014754901640117168, 'percentile_99_5': 0.4977976381778717, 'std': 0.12025152146816254}}} 

2024-12-09 13:07:43.949095: unpacking dataset...
2024-12-09 13:07:49.792165: unpacking done...
2024-12-09 13:07:49.943053: Unable to plot network architecture: nnUNet_compile is enabled!
2024-12-09 13:07:50.367224: 
2024-12-09 13:07:50.368484: Epoch 0
2024-12-09 13:07:50.369429: Current learning rate: 0.01
2024-12-09 13:11:26.346589: Validation loss improved from 1000.00000 to -0.24259! Patience: 0/50
2024-12-09 13:11:26.377494: train_loss -0.1432
2024-12-09 13:11:26.405492: val_loss -0.2426
2024-12-09 13:11:26.406799: Pseudo dice [0.5854]
2024-12-09 13:11:26.407780: Epoch time: 215.98 s
2024-12-09 13:11:26.408532: Yayy! New best EMA pseudo Dice: 0.5854
2024-12-09 13:11:29.526838: 
2024-12-09 13:11:29.528988: Epoch 1
2024-12-09 13:11:29.529805: Current learning rate: 0.00999
2024-12-09 13:12:56.873777: Validation loss improved from -0.24259 to -0.26127! Patience: 0/50
2024-12-09 13:12:56.874743: train_loss -0.3066
2024-12-09 13:12:56.875457: val_loss -0.2613
2024-12-09 13:12:56.876243: Pseudo dice [0.5684]
2024-12-09 13:12:56.877004: Epoch time: 87.35 s
2024-12-09 13:12:58.131108: 
2024-12-09 13:12:58.133091: Epoch 2
2024-12-09 13:12:58.133953: Current learning rate: 0.00998
2024-12-09 13:14:25.840498: Validation loss improved from -0.26127 to -0.31786! Patience: 0/50
2024-12-09 13:14:25.841594: train_loss -0.353
2024-12-09 13:14:25.842482: val_loss -0.3179
2024-12-09 13:14:25.843263: Pseudo dice [0.6292]
2024-12-09 13:14:25.844021: Epoch time: 87.71 s
2024-12-09 13:14:25.844800: Yayy! New best EMA pseudo Dice: 0.5882
2024-12-09 13:14:27.604501: 
2024-12-09 13:14:27.606229: Epoch 3
2024-12-09 13:14:27.607035: Current learning rate: 0.00997
2024-12-09 13:15:55.494692: Validation loss did not improve from -0.31786. Patience: 1/50
2024-12-09 13:15:55.495918: train_loss -0.3897
2024-12-09 13:15:55.496781: val_loss -0.2942
2024-12-09 13:15:55.497590: Pseudo dice [0.5978]
2024-12-09 13:15:55.498239: Epoch time: 87.89 s
2024-12-09 13:15:55.498842: Yayy! New best EMA pseudo Dice: 0.5892
2024-12-09 13:15:57.174762: 
2024-12-09 13:15:57.176142: Epoch 4
2024-12-09 13:15:57.176995: Current learning rate: 0.00996
2024-12-09 13:17:25.002040: Validation loss improved from -0.31786 to -0.33905! Patience: 1/50
2024-12-09 13:17:25.003122: train_loss -0.426
2024-12-09 13:17:25.003867: val_loss -0.3391
2024-12-09 13:17:25.004637: Pseudo dice [0.6269]
2024-12-09 13:17:25.005308: Epoch time: 87.83 s
2024-12-09 13:17:25.352763: Yayy! New best EMA pseudo Dice: 0.593
2024-12-09 13:17:27.017390: 
2024-12-09 13:17:27.019136: Epoch 5
2024-12-09 13:17:27.020287: Current learning rate: 0.00995
2024-12-09 13:18:54.841599: Validation loss did not improve from -0.33905. Patience: 1/50
2024-12-09 13:18:54.842793: train_loss -0.4566
2024-12-09 13:18:54.843697: val_loss -0.3265
2024-12-09 13:18:54.844380: Pseudo dice [0.613]
2024-12-09 13:18:54.845119: Epoch time: 87.83 s
2024-12-09 13:18:54.845757: Yayy! New best EMA pseudo Dice: 0.595
2024-12-09 13:18:56.456471: 
2024-12-09 13:18:56.458500: Epoch 6
2024-12-09 13:18:56.459449: Current learning rate: 0.00995
2024-12-09 13:20:24.301175: Validation loss did not improve from -0.33905. Patience: 2/50
2024-12-09 13:20:24.302098: train_loss -0.4583
2024-12-09 13:20:24.302866: val_loss -0.3289
2024-12-09 13:20:24.303611: Pseudo dice [0.6326]
2024-12-09 13:20:24.304290: Epoch time: 87.85 s
2024-12-09 13:20:24.304985: Yayy! New best EMA pseudo Dice: 0.5987
2024-12-09 13:20:25.913009: 
2024-12-09 13:20:25.913851: Epoch 7
2024-12-09 13:20:25.914625: Current learning rate: 0.00994
2024-12-09 13:21:53.803866: Validation loss did not improve from -0.33905. Patience: 3/50
2024-12-09 13:21:53.805074: train_loss -0.4707
2024-12-09 13:21:53.805931: val_loss -0.2109
2024-12-09 13:21:53.806603: Pseudo dice [0.5574]
2024-12-09 13:21:53.807283: Epoch time: 87.89 s
2024-12-09 13:21:55.475300: 
2024-12-09 13:21:55.476794: Epoch 8
2024-12-09 13:21:55.477799: Current learning rate: 0.00993
2024-12-09 13:23:23.610368: Validation loss improved from -0.33905 to -0.33917! Patience: 3/50
2024-12-09 13:23:23.611821: train_loss -0.4849
2024-12-09 13:23:23.613075: val_loss -0.3392
2024-12-09 13:23:23.613938: Pseudo dice [0.6313]
2024-12-09 13:23:23.615065: Epoch time: 88.14 s
2024-12-09 13:23:24.984585: 
2024-12-09 13:23:24.986160: Epoch 9
2024-12-09 13:23:24.987022: Current learning rate: 0.00992
2024-12-09 13:24:53.128840: Validation loss improved from -0.33917 to -0.36823! Patience: 0/50
2024-12-09 13:24:53.129982: train_loss -0.5056
2024-12-09 13:24:53.130714: val_loss -0.3682
2024-12-09 13:24:53.131346: Pseudo dice [0.648]
2024-12-09 13:24:53.132119: Epoch time: 88.15 s
2024-12-09 13:24:53.517135: Yayy! New best EMA pseudo Dice: 0.6032
2024-12-09 13:24:55.120984: 
2024-12-09 13:24:55.122232: Epoch 10
2024-12-09 13:24:55.122944: Current learning rate: 0.00991
2024-12-09 13:26:23.270759: Validation loss improved from -0.36823 to -0.38043! Patience: 0/50
2024-12-09 13:26:23.271812: train_loss -0.5054
2024-12-09 13:26:23.272655: val_loss -0.3804
2024-12-09 13:26:23.273380: Pseudo dice [0.6659]
2024-12-09 13:26:23.274001: Epoch time: 88.15 s
2024-12-09 13:26:23.274680: Yayy! New best EMA pseudo Dice: 0.6095
2024-12-09 13:26:24.904879: 
2024-12-09 13:26:24.906451: Epoch 11
2024-12-09 13:26:24.907354: Current learning rate: 0.0099
2024-12-09 13:27:52.939328: Validation loss did not improve from -0.38043. Patience: 1/50
2024-12-09 13:27:52.940271: train_loss -0.5265
2024-12-09 13:27:52.941094: val_loss -0.2968
2024-12-09 13:27:52.941762: Pseudo dice [0.5876]
2024-12-09 13:27:52.942364: Epoch time: 88.04 s
2024-12-09 13:27:54.244020: 
2024-12-09 13:27:54.245478: Epoch 12
2024-12-09 13:27:54.246199: Current learning rate: 0.00989
2024-12-09 13:29:22.314327: Validation loss did not improve from -0.38043. Patience: 2/50
2024-12-09 13:29:22.315423: train_loss -0.5368
2024-12-09 13:29:22.316388: val_loss -0.3356
2024-12-09 13:29:22.317124: Pseudo dice [0.639]
2024-12-09 13:29:22.318016: Epoch time: 88.07 s
2024-12-09 13:29:22.318795: Yayy! New best EMA pseudo Dice: 0.6105
2024-12-09 13:29:23.916777: 
2024-12-09 13:29:23.918302: Epoch 13
2024-12-09 13:29:23.919366: Current learning rate: 0.00988
2024-12-09 13:30:51.905090: Validation loss improved from -0.38043 to -0.40094! Patience: 2/50
2024-12-09 13:30:51.906148: train_loss -0.5457
2024-12-09 13:30:51.907468: val_loss -0.4009
2024-12-09 13:30:51.908273: Pseudo dice [0.6712]
2024-12-09 13:30:51.909003: Epoch time: 87.99 s
2024-12-09 13:30:51.909729: Yayy! New best EMA pseudo Dice: 0.6166
2024-12-09 13:30:53.584645: 
2024-12-09 13:30:53.586414: Epoch 14
2024-12-09 13:30:53.587320: Current learning rate: 0.00987
2024-12-09 13:32:21.592662: Validation loss improved from -0.40094 to -0.40910! Patience: 0/50
2024-12-09 13:32:21.593752: train_loss -0.5388
2024-12-09 13:32:21.594588: val_loss -0.4091
2024-12-09 13:32:21.595336: Pseudo dice [0.6823]
2024-12-09 13:32:21.596098: Epoch time: 88.01 s
2024-12-09 13:32:21.961107: Yayy! New best EMA pseudo Dice: 0.6231
2024-12-09 13:32:23.579275: 
2024-12-09 13:32:23.580865: Epoch 15
2024-12-09 13:32:23.581468: Current learning rate: 0.00986
2024-12-09 13:33:51.521484: Validation loss did not improve from -0.40910. Patience: 1/50
2024-12-09 13:33:51.522779: train_loss -0.5604
2024-12-09 13:33:51.523687: val_loss -0.3763
2024-12-09 13:33:51.524557: Pseudo dice [0.676]
2024-12-09 13:33:51.525537: Epoch time: 87.94 s
2024-12-09 13:33:51.526289: Yayy! New best EMA pseudo Dice: 0.6284
2024-12-09 13:33:53.135467: 
2024-12-09 13:33:53.136692: Epoch 16
2024-12-09 13:33:53.137709: Current learning rate: 0.00986
2024-12-09 13:35:21.439272: Validation loss did not improve from -0.40910. Patience: 2/50
2024-12-09 13:35:21.440344: train_loss -0.5543
2024-12-09 13:35:21.441220: val_loss -0.3715
2024-12-09 13:35:21.441910: Pseudo dice [0.6551]
2024-12-09 13:35:21.442659: Epoch time: 88.31 s
2024-12-09 13:35:21.443383: Yayy! New best EMA pseudo Dice: 0.6311
2024-12-09 13:35:23.080204: 
2024-12-09 13:35:23.082120: Epoch 17
2024-12-09 13:35:23.083182: Current learning rate: 0.00985
2024-12-09 13:36:51.387580: Validation loss did not improve from -0.40910. Patience: 3/50
2024-12-09 13:36:51.388610: train_loss -0.5623
2024-12-09 13:36:51.389667: val_loss -0.3838
2024-12-09 13:36:51.390508: Pseudo dice [0.652]
2024-12-09 13:36:51.391274: Epoch time: 88.31 s
2024-12-09 13:36:51.391905: Yayy! New best EMA pseudo Dice: 0.6332
2024-12-09 13:36:53.389931: 
2024-12-09 13:36:53.391625: Epoch 18
2024-12-09 13:36:53.392432: Current learning rate: 0.00984
2024-12-09 13:38:21.778231: Validation loss did not improve from -0.40910. Patience: 4/50
2024-12-09 13:38:21.779381: train_loss -0.5658
2024-12-09 13:38:21.780278: val_loss -0.1439
2024-12-09 13:38:21.781002: Pseudo dice [0.5044]
2024-12-09 13:38:21.781665: Epoch time: 88.39 s
2024-12-09 13:38:23.107833: 
2024-12-09 13:38:23.108694: Epoch 19
2024-12-09 13:38:23.109273: Current learning rate: 0.00983
2024-12-09 13:39:51.427589: Validation loss did not improve from -0.40910. Patience: 5/50
2024-12-09 13:39:51.428705: train_loss -0.567
2024-12-09 13:39:51.429524: val_loss -0.349
2024-12-09 13:39:51.430305: Pseudo dice [0.6264]
2024-12-09 13:39:51.431137: Epoch time: 88.32 s
2024-12-09 13:39:53.147360: 
2024-12-09 13:39:53.149215: Epoch 20
2024-12-09 13:39:53.150522: Current learning rate: 0.00982
2024-12-09 13:41:21.416170: Validation loss improved from -0.40910 to -0.45401! Patience: 5/50
2024-12-09 13:41:21.417096: train_loss -0.5679
2024-12-09 13:41:21.417987: val_loss -0.454
2024-12-09 13:41:21.418764: Pseudo dice [0.7132]
2024-12-09 13:41:21.419519: Epoch time: 88.27 s
2024-12-09 13:41:22.757407: 
2024-12-09 13:41:22.759011: Epoch 21
2024-12-09 13:41:22.759769: Current learning rate: 0.00981
2024-12-09 13:42:51.014036: Validation loss did not improve from -0.45401. Patience: 1/50
2024-12-09 13:42:51.015324: train_loss -0.5687
2024-12-09 13:42:51.016363: val_loss -0.2867
2024-12-09 13:42:51.017054: Pseudo dice [0.5996]
2024-12-09 13:42:51.017883: Epoch time: 88.26 s
2024-12-09 13:42:52.282201: 
2024-12-09 13:42:52.284027: Epoch 22
2024-12-09 13:42:52.284914: Current learning rate: 0.0098
2024-12-09 13:44:20.584861: Validation loss did not improve from -0.45401. Patience: 2/50
2024-12-09 13:44:20.585942: train_loss -0.5835
2024-12-09 13:44:20.587000: val_loss -0.1712
2024-12-09 13:44:20.587955: Pseudo dice [0.5464]
2024-12-09 13:44:20.588749: Epoch time: 88.3 s
2024-12-09 13:44:21.857713: 
2024-12-09 13:44:21.859328: Epoch 23
2024-12-09 13:44:21.860317: Current learning rate: 0.00979
2024-12-09 13:45:50.140041: Validation loss did not improve from -0.45401. Patience: 3/50
2024-12-09 13:45:50.140963: train_loss -0.5891
2024-12-09 13:45:50.141876: val_loss -0.3695
2024-12-09 13:45:50.142579: Pseudo dice [0.6239]
2024-12-09 13:45:50.143336: Epoch time: 88.28 s
2024-12-09 13:45:51.443954: 
2024-12-09 13:45:51.445515: Epoch 24
2024-12-09 13:45:51.446228: Current learning rate: 0.00978
2024-12-09 13:47:19.965259: Validation loss did not improve from -0.45401. Patience: 4/50
2024-12-09 13:47:19.966184: train_loss -0.577
2024-12-09 13:47:19.966910: val_loss -0.3472
2024-12-09 13:47:19.967609: Pseudo dice [0.6359]
2024-12-09 13:47:19.968322: Epoch time: 88.52 s
2024-12-09 13:47:21.609396: 
2024-12-09 13:47:21.610693: Epoch 25
2024-12-09 13:47:21.611398: Current learning rate: 0.00977
2024-12-09 13:48:49.974061: Validation loss did not improve from -0.45401. Patience: 5/50
2024-12-09 13:48:49.975227: train_loss -0.5838
2024-12-09 13:48:49.976071: val_loss -0.2941
2024-12-09 13:48:49.976886: Pseudo dice [0.5982]
2024-12-09 13:48:49.977698: Epoch time: 88.37 s
2024-12-09 13:48:51.226387: 
2024-12-09 13:48:51.228093: Epoch 26
2024-12-09 13:48:51.228906: Current learning rate: 0.00977
2024-12-09 13:50:19.430779: Validation loss did not improve from -0.45401. Patience: 6/50
2024-12-09 13:50:19.431634: train_loss -0.6011
2024-12-09 13:50:19.432624: val_loss -0.3843
2024-12-09 13:50:19.433402: Pseudo dice [0.6576]
2024-12-09 13:50:19.434100: Epoch time: 88.21 s
2024-12-09 13:50:20.704546: 
2024-12-09 13:50:20.705829: Epoch 27
2024-12-09 13:50:20.706617: Current learning rate: 0.00976
2024-12-09 13:51:49.096585: Validation loss did not improve from -0.45401. Patience: 7/50
2024-12-09 13:51:49.097661: train_loss -0.5975
2024-12-09 13:51:49.098552: val_loss -0.336
2024-12-09 13:51:49.099473: Pseudo dice [0.6382]
2024-12-09 13:51:49.100291: Epoch time: 88.39 s
2024-12-09 13:51:50.375256: 
2024-12-09 13:51:50.376944: Epoch 28
2024-12-09 13:51:50.377643: Current learning rate: 0.00975
2024-12-09 13:53:18.738118: Validation loss did not improve from -0.45401. Patience: 8/50
2024-12-09 13:53:18.739297: train_loss -0.5946
2024-12-09 13:53:18.740466: val_loss -0.4497
2024-12-09 13:53:18.741152: Pseudo dice [0.7037]
2024-12-09 13:53:18.741860: Epoch time: 88.37 s
2024-12-09 13:53:19.993140: 
2024-12-09 13:53:19.994773: Epoch 29
2024-12-09 13:53:19.995704: Current learning rate: 0.00974
2024-12-09 13:54:48.318983: Validation loss did not improve from -0.45401. Patience: 9/50
2024-12-09 13:54:48.319909: train_loss -0.6074
2024-12-09 13:54:48.320588: val_loss -0.235
2024-12-09 13:54:48.321331: Pseudo dice [0.5746]
2024-12-09 13:54:48.321957: Epoch time: 88.33 s
2024-12-09 13:54:50.281237: 
2024-12-09 13:54:50.282823: Epoch 30
2024-12-09 13:54:50.283588: Current learning rate: 0.00973
2024-12-09 13:56:18.497096: Validation loss did not improve from -0.45401. Patience: 10/50
2024-12-09 13:56:18.498083: train_loss -0.5996
2024-12-09 13:56:18.498892: val_loss -0.3549
2024-12-09 13:56:18.499605: Pseudo dice [0.6461]
2024-12-09 13:56:18.500196: Epoch time: 88.22 s
2024-12-09 13:56:19.750942: 
2024-12-09 13:56:19.753309: Epoch 31
2024-12-09 13:56:19.754267: Current learning rate: 0.00972
2024-12-09 13:57:47.955484: Validation loss did not improve from -0.45401. Patience: 11/50
2024-12-09 13:57:47.956599: train_loss -0.6094
2024-12-09 13:57:47.957509: val_loss -0.3984
2024-12-09 13:57:47.958277: Pseudo dice [0.6657]
2024-12-09 13:57:47.958994: Epoch time: 88.21 s
2024-12-09 13:57:49.233258: 
2024-12-09 13:57:49.234698: Epoch 32
2024-12-09 13:57:49.235777: Current learning rate: 0.00971
2024-12-09 13:59:17.408654: Validation loss did not improve from -0.45401. Patience: 12/50
2024-12-09 13:59:17.409614: train_loss -0.6129
2024-12-09 13:59:17.410551: val_loss -0.3255
2024-12-09 13:59:17.411235: Pseudo dice [0.6287]
2024-12-09 13:59:17.412047: Epoch time: 88.18 s
2024-12-09 13:59:18.737765: 
2024-12-09 13:59:18.739252: Epoch 33
2024-12-09 13:59:18.740272: Current learning rate: 0.0097
2024-12-09 14:00:46.659211: Validation loss did not improve from -0.45401. Patience: 13/50
2024-12-09 14:00:46.660286: train_loss -0.6146
2024-12-09 14:00:46.661136: val_loss -0.3903
2024-12-09 14:00:46.661913: Pseudo dice [0.6602]
2024-12-09 14:00:46.662515: Epoch time: 87.92 s
2024-12-09 14:00:46.663275: Yayy! New best EMA pseudo Dice: 0.6346
2024-12-09 14:00:48.297928: 
2024-12-09 14:00:48.299409: Epoch 34
2024-12-09 14:00:48.300093: Current learning rate: 0.00969
2024-12-09 14:02:16.286944: Validation loss did not improve from -0.45401. Patience: 14/50
2024-12-09 14:02:16.288095: train_loss -0.6064
2024-12-09 14:02:16.288838: val_loss -0.3196
2024-12-09 14:02:16.289413: Pseudo dice [0.6369]
2024-12-09 14:02:16.290090: Epoch time: 87.99 s
2024-12-09 14:02:16.640806: Yayy! New best EMA pseudo Dice: 0.6349
2024-12-09 14:02:18.368142: 
2024-12-09 14:02:18.369937: Epoch 35
2024-12-09 14:02:18.370746: Current learning rate: 0.00968
2024-12-09 14:03:46.348843: Validation loss did not improve from -0.45401. Patience: 15/50
2024-12-09 14:03:46.349725: train_loss -0.6189
2024-12-09 14:03:46.350420: val_loss -0.3934
2024-12-09 14:03:46.351173: Pseudo dice [0.6597]
2024-12-09 14:03:46.351813: Epoch time: 87.98 s
2024-12-09 14:03:46.352401: Yayy! New best EMA pseudo Dice: 0.6374
2024-12-09 14:03:47.967215: 
2024-12-09 14:03:47.968572: Epoch 36
2024-12-09 14:03:47.969291: Current learning rate: 0.00968
2024-12-09 14:05:15.961033: Validation loss did not improve from -0.45401. Patience: 16/50
2024-12-09 14:05:15.962206: train_loss -0.6112
2024-12-09 14:05:15.963201: val_loss -0.4026
2024-12-09 14:05:15.964138: Pseudo dice [0.6872]
2024-12-09 14:05:15.965123: Epoch time: 88.0 s
2024-12-09 14:05:15.966000: Yayy! New best EMA pseudo Dice: 0.6423
2024-12-09 14:05:17.666476: 
2024-12-09 14:05:17.667914: Epoch 37
2024-12-09 14:05:17.669014: Current learning rate: 0.00967
2024-12-09 14:06:45.776851: Validation loss did not improve from -0.45401. Patience: 17/50
2024-12-09 14:06:45.778108: train_loss -0.619
2024-12-09 14:06:45.778829: val_loss -0.3778
2024-12-09 14:06:45.779508: Pseudo dice [0.6632]
2024-12-09 14:06:45.780220: Epoch time: 88.11 s
2024-12-09 14:06:45.780998: Yayy! New best EMA pseudo Dice: 0.6444
2024-12-09 14:06:47.418150: 
2024-12-09 14:06:47.419781: Epoch 38
2024-12-09 14:06:47.420713: Current learning rate: 0.00966
2024-12-09 14:08:15.553493: Validation loss did not improve from -0.45401. Patience: 18/50
2024-12-09 14:08:15.554308: train_loss -0.6225
2024-12-09 14:08:15.555215: val_loss -0.3625
2024-12-09 14:08:15.555931: Pseudo dice [0.6479]
2024-12-09 14:08:15.556686: Epoch time: 88.14 s
2024-12-09 14:08:15.557456: Yayy! New best EMA pseudo Dice: 0.6448
2024-12-09 14:08:17.236151: 
2024-12-09 14:08:17.237391: Epoch 39
2024-12-09 14:08:17.238198: Current learning rate: 0.00965
2024-12-09 14:09:45.285216: Validation loss did not improve from -0.45401. Patience: 19/50
2024-12-09 14:09:45.286266: train_loss -0.64
2024-12-09 14:09:45.287187: val_loss -0.3898
2024-12-09 14:09:45.287928: Pseudo dice [0.6826]
2024-12-09 14:09:45.288576: Epoch time: 88.05 s
2024-12-09 14:09:45.635414: Yayy! New best EMA pseudo Dice: 0.6486
2024-12-09 14:09:47.689819: 
2024-12-09 14:09:47.691360: Epoch 40
2024-12-09 14:09:47.692062: Current learning rate: 0.00964
2024-12-09 14:11:15.753362: Validation loss did not improve from -0.45401. Patience: 20/50
2024-12-09 14:11:15.754385: train_loss -0.6246
2024-12-09 14:11:15.755396: val_loss -0.3483
2024-12-09 14:11:15.756140: Pseudo dice [0.6452]
2024-12-09 14:11:15.756942: Epoch time: 88.07 s
2024-12-09 14:11:17.118670: 
2024-12-09 14:11:17.120227: Epoch 41
2024-12-09 14:11:17.121198: Current learning rate: 0.00963
2024-12-09 14:12:45.448534: Validation loss did not improve from -0.45401. Patience: 21/50
2024-12-09 14:12:45.449822: train_loss -0.6343
2024-12-09 14:12:45.450598: val_loss -0.4241
2024-12-09 14:12:45.451260: Pseudo dice [0.7029]
2024-12-09 14:12:45.451896: Epoch time: 88.33 s
2024-12-09 14:12:45.452519: Yayy! New best EMA pseudo Dice: 0.6537
2024-12-09 14:12:47.042377: 
2024-12-09 14:12:47.047755: Epoch 42
2024-12-09 14:12:47.048510: Current learning rate: 0.00962
2024-12-09 14:14:15.261875: Validation loss did not improve from -0.45401. Patience: 22/50
2024-12-09 14:14:15.263684: train_loss -0.6419
2024-12-09 14:14:15.264849: val_loss -0.3607
2024-12-09 14:14:15.265472: Pseudo dice [0.6547]
2024-12-09 14:14:15.266073: Epoch time: 88.22 s
2024-12-09 14:14:15.266692: Yayy! New best EMA pseudo Dice: 0.6538
2024-12-09 14:14:17.181581: 
2024-12-09 14:14:17.194645: Epoch 43
2024-12-09 14:14:17.196068: Current learning rate: 0.00961
2024-12-09 14:15:45.416950: Validation loss did not improve from -0.45401. Patience: 23/50
2024-12-09 14:15:45.418041: train_loss -0.6447
2024-12-09 14:15:45.418916: val_loss -0.3006
2024-12-09 14:15:45.419894: Pseudo dice [0.6275]
2024-12-09 14:15:45.420664: Epoch time: 88.24 s
2024-12-09 14:15:46.733657: 
2024-12-09 14:15:46.735322: Epoch 44
2024-12-09 14:15:46.736292: Current learning rate: 0.0096
2024-12-09 14:17:15.011109: Validation loss did not improve from -0.45401. Patience: 24/50
2024-12-09 14:17:15.012807: train_loss -0.6431
2024-12-09 14:17:15.015169: val_loss -0.3205
2024-12-09 14:17:15.016146: Pseudo dice [0.6268]
2024-12-09 14:17:15.017503: Epoch time: 88.28 s
2024-12-09 14:17:16.777397: 
2024-12-09 14:17:16.778269: Epoch 45
2024-12-09 14:17:16.778890: Current learning rate: 0.00959
2024-12-09 14:18:44.758107: Validation loss did not improve from -0.45401. Patience: 25/50
2024-12-09 14:18:44.759344: train_loss -0.6421
2024-12-09 14:18:44.760296: val_loss -0.2337
2024-12-09 14:18:44.761024: Pseudo dice [0.5718]
2024-12-09 14:18:44.761919: Epoch time: 87.98 s
2024-12-09 14:18:46.059644: 
2024-12-09 14:18:46.061024: Epoch 46
2024-12-09 14:18:46.062217: Current learning rate: 0.00959
2024-12-09 14:20:14.270041: Validation loss did not improve from -0.45401. Patience: 26/50
2024-12-09 14:20:14.271036: train_loss -0.6431
2024-12-09 14:20:14.272071: val_loss -0.3897
2024-12-09 14:20:14.272966: Pseudo dice [0.6502]
2024-12-09 14:20:14.273762: Epoch time: 88.21 s
2024-12-09 14:20:15.541235: 
2024-12-09 14:20:15.545071: Epoch 47
2024-12-09 14:20:15.546079: Current learning rate: 0.00958
2024-12-09 14:21:43.624511: Validation loss did not improve from -0.45401. Patience: 27/50
2024-12-09 14:21:43.625614: train_loss -0.6368
2024-12-09 14:21:43.626558: val_loss -0.4107
2024-12-09 14:21:43.627445: Pseudo dice [0.6794]
2024-12-09 14:21:43.628220: Epoch time: 88.09 s
2024-12-09 14:21:44.880945: 
2024-12-09 14:21:44.882550: Epoch 48
2024-12-09 14:21:44.883707: Current learning rate: 0.00957
2024-12-09 14:23:12.965412: Validation loss improved from -0.45401 to -0.47301! Patience: 27/50
2024-12-09 14:23:12.966554: train_loss -0.6435
2024-12-09 14:23:12.967668: val_loss -0.473
2024-12-09 14:23:12.968590: Pseudo dice [0.7228]
2024-12-09 14:23:12.969583: Epoch time: 88.09 s
2024-12-09 14:23:14.247035: 
2024-12-09 14:23:14.248225: Epoch 49
2024-12-09 14:23:14.249170: Current learning rate: 0.00956
2024-12-09 14:24:42.406302: Validation loss did not improve from -0.47301. Patience: 1/50
2024-12-09 14:24:42.407432: train_loss -0.6469
2024-12-09 14:24:42.408372: val_loss -0.4379
2024-12-09 14:24:42.409095: Pseudo dice [0.7072]
2024-12-09 14:24:42.409745: Epoch time: 88.16 s
2024-12-09 14:24:42.783398: Yayy! New best EMA pseudo Dice: 0.6588
2024-12-09 14:24:44.379223: 
2024-12-09 14:24:44.380560: Epoch 50
2024-12-09 14:24:44.381488: Current learning rate: 0.00955
2024-12-09 14:26:12.490363: Validation loss did not improve from -0.47301. Patience: 2/50
2024-12-09 14:26:12.491409: train_loss -0.6452
2024-12-09 14:26:12.492176: val_loss -0.4239
2024-12-09 14:26:12.492772: Pseudo dice [0.6857]
2024-12-09 14:26:12.493505: Epoch time: 88.11 s
2024-12-09 14:26:12.494218: Yayy! New best EMA pseudo Dice: 0.6615
2024-12-09 14:26:14.463426: 
2024-12-09 14:26:14.464653: Epoch 51
2024-12-09 14:26:14.465433: Current learning rate: 0.00954
2024-12-09 14:27:42.514955: Validation loss did not improve from -0.47301. Patience: 3/50
2024-12-09 14:27:42.515996: train_loss -0.6596
2024-12-09 14:27:42.517091: val_loss -0.4658
2024-12-09 14:27:42.518153: Pseudo dice [0.7092]
2024-12-09 14:27:42.519114: Epoch time: 88.05 s
2024-12-09 14:27:42.519893: Yayy! New best EMA pseudo Dice: 0.6662
2024-12-09 14:27:44.116829: 
2024-12-09 14:27:44.118305: Epoch 52
2024-12-09 14:27:44.119040: Current learning rate: 0.00953
2024-12-09 14:29:12.389192: Validation loss did not improve from -0.47301. Patience: 4/50
2024-12-09 14:29:12.390417: train_loss -0.6502
2024-12-09 14:29:12.391419: val_loss -0.3188
2024-12-09 14:29:12.392325: Pseudo dice [0.6124]
2024-12-09 14:29:12.393226: Epoch time: 88.27 s
2024-12-09 14:29:13.646488: 
2024-12-09 14:29:13.647919: Epoch 53
2024-12-09 14:29:13.648636: Current learning rate: 0.00952
2024-12-09 14:30:41.976738: Validation loss did not improve from -0.47301. Patience: 5/50
2024-12-09 14:30:41.977915: train_loss -0.6483
2024-12-09 14:30:41.978945: val_loss -0.3403
2024-12-09 14:30:41.979569: Pseudo dice [0.6316]
2024-12-09 14:30:41.980215: Epoch time: 88.33 s
2024-12-09 14:30:43.230499: 
2024-12-09 14:30:43.231962: Epoch 54
2024-12-09 14:30:43.232888: Current learning rate: 0.00951
2024-12-09 14:32:11.504210: Validation loss did not improve from -0.47301. Patience: 6/50
2024-12-09 14:32:11.505261: train_loss -0.655
2024-12-09 14:32:11.505928: val_loss -0.4015
2024-12-09 14:32:11.506551: Pseudo dice [0.6684]
2024-12-09 14:32:11.507297: Epoch time: 88.28 s
2024-12-09 14:32:13.130459: 
2024-12-09 14:32:13.131635: Epoch 55
2024-12-09 14:32:13.132281: Current learning rate: 0.0095
2024-12-09 14:33:41.314929: Validation loss did not improve from -0.47301. Patience: 7/50
2024-12-09 14:33:41.316110: train_loss -0.6676
2024-12-09 14:33:41.316994: val_loss -0.4551
2024-12-09 14:33:41.317729: Pseudo dice [0.7126]
2024-12-09 14:33:41.318531: Epoch time: 88.19 s
2024-12-09 14:33:42.592840: 
2024-12-09 14:33:42.594216: Epoch 56
2024-12-09 14:33:42.595014: Current learning rate: 0.00949
2024-12-09 14:35:10.845289: Validation loss did not improve from -0.47301. Patience: 8/50
2024-12-09 14:35:10.846205: train_loss -0.655
2024-12-09 14:35:10.846998: val_loss -0.3352
2024-12-09 14:35:10.847607: Pseudo dice [0.6347]
2024-12-09 14:35:10.848271: Epoch time: 88.25 s
2024-12-09 14:35:12.158435: 
2024-12-09 14:35:12.159720: Epoch 57
2024-12-09 14:35:12.160433: Current learning rate: 0.00949
2024-12-09 14:36:40.300206: Validation loss did not improve from -0.47301. Patience: 9/50
2024-12-09 14:36:40.301037: train_loss -0.6688
2024-12-09 14:36:40.302280: val_loss -0.4327
2024-12-09 14:36:40.303106: Pseudo dice [0.6902]
2024-12-09 14:36:40.303831: Epoch time: 88.14 s
2024-12-09 14:36:41.566148: 
2024-12-09 14:36:41.567884: Epoch 58
2024-12-09 14:36:41.568819: Current learning rate: 0.00948
2024-12-09 14:38:09.721942: Validation loss did not improve from -0.47301. Patience: 10/50
2024-12-09 14:38:09.722763: train_loss -0.6608
2024-12-09 14:38:09.723607: val_loss -0.3988
2024-12-09 14:38:09.724277: Pseudo dice [0.6825]
2024-12-09 14:38:09.725005: Epoch time: 88.16 s
2024-12-09 14:38:11.052039: 
2024-12-09 14:38:11.053507: Epoch 59
2024-12-09 14:38:11.054285: Current learning rate: 0.00947
2024-12-09 14:39:39.168631: Validation loss did not improve from -0.47301. Patience: 11/50
2024-12-09 14:39:39.169832: train_loss -0.669
2024-12-09 14:39:39.170856: val_loss -0.3624
2024-12-09 14:39:39.171650: Pseudo dice [0.6538]
2024-12-09 14:39:39.172384: Epoch time: 88.12 s
2024-12-09 14:39:40.808502: 
2024-12-09 14:39:40.810310: Epoch 60
2024-12-09 14:39:40.811182: Current learning rate: 0.00946
2024-12-09 14:41:08.781761: Validation loss did not improve from -0.47301. Patience: 12/50
2024-12-09 14:41:08.782791: train_loss -0.6684
2024-12-09 14:41:08.783506: val_loss -0.3784
2024-12-09 14:41:08.784261: Pseudo dice [0.6688]
2024-12-09 14:41:08.784925: Epoch time: 87.98 s
2024-12-09 14:41:10.066411: 
2024-12-09 14:41:10.068161: Epoch 61
2024-12-09 14:41:10.068987: Current learning rate: 0.00945
2024-12-09 14:42:38.058377: Validation loss did not improve from -0.47301. Patience: 13/50
2024-12-09 14:42:38.059567: train_loss -0.6686
2024-12-09 14:42:38.060960: val_loss -0.4415
2024-12-09 14:42:38.061999: Pseudo dice [0.6946]
2024-12-09 14:42:38.063003: Epoch time: 87.99 s
2024-12-09 14:42:38.063913: Yayy! New best EMA pseudo Dice: 0.6682
2024-12-09 14:42:40.044941: 
2024-12-09 14:42:40.046823: Epoch 62
2024-12-09 14:42:40.047753: Current learning rate: 0.00944
2024-12-09 14:44:07.887263: Validation loss did not improve from -0.47301. Patience: 14/50
2024-12-09 14:44:07.888429: train_loss -0.6735
2024-12-09 14:44:07.889334: val_loss -0.4603
2024-12-09 14:44:07.890086: Pseudo dice [0.7065]
2024-12-09 14:44:07.890969: Epoch time: 87.84 s
2024-12-09 14:44:07.891605: Yayy! New best EMA pseudo Dice: 0.672
2024-12-09 14:44:09.501919: 
2024-12-09 14:44:09.503582: Epoch 63
2024-12-09 14:44:09.504567: Current learning rate: 0.00943
2024-12-09 14:45:37.285143: Validation loss did not improve from -0.47301. Patience: 15/50
2024-12-09 14:45:37.286121: train_loss -0.677
2024-12-09 14:45:37.287002: val_loss -0.373
2024-12-09 14:45:37.287910: Pseudo dice [0.6599]
2024-12-09 14:45:37.288700: Epoch time: 87.79 s
2024-12-09 14:45:38.607171: 
2024-12-09 14:45:38.608618: Epoch 64
2024-12-09 14:45:38.609343: Current learning rate: 0.00942
2024-12-09 14:47:06.481715: Validation loss did not improve from -0.47301. Patience: 16/50
2024-12-09 14:47:06.482620: train_loss -0.6759
2024-12-09 14:47:06.483434: val_loss -0.4091
2024-12-09 14:47:06.484063: Pseudo dice [0.6687]
2024-12-09 14:47:06.484715: Epoch time: 87.88 s
2024-12-09 14:47:08.108268: 
2024-12-09 14:47:08.110013: Epoch 65
2024-12-09 14:47:08.110917: Current learning rate: 0.00941
2024-12-09 14:48:35.825445: Validation loss did not improve from -0.47301. Patience: 17/50
2024-12-09 14:48:35.826697: train_loss -0.6796
2024-12-09 14:48:35.827910: val_loss -0.3944
2024-12-09 14:48:35.828778: Pseudo dice [0.6768]
2024-12-09 14:48:35.829640: Epoch time: 87.72 s
2024-12-09 14:48:37.145839: 
2024-12-09 14:48:37.147377: Epoch 66
2024-12-09 14:48:37.148207: Current learning rate: 0.0094
2024-12-09 14:50:04.776256: Validation loss did not improve from -0.47301. Patience: 18/50
2024-12-09 14:50:04.777324: train_loss -0.6738
2024-12-09 14:50:04.778107: val_loss -0.3903
2024-12-09 14:50:04.778707: Pseudo dice [0.6667]
2024-12-09 14:50:04.779354: Epoch time: 87.63 s
2024-12-09 14:50:06.119625: 
2024-12-09 14:50:06.121122: Epoch 67
2024-12-09 14:50:06.122320: Current learning rate: 0.00939
2024-12-09 14:51:33.829439: Validation loss did not improve from -0.47301. Patience: 19/50
2024-12-09 14:51:33.830466: train_loss -0.6779
2024-12-09 14:51:33.831359: val_loss -0.416
2024-12-09 14:51:33.832208: Pseudo dice [0.6851]
2024-12-09 14:51:33.833021: Epoch time: 87.71 s
2024-12-09 14:51:33.833779: Yayy! New best EMA pseudo Dice: 0.6722
2024-12-09 14:51:35.614086: 
2024-12-09 14:51:35.615682: Epoch 68
2024-12-09 14:51:35.616481: Current learning rate: 0.00939
2024-12-09 14:53:03.479914: Validation loss did not improve from -0.47301. Patience: 20/50
2024-12-09 14:53:03.480629: train_loss -0.679
2024-12-09 14:53:03.481514: val_loss -0.4224
2024-12-09 14:53:03.482432: Pseudo dice [0.6989]
2024-12-09 14:53:03.483409: Epoch time: 87.87 s
2024-12-09 14:53:03.484158: Yayy! New best EMA pseudo Dice: 0.6749
2024-12-09 14:53:05.162245: 
2024-12-09 14:53:05.163680: Epoch 69
2024-12-09 14:53:05.164631: Current learning rate: 0.00938
2024-12-09 14:54:33.063918: Validation loss did not improve from -0.47301. Patience: 21/50
2024-12-09 14:54:33.064989: train_loss -0.6847
2024-12-09 14:54:33.065957: val_loss -0.4023
2024-12-09 14:54:33.066587: Pseudo dice [0.6924]
2024-12-09 14:54:33.067215: Epoch time: 87.9 s
2024-12-09 14:54:33.434573: Yayy! New best EMA pseudo Dice: 0.6766
2024-12-09 14:54:35.139262: 
2024-12-09 14:54:35.140741: Epoch 70
2024-12-09 14:54:35.141811: Current learning rate: 0.00937
2024-12-09 14:56:02.951445: Validation loss did not improve from -0.47301. Patience: 22/50
2024-12-09 14:56:02.952794: train_loss -0.6817
2024-12-09 14:56:02.953643: val_loss -0.4542
2024-12-09 14:56:02.954437: Pseudo dice [0.6896]
2024-12-09 14:56:02.955355: Epoch time: 87.81 s
2024-12-09 14:56:02.956193: Yayy! New best EMA pseudo Dice: 0.6779
2024-12-09 14:56:04.672238: 
2024-12-09 14:56:04.673800: Epoch 71
2024-12-09 14:56:04.674739: Current learning rate: 0.00936
2024-12-09 14:57:32.531713: Validation loss did not improve from -0.47301. Patience: 23/50
2024-12-09 14:57:32.532982: train_loss -0.6797
2024-12-09 14:57:32.533896: val_loss -0.3846
2024-12-09 14:57:32.534765: Pseudo dice [0.6771]
2024-12-09 14:57:32.535537: Epoch time: 87.86 s
2024-12-09 14:57:33.903168: 
2024-12-09 14:57:33.904535: Epoch 72
2024-12-09 14:57:33.905326: Current learning rate: 0.00935
2024-12-09 14:59:01.725412: Validation loss did not improve from -0.47301. Patience: 24/50
2024-12-09 14:59:01.726340: train_loss -0.6785
2024-12-09 14:59:01.727087: val_loss -0.4385
2024-12-09 14:59:01.727668: Pseudo dice [0.6936]
2024-12-09 14:59:01.728375: Epoch time: 87.82 s
2024-12-09 14:59:01.729079: Yayy! New best EMA pseudo Dice: 0.6794
2024-12-09 14:59:03.776025: 
2024-12-09 14:59:03.777393: Epoch 73
2024-12-09 14:59:03.778045: Current learning rate: 0.00934
2024-12-09 15:00:31.712577: Validation loss did not improve from -0.47301. Patience: 25/50
2024-12-09 15:00:31.713423: train_loss -0.6859
2024-12-09 15:00:31.714288: val_loss -0.3964
2024-12-09 15:00:31.714909: Pseudo dice [0.676]
2024-12-09 15:00:31.715487: Epoch time: 87.94 s
2024-12-09 15:00:33.064927: 
2024-12-09 15:00:33.066655: Epoch 74
2024-12-09 15:00:33.067307: Current learning rate: 0.00933
2024-12-09 15:02:01.040327: Validation loss did not improve from -0.47301. Patience: 26/50
2024-12-09 15:02:01.041159: train_loss -0.6812
2024-12-09 15:02:01.042130: val_loss -0.2914
2024-12-09 15:02:01.042907: Pseudo dice [0.6298]
2024-12-09 15:02:01.043724: Epoch time: 87.98 s
2024-12-09 15:02:02.727218: 
2024-12-09 15:02:02.728472: Epoch 75
2024-12-09 15:02:02.729451: Current learning rate: 0.00932
2024-12-09 15:03:30.602262: Validation loss did not improve from -0.47301. Patience: 27/50
2024-12-09 15:03:30.603343: train_loss -0.6853
2024-12-09 15:03:30.604135: val_loss -0.3955
2024-12-09 15:03:30.604844: Pseudo dice [0.6777]
2024-12-09 15:03:30.605670: Epoch time: 87.88 s
2024-12-09 15:03:31.945909: 
2024-12-09 15:03:31.947444: Epoch 76
2024-12-09 15:03:31.948149: Current learning rate: 0.00931
2024-12-09 15:04:59.932846: Validation loss did not improve from -0.47301. Patience: 28/50
2024-12-09 15:04:59.933836: train_loss -0.6919
2024-12-09 15:04:59.934703: val_loss -0.4277
2024-12-09 15:04:59.935398: Pseudo dice [0.6864]
2024-12-09 15:04:59.936098: Epoch time: 87.99 s
2024-12-09 15:05:01.243531: 
2024-12-09 15:05:01.244945: Epoch 77
2024-12-09 15:05:01.245880: Current learning rate: 0.0093
2024-12-09 15:06:29.337907: Validation loss did not improve from -0.47301. Patience: 29/50
2024-12-09 15:06:29.339240: train_loss -0.6916
2024-12-09 15:06:29.340375: val_loss -0.4407
2024-12-09 15:06:29.341193: Pseudo dice [0.6816]
2024-12-09 15:06:29.342000: Epoch time: 88.1 s
2024-12-09 15:06:30.701851: 
2024-12-09 15:06:30.703011: Epoch 78
2024-12-09 15:06:30.703801: Current learning rate: 0.0093
2024-12-09 15:07:58.863306: Validation loss did not improve from -0.47301. Patience: 30/50
2024-12-09 15:07:58.864330: train_loss -0.6921
2024-12-09 15:07:58.865192: val_loss -0.3816
2024-12-09 15:07:58.866014: Pseudo dice [0.6567]
2024-12-09 15:07:58.866772: Epoch time: 88.16 s
2024-12-09 15:08:00.263612: 
2024-12-09 15:08:00.265087: Epoch 79
2024-12-09 15:08:00.265808: Current learning rate: 0.00929
2024-12-09 15:09:28.309825: Validation loss did not improve from -0.47301. Patience: 31/50
2024-12-09 15:09:28.310691: train_loss -0.7039
2024-12-09 15:09:28.311455: val_loss -0.2681
2024-12-09 15:09:28.312060: Pseudo dice [0.605]
2024-12-09 15:09:28.312744: Epoch time: 88.05 s
2024-12-09 15:09:30.092759: 
2024-12-09 15:09:30.093757: Epoch 80
2024-12-09 15:09:30.094415: Current learning rate: 0.00928
2024-12-09 15:10:58.095820: Validation loss did not improve from -0.47301. Patience: 32/50
2024-12-09 15:10:58.096756: train_loss -0.7006
2024-12-09 15:10:58.097454: val_loss -0.428
2024-12-09 15:10:58.098089: Pseudo dice [0.6905]
2024-12-09 15:10:58.098737: Epoch time: 88.0 s
2024-12-09 15:10:59.491207: 
2024-12-09 15:10:59.492464: Epoch 81
2024-12-09 15:10:59.493402: Current learning rate: 0.00927
2024-12-09 15:12:28.641550: Validation loss did not improve from -0.47301. Patience: 33/50
2024-12-09 15:12:28.642783: train_loss -0.6983
2024-12-09 15:12:28.643533: val_loss -0.4425
2024-12-09 15:12:28.644251: Pseudo dice [0.6946]
2024-12-09 15:12:28.644933: Epoch time: 89.15 s
2024-12-09 15:12:30.085098: 
2024-12-09 15:12:30.086405: Epoch 82
2024-12-09 15:12:30.087246: Current learning rate: 0.00926
2024-12-09 15:13:58.131078: Validation loss did not improve from -0.47301. Patience: 34/50
2024-12-09 15:13:58.132174: train_loss -0.6994
2024-12-09 15:13:58.133034: val_loss -0.3817
2024-12-09 15:13:58.133759: Pseudo dice [0.6665]
2024-12-09 15:13:58.134433: Epoch time: 88.05 s
2024-12-09 15:13:59.859589: 
2024-12-09 15:13:59.861327: Epoch 83
2024-12-09 15:13:59.862100: Current learning rate: 0.00925
2024-12-09 15:15:27.949331: Validation loss did not improve from -0.47301. Patience: 35/50
2024-12-09 15:15:27.950655: train_loss -0.6995
2024-12-09 15:15:27.951518: val_loss -0.293
2024-12-09 15:15:27.952270: Pseudo dice [0.6235]
2024-12-09 15:15:27.952971: Epoch time: 88.09 s
2024-12-09 15:15:29.239504: 
2024-12-09 15:15:29.241258: Epoch 84
2024-12-09 15:15:29.242063: Current learning rate: 0.00924
2024-12-09 15:16:57.419802: Validation loss did not improve from -0.47301. Patience: 36/50
2024-12-09 15:16:57.421147: train_loss -0.6801
2024-12-09 15:16:57.421928: val_loss -0.3307
2024-12-09 15:16:57.422671: Pseudo dice [0.6303]
2024-12-09 15:16:57.423275: Epoch time: 88.18 s
2024-12-09 15:16:59.144658: 
2024-12-09 15:16:59.146378: Epoch 85
2024-12-09 15:16:59.147286: Current learning rate: 0.00923
2024-12-09 15:18:28.074336: Validation loss did not improve from -0.47301. Patience: 37/50
2024-12-09 15:18:28.097532: train_loss -0.6839
2024-12-09 15:18:28.099444: val_loss -0.3293
2024-12-09 15:18:28.100422: Pseudo dice [0.6309]
2024-12-09 15:18:28.101321: Epoch time: 88.93 s
2024-12-09 15:18:29.644843: 
2024-12-09 15:18:29.646372: Epoch 86
2024-12-09 15:18:29.647251: Current learning rate: 0.00922
2024-12-09 15:19:57.468759: Validation loss did not improve from -0.47301. Patience: 38/50
2024-12-09 15:19:57.469387: train_loss -0.6854
2024-12-09 15:19:57.470298: val_loss -0.3922
2024-12-09 15:19:57.471086: Pseudo dice [0.6703]
2024-12-09 15:19:57.471934: Epoch time: 87.83 s
2024-12-09 15:19:58.706381: 
2024-12-09 15:19:58.707822: Epoch 87
2024-12-09 15:19:58.708627: Current learning rate: 0.00921
2024-12-09 15:21:26.602027: Validation loss did not improve from -0.47301. Patience: 39/50
2024-12-09 15:21:26.603262: train_loss -0.6923
2024-12-09 15:21:26.604249: val_loss -0.4182
2024-12-09 15:21:26.605166: Pseudo dice [0.6853]
2024-12-09 15:21:26.606154: Epoch time: 87.9 s
2024-12-09 15:21:27.900366: 
2024-12-09 15:21:27.901653: Epoch 88
2024-12-09 15:21:27.902438: Current learning rate: 0.0092
2024-12-09 15:22:55.852004: Validation loss did not improve from -0.47301. Patience: 40/50
2024-12-09 15:22:55.853141: train_loss -0.696
2024-12-09 15:22:55.854092: val_loss -0.3657
2024-12-09 15:22:55.854846: Pseudo dice [0.6663]
2024-12-09 15:22:55.855477: Epoch time: 87.95 s
2024-12-09 15:22:57.120414: 
2024-12-09 15:22:57.121778: Epoch 89
2024-12-09 15:22:57.122513: Current learning rate: 0.0092
2024-12-09 15:24:24.987929: Validation loss did not improve from -0.47301. Patience: 41/50
2024-12-09 15:24:24.988784: train_loss -0.6934
2024-12-09 15:24:24.989812: val_loss -0.4067
2024-12-09 15:24:24.990573: Pseudo dice [0.6823]
2024-12-09 15:24:24.991338: Epoch time: 87.87 s
2024-12-09 15:24:26.588985: 
2024-12-09 15:24:26.590247: Epoch 90
2024-12-09 15:24:26.590937: Current learning rate: 0.00919
2024-12-09 15:25:54.507716: Validation loss did not improve from -0.47301. Patience: 42/50
2024-12-09 15:25:54.508743: train_loss -0.6893
2024-12-09 15:25:54.509918: val_loss -0.4587
2024-12-09 15:25:54.510882: Pseudo dice [0.702]
2024-12-09 15:25:54.511795: Epoch time: 87.92 s
2024-12-09 15:25:55.808655: 
2024-12-09 15:25:55.810303: Epoch 91
2024-12-09 15:25:55.811206: Current learning rate: 0.00918
2024-12-09 15:27:23.710652: Validation loss did not improve from -0.47301. Patience: 43/50
2024-12-09 15:27:23.711362: train_loss -0.7012
2024-12-09 15:27:23.712085: val_loss -0.4255
2024-12-09 15:27:23.712789: Pseudo dice [0.7022]
2024-12-09 15:27:23.713475: Epoch time: 87.9 s
2024-12-09 15:27:24.930137: 
2024-12-09 15:27:24.931786: Epoch 92
2024-12-09 15:27:24.932536: Current learning rate: 0.00917
2024-12-09 15:28:52.729967: Validation loss did not improve from -0.47301. Patience: 44/50
2024-12-09 15:28:52.730779: train_loss -0.7
2024-12-09 15:28:52.731676: val_loss -0.3057
2024-12-09 15:28:52.732484: Pseudo dice [0.6411]
2024-12-09 15:28:52.733157: Epoch time: 87.8 s
2024-12-09 15:28:54.003346: 
2024-12-09 15:28:54.005035: Epoch 93
2024-12-09 15:28:54.005738: Current learning rate: 0.00916
2024-12-09 15:30:21.601167: Validation loss did not improve from -0.47301. Patience: 45/50
2024-12-09 15:30:21.602253: train_loss -0.7076
2024-12-09 15:30:21.603443: val_loss -0.348
2024-12-09 15:30:21.604417: Pseudo dice [0.626]
2024-12-09 15:30:21.605379: Epoch time: 87.6 s
2024-12-09 15:30:23.297585: 
2024-12-09 15:30:23.298995: Epoch 94
2024-12-09 15:30:23.299858: Current learning rate: 0.00915
2024-12-09 15:31:50.940853: Validation loss did not improve from -0.47301. Patience: 46/50
2024-12-09 15:31:50.942089: train_loss -0.709
2024-12-09 15:31:50.943168: val_loss -0.4398
2024-12-09 15:31:50.943944: Pseudo dice [0.696]
2024-12-09 15:31:50.944685: Epoch time: 87.65 s
2024-12-09 15:31:52.607999: 
2024-12-09 15:31:52.609538: Epoch 95
2024-12-09 15:31:52.610393: Current learning rate: 0.00914
2024-12-09 15:33:20.202976: Validation loss did not improve from -0.47301. Patience: 47/50
2024-12-09 15:33:20.204027: train_loss -0.7078
2024-12-09 15:33:20.204965: val_loss -0.3503
2024-12-09 15:33:20.205656: Pseudo dice [0.6395]
2024-12-09 15:33:20.206360: Epoch time: 87.6 s
2024-12-09 15:33:21.483300: 
2024-12-09 15:33:21.484771: Epoch 96
2024-12-09 15:33:21.485514: Current learning rate: 0.00913
2024-12-09 15:34:49.097255: Validation loss did not improve from -0.47301. Patience: 48/50
2024-12-09 15:34:49.098523: train_loss -0.7089
2024-12-09 15:34:49.099429: val_loss -0.3889
2024-12-09 15:34:49.100197: Pseudo dice [0.6666]
2024-12-09 15:34:49.100852: Epoch time: 87.62 s
2024-12-09 15:34:50.346953: 
2024-12-09 15:34:50.348325: Epoch 97
2024-12-09 15:34:50.349090: Current learning rate: 0.00912
2024-12-09 15:36:17.987475: Validation loss did not improve from -0.47301. Patience: 49/50
2024-12-09 15:36:17.988698: train_loss -0.7149
2024-12-09 15:36:17.989884: val_loss -0.3731
2024-12-09 15:36:17.990596: Pseudo dice [0.6837]
2024-12-09 15:36:17.991351: Epoch time: 87.64 s
2024-12-09 15:36:19.313591: 
2024-12-09 15:36:19.314751: Epoch 98
2024-12-09 15:36:19.315552: Current learning rate: 0.00911
2024-12-09 15:37:46.873538: Validation loss did not improve from -0.47301. Patience: 50/50
2024-12-09 15:37:46.874450: train_loss -0.7105
2024-12-09 15:37:46.875270: val_loss -0.4049
2024-12-09 15:37:46.876082: Pseudo dice [0.6901]
2024-12-09 15:37:46.876915: Epoch time: 87.56 s
2024-12-09 15:37:48.192515: Patience reached. Stopping training.
2024-12-09 15:37:48.602720: Training done.
2024-12-09 15:37:48.943952: Using splits from existing split file: /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset307_Sohee_Calcium_OCT_CrossValidation/splits_final.json
2024-12-09 15:37:48.946708: The split file contains 5 splits.
2024-12-09 15:37:48.947832: Desired fold for training: 3
2024-12-09 15:37:48.948837: This split has 7 training and 1 validation cases.
2024-12-09 15:37:48.949493: predicting 701-013
2024-12-09 15:37:48.959060: 701-013, shape torch.Size([1, 375, 498, 498]), rank 0
2024-12-09 15:40:16.296576: Validation complete
2024-12-09 15:40:16.297436: Mean Validation Dice:  0.6817910163830401
2024-12-09 13:07:49.792076: unpacking done...
2024-12-09 13:07:49.942803: Unable to plot network architecture: nnUNet_compile is enabled!
2024-12-09 13:07:50.379772: 
2024-12-09 13:07:50.380710: Epoch 0
2024-12-09 13:07:50.381741: Current learning rate: 0.01
2024-12-09 13:11:26.347225: Validation loss improved from 1000.00000 to -0.27955! Patience: 0/50
2024-12-09 13:11:26.377306: train_loss -0.1295
2024-12-09 13:11:26.405414: val_loss -0.2795
2024-12-09 13:11:26.406723: Pseudo dice [0.5782]
2024-12-09 13:11:26.407810: Epoch time: 215.97 s
2024-12-09 13:11:26.408967: Yayy! New best EMA pseudo Dice: 0.5782
2024-12-09 13:11:29.538959: 
2024-12-09 13:11:29.540699: Epoch 1
2024-12-09 13:11:29.541631: Current learning rate: 0.00999
2024-12-09 13:12:59.525048: Validation loss improved from -0.27955 to -0.40325! Patience: 0/50
2024-12-09 13:12:59.525966: train_loss -0.3032
2024-12-09 13:12:59.526888: val_loss -0.4033
2024-12-09 13:12:59.527622: Pseudo dice [0.6623]
2024-12-09 13:12:59.528556: Epoch time: 89.99 s
2024-12-09 13:12:59.529254: Yayy! New best EMA pseudo Dice: 0.5866
2024-12-09 13:13:01.061057: 
2024-12-09 13:13:01.062468: Epoch 2
2024-12-09 13:13:01.063308: Current learning rate: 0.00998
2024-12-09 13:14:31.570001: Validation loss did not improve from -0.40325. Patience: 1/50
2024-12-09 13:14:31.571117: train_loss -0.3605
2024-12-09 13:14:31.572053: val_loss -0.3311
2024-12-09 13:14:31.572838: Pseudo dice [0.6156]
2024-12-09 13:14:31.573618: Epoch time: 90.51 s
2024-12-09 13:14:31.574393: Yayy! New best EMA pseudo Dice: 0.5895
2024-12-09 13:14:33.189998: 
2024-12-09 13:14:33.191728: Epoch 3
2024-12-09 13:14:33.192586: Current learning rate: 0.00997
2024-12-09 13:16:04.024628: Validation loss improved from -0.40325 to -0.43061! Patience: 1/50
2024-12-09 13:16:04.025735: train_loss -0.3977
2024-12-09 13:16:04.026833: val_loss -0.4306
2024-12-09 13:16:04.027616: Pseudo dice [0.6655]
2024-12-09 13:16:04.028598: Epoch time: 90.84 s
2024-12-09 13:16:04.029371: Yayy! New best EMA pseudo Dice: 0.5971
2024-12-09 13:16:05.592605: 
2024-12-09 13:16:05.594182: Epoch 4
2024-12-09 13:16:05.595496: Current learning rate: 0.00996
2024-12-09 13:17:36.548866: Validation loss improved from -0.43061 to -0.44059! Patience: 0/50
2024-12-09 13:17:36.550019: train_loss -0.4254
2024-12-09 13:17:36.550926: val_loss -0.4406
2024-12-09 13:17:36.551697: Pseudo dice [0.6819]
2024-12-09 13:17:36.552350: Epoch time: 90.96 s
2024-12-09 13:17:36.847550: Yayy! New best EMA pseudo Dice: 0.6056
2024-12-09 13:17:38.465621: 
2024-12-09 13:17:38.467110: Epoch 5
2024-12-09 13:17:38.468271: Current learning rate: 0.00995
2024-12-09 13:19:09.493377: Validation loss improved from -0.44059 to -0.47825! Patience: 0/50
2024-12-09 13:19:09.494297: train_loss -0.4456
2024-12-09 13:19:09.495122: val_loss -0.4783
2024-12-09 13:19:09.495915: Pseudo dice [0.6985]
2024-12-09 13:19:09.496635: Epoch time: 91.03 s
2024-12-09 13:19:09.497357: Yayy! New best EMA pseudo Dice: 0.6149
2024-12-09 13:19:11.149532: 
2024-12-09 13:19:11.151061: Epoch 6
2024-12-09 13:19:11.151967: Current learning rate: 0.00995
2024-12-09 13:20:42.241877: Validation loss did not improve from -0.47825. Patience: 1/50
2024-12-09 13:20:42.243221: train_loss -0.4425
2024-12-09 13:20:42.244053: val_loss -0.4451
2024-12-09 13:20:42.244965: Pseudo dice [0.6826]
2024-12-09 13:20:42.245776: Epoch time: 91.09 s
2024-12-09 13:20:42.246459: Yayy! New best EMA pseudo Dice: 0.6216
2024-12-09 13:20:43.847592: 
2024-12-09 13:20:43.849100: Epoch 7
2024-12-09 13:20:43.849887: Current learning rate: 0.00994
2024-12-09 13:22:14.826219: Validation loss did not improve from -0.47825. Patience: 2/50
2024-12-09 13:22:14.827252: train_loss -0.4523
2024-12-09 13:22:14.828402: val_loss -0.4633
2024-12-09 13:22:14.829333: Pseudo dice [0.6953]
2024-12-09 13:22:14.830295: Epoch time: 90.98 s
2024-12-09 13:22:14.831293: Yayy! New best EMA pseudo Dice: 0.629
2024-12-09 13:22:16.712277: 
2024-12-09 13:22:16.713953: Epoch 8
2024-12-09 13:22:16.715015: Current learning rate: 0.00993
2024-12-09 13:23:47.843794: Validation loss did not improve from -0.47825. Patience: 3/50
2024-12-09 13:23:47.844892: train_loss -0.4667
2024-12-09 13:23:47.845793: val_loss -0.4723
2024-12-09 13:23:47.846563: Pseudo dice [0.7011]
2024-12-09 13:23:47.847267: Epoch time: 91.13 s
2024-12-09 13:23:47.848075: Yayy! New best EMA pseudo Dice: 0.6362
2024-12-09 13:23:49.425595: 
2024-12-09 13:23:49.427269: Epoch 9
2024-12-09 13:23:49.428374: Current learning rate: 0.00992
2024-12-09 13:25:20.581593: Validation loss did not improve from -0.47825. Patience: 4/50
2024-12-09 13:25:20.582866: train_loss -0.4798
2024-12-09 13:25:20.584019: val_loss -0.4702
2024-12-09 13:25:20.585024: Pseudo dice [0.6914]
2024-12-09 13:25:20.585882: Epoch time: 91.16 s
2024-12-09 13:25:20.919707: Yayy! New best EMA pseudo Dice: 0.6417
2024-12-09 13:25:22.438759: 
2024-12-09 13:25:22.440809: Epoch 10
2024-12-09 13:25:22.442031: Current learning rate: 0.00991
2024-12-09 13:26:53.569940: Validation loss improved from -0.47825 to -0.48002! Patience: 4/50
2024-12-09 13:26:53.571213: train_loss -0.4738
2024-12-09 13:26:53.572167: val_loss -0.48
2024-12-09 13:26:53.572966: Pseudo dice [0.6889]
2024-12-09 13:26:53.573787: Epoch time: 91.13 s
2024-12-09 13:26:53.574604: Yayy! New best EMA pseudo Dice: 0.6465
2024-12-09 13:26:55.182421: 
2024-12-09 13:26:55.184104: Epoch 11
2024-12-09 13:26:55.184969: Current learning rate: 0.0099
2024-12-09 13:28:26.306925: Validation loss improved from -0.48002 to -0.50098! Patience: 0/50
2024-12-09 13:28:26.308318: train_loss -0.5097
2024-12-09 13:28:26.309377: val_loss -0.501
2024-12-09 13:28:26.310141: Pseudo dice [0.7127]
2024-12-09 13:28:26.310818: Epoch time: 91.13 s
2024-12-09 13:28:26.311573: Yayy! New best EMA pseudo Dice: 0.6531
2024-12-09 13:28:27.843361: 
2024-12-09 13:28:27.844239: Epoch 12
2024-12-09 13:28:27.844992: Current learning rate: 0.00989
2024-12-09 13:29:58.980400: Validation loss improved from -0.50098 to -0.51518! Patience: 0/50
2024-12-09 13:29:58.981291: train_loss -0.5039
2024-12-09 13:29:58.982121: val_loss -0.5152
2024-12-09 13:29:58.982903: Pseudo dice [0.7192]
2024-12-09 13:29:58.983737: Epoch time: 91.14 s
2024-12-09 13:29:58.984615: Yayy! New best EMA pseudo Dice: 0.6597
2024-12-09 13:30:00.539869: 
2024-12-09 13:30:00.541499: Epoch 13
2024-12-09 13:30:00.542306: Current learning rate: 0.00988
2024-12-09 13:31:31.687485: Validation loss improved from -0.51518 to -0.51593! Patience: 0/50
2024-12-09 13:31:31.688483: train_loss -0.5201
2024-12-09 13:31:31.689455: val_loss -0.5159
2024-12-09 13:31:31.690255: Pseudo dice [0.7236]
2024-12-09 13:31:31.691204: Epoch time: 91.15 s
2024-12-09 13:31:31.692022: Yayy! New best EMA pseudo Dice: 0.6661
2024-12-09 13:31:33.278707: 
2024-12-09 13:31:33.279783: Epoch 14
2024-12-09 13:31:33.280597: Current learning rate: 0.00987
2024-12-09 13:33:04.468568: Validation loss improved from -0.51593 to -0.53214! Patience: 0/50
2024-12-09 13:33:04.469877: train_loss -0.5028
2024-12-09 13:33:04.471220: val_loss -0.5321
2024-12-09 13:33:04.472023: Pseudo dice [0.7349]
2024-12-09 13:33:04.472774: Epoch time: 91.19 s
2024-12-09 13:33:04.823359: Yayy! New best EMA pseudo Dice: 0.673
2024-12-09 13:33:06.405033: 
2024-12-09 13:33:06.406552: Epoch 15
2024-12-09 13:33:06.407812: Current learning rate: 0.00986
2024-12-09 13:34:37.539915: Validation loss improved from -0.53214 to -0.53340! Patience: 0/50
2024-12-09 13:34:37.541024: train_loss -0.5175
2024-12-09 13:34:37.541952: val_loss -0.5334
2024-12-09 13:34:37.542660: Pseudo dice [0.7249]
2024-12-09 13:34:37.543363: Epoch time: 91.14 s
2024-12-09 13:34:37.544023: Yayy! New best EMA pseudo Dice: 0.6782
2024-12-09 13:34:39.164253: 
2024-12-09 13:34:39.165528: Epoch 16
2024-12-09 13:34:39.166461: Current learning rate: 0.00986
2024-12-09 13:36:10.422978: Validation loss did not improve from -0.53340. Patience: 1/50
2024-12-09 13:36:10.423797: train_loss -0.5356
2024-12-09 13:36:10.424722: val_loss -0.5201
2024-12-09 13:36:10.425541: Pseudo dice [0.7266]
2024-12-09 13:36:10.426393: Epoch time: 91.26 s
2024-12-09 13:36:10.427219: Yayy! New best EMA pseudo Dice: 0.683
2024-12-09 13:36:12.333678: 
2024-12-09 13:36:12.335593: Epoch 17
2024-12-09 13:36:12.336706: Current learning rate: 0.00985
2024-12-09 13:37:43.635571: Validation loss improved from -0.53340 to -0.53648! Patience: 1/50
2024-12-09 13:37:43.636284: train_loss -0.5515
2024-12-09 13:37:43.637233: val_loss -0.5365
2024-12-09 13:37:43.637925: Pseudo dice [0.7314]
2024-12-09 13:37:43.638660: Epoch time: 91.3 s
2024-12-09 13:37:43.639278: Yayy! New best EMA pseudo Dice: 0.6878
2024-12-09 13:37:45.231666: 
2024-12-09 13:37:45.233535: Epoch 18
2024-12-09 13:37:45.234322: Current learning rate: 0.00984
2024-12-09 13:39:16.547214: Validation loss improved from -0.53648 to -0.55616! Patience: 0/50
2024-12-09 13:39:16.548244: train_loss -0.5467
2024-12-09 13:39:16.549264: val_loss -0.5562
2024-12-09 13:39:16.550250: Pseudo dice [0.7415]
2024-12-09 13:39:16.551069: Epoch time: 91.32 s
2024-12-09 13:39:16.552038: Yayy! New best EMA pseudo Dice: 0.6932
2024-12-09 13:39:18.130483: 
2024-12-09 13:39:18.132154: Epoch 19
2024-12-09 13:39:18.133205: Current learning rate: 0.00983
2024-12-09 13:40:49.433811: Validation loss improved from -0.55616 to -0.56953! Patience: 0/50
2024-12-09 13:40:49.435032: train_loss -0.5561
2024-12-09 13:40:49.436301: val_loss -0.5695
2024-12-09 13:40:49.437052: Pseudo dice [0.7551]
2024-12-09 13:40:49.437988: Epoch time: 91.31 s
2024-12-09 13:40:49.773087: Yayy! New best EMA pseudo Dice: 0.6994
2024-12-09 13:40:51.362105: 
2024-12-09 13:40:51.363311: Epoch 20
2024-12-09 13:40:51.363986: Current learning rate: 0.00982
2024-12-09 13:42:22.704569: Validation loss did not improve from -0.56953. Patience: 1/50
2024-12-09 13:42:22.705680: train_loss -0.5497
2024-12-09 13:42:22.706623: val_loss -0.5545
2024-12-09 13:42:22.707471: Pseudo dice [0.7458]
2024-12-09 13:42:22.708251: Epoch time: 91.34 s
2024-12-09 13:42:22.708919: Yayy! New best EMA pseudo Dice: 0.704
2024-12-09 13:42:24.313601: 
2024-12-09 13:42:24.315223: Epoch 21
2024-12-09 13:42:24.316130: Current learning rate: 0.00981
2024-12-09 13:43:55.518949: Validation loss did not improve from -0.56953. Patience: 2/50
2024-12-09 13:43:55.520143: train_loss -0.5535
2024-12-09 13:43:55.521020: val_loss -0.5404
2024-12-09 13:43:55.521726: Pseudo dice [0.728]
2024-12-09 13:43:55.522485: Epoch time: 91.21 s
2024-12-09 13:43:55.523361: Yayy! New best EMA pseudo Dice: 0.7064
2024-12-09 13:43:57.036980: 
2024-12-09 13:43:57.038496: Epoch 22
2024-12-09 13:43:57.039364: Current learning rate: 0.0098
2024-12-09 13:45:28.334137: Validation loss did not improve from -0.56953. Patience: 3/50
2024-12-09 13:45:28.335399: train_loss -0.5611
2024-12-09 13:45:28.336936: val_loss -0.5253
2024-12-09 13:45:28.338054: Pseudo dice [0.7184]
2024-12-09 13:45:28.338998: Epoch time: 91.3 s
2024-12-09 13:45:28.339788: Yayy! New best EMA pseudo Dice: 0.7076
2024-12-09 13:45:29.863099: 
2024-12-09 13:45:29.864298: Epoch 23
2024-12-09 13:45:29.865384: Current learning rate: 0.00979
2024-12-09 13:47:01.196833: Validation loss did not improve from -0.56953. Patience: 4/50
2024-12-09 13:47:01.198024: train_loss -0.564
2024-12-09 13:47:01.199060: val_loss -0.5542
2024-12-09 13:47:01.199883: Pseudo dice [0.739]
2024-12-09 13:47:01.200866: Epoch time: 91.34 s
2024-12-09 13:47:01.201620: Yayy! New best EMA pseudo Dice: 0.7108
2024-12-09 13:47:02.706438: 
2024-12-09 13:47:02.708036: Epoch 24
2024-12-09 13:47:02.709088: Current learning rate: 0.00978
2024-12-09 13:48:33.961719: Validation loss did not improve from -0.56953. Patience: 5/50
2024-12-09 13:48:33.962926: train_loss -0.5666
2024-12-09 13:48:33.963841: val_loss -0.5437
2024-12-09 13:48:33.964560: Pseudo dice [0.7373]
2024-12-09 13:48:33.965386: Epoch time: 91.26 s
2024-12-09 13:48:34.322016: Yayy! New best EMA pseudo Dice: 0.7134
2024-12-09 13:48:35.814296: 
2024-12-09 13:48:35.815142: Epoch 25
2024-12-09 13:48:35.815928: Current learning rate: 0.00977
2024-12-09 13:50:06.922327: Validation loss did not improve from -0.56953. Patience: 6/50
2024-12-09 13:50:06.923373: train_loss -0.5691
2024-12-09 13:50:06.924248: val_loss -0.5269
2024-12-09 13:50:06.924997: Pseudo dice [0.723]
2024-12-09 13:50:06.925775: Epoch time: 91.11 s
2024-12-09 13:50:06.926509: Yayy! New best EMA pseudo Dice: 0.7144
2024-12-09 13:50:08.418394: 
2024-12-09 13:50:08.420195: Epoch 26
2024-12-09 13:50:08.421391: Current learning rate: 0.00977
2024-12-09 13:51:39.578444: Validation loss did not improve from -0.56953. Patience: 7/50
2024-12-09 13:51:39.579272: train_loss -0.5759
2024-12-09 13:51:39.580083: val_loss -0.547
2024-12-09 13:51:39.580821: Pseudo dice [0.7358]
2024-12-09 13:51:39.581514: Epoch time: 91.16 s
2024-12-09 13:51:39.582129: Yayy! New best EMA pseudo Dice: 0.7165
2024-12-09 13:51:41.123693: 
2024-12-09 13:51:41.125453: Epoch 27
2024-12-09 13:51:41.126689: Current learning rate: 0.00976
2024-12-09 13:53:12.307135: Validation loss did not improve from -0.56953. Patience: 8/50
2024-12-09 13:53:12.307979: train_loss -0.5894
2024-12-09 13:53:12.308970: val_loss -0.5692
2024-12-09 13:53:12.309696: Pseudo dice [0.7493]
2024-12-09 13:53:12.310418: Epoch time: 91.19 s
2024-12-09 13:53:12.311204: Yayy! New best EMA pseudo Dice: 0.7198
2024-12-09 13:53:14.137568: 
2024-12-09 13:53:14.138919: Epoch 28
2024-12-09 13:53:14.139688: Current learning rate: 0.00975
2024-12-09 13:54:45.394646: Validation loss improved from -0.56953 to -0.57291! Patience: 8/50
2024-12-09 13:54:45.395913: train_loss -0.5849
2024-12-09 13:54:45.396754: val_loss -0.5729
2024-12-09 13:54:45.397459: Pseudo dice [0.752]
2024-12-09 13:54:45.398085: Epoch time: 91.26 s
2024-12-09 13:54:45.398736: Yayy! New best EMA pseudo Dice: 0.723
2024-12-09 13:54:46.917802: 
2024-12-09 13:54:46.919515: Epoch 29
2024-12-09 13:54:46.920567: Current learning rate: 0.00974
2024-12-09 13:56:18.043052: Validation loss improved from -0.57291 to -0.57340! Patience: 0/50
2024-12-09 13:56:18.044056: train_loss -0.5947
2024-12-09 13:56:18.045009: val_loss -0.5734
2024-12-09 13:56:18.045828: Pseudo dice [0.7541]
2024-12-09 13:56:18.046693: Epoch time: 91.13 s
2024-12-09 13:56:18.402719: Yayy! New best EMA pseudo Dice: 0.7261
2024-12-09 13:56:19.978147: 
2024-12-09 13:56:19.979899: Epoch 30
2024-12-09 13:56:19.981066: Current learning rate: 0.00973
2024-12-09 13:57:51.134073: Validation loss did not improve from -0.57340. Patience: 1/50
2024-12-09 13:57:51.135278: train_loss -0.5958
2024-12-09 13:57:51.136402: val_loss -0.5414
2024-12-09 13:57:51.137386: Pseudo dice [0.7359]
2024-12-09 13:57:51.138180: Epoch time: 91.16 s
2024-12-09 13:57:51.139082: Yayy! New best EMA pseudo Dice: 0.7271
2024-12-09 13:57:52.733050: 
2024-12-09 13:57:52.734558: Epoch 31
2024-12-09 13:57:52.735947: Current learning rate: 0.00972
2024-12-09 13:59:23.799036: Validation loss did not improve from -0.57340. Patience: 2/50
2024-12-09 13:59:23.800210: train_loss -0.6023
2024-12-09 13:59:23.801203: val_loss -0.5694
2024-12-09 13:59:23.802158: Pseudo dice [0.7527]
2024-12-09 13:59:23.803113: Epoch time: 91.07 s
2024-12-09 13:59:23.804124: Yayy! New best EMA pseudo Dice: 0.7297
2024-12-09 13:59:25.361592: 
2024-12-09 13:59:25.363296: Epoch 32
2024-12-09 13:59:25.364591: Current learning rate: 0.00971
2024-12-09 14:00:56.484370: Validation loss did not improve from -0.57340. Patience: 3/50
2024-12-09 14:00:56.485601: train_loss -0.6082
2024-12-09 14:00:56.486542: val_loss -0.5682
2024-12-09 14:00:56.487259: Pseudo dice [0.744]
2024-12-09 14:00:56.487966: Epoch time: 91.12 s
2024-12-09 14:00:56.488675: Yayy! New best EMA pseudo Dice: 0.7311
2024-12-09 14:00:58.093161: 
2024-12-09 14:00:58.094629: Epoch 33
2024-12-09 14:00:58.095830: Current learning rate: 0.0097
2024-12-09 14:02:29.193840: Validation loss did not improve from -0.57340. Patience: 4/50
2024-12-09 14:02:29.194902: train_loss -0.6151
2024-12-09 14:02:29.195677: val_loss -0.5656
2024-12-09 14:02:29.196332: Pseudo dice [0.7515]
2024-12-09 14:02:29.196984: Epoch time: 91.1 s
2024-12-09 14:02:29.197619: Yayy! New best EMA pseudo Dice: 0.7331
2024-12-09 14:02:30.805071: 
2024-12-09 14:02:30.806665: Epoch 34
2024-12-09 14:02:30.807420: Current learning rate: 0.00969
2024-12-09 14:04:01.891579: Validation loss did not improve from -0.57340. Patience: 5/50
2024-12-09 14:04:01.892432: train_loss -0.6222
2024-12-09 14:04:01.893417: val_loss -0.5725
2024-12-09 14:04:01.894217: Pseudo dice [0.7567]
2024-12-09 14:04:01.895071: Epoch time: 91.09 s
2024-12-09 14:04:02.252655: Yayy! New best EMA pseudo Dice: 0.7355
2024-12-09 14:04:03.828249: 
2024-12-09 14:04:03.829778: Epoch 35
2024-12-09 14:04:03.830650: Current learning rate: 0.00968
2024-12-09 14:05:34.922584: Validation loss did not improve from -0.57340. Patience: 6/50
2024-12-09 14:05:34.923720: train_loss -0.6147
2024-12-09 14:05:34.925214: val_loss -0.5393
2024-12-09 14:05:34.926112: Pseudo dice [0.7312]
2024-12-09 14:05:34.926998: Epoch time: 91.1 s
2024-12-09 14:05:36.199505: 
2024-12-09 14:05:36.201181: Epoch 36
2024-12-09 14:05:36.202579: Current learning rate: 0.00968
2024-12-09 14:07:07.310688: Validation loss improved from -0.57340 to -0.58216! Patience: 6/50
2024-12-09 14:07:07.311754: train_loss -0.6124
2024-12-09 14:07:07.312640: val_loss -0.5822
2024-12-09 14:07:07.313418: Pseudo dice [0.7573]
2024-12-09 14:07:07.314147: Epoch time: 91.11 s
2024-12-09 14:07:07.314960: Yayy! New best EMA pseudo Dice: 0.7373
2024-12-09 14:07:08.910420: 
2024-12-09 14:07:08.912420: Epoch 37
2024-12-09 14:07:08.913330: Current learning rate: 0.00967
2024-12-09 14:08:39.840189: Validation loss did not improve from -0.58216. Patience: 1/50
2024-12-09 14:08:39.840918: train_loss -0.6153
2024-12-09 14:08:39.841957: val_loss -0.571
2024-12-09 14:08:39.842792: Pseudo dice [0.7571]
2024-12-09 14:08:39.843614: Epoch time: 90.93 s
2024-12-09 14:08:39.844308: Yayy! New best EMA pseudo Dice: 0.7393
2024-12-09 14:08:41.921017: 
2024-12-09 14:08:41.923002: Epoch 38
2024-12-09 14:08:41.924144: Current learning rate: 0.00966
2024-12-09 14:10:12.935975: Validation loss did not improve from -0.58216. Patience: 2/50
2024-12-09 14:10:12.937157: train_loss -0.6206
2024-12-09 14:10:12.938199: val_loss -0.5448
2024-12-09 14:10:12.938934: Pseudo dice [0.7264]
2024-12-09 14:10:12.939674: Epoch time: 91.02 s
2024-12-09 14:10:14.189049: 
2024-12-09 14:10:14.190840: Epoch 39
2024-12-09 14:10:14.191870: Current learning rate: 0.00965
2024-12-09 14:11:45.159122: Validation loss improved from -0.58216 to -0.58559! Patience: 2/50
2024-12-09 14:11:45.160348: train_loss -0.6107
2024-12-09 14:11:45.161225: val_loss -0.5856
2024-12-09 14:11:45.162042: Pseudo dice [0.7657]
2024-12-09 14:11:45.162737: Epoch time: 90.97 s
2024-12-09 14:11:45.528183: Yayy! New best EMA pseudo Dice: 0.7408
2024-12-09 14:11:47.142539: 
2024-12-09 14:11:47.144315: Epoch 40
2024-12-09 14:11:47.145245: Current learning rate: 0.00964
2024-12-09 14:13:18.134284: Validation loss did not improve from -0.58559. Patience: 1/50
2024-12-09 14:13:18.135499: train_loss -0.6179
2024-12-09 14:13:18.136395: val_loss -0.5753
2024-12-09 14:13:18.137205: Pseudo dice [0.7529]
2024-12-09 14:13:18.138002: Epoch time: 90.99 s
2024-12-09 14:13:18.138728: Yayy! New best EMA pseudo Dice: 0.742
2024-12-09 14:13:19.776556: 
2024-12-09 14:13:19.779659: Epoch 41
2024-12-09 14:13:19.780985: Current learning rate: 0.00963
2024-12-09 14:14:50.931030: Validation loss improved from -0.58559 to -0.58642! Patience: 1/50
2024-12-09 14:14:50.932818: train_loss -0.633
2024-12-09 14:14:50.934003: val_loss -0.5864
2024-12-09 14:14:50.934907: Pseudo dice [0.7631]
2024-12-09 14:14:50.936286: Epoch time: 91.16 s
2024-12-09 14:14:50.937639: Yayy! New best EMA pseudo Dice: 0.7441
2024-12-09 14:14:52.582134: 
2024-12-09 14:14:52.583773: Epoch 42
2024-12-09 14:14:52.584650: Current learning rate: 0.00962
2024-12-09 14:16:23.602123: Validation loss improved from -0.58642 to -0.58702! Patience: 0/50
2024-12-09 14:16:23.603587: train_loss -0.6328
2024-12-09 14:16:23.604786: val_loss -0.587
2024-12-09 14:16:23.605745: Pseudo dice [0.7542]
2024-12-09 14:16:23.606488: Epoch time: 91.02 s
2024-12-09 14:16:23.607278: Yayy! New best EMA pseudo Dice: 0.7451
2024-12-09 14:16:25.179219: 
2024-12-09 14:16:25.181146: Epoch 43
2024-12-09 14:16:25.182343: Current learning rate: 0.00961
2024-12-09 14:17:56.190270: Validation loss improved from -0.58702 to -0.59884! Patience: 0/50
2024-12-09 14:17:56.190966: train_loss -0.6433
2024-12-09 14:17:56.191827: val_loss -0.5988
2024-12-09 14:17:56.192594: Pseudo dice [0.7669]
2024-12-09 14:17:56.193353: Epoch time: 91.01 s
2024-12-09 14:17:56.194130: Yayy! New best EMA pseudo Dice: 0.7473
2024-12-09 14:17:57.777668: 
2024-12-09 14:17:57.780191: Epoch 44
2024-12-09 14:17:57.781542: Current learning rate: 0.0096
2024-12-09 14:19:28.791049: Validation loss did not improve from -0.59884. Patience: 1/50
2024-12-09 14:19:28.792252: train_loss -0.6317
2024-12-09 14:19:28.793213: val_loss -0.5607
2024-12-09 14:19:28.793937: Pseudo dice [0.7412]
2024-12-09 14:19:28.794569: Epoch time: 91.02 s
2024-12-09 14:19:30.352257: 
2024-12-09 14:19:30.354191: Epoch 45
2024-12-09 14:19:30.355337: Current learning rate: 0.00959
2024-12-09 14:21:01.522717: Validation loss did not improve from -0.59884. Patience: 2/50
2024-12-09 14:21:01.523796: train_loss -0.6322
2024-12-09 14:21:01.524682: val_loss -0.5561
2024-12-09 14:21:01.525463: Pseudo dice [0.7294]
2024-12-09 14:21:01.526244: Epoch time: 91.17 s
2024-12-09 14:21:02.753171: 
2024-12-09 14:21:02.754868: Epoch 46
2024-12-09 14:21:02.756050: Current learning rate: 0.00959
2024-12-09 14:22:33.927783: Validation loss did not improve from -0.59884. Patience: 3/50
2024-12-09 14:22:33.928607: train_loss -0.6343
2024-12-09 14:22:33.929566: val_loss -0.5888
2024-12-09 14:22:33.930238: Pseudo dice [0.762]
2024-12-09 14:22:33.931022: Epoch time: 91.18 s
2024-12-09 14:22:35.129441: 
2024-12-09 14:22:35.131224: Epoch 47
2024-12-09 14:22:35.132374: Current learning rate: 0.00958
2024-12-09 14:24:06.344540: Validation loss did not improve from -0.59884. Patience: 4/50
2024-12-09 14:24:06.345438: train_loss -0.6327
2024-12-09 14:24:06.346646: val_loss -0.5919
2024-12-09 14:24:06.347745: Pseudo dice [0.7644]
2024-12-09 14:24:06.348946: Epoch time: 91.22 s
2024-12-09 14:24:06.349931: Yayy! New best EMA pseudo Dice: 0.7484
2024-12-09 14:24:07.897513: 
2024-12-09 14:24:07.899338: Epoch 48
2024-12-09 14:24:07.900709: Current learning rate: 0.00957
2024-12-09 14:25:39.379665: Validation loss did not improve from -0.59884. Patience: 5/50
2024-12-09 14:25:39.380626: train_loss -0.6302
2024-12-09 14:25:39.381586: val_loss -0.5637
2024-12-09 14:25:39.382268: Pseudo dice [0.7493]
2024-12-09 14:25:39.382957: Epoch time: 91.48 s
2024-12-09 14:25:39.383576: Yayy! New best EMA pseudo Dice: 0.7485
2024-12-09 14:25:40.946990: 
2024-12-09 14:25:40.948423: Epoch 49
2024-12-09 14:25:40.949230: Current learning rate: 0.00956
2024-12-09 14:27:12.130350: Validation loss did not improve from -0.59884. Patience: 6/50
2024-12-09 14:27:12.131347: train_loss -0.6463
2024-12-09 14:27:12.132342: val_loss -0.5155
2024-12-09 14:27:12.133261: Pseudo dice [0.7117]
2024-12-09 14:27:12.134179: Epoch time: 91.19 s
2024-12-09 14:27:13.717528: 
2024-12-09 14:27:13.719388: Epoch 50
2024-12-09 14:27:13.720556: Current learning rate: 0.00955
2024-12-09 14:28:44.939067: Validation loss did not improve from -0.59884. Patience: 7/50
2024-12-09 14:28:44.939791: train_loss -0.6412
2024-12-09 14:28:44.940557: val_loss -0.5508
2024-12-09 14:28:44.941365: Pseudo dice [0.7448]
2024-12-09 14:28:44.942076: Epoch time: 91.22 s
2024-12-09 14:28:46.143141: 
2024-12-09 14:28:46.144649: Epoch 51
2024-12-09 14:28:46.146010: Current learning rate: 0.00954
2024-12-09 14:30:17.335632: Validation loss did not improve from -0.59884. Patience: 8/50
2024-12-09 14:30:17.336686: train_loss -0.6502
2024-12-09 14:30:17.337799: val_loss -0.5484
2024-12-09 14:30:17.338696: Pseudo dice [0.7371]
2024-12-09 14:30:17.339523: Epoch time: 91.19 s
2024-12-09 14:30:18.550192: 
2024-12-09 14:30:18.551662: Epoch 52
2024-12-09 14:30:18.552566: Current learning rate: 0.00953
2024-12-09 14:31:49.884885: Validation loss did not improve from -0.59884. Patience: 9/50
2024-12-09 14:31:49.886265: train_loss -0.6405
2024-12-09 14:31:49.887187: val_loss -0.5713
2024-12-09 14:31:49.887993: Pseudo dice [0.7563]
2024-12-09 14:31:49.888656: Epoch time: 91.34 s
2024-12-09 14:31:51.166854: 
2024-12-09 14:31:51.168388: Epoch 53
2024-12-09 14:31:51.169189: Current learning rate: 0.00952
2024-12-09 14:33:22.357603: Validation loss did not improve from -0.59884. Patience: 10/50
2024-12-09 14:33:22.359171: train_loss -0.6557
2024-12-09 14:33:22.360404: val_loss -0.5829
2024-12-09 14:33:22.361171: Pseudo dice [0.7589]
2024-12-09 14:33:22.361904: Epoch time: 91.19 s
2024-12-09 14:33:23.554818: 
2024-12-09 14:33:23.555714: Epoch 54
2024-12-09 14:33:23.556387: Current learning rate: 0.00951
2024-12-09 14:34:54.797223: Validation loss did not improve from -0.59884. Patience: 11/50
2024-12-09 14:34:54.798338: train_loss -0.659
2024-12-09 14:34:54.799233: val_loss -0.573
2024-12-09 14:34:54.800113: Pseudo dice [0.7603]
2024-12-09 14:34:54.800882: Epoch time: 91.24 s
2024-12-09 14:34:56.431370: 
2024-12-09 14:34:56.432989: Epoch 55
2024-12-09 14:34:56.434236: Current learning rate: 0.0095
2024-12-09 14:36:27.676742: Validation loss did not improve from -0.59884. Patience: 12/50
2024-12-09 14:36:27.677992: train_loss -0.6645
2024-12-09 14:36:27.679106: val_loss -0.583
2024-12-09 14:36:27.679898: Pseudo dice [0.7605]
2024-12-09 14:36:27.680730: Epoch time: 91.25 s
2024-12-09 14:36:27.681359: Yayy! New best EMA pseudo Dice: 0.7493
2024-12-09 14:36:29.285538: 
2024-12-09 14:36:29.287056: Epoch 56
2024-12-09 14:36:29.287940: Current learning rate: 0.00949
2024-12-09 14:38:00.422222: Validation loss improved from -0.59884 to -0.60845! Patience: 12/50
2024-12-09 14:38:00.423522: train_loss -0.6688
2024-12-09 14:38:00.424515: val_loss -0.6085
2024-12-09 14:38:00.425271: Pseudo dice [0.7749]
2024-12-09 14:38:00.426080: Epoch time: 91.14 s
2024-12-09 14:38:00.426896: Yayy! New best EMA pseudo Dice: 0.7518
2024-12-09 14:38:02.000441: 
2024-12-09 14:38:02.002076: Epoch 57
2024-12-09 14:38:02.003127: Current learning rate: 0.00949
2024-12-09 14:39:33.176027: Validation loss did not improve from -0.60845. Patience: 1/50
2024-12-09 14:39:33.176913: train_loss -0.6559
2024-12-09 14:39:33.177731: val_loss -0.5743
2024-12-09 14:39:33.178479: Pseudo dice [0.7529]
2024-12-09 14:39:33.179213: Epoch time: 91.18 s
2024-12-09 14:39:33.179923: Yayy! New best EMA pseudo Dice: 0.7519
2024-12-09 14:39:34.750725: 
2024-12-09 14:39:34.752600: Epoch 58
2024-12-09 14:39:34.753477: Current learning rate: 0.00948
2024-12-09 14:41:05.843633: Validation loss did not improve from -0.60845. Patience: 2/50
2024-12-09 14:41:05.844718: train_loss -0.6544
2024-12-09 14:41:05.845762: val_loss -0.5925
2024-12-09 14:41:05.846507: Pseudo dice [0.7679]
2024-12-09 14:41:05.847176: Epoch time: 91.09 s
2024-12-09 14:41:05.847801: Yayy! New best EMA pseudo Dice: 0.7535
2024-12-09 14:41:07.742588: 
2024-12-09 14:41:07.743924: Epoch 59
2024-12-09 14:41:07.744765: Current learning rate: 0.00947
2024-12-09 14:42:38.789070: Validation loss did not improve from -0.60845. Patience: 3/50
2024-12-09 14:42:38.790294: train_loss -0.6631
2024-12-09 14:42:38.791490: val_loss -0.5986
2024-12-09 14:42:38.792347: Pseudo dice [0.7706]
2024-12-09 14:42:38.793209: Epoch time: 91.05 s
2024-12-09 14:42:39.136433: Yayy! New best EMA pseudo Dice: 0.7552
2024-12-09 14:42:40.711848: 
2024-12-09 14:42:40.713177: Epoch 60
2024-12-09 14:42:40.714257: Current learning rate: 0.00946
2024-12-09 14:44:11.708608: Validation loss did not improve from -0.60845. Patience: 4/50
2024-12-09 14:44:11.709801: train_loss -0.6686
2024-12-09 14:44:11.710930: val_loss -0.5728
2024-12-09 14:44:11.711682: Pseudo dice [0.7606]
2024-12-09 14:44:11.712714: Epoch time: 91.0 s
2024-12-09 14:44:11.713599: Yayy! New best EMA pseudo Dice: 0.7558
2024-12-09 14:44:13.272875: 
2024-12-09 14:44:13.274233: Epoch 61
2024-12-09 14:44:13.275084: Current learning rate: 0.00945
2024-12-09 14:45:44.278191: Validation loss did not improve from -0.60845. Patience: 5/50
2024-12-09 14:45:44.279331: train_loss -0.6719
2024-12-09 14:45:44.280585: val_loss -0.604
2024-12-09 14:45:44.281545: Pseudo dice [0.7782]
2024-12-09 14:45:44.282428: Epoch time: 91.01 s
2024-12-09 14:45:44.283369: Yayy! New best EMA pseudo Dice: 0.758
2024-12-09 14:45:45.882571: 
2024-12-09 14:45:45.884015: Epoch 62
2024-12-09 14:45:45.885091: Current learning rate: 0.00944
2024-12-09 14:47:16.952554: Validation loss did not improve from -0.60845. Patience: 6/50
2024-12-09 14:47:16.953838: train_loss -0.666
2024-12-09 14:47:16.954781: val_loss -0.5829
2024-12-09 14:47:16.955501: Pseudo dice [0.7538]
2024-12-09 14:47:16.956194: Epoch time: 91.07 s
2024-12-09 14:47:18.181936: 
2024-12-09 14:47:18.183447: Epoch 63
2024-12-09 14:47:18.184330: Current learning rate: 0.00943
2024-12-09 14:48:49.008480: Validation loss did not improve from -0.60845. Patience: 7/50
2024-12-09 14:48:49.009548: train_loss -0.6689
2024-12-09 14:48:49.010365: val_loss -0.5538
2024-12-09 14:48:49.011158: Pseudo dice [0.7452]
2024-12-09 14:48:49.011867: Epoch time: 90.83 s
2024-12-09 14:48:50.260062: 
2024-12-09 14:48:50.261341: Epoch 64
2024-12-09 14:48:50.262122: Current learning rate: 0.00942
2024-12-09 14:50:21.065787: Validation loss did not improve from -0.60845. Patience: 8/50
2024-12-09 14:50:21.066482: train_loss -0.6701
2024-12-09 14:50:21.067486: val_loss -0.5857
2024-12-09 14:50:21.068239: Pseudo dice [0.7589]
2024-12-09 14:50:21.068944: Epoch time: 90.81 s
2024-12-09 14:50:22.881338: 
2024-12-09 14:50:22.882973: Epoch 65
2024-12-09 14:50:22.883700: Current learning rate: 0.00941
2024-12-09 14:51:53.596165: Validation loss did not improve from -0.60845. Patience: 9/50
2024-12-09 14:51:53.597371: train_loss -0.6708
2024-12-09 14:51:53.598309: val_loss -0.5841
2024-12-09 14:51:53.599146: Pseudo dice [0.7605]
2024-12-09 14:51:53.599808: Epoch time: 90.72 s
2024-12-09 14:51:54.848819: 
2024-12-09 14:51:54.850110: Epoch 66
2024-12-09 14:51:54.850816: Current learning rate: 0.0094
2024-12-09 14:53:25.837597: Validation loss did not improve from -0.60845. Patience: 10/50
2024-12-09 14:53:25.838645: train_loss -0.678
2024-12-09 14:53:25.839540: val_loss -0.5834
2024-12-09 14:53:25.840263: Pseudo dice [0.7573]
2024-12-09 14:53:25.841055: Epoch time: 90.99 s
2024-12-09 14:53:27.115028: 
2024-12-09 14:53:27.116812: Epoch 67
2024-12-09 14:53:27.117691: Current learning rate: 0.00939
2024-12-09 14:54:58.044818: Validation loss did not improve from -0.60845. Patience: 11/50
2024-12-09 14:54:58.045509: train_loss -0.6688
2024-12-09 14:54:58.046365: val_loss -0.5973
2024-12-09 14:54:58.047065: Pseudo dice [0.7678]
2024-12-09 14:54:58.047825: Epoch time: 90.93 s
2024-12-09 14:54:58.048506: Yayy! New best EMA pseudo Dice: 0.7581
2024-12-09 14:54:59.692350: 
2024-12-09 14:54:59.693681: Epoch 68
2024-12-09 14:54:59.694629: Current learning rate: 0.00939
2024-12-09 14:56:30.510773: Validation loss did not improve from -0.60845. Patience: 12/50
2024-12-09 14:56:30.511661: train_loss -0.6682
2024-12-09 14:56:30.512527: val_loss -0.6022
2024-12-09 14:56:30.513375: Pseudo dice [0.7775]
2024-12-09 14:56:30.514204: Epoch time: 90.82 s
2024-12-09 14:56:30.514908: Yayy! New best EMA pseudo Dice: 0.76
2024-12-09 14:56:32.200652: 
2024-12-09 14:56:32.202706: Epoch 69
2024-12-09 14:56:32.203805: Current learning rate: 0.00938
2024-12-09 14:58:02.920024: Validation loss did not improve from -0.60845. Patience: 13/50
2024-12-09 14:58:02.921148: train_loss -0.6814
2024-12-09 14:58:02.922062: val_loss -0.5843
2024-12-09 14:58:02.922745: Pseudo dice [0.7598]
2024-12-09 14:58:02.923468: Epoch time: 90.72 s
2024-12-09 14:58:04.877107: 
2024-12-09 14:58:04.879003: Epoch 70
2024-12-09 14:58:04.880138: Current learning rate: 0.00937
2024-12-09 14:59:35.789992: Validation loss did not improve from -0.60845. Patience: 14/50
2024-12-09 14:59:35.791069: train_loss -0.6783
2024-12-09 14:59:35.792112: val_loss -0.601
2024-12-09 14:59:35.793084: Pseudo dice [0.7725]
2024-12-09 14:59:35.794095: Epoch time: 90.91 s
2024-12-09 14:59:35.794800: Yayy! New best EMA pseudo Dice: 0.7613
2024-12-09 14:59:37.394758: 
2024-12-09 14:59:37.396101: Epoch 71
2024-12-09 14:59:37.396893: Current learning rate: 0.00936
2024-12-09 15:01:08.247935: Validation loss did not improve from -0.60845. Patience: 15/50
2024-12-09 15:01:08.249248: train_loss -0.6848
2024-12-09 15:01:08.250628: val_loss -0.605
2024-12-09 15:01:08.251452: Pseudo dice [0.771]
2024-12-09 15:01:08.252286: Epoch time: 90.86 s
2024-12-09 15:01:08.253099: Yayy! New best EMA pseudo Dice: 0.7622
2024-12-09 15:01:09.847382: 
2024-12-09 15:01:09.849003: Epoch 72
2024-12-09 15:01:09.849945: Current learning rate: 0.00935
2024-12-09 15:02:40.778301: Validation loss did not improve from -0.60845. Patience: 16/50
2024-12-09 15:02:40.779286: train_loss -0.6863
2024-12-09 15:02:40.780371: val_loss -0.6052
2024-12-09 15:02:40.781345: Pseudo dice [0.7673]
2024-12-09 15:02:40.782244: Epoch time: 90.93 s
2024-12-09 15:02:40.783185: Yayy! New best EMA pseudo Dice: 0.7627
2024-12-09 15:02:42.505392: 
2024-12-09 15:02:42.507260: Epoch 73
2024-12-09 15:02:42.508248: Current learning rate: 0.00934
2024-12-09 15:04:13.561009: Validation loss did not improve from -0.60845. Patience: 17/50
2024-12-09 15:04:13.561924: train_loss -0.6831
2024-12-09 15:04:13.563012: val_loss -0.5795
2024-12-09 15:04:13.563766: Pseudo dice [0.7613]
2024-12-09 15:04:13.564565: Epoch time: 91.06 s
2024-12-09 15:04:14.836585: 
2024-12-09 15:04:14.838391: Epoch 74
2024-12-09 15:04:14.839181: Current learning rate: 0.00933
2024-12-09 15:05:45.832805: Validation loss did not improve from -0.60845. Patience: 18/50
2024-12-09 15:05:45.833977: train_loss -0.6818
2024-12-09 15:05:45.834962: val_loss -0.5894
2024-12-09 15:05:45.835681: Pseudo dice [0.7697]
2024-12-09 15:05:45.836429: Epoch time: 91.0 s
2024-12-09 15:05:46.226799: Yayy! New best EMA pseudo Dice: 0.7633
2024-12-09 15:05:47.858370: 
2024-12-09 15:05:47.859833: Epoch 75
2024-12-09 15:05:47.860838: Current learning rate: 0.00932
2024-12-09 15:07:18.961927: Validation loss did not improve from -0.60845. Patience: 19/50
2024-12-09 15:07:18.962602: train_loss -0.6839
2024-12-09 15:07:18.963429: val_loss -0.5932
2024-12-09 15:07:18.964185: Pseudo dice [0.7697]
2024-12-09 15:07:18.964795: Epoch time: 91.11 s
2024-12-09 15:07:18.965583: Yayy! New best EMA pseudo Dice: 0.7639
2024-12-09 15:07:20.596439: 
2024-12-09 15:07:20.597983: Epoch 76
2024-12-09 15:07:20.598756: Current learning rate: 0.00931
2024-12-09 15:08:51.635383: Validation loss improved from -0.60845 to -0.61156! Patience: 19/50
2024-12-09 15:08:51.636388: train_loss -0.6887
2024-12-09 15:08:51.637423: val_loss -0.6116
2024-12-09 15:08:51.638546: Pseudo dice [0.7758]
2024-12-09 15:08:51.639435: Epoch time: 91.04 s
2024-12-09 15:08:51.640284: Yayy! New best EMA pseudo Dice: 0.7651
2024-12-09 15:08:53.272226: 
2024-12-09 15:08:53.273849: Epoch 77
2024-12-09 15:08:53.274967: Current learning rate: 0.0093
2024-12-09 15:10:24.353607: Validation loss did not improve from -0.61156. Patience: 1/50
2024-12-09 15:10:24.354395: train_loss -0.6833
2024-12-09 15:10:24.355317: val_loss -0.5873
2024-12-09 15:10:24.355948: Pseudo dice [0.7667]
2024-12-09 15:10:24.356627: Epoch time: 91.08 s
2024-12-09 15:10:24.357419: Yayy! New best EMA pseudo Dice: 0.7653
2024-12-09 15:10:26.034525: 
2024-12-09 15:10:26.036235: Epoch 78
2024-12-09 15:10:26.037087: Current learning rate: 0.0093
2024-12-09 15:11:57.135189: Validation loss improved from -0.61156 to -0.61499! Patience: 1/50
2024-12-09 15:11:57.136201: train_loss -0.6867
2024-12-09 15:11:57.137401: val_loss -0.615
2024-12-09 15:11:57.138665: Pseudo dice [0.7829]
2024-12-09 15:11:57.139748: Epoch time: 91.1 s
2024-12-09 15:11:57.140521: Yayy! New best EMA pseudo Dice: 0.767
2024-12-09 15:11:58.828821: 
2024-12-09 15:11:58.831592: Epoch 79
2024-12-09 15:11:58.832609: Current learning rate: 0.00929
2024-12-09 15:13:30.050565: Validation loss did not improve from -0.61499. Patience: 1/50
2024-12-09 15:13:30.051761: train_loss -0.6996
2024-12-09 15:13:30.052692: val_loss -0.6071
2024-12-09 15:13:30.053468: Pseudo dice [0.7726]
2024-12-09 15:13:30.054203: Epoch time: 91.22 s
2024-12-09 15:13:30.497198: Yayy! New best EMA pseudo Dice: 0.7676
2024-12-09 15:13:32.658746: 
2024-12-09 15:13:32.660100: Epoch 80
2024-12-09 15:13:32.660752: Current learning rate: 0.00928
2024-12-09 15:15:03.718728: Validation loss did not improve from -0.61499. Patience: 2/50
2024-12-09 15:15:03.719656: train_loss -0.69
2024-12-09 15:15:03.720654: val_loss -0.5818
2024-12-09 15:15:03.721529: Pseudo dice [0.7721]
2024-12-09 15:15:03.722448: Epoch time: 91.06 s
2024-12-09 15:15:03.723282: Yayy! New best EMA pseudo Dice: 0.7681
2024-12-09 15:15:05.357390: 
2024-12-09 15:15:05.359560: Epoch 81
2024-12-09 15:15:05.360612: Current learning rate: 0.00927
2024-12-09 15:16:36.554849: Validation loss did not improve from -0.61499. Patience: 3/50
2024-12-09 15:16:36.556848: train_loss -0.694
2024-12-09 15:16:36.557879: val_loss -0.5965
2024-12-09 15:16:36.558789: Pseudo dice [0.7704]
2024-12-09 15:16:36.559517: Epoch time: 91.2 s
2024-12-09 15:16:36.560383: Yayy! New best EMA pseudo Dice: 0.7683
2024-12-09 15:16:38.212800: 
2024-12-09 15:16:38.214597: Epoch 82
2024-12-09 15:16:38.215421: Current learning rate: 0.00926
2024-12-09 15:18:09.723035: Validation loss did not improve from -0.61499. Patience: 4/50
2024-12-09 15:18:09.726948: train_loss -0.7062
2024-12-09 15:18:09.728516: val_loss -0.5932
2024-12-09 15:18:09.729259: Pseudo dice [0.7637]
2024-12-09 15:18:09.730060: Epoch time: 91.51 s
2024-12-09 15:18:10.956095: 
2024-12-09 15:18:10.958144: Epoch 83
2024-12-09 15:18:10.958944: Current learning rate: 0.00925
2024-12-09 15:19:42.366052: Validation loss did not improve from -0.61499. Patience: 5/50
2024-12-09 15:19:42.367920: train_loss -0.6997
2024-12-09 15:19:42.370796: val_loss -0.5815
2024-12-09 15:19:42.371869: Pseudo dice [0.7624]
2024-12-09 15:19:42.373495: Epoch time: 91.41 s
2024-12-09 15:19:43.648657: 
2024-12-09 15:19:43.650311: Epoch 84
2024-12-09 15:19:43.651487: Current learning rate: 0.00924
2024-12-09 15:21:14.856425: Validation loss did not improve from -0.61499. Patience: 6/50
2024-12-09 15:21:14.857495: train_loss -0.702
2024-12-09 15:21:14.858406: val_loss -0.6122
2024-12-09 15:21:14.859153: Pseudo dice [0.7778]
2024-12-09 15:21:14.859973: Epoch time: 91.21 s
2024-12-09 15:21:15.311865: Yayy! New best EMA pseudo Dice: 0.7683
2024-12-09 15:21:16.824938: 
2024-12-09 15:21:16.826404: Epoch 85
2024-12-09 15:21:16.827526: Current learning rate: 0.00923
2024-12-09 15:22:47.901889: Validation loss did not improve from -0.61499. Patience: 7/50
2024-12-09 15:22:47.902948: train_loss -0.7071
2024-12-09 15:22:47.903703: val_loss -0.5985
2024-12-09 15:22:47.904438: Pseudo dice [0.7738]
2024-12-09 15:22:47.905215: Epoch time: 91.08 s
2024-12-09 15:22:47.905987: Yayy! New best EMA pseudo Dice: 0.7689
2024-12-09 15:22:49.472229: 
2024-12-09 15:22:49.473843: Epoch 86
2024-12-09 15:22:49.474812: Current learning rate: 0.00922
2024-12-09 15:24:20.591130: Validation loss did not improve from -0.61499. Patience: 8/50
2024-12-09 15:24:20.592107: train_loss -0.7034
2024-12-09 15:24:20.593036: val_loss -0.5884
2024-12-09 15:24:20.593812: Pseudo dice [0.7663]
2024-12-09 15:24:20.594608: Epoch time: 91.12 s
2024-12-09 15:24:21.784064: 
2024-12-09 15:24:21.786780: Epoch 87
2024-12-09 15:24:21.787996: Current learning rate: 0.00921
2024-12-09 15:25:52.940134: Validation loss did not improve from -0.61499. Patience: 9/50
2024-12-09 15:25:52.941424: train_loss -0.7042
2024-12-09 15:25:52.942819: val_loss -0.5918
2024-12-09 15:25:52.944022: Pseudo dice [0.7682]
2024-12-09 15:25:52.945185: Epoch time: 91.16 s
2024-12-09 15:25:54.159991: 
2024-12-09 15:25:54.161681: Epoch 88
2024-12-09 15:25:54.162733: Current learning rate: 0.0092
2024-12-09 15:27:25.256977: Validation loss did not improve from -0.61499. Patience: 10/50
2024-12-09 15:27:25.258250: train_loss -0.708
2024-12-09 15:27:25.259172: val_loss -0.587
2024-12-09 15:27:25.259856: Pseudo dice [0.7665]
2024-12-09 15:27:25.260503: Epoch time: 91.1 s
2024-12-09 15:27:26.452047: 
2024-12-09 15:27:26.453848: Epoch 89
2024-12-09 15:27:26.454893: Current learning rate: 0.0092
2024-12-09 15:28:57.595129: Validation loss did not improve from -0.61499. Patience: 11/50
2024-12-09 15:28:57.595963: train_loss -0.7034
2024-12-09 15:28:57.596786: val_loss -0.6058
2024-12-09 15:28:57.597456: Pseudo dice [0.7795]
2024-12-09 15:28:57.598291: Epoch time: 91.14 s
2024-12-09 15:28:57.948686: Yayy! New best EMA pseudo Dice: 0.7695
2024-12-09 15:28:59.518927: 
2024-12-09 15:28:59.520252: Epoch 90
2024-12-09 15:28:59.521029: Current learning rate: 0.00919
2024-12-09 15:30:30.591859: Validation loss did not improve from -0.61499. Patience: 12/50
2024-12-09 15:30:30.593028: train_loss -0.6983
2024-12-09 15:30:30.593872: val_loss -0.5864
2024-12-09 15:30:30.594647: Pseudo dice [0.7591]
2024-12-09 15:30:30.595290: Epoch time: 91.08 s
2024-12-09 15:30:32.112510: 
2024-12-09 15:30:32.114395: Epoch 91
2024-12-09 15:30:32.115208: Current learning rate: 0.00918
2024-12-09 15:32:03.167711: Validation loss did not improve from -0.61499. Patience: 13/50
2024-12-09 15:32:03.168808: train_loss -0.6998
2024-12-09 15:32:03.169983: val_loss -0.602
2024-12-09 15:32:03.170907: Pseudo dice [0.7747]
2024-12-09 15:32:03.171902: Epoch time: 91.06 s
2024-12-09 15:32:04.323113: 
2024-12-09 15:32:04.325375: Epoch 92
2024-12-09 15:32:04.326532: Current learning rate: 0.00917
2024-12-09 15:33:35.480549: Validation loss did not improve from -0.61499. Patience: 14/50
2024-12-09 15:33:35.481619: train_loss -0.7087
2024-12-09 15:33:35.482469: val_loss -0.5947
2024-12-09 15:33:35.483199: Pseudo dice [0.7709]
2024-12-09 15:33:35.483900: Epoch time: 91.16 s
2024-12-09 15:33:36.632598: 
2024-12-09 15:33:36.634079: Epoch 93
2024-12-09 15:33:36.634843: Current learning rate: 0.00916
2024-12-09 15:35:07.701067: Validation loss did not improve from -0.61499. Patience: 15/50
2024-12-09 15:35:07.701961: train_loss -0.7072
2024-12-09 15:35:07.702763: val_loss -0.6121
2024-12-09 15:35:07.703397: Pseudo dice [0.7752]
2024-12-09 15:35:07.704031: Epoch time: 91.07 s
2024-12-09 15:35:07.704629: Yayy! New best EMA pseudo Dice: 0.7699
2024-12-09 15:35:09.251322: 
2024-12-09 15:35:09.253409: Epoch 94
2024-12-09 15:35:09.254185: Current learning rate: 0.00915
2024-12-09 15:36:40.367703: Validation loss did not improve from -0.61499. Patience: 16/50
2024-12-09 15:36:40.368561: train_loss -0.7027
2024-12-09 15:36:40.369627: val_loss -0.5804
2024-12-09 15:36:40.370587: Pseudo dice [0.7629]
2024-12-09 15:36:40.371536: Epoch time: 91.12 s
2024-12-09 15:36:41.904217: 
2024-12-09 15:36:41.906264: Epoch 95
2024-12-09 15:36:41.907314: Current learning rate: 0.00914
2024-12-09 15:38:12.971195: Validation loss did not improve from -0.61499. Patience: 17/50
2024-12-09 15:38:12.972221: train_loss -0.7068
2024-12-09 15:38:12.973359: val_loss -0.6045
2024-12-09 15:38:12.974412: Pseudo dice [0.7774]
2024-12-09 15:38:12.975284: Epoch time: 91.07 s
2024-12-09 15:38:12.976140: Yayy! New best EMA pseudo Dice: 0.77
2024-12-09 15:38:14.497923: 
2024-12-09 15:38:14.499334: Epoch 96
2024-12-09 15:38:14.500409: Current learning rate: 0.00913
2024-12-09 15:39:45.346660: Validation loss did not improve from -0.61499. Patience: 18/50
2024-12-09 15:39:45.347761: train_loss -0.7109
2024-12-09 15:39:45.348932: val_loss -0.6059
2024-12-09 15:39:45.349762: Pseudo dice [0.7719]
2024-12-09 15:39:45.350520: Epoch time: 90.85 s
2024-12-09 15:39:45.351613: Yayy! New best EMA pseudo Dice: 0.7702
2024-12-09 15:39:46.880141: 
2024-12-09 15:39:46.881942: Epoch 97
2024-12-09 15:39:46.883035: Current learning rate: 0.00912
2024-12-09 15:41:17.560613: Validation loss did not improve from -0.61499. Patience: 19/50
2024-12-09 15:41:17.561810: train_loss -0.704
2024-12-09 15:41:17.563014: val_loss -0.5818
2024-12-09 15:41:17.563840: Pseudo dice [0.7603]
2024-12-09 15:41:17.564660: Epoch time: 90.68 s
2024-12-09 15:41:18.755922: 
2024-12-09 15:41:18.759137: Epoch 98
2024-12-09 15:41:18.760716: Current learning rate: 0.00911
2024-12-09 15:42:48.353714: Validation loss did not improve from -0.61499. Patience: 20/50
2024-12-09 15:42:48.355126: train_loss -0.7084
2024-12-09 15:42:48.356005: val_loss -0.5918
2024-12-09 15:42:48.357328: Pseudo dice [0.769]
2024-12-09 15:42:48.358304: Epoch time: 89.6 s
2024-12-09 15:42:49.520945: 
2024-12-09 15:42:49.522938: Epoch 99
2024-12-09 15:42:49.523916: Current learning rate: 0.0091
2024-12-09 15:44:18.208157: Validation loss did not improve from -0.61499. Patience: 21/50
2024-12-09 15:44:18.209244: train_loss -0.713
2024-12-09 15:44:18.210249: val_loss -0.6041
2024-12-09 15:44:18.211279: Pseudo dice [0.776]
2024-12-09 15:44:18.212190: Epoch time: 88.69 s
2024-12-09 15:44:19.702224: 
2024-12-09 15:44:19.704240: Epoch 100
2024-12-09 15:44:19.705277: Current learning rate: 0.0091
2024-12-09 15:45:47.973074: Validation loss did not improve from -0.61499. Patience: 22/50
2024-12-09 15:45:47.974179: train_loss -0.7191
2024-12-09 15:45:47.975109: val_loss -0.5962
2024-12-09 15:45:47.976033: Pseudo dice [0.7694]
2024-12-09 15:45:47.976918: Epoch time: 88.27 s
2024-12-09 15:45:49.152575: 
2024-12-09 15:45:49.154137: Epoch 101
2024-12-09 15:45:49.155558: Current learning rate: 0.00909
2024-12-09 15:47:17.700660: Validation loss did not improve from -0.61499. Patience: 23/50
2024-12-09 15:47:17.701781: train_loss -0.7151
2024-12-09 15:47:17.703047: val_loss -0.6076
2024-12-09 15:47:17.703917: Pseudo dice [0.7804]
2024-12-09 15:47:17.704863: Epoch time: 88.55 s
2024-12-09 15:47:17.705679: Yayy! New best EMA pseudo Dice: 0.7709
2024-12-09 15:47:19.737712: 
2024-12-09 15:47:19.739567: Epoch 102
2024-12-09 15:47:19.741047: Current learning rate: 0.00908
2024-12-09 15:48:50.446726: Validation loss improved from -0.61499 to -0.61597! Patience: 23/50
2024-12-09 15:48:50.447782: train_loss -0.7244
2024-12-09 15:48:50.448931: val_loss -0.616
2024-12-09 15:48:50.450055: Pseudo dice [0.7794]
2024-12-09 15:48:50.451167: Epoch time: 90.71 s
2024-12-09 15:48:50.452232: Yayy! New best EMA pseudo Dice: 0.7717
2024-12-09 15:48:51.952992: 
2024-12-09 15:48:51.954551: Epoch 103
2024-12-09 15:48:51.955888: Current learning rate: 0.00907
2024-12-09 15:50:22.488208: Validation loss did not improve from -0.61597. Patience: 1/50
2024-12-09 15:50:22.489531: train_loss -0.7192
2024-12-09 15:50:22.490672: val_loss -0.5986
2024-12-09 15:50:22.491470: Pseudo dice [0.7784]
2024-12-09 15:50:22.492262: Epoch time: 90.54 s
2024-12-09 15:50:22.493059: Yayy! New best EMA pseudo Dice: 0.7724
2024-12-09 15:50:23.972522: 
2024-12-09 15:50:23.974219: Epoch 104
2024-12-09 15:50:23.975717: Current learning rate: 0.00906
2024-12-09 15:51:54.646946: Validation loss did not improve from -0.61597. Patience: 2/50
2024-12-09 15:51:54.648204: train_loss -0.7157
2024-12-09 15:51:54.649575: val_loss -0.6092
2024-12-09 15:51:54.650629: Pseudo dice [0.7763]
2024-12-09 15:51:54.651845: Epoch time: 90.68 s
2024-12-09 15:51:55.001517: Yayy! New best EMA pseudo Dice: 0.7728
2024-12-09 15:51:56.510506: 
2024-12-09 15:51:56.512788: Epoch 105
2024-12-09 15:51:56.514201: Current learning rate: 0.00905
2024-12-09 15:53:26.805661: Validation loss did not improve from -0.61597. Patience: 3/50
2024-12-09 15:53:26.806871: train_loss -0.719
2024-12-09 15:53:26.808345: val_loss -0.6033
2024-12-09 15:53:26.809252: Pseudo dice [0.7732]
2024-12-09 15:53:26.810338: Epoch time: 90.3 s
2024-12-09 15:53:26.811410: Yayy! New best EMA pseudo Dice: 0.7728
2024-12-09 15:53:28.310891: 
2024-12-09 15:53:28.312667: Epoch 106
2024-12-09 15:53:28.313549: Current learning rate: 0.00904
2024-12-09 15:54:58.499038: Validation loss did not improve from -0.61597. Patience: 4/50
2024-12-09 15:54:58.500080: train_loss -0.7188
2024-12-09 15:54:58.501028: val_loss -0.587
2024-12-09 15:54:58.502006: Pseudo dice [0.7658]
2024-12-09 15:54:58.503169: Epoch time: 90.19 s
2024-12-09 15:54:59.645115: 
2024-12-09 15:54:59.647367: Epoch 107
2024-12-09 15:54:59.648443: Current learning rate: 0.00903
2024-12-09 15:56:30.120081: Validation loss did not improve from -0.61597. Patience: 5/50
2024-12-09 15:56:30.121331: train_loss -0.7157
2024-12-09 15:56:30.122194: val_loss -0.5897
2024-12-09 15:56:30.122896: Pseudo dice [0.764]
2024-12-09 15:56:30.123620: Epoch time: 90.48 s
2024-12-09 15:56:31.263772: 
2024-12-09 15:56:31.265684: Epoch 108
2024-12-09 15:56:31.266678: Current learning rate: 0.00902
2024-12-09 15:58:01.615539: Validation loss did not improve from -0.61597. Patience: 6/50
2024-12-09 15:58:01.616649: train_loss -0.7264
2024-12-09 15:58:01.617810: val_loss -0.6108
2024-12-09 15:58:01.619084: Pseudo dice [0.7842]
2024-12-09 15:58:01.620108: Epoch time: 90.35 s
2024-12-09 15:58:02.783606: 
2024-12-09 15:58:02.785418: Epoch 109
2024-12-09 15:58:02.786725: Current learning rate: 0.00901
2024-12-09 15:59:33.004075: Validation loss improved from -0.61597 to -0.62289! Patience: 6/50
2024-12-09 15:59:33.004914: train_loss -0.7234
2024-12-09 15:59:33.005902: val_loss -0.6229
2024-12-09 15:59:33.006946: Pseudo dice [0.7888]
2024-12-09 15:59:33.007794: Epoch time: 90.22 s
2024-12-09 15:59:33.366257: Yayy! New best EMA pseudo Dice: 0.7742
2024-12-09 15:59:34.838516: 
2024-12-09 15:59:34.840359: Epoch 110
2024-12-09 15:59:34.841418: Current learning rate: 0.009
2024-12-09 16:01:05.124924: Validation loss did not improve from -0.62289. Patience: 1/50
2024-12-09 16:01:05.126155: train_loss -0.7303
2024-12-09 16:01:05.126976: val_loss -0.6049
2024-12-09 16:01:05.127705: Pseudo dice [0.7727]
2024-12-09 16:01:05.128453: Epoch time: 90.29 s
2024-12-09 16:01:06.283208: 
2024-12-09 16:01:06.285187: Epoch 111
2024-12-09 16:01:06.286454: Current learning rate: 0.009
2024-12-09 16:02:36.561758: Validation loss did not improve from -0.62289. Patience: 2/50
2024-12-09 16:02:36.562703: train_loss -0.7284
2024-12-09 16:02:36.563848: val_loss -0.6062
2024-12-09 16:02:36.564741: Pseudo dice [0.7766]
2024-12-09 16:02:36.565750: Epoch time: 90.28 s
2024-12-09 16:02:36.566812: Yayy! New best EMA pseudo Dice: 0.7743
2024-12-09 16:02:38.100515: 
2024-12-09 16:02:38.102534: Epoch 112
2024-12-09 16:02:38.103490: Current learning rate: 0.00899
2024-12-09 16:04:08.579973: Validation loss did not improve from -0.62289. Patience: 3/50
2024-12-09 16:04:08.581344: train_loss -0.7283
2024-12-09 16:04:08.582593: val_loss -0.6199
2024-12-09 16:04:08.583480: Pseudo dice [0.7848]
2024-12-09 16:04:08.584556: Epoch time: 90.48 s
2024-12-09 16:04:08.585591: Yayy! New best EMA pseudo Dice: 0.7754
2024-12-09 16:04:10.455168: 
2024-12-09 16:04:10.457571: Epoch 113
2024-12-09 16:04:10.459335: Current learning rate: 0.00898
2024-12-09 16:05:41.158634: Validation loss did not improve from -0.62289. Patience: 4/50
2024-12-09 16:05:41.159709: train_loss -0.7309
2024-12-09 16:05:41.160766: val_loss -0.571
2024-12-09 16:05:41.161740: Pseudo dice [0.761]
2024-12-09 16:05:41.162768: Epoch time: 90.71 s
2024-12-09 16:05:42.426941: 
2024-12-09 16:05:42.428726: Epoch 114
2024-12-09 16:05:42.429909: Current learning rate: 0.00897
2024-12-09 16:07:12.895938: Validation loss did not improve from -0.62289. Patience: 5/50
2024-12-09 16:07:12.897190: train_loss -0.733
2024-12-09 16:07:12.898246: val_loss -0.5956
2024-12-09 16:07:12.899034: Pseudo dice [0.7695]
2024-12-09 16:07:12.899912: Epoch time: 90.47 s
2024-12-09 16:07:14.436129: 
2024-12-09 16:07:14.438303: Epoch 115
2024-12-09 16:07:14.439178: Current learning rate: 0.00896
2024-12-09 16:08:44.797269: Validation loss improved from -0.62289 to -0.62316! Patience: 5/50
2024-12-09 16:08:44.798499: train_loss -0.7244
2024-12-09 16:08:44.799846: val_loss -0.6232
2024-12-09 16:08:44.801107: Pseudo dice [0.7824]
2024-12-09 16:08:44.802193: Epoch time: 90.36 s
2024-12-09 16:08:46.002967: 
2024-12-09 16:08:46.004471: Epoch 116
2024-12-09 16:08:46.005925: Current learning rate: 0.00895
2024-12-09 16:10:16.328250: Validation loss did not improve from -0.62316. Patience: 1/50
2024-12-09 16:10:16.329504: train_loss -0.7292
2024-12-09 16:10:16.330591: val_loss -0.5802
2024-12-09 16:10:16.331698: Pseudo dice [0.7666]
2024-12-09 16:10:16.332782: Epoch time: 90.33 s
2024-12-09 16:10:17.514087: 
2024-12-09 16:10:17.515936: Epoch 117
2024-12-09 16:10:17.517356: Current learning rate: 0.00894
2024-12-09 16:11:47.819746: Validation loss did not improve from -0.62316. Patience: 2/50
2024-12-09 16:11:47.820848: train_loss -0.7263
2024-12-09 16:11:47.821886: val_loss -0.6017
2024-12-09 16:11:47.822567: Pseudo dice [0.7665]
2024-12-09 16:11:47.823417: Epoch time: 90.31 s
2024-12-09 16:11:49.017273: 
2024-12-09 16:11:49.019220: Epoch 118
2024-12-09 16:11:49.020242: Current learning rate: 0.00893
2024-12-09 16:13:19.372254: Validation loss did not improve from -0.62316. Patience: 3/50
2024-12-09 16:13:19.373494: train_loss -0.7332
2024-12-09 16:13:19.374780: val_loss -0.6078
2024-12-09 16:13:19.375773: Pseudo dice [0.7763]
2024-12-09 16:13:19.376782: Epoch time: 90.36 s
2024-12-09 16:13:20.594172: 
2024-12-09 16:13:20.596286: Epoch 119
2024-12-09 16:13:20.597563: Current learning rate: 0.00892
2024-12-09 16:14:50.927085: Validation loss improved from -0.62316 to -0.63124! Patience: 3/50
2024-12-09 16:14:50.928167: train_loss -0.7274
2024-12-09 16:14:50.929071: val_loss -0.6312
2024-12-09 16:14:50.929824: Pseudo dice [0.7831]
2024-12-09 16:14:50.930705: Epoch time: 90.33 s
2024-12-09 16:14:52.460938: 
2024-12-09 16:14:52.462626: Epoch 120
2024-12-09 16:14:52.463845: Current learning rate: 0.00891
2024-12-09 16:16:22.718167: Validation loss did not improve from -0.63124. Patience: 1/50
2024-12-09 16:16:22.719379: train_loss -0.7358
2024-12-09 16:16:22.720344: val_loss -0.6197
2024-12-09 16:16:22.721129: Pseudo dice [0.7908]
2024-12-09 16:16:22.721794: Epoch time: 90.26 s
2024-12-09 16:16:22.722579: Yayy! New best EMA pseudo Dice: 0.7759
2024-12-09 16:16:24.285579: 
2024-12-09 16:16:24.287777: Epoch 121
2024-12-09 16:16:24.289051: Current learning rate: 0.0089
2024-12-09 16:17:54.664373: Validation loss did not improve from -0.63124. Patience: 2/50
2024-12-09 16:17:54.665631: train_loss -0.7342
2024-12-09 16:17:54.666608: val_loss -0.5928
2024-12-09 16:17:54.667588: Pseudo dice [0.7732]
2024-12-09 16:17:54.668626: Epoch time: 90.38 s
2024-12-09 16:17:55.856039: 
2024-12-09 16:17:55.857815: Epoch 122
2024-12-09 16:17:55.859003: Current learning rate: 0.00889
2024-12-09 16:19:26.256189: Validation loss did not improve from -0.63124. Patience: 3/50
2024-12-09 16:19:26.257167: train_loss -0.7314
2024-12-09 16:19:26.258315: val_loss -0.5897
2024-12-09 16:19:26.259211: Pseudo dice [0.77]
2024-12-09 16:19:26.260123: Epoch time: 90.4 s
2024-12-09 16:19:27.446914: 
2024-12-09 16:19:27.448631: Epoch 123
2024-12-09 16:19:27.449720: Current learning rate: 0.00889
2024-12-09 16:20:58.053018: Validation loss did not improve from -0.63124. Patience: 4/50
2024-12-09 16:20:58.054361: train_loss -0.733
2024-12-09 16:20:58.055912: val_loss -0.5847
2024-12-09 16:20:58.057222: Pseudo dice [0.7648]
2024-12-09 16:20:58.058524: Epoch time: 90.61 s
2024-12-09 16:20:59.850922: 
2024-12-09 16:20:59.852616: Epoch 124
2024-12-09 16:20:59.853918: Current learning rate: 0.00888
2024-12-09 16:22:30.471477: Validation loss did not improve from -0.63124. Patience: 5/50
2024-12-09 16:22:30.472543: train_loss -0.7306
2024-12-09 16:22:30.473468: val_loss -0.5938
2024-12-09 16:22:30.474268: Pseudo dice [0.7578]
2024-12-09 16:22:30.475231: Epoch time: 90.62 s
2024-12-09 16:22:32.042202: 
2024-12-09 16:22:32.044050: Epoch 125
2024-12-09 16:22:32.045270: Current learning rate: 0.00887
2024-12-09 16:24:02.639447: Validation loss did not improve from -0.63124. Patience: 6/50
2024-12-09 16:24:02.640896: train_loss -0.7334
2024-12-09 16:24:02.641930: val_loss -0.6184
2024-12-09 16:24:02.642734: Pseudo dice [0.7814]
2024-12-09 16:24:02.643605: Epoch time: 90.6 s
2024-12-09 16:24:03.927184: 
2024-12-09 16:24:03.929124: Epoch 126
2024-12-09 16:24:03.930396: Current learning rate: 0.00886
2024-12-09 16:25:34.774719: Validation loss did not improve from -0.63124. Patience: 7/50
2024-12-09 16:25:34.775766: train_loss -0.7405
2024-12-09 16:25:34.776885: val_loss -0.5592
2024-12-09 16:25:34.777816: Pseudo dice [0.7476]
2024-12-09 16:25:34.778590: Epoch time: 90.85 s
2024-12-09 16:25:35.973544: 
2024-12-09 16:25:35.975366: Epoch 127
2024-12-09 16:25:35.976519: Current learning rate: 0.00885
2024-12-09 16:27:06.886151: Validation loss did not improve from -0.63124. Patience: 8/50
2024-12-09 16:27:06.887261: train_loss -0.7376
2024-12-09 16:27:06.888435: val_loss -0.5925
2024-12-09 16:27:06.889559: Pseudo dice [0.7686]
2024-12-09 16:27:06.890380: Epoch time: 90.91 s
2024-12-09 16:27:08.088801: 
2024-12-09 16:27:08.090671: Epoch 128
2024-12-09 16:27:08.091875: Current learning rate: 0.00884
2024-12-09 16:28:38.910905: Validation loss did not improve from -0.63124. Patience: 9/50
2024-12-09 16:28:38.912078: train_loss -0.7402
2024-12-09 16:28:38.913170: val_loss -0.5879
2024-12-09 16:28:38.913982: Pseudo dice [0.767]
2024-12-09 16:28:38.914903: Epoch time: 90.82 s
2024-12-09 16:28:40.162698: 
2024-12-09 16:28:40.164317: Epoch 129
2024-12-09 16:28:40.165758: Current learning rate: 0.00883
2024-12-09 16:30:11.016306: Validation loss did not improve from -0.63124. Patience: 10/50
2024-12-09 16:30:11.017838: train_loss -0.7387
2024-12-09 16:30:11.019294: val_loss -0.6081
2024-12-09 16:30:11.019913: Pseudo dice [0.7764]
2024-12-09 16:30:11.020705: Epoch time: 90.86 s
2024-12-09 16:30:12.704323: 
2024-12-09 16:30:12.706156: Epoch 130
2024-12-09 16:30:12.707059: Current learning rate: 0.00882
2024-12-09 16:31:43.421208: Validation loss did not improve from -0.63124. Patience: 11/50
2024-12-09 16:31:43.422079: train_loss -0.7386
2024-12-09 16:31:43.423154: val_loss -0.5829
2024-12-09 16:31:43.424014: Pseudo dice [0.7647]
2024-12-09 16:31:43.424824: Epoch time: 90.72 s
2024-12-09 16:31:44.664333: 
2024-12-09 16:31:44.666459: Epoch 131
2024-12-09 16:31:44.667392: Current learning rate: 0.00881
2024-12-09 16:33:15.339195: Validation loss did not improve from -0.63124. Patience: 12/50
2024-12-09 16:33:15.340197: train_loss -0.7359
2024-12-09 16:33:15.341128: val_loss -0.5826
2024-12-09 16:33:15.341923: Pseudo dice [0.7627]
2024-12-09 16:33:15.342675: Epoch time: 90.68 s
2024-12-09 16:33:16.554249: 
2024-12-09 16:33:16.556611: Epoch 132
2024-12-09 16:33:16.558186: Current learning rate: 0.0088
2024-12-09 16:34:47.182082: Validation loss did not improve from -0.63124. Patience: 13/50
2024-12-09 16:34:47.183472: train_loss -0.7374
2024-12-09 16:34:47.184638: val_loss -0.6095
2024-12-09 16:34:47.185442: Pseudo dice [0.775]
2024-12-09 16:34:47.186481: Epoch time: 90.63 s
2024-12-09 16:34:48.375904: 
2024-12-09 16:34:48.377591: Epoch 133
2024-12-09 16:34:48.378861: Current learning rate: 0.00879
2024-12-09 16:36:18.921109: Validation loss did not improve from -0.63124. Patience: 14/50
2024-12-09 16:36:18.922513: train_loss -0.748
2024-12-09 16:36:18.923913: val_loss -0.6186
2024-12-09 16:36:18.925032: Pseudo dice [0.7848]
2024-12-09 16:36:18.926236: Epoch time: 90.55 s
2024-12-09 16:36:20.165244: 
2024-12-09 16:36:20.167439: Epoch 134
2024-12-09 16:36:20.168652: Current learning rate: 0.00879
2024-12-09 16:37:50.750830: Validation loss did not improve from -0.63124. Patience: 15/50
2024-12-09 16:37:50.751824: train_loss -0.743
2024-12-09 16:37:50.752668: val_loss -0.6167
2024-12-09 16:37:50.753642: Pseudo dice [0.7863]
2024-12-09 16:37:50.754409: Epoch time: 90.59 s
2024-12-09 16:37:52.790867: 
2024-12-09 16:37:52.792850: Epoch 135
2024-12-09 16:37:52.793926: Current learning rate: 0.00878
2024-12-09 16:39:23.346394: Validation loss did not improve from -0.63124. Patience: 16/50
2024-12-09 16:39:23.347556: train_loss -0.7412
2024-12-09 16:39:23.348559: val_loss -0.5775
2024-12-09 16:39:23.349473: Pseudo dice [0.7642]
2024-12-09 16:39:23.350409: Epoch time: 90.56 s
2024-12-09 16:39:24.558092: 
2024-12-09 16:39:24.559545: Epoch 136
2024-12-09 16:39:24.560587: Current learning rate: 0.00877
2024-12-09 16:40:55.062936: Validation loss did not improve from -0.63124. Patience: 17/50
2024-12-09 16:40:55.064078: train_loss -0.7435
2024-12-09 16:40:55.065215: val_loss -0.621
2024-12-09 16:40:55.066306: Pseudo dice [0.7852]
2024-12-09 16:40:55.067277: Epoch time: 90.51 s
2024-12-09 16:40:56.277109: 
2024-12-09 16:40:56.279030: Epoch 137
2024-12-09 16:40:56.280270: Current learning rate: 0.00876
2024-12-09 16:42:26.689633: Validation loss did not improve from -0.63124. Patience: 18/50
2024-12-09 16:42:26.690814: train_loss -0.7365
2024-12-09 16:42:26.691729: val_loss -0.6075
2024-12-09 16:42:26.692462: Pseudo dice [0.7761]
2024-12-09 16:42:26.693320: Epoch time: 90.41 s
2024-12-09 16:42:27.916625: 
2024-12-09 16:42:27.918279: Epoch 138
2024-12-09 16:42:27.919581: Current learning rate: 0.00875
2024-12-09 16:43:58.232178: Validation loss did not improve from -0.63124. Patience: 19/50
2024-12-09 16:43:58.233402: train_loss -0.7287
2024-12-09 16:43:58.234605: val_loss -0.5812
2024-12-09 16:43:58.235645: Pseudo dice [0.7552]
2024-12-09 16:43:58.236653: Epoch time: 90.32 s
2024-12-09 16:43:59.416756: 
2024-12-09 16:43:59.418347: Epoch 139
2024-12-09 16:43:59.419704: Current learning rate: 0.00874
2024-12-09 16:45:32.099252: Validation loss did not improve from -0.63124. Patience: 20/50
2024-12-09 16:45:32.103870: train_loss -0.7324
2024-12-09 16:45:32.106540: val_loss -0.5878
2024-12-09 16:45:32.107402: Pseudo dice [0.7619]
2024-12-09 16:45:32.115036: Epoch time: 92.69 s
2024-12-09 16:45:34.211151: 
2024-12-09 16:45:34.212396: Epoch 140
2024-12-09 16:45:34.213336: Current learning rate: 0.00873
2024-12-09 16:47:04.050288: Validation loss did not improve from -0.63124. Patience: 21/50
2024-12-09 16:47:04.051289: train_loss -0.7297
2024-12-09 16:47:04.052274: val_loss -0.6123
2024-12-09 16:47:04.053282: Pseudo dice [0.7839]
2024-12-09 16:47:04.054386: Epoch time: 89.84 s
2024-12-09 16:47:05.262208: 
2024-12-09 16:47:05.264162: Epoch 141
2024-12-09 16:47:05.265653: Current learning rate: 0.00872
2024-12-09 16:48:35.286026: Validation loss improved from -0.63124 to -0.63783! Patience: 21/50
2024-12-09 16:48:35.287077: train_loss -0.7339
2024-12-09 16:48:35.288161: val_loss -0.6378
2024-12-09 16:48:35.289105: Pseudo dice [0.7938]
2024-12-09 16:48:35.290113: Epoch time: 90.03 s
2024-12-09 16:48:36.508420: 
2024-12-09 16:48:36.509805: Epoch 142
2024-12-09 16:48:36.510845: Current learning rate: 0.00871
2024-12-09 16:50:06.388249: Validation loss did not improve from -0.63783. Patience: 1/50
2024-12-09 16:50:06.389346: train_loss -0.7456
2024-12-09 16:50:06.390444: val_loss -0.6256
2024-12-09 16:50:06.391349: Pseudo dice [0.7904]
2024-12-09 16:50:06.392044: Epoch time: 89.88 s
2024-12-09 16:50:06.392771: Yayy! New best EMA pseudo Dice: 0.7759
2024-12-09 16:50:07.950500: 
2024-12-09 16:50:07.952569: Epoch 143
2024-12-09 16:50:07.953472: Current learning rate: 0.0087
2024-12-09 16:51:37.813799: Validation loss did not improve from -0.63783. Patience: 2/50
2024-12-09 16:51:37.814793: train_loss -0.7412
2024-12-09 16:51:37.816015: val_loss -0.6007
2024-12-09 16:51:37.816673: Pseudo dice [0.776]
2024-12-09 16:51:37.817612: Epoch time: 89.87 s
2024-12-09 16:51:37.818431: Yayy! New best EMA pseudo Dice: 0.7759
2024-12-09 16:51:39.369356: 
2024-12-09 16:51:39.370942: Epoch 144
2024-12-09 16:51:39.372268: Current learning rate: 0.00869
2024-12-09 16:53:09.012147: Validation loss did not improve from -0.63783. Patience: 3/50
2024-12-09 16:53:09.013313: train_loss -0.7507
2024-12-09 16:53:09.014410: val_loss -0.6341
2024-12-09 16:53:09.015450: Pseudo dice [0.7905]
2024-12-09 16:53:09.016233: Epoch time: 89.64 s
2024-12-09 16:53:09.377237: Yayy! New best EMA pseudo Dice: 0.7774
2024-12-09 16:53:10.899453: 
2024-12-09 16:53:10.900941: Epoch 145
2024-12-09 16:53:10.901961: Current learning rate: 0.00868
2024-12-09 16:54:40.769288: Validation loss did not improve from -0.63783. Patience: 4/50
2024-12-09 16:54:40.770087: train_loss -0.7545
2024-12-09 16:54:40.771688: val_loss -0.6129
2024-12-09 16:54:40.772910: Pseudo dice [0.7807]
2024-12-09 16:54:40.774080: Epoch time: 89.87 s
2024-12-09 16:54:40.775067: Yayy! New best EMA pseudo Dice: 0.7777
2024-12-09 16:54:43.566082: 
2024-12-09 16:54:43.568065: Epoch 146
2024-12-09 16:54:43.569276: Current learning rate: 0.00868
2024-12-09 16:56:13.561452: Validation loss did not improve from -0.63783. Patience: 5/50
2024-12-09 16:56:13.562371: train_loss -0.7506
2024-12-09 16:56:13.563406: val_loss -0.5998
2024-12-09 16:56:13.564414: Pseudo dice [0.7783]
2024-12-09 16:56:13.565446: Epoch time: 90.0 s
2024-12-09 16:56:13.566223: Yayy! New best EMA pseudo Dice: 0.7778
2024-12-09 16:56:15.054735: 
2024-12-09 16:56:15.056240: Epoch 147
2024-12-09 16:56:15.057215: Current learning rate: 0.00867
2024-12-09 16:57:45.071781: Validation loss did not improve from -0.63783. Patience: 6/50
2024-12-09 16:57:45.073054: train_loss -0.7515
2024-12-09 16:57:45.074163: val_loss -0.6118
2024-12-09 16:57:45.074940: Pseudo dice [0.7847]
2024-12-09 16:57:45.075811: Epoch time: 90.02 s
2024-12-09 16:57:45.076667: Yayy! New best EMA pseudo Dice: 0.7785
2024-12-09 16:57:46.587483: 
2024-12-09 16:57:46.589044: Epoch 148
2024-12-09 16:57:46.590007: Current learning rate: 0.00866
2024-12-09 16:59:16.679027: Validation loss did not improve from -0.63783. Patience: 7/50
2024-12-09 16:59:16.680205: train_loss -0.7478
2024-12-09 16:59:16.681006: val_loss -0.5896
2024-12-09 16:59:16.681699: Pseudo dice [0.7637]
2024-12-09 16:59:16.682557: Epoch time: 90.09 s
2024-12-09 16:59:17.888999: 
2024-12-09 16:59:17.890592: Epoch 149
2024-12-09 16:59:17.891651: Current learning rate: 0.00865
2024-12-09 17:00:48.095643: Validation loss did not improve from -0.63783. Patience: 8/50
2024-12-09 17:00:48.096842: train_loss -0.7411
2024-12-09 17:00:48.098167: val_loss -0.6123
2024-12-09 17:00:48.099207: Pseudo dice [0.7821]
2024-12-09 17:00:48.100205: Epoch time: 90.21 s
2024-12-09 17:00:49.797421: 
2024-12-09 17:00:49.799245: Epoch 150
2024-12-09 17:00:49.800675: Current learning rate: 0.00864
2024-12-09 17:02:19.810951: Validation loss did not improve from -0.63783. Patience: 9/50
2024-12-09 17:02:19.812239: train_loss -0.7537
2024-12-09 17:02:19.813203: val_loss -0.6108
2024-12-09 17:02:19.814189: Pseudo dice [0.7828]
2024-12-09 17:02:19.815168: Epoch time: 90.02 s
2024-12-09 17:02:21.044217: 
2024-12-09 17:02:21.045837: Epoch 151
2024-12-09 17:02:21.047247: Current learning rate: 0.00863
2024-12-09 17:03:51.215494: Validation loss did not improve from -0.63783. Patience: 10/50
2024-12-09 17:03:51.216561: train_loss -0.7519
2024-12-09 17:03:51.217418: val_loss -0.6094
2024-12-09 17:03:51.218184: Pseudo dice [0.7738]
2024-12-09 17:03:51.218999: Epoch time: 90.17 s
2024-12-09 17:03:52.440027: 
2024-12-09 17:03:52.441441: Epoch 152
2024-12-09 17:03:52.442616: Current learning rate: 0.00862
2024-12-09 17:05:22.593983: Validation loss improved from -0.63783 to -0.63839! Patience: 10/50
2024-12-09 17:05:22.595169: train_loss -0.7519
2024-12-09 17:05:22.596222: val_loss -0.6384
2024-12-09 17:05:22.597188: Pseudo dice [0.7933]
2024-12-09 17:05:22.598096: Epoch time: 90.16 s
2024-12-09 17:05:22.598790: Yayy! New best EMA pseudo Dice: 0.7792
2024-12-09 17:05:24.104953: 
2024-12-09 17:05:24.106891: Epoch 153
2024-12-09 17:05:24.108227: Current learning rate: 0.00861
2024-12-09 17:06:54.087066: Validation loss did not improve from -0.63839. Patience: 1/50
2024-12-09 17:06:54.088643: train_loss -0.7471
2024-12-09 17:06:54.089992: val_loss -0.6057
2024-12-09 17:06:54.090683: Pseudo dice [0.7819]
2024-12-09 17:06:54.091598: Epoch time: 89.98 s
2024-12-09 17:06:54.092306: Yayy! New best EMA pseudo Dice: 0.7794
2024-12-09 17:06:55.672112: 
2024-12-09 17:06:55.674011: Epoch 154
2024-12-09 17:06:55.675224: Current learning rate: 0.0086
2024-12-09 17:08:25.589402: Validation loss did not improve from -0.63839. Patience: 2/50
2024-12-09 17:08:25.590533: train_loss -0.7481
2024-12-09 17:08:25.591699: val_loss -0.6177
2024-12-09 17:08:25.592926: Pseudo dice [0.7804]
2024-12-09 17:08:25.594039: Epoch time: 89.92 s
2024-12-09 17:08:26.028592: Yayy! New best EMA pseudo Dice: 0.7795
2024-12-09 17:08:27.584564: 
2024-12-09 17:08:27.586429: Epoch 155
2024-12-09 17:08:27.587407: Current learning rate: 0.00859
2024-12-09 17:09:57.196348: Validation loss did not improve from -0.63839. Patience: 3/50
2024-12-09 17:09:57.197520: train_loss -0.7513
2024-12-09 17:09:57.198981: val_loss -0.6032
2024-12-09 17:09:57.200272: Pseudo dice [0.7639]
2024-12-09 17:09:57.201615: Epoch time: 89.61 s
2024-12-09 17:09:58.743624: 
2024-12-09 17:09:58.745753: Epoch 156
2024-12-09 17:09:58.747070: Current learning rate: 0.00858
2024-12-09 17:11:28.440180: Validation loss did not improve from -0.63839. Patience: 4/50
2024-12-09 17:11:28.441466: train_loss -0.7446
2024-12-09 17:11:28.442558: val_loss -0.5703
2024-12-09 17:11:28.443413: Pseudo dice [0.7566]
2024-12-09 17:11:28.444546: Epoch time: 89.7 s
2024-12-09 17:11:29.666447: 
2024-12-09 17:11:29.668551: Epoch 157
2024-12-09 17:11:29.669649: Current learning rate: 0.00858
2024-12-09 17:12:59.434450: Validation loss did not improve from -0.63839. Patience: 5/50
2024-12-09 17:12:59.435444: train_loss -0.7496
2024-12-09 17:12:59.436490: val_loss -0.586
2024-12-09 17:12:59.437226: Pseudo dice [0.7723]
2024-12-09 17:12:59.438227: Epoch time: 89.77 s
2024-12-09 17:13:00.658427: 
2024-12-09 17:13:00.660182: Epoch 158
2024-12-09 17:13:00.661041: Current learning rate: 0.00857
2024-12-09 17:14:30.287264: Validation loss did not improve from -0.63839. Patience: 6/50
2024-12-09 17:14:30.288228: train_loss -0.7494
2024-12-09 17:14:30.289477: val_loss -0.6179
2024-12-09 17:14:30.290201: Pseudo dice [0.7833]
2024-12-09 17:14:30.291192: Epoch time: 89.63 s
2024-12-09 17:14:31.526988: 
2024-12-09 17:14:31.528643: Epoch 159
2024-12-09 17:14:31.529741: Current learning rate: 0.00856
2024-12-09 17:16:01.379632: Validation loss did not improve from -0.63839. Patience: 7/50
2024-12-09 17:16:01.381083: train_loss -0.7504
2024-12-09 17:16:01.382356: val_loss -0.6066
2024-12-09 17:16:01.383491: Pseudo dice [0.7816]
2024-12-09 17:16:01.384631: Epoch time: 89.85 s
2024-12-09 17:16:02.969736: 
2024-12-09 17:16:02.971877: Epoch 160
2024-12-09 17:16:02.972717: Current learning rate: 0.00855
2024-12-09 17:17:32.736848: Validation loss did not improve from -0.63839. Patience: 8/50
2024-12-09 17:17:32.738221: train_loss -0.7499
2024-12-09 17:17:32.739824: val_loss -0.5999
2024-12-09 17:17:32.740722: Pseudo dice [0.7664]
2024-12-09 17:17:32.741681: Epoch time: 89.77 s
2024-12-09 17:17:34.021383: 
2024-12-09 17:17:34.023241: Epoch 161
2024-12-09 17:17:34.024570: Current learning rate: 0.00854
2024-12-09 17:19:04.103163: Validation loss did not improve from -0.63839. Patience: 9/50
2024-12-09 17:19:04.103930: train_loss -0.7563
2024-12-09 17:19:04.105188: val_loss -0.5946
2024-12-09 17:19:04.106156: Pseudo dice [0.776]
2024-12-09 17:19:04.106956: Epoch time: 90.08 s
2024-12-09 17:19:05.354740: 
2024-12-09 17:19:05.356031: Epoch 162
2024-12-09 17:19:05.356905: Current learning rate: 0.00853
2024-12-09 17:20:35.640828: Validation loss did not improve from -0.63839. Patience: 10/50
2024-12-09 17:20:35.642069: train_loss -0.7587
2024-12-09 17:20:35.643142: val_loss -0.6177
2024-12-09 17:20:35.643987: Pseudo dice [0.7871]
2024-12-09 17:20:35.644954: Epoch time: 90.29 s
2024-12-09 17:20:36.859372: 
2024-12-09 17:20:36.861310: Epoch 163
2024-12-09 17:20:36.862589: Current learning rate: 0.00852
2024-12-09 17:22:07.087923: Validation loss did not improve from -0.63839. Patience: 11/50
2024-12-09 17:22:07.089192: train_loss -0.7556
2024-12-09 17:22:07.090835: val_loss -0.5951
2024-12-09 17:22:07.091940: Pseudo dice [0.7746]
2024-12-09 17:22:07.093123: Epoch time: 90.23 s
2024-12-09 17:22:08.311876: 
2024-12-09 17:22:08.314571: Epoch 164
2024-12-09 17:22:08.315788: Current learning rate: 0.00851
2024-12-09 17:23:38.521397: Validation loss did not improve from -0.63839. Patience: 12/50
2024-12-09 17:23:38.522497: train_loss -0.7585
2024-12-09 17:23:38.523295: val_loss -0.6061
2024-12-09 17:23:38.524260: Pseudo dice [0.7785]
2024-12-09 17:23:38.525133: Epoch time: 90.21 s
2024-12-09 17:23:40.088265: 
2024-12-09 17:23:40.089973: Epoch 165
2024-12-09 17:23:40.090672: Current learning rate: 0.0085
2024-12-09 17:25:10.275933: Validation loss did not improve from -0.63839. Patience: 13/50
2024-12-09 17:25:10.277292: train_loss -0.7563
2024-12-09 17:25:10.278319: val_loss -0.5868
2024-12-09 17:25:10.279194: Pseudo dice [0.7679]
2024-12-09 17:25:10.280108: Epoch time: 90.19 s
2024-12-09 17:25:11.789946: 
2024-12-09 17:25:11.791260: Epoch 166
2024-12-09 17:25:11.792073: Current learning rate: 0.00849
2024-12-09 17:26:42.054136: Validation loss did not improve from -0.63839. Patience: 14/50
2024-12-09 17:26:42.055350: train_loss -0.7451
2024-12-09 17:26:42.056623: val_loss -0.6164
2024-12-09 17:26:42.057589: Pseudo dice [0.7786]
2024-12-09 17:26:42.058659: Epoch time: 90.27 s
2024-12-09 17:26:43.256776: 
2024-12-09 17:26:43.258656: Epoch 167
2024-12-09 17:26:43.259569: Current learning rate: 0.00848
2024-12-09 17:28:13.597875: Validation loss did not improve from -0.63839. Patience: 15/50
2024-12-09 17:28:13.599045: train_loss -0.7424
2024-12-09 17:28:13.600075: val_loss -0.62
2024-12-09 17:28:13.600754: Pseudo dice [0.7862]
2024-12-09 17:28:13.601558: Epoch time: 90.34 s
2024-12-09 17:28:14.799653: 
2024-12-09 17:28:14.801213: Epoch 168
2024-12-09 17:28:14.802147: Current learning rate: 0.00847
2024-12-09 17:29:45.142084: Validation loss did not improve from -0.63839. Patience: 16/50
2024-12-09 17:29:45.143194: train_loss -0.7519
2024-12-09 17:29:45.144263: val_loss -0.6306
2024-12-09 17:29:45.145201: Pseudo dice [0.7891]
2024-12-09 17:29:45.146043: Epoch time: 90.34 s
2024-12-09 17:29:46.366710: 
2024-12-09 17:29:46.368572: Epoch 169
2024-12-09 17:29:46.369659: Current learning rate: 0.00847
2024-12-09 17:31:16.825591: Validation loss did not improve from -0.63839. Patience: 17/50
2024-12-09 17:31:16.827018: train_loss -0.7574
2024-12-09 17:31:16.828283: val_loss -0.616
2024-12-09 17:31:16.829345: Pseudo dice [0.7828]
2024-12-09 17:31:16.830148: Epoch time: 90.46 s
2024-12-09 17:31:18.414769: 
2024-12-09 17:31:18.416722: Epoch 170
2024-12-09 17:31:18.417850: Current learning rate: 0.00846
2024-12-09 17:32:48.972997: Validation loss did not improve from -0.63839. Patience: 18/50
2024-12-09 17:32:48.974303: train_loss -0.7558
2024-12-09 17:32:48.975569: val_loss -0.613
2024-12-09 17:32:48.976548: Pseudo dice [0.7777]
2024-12-09 17:32:48.977551: Epoch time: 90.56 s
2024-12-09 17:32:50.191350: 
2024-12-09 17:32:50.192821: Epoch 171
2024-12-09 17:32:50.194252: Current learning rate: 0.00845
2024-12-09 17:34:20.604533: Validation loss did not improve from -0.63839. Patience: 19/50
2024-12-09 17:34:20.605540: train_loss -0.7596
2024-12-09 17:34:20.606630: val_loss -0.6211
2024-12-09 17:34:20.607686: Pseudo dice [0.79]
2024-12-09 17:34:20.608790: Epoch time: 90.42 s
2024-12-09 17:34:20.609882: Yayy! New best EMA pseudo Dice: 0.7799
2024-12-09 17:34:22.222275: 
2024-12-09 17:34:22.224263: Epoch 172
2024-12-09 17:34:22.225442: Current learning rate: 0.00844
2024-12-09 17:35:52.367825: Validation loss did not improve from -0.63839. Patience: 20/50
2024-12-09 17:35:52.369071: train_loss -0.7564
2024-12-09 17:35:52.370151: val_loss -0.5832
2024-12-09 17:35:52.371216: Pseudo dice [0.7652]
2024-12-09 17:35:52.372018: Epoch time: 90.15 s
2024-12-09 17:35:53.575084: 
2024-12-09 17:35:53.576833: Epoch 173
2024-12-09 17:35:53.577879: Current learning rate: 0.00843
2024-12-09 17:37:23.809566: Validation loss did not improve from -0.63839. Patience: 21/50
2024-12-09 17:37:23.811034: train_loss -0.7581
2024-12-09 17:37:23.812361: val_loss -0.6097
2024-12-09 17:37:23.813562: Pseudo dice [0.7791]
2024-12-09 17:37:23.814301: Epoch time: 90.24 s
2024-12-09 17:37:25.047703: 
2024-12-09 17:37:25.049268: Epoch 174
2024-12-09 17:37:25.050348: Current learning rate: 0.00842
2024-12-09 17:38:55.302068: Validation loss did not improve from -0.63839. Patience: 22/50
2024-12-09 17:38:55.303273: train_loss -0.7589
2024-12-09 17:38:55.304451: val_loss -0.5828
2024-12-09 17:38:55.305287: Pseudo dice [0.7678]
2024-12-09 17:38:55.306102: Epoch time: 90.26 s
2024-12-09 17:38:56.858208: 
2024-12-09 17:38:56.859657: Epoch 175
2024-12-09 17:38:56.860620: Current learning rate: 0.00841
2024-12-09 17:40:26.955969: Validation loss did not improve from -0.63839. Patience: 23/50
2024-12-09 17:40:26.956815: train_loss -0.7651
2024-12-09 17:40:26.957814: val_loss -0.636
2024-12-09 17:40:26.958779: Pseudo dice [0.7848]
2024-12-09 17:40:26.959852: Epoch time: 90.1 s
2024-12-09 17:40:28.149228: 
2024-12-09 17:40:28.151313: Epoch 176
2024-12-09 17:40:28.152458: Current learning rate: 0.0084
2024-12-09 17:41:58.321551: Validation loss did not improve from -0.63839. Patience: 24/50
2024-12-09 17:41:58.322752: train_loss -0.7532
2024-12-09 17:41:58.324269: val_loss -0.5973
2024-12-09 17:41:58.325573: Pseudo dice [0.7762]
2024-12-09 17:41:58.326700: Epoch time: 90.17 s
2024-12-09 17:41:59.845279: 
2024-12-09 17:41:59.847009: Epoch 177
2024-12-09 17:41:59.848043: Current learning rate: 0.00839
2024-12-09 17:43:30.030746: Validation loss did not improve from -0.63839. Patience: 25/50
2024-12-09 17:43:30.031661: train_loss -0.7522
2024-12-09 17:43:30.032682: val_loss -0.594
2024-12-09 17:43:30.033554: Pseudo dice [0.7707]
2024-12-09 17:43:30.034299: Epoch time: 90.19 s
2024-12-09 17:43:31.227875: 
2024-12-09 17:43:31.229278: Epoch 178
2024-12-09 17:43:31.230128: Current learning rate: 0.00838
2024-12-09 17:45:01.410021: Validation loss did not improve from -0.63839. Patience: 26/50
2024-12-09 17:45:01.411185: train_loss -0.7587
2024-12-09 17:45:01.412502: val_loss -0.6281
2024-12-09 17:45:01.413540: Pseudo dice [0.7985]
2024-12-09 17:45:01.414681: Epoch time: 90.18 s
2024-12-09 17:45:02.635494: 
2024-12-09 17:45:02.637659: Epoch 179
2024-12-09 17:45:02.638972: Current learning rate: 0.00837
2024-12-09 17:46:32.836797: Validation loss did not improve from -0.63839. Patience: 27/50
2024-12-09 17:46:32.837903: train_loss -0.7621
2024-12-09 17:46:32.839265: val_loss -0.6028
2024-12-09 17:46:32.840532: Pseudo dice [0.7721]
2024-12-09 17:46:32.841738: Epoch time: 90.2 s
2024-12-09 17:46:34.369712: 
2024-12-09 17:46:34.371277: Epoch 180
2024-12-09 17:46:34.372451: Current learning rate: 0.00836
2024-12-09 17:48:04.768646: Validation loss did not improve from -0.63839. Patience: 28/50
2024-12-09 17:48:04.769943: train_loss -0.7629
2024-12-09 17:48:04.771439: val_loss -0.6198
2024-12-09 17:48:04.772415: Pseudo dice [0.7824]
2024-12-09 17:48:04.773203: Epoch time: 90.4 s
2024-12-09 17:48:05.979376: 
2024-12-09 17:48:05.981470: Epoch 181
2024-12-09 17:48:05.982696: Current learning rate: 0.00836
2024-12-09 17:49:36.933610: Validation loss did not improve from -0.63839. Patience: 29/50
2024-12-09 17:49:36.954311: train_loss -0.7563
2024-12-09 17:49:36.958030: val_loss -0.6112
2024-12-09 17:49:36.958930: Pseudo dice [0.7729]
2024-12-09 17:49:36.960618: Epoch time: 90.97 s
2024-12-09 17:49:38.388703: 
2024-12-09 17:49:38.390131: Epoch 182
2024-12-09 17:49:38.390937: Current learning rate: 0.00835
2024-12-09 17:51:09.238194: Validation loss did not improve from -0.63839. Patience: 30/50
2024-12-09 17:51:09.241367: train_loss -0.7562
2024-12-09 17:51:09.242985: val_loss -0.6305
2024-12-09 17:51:09.244035: Pseudo dice [0.7902]
2024-12-09 17:51:09.245257: Epoch time: 90.85 s
2024-12-09 17:51:10.454797: 
2024-12-09 17:51:10.456745: Epoch 183
2024-12-09 17:51:10.458167: Current learning rate: 0.00834
2024-12-09 17:52:40.883128: Validation loss did not improve from -0.63839. Patience: 31/50
2024-12-09 17:52:40.884086: train_loss -0.7594
2024-12-09 17:52:40.885250: val_loss -0.5994
2024-12-09 17:52:40.886230: Pseudo dice [0.7763]
2024-12-09 17:52:40.887036: Epoch time: 90.43 s
2024-12-09 17:52:42.091464: 
2024-12-09 17:52:42.092824: Epoch 184
2024-12-09 17:52:42.093810: Current learning rate: 0.00833
2024-12-09 17:54:12.569371: Validation loss did not improve from -0.63839. Patience: 32/50
2024-12-09 17:54:12.570660: train_loss -0.7616
2024-12-09 17:54:12.571856: val_loss -0.6187
2024-12-09 17:54:12.572594: Pseudo dice [0.7839]
2024-12-09 17:54:12.573504: Epoch time: 90.48 s
2024-12-09 17:54:14.633432: 
2024-12-09 17:54:14.635728: Epoch 185
2024-12-09 17:54:14.636856: Current learning rate: 0.00832
2024-12-09 17:55:45.114863: Validation loss did not improve from -0.63839. Patience: 33/50
2024-12-09 17:55:45.116157: train_loss -0.7645
2024-12-09 17:55:45.117444: val_loss -0.6091
2024-12-09 17:55:45.118478: Pseudo dice [0.7779]
2024-12-09 17:55:45.119397: Epoch time: 90.48 s
2024-12-09 17:55:46.343974: 
2024-12-09 17:55:46.346085: Epoch 186
2024-12-09 17:55:46.347443: Current learning rate: 0.00831
2024-12-09 17:57:16.596077: Validation loss did not improve from -0.63839. Patience: 34/50
2024-12-09 17:57:16.597445: train_loss -0.7588
2024-12-09 17:57:16.598513: val_loss -0.6036
2024-12-09 17:57:16.599468: Pseudo dice [0.7752]
2024-12-09 17:57:16.600405: Epoch time: 90.25 s
2024-12-09 17:57:18.680980: 
2024-12-09 17:57:18.682720: Epoch 187
2024-12-09 17:57:18.683573: Current learning rate: 0.0083
2024-12-09 17:58:48.962042: Validation loss did not improve from -0.63839. Patience: 35/50
2024-12-09 17:58:48.963317: train_loss -0.7636
2024-12-09 17:58:48.964637: val_loss -0.6272
2024-12-09 17:58:48.966013: Pseudo dice [0.7921]
2024-12-09 17:58:48.967443: Epoch time: 90.28 s
2024-12-09 17:58:48.968872: Yayy! New best EMA pseudo Dice: 0.7804
2024-12-09 17:58:50.525409: 
2024-12-09 17:58:50.527500: Epoch 188
2024-12-09 17:58:50.528770: Current learning rate: 0.00829
2024-12-09 18:00:20.802733: Validation loss did not improve from -0.63839. Patience: 36/50
2024-12-09 18:00:20.804018: train_loss -0.7672
2024-12-09 18:00:20.805588: val_loss -0.6112
2024-12-09 18:00:20.806484: Pseudo dice [0.7839]
2024-12-09 18:00:20.807204: Epoch time: 90.28 s
2024-12-09 18:00:20.807988: Yayy! New best EMA pseudo Dice: 0.7807
2024-12-09 18:00:22.323310: 
2024-12-09 18:00:22.325308: Epoch 189
2024-12-09 18:00:22.326424: Current learning rate: 0.00828
2024-12-09 18:01:52.436996: Validation loss did not improve from -0.63839. Patience: 37/50
2024-12-09 18:01:52.437633: train_loss -0.7648
2024-12-09 18:01:52.438462: val_loss -0.6186
2024-12-09 18:01:52.439319: Pseudo dice [0.7878]
2024-12-09 18:01:52.440083: Epoch time: 90.12 s
2024-12-09 18:01:52.790575: Yayy! New best EMA pseudo Dice: 0.7814
2024-12-09 18:01:54.316778: 
2024-12-09 18:01:54.318725: Epoch 190
2024-12-09 18:01:54.319906: Current learning rate: 0.00827
2024-12-09 18:03:24.565008: Validation loss did not improve from -0.63839. Patience: 38/50
2024-12-09 18:03:24.566596: train_loss -0.762
2024-12-09 18:03:24.567930: val_loss -0.5869
2024-12-09 18:03:24.568876: Pseudo dice [0.7733]
2024-12-09 18:03:24.569706: Epoch time: 90.25 s
2024-12-09 18:03:25.784333: 
2024-12-09 18:03:25.786372: Epoch 191
2024-12-09 18:03:25.787779: Current learning rate: 0.00826
2024-12-09 18:04:56.027282: Validation loss did not improve from -0.63839. Patience: 39/50
2024-12-09 18:04:56.028278: train_loss -0.7651
2024-12-09 18:04:56.029243: val_loss -0.5686
2024-12-09 18:04:56.030315: Pseudo dice [0.7536]
2024-12-09 18:04:56.031436: Epoch time: 90.24 s
2024-12-09 18:04:57.261600: 
2024-12-09 18:04:57.263233: Epoch 192
2024-12-09 18:04:57.264293: Current learning rate: 0.00825
2024-12-09 18:06:27.517941: Validation loss did not improve from -0.63839. Patience: 40/50
2024-12-09 18:06:27.518933: train_loss -0.7625
2024-12-09 18:06:27.520201: val_loss -0.6116
2024-12-09 18:06:27.521245: Pseudo dice [0.781]
2024-12-09 18:06:27.522155: Epoch time: 90.26 s
2024-12-09 18:06:28.751453: 
2024-12-09 18:06:28.752728: Epoch 193
2024-12-09 18:06:28.754020: Current learning rate: 0.00824
2024-12-09 18:07:58.995396: Validation loss did not improve from -0.63839. Patience: 41/50
2024-12-09 18:07:58.996772: train_loss -0.7668
2024-12-09 18:07:58.997844: val_loss -0.5818
2024-12-09 18:07:58.998682: Pseudo dice [0.7682]
2024-12-09 18:07:58.999541: Epoch time: 90.25 s
2024-12-09 18:08:00.214391: 
2024-12-09 18:08:00.216169: Epoch 194
2024-12-09 18:08:00.217188: Current learning rate: 0.00824
2024-12-09 18:09:30.694577: Validation loss did not improve from -0.63839. Patience: 42/50
2024-12-09 18:09:30.695796: train_loss -0.765
2024-12-09 18:09:30.696724: val_loss -0.5929
2024-12-09 18:09:30.697441: Pseudo dice [0.7771]
2024-12-09 18:09:30.698078: Epoch time: 90.48 s
2024-12-09 18:09:32.248144: 
2024-12-09 18:09:32.249852: Epoch 195
2024-12-09 18:09:32.251099: Current learning rate: 0.00823
2024-12-09 18:11:02.517484: Validation loss did not improve from -0.63839. Patience: 43/50
2024-12-09 18:11:02.518997: train_loss -0.7695
2024-12-09 18:11:02.520025: val_loss -0.6107
2024-12-09 18:11:02.520754: Pseudo dice [0.7857]
2024-12-09 18:11:02.521399: Epoch time: 90.27 s
2024-12-09 18:11:03.816483: 
2024-12-09 18:11:03.817822: Epoch 196
2024-12-09 18:11:03.818649: Current learning rate: 0.00822
2024-12-09 18:12:34.210284: Validation loss did not improve from -0.63839. Patience: 44/50
2024-12-09 18:12:34.211238: train_loss -0.7653
2024-12-09 18:12:34.212138: val_loss -0.6001
2024-12-09 18:12:34.212978: Pseudo dice [0.7781]
2024-12-09 18:12:34.213658: Epoch time: 90.4 s
2024-12-09 18:12:35.862863: 
2024-12-09 18:12:35.864716: Epoch 197
2024-12-09 18:12:35.866020: Current learning rate: 0.00821
2024-12-09 18:14:06.056378: Validation loss did not improve from -0.63839. Patience: 45/50
2024-12-09 18:14:06.057504: train_loss -0.7686
2024-12-09 18:14:06.058680: val_loss -0.6098
2024-12-09 18:14:06.059391: Pseudo dice [0.7824]
2024-12-09 18:14:06.060363: Epoch time: 90.2 s
2024-12-09 18:14:07.288032: 
2024-12-09 18:14:07.290498: Epoch 198
2024-12-09 18:14:07.292264: Current learning rate: 0.0082
2024-12-09 18:15:37.578839: Validation loss did not improve from -0.63839. Patience: 46/50
2024-12-09 18:15:37.579880: train_loss -0.7687
2024-12-09 18:15:37.581152: val_loss -0.6224
2024-12-09 18:15:37.582167: Pseudo dice [0.7903]
2024-12-09 18:15:37.583105: Epoch time: 90.29 s
2024-12-09 18:15:38.852251: 
2024-12-09 18:15:38.854488: Epoch 199
2024-12-09 18:15:38.855730: Current learning rate: 0.00819
2024-12-09 18:17:09.212420: Validation loss did not improve from -0.63839. Patience: 47/50
2024-12-09 18:17:09.213475: train_loss -0.7712
2024-12-09 18:17:09.214636: val_loss -0.6108
2024-12-09 18:17:09.215486: Pseudo dice [0.7755]
2024-12-09 18:17:09.216262: Epoch time: 90.36 s
2024-12-09 18:17:10.780191: 
2024-12-09 18:17:10.782094: Epoch 200
2024-12-09 18:17:10.783500: Current learning rate: 0.00818
2024-12-09 18:18:41.022434: Validation loss did not improve from -0.63839. Patience: 48/50
2024-12-09 18:18:41.023478: train_loss -0.7491
2024-12-09 18:18:41.024593: val_loss -0.604
2024-12-09 18:18:41.025283: Pseudo dice [0.7784]
2024-12-09 18:18:41.026017: Epoch time: 90.24 s
2024-12-09 18:18:42.259038: 
2024-12-09 18:18:42.260937: Epoch 201
2024-12-09 18:18:42.262242: Current learning rate: 0.00817
2024-12-09 18:20:12.182418: Validation loss did not improve from -0.63839. Patience: 49/50
2024-12-09 18:20:12.183630: train_loss -0.7407
2024-12-09 18:20:12.184744: val_loss -0.5782
2024-12-09 18:20:12.185719: Pseudo dice [0.7663]
2024-12-09 18:20:12.186598: Epoch time: 89.93 s
2024-12-09 18:20:13.405571: 
2024-12-09 18:20:13.407394: Epoch 202
2024-12-09 18:20:13.408425: Current learning rate: 0.00816
2024-12-09 18:21:43.338940: Validation loss did not improve from -0.63839. Patience: 50/50
2024-12-09 18:21:43.340558: train_loss -0.7499
2024-12-09 18:21:43.342328: val_loss -0.5955
2024-12-09 18:21:43.343332: Pseudo dice [0.7712]
2024-12-09 18:21:43.344430: Epoch time: 89.94 s
2024-12-09 18:21:44.594126: Patience reached. Stopping training.
2024-12-09 18:21:45.054362: Training done.
2024-12-09 18:21:45.269655: Using splits from existing split file: /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset307_Sohee_Calcium_OCT_CrossValidation/splits_final.json
2024-12-09 18:21:45.273457: The split file contains 5 splits.
2024-12-09 18:21:45.274777: Desired fold for training: 2
2024-12-09 18:21:45.275817: This split has 6 training and 2 validation cases.
2024-12-09 18:21:45.277009: predicting 101-044
2024-12-09 18:21:45.369047: 101-044, shape torch.Size([1, 404, 498, 498]), rank 0
2024-12-09 18:23:44.713768: predicting 704-003
2024-12-09 18:23:44.736889: 704-003, shape torch.Size([1, 375, 498, 498]), rank 0
2024-12-09 18:25:36.690425: Validation complete
2024-12-09 18:25:36.691849: Mean Validation Dice:  0.7660555149494561

############################
INFO: You are using the old nnU-Net default plans. We have updated our recommendations. Please consider using those instead! Read more here: https://github.com/MIC-DKFZ/nnUNet/blob/master/documentation/resenc_presets.md
############################

Using device: cuda:0

#######################################################################
Please cite the following paper when using nnU-Net:
Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.
#######################################################################

2024-12-09 18:25:51.756910: Using torch.compile...
/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
################### Loading pretrained weights from file  /home/gridsan/nchutisilp/datasets/ModelGenesisOutputs/ModelGenesisNNUNetPretrainingV2_noNorm_correct_orientation/Converted_nnUNet_Genesis_OCT_Best.pt ###################
Below is the list of overlapping blocks in pretrained model and nnUNet architecture:
encoder.stages.0.0.convs.0.conv.weight shape torch.Size([32, 1, 3, 3, 3])
encoder.stages.0.0.convs.0.conv.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.norm.weight shape torch.Size([32])
encoder.stages.0.0.convs.0.norm.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.0.weight shape torch.Size([32, 1, 3, 3, 3])
encoder.stages.0.0.convs.0.all_modules.0.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.1.weight shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.1.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
encoder.stages.0.0.convs.1.conv.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.norm.weight shape torch.Size([32])
encoder.stages.0.0.convs.1.norm.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
encoder.stages.0.0.convs.1.all_modules.0.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.1.weight shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.1.bias shape torch.Size([32])
encoder.stages.1.0.convs.0.conv.weight shape torch.Size([64, 32, 3, 3, 3])
encoder.stages.1.0.convs.0.conv.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.norm.weight shape torch.Size([64])
encoder.stages.1.0.convs.0.norm.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.0.weight shape torch.Size([64, 32, 3, 3, 3])
encoder.stages.1.0.convs.0.all_modules.0.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.1.weight shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.1.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
encoder.stages.1.0.convs.1.conv.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.norm.weight shape torch.Size([64])
encoder.stages.1.0.convs.1.norm.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
encoder.stages.1.0.convs.1.all_modules.0.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.1.weight shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.1.bias shape torch.Size([64])
encoder.stages.2.0.convs.0.conv.weight shape torch.Size([128, 64, 3, 3, 3])
encoder.stages.2.0.convs.0.conv.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.norm.weight shape torch.Size([128])
encoder.stages.2.0.convs.0.norm.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.0.weight shape torch.Size([128, 64, 3, 3, 3])
encoder.stages.2.0.convs.0.all_modules.0.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.1.weight shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.1.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
encoder.stages.2.0.convs.1.conv.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.norm.weight shape torch.Size([128])
encoder.stages.2.0.convs.1.norm.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
encoder.stages.2.0.convs.1.all_modules.0.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.1.weight shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.1.bias shape torch.Size([128])
encoder.stages.3.0.convs.0.conv.weight shape torch.Size([256, 128, 3, 3, 3])
encoder.stages.3.0.convs.0.conv.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.norm.weight shape torch.Size([256])
encoder.stages.3.0.convs.0.norm.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.0.weight shape torch.Size([256, 128, 3, 3, 3])
encoder.stages.3.0.convs.0.all_modules.0.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.1.weight shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.1.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
encoder.stages.3.0.convs.1.conv.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.norm.weight shape torch.Size([256])
encoder.stages.3.0.convs.1.norm.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
encoder.stages.3.0.convs.1.all_modules.0.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.1.weight shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.1.bias shape torch.Size([256])
encoder.stages.4.0.convs.0.conv.weight shape torch.Size([320, 256, 3, 3, 3])
encoder.stages.4.0.convs.0.conv.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.norm.weight shape torch.Size([320])
encoder.stages.4.0.convs.0.norm.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.0.weight shape torch.Size([320, 256, 3, 3, 3])
encoder.stages.4.0.convs.0.all_modules.0.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.1.weight shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.1.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.4.0.convs.1.conv.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.norm.weight shape torch.Size([320])
encoder.stages.4.0.convs.1.norm.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.4.0.convs.1.all_modules.0.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.1.weight shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.1.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.0.conv.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.norm.weight shape torch.Size([320])
encoder.stages.5.0.convs.0.norm.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.0.all_modules.0.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.1.weight shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.1.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.1.conv.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.norm.weight shape torch.Size([320])
encoder.stages.5.0.convs.1.norm.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.1.all_modules.0.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.1.weight shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.0.0.convs.0.conv.weight shape torch.Size([32, 1, 3, 3, 3])
decoder.encoder.stages.0.0.convs.0.conv.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.norm.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.norm.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.0.weight shape torch.Size([32, 1, 3, 3, 3])
decoder.encoder.stages.0.0.convs.0.all_modules.0.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.1.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.1.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.encoder.stages.0.0.convs.1.conv.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.norm.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.norm.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.encoder.stages.0.0.convs.1.all_modules.0.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.1.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.1.bias shape torch.Size([32])
decoder.encoder.stages.1.0.convs.0.conv.weight shape torch.Size([64, 32, 3, 3, 3])
decoder.encoder.stages.1.0.convs.0.conv.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.norm.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.norm.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.0.weight shape torch.Size([64, 32, 3, 3, 3])
decoder.encoder.stages.1.0.convs.0.all_modules.0.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.1.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.1.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.encoder.stages.1.0.convs.1.conv.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.norm.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.norm.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.encoder.stages.1.0.convs.1.all_modules.0.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.1.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.1.bias shape torch.Size([64])
decoder.encoder.stages.2.0.convs.0.conv.weight shape torch.Size([128, 64, 3, 3, 3])
decoder.encoder.stages.2.0.convs.0.conv.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.norm.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.norm.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.0.weight shape torch.Size([128, 64, 3, 3, 3])
decoder.encoder.stages.2.0.convs.0.all_modules.0.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.1.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.1.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.encoder.stages.2.0.convs.1.conv.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.norm.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.norm.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.encoder.stages.2.0.convs.1.all_modules.0.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.1.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.1.bias shape torch.Size([128])
decoder.encoder.stages.3.0.convs.0.conv.weight shape torch.Size([256, 128, 3, 3, 3])
decoder.encoder.stages.3.0.convs.0.conv.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.norm.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.norm.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.0.weight shape torch.Size([256, 128, 3, 3, 3])
decoder.encoder.stages.3.0.convs.0.all_modules.0.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.1.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.1.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.encoder.stages.3.0.convs.1.conv.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.norm.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.norm.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.encoder.stages.3.0.convs.1.all_modules.0.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.1.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.1.bias shape torch.Size([256])
decoder.encoder.stages.4.0.convs.0.conv.weight shape torch.Size([320, 256, 3, 3, 3])
decoder.encoder.stages.4.0.convs.0.conv.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.norm.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.norm.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.0.weight shape torch.Size([320, 256, 3, 3, 3])
decoder.encoder.stages.4.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.4.0.convs.1.conv.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.norm.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.norm.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.4.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.0.conv.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.norm.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.norm.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.1.conv.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.norm.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.norm.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.stages.0.convs.0.conv.weight shape torch.Size([320, 640, 3, 3, 3])
decoder.stages.0.convs.0.conv.bias shape torch.Size([320])
decoder.stages.0.convs.0.norm.weight shape torch.Size([320])
decoder.stages.0.convs.0.norm.bias shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.0.weight shape torch.Size([320, 640, 3, 3, 3])
decoder.stages.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.stages.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.stages.0.convs.1.conv.bias shape torch.Size([320])
decoder.stages.0.convs.1.norm.weight shape torch.Size([320])
decoder.stages.0.convs.1.norm.bias shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.stages.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.stages.1.convs.0.conv.weight shape torch.Size([256, 512, 3, 3, 3])
decoder.stages.1.convs.0.conv.bias shape torch.Size([256])
decoder.stages.1.convs.0.norm.weight shape torch.Size([256])
decoder.stages.1.convs.0.norm.bias shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.0.weight shape torch.Size([256, 512, 3, 3, 3])
decoder.stages.1.convs.0.all_modules.0.bias shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.1.weight shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.1.bias shape torch.Size([256])
decoder.stages.1.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.stages.1.convs.1.conv.bias shape torch.Size([256])
decoder.stages.1.convs.1.norm.weight shape torch.Size([256])
decoder.stages.1.convs.1.norm.bias shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.stages.1.convs.1.all_modules.0.bias shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.1.weight shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.1.bias shape torch.Size([256])
decoder.stages.2.convs.0.conv.weight shape torch.Size([128, 256, 3, 3, 3])
decoder.stages.2.convs.0.conv.bias shape torch.Size([128])
decoder.stages.2.convs.0.norm.weight shape torch.Size([128])
decoder.stages.2.convs.0.norm.bias shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.0.weight shape torch.Size([128, 256, 3, 3, 3])
decoder.stages.2.convs.0.all_modules.0.bias shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.1.weight shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.1.bias shape torch.Size([128])
decoder.stages.2.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.stages.2.convs.1.conv.bias shape torch.Size([128])
decoder.stages.2.convs.1.norm.weight shape torch.Size([128])
decoder.stages.2.convs.1.norm.bias shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.stages.2.convs.1.all_modules.0.bias shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.1.weight shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.1.bias shape torch.Size([128])
decoder.stages.3.convs.0.conv.weight shape torch.Size([64, 128, 3, 3, 3])
decoder.stages.3.convs.0.conv.bias shape torch.Size([64])
decoder.stages.3.convs.0.norm.weight shape torch.Size([64])
decoder.stages.3.convs.0.norm.bias shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.0.weight shape torch.Size([64, 128, 3, 3, 3])
decoder.stages.3.convs.0.all_modules.0.bias shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.1.weight shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.1.bias shape torch.Size([64])
decoder.stages.3.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.stages.3.convs.1.conv.bias shape torch.Size([64])
decoder.stages.3.convs.1.norm.weight shape torch.Size([64])
decoder.stages.3.convs.1.norm.bias shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.stages.3.convs.1.all_modules.0.bias shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.1.weight shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.1.bias shape torch.Size([64])
decoder.stages.4.convs.0.conv.weight shape torch.Size([32, 64, 3, 3, 3])
decoder.stages.4.convs.0.conv.bias shape torch.Size([32])
decoder.stages.4.convs.0.norm.weight shape torch.Size([32])
decoder.stages.4.convs.0.norm.bias shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.0.weight shape torch.Size([32, 64, 3, 3, 3])
decoder.stages.4.convs.0.all_modules.0.bias shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.1.weight shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.1.bias shape torch.Size([32])
decoder.stages.4.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.stages.4.convs.1.conv.bias shape torch.Size([32])
decoder.stages.4.convs.1.norm.weight shape torch.Size([32])
decoder.stages.4.convs.1.norm.bias shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.stages.4.convs.1.all_modules.0.bias shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.1.weight shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.1.bias shape torch.Size([32])
decoder.transpconvs.0.weight shape torch.Size([320, 320, 1, 2, 2])
decoder.transpconvs.0.bias shape torch.Size([320])
decoder.transpconvs.1.weight shape torch.Size([320, 256, 2, 2, 2])
decoder.transpconvs.1.bias shape torch.Size([256])
decoder.transpconvs.2.weight shape torch.Size([256, 128, 2, 2, 2])
decoder.transpconvs.2.bias shape torch.Size([128])
decoder.transpconvs.3.weight shape torch.Size([128, 64, 2, 2, 2])
decoder.transpconvs.3.bias shape torch.Size([64])
decoder.transpconvs.4.weight shape torch.Size([64, 32, 2, 2, 2])
decoder.transpconvs.4.bias shape torch.Size([32])
################### Done ###################
2024-12-09 18:26:14.171711: do_dummy_2d_data_aug: True
2024-12-09 18:26:14.174515: Using splits from existing split file: /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset307_Sohee_Calcium_OCT_CrossValidation/splits_final.json
2024-12-09 18:26:14.176550: The split file contains 5 splits.
2024-12-09 18:26:14.177448: Desired fold for training: 4
2024-12-09 18:26:14.178282: This split has 7 training and 1 validation cases.
using pin_memory on device 0
using pin_memory on device 0

This is the configuration used by this training:
Configuration name: 3d_32x160x128_b10
 {'data_identifier': 'nnUNetPlans_3d_fullres', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 10, 'patch_size': [32, 160, 128], 'median_image_size_in_voxels': [375.0, 498.0, 498.0], 'spacing': [1.0, 1.0, 1.0], 'normalization_schemes': ['ZScoreNormalization'], 'use_mask_for_norm': [False], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'architecture': {'network_class_name': 'dynamic_network_architectures.architectures.unet.PlainConvUNet', 'arch_kwargs': {'n_stages': 6, 'features_per_stage': [32, 64, 128, 256, 320, 320], 'conv_op': 'torch.nn.modules.conv.Conv3d', 'kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]], 'strides': [[1, 1, 1], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [1, 2, 2]], 'n_conv_per_stage': [2, 2, 2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2, 2, 2], 'conv_bias': True, 'norm_op': 'torch.nn.modules.instancenorm.InstanceNorm3d', 'norm_op_kwargs': {'eps': 1e-05, 'affine': True}, 'dropout_op': None, 'dropout_op_kwargs': None, 'nonlin': 'torch.nn.LeakyReLU', 'nonlin_kwargs': {'inplace': True}, 'deep_supervision': True}, '_kw_requires_import': ['conv_op', 'norm_op', 'dropout_op', 'nonlin']}, 'batch_dice': True, 'inherits_from': '3d_fullres'} 

These are the global plan.json settings:
 {'dataset_name': 'Dataset307_Sohee_Calcium_OCT_CrossValidation', 'plans_name': 'nnUNetPlans', 'original_median_spacing_after_transp': [1.0, 1.0, 1.0], 'original_median_shape_after_transp': [375, 498, 498], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 0.4977976381778717, 'mean': 0.13507525622844696, 'median': 0.09599608182907104, 'min': 0.0, 'percentile_00_5': 0.014754901640117168, 'percentile_99_5': 0.4977976381778717, 'std': 0.12025152146816254}}} 

2024-12-09 18:26:15.574450: unpacking dataset...
2024-12-09 18:26:19.591845: unpacking done...
2024-12-09 18:26:19.946391: Unable to plot network architecture: nnUNet_compile is enabled!
2024-12-09 18:26:20.193079: 
2024-12-09 18:26:20.194854: Epoch 0
2024-12-09 18:26:20.196133: Current learning rate: 0.01
2024-12-09 18:29:14.854827: Validation loss improved from 1000.00000 to -0.24883! Patience: 0/50
2024-12-09 18:29:14.855871: train_loss -0.1249
2024-12-09 18:29:14.857130: val_loss -0.2488
2024-12-09 18:29:14.858097: Pseudo dice [0.5859]
2024-12-09 18:29:14.859403: Epoch time: 174.66 s
2024-12-09 18:29:14.860204: Yayy! New best EMA pseudo Dice: 0.5859
2024-12-09 18:29:16.853413: 
2024-12-09 18:29:16.855130: Epoch 1
2024-12-09 18:29:16.856050: Current learning rate: 0.00999
2024-12-09 18:30:44.692519: Validation loss improved from -0.24883 to -0.29973! Patience: 0/50
2024-12-09 18:30:44.693809: train_loss -0.2683
2024-12-09 18:30:44.694954: val_loss -0.2997
2024-12-09 18:30:44.696315: Pseudo dice [0.6004]
2024-12-09 18:30:44.697042: Epoch time: 87.84 s
2024-12-09 18:30:44.697658: Yayy! New best EMA pseudo Dice: 0.5873
2024-12-09 18:30:46.232054: 
2024-12-09 18:30:46.233549: Epoch 2
2024-12-09 18:30:46.234833: Current learning rate: 0.00998
2024-12-09 18:32:14.724913: Validation loss improved from -0.29973 to -0.35259! Patience: 0/50
2024-12-09 18:32:14.726027: train_loss -0.3395
2024-12-09 18:32:14.727216: val_loss -0.3526
2024-12-09 18:32:14.728022: Pseudo dice [0.639]
2024-12-09 18:32:14.728966: Epoch time: 88.5 s
2024-12-09 18:32:14.729931: Yayy! New best EMA pseudo Dice: 0.5925
2024-12-09 18:32:16.312809: 
2024-12-09 18:32:16.314376: Epoch 3
2024-12-09 18:32:16.315176: Current learning rate: 0.00997
2024-12-09 18:33:45.016261: Validation loss improved from -0.35259 to -0.39465! Patience: 0/50
2024-12-09 18:33:45.017390: train_loss -0.3704
2024-12-09 18:33:45.018328: val_loss -0.3946
2024-12-09 18:33:45.019282: Pseudo dice [0.6556]
2024-12-09 18:33:45.020400: Epoch time: 88.71 s
2024-12-09 18:33:45.021235: Yayy! New best EMA pseudo Dice: 0.5988
2024-12-09 18:33:46.556964: 
2024-12-09 18:33:46.558625: Epoch 4
2024-12-09 18:33:46.559488: Current learning rate: 0.00996
2024-12-09 18:35:15.021870: Validation loss improved from -0.39465 to -0.41616! Patience: 0/50
2024-12-09 18:35:15.023100: train_loss -0.4083
2024-12-09 18:35:15.024155: val_loss -0.4162
2024-12-09 18:35:15.024881: Pseudo dice [0.6663]
2024-12-09 18:35:15.025495: Epoch time: 88.47 s
2024-12-09 18:35:15.364250: Yayy! New best EMA pseudo Dice: 0.6055
2024-12-09 18:35:16.951107: 
2024-12-09 18:35:16.952643: Epoch 5
2024-12-09 18:35:16.953650: Current learning rate: 0.00995
2024-12-09 18:36:45.345126: Validation loss did not improve from -0.41616. Patience: 1/50
2024-12-09 18:36:45.346949: train_loss -0.4228
2024-12-09 18:36:45.348416: val_loss -0.4085
2024-12-09 18:36:45.349526: Pseudo dice [0.6577]
2024-12-09 18:36:45.350761: Epoch time: 88.4 s
2024-12-09 18:36:45.351629: Yayy! New best EMA pseudo Dice: 0.6108
2024-12-09 18:36:46.859988: 
2024-12-09 18:36:46.861613: Epoch 6
2024-12-09 18:36:46.862744: Current learning rate: 0.00995
2024-12-09 18:38:15.351478: Validation loss did not improve from -0.41616. Patience: 2/50
2024-12-09 18:38:15.352550: train_loss -0.4389
2024-12-09 18:38:15.353854: val_loss -0.3412
2024-12-09 18:38:15.355072: Pseudo dice [0.6286]
2024-12-09 18:38:15.355753: Epoch time: 88.49 s
2024-12-09 18:38:15.356454: Yayy! New best EMA pseudo Dice: 0.6125
2024-12-09 18:38:16.945677: 
2024-12-09 18:38:16.948428: Epoch 7
2024-12-09 18:38:16.949658: Current learning rate: 0.00994
2024-12-09 18:39:45.437445: Validation loss improved from -0.41616 to -0.43730! Patience: 2/50
2024-12-09 18:39:45.438785: train_loss -0.4505
2024-12-09 18:39:45.439711: val_loss -0.4373
2024-12-09 18:39:45.440467: Pseudo dice [0.685]
2024-12-09 18:39:45.441139: Epoch time: 88.49 s
2024-12-09 18:39:45.441828: Yayy! New best EMA pseudo Dice: 0.6198
2024-12-09 18:39:47.324207: 
2024-12-09 18:39:47.325907: Epoch 8
2024-12-09 18:39:47.327020: Current learning rate: 0.00993
2024-12-09 18:41:16.136699: Validation loss did not improve from -0.43730. Patience: 1/50
2024-12-09 18:41:16.138075: train_loss -0.4642
2024-12-09 18:41:16.139017: val_loss -0.4224
2024-12-09 18:41:16.139998: Pseudo dice [0.6731]
2024-12-09 18:41:16.141037: Epoch time: 88.81 s
2024-12-09 18:41:16.142009: Yayy! New best EMA pseudo Dice: 0.6251
2024-12-09 18:41:17.735797: 
2024-12-09 18:41:17.737751: Epoch 9
2024-12-09 18:41:17.738859: Current learning rate: 0.00992
2024-12-09 18:42:46.517579: Validation loss improved from -0.43730 to -0.44441! Patience: 1/50
2024-12-09 18:42:46.518512: train_loss -0.4956
2024-12-09 18:42:46.519316: val_loss -0.4444
2024-12-09 18:42:46.520073: Pseudo dice [0.6817]
2024-12-09 18:42:46.520693: Epoch time: 88.78 s
2024-12-09 18:42:46.848982: Yayy! New best EMA pseudo Dice: 0.6308
2024-12-09 18:42:48.347948: 
2024-12-09 18:42:48.350028: Epoch 10
2024-12-09 18:42:48.350955: Current learning rate: 0.00991
2024-12-09 18:44:17.187100: Validation loss did not improve from -0.44441. Patience: 1/50
2024-12-09 18:44:17.188262: train_loss -0.4933
2024-12-09 18:44:17.189056: val_loss -0.3953
2024-12-09 18:44:17.189731: Pseudo dice [0.6646]
2024-12-09 18:44:17.190431: Epoch time: 88.84 s
2024-12-09 18:44:17.191013: Yayy! New best EMA pseudo Dice: 0.6342
2024-12-09 18:44:18.713226: 
2024-12-09 18:44:18.714485: Epoch 11
2024-12-09 18:44:18.715170: Current learning rate: 0.0099
2024-12-09 18:45:47.471399: Validation loss improved from -0.44441 to -0.47297! Patience: 1/50
2024-12-09 18:45:47.472492: train_loss -0.5134
2024-12-09 18:45:47.473806: val_loss -0.473
2024-12-09 18:45:47.475045: Pseudo dice [0.7105]
2024-12-09 18:45:47.475921: Epoch time: 88.76 s
2024-12-09 18:45:47.476867: Yayy! New best EMA pseudo Dice: 0.6418
2024-12-09 18:45:48.983486: 
2024-12-09 18:45:48.985909: Epoch 12
2024-12-09 18:45:48.987158: Current learning rate: 0.00989
2024-12-09 18:47:17.755340: Validation loss did not improve from -0.47297. Patience: 1/50
2024-12-09 18:47:17.756572: train_loss -0.5228
2024-12-09 18:47:17.757429: val_loss -0.3413
2024-12-09 18:47:17.758245: Pseudo dice [0.6594]
2024-12-09 18:47:17.759564: Epoch time: 88.77 s
2024-12-09 18:47:17.760415: Yayy! New best EMA pseudo Dice: 0.6436
2024-12-09 18:47:19.305796: 
2024-12-09 18:47:19.307579: Epoch 13
2024-12-09 18:47:19.308403: Current learning rate: 0.00988
2024-12-09 18:48:47.967144: Validation loss did not improve from -0.47297. Patience: 2/50
2024-12-09 18:48:47.968907: train_loss -0.5278
2024-12-09 18:48:47.970560: val_loss -0.3873
2024-12-09 18:48:47.971838: Pseudo dice [0.6596]
2024-12-09 18:48:47.972872: Epoch time: 88.66 s
2024-12-09 18:48:47.974195: Yayy! New best EMA pseudo Dice: 0.6452
2024-12-09 18:48:49.538036: 
2024-12-09 18:48:49.539891: Epoch 14
2024-12-09 18:48:49.540940: Current learning rate: 0.00987
2024-12-09 18:50:18.302480: Validation loss improved from -0.47297 to -0.48312! Patience: 2/50
2024-12-09 18:50:18.303305: train_loss -0.5184
2024-12-09 18:50:18.304378: val_loss -0.4831
2024-12-09 18:50:18.305507: Pseudo dice [0.7122]
2024-12-09 18:50:18.306400: Epoch time: 88.77 s
2024-12-09 18:50:18.639512: Yayy! New best EMA pseudo Dice: 0.6519
2024-12-09 18:50:20.198144: 
2024-12-09 18:50:20.199960: Epoch 15
2024-12-09 18:50:20.200807: Current learning rate: 0.00986
2024-12-09 18:51:48.981588: Validation loss did not improve from -0.48312. Patience: 1/50
2024-12-09 18:51:48.982482: train_loss -0.5236
2024-12-09 18:51:48.983490: val_loss -0.4135
2024-12-09 18:51:48.984358: Pseudo dice [0.6797]
2024-12-09 18:51:48.985292: Epoch time: 88.79 s
2024-12-09 18:51:48.986278: Yayy! New best EMA pseudo Dice: 0.6547
2024-12-09 18:51:50.538428: 
2024-12-09 18:51:50.541293: Epoch 16
2024-12-09 18:51:50.542608: Current learning rate: 0.00986
2024-12-09 18:53:19.645902: Validation loss did not improve from -0.48312. Patience: 2/50
2024-12-09 18:53:19.647175: train_loss -0.5332
2024-12-09 18:53:19.648184: val_loss -0.4544
2024-12-09 18:53:19.648865: Pseudo dice [0.6892]
2024-12-09 18:53:19.649621: Epoch time: 89.11 s
2024-12-09 18:53:19.650343: Yayy! New best EMA pseudo Dice: 0.6581
2024-12-09 18:53:21.233432: 
2024-12-09 18:53:21.235699: Epoch 17
2024-12-09 18:53:21.236529: Current learning rate: 0.00985
2024-12-09 18:54:50.359436: Validation loss did not improve from -0.48312. Patience: 3/50
2024-12-09 18:54:50.360532: train_loss -0.5511
2024-12-09 18:54:50.361628: val_loss -0.4446
2024-12-09 18:54:50.362330: Pseudo dice [0.6921]
2024-12-09 18:54:50.363552: Epoch time: 89.13 s
2024-12-09 18:54:50.364611: Yayy! New best EMA pseudo Dice: 0.6615
2024-12-09 18:54:51.870632: 
2024-12-09 18:54:51.872318: Epoch 18
2024-12-09 18:54:51.873311: Current learning rate: 0.00984
2024-12-09 18:56:21.015278: Validation loss did not improve from -0.48312. Patience: 4/50
2024-12-09 18:56:21.016535: train_loss -0.5401
2024-12-09 18:56:21.017510: val_loss -0.4352
2024-12-09 18:56:21.018254: Pseudo dice [0.6876]
2024-12-09 18:56:21.019005: Epoch time: 89.15 s
2024-12-09 18:56:21.019695: Yayy! New best EMA pseudo Dice: 0.6641
2024-12-09 18:56:22.831704: 
2024-12-09 18:56:22.833853: Epoch 19
2024-12-09 18:56:22.834899: Current learning rate: 0.00983
2024-12-09 18:57:51.812480: Validation loss did not improve from -0.48312. Patience: 5/50
2024-12-09 18:57:51.813559: train_loss -0.5549
2024-12-09 18:57:51.814901: val_loss -0.4704
2024-12-09 18:57:51.815742: Pseudo dice [0.7019]
2024-12-09 18:57:51.816586: Epoch time: 88.98 s
2024-12-09 18:57:52.168736: Yayy! New best EMA pseudo Dice: 0.6679
2024-12-09 18:57:53.718630: 
2024-12-09 18:57:53.720739: Epoch 20
2024-12-09 18:57:53.721838: Current learning rate: 0.00982
2024-12-09 18:59:22.345379: Validation loss did not improve from -0.48312. Patience: 6/50
2024-12-09 18:59:22.347057: train_loss -0.5687
2024-12-09 18:59:22.349082: val_loss -0.4463
2024-12-09 18:59:22.350248: Pseudo dice [0.695]
2024-12-09 18:59:22.350895: Epoch time: 88.63 s
2024-12-09 18:59:22.351514: Yayy! New best EMA pseudo Dice: 0.6706
2024-12-09 18:59:23.906887: 
2024-12-09 18:59:23.908520: Epoch 21
2024-12-09 18:59:23.909419: Current learning rate: 0.00981
2024-12-09 19:00:52.612936: Validation loss did not improve from -0.48312. Patience: 7/50
2024-12-09 19:00:52.614012: train_loss -0.5747
2024-12-09 19:00:52.615062: val_loss -0.4599
2024-12-09 19:00:52.616217: Pseudo dice [0.6964]
2024-12-09 19:00:52.617070: Epoch time: 88.71 s
2024-12-09 19:00:52.618165: Yayy! New best EMA pseudo Dice: 0.6732
2024-12-09 19:00:54.105387: 
2024-12-09 19:00:54.107081: Epoch 22
2024-12-09 19:00:54.107850: Current learning rate: 0.0098
2024-12-09 19:02:22.840575: Validation loss did not improve from -0.48312. Patience: 8/50
2024-12-09 19:02:22.841550: train_loss -0.5761
2024-12-09 19:02:22.842617: val_loss -0.4462
2024-12-09 19:02:22.843765: Pseudo dice [0.7011]
2024-12-09 19:02:22.844758: Epoch time: 88.74 s
2024-12-09 19:02:22.845787: Yayy! New best EMA pseudo Dice: 0.676
2024-12-09 19:02:24.348853: 
2024-12-09 19:02:24.351075: Epoch 23
2024-12-09 19:02:24.352181: Current learning rate: 0.00979
2024-12-09 19:03:53.154377: Validation loss improved from -0.48312 to -0.48322! Patience: 8/50
2024-12-09 19:03:53.155807: train_loss -0.5791
2024-12-09 19:03:53.156996: val_loss -0.4832
2024-12-09 19:03:53.157968: Pseudo dice [0.7155]
2024-12-09 19:03:53.158872: Epoch time: 88.81 s
2024-12-09 19:03:53.159429: Yayy! New best EMA pseudo Dice: 0.6799
2024-12-09 19:03:54.706487: 
2024-12-09 19:03:54.709284: Epoch 24
2024-12-09 19:03:54.710465: Current learning rate: 0.00978
2024-12-09 19:05:23.532758: Validation loss did not improve from -0.48322. Patience: 1/50
2024-12-09 19:05:23.533722: train_loss -0.5808
2024-12-09 19:05:23.534617: val_loss -0.4504
2024-12-09 19:05:23.535667: Pseudo dice [0.6998]
2024-12-09 19:05:23.536819: Epoch time: 88.83 s
2024-12-09 19:05:23.871402: Yayy! New best EMA pseudo Dice: 0.6819
2024-12-09 19:05:25.404609: 
2024-12-09 19:05:25.406706: Epoch 25
2024-12-09 19:05:25.407923: Current learning rate: 0.00977
2024-12-09 19:06:54.041692: Validation loss did not improve from -0.48322. Patience: 2/50
2024-12-09 19:06:54.042885: train_loss -0.587
2024-12-09 19:06:54.043813: val_loss -0.4754
2024-12-09 19:06:54.044509: Pseudo dice [0.7074]
2024-12-09 19:06:54.045415: Epoch time: 88.64 s
2024-12-09 19:06:54.046376: Yayy! New best EMA pseudo Dice: 0.6845
2024-12-09 19:06:55.591168: 
2024-12-09 19:06:55.592423: Epoch 26
2024-12-09 19:06:55.593160: Current learning rate: 0.00977
2024-12-09 19:08:24.294716: Validation loss improved from -0.48322 to -0.48478! Patience: 2/50
2024-12-09 19:08:24.295668: train_loss -0.5919
2024-12-09 19:08:24.296542: val_loss -0.4848
2024-12-09 19:08:24.297290: Pseudo dice [0.713]
2024-12-09 19:08:24.298230: Epoch time: 88.71 s
2024-12-09 19:08:24.299113: Yayy! New best EMA pseudo Dice: 0.6873
2024-12-09 19:08:25.802720: 
2024-12-09 19:08:25.803800: Epoch 27
2024-12-09 19:08:25.804691: Current learning rate: 0.00976
2024-12-09 19:09:54.596694: Validation loss did not improve from -0.48478. Patience: 1/50
2024-12-09 19:09:54.597588: train_loss -0.5948
2024-12-09 19:09:54.598806: val_loss -0.4826
2024-12-09 19:09:54.599856: Pseudo dice [0.7126]
2024-12-09 19:09:54.600846: Epoch time: 88.8 s
2024-12-09 19:09:54.601754: Yayy! New best EMA pseudo Dice: 0.6898
2024-12-09 19:09:56.125633: 
2024-12-09 19:09:56.127129: Epoch 28
2024-12-09 19:09:56.127935: Current learning rate: 0.00975
2024-12-09 19:11:25.223954: Validation loss did not improve from -0.48478. Patience: 2/50
2024-12-09 19:11:25.225358: train_loss -0.5992
2024-12-09 19:11:25.226690: val_loss -0.4608
2024-12-09 19:11:25.227606: Pseudo dice [0.6929]
2024-12-09 19:11:25.228515: Epoch time: 89.1 s
2024-12-09 19:11:25.229496: Yayy! New best EMA pseudo Dice: 0.6902
2024-12-09 19:11:27.046428: 
2024-12-09 19:11:27.048543: Epoch 29
2024-12-09 19:11:27.049326: Current learning rate: 0.00974
2024-12-09 19:12:56.166383: Validation loss did not improve from -0.48478. Patience: 3/50
2024-12-09 19:12:56.167737: train_loss -0.6076
2024-12-09 19:12:56.169173: val_loss -0.4597
2024-12-09 19:12:56.170019: Pseudo dice [0.7024]
2024-12-09 19:12:56.171031: Epoch time: 89.12 s
2024-12-09 19:12:56.505086: Yayy! New best EMA pseudo Dice: 0.6914
2024-12-09 19:12:57.994299: 
2024-12-09 19:12:57.996410: Epoch 30
2024-12-09 19:12:57.997616: Current learning rate: 0.00973
2024-12-09 19:14:27.142472: Validation loss did not improve from -0.48478. Patience: 4/50
2024-12-09 19:14:27.143881: train_loss -0.592
2024-12-09 19:14:27.145071: val_loss -0.4639
2024-12-09 19:14:27.146214: Pseudo dice [0.695]
2024-12-09 19:14:27.147178: Epoch time: 89.15 s
2024-12-09 19:14:27.147960: Yayy! New best EMA pseudo Dice: 0.6917
2024-12-09 19:14:28.668164: 
2024-12-09 19:14:28.669918: Epoch 31
2024-12-09 19:14:28.671062: Current learning rate: 0.00972
2024-12-09 19:15:57.705510: Validation loss did not improve from -0.48478. Patience: 5/50
2024-12-09 19:15:57.706667: train_loss -0.611
2024-12-09 19:15:57.707471: val_loss -0.4216
2024-12-09 19:15:57.708239: Pseudo dice [0.6763]
2024-12-09 19:15:57.709037: Epoch time: 89.04 s
2024-12-09 19:15:58.911922: 
2024-12-09 19:15:58.913496: Epoch 32
2024-12-09 19:15:58.914302: Current learning rate: 0.00971
2024-12-09 19:17:28.083944: Validation loss did not improve from -0.48478. Patience: 6/50
2024-12-09 19:17:28.084860: train_loss -0.6042
2024-12-09 19:17:28.086054: val_loss -0.4451
2024-12-09 19:17:28.086867: Pseudo dice [0.6901]
2024-12-09 19:17:28.087944: Epoch time: 89.17 s
2024-12-09 19:17:29.259836: 
2024-12-09 19:17:29.261853: Epoch 33
2024-12-09 19:17:29.262745: Current learning rate: 0.0097
2024-12-09 19:18:58.251949: Validation loss improved from -0.48478 to -0.48752! Patience: 6/50
2024-12-09 19:18:58.253417: train_loss -0.6112
2024-12-09 19:18:58.254632: val_loss -0.4875
2024-12-09 19:18:58.255588: Pseudo dice [0.7127]
2024-12-09 19:18:58.256914: Epoch time: 88.99 s
2024-12-09 19:18:58.257769: Yayy! New best EMA pseudo Dice: 0.6924
2024-12-09 19:18:59.785883: 
2024-12-09 19:18:59.787615: Epoch 34
2024-12-09 19:18:59.788423: Current learning rate: 0.00969
2024-12-09 19:20:28.831285: Validation loss did not improve from -0.48752. Patience: 1/50
2024-12-09 19:20:28.832315: train_loss -0.6207
2024-12-09 19:20:28.833102: val_loss -0.4425
2024-12-09 19:20:28.834308: Pseudo dice [0.6975]
2024-12-09 19:20:28.835505: Epoch time: 89.05 s
2024-12-09 19:20:29.185445: Yayy! New best EMA pseudo Dice: 0.6929
2024-12-09 19:20:30.730899: 
2024-12-09 19:20:30.732342: Epoch 35
2024-12-09 19:20:30.733428: Current learning rate: 0.00968
2024-12-09 19:21:59.791259: Validation loss did not improve from -0.48752. Patience: 2/50
2024-12-09 19:21:59.792260: train_loss -0.6209
2024-12-09 19:21:59.793687: val_loss -0.4401
2024-12-09 19:21:59.794641: Pseudo dice [0.6911]
2024-12-09 19:21:59.795461: Epoch time: 89.06 s
2024-12-09 19:22:01.020142: 
2024-12-09 19:22:01.021739: Epoch 36
2024-12-09 19:22:01.022826: Current learning rate: 0.00968
2024-12-09 19:23:30.259316: Validation loss improved from -0.48752 to -0.49260! Patience: 2/50
2024-12-09 19:23:30.260620: train_loss -0.6231
2024-12-09 19:23:30.261541: val_loss -0.4926
2024-12-09 19:23:30.262292: Pseudo dice [0.7092]
2024-12-09 19:23:30.262968: Epoch time: 89.24 s
2024-12-09 19:23:30.263577: Yayy! New best EMA pseudo Dice: 0.6944
2024-12-09 19:23:31.833564: 
2024-12-09 19:23:31.835455: Epoch 37
2024-12-09 19:23:31.836679: Current learning rate: 0.00967
2024-12-09 19:25:00.865130: Validation loss did not improve from -0.49260. Patience: 1/50
2024-12-09 19:25:00.866149: train_loss -0.6221
2024-12-09 19:25:00.867198: val_loss -0.4893
2024-12-09 19:25:00.868004: Pseudo dice [0.7207]
2024-12-09 19:25:00.868781: Epoch time: 89.03 s
2024-12-09 19:25:00.869779: Yayy! New best EMA pseudo Dice: 0.697
2024-12-09 19:25:02.428270: 
2024-12-09 19:25:02.430324: Epoch 38
2024-12-09 19:25:02.431388: Current learning rate: 0.00966
2024-12-09 19:26:31.481168: Validation loss did not improve from -0.49260. Patience: 2/50
2024-12-09 19:26:31.482340: train_loss -0.6256
2024-12-09 19:26:31.483249: val_loss -0.4797
2024-12-09 19:26:31.484008: Pseudo dice [0.7015]
2024-12-09 19:26:31.485110: Epoch time: 89.05 s
2024-12-09 19:26:31.485845: Yayy! New best EMA pseudo Dice: 0.6975
2024-12-09 19:26:33.375484: 
2024-12-09 19:26:33.377721: Epoch 39
2024-12-09 19:26:33.378973: Current learning rate: 0.00965
2024-12-09 19:28:02.476094: Validation loss did not improve from -0.49260. Patience: 3/50
2024-12-09 19:28:02.478075: train_loss -0.6276
2024-12-09 19:28:02.479485: val_loss -0.4692
2024-12-09 19:28:02.480620: Pseudo dice [0.7073]
2024-12-09 19:28:02.481553: Epoch time: 89.1 s
2024-12-09 19:28:02.830577: Yayy! New best EMA pseudo Dice: 0.6985
2024-12-09 19:28:04.417565: 
2024-12-09 19:28:04.419881: Epoch 40
2024-12-09 19:28:04.420616: Current learning rate: 0.00964
2024-12-09 19:29:33.524912: Validation loss did not improve from -0.49260. Patience: 4/50
2024-12-09 19:29:33.525967: train_loss -0.6224
2024-12-09 19:29:33.526886: val_loss -0.4847
2024-12-09 19:29:33.527665: Pseudo dice [0.7123]
2024-12-09 19:29:33.528406: Epoch time: 89.11 s
2024-12-09 19:29:33.529060: Yayy! New best EMA pseudo Dice: 0.6999
2024-12-09 19:29:35.171240: 
2024-12-09 19:29:35.173159: Epoch 41
2024-12-09 19:29:35.173984: Current learning rate: 0.00963
2024-12-09 19:31:04.391742: Validation loss did not improve from -0.49260. Patience: 5/50
2024-12-09 19:31:04.393302: train_loss -0.6319
2024-12-09 19:31:04.394649: val_loss -0.4861
2024-12-09 19:31:04.395701: Pseudo dice [0.7154]
2024-12-09 19:31:04.396638: Epoch time: 89.22 s
2024-12-09 19:31:04.397336: Yayy! New best EMA pseudo Dice: 0.7014
2024-12-09 19:31:05.925758: 
2024-12-09 19:31:05.927402: Epoch 42
2024-12-09 19:31:05.928254: Current learning rate: 0.00962
2024-12-09 19:32:35.173367: Validation loss improved from -0.49260 to -0.51545! Patience: 5/50
2024-12-09 19:32:35.177301: train_loss -0.6378
2024-12-09 19:32:35.180711: val_loss -0.5155
2024-12-09 19:32:35.181637: Pseudo dice [0.7402]
2024-12-09 19:32:35.183316: Epoch time: 89.25 s
2024-12-09 19:32:35.184057: Yayy! New best EMA pseudo Dice: 0.7053
2024-12-09 19:32:36.840988: 
2024-12-09 19:32:36.842389: Epoch 43
2024-12-09 19:32:36.843309: Current learning rate: 0.00961
2024-12-09 19:34:05.922925: Validation loss did not improve from -0.51545. Patience: 1/50
2024-12-09 19:34:05.924271: train_loss -0.6325
2024-12-09 19:34:05.925663: val_loss -0.4762
2024-12-09 19:34:05.926667: Pseudo dice [0.7073]
2024-12-09 19:34:05.927925: Epoch time: 89.08 s
2024-12-09 19:34:05.929065: Yayy! New best EMA pseudo Dice: 0.7055
2024-12-09 19:34:07.461798: 
2024-12-09 19:34:07.463804: Epoch 44
2024-12-09 19:34:07.464755: Current learning rate: 0.0096
2024-12-09 19:35:36.676685: Validation loss did not improve from -0.51545. Patience: 2/50
2024-12-09 19:35:36.677802: train_loss -0.6345
2024-12-09 19:35:36.678625: val_loss -0.4605
2024-12-09 19:35:36.679385: Pseudo dice [0.7117]
2024-12-09 19:35:36.680096: Epoch time: 89.22 s
2024-12-09 19:35:37.035473: Yayy! New best EMA pseudo Dice: 0.7061
2024-12-09 19:35:38.576008: 
2024-12-09 19:35:38.577860: Epoch 45
2024-12-09 19:35:38.578870: Current learning rate: 0.00959
2024-12-09 19:37:08.002953: Validation loss did not improve from -0.51545. Patience: 3/50
2024-12-09 19:37:08.003960: train_loss -0.6446
2024-12-09 19:37:08.004829: val_loss -0.5095
2024-12-09 19:37:08.005849: Pseudo dice [0.7271]
2024-12-09 19:37:08.006942: Epoch time: 89.43 s
2024-12-09 19:37:08.007741: Yayy! New best EMA pseudo Dice: 0.7082
2024-12-09 19:37:09.553829: 
2024-12-09 19:37:09.555305: Epoch 46
2024-12-09 19:37:09.556428: Current learning rate: 0.00959
2024-12-09 19:38:39.060830: Validation loss did not improve from -0.51545. Patience: 4/50
2024-12-09 19:38:39.062573: train_loss -0.6474
2024-12-09 19:38:39.063852: val_loss -0.4801
2024-12-09 19:38:39.064977: Pseudo dice [0.7196]
2024-12-09 19:38:39.066233: Epoch time: 89.51 s
2024-12-09 19:38:39.067007: Yayy! New best EMA pseudo Dice: 0.7093
2024-12-09 19:38:40.609184: 
2024-12-09 19:38:40.611359: Epoch 47
2024-12-09 19:38:40.612210: Current learning rate: 0.00958
2024-12-09 19:40:10.008173: Validation loss did not improve from -0.51545. Patience: 5/50
2024-12-09 19:40:10.009176: train_loss -0.6442
2024-12-09 19:40:10.010035: val_loss -0.4896
2024-12-09 19:40:10.010697: Pseudo dice [0.7144]
2024-12-09 19:40:10.011517: Epoch time: 89.4 s
2024-12-09 19:40:10.012220: Yayy! New best EMA pseudo Dice: 0.7098
2024-12-09 19:40:11.541314: 
2024-12-09 19:40:11.543174: Epoch 48
2024-12-09 19:40:11.544247: Current learning rate: 0.00957
2024-12-09 19:41:41.000390: Validation loss did not improve from -0.51545. Patience: 6/50
2024-12-09 19:41:41.001360: train_loss -0.6451
2024-12-09 19:41:41.002289: val_loss -0.5048
2024-12-09 19:41:41.003096: Pseudo dice [0.727]
2024-12-09 19:41:41.003785: Epoch time: 89.46 s
2024-12-09 19:41:41.004403: Yayy! New best EMA pseudo Dice: 0.7116
2024-12-09 19:41:42.529661: 
2024-12-09 19:41:42.531248: Epoch 49
2024-12-09 19:41:42.532095: Current learning rate: 0.00956
2024-12-09 19:43:12.118204: Validation loss improved from -0.51545 to -0.51823! Patience: 6/50
2024-12-09 19:43:12.119032: train_loss -0.6538
2024-12-09 19:43:12.120477: val_loss -0.5182
2024-12-09 19:43:12.121537: Pseudo dice [0.7335]
2024-12-09 19:43:12.122190: Epoch time: 89.59 s
2024-12-09 19:43:12.949507: Yayy! New best EMA pseudo Dice: 0.7137
2024-12-09 19:43:14.480014: 
2024-12-09 19:43:14.481926: Epoch 50
2024-12-09 19:43:14.483273: Current learning rate: 0.00955
2024-12-09 19:44:44.109837: Validation loss did not improve from -0.51823. Patience: 1/50
2024-12-09 19:44:44.110849: train_loss -0.6499
2024-12-09 19:44:44.111597: val_loss -0.4787
2024-12-09 19:44:44.112262: Pseudo dice [0.7099]
2024-12-09 19:44:44.113280: Epoch time: 89.63 s
2024-12-09 19:44:45.307481: 
2024-12-09 19:44:45.308929: Epoch 51
2024-12-09 19:44:45.309956: Current learning rate: 0.00954
2024-12-09 19:46:14.929390: Validation loss did not improve from -0.51823. Patience: 2/50
2024-12-09 19:46:14.930254: train_loss -0.6593
2024-12-09 19:46:14.931445: val_loss -0.4733
2024-12-09 19:46:14.932351: Pseudo dice [0.7079]
2024-12-09 19:46:14.933215: Epoch time: 89.62 s
2024-12-09 19:46:16.126226: 
2024-12-09 19:46:16.127930: Epoch 52
2024-12-09 19:46:16.129468: Current learning rate: 0.00953
2024-12-09 19:47:45.885837: Validation loss did not improve from -0.51823. Patience: 3/50
2024-12-09 19:47:45.887025: train_loss -0.6651
2024-12-09 19:47:45.887889: val_loss -0.516
2024-12-09 19:47:45.888619: Pseudo dice [0.7309]
2024-12-09 19:47:45.889725: Epoch time: 89.76 s
2024-12-09 19:47:45.890552: Yayy! New best EMA pseudo Dice: 0.7146
2024-12-09 19:47:47.427039: 
2024-12-09 19:47:47.428865: Epoch 53
2024-12-09 19:47:47.430062: Current learning rate: 0.00952
2024-12-09 19:49:17.029748: Validation loss did not improve from -0.51823. Patience: 4/50
2024-12-09 19:49:17.031013: train_loss -0.6581
2024-12-09 19:49:17.032712: val_loss -0.4559
2024-12-09 19:49:17.033813: Pseudo dice [0.7105]
2024-12-09 19:49:17.035025: Epoch time: 89.6 s
2024-12-09 19:49:18.217632: 
2024-12-09 19:49:18.219091: Epoch 54
2024-12-09 19:49:18.220300: Current learning rate: 0.00951
2024-12-09 19:50:47.457203: Validation loss did not improve from -0.51823. Patience: 5/50
2024-12-09 19:50:47.458069: train_loss -0.6493
2024-12-09 19:50:47.459122: val_loss -0.4825
2024-12-09 19:50:47.460095: Pseudo dice [0.7105]
2024-12-09 19:50:47.460843: Epoch time: 89.24 s
2024-12-09 19:50:49.016341: 
2024-12-09 19:50:49.017737: Epoch 55
2024-12-09 19:50:49.018423: Current learning rate: 0.0095
2024-12-09 19:52:18.251711: Validation loss did not improve from -0.51823. Patience: 6/50
2024-12-09 19:52:18.252891: train_loss -0.6605
2024-12-09 19:52:18.253789: val_loss -0.5055
2024-12-09 19:52:18.254851: Pseudo dice [0.7268]
2024-12-09 19:52:18.256082: Epoch time: 89.24 s
2024-12-09 19:52:18.257215: Yayy! New best EMA pseudo Dice: 0.7151
2024-12-09 19:52:19.803725: 
2024-12-09 19:52:19.805602: Epoch 56
2024-12-09 19:52:19.806797: Current learning rate: 0.00949
2024-12-09 19:53:48.953136: Validation loss did not improve from -0.51823. Patience: 7/50
2024-12-09 19:53:48.954139: train_loss -0.6663
2024-12-09 19:53:48.955072: val_loss -0.4938
2024-12-09 19:53:48.955919: Pseudo dice [0.7218]
2024-12-09 19:53:48.956653: Epoch time: 89.15 s
2024-12-09 19:53:48.957378: Yayy! New best EMA pseudo Dice: 0.7158
2024-12-09 19:53:50.500242: 
2024-12-09 19:53:50.502172: Epoch 57
2024-12-09 19:53:50.503227: Current learning rate: 0.00949
2024-12-09 19:55:19.557700: Validation loss did not improve from -0.51823. Patience: 8/50
2024-12-09 19:55:19.558777: train_loss -0.6656
2024-12-09 19:55:19.559711: val_loss -0.4624
2024-12-09 19:55:19.560461: Pseudo dice [0.696]
2024-12-09 19:55:19.561172: Epoch time: 89.06 s
2024-12-09 19:55:20.770130: 
2024-12-09 19:55:20.771890: Epoch 58
2024-12-09 19:55:20.772688: Current learning rate: 0.00948
2024-12-09 19:56:50.030771: Validation loss did not improve from -0.51823. Patience: 9/50
2024-12-09 19:56:50.031801: train_loss -0.6568
2024-12-09 19:56:50.032666: val_loss -0.4798
2024-12-09 19:56:50.033449: Pseudo dice [0.7085]
2024-12-09 19:56:50.034265: Epoch time: 89.26 s
2024-12-09 19:56:51.258200: 
2024-12-09 19:56:51.259672: Epoch 59
2024-12-09 19:56:51.260365: Current learning rate: 0.00947
2024-12-09 19:58:20.383739: Validation loss did not improve from -0.51823. Patience: 10/50
2024-12-09 19:58:20.384604: train_loss -0.6755
2024-12-09 19:58:20.385880: val_loss -0.4939
2024-12-09 19:58:20.387025: Pseudo dice [0.7173]
2024-12-09 19:58:20.388178: Epoch time: 89.13 s
2024-12-09 19:58:22.267622: 
2024-12-09 19:58:22.269751: Epoch 60
2024-12-09 19:58:22.270969: Current learning rate: 0.00946
2024-12-09 19:59:51.336441: Validation loss did not improve from -0.51823. Patience: 11/50
2024-12-09 19:59:51.337534: train_loss -0.6675
2024-12-09 19:59:51.338361: val_loss -0.438
2024-12-09 19:59:51.339463: Pseudo dice [0.7029]
2024-12-09 19:59:51.340279: Epoch time: 89.07 s
2024-12-09 19:59:52.575162: 
2024-12-09 19:59:52.576701: Epoch 61
2024-12-09 19:59:52.577926: Current learning rate: 0.00945
2024-12-09 20:01:21.911555: Validation loss did not improve from -0.51823. Patience: 12/50
2024-12-09 20:01:21.912542: train_loss -0.6566
2024-12-09 20:01:21.913614: val_loss -0.4872
2024-12-09 20:01:21.914651: Pseudo dice [0.7132]
2024-12-09 20:01:21.915969: Epoch time: 89.34 s
2024-12-09 20:01:23.127818: 
2024-12-09 20:01:23.129692: Epoch 62
2024-12-09 20:01:23.131078: Current learning rate: 0.00944
2024-12-09 20:02:52.754017: Validation loss did not improve from -0.51823. Patience: 13/50
2024-12-09 20:02:52.754922: train_loss -0.6674
2024-12-09 20:02:52.755875: val_loss -0.5041
2024-12-09 20:02:52.756441: Pseudo dice [0.7222]
2024-12-09 20:02:52.757085: Epoch time: 89.63 s
2024-12-09 20:02:53.969232: 
2024-12-09 20:02:53.970920: Epoch 63
2024-12-09 20:02:53.971732: Current learning rate: 0.00943
2024-12-09 20:04:23.487361: Validation loss did not improve from -0.51823. Patience: 14/50
2024-12-09 20:04:23.488622: train_loss -0.6761
2024-12-09 20:04:23.489944: val_loss -0.4959
2024-12-09 20:04:23.491102: Pseudo dice [0.726]
2024-12-09 20:04:23.492259: Epoch time: 89.52 s
2024-12-09 20:04:24.717017: 
2024-12-09 20:04:24.719057: Epoch 64
2024-12-09 20:04:24.720206: Current learning rate: 0.00942
2024-12-09 20:05:54.160878: Validation loss did not improve from -0.51823. Patience: 15/50
2024-12-09 20:05:54.161969: train_loss -0.6766
2024-12-09 20:05:54.162771: val_loss -0.4781
2024-12-09 20:05:54.163445: Pseudo dice [0.7154]
2024-12-09 20:05:54.164233: Epoch time: 89.45 s
2024-12-09 20:05:55.735111: 
2024-12-09 20:05:55.736959: Epoch 65
2024-12-09 20:05:55.738016: Current learning rate: 0.00941
2024-12-09 20:07:25.162970: Validation loss improved from -0.51823 to -0.52940! Patience: 15/50
2024-12-09 20:07:25.164176: train_loss -0.6775
2024-12-09 20:07:25.165751: val_loss -0.5294
2024-12-09 20:07:25.167100: Pseudo dice [0.7421]
2024-12-09 20:07:25.168365: Epoch time: 89.43 s
2024-12-09 20:07:25.169284: Yayy! New best EMA pseudo Dice: 0.7176
2024-12-09 20:07:26.737067: 
2024-12-09 20:07:26.739239: Epoch 66
2024-12-09 20:07:26.740485: Current learning rate: 0.0094
2024-12-09 20:08:56.214047: Validation loss did not improve from -0.52940. Patience: 1/50
2024-12-09 20:08:56.215126: train_loss -0.6787
2024-12-09 20:08:56.215913: val_loss -0.4821
2024-12-09 20:08:56.216661: Pseudo dice [0.711]
2024-12-09 20:08:56.217289: Epoch time: 89.48 s
2024-12-09 20:08:57.429247: 
2024-12-09 20:08:57.430798: Epoch 67
2024-12-09 20:08:57.431622: Current learning rate: 0.00939
2024-12-09 20:10:26.925445: Validation loss did not improve from -0.52940. Patience: 2/50
2024-12-09 20:10:26.926331: train_loss -0.679
2024-12-09 20:10:26.927398: val_loss -0.4644
2024-12-09 20:10:26.928223: Pseudo dice [0.7134]
2024-12-09 20:10:26.929030: Epoch time: 89.5 s
2024-12-09 20:10:28.173028: 
2024-12-09 20:10:28.174827: Epoch 68
2024-12-09 20:10:28.175667: Current learning rate: 0.00939
2024-12-09 20:11:57.554300: Validation loss did not improve from -0.52940. Patience: 3/50
2024-12-09 20:11:57.555356: train_loss -0.6778
2024-12-09 20:11:57.556235: val_loss -0.4849
2024-12-09 20:11:57.557006: Pseudo dice [0.7199]
2024-12-09 20:11:57.557923: Epoch time: 89.38 s
2024-12-09 20:11:58.800326: 
2024-12-09 20:11:58.802154: Epoch 69
2024-12-09 20:11:58.803293: Current learning rate: 0.00938
2024-12-09 20:13:28.469339: Validation loss did not improve from -0.52940. Patience: 4/50
2024-12-09 20:13:28.470403: train_loss -0.6775
2024-12-09 20:13:28.471427: val_loss -0.4862
2024-12-09 20:13:28.472305: Pseudo dice [0.7212]
2024-12-09 20:13:28.473393: Epoch time: 89.67 s
2024-12-09 20:13:30.410274: 
2024-12-09 20:13:30.412249: Epoch 70
2024-12-09 20:13:30.413219: Current learning rate: 0.00937
2024-12-09 20:15:00.083948: Validation loss improved from -0.52940 to -0.53103! Patience: 4/50
2024-12-09 20:15:00.085557: train_loss -0.686
2024-12-09 20:15:00.086780: val_loss -0.531
2024-12-09 20:15:00.087707: Pseudo dice [0.7375]
2024-12-09 20:15:00.088891: Epoch time: 89.68 s
2024-12-09 20:15:00.089586: Yayy! New best EMA pseudo Dice: 0.7194
2024-12-09 20:15:01.695324: 
2024-12-09 20:15:01.696731: Epoch 71
2024-12-09 20:15:01.697625: Current learning rate: 0.00936
2024-12-09 20:16:31.154506: Validation loss did not improve from -0.53103. Patience: 1/50
2024-12-09 20:16:31.155745: train_loss -0.6834
2024-12-09 20:16:31.156545: val_loss -0.4794
2024-12-09 20:16:31.157231: Pseudo dice [0.7075]
2024-12-09 20:16:31.157942: Epoch time: 89.46 s
2024-12-09 20:16:32.445475: 
2024-12-09 20:16:32.447247: Epoch 72
2024-12-09 20:16:32.447982: Current learning rate: 0.00935
2024-12-09 20:18:01.940010: Validation loss did not improve from -0.53103. Patience: 2/50
2024-12-09 20:18:01.941566: train_loss -0.6873
2024-12-09 20:18:01.942930: val_loss -0.507
2024-12-09 20:18:01.944284: Pseudo dice [0.7361]
2024-12-09 20:18:01.945128: Epoch time: 89.5 s
2024-12-09 20:18:01.945893: Yayy! New best EMA pseudo Dice: 0.72
2024-12-09 20:18:03.905364: 
2024-12-09 20:18:03.907080: Epoch 73
2024-12-09 20:18:03.908261: Current learning rate: 0.00934
2024-12-09 20:19:33.445687: Validation loss did not improve from -0.53103. Patience: 3/50
2024-12-09 20:19:33.447358: train_loss -0.6897
2024-12-09 20:19:33.448289: val_loss -0.4908
2024-12-09 20:19:33.449412: Pseudo dice [0.7269]
2024-12-09 20:19:33.450883: Epoch time: 89.54 s
2024-12-09 20:19:33.451670: Yayy! New best EMA pseudo Dice: 0.7207
2024-12-09 20:19:35.033684: 
2024-12-09 20:19:35.035410: Epoch 74
2024-12-09 20:19:35.036347: Current learning rate: 0.00933
2024-12-09 20:21:04.477182: Validation loss did not improve from -0.53103. Patience: 4/50
2024-12-09 20:21:04.478533: train_loss -0.6899
2024-12-09 20:21:04.480147: val_loss -0.4739
2024-12-09 20:21:04.480789: Pseudo dice [0.7087]
2024-12-09 20:21:04.481884: Epoch time: 89.45 s
2024-12-09 20:21:06.141267: 
2024-12-09 20:21:06.143145: Epoch 75
2024-12-09 20:21:06.143991: Current learning rate: 0.00932
2024-12-09 20:22:35.724587: Validation loss did not improve from -0.53103. Patience: 5/50
2024-12-09 20:22:35.725678: train_loss -0.6865
2024-12-09 20:22:35.726627: val_loss -0.5044
2024-12-09 20:22:35.727413: Pseudo dice [0.7204]
2024-12-09 20:22:35.728123: Epoch time: 89.59 s
2024-12-09 20:22:36.981156: 
2024-12-09 20:22:36.983008: Epoch 76
2024-12-09 20:22:36.983989: Current learning rate: 0.00931
2024-12-09 20:24:06.427330: Validation loss did not improve from -0.53103. Patience: 6/50
2024-12-09 20:24:06.428480: train_loss -0.6916
2024-12-09 20:24:06.429385: val_loss -0.4749
2024-12-09 20:24:06.430274: Pseudo dice [0.7126]
2024-12-09 20:24:06.431523: Epoch time: 89.45 s
2024-12-09 20:24:07.671420: 
2024-12-09 20:24:07.673049: Epoch 77
2024-12-09 20:24:07.673841: Current learning rate: 0.0093
2024-12-09 20:25:37.290168: Validation loss did not improve from -0.53103. Patience: 7/50
2024-12-09 20:25:37.291200: train_loss -0.6864
2024-12-09 20:25:37.292095: val_loss -0.4877
2024-12-09 20:25:37.293164: Pseudo dice [0.7178]
2024-12-09 20:25:37.294214: Epoch time: 89.62 s
2024-12-09 20:25:38.555209: 
2024-12-09 20:25:38.557315: Epoch 78
2024-12-09 20:25:38.558221: Current learning rate: 0.0093
2024-12-09 20:27:08.176721: Validation loss did not improve from -0.53103. Patience: 8/50
2024-12-09 20:27:08.178405: train_loss -0.6854
2024-12-09 20:27:08.180228: val_loss -0.4734
2024-12-09 20:27:08.181163: Pseudo dice [0.707]
2024-12-09 20:27:08.181765: Epoch time: 89.62 s
2024-12-09 20:27:09.458912: 
2024-12-09 20:27:09.460428: Epoch 79
2024-12-09 20:27:09.461237: Current learning rate: 0.00929
2024-12-09 20:28:39.017498: Validation loss improved from -0.53103 to -0.53692! Patience: 8/50
2024-12-09 20:28:39.019038: train_loss -0.6973
2024-12-09 20:28:39.020013: val_loss -0.5369
2024-12-09 20:28:39.020945: Pseudo dice [0.7387]
2024-12-09 20:28:39.022224: Epoch time: 89.56 s
2024-12-09 20:28:40.961823: 
2024-12-09 20:28:40.963449: Epoch 80
2024-12-09 20:28:40.964176: Current learning rate: 0.00928
2024-12-09 20:30:10.385436: Validation loss did not improve from -0.53692. Patience: 1/50
2024-12-09 20:30:10.386781: train_loss -0.6916
2024-12-09 20:30:10.387712: val_loss -0.5198
2024-12-09 20:30:10.388831: Pseudo dice [0.7311]
2024-12-09 20:30:10.389679: Epoch time: 89.43 s
2024-12-09 20:30:10.390356: Yayy! New best EMA pseudo Dice: 0.7208
2024-12-09 20:30:12.007047: 
2024-12-09 20:30:12.009384: Epoch 81
2024-12-09 20:30:12.010099: Current learning rate: 0.00927
2024-12-09 20:31:41.521949: Validation loss did not improve from -0.53692. Patience: 2/50
2024-12-09 20:31:41.523349: train_loss -0.6985
2024-12-09 20:31:41.524324: val_loss -0.4976
2024-12-09 20:31:41.525343: Pseudo dice [0.724]
2024-12-09 20:31:41.526183: Epoch time: 89.52 s
2024-12-09 20:31:41.527117: Yayy! New best EMA pseudo Dice: 0.7212
2024-12-09 20:31:43.152766: 
2024-12-09 20:31:43.154273: Epoch 82
2024-12-09 20:31:43.155151: Current learning rate: 0.00926
2024-12-09 20:33:12.598407: Validation loss did not improve from -0.53692. Patience: 3/50
2024-12-09 20:33:12.599377: train_loss -0.6936
2024-12-09 20:33:12.600153: val_loss -0.4953
2024-12-09 20:33:12.601115: Pseudo dice [0.7318]
2024-12-09 20:33:12.602132: Epoch time: 89.45 s
2024-12-09 20:33:12.602839: Yayy! New best EMA pseudo Dice: 0.7222
2024-12-09 20:33:14.143036: 
2024-12-09 20:33:14.144800: Epoch 83
2024-12-09 20:33:14.145830: Current learning rate: 0.00925
2024-12-09 20:34:43.821059: Validation loss did not improve from -0.53692. Patience: 4/50
2024-12-09 20:34:43.848024: train_loss -0.7044
2024-12-09 20:34:43.851282: val_loss -0.4976
2024-12-09 20:34:43.852479: Pseudo dice [0.7275]
2024-12-09 20:34:43.854208: Epoch time: 89.7 s
2024-12-09 20:34:43.856198: Yayy! New best EMA pseudo Dice: 0.7228
2024-12-09 20:34:45.475096: 
2024-12-09 20:34:45.476750: Epoch 84
2024-12-09 20:34:45.477668: Current learning rate: 0.00924
2024-12-09 20:36:14.886968: Validation loss did not improve from -0.53692. Patience: 5/50
2024-12-09 20:36:14.888375: train_loss -0.6974
2024-12-09 20:36:14.889728: val_loss -0.5102
2024-12-09 20:36:14.890420: Pseudo dice [0.7331]
2024-12-09 20:36:14.891106: Epoch time: 89.41 s
2024-12-09 20:36:15.251651: Yayy! New best EMA pseudo Dice: 0.7238
2024-12-09 20:36:16.765614: 
2024-12-09 20:36:16.825796: Epoch 85
2024-12-09 20:36:16.826930: Current learning rate: 0.00923
2024-12-09 20:37:46.225628: Validation loss did not improve from -0.53692. Patience: 6/50
2024-12-09 20:37:46.228484: train_loss -0.6951
2024-12-09 20:37:46.230019: val_loss -0.4451
2024-12-09 20:37:46.231118: Pseudo dice [0.7124]
2024-12-09 20:37:46.231922: Epoch time: 89.46 s
2024-12-09 20:37:47.461842: 
2024-12-09 20:37:47.463736: Epoch 86
2024-12-09 20:37:47.465077: Current learning rate: 0.00922
2024-12-09 20:39:17.002526: Validation loss did not improve from -0.53692. Patience: 7/50
2024-12-09 20:39:17.003589: train_loss -0.6945
2024-12-09 20:39:17.004261: val_loss -0.5257
2024-12-09 20:39:17.004962: Pseudo dice [0.736]
2024-12-09 20:39:17.005573: Epoch time: 89.54 s
2024-12-09 20:39:17.006543: Yayy! New best EMA pseudo Dice: 0.724
2024-12-09 20:39:18.546978: 
2024-12-09 20:39:18.548375: Epoch 87
2024-12-09 20:39:18.549073: Current learning rate: 0.00921
2024-12-09 20:40:47.961931: Validation loss did not improve from -0.53692. Patience: 8/50
2024-12-09 20:40:47.963338: train_loss -0.702
2024-12-09 20:40:47.964535: val_loss -0.4865
2024-12-09 20:40:47.965360: Pseudo dice [0.7177]
2024-12-09 20:40:47.966166: Epoch time: 89.42 s
2024-12-09 20:40:49.154349: 
2024-12-09 20:40:49.156444: Epoch 88
2024-12-09 20:40:49.157609: Current learning rate: 0.0092
2024-12-09 20:42:18.443234: Validation loss did not improve from -0.53692. Patience: 9/50
2024-12-09 20:42:18.444347: train_loss -0.7098
2024-12-09 20:42:18.445638: val_loss -0.4809
2024-12-09 20:42:18.446593: Pseudo dice [0.7127]
2024-12-09 20:42:18.447433: Epoch time: 89.29 s
2024-12-09 20:42:19.649258: 
2024-12-09 20:42:19.651029: Epoch 89
2024-12-09 20:42:19.652023: Current learning rate: 0.0092
2024-12-09 20:43:49.139857: Validation loss did not improve from -0.53692. Patience: 10/50
2024-12-09 20:43:49.141023: train_loss -0.6992
2024-12-09 20:43:49.142133: val_loss -0.5197
2024-12-09 20:43:49.142846: Pseudo dice [0.7296]
2024-12-09 20:43:49.143882: Epoch time: 89.49 s
2024-12-09 20:43:50.714089: 
2024-12-09 20:43:50.715741: Epoch 90
2024-12-09 20:43:50.716493: Current learning rate: 0.00919
2024-12-09 20:45:20.361078: Validation loss did not improve from -0.53692. Patience: 11/50
2024-12-09 20:45:20.363025: train_loss -0.7122
2024-12-09 20:45:20.363977: val_loss -0.5279
2024-12-09 20:45:20.364949: Pseudo dice [0.732]
2024-12-09 20:45:20.365676: Epoch time: 89.65 s
2024-12-09 20:45:21.549380: 
2024-12-09 20:45:21.551663: Epoch 91
2024-12-09 20:45:21.552705: Current learning rate: 0.00918
2024-12-09 20:46:50.945347: Validation loss did not improve from -0.53692. Patience: 12/50
2024-12-09 20:46:50.946978: train_loss -0.7114
2024-12-09 20:46:50.948380: val_loss -0.5273
2024-12-09 20:46:50.948957: Pseudo dice [0.7375]
2024-12-09 20:46:50.949969: Epoch time: 89.4 s
2024-12-09 20:46:50.950578: Yayy! New best EMA pseudo Dice: 0.7253
2024-12-09 20:46:52.471166: 
2024-12-09 20:46:52.473404: Epoch 92
2024-12-09 20:46:52.474141: Current learning rate: 0.00917
2024-12-09 20:48:21.879006: Validation loss did not improve from -0.53692. Patience: 13/50
2024-12-09 20:48:21.880397: train_loss -0.7123
2024-12-09 20:48:21.881516: val_loss -0.5122
2024-12-09 20:48:21.882629: Pseudo dice [0.7259]
2024-12-09 20:48:21.883450: Epoch time: 89.41 s
2024-12-09 20:48:21.884325: Yayy! New best EMA pseudo Dice: 0.7253
2024-12-09 20:48:23.415998: 
2024-12-09 20:48:23.418060: Epoch 93
2024-12-09 20:48:23.419476: Current learning rate: 0.00916
2024-12-09 20:49:52.776628: Validation loss did not improve from -0.53692. Patience: 14/50
2024-12-09 20:49:52.777752: train_loss -0.7035
2024-12-09 20:49:52.778614: val_loss -0.4966
2024-12-09 20:49:52.779550: Pseudo dice [0.7299]
2024-12-09 20:49:52.780448: Epoch time: 89.36 s
2024-12-09 20:49:52.781414: Yayy! New best EMA pseudo Dice: 0.7258
2024-12-09 20:49:54.300574: 
2024-12-09 20:49:54.302383: Epoch 94
2024-12-09 20:49:54.303189: Current learning rate: 0.00915
2024-12-09 20:51:23.491927: Validation loss did not improve from -0.53692. Patience: 15/50
2024-12-09 20:51:23.492711: train_loss -0.703
2024-12-09 20:51:23.493787: val_loss -0.4888
2024-12-09 20:51:23.494879: Pseudo dice [0.7271]
2024-12-09 20:51:23.495645: Epoch time: 89.19 s
2024-12-09 20:51:23.854340: Yayy! New best EMA pseudo Dice: 0.7259
2024-12-09 20:51:25.373534: 
2024-12-09 20:51:25.375665: Epoch 95
2024-12-09 20:51:25.376541: Current learning rate: 0.00914
2024-12-09 20:52:54.825968: Validation loss did not improve from -0.53692. Patience: 16/50
2024-12-09 20:52:54.826967: train_loss -0.7076
2024-12-09 20:52:54.827760: val_loss -0.5057
2024-12-09 20:52:54.828442: Pseudo dice [0.7295]
2024-12-09 20:52:54.829275: Epoch time: 89.45 s
2024-12-09 20:52:54.830172: Yayy! New best EMA pseudo Dice: 0.7263
2024-12-09 20:52:56.366504: 
2024-12-09 20:52:56.368580: Epoch 96
2024-12-09 20:52:56.369507: Current learning rate: 0.00913
2024-12-09 20:54:25.936633: Validation loss did not improve from -0.53692. Patience: 17/50
2024-12-09 20:54:25.937871: train_loss -0.7085
2024-12-09 20:54:25.938748: val_loss -0.4952
2024-12-09 20:54:25.939478: Pseudo dice [0.7265]
2024-12-09 20:54:25.940552: Epoch time: 89.57 s
2024-12-09 20:54:25.941221: Yayy! New best EMA pseudo Dice: 0.7263
2024-12-09 20:54:27.438069: 
2024-12-09 20:54:27.440328: Epoch 97
2024-12-09 20:54:27.441036: Current learning rate: 0.00912
2024-12-09 20:55:56.937581: Validation loss did not improve from -0.53692. Patience: 18/50
2024-12-09 20:55:56.938687: train_loss -0.7111
2024-12-09 20:55:56.940061: val_loss -0.4846
2024-12-09 20:55:56.941231: Pseudo dice [0.6989]
2024-12-09 20:55:56.941957: Epoch time: 89.5 s
2024-12-09 20:55:58.126430: 
2024-12-09 20:55:58.128242: Epoch 98
2024-12-09 20:55:58.129227: Current learning rate: 0.00911
2024-12-09 20:57:27.533388: Validation loss did not improve from -0.53692. Patience: 19/50
2024-12-09 20:57:27.534236: train_loss -0.7124
2024-12-09 20:57:27.535135: val_loss -0.5153
2024-12-09 20:57:27.535836: Pseudo dice [0.7256]
2024-12-09 20:57:27.536536: Epoch time: 89.41 s
2024-12-09 20:57:28.735787: 
2024-12-09 20:57:28.737375: Epoch 99
2024-12-09 20:57:28.738126: Current learning rate: 0.0091
2024-12-09 20:58:58.101078: Validation loss did not improve from -0.53692. Patience: 20/50
2024-12-09 20:58:58.102285: train_loss -0.719
2024-12-09 20:58:58.103170: val_loss -0.4815
2024-12-09 20:58:58.103857: Pseudo dice [0.7171]
2024-12-09 20:58:58.104643: Epoch time: 89.37 s
2024-12-09 20:58:59.673751: 
2024-12-09 20:58:59.676803: Epoch 100
2024-12-09 20:58:59.678138: Current learning rate: 0.0091
2024-12-09 21:00:28.950555: Validation loss did not improve from -0.53692. Patience: 21/50
2024-12-09 21:00:28.952032: train_loss -0.7208
2024-12-09 21:00:28.952940: val_loss -0.4862
2024-12-09 21:00:28.953724: Pseudo dice [0.7157]
2024-12-09 21:00:28.954562: Epoch time: 89.28 s
2024-12-09 21:00:30.480076: 
2024-12-09 21:00:30.482169: Epoch 101
2024-12-09 21:00:30.483174: Current learning rate: 0.00909
2024-12-09 21:01:59.825374: Validation loss did not improve from -0.53692. Patience: 22/50
2024-12-09 21:01:59.826317: train_loss -0.7187
2024-12-09 21:01:59.827024: val_loss -0.515
2024-12-09 21:01:59.827963: Pseudo dice [0.7347]
2024-12-09 21:01:59.829018: Epoch time: 89.35 s
2024-12-09 21:02:01.029761: 
2024-12-09 21:02:01.031564: Epoch 102
2024-12-09 21:02:01.032315: Current learning rate: 0.00908
2024-12-09 21:03:30.362497: Validation loss did not improve from -0.53692. Patience: 23/50
2024-12-09 21:03:30.363751: train_loss -0.7126
2024-12-09 21:03:30.364835: val_loss -0.5064
2024-12-09 21:03:30.365994: Pseudo dice [0.7286]
2024-12-09 21:03:30.366947: Epoch time: 89.33 s
2024-12-09 21:03:31.577450: 
2024-12-09 21:03:31.579636: Epoch 103
2024-12-09 21:03:31.580599: Current learning rate: 0.00907
2024-12-09 21:05:00.936633: Validation loss did not improve from -0.53692. Patience: 24/50
2024-12-09 21:05:00.938041: train_loss -0.7105
2024-12-09 21:05:00.938812: val_loss -0.5193
2024-12-09 21:05:00.939454: Pseudo dice [0.7406]
2024-12-09 21:05:00.940202: Epoch time: 89.36 s
2024-12-09 21:05:02.126358: 
2024-12-09 21:05:02.128217: Epoch 104
2024-12-09 21:05:02.129253: Current learning rate: 0.00906
2024-12-09 21:06:31.571632: Validation loss did not improve from -0.53692. Patience: 25/50
2024-12-09 21:06:31.572715: train_loss -0.721
2024-12-09 21:06:31.573489: val_loss -0.5106
2024-12-09 21:06:31.574385: Pseudo dice [0.7254]
2024-12-09 21:06:31.575346: Epoch time: 89.45 s
2024-12-09 21:06:33.113547: 
2024-12-09 21:06:33.115049: Epoch 105
2024-12-09 21:06:33.115927: Current learning rate: 0.00905
2024-12-09 21:08:02.474912: Validation loss did not improve from -0.53692. Patience: 26/50
2024-12-09 21:08:02.475711: train_loss -0.7134
2024-12-09 21:08:02.477098: val_loss -0.4989
2024-12-09 21:08:02.478178: Pseudo dice [0.733]
2024-12-09 21:08:02.479141: Epoch time: 89.36 s
2024-12-09 21:08:02.480026: Yayy! New best EMA pseudo Dice: 0.7264
2024-12-09 21:08:04.044709: 
2024-12-09 21:08:04.046801: Epoch 106
2024-12-09 21:08:04.047890: Current learning rate: 0.00904
2024-12-09 21:09:33.437980: Validation loss did not improve from -0.53692. Patience: 27/50
2024-12-09 21:09:33.439224: train_loss -0.725
2024-12-09 21:09:33.440130: val_loss -0.4951
2024-12-09 21:09:33.440708: Pseudo dice [0.7229]
2024-12-09 21:09:33.441318: Epoch time: 89.4 s
2024-12-09 21:09:34.658457: 
2024-12-09 21:09:34.660129: Epoch 107
2024-12-09 21:09:34.661240: Current learning rate: 0.00903
2024-12-09 21:11:04.350049: Validation loss did not improve from -0.53692. Patience: 28/50
2024-12-09 21:11:04.351752: train_loss -0.7094
2024-12-09 21:11:04.353419: val_loss -0.5047
2024-12-09 21:11:04.354477: Pseudo dice [0.737]
2024-12-09 21:11:04.355305: Epoch time: 89.69 s
2024-12-09 21:11:04.356043: Yayy! New best EMA pseudo Dice: 0.7272
2024-12-09 21:11:05.904852: 
2024-12-09 21:11:05.906931: Epoch 108
2024-12-09 21:11:05.908141: Current learning rate: 0.00902
2024-12-09 21:12:35.543221: Validation loss did not improve from -0.53692. Patience: 29/50
2024-12-09 21:12:35.544475: train_loss -0.7178
2024-12-09 21:12:35.545761: val_loss -0.5105
2024-12-09 21:12:35.546745: Pseudo dice [0.7178]
2024-12-09 21:12:35.547478: Epoch time: 89.64 s
2024-12-09 21:12:36.761109: 
2024-12-09 21:12:36.763627: Epoch 109
2024-12-09 21:12:36.764404: Current learning rate: 0.00901
2024-12-09 21:14:06.362326: Validation loss did not improve from -0.53692. Patience: 30/50
2024-12-09 21:14:06.363386: train_loss -0.7223
2024-12-09 21:14:06.364736: val_loss -0.4788
2024-12-09 21:14:06.365768: Pseudo dice [0.7172]
2024-12-09 21:14:06.367083: Epoch time: 89.6 s
2024-12-09 21:14:07.962775: 
2024-12-09 21:14:07.964602: Epoch 110
2024-12-09 21:14:07.965833: Current learning rate: 0.009
2024-12-09 21:15:37.385202: Validation loss did not improve from -0.53692. Patience: 31/50
2024-12-09 21:15:37.386161: train_loss -0.7217
2024-12-09 21:15:37.387423: val_loss -0.474
2024-12-09 21:15:37.388343: Pseudo dice [0.7057]
2024-12-09 21:15:37.389356: Epoch time: 89.42 s
2024-12-09 21:15:38.585844: 
2024-12-09 21:15:38.588012: Epoch 111
2024-12-09 21:15:38.589175: Current learning rate: 0.009
2024-12-09 21:17:07.959496: Validation loss did not improve from -0.53692. Patience: 32/50
2024-12-09 21:17:07.960413: train_loss -0.7259
2024-12-09 21:17:07.961189: val_loss -0.4827
2024-12-09 21:17:07.961871: Pseudo dice [0.7212]
2024-12-09 21:17:07.962513: Epoch time: 89.38 s
2024-12-09 21:17:09.479911: 
2024-12-09 21:17:09.481489: Epoch 112
2024-12-09 21:17:09.482451: Current learning rate: 0.00899
2024-12-09 21:18:38.917925: Validation loss did not improve from -0.53692. Patience: 33/50
2024-12-09 21:18:38.919555: train_loss -0.7242
2024-12-09 21:18:38.920538: val_loss -0.4785
2024-12-09 21:18:38.921307: Pseudo dice [0.7258]
2024-12-09 21:18:38.922152: Epoch time: 89.44 s
2024-12-09 21:18:40.140456: 
2024-12-09 21:18:40.142068: Epoch 113
2024-12-09 21:18:40.142893: Current learning rate: 0.00898
2024-12-09 21:20:09.554487: Validation loss did not improve from -0.53692. Patience: 34/50
2024-12-09 21:20:09.555689: train_loss -0.7235
2024-12-09 21:20:09.556650: val_loss -0.5188
2024-12-09 21:20:09.557650: Pseudo dice [0.733]
2024-12-09 21:20:09.558774: Epoch time: 89.42 s
2024-12-09 21:20:10.762075: 
2024-12-09 21:20:10.763753: Epoch 114
2024-12-09 21:20:10.764586: Current learning rate: 0.00897
2024-12-09 21:21:40.159894: Validation loss did not improve from -0.53692. Patience: 35/50
2024-12-09 21:21:40.160828: train_loss -0.7324
2024-12-09 21:21:40.162086: val_loss -0.51
2024-12-09 21:21:40.163241: Pseudo dice [0.7284]
2024-12-09 21:21:40.163969: Epoch time: 89.4 s
2024-12-09 21:21:41.742699: 
2024-12-09 21:21:41.744396: Epoch 115
2024-12-09 21:21:41.745339: Current learning rate: 0.00896
2024-12-09 21:23:11.165759: Validation loss did not improve from -0.53692. Patience: 36/50
2024-12-09 21:23:11.166830: train_loss -0.7274
2024-12-09 21:23:11.168035: val_loss -0.4735
2024-12-09 21:23:11.168995: Pseudo dice [0.7227]
2024-12-09 21:23:11.170034: Epoch time: 89.43 s
2024-12-09 21:23:12.403835: 
2024-12-09 21:23:12.405809: Epoch 116
2024-12-09 21:23:12.407023: Current learning rate: 0.00895
2024-12-09 21:24:41.864072: Validation loss did not improve from -0.53692. Patience: 37/50
2024-12-09 21:24:41.865063: train_loss -0.7281
2024-12-09 21:24:41.866369: val_loss -0.5186
2024-12-09 21:24:41.867297: Pseudo dice [0.7358]
2024-12-09 21:24:41.868508: Epoch time: 89.46 s
2024-12-09 21:24:43.109046: 
2024-12-09 21:24:43.110468: Epoch 117
2024-12-09 21:24:43.111474: Current learning rate: 0.00894
2024-12-09 21:26:12.539656: Validation loss did not improve from -0.53692. Patience: 38/50
2024-12-09 21:26:12.540639: train_loss -0.7265
2024-12-09 21:26:12.541626: val_loss -0.5083
2024-12-09 21:26:12.542351: Pseudo dice [0.7313]
2024-12-09 21:26:12.543020: Epoch time: 89.43 s
2024-12-09 21:26:13.794545: 
2024-12-09 21:26:13.796559: Epoch 118
2024-12-09 21:26:13.797277: Current learning rate: 0.00893
2024-12-09 21:27:43.417002: Validation loss did not improve from -0.53692. Patience: 39/50
2024-12-09 21:27:43.417839: train_loss -0.726
2024-12-09 21:27:43.419168: val_loss -0.5028
2024-12-09 21:27:43.420225: Pseudo dice [0.7228]
2024-12-09 21:27:43.421251: Epoch time: 89.62 s
2024-12-09 21:27:44.647002: 
2024-12-09 21:27:44.648696: Epoch 119
2024-12-09 21:27:44.649982: Current learning rate: 0.00892
2024-12-09 21:29:13.962450: Validation loss did not improve from -0.53692. Patience: 40/50
2024-12-09 21:29:13.963433: train_loss -0.7329
2024-12-09 21:29:13.964557: val_loss -0.505
2024-12-09 21:29:13.965918: Pseudo dice [0.7256]
2024-12-09 21:29:13.966633: Epoch time: 89.32 s
2024-12-09 21:29:15.570507: 
2024-12-09 21:29:15.572597: Epoch 120
2024-12-09 21:29:15.573623: Current learning rate: 0.00891
2024-12-09 21:30:45.021113: Validation loss did not improve from -0.53692. Patience: 41/50
2024-12-09 21:30:45.022094: train_loss -0.7061
2024-12-09 21:30:45.023019: val_loss -0.4972
2024-12-09 21:30:45.023801: Pseudo dice [0.7251]
2024-12-09 21:30:45.024482: Epoch time: 89.45 s
2024-12-09 21:30:46.251121: 
2024-12-09 21:30:46.253477: Epoch 121
2024-12-09 21:30:46.254498: Current learning rate: 0.0089
2024-12-09 21:32:15.643067: Validation loss did not improve from -0.53692. Patience: 42/50
2024-12-09 21:32:15.644185: train_loss -0.7159
2024-12-09 21:32:15.645710: val_loss -0.4673
2024-12-09 21:32:15.646328: Pseudo dice [0.7051]
2024-12-09 21:32:15.647354: Epoch time: 89.39 s
2024-12-09 21:32:17.173430: 
2024-12-09 21:32:17.175287: Epoch 122
2024-12-09 21:32:17.176207: Current learning rate: 0.00889
2024-12-09 21:33:46.578369: Validation loss did not improve from -0.53692. Patience: 43/50
2024-12-09 21:33:46.579634: train_loss -0.7172
2024-12-09 21:33:46.580909: val_loss -0.4695
2024-12-09 21:33:46.581968: Pseudo dice [0.7111]
2024-12-09 21:33:46.583064: Epoch time: 89.41 s
2024-12-09 21:33:47.800638: 
2024-12-09 21:33:47.802984: Epoch 123
2024-12-09 21:33:47.803856: Current learning rate: 0.00889
2024-12-09 21:35:17.218734: Validation loss did not improve from -0.53692. Patience: 44/50
2024-12-09 21:35:17.219884: train_loss -0.7211
2024-12-09 21:35:17.221182: val_loss -0.5224
2024-12-09 21:35:17.222126: Pseudo dice [0.7335]
2024-12-09 21:35:17.222990: Epoch time: 89.42 s
2024-12-09 21:35:18.455395: 
2024-12-09 21:35:18.456976: Epoch 124
2024-12-09 21:35:18.458177: Current learning rate: 0.00888
2024-12-09 21:36:47.884045: Validation loss did not improve from -0.53692. Patience: 45/50
2024-12-09 21:36:47.884925: train_loss -0.7288
2024-12-09 21:36:47.885778: val_loss -0.5287
2024-12-09 21:36:47.886954: Pseudo dice [0.7363]
2024-12-09 21:36:47.887908: Epoch time: 89.43 s
2024-12-09 21:36:49.472344: 
2024-12-09 21:36:49.474491: Epoch 125
2024-12-09 21:36:49.475768: Current learning rate: 0.00887
2024-12-09 21:38:18.828170: Validation loss did not improve from -0.53692. Patience: 46/50
2024-12-09 21:38:18.829472: train_loss -0.7276
2024-12-09 21:38:18.830614: val_loss -0.479
2024-12-09 21:38:18.831574: Pseudo dice [0.7206]
2024-12-09 21:38:18.832358: Epoch time: 89.36 s
2024-12-09 21:38:20.068894: 
2024-12-09 21:38:20.070616: Epoch 126
2024-12-09 21:38:20.071767: Current learning rate: 0.00886
2024-12-09 21:39:49.777470: Validation loss did not improve from -0.53692. Patience: 47/50
2024-12-09 21:39:49.782100: train_loss -0.73
2024-12-09 21:39:49.784950: val_loss -0.4856
2024-12-09 21:39:49.786194: Pseudo dice [0.721]
2024-12-09 21:39:49.788155: Epoch time: 89.71 s
2024-12-09 21:39:51.084622: 
2024-12-09 21:39:51.086446: Epoch 127
2024-12-09 21:39:51.087528: Current learning rate: 0.00885
2024-12-09 21:41:20.700059: Validation loss did not improve from -0.53692. Patience: 48/50
2024-12-09 21:41:20.700850: train_loss -0.7424
2024-12-09 21:41:20.701776: val_loss -0.5157
2024-12-09 21:41:20.702872: Pseudo dice [0.7299]
2024-12-09 21:41:20.703575: Epoch time: 89.62 s
2024-12-09 21:41:21.945539: 
2024-12-09 21:41:21.947431: Epoch 128
2024-12-09 21:41:21.948259: Current learning rate: 0.00884
2024-12-09 21:42:51.285266: Validation loss did not improve from -0.53692. Patience: 49/50
2024-12-09 21:42:51.288732: train_loss -0.7395
2024-12-09 21:42:51.290095: val_loss -0.5287
2024-12-09 21:42:51.290756: Pseudo dice [0.7351]
2024-12-09 21:42:51.291435: Epoch time: 89.34 s
2024-12-09 21:42:52.542175: 
2024-12-09 21:42:52.544505: Epoch 129
2024-12-09 21:42:52.545585: Current learning rate: 0.00883
2024-12-09 21:44:21.987142: Validation loss improved from -0.53692 to -0.54278! Patience: 49/50
2024-12-09 21:44:21.988801: train_loss -0.7373
2024-12-09 21:44:21.989579: val_loss -0.5428
2024-12-09 21:44:21.990675: Pseudo dice [0.7437]
2024-12-09 21:44:21.992036: Epoch time: 89.45 s
2024-12-09 21:44:22.359380: Yayy! New best EMA pseudo Dice: 0.7275
2024-12-09 21:44:23.893145: 
2024-12-09 21:44:23.895028: Epoch 130
2024-12-09 21:44:23.896474: Current learning rate: 0.00882
2024-12-09 21:45:53.137268: Validation loss did not improve from -0.54278. Patience: 1/50
2024-12-09 21:45:53.138436: train_loss -0.7346
2024-12-09 21:45:53.139262: val_loss -0.5308
2024-12-09 21:45:53.140415: Pseudo dice [0.7419]
2024-12-09 21:45:53.141240: Epoch time: 89.25 s
2024-12-09 21:45:53.142167: Yayy! New best EMA pseudo Dice: 0.7289
2024-12-09 21:45:54.727769: 
2024-12-09 21:45:54.729447: Epoch 131
2024-12-09 21:45:54.730156: Current learning rate: 0.00881
2024-12-09 21:47:24.030030: Validation loss did not improve from -0.54278. Patience: 2/50
2024-12-09 21:47:24.030900: train_loss -0.7361
2024-12-09 21:47:24.031844: val_loss -0.5202
2024-12-09 21:47:24.032774: Pseudo dice [0.7392]
2024-12-09 21:47:24.033668: Epoch time: 89.3 s
2024-12-09 21:47:24.034528: Yayy! New best EMA pseudo Dice: 0.73
2024-12-09 21:47:25.612365: 
2024-12-09 21:47:25.614310: Epoch 132
2024-12-09 21:47:25.615782: Current learning rate: 0.0088
2024-12-09 21:48:56.081136: Validation loss did not improve from -0.54278. Patience: 3/50
2024-12-09 21:48:56.082230: train_loss -0.7302
2024-12-09 21:48:56.083249: val_loss -0.499
2024-12-09 21:48:56.084070: Pseudo dice [0.7283]
2024-12-09 21:48:56.085207: Epoch time: 90.47 s
2024-12-09 21:48:57.327131: 
2024-12-09 21:48:57.330012: Epoch 133
2024-12-09 21:48:57.331820: Current learning rate: 0.00879
2024-12-09 21:50:26.703944: Validation loss did not improve from -0.54278. Patience: 4/50
2024-12-09 21:50:26.704797: train_loss -0.7387
2024-12-09 21:50:26.705504: val_loss -0.4923
2024-12-09 21:50:26.706290: Pseudo dice [0.7196]
2024-12-09 21:50:26.707056: Epoch time: 89.38 s
2024-12-09 21:50:27.932960: 
2024-12-09 21:50:27.934273: Epoch 134
2024-12-09 21:50:27.935074: Current learning rate: 0.00879
2024-12-09 21:51:57.052008: Validation loss did not improve from -0.54278. Patience: 5/50
2024-12-09 21:51:57.052844: train_loss -0.7389
2024-12-09 21:51:57.053606: val_loss -0.4959
2024-12-09 21:51:57.054153: Pseudo dice [0.7296]
2024-12-09 21:51:57.054860: Epoch time: 89.12 s
2024-12-09 21:51:58.649391: 
2024-12-09 21:51:58.651756: Epoch 135
2024-12-09 21:51:58.652706: Current learning rate: 0.00878
2024-12-09 21:53:27.597615: Validation loss did not improve from -0.54278. Patience: 6/50
2024-12-09 21:53:27.598530: train_loss -0.743
2024-12-09 21:53:27.599586: val_loss -0.5074
2024-12-09 21:53:27.600376: Pseudo dice [0.7348]
2024-12-09 21:53:27.601099: Epoch time: 88.95 s
2024-12-09 21:53:28.860970: 
2024-12-09 21:53:28.863158: Epoch 136
2024-12-09 21:53:28.864471: Current learning rate: 0.00877
2024-12-09 21:54:57.873011: Validation loss did not improve from -0.54278. Patience: 7/50
2024-12-09 21:54:57.873945: train_loss -0.7404
2024-12-09 21:54:57.875017: val_loss -0.5395
2024-12-09 21:54:57.876233: Pseudo dice [0.7497]
2024-12-09 21:54:57.877310: Epoch time: 89.01 s
2024-12-09 21:54:57.877944: Yayy! New best EMA pseudo Dice: 0.7315
2024-12-09 21:54:59.482426: 
2024-12-09 21:54:59.484210: Epoch 137
2024-12-09 21:54:59.485039: Current learning rate: 0.00876
2024-12-09 21:56:28.442375: Validation loss did not improve from -0.54278. Patience: 8/50
2024-12-09 21:56:28.443333: train_loss -0.7418
2024-12-09 21:56:28.444163: val_loss -0.5198
2024-12-09 21:56:28.445066: Pseudo dice [0.7406]
2024-12-09 21:56:28.445805: Epoch time: 88.96 s
2024-12-09 21:56:28.446633: Yayy! New best EMA pseudo Dice: 0.7324
2024-12-09 21:56:30.033875: 
2024-12-09 21:56:30.036021: Epoch 138
2024-12-09 21:56:30.037149: Current learning rate: 0.00875
2024-12-09 21:57:59.100268: Validation loss did not improve from -0.54278. Patience: 9/50
2024-12-09 21:57:59.101611: train_loss -0.7146
2024-12-09 21:57:59.102849: val_loss -0.4792
2024-12-09 21:57:59.103586: Pseudo dice [0.7074]
2024-12-09 21:57:59.104424: Epoch time: 89.07 s
2024-12-09 21:58:00.353184: 
2024-12-09 21:58:00.354455: Epoch 139
2024-12-09 21:58:00.355298: Current learning rate: 0.00874
2024-12-09 21:59:29.354537: Validation loss did not improve from -0.54278. Patience: 10/50
2024-12-09 21:59:29.356009: train_loss -0.7279
2024-12-09 21:59:29.356828: val_loss -0.5365
2024-12-09 21:59:29.357466: Pseudo dice [0.7437]
2024-12-09 21:59:29.358254: Epoch time: 89.0 s
2024-12-09 21:59:30.956772: 
2024-12-09 21:59:30.959045: Epoch 140
2024-12-09 21:59:30.959849: Current learning rate: 0.00873
2024-12-09 22:00:59.917312: Validation loss did not improve from -0.54278. Patience: 11/50
2024-12-09 22:00:59.918530: train_loss -0.7353
2024-12-09 22:00:59.919789: val_loss -0.5075
2024-12-09 22:00:59.920849: Pseudo dice [0.7286]
2024-12-09 22:00:59.921504: Epoch time: 88.96 s
2024-12-09 22:01:01.179897: 
2024-12-09 22:01:01.181828: Epoch 141
2024-12-09 22:01:01.182895: Current learning rate: 0.00872
2024-12-09 22:02:30.178649: Validation loss did not improve from -0.54278. Patience: 12/50
2024-12-09 22:02:30.179868: train_loss -0.7356
2024-12-09 22:02:30.181194: val_loss -0.5055
2024-12-09 22:02:30.181948: Pseudo dice [0.7376]
2024-12-09 22:02:30.182554: Epoch time: 89.0 s
2024-12-09 22:02:31.416147: 
2024-12-09 22:02:31.417678: Epoch 142
2024-12-09 22:02:31.418480: Current learning rate: 0.00871
2024-12-09 22:04:00.650007: Validation loss did not improve from -0.54278. Patience: 13/50
2024-12-09 22:04:00.650664: train_loss -0.7429
2024-12-09 22:04:00.651488: val_loss -0.4613
2024-12-09 22:04:00.652924: Pseudo dice [0.7099]
2024-12-09 22:04:00.653837: Epoch time: 89.24 s
2024-12-09 22:04:02.249435: 
2024-12-09 22:04:02.251745: Epoch 143
2024-12-09 22:04:02.252999: Current learning rate: 0.0087
2024-12-09 22:05:31.267153: Validation loss did not improve from -0.54278. Patience: 14/50
2024-12-09 22:05:31.268610: train_loss -0.7432
2024-12-09 22:05:31.269567: val_loss -0.5284
2024-12-09 22:05:31.270162: Pseudo dice [0.7457]
2024-12-09 22:05:31.270761: Epoch time: 89.02 s
2024-12-09 22:05:32.510157: 
2024-12-09 22:05:32.512169: Epoch 144
2024-12-09 22:05:32.512905: Current learning rate: 0.00869
2024-12-09 22:07:01.660919: Validation loss did not improve from -0.54278. Patience: 15/50
2024-12-09 22:07:01.662157: train_loss -0.7439
2024-12-09 22:07:01.663574: val_loss -0.4942
2024-12-09 22:07:01.664407: Pseudo dice [0.7155]
2024-12-09 22:07:01.665591: Epoch time: 89.15 s
2024-12-09 22:07:03.267526: 
2024-12-09 22:07:03.269320: Epoch 145
2024-12-09 22:07:03.270360: Current learning rate: 0.00868
2024-12-09 22:08:32.299313: Validation loss did not improve from -0.54278. Patience: 16/50
2024-12-09 22:08:32.300219: train_loss -0.7452
2024-12-09 22:08:32.301682: val_loss -0.4987
2024-12-09 22:08:32.302949: Pseudo dice [0.719]
2024-12-09 22:08:32.304046: Epoch time: 89.03 s
2024-12-09 22:08:33.547961: 
2024-12-09 22:08:33.550174: Epoch 146
2024-12-09 22:08:33.551276: Current learning rate: 0.00868
2024-12-09 22:10:02.616648: Validation loss did not improve from -0.54278. Patience: 17/50
2024-12-09 22:10:02.617599: train_loss -0.7291
2024-12-09 22:10:02.618328: val_loss -0.4809
2024-12-09 22:10:02.619022: Pseudo dice [0.7093]
2024-12-09 22:10:02.619704: Epoch time: 89.07 s
2024-12-09 22:10:03.854764: 
2024-12-09 22:10:03.856384: Epoch 147
2024-12-09 22:10:03.857534: Current learning rate: 0.00867
2024-12-09 22:11:32.970852: Validation loss did not improve from -0.54278. Patience: 18/50
2024-12-09 22:11:32.972132: train_loss -0.7147
2024-12-09 22:11:32.973087: val_loss -0.5093
2024-12-09 22:11:32.973761: Pseudo dice [0.7275]
2024-12-09 22:11:32.974508: Epoch time: 89.12 s
2024-12-09 22:11:34.222975: 
2024-12-09 22:11:34.224678: Epoch 148
2024-12-09 22:11:34.225976: Current learning rate: 0.00866
2024-12-09 22:13:03.281472: Validation loss did not improve from -0.54278. Patience: 19/50
2024-12-09 22:13:03.282279: train_loss -0.7285
2024-12-09 22:13:03.283092: val_loss -0.5209
2024-12-09 22:13:03.283899: Pseudo dice [0.724]
2024-12-09 22:13:03.284873: Epoch time: 89.06 s
2024-12-09 22:13:04.539091: 
2024-12-09 22:13:04.540966: Epoch 149
2024-12-09 22:13:04.542087: Current learning rate: 0.00865
2024-12-09 22:14:33.539766: Validation loss did not improve from -0.54278. Patience: 20/50
2024-12-09 22:14:33.540730: train_loss -0.7378
2024-12-09 22:14:33.542083: val_loss -0.4734
2024-12-09 22:14:33.543283: Pseudo dice [0.7171]
2024-12-09 22:14:33.544540: Epoch time: 89.0 s
2024-12-09 22:14:35.139484: 
2024-12-09 22:14:35.141221: Epoch 150
2024-12-09 22:14:35.142306: Current learning rate: 0.00864
2024-12-09 22:16:04.213082: Validation loss did not improve from -0.54278. Patience: 21/50
2024-12-09 22:16:04.214352: train_loss -0.7472
2024-12-09 22:16:04.215865: val_loss -0.5213
2024-12-09 22:16:04.216638: Pseudo dice [0.7387]
2024-12-09 22:16:04.217587: Epoch time: 89.08 s
2024-12-09 22:16:05.443113: 
2024-12-09 22:16:05.444868: Epoch 151
2024-12-09 22:16:05.445770: Current learning rate: 0.00863
2024-12-09 22:17:34.668289: Validation loss did not improve from -0.54278. Patience: 22/50
2024-12-09 22:17:34.669888: train_loss -0.7441
2024-12-09 22:17:34.670710: val_loss -0.4779
2024-12-09 22:17:34.671689: Pseudo dice [0.7108]
2024-12-09 22:17:34.672497: Epoch time: 89.23 s
2024-12-09 22:17:35.913732: 
2024-12-09 22:17:35.916092: Epoch 152
2024-12-09 22:17:35.916940: Current learning rate: 0.00862
2024-12-09 22:19:05.141011: Validation loss did not improve from -0.54278. Patience: 23/50
2024-12-09 22:19:05.141870: train_loss -0.7464
2024-12-09 22:19:05.142528: val_loss -0.4886
2024-12-09 22:19:05.143200: Pseudo dice [0.7293]
2024-12-09 22:19:05.143818: Epoch time: 89.23 s
2024-12-09 22:19:06.723416: 
2024-12-09 22:19:06.725010: Epoch 153
2024-12-09 22:19:06.726448: Current learning rate: 0.00861
2024-12-09 22:20:35.969083: Validation loss did not improve from -0.54278. Patience: 24/50
2024-12-09 22:20:35.970558: train_loss -0.7558
2024-12-09 22:20:35.971632: val_loss -0.4487
2024-12-09 22:20:35.972346: Pseudo dice [0.7234]
2024-12-09 22:20:35.973276: Epoch time: 89.25 s
2024-12-09 22:20:37.251415: 
2024-12-09 22:20:37.253269: Epoch 154
2024-12-09 22:20:37.254288: Current learning rate: 0.0086
2024-12-09 22:22:06.639493: Validation loss did not improve from -0.54278. Patience: 25/50
2024-12-09 22:22:06.641004: train_loss -0.7534
2024-12-09 22:22:06.642259: val_loss -0.5123
2024-12-09 22:22:06.643395: Pseudo dice [0.7392]
2024-12-09 22:22:06.644099: Epoch time: 89.39 s
2024-12-09 22:22:08.253775: 
2024-12-09 22:22:08.255592: Epoch 155
2024-12-09 22:22:08.257046: Current learning rate: 0.00859
2024-12-09 22:23:37.589143: Validation loss did not improve from -0.54278. Patience: 26/50
2024-12-09 22:23:37.590518: train_loss -0.7502
2024-12-09 22:23:37.591451: val_loss -0.495
2024-12-09 22:23:37.592164: Pseudo dice [0.7198]
2024-12-09 22:23:37.592807: Epoch time: 89.34 s
2024-12-09 22:23:38.857415: 
2024-12-09 22:23:38.859982: Epoch 156
2024-12-09 22:23:38.861117: Current learning rate: 0.00858
2024-12-09 22:25:08.333400: Validation loss did not improve from -0.54278. Patience: 27/50
2024-12-09 22:25:08.334389: train_loss -0.7475
2024-12-09 22:25:08.335560: val_loss -0.4862
2024-12-09 22:25:08.336426: Pseudo dice [0.7225]
2024-12-09 22:25:08.337140: Epoch time: 89.48 s
2024-12-09 22:25:09.593014: 
2024-12-09 22:25:09.594739: Epoch 157
2024-12-09 22:25:09.596120: Current learning rate: 0.00858
2024-12-09 22:26:39.024760: Validation loss did not improve from -0.54278. Patience: 28/50
2024-12-09 22:26:39.025551: train_loss -0.7505
2024-12-09 22:26:39.026282: val_loss -0.5033
2024-12-09 22:26:39.026944: Pseudo dice [0.7339]
2024-12-09 22:26:39.027655: Epoch time: 89.43 s
2024-12-09 22:26:40.284978: 
2024-12-09 22:26:40.287106: Epoch 158
2024-12-09 22:26:40.288021: Current learning rate: 0.00857
2024-12-09 22:28:09.680436: Validation loss improved from -0.54278 to -0.55843! Patience: 28/50
2024-12-09 22:28:09.681493: train_loss -0.7582
2024-12-09 22:28:09.682435: val_loss -0.5584
2024-12-09 22:28:09.683127: Pseudo dice [0.7531]
2024-12-09 22:28:09.683779: Epoch time: 89.4 s
2024-12-09 22:28:10.950079: 
2024-12-09 22:28:10.952178: Epoch 159
2024-12-09 22:28:10.953532: Current learning rate: 0.00856
2024-12-09 22:29:40.542933: Validation loss did not improve from -0.55843. Patience: 1/50
2024-12-09 22:29:40.543832: train_loss -0.7554
2024-12-09 22:29:40.544601: val_loss -0.5296
2024-12-09 22:29:40.545348: Pseudo dice [0.7444]
2024-12-09 22:29:40.545954: Epoch time: 89.59 s
2024-12-09 22:29:42.172274: 
2024-12-09 22:29:42.174558: Epoch 160
2024-12-09 22:29:42.175508: Current learning rate: 0.00855
2024-12-09 22:31:11.457778: Validation loss did not improve from -0.55843. Patience: 2/50
2024-12-09 22:31:11.459050: train_loss -0.7559
2024-12-09 22:31:11.460063: val_loss -0.5014
2024-12-09 22:31:11.461105: Pseudo dice [0.7274]
2024-12-09 22:31:11.461755: Epoch time: 89.29 s
2024-12-09 22:31:12.712361: 
2024-12-09 22:31:12.714282: Epoch 161
2024-12-09 22:31:12.715515: Current learning rate: 0.00854
2024-12-09 22:32:42.301057: Validation loss did not improve from -0.55843. Patience: 3/50
2024-12-09 22:32:42.302037: train_loss -0.7514
2024-12-09 22:32:42.302813: val_loss -0.502
2024-12-09 22:32:42.303421: Pseudo dice [0.7344]
2024-12-09 22:32:42.304238: Epoch time: 89.59 s
2024-12-09 22:32:43.568593: 
2024-12-09 22:32:43.570439: Epoch 162
2024-12-09 22:32:43.571084: Current learning rate: 0.00853
2024-12-09 22:34:13.092368: Validation loss did not improve from -0.55843. Patience: 4/50
2024-12-09 22:34:13.093350: train_loss -0.7557
2024-12-09 22:34:13.094225: val_loss -0.5255
2024-12-09 22:34:13.095316: Pseudo dice [0.7419]
2024-12-09 22:34:13.096387: Epoch time: 89.53 s
2024-12-09 22:34:14.355263: 
2024-12-09 22:34:14.357027: Epoch 163
2024-12-09 22:34:14.357735: Current learning rate: 0.00852
2024-12-09 22:35:44.047094: Validation loss did not improve from -0.55843. Patience: 5/50
2024-12-09 22:35:44.048363: train_loss -0.7565
2024-12-09 22:35:44.049550: val_loss -0.5315
2024-12-09 22:35:44.050596: Pseudo dice [0.7361]
2024-12-09 22:35:44.051527: Epoch time: 89.69 s
2024-12-09 22:35:45.328259: 
2024-12-09 22:35:45.330148: Epoch 164
2024-12-09 22:35:45.331019: Current learning rate: 0.00851
2024-12-09 22:37:14.738303: Validation loss did not improve from -0.55843. Patience: 6/50
2024-12-09 22:37:14.739108: train_loss -0.7547
2024-12-09 22:37:14.739805: val_loss -0.5201
2024-12-09 22:37:14.740487: Pseudo dice [0.7386]
2024-12-09 22:37:14.741055: Epoch time: 89.41 s
2024-12-09 22:37:15.098249: Yayy! New best EMA pseudo Dice: 0.7329
2024-12-09 22:37:16.675601: 
2024-12-09 22:37:16.677881: Epoch 165
2024-12-09 22:37:16.678756: Current learning rate: 0.0085
2024-12-09 22:38:46.101481: Validation loss did not improve from -0.55843. Patience: 7/50
2024-12-09 22:38:46.102389: train_loss -0.7549
2024-12-09 22:38:46.103384: val_loss -0.502
2024-12-09 22:38:46.104483: Pseudo dice [0.7215]
2024-12-09 22:38:46.105570: Epoch time: 89.43 s
2024-12-09 22:38:47.342724: 
2024-12-09 22:38:47.344325: Epoch 166
2024-12-09 22:38:47.345251: Current learning rate: 0.00849
2024-12-09 22:40:16.749741: Validation loss did not improve from -0.55843. Patience: 8/50
2024-12-09 22:40:16.751090: train_loss -0.7535
2024-12-09 22:40:16.751938: val_loss -0.4903
2024-12-09 22:40:16.752760: Pseudo dice [0.7193]
2024-12-09 22:40:16.753483: Epoch time: 89.41 s
2024-12-09 22:40:18.011925: 
2024-12-09 22:40:18.013918: Epoch 167
2024-12-09 22:40:18.015131: Current learning rate: 0.00848
2024-12-09 22:41:47.519941: Validation loss did not improve from -0.55843. Patience: 9/50
2024-12-09 22:41:47.520700: train_loss -0.7487
2024-12-09 22:41:47.521921: val_loss -0.503
2024-12-09 22:41:47.522854: Pseudo dice [0.7282]
2024-12-09 22:41:47.524160: Epoch time: 89.51 s
2024-12-09 22:41:48.775291: 
2024-12-09 22:41:48.777108: Epoch 168
2024-12-09 22:41:48.778210: Current learning rate: 0.00847
2024-12-09 22:43:18.299517: Validation loss did not improve from -0.55843. Patience: 10/50
2024-12-09 22:43:18.300706: train_loss -0.7526
2024-12-09 22:43:18.301939: val_loss -0.4967
2024-12-09 22:43:18.303028: Pseudo dice [0.7168]
2024-12-09 22:43:18.303962: Epoch time: 89.53 s
2024-12-09 22:43:19.584158: 
2024-12-09 22:43:19.585705: Epoch 169
2024-12-09 22:43:19.586955: Current learning rate: 0.00847
2024-12-09 22:44:49.071479: Validation loss did not improve from -0.55843. Patience: 11/50
2024-12-09 22:44:49.131778: train_loss -0.7574
2024-12-09 22:44:49.134340: val_loss -0.4662
2024-12-09 22:44:49.135263: Pseudo dice [0.7162]
2024-12-09 22:44:49.136911: Epoch time: 89.55 s
2024-12-09 22:44:50.833554: 
2024-12-09 22:44:50.835475: Epoch 170
2024-12-09 22:44:50.836812: Current learning rate: 0.00846
2024-12-09 22:46:20.076520: Validation loss did not improve from -0.55843. Patience: 12/50
2024-12-09 22:46:20.077502: train_loss -0.7597
2024-12-09 22:46:20.078181: val_loss -0.4976
2024-12-09 22:46:20.078818: Pseudo dice [0.7195]
2024-12-09 22:46:20.079538: Epoch time: 89.24 s
2024-12-09 22:46:21.339254: 
2024-12-09 22:46:21.340972: Epoch 171
2024-12-09 22:46:21.342050: Current learning rate: 0.00845
2024-12-09 22:47:50.736349: Validation loss did not improve from -0.55843. Patience: 13/50
2024-12-09 22:47:50.739715: train_loss -0.7549
2024-12-09 22:47:50.741092: val_loss -0.5033
2024-12-09 22:47:50.741896: Pseudo dice [0.7165]
2024-12-09 22:47:50.742809: Epoch time: 89.4 s
2024-12-09 22:47:52.013525: 
2024-12-09 22:47:52.015465: Epoch 172
2024-12-09 22:47:52.016773: Current learning rate: 0.00844
2024-12-09 22:49:21.464222: Validation loss did not improve from -0.55843. Patience: 14/50
2024-12-09 22:49:21.465066: train_loss -0.7573
2024-12-09 22:49:21.466138: val_loss -0.4905
2024-12-09 22:49:21.467205: Pseudo dice [0.7263]
2024-12-09 22:49:21.467995: Epoch time: 89.45 s
2024-12-09 22:49:22.756652: 
2024-12-09 22:49:22.758726: Epoch 173
2024-12-09 22:49:22.759711: Current learning rate: 0.00843
2024-12-09 22:50:52.209887: Validation loss did not improve from -0.55843. Patience: 15/50
2024-12-09 22:50:52.210784: train_loss -0.7523
2024-12-09 22:50:52.211668: val_loss -0.5187
2024-12-09 22:50:52.212514: Pseudo dice [0.7305]
2024-12-09 22:50:52.213638: Epoch time: 89.46 s
2024-12-09 22:50:53.989040: 
2024-12-09 22:50:53.990801: Epoch 174
2024-12-09 22:50:53.991736: Current learning rate: 0.00842
2024-12-09 22:52:23.453258: Validation loss did not improve from -0.55843. Patience: 16/50
2024-12-09 22:52:23.454242: train_loss -0.7517
2024-12-09 22:52:23.455103: val_loss -0.5088
2024-12-09 22:52:23.456218: Pseudo dice [0.7381]
2024-12-09 22:52:23.457196: Epoch time: 89.47 s
2024-12-09 22:52:25.084636: 
2024-12-09 22:52:25.086186: Epoch 175
2024-12-09 22:52:25.086836: Current learning rate: 0.00841
2024-12-09 22:53:54.583700: Validation loss did not improve from -0.55843. Patience: 17/50
2024-12-09 22:53:54.584833: train_loss -0.7456
2024-12-09 22:53:54.585701: val_loss -0.5238
2024-12-09 22:53:54.586710: Pseudo dice [0.7463]
2024-12-09 22:53:54.587534: Epoch time: 89.5 s
2024-12-09 22:53:55.864510: 
2024-12-09 22:53:55.866071: Epoch 176
2024-12-09 22:53:55.867351: Current learning rate: 0.0084
2024-12-09 22:55:25.659121: Validation loss did not improve from -0.55843. Patience: 18/50
2024-12-09 22:55:25.660089: train_loss -0.7538
2024-12-09 22:55:25.661268: val_loss -0.4928
2024-12-09 22:55:25.662250: Pseudo dice [0.729]
2024-12-09 22:55:25.663387: Epoch time: 89.8 s
2024-12-09 22:55:26.951895: 
2024-12-09 22:55:26.953374: Epoch 177
2024-12-09 22:55:26.954150: Current learning rate: 0.00839
2024-12-09 22:56:56.517308: Validation loss did not improve from -0.55843. Patience: 19/50
2024-12-09 22:56:56.518934: train_loss -0.7576
2024-12-09 22:56:56.520827: val_loss -0.5173
2024-12-09 22:56:56.521923: Pseudo dice [0.7405]
2024-12-09 22:56:56.522962: Epoch time: 89.57 s
2024-12-09 22:56:57.772815: 
2024-12-09 22:56:57.775201: Epoch 178
2024-12-09 22:56:57.776056: Current learning rate: 0.00838
2024-12-09 22:58:27.243058: Validation loss did not improve from -0.55843. Patience: 20/50
2024-12-09 22:58:27.244454: train_loss -0.759
2024-12-09 22:58:27.246064: val_loss -0.5
2024-12-09 22:58:27.247473: Pseudo dice [0.7313]
2024-12-09 22:58:27.248357: Epoch time: 89.47 s
2024-12-09 22:58:28.498057: 
2024-12-09 22:58:28.500176: Epoch 179
2024-12-09 22:58:28.501375: Current learning rate: 0.00837
2024-12-09 22:59:57.973194: Validation loss did not improve from -0.55843. Patience: 21/50
2024-12-09 22:59:57.974473: train_loss -0.7602
2024-12-09 22:59:57.975759: val_loss -0.5032
2024-12-09 22:59:57.976435: Pseudo dice [0.7204]
2024-12-09 22:59:57.977483: Epoch time: 89.48 s
2024-12-09 22:59:59.598899: 
2024-12-09 22:59:59.600296: Epoch 180
2024-12-09 22:59:59.601018: Current learning rate: 0.00836
2024-12-09 23:01:29.060021: Validation loss did not improve from -0.55843. Patience: 22/50
2024-12-09 23:01:29.060789: train_loss -0.748
2024-12-09 23:01:29.061726: val_loss -0.4936
2024-12-09 23:01:29.062809: Pseudo dice [0.725]
2024-12-09 23:01:29.063541: Epoch time: 89.46 s
2024-12-09 23:01:30.334337: 
2024-12-09 23:01:30.335747: Epoch 181
2024-12-09 23:01:30.337040: Current learning rate: 0.00836
2024-12-09 23:02:59.711928: Validation loss did not improve from -0.55843. Patience: 23/50
2024-12-09 23:02:59.713052: train_loss -0.7555
2024-12-09 23:02:59.714161: val_loss -0.5024
2024-12-09 23:02:59.715259: Pseudo dice [0.7195]
2024-12-09 23:02:59.716360: Epoch time: 89.38 s
2024-12-09 23:03:00.967414: 
2024-12-09 23:03:00.969203: Epoch 182
2024-12-09 23:03:00.970357: Current learning rate: 0.00835
2024-12-09 23:04:30.346296: Validation loss did not improve from -0.55843. Patience: 24/50
2024-12-09 23:04:30.347514: train_loss -0.7613
2024-12-09 23:04:30.348839: val_loss -0.5074
2024-12-09 23:04:30.349710: Pseudo dice [0.7354]
2024-12-09 23:04:30.350663: Epoch time: 89.38 s
2024-12-09 23:04:31.633956: 
2024-12-09 23:04:31.635962: Epoch 183
2024-12-09 23:04:31.637127: Current learning rate: 0.00834
2024-12-09 23:06:00.945211: Validation loss did not improve from -0.55843. Patience: 25/50
2024-12-09 23:06:00.946444: train_loss -0.7627
2024-12-09 23:06:00.947305: val_loss -0.4957
2024-12-09 23:06:00.947950: Pseudo dice [0.7255]
2024-12-09 23:06:00.948551: Epoch time: 89.31 s
2024-12-09 23:06:02.536338: 
2024-12-09 23:06:02.538875: Epoch 184
2024-12-09 23:06:02.539798: Current learning rate: 0.00833
2024-12-09 23:07:31.857630: Validation loss did not improve from -0.55843. Patience: 26/50
2024-12-09 23:07:31.858957: train_loss -0.7668
2024-12-09 23:07:31.859782: val_loss -0.5021
2024-12-09 23:07:31.860444: Pseudo dice [0.7262]
2024-12-09 23:07:31.861593: Epoch time: 89.32 s
2024-12-09 23:07:33.482250: 
2024-12-09 23:07:33.483555: Epoch 185
2024-12-09 23:07:33.484204: Current learning rate: 0.00832
2024-12-09 23:09:02.909277: Validation loss did not improve from -0.55843. Patience: 27/50
2024-12-09 23:09:02.910215: train_loss -0.7455
2024-12-09 23:09:02.911386: val_loss -0.4965
2024-12-09 23:09:02.912305: Pseudo dice [0.7306]
2024-12-09 23:09:02.913155: Epoch time: 89.43 s
2024-12-09 23:09:04.176745: 
2024-12-09 23:09:04.178598: Epoch 186
2024-12-09 23:09:04.179599: Current learning rate: 0.00831
2024-12-09 23:10:33.519404: Validation loss did not improve from -0.55843. Patience: 28/50
2024-12-09 23:10:33.520774: train_loss -0.751
2024-12-09 23:10:33.521525: val_loss -0.4776
2024-12-09 23:10:33.522643: Pseudo dice [0.7066]
2024-12-09 23:10:33.523506: Epoch time: 89.34 s
2024-12-09 23:10:34.775686: 
2024-12-09 23:10:34.777476: Epoch 187
2024-12-09 23:10:34.778325: Current learning rate: 0.0083
2024-12-09 23:12:04.083910: Validation loss did not improve from -0.55843. Patience: 29/50
2024-12-09 23:12:04.085287: train_loss -0.7559
2024-12-09 23:12:04.086565: val_loss -0.4876
2024-12-09 23:12:04.087541: Pseudo dice [0.7268]
2024-12-09 23:12:04.088630: Epoch time: 89.31 s
2024-12-09 23:12:05.356706: 
2024-12-09 23:12:05.358642: Epoch 188
2024-12-09 23:12:05.359772: Current learning rate: 0.00829
2024-12-09 23:13:34.839607: Validation loss did not improve from -0.55843. Patience: 30/50
2024-12-09 23:13:34.841020: train_loss -0.7512
2024-12-09 23:13:34.842647: val_loss -0.491
2024-12-09 23:13:34.843870: Pseudo dice [0.727]
2024-12-09 23:13:34.845004: Epoch time: 89.49 s
2024-12-09 23:13:36.108348: 
2024-12-09 23:13:36.110197: Epoch 189
2024-12-09 23:13:36.111239: Current learning rate: 0.00828
2024-12-09 23:15:05.591639: Validation loss did not improve from -0.55843. Patience: 31/50
2024-12-09 23:15:05.592898: train_loss -0.7547
2024-12-09 23:15:05.593759: val_loss -0.4983
2024-12-09 23:15:05.594517: Pseudo dice [0.7355]
2024-12-09 23:15:05.595492: Epoch time: 89.49 s
2024-12-09 23:15:07.190087: 
2024-12-09 23:15:07.191845: Epoch 190
2024-12-09 23:15:07.193170: Current learning rate: 0.00827
2024-12-09 23:16:36.613698: Validation loss did not improve from -0.55843. Patience: 32/50
2024-12-09 23:16:36.614600: train_loss -0.7578
2024-12-09 23:16:36.615350: val_loss -0.5004
2024-12-09 23:16:36.616235: Pseudo dice [0.7309]
2024-12-09 23:16:36.617220: Epoch time: 89.43 s
2024-12-09 23:16:37.853264: 
2024-12-09 23:16:37.854395: Epoch 191
2024-12-09 23:16:37.855099: Current learning rate: 0.00826
2024-12-09 23:18:07.333401: Validation loss did not improve from -0.55843. Patience: 33/50
2024-12-09 23:18:07.334761: train_loss -0.7649
2024-12-09 23:18:07.336101: val_loss -0.5009
2024-12-09 23:18:07.337487: Pseudo dice [0.7255]
2024-12-09 23:18:07.338545: Epoch time: 89.48 s
2024-12-09 23:18:08.620935: 
2024-12-09 23:18:08.623094: Epoch 192
2024-12-09 23:18:08.624205: Current learning rate: 0.00825
2024-12-09 23:19:37.694935: Validation loss did not improve from -0.55843. Patience: 34/50
2024-12-09 23:19:37.695776: train_loss -0.7646
2024-12-09 23:19:37.696574: val_loss -0.4925
2024-12-09 23:19:37.697143: Pseudo dice [0.7294]
2024-12-09 23:19:37.697853: Epoch time: 89.08 s
2024-12-09 23:19:38.956741: 
2024-12-09 23:19:38.958400: Epoch 193
2024-12-09 23:19:38.959151: Current learning rate: 0.00824
2024-12-09 23:21:08.015983: Validation loss did not improve from -0.55843. Patience: 35/50
2024-12-09 23:21:08.017128: train_loss -0.7646
2024-12-09 23:21:08.018162: val_loss -0.5173
2024-12-09 23:21:08.018994: Pseudo dice [0.7368]
2024-12-09 23:21:08.019765: Epoch time: 89.06 s
2024-12-09 23:21:09.288340: 
2024-12-09 23:21:09.290115: Epoch 194
2024-12-09 23:21:09.291314: Current learning rate: 0.00824
2024-12-09 23:22:38.320979: Validation loss did not improve from -0.55843. Patience: 36/50
2024-12-09 23:22:38.322031: train_loss -0.7635
2024-12-09 23:22:38.323197: val_loss -0.467
2024-12-09 23:22:38.324404: Pseudo dice [0.7083]
2024-12-09 23:22:38.325309: Epoch time: 89.03 s
2024-12-09 23:22:40.256305: 
2024-12-09 23:22:40.257829: Epoch 195
2024-12-09 23:22:40.258614: Current learning rate: 0.00823
2024-12-09 23:24:09.256967: Validation loss did not improve from -0.55843. Patience: 37/50
2024-12-09 23:24:09.258077: train_loss -0.7623
2024-12-09 23:24:09.259330: val_loss -0.55
2024-12-09 23:24:09.260169: Pseudo dice [0.7509]
2024-12-09 23:24:09.261020: Epoch time: 89.0 s
2024-12-09 23:24:10.553253: 
2024-12-09 23:24:10.554446: Epoch 196
2024-12-09 23:24:10.555133: Current learning rate: 0.00822
2024-12-09 23:25:39.563263: Validation loss did not improve from -0.55843. Patience: 38/50
2024-12-09 23:25:39.564564: train_loss -0.7671
2024-12-09 23:25:39.565832: val_loss -0.4868
2024-12-09 23:25:39.566813: Pseudo dice [0.7247]
2024-12-09 23:25:39.567458: Epoch time: 89.01 s
2024-12-09 23:25:40.841767: 
2024-12-09 23:25:40.843646: Epoch 197
2024-12-09 23:25:40.844751: Current learning rate: 0.00821
2024-12-09 23:27:09.856594: Validation loss did not improve from -0.55843. Patience: 39/50
2024-12-09 23:27:09.857516: train_loss -0.7626
2024-12-09 23:27:09.858462: val_loss -0.5196
2024-12-09 23:27:09.859543: Pseudo dice [0.7344]
2024-12-09 23:27:09.860214: Epoch time: 89.02 s
2024-12-09 23:27:11.154946: 
2024-12-09 23:27:11.157448: Epoch 198
2024-12-09 23:27:11.158517: Current learning rate: 0.0082
2024-12-09 23:28:40.123510: Validation loss did not improve from -0.55843. Patience: 40/50
2024-12-09 23:28:40.124436: train_loss -0.7597
2024-12-09 23:28:40.125831: val_loss -0.4889
2024-12-09 23:28:40.127142: Pseudo dice [0.7212]
2024-12-09 23:28:40.128396: Epoch time: 88.97 s
2024-12-09 23:28:41.400256: 
2024-12-09 23:28:41.401692: Epoch 199
2024-12-09 23:28:41.403148: Current learning rate: 0.00819
2024-12-09 23:30:10.445174: Validation loss did not improve from -0.55843. Patience: 41/50
2024-12-09 23:30:10.446669: train_loss -0.764
2024-12-09 23:30:10.447724: val_loss -0.4985
2024-12-09 23:30:10.448675: Pseudo dice [0.7235]
2024-12-09 23:30:10.449507: Epoch time: 89.05 s
2024-12-09 23:30:12.081946: 
2024-12-09 23:30:12.084131: Epoch 200
2024-12-09 23:30:12.085246: Current learning rate: 0.00818
2024-12-09 23:31:41.437809: Validation loss did not improve from -0.55843. Patience: 42/50
2024-12-09 23:31:41.439283: train_loss -0.7653
2024-12-09 23:31:41.440625: val_loss -0.5166
2024-12-09 23:31:41.441550: Pseudo dice [0.7312]
2024-12-09 23:31:41.442699: Epoch time: 89.36 s
2024-12-09 23:31:42.741792: 
2024-12-09 23:31:42.743420: Epoch 201
2024-12-09 23:31:42.744524: Current learning rate: 0.00817
2024-12-09 23:33:12.158628: Validation loss did not improve from -0.55843. Patience: 43/50
2024-12-09 23:33:12.159966: train_loss -0.7687
2024-12-09 23:33:12.161092: val_loss -0.5141
2024-12-09 23:33:12.162051: Pseudo dice [0.7295]
2024-12-09 23:33:12.162866: Epoch time: 89.42 s
2024-12-09 23:33:13.453243: 
2024-12-09 23:33:13.454542: Epoch 202
2024-12-09 23:33:13.455543: Current learning rate: 0.00816
2024-12-09 23:34:42.827985: Validation loss did not improve from -0.55843. Patience: 44/50
2024-12-09 23:34:42.829045: train_loss -0.7726
2024-12-09 23:34:42.829960: val_loss -0.5066
2024-12-09 23:34:42.831137: Pseudo dice [0.7375]
2024-12-09 23:34:42.832126: Epoch time: 89.38 s
2024-12-09 23:34:44.095602: 
2024-12-09 23:34:44.097097: Epoch 203
2024-12-09 23:34:44.097897: Current learning rate: 0.00815
2024-12-09 23:36:13.452788: Validation loss did not improve from -0.55843. Patience: 45/50
2024-12-09 23:36:13.454066: train_loss -0.7714
2024-12-09 23:36:13.455964: val_loss -0.5107
2024-12-09 23:36:13.456759: Pseudo dice [0.7377]
2024-12-09 23:36:13.457472: Epoch time: 89.36 s
2024-12-09 23:36:14.721174: 
2024-12-09 23:36:14.723327: Epoch 204
2024-12-09 23:36:14.724419: Current learning rate: 0.00814
2024-12-09 23:37:44.139594: Validation loss did not improve from -0.55843. Patience: 46/50
2024-12-09 23:37:44.140564: train_loss -0.7715
2024-12-09 23:37:44.141497: val_loss -0.4765
2024-12-09 23:37:44.142274: Pseudo dice [0.7224]
2024-12-09 23:37:44.143019: Epoch time: 89.42 s
2024-12-09 23:37:46.131953: 
2024-12-09 23:37:46.133478: Epoch 205
2024-12-09 23:37:46.134219: Current learning rate: 0.00813
2024-12-09 23:39:15.515208: Validation loss did not improve from -0.55843. Patience: 47/50
2024-12-09 23:39:15.516721: train_loss -0.7632
2024-12-09 23:39:15.518458: val_loss -0.5192
2024-12-09 23:39:15.519574: Pseudo dice [0.734]
2024-12-09 23:39:15.520673: Epoch time: 89.39 s
2024-12-09 23:39:16.731331: 
2024-12-09 23:39:16.733131: Epoch 206
2024-12-09 23:39:16.734092: Current learning rate: 0.00813
2024-12-09 23:40:46.082528: Validation loss did not improve from -0.55843. Patience: 48/50
2024-12-09 23:40:46.083393: train_loss -0.7628
2024-12-09 23:40:46.084591: val_loss -0.4908
2024-12-09 23:40:46.085541: Pseudo dice [0.7222]
2024-12-09 23:40:46.086251: Epoch time: 89.35 s
2024-12-09 23:40:47.281136: 
2024-12-09 23:40:47.282606: Epoch 207
2024-12-09 23:40:47.284008: Current learning rate: 0.00812
2024-12-09 23:42:16.701394: Validation loss did not improve from -0.55843. Patience: 49/50
2024-12-09 23:42:16.702510: train_loss -0.7631
2024-12-09 23:42:16.703626: val_loss -0.4842
2024-12-09 23:42:16.704397: Pseudo dice [0.7194]
2024-12-09 23:42:16.705148: Epoch time: 89.42 s
2024-12-09 23:42:17.894917: 
2024-12-09 23:42:17.896693: Epoch 208
2024-12-09 23:42:17.897553: Current learning rate: 0.00811
2024-12-09 23:43:47.423782: Validation loss did not improve from -0.55843. Patience: 50/50
2024-12-09 23:43:47.424514: train_loss -0.7569
2024-12-09 23:43:47.425465: val_loss -0.5101
2024-12-09 23:43:47.426073: Pseudo dice [0.7264]
2024-12-09 23:43:47.426877: Epoch time: 89.53 s
2024-12-09 23:43:48.636531: Patience reached. Stopping training.
2024-12-09 23:43:49.065141: Training done.
2024-12-09 23:43:49.273256: Using splits from existing split file: /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset307_Sohee_Calcium_OCT_CrossValidation/splits_final.json
2024-12-09 23:43:49.288991: The split file contains 5 splits.
2024-12-09 23:43:49.290714: Desired fold for training: 4
2024-12-09 23:43:49.291808: This split has 7 training and 1 validation cases.
2024-12-09 23:43:49.292936: predicting 101-045
2024-12-09 23:43:49.311913: 101-045, shape torch.Size([1, 375, 498, 498]), rank 0
2024-12-09 23:48:07.482509: Validation complete
2024-12-09 23:48:07.483775: Mean Validation Dice:  0.741221414786352
