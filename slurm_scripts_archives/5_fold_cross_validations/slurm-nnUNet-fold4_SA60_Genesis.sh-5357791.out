/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/bin/python
Starting training with CONFIG=3d_32x160x128_b10, DATASET_ID=307, TRAINER=nnUNetTrainerScaleAnalysis60
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/nnunetv2/training/nnUNetTrainer/nnUNetTrainer.py:169: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  self.grad_scaler = GradScaler() if self.device.type == 'cuda' else None

############################
INFO: You are using the old nnU-Net default plans. We have updated our recommendations. Please consider using those instead! Read more here: https://github.com/MIC-DKFZ/nnUNet/blob/master/documentation/resenc_presets.md
############################

Using device: cuda:0

#######################################################################
Please cite the following paper when using nnU-Net:
Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.
#######################################################################

2025-10-15 05:28:08.458036: Using torch.compile...
################### Loading pretrained weights from file  /nfs/erelab001/shared/Computational_Group/Naravich/datasets/nnUNet_Datasets/../ModelGenesisOutputs/ModelGenesisNNUNetPretrainingV2_noNorm_correct_orientation/Converted_nnUNet_Genesis_OCT_Best.pt ###################
Below is the list of overlapping blocks in pretrained model and nnUNet architecture:
encoder.stages.0.0.convs.0.conv.weight shape torch.Size([32, 1, 3, 3, 3])
encoder.stages.0.0.convs.0.conv.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.norm.weight shape torch.Size([32])
encoder.stages.0.0.convs.0.norm.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.0.weight shape torch.Size([32, 1, 3, 3, 3])
encoder.stages.0.0.convs.0.all_modules.0.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.1.weight shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.1.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
encoder.stages.0.0.convs.1.conv.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.norm.weight shape torch.Size([32])
encoder.stages.0.0.convs.1.norm.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
encoder.stages.0.0.convs.1.all_modules.0.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.1.weight shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.1.bias shape torch.Size([32])
encoder.stages.1.0.convs.0.conv.weight shape torch.Size([64, 32, 3, 3, 3])
encoder.stages.1.0.convs.0.conv.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.norm.weight shape torch.Size([64])
encoder.stages.1.0.convs.0.norm.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.0.weight shape torch.Size([64, 32, 3, 3, 3])
encoder.stages.1.0.convs.0.all_modules.0.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.1.weight shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.1.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
encoder.stages.1.0.convs.1.conv.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.norm.weight shape torch.Size([64])
encoder.stages.1.0.convs.1.norm.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
encoder.stages.1.0.convs.1.all_modules.0.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.1.weight shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.1.bias shape torch.Size([64])
encoder.stages.2.0.convs.0.conv.weight shape torch.Size([128, 64, 3, 3, 3])
encoder.stages.2.0.convs.0.conv.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.norm.weight shape torch.Size([128])
encoder.stages.2.0.convs.0.norm.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.0.weight shape torch.Size([128, 64, 3, 3, 3])
encoder.stages.2.0.convs.0.all_modules.0.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.1.weight shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.1.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
encoder.stages.2.0.convs.1.conv.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.norm.weight shape torch.Size([128])
encoder.stages.2.0.convs.1.norm.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
encoder.stages.2.0.convs.1.all_modules.0.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.1.weight shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.1.bias shape torch.Size([128])
encoder.stages.3.0.convs.0.conv.weight shape torch.Size([256, 128, 3, 3, 3])
encoder.stages.3.0.convs.0.conv.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.norm.weight shape torch.Size([256])
encoder.stages.3.0.convs.0.norm.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.0.weight shape torch.Size([256, 128, 3, 3, 3])
encoder.stages.3.0.convs.0.all_modules.0.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.1.weight shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.1.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
encoder.stages.3.0.convs.1.conv.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.norm.weight shape torch.Size([256])
encoder.stages.3.0.convs.1.norm.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
encoder.stages.3.0.convs.1.all_modules.0.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.1.weight shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.1.bias shape torch.Size([256])
encoder.stages.4.0.convs.0.conv.weight shape torch.Size([320, 256, 3, 3, 3])
encoder.stages.4.0.convs.0.conv.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.norm.weight shape torch.Size([320])
encoder.stages.4.0.convs.0.norm.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.0.weight shape torch.Size([320, 256, 3, 3, 3])
encoder.stages.4.0.convs.0.all_modules.0.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.1.weight shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.1.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.4.0.convs.1.conv.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.norm.weight shape torch.Size([320])
encoder.stages.4.0.convs.1.norm.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.4.0.convs.1.all_modules.0.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.1.weight shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.1.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.0.conv.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.norm.weight shape torch.Size([320])
encoder.stages.5.0.convs.0.norm.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.0.all_modules.0.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.1.weight shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.1.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.1.conv.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.norm.weight shape torch.Size([320])
encoder.stages.5.0.convs.1.norm.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.1.all_modules.0.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.1.weight shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.0.0.convs.0.conv.weight shape torch.Size([32, 1, 3, 3, 3])
decoder.encoder.stages.0.0.convs.0.conv.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.norm.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.norm.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.0.weight shape torch.Size([32, 1, 3, 3, 3])
decoder.encoder.stages.0.0.convs.0.all_modules.0.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.1.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.1.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.encoder.stages.0.0.convs.1.conv.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.norm.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.norm.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.encoder.stages.0.0.convs.1.all_modules.0.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.1.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.1.bias shape torch.Size([32])
decoder.encoder.stages.1.0.convs.0.conv.weight shape torch.Size([64, 32, 3, 3, 3])
decoder.encoder.stages.1.0.convs.0.conv.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.norm.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.norm.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.0.weight shape torch.Size([64, 32, 3, 3, 3])
decoder.encoder.stages.1.0.convs.0.all_modules.0.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.1.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.1.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.encoder.stages.1.0.convs.1.conv.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.norm.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.norm.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.encoder.stages.1.0.convs.1.all_modules.0.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.1.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.1.bias shape torch.Size([64])
decoder.encoder.stages.2.0.convs.0.conv.weight shape torch.Size([128, 64, 3, 3, 3])
decoder.encoder.stages.2.0.convs.0.conv.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.norm.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.norm.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.0.weight shape torch.Size([128, 64, 3, 3, 3])
decoder.encoder.stages.2.0.convs.0.all_modules.0.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.1.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.1.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.encoder.stages.2.0.convs.1.conv.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.norm.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.norm.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.encoder.stages.2.0.convs.1.all_modules.0.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.1.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.1.bias shape torch.Size([128])
decoder.encoder.stages.3.0.convs.0.conv.weight shape torch.Size([256, 128, 3, 3, 3])
decoder.encoder.stages.3.0.convs.0.conv.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.norm.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.norm.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.0.weight shape torch.Size([256, 128, 3, 3, 3])
decoder.encoder.stages.3.0.convs.0.all_modules.0.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.1.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.1.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.encoder.stages.3.0.convs.1.conv.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.norm.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.norm.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.encoder.stages.3.0.convs.1.all_modules.0.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.1.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.1.bias shape torch.Size([256])
decoder.encoder.stages.4.0.convs.0.conv.weight shape torch.Size([320, 256, 3, 3, 3])
decoder.encoder.stages.4.0.convs.0.conv.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.norm.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.norm.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.0.weight shape torch.Size([320, 256, 3, 3, 3])
decoder.encoder.stages.4.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.4.0.convs.1.conv.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.norm.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.norm.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.4.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.0.conv.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.norm.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.norm.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.1.conv.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.norm.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.norm.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.stages.0.convs.0.conv.weight shape torch.Size([320, 640, 3, 3, 3])
decoder.stages.0.convs.0.conv.bias shape torch.Size([320])
decoder.stages.0.convs.0.norm.weight shape torch.Size([320])
decoder.stages.0.convs.0.norm.bias shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.0.weight shape torch.Size([320, 640, 3, 3, 3])
decoder.stages.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.stages.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.stages.0.convs.1.conv.bias shape torch.Size([320])
decoder.stages.0.convs.1.norm.weight shape torch.Size([320])
decoder.stages.0.convs.1.norm.bias shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.stages.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.stages.1.convs.0.conv.weight shape torch.Size([256, 512, 3, 3, 3])
decoder.stages.1.convs.0.conv.bias shape torch.Size([256])
decoder.stages.1.convs.0.norm.weight shape torch.Size([256])
decoder.stages.1.convs.0.norm.bias shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.0.weight shape torch.Size([256, 512, 3, 3, 3])
decoder.stages.1.convs.0.all_modules.0.bias shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.1.weight shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.1.bias shape torch.Size([256])
decoder.stages.1.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.stages.1.convs.1.conv.bias shape torch.Size([256])
decoder.stages.1.convs.1.norm.weight shape torch.Size([256])
decoder.stages.1.convs.1.norm.bias shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.stages.1.convs.1.all_modules.0.bias shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.1.weight shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.1.bias shape torch.Size([256])
decoder.stages.2.convs.0.conv.weight shape torch.Size([128, 256, 3, 3, 3])
decoder.stages.2.convs.0.conv.bias shape torch.Size([128])
decoder.stages.2.convs.0.norm.weight shape torch.Size([128])
decoder.stages.2.convs.0.norm.bias shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.0.weight shape torch.Size([128, 256, 3, 3, 3])
decoder.stages.2.convs.0.all_modules.0.bias shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.1.weight shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.1.bias shape torch.Size([128])
decoder.stages.2.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.stages.2.convs.1.conv.bias shape torch.Size([128])
decoder.stages.2.convs.1.norm.weight shape torch.Size([128])
decoder.stages.2.convs.1.norm.bias shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.stages.2.convs.1.all_modules.0.bias shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.1.weight shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.1.bias shape torch.Size([128])
decoder.stages.3.convs.0.conv.weight shape torch.Size([64, 128, 3, 3, 3])
decoder.stages.3.convs.0.conv.bias shape torch.Size([64])
decoder.stages.3.convs.0.norm.weight shape torch.Size([64])
decoder.stages.3.convs.0.norm.bias shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.0.weight shape torch.Size([64, 128, 3, 3, 3])
decoder.stages.3.convs.0.all_modules.0.bias shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.1.weight shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.1.bias shape torch.Size([64])
decoder.stages.3.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.stages.3.convs.1.conv.bias shape torch.Size([64])
decoder.stages.3.convs.1.norm.weight shape torch.Size([64])
decoder.stages.3.convs.1.norm.bias shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.stages.3.convs.1.all_modules.0.bias shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.1.weight shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.1.bias shape torch.Size([64])
decoder.stages.4.convs.0.conv.weight shape torch.Size([32, 64, 3, 3, 3])
decoder.stages.4.convs.0.conv.bias shape torch.Size([32])
decoder.stages.4.convs.0.norm.weight shape torch.Size([32])
decoder.stages.4.convs.0.norm.bias shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.0.weight shape torch.Size([32, 64, 3, 3, 3])
decoder.stages.4.convs.0.all_modules.0.bias shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.1.weight shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.1.bias shape torch.Size([32])
decoder.stages.4.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.stages.4.convs.1.conv.bias shape torch.Size([32])
decoder.stages.4.convs.1.norm.weight shape torch.Size([32])
decoder.stages.4.convs.1.norm.bias shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.stages.4.convs.1.all_modules.0.bias shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.1.weight shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.1.bias shape torch.Size([32])
decoder.transpconvs.0.weight shape torch.Size([320, 320, 1, 2, 2])
decoder.transpconvs.0.bias shape torch.Size([320])
decoder.transpconvs.1.weight shape torch.Size([320, 256, 2, 2, 2])
decoder.transpconvs.1.bias shape torch.Size([256])
decoder.transpconvs.2.weight shape torch.Size([256, 128, 2, 2, 2])
decoder.transpconvs.2.bias shape torch.Size([128])
decoder.transpconvs.3.weight shape torch.Size([128, 64, 2, 2, 2])
decoder.transpconvs.3.bias shape torch.Size([64])
decoder.transpconvs.4.weight shape torch.Size([64, 32, 2, 2, 2])
decoder.transpconvs.4.bias shape torch.Size([32])
################### Done ###################
2025-10-15 05:28:16.293998: do_dummy_2d_data_aug: True
2025-10-15 05:28:16.294500: Using splits from existing split file: /nfs/erelab001/shared/Computational_Group/Naravich/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset307_Sohee_Calcium_OCT_CrossValidation/splits_final_60.json
2025-10-15 05:28:16.294885: The split file contains 5 splits.
2025-10-15 05:28:16.295032: Desired fold for training: 4
2025-10-15 05:28:16.295140: This split has 4 training and 4 validation cases.
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/fft_conv_pytorch/fft_conv.py:139: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:306.)
  output = output[crop_slices].contiguous()
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/fft_conv_pytorch/fft_conv.py:139: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:306.)
  output = output[crop_slices].contiguous()
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/fft_conv_pytorch/fft_conv.py:139: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:306.)
  output = output[crop_slices].contiguous()
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/fft_conv_pytorch/fft_conv.py:139: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:306.)
  output = output[crop_slices].contiguous()
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/fft_conv_pytorch/fft_conv.py:139: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:306.)
  output = output[crop_slices].contiguous()
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/fft_conv_pytorch/fft_conv.py:139: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:306.)
  output = output[crop_slices].contiguous()
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/fft_conv_pytorch/fft_conv.py:139: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:306.)
  output = output[crop_slices].contiguous()
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/fft_conv_pytorch/fft_conv.py:139: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:306.)
  output = output[crop_slices].contiguous()
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/fft_conv_pytorch/fft_conv.py:139: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:306.)
  output = output[crop_slices].contiguous()
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/fft_conv_pytorch/fft_conv.py:139: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:306.)
  output = output[crop_slices].contiguous()
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/fft_conv_pytorch/fft_conv.py:139: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:306.)
  output = output[crop_slices].contiguous()
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/fft_conv_pytorch/fft_conv.py:139: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:306.)
  output = output[crop_slices].contiguous()
using pin_memory on device 0
using pin_memory on device 0

This is the configuration used by this training:
Configuration name: 3d_32x160x128_b10
 {'data_identifier': 'nnUNetPlans_3d_fullres', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 10, 'patch_size': [32, 160, 128], 'median_image_size_in_voxels': [375.0, 498.0, 498.0], 'spacing': [1.0, 1.0, 1.0], 'normalization_schemes': ['ZScoreNormalization'], 'use_mask_for_norm': [False], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'architecture': {'network_class_name': 'dynamic_network_architectures.architectures.unet.PlainConvUNet', 'arch_kwargs': {'n_stages': 6, 'features_per_stage': [32, 64, 128, 256, 320, 320], 'conv_op': 'torch.nn.modules.conv.Conv3d', 'kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]], 'strides': [[1, 1, 1], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [1, 2, 2]], 'n_conv_per_stage': [2, 2, 2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2, 2, 2], 'conv_bias': True, 'norm_op': 'torch.nn.modules.instancenorm.InstanceNorm3d', 'norm_op_kwargs': {'eps': 1e-05, 'affine': True}, 'dropout_op': None, 'dropout_op_kwargs': None, 'nonlin': 'torch.nn.LeakyReLU', 'nonlin_kwargs': {'inplace': True}, 'deep_supervision': True}, '_kw_requires_import': ['conv_op', 'norm_op', 'dropout_op', 'nonlin']}, 'batch_dice': True, 'inherits_from': '3d_fullres'} 

These are the global plan.json settings:
 {'dataset_name': 'Dataset307_Sohee_Calcium_OCT_CrossValidation', 'plans_name': 'nnUNetPlans', 'original_median_spacing_after_transp': [1.0, 1.0, 1.0], 'original_median_shape_after_transp': [375, 498, 498], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 0.4977976381778717, 'mean': 0.13507525622844696, 'median': 0.09599608182907104, 'min': 0.0, 'percentile_00_5': 0.014754901640117168, 'percentile_99_5': 0.4977976381778717, 'std': 0.12025152146816254}}} 

2025-10-15 05:28:21.119888: unpacking dataset...
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/torch/_inductor/lowering.py:7095: UserWarning: 
Online softmax is disabled on the fly since Inductor decides to
split the reduction. Cut an issue to PyTorch if this is an
important use case and you want to speed it up with online
softmax.

  warnings.warn(
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/torch/_inductor/lowering.py:7095: UserWarning: 
Online softmax is disabled on the fly since Inductor decides to
split the reduction. Cut an issue to PyTorch if this is an
important use case and you want to speed it up with online
softmax.

  warnings.warn(
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/torch/_inductor/lowering.py:7095: UserWarning: 
Online softmax is disabled on the fly since Inductor decides to
split the reduction. Cut an issue to PyTorch if this is an
important use case and you want to speed it up with online
softmax.

  warnings.warn(
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/torch/_inductor/lowering.py:7095: UserWarning: 
Online softmax is disabled on the fly since Inductor decides to
split the reduction. Cut an issue to PyTorch if this is an
important use case and you want to speed it up with online
softmax.

  warnings.warn(
2025-10-15 05:28:26.001170: unpacking done...
2025-10-15 05:28:26.003490: Unable to plot network architecture: nnUNet_compile is enabled!
2025-10-15 05:28:26.109202: 
2025-10-15 05:28:26.109411: Epoch 0
2025-10-15 05:28:26.109632: Current learning rate: 0.01
2025-10-15 05:29:46.295897: Validation loss improved from 1000.00000 to -0.17826! Patience: 0/50
2025-10-15 05:29:46.297144: train_loss -0.1294
2025-10-15 05:29:46.297555: val_loss -0.1783
2025-10-15 05:29:46.297892: Pseudo dice [np.float32(0.5562)]
2025-10-15 05:29:46.298270: Epoch time: 80.19 s
2025-10-15 05:29:46.298747: Yayy! New best EMA pseudo Dice: 0.5562000274658203
2025-10-15 05:29:47.227222: 
2025-10-15 05:29:47.227530: Epoch 1
2025-10-15 05:29:47.227713: Current learning rate: 0.00994
2025-10-15 05:30:32.983899: Validation loss improved from -0.17826 to -0.30277! Patience: 0/50
2025-10-15 05:30:32.984381: train_loss -0.3131
2025-10-15 05:30:32.984554: val_loss -0.3028
2025-10-15 05:30:32.984748: Pseudo dice [np.float32(0.609)]
2025-10-15 05:30:32.984955: Epoch time: 45.76 s
2025-10-15 05:30:32.985169: Yayy! New best EMA pseudo Dice: 0.5615000128746033
2025-10-15 05:30:34.032940: 
2025-10-15 05:30:34.033287: Epoch 2
2025-10-15 05:30:34.033468: Current learning rate: 0.00988
2025-10-15 05:31:19.962583: Validation loss improved from -0.30277 to -0.33147! Patience: 0/50
2025-10-15 05:31:19.963100: train_loss -0.3668
2025-10-15 05:31:19.963257: val_loss -0.3315
2025-10-15 05:31:19.963393: Pseudo dice [np.float32(0.621)]
2025-10-15 05:31:19.963563: Epoch time: 45.93 s
2025-10-15 05:31:19.963692: Yayy! New best EMA pseudo Dice: 0.5674999952316284
2025-10-15 05:31:21.047000: 
2025-10-15 05:31:21.047284: Epoch 3
2025-10-15 05:31:21.047466: Current learning rate: 0.00982
2025-10-15 05:32:06.965980: Validation loss improved from -0.33147 to -0.37551! Patience: 0/50
2025-10-15 05:32:06.966922: train_loss -0.4146
2025-10-15 05:32:06.967059: val_loss -0.3755
2025-10-15 05:32:06.967247: Pseudo dice [np.float32(0.642)]
2025-10-15 05:32:06.967459: Epoch time: 45.92 s
2025-10-15 05:32:06.967641: Yayy! New best EMA pseudo Dice: 0.5748999714851379
2025-10-15 05:32:08.027383: 
2025-10-15 05:32:08.027689: Epoch 4
2025-10-15 05:32:08.027901: Current learning rate: 0.00976
2025-10-15 05:32:53.950755: Validation loss improved from -0.37551 to -0.37576! Patience: 0/50
2025-10-15 05:32:53.951459: train_loss -0.4475
2025-10-15 05:32:53.951610: val_loss -0.3758
2025-10-15 05:32:53.951803: Pseudo dice [np.float32(0.6408)]
2025-10-15 05:32:53.952050: Epoch time: 45.92 s
2025-10-15 05:32:54.354018: Yayy! New best EMA pseudo Dice: 0.5814999938011169
2025-10-15 05:32:55.418352: 
2025-10-15 05:32:55.418698: Epoch 5
2025-10-15 05:32:55.418965: Current learning rate: 0.0097
2025-10-15 05:33:41.403630: Validation loss improved from -0.37576 to -0.42215! Patience: 0/50
2025-10-15 05:33:41.404088: train_loss -0.455
2025-10-15 05:33:41.404317: val_loss -0.4221
2025-10-15 05:33:41.404539: Pseudo dice [np.float32(0.6668)]
2025-10-15 05:33:41.404780: Epoch time: 45.99 s
2025-10-15 05:33:41.404996: Yayy! New best EMA pseudo Dice: 0.5899999737739563
2025-10-15 05:33:42.469375: 
2025-10-15 05:33:42.469644: Epoch 6
2025-10-15 05:33:42.469843: Current learning rate: 0.00964
2025-10-15 05:34:28.488793: Validation loss did not improve from -0.42215. Patience: 1/50
2025-10-15 05:34:28.489430: train_loss -0.457
2025-10-15 05:34:28.489613: val_loss -0.3688
2025-10-15 05:34:28.489784: Pseudo dice [np.float32(0.6454)]
2025-10-15 05:34:28.489933: Epoch time: 46.02 s
2025-10-15 05:34:28.490085: Yayy! New best EMA pseudo Dice: 0.5956000089645386
2025-10-15 05:34:29.553271: 
2025-10-15 05:34:29.553602: Epoch 7
2025-10-15 05:34:29.553827: Current learning rate: 0.00958
2025-10-15 05:35:15.533931: Validation loss improved from -0.42215 to -0.44146! Patience: 1/50
2025-10-15 05:35:15.534536: train_loss -0.4953
2025-10-15 05:35:15.534752: val_loss -0.4415
2025-10-15 05:35:15.534932: Pseudo dice [np.float32(0.679)]
2025-10-15 05:35:15.535106: Epoch time: 45.98 s
2025-10-15 05:35:15.535345: Yayy! New best EMA pseudo Dice: 0.6039000153541565
2025-10-15 05:35:16.601779: 
2025-10-15 05:35:16.602077: Epoch 8
2025-10-15 05:35:16.602264: Current learning rate: 0.00952
2025-10-15 05:36:02.631660: Validation loss did not improve from -0.44146. Patience: 1/50
2025-10-15 05:36:02.632315: train_loss -0.4901
2025-10-15 05:36:02.632539: val_loss -0.4112
2025-10-15 05:36:02.632694: Pseudo dice [np.float32(0.6714)]
2025-10-15 05:36:02.632887: Epoch time: 46.03 s
2025-10-15 05:36:02.633051: Yayy! New best EMA pseudo Dice: 0.6107000112533569
2025-10-15 05:36:03.720048: 
2025-10-15 05:36:03.720319: Epoch 9
2025-10-15 05:36:03.720551: Current learning rate: 0.00946
2025-10-15 05:36:49.726427: Validation loss did not improve from -0.44146. Patience: 2/50
2025-10-15 05:36:49.726825: train_loss -0.5142
2025-10-15 05:36:49.727026: val_loss -0.4112
2025-10-15 05:36:49.727156: Pseudo dice [np.float32(0.6689)]
2025-10-15 05:36:49.727312: Epoch time: 46.01 s
2025-10-15 05:36:50.156003: Yayy! New best EMA pseudo Dice: 0.6165000200271606
2025-10-15 05:36:51.195954: 
2025-10-15 05:36:51.196197: Epoch 10
2025-10-15 05:36:51.196411: Current learning rate: 0.0094
2025-10-15 05:37:37.153099: Validation loss did not improve from -0.44146. Patience: 3/50
2025-10-15 05:37:37.153899: train_loss -0.5311
2025-10-15 05:37:37.154085: val_loss -0.4218
2025-10-15 05:37:37.154232: Pseudo dice [np.float32(0.6652)]
2025-10-15 05:37:37.154443: Epoch time: 45.96 s
2025-10-15 05:37:37.154649: Yayy! New best EMA pseudo Dice: 0.621399998664856
2025-10-15 05:37:38.222251: 
2025-10-15 05:37:38.222518: Epoch 11
2025-10-15 05:37:38.222693: Current learning rate: 0.00934
2025-10-15 05:38:24.204164: Validation loss improved from -0.44146 to -0.44435! Patience: 3/50
2025-10-15 05:38:24.204627: train_loss -0.5384
2025-10-15 05:38:24.204855: val_loss -0.4444
2025-10-15 05:38:24.204991: Pseudo dice [np.float32(0.6873)]
2025-10-15 05:38:24.205214: Epoch time: 45.98 s
2025-10-15 05:38:24.205354: Yayy! New best EMA pseudo Dice: 0.6279000043869019
2025-10-15 05:38:25.716185: 
2025-10-15 05:38:25.716424: Epoch 12
2025-10-15 05:38:25.716628: Current learning rate: 0.00928
2025-10-15 05:39:11.754617: Validation loss improved from -0.44435 to -0.45150! Patience: 0/50
2025-10-15 05:39:11.755244: train_loss -0.5452
2025-10-15 05:39:11.755426: val_loss -0.4515
2025-10-15 05:39:11.755556: Pseudo dice [np.float32(0.6787)]
2025-10-15 05:39:11.755770: Epoch time: 46.04 s
2025-10-15 05:39:11.755945: Yayy! New best EMA pseudo Dice: 0.6330000162124634
2025-10-15 05:39:12.829945: 
2025-10-15 05:39:12.830248: Epoch 13
2025-10-15 05:39:12.830517: Current learning rate: 0.00922
2025-10-15 05:39:58.898915: Validation loss did not improve from -0.45150. Patience: 1/50
2025-10-15 05:39:58.899348: train_loss -0.5422
2025-10-15 05:39:58.899520: val_loss -0.43
2025-10-15 05:39:58.899724: Pseudo dice [np.float32(0.6684)]
2025-10-15 05:39:58.899867: Epoch time: 46.07 s
2025-10-15 05:39:58.900038: Yayy! New best EMA pseudo Dice: 0.6366000175476074
2025-10-15 05:39:59.965245: 
2025-10-15 05:39:59.965613: Epoch 14
2025-10-15 05:39:59.965839: Current learning rate: 0.00916
2025-10-15 05:40:46.008710: Validation loss did not improve from -0.45150. Patience: 2/50
2025-10-15 05:40:46.009363: train_loss -0.5485
2025-10-15 05:40:46.009586: val_loss -0.4315
2025-10-15 05:40:46.009794: Pseudo dice [np.float32(0.6858)]
2025-10-15 05:40:46.009969: Epoch time: 46.04 s
2025-10-15 05:40:46.446702: Yayy! New best EMA pseudo Dice: 0.6414999961853027
2025-10-15 05:40:47.481530: 
2025-10-15 05:40:47.481868: Epoch 15
2025-10-15 05:40:47.482083: Current learning rate: 0.0091
2025-10-15 05:41:33.543023: Validation loss did not improve from -0.45150. Patience: 3/50
2025-10-15 05:41:33.543523: train_loss -0.5645
2025-10-15 05:41:33.543742: val_loss -0.4288
2025-10-15 05:41:33.543940: Pseudo dice [np.float32(0.6855)]
2025-10-15 05:41:33.544173: Epoch time: 46.06 s
2025-10-15 05:41:33.544339: Yayy! New best EMA pseudo Dice: 0.6459000110626221
2025-10-15 05:41:34.614550: 
2025-10-15 05:41:34.614825: Epoch 16
2025-10-15 05:41:34.615002: Current learning rate: 0.00903
2025-10-15 05:42:20.729321: Validation loss improved from -0.45150 to -0.49422! Patience: 3/50
2025-10-15 05:42:20.730196: train_loss -0.5764
2025-10-15 05:42:20.730459: val_loss -0.4942
2025-10-15 05:42:20.730661: Pseudo dice [np.float32(0.7166)]
2025-10-15 05:42:20.730857: Epoch time: 46.12 s
2025-10-15 05:42:20.731048: Yayy! New best EMA pseudo Dice: 0.652999997138977
2025-10-15 05:42:21.814091: 
2025-10-15 05:42:21.814458: Epoch 17
2025-10-15 05:42:21.814680: Current learning rate: 0.00897
2025-10-15 05:43:07.893465: Validation loss did not improve from -0.49422. Patience: 1/50
2025-10-15 05:43:07.893900: train_loss -0.5669
2025-10-15 05:43:07.894101: val_loss -0.4601
2025-10-15 05:43:07.894233: Pseudo dice [np.float32(0.6861)]
2025-10-15 05:43:07.894381: Epoch time: 46.08 s
2025-10-15 05:43:07.894513: Yayy! New best EMA pseudo Dice: 0.6563000082969666
2025-10-15 05:43:08.968080: 
2025-10-15 05:43:08.968590: Epoch 18
2025-10-15 05:43:08.968801: Current learning rate: 0.00891
2025-10-15 05:43:54.996688: Validation loss did not improve from -0.49422. Patience: 2/50
2025-10-15 05:43:54.997361: train_loss -0.5788
2025-10-15 05:43:54.997563: val_loss -0.4858
2025-10-15 05:43:54.997714: Pseudo dice [np.float32(0.7059)]
2025-10-15 05:43:54.997882: Epoch time: 46.03 s
2025-10-15 05:43:54.998003: Yayy! New best EMA pseudo Dice: 0.6611999869346619
2025-10-15 05:43:56.081203: 
2025-10-15 05:43:56.081536: Epoch 19
2025-10-15 05:43:56.081739: Current learning rate: 0.00885
2025-10-15 05:44:42.139948: Validation loss did not improve from -0.49422. Patience: 3/50
2025-10-15 05:44:42.140352: train_loss -0.5916
2025-10-15 05:44:42.140563: val_loss -0.4834
2025-10-15 05:44:42.140689: Pseudo dice [np.float32(0.7059)]
2025-10-15 05:44:42.140854: Epoch time: 46.06 s
2025-10-15 05:44:42.578581: Yayy! New best EMA pseudo Dice: 0.6657000184059143
2025-10-15 05:44:43.640960: 
2025-10-15 05:44:43.641235: Epoch 20
2025-10-15 05:44:43.641439: Current learning rate: 0.00879
2025-10-15 05:45:29.700982: Validation loss did not improve from -0.49422. Patience: 4/50
2025-10-15 05:45:29.701698: train_loss -0.5875
2025-10-15 05:45:29.701854: val_loss -0.4799
2025-10-15 05:45:29.702065: Pseudo dice [np.float32(0.7096)]
2025-10-15 05:45:29.702271: Epoch time: 46.06 s
2025-10-15 05:45:29.702427: Yayy! New best EMA pseudo Dice: 0.6700999736785889
2025-10-15 05:45:30.787927: 
2025-10-15 05:45:30.788676: Epoch 21
2025-10-15 05:45:30.788938: Current learning rate: 0.00873
2025-10-15 05:46:16.897938: Validation loss did not improve from -0.49422. Patience: 5/50
2025-10-15 05:46:16.898323: train_loss -0.5943
2025-10-15 05:46:16.898545: val_loss -0.4879
2025-10-15 05:46:16.898819: Pseudo dice [np.float32(0.7085)]
2025-10-15 05:46:16.899106: Epoch time: 46.11 s
2025-10-15 05:46:16.899246: Yayy! New best EMA pseudo Dice: 0.6739000082015991
2025-10-15 05:46:17.978994: 
2025-10-15 05:46:17.979307: Epoch 22
2025-10-15 05:46:17.979506: Current learning rate: 0.00867
2025-10-15 05:47:03.991897: Validation loss improved from -0.49422 to -0.50405! Patience: 5/50
2025-10-15 05:47:03.992611: train_loss -0.5942
2025-10-15 05:47:03.992773: val_loss -0.5041
2025-10-15 05:47:03.992918: Pseudo dice [np.float32(0.7196)]
2025-10-15 05:47:03.993064: Epoch time: 46.01 s
2025-10-15 05:47:03.993221: Yayy! New best EMA pseudo Dice: 0.6784999966621399
2025-10-15 05:47:05.053506: 
2025-10-15 05:47:05.053824: Epoch 23
2025-10-15 05:47:05.054025: Current learning rate: 0.00861
2025-10-15 05:47:51.028792: Validation loss improved from -0.50405 to -0.50517! Patience: 0/50
2025-10-15 05:47:51.029292: train_loss -0.5886
2025-10-15 05:47:51.029544: val_loss -0.5052
2025-10-15 05:47:51.029690: Pseudo dice [np.float32(0.7268)]
2025-10-15 05:47:51.029851: Epoch time: 45.98 s
2025-10-15 05:47:51.030008: Yayy! New best EMA pseudo Dice: 0.6833000183105469
2025-10-15 05:47:52.100110: 
2025-10-15 05:47:52.100457: Epoch 24
2025-10-15 05:47:52.100680: Current learning rate: 0.00855
2025-10-15 05:48:38.150642: Validation loss improved from -0.50517 to -0.52918! Patience: 0/50
2025-10-15 05:48:38.151828: train_loss -0.6034
2025-10-15 05:48:38.152236: val_loss -0.5292
2025-10-15 05:48:38.152583: Pseudo dice [np.float32(0.7353)]
2025-10-15 05:48:38.152989: Epoch time: 46.05 s
2025-10-15 05:48:38.599485: Yayy! New best EMA pseudo Dice: 0.6884999871253967
2025-10-15 05:48:39.676606: 
2025-10-15 05:48:39.676957: Epoch 25
2025-10-15 05:48:39.677216: Current learning rate: 0.00849
2025-10-15 05:49:25.685534: Validation loss did not improve from -0.52918. Patience: 1/50
2025-10-15 05:49:25.685991: train_loss -0.6047
2025-10-15 05:49:25.686143: val_loss -0.5108
2025-10-15 05:49:25.686277: Pseudo dice [np.float32(0.7251)]
2025-10-15 05:49:25.686460: Epoch time: 46.01 s
2025-10-15 05:49:25.686610: Yayy! New best EMA pseudo Dice: 0.6922000050544739
2025-10-15 05:49:26.760911: 
2025-10-15 05:49:26.761252: Epoch 26
2025-10-15 05:49:26.761449: Current learning rate: 0.00843
2025-10-15 05:50:12.782965: Validation loss did not improve from -0.52918. Patience: 2/50
2025-10-15 05:50:12.783612: train_loss -0.6157
2025-10-15 05:50:12.783758: val_loss -0.4922
2025-10-15 05:50:12.783894: Pseudo dice [np.float32(0.7112)]
2025-10-15 05:50:12.784028: Epoch time: 46.02 s
2025-10-15 05:50:12.784145: Yayy! New best EMA pseudo Dice: 0.694100022315979
2025-10-15 05:50:14.343606: 
2025-10-15 05:50:14.343946: Epoch 27
2025-10-15 05:50:14.344211: Current learning rate: 0.00836
2025-10-15 05:51:00.345811: Validation loss did not improve from -0.52918. Patience: 3/50
2025-10-15 05:51:00.346256: train_loss -0.6207
2025-10-15 05:51:00.346445: val_loss -0.4885
2025-10-15 05:51:00.346600: Pseudo dice [np.float32(0.7088)]
2025-10-15 05:51:00.346759: Epoch time: 46.0 s
2025-10-15 05:51:00.346896: Yayy! New best EMA pseudo Dice: 0.6955999732017517
2025-10-15 05:51:01.406620: 
2025-10-15 05:51:01.406937: Epoch 28
2025-10-15 05:51:01.407155: Current learning rate: 0.0083
2025-10-15 05:51:47.403632: Validation loss did not improve from -0.52918. Patience: 4/50
2025-10-15 05:51:47.404234: train_loss -0.6197
2025-10-15 05:51:47.404384: val_loss -0.5169
2025-10-15 05:51:47.404510: Pseudo dice [np.float32(0.7318)]
2025-10-15 05:51:47.404659: Epoch time: 46.0 s
2025-10-15 05:51:47.404775: Yayy! New best EMA pseudo Dice: 0.6991999745368958
2025-10-15 05:51:48.446315: 
2025-10-15 05:51:48.446676: Epoch 29
2025-10-15 05:51:48.446939: Current learning rate: 0.00824
2025-10-15 05:52:34.461198: Validation loss did not improve from -0.52918. Patience: 5/50
2025-10-15 05:52:34.461635: train_loss -0.6245
2025-10-15 05:52:34.461785: val_loss -0.477
2025-10-15 05:52:34.461938: Pseudo dice [np.float32(0.7079)]
2025-10-15 05:52:34.462091: Epoch time: 46.02 s
2025-10-15 05:52:34.896246: Yayy! New best EMA pseudo Dice: 0.699999988079071
2025-10-15 05:52:35.962008: 
2025-10-15 05:52:35.962298: Epoch 30
2025-10-15 05:52:35.962463: Current learning rate: 0.00818
2025-10-15 05:53:21.965974: Validation loss did not improve from -0.52918. Patience: 6/50
2025-10-15 05:53:21.967037: train_loss -0.6386
2025-10-15 05:53:21.967519: val_loss -0.5097
2025-10-15 05:53:21.967819: Pseudo dice [np.float32(0.7241)]
2025-10-15 05:53:21.968156: Epoch time: 46.01 s
2025-10-15 05:53:21.968378: Yayy! New best EMA pseudo Dice: 0.7024000287055969
2025-10-15 05:53:23.032166: 
2025-10-15 05:53:23.032552: Epoch 31
2025-10-15 05:53:23.032752: Current learning rate: 0.00812
2025-10-15 05:54:08.976446: Validation loss improved from -0.52918 to -0.53851! Patience: 6/50
2025-10-15 05:54:08.976855: train_loss -0.628
2025-10-15 05:54:08.977040: val_loss -0.5385
2025-10-15 05:54:08.977185: Pseudo dice [np.float32(0.7367)]
2025-10-15 05:54:08.977328: Epoch time: 45.95 s
2025-10-15 05:54:08.977458: Yayy! New best EMA pseudo Dice: 0.7059000134468079
2025-10-15 05:54:10.047919: 
2025-10-15 05:54:10.048206: Epoch 32
2025-10-15 05:54:10.048426: Current learning rate: 0.00806
2025-10-15 05:54:56.052433: Validation loss improved from -0.53851 to -0.53958! Patience: 0/50
2025-10-15 05:54:56.053126: train_loss -0.6414
2025-10-15 05:54:56.053339: val_loss -0.5396
2025-10-15 05:54:56.053537: Pseudo dice [np.float32(0.7458)]
2025-10-15 05:54:56.053733: Epoch time: 46.01 s
2025-10-15 05:54:56.054094: Yayy! New best EMA pseudo Dice: 0.7099000215530396
2025-10-15 05:54:57.154302: 
2025-10-15 05:54:57.154583: Epoch 33
2025-10-15 05:54:57.154797: Current learning rate: 0.008
2025-10-15 05:55:43.195140: Validation loss did not improve from -0.53958. Patience: 1/50
2025-10-15 05:55:43.195642: train_loss -0.6472
2025-10-15 05:55:43.195858: val_loss -0.5374
2025-10-15 05:55:43.196015: Pseudo dice [np.float32(0.7368)]
2025-10-15 05:55:43.196153: Epoch time: 46.04 s
2025-10-15 05:55:43.196271: Yayy! New best EMA pseudo Dice: 0.7125999927520752
2025-10-15 05:55:44.271996: 
2025-10-15 05:55:44.272258: Epoch 34
2025-10-15 05:55:44.272457: Current learning rate: 0.00793
2025-10-15 05:56:30.304517: Validation loss did not improve from -0.53958. Patience: 2/50
2025-10-15 05:56:30.305401: train_loss -0.6529
2025-10-15 05:56:30.305652: val_loss -0.4966
2025-10-15 05:56:30.305832: Pseudo dice [np.float32(0.7196)]
2025-10-15 05:56:30.306054: Epoch time: 46.03 s
2025-10-15 05:56:30.753411: Yayy! New best EMA pseudo Dice: 0.7132999897003174
2025-10-15 05:56:31.817441: 
2025-10-15 05:56:31.817780: Epoch 35
2025-10-15 05:56:31.817997: Current learning rate: 0.00787
2025-10-15 05:57:17.826832: Validation loss did not improve from -0.53958. Patience: 3/50
2025-10-15 05:57:17.827255: train_loss -0.6505
2025-10-15 05:57:17.827446: val_loss -0.5165
2025-10-15 05:57:17.827625: Pseudo dice [np.float32(0.7282)]
2025-10-15 05:57:17.827819: Epoch time: 46.01 s
2025-10-15 05:57:17.827953: Yayy! New best EMA pseudo Dice: 0.7148000001907349
2025-10-15 05:57:18.910008: 
2025-10-15 05:57:18.910395: Epoch 36
2025-10-15 05:57:18.910619: Current learning rate: 0.00781
2025-10-15 05:58:04.937827: Validation loss did not improve from -0.53958. Patience: 4/50
2025-10-15 05:58:04.938528: train_loss -0.6542
2025-10-15 05:58:04.938692: val_loss -0.5189
2025-10-15 05:58:04.938891: Pseudo dice [np.float32(0.7187)]
2025-10-15 05:58:04.939042: Epoch time: 46.03 s
2025-10-15 05:58:04.939187: Yayy! New best EMA pseudo Dice: 0.7152000069618225
2025-10-15 05:58:06.029811: 
2025-10-15 05:58:06.030087: Epoch 37
2025-10-15 05:58:06.030268: Current learning rate: 0.00775
2025-10-15 05:58:52.106548: Validation loss did not improve from -0.53958. Patience: 5/50
2025-10-15 05:58:52.107038: train_loss -0.6506
2025-10-15 05:58:52.107229: val_loss -0.5094
2025-10-15 05:58:52.107409: Pseudo dice [np.float32(0.7203)]
2025-10-15 05:58:52.107619: Epoch time: 46.08 s
2025-10-15 05:58:52.107793: Yayy! New best EMA pseudo Dice: 0.7156999707221985
2025-10-15 05:58:53.189805: 
2025-10-15 05:58:53.190133: Epoch 38
2025-10-15 05:58:53.190424: Current learning rate: 0.00769
2025-10-15 05:59:39.254688: Validation loss did not improve from -0.53958. Patience: 6/50
2025-10-15 05:59:39.255285: train_loss -0.6612
2025-10-15 05:59:39.255465: val_loss -0.5044
2025-10-15 05:59:39.255605: Pseudo dice [np.float32(0.7271)]
2025-10-15 05:59:39.255800: Epoch time: 46.07 s
2025-10-15 05:59:39.256049: Yayy! New best EMA pseudo Dice: 0.7167999744415283
2025-10-15 05:59:40.340648: 
2025-10-15 05:59:40.340890: Epoch 39
2025-10-15 05:59:40.341099: Current learning rate: 0.00763
2025-10-15 06:00:26.422019: Validation loss did not improve from -0.53958. Patience: 7/50
2025-10-15 06:00:26.422441: train_loss -0.6555
2025-10-15 06:00:26.422651: val_loss -0.4999
2025-10-15 06:00:26.422823: Pseudo dice [np.float32(0.7251)]
2025-10-15 06:00:26.422967: Epoch time: 46.08 s
2025-10-15 06:00:26.863487: Yayy! New best EMA pseudo Dice: 0.7175999879837036
2025-10-15 06:00:27.939000: 
2025-10-15 06:00:27.939342: Epoch 40
2025-10-15 06:00:27.939571: Current learning rate: 0.00756
2025-10-15 06:01:13.994104: Validation loss did not improve from -0.53958. Patience: 8/50
2025-10-15 06:01:13.994817: train_loss -0.6661
2025-10-15 06:01:13.995007: val_loss -0.5343
2025-10-15 06:01:13.995162: Pseudo dice [np.float32(0.7368)]
2025-10-15 06:01:13.995314: Epoch time: 46.06 s
2025-10-15 06:01:13.995446: Yayy! New best EMA pseudo Dice: 0.7196000218391418
2025-10-15 06:01:15.089716: 
2025-10-15 06:01:15.090076: Epoch 41
2025-10-15 06:01:15.090296: Current learning rate: 0.0075
2025-10-15 06:02:01.082592: Validation loss did not improve from -0.53958. Patience: 9/50
2025-10-15 06:02:01.082988: train_loss -0.6664
2025-10-15 06:02:01.083153: val_loss -0.4889
2025-10-15 06:02:01.083344: Pseudo dice [np.float32(0.711)]
2025-10-15 06:02:01.083532: Epoch time: 45.99 s
2025-10-15 06:02:01.706072: 
2025-10-15 06:02:01.706386: Epoch 42
2025-10-15 06:02:01.706584: Current learning rate: 0.00744
2025-10-15 06:02:47.678030: Validation loss did not improve from -0.53958. Patience: 10/50
2025-10-15 06:02:47.678629: train_loss -0.6641
2025-10-15 06:02:47.678850: val_loss -0.4987
2025-10-15 06:02:47.679091: Pseudo dice [np.float32(0.7217)]
2025-10-15 06:02:47.679425: Epoch time: 45.97 s
2025-10-15 06:02:48.799921: 
2025-10-15 06:02:48.800282: Epoch 43
2025-10-15 06:02:48.800474: Current learning rate: 0.00738
2025-10-15 06:03:34.795454: Validation loss did not improve from -0.53958. Patience: 11/50
2025-10-15 06:03:34.795904: train_loss -0.6574
2025-10-15 06:03:34.796133: val_loss -0.5246
2025-10-15 06:03:34.796259: Pseudo dice [np.float32(0.7306)]
2025-10-15 06:03:34.796423: Epoch time: 46.0 s
2025-10-15 06:03:34.796552: Yayy! New best EMA pseudo Dice: 0.7202000021934509
2025-10-15 06:03:35.862798: 
2025-10-15 06:03:35.863118: Epoch 44
2025-10-15 06:03:35.863367: Current learning rate: 0.00732
2025-10-15 06:04:21.857937: Validation loss did not improve from -0.53958. Patience: 12/50
2025-10-15 06:04:21.858687: train_loss -0.6711
2025-10-15 06:04:21.858899: val_loss -0.5321
2025-10-15 06:04:21.859068: Pseudo dice [np.float32(0.74)]
2025-10-15 06:04:21.859244: Epoch time: 46.0 s
2025-10-15 06:04:22.321639: Yayy! New best EMA pseudo Dice: 0.722100019454956
2025-10-15 06:04:23.366407: 
2025-10-15 06:04:23.366688: Epoch 45
2025-10-15 06:04:23.366878: Current learning rate: 0.00725
2025-10-15 06:05:09.431854: Validation loss did not improve from -0.53958. Patience: 13/50
2025-10-15 06:05:09.432277: train_loss -0.6765
2025-10-15 06:05:09.432440: val_loss -0.4953
2025-10-15 06:05:09.432610: Pseudo dice [np.float32(0.715)]
2025-10-15 06:05:09.432987: Epoch time: 46.07 s
2025-10-15 06:05:10.053774: 
2025-10-15 06:05:10.054139: Epoch 46
2025-10-15 06:05:10.054317: Current learning rate: 0.00719
2025-10-15 06:05:56.051515: Validation loss did not improve from -0.53958. Patience: 14/50
2025-10-15 06:05:56.052191: train_loss -0.6811
2025-10-15 06:05:56.052557: val_loss -0.5147
2025-10-15 06:05:56.052858: Pseudo dice [np.float32(0.7331)]
2025-10-15 06:05:56.053146: Epoch time: 46.0 s
2025-10-15 06:05:56.053437: Yayy! New best EMA pseudo Dice: 0.722599983215332
2025-10-15 06:05:57.144742: 
2025-10-15 06:05:57.145089: Epoch 47
2025-10-15 06:05:57.145346: Current learning rate: 0.00713
2025-10-15 06:06:43.168848: Validation loss did not improve from -0.53958. Patience: 15/50
2025-10-15 06:06:43.169332: train_loss -0.6852
2025-10-15 06:06:43.169500: val_loss -0.5226
2025-10-15 06:06:43.169680: Pseudo dice [np.float32(0.7305)]
2025-10-15 06:06:43.169889: Epoch time: 46.03 s
2025-10-15 06:06:43.170041: Yayy! New best EMA pseudo Dice: 0.7233999967575073
2025-10-15 06:06:44.238134: 
2025-10-15 06:06:44.238550: Epoch 48
2025-10-15 06:06:44.238859: Current learning rate: 0.00707
2025-10-15 06:07:30.269446: Validation loss did not improve from -0.53958. Patience: 16/50
2025-10-15 06:07:30.270179: train_loss -0.6669
2025-10-15 06:07:30.270329: val_loss -0.5012
2025-10-15 06:07:30.270470: Pseudo dice [np.float32(0.7157)]
2025-10-15 06:07:30.270634: Epoch time: 46.03 s
2025-10-15 06:07:30.903132: 
2025-10-15 06:07:30.903665: Epoch 49
2025-10-15 06:07:30.904082: Current learning rate: 0.007
2025-10-15 06:08:16.936293: Validation loss improved from -0.53958 to -0.54357! Patience: 16/50
2025-10-15 06:08:16.936774: train_loss -0.6823
2025-10-15 06:08:16.936936: val_loss -0.5436
2025-10-15 06:08:16.937090: Pseudo dice [np.float32(0.7376)]
2025-10-15 06:08:16.937253: Epoch time: 46.03 s
2025-10-15 06:08:17.388573: Yayy! New best EMA pseudo Dice: 0.7240999937057495
2025-10-15 06:08:18.462420: 
2025-10-15 06:08:18.462785: Epoch 50
2025-10-15 06:08:18.463027: Current learning rate: 0.00694
2025-10-15 06:09:04.524392: Validation loss did not improve from -0.54357. Patience: 1/50
2025-10-15 06:09:04.525057: train_loss -0.6905
2025-10-15 06:09:04.525227: val_loss -0.5166
2025-10-15 06:09:04.525377: Pseudo dice [np.float32(0.7279)]
2025-10-15 06:09:04.525521: Epoch time: 46.06 s
2025-10-15 06:09:04.525696: Yayy! New best EMA pseudo Dice: 0.7245000004768372
2025-10-15 06:09:05.612050: 
2025-10-15 06:09:05.612544: Epoch 51
2025-10-15 06:09:05.612818: Current learning rate: 0.00688
2025-10-15 06:09:51.671464: Validation loss did not improve from -0.54357. Patience: 2/50
2025-10-15 06:09:51.672069: train_loss -0.6853
2025-10-15 06:09:51.672332: val_loss -0.5252
2025-10-15 06:09:51.672529: Pseudo dice [np.float32(0.7317)]
2025-10-15 06:09:51.672742: Epoch time: 46.06 s
2025-10-15 06:09:51.672919: Yayy! New best EMA pseudo Dice: 0.7251999974250793
2025-10-15 06:09:52.772403: 
2025-10-15 06:09:52.772840: Epoch 52
2025-10-15 06:09:52.773272: Current learning rate: 0.00682
2025-10-15 06:10:38.858563: Validation loss did not improve from -0.54357. Patience: 3/50
2025-10-15 06:10:38.859193: train_loss -0.6874
2025-10-15 06:10:38.859368: val_loss -0.5309
2025-10-15 06:10:38.859515: Pseudo dice [np.float32(0.7458)]
2025-10-15 06:10:38.859693: Epoch time: 46.09 s
2025-10-15 06:10:38.859814: Yayy! New best EMA pseudo Dice: 0.7272999882698059
2025-10-15 06:10:39.945963: 
2025-10-15 06:10:39.946280: Epoch 53
2025-10-15 06:10:39.946490: Current learning rate: 0.00675
2025-10-15 06:11:25.942748: Validation loss improved from -0.54357 to -0.54715! Patience: 3/50
2025-10-15 06:11:25.943177: train_loss -0.6756
2025-10-15 06:11:25.943331: val_loss -0.5471
2025-10-15 06:11:25.943470: Pseudo dice [np.float32(0.7413)]
2025-10-15 06:11:25.943668: Epoch time: 46.0 s
2025-10-15 06:11:25.943822: Yayy! New best EMA pseudo Dice: 0.7286999821662903
2025-10-15 06:11:27.030141: 
2025-10-15 06:11:27.030500: Epoch 54
2025-10-15 06:11:27.030799: Current learning rate: 0.00669
2025-10-15 06:12:13.040755: Validation loss did not improve from -0.54715. Patience: 1/50
2025-10-15 06:12:13.041361: train_loss -0.6932
2025-10-15 06:12:13.041566: val_loss -0.5154
2025-10-15 06:12:13.041793: Pseudo dice [np.float32(0.7303)]
2025-10-15 06:12:13.041978: Epoch time: 46.01 s
2025-10-15 06:12:13.487251: Yayy! New best EMA pseudo Dice: 0.7287999987602234
2025-10-15 06:12:14.552079: 
2025-10-15 06:12:14.552451: Epoch 55
2025-10-15 06:12:14.552729: Current learning rate: 0.00663
2025-10-15 06:13:00.600114: Validation loss did not improve from -0.54715. Patience: 2/50
2025-10-15 06:13:00.600555: train_loss -0.6896
2025-10-15 06:13:00.600737: val_loss -0.5329
2025-10-15 06:13:00.600905: Pseudo dice [np.float32(0.7325)]
2025-10-15 06:13:00.601060: Epoch time: 46.05 s
2025-10-15 06:13:00.601219: Yayy! New best EMA pseudo Dice: 0.729200005531311
2025-10-15 06:13:01.677528: 
2025-10-15 06:13:01.677795: Epoch 56
2025-10-15 06:13:01.678044: Current learning rate: 0.00657
2025-10-15 06:13:47.748286: Validation loss improved from -0.54715 to -0.54875! Patience: 2/50
2025-10-15 06:13:47.748963: train_loss -0.7033
2025-10-15 06:13:47.749138: val_loss -0.5488
2025-10-15 06:13:47.749347: Pseudo dice [np.float32(0.7411)]
2025-10-15 06:13:47.749514: Epoch time: 46.07 s
2025-10-15 06:13:47.749661: Yayy! New best EMA pseudo Dice: 0.730400025844574
2025-10-15 06:13:48.833889: 
2025-10-15 06:13:48.834193: Epoch 57
2025-10-15 06:13:48.834378: Current learning rate: 0.0065
2025-10-15 06:14:34.895281: Validation loss did not improve from -0.54875. Patience: 1/50
2025-10-15 06:14:34.895756: train_loss -0.7006
2025-10-15 06:14:34.895957: val_loss -0.546
2025-10-15 06:14:34.896116: Pseudo dice [np.float32(0.7469)]
2025-10-15 06:14:34.896263: Epoch time: 46.06 s
2025-10-15 06:14:34.896395: Yayy! New best EMA pseudo Dice: 0.7319999933242798
2025-10-15 06:14:36.429687: 
2025-10-15 06:14:36.429954: Epoch 58
2025-10-15 06:14:36.430143: Current learning rate: 0.00644
2025-10-15 06:15:22.479666: Validation loss improved from -0.54875 to -0.56925! Patience: 1/50
2025-10-15 06:15:22.480510: train_loss -0.7067
2025-10-15 06:15:22.480897: val_loss -0.5692
2025-10-15 06:15:22.481137: Pseudo dice [np.float32(0.7612)]
2025-10-15 06:15:22.481349: Epoch time: 46.05 s
2025-10-15 06:15:22.481508: Yayy! New best EMA pseudo Dice: 0.7350000143051147
2025-10-15 06:15:23.586020: 
2025-10-15 06:15:23.586365: Epoch 59
2025-10-15 06:15:23.586570: Current learning rate: 0.00638
2025-10-15 06:16:09.678814: Validation loss did not improve from -0.56925. Patience: 1/50
2025-10-15 06:16:09.679249: train_loss -0.7086
2025-10-15 06:16:09.679541: val_loss -0.5396
2025-10-15 06:16:09.679722: Pseudo dice [np.float32(0.7434)]
2025-10-15 06:16:09.679871: Epoch time: 46.09 s
2025-10-15 06:16:10.122080: Yayy! New best EMA pseudo Dice: 0.73580002784729
2025-10-15 06:16:11.178801: 
2025-10-15 06:16:11.179362: Epoch 60
2025-10-15 06:16:11.179784: Current learning rate: 0.00631
2025-10-15 06:16:57.313388: Validation loss did not improve from -0.56925. Patience: 2/50
2025-10-15 06:16:57.313973: train_loss -0.7047
2025-10-15 06:16:57.314117: val_loss -0.5478
2025-10-15 06:16:57.314261: Pseudo dice [np.float32(0.7526)]
2025-10-15 06:16:57.314527: Epoch time: 46.14 s
2025-10-15 06:16:57.314701: Yayy! New best EMA pseudo Dice: 0.737500011920929
2025-10-15 06:16:58.391110: 
2025-10-15 06:16:58.391423: Epoch 61
2025-10-15 06:16:58.391677: Current learning rate: 0.00625
2025-10-15 06:17:44.530568: Validation loss did not improve from -0.56925. Patience: 3/50
2025-10-15 06:17:44.530987: train_loss -0.7138
2025-10-15 06:17:44.531168: val_loss -0.5553
2025-10-15 06:17:44.531296: Pseudo dice [np.float32(0.7544)]
2025-10-15 06:17:44.531436: Epoch time: 46.14 s
2025-10-15 06:17:44.531590: Yayy! New best EMA pseudo Dice: 0.7391999959945679
2025-10-15 06:17:45.623613: 
2025-10-15 06:17:45.623878: Epoch 62
2025-10-15 06:17:45.624096: Current learning rate: 0.00619
2025-10-15 06:18:31.679644: Validation loss did not improve from -0.56925. Patience: 4/50
2025-10-15 06:18:31.680286: train_loss -0.716
2025-10-15 06:18:31.680534: val_loss -0.5208
2025-10-15 06:18:31.680727: Pseudo dice [np.float32(0.7275)]
2025-10-15 06:18:31.680949: Epoch time: 46.06 s
2025-10-15 06:18:32.317996: 
2025-10-15 06:18:32.318283: Epoch 63
2025-10-15 06:18:32.318594: Current learning rate: 0.00612
2025-10-15 06:19:18.427653: Validation loss did not improve from -0.56925. Patience: 5/50
2025-10-15 06:19:18.428114: train_loss -0.7153
2025-10-15 06:19:18.428276: val_loss -0.5352
2025-10-15 06:19:18.428407: Pseudo dice [np.float32(0.7376)]
2025-10-15 06:19:18.428564: Epoch time: 46.11 s
2025-10-15 06:19:19.067247: 
2025-10-15 06:19:19.067569: Epoch 64
2025-10-15 06:19:19.067801: Current learning rate: 0.00606
2025-10-15 06:20:05.180930: Validation loss did not improve from -0.56925. Patience: 6/50
2025-10-15 06:20:05.181779: train_loss -0.7078
2025-10-15 06:20:05.182033: val_loss -0.5434
2025-10-15 06:20:05.182227: Pseudo dice [np.float32(0.754)]
2025-10-15 06:20:05.182466: Epoch time: 46.12 s
2025-10-15 06:20:05.620466: Yayy! New best EMA pseudo Dice: 0.7396000027656555
2025-10-15 06:20:06.696165: 
2025-10-15 06:20:06.696428: Epoch 65
2025-10-15 06:20:06.696652: Current learning rate: 0.006
2025-10-15 06:20:52.766147: Validation loss did not improve from -0.56925. Patience: 7/50
2025-10-15 06:20:52.766881: train_loss -0.7115
2025-10-15 06:20:52.767297: val_loss -0.5318
2025-10-15 06:20:52.767507: Pseudo dice [np.float32(0.7317)]
2025-10-15 06:20:52.767679: Epoch time: 46.07 s
2025-10-15 06:20:53.402597: 
2025-10-15 06:20:53.402934: Epoch 66
2025-10-15 06:20:53.403115: Current learning rate: 0.00593
2025-10-15 06:21:39.471028: Validation loss did not improve from -0.56925. Patience: 8/50
2025-10-15 06:21:39.471669: train_loss -0.7204
2025-10-15 06:21:39.471846: val_loss -0.5267
2025-10-15 06:21:39.472064: Pseudo dice [np.float32(0.7418)]
2025-10-15 06:21:39.472221: Epoch time: 46.07 s
2025-10-15 06:21:40.107045: 
2025-10-15 06:21:40.107370: Epoch 67
2025-10-15 06:21:40.107555: Current learning rate: 0.00587
2025-10-15 06:22:26.136406: Validation loss did not improve from -0.56925. Patience: 9/50
2025-10-15 06:22:26.136842: train_loss -0.7225
2025-10-15 06:22:26.137023: val_loss -0.5436
2025-10-15 06:22:26.137257: Pseudo dice [np.float32(0.7461)]
2025-10-15 06:22:26.137432: Epoch time: 46.03 s
2025-10-15 06:22:26.137570: Yayy! New best EMA pseudo Dice: 0.739799976348877
2025-10-15 06:22:27.214903: 
2025-10-15 06:22:27.215223: Epoch 68
2025-10-15 06:22:27.215430: Current learning rate: 0.00581
2025-10-15 06:23:13.252752: Validation loss did not improve from -0.56925. Patience: 10/50
2025-10-15 06:23:13.253572: train_loss -0.7212
2025-10-15 06:23:13.253865: val_loss -0.5426
2025-10-15 06:23:13.254090: Pseudo dice [np.float32(0.741)]
2025-10-15 06:23:13.254360: Epoch time: 46.04 s
2025-10-15 06:23:13.254578: Yayy! New best EMA pseudo Dice: 0.7398999929428101
2025-10-15 06:23:14.343148: 
2025-10-15 06:23:14.343501: Epoch 69
2025-10-15 06:23:14.343779: Current learning rate: 0.00574
2025-10-15 06:24:00.299473: Validation loss did not improve from -0.56925. Patience: 11/50
2025-10-15 06:24:00.299963: train_loss -0.721
2025-10-15 06:24:00.300119: val_loss -0.5299
2025-10-15 06:24:00.300245: Pseudo dice [np.float32(0.7494)]
2025-10-15 06:24:00.300395: Epoch time: 45.96 s
2025-10-15 06:24:00.756380: Yayy! New best EMA pseudo Dice: 0.7408999800682068
2025-10-15 06:24:01.823897: 
2025-10-15 06:24:01.824168: Epoch 70
2025-10-15 06:24:01.824368: Current learning rate: 0.00568
2025-10-15 06:24:47.942221: Validation loss did not improve from -0.56925. Patience: 12/50
2025-10-15 06:24:47.942803: train_loss -0.7208
2025-10-15 06:24:47.942971: val_loss -0.5514
2025-10-15 06:24:47.943088: Pseudo dice [np.float32(0.7501)]
2025-10-15 06:24:47.943240: Epoch time: 46.12 s
2025-10-15 06:24:47.943438: Yayy! New best EMA pseudo Dice: 0.7418000102043152
2025-10-15 06:24:49.020586: 
2025-10-15 06:24:49.020879: Epoch 71
2025-10-15 06:24:49.021079: Current learning rate: 0.00562
2025-10-15 06:25:35.068779: Validation loss did not improve from -0.56925. Patience: 13/50
2025-10-15 06:25:35.069499: train_loss -0.7233
2025-10-15 06:25:35.069837: val_loss -0.528
2025-10-15 06:25:35.070198: Pseudo dice [np.float32(0.7444)]
2025-10-15 06:25:35.070457: Epoch time: 46.05 s
2025-10-15 06:25:35.070681: Yayy! New best EMA pseudo Dice: 0.7419999837875366
2025-10-15 06:25:36.163906: 
2025-10-15 06:25:36.164186: Epoch 72
2025-10-15 06:25:36.164396: Current learning rate: 0.00555
2025-10-15 06:26:22.254704: Validation loss did not improve from -0.56925. Patience: 14/50
2025-10-15 06:26:22.255480: train_loss -0.7244
2025-10-15 06:26:22.255671: val_loss -0.5343
2025-10-15 06:26:22.255858: Pseudo dice [np.float32(0.7433)]
2025-10-15 06:26:22.256005: Epoch time: 46.09 s
2025-10-15 06:26:22.256134: Yayy! New best EMA pseudo Dice: 0.7422000169754028
2025-10-15 06:26:23.344598: 
2025-10-15 06:26:23.344842: Epoch 73
2025-10-15 06:26:23.345091: Current learning rate: 0.00549
2025-10-15 06:27:09.444253: Validation loss did not improve from -0.56925. Patience: 15/50
2025-10-15 06:27:09.444714: train_loss -0.7172
2025-10-15 06:27:09.444892: val_loss -0.5314
2025-10-15 06:27:09.445041: Pseudo dice [np.float32(0.7374)]
2025-10-15 06:27:09.445211: Epoch time: 46.1 s
2025-10-15 06:27:10.515463: 
2025-10-15 06:27:10.515732: Epoch 74
2025-10-15 06:27:10.515949: Current learning rate: 0.00542
2025-10-15 06:27:56.592088: Validation loss did not improve from -0.56925. Patience: 16/50
2025-10-15 06:27:56.592753: train_loss -0.7258
2025-10-15 06:27:56.592930: val_loss -0.5418
2025-10-15 06:27:56.593067: Pseudo dice [np.float32(0.7396)]
2025-10-15 06:27:56.593279: Epoch time: 46.08 s
2025-10-15 06:27:57.680379: 
2025-10-15 06:27:57.680647: Epoch 75
2025-10-15 06:27:57.680887: Current learning rate: 0.00536
2025-10-15 06:28:43.700583: Validation loss did not improve from -0.56925. Patience: 17/50
2025-10-15 06:28:43.701092: train_loss -0.7293
2025-10-15 06:28:43.701327: val_loss -0.5546
2025-10-15 06:28:43.701569: Pseudo dice [np.float32(0.748)]
2025-10-15 06:28:43.701809: Epoch time: 46.02 s
2025-10-15 06:28:44.330362: 
2025-10-15 06:28:44.330657: Epoch 76
2025-10-15 06:28:44.330882: Current learning rate: 0.00529
2025-10-15 06:29:30.337709: Validation loss improved from -0.56925 to -0.58572! Patience: 17/50
2025-10-15 06:29:30.338304: train_loss -0.7341
2025-10-15 06:29:30.338486: val_loss -0.5857
2025-10-15 06:29:30.338637: Pseudo dice [np.float32(0.7708)]
2025-10-15 06:29:30.338773: Epoch time: 46.01 s
2025-10-15 06:29:30.338906: Yayy! New best EMA pseudo Dice: 0.7450000047683716
2025-10-15 06:29:31.419748: 
2025-10-15 06:29:31.420100: Epoch 77
2025-10-15 06:29:31.420357: Current learning rate: 0.00523
2025-10-15 06:30:17.506114: Validation loss did not improve from -0.58572. Patience: 1/50
2025-10-15 06:30:17.506538: train_loss -0.7379
2025-10-15 06:30:17.506696: val_loss -0.5187
2025-10-15 06:30:17.506818: Pseudo dice [np.float32(0.7346)]
2025-10-15 06:30:17.506976: Epoch time: 46.09 s
2025-10-15 06:30:18.149379: 
2025-10-15 06:30:18.149638: Epoch 78
2025-10-15 06:30:18.149839: Current learning rate: 0.00517
2025-10-15 06:31:04.215252: Validation loss did not improve from -0.58572. Patience: 2/50
2025-10-15 06:31:04.215803: train_loss -0.7354
2025-10-15 06:31:04.215952: val_loss -0.5192
2025-10-15 06:31:04.216080: Pseudo dice [np.float32(0.736)]
2025-10-15 06:31:04.216233: Epoch time: 46.07 s
2025-10-15 06:31:04.854996: 
2025-10-15 06:31:04.855318: Epoch 79
2025-10-15 06:31:04.855542: Current learning rate: 0.0051
2025-10-15 06:31:50.891666: Validation loss did not improve from -0.58572. Patience: 3/50
2025-10-15 06:31:50.892265: train_loss -0.7413
2025-10-15 06:31:50.892528: val_loss -0.5098
2025-10-15 06:31:50.892726: Pseudo dice [np.float32(0.73)]
2025-10-15 06:31:50.893125: Epoch time: 46.04 s
2025-10-15 06:31:51.972033: 
2025-10-15 06:31:51.972347: Epoch 80
2025-10-15 06:31:51.972580: Current learning rate: 0.00504
2025-10-15 06:32:38.044516: Validation loss did not improve from -0.58572. Patience: 4/50
2025-10-15 06:32:38.045177: train_loss -0.7441
2025-10-15 06:32:38.045439: val_loss -0.5205
2025-10-15 06:32:38.045717: Pseudo dice [np.float32(0.732)]
2025-10-15 06:32:38.045996: Epoch time: 46.07 s
2025-10-15 06:32:38.688411: 
2025-10-15 06:32:38.688738: Epoch 81
2025-10-15 06:32:38.688935: Current learning rate: 0.00497
2025-10-15 06:33:24.770841: Validation loss did not improve from -0.58572. Patience: 5/50
2025-10-15 06:33:24.771318: train_loss -0.7412
2025-10-15 06:33:24.771593: val_loss -0.5449
2025-10-15 06:33:24.771830: Pseudo dice [np.float32(0.7476)]
2025-10-15 06:33:24.772087: Epoch time: 46.08 s
2025-10-15 06:33:25.411239: 
2025-10-15 06:33:25.411566: Epoch 82
2025-10-15 06:33:25.411776: Current learning rate: 0.00491
2025-10-15 06:34:11.464976: Validation loss did not improve from -0.58572. Patience: 6/50
2025-10-15 06:34:11.465669: train_loss -0.744
2025-10-15 06:34:11.465856: val_loss -0.515
2025-10-15 06:34:11.466026: Pseudo dice [np.float32(0.7291)]
2025-10-15 06:34:11.466179: Epoch time: 46.06 s
2025-10-15 06:34:12.087254: 
2025-10-15 06:34:12.087597: Epoch 83
2025-10-15 06:34:12.087796: Current learning rate: 0.00484
2025-10-15 06:34:58.178694: Validation loss did not improve from -0.58572. Patience: 7/50
2025-10-15 06:34:58.179408: train_loss -0.7448
2025-10-15 06:34:58.179919: val_loss -0.5404
2025-10-15 06:34:58.180340: Pseudo dice [np.float32(0.745)]
2025-10-15 06:34:58.180779: Epoch time: 46.09 s
2025-10-15 06:34:58.805730: 
2025-10-15 06:34:58.806112: Epoch 84
2025-10-15 06:34:58.806367: Current learning rate: 0.00478
2025-10-15 06:35:44.862210: Validation loss did not improve from -0.58572. Patience: 8/50
2025-10-15 06:35:44.863077: train_loss -0.7447
2025-10-15 06:35:44.863513: val_loss -0.5567
2025-10-15 06:35:44.863883: Pseudo dice [np.float32(0.7496)]
2025-10-15 06:35:44.864171: Epoch time: 46.06 s
2025-10-15 06:35:45.926034: 
2025-10-15 06:35:45.926345: Epoch 85
2025-10-15 06:35:45.926580: Current learning rate: 0.00471
2025-10-15 06:36:31.966715: Validation loss did not improve from -0.58572. Patience: 9/50
2025-10-15 06:36:31.967113: train_loss -0.7444
2025-10-15 06:36:31.967442: val_loss -0.5175
2025-10-15 06:36:31.967606: Pseudo dice [np.float32(0.7299)]
2025-10-15 06:36:31.967763: Epoch time: 46.04 s
2025-10-15 06:36:32.591696: 
2025-10-15 06:36:32.591951: Epoch 86
2025-10-15 06:36:32.592165: Current learning rate: 0.00465
2025-10-15 06:37:18.608333: Validation loss did not improve from -0.58572. Patience: 10/50
2025-10-15 06:37:18.609041: train_loss -0.7462
2025-10-15 06:37:18.609298: val_loss -0.5438
2025-10-15 06:37:18.609609: Pseudo dice [np.float32(0.7432)]
2025-10-15 06:37:18.609858: Epoch time: 46.02 s
2025-10-15 06:37:19.235669: 
2025-10-15 06:37:19.236005: Epoch 87
2025-10-15 06:37:19.236189: Current learning rate: 0.00458
2025-10-15 06:38:05.262877: Validation loss did not improve from -0.58572. Patience: 11/50
2025-10-15 06:38:05.263383: train_loss -0.7495
2025-10-15 06:38:05.263668: val_loss -0.5339
2025-10-15 06:38:05.263957: Pseudo dice [np.float32(0.7467)]
2025-10-15 06:38:05.264246: Epoch time: 46.03 s
2025-10-15 06:38:05.889216: 
2025-10-15 06:38:05.889653: Epoch 88
2025-10-15 06:38:05.890010: Current learning rate: 0.00452
2025-10-15 06:38:51.908519: Validation loss did not improve from -0.58572. Patience: 12/50
2025-10-15 06:38:51.909090: train_loss -0.7479
2025-10-15 06:38:51.909266: val_loss -0.5447
2025-10-15 06:38:51.909387: Pseudo dice [np.float32(0.7521)]
2025-10-15 06:38:51.909570: Epoch time: 46.02 s
2025-10-15 06:38:53.021775: 
2025-10-15 06:38:53.022123: Epoch 89
2025-10-15 06:38:53.022336: Current learning rate: 0.00445
2025-10-15 06:39:39.082659: Validation loss did not improve from -0.58572. Patience: 13/50
2025-10-15 06:39:39.083145: train_loss -0.7481
2025-10-15 06:39:39.083316: val_loss -0.5237
2025-10-15 06:39:39.083476: Pseudo dice [np.float32(0.7403)]
2025-10-15 06:39:39.083716: Epoch time: 46.06 s
2025-10-15 06:39:40.151350: 
2025-10-15 06:39:40.151653: Epoch 90
2025-10-15 06:39:40.151896: Current learning rate: 0.00438
2025-10-15 06:40:26.183969: Validation loss did not improve from -0.58572. Patience: 14/50
2025-10-15 06:40:26.184629: train_loss -0.7513
2025-10-15 06:40:26.184781: val_loss -0.5494
2025-10-15 06:40:26.184916: Pseudo dice [np.float32(0.7478)]
2025-10-15 06:40:26.185053: Epoch time: 46.03 s
2025-10-15 06:40:26.808542: 
2025-10-15 06:40:26.808900: Epoch 91
2025-10-15 06:40:26.809114: Current learning rate: 0.00432
2025-10-15 06:41:12.836956: Validation loss did not improve from -0.58572. Patience: 15/50
2025-10-15 06:41:12.837412: train_loss -0.7532
2025-10-15 06:41:12.837642: val_loss -0.553
2025-10-15 06:41:12.837817: Pseudo dice [np.float32(0.7535)]
2025-10-15 06:41:12.837971: Epoch time: 46.03 s
2025-10-15 06:41:13.464913: 
2025-10-15 06:41:13.465352: Epoch 92
2025-10-15 06:41:13.465658: Current learning rate: 0.00425
2025-10-15 06:41:59.509825: Validation loss did not improve from -0.58572. Patience: 16/50
2025-10-15 06:41:59.510438: train_loss -0.7576
2025-10-15 06:41:59.510635: val_loss -0.5472
2025-10-15 06:41:59.510806: Pseudo dice [np.float32(0.7516)]
2025-10-15 06:41:59.510988: Epoch time: 46.05 s
2025-10-15 06:42:00.134171: 
2025-10-15 06:42:00.134463: Epoch 93
2025-10-15 06:42:00.134733: Current learning rate: 0.00419
2025-10-15 06:42:46.194009: Validation loss did not improve from -0.58572. Patience: 17/50
2025-10-15 06:42:46.194429: train_loss -0.7558
2025-10-15 06:42:46.194638: val_loss -0.5223
2025-10-15 06:42:46.194832: Pseudo dice [np.float32(0.7423)]
2025-10-15 06:42:46.195041: Epoch time: 46.06 s
2025-10-15 06:42:46.819595: 
2025-10-15 06:42:46.819952: Epoch 94
2025-10-15 06:42:46.820205: Current learning rate: 0.00412
2025-10-15 06:43:32.865442: Validation loss did not improve from -0.58572. Patience: 18/50
2025-10-15 06:43:32.866024: train_loss -0.7575
2025-10-15 06:43:32.866193: val_loss -0.5341
2025-10-15 06:43:32.866354: Pseudo dice [np.float32(0.7396)]
2025-10-15 06:43:32.866544: Epoch time: 46.05 s
2025-10-15 06:43:33.925470: 
2025-10-15 06:43:33.925774: Epoch 95
2025-10-15 06:43:33.925964: Current learning rate: 0.00405
2025-10-15 06:44:19.965062: Validation loss did not improve from -0.58572. Patience: 19/50
2025-10-15 06:44:19.965510: train_loss -0.7529
2025-10-15 06:44:19.965726: val_loss -0.5495
2025-10-15 06:44:19.965914: Pseudo dice [np.float32(0.7488)]
2025-10-15 06:44:19.966051: Epoch time: 46.04 s
2025-10-15 06:44:20.592019: 
2025-10-15 06:44:20.592363: Epoch 96
2025-10-15 06:44:20.592572: Current learning rate: 0.00399
2025-10-15 06:45:06.698101: Validation loss did not improve from -0.58572. Patience: 20/50
2025-10-15 06:45:06.698675: train_loss -0.7582
2025-10-15 06:45:06.698859: val_loss -0.5172
2025-10-15 06:45:06.699000: Pseudo dice [np.float32(0.741)]
2025-10-15 06:45:06.699148: Epoch time: 46.11 s
2025-10-15 06:45:07.333252: 
2025-10-15 06:45:07.333748: Epoch 97
2025-10-15 06:45:07.333953: Current learning rate: 0.00392
2025-10-15 06:45:53.436734: Validation loss did not improve from -0.58572. Patience: 21/50
2025-10-15 06:45:53.437267: train_loss -0.7588
2025-10-15 06:45:53.437621: val_loss -0.5422
2025-10-15 06:45:53.437870: Pseudo dice [np.float32(0.7518)]
2025-10-15 06:45:53.438196: Epoch time: 46.1 s
2025-10-15 06:45:54.068095: 
2025-10-15 06:45:54.068377: Epoch 98
2025-10-15 06:45:54.068590: Current learning rate: 0.00385
2025-10-15 06:46:40.124052: Validation loss did not improve from -0.58572. Patience: 22/50
2025-10-15 06:46:40.124993: train_loss -0.7564
2025-10-15 06:46:40.125328: val_loss -0.5434
2025-10-15 06:46:40.125511: Pseudo dice [np.float32(0.7559)]
2025-10-15 06:46:40.125827: Epoch time: 46.06 s
2025-10-15 06:46:40.126175: Yayy! New best EMA pseudo Dice: 0.7458999752998352
2025-10-15 06:46:41.212533: 
2025-10-15 06:46:41.212837: Epoch 99
2025-10-15 06:46:41.213041: Current learning rate: 0.00379
2025-10-15 06:47:27.343061: Validation loss did not improve from -0.58572. Patience: 23/50
2025-10-15 06:47:27.343839: train_loss -0.7608
2025-10-15 06:47:27.344095: val_loss -0.547
2025-10-15 06:47:27.344424: Pseudo dice [np.float32(0.7506)]
2025-10-15 06:47:27.344613: Epoch time: 46.13 s
2025-10-15 06:47:27.801053: Yayy! New best EMA pseudo Dice: 0.746399998664856
2025-10-15 06:47:28.852473: 
2025-10-15 06:47:28.852893: Epoch 100
2025-10-15 06:47:28.853091: Current learning rate: 0.00372
2025-10-15 06:48:14.939170: Validation loss did not improve from -0.58572. Patience: 24/50
2025-10-15 06:48:14.939815: train_loss -0.7657
2025-10-15 06:48:14.939984: val_loss -0.5216
2025-10-15 06:48:14.940113: Pseudo dice [np.float32(0.7298)]
2025-10-15 06:48:14.940257: Epoch time: 46.09 s
2025-10-15 06:48:15.565135: 
2025-10-15 06:48:15.565471: Epoch 101
2025-10-15 06:48:15.565696: Current learning rate: 0.00365
2025-10-15 06:49:01.614355: Validation loss did not improve from -0.58572. Patience: 25/50
2025-10-15 06:49:01.614864: train_loss -0.7582
2025-10-15 06:49:01.615101: val_loss -0.5453
2025-10-15 06:49:01.615315: Pseudo dice [np.float32(0.7462)]
2025-10-15 06:49:01.615532: Epoch time: 46.05 s
2025-10-15 06:49:02.245914: 
2025-10-15 06:49:02.246264: Epoch 102
2025-10-15 06:49:02.246552: Current learning rate: 0.00359
2025-10-15 06:49:48.286746: Validation loss did not improve from -0.58572. Patience: 26/50
2025-10-15 06:49:48.287868: train_loss -0.7645
2025-10-15 06:49:48.288183: val_loss -0.5116
2025-10-15 06:49:48.288354: Pseudo dice [np.float32(0.7272)]
2025-10-15 06:49:48.288563: Epoch time: 46.04 s
2025-10-15 06:49:48.922712: 
2025-10-15 06:49:48.923236: Epoch 103
2025-10-15 06:49:48.923736: Current learning rate: 0.00352
2025-10-15 06:50:34.961977: Validation loss did not improve from -0.58572. Patience: 27/50
2025-10-15 06:50:34.962391: train_loss -0.7638
2025-10-15 06:50:34.962558: val_loss -0.531
2025-10-15 06:50:34.962716: Pseudo dice [np.float32(0.7414)]
2025-10-15 06:50:34.962872: Epoch time: 46.04 s
2025-10-15 06:50:35.601681: 
2025-10-15 06:50:35.601976: Epoch 104
2025-10-15 06:50:35.602169: Current learning rate: 0.00345
2025-10-15 06:51:21.615183: Validation loss did not improve from -0.58572. Patience: 28/50
2025-10-15 06:51:21.615719: train_loss -0.7628
2025-10-15 06:51:21.615873: val_loss -0.5547
2025-10-15 06:51:21.616017: Pseudo dice [np.float32(0.7556)]
2025-10-15 06:51:21.616329: Epoch time: 46.01 s
2025-10-15 06:51:23.113428: 
2025-10-15 06:51:23.113869: Epoch 105
2025-10-15 06:51:23.114108: Current learning rate: 0.00338
2025-10-15 06:52:09.123130: Validation loss did not improve from -0.58572. Patience: 29/50
2025-10-15 06:52:09.123655: train_loss -0.7656
2025-10-15 06:52:09.123840: val_loss -0.5441
2025-10-15 06:52:09.124073: Pseudo dice [np.float32(0.7515)]
2025-10-15 06:52:09.124327: Epoch time: 46.01 s
2025-10-15 06:52:09.761515: 
2025-10-15 06:52:09.761989: Epoch 106
2025-10-15 06:52:09.762356: Current learning rate: 0.00332
2025-10-15 06:52:55.855769: Validation loss did not improve from -0.58572. Patience: 30/50
2025-10-15 06:52:55.856352: train_loss -0.7668
2025-10-15 06:52:55.856515: val_loss -0.521
2025-10-15 06:52:55.856636: Pseudo dice [np.float32(0.738)]
2025-10-15 06:52:55.856771: Epoch time: 46.1 s
2025-10-15 06:52:56.486662: 
2025-10-15 06:52:56.486982: Epoch 107
2025-10-15 06:52:56.487192: Current learning rate: 0.00325
2025-10-15 06:53:42.557482: Validation loss did not improve from -0.58572. Patience: 31/50
2025-10-15 06:53:42.557946: train_loss -0.7693
2025-10-15 06:53:42.558124: val_loss -0.5626
2025-10-15 06:53:42.558364: Pseudo dice [np.float32(0.7596)]
2025-10-15 06:53:42.558517: Epoch time: 46.07 s
2025-10-15 06:53:43.189764: 
2025-10-15 06:53:43.190126: Epoch 108
2025-10-15 06:53:43.190363: Current learning rate: 0.00318
2025-10-15 06:54:29.219197: Validation loss did not improve from -0.58572. Patience: 32/50
2025-10-15 06:54:29.219945: train_loss -0.7654
2025-10-15 06:54:29.220265: val_loss -0.5684
2025-10-15 06:54:29.220509: Pseudo dice [np.float32(0.7548)]
2025-10-15 06:54:29.220815: Epoch time: 46.03 s
2025-10-15 06:54:29.221079: Yayy! New best EMA pseudo Dice: 0.7466999888420105
2025-10-15 06:54:30.302342: 
2025-10-15 06:54:30.302734: Epoch 109
2025-10-15 06:54:30.302915: Current learning rate: 0.00311
2025-10-15 06:55:16.377563: Validation loss did not improve from -0.58572. Patience: 33/50
2025-10-15 06:55:16.378086: train_loss -0.7695
2025-10-15 06:55:16.378328: val_loss -0.5755
2025-10-15 06:55:16.378538: Pseudo dice [np.float32(0.7655)]
2025-10-15 06:55:16.378779: Epoch time: 46.08 s
2025-10-15 06:55:16.827242: Yayy! New best EMA pseudo Dice: 0.7486000061035156
2025-10-15 06:55:17.890799: 
2025-10-15 06:55:17.891099: Epoch 110
2025-10-15 06:55:17.891289: Current learning rate: 0.00304
2025-10-15 06:56:03.997071: Validation loss did not improve from -0.58572. Patience: 34/50
2025-10-15 06:56:03.997866: train_loss -0.7714
2025-10-15 06:56:03.998166: val_loss -0.5203
2025-10-15 06:56:03.998341: Pseudo dice [np.float32(0.7387)]
2025-10-15 06:56:03.998550: Epoch time: 46.11 s
2025-10-15 06:56:04.639091: 
2025-10-15 06:56:04.639446: Epoch 111
2025-10-15 06:56:04.639641: Current learning rate: 0.00297
2025-10-15 06:56:50.711349: Validation loss did not improve from -0.58572. Patience: 35/50
2025-10-15 06:56:50.711802: train_loss -0.7712
2025-10-15 06:56:50.712167: val_loss -0.5494
2025-10-15 06:56:50.712441: Pseudo dice [np.float32(0.7576)]
2025-10-15 06:56:50.712650: Epoch time: 46.07 s
2025-10-15 06:56:50.713139: Yayy! New best EMA pseudo Dice: 0.7486000061035156
2025-10-15 06:56:51.799374: 
2025-10-15 06:56:51.799725: Epoch 112
2025-10-15 06:56:51.800045: Current learning rate: 0.00291
2025-10-15 06:57:38.002016: Validation loss did not improve from -0.58572. Patience: 36/50
2025-10-15 06:57:38.002785: train_loss -0.7719
2025-10-15 06:57:38.003090: val_loss -0.5112
2025-10-15 06:57:38.003325: Pseudo dice [np.float32(0.7413)]
2025-10-15 06:57:38.003584: Epoch time: 46.2 s
2025-10-15 06:57:38.643268: 
2025-10-15 06:57:38.643657: Epoch 113
2025-10-15 06:57:38.643946: Current learning rate: 0.00284
2025-10-15 06:58:24.676499: Validation loss did not improve from -0.58572. Patience: 37/50
2025-10-15 06:58:24.676942: train_loss -0.7725
2025-10-15 06:58:24.677117: val_loss -0.5341
2025-10-15 06:58:24.677270: Pseudo dice [np.float32(0.7422)]
2025-10-15 06:58:24.677406: Epoch time: 46.03 s
2025-10-15 06:58:25.317843: 
2025-10-15 06:58:25.318199: Epoch 114
2025-10-15 06:58:25.318482: Current learning rate: 0.00277
2025-10-15 06:59:11.429576: Validation loss did not improve from -0.58572. Patience: 38/50
2025-10-15 06:59:11.430343: train_loss -0.7733
2025-10-15 06:59:11.430705: val_loss -0.5348
2025-10-15 06:59:11.430842: Pseudo dice [np.float32(0.748)]
2025-10-15 06:59:11.431047: Epoch time: 46.11 s
2025-10-15 06:59:12.520886: 
2025-10-15 06:59:12.521168: Epoch 115
2025-10-15 06:59:12.521366: Current learning rate: 0.0027
2025-10-15 06:59:58.556662: Validation loss did not improve from -0.58572. Patience: 39/50
2025-10-15 06:59:58.557798: train_loss -0.7719
2025-10-15 06:59:58.558088: val_loss -0.543
2025-10-15 06:59:58.558216: Pseudo dice [np.float32(0.7498)]
2025-10-15 06:59:58.558371: Epoch time: 46.04 s
2025-10-15 06:59:59.196818: 
2025-10-15 06:59:59.197121: Epoch 116
2025-10-15 06:59:59.197310: Current learning rate: 0.00263
2025-10-15 07:00:45.267800: Validation loss did not improve from -0.58572. Patience: 40/50
2025-10-15 07:00:45.268928: train_loss -0.7741
2025-10-15 07:00:45.269476: val_loss -0.5571
2025-10-15 07:00:45.269836: Pseudo dice [np.float32(0.7539)]
2025-10-15 07:00:45.270156: Epoch time: 46.07 s
2025-10-15 07:00:45.914923: 
2025-10-15 07:00:45.915319: Epoch 117
2025-10-15 07:00:45.915527: Current learning rate: 0.00256
2025-10-15 07:01:32.023383: Validation loss did not improve from -0.58572. Patience: 41/50
2025-10-15 07:01:32.023833: train_loss -0.7789
2025-10-15 07:01:32.024085: val_loss -0.5444
2025-10-15 07:01:32.024301: Pseudo dice [np.float32(0.7427)]
2025-10-15 07:01:32.024516: Epoch time: 46.11 s
2025-10-15 07:01:32.659625: 
2025-10-15 07:01:32.659952: Epoch 118
2025-10-15 07:01:32.660231: Current learning rate: 0.00249
2025-10-15 07:02:18.755279: Validation loss did not improve from -0.58572. Patience: 42/50
2025-10-15 07:02:18.755872: train_loss -0.7789
2025-10-15 07:02:18.756044: val_loss -0.5149
2025-10-15 07:02:18.756190: Pseudo dice [np.float32(0.7311)]
2025-10-15 07:02:18.756345: Epoch time: 46.1 s
2025-10-15 07:02:19.394989: 
2025-10-15 07:02:19.395346: Epoch 119
2025-10-15 07:02:19.395601: Current learning rate: 0.00242
2025-10-15 07:03:05.461379: Validation loss did not improve from -0.58572. Patience: 43/50
2025-10-15 07:03:05.461879: train_loss -0.7756
2025-10-15 07:03:05.462071: val_loss -0.55
2025-10-15 07:03:05.462235: Pseudo dice [np.float32(0.7605)]
2025-10-15 07:03:05.462425: Epoch time: 46.07 s
2025-10-15 07:03:06.581807: 
2025-10-15 07:03:06.582106: Epoch 120
2025-10-15 07:03:06.582310: Current learning rate: 0.00235
2025-10-15 07:03:52.602807: Validation loss did not improve from -0.58572. Patience: 44/50
2025-10-15 07:03:52.603450: train_loss -0.7776
2025-10-15 07:03:52.603682: val_loss -0.5511
2025-10-15 07:03:52.603839: Pseudo dice [np.float32(0.7522)]
2025-10-15 07:03:52.604005: Epoch time: 46.02 s
2025-10-15 07:03:53.740949: 
2025-10-15 07:03:53.741328: Epoch 121
2025-10-15 07:03:53.741587: Current learning rate: 0.00228
2025-10-15 07:04:39.813852: Validation loss did not improve from -0.58572. Patience: 45/50
2025-10-15 07:04:39.814481: train_loss -0.7817
2025-10-15 07:04:39.814801: val_loss -0.5417
2025-10-15 07:04:39.814990: Pseudo dice [np.float32(0.7489)]
2025-10-15 07:04:39.815182: Epoch time: 46.07 s
2025-10-15 07:04:40.470224: 
2025-10-15 07:04:40.470551: Epoch 122
2025-10-15 07:04:40.470756: Current learning rate: 0.00221
2025-10-15 07:05:26.531889: Validation loss did not improve from -0.58572. Patience: 46/50
2025-10-15 07:05:26.532635: train_loss -0.7788
2025-10-15 07:05:26.532864: val_loss -0.5291
2025-10-15 07:05:26.533016: Pseudo dice [np.float32(0.7417)]
2025-10-15 07:05:26.533202: Epoch time: 46.06 s
2025-10-15 07:05:27.175594: 
2025-10-15 07:05:27.175960: Epoch 123
2025-10-15 07:05:27.176134: Current learning rate: 0.00214
2025-10-15 07:06:13.242481: Validation loss did not improve from -0.58572. Patience: 47/50
2025-10-15 07:06:13.242970: train_loss -0.776
2025-10-15 07:06:13.243123: val_loss -0.5583
2025-10-15 07:06:13.243264: Pseudo dice [np.float32(0.7545)]
2025-10-15 07:06:13.243428: Epoch time: 46.07 s
2025-10-15 07:06:13.892052: 
2025-10-15 07:06:13.892370: Epoch 124
2025-10-15 07:06:13.892590: Current learning rate: 0.00207
2025-10-15 07:06:59.977652: Validation loss did not improve from -0.58572. Patience: 48/50
2025-10-15 07:06:59.978978: train_loss -0.7833
2025-10-15 07:06:59.979354: val_loss -0.5583
2025-10-15 07:06:59.979683: Pseudo dice [np.float32(0.7542)]
2025-10-15 07:06:59.980060: Epoch time: 46.09 s
2025-10-15 07:07:00.457744: Yayy! New best EMA pseudo Dice: 0.7487000226974487
2025-10-15 07:07:01.569955: 
2025-10-15 07:07:01.570250: Epoch 125
2025-10-15 07:07:01.570475: Current learning rate: 0.00199
2025-10-15 07:07:47.656075: Validation loss did not improve from -0.58572. Patience: 49/50
2025-10-15 07:07:47.656502: train_loss -0.7818
2025-10-15 07:07:47.656675: val_loss -0.5552
2025-10-15 07:07:47.656832: Pseudo dice [np.float32(0.7567)]
2025-10-15 07:07:47.656974: Epoch time: 46.09 s
2025-10-15 07:07:47.657124: Yayy! New best EMA pseudo Dice: 0.7494999766349792
2025-10-15 07:07:48.726311: 
2025-10-15 07:07:48.726648: Epoch 126
2025-10-15 07:07:48.726810: Current learning rate: 0.00192
2025-10-15 07:08:34.752800: Validation loss did not improve from -0.58572. Patience: 50/50
2025-10-15 07:08:34.753433: train_loss -0.7797
2025-10-15 07:08:34.753613: val_loss -0.5432
2025-10-15 07:08:34.753737: Pseudo dice [np.float32(0.7482)]
2025-10-15 07:08:34.753933: Epoch time: 46.03 s
2025-10-15 07:08:35.388015: 
2025-10-15 07:08:35.388536: Epoch 127
2025-10-15 07:08:35.388927: Current learning rate: 0.00185
2025-10-15 07:09:21.475634: Validation loss did not improve from -0.58572. Patience: 51/50
2025-10-15 07:09:21.476323: train_loss -0.7824
2025-10-15 07:09:21.476768: val_loss -0.5412
2025-10-15 07:09:21.477124: Pseudo dice [np.float32(0.7589)]
2025-10-15 07:09:21.477435: Epoch time: 46.09 s
2025-10-15 07:09:21.477691: Yayy! New best EMA pseudo Dice: 0.7502999901771545
2025-10-15 07:09:22.555095: 
2025-10-15 07:09:22.555601: Epoch 128
2025-10-15 07:09:22.556086: Current learning rate: 0.00178
2025-10-15 07:10:08.551337: Validation loss did not improve from -0.58572. Patience: 52/50
2025-10-15 07:10:08.552398: train_loss -0.7864
2025-10-15 07:10:08.552786: val_loss -0.531
2025-10-15 07:10:08.553007: Pseudo dice [np.float32(0.7424)]
2025-10-15 07:10:08.553170: Epoch time: 46.0 s
2025-10-15 07:10:09.182925: 
2025-10-15 07:10:09.183305: Epoch 129
2025-10-15 07:10:09.183579: Current learning rate: 0.0017
2025-10-15 07:10:55.162579: Validation loss did not improve from -0.58572. Patience: 53/50
2025-10-15 07:10:55.163128: train_loss -0.7844
2025-10-15 07:10:55.163426: val_loss -0.5668
2025-10-15 07:10:55.163687: Pseudo dice [np.float32(0.7674)]
2025-10-15 07:10:55.163933: Epoch time: 45.98 s
2025-10-15 07:10:55.591392: Yayy! New best EMA pseudo Dice: 0.7512999773025513
2025-10-15 07:10:56.641516: 
2025-10-15 07:10:56.641783: Epoch 130
2025-10-15 07:10:56.641966: Current learning rate: 0.00163
2025-10-15 07:11:42.683940: Validation loss did not improve from -0.58572. Patience: 54/50
2025-10-15 07:11:42.684807: train_loss -0.7857
2025-10-15 07:11:42.685040: val_loss -0.5323
2025-10-15 07:11:42.685243: Pseudo dice [np.float32(0.7418)]
2025-10-15 07:11:42.685489: Epoch time: 46.04 s
2025-10-15 07:11:43.325543: 
2025-10-15 07:11:43.325971: Epoch 131
2025-10-15 07:11:43.326265: Current learning rate: 0.00156
2025-10-15 07:12:29.378500: Validation loss did not improve from -0.58572. Patience: 55/50
2025-10-15 07:12:29.378931: train_loss -0.7887
2025-10-15 07:12:29.379102: val_loss -0.5306
2025-10-15 07:12:29.379248: Pseudo dice [np.float32(0.7418)]
2025-10-15 07:12:29.379405: Epoch time: 46.05 s
2025-10-15 07:12:30.021095: 
2025-10-15 07:12:30.021407: Epoch 132
2025-10-15 07:12:30.021591: Current learning rate: 0.00148
2025-10-15 07:13:16.151055: Validation loss did not improve from -0.58572. Patience: 56/50
2025-10-15 07:13:16.151910: train_loss -0.7914
2025-10-15 07:13:16.152206: val_loss -0.5315
2025-10-15 07:13:16.152467: Pseudo dice [np.float32(0.7484)]
2025-10-15 07:13:16.152847: Epoch time: 46.13 s
2025-10-15 07:13:16.797880: 
2025-10-15 07:13:16.798192: Epoch 133
2025-10-15 07:13:16.798375: Current learning rate: 0.00141
2025-10-15 07:14:02.875968: Validation loss did not improve from -0.58572. Patience: 57/50
2025-10-15 07:14:02.876443: train_loss -0.7879
2025-10-15 07:14:02.876625: val_loss -0.5456
2025-10-15 07:14:02.876846: Pseudo dice [np.float32(0.7497)]
2025-10-15 07:14:02.876992: Epoch time: 46.08 s
2025-10-15 07:14:03.509245: 
2025-10-15 07:14:03.509553: Epoch 134
2025-10-15 07:14:03.509738: Current learning rate: 0.00133
2025-10-15 07:14:49.555187: Validation loss did not improve from -0.58572. Patience: 58/50
2025-10-15 07:14:49.555843: train_loss -0.7879
2025-10-15 07:14:49.556029: val_loss -0.5603
2025-10-15 07:14:49.556176: Pseudo dice [np.float32(0.7663)]
2025-10-15 07:14:49.556338: Epoch time: 46.05 s
2025-10-15 07:14:50.650120: 
2025-10-15 07:14:50.650445: Epoch 135
2025-10-15 07:14:50.650632: Current learning rate: 0.00126
2025-10-15 07:15:36.626841: Validation loss did not improve from -0.58572. Patience: 59/50
2025-10-15 07:15:36.627271: train_loss -0.7877
2025-10-15 07:15:36.627450: val_loss -0.5583
2025-10-15 07:15:36.627652: Pseudo dice [np.float32(0.7588)]
2025-10-15 07:15:36.627810: Epoch time: 45.98 s
2025-10-15 07:15:36.627945: Yayy! New best EMA pseudo Dice: 0.7519000172615051
2025-10-15 07:15:38.217375: 
2025-10-15 07:15:38.217796: Epoch 136
2025-10-15 07:15:38.218033: Current learning rate: 0.00118
2025-10-15 07:16:24.226844: Validation loss did not improve from -0.58572. Patience: 60/50
2025-10-15 07:16:24.228182: train_loss -0.7869
2025-10-15 07:16:24.228570: val_loss -0.5359
2025-10-15 07:16:24.228932: Pseudo dice [np.float32(0.7482)]
2025-10-15 07:16:24.229242: Epoch time: 46.01 s
2025-10-15 07:16:24.868510: 
2025-10-15 07:16:24.868865: Epoch 137
2025-10-15 07:16:24.869058: Current learning rate: 0.00111
2025-10-15 07:17:10.831117: Validation loss did not improve from -0.58572. Patience: 61/50
2025-10-15 07:17:10.831564: train_loss -0.7878
2025-10-15 07:17:10.831754: val_loss -0.5422
2025-10-15 07:17:10.831906: Pseudo dice [np.float32(0.7502)]
2025-10-15 07:17:10.832131: Epoch time: 45.96 s
2025-10-15 07:17:11.481059: 
2025-10-15 07:17:11.481399: Epoch 138
2025-10-15 07:17:11.481614: Current learning rate: 0.00103
2025-10-15 07:17:57.480583: Validation loss did not improve from -0.58572. Patience: 62/50
2025-10-15 07:17:57.481727: train_loss -0.7888
2025-10-15 07:17:57.482074: val_loss -0.5202
2025-10-15 07:17:57.482311: Pseudo dice [np.float32(0.7399)]
2025-10-15 07:17:57.482473: Epoch time: 46.0 s
2025-10-15 07:17:58.127212: 
2025-10-15 07:17:58.127557: Epoch 139
2025-10-15 07:17:58.127816: Current learning rate: 0.00095
2025-10-15 07:18:44.099624: Validation loss did not improve from -0.58572. Patience: 63/50
2025-10-15 07:18:44.100049: train_loss -0.7877
2025-10-15 07:18:44.100322: val_loss -0.5769
2025-10-15 07:18:44.100514: Pseudo dice [np.float32(0.7667)]
2025-10-15 07:18:44.100674: Epoch time: 45.97 s
2025-10-15 07:18:45.213898: 
2025-10-15 07:18:45.214221: Epoch 140
2025-10-15 07:18:45.214434: Current learning rate: 0.00087
2025-10-15 07:19:31.238800: Validation loss did not improve from -0.58572. Patience: 64/50
2025-10-15 07:19:31.239466: train_loss -0.7919
2025-10-15 07:19:31.239631: val_loss -0.5263
2025-10-15 07:19:31.239754: Pseudo dice [np.float32(0.743)]
2025-10-15 07:19:31.239910: Epoch time: 46.03 s
2025-10-15 07:19:31.887377: 
2025-10-15 07:19:31.887735: Epoch 141
2025-10-15 07:19:31.887984: Current learning rate: 0.00079
2025-10-15 07:20:17.880417: Validation loss did not improve from -0.58572. Patience: 65/50
2025-10-15 07:20:17.880850: train_loss -0.7886
2025-10-15 07:20:17.881027: val_loss -0.5571
2025-10-15 07:20:17.881173: Pseudo dice [np.float32(0.754)]
2025-10-15 07:20:17.881350: Epoch time: 45.99 s
2025-10-15 07:20:18.522211: 
2025-10-15 07:20:18.522538: Epoch 142
2025-10-15 07:20:18.522732: Current learning rate: 0.00071
2025-10-15 07:21:04.533308: Validation loss did not improve from -0.58572. Patience: 66/50
2025-10-15 07:21:04.533888: train_loss -0.7903
2025-10-15 07:21:04.534271: val_loss -0.5408
2025-10-15 07:21:04.534489: Pseudo dice [np.float32(0.7498)]
2025-10-15 07:21:04.534637: Epoch time: 46.01 s
2025-10-15 07:21:05.176492: 
2025-10-15 07:21:05.176792: Epoch 143
2025-10-15 07:21:05.176969: Current learning rate: 0.00063
2025-10-15 07:21:51.136640: Validation loss did not improve from -0.58572. Patience: 67/50
2025-10-15 07:21:51.137080: train_loss -0.7914
2025-10-15 07:21:51.137231: val_loss -0.5226
2025-10-15 07:21:51.137399: Pseudo dice [np.float32(0.7455)]
2025-10-15 07:21:51.137554: Epoch time: 45.96 s
2025-10-15 07:21:51.778862: 
2025-10-15 07:21:51.779212: Epoch 144
2025-10-15 07:21:51.779423: Current learning rate: 0.00055
2025-10-15 07:22:37.729678: Validation loss did not improve from -0.58572. Patience: 68/50
2025-10-15 07:22:37.730375: train_loss -0.7956
2025-10-15 07:22:37.730577: val_loss -0.5294
2025-10-15 07:22:37.730793: Pseudo dice [np.float32(0.7401)]
2025-10-15 07:22:37.730946: Epoch time: 45.95 s
2025-10-15 07:22:38.833888: 
2025-10-15 07:22:38.834229: Epoch 145
2025-10-15 07:22:38.834444: Current learning rate: 0.00047
2025-10-15 07:23:24.810925: Validation loss did not improve from -0.58572. Patience: 69/50
2025-10-15 07:23:24.811393: train_loss -0.794
2025-10-15 07:23:24.811587: val_loss -0.5592
2025-10-15 07:23:24.811742: Pseudo dice [np.float32(0.7535)]
2025-10-15 07:23:24.811894: Epoch time: 45.98 s
2025-10-15 07:23:25.463976: 
2025-10-15 07:23:25.464309: Epoch 146
2025-10-15 07:23:25.464584: Current learning rate: 0.00038
2025-10-15 07:24:11.424972: Validation loss did not improve from -0.58572. Patience: 70/50
2025-10-15 07:24:11.425681: train_loss -0.7954
2025-10-15 07:24:11.425925: val_loss -0.5316
2025-10-15 07:24:11.426187: Pseudo dice [np.float32(0.7381)]
2025-10-15 07:24:11.426459: Epoch time: 45.96 s
2025-10-15 07:24:12.070937: 
2025-10-15 07:24:12.071272: Epoch 147
2025-10-15 07:24:12.071552: Current learning rate: 0.0003
2025-10-15 07:24:58.045843: Validation loss did not improve from -0.58572. Patience: 71/50
2025-10-15 07:24:58.046283: train_loss -0.7938
2025-10-15 07:24:58.046608: val_loss -0.5273
2025-10-15 07:24:58.046870: Pseudo dice [np.float32(0.7436)]
2025-10-15 07:24:58.047189: Epoch time: 45.98 s
2025-10-15 07:24:58.689236: 
2025-10-15 07:24:58.689568: Epoch 148
2025-10-15 07:24:58.689862: Current learning rate: 0.00021
2025-10-15 07:25:44.624628: Validation loss did not improve from -0.58572. Patience: 72/50
2025-10-15 07:25:44.625253: train_loss -0.7917
2025-10-15 07:25:44.625428: val_loss -0.5434
2025-10-15 07:25:44.625564: Pseudo dice [np.float32(0.7462)]
2025-10-15 07:25:44.625709: Epoch time: 45.94 s
2025-10-15 07:25:45.259917: 
2025-10-15 07:25:45.260266: Epoch 149
2025-10-15 07:25:45.260440: Current learning rate: 0.00011
2025-10-15 07:26:31.271606: Validation loss did not improve from -0.58572. Patience: 73/50
2025-10-15 07:26:31.272080: train_loss -0.7959
2025-10-15 07:26:31.272269: val_loss -0.5407
2025-10-15 07:26:31.272470: Pseudo dice [np.float32(0.7498)]
2025-10-15 07:26:31.272671: Epoch time: 46.01 s
2025-10-15 07:26:32.405703: Training done.
2025-10-15 07:26:32.413817: Using splits from existing split file: /nfs/erelab001/shared/Computational_Group/Naravich/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset307_Sohee_Calcium_OCT_CrossValidation/splits_final_60.json
2025-10-15 07:26:32.414160: The split file contains 5 splits.
2025-10-15 07:26:32.414310: Desired fold for training: 4
2025-10-15 07:26:32.414452: This split has 4 training and 4 validation cases.
2025-10-15 07:26:32.414671: predicting 101-044
2025-10-15 07:26:32.416620: 101-044, shape torch.Size([1, 404, 498, 498]), rank 0
2025-10-15 07:27:22.457187: predicting 101-045
2025-10-15 07:27:22.465794: 101-045, shape torch.Size([1, 375, 498, 498]), rank 0
2025-10-15 07:27:56.645218: predicting 401-004
2025-10-15 07:27:56.652848: 401-004, shape torch.Size([1, 375, 498, 498]), rank 0
2025-10-15 07:28:30.834766: predicting 706-005
2025-10-15 07:28:30.842101: 706-005, shape torch.Size([1, 375, 498, 498]), rank 0
2025-10-15 07:29:17.875209: Validation complete
2025-10-15 07:29:17.875548: Mean Validation Dice:  0.7418967874260581
Finished training fold 4 saving to /nfs/erelab001/shared/Computational_Group/Naravich/datasets/nnUNet_Datasets/nnUNet_results/Dataset307_Sohee_Calcium_OCT_CrossValidation/nnUNetTrainerScaleAnalysis60__nnUNetPlans__3d_32x160x128_b10/fold_4_Genesis_Pretrained
