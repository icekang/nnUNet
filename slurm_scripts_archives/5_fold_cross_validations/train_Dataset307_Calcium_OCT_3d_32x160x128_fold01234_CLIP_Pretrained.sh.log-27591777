
CommandNotFoundError: Your shell has not been properly configured to use 'conda activate'.
To initialize your shell, run

    $ conda init <SHELL_NAME>

Currently supported shells are:
  - bash
  - fish
  - tcsh
  - xonsh
  - zsh
  - powershell

See 'conda init --help' for more information and options.

IMPORTANT: You may need to close and restart your shell after running 'conda init'.


/home/gridsan/nchutisilp/.conda/envs/nnunet/bin/python
Starting training with CONFIG=3d_32x160x128_b10, DATASET_ID=307, TRAINER=nnUNetTrainer

############################
INFO: You are using the old nnU-Net default plans. We have updated our recommendations. Please consider using those instead! Read more here: https://github.com/MIC-DKFZ/nnUNet/blob/master/documentation/resenc_presets.md
############################

Using device: cuda:0

#######################################################################
Please cite the following paper when using nnU-Net:
Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.
#######################################################################

2024-12-11 18:39:14.834601: Using torch.compile...

############################
INFO: You are using the old nnU-Net default plans. We have updated our recommendations. Please consider using those instead! Read more here: https://github.com/MIC-DKFZ/nnUNet/blob/master/documentation/resenc_presets.md
############################

Using device: cuda:0

#######################################################################
Please cite the following paper when using nnU-Net:
Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.
#######################################################################

2024-12-11 18:39:14.834907: Using torch.compile...
/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
################### Loading pretrained weights from file  /home/gridsan/nchutisilp/projects/OpenAI-CLIP/clip_pretrained_nnUNet.pt ###################
Below is the list of overlapping blocks in pretrained model and nnUNet architecture:
encoder.stages.0.0.convs.0.conv.weight shape torch.Size([32, 1, 3, 3, 3])
encoder.stages.0.0.convs.0.conv.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.norm.weight shape torch.Size([32])
encoder.stages.0.0.convs.0.norm.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.0.weight shape torch.Size([32, 1, 3, 3, 3])
encoder.stages.0.0.convs.0.all_modules.0.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.1.weight shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.1.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
encoder.stages.0.0.convs.1.conv.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.norm.weight shape torch.Size([32])
encoder.stages.0.0.convs.1.norm.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
encoder.stages.0.0.convs.1.all_modules.0.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.1.weight shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.1.bias shape torch.Size([32])
encoder.stages.1.0.convs.0.conv.weight shape torch.Size([64, 32, 3, 3, 3])
encoder.stages.1.0.convs.0.conv.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.norm.weight shape torch.Size([64])
encoder.stages.1.0.convs.0.norm.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.0.weight shape torch.Size([64, 32, 3, 3, 3])
encoder.stages.1.0.convs.0.all_modules.0.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.1.weight shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.1.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
encoder.stages.1.0.convs.1.conv.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.norm.weight shape torch.Size([64])
encoder.stages.1.0.convs.1.norm.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
encoder.stages.1.0.convs.1.all_modules.0.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.1.weight shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.1.bias shape torch.Size([64])
encoder.stages.2.0.convs.0.conv.weight shape torch.Size([128, 64, 3, 3, 3])
encoder.stages.2.0.convs.0.conv.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.norm.weight shape torch.Size([128])
encoder.stages.2.0.convs.0.norm.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.0.weight shape torch.Size([128, 64, 3, 3, 3])
encoder.stages.2.0.convs.0.all_modules.0.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.1.weight shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.1.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
encoder.stages.2.0.convs.1.conv.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.norm.weight shape torch.Size([128])
encoder.stages.2.0.convs.1.norm.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
encoder.stages.2.0.convs.1.all_modules.0.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.1.weight shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.1.bias shape torch.Size([128])
encoder.stages.3.0.convs.0.conv.weight shape torch.Size([256, 128, 3, 3, 3])
encoder.stages.3.0.convs.0.conv.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.norm.weight shape torch.Size([256])
encoder.stages.3.0.convs.0.norm.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.0.weight shape torch.Size([256, 128, 3, 3, 3])
encoder.stages.3.0.convs.0.all_modules.0.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.1.weight shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.1.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
encoder.stages.3.0.convs.1.conv.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.norm.weight shape torch.Size([256])
encoder.stages.3.0.convs.1.norm.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
encoder.stages.3.0.convs.1.all_modules.0.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.1.weight shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.1.bias shape torch.Size([256])
encoder.stages.4.0.convs.0.conv.weight shape torch.Size([320, 256, 3, 3, 3])
encoder.stages.4.0.convs.0.conv.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.norm.weight shape torch.Size([320])
encoder.stages.4.0.convs.0.norm.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.0.weight shape torch.Size([320, 256, 3, 3, 3])
encoder.stages.4.0.convs.0.all_modules.0.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.1.weight shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.1.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.4.0.convs.1.conv.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.norm.weight shape torch.Size([320])
encoder.stages.4.0.convs.1.norm.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.4.0.convs.1.all_modules.0.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.1.weight shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.1.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.0.conv.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.norm.weight shape torch.Size([320])
encoder.stages.5.0.convs.0.norm.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.0.all_modules.0.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.1.weight shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.1.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.1.conv.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.norm.weight shape torch.Size([320])
encoder.stages.5.0.convs.1.norm.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.1.all_modules.0.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.1.weight shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.0.0.convs.0.conv.weight shape torch.Size([32, 1, 3, 3, 3])
decoder.encoder.stages.0.0.convs.0.conv.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.norm.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.norm.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.0.weight shape torch.Size([32, 1, 3, 3, 3])
decoder.encoder.stages.0.0.convs.0.all_modules.0.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.1.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.1.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.encoder.stages.0.0.convs.1.conv.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.norm.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.norm.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.encoder.stages.0.0.convs.1.all_modules.0.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.1.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.1.bias shape torch.Size([32])
decoder.encoder.stages.1.0.convs.0.conv.weight shape torch.Size([64, 32, 3, 3, 3])
decoder.encoder.stages.1.0.convs.0.conv.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.norm.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.norm.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.0.weight shape torch.Size([64, 32, 3, 3, 3])
decoder.encoder.stages.1.0.convs.0.all_modules.0.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.1.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.1.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.encoder.stages.1.0.convs.1.conv.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.norm.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.norm.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.encoder.stages.1.0.convs.1.all_modules.0.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.1.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.1.bias shape torch.Size([64])
decoder.encoder.stages.2.0.convs.0.conv.weight shape torch.Size([128, 64, 3, 3, 3])
decoder.encoder.stages.2.0.convs.0.conv.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.norm.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.norm.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.0.weight shape torch.Size([128, 64, 3, 3, 3])
decoder.encoder.stages.2.0.convs.0.all_modules.0.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.1.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.1.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.encoder.stages.2.0.convs.1.conv.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.norm.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.norm.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.encoder.stages.2.0.convs.1.all_modules.0.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.1.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.1.bias shape torch.Size([128])
decoder.encoder.stages.3.0.convs.0.conv.weight shape torch.Size([256, 128, 3, 3, 3])
decoder.encoder.stages.3.0.convs.0.conv.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.norm.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.norm.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.0.weight shape torch.Size([256, 128, 3, 3, 3])
decoder.encoder.stages.3.0.convs.0.all_modules.0.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.1.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.1.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.encoder.stages.3.0.convs.1.conv.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.norm.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.norm.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.encoder.stages.3.0.convs.1.all_modules.0.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.1.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.1.bias shape torch.Size([256])
decoder.encoder.stages.4.0.convs.0.conv.weight shape torch.Size([320, 256, 3, 3, 3])
decoder.encoder.stages.4.0.convs.0.conv.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.norm.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.norm.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.0.weight shape torch.Size([320, 256, 3, 3, 3])
decoder.encoder.stages.4.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.4.0.convs.1.conv.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.norm.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.norm.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.4.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.0.conv.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.norm.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.norm.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.1.conv.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.norm.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.norm.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.stages.0.convs.0.conv.weight shape torch.Size([320, 640, 3, 3, 3])
decoder.stages.0.convs.0.conv.bias shape torch.Size([320])
decoder.stages.0.convs.0.norm.weight shape torch.Size([320])
decoder.stages.0.convs.0.norm.bias shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.0.weight shape torch.Size([320, 640, 3, 3, 3])
decoder.stages.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.stages.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.stages.0.convs.1.conv.bias shape torch.Size([320])
decoder.stages.0.convs.1.norm.weight shape torch.Size([320])
decoder.stages.0.convs.1.norm.bias shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.stages.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.stages.1.convs.0.conv.weight shape torch.Size([256, 512, 3, 3, 3])
decoder.stages.1.convs.0.conv.bias shape torch.Size([256])
decoder.stages.1.convs.0.norm.weight shape torch.Size([256])
decoder.stages.1.convs.0.norm.bias shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.0.weight shape torch.Size([256, 512, 3, 3, 3])
decoder.stages.1.convs.0.all_modules.0.bias shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.1.weight shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.1.bias shape torch.Size([256])
decoder.stages.1.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.stages.1.convs.1.conv.bias shape torch.Size([256])
decoder.stages.1.convs.1.norm.weight shape torch.Size([256])
decoder.stages.1.convs.1.norm.bias shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.stages.1.convs.1.all_modules.0.bias shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.1.weight shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.1.bias shape torch.Size([256])
decoder.stages.2.convs.0.conv.weight shape torch.Size([128, 256, 3, 3, 3])
decoder.stages.2.convs.0.conv.bias shape torch.Size([128])
decoder.stages.2.convs.0.norm.weight shape torch.Size([128])
decoder.stages.2.convs.0.norm.bias shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.0.weight shape torch.Size([128, 256, 3, 3, 3])
decoder.stages.2.convs.0.all_modules.0.bias shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.1.weight shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.1.bias shape torch.Size([128])
decoder.stages.2.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.stages.2.convs.1.conv.bias shape torch.Size([128])
decoder.stages.2.convs.1.norm.weight shape torch.Size([128])
decoder.stages.2.convs.1.norm.bias shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.stages.2.convs.1.all_modules.0.bias shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.1.weight shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.1.bias shape torch.Size([128])
decoder.stages.3.convs.0.conv.weight shape torch.Size([64, 128, 3, 3, 3])
decoder.stages.3.convs.0.conv.bias shape torch.Size([64])
decoder.stages.3.convs.0.norm.weight shape torch.Size([64])
decoder.stages.3.convs.0.norm.bias shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.0.weight shape torch.Size([64, 128, 3, 3, 3])
decoder.stages.3.convs.0.all_modules.0.bias shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.1.weight shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.1.bias shape torch.Size([64])
decoder.stages.3.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.stages.3.convs.1.conv.bias shape torch.Size([64])
decoder.stages.3.convs.1.norm.weight shape torch.Size([64])
decoder.stages.3.convs.1.norm.bias shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.stages.3.convs.1.all_modules.0.bias shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.1.weight shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.1.bias shape torch.Size([64])
decoder.stages.4.convs.0.conv.weight shape torch.Size([32, 64, 3, 3, 3])
decoder.stages.4.convs.0.conv.bias shape torch.Size([32])
decoder.stages.4.convs.0.norm.weight shape torch.Size([32])
decoder.stages.4.convs.0.norm.bias shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.0.weight shape torch.Size([32, 64, 3, 3, 3])
decoder.stages.4.convs.0.all_modules.0.bias shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.1.weight shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.1.bias shape torch.Size([32])
decoder.stages.4.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.stages.4.convs.1.conv.bias shape torch.Size([32])
decoder.stages.4.convs.1.norm.weight shape torch.Size([32])
decoder.stages.4.convs.1.norm.bias shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.stages.4.convs.1.all_modules.0.bias shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.1.weight shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.1.bias shape torch.Size([32])
decoder.transpconvs.0.weight shape torch.Size([320, 320, 1, 2, 2])
decoder.transpconvs.0.bias shape torch.Size([320])
decoder.transpconvs.1.weight shape torch.Size([320, 256, 2, 2, 2])
decoder.transpconvs.1.bias shape torch.Size([256])
decoder.transpconvs.2.weight shape torch.Size([256, 128, 2, 2, 2])
decoder.transpconvs.2.bias shape torch.Size([128])
decoder.transpconvs.3.weight shape torch.Size([128, 64, 2, 2, 2])
decoder.transpconvs.3.bias shape torch.Size([64])
decoder.transpconvs.4.weight shape torch.Size([64, 32, 2, 2, 2])
decoder.transpconvs.4.bias shape torch.Size([32])
################### Done ###################
2024-12-11 18:39:19.177035: do_dummy_2d_data_aug: True
2024-12-11 18:39:19.180425: Using splits from existing split file: /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset307_Sohee_Calcium_OCT_CrossValidation/splits_final.json
2024-12-11 18:39:19.183161: The split file contains 5 splits.
2024-12-11 18:39:19.184253: Desired fold for training: 1
2024-12-11 18:39:19.185438: This split has 6 training and 2 validation cases.
################### Loading pretrained weights from file  /home/gridsan/nchutisilp/projects/OpenAI-CLIP/clip_pretrained_nnUNet.pt ###################
Below is the list of overlapping blocks in pretrained model and nnUNet architecture:
encoder.stages.0.0.convs.0.conv.weight shape torch.Size([32, 1, 3, 3, 3])
encoder.stages.0.0.convs.0.conv.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.norm.weight shape torch.Size([32])
encoder.stages.0.0.convs.0.norm.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.0.weight shape torch.Size([32, 1, 3, 3, 3])
encoder.stages.0.0.convs.0.all_modules.0.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.1.weight shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.1.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
encoder.stages.0.0.convs.1.conv.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.norm.weight shape torch.Size([32])
encoder.stages.0.0.convs.1.norm.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
encoder.stages.0.0.convs.1.all_modules.0.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.1.weight shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.1.bias shape torch.Size([32])
encoder.stages.1.0.convs.0.conv.weight shape torch.Size([64, 32, 3, 3, 3])
encoder.stages.1.0.convs.0.conv.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.norm.weight shape torch.Size([64])
encoder.stages.1.0.convs.0.norm.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.0.weight shape torch.Size([64, 32, 3, 3, 3])
encoder.stages.1.0.convs.0.all_modules.0.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.1.weight shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.1.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
encoder.stages.1.0.convs.1.conv.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.norm.weight shape torch.Size([64])
encoder.stages.1.0.convs.1.norm.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
encoder.stages.1.0.convs.1.all_modules.0.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.1.weight shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.1.bias shape torch.Size([64])
encoder.stages.2.0.convs.0.conv.weight shape torch.Size([128, 64, 3, 3, 3])
encoder.stages.2.0.convs.0.conv.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.norm.weight shape torch.Size([128])
encoder.stages.2.0.convs.0.norm.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.0.weight shape torch.Size([128, 64, 3, 3, 3])
encoder.stages.2.0.convs.0.all_modules.0.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.1.weight shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.1.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
encoder.stages.2.0.convs.1.conv.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.norm.weight shape torch.Size([128])
encoder.stages.2.0.convs.1.norm.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
encoder.stages.2.0.convs.1.all_modules.0.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.1.weight shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.1.bias shape torch.Size([128])
encoder.stages.3.0.convs.0.conv.weight shape torch.Size([256, 128, 3, 3, 3])
encoder.stages.3.0.convs.0.conv.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.norm.weight shape torch.Size([256])
encoder.stages.3.0.convs.0.norm.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.0.weight shape torch.Size([256, 128, 3, 3, 3])
encoder.stages.3.0.convs.0.all_modules.0.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.1.weight shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.1.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
encoder.stages.3.0.convs.1.conv.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.norm.weight shape torch.Size([256])
encoder.stages.3.0.convs.1.norm.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
encoder.stages.3.0.convs.1.all_modules.0.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.1.weight shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.1.bias shape torch.Size([256])
encoder.stages.4.0.convs.0.conv.weight shape torch.Size([320, 256, 3, 3, 3])
encoder.stages.4.0.convs.0.conv.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.norm.weight shape torch.Size([320])
encoder.stages.4.0.convs.0.norm.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.0.weight shape torch.Size([320, 256, 3, 3, 3])
encoder.stages.4.0.convs.0.all_modules.0.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.1.weight shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.1.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.4.0.convs.1.conv.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.norm.weight shape torch.Size([320])
encoder.stages.4.0.convs.1.norm.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.4.0.convs.1.all_modules.0.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.1.weight shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.1.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.0.conv.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.norm.weight shape torch.Size([320])
encoder.stages.5.0.convs.0.norm.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.0.all_modules.0.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.1.weight shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.1.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.1.conv.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.norm.weight shape torch.Size([320])
encoder.stages.5.0.convs.1.norm.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.1.all_modules.0.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.1.weight shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.0.0.convs.0.conv.weight shape torch.Size([32, 1, 3, 3, 3])
decoder.encoder.stages.0.0.convs.0.conv.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.norm.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.norm.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.0.weight shape torch.Size([32, 1, 3, 3, 3])
decoder.encoder.stages.0.0.convs.0.all_modules.0.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.1.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.1.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.encoder.stages.0.0.convs.1.conv.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.norm.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.norm.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.encoder.stages.0.0.convs.1.all_modules.0.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.1.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.1.bias shape torch.Size([32])
decoder.encoder.stages.1.0.convs.0.conv.weight shape torch.Size([64, 32, 3, 3, 3])
decoder.encoder.stages.1.0.convs.0.conv.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.norm.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.norm.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.0.weight shape torch.Size([64, 32, 3, 3, 3])
decoder.encoder.stages.1.0.convs.0.all_modules.0.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.1.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.1.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.encoder.stages.1.0.convs.1.conv.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.norm.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.norm.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.encoder.stages.1.0.convs.1.all_modules.0.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.1.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.1.bias shape torch.Size([64])
decoder.encoder.stages.2.0.convs.0.conv.weight shape torch.Size([128, 64, 3, 3, 3])
decoder.encoder.stages.2.0.convs.0.conv.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.norm.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.norm.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.0.weight shape torch.Size([128, 64, 3, 3, 3])
decoder.encoder.stages.2.0.convs.0.all_modules.0.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.1.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.1.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.encoder.stages.2.0.convs.1.conv.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.norm.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.norm.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.encoder.stages.2.0.convs.1.all_modules.0.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.1.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.1.bias shape torch.Size([128])
decoder.encoder.stages.3.0.convs.0.conv.weight shape torch.Size([256, 128, 3, 3, 3])
decoder.encoder.stages.3.0.convs.0.conv.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.norm.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.norm.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.0.weight shape torch.Size([256, 128, 3, 3, 3])
decoder.encoder.stages.3.0.convs.0.all_modules.0.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.1.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.1.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.encoder.stages.3.0.convs.1.conv.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.norm.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.norm.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.encoder.stages.3.0.convs.1.all_modules.0.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.1.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.1.bias shape torch.Size([256])
decoder.encoder.stages.4.0.convs.0.conv.weight shape torch.Size([320, 256, 3, 3, 3])
decoder.encoder.stages.4.0.convs.0.conv.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.norm.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.norm.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.0.weight shape torch.Size([320, 256, 3, 3, 3])
decoder.encoder.stages.4.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.4.0.convs.1.conv.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.norm.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.norm.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.4.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.0.conv.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.norm.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.norm.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.1.conv.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.norm.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.norm.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.stages.0.convs.0.conv.weight shape torch.Size([320, 640, 3, 3, 3])
decoder.stages.0.convs.0.conv.bias shape torch.Size([320])
decoder.stages.0.convs.0.norm.weight shape torch.Size([320])
decoder.stages.0.convs.0.norm.bias shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.0.weight shape torch.Size([320, 640, 3, 3, 3])
decoder.stages.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.stages.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.stages.0.convs.1.conv.bias shape torch.Size([320])
decoder.stages.0.convs.1.norm.weight shape torch.Size([320])
decoder.stages.0.convs.1.norm.bias shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.stages.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.stages.1.convs.0.conv.weight shape torch.Size([256, 512, 3, 3, 3])
decoder.stages.1.convs.0.conv.bias shape torch.Size([256])
decoder.stages.1.convs.0.norm.weight shape torch.Size([256])
decoder.stages.1.convs.0.norm.bias shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.0.weight shape torch.Size([256, 512, 3, 3, 3])
decoder.stages.1.convs.0.all_modules.0.bias shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.1.weight shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.1.bias shape torch.Size([256])
decoder.stages.1.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.stages.1.convs.1.conv.bias shape torch.Size([256])
decoder.stages.1.convs.1.norm.weight shape torch.Size([256])
decoder.stages.1.convs.1.norm.bias shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.stages.1.convs.1.all_modules.0.bias shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.1.weight shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.1.bias shape torch.Size([256])
decoder.stages.2.convs.0.conv.weight shape torch.Size([128, 256, 3, 3, 3])
decoder.stages.2.convs.0.conv.bias shape torch.Size([128])
decoder.stages.2.convs.0.norm.weight shape torch.Size([128])
decoder.stages.2.convs.0.norm.bias shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.0.weight shape torch.Size([128, 256, 3, 3, 3])
decoder.stages.2.convs.0.all_modules.0.bias shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.1.weight shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.1.bias shape torch.Size([128])
decoder.stages.2.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.stages.2.convs.1.conv.bias shape torch.Size([128])
decoder.stages.2.convs.1.norm.weight shape torch.Size([128])
decoder.stages.2.convs.1.norm.bias shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.stages.2.convs.1.all_modules.0.bias shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.1.weight shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.1.bias shape torch.Size([128])
decoder.stages.3.convs.0.conv.weight shape torch.Size([64, 128, 3, 3, 3])
decoder.stages.3.convs.0.conv.bias shape torch.Size([64])
decoder.stages.3.convs.0.norm.weight shape torch.Size([64])
decoder.stages.3.convs.0.norm.bias shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.0.weight shape torch.Size([64, 128, 3, 3, 3])
decoder.stages.3.convs.0.all_modules.0.bias shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.1.weight shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.1.bias shape torch.Size([64])
decoder.stages.3.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.stages.3.convs.1.conv.bias shape torch.Size([64])
decoder.stages.3.convs.1.norm.weight shape torch.Size([64])
decoder.stages.3.convs.1.norm.bias shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.stages.3.convs.1.all_modules.0.bias shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.1.weight shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.1.bias shape torch.Size([64])
decoder.stages.4.convs.0.conv.weight shape torch.Size([32, 64, 3, 3, 3])
decoder.stages.4.convs.0.conv.bias shape torch.Size([32])
decoder.stages.4.convs.0.norm.weight shape torch.Size([32])
decoder.stages.4.convs.0.norm.bias shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.0.weight shape torch.Size([32, 64, 3, 3, 3])
decoder.stages.4.convs.0.all_modules.0.bias shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.1.weight shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.1.bias shape torch.Size([32])
decoder.stages.4.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.stages.4.convs.1.conv.bias shape torch.Size([32])
decoder.stages.4.convs.1.norm.weight shape torch.Size([32])
decoder.stages.4.convs.1.norm.bias shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.stages.4.convs.1.all_modules.0.bias shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.1.weight shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.1.bias shape torch.Size([32])
decoder.transpconvs.0.weight shape torch.Size([320, 320, 1, 2, 2])
decoder.transpconvs.0.bias shape torch.Size([320])
decoder.transpconvs.1.weight shape torch.Size([320, 256, 2, 2, 2])
decoder.transpconvs.1.bias shape torch.Size([256])
decoder.transpconvs.2.weight shape torch.Size([256, 128, 2, 2, 2])
decoder.transpconvs.2.bias shape torch.Size([128])
decoder.transpconvs.3.weight shape torch.Size([128, 64, 2, 2, 2])
decoder.transpconvs.3.bias shape torch.Size([64])
decoder.transpconvs.4.weight shape torch.Size([64, 32, 2, 2, 2])
decoder.transpconvs.4.bias shape torch.Size([32])
################### Done ###################
2024-12-11 18:39:19.177014: do_dummy_2d_data_aug: True
2024-12-11 18:39:19.180410: Using splits from existing split file: /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset307_Sohee_Calcium_OCT_CrossValidation/splits_final.json
2024-12-11 18:39:19.183008: The split file contains 5 splits.
2024-12-11 18:39:19.184402: Desired fold for training: 0
2024-12-11 18:39:19.185527: This split has 6 training and 2 validation cases.
using pin_memory on device 0
using pin_memory on device 0
using pin_memory on device 0

This is the configuration used by this training:
Configuration name: 3d_32x160x128_b10
 {'data_identifier': 'nnUNetPlans_3d_fullres', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 10, 'patch_size': [32, 160, 128], 'median_image_size_in_voxels': [375.0, 498.0, 498.0], 'spacing': [1.0, 1.0, 1.0], 'normalization_schemes': ['ZScoreNormalization'], 'use_mask_for_norm': [False], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'architecture': {'network_class_name': 'dynamic_network_architectures.architectures.unet.PlainConvUNet', 'arch_kwargs': {'n_stages': 6, 'features_per_stage': [32, 64, 128, 256, 320, 320], 'conv_op': 'torch.nn.modules.conv.Conv3d', 'kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]], 'strides': [[1, 1, 1], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [1, 2, 2]], 'n_conv_per_stage': [2, 2, 2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2, 2, 2], 'conv_bias': True, 'norm_op': 'torch.nn.modules.instancenorm.InstanceNorm3d', 'norm_op_kwargs': {'eps': 1e-05, 'affine': True}, 'dropout_op': None, 'dropout_op_kwargs': None, 'nonlin': 'torch.nn.LeakyReLU', 'nonlin_kwargs': {'inplace': True}, 'deep_supervision': True}, '_kw_requires_import': ['conv_op', 'norm_op', 'dropout_op', 'nonlin']}, 'batch_dice': True, 'inherits_from': '3d_fullres'} 

These are the global plan.json settings:
 {'dataset_name': 'Dataset307_Sohee_Calcium_OCT_CrossValidation', 'plans_name': 'nnUNetPlans', 'original_median_spacing_after_transp': [1.0, 1.0, 1.0], 'original_median_shape_after_transp': [375, 498, 498], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 0.4977976381778717, 'mean': 0.13507525622844696, 'median': 0.09599608182907104, 'min': 0.0, 'percentile_00_5': 0.014754901640117168, 'percentile_99_5': 0.4977976381778717, 'std': 0.12025152146816254}}} 

2024-12-11 18:39:27.306134: unpacking dataset...
using pin_memory on device 0

This is the configuration used by this training:
Configuration name: 3d_32x160x128_b10
 {'data_identifier': 'nnUNetPlans_3d_fullres', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 10, 'patch_size': [32, 160, 128], 'median_image_size_in_voxels': [375.0, 498.0, 498.0], 'spacing': [1.0, 1.0, 1.0], 'normalization_schemes': ['ZScoreNormalization'], 'use_mask_for_norm': [False], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'architecture': {'network_class_name': 'dynamic_network_architectures.architectures.unet.PlainConvUNet', 'arch_kwargs': {'n_stages': 6, 'features_per_stage': [32, 64, 128, 256, 320, 320], 'conv_op': 'torch.nn.modules.conv.Conv3d', 'kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]], 'strides': [[1, 1, 1], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [1, 2, 2]], 'n_conv_per_stage': [2, 2, 2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2, 2, 2], 'conv_bias': True, 'norm_op': 'torch.nn.modules.instancenorm.InstanceNorm3d', 'norm_op_kwargs': {'eps': 1e-05, 'affine': True}, 'dropout_op': None, 'dropout_op_kwargs': None, 'nonlin': 'torch.nn.LeakyReLU', 'nonlin_kwargs': {'inplace': True}, 'deep_supervision': True}, '_kw_requires_import': ['conv_op', 'norm_op', 'dropout_op', 'nonlin']}, 'batch_dice': True, 'inherits_from': '3d_fullres'} 

These are the global plan.json settings:
 {'dataset_name': 'Dataset307_Sohee_Calcium_OCT_CrossValidation', 'plans_name': 'nnUNetPlans', 'original_median_spacing_after_transp': [1.0, 1.0, 1.0], 'original_median_shape_after_transp': [375, 498, 498], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 0.4977976381778717, 'mean': 0.13507525622844696, 'median': 0.09599608182907104, 'min': 0.0, 'percentile_00_5': 0.014754901640117168, 'percentile_99_5': 0.4977976381778717, 'std': 0.12025152146816254}}} 

2024-12-11 18:39:27.349339: unpacking dataset...
2024-12-11 18:39:32.979033: unpacking done...
2024-12-11 18:39:33.023126: Unable to plot network architecture: nnUNet_compile is enabled!
2024-12-11 18:39:33.091481: 
2024-12-11 18:39:33.093073: Epoch 0
2024-12-11 18:39:33.094282: Current learning rate: 0.01
2024-12-11 18:42:27.364544: Validation loss improved from 1000.00000 to -0.16678! Patience: 0/50
2024-12-11 18:42:27.367754: train_loss -0.0881
2024-12-11 18:42:27.369195: val_loss -0.1668
2024-12-11 18:42:27.369883: Pseudo dice [0.5519]
2024-12-11 18:42:27.370675: Epoch time: 174.28 s
2024-12-11 18:42:27.371361: Yayy! New best EMA pseudo Dice: 0.5519
2024-12-11 18:42:28.844877: 
2024-12-11 18:42:28.846349: Epoch 1
2024-12-11 18:42:28.847173: Current learning rate: 0.00999
2024-12-11 18:43:59.363938: Validation loss improved from -0.16678 to -0.20433! Patience: 0/50
2024-12-11 18:43:59.365574: train_loss -0.2396
2024-12-11 18:43:59.366905: val_loss -0.2043
2024-12-11 18:43:59.367736: Pseudo dice [0.5801]
2024-12-11 18:43:59.368531: Epoch time: 90.52 s
2024-12-11 18:43:59.369208: Yayy! New best EMA pseudo Dice: 0.5547
2024-12-11 18:44:01.016712: 
2024-12-11 18:44:01.018878: Epoch 2
2024-12-11 18:44:01.019709: Current learning rate: 0.00998
2024-12-11 18:45:31.849273: Validation loss improved from -0.20433 to -0.29172! Patience: 0/50
2024-12-11 18:45:31.851044: train_loss -0.3012
2024-12-11 18:45:31.851947: val_loss -0.2917
2024-12-11 18:45:31.852788: Pseudo dice [0.6318]
2024-12-11 18:45:31.853651: Epoch time: 90.84 s
2024-12-11 18:45:31.854485: Yayy! New best EMA pseudo Dice: 0.5624
2024-12-11 18:45:33.571479: 
2024-12-11 18:45:33.573195: Epoch 3
2024-12-11 18:45:33.574246: Current learning rate: 0.00997
2024-12-11 18:47:04.000755: Validation loss improved from -0.29172 to -0.34894! Patience: 0/50
2024-12-11 18:47:04.001992: train_loss -0.3231
2024-12-11 18:47:04.003091: val_loss -0.3489
2024-12-11 18:47:04.004018: Pseudo dice [0.6661]
2024-12-11 18:47:04.004966: Epoch time: 90.43 s
2024-12-11 18:47:04.005832: Yayy! New best EMA pseudo Dice: 0.5728
2024-12-11 18:47:05.702490: 
2024-12-11 18:47:05.704457: Epoch 4
2024-12-11 18:47:05.705498: Current learning rate: 0.00996
2024-12-11 18:48:35.886640: Validation loss did not improve from -0.34894. Patience: 1/50
2024-12-11 18:48:35.887950: train_loss -0.3302
2024-12-11 18:48:35.889170: val_loss -0.2215
2024-12-11 18:48:35.889891: Pseudo dice [0.5655]
2024-12-11 18:48:35.890642: Epoch time: 90.19 s
2024-12-11 18:48:37.688378: 
2024-12-11 18:48:37.689449: Epoch 5
2024-12-11 18:48:37.690409: Current learning rate: 0.00995
2024-12-11 18:50:07.301483: Validation loss did not improve from -0.34894. Patience: 2/50
2024-12-11 18:50:07.302972: train_loss -0.3651
2024-12-11 18:50:07.303944: val_loss -0.2984
2024-12-11 18:50:07.304677: Pseudo dice [0.6261]
2024-12-11 18:50:07.305420: Epoch time: 89.62 s
2024-12-11 18:50:07.306157: Yayy! New best EMA pseudo Dice: 0.5775
2024-12-11 18:50:08.882547: 
2024-12-11 18:50:08.884091: Epoch 6
2024-12-11 18:50:08.885107: Current learning rate: 0.00995
2024-12-11 18:51:38.272283: Validation loss did not improve from -0.34894. Patience: 3/50
2024-12-11 18:51:38.273500: train_loss -0.4039
2024-12-11 18:51:38.274249: val_loss -0.3133
2024-12-11 18:51:38.274920: Pseudo dice [0.6342]
2024-12-11 18:51:38.275584: Epoch time: 89.39 s
2024-12-11 18:51:38.276273: Yayy! New best EMA pseudo Dice: 0.5832
2024-12-11 18:51:39.905695: 
2024-12-11 18:51:39.907284: Epoch 7
2024-12-11 18:51:39.908034: Current learning rate: 0.00994
2024-12-11 18:53:09.667799: Validation loss improved from -0.34894 to -0.39677! Patience: 3/50
2024-12-11 18:53:09.669066: train_loss -0.4279
2024-12-11 18:53:09.670005: val_loss -0.3968
2024-12-11 18:53:09.670858: Pseudo dice [0.6937]
2024-12-11 18:53:09.671653: Epoch time: 89.76 s
2024-12-11 18:53:09.672338: Yayy! New best EMA pseudo Dice: 0.5942
2024-12-11 18:53:11.316375: 
2024-12-11 18:53:11.318063: Epoch 8
2024-12-11 18:53:11.318985: Current learning rate: 0.00993
2024-12-11 18:54:41.483432: Validation loss improved from -0.39677 to -0.42466! Patience: 0/50
2024-12-11 18:54:41.484611: train_loss -0.4459
2024-12-11 18:54:41.485530: val_loss -0.4247
2024-12-11 18:54:41.486237: Pseudo dice [0.717]
2024-12-11 18:54:41.486923: Epoch time: 90.17 s
2024-12-11 18:54:41.487617: Yayy! New best EMA pseudo Dice: 0.6065
2024-12-11 18:54:43.577899: 
2024-12-11 18:54:43.579867: Epoch 9
2024-12-11 18:54:43.580628: Current learning rate: 0.00992
2024-12-11 18:56:13.538571: Validation loss improved from -0.42466 to -0.42952! Patience: 0/50
2024-12-11 18:56:13.539557: train_loss -0.4506
2024-12-11 18:56:13.540329: val_loss -0.4295
2024-12-11 18:56:13.541037: Pseudo dice [0.7234]
2024-12-11 18:56:13.541762: Epoch time: 89.96 s
2024-12-11 18:56:13.903643: Yayy! New best EMA pseudo Dice: 0.6182
2024-12-11 18:56:15.480322: 
2024-12-11 18:56:15.481836: Epoch 10
2024-12-11 18:56:15.482630: Current learning rate: 0.00991
2024-12-11 18:57:46.699755: Validation loss did not improve from -0.42952. Patience: 1/50
2024-12-11 18:57:46.700917: train_loss -0.4646
2024-12-11 18:57:46.701798: val_loss -0.35
2024-12-11 18:57:46.702497: Pseudo dice [0.6583]
2024-12-11 18:57:46.703442: Epoch time: 91.22 s
2024-12-11 18:57:46.704262: Yayy! New best EMA pseudo Dice: 0.6222
2024-12-11 18:57:48.304027: 
2024-12-11 18:57:48.305715: Epoch 11
2024-12-11 18:57:48.306463: Current learning rate: 0.0099
2024-12-11 18:59:19.527421: Validation loss did not improve from -0.42952. Patience: 2/50
2024-12-11 18:59:19.528216: train_loss -0.4661
2024-12-11 18:59:19.529202: val_loss -0.4292
2024-12-11 18:59:19.529869: Pseudo dice [0.7067]
2024-12-11 18:59:19.530572: Epoch time: 91.23 s
2024-12-11 18:59:19.531333: Yayy! New best EMA pseudo Dice: 0.6306
2024-12-11 18:59:21.167146: 
2024-12-11 18:59:21.168955: Epoch 12
2024-12-11 18:59:21.170076: Current learning rate: 0.00989
2024-12-11 19:00:51.706254: Validation loss did not improve from -0.42952. Patience: 3/50
2024-12-11 19:00:51.707918: train_loss -0.4778
2024-12-11 19:00:51.709202: val_loss -0.3978
2024-12-11 19:00:51.709967: Pseudo dice [0.7005]
2024-12-11 19:00:51.710880: Epoch time: 90.54 s
2024-12-11 19:00:51.711676: Yayy! New best EMA pseudo Dice: 0.6376
2024-12-11 19:00:53.318953: 
2024-12-11 19:00:53.320578: Epoch 13
2024-12-11 19:00:53.321383: Current learning rate: 0.00988
2024-12-11 19:02:23.647535: Validation loss improved from -0.42952 to -0.48305! Patience: 3/50
2024-12-11 19:02:23.648270: train_loss -0.4943
2024-12-11 19:02:23.649415: val_loss -0.483
2024-12-11 19:02:23.650501: Pseudo dice [0.7305]
2024-12-11 19:02:23.651650: Epoch time: 90.33 s
2024-12-11 19:02:23.652653: Yayy! New best EMA pseudo Dice: 0.6469
2024-12-11 19:02:25.261144: 
2024-12-11 19:02:25.262785: Epoch 14
2024-12-11 19:02:25.263605: Current learning rate: 0.00987
2024-12-11 19:03:55.817655: Validation loss did not improve from -0.48305. Patience: 1/50
2024-12-11 19:03:55.818768: train_loss -0.514
2024-12-11 19:03:55.819540: val_loss -0.4117
2024-12-11 19:03:55.820215: Pseudo dice [0.696]
2024-12-11 19:03:55.820864: Epoch time: 90.56 s
2024-12-11 19:03:56.183073: Yayy! New best EMA pseudo Dice: 0.6518
2024-12-11 19:03:57.798313: 
2024-12-11 19:03:57.799599: Epoch 15
2024-12-11 19:03:57.800668: Current learning rate: 0.00986
2024-12-11 19:05:28.702586: Validation loss improved from -0.48305 to -0.49729! Patience: 1/50
2024-12-11 19:05:28.703461: train_loss -0.4986
2024-12-11 19:05:28.704334: val_loss -0.4973
2024-12-11 19:05:28.705140: Pseudo dice [0.7403]
2024-12-11 19:05:28.705893: Epoch time: 90.91 s
2024-12-11 19:05:28.706644: Yayy! New best EMA pseudo Dice: 0.6607
2024-12-11 19:05:30.318954: 
2024-12-11 19:05:30.320262: Epoch 16
2024-12-11 19:05:30.321195: Current learning rate: 0.00986
2024-12-11 19:07:01.450791: Validation loss did not improve from -0.49729. Patience: 1/50
2024-12-11 19:07:01.451871: train_loss -0.5097
2024-12-11 19:07:01.453007: val_loss -0.4752
2024-12-11 19:07:01.453765: Pseudo dice [0.73]
2024-12-11 19:07:01.454475: Epoch time: 91.13 s
2024-12-11 19:07:01.455173: Yayy! New best EMA pseudo Dice: 0.6676
2024-12-11 19:07:03.122231: 
2024-12-11 19:07:03.123867: Epoch 17
2024-12-11 19:07:03.124922: Current learning rate: 0.00985
2024-12-11 19:08:34.110307: Validation loss did not improve from -0.49729. Patience: 2/50
2024-12-11 19:08:34.111499: train_loss -0.5099
2024-12-11 19:08:34.112484: val_loss -0.4803
2024-12-11 19:08:34.113347: Pseudo dice [0.7391]
2024-12-11 19:08:34.113997: Epoch time: 90.99 s
2024-12-11 19:08:34.114779: Yayy! New best EMA pseudo Dice: 0.6748
2024-12-11 19:08:35.830019: 
2024-12-11 19:08:35.831904: Epoch 18
2024-12-11 19:08:35.832983: Current learning rate: 0.00984
2024-12-11 19:10:05.415682: Validation loss did not improve from -0.49729. Patience: 3/50
2024-12-11 19:10:05.416720: train_loss -0.5072
2024-12-11 19:10:05.417651: val_loss -0.3436
2024-12-11 19:10:05.418300: Pseudo dice [0.6602]
2024-12-11 19:10:05.419001: Epoch time: 89.59 s
2024-12-11 19:10:06.730895: 
2024-12-11 19:10:06.732433: Epoch 19
2024-12-11 19:10:06.733240: Current learning rate: 0.00983
2024-12-11 19:11:36.733934: Validation loss did not improve from -0.49729. Patience: 4/50
2024-12-11 19:11:36.735161: train_loss -0.5301
2024-12-11 19:11:36.736065: val_loss -0.3805
2024-12-11 19:11:36.736760: Pseudo dice [0.6863]
2024-12-11 19:11:36.737465: Epoch time: 90.01 s
2024-12-11 19:11:38.355884: 
2024-12-11 19:11:38.357495: Epoch 20
2024-12-11 19:11:38.358322: Current learning rate: 0.00982
2024-12-11 19:13:08.128109: Validation loss did not improve from -0.49729. Patience: 5/50
2024-12-11 19:13:08.129166: train_loss -0.5158
2024-12-11 19:13:08.130153: val_loss -0.4597
2024-12-11 19:13:08.131019: Pseudo dice [0.7301]
2024-12-11 19:13:08.131856: Epoch time: 89.77 s
2024-12-11 19:13:08.132733: Yayy! New best EMA pseudo Dice: 0.6801
2024-12-11 19:13:09.769061: 
2024-12-11 19:13:09.770234: Epoch 21
2024-12-11 19:13:09.771396: Current learning rate: 0.00981
2024-12-11 19:14:39.544822: Validation loss did not improve from -0.49729. Patience: 6/50
2024-12-11 19:14:39.546125: train_loss -0.5308
2024-12-11 19:14:39.547269: val_loss -0.4504
2024-12-11 19:14:39.548169: Pseudo dice [0.7202]
2024-12-11 19:14:39.549176: Epoch time: 89.78 s
2024-12-11 19:14:39.549938: Yayy! New best EMA pseudo Dice: 0.6842
2024-12-11 19:14:41.096117: 
2024-12-11 19:14:41.098019: Epoch 22
2024-12-11 19:14:41.099486: Current learning rate: 0.0098
2024-12-11 19:16:10.841424: Validation loss did not improve from -0.49729. Patience: 7/50
2024-12-11 19:16:10.842352: train_loss -0.5442
2024-12-11 19:16:10.843407: val_loss -0.4851
2024-12-11 19:16:10.844610: Pseudo dice [0.7437]
2024-12-11 19:16:10.845422: Epoch time: 89.75 s
2024-12-11 19:16:10.846261: Yayy! New best EMA pseudo Dice: 0.6901
2024-12-11 19:16:12.427053: 
2024-12-11 19:16:12.429118: Epoch 23
2024-12-11 19:16:12.430318: Current learning rate: 0.00979
2024-12-11 19:17:42.196842: Validation loss improved from -0.49729 to -0.50170! Patience: 7/50
2024-12-11 19:17:42.197930: train_loss -0.5408
2024-12-11 19:17:42.198902: val_loss -0.5017
2024-12-11 19:17:42.199704: Pseudo dice [0.7483]
2024-12-11 19:17:42.200608: Epoch time: 89.77 s
2024-12-11 19:17:42.201460: Yayy! New best EMA pseudo Dice: 0.6959
2024-12-11 19:17:43.768631: 
2024-12-11 19:17:43.770758: Epoch 24
2024-12-11 19:17:43.771709: Current learning rate: 0.00978
2024-12-11 19:19:13.276253: Validation loss did not improve from -0.50170. Patience: 1/50
2024-12-11 19:19:13.277436: train_loss -0.5503
2024-12-11 19:19:13.278282: val_loss -0.4785
2024-12-11 19:19:13.279131: Pseudo dice [0.7307]
2024-12-11 19:19:13.279834: Epoch time: 89.51 s
2024-12-11 19:19:13.633463: Yayy! New best EMA pseudo Dice: 0.6994
2024-12-11 19:19:15.253999: 
2024-12-11 19:19:15.255867: Epoch 25
2024-12-11 19:19:15.257095: Current learning rate: 0.00977
2024-12-11 19:20:44.544812: Validation loss did not improve from -0.50170. Patience: 2/50
2024-12-11 19:20:44.545799: train_loss -0.5513
2024-12-11 19:20:44.546839: val_loss -0.5017
2024-12-11 19:20:44.547613: Pseudo dice [0.7485]
2024-12-11 19:20:44.548458: Epoch time: 89.29 s
2024-12-11 19:20:44.549122: Yayy! New best EMA pseudo Dice: 0.7043
2024-12-11 19:20:46.097064: 
2024-12-11 19:20:46.098441: Epoch 26
2024-12-11 19:20:46.099259: Current learning rate: 0.00977
2024-12-11 19:22:16.642581: Validation loss did not improve from -0.50170. Patience: 3/50
2024-12-11 19:22:16.643479: train_loss -0.5553
2024-12-11 19:22:16.644487: val_loss -0.4916
2024-12-11 19:22:16.645131: Pseudo dice [0.737]
2024-12-11 19:22:16.645899: Epoch time: 90.55 s
2024-12-11 19:22:16.646507: Yayy! New best EMA pseudo Dice: 0.7076
2024-12-11 19:22:18.234966: 
2024-12-11 19:22:18.236265: Epoch 27
2024-12-11 19:22:18.237414: Current learning rate: 0.00976
2024-12-11 19:23:48.744787: Validation loss improved from -0.50170 to -0.50806! Patience: 3/50
2024-12-11 19:23:48.745854: train_loss -0.5676
2024-12-11 19:23:48.747080: val_loss -0.5081
2024-12-11 19:23:48.748089: Pseudo dice [0.7606]
2024-12-11 19:23:48.749131: Epoch time: 90.51 s
2024-12-11 19:23:48.750123: Yayy! New best EMA pseudo Dice: 0.7129
2024-12-11 19:23:50.311923: 
2024-12-11 19:23:50.313366: Epoch 28
2024-12-11 19:23:50.314392: Current learning rate: 0.00975
2024-12-11 19:25:21.001702: Validation loss did not improve from -0.50806. Patience: 1/50
2024-12-11 19:25:21.002845: train_loss -0.5686
2024-12-11 19:25:21.003841: val_loss -0.5003
2024-12-11 19:25:21.004662: Pseudo dice [0.7445]
2024-12-11 19:25:21.005458: Epoch time: 90.69 s
2024-12-11 19:25:21.006138: Yayy! New best EMA pseudo Dice: 0.716
2024-12-11 19:25:22.631065: 
2024-12-11 19:25:22.632705: Epoch 29
2024-12-11 19:25:22.633683: Current learning rate: 0.00974
2024-12-11 19:26:53.172970: Validation loss did not improve from -0.50806. Patience: 2/50
2024-12-11 19:26:53.173995: train_loss -0.5847
2024-12-11 19:26:53.175215: val_loss -0.4945
2024-12-11 19:26:53.176292: Pseudo dice [0.7472]
2024-12-11 19:26:53.177041: Epoch time: 90.54 s
2024-12-11 19:26:53.532099: Yayy! New best EMA pseudo Dice: 0.7192
2024-12-11 19:26:55.482496: 
2024-12-11 19:26:55.484393: Epoch 30
2024-12-11 19:26:55.485565: Current learning rate: 0.00973
2024-12-11 19:28:25.431500: Validation loss improved from -0.50806 to -0.51298! Patience: 2/50
2024-12-11 19:28:25.432387: train_loss -0.5799
2024-12-11 19:28:25.433177: val_loss -0.513
2024-12-11 19:28:25.433767: Pseudo dice [0.7468]
2024-12-11 19:28:25.434538: Epoch time: 89.95 s
2024-12-11 19:28:25.435319: Yayy! New best EMA pseudo Dice: 0.7219
2024-12-11 19:28:27.003278: 
2024-12-11 19:28:27.005139: Epoch 31
2024-12-11 19:28:27.006169: Current learning rate: 0.00972
2024-12-11 19:29:56.987197: Validation loss improved from -0.51298 to -0.51829! Patience: 0/50
2024-12-11 19:29:56.988069: train_loss -0.5923
2024-12-11 19:29:56.989116: val_loss -0.5183
2024-12-11 19:29:56.989742: Pseudo dice [0.7568]
2024-12-11 19:29:56.990387: Epoch time: 89.99 s
2024-12-11 19:29:56.991190: Yayy! New best EMA pseudo Dice: 0.7254
2024-12-11 19:29:58.635678: 
2024-12-11 19:29:58.637071: Epoch 32
2024-12-11 19:29:58.638081: Current learning rate: 0.00971
2024-12-11 19:31:28.737378: Validation loss did not improve from -0.51829. Patience: 1/50
2024-12-11 19:31:28.738171: train_loss -0.5782
2024-12-11 19:31:28.738971: val_loss -0.5121
2024-12-11 19:31:28.739839: Pseudo dice [0.7474]
2024-12-11 19:31:28.740509: Epoch time: 90.1 s
2024-12-11 19:31:28.741278: Yayy! New best EMA pseudo Dice: 0.7276
2024-12-11 19:31:30.307409: 
2024-12-11 19:31:30.308859: Epoch 33
2024-12-11 19:31:30.310011: Current learning rate: 0.0097
2024-12-11 19:33:00.425004: Validation loss did not improve from -0.51829. Patience: 2/50
2024-12-11 19:33:00.426190: train_loss -0.5927
2024-12-11 19:33:00.427032: val_loss -0.498
2024-12-11 19:33:00.427776: Pseudo dice [0.7423]
2024-12-11 19:33:00.428496: Epoch time: 90.12 s
2024-12-11 19:33:00.429207: Yayy! New best EMA pseudo Dice: 0.7291
2024-12-11 19:33:02.054034: 
2024-12-11 19:33:02.055618: Epoch 34
2024-12-11 19:33:02.056644: Current learning rate: 0.00969
2024-12-11 19:34:32.353693: Validation loss did not improve from -0.51829. Patience: 3/50
2024-12-11 19:34:32.354544: train_loss -0.6025
2024-12-11 19:34:32.355386: val_loss -0.4782
2024-12-11 19:34:32.356188: Pseudo dice [0.7351]
2024-12-11 19:34:32.356964: Epoch time: 90.3 s
2024-12-11 19:34:32.711763: Yayy! New best EMA pseudo Dice: 0.7297
2024-12-11 19:34:34.297209: 
2024-12-11 19:34:34.298759: Epoch 35
2024-12-11 19:34:34.299586: Current learning rate: 0.00968
2024-12-11 19:36:04.728077: Validation loss did not improve from -0.51829. Patience: 4/50
2024-12-11 19:36:04.729284: train_loss -0.5883
2024-12-11 19:36:04.730781: val_loss -0.4934
2024-12-11 19:36:04.731818: Pseudo dice [0.7484]
2024-12-11 19:36:04.733168: Epoch time: 90.43 s
2024-12-11 19:36:04.734148: Yayy! New best EMA pseudo Dice: 0.7316
2024-12-11 19:36:06.317984: 
2024-12-11 19:36:06.319807: Epoch 36
2024-12-11 19:36:06.320882: Current learning rate: 0.00968
2024-12-11 19:37:36.553850: Validation loss did not improve from -0.51829. Patience: 5/50
2024-12-11 19:37:36.555059: train_loss -0.5943
2024-12-11 19:37:36.556257: val_loss -0.4477
2024-12-11 19:37:36.557029: Pseudo dice [0.7176]
2024-12-11 19:37:36.557689: Epoch time: 90.24 s
2024-12-11 19:37:37.891437: 
2024-12-11 19:37:37.893079: Epoch 37
2024-12-11 19:37:37.893889: Current learning rate: 0.00967
2024-12-11 19:39:08.359078: Validation loss did not improve from -0.51829. Patience: 6/50
2024-12-11 19:39:08.359772: train_loss -0.5911
2024-12-11 19:39:08.360508: val_loss -0.4957
2024-12-11 19:39:08.361151: Pseudo dice [0.754]
2024-12-11 19:39:08.361962: Epoch time: 90.47 s
2024-12-11 19:39:08.362640: Yayy! New best EMA pseudo Dice: 0.7325
2024-12-11 19:39:09.955558: 
2024-12-11 19:39:09.956884: Epoch 38
2024-12-11 19:39:09.957664: Current learning rate: 0.00966
2024-12-11 19:40:40.311643: Validation loss did not improve from -0.51829. Patience: 7/50
2024-12-11 19:40:40.312536: train_loss -0.6013
2024-12-11 19:40:40.313344: val_loss -0.5087
2024-12-11 19:40:40.314182: Pseudo dice [0.7549]
2024-12-11 19:40:40.314898: Epoch time: 90.36 s
2024-12-11 19:40:40.315651: Yayy! New best EMA pseudo Dice: 0.7348
2024-12-11 19:40:41.885834: 
2024-12-11 19:40:41.887680: Epoch 39
2024-12-11 19:40:41.888850: Current learning rate: 0.00965
2024-12-11 19:42:11.959261: Validation loss did not improve from -0.51829. Patience: 8/50
2024-12-11 19:42:11.960207: train_loss -0.5887
2024-12-11 19:42:11.961229: val_loss -0.5029
2024-12-11 19:42:11.962035: Pseudo dice [0.7523]
2024-12-11 19:42:11.962801: Epoch time: 90.08 s
2024-12-11 19:42:12.312183: Yayy! New best EMA pseudo Dice: 0.7365
2024-12-11 19:42:14.270675: 
2024-12-11 19:42:14.271981: Epoch 40
2024-12-11 19:42:14.272727: Current learning rate: 0.00964
2024-12-11 19:43:44.429151: Validation loss did not improve from -0.51829. Patience: 9/50
2024-12-11 19:43:44.430109: train_loss -0.6068
2024-12-11 19:43:44.430913: val_loss -0.4983
2024-12-11 19:43:44.431588: Pseudo dice [0.7431]
2024-12-11 19:43:44.432302: Epoch time: 90.16 s
2024-12-11 19:43:44.433014: Yayy! New best EMA pseudo Dice: 0.7372
2024-12-11 19:43:46.043665: 
2024-12-11 19:43:46.045377: Epoch 41
2024-12-11 19:43:46.046204: Current learning rate: 0.00963
2024-12-11 19:45:16.278534: Validation loss improved from -0.51829 to -0.52779! Patience: 9/50
2024-12-11 19:45:16.280948: train_loss -0.6179
2024-12-11 19:45:16.282450: val_loss -0.5278
2024-12-11 19:45:16.283274: Pseudo dice [0.763]
2024-12-11 19:45:16.284108: Epoch time: 90.24 s
2024-12-11 19:45:16.284789: Yayy! New best EMA pseudo Dice: 0.7398
2024-12-11 19:45:17.881079: 
2024-12-11 19:45:17.882673: Epoch 42
2024-12-11 19:45:17.883511: Current learning rate: 0.00962
2024-12-11 19:46:48.250292: Validation loss did not improve from -0.52779. Patience: 1/50
2024-12-11 19:46:48.251459: train_loss -0.6138
2024-12-11 19:46:48.252585: val_loss -0.4723
2024-12-11 19:46:48.253292: Pseudo dice [0.7298]
2024-12-11 19:46:48.253946: Epoch time: 90.37 s
2024-12-11 19:46:49.471686: 
2024-12-11 19:46:49.473677: Epoch 43
2024-12-11 19:46:49.474866: Current learning rate: 0.00961
2024-12-11 19:48:19.762006: Validation loss improved from -0.52779 to -0.54723! Patience: 1/50
2024-12-11 19:48:19.762796: train_loss -0.6138
2024-12-11 19:48:19.763601: val_loss -0.5472
2024-12-11 19:48:19.764378: Pseudo dice [0.7742]
2024-12-11 19:48:19.765172: Epoch time: 90.29 s
2024-12-11 19:48:19.765991: Yayy! New best EMA pseudo Dice: 0.7423
2024-12-11 19:48:21.302956: 
2024-12-11 19:48:21.304561: Epoch 44
2024-12-11 19:48:21.305573: Current learning rate: 0.0096
2024-12-11 19:49:51.808118: Validation loss did not improve from -0.54723. Patience: 1/50
2024-12-11 19:49:51.809127: train_loss -0.6138
2024-12-11 19:49:51.810017: val_loss -0.4984
2024-12-11 19:49:51.810783: Pseudo dice [0.7477]
2024-12-11 19:49:51.811510: Epoch time: 90.51 s
2024-12-11 19:49:52.155174: Yayy! New best EMA pseudo Dice: 0.7428
2024-12-11 19:49:53.692849: 
2024-12-11 19:49:53.694254: Epoch 45
2024-12-11 19:49:53.695044: Current learning rate: 0.00959
2024-12-11 19:51:23.886070: Validation loss did not improve from -0.54723. Patience: 2/50
2024-12-11 19:51:23.887039: train_loss -0.6162
2024-12-11 19:51:23.887814: val_loss -0.5389
2024-12-11 19:51:23.888542: Pseudo dice [0.77]
2024-12-11 19:51:23.889162: Epoch time: 90.2 s
2024-12-11 19:51:23.889820: Yayy! New best EMA pseudo Dice: 0.7456
2024-12-11 19:51:25.434831: 
2024-12-11 19:51:25.436236: Epoch 46
2024-12-11 19:51:25.436984: Current learning rate: 0.00959
2024-12-11 19:52:55.361439: Validation loss did not improve from -0.54723. Patience: 3/50
2024-12-11 19:52:55.362564: train_loss -0.6074
2024-12-11 19:52:55.363686: val_loss -0.481
2024-12-11 19:52:55.364660: Pseudo dice [0.7482]
2024-12-11 19:52:55.365633: Epoch time: 89.93 s
2024-12-11 19:52:55.366509: Yayy! New best EMA pseudo Dice: 0.7458
2024-12-11 19:52:56.923533: 
2024-12-11 19:52:56.924968: Epoch 47
2024-12-11 19:52:56.925879: Current learning rate: 0.00958
2024-12-11 19:54:26.579547: Validation loss did not improve from -0.54723. Patience: 4/50
2024-12-11 19:54:26.580660: train_loss -0.627
2024-12-11 19:54:26.581525: val_loss -0.5181
2024-12-11 19:54:26.582275: Pseudo dice [0.7568]
2024-12-11 19:54:26.582948: Epoch time: 89.66 s
2024-12-11 19:54:26.583497: Yayy! New best EMA pseudo Dice: 0.7469
2024-12-11 19:54:28.219072: 
2024-12-11 19:54:28.220614: Epoch 48
2024-12-11 19:54:28.221431: Current learning rate: 0.00957
2024-12-11 19:55:57.655804: Validation loss did not improve from -0.54723. Patience: 5/50
2024-12-11 19:55:57.656731: train_loss -0.6065
2024-12-11 19:55:57.657669: val_loss -0.5428
2024-12-11 19:55:57.658412: Pseudo dice [0.7734]
2024-12-11 19:55:57.659269: Epoch time: 89.44 s
2024-12-11 19:55:57.659942: Yayy! New best EMA pseudo Dice: 0.7496
2024-12-11 19:55:59.267152: 
2024-12-11 19:55:59.268731: Epoch 49
2024-12-11 19:55:59.269541: Current learning rate: 0.00956
2024-12-11 19:57:28.603641: Validation loss did not improve from -0.54723. Patience: 6/50
2024-12-11 19:57:28.604700: train_loss -0.6241
2024-12-11 19:57:28.605797: val_loss -0.4821
2024-12-11 19:57:28.606693: Pseudo dice [0.7412]
2024-12-11 19:57:28.607516: Epoch time: 89.34 s
2024-12-11 19:57:30.200440: 
2024-12-11 19:57:30.202163: Epoch 50
2024-12-11 19:57:30.203097: Current learning rate: 0.00955
2024-12-11 19:58:59.958785: Validation loss did not improve from -0.54723. Patience: 7/50
2024-12-11 19:58:59.961436: train_loss -0.6342
2024-12-11 19:58:59.962782: val_loss -0.5127
2024-12-11 19:58:59.963467: Pseudo dice [0.7486]
2024-12-11 19:58:59.964214: Epoch time: 89.76 s
2024-12-11 19:59:01.640384: 
2024-12-11 19:59:01.642186: Epoch 51
2024-12-11 19:59:01.643265: Current learning rate: 0.00954
2024-12-11 20:00:31.646733: Validation loss did not improve from -0.54723. Patience: 8/50
2024-12-11 20:00:31.647865: train_loss -0.6328
2024-12-11 20:00:31.648934: val_loss -0.4863
2024-12-11 20:00:31.649959: Pseudo dice [0.7393]
2024-12-11 20:00:31.650736: Epoch time: 90.01 s
2024-12-11 20:00:32.906509: 
2024-12-11 20:00:32.908156: Epoch 52
2024-12-11 20:00:32.909058: Current learning rate: 0.00953
2024-12-11 20:02:03.036863: Validation loss did not improve from -0.54723. Patience: 9/50
2024-12-11 20:02:03.037921: train_loss -0.6313
2024-12-11 20:02:03.038769: val_loss -0.4953
2024-12-11 20:02:03.039489: Pseudo dice [0.7445]
2024-12-11 20:02:03.040167: Epoch time: 90.13 s
2024-12-11 20:02:04.258048: 
2024-12-11 20:02:04.259413: Epoch 53
2024-12-11 20:02:04.260339: Current learning rate: 0.00952
2024-12-11 20:03:34.278081: Validation loss did not improve from -0.54723. Patience: 10/50
2024-12-11 20:03:34.279320: train_loss -0.6315
2024-12-11 20:03:34.280145: val_loss -0.5336
2024-12-11 20:03:34.281071: Pseudo dice [0.7701]
2024-12-11 20:03:34.281752: Epoch time: 90.02 s
2024-12-11 20:03:34.282351: Yayy! New best EMA pseudo Dice: 0.7497
2024-12-11 20:03:35.829478: 
2024-12-11 20:03:35.831074: Epoch 54
2024-12-11 20:03:35.831895: Current learning rate: 0.00951
2024-12-11 20:05:05.852956: Validation loss did not improve from -0.54723. Patience: 11/50
2024-12-11 20:05:05.853733: train_loss -0.6414
2024-12-11 20:05:05.854686: val_loss -0.5456
2024-12-11 20:05:05.855514: Pseudo dice [0.7771]
2024-12-11 20:05:05.856304: Epoch time: 90.03 s
2024-12-11 20:05:06.214558: Yayy! New best EMA pseudo Dice: 0.7525
2024-12-11 20:05:07.819354: 
2024-12-11 20:05:07.821172: Epoch 55
2024-12-11 20:05:07.822403: Current learning rate: 0.0095
2024-12-11 20:06:38.019612: Validation loss did not improve from -0.54723. Patience: 12/50
2024-12-11 20:06:38.021175: train_loss -0.6487
2024-12-11 20:06:38.022524: val_loss -0.4978
2024-12-11 20:06:38.023280: Pseudo dice [0.7516]
2024-12-11 20:06:38.024246: Epoch time: 90.2 s
2024-12-11 20:06:39.270334: 
2024-12-11 20:06:39.271829: Epoch 56
2024-12-11 20:06:39.272634: Current learning rate: 0.00949
2024-12-11 20:08:09.805530: Validation loss did not improve from -0.54723. Patience: 13/50
2024-12-11 20:08:09.806626: train_loss -0.6461
2024-12-11 20:08:09.807745: val_loss -0.524
2024-12-11 20:08:09.808638: Pseudo dice [0.7654]
2024-12-11 20:08:09.809437: Epoch time: 90.54 s
2024-12-11 20:08:09.810318: Yayy! New best EMA pseudo Dice: 0.7537
2024-12-11 20:08:11.445810: 
2024-12-11 20:08:11.447279: Epoch 57
2024-12-11 20:08:11.448492: Current learning rate: 0.00949
2024-12-11 20:09:42.054812: Validation loss improved from -0.54723 to -0.55877! Patience: 13/50
2024-12-11 20:09:42.055989: train_loss -0.646
2024-12-11 20:09:42.056998: val_loss -0.5588
2024-12-11 20:09:42.057795: Pseudo dice [0.7819]
2024-12-11 20:09:42.058767: Epoch time: 90.61 s
2024-12-11 20:09:42.059522: Yayy! New best EMA pseudo Dice: 0.7565
2024-12-11 20:09:43.602069: 
2024-12-11 20:09:43.604193: Epoch 58
2024-12-11 20:09:43.605101: Current learning rate: 0.00948
2024-12-11 20:11:14.214106: Validation loss did not improve from -0.55877. Patience: 1/50
2024-12-11 20:11:14.215131: train_loss -0.6506
2024-12-11 20:11:14.216062: val_loss -0.421
2024-12-11 20:11:14.216857: Pseudo dice [0.717]
2024-12-11 20:11:14.217999: Epoch time: 90.61 s
2024-12-11 20:11:15.442261: 
2024-12-11 20:11:15.443761: Epoch 59
2024-12-11 20:11:15.444596: Current learning rate: 0.00947
2024-12-11 20:12:46.108433: Validation loss did not improve from -0.55877. Patience: 2/50
2024-12-11 20:12:46.109570: train_loss -0.656
2024-12-11 20:12:46.110371: val_loss -0.4432
2024-12-11 20:12:46.111008: Pseudo dice [0.7148]
2024-12-11 20:12:46.111806: Epoch time: 90.67 s
2024-12-11 20:12:47.685702: 
2024-12-11 20:12:47.686848: Epoch 60
2024-12-11 20:12:47.687890: Current learning rate: 0.00946
2024-12-11 20:14:17.960547: Validation loss did not improve from -0.55877. Patience: 3/50
2024-12-11 20:14:17.961907: train_loss -0.66
2024-12-11 20:14:17.962751: val_loss -0.5249
2024-12-11 20:14:17.963552: Pseudo dice [0.77]
2024-12-11 20:14:17.964309: Epoch time: 90.28 s
2024-12-11 20:14:19.186549: 
2024-12-11 20:14:19.188292: Epoch 61
2024-12-11 20:14:19.189370: Current learning rate: 0.00945
2024-12-11 20:15:49.069570: Validation loss did not improve from -0.55877. Patience: 4/50
2024-12-11 20:15:49.070426: train_loss -0.6543
2024-12-11 20:15:49.071373: val_loss -0.5068
2024-12-11 20:15:49.072249: Pseudo dice [0.747]
2024-12-11 20:15:49.073527: Epoch time: 89.88 s
2024-12-11 20:15:50.632672: 
2024-12-11 20:15:50.634394: Epoch 62
2024-12-11 20:15:50.635534: Current learning rate: 0.00944
2024-12-11 20:17:20.140271: Validation loss did not improve from -0.55877. Patience: 5/50
2024-12-11 20:17:20.141386: train_loss -0.6668
2024-12-11 20:17:20.142282: val_loss -0.5118
2024-12-11 20:17:20.142983: Pseudo dice [0.7558]
2024-12-11 20:17:20.143805: Epoch time: 89.51 s
2024-12-11 20:17:21.418176: 
2024-12-11 20:17:21.419485: Epoch 63
2024-12-11 20:17:21.420523: Current learning rate: 0.00943
2024-12-11 20:18:50.548759: Validation loss did not improve from -0.55877. Patience: 6/50
2024-12-11 20:18:50.549724: train_loss -0.6637
2024-12-11 20:18:50.550955: val_loss -0.521
2024-12-11 20:18:50.551948: Pseudo dice [0.7598]
2024-12-11 20:18:50.552909: Epoch time: 89.13 s
2024-12-11 20:18:51.821627: 
2024-12-11 20:18:51.823208: Epoch 64
2024-12-11 20:18:51.824518: Current learning rate: 0.00942
2024-12-11 20:20:20.889018: Validation loss did not improve from -0.55877. Patience: 7/50
2024-12-11 20:20:20.889987: train_loss -0.6487
2024-12-11 20:20:20.891078: val_loss -0.4565
2024-12-11 20:20:20.891768: Pseudo dice [0.7316]
2024-12-11 20:20:20.892436: Epoch time: 89.07 s
2024-12-11 20:20:22.503138: 
2024-12-11 20:20:22.505286: Epoch 65
2024-12-11 20:20:22.506276: Current learning rate: 0.00941
2024-12-11 20:21:52.816476: Validation loss did not improve from -0.55877. Patience: 8/50
2024-12-11 20:21:52.817752: train_loss -0.6543
2024-12-11 20:21:52.818660: val_loss -0.5022
2024-12-11 20:21:52.819387: Pseudo dice [0.75]
2024-12-11 20:21:52.820064: Epoch time: 90.32 s
2024-12-11 20:21:54.072251: 
2024-12-11 20:21:54.074336: Epoch 66
2024-12-11 20:21:54.075276: Current learning rate: 0.0094
2024-12-11 20:23:24.335621: Validation loss did not improve from -0.55877. Patience: 9/50
2024-12-11 20:23:24.336370: train_loss -0.6658
2024-12-11 20:23:24.337740: val_loss -0.5457
2024-12-11 20:23:24.338819: Pseudo dice [0.7637]
2024-12-11 20:23:24.339870: Epoch time: 90.27 s
2024-12-11 20:23:25.638315: 
2024-12-11 20:23:25.640171: Epoch 67
2024-12-11 20:23:25.641136: Current learning rate: 0.00939
2024-12-11 20:24:55.793057: Validation loss did not improve from -0.55877. Patience: 10/50
2024-12-11 20:24:55.794130: train_loss -0.6677
2024-12-11 20:24:55.795033: val_loss -0.4791
2024-12-11 20:24:55.795823: Pseudo dice [0.7461]
2024-12-11 20:24:55.796584: Epoch time: 90.16 s
2024-12-11 20:24:57.056148: 
2024-12-11 20:24:57.057867: Epoch 68
2024-12-11 20:24:57.058818: Current learning rate: 0.00939
2024-12-11 20:26:27.199235: Validation loss did not improve from -0.55877. Patience: 11/50
2024-12-11 20:26:27.200330: train_loss -0.6704
2024-12-11 20:26:27.201283: val_loss -0.5165
2024-12-11 20:26:27.201931: Pseudo dice [0.7645]
2024-12-11 20:26:27.202570: Epoch time: 90.15 s
2024-12-11 20:26:28.497917: 
2024-12-11 20:26:28.499522: Epoch 69
2024-12-11 20:26:28.500422: Current learning rate: 0.00938
2024-12-11 20:27:58.634067: Validation loss did not improve from -0.55877. Patience: 12/50
2024-12-11 20:27:58.635290: train_loss -0.6748
2024-12-11 20:27:58.636189: val_loss -0.5088
2024-12-11 20:27:58.636977: Pseudo dice [0.7668]
2024-12-11 20:27:58.637640: Epoch time: 90.14 s
2024-12-11 20:28:00.303585: 
2024-12-11 20:28:00.305475: Epoch 70
2024-12-11 20:28:00.306425: Current learning rate: 0.00937
2024-12-11 20:29:30.405832: Validation loss did not improve from -0.55877. Patience: 13/50
2024-12-11 20:29:30.406929: train_loss -0.67
2024-12-11 20:29:30.408041: val_loss -0.4334
2024-12-11 20:29:30.409061: Pseudo dice [0.7235]
2024-12-11 20:29:30.409938: Epoch time: 90.1 s
2024-12-11 20:29:31.753071: 
2024-12-11 20:29:31.755642: Epoch 71
2024-12-11 20:29:31.756611: Current learning rate: 0.00936
2024-12-11 20:31:01.807204: Validation loss did not improve from -0.55877. Patience: 14/50
2024-12-11 20:31:01.808821: train_loss -0.6745
2024-12-11 20:31:01.809955: val_loss -0.5267
2024-12-11 20:31:01.810650: Pseudo dice [0.7674]
2024-12-11 20:31:01.811403: Epoch time: 90.06 s
2024-12-11 20:31:03.071874: 
2024-12-11 20:31:03.073829: Epoch 72
2024-12-11 20:31:03.074857: Current learning rate: 0.00935
2024-12-11 20:32:33.213330: Validation loss did not improve from -0.55877. Patience: 15/50
2024-12-11 20:32:33.214721: train_loss -0.6762
2024-12-11 20:32:33.216040: val_loss -0.5121
2024-12-11 20:32:33.217029: Pseudo dice [0.7495]
2024-12-11 20:32:33.218034: Epoch time: 90.14 s
2024-12-11 20:32:34.807904: 
2024-12-11 20:32:34.809723: Epoch 73
2024-12-11 20:32:34.810821: Current learning rate: 0.00934
2024-12-11 20:34:05.203038: Validation loss did not improve from -0.55877. Patience: 16/50
2024-12-11 20:34:05.203864: train_loss -0.677
2024-12-11 20:34:05.205097: val_loss -0.484
2024-12-11 20:34:05.205960: Pseudo dice [0.7528]
2024-12-11 20:34:05.206974: Epoch time: 90.4 s
2024-12-11 20:34:06.486630: 
2024-12-11 20:34:06.488885: Epoch 74
2024-12-11 20:34:06.489685: Current learning rate: 0.00933
2024-12-11 20:35:36.648279: Validation loss improved from -0.55877 to -0.57610! Patience: 16/50
2024-12-11 20:35:36.649556: train_loss -0.6766
2024-12-11 20:35:36.650745: val_loss -0.5761
2024-12-11 20:35:36.651740: Pseudo dice [0.7951]
2024-12-11 20:35:36.652555: Epoch time: 90.16 s
2024-12-11 20:35:38.312606: 
2024-12-11 20:35:38.314760: Epoch 75
2024-12-11 20:35:38.315840: Current learning rate: 0.00932
2024-12-11 20:37:08.137819: Validation loss did not improve from -0.57610. Patience: 1/50
2024-12-11 20:37:08.139202: train_loss -0.6838
2024-12-11 20:37:08.140336: val_loss -0.5107
2024-12-11 20:37:08.141168: Pseudo dice [0.768]
2024-12-11 20:37:08.142085: Epoch time: 89.83 s
2024-12-11 20:37:08.142827: Yayy! New best EMA pseudo Dice: 0.7575
2024-12-11 20:37:09.783180: 
2024-12-11 20:37:09.784899: Epoch 76
2024-12-11 20:37:09.785820: Current learning rate: 0.00931
2024-12-11 20:38:39.606940: Validation loss did not improve from -0.57610. Patience: 2/50
2024-12-11 20:38:39.608416: train_loss -0.6658
2024-12-11 20:38:39.609546: val_loss -0.521
2024-12-11 20:38:39.610285: Pseudo dice [0.7577]
2024-12-11 20:38:39.611065: Epoch time: 89.83 s
2024-12-11 20:38:39.611793: Yayy! New best EMA pseudo Dice: 0.7576
2024-12-11 20:38:41.264498: 
2024-12-11 20:38:41.266199: Epoch 77
2024-12-11 20:38:41.267032: Current learning rate: 0.0093
2024-12-11 20:40:10.956664: Validation loss did not improve from -0.57610. Patience: 3/50
2024-12-11 20:40:10.957853: train_loss -0.6682
2024-12-11 20:40:10.958972: val_loss -0.542
2024-12-11 20:40:10.960067: Pseudo dice [0.7754]
2024-12-11 20:40:10.961099: Epoch time: 89.69 s
2024-12-11 20:40:10.962122: Yayy! New best EMA pseudo Dice: 0.7593
2024-12-11 20:40:12.662713: 
2024-12-11 20:40:12.665003: Epoch 78
2024-12-11 20:40:12.666420: Current learning rate: 0.0093
2024-12-11 20:41:42.299389: Validation loss did not improve from -0.57610. Patience: 4/50
2024-12-11 20:41:42.300622: train_loss -0.6799
2024-12-11 20:41:42.301570: val_loss -0.4892
2024-12-11 20:41:42.302623: Pseudo dice [0.7469]
2024-12-11 20:41:42.303715: Epoch time: 89.64 s
2024-12-11 20:41:43.647456: 
2024-12-11 20:41:43.649297: Epoch 79
2024-12-11 20:41:43.650375: Current learning rate: 0.00929
2024-12-11 20:43:13.157979: Validation loss did not improve from -0.57610. Patience: 5/50
2024-12-11 20:43:13.159284: train_loss -0.6916
2024-12-11 20:43:13.160129: val_loss -0.5289
2024-12-11 20:43:13.160904: Pseudo dice [0.7648]
2024-12-11 20:43:13.161736: Epoch time: 89.51 s
2024-12-11 20:43:14.791751: 
2024-12-11 20:43:14.793548: Epoch 80
2024-12-11 20:43:14.794617: Current learning rate: 0.00928
2024-12-11 20:44:44.154349: Validation loss did not improve from -0.57610. Patience: 6/50
2024-12-11 20:44:44.155439: train_loss -0.6921
2024-12-11 20:44:44.156374: val_loss -0.5186
2024-12-11 20:44:44.157016: Pseudo dice [0.7699]
2024-12-11 20:44:44.157809: Epoch time: 89.36 s
2024-12-11 20:44:44.158854: Yayy! New best EMA pseudo Dice: 0.7599
2024-12-11 20:44:45.802444: 
2024-12-11 20:44:45.803799: Epoch 81
2024-12-11 20:44:45.804811: Current learning rate: 0.00927
2024-12-11 20:46:15.147640: Validation loss did not improve from -0.57610. Patience: 7/50
2024-12-11 20:46:15.149153: train_loss -0.6808
2024-12-11 20:46:15.150363: val_loss -0.519
2024-12-11 20:46:15.151061: Pseudo dice [0.7578]
2024-12-11 20:46:15.151867: Epoch time: 89.35 s
2024-12-11 20:46:16.444081: 
2024-12-11 20:46:16.446362: Epoch 82
2024-12-11 20:46:16.447406: Current learning rate: 0.00926
2024-12-11 20:47:46.003684: Validation loss did not improve from -0.57610. Patience: 8/50
2024-12-11 20:47:46.004622: train_loss -0.6863
2024-12-11 20:47:46.005587: val_loss -0.5596
2024-12-11 20:47:46.006377: Pseudo dice [0.7808]
2024-12-11 20:47:46.007243: Epoch time: 89.56 s
2024-12-11 20:47:46.008009: Yayy! New best EMA pseudo Dice: 0.7618
2024-12-11 20:47:47.930503: 
2024-12-11 20:47:47.932274: Epoch 83
2024-12-11 20:47:47.933428: Current learning rate: 0.00925
2024-12-11 20:49:17.639334: Validation loss did not improve from -0.57610. Patience: 9/50
2024-12-11 20:49:17.641196: train_loss -0.6883
2024-12-11 20:49:17.642188: val_loss -0.5159
2024-12-11 20:49:17.642974: Pseudo dice [0.7645]
2024-12-11 20:49:17.643734: Epoch time: 89.71 s
2024-12-11 20:49:17.644359: Yayy! New best EMA pseudo Dice: 0.7621
2024-12-11 20:49:19.312150: 
2024-12-11 20:49:19.313812: Epoch 84
2024-12-11 20:49:19.314891: Current learning rate: 0.00924
2024-12-11 20:50:48.926880: Validation loss did not improve from -0.57610. Patience: 10/50
2024-12-11 20:50:48.930985: train_loss -0.6896
2024-12-11 20:50:48.933543: val_loss -0.5092
2024-12-11 20:50:48.934377: Pseudo dice [0.7483]
2024-12-11 20:50:48.935720: Epoch time: 89.62 s
2024-12-11 20:50:50.584627: 
2024-12-11 20:50:50.586318: Epoch 85
2024-12-11 20:50:50.587108: Current learning rate: 0.00923
2024-12-11 20:52:19.875551: Validation loss did not improve from -0.57610. Patience: 11/50
2024-12-11 20:52:19.876663: train_loss -0.6892
2024-12-11 20:52:19.877464: val_loss -0.5152
2024-12-11 20:52:19.878155: Pseudo dice [0.7448]
2024-12-11 20:52:19.878931: Epoch time: 89.29 s
2024-12-11 20:52:21.102199: 
2024-12-11 20:52:21.104384: Epoch 86
2024-12-11 20:52:21.105220: Current learning rate: 0.00922
2024-12-11 20:53:50.412924: Validation loss did not improve from -0.57610. Patience: 12/50
2024-12-11 20:53:50.414012: train_loss -0.6871
2024-12-11 20:53:50.415018: val_loss -0.5157
2024-12-11 20:53:50.416005: Pseudo dice [0.7696]
2024-12-11 20:53:50.417174: Epoch time: 89.31 s
2024-12-11 20:53:51.718816: 
2024-12-11 20:53:51.720745: Epoch 87
2024-12-11 20:53:51.721812: Current learning rate: 0.00921
2024-12-11 20:55:20.872637: Validation loss did not improve from -0.57610. Patience: 13/50
2024-12-11 20:55:20.873715: train_loss -0.6954
2024-12-11 20:55:20.874841: val_loss -0.4998
2024-12-11 20:55:20.875717: Pseudo dice [0.7581]
2024-12-11 20:55:20.876613: Epoch time: 89.16 s
2024-12-11 20:55:22.086998: 
2024-12-11 20:55:22.088613: Epoch 88
2024-12-11 20:55:22.089442: Current learning rate: 0.0092
2024-12-11 20:56:51.341341: Validation loss did not improve from -0.57610. Patience: 14/50
2024-12-11 20:56:51.342296: train_loss -0.7024
2024-12-11 20:56:51.343202: val_loss -0.4775
2024-12-11 20:56:51.344216: Pseudo dice [0.7458]
2024-12-11 20:56:51.345114: Epoch time: 89.26 s
2024-12-11 20:56:52.581367: 
2024-12-11 20:56:52.583275: Epoch 89
2024-12-11 20:56:52.584091: Current learning rate: 0.0092
2024-12-11 20:58:21.931049: Validation loss did not improve from -0.57610. Patience: 15/50
2024-12-11 20:58:21.931944: train_loss -0.7005
2024-12-11 20:58:21.932759: val_loss -0.5273
2024-12-11 20:58:21.933476: Pseudo dice [0.7646]
2024-12-11 20:58:21.934216: Epoch time: 89.35 s
2024-12-11 20:58:23.512123: 
2024-12-11 20:58:23.513921: Epoch 90
2024-12-11 20:58:23.515115: Current learning rate: 0.00919
2024-12-11 20:59:53.084589: Validation loss improved from -0.57610 to -0.57663! Patience: 15/50
2024-12-11 20:59:53.085894: train_loss -0.6933
2024-12-11 20:59:53.087095: val_loss -0.5766
2024-12-11 20:59:53.088027: Pseudo dice [0.7895]
2024-12-11 20:59:53.088820: Epoch time: 89.57 s
2024-12-11 20:59:53.089567: Yayy! New best EMA pseudo Dice: 0.7622
2024-12-11 20:59:54.690385: 
2024-12-11 20:59:54.692584: Epoch 91
2024-12-11 20:59:54.694065: Current learning rate: 0.00918
2024-12-11 21:01:24.447861: Validation loss did not improve from -0.57663. Patience: 1/50
2024-12-11 21:01:24.449023: train_loss -0.6893
2024-12-11 21:01:24.450061: val_loss -0.5011
2024-12-11 21:01:24.451151: Pseudo dice [0.7634]
2024-12-11 21:01:24.452068: Epoch time: 89.76 s
2024-12-11 21:01:24.453017: Yayy! New best EMA pseudo Dice: 0.7623
2024-12-11 21:01:25.998184: 
2024-12-11 21:01:25.999668: Epoch 92
2024-12-11 21:01:26.000664: Current learning rate: 0.00917
2024-12-11 21:02:55.920713: Validation loss did not improve from -0.57663. Patience: 2/50
2024-12-11 21:02:55.922652: train_loss -0.6879
2024-12-11 21:02:55.923866: val_loss -0.4784
2024-12-11 21:02:55.924605: Pseudo dice [0.7453]
2024-12-11 21:02:55.925425: Epoch time: 89.93 s
2024-12-11 21:02:57.149980: 
2024-12-11 21:02:57.151387: Epoch 93
2024-12-11 21:02:57.152089: Current learning rate: 0.00916
2024-12-11 21:04:26.918594: Validation loss did not improve from -0.57663. Patience: 3/50
2024-12-11 21:04:26.919803: train_loss -0.6895
2024-12-11 21:04:26.920874: val_loss -0.4779
2024-12-11 21:04:26.921779: Pseudo dice [0.7422]
2024-12-11 21:04:26.922676: Epoch time: 89.77 s
2024-12-11 21:04:28.491043: 
2024-12-11 21:04:28.492647: Epoch 94
2024-12-11 21:04:28.493737: Current learning rate: 0.00915
2024-12-11 21:05:58.322776: Validation loss did not improve from -0.57663. Patience: 4/50
2024-12-11 21:05:58.323644: train_loss -0.6979
2024-12-11 21:05:58.324519: val_loss -0.4936
2024-12-11 21:05:58.325669: Pseudo dice [0.7469]
2024-12-11 21:05:58.326938: Epoch time: 89.83 s
2024-12-11 21:05:59.852945: 
2024-12-11 21:05:59.854635: Epoch 95
2024-12-11 21:05:59.855883: Current learning rate: 0.00914
2024-12-11 21:07:29.803080: Validation loss did not improve from -0.57663. Patience: 5/50
2024-12-11 21:07:29.804191: train_loss -0.6995
2024-12-11 21:07:29.805039: val_loss -0.5434
2024-12-11 21:07:29.805892: Pseudo dice [0.7776]
2024-12-11 21:07:29.806542: Epoch time: 89.95 s
2024-12-11 21:07:31.001094: 
2024-12-11 21:07:31.002738: Epoch 96
2024-12-11 21:07:31.003929: Current learning rate: 0.00913
2024-12-11 21:09:01.090147: Validation loss did not improve from -0.57663. Patience: 6/50
2024-12-11 21:09:01.091438: train_loss -0.7014
2024-12-11 21:09:01.092870: val_loss -0.5426
2024-12-11 21:09:01.093953: Pseudo dice [0.7755]
2024-12-11 21:09:01.095124: Epoch time: 90.09 s
2024-12-11 21:09:02.359791: 
2024-12-11 21:09:02.361349: Epoch 97
2024-12-11 21:09:02.362598: Current learning rate: 0.00912
2024-12-11 21:10:32.424543: Validation loss did not improve from -0.57663. Patience: 7/50
2024-12-11 21:10:32.425694: train_loss -0.7096
2024-12-11 21:10:32.426705: val_loss -0.4706
2024-12-11 21:10:32.427696: Pseudo dice [0.7387]
2024-12-11 21:10:32.428584: Epoch time: 90.07 s
2024-12-11 21:10:33.650931: 
2024-12-11 21:10:33.652750: Epoch 98
2024-12-11 21:10:33.653747: Current learning rate: 0.00911
2024-12-11 21:12:03.480632: Validation loss did not improve from -0.57663. Patience: 8/50
2024-12-11 21:12:03.481758: train_loss -0.7037
2024-12-11 21:12:03.482643: val_loss -0.4144
2024-12-11 21:12:03.483380: Pseudo dice [0.7168]
2024-12-11 21:12:03.484087: Epoch time: 89.83 s
2024-12-11 21:12:04.689081: 
2024-12-11 21:12:04.690754: Epoch 99
2024-12-11 21:12:04.691687: Current learning rate: 0.0091
2024-12-11 21:13:34.402296: Validation loss did not improve from -0.57663. Patience: 9/50
2024-12-11 21:13:34.403170: train_loss -0.7153
2024-12-11 21:13:34.403923: val_loss -0.5142
2024-12-11 21:13:34.404550: Pseudo dice [0.7655]
2024-12-11 21:13:34.405322: Epoch time: 89.72 s
2024-12-11 21:13:35.998118: 
2024-12-11 21:13:36.000092: Epoch 100
2024-12-11 21:13:36.001401: Current learning rate: 0.0091
2024-12-11 21:15:05.847921: Validation loss did not improve from -0.57663. Patience: 10/50
2024-12-11 21:15:05.849184: train_loss -0.7184
2024-12-11 21:15:05.850119: val_loss -0.5207
2024-12-11 21:15:05.850830: Pseudo dice [0.762]
2024-12-11 21:15:05.851499: Epoch time: 89.85 s
2024-12-11 21:15:07.069804: 
2024-12-11 21:15:07.071720: Epoch 101
2024-12-11 21:15:07.072483: Current learning rate: 0.00909
2024-12-11 21:16:37.148371: Validation loss did not improve from -0.57663. Patience: 11/50
2024-12-11 21:16:37.149204: train_loss -0.7176
2024-12-11 21:16:37.150513: val_loss -0.4512
2024-12-11 21:16:37.151658: Pseudo dice [0.7318]
2024-12-11 21:16:37.152630: Epoch time: 90.08 s
2024-12-11 21:16:38.334134: 
2024-12-11 21:16:38.336176: Epoch 102
2024-12-11 21:16:38.337394: Current learning rate: 0.00908
2024-12-11 21:18:08.241992: Validation loss did not improve from -0.57663. Patience: 12/50
2024-12-11 21:18:08.243287: train_loss -0.716
2024-12-11 21:18:08.244687: val_loss -0.5582
2024-12-11 21:18:08.245689: Pseudo dice [0.7849]
2024-12-11 21:18:08.246727: Epoch time: 89.91 s
2024-12-11 21:18:09.451017: 
2024-12-11 21:18:09.453083: Epoch 103
2024-12-11 21:18:09.453972: Current learning rate: 0.00907
2024-12-11 21:19:39.247861: Validation loss did not improve from -0.57663. Patience: 13/50
2024-12-11 21:19:39.248650: train_loss -0.719
2024-12-11 21:19:39.249655: val_loss -0.5075
2024-12-11 21:19:39.250568: Pseudo dice [0.7544]
2024-12-11 21:19:39.251677: Epoch time: 89.8 s
2024-12-11 21:19:40.482100: 
2024-12-11 21:19:40.483901: Epoch 104
2024-12-11 21:19:40.484923: Current learning rate: 0.00906
2024-12-11 21:21:10.380297: Validation loss did not improve from -0.57663. Patience: 14/50
2024-12-11 21:21:10.381412: train_loss -0.7133
2024-12-11 21:21:10.382296: val_loss -0.5224
2024-12-11 21:21:10.383152: Pseudo dice [0.7622]
2024-12-11 21:21:10.384074: Epoch time: 89.9 s
2024-12-11 21:21:12.265621: 
2024-12-11 21:21:12.267407: Epoch 105
2024-12-11 21:21:12.268357: Current learning rate: 0.00905
2024-12-11 21:22:42.044770: Validation loss did not improve from -0.57663. Patience: 15/50
2024-12-11 21:22:42.045877: train_loss -0.7162
2024-12-11 21:22:42.046769: val_loss -0.5212
2024-12-11 21:22:42.047685: Pseudo dice [0.7562]
2024-12-11 21:22:42.048501: Epoch time: 89.78 s
2024-12-11 21:22:43.270410: 
2024-12-11 21:22:43.272660: Epoch 106
2024-12-11 21:22:43.273599: Current learning rate: 0.00904
2024-12-11 21:24:12.854121: Validation loss did not improve from -0.57663. Patience: 16/50
2024-12-11 21:24:12.855180: train_loss -0.7015
2024-12-11 21:24:12.856027: val_loss -0.53
2024-12-11 21:24:12.856714: Pseudo dice [0.7596]
2024-12-11 21:24:12.857409: Epoch time: 89.59 s
2024-12-11 21:24:14.141862: 
2024-12-11 21:24:14.143640: Epoch 107
2024-12-11 21:24:14.144518: Current learning rate: 0.00903
2024-12-11 21:25:43.465788: Validation loss did not improve from -0.57663. Patience: 17/50
2024-12-11 21:25:43.467165: train_loss -0.704
2024-12-11 21:25:43.467916: val_loss -0.4857
2024-12-11 21:25:43.468588: Pseudo dice [0.7586]
2024-12-11 21:25:43.469361: Epoch time: 89.33 s
2024-12-11 21:25:44.714632: 
2024-12-11 21:25:44.716099: Epoch 108
2024-12-11 21:25:44.716772: Current learning rate: 0.00902
2024-12-11 21:27:13.837198: Validation loss did not improve from -0.57663. Patience: 18/50
2024-12-11 21:27:13.838383: train_loss -0.718
2024-12-11 21:27:13.839447: val_loss -0.4981
2024-12-11 21:27:13.840148: Pseudo dice [0.7607]
2024-12-11 21:27:13.841007: Epoch time: 89.12 s
2024-12-11 21:27:15.085059: 
2024-12-11 21:27:15.086890: Epoch 109
2024-12-11 21:27:15.087963: Current learning rate: 0.00901
2024-12-11 21:28:44.221133: Validation loss did not improve from -0.57663. Patience: 19/50
2024-12-11 21:28:44.222008: train_loss -0.712
2024-12-11 21:28:44.222923: val_loss -0.5417
2024-12-11 21:28:44.223666: Pseudo dice [0.7807]
2024-12-11 21:28:44.224536: Epoch time: 89.14 s
2024-12-11 21:28:45.830760: 
2024-12-11 21:28:45.832691: Epoch 110
2024-12-11 21:28:45.833826: Current learning rate: 0.009
2024-12-11 21:30:14.960607: Validation loss did not improve from -0.57663. Patience: 20/50
2024-12-11 21:30:14.961903: train_loss -0.7223
2024-12-11 21:30:14.963111: val_loss -0.5516
2024-12-11 21:30:14.964060: Pseudo dice [0.7769]
2024-12-11 21:30:14.964934: Epoch time: 89.13 s
2024-12-11 21:30:16.228605: 
2024-12-11 21:30:16.230144: Epoch 111
2024-12-11 21:30:16.231095: Current learning rate: 0.009
2024-12-11 21:31:45.547250: Validation loss did not improve from -0.57663. Patience: 21/50
2024-12-11 21:31:45.549202: train_loss -0.7215
2024-12-11 21:31:45.550543: val_loss -0.481
2024-12-11 21:31:45.551603: Pseudo dice [0.7497]
2024-12-11 21:31:45.552439: Epoch time: 89.32 s
2024-12-11 21:31:46.772488: 
2024-12-11 21:31:46.774312: Epoch 112
2024-12-11 21:31:46.775038: Current learning rate: 0.00899
2024-12-11 21:33:16.135344: Validation loss did not improve from -0.57663. Patience: 22/50
2024-12-11 21:33:16.136376: train_loss -0.7264
2024-12-11 21:33:16.137773: val_loss -0.5119
2024-12-11 21:33:16.138567: Pseudo dice [0.7652]
2024-12-11 21:33:16.139305: Epoch time: 89.37 s
2024-12-11 21:33:17.370376: 
2024-12-11 21:33:17.371972: Epoch 113
2024-12-11 21:33:17.372775: Current learning rate: 0.00898
2024-12-11 21:34:46.797401: Validation loss did not improve from -0.57663. Patience: 23/50
2024-12-11 21:34:46.798262: train_loss -0.7236
2024-12-11 21:34:46.798997: val_loss -0.4588
2024-12-11 21:34:46.799773: Pseudo dice [0.721]
2024-12-11 21:34:46.800525: Epoch time: 89.43 s
2024-12-11 21:34:47.997189: 
2024-12-11 21:34:47.998862: Epoch 114
2024-12-11 21:34:47.999601: Current learning rate: 0.00897
2024-12-11 21:36:17.383516: Validation loss did not improve from -0.57663. Patience: 24/50
2024-12-11 21:36:17.384588: train_loss -0.7169
2024-12-11 21:36:17.385495: val_loss -0.4428
2024-12-11 21:36:17.386201: Pseudo dice [0.7349]
2024-12-11 21:36:17.386855: Epoch time: 89.39 s
2024-12-11 21:36:18.925595: 
2024-12-11 21:36:18.926787: Epoch 115
2024-12-11 21:36:18.927527: Current learning rate: 0.00896
2024-12-11 21:37:48.100323: Validation loss did not improve from -0.57663. Patience: 25/50
2024-12-11 21:37:48.101415: train_loss -0.7261
2024-12-11 21:37:48.102248: val_loss -0.5216
2024-12-11 21:37:48.103131: Pseudo dice [0.7624]
2024-12-11 21:37:48.103916: Epoch time: 89.18 s
2024-12-11 21:37:49.692450: 
2024-12-11 21:37:49.693849: Epoch 116
2024-12-11 21:37:49.694625: Current learning rate: 0.00895
2024-12-11 21:39:19.107276: Validation loss did not improve from -0.57663. Patience: 26/50
2024-12-11 21:39:19.108531: train_loss -0.7228
2024-12-11 21:39:19.109857: val_loss -0.5049
2024-12-11 21:39:19.110852: Pseudo dice [0.7569]
2024-12-11 21:39:19.111752: Epoch time: 89.42 s
2024-12-11 21:39:20.369912: 
2024-12-11 21:39:20.371752: Epoch 117
2024-12-11 21:39:20.372869: Current learning rate: 0.00894
2024-12-11 21:40:49.991147: Validation loss did not improve from -0.57663. Patience: 27/50
2024-12-11 21:40:49.992192: train_loss -0.7251
2024-12-11 21:40:49.993129: val_loss -0.5359
2024-12-11 21:40:49.994123: Pseudo dice [0.7659]
2024-12-11 21:40:49.995283: Epoch time: 89.62 s
2024-12-11 21:40:51.297440: 
2024-12-11 21:40:51.299188: Epoch 118
2024-12-11 21:40:51.300282: Current learning rate: 0.00893
2024-12-11 21:42:20.910700: Validation loss did not improve from -0.57663. Patience: 28/50
2024-12-11 21:42:20.912039: train_loss -0.7204
2024-12-11 21:42:20.913156: val_loss -0.4854
2024-12-11 21:42:20.913898: Pseudo dice [0.7366]
2024-12-11 21:42:20.914628: Epoch time: 89.62 s
2024-12-11 21:42:22.190981: 
2024-12-11 21:42:22.192638: Epoch 119
2024-12-11 21:42:22.193422: Current learning rate: 0.00892
2024-12-11 21:43:51.676208: Validation loss did not improve from -0.57663. Patience: 29/50
2024-12-11 21:43:51.677334: train_loss -0.7142
2024-12-11 21:43:51.678265: val_loss -0.5308
2024-12-11 21:43:51.679081: Pseudo dice [0.775]
2024-12-11 21:43:51.679882: Epoch time: 89.49 s
2024-12-11 21:43:53.299933: 
2024-12-11 21:43:53.301216: Epoch 120
2024-12-11 21:43:53.302120: Current learning rate: 0.00891
2024-12-11 21:45:22.653289: Validation loss did not improve from -0.57663. Patience: 30/50
2024-12-11 21:45:22.654314: train_loss -0.7199
2024-12-11 21:45:22.655114: val_loss -0.5169
2024-12-11 21:45:22.655859: Pseudo dice [0.776]
2024-12-11 21:45:22.656679: Epoch time: 89.36 s
2024-12-11 21:45:23.903304: 
2024-12-11 21:45:23.905101: Epoch 121
2024-12-11 21:45:23.905915: Current learning rate: 0.0089
2024-12-11 21:46:53.164110: Validation loss did not improve from -0.57663. Patience: 31/50
2024-12-11 21:46:53.165099: train_loss -0.719
2024-12-11 21:46:53.166322: val_loss -0.4355
2024-12-11 21:46:53.167264: Pseudo dice [0.729]
2024-12-11 21:46:53.168299: Epoch time: 89.26 s
2024-12-11 21:46:54.407775: 
2024-12-11 21:46:54.408735: Epoch 122
2024-12-11 21:46:54.409629: Current learning rate: 0.00889
2024-12-11 21:48:23.692772: Validation loss did not improve from -0.57663. Patience: 32/50
2024-12-11 21:48:23.694106: train_loss -0.722
2024-12-11 21:48:23.695200: val_loss -0.5353
2024-12-11 21:48:23.696280: Pseudo dice [0.7702]
2024-12-11 21:48:23.697225: Epoch time: 89.29 s
2024-12-11 21:48:24.962389: 
2024-12-11 21:48:24.964206: Epoch 123
2024-12-11 21:48:24.965177: Current learning rate: 0.00889
2024-12-11 21:49:54.123877: Validation loss did not improve from -0.57663. Patience: 33/50
2024-12-11 21:49:54.124775: train_loss -0.7371
2024-12-11 21:49:54.125788: val_loss -0.4497
2024-12-11 21:49:54.126653: Pseudo dice [0.7373]
2024-12-11 21:49:54.127508: Epoch time: 89.16 s
2024-12-11 21:49:55.449267: 
2024-12-11 21:49:55.451016: Epoch 124
2024-12-11 21:49:55.451828: Current learning rate: 0.00888
2024-12-11 21:51:24.531726: Validation loss did not improve from -0.57663. Patience: 34/50
2024-12-11 21:51:24.532973: train_loss -0.7329
2024-12-11 21:51:24.533879: val_loss -0.4743
2024-12-11 21:51:24.534866: Pseudo dice [0.7523]
2024-12-11 21:51:24.535639: Epoch time: 89.08 s
2024-12-11 21:51:26.257205: 
2024-12-11 21:51:26.259013: Epoch 125
2024-12-11 21:51:26.259841: Current learning rate: 0.00887
2024-12-11 21:52:55.300796: Validation loss did not improve from -0.57663. Patience: 35/50
2024-12-11 21:52:55.301974: train_loss -0.7303
2024-12-11 21:52:55.302999: val_loss -0.5558
2024-12-11 21:52:55.303772: Pseudo dice [0.7807]
2024-12-11 21:52:55.304604: Epoch time: 89.05 s
2024-12-11 21:52:56.583225: 
2024-12-11 21:52:56.584733: Epoch 126
2024-12-11 21:52:56.585461: Current learning rate: 0.00886
2024-12-11 21:54:25.743987: Validation loss did not improve from -0.57663. Patience: 36/50
2024-12-11 21:54:25.745272: train_loss -0.7362
2024-12-11 21:54:25.746067: val_loss -0.5031
2024-12-11 21:54:25.746858: Pseudo dice [0.7555]
2024-12-11 21:54:25.747607: Epoch time: 89.16 s
2024-12-11 21:54:27.411587: 
2024-12-11 21:54:27.413090: Epoch 127
2024-12-11 21:54:27.413801: Current learning rate: 0.00885
2024-12-11 21:55:56.528920: Validation loss did not improve from -0.57663. Patience: 37/50
2024-12-11 21:55:56.530182: train_loss -0.7361
2024-12-11 21:55:56.531250: val_loss -0.4873
2024-12-11 21:55:56.532106: Pseudo dice [0.7626]
2024-12-11 21:55:56.533093: Epoch time: 89.12 s
2024-12-11 21:55:57.802557: 
2024-12-11 21:55:57.804659: Epoch 128
2024-12-11 21:55:57.805394: Current learning rate: 0.00884
2024-12-11 21:57:26.906528: Validation loss did not improve from -0.57663. Patience: 38/50
2024-12-11 21:57:26.907476: train_loss -0.7323
2024-12-11 21:57:26.908410: val_loss -0.4805
2024-12-11 21:57:26.909110: Pseudo dice [0.7463]
2024-12-11 21:57:26.909916: Epoch time: 89.11 s
2024-12-11 21:57:28.237435: 
2024-12-11 21:57:28.238553: Epoch 129
2024-12-11 21:57:28.239342: Current learning rate: 0.00883
2024-12-11 21:58:57.341292: Validation loss did not improve from -0.57663. Patience: 39/50
2024-12-11 21:58:57.342227: train_loss -0.7306
2024-12-11 21:58:57.343210: val_loss -0.5318
2024-12-11 21:58:57.344119: Pseudo dice [0.7729]
2024-12-11 21:58:57.345124: Epoch time: 89.11 s
2024-12-11 21:58:58.950993: 
2024-12-11 21:58:58.952855: Epoch 130
2024-12-11 21:58:58.953781: Current learning rate: 0.00882
2024-12-11 22:00:28.279677: Validation loss did not improve from -0.57663. Patience: 40/50
2024-12-11 22:00:28.280827: train_loss -0.731
2024-12-11 22:00:28.282170: val_loss -0.4931
2024-12-11 22:00:28.283130: Pseudo dice [0.7541]
2024-12-11 22:00:28.284106: Epoch time: 89.33 s
2024-12-11 22:00:29.594052: 
2024-12-11 22:00:29.596142: Epoch 131
2024-12-11 22:00:29.597242: Current learning rate: 0.00881
2024-12-11 22:01:58.976499: Validation loss did not improve from -0.57663. Patience: 41/50
2024-12-11 22:01:58.977530: train_loss -0.733
2024-12-11 22:01:58.978695: val_loss -0.5324
2024-12-11 22:01:58.979748: Pseudo dice [0.7768]
2024-12-11 22:01:58.980552: Epoch time: 89.38 s
2024-12-11 22:02:00.277199: 
2024-12-11 22:02:00.278994: Epoch 132
2024-12-11 22:02:00.279842: Current learning rate: 0.0088
2024-12-11 22:03:29.722350: Validation loss did not improve from -0.57663. Patience: 42/50
2024-12-11 22:03:29.723642: train_loss -0.7241
2024-12-11 22:03:29.724697: val_loss -0.4702
2024-12-11 22:03:29.725451: Pseudo dice [0.7455]
2024-12-11 22:03:29.726335: Epoch time: 89.45 s
2024-12-11 22:03:31.030097: 
2024-12-11 22:03:31.031526: Epoch 133
2024-12-11 22:03:31.032219: Current learning rate: 0.00879
2024-12-11 22:05:00.569585: Validation loss did not improve from -0.57663. Patience: 43/50
2024-12-11 22:05:00.570693: train_loss -0.7224
2024-12-11 22:05:00.571521: val_loss -0.5152
2024-12-11 22:05:00.572258: Pseudo dice [0.755]
2024-12-11 22:05:00.572932: Epoch time: 89.54 s
2024-12-11 22:05:01.854552: 
2024-12-11 22:05:01.855950: Epoch 134
2024-12-11 22:05:01.856679: Current learning rate: 0.00879
2024-12-11 22:06:31.197319: Validation loss did not improve from -0.57663. Patience: 44/50
2024-12-11 22:06:31.198496: train_loss -0.7217
2024-12-11 22:06:31.199469: val_loss -0.4868
2024-12-11 22:06:31.200267: Pseudo dice [0.7481]
2024-12-11 22:06:31.201090: Epoch time: 89.34 s
2024-12-11 22:06:32.811497: 
2024-12-11 22:06:32.813255: Epoch 135
2024-12-11 22:06:32.814133: Current learning rate: 0.00878
2024-12-11 22:08:02.015315: Validation loss did not improve from -0.57663. Patience: 45/50
2024-12-11 22:08:02.017734: train_loss -0.7308
2024-12-11 22:08:02.018899: val_loss -0.5055
2024-12-11 22:08:02.019531: Pseudo dice [0.7575]
2024-12-11 22:08:02.020175: Epoch time: 89.21 s
2024-12-11 22:08:03.318918: 
2024-12-11 22:08:03.320587: Epoch 136
2024-12-11 22:08:03.321493: Current learning rate: 0.00877
2024-12-11 22:09:32.446058: Validation loss did not improve from -0.57663. Patience: 46/50
2024-12-11 22:09:32.446938: train_loss -0.7366
2024-12-11 22:09:32.447839: val_loss -0.5385
2024-12-11 22:09:32.448626: Pseudo dice [0.7709]
2024-12-11 22:09:32.449376: Epoch time: 89.13 s
2024-12-11 22:09:33.724845: 
2024-12-11 22:09:33.726811: Epoch 137
2024-12-11 22:09:33.727555: Current learning rate: 0.00876
2024-12-11 22:11:02.820013: Validation loss did not improve from -0.57663. Patience: 47/50
2024-12-11 22:11:02.821060: train_loss -0.7417
2024-12-11 22:11:02.822394: val_loss -0.5136
2024-12-11 22:11:02.823382: Pseudo dice [0.7647]
2024-12-11 22:11:02.824246: Epoch time: 89.1 s
2024-12-11 22:11:04.448289: 
2024-12-11 22:11:04.449764: Epoch 138
2024-12-11 22:11:04.450790: Current learning rate: 0.00875
2024-12-11 22:12:33.516420: Validation loss did not improve from -0.57663. Patience: 48/50
2024-12-11 22:12:33.517304: train_loss -0.7333
2024-12-11 22:12:33.518198: val_loss -0.4804
2024-12-11 22:12:33.518939: Pseudo dice [0.7537]
2024-12-11 22:12:33.519659: Epoch time: 89.07 s
2024-12-11 22:12:34.806253: 
2024-12-11 22:12:34.807887: Epoch 139
2024-12-11 22:12:34.808928: Current learning rate: 0.00874
2024-12-11 22:14:03.882290: Validation loss did not improve from -0.57663. Patience: 49/50
2024-12-11 22:14:03.883444: train_loss -0.7343
2024-12-11 22:14:03.884555: val_loss -0.532
2024-12-11 22:14:03.885489: Pseudo dice [0.7802]
2024-12-11 22:14:03.886266: Epoch time: 89.08 s
2024-12-11 22:14:05.519585: 
2024-12-11 22:14:05.521430: Epoch 140
2024-12-11 22:14:05.522240: Current learning rate: 0.00873
2024-12-11 22:15:34.552004: Validation loss did not improve from -0.57663. Patience: 50/50
2024-12-11 22:15:34.553187: train_loss -0.7444
2024-12-11 22:15:34.554216: val_loss -0.536
2024-12-11 22:15:34.555073: Pseudo dice [0.7675]
2024-12-11 22:15:34.555884: Epoch time: 89.03 s
2024-12-11 22:15:35.829437: Patience reached. Stopping training.
2024-12-11 22:15:36.207752: Training done.
2024-12-11 22:15:36.427247: Using splits from existing split file: /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset307_Sohee_Calcium_OCT_CrossValidation/splits_final.json
2024-12-11 22:15:36.429667: The split file contains 5 splits.
2024-12-11 22:15:36.430554: Desired fold for training: 0
2024-12-11 22:15:36.431350: This split has 6 training and 2 validation cases.
2024-12-11 22:15:36.432240: predicting 106-002
2024-12-11 22:15:36.444591: 106-002, shape torch.Size([1, 539, 498, 498]), rank 0
2024-12-11 22:18:01.059664: predicting 706-005
2024-12-11 22:18:01.092685: 706-005, shape torch.Size([1, 375, 498, 498]), rank 0
2024-12-11 22:19:52.731176: Validation complete
2024-12-11 22:19:52.732091: Mean Validation Dice:  0.7671797174334805
2024-12-11 18:39:32.978973: unpacking done...
2024-12-11 18:39:33.023051: Unable to plot network architecture: nnUNet_compile is enabled!
2024-12-11 18:39:33.104897: 
2024-12-11 18:39:33.106114: Epoch 0
2024-12-11 18:39:33.106977: Current learning rate: 0.01
2024-12-11 18:42:27.287262: Validation loss improved from 1000.00000 to -0.14781! Patience: 0/50
2024-12-11 18:42:27.289136: train_loss -0.0916
2024-12-11 18:42:27.290515: val_loss -0.1478
2024-12-11 18:42:27.291289: Pseudo dice [0.4953]
2024-12-11 18:42:27.291970: Epoch time: 174.18 s
2024-12-11 18:42:27.292566: Yayy! New best EMA pseudo Dice: 0.4953
2024-12-11 18:42:28.724691: 
2024-12-11 18:42:28.726594: Epoch 1
2024-12-11 18:42:28.727438: Current learning rate: 0.00999
2024-12-11 18:43:55.608341: Validation loss improved from -0.14781 to -0.19919! Patience: 0/50
2024-12-11 18:43:55.609227: train_loss -0.2407
2024-12-11 18:43:55.609927: val_loss -0.1992
2024-12-11 18:43:55.610774: Pseudo dice [0.5338]
2024-12-11 18:43:55.611532: Epoch time: 86.89 s
2024-12-11 18:43:55.612390: Yayy! New best EMA pseudo Dice: 0.4992
2024-12-11 18:43:57.195847: 
2024-12-11 18:43:57.197711: Epoch 2
2024-12-11 18:43:57.198680: Current learning rate: 0.00998
2024-12-11 18:45:24.204667: Validation loss improved from -0.19919 to -0.22397! Patience: 0/50
2024-12-11 18:45:24.205529: train_loss -0.2882
2024-12-11 18:45:24.206350: val_loss -0.224
2024-12-11 18:45:24.207197: Pseudo dice [0.5338]
2024-12-11 18:45:24.207895: Epoch time: 87.01 s
2024-12-11 18:45:24.208658: Yayy! New best EMA pseudo Dice: 0.5026
2024-12-11 18:45:25.872216: 
2024-12-11 18:45:25.873405: Epoch 3
2024-12-11 18:45:25.874233: Current learning rate: 0.00997
2024-12-11 18:46:52.725325: Validation loss improved from -0.22397 to -0.25231! Patience: 0/50
2024-12-11 18:46:52.726774: train_loss -0.3302
2024-12-11 18:46:52.727603: val_loss -0.2523
2024-12-11 18:46:52.728230: Pseudo dice [0.5773]
2024-12-11 18:46:52.729006: Epoch time: 86.86 s
2024-12-11 18:46:52.729802: Yayy! New best EMA pseudo Dice: 0.5101
2024-12-11 18:46:54.366270: 
2024-12-11 18:46:54.368001: Epoch 4
2024-12-11 18:46:54.368740: Current learning rate: 0.00996
2024-12-11 18:48:21.039272: Validation loss improved from -0.25231 to -0.27500! Patience: 0/50
2024-12-11 18:48:21.040281: train_loss -0.3433
2024-12-11 18:48:21.041048: val_loss -0.275
2024-12-11 18:48:21.041682: Pseudo dice [0.5883]
2024-12-11 18:48:21.042449: Epoch time: 86.68 s
2024-12-11 18:48:21.337793: Yayy! New best EMA pseudo Dice: 0.5179
2024-12-11 18:48:22.938007: 
2024-12-11 18:48:22.939852: Epoch 5
2024-12-11 18:48:22.940571: Current learning rate: 0.00995
2024-12-11 18:49:49.860613: Validation loss did not improve from -0.27500. Patience: 1/50
2024-12-11 18:49:49.861794: train_loss -0.3673
2024-12-11 18:49:49.862582: val_loss -0.2594
2024-12-11 18:49:49.863210: Pseudo dice [0.5776]
2024-12-11 18:49:49.864111: Epoch time: 86.93 s
2024-12-11 18:49:49.864800: Yayy! New best EMA pseudo Dice: 0.5239
2024-12-11 18:49:51.419594: 
2024-12-11 18:49:51.421259: Epoch 6
2024-12-11 18:49:51.422179: Current learning rate: 0.00995
2024-12-11 18:51:18.196407: Validation loss improved from -0.27500 to -0.30011! Patience: 1/50
2024-12-11 18:51:18.197277: train_loss -0.3677
2024-12-11 18:51:18.198188: val_loss -0.3001
2024-12-11 18:51:18.198963: Pseudo dice [0.5784]
2024-12-11 18:51:18.199770: Epoch time: 86.78 s
2024-12-11 18:51:18.200499: Yayy! New best EMA pseudo Dice: 0.5293
2024-12-11 18:51:19.781246: 
2024-12-11 18:51:19.783029: Epoch 7
2024-12-11 18:51:19.783914: Current learning rate: 0.00994
2024-12-11 18:52:46.546449: Validation loss improved from -0.30011 to -0.31260! Patience: 0/50
2024-12-11 18:52:46.547501: train_loss -0.3996
2024-12-11 18:52:46.548458: val_loss -0.3126
2024-12-11 18:52:46.549146: Pseudo dice [0.6094]
2024-12-11 18:52:46.549840: Epoch time: 86.77 s
2024-12-11 18:52:46.550492: Yayy! New best EMA pseudo Dice: 0.5373
2024-12-11 18:52:48.155840: 
2024-12-11 18:52:48.157077: Epoch 8
2024-12-11 18:52:48.157843: Current learning rate: 0.00993
2024-12-11 18:54:15.060377: Validation loss improved from -0.31260 to -0.32014! Patience: 0/50
2024-12-11 18:54:15.061439: train_loss -0.4054
2024-12-11 18:54:15.062167: val_loss -0.3201
2024-12-11 18:54:15.062829: Pseudo dice [0.6131]
2024-12-11 18:54:15.063573: Epoch time: 86.91 s
2024-12-11 18:54:15.064348: Yayy! New best EMA pseudo Dice: 0.5449
2024-12-11 18:54:17.168287: 
2024-12-11 18:54:17.169485: Epoch 9
2024-12-11 18:54:17.170237: Current learning rate: 0.00992
2024-12-11 18:55:44.361501: Validation loss improved from -0.32014 to -0.39589! Patience: 0/50
2024-12-11 18:55:44.362676: train_loss -0.4286
2024-12-11 18:55:44.363542: val_loss -0.3959
2024-12-11 18:55:44.364331: Pseudo dice [0.6551]
2024-12-11 18:55:44.364978: Epoch time: 87.2 s
2024-12-11 18:55:44.732178: Yayy! New best EMA pseudo Dice: 0.5559
2024-12-11 18:55:46.292408: 
2024-12-11 18:55:46.293706: Epoch 10
2024-12-11 18:55:46.294356: Current learning rate: 0.00991
2024-12-11 18:57:13.352127: Validation loss did not improve from -0.39589. Patience: 1/50
2024-12-11 18:57:13.353207: train_loss -0.4358
2024-12-11 18:57:13.354330: val_loss -0.2998
2024-12-11 18:57:13.355229: Pseudo dice [0.6048]
2024-12-11 18:57:13.356017: Epoch time: 87.06 s
2024-12-11 18:57:13.356835: Yayy! New best EMA pseudo Dice: 0.5608
2024-12-11 18:57:14.953364: 
2024-12-11 18:57:14.955140: Epoch 11
2024-12-11 18:57:14.956148: Current learning rate: 0.0099
2024-12-11 18:58:42.026927: Validation loss did not improve from -0.39589. Patience: 2/50
2024-12-11 18:58:42.027994: train_loss -0.4582
2024-12-11 18:58:42.028803: val_loss -0.3904
2024-12-11 18:58:42.029569: Pseudo dice [0.6501]
2024-12-11 18:58:42.030229: Epoch time: 87.08 s
2024-12-11 18:58:42.030904: Yayy! New best EMA pseudo Dice: 0.5698
2024-12-11 18:58:43.599820: 
2024-12-11 18:58:43.601380: Epoch 12
2024-12-11 18:58:43.602305: Current learning rate: 0.00989
2024-12-11 19:00:10.531842: Validation loss did not improve from -0.39589. Patience: 3/50
2024-12-11 19:00:10.532912: train_loss -0.4659
2024-12-11 19:00:10.534045: val_loss -0.3849
2024-12-11 19:00:10.534754: Pseudo dice [0.6484]
2024-12-11 19:00:10.535707: Epoch time: 86.93 s
2024-12-11 19:00:10.536641: Yayy! New best EMA pseudo Dice: 0.5776
2024-12-11 19:00:12.145700: 
2024-12-11 19:00:12.147551: Epoch 13
2024-12-11 19:00:12.148498: Current learning rate: 0.00988
2024-12-11 19:01:38.940048: Validation loss did not improve from -0.39589. Patience: 4/50
2024-12-11 19:01:38.941454: train_loss -0.4722
2024-12-11 19:01:38.942602: val_loss -0.39
2024-12-11 19:01:38.943407: Pseudo dice [0.6416]
2024-12-11 19:01:38.944095: Epoch time: 86.8 s
2024-12-11 19:01:38.945069: Yayy! New best EMA pseudo Dice: 0.584
2024-12-11 19:01:40.554062: 
2024-12-11 19:01:40.555822: Epoch 14
2024-12-11 19:01:40.556671: Current learning rate: 0.00987
2024-12-11 19:03:07.473386: Validation loss improved from -0.39589 to -0.41201! Patience: 4/50
2024-12-11 19:03:07.474321: train_loss -0.48
2024-12-11 19:03:07.475121: val_loss -0.412
2024-12-11 19:03:07.475923: Pseudo dice [0.6724]
2024-12-11 19:03:07.476647: Epoch time: 86.92 s
2024-12-11 19:03:07.835990: Yayy! New best EMA pseudo Dice: 0.5928
2024-12-11 19:03:09.470639: 
2024-12-11 19:03:09.472456: Epoch 15
2024-12-11 19:03:09.473220: Current learning rate: 0.00986
2024-12-11 19:04:36.313658: Validation loss improved from -0.41201 to -0.44711! Patience: 0/50
2024-12-11 19:04:36.315363: train_loss -0.481
2024-12-11 19:04:36.316711: val_loss -0.4471
2024-12-11 19:04:36.317688: Pseudo dice [0.6752]
2024-12-11 19:04:36.318633: Epoch time: 86.85 s
2024-12-11 19:04:36.319268: Yayy! New best EMA pseudo Dice: 0.6011
2024-12-11 19:04:37.944972: 
2024-12-11 19:04:37.946882: Epoch 16
2024-12-11 19:04:37.947631: Current learning rate: 0.00986
2024-12-11 19:06:04.775639: Validation loss improved from -0.44711 to -0.44785! Patience: 0/50
2024-12-11 19:06:04.776811: train_loss -0.4965
2024-12-11 19:06:04.777921: val_loss -0.4479
2024-12-11 19:06:04.778720: Pseudo dice [0.6792]
2024-12-11 19:06:04.779604: Epoch time: 86.83 s
2024-12-11 19:06:04.780584: Yayy! New best EMA pseudo Dice: 0.6089
2024-12-11 19:06:06.452820: 
2024-12-11 19:06:06.454592: Epoch 17
2024-12-11 19:06:06.455543: Current learning rate: 0.00985
2024-12-11 19:07:33.371505: Validation loss improved from -0.44785 to -0.46963! Patience: 0/50
2024-12-11 19:07:33.372472: train_loss -0.5057
2024-12-11 19:07:33.373515: val_loss -0.4696
2024-12-11 19:07:33.374353: Pseudo dice [0.6935]
2024-12-11 19:07:33.374921: Epoch time: 86.92 s
2024-12-11 19:07:33.375535: Yayy! New best EMA pseudo Dice: 0.6174
2024-12-11 19:07:35.696258: 
2024-12-11 19:07:35.697862: Epoch 18
2024-12-11 19:07:35.698581: Current learning rate: 0.00984
2024-12-11 19:09:02.634285: Validation loss did not improve from -0.46963. Patience: 1/50
2024-12-11 19:09:02.635021: train_loss -0.5174
2024-12-11 19:09:02.635731: val_loss -0.4417
2024-12-11 19:09:02.636355: Pseudo dice [0.678]
2024-12-11 19:09:02.636971: Epoch time: 86.94 s
2024-12-11 19:09:02.637715: Yayy! New best EMA pseudo Dice: 0.6234
2024-12-11 19:09:04.311167: 
2024-12-11 19:09:04.312854: Epoch 19
2024-12-11 19:09:04.313577: Current learning rate: 0.00983
2024-12-11 19:10:30.994157: Validation loss did not improve from -0.46963. Patience: 2/50
2024-12-11 19:10:30.995296: train_loss -0.5054
2024-12-11 19:10:30.996420: val_loss -0.4026
2024-12-11 19:10:30.997177: Pseudo dice [0.6599]
2024-12-11 19:10:30.997966: Epoch time: 86.69 s
2024-12-11 19:10:31.355939: Yayy! New best EMA pseudo Dice: 0.6271
2024-12-11 19:10:33.076815: 
2024-12-11 19:10:33.078405: Epoch 20
2024-12-11 19:10:33.079325: Current learning rate: 0.00982
2024-12-11 19:11:59.807803: Validation loss did not improve from -0.46963. Patience: 3/50
2024-12-11 19:11:59.809000: train_loss -0.5179
2024-12-11 19:11:59.809812: val_loss -0.3996
2024-12-11 19:11:59.810473: Pseudo dice [0.6533]
2024-12-11 19:11:59.811168: Epoch time: 86.73 s
2024-12-11 19:11:59.811741: Yayy! New best EMA pseudo Dice: 0.6297
2024-12-11 19:12:01.455214: 
2024-12-11 19:12:01.456865: Epoch 21
2024-12-11 19:12:01.457896: Current learning rate: 0.00981
2024-12-11 19:13:28.142087: Validation loss did not improve from -0.46963. Patience: 4/50
2024-12-11 19:13:28.143201: train_loss -0.5309
2024-12-11 19:13:28.144228: val_loss -0.4641
2024-12-11 19:13:28.145076: Pseudo dice [0.6989]
2024-12-11 19:13:28.145705: Epoch time: 86.69 s
2024-12-11 19:13:28.146418: Yayy! New best EMA pseudo Dice: 0.6366
2024-12-11 19:13:29.709403: 
2024-12-11 19:13:29.710519: Epoch 22
2024-12-11 19:13:29.711276: Current learning rate: 0.0098
2024-12-11 19:14:56.501088: Validation loss did not improve from -0.46963. Patience: 5/50
2024-12-11 19:14:56.502218: train_loss -0.536
2024-12-11 19:14:56.502969: val_loss -0.4516
2024-12-11 19:14:56.503597: Pseudo dice [0.6866]
2024-12-11 19:14:56.504170: Epoch time: 86.79 s
2024-12-11 19:14:56.504759: Yayy! New best EMA pseudo Dice: 0.6416
2024-12-11 19:14:58.118079: 
2024-12-11 19:14:58.120021: Epoch 23
2024-12-11 19:14:58.121356: Current learning rate: 0.00979
2024-12-11 19:16:25.012052: Validation loss improved from -0.46963 to -0.48350! Patience: 5/50
2024-12-11 19:16:25.013028: train_loss -0.54
2024-12-11 19:16:25.013849: val_loss -0.4835
2024-12-11 19:16:25.014650: Pseudo dice [0.7101]
2024-12-11 19:16:25.015371: Epoch time: 86.9 s
2024-12-11 19:16:25.016145: Yayy! New best EMA pseudo Dice: 0.6485
2024-12-11 19:16:26.507299: 
2024-12-11 19:16:26.509032: Epoch 24
2024-12-11 19:16:26.509808: Current learning rate: 0.00978
2024-12-11 19:17:53.389243: Validation loss did not improve from -0.48350. Patience: 1/50
2024-12-11 19:17:53.389898: train_loss -0.5455
2024-12-11 19:17:53.390932: val_loss -0.4699
2024-12-11 19:17:53.391599: Pseudo dice [0.6954]
2024-12-11 19:17:53.392276: Epoch time: 86.88 s
2024-12-11 19:17:53.736878: Yayy! New best EMA pseudo Dice: 0.6532
2024-12-11 19:17:55.278078: 
2024-12-11 19:17:55.279283: Epoch 25
2024-12-11 19:17:55.280051: Current learning rate: 0.00977
2024-12-11 19:19:22.358466: Validation loss did not improve from -0.48350. Patience: 2/50
2024-12-11 19:19:22.359519: train_loss -0.5296
2024-12-11 19:19:22.360459: val_loss -0.4527
2024-12-11 19:19:22.361200: Pseudo dice [0.6849]
2024-12-11 19:19:22.361990: Epoch time: 87.08 s
2024-12-11 19:19:22.362905: Yayy! New best EMA pseudo Dice: 0.6563
2024-12-11 19:19:23.953249: 
2024-12-11 19:19:23.955023: Epoch 26
2024-12-11 19:19:23.955950: Current learning rate: 0.00977
2024-12-11 19:20:50.911186: Validation loss did not improve from -0.48350. Patience: 3/50
2024-12-11 19:20:50.912374: train_loss -0.5537
2024-12-11 19:20:50.913527: val_loss -0.4534
2024-12-11 19:20:50.914261: Pseudo dice [0.6882]
2024-12-11 19:20:50.915027: Epoch time: 86.96 s
2024-12-11 19:20:50.915820: Yayy! New best EMA pseudo Dice: 0.6595
2024-12-11 19:20:52.528046: 
2024-12-11 19:20:52.529587: Epoch 27
2024-12-11 19:20:52.530406: Current learning rate: 0.00976
2024-12-11 19:22:19.337632: Validation loss did not improve from -0.48350. Patience: 4/50
2024-12-11 19:22:19.338843: train_loss -0.5535
2024-12-11 19:22:19.339660: val_loss -0.4672
2024-12-11 19:22:19.340525: Pseudo dice [0.6996]
2024-12-11 19:22:19.341120: Epoch time: 86.81 s
2024-12-11 19:22:19.341758: Yayy! New best EMA pseudo Dice: 0.6635
2024-12-11 19:22:20.930093: 
2024-12-11 19:22:20.932487: Epoch 28
2024-12-11 19:22:20.934192: Current learning rate: 0.00975
2024-12-11 19:23:47.913980: Validation loss did not improve from -0.48350. Patience: 5/50
2024-12-11 19:23:47.915041: train_loss -0.557
2024-12-11 19:23:47.915908: val_loss -0.4831
2024-12-11 19:23:47.916740: Pseudo dice [0.7063]
2024-12-11 19:23:47.917808: Epoch time: 86.99 s
2024-12-11 19:23:47.918615: Yayy! New best EMA pseudo Dice: 0.6678
2024-12-11 19:23:49.806087: 
2024-12-11 19:23:49.807624: Epoch 29
2024-12-11 19:23:49.808679: Current learning rate: 0.00974
2024-12-11 19:25:16.797862: Validation loss did not improve from -0.48350. Patience: 6/50
2024-12-11 19:25:16.798897: train_loss -0.5604
2024-12-11 19:25:16.799839: val_loss -0.4746
2024-12-11 19:25:16.800438: Pseudo dice [0.7029]
2024-12-11 19:25:16.801076: Epoch time: 86.99 s
2024-12-11 19:25:17.165583: Yayy! New best EMA pseudo Dice: 0.6713
2024-12-11 19:25:18.743343: 
2024-12-11 19:25:18.744559: Epoch 30
2024-12-11 19:25:18.745255: Current learning rate: 0.00973
2024-12-11 19:26:45.788546: Validation loss did not improve from -0.48350. Patience: 7/50
2024-12-11 19:26:45.789241: train_loss -0.5693
2024-12-11 19:26:45.790037: val_loss -0.4282
2024-12-11 19:26:45.790799: Pseudo dice [0.6687]
2024-12-11 19:26:45.791585: Epoch time: 87.05 s
2024-12-11 19:26:47.032420: 
2024-12-11 19:26:47.033940: Epoch 31
2024-12-11 19:26:47.034739: Current learning rate: 0.00972
2024-12-11 19:28:14.264988: Validation loss did not improve from -0.48350. Patience: 8/50
2024-12-11 19:28:14.265986: train_loss -0.5749
2024-12-11 19:28:14.266951: val_loss -0.4681
2024-12-11 19:28:14.267843: Pseudo dice [0.697]
2024-12-11 19:28:14.268519: Epoch time: 87.23 s
2024-12-11 19:28:14.269230: Yayy! New best EMA pseudo Dice: 0.6736
2024-12-11 19:28:15.867569: 
2024-12-11 19:28:15.869290: Epoch 32
2024-12-11 19:28:15.870605: Current learning rate: 0.00971
2024-12-11 19:29:43.173467: Validation loss did not improve from -0.48350. Patience: 9/50
2024-12-11 19:29:43.174582: train_loss -0.5795
2024-12-11 19:29:43.175498: val_loss -0.414
2024-12-11 19:29:43.176304: Pseudo dice [0.6703]
2024-12-11 19:29:43.177165: Epoch time: 87.31 s
2024-12-11 19:29:44.433262: 
2024-12-11 19:29:44.435101: Epoch 33
2024-12-11 19:29:44.435865: Current learning rate: 0.0097
2024-12-11 19:31:11.690719: Validation loss did not improve from -0.48350. Patience: 10/50
2024-12-11 19:31:11.691961: train_loss -0.5708
2024-12-11 19:31:11.692855: val_loss -0.4673
2024-12-11 19:31:11.693477: Pseudo dice [0.6969]
2024-12-11 19:31:11.694289: Epoch time: 87.26 s
2024-12-11 19:31:11.694885: Yayy! New best EMA pseudo Dice: 0.6757
2024-12-11 19:31:13.272557: 
2024-12-11 19:31:13.274084: Epoch 34
2024-12-11 19:31:13.274729: Current learning rate: 0.00969
2024-12-11 19:32:40.511201: Validation loss improved from -0.48350 to -0.48561! Patience: 10/50
2024-12-11 19:32:40.511981: train_loss -0.5871
2024-12-11 19:32:40.512702: val_loss -0.4856
2024-12-11 19:32:40.513592: Pseudo dice [0.7112]
2024-12-11 19:32:40.514450: Epoch time: 87.24 s
2024-12-11 19:32:40.879644: Yayy! New best EMA pseudo Dice: 0.6792
2024-12-11 19:32:42.479462: 
2024-12-11 19:32:42.481215: Epoch 35
2024-12-11 19:32:42.482045: Current learning rate: 0.00968
2024-12-11 19:34:09.731686: Validation loss did not improve from -0.48561. Patience: 1/50
2024-12-11 19:34:09.732781: train_loss -0.5878
2024-12-11 19:34:09.733532: val_loss -0.4051
2024-12-11 19:34:09.734275: Pseudo dice [0.6613]
2024-12-11 19:34:09.734968: Epoch time: 87.25 s
2024-12-11 19:34:10.999230: 
2024-12-11 19:34:11.000622: Epoch 36
2024-12-11 19:34:11.001337: Current learning rate: 0.00968
2024-12-11 19:35:38.219343: Validation loss did not improve from -0.48561. Patience: 2/50
2024-12-11 19:35:38.220245: train_loss -0.5902
2024-12-11 19:35:38.221121: val_loss -0.4423
2024-12-11 19:35:38.221741: Pseudo dice [0.6746]
2024-12-11 19:35:38.222512: Epoch time: 87.22 s
2024-12-11 19:35:39.481459: 
2024-12-11 19:35:39.483126: Epoch 37
2024-12-11 19:35:39.484086: Current learning rate: 0.00967
2024-12-11 19:37:06.813740: Validation loss improved from -0.48561 to -0.49644! Patience: 2/50
2024-12-11 19:37:06.814587: train_loss -0.6042
2024-12-11 19:37:06.815382: val_loss -0.4964
2024-12-11 19:37:06.816082: Pseudo dice [0.7119]
2024-12-11 19:37:06.816796: Epoch time: 87.33 s
2024-12-11 19:37:06.817519: Yayy! New best EMA pseudo Dice: 0.6806
2024-12-11 19:37:08.411506: 
2024-12-11 19:37:08.412805: Epoch 38
2024-12-11 19:37:08.413599: Current learning rate: 0.00966
2024-12-11 19:38:35.782982: Validation loss did not improve from -0.49644. Patience: 1/50
2024-12-11 19:38:35.783683: train_loss -0.6018
2024-12-11 19:38:35.784482: val_loss -0.4823
2024-12-11 19:38:35.785339: Pseudo dice [0.7088]
2024-12-11 19:38:35.786267: Epoch time: 87.37 s
2024-12-11 19:38:35.787115: Yayy! New best EMA pseudo Dice: 0.6834
2024-12-11 19:38:37.404279: 
2024-12-11 19:38:37.405906: Epoch 39
2024-12-11 19:38:37.406829: Current learning rate: 0.00965
2024-12-11 19:40:04.791069: Validation loss did not improve from -0.49644. Patience: 2/50
2024-12-11 19:40:04.792164: train_loss -0.5957
2024-12-11 19:40:04.793134: val_loss -0.4803
2024-12-11 19:40:04.793745: Pseudo dice [0.7009]
2024-12-11 19:40:04.794347: Epoch time: 87.39 s
2024-12-11 19:40:05.181295: Yayy! New best EMA pseudo Dice: 0.6852
2024-12-11 19:40:07.200275: 
2024-12-11 19:40:07.201642: Epoch 40
2024-12-11 19:40:07.202711: Current learning rate: 0.00964
2024-12-11 19:41:34.376348: Validation loss did not improve from -0.49644. Patience: 3/50
2024-12-11 19:41:34.377324: train_loss -0.6057
2024-12-11 19:41:34.378202: val_loss -0.4421
2024-12-11 19:41:34.379021: Pseudo dice [0.6746]
2024-12-11 19:41:34.379707: Epoch time: 87.18 s
2024-12-11 19:41:35.674916: 
2024-12-11 19:41:35.676586: Epoch 41
2024-12-11 19:41:35.677764: Current learning rate: 0.00963
2024-12-11 19:43:02.972461: Validation loss did not improve from -0.49644. Patience: 4/50
2024-12-11 19:43:02.973369: train_loss -0.605
2024-12-11 19:43:02.974215: val_loss -0.4742
2024-12-11 19:43:02.974983: Pseudo dice [0.7034]
2024-12-11 19:43:02.975893: Epoch time: 87.3 s
2024-12-11 19:43:02.976538: Yayy! New best EMA pseudo Dice: 0.686
2024-12-11 19:43:04.494750: 
2024-12-11 19:43:04.496666: Epoch 42
2024-12-11 19:43:04.497537: Current learning rate: 0.00962
2024-12-11 19:44:31.807883: Validation loss did not improve from -0.49644. Patience: 5/50
2024-12-11 19:44:31.808897: train_loss -0.6129
2024-12-11 19:44:31.809899: val_loss -0.4633
2024-12-11 19:44:31.810951: Pseudo dice [0.6965]
2024-12-11 19:44:31.812034: Epoch time: 87.32 s
2024-12-11 19:44:31.813023: Yayy! New best EMA pseudo Dice: 0.6871
2024-12-11 19:44:33.378088: 
2024-12-11 19:44:33.379611: Epoch 43
2024-12-11 19:44:33.380314: Current learning rate: 0.00961
2024-12-11 19:46:00.792077: Validation loss did not improve from -0.49644. Patience: 6/50
2024-12-11 19:46:00.794342: train_loss -0.6055
2024-12-11 19:46:00.795608: val_loss -0.4534
2024-12-11 19:46:00.796444: Pseudo dice [0.6801]
2024-12-11 19:46:00.797702: Epoch time: 87.42 s
2024-12-11 19:46:02.010696: 
2024-12-11 19:46:02.012752: Epoch 44
2024-12-11 19:46:02.013796: Current learning rate: 0.0096
2024-12-11 19:47:29.422324: Validation loss improved from -0.49644 to -0.53264! Patience: 6/50
2024-12-11 19:47:29.423576: train_loss -0.6041
2024-12-11 19:47:29.424535: val_loss -0.5326
2024-12-11 19:47:29.425480: Pseudo dice [0.734]
2024-12-11 19:47:29.426265: Epoch time: 87.41 s
2024-12-11 19:47:29.812348: Yayy! New best EMA pseudo Dice: 0.6912
2024-12-11 19:47:31.444227: 
2024-12-11 19:47:31.446659: Epoch 45
2024-12-11 19:47:31.448058: Current learning rate: 0.00959
2024-12-11 19:48:58.849234: Validation loss did not improve from -0.53264. Patience: 1/50
2024-12-11 19:48:58.850104: train_loss -0.6115
2024-12-11 19:48:58.851082: val_loss -0.4956
2024-12-11 19:48:58.851874: Pseudo dice [0.7165]
2024-12-11 19:48:58.852557: Epoch time: 87.41 s
2024-12-11 19:48:58.853350: Yayy! New best EMA pseudo Dice: 0.6937
2024-12-11 19:49:00.423025: 
2024-12-11 19:49:00.424589: Epoch 46
2024-12-11 19:49:00.425331: Current learning rate: 0.00959
2024-12-11 19:50:27.710523: Validation loss did not improve from -0.53264. Patience: 2/50
2024-12-11 19:50:27.711238: train_loss -0.6213
2024-12-11 19:50:27.712140: val_loss -0.4587
2024-12-11 19:50:27.713007: Pseudo dice [0.6855]
2024-12-11 19:50:27.713701: Epoch time: 87.29 s
2024-12-11 19:50:28.889994: 
2024-12-11 19:50:28.891549: Epoch 47
2024-12-11 19:50:28.892204: Current learning rate: 0.00958
2024-12-11 19:51:56.082854: Validation loss did not improve from -0.53264. Patience: 3/50
2024-12-11 19:51:56.083933: train_loss -0.6061
2024-12-11 19:51:56.084749: val_loss -0.5121
2024-12-11 19:51:56.085501: Pseudo dice [0.7219]
2024-12-11 19:51:56.086268: Epoch time: 87.2 s
2024-12-11 19:51:56.086941: Yayy! New best EMA pseudo Dice: 0.6958
2024-12-11 19:51:57.599727: 
2024-12-11 19:51:57.600777: Epoch 48
2024-12-11 19:51:57.601864: Current learning rate: 0.00957
2024-12-11 19:53:24.386549: Validation loss did not improve from -0.53264. Patience: 4/50
2024-12-11 19:53:24.387705: train_loss -0.6192
2024-12-11 19:53:24.388528: val_loss -0.5044
2024-12-11 19:53:24.389099: Pseudo dice [0.7207]
2024-12-11 19:53:24.389674: Epoch time: 86.79 s
2024-12-11 19:53:24.390215: Yayy! New best EMA pseudo Dice: 0.6983
2024-12-11 19:53:25.969049: 
2024-12-11 19:53:25.970482: Epoch 49
2024-12-11 19:53:25.971318: Current learning rate: 0.00956
2024-12-11 19:54:53.192627: Validation loss did not improve from -0.53264. Patience: 5/50
2024-12-11 19:54:53.193460: train_loss -0.6279
2024-12-11 19:54:53.194269: val_loss -0.5256
2024-12-11 19:54:53.195074: Pseudo dice [0.7297]
2024-12-11 19:54:53.195856: Epoch time: 87.23 s
2024-12-11 19:54:53.549783: Yayy! New best EMA pseudo Dice: 0.7014
2024-12-11 19:54:55.517214: 
2024-12-11 19:54:55.518990: Epoch 50
2024-12-11 19:54:55.519770: Current learning rate: 0.00955
2024-12-11 19:56:22.507579: Validation loss did not improve from -0.53264. Patience: 6/50
2024-12-11 19:56:22.508644: train_loss -0.6236
2024-12-11 19:56:22.509566: val_loss -0.4845
2024-12-11 19:56:22.510349: Pseudo dice [0.7103]
2024-12-11 19:56:22.510970: Epoch time: 86.99 s
2024-12-11 19:56:22.511683: Yayy! New best EMA pseudo Dice: 0.7023
2024-12-11 19:56:24.078814: 
2024-12-11 19:56:24.081110: Epoch 51
2024-12-11 19:56:24.082096: Current learning rate: 0.00954
2024-12-11 19:57:50.966059: Validation loss did not improve from -0.53264. Patience: 7/50
2024-12-11 19:57:50.967317: train_loss -0.6304
2024-12-11 19:57:50.969654: val_loss -0.4753
2024-12-11 19:57:50.970572: Pseudo dice [0.7052]
2024-12-11 19:57:50.971714: Epoch time: 86.89 s
2024-12-11 19:57:50.972428: Yayy! New best EMA pseudo Dice: 0.7026
2024-12-11 19:57:52.619090: 
2024-12-11 19:57:52.620615: Epoch 52
2024-12-11 19:57:52.622084: Current learning rate: 0.00953
2024-12-11 19:59:19.747353: Validation loss did not improve from -0.53264. Patience: 8/50
2024-12-11 19:59:19.748517: train_loss -0.6405
2024-12-11 19:59:19.749442: val_loss -0.5032
2024-12-11 19:59:19.750155: Pseudo dice [0.7173]
2024-12-11 19:59:19.751117: Epoch time: 87.13 s
2024-12-11 19:59:19.751776: Yayy! New best EMA pseudo Dice: 0.7041
2024-12-11 19:59:21.322902: 
2024-12-11 19:59:21.324493: Epoch 53
2024-12-11 19:59:21.325443: Current learning rate: 0.00952
2024-12-11 20:00:48.759427: Validation loss did not improve from -0.53264. Patience: 9/50
2024-12-11 20:00:48.760462: train_loss -0.6262
2024-12-11 20:00:48.761408: val_loss -0.4944
2024-12-11 20:00:48.762171: Pseudo dice [0.7184]
2024-12-11 20:00:48.762816: Epoch time: 87.44 s
2024-12-11 20:00:48.763344: Yayy! New best EMA pseudo Dice: 0.7055
2024-12-11 20:00:50.358168: 
2024-12-11 20:00:50.359776: Epoch 54
2024-12-11 20:00:50.360568: Current learning rate: 0.00951
2024-12-11 20:02:17.396477: Validation loss did not improve from -0.53264. Patience: 10/50
2024-12-11 20:02:17.397441: train_loss -0.6366
2024-12-11 20:02:17.398105: val_loss -0.5199
2024-12-11 20:02:17.398899: Pseudo dice [0.7322]
2024-12-11 20:02:17.399627: Epoch time: 87.04 s
2024-12-11 20:02:17.766317: Yayy! New best EMA pseudo Dice: 0.7082
2024-12-11 20:02:19.343261: 
2024-12-11 20:02:19.344753: Epoch 55
2024-12-11 20:02:19.345850: Current learning rate: 0.0095
2024-12-11 20:03:46.579335: Validation loss did not improve from -0.53264. Patience: 11/50
2024-12-11 20:03:46.580354: train_loss -0.6237
2024-12-11 20:03:46.581295: val_loss -0.4917
2024-12-11 20:03:46.582045: Pseudo dice [0.7095]
2024-12-11 20:03:46.582931: Epoch time: 87.24 s
2024-12-11 20:03:46.583570: Yayy! New best EMA pseudo Dice: 0.7083
2024-12-11 20:03:48.167968: 
2024-12-11 20:03:48.169551: Epoch 56
2024-12-11 20:03:48.170306: Current learning rate: 0.00949
2024-12-11 20:05:15.472348: Validation loss did not improve from -0.53264. Patience: 12/50
2024-12-11 20:05:15.473630: train_loss -0.6361
2024-12-11 20:05:15.474465: val_loss -0.4689
2024-12-11 20:05:15.475236: Pseudo dice [0.692]
2024-12-11 20:05:15.475966: Epoch time: 87.31 s
2024-12-11 20:05:16.714238: 
2024-12-11 20:05:16.715628: Epoch 57
2024-12-11 20:05:16.716466: Current learning rate: 0.00949
2024-12-11 20:06:44.010787: Validation loss did not improve from -0.53264. Patience: 13/50
2024-12-11 20:06:44.012092: train_loss -0.6489
2024-12-11 20:06:44.013205: val_loss -0.5128
2024-12-11 20:06:44.014040: Pseudo dice [0.7279]
2024-12-11 20:06:44.014788: Epoch time: 87.3 s
2024-12-11 20:06:44.015514: Yayy! New best EMA pseudo Dice: 0.7088
2024-12-11 20:06:45.652231: 
2024-12-11 20:06:45.654205: Epoch 58
2024-12-11 20:06:45.655042: Current learning rate: 0.00948
2024-12-11 20:08:12.988728: Validation loss did not improve from -0.53264. Patience: 14/50
2024-12-11 20:08:12.989773: train_loss -0.6524
2024-12-11 20:08:12.990535: val_loss -0.4771
2024-12-11 20:08:12.991171: Pseudo dice [0.7024]
2024-12-11 20:08:12.991825: Epoch time: 87.34 s
2024-12-11 20:08:14.262865: 
2024-12-11 20:08:14.264247: Epoch 59
2024-12-11 20:08:14.265000: Current learning rate: 0.00947
2024-12-11 20:09:41.626844: Validation loss did not improve from -0.53264. Patience: 15/50
2024-12-11 20:09:41.627871: train_loss -0.6428
2024-12-11 20:09:41.628884: val_loss -0.4838
2024-12-11 20:09:41.629854: Pseudo dice [0.7026]
2024-12-11 20:09:41.630859: Epoch time: 87.37 s
2024-12-11 20:09:43.288893: 
2024-12-11 20:09:43.290554: Epoch 60
2024-12-11 20:09:43.291664: Current learning rate: 0.00946
2024-12-11 20:11:10.752798: Validation loss did not improve from -0.53264. Patience: 16/50
2024-12-11 20:11:10.753832: train_loss -0.6341
2024-12-11 20:11:10.754591: val_loss -0.5043
2024-12-11 20:11:10.755306: Pseudo dice [0.7112]
2024-12-11 20:11:10.755917: Epoch time: 87.47 s
2024-12-11 20:11:12.348989: 
2024-12-11 20:11:12.350633: Epoch 61
2024-12-11 20:11:12.351595: Current learning rate: 0.00945
2024-12-11 20:12:39.712982: Validation loss did not improve from -0.53264. Patience: 17/50
2024-12-11 20:12:39.714020: train_loss -0.6519
2024-12-11 20:12:39.714925: val_loss -0.5209
2024-12-11 20:12:39.715525: Pseudo dice [0.7328]
2024-12-11 20:12:39.716237: Epoch time: 87.37 s
2024-12-11 20:12:39.716905: Yayy! New best EMA pseudo Dice: 0.7104
2024-12-11 20:12:41.330700: 
2024-12-11 20:12:41.332578: Epoch 62
2024-12-11 20:12:41.333319: Current learning rate: 0.00944
2024-12-11 20:14:08.574822: Validation loss did not improve from -0.53264. Patience: 18/50
2024-12-11 20:14:08.575966: train_loss -0.6398
2024-12-11 20:14:08.576863: val_loss -0.507
2024-12-11 20:14:08.577711: Pseudo dice [0.7169]
2024-12-11 20:14:08.578566: Epoch time: 87.25 s
2024-12-11 20:14:08.579405: Yayy! New best EMA pseudo Dice: 0.7111
2024-12-11 20:14:10.134085: 
2024-12-11 20:14:10.135814: Epoch 63
2024-12-11 20:14:10.136688: Current learning rate: 0.00943
2024-12-11 20:15:37.182060: Validation loss did not improve from -0.53264. Patience: 19/50
2024-12-11 20:15:37.183245: train_loss -0.6432
2024-12-11 20:15:37.184064: val_loss -0.5304
2024-12-11 20:15:37.184867: Pseudo dice [0.7345]
2024-12-11 20:15:37.185630: Epoch time: 87.05 s
2024-12-11 20:15:37.186353: Yayy! New best EMA pseudo Dice: 0.7134
2024-12-11 20:15:38.807604: 
2024-12-11 20:15:38.809323: Epoch 64
2024-12-11 20:15:38.810098: Current learning rate: 0.00942
2024-12-11 20:17:05.790560: Validation loss did not improve from -0.53264. Patience: 20/50
2024-12-11 20:17:05.791746: train_loss -0.6373
2024-12-11 20:17:05.792514: val_loss -0.522
2024-12-11 20:17:05.793140: Pseudo dice [0.7324]
2024-12-11 20:17:05.793805: Epoch time: 86.99 s
2024-12-11 20:17:06.162009: Yayy! New best EMA pseudo Dice: 0.7153
2024-12-11 20:17:07.740150: 
2024-12-11 20:17:07.741800: Epoch 65
2024-12-11 20:17:07.742600: Current learning rate: 0.00941
2024-12-11 20:18:34.830909: Validation loss did not improve from -0.53264. Patience: 21/50
2024-12-11 20:18:34.831765: train_loss -0.6508
2024-12-11 20:18:34.832495: val_loss -0.4724
2024-12-11 20:18:34.833153: Pseudo dice [0.6987]
2024-12-11 20:18:34.833765: Epoch time: 87.09 s
2024-12-11 20:18:36.088535: 
2024-12-11 20:18:36.089675: Epoch 66
2024-12-11 20:18:36.090424: Current learning rate: 0.0094
2024-12-11 20:20:02.925652: Validation loss did not improve from -0.53264. Patience: 22/50
2024-12-11 20:20:02.926783: train_loss -0.6441
2024-12-11 20:20:02.927739: val_loss -0.5091
2024-12-11 20:20:02.928386: Pseudo dice [0.7156]
2024-12-11 20:20:02.929154: Epoch time: 86.84 s
2024-12-11 20:20:04.151197: 
2024-12-11 20:20:04.153464: Epoch 67
2024-12-11 20:20:04.154335: Current learning rate: 0.00939
2024-12-11 20:21:30.796623: Validation loss did not improve from -0.53264. Patience: 23/50
2024-12-11 20:21:30.798064: train_loss -0.6577
2024-12-11 20:21:30.798955: val_loss -0.5089
2024-12-11 20:21:30.799568: Pseudo dice [0.7223]
2024-12-11 20:21:30.800286: Epoch time: 86.65 s
2024-12-11 20:21:32.075976: 
2024-12-11 20:21:32.077650: Epoch 68
2024-12-11 20:21:32.078617: Current learning rate: 0.00939
2024-12-11 20:22:58.770403: Validation loss did not improve from -0.53264. Patience: 24/50
2024-12-11 20:22:58.771261: train_loss -0.6601
2024-12-11 20:22:58.772134: val_loss -0.5276
2024-12-11 20:22:58.773031: Pseudo dice [0.7313]
2024-12-11 20:22:58.773971: Epoch time: 86.7 s
2024-12-11 20:22:58.774804: Yayy! New best EMA pseudo Dice: 0.7164
2024-12-11 20:23:00.498647: 
2024-12-11 20:23:00.500249: Epoch 69
2024-12-11 20:23:00.501094: Current learning rate: 0.00938
2024-12-11 20:24:27.206795: Validation loss did not improve from -0.53264. Patience: 25/50
2024-12-11 20:24:27.208136: train_loss -0.661
2024-12-11 20:24:27.209427: val_loss -0.4882
2024-12-11 20:24:27.210245: Pseudo dice [0.7075]
2024-12-11 20:24:27.211211: Epoch time: 86.71 s
2024-12-11 20:24:28.795465: 
2024-12-11 20:24:28.797213: Epoch 70
2024-12-11 20:24:28.798073: Current learning rate: 0.00937
2024-12-11 20:25:55.459840: Validation loss improved from -0.53264 to -0.53974! Patience: 25/50
2024-12-11 20:25:55.462185: train_loss -0.6588
2024-12-11 20:25:55.463930: val_loss -0.5397
2024-12-11 20:25:55.464651: Pseudo dice [0.7365]
2024-12-11 20:25:55.465534: Epoch time: 86.67 s
2024-12-11 20:25:55.466353: Yayy! New best EMA pseudo Dice: 0.7176
2024-12-11 20:25:57.097198: 
2024-12-11 20:25:57.099156: Epoch 71
2024-12-11 20:25:57.100069: Current learning rate: 0.00936
2024-12-11 20:27:23.789108: Validation loss did not improve from -0.53974. Patience: 1/50
2024-12-11 20:27:23.790117: train_loss -0.6568
2024-12-11 20:27:23.790979: val_loss -0.4992
2024-12-11 20:27:23.791723: Pseudo dice [0.7096]
2024-12-11 20:27:23.792483: Epoch time: 86.69 s
2024-12-11 20:27:25.446204: 
2024-12-11 20:27:25.448005: Epoch 72
2024-12-11 20:27:25.448970: Current learning rate: 0.00935
2024-12-11 20:28:52.119702: Validation loss did not improve from -0.53974. Patience: 2/50
2024-12-11 20:28:52.121128: train_loss -0.6669
2024-12-11 20:28:52.122066: val_loss -0.5328
2024-12-11 20:28:52.122772: Pseudo dice [0.729]
2024-12-11 20:28:52.123589: Epoch time: 86.68 s
2024-12-11 20:28:52.124401: Yayy! New best EMA pseudo Dice: 0.718
2024-12-11 20:28:53.714664: 
2024-12-11 20:28:53.716854: Epoch 73
2024-12-11 20:28:53.717764: Current learning rate: 0.00934
2024-12-11 20:30:20.398823: Validation loss did not improve from -0.53974. Patience: 3/50
2024-12-11 20:30:20.399999: train_loss -0.6657
2024-12-11 20:30:20.400846: val_loss -0.5103
2024-12-11 20:30:20.401565: Pseudo dice [0.7253]
2024-12-11 20:30:20.402666: Epoch time: 86.69 s
2024-12-11 20:30:20.403599: Yayy! New best EMA pseudo Dice: 0.7187
2024-12-11 20:30:22.023587: 
2024-12-11 20:30:22.025011: Epoch 74
2024-12-11 20:30:22.025831: Current learning rate: 0.00933
2024-12-11 20:31:48.768457: Validation loss did not improve from -0.53974. Patience: 4/50
2024-12-11 20:31:48.769488: train_loss -0.6674
2024-12-11 20:31:48.770585: val_loss -0.5357
2024-12-11 20:31:48.771561: Pseudo dice [0.7362]
2024-12-11 20:31:48.772382: Epoch time: 86.75 s
2024-12-11 20:31:49.145891: Yayy! New best EMA pseudo Dice: 0.7205
2024-12-11 20:31:50.770552: 
2024-12-11 20:31:50.773111: Epoch 75
2024-12-11 20:31:50.774120: Current learning rate: 0.00932
2024-12-11 20:33:17.762754: Validation loss improved from -0.53974 to -0.54669! Patience: 4/50
2024-12-11 20:33:17.764080: train_loss -0.6714
2024-12-11 20:33:17.765403: val_loss -0.5467
2024-12-11 20:33:17.766322: Pseudo dice [0.7447]
2024-12-11 20:33:17.767208: Epoch time: 86.99 s
2024-12-11 20:33:17.767973: Yayy! New best EMA pseudo Dice: 0.7229
2024-12-11 20:33:19.483818: 
2024-12-11 20:33:19.485328: Epoch 76
2024-12-11 20:33:19.486131: Current learning rate: 0.00931
2024-12-11 20:34:46.442864: Validation loss improved from -0.54669 to -0.55419! Patience: 0/50
2024-12-11 20:34:46.444160: train_loss -0.6756
2024-12-11 20:34:46.445058: val_loss -0.5542
2024-12-11 20:34:46.445830: Pseudo dice [0.7454]
2024-12-11 20:34:46.446628: Epoch time: 86.96 s
2024-12-11 20:34:46.447428: Yayy! New best EMA pseudo Dice: 0.7252
2024-12-11 20:34:48.120846: 
2024-12-11 20:34:48.122017: Epoch 77
2024-12-11 20:34:48.122771: Current learning rate: 0.0093
2024-12-11 20:36:14.828617: Validation loss did not improve from -0.55419. Patience: 1/50
2024-12-11 20:36:14.829463: train_loss -0.6742
2024-12-11 20:36:14.830480: val_loss -0.5098
2024-12-11 20:36:14.831234: Pseudo dice [0.7258]
2024-12-11 20:36:14.831850: Epoch time: 86.71 s
2024-12-11 20:36:14.832559: Yayy! New best EMA pseudo Dice: 0.7252
2024-12-11 20:36:16.450867: 
2024-12-11 20:36:16.452766: Epoch 78
2024-12-11 20:36:16.453782: Current learning rate: 0.0093
2024-12-11 20:37:43.207606: Validation loss did not improve from -0.55419. Patience: 2/50
2024-12-11 20:37:43.208794: train_loss -0.6854
2024-12-11 20:37:43.209625: val_loss -0.4729
2024-12-11 20:37:43.210380: Pseudo dice [0.7044]
2024-12-11 20:37:43.211023: Epoch time: 86.76 s
2024-12-11 20:37:44.527567: 
2024-12-11 20:37:44.529683: Epoch 79
2024-12-11 20:37:44.530857: Current learning rate: 0.00929
2024-12-11 20:39:11.163583: Validation loss did not improve from -0.55419. Patience: 3/50
2024-12-11 20:39:11.164653: train_loss -0.6817
2024-12-11 20:39:11.165691: val_loss -0.5174
2024-12-11 20:39:11.166597: Pseudo dice [0.7273]
2024-12-11 20:39:11.167414: Epoch time: 86.64 s
2024-12-11 20:39:12.784687: 
2024-12-11 20:39:12.786011: Epoch 80
2024-12-11 20:39:12.786926: Current learning rate: 0.00928
2024-12-11 20:40:39.403132: Validation loss did not improve from -0.55419. Patience: 4/50
2024-12-11 20:40:39.403942: train_loss -0.6737
2024-12-11 20:40:39.404878: val_loss -0.5262
2024-12-11 20:40:39.405762: Pseudo dice [0.7359]
2024-12-11 20:40:39.406531: Epoch time: 86.62 s
2024-12-11 20:40:40.734534: 
2024-12-11 20:40:40.736000: Epoch 81
2024-12-11 20:40:40.736882: Current learning rate: 0.00927
2024-12-11 20:42:07.432530: Validation loss did not improve from -0.55419. Patience: 5/50
2024-12-11 20:42:07.433275: train_loss -0.6698
2024-12-11 20:42:07.434223: val_loss -0.5023
2024-12-11 20:42:07.434973: Pseudo dice [0.7043]
2024-12-11 20:42:07.435817: Epoch time: 86.7 s
2024-12-11 20:42:09.156356: 
2024-12-11 20:42:09.157929: Epoch 82
2024-12-11 20:42:09.158843: Current learning rate: 0.00926
2024-12-11 20:43:35.804748: Validation loss did not improve from -0.55419. Patience: 6/50
2024-12-11 20:43:35.806023: train_loss -0.6759
2024-12-11 20:43:35.806864: val_loss -0.4775
2024-12-11 20:43:35.807565: Pseudo dice [0.7113]
2024-12-11 20:43:35.808165: Epoch time: 86.65 s
2024-12-11 20:43:37.052102: 
2024-12-11 20:43:37.053302: Epoch 83
2024-12-11 20:43:37.054081: Current learning rate: 0.00925
2024-12-11 20:45:03.665955: Validation loss did not improve from -0.55419. Patience: 7/50
2024-12-11 20:45:03.666956: train_loss -0.6856
2024-12-11 20:45:03.667784: val_loss -0.5091
2024-12-11 20:45:03.668439: Pseudo dice [0.7232]
2024-12-11 20:45:03.669288: Epoch time: 86.62 s
2024-12-11 20:45:04.871263: 
2024-12-11 20:45:04.873773: Epoch 84
2024-12-11 20:45:04.874688: Current learning rate: 0.00924
2024-12-11 20:46:31.525377: Validation loss did not improve from -0.55419. Patience: 8/50
2024-12-11 20:46:31.526231: train_loss -0.6838
2024-12-11 20:46:31.527589: val_loss -0.5202
2024-12-11 20:46:31.528774: Pseudo dice [0.7181]
2024-12-11 20:46:31.529751: Epoch time: 86.66 s
2024-12-11 20:46:33.096045: 
2024-12-11 20:46:33.097871: Epoch 85
2024-12-11 20:46:33.098798: Current learning rate: 0.00923
2024-12-11 20:47:59.827811: Validation loss did not improve from -0.55419. Patience: 9/50
2024-12-11 20:47:59.828760: train_loss -0.6766
2024-12-11 20:47:59.830005: val_loss -0.5201
2024-12-11 20:47:59.830963: Pseudo dice [0.7266]
2024-12-11 20:47:59.831661: Epoch time: 86.73 s
2024-12-11 20:48:01.068411: 
2024-12-11 20:48:01.069765: Epoch 86
2024-12-11 20:48:01.070435: Current learning rate: 0.00922
2024-12-11 20:49:27.895680: Validation loss did not improve from -0.55419. Patience: 10/50
2024-12-11 20:49:27.897083: train_loss -0.6782
2024-12-11 20:49:27.897820: val_loss -0.5143
2024-12-11 20:49:27.898492: Pseudo dice [0.7307]
2024-12-11 20:49:27.899137: Epoch time: 86.83 s
2024-12-11 20:49:29.102100: 
2024-12-11 20:49:29.106785: Epoch 87
2024-12-11 20:49:29.107766: Current learning rate: 0.00921
2024-12-11 20:50:55.673674: Validation loss did not improve from -0.55419. Patience: 11/50
2024-12-11 20:50:55.690101: train_loss -0.6756
2024-12-11 20:50:55.691227: val_loss -0.5415
2024-12-11 20:50:55.691955: Pseudo dice [0.735]
2024-12-11 20:50:55.692719: Epoch time: 86.59 s
2024-12-11 20:50:56.985525: 
2024-12-11 20:50:56.987083: Epoch 88
2024-12-11 20:50:56.987964: Current learning rate: 0.0092
2024-12-11 20:52:23.537891: Validation loss did not improve from -0.55419. Patience: 12/50
2024-12-11 20:52:23.539060: train_loss -0.674
2024-12-11 20:52:23.540162: val_loss -0.5104
2024-12-11 20:52:23.541097: Pseudo dice [0.724]
2024-12-11 20:52:23.542080: Epoch time: 86.55 s
2024-12-11 20:52:24.794639: 
2024-12-11 20:52:24.796400: Epoch 89
2024-12-11 20:52:24.797467: Current learning rate: 0.0092
2024-12-11 20:53:51.370524: Validation loss did not improve from -0.55419. Patience: 13/50
2024-12-11 20:53:51.371562: train_loss -0.6736
2024-12-11 20:53:51.372667: val_loss -0.5169
2024-12-11 20:53:51.373532: Pseudo dice [0.7276]
2024-12-11 20:53:51.374331: Epoch time: 86.58 s
2024-12-11 20:53:52.904913: 
2024-12-11 20:53:52.906695: Epoch 90
2024-12-11 20:53:52.907610: Current learning rate: 0.00919
2024-12-11 20:55:19.507984: Validation loss did not improve from -0.55419. Patience: 14/50
2024-12-11 20:55:19.509275: train_loss -0.6855
2024-12-11 20:55:19.510213: val_loss -0.5119
2024-12-11 20:55:19.511198: Pseudo dice [0.7257]
2024-12-11 20:55:19.512054: Epoch time: 86.61 s
2024-12-11 20:55:20.756602: 
2024-12-11 20:55:20.757982: Epoch 91
2024-12-11 20:55:20.759223: Current learning rate: 0.00918
2024-12-11 20:56:47.530650: Validation loss did not improve from -0.55419. Patience: 15/50
2024-12-11 20:56:47.532032: train_loss -0.689
2024-12-11 20:56:47.532748: val_loss -0.5273
2024-12-11 20:56:47.533685: Pseudo dice [0.7302]
2024-12-11 20:56:47.534437: Epoch time: 86.78 s
2024-12-11 20:56:48.728590: 
2024-12-11 20:56:48.730548: Epoch 92
2024-12-11 20:56:48.731560: Current learning rate: 0.00917
2024-12-11 20:58:15.494052: Validation loss did not improve from -0.55419. Patience: 16/50
2024-12-11 20:58:15.495363: train_loss -0.6866
2024-12-11 20:58:15.496173: val_loss -0.5197
2024-12-11 20:58:15.496904: Pseudo dice [0.7349]
2024-12-11 20:58:15.497701: Epoch time: 86.77 s
2024-12-11 20:58:15.498428: Yayy! New best EMA pseudo Dice: 0.7261
2024-12-11 20:58:17.476636: 
2024-12-11 20:58:17.477625: Epoch 93
2024-12-11 20:58:17.478493: Current learning rate: 0.00916
2024-12-11 20:59:44.321801: Validation loss did not improve from -0.55419. Patience: 17/50
2024-12-11 20:59:44.322664: train_loss -0.6872
2024-12-11 20:59:44.323484: val_loss -0.5201
2024-12-11 20:59:44.324176: Pseudo dice [0.727]
2024-12-11 20:59:44.324843: Epoch time: 86.85 s
2024-12-11 20:59:44.325479: Yayy! New best EMA pseudo Dice: 0.7262
2024-12-11 20:59:45.917357: 
2024-12-11 20:59:45.918916: Epoch 94
2024-12-11 20:59:45.919831: Current learning rate: 0.00915
2024-12-11 21:01:12.830932: Validation loss did not improve from -0.55419. Patience: 18/50
2024-12-11 21:01:12.831705: train_loss -0.6901
2024-12-11 21:01:12.832589: val_loss -0.5182
2024-12-11 21:01:12.833453: Pseudo dice [0.7277]
2024-12-11 21:01:12.834304: Epoch time: 86.92 s
2024-12-11 21:01:13.197998: Yayy! New best EMA pseudo Dice: 0.7263
2024-12-11 21:01:14.755819: 
2024-12-11 21:01:14.757982: Epoch 95
2024-12-11 21:01:14.759058: Current learning rate: 0.00914
2024-12-11 21:02:41.838523: Validation loss did not improve from -0.55419. Patience: 19/50
2024-12-11 21:02:41.839512: train_loss -0.6909
2024-12-11 21:02:41.840291: val_loss -0.4983
2024-12-11 21:02:41.840918: Pseudo dice [0.7246]
2024-12-11 21:02:41.841534: Epoch time: 87.09 s
2024-12-11 21:02:43.104569: 
2024-12-11 21:02:43.106349: Epoch 96
2024-12-11 21:02:43.107211: Current learning rate: 0.00913
2024-12-11 21:04:10.116903: Validation loss improved from -0.55419 to -0.57209! Patience: 19/50
2024-12-11 21:04:10.118203: train_loss -0.6881
2024-12-11 21:04:10.119088: val_loss -0.5721
2024-12-11 21:04:10.119868: Pseudo dice [0.7523]
2024-12-11 21:04:10.120535: Epoch time: 87.01 s
2024-12-11 21:04:10.121341: Yayy! New best EMA pseudo Dice: 0.7287
2024-12-11 21:04:11.668993: 
2024-12-11 21:04:11.670741: Epoch 97
2024-12-11 21:04:11.671986: Current learning rate: 0.00912
2024-12-11 21:05:38.691964: Validation loss did not improve from -0.57209. Patience: 1/50
2024-12-11 21:05:38.693250: train_loss -0.6914
2024-12-11 21:05:38.694355: val_loss -0.5269
2024-12-11 21:05:38.695068: Pseudo dice [0.7247]
2024-12-11 21:05:38.695885: Epoch time: 87.03 s
2024-12-11 21:05:39.964138: 
2024-12-11 21:05:39.965598: Epoch 98
2024-12-11 21:05:39.966427: Current learning rate: 0.00911
2024-12-11 21:07:07.143059: Validation loss improved from -0.57209 to -0.58739! Patience: 1/50
2024-12-11 21:07:07.144440: train_loss -0.7028
2024-12-11 21:07:07.145371: val_loss -0.5874
2024-12-11 21:07:07.146033: Pseudo dice [0.7642]
2024-12-11 21:07:07.146843: Epoch time: 87.18 s
2024-12-11 21:07:07.147474: Yayy! New best EMA pseudo Dice: 0.7319
2024-12-11 21:07:08.752517: 
2024-12-11 21:07:08.754561: Epoch 99
2024-12-11 21:07:08.755270: Current learning rate: 0.0091
2024-12-11 21:08:35.735259: Validation loss did not improve from -0.58739. Patience: 1/50
2024-12-11 21:08:35.736604: train_loss -0.7014
2024-12-11 21:08:35.737545: val_loss -0.5334
2024-12-11 21:08:35.738188: Pseudo dice [0.7372]
2024-12-11 21:08:35.738941: Epoch time: 86.98 s
2024-12-11 21:08:36.119865: Yayy! New best EMA pseudo Dice: 0.7325
2024-12-11 21:08:37.712186: 
2024-12-11 21:08:37.713878: Epoch 100
2024-12-11 21:08:37.714871: Current learning rate: 0.0091
2024-12-11 21:10:04.575084: Validation loss did not improve from -0.58739. Patience: 2/50
2024-12-11 21:10:04.576368: train_loss -0.7026
2024-12-11 21:10:04.577180: val_loss -0.518
2024-12-11 21:10:04.577995: Pseudo dice [0.7206]
2024-12-11 21:10:04.578763: Epoch time: 86.87 s
2024-12-11 21:10:05.839427: 
2024-12-11 21:10:05.840273: Epoch 101
2024-12-11 21:10:05.841028: Current learning rate: 0.00909
2024-12-11 21:11:32.743937: Validation loss did not improve from -0.58739. Patience: 3/50
2024-12-11 21:11:32.744920: train_loss -0.6965
2024-12-11 21:11:32.745691: val_loss -0.5227
2024-12-11 21:11:32.746445: Pseudo dice [0.7326]
2024-12-11 21:11:32.747187: Epoch time: 86.91 s
2024-12-11 21:11:34.003872: 
2024-12-11 21:11:34.005605: Epoch 102
2024-12-11 21:11:34.006609: Current learning rate: 0.00908
2024-12-11 21:13:00.762451: Validation loss did not improve from -0.58739. Patience: 4/50
2024-12-11 21:13:00.763843: train_loss -0.7046
2024-12-11 21:13:00.764772: val_loss -0.5296
2024-12-11 21:13:00.765502: Pseudo dice [0.7335]
2024-12-11 21:13:00.766335: Epoch time: 86.76 s
2024-12-11 21:13:02.058214: 
2024-12-11 21:13:02.059721: Epoch 103
2024-12-11 21:13:02.060562: Current learning rate: 0.00907
2024-12-11 21:14:28.858586: Validation loss did not improve from -0.58739. Patience: 5/50
2024-12-11 21:14:28.859828: train_loss -0.7015
2024-12-11 21:14:28.860696: val_loss -0.5159
2024-12-11 21:14:28.861311: Pseudo dice [0.7271]
2024-12-11 21:14:28.861902: Epoch time: 86.8 s
2024-12-11 21:14:30.482191: 
2024-12-11 21:14:30.483539: Epoch 104
2024-12-11 21:14:30.484222: Current learning rate: 0.00906
2024-12-11 21:15:57.416262: Validation loss did not improve from -0.58739. Patience: 6/50
2024-12-11 21:15:57.417487: train_loss -0.7017
2024-12-11 21:15:57.418354: val_loss -0.5026
2024-12-11 21:15:57.419043: Pseudo dice [0.7212]
2024-12-11 21:15:57.419633: Epoch time: 86.94 s
2024-12-11 21:15:58.970738: 
2024-12-11 21:15:58.972126: Epoch 105
2024-12-11 21:15:58.972830: Current learning rate: 0.00905
2024-12-11 21:17:25.713060: Validation loss did not improve from -0.58739. Patience: 7/50
2024-12-11 21:17:25.714285: train_loss -0.7105
2024-12-11 21:17:25.715037: val_loss -0.5245
2024-12-11 21:17:25.715802: Pseudo dice [0.736]
2024-12-11 21:17:25.716512: Epoch time: 86.74 s
2024-12-11 21:17:26.943836: 
2024-12-11 21:17:26.946512: Epoch 106
2024-12-11 21:17:26.947549: Current learning rate: 0.00904
2024-12-11 21:18:53.506111: Validation loss did not improve from -0.58739. Patience: 8/50
2024-12-11 21:18:53.507353: train_loss -0.7065
2024-12-11 21:18:53.508302: val_loss -0.5493
2024-12-11 21:18:53.508974: Pseudo dice [0.7445]
2024-12-11 21:18:53.509692: Epoch time: 86.56 s
2024-12-11 21:18:54.744622: 
2024-12-11 21:18:54.746158: Epoch 107
2024-12-11 21:18:54.746925: Current learning rate: 0.00903
2024-12-11 21:20:21.466256: Validation loss did not improve from -0.58739. Patience: 9/50
2024-12-11 21:20:21.467203: train_loss -0.7097
2024-12-11 21:20:21.468065: val_loss -0.5189
2024-12-11 21:20:21.468741: Pseudo dice [0.7343]
2024-12-11 21:20:21.469412: Epoch time: 86.72 s
2024-12-11 21:20:22.723384: 
2024-12-11 21:20:22.724498: Epoch 108
2024-12-11 21:20:22.725104: Current learning rate: 0.00902
2024-12-11 21:21:49.619008: Validation loss did not improve from -0.58739. Patience: 10/50
2024-12-11 21:21:49.620305: train_loss -0.7075
2024-12-11 21:21:49.621119: val_loss -0.5475
2024-12-11 21:21:49.621775: Pseudo dice [0.749]
2024-12-11 21:21:49.622408: Epoch time: 86.9 s
2024-12-11 21:21:49.623049: Yayy! New best EMA pseudo Dice: 0.734
2024-12-11 21:21:51.213954: 
2024-12-11 21:21:51.214909: Epoch 109
2024-12-11 21:21:51.215560: Current learning rate: 0.00901
2024-12-11 21:23:17.954467: Validation loss did not improve from -0.58739. Patience: 11/50
2024-12-11 21:23:17.955984: train_loss -0.7023
2024-12-11 21:23:17.957110: val_loss -0.5178
2024-12-11 21:23:17.957732: Pseudo dice [0.7257]
2024-12-11 21:23:17.958549: Epoch time: 86.74 s
2024-12-11 21:23:19.567103: 
2024-12-11 21:23:19.569083: Epoch 110
2024-12-11 21:23:19.569943: Current learning rate: 0.009
2024-12-11 21:24:46.243939: Validation loss did not improve from -0.58739. Patience: 12/50
2024-12-11 21:24:46.244820: train_loss -0.7094
2024-12-11 21:24:46.246069: val_loss -0.5215
2024-12-11 21:24:46.247366: Pseudo dice [0.7302]
2024-12-11 21:24:46.248287: Epoch time: 86.68 s
2024-12-11 21:24:47.460418: 
2024-12-11 21:24:47.461953: Epoch 111
2024-12-11 21:24:47.462712: Current learning rate: 0.009
2024-12-11 21:26:14.089873: Validation loss did not improve from -0.58739. Patience: 13/50
2024-12-11 21:26:14.091602: train_loss -0.7052
2024-12-11 21:26:14.092638: val_loss -0.5236
2024-12-11 21:26:14.093472: Pseudo dice [0.7346]
2024-12-11 21:26:14.094218: Epoch time: 86.63 s
2024-12-11 21:26:15.331311: 
2024-12-11 21:26:15.332897: Epoch 112
2024-12-11 21:26:15.333613: Current learning rate: 0.00899
2024-12-11 21:27:42.059881: Validation loss did not improve from -0.58739. Patience: 14/50
2024-12-11 21:27:42.060587: train_loss -0.7121
2024-12-11 21:27:42.061217: val_loss -0.4901
2024-12-11 21:27:42.061843: Pseudo dice [0.7206]
2024-12-11 21:27:42.062572: Epoch time: 86.73 s
2024-12-11 21:27:43.328747: 
2024-12-11 21:27:43.329709: Epoch 113
2024-12-11 21:27:43.330317: Current learning rate: 0.00898
2024-12-11 21:29:09.799427: Validation loss did not improve from -0.58739. Patience: 15/50
2024-12-11 21:29:09.800582: train_loss -0.7093
2024-12-11 21:29:09.801371: val_loss -0.5124
2024-12-11 21:29:09.802025: Pseudo dice [0.725]
2024-12-11 21:29:09.802760: Epoch time: 86.47 s
2024-12-11 21:29:11.006652: 
2024-12-11 21:29:11.008279: Epoch 114
2024-12-11 21:29:11.009185: Current learning rate: 0.00897
2024-12-11 21:30:37.639342: Validation loss did not improve from -0.58739. Patience: 16/50
2024-12-11 21:30:37.640596: train_loss -0.7066
2024-12-11 21:30:37.641964: val_loss -0.492
2024-12-11 21:30:37.642671: Pseudo dice [0.7077]
2024-12-11 21:30:37.643397: Epoch time: 86.63 s
2024-12-11 21:30:39.643758: 
2024-12-11 21:30:39.645680: Epoch 115
2024-12-11 21:30:39.646538: Current learning rate: 0.00896
2024-12-11 21:32:06.383514: Validation loss did not improve from -0.58739. Patience: 17/50
2024-12-11 21:32:06.384573: train_loss -0.7048
2024-12-11 21:32:06.385481: val_loss -0.5516
2024-12-11 21:32:06.386237: Pseudo dice [0.746]
2024-12-11 21:32:06.386972: Epoch time: 86.74 s
2024-12-11 21:32:07.651259: 
2024-12-11 21:32:07.652999: Epoch 116
2024-12-11 21:32:07.653669: Current learning rate: 0.00895
2024-12-11 21:33:34.283739: Validation loss did not improve from -0.58739. Patience: 18/50
2024-12-11 21:33:34.284654: train_loss -0.7098
2024-12-11 21:33:34.285563: val_loss -0.5239
2024-12-11 21:33:34.286191: Pseudo dice [0.7272]
2024-12-11 21:33:34.286830: Epoch time: 86.63 s
2024-12-11 21:33:35.546525: 
2024-12-11 21:33:35.548177: Epoch 117
2024-12-11 21:33:35.549126: Current learning rate: 0.00894
2024-12-11 21:35:02.119696: Validation loss did not improve from -0.58739. Patience: 19/50
2024-12-11 21:35:02.120748: train_loss -0.7062
2024-12-11 21:35:02.121655: val_loss -0.4947
2024-12-11 21:35:02.122560: Pseudo dice [0.7159]
2024-12-11 21:35:02.123457: Epoch time: 86.58 s
2024-12-11 21:35:03.355771: 
2024-12-11 21:35:03.357839: Epoch 118
2024-12-11 21:35:03.358770: Current learning rate: 0.00893
2024-12-11 21:36:29.578151: Validation loss did not improve from -0.58739. Patience: 20/50
2024-12-11 21:36:29.580519: train_loss -0.6974
2024-12-11 21:36:29.581797: val_loss -0.5573
2024-12-11 21:36:29.582461: Pseudo dice [0.7582]
2024-12-11 21:36:29.583300: Epoch time: 86.23 s
2024-12-11 21:36:30.868349: 
2024-12-11 21:36:30.869817: Epoch 119
2024-12-11 21:36:30.870713: Current learning rate: 0.00892
2024-12-11 21:37:57.058530: Validation loss did not improve from -0.58739. Patience: 21/50
2024-12-11 21:37:57.059628: train_loss -0.7063
2024-12-11 21:37:57.060641: val_loss -0.5619
2024-12-11 21:37:57.061312: Pseudo dice [0.7482]
2024-12-11 21:37:57.062138: Epoch time: 86.19 s
2024-12-11 21:37:58.648299: 
2024-12-11 21:37:58.649774: Epoch 120
2024-12-11 21:37:58.650500: Current learning rate: 0.00891
2024-12-11 21:39:25.221899: Validation loss did not improve from -0.58739. Patience: 22/50
2024-12-11 21:39:25.223108: train_loss -0.7095
2024-12-11 21:39:25.224252: val_loss -0.508
2024-12-11 21:39:25.225041: Pseudo dice [0.7258]
2024-12-11 21:39:25.225811: Epoch time: 86.58 s
2024-12-11 21:39:26.490041: 
2024-12-11 21:39:26.491829: Epoch 121
2024-12-11 21:39:26.492645: Current learning rate: 0.0089
2024-12-11 21:40:52.823645: Validation loss did not improve from -0.58739. Patience: 23/50
2024-12-11 21:40:52.824605: train_loss -0.7131
2024-12-11 21:40:52.825506: val_loss -0.5366
2024-12-11 21:40:52.826511: Pseudo dice [0.7363]
2024-12-11 21:40:52.827324: Epoch time: 86.34 s
2024-12-11 21:40:54.069519: 
2024-12-11 21:40:54.071070: Epoch 122
2024-12-11 21:40:54.072006: Current learning rate: 0.00889
2024-12-11 21:42:20.387603: Validation loss did not improve from -0.58739. Patience: 24/50
2024-12-11 21:42:20.388773: train_loss -0.7164
2024-12-11 21:42:20.389613: val_loss -0.5376
2024-12-11 21:42:20.390306: Pseudo dice [0.7402]
2024-12-11 21:42:20.391035: Epoch time: 86.32 s
2024-12-11 21:42:21.687221: 
2024-12-11 21:42:21.689165: Epoch 123
2024-12-11 21:42:21.690336: Current learning rate: 0.00889
2024-12-11 21:43:47.992764: Validation loss did not improve from -0.58739. Patience: 25/50
2024-12-11 21:43:47.993700: train_loss -0.7155
2024-12-11 21:43:47.994705: val_loss -0.4981
2024-12-11 21:43:47.995491: Pseudo dice [0.7199]
2024-12-11 21:43:47.996244: Epoch time: 86.31 s
2024-12-11 21:43:49.257102: 
2024-12-11 21:43:49.259125: Epoch 124
2024-12-11 21:43:49.259949: Current learning rate: 0.00888
2024-12-11 21:45:15.519264: Validation loss did not improve from -0.58739. Patience: 26/50
2024-12-11 21:45:15.520604: train_loss -0.7216
2024-12-11 21:45:15.521847: val_loss -0.5195
2024-12-11 21:45:15.522529: Pseudo dice [0.7323]
2024-12-11 21:45:15.523186: Epoch time: 86.26 s
2024-12-11 21:45:17.098361: 
2024-12-11 21:45:17.099601: Epoch 125
2024-12-11 21:45:17.100368: Current learning rate: 0.00887
2024-12-11 21:46:43.276416: Validation loss did not improve from -0.58739. Patience: 27/50
2024-12-11 21:46:43.277542: train_loss -0.7252
2024-12-11 21:46:43.278358: val_loss -0.5188
2024-12-11 21:46:43.279125: Pseudo dice [0.7388]
2024-12-11 21:46:43.279798: Epoch time: 86.18 s
2024-12-11 21:46:44.863223: 
2024-12-11 21:46:44.865179: Epoch 126
2024-12-11 21:46:44.865936: Current learning rate: 0.00886
2024-12-11 21:48:11.178994: Validation loss did not improve from -0.58739. Patience: 28/50
2024-12-11 21:48:11.180146: train_loss -0.7213
2024-12-11 21:48:11.181204: val_loss -0.5106
2024-12-11 21:48:11.182025: Pseudo dice [0.7359]
2024-12-11 21:48:11.182722: Epoch time: 86.32 s
2024-12-11 21:48:12.444567: 
2024-12-11 21:48:12.445975: Epoch 127
2024-12-11 21:48:12.446778: Current learning rate: 0.00885
2024-12-11 21:49:38.783489: Validation loss did not improve from -0.58739. Patience: 29/50
2024-12-11 21:49:38.784729: train_loss -0.7246
2024-12-11 21:49:38.785775: val_loss -0.5699
2024-12-11 21:49:38.786556: Pseudo dice [0.7593]
2024-12-11 21:49:38.787394: Epoch time: 86.34 s
2024-12-11 21:49:38.788193: Yayy! New best EMA pseudo Dice: 0.7359
2024-12-11 21:49:40.400589: 
2024-12-11 21:49:40.402615: Epoch 128
2024-12-11 21:49:40.403396: Current learning rate: 0.00884
2024-12-11 21:51:07.142627: Validation loss did not improve from -0.58739. Patience: 30/50
2024-12-11 21:51:07.143840: train_loss -0.7165
2024-12-11 21:51:07.144991: val_loss -0.5373
2024-12-11 21:51:07.145757: Pseudo dice [0.7489]
2024-12-11 21:51:07.146506: Epoch time: 86.74 s
2024-12-11 21:51:07.147322: Yayy! New best EMA pseudo Dice: 0.7372
2024-12-11 21:51:08.707057: 
2024-12-11 21:51:08.708562: Epoch 129
2024-12-11 21:51:08.709337: Current learning rate: 0.00883
2024-12-11 21:52:35.048935: Validation loss did not improve from -0.58739. Patience: 31/50
2024-12-11 21:52:35.050075: train_loss -0.7283
2024-12-11 21:52:35.050981: val_loss -0.532
2024-12-11 21:52:35.051821: Pseudo dice [0.7391]
2024-12-11 21:52:35.052474: Epoch time: 86.34 s
2024-12-11 21:52:35.417528: Yayy! New best EMA pseudo Dice: 0.7374
2024-12-11 21:52:36.982136: 
2024-12-11 21:52:36.983984: Epoch 130
2024-12-11 21:52:36.984683: Current learning rate: 0.00882
2024-12-11 21:54:03.890011: Validation loss did not improve from -0.58739. Patience: 32/50
2024-12-11 21:54:03.891187: train_loss -0.7241
2024-12-11 21:54:03.892048: val_loss -0.504
2024-12-11 21:54:03.892864: Pseudo dice [0.724]
2024-12-11 21:54:03.893662: Epoch time: 86.91 s
2024-12-11 21:54:05.180209: 
2024-12-11 21:54:05.181640: Epoch 131
2024-12-11 21:54:05.182389: Current learning rate: 0.00881
2024-12-11 21:55:31.568769: Validation loss did not improve from -0.58739. Patience: 33/50
2024-12-11 21:55:31.575136: train_loss -0.7172
2024-12-11 21:55:31.578264: val_loss -0.5218
2024-12-11 21:55:31.579252: Pseudo dice [0.7389]
2024-12-11 21:55:31.580683: Epoch time: 86.4 s
2024-12-11 21:55:32.819048: 
2024-12-11 21:55:32.820784: Epoch 132
2024-12-11 21:55:32.821665: Current learning rate: 0.0088
2024-12-11 21:56:59.053038: Validation loss did not improve from -0.58739. Patience: 34/50
2024-12-11 21:56:59.053894: train_loss -0.7231
2024-12-11 21:56:59.054977: val_loss -0.495
2024-12-11 21:56:59.055927: Pseudo dice [0.7231]
2024-12-11 21:56:59.056847: Epoch time: 86.24 s
2024-12-11 21:57:00.293702: 
2024-12-11 21:57:00.295365: Epoch 133
2024-12-11 21:57:00.296396: Current learning rate: 0.00879
2024-12-11 21:58:26.472724: Validation loss did not improve from -0.58739. Patience: 35/50
2024-12-11 21:58:26.474111: train_loss -0.7183
2024-12-11 21:58:26.475312: val_loss -0.5006
2024-12-11 21:58:26.476286: Pseudo dice [0.7181]
2024-12-11 21:58:26.477278: Epoch time: 86.18 s
2024-12-11 21:58:27.720134: 
2024-12-11 21:58:27.721299: Epoch 134
2024-12-11 21:58:27.722225: Current learning rate: 0.00879
2024-12-11 21:59:53.983300: Validation loss did not improve from -0.58739. Patience: 36/50
2024-12-11 21:59:53.984431: train_loss -0.7199
2024-12-11 21:59:53.985451: val_loss -0.5215
2024-12-11 21:59:53.986107: Pseudo dice [0.7291]
2024-12-11 21:59:53.986788: Epoch time: 86.27 s
2024-12-11 21:59:55.573626: 
2024-12-11 21:59:55.575309: Epoch 135
2024-12-11 21:59:55.576100: Current learning rate: 0.00878
2024-12-11 22:01:21.965440: Validation loss did not improve from -0.58739. Patience: 37/50
2024-12-11 22:01:21.966750: train_loss -0.7248
2024-12-11 22:01:21.967829: val_loss -0.53
2024-12-11 22:01:21.968782: Pseudo dice [0.7373]
2024-12-11 22:01:21.969474: Epoch time: 86.39 s
2024-12-11 22:01:23.300769: 
2024-12-11 22:01:23.302613: Epoch 136
2024-12-11 22:01:23.303473: Current learning rate: 0.00877
2024-12-11 22:02:49.680734: Validation loss did not improve from -0.58739. Patience: 38/50
2024-12-11 22:02:49.681881: train_loss -0.7204
2024-12-11 22:02:49.682806: val_loss -0.5401
2024-12-11 22:02:49.683603: Pseudo dice [0.7385]
2024-12-11 22:02:49.684374: Epoch time: 86.38 s
2024-12-11 22:02:51.624754: 
2024-12-11 22:02:51.626521: Epoch 137
2024-12-11 22:02:51.627352: Current learning rate: 0.00876
2024-12-11 22:04:18.028883: Validation loss did not improve from -0.58739. Patience: 39/50
2024-12-11 22:04:18.030637: train_loss -0.7121
2024-12-11 22:04:18.031818: val_loss -0.5263
2024-12-11 22:04:18.032605: Pseudo dice [0.7353]
2024-12-11 22:04:18.033398: Epoch time: 86.41 s
2024-12-11 22:04:19.281388: 
2024-12-11 22:04:19.282912: Epoch 138
2024-12-11 22:04:19.283770: Current learning rate: 0.00875
2024-12-11 22:05:45.742464: Validation loss did not improve from -0.58739. Patience: 40/50
2024-12-11 22:05:45.743695: train_loss -0.726
2024-12-11 22:05:45.745003: val_loss -0.54
2024-12-11 22:05:45.745925: Pseudo dice [0.741]
2024-12-11 22:05:45.746794: Epoch time: 86.46 s
2024-12-11 22:05:47.003828: 
2024-12-11 22:05:47.004898: Epoch 139
2024-12-11 22:05:47.005752: Current learning rate: 0.00874
2024-12-11 22:07:13.486684: Validation loss did not improve from -0.58739. Patience: 41/50
2024-12-11 22:07:13.487674: train_loss -0.7371
2024-12-11 22:07:13.488706: val_loss -0.5462
2024-12-11 22:07:13.489421: Pseudo dice [0.7501]
2024-12-11 22:07:13.490074: Epoch time: 86.48 s
2024-12-11 22:07:15.135565: 
2024-12-11 22:07:15.136773: Epoch 140
2024-12-11 22:07:15.137487: Current learning rate: 0.00873
2024-12-11 22:08:41.742136: Validation loss did not improve from -0.58739. Patience: 42/50
2024-12-11 22:08:41.743391: train_loss -0.7288
2024-12-11 22:08:41.744347: val_loss -0.5342
2024-12-11 22:08:41.745376: Pseudo dice [0.7412]
2024-12-11 22:08:41.746141: Epoch time: 86.61 s
2024-12-11 22:08:43.047025: 
2024-12-11 22:08:43.048460: Epoch 141
2024-12-11 22:08:43.049453: Current learning rate: 0.00872
2024-12-11 22:10:09.605620: Validation loss did not improve from -0.58739. Patience: 43/50
2024-12-11 22:10:09.606945: train_loss -0.7272
2024-12-11 22:10:09.608435: val_loss -0.5626
2024-12-11 22:10:09.610104: Pseudo dice [0.7585]
2024-12-11 22:10:09.611609: Epoch time: 86.56 s
2024-12-11 22:10:09.612817: Yayy! New best EMA pseudo Dice: 0.7389
2024-12-11 22:10:11.245342: 
2024-12-11 22:10:11.246666: Epoch 142
2024-12-11 22:10:11.247389: Current learning rate: 0.00871
2024-12-11 22:11:37.784180: Validation loss did not improve from -0.58739. Patience: 44/50
2024-12-11 22:11:37.785321: train_loss -0.7292
2024-12-11 22:11:37.786280: val_loss -0.5224
2024-12-11 22:11:37.786934: Pseudo dice [0.7358]
2024-12-11 22:11:37.787695: Epoch time: 86.54 s
2024-12-11 22:11:39.064596: 
2024-12-11 22:11:39.066921: Epoch 143
2024-12-11 22:11:39.067768: Current learning rate: 0.0087
2024-12-11 22:13:05.555979: Validation loss did not improve from -0.58739. Patience: 45/50
2024-12-11 22:13:05.556955: train_loss -0.7338
2024-12-11 22:13:05.557954: val_loss -0.5315
2024-12-11 22:13:05.558612: Pseudo dice [0.7431]
2024-12-11 22:13:05.559355: Epoch time: 86.49 s
2024-12-11 22:13:05.560032: Yayy! New best EMA pseudo Dice: 0.739
2024-12-11 22:13:07.177445: 
2024-12-11 22:13:07.179061: Epoch 144
2024-12-11 22:13:07.179864: Current learning rate: 0.00869
2024-12-11 22:14:33.649193: Validation loss did not improve from -0.58739. Patience: 46/50
2024-12-11 22:14:33.650284: train_loss -0.7339
2024-12-11 22:14:33.651492: val_loss -0.5102
2024-12-11 22:14:33.652456: Pseudo dice [0.7314]
2024-12-11 22:14:33.653405: Epoch time: 86.47 s
2024-12-11 22:14:35.242857: 
2024-12-11 22:14:35.244776: Epoch 145
2024-12-11 22:14:35.245954: Current learning rate: 0.00868
2024-12-11 22:16:01.622993: Validation loss did not improve from -0.58739. Patience: 47/50
2024-12-11 22:16:01.623900: train_loss -0.7383
2024-12-11 22:16:01.624743: val_loss -0.5101
2024-12-11 22:16:01.625465: Pseudo dice [0.7238]
2024-12-11 22:16:01.626229: Epoch time: 86.38 s
2024-12-11 22:16:02.878023: 
2024-12-11 22:16:02.879806: Epoch 146
2024-12-11 22:16:02.880613: Current learning rate: 0.00868
2024-12-11 22:17:29.130878: Validation loss did not improve from -0.58739. Patience: 48/50
2024-12-11 22:17:29.132192: train_loss -0.7298
2024-12-11 22:17:29.133708: val_loss -0.4794
2024-12-11 22:17:29.134649: Pseudo dice [0.7267]
2024-12-11 22:17:29.135670: Epoch time: 86.26 s
2024-12-11 22:17:30.743475: 
2024-12-11 22:17:30.745500: Epoch 147
2024-12-11 22:17:30.746885: Current learning rate: 0.00867
2024-12-11 22:18:57.266120: Validation loss did not improve from -0.58739. Patience: 49/50
2024-12-11 22:18:57.267077: train_loss -0.7294
2024-12-11 22:18:57.268001: val_loss -0.5466
2024-12-11 22:18:57.268633: Pseudo dice [0.7444]
2024-12-11 22:18:57.269260: Epoch time: 86.52 s
2024-12-11 22:18:58.520975: 
2024-12-11 22:18:58.523484: Epoch 148
2024-12-11 22:18:58.524991: Current learning rate: 0.00866
2024-12-11 22:20:25.427625: Validation loss did not improve from -0.58739. Patience: 50/50
2024-12-11 22:20:25.429634: train_loss -0.735
2024-12-11 22:20:25.430478: val_loss -0.5446
2024-12-11 22:20:25.431571: Pseudo dice [0.7434]
2024-12-11 22:20:25.432378: Epoch time: 86.91 s
2024-12-11 22:20:26.694913: Patience reached. Stopping training.
2024-12-11 22:20:27.071031: Training done.
2024-12-11 22:20:27.191530: Using splits from existing split file: /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset307_Sohee_Calcium_OCT_CrossValidation/splits_final.json
2024-12-11 22:20:27.193710: The split file contains 5 splits.
2024-12-11 22:20:27.194690: Desired fold for training: 1
2024-12-11 22:20:27.195616: This split has 6 training and 2 validation cases.
2024-12-11 22:20:27.196723: predicting 101-019
2024-12-11 22:20:27.208840: 101-019, shape torch.Size([1, 375, 498, 498]), rank 0
2024-12-11 22:22:05.237429: predicting 401-004
2024-12-11 22:22:05.262431: 401-004, shape torch.Size([1, 375, 498, 498]), rank 0
2024-12-11 22:23:51.660466: Validation complete
2024-12-11 22:23:51.661716: Mean Validation Dice:  0.7197543600486964

############################
INFO: You are using the old nnU-Net default plans. We have updated our recommendations. Please consider using those instead! Read more here: https://github.com/MIC-DKFZ/nnUNet/blob/master/documentation/resenc_presets.md
############################

Using device: cuda:0

#######################################################################
Please cite the following paper when using nnU-Net:
Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.
#######################################################################

2024-12-11 22:23:58.483013: Using torch.compile...

############################
INFO: You are using the old nnU-Net default plans. We have updated our recommendations. Please consider using those instead! Read more here: https://github.com/MIC-DKFZ/nnUNet/blob/master/documentation/resenc_presets.md
############################

Using device: cuda:0

#######################################################################
Please cite the following paper when using nnU-Net:
Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.
#######################################################################

2024-12-11 22:23:58.482168: Using torch.compile...
/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
################### Loading pretrained weights from file  /home/gridsan/nchutisilp/projects/OpenAI-CLIP/clip_pretrained_nnUNet.pt ###################
Below is the list of overlapping blocks in pretrained model and nnUNet architecture:
encoder.stages.0.0.convs.0.conv.weight shape torch.Size([32, 1, 3, 3, 3])
encoder.stages.0.0.convs.0.conv.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.norm.weight shape torch.Size([32])
encoder.stages.0.0.convs.0.norm.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.0.weight shape torch.Size([32, 1, 3, 3, 3])
encoder.stages.0.0.convs.0.all_modules.0.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.1.weight shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.1.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
encoder.stages.0.0.convs.1.conv.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.norm.weight shape torch.Size([32])
encoder.stages.0.0.convs.1.norm.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
encoder.stages.0.0.convs.1.all_modules.0.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.1.weight shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.1.bias shape torch.Size([32])
encoder.stages.1.0.convs.0.conv.weight shape torch.Size([64, 32, 3, 3, 3])
encoder.stages.1.0.convs.0.conv.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.norm.weight shape torch.Size([64])
encoder.stages.1.0.convs.0.norm.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.0.weight shape torch.Size([64, 32, 3, 3, 3])
encoder.stages.1.0.convs.0.all_modules.0.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.1.weight shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.1.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
encoder.stages.1.0.convs.1.conv.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.norm.weight shape torch.Size([64])
encoder.stages.1.0.convs.1.norm.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
encoder.stages.1.0.convs.1.all_modules.0.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.1.weight shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.1.bias shape torch.Size([64])
encoder.stages.2.0.convs.0.conv.weight shape torch.Size([128, 64, 3, 3, 3])
encoder.stages.2.0.convs.0.conv.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.norm.weight shape torch.Size([128])
encoder.stages.2.0.convs.0.norm.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.0.weight shape torch.Size([128, 64, 3, 3, 3])
encoder.stages.2.0.convs.0.all_modules.0.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.1.weight shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.1.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
encoder.stages.2.0.convs.1.conv.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.norm.weight shape torch.Size([128])
encoder.stages.2.0.convs.1.norm.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
encoder.stages.2.0.convs.1.all_modules.0.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.1.weight shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.1.bias shape torch.Size([128])
encoder.stages.3.0.convs.0.conv.weight shape torch.Size([256, 128, 3, 3, 3])
encoder.stages.3.0.convs.0.conv.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.norm.weight shape torch.Size([256])
encoder.stages.3.0.convs.0.norm.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.0.weight shape torch.Size([256, 128, 3, 3, 3])
encoder.stages.3.0.convs.0.all_modules.0.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.1.weight shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.1.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
encoder.stages.3.0.convs.1.conv.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.norm.weight shape torch.Size([256])
encoder.stages.3.0.convs.1.norm.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
encoder.stages.3.0.convs.1.all_modules.0.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.1.weight shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.1.bias shape torch.Size([256])
encoder.stages.4.0.convs.0.conv.weight shape torch.Size([320, 256, 3, 3, 3])
encoder.stages.4.0.convs.0.conv.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.norm.weight shape torch.Size([320])
encoder.stages.4.0.convs.0.norm.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.0.weight shape torch.Size([320, 256, 3, 3, 3])
encoder.stages.4.0.convs.0.all_modules.0.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.1.weight shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.1.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.4.0.convs.1.conv.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.norm.weight shape torch.Size([320])
encoder.stages.4.0.convs.1.norm.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.4.0.convs.1.all_modules.0.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.1.weight shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.1.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.0.conv.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.norm.weight shape torch.Size([320])
encoder.stages.5.0.convs.0.norm.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.0.all_modules.0.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.1.weight shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.1.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.1.conv.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.norm.weight shape torch.Size([320])
encoder.stages.5.0.convs.1.norm.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.1.all_modules.0.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.1.weight shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.0.0.convs.0.conv.weight shape torch.Size([32, 1, 3, 3, 3])
decoder.encoder.stages.0.0.convs.0.conv.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.norm.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.norm.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.0.weight shape torch.Size([32, 1, 3, 3, 3])
decoder.encoder.stages.0.0.convs.0.all_modules.0.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.1.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.1.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.encoder.stages.0.0.convs.1.conv.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.norm.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.norm.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.encoder.stages.0.0.convs.1.all_modules.0.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.1.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.1.bias shape torch.Size([32])
decoder.encoder.stages.1.0.convs.0.conv.weight shape torch.Size([64, 32, 3, 3, 3])
decoder.encoder.stages.1.0.convs.0.conv.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.norm.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.norm.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.0.weight shape torch.Size([64, 32, 3, 3, 3])
decoder.encoder.stages.1.0.convs.0.all_modules.0.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.1.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.1.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.encoder.stages.1.0.convs.1.conv.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.norm.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.norm.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.encoder.stages.1.0.convs.1.all_modules.0.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.1.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.1.bias shape torch.Size([64])
decoder.encoder.stages.2.0.convs.0.conv.weight shape torch.Size([128, 64, 3, 3, 3])
decoder.encoder.stages.2.0.convs.0.conv.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.norm.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.norm.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.0.weight shape torch.Size([128, 64, 3, 3, 3])
decoder.encoder.stages.2.0.convs.0.all_modules.0.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.1.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.1.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.encoder.stages.2.0.convs.1.conv.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.norm.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.norm.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.encoder.stages.2.0.convs.1.all_modules.0.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.1.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.1.bias shape torch.Size([128])
decoder.encoder.stages.3.0.convs.0.conv.weight shape torch.Size([256, 128, 3, 3, 3])
decoder.encoder.stages.3.0.convs.0.conv.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.norm.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.norm.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.0.weight shape torch.Size([256, 128, 3, 3, 3])
decoder.encoder.stages.3.0.convs.0.all_modules.0.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.1.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.1.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.encoder.stages.3.0.convs.1.conv.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.norm.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.norm.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.encoder.stages.3.0.convs.1.all_modules.0.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.1.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.1.bias shape torch.Size([256])
decoder.encoder.stages.4.0.convs.0.conv.weight shape torch.Size([320, 256, 3, 3, 3])
decoder.encoder.stages.4.0.convs.0.conv.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.norm.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.norm.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.0.weight shape torch.Size([320, 256, 3, 3, 3])
decoder.encoder.stages.4.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.4.0.convs.1.conv.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.norm.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.norm.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.4.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.0.conv.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.norm.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.norm.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.1.conv.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.norm.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.norm.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.stages.0.convs.0.conv.weight shape torch.Size([320, 640, 3, 3, 3])
decoder.stages.0.convs.0.conv.bias shape torch.Size([320])
decoder.stages.0.convs.0.norm.weight shape torch.Size([320])
decoder.stages.0.convs.0.norm.bias shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.0.weight shape torch.Size([320, 640, 3, 3, 3])
decoder.stages.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.stages.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.stages.0.convs.1.conv.bias shape torch.Size([320])
decoder.stages.0.convs.1.norm.weight shape torch.Size([320])
decoder.stages.0.convs.1.norm.bias shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.stages.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.stages.1.convs.0.conv.weight shape torch.Size([256, 512, 3, 3, 3])
decoder.stages.1.convs.0.conv.bias shape torch.Size([256])
decoder.stages.1.convs.0.norm.weight shape torch.Size([256])
decoder.stages.1.convs.0.norm.bias shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.0.weight shape torch.Size([256, 512, 3, 3, 3])
decoder.stages.1.convs.0.all_modules.0.bias shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.1.weight shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.1.bias shape torch.Size([256])
decoder.stages.1.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.stages.1.convs.1.conv.bias shape torch.Size([256])
decoder.stages.1.convs.1.norm.weight shape torch.Size([256])
decoder.stages.1.convs.1.norm.bias shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.stages.1.convs.1.all_modules.0.bias shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.1.weight shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.1.bias shape torch.Size([256])
decoder.stages.2.convs.0.conv.weight shape torch.Size([128, 256, 3, 3, 3])
decoder.stages.2.convs.0.conv.bias shape torch.Size([128])
decoder.stages.2.convs.0.norm.weight shape torch.Size([128])
decoder.stages.2.convs.0.norm.bias shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.0.weight shape torch.Size([128, 256, 3, 3, 3])
decoder.stages.2.convs.0.all_modules.0.bias shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.1.weight shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.1.bias shape torch.Size([128])
decoder.stages.2.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.stages.2.convs.1.conv.bias shape torch.Size([128])
decoder.stages.2.convs.1.norm.weight shape torch.Size([128])
decoder.stages.2.convs.1.norm.bias shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.stages.2.convs.1.all_modules.0.bias shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.1.weight shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.1.bias shape torch.Size([128])
decoder.stages.3.convs.0.conv.weight shape torch.Size([64, 128, 3, 3, 3])
decoder.stages.3.convs.0.conv.bias shape torch.Size([64])
decoder.stages.3.convs.0.norm.weight shape torch.Size([64])
decoder.stages.3.convs.0.norm.bias shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.0.weight shape torch.Size([64, 128, 3, 3, 3])
decoder.stages.3.convs.0.all_modules.0.bias shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.1.weight shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.1.bias shape torch.Size([64])
decoder.stages.3.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.stages.3.convs.1.conv.bias shape torch.Size([64])
decoder.stages.3.convs.1.norm.weight shape torch.Size([64])
decoder.stages.3.convs.1.norm.bias shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.stages.3.convs.1.all_modules.0.bias shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.1.weight shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.1.bias shape torch.Size([64])
decoder.stages.4.convs.0.conv.weight shape torch.Size([32, 64, 3, 3, 3])
decoder.stages.4.convs.0.conv.bias shape torch.Size([32])
decoder.stages.4.convs.0.norm.weight shape torch.Size([32])
decoder.stages.4.convs.0.norm.bias shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.0.weight shape torch.Size([32, 64, 3, 3, 3])
decoder.stages.4.convs.0.all_modules.0.bias shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.1.weight shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.1.bias shape torch.Size([32])
decoder.stages.4.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.stages.4.convs.1.conv.bias shape torch.Size([32])
decoder.stages.4.convs.1.norm.weight shape torch.Size([32])
decoder.stages.4.convs.1.norm.bias shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.stages.4.convs.1.all_modules.0.bias shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.1.weight shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.1.bias shape torch.Size([32])
decoder.transpconvs.0.weight shape torch.Size([320, 320, 1, 2, 2])
decoder.transpconvs.0.bias shape torch.Size([320])
decoder.transpconvs.1.weight shape torch.Size([320, 256, 2, 2, 2])
decoder.transpconvs.1.bias shape torch.Size([256])
decoder.transpconvs.2.weight shape torch.Size([256, 128, 2, 2, 2])
decoder.transpconvs.2.bias shape torch.Size([128])
decoder.transpconvs.3.weight shape torch.Size([128, 64, 2, 2, 2])
decoder.transpconvs.3.bias shape torch.Size([64])
decoder.transpconvs.4.weight shape torch.Size([64, 32, 2, 2, 2])
decoder.transpconvs.4.bias shape torch.Size([32])
################### Done ###################
2024-12-11 22:24:12.025863: do_dummy_2d_data_aug: True
2024-12-11 22:24:12.027931: Using splits from existing split file: /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset307_Sohee_Calcium_OCT_CrossValidation/splits_final.json
2024-12-11 22:24:12.029403: The split file contains 5 splits.
2024-12-11 22:24:12.030209: Desired fold for training: 3
2024-12-11 22:24:12.030975: This split has 7 training and 1 validation cases.
################### Loading pretrained weights from file  /home/gridsan/nchutisilp/projects/OpenAI-CLIP/clip_pretrained_nnUNet.pt ###################
Below is the list of overlapping blocks in pretrained model and nnUNet architecture:
encoder.stages.0.0.convs.0.conv.weight shape torch.Size([32, 1, 3, 3, 3])
encoder.stages.0.0.convs.0.conv.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.norm.weight shape torch.Size([32])
encoder.stages.0.0.convs.0.norm.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.0.weight shape torch.Size([32, 1, 3, 3, 3])
encoder.stages.0.0.convs.0.all_modules.0.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.1.weight shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.1.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
encoder.stages.0.0.convs.1.conv.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.norm.weight shape torch.Size([32])
encoder.stages.0.0.convs.1.norm.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
encoder.stages.0.0.convs.1.all_modules.0.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.1.weight shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.1.bias shape torch.Size([32])
encoder.stages.1.0.convs.0.conv.weight shape torch.Size([64, 32, 3, 3, 3])
encoder.stages.1.0.convs.0.conv.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.norm.weight shape torch.Size([64])
encoder.stages.1.0.convs.0.norm.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.0.weight shape torch.Size([64, 32, 3, 3, 3])
encoder.stages.1.0.convs.0.all_modules.0.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.1.weight shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.1.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
encoder.stages.1.0.convs.1.conv.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.norm.weight shape torch.Size([64])
encoder.stages.1.0.convs.1.norm.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
encoder.stages.1.0.convs.1.all_modules.0.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.1.weight shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.1.bias shape torch.Size([64])
encoder.stages.2.0.convs.0.conv.weight shape torch.Size([128, 64, 3, 3, 3])
encoder.stages.2.0.convs.0.conv.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.norm.weight shape torch.Size([128])
encoder.stages.2.0.convs.0.norm.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.0.weight shape torch.Size([128, 64, 3, 3, 3])
encoder.stages.2.0.convs.0.all_modules.0.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.1.weight shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.1.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
encoder.stages.2.0.convs.1.conv.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.norm.weight shape torch.Size([128])
encoder.stages.2.0.convs.1.norm.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
encoder.stages.2.0.convs.1.all_modules.0.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.1.weight shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.1.bias shape torch.Size([128])
encoder.stages.3.0.convs.0.conv.weight shape torch.Size([256, 128, 3, 3, 3])
encoder.stages.3.0.convs.0.conv.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.norm.weight shape torch.Size([256])
encoder.stages.3.0.convs.0.norm.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.0.weight shape torch.Size([256, 128, 3, 3, 3])
encoder.stages.3.0.convs.0.all_modules.0.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.1.weight shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.1.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
encoder.stages.3.0.convs.1.conv.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.norm.weight shape torch.Size([256])
encoder.stages.3.0.convs.1.norm.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
encoder.stages.3.0.convs.1.all_modules.0.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.1.weight shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.1.bias shape torch.Size([256])
encoder.stages.4.0.convs.0.conv.weight shape torch.Size([320, 256, 3, 3, 3])
encoder.stages.4.0.convs.0.conv.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.norm.weight shape torch.Size([320])
encoder.stages.4.0.convs.0.norm.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.0.weight shape torch.Size([320, 256, 3, 3, 3])
encoder.stages.4.0.convs.0.all_modules.0.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.1.weight shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.1.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.4.0.convs.1.conv.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.norm.weight shape torch.Size([320])
encoder.stages.4.0.convs.1.norm.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.4.0.convs.1.all_modules.0.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.1.weight shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.1.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.0.conv.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.norm.weight shape torch.Size([320])
encoder.stages.5.0.convs.0.norm.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.0.all_modules.0.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.1.weight shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.1.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.1.conv.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.norm.weight shape torch.Size([320])
encoder.stages.5.0.convs.1.norm.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.1.all_modules.0.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.1.weight shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.0.0.convs.0.conv.weight shape torch.Size([32, 1, 3, 3, 3])
decoder.encoder.stages.0.0.convs.0.conv.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.norm.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.norm.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.0.weight shape torch.Size([32, 1, 3, 3, 3])
decoder.encoder.stages.0.0.convs.0.all_modules.0.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.1.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.1.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.encoder.stages.0.0.convs.1.conv.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.norm.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.norm.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.encoder.stages.0.0.convs.1.all_modules.0.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.1.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.1.bias shape torch.Size([32])
decoder.encoder.stages.1.0.convs.0.conv.weight shape torch.Size([64, 32, 3, 3, 3])
decoder.encoder.stages.1.0.convs.0.conv.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.norm.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.norm.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.0.weight shape torch.Size([64, 32, 3, 3, 3])
decoder.encoder.stages.1.0.convs.0.all_modules.0.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.1.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.1.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.encoder.stages.1.0.convs.1.conv.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.norm.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.norm.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.encoder.stages.1.0.convs.1.all_modules.0.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.1.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.1.bias shape torch.Size([64])
decoder.encoder.stages.2.0.convs.0.conv.weight shape torch.Size([128, 64, 3, 3, 3])
decoder.encoder.stages.2.0.convs.0.conv.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.norm.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.norm.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.0.weight shape torch.Size([128, 64, 3, 3, 3])
decoder.encoder.stages.2.0.convs.0.all_modules.0.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.1.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.1.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.encoder.stages.2.0.convs.1.conv.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.norm.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.norm.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.encoder.stages.2.0.convs.1.all_modules.0.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.1.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.1.bias shape torch.Size([128])
decoder.encoder.stages.3.0.convs.0.conv.weight shape torch.Size([256, 128, 3, 3, 3])
decoder.encoder.stages.3.0.convs.0.conv.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.norm.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.norm.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.0.weight shape torch.Size([256, 128, 3, 3, 3])
decoder.encoder.stages.3.0.convs.0.all_modules.0.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.1.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.1.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.encoder.stages.3.0.convs.1.conv.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.norm.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.norm.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.encoder.stages.3.0.convs.1.all_modules.0.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.1.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.1.bias shape torch.Size([256])
decoder.encoder.stages.4.0.convs.0.conv.weight shape torch.Size([320, 256, 3, 3, 3])
decoder.encoder.stages.4.0.convs.0.conv.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.norm.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.norm.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.0.weight shape torch.Size([320, 256, 3, 3, 3])
decoder.encoder.stages.4.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.4.0.convs.1.conv.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.norm.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.norm.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.4.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.0.conv.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.norm.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.norm.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.1.conv.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.norm.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.norm.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.stages.0.convs.0.conv.weight shape torch.Size([320, 640, 3, 3, 3])
decoder.stages.0.convs.0.conv.bias shape torch.Size([320])
decoder.stages.0.convs.0.norm.weight shape torch.Size([320])
decoder.stages.0.convs.0.norm.bias shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.0.weight shape torch.Size([320, 640, 3, 3, 3])
decoder.stages.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.stages.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.stages.0.convs.1.conv.bias shape torch.Size([320])
decoder.stages.0.convs.1.norm.weight shape torch.Size([320])
decoder.stages.0.convs.1.norm.bias shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.stages.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.stages.1.convs.0.conv.weight shape torch.Size([256, 512, 3, 3, 3])
decoder.stages.1.convs.0.conv.bias shape torch.Size([256])
decoder.stages.1.convs.0.norm.weight shape torch.Size([256])
decoder.stages.1.convs.0.norm.bias shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.0.weight shape torch.Size([256, 512, 3, 3, 3])
decoder.stages.1.convs.0.all_modules.0.bias shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.1.weight shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.1.bias shape torch.Size([256])
decoder.stages.1.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.stages.1.convs.1.conv.bias shape torch.Size([256])
decoder.stages.1.convs.1.norm.weight shape torch.Size([256])
decoder.stages.1.convs.1.norm.bias shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.stages.1.convs.1.all_modules.0.bias shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.1.weight shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.1.bias shape torch.Size([256])
decoder.stages.2.convs.0.conv.weight shape torch.Size([128, 256, 3, 3, 3])
decoder.stages.2.convs.0.conv.bias shape torch.Size([128])
decoder.stages.2.convs.0.norm.weight shape torch.Size([128])
decoder.stages.2.convs.0.norm.bias shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.0.weight shape torch.Size([128, 256, 3, 3, 3])
decoder.stages.2.convs.0.all_modules.0.bias shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.1.weight shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.1.bias shape torch.Size([128])
decoder.stages.2.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.stages.2.convs.1.conv.bias shape torch.Size([128])
decoder.stages.2.convs.1.norm.weight shape torch.Size([128])
decoder.stages.2.convs.1.norm.bias shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.stages.2.convs.1.all_modules.0.bias shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.1.weight shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.1.bias shape torch.Size([128])
decoder.stages.3.convs.0.conv.weight shape torch.Size([64, 128, 3, 3, 3])
decoder.stages.3.convs.0.conv.bias shape torch.Size([64])
decoder.stages.3.convs.0.norm.weight shape torch.Size([64])
decoder.stages.3.convs.0.norm.bias shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.0.weight shape torch.Size([64, 128, 3, 3, 3])
decoder.stages.3.convs.0.all_modules.0.bias shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.1.weight shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.1.bias shape torch.Size([64])
decoder.stages.3.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.stages.3.convs.1.conv.bias shape torch.Size([64])
decoder.stages.3.convs.1.norm.weight shape torch.Size([64])
decoder.stages.3.convs.1.norm.bias shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.stages.3.convs.1.all_modules.0.bias shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.1.weight shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.1.bias shape torch.Size([64])
decoder.stages.4.convs.0.conv.weight shape torch.Size([32, 64, 3, 3, 3])
decoder.stages.4.convs.0.conv.bias shape torch.Size([32])
decoder.stages.4.convs.0.norm.weight shape torch.Size([32])
decoder.stages.4.convs.0.norm.bias shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.0.weight shape torch.Size([32, 64, 3, 3, 3])
decoder.stages.4.convs.0.all_modules.0.bias shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.1.weight shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.1.bias shape torch.Size([32])
decoder.stages.4.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.stages.4.convs.1.conv.bias shape torch.Size([32])
decoder.stages.4.convs.1.norm.weight shape torch.Size([32])
decoder.stages.4.convs.1.norm.bias shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.stages.4.convs.1.all_modules.0.bias shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.1.weight shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.1.bias shape torch.Size([32])
decoder.transpconvs.0.weight shape torch.Size([320, 320, 1, 2, 2])
decoder.transpconvs.0.bias shape torch.Size([320])
decoder.transpconvs.1.weight shape torch.Size([320, 256, 2, 2, 2])
decoder.transpconvs.1.bias shape torch.Size([256])
decoder.transpconvs.2.weight shape torch.Size([256, 128, 2, 2, 2])
decoder.transpconvs.2.bias shape torch.Size([128])
decoder.transpconvs.3.weight shape torch.Size([128, 64, 2, 2, 2])
decoder.transpconvs.3.bias shape torch.Size([64])
decoder.transpconvs.4.weight shape torch.Size([64, 32, 2, 2, 2])
decoder.transpconvs.4.bias shape torch.Size([32])
################### Done ###################
2024-12-11 22:24:12.026261: do_dummy_2d_data_aug: True
2024-12-11 22:24:12.027989: Using splits from existing split file: /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset307_Sohee_Calcium_OCT_CrossValidation/splits_final.json
2024-12-11 22:24:12.029698: The split file contains 5 splits.
2024-12-11 22:24:12.030443: Desired fold for training: 2
2024-12-11 22:24:12.031139: This split has 6 training and 2 validation cases.
using pin_memory on device 0
using pin_memory on device 0
using pin_memory on device 0

This is the configuration used by this training:
Configuration name: 3d_32x160x128_b10
 {'data_identifier': 'nnUNetPlans_3d_fullres', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 10, 'patch_size': [32, 160, 128], 'median_image_size_in_voxels': [375.0, 498.0, 498.0], 'spacing': [1.0, 1.0, 1.0], 'normalization_schemes': ['ZScoreNormalization'], 'use_mask_for_norm': [False], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'architecture': {'network_class_name': 'dynamic_network_architectures.architectures.unet.PlainConvUNet', 'arch_kwargs': {'n_stages': 6, 'features_per_stage': [32, 64, 128, 256, 320, 320], 'conv_op': 'torch.nn.modules.conv.Conv3d', 'kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]], 'strides': [[1, 1, 1], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [1, 2, 2]], 'n_conv_per_stage': [2, 2, 2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2, 2, 2], 'conv_bias': True, 'norm_op': 'torch.nn.modules.instancenorm.InstanceNorm3d', 'norm_op_kwargs': {'eps': 1e-05, 'affine': True}, 'dropout_op': None, 'dropout_op_kwargs': None, 'nonlin': 'torch.nn.LeakyReLU', 'nonlin_kwargs': {'inplace': True}, 'deep_supervision': True}, '_kw_requires_import': ['conv_op', 'norm_op', 'dropout_op', 'nonlin']}, 'batch_dice': True, 'inherits_from': '3d_fullres'} 

These are the global plan.json settings:
 {'dataset_name': 'Dataset307_Sohee_Calcium_OCT_CrossValidation', 'plans_name': 'nnUNetPlans', 'original_median_spacing_after_transp': [1.0, 1.0, 1.0], 'original_median_shape_after_transp': [375, 498, 498], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 0.4977976381778717, 'mean': 0.13507525622844696, 'median': 0.09599608182907104, 'min': 0.0, 'percentile_00_5': 0.014754901640117168, 'percentile_99_5': 0.4977976381778717, 'std': 0.12025152146816254}}} 

2024-12-11 22:24:14.955048: unpacking dataset...
using pin_memory on device 0

This is the configuration used by this training:
Configuration name: 3d_32x160x128_b10
 {'data_identifier': 'nnUNetPlans_3d_fullres', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 10, 'patch_size': [32, 160, 128], 'median_image_size_in_voxels': [375.0, 498.0, 498.0], 'spacing': [1.0, 1.0, 1.0], 'normalization_schemes': ['ZScoreNormalization'], 'use_mask_for_norm': [False], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'architecture': {'network_class_name': 'dynamic_network_architectures.architectures.unet.PlainConvUNet', 'arch_kwargs': {'n_stages': 6, 'features_per_stage': [32, 64, 128, 256, 320, 320], 'conv_op': 'torch.nn.modules.conv.Conv3d', 'kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]], 'strides': [[1, 1, 1], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [1, 2, 2]], 'n_conv_per_stage': [2, 2, 2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2, 2, 2], 'conv_bias': True, 'norm_op': 'torch.nn.modules.instancenorm.InstanceNorm3d', 'norm_op_kwargs': {'eps': 1e-05, 'affine': True}, 'dropout_op': None, 'dropout_op_kwargs': None, 'nonlin': 'torch.nn.LeakyReLU', 'nonlin_kwargs': {'inplace': True}, 'deep_supervision': True}, '_kw_requires_import': ['conv_op', 'norm_op', 'dropout_op', 'nonlin']}, 'batch_dice': True, 'inherits_from': '3d_fullres'} 

These are the global plan.json settings:
 {'dataset_name': 'Dataset307_Sohee_Calcium_OCT_CrossValidation', 'plans_name': 'nnUNetPlans', 'original_median_spacing_after_transp': [1.0, 1.0, 1.0], 'original_median_shape_after_transp': [375, 498, 498], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 0.4977976381778717, 'mean': 0.13507525622844696, 'median': 0.09599608182907104, 'min': 0.0, 'percentile_00_5': 0.014754901640117168, 'percentile_99_5': 0.4977976381778717, 'std': 0.12025152146816254}}} 

2024-12-11 22:24:14.978436: unpacking dataset...
2024-12-11 22:24:19.145071: unpacking done...
2024-12-11 22:24:19.274730: Unable to plot network architecture: nnUNet_compile is enabled!
2024-12-11 22:24:19.410114: 
2024-12-11 22:24:19.410956: Epoch 0
2024-12-11 22:24:19.411780: Current learning rate: 0.01
2024-12-11 22:26:48.417063: Validation loss improved from 1000.00000 to -0.20901! Patience: 0/50
2024-12-11 22:26:48.418329: train_loss -0.1153
2024-12-11 22:26:48.419348: val_loss -0.209
2024-12-11 22:26:48.420137: Pseudo dice [0.5576]
2024-12-11 22:26:48.420790: Epoch time: 149.01 s
2024-12-11 22:26:48.421698: Yayy! New best EMA pseudo Dice: 0.5576
2024-12-11 22:26:50.357328: 
2024-12-11 22:26:50.359089: Epoch 1
2024-12-11 22:26:50.359947: Current learning rate: 0.00999
2024-12-11 22:28:17.082295: Validation loss did not improve from -0.20901. Patience: 1/50
2024-12-11 22:28:17.083652: train_loss -0.255
2024-12-11 22:28:17.084625: val_loss -0.1688
2024-12-11 22:28:17.085429: Pseudo dice [0.5009]
2024-12-11 22:28:17.086318: Epoch time: 86.73 s
2024-12-11 22:28:18.389413: 
2024-12-11 22:28:18.390620: Epoch 2
2024-12-11 22:28:18.391439: Current learning rate: 0.00998
2024-12-11 22:29:45.221359: Validation loss improved from -0.20901 to -0.26835! Patience: 1/50
2024-12-11 22:29:45.222511: train_loss -0.3026
2024-12-11 22:29:45.223310: val_loss -0.2683
2024-12-11 22:29:45.224130: Pseudo dice [0.5966]
2024-12-11 22:29:45.224774: Epoch time: 86.83 s
2024-12-11 22:29:46.567128: 
2024-12-11 22:29:46.569170: Epoch 3
2024-12-11 22:29:46.570284: Current learning rate: 0.00997
2024-12-11 22:31:13.397574: Validation loss did not improve from -0.26835. Patience: 1/50
2024-12-11 22:31:13.398871: train_loss -0.337
2024-12-11 22:31:13.400098: val_loss -0.2609
2024-12-11 22:31:13.401000: Pseudo dice [0.5933]
2024-12-11 22:31:13.402058: Epoch time: 86.83 s
2024-12-11 22:31:13.403104: Yayy! New best EMA pseudo Dice: 0.5601
2024-12-11 22:31:15.074023: 
2024-12-11 22:31:15.075584: Epoch 4
2024-12-11 22:31:15.076500: Current learning rate: 0.00996
2024-12-11 22:32:41.885683: Validation loss improved from -0.26835 to -0.31657! Patience: 1/50
2024-12-11 22:32:41.886711: train_loss -0.3687
2024-12-11 22:32:41.887659: val_loss -0.3166
2024-12-11 22:32:41.888406: Pseudo dice [0.6135]
2024-12-11 22:32:41.889046: Epoch time: 86.81 s
2024-12-11 22:32:42.218219: Yayy! New best EMA pseudo Dice: 0.5654
2024-12-11 22:32:43.846984: 
2024-12-11 22:32:43.848764: Epoch 5
2024-12-11 22:32:43.849490: Current learning rate: 0.00995
2024-12-11 22:34:10.549909: Validation loss did not improve from -0.31657. Patience: 1/50
2024-12-11 22:34:10.551221: train_loss -0.3985
2024-12-11 22:34:10.552220: val_loss -0.2782
2024-12-11 22:34:10.552889: Pseudo dice [0.6023]
2024-12-11 22:34:10.553567: Epoch time: 86.71 s
2024-12-11 22:34:10.554177: Yayy! New best EMA pseudo Dice: 0.5691
2024-12-11 22:34:12.158994: 
2024-12-11 22:34:12.160389: Epoch 6
2024-12-11 22:34:12.161978: Current learning rate: 0.00995
2024-12-11 22:35:38.914295: Validation loss improved from -0.31657 to -0.34346! Patience: 1/50
2024-12-11 22:35:38.915477: train_loss -0.4038
2024-12-11 22:35:38.916659: val_loss -0.3435
2024-12-11 22:35:38.917683: Pseudo dice [0.6397]
2024-12-11 22:35:38.918795: Epoch time: 86.76 s
2024-12-11 22:35:38.919818: Yayy! New best EMA pseudo Dice: 0.5762
2024-12-11 22:35:40.589148: 
2024-12-11 22:35:40.590955: Epoch 7
2024-12-11 22:35:40.592097: Current learning rate: 0.00994
2024-12-11 22:37:07.277316: Validation loss did not improve from -0.34346. Patience: 1/50
2024-12-11 22:37:07.278537: train_loss -0.4342
2024-12-11 22:37:07.279778: val_loss -0.3267
2024-12-11 22:37:07.280699: Pseudo dice [0.6147]
2024-12-11 22:37:07.281484: Epoch time: 86.69 s
2024-12-11 22:37:07.282281: Yayy! New best EMA pseudo Dice: 0.58
2024-12-11 22:37:08.906546: 
2024-12-11 22:37:08.907805: Epoch 8
2024-12-11 22:37:08.908577: Current learning rate: 0.00993
2024-12-11 22:38:35.973536: Validation loss improved from -0.34346 to -0.34600! Patience: 1/50
2024-12-11 22:38:35.974703: train_loss -0.4657
2024-12-11 22:38:35.975617: val_loss -0.346
2024-12-11 22:38:35.976245: Pseudo dice [0.6445]
2024-12-11 22:38:35.976887: Epoch time: 87.07 s
2024-12-11 22:38:35.977563: Yayy! New best EMA pseudo Dice: 0.5865
2024-12-11 22:38:38.254494: 
2024-12-11 22:38:38.256239: Epoch 9
2024-12-11 22:38:38.257031: Current learning rate: 0.00992
2024-12-11 22:40:05.100461: Validation loss did not improve from -0.34600. Patience: 1/50
2024-12-11 22:40:05.101519: train_loss -0.4745
2024-12-11 22:40:05.102563: val_loss -0.2284
2024-12-11 22:40:05.103226: Pseudo dice [0.5505]
2024-12-11 22:40:05.103923: Epoch time: 86.85 s
2024-12-11 22:40:06.658028: 
2024-12-11 22:40:06.659834: Epoch 10
2024-12-11 22:40:06.661029: Current learning rate: 0.00991
2024-12-11 22:41:33.538895: Validation loss improved from -0.34600 to -0.36896! Patience: 1/50
2024-12-11 22:41:33.539879: train_loss -0.4805
2024-12-11 22:41:33.540827: val_loss -0.369
2024-12-11 22:41:33.541540: Pseudo dice [0.6544]
2024-12-11 22:41:33.542202: Epoch time: 86.88 s
2024-12-11 22:41:33.542897: Yayy! New best EMA pseudo Dice: 0.59
2024-12-11 22:41:35.147567: 
2024-12-11 22:41:35.149038: Epoch 11
2024-12-11 22:41:35.149810: Current learning rate: 0.0099
2024-12-11 22:43:02.092990: Validation loss did not improve from -0.36896. Patience: 1/50
2024-12-11 22:43:02.094481: train_loss -0.4847
2024-12-11 22:43:02.095683: val_loss -0.1369
2024-12-11 22:43:02.096570: Pseudo dice [0.4739]
2024-12-11 22:43:02.097522: Epoch time: 86.95 s
2024-12-11 22:43:03.415391: 
2024-12-11 22:43:03.416960: Epoch 12
2024-12-11 22:43:03.417859: Current learning rate: 0.00989
2024-12-11 22:44:30.352624: Validation loss improved from -0.36896 to -0.37205! Patience: 1/50
2024-12-11 22:44:30.353991: train_loss -0.5077
2024-12-11 22:44:30.355257: val_loss -0.3721
2024-12-11 22:44:30.356130: Pseudo dice [0.6515]
2024-12-11 22:44:30.357432: Epoch time: 86.94 s
2024-12-11 22:44:31.636453: 
2024-12-11 22:44:31.638061: Epoch 13
2024-12-11 22:44:31.638949: Current learning rate: 0.00988
2024-12-11 22:45:58.615358: Validation loss did not improve from -0.37205. Patience: 1/50
2024-12-11 22:45:58.616936: train_loss -0.5183
2024-12-11 22:45:58.618394: val_loss -0.3625
2024-12-11 22:45:58.619483: Pseudo dice [0.657]
2024-12-11 22:45:58.620112: Epoch time: 86.98 s
2024-12-11 22:45:58.620821: Yayy! New best EMA pseudo Dice: 0.5929
2024-12-11 22:46:00.213261: 
2024-12-11 22:46:00.214696: Epoch 14
2024-12-11 22:46:00.215509: Current learning rate: 0.00987
2024-12-11 22:47:27.063470: Validation loss did not improve from -0.37205. Patience: 2/50
2024-12-11 22:47:27.064588: train_loss -0.5198
2024-12-11 22:47:27.065504: val_loss -0.3316
2024-12-11 22:47:27.066464: Pseudo dice [0.624]
2024-12-11 22:47:27.067402: Epoch time: 86.85 s
2024-12-11 22:47:27.458396: Yayy! New best EMA pseudo Dice: 0.596
2024-12-11 22:47:29.122254: 
2024-12-11 22:47:29.124194: Epoch 15
2024-12-11 22:47:29.125282: Current learning rate: 0.00986
2024-12-11 22:48:55.831812: Validation loss did not improve from -0.37205. Patience: 3/50
2024-12-11 22:48:55.833015: train_loss -0.5285
2024-12-11 22:48:55.833837: val_loss -0.2388
2024-12-11 22:48:55.834620: Pseudo dice [0.548]
2024-12-11 22:48:55.835510: Epoch time: 86.71 s
2024-12-11 22:48:57.114122: 
2024-12-11 22:48:57.115752: Epoch 16
2024-12-11 22:48:57.116739: Current learning rate: 0.00986
2024-12-11 22:50:24.210528: Validation loss did not improve from -0.37205. Patience: 4/50
2024-12-11 22:50:24.211714: train_loss -0.5403
2024-12-11 22:50:24.212666: val_loss -0.3564
2024-12-11 22:50:24.213480: Pseudo dice [0.659]
2024-12-11 22:50:24.214224: Epoch time: 87.1 s
2024-12-11 22:50:24.214897: Yayy! New best EMA pseudo Dice: 0.598
2024-12-11 22:50:25.876856: 
2024-12-11 22:50:25.878515: Epoch 17
2024-12-11 22:50:25.879326: Current learning rate: 0.00985
2024-12-11 22:51:52.855753: Validation loss did not improve from -0.37205. Patience: 5/50
2024-12-11 22:51:52.856915: train_loss -0.5381
2024-12-11 22:51:52.857910: val_loss -0.3643
2024-12-11 22:51:52.858759: Pseudo dice [0.6615]
2024-12-11 22:51:52.859563: Epoch time: 86.98 s
2024-12-11 22:51:52.860353: Yayy! New best EMA pseudo Dice: 0.6043
2024-12-11 22:51:54.504946: 
2024-12-11 22:51:54.506578: Epoch 18
2024-12-11 22:51:54.507383: Current learning rate: 0.00984
2024-12-11 22:53:21.443557: Validation loss did not improve from -0.37205. Patience: 6/50
2024-12-11 22:53:21.444607: train_loss -0.5395
2024-12-11 22:53:21.445314: val_loss -0.3208
2024-12-11 22:53:21.445941: Pseudo dice [0.6222]
2024-12-11 22:53:21.446629: Epoch time: 86.94 s
2024-12-11 22:53:21.447272: Yayy! New best EMA pseudo Dice: 0.6061
2024-12-11 22:53:23.532885: 
2024-12-11 22:53:23.533682: Epoch 19
2024-12-11 22:53:23.534338: Current learning rate: 0.00983
2024-12-11 22:54:50.557784: Validation loss did not improve from -0.37205. Patience: 7/50
2024-12-11 22:54:50.558848: train_loss -0.5492
2024-12-11 22:54:50.559596: val_loss -0.2965
2024-12-11 22:54:50.560275: Pseudo dice [0.5958]
2024-12-11 22:54:50.560858: Epoch time: 87.03 s
2024-12-11 22:54:52.203743: 
2024-12-11 22:54:52.205207: Epoch 20
2024-12-11 22:54:52.205938: Current learning rate: 0.00982
2024-12-11 22:56:19.113329: Validation loss did not improve from -0.37205. Patience: 8/50
2024-12-11 22:56:19.114263: train_loss -0.5544
2024-12-11 22:56:19.115260: val_loss -0.3146
2024-12-11 22:56:19.116020: Pseudo dice [0.6231]
2024-12-11 22:56:19.116623: Epoch time: 86.91 s
2024-12-11 22:56:19.117199: Yayy! New best EMA pseudo Dice: 0.6069
2024-12-11 22:56:20.823671: 
2024-12-11 22:56:20.825319: Epoch 21
2024-12-11 22:56:20.826385: Current learning rate: 0.00981
2024-12-11 22:57:47.747827: Validation loss did not improve from -0.37205. Patience: 9/50
2024-12-11 22:57:47.749088: train_loss -0.5598
2024-12-11 22:57:47.749916: val_loss -0.3566
2024-12-11 22:57:47.750702: Pseudo dice [0.6459]
2024-12-11 22:57:47.751458: Epoch time: 86.93 s
2024-12-11 22:57:47.752125: Yayy! New best EMA pseudo Dice: 0.6108
2024-12-11 22:57:49.341232: 
2024-12-11 22:57:49.342689: Epoch 22
2024-12-11 22:57:49.343676: Current learning rate: 0.0098
2024-12-11 22:59:16.285437: Validation loss improved from -0.37205 to -0.37832! Patience: 9/50
2024-12-11 22:59:16.286720: train_loss -0.5581
2024-12-11 22:59:16.287586: val_loss -0.3783
2024-12-11 22:59:16.288437: Pseudo dice [0.6559]
2024-12-11 22:59:16.289175: Epoch time: 86.95 s
2024-12-11 22:59:16.289922: Yayy! New best EMA pseudo Dice: 0.6153
2024-12-11 22:59:17.897768: 
2024-12-11 22:59:17.899292: Epoch 23
2024-12-11 22:59:17.900089: Current learning rate: 0.00979
2024-12-11 23:00:44.789637: Validation loss improved from -0.37832 to -0.39122! Patience: 0/50
2024-12-11 23:00:44.790456: train_loss -0.5752
2024-12-11 23:00:44.791539: val_loss -0.3912
2024-12-11 23:00:44.792351: Pseudo dice [0.6602]
2024-12-11 23:00:44.793092: Epoch time: 86.89 s
2024-12-11 23:00:44.793795: Yayy! New best EMA pseudo Dice: 0.6198
2024-12-11 23:00:46.382208: 
2024-12-11 23:00:46.383718: Epoch 24
2024-12-11 23:00:46.384845: Current learning rate: 0.00978
2024-12-11 23:02:13.374239: Validation loss improved from -0.39122 to -0.39689! Patience: 0/50
2024-12-11 23:02:13.375420: train_loss -0.5775
2024-12-11 23:02:13.376580: val_loss -0.3969
2024-12-11 23:02:13.377255: Pseudo dice [0.6635]
2024-12-11 23:02:13.378024: Epoch time: 86.99 s
2024-12-11 23:02:13.738398: Yayy! New best EMA pseudo Dice: 0.6242
2024-12-11 23:02:15.295817: 
2024-12-11 23:02:15.297305: Epoch 25
2024-12-11 23:02:15.298000: Current learning rate: 0.00977
2024-12-11 23:03:42.443776: Validation loss did not improve from -0.39689. Patience: 1/50
2024-12-11 23:03:42.444730: train_loss -0.5613
2024-12-11 23:03:42.445500: val_loss -0.3516
2024-12-11 23:03:42.446537: Pseudo dice [0.6403]
2024-12-11 23:03:42.447364: Epoch time: 87.15 s
2024-12-11 23:03:42.448073: Yayy! New best EMA pseudo Dice: 0.6258
2024-12-11 23:03:44.051455: 
2024-12-11 23:03:44.053473: Epoch 26
2024-12-11 23:03:44.054151: Current learning rate: 0.00977
2024-12-11 23:05:11.192812: Validation loss did not improve from -0.39689. Patience: 2/50
2024-12-11 23:05:11.194012: train_loss -0.5742
2024-12-11 23:05:11.195003: val_loss -0.3937
2024-12-11 23:05:11.195781: Pseudo dice [0.6731]
2024-12-11 23:05:11.196524: Epoch time: 87.14 s
2024-12-11 23:05:11.197247: Yayy! New best EMA pseudo Dice: 0.6305
2024-12-11 23:05:12.822821: 
2024-12-11 23:05:12.824632: Epoch 27
2024-12-11 23:05:12.825301: Current learning rate: 0.00976
2024-12-11 23:06:40.055240: Validation loss did not improve from -0.39689. Patience: 3/50
2024-12-11 23:06:40.056298: train_loss -0.5875
2024-12-11 23:06:40.057334: val_loss -0.3642
2024-12-11 23:06:40.058106: Pseudo dice [0.6502]
2024-12-11 23:06:40.058957: Epoch time: 87.23 s
2024-12-11 23:06:40.059567: Yayy! New best EMA pseudo Dice: 0.6325
2024-12-11 23:06:41.632558: 
2024-12-11 23:06:41.634423: Epoch 28
2024-12-11 23:06:41.635249: Current learning rate: 0.00975
2024-12-11 23:08:08.777148: Validation loss did not improve from -0.39689. Patience: 4/50
2024-12-11 23:08:08.778289: train_loss -0.5948
2024-12-11 23:08:08.779155: val_loss -0.3263
2024-12-11 23:08:08.779822: Pseudo dice [0.6296]
2024-12-11 23:08:08.780481: Epoch time: 87.15 s
2024-12-11 23:08:10.027025: 
2024-12-11 23:08:10.029662: Epoch 29
2024-12-11 23:08:10.030497: Current learning rate: 0.00974
2024-12-11 23:09:37.289726: Validation loss did not improve from -0.39689. Patience: 5/50
2024-12-11 23:09:37.291049: train_loss -0.5938
2024-12-11 23:09:37.291820: val_loss -0.3441
2024-12-11 23:09:37.292502: Pseudo dice [0.638]
2024-12-11 23:09:37.293298: Epoch time: 87.27 s
2024-12-11 23:09:37.656406: Yayy! New best EMA pseudo Dice: 0.6328
2024-12-11 23:09:39.661582: 
2024-12-11 23:09:39.663541: Epoch 30
2024-12-11 23:09:39.664456: Current learning rate: 0.00973
2024-12-11 23:11:06.800059: Validation loss did not improve from -0.39689. Patience: 6/50
2024-12-11 23:11:06.801115: train_loss -0.5948
2024-12-11 23:11:06.802017: val_loss -0.3683
2024-12-11 23:11:06.802897: Pseudo dice [0.6492]
2024-12-11 23:11:06.803678: Epoch time: 87.14 s
2024-12-11 23:11:06.804505: Yayy! New best EMA pseudo Dice: 0.6344
2024-12-11 23:11:08.504205: 
2024-12-11 23:11:08.505468: Epoch 31
2024-12-11 23:11:08.506385: Current learning rate: 0.00972
2024-12-11 23:12:35.606966: Validation loss did not improve from -0.39689. Patience: 7/50
2024-12-11 23:12:35.607820: train_loss -0.5914
2024-12-11 23:12:35.608457: val_loss -0.2974
2024-12-11 23:12:35.609041: Pseudo dice [0.6295]
2024-12-11 23:12:35.609662: Epoch time: 87.1 s
2024-12-11 23:12:36.878006: 
2024-12-11 23:12:36.879272: Epoch 32
2024-12-11 23:12:36.879916: Current learning rate: 0.00971
2024-12-11 23:14:04.183310: Validation loss did not improve from -0.39689. Patience: 8/50
2024-12-11 23:14:04.184582: train_loss -0.5997
2024-12-11 23:14:04.185283: val_loss -0.3628
2024-12-11 23:14:04.185871: Pseudo dice [0.6423]
2024-12-11 23:14:04.186423: Epoch time: 87.31 s
2024-12-11 23:14:04.186992: Yayy! New best EMA pseudo Dice: 0.6348
2024-12-11 23:14:05.792437: 
2024-12-11 23:14:05.794501: Epoch 33
2024-12-11 23:14:05.795646: Current learning rate: 0.0097
2024-12-11 23:15:32.981228: Validation loss improved from -0.39689 to -0.39864! Patience: 8/50
2024-12-11 23:15:32.982427: train_loss -0.6029
2024-12-11 23:15:32.983432: val_loss -0.3986
2024-12-11 23:15:32.984257: Pseudo dice [0.6787]
2024-12-11 23:15:32.985184: Epoch time: 87.19 s
2024-12-11 23:15:32.985987: Yayy! New best EMA pseudo Dice: 0.6391
2024-12-11 23:15:34.618331: 
2024-12-11 23:15:34.619726: Epoch 34
2024-12-11 23:15:34.620863: Current learning rate: 0.00969
2024-12-11 23:17:01.634383: Validation loss did not improve from -0.39864. Patience: 1/50
2024-12-11 23:17:01.635373: train_loss -0.6092
2024-12-11 23:17:01.636265: val_loss -0.2788
2024-12-11 23:17:01.637231: Pseudo dice [0.6133]
2024-12-11 23:17:01.637971: Epoch time: 87.02 s
2024-12-11 23:17:03.265714: 
2024-12-11 23:17:03.267558: Epoch 35
2024-12-11 23:17:03.268881: Current learning rate: 0.00968
2024-12-11 23:18:30.260473: Validation loss did not improve from -0.39864. Patience: 2/50
2024-12-11 23:18:30.261478: train_loss -0.6101
2024-12-11 23:18:30.262192: val_loss -0.3633
2024-12-11 23:18:30.263295: Pseudo dice [0.6522]
2024-12-11 23:18:30.264118: Epoch time: 87.0 s
2024-12-11 23:18:31.525650: 
2024-12-11 23:18:31.527173: Epoch 36
2024-12-11 23:18:31.527898: Current learning rate: 0.00968
2024-12-11 23:19:58.489304: Validation loss improved from -0.39864 to -0.40708! Patience: 2/50
2024-12-11 23:19:58.490424: train_loss -0.6139
2024-12-11 23:19:58.491179: val_loss -0.4071
2024-12-11 23:19:58.491985: Pseudo dice [0.6773]
2024-12-11 23:19:58.492766: Epoch time: 86.97 s
2024-12-11 23:19:58.493517: Yayy! New best EMA pseudo Dice: 0.642
2024-12-11 23:20:00.167924: 
2024-12-11 23:20:00.169407: Epoch 37
2024-12-11 23:20:00.170196: Current learning rate: 0.00967
2024-12-11 23:21:27.158865: Validation loss did not improve from -0.40708. Patience: 1/50
2024-12-11 23:21:27.159832: train_loss -0.6122
2024-12-11 23:21:27.160748: val_loss -0.3525
2024-12-11 23:21:27.161382: Pseudo dice [0.6588]
2024-12-11 23:21:27.162001: Epoch time: 86.99 s
2024-12-11 23:21:27.162858: Yayy! New best EMA pseudo Dice: 0.6437
2024-12-11 23:21:28.800545: 
2024-12-11 23:21:28.801921: Epoch 38
2024-12-11 23:21:28.802693: Current learning rate: 0.00966
2024-12-11 23:22:55.994983: Validation loss improved from -0.40708 to -0.43057! Patience: 1/50
2024-12-11 23:22:55.996023: train_loss -0.6045
2024-12-11 23:22:55.996772: val_loss -0.4306
2024-12-11 23:22:55.997335: Pseudo dice [0.6871]
2024-12-11 23:22:55.998030: Epoch time: 87.2 s
2024-12-11 23:22:55.998694: Yayy! New best EMA pseudo Dice: 0.6481
2024-12-11 23:22:57.621208: 
2024-12-11 23:22:57.623204: Epoch 39
2024-12-11 23:22:57.623920: Current learning rate: 0.00965
2024-12-11 23:24:24.579214: Validation loss did not improve from -0.43057. Patience: 1/50
2024-12-11 23:24:24.580480: train_loss -0.6103
2024-12-11 23:24:24.581375: val_loss -0.4283
2024-12-11 23:24:24.582098: Pseudo dice [0.6914]
2024-12-11 23:24:24.582861: Epoch time: 86.96 s
2024-12-11 23:24:24.974236: Yayy! New best EMA pseudo Dice: 0.6524
2024-12-11 23:24:26.980654: 
2024-12-11 23:24:26.982596: Epoch 40
2024-12-11 23:24:26.983657: Current learning rate: 0.00964
2024-12-11 23:25:53.903710: Validation loss did not improve from -0.43057. Patience: 2/50
2024-12-11 23:25:53.905014: train_loss -0.6114
2024-12-11 23:25:53.906021: val_loss -0.3724
2024-12-11 23:25:53.906903: Pseudo dice [0.66]
2024-12-11 23:25:53.907660: Epoch time: 86.93 s
2024-12-11 23:25:53.908316: Yayy! New best EMA pseudo Dice: 0.6532
2024-12-11 23:25:55.599995: 
2024-12-11 23:25:55.601851: Epoch 41
2024-12-11 23:25:55.602642: Current learning rate: 0.00963
2024-12-11 23:27:22.597673: Validation loss did not improve from -0.43057. Patience: 3/50
2024-12-11 23:27:22.598871: train_loss -0.6058
2024-12-11 23:27:22.599689: val_loss -0.4052
2024-12-11 23:27:22.600399: Pseudo dice [0.6925]
2024-12-11 23:27:22.601160: Epoch time: 87.0 s
2024-12-11 23:27:22.601745: Yayy! New best EMA pseudo Dice: 0.6571
2024-12-11 23:27:24.173040: 
2024-12-11 23:27:24.174310: Epoch 42
2024-12-11 23:27:24.174997: Current learning rate: 0.00962
2024-12-11 23:28:51.162879: Validation loss did not improve from -0.43057. Patience: 4/50
2024-12-11 23:28:51.164003: train_loss -0.6222
2024-12-11 23:28:51.165155: val_loss -0.4168
2024-12-11 23:28:51.166152: Pseudo dice [0.6867]
2024-12-11 23:28:51.167159: Epoch time: 86.99 s
2024-12-11 23:28:51.167985: Yayy! New best EMA pseudo Dice: 0.6601
2024-12-11 23:28:52.731851: 
2024-12-11 23:28:52.733782: Epoch 43
2024-12-11 23:28:52.734660: Current learning rate: 0.00961
2024-12-11 23:30:19.993283: Validation loss did not improve from -0.43057. Patience: 5/50
2024-12-11 23:30:19.994657: train_loss -0.6208
2024-12-11 23:30:19.996680: val_loss -0.4021
2024-12-11 23:30:19.997469: Pseudo dice [0.6904]
2024-12-11 23:30:19.999056: Epoch time: 87.26 s
2024-12-11 23:30:20.000457: Yayy! New best EMA pseudo Dice: 0.6631
2024-12-11 23:30:21.579035: 
2024-12-11 23:30:21.581077: Epoch 44
2024-12-11 23:30:21.582264: Current learning rate: 0.0096
2024-12-11 23:31:48.487068: Validation loss improved from -0.43057 to -0.43382! Patience: 5/50
2024-12-11 23:31:48.488333: train_loss -0.6173
2024-12-11 23:31:48.489443: val_loss -0.4338
2024-12-11 23:31:48.490011: Pseudo dice [0.6892]
2024-12-11 23:31:48.490681: Epoch time: 86.91 s
2024-12-11 23:31:48.856545: Yayy! New best EMA pseudo Dice: 0.6657
2024-12-11 23:31:50.433129: 
2024-12-11 23:31:50.434940: Epoch 45
2024-12-11 23:31:50.435586: Current learning rate: 0.00959
2024-12-11 23:33:17.252023: Validation loss did not improve from -0.43382. Patience: 1/50
2024-12-11 23:33:17.253073: train_loss -0.6241
2024-12-11 23:33:17.254341: val_loss -0.3498
2024-12-11 23:33:17.254972: Pseudo dice [0.653]
2024-12-11 23:33:17.255707: Epoch time: 86.82 s
2024-12-11 23:33:18.451209: 
2024-12-11 23:33:18.452320: Epoch 46
2024-12-11 23:33:18.453019: Current learning rate: 0.00959
2024-12-11 23:34:45.149067: Validation loss did not improve from -0.43382. Patience: 2/50
2024-12-11 23:34:45.150077: train_loss -0.6263
2024-12-11 23:34:45.151026: val_loss -0.2842
2024-12-11 23:34:45.151671: Pseudo dice [0.5939]
2024-12-11 23:34:45.152268: Epoch time: 86.7 s
2024-12-11 23:34:46.350236: 
2024-12-11 23:34:46.351780: Epoch 47
2024-12-11 23:34:46.352615: Current learning rate: 0.00958
2024-12-11 23:36:13.145747: Validation loss did not improve from -0.43382. Patience: 3/50
2024-12-11 23:36:13.146616: train_loss -0.6335
2024-12-11 23:36:13.147567: val_loss -0.4159
2024-12-11 23:36:13.148212: Pseudo dice [0.6866]
2024-12-11 23:36:13.148850: Epoch time: 86.8 s
2024-12-11 23:36:14.324755: 
2024-12-11 23:36:14.326015: Epoch 48
2024-12-11 23:36:14.326895: Current learning rate: 0.00957
2024-12-11 23:37:41.230856: Validation loss did not improve from -0.43382. Patience: 4/50
2024-12-11 23:37:41.232062: train_loss -0.6342
2024-12-11 23:37:41.233068: val_loss -0.3881
2024-12-11 23:37:41.233711: Pseudo dice [0.6633]
2024-12-11 23:37:41.234334: Epoch time: 86.91 s
2024-12-11 23:37:42.453688: 
2024-12-11 23:37:42.455331: Epoch 49
2024-12-11 23:37:42.456265: Current learning rate: 0.00956
2024-12-11 23:39:09.547265: Validation loss did not improve from -0.43382. Patience: 5/50
2024-12-11 23:39:09.548391: train_loss -0.6402
2024-12-11 23:39:09.549333: val_loss -0.3548
2024-12-11 23:39:09.550395: Pseudo dice [0.6503]
2024-12-11 23:39:09.551177: Epoch time: 87.1 s
2024-12-11 23:39:11.091338: 
2024-12-11 23:39:11.092417: Epoch 50
2024-12-11 23:39:11.093239: Current learning rate: 0.00955
2024-12-11 23:40:38.236545: Validation loss did not improve from -0.43382. Patience: 6/50
2024-12-11 23:40:38.237723: train_loss -0.636
2024-12-11 23:40:38.238623: val_loss -0.3598
2024-12-11 23:40:38.239323: Pseudo dice [0.655]
2024-12-11 23:40:38.239973: Epoch time: 87.15 s
2024-12-11 23:40:39.967357: 
2024-12-11 23:40:39.968950: Epoch 51
2024-12-11 23:40:39.970169: Current learning rate: 0.00954
2024-12-11 23:42:07.008933: Validation loss did not improve from -0.43382. Patience: 7/50
2024-12-11 23:42:07.009859: train_loss -0.6325
2024-12-11 23:42:07.010609: val_loss -0.252
2024-12-11 23:42:07.011297: Pseudo dice [0.5788]
2024-12-11 23:42:07.011893: Epoch time: 87.04 s
2024-12-11 23:42:08.268641: 
2024-12-11 23:42:08.269954: Epoch 52
2024-12-11 23:42:08.270600: Current learning rate: 0.00953
2024-12-11 23:43:35.300291: Validation loss did not improve from -0.43382. Patience: 8/50
2024-12-11 23:43:35.301544: train_loss -0.6292
2024-12-11 23:43:35.302573: val_loss -0.3
2024-12-11 23:43:35.303248: Pseudo dice [0.616]
2024-12-11 23:43:35.303827: Epoch time: 87.03 s
2024-12-11 23:43:36.497131: 
2024-12-11 23:43:36.498780: Epoch 53
2024-12-11 23:43:36.499480: Current learning rate: 0.00952
2024-12-11 23:45:03.355198: Validation loss did not improve from -0.43382. Patience: 9/50
2024-12-11 23:45:03.356048: train_loss -0.6373
2024-12-11 23:45:03.356881: val_loss -0.2854
2024-12-11 23:45:03.357597: Pseudo dice [0.5982]
2024-12-11 23:45:03.358305: Epoch time: 86.86 s
2024-12-11 23:45:04.554088: 
2024-12-11 23:45:04.555953: Epoch 54
2024-12-11 23:45:04.556597: Current learning rate: 0.00951
2024-12-11 23:46:31.443450: Validation loss did not improve from -0.43382. Patience: 10/50
2024-12-11 23:46:31.444625: train_loss -0.6458
2024-12-11 23:46:31.445405: val_loss -0.3975
2024-12-11 23:46:31.446281: Pseudo dice [0.6651]
2024-12-11 23:46:31.447069: Epoch time: 86.89 s
2024-12-11 23:46:33.028630: 
2024-12-11 23:46:33.030041: Epoch 55
2024-12-11 23:46:33.030724: Current learning rate: 0.0095
2024-12-11 23:47:59.769479: Validation loss did not improve from -0.43382. Patience: 11/50
2024-12-11 23:47:59.770602: train_loss -0.6421
2024-12-11 23:47:59.771578: val_loss -0.4154
2024-12-11 23:47:59.772252: Pseudo dice [0.6757]
2024-12-11 23:47:59.772929: Epoch time: 86.74 s
2024-12-11 23:48:01.053355: 
2024-12-11 23:48:01.054933: Epoch 56
2024-12-11 23:48:01.055796: Current learning rate: 0.00949
2024-12-11 23:49:27.813878: Validation loss did not improve from -0.43382. Patience: 12/50
2024-12-11 23:49:27.815174: train_loss -0.6445
2024-12-11 23:49:27.816829: val_loss -0.3295
2024-12-11 23:49:27.817607: Pseudo dice [0.6331]
2024-12-11 23:49:27.818255: Epoch time: 86.76 s
2024-12-11 23:49:29.068559: 
2024-12-11 23:49:29.070788: Epoch 57
2024-12-11 23:49:29.071870: Current learning rate: 0.00949
2024-12-11 23:50:55.987507: Validation loss did not improve from -0.43382. Patience: 13/50
2024-12-11 23:50:55.988529: train_loss -0.6487
2024-12-11 23:50:55.989751: val_loss -0.2843
2024-12-11 23:50:55.990677: Pseudo dice [0.6308]
2024-12-11 23:50:55.991465: Epoch time: 86.92 s
2024-12-11 23:50:57.203714: 
2024-12-11 23:50:57.205614: Epoch 58
2024-12-11 23:50:57.206433: Current learning rate: 0.00948
2024-12-11 23:52:24.160776: Validation loss improved from -0.43382 to -0.45408! Patience: 13/50
2024-12-11 23:52:24.162029: train_loss -0.6446
2024-12-11 23:52:24.162962: val_loss -0.4541
2024-12-11 23:52:24.163760: Pseudo dice [0.6994]
2024-12-11 23:52:24.164405: Epoch time: 86.96 s
2024-12-11 23:52:25.403860: 
2024-12-11 23:52:25.405088: Epoch 59
2024-12-11 23:52:25.405914: Current learning rate: 0.00947
2024-12-11 23:53:52.500977: Validation loss did not improve from -0.45408. Patience: 1/50
2024-12-11 23:53:52.502217: train_loss -0.6498
2024-12-11 23:53:52.503418: val_loss -0.3839
2024-12-11 23:53:52.504354: Pseudo dice [0.6651]
2024-12-11 23:53:52.505187: Epoch time: 87.1 s
2024-12-11 23:53:54.111909: 
2024-12-11 23:53:54.112841: Epoch 60
2024-12-11 23:53:54.113589: Current learning rate: 0.00946
2024-12-11 23:55:20.947063: Validation loss did not improve from -0.45408. Patience: 2/50
2024-12-11 23:55:20.948276: train_loss -0.6515
2024-12-11 23:55:20.949387: val_loss -0.3951
2024-12-11 23:55:20.950071: Pseudo dice [0.6724]
2024-12-11 23:55:20.950721: Epoch time: 86.84 s
2024-12-11 23:55:22.182272: 
2024-12-11 23:55:22.184129: Epoch 61
2024-12-11 23:55:22.184935: Current learning rate: 0.00945
2024-12-11 23:56:49.066700: Validation loss did not improve from -0.45408. Patience: 3/50
2024-12-11 23:56:49.067915: train_loss -0.6602
2024-12-11 23:56:49.068809: val_loss -0.2592
2024-12-11 23:56:49.069593: Pseudo dice [0.5927]
2024-12-11 23:56:49.070434: Epoch time: 86.89 s
2024-12-11 23:56:50.638570: 
2024-12-11 23:56:50.639681: Epoch 62
2024-12-11 23:56:50.640867: Current learning rate: 0.00944
2024-12-11 23:58:17.600701: Validation loss improved from -0.45408 to -0.45595! Patience: 3/50
2024-12-11 23:58:17.601981: train_loss -0.6551
2024-12-11 23:58:17.603049: val_loss -0.4559
2024-12-11 23:58:17.603987: Pseudo dice [0.7116]
2024-12-11 23:58:17.604832: Epoch time: 86.96 s
2024-12-11 23:58:18.823743: 
2024-12-11 23:58:18.825367: Epoch 63
2024-12-11 23:58:18.826338: Current learning rate: 0.00943
2024-12-11 23:59:45.749473: Validation loss did not improve from -0.45595. Patience: 1/50
2024-12-11 23:59:45.750582: train_loss -0.6584
2024-12-11 23:59:45.751380: val_loss -0.3424
2024-12-11 23:59:45.752179: Pseudo dice [0.6422]
2024-12-11 23:59:45.752963: Epoch time: 86.93 s
2024-12-11 23:59:46.971836: 
2024-12-11 23:59:46.973161: Epoch 64
2024-12-11 23:59:46.973919: Current learning rate: 0.00942
2024-12-12 00:01:14.021883: Validation loss did not improve from -0.45595. Patience: 2/50
2024-12-12 00:01:14.022773: train_loss -0.6626
2024-12-12 00:01:14.023575: val_loss -0.2831
2024-12-12 00:01:14.024376: Pseudo dice [0.6067]
2024-12-12 00:01:14.025075: Epoch time: 87.05 s
2024-12-12 00:01:15.627145: 
2024-12-12 00:01:15.628797: Epoch 65
2024-12-12 00:01:15.629573: Current learning rate: 0.00941
2024-12-12 00:02:42.583342: Validation loss did not improve from -0.45595. Patience: 3/50
2024-12-12 00:02:42.584461: train_loss -0.6645
2024-12-12 00:02:42.585327: val_loss -0.3802
2024-12-12 00:02:42.586128: Pseudo dice [0.6663]
2024-12-12 00:02:42.586864: Epoch time: 86.96 s
2024-12-12 00:02:43.878138: 
2024-12-12 00:02:43.879716: Epoch 66
2024-12-12 00:02:43.880539: Current learning rate: 0.0094
2024-12-12 00:04:10.863477: Validation loss did not improve from -0.45595. Patience: 4/50
2024-12-12 00:04:10.864390: train_loss -0.6678
2024-12-12 00:04:10.865422: val_loss -0.4299
2024-12-12 00:04:10.866324: Pseudo dice [0.6907]
2024-12-12 00:04:10.867169: Epoch time: 86.99 s
2024-12-12 00:04:12.173589: 
2024-12-12 00:04:12.175281: Epoch 67
2024-12-12 00:04:12.176679: Current learning rate: 0.00939
2024-12-12 00:05:39.276566: Validation loss did not improve from -0.45595. Patience: 5/50
2024-12-12 00:05:39.277761: train_loss -0.6598
2024-12-12 00:05:39.278609: val_loss -0.2653
2024-12-12 00:05:39.279267: Pseudo dice [0.5868]
2024-12-12 00:05:39.279883: Epoch time: 87.11 s
2024-12-12 00:05:40.534677: 
2024-12-12 00:05:40.536441: Epoch 68
2024-12-12 00:05:40.537162: Current learning rate: 0.00939
2024-12-12 00:07:07.659928: Validation loss did not improve from -0.45595. Patience: 6/50
2024-12-12 00:07:07.661279: train_loss -0.6723
2024-12-12 00:07:07.662259: val_loss -0.2515
2024-12-12 00:07:07.662948: Pseudo dice [0.609]
2024-12-12 00:07:07.663708: Epoch time: 87.13 s
2024-12-12 00:07:08.906744: 
2024-12-12 00:07:08.908506: Epoch 69
2024-12-12 00:07:08.909209: Current learning rate: 0.00938
2024-12-12 00:08:36.041317: Validation loss did not improve from -0.45595. Patience: 7/50
2024-12-12 00:08:36.042305: train_loss -0.6725
2024-12-12 00:08:36.043416: val_loss -0.352
2024-12-12 00:08:36.044291: Pseudo dice [0.658]
2024-12-12 00:08:36.045197: Epoch time: 87.14 s
2024-12-12 00:08:37.696898: 
2024-12-12 00:08:37.699170: Epoch 70
2024-12-12 00:08:37.700368: Current learning rate: 0.00937
2024-12-12 00:10:04.790839: Validation loss did not improve from -0.45595. Patience: 8/50
2024-12-12 00:10:04.791981: train_loss -0.6782
2024-12-12 00:10:04.792951: val_loss -0.3739
2024-12-12 00:10:04.793646: Pseudo dice [0.6608]
2024-12-12 00:10:04.794329: Epoch time: 87.1 s
2024-12-12 00:10:06.096867: 
2024-12-12 00:10:06.098201: Epoch 71
2024-12-12 00:10:06.099012: Current learning rate: 0.00936
2024-12-12 00:11:33.205832: Validation loss did not improve from -0.45595. Patience: 9/50
2024-12-12 00:11:33.206929: train_loss -0.6749
2024-12-12 00:11:33.207840: val_loss -0.3423
2024-12-12 00:11:33.208758: Pseudo dice [0.6394]
2024-12-12 00:11:33.209563: Epoch time: 87.11 s
2024-12-12 00:11:34.485026: 
2024-12-12 00:11:34.486454: Epoch 72
2024-12-12 00:11:34.487524: Current learning rate: 0.00935
2024-12-12 00:13:01.563481: Validation loss did not improve from -0.45595. Patience: 10/50
2024-12-12 00:13:01.564608: train_loss -0.6608
2024-12-12 00:13:01.565520: val_loss -0.373
2024-12-12 00:13:01.566429: Pseudo dice [0.6522]
2024-12-12 00:13:01.567308: Epoch time: 87.08 s
2024-12-12 00:13:03.192028: 
2024-12-12 00:13:03.194188: Epoch 73
2024-12-12 00:13:03.195119: Current learning rate: 0.00934
2024-12-12 00:14:30.033577: Validation loss did not improve from -0.45595. Patience: 11/50
2024-12-12 00:14:30.034556: train_loss -0.6641
2024-12-12 00:14:30.035640: val_loss -0.4202
2024-12-12 00:14:30.036418: Pseudo dice [0.6872]
2024-12-12 00:14:30.037024: Epoch time: 86.84 s
2024-12-12 00:14:31.315340: 
2024-12-12 00:14:31.317036: Epoch 74
2024-12-12 00:14:31.317790: Current learning rate: 0.00933
2024-12-12 00:15:58.156804: Validation loss did not improve from -0.45595. Patience: 12/50
2024-12-12 00:15:58.158265: train_loss -0.6748
2024-12-12 00:15:58.159214: val_loss -0.3669
2024-12-12 00:15:58.159914: Pseudo dice [0.6535]
2024-12-12 00:15:58.160566: Epoch time: 86.84 s
2024-12-12 00:15:59.853136: 
2024-12-12 00:15:59.855816: Epoch 75
2024-12-12 00:15:59.856788: Current learning rate: 0.00932
2024-12-12 00:17:26.649794: Validation loss did not improve from -0.45595. Patience: 13/50
2024-12-12 00:17:26.650967: train_loss -0.6659
2024-12-12 00:17:26.651859: val_loss -0.3706
2024-12-12 00:17:26.652527: Pseudo dice [0.6517]
2024-12-12 00:17:26.653236: Epoch time: 86.8 s
2024-12-12 00:17:27.905371: 
2024-12-12 00:17:27.907022: Epoch 76
2024-12-12 00:17:27.907826: Current learning rate: 0.00931
2024-12-12 00:18:54.618780: Validation loss did not improve from -0.45595. Patience: 14/50
2024-12-12 00:18:54.619897: train_loss -0.6797
2024-12-12 00:18:54.620790: val_loss -0.2543
2024-12-12 00:18:54.621738: Pseudo dice [0.5975]
2024-12-12 00:18:54.622444: Epoch time: 86.72 s
2024-12-12 00:18:55.900819: 
2024-12-12 00:18:55.902549: Epoch 77
2024-12-12 00:18:55.903376: Current learning rate: 0.0093
2024-12-12 00:20:22.692085: Validation loss did not improve from -0.45595. Patience: 15/50
2024-12-12 00:20:22.693305: train_loss -0.6757
2024-12-12 00:20:22.694060: val_loss -0.4189
2024-12-12 00:20:22.694813: Pseudo dice [0.6859]
2024-12-12 00:20:22.695545: Epoch time: 86.79 s
2024-12-12 00:20:23.980126: 
2024-12-12 00:20:23.982147: Epoch 78
2024-12-12 00:20:23.983042: Current learning rate: 0.0093
2024-12-12 00:21:50.820409: Validation loss did not improve from -0.45595. Patience: 16/50
2024-12-12 00:21:50.821412: train_loss -0.683
2024-12-12 00:21:50.822245: val_loss -0.2831
2024-12-12 00:21:50.823064: Pseudo dice [0.6101]
2024-12-12 00:21:50.823941: Epoch time: 86.84 s
2024-12-12 00:21:52.093425: 
2024-12-12 00:21:52.095284: Epoch 79
2024-12-12 00:21:52.096111: Current learning rate: 0.00929
2024-12-12 00:23:18.822432: Validation loss did not improve from -0.45595. Patience: 17/50
2024-12-12 00:23:18.823435: train_loss -0.6817
2024-12-12 00:23:18.824409: val_loss -0.3487
2024-12-12 00:23:18.825158: Pseudo dice [0.6318]
2024-12-12 00:23:18.825900: Epoch time: 86.73 s
2024-12-12 00:23:20.488863: 
2024-12-12 00:23:20.490137: Epoch 80
2024-12-12 00:23:20.490804: Current learning rate: 0.00928
2024-12-12 00:24:47.428527: Validation loss did not improve from -0.45595. Patience: 18/50
2024-12-12 00:24:47.429560: train_loss -0.674
2024-12-12 00:24:47.430570: val_loss -0.351
2024-12-12 00:24:47.431325: Pseudo dice [0.645]
2024-12-12 00:24:47.432085: Epoch time: 86.94 s
2024-12-12 00:24:48.691613: 
2024-12-12 00:24:48.693136: Epoch 81
2024-12-12 00:24:48.694025: Current learning rate: 0.00927
2024-12-12 00:26:15.755530: Validation loss did not improve from -0.45595. Patience: 19/50
2024-12-12 00:26:15.756609: train_loss -0.6741
2024-12-12 00:26:15.757611: val_loss -0.3715
2024-12-12 00:26:15.758548: Pseudo dice [0.6648]
2024-12-12 00:26:15.759569: Epoch time: 87.07 s
2024-12-12 00:26:17.035369: 
2024-12-12 00:26:17.036857: Epoch 82
2024-12-12 00:26:17.037650: Current learning rate: 0.00926
2024-12-12 00:27:46.165754: Validation loss did not improve from -0.45595. Patience: 20/50
2024-12-12 00:27:46.170003: train_loss -0.6807
2024-12-12 00:27:46.171520: val_loss -0.351
2024-12-12 00:27:46.172452: Pseudo dice [0.6415]
2024-12-12 00:27:46.173339: Epoch time: 89.13 s
2024-12-12 00:27:47.503326: 
2024-12-12 00:27:47.505011: Epoch 83
2024-12-12 00:27:47.505824: Current learning rate: 0.00925
2024-12-12 00:29:14.444121: Validation loss did not improve from -0.45595. Patience: 21/50
2024-12-12 00:29:14.445487: train_loss -0.6868
2024-12-12 00:29:14.446596: val_loss -0.3246
2024-12-12 00:29:14.447646: Pseudo dice [0.6376]
2024-12-12 00:29:14.448497: Epoch time: 86.94 s
2024-12-12 00:29:16.058317: 
2024-12-12 00:29:16.059873: Epoch 84
2024-12-12 00:29:16.060899: Current learning rate: 0.00924
2024-12-12 00:30:42.851525: Validation loss did not improve from -0.45595. Patience: 22/50
2024-12-12 00:30:42.852619: train_loss -0.6905
2024-12-12 00:30:42.853727: val_loss -0.3989
2024-12-12 00:30:42.854433: Pseudo dice [0.6911]
2024-12-12 00:30:42.855293: Epoch time: 86.8 s
2024-12-12 00:30:44.454505: 
2024-12-12 00:30:44.456116: Epoch 85
2024-12-12 00:30:44.456958: Current learning rate: 0.00923
2024-12-12 00:32:11.267436: Validation loss did not improve from -0.45595. Patience: 23/50
2024-12-12 00:32:11.268530: train_loss -0.6917
2024-12-12 00:32:11.269433: val_loss -0.3437
2024-12-12 00:32:11.270056: Pseudo dice [0.6439]
2024-12-12 00:32:11.270626: Epoch time: 86.81 s
2024-12-12 00:32:12.538013: 
2024-12-12 00:32:12.539844: Epoch 86
2024-12-12 00:32:12.540962: Current learning rate: 0.00922
2024-12-12 00:33:39.373972: Validation loss did not improve from -0.45595. Patience: 24/50
2024-12-12 00:33:39.375221: train_loss -0.6917
2024-12-12 00:33:39.376451: val_loss -0.3381
2024-12-12 00:33:39.377413: Pseudo dice [0.6615]
2024-12-12 00:33:39.378534: Epoch time: 86.84 s
2024-12-12 00:33:40.595474: 
2024-12-12 00:33:40.597183: Epoch 87
2024-12-12 00:33:40.598210: Current learning rate: 0.00921
2024-12-12 00:35:07.816102: Validation loss did not improve from -0.45595. Patience: 25/50
2024-12-12 00:35:07.818571: train_loss -0.6857
2024-12-12 00:35:07.819407: val_loss -0.4012
2024-12-12 00:35:07.820064: Pseudo dice [0.6885]
2024-12-12 00:35:07.820732: Epoch time: 87.22 s
2024-12-12 00:35:09.075378: 
2024-12-12 00:35:09.077263: Epoch 88
2024-12-12 00:35:09.078038: Current learning rate: 0.0092
2024-12-12 00:36:35.877065: Validation loss did not improve from -0.45595. Patience: 26/50
2024-12-12 00:36:35.878227: train_loss -0.6866
2024-12-12 00:36:35.879404: val_loss -0.3902
2024-12-12 00:36:35.880299: Pseudo dice [0.6602]
2024-12-12 00:36:35.881233: Epoch time: 86.8 s
2024-12-12 00:36:37.101547: 
2024-12-12 00:36:37.103358: Epoch 89
2024-12-12 00:36:37.104362: Current learning rate: 0.0092
2024-12-12 00:38:03.835843: Validation loss did not improve from -0.45595. Patience: 27/50
2024-12-12 00:38:03.837361: train_loss -0.6932
2024-12-12 00:38:03.838586: val_loss -0.3259
2024-12-12 00:38:03.839489: Pseudo dice [0.6426]
2024-12-12 00:38:03.840693: Epoch time: 86.74 s
2024-12-12 00:38:05.475457: 
2024-12-12 00:38:05.477099: Epoch 90
2024-12-12 00:38:05.477981: Current learning rate: 0.00919
2024-12-12 00:39:33.191013: Validation loss did not improve from -0.45595. Patience: 28/50
2024-12-12 00:39:33.192225: train_loss -0.6994
2024-12-12 00:39:33.193261: val_loss -0.3522
2024-12-12 00:39:33.194103: Pseudo dice [0.6429]
2024-12-12 00:39:33.195174: Epoch time: 87.72 s
2024-12-12 00:39:34.521980: 
2024-12-12 00:39:34.523764: Epoch 91
2024-12-12 00:39:34.524684: Current learning rate: 0.00918
2024-12-12 00:41:02.123047: Validation loss did not improve from -0.45595. Patience: 29/50
2024-12-12 00:41:02.124255: train_loss -0.6937
2024-12-12 00:41:02.125125: val_loss -0.3177
2024-12-12 00:41:02.126021: Pseudo dice [0.6459]
2024-12-12 00:41:02.126899: Epoch time: 87.6 s
2024-12-12 00:41:03.415478: 
2024-12-12 00:41:03.416793: Epoch 92
2024-12-12 00:41:03.417721: Current learning rate: 0.00917
2024-12-12 00:42:30.151652: Validation loss did not improve from -0.45595. Patience: 30/50
2024-12-12 00:42:30.152846: train_loss -0.6933
2024-12-12 00:42:30.153788: val_loss -0.3734
2024-12-12 00:42:30.154663: Pseudo dice [0.6733]
2024-12-12 00:42:30.155354: Epoch time: 86.74 s
2024-12-12 00:42:31.331791: 
2024-12-12 00:42:31.333714: Epoch 93
2024-12-12 00:42:31.334480: Current learning rate: 0.00916
2024-12-12 00:43:58.080492: Validation loss did not improve from -0.45595. Patience: 31/50
2024-12-12 00:43:58.081712: train_loss -0.6962
2024-12-12 00:43:58.082458: val_loss -0.3094
2024-12-12 00:43:58.083132: Pseudo dice [0.6327]
2024-12-12 00:43:58.083766: Epoch time: 86.75 s
2024-12-12 00:43:59.287504: 
2024-12-12 00:43:59.289663: Epoch 94
2024-12-12 00:43:59.290634: Current learning rate: 0.00915
2024-12-12 00:45:25.984152: Validation loss did not improve from -0.45595. Patience: 32/50
2024-12-12 00:45:25.985093: train_loss -0.6992
2024-12-12 00:45:25.986051: val_loss -0.3963
2024-12-12 00:45:25.986885: Pseudo dice [0.66]
2024-12-12 00:45:25.987567: Epoch time: 86.7 s
2024-12-12 00:45:27.937530: 
2024-12-12 00:45:27.938777: Epoch 95
2024-12-12 00:45:27.939689: Current learning rate: 0.00914
2024-12-12 00:46:55.353522: Validation loss did not improve from -0.45595. Patience: 33/50
2024-12-12 00:46:55.354608: train_loss -0.6931
2024-12-12 00:46:55.355489: val_loss -0.3703
2024-12-12 00:46:55.356231: Pseudo dice [0.6654]
2024-12-12 00:46:55.357016: Epoch time: 87.42 s
2024-12-12 00:46:56.569609: 
2024-12-12 00:46:56.571501: Epoch 96
2024-12-12 00:46:56.572354: Current learning rate: 0.00913
2024-12-12 00:48:23.983139: Validation loss did not improve from -0.45595. Patience: 34/50
2024-12-12 00:48:23.984268: train_loss -0.6922
2024-12-12 00:48:23.985587: val_loss -0.3057
2024-12-12 00:48:23.986650: Pseudo dice [0.618]
2024-12-12 00:48:23.987546: Epoch time: 87.42 s
2024-12-12 00:48:25.235837: 
2024-12-12 00:48:25.238071: Epoch 97
2024-12-12 00:48:25.239157: Current learning rate: 0.00912
2024-12-12 00:49:52.696422: Validation loss did not improve from -0.45595. Patience: 35/50
2024-12-12 00:49:52.697870: train_loss -0.6928
2024-12-12 00:49:52.698930: val_loss -0.4286
2024-12-12 00:49:52.699595: Pseudo dice [0.689]
2024-12-12 00:49:52.700331: Epoch time: 87.46 s
2024-12-12 00:49:53.925496: 
2024-12-12 00:49:53.926831: Epoch 98
2024-12-12 00:49:53.927862: Current learning rate: 0.00911
2024-12-12 00:51:21.252688: Validation loss did not improve from -0.45595. Patience: 36/50
2024-12-12 00:51:21.253968: train_loss -0.6954
2024-12-12 00:51:21.255333: val_loss -0.4185
2024-12-12 00:51:21.256354: Pseudo dice [0.6923]
2024-12-12 00:51:21.257233: Epoch time: 87.33 s
2024-12-12 00:51:22.460699: 
2024-12-12 00:51:22.462739: Epoch 99
2024-12-12 00:51:22.463660: Current learning rate: 0.0091
2024-12-12 00:52:49.206224: Validation loss improved from -0.45595 to -0.46703! Patience: 36/50
2024-12-12 00:52:49.207467: train_loss -0.6969
2024-12-12 00:52:49.208529: val_loss -0.467
2024-12-12 00:52:49.209270: Pseudo dice [0.7084]
2024-12-12 00:52:49.209946: Epoch time: 86.75 s
2024-12-12 00:52:50.805134: 
2024-12-12 00:52:50.806729: Epoch 100
2024-12-12 00:52:50.807808: Current learning rate: 0.0091
2024-12-12 00:54:18.055867: Validation loss did not improve from -0.46703. Patience: 1/50
2024-12-12 00:54:18.056855: train_loss -0.6983
2024-12-12 00:54:18.057761: val_loss -0.0101
2024-12-12 00:54:18.058642: Pseudo dice [0.4636]
2024-12-12 00:54:18.059374: Epoch time: 87.25 s
2024-12-12 00:54:19.288160: 
2024-12-12 00:54:19.289876: Epoch 101
2024-12-12 00:54:19.290808: Current learning rate: 0.00909
2024-12-12 00:55:46.126655: Validation loss did not improve from -0.46703. Patience: 2/50
2024-12-12 00:55:46.127648: train_loss -0.7013
2024-12-12 00:55:46.128938: val_loss -0.376
2024-12-12 00:55:46.129553: Pseudo dice [0.6711]
2024-12-12 00:55:46.130157: Epoch time: 86.84 s
2024-12-12 00:55:47.336502: 
2024-12-12 00:55:47.338136: Epoch 102
2024-12-12 00:55:47.338869: Current learning rate: 0.00908
2024-12-12 00:57:14.226183: Validation loss did not improve from -0.46703. Patience: 3/50
2024-12-12 00:57:14.227534: train_loss -0.7044
2024-12-12 00:57:14.228495: val_loss -0.4154
2024-12-12 00:57:14.229256: Pseudo dice [0.6801]
2024-12-12 00:57:14.229949: Epoch time: 86.89 s
2024-12-12 00:57:15.500776: 
2024-12-12 00:57:15.502307: Epoch 103
2024-12-12 00:57:15.503016: Current learning rate: 0.00907
2024-12-12 00:58:42.387527: Validation loss did not improve from -0.46703. Patience: 4/50
2024-12-12 00:58:42.388513: train_loss -0.7067
2024-12-12 00:58:42.389587: val_loss -0.433
2024-12-12 00:58:42.390771: Pseudo dice [0.7004]
2024-12-12 00:58:42.391620: Epoch time: 86.89 s
2024-12-12 00:58:43.617599: 
2024-12-12 00:58:43.619190: Epoch 104
2024-12-12 00:58:43.619849: Current learning rate: 0.00906
2024-12-12 01:00:10.427015: Validation loss did not improve from -0.46703. Patience: 5/50
2024-12-12 01:00:10.428120: train_loss -0.7093
2024-12-12 01:00:10.429031: val_loss -0.4132
2024-12-12 01:00:10.429703: Pseudo dice [0.6904]
2024-12-12 01:00:10.430385: Epoch time: 86.81 s
2024-12-12 01:00:12.011111: 
2024-12-12 01:00:12.012814: Epoch 105
2024-12-12 01:00:12.013670: Current learning rate: 0.00905
2024-12-12 01:01:38.790005: Validation loss did not improve from -0.46703. Patience: 6/50
2024-12-12 01:01:38.791094: train_loss -0.7056
2024-12-12 01:01:38.791824: val_loss -0.3346
2024-12-12 01:01:38.792438: Pseudo dice [0.6341]
2024-12-12 01:01:38.793196: Epoch time: 86.78 s
2024-12-12 01:01:40.011125: 
2024-12-12 01:01:40.013022: Epoch 106
2024-12-12 01:01:40.013721: Current learning rate: 0.00904
2024-12-12 01:03:06.885173: Validation loss did not improve from -0.46703. Patience: 7/50
2024-12-12 01:03:06.886347: train_loss -0.7088
2024-12-12 01:03:06.887156: val_loss -0.3894
2024-12-12 01:03:06.887970: Pseudo dice [0.6731]
2024-12-12 01:03:06.888581: Epoch time: 86.88 s
2024-12-12 01:03:08.448261: 
2024-12-12 01:03:08.450095: Epoch 107
2024-12-12 01:03:08.450870: Current learning rate: 0.00903
2024-12-12 01:04:35.298162: Validation loss did not improve from -0.46703. Patience: 8/50
2024-12-12 01:04:35.299219: train_loss -0.714
2024-12-12 01:04:35.300359: val_loss -0.4374
2024-12-12 01:04:35.301422: Pseudo dice [0.6971]
2024-12-12 01:04:35.302346: Epoch time: 86.85 s
2024-12-12 01:04:36.545322: 
2024-12-12 01:04:36.547007: Epoch 108
2024-12-12 01:04:36.547952: Current learning rate: 0.00902
2024-12-12 01:06:03.382896: Validation loss did not improve from -0.46703. Patience: 9/50
2024-12-12 01:06:03.383853: train_loss -0.7085
2024-12-12 01:06:03.384660: val_loss -0.3906
2024-12-12 01:06:03.385414: Pseudo dice [0.6843]
2024-12-12 01:06:03.386145: Epoch time: 86.84 s
2024-12-12 01:06:04.616861: 
2024-12-12 01:06:04.618397: Epoch 109
2024-12-12 01:06:04.619241: Current learning rate: 0.00901
2024-12-12 01:07:31.628646: Validation loss did not improve from -0.46703. Patience: 10/50
2024-12-12 01:07:31.629789: train_loss -0.7029
2024-12-12 01:07:31.630532: val_loss -0.4351
2024-12-12 01:07:31.631268: Pseudo dice [0.6822]
2024-12-12 01:07:31.631898: Epoch time: 87.01 s
2024-12-12 01:07:33.294449: 
2024-12-12 01:07:33.295734: Epoch 110
2024-12-12 01:07:33.296406: Current learning rate: 0.009
2024-12-12 01:09:01.015331: Validation loss did not improve from -0.46703. Patience: 11/50
2024-12-12 01:09:01.016392: train_loss -0.7048
2024-12-12 01:09:01.017856: val_loss -0.3814
2024-12-12 01:09:01.018569: Pseudo dice [0.6808]
2024-12-12 01:09:01.019305: Epoch time: 87.72 s
2024-12-12 01:09:01.019927: Yayy! New best EMA pseudo Dice: 0.667
2024-12-12 01:09:02.615126: 
2024-12-12 01:09:02.616404: Epoch 111
2024-12-12 01:09:02.617166: Current learning rate: 0.009
2024-12-12 01:10:30.262495: Validation loss did not improve from -0.46703. Patience: 12/50
2024-12-12 01:10:30.264007: train_loss -0.7028
2024-12-12 01:10:30.265517: val_loss -0.4299
2024-12-12 01:10:30.266444: Pseudo dice [0.7035]
2024-12-12 01:10:30.267131: Epoch time: 87.65 s
2024-12-12 01:10:30.267760: Yayy! New best EMA pseudo Dice: 0.6707
2024-12-12 01:10:31.857496: 
2024-12-12 01:10:31.859221: Epoch 112
2024-12-12 01:10:31.859956: Current learning rate: 0.00899
2024-12-12 01:11:59.409421: Validation loss did not improve from -0.46703. Patience: 13/50
2024-12-12 01:11:59.410546: train_loss -0.7136
2024-12-12 01:11:59.411565: val_loss -0.3475
2024-12-12 01:11:59.412548: Pseudo dice [0.6636]
2024-12-12 01:11:59.413362: Epoch time: 87.55 s
2024-12-12 01:12:00.640678: 
2024-12-12 01:12:00.642889: Epoch 113
2024-12-12 01:12:00.643853: Current learning rate: 0.00898
2024-12-12 01:13:28.052892: Validation loss did not improve from -0.46703. Patience: 14/50
2024-12-12 01:13:28.054165: train_loss -0.7107
2024-12-12 01:13:28.055388: val_loss -0.3695
2024-12-12 01:13:28.056179: Pseudo dice [0.6555]
2024-12-12 01:13:28.056874: Epoch time: 87.41 s
2024-12-12 01:13:29.307184: 
2024-12-12 01:13:29.308778: Epoch 114
2024-12-12 01:13:29.309502: Current learning rate: 0.00897
2024-12-12 01:14:56.911575: Validation loss did not improve from -0.46703. Patience: 15/50
2024-12-12 01:14:56.912594: train_loss -0.7191
2024-12-12 01:14:56.913429: val_loss -0.359
2024-12-12 01:14:56.914072: Pseudo dice [0.661]
2024-12-12 01:14:56.914743: Epoch time: 87.61 s
2024-12-12 01:14:58.516124: 
2024-12-12 01:14:58.518167: Epoch 115
2024-12-12 01:14:58.518995: Current learning rate: 0.00896
2024-12-12 01:16:26.053009: Validation loss did not improve from -0.46703. Patience: 16/50
2024-12-12 01:16:26.054352: train_loss -0.7076
2024-12-12 01:16:26.055304: val_loss -0.3145
2024-12-12 01:16:26.056066: Pseudo dice [0.6229]
2024-12-12 01:16:26.056876: Epoch time: 87.54 s
2024-12-12 01:16:27.318402: 
2024-12-12 01:16:27.319495: Epoch 116
2024-12-12 01:16:27.320171: Current learning rate: 0.00895
2024-12-12 01:17:54.856704: Validation loss did not improve from -0.46703. Patience: 17/50
2024-12-12 01:17:54.858093: train_loss -0.7092
2024-12-12 01:17:54.859126: val_loss -0.3787
2024-12-12 01:17:54.859724: Pseudo dice [0.6653]
2024-12-12 01:17:54.860382: Epoch time: 87.54 s
2024-12-12 01:17:56.113919: 
2024-12-12 01:17:56.115483: Epoch 117
2024-12-12 01:17:56.116191: Current learning rate: 0.00894
2024-12-12 01:19:23.679497: Validation loss did not improve from -0.46703. Patience: 18/50
2024-12-12 01:19:23.680716: train_loss -0.7139
2024-12-12 01:19:23.681699: val_loss -0.3913
2024-12-12 01:19:23.682387: Pseudo dice [0.6777]
2024-12-12 01:19:23.683048: Epoch time: 87.57 s
2024-12-12 01:19:25.318264: 
2024-12-12 01:19:25.319860: Epoch 118
2024-12-12 01:19:25.320548: Current learning rate: 0.00893
2024-12-12 01:20:52.859627: Validation loss did not improve from -0.46703. Patience: 19/50
2024-12-12 01:20:52.860657: train_loss -0.7068
2024-12-12 01:20:52.861479: val_loss -0.2524
2024-12-12 01:20:52.862137: Pseudo dice [0.6008]
2024-12-12 01:20:52.862888: Epoch time: 87.54 s
2024-12-12 01:20:54.125483: 
2024-12-12 01:20:54.127403: Epoch 119
2024-12-12 01:20:54.128196: Current learning rate: 0.00892
2024-12-12 01:22:21.117351: Validation loss did not improve from -0.46703. Patience: 20/50
2024-12-12 01:22:21.118821: train_loss -0.7053
2024-12-12 01:22:21.119801: val_loss -0.3464
2024-12-12 01:22:21.120548: Pseudo dice [0.6608]
2024-12-12 01:22:21.121320: Epoch time: 86.99 s
2024-12-12 01:22:22.809952: 
2024-12-12 01:22:22.812079: Epoch 120
2024-12-12 01:22:22.813005: Current learning rate: 0.00891
2024-12-12 01:23:50.455761: Validation loss did not improve from -0.46703. Patience: 21/50
2024-12-12 01:23:50.457101: train_loss -0.7097
2024-12-12 01:23:50.458025: val_loss -0.353
2024-12-12 01:23:50.458710: Pseudo dice [0.6559]
2024-12-12 01:23:50.459496: Epoch time: 87.65 s
2024-12-12 01:23:51.720092: 
2024-12-12 01:23:51.721912: Epoch 121
2024-12-12 01:23:51.722748: Current learning rate: 0.0089
2024-12-12 01:25:19.314058: Validation loss did not improve from -0.46703. Patience: 22/50
2024-12-12 01:25:19.315318: train_loss -0.7149
2024-12-12 01:25:19.316181: val_loss -0.4359
2024-12-12 01:25:19.316834: Pseudo dice [0.6975]
2024-12-12 01:25:19.317550: Epoch time: 87.6 s
2024-12-12 01:25:20.562199: 
2024-12-12 01:25:20.564008: Epoch 122
2024-12-12 01:25:20.565120: Current learning rate: 0.00889
2024-12-12 01:26:48.086591: Validation loss did not improve from -0.46703. Patience: 23/50
2024-12-12 01:26:48.088056: train_loss -0.718
2024-12-12 01:26:48.088845: val_loss -0.3223
2024-12-12 01:26:48.089598: Pseudo dice [0.645]
2024-12-12 01:26:48.090296: Epoch time: 87.53 s
2024-12-12 01:26:49.340138: 
2024-12-12 01:26:49.341742: Epoch 123
2024-12-12 01:26:49.342386: Current learning rate: 0.00889
2024-12-12 01:28:16.877006: Validation loss did not improve from -0.46703. Patience: 24/50
2024-12-12 01:28:16.878382: train_loss -0.7201
2024-12-12 01:28:16.879680: val_loss -0.308
2024-12-12 01:28:16.880466: Pseudo dice [0.6432]
2024-12-12 01:28:16.881235: Epoch time: 87.54 s
2024-12-12 01:28:18.156430: 
2024-12-12 01:28:18.158082: Epoch 124
2024-12-12 01:28:18.158850: Current learning rate: 0.00888
2024-12-12 01:29:45.672202: Validation loss did not improve from -0.46703. Patience: 25/50
2024-12-12 01:29:45.673292: train_loss -0.7184
2024-12-12 01:29:45.674492: val_loss -0.3997
2024-12-12 01:29:45.675466: Pseudo dice [0.675]
2024-12-12 01:29:45.676347: Epoch time: 87.52 s
2024-12-12 01:29:47.297213: 
2024-12-12 01:29:47.299013: Epoch 125
2024-12-12 01:29:47.300010: Current learning rate: 0.00887
2024-12-12 01:31:14.774786: Validation loss did not improve from -0.46703. Patience: 26/50
2024-12-12 01:31:14.775771: train_loss -0.7222
2024-12-12 01:31:14.776748: val_loss -0.3608
2024-12-12 01:31:14.777415: Pseudo dice [0.662]
2024-12-12 01:31:14.778075: Epoch time: 87.48 s
2024-12-12 01:31:16.041702: 
2024-12-12 01:31:16.043120: Epoch 126
2024-12-12 01:31:16.043884: Current learning rate: 0.00886
2024-12-12 01:32:43.877037: Validation loss did not improve from -0.46703. Patience: 27/50
2024-12-12 01:32:43.880763: train_loss -0.7234
2024-12-12 01:32:43.882039: val_loss -0.2657
2024-12-12 01:32:43.882709: Pseudo dice [0.6012]
2024-12-12 01:32:43.883384: Epoch time: 87.84 s
2024-12-12 01:32:45.172716: 
2024-12-12 01:32:45.174186: Epoch 127
2024-12-12 01:32:45.175011: Current learning rate: 0.00885
2024-12-12 01:34:12.646105: Validation loss did not improve from -0.46703. Patience: 28/50
2024-12-12 01:34:12.647157: train_loss -0.7182
2024-12-12 01:34:12.647891: val_loss -0.3264
2024-12-12 01:34:12.648662: Pseudo dice [0.6203]
2024-12-12 01:34:12.649334: Epoch time: 87.48 s
2024-12-12 01:34:14.629391: 
2024-12-12 01:34:14.630968: Epoch 128
2024-12-12 01:34:14.631721: Current learning rate: 0.00884
2024-12-12 01:35:42.250112: Validation loss did not improve from -0.46703. Patience: 29/50
2024-12-12 01:35:42.251257: train_loss -0.7233
2024-12-12 01:35:42.252176: val_loss -0.3298
2024-12-12 01:35:42.252819: Pseudo dice [0.6325]
2024-12-12 01:35:42.253509: Epoch time: 87.62 s
2024-12-12 01:35:43.507633: 
2024-12-12 01:35:43.509057: Epoch 129
2024-12-12 01:35:43.509875: Current learning rate: 0.00883
2024-12-12 01:37:11.098213: Validation loss did not improve from -0.46703. Patience: 30/50
2024-12-12 01:37:11.099339: train_loss -0.7263
2024-12-12 01:37:11.100068: val_loss -0.3497
2024-12-12 01:37:11.100927: Pseudo dice [0.6449]
2024-12-12 01:37:11.101715: Epoch time: 87.59 s
2024-12-12 01:37:12.720107: 
2024-12-12 01:37:12.721913: Epoch 130
2024-12-12 01:37:12.722707: Current learning rate: 0.00882
2024-12-12 01:38:40.304708: Validation loss did not improve from -0.46703. Patience: 31/50
2024-12-12 01:38:40.305970: train_loss -0.7314
2024-12-12 01:38:40.307217: val_loss -0.4027
2024-12-12 01:38:40.308212: Pseudo dice [0.6699]
2024-12-12 01:38:40.309134: Epoch time: 87.59 s
2024-12-12 01:38:41.558351: 
2024-12-12 01:38:41.559745: Epoch 131
2024-12-12 01:38:41.560837: Current learning rate: 0.00881
2024-12-12 01:40:08.675492: Validation loss did not improve from -0.46703. Patience: 32/50
2024-12-12 01:40:08.678373: train_loss -0.7261
2024-12-12 01:40:08.680657: val_loss -0.3166
2024-12-12 01:40:08.681444: Pseudo dice [0.6291]
2024-12-12 01:40:08.682595: Epoch time: 87.12 s
2024-12-12 01:40:10.005826: 
2024-12-12 01:40:10.007684: Epoch 132
2024-12-12 01:40:10.008379: Current learning rate: 0.0088
2024-12-12 01:41:36.957884: Validation loss did not improve from -0.46703. Patience: 33/50
2024-12-12 01:41:36.959168: train_loss -0.7299
2024-12-12 01:41:36.960132: val_loss -0.4037
2024-12-12 01:41:36.960780: Pseudo dice [0.677]
2024-12-12 01:41:36.961461: Epoch time: 86.95 s
2024-12-12 01:41:38.272737: 
2024-12-12 01:41:38.273866: Epoch 133
2024-12-12 01:41:38.274527: Current learning rate: 0.00879
2024-12-12 01:43:05.173878: Validation loss did not improve from -0.46703. Patience: 34/50
2024-12-12 01:43:05.174982: train_loss -0.7237
2024-12-12 01:43:05.176312: val_loss -0.3514
2024-12-12 01:43:05.177237: Pseudo dice [0.6424]
2024-12-12 01:43:05.178261: Epoch time: 86.9 s
2024-12-12 01:43:06.439902: 
2024-12-12 01:43:06.441489: Epoch 134
2024-12-12 01:43:06.442391: Current learning rate: 0.00879
2024-12-12 01:44:33.403870: Validation loss did not improve from -0.46703. Patience: 35/50
2024-12-12 01:44:33.405234: train_loss -0.7269
2024-12-12 01:44:33.406399: val_loss -0.3443
2024-12-12 01:44:33.407245: Pseudo dice [0.6348]
2024-12-12 01:44:33.408207: Epoch time: 86.97 s
2024-12-12 01:44:35.049066: 
2024-12-12 01:44:35.050621: Epoch 135
2024-12-12 01:44:35.051897: Current learning rate: 0.00878
2024-12-12 01:46:02.322265: Validation loss did not improve from -0.46703. Patience: 36/50
2024-12-12 01:46:02.323501: train_loss -0.7292
2024-12-12 01:46:02.324739: val_loss -0.2988
2024-12-12 01:46:02.325439: Pseudo dice [0.6152]
2024-12-12 01:46:02.326133: Epoch time: 87.28 s
2024-12-12 01:46:03.604804: 
2024-12-12 01:46:03.606655: Epoch 136
2024-12-12 01:46:03.607423: Current learning rate: 0.00877
2024-12-12 01:47:30.532014: Validation loss did not improve from -0.46703. Patience: 37/50
2024-12-12 01:47:30.533402: train_loss -0.7243
2024-12-12 01:47:30.534177: val_loss -0.4137
2024-12-12 01:47:30.534852: Pseudo dice [0.6892]
2024-12-12 01:47:30.535463: Epoch time: 86.93 s
2024-12-12 01:47:31.835093: 
2024-12-12 01:47:31.836659: Epoch 137
2024-12-12 01:47:31.837534: Current learning rate: 0.00876
2024-12-12 01:48:58.814021: Validation loss did not improve from -0.46703. Patience: 38/50
2024-12-12 01:48:58.815255: train_loss -0.722
2024-12-12 01:48:58.816346: val_loss -0.36
2024-12-12 01:48:58.817387: Pseudo dice [0.6658]
2024-12-12 01:48:58.818379: Epoch time: 86.98 s
2024-12-12 01:49:00.119268: 
2024-12-12 01:49:00.120920: Epoch 138
2024-12-12 01:49:00.122022: Current learning rate: 0.00875
2024-12-12 01:50:27.066377: Validation loss did not improve from -0.46703. Patience: 39/50
2024-12-12 01:50:27.067288: train_loss -0.7338
2024-12-12 01:50:27.068078: val_loss -0.3415
2024-12-12 01:50:27.068886: Pseudo dice [0.6425]
2024-12-12 01:50:27.069583: Epoch time: 86.95 s
2024-12-12 01:50:28.839946: 
2024-12-12 01:50:28.842373: Epoch 139
2024-12-12 01:50:28.843257: Current learning rate: 0.00874
2024-12-12 01:51:55.954293: Validation loss did not improve from -0.46703. Patience: 40/50
2024-12-12 01:51:55.955374: train_loss -0.7403
2024-12-12 01:51:55.956121: val_loss -0.3521
2024-12-12 01:51:55.956807: Pseudo dice [0.671]
2024-12-12 01:51:55.957437: Epoch time: 87.12 s
2024-12-12 01:51:57.748260: 
2024-12-12 01:51:57.749859: Epoch 140
2024-12-12 01:51:57.750891: Current learning rate: 0.00873
2024-12-12 01:53:24.778615: Validation loss did not improve from -0.46703. Patience: 41/50
2024-12-12 01:53:24.779656: train_loss -0.7333
2024-12-12 01:53:24.780854: val_loss -0.3711
2024-12-12 01:53:24.781718: Pseudo dice [0.6572]
2024-12-12 01:53:24.782540: Epoch time: 87.03 s
2024-12-12 01:53:26.084399: 
2024-12-12 01:53:26.086206: Epoch 141
2024-12-12 01:53:26.087170: Current learning rate: 0.00872
2024-12-12 01:54:53.286606: Validation loss did not improve from -0.46703. Patience: 42/50
2024-12-12 01:54:53.287920: train_loss -0.7386
2024-12-12 01:54:53.288665: val_loss -0.3176
2024-12-12 01:54:53.289299: Pseudo dice [0.647]
2024-12-12 01:54:53.289953: Epoch time: 87.2 s
2024-12-12 01:54:54.621440: 
2024-12-12 01:54:54.623053: Epoch 142
2024-12-12 01:54:54.624031: Current learning rate: 0.00871
2024-12-12 01:56:21.765896: Validation loss did not improve from -0.46703. Patience: 43/50
2024-12-12 01:56:21.767025: train_loss -0.7361
2024-12-12 01:56:21.768041: val_loss -0.3982
2024-12-12 01:56:21.768938: Pseudo dice [0.6843]
2024-12-12 01:56:21.769730: Epoch time: 87.15 s
2024-12-12 01:56:23.054478: 
2024-12-12 01:56:23.056208: Epoch 143
2024-12-12 01:56:23.057405: Current learning rate: 0.0087
2024-12-12 01:57:50.228794: Validation loss did not improve from -0.46703. Patience: 44/50
2024-12-12 01:57:50.229820: train_loss -0.7362
2024-12-12 01:57:50.230593: val_loss -0.3759
2024-12-12 01:57:50.231375: Pseudo dice [0.6748]
2024-12-12 01:57:50.232006: Epoch time: 87.18 s
2024-12-12 01:57:51.558153: 
2024-12-12 01:57:51.559700: Epoch 144
2024-12-12 01:57:51.560629: Current learning rate: 0.00869
2024-12-12 01:59:18.819022: Validation loss did not improve from -0.46703. Patience: 45/50
2024-12-12 01:59:18.820132: train_loss -0.7396
2024-12-12 01:59:18.821012: val_loss -0.3989
2024-12-12 01:59:18.821760: Pseudo dice [0.6793]
2024-12-12 01:59:18.822441: Epoch time: 87.26 s
2024-12-12 01:59:20.462538: 
2024-12-12 01:59:20.464485: Epoch 145
2024-12-12 01:59:20.465383: Current learning rate: 0.00868
2024-12-12 02:00:47.841622: Validation loss did not improve from -0.46703. Patience: 46/50
2024-12-12 02:00:47.843212: train_loss -0.7366
2024-12-12 02:00:47.844204: val_loss -0.324
2024-12-12 02:00:47.845031: Pseudo dice [0.6414]
2024-12-12 02:00:47.846057: Epoch time: 87.38 s
2024-12-12 02:00:49.107114: 
2024-12-12 02:00:49.108513: Epoch 146
2024-12-12 02:00:49.109219: Current learning rate: 0.00868
2024-12-12 02:02:16.450803: Validation loss did not improve from -0.46703. Patience: 47/50
2024-12-12 02:02:16.451728: train_loss -0.7362
2024-12-12 02:02:16.452510: val_loss -0.3462
2024-12-12 02:02:16.453114: Pseudo dice [0.6401]
2024-12-12 02:02:16.453720: Epoch time: 87.35 s
2024-12-12 02:02:17.700754: 
2024-12-12 02:02:17.702966: Epoch 147
2024-12-12 02:02:17.703762: Current learning rate: 0.00867
2024-12-12 02:03:45.079138: Validation loss did not improve from -0.46703. Patience: 48/50
2024-12-12 02:03:45.080166: train_loss -0.7363
2024-12-12 02:03:45.081437: val_loss -0.3402
2024-12-12 02:03:45.082255: Pseudo dice [0.6551]
2024-12-12 02:03:45.083499: Epoch time: 87.38 s
2024-12-12 02:03:46.370356: 
2024-12-12 02:03:46.372334: Epoch 148
2024-12-12 02:03:46.373153: Current learning rate: 0.00866
2024-12-12 02:05:13.667188: Validation loss did not improve from -0.46703. Patience: 49/50
2024-12-12 02:05:13.668272: train_loss -0.7356
2024-12-12 02:05:13.669157: val_loss -0.39
2024-12-12 02:05:13.669919: Pseudo dice [0.6683]
2024-12-12 02:05:13.670679: Epoch time: 87.3 s
2024-12-12 02:05:14.949927: 
2024-12-12 02:05:14.951277: Epoch 149
2024-12-12 02:05:14.952080: Current learning rate: 0.00865
2024-12-12 02:06:42.261565: Validation loss did not improve from -0.46703. Patience: 50/50
2024-12-12 02:06:42.262504: train_loss -0.7349
2024-12-12 02:06:42.263357: val_loss -0.1965
2024-12-12 02:06:42.264048: Pseudo dice [0.582]
2024-12-12 02:06:42.264859: Epoch time: 87.31 s
2024-12-12 02:06:44.317112: Patience reached. Stopping training.
2024-12-12 02:06:44.731331: Training done.
2024-12-12 02:06:45.403436: Using splits from existing split file: /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset307_Sohee_Calcium_OCT_CrossValidation/splits_final.json
2024-12-12 02:06:45.419379: The split file contains 5 splits.
2024-12-12 02:06:45.420779: Desired fold for training: 3
2024-12-12 02:06:45.422031: This split has 7 training and 1 validation cases.
2024-12-12 02:06:45.423161: predicting 701-013
2024-12-12 02:06:45.452409: 701-013, shape torch.Size([1, 375, 498, 498]), rank 0
2024-12-12 02:10:52.055424: Validation complete
2024-12-12 02:10:52.056076: Mean Validation Dice:  0.6124900567824043
2024-12-11 22:24:19.262389: unpacking done...
2024-12-11 22:24:19.275044: Unable to plot network architecture: nnUNet_compile is enabled!
2024-12-11 22:24:19.406994: 
2024-12-11 22:24:19.408750: Epoch 0
2024-12-11 22:24:19.409771: Current learning rate: 0.01
2024-12-11 22:26:50.370044: Validation loss improved from 1000.00000 to -0.20024! Patience: 0/50
2024-12-11 22:26:50.371005: train_loss -0.0553
2024-12-11 22:26:50.371907: val_loss -0.2002
2024-12-11 22:26:50.372605: Pseudo dice [0.5267]
2024-12-11 22:26:50.373291: Epoch time: 150.97 s
2024-12-11 22:26:50.374062: Yayy! New best EMA pseudo Dice: 0.5267
2024-12-11 22:26:51.758228: 
2024-12-11 22:26:51.759941: Epoch 1
2024-12-11 22:26:51.761102: Current learning rate: 0.00999
2024-12-11 22:28:20.997826: Validation loss improved from -0.20024 to -0.26705! Patience: 0/50
2024-12-11 22:28:20.998976: train_loss -0.2063
2024-12-11 22:28:20.999986: val_loss -0.2671
2024-12-11 22:28:21.000780: Pseudo dice [0.5622]
2024-12-11 22:28:21.001596: Epoch time: 89.24 s
2024-12-11 22:28:21.002389: Yayy! New best EMA pseudo Dice: 0.5302
2024-12-11 22:28:22.612545: 
2024-12-11 22:28:22.614294: Epoch 2
2024-12-11 22:28:22.615098: Current learning rate: 0.00998
2024-12-11 22:29:52.542698: Validation loss improved from -0.26705 to -0.28688! Patience: 0/50
2024-12-11 22:29:52.543880: train_loss -0.2705
2024-12-11 22:29:52.545000: val_loss -0.2869
2024-12-11 22:29:52.545856: Pseudo dice [0.5871]
2024-12-11 22:29:52.546573: Epoch time: 89.93 s
2024-12-11 22:29:52.547368: Yayy! New best EMA pseudo Dice: 0.5359
2024-12-11 22:29:54.144266: 
2024-12-11 22:29:54.146097: Epoch 3
2024-12-11 22:29:54.147096: Current learning rate: 0.00997
2024-12-11 22:31:24.412227: Validation loss did not improve from -0.28688. Patience: 1/50
2024-12-11 22:31:24.413555: train_loss -0.2836
2024-12-11 22:31:24.414420: val_loss -0.2824
2024-12-11 22:31:24.415156: Pseudo dice [0.5889]
2024-12-11 22:31:24.416127: Epoch time: 90.27 s
2024-12-11 22:31:24.416967: Yayy! New best EMA pseudo Dice: 0.5412
2024-12-11 22:31:25.997467: 
2024-12-11 22:31:25.998930: Epoch 4
2024-12-11 22:31:25.999742: Current learning rate: 0.00996
2024-12-11 22:32:56.256966: Validation loss improved from -0.28688 to -0.31848! Patience: 1/50
2024-12-11 22:32:56.258072: train_loss -0.328
2024-12-11 22:32:56.259040: val_loss -0.3185
2024-12-11 22:32:56.259983: Pseudo dice [0.6062]
2024-12-11 22:32:56.260869: Epoch time: 90.26 s
2024-12-11 22:32:56.640619: Yayy! New best EMA pseudo Dice: 0.5477
2024-12-11 22:32:58.337502: 
2024-12-11 22:32:58.339384: Epoch 5
2024-12-11 22:32:58.340112: Current learning rate: 0.00995
2024-12-11 22:34:28.633861: Validation loss improved from -0.31848 to -0.38371! Patience: 0/50
2024-12-11 22:34:28.635100: train_loss -0.3533
2024-12-11 22:34:28.636039: val_loss -0.3837
2024-12-11 22:34:28.636845: Pseudo dice [0.6292]
2024-12-11 22:34:28.637656: Epoch time: 90.3 s
2024-12-11 22:34:28.638457: Yayy! New best EMA pseudo Dice: 0.5558
2024-12-11 22:34:30.209491: 
2024-12-11 22:34:30.211224: Epoch 6
2024-12-11 22:34:30.211965: Current learning rate: 0.00995
2024-12-11 22:36:00.487317: Validation loss improved from -0.38371 to -0.39335! Patience: 0/50
2024-12-11 22:36:00.488557: train_loss -0.3829
2024-12-11 22:36:00.489592: val_loss -0.3934
2024-12-11 22:36:00.490408: Pseudo dice [0.6408]
2024-12-11 22:36:00.491330: Epoch time: 90.28 s
2024-12-11 22:36:00.492113: Yayy! New best EMA pseudo Dice: 0.5643
2024-12-11 22:36:02.047308: 
2024-12-11 22:36:02.049644: Epoch 7
2024-12-11 22:36:02.050657: Current learning rate: 0.00994
2024-12-11 22:37:32.330353: Validation loss did not improve from -0.39335. Patience: 1/50
2024-12-11 22:37:32.331425: train_loss -0.3921
2024-12-11 22:37:32.332437: val_loss -0.388
2024-12-11 22:37:32.333165: Pseudo dice [0.6427]
2024-12-11 22:37:32.333949: Epoch time: 90.29 s
2024-12-11 22:37:32.334629: Yayy! New best EMA pseudo Dice: 0.5722
2024-12-11 22:37:34.240677: 
2024-12-11 22:37:34.242228: Epoch 8
2024-12-11 22:37:34.243085: Current learning rate: 0.00993
2024-12-11 22:39:04.945835: Validation loss improved from -0.39335 to -0.40052! Patience: 1/50
2024-12-11 22:39:04.946882: train_loss -0.394
2024-12-11 22:39:04.947793: val_loss -0.4005
2024-12-11 22:39:04.948572: Pseudo dice [0.6537]
2024-12-11 22:39:04.949494: Epoch time: 90.71 s
2024-12-11 22:39:04.950487: Yayy! New best EMA pseudo Dice: 0.5803
2024-12-11 22:39:06.576343: 
2024-12-11 22:39:06.578118: Epoch 9
2024-12-11 22:39:06.579134: Current learning rate: 0.00992
2024-12-11 22:40:37.396128: Validation loss improved from -0.40052 to -0.40789! Patience: 0/50
2024-12-11 22:40:37.397011: train_loss -0.4175
2024-12-11 22:40:37.397851: val_loss -0.4079
2024-12-11 22:40:37.398632: Pseudo dice [0.6475]
2024-12-11 22:40:37.399402: Epoch time: 90.82 s
2024-12-11 22:40:37.743723: Yayy! New best EMA pseudo Dice: 0.587
2024-12-11 22:40:39.356774: 
2024-12-11 22:40:39.358113: Epoch 10
2024-12-11 22:40:39.359314: Current learning rate: 0.00991
2024-12-11 22:42:10.113051: Validation loss improved from -0.40789 to -0.45131! Patience: 0/50
2024-12-11 22:42:10.114314: train_loss -0.4286
2024-12-11 22:42:10.115175: val_loss -0.4513
2024-12-11 22:42:10.115874: Pseudo dice [0.6769]
2024-12-11 22:42:10.116900: Epoch time: 90.76 s
2024-12-11 22:42:10.117542: Yayy! New best EMA pseudo Dice: 0.596
2024-12-11 22:42:11.727585: 
2024-12-11 22:42:11.729123: Epoch 11
2024-12-11 22:42:11.729950: Current learning rate: 0.0099
2024-12-11 22:43:42.750390: Validation loss improved from -0.45131 to -0.48560! Patience: 0/50
2024-12-11 22:43:42.751647: train_loss -0.444
2024-12-11 22:43:42.752600: val_loss -0.4856
2024-12-11 22:43:42.753505: Pseudo dice [0.697]
2024-12-11 22:43:42.754228: Epoch time: 91.02 s
2024-12-11 22:43:42.754998: Yayy! New best EMA pseudo Dice: 0.6061
2024-12-11 22:43:44.284313: 
2024-12-11 22:43:44.286408: Epoch 12
2024-12-11 22:43:44.287382: Current learning rate: 0.00989
2024-12-11 22:45:15.305310: Validation loss did not improve from -0.48560. Patience: 1/50
2024-12-11 22:45:15.306412: train_loss -0.4467
2024-12-11 22:45:15.307361: val_loss -0.4625
2024-12-11 22:45:15.308173: Pseudo dice [0.6856]
2024-12-11 22:45:15.308946: Epoch time: 91.02 s
2024-12-11 22:45:15.309769: Yayy! New best EMA pseudo Dice: 0.6141
2024-12-11 22:45:16.945085: 
2024-12-11 22:45:16.946768: Epoch 13
2024-12-11 22:45:16.947618: Current learning rate: 0.00988
2024-12-11 22:46:47.912334: Validation loss did not improve from -0.48560. Patience: 2/50
2024-12-11 22:46:47.913138: train_loss -0.469
2024-12-11 22:46:47.914084: val_loss -0.4069
2024-12-11 22:46:47.914726: Pseudo dice [0.6489]
2024-12-11 22:46:47.915457: Epoch time: 90.97 s
2024-12-11 22:46:47.916249: Yayy! New best EMA pseudo Dice: 0.6175
2024-12-11 22:46:49.504110: 
2024-12-11 22:46:49.505758: Epoch 14
2024-12-11 22:46:49.506602: Current learning rate: 0.00987
2024-12-11 22:48:20.314603: Validation loss did not improve from -0.48560. Patience: 3/50
2024-12-11 22:48:20.315866: train_loss -0.4589
2024-12-11 22:48:20.317434: val_loss -0.4667
2024-12-11 22:48:20.318383: Pseudo dice [0.6903]
2024-12-11 22:48:20.319341: Epoch time: 90.81 s
2024-12-11 22:48:20.663888: Yayy! New best EMA pseudo Dice: 0.6248
2024-12-11 22:48:22.256944: 
2024-12-11 22:48:22.258399: Epoch 15
2024-12-11 22:48:22.259639: Current learning rate: 0.00986
2024-12-11 22:49:52.884580: Validation loss did not improve from -0.48560. Patience: 4/50
2024-12-11 22:49:52.885772: train_loss -0.4842
2024-12-11 22:49:52.886989: val_loss -0.4356
2024-12-11 22:49:52.888122: Pseudo dice [0.6709]
2024-12-11 22:49:52.889233: Epoch time: 90.63 s
2024-12-11 22:49:52.890350: Yayy! New best EMA pseudo Dice: 0.6294
2024-12-11 22:49:54.478589: 
2024-12-11 22:49:54.480268: Epoch 16
2024-12-11 22:49:54.481482: Current learning rate: 0.00986
2024-12-11 22:51:25.438751: Validation loss improved from -0.48560 to -0.50068! Patience: 4/50
2024-12-11 22:51:25.439431: train_loss -0.4881
2024-12-11 22:51:25.440156: val_loss -0.5007
2024-12-11 22:51:25.440813: Pseudo dice [0.7094]
2024-12-11 22:51:25.441469: Epoch time: 90.96 s
2024-12-11 22:51:25.442131: Yayy! New best EMA pseudo Dice: 0.6374
2024-12-11 22:51:27.047736: 
2024-12-11 22:51:27.049861: Epoch 17
2024-12-11 22:51:27.050893: Current learning rate: 0.00985
2024-12-11 22:52:57.908483: Validation loss did not improve from -0.50068. Patience: 1/50
2024-12-11 22:52:57.909680: train_loss -0.4903
2024-12-11 22:52:57.910716: val_loss -0.4805
2024-12-11 22:52:57.911546: Pseudo dice [0.6944]
2024-12-11 22:52:57.912627: Epoch time: 90.86 s
2024-12-11 22:52:57.913847: Yayy! New best EMA pseudo Dice: 0.6431
2024-12-11 22:52:59.526640: 
2024-12-11 22:52:59.529005: Epoch 18
2024-12-11 22:52:59.530117: Current learning rate: 0.00984
2024-12-11 22:54:30.425097: Validation loss did not improve from -0.50068. Patience: 2/50
2024-12-11 22:54:30.426355: train_loss -0.5122
2024-12-11 22:54:30.427393: val_loss -0.454
2024-12-11 22:54:30.428134: Pseudo dice [0.6779]
2024-12-11 22:54:30.428813: Epoch time: 90.9 s
2024-12-11 22:54:30.429518: Yayy! New best EMA pseudo Dice: 0.6466
2024-12-11 22:54:32.369516: 
2024-12-11 22:54:32.371228: Epoch 19
2024-12-11 22:54:32.372088: Current learning rate: 0.00983
2024-12-11 22:56:03.156110: Validation loss did not improve from -0.50068. Patience: 3/50
2024-12-11 22:56:03.157198: train_loss -0.5165
2024-12-11 22:56:03.158230: val_loss -0.4788
2024-12-11 22:56:03.159023: Pseudo dice [0.7008]
2024-12-11 22:56:03.159717: Epoch time: 90.79 s
2024-12-11 22:56:03.500040: Yayy! New best EMA pseudo Dice: 0.652
2024-12-11 22:56:05.100461: 
2024-12-11 22:56:05.102083: Epoch 20
2024-12-11 22:56:05.103022: Current learning rate: 0.00982
2024-12-11 22:57:35.918624: Validation loss improved from -0.50068 to -0.51158! Patience: 3/50
2024-12-11 22:57:35.919547: train_loss -0.5148
2024-12-11 22:57:35.920337: val_loss -0.5116
2024-12-11 22:57:35.921077: Pseudo dice [0.7119]
2024-12-11 22:57:35.921738: Epoch time: 90.82 s
2024-12-11 22:57:35.922403: Yayy! New best EMA pseudo Dice: 0.658
2024-12-11 22:57:37.527892: 
2024-12-11 22:57:37.529614: Epoch 21
2024-12-11 22:57:37.530597: Current learning rate: 0.00981
2024-12-11 22:59:08.358788: Validation loss did not improve from -0.51158. Patience: 1/50
2024-12-11 22:59:08.359558: train_loss -0.5295
2024-12-11 22:59:08.360403: val_loss -0.4947
2024-12-11 22:59:08.361241: Pseudo dice [0.7129]
2024-12-11 22:59:08.362032: Epoch time: 90.83 s
2024-12-11 22:59:08.362798: Yayy! New best EMA pseudo Dice: 0.6635
2024-12-11 22:59:09.854275: 
2024-12-11 22:59:09.856215: Epoch 22
2024-12-11 22:59:09.857236: Current learning rate: 0.0098
2024-12-11 23:00:40.694968: Validation loss did not improve from -0.51158. Patience: 2/50
2024-12-11 23:00:40.695857: train_loss -0.5258
2024-12-11 23:00:40.696748: val_loss -0.45
2024-12-11 23:00:40.697737: Pseudo dice [0.6884]
2024-12-11 23:00:40.698524: Epoch time: 90.84 s
2024-12-11 23:00:40.699299: Yayy! New best EMA pseudo Dice: 0.666
2024-12-11 23:00:42.275145: 
2024-12-11 23:00:42.276989: Epoch 23
2024-12-11 23:00:42.278034: Current learning rate: 0.00979
2024-12-11 23:02:13.080372: Validation loss improved from -0.51158 to -0.51734! Patience: 2/50
2024-12-11 23:02:13.081130: train_loss -0.5323
2024-12-11 23:02:13.082181: val_loss -0.5173
2024-12-11 23:02:13.082889: Pseudo dice [0.7217]
2024-12-11 23:02:13.083678: Epoch time: 90.81 s
2024-12-11 23:02:13.084291: Yayy! New best EMA pseudo Dice: 0.6716
2024-12-11 23:02:14.643577: 
2024-12-11 23:02:14.645062: Epoch 24
2024-12-11 23:02:14.645931: Current learning rate: 0.00978
2024-12-11 23:03:45.642348: Validation loss improved from -0.51734 to -0.53201! Patience: 0/50
2024-12-11 23:03:45.643563: train_loss -0.5421
2024-12-11 23:03:45.644470: val_loss -0.532
2024-12-11 23:03:45.645262: Pseudo dice [0.7202]
2024-12-11 23:03:45.646161: Epoch time: 91.0 s
2024-12-11 23:03:45.992916: Yayy! New best EMA pseudo Dice: 0.6764
2024-12-11 23:03:47.536974: 
2024-12-11 23:03:47.538973: Epoch 25
2024-12-11 23:03:47.540305: Current learning rate: 0.00977
2024-12-11 23:05:18.315694: Validation loss did not improve from -0.53201. Patience: 1/50
2024-12-11 23:05:18.316391: train_loss -0.5461
2024-12-11 23:05:18.317485: val_loss -0.4912
2024-12-11 23:05:18.318356: Pseudo dice [0.7002]
2024-12-11 23:05:18.319124: Epoch time: 90.78 s
2024-12-11 23:05:18.319818: Yayy! New best EMA pseudo Dice: 0.6788
2024-12-11 23:05:19.861276: 
2024-12-11 23:05:19.862862: Epoch 26
2024-12-11 23:05:19.864048: Current learning rate: 0.00977
2024-12-11 23:06:50.800524: Validation loss did not improve from -0.53201. Patience: 2/50
2024-12-11 23:06:50.801754: train_loss -0.553
2024-12-11 23:06:50.803293: val_loss -0.5255
2024-12-11 23:06:50.804290: Pseudo dice [0.7237]
2024-12-11 23:06:50.805403: Epoch time: 90.94 s
2024-12-11 23:06:50.806334: Yayy! New best EMA pseudo Dice: 0.6833
2024-12-11 23:06:52.343929: 
2024-12-11 23:06:52.345715: Epoch 27
2024-12-11 23:06:52.346729: Current learning rate: 0.00976
2024-12-11 23:08:23.295920: Validation loss did not improve from -0.53201. Patience: 3/50
2024-12-11 23:08:23.296870: train_loss -0.55
2024-12-11 23:08:23.297776: val_loss -0.5184
2024-12-11 23:08:23.298465: Pseudo dice [0.7162]
2024-12-11 23:08:23.299153: Epoch time: 90.95 s
2024-12-11 23:08:23.299979: Yayy! New best EMA pseudo Dice: 0.6866
2024-12-11 23:08:24.834168: 
2024-12-11 23:08:24.835871: Epoch 28
2024-12-11 23:08:24.836577: Current learning rate: 0.00975
2024-12-11 23:09:55.771605: Validation loss did not improve from -0.53201. Patience: 4/50
2024-12-11 23:09:55.772624: train_loss -0.5665
2024-12-11 23:09:55.773379: val_loss -0.5054
2024-12-11 23:09:55.774038: Pseudo dice [0.7118]
2024-12-11 23:09:55.774790: Epoch time: 90.94 s
2024-12-11 23:09:55.775575: Yayy! New best EMA pseudo Dice: 0.6891
2024-12-11 23:09:57.304067: 
2024-12-11 23:09:57.306366: Epoch 29
2024-12-11 23:09:57.307464: Current learning rate: 0.00974
2024-12-11 23:11:28.522746: Validation loss did not improve from -0.53201. Patience: 5/50
2024-12-11 23:11:28.523917: train_loss -0.5589
2024-12-11 23:11:28.524821: val_loss -0.4914
2024-12-11 23:11:28.525627: Pseudo dice [0.7079]
2024-12-11 23:11:28.526423: Epoch time: 91.22 s
2024-12-11 23:11:28.876913: Yayy! New best EMA pseudo Dice: 0.691
2024-12-11 23:11:30.449113: 
2024-12-11 23:11:30.450575: Epoch 30
2024-12-11 23:11:30.451312: Current learning rate: 0.00973
2024-12-11 23:13:01.376263: Validation loss did not improve from -0.53201. Patience: 6/50
2024-12-11 23:13:01.377461: train_loss -0.5615
2024-12-11 23:13:01.378534: val_loss -0.5248
2024-12-11 23:13:01.379273: Pseudo dice [0.7233]
2024-12-11 23:13:01.380050: Epoch time: 90.93 s
2024-12-11 23:13:01.380881: Yayy! New best EMA pseudo Dice: 0.6942
2024-12-11 23:13:03.020440: 
2024-12-11 23:13:03.022172: Epoch 31
2024-12-11 23:13:03.022968: Current learning rate: 0.00972
2024-12-11 23:14:33.981126: Validation loss did not improve from -0.53201. Patience: 7/50
2024-12-11 23:14:33.981951: train_loss -0.5668
2024-12-11 23:14:33.982963: val_loss -0.4999
2024-12-11 23:14:33.983660: Pseudo dice [0.7172]
2024-12-11 23:14:33.984463: Epoch time: 90.96 s
2024-12-11 23:14:33.985189: Yayy! New best EMA pseudo Dice: 0.6965
2024-12-11 23:14:35.529678: 
2024-12-11 23:14:35.531810: Epoch 32
2024-12-11 23:14:35.532675: Current learning rate: 0.00971
2024-12-11 23:16:06.452091: Validation loss improved from -0.53201 to -0.54654! Patience: 7/50
2024-12-11 23:16:06.453207: train_loss -0.5621
2024-12-11 23:16:06.454157: val_loss -0.5465
2024-12-11 23:16:06.454940: Pseudo dice [0.7461]
2024-12-11 23:16:06.455763: Epoch time: 90.92 s
2024-12-11 23:16:06.456477: Yayy! New best EMA pseudo Dice: 0.7015
2024-12-11 23:16:07.999391: 
2024-12-11 23:16:08.000836: Epoch 33
2024-12-11 23:16:08.001565: Current learning rate: 0.0097
2024-12-11 23:17:39.120583: Validation loss improved from -0.54654 to -0.55393! Patience: 0/50
2024-12-11 23:17:39.122019: train_loss -0.5733
2024-12-11 23:17:39.123658: val_loss -0.5539
2024-12-11 23:17:39.124878: Pseudo dice [0.7514]
2024-12-11 23:17:39.125699: Epoch time: 91.12 s
2024-12-11 23:17:39.126475: Yayy! New best EMA pseudo Dice: 0.7065
2024-12-11 23:17:40.684561: 
2024-12-11 23:17:40.685999: Epoch 34
2024-12-11 23:17:40.686844: Current learning rate: 0.00969
2024-12-11 23:19:11.651186: Validation loss did not improve from -0.55393. Patience: 1/50
2024-12-11 23:19:11.652140: train_loss -0.5686
2024-12-11 23:19:11.653075: val_loss -0.5244
2024-12-11 23:19:11.653795: Pseudo dice [0.7265]
2024-12-11 23:19:11.654839: Epoch time: 90.97 s
2024-12-11 23:19:11.994528: Yayy! New best EMA pseudo Dice: 0.7085
2024-12-11 23:19:13.534192: 
2024-12-11 23:19:13.535571: Epoch 35
2024-12-11 23:19:13.536656: Current learning rate: 0.00968
2024-12-11 23:20:44.456850: Validation loss did not improve from -0.55393. Patience: 2/50
2024-12-11 23:20:44.457884: train_loss -0.5804
2024-12-11 23:20:44.458784: val_loss -0.5326
2024-12-11 23:20:44.459575: Pseudo dice [0.7267]
2024-12-11 23:20:44.460307: Epoch time: 90.92 s
2024-12-11 23:20:44.461201: Yayy! New best EMA pseudo Dice: 0.7103
2024-12-11 23:20:45.994892: 
2024-12-11 23:20:45.996306: Epoch 36
2024-12-11 23:20:45.996973: Current learning rate: 0.00968
2024-12-11 23:22:17.111849: Validation loss improved from -0.55393 to -0.56723! Patience: 2/50
2024-12-11 23:22:17.113000: train_loss -0.592
2024-12-11 23:22:17.114077: val_loss -0.5672
2024-12-11 23:22:17.114881: Pseudo dice [0.753]
2024-12-11 23:22:17.115744: Epoch time: 91.12 s
2024-12-11 23:22:17.116442: Yayy! New best EMA pseudo Dice: 0.7146
2024-12-11 23:22:18.693642: 
2024-12-11 23:22:18.695879: Epoch 37
2024-12-11 23:22:18.696991: Current learning rate: 0.00967
2024-12-11 23:23:49.622126: Validation loss did not improve from -0.56723. Patience: 1/50
2024-12-11 23:23:49.623055: train_loss -0.5872
2024-12-11 23:23:49.623918: val_loss -0.5436
2024-12-11 23:23:49.624615: Pseudo dice [0.737]
2024-12-11 23:23:49.625363: Epoch time: 90.93 s
2024-12-11 23:23:49.626117: Yayy! New best EMA pseudo Dice: 0.7168
2024-12-11 23:23:51.178425: 
2024-12-11 23:23:51.180014: Epoch 38
2024-12-11 23:23:51.181226: Current learning rate: 0.00966
2024-12-11 23:25:22.089648: Validation loss did not improve from -0.56723. Patience: 2/50
2024-12-11 23:25:22.090670: train_loss -0.595
2024-12-11 23:25:22.091532: val_loss -0.5343
2024-12-11 23:25:22.092347: Pseudo dice [0.7229]
2024-12-11 23:25:22.093180: Epoch time: 90.91 s
2024-12-11 23:25:22.093991: Yayy! New best EMA pseudo Dice: 0.7174
2024-12-11 23:25:23.656595: 
2024-12-11 23:25:23.657859: Epoch 39
2024-12-11 23:25:23.659194: Current learning rate: 0.00965
2024-12-11 23:26:54.522367: Validation loss did not improve from -0.56723. Patience: 3/50
2024-12-11 23:26:54.524174: train_loss -0.6008
2024-12-11 23:26:54.526110: val_loss -0.5297
2024-12-11 23:26:54.527509: Pseudo dice [0.7277]
2024-12-11 23:26:54.529049: Epoch time: 90.87 s
2024-12-11 23:26:54.881632: Yayy! New best EMA pseudo Dice: 0.7184
2024-12-11 23:26:56.781455: 
2024-12-11 23:26:56.783297: Epoch 40
2024-12-11 23:26:56.784736: Current learning rate: 0.00964
2024-12-11 23:28:27.652602: Validation loss did not improve from -0.56723. Patience: 4/50
2024-12-11 23:28:27.653706: train_loss -0.6066
2024-12-11 23:28:27.654500: val_loss -0.5493
2024-12-11 23:28:27.655343: Pseudo dice [0.7401]
2024-12-11 23:28:27.656086: Epoch time: 90.87 s
2024-12-11 23:28:27.656793: Yayy! New best EMA pseudo Dice: 0.7206
2024-12-11 23:28:29.261831: 
2024-12-11 23:28:29.263533: Epoch 41
2024-12-11 23:28:29.264400: Current learning rate: 0.00963
2024-12-11 23:30:00.251827: Validation loss did not improve from -0.56723. Patience: 5/50
2024-12-11 23:30:00.266496: train_loss -0.5946
2024-12-11 23:30:00.267784: val_loss -0.5345
2024-12-11 23:30:00.268549: Pseudo dice [0.7315]
2024-12-11 23:30:00.269584: Epoch time: 91.0 s
2024-12-11 23:30:00.270408: Yayy! New best EMA pseudo Dice: 0.7217
2024-12-11 23:30:02.110151: 
2024-12-11 23:30:02.122086: Epoch 42
2024-12-11 23:30:02.123132: Current learning rate: 0.00962
2024-12-11 23:31:33.087186: Validation loss did not improve from -0.56723. Patience: 6/50
2024-12-11 23:31:33.088263: train_loss -0.6082
2024-12-11 23:31:33.089206: val_loss -0.5587
2024-12-11 23:31:33.089978: Pseudo dice [0.7386]
2024-12-11 23:31:33.090768: Epoch time: 90.98 s
2024-12-11 23:31:33.091409: Yayy! New best EMA pseudo Dice: 0.7234
2024-12-11 23:31:34.628158: 
2024-12-11 23:31:34.629750: Epoch 43
2024-12-11 23:31:34.630620: Current learning rate: 0.00961
2024-12-11 23:33:05.487052: Validation loss did not improve from -0.56723. Patience: 7/50
2024-12-11 23:33:05.488204: train_loss -0.6176
2024-12-11 23:33:05.489289: val_loss -0.535
2024-12-11 23:33:05.490138: Pseudo dice [0.7277]
2024-12-11 23:33:05.491262: Epoch time: 90.86 s
2024-12-11 23:33:05.492110: Yayy! New best EMA pseudo Dice: 0.7238
2024-12-11 23:33:07.026302: 
2024-12-11 23:33:07.028365: Epoch 44
2024-12-11 23:33:07.029202: Current learning rate: 0.0096
2024-12-11 23:34:37.961208: Validation loss did not improve from -0.56723. Patience: 8/50
2024-12-11 23:34:37.962357: train_loss -0.615
2024-12-11 23:34:37.963405: val_loss -0.5659
2024-12-11 23:34:37.964351: Pseudo dice [0.7431]
2024-12-11 23:34:37.965235: Epoch time: 90.94 s
2024-12-11 23:34:38.306629: Yayy! New best EMA pseudo Dice: 0.7257
2024-12-11 23:34:39.793564: 
2024-12-11 23:34:39.795506: Epoch 45
2024-12-11 23:34:39.796362: Current learning rate: 0.00959
2024-12-11 23:36:10.704700: Validation loss did not improve from -0.56723. Patience: 9/50
2024-12-11 23:36:10.705971: train_loss -0.6089
2024-12-11 23:36:10.707279: val_loss -0.5521
2024-12-11 23:36:10.708297: Pseudo dice [0.7389]
2024-12-11 23:36:10.709267: Epoch time: 90.91 s
2024-12-11 23:36:10.710023: Yayy! New best EMA pseudo Dice: 0.7271
2024-12-11 23:36:12.268399: 
2024-12-11 23:36:12.270054: Epoch 46
2024-12-11 23:36:12.271131: Current learning rate: 0.00959
2024-12-11 23:37:43.332487: Validation loss did not improve from -0.56723. Patience: 10/50
2024-12-11 23:37:43.333550: train_loss -0.5993
2024-12-11 23:37:43.334620: val_loss -0.5472
2024-12-11 23:37:43.335563: Pseudo dice [0.7363]
2024-12-11 23:37:43.336572: Epoch time: 91.07 s
2024-12-11 23:37:43.337505: Yayy! New best EMA pseudo Dice: 0.728
2024-12-11 23:37:44.896163: 
2024-12-11 23:37:44.897730: Epoch 47
2024-12-11 23:37:44.898576: Current learning rate: 0.00958
2024-12-11 23:39:16.059033: Validation loss did not improve from -0.56723. Patience: 11/50
2024-12-11 23:39:16.060069: train_loss -0.6219
2024-12-11 23:39:16.061472: val_loss -0.5509
2024-12-11 23:39:16.062363: Pseudo dice [0.7484]
2024-12-11 23:39:16.063167: Epoch time: 91.17 s
2024-12-11 23:39:16.064371: Yayy! New best EMA pseudo Dice: 0.73
2024-12-11 23:39:17.579231: 
2024-12-11 23:39:17.581071: Epoch 48
2024-12-11 23:39:17.582148: Current learning rate: 0.00957
2024-12-11 23:40:48.751570: Validation loss improved from -0.56723 to -0.58084! Patience: 11/50
2024-12-11 23:40:48.752757: train_loss -0.6133
2024-12-11 23:40:48.753646: val_loss -0.5808
2024-12-11 23:40:48.754376: Pseudo dice [0.7513]
2024-12-11 23:40:48.755112: Epoch time: 91.17 s
2024-12-11 23:40:48.755868: Yayy! New best EMA pseudo Dice: 0.7322
2024-12-11 23:40:50.294497: 
2024-12-11 23:40:50.296330: Epoch 49
2024-12-11 23:40:50.297089: Current learning rate: 0.00956
2024-12-11 23:42:21.465392: Validation loss did not improve from -0.58084. Patience: 1/50
2024-12-11 23:42:21.466532: train_loss -0.6184
2024-12-11 23:42:21.467559: val_loss -0.5341
2024-12-11 23:42:21.468359: Pseudo dice [0.725]
2024-12-11 23:42:21.469100: Epoch time: 91.17 s
2024-12-11 23:42:23.271773: 
2024-12-11 23:42:23.273643: Epoch 50
2024-12-11 23:42:23.274732: Current learning rate: 0.00955
2024-12-11 23:43:54.429658: Validation loss did not improve from -0.58084. Patience: 2/50
2024-12-11 23:43:54.430829: train_loss -0.6236
2024-12-11 23:43:54.431725: val_loss -0.4751
2024-12-11 23:43:54.432622: Pseudo dice [0.701]
2024-12-11 23:43:54.433294: Epoch time: 91.16 s
2024-12-11 23:43:55.590775: 
2024-12-11 23:43:55.592494: Epoch 51
2024-12-11 23:43:55.593341: Current learning rate: 0.00954
2024-12-11 23:45:26.780000: Validation loss improved from -0.58084 to -0.59189! Patience: 2/50
2024-12-11 23:45:26.781145: train_loss -0.6246
2024-12-11 23:45:26.781995: val_loss -0.5919
2024-12-11 23:45:26.782749: Pseudo dice [0.7673]
2024-12-11 23:45:26.783588: Epoch time: 91.19 s
2024-12-11 23:45:26.784243: Yayy! New best EMA pseudo Dice: 0.7323
2024-12-11 23:45:28.310264: 
2024-12-11 23:45:28.312577: Epoch 52
2024-12-11 23:45:28.313676: Current learning rate: 0.00953
2024-12-11 23:46:59.168372: Validation loss did not improve from -0.59189. Patience: 1/50
2024-12-11 23:46:59.169360: train_loss -0.6445
2024-12-11 23:46:59.170192: val_loss -0.5532
2024-12-11 23:46:59.171081: Pseudo dice [0.7418]
2024-12-11 23:46:59.171893: Epoch time: 90.86 s
2024-12-11 23:46:59.172713: Yayy! New best EMA pseudo Dice: 0.7332
2024-12-11 23:47:00.719720: 
2024-12-11 23:47:00.721195: Epoch 53
2024-12-11 23:47:00.722362: Current learning rate: 0.00952
2024-12-11 23:48:31.349401: Validation loss did not improve from -0.59189. Patience: 2/50
2024-12-11 23:48:31.350292: train_loss -0.6313
2024-12-11 23:48:31.351202: val_loss -0.5759
2024-12-11 23:48:31.352048: Pseudo dice [0.7536]
2024-12-11 23:48:31.352976: Epoch time: 90.63 s
2024-12-11 23:48:31.353843: Yayy! New best EMA pseudo Dice: 0.7353
2024-12-11 23:48:32.850844: 
2024-12-11 23:48:32.852816: Epoch 54
2024-12-11 23:48:32.854020: Current learning rate: 0.00951
2024-12-11 23:50:03.426416: Validation loss improved from -0.59189 to -0.59204! Patience: 2/50
2024-12-11 23:50:03.427568: train_loss -0.6176
2024-12-11 23:50:03.428759: val_loss -0.592
2024-12-11 23:50:03.429674: Pseudo dice [0.7639]
2024-12-11 23:50:03.430537: Epoch time: 90.58 s
2024-12-11 23:50:03.799344: Yayy! New best EMA pseudo Dice: 0.7381
2024-12-11 23:50:05.315889: 
2024-12-11 23:50:05.317720: Epoch 55
2024-12-11 23:50:05.318635: Current learning rate: 0.0095
2024-12-11 23:51:36.089253: Validation loss did not improve from -0.59204. Patience: 1/50
2024-12-11 23:51:36.090583: train_loss -0.6344
2024-12-11 23:51:36.091496: val_loss -0.5626
2024-12-11 23:51:36.092159: Pseudo dice [0.7428]
2024-12-11 23:51:36.092896: Epoch time: 90.78 s
2024-12-11 23:51:36.093514: Yayy! New best EMA pseudo Dice: 0.7386
2024-12-11 23:51:37.635863: 
2024-12-11 23:51:37.637508: Epoch 56
2024-12-11 23:51:37.638226: Current learning rate: 0.00949
2024-12-11 23:53:08.536228: Validation loss did not improve from -0.59204. Patience: 2/50
2024-12-11 23:53:08.537302: train_loss -0.6343
2024-12-11 23:53:08.538295: val_loss -0.5822
2024-12-11 23:53:08.539038: Pseudo dice [0.7629]
2024-12-11 23:53:08.539815: Epoch time: 90.9 s
2024-12-11 23:53:08.540631: Yayy! New best EMA pseudo Dice: 0.741
2024-12-11 23:53:10.055897: 
2024-12-11 23:53:10.057482: Epoch 57
2024-12-11 23:53:10.058409: Current learning rate: 0.00949
2024-12-11 23:54:40.718067: Validation loss improved from -0.59204 to -0.60443! Patience: 2/50
2024-12-11 23:54:40.718917: train_loss -0.6375
2024-12-11 23:54:40.720019: val_loss -0.6044
2024-12-11 23:54:40.720916: Pseudo dice [0.7677]
2024-12-11 23:54:40.721829: Epoch time: 90.66 s
2024-12-11 23:54:40.722686: Yayy! New best EMA pseudo Dice: 0.7437
2024-12-11 23:54:42.438072: 
2024-12-11 23:54:42.439708: Epoch 58
2024-12-11 23:54:42.440709: Current learning rate: 0.00948
2024-12-11 23:56:12.965959: Validation loss did not improve from -0.60443. Patience: 1/50
2024-12-11 23:56:12.966679: train_loss -0.6405
2024-12-11 23:56:12.967443: val_loss -0.5826
2024-12-11 23:56:12.968190: Pseudo dice [0.7616]
2024-12-11 23:56:12.968916: Epoch time: 90.53 s
2024-12-11 23:56:12.969594: Yayy! New best EMA pseudo Dice: 0.7455
2024-12-11 23:56:14.511323: 
2024-12-11 23:56:14.512918: Epoch 59
2024-12-11 23:56:14.513983: Current learning rate: 0.00947
2024-12-11 23:57:44.884427: Validation loss did not improve from -0.60443. Patience: 2/50
2024-12-11 23:57:44.885305: train_loss -0.6456
2024-12-11 23:57:44.886223: val_loss -0.5678
2024-12-11 23:57:44.886979: Pseudo dice [0.7537]
2024-12-11 23:57:44.887677: Epoch time: 90.37 s
2024-12-11 23:57:45.241868: Yayy! New best EMA pseudo Dice: 0.7463
2024-12-11 23:57:46.775640: 
2024-12-11 23:57:46.777295: Epoch 60
2024-12-11 23:57:46.778135: Current learning rate: 0.00946
2024-12-11 23:59:17.093330: Validation loss did not improve from -0.60443. Patience: 3/50
2024-12-11 23:59:17.094128: train_loss -0.6472
2024-12-11 23:59:17.095166: val_loss -0.5882
2024-12-11 23:59:17.096173: Pseudo dice [0.7636]
2024-12-11 23:59:17.097391: Epoch time: 90.32 s
2024-12-11 23:59:17.098459: Yayy! New best EMA pseudo Dice: 0.748
2024-12-11 23:59:18.649113: 
2024-12-11 23:59:18.650632: Epoch 61
2024-12-11 23:59:18.651639: Current learning rate: 0.00945
2024-12-12 00:00:49.557262: Validation loss did not improve from -0.60443. Patience: 4/50
2024-12-12 00:00:49.558269: train_loss -0.6428
2024-12-12 00:00:49.559154: val_loss -0.5929
2024-12-12 00:00:49.559933: Pseudo dice [0.7659]
2024-12-12 00:00:49.560736: Epoch time: 90.91 s
2024-12-12 00:00:49.561368: Yayy! New best EMA pseudo Dice: 0.7498
2024-12-12 00:00:51.102503: 
2024-12-12 00:00:51.104278: Epoch 62
2024-12-12 00:00:51.105145: Current learning rate: 0.00944
2024-12-12 00:02:21.696155: Validation loss did not improve from -0.60443. Patience: 5/50
2024-12-12 00:02:21.697235: train_loss -0.6416
2024-12-12 00:02:21.698221: val_loss -0.5498
2024-12-12 00:02:21.699009: Pseudo dice [0.7395]
2024-12-12 00:02:21.699803: Epoch time: 90.6 s
2024-12-12 00:02:22.918311: 
2024-12-12 00:02:22.920398: Epoch 63
2024-12-12 00:02:22.921710: Current learning rate: 0.00943
2024-12-12 00:03:53.167757: Validation loss did not improve from -0.60443. Patience: 6/50
2024-12-12 00:03:53.169170: train_loss -0.6582
2024-12-12 00:03:53.170261: val_loss -0.5641
2024-12-12 00:03:53.171022: Pseudo dice [0.746]
2024-12-12 00:03:53.172090: Epoch time: 90.25 s
2024-12-12 00:03:54.390209: 
2024-12-12 00:03:54.391892: Epoch 64
2024-12-12 00:03:54.393106: Current learning rate: 0.00942
2024-12-12 00:05:24.668616: Validation loss did not improve from -0.60443. Patience: 7/50
2024-12-12 00:05:24.669580: train_loss -0.6592
2024-12-12 00:05:24.670625: val_loss -0.5586
2024-12-12 00:05:24.671540: Pseudo dice [0.7392]
2024-12-12 00:05:24.672287: Epoch time: 90.28 s
2024-12-12 00:05:26.194929: 
2024-12-12 00:05:26.196686: Epoch 65
2024-12-12 00:05:26.197562: Current learning rate: 0.00941
2024-12-12 00:06:56.534037: Validation loss improved from -0.60443 to -0.60558! Patience: 7/50
2024-12-12 00:06:56.535128: train_loss -0.6474
2024-12-12 00:06:56.535994: val_loss -0.6056
2024-12-12 00:06:56.536779: Pseudo dice [0.7676]
2024-12-12 00:06:56.537463: Epoch time: 90.34 s
2024-12-12 00:06:57.749078: 
2024-12-12 00:06:57.751056: Epoch 66
2024-12-12 00:06:57.751977: Current learning rate: 0.0094
2024-12-12 00:08:28.074764: Validation loss did not improve from -0.60558. Patience: 1/50
2024-12-12 00:08:28.076320: train_loss -0.6561
2024-12-12 00:08:28.077386: val_loss -0.5468
2024-12-12 00:08:28.078247: Pseudo dice [0.7378]
2024-12-12 00:08:28.079040: Epoch time: 90.33 s
2024-12-12 00:08:29.332983: 
2024-12-12 00:08:29.335166: Epoch 67
2024-12-12 00:08:29.336589: Current learning rate: 0.00939
2024-12-12 00:09:59.596721: Validation loss did not improve from -0.60558. Patience: 2/50
2024-12-12 00:09:59.597962: train_loss -0.6562
2024-12-12 00:09:59.599032: val_loss -0.5781
2024-12-12 00:09:59.599917: Pseudo dice [0.7533]
2024-12-12 00:09:59.600850: Epoch time: 90.27 s
2024-12-12 00:10:00.891684: 
2024-12-12 00:10:00.892964: Epoch 68
2024-12-12 00:10:00.893871: Current learning rate: 0.00939
2024-12-12 00:11:31.221400: Validation loss did not improve from -0.60558. Patience: 3/50
2024-12-12 00:11:31.222576: train_loss -0.6618
2024-12-12 00:11:31.224023: val_loss -0.557
2024-12-12 00:11:31.225100: Pseudo dice [0.743]
2024-12-12 00:11:31.226163: Epoch time: 90.33 s
2024-12-12 00:11:32.553018: 
2024-12-12 00:11:32.555046: Epoch 69
2024-12-12 00:11:32.556251: Current learning rate: 0.00938
2024-12-12 00:13:02.893813: Validation loss did not improve from -0.60558. Patience: 4/50
2024-12-12 00:13:02.894773: train_loss -0.653
2024-12-12 00:13:02.896116: val_loss -0.5585
2024-12-12 00:13:02.896941: Pseudo dice [0.7385]
2024-12-12 00:13:02.897723: Epoch time: 90.34 s
2024-12-12 00:13:04.476889: 
2024-12-12 00:13:04.478801: Epoch 70
2024-12-12 00:13:04.479929: Current learning rate: 0.00937
2024-12-12 00:14:34.761964: Validation loss did not improve from -0.60558. Patience: 5/50
2024-12-12 00:14:34.763283: train_loss -0.6564
2024-12-12 00:14:34.764485: val_loss -0.5749
2024-12-12 00:14:34.765134: Pseudo dice [0.7578]
2024-12-12 00:14:34.765856: Epoch time: 90.29 s
2024-12-12 00:14:36.017925: 
2024-12-12 00:14:36.019478: Epoch 71
2024-12-12 00:14:36.020261: Current learning rate: 0.00936
2024-12-12 00:16:06.577772: Validation loss did not improve from -0.60558. Patience: 6/50
2024-12-12 00:16:06.579040: train_loss -0.663
2024-12-12 00:16:06.580326: val_loss -0.5919
2024-12-12 00:16:06.581553: Pseudo dice [0.7665]
2024-12-12 00:16:06.582710: Epoch time: 90.56 s
2024-12-12 00:16:06.583850: Yayy! New best EMA pseudo Dice: 0.7502
2024-12-12 00:16:08.526037: 
2024-12-12 00:16:08.527695: Epoch 72
2024-12-12 00:16:08.528749: Current learning rate: 0.00935
2024-12-12 00:17:39.237240: Validation loss did not improve from -0.60558. Patience: 7/50
2024-12-12 00:17:39.238427: train_loss -0.6636
2024-12-12 00:17:39.239364: val_loss -0.5799
2024-12-12 00:17:39.240082: Pseudo dice [0.752]
2024-12-12 00:17:39.240741: Epoch time: 90.71 s
2024-12-12 00:17:39.241400: Yayy! New best EMA pseudo Dice: 0.7504
2024-12-12 00:17:40.798794: 
2024-12-12 00:17:40.800221: Epoch 73
2024-12-12 00:17:40.800962: Current learning rate: 0.00934
2024-12-12 00:19:11.381207: Validation loss did not improve from -0.60558. Patience: 8/50
2024-12-12 00:19:11.382649: train_loss -0.6656
2024-12-12 00:19:11.383704: val_loss -0.5735
2024-12-12 00:19:11.384661: Pseudo dice [0.7601]
2024-12-12 00:19:11.385668: Epoch time: 90.58 s
2024-12-12 00:19:11.386656: Yayy! New best EMA pseudo Dice: 0.7513
2024-12-12 00:19:12.973385: 
2024-12-12 00:19:12.975376: Epoch 74
2024-12-12 00:19:12.976624: Current learning rate: 0.00933
2024-12-12 00:20:43.679557: Validation loss did not improve from -0.60558. Patience: 9/50
2024-12-12 00:20:43.680800: train_loss -0.671
2024-12-12 00:20:43.681800: val_loss -0.5913
2024-12-12 00:20:43.682525: Pseudo dice [0.7676]
2024-12-12 00:20:43.683315: Epoch time: 90.71 s
2024-12-12 00:20:44.050898: Yayy! New best EMA pseudo Dice: 0.753
2024-12-12 00:20:45.624942: 
2024-12-12 00:20:45.626659: Epoch 75
2024-12-12 00:20:45.627401: Current learning rate: 0.00932
2024-12-12 00:22:16.405015: Validation loss did not improve from -0.60558. Patience: 10/50
2024-12-12 00:22:16.406313: train_loss -0.6684
2024-12-12 00:22:16.407768: val_loss -0.573
2024-12-12 00:22:16.408692: Pseudo dice [0.7526]
2024-12-12 00:22:16.409653: Epoch time: 90.78 s
2024-12-12 00:22:17.652846: 
2024-12-12 00:22:17.654655: Epoch 76
2024-12-12 00:22:17.655753: Current learning rate: 0.00931
2024-12-12 00:23:48.231810: Validation loss did not improve from -0.60558. Patience: 11/50
2024-12-12 00:23:48.233186: train_loss -0.6832
2024-12-12 00:23:48.234301: val_loss -0.5621
2024-12-12 00:23:48.235078: Pseudo dice [0.7476]
2024-12-12 00:23:48.235777: Epoch time: 90.58 s
2024-12-12 00:23:49.473464: 
2024-12-12 00:23:49.475807: Epoch 77
2024-12-12 00:23:49.476752: Current learning rate: 0.0093
2024-12-12 00:25:20.003283: Validation loss did not improve from -0.60558. Patience: 12/50
2024-12-12 00:25:20.004062: train_loss -0.6715
2024-12-12 00:25:20.004823: val_loss -0.5714
2024-12-12 00:25:20.005625: Pseudo dice [0.7565]
2024-12-12 00:25:20.006439: Epoch time: 90.53 s
2024-12-12 00:25:21.277421: 
2024-12-12 00:25:21.279066: Epoch 78
2024-12-12 00:25:21.279923: Current learning rate: 0.0093
2024-12-12 00:26:51.904337: Validation loss did not improve from -0.60558. Patience: 13/50
2024-12-12 00:26:51.905159: train_loss -0.6747
2024-12-12 00:26:51.906054: val_loss -0.5733
2024-12-12 00:26:51.906750: Pseudo dice [0.7576]
2024-12-12 00:26:51.907513: Epoch time: 90.63 s
2024-12-12 00:26:51.908236: Yayy! New best EMA pseudo Dice: 0.7533
2024-12-12 00:26:53.533809: 
2024-12-12 00:26:53.536052: Epoch 79
2024-12-12 00:26:53.536912: Current learning rate: 0.00929
2024-12-12 00:28:25.842600: Validation loss did not improve from -0.60558. Patience: 14/50
2024-12-12 00:28:25.843922: train_loss -0.6696
2024-12-12 00:28:25.844919: val_loss -0.586
2024-12-12 00:28:25.845697: Pseudo dice [0.7619]
2024-12-12 00:28:25.846329: Epoch time: 92.31 s
2024-12-12 00:28:26.510542: Yayy! New best EMA pseudo Dice: 0.7541
2024-12-12 00:28:28.101650: 
2024-12-12 00:28:28.103276: Epoch 80
2024-12-12 00:28:28.104625: Current learning rate: 0.00928
2024-12-12 00:29:58.473983: Validation loss did not improve from -0.60558. Patience: 15/50
2024-12-12 00:29:58.475172: train_loss -0.685
2024-12-12 00:29:58.476079: val_loss -0.6015
2024-12-12 00:29:58.476736: Pseudo dice [0.7707]
2024-12-12 00:29:58.477386: Epoch time: 90.37 s
2024-12-12 00:29:58.478035: Yayy! New best EMA pseudo Dice: 0.7558
2024-12-12 00:30:00.062691: 
2024-12-12 00:30:00.064263: Epoch 81
2024-12-12 00:30:00.064961: Current learning rate: 0.00927
2024-12-12 00:31:30.561391: Validation loss did not improve from -0.60558. Patience: 16/50
2024-12-12 00:31:30.562084: train_loss -0.6818
2024-12-12 00:31:30.562929: val_loss -0.5852
2024-12-12 00:31:30.563630: Pseudo dice [0.7656]
2024-12-12 00:31:30.564426: Epoch time: 90.5 s
2024-12-12 00:31:30.565196: Yayy! New best EMA pseudo Dice: 0.7568
2024-12-12 00:31:32.467446: 
2024-12-12 00:31:32.469248: Epoch 82
2024-12-12 00:31:32.470475: Current learning rate: 0.00926
2024-12-12 00:33:02.933995: Validation loss did not improve from -0.60558. Patience: 17/50
2024-12-12 00:33:02.935107: train_loss -0.678
2024-12-12 00:33:02.936532: val_loss -0.5598
2024-12-12 00:33:02.937488: Pseudo dice [0.755]
2024-12-12 00:33:02.938303: Epoch time: 90.47 s
2024-12-12 00:33:04.103910: 
2024-12-12 00:33:04.105879: Epoch 83
2024-12-12 00:33:04.107004: Current learning rate: 0.00925
2024-12-12 00:34:35.151016: Validation loss did not improve from -0.60558. Patience: 18/50
2024-12-12 00:34:35.153225: train_loss -0.682
2024-12-12 00:34:35.155662: val_loss -0.5765
2024-12-12 00:34:35.156769: Pseudo dice [0.7612]
2024-12-12 00:34:35.158210: Epoch time: 91.05 s
2024-12-12 00:34:35.159079: Yayy! New best EMA pseudo Dice: 0.7571
2024-12-12 00:34:36.740141: 
2024-12-12 00:34:36.742106: Epoch 84
2024-12-12 00:34:36.743362: Current learning rate: 0.00924
2024-12-12 00:36:07.489240: Validation loss did not improve from -0.60558. Patience: 19/50
2024-12-12 00:36:07.491441: train_loss -0.6784
2024-12-12 00:36:07.492358: val_loss -0.569
2024-12-12 00:36:07.493089: Pseudo dice [0.7511]
2024-12-12 00:36:07.494450: Epoch time: 90.75 s
2024-12-12 00:36:09.033988: 
2024-12-12 00:36:09.035631: Epoch 85
2024-12-12 00:36:09.036667: Current learning rate: 0.00923
2024-12-12 00:37:39.755092: Validation loss did not improve from -0.60558. Patience: 20/50
2024-12-12 00:37:39.756502: train_loss -0.6827
2024-12-12 00:37:39.757718: val_loss -0.5845
2024-12-12 00:37:39.758894: Pseudo dice [0.7671]
2024-12-12 00:37:39.759702: Epoch time: 90.72 s
2024-12-12 00:37:39.760449: Yayy! New best EMA pseudo Dice: 0.7575
2024-12-12 00:37:41.396569: 
2024-12-12 00:37:41.398676: Epoch 86
2024-12-12 00:37:41.399506: Current learning rate: 0.00922
2024-12-12 00:39:12.180457: Validation loss did not improve from -0.60558. Patience: 21/50
2024-12-12 00:39:12.181616: train_loss -0.6758
2024-12-12 00:39:12.182636: val_loss -0.5649
2024-12-12 00:39:12.183392: Pseudo dice [0.7474]
2024-12-12 00:39:12.184162: Epoch time: 90.79 s
2024-12-12 00:39:13.365785: 
2024-12-12 00:39:13.367226: Epoch 87
2024-12-12 00:39:13.367980: Current learning rate: 0.00921
2024-12-12 00:40:44.057628: Validation loss did not improve from -0.60558. Patience: 22/50
2024-12-12 00:40:44.058648: train_loss -0.6814
2024-12-12 00:40:44.059699: val_loss -0.5649
2024-12-12 00:40:44.060617: Pseudo dice [0.7572]
2024-12-12 00:40:44.061361: Epoch time: 90.69 s
2024-12-12 00:40:45.251863: 
2024-12-12 00:40:45.253703: Epoch 88
2024-12-12 00:40:45.254543: Current learning rate: 0.0092
2024-12-12 00:42:16.006403: Validation loss did not improve from -0.60558. Patience: 23/50
2024-12-12 00:42:16.007482: train_loss -0.6836
2024-12-12 00:42:16.008477: val_loss -0.586
2024-12-12 00:42:16.009164: Pseudo dice [0.7643]
2024-12-12 00:42:16.009914: Epoch time: 90.76 s
2024-12-12 00:42:17.175912: 
2024-12-12 00:42:17.177740: Epoch 89
2024-12-12 00:42:17.178808: Current learning rate: 0.0092
2024-12-12 00:43:47.931144: Validation loss did not improve from -0.60558. Patience: 24/50
2024-12-12 00:43:47.932284: train_loss -0.6857
2024-12-12 00:43:47.933585: val_loss -0.5754
2024-12-12 00:43:47.934407: Pseudo dice [0.7501]
2024-12-12 00:43:47.935163: Epoch time: 90.76 s
2024-12-12 00:43:49.459525: 
2024-12-12 00:43:49.461326: Epoch 90
2024-12-12 00:43:49.462554: Current learning rate: 0.00919
2024-12-12 00:45:20.143330: Validation loss did not improve from -0.60558. Patience: 25/50
2024-12-12 00:45:20.144499: train_loss -0.6867
2024-12-12 00:45:20.145431: val_loss -0.5842
2024-12-12 00:45:20.146048: Pseudo dice [0.7659]
2024-12-12 00:45:20.146742: Epoch time: 90.69 s
2024-12-12 00:45:20.147515: Yayy! New best EMA pseudo Dice: 0.7576
2024-12-12 00:45:21.750005: 
2024-12-12 00:45:21.751870: Epoch 91
2024-12-12 00:45:21.752753: Current learning rate: 0.00918
2024-12-12 00:46:52.454185: Validation loss did not improve from -0.60558. Patience: 26/50
2024-12-12 00:46:52.456845: train_loss -0.6935
2024-12-12 00:46:52.457869: val_loss -0.5819
2024-12-12 00:46:52.458729: Pseudo dice [0.7606]
2024-12-12 00:46:52.459506: Epoch time: 90.71 s
2024-12-12 00:46:52.460317: Yayy! New best EMA pseudo Dice: 0.7579
2024-12-12 00:46:54.036281: 
2024-12-12 00:46:54.037799: Epoch 92
2024-12-12 00:46:54.038747: Current learning rate: 0.00917
2024-12-12 00:48:24.821861: Validation loss improved from -0.60558 to -0.61103! Patience: 26/50
2024-12-12 00:48:24.822879: train_loss -0.6964
2024-12-12 00:48:24.824198: val_loss -0.611
2024-12-12 00:48:24.825083: Pseudo dice [0.7736]
2024-12-12 00:48:24.825980: Epoch time: 90.79 s
2024-12-12 00:48:24.826811: Yayy! New best EMA pseudo Dice: 0.7594
2024-12-12 00:48:26.360855: 
2024-12-12 00:48:26.362862: Epoch 93
2024-12-12 00:48:26.364007: Current learning rate: 0.00916
2024-12-12 00:49:57.237505: Validation loss did not improve from -0.61103. Patience: 1/50
2024-12-12 00:49:57.238752: train_loss -0.7012
2024-12-12 00:49:57.239684: val_loss -0.6037
2024-12-12 00:49:57.240397: Pseudo dice [0.7678]
2024-12-12 00:49:57.241046: Epoch time: 90.88 s
2024-12-12 00:49:57.241689: Yayy! New best EMA pseudo Dice: 0.7603
2024-12-12 00:49:59.095375: 
2024-12-12 00:49:59.097436: Epoch 94
2024-12-12 00:49:59.098191: Current learning rate: 0.00915
2024-12-12 00:51:30.010382: Validation loss did not improve from -0.61103. Patience: 2/50
2024-12-12 00:51:30.011490: train_loss -0.6928
2024-12-12 00:51:30.012259: val_loss -0.6107
2024-12-12 00:51:30.012949: Pseudo dice [0.7771]
2024-12-12 00:51:30.013671: Epoch time: 90.92 s
2024-12-12 00:51:30.355434: Yayy! New best EMA pseudo Dice: 0.7619
2024-12-12 00:51:31.876409: 
2024-12-12 00:51:31.878469: Epoch 95
2024-12-12 00:51:31.879256: Current learning rate: 0.00914
2024-12-12 00:53:02.464747: Validation loss did not improve from -0.61103. Patience: 3/50
2024-12-12 00:53:02.465955: train_loss -0.6949
2024-12-12 00:53:02.466958: val_loss -0.5963
2024-12-12 00:53:02.467889: Pseudo dice [0.7731]
2024-12-12 00:53:02.468718: Epoch time: 90.59 s
2024-12-12 00:53:02.469413: Yayy! New best EMA pseudo Dice: 0.7631
2024-12-12 00:53:03.984374: 
2024-12-12 00:53:03.986323: Epoch 96
2024-12-12 00:53:03.987551: Current learning rate: 0.00913
2024-12-12 00:54:34.602726: Validation loss did not improve from -0.61103. Patience: 4/50
2024-12-12 00:54:34.603858: train_loss -0.6904
2024-12-12 00:54:34.604739: val_loss -0.5988
2024-12-12 00:54:34.605658: Pseudo dice [0.7664]
2024-12-12 00:54:34.606487: Epoch time: 90.62 s
2024-12-12 00:54:34.607364: Yayy! New best EMA pseudo Dice: 0.7634
2024-12-12 00:54:36.153745: 
2024-12-12 00:54:36.155628: Epoch 97
2024-12-12 00:54:36.156882: Current learning rate: 0.00912
2024-12-12 00:56:06.565549: Validation loss did not improve from -0.61103. Patience: 5/50
2024-12-12 00:56:06.566303: train_loss -0.6979
2024-12-12 00:56:06.567116: val_loss -0.576
2024-12-12 00:56:06.567808: Pseudo dice [0.7586]
2024-12-12 00:56:06.568537: Epoch time: 90.41 s
2024-12-12 00:56:07.762217: 
2024-12-12 00:56:07.764005: Epoch 98
2024-12-12 00:56:07.765052: Current learning rate: 0.00911
2024-12-12 00:57:38.088376: Validation loss did not improve from -0.61103. Patience: 6/50
2024-12-12 00:57:38.089306: train_loss -0.6977
2024-12-12 00:57:38.090199: val_loss -0.5973
2024-12-12 00:57:38.091004: Pseudo dice [0.7716]
2024-12-12 00:57:38.091972: Epoch time: 90.33 s
2024-12-12 00:57:38.092761: Yayy! New best EMA pseudo Dice: 0.7638
2024-12-12 00:57:39.615167: 
2024-12-12 00:57:39.616902: Epoch 99
2024-12-12 00:57:39.617734: Current learning rate: 0.0091
2024-12-12 00:59:09.971069: Validation loss did not improve from -0.61103. Patience: 7/50
2024-12-12 00:59:09.972249: train_loss -0.6978
2024-12-12 00:59:09.973371: val_loss -0.5878
2024-12-12 00:59:09.974200: Pseudo dice [0.7617]
2024-12-12 00:59:09.974911: Epoch time: 90.36 s
2024-12-12 00:59:11.518008: 
2024-12-12 00:59:11.520128: Epoch 100
2024-12-12 00:59:11.520953: Current learning rate: 0.0091
2024-12-12 01:00:41.833315: Validation loss did not improve from -0.61103. Patience: 8/50
2024-12-12 01:00:41.834446: train_loss -0.7019
2024-12-12 01:00:41.835683: val_loss -0.5993
2024-12-12 01:00:41.837034: Pseudo dice [0.7656]
2024-12-12 01:00:41.838262: Epoch time: 90.32 s
2024-12-12 01:00:43.037265: 
2024-12-12 01:00:43.039382: Epoch 101
2024-12-12 01:00:43.040636: Current learning rate: 0.00909
2024-12-12 01:02:13.327487: Validation loss did not improve from -0.61103. Patience: 9/50
2024-12-12 01:02:13.328723: train_loss -0.7012
2024-12-12 01:02:13.329695: val_loss -0.5954
2024-12-12 01:02:13.330462: Pseudo dice [0.765]
2024-12-12 01:02:13.331198: Epoch time: 90.29 s
2024-12-12 01:02:13.331946: Yayy! New best EMA pseudo Dice: 0.7639
2024-12-12 01:02:14.887122: 
2024-12-12 01:02:14.889037: Epoch 102
2024-12-12 01:02:14.889761: Current learning rate: 0.00908
2024-12-12 01:03:45.163077: Validation loss did not improve from -0.61103. Patience: 10/50
2024-12-12 01:03:45.164302: train_loss -0.7098
2024-12-12 01:03:45.165322: val_loss -0.6016
2024-12-12 01:03:45.165987: Pseudo dice [0.7774]
2024-12-12 01:03:45.166793: Epoch time: 90.28 s
2024-12-12 01:03:45.167985: Yayy! New best EMA pseudo Dice: 0.7652
2024-12-12 01:03:46.744569: 
2024-12-12 01:03:46.746232: Epoch 103
2024-12-12 01:03:46.747472: Current learning rate: 0.00907
2024-12-12 01:05:17.001641: Validation loss did not improve from -0.61103. Patience: 11/50
2024-12-12 01:05:17.002692: train_loss -0.7087
2024-12-12 01:05:17.003552: val_loss -0.5717
2024-12-12 01:05:17.004219: Pseudo dice [0.7613]
2024-12-12 01:05:17.004898: Epoch time: 90.26 s
2024-12-12 01:05:18.499960: 
2024-12-12 01:05:18.501625: Epoch 104
2024-12-12 01:05:18.502441: Current learning rate: 0.00906
2024-12-12 01:06:48.735051: Validation loss did not improve from -0.61103. Patience: 12/50
2024-12-12 01:06:48.736300: train_loss -0.707
2024-12-12 01:06:48.737245: val_loss -0.5801
2024-12-12 01:06:48.737902: Pseudo dice [0.7627]
2024-12-12 01:06:48.738659: Epoch time: 90.24 s
2024-12-12 01:06:50.282095: 
2024-12-12 01:06:50.283172: Epoch 105
2024-12-12 01:06:50.284102: Current learning rate: 0.00905
2024-12-12 01:08:20.727766: Validation loss did not improve from -0.61103. Patience: 13/50
2024-12-12 01:08:20.728729: train_loss -0.7036
2024-12-12 01:08:20.729628: val_loss -0.5686
2024-12-12 01:08:20.730475: Pseudo dice [0.7475]
2024-12-12 01:08:20.731337: Epoch time: 90.45 s
2024-12-12 01:08:21.920956: 
2024-12-12 01:08:21.922730: Epoch 106
2024-12-12 01:08:21.923885: Current learning rate: 0.00904
2024-12-12 01:09:52.504762: Validation loss improved from -0.61103 to -0.61687! Patience: 13/50
2024-12-12 01:09:52.505879: train_loss -0.7034
2024-12-12 01:09:52.506804: val_loss -0.6169
2024-12-12 01:09:52.507534: Pseudo dice [0.784]
2024-12-12 01:09:52.508249: Epoch time: 90.59 s
2024-12-12 01:09:53.698533: 
2024-12-12 01:09:53.700225: Epoch 107
2024-12-12 01:09:53.701166: Current learning rate: 0.00903
2024-12-12 01:11:24.203777: Validation loss did not improve from -0.61687. Patience: 1/50
2024-12-12 01:11:24.204569: train_loss -0.7113
2024-12-12 01:11:24.206034: val_loss -0.6016
2024-12-12 01:11:24.206864: Pseudo dice [0.7731]
2024-12-12 01:11:24.207623: Epoch time: 90.51 s
2024-12-12 01:11:24.208419: Yayy! New best EMA pseudo Dice: 0.7658
2024-12-12 01:11:25.766067: 
2024-12-12 01:11:25.767831: Epoch 108
2024-12-12 01:11:25.769200: Current learning rate: 0.00902
2024-12-12 01:12:56.379520: Validation loss did not improve from -0.61687. Patience: 2/50
2024-12-12 01:12:56.380553: train_loss -0.7106
2024-12-12 01:12:56.381341: val_loss -0.605
2024-12-12 01:12:56.382151: Pseudo dice [0.7735]
2024-12-12 01:12:56.382947: Epoch time: 90.62 s
2024-12-12 01:12:56.383605: Yayy! New best EMA pseudo Dice: 0.7666
2024-12-12 01:12:57.896251: 
2024-12-12 01:12:57.897779: Epoch 109
2024-12-12 01:12:57.898881: Current learning rate: 0.00901
2024-12-12 01:14:28.351612: Validation loss did not improve from -0.61687. Patience: 3/50
2024-12-12 01:14:28.352533: train_loss -0.7084
2024-12-12 01:14:28.353439: val_loss -0.6004
2024-12-12 01:14:28.354114: Pseudo dice [0.7707]
2024-12-12 01:14:28.354761: Epoch time: 90.46 s
2024-12-12 01:14:28.713090: Yayy! New best EMA pseudo Dice: 0.767
2024-12-12 01:14:30.269603: 
2024-12-12 01:14:30.271473: Epoch 110
2024-12-12 01:14:30.272542: Current learning rate: 0.009
2024-12-12 01:16:00.750111: Validation loss did not improve from -0.61687. Patience: 4/50
2024-12-12 01:16:00.751658: train_loss -0.6903
2024-12-12 01:16:00.752927: val_loss -0.5918
2024-12-12 01:16:00.753734: Pseudo dice [0.7633]
2024-12-12 01:16:00.754453: Epoch time: 90.48 s
2024-12-12 01:16:01.964862: 
2024-12-12 01:16:01.966851: Epoch 111
2024-12-12 01:16:01.967832: Current learning rate: 0.009
2024-12-12 01:17:32.583991: Validation loss did not improve from -0.61687. Patience: 5/50
2024-12-12 01:17:32.585041: train_loss -0.7018
2024-12-12 01:17:32.586007: val_loss -0.6035
2024-12-12 01:17:32.586813: Pseudo dice [0.7748]
2024-12-12 01:17:32.587547: Epoch time: 90.62 s
2024-12-12 01:17:32.588255: Yayy! New best EMA pseudo Dice: 0.7675
2024-12-12 01:17:34.112667: 
2024-12-12 01:17:34.114799: Epoch 112
2024-12-12 01:17:34.115693: Current learning rate: 0.00899
2024-12-12 01:19:04.620161: Validation loss did not improve from -0.61687. Patience: 6/50
2024-12-12 01:19:04.621171: train_loss -0.7119
2024-12-12 01:19:04.622434: val_loss -0.5911
2024-12-12 01:19:04.623389: Pseudo dice [0.7677]
2024-12-12 01:19:04.624312: Epoch time: 90.51 s
2024-12-12 01:19:04.625330: Yayy! New best EMA pseudo Dice: 0.7675
2024-12-12 01:19:06.164222: 
2024-12-12 01:19:06.166074: Epoch 113
2024-12-12 01:19:06.166830: Current learning rate: 0.00898
2024-12-12 01:20:36.875600: Validation loss did not improve from -0.61687. Patience: 7/50
2024-12-12 01:20:36.877043: train_loss -0.7173
2024-12-12 01:20:36.878233: val_loss -0.5831
2024-12-12 01:20:36.879242: Pseudo dice [0.7583]
2024-12-12 01:20:36.880234: Epoch time: 90.71 s
2024-12-12 01:20:38.068247: 
2024-12-12 01:20:38.070118: Epoch 114
2024-12-12 01:20:38.071130: Current learning rate: 0.00897
2024-12-12 01:22:09.010598: Validation loss did not improve from -0.61687. Patience: 8/50
2024-12-12 01:22:09.011349: train_loss -0.7049
2024-12-12 01:22:09.012349: val_loss -0.6018
2024-12-12 01:22:09.013214: Pseudo dice [0.7706]
2024-12-12 01:22:09.013976: Epoch time: 90.94 s
2024-12-12 01:22:10.916562: 
2024-12-12 01:22:10.918855: Epoch 115
2024-12-12 01:22:10.919865: Current learning rate: 0.00896
2024-12-12 01:23:41.451381: Validation loss did not improve from -0.61687. Patience: 9/50
2024-12-12 01:23:41.452462: train_loss -0.7032
2024-12-12 01:23:41.453338: val_loss -0.5994
2024-12-12 01:23:41.453970: Pseudo dice [0.7743]
2024-12-12 01:23:41.454643: Epoch time: 90.54 s
2024-12-12 01:23:41.455266: Yayy! New best EMA pseudo Dice: 0.7677
2024-12-12 01:23:42.967949: 
2024-12-12 01:23:42.969858: Epoch 116
2024-12-12 01:23:42.970619: Current learning rate: 0.00895
2024-12-12 01:25:13.540815: Validation loss did not improve from -0.61687. Patience: 10/50
2024-12-12 01:25:13.542194: train_loss -0.7055
2024-12-12 01:25:13.543283: val_loss -0.5829
2024-12-12 01:25:13.544135: Pseudo dice [0.7616]
2024-12-12 01:25:13.544993: Epoch time: 90.58 s
2024-12-12 01:25:14.801831: 
2024-12-12 01:25:14.803277: Epoch 117
2024-12-12 01:25:14.804511: Current learning rate: 0.00894
2024-12-12 01:26:45.332868: Validation loss did not improve from -0.61687. Patience: 11/50
2024-12-12 01:26:45.334328: train_loss -0.7137
2024-12-12 01:26:45.335225: val_loss -0.5812
2024-12-12 01:26:45.336142: Pseudo dice [0.7575]
2024-12-12 01:26:45.336818: Epoch time: 90.53 s
2024-12-12 01:26:46.583114: 
2024-12-12 01:26:46.585135: Epoch 118
2024-12-12 01:26:46.585978: Current learning rate: 0.00893
2024-12-12 01:28:17.170846: Validation loss did not improve from -0.61687. Patience: 12/50
2024-12-12 01:28:17.171843: train_loss -0.7116
2024-12-12 01:28:17.172642: val_loss -0.5921
2024-12-12 01:28:17.173265: Pseudo dice [0.7657]
2024-12-12 01:28:17.174006: Epoch time: 90.59 s
2024-12-12 01:28:18.376097: 
2024-12-12 01:28:18.378059: Epoch 119
2024-12-12 01:28:18.378830: Current learning rate: 0.00892
2024-12-12 01:29:48.959777: Validation loss did not improve from -0.61687. Patience: 13/50
2024-12-12 01:29:48.960801: train_loss -0.7171
2024-12-12 01:29:48.962169: val_loss -0.6085
2024-12-12 01:29:48.963216: Pseudo dice [0.7788]
2024-12-12 01:29:48.964548: Epoch time: 90.59 s
2024-12-12 01:29:50.572153: 
2024-12-12 01:29:50.574017: Epoch 120
2024-12-12 01:29:50.574794: Current learning rate: 0.00891
2024-12-12 01:31:21.033448: Validation loss did not improve from -0.61687. Patience: 14/50
2024-12-12 01:31:21.034184: train_loss -0.722
2024-12-12 01:31:21.034976: val_loss -0.5969
2024-12-12 01:31:21.035648: Pseudo dice [0.7658]
2024-12-12 01:31:21.036332: Epoch time: 90.46 s
2024-12-12 01:31:22.234853: 
2024-12-12 01:31:22.236758: Epoch 121
2024-12-12 01:31:22.237780: Current learning rate: 0.0089
2024-12-12 01:32:52.896623: Validation loss did not improve from -0.61687. Patience: 15/50
2024-12-12 01:32:52.897668: train_loss -0.7224
2024-12-12 01:32:52.898765: val_loss -0.5962
2024-12-12 01:32:52.899612: Pseudo dice [0.7702]
2024-12-12 01:32:52.900403: Epoch time: 90.66 s
2024-12-12 01:32:54.138016: 
2024-12-12 01:32:54.140059: Epoch 122
2024-12-12 01:32:54.140835: Current learning rate: 0.00889
2024-12-12 01:34:24.732882: Validation loss did not improve from -0.61687. Patience: 16/50
2024-12-12 01:34:24.734144: train_loss -0.7124
2024-12-12 01:34:24.735160: val_loss -0.6003
2024-12-12 01:34:24.735911: Pseudo dice [0.7729]
2024-12-12 01:34:24.736562: Epoch time: 90.6 s
2024-12-12 01:34:24.737280: Yayy! New best EMA pseudo Dice: 0.768
2024-12-12 01:34:26.375051: 
2024-12-12 01:34:26.377319: Epoch 123
2024-12-12 01:34:26.378073: Current learning rate: 0.00889
2024-12-12 01:35:56.974580: Validation loss did not improve from -0.61687. Patience: 17/50
2024-12-12 01:35:56.975375: train_loss -0.7037
2024-12-12 01:35:56.976273: val_loss -0.5798
2024-12-12 01:35:56.977055: Pseudo dice [0.7649]
2024-12-12 01:35:56.977692: Epoch time: 90.6 s
2024-12-12 01:35:58.220717: 
2024-12-12 01:35:58.222861: Epoch 124
2024-12-12 01:35:58.223782: Current learning rate: 0.00888
2024-12-12 01:37:28.852656: Validation loss did not improve from -0.61687. Patience: 18/50
2024-12-12 01:37:28.854027: train_loss -0.7102
2024-12-12 01:37:28.855231: val_loss -0.601
2024-12-12 01:37:28.856169: Pseudo dice [0.775]
2024-12-12 01:37:28.857012: Epoch time: 90.63 s
2024-12-12 01:37:29.212301: Yayy! New best EMA pseudo Dice: 0.7685
2024-12-12 01:37:30.758110: 
2024-12-12 01:37:30.759920: Epoch 125
2024-12-12 01:37:30.761018: Current learning rate: 0.00887
2024-12-12 01:39:01.364165: Validation loss improved from -0.61687 to -0.61833! Patience: 18/50
2024-12-12 01:39:01.365170: train_loss -0.7181
2024-12-12 01:39:01.366300: val_loss -0.6183
2024-12-12 01:39:01.367289: Pseudo dice [0.7804]
2024-12-12 01:39:01.368197: Epoch time: 90.61 s
2024-12-12 01:39:01.368943: Yayy! New best EMA pseudo Dice: 0.7696
2024-12-12 01:39:03.305869: 
2024-12-12 01:39:03.307579: Epoch 126
2024-12-12 01:39:03.308365: Current learning rate: 0.00886
2024-12-12 01:40:34.132568: Validation loss improved from -0.61833 to -0.62595! Patience: 0/50
2024-12-12 01:40:34.135012: train_loss -0.7105
2024-12-12 01:40:34.136342: val_loss -0.626
2024-12-12 01:40:34.137201: Pseudo dice [0.7872]
2024-12-12 01:40:34.138599: Epoch time: 90.83 s
2024-12-12 01:40:34.139978: Yayy! New best EMA pseudo Dice: 0.7714
2024-12-12 01:40:35.716756: 
2024-12-12 01:40:35.718889: Epoch 127
2024-12-12 01:40:35.719741: Current learning rate: 0.00885
2024-12-12 01:42:06.524486: Validation loss did not improve from -0.62595. Patience: 1/50
2024-12-12 01:42:06.525357: train_loss -0.7193
2024-12-12 01:42:06.526388: val_loss -0.5969
2024-12-12 01:42:06.527235: Pseudo dice [0.7701]
2024-12-12 01:42:06.528211: Epoch time: 90.81 s
2024-12-12 01:42:07.753768: 
2024-12-12 01:42:07.755383: Epoch 128
2024-12-12 01:42:07.756741: Current learning rate: 0.00884
2024-12-12 01:43:38.461377: Validation loss did not improve from -0.62595. Patience: 2/50
2024-12-12 01:43:38.462264: train_loss -0.7241
2024-12-12 01:43:38.463172: val_loss -0.5832
2024-12-12 01:43:38.463830: Pseudo dice [0.7658]
2024-12-12 01:43:38.464856: Epoch time: 90.71 s
2024-12-12 01:43:39.709967: 
2024-12-12 01:43:39.712054: Epoch 129
2024-12-12 01:43:39.713143: Current learning rate: 0.00883
2024-12-12 01:45:10.395835: Validation loss did not improve from -0.62595. Patience: 3/50
2024-12-12 01:45:10.397008: train_loss -0.7233
2024-12-12 01:45:10.397823: val_loss -0.5757
2024-12-12 01:45:10.398606: Pseudo dice [0.7642]
2024-12-12 01:45:10.399341: Epoch time: 90.69 s
2024-12-12 01:45:11.933604: 
2024-12-12 01:45:11.935279: Epoch 130
2024-12-12 01:45:11.936241: Current learning rate: 0.00882
2024-12-12 01:46:42.541503: Validation loss did not improve from -0.62595. Patience: 4/50
2024-12-12 01:46:42.542533: train_loss -0.7323
2024-12-12 01:46:42.543444: val_loss -0.6069
2024-12-12 01:46:42.544173: Pseudo dice [0.7773]
2024-12-12 01:46:42.544925: Epoch time: 90.61 s
2024-12-12 01:46:43.748419: 
2024-12-12 01:46:43.750030: Epoch 131
2024-12-12 01:46:43.751037: Current learning rate: 0.00881
2024-12-12 01:48:14.661156: Validation loss did not improve from -0.62595. Patience: 5/50
2024-12-12 01:48:14.662075: train_loss -0.7314
2024-12-12 01:48:14.662833: val_loss -0.6037
2024-12-12 01:48:14.663460: Pseudo dice [0.7687]
2024-12-12 01:48:14.664069: Epoch time: 90.91 s
2024-12-12 01:48:15.869021: 
2024-12-12 01:48:15.870513: Epoch 132
2024-12-12 01:48:15.871441: Current learning rate: 0.0088
2024-12-12 01:49:46.729688: Validation loss did not improve from -0.62595. Patience: 6/50
2024-12-12 01:49:46.731086: train_loss -0.727
2024-12-12 01:49:46.732275: val_loss -0.6038
2024-12-12 01:49:46.733067: Pseudo dice [0.7757]
2024-12-12 01:49:46.733954: Epoch time: 90.86 s
2024-12-12 01:49:47.986499: 
2024-12-12 01:49:47.987881: Epoch 133
2024-12-12 01:49:47.988609: Current learning rate: 0.00879
2024-12-12 01:51:18.843440: Validation loss did not improve from -0.62595. Patience: 7/50
2024-12-12 01:51:18.846082: train_loss -0.7331
2024-12-12 01:51:18.847116: val_loss -0.6168
2024-12-12 01:51:18.847867: Pseudo dice [0.7825]
2024-12-12 01:51:18.848764: Epoch time: 90.86 s
2024-12-12 01:51:18.849481: Yayy! New best EMA pseudo Dice: 0.7722
2024-12-12 01:51:20.421634: 
2024-12-12 01:51:20.423627: Epoch 134
2024-12-12 01:51:20.424539: Current learning rate: 0.00879
2024-12-12 01:52:51.304425: Validation loss did not improve from -0.62595. Patience: 8/50
2024-12-12 01:52:51.305100: train_loss -0.7343
2024-12-12 01:52:51.305996: val_loss -0.585
2024-12-12 01:52:51.306675: Pseudo dice [0.7638]
2024-12-12 01:52:51.307355: Epoch time: 90.88 s
2024-12-12 01:52:52.868578: 
2024-12-12 01:52:52.869688: Epoch 135
2024-12-12 01:52:52.870556: Current learning rate: 0.00878
2024-12-12 01:54:23.830189: Validation loss did not improve from -0.62595. Patience: 9/50
2024-12-12 01:54:23.831437: train_loss -0.7298
2024-12-12 01:54:23.832303: val_loss -0.5814
2024-12-12 01:54:23.833045: Pseudo dice [0.7659]
2024-12-12 01:54:23.833827: Epoch time: 90.96 s
2024-12-12 01:54:25.068530: 
2024-12-12 01:54:25.070470: Epoch 136
2024-12-12 01:54:25.071476: Current learning rate: 0.00877
2024-12-12 01:55:56.077388: Validation loss did not improve from -0.62595. Patience: 10/50
2024-12-12 01:55:56.078386: train_loss -0.7295
2024-12-12 01:55:56.079363: val_loss -0.6052
2024-12-12 01:55:56.080039: Pseudo dice [0.7771]
2024-12-12 01:55:56.080716: Epoch time: 91.01 s
2024-12-12 01:55:57.804499: 
2024-12-12 01:55:57.806381: Epoch 137
2024-12-12 01:55:57.807231: Current learning rate: 0.00876
2024-12-12 01:57:28.819686: Validation loss did not improve from -0.62595. Patience: 11/50
2024-12-12 01:57:28.820491: train_loss -0.7351
2024-12-12 01:57:28.821732: val_loss -0.5976
2024-12-12 01:57:28.822581: Pseudo dice [0.7686]
2024-12-12 01:57:28.823456: Epoch time: 91.02 s
2024-12-12 01:57:30.073802: 
2024-12-12 01:57:30.075663: Epoch 138
2024-12-12 01:57:30.076570: Current learning rate: 0.00875
2024-12-12 01:59:01.174533: Validation loss did not improve from -0.62595. Patience: 12/50
2024-12-12 01:59:01.175720: train_loss -0.7304
2024-12-12 01:59:01.176640: val_loss -0.61
2024-12-12 01:59:01.177410: Pseudo dice [0.7789]
2024-12-12 01:59:01.178217: Epoch time: 91.1 s
2024-12-12 01:59:02.420138: 
2024-12-12 01:59:02.421905: Epoch 139
2024-12-12 01:59:02.422673: Current learning rate: 0.00874
2024-12-12 02:00:33.626233: Validation loss did not improve from -0.62595. Patience: 13/50
2024-12-12 02:00:33.627176: train_loss -0.7327
2024-12-12 02:00:33.628302: val_loss -0.5905
2024-12-12 02:00:33.629247: Pseudo dice [0.7676]
2024-12-12 02:00:33.630033: Epoch time: 91.21 s
2024-12-12 02:00:35.207819: 
2024-12-12 02:00:35.209548: Epoch 140
2024-12-12 02:00:35.210630: Current learning rate: 0.00873
2024-12-12 02:02:06.478708: Validation loss did not improve from -0.62595. Patience: 14/50
2024-12-12 02:02:06.479671: train_loss -0.7291
2024-12-12 02:02:06.480643: val_loss -0.624
2024-12-12 02:02:06.481431: Pseudo dice [0.7872]
2024-12-12 02:02:06.482157: Epoch time: 91.27 s
2024-12-12 02:02:06.482895: Yayy! New best EMA pseudo Dice: 0.7731
2024-12-12 02:02:08.114309: 
2024-12-12 02:02:08.115841: Epoch 141
2024-12-12 02:02:08.116852: Current learning rate: 0.00872
2024-12-12 02:03:39.281524: Validation loss did not improve from -0.62595. Patience: 15/50
2024-12-12 02:03:39.282276: train_loss -0.7401
2024-12-12 02:03:39.283140: val_loss -0.5967
2024-12-12 02:03:39.283979: Pseudo dice [0.7626]
2024-12-12 02:03:39.284697: Epoch time: 91.17 s
2024-12-12 02:03:40.567462: 
2024-12-12 02:03:40.568985: Epoch 142
2024-12-12 02:03:40.570282: Current learning rate: 0.00871
2024-12-12 02:05:11.785671: Validation loss did not improve from -0.62595. Patience: 16/50
2024-12-12 02:05:11.786630: train_loss -0.7369
2024-12-12 02:05:11.787655: val_loss -0.5942
2024-12-12 02:05:11.788309: Pseudo dice [0.7649]
2024-12-12 02:05:11.789006: Epoch time: 91.22 s
2024-12-12 02:05:13.072636: 
2024-12-12 02:05:13.074171: Epoch 143
2024-12-12 02:05:13.075269: Current learning rate: 0.0087
2024-12-12 02:06:44.226126: Validation loss did not improve from -0.62595. Patience: 17/50
2024-12-12 02:06:44.227095: train_loss -0.7391
2024-12-12 02:06:44.227798: val_loss -0.61
2024-12-12 02:06:44.228580: Pseudo dice [0.7752]
2024-12-12 02:06:44.229350: Epoch time: 91.16 s
2024-12-12 02:06:45.489069: 
2024-12-12 02:06:45.490921: Epoch 144
2024-12-12 02:06:45.491884: Current learning rate: 0.00869
2024-12-12 02:08:16.566170: Validation loss did not improve from -0.62595. Patience: 18/50
2024-12-12 02:08:16.567346: train_loss -0.7438
2024-12-12 02:08:16.568570: val_loss -0.586
2024-12-12 02:08:16.569386: Pseudo dice [0.76]
2024-12-12 02:08:16.570061: Epoch time: 91.08 s
2024-12-12 02:08:18.187595: 
2024-12-12 02:08:18.189457: Epoch 145
2024-12-12 02:08:18.190746: Current learning rate: 0.00868
2024-12-12 02:09:49.138555: Validation loss did not improve from -0.62595. Patience: 19/50
2024-12-12 02:09:49.139561: train_loss -0.7359
2024-12-12 02:09:49.140612: val_loss -0.6169
2024-12-12 02:09:49.141572: Pseudo dice [0.7803]
2024-12-12 02:09:49.142623: Epoch time: 90.95 s
2024-12-12 02:09:50.399241: 
2024-12-12 02:09:50.401570: Epoch 146
2024-12-12 02:09:50.402647: Current learning rate: 0.00868
2024-12-12 02:11:20.730577: Validation loss did not improve from -0.62595. Patience: 20/50
2024-12-12 02:11:20.731649: train_loss -0.7363
2024-12-12 02:11:20.732978: val_loss -0.6103
2024-12-12 02:11:20.733759: Pseudo dice [0.772]
2024-12-12 02:11:20.734517: Epoch time: 90.33 s
2024-12-12 02:11:21.966714: 
2024-12-12 02:11:21.968440: Epoch 147
2024-12-12 02:11:21.969239: Current learning rate: 0.00867
2024-12-12 02:12:51.818986: Validation loss did not improve from -0.62595. Patience: 21/50
2024-12-12 02:12:51.820172: train_loss -0.7174
2024-12-12 02:12:51.821301: val_loss -0.6213
2024-12-12 02:12:51.822175: Pseudo dice [0.7811]
2024-12-12 02:12:51.822876: Epoch time: 89.85 s
2024-12-12 02:12:53.360850: 
2024-12-12 02:12:53.363157: Epoch 148
2024-12-12 02:12:53.364349: Current learning rate: 0.00866
2024-12-12 02:14:22.835371: Validation loss did not improve from -0.62595. Patience: 22/50
2024-12-12 02:14:22.836555: train_loss -0.7265
2024-12-12 02:14:22.837851: val_loss -0.5951
2024-12-12 02:14:22.839112: Pseudo dice [0.7705]
2024-12-12 02:14:22.840072: Epoch time: 89.48 s
2024-12-12 02:14:24.058580: 
2024-12-12 02:14:24.060472: Epoch 149
2024-12-12 02:14:24.061432: Current learning rate: 0.00865
2024-12-12 02:15:53.071658: Validation loss did not improve from -0.62595. Patience: 23/50
2024-12-12 02:15:53.073024: train_loss -0.7367
2024-12-12 02:15:53.074330: val_loss -0.5806
2024-12-12 02:15:53.075293: Pseudo dice [0.7671]
2024-12-12 02:15:53.076254: Epoch time: 89.02 s
2024-12-12 02:15:54.632125: 
2024-12-12 02:15:54.634506: Epoch 150
2024-12-12 02:15:54.635704: Current learning rate: 0.00864
2024-12-12 02:17:23.487578: Validation loss did not improve from -0.62595. Patience: 24/50
2024-12-12 02:17:23.488621: train_loss -0.7393
2024-12-12 02:17:23.489892: val_loss -0.5976
2024-12-12 02:17:23.490782: Pseudo dice [0.7647]
2024-12-12 02:17:23.491757: Epoch time: 88.86 s
2024-12-12 02:17:24.703640: 
2024-12-12 02:17:24.706135: Epoch 151
2024-12-12 02:17:24.707413: Current learning rate: 0.00863
2024-12-12 02:18:53.503442: Validation loss did not improve from -0.62595. Patience: 25/50
2024-12-12 02:18:53.504639: train_loss -0.7413
2024-12-12 02:18:53.505662: val_loss -0.5885
2024-12-12 02:18:53.506519: Pseudo dice [0.7695]
2024-12-12 02:18:53.507499: Epoch time: 88.8 s
2024-12-12 02:18:54.700665: 
2024-12-12 02:18:54.702873: Epoch 152
2024-12-12 02:18:54.703680: Current learning rate: 0.00862
2024-12-12 02:20:23.547408: Validation loss did not improve from -0.62595. Patience: 26/50
2024-12-12 02:20:23.549035: train_loss -0.7364
2024-12-12 02:20:23.550485: val_loss -0.5694
2024-12-12 02:20:23.551625: Pseudo dice [0.7521]
2024-12-12 02:20:23.552920: Epoch time: 88.85 s
2024-12-12 02:20:24.748290: 
2024-12-12 02:20:24.750348: Epoch 153
2024-12-12 02:20:24.751531: Current learning rate: 0.00861
2024-12-12 02:21:53.495099: Validation loss did not improve from -0.62595. Patience: 27/50
2024-12-12 02:21:53.496250: train_loss -0.7393
2024-12-12 02:21:53.497436: val_loss -0.6029
2024-12-12 02:21:53.498213: Pseudo dice [0.7704]
2024-12-12 02:21:53.499212: Epoch time: 88.75 s
2024-12-12 02:21:54.736950: 
2024-12-12 02:21:54.739113: Epoch 154
2024-12-12 02:21:54.740571: Current learning rate: 0.0086
2024-12-12 02:23:23.540931: Validation loss did not improve from -0.62595. Patience: 28/50
2024-12-12 02:23:23.542246: train_loss -0.733
2024-12-12 02:23:23.543315: val_loss -0.5808
2024-12-12 02:23:23.544045: Pseudo dice [0.7584]
2024-12-12 02:23:23.544851: Epoch time: 88.81 s
2024-12-12 02:23:25.081996: 
2024-12-12 02:23:25.083910: Epoch 155
2024-12-12 02:23:25.085033: Current learning rate: 0.00859
2024-12-12 02:24:53.802488: Validation loss did not improve from -0.62595. Patience: 29/50
2024-12-12 02:24:53.803804: train_loss -0.7255
2024-12-12 02:24:53.805060: val_loss -0.5949
2024-12-12 02:24:53.805848: Pseudo dice [0.7728]
2024-12-12 02:24:53.806509: Epoch time: 88.72 s
2024-12-12 02:24:55.032327: 
2024-12-12 02:24:55.034456: Epoch 156
2024-12-12 02:24:55.035674: Current learning rate: 0.00858
2024-12-12 02:26:23.648373: Validation loss did not improve from -0.62595. Patience: 30/50
2024-12-12 02:26:23.649749: train_loss -0.7303
2024-12-12 02:26:23.651174: val_loss -0.6002
2024-12-12 02:26:23.652016: Pseudo dice [0.7674]
2024-12-12 02:26:23.653039: Epoch time: 88.62 s
2024-12-12 02:26:24.876551: 
2024-12-12 02:26:24.878292: Epoch 157
2024-12-12 02:26:24.879598: Current learning rate: 0.00858
2024-12-12 02:27:53.788577: Validation loss did not improve from -0.62595. Patience: 31/50
2024-12-12 02:27:53.789822: train_loss -0.7372
2024-12-12 02:27:53.791061: val_loss -0.611
2024-12-12 02:27:53.791894: Pseudo dice [0.7807]
2024-12-12 02:27:53.793157: Epoch time: 88.91 s
2024-12-12 02:27:55.329757: 
2024-12-12 02:27:55.331553: Epoch 158
2024-12-12 02:27:55.332977: Current learning rate: 0.00857
2024-12-12 02:29:24.165478: Validation loss did not improve from -0.62595. Patience: 32/50
2024-12-12 02:29:24.167108: train_loss -0.7471
2024-12-12 02:29:24.168614: val_loss -0.6009
2024-12-12 02:29:24.169533: Pseudo dice [0.7727]
2024-12-12 02:29:24.170366: Epoch time: 88.84 s
2024-12-12 02:29:25.400518: 
2024-12-12 02:29:25.403118: Epoch 159
2024-12-12 02:29:25.404393: Current learning rate: 0.00856
2024-12-12 02:30:54.194779: Validation loss did not improve from -0.62595. Patience: 33/50
2024-12-12 02:30:54.196157: train_loss -0.7462
2024-12-12 02:30:54.197587: val_loss -0.6177
2024-12-12 02:30:54.198616: Pseudo dice [0.7779]
2024-12-12 02:30:54.199718: Epoch time: 88.8 s
2024-12-12 02:30:55.735127: 
2024-12-12 02:30:55.737061: Epoch 160
2024-12-12 02:30:55.738320: Current learning rate: 0.00855
2024-12-12 02:32:24.548153: Validation loss did not improve from -0.62595. Patience: 34/50
2024-12-12 02:32:24.549381: train_loss -0.7501
2024-12-12 02:32:24.550493: val_loss -0.5941
2024-12-12 02:32:24.551258: Pseudo dice [0.7726]
2024-12-12 02:32:24.552100: Epoch time: 88.82 s
2024-12-12 02:32:25.766103: 
2024-12-12 02:32:25.768380: Epoch 161
2024-12-12 02:32:25.769644: Current learning rate: 0.00854
2024-12-12 02:33:54.626623: Validation loss did not improve from -0.62595. Patience: 35/50
2024-12-12 02:33:54.628093: train_loss -0.7465
2024-12-12 02:33:54.629102: val_loss -0.5929
2024-12-12 02:33:54.630136: Pseudo dice [0.7703]
2024-12-12 02:33:54.630986: Epoch time: 88.86 s
2024-12-12 02:33:55.855387: 
2024-12-12 02:33:55.857446: Epoch 162
2024-12-12 02:33:55.858526: Current learning rate: 0.00853
2024-12-12 02:35:24.736820: Validation loss did not improve from -0.62595. Patience: 36/50
2024-12-12 02:35:24.737856: train_loss -0.7445
2024-12-12 02:35:24.738840: val_loss -0.5843
2024-12-12 02:35:24.739691: Pseudo dice [0.7628]
2024-12-12 02:35:24.740574: Epoch time: 88.88 s
2024-12-12 02:35:25.962984: 
2024-12-12 02:35:25.964863: Epoch 163
2024-12-12 02:35:25.966160: Current learning rate: 0.00852
2024-12-12 02:36:54.823027: Validation loss did not improve from -0.62595. Patience: 37/50
2024-12-12 02:36:54.824383: train_loss -0.7425
2024-12-12 02:36:54.825594: val_loss -0.5783
2024-12-12 02:36:54.826627: Pseudo dice [0.7647]
2024-12-12 02:36:54.827667: Epoch time: 88.86 s
2024-12-12 02:36:56.020728: 
2024-12-12 02:36:56.023011: Epoch 164
2024-12-12 02:36:56.024671: Current learning rate: 0.00851
2024-12-12 02:38:24.954179: Validation loss did not improve from -0.62595. Patience: 38/50
2024-12-12 02:38:24.955238: train_loss -0.7468
2024-12-12 02:38:24.956006: val_loss -0.5902
2024-12-12 02:38:24.956673: Pseudo dice [0.7693]
2024-12-12 02:38:24.957382: Epoch time: 88.94 s
2024-12-12 02:38:26.482518: 
2024-12-12 02:38:26.484811: Epoch 165
2024-12-12 02:38:26.485906: Current learning rate: 0.0085
2024-12-12 02:39:55.609151: Validation loss did not improve from -0.62595. Patience: 39/50
2024-12-12 02:39:55.610285: train_loss -0.7459
2024-12-12 02:39:55.611560: val_loss -0.6212
2024-12-12 02:39:55.612705: Pseudo dice [0.7842]
2024-12-12 02:39:55.613909: Epoch time: 89.13 s
2024-12-12 02:39:56.795940: 
2024-12-12 02:39:56.798393: Epoch 166
2024-12-12 02:39:56.799827: Current learning rate: 0.00849
2024-12-12 02:41:26.015832: Validation loss improved from -0.62595 to -0.63823! Patience: 39/50
2024-12-12 02:41:26.016843: train_loss -0.7498
2024-12-12 02:41:26.018188: val_loss -0.6382
2024-12-12 02:41:26.019207: Pseudo dice [0.7914]
2024-12-12 02:41:26.020195: Epoch time: 89.22 s
2024-12-12 02:41:27.277188: 
2024-12-12 02:41:27.279219: Epoch 167
2024-12-12 02:41:27.280683: Current learning rate: 0.00848
2024-12-12 02:42:56.304701: Validation loss did not improve from -0.63823. Patience: 1/50
2024-12-12 02:42:56.306071: train_loss -0.7458
2024-12-12 02:42:56.307274: val_loss -0.5698
2024-12-12 02:42:56.308262: Pseudo dice [0.756]
2024-12-12 02:42:56.308967: Epoch time: 89.03 s
2024-12-12 02:42:57.826756: 
2024-12-12 02:42:57.829229: Epoch 168
2024-12-12 02:42:57.830559: Current learning rate: 0.00847
2024-12-12 02:44:26.613077: Validation loss did not improve from -0.63823. Patience: 2/50
2024-12-12 02:44:26.614405: train_loss -0.7429
2024-12-12 02:44:26.615357: val_loss -0.602
2024-12-12 02:44:26.616801: Pseudo dice [0.7775]
2024-12-12 02:44:26.617705: Epoch time: 88.79 s
2024-12-12 02:44:27.804415: 
2024-12-12 02:44:27.806413: Epoch 169
2024-12-12 02:44:27.807632: Current learning rate: 0.00847
2024-12-12 02:45:56.697504: Validation loss did not improve from -0.63823. Patience: 3/50
2024-12-12 02:45:56.699239: train_loss -0.7456
2024-12-12 02:45:56.700616: val_loss -0.573
2024-12-12 02:45:56.701608: Pseudo dice [0.7501]
2024-12-12 02:45:56.702375: Epoch time: 88.9 s
2024-12-12 02:45:58.267609: 
2024-12-12 02:45:58.269724: Epoch 170
2024-12-12 02:45:58.270594: Current learning rate: 0.00846
2024-12-12 02:47:27.293504: Validation loss did not improve from -0.63823. Patience: 4/50
2024-12-12 02:47:27.294664: train_loss -0.7329
2024-12-12 02:47:27.296462: val_loss -0.5784
2024-12-12 02:47:27.297819: Pseudo dice [0.7622]
2024-12-12 02:47:27.298800: Epoch time: 89.03 s
2024-12-12 02:47:28.517508: 
2024-12-12 02:47:28.519465: Epoch 171
2024-12-12 02:47:28.520643: Current learning rate: 0.00845
2024-12-12 02:48:57.458366: Validation loss did not improve from -0.63823. Patience: 5/50
2024-12-12 02:48:57.459527: train_loss -0.7435
2024-12-12 02:48:57.460639: val_loss -0.6002
2024-12-12 02:48:57.461840: Pseudo dice [0.7696]
2024-12-12 02:48:57.463126: Epoch time: 88.94 s
2024-12-12 02:48:58.681841: 
2024-12-12 02:48:58.684075: Epoch 172
2024-12-12 02:48:58.685091: Current learning rate: 0.00844
2024-12-12 02:50:27.604865: Validation loss did not improve from -0.63823. Patience: 6/50
2024-12-12 02:50:27.606030: train_loss -0.7452
2024-12-12 02:50:27.607262: val_loss -0.6272
2024-12-12 02:50:27.608219: Pseudo dice [0.7866]
2024-12-12 02:50:27.609129: Epoch time: 88.93 s
2024-12-12 02:50:28.795331: 
2024-12-12 02:50:28.797295: Epoch 173
2024-12-12 02:50:28.798638: Current learning rate: 0.00843
2024-12-12 02:51:57.759135: Validation loss did not improve from -0.63823. Patience: 7/50
2024-12-12 02:51:57.760144: train_loss -0.7368
2024-12-12 02:51:57.761297: val_loss -0.5772
2024-12-12 02:51:57.762520: Pseudo dice [0.7637]
2024-12-12 02:51:57.763649: Epoch time: 88.97 s
2024-12-12 02:51:58.947791: 
2024-12-12 02:51:58.949992: Epoch 174
2024-12-12 02:51:58.950984: Current learning rate: 0.00842
2024-12-12 02:53:27.780149: Validation loss did not improve from -0.63823. Patience: 8/50
2024-12-12 02:53:27.781365: train_loss -0.7399
2024-12-12 02:53:27.782630: val_loss -0.5932
2024-12-12 02:53:27.783491: Pseudo dice [0.7642]
2024-12-12 02:53:27.784453: Epoch time: 88.83 s
2024-12-12 02:53:29.314035: 
2024-12-12 02:53:29.316443: Epoch 175
2024-12-12 02:53:29.317507: Current learning rate: 0.00841
2024-12-12 02:54:58.406195: Validation loss did not improve from -0.63823. Patience: 9/50
2024-12-12 02:54:58.407282: train_loss -0.7486
2024-12-12 02:54:58.408440: val_loss -0.5819
2024-12-12 02:54:58.409382: Pseudo dice [0.7627]
2024-12-12 02:54:58.410446: Epoch time: 89.09 s
2024-12-12 02:54:59.625354: 
2024-12-12 02:54:59.627481: Epoch 176
2024-12-12 02:54:59.628299: Current learning rate: 0.0084
2024-12-12 02:56:28.783908: Validation loss did not improve from -0.63823. Patience: 10/50
2024-12-12 02:56:28.785187: train_loss -0.7544
2024-12-12 02:56:28.786244: val_loss -0.5822
2024-12-12 02:56:28.787270: Pseudo dice [0.7695]
2024-12-12 02:56:28.788714: Epoch time: 89.16 s
2024-12-12 02:56:30.001400: 
2024-12-12 02:56:30.003033: Epoch 177
2024-12-12 02:56:30.004017: Current learning rate: 0.00839
2024-12-12 02:57:59.089886: Validation loss did not improve from -0.63823. Patience: 11/50
2024-12-12 02:57:59.091389: train_loss -0.7549
2024-12-12 02:57:59.092768: val_loss -0.6082
2024-12-12 02:57:59.093847: Pseudo dice [0.7735]
2024-12-12 02:57:59.094939: Epoch time: 89.09 s
2024-12-12 02:58:00.314420: 
2024-12-12 02:58:00.316337: Epoch 178
2024-12-12 02:58:00.317220: Current learning rate: 0.00838
2024-12-12 02:59:29.435881: Validation loss did not improve from -0.63823. Patience: 12/50
2024-12-12 02:59:29.436980: train_loss -0.7276
2024-12-12 02:59:29.438115: val_loss -0.6064
2024-12-12 02:59:29.438945: Pseudo dice [0.7734]
2024-12-12 02:59:29.439819: Epoch time: 89.12 s
2024-12-12 02:59:30.940711: 
2024-12-12 02:59:30.943353: Epoch 179
2024-12-12 02:59:30.945260: Current learning rate: 0.00837
2024-12-12 03:00:59.843512: Validation loss did not improve from -0.63823. Patience: 13/50
2024-12-12 03:00:59.844484: train_loss -0.7356
2024-12-12 03:00:59.845748: val_loss -0.6061
2024-12-12 03:00:59.846694: Pseudo dice [0.7703]
2024-12-12 03:00:59.847784: Epoch time: 88.9 s
2024-12-12 03:01:01.417998: 
2024-12-12 03:01:01.419976: Epoch 180
2024-12-12 03:01:01.421077: Current learning rate: 0.00836
2024-12-12 03:02:29.763010: Validation loss did not improve from -0.63823. Patience: 14/50
2024-12-12 03:02:29.764351: train_loss -0.7378
2024-12-12 03:02:29.765683: val_loss -0.6133
2024-12-12 03:02:29.766593: Pseudo dice [0.7813]
2024-12-12 03:02:29.767522: Epoch time: 88.35 s
2024-12-12 03:02:31.009426: 
2024-12-12 03:02:31.011762: Epoch 181
2024-12-12 03:02:31.012848: Current learning rate: 0.00836
2024-12-12 03:03:59.162968: Validation loss did not improve from -0.63823. Patience: 15/50
2024-12-12 03:03:59.164071: train_loss -0.7457
2024-12-12 03:03:59.165382: val_loss -0.6137
2024-12-12 03:03:59.166385: Pseudo dice [0.7781]
2024-12-12 03:03:59.167475: Epoch time: 88.16 s
2024-12-12 03:04:00.386585: 
2024-12-12 03:04:00.388544: Epoch 182
2024-12-12 03:04:00.389517: Current learning rate: 0.00835
2024-12-12 03:05:28.862378: Validation loss did not improve from -0.63823. Patience: 16/50
2024-12-12 03:05:28.863904: train_loss -0.7499
2024-12-12 03:05:28.865476: val_loss -0.6005
2024-12-12 03:05:28.866717: Pseudo dice [0.7777]
2024-12-12 03:05:28.867938: Epoch time: 88.48 s
2024-12-12 03:05:30.059056: 
2024-12-12 03:05:30.060853: Epoch 183
2024-12-12 03:05:30.061986: Current learning rate: 0.00834
2024-12-12 03:06:58.364699: Validation loss did not improve from -0.63823. Patience: 17/50
2024-12-12 03:06:58.366217: train_loss -0.7529
2024-12-12 03:06:58.367468: val_loss -0.6031
2024-12-12 03:06:58.368498: Pseudo dice [0.7793]
2024-12-12 03:06:58.369490: Epoch time: 88.31 s
2024-12-12 03:06:59.568032: 
2024-12-12 03:06:59.570150: Epoch 184
2024-12-12 03:06:59.571170: Current learning rate: 0.00833
2024-12-12 03:08:28.058528: Validation loss did not improve from -0.63823. Patience: 18/50
2024-12-12 03:08:28.060101: train_loss -0.7528
2024-12-12 03:08:28.061223: val_loss -0.6083
2024-12-12 03:08:28.061881: Pseudo dice [0.7797]
2024-12-12 03:08:28.062771: Epoch time: 88.49 s
2024-12-12 03:08:28.417110: Yayy! New best EMA pseudo Dice: 0.7737
2024-12-12 03:08:29.931711: 
2024-12-12 03:08:29.933777: Epoch 185
2024-12-12 03:08:29.934576: Current learning rate: 0.00832
2024-12-12 03:09:58.310796: Validation loss did not improve from -0.63823. Patience: 19/50
2024-12-12 03:09:58.312003: train_loss -0.7557
2024-12-12 03:09:58.313360: val_loss -0.6003
2024-12-12 03:09:58.314290: Pseudo dice [0.7769]
2024-12-12 03:09:58.315080: Epoch time: 88.38 s
2024-12-12 03:09:58.315816: Yayy! New best EMA pseudo Dice: 0.774
2024-12-12 03:09:59.812949: 
2024-12-12 03:09:59.815655: Epoch 186
2024-12-12 03:09:59.817038: Current learning rate: 0.00831
2024-12-12 03:11:28.114882: Validation loss did not improve from -0.63823. Patience: 20/50
2024-12-12 03:11:28.115942: train_loss -0.7602
2024-12-12 03:11:28.116689: val_loss -0.6107
2024-12-12 03:11:28.117512: Pseudo dice [0.7762]
2024-12-12 03:11:28.118593: Epoch time: 88.3 s
2024-12-12 03:11:28.119412: Yayy! New best EMA pseudo Dice: 0.7742
2024-12-12 03:11:29.677977: 
2024-12-12 03:11:29.680128: Epoch 187
2024-12-12 03:11:29.681434: Current learning rate: 0.0083
2024-12-12 03:12:58.068576: Validation loss did not improve from -0.63823. Patience: 21/50
2024-12-12 03:12:58.069835: train_loss -0.7622
2024-12-12 03:12:58.070685: val_loss -0.6003
2024-12-12 03:12:58.071526: Pseudo dice [0.7773]
2024-12-12 03:12:58.072628: Epoch time: 88.39 s
2024-12-12 03:12:58.073564: Yayy! New best EMA pseudo Dice: 0.7745
2024-12-12 03:12:59.633916: 
2024-12-12 03:12:59.636301: Epoch 188
2024-12-12 03:12:59.637198: Current learning rate: 0.00829
2024-12-12 03:14:28.329986: Validation loss did not improve from -0.63823. Patience: 22/50
2024-12-12 03:14:28.331164: train_loss -0.7577
2024-12-12 03:14:28.332331: val_loss -0.6235
2024-12-12 03:14:28.333673: Pseudo dice [0.783]
2024-12-12 03:14:28.334738: Epoch time: 88.7 s
2024-12-12 03:14:28.335588: Yayy! New best EMA pseudo Dice: 0.7754
2024-12-12 03:14:30.217060: 
2024-12-12 03:14:30.219568: Epoch 189
2024-12-12 03:14:30.220524: Current learning rate: 0.00828
2024-12-12 03:15:58.989042: Validation loss did not improve from -0.63823. Patience: 23/50
2024-12-12 03:15:59.061517: train_loss -0.7588
2024-12-12 03:15:59.064822: val_loss -0.6109
2024-12-12 03:15:59.066922: Pseudo dice [0.7854]
2024-12-12 03:15:59.068640: Epoch time: 88.84 s
2024-12-12 03:15:59.555286: Yayy! New best EMA pseudo Dice: 0.7764
2024-12-12 03:16:01.307125: 
2024-12-12 03:16:01.309763: Epoch 190
2024-12-12 03:16:01.313113: Current learning rate: 0.00827
2024-12-12 03:17:30.153047: Validation loss did not improve from -0.63823. Patience: 24/50
2024-12-12 03:17:30.155295: train_loss -0.7519
2024-12-12 03:17:30.156548: val_loss -0.6218
2024-12-12 03:17:30.157400: Pseudo dice [0.7875]
2024-12-12 03:17:30.158534: Epoch time: 88.85 s
2024-12-12 03:17:30.159532: Yayy! New best EMA pseudo Dice: 0.7775
2024-12-12 03:17:31.706870: 
2024-12-12 03:17:31.708616: Epoch 191
2024-12-12 03:17:31.709606: Current learning rate: 0.00826
2024-12-12 03:19:00.394150: Validation loss did not improve from -0.63823. Patience: 25/50
2024-12-12 03:19:00.395453: train_loss -0.759
2024-12-12 03:19:00.396407: val_loss -0.5939
2024-12-12 03:19:00.397132: Pseudo dice [0.7707]
2024-12-12 03:19:00.397879: Epoch time: 88.69 s
2024-12-12 03:19:01.601091: 
2024-12-12 03:19:01.603028: Epoch 192
2024-12-12 03:19:01.603755: Current learning rate: 0.00825
2024-12-12 03:20:30.253382: Validation loss did not improve from -0.63823. Patience: 26/50
2024-12-12 03:20:30.254495: train_loss -0.7611
2024-12-12 03:20:30.255843: val_loss -0.6138
2024-12-12 03:20:30.256755: Pseudo dice [0.782]
2024-12-12 03:20:30.257605: Epoch time: 88.65 s
2024-12-12 03:20:31.485635: 
2024-12-12 03:20:31.487420: Epoch 193
2024-12-12 03:20:31.488655: Current learning rate: 0.00824
2024-12-12 03:22:00.267783: Validation loss did not improve from -0.63823. Patience: 27/50
2024-12-12 03:22:00.269348: train_loss -0.7351
2024-12-12 03:22:00.271027: val_loss -0.6035
2024-12-12 03:22:00.272011: Pseudo dice [0.7763]
2024-12-12 03:22:00.273118: Epoch time: 88.78 s
2024-12-12 03:22:01.486498: 
2024-12-12 03:22:01.488617: Epoch 194
2024-12-12 03:22:01.489793: Current learning rate: 0.00824
2024-12-12 03:23:30.245794: Validation loss did not improve from -0.63823. Patience: 28/50
2024-12-12 03:23:30.246726: train_loss -0.7401
2024-12-12 03:23:30.247939: val_loss -0.6134
2024-12-12 03:23:30.249223: Pseudo dice [0.7784]
2024-12-12 03:23:30.249988: Epoch time: 88.76 s
2024-12-12 03:23:31.796888: 
2024-12-12 03:23:31.798963: Epoch 195
2024-12-12 03:23:31.800074: Current learning rate: 0.00823
2024-12-12 03:25:00.341415: Validation loss did not improve from -0.63823. Patience: 29/50
2024-12-12 03:25:00.342835: train_loss -0.7497
2024-12-12 03:25:00.344295: val_loss -0.5784
2024-12-12 03:25:00.345315: Pseudo dice [0.7619]
2024-12-12 03:25:00.346084: Epoch time: 88.55 s
2024-12-12 03:25:01.584634: 
2024-12-12 03:25:01.586574: Epoch 196
2024-12-12 03:25:01.587661: Current learning rate: 0.00822
2024-12-12 03:26:30.430325: Validation loss did not improve from -0.63823. Patience: 30/50
2024-12-12 03:26:30.431705: train_loss -0.7475
2024-12-12 03:26:30.432830: val_loss -0.6059
2024-12-12 03:26:30.433903: Pseudo dice [0.7673]
2024-12-12 03:26:30.434852: Epoch time: 88.85 s
2024-12-12 03:26:31.672095: 
2024-12-12 03:26:31.674646: Epoch 197
2024-12-12 03:26:31.675905: Current learning rate: 0.00821
2024-12-12 03:28:00.505823: Validation loss did not improve from -0.63823. Patience: 31/50
2024-12-12 03:28:00.507031: train_loss -0.7565
2024-12-12 03:28:00.508472: val_loss -0.6231
2024-12-12 03:28:00.509476: Pseudo dice [0.7857]
2024-12-12 03:28:00.510603: Epoch time: 88.84 s
2024-12-12 03:28:01.740084: 
2024-12-12 03:28:01.742326: Epoch 198
2024-12-12 03:28:01.743462: Current learning rate: 0.0082
2024-12-12 03:29:30.538447: Validation loss did not improve from -0.63823. Patience: 32/50
2024-12-12 03:29:30.539854: train_loss -0.7603
2024-12-12 03:29:30.541821: val_loss -0.6031
2024-12-12 03:29:30.542868: Pseudo dice [0.7683]
2024-12-12 03:29:30.543769: Epoch time: 88.8 s
2024-12-12 03:29:33.082603: 
2024-12-12 03:29:33.085057: Epoch 199
2024-12-12 03:29:33.086360: Current learning rate: 0.00819
2024-12-12 03:31:01.658429: Validation loss did not improve from -0.63823. Patience: 33/50
2024-12-12 03:31:01.659286: train_loss -0.7599
2024-12-12 03:31:01.660373: val_loss -0.6028
2024-12-12 03:31:01.661431: Pseudo dice [0.7708]
2024-12-12 03:31:01.662533: Epoch time: 88.58 s
2024-12-12 03:31:03.300367: 
2024-12-12 03:31:03.302195: Epoch 200
2024-12-12 03:31:03.303652: Current learning rate: 0.00818
2024-12-12 03:32:31.877899: Validation loss did not improve from -0.63823. Patience: 34/50
2024-12-12 03:32:31.878889: train_loss -0.7574
2024-12-12 03:32:31.879901: val_loss -0.5829
2024-12-12 03:32:31.880654: Pseudo dice [0.7674]
2024-12-12 03:32:31.881456: Epoch time: 88.58 s
2024-12-12 03:32:33.144838: 
2024-12-12 03:32:33.146772: Epoch 201
2024-12-12 03:32:33.147751: Current learning rate: 0.00817
2024-12-12 03:34:01.716786: Validation loss did not improve from -0.63823. Patience: 35/50
2024-12-12 03:34:01.718112: train_loss -0.7643
2024-12-12 03:34:01.719527: val_loss -0.6131
2024-12-12 03:34:01.720517: Pseudo dice [0.7862]
2024-12-12 03:34:01.721431: Epoch time: 88.57 s
2024-12-12 03:34:02.974773: 
2024-12-12 03:34:02.976556: Epoch 202
2024-12-12 03:34:02.977561: Current learning rate: 0.00816
2024-12-12 03:35:31.637017: Validation loss did not improve from -0.63823. Patience: 36/50
2024-12-12 03:35:31.638391: train_loss -0.7584
2024-12-12 03:35:31.639545: val_loss -0.5993
2024-12-12 03:35:31.640467: Pseudo dice [0.7795]
2024-12-12 03:35:31.641484: Epoch time: 88.66 s
2024-12-12 03:35:32.876379: 
2024-12-12 03:35:32.878141: Epoch 203
2024-12-12 03:35:32.879151: Current learning rate: 0.00815
2024-12-12 03:37:01.619895: Validation loss did not improve from -0.63823. Patience: 37/50
2024-12-12 03:37:01.620790: train_loss -0.7586
2024-12-12 03:37:01.621603: val_loss -0.6083
2024-12-12 03:37:01.622622: Pseudo dice [0.7811]
2024-12-12 03:37:01.623746: Epoch time: 88.75 s
2024-12-12 03:37:02.837353: 
2024-12-12 03:37:02.839353: Epoch 204
2024-12-12 03:37:02.840090: Current learning rate: 0.00814
2024-12-12 03:38:31.534147: Validation loss did not improve from -0.63823. Patience: 38/50
2024-12-12 03:38:31.535454: train_loss -0.7619
2024-12-12 03:38:31.537132: val_loss -0.6098
2024-12-12 03:38:31.538221: Pseudo dice [0.781]
2024-12-12 03:38:31.539252: Epoch time: 88.7 s
2024-12-12 03:38:33.166763: 
2024-12-12 03:38:33.168919: Epoch 205
2024-12-12 03:38:33.170161: Current learning rate: 0.00813
2024-12-12 03:40:01.786142: Validation loss did not improve from -0.63823. Patience: 39/50
2024-12-12 03:40:01.787149: train_loss -0.7626
2024-12-12 03:40:01.787974: val_loss -0.5913
2024-12-12 03:40:01.788880: Pseudo dice [0.7674]
2024-12-12 03:40:01.789895: Epoch time: 88.62 s
2024-12-12 03:40:02.956136: 
2024-12-12 03:40:02.958156: Epoch 206
2024-12-12 03:40:02.959485: Current learning rate: 0.00813
2024-12-12 03:41:31.553313: Validation loss did not improve from -0.63823. Patience: 40/50
2024-12-12 03:41:31.554132: train_loss -0.7613
2024-12-12 03:41:31.555234: val_loss -0.605
2024-12-12 03:41:31.556301: Pseudo dice [0.7789]
2024-12-12 03:41:31.557218: Epoch time: 88.6 s
2024-12-12 03:41:32.693241: 
2024-12-12 03:41:32.695043: Epoch 207
2024-12-12 03:41:32.696211: Current learning rate: 0.00812
2024-12-12 03:43:01.400437: Validation loss did not improve from -0.63823. Patience: 41/50
2024-12-12 03:43:01.401820: train_loss -0.7611
2024-12-12 03:43:01.403208: val_loss -0.5843
2024-12-12 03:43:01.404405: Pseudo dice [0.7616]
2024-12-12 03:43:01.405434: Epoch time: 88.71 s
2024-12-12 03:43:02.545434: 
2024-12-12 03:43:02.547275: Epoch 208
2024-12-12 03:43:02.548137: Current learning rate: 0.00811
2024-12-12 03:44:31.502440: Validation loss did not improve from -0.63823. Patience: 42/50
2024-12-12 03:44:31.503839: train_loss -0.7676
2024-12-12 03:44:31.505618: val_loss -0.6073
2024-12-12 03:44:31.506583: Pseudo dice [0.7859]
2024-12-12 03:44:31.507645: Epoch time: 88.96 s
2024-12-12 03:44:32.659236: 
2024-12-12 03:44:32.661605: Epoch 209
2024-12-12 03:44:32.662987: Current learning rate: 0.0081
2024-12-12 03:46:01.433918: Validation loss did not improve from -0.63823. Patience: 43/50
2024-12-12 03:46:01.434988: train_loss -0.7673
2024-12-12 03:46:01.436157: val_loss -0.5892
2024-12-12 03:46:01.436957: Pseudo dice [0.7724]
2024-12-12 03:46:01.437704: Epoch time: 88.78 s
2024-12-12 03:46:03.277306: 
2024-12-12 03:46:03.279661: Epoch 210
2024-12-12 03:46:03.280790: Current learning rate: 0.00809
2024-12-12 03:47:31.952690: Validation loss did not improve from -0.63823. Patience: 44/50
2024-12-12 03:47:31.953913: train_loss -0.7673
2024-12-12 03:47:31.955161: val_loss -0.5855
2024-12-12 03:47:31.956028: Pseudo dice [0.7651]
2024-12-12 03:47:31.956942: Epoch time: 88.68 s
2024-12-12 03:47:33.105452: 
2024-12-12 03:47:33.106756: Epoch 211
2024-12-12 03:47:33.107937: Current learning rate: 0.00808
2024-12-12 03:49:02.021999: Validation loss did not improve from -0.63823. Patience: 45/50
2024-12-12 03:49:02.023423: train_loss -0.7627
2024-12-12 03:49:02.024786: val_loss -0.6123
2024-12-12 03:49:02.025901: Pseudo dice [0.7807]
2024-12-12 03:49:02.027158: Epoch time: 88.92 s
2024-12-12 03:49:03.200178: 
2024-12-12 03:49:03.201893: Epoch 212
2024-12-12 03:49:03.202825: Current learning rate: 0.00807
2024-12-12 03:50:32.217695: Validation loss did not improve from -0.63823. Patience: 46/50
2024-12-12 03:50:32.219162: train_loss -0.7697
2024-12-12 03:50:32.220434: val_loss -0.6158
2024-12-12 03:50:32.221354: Pseudo dice [0.7792]
2024-12-12 03:50:32.222343: Epoch time: 89.02 s
2024-12-12 03:50:33.392897: 
2024-12-12 03:50:33.394966: Epoch 213
2024-12-12 03:50:33.395913: Current learning rate: 0.00806
2024-12-12 03:52:02.408495: Validation loss did not improve from -0.63823. Patience: 47/50
2024-12-12 03:52:02.409843: train_loss -0.77
2024-12-12 03:52:02.411306: val_loss -0.626
2024-12-12 03:52:02.412277: Pseudo dice [0.7833]
2024-12-12 03:52:02.413329: Epoch time: 89.02 s
2024-12-12 03:52:03.598662: 
2024-12-12 03:52:03.600829: Epoch 214
2024-12-12 03:52:03.602199: Current learning rate: 0.00805
2024-12-12 03:53:32.622150: Validation loss did not improve from -0.63823. Patience: 48/50
2024-12-12 03:53:32.623416: train_loss -0.769
2024-12-12 03:53:32.624750: val_loss -0.5843
2024-12-12 03:53:32.625916: Pseudo dice [0.7619]
2024-12-12 03:53:32.626890: Epoch time: 89.03 s
2024-12-12 03:53:34.135602: 
2024-12-12 03:53:34.137648: Epoch 215
2024-12-12 03:53:34.139126: Current learning rate: 0.00804
2024-12-12 03:55:02.946185: Validation loss did not improve from -0.63823. Patience: 49/50
2024-12-12 03:55:02.947319: train_loss -0.7744
2024-12-12 03:55:02.948528: val_loss -0.6013
2024-12-12 03:55:02.949394: Pseudo dice [0.7716]
2024-12-12 03:55:02.950291: Epoch time: 88.81 s
2024-12-12 03:55:04.123227: 
2024-12-12 03:55:04.125381: Epoch 216
2024-12-12 03:55:04.126542: Current learning rate: 0.00803
2024-12-12 03:56:32.957418: Validation loss did not improve from -0.63823. Patience: 50/50
2024-12-12 03:56:32.958452: train_loss -0.7711
2024-12-12 03:56:32.959656: val_loss -0.6028
2024-12-12 03:56:32.960573: Pseudo dice [0.7829]
2024-12-12 03:56:32.961550: Epoch time: 88.84 s
2024-12-12 03:56:34.132400: Patience reached. Stopping training.
2024-12-12 03:56:34.562840: Training done.
2024-12-12 03:56:34.780155: Using splits from existing split file: /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset307_Sohee_Calcium_OCT_CrossValidation/splits_final.json
2024-12-12 03:56:34.783433: The split file contains 5 splits.
2024-12-12 03:56:34.784595: Desired fold for training: 2
2024-12-12 03:56:34.785678: This split has 6 training and 2 validation cases.
2024-12-12 03:56:34.786723: predicting 101-044
2024-12-12 03:56:34.802251: 101-044, shape torch.Size([1, 404, 498, 498]), rank 0
2024-12-12 03:58:24.128638: predicting 704-003
2024-12-12 03:58:24.158391: 704-003, shape torch.Size([1, 375, 498, 498]), rank 0
2024-12-12 04:00:14.437314: Validation complete
2024-12-12 04:00:14.438591: Mean Validation Dice:  0.7578965172547324

############################
INFO: You are using the old nnU-Net default plans. We have updated our recommendations. Please consider using those instead! Read more here: https://github.com/MIC-DKFZ/nnUNet/blob/master/documentation/resenc_presets.md
############################

Using device: cuda:0

#######################################################################
Please cite the following paper when using nnU-Net:
Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.
#######################################################################

2024-12-12 04:00:24.842330: Using torch.compile...
/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
################### Loading pretrained weights from file  /home/gridsan/nchutisilp/projects/OpenAI-CLIP/clip_pretrained_nnUNet.pt ###################
Below is the list of overlapping blocks in pretrained model and nnUNet architecture:
encoder.stages.0.0.convs.0.conv.weight shape torch.Size([32, 1, 3, 3, 3])
encoder.stages.0.0.convs.0.conv.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.norm.weight shape torch.Size([32])
encoder.stages.0.0.convs.0.norm.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.0.weight shape torch.Size([32, 1, 3, 3, 3])
encoder.stages.0.0.convs.0.all_modules.0.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.1.weight shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.1.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
encoder.stages.0.0.convs.1.conv.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.norm.weight shape torch.Size([32])
encoder.stages.0.0.convs.1.norm.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
encoder.stages.0.0.convs.1.all_modules.0.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.1.weight shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.1.bias shape torch.Size([32])
encoder.stages.1.0.convs.0.conv.weight shape torch.Size([64, 32, 3, 3, 3])
encoder.stages.1.0.convs.0.conv.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.norm.weight shape torch.Size([64])
encoder.stages.1.0.convs.0.norm.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.0.weight shape torch.Size([64, 32, 3, 3, 3])
encoder.stages.1.0.convs.0.all_modules.0.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.1.weight shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.1.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
encoder.stages.1.0.convs.1.conv.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.norm.weight shape torch.Size([64])
encoder.stages.1.0.convs.1.norm.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
encoder.stages.1.0.convs.1.all_modules.0.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.1.weight shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.1.bias shape torch.Size([64])
encoder.stages.2.0.convs.0.conv.weight shape torch.Size([128, 64, 3, 3, 3])
encoder.stages.2.0.convs.0.conv.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.norm.weight shape torch.Size([128])
encoder.stages.2.0.convs.0.norm.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.0.weight shape torch.Size([128, 64, 3, 3, 3])
encoder.stages.2.0.convs.0.all_modules.0.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.1.weight shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.1.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
encoder.stages.2.0.convs.1.conv.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.norm.weight shape torch.Size([128])
encoder.stages.2.0.convs.1.norm.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
encoder.stages.2.0.convs.1.all_modules.0.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.1.weight shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.1.bias shape torch.Size([128])
encoder.stages.3.0.convs.0.conv.weight shape torch.Size([256, 128, 3, 3, 3])
encoder.stages.3.0.convs.0.conv.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.norm.weight shape torch.Size([256])
encoder.stages.3.0.convs.0.norm.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.0.weight shape torch.Size([256, 128, 3, 3, 3])
encoder.stages.3.0.convs.0.all_modules.0.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.1.weight shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.1.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
encoder.stages.3.0.convs.1.conv.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.norm.weight shape torch.Size([256])
encoder.stages.3.0.convs.1.norm.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
encoder.stages.3.0.convs.1.all_modules.0.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.1.weight shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.1.bias shape torch.Size([256])
encoder.stages.4.0.convs.0.conv.weight shape torch.Size([320, 256, 3, 3, 3])
encoder.stages.4.0.convs.0.conv.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.norm.weight shape torch.Size([320])
encoder.stages.4.0.convs.0.norm.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.0.weight shape torch.Size([320, 256, 3, 3, 3])
encoder.stages.4.0.convs.0.all_modules.0.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.1.weight shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.1.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.4.0.convs.1.conv.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.norm.weight shape torch.Size([320])
encoder.stages.4.0.convs.1.norm.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.4.0.convs.1.all_modules.0.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.1.weight shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.1.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.0.conv.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.norm.weight shape torch.Size([320])
encoder.stages.5.0.convs.0.norm.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.0.all_modules.0.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.1.weight shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.1.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.1.conv.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.norm.weight shape torch.Size([320])
encoder.stages.5.0.convs.1.norm.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.1.all_modules.0.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.1.weight shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.0.0.convs.0.conv.weight shape torch.Size([32, 1, 3, 3, 3])
decoder.encoder.stages.0.0.convs.0.conv.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.norm.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.norm.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.0.weight shape torch.Size([32, 1, 3, 3, 3])
decoder.encoder.stages.0.0.convs.0.all_modules.0.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.1.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.1.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.encoder.stages.0.0.convs.1.conv.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.norm.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.norm.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.encoder.stages.0.0.convs.1.all_modules.0.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.1.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.1.bias shape torch.Size([32])
decoder.encoder.stages.1.0.convs.0.conv.weight shape torch.Size([64, 32, 3, 3, 3])
decoder.encoder.stages.1.0.convs.0.conv.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.norm.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.norm.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.0.weight shape torch.Size([64, 32, 3, 3, 3])
decoder.encoder.stages.1.0.convs.0.all_modules.0.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.1.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.1.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.encoder.stages.1.0.convs.1.conv.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.norm.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.norm.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.encoder.stages.1.0.convs.1.all_modules.0.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.1.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.1.bias shape torch.Size([64])
decoder.encoder.stages.2.0.convs.0.conv.weight shape torch.Size([128, 64, 3, 3, 3])
decoder.encoder.stages.2.0.convs.0.conv.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.norm.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.norm.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.0.weight shape torch.Size([128, 64, 3, 3, 3])
decoder.encoder.stages.2.0.convs.0.all_modules.0.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.1.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.1.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.encoder.stages.2.0.convs.1.conv.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.norm.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.norm.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.encoder.stages.2.0.convs.1.all_modules.0.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.1.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.1.bias shape torch.Size([128])
decoder.encoder.stages.3.0.convs.0.conv.weight shape torch.Size([256, 128, 3, 3, 3])
decoder.encoder.stages.3.0.convs.0.conv.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.norm.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.norm.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.0.weight shape torch.Size([256, 128, 3, 3, 3])
decoder.encoder.stages.3.0.convs.0.all_modules.0.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.1.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.1.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.encoder.stages.3.0.convs.1.conv.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.norm.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.norm.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.encoder.stages.3.0.convs.1.all_modules.0.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.1.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.1.bias shape torch.Size([256])
decoder.encoder.stages.4.0.convs.0.conv.weight shape torch.Size([320, 256, 3, 3, 3])
decoder.encoder.stages.4.0.convs.0.conv.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.norm.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.norm.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.0.weight shape torch.Size([320, 256, 3, 3, 3])
decoder.encoder.stages.4.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.4.0.convs.1.conv.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.norm.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.norm.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.4.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.0.conv.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.norm.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.norm.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.1.conv.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.norm.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.norm.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.stages.0.convs.0.conv.weight shape torch.Size([320, 640, 3, 3, 3])
decoder.stages.0.convs.0.conv.bias shape torch.Size([320])
decoder.stages.0.convs.0.norm.weight shape torch.Size([320])
decoder.stages.0.convs.0.norm.bias shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.0.weight shape torch.Size([320, 640, 3, 3, 3])
decoder.stages.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.stages.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.stages.0.convs.1.conv.bias shape torch.Size([320])
decoder.stages.0.convs.1.norm.weight shape torch.Size([320])
decoder.stages.0.convs.1.norm.bias shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.stages.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.stages.1.convs.0.conv.weight shape torch.Size([256, 512, 3, 3, 3])
decoder.stages.1.convs.0.conv.bias shape torch.Size([256])
decoder.stages.1.convs.0.norm.weight shape torch.Size([256])
decoder.stages.1.convs.0.norm.bias shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.0.weight shape torch.Size([256, 512, 3, 3, 3])
decoder.stages.1.convs.0.all_modules.0.bias shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.1.weight shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.1.bias shape torch.Size([256])
decoder.stages.1.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.stages.1.convs.1.conv.bias shape torch.Size([256])
decoder.stages.1.convs.1.norm.weight shape torch.Size([256])
decoder.stages.1.convs.1.norm.bias shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.stages.1.convs.1.all_modules.0.bias shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.1.weight shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.1.bias shape torch.Size([256])
decoder.stages.2.convs.0.conv.weight shape torch.Size([128, 256, 3, 3, 3])
decoder.stages.2.convs.0.conv.bias shape torch.Size([128])
decoder.stages.2.convs.0.norm.weight shape torch.Size([128])
decoder.stages.2.convs.0.norm.bias shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.0.weight shape torch.Size([128, 256, 3, 3, 3])
decoder.stages.2.convs.0.all_modules.0.bias shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.1.weight shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.1.bias shape torch.Size([128])
decoder.stages.2.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.stages.2.convs.1.conv.bias shape torch.Size([128])
decoder.stages.2.convs.1.norm.weight shape torch.Size([128])
decoder.stages.2.convs.1.norm.bias shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.stages.2.convs.1.all_modules.0.bias shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.1.weight shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.1.bias shape torch.Size([128])
decoder.stages.3.convs.0.conv.weight shape torch.Size([64, 128, 3, 3, 3])
decoder.stages.3.convs.0.conv.bias shape torch.Size([64])
decoder.stages.3.convs.0.norm.weight shape torch.Size([64])
decoder.stages.3.convs.0.norm.bias shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.0.weight shape torch.Size([64, 128, 3, 3, 3])
decoder.stages.3.convs.0.all_modules.0.bias shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.1.weight shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.1.bias shape torch.Size([64])
decoder.stages.3.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.stages.3.convs.1.conv.bias shape torch.Size([64])
decoder.stages.3.convs.1.norm.weight shape torch.Size([64])
decoder.stages.3.convs.1.norm.bias shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.stages.3.convs.1.all_modules.0.bias shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.1.weight shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.1.bias shape torch.Size([64])
decoder.stages.4.convs.0.conv.weight shape torch.Size([32, 64, 3, 3, 3])
decoder.stages.4.convs.0.conv.bias shape torch.Size([32])
decoder.stages.4.convs.0.norm.weight shape torch.Size([32])
decoder.stages.4.convs.0.norm.bias shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.0.weight shape torch.Size([32, 64, 3, 3, 3])
decoder.stages.4.convs.0.all_modules.0.bias shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.1.weight shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.1.bias shape torch.Size([32])
decoder.stages.4.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.stages.4.convs.1.conv.bias shape torch.Size([32])
decoder.stages.4.convs.1.norm.weight shape torch.Size([32])
decoder.stages.4.convs.1.norm.bias shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.stages.4.convs.1.all_modules.0.bias shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.1.weight shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.1.bias shape torch.Size([32])
decoder.transpconvs.0.weight shape torch.Size([320, 320, 1, 2, 2])
decoder.transpconvs.0.bias shape torch.Size([320])
decoder.transpconvs.1.weight shape torch.Size([320, 256, 2, 2, 2])
decoder.transpconvs.1.bias shape torch.Size([256])
decoder.transpconvs.2.weight shape torch.Size([256, 128, 2, 2, 2])
decoder.transpconvs.2.bias shape torch.Size([128])
decoder.transpconvs.3.weight shape torch.Size([128, 64, 2, 2, 2])
decoder.transpconvs.3.bias shape torch.Size([64])
decoder.transpconvs.4.weight shape torch.Size([64, 32, 2, 2, 2])
decoder.transpconvs.4.bias shape torch.Size([32])
################### Done ###################
2024-12-12 04:00:49.641769: do_dummy_2d_data_aug: True
2024-12-12 04:00:49.645402: Using splits from existing split file: /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset307_Sohee_Calcium_OCT_CrossValidation/splits_final.json
2024-12-12 04:00:49.648418: The split file contains 5 splits.
2024-12-12 04:00:49.649743: Desired fold for training: 4
2024-12-12 04:00:49.651109: This split has 7 training and 1 validation cases.
using pin_memory on device 0
using pin_memory on device 0

This is the configuration used by this training:
Configuration name: 3d_32x160x128_b10
 {'data_identifier': 'nnUNetPlans_3d_fullres', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 10, 'patch_size': [32, 160, 128], 'median_image_size_in_voxels': [375.0, 498.0, 498.0], 'spacing': [1.0, 1.0, 1.0], 'normalization_schemes': ['ZScoreNormalization'], 'use_mask_for_norm': [False], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'architecture': {'network_class_name': 'dynamic_network_architectures.architectures.unet.PlainConvUNet', 'arch_kwargs': {'n_stages': 6, 'features_per_stage': [32, 64, 128, 256, 320, 320], 'conv_op': 'torch.nn.modules.conv.Conv3d', 'kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]], 'strides': [[1, 1, 1], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [1, 2, 2]], 'n_conv_per_stage': [2, 2, 2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2, 2, 2], 'conv_bias': True, 'norm_op': 'torch.nn.modules.instancenorm.InstanceNorm3d', 'norm_op_kwargs': {'eps': 1e-05, 'affine': True}, 'dropout_op': None, 'dropout_op_kwargs': None, 'nonlin': 'torch.nn.LeakyReLU', 'nonlin_kwargs': {'inplace': True}, 'deep_supervision': True}, '_kw_requires_import': ['conv_op', 'norm_op', 'dropout_op', 'nonlin']}, 'batch_dice': True, 'inherits_from': '3d_fullres'} 

These are the global plan.json settings:
 {'dataset_name': 'Dataset307_Sohee_Calcium_OCT_CrossValidation', 'plans_name': 'nnUNetPlans', 'original_median_spacing_after_transp': [1.0, 1.0, 1.0], 'original_median_shape_after_transp': [375, 498, 498], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 0.4977976381778717, 'mean': 0.13507525622844696, 'median': 0.09599608182907104, 'min': 0.0, 'percentile_00_5': 0.014754901640117168, 'percentile_99_5': 0.4977976381778717, 'std': 0.12025152146816254}}} 

2024-12-12 04:00:54.177044: unpacking dataset...
2024-12-12 04:00:57.989334: unpacking done...
2024-12-12 04:00:58.374555: Unable to plot network architecture: nnUNet_compile is enabled!
2024-12-12 04:00:58.552989: 
2024-12-12 04:00:58.555839: Epoch 0
2024-12-12 04:00:58.558296: Current learning rate: 0.01
2024-12-12 04:03:59.979351: Validation loss improved from 1000.00000 to -0.23236! Patience: 0/50
2024-12-12 04:03:59.980597: train_loss -0.0827
2024-12-12 04:03:59.982070: val_loss -0.2324
2024-12-12 04:03:59.983066: Pseudo dice [0.5503]
2024-12-12 04:03:59.984017: Epoch time: 181.43 s
2024-12-12 04:03:59.985008: Yayy! New best EMA pseudo Dice: 0.5503
2024-12-12 04:04:02.241786: 
2024-12-12 04:04:02.244258: Epoch 1
2024-12-12 04:04:02.245685: Current learning rate: 0.00999
2024-12-12 04:05:30.915731: Validation loss improved from -0.23236 to -0.24827! Patience: 0/50
2024-12-12 04:05:30.916817: train_loss -0.2361
2024-12-12 04:05:30.918280: val_loss -0.2483
2024-12-12 04:05:30.919381: Pseudo dice [0.548]
2024-12-12 04:05:30.920376: Epoch time: 88.68 s
2024-12-12 04:05:32.119539: 
2024-12-12 04:05:32.121515: Epoch 2
2024-12-12 04:05:32.122718: Current learning rate: 0.00998
2024-12-12 04:07:00.382666: Validation loss improved from -0.24827 to -0.27239! Patience: 0/50
2024-12-12 04:07:00.383940: train_loss -0.2742
2024-12-12 04:07:00.385065: val_loss -0.2724
2024-12-12 04:07:00.385845: Pseudo dice [0.5861]
2024-12-12 04:07:00.386507: Epoch time: 88.27 s
2024-12-12 04:07:00.387314: Yayy! New best EMA pseudo Dice: 0.5537
2024-12-12 04:07:01.988143: 
2024-12-12 04:07:01.990157: Epoch 3
2024-12-12 04:07:01.991271: Current learning rate: 0.00997
2024-12-12 04:08:30.119285: Validation loss improved from -0.27239 to -0.31342! Patience: 0/50
2024-12-12 04:08:30.120599: train_loss -0.2955
2024-12-12 04:08:30.122053: val_loss -0.3134
2024-12-12 04:08:30.123176: Pseudo dice [0.62]
2024-12-12 04:08:30.124099: Epoch time: 88.13 s
2024-12-12 04:08:30.125024: Yayy! New best EMA pseudo Dice: 0.5603
2024-12-12 04:08:31.666006: 
2024-12-12 04:08:31.667927: Epoch 4
2024-12-12 04:08:31.669039: Current learning rate: 0.00996
2024-12-12 04:09:59.535115: Validation loss improved from -0.31342 to -0.34822! Patience: 0/50
2024-12-12 04:09:59.536320: train_loss -0.3389
2024-12-12 04:09:59.537591: val_loss -0.3482
2024-12-12 04:09:59.538876: Pseudo dice [0.638]
2024-12-12 04:09:59.540001: Epoch time: 87.87 s
2024-12-12 04:09:59.853336: Yayy! New best EMA pseudo Dice: 0.5681
2024-12-12 04:10:01.469249: 
2024-12-12 04:10:01.471274: Epoch 5
2024-12-12 04:10:01.472435: Current learning rate: 0.00995
2024-12-12 04:11:29.446017: Validation loss did not improve from -0.34822. Patience: 1/50
2024-12-12 04:11:29.447677: train_loss -0.3483
2024-12-12 04:11:29.449010: val_loss -0.302
2024-12-12 04:11:29.449667: Pseudo dice [0.6309]
2024-12-12 04:11:29.450543: Epoch time: 87.98 s
2024-12-12 04:11:29.451168: Yayy! New best EMA pseudo Dice: 0.5744
2024-12-12 04:11:30.962058: 
2024-12-12 04:11:30.964247: Epoch 6
2024-12-12 04:11:30.965220: Current learning rate: 0.00995
2024-12-12 04:12:58.811888: Validation loss did not improve from -0.34822. Patience: 2/50
2024-12-12 04:12:58.813196: train_loss -0.3857
2024-12-12 04:12:58.814527: val_loss -0.335
2024-12-12 04:12:58.815654: Pseudo dice [0.6216]
2024-12-12 04:12:58.816683: Epoch time: 87.85 s
2024-12-12 04:12:58.817673: Yayy! New best EMA pseudo Dice: 0.5791
2024-12-12 04:13:00.346187: 
2024-12-12 04:13:00.348178: Epoch 7
2024-12-12 04:13:00.349487: Current learning rate: 0.00994
2024-12-12 04:14:28.061669: Validation loss improved from -0.34822 to -0.36785! Patience: 2/50
2024-12-12 04:14:28.062844: train_loss -0.4071
2024-12-12 04:14:28.063669: val_loss -0.3678
2024-12-12 04:14:28.064344: Pseudo dice [0.6373]
2024-12-12 04:14:28.065178: Epoch time: 87.72 s
2024-12-12 04:14:28.066009: Yayy! New best EMA pseudo Dice: 0.5849
2024-12-12 04:14:29.642890: 
2024-12-12 04:14:29.645036: Epoch 8
2024-12-12 04:14:29.646273: Current learning rate: 0.00993
2024-12-12 04:15:58.199426: Validation loss did not improve from -0.36785. Patience: 1/50
2024-12-12 04:15:58.200666: train_loss -0.4147
2024-12-12 04:15:58.202078: val_loss -0.3493
2024-12-12 04:15:58.203044: Pseudo dice [0.6294]
2024-12-12 04:15:58.203716: Epoch time: 88.56 s
2024-12-12 04:15:58.204519: Yayy! New best EMA pseudo Dice: 0.5894
2024-12-12 04:16:00.138049: 
2024-12-12 04:16:00.139932: Epoch 9
2024-12-12 04:16:00.140848: Current learning rate: 0.00992
2024-12-12 04:17:28.309657: Validation loss did not improve from -0.36785. Patience: 2/50
2024-12-12 04:17:28.311102: train_loss -0.423
2024-12-12 04:17:28.312427: val_loss -0.3645
2024-12-12 04:17:28.313340: Pseudo dice [0.6563]
2024-12-12 04:17:28.314089: Epoch time: 88.17 s
2024-12-12 04:17:28.663377: Yayy! New best EMA pseudo Dice: 0.5961
2024-12-12 04:17:30.137499: 
2024-12-12 04:17:30.139551: Epoch 10
2024-12-12 04:17:30.141021: Current learning rate: 0.00991
2024-12-12 04:18:58.220588: Validation loss did not improve from -0.36785. Patience: 3/50
2024-12-12 04:18:58.221819: train_loss -0.4438
2024-12-12 04:18:58.223322: val_loss -0.305
2024-12-12 04:18:58.224128: Pseudo dice [0.626]
2024-12-12 04:18:58.225021: Epoch time: 88.09 s
2024-12-12 04:18:58.225873: Yayy! New best EMA pseudo Dice: 0.599
2024-12-12 04:18:59.821896: 
2024-12-12 04:18:59.824323: Epoch 11
2024-12-12 04:18:59.825797: Current learning rate: 0.0099
2024-12-12 04:20:28.005772: Validation loss improved from -0.36785 to -0.38244! Patience: 3/50
2024-12-12 04:20:28.007153: train_loss -0.451
2024-12-12 04:20:28.008491: val_loss -0.3824
2024-12-12 04:20:28.009610: Pseudo dice [0.6564]
2024-12-12 04:20:28.010540: Epoch time: 88.19 s
2024-12-12 04:20:28.011544: Yayy! New best EMA pseudo Dice: 0.6048
2024-12-12 04:20:29.502640: 
2024-12-12 04:20:29.505043: Epoch 12
2024-12-12 04:20:29.506352: Current learning rate: 0.00989
2024-12-12 04:21:57.489541: Validation loss improved from -0.38244 to -0.42151! Patience: 0/50
2024-12-12 04:21:57.490824: train_loss -0.4695
2024-12-12 04:21:57.491706: val_loss -0.4215
2024-12-12 04:21:57.492458: Pseudo dice [0.682]
2024-12-12 04:21:57.493085: Epoch time: 87.99 s
2024-12-12 04:21:57.494033: Yayy! New best EMA pseudo Dice: 0.6125
2024-12-12 04:21:59.073418: 
2024-12-12 04:21:59.075316: Epoch 13
2024-12-12 04:21:59.076298: Current learning rate: 0.00988
2024-12-12 04:23:27.029364: Validation loss improved from -0.42151 to -0.42220! Patience: 0/50
2024-12-12 04:23:27.030534: train_loss -0.4803
2024-12-12 04:23:27.031750: val_loss -0.4222
2024-12-12 04:23:27.032896: Pseudo dice [0.6785]
2024-12-12 04:23:27.033699: Epoch time: 87.96 s
2024-12-12 04:23:27.034322: Yayy! New best EMA pseudo Dice: 0.6191
2024-12-12 04:23:28.537462: 
2024-12-12 04:23:28.539572: Epoch 14
2024-12-12 04:23:28.540741: Current learning rate: 0.00987
2024-12-12 04:24:56.656641: Validation loss did not improve from -0.42220. Patience: 1/50
2024-12-12 04:24:56.657961: train_loss -0.488
2024-12-12 04:24:56.659082: val_loss -0.3956
2024-12-12 04:24:56.659947: Pseudo dice [0.6654]
2024-12-12 04:24:56.660716: Epoch time: 88.12 s
2024-12-12 04:24:57.005192: Yayy! New best EMA pseudo Dice: 0.6237
2024-12-12 04:24:58.544311: 
2024-12-12 04:24:58.545954: Epoch 15
2024-12-12 04:24:58.547284: Current learning rate: 0.00986
2024-12-12 04:26:26.707711: Validation loss did not improve from -0.42220. Patience: 2/50
2024-12-12 04:26:26.709063: train_loss -0.5002
2024-12-12 04:26:26.710134: val_loss -0.4024
2024-12-12 04:26:26.710957: Pseudo dice [0.668]
2024-12-12 04:26:26.711685: Epoch time: 88.17 s
2024-12-12 04:26:26.712376: Yayy! New best EMA pseudo Dice: 0.6282
2024-12-12 04:26:28.228400: 
2024-12-12 04:26:28.230234: Epoch 16
2024-12-12 04:26:28.231601: Current learning rate: 0.00986
2024-12-12 04:27:56.811059: Validation loss improved from -0.42220 to -0.43176! Patience: 2/50
2024-12-12 04:27:56.812270: train_loss -0.5006
2024-12-12 04:27:56.813429: val_loss -0.4318
2024-12-12 04:27:56.814479: Pseudo dice [0.6761]
2024-12-12 04:27:56.815254: Epoch time: 88.58 s
2024-12-12 04:27:56.815929: Yayy! New best EMA pseudo Dice: 0.633
2024-12-12 04:27:58.373832: 
2024-12-12 04:27:58.375925: Epoch 17
2024-12-12 04:27:58.377250: Current learning rate: 0.00985
2024-12-12 04:29:27.099262: Validation loss did not improve from -0.43176. Patience: 1/50
2024-12-12 04:29:27.100519: train_loss -0.4994
2024-12-12 04:29:27.101578: val_loss -0.403
2024-12-12 04:29:27.102455: Pseudo dice [0.6613]
2024-12-12 04:29:27.103497: Epoch time: 88.73 s
2024-12-12 04:29:27.104105: Yayy! New best EMA pseudo Dice: 0.6358
2024-12-12 04:29:28.631226: 
2024-12-12 04:29:28.633042: Epoch 18
2024-12-12 04:29:28.634310: Current learning rate: 0.00984
2024-12-12 04:30:57.345917: Validation loss did not improve from -0.43176. Patience: 2/50
2024-12-12 04:30:57.347324: train_loss -0.5118
2024-12-12 04:30:57.348741: val_loss -0.4316
2024-12-12 04:30:57.349792: Pseudo dice [0.6801]
2024-12-12 04:30:57.350796: Epoch time: 88.72 s
2024-12-12 04:30:57.351795: Yayy! New best EMA pseudo Dice: 0.6402
2024-12-12 04:30:58.908090: 
2024-12-12 04:30:58.910351: Epoch 19
2024-12-12 04:30:58.911742: Current learning rate: 0.00983
2024-12-12 04:32:27.635146: Validation loss improved from -0.43176 to -0.45286! Patience: 2/50
2024-12-12 04:32:27.636127: train_loss -0.523
2024-12-12 04:32:27.637094: val_loss -0.4529
2024-12-12 04:32:27.637733: Pseudo dice [0.6905]
2024-12-12 04:32:27.638744: Epoch time: 88.73 s
2024-12-12 04:32:27.984297: Yayy! New best EMA pseudo Dice: 0.6453
2024-12-12 04:32:29.796151: 
2024-12-12 04:32:29.797937: Epoch 20
2024-12-12 04:32:29.798926: Current learning rate: 0.00982
2024-12-12 04:33:58.500466: Validation loss did not improve from -0.45286. Patience: 1/50
2024-12-12 04:33:58.501753: train_loss -0.5146
2024-12-12 04:33:58.503498: val_loss -0.4134
2024-12-12 04:33:58.504874: Pseudo dice [0.68]
2024-12-12 04:33:58.505963: Epoch time: 88.71 s
2024-12-12 04:33:58.506850: Yayy! New best EMA pseudo Dice: 0.6487
2024-12-12 04:34:00.046531: 
2024-12-12 04:34:00.048736: Epoch 21
2024-12-12 04:34:00.049672: Current learning rate: 0.00981
2024-12-12 04:35:28.508435: Validation loss did not improve from -0.45286. Patience: 2/50
2024-12-12 04:35:28.509874: train_loss -0.5351
2024-12-12 04:35:28.511093: val_loss -0.4427
2024-12-12 04:35:28.511945: Pseudo dice [0.6987]
2024-12-12 04:35:28.512810: Epoch time: 88.46 s
2024-12-12 04:35:28.513621: Yayy! New best EMA pseudo Dice: 0.6537
2024-12-12 04:35:29.980734: 
2024-12-12 04:35:29.982536: Epoch 22
2024-12-12 04:35:29.983515: Current learning rate: 0.0098
2024-12-12 04:36:58.069214: Validation loss did not improve from -0.45286. Patience: 3/50
2024-12-12 04:36:58.070511: train_loss -0.5447
2024-12-12 04:36:58.071397: val_loss -0.4239
2024-12-12 04:36:58.072226: Pseudo dice [0.6841]
2024-12-12 04:36:58.073048: Epoch time: 88.09 s
2024-12-12 04:36:58.073900: Yayy! New best EMA pseudo Dice: 0.6568
2024-12-12 04:36:59.567810: 
2024-12-12 04:36:59.569989: Epoch 23
2024-12-12 04:36:59.571028: Current learning rate: 0.00979
2024-12-12 04:38:27.614036: Validation loss improved from -0.45286 to -0.48635! Patience: 3/50
2024-12-12 04:38:27.615333: train_loss -0.5411
2024-12-12 04:38:27.616503: val_loss -0.4863
2024-12-12 04:38:27.617306: Pseudo dice [0.7131]
2024-12-12 04:38:27.618101: Epoch time: 88.05 s
2024-12-12 04:38:27.618936: Yayy! New best EMA pseudo Dice: 0.6624
2024-12-12 04:38:29.078962: 
2024-12-12 04:38:29.080528: Epoch 24
2024-12-12 04:38:29.081897: Current learning rate: 0.00978
2024-12-12 04:39:57.142277: Validation loss improved from -0.48635 to -0.48742! Patience: 0/50
2024-12-12 04:39:57.143563: train_loss -0.5435
2024-12-12 04:39:57.144982: val_loss -0.4874
2024-12-12 04:39:57.146070: Pseudo dice [0.7205]
2024-12-12 04:39:57.147065: Epoch time: 88.07 s
2024-12-12 04:39:57.485368: Yayy! New best EMA pseudo Dice: 0.6682
2024-12-12 04:39:58.951022: 
2024-12-12 04:39:58.953346: Epoch 25
2024-12-12 04:39:58.954870: Current learning rate: 0.00977
2024-12-12 04:41:26.834184: Validation loss did not improve from -0.48742. Patience: 1/50
2024-12-12 04:41:26.835181: train_loss -0.5593
2024-12-12 04:41:26.836288: val_loss -0.4608
2024-12-12 04:41:26.837427: Pseudo dice [0.6926]
2024-12-12 04:41:26.838721: Epoch time: 87.89 s
2024-12-12 04:41:26.839991: Yayy! New best EMA pseudo Dice: 0.6706
2024-12-12 04:41:28.311289: 
2024-12-12 04:41:28.313030: Epoch 26
2024-12-12 04:41:28.314059: Current learning rate: 0.00977
2024-12-12 04:42:56.203615: Validation loss did not improve from -0.48742. Patience: 2/50
2024-12-12 04:42:56.204961: train_loss -0.5582
2024-12-12 04:42:56.206110: val_loss -0.4681
2024-12-12 04:42:56.206797: Pseudo dice [0.6868]
2024-12-12 04:42:56.207838: Epoch time: 87.89 s
2024-12-12 04:42:56.208920: Yayy! New best EMA pseudo Dice: 0.6723
2024-12-12 04:42:57.666163: 
2024-12-12 04:42:57.667953: Epoch 27
2024-12-12 04:42:57.668651: Current learning rate: 0.00976
2024-12-12 04:44:25.470127: Validation loss did not improve from -0.48742. Patience: 3/50
2024-12-12 04:44:25.471383: train_loss -0.559
2024-12-12 04:44:25.472600: val_loss -0.4366
2024-12-12 04:44:25.473497: Pseudo dice [0.6968]
2024-12-12 04:44:25.474717: Epoch time: 87.81 s
2024-12-12 04:44:25.476018: Yayy! New best EMA pseudo Dice: 0.6747
2024-12-12 04:44:26.986160: 
2024-12-12 04:44:26.988358: Epoch 28
2024-12-12 04:44:26.989510: Current learning rate: 0.00975
2024-12-12 04:45:54.877312: Validation loss did not improve from -0.48742. Patience: 4/50
2024-12-12 04:45:54.878491: train_loss -0.5622
2024-12-12 04:45:54.879744: val_loss -0.4742
2024-12-12 04:45:54.880952: Pseudo dice [0.7093]
2024-12-12 04:45:54.881904: Epoch time: 87.89 s
2024-12-12 04:45:54.882634: Yayy! New best EMA pseudo Dice: 0.6782
2024-12-12 04:45:56.372118: 
2024-12-12 04:45:56.374281: Epoch 29
2024-12-12 04:45:56.375180: Current learning rate: 0.00974
2024-12-12 04:47:24.333358: Validation loss did not improve from -0.48742. Patience: 5/50
2024-12-12 04:47:24.335052: train_loss -0.5752
2024-12-12 04:47:24.337180: val_loss -0.4654
2024-12-12 04:47:24.338211: Pseudo dice [0.7004]
2024-12-12 04:47:24.338990: Epoch time: 87.96 s
2024-12-12 04:47:24.674606: Yayy! New best EMA pseudo Dice: 0.6804
2024-12-12 04:47:26.471110: 
2024-12-12 04:47:26.472879: Epoch 30
2024-12-12 04:47:26.473988: Current learning rate: 0.00973
2024-12-12 04:48:54.599619: Validation loss did not improve from -0.48742. Patience: 6/50
2024-12-12 04:48:54.600884: train_loss -0.572
2024-12-12 04:48:54.601777: val_loss -0.4795
2024-12-12 04:48:54.602543: Pseudo dice [0.7083]
2024-12-12 04:48:54.603358: Epoch time: 88.13 s
2024-12-12 04:48:54.604083: Yayy! New best EMA pseudo Dice: 0.6832
2024-12-12 04:48:56.077932: 
2024-12-12 04:48:56.079956: Epoch 31
2024-12-12 04:48:56.080961: Current learning rate: 0.00972
2024-12-12 04:50:24.133884: Validation loss did not improve from -0.48742. Patience: 7/50
2024-12-12 04:50:24.135166: train_loss -0.5878
2024-12-12 04:50:24.136500: val_loss -0.4038
2024-12-12 04:50:24.137384: Pseudo dice [0.6759]
2024-12-12 04:50:24.138173: Epoch time: 88.06 s
2024-12-12 04:50:25.340467: 
2024-12-12 04:50:25.342384: Epoch 32
2024-12-12 04:50:25.343797: Current learning rate: 0.00971
2024-12-12 04:51:53.571299: Validation loss did not improve from -0.48742. Patience: 8/50
2024-12-12 04:51:53.572772: train_loss -0.5817
2024-12-12 04:51:53.574455: val_loss -0.4417
2024-12-12 04:51:53.575618: Pseudo dice [0.6812]
2024-12-12 04:51:53.576835: Epoch time: 88.23 s
2024-12-12 04:51:54.745356: 
2024-12-12 04:51:54.747842: Epoch 33
2024-12-12 04:51:54.748796: Current learning rate: 0.0097
2024-12-12 04:53:22.859147: Validation loss improved from -0.48742 to -0.50127! Patience: 8/50
2024-12-12 04:53:22.860820: train_loss -0.5872
2024-12-12 04:53:22.862400: val_loss -0.5013
2024-12-12 04:53:22.863505: Pseudo dice [0.7153]
2024-12-12 04:53:22.864822: Epoch time: 88.12 s
2024-12-12 04:53:22.865790: Yayy! New best EMA pseudo Dice: 0.6856
2024-12-12 04:53:24.394046: 
2024-12-12 04:53:24.396404: Epoch 34
2024-12-12 04:53:24.397607: Current learning rate: 0.00969
2024-12-12 04:54:52.338124: Validation loss did not improve from -0.50127. Patience: 1/50
2024-12-12 04:54:52.339416: train_loss -0.583
2024-12-12 04:54:52.340671: val_loss -0.4368
2024-12-12 04:54:52.341686: Pseudo dice [0.6935]
2024-12-12 04:54:52.342427: Epoch time: 87.95 s
2024-12-12 04:54:52.680824: Yayy! New best EMA pseudo Dice: 0.6864
2024-12-12 04:54:54.211730: 
2024-12-12 04:54:54.213705: Epoch 35
2024-12-12 04:54:54.214986: Current learning rate: 0.00968
2024-12-12 04:56:22.313057: Validation loss did not improve from -0.50127. Patience: 2/50
2024-12-12 04:56:22.314339: train_loss -0.5922
2024-12-12 04:56:22.316043: val_loss -0.4728
2024-12-12 04:56:22.317106: Pseudo dice [0.7107]
2024-12-12 04:56:22.318131: Epoch time: 88.1 s
2024-12-12 04:56:22.319188: Yayy! New best EMA pseudo Dice: 0.6888
2024-12-12 04:56:23.841442: 
2024-12-12 04:56:23.843041: Epoch 36
2024-12-12 04:56:23.844156: Current learning rate: 0.00968
2024-12-12 04:57:51.958382: Validation loss did not improve from -0.50127. Patience: 3/50
2024-12-12 04:57:51.959720: train_loss -0.5962
2024-12-12 04:57:51.960757: val_loss -0.468
2024-12-12 04:57:51.961678: Pseudo dice [0.6974]
2024-12-12 04:57:51.962654: Epoch time: 88.12 s
2024-12-12 04:57:51.963634: Yayy! New best EMA pseudo Dice: 0.6897
2024-12-12 04:57:53.515908: 
2024-12-12 04:57:53.518138: Epoch 37
2024-12-12 04:57:53.519468: Current learning rate: 0.00967
2024-12-12 04:59:21.553296: Validation loss did not improve from -0.50127. Patience: 4/50
2024-12-12 04:59:21.554629: train_loss -0.6007
2024-12-12 04:59:21.556040: val_loss -0.4647
2024-12-12 04:59:21.557117: Pseudo dice [0.7016]
2024-12-12 04:59:21.558049: Epoch time: 88.04 s
2024-12-12 04:59:21.558910: Yayy! New best EMA pseudo Dice: 0.6909
2024-12-12 04:59:23.150545: 
2024-12-12 04:59:23.152911: Epoch 38
2024-12-12 04:59:23.154989: Current learning rate: 0.00966
2024-12-12 05:00:51.535045: Validation loss did not improve from -0.50127. Patience: 5/50
2024-12-12 05:00:51.536360: train_loss -0.5979
2024-12-12 05:00:51.537573: val_loss -0.4622
2024-12-12 05:00:51.538678: Pseudo dice [0.6996]
2024-12-12 05:00:51.539703: Epoch time: 88.39 s
2024-12-12 05:00:51.540711: Yayy! New best EMA pseudo Dice: 0.6918
2024-12-12 05:00:53.082697: 
2024-12-12 05:00:53.084862: Epoch 39
2024-12-12 05:00:53.085945: Current learning rate: 0.00965
2024-12-12 05:02:21.216222: Validation loss improved from -0.50127 to -0.50400! Patience: 5/50
2024-12-12 05:02:21.217914: train_loss -0.5991
2024-12-12 05:02:21.219420: val_loss -0.504
2024-12-12 05:02:21.220447: Pseudo dice [0.7121]
2024-12-12 05:02:21.221341: Epoch time: 88.14 s
2024-12-12 05:02:21.565869: Yayy! New best EMA pseudo Dice: 0.6938
2024-12-12 05:02:23.459074: 
2024-12-12 05:02:23.461455: Epoch 40
2024-12-12 05:02:23.462531: Current learning rate: 0.00964
2024-12-12 05:03:52.118754: Validation loss did not improve from -0.50400. Patience: 1/50
2024-12-12 05:03:52.119994: train_loss -0.6018
2024-12-12 05:03:52.121563: val_loss -0.4494
2024-12-12 05:03:52.122524: Pseudo dice [0.6906]
2024-12-12 05:03:52.123441: Epoch time: 88.66 s
2024-12-12 05:03:53.355263: 
2024-12-12 05:03:53.357624: Epoch 41
2024-12-12 05:03:53.358646: Current learning rate: 0.00963
2024-12-12 05:05:21.397426: Validation loss did not improve from -0.50400. Patience: 2/50
2024-12-12 05:05:21.398859: train_loss -0.615
2024-12-12 05:05:21.400368: val_loss -0.4824
2024-12-12 05:05:21.401312: Pseudo dice [0.7155]
2024-12-12 05:05:21.402162: Epoch time: 88.04 s
2024-12-12 05:05:21.403449: Yayy! New best EMA pseudo Dice: 0.6957
2024-12-12 05:05:22.864908: 
2024-12-12 05:05:22.867287: Epoch 42
2024-12-12 05:05:22.868163: Current learning rate: 0.00962
2024-12-12 05:06:51.295290: Validation loss improved from -0.50400 to -0.50818! Patience: 2/50
2024-12-12 05:06:51.324472: train_loss -0.6155
2024-12-12 05:06:51.328427: val_loss -0.5082
2024-12-12 05:06:51.329266: Pseudo dice [0.7254]
2024-12-12 05:06:51.330680: Epoch time: 88.43 s
2024-12-12 05:06:51.331554: Yayy! New best EMA pseudo Dice: 0.6987
2024-12-12 05:06:53.344524: 
2024-12-12 05:06:53.346789: Epoch 43
2024-12-12 05:06:53.347933: Current learning rate: 0.00961
2024-12-12 05:08:23.076348: Validation loss did not improve from -0.50818. Patience: 1/50
2024-12-12 05:08:23.108081: train_loss -0.6112
2024-12-12 05:08:23.111321: val_loss -0.4803
2024-12-12 05:08:23.112375: Pseudo dice [0.7064]
2024-12-12 05:08:23.113757: Epoch time: 89.76 s
2024-12-12 05:08:23.115424: Yayy! New best EMA pseudo Dice: 0.6994
2024-12-12 05:08:25.096965: 
2024-12-12 05:08:25.099788: Epoch 44
2024-12-12 05:08:25.101362: Current learning rate: 0.0096
2024-12-12 05:09:53.157825: Validation loss did not improve from -0.50818. Patience: 2/50
2024-12-12 05:09:53.158819: train_loss -0.6258
2024-12-12 05:09:53.160074: val_loss -0.4378
2024-12-12 05:09:53.161054: Pseudo dice [0.6977]
2024-12-12 05:09:53.162191: Epoch time: 88.06 s
2024-12-12 05:09:54.638092: 
2024-12-12 05:09:54.640228: Epoch 45
2024-12-12 05:09:54.641107: Current learning rate: 0.00959
2024-12-12 05:11:22.717833: Validation loss did not improve from -0.50818. Patience: 3/50
2024-12-12 05:11:22.719680: train_loss -0.6318
2024-12-12 05:11:22.720834: val_loss -0.4731
2024-12-12 05:11:22.721636: Pseudo dice [0.7029]
2024-12-12 05:11:22.722416: Epoch time: 88.08 s
2024-12-12 05:11:22.723180: Yayy! New best EMA pseudo Dice: 0.6996
2024-12-12 05:11:24.253449: 
2024-12-12 05:11:24.255277: Epoch 46
2024-12-12 05:11:24.256557: Current learning rate: 0.00959
2024-12-12 05:12:52.432366: Validation loss did not improve from -0.50818. Patience: 4/50
2024-12-12 05:12:52.433367: train_loss -0.6315
2024-12-12 05:12:52.434480: val_loss -0.4872
2024-12-12 05:12:52.435591: Pseudo dice [0.7168]
2024-12-12 05:12:52.436572: Epoch time: 88.18 s
2024-12-12 05:12:52.437435: Yayy! New best EMA pseudo Dice: 0.7013
2024-12-12 05:12:53.923050: 
2024-12-12 05:12:53.925093: Epoch 47
2024-12-12 05:12:53.926492: Current learning rate: 0.00958
2024-12-12 05:14:22.505144: Validation loss did not improve from -0.50818. Patience: 5/50
2024-12-12 05:14:22.506319: train_loss -0.6322
2024-12-12 05:14:22.507385: val_loss -0.4817
2024-12-12 05:14:22.508048: Pseudo dice [0.705]
2024-12-12 05:14:22.508795: Epoch time: 88.58 s
2024-12-12 05:14:22.509569: Yayy! New best EMA pseudo Dice: 0.7017
2024-12-12 05:14:23.961957: 
2024-12-12 05:14:23.964170: Epoch 48
2024-12-12 05:14:23.965206: Current learning rate: 0.00957
2024-12-12 05:15:52.558679: Validation loss did not improve from -0.50818. Patience: 6/50
2024-12-12 05:15:52.559962: train_loss -0.6319
2024-12-12 05:15:52.561478: val_loss -0.4899
2024-12-12 05:15:52.562448: Pseudo dice [0.7194]
2024-12-12 05:15:52.563307: Epoch time: 88.6 s
2024-12-12 05:15:52.564201: Yayy! New best EMA pseudo Dice: 0.7035
2024-12-12 05:15:54.062037: 
2024-12-12 05:15:54.064692: Epoch 49
2024-12-12 05:15:54.066121: Current learning rate: 0.00956
2024-12-12 05:17:22.696369: Validation loss did not improve from -0.50818. Patience: 7/50
2024-12-12 05:17:22.697433: train_loss -0.6363
2024-12-12 05:17:22.698438: val_loss -0.4933
2024-12-12 05:17:22.699661: Pseudo dice [0.7135]
2024-12-12 05:17:22.700639: Epoch time: 88.64 s
2024-12-12 05:17:23.043121: Yayy! New best EMA pseudo Dice: 0.7045
2024-12-12 05:17:25.768726: 
2024-12-12 05:17:25.770478: Epoch 50
2024-12-12 05:17:25.771338: Current learning rate: 0.00955
2024-12-12 05:18:54.395342: Validation loss did not improve from -0.50818. Patience: 8/50
2024-12-12 05:18:54.396735: train_loss -0.6325
2024-12-12 05:18:54.397902: val_loss -0.4648
2024-12-12 05:18:54.399177: Pseudo dice [0.7123]
2024-12-12 05:18:54.400270: Epoch time: 88.63 s
2024-12-12 05:18:54.401246: Yayy! New best EMA pseudo Dice: 0.7053
2024-12-12 05:18:55.878355: 
2024-12-12 05:18:55.880529: Epoch 51
2024-12-12 05:18:55.881992: Current learning rate: 0.00954
2024-12-12 05:20:24.498318: Validation loss did not improve from -0.50818. Patience: 9/50
2024-12-12 05:20:24.499662: train_loss -0.6449
2024-12-12 05:20:24.500952: val_loss -0.4487
2024-12-12 05:20:24.501876: Pseudo dice [0.6893]
2024-12-12 05:20:24.502685: Epoch time: 88.62 s
2024-12-12 05:20:25.651514: 
2024-12-12 05:20:25.653514: Epoch 52
2024-12-12 05:20:25.654392: Current learning rate: 0.00953
2024-12-12 05:21:54.268707: Validation loss did not improve from -0.50818. Patience: 10/50
2024-12-12 05:21:54.270213: train_loss -0.6426
2024-12-12 05:21:54.271585: val_loss -0.4977
2024-12-12 05:21:54.272604: Pseudo dice [0.7194]
2024-12-12 05:21:54.273385: Epoch time: 88.62 s
2024-12-12 05:21:55.403811: 
2024-12-12 05:21:55.406222: Epoch 53
2024-12-12 05:21:55.407397: Current learning rate: 0.00952
2024-12-12 05:23:24.018812: Validation loss did not improve from -0.50818. Patience: 11/50
2024-12-12 05:23:24.020291: train_loss -0.6309
2024-12-12 05:23:24.021497: val_loss -0.4658
2024-12-12 05:23:24.022313: Pseudo dice [0.7066]
2024-12-12 05:23:24.023142: Epoch time: 88.62 s
2024-12-12 05:23:24.024053: Yayy! New best EMA pseudo Dice: 0.7054
2024-12-12 05:23:25.463331: 
2024-12-12 05:23:25.465624: Epoch 54
2024-12-12 05:23:25.466712: Current learning rate: 0.00951
2024-12-12 05:24:53.947602: Validation loss did not improve from -0.50818. Patience: 12/50
2024-12-12 05:24:53.948845: train_loss -0.6481
2024-12-12 05:24:53.950061: val_loss -0.4975
2024-12-12 05:24:53.950867: Pseudo dice [0.7173]
2024-12-12 05:24:53.951617: Epoch time: 88.49 s
2024-12-12 05:24:54.293889: Yayy! New best EMA pseudo Dice: 0.7066
2024-12-12 05:24:55.789956: 
2024-12-12 05:24:55.791921: Epoch 55
2024-12-12 05:24:55.793144: Current learning rate: 0.0095
2024-12-12 05:26:24.392778: Validation loss improved from -0.50818 to -0.52871! Patience: 12/50
2024-12-12 05:26:24.394112: train_loss -0.6523
2024-12-12 05:26:24.395436: val_loss -0.5287
2024-12-12 05:26:24.396465: Pseudo dice [0.7326]
2024-12-12 05:26:24.397652: Epoch time: 88.61 s
2024-12-12 05:26:24.398537: Yayy! New best EMA pseudo Dice: 0.7092
2024-12-12 05:26:25.885442: 
2024-12-12 05:26:25.887618: Epoch 56
2024-12-12 05:26:25.888769: Current learning rate: 0.00949
2024-12-12 05:27:54.762347: Validation loss did not improve from -0.52871. Patience: 1/50
2024-12-12 05:27:54.763705: train_loss -0.6471
2024-12-12 05:27:54.765034: val_loss -0.4636
2024-12-12 05:27:54.766159: Pseudo dice [0.7071]
2024-12-12 05:27:54.767310: Epoch time: 88.88 s
2024-12-12 05:27:55.922091: 
2024-12-12 05:27:55.924142: Epoch 57
2024-12-12 05:27:55.925534: Current learning rate: 0.00949
2024-12-12 05:29:24.907813: Validation loss did not improve from -0.52871. Patience: 2/50
2024-12-12 05:29:24.909304: train_loss -0.6458
2024-12-12 05:29:24.910995: val_loss -0.4952
2024-12-12 05:29:24.912118: Pseudo dice [0.7074]
2024-12-12 05:29:24.913342: Epoch time: 88.99 s
2024-12-12 05:29:26.127876: 
2024-12-12 05:29:26.130310: Epoch 58
2024-12-12 05:29:26.131752: Current learning rate: 0.00948
2024-12-12 05:30:54.969188: Validation loss did not improve from -0.52871. Patience: 3/50
2024-12-12 05:30:54.970777: train_loss -0.6527
2024-12-12 05:30:54.971801: val_loss -0.4717
2024-12-12 05:30:54.972620: Pseudo dice [0.7216]
2024-12-12 05:30:54.973245: Epoch time: 88.84 s
2024-12-12 05:30:54.974050: Yayy! New best EMA pseudo Dice: 0.7101
2024-12-12 05:30:56.468977: 
2024-12-12 05:30:56.471030: Epoch 59
2024-12-12 05:30:56.472056: Current learning rate: 0.00947
2024-12-12 05:32:24.951786: Validation loss did not improve from -0.52871. Patience: 4/50
2024-12-12 05:32:24.953156: train_loss -0.6485
2024-12-12 05:32:24.954347: val_loss -0.4533
2024-12-12 05:32:24.955190: Pseudo dice [0.705]
2024-12-12 05:32:24.955960: Epoch time: 88.49 s
2024-12-12 05:32:26.478639: 
2024-12-12 05:32:26.480734: Epoch 60
2024-12-12 05:32:26.481738: Current learning rate: 0.00946
2024-12-12 05:33:54.996817: Validation loss did not improve from -0.52871. Patience: 5/50
2024-12-12 05:33:54.998341: train_loss -0.6537
2024-12-12 05:33:54.999655: val_loss -0.5035
2024-12-12 05:33:55.000392: Pseudo dice [0.7127]
2024-12-12 05:33:55.001055: Epoch time: 88.52 s
2024-12-12 05:33:56.486374: 
2024-12-12 05:33:56.488776: Epoch 61
2024-12-12 05:33:56.489805: Current learning rate: 0.00945
2024-12-12 05:35:25.009005: Validation loss did not improve from -0.52871. Patience: 6/50
2024-12-12 05:35:25.010304: train_loss -0.6436
2024-12-12 05:35:25.011688: val_loss -0.4559
2024-12-12 05:35:25.012692: Pseudo dice [0.7059]
2024-12-12 05:35:25.013974: Epoch time: 88.52 s
2024-12-12 05:35:26.211442: 
2024-12-12 05:35:26.213695: Epoch 62
2024-12-12 05:35:26.214786: Current learning rate: 0.00944
2024-12-12 05:36:54.723510: Validation loss did not improve from -0.52871. Patience: 7/50
2024-12-12 05:36:54.725029: train_loss -0.652
2024-12-12 05:36:54.726229: val_loss -0.4681
2024-12-12 05:36:54.727098: Pseudo dice [0.7066]
2024-12-12 05:36:54.728055: Epoch time: 88.51 s
2024-12-12 05:36:55.914395: 
2024-12-12 05:36:55.916393: Epoch 63
2024-12-12 05:36:55.917389: Current learning rate: 0.00943
2024-12-12 05:38:24.321820: Validation loss did not improve from -0.52871. Patience: 8/50
2024-12-12 05:38:24.323217: train_loss -0.6576
2024-12-12 05:38:24.324082: val_loss -0.4591
2024-12-12 05:38:24.325009: Pseudo dice [0.7004]
2024-12-12 05:38:24.325756: Epoch time: 88.41 s
2024-12-12 05:38:25.489479: 
2024-12-12 05:38:25.491460: Epoch 64
2024-12-12 05:38:25.492601: Current learning rate: 0.00942
2024-12-12 05:39:53.753097: Validation loss did not improve from -0.52871. Patience: 9/50
2024-12-12 05:39:53.754488: train_loss -0.6605
2024-12-12 05:39:53.756095: val_loss -0.5053
2024-12-12 05:39:53.757201: Pseudo dice [0.7259]
2024-12-12 05:39:53.758215: Epoch time: 88.27 s
2024-12-12 05:39:55.296875: 
2024-12-12 05:39:55.299738: Epoch 65
2024-12-12 05:39:55.301519: Current learning rate: 0.00941
2024-12-12 05:41:23.630600: Validation loss did not improve from -0.52871. Patience: 10/50
2024-12-12 05:41:23.631979: train_loss -0.663
2024-12-12 05:41:23.633325: val_loss -0.5011
2024-12-12 05:41:23.634339: Pseudo dice [0.7199]
2024-12-12 05:41:23.635386: Epoch time: 88.34 s
2024-12-12 05:41:23.636333: Yayy! New best EMA pseudo Dice: 0.7111
2024-12-12 05:41:25.118385: 
2024-12-12 05:41:25.120156: Epoch 66
2024-12-12 05:41:25.121242: Current learning rate: 0.0094
2024-12-12 05:42:53.379169: Validation loss did not improve from -0.52871. Patience: 11/50
2024-12-12 05:42:53.380545: train_loss -0.6701
2024-12-12 05:42:53.381937: val_loss -0.4845
2024-12-12 05:42:53.382927: Pseudo dice [0.7121]
2024-12-12 05:42:53.383879: Epoch time: 88.26 s
2024-12-12 05:42:53.384564: Yayy! New best EMA pseudo Dice: 0.7112
2024-12-12 05:42:54.913604: 
2024-12-12 05:42:54.915320: Epoch 67
2024-12-12 05:42:54.916353: Current learning rate: 0.00939
2024-12-12 05:44:23.400801: Validation loss did not improve from -0.52871. Patience: 12/50
2024-12-12 05:44:23.401963: train_loss -0.6561
2024-12-12 05:44:23.403644: val_loss -0.4735
2024-12-12 05:44:23.404718: Pseudo dice [0.7146]
2024-12-12 05:44:23.406045: Epoch time: 88.49 s
2024-12-12 05:44:23.407065: Yayy! New best EMA pseudo Dice: 0.7115
2024-12-12 05:44:24.923113: 
2024-12-12 05:44:24.925760: Epoch 68
2024-12-12 05:44:24.927480: Current learning rate: 0.00939
2024-12-12 05:45:53.644212: Validation loss did not improve from -0.52871. Patience: 13/50
2024-12-12 05:45:53.645808: train_loss -0.6612
2024-12-12 05:45:53.646800: val_loss -0.4843
2024-12-12 05:45:53.647427: Pseudo dice [0.7102]
2024-12-12 05:45:53.648059: Epoch time: 88.72 s
2024-12-12 05:45:54.831830: 
2024-12-12 05:45:54.834123: Epoch 69
2024-12-12 05:45:54.834906: Current learning rate: 0.00938
2024-12-12 05:47:23.406189: Validation loss did not improve from -0.52871. Patience: 14/50
2024-12-12 05:47:23.407524: train_loss -0.6525
2024-12-12 05:47:23.408841: val_loss -0.527
2024-12-12 05:47:23.409766: Pseudo dice [0.7329]
2024-12-12 05:47:23.410506: Epoch time: 88.58 s
2024-12-12 05:47:23.752578: Yayy! New best EMA pseudo Dice: 0.7135
2024-12-12 05:47:25.228074: 
2024-12-12 05:47:25.230318: Epoch 70
2024-12-12 05:47:25.231537: Current learning rate: 0.00937
2024-12-12 05:48:53.565171: Validation loss did not improve from -0.52871. Patience: 15/50
2024-12-12 05:48:53.566726: train_loss -0.6614
2024-12-12 05:48:53.568292: val_loss -0.5026
2024-12-12 05:48:53.569338: Pseudo dice [0.7251]
2024-12-12 05:48:53.570358: Epoch time: 88.34 s
2024-12-12 05:48:53.571369: Yayy! New best EMA pseudo Dice: 0.7147
2024-12-12 05:48:55.369565: 
2024-12-12 05:48:55.371521: Epoch 71
2024-12-12 05:48:55.372730: Current learning rate: 0.00936
2024-12-12 05:50:23.752332: Validation loss did not improve from -0.52871. Patience: 16/50
2024-12-12 05:50:23.754068: train_loss -0.6795
2024-12-12 05:50:23.756031: val_loss -0.4605
2024-12-12 05:50:23.757077: Pseudo dice [0.7083]
2024-12-12 05:50:23.757857: Epoch time: 88.39 s
2024-12-12 05:50:24.963723: 
2024-12-12 05:50:24.965830: Epoch 72
2024-12-12 05:50:24.966712: Current learning rate: 0.00935
2024-12-12 05:51:53.338003: Validation loss did not improve from -0.52871. Patience: 17/50
2024-12-12 05:51:53.339349: train_loss -0.6743
2024-12-12 05:51:53.340477: val_loss -0.4877
2024-12-12 05:51:53.341450: Pseudo dice [0.7178]
2024-12-12 05:51:53.342137: Epoch time: 88.38 s
2024-12-12 05:51:54.554812: 
2024-12-12 05:51:54.556502: Epoch 73
2024-12-12 05:51:54.557553: Current learning rate: 0.00934
2024-12-12 05:53:22.936765: Validation loss did not improve from -0.52871. Patience: 18/50
2024-12-12 05:53:22.938176: train_loss -0.6744
2024-12-12 05:53:22.939400: val_loss -0.5045
2024-12-12 05:53:22.940380: Pseudo dice [0.7223]
2024-12-12 05:53:22.941404: Epoch time: 88.38 s
2024-12-12 05:53:22.942342: Yayy! New best EMA pseudo Dice: 0.7152
2024-12-12 05:53:24.466280: 
2024-12-12 05:53:24.468196: Epoch 74
2024-12-12 05:53:24.469670: Current learning rate: 0.00933
2024-12-12 05:54:52.853152: Validation loss did not improve from -0.52871. Patience: 19/50
2024-12-12 05:54:52.854816: train_loss -0.6716
2024-12-12 05:54:52.856609: val_loss -0.4822
2024-12-12 05:54:52.857429: Pseudo dice [0.7146]
2024-12-12 05:54:52.858210: Epoch time: 88.39 s
2024-12-12 05:54:54.375261: 
2024-12-12 05:54:54.377359: Epoch 75
2024-12-12 05:54:54.378536: Current learning rate: 0.00932
2024-12-12 05:56:22.781118: Validation loss did not improve from -0.52871. Patience: 20/50
2024-12-12 05:56:22.782287: train_loss -0.6704
2024-12-12 05:56:22.783648: val_loss -0.481
2024-12-12 05:56:22.784591: Pseudo dice [0.7098]
2024-12-12 05:56:22.785410: Epoch time: 88.41 s
2024-12-12 05:56:24.004074: 
2024-12-12 05:56:24.005882: Epoch 76
2024-12-12 05:56:24.007189: Current learning rate: 0.00931
2024-12-12 05:57:52.389360: Validation loss did not improve from -0.52871. Patience: 21/50
2024-12-12 05:57:52.390776: train_loss -0.6788
2024-12-12 05:57:52.391865: val_loss -0.4939
2024-12-12 05:57:52.392544: Pseudo dice [0.72]
2024-12-12 05:57:52.393281: Epoch time: 88.39 s
2024-12-12 05:57:53.590060: 
2024-12-12 05:57:53.591923: Epoch 77
2024-12-12 05:57:53.593364: Current learning rate: 0.0093
2024-12-12 05:59:22.131548: Validation loss did not improve from -0.52871. Patience: 22/50
2024-12-12 05:59:22.132828: train_loss -0.6731
2024-12-12 05:59:22.134068: val_loss -0.517
2024-12-12 05:59:22.134849: Pseudo dice [0.7387]
2024-12-12 05:59:22.135589: Epoch time: 88.54 s
2024-12-12 05:59:22.136417: Yayy! New best EMA pseudo Dice: 0.7175
2024-12-12 05:59:23.679927: 
2024-12-12 05:59:23.681365: Epoch 78
2024-12-12 05:59:23.682476: Current learning rate: 0.0093
2024-12-12 06:00:52.473493: Validation loss did not improve from -0.52871. Patience: 23/50
2024-12-12 06:00:52.474830: train_loss -0.6849
2024-12-12 06:00:52.475997: val_loss -0.5027
2024-12-12 06:00:52.476975: Pseudo dice [0.7181]
2024-12-12 06:00:52.477916: Epoch time: 88.8 s
2024-12-12 06:00:52.478712: Yayy! New best EMA pseudo Dice: 0.7176
2024-12-12 06:00:54.024642: 
2024-12-12 06:00:54.026567: Epoch 79
2024-12-12 06:00:54.027982: Current learning rate: 0.00929
2024-12-12 06:02:22.777862: Validation loss did not improve from -0.52871. Patience: 24/50
2024-12-12 06:02:22.779135: train_loss -0.6758
2024-12-12 06:02:22.780561: val_loss -0.4736
2024-12-12 06:02:22.781464: Pseudo dice [0.7004]
2024-12-12 06:02:22.782106: Epoch time: 88.76 s
2024-12-12 06:02:24.340140: 
2024-12-12 06:02:24.342178: Epoch 80
2024-12-12 06:02:24.343331: Current learning rate: 0.00928
2024-12-12 06:03:53.192055: Validation loss did not improve from -0.52871. Patience: 25/50
2024-12-12 06:03:53.193478: train_loss -0.6791
2024-12-12 06:03:53.194796: val_loss -0.5006
2024-12-12 06:03:53.195699: Pseudo dice [0.7149]
2024-12-12 06:03:53.196553: Epoch time: 88.85 s
2024-12-12 06:03:54.749601: 
2024-12-12 06:03:54.751711: Epoch 81
2024-12-12 06:03:54.753057: Current learning rate: 0.00927
2024-12-12 06:05:23.578167: Validation loss did not improve from -0.52871. Patience: 26/50
2024-12-12 06:05:23.579516: train_loss -0.691
2024-12-12 06:05:23.580608: val_loss -0.5226
2024-12-12 06:05:23.581472: Pseudo dice [0.7329]
2024-12-12 06:05:23.582408: Epoch time: 88.83 s
2024-12-12 06:05:24.832641: 
2024-12-12 06:05:24.834888: Epoch 82
2024-12-12 06:05:24.836257: Current learning rate: 0.00926
2024-12-12 06:06:53.537555: Validation loss did not improve from -0.52871. Patience: 27/50
2024-12-12 06:06:53.539539: train_loss -0.6824
2024-12-12 06:06:53.540593: val_loss -0.5162
2024-12-12 06:06:53.541318: Pseudo dice [0.7277]
2024-12-12 06:06:53.542035: Epoch time: 88.71 s
2024-12-12 06:06:53.542774: Yayy! New best EMA pseudo Dice: 0.7185
2024-12-12 06:06:55.053062: 
2024-12-12 06:06:55.056106: Epoch 83
2024-12-12 06:06:55.057572: Current learning rate: 0.00925
2024-12-12 06:08:24.156920: Validation loss did not improve from -0.52871. Patience: 28/50
2024-12-12 06:08:24.163291: train_loss -0.6881
2024-12-12 06:08:24.165101: val_loss -0.4927
2024-12-12 06:08:24.166343: Pseudo dice [0.7121]
2024-12-12 06:08:24.167847: Epoch time: 89.11 s
2024-12-12 06:08:25.390774: 
2024-12-12 06:08:25.392816: Epoch 84
2024-12-12 06:08:25.393942: Current learning rate: 0.00924
2024-12-12 06:09:53.816942: Validation loss did not improve from -0.52871. Patience: 29/50
2024-12-12 06:09:53.818364: train_loss -0.6969
2024-12-12 06:09:53.819744: val_loss -0.482
2024-12-12 06:09:53.820628: Pseudo dice [0.725]
2024-12-12 06:09:53.821815: Epoch time: 88.43 s
2024-12-12 06:09:54.192283: Yayy! New best EMA pseudo Dice: 0.7186
2024-12-12 06:09:55.748183: 
2024-12-12 06:09:55.750504: Epoch 85
2024-12-12 06:09:55.751624: Current learning rate: 0.00923
2024-12-12 06:11:24.237686: Validation loss did not improve from -0.52871. Patience: 30/50
2024-12-12 06:11:24.239430: train_loss -0.692
2024-12-12 06:11:24.241271: val_loss -0.488
2024-12-12 06:11:24.242071: Pseudo dice [0.7166]
2024-12-12 06:11:24.243053: Epoch time: 88.49 s
2024-12-12 06:11:25.437917: 
2024-12-12 06:11:25.440290: Epoch 86
2024-12-12 06:11:25.441719: Current learning rate: 0.00922
2024-12-12 06:12:54.079547: Validation loss did not improve from -0.52871. Patience: 31/50
2024-12-12 06:12:54.081533: train_loss -0.6906
2024-12-12 06:12:54.084846: val_loss -0.5062
2024-12-12 06:12:54.085990: Pseudo dice [0.7235]
2024-12-12 06:12:54.087415: Epoch time: 88.64 s
2024-12-12 06:12:54.088095: Yayy! New best EMA pseudo Dice: 0.7189
2024-12-12 06:12:55.577878: 
2024-12-12 06:12:55.579946: Epoch 87
2024-12-12 06:12:55.580839: Current learning rate: 0.00921
2024-12-12 06:14:24.203695: Validation loss did not improve from -0.52871. Patience: 32/50
2024-12-12 06:14:24.205097: train_loss -0.698
2024-12-12 06:14:24.206110: val_loss -0.5139
2024-12-12 06:14:24.207081: Pseudo dice [0.7311]
2024-12-12 06:14:24.207997: Epoch time: 88.63 s
2024-12-12 06:14:24.208778: Yayy! New best EMA pseudo Dice: 0.7201
2024-12-12 06:14:25.677015: 
2024-12-12 06:14:25.678976: Epoch 88
2024-12-12 06:14:25.680166: Current learning rate: 0.0092
2024-12-12 06:15:54.310769: Validation loss did not improve from -0.52871. Patience: 33/50
2024-12-12 06:15:54.312396: train_loss -0.6926
2024-12-12 06:15:54.314152: val_loss -0.4823
2024-12-12 06:15:54.315057: Pseudo dice [0.7115]
2024-12-12 06:15:54.316097: Epoch time: 88.64 s
2024-12-12 06:15:55.479816: 
2024-12-12 06:15:55.481934: Epoch 89
2024-12-12 06:15:55.482911: Current learning rate: 0.0092
2024-12-12 06:17:24.196696: Validation loss did not improve from -0.52871. Patience: 34/50
2024-12-12 06:17:24.197990: train_loss -0.6923
2024-12-12 06:17:24.199325: val_loss -0.5172
2024-12-12 06:17:24.200069: Pseudo dice [0.729]
2024-12-12 06:17:24.201087: Epoch time: 88.72 s
2024-12-12 06:17:24.537374: Yayy! New best EMA pseudo Dice: 0.7202
2024-12-12 06:17:26.042332: 
2024-12-12 06:17:26.044785: Epoch 90
2024-12-12 06:17:26.045859: Current learning rate: 0.00919
2024-12-12 06:18:55.076438: Validation loss did not improve from -0.52871. Patience: 35/50
2024-12-12 06:18:55.077783: train_loss -0.6897
2024-12-12 06:18:55.078782: val_loss -0.4807
2024-12-12 06:18:55.080012: Pseudo dice [0.718]
2024-12-12 06:18:55.081133: Epoch time: 89.04 s
2024-12-12 06:18:56.243776: 
2024-12-12 06:18:56.245084: Epoch 91
2024-12-12 06:18:56.246168: Current learning rate: 0.00918
2024-12-12 06:20:25.336299: Validation loss did not improve from -0.52871. Patience: 36/50
2024-12-12 06:20:25.337495: train_loss -0.6937
2024-12-12 06:20:25.338482: val_loss -0.49
2024-12-12 06:20:25.339280: Pseudo dice [0.7202]
2024-12-12 06:20:25.340305: Epoch time: 89.09 s
2024-12-12 06:20:26.851334: 
2024-12-12 06:20:26.853590: Epoch 92
2024-12-12 06:20:26.854646: Current learning rate: 0.00917
2024-12-12 06:21:56.094376: Validation loss did not improve from -0.52871. Patience: 37/50
2024-12-12 06:21:56.095778: train_loss -0.6816
2024-12-12 06:21:56.096922: val_loss -0.4804
2024-12-12 06:21:56.097589: Pseudo dice [0.7062]
2024-12-12 06:21:56.098314: Epoch time: 89.25 s
2024-12-12 06:21:57.243663: 
2024-12-12 06:21:57.245950: Epoch 93
2024-12-12 06:21:57.247051: Current learning rate: 0.00916
2024-12-12 06:23:26.336189: Validation loss did not improve from -0.52871. Patience: 38/50
2024-12-12 06:23:26.337744: train_loss -0.6812
2024-12-12 06:23:26.339216: val_loss -0.4985
2024-12-12 06:23:26.340259: Pseudo dice [0.7241]
2024-12-12 06:23:26.341373: Epoch time: 89.1 s
2024-12-12 06:23:27.501362: 
2024-12-12 06:23:27.503345: Epoch 94
2024-12-12 06:23:27.504341: Current learning rate: 0.00915
2024-12-12 06:24:56.567925: Validation loss did not improve from -0.52871. Patience: 39/50
2024-12-12 06:24:56.569304: train_loss -0.6979
2024-12-12 06:24:56.570568: val_loss -0.5009
2024-12-12 06:24:56.571444: Pseudo dice [0.717]
2024-12-12 06:24:56.572269: Epoch time: 89.07 s
2024-12-12 06:24:58.036765: 
2024-12-12 06:24:58.038882: Epoch 95
2024-12-12 06:24:58.040115: Current learning rate: 0.00914
2024-12-12 06:26:27.002024: Validation loss did not improve from -0.52871. Patience: 40/50
2024-12-12 06:26:27.003089: train_loss -0.7018
2024-12-12 06:26:27.004320: val_loss -0.5138
2024-12-12 06:26:27.005099: Pseudo dice [0.7307]
2024-12-12 06:26:27.005721: Epoch time: 88.97 s
2024-12-12 06:26:28.151035: 
2024-12-12 06:26:28.153142: Epoch 96
2024-12-12 06:26:28.154163: Current learning rate: 0.00913
2024-12-12 06:27:57.113686: Validation loss did not improve from -0.52871. Patience: 41/50
2024-12-12 06:27:57.115052: train_loss -0.7041
2024-12-12 06:27:57.116399: val_loss -0.4423
2024-12-12 06:27:57.117270: Pseudo dice [0.6992]
2024-12-12 06:27:57.118165: Epoch time: 88.97 s
2024-12-12 06:27:58.284359: 
2024-12-12 06:27:58.287614: Epoch 97
2024-12-12 06:27:58.289662: Current learning rate: 0.00912
2024-12-12 06:29:27.401060: Validation loss did not improve from -0.52871. Patience: 42/50
2024-12-12 06:29:27.402111: train_loss -0.7017
2024-12-12 06:29:27.403365: val_loss -0.5247
2024-12-12 06:29:27.404345: Pseudo dice [0.7307]
2024-12-12 06:29:27.405252: Epoch time: 89.12 s
2024-12-12 06:29:28.574954: 
2024-12-12 06:29:28.577455: Epoch 98
2024-12-12 06:29:28.578932: Current learning rate: 0.00911
2024-12-12 06:30:57.813629: Validation loss did not improve from -0.52871. Patience: 43/50
2024-12-12 06:30:57.814765: train_loss -0.7093
2024-12-12 06:30:57.815748: val_loss -0.5068
2024-12-12 06:30:57.816586: Pseudo dice [0.7213]
2024-12-12 06:30:57.817394: Epoch time: 89.24 s
2024-12-12 06:30:58.999331: 
2024-12-12 06:30:59.001214: Epoch 99
2024-12-12 06:30:59.001998: Current learning rate: 0.0091
2024-12-12 06:32:28.265997: Validation loss did not improve from -0.52871. Patience: 44/50
2024-12-12 06:32:28.267279: train_loss -0.7031
2024-12-12 06:32:28.268295: val_loss -0.4965
2024-12-12 06:32:28.269171: Pseudo dice [0.7163]
2024-12-12 06:32:28.269995: Epoch time: 89.27 s
2024-12-12 06:32:29.766952: 
2024-12-12 06:32:29.768933: Epoch 100
2024-12-12 06:32:29.770083: Current learning rate: 0.0091
2024-12-12 06:33:59.106337: Validation loss did not improve from -0.52871. Patience: 45/50
2024-12-12 06:33:59.107503: train_loss -0.7055
2024-12-12 06:33:59.108512: val_loss -0.5102
2024-12-12 06:33:59.109273: Pseudo dice [0.7149]
2024-12-12 06:33:59.110138: Epoch time: 89.34 s
2024-12-12 06:34:00.305521: 
2024-12-12 06:34:00.307335: Epoch 101
2024-12-12 06:34:00.308300: Current learning rate: 0.00909
2024-12-12 06:35:29.367042: Validation loss did not improve from -0.52871. Patience: 46/50
2024-12-12 06:35:29.368336: train_loss -0.7032
2024-12-12 06:35:29.369200: val_loss -0.4725
2024-12-12 06:35:29.369933: Pseudo dice [0.7171]
2024-12-12 06:35:29.370679: Epoch time: 89.06 s
2024-12-12 06:35:30.550619: 
2024-12-12 06:35:30.552629: Epoch 102
2024-12-12 06:35:30.553883: Current learning rate: 0.00908
2024-12-12 06:36:59.527335: Validation loss did not improve from -0.52871. Patience: 47/50
2024-12-12 06:36:59.528618: train_loss -0.7028
2024-12-12 06:36:59.529902: val_loss -0.498
2024-12-12 06:36:59.531101: Pseudo dice [0.7289]
2024-12-12 06:36:59.532030: Epoch time: 88.98 s
2024-12-12 06:37:01.056649: 
2024-12-12 06:37:01.058659: Epoch 103
2024-12-12 06:37:01.059793: Current learning rate: 0.00907
2024-12-12 06:38:30.146130: Validation loss did not improve from -0.52871. Patience: 48/50
2024-12-12 06:38:30.147442: train_loss -0.7018
2024-12-12 06:38:30.148401: val_loss -0.5055
2024-12-12 06:38:30.149533: Pseudo dice [0.7282]
2024-12-12 06:38:30.150544: Epoch time: 89.09 s
2024-12-12 06:38:30.151413: Yayy! New best EMA pseudo Dice: 0.7205
2024-12-12 06:38:31.629084: 
2024-12-12 06:38:31.631366: Epoch 104
2024-12-12 06:38:31.632399: Current learning rate: 0.00906
2024-12-12 06:40:00.591959: Validation loss did not improve from -0.52871. Patience: 49/50
2024-12-12 06:40:00.593100: train_loss -0.7076
2024-12-12 06:40:00.593998: val_loss -0.5281
2024-12-12 06:40:00.594982: Pseudo dice [0.735]
2024-12-12 06:40:00.596215: Epoch time: 88.96 s
2024-12-12 06:40:00.936897: Yayy! New best EMA pseudo Dice: 0.7219
2024-12-12 06:40:02.427495: 
2024-12-12 06:40:02.429604: Epoch 105
2024-12-12 06:40:02.430649: Current learning rate: 0.00905
2024-12-12 06:41:31.334481: Validation loss did not improve from -0.52871. Patience: 50/50
2024-12-12 06:41:31.335777: train_loss -0.7117
2024-12-12 06:41:31.337298: val_loss -0.5114
2024-12-12 06:41:31.338325: Pseudo dice [0.7287]
2024-12-12 06:41:31.339293: Epoch time: 88.91 s
2024-12-12 06:41:31.340239: Yayy! New best EMA pseudo Dice: 0.7226
2024-12-12 06:41:32.854427: Patience reached. Stopping training.
2024-12-12 06:41:33.248043: Training done.
2024-12-12 06:41:33.472291: Using splits from existing split file: /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset307_Sohee_Calcium_OCT_CrossValidation/splits_final.json
2024-12-12 06:41:33.500635: The split file contains 5 splits.
2024-12-12 06:41:33.502319: Desired fold for training: 4
2024-12-12 06:41:33.503705: This split has 7 training and 1 validation cases.
2024-12-12 06:41:33.505162: predicting 101-045
2024-12-12 06:41:33.555035: 101-045, shape torch.Size([1, 375, 498, 498]), rank 0
2024-12-12 06:44:51.518444: Validation complete
2024-12-12 06:44:51.519884: Mean Validation Dice:  0.7268788866995375
