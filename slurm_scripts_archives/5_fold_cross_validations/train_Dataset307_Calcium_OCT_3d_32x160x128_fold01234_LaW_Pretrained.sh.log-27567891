
CommandNotFoundError: Your shell has not been properly configured to use 'conda activate'.
To initialize your shell, run

    $ conda init <SHELL_NAME>

Currently supported shells are:
  - bash
  - fish
  - tcsh
  - xonsh
  - zsh
  - powershell

See 'conda init --help' for more information and options.

IMPORTANT: You may need to close and restart your shell after running 'conda init'.


/home/gridsan/nchutisilp/.conda/envs/nnunet/bin/python
Starting training with CONFIG=3d_32x160x128_b10, DATASET_ID=308, TRAINER=nnUNetTrainer

############################
INFO: You are using the old nnU-Net default plans. We have updated our recommendations. Please consider using those instead! Read more here: https://github.com/MIC-DKFZ/nnUNet/blob/master/documentation/resenc_presets.md
############################

Using device: cuda:0

#######################################################################
Please cite the following paper when using nnU-Net:
Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.
#######################################################################

2024-12-07 03:12:43.871265: Using torch.compile...

############################
INFO: You are using the old nnU-Net default plans. We have updated our recommendations. Please consider using those instead! Read more here: https://github.com/MIC-DKFZ/nnUNet/blob/master/documentation/resenc_presets.md
############################

Using device: cuda:0

#######################################################################
Please cite the following paper when using nnU-Net:
Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.
#######################################################################

2024-12-07 03:12:43.871821: Using torch.compile...
/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
################### Loading pretrained weights from file  /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_results/Dataset300_Lumen_and_Wall_OCT/nnUNetTrainer__nnUNetPreprocessPlans__3d_fullres/fold_all/checkpoint_best.pth ###################
Below is the list of overlapping blocks in pretrained model and nnUNet architecture:
encoder.stages.0.0.convs.0.conv.weight shape torch.Size([32, 1, 3, 3, 3])
encoder.stages.0.0.convs.0.conv.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.norm.weight shape torch.Size([32])
encoder.stages.0.0.convs.0.norm.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.0.weight shape torch.Size([32, 1, 3, 3, 3])
encoder.stages.0.0.convs.0.all_modules.0.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.1.weight shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.1.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
encoder.stages.0.0.convs.1.conv.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.norm.weight shape torch.Size([32])
encoder.stages.0.0.convs.1.norm.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
encoder.stages.0.0.convs.1.all_modules.0.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.1.weight shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.1.bias shape torch.Size([32])
encoder.stages.1.0.convs.0.conv.weight shape torch.Size([64, 32, 3, 3, 3])
encoder.stages.1.0.convs.0.conv.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.norm.weight shape torch.Size([64])
encoder.stages.1.0.convs.0.norm.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.0.weight shape torch.Size([64, 32, 3, 3, 3])
encoder.stages.1.0.convs.0.all_modules.0.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.1.weight shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.1.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
encoder.stages.1.0.convs.1.conv.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.norm.weight shape torch.Size([64])
encoder.stages.1.0.convs.1.norm.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
encoder.stages.1.0.convs.1.all_modules.0.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.1.weight shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.1.bias shape torch.Size([64])
encoder.stages.2.0.convs.0.conv.weight shape torch.Size([128, 64, 3, 3, 3])
encoder.stages.2.0.convs.0.conv.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.norm.weight shape torch.Size([128])
encoder.stages.2.0.convs.0.norm.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.0.weight shape torch.Size([128, 64, 3, 3, 3])
encoder.stages.2.0.convs.0.all_modules.0.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.1.weight shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.1.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
encoder.stages.2.0.convs.1.conv.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.norm.weight shape torch.Size([128])
encoder.stages.2.0.convs.1.norm.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
encoder.stages.2.0.convs.1.all_modules.0.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.1.weight shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.1.bias shape torch.Size([128])
encoder.stages.3.0.convs.0.conv.weight shape torch.Size([256, 128, 3, 3, 3])
encoder.stages.3.0.convs.0.conv.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.norm.weight shape torch.Size([256])
encoder.stages.3.0.convs.0.norm.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.0.weight shape torch.Size([256, 128, 3, 3, 3])
encoder.stages.3.0.convs.0.all_modules.0.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.1.weight shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.1.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
encoder.stages.3.0.convs.1.conv.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.norm.weight shape torch.Size([256])
encoder.stages.3.0.convs.1.norm.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
encoder.stages.3.0.convs.1.all_modules.0.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.1.weight shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.1.bias shape torch.Size([256])
encoder.stages.4.0.convs.0.conv.weight shape torch.Size([320, 256, 3, 3, 3])
encoder.stages.4.0.convs.0.conv.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.norm.weight shape torch.Size([320])
encoder.stages.4.0.convs.0.norm.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.0.weight shape torch.Size([320, 256, 3, 3, 3])
encoder.stages.4.0.convs.0.all_modules.0.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.1.weight shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.1.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.4.0.convs.1.conv.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.norm.weight shape torch.Size([320])
encoder.stages.4.0.convs.1.norm.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.4.0.convs.1.all_modules.0.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.1.weight shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.1.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.0.conv.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.norm.weight shape torch.Size([320])
encoder.stages.5.0.convs.0.norm.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.0.all_modules.0.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.1.weight shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.1.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.1.conv.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.norm.weight shape torch.Size([320])
encoder.stages.5.0.convs.1.norm.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.1.all_modules.0.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.1.weight shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.0.0.convs.0.conv.weight shape torch.Size([32, 1, 3, 3, 3])
decoder.encoder.stages.0.0.convs.0.conv.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.norm.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.norm.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.0.weight shape torch.Size([32, 1, 3, 3, 3])
decoder.encoder.stages.0.0.convs.0.all_modules.0.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.1.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.1.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.encoder.stages.0.0.convs.1.conv.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.norm.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.norm.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.encoder.stages.0.0.convs.1.all_modules.0.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.1.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.1.bias shape torch.Size([32])
decoder.encoder.stages.1.0.convs.0.conv.weight shape torch.Size([64, 32, 3, 3, 3])
decoder.encoder.stages.1.0.convs.0.conv.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.norm.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.norm.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.0.weight shape torch.Size([64, 32, 3, 3, 3])
decoder.encoder.stages.1.0.convs.0.all_modules.0.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.1.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.1.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.encoder.stages.1.0.convs.1.conv.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.norm.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.norm.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.encoder.stages.1.0.convs.1.all_modules.0.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.1.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.1.bias shape torch.Size([64])
decoder.encoder.stages.2.0.convs.0.conv.weight shape torch.Size([128, 64, 3, 3, 3])
decoder.encoder.stages.2.0.convs.0.conv.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.norm.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.norm.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.0.weight shape torch.Size([128, 64, 3, 3, 3])
decoder.encoder.stages.2.0.convs.0.all_modules.0.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.1.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.1.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.encoder.stages.2.0.convs.1.conv.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.norm.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.norm.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.encoder.stages.2.0.convs.1.all_modules.0.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.1.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.1.bias shape torch.Size([128])
decoder.encoder.stages.3.0.convs.0.conv.weight shape torch.Size([256, 128, 3, 3, 3])
decoder.encoder.stages.3.0.convs.0.conv.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.norm.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.norm.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.0.weight shape torch.Size([256, 128, 3, 3, 3])
decoder.encoder.stages.3.0.convs.0.all_modules.0.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.1.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.1.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.encoder.stages.3.0.convs.1.conv.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.norm.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.norm.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.encoder.stages.3.0.convs.1.all_modules.0.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.1.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.1.bias shape torch.Size([256])
decoder.encoder.stages.4.0.convs.0.conv.weight shape torch.Size([320, 256, 3, 3, 3])
decoder.encoder.stages.4.0.convs.0.conv.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.norm.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.norm.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.0.weight shape torch.Size([320, 256, 3, 3, 3])
decoder.encoder.stages.4.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.4.0.convs.1.conv.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.norm.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.norm.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.4.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.0.conv.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.norm.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.norm.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.1.conv.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.norm.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.norm.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.stages.0.convs.0.conv.weight shape torch.Size([320, 640, 3, 3, 3])
decoder.stages.0.convs.0.conv.bias shape torch.Size([320])
decoder.stages.0.convs.0.norm.weight shape torch.Size([320])
decoder.stages.0.convs.0.norm.bias shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.0.weight shape torch.Size([320, 640, 3, 3, 3])
decoder.stages.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.stages.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.stages.0.convs.1.conv.bias shape torch.Size([320])
decoder.stages.0.convs.1.norm.weight shape torch.Size([320])
decoder.stages.0.convs.1.norm.bias shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.stages.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.stages.1.convs.0.conv.weight shape torch.Size([256, 512, 3, 3, 3])
decoder.stages.1.convs.0.conv.bias shape torch.Size([256])
decoder.stages.1.convs.0.norm.weight shape torch.Size([256])
decoder.stages.1.convs.0.norm.bias shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.0.weight shape torch.Size([256, 512, 3, 3, 3])
decoder.stages.1.convs.0.all_modules.0.bias shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.1.weight shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.1.bias shape torch.Size([256])
decoder.stages.1.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.stages.1.convs.1.conv.bias shape torch.Size([256])
decoder.stages.1.convs.1.norm.weight shape torch.Size([256])
decoder.stages.1.convs.1.norm.bias shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.stages.1.convs.1.all_modules.0.bias shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.1.weight shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.1.bias shape torch.Size([256])
decoder.stages.2.convs.0.conv.weight shape torch.Size([128, 256, 3, 3, 3])
decoder.stages.2.convs.0.conv.bias shape torch.Size([128])
decoder.stages.2.convs.0.norm.weight shape torch.Size([128])
decoder.stages.2.convs.0.norm.bias shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.0.weight shape torch.Size([128, 256, 3, 3, 3])
decoder.stages.2.convs.0.all_modules.0.bias shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.1.weight shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.1.bias shape torch.Size([128])
decoder.stages.2.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.stages.2.convs.1.conv.bias shape torch.Size([128])
decoder.stages.2.convs.1.norm.weight shape torch.Size([128])
decoder.stages.2.convs.1.norm.bias shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.stages.2.convs.1.all_modules.0.bias shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.1.weight shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.1.bias shape torch.Size([128])
decoder.stages.3.convs.0.conv.weight shape torch.Size([64, 128, 3, 3, 3])
decoder.stages.3.convs.0.conv.bias shape torch.Size([64])
decoder.stages.3.convs.0.norm.weight shape torch.Size([64])
decoder.stages.3.convs.0.norm.bias shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.0.weight shape torch.Size([64, 128, 3, 3, 3])
decoder.stages.3.convs.0.all_modules.0.bias shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.1.weight shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.1.bias shape torch.Size([64])
decoder.stages.3.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.stages.3.convs.1.conv.bias shape torch.Size([64])
decoder.stages.3.convs.1.norm.weight shape torch.Size([64])
decoder.stages.3.convs.1.norm.bias shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.stages.3.convs.1.all_modules.0.bias shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.1.weight shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.1.bias shape torch.Size([64])
decoder.stages.4.convs.0.conv.weight shape torch.Size([32, 64, 3, 3, 3])
decoder.stages.4.convs.0.conv.bias shape torch.Size([32])
decoder.stages.4.convs.0.norm.weight shape torch.Size([32])
decoder.stages.4.convs.0.norm.bias shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.0.weight shape torch.Size([32, 64, 3, 3, 3])
decoder.stages.4.convs.0.all_modules.0.bias shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.1.weight shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.1.bias shape torch.Size([32])
decoder.stages.4.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.stages.4.convs.1.conv.bias shape torch.Size([32])
decoder.stages.4.convs.1.norm.weight shape torch.Size([32])
decoder.stages.4.convs.1.norm.bias shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.stages.4.convs.1.all_modules.0.bias shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.1.weight shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.1.bias shape torch.Size([32])
decoder.transpconvs.0.weight shape torch.Size([320, 320, 1, 2, 2])
decoder.transpconvs.0.bias shape torch.Size([320])
decoder.transpconvs.1.weight shape torch.Size([320, 256, 2, 2, 2])
decoder.transpconvs.1.bias shape torch.Size([256])
decoder.transpconvs.2.weight shape torch.Size([256, 128, 2, 2, 2])
decoder.transpconvs.2.bias shape torch.Size([128])
decoder.transpconvs.3.weight shape torch.Size([128, 64, 2, 2, 2])
decoder.transpconvs.3.bias shape torch.Size([64])
decoder.transpconvs.4.weight shape torch.Size([64, 32, 2, 2, 2])
decoder.transpconvs.4.bias shape torch.Size([32])
################### Done ###################
2024-12-07 03:12:55.612396: do_dummy_2d_data_aug: True
2024-12-07 03:12:55.616748: Using splits from existing split file: /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset307_Sohee_Calcium_OCT_CrossValidation/splits_final.json
2024-12-07 03:12:55.639607: The split file contains 5 splits.
2024-12-07 03:12:55.641513: Desired fold for training: 1
2024-12-07 03:12:55.642835: This split has 6 training and 2 validation cases.
################### Loading pretrained weights from file  /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_results/Dataset300_Lumen_and_Wall_OCT/nnUNetTrainer__nnUNetPreprocessPlans__3d_fullres/fold_all/checkpoint_best.pth ###################
Below is the list of overlapping blocks in pretrained model and nnUNet architecture:
encoder.stages.0.0.convs.0.conv.weight shape torch.Size([32, 1, 3, 3, 3])
encoder.stages.0.0.convs.0.conv.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.norm.weight shape torch.Size([32])
encoder.stages.0.0.convs.0.norm.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.0.weight shape torch.Size([32, 1, 3, 3, 3])
encoder.stages.0.0.convs.0.all_modules.0.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.1.weight shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.1.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
encoder.stages.0.0.convs.1.conv.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.norm.weight shape torch.Size([32])
encoder.stages.0.0.convs.1.norm.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
encoder.stages.0.0.convs.1.all_modules.0.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.1.weight shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.1.bias shape torch.Size([32])
encoder.stages.1.0.convs.0.conv.weight shape torch.Size([64, 32, 3, 3, 3])
encoder.stages.1.0.convs.0.conv.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.norm.weight shape torch.Size([64])
encoder.stages.1.0.convs.0.norm.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.0.weight shape torch.Size([64, 32, 3, 3, 3])
encoder.stages.1.0.convs.0.all_modules.0.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.1.weight shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.1.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
encoder.stages.1.0.convs.1.conv.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.norm.weight shape torch.Size([64])
encoder.stages.1.0.convs.1.norm.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
encoder.stages.1.0.convs.1.all_modules.0.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.1.weight shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.1.bias shape torch.Size([64])
encoder.stages.2.0.convs.0.conv.weight shape torch.Size([128, 64, 3, 3, 3])
encoder.stages.2.0.convs.0.conv.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.norm.weight shape torch.Size([128])
encoder.stages.2.0.convs.0.norm.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.0.weight shape torch.Size([128, 64, 3, 3, 3])
encoder.stages.2.0.convs.0.all_modules.0.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.1.weight shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.1.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
encoder.stages.2.0.convs.1.conv.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.norm.weight shape torch.Size([128])
encoder.stages.2.0.convs.1.norm.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
encoder.stages.2.0.convs.1.all_modules.0.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.1.weight shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.1.bias shape torch.Size([128])
encoder.stages.3.0.convs.0.conv.weight shape torch.Size([256, 128, 3, 3, 3])
encoder.stages.3.0.convs.0.conv.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.norm.weight shape torch.Size([256])
encoder.stages.3.0.convs.0.norm.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.0.weight shape torch.Size([256, 128, 3, 3, 3])
encoder.stages.3.0.convs.0.all_modules.0.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.1.weight shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.1.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
encoder.stages.3.0.convs.1.conv.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.norm.weight shape torch.Size([256])
encoder.stages.3.0.convs.1.norm.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
encoder.stages.3.0.convs.1.all_modules.0.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.1.weight shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.1.bias shape torch.Size([256])
encoder.stages.4.0.convs.0.conv.weight shape torch.Size([320, 256, 3, 3, 3])
encoder.stages.4.0.convs.0.conv.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.norm.weight shape torch.Size([320])
encoder.stages.4.0.convs.0.norm.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.0.weight shape torch.Size([320, 256, 3, 3, 3])
encoder.stages.4.0.convs.0.all_modules.0.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.1.weight shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.1.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.4.0.convs.1.conv.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.norm.weight shape torch.Size([320])
encoder.stages.4.0.convs.1.norm.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.4.0.convs.1.all_modules.0.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.1.weight shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.1.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.0.conv.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.norm.weight shape torch.Size([320])
encoder.stages.5.0.convs.0.norm.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.0.all_modules.0.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.1.weight shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.1.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.1.conv.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.norm.weight shape torch.Size([320])
encoder.stages.5.0.convs.1.norm.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.1.all_modules.0.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.1.weight shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.0.0.convs.0.conv.weight shape torch.Size([32, 1, 3, 3, 3])
decoder.encoder.stages.0.0.convs.0.conv.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.norm.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.norm.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.0.weight shape torch.Size([32, 1, 3, 3, 3])
decoder.encoder.stages.0.0.convs.0.all_modules.0.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.1.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.1.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.encoder.stages.0.0.convs.1.conv.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.norm.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.norm.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.encoder.stages.0.0.convs.1.all_modules.0.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.1.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.1.bias shape torch.Size([32])
decoder.encoder.stages.1.0.convs.0.conv.weight shape torch.Size([64, 32, 3, 3, 3])
decoder.encoder.stages.1.0.convs.0.conv.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.norm.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.norm.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.0.weight shape torch.Size([64, 32, 3, 3, 3])
decoder.encoder.stages.1.0.convs.0.all_modules.0.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.1.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.1.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.encoder.stages.1.0.convs.1.conv.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.norm.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.norm.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.encoder.stages.1.0.convs.1.all_modules.0.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.1.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.1.bias shape torch.Size([64])
decoder.encoder.stages.2.0.convs.0.conv.weight shape torch.Size([128, 64, 3, 3, 3])
decoder.encoder.stages.2.0.convs.0.conv.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.norm.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.norm.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.0.weight shape torch.Size([128, 64, 3, 3, 3])
decoder.encoder.stages.2.0.convs.0.all_modules.0.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.1.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.1.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.encoder.stages.2.0.convs.1.conv.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.norm.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.norm.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.encoder.stages.2.0.convs.1.all_modules.0.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.1.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.1.bias shape torch.Size([128])
decoder.encoder.stages.3.0.convs.0.conv.weight shape torch.Size([256, 128, 3, 3, 3])
decoder.encoder.stages.3.0.convs.0.conv.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.norm.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.norm.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.0.weight shape torch.Size([256, 128, 3, 3, 3])
decoder.encoder.stages.3.0.convs.0.all_modules.0.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.1.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.1.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.encoder.stages.3.0.convs.1.conv.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.norm.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.norm.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.encoder.stages.3.0.convs.1.all_modules.0.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.1.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.1.bias shape torch.Size([256])
decoder.encoder.stages.4.0.convs.0.conv.weight shape torch.Size([320, 256, 3, 3, 3])
decoder.encoder.stages.4.0.convs.0.conv.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.norm.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.norm.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.0.weight shape torch.Size([320, 256, 3, 3, 3])
decoder.encoder.stages.4.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.4.0.convs.1.conv.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.norm.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.norm.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.4.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.0.conv.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.norm.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.norm.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.1.conv.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.norm.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.norm.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.stages.0.convs.0.conv.weight shape torch.Size([320, 640, 3, 3, 3])
decoder.stages.0.convs.0.conv.bias shape torch.Size([320])
decoder.stages.0.convs.0.norm.weight shape torch.Size([320])
decoder.stages.0.convs.0.norm.bias shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.0.weight shape torch.Size([320, 640, 3, 3, 3])
decoder.stages.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.stages.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.stages.0.convs.1.conv.bias shape torch.Size([320])
decoder.stages.0.convs.1.norm.weight shape torch.Size([320])
decoder.stages.0.convs.1.norm.bias shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.stages.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.stages.1.convs.0.conv.weight shape torch.Size([256, 512, 3, 3, 3])
decoder.stages.1.convs.0.conv.bias shape torch.Size([256])
decoder.stages.1.convs.0.norm.weight shape torch.Size([256])
decoder.stages.1.convs.0.norm.bias shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.0.weight shape torch.Size([256, 512, 3, 3, 3])
decoder.stages.1.convs.0.all_modules.0.bias shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.1.weight shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.1.bias shape torch.Size([256])
decoder.stages.1.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.stages.1.convs.1.conv.bias shape torch.Size([256])
decoder.stages.1.convs.1.norm.weight shape torch.Size([256])
decoder.stages.1.convs.1.norm.bias shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.stages.1.convs.1.all_modules.0.bias shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.1.weight shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.1.bias shape torch.Size([256])
decoder.stages.2.convs.0.conv.weight shape torch.Size([128, 256, 3, 3, 3])
decoder.stages.2.convs.0.conv.bias shape torch.Size([128])
decoder.stages.2.convs.0.norm.weight shape torch.Size([128])
decoder.stages.2.convs.0.norm.bias shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.0.weight shape torch.Size([128, 256, 3, 3, 3])
decoder.stages.2.convs.0.all_modules.0.bias shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.1.weight shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.1.bias shape torch.Size([128])
decoder.stages.2.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.stages.2.convs.1.conv.bias shape torch.Size([128])
decoder.stages.2.convs.1.norm.weight shape torch.Size([128])
decoder.stages.2.convs.1.norm.bias shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.stages.2.convs.1.all_modules.0.bias shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.1.weight shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.1.bias shape torch.Size([128])
decoder.stages.3.convs.0.conv.weight shape torch.Size([64, 128, 3, 3, 3])
decoder.stages.3.convs.0.conv.bias shape torch.Size([64])
decoder.stages.3.convs.0.norm.weight shape torch.Size([64])
decoder.stages.3.convs.0.norm.bias shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.0.weight shape torch.Size([64, 128, 3, 3, 3])
decoder.stages.3.convs.0.all_modules.0.bias shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.1.weight shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.1.bias shape torch.Size([64])
decoder.stages.3.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.stages.3.convs.1.conv.bias shape torch.Size([64])
decoder.stages.3.convs.1.norm.weight shape torch.Size([64])
decoder.stages.3.convs.1.norm.bias shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.stages.3.convs.1.all_modules.0.bias shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.1.weight shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.1.bias shape torch.Size([64])
decoder.stages.4.convs.0.conv.weight shape torch.Size([32, 64, 3, 3, 3])
decoder.stages.4.convs.0.conv.bias shape torch.Size([32])
decoder.stages.4.convs.0.norm.weight shape torch.Size([32])
decoder.stages.4.convs.0.norm.bias shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.0.weight shape torch.Size([32, 64, 3, 3, 3])
decoder.stages.4.convs.0.all_modules.0.bias shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.1.weight shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.1.bias shape torch.Size([32])
decoder.stages.4.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.stages.4.convs.1.conv.bias shape torch.Size([32])
decoder.stages.4.convs.1.norm.weight shape torch.Size([32])
decoder.stages.4.convs.1.norm.bias shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.stages.4.convs.1.all_modules.0.bias shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.1.weight shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.1.bias shape torch.Size([32])
decoder.transpconvs.0.weight shape torch.Size([320, 320, 1, 2, 2])
decoder.transpconvs.0.bias shape torch.Size([320])
decoder.transpconvs.1.weight shape torch.Size([320, 256, 2, 2, 2])
decoder.transpconvs.1.bias shape torch.Size([256])
decoder.transpconvs.2.weight shape torch.Size([256, 128, 2, 2, 2])
decoder.transpconvs.2.bias shape torch.Size([128])
decoder.transpconvs.3.weight shape torch.Size([128, 64, 2, 2, 2])
decoder.transpconvs.3.bias shape torch.Size([64])
decoder.transpconvs.4.weight shape torch.Size([64, 32, 2, 2, 2])
decoder.transpconvs.4.bias shape torch.Size([32])
################### Done ###################
2024-12-07 03:12:55.612387: do_dummy_2d_data_aug: True
2024-12-07 03:12:55.616724: Using splits from existing split file: /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset307_Sohee_Calcium_OCT_CrossValidation/splits_final.json
2024-12-07 03:12:55.640037: The split file contains 5 splits.
2024-12-07 03:12:55.641633: Desired fold for training: 0
2024-12-07 03:12:55.642842: This split has 6 training and 2 validation cases.
using pin_memory on device 0
using pin_memory on device 0
using pin_memory on device 0

This is the configuration used by this training:
Configuration name: 3d_32x160x128_b10
 {'data_identifier': 'nnUNetPlans_3d_fullres', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 10, 'patch_size': [32, 160, 128], 'median_image_size_in_voxels': [375.0, 498.0, 498.0], 'spacing': [1.0, 1.0, 1.0], 'normalization_schemes': ['ZScoreNormalization'], 'use_mask_for_norm': [False], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'architecture': {'network_class_name': 'dynamic_network_architectures.architectures.unet.PlainConvUNet', 'arch_kwargs': {'n_stages': 6, 'features_per_stage': [32, 64, 128, 256, 320, 320], 'conv_op': 'torch.nn.modules.conv.Conv3d', 'kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]], 'strides': [[1, 1, 1], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [1, 2, 2]], 'n_conv_per_stage': [2, 2, 2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2, 2, 2], 'conv_bias': True, 'norm_op': 'torch.nn.modules.instancenorm.InstanceNorm3d', 'norm_op_kwargs': {'eps': 1e-05, 'affine': True}, 'dropout_op': None, 'dropout_op_kwargs': None, 'nonlin': 'torch.nn.LeakyReLU', 'nonlin_kwargs': {'inplace': True}, 'deep_supervision': True}, '_kw_requires_import': ['conv_op', 'norm_op', 'dropout_op', 'nonlin']}, 'batch_dice': True, 'inherits_from': '3d_fullres'} 

These are the global plan.json settings:
 {'dataset_name': 'Dataset307_Sohee_Calcium_OCT_CrossValidation', 'plans_name': 'nnUNetPlans', 'original_median_spacing_after_transp': [1.0, 1.0, 1.0], 'original_median_shape_after_transp': [375, 498, 498], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 0.4977976381778717, 'mean': 0.13507525622844696, 'median': 0.09599608182907104, 'min': 0.0, 'percentile_00_5': 0.014754901640117168, 'percentile_99_5': 0.4977976381778717, 'std': 0.12025152146816254}}} 

2024-12-07 03:12:59.536045: unpacking dataset...
using pin_memory on device 0

This is the configuration used by this training:
Configuration name: 3d_32x160x128_b10
 {'data_identifier': 'nnUNetPlans_3d_fullres', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 10, 'patch_size': [32, 160, 128], 'median_image_size_in_voxels': [375.0, 498.0, 498.0], 'spacing': [1.0, 1.0, 1.0], 'normalization_schemes': ['ZScoreNormalization'], 'use_mask_for_norm': [False], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'architecture': {'network_class_name': 'dynamic_network_architectures.architectures.unet.PlainConvUNet', 'arch_kwargs': {'n_stages': 6, 'features_per_stage': [32, 64, 128, 256, 320, 320], 'conv_op': 'torch.nn.modules.conv.Conv3d', 'kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]], 'strides': [[1, 1, 1], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [1, 2, 2]], 'n_conv_per_stage': [2, 2, 2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2, 2, 2], 'conv_bias': True, 'norm_op': 'torch.nn.modules.instancenorm.InstanceNorm3d', 'norm_op_kwargs': {'eps': 1e-05, 'affine': True}, 'dropout_op': None, 'dropout_op_kwargs': None, 'nonlin': 'torch.nn.LeakyReLU', 'nonlin_kwargs': {'inplace': True}, 'deep_supervision': True}, '_kw_requires_import': ['conv_op', 'norm_op', 'dropout_op', 'nonlin']}, 'batch_dice': True, 'inherits_from': '3d_fullres'} 

These are the global plan.json settings:
 {'dataset_name': 'Dataset307_Sohee_Calcium_OCT_CrossValidation', 'plans_name': 'nnUNetPlans', 'original_median_spacing_after_transp': [1.0, 1.0, 1.0], 'original_median_shape_after_transp': [375, 498, 498], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 0.4977976381778717, 'mean': 0.13507525622844696, 'median': 0.09599608182907104, 'min': 0.0, 'percentile_00_5': 0.014754901640117168, 'percentile_99_5': 0.4977976381778717, 'std': 0.12025152146816254}}} 

2024-12-07 03:13:00.750749: unpacking dataset...
2024-12-07 03:13:05.470947: unpacking done...
2024-12-07 03:13:05.628499: Unable to plot network architecture: nnUNet_compile is enabled!
2024-12-07 03:13:05.736324: 
2024-12-07 03:13:05.737432: Epoch 0
2024-12-07 03:13:05.738316: Current learning rate: 0.01
2024-12-07 03:16:02.828868: Validation loss improved from 1000.00000 to -0.41094! Patience: 0/50
2024-12-07 03:16:02.830055: train_loss -0.3009
2024-12-07 03:16:02.831115: val_loss -0.4109
2024-12-07 03:16:02.831866: Pseudo dice [0.715]
2024-12-07 03:16:02.832559: Epoch time: 177.09 s
2024-12-07 03:16:02.833190: Yayy! New best EMA pseudo Dice: 0.715
2024-12-07 03:16:04.817745: 
2024-12-07 03:16:04.818878: Epoch 1
2024-12-07 03:16:04.819586: Current learning rate: 0.00999
2024-12-07 03:18:10.526300: Validation loss improved from -0.41094 to -0.44376! Patience: 0/50
2024-12-07 03:18:10.527220: train_loss -0.4675
2024-12-07 03:18:10.527969: val_loss -0.4438
2024-12-07 03:18:10.528693: Pseudo dice [0.7221]
2024-12-07 03:18:10.529408: Epoch time: 125.71 s
2024-12-07 03:18:10.530059: Yayy! New best EMA pseudo Dice: 0.7157
2024-12-07 03:18:12.307010: 
2024-12-07 03:18:12.308151: Epoch 2
2024-12-07 03:18:12.309009: Current learning rate: 0.00998
2024-12-07 03:20:49.345684: Validation loss improved from -0.44376 to -0.46809! Patience: 0/50
2024-12-07 03:20:49.346808: train_loss -0.4879
2024-12-07 03:20:49.347741: val_loss -0.4681
2024-12-07 03:20:49.348729: Pseudo dice [0.7414]
2024-12-07 03:20:49.349627: Epoch time: 157.04 s
2024-12-07 03:20:49.350493: Yayy! New best EMA pseudo Dice: 0.7182
2024-12-07 03:20:51.272702: 
2024-12-07 03:20:51.273954: Epoch 3
2024-12-07 03:20:51.274740: Current learning rate: 0.00997
2024-12-07 03:23:48.981790: Validation loss did not improve from -0.46809. Patience: 1/50
2024-12-07 03:23:48.982856: train_loss -0.5045
2024-12-07 03:23:48.983868: val_loss -0.4408
2024-12-07 03:23:48.984733: Pseudo dice [0.7238]
2024-12-07 03:23:48.985631: Epoch time: 177.71 s
2024-12-07 03:23:48.986421: Yayy! New best EMA pseudo Dice: 0.7188
2024-12-07 03:23:50.944708: 
2024-12-07 03:23:50.945987: Epoch 4
2024-12-07 03:23:50.946975: Current learning rate: 0.00996
2024-12-07 03:27:03.678010: Validation loss improved from -0.46809 to -0.47511! Patience: 1/50
2024-12-07 03:27:03.678858: train_loss -0.5337
2024-12-07 03:27:03.679822: val_loss -0.4751
2024-12-07 03:27:03.680621: Pseudo dice [0.7333]
2024-12-07 03:27:03.681393: Epoch time: 192.74 s
2024-12-07 03:27:04.108217: Yayy! New best EMA pseudo Dice: 0.7203
2024-12-07 03:27:06.040951: 
2024-12-07 03:27:06.042385: Epoch 5
2024-12-07 03:27:06.043217: Current learning rate: 0.00995
2024-12-07 03:30:23.520357: Validation loss improved from -0.47511 to -0.50617! Patience: 0/50
2024-12-07 03:30:23.521380: train_loss -0.5487
2024-12-07 03:30:23.522152: val_loss -0.5062
2024-12-07 03:30:23.522834: Pseudo dice [0.7501]
2024-12-07 03:30:23.523589: Epoch time: 197.48 s
2024-12-07 03:30:23.524304: Yayy! New best EMA pseudo Dice: 0.7232
2024-12-07 03:30:25.401582: 
2024-12-07 03:30:25.402955: Epoch 6
2024-12-07 03:30:25.403672: Current learning rate: 0.00995
2024-12-07 03:33:39.644904: Validation loss did not improve from -0.50617. Patience: 1/50
2024-12-07 03:33:39.645859: train_loss -0.5543
2024-12-07 03:33:39.646640: val_loss -0.4756
2024-12-07 03:33:39.647382: Pseudo dice [0.7372]
2024-12-07 03:33:39.648088: Epoch time: 194.25 s
2024-12-07 03:33:39.648838: Yayy! New best EMA pseudo Dice: 0.7246
2024-12-07 03:33:41.509800: 
2024-12-07 03:33:41.511338: Epoch 7
2024-12-07 03:33:41.512268: Current learning rate: 0.00994
2024-12-07 03:37:10.087722: Validation loss did not improve from -0.50617. Patience: 2/50
2024-12-07 03:37:10.088752: train_loss -0.5646
2024-12-07 03:37:10.089774: val_loss -0.493
2024-12-07 03:37:10.090511: Pseudo dice [0.7476]
2024-12-07 03:37:10.091317: Epoch time: 208.58 s
2024-12-07 03:37:10.092108: Yayy! New best EMA pseudo Dice: 0.7269
2024-12-07 03:37:12.417862: 
2024-12-07 03:37:12.418947: Epoch 8
2024-12-07 03:37:12.419779: Current learning rate: 0.00993
2024-12-07 03:40:40.459601: Validation loss improved from -0.50617 to -0.52218! Patience: 2/50
2024-12-07 03:40:40.460511: train_loss -0.5757
2024-12-07 03:40:40.461403: val_loss -0.5222
2024-12-07 03:40:40.462125: Pseudo dice [0.761]
2024-12-07 03:40:40.462898: Epoch time: 208.04 s
2024-12-07 03:40:40.463670: Yayy! New best EMA pseudo Dice: 0.7303
2024-12-07 03:40:42.396091: 
2024-12-07 03:40:42.397332: Epoch 9
2024-12-07 03:40:42.398138: Current learning rate: 0.00992
2024-12-07 03:44:19.297114: Validation loss did not improve from -0.52218. Patience: 1/50
2024-12-07 03:44:19.298100: train_loss -0.578
2024-12-07 03:44:19.299023: val_loss -0.4992
2024-12-07 03:44:19.299796: Pseudo dice [0.7525]
2024-12-07 03:44:19.300607: Epoch time: 216.9 s
2024-12-07 03:44:19.728391: Yayy! New best EMA pseudo Dice: 0.7325
2024-12-07 03:44:21.499685: 
2024-12-07 03:44:21.501287: Epoch 10
2024-12-07 03:44:21.502348: Current learning rate: 0.00991
2024-12-07 03:47:54.920435: Validation loss improved from -0.52218 to -0.53642! Patience: 1/50
2024-12-07 03:47:54.921522: train_loss -0.5973
2024-12-07 03:47:54.922267: val_loss -0.5364
2024-12-07 03:47:54.923062: Pseudo dice [0.7727]
2024-12-07 03:47:54.923777: Epoch time: 213.42 s
2024-12-07 03:47:54.924539: Yayy! New best EMA pseudo Dice: 0.7366
2024-12-07 03:47:56.757287: 
2024-12-07 03:47:56.758787: Epoch 11
2024-12-07 03:47:56.759568: Current learning rate: 0.0099
2024-12-07 03:51:51.584987: Validation loss did not improve from -0.53642. Patience: 1/50
2024-12-07 03:51:51.586002: train_loss -0.6009
2024-12-07 03:51:51.586828: val_loss -0.48
2024-12-07 03:51:51.587562: Pseudo dice [0.7416]
2024-12-07 03:51:51.588306: Epoch time: 234.83 s
2024-12-07 03:51:51.589064: Yayy! New best EMA pseudo Dice: 0.7371
2024-12-07 03:51:53.420993: 
2024-12-07 03:51:53.422411: Epoch 12
2024-12-07 03:51:53.423312: Current learning rate: 0.00989
2024-12-07 03:56:45.198134: Validation loss did not improve from -0.53642. Patience: 2/50
2024-12-07 03:56:45.199184: train_loss -0.607
2024-12-07 03:56:45.199934: val_loss -0.5189
2024-12-07 03:56:45.200696: Pseudo dice [0.7518]
2024-12-07 03:56:45.201468: Epoch time: 291.78 s
2024-12-07 03:56:45.202282: Yayy! New best EMA pseudo Dice: 0.7385
2024-12-07 03:56:47.008933: 
2024-12-07 03:56:47.010279: Epoch 13
2024-12-07 03:56:47.011029: Current learning rate: 0.00988
2024-12-07 04:02:17.121508: Validation loss did not improve from -0.53642. Patience: 3/50
2024-12-07 04:02:17.122294: train_loss -0.6046
2024-12-07 04:02:17.123070: val_loss -0.5156
2024-12-07 04:02:17.123917: Pseudo dice [0.7625]
2024-12-07 04:02:17.124633: Epoch time: 330.11 s
2024-12-07 04:02:17.125310: Yayy! New best EMA pseudo Dice: 0.7409
2024-12-07 04:02:19.013097: 
2024-12-07 04:02:19.014408: Epoch 14
2024-12-07 04:02:19.015291: Current learning rate: 0.00987
2024-12-07 04:08:08.937926: Validation loss did not improve from -0.53642. Patience: 4/50
2024-12-07 04:08:08.938922: train_loss -0.6113
2024-12-07 04:08:08.939848: val_loss -0.4941
2024-12-07 04:08:08.940826: Pseudo dice [0.7481]
2024-12-07 04:08:08.941748: Epoch time: 349.93 s
2024-12-07 04:08:09.361993: Yayy! New best EMA pseudo Dice: 0.7416
2024-12-07 04:08:11.243874: 
2024-12-07 04:08:11.245293: Epoch 15
2024-12-07 04:08:11.246064: Current learning rate: 0.00986
2024-12-07 04:14:03.153763: Validation loss improved from -0.53642 to -0.55276! Patience: 4/50
2024-12-07 04:14:03.154801: train_loss -0.6068
2024-12-07 04:14:03.155620: val_loss -0.5528
2024-12-07 04:14:03.156280: Pseudo dice [0.7806]
2024-12-07 04:14:03.157174: Epoch time: 351.91 s
2024-12-07 04:14:03.158010: Yayy! New best EMA pseudo Dice: 0.7455
2024-12-07 04:14:04.996993: 
2024-12-07 04:14:04.998962: Epoch 16
2024-12-07 04:14:05.000044: Current learning rate: 0.00986
2024-12-07 04:19:53.961236: Validation loss did not improve from -0.55276. Patience: 1/50
2024-12-07 04:19:53.964113: train_loss -0.62
2024-12-07 04:19:53.965426: val_loss -0.4915
2024-12-07 04:19:53.966384: Pseudo dice [0.7393]
2024-12-07 04:19:53.967505: Epoch time: 348.97 s
2024-12-07 04:19:55.491227: 
2024-12-07 04:19:55.492561: Epoch 17
2024-12-07 04:19:55.493446: Current learning rate: 0.00985
2024-12-07 04:25:32.161880: Validation loss did not improve from -0.55276. Patience: 2/50
2024-12-07 04:25:32.162925: train_loss -0.6297
2024-12-07 04:25:32.163738: val_loss -0.5092
2024-12-07 04:25:32.164560: Pseudo dice [0.7487]
2024-12-07 04:25:32.165335: Epoch time: 336.67 s
2024-12-07 04:25:33.612753: 
2024-12-07 04:25:33.614134: Epoch 18
2024-12-07 04:25:33.614960: Current learning rate: 0.00984
2024-12-07 04:30:46.558659: Validation loss did not improve from -0.55276. Patience: 3/50
2024-12-07 04:30:46.559701: train_loss -0.6287
2024-12-07 04:30:46.560493: val_loss -0.5332
2024-12-07 04:30:46.561275: Pseudo dice [0.763]
2024-12-07 04:30:46.562005: Epoch time: 312.95 s
2024-12-07 04:30:46.562641: Yayy! New best EMA pseudo Dice: 0.7471
2024-12-07 04:30:48.893797: 
2024-12-07 04:30:48.895223: Epoch 19
2024-12-07 04:30:48.896185: Current learning rate: 0.00983
2024-12-07 04:36:08.816813: Validation loss did not improve from -0.55276. Patience: 4/50
2024-12-07 04:36:08.817964: train_loss -0.6268
2024-12-07 04:36:08.819169: val_loss -0.5011
2024-12-07 04:36:08.820159: Pseudo dice [0.7492]
2024-12-07 04:36:08.821072: Epoch time: 319.93 s
2024-12-07 04:36:09.242106: Yayy! New best EMA pseudo Dice: 0.7473
2024-12-07 04:36:11.127141: 
2024-12-07 04:36:11.128669: Epoch 20
2024-12-07 04:36:11.129733: Current learning rate: 0.00982
2024-12-07 04:41:50.456559: Validation loss did not improve from -0.55276. Patience: 5/50
2024-12-07 04:41:50.457667: train_loss -0.6277
2024-12-07 04:41:50.458389: val_loss -0.5304
2024-12-07 04:41:50.459010: Pseudo dice [0.7711]
2024-12-07 04:41:50.459659: Epoch time: 339.33 s
2024-12-07 04:41:50.460396: Yayy! New best EMA pseudo Dice: 0.7497
2024-12-07 04:41:52.442733: 
2024-12-07 04:41:52.443974: Epoch 21
2024-12-07 04:41:52.444696: Current learning rate: 0.00981
2024-12-07 04:47:40.534458: Validation loss did not improve from -0.55276. Patience: 6/50
2024-12-07 04:47:40.535396: train_loss -0.6348
2024-12-07 04:47:40.536272: val_loss -0.5142
2024-12-07 04:47:40.537035: Pseudo dice [0.7514]
2024-12-07 04:47:40.537708: Epoch time: 348.09 s
2024-12-07 04:47:40.538463: Yayy! New best EMA pseudo Dice: 0.7498
2024-12-07 04:47:42.385713: 
2024-12-07 04:47:42.387023: Epoch 22
2024-12-07 04:47:42.387816: Current learning rate: 0.0098
2024-12-07 04:53:28.201754: Validation loss did not improve from -0.55276. Patience: 7/50
2024-12-07 04:53:28.202946: train_loss -0.6494
2024-12-07 04:53:28.203820: val_loss -0.5381
2024-12-07 04:53:28.204480: Pseudo dice [0.775]
2024-12-07 04:53:28.205198: Epoch time: 345.82 s
2024-12-07 04:53:28.205922: Yayy! New best EMA pseudo Dice: 0.7524
2024-12-07 04:53:30.012557: 
2024-12-07 04:53:30.013948: Epoch 23
2024-12-07 04:53:30.014621: Current learning rate: 0.00979
2024-12-07 04:59:04.255524: Validation loss did not improve from -0.55276. Patience: 8/50
2024-12-07 04:59:04.256619: train_loss -0.6549
2024-12-07 04:59:04.257375: val_loss -0.5438
2024-12-07 04:59:04.258055: Pseudo dice [0.7721]
2024-12-07 04:59:04.258806: Epoch time: 334.25 s
2024-12-07 04:59:04.259521: Yayy! New best EMA pseudo Dice: 0.7543
2024-12-07 04:59:06.017993: 
2024-12-07 04:59:06.019280: Epoch 24
2024-12-07 04:59:06.019976: Current learning rate: 0.00978
2024-12-07 05:04:44.090341: Validation loss improved from -0.55276 to -0.55410! Patience: 8/50
2024-12-07 05:04:44.091202: train_loss -0.6552
2024-12-07 05:04:44.092059: val_loss -0.5541
2024-12-07 05:04:44.092947: Pseudo dice [0.7833]
2024-12-07 05:04:44.093809: Epoch time: 338.07 s
2024-12-07 05:04:44.454978: Yayy! New best EMA pseudo Dice: 0.7572
2024-12-07 05:04:46.266994: 
2024-12-07 05:04:46.268486: Epoch 25
2024-12-07 05:04:46.269262: Current learning rate: 0.00977
2024-12-07 05:10:22.681183: Validation loss did not improve from -0.55410. Patience: 1/50
2024-12-07 05:10:22.682160: train_loss -0.6455
2024-12-07 05:10:22.682927: val_loss -0.5534
2024-12-07 05:10:22.683585: Pseudo dice [0.7675]
2024-12-07 05:10:22.684241: Epoch time: 336.42 s
2024-12-07 05:10:22.684964: Yayy! New best EMA pseudo Dice: 0.7583
2024-12-07 05:10:24.500278: 
2024-12-07 05:10:24.501550: Epoch 26
2024-12-07 05:10:24.502468: Current learning rate: 0.00977
2024-12-07 05:16:05.819820: Validation loss did not improve from -0.55410. Patience: 2/50
2024-12-07 05:16:05.820685: train_loss -0.6499
2024-12-07 05:16:05.821880: val_loss -0.5522
2024-12-07 05:16:05.822942: Pseudo dice [0.7714]
2024-12-07 05:16:05.823934: Epoch time: 341.32 s
2024-12-07 05:16:05.824947: Yayy! New best EMA pseudo Dice: 0.7596
2024-12-07 05:16:07.637264: 
2024-12-07 05:16:07.638639: Epoch 27
2024-12-07 05:16:07.639868: Current learning rate: 0.00976
2024-12-07 05:22:05.979292: Validation loss did not improve from -0.55410. Patience: 3/50
2024-12-07 05:22:05.981888: train_loss -0.6533
2024-12-07 05:22:05.982959: val_loss -0.5253
2024-12-07 05:22:05.983823: Pseudo dice [0.76]
2024-12-07 05:22:05.984686: Epoch time: 358.35 s
2024-12-07 05:22:05.985467: Yayy! New best EMA pseudo Dice: 0.7596
2024-12-07 05:22:07.796950: 
2024-12-07 05:22:07.798453: Epoch 28
2024-12-07 05:22:07.799445: Current learning rate: 0.00975
2024-12-07 05:28:04.092109: Validation loss did not improve from -0.55410. Patience: 4/50
2024-12-07 05:28:04.094199: train_loss -0.6544
2024-12-07 05:28:04.095886: val_loss -0.5518
2024-12-07 05:28:04.096804: Pseudo dice [0.7783]
2024-12-07 05:28:04.097749: Epoch time: 356.3 s
2024-12-07 05:28:04.098433: Yayy! New best EMA pseudo Dice: 0.7615
2024-12-07 05:28:06.328309: 
2024-12-07 05:28:06.329522: Epoch 29
2024-12-07 05:28:06.330307: Current learning rate: 0.00974
2024-12-07 05:34:08.381116: Validation loss improved from -0.55410 to -0.55417! Patience: 4/50
2024-12-07 05:34:08.382134: train_loss -0.6656
2024-12-07 05:34:08.383030: val_loss -0.5542
2024-12-07 05:34:08.383926: Pseudo dice [0.7804]
2024-12-07 05:34:08.384740: Epoch time: 362.06 s
2024-12-07 05:34:08.800525: Yayy! New best EMA pseudo Dice: 0.7634
2024-12-07 05:34:10.710467: 
2024-12-07 05:34:10.711755: Epoch 30
2024-12-07 05:34:10.712589: Current learning rate: 0.00973
2024-12-07 05:39:56.771758: Validation loss did not improve from -0.55417. Patience: 1/50
2024-12-07 05:39:56.774332: train_loss -0.6575
2024-12-07 05:39:56.775165: val_loss -0.5447
2024-12-07 05:39:56.775894: Pseudo dice [0.7748]
2024-12-07 05:39:56.776573: Epoch time: 346.07 s
2024-12-07 05:39:56.777364: Yayy! New best EMA pseudo Dice: 0.7645
2024-12-07 05:39:58.622727: 
2024-12-07 05:39:58.624166: Epoch 31
2024-12-07 05:39:58.625088: Current learning rate: 0.00972
2024-12-07 05:45:52.658752: Validation loss did not improve from -0.55417. Patience: 2/50
2024-12-07 05:45:52.659736: train_loss -0.6602
2024-12-07 05:45:52.660691: val_loss -0.5404
2024-12-07 05:45:52.661370: Pseudo dice [0.7706]
2024-12-07 05:45:52.662100: Epoch time: 354.04 s
2024-12-07 05:45:52.662810: Yayy! New best EMA pseudo Dice: 0.7651
2024-12-07 05:45:54.523473: 
2024-12-07 05:45:54.524907: Epoch 32
2024-12-07 05:45:54.525806: Current learning rate: 0.00971
2024-12-07 05:51:36.318780: Validation loss did not improve from -0.55417. Patience: 3/50
2024-12-07 05:51:36.319921: train_loss -0.6586
2024-12-07 05:51:36.321127: val_loss -0.5192
2024-12-07 05:51:36.322153: Pseudo dice [0.7588]
2024-12-07 05:51:36.323112: Epoch time: 341.8 s
2024-12-07 05:51:37.894682: 
2024-12-07 05:51:37.896109: Epoch 33
2024-12-07 05:51:37.897172: Current learning rate: 0.0097
2024-12-07 05:57:31.970793: Validation loss did not improve from -0.55417. Patience: 4/50
2024-12-07 05:57:31.971842: train_loss -0.6732
2024-12-07 05:57:31.972615: val_loss -0.5219
2024-12-07 05:57:31.973422: Pseudo dice [0.761]
2024-12-07 05:57:31.974191: Epoch time: 354.08 s
2024-12-07 05:57:33.436881: 
2024-12-07 05:57:33.438135: Epoch 34
2024-12-07 05:57:33.449680: Current learning rate: 0.00969
2024-12-07 06:03:27.750459: Validation loss did not improve from -0.55417. Patience: 5/50
2024-12-07 06:03:27.751403: train_loss -0.6767
2024-12-07 06:03:27.752308: val_loss -0.521
2024-12-07 06:03:27.753220: Pseudo dice [0.7652]
2024-12-07 06:03:27.754161: Epoch time: 354.32 s
2024-12-07 06:03:29.605751: 
2024-12-07 06:03:29.607186: Epoch 35
2024-12-07 06:03:29.608467: Current learning rate: 0.00968
2024-12-07 06:09:24.229261: Validation loss did not improve from -0.55417. Patience: 6/50
2024-12-07 06:09:24.230211: train_loss -0.6783
2024-12-07 06:09:24.231172: val_loss -0.492
2024-12-07 06:09:24.232020: Pseudo dice [0.7527]
2024-12-07 06:09:24.232712: Epoch time: 354.63 s
2024-12-07 06:09:25.687994: 
2024-12-07 06:09:25.689220: Epoch 36
2024-12-07 06:09:25.689982: Current learning rate: 0.00968
2024-12-07 06:15:16.269562: Validation loss did not improve from -0.55417. Patience: 7/50
2024-12-07 06:15:16.270698: train_loss -0.665
2024-12-07 06:15:16.271766: val_loss -0.5416
2024-12-07 06:15:16.272840: Pseudo dice [0.7669]
2024-12-07 06:15:16.273837: Epoch time: 350.58 s
2024-12-07 06:15:17.724331: 
2024-12-07 06:15:17.725726: Epoch 37
2024-12-07 06:15:17.726908: Current learning rate: 0.00967
2024-12-07 06:21:04.780760: Validation loss improved from -0.55417 to -0.57853! Patience: 7/50
2024-12-07 06:21:04.781739: train_loss -0.6804
2024-12-07 06:21:04.782554: val_loss -0.5785
2024-12-07 06:21:04.783321: Pseudo dice [0.7843]
2024-12-07 06:21:04.784135: Epoch time: 347.06 s
2024-12-07 06:21:04.784953: Yayy! New best EMA pseudo Dice: 0.7656
2024-12-07 06:21:06.676040: 
2024-12-07 06:21:06.677310: Epoch 38
2024-12-07 06:21:06.678071: Current learning rate: 0.00966
2024-12-07 06:26:44.310193: Validation loss did not improve from -0.57853. Patience: 1/50
2024-12-07 06:26:44.311385: train_loss -0.6837
2024-12-07 06:26:44.312299: val_loss -0.5397
2024-12-07 06:26:44.313094: Pseudo dice [0.776]
2024-12-07 06:26:44.313920: Epoch time: 337.64 s
2024-12-07 06:26:44.314731: Yayy! New best EMA pseudo Dice: 0.7666
2024-12-07 06:26:46.272922: 
2024-12-07 06:26:46.274444: Epoch 39
2024-12-07 06:26:46.275375: Current learning rate: 0.00965
2024-12-07 06:32:47.099745: Validation loss did not improve from -0.57853. Patience: 2/50
2024-12-07 06:32:47.101658: train_loss -0.6846
2024-12-07 06:32:47.102933: val_loss -0.5392
2024-12-07 06:32:47.104001: Pseudo dice [0.7703]
2024-12-07 06:32:47.105131: Epoch time: 360.83 s
2024-12-07 06:32:47.510933: Yayy! New best EMA pseudo Dice: 0.767
2024-12-07 06:32:49.503300: 
2024-12-07 06:32:49.505004: Epoch 40
2024-12-07 06:32:49.506058: Current learning rate: 0.00964
2024-12-07 06:39:09.496151: Validation loss did not improve from -0.57853. Patience: 3/50
2024-12-07 06:39:09.497102: train_loss -0.6818
2024-12-07 06:39:09.498078: val_loss -0.5111
2024-12-07 06:39:09.498979: Pseudo dice [0.7571]
2024-12-07 06:39:09.499995: Epoch time: 380.0 s
2024-12-07 06:39:11.019261: 
2024-12-07 06:39:11.020532: Epoch 41
2024-12-07 06:39:11.021800: Current learning rate: 0.00963
2024-12-07 06:45:35.735697: Validation loss did not improve from -0.57853. Patience: 4/50
2024-12-07 06:45:35.736752: train_loss -0.6853
2024-12-07 06:45:35.737557: val_loss -0.5537
2024-12-07 06:45:35.738427: Pseudo dice [0.7815]
2024-12-07 06:45:35.739208: Epoch time: 384.72 s
2024-12-07 06:45:35.740033: Yayy! New best EMA pseudo Dice: 0.7675
2024-12-07 06:45:37.557899: 
2024-12-07 06:45:37.559362: Epoch 42
2024-12-07 06:45:37.560232: Current learning rate: 0.00962
2024-12-07 06:51:57.914905: Validation loss did not improve from -0.57853. Patience: 5/50
2024-12-07 06:51:57.915896: train_loss -0.6886
2024-12-07 06:51:57.916668: val_loss -0.5093
2024-12-07 06:51:57.917358: Pseudo dice [0.7558]
2024-12-07 06:51:57.918026: Epoch time: 380.36 s
2024-12-07 06:51:59.324227: 
2024-12-07 06:51:59.325363: Epoch 43
2024-12-07 06:51:59.326353: Current learning rate: 0.00961
2024-12-07 06:58:32.293922: Validation loss improved from -0.57853 to -0.58565! Patience: 5/50
2024-12-07 06:58:32.294977: train_loss -0.6857
2024-12-07 06:58:32.295948: val_loss -0.5857
2024-12-07 06:58:32.296881: Pseudo dice [0.7918]
2024-12-07 06:58:32.297837: Epoch time: 392.97 s
2024-12-07 06:58:32.298687: Yayy! New best EMA pseudo Dice: 0.7689
2024-12-07 06:58:34.235865: 
2024-12-07 06:58:34.237432: Epoch 44
2024-12-07 06:58:34.238371: Current learning rate: 0.0096
2024-12-07 07:05:00.266176: Validation loss did not improve from -0.58565. Patience: 1/50
2024-12-07 07:05:00.267058: train_loss -0.6865
2024-12-07 07:05:00.268356: val_loss -0.5284
2024-12-07 07:05:00.269338: Pseudo dice [0.7666]
2024-12-07 07:05:00.270220: Epoch time: 386.03 s
2024-12-07 07:05:02.117660: 
2024-12-07 07:05:02.118732: Epoch 45
2024-12-07 07:05:02.119687: Current learning rate: 0.00959
2024-12-07 07:11:34.662741: Validation loss did not improve from -0.58565. Patience: 2/50
2024-12-07 07:11:34.663660: train_loss -0.6937
2024-12-07 07:11:34.664345: val_loss -0.5231
2024-12-07 07:11:34.665111: Pseudo dice [0.7646]
2024-12-07 07:11:34.665735: Epoch time: 392.55 s
2024-12-07 07:11:36.080874: 
2024-12-07 07:11:36.082136: Epoch 46
2024-12-07 07:11:36.082878: Current learning rate: 0.00959
2024-12-07 07:18:07.571623: Validation loss did not improve from -0.58565. Patience: 3/50
2024-12-07 07:18:07.572583: train_loss -0.6854
2024-12-07 07:18:07.573427: val_loss -0.5438
2024-12-07 07:18:07.574105: Pseudo dice [0.782]
2024-12-07 07:18:07.574773: Epoch time: 391.49 s
2024-12-07 07:18:07.575454: Yayy! New best EMA pseudo Dice: 0.7696
2024-12-07 07:18:09.361965: 
2024-12-07 07:18:09.363391: Epoch 47
2024-12-07 07:18:09.364120: Current learning rate: 0.00958
2024-12-07 07:24:44.166533: Validation loss did not improve from -0.58565. Patience: 4/50
2024-12-07 07:24:44.167530: train_loss -0.6883
2024-12-07 07:24:44.168380: val_loss -0.5517
2024-12-07 07:24:44.169163: Pseudo dice [0.7771]
2024-12-07 07:24:44.169926: Epoch time: 394.81 s
2024-12-07 07:24:44.170588: Yayy! New best EMA pseudo Dice: 0.7704
2024-12-07 07:24:45.960053: 
2024-12-07 07:24:45.961306: Epoch 48
2024-12-07 07:24:45.962099: Current learning rate: 0.00957
2024-12-07 07:31:13.677897: Validation loss did not improve from -0.58565. Patience: 5/50
2024-12-07 07:31:13.678801: train_loss -0.7013
2024-12-07 07:31:13.679666: val_loss -0.5457
2024-12-07 07:31:13.680470: Pseudo dice [0.7758]
2024-12-07 07:31:13.681281: Epoch time: 387.72 s
2024-12-07 07:31:13.682150: Yayy! New best EMA pseudo Dice: 0.7709
2024-12-07 07:31:15.508919: 
2024-12-07 07:31:15.510568: Epoch 49
2024-12-07 07:31:15.511696: Current learning rate: 0.00956
2024-12-07 07:37:46.643972: Validation loss did not improve from -0.58565. Patience: 6/50
2024-12-07 07:37:46.645984: train_loss -0.6953
2024-12-07 07:37:46.648217: val_loss -0.5459
2024-12-07 07:37:46.649430: Pseudo dice [0.7683]
2024-12-07 07:37:46.650961: Epoch time: 391.14 s
2024-12-07 07:37:48.842022: 
2024-12-07 07:37:48.843515: Epoch 50
2024-12-07 07:37:48.844481: Current learning rate: 0.00955
2024-12-07 07:44:28.218268: Validation loss did not improve from -0.58565. Patience: 7/50
2024-12-07 07:44:28.219238: train_loss -0.6884
2024-12-07 07:44:28.219971: val_loss -0.574
2024-12-07 07:44:28.220604: Pseudo dice [0.7907]
2024-12-07 07:44:28.221269: Epoch time: 399.38 s
2024-12-07 07:44:28.221888: Yayy! New best EMA pseudo Dice: 0.7727
2024-12-07 07:44:30.074631: 
2024-12-07 07:44:30.076046: Epoch 51
2024-12-07 07:44:30.076863: Current learning rate: 0.00954
2024-12-07 07:51:06.714602: Validation loss did not improve from -0.58565. Patience: 8/50
2024-12-07 07:51:06.715600: train_loss -0.6957
2024-12-07 07:51:06.716427: val_loss -0.5201
2024-12-07 07:51:06.717248: Pseudo dice [0.7707]
2024-12-07 07:51:06.718051: Epoch time: 396.64 s
2024-12-07 07:51:08.281651: 
2024-12-07 07:51:08.283051: Epoch 52
2024-12-07 07:51:08.283848: Current learning rate: 0.00953
2024-12-07 07:57:38.241283: Validation loss did not improve from -0.58565. Patience: 9/50
2024-12-07 07:57:38.242424: train_loss -0.6991
2024-12-07 07:57:38.243125: val_loss -0.5104
2024-12-07 07:57:38.243798: Pseudo dice [0.7582]
2024-12-07 07:57:38.244438: Epoch time: 389.96 s
2024-12-07 07:57:39.723114: 
2024-12-07 07:57:39.724614: Epoch 53
2024-12-07 07:57:39.725304: Current learning rate: 0.00952
2024-12-07 08:04:13.455717: Validation loss did not improve from -0.58565. Patience: 10/50
2024-12-07 08:04:13.456661: train_loss -0.7074
2024-12-07 08:04:13.457452: val_loss -0.5797
2024-12-07 08:04:13.458139: Pseudo dice [0.7948]
2024-12-07 08:04:13.459066: Epoch time: 393.74 s
2024-12-07 08:04:13.459982: Yayy! New best EMA pseudo Dice: 0.7734
2024-12-07 08:04:15.330575: 
2024-12-07 08:04:15.332102: Epoch 54
2024-12-07 08:04:15.333252: Current learning rate: 0.00951
2024-12-07 08:10:47.145093: Validation loss did not improve from -0.58565. Patience: 11/50
2024-12-07 08:10:47.146170: train_loss -0.7018
2024-12-07 08:10:47.147048: val_loss -0.5587
2024-12-07 08:10:47.147834: Pseudo dice [0.782]
2024-12-07 08:10:47.148537: Epoch time: 391.82 s
2024-12-07 08:10:47.513359: Yayy! New best EMA pseudo Dice: 0.7743
2024-12-07 08:10:49.344190: 
2024-12-07 08:10:49.345719: Epoch 55
2024-12-07 08:10:49.346542: Current learning rate: 0.0095
2024-12-07 08:17:16.407585: Validation loss did not improve from -0.58565. Patience: 12/50
2024-12-07 08:17:16.408617: train_loss -0.6978
2024-12-07 08:17:16.409573: val_loss -0.5475
2024-12-07 08:17:16.410492: Pseudo dice [0.7718]
2024-12-07 08:17:16.411255: Epoch time: 387.07 s
2024-12-07 08:17:17.860597: 
2024-12-07 08:17:17.862211: Epoch 56
2024-12-07 08:17:17.863000: Current learning rate: 0.00949
2024-12-07 08:23:34.524620: Validation loss did not improve from -0.58565. Patience: 13/50
2024-12-07 08:23:34.525729: train_loss -0.7056
2024-12-07 08:23:34.526560: val_loss -0.5243
2024-12-07 08:23:34.527175: Pseudo dice [0.7652]
2024-12-07 08:23:34.527952: Epoch time: 376.67 s
2024-12-07 08:23:35.970879: 
2024-12-07 08:23:35.972374: Epoch 57
2024-12-07 08:23:35.973336: Current learning rate: 0.00949
2024-12-07 08:29:50.675481: Validation loss did not improve from -0.58565. Patience: 14/50
2024-12-07 08:29:50.676507: train_loss -0.7091
2024-12-07 08:29:50.677331: val_loss -0.5629
2024-12-07 08:29:50.678216: Pseudo dice [0.7754]
2024-12-07 08:29:50.679036: Epoch time: 374.71 s
2024-12-07 08:29:52.136068: 
2024-12-07 08:29:52.137495: Epoch 58
2024-12-07 08:29:52.138370: Current learning rate: 0.00948
2024-12-07 08:36:12.526204: Validation loss did not improve from -0.58565. Patience: 15/50
2024-12-07 08:36:12.527292: train_loss -0.7143
2024-12-07 08:36:12.528120: val_loss -0.5443
2024-12-07 08:36:12.528908: Pseudo dice [0.7776]
2024-12-07 08:36:12.529648: Epoch time: 380.39 s
2024-12-07 08:36:13.983887: 
2024-12-07 08:36:13.985013: Epoch 59
2024-12-07 08:36:13.985878: Current learning rate: 0.00947
2024-12-07 08:42:27.972884: Validation loss did not improve from -0.58565. Patience: 16/50
2024-12-07 08:42:27.974947: train_loss -0.7062
2024-12-07 08:42:27.976794: val_loss -0.4895
2024-12-07 08:42:27.977646: Pseudo dice [0.7492]
2024-12-07 08:42:27.978691: Epoch time: 373.99 s
2024-12-07 08:42:30.722003: 
2024-12-07 08:42:30.723369: Epoch 60
2024-12-07 08:42:30.724223: Current learning rate: 0.00946
2024-12-07 08:48:30.898182: Validation loss did not improve from -0.58565. Patience: 17/50
2024-12-07 08:48:30.899212: train_loss -0.6868
2024-12-07 08:48:30.900057: val_loss -0.5005
2024-12-07 08:48:30.900850: Pseudo dice [0.7662]
2024-12-07 08:48:30.901622: Epoch time: 360.18 s
2024-12-07 08:48:32.324514: 
2024-12-07 08:48:32.325819: Epoch 61
2024-12-07 08:48:32.326660: Current learning rate: 0.00945
2024-12-07 08:54:31.639192: Validation loss improved from -0.58565 to -0.59150! Patience: 17/50
2024-12-07 08:54:31.640232: train_loss -0.7097
2024-12-07 08:54:31.641036: val_loss -0.5915
2024-12-07 08:54:31.641705: Pseudo dice [0.7993]
2024-12-07 08:54:31.642507: Epoch time: 359.32 s
2024-12-07 08:54:33.116845: 
2024-12-07 08:54:33.118150: Epoch 62
2024-12-07 08:54:33.118820: Current learning rate: 0.00944
2024-12-07 09:00:50.937386: Validation loss did not improve from -0.59150. Patience: 1/50
2024-12-07 09:00:50.938460: train_loss -0.7132
2024-12-07 09:00:50.939276: val_loss -0.5384
2024-12-07 09:00:50.939942: Pseudo dice [0.7671]
2024-12-07 09:00:50.940742: Epoch time: 377.82 s
2024-12-07 09:00:52.366868: 
2024-12-07 09:00:52.368048: Epoch 63
2024-12-07 09:00:52.368774: Current learning rate: 0.00943
2024-12-07 09:07:12.770041: Validation loss did not improve from -0.59150. Patience: 2/50
2024-12-07 09:07:12.771114: train_loss -0.7145
2024-12-07 09:07:12.771880: val_loss -0.5073
2024-12-07 09:07:12.772601: Pseudo dice [0.7567]
2024-12-07 09:07:12.773309: Epoch time: 380.41 s
2024-12-07 09:07:14.185799: 
2024-12-07 09:07:14.187193: Epoch 64
2024-12-07 09:07:14.187988: Current learning rate: 0.00942
2024-12-07 09:13:31.874582: Validation loss did not improve from -0.59150. Patience: 3/50
2024-12-07 09:13:31.875627: train_loss -0.7215
2024-12-07 09:13:31.876608: val_loss -0.5725
2024-12-07 09:13:31.877582: Pseudo dice [0.7894]
2024-12-07 09:13:31.878419: Epoch time: 377.69 s
2024-12-07 09:13:33.715859: 
2024-12-07 09:13:33.717332: Epoch 65
2024-12-07 09:13:33.718450: Current learning rate: 0.00941
2024-12-07 09:19:39.811115: Validation loss did not improve from -0.59150. Patience: 4/50
2024-12-07 09:19:39.812211: train_loss -0.7212
2024-12-07 09:19:39.813394: val_loss -0.4977
2024-12-07 09:19:39.814376: Pseudo dice [0.7541]
2024-12-07 09:19:39.815181: Epoch time: 366.1 s
2024-12-07 09:19:41.277565: 
2024-12-07 09:19:41.279103: Epoch 66
2024-12-07 09:19:41.279893: Current learning rate: 0.0094
2024-12-07 09:25:29.176069: Validation loss did not improve from -0.59150. Patience: 5/50
2024-12-07 09:25:29.177178: train_loss -0.7147
2024-12-07 09:25:29.178078: val_loss -0.5336
2024-12-07 09:25:29.178830: Pseudo dice [0.7611]
2024-12-07 09:25:29.179507: Epoch time: 347.9 s
2024-12-07 09:25:30.681731: 
2024-12-07 09:25:30.683134: Epoch 67
2024-12-07 09:25:30.683881: Current learning rate: 0.00939
2024-12-07 09:31:20.612272: Validation loss did not improve from -0.59150. Patience: 6/50
2024-12-07 09:31:20.613281: train_loss -0.7163
2024-12-07 09:31:20.614243: val_loss -0.542
2024-12-07 09:31:20.615041: Pseudo dice [0.784]
2024-12-07 09:31:20.615822: Epoch time: 349.93 s
2024-12-07 09:31:22.118915: 
2024-12-07 09:31:22.120336: Epoch 68
2024-12-07 09:31:22.121090: Current learning rate: 0.00939
2024-12-07 09:37:10.796813: Validation loss did not improve from -0.59150. Patience: 7/50
2024-12-07 09:37:10.797703: train_loss -0.7209
2024-12-07 09:37:10.798495: val_loss -0.5356
2024-12-07 09:37:10.799291: Pseudo dice [0.7676]
2024-12-07 09:37:10.800221: Epoch time: 348.68 s
2024-12-07 09:37:12.288023: 
2024-12-07 09:37:12.289482: Epoch 69
2024-12-07 09:37:12.290398: Current learning rate: 0.00938
2024-12-07 09:43:08.404999: Validation loss did not improve from -0.59150. Patience: 8/50
2024-12-07 09:43:08.407165: train_loss -0.7201
2024-12-07 09:43:08.408233: val_loss -0.5175
2024-12-07 09:43:08.409139: Pseudo dice [0.7612]
2024-12-07 09:43:08.410377: Epoch time: 356.12 s
2024-12-07 09:43:10.350441: 
2024-12-07 09:43:10.352181: Epoch 70
2024-12-07 09:43:10.353314: Current learning rate: 0.00937
2024-12-07 09:48:57.883815: Validation loss did not improve from -0.59150. Patience: 9/50
2024-12-07 09:48:57.885738: train_loss -0.7194
2024-12-07 09:48:57.887574: val_loss -0.4983
2024-12-07 09:48:57.888314: Pseudo dice [0.7517]
2024-12-07 09:48:57.889285: Epoch time: 347.54 s
2024-12-07 09:48:59.809495: 
2024-12-07 09:48:59.810917: Epoch 71
2024-12-07 09:48:59.811672: Current learning rate: 0.00936
2024-12-07 09:54:30.204239: Validation loss did not improve from -0.59150. Patience: 10/50
2024-12-07 09:54:30.204992: train_loss -0.7223
2024-12-07 09:54:30.205984: val_loss -0.5196
2024-12-07 09:54:30.206773: Pseudo dice [0.7628]
2024-12-07 09:54:30.207523: Epoch time: 330.4 s
2024-12-07 09:54:31.653561: 
2024-12-07 09:54:31.654928: Epoch 72
2024-12-07 09:54:31.655675: Current learning rate: 0.00935
2024-12-07 09:59:12.742120: Validation loss did not improve from -0.59150. Patience: 11/50
2024-12-07 09:59:12.743191: train_loss -0.7201
2024-12-07 09:59:12.744077: val_loss -0.556
2024-12-07 09:59:12.744878: Pseudo dice [0.7805]
2024-12-07 09:59:12.745613: Epoch time: 281.09 s
2024-12-07 09:59:14.212855: 
2024-12-07 09:59:14.214246: Epoch 73
2024-12-07 09:59:14.215066: Current learning rate: 0.00934
2024-12-07 10:04:28.296371: Validation loss did not improve from -0.59150. Patience: 12/50
2024-12-07 10:04:28.297328: train_loss -0.7271
2024-12-07 10:04:28.298367: val_loss -0.5699
2024-12-07 10:04:28.299171: Pseudo dice [0.7942]
2024-12-07 10:04:28.299979: Epoch time: 314.09 s
2024-12-07 10:04:29.745509: 
2024-12-07 10:04:29.747031: Epoch 74
2024-12-07 10:04:29.747987: Current learning rate: 0.00933
2024-12-07 10:10:02.845857: Validation loss did not improve from -0.59150. Patience: 13/50
2024-12-07 10:10:02.846792: train_loss -0.7327
2024-12-07 10:10:02.847754: val_loss -0.5616
2024-12-07 10:10:02.848691: Pseudo dice [0.7868]
2024-12-07 10:10:02.849660: Epoch time: 333.1 s
2024-12-07 10:10:04.779192: 
2024-12-07 10:10:04.780909: Epoch 75
2024-12-07 10:10:04.781948: Current learning rate: 0.00932
2024-12-07 10:15:48.847053: Validation loss did not improve from -0.59150. Patience: 14/50
2024-12-07 10:15:48.847798: train_loss -0.7321
2024-12-07 10:15:48.848511: val_loss -0.5157
2024-12-07 10:15:48.849155: Pseudo dice [0.76]
2024-12-07 10:15:48.849838: Epoch time: 344.07 s
2024-12-07 10:15:50.317340: 
2024-12-07 10:15:50.318876: Epoch 76
2024-12-07 10:15:50.319701: Current learning rate: 0.00931
2024-12-07 10:21:19.096514: Validation loss did not improve from -0.59150. Patience: 15/50
2024-12-07 10:21:19.097373: train_loss -0.7316
2024-12-07 10:21:19.098279: val_loss -0.4851
2024-12-07 10:21:19.099111: Pseudo dice [0.7495]
2024-12-07 10:21:19.099931: Epoch time: 328.78 s
2024-12-07 10:21:20.576176: 
2024-12-07 10:21:20.577744: Epoch 77
2024-12-07 10:21:20.578708: Current learning rate: 0.0093
2024-12-07 10:26:15.563610: Validation loss did not improve from -0.59150. Patience: 16/50
2024-12-07 10:26:15.564558: train_loss -0.7287
2024-12-07 10:26:15.565424: val_loss -0.5077
2024-12-07 10:26:15.566123: Pseudo dice [0.7536]
2024-12-07 10:26:15.566889: Epoch time: 294.99 s
2024-12-07 10:26:17.136130: 
2024-12-07 10:26:17.137668: Epoch 78
2024-12-07 10:26:17.138452: Current learning rate: 0.0093
2024-12-07 10:31:39.611362: Validation loss did not improve from -0.59150. Patience: 17/50
2024-12-07 10:31:39.612510: train_loss -0.7323
2024-12-07 10:31:39.613315: val_loss -0.5467
2024-12-07 10:31:39.614022: Pseudo dice [0.7742]
2024-12-07 10:31:39.614662: Epoch time: 322.48 s
2024-12-07 10:31:41.141520: 
2024-12-07 10:31:41.142909: Epoch 79
2024-12-07 10:31:41.143647: Current learning rate: 0.00929
2024-12-07 10:37:07.086479: Validation loss did not improve from -0.59150. Patience: 18/50
2024-12-07 10:37:07.087751: train_loss -0.7271
2024-12-07 10:37:07.088526: val_loss -0.5209
2024-12-07 10:37:07.089154: Pseudo dice [0.7677]
2024-12-07 10:37:07.089815: Epoch time: 325.95 s
2024-12-07 10:37:08.987369: 
2024-12-07 10:37:08.988293: Epoch 80
2024-12-07 10:37:08.988946: Current learning rate: 0.00928
2024-12-07 10:42:45.719551: Validation loss did not improve from -0.59150. Patience: 19/50
2024-12-07 10:42:45.720635: train_loss -0.7298
2024-12-07 10:42:45.721565: val_loss -0.5293
2024-12-07 10:42:45.722358: Pseudo dice [0.7628]
2024-12-07 10:42:45.723063: Epoch time: 336.73 s
2024-12-07 10:42:47.607632: 
2024-12-07 10:42:47.608877: Epoch 81
2024-12-07 10:42:47.609588: Current learning rate: 0.00927
2024-12-07 10:48:20.852292: Validation loss did not improve from -0.59150. Patience: 20/50
2024-12-07 10:48:20.854357: train_loss -0.7358
2024-12-07 10:48:20.855374: val_loss -0.5013
2024-12-07 10:48:20.856256: Pseudo dice [0.7545]
2024-12-07 10:48:20.857168: Epoch time: 333.25 s
2024-12-07 10:48:22.363817: 
2024-12-07 10:48:22.365308: Epoch 82
2024-12-07 10:48:22.366216: Current learning rate: 0.00926
2024-12-07 10:53:25.273959: Validation loss did not improve from -0.59150. Patience: 21/50
2024-12-07 10:53:25.288115: train_loss -0.7305
2024-12-07 10:53:25.290555: val_loss -0.5263
2024-12-07 10:53:25.291835: Pseudo dice [0.7725]
2024-12-07 10:53:25.292920: Epoch time: 302.93 s
2024-12-07 10:53:26.739455: 
2024-12-07 10:53:26.740909: Epoch 83
2024-12-07 10:53:26.741890: Current learning rate: 0.00925
2024-12-07 10:58:52.542743: Validation loss did not improve from -0.59150. Patience: 22/50
2024-12-07 10:58:52.543817: train_loss -0.7316
2024-12-07 10:58:52.544715: val_loss -0.5446
2024-12-07 10:58:52.545404: Pseudo dice [0.7686]
2024-12-07 10:58:52.546114: Epoch time: 325.81 s
2024-12-07 10:58:53.940732: 
2024-12-07 10:58:53.942146: Epoch 84
2024-12-07 10:58:53.942990: Current learning rate: 0.00924
2024-12-07 11:04:25.909513: Validation loss did not improve from -0.59150. Patience: 23/50
2024-12-07 11:04:25.910578: train_loss -0.7345
2024-12-07 11:04:25.911435: val_loss -0.5132
2024-12-07 11:04:25.912158: Pseudo dice [0.7616]
2024-12-07 11:04:25.912842: Epoch time: 331.97 s
2024-12-07 11:04:27.745127: 
2024-12-07 11:04:27.746426: Epoch 85
2024-12-07 11:04:27.747199: Current learning rate: 0.00923
2024-12-07 11:09:53.378298: Validation loss did not improve from -0.59150. Patience: 24/50
2024-12-07 11:09:53.379345: train_loss -0.7303
2024-12-07 11:09:53.380138: val_loss -0.5036
2024-12-07 11:09:53.380859: Pseudo dice [0.7543]
2024-12-07 11:09:53.381660: Epoch time: 325.64 s
2024-12-07 11:09:54.761670: 
2024-12-07 11:09:54.762975: Epoch 86
2024-12-07 11:09:54.763710: Current learning rate: 0.00922
2024-12-07 11:15:05.708843: Validation loss did not improve from -0.59150. Patience: 25/50
2024-12-07 11:15:05.710023: train_loss -0.7337
2024-12-07 11:15:05.711049: val_loss -0.5213
2024-12-07 11:15:05.711819: Pseudo dice [0.764]
2024-12-07 11:15:05.712605: Epoch time: 310.95 s
2024-12-07 11:15:07.193822: 
2024-12-07 11:15:07.195286: Epoch 87
2024-12-07 11:15:07.196122: Current learning rate: 0.00921
2024-12-07 11:20:16.520789: Validation loss did not improve from -0.59150. Patience: 26/50
2024-12-07 11:20:16.521789: train_loss -0.7374
2024-12-07 11:20:16.522732: val_loss -0.561
2024-12-07 11:20:16.523641: Pseudo dice [0.7855]
2024-12-07 11:20:16.524478: Epoch time: 309.33 s
2024-12-07 11:20:17.944639: 
2024-12-07 11:20:17.946008: Epoch 88
2024-12-07 11:20:17.946917: Current learning rate: 0.0092
2024-12-07 11:25:33.669221: Validation loss did not improve from -0.59150. Patience: 27/50
2024-12-07 11:25:33.670342: train_loss -0.7334
2024-12-07 11:25:33.671220: val_loss -0.4958
2024-12-07 11:25:33.671982: Pseudo dice [0.7588]
2024-12-07 11:25:33.672755: Epoch time: 315.73 s
2024-12-07 11:25:35.067414: 
2024-12-07 11:25:35.068877: Epoch 89
2024-12-07 11:25:35.069681: Current learning rate: 0.0092
2024-12-07 11:31:20.876402: Validation loss did not improve from -0.59150. Patience: 28/50
2024-12-07 11:31:20.877598: train_loss -0.7318
2024-12-07 11:31:20.878474: val_loss -0.504
2024-12-07 11:31:20.879370: Pseudo dice [0.7549]
2024-12-07 11:31:20.880086: Epoch time: 345.81 s
2024-12-07 11:31:22.705860: 
2024-12-07 11:31:22.707290: Epoch 90
2024-12-07 11:31:22.708124: Current learning rate: 0.00919
2024-12-07 11:36:47.231990: Validation loss did not improve from -0.59150. Patience: 29/50
2024-12-07 11:36:47.233001: train_loss -0.7308
2024-12-07 11:36:47.233881: val_loss -0.5504
2024-12-07 11:36:47.234537: Pseudo dice [0.7726]
2024-12-07 11:36:47.235252: Epoch time: 324.53 s
2024-12-07 11:36:48.626751: 
2024-12-07 11:36:48.627883: Epoch 91
2024-12-07 11:36:48.628716: Current learning rate: 0.00918
2024-12-07 11:41:58.158025: Validation loss did not improve from -0.59150. Patience: 30/50
2024-12-07 11:41:58.159144: train_loss -0.7354
2024-12-07 11:41:58.159922: val_loss -0.5289
2024-12-07 11:41:58.160567: Pseudo dice [0.7668]
2024-12-07 11:41:58.161350: Epoch time: 309.53 s
2024-12-07 11:41:59.564885: 
2024-12-07 11:41:59.566143: Epoch 92
2024-12-07 11:41:59.567038: Current learning rate: 0.00917
2024-12-07 11:47:18.587085: Validation loss did not improve from -0.59150. Patience: 31/50
2024-12-07 11:47:18.588142: train_loss -0.7397
2024-12-07 11:47:18.588930: val_loss -0.5549
2024-12-07 11:47:18.589624: Pseudo dice [0.7951]
2024-12-07 11:47:18.590393: Epoch time: 319.02 s
2024-12-07 11:47:20.403299: 
2024-12-07 11:47:20.404541: Epoch 93
2024-12-07 11:47:20.405419: Current learning rate: 0.00916
2024-12-07 11:52:43.702424: Validation loss did not improve from -0.59150. Patience: 32/50
2024-12-07 11:52:43.703799: train_loss -0.7402
2024-12-07 11:52:43.704797: val_loss -0.477
2024-12-07 11:52:43.705590: Pseudo dice [0.7497]
2024-12-07 11:52:43.706403: Epoch time: 323.3 s
2024-12-07 11:52:45.116721: 
2024-12-07 11:52:45.118031: Epoch 94
2024-12-07 11:52:45.118765: Current learning rate: 0.00915
2024-12-07 11:58:31.481987: Validation loss did not improve from -0.59150. Patience: 33/50
2024-12-07 11:58:31.484009: train_loss -0.7336
2024-12-07 11:58:31.486202: val_loss -0.4898
2024-12-07 11:58:31.487429: Pseudo dice [0.7433]
2024-12-07 11:58:31.488607: Epoch time: 346.37 s
2024-12-07 11:58:33.410988: 
2024-12-07 11:58:33.412398: Epoch 95
2024-12-07 11:58:33.413170: Current learning rate: 0.00914
2024-12-07 12:04:07.068694: Validation loss did not improve from -0.59150. Patience: 34/50
2024-12-07 12:04:07.069704: train_loss -0.7295
2024-12-07 12:04:07.070618: val_loss -0.476
2024-12-07 12:04:07.071586: Pseudo dice [0.7516]
2024-12-07 12:04:07.072578: Epoch time: 333.66 s
2024-12-07 12:04:08.474020: 
2024-12-07 12:04:08.475546: Epoch 96
2024-12-07 12:04:08.476608: Current learning rate: 0.00913
2024-12-07 12:09:17.633763: Validation loss did not improve from -0.59150. Patience: 35/50
2024-12-07 12:09:17.634732: train_loss -0.7311
2024-12-07 12:09:17.635480: val_loss -0.5448
2024-12-07 12:09:17.636148: Pseudo dice [0.7747]
2024-12-07 12:09:17.636970: Epoch time: 309.16 s
2024-12-07 12:09:19.025262: 
2024-12-07 12:09:19.026486: Epoch 97
2024-12-07 12:09:19.027276: Current learning rate: 0.00912
2024-12-07 12:14:12.216141: failed to log:  (<class 'FileNotFoundError'>, FileNotFoundError(2, 'No such file or directory'), <traceback object at 0x7f651a87f400>)
2024-12-07 12:14:12.216141: failed to log:  (<class 'FileNotFoundError'>, FileNotFoundError(2, 'No such file or directory'), <traceback object at 0x7f651a87f400>)
2024-12-07 12:14:12.216141: failed to log:  (<class 'FileNotFoundError'>, FileNotFoundError(2, 'No such file or directory'), <traceback object at 0x7f651a87f400>)
2024-12-07 12:14:12.216141: failed to log:  (<class 'FileNotFoundError'>, FileNotFoundError(2, 'No such file or directory'), <traceback object at 0x7f651a87f400>)
2024-12-07 12:14:12.216141: failed to log:  (<class 'FileNotFoundError'>, FileNotFoundError(2, 'No such file or directory'), <traceback object at 0x7f651a87f400>)
2024-12-07 12:14:12.216141: Validation loss did not improve from -0.59150. Patience: 36/50
2024-12-07 12:14:14.720225: failed to log:  (<class 'FileNotFoundError'>, FileNotFoundError(2, 'No such file or directory'), <traceback object at 0x7f651a87f400>)
2024-12-07 12:14:14.720225: failed to log:  (<class 'FileNotFoundError'>, FileNotFoundError(2, 'No such file or directory'), <traceback object at 0x7f651a87f400>)
2024-12-07 12:14:14.720225: failed to log:  (<class 'FileNotFoundError'>, FileNotFoundError(2, 'No such file or directory'), <traceback object at 0x7f651a87f400>)
2024-12-07 12:14:14.720225: failed to log:  (<class 'FileNotFoundError'>, FileNotFoundError(2, 'No such file or directory'), <traceback object at 0x7f651a87f400>)
2024-12-07 12:14:14.720225: failed to log:  (<class 'FileNotFoundError'>, FileNotFoundError(2, 'No such file or directory'), <traceback object at 0x7f651a87f400>)
2024-12-07 12:14:14.720225: train_loss -0.7407
2024-12-07 12:14:17.223084: failed to log:  (<class 'FileNotFoundError'>, FileNotFoundError(2, 'No such file or directory'), <traceback object at 0x7f65644ea500>)
2024-12-07 12:14:17.223084: failed to log:  (<class 'FileNotFoundError'>, FileNotFoundError(2, 'No such file or directory'), <traceback object at 0x7f65644ea500>)
2024-12-07 12:14:17.223084: failed to log:  (<class 'FileNotFoundError'>, FileNotFoundError(2, 'No such file or directory'), <traceback object at 0x7f65644ea500>)
2024-12-07 12:14:17.223084: failed to log:  (<class 'FileNotFoundError'>, FileNotFoundError(2, 'No such file or directory'), <traceback object at 0x7f65644ea500>)
2024-12-07 12:14:17.223084: failed to log:  (<class 'FileNotFoundError'>, FileNotFoundError(2, 'No such file or directory'), <traceback object at 0x7f65644ea500>)
2024-12-07 12:14:17.223084: val_loss -0.5517
2024-12-07 12:14:19.726096: failed to log:  (<class 'FileNotFoundError'>, FileNotFoundError(2, 'No such file or directory'), <traceback object at 0x7f65647f87c0>)
2024-12-07 12:14:19.726096: failed to log:  (<class 'FileNotFoundError'>, FileNotFoundError(2, 'No such file or directory'), <traceback object at 0x7f65647f87c0>)
2024-12-07 12:14:19.726096: failed to log:  (<class 'FileNotFoundError'>, FileNotFoundError(2, 'No such file or directory'), <traceback object at 0x7f65647f87c0>)
2024-12-07 12:14:19.726096: failed to log:  (<class 'FileNotFoundError'>, FileNotFoundError(2, 'No such file or directory'), <traceback object at 0x7f65647f87c0>)
2024-12-07 12:14:19.726096: failed to log:  (<class 'FileNotFoundError'>, FileNotFoundError(2, 'No such file or directory'), <traceback object at 0x7f65647f87c0>)
2024-12-07 12:14:19.726096: Pseudo dice [0.7773]
2024-12-07 12:14:22.229247: failed to log:  (<class 'FileNotFoundError'>, FileNotFoundError(2, 'No such file or directory'), <traceback object at 0x7f65648c2240>)
2024-12-07 12:14:22.229247: failed to log:  (<class 'FileNotFoundError'>, FileNotFoundError(2, 'No such file or directory'), <traceback object at 0x7f65648c2240>)
2024-12-07 12:14:22.229247: failed to log:  (<class 'FileNotFoundError'>, FileNotFoundError(2, 'No such file or directory'), <traceback object at 0x7f65648c2240>)
2024-12-07 12:14:22.229247: failed to log:  (<class 'FileNotFoundError'>, FileNotFoundError(2, 'No such file or directory'), <traceback object at 0x7f65648c2240>)
2024-12-07 12:14:22.229247: failed to log:  (<class 'FileNotFoundError'>, FileNotFoundError(2, 'No such file or directory'), <traceback object at 0x7f65648c2240>)
2024-12-07 12:14:22.229247: Epoch time: 295.7 s
Traceback (most recent call last):
  File "/home/gridsan/nchutisilp/.conda/envs/nnunet/bin/nnUNetv2_train", line 8, in <module>
    sys.exit(run_training_entry())
  File "/home/gridsan/nchutisilp/projects/nnUNet/nnunetv2/run/run_training.py", line 275, in run_training_entry
    run_training(args.dataset_name_or_id, args.configuration, args.fold, args.tr, args.p, args.pretrained_weights,
  File "/home/gridsan/nchutisilp/projects/nnUNet/nnunetv2/run/run_training.py", line 211, in run_training
    nnunet_trainer.run_training()
  File "/home/gridsan/nchutisilp/projects/nnUNet/nnunetv2/training/nnUNetTrainer/nnUNetTrainer.py", line 1411, in run_training
    self.on_epoch_end()
  File "/home/gridsan/nchutisilp/projects/nnUNet/nnunetv2/training/nnUNetTrainer/nnUNetTrainer.py", line 1174, in on_epoch_end
    self.logger.plot_progress_png(self.output_folder)
  File "/home/gridsan/nchutisilp/projects/nnUNet/nnunetv2/training/logging/nnunet_logger.py", line 99, in plot_progress_png
    fig.savefig(join(output_folder, "progress.png"))
  File "/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/matplotlib/figure.py", line 3390, in savefig
    self.canvas.print_figure(fname, **kwargs)
  File "/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/matplotlib/backend_bases.py", line 2193, in print_figure
    result = print_method(
  File "/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/matplotlib/backend_bases.py", line 2043, in <lambda>
    print_method = functools.wraps(meth)(lambda *args, **kwargs: meth(
  File "/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/matplotlib/backends/backend_agg.py", line 497, in print_png
    self._print_pil(filename_or_obj, "png", pil_kwargs, metadata)
  File "/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/matplotlib/backends/backend_agg.py", line 446, in _print_pil
    mpl.image.imsave(
  File "/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/matplotlib/image.py", line 1656, in imsave
    image.save(fname, **pil_kwargs)
  File "/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/PIL/Image.py", line 2456, in save
    fp = builtins.open(filename, "w+b")
FileNotFoundError: [Errno 2] No such file or directory: '/home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_results/Dataset307_Sohee_Calcium_OCT_CrossValidation/nnUNetTrainer__nnUNetPlans__3d_32x160x128_b10/fold_0/progress.png'
Exception in thread Thread-3:
Traceback (most recent call last):
  File "/home/gridsan/nchutisilp/.conda/envs/nnunet/lib/python3.9/threading.py", line 980, in _bootstrap_inner
Exception in thread Thread-2:
Traceback (most recent call last):
  File "/home/gridsan/nchutisilp/.conda/envs/nnunet/lib/python3.9/threading.py", line 980, in _bootstrap_inner
    self.run()
  File "/home/gridsan/nchutisilp/.conda/envs/nnunet/lib/python3.9/threading.py", line 917, in run
    self.run()
  File "/home/gridsan/nchutisilp/.conda/envs/nnunet/lib/python3.9/threading.py", line 917, in run
    self._target(*self._args, **self._kwargs)
  File "/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/batchgenerators/dataloading/nondet_multi_threaded_augmenter.py", line 125, in results_loop
    self._target(*self._args, **self._kwargs)
  File "/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/batchgenerators/dataloading/nondet_multi_threaded_augmenter.py", line 125, in results_loop
    raise e
  File "/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/batchgenerators/dataloading/nondet_multi_threaded_augmenter.py", line 103, in results_loop
    raise e
  File "/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/batchgenerators/dataloading/nondet_multi_threaded_augmenter.py", line 103, in results_loop
    raise RuntimeError("One or more background workers are no longer alive. Exiting. Please check the "
RuntimeError: One or more background workers are no longer alive. Exiting. Please check the print statements above for the actual error message
    raise RuntimeError("One or more background workers are no longer alive. Exiting. Please check the "
RuntimeError: One or more background workers are no longer alive. Exiting. Please check the print statements above for the actual error message
2024-12-07 03:13:05.472402: unpacking done...
2024-12-07 03:13:05.628843: Unable to plot network architecture: nnUNet_compile is enabled!
2024-12-07 03:13:05.745580: 
2024-12-07 03:13:05.746447: Epoch 0
2024-12-07 03:13:05.747288: Current learning rate: 0.01
2024-12-07 03:16:05.711592: Validation loss improved from 1000.00000 to -0.42302! Patience: 0/50
2024-12-07 03:16:05.712716: train_loss -0.3318
2024-12-07 03:16:05.713935: val_loss -0.423
2024-12-07 03:16:05.714953: Pseudo dice [0.6695]
2024-12-07 03:16:05.715931: Epoch time: 179.97 s
2024-12-07 03:16:05.716713: Yayy! New best EMA pseudo Dice: 0.6695
2024-12-07 03:16:07.408878: 
2024-12-07 03:16:07.410358: Epoch 1
2024-12-07 03:16:07.411253: Current learning rate: 0.00999
2024-12-07 03:18:14.570208: Validation loss improved from -0.42302 to -0.47093! Patience: 0/50
2024-12-07 03:18:14.571207: train_loss -0.4723
2024-12-07 03:18:14.572035: val_loss -0.4709
2024-12-07 03:18:14.572923: Pseudo dice [0.6981]
2024-12-07 03:18:14.573990: Epoch time: 127.16 s
2024-12-07 03:18:14.574945: Yayy! New best EMA pseudo Dice: 0.6723
2024-12-07 03:18:16.486489: 
2024-12-07 03:18:16.487798: Epoch 2
2024-12-07 03:18:16.488567: Current learning rate: 0.00998
2024-12-07 03:20:28.666962: Validation loss did not improve from -0.47093. Patience: 1/50
2024-12-07 03:20:28.668098: train_loss -0.5044
2024-12-07 03:20:28.668928: val_loss -0.4428
2024-12-07 03:20:28.669653: Pseudo dice [0.6808]
2024-12-07 03:20:28.670319: Epoch time: 132.18 s
2024-12-07 03:20:28.670991: Yayy! New best EMA pseudo Dice: 0.6732
2024-12-07 03:20:30.658303: 
2024-12-07 03:20:30.660178: Epoch 3
2024-12-07 03:20:30.660884: Current learning rate: 0.00997
2024-12-07 03:23:20.403239: Validation loss improved from -0.47093 to -0.49115! Patience: 1/50
2024-12-07 03:23:20.404671: train_loss -0.5189
2024-12-07 03:23:20.405572: val_loss -0.4912
2024-12-07 03:23:20.406265: Pseudo dice [0.7183]
2024-12-07 03:23:20.406857: Epoch time: 169.75 s
2024-12-07 03:23:20.407500: Yayy! New best EMA pseudo Dice: 0.6777
2024-12-07 03:23:22.291146: 
2024-12-07 03:23:22.292207: Epoch 4
2024-12-07 03:23:22.292974: Current learning rate: 0.00996
2024-12-07 03:26:28.635427: Validation loss improved from -0.49115 to -0.49612! Patience: 0/50
2024-12-07 03:26:28.636361: train_loss -0.536
2024-12-07 03:26:28.637244: val_loss -0.4961
2024-12-07 03:26:28.637892: Pseudo dice [0.7125]
2024-12-07 03:26:28.638554: Epoch time: 186.35 s
2024-12-07 03:26:29.077500: Yayy! New best EMA pseudo Dice: 0.6812
2024-12-07 03:26:30.952580: 
2024-12-07 03:26:30.954011: Epoch 5
2024-12-07 03:26:30.954852: Current learning rate: 0.00995
2024-12-07 03:29:51.904593: Validation loss did not improve from -0.49612. Patience: 1/50
2024-12-07 03:29:51.905531: train_loss -0.5514
2024-12-07 03:29:51.906529: val_loss -0.4799
2024-12-07 03:29:51.907343: Pseudo dice [0.6985]
2024-12-07 03:29:51.908040: Epoch time: 200.95 s
2024-12-07 03:29:51.908810: Yayy! New best EMA pseudo Dice: 0.6829
2024-12-07 03:29:53.680873: 
2024-12-07 03:29:53.682103: Epoch 6
2024-12-07 03:29:53.682980: Current learning rate: 0.00995
2024-12-07 03:32:49.871650: Validation loss improved from -0.49612 to -0.49725! Patience: 1/50
2024-12-07 03:32:49.872705: train_loss -0.558
2024-12-07 03:32:49.873485: val_loss -0.4972
2024-12-07 03:32:49.874131: Pseudo dice [0.7165]
2024-12-07 03:32:49.874864: Epoch time: 176.19 s
2024-12-07 03:32:49.875628: Yayy! New best EMA pseudo Dice: 0.6863
2024-12-07 03:32:51.696929: 
2024-12-07 03:32:51.698360: Epoch 7
2024-12-07 03:32:51.699357: Current learning rate: 0.00994
2024-12-07 03:36:18.594361: Validation loss did not improve from -0.49725. Patience: 1/50
2024-12-07 03:36:18.595427: train_loss -0.5667
2024-12-07 03:36:18.596204: val_loss -0.475
2024-12-07 03:36:18.596929: Pseudo dice [0.6993]
2024-12-07 03:36:18.597639: Epoch time: 206.9 s
2024-12-07 03:36:18.598514: Yayy! New best EMA pseudo Dice: 0.6876
2024-12-07 03:36:21.227018: 
2024-12-07 03:36:21.228528: Epoch 8
2024-12-07 03:36:21.229544: Current learning rate: 0.00993
2024-12-07 03:39:40.226488: Validation loss improved from -0.49725 to -0.51940! Patience: 1/50
2024-12-07 03:39:40.227614: train_loss -0.5736
2024-12-07 03:39:40.228594: val_loss -0.5194
2024-12-07 03:39:40.229353: Pseudo dice [0.7272]
2024-12-07 03:39:40.230132: Epoch time: 199.0 s
2024-12-07 03:39:40.230772: Yayy! New best EMA pseudo Dice: 0.6915
2024-12-07 03:39:42.144367: 
2024-12-07 03:39:42.146012: Epoch 9
2024-12-07 03:39:42.146922: Current learning rate: 0.00992
2024-12-07 03:43:14.439986: Validation loss did not improve from -0.51940. Patience: 1/50
2024-12-07 03:43:14.441015: train_loss -0.5755
2024-12-07 03:43:14.441848: val_loss -0.5138
2024-12-07 03:43:14.442616: Pseudo dice [0.7166]
2024-12-07 03:43:14.443359: Epoch time: 212.3 s
2024-12-07 03:43:14.877165: Yayy! New best EMA pseudo Dice: 0.694
2024-12-07 03:43:16.659838: 
2024-12-07 03:43:16.661088: Epoch 10
2024-12-07 03:43:16.661812: Current learning rate: 0.00991
2024-12-07 03:46:48.898418: Validation loss improved from -0.51940 to -0.51999! Patience: 1/50
2024-12-07 03:46:48.899501: train_loss -0.5901
2024-12-07 03:46:48.900554: val_loss -0.52
2024-12-07 03:46:48.901575: Pseudo dice [0.7265]
2024-12-07 03:46:48.902537: Epoch time: 212.24 s
2024-12-07 03:46:48.903483: Yayy! New best EMA pseudo Dice: 0.6973
2024-12-07 03:46:50.718459: 
2024-12-07 03:46:50.719755: Epoch 11
2024-12-07 03:46:50.720922: Current learning rate: 0.0099
2024-12-07 03:50:22.774843: Validation loss improved from -0.51999 to -0.53334! Patience: 0/50
2024-12-07 03:50:22.775914: train_loss -0.6042
2024-12-07 03:50:22.776796: val_loss -0.5333
2024-12-07 03:50:22.777635: Pseudo dice [0.7309]
2024-12-07 03:50:22.778379: Epoch time: 212.06 s
2024-12-07 03:50:22.779045: Yayy! New best EMA pseudo Dice: 0.7006
2024-12-07 03:50:24.573155: 
2024-12-07 03:50:24.574500: Epoch 12
2024-12-07 03:50:24.575212: Current learning rate: 0.00989
2024-12-07 03:55:06.456644: Validation loss did not improve from -0.53334. Patience: 1/50
2024-12-07 03:55:06.457823: train_loss -0.5953
2024-12-07 03:55:06.458752: val_loss -0.5121
2024-12-07 03:55:06.459591: Pseudo dice [0.7107]
2024-12-07 03:55:06.460449: Epoch time: 281.89 s
2024-12-07 03:55:06.461458: Yayy! New best EMA pseudo Dice: 0.7016
2024-12-07 03:55:08.464293: 
2024-12-07 03:55:08.466076: Epoch 13
2024-12-07 03:55:08.467307: Current learning rate: 0.00988
2024-12-07 04:00:09.604869: Validation loss did not improve from -0.53334. Patience: 2/50
2024-12-07 04:00:09.606081: train_loss -0.5993
2024-12-07 04:00:09.607165: val_loss -0.5078
2024-12-07 04:00:09.608108: Pseudo dice [0.7199]
2024-12-07 04:00:09.608966: Epoch time: 301.14 s
2024-12-07 04:00:09.609790: Yayy! New best EMA pseudo Dice: 0.7035
2024-12-07 04:00:11.537642: 
2024-12-07 04:00:11.539115: Epoch 14
2024-12-07 04:00:11.540033: Current learning rate: 0.00987
2024-12-07 04:05:33.335524: Validation loss did not improve from -0.53334. Patience: 3/50
2024-12-07 04:05:33.337462: train_loss -0.6116
2024-12-07 04:05:33.338520: val_loss -0.5248
2024-12-07 04:05:33.339455: Pseudo dice [0.7306]
2024-12-07 04:05:33.340468: Epoch time: 321.8 s
2024-12-07 04:05:33.774243: Yayy! New best EMA pseudo Dice: 0.7062
2024-12-07 04:05:35.626273: 
2024-12-07 04:05:35.627769: Epoch 15
2024-12-07 04:05:35.628550: Current learning rate: 0.00986
2024-12-07 04:11:00.832000: Validation loss did not improve from -0.53334. Patience: 4/50
2024-12-07 04:11:00.833118: train_loss -0.6149
2024-12-07 04:11:00.833954: val_loss -0.527
2024-12-07 04:11:00.834693: Pseudo dice [0.7271]
2024-12-07 04:11:00.835440: Epoch time: 325.21 s
2024-12-07 04:11:00.836118: Yayy! New best EMA pseudo Dice: 0.7083
2024-12-07 04:11:02.713947: 
2024-12-07 04:11:02.715392: Epoch 16
2024-12-07 04:11:02.716192: Current learning rate: 0.00986
2024-12-07 04:16:35.158876: Validation loss did not improve from -0.53334. Patience: 5/50
2024-12-07 04:16:35.160001: train_loss -0.6249
2024-12-07 04:16:35.160813: val_loss -0.5076
2024-12-07 04:16:35.161546: Pseudo dice [0.7218]
2024-12-07 04:16:35.162288: Epoch time: 332.45 s
2024-12-07 04:16:35.163105: Yayy! New best EMA pseudo Dice: 0.7096
2024-12-07 04:16:37.132633: 
2024-12-07 04:16:37.134758: Epoch 17
2024-12-07 04:16:37.135821: Current learning rate: 0.00985
2024-12-07 04:22:09.157507: Validation loss improved from -0.53334 to -0.53338! Patience: 5/50
2024-12-07 04:22:09.158666: train_loss -0.6293
2024-12-07 04:22:09.160319: val_loss -0.5334
2024-12-07 04:22:09.161046: Pseudo dice [0.7337]
2024-12-07 04:22:09.161915: Epoch time: 332.03 s
2024-12-07 04:22:09.162692: Yayy! New best EMA pseudo Dice: 0.712
2024-12-07 04:22:11.503790: 
2024-12-07 04:22:11.505099: Epoch 18
2024-12-07 04:22:11.506057: Current learning rate: 0.00984
2024-12-07 04:27:40.447562: Validation loss did not improve from -0.53338. Patience: 1/50
2024-12-07 04:27:40.448558: train_loss -0.6351
2024-12-07 04:27:40.449344: val_loss -0.497
2024-12-07 04:27:40.450026: Pseudo dice [0.7168]
2024-12-07 04:27:40.450709: Epoch time: 328.95 s
2024-12-07 04:27:40.451381: Yayy! New best EMA pseudo Dice: 0.7125
2024-12-07 04:27:42.305567: 
2024-12-07 04:27:42.307040: Epoch 19
2024-12-07 04:27:42.307881: Current learning rate: 0.00983
2024-12-07 04:33:27.959390: Validation loss improved from -0.53338 to -0.53873! Patience: 1/50
2024-12-07 04:33:27.960433: train_loss -0.6279
2024-12-07 04:33:27.961154: val_loss -0.5387
2024-12-07 04:33:27.962076: Pseudo dice [0.7306]
2024-12-07 04:33:27.962815: Epoch time: 345.66 s
2024-12-07 04:33:28.380300: Yayy! New best EMA pseudo Dice: 0.7143
2024-12-07 04:33:30.266563: 
2024-12-07 04:33:30.267907: Epoch 20
2024-12-07 04:33:30.268693: Current learning rate: 0.00982
2024-12-07 04:39:33.089379: Validation loss did not improve from -0.53873. Patience: 1/50
2024-12-07 04:39:33.090326: train_loss -0.6322
2024-12-07 04:39:33.091184: val_loss -0.5337
2024-12-07 04:39:33.091984: Pseudo dice [0.7303]
2024-12-07 04:39:33.092843: Epoch time: 362.83 s
2024-12-07 04:39:33.093670: Yayy! New best EMA pseudo Dice: 0.7159
2024-12-07 04:39:35.015106: 
2024-12-07 04:39:35.016545: Epoch 21
2024-12-07 04:39:35.017280: Current learning rate: 0.00981
2024-12-07 04:45:33.119791: Validation loss improved from -0.53873 to -0.55872! Patience: 1/50
2024-12-07 04:45:33.120963: train_loss -0.6282
2024-12-07 04:45:33.121764: val_loss -0.5587
2024-12-07 04:45:33.122582: Pseudo dice [0.7556]
2024-12-07 04:45:33.123312: Epoch time: 358.11 s
2024-12-07 04:45:33.123999: Yayy! New best EMA pseudo Dice: 0.7199
2024-12-07 04:45:35.018026: 
2024-12-07 04:45:35.019406: Epoch 22
2024-12-07 04:45:35.020176: Current learning rate: 0.0098
2024-12-07 04:51:14.520039: Validation loss did not improve from -0.55872. Patience: 1/50
2024-12-07 04:51:14.521138: train_loss -0.6396
2024-12-07 04:51:14.521944: val_loss -0.5514
2024-12-07 04:51:14.522672: Pseudo dice [0.7439]
2024-12-07 04:51:14.523308: Epoch time: 339.5 s
2024-12-07 04:51:14.524086: Yayy! New best EMA pseudo Dice: 0.7223
2024-12-07 04:51:16.348191: 
2024-12-07 04:51:16.349632: Epoch 23
2024-12-07 04:51:16.350551: Current learning rate: 0.00979
2024-12-07 04:56:31.799911: Validation loss did not improve from -0.55872. Patience: 2/50
2024-12-07 04:56:31.800900: train_loss -0.6417
2024-12-07 04:56:31.801759: val_loss -0.5423
2024-12-07 04:56:31.802533: Pseudo dice [0.7396]
2024-12-07 04:56:31.803308: Epoch time: 315.45 s
2024-12-07 04:56:31.803980: Yayy! New best EMA pseudo Dice: 0.724
2024-12-07 04:56:33.596095: 
2024-12-07 04:56:33.597550: Epoch 24
2024-12-07 04:56:33.598360: Current learning rate: 0.00978
2024-12-07 05:02:12.023315: Validation loss improved from -0.55872 to -0.57615! Patience: 2/50
2024-12-07 05:02:12.024345: train_loss -0.6356
2024-12-07 05:02:12.025074: val_loss -0.5761
2024-12-07 05:02:12.025881: Pseudo dice [0.7562]
2024-12-07 05:02:12.026675: Epoch time: 338.43 s
2024-12-07 05:02:12.443961: Yayy! New best EMA pseudo Dice: 0.7272
2024-12-07 05:02:14.216962: 
2024-12-07 05:02:14.218409: Epoch 25
2024-12-07 05:02:14.219460: Current learning rate: 0.00977
2024-12-07 05:07:54.724832: Validation loss did not improve from -0.57615. Patience: 1/50
2024-12-07 05:07:54.725906: train_loss -0.6397
2024-12-07 05:07:54.726843: val_loss -0.5605
2024-12-07 05:07:54.727575: Pseudo dice [0.7445]
2024-12-07 05:07:54.728431: Epoch time: 340.51 s
2024-12-07 05:07:54.729162: Yayy! New best EMA pseudo Dice: 0.729
2024-12-07 05:07:56.556175: 
2024-12-07 05:07:56.557422: Epoch 26
2024-12-07 05:07:56.558319: Current learning rate: 0.00977
2024-12-07 05:13:48.088643: Validation loss did not improve from -0.57615. Patience: 2/50
2024-12-07 05:13:48.089641: train_loss -0.6563
2024-12-07 05:13:48.090396: val_loss -0.5686
2024-12-07 05:13:48.091085: Pseudo dice [0.7533]
2024-12-07 05:13:48.091791: Epoch time: 351.53 s
2024-12-07 05:13:48.092437: Yayy! New best EMA pseudo Dice: 0.7314
2024-12-07 05:13:49.876058: 
2024-12-07 05:13:49.877402: Epoch 27
2024-12-07 05:13:49.878309: Current learning rate: 0.00976
2024-12-07 05:19:47.495570: Validation loss did not improve from -0.57615. Patience: 3/50
2024-12-07 05:19:47.496376: train_loss -0.6546
2024-12-07 05:19:47.497354: val_loss -0.572
2024-12-07 05:19:47.498131: Pseudo dice [0.761]
2024-12-07 05:19:47.498991: Epoch time: 357.62 s
2024-12-07 05:19:47.499822: Yayy! New best EMA pseudo Dice: 0.7344
2024-12-07 05:19:49.319805: 
2024-12-07 05:19:49.321151: Epoch 28
2024-12-07 05:19:49.321965: Current learning rate: 0.00975
2024-12-07 05:25:41.331420: Validation loss did not improve from -0.57615. Patience: 4/50
2024-12-07 05:25:41.335325: train_loss -0.6597
2024-12-07 05:25:41.336529: val_loss -0.5384
2024-12-07 05:25:41.337340: Pseudo dice [0.7384]
2024-12-07 05:25:41.338456: Epoch time: 352.02 s
2024-12-07 05:25:41.339428: Yayy! New best EMA pseudo Dice: 0.7348
2024-12-07 05:25:43.603326: 
2024-12-07 05:25:43.604865: Epoch 29
2024-12-07 05:25:43.605667: Current learning rate: 0.00974
2024-12-07 05:31:40.658542: Validation loss did not improve from -0.57615. Patience: 5/50
2024-12-07 05:31:40.659614: train_loss -0.6522
2024-12-07 05:31:40.660342: val_loss -0.5355
2024-12-07 05:31:40.660959: Pseudo dice [0.7468]
2024-12-07 05:31:40.661624: Epoch time: 357.06 s
2024-12-07 05:31:41.063321: Yayy! New best EMA pseudo Dice: 0.736
2024-12-07 05:31:42.892777: 
2024-12-07 05:31:42.894154: Epoch 30
2024-12-07 05:31:42.894941: Current learning rate: 0.00973
2024-12-07 05:37:44.961821: Validation loss did not improve from -0.57615. Patience: 6/50
2024-12-07 05:37:44.962855: train_loss -0.6585
2024-12-07 05:37:44.963798: val_loss -0.5405
2024-12-07 05:37:44.964502: Pseudo dice [0.74]
2024-12-07 05:37:44.965134: Epoch time: 362.07 s
2024-12-07 05:37:44.965873: Yayy! New best EMA pseudo Dice: 0.7364
2024-12-07 05:37:46.806215: 
2024-12-07 05:37:46.807703: Epoch 31
2024-12-07 05:37:46.808631: Current learning rate: 0.00972
2024-12-07 05:43:48.081826: Validation loss did not improve from -0.57615. Patience: 7/50
2024-12-07 05:43:48.082934: train_loss -0.6568
2024-12-07 05:43:48.083689: val_loss -0.5348
2024-12-07 05:43:48.084363: Pseudo dice [0.7308]
2024-12-07 05:43:48.085070: Epoch time: 361.28 s
2024-12-07 05:43:49.498774: 
2024-12-07 05:43:49.499845: Epoch 32
2024-12-07 05:43:49.500602: Current learning rate: 0.00971
2024-12-07 05:49:57.274313: Validation loss did not improve from -0.57615. Patience: 8/50
2024-12-07 05:49:57.275342: train_loss -0.6536
2024-12-07 05:49:57.276244: val_loss -0.5312
2024-12-07 05:49:57.277044: Pseudo dice [0.7293]
2024-12-07 05:49:57.277685: Epoch time: 367.78 s
2024-12-07 05:49:58.704817: 
2024-12-07 05:49:58.706231: Epoch 33
2024-12-07 05:49:58.706926: Current learning rate: 0.0097
2024-12-07 05:56:13.760937: Validation loss did not improve from -0.57615. Patience: 9/50
2024-12-07 05:56:13.761939: train_loss -0.6624
2024-12-07 05:56:13.762872: val_loss -0.569
2024-12-07 05:56:13.763818: Pseudo dice [0.7534]
2024-12-07 05:56:13.764602: Epoch time: 375.06 s
2024-12-07 05:56:13.765362: Yayy! New best EMA pseudo Dice: 0.737
2024-12-07 05:56:15.611483: 
2024-12-07 05:56:15.612966: Epoch 34
2024-12-07 05:56:15.613741: Current learning rate: 0.00969
2024-12-07 06:02:34.463549: Validation loss did not improve from -0.57615. Patience: 10/50
2024-12-07 06:02:34.464508: train_loss -0.6604
2024-12-07 06:02:34.465318: val_loss -0.5489
2024-12-07 06:02:34.466361: Pseudo dice [0.7422]
2024-12-07 06:02:34.467059: Epoch time: 378.85 s
2024-12-07 06:02:34.845558: Yayy! New best EMA pseudo Dice: 0.7375
2024-12-07 06:02:36.689048: 
2024-12-07 06:02:36.690394: Epoch 35
2024-12-07 06:02:36.691206: Current learning rate: 0.00968
2024-12-07 06:08:46.567530: Validation loss did not improve from -0.57615. Patience: 11/50
2024-12-07 06:08:46.568841: train_loss -0.6614
2024-12-07 06:08:46.569813: val_loss -0.554
2024-12-07 06:08:46.570700: Pseudo dice [0.7456]
2024-12-07 06:08:46.571678: Epoch time: 369.88 s
2024-12-07 06:08:46.572510: Yayy! New best EMA pseudo Dice: 0.7383
2024-12-07 06:08:48.490817: 
2024-12-07 06:08:48.492137: Epoch 36
2024-12-07 06:08:48.492886: Current learning rate: 0.00968
2024-12-07 06:14:38.055586: Validation loss did not improve from -0.57615. Patience: 12/50
2024-12-07 06:14:38.056675: train_loss -0.6657
2024-12-07 06:14:38.057656: val_loss -0.5651
2024-12-07 06:14:38.058563: Pseudo dice [0.7529]
2024-12-07 06:14:38.059758: Epoch time: 349.57 s
2024-12-07 06:14:38.060950: Yayy! New best EMA pseudo Dice: 0.7398
2024-12-07 06:14:39.983061: 
2024-12-07 06:14:39.984520: Epoch 37
2024-12-07 06:14:39.985631: Current learning rate: 0.00967
2024-12-07 06:20:16.795573: Validation loss did not improve from -0.57615. Patience: 13/50
2024-12-07 06:20:16.796520: train_loss -0.6748
2024-12-07 06:20:16.797265: val_loss -0.543
2024-12-07 06:20:16.797882: Pseudo dice [0.7469]
2024-12-07 06:20:16.798618: Epoch time: 336.81 s
2024-12-07 06:20:16.799402: Yayy! New best EMA pseudo Dice: 0.7405
2024-12-07 06:20:18.689653: 
2024-12-07 06:20:18.691007: Epoch 38
2024-12-07 06:20:18.691730: Current learning rate: 0.00966
2024-12-07 06:25:57.823099: Validation loss did not improve from -0.57615. Patience: 14/50
2024-12-07 06:25:57.824133: train_loss -0.6793
2024-12-07 06:25:57.824970: val_loss -0.5336
2024-12-07 06:25:57.825680: Pseudo dice [0.7346]
2024-12-07 06:25:57.826442: Epoch time: 339.14 s
2024-12-07 06:25:59.361760: 
2024-12-07 06:25:59.363108: Epoch 39
2024-12-07 06:25:59.363968: Current learning rate: 0.00965
2024-12-07 06:31:31.182059: Validation loss did not improve from -0.57615. Patience: 15/50
2024-12-07 06:31:31.186684: train_loss -0.6704
2024-12-07 06:31:31.188560: val_loss -0.5368
2024-12-07 06:31:31.189245: Pseudo dice [0.7394]
2024-12-07 06:31:31.190240: Epoch time: 331.83 s
2024-12-07 06:31:33.560325: 
2024-12-07 06:31:33.561709: Epoch 40
2024-12-07 06:31:33.562552: Current learning rate: 0.00964
2024-12-07 06:37:25.254405: Validation loss did not improve from -0.57615. Patience: 16/50
2024-12-07 06:37:25.255639: train_loss -0.6753
2024-12-07 06:37:25.256838: val_loss -0.5592
2024-12-07 06:37:25.257741: Pseudo dice [0.7489]
2024-12-07 06:37:25.258706: Epoch time: 351.7 s
2024-12-07 06:37:25.259794: Yayy! New best EMA pseudo Dice: 0.7408
2024-12-07 06:37:27.177832: 
2024-12-07 06:37:27.179481: Epoch 41
2024-12-07 06:37:27.180514: Current learning rate: 0.00963
2024-12-07 06:43:37.209922: Validation loss did not improve from -0.57615. Patience: 17/50
2024-12-07 06:43:37.210964: train_loss -0.6809
2024-12-07 06:43:37.211887: val_loss -0.5626
2024-12-07 06:43:37.212849: Pseudo dice [0.7513]
2024-12-07 06:43:37.213938: Epoch time: 370.03 s
2024-12-07 06:43:37.214907: Yayy! New best EMA pseudo Dice: 0.7418
2024-12-07 06:43:39.080371: 
2024-12-07 06:43:39.081700: Epoch 42
2024-12-07 06:43:39.082494: Current learning rate: 0.00962
2024-12-07 06:49:48.121564: Validation loss improved from -0.57615 to -0.58573! Patience: 17/50
2024-12-07 06:49:48.122694: train_loss -0.6773
2024-12-07 06:49:48.123567: val_loss -0.5857
2024-12-07 06:49:48.124233: Pseudo dice [0.7607]
2024-12-07 06:49:48.124975: Epoch time: 369.04 s
2024-12-07 06:49:48.125639: Yayy! New best EMA pseudo Dice: 0.7437
2024-12-07 06:49:49.946958: 
2024-12-07 06:49:49.948438: Epoch 43
2024-12-07 06:49:49.949240: Current learning rate: 0.00961
2024-12-07 06:55:58.798595: Validation loss did not improve from -0.58573. Patience: 1/50
2024-12-07 06:55:58.799681: train_loss -0.6841
2024-12-07 06:55:58.800597: val_loss -0.5372
2024-12-07 06:55:58.801595: Pseudo dice [0.7394]
2024-12-07 06:55:58.802485: Epoch time: 368.85 s
2024-12-07 06:56:00.219149: 
2024-12-07 06:56:00.220604: Epoch 44
2024-12-07 06:56:00.221766: Current learning rate: 0.0096
2024-12-07 07:02:24.624561: Validation loss did not improve from -0.58573. Patience: 2/50
2024-12-07 07:02:24.625542: train_loss -0.6908
2024-12-07 07:02:24.626388: val_loss -0.5493
2024-12-07 07:02:24.627093: Pseudo dice [0.7425]
2024-12-07 07:02:24.627915: Epoch time: 384.41 s
2024-12-07 07:02:26.458518: 
2024-12-07 07:02:26.459705: Epoch 45
2024-12-07 07:02:26.460501: Current learning rate: 0.00959
2024-12-07 07:09:05.051060: Validation loss did not improve from -0.58573. Patience: 3/50
2024-12-07 07:09:05.052074: train_loss -0.6917
2024-12-07 07:09:05.053303: val_loss -0.5418
2024-12-07 07:09:05.054556: Pseudo dice [0.7444]
2024-12-07 07:09:05.055614: Epoch time: 398.6 s
2024-12-07 07:09:06.458989: 
2024-12-07 07:09:06.460546: Epoch 46
2024-12-07 07:09:06.461641: Current learning rate: 0.00959
2024-12-07 07:15:45.657005: Validation loss did not improve from -0.58573. Patience: 4/50
2024-12-07 07:15:45.658035: train_loss -0.6954
2024-12-07 07:15:45.659001: val_loss -0.559
2024-12-07 07:15:45.659932: Pseudo dice [0.7579]
2024-12-07 07:15:45.660779: Epoch time: 399.2 s
2024-12-07 07:15:45.661585: Yayy! New best EMA pseudo Dice: 0.7448
2024-12-07 07:15:47.422776: 
2024-12-07 07:15:47.424089: Epoch 47
2024-12-07 07:15:47.425044: Current learning rate: 0.00958
2024-12-07 07:22:39.114367: Validation loss did not improve from -0.58573. Patience: 5/50
2024-12-07 07:22:39.115414: train_loss -0.6874
2024-12-07 07:22:39.116273: val_loss -0.5433
2024-12-07 07:22:39.117011: Pseudo dice [0.7457]
2024-12-07 07:22:39.117700: Epoch time: 411.69 s
2024-12-07 07:22:39.118439: Yayy! New best EMA pseudo Dice: 0.7449
2024-12-07 07:22:40.945721: 
2024-12-07 07:22:40.947135: Epoch 48
2024-12-07 07:22:40.947939: Current learning rate: 0.00957
2024-12-07 07:29:22.121430: Validation loss did not improve from -0.58573. Patience: 6/50
2024-12-07 07:29:22.122280: train_loss -0.6812
2024-12-07 07:29:22.123013: val_loss -0.5708
2024-12-07 07:29:22.123749: Pseudo dice [0.7533]
2024-12-07 07:29:22.124509: Epoch time: 401.18 s
2024-12-07 07:29:22.125297: Yayy! New best EMA pseudo Dice: 0.7457
2024-12-07 07:29:23.934105: 
2024-12-07 07:29:23.935472: Epoch 49
2024-12-07 07:29:23.936155: Current learning rate: 0.00956
2024-12-07 07:35:56.739790: Validation loss did not improve from -0.58573. Patience: 7/50
2024-12-07 07:35:56.746066: train_loss -0.6832
2024-12-07 07:35:56.747388: val_loss -0.5293
2024-12-07 07:35:56.748150: Pseudo dice [0.7354]
2024-12-07 07:35:56.749205: Epoch time: 392.81 s
2024-12-07 07:35:59.132458: 
2024-12-07 07:35:59.134120: Epoch 50
2024-12-07 07:35:59.135110: Current learning rate: 0.00955
2024-12-07 07:42:19.374534: Validation loss did not improve from -0.58573. Patience: 8/50
2024-12-07 07:42:19.375530: train_loss -0.6847
2024-12-07 07:42:19.376209: val_loss -0.5848
2024-12-07 07:42:19.376876: Pseudo dice [0.7551]
2024-12-07 07:42:19.377563: Epoch time: 380.24 s
2024-12-07 07:42:19.378332: Yayy! New best EMA pseudo Dice: 0.7457
2024-12-07 07:42:21.163754: 
2024-12-07 07:42:21.165121: Epoch 51
2024-12-07 07:42:21.165921: Current learning rate: 0.00954
2024-12-07 07:48:30.072572: Validation loss did not improve from -0.58573. Patience: 9/50
2024-12-07 07:48:30.073652: train_loss -0.6888
2024-12-07 07:48:30.074422: val_loss -0.5738
2024-12-07 07:48:30.075087: Pseudo dice [0.7555]
2024-12-07 07:48:30.075759: Epoch time: 368.91 s
2024-12-07 07:48:30.076425: Yayy! New best EMA pseudo Dice: 0.7467
2024-12-07 07:48:31.904882: 
2024-12-07 07:48:31.906381: Epoch 52
2024-12-07 07:48:31.907127: Current learning rate: 0.00953
2024-12-07 07:54:33.591933: Validation loss did not improve from -0.58573. Patience: 10/50
2024-12-07 07:54:33.593098: train_loss -0.6893
2024-12-07 07:54:33.593880: val_loss -0.5816
2024-12-07 07:54:33.594559: Pseudo dice [0.7616]
2024-12-07 07:54:33.595345: Epoch time: 361.69 s
2024-12-07 07:54:33.596207: Yayy! New best EMA pseudo Dice: 0.7482
2024-12-07 07:54:35.423912: 
2024-12-07 07:54:35.425562: Epoch 53
2024-12-07 07:54:35.426708: Current learning rate: 0.00952
2024-12-07 08:00:31.104897: Validation loss did not improve from -0.58573. Patience: 11/50
2024-12-07 08:00:31.106040: train_loss -0.6948
2024-12-07 08:00:31.107083: val_loss -0.5685
2024-12-07 08:00:31.108010: Pseudo dice [0.7594]
2024-12-07 08:00:31.108947: Epoch time: 355.68 s
2024-12-07 08:00:31.109891: Yayy! New best EMA pseudo Dice: 0.7493
2024-12-07 08:00:32.971654: 
2024-12-07 08:00:32.972531: Epoch 54
2024-12-07 08:00:32.973324: Current learning rate: 0.00951
2024-12-07 08:06:45.618426: Validation loss did not improve from -0.58573. Patience: 12/50
2024-12-07 08:06:45.619572: train_loss -0.6961
2024-12-07 08:06:45.620445: val_loss -0.5657
2024-12-07 08:06:45.621141: Pseudo dice [0.756]
2024-12-07 08:06:45.621830: Epoch time: 372.65 s
2024-12-07 08:06:46.048429: Yayy! New best EMA pseudo Dice: 0.75
2024-12-07 08:06:47.883282: 
2024-12-07 08:06:47.884638: Epoch 55
2024-12-07 08:06:47.885530: Current learning rate: 0.0095
2024-12-07 08:13:25.193616: Validation loss did not improve from -0.58573. Patience: 13/50
2024-12-07 08:13:25.195130: train_loss -0.701
2024-12-07 08:13:25.196079: val_loss -0.5559
2024-12-07 08:13:25.196801: Pseudo dice [0.7487]
2024-12-07 08:13:25.197622: Epoch time: 397.31 s
2024-12-07 08:13:26.595800: 
2024-12-07 08:13:26.597285: Epoch 56
2024-12-07 08:13:26.598312: Current learning rate: 0.00949
2024-12-07 08:20:15.116062: Validation loss did not improve from -0.58573. Patience: 14/50
2024-12-07 08:20:15.117004: train_loss -0.6953
2024-12-07 08:20:15.117883: val_loss -0.5632
2024-12-07 08:20:15.118704: Pseudo dice [0.7471]
2024-12-07 08:20:15.119404: Epoch time: 408.52 s
2024-12-07 08:20:16.541042: 
2024-12-07 08:20:16.542475: Epoch 57
2024-12-07 08:20:16.543327: Current learning rate: 0.00949
2024-12-07 08:26:55.437542: Validation loss did not improve from -0.58573. Patience: 15/50
2024-12-07 08:26:55.438608: train_loss -0.6984
2024-12-07 08:26:55.439411: val_loss -0.5712
2024-12-07 08:26:55.440055: Pseudo dice [0.7622]
2024-12-07 08:26:55.440774: Epoch time: 398.9 s
2024-12-07 08:26:55.441463: Yayy! New best EMA pseudo Dice: 0.7509
2024-12-07 08:26:57.265935: 
2024-12-07 08:26:57.267533: Epoch 58
2024-12-07 08:26:57.268443: Current learning rate: 0.00948
2024-12-07 08:33:42.601854: Validation loss did not improve from -0.58573. Patience: 16/50
2024-12-07 08:33:42.603044: train_loss -0.7008
2024-12-07 08:33:42.604027: val_loss -0.5742
2024-12-07 08:33:42.604623: Pseudo dice [0.7628]
2024-12-07 08:33:42.605337: Epoch time: 405.34 s
2024-12-07 08:33:42.605962: Yayy! New best EMA pseudo Dice: 0.752
2024-12-07 08:33:44.500883: 
2024-12-07 08:33:44.502191: Epoch 59
2024-12-07 08:33:44.502924: Current learning rate: 0.00947
2024-12-07 08:40:18.207073: Validation loss did not improve from -0.58573. Patience: 17/50
2024-12-07 08:40:18.245696: train_loss -0.6948
2024-12-07 08:40:18.247045: val_loss -0.5552
2024-12-07 08:40:18.248021: Pseudo dice [0.7433]
2024-12-07 08:40:18.249082: Epoch time: 393.75 s
2024-12-07 08:40:20.168524: 
2024-12-07 08:40:20.169769: Epoch 60
2024-12-07 08:40:20.170633: Current learning rate: 0.00946
2024-12-07 08:46:14.789876: Validation loss did not improve from -0.58573. Patience: 18/50
2024-12-07 08:46:14.790874: train_loss -0.6984
2024-12-07 08:46:14.791609: val_loss -0.5545
2024-12-07 08:46:14.792306: Pseudo dice [0.7575]
2024-12-07 08:46:14.793152: Epoch time: 354.62 s
2024-12-07 08:46:16.654950: 
2024-12-07 08:46:16.656421: Epoch 61
2024-12-07 08:46:16.657195: Current learning rate: 0.00945
2024-12-07 08:52:05.549924: Validation loss did not improve from -0.58573. Patience: 19/50
2024-12-07 08:52:05.551053: train_loss -0.7012
2024-12-07 08:52:05.551809: val_loss -0.5584
2024-12-07 08:52:05.552617: Pseudo dice [0.7487]
2024-12-07 08:52:05.553323: Epoch time: 348.9 s
2024-12-07 08:52:07.007475: 
2024-12-07 08:52:07.008869: Epoch 62
2024-12-07 08:52:07.009569: Current learning rate: 0.00944
2024-12-07 08:58:11.544871: Validation loss did not improve from -0.58573. Patience: 20/50
2024-12-07 08:58:11.546016: train_loss -0.7013
2024-12-07 08:58:11.546963: val_loss -0.5658
2024-12-07 08:58:11.547937: Pseudo dice [0.7611]
2024-12-07 08:58:11.548907: Epoch time: 364.54 s
2024-12-07 08:58:11.549795: Yayy! New best EMA pseudo Dice: 0.7525
2024-12-07 08:58:13.424311: 
2024-12-07 08:58:13.425940: Epoch 63
2024-12-07 08:58:13.427143: Current learning rate: 0.00943
2024-12-07 09:04:27.079673: Validation loss did not improve from -0.58573. Patience: 21/50
2024-12-07 09:04:27.080833: train_loss -0.7041
2024-12-07 09:04:27.081680: val_loss -0.5564
2024-12-07 09:04:27.082515: Pseudo dice [0.7548]
2024-12-07 09:04:27.083323: Epoch time: 373.66 s
2024-12-07 09:04:27.084193: Yayy! New best EMA pseudo Dice: 0.7527
2024-12-07 09:04:28.938589: 
2024-12-07 09:04:28.939729: Epoch 64
2024-12-07 09:04:28.940479: Current learning rate: 0.00942
2024-12-07 09:10:43.751089: Validation loss did not improve from -0.58573. Patience: 22/50
2024-12-07 09:10:43.752084: train_loss -0.7068
2024-12-07 09:10:43.752845: val_loss -0.5493
2024-12-07 09:10:43.753645: Pseudo dice [0.7422]
2024-12-07 09:10:43.754419: Epoch time: 374.81 s
2024-12-07 09:10:45.640638: 
2024-12-07 09:10:45.641917: Epoch 65
2024-12-07 09:10:45.642955: Current learning rate: 0.00941
2024-12-07 09:16:55.634962: Validation loss did not improve from -0.58573. Patience: 23/50
2024-12-07 09:16:55.636043: train_loss -0.7057
2024-12-07 09:16:55.637001: val_loss -0.5562
2024-12-07 09:16:55.637800: Pseudo dice [0.7493]
2024-12-07 09:16:55.638690: Epoch time: 370.0 s
2024-12-07 09:16:57.072289: 
2024-12-07 09:16:57.090719: Epoch 66
2024-12-07 09:16:57.091754: Current learning rate: 0.0094
2024-12-07 09:22:20.330182: Validation loss did not improve from -0.58573. Patience: 24/50
2024-12-07 09:22:20.330928: train_loss -0.7082
2024-12-07 09:22:20.331696: val_loss -0.5609
2024-12-07 09:22:20.332436: Pseudo dice [0.7623]
2024-12-07 09:22:20.333113: Epoch time: 323.26 s
2024-12-07 09:22:21.787692: 
2024-12-07 09:22:21.789227: Epoch 67
2024-12-07 09:22:21.790324: Current learning rate: 0.00939
2024-12-07 09:27:27.763397: Validation loss did not improve from -0.58573. Patience: 25/50
2024-12-07 09:27:27.764434: train_loss -0.707
2024-12-07 09:27:27.765442: val_loss -0.5649
2024-12-07 09:27:27.766541: Pseudo dice [0.7546]
2024-12-07 09:27:27.767459: Epoch time: 305.98 s
2024-12-07 09:27:27.768179: Yayy! New best EMA pseudo Dice: 0.7527
2024-12-07 09:27:29.674629: 
2024-12-07 09:27:29.676086: Epoch 68
2024-12-07 09:27:29.676952: Current learning rate: 0.00939
2024-12-07 09:33:13.423737: Validation loss did not improve from -0.58573. Patience: 26/50
2024-12-07 09:33:13.424764: train_loss -0.7098
2024-12-07 09:33:13.425597: val_loss -0.5688
2024-12-07 09:33:13.426395: Pseudo dice [0.7538]
2024-12-07 09:33:13.427132: Epoch time: 343.75 s
2024-12-07 09:33:13.427866: Yayy! New best EMA pseudo Dice: 0.7528
2024-12-07 09:33:15.306334: 
2024-12-07 09:33:15.307765: Epoch 69
2024-12-07 09:33:15.308571: Current learning rate: 0.00938
2024-12-07 09:39:15.188836: Validation loss did not improve from -0.58573. Patience: 27/50
2024-12-07 09:39:15.189945: train_loss -0.7127
2024-12-07 09:39:15.190860: val_loss -0.5632
2024-12-07 09:39:15.191732: Pseudo dice [0.7582]
2024-12-07 09:39:15.192627: Epoch time: 359.88 s
2024-12-07 09:39:15.605475: Yayy! New best EMA pseudo Dice: 0.7534
2024-12-07 09:39:17.483950: 
2024-12-07 09:39:17.485461: Epoch 70
2024-12-07 09:39:17.486322: Current learning rate: 0.00937
2024-12-07 09:44:40.586301: Validation loss did not improve from -0.58573. Patience: 28/50
2024-12-07 09:44:40.589028: train_loss -0.7156
2024-12-07 09:44:40.589927: val_loss -0.5385
2024-12-07 09:44:40.590638: Pseudo dice [0.7432]
2024-12-07 09:44:40.591466: Epoch time: 323.11 s
2024-12-07 09:44:42.094346: 
2024-12-07 09:44:42.095681: Epoch 71
2024-12-07 09:44:42.096414: Current learning rate: 0.00936
2024-12-07 09:49:52.135992: Validation loss did not improve from -0.58573. Patience: 29/50
2024-12-07 09:49:52.137218: train_loss -0.7129
2024-12-07 09:49:52.138110: val_loss -0.5624
2024-12-07 09:49:52.138875: Pseudo dice [0.745]
2024-12-07 09:49:52.139583: Epoch time: 310.04 s
2024-12-07 09:49:54.031672: 
2024-12-07 09:49:54.032667: Epoch 72
2024-12-07 09:49:54.033516: Current learning rate: 0.00935
2024-12-07 09:54:57.047970: Validation loss did not improve from -0.58573. Patience: 30/50
2024-12-07 09:54:57.049032: train_loss -0.715
2024-12-07 09:54:57.049852: val_loss -0.5582
2024-12-07 09:54:57.050575: Pseudo dice [0.7599]
2024-12-07 09:54:57.051269: Epoch time: 303.02 s
2024-12-07 09:54:58.545997: 
2024-12-07 09:54:58.547309: Epoch 73
2024-12-07 09:54:58.548178: Current learning rate: 0.00934
2024-12-07 10:00:09.920272: Validation loss improved from -0.58573 to -0.59148! Patience: 30/50
2024-12-07 10:00:09.921365: train_loss -0.7202
2024-12-07 10:00:09.922196: val_loss -0.5915
2024-12-07 10:00:09.922955: Pseudo dice [0.7685]
2024-12-07 10:00:09.923626: Epoch time: 311.38 s
2024-12-07 10:00:09.924245: Yayy! New best EMA pseudo Dice: 0.754
2024-12-07 10:00:12.113858: 
2024-12-07 10:00:12.115381: Epoch 74
2024-12-07 10:00:12.116292: Current learning rate: 0.00933
2024-12-07 10:06:12.860819: Validation loss did not improve from -0.59148. Patience: 1/50
2024-12-07 10:06:12.861984: train_loss -0.7241
2024-12-07 10:06:12.863196: val_loss -0.5161
2024-12-07 10:06:12.864308: Pseudo dice [0.7332]
2024-12-07 10:06:12.865674: Epoch time: 360.75 s
2024-12-07 10:06:14.838062: 
2024-12-07 10:06:14.839619: Epoch 75
2024-12-07 10:06:14.840631: Current learning rate: 0.00932
2024-12-07 10:12:02.474987: Validation loss did not improve from -0.59148. Patience: 2/50
2024-12-07 10:12:02.476182: train_loss -0.7121
2024-12-07 10:12:02.477397: val_loss -0.5443
2024-12-07 10:12:02.478267: Pseudo dice [0.7395]
2024-12-07 10:12:02.479282: Epoch time: 347.64 s
2024-12-07 10:12:04.081747: 
2024-12-07 10:12:04.083335: Epoch 76
2024-12-07 10:12:04.084399: Current learning rate: 0.00931
2024-12-07 10:16:52.920731: Validation loss did not improve from -0.59148. Patience: 3/50
2024-12-07 10:16:52.921785: train_loss -0.7193
2024-12-07 10:16:52.922573: val_loss -0.5384
2024-12-07 10:16:52.923285: Pseudo dice [0.7459]
2024-12-07 10:16:52.924043: Epoch time: 288.84 s
2024-12-07 10:16:54.455168: 
2024-12-07 10:16:54.456629: Epoch 77
2024-12-07 10:16:54.457558: Current learning rate: 0.0093
2024-12-07 10:22:15.688423: Validation loss improved from -0.59148 to -0.60575! Patience: 3/50
2024-12-07 10:22:15.689720: train_loss -0.7189
2024-12-07 10:22:15.690689: val_loss -0.6058
2024-12-07 10:22:15.691561: Pseudo dice [0.7748]
2024-12-07 10:22:15.692452: Epoch time: 321.24 s
2024-12-07 10:22:17.213263: 
2024-12-07 10:22:17.214551: Epoch 78
2024-12-07 10:22:17.215237: Current learning rate: 0.0093
2024-12-07 10:27:17.462007: Validation loss did not improve from -0.60575. Patience: 1/50
2024-12-07 10:27:17.463101: train_loss -0.7233
2024-12-07 10:27:17.463899: val_loss -0.5339
2024-12-07 10:27:17.464627: Pseudo dice [0.7447]
2024-12-07 10:27:17.465403: Epoch time: 300.25 s
2024-12-07 10:27:19.033351: 
2024-12-07 10:27:19.035095: Epoch 79
2024-12-07 10:27:19.036226: Current learning rate: 0.00929
2024-12-07 10:33:15.374447: Validation loss did not improve from -0.60575. Patience: 2/50
2024-12-07 10:33:15.375412: train_loss -0.7215
2024-12-07 10:33:15.376383: val_loss -0.577
2024-12-07 10:33:15.377298: Pseudo dice [0.7578]
2024-12-07 10:33:15.378265: Epoch time: 356.34 s
2024-12-07 10:33:17.333519: 
2024-12-07 10:33:17.334784: Epoch 80
2024-12-07 10:33:17.335697: Current learning rate: 0.00928
2024-12-07 10:39:02.670393: Validation loss did not improve from -0.60575. Patience: 3/50
2024-12-07 10:39:02.671344: train_loss -0.7152
2024-12-07 10:39:02.672109: val_loss -0.5615
2024-12-07 10:39:02.672937: Pseudo dice [0.7574]
2024-12-07 10:39:02.673698: Epoch time: 345.34 s
2024-12-07 10:39:04.180610: 
2024-12-07 10:39:04.182216: Epoch 81
2024-12-07 10:39:04.183304: Current learning rate: 0.00927
2024-12-07 10:44:14.023959: Validation loss did not improve from -0.60575. Patience: 4/50
2024-12-07 10:44:14.025019: train_loss -0.7208
2024-12-07 10:44:14.026152: val_loss -0.5568
2024-12-07 10:44:14.027067: Pseudo dice [0.7513]
2024-12-07 10:44:14.028095: Epoch time: 309.85 s
2024-12-07 10:44:16.035697: 
2024-12-07 10:44:16.037462: Epoch 82
2024-12-07 10:44:16.038351: Current learning rate: 0.00926
2024-12-07 10:49:20.862638: Validation loss did not improve from -0.60575. Patience: 5/50
2024-12-07 10:49:20.864643: train_loss -0.7252
2024-12-07 10:49:20.865572: val_loss -0.5884
2024-12-07 10:49:20.866214: Pseudo dice [0.7673]
2024-12-07 10:49:20.866853: Epoch time: 304.83 s
2024-12-07 10:49:20.867516: Yayy! New best EMA pseudo Dice: 0.7543
2024-12-07 10:49:22.747960: 
2024-12-07 10:49:22.749322: Epoch 83
2024-12-07 10:49:22.750161: Current learning rate: 0.00925
2024-12-07 10:54:40.858678: Validation loss did not improve from -0.60575. Patience: 6/50
2024-12-07 10:54:40.859861: train_loss -0.7307
2024-12-07 10:54:40.860776: val_loss -0.559
2024-12-07 10:54:40.861565: Pseudo dice [0.749]
2024-12-07 10:54:40.862382: Epoch time: 318.11 s
2024-12-07 10:54:42.294347: 
2024-12-07 10:54:42.295538: Epoch 84
2024-12-07 10:54:42.296428: Current learning rate: 0.00924
2024-12-07 11:00:32.561683: Validation loss did not improve from -0.60575. Patience: 7/50
2024-12-07 11:00:32.562825: train_loss -0.7247
2024-12-07 11:00:32.563671: val_loss -0.5523
2024-12-07 11:00:32.564569: Pseudo dice [0.7407]
2024-12-07 11:00:32.565332: Epoch time: 350.27 s
2024-12-07 11:00:34.407211: 
2024-12-07 11:00:34.408581: Epoch 85
2024-12-07 11:00:34.409354: Current learning rate: 0.00923
2024-12-07 11:06:12.326175: Validation loss did not improve from -0.60575. Patience: 8/50
2024-12-07 11:06:12.327688: train_loss -0.7236
2024-12-07 11:06:12.328714: val_loss -0.5625
2024-12-07 11:06:12.329601: Pseudo dice [0.7537]
2024-12-07 11:06:12.330522: Epoch time: 337.92 s
2024-12-07 11:06:13.762575: 
2024-12-07 11:06:13.764142: Epoch 86
2024-12-07 11:06:13.765137: Current learning rate: 0.00922
2024-12-07 11:11:29.173647: Validation loss did not improve from -0.60575. Patience: 9/50
2024-12-07 11:11:29.174601: train_loss -0.7278
2024-12-07 11:11:29.175626: val_loss -0.5432
2024-12-07 11:11:29.176684: Pseudo dice [0.7387]
2024-12-07 11:11:29.177657: Epoch time: 315.41 s
2024-12-07 11:11:30.635842: 
2024-12-07 11:11:30.637377: Epoch 87
2024-12-07 11:11:30.638418: Current learning rate: 0.00921
2024-12-07 11:16:26.147852: Validation loss did not improve from -0.60575. Patience: 10/50
2024-12-07 11:16:26.148993: train_loss -0.7348
2024-12-07 11:16:26.150292: val_loss -0.5598
2024-12-07 11:16:26.151172: Pseudo dice [0.7516]
2024-12-07 11:16:26.152140: Epoch time: 295.51 s
2024-12-07 11:16:27.577621: 
2024-12-07 11:16:27.579180: Epoch 88
2024-12-07 11:16:27.580127: Current learning rate: 0.0092
2024-12-07 11:21:51.008308: Validation loss did not improve from -0.60575. Patience: 11/50
2024-12-07 11:21:51.009277: train_loss -0.7345
2024-12-07 11:21:51.010011: val_loss -0.5582
2024-12-07 11:21:51.010670: Pseudo dice [0.7492]
2024-12-07 11:21:51.011586: Epoch time: 323.43 s
2024-12-07 11:21:52.441972: 
2024-12-07 11:21:52.443354: Epoch 89
2024-12-07 11:21:52.443979: Current learning rate: 0.0092
2024-12-07 11:27:52.376458: Validation loss did not improve from -0.60575. Patience: 12/50
2024-12-07 11:27:52.377531: train_loss -0.7319
2024-12-07 11:27:52.378222: val_loss -0.5708
2024-12-07 11:27:52.378814: Pseudo dice [0.7529]
2024-12-07 11:27:52.379465: Epoch time: 359.94 s
2024-12-07 11:27:54.205795: 
2024-12-07 11:27:54.207322: Epoch 90
2024-12-07 11:27:54.208441: Current learning rate: 0.00919
2024-12-07 11:33:31.920064: Validation loss did not improve from -0.60575. Patience: 13/50
2024-12-07 11:33:31.921073: train_loss -0.7329
2024-12-07 11:33:31.921870: val_loss -0.5363
2024-12-07 11:33:31.922538: Pseudo dice [0.7439]
2024-12-07 11:33:31.923289: Epoch time: 337.72 s
2024-12-07 11:33:33.357171: 
2024-12-07 11:33:33.358639: Epoch 91
2024-12-07 11:33:33.359428: Current learning rate: 0.00918
2024-12-07 11:38:44.340939: Validation loss did not improve from -0.60575. Patience: 14/50
2024-12-07 11:38:44.341844: train_loss -0.7303
2024-12-07 11:38:44.342879: val_loss -0.5865
2024-12-07 11:38:44.343848: Pseudo dice [0.7682]
2024-12-07 11:38:44.344914: Epoch time: 310.99 s
2024-12-07 11:38:45.739397: 
2024-12-07 11:38:45.740976: Epoch 92
2024-12-07 11:38:45.742151: Current learning rate: 0.00917
2024-12-07 11:44:04.584853: Validation loss did not improve from -0.60575. Patience: 15/50
2024-12-07 11:44:04.585752: train_loss -0.7403
2024-12-07 11:44:04.586502: val_loss -0.5693
2024-12-07 11:44:04.587247: Pseudo dice [0.7539]
2024-12-07 11:44:04.587978: Epoch time: 318.85 s
2024-12-07 11:44:06.550683: 
2024-12-07 11:44:06.552212: Epoch 93
2024-12-07 11:44:06.553294: Current learning rate: 0.00916
2024-12-07 11:49:04.565891: Validation loss did not improve from -0.60575. Patience: 16/50
2024-12-07 11:49:04.566860: train_loss -0.7369
2024-12-07 11:49:04.567805: val_loss -0.5651
2024-12-07 11:49:04.568753: Pseudo dice [0.7596]
2024-12-07 11:49:04.569512: Epoch time: 298.02 s
2024-12-07 11:49:05.974354: 
2024-12-07 11:49:05.975543: Epoch 94
2024-12-07 11:49:05.976432: Current learning rate: 0.00915
2024-12-07 11:55:10.036904: Validation loss did not improve from -0.60575. Patience: 17/50
2024-12-07 11:55:10.041176: train_loss -0.7304
2024-12-07 11:55:10.042179: val_loss -0.5548
2024-12-07 11:55:10.042937: Pseudo dice [0.7515]
2024-12-07 11:55:10.043796: Epoch time: 364.07 s
2024-12-07 11:55:11.856438: 
2024-12-07 11:55:11.857681: Epoch 95
2024-12-07 11:55:11.858489: Current learning rate: 0.00914
2024-12-07 12:00:38.341941: Validation loss did not improve from -0.60575. Patience: 18/50
2024-12-07 12:00:38.343151: train_loss -0.7385
2024-12-07 12:00:38.343931: val_loss -0.5581
2024-12-07 12:00:38.344653: Pseudo dice [0.7518]
2024-12-07 12:00:38.345493: Epoch time: 326.49 s
2024-12-07 12:00:39.776663: 
2024-12-07 12:00:39.778132: Epoch 96
2024-12-07 12:00:39.779163: Current learning rate: 0.00913
2024-12-07 12:06:06.420950: Validation loss did not improve from -0.60575. Patience: 19/50
2024-12-07 12:06:06.422042: train_loss -0.7394
2024-12-07 12:06:06.422807: val_loss -0.5282
2024-12-07 12:06:06.423573: Pseudo dice [0.7337]
2024-12-07 12:06:06.424278: Epoch time: 326.65 s
2024-12-07 12:06:07.846693: 
2024-12-07 12:06:07.848102: Epoch 97
2024-12-07 12:06:07.848912: Current learning rate: 0.00912
2024-12-07 12:11:18.862941: Validation loss did not improve from -0.60575. Patience: 20/50
2024-12-07 12:11:18.864000: train_loss -0.7376
2024-12-07 12:11:18.864810: val_loss -0.5818
2024-12-07 12:11:18.865526: Pseudo dice [0.7616]
2024-12-07 12:11:18.866338: Epoch time: 311.02 s
2024-12-07 12:11:20.355301: 
2024-12-07 12:11:20.356569: Epoch 98
2024-12-07 12:11:20.357469: Current learning rate: 0.00911
2024-12-07 12:16:34.155592: failed to log:  (<class 'FileNotFoundError'>, FileNotFoundError(2, 'No such file or directory'), <traceback object at 0x7f05d8663280>)
2024-12-07 12:16:34.155592: failed to log:  (<class 'FileNotFoundError'>, FileNotFoundError(2, 'No such file or directory'), <traceback object at 0x7f05d8663280>)
2024-12-07 12:16:34.155592: failed to log:  (<class 'FileNotFoundError'>, FileNotFoundError(2, 'No such file or directory'), <traceback object at 0x7f05d8663280>)
2024-12-07 12:16:34.155592: failed to log:  (<class 'FileNotFoundError'>, FileNotFoundError(2, 'No such file or directory'), <traceback object at 0x7f05d8663280>)
2024-12-07 12:16:34.155592: failed to log:  (<class 'FileNotFoundError'>, FileNotFoundError(2, 'No such file or directory'), <traceback object at 0x7f05d8663280>)
2024-12-07 12:16:34.155592: Validation loss did not improve from -0.60575. Patience: 21/50
2024-12-07 12:16:36.659139: failed to log:  (<class 'FileNotFoundError'>, FileNotFoundError(2, 'No such file or directory'), <traceback object at 0x7f05d8663280>)
2024-12-07 12:16:36.659139: failed to log:  (<class 'FileNotFoundError'>, FileNotFoundError(2, 'No such file or directory'), <traceback object at 0x7f05d8663280>)
2024-12-07 12:16:36.659139: failed to log:  (<class 'FileNotFoundError'>, FileNotFoundError(2, 'No such file or directory'), <traceback object at 0x7f05d8663280>)
2024-12-07 12:16:36.659139: failed to log:  (<class 'FileNotFoundError'>, FileNotFoundError(2, 'No such file or directory'), <traceback object at 0x7f05d8663280>)
2024-12-07 12:16:36.659139: failed to log:  (<class 'FileNotFoundError'>, FileNotFoundError(2, 'No such file or directory'), <traceback object at 0x7f05d8663280>)
2024-12-07 12:16:36.659139: train_loss -0.7322
2024-12-07 12:16:39.162551: failed to log:  (<class 'FileNotFoundError'>, FileNotFoundError(2, 'No such file or directory'), <traceback object at 0x7f05d83d2b40>)
2024-12-07 12:16:39.162551: failed to log:  (<class 'FileNotFoundError'>, FileNotFoundError(2, 'No such file or directory'), <traceback object at 0x7f05d83d2b40>)
2024-12-07 12:16:39.162551: failed to log:  (<class 'FileNotFoundError'>, FileNotFoundError(2, 'No such file or directory'), <traceback object at 0x7f05d83d2b40>)
2024-12-07 12:16:39.162551: failed to log:  (<class 'FileNotFoundError'>, FileNotFoundError(2, 'No such file or directory'), <traceback object at 0x7f05d83d2b40>)
2024-12-07 12:16:39.162551: failed to log:  (<class 'FileNotFoundError'>, FileNotFoundError(2, 'No such file or directory'), <traceback object at 0x7f05d83d2b40>)
2024-12-07 12:16:39.162551: val_loss -0.5555
2024-12-07 12:16:41.665623: failed to log:  (<class 'FileNotFoundError'>, FileNotFoundError(2, 'No such file or directory'), <traceback object at 0x7f05d0ee0300>)
2024-12-07 12:16:41.665623: failed to log:  (<class 'FileNotFoundError'>, FileNotFoundError(2, 'No such file or directory'), <traceback object at 0x7f05d0ee0300>)
2024-12-07 12:16:41.665623: failed to log:  (<class 'FileNotFoundError'>, FileNotFoundError(2, 'No such file or directory'), <traceback object at 0x7f05d0ee0300>)
2024-12-07 12:16:41.665623: failed to log:  (<class 'FileNotFoundError'>, FileNotFoundError(2, 'No such file or directory'), <traceback object at 0x7f05d0ee0300>)
2024-12-07 12:16:41.665623: failed to log:  (<class 'FileNotFoundError'>, FileNotFoundError(2, 'No such file or directory'), <traceback object at 0x7f05d0ee0300>)
2024-12-07 12:16:41.665623: Pseudo dice [0.7497]
2024-12-07 12:16:44.168794: failed to log:  (<class 'FileNotFoundError'>, FileNotFoundError(2, 'No such file or directory'), <traceback object at 0x7f05d87b0400>)
2024-12-07 12:16:44.168794: failed to log:  (<class 'FileNotFoundError'>, FileNotFoundError(2, 'No such file or directory'), <traceback object at 0x7f05d87b0400>)
2024-12-07 12:16:44.168794: failed to log:  (<class 'FileNotFoundError'>, FileNotFoundError(2, 'No such file or directory'), <traceback object at 0x7f05d87b0400>)
2024-12-07 12:16:44.168794: failed to log:  (<class 'FileNotFoundError'>, FileNotFoundError(2, 'No such file or directory'), <traceback object at 0x7f05d87b0400>)
2024-12-07 12:16:44.168794: failed to log:  (<class 'FileNotFoundError'>, FileNotFoundError(2, 'No such file or directory'), <traceback object at 0x7f05d87b0400>)
2024-12-07 12:16:44.168794: Epoch time: 316.31 s
Traceback (most recent call last):
  File "/home/gridsan/nchutisilp/.conda/envs/nnunet/bin/nnUNetv2_train", line 8, in <module>
    sys.exit(run_training_entry())
  File "/home/gridsan/nchutisilp/projects/nnUNet/nnunetv2/run/run_training.py", line 275, in run_training_entry
    run_training(args.dataset_name_or_id, args.configuration, args.fold, args.tr, args.p, args.pretrained_weights,
  File "/home/gridsan/nchutisilp/projects/nnUNet/nnunetv2/run/run_training.py", line 211, in run_training
    nnunet_trainer.run_training()
  File "/home/gridsan/nchutisilp/projects/nnUNet/nnunetv2/training/nnUNetTrainer/nnUNetTrainer.py", line 1411, in run_training
    self.on_epoch_end()
  File "/home/gridsan/nchutisilp/projects/nnUNet/nnunetv2/training/nnUNetTrainer/nnUNetTrainer.py", line 1174, in on_epoch_end
    self.logger.plot_progress_png(self.output_folder)
  File "/home/gridsan/nchutisilp/projects/nnUNet/nnunetv2/training/logging/nnunet_logger.py", line 99, in plot_progress_png
    fig.savefig(join(output_folder, "progress.png"))
  File "/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/matplotlib/figure.py", line 3390, in savefig
    self.canvas.print_figure(fname, **kwargs)
  File "/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/matplotlib/backend_bases.py", line 2193, in print_figure
    result = print_method(
  File "/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/matplotlib/backend_bases.py", line 2043, in <lambda>
    print_method = functools.wraps(meth)(lambda *args, **kwargs: meth(
  File "/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/matplotlib/backends/backend_agg.py", line 497, in print_png
    self._print_pil(filename_or_obj, "png", pil_kwargs, metadata)
  File "/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/matplotlib/backends/backend_agg.py", line 446, in _print_pil
    mpl.image.imsave(
  File "/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/matplotlib/image.py", line 1656, in imsave
    image.save(fname, **pil_kwargs)
  File "/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/PIL/Image.py", line 2456, in save
    fp = builtins.open(filename, "w+b")
FileNotFoundError: [Errno 2] No such file or directory: '/home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_results/Dataset307_Sohee_Calcium_OCT_CrossValidation/nnUNetTrainer__nnUNetPlans__3d_32x160x128_b10/fold_1/progress.png'
Exception in thread Thread-3:
Traceback (most recent call last):
  File "/home/gridsan/nchutisilp/.conda/envs/nnunet/lib/python3.9/threading.py", line 980, in _bootstrap_inner
    self.run()
  File "/home/gridsan/nchutisilp/.conda/envs/nnunet/lib/python3.9/threading.py", line 917, in run
    self._target(*self._args, **self._kwargs)
  File "/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/batchgenerators/dataloading/nondet_multi_threaded_augmenter.py", line 125, in results_loop
    raise e
  File "/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/batchgenerators/dataloading/nondet_multi_threaded_augmenter.py", line 103, in results_loop
    raise RuntimeError("One or more background workers are no longer alive. Exiting. Please check the "
RuntimeError: One or more background workers are no longer alive. Exiting. Please check the print statements above for the actual error message
Exception in thread Thread-2:
Traceback (most recent call last):
  File "/home/gridsan/nchutisilp/.conda/envs/nnunet/lib/python3.9/threading.py", line 980, in _bootstrap_inner
    self.run()
  File "/home/gridsan/nchutisilp/.conda/envs/nnunet/lib/python3.9/threading.py", line 917, in run
    self._target(*self._args, **self._kwargs)
  File "/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/batchgenerators/dataloading/nondet_multi_threaded_augmenter.py", line 125, in results_loop
    raise e
  File "/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/batchgenerators/dataloading/nondet_multi_threaded_augmenter.py", line 103, in results_loop
    raise RuntimeError("One or more background workers are no longer alive. Exiting. Please check the "
RuntimeError: One or more background workers are no longer alive. Exiting. Please check the print statements above for the actual error message
mv: cannot stat '/home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_results/Dataset308_Sohee_Calcium_OCT_CrossValidation/nnUNetTrainer__nnUNetPlans__3d_32x160x128_b10/fold_0': No such file or directory
mv: cannot stat '/home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_results/Dataset308_Sohee_Calcium_OCT_CrossValidation/nnUNetTrainer__nnUNetPlans__3d_32x160x128_b10/fold_1': No such file or directory

############################
INFO: You are using the old nnU-Net default plans. We have updated our recommendations. Please consider using those instead! Read more here: https://github.com/MIC-DKFZ/nnUNet/blob/master/documentation/resenc_presets.md
############################

Using device: cuda:0

#######################################################################
Please cite the following paper when using nnU-Net:
Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.
#######################################################################

2024-12-07 12:17:13.570303: Using torch.compile...

############################
INFO: You are using the old nnU-Net default plans. We have updated our recommendations. Please consider using those instead! Read more here: https://github.com/MIC-DKFZ/nnUNet/blob/master/documentation/resenc_presets.md
############################

Using device: cuda:0

#######################################################################
Please cite the following paper when using nnU-Net:
Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.
#######################################################################

2024-12-07 12:17:13.569091: Using torch.compile...
/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
################### Loading pretrained weights from file  /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_results/Dataset300_Lumen_and_Wall_OCT/nnUNetTrainer__nnUNetPreprocessPlans__3d_fullres/fold_all/checkpoint_best.pth ###################
Below is the list of overlapping blocks in pretrained model and nnUNet architecture:
encoder.stages.0.0.convs.0.conv.weight shape torch.Size([32, 1, 3, 3, 3])
encoder.stages.0.0.convs.0.conv.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.norm.weight shape torch.Size([32])
encoder.stages.0.0.convs.0.norm.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.0.weight shape torch.Size([32, 1, 3, 3, 3])
encoder.stages.0.0.convs.0.all_modules.0.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.1.weight shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.1.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
encoder.stages.0.0.convs.1.conv.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.norm.weight shape torch.Size([32])
encoder.stages.0.0.convs.1.norm.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
encoder.stages.0.0.convs.1.all_modules.0.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.1.weight shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.1.bias shape torch.Size([32])
encoder.stages.1.0.convs.0.conv.weight shape torch.Size([64, 32, 3, 3, 3])
encoder.stages.1.0.convs.0.conv.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.norm.weight shape torch.Size([64])
encoder.stages.1.0.convs.0.norm.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.0.weight shape torch.Size([64, 32, 3, 3, 3])
encoder.stages.1.0.convs.0.all_modules.0.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.1.weight shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.1.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
encoder.stages.1.0.convs.1.conv.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.norm.weight shape torch.Size([64])
encoder.stages.1.0.convs.1.norm.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
encoder.stages.1.0.convs.1.all_modules.0.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.1.weight shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.1.bias shape torch.Size([64])
encoder.stages.2.0.convs.0.conv.weight shape torch.Size([128, 64, 3, 3, 3])
encoder.stages.2.0.convs.0.conv.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.norm.weight shape torch.Size([128])
encoder.stages.2.0.convs.0.norm.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.0.weight shape torch.Size([128, 64, 3, 3, 3])
encoder.stages.2.0.convs.0.all_modules.0.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.1.weight shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.1.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
encoder.stages.2.0.convs.1.conv.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.norm.weight shape torch.Size([128])
encoder.stages.2.0.convs.1.norm.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
encoder.stages.2.0.convs.1.all_modules.0.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.1.weight shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.1.bias shape torch.Size([128])
encoder.stages.3.0.convs.0.conv.weight shape torch.Size([256, 128, 3, 3, 3])
encoder.stages.3.0.convs.0.conv.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.norm.weight shape torch.Size([256])
encoder.stages.3.0.convs.0.norm.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.0.weight shape torch.Size([256, 128, 3, 3, 3])
encoder.stages.3.0.convs.0.all_modules.0.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.1.weight shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.1.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
encoder.stages.3.0.convs.1.conv.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.norm.weight shape torch.Size([256])
encoder.stages.3.0.convs.1.norm.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
encoder.stages.3.0.convs.1.all_modules.0.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.1.weight shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.1.bias shape torch.Size([256])
encoder.stages.4.0.convs.0.conv.weight shape torch.Size([320, 256, 3, 3, 3])
encoder.stages.4.0.convs.0.conv.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.norm.weight shape torch.Size([320])
encoder.stages.4.0.convs.0.norm.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.0.weight shape torch.Size([320, 256, 3, 3, 3])
encoder.stages.4.0.convs.0.all_modules.0.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.1.weight shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.1.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.4.0.convs.1.conv.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.norm.weight shape torch.Size([320])
encoder.stages.4.0.convs.1.norm.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.4.0.convs.1.all_modules.0.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.1.weight shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.1.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.0.conv.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.norm.weight shape torch.Size([320])
encoder.stages.5.0.convs.0.norm.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.0.all_modules.0.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.1.weight shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.1.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.1.conv.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.norm.weight shape torch.Size([320])
encoder.stages.5.0.convs.1.norm.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.1.all_modules.0.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.1.weight shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.0.0.convs.0.conv.weight shape torch.Size([32, 1, 3, 3, 3])
decoder.encoder.stages.0.0.convs.0.conv.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.norm.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.norm.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.0.weight shape torch.Size([32, 1, 3, 3, 3])
decoder.encoder.stages.0.0.convs.0.all_modules.0.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.1.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.1.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.encoder.stages.0.0.convs.1.conv.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.norm.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.norm.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.encoder.stages.0.0.convs.1.all_modules.0.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.1.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.1.bias shape torch.Size([32])
decoder.encoder.stages.1.0.convs.0.conv.weight shape torch.Size([64, 32, 3, 3, 3])
decoder.encoder.stages.1.0.convs.0.conv.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.norm.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.norm.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.0.weight shape torch.Size([64, 32, 3, 3, 3])
decoder.encoder.stages.1.0.convs.0.all_modules.0.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.1.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.1.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.encoder.stages.1.0.convs.1.conv.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.norm.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.norm.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.encoder.stages.1.0.convs.1.all_modules.0.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.1.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.1.bias shape torch.Size([64])
decoder.encoder.stages.2.0.convs.0.conv.weight shape torch.Size([128, 64, 3, 3, 3])
decoder.encoder.stages.2.0.convs.0.conv.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.norm.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.norm.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.0.weight shape torch.Size([128, 64, 3, 3, 3])
decoder.encoder.stages.2.0.convs.0.all_modules.0.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.1.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.1.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.encoder.stages.2.0.convs.1.conv.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.norm.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.norm.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.encoder.stages.2.0.convs.1.all_modules.0.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.1.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.1.bias shape torch.Size([128])
decoder.encoder.stages.3.0.convs.0.conv.weight shape torch.Size([256, 128, 3, 3, 3])
decoder.encoder.stages.3.0.convs.0.conv.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.norm.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.norm.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.0.weight shape torch.Size([256, 128, 3, 3, 3])
decoder.encoder.stages.3.0.convs.0.all_modules.0.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.1.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.1.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.encoder.stages.3.0.convs.1.conv.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.norm.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.norm.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.encoder.stages.3.0.convs.1.all_modules.0.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.1.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.1.bias shape torch.Size([256])
decoder.encoder.stages.4.0.convs.0.conv.weight shape torch.Size([320, 256, 3, 3, 3])
decoder.encoder.stages.4.0.convs.0.conv.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.norm.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.norm.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.0.weight shape torch.Size([320, 256, 3, 3, 3])
decoder.encoder.stages.4.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.4.0.convs.1.conv.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.norm.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.norm.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.4.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.0.conv.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.norm.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.norm.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.1.conv.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.norm.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.norm.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.stages.0.convs.0.conv.weight shape torch.Size([320, 640, 3, 3, 3])
decoder.stages.0.convs.0.conv.bias shape torch.Size([320])
decoder.stages.0.convs.0.norm.weight shape torch.Size([320])
decoder.stages.0.convs.0.norm.bias shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.0.weight shape torch.Size([320, 640, 3, 3, 3])
decoder.stages.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.stages.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.stages.0.convs.1.conv.bias shape torch.Size([320])
decoder.stages.0.convs.1.norm.weight shape torch.Size([320])
decoder.stages.0.convs.1.norm.bias shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.stages.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.stages.1.convs.0.conv.weight shape torch.Size([256, 512, 3, 3, 3])
decoder.stages.1.convs.0.conv.bias shape torch.Size([256])
decoder.stages.1.convs.0.norm.weight shape torch.Size([256])
decoder.stages.1.convs.0.norm.bias shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.0.weight shape torch.Size([256, 512, 3, 3, 3])
decoder.stages.1.convs.0.all_modules.0.bias shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.1.weight shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.1.bias shape torch.Size([256])
decoder.stages.1.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.stages.1.convs.1.conv.bias shape torch.Size([256])
decoder.stages.1.convs.1.norm.weight shape torch.Size([256])
decoder.stages.1.convs.1.norm.bias shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.stages.1.convs.1.all_modules.0.bias shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.1.weight shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.1.bias shape torch.Size([256])
decoder.stages.2.convs.0.conv.weight shape torch.Size([128, 256, 3, 3, 3])
decoder.stages.2.convs.0.conv.bias shape torch.Size([128])
decoder.stages.2.convs.0.norm.weight shape torch.Size([128])
decoder.stages.2.convs.0.norm.bias shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.0.weight shape torch.Size([128, 256, 3, 3, 3])
decoder.stages.2.convs.0.all_modules.0.bias shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.1.weight shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.1.bias shape torch.Size([128])
decoder.stages.2.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.stages.2.convs.1.conv.bias shape torch.Size([128])
decoder.stages.2.convs.1.norm.weight shape torch.Size([128])
decoder.stages.2.convs.1.norm.bias shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.stages.2.convs.1.all_modules.0.bias shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.1.weight shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.1.bias shape torch.Size([128])
decoder.stages.3.convs.0.conv.weight shape torch.Size([64, 128, 3, 3, 3])
decoder.stages.3.convs.0.conv.bias shape torch.Size([64])
decoder.stages.3.convs.0.norm.weight shape torch.Size([64])
decoder.stages.3.convs.0.norm.bias shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.0.weight shape torch.Size([64, 128, 3, 3, 3])
decoder.stages.3.convs.0.all_modules.0.bias shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.1.weight shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.1.bias shape torch.Size([64])
decoder.stages.3.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.stages.3.convs.1.conv.bias shape torch.Size([64])
decoder.stages.3.convs.1.norm.weight shape torch.Size([64])
decoder.stages.3.convs.1.norm.bias shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.stages.3.convs.1.all_modules.0.bias shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.1.weight shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.1.bias shape torch.Size([64])
decoder.stages.4.convs.0.conv.weight shape torch.Size([32, 64, 3, 3, 3])
decoder.stages.4.convs.0.conv.bias shape torch.Size([32])
decoder.stages.4.convs.0.norm.weight shape torch.Size([32])
decoder.stages.4.convs.0.norm.bias shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.0.weight shape torch.Size([32, 64, 3, 3, 3])
decoder.stages.4.convs.0.all_modules.0.bias shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.1.weight shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.1.bias shape torch.Size([32])
decoder.stages.4.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.stages.4.convs.1.conv.bias shape torch.Size([32])
decoder.stages.4.convs.1.norm.weight shape torch.Size([32])
decoder.stages.4.convs.1.norm.bias shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.stages.4.convs.1.all_modules.0.bias shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.1.weight shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.1.bias shape torch.Size([32])
decoder.transpconvs.0.weight shape torch.Size([320, 320, 1, 2, 2])
decoder.transpconvs.0.bias shape torch.Size([320])
decoder.transpconvs.1.weight shape torch.Size([320, 256, 2, 2, 2])
decoder.transpconvs.1.bias shape torch.Size([256])
decoder.transpconvs.2.weight shape torch.Size([256, 128, 2, 2, 2])
decoder.transpconvs.2.bias shape torch.Size([128])
decoder.transpconvs.3.weight shape torch.Size([128, 64, 2, 2, 2])
decoder.transpconvs.3.bias shape torch.Size([64])
decoder.transpconvs.4.weight shape torch.Size([64, 32, 2, 2, 2])
decoder.transpconvs.4.bias shape torch.Size([32])
################### Done ###################
2024-12-07 12:17:18.710552: do_dummy_2d_data_aug: True
2024-12-07 12:17:18.713888: Using splits from existing split file: /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset307_Sohee_Calcium_OCT_CrossValidation/splits_final.json
2024-12-07 12:17:18.732891: The split file contains 5 splits.
2024-12-07 12:17:18.734689: Desired fold for training: 3
2024-12-07 12:17:18.735548: This split has 7 training and 1 validation cases.
################### Loading pretrained weights from file  /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_results/Dataset300_Lumen_and_Wall_OCT/nnUNetTrainer__nnUNetPreprocessPlans__3d_fullres/fold_all/checkpoint_best.pth ###################
Below is the list of overlapping blocks in pretrained model and nnUNet architecture:
encoder.stages.0.0.convs.0.conv.weight shape torch.Size([32, 1, 3, 3, 3])
encoder.stages.0.0.convs.0.conv.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.norm.weight shape torch.Size([32])
encoder.stages.0.0.convs.0.norm.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.0.weight shape torch.Size([32, 1, 3, 3, 3])
encoder.stages.0.0.convs.0.all_modules.0.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.1.weight shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.1.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
encoder.stages.0.0.convs.1.conv.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.norm.weight shape torch.Size([32])
encoder.stages.0.0.convs.1.norm.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
encoder.stages.0.0.convs.1.all_modules.0.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.1.weight shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.1.bias shape torch.Size([32])
encoder.stages.1.0.convs.0.conv.weight shape torch.Size([64, 32, 3, 3, 3])
encoder.stages.1.0.convs.0.conv.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.norm.weight shape torch.Size([64])
encoder.stages.1.0.convs.0.norm.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.0.weight shape torch.Size([64, 32, 3, 3, 3])
encoder.stages.1.0.convs.0.all_modules.0.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.1.weight shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.1.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
encoder.stages.1.0.convs.1.conv.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.norm.weight shape torch.Size([64])
encoder.stages.1.0.convs.1.norm.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
encoder.stages.1.0.convs.1.all_modules.0.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.1.weight shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.1.bias shape torch.Size([64])
encoder.stages.2.0.convs.0.conv.weight shape torch.Size([128, 64, 3, 3, 3])
encoder.stages.2.0.convs.0.conv.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.norm.weight shape torch.Size([128])
encoder.stages.2.0.convs.0.norm.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.0.weight shape torch.Size([128, 64, 3, 3, 3])
encoder.stages.2.0.convs.0.all_modules.0.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.1.weight shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.1.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
encoder.stages.2.0.convs.1.conv.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.norm.weight shape torch.Size([128])
encoder.stages.2.0.convs.1.norm.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
encoder.stages.2.0.convs.1.all_modules.0.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.1.weight shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.1.bias shape torch.Size([128])
encoder.stages.3.0.convs.0.conv.weight shape torch.Size([256, 128, 3, 3, 3])
encoder.stages.3.0.convs.0.conv.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.norm.weight shape torch.Size([256])
encoder.stages.3.0.convs.0.norm.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.0.weight shape torch.Size([256, 128, 3, 3, 3])
encoder.stages.3.0.convs.0.all_modules.0.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.1.weight shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.1.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
encoder.stages.3.0.convs.1.conv.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.norm.weight shape torch.Size([256])
encoder.stages.3.0.convs.1.norm.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
encoder.stages.3.0.convs.1.all_modules.0.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.1.weight shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.1.bias shape torch.Size([256])
encoder.stages.4.0.convs.0.conv.weight shape torch.Size([320, 256, 3, 3, 3])
encoder.stages.4.0.convs.0.conv.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.norm.weight shape torch.Size([320])
encoder.stages.4.0.convs.0.norm.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.0.weight shape torch.Size([320, 256, 3, 3, 3])
encoder.stages.4.0.convs.0.all_modules.0.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.1.weight shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.1.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.4.0.convs.1.conv.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.norm.weight shape torch.Size([320])
encoder.stages.4.0.convs.1.norm.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.4.0.convs.1.all_modules.0.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.1.weight shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.1.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.0.conv.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.norm.weight shape torch.Size([320])
encoder.stages.5.0.convs.0.norm.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.0.all_modules.0.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.1.weight shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.1.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.1.conv.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.norm.weight shape torch.Size([320])
encoder.stages.5.0.convs.1.norm.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.1.all_modules.0.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.1.weight shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.0.0.convs.0.conv.weight shape torch.Size([32, 1, 3, 3, 3])
decoder.encoder.stages.0.0.convs.0.conv.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.norm.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.norm.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.0.weight shape torch.Size([32, 1, 3, 3, 3])
decoder.encoder.stages.0.0.convs.0.all_modules.0.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.1.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.1.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.encoder.stages.0.0.convs.1.conv.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.norm.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.norm.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.encoder.stages.0.0.convs.1.all_modules.0.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.1.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.1.bias shape torch.Size([32])
decoder.encoder.stages.1.0.convs.0.conv.weight shape torch.Size([64, 32, 3, 3, 3])
decoder.encoder.stages.1.0.convs.0.conv.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.norm.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.norm.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.0.weight shape torch.Size([64, 32, 3, 3, 3])
decoder.encoder.stages.1.0.convs.0.all_modules.0.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.1.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.1.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.encoder.stages.1.0.convs.1.conv.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.norm.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.norm.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.encoder.stages.1.0.convs.1.all_modules.0.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.1.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.1.bias shape torch.Size([64])
decoder.encoder.stages.2.0.convs.0.conv.weight shape torch.Size([128, 64, 3, 3, 3])
decoder.encoder.stages.2.0.convs.0.conv.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.norm.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.norm.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.0.weight shape torch.Size([128, 64, 3, 3, 3])
decoder.encoder.stages.2.0.convs.0.all_modules.0.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.1.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.1.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.encoder.stages.2.0.convs.1.conv.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.norm.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.norm.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.encoder.stages.2.0.convs.1.all_modules.0.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.1.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.1.bias shape torch.Size([128])
decoder.encoder.stages.3.0.convs.0.conv.weight shape torch.Size([256, 128, 3, 3, 3])
decoder.encoder.stages.3.0.convs.0.conv.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.norm.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.norm.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.0.weight shape torch.Size([256, 128, 3, 3, 3])
decoder.encoder.stages.3.0.convs.0.all_modules.0.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.1.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.1.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.encoder.stages.3.0.convs.1.conv.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.norm.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.norm.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.encoder.stages.3.0.convs.1.all_modules.0.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.1.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.1.bias shape torch.Size([256])
decoder.encoder.stages.4.0.convs.0.conv.weight shape torch.Size([320, 256, 3, 3, 3])
decoder.encoder.stages.4.0.convs.0.conv.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.norm.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.norm.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.0.weight shape torch.Size([320, 256, 3, 3, 3])
decoder.encoder.stages.4.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.4.0.convs.1.conv.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.norm.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.norm.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.4.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.0.conv.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.norm.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.norm.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.1.conv.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.norm.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.norm.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.stages.0.convs.0.conv.weight shape torch.Size([320, 640, 3, 3, 3])
decoder.stages.0.convs.0.conv.bias shape torch.Size([320])
decoder.stages.0.convs.0.norm.weight shape torch.Size([320])
decoder.stages.0.convs.0.norm.bias shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.0.weight shape torch.Size([320, 640, 3, 3, 3])
decoder.stages.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.stages.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.stages.0.convs.1.conv.bias shape torch.Size([320])
decoder.stages.0.convs.1.norm.weight shape torch.Size([320])
decoder.stages.0.convs.1.norm.bias shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.stages.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.stages.1.convs.0.conv.weight shape torch.Size([256, 512, 3, 3, 3])
decoder.stages.1.convs.0.conv.bias shape torch.Size([256])
decoder.stages.1.convs.0.norm.weight shape torch.Size([256])
decoder.stages.1.convs.0.norm.bias shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.0.weight shape torch.Size([256, 512, 3, 3, 3])
decoder.stages.1.convs.0.all_modules.0.bias shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.1.weight shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.1.bias shape torch.Size([256])
decoder.stages.1.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.stages.1.convs.1.conv.bias shape torch.Size([256])
decoder.stages.1.convs.1.norm.weight shape torch.Size([256])
decoder.stages.1.convs.1.norm.bias shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.stages.1.convs.1.all_modules.0.bias shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.1.weight shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.1.bias shape torch.Size([256])
decoder.stages.2.convs.0.conv.weight shape torch.Size([128, 256, 3, 3, 3])
decoder.stages.2.convs.0.conv.bias shape torch.Size([128])
decoder.stages.2.convs.0.norm.weight shape torch.Size([128])
decoder.stages.2.convs.0.norm.bias shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.0.weight shape torch.Size([128, 256, 3, 3, 3])
decoder.stages.2.convs.0.all_modules.0.bias shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.1.weight shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.1.bias shape torch.Size([128])
decoder.stages.2.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.stages.2.convs.1.conv.bias shape torch.Size([128])
decoder.stages.2.convs.1.norm.weight shape torch.Size([128])
decoder.stages.2.convs.1.norm.bias shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.stages.2.convs.1.all_modules.0.bias shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.1.weight shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.1.bias shape torch.Size([128])
decoder.stages.3.convs.0.conv.weight shape torch.Size([64, 128, 3, 3, 3])
decoder.stages.3.convs.0.conv.bias shape torch.Size([64])
decoder.stages.3.convs.0.norm.weight shape torch.Size([64])
decoder.stages.3.convs.0.norm.bias shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.0.weight shape torch.Size([64, 128, 3, 3, 3])
decoder.stages.3.convs.0.all_modules.0.bias shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.1.weight shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.1.bias shape torch.Size([64])
decoder.stages.3.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.stages.3.convs.1.conv.bias shape torch.Size([64])
decoder.stages.3.convs.1.norm.weight shape torch.Size([64])
decoder.stages.3.convs.1.norm.bias shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.stages.3.convs.1.all_modules.0.bias shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.1.weight shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.1.bias shape torch.Size([64])
decoder.stages.4.convs.0.conv.weight shape torch.Size([32, 64, 3, 3, 3])
decoder.stages.4.convs.0.conv.bias shape torch.Size([32])
decoder.stages.4.convs.0.norm.weight shape torch.Size([32])
decoder.stages.4.convs.0.norm.bias shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.0.weight shape torch.Size([32, 64, 3, 3, 3])
decoder.stages.4.convs.0.all_modules.0.bias shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.1.weight shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.1.bias shape torch.Size([32])
decoder.stages.4.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.stages.4.convs.1.conv.bias shape torch.Size([32])
decoder.stages.4.convs.1.norm.weight shape torch.Size([32])
decoder.stages.4.convs.1.norm.bias shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.stages.4.convs.1.all_modules.0.bias shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.1.weight shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.1.bias shape torch.Size([32])
decoder.transpconvs.0.weight shape torch.Size([320, 320, 1, 2, 2])
decoder.transpconvs.0.bias shape torch.Size([320])
decoder.transpconvs.1.weight shape torch.Size([320, 256, 2, 2, 2])
decoder.transpconvs.1.bias shape torch.Size([256])
decoder.transpconvs.2.weight shape torch.Size([256, 128, 2, 2, 2])
decoder.transpconvs.2.bias shape torch.Size([128])
decoder.transpconvs.3.weight shape torch.Size([128, 64, 2, 2, 2])
decoder.transpconvs.3.bias shape torch.Size([64])
decoder.transpconvs.4.weight shape torch.Size([64, 32, 2, 2, 2])
decoder.transpconvs.4.bias shape torch.Size([32])
################### Done ###################
2024-12-07 12:17:18.710431: do_dummy_2d_data_aug: True
2024-12-07 12:17:18.713920: Using splits from existing split file: /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset307_Sohee_Calcium_OCT_CrossValidation/splits_final.json
2024-12-07 12:17:18.733572: The split file contains 5 splits.
2024-12-07 12:17:18.734986: Desired fold for training: 2
2024-12-07 12:17:18.735831: This split has 6 training and 2 validation cases.
using pin_memory on device 0
using pin_memory on device 0
using pin_memory on device 0

This is the configuration used by this training:
Configuration name: 3d_32x160x128_b10
 {'data_identifier': 'nnUNetPlans_3d_fullres', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 10, 'patch_size': [32, 160, 128], 'median_image_size_in_voxels': [375.0, 498.0, 498.0], 'spacing': [1.0, 1.0, 1.0], 'normalization_schemes': ['ZScoreNormalization'], 'use_mask_for_norm': [False], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'architecture': {'network_class_name': 'dynamic_network_architectures.architectures.unet.PlainConvUNet', 'arch_kwargs': {'n_stages': 6, 'features_per_stage': [32, 64, 128, 256, 320, 320], 'conv_op': 'torch.nn.modules.conv.Conv3d', 'kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]], 'strides': [[1, 1, 1], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [1, 2, 2]], 'n_conv_per_stage': [2, 2, 2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2, 2, 2], 'conv_bias': True, 'norm_op': 'torch.nn.modules.instancenorm.InstanceNorm3d', 'norm_op_kwargs': {'eps': 1e-05, 'affine': True}, 'dropout_op': None, 'dropout_op_kwargs': None, 'nonlin': 'torch.nn.LeakyReLU', 'nonlin_kwargs': {'inplace': True}, 'deep_supervision': True}, '_kw_requires_import': ['conv_op', 'norm_op', 'dropout_op', 'nonlin']}, 'batch_dice': True, 'inherits_from': '3d_fullres'} 

These are the global plan.json settings:
 {'dataset_name': 'Dataset307_Sohee_Calcium_OCT_CrossValidation', 'plans_name': 'nnUNetPlans', 'original_median_spacing_after_transp': [1.0, 1.0, 1.0], 'original_median_shape_after_transp': [375, 498, 498], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 0.4977976381778717, 'mean': 0.13507525622844696, 'median': 0.09599608182907104, 'min': 0.0, 'percentile_00_5': 0.014754901640117168, 'percentile_99_5': 0.4977976381778717, 'std': 0.12025152146816254}}} 

2024-12-07 12:17:49.733142: unpacking dataset...
using pin_memory on device 0

This is the configuration used by this training:
Configuration name: 3d_32x160x128_b10
 {'data_identifier': 'nnUNetPlans_3d_fullres', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 10, 'patch_size': [32, 160, 128], 'median_image_size_in_voxels': [375.0, 498.0, 498.0], 'spacing': [1.0, 1.0, 1.0], 'normalization_schemes': ['ZScoreNormalization'], 'use_mask_for_norm': [False], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'architecture': {'network_class_name': 'dynamic_network_architectures.architectures.unet.PlainConvUNet', 'arch_kwargs': {'n_stages': 6, 'features_per_stage': [32, 64, 128, 256, 320, 320], 'conv_op': 'torch.nn.modules.conv.Conv3d', 'kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]], 'strides': [[1, 1, 1], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [1, 2, 2]], 'n_conv_per_stage': [2, 2, 2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2, 2, 2], 'conv_bias': True, 'norm_op': 'torch.nn.modules.instancenorm.InstanceNorm3d', 'norm_op_kwargs': {'eps': 1e-05, 'affine': True}, 'dropout_op': None, 'dropout_op_kwargs': None, 'nonlin': 'torch.nn.LeakyReLU', 'nonlin_kwargs': {'inplace': True}, 'deep_supervision': True}, '_kw_requires_import': ['conv_op', 'norm_op', 'dropout_op', 'nonlin']}, 'batch_dice': True, 'inherits_from': '3d_fullres'} 

These are the global plan.json settings:
 {'dataset_name': 'Dataset307_Sohee_Calcium_OCT_CrossValidation', 'plans_name': 'nnUNetPlans', 'original_median_spacing_after_transp': [1.0, 1.0, 1.0], 'original_median_shape_after_transp': [375, 498, 498], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 0.4977976381778717, 'mean': 0.13507525622844696, 'median': 0.09599608182907104, 'min': 0.0, 'percentile_00_5': 0.014754901640117168, 'percentile_99_5': 0.4977976381778717, 'std': 0.12025152146816254}}} 

2024-12-07 12:17:49.882782: unpacking dataset...
2024-12-07 12:17:55.765175: unpacking done...
2024-12-07 12:17:55.787179: Unable to plot network architecture: nnUNet_compile is enabled!
2024-12-07 12:17:55.896161: 
2024-12-07 12:17:55.897519: Epoch 0
2024-12-07 12:17:55.898403: Current learning rate: 0.01
2024-12-07 12:25:43.898046: Validation loss improved from 1000.00000 to -0.43486! Patience: 0/50
2024-12-07 12:25:43.899485: train_loss -0.301
2024-12-07 12:25:43.900723: val_loss -0.4349
2024-12-07 12:25:43.901594: Pseudo dice [0.6685]
2024-12-07 12:25:43.902580: Epoch time: 468.0 s
2024-12-07 12:25:43.903573: Yayy! New best EMA pseudo Dice: 0.6685
2024-12-07 12:25:45.579270: 
2024-12-07 12:25:45.580659: Epoch 1
2024-12-07 12:25:45.582182: Current learning rate: 0.00999
2024-12-07 12:32:38.279985: Validation loss improved from -0.43486 to -0.48795! Patience: 0/50
2024-12-07 12:32:38.280889: train_loss -0.462
2024-12-07 12:32:38.281806: val_loss -0.4879
2024-12-07 12:32:38.282599: Pseudo dice [0.7079]
2024-12-07 12:32:38.283370: Epoch time: 412.7 s
2024-12-07 12:32:38.284073: Yayy! New best EMA pseudo Dice: 0.6724
2024-12-07 12:32:40.109354: 
2024-12-07 12:32:40.110730: Epoch 2
2024-12-07 12:32:40.111504: Current learning rate: 0.00998
2024-12-07 12:39:29.557043: Validation loss improved from -0.48795 to -0.50666! Patience: 0/50
2024-12-07 12:39:29.558036: train_loss -0.4875
2024-12-07 12:39:29.558974: val_loss -0.5067
2024-12-07 12:39:29.559977: Pseudo dice [0.7192]
2024-12-07 12:39:29.560974: Epoch time: 409.45 s
2024-12-07 12:39:29.561812: Yayy! New best EMA pseudo Dice: 0.6771
2024-12-07 12:39:31.427849: 
2024-12-07 12:39:31.429225: Epoch 3
2024-12-07 12:39:31.430110: Current learning rate: 0.00997
2024-12-07 12:46:10.014548: Validation loss improved from -0.50666 to -0.51170! Patience: 0/50
2024-12-07 12:46:10.015479: train_loss -0.5226
2024-12-07 12:46:10.016219: val_loss -0.5117
2024-12-07 12:46:10.016995: Pseudo dice [0.7201]
2024-12-07 12:46:10.017889: Epoch time: 398.59 s
2024-12-07 12:46:10.018686: Yayy! New best EMA pseudo Dice: 0.6814
2024-12-07 12:46:11.829479: 
2024-12-07 12:46:11.830674: Epoch 4
2024-12-07 12:46:11.831462: Current learning rate: 0.00996
2024-12-07 12:52:35.673606: Validation loss did not improve from -0.51170. Patience: 1/50
2024-12-07 12:52:35.674619: train_loss -0.5332
2024-12-07 12:52:35.675505: val_loss -0.5076
2024-12-07 12:52:35.676408: Pseudo dice [0.719]
2024-12-07 12:52:35.677257: Epoch time: 383.85 s
2024-12-07 12:52:36.097161: Yayy! New best EMA pseudo Dice: 0.6852
2024-12-07 12:52:37.932682: 
2024-12-07 12:52:37.933972: Epoch 5
2024-12-07 12:52:37.934779: Current learning rate: 0.00995
2024-12-07 12:59:05.294308: Validation loss improved from -0.51170 to -0.52898! Patience: 1/50
2024-12-07 12:59:05.295316: train_loss -0.5543
2024-12-07 12:59:05.296381: val_loss -0.529
2024-12-07 12:59:05.297277: Pseudo dice [0.7283]
2024-12-07 12:59:05.298079: Epoch time: 387.36 s
2024-12-07 12:59:05.298912: Yayy! New best EMA pseudo Dice: 0.6895
2024-12-07 12:59:07.042504: 
2024-12-07 12:59:07.043695: Epoch 6
2024-12-07 12:59:07.044507: Current learning rate: 0.00995
2024-12-07 13:05:36.050948: Validation loss improved from -0.52898 to -0.56093! Patience: 0/50
2024-12-07 13:05:36.052016: train_loss -0.5515
2024-12-07 13:05:36.053003: val_loss -0.5609
2024-12-07 13:05:36.053699: Pseudo dice [0.745]
2024-12-07 13:05:36.054405: Epoch time: 389.01 s
2024-12-07 13:05:36.055027: Yayy! New best EMA pseudo Dice: 0.695
2024-12-07 13:05:37.869937: 
2024-12-07 13:05:37.871122: Epoch 7
2024-12-07 13:05:37.871910: Current learning rate: 0.00994
2024-12-07 13:12:02.431088: Validation loss did not improve from -0.56093. Patience: 1/50
2024-12-07 13:12:02.432380: train_loss -0.568
2024-12-07 13:12:02.433257: val_loss -0.5404
2024-12-07 13:12:02.433896: Pseudo dice [0.7277]
2024-12-07 13:12:02.434592: Epoch time: 384.56 s
2024-12-07 13:12:02.435377: Yayy! New best EMA pseudo Dice: 0.6983
2024-12-07 13:12:04.240647: 
2024-12-07 13:12:04.241877: Epoch 8
2024-12-07 13:12:04.242686: Current learning rate: 0.00993
2024-12-07 13:18:35.765583: Validation loss did not improve from -0.56093. Patience: 2/50
2024-12-07 13:18:35.766406: train_loss -0.5735
2024-12-07 13:18:35.767324: val_loss -0.5436
2024-12-07 13:18:35.768168: Pseudo dice [0.7384]
2024-12-07 13:18:35.769017: Epoch time: 391.53 s
2024-12-07 13:18:35.769972: Yayy! New best EMA pseudo Dice: 0.7023
2024-12-07 13:18:38.067556: 
2024-12-07 13:18:38.069179: Epoch 9
2024-12-07 13:18:38.070334: Current learning rate: 0.00992
2024-12-07 13:25:22.535918: Validation loss did not improve from -0.56093. Patience: 3/50
2024-12-07 13:25:22.537427: train_loss -0.5755
2024-12-07 13:25:22.538427: val_loss -0.5458
2024-12-07 13:25:22.539229: Pseudo dice [0.7284]
2024-12-07 13:25:22.540019: Epoch time: 404.47 s
2024-12-07 13:25:22.978158: Yayy! New best EMA pseudo Dice: 0.7049
2024-12-07 13:25:24.733864: 
2024-12-07 13:25:24.735111: Epoch 10
2024-12-07 13:25:24.736435: Current learning rate: 0.00991
2024-12-07 13:32:01.587202: Validation loss did not improve from -0.56093. Patience: 4/50
2024-12-07 13:32:01.596616: train_loss -0.5983
2024-12-07 13:32:01.597925: val_loss -0.5311
2024-12-07 13:32:01.598602: Pseudo dice [0.7273]
2024-12-07 13:32:01.599437: Epoch time: 396.86 s
2024-12-07 13:32:01.600120: Yayy! New best EMA pseudo Dice: 0.7072
2024-12-07 13:32:03.439465: 
2024-12-07 13:32:03.441062: Epoch 11
2024-12-07 13:32:03.442007: Current learning rate: 0.0099
2024-12-07 13:38:38.731390: Validation loss did not improve from -0.56093. Patience: 5/50
2024-12-07 13:38:38.732331: train_loss -0.5998
2024-12-07 13:38:38.733179: val_loss -0.5195
2024-12-07 13:38:38.733817: Pseudo dice [0.722]
2024-12-07 13:38:38.734582: Epoch time: 395.29 s
2024-12-07 13:38:38.735205: Yayy! New best EMA pseudo Dice: 0.7086
2024-12-07 13:38:40.467815: 
2024-12-07 13:38:40.469251: Epoch 12
2024-12-07 13:38:40.470220: Current learning rate: 0.00989
2024-12-07 13:45:31.249976: Validation loss improved from -0.56093 to -0.56311! Patience: 5/50
2024-12-07 13:45:31.251022: train_loss -0.5817
2024-12-07 13:45:31.252222: val_loss -0.5631
2024-12-07 13:45:31.253014: Pseudo dice [0.7497]
2024-12-07 13:45:31.253841: Epoch time: 410.78 s
2024-12-07 13:45:31.254581: Yayy! New best EMA pseudo Dice: 0.7127
2024-12-07 13:45:33.019716: 
2024-12-07 13:45:33.021189: Epoch 13
2024-12-07 13:45:33.022391: Current learning rate: 0.00988
2024-12-07 13:52:42.467016: Validation loss did not improve from -0.56311. Patience: 1/50
2024-12-07 13:52:42.468117: train_loss -0.5965
2024-12-07 13:52:42.469165: val_loss -0.5249
2024-12-07 13:52:42.470068: Pseudo dice [0.7228]
2024-12-07 13:52:42.470958: Epoch time: 429.45 s
2024-12-07 13:52:42.471911: Yayy! New best EMA pseudo Dice: 0.7138
2024-12-07 13:52:44.238715: 
2024-12-07 13:52:44.240102: Epoch 14
2024-12-07 13:52:44.241060: Current learning rate: 0.00987
2024-12-07 13:59:45.543831: Validation loss did not improve from -0.56311. Patience: 2/50
2024-12-07 13:59:45.544906: train_loss -0.6098
2024-12-07 13:59:45.546150: val_loss -0.5396
2024-12-07 13:59:45.547114: Pseudo dice [0.7398]
2024-12-07 13:59:45.548049: Epoch time: 421.31 s
2024-12-07 13:59:45.946770: Yayy! New best EMA pseudo Dice: 0.7164
2024-12-07 13:59:47.738259: 
2024-12-07 13:59:47.739396: Epoch 15
2024-12-07 13:59:47.740317: Current learning rate: 0.00986
2024-12-07 14:06:43.282609: Validation loss did not improve from -0.56311. Patience: 3/50
2024-12-07 14:06:43.283775: train_loss -0.6157
2024-12-07 14:06:43.284694: val_loss -0.5261
2024-12-07 14:06:43.285497: Pseudo dice [0.727]
2024-12-07 14:06:43.286355: Epoch time: 415.55 s
2024-12-07 14:06:43.287141: Yayy! New best EMA pseudo Dice: 0.7174
2024-12-07 14:06:45.079255: 
2024-12-07 14:06:45.080629: Epoch 16
2024-12-07 14:06:45.081609: Current learning rate: 0.00986
2024-12-07 14:13:45.337706: Validation loss did not improve from -0.56311. Patience: 4/50
2024-12-07 14:13:45.338523: train_loss -0.6221
2024-12-07 14:13:45.339355: val_loss -0.5284
2024-12-07 14:13:45.340162: Pseudo dice [0.7294]
2024-12-07 14:13:45.340840: Epoch time: 420.26 s
2024-12-07 14:13:45.341474: Yayy! New best EMA pseudo Dice: 0.7186
2024-12-07 14:13:47.155909: 
2024-12-07 14:13:47.157581: Epoch 17
2024-12-07 14:13:47.158538: Current learning rate: 0.00985
2024-12-07 14:20:54.816987: Validation loss did not improve from -0.56311. Patience: 5/50
2024-12-07 14:20:54.818185: train_loss -0.6161
2024-12-07 14:20:54.819120: val_loss -0.5481
2024-12-07 14:20:54.820049: Pseudo dice [0.736]
2024-12-07 14:20:54.820866: Epoch time: 427.66 s
2024-12-07 14:20:54.821764: Yayy! New best EMA pseudo Dice: 0.7204
2024-12-07 14:20:56.691834: 
2024-12-07 14:20:56.693180: Epoch 18
2024-12-07 14:20:56.693896: Current learning rate: 0.00984
2024-12-07 14:27:42.533863: Validation loss did not improve from -0.56311. Patience: 6/50
2024-12-07 14:27:42.538232: train_loss -0.6163
2024-12-07 14:27:42.539845: val_loss -0.5346
2024-12-07 14:27:42.540655: Pseudo dice [0.7241]
2024-12-07 14:27:42.541508: Epoch time: 405.85 s
2024-12-07 14:27:42.542250: Yayy! New best EMA pseudo Dice: 0.7207
2024-12-07 14:27:44.855235: 
2024-12-07 14:27:44.856648: Epoch 19
2024-12-07 14:27:44.857451: Current learning rate: 0.00983
2024-12-07 14:34:24.463523: Validation loss improved from -0.56311 to -0.57902! Patience: 6/50
2024-12-07 14:34:24.464651: train_loss -0.6292
2024-12-07 14:34:24.465687: val_loss -0.579
2024-12-07 14:34:24.466612: Pseudo dice [0.7499]
2024-12-07 14:34:24.467453: Epoch time: 399.61 s
2024-12-07 14:34:24.904963: Yayy! New best EMA pseudo Dice: 0.7236
2024-12-07 14:34:26.710985: 
2024-12-07 14:34:26.712394: Epoch 20
2024-12-07 14:34:26.713169: Current learning rate: 0.00982
2024-12-07 14:41:12.351761: Validation loss did not improve from -0.57902. Patience: 1/50
2024-12-07 14:41:12.353052: train_loss -0.6342
2024-12-07 14:41:12.353874: val_loss -0.5664
2024-12-07 14:41:12.354669: Pseudo dice [0.7486]
2024-12-07 14:41:12.355488: Epoch time: 405.64 s
2024-12-07 14:41:12.356185: Yayy! New best EMA pseudo Dice: 0.7261
2024-12-07 14:41:14.246320: 
2024-12-07 14:41:14.247735: Epoch 21
2024-12-07 14:41:14.248513: Current learning rate: 0.00981
2024-12-07 14:47:51.861829: Validation loss did not improve from -0.57902. Patience: 2/50
2024-12-07 14:47:51.863561: train_loss -0.6322
2024-12-07 14:47:51.864538: val_loss -0.5616
2024-12-07 14:47:51.865219: Pseudo dice [0.7482]
2024-12-07 14:47:51.866019: Epoch time: 397.62 s
2024-12-07 14:47:51.866787: Yayy! New best EMA pseudo Dice: 0.7283
2024-12-07 14:47:53.613804: 
2024-12-07 14:47:53.615188: Epoch 22
2024-12-07 14:47:53.616045: Current learning rate: 0.0098
2024-12-07 14:54:30.183930: Validation loss did not improve from -0.57902. Patience: 3/50
2024-12-07 14:54:30.185053: train_loss -0.633
2024-12-07 14:54:30.186254: val_loss -0.5775
2024-12-07 14:54:30.187276: Pseudo dice [0.7559]
2024-12-07 14:54:30.188269: Epoch time: 396.57 s
2024-12-07 14:54:30.189239: Yayy! New best EMA pseudo Dice: 0.7311
2024-12-07 14:54:31.965132: 
2024-12-07 14:54:31.966916: Epoch 23
2024-12-07 14:54:31.967981: Current learning rate: 0.00979
2024-12-07 15:01:10.186754: Validation loss did not improve from -0.57902. Patience: 4/50
2024-12-07 15:01:10.187752: train_loss -0.6344
2024-12-07 15:01:10.188591: val_loss -0.5657
2024-12-07 15:01:10.189536: Pseudo dice [0.753]
2024-12-07 15:01:10.190423: Epoch time: 398.22 s
2024-12-07 15:01:10.191356: Yayy! New best EMA pseudo Dice: 0.7333
2024-12-07 15:01:11.945331: 
2024-12-07 15:01:11.946997: Epoch 24
2024-12-07 15:01:11.948012: Current learning rate: 0.00978
2024-12-07 15:08:05.678070: Validation loss did not improve from -0.57902. Patience: 5/50
2024-12-07 15:08:05.679196: train_loss -0.6369
2024-12-07 15:08:05.679992: val_loss -0.5316
2024-12-07 15:08:05.680656: Pseudo dice [0.7377]
2024-12-07 15:08:05.681573: Epoch time: 413.74 s
2024-12-07 15:08:06.099544: Yayy! New best EMA pseudo Dice: 0.7337
2024-12-07 15:08:07.858009: 
2024-12-07 15:08:07.859392: Epoch 25
2024-12-07 15:08:07.860310: Current learning rate: 0.00977
2024-12-07 15:14:46.809306: Validation loss improved from -0.57902 to -0.59411! Patience: 5/50
2024-12-07 15:14:46.810296: train_loss -0.6481
2024-12-07 15:14:46.811066: val_loss -0.5941
2024-12-07 15:14:46.811769: Pseudo dice [0.7673]
2024-12-07 15:14:46.812671: Epoch time: 398.95 s
2024-12-07 15:14:46.813379: Yayy! New best EMA pseudo Dice: 0.7371
2024-12-07 15:14:48.528503: 
2024-12-07 15:14:48.529842: Epoch 26
2024-12-07 15:14:48.530570: Current learning rate: 0.00977
2024-12-07 15:21:31.142644: Validation loss did not improve from -0.59411. Patience: 1/50
2024-12-07 15:21:31.143719: train_loss -0.6473
2024-12-07 15:21:31.144927: val_loss -0.5838
2024-12-07 15:21:31.145987: Pseudo dice [0.7632]
2024-12-07 15:21:31.147059: Epoch time: 402.62 s
2024-12-07 15:21:31.148085: Yayy! New best EMA pseudo Dice: 0.7397
2024-12-07 15:21:32.915658: 
2024-12-07 15:21:32.916926: Epoch 27
2024-12-07 15:21:32.917677: Current learning rate: 0.00976
2024-12-07 15:28:08.482450: Validation loss improved from -0.59411 to -0.60172! Patience: 1/50
2024-12-07 15:28:08.483446: train_loss -0.6578
2024-12-07 15:28:08.484349: val_loss -0.6017
2024-12-07 15:28:08.485032: Pseudo dice [0.7695]
2024-12-07 15:28:08.485706: Epoch time: 395.57 s
2024-12-07 15:28:08.486450: Yayy! New best EMA pseudo Dice: 0.7427
2024-12-07 15:28:10.311174: 
2024-12-07 15:28:10.312701: Epoch 28
2024-12-07 15:28:10.313833: Current learning rate: 0.00975
2024-12-07 15:34:51.992157: Validation loss did not improve from -0.60172. Patience: 1/50
2024-12-07 15:34:52.016210: train_loss -0.6588
2024-12-07 15:34:52.018190: val_loss -0.5876
2024-12-07 15:34:52.019128: Pseudo dice [0.7671]
2024-12-07 15:34:52.020246: Epoch time: 401.71 s
2024-12-07 15:34:52.021221: Yayy! New best EMA pseudo Dice: 0.7451
2024-12-07 15:34:53.777070: 
2024-12-07 15:34:53.778442: Epoch 29
2024-12-07 15:34:53.779112: Current learning rate: 0.00974
2024-12-07 15:41:40.618745: Validation loss improved from -0.60172 to -0.60806! Patience: 1/50
2024-12-07 15:41:40.619902: train_loss -0.6661
2024-12-07 15:41:40.620682: val_loss -0.6081
2024-12-07 15:41:40.621470: Pseudo dice [0.7821]
2024-12-07 15:41:40.622245: Epoch time: 406.84 s
2024-12-07 15:41:41.044424: Yayy! New best EMA pseudo Dice: 0.7488
2024-12-07 15:41:43.180894: 
2024-12-07 15:41:43.182417: Epoch 30
2024-12-07 15:41:43.183373: Current learning rate: 0.00973
2024-12-07 15:48:57.104359: Validation loss did not improve from -0.60806. Patience: 1/50
2024-12-07 15:48:57.105427: train_loss -0.6602
2024-12-07 15:48:57.106297: val_loss -0.5981
2024-12-07 15:48:57.107045: Pseudo dice [0.7663]
2024-12-07 15:48:57.107738: Epoch time: 433.93 s
2024-12-07 15:48:57.108571: Yayy! New best EMA pseudo Dice: 0.7506
2024-12-07 15:48:58.916843: 
2024-12-07 15:48:58.918442: Epoch 31
2024-12-07 15:48:58.919422: Current learning rate: 0.00972
2024-12-07 15:56:03.831455: Validation loss did not improve from -0.60806. Patience: 2/50
2024-12-07 15:56:03.832376: train_loss -0.6634
2024-12-07 15:56:03.833325: val_loss -0.5795
2024-12-07 15:56:03.834040: Pseudo dice [0.7589]
2024-12-07 15:56:03.834828: Epoch time: 424.92 s
2024-12-07 15:56:03.835583: Yayy! New best EMA pseudo Dice: 0.7514
2024-12-07 15:56:05.650413: 
2024-12-07 15:56:05.651809: Epoch 32
2024-12-07 15:56:05.652699: Current learning rate: 0.00971
2024-12-07 16:02:59.304448: Validation loss did not improve from -0.60806. Patience: 3/50
2024-12-07 16:02:59.305405: train_loss -0.6578
2024-12-07 16:02:59.306540: val_loss -0.5683
2024-12-07 16:02:59.307468: Pseudo dice [0.7536]
2024-12-07 16:02:59.308207: Epoch time: 413.66 s
2024-12-07 16:02:59.309067: Yayy! New best EMA pseudo Dice: 0.7516
2024-12-07 16:03:01.099242: 
2024-12-07 16:03:01.100592: Epoch 33
2024-12-07 16:03:01.101281: Current learning rate: 0.0097
2024-12-07 16:10:03.924361: Validation loss did not improve from -0.60806. Patience: 4/50
2024-12-07 16:10:03.925241: train_loss -0.6599
2024-12-07 16:10:03.925974: val_loss -0.5274
2024-12-07 16:10:03.926636: Pseudo dice [0.7273]
2024-12-07 16:10:03.927366: Epoch time: 422.83 s
2024-12-07 16:10:05.336730: 
2024-12-07 16:10:05.337694: Epoch 34
2024-12-07 16:10:05.338583: Current learning rate: 0.00969
2024-12-07 16:16:56.847282: Validation loss did not improve from -0.60806. Patience: 5/50
2024-12-07 16:16:56.848373: train_loss -0.6698
2024-12-07 16:16:56.849243: val_loss -0.5694
2024-12-07 16:16:56.850078: Pseudo dice [0.7508]
2024-12-07 16:16:56.850930: Epoch time: 411.51 s
2024-12-07 16:16:58.679926: 
2024-12-07 16:16:58.681104: Epoch 35
2024-12-07 16:16:58.681844: Current learning rate: 0.00968
2024-12-07 16:23:38.168307: Validation loss did not improve from -0.60806. Patience: 6/50
2024-12-07 16:23:38.169075: train_loss -0.6753
2024-12-07 16:23:38.169892: val_loss -0.5921
2024-12-07 16:23:38.170582: Pseudo dice [0.7677]
2024-12-07 16:23:38.171409: Epoch time: 399.49 s
2024-12-07 16:23:39.549267: 
2024-12-07 16:23:39.550649: Epoch 36
2024-12-07 16:23:39.551434: Current learning rate: 0.00968
2024-12-07 16:30:34.656111: Validation loss did not improve from -0.60806. Patience: 7/50
2024-12-07 16:30:34.657019: train_loss -0.6667
2024-12-07 16:30:34.657857: val_loss -0.585
2024-12-07 16:30:34.658660: Pseudo dice [0.7623]
2024-12-07 16:30:34.659338: Epoch time: 415.11 s
2024-12-07 16:30:34.659990: Yayy! New best EMA pseudo Dice: 0.7523
2024-12-07 16:30:36.465206: 
2024-12-07 16:30:36.466783: Epoch 37
2024-12-07 16:30:36.467588: Current learning rate: 0.00967
2024-12-07 16:37:32.220451: Validation loss did not improve from -0.60806. Patience: 8/50
2024-12-07 16:37:32.221630: train_loss -0.6813
2024-12-07 16:37:32.223181: val_loss -0.5986
2024-12-07 16:37:32.223936: Pseudo dice [0.7694]
2024-12-07 16:37:32.224869: Epoch time: 415.76 s
2024-12-07 16:37:32.225515: Yayy! New best EMA pseudo Dice: 0.754
2024-12-07 16:37:34.089465: 
2024-12-07 16:37:34.090774: Epoch 38
2024-12-07 16:37:34.091450: Current learning rate: 0.00966
2024-12-07 16:44:24.675178: Validation loss did not improve from -0.60806. Patience: 9/50
2024-12-07 16:44:24.676475: train_loss -0.6731
2024-12-07 16:44:24.677310: val_loss -0.5627
2024-12-07 16:44:24.678011: Pseudo dice [0.746]
2024-12-07 16:44:24.678812: Epoch time: 410.59 s
2024-12-07 16:44:26.088505: 
2024-12-07 16:44:26.090011: Epoch 39
2024-12-07 16:44:26.090912: Current learning rate: 0.00965
2024-12-07 16:51:19.865139: Validation loss did not improve from -0.60806. Patience: 10/50
2024-12-07 16:51:19.866101: train_loss -0.674
2024-12-07 16:51:19.866868: val_loss -0.5929
2024-12-07 16:51:19.867571: Pseudo dice [0.762]
2024-12-07 16:51:19.868478: Epoch time: 413.78 s
2024-12-07 16:51:20.334400: Yayy! New best EMA pseudo Dice: 0.7541
2024-12-07 16:51:22.517356: 
2024-12-07 16:51:22.518678: Epoch 40
2024-12-07 16:51:22.519409: Current learning rate: 0.00964
2024-12-07 16:58:14.572858: Validation loss did not improve from -0.60806. Patience: 11/50
2024-12-07 16:58:14.573836: train_loss -0.6736
2024-12-07 16:58:14.574727: val_loss -0.5744
2024-12-07 16:58:14.575455: Pseudo dice [0.7607]
2024-12-07 16:58:14.576261: Epoch time: 412.06 s
2024-12-07 16:58:14.577002: Yayy! New best EMA pseudo Dice: 0.7548
2024-12-07 16:58:16.429501: 
2024-12-07 16:58:16.430984: Epoch 41
2024-12-07 16:58:16.431828: Current learning rate: 0.00963
2024-12-07 17:05:09.608899: Validation loss did not improve from -0.60806. Patience: 12/50
2024-12-07 17:05:09.610082: train_loss -0.6736
2024-12-07 17:05:09.610968: val_loss -0.5756
2024-12-07 17:05:09.611607: Pseudo dice [0.7632]
2024-12-07 17:05:09.612315: Epoch time: 413.18 s
2024-12-07 17:05:09.612960: Yayy! New best EMA pseudo Dice: 0.7556
2024-12-07 17:05:11.417446: 
2024-12-07 17:05:11.418702: Epoch 42
2024-12-07 17:05:11.419475: Current learning rate: 0.00962
2024-12-07 17:12:06.001006: Validation loss did not improve from -0.60806. Patience: 13/50
2024-12-07 17:12:06.002025: train_loss -0.6835
2024-12-07 17:12:06.002882: val_loss -0.5811
2024-12-07 17:12:06.003549: Pseudo dice [0.7585]
2024-12-07 17:12:06.004217: Epoch time: 414.59 s
2024-12-07 17:12:06.004876: Yayy! New best EMA pseudo Dice: 0.7559
2024-12-07 17:12:07.762740: 
2024-12-07 17:12:07.764035: Epoch 43
2024-12-07 17:12:07.764735: Current learning rate: 0.00961
2024-12-07 17:19:11.440163: Validation loss did not improve from -0.60806. Patience: 14/50
2024-12-07 17:19:11.441292: train_loss -0.6808
2024-12-07 17:19:11.441992: val_loss -0.5955
2024-12-07 17:19:11.442594: Pseudo dice [0.7702]
2024-12-07 17:19:11.443387: Epoch time: 423.68 s
2024-12-07 17:19:11.443993: Yayy! New best EMA pseudo Dice: 0.7573
2024-12-07 17:19:13.170536: 
2024-12-07 17:19:13.171814: Epoch 44
2024-12-07 17:19:13.172517: Current learning rate: 0.0096
2024-12-07 17:26:04.053786: Validation loss did not improve from -0.60806. Patience: 15/50
2024-12-07 17:26:04.054786: train_loss -0.6841
2024-12-07 17:26:04.055630: val_loss -0.5794
2024-12-07 17:26:04.056312: Pseudo dice [0.7612]
2024-12-07 17:26:04.057075: Epoch time: 410.89 s
2024-12-07 17:26:04.465009: Yayy! New best EMA pseudo Dice: 0.7577
2024-12-07 17:26:06.277270: 
2024-12-07 17:26:06.278299: Epoch 45
2024-12-07 17:26:06.279073: Current learning rate: 0.00959
2024-12-07 17:32:35.255580: Validation loss did not improve from -0.60806. Patience: 16/50
2024-12-07 17:32:35.256722: train_loss -0.6881
2024-12-07 17:32:35.257892: val_loss -0.5833
2024-12-07 17:32:35.258941: Pseudo dice [0.7625]
2024-12-07 17:32:35.259923: Epoch time: 388.98 s
2024-12-07 17:32:35.260950: Yayy! New best EMA pseudo Dice: 0.7582
2024-12-07 17:32:37.026377: 
2024-12-07 17:32:37.027970: Epoch 46
2024-12-07 17:32:37.029007: Current learning rate: 0.00959
2024-12-07 17:39:19.375770: Validation loss did not improve from -0.60806. Patience: 17/50
2024-12-07 17:39:19.376708: train_loss -0.6851
2024-12-07 17:39:19.377553: val_loss -0.5667
2024-12-07 17:39:19.378332: Pseudo dice [0.751]
2024-12-07 17:39:19.379103: Epoch time: 402.35 s
2024-12-07 17:39:20.678921: 
2024-12-07 17:39:20.680171: Epoch 47
2024-12-07 17:39:20.680994: Current learning rate: 0.00958
2024-12-07 17:46:11.502827: Validation loss did not improve from -0.60806. Patience: 18/50
2024-12-07 17:46:11.504467: train_loss -0.6902
2024-12-07 17:46:11.505692: val_loss -0.5868
2024-12-07 17:46:11.506659: Pseudo dice [0.7625]
2024-12-07 17:46:11.507748: Epoch time: 410.83 s
2024-12-07 17:46:12.859064: 
2024-12-07 17:46:12.860504: Epoch 48
2024-12-07 17:46:12.861459: Current learning rate: 0.00957
2024-12-07 17:53:00.663981: Validation loss did not improve from -0.60806. Patience: 19/50
2024-12-07 17:53:00.665146: train_loss -0.6894
2024-12-07 17:53:00.666502: val_loss -0.6005
2024-12-07 17:53:00.667663: Pseudo dice [0.771]
2024-12-07 17:53:00.668800: Epoch time: 407.81 s
2024-12-07 17:53:00.669647: Yayy! New best EMA pseudo Dice: 0.7593
2024-12-07 17:53:02.453318: 
2024-12-07 17:53:02.454759: Epoch 49
2024-12-07 17:53:02.455927: Current learning rate: 0.00956
2024-12-07 18:00:01.675990: Validation loss did not improve from -0.60806. Patience: 20/50
2024-12-07 18:00:01.676990: train_loss -0.6848
2024-12-07 18:00:01.678366: val_loss -0.5975
2024-12-07 18:00:01.679319: Pseudo dice [0.7693]
2024-12-07 18:00:01.680456: Epoch time: 419.22 s
2024-12-07 18:00:02.136728: Yayy! New best EMA pseudo Dice: 0.7603
2024-12-07 18:00:03.937872: 
2024-12-07 18:00:03.939567: Epoch 50
2024-12-07 18:00:03.940948: Current learning rate: 0.00955
2024-12-07 18:06:46.534262: Validation loss did not improve from -0.60806. Patience: 21/50
2024-12-07 18:06:46.535123: train_loss -0.6931
2024-12-07 18:06:46.536116: val_loss -0.5763
2024-12-07 18:06:46.537122: Pseudo dice [0.7517]
2024-12-07 18:06:46.538073: Epoch time: 402.6 s
2024-12-07 18:06:48.225399: 
2024-12-07 18:06:48.226926: Epoch 51
2024-12-07 18:06:48.228075: Current learning rate: 0.00954
2024-12-07 18:13:46.157185: Validation loss did not improve from -0.60806. Patience: 22/50
2024-12-07 18:13:46.158224: train_loss -0.69
2024-12-07 18:13:46.159087: val_loss -0.5792
2024-12-07 18:13:46.160187: Pseudo dice [0.7578]
2024-12-07 18:13:46.161099: Epoch time: 417.93 s
2024-12-07 18:13:47.524765: 
2024-12-07 18:13:47.525981: Epoch 52
2024-12-07 18:13:47.526977: Current learning rate: 0.00953
2024-12-07 18:20:37.963398: Validation loss did not improve from -0.60806. Patience: 23/50
2024-12-07 18:20:37.964336: train_loss -0.6985
2024-12-07 18:20:37.965513: val_loss -0.551
2024-12-07 18:20:37.966508: Pseudo dice [0.7435]
2024-12-07 18:20:37.967548: Epoch time: 410.44 s
2024-12-07 18:20:39.328016: 
2024-12-07 18:20:39.329519: Epoch 53
2024-12-07 18:20:39.330639: Current learning rate: 0.00952
2024-12-07 18:27:37.563237: Validation loss did not improve from -0.60806. Patience: 24/50
2024-12-07 18:27:37.564219: train_loss -0.6987
2024-12-07 18:27:37.564934: val_loss -0.5713
2024-12-07 18:27:37.565577: Pseudo dice [0.7585]
2024-12-07 18:27:37.566306: Epoch time: 418.24 s
2024-12-07 18:27:38.939719: 
2024-12-07 18:27:38.941029: Epoch 54
2024-12-07 18:27:38.941990: Current learning rate: 0.00951
2024-12-07 18:34:37.379997: Validation loss did not improve from -0.60806. Patience: 25/50
2024-12-07 18:34:37.381051: train_loss -0.6935
2024-12-07 18:34:37.381851: val_loss -0.5977
2024-12-07 18:34:37.382607: Pseudo dice [0.7688]
2024-12-07 18:34:37.383259: Epoch time: 418.44 s
2024-12-07 18:34:39.202106: 
2024-12-07 18:34:39.203472: Epoch 55
2024-12-07 18:34:39.204209: Current learning rate: 0.0095
2024-12-07 18:41:41.535740: Validation loss did not improve from -0.60806. Patience: 26/50
2024-12-07 18:41:41.536594: train_loss -0.6909
2024-12-07 18:41:41.537425: val_loss -0.5967
2024-12-07 18:41:41.538121: Pseudo dice [0.7658]
2024-12-07 18:41:41.538914: Epoch time: 422.34 s
2024-12-07 18:41:42.907100: 
2024-12-07 18:41:42.908509: Epoch 56
2024-12-07 18:41:42.909501: Current learning rate: 0.00949
2024-12-07 18:48:32.095426: Validation loss did not improve from -0.60806. Patience: 27/50
2024-12-07 18:48:32.099041: train_loss -0.7016
2024-12-07 18:48:32.100354: val_loss -0.5969
2024-12-07 18:48:32.101422: Pseudo dice [0.772]
2024-12-07 18:48:32.102570: Epoch time: 409.19 s
2024-12-07 18:48:32.103654: Yayy! New best EMA pseudo Dice: 0.7608
2024-12-07 18:48:33.898532: 
2024-12-07 18:48:33.899958: Epoch 57
2024-12-07 18:48:33.900889: Current learning rate: 0.00949
2024-12-07 18:55:17.678055: Validation loss did not improve from -0.60806. Patience: 28/50
2024-12-07 18:55:17.678891: train_loss -0.7061
2024-12-07 18:55:17.679641: val_loss -0.5497
2024-12-07 18:55:17.680303: Pseudo dice [0.745]
2024-12-07 18:55:17.680999: Epoch time: 403.78 s
2024-12-07 18:55:19.018604: 
2024-12-07 18:55:19.019935: Epoch 58
2024-12-07 18:55:19.020760: Current learning rate: 0.00948
2024-12-07 19:02:18.194092: Validation loss improved from -0.60806 to -0.61174! Patience: 28/50
2024-12-07 19:02:18.195023: train_loss -0.7068
2024-12-07 19:02:18.196299: val_loss -0.6117
2024-12-07 19:02:18.197387: Pseudo dice [0.7774]
2024-12-07 19:02:18.198532: Epoch time: 419.18 s
2024-12-07 19:02:18.199484: Yayy! New best EMA pseudo Dice: 0.761
2024-12-07 19:02:19.970773: 
2024-12-07 19:02:19.971946: Epoch 59
2024-12-07 19:02:19.972756: Current learning rate: 0.00947
2024-12-07 19:09:18.913631: Validation loss did not improve from -0.61174. Patience: 1/50
2024-12-07 19:09:18.914584: train_loss -0.7036
2024-12-07 19:09:18.915631: val_loss -0.5721
2024-12-07 19:09:18.916585: Pseudo dice [0.7554]
2024-12-07 19:09:18.917512: Epoch time: 418.94 s
2024-12-07 19:09:20.685102: 
2024-12-07 19:09:20.686406: Epoch 60
2024-12-07 19:09:20.687160: Current learning rate: 0.00946
2024-12-07 19:16:21.975875: Validation loss improved from -0.61174 to -0.62992! Patience: 1/50
2024-12-07 19:16:21.976982: train_loss -0.7115
2024-12-07 19:16:21.978201: val_loss -0.6299
2024-12-07 19:16:21.979114: Pseudo dice [0.7893]
2024-12-07 19:16:21.980068: Epoch time: 421.29 s
2024-12-07 19:16:21.981006: Yayy! New best EMA pseudo Dice: 0.7634
2024-12-07 19:16:23.757885: 
2024-12-07 19:16:23.759491: Epoch 61
2024-12-07 19:16:23.760721: Current learning rate: 0.00945
2024-12-07 19:23:20.624003: Validation loss did not improve from -0.62992. Patience: 1/50
2024-12-07 19:23:20.625131: train_loss -0.7089
2024-12-07 19:23:20.625955: val_loss -0.556
2024-12-07 19:23:20.626786: Pseudo dice [0.7459]
2024-12-07 19:23:20.627513: Epoch time: 416.87 s
2024-12-07 19:23:22.180953: 
2024-12-07 19:23:22.182873: Epoch 62
2024-12-07 19:23:22.184107: Current learning rate: 0.00944
2024-12-07 19:30:32.000549: Validation loss did not improve from -0.62992. Patience: 2/50
2024-12-07 19:30:32.001514: train_loss -0.7073
2024-12-07 19:30:32.002362: val_loss -0.5802
2024-12-07 19:30:32.003163: Pseudo dice [0.7557]
2024-12-07 19:30:32.003917: Epoch time: 429.82 s
2024-12-07 19:30:33.757495: 
2024-12-07 19:30:33.758726: Epoch 63
2024-12-07 19:30:33.759481: Current learning rate: 0.00943
2024-12-07 19:37:20.805988: Validation loss did not improve from -0.62992. Patience: 3/50
2024-12-07 19:37:20.807035: train_loss -0.7054
2024-12-07 19:37:20.807915: val_loss -0.5848
2024-12-07 19:37:20.808679: Pseudo dice [0.7582]
2024-12-07 19:37:20.809413: Epoch time: 407.05 s
2024-12-07 19:37:22.163836: 
2024-12-07 19:37:22.165292: Epoch 64
2024-12-07 19:37:22.166267: Current learning rate: 0.00942
2024-12-07 19:44:14.165739: Validation loss did not improve from -0.62992. Patience: 4/50
2024-12-07 19:44:14.166865: train_loss -0.7096
2024-12-07 19:44:14.167750: val_loss -0.5892
2024-12-07 19:44:14.168556: Pseudo dice [0.772]
2024-12-07 19:44:14.169382: Epoch time: 412.0 s
2024-12-07 19:44:16.051868: 
2024-12-07 19:44:16.053287: Epoch 65
2024-12-07 19:44:16.054087: Current learning rate: 0.00941
2024-12-07 19:50:59.907088: Validation loss did not improve from -0.62992. Patience: 5/50
2024-12-07 19:50:59.908196: train_loss -0.711
2024-12-07 19:50:59.909030: val_loss -0.5884
2024-12-07 19:50:59.909863: Pseudo dice [0.7708]
2024-12-07 19:50:59.910601: Epoch time: 403.86 s
2024-12-07 19:51:01.304740: 
2024-12-07 19:51:01.306055: Epoch 66
2024-12-07 19:51:01.306857: Current learning rate: 0.0094
2024-12-07 19:57:41.513893: Validation loss did not improve from -0.62992. Patience: 6/50
2024-12-07 19:57:41.515804: train_loss -0.7081
2024-12-07 19:57:41.516851: val_loss -0.5885
2024-12-07 19:57:41.517732: Pseudo dice [0.7696]
2024-12-07 19:57:41.518479: Epoch time: 400.21 s
2024-12-07 19:57:41.519213: Yayy! New best EMA pseudo Dice: 0.7634
2024-12-07 19:57:43.534847: 
2024-12-07 19:57:43.536274: Epoch 67
2024-12-07 19:57:43.537072: Current learning rate: 0.00939
2024-12-07 20:04:31.994870: Validation loss did not improve from -0.62992. Patience: 7/50
2024-12-07 20:04:31.996075: train_loss -0.713
2024-12-07 20:04:31.997199: val_loss -0.5767
2024-12-07 20:04:31.998003: Pseudo dice [0.7555]
2024-12-07 20:04:31.998969: Epoch time: 408.46 s
2024-12-07 20:04:33.391511: 
2024-12-07 20:04:33.393094: Epoch 68
2024-12-07 20:04:33.394084: Current learning rate: 0.00939
2024-12-07 20:11:29.447663: Validation loss did not improve from -0.62992. Patience: 8/50
2024-12-07 20:11:29.448772: train_loss -0.7125
2024-12-07 20:11:29.449828: val_loss -0.5764
2024-12-07 20:11:29.450726: Pseudo dice [0.7529]
2024-12-07 20:11:29.451743: Epoch time: 416.06 s
2024-12-07 20:11:30.840525: 
2024-12-07 20:11:30.841679: Epoch 69
2024-12-07 20:11:30.843640: Current learning rate: 0.00938
2024-12-07 20:18:07.743143: Validation loss did not improve from -0.62992. Patience: 9/50
2024-12-07 20:18:07.744160: train_loss -0.7159
2024-12-07 20:18:07.745017: val_loss -0.5887
2024-12-07 20:18:07.745766: Pseudo dice [0.764]
2024-12-07 20:18:07.746587: Epoch time: 396.9 s
2024-12-07 20:18:09.556567: 
2024-12-07 20:18:09.557872: Epoch 70
2024-12-07 20:18:09.558843: Current learning rate: 0.00937
2024-12-07 20:24:54.827719: Validation loss did not improve from -0.62992. Patience: 10/50
2024-12-07 20:24:54.828772: train_loss -0.7266
2024-12-07 20:24:54.829596: val_loss -0.592
2024-12-07 20:24:54.830269: Pseudo dice [0.7678]
2024-12-07 20:24:54.831179: Epoch time: 405.27 s
2024-12-07 20:24:56.202914: 
2024-12-07 20:24:56.204373: Epoch 71
2024-12-07 20:24:56.205518: Current learning rate: 0.00936
2024-12-07 20:31:38.343056: Validation loss did not improve from -0.62992. Patience: 11/50
2024-12-07 20:31:38.344206: train_loss -0.7211
2024-12-07 20:31:38.345036: val_loss -0.5951
2024-12-07 20:31:38.345880: Pseudo dice [0.7683]
2024-12-07 20:31:38.346848: Epoch time: 402.14 s
2024-12-07 20:31:39.740157: 
2024-12-07 20:31:39.741769: Epoch 72
2024-12-07 20:31:39.742784: Current learning rate: 0.00935
2024-12-07 20:38:38.040367: Validation loss did not improve from -0.62992. Patience: 12/50
2024-12-07 20:38:38.041743: train_loss -0.7171
2024-12-07 20:38:38.043397: val_loss -0.5763
2024-12-07 20:38:38.044376: Pseudo dice [0.7645]
2024-12-07 20:38:38.045469: Epoch time: 418.3 s
2024-12-07 20:38:39.453589: 
2024-12-07 20:38:39.454777: Epoch 73
2024-12-07 20:38:39.455543: Current learning rate: 0.00934
2024-12-07 20:45:33.539642: Validation loss did not improve from -0.62992. Patience: 13/50
2024-12-07 20:45:33.540651: train_loss -0.7216
2024-12-07 20:45:33.541401: val_loss -0.5738
2024-12-07 20:45:33.542076: Pseudo dice [0.7631]
2024-12-07 20:45:33.542729: Epoch time: 414.09 s
2024-12-07 20:45:35.305849: 
2024-12-07 20:45:35.307080: Epoch 74
2024-12-07 20:45:35.307783: Current learning rate: 0.00933
2024-12-07 20:52:31.025620: Validation loss did not improve from -0.62992. Patience: 14/50
2024-12-07 20:52:31.026858: train_loss -0.7213
2024-12-07 20:52:31.027665: val_loss -0.579
2024-12-07 20:52:31.028411: Pseudo dice [0.7589]
2024-12-07 20:52:31.029286: Epoch time: 415.72 s
2024-12-07 20:52:32.912259: 
2024-12-07 20:52:32.913599: Epoch 75
2024-12-07 20:52:32.914565: Current learning rate: 0.00932
2024-12-07 20:59:30.397975: Validation loss did not improve from -0.62992. Patience: 15/50
2024-12-07 20:59:30.400456: train_loss -0.7191
2024-12-07 20:59:30.401207: val_loss -0.613
2024-12-07 20:59:30.401896: Pseudo dice [0.7759]
2024-12-07 20:59:30.402718: Epoch time: 417.49 s
2024-12-07 20:59:30.403593: Yayy! New best EMA pseudo Dice: 0.7641
2024-12-07 20:59:32.198634: 
2024-12-07 20:59:32.199951: Epoch 76
2024-12-07 20:59:32.200824: Current learning rate: 0.00931
2024-12-07 21:06:19.626307: Validation loss did not improve from -0.62992. Patience: 16/50
2024-12-07 21:06:19.627408: train_loss -0.726
2024-12-07 21:06:19.628298: val_loss -0.6077
2024-12-07 21:06:19.629019: Pseudo dice [0.7708]
2024-12-07 21:06:19.629789: Epoch time: 407.43 s
2024-12-07 21:06:19.630502: Yayy! New best EMA pseudo Dice: 0.7648
2024-12-07 21:06:21.424567: 
2024-12-07 21:06:21.425933: Epoch 77
2024-12-07 21:06:21.426663: Current learning rate: 0.0093
2024-12-07 21:13:16.665512: Validation loss did not improve from -0.62992. Patience: 17/50
2024-12-07 21:13:16.666533: train_loss -0.7095
2024-12-07 21:13:16.667284: val_loss -0.5786
2024-12-07 21:13:16.667969: Pseudo dice [0.7562]
2024-12-07 21:13:16.668753: Epoch time: 415.24 s
2024-12-07 21:13:18.116718: 
2024-12-07 21:13:18.118175: Epoch 78
2024-12-07 21:13:18.118937: Current learning rate: 0.0093
2024-12-07 21:20:13.807748: Validation loss did not improve from -0.62992. Patience: 18/50
2024-12-07 21:20:13.808547: train_loss -0.7207
2024-12-07 21:20:13.809429: val_loss -0.5946
2024-12-07 21:20:13.810082: Pseudo dice [0.7713]
2024-12-07 21:20:13.810781: Epoch time: 415.69 s
2024-12-07 21:20:15.225687: 
2024-12-07 21:20:15.227130: Epoch 79
2024-12-07 21:20:15.227931: Current learning rate: 0.00929
2024-12-07 21:27:04.936765: Validation loss did not improve from -0.62992. Patience: 19/50
2024-12-07 21:27:04.937722: train_loss -0.7183
2024-12-07 21:27:04.938470: val_loss -0.6172
2024-12-07 21:27:04.939226: Pseudo dice [0.7847]
2024-12-07 21:27:04.939898: Epoch time: 409.71 s
2024-12-07 21:27:05.342251: Yayy! New best EMA pseudo Dice: 0.7666
2024-12-07 21:27:07.182689: 
2024-12-07 21:27:07.184108: Epoch 80
2024-12-07 21:27:07.184966: Current learning rate: 0.00928
2024-12-07 21:33:53.777390: Validation loss did not improve from -0.62992. Patience: 20/50
2024-12-07 21:33:53.778304: train_loss -0.7214
2024-12-07 21:33:53.779186: val_loss -0.5991
2024-12-07 21:33:53.779918: Pseudo dice [0.7653]
2024-12-07 21:33:53.780693: Epoch time: 406.6 s
2024-12-07 21:33:55.173327: 
2024-12-07 21:33:55.174443: Epoch 81
2024-12-07 21:33:55.175136: Current learning rate: 0.00927
2024-12-07 21:40:42.842203: Validation loss did not improve from -0.62992. Patience: 21/50
2024-12-07 21:40:42.843202: train_loss -0.7204
2024-12-07 21:40:42.844402: val_loss -0.6142
2024-12-07 21:40:42.845394: Pseudo dice [0.7849]
2024-12-07 21:40:42.846319: Epoch time: 407.67 s
2024-12-07 21:40:42.847272: Yayy! New best EMA pseudo Dice: 0.7684
2024-12-07 21:40:44.805634: 
2024-12-07 21:40:44.807203: Epoch 82
2024-12-07 21:40:44.808479: Current learning rate: 0.00926
2024-12-07 21:47:22.476856: Validation loss did not improve from -0.62992. Patience: 22/50
2024-12-07 21:47:22.477549: train_loss -0.717
2024-12-07 21:47:22.478279: val_loss -0.605
2024-12-07 21:47:22.479143: Pseudo dice [0.7766]
2024-12-07 21:47:22.479978: Epoch time: 397.67 s
2024-12-07 21:47:22.480809: Yayy! New best EMA pseudo Dice: 0.7692
2024-12-07 21:47:24.196112: 
2024-12-07 21:47:24.197616: Epoch 83
2024-12-07 21:47:24.198755: Current learning rate: 0.00925
2024-12-07 21:54:05.239091: Validation loss did not improve from -0.62992. Patience: 23/50
2024-12-07 21:54:05.239979: train_loss -0.7251
2024-12-07 21:54:05.240978: val_loss -0.6212
2024-12-07 21:54:05.241740: Pseudo dice [0.7827]
2024-12-07 21:54:05.242440: Epoch time: 401.05 s
2024-12-07 21:54:05.243403: Yayy! New best EMA pseudo Dice: 0.7705
2024-12-07 21:54:07.308761: 
2024-12-07 21:54:07.310856: Epoch 84
2024-12-07 21:54:07.312022: Current learning rate: 0.00924
2024-12-07 22:00:52.573204: Validation loss did not improve from -0.62992. Patience: 24/50
2024-12-07 22:00:52.574383: train_loss -0.7301
2024-12-07 22:00:52.575739: val_loss -0.5663
2024-12-07 22:00:52.576825: Pseudo dice [0.7601]
2024-12-07 22:00:52.577859: Epoch time: 405.27 s
2024-12-07 22:00:54.384565: 
2024-12-07 22:00:54.386438: Epoch 85
2024-12-07 22:00:54.387346: Current learning rate: 0.00923
2024-12-07 22:07:51.064664: Validation loss did not improve from -0.62992. Patience: 25/50
2024-12-07 22:07:51.065740: train_loss -0.7314
2024-12-07 22:07:51.066530: val_loss -0.5817
2024-12-07 22:07:51.067298: Pseudo dice [0.7596]
2024-12-07 22:07:51.067975: Epoch time: 416.68 s
2024-12-07 22:07:52.395129: 
2024-12-07 22:07:52.396720: Epoch 86
2024-12-07 22:07:52.397643: Current learning rate: 0.00922
2024-12-07 22:14:44.579607: Validation loss did not improve from -0.62992. Patience: 26/50
2024-12-07 22:14:44.581442: train_loss -0.7281
2024-12-07 22:14:44.582414: val_loss -0.5884
2024-12-07 22:14:44.583218: Pseudo dice [0.7689]
2024-12-07 22:14:44.583994: Epoch time: 412.19 s
2024-12-07 22:14:45.944813: 
2024-12-07 22:14:45.946483: Epoch 87
2024-12-07 22:14:45.947275: Current learning rate: 0.00921
2024-12-07 22:21:32.676008: Validation loss did not improve from -0.62992. Patience: 27/50
2024-12-07 22:21:32.677091: train_loss -0.7305
2024-12-07 22:21:32.677935: val_loss -0.6012
2024-12-07 22:21:32.678756: Pseudo dice [0.7747]
2024-12-07 22:21:32.679702: Epoch time: 406.73 s
2024-12-07 22:21:34.013932: 
2024-12-07 22:21:34.015748: Epoch 88
2024-12-07 22:21:34.016682: Current learning rate: 0.0092
2024-12-07 22:28:33.553689: Validation loss did not improve from -0.62992. Patience: 28/50
2024-12-07 22:28:33.554568: train_loss -0.7298
2024-12-07 22:28:33.555334: val_loss -0.597
2024-12-07 22:28:33.555987: Pseudo dice [0.7733]
2024-12-07 22:28:33.556782: Epoch time: 419.54 s
2024-12-07 22:28:34.913863: 
2024-12-07 22:28:34.915246: Epoch 89
2024-12-07 22:28:34.916132: Current learning rate: 0.0092
2024-12-07 22:35:30.282135: Validation loss did not improve from -0.62992. Patience: 29/50
2024-12-07 22:35:30.283157: train_loss -0.7369
2024-12-07 22:35:30.284217: val_loss -0.5779
2024-12-07 22:35:30.285201: Pseudo dice [0.7627]
2024-12-07 22:35:30.286221: Epoch time: 415.37 s
2024-12-07 22:35:32.010194: 
2024-12-07 22:35:32.011871: Epoch 90
2024-12-07 22:35:32.012903: Current learning rate: 0.00919
2024-12-07 22:42:32.214855: Validation loss did not improve from -0.62992. Patience: 30/50
2024-12-07 22:42:32.215865: train_loss -0.7387
2024-12-07 22:42:32.216797: val_loss -0.5859
2024-12-07 22:42:32.217659: Pseudo dice [0.7672]
2024-12-07 22:42:32.218483: Epoch time: 420.21 s
2024-12-07 22:42:33.605946: 
2024-12-07 22:42:33.607418: Epoch 91
2024-12-07 22:42:33.608300: Current learning rate: 0.00918
2024-12-07 22:49:49.146667: Validation loss did not improve from -0.62992. Patience: 31/50
2024-12-07 22:49:49.147771: train_loss -0.7356
2024-12-07 22:49:49.148748: val_loss -0.5947
2024-12-07 22:49:49.149521: Pseudo dice [0.7691]
2024-12-07 22:49:49.150267: Epoch time: 435.54 s
2024-12-07 22:49:50.470034: 
2024-12-07 22:49:50.471371: Epoch 92
2024-12-07 22:49:50.472284: Current learning rate: 0.00917
2024-12-07 22:56:38.562058: Validation loss did not improve from -0.62992. Patience: 32/50
2024-12-07 22:56:38.563036: train_loss -0.7288
2024-12-07 22:56:38.563911: val_loss -0.6072
2024-12-07 22:56:38.564725: Pseudo dice [0.7772]
2024-12-07 22:56:38.565639: Epoch time: 408.1 s
2024-12-07 22:56:39.887056: 
2024-12-07 22:56:39.888515: Epoch 93
2024-12-07 22:56:39.889356: Current learning rate: 0.00916
2024-12-07 23:03:23.884665: Validation loss did not improve from -0.62992. Patience: 33/50
2024-12-07 23:03:23.885576: train_loss -0.7291
2024-12-07 23:03:23.886397: val_loss -0.5982
2024-12-07 23:03:23.887021: Pseudo dice [0.7754]
2024-12-07 23:03:23.887702: Epoch time: 404.0 s
2024-12-07 23:03:25.190068: 
2024-12-07 23:03:25.191457: Epoch 94
2024-12-07 23:03:25.192159: Current learning rate: 0.00915
2024-12-07 23:10:11.185298: Validation loss did not improve from -0.62992. Patience: 34/50
2024-12-07 23:10:11.223795: train_loss -0.7352
2024-12-07 23:10:11.224944: val_loss -0.563
2024-12-07 23:10:11.225996: Pseudo dice [0.7542]
2024-12-07 23:10:11.227297: Epoch time: 406.03 s
2024-12-07 23:10:13.106715: 
2024-12-07 23:10:13.107976: Epoch 95
2024-12-07 23:10:13.108730: Current learning rate: 0.00914
2024-12-07 23:17:02.600933: Validation loss did not improve from -0.62992. Patience: 35/50
2024-12-07 23:17:02.601925: train_loss -0.73
2024-12-07 23:17:02.602749: val_loss -0.5599
2024-12-07 23:17:02.603497: Pseudo dice [0.7509]
2024-12-07 23:17:02.604196: Epoch time: 409.5 s
2024-12-07 23:17:04.743122: 
2024-12-07 23:17:04.744829: Epoch 96
2024-12-07 23:17:04.745835: Current learning rate: 0.00913
2024-12-07 23:23:50.181667: Validation loss did not improve from -0.62992. Patience: 36/50
2024-12-07 23:23:50.182926: train_loss -0.7309
2024-12-07 23:23:50.183777: val_loss -0.6214
2024-12-07 23:23:50.184540: Pseudo dice [0.7779]
2024-12-07 23:23:50.185338: Epoch time: 405.44 s
2024-12-07 23:23:51.515697: 
2024-12-07 23:23:51.517071: Epoch 97
2024-12-07 23:23:51.517910: Current learning rate: 0.00912
2024-12-07 23:30:24.109761: Validation loss did not improve from -0.62992. Patience: 37/50
2024-12-07 23:30:24.110807: train_loss -0.7339
2024-12-07 23:30:24.111515: val_loss -0.6131
2024-12-07 23:30:24.112266: Pseudo dice [0.7868]
2024-12-07 23:30:24.113030: Epoch time: 392.6 s
2024-12-07 23:30:25.496315: 
2024-12-07 23:30:25.497960: Epoch 98
2024-12-07 23:30:25.498860: Current learning rate: 0.00911
2024-12-07 23:37:26.775994: Validation loss improved from -0.62992 to -0.63292! Patience: 37/50
2024-12-07 23:37:26.777180: train_loss -0.7347
2024-12-07 23:37:26.778180: val_loss -0.6329
2024-12-07 23:37:26.779079: Pseudo dice [0.7875]
2024-12-07 23:37:26.780119: Epoch time: 421.28 s
2024-12-07 23:37:26.781138: Yayy! New best EMA pseudo Dice: 0.7716
2024-12-07 23:37:28.634863: 
2024-12-07 23:37:28.636195: Epoch 99
2024-12-07 23:37:28.637158: Current learning rate: 0.0091
2024-12-07 23:44:20.142003: Validation loss did not improve from -0.63292. Patience: 1/50
2024-12-07 23:44:20.143077: train_loss -0.74
2024-12-07 23:44:20.143885: val_loss -0.5995
2024-12-07 23:44:20.144600: Pseudo dice [0.7685]
2024-12-07 23:44:20.145395: Epoch time: 411.51 s
2024-12-07 23:44:21.872614: 
2024-12-07 23:44:21.873894: Epoch 100
2024-12-07 23:44:21.874708: Current learning rate: 0.0091
2024-12-07 23:51:12.254145: Validation loss did not improve from -0.63292. Patience: 2/50
2024-12-07 23:51:12.255240: train_loss -0.7369
2024-12-07 23:51:12.256138: val_loss -0.5934
2024-12-07 23:51:12.257194: Pseudo dice [0.7666]
2024-12-07 23:51:12.258029: Epoch time: 410.38 s
2024-12-07 23:51:13.674994: 
2024-12-07 23:51:13.676596: Epoch 101
2024-12-07 23:51:13.677815: Current learning rate: 0.00909
2024-12-07 23:58:09.307929: Validation loss did not improve from -0.63292. Patience: 3/50
2024-12-07 23:58:09.309660: train_loss -0.7387
2024-12-07 23:58:09.310766: val_loss -0.5913
2024-12-07 23:58:09.311479: Pseudo dice [0.7701]
2024-12-07 23:58:09.312287: Epoch time: 415.64 s
2024-12-07 23:58:10.768163: 
2024-12-07 23:58:10.769578: Epoch 102
2024-12-07 23:58:10.770305: Current learning rate: 0.00908
2024-12-08 00:05:04.433747: Validation loss did not improve from -0.63292. Patience: 4/50
2024-12-08 00:05:04.434833: train_loss -0.7415
2024-12-08 00:05:04.435691: val_loss -0.6111
2024-12-08 00:05:04.436594: Pseudo dice [0.7767]
2024-12-08 00:05:04.437314: Epoch time: 413.67 s
2024-12-08 00:05:05.778823: 
2024-12-08 00:05:05.780431: Epoch 103
2024-12-08 00:05:05.781429: Current learning rate: 0.00907
2024-12-08 00:11:46.150580: Validation loss did not improve from -0.63292. Patience: 5/50
2024-12-08 00:11:46.152342: train_loss -0.7451
2024-12-08 00:11:46.153514: val_loss -0.5773
2024-12-08 00:11:46.154289: Pseudo dice [0.7603]
2024-12-08 00:11:46.155082: Epoch time: 400.37 s
2024-12-08 00:11:47.533282: 
2024-12-08 00:11:47.534894: Epoch 104
2024-12-08 00:11:47.535669: Current learning rate: 0.00906
2024-12-08 00:18:41.668635: Validation loss did not improve from -0.63292. Patience: 6/50
2024-12-08 00:18:41.669702: train_loss -0.7383
2024-12-08 00:18:41.670693: val_loss -0.6218
2024-12-08 00:18:41.671579: Pseudo dice [0.7815]
2024-12-08 00:18:41.672710: Epoch time: 414.14 s
2024-12-08 00:18:43.517872: 
2024-12-08 00:18:43.519382: Epoch 105
2024-12-08 00:18:43.520406: Current learning rate: 0.00905
2024-12-08 00:25:51.229561: Validation loss did not improve from -0.63292. Patience: 7/50
2024-12-08 00:25:51.230515: train_loss -0.7403
2024-12-08 00:25:51.231433: val_loss -0.5707
2024-12-08 00:25:51.232136: Pseudo dice [0.7562]
2024-12-08 00:25:51.232825: Epoch time: 427.71 s
2024-12-08 00:25:52.566006: 
2024-12-08 00:25:52.567248: Epoch 106
2024-12-08 00:25:52.567962: Current learning rate: 0.00904
2024-12-08 00:32:55.639409: Validation loss did not improve from -0.63292. Patience: 8/50
2024-12-08 00:32:55.640579: train_loss -0.7403
2024-12-08 00:32:55.641358: val_loss -0.6012
2024-12-08 00:32:55.642185: Pseudo dice [0.7718]
2024-12-08 00:32:55.642907: Epoch time: 423.08 s
2024-12-08 00:32:56.979542: 
2024-12-08 00:32:56.981238: Epoch 107
2024-12-08 00:32:56.982178: Current learning rate: 0.00903
2024-12-08 00:40:09.340416: Validation loss did not improve from -0.63292. Patience: 9/50
2024-12-08 00:40:09.341281: train_loss -0.7312
2024-12-08 00:40:09.342274: val_loss -0.5866
2024-12-08 00:40:09.343116: Pseudo dice [0.7653]
2024-12-08 00:40:09.343915: Epoch time: 432.36 s
2024-12-08 00:40:11.179551: 
2024-12-08 00:40:11.180857: Epoch 108
2024-12-08 00:40:11.181592: Current learning rate: 0.00902
2024-12-08 00:47:25.961868: Validation loss did not improve from -0.63292. Patience: 10/50
2024-12-08 00:47:25.963009: train_loss -0.7354
2024-12-08 00:47:25.963833: val_loss -0.5779
2024-12-08 00:47:25.964731: Pseudo dice [0.7602]
2024-12-08 00:47:25.965520: Epoch time: 434.78 s
2024-12-08 00:47:27.308966: 
2024-12-08 00:47:27.310304: Epoch 109
2024-12-08 00:47:27.311085: Current learning rate: 0.00901
2024-12-08 00:54:47.950801: Validation loss did not improve from -0.63292. Patience: 11/50
2024-12-08 00:54:47.951731: train_loss -0.7358
2024-12-08 00:54:47.952457: val_loss -0.5999
2024-12-08 00:54:47.953163: Pseudo dice [0.7702]
2024-12-08 00:54:47.953967: Epoch time: 440.64 s
2024-12-08 00:54:49.749207: 
2024-12-08 00:54:49.750490: Epoch 110
2024-12-08 00:54:49.751397: Current learning rate: 0.009
2024-12-08 01:02:03.423112: Validation loss did not improve from -0.63292. Patience: 12/50
2024-12-08 01:02:03.424190: train_loss -0.7374
2024-12-08 01:02:03.425143: val_loss -0.5872
2024-12-08 01:02:03.425898: Pseudo dice [0.7674]
2024-12-08 01:02:03.426698: Epoch time: 433.68 s
2024-12-08 01:02:04.776130: 
2024-12-08 01:02:04.777561: Epoch 111
2024-12-08 01:02:04.778626: Current learning rate: 0.009
2024-12-08 01:09:20.879761: Validation loss did not improve from -0.63292. Patience: 13/50
2024-12-08 01:09:20.880735: train_loss -0.7429
2024-12-08 01:09:20.881669: val_loss -0.5934
2024-12-08 01:09:20.882396: Pseudo dice [0.7755]
2024-12-08 01:09:20.883065: Epoch time: 436.11 s
2024-12-08 01:09:22.255517: 
2024-12-08 01:09:22.256857: Epoch 112
2024-12-08 01:09:22.257594: Current learning rate: 0.00899
2024-12-08 01:16:48.644572: Validation loss did not improve from -0.63292. Patience: 14/50
2024-12-08 01:16:48.645646: train_loss -0.7453
2024-12-08 01:16:48.646385: val_loss -0.6067
2024-12-08 01:16:48.647149: Pseudo dice [0.7774]
2024-12-08 01:16:48.647916: Epoch time: 446.39 s
2024-12-08 01:16:49.978643: 
2024-12-08 01:16:49.979930: Epoch 113
2024-12-08 01:16:49.980685: Current learning rate: 0.00898
2024-12-08 01:24:13.200788: Validation loss did not improve from -0.63292. Patience: 15/50
2024-12-08 01:24:13.201948: train_loss -0.7447
2024-12-08 01:24:13.202801: val_loss -0.6137
2024-12-08 01:24:13.203551: Pseudo dice [0.778]
2024-12-08 01:24:13.204260: Epoch time: 443.22 s
2024-12-08 01:24:14.562106: 
2024-12-08 01:24:14.563667: Epoch 114
2024-12-08 01:24:14.564553: Current learning rate: 0.00897
2024-12-08 01:31:34.666020: Validation loss did not improve from -0.63292. Patience: 16/50
2024-12-08 01:31:34.667371: train_loss -0.7422
2024-12-08 01:31:34.668125: val_loss -0.6035
2024-12-08 01:31:34.668741: Pseudo dice [0.7784]
2024-12-08 01:31:34.669418: Epoch time: 440.11 s
2024-12-08 01:31:35.110098: Yayy! New best EMA pseudo Dice: 0.7717
2024-12-08 01:31:36.875041: 
2024-12-08 01:31:36.876353: Epoch 115
2024-12-08 01:31:36.877222: Current learning rate: 0.00896
2024-12-08 01:39:00.579529: Validation loss did not improve from -0.63292. Patience: 17/50
2024-12-08 01:39:00.580523: train_loss -0.7457
2024-12-08 01:39:00.581513: val_loss -0.5909
2024-12-08 01:39:00.582467: Pseudo dice [0.7678]
2024-12-08 01:39:00.583333: Epoch time: 443.71 s
2024-12-08 01:39:01.959048: 
2024-12-08 01:39:01.960675: Epoch 116
2024-12-08 01:39:01.961716: Current learning rate: 0.00895
2024-12-08 01:46:22.859783: Validation loss did not improve from -0.63292. Patience: 18/50
2024-12-08 01:46:22.860918: train_loss -0.7406
2024-12-08 01:46:22.861801: val_loss -0.6114
2024-12-08 01:46:22.862579: Pseudo dice [0.7742]
2024-12-08 01:46:22.863365: Epoch time: 440.9 s
2024-12-08 01:46:24.274466: 
2024-12-08 01:46:24.275729: Epoch 117
2024-12-08 01:46:24.276628: Current learning rate: 0.00894
2024-12-08 01:53:41.054955: Validation loss did not improve from -0.63292. Patience: 19/50
2024-12-08 01:53:41.055995: train_loss -0.7384
2024-12-08 01:53:41.056998: val_loss -0.5872
2024-12-08 01:53:41.057766: Pseudo dice [0.7692]
2024-12-08 01:53:41.058582: Epoch time: 436.78 s
2024-12-08 01:53:42.563682: 
2024-12-08 01:53:42.565634: Epoch 118
2024-12-08 01:53:42.566496: Current learning rate: 0.00893
2024-12-08 02:00:47.963449: Validation loss did not improve from -0.63292. Patience: 20/50
2024-12-08 02:00:47.964668: train_loss -0.7429
2024-12-08 02:00:47.965474: val_loss -0.6153
2024-12-08 02:00:47.966235: Pseudo dice [0.7848]
2024-12-08 02:00:47.966978: Epoch time: 425.4 s
2024-12-08 02:00:47.967769: Yayy! New best EMA pseudo Dice: 0.7727
2024-12-08 02:00:50.269680: 
2024-12-08 02:00:50.271743: Epoch 119
2024-12-08 02:00:50.272757: Current learning rate: 0.00892
2024-12-08 02:07:15.220438: Validation loss did not improve from -0.63292. Patience: 21/50
2024-12-08 02:07:15.221454: train_loss -0.7453
2024-12-08 02:07:15.222436: val_loss -0.5882
2024-12-08 02:07:15.223262: Pseudo dice [0.7624]
2024-12-08 02:07:15.224037: Epoch time: 384.95 s
2024-12-08 02:07:17.053334: 
2024-12-08 02:07:17.055007: Epoch 120
2024-12-08 02:07:17.055978: Current learning rate: 0.00891
2024-12-08 02:13:40.563466: Validation loss did not improve from -0.63292. Patience: 22/50
2024-12-08 02:13:40.564486: train_loss -0.746
2024-12-08 02:13:40.565343: val_loss -0.5973
2024-12-08 02:13:40.566174: Pseudo dice [0.7682]
2024-12-08 02:13:40.566872: Epoch time: 383.51 s
2024-12-08 02:13:41.956185: 
2024-12-08 02:13:41.957942: Epoch 121
2024-12-08 02:13:41.958708: Current learning rate: 0.0089
2024-12-08 02:19:23.930765: failed to log:  (<class 'FileNotFoundError'>, FileNotFoundError(2, 'No such file or directory'), <traceback object at 0x7f3574682dc0>)
2024-12-08 02:19:23.930765: failed to log:  (<class 'FileNotFoundError'>, FileNotFoundError(2, 'No such file or directory'), <traceback object at 0x7f3570784f40>)
2024-12-08 02:19:23.930765: failed to log:  (<class 'FileNotFoundError'>, FileNotFoundError(2, 'No such file or directory'), <traceback object at 0x7f3570784f40>)
2024-12-08 02:19:23.930765: failed to log:  (<class 'FileNotFoundError'>, FileNotFoundError(2, 'No such file or directory'), <traceback object at 0x7f3570784f40>)
2024-12-08 02:19:23.930765: failed to log:  (<class 'FileNotFoundError'>, FileNotFoundError(2, 'No such file or directory'), <traceback object at 0x7f3570784f40>)
2024-12-08 02:19:23.930765: Validation loss did not improve from -0.63292. Patience: 23/50
2024-12-08 02:19:26.435125: failed to log:  (<class 'FileNotFoundError'>, FileNotFoundError(2, 'No such file or directory'), <traceback object at 0x7f3570784f40>)
2024-12-08 02:19:26.435125: failed to log:  (<class 'FileNotFoundError'>, FileNotFoundError(2, 'No such file or directory'), <traceback object at 0x7f3570784f40>)
2024-12-08 02:19:26.435125: failed to log:  (<class 'FileNotFoundError'>, FileNotFoundError(2, 'No such file or directory'), <traceback object at 0x7f3570784f40>)
2024-12-08 02:19:26.435125: failed to log:  (<class 'FileNotFoundError'>, FileNotFoundError(2, 'No such file or directory'), <traceback object at 0x7f3570784f40>)
2024-12-08 02:19:26.435125: failed to log:  (<class 'FileNotFoundError'>, FileNotFoundError(2, 'No such file or directory'), <traceback object at 0x7f3570784f40>)
2024-12-08 02:19:26.435125: train_loss -0.7532
2024-12-08 02:19:28.938502: failed to log:  (<class 'FileNotFoundError'>, FileNotFoundError(2, 'No such file or directory'), <traceback object at 0x7f3570b5de40>)
2024-12-08 02:19:28.938502: failed to log:  (<class 'FileNotFoundError'>, FileNotFoundError(2, 'No such file or directory'), <traceback object at 0x7f3570b5de40>)
2024-12-08 02:19:28.938502: failed to log:  (<class 'FileNotFoundError'>, FileNotFoundError(2, 'No such file or directory'), <traceback object at 0x7f3570b5de40>)
2024-12-08 02:19:28.938502: failed to log:  (<class 'FileNotFoundError'>, FileNotFoundError(2, 'No such file or directory'), <traceback object at 0x7f3570b5de40>)
2024-12-08 02:19:28.938502: failed to log:  (<class 'FileNotFoundError'>, FileNotFoundError(2, 'No such file or directory'), <traceback object at 0x7f3570b5de40>)
2024-12-08 02:19:28.938502: val_loss -0.5869
2024-12-08 02:19:31.441591: failed to log:  (<class 'FileNotFoundError'>, FileNotFoundError(2, 'No such file or directory'), <traceback object at 0x7f3570d88400>)
2024-12-08 02:19:31.441591: failed to log:  (<class 'FileNotFoundError'>, FileNotFoundError(2, 'No such file or directory'), <traceback object at 0x7f3570d88400>)
2024-12-08 02:19:31.441591: failed to log:  (<class 'FileNotFoundError'>, FileNotFoundError(2, 'No such file or directory'), <traceback object at 0x7f3570d88400>)
2024-12-08 02:19:31.441591: failed to log:  (<class 'FileNotFoundError'>, FileNotFoundError(2, 'No such file or directory'), <traceback object at 0x7f3570d88400>)
2024-12-08 02:19:31.441591: failed to log:  (<class 'FileNotFoundError'>, FileNotFoundError(2, 'No such file or directory'), <traceback object at 0x7f3570d88400>)
2024-12-08 02:19:31.441591: Pseudo dice [0.7659]
2024-12-08 02:19:33.944319: failed to log:  (<class 'FileNotFoundError'>, FileNotFoundError(2, 'No such file or directory'), <traceback object at 0x7f3546f1f680>)
2024-12-08 02:19:33.944319: failed to log:  (<class 'FileNotFoundError'>, FileNotFoundError(2, 'No such file or directory'), <traceback object at 0x7f3546f1f680>)
2024-12-08 02:19:33.944319: failed to log:  (<class 'FileNotFoundError'>, FileNotFoundError(2, 'No such file or directory'), <traceback object at 0x7f3546f1f680>)
2024-12-08 02:19:33.944319: failed to log:  (<class 'FileNotFoundError'>, FileNotFoundError(2, 'No such file or directory'), <traceback object at 0x7f3546f1f680>)
2024-12-08 02:19:33.944319: failed to log:  (<class 'FileNotFoundError'>, FileNotFoundError(2, 'No such file or directory'), <traceback object at 0x7f3546f1f680>)
2024-12-08 02:19:33.944319: Epoch time: 344.48 s
Traceback (most recent call last):
  File "/home/gridsan/nchutisilp/.conda/envs/nnunet/bin/nnUNetv2_train", line 8, in <module>
    sys.exit(run_training_entry())
  File "/home/gridsan/nchutisilp/projects/nnUNet/nnunetv2/run/run_training.py", line 275, in run_training_entry
    run_training(args.dataset_name_or_id, args.configuration, args.fold, args.tr, args.p, args.pretrained_weights,
  File "/home/gridsan/nchutisilp/projects/nnUNet/nnunetv2/run/run_training.py", line 211, in run_training
    nnunet_trainer.run_training()
  File "/home/gridsan/nchutisilp/projects/nnUNet/nnunetv2/training/nnUNetTrainer/nnUNetTrainer.py", line 1411, in run_training
    self.on_epoch_end()
  File "/home/gridsan/nchutisilp/projects/nnUNet/nnunetv2/training/nnUNetTrainer/nnUNetTrainer.py", line 1174, in on_epoch_end
    self.logger.plot_progress_png(self.output_folder)
  File "/home/gridsan/nchutisilp/projects/nnUNet/nnunetv2/training/logging/nnunet_logger.py", line 99, in plot_progress_png
    fig.savefig(join(output_folder, "progress.png"))
  File "/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/matplotlib/figure.py", line 3390, in savefig
    self.canvas.print_figure(fname, **kwargs)
  File "/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/matplotlib/backend_bases.py", line 2193, in print_figure
    result = print_method(
  File "/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/matplotlib/backend_bases.py", line 2043, in <lambda>
    print_method = functools.wraps(meth)(lambda *args, **kwargs: meth(
  File "/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/matplotlib/backends/backend_agg.py", line 497, in print_png
    self._print_pil(filename_or_obj, "png", pil_kwargs, metadata)
  File "/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/matplotlib/backends/backend_agg.py", line 446, in _print_pil
    mpl.image.imsave(
  File "/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/matplotlib/image.py", line 1656, in imsave
    image.save(fname, **pil_kwargs)
  File "/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/PIL/Image.py", line 2456, in save
    fp = builtins.open(filename, "w+b")
FileNotFoundError: [Errno 2] No such file or directory: '/home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_results/Dataset307_Sohee_Calcium_OCT_CrossValidation/nnUNetTrainer__nnUNetPlans__3d_32x160x128_b10/fold_2/progress.png'
Exception in thread Thread-2:
Traceback (most recent call last):
  File "/home/gridsan/nchutisilp/.conda/envs/nnunet/lib/python3.9/threading.py", line 980, in _bootstrap_inner
    self.run()
  File "/home/gridsan/nchutisilp/.conda/envs/nnunet/lib/python3.9/threading.py", line 917, in run
    self._target(*self._args, **self._kwargs)
  File "/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/batchgenerators/dataloading/nondet_multi_threaded_augmenter.py", line 125, in results_loop
Exception in thread Thread-3:
Traceback (most recent call last):
  File "/home/gridsan/nchutisilp/.conda/envs/nnunet/lib/python3.9/threading.py", line 980, in _bootstrap_inner
    raise e
  File "/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/batchgenerators/dataloading/nondet_multi_threaded_augmenter.py", line 103, in results_loop
    self.run()
  File "/home/gridsan/nchutisilp/.conda/envs/nnunet/lib/python3.9/threading.py", line 917, in run
    raise RuntimeError("One or more background workers are no longer alive. Exiting. Please check the "
RuntimeError: One or more background workers are no longer alive. Exiting. Please check the print statements above for the actual error message
    self._target(*self._args, **self._kwargs)
  File "/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/batchgenerators/dataloading/nondet_multi_threaded_augmenter.py", line 125, in results_loop
    raise e
  File "/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/batchgenerators/dataloading/nondet_multi_threaded_augmenter.py", line 103, in results_loop
    raise RuntimeError("One or more background workers are no longer alive. Exiting. Please check the "
RuntimeError: One or more background workers are no longer alive. Exiting. Please check the print statements above for the actual error message
2024-12-07 12:17:55.754048: unpacking done...
2024-12-07 12:17:55.786219: Unable to plot network architecture: nnUNet_compile is enabled!
2024-12-07 12:17:55.834915: 
2024-12-07 12:17:55.836301: Epoch 0
2024-12-07 12:17:55.837215: Current learning rate: 0.01
2024-12-07 12:25:36.964330: Validation loss improved from 1000.00000 to -0.34642! Patience: 0/50
2024-12-07 12:25:36.965297: train_loss -0.3375
2024-12-07 12:25:36.966172: val_loss -0.3464
2024-12-07 12:25:36.966987: Pseudo dice [0.6547]
2024-12-07 12:25:36.967796: Epoch time: 461.13 s
2024-12-07 12:25:36.968563: Yayy! New best EMA pseudo Dice: 0.6547
2024-12-07 12:25:38.575016: 
2024-12-07 12:25:38.576410: Epoch 1
2024-12-07 12:25:38.577343: Current learning rate: 0.00999
2024-12-07 12:32:18.415854: Validation loss improved from -0.34642 to -0.42798! Patience: 0/50
2024-12-07 12:32:18.416558: train_loss -0.4619
2024-12-07 12:32:18.417271: val_loss -0.428
2024-12-07 12:32:18.417879: Pseudo dice [0.693]
2024-12-07 12:32:18.418541: Epoch time: 399.84 s
2024-12-07 12:32:18.419179: Yayy! New best EMA pseudo Dice: 0.6585
2024-12-07 12:32:20.185923: 
2024-12-07 12:32:20.187241: Epoch 2
2024-12-07 12:32:20.187912: Current learning rate: 0.00998
2024-12-07 12:38:37.576860: Validation loss did not improve from -0.42798. Patience: 1/50
2024-12-07 12:38:37.577756: train_loss -0.5031
2024-12-07 12:38:37.578598: val_loss -0.4244
2024-12-07 12:38:37.579238: Pseudo dice [0.6878]
2024-12-07 12:38:37.579941: Epoch time: 377.39 s
2024-12-07 12:38:37.580546: Yayy! New best EMA pseudo Dice: 0.6615
2024-12-07 12:38:39.406564: 
2024-12-07 12:38:39.407914: Epoch 3
2024-12-07 12:38:39.408735: Current learning rate: 0.00997
2024-12-07 12:45:02.194009: Validation loss improved from -0.42798 to -0.45744! Patience: 1/50
2024-12-07 12:45:02.195056: train_loss -0.53
2024-12-07 12:45:02.195865: val_loss -0.4574
2024-12-07 12:45:02.196560: Pseudo dice [0.7196]
2024-12-07 12:45:02.197356: Epoch time: 382.79 s
2024-12-07 12:45:02.198006: Yayy! New best EMA pseudo Dice: 0.6673
2024-12-07 12:45:03.974106: 
2024-12-07 12:45:03.975495: Epoch 4
2024-12-07 12:45:03.976282: Current learning rate: 0.00996
2024-12-07 12:51:27.864118: Validation loss did not improve from -0.45744. Patience: 1/50
2024-12-07 12:51:27.865105: train_loss -0.5493
2024-12-07 12:51:27.865935: val_loss -0.3894
2024-12-07 12:51:27.866776: Pseudo dice [0.6637]
2024-12-07 12:51:27.867587: Epoch time: 383.89 s
2024-12-07 12:51:29.697084: 
2024-12-07 12:51:29.698620: Epoch 5
2024-12-07 12:51:29.699729: Current learning rate: 0.00995
2024-12-07 12:57:38.436683: Validation loss did not improve from -0.45744. Patience: 2/50
2024-12-07 12:57:38.437678: train_loss -0.5562
2024-12-07 12:57:38.438659: val_loss -0.4157
2024-12-07 12:57:38.439434: Pseudo dice [0.6801]
2024-12-07 12:57:38.440261: Epoch time: 368.74 s
2024-12-07 12:57:38.441039: Yayy! New best EMA pseudo Dice: 0.6682
2024-12-07 12:57:40.187891: 
2024-12-07 12:57:40.189250: Epoch 6
2024-12-07 12:57:40.190165: Current learning rate: 0.00995
2024-12-07 13:04:05.704601: Validation loss did not improve from -0.45744. Patience: 3/50
2024-12-07 13:04:05.705645: train_loss -0.569
2024-12-07 13:04:05.706488: val_loss -0.328
2024-12-07 13:04:05.707274: Pseudo dice [0.6207]
2024-12-07 13:04:05.707991: Epoch time: 385.52 s
2024-12-07 13:04:07.065619: 
2024-12-07 13:04:07.066924: Epoch 7
2024-12-07 13:04:07.067770: Current learning rate: 0.00994
2024-12-07 13:10:16.015308: Validation loss improved from -0.45744 to -0.49224! Patience: 3/50
2024-12-07 13:10:16.016206: train_loss -0.5872
2024-12-07 13:10:16.017042: val_loss -0.4922
2024-12-07 13:10:16.017877: Pseudo dice [0.7367]
2024-12-07 13:10:16.018596: Epoch time: 368.95 s
2024-12-07 13:10:16.019263: Yayy! New best EMA pseudo Dice: 0.6708
2024-12-07 13:10:18.305951: 
2024-12-07 13:10:18.307244: Epoch 8
2024-12-07 13:10:18.308063: Current learning rate: 0.00993
2024-12-07 13:16:30.184069: Validation loss did not improve from -0.49224. Patience: 1/50
2024-12-07 13:16:30.185095: train_loss -0.5766
2024-12-07 13:16:30.185878: val_loss -0.391
2024-12-07 13:16:30.186621: Pseudo dice [0.6686]
2024-12-07 13:16:30.187300: Epoch time: 371.88 s
2024-12-07 13:16:31.610975: 
2024-12-07 13:16:31.612121: Epoch 9
2024-12-07 13:16:31.612822: Current learning rate: 0.00992
2024-12-07 13:22:52.634106: Validation loss did not improve from -0.49224. Patience: 2/50
2024-12-07 13:22:52.635309: train_loss -0.5844
2024-12-07 13:22:52.637277: val_loss -0.3625
2024-12-07 13:22:52.638260: Pseudo dice [0.6262]
2024-12-07 13:22:52.639567: Epoch time: 381.03 s
2024-12-07 13:22:54.404469: 
2024-12-07 13:22:54.408300: Epoch 10
2024-12-07 13:22:54.409215: Current learning rate: 0.00991
2024-12-07 13:29:08.537583: Validation loss did not improve from -0.49224. Patience: 3/50
2024-12-07 13:29:08.538568: train_loss -0.5908
2024-12-07 13:29:08.539827: val_loss -0.4572
2024-12-07 13:29:08.540803: Pseudo dice [0.6987]
2024-12-07 13:29:08.541801: Epoch time: 374.14 s
2024-12-07 13:29:09.898054: 
2024-12-07 13:29:09.899549: Epoch 11
2024-12-07 13:29:09.900612: Current learning rate: 0.0099
2024-12-07 13:35:26.531049: Validation loss did not improve from -0.49224. Patience: 4/50
2024-12-07 13:35:26.532154: train_loss -0.6108
2024-12-07 13:35:26.532948: val_loss -0.4064
2024-12-07 13:35:26.533730: Pseudo dice [0.6736]
2024-12-07 13:35:26.534424: Epoch time: 376.64 s
2024-12-07 13:35:27.868237: 
2024-12-07 13:35:27.869686: Epoch 12
2024-12-07 13:35:27.870548: Current learning rate: 0.00989
2024-12-07 13:41:40.990923: Validation loss did not improve from -0.49224. Patience: 5/50
2024-12-07 13:41:40.991950: train_loss -0.6051
2024-12-07 13:41:40.992796: val_loss -0.488
2024-12-07 13:41:40.993535: Pseudo dice [0.7302]
2024-12-07 13:41:40.994310: Epoch time: 373.12 s
2024-12-07 13:41:40.995076: Yayy! New best EMA pseudo Dice: 0.6759
2024-12-07 13:41:42.776992: 
2024-12-07 13:41:42.778284: Epoch 13
2024-12-07 13:41:42.779066: Current learning rate: 0.00988
2024-12-07 13:48:03.261376: Validation loss did not improve from -0.49224. Patience: 6/50
2024-12-07 13:48:03.262425: train_loss -0.6165
2024-12-07 13:48:03.263239: val_loss -0.4761
2024-12-07 13:48:03.264014: Pseudo dice [0.7173]
2024-12-07 13:48:03.264615: Epoch time: 380.49 s
2024-12-07 13:48:03.265335: Yayy! New best EMA pseudo Dice: 0.68
2024-12-07 13:48:05.025212: 
2024-12-07 13:48:05.026724: Epoch 14
2024-12-07 13:48:05.027531: Current learning rate: 0.00987
2024-12-07 13:54:35.576851: Validation loss improved from -0.49224 to -0.50174! Patience: 6/50
2024-12-07 13:54:35.577894: train_loss -0.6239
2024-12-07 13:54:35.578619: val_loss -0.5017
2024-12-07 13:54:35.579281: Pseudo dice [0.7443]
2024-12-07 13:54:35.579941: Epoch time: 390.55 s
2024-12-07 13:54:35.985580: Yayy! New best EMA pseudo Dice: 0.6864
2024-12-07 13:54:37.785338: 
2024-12-07 13:54:37.786783: Epoch 15
2024-12-07 13:54:37.787721: Current learning rate: 0.00986
2024-12-07 14:00:59.868145: Validation loss did not improve from -0.50174. Patience: 1/50
2024-12-07 14:00:59.869219: train_loss -0.6215
2024-12-07 14:00:59.870057: val_loss -0.3419
2024-12-07 14:00:59.870733: Pseudo dice [0.6385]
2024-12-07 14:00:59.871532: Epoch time: 382.09 s
2024-12-07 14:01:01.267214: 
2024-12-07 14:01:01.268815: Epoch 16
2024-12-07 14:01:01.269827: Current learning rate: 0.00986
2024-12-07 14:07:14.149545: Validation loss did not improve from -0.50174. Patience: 2/50
2024-12-07 14:07:14.150713: train_loss -0.6308
2024-12-07 14:07:14.151506: val_loss -0.4013
2024-12-07 14:07:14.152610: Pseudo dice [0.6697]
2024-12-07 14:07:14.153871: Epoch time: 372.88 s
2024-12-07 14:07:15.602269: 
2024-12-07 14:07:15.603794: Epoch 17
2024-12-07 14:07:15.604796: Current learning rate: 0.00985
2024-12-07 14:13:34.054950: Validation loss did not improve from -0.50174. Patience: 3/50
2024-12-07 14:13:34.056046: train_loss -0.6354
2024-12-07 14:13:34.056840: val_loss -0.4447
2024-12-07 14:13:34.057528: Pseudo dice [0.7029]
2024-12-07 14:13:34.058217: Epoch time: 378.45 s
2024-12-07 14:13:35.497045: 
2024-12-07 14:13:35.498263: Epoch 18
2024-12-07 14:13:35.499120: Current learning rate: 0.00984
2024-12-07 14:19:58.019683: Validation loss did not improve from -0.50174. Patience: 4/50
2024-12-07 14:19:58.020631: train_loss -0.6431
2024-12-07 14:19:58.021636: val_loss -0.4069
2024-12-07 14:19:58.022774: Pseudo dice [0.6673]
2024-12-07 14:19:58.023710: Epoch time: 382.52 s
2024-12-07 14:19:59.944025: 
2024-12-07 14:19:59.945357: Epoch 19
2024-12-07 14:19:59.946408: Current learning rate: 0.00983
2024-12-07 14:27:02.450526: Validation loss did not improve from -0.50174. Patience: 5/50
2024-12-07 14:27:02.451536: train_loss -0.6379
2024-12-07 14:27:02.452783: val_loss -0.5003
2024-12-07 14:27:02.453920: Pseudo dice [0.7335]
2024-12-07 14:27:02.455040: Epoch time: 422.51 s
2024-12-07 14:27:04.427900: 
2024-12-07 14:27:04.429222: Epoch 20
2024-12-07 14:27:04.429959: Current learning rate: 0.00982
2024-12-07 14:34:00.053801: Validation loss did not improve from -0.50174. Patience: 6/50
2024-12-07 14:34:00.055813: train_loss -0.6358
2024-12-07 14:34:00.056792: val_loss -0.4999
2024-12-07 14:34:00.057921: Pseudo dice [0.7243]
2024-12-07 14:34:00.059371: Epoch time: 415.63 s
2024-12-07 14:34:00.060363: Yayy! New best EMA pseudo Dice: 0.6902
2024-12-07 14:34:01.971530: 
2024-12-07 14:34:01.972785: Epoch 21
2024-12-07 14:34:01.973594: Current learning rate: 0.00981
2024-12-07 14:40:30.871027: Validation loss did not improve from -0.50174. Patience: 7/50
2024-12-07 14:40:30.872143: train_loss -0.6446
2024-12-07 14:40:30.873013: val_loss -0.4826
2024-12-07 14:40:30.873664: Pseudo dice [0.7212]
2024-12-07 14:40:30.874347: Epoch time: 388.9 s
2024-12-07 14:40:30.875063: Yayy! New best EMA pseudo Dice: 0.6933
2024-12-07 14:40:32.655893: 
2024-12-07 14:40:32.657295: Epoch 22
2024-12-07 14:40:32.658083: Current learning rate: 0.0098
2024-12-07 14:46:54.349175: Validation loss did not improve from -0.50174. Patience: 8/50
2024-12-07 14:46:54.350212: train_loss -0.654
2024-12-07 14:46:54.350983: val_loss -0.4682
2024-12-07 14:46:54.351717: Pseudo dice [0.7092]
2024-12-07 14:46:54.352398: Epoch time: 381.7 s
2024-12-07 14:46:54.353137: Yayy! New best EMA pseudo Dice: 0.6949
2024-12-07 14:46:56.109758: 
2024-12-07 14:46:56.110937: Epoch 23
2024-12-07 14:46:56.111611: Current learning rate: 0.00979
2024-12-07 14:54:09.923912: Validation loss did not improve from -0.50174. Patience: 9/50
2024-12-07 14:54:09.924853: train_loss -0.6438
2024-12-07 14:54:09.925581: val_loss -0.4643
2024-12-07 14:54:09.926323: Pseudo dice [0.7045]
2024-12-07 14:54:09.927080: Epoch time: 433.82 s
2024-12-07 14:54:09.928200: Yayy! New best EMA pseudo Dice: 0.6958
2024-12-07 14:54:11.650098: 
2024-12-07 14:54:11.651767: Epoch 24
2024-12-07 14:54:11.652769: Current learning rate: 0.00978
2024-12-07 15:00:36.133566: Validation loss did not improve from -0.50174. Patience: 10/50
2024-12-07 15:00:36.134643: train_loss -0.6469
2024-12-07 15:00:36.135479: val_loss -0.4575
2024-12-07 15:00:36.136278: Pseudo dice [0.7088]
2024-12-07 15:00:36.137136: Epoch time: 384.49 s
2024-12-07 15:00:36.537178: Yayy! New best EMA pseudo Dice: 0.6971
2024-12-07 15:00:38.285478: 
2024-12-07 15:00:38.287242: Epoch 25
2024-12-07 15:00:38.288483: Current learning rate: 0.00977
2024-12-07 15:07:48.000639: Validation loss did not improve from -0.50174. Patience: 11/50
2024-12-07 15:07:48.001510: train_loss -0.6446
2024-12-07 15:07:48.002345: val_loss -0.3979
2024-12-07 15:07:48.002991: Pseudo dice [0.6732]
2024-12-07 15:07:48.003721: Epoch time: 429.72 s
2024-12-07 15:07:49.337276: 
2024-12-07 15:07:49.338621: Epoch 26
2024-12-07 15:07:49.339408: Current learning rate: 0.00977
2024-12-07 15:14:40.407212: Validation loss did not improve from -0.50174. Patience: 12/50
2024-12-07 15:14:40.408289: train_loss -0.6543
2024-12-07 15:14:40.409229: val_loss -0.4012
2024-12-07 15:14:40.410098: Pseudo dice [0.6825]
2024-12-07 15:14:40.410747: Epoch time: 411.07 s
2024-12-07 15:14:41.757080: 
2024-12-07 15:14:41.758479: Epoch 27
2024-12-07 15:14:41.759351: Current learning rate: 0.00976
2024-12-07 15:20:48.321349: Validation loss did not improve from -0.50174. Patience: 13/50
2024-12-07 15:20:48.322319: train_loss -0.6632
2024-12-07 15:20:48.323272: val_loss -0.3374
2024-12-07 15:20:48.323994: Pseudo dice [0.6454]
2024-12-07 15:20:48.324659: Epoch time: 366.57 s
2024-12-07 15:20:49.681934: 
2024-12-07 15:20:49.683134: Epoch 28
2024-12-07 15:20:49.683798: Current learning rate: 0.00975
2024-12-07 15:27:54.569574: Validation loss did not improve from -0.50174. Patience: 14/50
2024-12-07 15:27:54.570644: train_loss -0.6614
2024-12-07 15:27:54.571483: val_loss -0.312
2024-12-07 15:27:54.572267: Pseudo dice [0.6232]
2024-12-07 15:27:54.573110: Epoch time: 424.89 s
2024-12-07 15:27:55.957155: 
2024-12-07 15:27:55.958555: Epoch 29
2024-12-07 15:27:55.959440: Current learning rate: 0.00974
2024-12-07 15:35:17.162704: Validation loss did not improve from -0.50174. Patience: 15/50
2024-12-07 15:35:17.164823: train_loss -0.6598
2024-12-07 15:35:17.165877: val_loss -0.4769
2024-12-07 15:35:17.166672: Pseudo dice [0.7062]
2024-12-07 15:35:17.167427: Epoch time: 441.21 s
2024-12-07 15:35:19.312361: 
2024-12-07 15:35:19.313697: Epoch 30
2024-12-07 15:35:19.314542: Current learning rate: 0.00973
2024-12-07 15:41:58.006326: Validation loss did not improve from -0.50174. Patience: 16/50
2024-12-07 15:41:58.007660: train_loss -0.6719
2024-12-07 15:41:58.008734: val_loss -0.4657
2024-12-07 15:41:58.009679: Pseudo dice [0.7085]
2024-12-07 15:41:58.010639: Epoch time: 398.7 s
2024-12-07 15:41:59.382768: 
2024-12-07 15:41:59.384246: Epoch 31
2024-12-07 15:41:59.385411: Current learning rate: 0.00972
2024-12-07 15:48:38.432025: Validation loss did not improve from -0.50174. Patience: 17/50
2024-12-07 15:48:38.433432: train_loss -0.6664
2024-12-07 15:48:38.434326: val_loss -0.4409
2024-12-07 15:48:38.435127: Pseudo dice [0.6989]
2024-12-07 15:48:38.435836: Epoch time: 399.05 s
2024-12-07 15:48:39.820089: 
2024-12-07 15:48:39.821351: Epoch 32
2024-12-07 15:48:39.822051: Current learning rate: 0.00971
2024-12-07 15:55:39.941657: Validation loss did not improve from -0.50174. Patience: 18/50
2024-12-07 15:55:39.942726: train_loss -0.6721
2024-12-07 15:55:39.943514: val_loss -0.3482
2024-12-07 15:55:39.944177: Pseudo dice [0.659]
2024-12-07 15:55:39.944825: Epoch time: 420.12 s
2024-12-07 15:55:41.329903: 
2024-12-07 15:55:41.331248: Epoch 33
2024-12-07 15:55:41.331916: Current learning rate: 0.0097
2024-12-07 16:02:43.935660: Validation loss did not improve from -0.50174. Patience: 19/50
2024-12-07 16:02:43.936693: train_loss -0.6679
2024-12-07 16:02:43.937537: val_loss -0.4111
2024-12-07 16:02:43.938242: Pseudo dice [0.6981]
2024-12-07 16:02:43.938967: Epoch time: 422.61 s
2024-12-07 16:02:45.314671: 
2024-12-07 16:02:45.316049: Epoch 34
2024-12-07 16:02:45.316932: Current learning rate: 0.00969
2024-12-07 16:09:36.781441: Validation loss did not improve from -0.50174. Patience: 20/50
2024-12-07 16:09:36.782529: train_loss -0.6689
2024-12-07 16:09:36.783328: val_loss -0.4453
2024-12-07 16:09:36.783933: Pseudo dice [0.7113]
2024-12-07 16:09:36.784631: Epoch time: 411.47 s
2024-12-07 16:09:38.566009: 
2024-12-07 16:09:38.567304: Epoch 35
2024-12-07 16:09:38.568176: Current learning rate: 0.00968
2024-12-07 16:16:34.508404: Validation loss did not improve from -0.50174. Patience: 21/50
2024-12-07 16:16:34.509605: train_loss -0.6715
2024-12-07 16:16:34.510792: val_loss -0.4516
2024-12-07 16:16:34.511654: Pseudo dice [0.7018]
2024-12-07 16:16:34.512597: Epoch time: 415.94 s
2024-12-07 16:16:35.910833: 
2024-12-07 16:16:35.912256: Epoch 36
2024-12-07 16:16:35.913384: Current learning rate: 0.00968
2024-12-07 16:22:43.956796: Validation loss did not improve from -0.50174. Patience: 22/50
2024-12-07 16:22:43.957906: train_loss -0.6766
2024-12-07 16:22:43.958719: val_loss -0.4473
2024-12-07 16:22:43.959515: Pseudo dice [0.703]
2024-12-07 16:22:43.960209: Epoch time: 368.05 s
2024-12-07 16:22:45.406390: 
2024-12-07 16:22:45.407507: Epoch 37
2024-12-07 16:22:45.408272: Current learning rate: 0.00967
2024-12-07 16:30:06.752984: Validation loss improved from -0.50174 to -0.50286! Patience: 22/50
2024-12-07 16:30:06.753832: train_loss -0.6793
2024-12-07 16:30:06.754675: val_loss -0.5029
2024-12-07 16:30:06.755511: Pseudo dice [0.7313]
2024-12-07 16:30:06.756242: Epoch time: 441.35 s
2024-12-07 16:30:08.164194: 
2024-12-07 16:30:08.165437: Epoch 38
2024-12-07 16:30:08.166250: Current learning rate: 0.00966
2024-12-07 16:37:12.641203: Validation loss improved from -0.50286 to -0.50539! Patience: 0/50
2024-12-07 16:37:12.642141: train_loss -0.675
2024-12-07 16:37:12.642877: val_loss -0.5054
2024-12-07 16:37:12.643862: Pseudo dice [0.7457]
2024-12-07 16:37:12.644861: Epoch time: 424.48 s
2024-12-07 16:37:12.645808: Yayy! New best EMA pseudo Dice: 0.7005
2024-12-07 16:37:14.468601: 
2024-12-07 16:37:14.469821: Epoch 39
2024-12-07 16:37:14.471318: Current learning rate: 0.00965
2024-12-07 16:43:30.224547: Validation loss did not improve from -0.50539. Patience: 1/50
2024-12-07 16:43:30.227320: train_loss -0.6676
2024-12-07 16:43:30.228166: val_loss -0.4611
2024-12-07 16:43:30.228845: Pseudo dice [0.7155]
2024-12-07 16:43:30.229844: Epoch time: 375.76 s
2024-12-07 16:43:30.642043: Yayy! New best EMA pseudo Dice: 0.702
2024-12-07 16:43:32.494213: 
2024-12-07 16:43:32.495645: Epoch 40
2024-12-07 16:43:32.496629: Current learning rate: 0.00964
2024-12-07 16:50:51.482864: Validation loss did not improve from -0.50539. Patience: 2/50
2024-12-07 16:50:51.483982: train_loss -0.6781
2024-12-07 16:50:51.484981: val_loss -0.4693
2024-12-07 16:50:51.485712: Pseudo dice [0.712]
2024-12-07 16:50:51.486466: Epoch time: 438.99 s
2024-12-07 16:50:51.487125: Yayy! New best EMA pseudo Dice: 0.703
2024-12-07 16:50:54.554027: 
2024-12-07 16:50:54.555337: Epoch 41
2024-12-07 16:50:54.556002: Current learning rate: 0.00963
2024-12-07 16:57:56.946581: Validation loss did not improve from -0.50539. Patience: 3/50
2024-12-07 16:57:56.947669: train_loss -0.6807
2024-12-07 16:57:56.948655: val_loss -0.4211
2024-12-07 16:57:56.949679: Pseudo dice [0.6923]
2024-12-07 16:57:56.950609: Epoch time: 422.39 s
2024-12-07 16:57:58.287329: 
2024-12-07 16:57:58.288763: Epoch 42
2024-12-07 16:57:58.290015: Current learning rate: 0.00962
2024-12-07 17:04:50.949641: Validation loss did not improve from -0.50539. Patience: 4/50
2024-12-07 17:04:50.950604: train_loss -0.6867
2024-12-07 17:04:50.951336: val_loss -0.5025
2024-12-07 17:04:50.952007: Pseudo dice [0.7336]
2024-12-07 17:04:50.952673: Epoch time: 412.66 s
2024-12-07 17:04:50.953352: Yayy! New best EMA pseudo Dice: 0.7051
2024-12-07 17:04:52.673382: 
2024-12-07 17:04:52.674809: Epoch 43
2024-12-07 17:04:52.675498: Current learning rate: 0.00961
2024-12-07 17:11:13.612688: Validation loss did not improve from -0.50539. Patience: 5/50
2024-12-07 17:11:13.613723: train_loss -0.6916
2024-12-07 17:11:13.614624: val_loss -0.4953
2024-12-07 17:11:13.615457: Pseudo dice [0.7382]
2024-12-07 17:11:13.616261: Epoch time: 380.94 s
2024-12-07 17:11:13.617090: Yayy! New best EMA pseudo Dice: 0.7084
2024-12-07 17:11:15.407876: 
2024-12-07 17:11:15.409279: Epoch 44
2024-12-07 17:11:15.409988: Current learning rate: 0.0096
2024-12-07 17:18:26.342869: Validation loss did not improve from -0.50539. Patience: 6/50
2024-12-07 17:18:26.343632: train_loss -0.6862
2024-12-07 17:18:26.344656: val_loss -0.3476
2024-12-07 17:18:26.345459: Pseudo dice [0.6364]
2024-12-07 17:18:26.346147: Epoch time: 430.94 s
2024-12-07 17:18:28.082859: 
2024-12-07 17:18:28.084107: Epoch 45
2024-12-07 17:18:28.085069: Current learning rate: 0.00959
2024-12-07 17:25:05.139694: Validation loss did not improve from -0.50539. Patience: 7/50
2024-12-07 17:25:05.141325: train_loss -0.6913
2024-12-07 17:25:05.142622: val_loss -0.4436
2024-12-07 17:25:05.143430: Pseudo dice [0.7109]
2024-12-07 17:25:05.144155: Epoch time: 397.06 s
2024-12-07 17:25:06.520321: 
2024-12-07 17:25:06.521154: Epoch 46
2024-12-07 17:25:06.521875: Current learning rate: 0.00959
2024-12-07 17:32:02.724851: Validation loss did not improve from -0.50539. Patience: 8/50
2024-12-07 17:32:02.725791: train_loss -0.694
2024-12-07 17:32:02.726604: val_loss -0.413
2024-12-07 17:32:02.727333: Pseudo dice [0.6836]
2024-12-07 17:32:02.727960: Epoch time: 416.21 s
2024-12-07 17:32:04.090798: 
2024-12-07 17:32:04.091688: Epoch 47
2024-12-07 17:32:04.092642: Current learning rate: 0.00958
2024-12-07 17:38:37.426857: Validation loss did not improve from -0.50539. Patience: 9/50
2024-12-07 17:38:37.427599: train_loss -0.6895
2024-12-07 17:38:37.428388: val_loss -0.3645
2024-12-07 17:38:37.429071: Pseudo dice [0.6369]
2024-12-07 17:38:37.429903: Epoch time: 393.34 s
2024-12-07 17:38:38.797117: 
2024-12-07 17:38:38.798598: Epoch 48
2024-12-07 17:38:38.799646: Current learning rate: 0.00957
2024-12-07 17:45:52.963299: Validation loss did not improve from -0.50539. Patience: 10/50
2024-12-07 17:45:52.966899: train_loss -0.6966
2024-12-07 17:45:52.968315: val_loss -0.4404
2024-12-07 17:45:52.968906: Pseudo dice [0.7069]
2024-12-07 17:45:52.969858: Epoch time: 434.17 s
2024-12-07 17:45:54.349655: 
2024-12-07 17:45:54.350736: Epoch 49
2024-12-07 17:45:54.351444: Current learning rate: 0.00956
2024-12-07 17:52:26.132804: Validation loss did not improve from -0.50539. Patience: 11/50
2024-12-07 17:52:26.134141: train_loss -0.6968
2024-12-07 17:52:26.134964: val_loss -0.4247
2024-12-07 17:52:26.135861: Pseudo dice [0.6919]
2024-12-07 17:52:26.136586: Epoch time: 391.79 s
2024-12-07 17:52:27.903595: 
2024-12-07 17:52:27.905109: Epoch 50
2024-12-07 17:52:27.906080: Current learning rate: 0.00955
2024-12-07 17:59:35.394420: Validation loss did not improve from -0.50539. Patience: 12/50
2024-12-07 17:59:35.395445: train_loss -0.695
2024-12-07 17:59:35.396408: val_loss -0.3417
2024-12-07 17:59:35.397102: Pseudo dice [0.6483]
2024-12-07 17:59:35.397892: Epoch time: 427.49 s
2024-12-07 17:59:36.817291: 
2024-12-07 17:59:36.818670: Epoch 51
2024-12-07 17:59:36.819453: Current learning rate: 0.00954
2024-12-07 18:06:08.955485: Validation loss did not improve from -0.50539. Patience: 13/50
2024-12-07 18:06:08.956480: train_loss -0.6951
2024-12-07 18:06:08.957701: val_loss -0.4819
2024-12-07 18:06:08.958982: Pseudo dice [0.7308]
2024-12-07 18:06:08.960051: Epoch time: 392.14 s
2024-12-07 18:06:11.929173: 
2024-12-07 18:06:11.930548: Epoch 52
2024-12-07 18:06:11.931554: Current learning rate: 0.00953
2024-12-07 18:13:13.485712: Validation loss did not improve from -0.50539. Patience: 14/50
2024-12-07 18:13:13.486834: train_loss -0.6925
2024-12-07 18:13:13.487714: val_loss -0.4857
2024-12-07 18:13:13.488517: Pseudo dice [0.7265]
2024-12-07 18:13:13.489272: Epoch time: 421.56 s
2024-12-07 18:13:14.859619: 
2024-12-07 18:13:14.860833: Epoch 53
2024-12-07 18:13:14.861679: Current learning rate: 0.00952
2024-12-07 18:20:04.410754: Validation loss did not improve from -0.50539. Patience: 15/50
2024-12-07 18:20:04.411638: train_loss -0.698
2024-12-07 18:20:04.412450: val_loss -0.3951
2024-12-07 18:20:04.413078: Pseudo dice [0.6695]
2024-12-07 18:20:04.413892: Epoch time: 409.55 s
2024-12-07 18:20:05.820172: 
2024-12-07 18:20:05.821563: Epoch 54
2024-12-07 18:20:05.822237: Current learning rate: 0.00951
2024-12-07 18:27:00.990627: Validation loss did not improve from -0.50539. Patience: 16/50
2024-12-07 18:27:00.991780: train_loss -0.7022
2024-12-07 18:27:00.992894: val_loss -0.4709
2024-12-07 18:27:00.993578: Pseudo dice [0.7252]
2024-12-07 18:27:00.994258: Epoch time: 415.17 s
2024-12-07 18:27:02.766175: 
2024-12-07 18:27:02.767492: Epoch 55
2024-12-07 18:27:02.768343: Current learning rate: 0.0095
2024-12-07 18:33:47.068163: Validation loss did not improve from -0.50539. Patience: 17/50
2024-12-07 18:33:47.069307: train_loss -0.7045
2024-12-07 18:33:47.070147: val_loss -0.4982
2024-12-07 18:33:47.070938: Pseudo dice [0.7373]
2024-12-07 18:33:47.071703: Epoch time: 404.3 s
2024-12-07 18:33:48.484490: 
2024-12-07 18:33:48.485538: Epoch 56
2024-12-07 18:33:48.486273: Current learning rate: 0.00949
2024-12-07 18:40:39.110622: Validation loss did not improve from -0.50539. Patience: 18/50
2024-12-07 18:40:39.111565: train_loss -0.7059
2024-12-07 18:40:39.112477: val_loss -0.4135
2024-12-07 18:40:39.113295: Pseudo dice [0.6893]
2024-12-07 18:40:39.114301: Epoch time: 410.63 s
2024-12-07 18:40:40.526021: 
2024-12-07 18:40:40.527341: Epoch 57
2024-12-07 18:40:40.528190: Current learning rate: 0.00949
2024-12-07 18:47:30.787534: Validation loss did not improve from -0.50539. Patience: 19/50
2024-12-07 18:47:30.789649: train_loss -0.709
2024-12-07 18:47:30.791380: val_loss -0.445
2024-12-07 18:47:30.792287: Pseudo dice [0.7073]
2024-12-07 18:47:30.793138: Epoch time: 410.26 s
2024-12-07 18:47:32.184409: 
2024-12-07 18:47:32.185806: Epoch 58
2024-12-07 18:47:32.186561: Current learning rate: 0.00948
2024-12-07 18:54:13.065675: Validation loss improved from -0.50539 to -0.51067! Patience: 19/50
2024-12-07 18:54:13.066855: train_loss -0.7029
2024-12-07 18:54:13.068141: val_loss -0.5107
2024-12-07 18:54:13.069192: Pseudo dice [0.7434]
2024-12-07 18:54:13.070256: Epoch time: 400.88 s
2024-12-07 18:54:14.479037: 
2024-12-07 18:54:14.480521: Epoch 59
2024-12-07 18:54:14.481553: Current learning rate: 0.00947
2024-12-07 19:00:57.255012: Validation loss did not improve from -0.51067. Patience: 1/50
2024-12-07 19:00:57.256236: train_loss -0.6998
2024-12-07 19:00:57.257034: val_loss -0.449
2024-12-07 19:00:57.257787: Pseudo dice [0.7016]
2024-12-07 19:00:57.258559: Epoch time: 402.78 s
2024-12-07 19:00:59.078568: 
2024-12-07 19:00:59.079992: Epoch 60
2024-12-07 19:00:59.080839: Current learning rate: 0.00946
2024-12-07 19:07:54.410190: Validation loss did not improve from -0.51067. Patience: 2/50
2024-12-07 19:07:54.411739: train_loss -0.7025
2024-12-07 19:07:54.412779: val_loss -0.4867
2024-12-07 19:07:54.413518: Pseudo dice [0.7297]
2024-12-07 19:07:54.414195: Epoch time: 415.33 s
2024-12-07 19:07:55.849128: 
2024-12-07 19:07:55.850645: Epoch 61
2024-12-07 19:07:55.851373: Current learning rate: 0.00945
2024-12-07 19:15:09.349385: Validation loss did not improve from -0.51067. Patience: 3/50
2024-12-07 19:15:09.350396: train_loss -0.7081
2024-12-07 19:15:09.351284: val_loss -0.4555
2024-12-07 19:15:09.352218: Pseudo dice [0.7054]
2024-12-07 19:15:09.353186: Epoch time: 433.5 s
2024-12-07 19:15:10.754556: 
2024-12-07 19:15:10.756106: Epoch 62
2024-12-07 19:15:10.757168: Current learning rate: 0.00944
2024-12-07 19:21:46.854109: Validation loss did not improve from -0.51067. Patience: 4/50
2024-12-07 19:21:46.855054: train_loss -0.7069
2024-12-07 19:21:46.855861: val_loss -0.4254
2024-12-07 19:21:46.856621: Pseudo dice [0.6918]
2024-12-07 19:21:46.857333: Epoch time: 396.1 s
2024-12-07 19:21:49.030150: 
2024-12-07 19:21:49.031299: Epoch 63
2024-12-07 19:21:49.032200: Current learning rate: 0.00943
2024-12-07 19:29:05.475599: Validation loss did not improve from -0.51067. Patience: 5/50
2024-12-07 19:29:05.476632: train_loss -0.7075
2024-12-07 19:29:05.477421: val_loss -0.3541
2024-12-07 19:29:05.478174: Pseudo dice [0.6512]
2024-12-07 19:29:05.478889: Epoch time: 436.45 s
2024-12-07 19:29:06.904341: 
2024-12-07 19:29:06.905296: Epoch 64
2024-12-07 19:29:06.906089: Current learning rate: 0.00942
2024-12-07 19:35:59.439928: Validation loss did not improve from -0.51067. Patience: 6/50
2024-12-07 19:35:59.441194: train_loss -0.7141
2024-12-07 19:35:59.442344: val_loss -0.3815
2024-12-07 19:35:59.443126: Pseudo dice [0.6701]
2024-12-07 19:35:59.443796: Epoch time: 412.54 s
2024-12-07 19:36:01.285565: 
2024-12-07 19:36:01.286898: Epoch 65
2024-12-07 19:36:01.287695: Current learning rate: 0.00941
2024-12-07 19:42:30.542832: Validation loss did not improve from -0.51067. Patience: 7/50
2024-12-07 19:42:30.543847: train_loss -0.7124
2024-12-07 19:42:30.544852: val_loss -0.4712
2024-12-07 19:42:30.545863: Pseudo dice [0.719]
2024-12-07 19:42:30.546882: Epoch time: 389.26 s
2024-12-07 19:42:31.965881: 
2024-12-07 19:42:31.967279: Epoch 66
2024-12-07 19:42:31.968266: Current learning rate: 0.0094
2024-12-07 19:49:24.993762: Validation loss did not improve from -0.51067. Patience: 8/50
2024-12-07 19:49:24.994556: train_loss -0.7197
2024-12-07 19:49:24.995276: val_loss -0.457
2024-12-07 19:49:24.995975: Pseudo dice [0.7082]
2024-12-07 19:49:24.996683: Epoch time: 413.03 s
2024-12-07 19:49:26.416218: 
2024-12-07 19:49:26.417506: Epoch 67
2024-12-07 19:49:26.418359: Current learning rate: 0.00939
2024-12-07 19:56:20.065818: Validation loss did not improve from -0.51067. Patience: 9/50
2024-12-07 19:56:20.086301: train_loss -0.713
2024-12-07 19:56:20.088320: val_loss -0.4194
2024-12-07 19:56:20.089168: Pseudo dice [0.6862]
2024-12-07 19:56:20.090206: Epoch time: 413.67 s
2024-12-07 19:56:21.556227: 
2024-12-07 19:56:21.557396: Epoch 68
2024-12-07 19:56:21.558136: Current learning rate: 0.00939
2024-12-07 20:02:48.697782: Validation loss did not improve from -0.51067. Patience: 10/50
2024-12-07 20:02:48.699193: train_loss -0.7111
2024-12-07 20:02:48.699986: val_loss -0.3644
2024-12-07 20:02:48.700731: Pseudo dice [0.667]
2024-12-07 20:02:48.701513: Epoch time: 387.14 s
2024-12-07 20:02:50.151794: 
2024-12-07 20:02:50.153172: Epoch 69
2024-12-07 20:02:50.153995: Current learning rate: 0.00938
2024-12-07 20:09:57.899308: Validation loss did not improve from -0.51067. Patience: 11/50
2024-12-07 20:09:57.900872: train_loss -0.7089
2024-12-07 20:09:57.901898: val_loss -0.4666
2024-12-07 20:09:57.902935: Pseudo dice [0.73]
2024-12-07 20:09:57.903880: Epoch time: 427.75 s
2024-12-07 20:09:59.853172: 
2024-12-07 20:09:59.854462: Epoch 70
2024-12-07 20:09:59.855422: Current learning rate: 0.00937
2024-12-07 20:17:00.029449: Validation loss did not improve from -0.51067. Patience: 12/50
2024-12-07 20:17:00.030722: train_loss -0.7118
2024-12-07 20:17:00.031739: val_loss -0.406
2024-12-07 20:17:00.032654: Pseudo dice [0.6872]
2024-12-07 20:17:00.033669: Epoch time: 420.18 s
2024-12-07 20:17:01.470698: 
2024-12-07 20:17:01.472063: Epoch 71
2024-12-07 20:17:01.472996: Current learning rate: 0.00936
2024-12-07 20:24:25.431625: Validation loss did not improve from -0.51067. Patience: 13/50
2024-12-07 20:24:25.432809: train_loss -0.7156
2024-12-07 20:24:25.433723: val_loss -0.5062
2024-12-07 20:24:25.434571: Pseudo dice [0.7358]
2024-12-07 20:24:25.435312: Epoch time: 443.96 s
2024-12-07 20:24:26.890999: 
2024-12-07 20:24:26.892325: Epoch 72
2024-12-07 20:24:26.893142: Current learning rate: 0.00935
2024-12-07 20:30:59.787599: Validation loss did not improve from -0.51067. Patience: 14/50
2024-12-07 20:30:59.788681: train_loss -0.7126
2024-12-07 20:30:59.789720: val_loss -0.4235
2024-12-07 20:30:59.790690: Pseudo dice [0.7014]
2024-12-07 20:30:59.791681: Epoch time: 392.9 s
2024-12-07 20:31:01.594421: 
2024-12-07 20:31:01.595611: Epoch 73
2024-12-07 20:31:01.596410: Current learning rate: 0.00934
2024-12-07 20:37:27.393746: Validation loss did not improve from -0.51067. Patience: 15/50
2024-12-07 20:37:27.395405: train_loss -0.7098
2024-12-07 20:37:27.396639: val_loss -0.2034
2024-12-07 20:37:27.397363: Pseudo dice [0.5616]
2024-12-07 20:37:27.398046: Epoch time: 385.8 s
2024-12-07 20:37:28.826623: 
2024-12-07 20:37:28.828048: Epoch 74
2024-12-07 20:37:28.828876: Current learning rate: 0.00933
2024-12-07 20:44:41.662428: Validation loss did not improve from -0.51067. Patience: 16/50
2024-12-07 20:44:41.663101: train_loss -0.7089
2024-12-07 20:44:41.663901: val_loss -0.3545
2024-12-07 20:44:41.664757: Pseudo dice [0.6579]
2024-12-07 20:44:41.665669: Epoch time: 432.84 s
2024-12-07 20:44:43.513165: 
2024-12-07 20:44:43.514267: Epoch 75
2024-12-07 20:44:43.514965: Current learning rate: 0.00932
2024-12-07 20:51:23.192953: Validation loss did not improve from -0.51067. Patience: 17/50
2024-12-07 20:51:23.194006: train_loss -0.7099
2024-12-07 20:51:23.194929: val_loss -0.4447
2024-12-07 20:51:23.195728: Pseudo dice [0.7232]
2024-12-07 20:51:23.196436: Epoch time: 399.68 s
2024-12-07 20:51:24.732249: 
2024-12-07 20:51:24.733637: Epoch 76
2024-12-07 20:51:24.734462: Current learning rate: 0.00931
2024-12-07 20:58:01.166529: Validation loss did not improve from -0.51067. Patience: 18/50
2024-12-07 20:58:01.168494: train_loss -0.7148
2024-12-07 20:58:01.170261: val_loss -0.4396
2024-12-07 20:58:01.171180: Pseudo dice [0.7033]
2024-12-07 20:58:01.172145: Epoch time: 396.44 s
2024-12-07 20:58:02.651551: 
2024-12-07 20:58:02.652952: Epoch 77
2024-12-07 20:58:02.653689: Current learning rate: 0.0093
2024-12-07 21:05:11.562354: Validation loss did not improve from -0.51067. Patience: 19/50
2024-12-07 21:05:11.563796: train_loss -0.7252
2024-12-07 21:05:11.565172: val_loss -0.4562
2024-12-07 21:05:11.566284: Pseudo dice [0.715]
2024-12-07 21:05:11.567306: Epoch time: 428.91 s
2024-12-07 21:05:13.114229: 
2024-12-07 21:05:13.115791: Epoch 78
2024-12-07 21:05:13.116773: Current learning rate: 0.0093
2024-12-07 21:11:50.462727: Validation loss did not improve from -0.51067. Patience: 20/50
2024-12-07 21:11:50.463852: train_loss -0.724
2024-12-07 21:11:50.464568: val_loss -0.4643
2024-12-07 21:11:50.465282: Pseudo dice [0.7093]
2024-12-07 21:11:50.465914: Epoch time: 397.35 s
2024-12-07 21:11:51.927982: 
2024-12-07 21:11:51.929090: Epoch 79
2024-12-07 21:11:51.929820: Current learning rate: 0.00929
2024-12-07 21:18:58.118109: Validation loss did not improve from -0.51067. Patience: 21/50
2024-12-07 21:18:58.119261: train_loss -0.7233
2024-12-07 21:18:58.120121: val_loss -0.4341
2024-12-07 21:18:58.120797: Pseudo dice [0.6937]
2024-12-07 21:18:58.121590: Epoch time: 426.19 s
2024-12-07 21:18:59.984139: 
2024-12-07 21:18:59.985410: Epoch 80
2024-12-07 21:18:59.986146: Current learning rate: 0.00928
2024-12-07 21:25:39.881286: Validation loss did not improve from -0.51067. Patience: 22/50
2024-12-07 21:25:39.882376: train_loss -0.7227
2024-12-07 21:25:39.883340: val_loss -0.3406
2024-12-07 21:25:39.884140: Pseudo dice [0.6454]
2024-12-07 21:25:39.885041: Epoch time: 399.9 s
2024-12-07 21:25:41.324169: 
2024-12-07 21:25:41.325395: Epoch 81
2024-12-07 21:25:41.326358: Current learning rate: 0.00927
2024-12-07 21:32:17.810908: Validation loss did not improve from -0.51067. Patience: 23/50
2024-12-07 21:32:17.811774: train_loss -0.7164
2024-12-07 21:32:17.812547: val_loss -0.4549
2024-12-07 21:32:17.813281: Pseudo dice [0.7087]
2024-12-07 21:32:17.814040: Epoch time: 396.49 s
2024-12-07 21:32:19.248266: 
2024-12-07 21:32:19.249718: Epoch 82
2024-12-07 21:32:19.250455: Current learning rate: 0.00926
2024-12-07 21:39:38.013952: Validation loss did not improve from -0.51067. Patience: 24/50
2024-12-07 21:39:38.014953: train_loss -0.7172
2024-12-07 21:39:38.015706: val_loss -0.3941
2024-12-07 21:39:38.016392: Pseudo dice [0.6941]
2024-12-07 21:39:38.017131: Epoch time: 438.77 s
2024-12-07 21:39:39.356809: 
2024-12-07 21:39:39.358135: Epoch 83
2024-12-07 21:39:39.358825: Current learning rate: 0.00925
2024-12-07 21:46:12.844149: Validation loss did not improve from -0.51067. Patience: 25/50
2024-12-07 21:46:12.845118: train_loss -0.7316
2024-12-07 21:46:12.846036: val_loss -0.4732
2024-12-07 21:46:12.846829: Pseudo dice [0.7282]
2024-12-07 21:46:12.847497: Epoch time: 393.49 s
2024-12-07 21:46:14.788542: 
2024-12-07 21:46:14.789776: Epoch 84
2024-12-07 21:46:14.790517: Current learning rate: 0.00924
2024-12-07 21:53:18.812966: Validation loss did not improve from -0.51067. Patience: 26/50
2024-12-07 21:53:18.813931: train_loss -0.7319
2024-12-07 21:53:18.815183: val_loss -0.4478
2024-12-07 21:53:18.816214: Pseudo dice [0.7106]
2024-12-07 21:53:18.817219: Epoch time: 424.03 s
2024-12-07 21:53:20.604151: 
2024-12-07 21:53:20.605547: Epoch 85
2024-12-07 21:53:20.606579: Current learning rate: 0.00923
2024-12-07 21:59:50.995303: Validation loss did not improve from -0.51067. Patience: 27/50
2024-12-07 21:59:50.996315: train_loss -0.735
2024-12-07 21:59:50.997264: val_loss -0.3262
2024-12-07 21:59:50.998006: Pseudo dice [0.6545]
2024-12-07 21:59:50.998893: Epoch time: 390.39 s
2024-12-07 21:59:52.372286: 
2024-12-07 21:59:52.373385: Epoch 86
2024-12-07 21:59:52.374253: Current learning rate: 0.00922
2024-12-07 22:07:05.219855: Validation loss did not improve from -0.51067. Patience: 28/50
2024-12-07 22:07:05.224097: train_loss -0.731
2024-12-07 22:07:05.225824: val_loss -0.4401
2024-12-07 22:07:05.226945: Pseudo dice [0.6985]
2024-12-07 22:07:05.228241: Epoch time: 432.85 s
2024-12-07 22:07:06.622582: 
2024-12-07 22:07:06.623908: Epoch 87
2024-12-07 22:07:06.624755: Current learning rate: 0.00921
2024-12-07 22:13:57.282820: Validation loss did not improve from -0.51067. Patience: 29/50
2024-12-07 22:13:57.283953: train_loss -0.7296
2024-12-07 22:13:57.284839: val_loss -0.4141
2024-12-07 22:13:57.285710: Pseudo dice [0.6776]
2024-12-07 22:13:57.286583: Epoch time: 410.66 s
2024-12-07 22:13:58.766117: 
2024-12-07 22:13:58.767550: Epoch 88
2024-12-07 22:13:58.768490: Current learning rate: 0.0092
2024-12-07 22:20:31.138829: Validation loss did not improve from -0.51067. Patience: 30/50
2024-12-07 22:20:31.139992: train_loss -0.7359
2024-12-07 22:20:31.140911: val_loss -0.3818
2024-12-07 22:20:31.141676: Pseudo dice [0.6862]
2024-12-07 22:20:31.142360: Epoch time: 392.38 s
2024-12-07 22:20:32.497591: 
2024-12-07 22:20:32.498706: Epoch 89
2024-12-07 22:20:32.499514: Current learning rate: 0.0092
2024-12-07 22:27:27.217579: Validation loss did not improve from -0.51067. Patience: 31/50
2024-12-07 22:27:27.218294: train_loss -0.7342
2024-12-07 22:27:27.219192: val_loss -0.4494
2024-12-07 22:27:27.220066: Pseudo dice [0.7168]
2024-12-07 22:27:27.220959: Epoch time: 414.72 s
2024-12-07 22:27:29.087639: 
2024-12-07 22:27:29.089245: Epoch 90
2024-12-07 22:27:29.090383: Current learning rate: 0.00919
2024-12-07 22:34:09.907046: Validation loss did not improve from -0.51067. Patience: 32/50
2024-12-07 22:34:09.907973: train_loss -0.731
2024-12-07 22:34:09.908843: val_loss -0.4649
2024-12-07 22:34:09.909487: Pseudo dice [0.7293]
2024-12-07 22:34:09.910236: Epoch time: 400.82 s
2024-12-07 22:34:11.258006: 
2024-12-07 22:34:11.259376: Epoch 91
2024-12-07 22:34:11.260205: Current learning rate: 0.00918
2024-12-07 22:40:55.053111: Validation loss did not improve from -0.51067. Patience: 33/50
2024-12-07 22:40:55.053973: train_loss -0.7295
2024-12-07 22:40:55.054810: val_loss -0.3326
2024-12-07 22:40:55.055576: Pseudo dice [0.6654]
2024-12-07 22:40:55.056364: Epoch time: 403.8 s
2024-12-07 22:40:56.378842: 
2024-12-07 22:40:56.380256: Epoch 92
2024-12-07 22:40:56.381149: Current learning rate: 0.00917
2024-12-07 22:47:59.680939: Validation loss did not improve from -0.51067. Patience: 34/50
2024-12-07 22:47:59.681997: train_loss -0.731
2024-12-07 22:47:59.683069: val_loss -0.3276
2024-12-07 22:47:59.683720: Pseudo dice [0.6588]
2024-12-07 22:47:59.684362: Epoch time: 423.3 s
2024-12-07 22:48:01.047075: 
2024-12-07 22:48:01.048491: Epoch 93
2024-12-07 22:48:01.049309: Current learning rate: 0.00916
2024-12-07 22:55:02.379053: Validation loss did not improve from -0.51067. Patience: 35/50
2024-12-07 22:55:02.379963: train_loss -0.7338
2024-12-07 22:55:02.380791: val_loss -0.3989
2024-12-07 22:55:02.381569: Pseudo dice [0.6757]
2024-12-07 22:55:02.382390: Epoch time: 421.33 s
2024-12-07 22:55:03.709264: 
2024-12-07 22:55:03.710478: Epoch 94
2024-12-07 22:55:03.711409: Current learning rate: 0.00915
2024-12-07 23:01:30.880435: Validation loss did not improve from -0.51067. Patience: 36/50
2024-12-07 23:01:30.881474: train_loss -0.7356
2024-12-07 23:01:30.882621: val_loss -0.4348
2024-12-07 23:01:30.883640: Pseudo dice [0.7067]
2024-12-07 23:01:30.884417: Epoch time: 387.17 s
2024-12-07 23:01:34.083683: 
2024-12-07 23:01:34.085208: Epoch 95
2024-12-07 23:01:34.086206: Current learning rate: 0.00914
2024-12-07 23:08:14.798313: Validation loss did not improve from -0.51067. Patience: 37/50
2024-12-07 23:08:14.800408: train_loss -0.729
2024-12-07 23:08:14.802448: val_loss -0.411
2024-12-07 23:08:14.803277: Pseudo dice [0.6955]
2024-12-07 23:08:14.804282: Epoch time: 400.72 s
2024-12-07 23:08:16.151751: 
2024-12-07 23:08:16.153089: Epoch 96
2024-12-07 23:08:16.153803: Current learning rate: 0.00913
2024-12-07 23:15:31.748163: Validation loss did not improve from -0.51067. Patience: 38/50
2024-12-07 23:15:31.749171: train_loss -0.7332
2024-12-07 23:15:31.750157: val_loss -0.4217
2024-12-07 23:15:31.751053: Pseudo dice [0.7071]
2024-12-07 23:15:31.752078: Epoch time: 435.6 s
2024-12-07 23:15:33.183391: 
2024-12-07 23:15:33.184765: Epoch 97
2024-12-07 23:15:33.185675: Current learning rate: 0.00912
2024-12-07 23:22:26.694436: Validation loss did not improve from -0.51067. Patience: 39/50
2024-12-07 23:22:26.695847: train_loss -0.7415
2024-12-07 23:22:26.696752: val_loss -0.3923
2024-12-07 23:22:26.697689: Pseudo dice [0.6827]
2024-12-07 23:22:26.698439: Epoch time: 413.51 s
2024-12-07 23:22:28.057080: 
2024-12-07 23:22:28.058297: Epoch 98
2024-12-07 23:22:28.059071: Current learning rate: 0.00911
2024-12-07 23:29:23.356886: Validation loss did not improve from -0.51067. Patience: 40/50
2024-12-07 23:29:23.357848: train_loss -0.7383
2024-12-07 23:29:23.359062: val_loss -0.3681
2024-12-07 23:29:23.360308: Pseudo dice [0.6769]
2024-12-07 23:29:23.361300: Epoch time: 415.3 s
2024-12-07 23:29:24.723048: 
2024-12-07 23:29:24.724381: Epoch 99
2024-12-07 23:29:24.725236: Current learning rate: 0.0091
2024-12-07 23:35:54.535059: Validation loss did not improve from -0.51067. Patience: 41/50
2024-12-07 23:35:54.536065: train_loss -0.7443
2024-12-07 23:35:54.537117: val_loss -0.3841
2024-12-07 23:35:54.538130: Pseudo dice [0.6742]
2024-12-07 23:35:54.539007: Epoch time: 389.81 s
2024-12-07 23:35:56.308069: 
2024-12-07 23:35:56.309361: Epoch 100
2024-12-07 23:35:56.310224: Current learning rate: 0.0091
2024-12-07 23:42:43.834786: Validation loss improved from -0.51067 to -0.51231! Patience: 41/50
2024-12-07 23:42:43.836052: train_loss -0.7442
2024-12-07 23:42:43.836879: val_loss -0.5123
2024-12-07 23:42:43.837535: Pseudo dice [0.7452]
2024-12-07 23:42:43.838337: Epoch time: 407.53 s
2024-12-07 23:42:45.186250: 
2024-12-07 23:42:45.187772: Epoch 101
2024-12-07 23:42:45.188500: Current learning rate: 0.00909
2024-12-07 23:49:19.181933: Validation loss did not improve from -0.51231. Patience: 1/50
2024-12-07 23:49:19.182904: train_loss -0.7326
2024-12-07 23:49:19.183841: val_loss -0.4113
2024-12-07 23:49:19.184497: Pseudo dice [0.7026]
2024-12-07 23:49:19.185247: Epoch time: 394.0 s
2024-12-07 23:49:20.532883: 
2024-12-07 23:49:20.534250: Epoch 102
2024-12-07 23:49:20.534954: Current learning rate: 0.00908
2024-12-07 23:56:30.244053: Validation loss did not improve from -0.51231. Patience: 2/50
2024-12-07 23:56:30.245116: train_loss -0.7236
2024-12-07 23:56:30.245900: val_loss -0.4229
2024-12-07 23:56:30.246589: Pseudo dice [0.6903]
2024-12-07 23:56:30.247351: Epoch time: 429.71 s
2024-12-07 23:56:31.595934: 
2024-12-07 23:56:31.597281: Epoch 103
2024-12-07 23:56:31.598036: Current learning rate: 0.00907
2024-12-08 00:03:23.919730: Validation loss did not improve from -0.51231. Patience: 3/50
2024-12-08 00:03:23.920694: train_loss -0.7302
2024-12-08 00:03:23.921460: val_loss -0.4198
2024-12-08 00:03:23.922495: Pseudo dice [0.6985]
2024-12-08 00:03:23.923305: Epoch time: 412.33 s
2024-12-08 00:03:25.288317: 
2024-12-08 00:03:25.289497: Epoch 104
2024-12-08 00:03:25.290207: Current learning rate: 0.00906
2024-12-08 00:10:23.063029: Validation loss did not improve from -0.51231. Patience: 4/50
2024-12-08 00:10:23.064085: train_loss -0.7408
2024-12-08 00:10:23.064971: val_loss -0.4481
2024-12-08 00:10:23.065688: Pseudo dice [0.713]
2024-12-08 00:10:23.066388: Epoch time: 417.78 s
2024-12-08 00:10:24.829512: 
2024-12-08 00:10:24.830864: Epoch 105
2024-12-08 00:10:24.831600: Current learning rate: 0.00905
2024-12-08 00:17:01.941247: Validation loss did not improve from -0.51231. Patience: 5/50
2024-12-08 00:17:01.946110: train_loss -0.7396
2024-12-08 00:17:01.947958: val_loss -0.4187
2024-12-08 00:17:01.948894: Pseudo dice [0.7014]
2024-12-08 00:17:01.949996: Epoch time: 397.12 s
2024-12-08 00:17:03.358302: 
2024-12-08 00:17:03.359574: Epoch 106
2024-12-08 00:17:03.360383: Current learning rate: 0.00904
2024-12-08 00:23:29.956263: Validation loss did not improve from -0.51231. Patience: 6/50
2024-12-08 00:23:29.957236: train_loss -0.7434
2024-12-08 00:23:29.958042: val_loss -0.4082
2024-12-08 00:23:29.958908: Pseudo dice [0.6815]
2024-12-08 00:23:29.959893: Epoch time: 386.6 s
2024-12-08 00:23:32.316593: 
2024-12-08 00:23:32.318110: Epoch 107
2024-12-08 00:23:32.319051: Current learning rate: 0.00903
2024-12-08 00:30:48.191037: Validation loss did not improve from -0.51231. Patience: 7/50
2024-12-08 00:30:48.192321: train_loss -0.7401
2024-12-08 00:30:48.193427: val_loss -0.425
2024-12-08 00:30:48.194389: Pseudo dice [0.7016]
2024-12-08 00:30:48.195334: Epoch time: 435.88 s
2024-12-08 00:30:49.551438: 
2024-12-08 00:30:49.552881: Epoch 108
2024-12-08 00:30:49.553962: Current learning rate: 0.00902
2024-12-08 00:37:27.236077: Validation loss did not improve from -0.51231. Patience: 8/50
2024-12-08 00:37:27.237112: train_loss -0.7482
2024-12-08 00:37:27.238056: val_loss -0.3971
2024-12-08 00:37:27.238908: Pseudo dice [0.6851]
2024-12-08 00:37:27.239932: Epoch time: 397.69 s
2024-12-08 00:37:28.596399: 
2024-12-08 00:37:28.597613: Epoch 109
2024-12-08 00:37:28.598442: Current learning rate: 0.00901
2024-12-08 00:44:00.178655: Validation loss did not improve from -0.51231. Patience: 9/50
2024-12-08 00:44:00.179700: train_loss -0.7474
2024-12-08 00:44:00.180404: val_loss -0.4696
2024-12-08 00:44:00.181081: Pseudo dice [0.7166]
2024-12-08 00:44:00.181754: Epoch time: 391.58 s
2024-12-08 00:44:01.969477: 
2024-12-08 00:44:01.970987: Epoch 110
2024-12-08 00:44:01.971902: Current learning rate: 0.009
2024-12-08 00:50:41.925833: Validation loss did not improve from -0.51231. Patience: 10/50
2024-12-08 00:50:41.927114: train_loss -0.7436
2024-12-08 00:50:41.928235: val_loss -0.4295
2024-12-08 00:50:41.929240: Pseudo dice [0.7084]
2024-12-08 00:50:41.930493: Epoch time: 399.96 s
2024-12-08 00:50:43.295909: 
2024-12-08 00:50:43.297607: Epoch 111
2024-12-08 00:50:43.298836: Current learning rate: 0.009
2024-12-08 00:57:10.362275: Validation loss did not improve from -0.51231. Patience: 11/50
2024-12-08 00:57:10.363282: train_loss -0.7525
2024-12-08 00:57:10.364202: val_loss -0.44
2024-12-08 00:57:10.365100: Pseudo dice [0.71]
2024-12-08 00:57:10.365905: Epoch time: 387.07 s
2024-12-08 00:57:11.768427: 
2024-12-08 00:57:11.769786: Epoch 112
2024-12-08 00:57:11.770632: Current learning rate: 0.00899
2024-12-08 01:03:39.943327: Validation loss did not improve from -0.51231. Patience: 12/50
2024-12-08 01:03:39.944386: train_loss -0.7499
2024-12-08 01:03:39.945161: val_loss -0.5123
2024-12-08 01:03:39.945847: Pseudo dice [0.7473]
2024-12-08 01:03:39.946512: Epoch time: 388.18 s
2024-12-08 01:03:41.348200: 
2024-12-08 01:03:41.349577: Epoch 113
2024-12-08 01:03:41.350264: Current learning rate: 0.00898
2024-12-08 01:10:11.707047: Validation loss did not improve from -0.51231. Patience: 13/50
2024-12-08 01:10:11.708065: train_loss -0.7458
2024-12-08 01:10:11.709106: val_loss -0.3469
2024-12-08 01:10:11.710109: Pseudo dice [0.6559]
2024-12-08 01:10:11.711117: Epoch time: 390.36 s
2024-12-08 01:10:13.076231: 
2024-12-08 01:10:13.077840: Epoch 114
2024-12-08 01:10:13.078836: Current learning rate: 0.00897
2024-12-08 01:16:23.233363: Validation loss did not improve from -0.51231. Patience: 14/50
2024-12-08 01:16:23.234594: train_loss -0.7443
2024-12-08 01:16:23.235549: val_loss -0.4474
2024-12-08 01:16:23.236277: Pseudo dice [0.7002]
2024-12-08 01:16:23.236977: Epoch time: 370.16 s
2024-12-08 01:16:25.013890: 
2024-12-08 01:16:25.015327: Epoch 115
2024-12-08 01:16:25.016018: Current learning rate: 0.00896
2024-12-08 01:22:49.529268: Validation loss did not improve from -0.51231. Patience: 15/50
2024-12-08 01:22:49.533181: train_loss -0.7462
2024-12-08 01:22:49.534859: val_loss -0.4452
2024-12-08 01:22:49.535815: Pseudo dice [0.7069]
2024-12-08 01:22:49.536966: Epoch time: 384.52 s
2024-12-08 01:22:51.001143: 
2024-12-08 01:22:51.002614: Epoch 116
2024-12-08 01:22:51.003359: Current learning rate: 0.00895
2024-12-08 01:29:24.271922: Validation loss did not improve from -0.51231. Patience: 16/50
2024-12-08 01:29:24.273082: train_loss -0.7474
2024-12-08 01:29:24.273966: val_loss -0.4297
2024-12-08 01:29:24.274750: Pseudo dice [0.7129]
2024-12-08 01:29:24.275608: Epoch time: 393.27 s
2024-12-08 01:29:25.661865: 
2024-12-08 01:29:25.663235: Epoch 117
2024-12-08 01:29:25.664181: Current learning rate: 0.00894
2024-12-08 01:35:57.329847: Validation loss did not improve from -0.51231. Patience: 17/50
2024-12-08 01:35:57.330874: train_loss -0.7464
2024-12-08 01:35:57.331959: val_loss -0.4479
2024-12-08 01:35:57.332763: Pseudo dice [0.7024]
2024-12-08 01:35:57.333450: Epoch time: 391.67 s
2024-12-08 01:35:59.667479: 
2024-12-08 01:35:59.668798: Epoch 118
2024-12-08 01:35:59.669648: Current learning rate: 0.00893
2024-12-08 01:42:36.372149: Validation loss did not improve from -0.51231. Patience: 18/50
2024-12-08 01:42:36.373188: train_loss -0.7478
2024-12-08 01:42:36.373987: val_loss -0.4109
2024-12-08 01:42:36.374613: Pseudo dice [0.6841]
2024-12-08 01:42:36.375344: Epoch time: 396.71 s
2024-12-08 01:42:37.778417: 
2024-12-08 01:42:37.779777: Epoch 119
2024-12-08 01:42:37.780775: Current learning rate: 0.00892
2024-12-08 01:49:20.885968: Validation loss did not improve from -0.51231. Patience: 19/50
2024-12-08 01:49:20.887016: train_loss -0.742
2024-12-08 01:49:20.887808: val_loss -0.3426
2024-12-08 01:49:20.888668: Pseudo dice [0.6632]
2024-12-08 01:49:20.889468: Epoch time: 403.11 s
2024-12-08 01:49:22.688819: 
2024-12-08 01:49:22.690202: Epoch 120
2024-12-08 01:49:22.691095: Current learning rate: 0.00891
2024-12-08 01:55:43.719847: Validation loss did not improve from -0.51231. Patience: 20/50
2024-12-08 01:55:43.720823: train_loss -0.7478
2024-12-08 01:55:43.721865: val_loss -0.4571
2024-12-08 01:55:43.722688: Pseudo dice [0.7209]
2024-12-08 01:55:43.723692: Epoch time: 381.03 s
2024-12-08 01:55:45.154323: 
2024-12-08 01:55:45.155846: Epoch 121
2024-12-08 01:55:45.156808: Current learning rate: 0.0089
2024-12-08 02:02:01.647241: Validation loss did not improve from -0.51231. Patience: 21/50
2024-12-08 02:02:01.648259: train_loss -0.7486
2024-12-08 02:02:01.649157: val_loss -0.447
2024-12-08 02:02:01.650070: Pseudo dice [0.7144]
2024-12-08 02:02:01.650863: Epoch time: 376.5 s
2024-12-08 02:02:03.055225: 
2024-12-08 02:02:03.056596: Epoch 122
2024-12-08 02:02:03.057421: Current learning rate: 0.00889
2024-12-08 02:08:29.962998: Validation loss did not improve from -0.51231. Patience: 22/50
2024-12-08 02:08:29.964053: train_loss -0.7502
2024-12-08 02:08:29.964819: val_loss -0.4399
2024-12-08 02:08:29.965508: Pseudo dice [0.7079]
2024-12-08 02:08:29.966230: Epoch time: 386.91 s
2024-12-08 02:08:31.385002: 
2024-12-08 02:08:31.386416: Epoch 123
2024-12-08 02:08:31.387158: Current learning rate: 0.00889
2024-12-08 02:14:30.794856: Validation loss did not improve from -0.51231. Patience: 23/50
2024-12-08 02:14:30.795928: train_loss -0.7481
2024-12-08 02:14:30.796738: val_loss -0.3427
2024-12-08 02:14:30.797613: Pseudo dice [0.6526]
2024-12-08 02:14:30.798319: Epoch time: 359.41 s
2024-12-08 02:14:32.210724: 
2024-12-08 02:14:32.211905: Epoch 124
2024-12-08 02:14:32.212782: Current learning rate: 0.00888
2024-12-08 02:19:51.100650: Validation loss did not improve from -0.51231. Patience: 24/50
2024-12-08 02:19:51.101773: train_loss -0.7456
2024-12-08 02:19:51.102926: val_loss -0.4072
2024-12-08 02:19:51.103642: Pseudo dice [0.6808]
2024-12-08 02:19:51.104425: Epoch time: 318.89 s
2024-12-08 02:19:52.870136: 
2024-12-08 02:19:52.870902: Epoch 125
2024-12-08 02:19:52.871536: Current learning rate: 0.00887
2024-12-08 02:24:12.515840: Validation loss did not improve from -0.51231. Patience: 25/50
2024-12-08 02:24:12.518106: train_loss -0.7452
2024-12-08 02:24:12.518935: val_loss -0.3763
2024-12-08 02:24:12.519683: Pseudo dice [0.6664]
2024-12-08 02:24:12.520687: Epoch time: 259.65 s
2024-12-08 02:24:13.945283: 
2024-12-08 02:24:13.946813: Epoch 126
2024-12-08 02:24:13.947525: Current learning rate: 0.00886
2024-12-08 02:28:32.931612: Validation loss did not improve from -0.51231. Patience: 26/50
2024-12-08 02:28:32.934523: train_loss -0.7516
2024-12-08 02:28:32.935752: val_loss -0.3606
2024-12-08 02:28:32.936759: Pseudo dice [0.6712]
2024-12-08 02:28:32.937677: Epoch time: 258.99 s
2024-12-08 02:28:34.367358: 
2024-12-08 02:28:34.368749: Epoch 127
2024-12-08 02:28:34.369675: Current learning rate: 0.00885
2024-12-08 02:32:53.057642: Validation loss did not improve from -0.51231. Patience: 27/50
2024-12-08 02:32:53.058725: train_loss -0.7462
2024-12-08 02:32:53.059662: val_loss -0.4156
2024-12-08 02:32:53.060453: Pseudo dice [0.7091]
2024-12-08 02:32:53.061236: Epoch time: 258.69 s
2024-12-08 02:32:54.449723: 
2024-12-08 02:32:54.450970: Epoch 128
2024-12-08 02:32:54.451603: Current learning rate: 0.00884
2024-12-08 02:37:15.627347: Validation loss did not improve from -0.51231. Patience: 28/50
2024-12-08 02:37:15.628765: train_loss -0.7499
2024-12-08 02:37:15.629688: val_loss -0.4502
2024-12-08 02:37:15.630539: Pseudo dice [0.7074]
2024-12-08 02:37:15.631223: Epoch time: 261.18 s
2024-12-08 02:37:17.543715: 
2024-12-08 02:37:17.544919: Epoch 129
2024-12-08 02:37:17.545586: Current learning rate: 0.00883
2024-12-08 02:41:36.975983: Validation loss did not improve from -0.51231. Patience: 29/50
2024-12-08 02:41:36.977127: train_loss -0.7498
2024-12-08 02:41:36.977865: val_loss -0.4326
2024-12-08 02:41:36.978584: Pseudo dice [0.7074]
2024-12-08 02:41:36.979358: Epoch time: 259.43 s
2024-12-08 02:41:38.813344: 
2024-12-08 02:41:38.814792: Epoch 130
2024-12-08 02:41:38.815595: Current learning rate: 0.00882
2024-12-08 02:45:57.289116: Validation loss did not improve from -0.51231. Patience: 30/50
2024-12-08 02:45:57.290140: train_loss -0.7502
2024-12-08 02:45:57.291088: val_loss -0.4924
2024-12-08 02:45:57.291758: Pseudo dice [0.7373]
2024-12-08 02:45:57.292437: Epoch time: 258.48 s
2024-12-08 02:45:58.709988: 
2024-12-08 02:45:58.711257: Epoch 131
2024-12-08 02:45:58.712034: Current learning rate: 0.00881
2024-12-08 02:50:21.880567: Validation loss did not improve from -0.51231. Patience: 31/50
2024-12-08 02:50:21.881561: train_loss -0.7519
2024-12-08 02:50:21.882481: val_loss -0.4139
2024-12-08 02:50:21.883272: Pseudo dice [0.6828]
2024-12-08 02:50:21.884064: Epoch time: 263.17 s
2024-12-08 02:50:23.328781: 
2024-12-08 02:50:23.330226: Epoch 132
2024-12-08 02:50:23.331058: Current learning rate: 0.0088
2024-12-08 02:54:49.572276: Validation loss did not improve from -0.51231. Patience: 32/50
2024-12-08 02:54:49.573334: train_loss -0.7478
2024-12-08 02:54:49.574139: val_loss -0.2927
2024-12-08 02:54:49.575171: Pseudo dice [0.6126]
2024-12-08 02:54:49.576100: Epoch time: 266.25 s
2024-12-08 02:54:50.972280: 
2024-12-08 02:54:50.973843: Epoch 133
2024-12-08 02:54:50.975204: Current learning rate: 0.00879
2024-12-08 02:59:08.076379: Validation loss did not improve from -0.51231. Patience: 33/50
2024-12-08 02:59:08.077072: train_loss -0.7462
2024-12-08 02:59:08.077968: val_loss -0.3655
2024-12-08 02:59:08.078569: Pseudo dice [0.6658]
2024-12-08 02:59:08.079240: Epoch time: 257.11 s
2024-12-08 02:59:09.496464: 
2024-12-08 02:59:09.497718: Epoch 134
2024-12-08 02:59:09.498553: Current learning rate: 0.00879
2024-12-08 03:03:26.688614: Validation loss did not improve from -0.51231. Patience: 34/50
2024-12-08 03:03:26.689744: train_loss -0.7508
2024-12-08 03:03:26.690988: val_loss -0.4139
2024-12-08 03:03:26.691836: Pseudo dice [0.6988]
2024-12-08 03:03:26.692621: Epoch time: 257.19 s
2024-12-08 03:03:28.598345: 
2024-12-08 03:03:28.599703: Epoch 135
2024-12-08 03:03:28.600435: Current learning rate: 0.00878
2024-12-08 03:07:49.797132: Validation loss did not improve from -0.51231. Patience: 35/50
2024-12-08 03:07:49.798283: train_loss -0.7571
2024-12-08 03:07:49.799281: val_loss -0.4117
2024-12-08 03:07:49.800072: Pseudo dice [0.6916]
2024-12-08 03:07:49.800882: Epoch time: 261.2 s
2024-12-08 03:07:51.246207: 
2024-12-08 03:07:51.247604: Epoch 136
2024-12-08 03:07:51.248569: Current learning rate: 0.00877
2024-12-08 03:12:10.057805: Validation loss did not improve from -0.51231. Patience: 36/50
2024-12-08 03:12:10.058847: train_loss -0.7521
2024-12-08 03:12:10.059659: val_loss -0.4489
2024-12-08 03:12:10.060392: Pseudo dice [0.722]
2024-12-08 03:12:10.061065: Epoch time: 258.81 s
2024-12-08 03:12:11.489519: 
2024-12-08 03:12:11.490944: Epoch 137
2024-12-08 03:12:11.491790: Current learning rate: 0.00876
2024-12-08 03:16:25.050670: Validation loss did not improve from -0.51231. Patience: 37/50
2024-12-08 03:16:25.051653: train_loss -0.7483
2024-12-08 03:16:25.052345: val_loss -0.4009
2024-12-08 03:16:25.053122: Pseudo dice [0.6921]
2024-12-08 03:16:25.053809: Epoch time: 253.56 s
2024-12-08 03:16:26.475446: 
2024-12-08 03:16:26.476819: Epoch 138
2024-12-08 03:16:26.477657: Current learning rate: 0.00875
2024-12-08 03:20:43.180136: Validation loss did not improve from -0.51231. Patience: 38/50
2024-12-08 03:20:43.181154: train_loss -0.7499
2024-12-08 03:20:43.182011: val_loss -0.4886
2024-12-08 03:20:43.182850: Pseudo dice [0.7277]
2024-12-08 03:20:43.183683: Epoch time: 256.71 s
2024-12-08 03:20:44.615297: 
2024-12-08 03:20:44.616575: Epoch 139
2024-12-08 03:20:44.617425: Current learning rate: 0.00874
2024-12-08 03:24:55.706968: Validation loss did not improve from -0.51231. Patience: 39/50
2024-12-08 03:24:55.709860: train_loss -0.759
2024-12-08 03:24:55.711951: val_loss -0.3661
2024-12-08 03:24:55.712708: Pseudo dice [0.6502]
2024-12-08 03:24:55.713580: Epoch time: 251.1 s
2024-12-08 03:24:58.132138: 
2024-12-08 03:24:58.133748: Epoch 140
2024-12-08 03:24:58.134985: Current learning rate: 0.00873
2024-12-08 03:29:16.741127: Validation loss did not improve from -0.51231. Patience: 40/50
2024-12-08 03:29:16.742541: train_loss -0.7588
2024-12-08 03:29:16.743562: val_loss -0.4413
2024-12-08 03:29:16.744401: Pseudo dice [0.7172]
2024-12-08 03:29:16.745415: Epoch time: 258.61 s
2024-12-08 03:29:18.219091: 
2024-12-08 03:29:18.220547: Epoch 141
2024-12-08 03:29:18.221401: Current learning rate: 0.00872
2024-12-08 03:33:43.482821: Validation loss did not improve from -0.51231. Patience: 41/50
2024-12-08 03:33:43.485163: train_loss -0.7584
2024-12-08 03:33:43.486123: val_loss -0.3922
2024-12-08 03:33:43.486852: Pseudo dice [0.6856]
2024-12-08 03:33:43.487638: Epoch time: 265.27 s
2024-12-08 03:33:45.008467: 
2024-12-08 03:33:45.009942: Epoch 142
2024-12-08 03:33:45.010742: Current learning rate: 0.00871
2024-12-08 03:38:06.885100: Validation loss did not improve from -0.51231. Patience: 42/50
2024-12-08 03:38:06.885991: train_loss -0.762
2024-12-08 03:38:06.886930: val_loss -0.3253
2024-12-08 03:38:06.887719: Pseudo dice [0.638]
2024-12-08 03:38:06.888505: Epoch time: 261.88 s
2024-12-08 03:38:08.349366: 
2024-12-08 03:38:08.350862: Epoch 143
2024-12-08 03:38:08.351691: Current learning rate: 0.0087
2024-12-08 03:42:31.666248: Validation loss did not improve from -0.51231. Patience: 43/50
2024-12-08 03:42:31.667853: train_loss -0.7609
2024-12-08 03:42:31.668694: val_loss -0.4643
2024-12-08 03:42:31.669467: Pseudo dice [0.7166]
2024-12-08 03:42:31.670197: Epoch time: 263.32 s
2024-12-08 03:42:33.102533: 
2024-12-08 03:42:33.104141: Epoch 144
2024-12-08 03:42:33.104849: Current learning rate: 0.00869
2024-12-08 03:46:54.626098: Validation loss did not improve from -0.51231. Patience: 44/50
2024-12-08 03:46:54.627129: train_loss -0.7685
2024-12-08 03:46:54.627909: val_loss -0.4009
2024-12-08 03:46:54.628602: Pseudo dice [0.7004]
2024-12-08 03:46:54.629234: Epoch time: 261.53 s
2024-12-08 03:46:56.505282: 
2024-12-08 03:46:56.506497: Epoch 145
2024-12-08 03:46:56.507291: Current learning rate: 0.00868
2024-12-08 03:51:19.798480: Validation loss did not improve from -0.51231. Patience: 45/50
2024-12-08 03:51:19.799659: train_loss -0.7646
2024-12-08 03:51:19.800514: val_loss -0.5068
2024-12-08 03:51:19.801512: Pseudo dice [0.7393]
2024-12-08 03:51:19.802320: Epoch time: 263.3 s
2024-12-08 03:51:21.241635: 
2024-12-08 03:51:21.243121: Epoch 146
2024-12-08 03:51:21.243959: Current learning rate: 0.00868
2024-12-08 03:54:54.614703: Validation loss did not improve from -0.51231. Patience: 46/50
2024-12-08 03:54:54.615841: train_loss -0.7571
2024-12-08 03:54:54.616616: val_loss -0.3946
2024-12-08 03:54:54.617251: Pseudo dice [0.6789]
2024-12-08 03:54:54.618061: Epoch time: 213.38 s
2024-12-08 03:54:56.052646: 
2024-12-08 03:54:56.053938: Epoch 147
2024-12-08 03:54:56.054781: Current learning rate: 0.00867
2024-12-08 03:58:43.419022: Validation loss did not improve from -0.51231. Patience: 47/50
2024-12-08 03:58:43.419976: train_loss -0.7492
2024-12-08 03:58:43.421139: val_loss -0.3005
2024-12-08 03:58:43.422328: Pseudo dice [0.6361]
2024-12-08 03:58:43.423329: Epoch time: 227.37 s
2024-12-08 03:58:44.854741: 
2024-12-08 03:58:44.856045: Epoch 148
2024-12-08 03:58:44.856924: Current learning rate: 0.00866
2024-12-08 04:03:08.464432: Validation loss did not improve from -0.51231. Patience: 48/50
2024-12-08 04:03:08.465281: train_loss -0.7459
2024-12-08 04:03:08.466395: val_loss -0.3722
2024-12-08 04:03:08.467356: Pseudo dice [0.6689]
2024-12-08 04:03:08.468159: Epoch time: 263.61 s
2024-12-08 04:03:09.893917: 
2024-12-08 04:03:09.895274: Epoch 149
2024-12-08 04:03:09.896097: Current learning rate: 0.00865
2024-12-08 04:07:34.986468: Validation loss did not improve from -0.51231. Patience: 49/50
2024-12-08 04:07:34.987539: train_loss -0.7548
2024-12-08 04:07:34.988671: val_loss -0.4386
2024-12-08 04:07:34.989607: Pseudo dice [0.6983]
2024-12-08 04:07:34.990384: Epoch time: 265.09 s
2024-12-08 04:07:36.780614: 
2024-12-08 04:07:36.782116: Epoch 150
2024-12-08 04:07:36.783059: Current learning rate: 0.00864
2024-12-08 04:12:01.105294: Validation loss did not improve from -0.51231. Patience: 50/50
2024-12-08 04:12:01.106363: train_loss -0.7659
2024-12-08 04:12:01.107090: val_loss -0.3083
2024-12-08 04:12:01.107766: Pseudo dice [0.6355]
2024-12-08 04:12:01.108444: Epoch time: 264.33 s
2024-12-08 04:12:02.550240: Patience reached. Stopping training.
2024-12-08 04:12:03.076967: Training done.
2024-12-08 04:12:03.195258: Using splits from existing split file: /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset307_Sohee_Calcium_OCT_CrossValidation/splits_final.json
2024-12-08 04:12:03.197337: The split file contains 5 splits.
2024-12-08 04:12:03.197964: Desired fold for training: 3
2024-12-08 04:12:03.198565: This split has 7 training and 1 validation cases.
2024-12-08 04:12:03.199347: predicting 701-013
2024-12-08 04:12:03.273439: 701-013, shape torch.Size([1, 375, 498, 498]), rank 0
2024-12-08 04:14:44.259173: Validation complete
2024-12-08 04:14:44.260509: Mean Validation Dice:  0.6310717853147866
mv: cannot stat '/home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_results/Dataset308_Sohee_Calcium_OCT_CrossValidation/nnUNetTrainer__nnUNetPlans__3d_32x160x128_b10/fold_2': No such file or directory
mv: cannot stat '/home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_results/Dataset308_Sohee_Calcium_OCT_CrossValidation/nnUNetTrainer__nnUNetPlans__3d_32x160x128_b10/fold_3': No such file or directory

############################
INFO: You are using the old nnU-Net default plans. We have updated our recommendations. Please consider using those instead! Read more here: https://github.com/MIC-DKFZ/nnUNet/blob/master/documentation/resenc_presets.md
############################

Using device: cuda:0

#######################################################################
Please cite the following paper when using nnU-Net:
Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.
#######################################################################

2024-12-08 04:14:51.161946: Using torch.compile...
/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
################### Loading pretrained weights from file  /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_results/Dataset300_Lumen_and_Wall_OCT/nnUNetTrainer__nnUNetPreprocessPlans__3d_fullres/fold_all/checkpoint_best.pth ###################
Below is the list of overlapping blocks in pretrained model and nnUNet architecture:
encoder.stages.0.0.convs.0.conv.weight shape torch.Size([32, 1, 3, 3, 3])
encoder.stages.0.0.convs.0.conv.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.norm.weight shape torch.Size([32])
encoder.stages.0.0.convs.0.norm.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.0.weight shape torch.Size([32, 1, 3, 3, 3])
encoder.stages.0.0.convs.0.all_modules.0.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.1.weight shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.1.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
encoder.stages.0.0.convs.1.conv.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.norm.weight shape torch.Size([32])
encoder.stages.0.0.convs.1.norm.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
encoder.stages.0.0.convs.1.all_modules.0.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.1.weight shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.1.bias shape torch.Size([32])
encoder.stages.1.0.convs.0.conv.weight shape torch.Size([64, 32, 3, 3, 3])
encoder.stages.1.0.convs.0.conv.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.norm.weight shape torch.Size([64])
encoder.stages.1.0.convs.0.norm.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.0.weight shape torch.Size([64, 32, 3, 3, 3])
encoder.stages.1.0.convs.0.all_modules.0.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.1.weight shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.1.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
encoder.stages.1.0.convs.1.conv.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.norm.weight shape torch.Size([64])
encoder.stages.1.0.convs.1.norm.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
encoder.stages.1.0.convs.1.all_modules.0.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.1.weight shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.1.bias shape torch.Size([64])
encoder.stages.2.0.convs.0.conv.weight shape torch.Size([128, 64, 3, 3, 3])
encoder.stages.2.0.convs.0.conv.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.norm.weight shape torch.Size([128])
encoder.stages.2.0.convs.0.norm.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.0.weight shape torch.Size([128, 64, 3, 3, 3])
encoder.stages.2.0.convs.0.all_modules.0.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.1.weight shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.1.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
encoder.stages.2.0.convs.1.conv.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.norm.weight shape torch.Size([128])
encoder.stages.2.0.convs.1.norm.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
encoder.stages.2.0.convs.1.all_modules.0.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.1.weight shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.1.bias shape torch.Size([128])
encoder.stages.3.0.convs.0.conv.weight shape torch.Size([256, 128, 3, 3, 3])
encoder.stages.3.0.convs.0.conv.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.norm.weight shape torch.Size([256])
encoder.stages.3.0.convs.0.norm.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.0.weight shape torch.Size([256, 128, 3, 3, 3])
encoder.stages.3.0.convs.0.all_modules.0.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.1.weight shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.1.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
encoder.stages.3.0.convs.1.conv.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.norm.weight shape torch.Size([256])
encoder.stages.3.0.convs.1.norm.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
encoder.stages.3.0.convs.1.all_modules.0.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.1.weight shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.1.bias shape torch.Size([256])
encoder.stages.4.0.convs.0.conv.weight shape torch.Size([320, 256, 3, 3, 3])
encoder.stages.4.0.convs.0.conv.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.norm.weight shape torch.Size([320])
encoder.stages.4.0.convs.0.norm.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.0.weight shape torch.Size([320, 256, 3, 3, 3])
encoder.stages.4.0.convs.0.all_modules.0.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.1.weight shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.1.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.4.0.convs.1.conv.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.norm.weight shape torch.Size([320])
encoder.stages.4.0.convs.1.norm.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.4.0.convs.1.all_modules.0.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.1.weight shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.1.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.0.conv.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.norm.weight shape torch.Size([320])
encoder.stages.5.0.convs.0.norm.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.0.all_modules.0.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.1.weight shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.1.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.1.conv.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.norm.weight shape torch.Size([320])
encoder.stages.5.0.convs.1.norm.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.1.all_modules.0.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.1.weight shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.0.0.convs.0.conv.weight shape torch.Size([32, 1, 3, 3, 3])
decoder.encoder.stages.0.0.convs.0.conv.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.norm.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.norm.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.0.weight shape torch.Size([32, 1, 3, 3, 3])
decoder.encoder.stages.0.0.convs.0.all_modules.0.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.1.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.1.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.encoder.stages.0.0.convs.1.conv.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.norm.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.norm.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.encoder.stages.0.0.convs.1.all_modules.0.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.1.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.1.bias shape torch.Size([32])
decoder.encoder.stages.1.0.convs.0.conv.weight shape torch.Size([64, 32, 3, 3, 3])
decoder.encoder.stages.1.0.convs.0.conv.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.norm.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.norm.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.0.weight shape torch.Size([64, 32, 3, 3, 3])
decoder.encoder.stages.1.0.convs.0.all_modules.0.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.1.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.1.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.encoder.stages.1.0.convs.1.conv.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.norm.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.norm.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.encoder.stages.1.0.convs.1.all_modules.0.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.1.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.1.bias shape torch.Size([64])
decoder.encoder.stages.2.0.convs.0.conv.weight shape torch.Size([128, 64, 3, 3, 3])
decoder.encoder.stages.2.0.convs.0.conv.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.norm.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.norm.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.0.weight shape torch.Size([128, 64, 3, 3, 3])
decoder.encoder.stages.2.0.convs.0.all_modules.0.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.1.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.1.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.encoder.stages.2.0.convs.1.conv.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.norm.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.norm.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.encoder.stages.2.0.convs.1.all_modules.0.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.1.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.1.bias shape torch.Size([128])
decoder.encoder.stages.3.0.convs.0.conv.weight shape torch.Size([256, 128, 3, 3, 3])
decoder.encoder.stages.3.0.convs.0.conv.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.norm.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.norm.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.0.weight shape torch.Size([256, 128, 3, 3, 3])
decoder.encoder.stages.3.0.convs.0.all_modules.0.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.1.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.1.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.encoder.stages.3.0.convs.1.conv.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.norm.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.norm.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.encoder.stages.3.0.convs.1.all_modules.0.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.1.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.1.bias shape torch.Size([256])
decoder.encoder.stages.4.0.convs.0.conv.weight shape torch.Size([320, 256, 3, 3, 3])
decoder.encoder.stages.4.0.convs.0.conv.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.norm.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.norm.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.0.weight shape torch.Size([320, 256, 3, 3, 3])
decoder.encoder.stages.4.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.4.0.convs.1.conv.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.norm.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.norm.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.4.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.0.conv.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.norm.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.norm.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.1.conv.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.norm.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.norm.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.stages.0.convs.0.conv.weight shape torch.Size([320, 640, 3, 3, 3])
decoder.stages.0.convs.0.conv.bias shape torch.Size([320])
decoder.stages.0.convs.0.norm.weight shape torch.Size([320])
decoder.stages.0.convs.0.norm.bias shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.0.weight shape torch.Size([320, 640, 3, 3, 3])
decoder.stages.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.stages.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.stages.0.convs.1.conv.bias shape torch.Size([320])
decoder.stages.0.convs.1.norm.weight shape torch.Size([320])
decoder.stages.0.convs.1.norm.bias shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.stages.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.stages.1.convs.0.conv.weight shape torch.Size([256, 512, 3, 3, 3])
decoder.stages.1.convs.0.conv.bias shape torch.Size([256])
decoder.stages.1.convs.0.norm.weight shape torch.Size([256])
decoder.stages.1.convs.0.norm.bias shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.0.weight shape torch.Size([256, 512, 3, 3, 3])
decoder.stages.1.convs.0.all_modules.0.bias shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.1.weight shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.1.bias shape torch.Size([256])
decoder.stages.1.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.stages.1.convs.1.conv.bias shape torch.Size([256])
decoder.stages.1.convs.1.norm.weight shape torch.Size([256])
decoder.stages.1.convs.1.norm.bias shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.stages.1.convs.1.all_modules.0.bias shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.1.weight shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.1.bias shape torch.Size([256])
decoder.stages.2.convs.0.conv.weight shape torch.Size([128, 256, 3, 3, 3])
decoder.stages.2.convs.0.conv.bias shape torch.Size([128])
decoder.stages.2.convs.0.norm.weight shape torch.Size([128])
decoder.stages.2.convs.0.norm.bias shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.0.weight shape torch.Size([128, 256, 3, 3, 3])
decoder.stages.2.convs.0.all_modules.0.bias shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.1.weight shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.1.bias shape torch.Size([128])
decoder.stages.2.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.stages.2.convs.1.conv.bias shape torch.Size([128])
decoder.stages.2.convs.1.norm.weight shape torch.Size([128])
decoder.stages.2.convs.1.norm.bias shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.stages.2.convs.1.all_modules.0.bias shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.1.weight shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.1.bias shape torch.Size([128])
decoder.stages.3.convs.0.conv.weight shape torch.Size([64, 128, 3, 3, 3])
decoder.stages.3.convs.0.conv.bias shape torch.Size([64])
decoder.stages.3.convs.0.norm.weight shape torch.Size([64])
decoder.stages.3.convs.0.norm.bias shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.0.weight shape torch.Size([64, 128, 3, 3, 3])
decoder.stages.3.convs.0.all_modules.0.bias shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.1.weight shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.1.bias shape torch.Size([64])
decoder.stages.3.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.stages.3.convs.1.conv.bias shape torch.Size([64])
decoder.stages.3.convs.1.norm.weight shape torch.Size([64])
decoder.stages.3.convs.1.norm.bias shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.stages.3.convs.1.all_modules.0.bias shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.1.weight shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.1.bias shape torch.Size([64])
decoder.stages.4.convs.0.conv.weight shape torch.Size([32, 64, 3, 3, 3])
decoder.stages.4.convs.0.conv.bias shape torch.Size([32])
decoder.stages.4.convs.0.norm.weight shape torch.Size([32])
decoder.stages.4.convs.0.norm.bias shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.0.weight shape torch.Size([32, 64, 3, 3, 3])
decoder.stages.4.convs.0.all_modules.0.bias shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.1.weight shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.1.bias shape torch.Size([32])
decoder.stages.4.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.stages.4.convs.1.conv.bias shape torch.Size([32])
decoder.stages.4.convs.1.norm.weight shape torch.Size([32])
decoder.stages.4.convs.1.norm.bias shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.stages.4.convs.1.all_modules.0.bias shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.1.weight shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.1.bias shape torch.Size([32])
decoder.transpconvs.0.weight shape torch.Size([320, 320, 1, 2, 2])
decoder.transpconvs.0.bias shape torch.Size([320])
decoder.transpconvs.1.weight shape torch.Size([320, 256, 2, 2, 2])
decoder.transpconvs.1.bias shape torch.Size([256])
decoder.transpconvs.2.weight shape torch.Size([256, 128, 2, 2, 2])
decoder.transpconvs.2.bias shape torch.Size([128])
decoder.transpconvs.3.weight shape torch.Size([128, 64, 2, 2, 2])
decoder.transpconvs.3.bias shape torch.Size([64])
decoder.transpconvs.4.weight shape torch.Size([64, 32, 2, 2, 2])
decoder.transpconvs.4.bias shape torch.Size([32])
################### Done ###################
2024-12-08 04:14:57.556438: do_dummy_2d_data_aug: True
2024-12-08 04:14:57.559169: Using splits from existing split file: /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset307_Sohee_Calcium_OCT_CrossValidation/splits_final.json
2024-12-08 04:14:57.561058: The split file contains 5 splits.
2024-12-08 04:14:57.562520: Desired fold for training: 4
2024-12-08 04:14:57.563534: This split has 7 training and 1 validation cases.
using pin_memory on device 0
using pin_memory on device 0

This is the configuration used by this training:
Configuration name: 3d_32x160x128_b10
 {'data_identifier': 'nnUNetPlans_3d_fullres', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 10, 'patch_size': [32, 160, 128], 'median_image_size_in_voxels': [375.0, 498.0, 498.0], 'spacing': [1.0, 1.0, 1.0], 'normalization_schemes': ['ZScoreNormalization'], 'use_mask_for_norm': [False], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'architecture': {'network_class_name': 'dynamic_network_architectures.architectures.unet.PlainConvUNet', 'arch_kwargs': {'n_stages': 6, 'features_per_stage': [32, 64, 128, 256, 320, 320], 'conv_op': 'torch.nn.modules.conv.Conv3d', 'kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]], 'strides': [[1, 1, 1], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [1, 2, 2]], 'n_conv_per_stage': [2, 2, 2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2, 2, 2], 'conv_bias': True, 'norm_op': 'torch.nn.modules.instancenorm.InstanceNorm3d', 'norm_op_kwargs': {'eps': 1e-05, 'affine': True}, 'dropout_op': None, 'dropout_op_kwargs': None, 'nonlin': 'torch.nn.LeakyReLU', 'nonlin_kwargs': {'inplace': True}, 'deep_supervision': True}, '_kw_requires_import': ['conv_op', 'norm_op', 'dropout_op', 'nonlin']}, 'batch_dice': True, 'inherits_from': '3d_fullres'} 

These are the global plan.json settings:
 {'dataset_name': 'Dataset307_Sohee_Calcium_OCT_CrossValidation', 'plans_name': 'nnUNetPlans', 'original_median_spacing_after_transp': [1.0, 1.0, 1.0], 'original_median_shape_after_transp': [375, 498, 498], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 0.4977976381778717, 'mean': 0.13507525622844696, 'median': 0.09599608182907104, 'min': 0.0, 'percentile_00_5': 0.014754901640117168, 'percentile_99_5': 0.4977976381778717, 'std': 0.12025152146816254}}} 

2024-12-08 04:15:18.580168: unpacking dataset...
2024-12-08 04:15:22.836600: unpacking done...
2024-12-08 04:15:22.871491: Unable to plot network architecture: nnUNet_compile is enabled!
2024-12-08 04:15:22.938492: 
2024-12-08 04:15:22.939950: Epoch 0
2024-12-08 04:15:22.941128: Current learning rate: 0.01
2024-12-08 04:20:39.658839: Validation loss improved from 1000.00000 to -0.42337! Patience: 0/50
2024-12-08 04:20:39.660039: train_loss -0.3147
2024-12-08 04:20:39.660834: val_loss -0.4234
2024-12-08 04:20:39.661592: Pseudo dice [0.6868]
2024-12-08 04:20:39.662277: Epoch time: 316.72 s
2024-12-08 04:20:39.663026: Yayy! New best EMA pseudo Dice: 0.6868
2024-12-08 04:20:41.294574: 
2024-12-08 04:20:41.296011: Epoch 1
2024-12-08 04:20:41.296810: Current learning rate: 0.00999
2024-12-08 04:25:10.538987: Validation loss did not improve from -0.42337. Patience: 1/50
2024-12-08 04:25:10.539803: train_loss -0.4774
2024-12-08 04:25:10.540466: val_loss -0.4122
2024-12-08 04:25:10.541043: Pseudo dice [0.6715]
2024-12-08 04:25:10.541634: Epoch time: 269.25 s
2024-12-08 04:25:11.921329: 
2024-12-08 04:25:11.922813: Epoch 2
2024-12-08 04:25:11.923603: Current learning rate: 0.00998
2024-12-08 04:29:44.501242: Validation loss improved from -0.42337 to -0.44818! Patience: 1/50
2024-12-08 04:29:44.502302: train_loss -0.4988
2024-12-08 04:29:44.503236: val_loss -0.4482
2024-12-08 04:29:44.503966: Pseudo dice [0.693]
2024-12-08 04:29:44.504664: Epoch time: 272.58 s
2024-12-08 04:29:46.043576: 
2024-12-08 04:29:46.046080: Epoch 3
2024-12-08 04:29:46.047056: Current learning rate: 0.00997
2024-12-08 04:34:09.297429: Validation loss did not improve from -0.44818. Patience: 1/50
2024-12-08 04:34:09.298557: train_loss -0.521
2024-12-08 04:34:09.299703: val_loss -0.3652
2024-12-08 04:34:09.300613: Pseudo dice [0.6555]
2024-12-08 04:34:09.301467: Epoch time: 263.26 s
2024-12-08 04:34:10.852566: 
2024-12-08 04:34:10.854112: Epoch 4
2024-12-08 04:34:10.855182: Current learning rate: 0.00996
2024-12-08 04:38:36.896731: Validation loss did not improve from -0.44818. Patience: 2/50
2024-12-08 04:38:36.897742: train_loss -0.5283
2024-12-08 04:38:36.898620: val_loss -0.4148
2024-12-08 04:38:36.899390: Pseudo dice [0.6637]
2024-12-08 04:38:36.900104: Epoch time: 266.05 s
2024-12-08 04:38:38.732750: 
2024-12-08 04:38:38.734119: Epoch 5
2024-12-08 04:38:38.734925: Current learning rate: 0.00995
2024-12-08 04:43:04.463311: Validation loss improved from -0.44818 to -0.50081! Patience: 2/50
2024-12-08 04:43:04.464389: train_loss -0.5369
2024-12-08 04:43:04.465355: val_loss -0.5008
2024-12-08 04:43:04.466333: Pseudo dice [0.7142]
2024-12-08 04:43:04.467164: Epoch time: 265.73 s
2024-12-08 04:43:05.833125: 
2024-12-08 04:43:05.835950: Epoch 6
2024-12-08 04:43:05.837128: Current learning rate: 0.00995
2024-12-08 04:47:35.433807: Validation loss did not improve from -0.50081. Patience: 1/50
2024-12-08 04:47:35.434924: train_loss -0.5636
2024-12-08 04:47:35.435946: val_loss -0.4844
2024-12-08 04:47:35.436805: Pseudo dice [0.7187]
2024-12-08 04:47:35.437635: Epoch time: 269.6 s
2024-12-08 04:47:35.438374: Yayy! New best EMA pseudo Dice: 0.6878
2024-12-08 04:47:37.241487: 
2024-12-08 04:47:37.242923: Epoch 7
2024-12-08 04:47:37.243727: Current learning rate: 0.00994
2024-12-08 04:52:14.417165: Validation loss did not improve from -0.50081. Patience: 2/50
2024-12-08 04:52:14.418375: train_loss -0.5876
2024-12-08 04:52:14.419158: val_loss -0.4543
2024-12-08 04:52:14.420075: Pseudo dice [0.6952]
2024-12-08 04:52:14.420789: Epoch time: 277.18 s
2024-12-08 04:52:14.421540: Yayy! New best EMA pseudo Dice: 0.6885
2024-12-08 04:52:16.202785: 
2024-12-08 04:52:16.204066: Epoch 8
2024-12-08 04:52:16.205106: Current learning rate: 0.00993
2024-12-08 04:56:42.087235: Validation loss did not improve from -0.50081. Patience: 3/50
2024-12-08 04:56:42.088437: train_loss -0.5887
2024-12-08 04:56:42.089346: val_loss -0.4036
2024-12-08 04:56:42.090076: Pseudo dice [0.6593]
2024-12-08 04:56:42.090835: Epoch time: 265.89 s
2024-12-08 04:56:44.140970: 
2024-12-08 04:56:44.142928: Epoch 9
2024-12-08 04:56:44.143905: Current learning rate: 0.00992
2024-12-08 05:01:16.859944: Validation loss did not improve from -0.50081. Patience: 4/50
2024-12-08 05:01:16.861095: train_loss -0.582
2024-12-08 05:01:16.862085: val_loss -0.4602
2024-12-08 05:01:16.863013: Pseudo dice [0.7044]
2024-12-08 05:01:16.863924: Epoch time: 272.72 s
2024-12-08 05:01:18.662262: 
2024-12-08 05:01:18.664337: Epoch 10
2024-12-08 05:01:18.665360: Current learning rate: 0.00991
2024-12-08 05:05:53.692873: Validation loss did not improve from -0.50081. Patience: 5/50
2024-12-08 05:05:53.693968: train_loss -0.5911
2024-12-08 05:05:53.695023: val_loss -0.444
2024-12-08 05:05:53.696099: Pseudo dice [0.6971]
2024-12-08 05:05:53.697110: Epoch time: 275.03 s
2024-12-08 05:05:55.058574: 
2024-12-08 05:05:55.060104: Epoch 11
2024-12-08 05:05:55.061284: Current learning rate: 0.0099
2024-12-08 05:10:26.301098: Validation loss did not improve from -0.50081. Patience: 6/50
2024-12-08 05:10:26.302326: train_loss -0.5914
2024-12-08 05:10:26.306028: val_loss -0.4637
2024-12-08 05:10:26.307223: Pseudo dice [0.7047]
2024-12-08 05:10:26.308208: Epoch time: 271.25 s
2024-12-08 05:10:26.309251: Yayy! New best EMA pseudo Dice: 0.6901
2024-12-08 05:10:28.094184: 
2024-12-08 05:10:28.095889: Epoch 12
2024-12-08 05:10:28.097131: Current learning rate: 0.00989
2024-12-08 05:14:54.853611: Validation loss did not improve from -0.50081. Patience: 7/50
2024-12-08 05:14:54.854681: train_loss -0.6039
2024-12-08 05:14:54.855409: val_loss -0.4451
2024-12-08 05:14:54.856174: Pseudo dice [0.6932]
2024-12-08 05:14:54.856869: Epoch time: 266.76 s
2024-12-08 05:14:54.857536: Yayy! New best EMA pseudo Dice: 0.6904
2024-12-08 05:14:56.814142: 
2024-12-08 05:14:56.815524: Epoch 13
2024-12-08 05:14:56.816345: Current learning rate: 0.00988
2024-12-08 05:19:52.017542: Validation loss did not improve from -0.50081. Patience: 8/50
2024-12-08 05:19:52.018691: train_loss -0.6251
2024-12-08 05:19:52.020149: val_loss -0.4439
2024-12-08 05:19:52.020874: Pseudo dice [0.6877]
2024-12-08 05:19:52.021746: Epoch time: 295.21 s
2024-12-08 05:19:53.464235: 
2024-12-08 05:19:53.465702: Epoch 14
2024-12-08 05:19:53.466502: Current learning rate: 0.00987
2024-12-08 05:24:28.556807: Validation loss did not improve from -0.50081. Patience: 9/50
2024-12-08 05:24:28.559643: train_loss -0.6287
2024-12-08 05:24:28.560785: val_loss -0.4911
2024-12-08 05:24:28.561477: Pseudo dice [0.713]
2024-12-08 05:24:28.562156: Epoch time: 275.1 s
2024-12-08 05:24:28.981129: Yayy! New best EMA pseudo Dice: 0.6924
2024-12-08 05:24:30.829900: 
2024-12-08 05:24:30.831288: Epoch 15
2024-12-08 05:24:30.831922: Current learning rate: 0.00986
2024-12-08 05:29:05.147737: Validation loss improved from -0.50081 to -0.50086! Patience: 9/50
2024-12-08 05:29:05.148876: train_loss -0.6247
2024-12-08 05:29:05.149882: val_loss -0.5009
2024-12-08 05:29:05.150835: Pseudo dice [0.7225]
2024-12-08 05:29:05.151770: Epoch time: 274.32 s
2024-12-08 05:29:05.152689: Yayy! New best EMA pseudo Dice: 0.6954
2024-12-08 05:29:07.002077: 
2024-12-08 05:29:07.003293: Epoch 16
2024-12-08 05:29:07.004358: Current learning rate: 0.00986
2024-12-08 05:33:40.037153: Validation loss did not improve from -0.50086. Patience: 1/50
2024-12-08 05:33:40.038529: train_loss -0.6267
2024-12-08 05:33:40.039761: val_loss -0.4641
2024-12-08 05:33:40.040600: Pseudo dice [0.6964]
2024-12-08 05:33:40.041328: Epoch time: 273.04 s
2024-12-08 05:33:40.041998: Yayy! New best EMA pseudo Dice: 0.6955
2024-12-08 05:33:41.997377: 
2024-12-08 05:33:41.999399: Epoch 17
2024-12-08 05:33:42.000409: Current learning rate: 0.00985
2024-12-08 05:38:11.741708: Validation loss did not improve from -0.50086. Patience: 2/50
2024-12-08 05:38:11.742847: train_loss -0.6211
2024-12-08 05:38:11.743705: val_loss -0.4972
2024-12-08 05:38:11.744522: Pseudo dice [0.719]
2024-12-08 05:38:11.745267: Epoch time: 269.75 s
2024-12-08 05:38:11.745986: Yayy! New best EMA pseudo Dice: 0.6979
2024-12-08 05:38:13.606013: 
2024-12-08 05:38:13.607627: Epoch 18
2024-12-08 05:38:13.608446: Current learning rate: 0.00984
2024-12-08 05:42:43.935857: Validation loss did not improve from -0.50086. Patience: 3/50
2024-12-08 05:42:43.937227: train_loss -0.6303
2024-12-08 05:42:43.938088: val_loss -0.4638
2024-12-08 05:42:43.938914: Pseudo dice [0.7096]
2024-12-08 05:42:43.939721: Epoch time: 270.33 s
2024-12-08 05:42:43.940379: Yayy! New best EMA pseudo Dice: 0.699
2024-12-08 05:42:46.502405: 
2024-12-08 05:42:46.503880: Epoch 19
2024-12-08 05:42:46.504745: Current learning rate: 0.00983
2024-12-08 05:47:15.373629: Validation loss did not improve from -0.50086. Patience: 4/50
2024-12-08 05:47:15.375423: train_loss -0.626
2024-12-08 05:47:15.376593: val_loss -0.4771
2024-12-08 05:47:15.377474: Pseudo dice [0.7089]
2024-12-08 05:47:15.378331: Epoch time: 268.87 s
2024-12-08 05:47:15.780128: Yayy! New best EMA pseudo Dice: 0.7
2024-12-08 05:47:17.600425: 
2024-12-08 05:47:17.601794: Epoch 20
2024-12-08 05:47:17.602534: Current learning rate: 0.00982
2024-12-08 05:51:22.451027: Validation loss did not improve from -0.50086. Patience: 5/50
2024-12-08 05:51:22.452588: train_loss -0.6409
2024-12-08 05:51:22.453369: val_loss -0.4798
2024-12-08 05:51:22.454114: Pseudo dice [0.7161]
2024-12-08 05:51:22.454755: Epoch time: 244.85 s
2024-12-08 05:51:22.455378: Yayy! New best EMA pseudo Dice: 0.7016
2024-12-08 05:51:24.348701: 
2024-12-08 05:51:24.349997: Epoch 21
2024-12-08 05:51:24.350763: Current learning rate: 0.00981
2024-12-08 05:54:26.978172: failed to log:  (<class 'FileNotFoundError'>, FileNotFoundError(2, 'No such file or directory'), <traceback object at 0x7f809089d1c0>)
2024-12-08 05:54:26.978172: failed to log:  (<class 'FileNotFoundError'>, FileNotFoundError(2, 'No such file or directory'), <traceback object at 0x7f809089d1c0>)
2024-12-08 05:54:26.978172: failed to log:  (<class 'FileNotFoundError'>, FileNotFoundError(2, 'No such file or directory'), <traceback object at 0x7f809089d1c0>)
2024-12-08 05:54:26.978172: failed to log:  (<class 'FileNotFoundError'>, FileNotFoundError(2, 'No such file or directory'), <traceback object at 0x7f809089d1c0>)
2024-12-08 05:54:26.978172: failed to log:  (<class 'FileNotFoundError'>, FileNotFoundError(2, 'No such file or directory'), <traceback object at 0x7f809089d1c0>)
2024-12-08 05:54:26.978172: Validation loss did not improve from -0.50086. Patience: 6/50
2024-12-08 05:54:29.482522: failed to log:  (<class 'FileNotFoundError'>, FileNotFoundError(2, 'No such file or directory'), <traceback object at 0x7f809089d1c0>)
2024-12-08 05:54:29.482522: failed to log:  (<class 'FileNotFoundError'>, FileNotFoundError(2, 'No such file or directory'), <traceback object at 0x7f809089d1c0>)
2024-12-08 05:54:29.482522: failed to log:  (<class 'FileNotFoundError'>, FileNotFoundError(2, 'No such file or directory'), <traceback object at 0x7f809089d1c0>)
2024-12-08 05:54:29.482522: failed to log:  (<class 'FileNotFoundError'>, FileNotFoundError(2, 'No such file or directory'), <traceback object at 0x7f809089d1c0>)
2024-12-08 05:54:29.482522: failed to log:  (<class 'FileNotFoundError'>, FileNotFoundError(2, 'No such file or directory'), <traceback object at 0x7f809089d1c0>)
2024-12-08 05:54:29.482522: train_loss -0.6401
2024-12-08 05:54:31.986681: failed to log:  (<class 'FileNotFoundError'>, FileNotFoundError(2, 'No such file or directory'), <traceback object at 0x7f806002ce80>)
2024-12-08 05:54:31.986681: failed to log:  (<class 'FileNotFoundError'>, FileNotFoundError(2, 'No such file or directory'), <traceback object at 0x7f806002ce80>)
2024-12-08 05:54:31.986681: failed to log:  (<class 'FileNotFoundError'>, FileNotFoundError(2, 'No such file or directory'), <traceback object at 0x7f806002ce80>)
2024-12-08 05:54:31.986681: failed to log:  (<class 'FileNotFoundError'>, FileNotFoundError(2, 'No such file or directory'), <traceback object at 0x7f806002ce80>)
2024-12-08 05:54:31.986681: failed to log:  (<class 'FileNotFoundError'>, FileNotFoundError(2, 'No such file or directory'), <traceback object at 0x7f806002ce80>)
2024-12-08 05:54:31.986681: val_loss -0.4623
2024-12-08 05:54:34.489929: failed to log:  (<class 'FileNotFoundError'>, FileNotFoundError(2, 'No such file or directory'), <traceback object at 0x7f80a1af25c0>)
2024-12-08 05:54:34.489929: failed to log:  (<class 'FileNotFoundError'>, FileNotFoundError(2, 'No such file or directory'), <traceback object at 0x7f80a1af25c0>)
2024-12-08 05:54:34.489929: failed to log:  (<class 'FileNotFoundError'>, FileNotFoundError(2, 'No such file or directory'), <traceback object at 0x7f80a1af25c0>)
2024-12-08 05:54:34.489929: failed to log:  (<class 'FileNotFoundError'>, FileNotFoundError(2, 'No such file or directory'), <traceback object at 0x7f80a1af25c0>)
2024-12-08 05:54:34.489929: failed to log:  (<class 'FileNotFoundError'>, FileNotFoundError(2, 'No such file or directory'), <traceback object at 0x7f80a1af25c0>)
2024-12-08 05:54:34.489929: Pseudo dice [0.7047]
2024-12-08 05:54:36.993301: failed to log:  (<class 'FileNotFoundError'>, FileNotFoundError(2, 'No such file or directory'), <traceback object at 0x7f80a0680300>)
2024-12-08 05:54:36.993301: failed to log:  (<class 'FileNotFoundError'>, FileNotFoundError(2, 'No such file or directory'), <traceback object at 0x7f80a0680300>)
2024-12-08 05:54:36.993301: failed to log:  (<class 'FileNotFoundError'>, FileNotFoundError(2, 'No such file or directory'), <traceback object at 0x7f80a0680300>)
2024-12-08 05:54:36.993301: failed to log:  (<class 'FileNotFoundError'>, FileNotFoundError(2, 'No such file or directory'), <traceback object at 0x7f80a0680300>)
2024-12-08 05:54:36.993301: failed to log:  (<class 'FileNotFoundError'>, FileNotFoundError(2, 'No such file or directory'), <traceback object at 0x7f80a0680300>)
2024-12-08 05:54:36.993301: Epoch time: 185.13 s
2024-12-08 05:54:39.496230: failed to log:  (<class 'FileNotFoundError'>, FileNotFoundError(2, 'No such file or directory'), <traceback object at 0x7f80a1a75900>)
2024-12-08 05:54:39.496230: failed to log:  (<class 'FileNotFoundError'>, FileNotFoundError(2, 'No such file or directory'), <traceback object at 0x7f80a1a75900>)
2024-12-08 05:54:39.496230: failed to log:  (<class 'FileNotFoundError'>, FileNotFoundError(2, 'No such file or directory'), <traceback object at 0x7f80a1a75900>)
2024-12-08 05:54:39.496230: failed to log:  (<class 'FileNotFoundError'>, FileNotFoundError(2, 'No such file or directory'), <traceback object at 0x7f80a1a75900>)
2024-12-08 05:54:39.496230: failed to log:  (<class 'FileNotFoundError'>, FileNotFoundError(2, 'No such file or directory'), <traceback object at 0x7f80a1a75900>)
2024-12-08 05:54:39.496230: Yayy! New best EMA pseudo Dice: 0.7019
Traceback (most recent call last):
  File "/home/gridsan/nchutisilp/.conda/envs/nnunet/bin/nnUNetv2_train", line 8, in <module>
    sys.exit(run_training_entry())
  File "/home/gridsan/nchutisilp/projects/nnUNet/nnunetv2/run/run_training.py", line 275, in run_training_entry
    run_training(args.dataset_name_or_id, args.configuration, args.fold, args.tr, args.p, args.pretrained_weights,
  File "/home/gridsan/nchutisilp/projects/nnUNet/nnunetv2/run/run_training.py", line 211, in run_training
    nnunet_trainer.run_training()
  File "/home/gridsan/nchutisilp/projects/nnUNet/nnunetv2/training/nnUNetTrainer/nnUNetTrainer.py", line 1411, in run_training
    self.on_epoch_end()
  File "/home/gridsan/nchutisilp/projects/nnUNet/nnunetv2/training/nnUNetTrainer/nnUNetTrainer.py", line 1171, in on_epoch_end
    self.save_checkpoint(join(self.output_folder, 'checkpoint_best.pth'))
  File "/home/gridsan/nchutisilp/projects/nnUNet/nnunetv2/training/nnUNetTrainer/nnUNetTrainer.py", line 1201, in save_checkpoint
    torch.save(checkpoint, filename)
  File "/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/serialization.py", line 627, in save
    with _open_zipfile_writer(f) as opened_zipfile:
  File "/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/serialization.py", line 501, in _open_zipfile_writer
    return container(name_or_buffer)
  File "/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/serialization.py", line 472, in __init__
    super().__init__(torch._C.PyTorchFileWriter(self.name))
RuntimeError: Parent directory /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_results/Dataset307_Sohee_Calcium_OCT_CrossValidation/nnUNetTrainer__nnUNetPlans__3d_32x160x128_b10/fold_4 does not exist.
Exception in thread Thread-3:
Traceback (most recent call last):
  File "/home/gridsan/nchutisilp/.conda/envs/nnunet/lib/python3.9/threading.py", line 980, in _bootstrap_inner
Exception in thread Thread-2:
Traceback (most recent call last):
  File "/home/gridsan/nchutisilp/.conda/envs/nnunet/lib/python3.9/threading.py", line 980, in _bootstrap_inner
    self.run()
  File "/home/gridsan/nchutisilp/.conda/envs/nnunet/lib/python3.9/threading.py", line 917, in run
    self.run()
  File "/home/gridsan/nchutisilp/.conda/envs/nnunet/lib/python3.9/threading.py", line 917, in run
    self._target(*self._args, **self._kwargs)
  File "/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/batchgenerators/dataloading/nondet_multi_threaded_augmenter.py", line 125, in results_loop
    self._target(*self._args, **self._kwargs)
  File "/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/batchgenerators/dataloading/nondet_multi_threaded_augmenter.py", line 125, in results_loop
    raise e
  File "/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/batchgenerators/dataloading/nondet_multi_threaded_augmenter.py", line 103, in results_loop
    raise e
  File "/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/batchgenerators/dataloading/nondet_multi_threaded_augmenter.py", line 103, in results_loop
    raise RuntimeError("One or more background workers are no longer alive. Exiting. Please check the "
RuntimeError: One or more background workers are no longer alive. Exiting. Please check the print statements above for the actual error message
    raise RuntimeError("One or more background workers are no longer alive. Exiting. Please check the "
RuntimeError: One or more background workers are no longer alive. Exiting. Please check the print statements above for the actual error message
mv: cannot stat '/home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_results/Dataset308_Sohee_Calcium_OCT_CrossValidation/nnUNetTrainer__nnUNetPlans__3d_32x160x128_b10/fold_4': No such file or directory
