/home/gridsan/nchutisilp/.conda/envs/nnunet/bin/python
Starting training with CONFIG=3d_32x160x128_b10, DATASET_ID=309, TRAINER=nnUNetTrainer

############################
INFO: You are using the old nnU-Net default plans. We have updated our recommendations. Please consider using those instead! Read more here: https://github.com/MIC-DKFZ/nnUNet/blob/master/documentation/resenc_presets.md
############################

Using device: cuda:0

#######################################################################
Please cite the following paper when using nnU-Net:
Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.
#######################################################################

2024-12-17 01:41:31.565152: Using torch.compile...

############################
INFO: You are using the old nnU-Net default plans. We have updated our recommendations. Please consider using those instead! Read more here: https://github.com/MIC-DKFZ/nnUNet/blob/master/documentation/resenc_presets.md
############################

Using device: cuda:0

#######################################################################
Please cite the following paper when using nnU-Net:
Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.
#######################################################################

2024-12-17 01:41:31.570190: Using torch.compile...
/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
################### Loading pretrained weights from file  /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_results/Dataset300_Lumen_and_Wall_OCT/nnUNetTrainer__nnUNetPreprocessPlans__3d_fullres/fold_all/checkpoint_best.pth ###################
Below is the list of overlapping blocks in pretrained model and nnUNet architecture:
encoder.stages.0.0.convs.0.conv.weight shape torch.Size([32, 1, 3, 3, 3])
encoder.stages.0.0.convs.0.conv.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.norm.weight shape torch.Size([32])
encoder.stages.0.0.convs.0.norm.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.0.weight shape torch.Size([32, 1, 3, 3, 3])
encoder.stages.0.0.convs.0.all_modules.0.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.1.weight shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.1.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
encoder.stages.0.0.convs.1.conv.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.norm.weight shape torch.Size([32])
encoder.stages.0.0.convs.1.norm.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
encoder.stages.0.0.convs.1.all_modules.0.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.1.weight shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.1.bias shape torch.Size([32])
encoder.stages.1.0.convs.0.conv.weight shape torch.Size([64, 32, 3, 3, 3])
encoder.stages.1.0.convs.0.conv.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.norm.weight shape torch.Size([64])
encoder.stages.1.0.convs.0.norm.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.0.weight shape torch.Size([64, 32, 3, 3, 3])
encoder.stages.1.0.convs.0.all_modules.0.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.1.weight shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.1.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
encoder.stages.1.0.convs.1.conv.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.norm.weight shape torch.Size([64])
encoder.stages.1.0.convs.1.norm.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
encoder.stages.1.0.convs.1.all_modules.0.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.1.weight shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.1.bias shape torch.Size([64])
encoder.stages.2.0.convs.0.conv.weight shape torch.Size([128, 64, 3, 3, 3])
encoder.stages.2.0.convs.0.conv.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.norm.weight shape torch.Size([128])
encoder.stages.2.0.convs.0.norm.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.0.weight shape torch.Size([128, 64, 3, 3, 3])
encoder.stages.2.0.convs.0.all_modules.0.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.1.weight shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.1.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
encoder.stages.2.0.convs.1.conv.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.norm.weight shape torch.Size([128])
encoder.stages.2.0.convs.1.norm.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
encoder.stages.2.0.convs.1.all_modules.0.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.1.weight shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.1.bias shape torch.Size([128])
encoder.stages.3.0.convs.0.conv.weight shape torch.Size([256, 128, 3, 3, 3])
encoder.stages.3.0.convs.0.conv.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.norm.weight shape torch.Size([256])
encoder.stages.3.0.convs.0.norm.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.0.weight shape torch.Size([256, 128, 3, 3, 3])
encoder.stages.3.0.convs.0.all_modules.0.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.1.weight shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.1.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
encoder.stages.3.0.convs.1.conv.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.norm.weight shape torch.Size([256])
encoder.stages.3.0.convs.1.norm.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
encoder.stages.3.0.convs.1.all_modules.0.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.1.weight shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.1.bias shape torch.Size([256])
encoder.stages.4.0.convs.0.conv.weight shape torch.Size([320, 256, 3, 3, 3])
encoder.stages.4.0.convs.0.conv.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.norm.weight shape torch.Size([320])
encoder.stages.4.0.convs.0.norm.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.0.weight shape torch.Size([320, 256, 3, 3, 3])
encoder.stages.4.0.convs.0.all_modules.0.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.1.weight shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.1.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.4.0.convs.1.conv.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.norm.weight shape torch.Size([320])
encoder.stages.4.0.convs.1.norm.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.4.0.convs.1.all_modules.0.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.1.weight shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.1.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.0.conv.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.norm.weight shape torch.Size([320])
encoder.stages.5.0.convs.0.norm.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.0.all_modules.0.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.1.weight shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.1.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.1.conv.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.norm.weight shape torch.Size([320])
encoder.stages.5.0.convs.1.norm.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.1.all_modules.0.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.1.weight shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.0.0.convs.0.conv.weight shape torch.Size([32, 1, 3, 3, 3])
decoder.encoder.stages.0.0.convs.0.conv.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.norm.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.norm.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.0.weight shape torch.Size([32, 1, 3, 3, 3])
decoder.encoder.stages.0.0.convs.0.all_modules.0.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.1.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.1.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.encoder.stages.0.0.convs.1.conv.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.norm.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.norm.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.encoder.stages.0.0.convs.1.all_modules.0.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.1.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.1.bias shape torch.Size([32])
decoder.encoder.stages.1.0.convs.0.conv.weight shape torch.Size([64, 32, 3, 3, 3])
decoder.encoder.stages.1.0.convs.0.conv.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.norm.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.norm.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.0.weight shape torch.Size([64, 32, 3, 3, 3])
decoder.encoder.stages.1.0.convs.0.all_modules.0.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.1.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.1.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.encoder.stages.1.0.convs.1.conv.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.norm.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.norm.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.encoder.stages.1.0.convs.1.all_modules.0.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.1.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.1.bias shape torch.Size([64])
decoder.encoder.stages.2.0.convs.0.conv.weight shape torch.Size([128, 64, 3, 3, 3])
decoder.encoder.stages.2.0.convs.0.conv.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.norm.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.norm.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.0.weight shape torch.Size([128, 64, 3, 3, 3])
decoder.encoder.stages.2.0.convs.0.all_modules.0.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.1.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.1.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.encoder.stages.2.0.convs.1.conv.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.norm.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.norm.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.encoder.stages.2.0.convs.1.all_modules.0.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.1.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.1.bias shape torch.Size([128])
decoder.encoder.stages.3.0.convs.0.conv.weight shape torch.Size([256, 128, 3, 3, 3])
decoder.encoder.stages.3.0.convs.0.conv.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.norm.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.norm.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.0.weight shape torch.Size([256, 128, 3, 3, 3])
decoder.encoder.stages.3.0.convs.0.all_modules.0.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.1.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.1.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.encoder.stages.3.0.convs.1.conv.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.norm.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.norm.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.encoder.stages.3.0.convs.1.all_modules.0.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.1.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.1.bias shape torch.Size([256])
decoder.encoder.stages.4.0.convs.0.conv.weight shape torch.Size([320, 256, 3, 3, 3])
decoder.encoder.stages.4.0.convs.0.conv.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.norm.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.norm.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.0.weight shape torch.Size([320, 256, 3, 3, 3])
decoder.encoder.stages.4.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.4.0.convs.1.conv.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.norm.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.norm.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.4.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.0.conv.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.norm.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.norm.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.1.conv.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.norm.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.norm.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.stages.0.convs.0.conv.weight shape torch.Size([320, 640, 3, 3, 3])
decoder.stages.0.convs.0.conv.bias shape torch.Size([320])
decoder.stages.0.convs.0.norm.weight shape torch.Size([320])
decoder.stages.0.convs.0.norm.bias shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.0.weight shape torch.Size([320, 640, 3, 3, 3])
decoder.stages.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.stages.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.stages.0.convs.1.conv.bias shape torch.Size([320])
decoder.stages.0.convs.1.norm.weight shape torch.Size([320])
decoder.stages.0.convs.1.norm.bias shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.stages.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.stages.1.convs.0.conv.weight shape torch.Size([256, 512, 3, 3, 3])
decoder.stages.1.convs.0.conv.bias shape torch.Size([256])
decoder.stages.1.convs.0.norm.weight shape torch.Size([256])
decoder.stages.1.convs.0.norm.bias shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.0.weight shape torch.Size([256, 512, 3, 3, 3])
decoder.stages.1.convs.0.all_modules.0.bias shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.1.weight shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.1.bias shape torch.Size([256])
decoder.stages.1.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.stages.1.convs.1.conv.bias shape torch.Size([256])
decoder.stages.1.convs.1.norm.weight shape torch.Size([256])
decoder.stages.1.convs.1.norm.bias shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.stages.1.convs.1.all_modules.0.bias shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.1.weight shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.1.bias shape torch.Size([256])
decoder.stages.2.convs.0.conv.weight shape torch.Size([128, 256, 3, 3, 3])
decoder.stages.2.convs.0.conv.bias shape torch.Size([128])
decoder.stages.2.convs.0.norm.weight shape torch.Size([128])
decoder.stages.2.convs.0.norm.bias shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.0.weight shape torch.Size([128, 256, 3, 3, 3])
decoder.stages.2.convs.0.all_modules.0.bias shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.1.weight shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.1.bias shape torch.Size([128])
decoder.stages.2.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.stages.2.convs.1.conv.bias shape torch.Size([128])
decoder.stages.2.convs.1.norm.weight shape torch.Size([128])
decoder.stages.2.convs.1.norm.bias shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.stages.2.convs.1.all_modules.0.bias shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.1.weight shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.1.bias shape torch.Size([128])
decoder.stages.3.convs.0.conv.weight shape torch.Size([64, 128, 3, 3, 3])
decoder.stages.3.convs.0.conv.bias shape torch.Size([64])
decoder.stages.3.convs.0.norm.weight shape torch.Size([64])
decoder.stages.3.convs.0.norm.bias shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.0.weight shape torch.Size([64, 128, 3, 3, 3])
decoder.stages.3.convs.0.all_modules.0.bias shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.1.weight shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.1.bias shape torch.Size([64])
decoder.stages.3.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.stages.3.convs.1.conv.bias shape torch.Size([64])
decoder.stages.3.convs.1.norm.weight shape torch.Size([64])
decoder.stages.3.convs.1.norm.bias shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.stages.3.convs.1.all_modules.0.bias shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.1.weight shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.1.bias shape torch.Size([64])
decoder.stages.4.convs.0.conv.weight shape torch.Size([32, 64, 3, 3, 3])
decoder.stages.4.convs.0.conv.bias shape torch.Size([32])
decoder.stages.4.convs.0.norm.weight shape torch.Size([32])
decoder.stages.4.convs.0.norm.bias shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.0.weight shape torch.Size([32, 64, 3, 3, 3])
decoder.stages.4.convs.0.all_modules.0.bias shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.1.weight shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.1.bias shape torch.Size([32])
decoder.stages.4.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.stages.4.convs.1.conv.bias shape torch.Size([32])
decoder.stages.4.convs.1.norm.weight shape torch.Size([32])
decoder.stages.4.convs.1.norm.bias shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.stages.4.convs.1.all_modules.0.bias shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.1.weight shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.1.bias shape torch.Size([32])
decoder.transpconvs.0.weight shape torch.Size([320, 320, 1, 2, 2])
decoder.transpconvs.0.bias shape torch.Size([320])
decoder.transpconvs.1.weight shape torch.Size([320, 256, 2, 2, 2])
decoder.transpconvs.1.bias shape torch.Size([256])
decoder.transpconvs.2.weight shape torch.Size([256, 128, 2, 2, 2])
decoder.transpconvs.2.bias shape torch.Size([128])
decoder.transpconvs.3.weight shape torch.Size([128, 64, 2, 2, 2])
decoder.transpconvs.3.bias shape torch.Size([64])
decoder.transpconvs.4.weight shape torch.Size([64, 32, 2, 2, 2])
decoder.transpconvs.4.bias shape torch.Size([32])
################### Done ###################
2024-12-17 01:41:36.332297: do_dummy_2d_data_aug: True
2024-12-17 01:41:36.348778: Using splits from existing split file: /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset309_Sohee_Calcium_OCT_CrossValidation/splits_final.json
2024-12-17 01:41:36.360397: The split file contains 5 splits.
2024-12-17 01:41:36.362051: Desired fold for training: 1
2024-12-17 01:41:36.363189: This split has 6 training and 2 validation cases.
################### Loading pretrained weights from file  /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_results/Dataset300_Lumen_and_Wall_OCT/nnUNetTrainer__nnUNetPreprocessPlans__3d_fullres/fold_all/checkpoint_best.pth ###################
Below is the list of overlapping blocks in pretrained model and nnUNet architecture:
encoder.stages.0.0.convs.0.conv.weight shape torch.Size([32, 1, 3, 3, 3])
encoder.stages.0.0.convs.0.conv.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.norm.weight shape torch.Size([32])
encoder.stages.0.0.convs.0.norm.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.0.weight shape torch.Size([32, 1, 3, 3, 3])
encoder.stages.0.0.convs.0.all_modules.0.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.1.weight shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.1.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
encoder.stages.0.0.convs.1.conv.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.norm.weight shape torch.Size([32])
encoder.stages.0.0.convs.1.norm.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
encoder.stages.0.0.convs.1.all_modules.0.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.1.weight shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.1.bias shape torch.Size([32])
encoder.stages.1.0.convs.0.conv.weight shape torch.Size([64, 32, 3, 3, 3])
encoder.stages.1.0.convs.0.conv.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.norm.weight shape torch.Size([64])
encoder.stages.1.0.convs.0.norm.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.0.weight shape torch.Size([64, 32, 3, 3, 3])
encoder.stages.1.0.convs.0.all_modules.0.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.1.weight shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.1.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
encoder.stages.1.0.convs.1.conv.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.norm.weight shape torch.Size([64])
encoder.stages.1.0.convs.1.norm.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
encoder.stages.1.0.convs.1.all_modules.0.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.1.weight shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.1.bias shape torch.Size([64])
encoder.stages.2.0.convs.0.conv.weight shape torch.Size([128, 64, 3, 3, 3])
encoder.stages.2.0.convs.0.conv.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.norm.weight shape torch.Size([128])
encoder.stages.2.0.convs.0.norm.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.0.weight shape torch.Size([128, 64, 3, 3, 3])
encoder.stages.2.0.convs.0.all_modules.0.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.1.weight shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.1.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
encoder.stages.2.0.convs.1.conv.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.norm.weight shape torch.Size([128])
encoder.stages.2.0.convs.1.norm.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
encoder.stages.2.0.convs.1.all_modules.0.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.1.weight shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.1.bias shape torch.Size([128])
encoder.stages.3.0.convs.0.conv.weight shape torch.Size([256, 128, 3, 3, 3])
encoder.stages.3.0.convs.0.conv.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.norm.weight shape torch.Size([256])
encoder.stages.3.0.convs.0.norm.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.0.weight shape torch.Size([256, 128, 3, 3, 3])
encoder.stages.3.0.convs.0.all_modules.0.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.1.weight shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.1.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
encoder.stages.3.0.convs.1.conv.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.norm.weight shape torch.Size([256])
encoder.stages.3.0.convs.1.norm.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
encoder.stages.3.0.convs.1.all_modules.0.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.1.weight shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.1.bias shape torch.Size([256])
encoder.stages.4.0.convs.0.conv.weight shape torch.Size([320, 256, 3, 3, 3])
encoder.stages.4.0.convs.0.conv.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.norm.weight shape torch.Size([320])
encoder.stages.4.0.convs.0.norm.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.0.weight shape torch.Size([320, 256, 3, 3, 3])
encoder.stages.4.0.convs.0.all_modules.0.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.1.weight shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.1.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.4.0.convs.1.conv.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.norm.weight shape torch.Size([320])
encoder.stages.4.0.convs.1.norm.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.4.0.convs.1.all_modules.0.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.1.weight shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.1.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.0.conv.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.norm.weight shape torch.Size([320])
encoder.stages.5.0.convs.0.norm.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.0.all_modules.0.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.1.weight shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.1.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.1.conv.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.norm.weight shape torch.Size([320])
encoder.stages.5.0.convs.1.norm.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.1.all_modules.0.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.1.weight shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.0.0.convs.0.conv.weight shape torch.Size([32, 1, 3, 3, 3])
decoder.encoder.stages.0.0.convs.0.conv.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.norm.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.norm.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.0.weight shape torch.Size([32, 1, 3, 3, 3])
decoder.encoder.stages.0.0.convs.0.all_modules.0.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.1.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.1.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.encoder.stages.0.0.convs.1.conv.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.norm.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.norm.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.encoder.stages.0.0.convs.1.all_modules.0.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.1.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.1.bias shape torch.Size([32])
decoder.encoder.stages.1.0.convs.0.conv.weight shape torch.Size([64, 32, 3, 3, 3])
decoder.encoder.stages.1.0.convs.0.conv.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.norm.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.norm.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.0.weight shape torch.Size([64, 32, 3, 3, 3])
decoder.encoder.stages.1.0.convs.0.all_modules.0.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.1.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.1.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.encoder.stages.1.0.convs.1.conv.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.norm.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.norm.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.encoder.stages.1.0.convs.1.all_modules.0.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.1.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.1.bias shape torch.Size([64])
decoder.encoder.stages.2.0.convs.0.conv.weight shape torch.Size([128, 64, 3, 3, 3])
decoder.encoder.stages.2.0.convs.0.conv.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.norm.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.norm.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.0.weight shape torch.Size([128, 64, 3, 3, 3])
decoder.encoder.stages.2.0.convs.0.all_modules.0.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.1.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.1.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.encoder.stages.2.0.convs.1.conv.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.norm.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.norm.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.encoder.stages.2.0.convs.1.all_modules.0.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.1.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.1.bias shape torch.Size([128])
decoder.encoder.stages.3.0.convs.0.conv.weight shape torch.Size([256, 128, 3, 3, 3])
decoder.encoder.stages.3.0.convs.0.conv.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.norm.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.norm.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.0.weight shape torch.Size([256, 128, 3, 3, 3])
decoder.encoder.stages.3.0.convs.0.all_modules.0.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.1.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.1.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.encoder.stages.3.0.convs.1.conv.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.norm.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.norm.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.encoder.stages.3.0.convs.1.all_modules.0.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.1.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.1.bias shape torch.Size([256])
decoder.encoder.stages.4.0.convs.0.conv.weight shape torch.Size([320, 256, 3, 3, 3])
decoder.encoder.stages.4.0.convs.0.conv.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.norm.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.norm.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.0.weight shape torch.Size([320, 256, 3, 3, 3])
decoder.encoder.stages.4.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.4.0.convs.1.conv.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.norm.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.norm.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.4.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.0.conv.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.norm.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.norm.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.1.conv.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.norm.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.norm.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.stages.0.convs.0.conv.weight shape torch.Size([320, 640, 3, 3, 3])
decoder.stages.0.convs.0.conv.bias shape torch.Size([320])
decoder.stages.0.convs.0.norm.weight shape torch.Size([320])
decoder.stages.0.convs.0.norm.bias shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.0.weight shape torch.Size([320, 640, 3, 3, 3])
decoder.stages.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.stages.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.stages.0.convs.1.conv.bias shape torch.Size([320])
decoder.stages.0.convs.1.norm.weight shape torch.Size([320])
decoder.stages.0.convs.1.norm.bias shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.stages.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.stages.1.convs.0.conv.weight shape torch.Size([256, 512, 3, 3, 3])
decoder.stages.1.convs.0.conv.bias shape torch.Size([256])
decoder.stages.1.convs.0.norm.weight shape torch.Size([256])
decoder.stages.1.convs.0.norm.bias shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.0.weight shape torch.Size([256, 512, 3, 3, 3])
decoder.stages.1.convs.0.all_modules.0.bias shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.1.weight shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.1.bias shape torch.Size([256])
decoder.stages.1.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.stages.1.convs.1.conv.bias shape torch.Size([256])
decoder.stages.1.convs.1.norm.weight shape torch.Size([256])
decoder.stages.1.convs.1.norm.bias shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.stages.1.convs.1.all_modules.0.bias shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.1.weight shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.1.bias shape torch.Size([256])
decoder.stages.2.convs.0.conv.weight shape torch.Size([128, 256, 3, 3, 3])
decoder.stages.2.convs.0.conv.bias shape torch.Size([128])
decoder.stages.2.convs.0.norm.weight shape torch.Size([128])
decoder.stages.2.convs.0.norm.bias shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.0.weight shape torch.Size([128, 256, 3, 3, 3])
decoder.stages.2.convs.0.all_modules.0.bias shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.1.weight shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.1.bias shape torch.Size([128])
decoder.stages.2.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.stages.2.convs.1.conv.bias shape torch.Size([128])
decoder.stages.2.convs.1.norm.weight shape torch.Size([128])
decoder.stages.2.convs.1.norm.bias shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.stages.2.convs.1.all_modules.0.bias shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.1.weight shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.1.bias shape torch.Size([128])
decoder.stages.3.convs.0.conv.weight shape torch.Size([64, 128, 3, 3, 3])
decoder.stages.3.convs.0.conv.bias shape torch.Size([64])
decoder.stages.3.convs.0.norm.weight shape torch.Size([64])
decoder.stages.3.convs.0.norm.bias shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.0.weight shape torch.Size([64, 128, 3, 3, 3])
decoder.stages.3.convs.0.all_modules.0.bias shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.1.weight shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.1.bias shape torch.Size([64])
decoder.stages.3.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.stages.3.convs.1.conv.bias shape torch.Size([64])
decoder.stages.3.convs.1.norm.weight shape torch.Size([64])
decoder.stages.3.convs.1.norm.bias shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.stages.3.convs.1.all_modules.0.bias shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.1.weight shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.1.bias shape torch.Size([64])
decoder.stages.4.convs.0.conv.weight shape torch.Size([32, 64, 3, 3, 3])
decoder.stages.4.convs.0.conv.bias shape torch.Size([32])
decoder.stages.4.convs.0.norm.weight shape torch.Size([32])
decoder.stages.4.convs.0.norm.bias shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.0.weight shape torch.Size([32, 64, 3, 3, 3])
decoder.stages.4.convs.0.all_modules.0.bias shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.1.weight shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.1.bias shape torch.Size([32])
decoder.stages.4.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.stages.4.convs.1.conv.bias shape torch.Size([32])
decoder.stages.4.convs.1.norm.weight shape torch.Size([32])
decoder.stages.4.convs.1.norm.bias shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.stages.4.convs.1.all_modules.0.bias shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.1.weight shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.1.bias shape torch.Size([32])
decoder.transpconvs.0.weight shape torch.Size([320, 320, 1, 2, 2])
decoder.transpconvs.0.bias shape torch.Size([320])
decoder.transpconvs.1.weight shape torch.Size([320, 256, 2, 2, 2])
decoder.transpconvs.1.bias shape torch.Size([256])
decoder.transpconvs.2.weight shape torch.Size([256, 128, 2, 2, 2])
decoder.transpconvs.2.bias shape torch.Size([128])
decoder.transpconvs.3.weight shape torch.Size([128, 64, 2, 2, 2])
decoder.transpconvs.3.bias shape torch.Size([64])
decoder.transpconvs.4.weight shape torch.Size([64, 32, 2, 2, 2])
decoder.transpconvs.4.bias shape torch.Size([32])
################### Done ###################
2024-12-17 01:41:36.330266: do_dummy_2d_data_aug: True
2024-12-17 01:41:36.348763: Using splits from existing split file: /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset309_Sohee_Calcium_OCT_CrossValidation/splits_final.json
2024-12-17 01:41:36.360874: The split file contains 5 splits.
2024-12-17 01:41:36.362054: Desired fold for training: 0
2024-12-17 01:41:36.363341: This split has 6 training and 2 validation cases.
using pin_memory on device 0
using pin_memory on device 0
using pin_memory on device 0

This is the configuration used by this training:
Configuration name: 3d_32x160x128_b10
 {'data_identifier': 'nnUNetPlans_3d_fullres', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 10, 'patch_size': [32, 160, 128], 'median_image_size_in_voxels': [375.0, 498.0, 498.0], 'spacing': [1.0, 1.0, 1.0], 'normalization_schemes': ['ZScoreNormalization'], 'use_mask_for_norm': [False], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'architecture': {'network_class_name': 'dynamic_network_architectures.architectures.unet.PlainConvUNet', 'arch_kwargs': {'n_stages': 6, 'features_per_stage': [32, 64, 128, 256, 320, 320], 'conv_op': 'torch.nn.modules.conv.Conv3d', 'kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]], 'strides': [[1, 1, 1], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [1, 2, 2]], 'n_conv_per_stage': [2, 2, 2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2, 2, 2], 'conv_bias': True, 'norm_op': 'torch.nn.modules.instancenorm.InstanceNorm3d', 'norm_op_kwargs': {'eps': 1e-05, 'affine': True}, 'dropout_op': None, 'dropout_op_kwargs': None, 'nonlin': 'torch.nn.LeakyReLU', 'nonlin_kwargs': {'inplace': True}, 'deep_supervision': True}, '_kw_requires_import': ['conv_op', 'norm_op', 'dropout_op', 'nonlin']}, 'batch_dice': True, 'inherits_from': '3d_fullres'} 

These are the global plan.json settings:
 {'dataset_name': 'Dataset309_Sohee_Calcium_OCT_CrossValidation', 'plans_name': 'nnUNetPlans', 'original_median_spacing_after_transp': [1.0, 1.0, 1.0], 'original_median_shape_after_transp': [375, 498, 498], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 0.4977976381778717, 'mean': 0.13507525622844696, 'median': 0.09599608182907104, 'min': 0.0, 'percentile_00_5': 0.014754901640117168, 'percentile_99_5': 0.4977976381778717, 'std': 0.12025152146816254}}} 

2024-12-17 01:41:44.177870: unpacking dataset...
using pin_memory on device 0

This is the configuration used by this training:
Configuration name: 3d_32x160x128_b10
 {'data_identifier': 'nnUNetPlans_3d_fullres', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 10, 'patch_size': [32, 160, 128], 'median_image_size_in_voxels': [375.0, 498.0, 498.0], 'spacing': [1.0, 1.0, 1.0], 'normalization_schemes': ['ZScoreNormalization'], 'use_mask_for_norm': [False], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'architecture': {'network_class_name': 'dynamic_network_architectures.architectures.unet.PlainConvUNet', 'arch_kwargs': {'n_stages': 6, 'features_per_stage': [32, 64, 128, 256, 320, 320], 'conv_op': 'torch.nn.modules.conv.Conv3d', 'kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]], 'strides': [[1, 1, 1], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [1, 2, 2]], 'n_conv_per_stage': [2, 2, 2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2, 2, 2], 'conv_bias': True, 'norm_op': 'torch.nn.modules.instancenorm.InstanceNorm3d', 'norm_op_kwargs': {'eps': 1e-05, 'affine': True}, 'dropout_op': None, 'dropout_op_kwargs': None, 'nonlin': 'torch.nn.LeakyReLU', 'nonlin_kwargs': {'inplace': True}, 'deep_supervision': True}, '_kw_requires_import': ['conv_op', 'norm_op', 'dropout_op', 'nonlin']}, 'batch_dice': True, 'inherits_from': '3d_fullres'} 

These are the global plan.json settings:
 {'dataset_name': 'Dataset309_Sohee_Calcium_OCT_CrossValidation', 'plans_name': 'nnUNetPlans', 'original_median_spacing_after_transp': [1.0, 1.0, 1.0], 'original_median_shape_after_transp': [375, 498, 498], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 0.4977976381778717, 'mean': 0.13507525622844696, 'median': 0.09599608182907104, 'min': 0.0, 'percentile_00_5': 0.014754901640117168, 'percentile_99_5': 0.4977976381778717, 'std': 0.12025152146816254}}} 

2024-12-17 01:41:44.450045: unpacking dataset...
2024-12-17 01:41:48.660316: unpacking done...
2024-12-17 01:41:49.065528: Unable to plot network architecture: nnUNet_compile is enabled!
2024-12-17 01:41:49.131346: 
2024-12-17 01:41:49.132666: Epoch 0
2024-12-17 01:41:49.133797: Current learning rate: 0.01
2024-12-17 01:44:22.572932: Validation loss improved from 1000.00000 to -0.39223! Patience: 0/50
2024-12-17 01:44:22.574545: train_loss -0.3181
2024-12-17 01:44:22.575863: val_loss -0.3922
2024-12-17 01:44:22.576615: Pseudo dice [0.6911]
2024-12-17 01:44:22.577372: Epoch time: 153.45 s
2024-12-17 01:44:22.578215: Yayy! New best EMA pseudo Dice: 0.6911
2024-12-17 01:44:24.135981: 
2024-12-17 01:44:24.137902: Epoch 1
2024-12-17 01:44:24.138922: Current learning rate: 0.00994
2024-12-17 01:45:51.146776: Validation loss improved from -0.39223 to -0.46379! Patience: 0/50
2024-12-17 01:45:51.147718: train_loss -0.4682
2024-12-17 01:45:51.148520: val_loss -0.4638
2024-12-17 01:45:51.149268: Pseudo dice [0.7415]
2024-12-17 01:45:51.150184: Epoch time: 87.01 s
2024-12-17 01:45:51.150826: Yayy! New best EMA pseudo Dice: 0.6961
2024-12-17 01:45:52.807063: 
2024-12-17 01:45:52.809114: Epoch 2
2024-12-17 01:45:52.810021: Current learning rate: 0.00988
2024-12-17 01:47:20.582774: Validation loss improved from -0.46379 to -0.48949! Patience: 0/50
2024-12-17 01:47:20.583901: train_loss -0.5142
2024-12-17 01:47:20.584822: val_loss -0.4895
2024-12-17 01:47:20.585516: Pseudo dice [0.7446]
2024-12-17 01:47:20.586221: Epoch time: 87.78 s
2024-12-17 01:47:20.586855: Yayy! New best EMA pseudo Dice: 0.701
2024-12-17 01:47:22.279103: 
2024-12-17 01:47:22.280817: Epoch 3
2024-12-17 01:47:22.281473: Current learning rate: 0.00982
2024-12-17 01:48:50.276661: Validation loss improved from -0.48949 to -0.50109! Patience: 0/50
2024-12-17 01:48:50.277870: train_loss -0.5167
2024-12-17 01:48:50.278659: val_loss -0.5011
2024-12-17 01:48:50.279394: Pseudo dice [0.7487]
2024-12-17 01:48:50.280148: Epoch time: 88.0 s
2024-12-17 01:48:50.280745: Yayy! New best EMA pseudo Dice: 0.7058
2024-12-17 01:48:51.966396: 
2024-12-17 01:48:51.967973: Epoch 4
2024-12-17 01:48:51.968976: Current learning rate: 0.00976
2024-12-17 01:50:20.273695: Validation loss did not improve from -0.50109. Patience: 1/50
2024-12-17 01:50:20.274768: train_loss -0.5219
2024-12-17 01:50:20.275778: val_loss -0.4968
2024-12-17 01:50:20.276738: Pseudo dice [0.7511]
2024-12-17 01:50:20.277810: Epoch time: 88.31 s
2024-12-17 01:50:20.685128: Yayy! New best EMA pseudo Dice: 0.7103
2024-12-17 01:50:22.349845: 
2024-12-17 01:50:22.351578: Epoch 5
2024-12-17 01:50:22.352772: Current learning rate: 0.0097
2024-12-17 01:51:50.577429: Validation loss did not improve from -0.50109. Patience: 2/50
2024-12-17 01:51:50.578444: train_loss -0.5507
2024-12-17 01:51:50.579391: val_loss -0.4958
2024-12-17 01:51:50.580300: Pseudo dice [0.7356]
2024-12-17 01:51:50.581144: Epoch time: 88.23 s
2024-12-17 01:51:50.581950: Yayy! New best EMA pseudo Dice: 0.7128
2024-12-17 01:51:52.159203: 
2024-12-17 01:51:52.160728: Epoch 6
2024-12-17 01:51:52.161669: Current learning rate: 0.00964
2024-12-17 01:53:20.296054: Validation loss improved from -0.50109 to -0.51483! Patience: 2/50
2024-12-17 01:53:20.297115: train_loss -0.5677
2024-12-17 01:53:20.298189: val_loss -0.5148
2024-12-17 01:53:20.298957: Pseudo dice [0.7568]
2024-12-17 01:53:20.299749: Epoch time: 88.14 s
2024-12-17 01:53:20.300534: Yayy! New best EMA pseudo Dice: 0.7172
2024-12-17 01:53:21.912812: 
2024-12-17 01:53:21.914555: Epoch 7
2024-12-17 01:53:21.915653: Current learning rate: 0.00958
2024-12-17 01:54:50.044645: Validation loss improved from -0.51483 to -0.52052! Patience: 0/50
2024-12-17 01:54:50.045541: train_loss -0.581
2024-12-17 01:54:50.046619: val_loss -0.5205
2024-12-17 01:54:50.047499: Pseudo dice [0.757]
2024-12-17 01:54:50.048429: Epoch time: 88.13 s
2024-12-17 01:54:50.049328: Yayy! New best EMA pseudo Dice: 0.7212
2024-12-17 01:54:52.326646: 
2024-12-17 01:54:52.328228: Epoch 8
2024-12-17 01:54:52.329215: Current learning rate: 0.00952
2024-12-17 01:56:20.796090: Validation loss did not improve from -0.52052. Patience: 1/50
2024-12-17 01:56:20.797136: train_loss -0.5879
2024-12-17 01:56:20.798090: val_loss -0.5066
2024-12-17 01:56:20.798758: Pseudo dice [0.7467]
2024-12-17 01:56:20.799494: Epoch time: 88.47 s
2024-12-17 01:56:20.800304: Yayy! New best EMA pseudo Dice: 0.7237
2024-12-17 01:56:22.461321: 
2024-12-17 01:56:22.463029: Epoch 9
2024-12-17 01:56:22.464027: Current learning rate: 0.00946
2024-12-17 01:57:50.995858: Validation loss improved from -0.52052 to -0.54061! Patience: 1/50
2024-12-17 01:57:50.997097: train_loss -0.5909
2024-12-17 01:57:50.998068: val_loss -0.5406
2024-12-17 01:57:50.998711: Pseudo dice [0.7738]
2024-12-17 01:57:50.999320: Epoch time: 88.54 s
2024-12-17 01:57:51.376333: Yayy! New best EMA pseudo Dice: 0.7287
2024-12-17 01:57:52.952883: 
2024-12-17 01:57:52.954615: Epoch 10
2024-12-17 01:57:52.955456: Current learning rate: 0.0094
2024-12-17 01:59:21.499331: Validation loss did not improve from -0.54061. Patience: 1/50
2024-12-17 01:59:21.500471: train_loss -0.601
2024-12-17 01:59:21.501347: val_loss -0.4852
2024-12-17 01:59:21.502113: Pseudo dice [0.7399]
2024-12-17 01:59:21.502806: Epoch time: 88.55 s
2024-12-17 01:59:21.503433: Yayy! New best EMA pseudo Dice: 0.7299
2024-12-17 01:59:23.084422: 
2024-12-17 01:59:23.086059: Epoch 11
2024-12-17 01:59:23.086896: Current learning rate: 0.00934
2024-12-17 02:00:51.662472: Validation loss did not improve from -0.54061. Patience: 2/50
2024-12-17 02:00:51.663588: train_loss -0.5969
2024-12-17 02:00:51.664442: val_loss -0.5031
2024-12-17 02:00:51.665274: Pseudo dice [0.7541]
2024-12-17 02:00:51.666460: Epoch time: 88.58 s
2024-12-17 02:00:51.667442: Yayy! New best EMA pseudo Dice: 0.7323
2024-12-17 02:00:53.268200: 
2024-12-17 02:00:53.269875: Epoch 12
2024-12-17 02:00:53.270997: Current learning rate: 0.00928
2024-12-17 02:02:21.770036: Validation loss did not improve from -0.54061. Patience: 3/50
2024-12-17 02:02:21.771335: train_loss -0.6045
2024-12-17 02:02:21.772443: val_loss -0.5385
2024-12-17 02:02:21.773177: Pseudo dice [0.7792]
2024-12-17 02:02:21.773886: Epoch time: 88.5 s
2024-12-17 02:02:21.774724: Yayy! New best EMA pseudo Dice: 0.737
2024-12-17 02:02:23.404781: 
2024-12-17 02:02:23.406295: Epoch 13
2024-12-17 02:02:23.407139: Current learning rate: 0.00922
2024-12-17 02:03:51.940476: Validation loss did not improve from -0.54061. Patience: 4/50
2024-12-17 02:03:51.941809: train_loss -0.6044
2024-12-17 02:03:51.942811: val_loss -0.5384
2024-12-17 02:03:51.943611: Pseudo dice [0.7775]
2024-12-17 02:03:51.944366: Epoch time: 88.54 s
2024-12-17 02:03:51.945073: Yayy! New best EMA pseudo Dice: 0.741
2024-12-17 02:03:53.577055: 
2024-12-17 02:03:53.578949: Epoch 14
2024-12-17 02:03:53.579821: Current learning rate: 0.00916
2024-12-17 02:05:22.161734: Validation loss did not improve from -0.54061. Patience: 5/50
2024-12-17 02:05:22.163007: train_loss -0.6083
2024-12-17 02:05:22.163815: val_loss -0.5126
2024-12-17 02:05:22.164537: Pseudo dice [0.7627]
2024-12-17 02:05:22.165224: Epoch time: 88.59 s
2024-12-17 02:05:22.544878: Yayy! New best EMA pseudo Dice: 0.7432
2024-12-17 02:05:24.196160: 
2024-12-17 02:05:24.197859: Epoch 15
2024-12-17 02:05:24.198574: Current learning rate: 0.0091
2024-12-17 02:06:52.593371: Validation loss did not improve from -0.54061. Patience: 6/50
2024-12-17 02:06:52.594367: train_loss -0.6084
2024-12-17 02:06:52.595393: val_loss -0.5336
2024-12-17 02:06:52.596269: Pseudo dice [0.7612]
2024-12-17 02:06:52.596891: Epoch time: 88.4 s
2024-12-17 02:06:52.597546: Yayy! New best EMA pseudo Dice: 0.745
2024-12-17 02:06:54.324084: 
2024-12-17 02:06:54.325620: Epoch 16
2024-12-17 02:06:54.326370: Current learning rate: 0.00903
2024-12-17 02:08:23.189123: Validation loss did not improve from -0.54061. Patience: 7/50
2024-12-17 02:08:23.190373: train_loss -0.6345
2024-12-17 02:08:23.191423: val_loss -0.5161
2024-12-17 02:08:23.192406: Pseudo dice [0.7641]
2024-12-17 02:08:23.193240: Epoch time: 88.87 s
2024-12-17 02:08:23.194022: Yayy! New best EMA pseudo Dice: 0.7469
2024-12-17 02:08:24.865036: 
2024-12-17 02:08:24.867252: Epoch 17
2024-12-17 02:08:24.868390: Current learning rate: 0.00897
2024-12-17 02:09:53.735782: Validation loss improved from -0.54061 to -0.55303! Patience: 7/50
2024-12-17 02:09:53.736609: train_loss -0.628
2024-12-17 02:09:53.737403: val_loss -0.553
2024-12-17 02:09:53.738223: Pseudo dice [0.7849]
2024-12-17 02:09:53.739013: Epoch time: 88.87 s
2024-12-17 02:09:53.739835: Yayy! New best EMA pseudo Dice: 0.7507
2024-12-17 02:09:55.423181: 
2024-12-17 02:09:55.424957: Epoch 18
2024-12-17 02:09:55.426071: Current learning rate: 0.00891
2024-12-17 02:11:24.317229: Validation loss did not improve from -0.55303. Patience: 1/50
2024-12-17 02:11:24.318319: train_loss -0.6361
2024-12-17 02:11:24.319081: val_loss -0.5307
2024-12-17 02:11:24.319709: Pseudo dice [0.7667]
2024-12-17 02:11:24.320395: Epoch time: 88.9 s
2024-12-17 02:11:24.321061: Yayy! New best EMA pseudo Dice: 0.7523
2024-12-17 02:11:26.476543: 
2024-12-17 02:11:26.478271: Epoch 19
2024-12-17 02:11:26.479039: Current learning rate: 0.00885
2024-12-17 02:12:55.413703: Validation loss did not improve from -0.55303. Patience: 2/50
2024-12-17 02:12:55.414412: train_loss -0.6377
2024-12-17 02:12:55.415317: val_loss -0.5278
2024-12-17 02:12:55.416062: Pseudo dice [0.7673]
2024-12-17 02:12:55.416871: Epoch time: 88.94 s
2024-12-17 02:12:55.801882: Yayy! New best EMA pseudo Dice: 0.7538
2024-12-17 02:12:57.471275: 
2024-12-17 02:12:57.472986: Epoch 20
2024-12-17 02:12:57.473881: Current learning rate: 0.00879
2024-12-17 02:14:26.278016: Validation loss did not improve from -0.55303. Patience: 3/50
2024-12-17 02:14:26.278822: train_loss -0.6391
2024-12-17 02:14:26.279504: val_loss -0.5268
2024-12-17 02:14:26.280100: Pseudo dice [0.7749]
2024-12-17 02:14:26.280854: Epoch time: 88.81 s
2024-12-17 02:14:26.281586: Yayy! New best EMA pseudo Dice: 0.7559
2024-12-17 02:14:28.023870: 
2024-12-17 02:14:28.025369: Epoch 21
2024-12-17 02:14:28.026312: Current learning rate: 0.00873
2024-12-17 02:15:56.887982: Validation loss improved from -0.55303 to -0.55358! Patience: 3/50
2024-12-17 02:15:56.888858: train_loss -0.6399
2024-12-17 02:15:56.889750: val_loss -0.5536
2024-12-17 02:15:56.890552: Pseudo dice [0.7828]
2024-12-17 02:15:56.891220: Epoch time: 88.87 s
2024-12-17 02:15:56.891917: Yayy! New best EMA pseudo Dice: 0.7586
2024-12-17 02:15:58.563928: 
2024-12-17 02:15:58.565198: Epoch 22
2024-12-17 02:15:58.566122: Current learning rate: 0.00867
2024-12-17 02:17:27.468716: Validation loss improved from -0.55358 to -0.57330! Patience: 0/50
2024-12-17 02:17:27.469899: train_loss -0.6415
2024-12-17 02:17:27.471248: val_loss -0.5733
2024-12-17 02:17:27.472357: Pseudo dice [0.7839]
2024-12-17 02:17:27.473452: Epoch time: 88.91 s
2024-12-17 02:17:27.474478: Yayy! New best EMA pseudo Dice: 0.7611
2024-12-17 02:17:29.104172: 
2024-12-17 02:17:29.106326: Epoch 23
2024-12-17 02:17:29.107196: Current learning rate: 0.00861
2024-12-17 02:18:58.046343: Validation loss did not improve from -0.57330. Patience: 1/50
2024-12-17 02:18:58.047614: train_loss -0.648
2024-12-17 02:18:58.048567: val_loss -0.5529
2024-12-17 02:18:58.049322: Pseudo dice [0.7795]
2024-12-17 02:18:58.049949: Epoch time: 88.94 s
2024-12-17 02:18:58.050697: Yayy! New best EMA pseudo Dice: 0.763
2024-12-17 02:18:59.634039: 
2024-12-17 02:18:59.635669: Epoch 24
2024-12-17 02:18:59.636431: Current learning rate: 0.00855
2024-12-17 02:20:28.740513: Validation loss did not improve from -0.57330. Patience: 2/50
2024-12-17 02:20:28.741922: train_loss -0.6573
2024-12-17 02:20:28.743021: val_loss -0.5326
2024-12-17 02:20:28.743674: Pseudo dice [0.7612]
2024-12-17 02:20:28.744422: Epoch time: 89.11 s
2024-12-17 02:20:30.428556: 
2024-12-17 02:20:30.430004: Epoch 25
2024-12-17 02:20:30.430663: Current learning rate: 0.00849
2024-12-17 02:21:59.543597: Validation loss did not improve from -0.57330. Patience: 3/50
2024-12-17 02:21:59.544667: train_loss -0.6573
2024-12-17 02:21:59.545917: val_loss -0.5367
2024-12-17 02:21:59.546775: Pseudo dice [0.7648]
2024-12-17 02:21:59.547590: Epoch time: 89.12 s
2024-12-17 02:21:59.548403: Yayy! New best EMA pseudo Dice: 0.763
2024-12-17 02:22:01.171295: 
2024-12-17 02:22:01.172739: Epoch 26
2024-12-17 02:22:01.173640: Current learning rate: 0.00843
2024-12-17 02:23:30.362608: Validation loss did not improve from -0.57330. Patience: 4/50
2024-12-17 02:23:30.363807: train_loss -0.6631
2024-12-17 02:23:30.364661: val_loss -0.537
2024-12-17 02:23:30.365509: Pseudo dice [0.7625]
2024-12-17 02:23:30.366337: Epoch time: 89.19 s
2024-12-17 02:23:31.605053: 
2024-12-17 02:23:31.606878: Epoch 27
2024-12-17 02:23:31.607932: Current learning rate: 0.00836
2024-12-17 02:25:00.769093: Validation loss did not improve from -0.57330. Patience: 5/50
2024-12-17 02:25:00.769901: train_loss -0.6682
2024-12-17 02:25:00.770750: val_loss -0.5511
2024-12-17 02:25:00.771466: Pseudo dice [0.7738]
2024-12-17 02:25:00.772397: Epoch time: 89.17 s
2024-12-17 02:25:00.773105: Yayy! New best EMA pseudo Dice: 0.764
2024-12-17 02:25:02.420943: 
2024-12-17 02:25:02.422333: Epoch 28
2024-12-17 02:25:02.423271: Current learning rate: 0.0083
2024-12-17 02:26:31.583383: Validation loss did not improve from -0.57330. Patience: 6/50
2024-12-17 02:26:31.584513: train_loss -0.6682
2024-12-17 02:26:31.585395: val_loss -0.5469
2024-12-17 02:26:31.586032: Pseudo dice [0.7716]
2024-12-17 02:26:31.586717: Epoch time: 89.16 s
2024-12-17 02:26:31.587411: Yayy! New best EMA pseudo Dice: 0.7648
2024-12-17 02:26:33.227563: 
2024-12-17 02:26:33.229146: Epoch 29
2024-12-17 02:26:33.230126: Current learning rate: 0.00824
2024-12-17 02:28:02.260379: Validation loss did not improve from -0.57330. Patience: 7/50
2024-12-17 02:28:02.261325: train_loss -0.6669
2024-12-17 02:28:02.262259: val_loss -0.5472
2024-12-17 02:28:02.263003: Pseudo dice [0.7839]
2024-12-17 02:28:02.263712: Epoch time: 89.03 s
2024-12-17 02:28:03.072733: Yayy! New best EMA pseudo Dice: 0.7667
2024-12-17 02:28:04.766534: 
2024-12-17 02:28:04.768690: Epoch 30
2024-12-17 02:28:04.769536: Current learning rate: 0.00818
2024-12-17 02:29:33.576377: Validation loss did not improve from -0.57330. Patience: 8/50
2024-12-17 02:29:33.577305: train_loss -0.6632
2024-12-17 02:29:33.578097: val_loss -0.5524
2024-12-17 02:29:33.578753: Pseudo dice [0.7754]
2024-12-17 02:29:33.579590: Epoch time: 88.81 s
2024-12-17 02:29:33.580170: Yayy! New best EMA pseudo Dice: 0.7676
2024-12-17 02:29:35.192513: 
2024-12-17 02:29:35.194297: Epoch 31
2024-12-17 02:29:35.195218: Current learning rate: 0.00812
2024-12-17 02:31:03.912181: Validation loss did not improve from -0.57330. Patience: 9/50
2024-12-17 02:31:03.913383: train_loss -0.6777
2024-12-17 02:31:03.914296: val_loss -0.5616
2024-12-17 02:31:03.915078: Pseudo dice [0.7803]
2024-12-17 02:31:03.915742: Epoch time: 88.72 s
2024-12-17 02:31:03.916403: Yayy! New best EMA pseudo Dice: 0.7688
2024-12-17 02:31:05.557140: 
2024-12-17 02:31:05.558749: Epoch 32
2024-12-17 02:31:05.559575: Current learning rate: 0.00806
2024-12-17 02:32:34.287220: Validation loss did not improve from -0.57330. Patience: 10/50
2024-12-17 02:32:34.288604: train_loss -0.683
2024-12-17 02:32:34.289705: val_loss -0.5303
2024-12-17 02:32:34.290341: Pseudo dice [0.7663]
2024-12-17 02:32:34.291261: Epoch time: 88.73 s
2024-12-17 02:32:35.579949: 
2024-12-17 02:32:35.581623: Epoch 33
2024-12-17 02:32:35.582422: Current learning rate: 0.008
2024-12-17 02:34:04.402612: Validation loss did not improve from -0.57330. Patience: 11/50
2024-12-17 02:34:04.403579: train_loss -0.6753
2024-12-17 02:34:04.404457: val_loss -0.5537
2024-12-17 02:34:04.405323: Pseudo dice [0.7816]
2024-12-17 02:34:04.406164: Epoch time: 88.82 s
2024-12-17 02:34:04.406942: Yayy! New best EMA pseudo Dice: 0.7699
2024-12-17 02:34:06.050507: 
2024-12-17 02:34:06.051869: Epoch 34
2024-12-17 02:34:06.052684: Current learning rate: 0.00793
2024-12-17 02:35:34.832052: Validation loss did not improve from -0.57330. Patience: 12/50
2024-12-17 02:35:34.833418: train_loss -0.6725
2024-12-17 02:35:34.834306: val_loss -0.5316
2024-12-17 02:35:34.835104: Pseudo dice [0.769]
2024-12-17 02:35:34.835944: Epoch time: 88.78 s
2024-12-17 02:35:36.473815: 
2024-12-17 02:35:36.475477: Epoch 35
2024-12-17 02:35:36.476393: Current learning rate: 0.00787
2024-12-17 02:37:05.350512: Validation loss did not improve from -0.57330. Patience: 13/50
2024-12-17 02:37:05.351535: train_loss -0.6739
2024-12-17 02:37:05.352630: val_loss -0.5204
2024-12-17 02:37:05.353549: Pseudo dice [0.7575]
2024-12-17 02:37:05.354426: Epoch time: 88.88 s
2024-12-17 02:37:06.653522: 
2024-12-17 02:37:06.655221: Epoch 36
2024-12-17 02:37:06.656123: Current learning rate: 0.00781
2024-12-17 02:38:35.475358: Validation loss did not improve from -0.57330. Patience: 14/50
2024-12-17 02:38:35.476593: train_loss -0.6851
2024-12-17 02:38:35.477839: val_loss -0.5208
2024-12-17 02:38:35.478938: Pseudo dice [0.7662]
2024-12-17 02:38:35.479847: Epoch time: 88.82 s
2024-12-17 02:38:36.769064: 
2024-12-17 02:38:36.770741: Epoch 37
2024-12-17 02:38:36.771575: Current learning rate: 0.00775
2024-12-17 02:40:05.644107: Validation loss did not improve from -0.57330. Patience: 15/50
2024-12-17 02:40:05.645380: train_loss -0.6822
2024-12-17 02:40:05.646289: val_loss -0.5225
2024-12-17 02:40:05.647075: Pseudo dice [0.7617]
2024-12-17 02:40:05.648057: Epoch time: 88.88 s
2024-12-17 02:40:07.005545: 
2024-12-17 02:40:07.007221: Epoch 38
2024-12-17 02:40:07.008115: Current learning rate: 0.00769
2024-12-17 02:41:36.021240: Validation loss did not improve from -0.57330. Patience: 16/50
2024-12-17 02:41:36.021873: train_loss -0.6885
2024-12-17 02:41:36.022599: val_loss -0.5622
2024-12-17 02:41:36.023341: Pseudo dice [0.791]
2024-12-17 02:41:36.024084: Epoch time: 89.02 s
2024-12-17 02:41:36.024872: Yayy! New best EMA pseudo Dice: 0.77
2024-12-17 02:41:37.670405: 
2024-12-17 02:41:37.671829: Epoch 39
2024-12-17 02:41:37.672579: Current learning rate: 0.00763
2024-12-17 02:43:06.695825: Validation loss did not improve from -0.57330. Patience: 17/50
2024-12-17 02:43:06.696762: train_loss -0.6851
2024-12-17 02:43:06.697723: val_loss -0.5492
2024-12-17 02:43:06.698447: Pseudo dice [0.7772]
2024-12-17 02:43:06.699179: Epoch time: 89.03 s
2024-12-17 02:43:07.080830: Yayy! New best EMA pseudo Dice: 0.7707
2024-12-17 02:43:09.222811: 
2024-12-17 02:43:09.225173: Epoch 40
2024-12-17 02:43:09.226254: Current learning rate: 0.00756
2024-12-17 02:44:38.203527: Validation loss did not improve from -0.57330. Patience: 18/50
2024-12-17 02:44:38.204711: train_loss -0.6931
2024-12-17 02:44:38.205711: val_loss -0.5117
2024-12-17 02:44:38.206416: Pseudo dice [0.7642]
2024-12-17 02:44:38.207158: Epoch time: 88.98 s
2024-12-17 02:44:39.547933: 
2024-12-17 02:44:39.549779: Epoch 41
2024-12-17 02:44:39.550540: Current learning rate: 0.0075
2024-12-17 02:46:08.691693: Validation loss did not improve from -0.57330. Patience: 19/50
2024-12-17 02:46:08.693372: train_loss -0.6887
2024-12-17 02:46:08.694628: val_loss -0.5113
2024-12-17 02:46:08.695473: Pseudo dice [0.7557]
2024-12-17 02:46:08.696280: Epoch time: 89.15 s
2024-12-17 02:46:09.933128: 
2024-12-17 02:46:09.936090: Epoch 42
2024-12-17 02:46:09.937573: Current learning rate: 0.00744
2024-12-17 02:47:39.238482: Validation loss did not improve from -0.57330. Patience: 20/50
2024-12-17 02:47:39.241085: train_loss -0.6839
2024-12-17 02:47:39.242578: val_loss -0.5627
2024-12-17 02:47:39.243570: Pseudo dice [0.7875]
2024-12-17 02:47:39.244491: Epoch time: 89.31 s
2024-12-17 02:47:40.492120: 
2024-12-17 02:47:40.494467: Epoch 43
2024-12-17 02:47:40.495679: Current learning rate: 0.00738
2024-12-17 02:49:09.534410: Validation loss did not improve from -0.57330. Patience: 21/50
2024-12-17 02:49:09.535526: train_loss -0.6922
2024-12-17 02:49:09.536492: val_loss -0.5519
2024-12-17 02:49:09.537305: Pseudo dice [0.7735]
2024-12-17 02:49:09.538047: Epoch time: 89.04 s
2024-12-17 02:49:09.538685: Yayy! New best EMA pseudo Dice: 0.7708
2024-12-17 02:49:11.176152: 
2024-12-17 02:49:11.178267: Epoch 44
2024-12-17 02:49:11.179190: Current learning rate: 0.00732
2024-12-17 02:50:39.976341: Validation loss did not improve from -0.57330. Patience: 22/50
2024-12-17 02:50:39.977133: train_loss -0.6888
2024-12-17 02:50:39.977950: val_loss -0.5457
2024-12-17 02:50:39.978692: Pseudo dice [0.7769]
2024-12-17 02:50:39.979405: Epoch time: 88.8 s
2024-12-17 02:50:40.355575: Yayy! New best EMA pseudo Dice: 0.7714
2024-12-17 02:50:41.926869: 
2024-12-17 02:50:41.929020: Epoch 45
2024-12-17 02:50:41.929852: Current learning rate: 0.00725
2024-12-17 02:52:10.798179: Validation loss did not improve from -0.57330. Patience: 23/50
2024-12-17 02:52:10.799053: train_loss -0.6962
2024-12-17 02:52:10.800096: val_loss -0.5529
2024-12-17 02:52:10.800803: Pseudo dice [0.7795]
2024-12-17 02:52:10.801524: Epoch time: 88.87 s
2024-12-17 02:52:10.802302: Yayy! New best EMA pseudo Dice: 0.7722
2024-12-17 02:52:12.478759: 
2024-12-17 02:52:12.480832: Epoch 46
2024-12-17 02:52:12.481518: Current learning rate: 0.00719
2024-12-17 02:53:41.298730: Validation loss did not improve from -0.57330. Patience: 24/50
2024-12-17 02:53:41.300052: train_loss -0.7065
2024-12-17 02:53:41.301041: val_loss -0.5589
2024-12-17 02:53:41.301697: Pseudo dice [0.7755]
2024-12-17 02:53:41.302476: Epoch time: 88.82 s
2024-12-17 02:53:41.303240: Yayy! New best EMA pseudo Dice: 0.7726
2024-12-17 02:53:42.942008: 
2024-12-17 02:53:42.943994: Epoch 47
2024-12-17 02:53:42.944809: Current learning rate: 0.00713
2024-12-17 02:55:11.718437: Validation loss did not improve from -0.57330. Patience: 25/50
2024-12-17 02:55:11.719479: train_loss -0.7117
2024-12-17 02:55:11.720419: val_loss -0.5464
2024-12-17 02:55:11.721087: Pseudo dice [0.7648]
2024-12-17 02:55:11.721720: Epoch time: 88.78 s
2024-12-17 02:55:12.990742: 
2024-12-17 02:55:12.992453: Epoch 48
2024-12-17 02:55:12.993130: Current learning rate: 0.00707
2024-12-17 02:56:41.838760: Validation loss did not improve from -0.57330. Patience: 26/50
2024-12-17 02:56:41.839870: train_loss -0.7083
2024-12-17 02:56:41.840652: val_loss -0.5394
2024-12-17 02:56:41.841458: Pseudo dice [0.7647]
2024-12-17 02:56:41.842197: Epoch time: 88.85 s
2024-12-17 02:56:43.107244: 
2024-12-17 02:56:43.109315: Epoch 49
2024-12-17 02:56:43.110116: Current learning rate: 0.007
2024-12-17 02:58:12.009700: Validation loss did not improve from -0.57330. Patience: 27/50
2024-12-17 02:58:12.010724: train_loss -0.6951
2024-12-17 02:58:12.011638: val_loss -0.5111
2024-12-17 02:58:12.012316: Pseudo dice [0.754]
2024-12-17 02:58:12.013097: Epoch time: 88.9 s
2024-12-17 02:58:14.353524: 
2024-12-17 02:58:14.355399: Epoch 50
2024-12-17 02:58:14.356220: Current learning rate: 0.00694
2024-12-17 02:59:43.233447: Validation loss did not improve from -0.57330. Patience: 28/50
2024-12-17 02:59:43.234516: train_loss -0.7007
2024-12-17 02:59:43.235543: val_loss -0.5413
2024-12-17 02:59:43.236226: Pseudo dice [0.7776]
2024-12-17 02:59:43.236997: Epoch time: 88.88 s
2024-12-17 02:59:44.538500: 
2024-12-17 02:59:44.539729: Epoch 51
2024-12-17 02:59:44.540427: Current learning rate: 0.00688
2024-12-17 03:01:13.547063: Validation loss did not improve from -0.57330. Patience: 29/50
2024-12-17 03:01:13.548138: train_loss -0.7091
2024-12-17 03:01:13.549139: val_loss -0.5672
2024-12-17 03:01:13.550091: Pseudo dice [0.7822]
2024-12-17 03:01:13.550930: Epoch time: 89.01 s
2024-12-17 03:01:14.808924: 
2024-12-17 03:01:14.810726: Epoch 52
2024-12-17 03:01:14.811636: Current learning rate: 0.00682
2024-12-17 03:02:43.959362: Validation loss did not improve from -0.57330. Patience: 30/50
2024-12-17 03:02:43.960610: train_loss -0.7105
2024-12-17 03:02:43.961765: val_loss -0.55
2024-12-17 03:02:43.962812: Pseudo dice [0.7835]
2024-12-17 03:02:43.963675: Epoch time: 89.15 s
2024-12-17 03:02:43.964538: Yayy! New best EMA pseudo Dice: 0.7726
2024-12-17 03:02:45.606316: 
2024-12-17 03:02:45.608154: Epoch 53
2024-12-17 03:02:45.609080: Current learning rate: 0.00675
2024-12-17 03:04:14.742786: Validation loss did not improve from -0.57330. Patience: 31/50
2024-12-17 03:04:14.743908: train_loss -0.7149
2024-12-17 03:04:14.744748: val_loss -0.5587
2024-12-17 03:04:14.745607: Pseudo dice [0.777]
2024-12-17 03:04:14.746440: Epoch time: 89.14 s
2024-12-17 03:04:14.747166: Yayy! New best EMA pseudo Dice: 0.773
2024-12-17 03:04:16.383105: 
2024-12-17 03:04:16.384783: Epoch 54
2024-12-17 03:04:16.385752: Current learning rate: 0.00669
2024-12-17 03:05:45.607643: Validation loss did not improve from -0.57330. Patience: 32/50
2024-12-17 03:05:45.608728: train_loss -0.7136
2024-12-17 03:05:45.609702: val_loss -0.5388
2024-12-17 03:05:45.610354: Pseudo dice [0.771]
2024-12-17 03:05:45.611117: Epoch time: 89.23 s
2024-12-17 03:05:47.336593: 
2024-12-17 03:05:47.338542: Epoch 55
2024-12-17 03:05:47.339373: Current learning rate: 0.00663
2024-12-17 03:07:16.516820: Validation loss did not improve from -0.57330. Patience: 33/50
2024-12-17 03:07:16.518164: train_loss -0.721
2024-12-17 03:07:16.519048: val_loss -0.5511
2024-12-17 03:07:16.519841: Pseudo dice [0.7823]
2024-12-17 03:07:16.520618: Epoch time: 89.18 s
2024-12-17 03:07:16.521274: Yayy! New best EMA pseudo Dice: 0.7738
2024-12-17 03:07:18.138957: 
2024-12-17 03:07:18.140700: Epoch 56
2024-12-17 03:07:18.141427: Current learning rate: 0.00657
2024-12-17 03:08:47.219307: Validation loss did not improve from -0.57330. Patience: 34/50
2024-12-17 03:08:47.220602: train_loss -0.7169
2024-12-17 03:08:47.221753: val_loss -0.5374
2024-12-17 03:08:47.222576: Pseudo dice [0.7699]
2024-12-17 03:08:47.223586: Epoch time: 89.08 s
2024-12-17 03:08:48.489523: 
2024-12-17 03:08:48.491484: Epoch 57
2024-12-17 03:08:48.492380: Current learning rate: 0.0065
2024-12-17 03:10:17.646796: Validation loss did not improve from -0.57330. Patience: 35/50
2024-12-17 03:10:17.647951: train_loss -0.7166
2024-12-17 03:10:17.648768: val_loss -0.5228
2024-12-17 03:10:17.649397: Pseudo dice [0.7686]
2024-12-17 03:10:17.650178: Epoch time: 89.16 s
2024-12-17 03:10:18.894194: 
2024-12-17 03:10:18.895592: Epoch 58
2024-12-17 03:10:18.896184: Current learning rate: 0.00644
2024-12-17 03:11:48.118641: Validation loss did not improve from -0.57330. Patience: 36/50
2024-12-17 03:11:48.120057: train_loss -0.7187
2024-12-17 03:11:48.121201: val_loss -0.549
2024-12-17 03:11:48.121946: Pseudo dice [0.7815]
2024-12-17 03:11:48.122762: Epoch time: 89.23 s
2024-12-17 03:11:49.441923: 
2024-12-17 03:11:49.444009: Epoch 59
2024-12-17 03:11:49.444861: Current learning rate: 0.00638
2024-12-17 03:13:18.607186: Validation loss did not improve from -0.57330. Patience: 37/50
2024-12-17 03:13:18.608369: train_loss -0.7176
2024-12-17 03:13:18.609223: val_loss -0.5247
2024-12-17 03:13:18.609869: Pseudo dice [0.7676]
2024-12-17 03:13:18.610475: Epoch time: 89.17 s
2024-12-17 03:13:20.278036: 
2024-12-17 03:13:20.279532: Epoch 60
2024-12-17 03:13:20.280240: Current learning rate: 0.00631
2024-12-17 03:14:49.727253: Validation loss did not improve from -0.57330. Patience: 38/50
2024-12-17 03:14:49.728368: train_loss -0.7192
2024-12-17 03:14:49.729149: val_loss -0.5523
2024-12-17 03:14:49.729751: Pseudo dice [0.782]
2024-12-17 03:14:49.730533: Epoch time: 89.45 s
2024-12-17 03:14:49.731269: Yayy! New best EMA pseudo Dice: 0.774
2024-12-17 03:14:51.908766: 
2024-12-17 03:14:51.910563: Epoch 61
2024-12-17 03:14:51.911544: Current learning rate: 0.00625
2024-12-17 03:16:21.280476: Validation loss did not improve from -0.57330. Patience: 39/50
2024-12-17 03:16:21.281543: train_loss -0.7276
2024-12-17 03:16:21.282405: val_loss -0.5433
2024-12-17 03:16:21.282993: Pseudo dice [0.7751]
2024-12-17 03:16:21.283624: Epoch time: 89.37 s
2024-12-17 03:16:21.284233: Yayy! New best EMA pseudo Dice: 0.7741
2024-12-17 03:16:22.914046: 
2024-12-17 03:16:22.916152: Epoch 62
2024-12-17 03:16:22.917396: Current learning rate: 0.00619
2024-12-17 03:17:52.140378: Validation loss did not improve from -0.57330. Patience: 40/50
2024-12-17 03:17:52.141562: train_loss -0.7231
2024-12-17 03:17:52.142792: val_loss -0.5695
2024-12-17 03:17:52.143645: Pseudo dice [0.7902]
2024-12-17 03:17:52.144568: Epoch time: 89.23 s
2024-12-17 03:17:52.145508: Yayy! New best EMA pseudo Dice: 0.7758
2024-12-17 03:17:53.867008: 
2024-12-17 03:17:53.868674: Epoch 63
2024-12-17 03:17:53.869557: Current learning rate: 0.00612
2024-12-17 03:19:22.966766: Validation loss did not improve from -0.57330. Patience: 41/50
2024-12-17 03:19:22.967528: train_loss -0.7239
2024-12-17 03:19:22.968402: val_loss -0.5413
2024-12-17 03:19:22.969207: Pseudo dice [0.7764]
2024-12-17 03:19:22.970096: Epoch time: 89.1 s
2024-12-17 03:19:22.971063: Yayy! New best EMA pseudo Dice: 0.7758
2024-12-17 03:19:24.643476: 
2024-12-17 03:19:24.644984: Epoch 64
2024-12-17 03:19:24.645698: Current learning rate: 0.00606
2024-12-17 03:20:53.610788: Validation loss did not improve from -0.57330. Patience: 42/50
2024-12-17 03:20:53.611959: train_loss -0.7333
2024-12-17 03:20:53.613049: val_loss -0.5632
2024-12-17 03:20:53.613869: Pseudo dice [0.7835]
2024-12-17 03:20:53.614784: Epoch time: 88.97 s
2024-12-17 03:20:54.019392: Yayy! New best EMA pseudo Dice: 0.7766
2024-12-17 03:20:55.664811: 
2024-12-17 03:20:55.666360: Epoch 65
2024-12-17 03:20:55.667163: Current learning rate: 0.006
2024-12-17 03:22:24.414768: Validation loss did not improve from -0.57330. Patience: 43/50
2024-12-17 03:22:24.415877: train_loss -0.7265
2024-12-17 03:22:24.416861: val_loss -0.5339
2024-12-17 03:22:24.417647: Pseudo dice [0.777]
2024-12-17 03:22:24.418572: Epoch time: 88.75 s
2024-12-17 03:22:24.419315: Yayy! New best EMA pseudo Dice: 0.7766
2024-12-17 03:22:26.083536: 
2024-12-17 03:22:26.085526: Epoch 66
2024-12-17 03:22:26.086594: Current learning rate: 0.00593
2024-12-17 03:23:54.792640: Validation loss did not improve from -0.57330. Patience: 44/50
2024-12-17 03:23:54.793372: train_loss -0.7279
2024-12-17 03:23:54.794090: val_loss -0.5301
2024-12-17 03:23:54.794886: Pseudo dice [0.7651]
2024-12-17 03:23:54.795561: Epoch time: 88.71 s
2024-12-17 03:23:56.074325: 
2024-12-17 03:23:56.075721: Epoch 67
2024-12-17 03:23:56.076537: Current learning rate: 0.00587
2024-12-17 03:25:24.826500: Validation loss did not improve from -0.57330. Patience: 45/50
2024-12-17 03:25:24.827863: train_loss -0.7291
2024-12-17 03:25:24.829066: val_loss -0.5425
2024-12-17 03:25:24.830281: Pseudo dice [0.7808]
2024-12-17 03:25:24.831202: Epoch time: 88.75 s
2024-12-17 03:25:26.119403: 
2024-12-17 03:25:26.121485: Epoch 68
2024-12-17 03:25:26.122404: Current learning rate: 0.00581
2024-12-17 03:26:54.936602: Validation loss did not improve from -0.57330. Patience: 46/50
2024-12-17 03:26:54.937970: train_loss -0.7355
2024-12-17 03:26:54.938814: val_loss -0.5294
2024-12-17 03:26:54.939476: Pseudo dice [0.754]
2024-12-17 03:26:54.940151: Epoch time: 88.82 s
2024-12-17 03:26:56.250434: 
2024-12-17 03:26:56.252288: Epoch 69
2024-12-17 03:26:56.253059: Current learning rate: 0.00574
2024-12-17 03:28:25.062309: Validation loss did not improve from -0.57330. Patience: 47/50
2024-12-17 03:28:25.063481: train_loss -0.732
2024-12-17 03:28:25.064525: val_loss -0.5401
2024-12-17 03:28:25.065437: Pseudo dice [0.7716]
2024-12-17 03:28:25.066362: Epoch time: 88.81 s
2024-12-17 03:28:26.733529: 
2024-12-17 03:28:26.735231: Epoch 70
2024-12-17 03:28:26.736061: Current learning rate: 0.00568
2024-12-17 03:29:55.461315: Validation loss did not improve from -0.57330. Patience: 48/50
2024-12-17 03:29:55.462152: train_loss -0.7353
2024-12-17 03:29:55.462883: val_loss -0.5405
2024-12-17 03:29:55.463591: Pseudo dice [0.7721]
2024-12-17 03:29:55.464368: Epoch time: 88.73 s
2024-12-17 03:29:56.781704: 
2024-12-17 03:29:56.783349: Epoch 71
2024-12-17 03:29:56.784187: Current learning rate: 0.00562
2024-12-17 03:31:25.580123: Validation loss did not improve from -0.57330. Patience: 49/50
2024-12-17 03:31:25.581249: train_loss -0.7355
2024-12-17 03:31:25.582431: val_loss -0.5575
2024-12-17 03:31:25.583378: Pseudo dice [0.785]
2024-12-17 03:31:25.584232: Epoch time: 88.8 s
2024-12-17 03:31:27.358721: 
2024-12-17 03:31:27.360654: Epoch 72
2024-12-17 03:31:27.361409: Current learning rate: 0.00555
2024-12-17 03:32:56.213925: Validation loss did not improve from -0.57330. Patience: 50/50
2024-12-17 03:32:56.215251: train_loss -0.7273
2024-12-17 03:32:56.216270: val_loss -0.5151
2024-12-17 03:32:56.217063: Pseudo dice [0.7651]
2024-12-17 03:32:56.217754: Epoch time: 88.86 s
2024-12-17 03:32:57.532627: 
2024-12-17 03:32:57.534714: Epoch 73
2024-12-17 03:32:57.535613: Current learning rate: 0.00549
2024-12-17 03:34:26.576911: Validation loss did not improve from -0.57330. Patience: 51/50
2024-12-17 03:34:26.578079: train_loss -0.7299
2024-12-17 03:34:26.579028: val_loss -0.5364
2024-12-17 03:34:26.579908: Pseudo dice [0.7742]
2024-12-17 03:34:26.580775: Epoch time: 89.05 s
2024-12-17 03:34:27.858657: 
2024-12-17 03:34:27.860587: Epoch 74
2024-12-17 03:34:27.861321: Current learning rate: 0.00542
2024-12-17 03:35:57.081086: Validation loss did not improve from -0.57330. Patience: 52/50
2024-12-17 03:35:57.082299: train_loss -0.7265
2024-12-17 03:35:57.083069: val_loss -0.527
2024-12-17 03:35:57.083738: Pseudo dice [0.7613]
2024-12-17 03:35:57.084485: Epoch time: 89.22 s
2024-12-17 03:35:58.701692: 
2024-12-17 03:35:58.703711: Epoch 75
2024-12-17 03:35:58.704505: Current learning rate: 0.00536
2024-12-17 03:37:27.856445: Validation loss did not improve from -0.57330. Patience: 53/50
2024-12-17 03:37:27.857394: train_loss -0.7351
2024-12-17 03:37:27.858228: val_loss -0.547
2024-12-17 03:37:27.858963: Pseudo dice [0.7776]
2024-12-17 03:37:27.859658: Epoch time: 89.16 s
2024-12-17 03:37:29.174331: 
2024-12-17 03:37:29.176037: Epoch 76
2024-12-17 03:37:29.177020: Current learning rate: 0.00529
2024-12-17 03:38:58.266172: Validation loss did not improve from -0.57330. Patience: 54/50
2024-12-17 03:38:58.267359: train_loss -0.7459
2024-12-17 03:38:58.268492: val_loss -0.555
2024-12-17 03:38:58.269398: Pseudo dice [0.7752]
2024-12-17 03:38:58.270349: Epoch time: 89.09 s
2024-12-17 03:38:59.562435: 
2024-12-17 03:38:59.563923: Epoch 77
2024-12-17 03:38:59.564919: Current learning rate: 0.00523
2024-12-17 03:40:28.741836: Validation loss did not improve from -0.57330. Patience: 55/50
2024-12-17 03:40:28.743007: train_loss -0.7443
2024-12-17 03:40:28.743949: val_loss -0.4959
2024-12-17 03:40:28.744789: Pseudo dice [0.7611]
2024-12-17 03:40:28.745678: Epoch time: 89.18 s
2024-12-17 03:40:30.093234: 
2024-12-17 03:40:30.094778: Epoch 78
2024-12-17 03:40:30.095527: Current learning rate: 0.00517
2024-12-17 03:41:59.279190: Validation loss did not improve from -0.57330. Patience: 56/50
2024-12-17 03:41:59.280335: train_loss -0.7436
2024-12-17 03:41:59.281252: val_loss -0.5084
2024-12-17 03:41:59.282007: Pseudo dice [0.7551]
2024-12-17 03:41:59.282835: Epoch time: 89.19 s
2024-12-17 03:42:00.641139: 
2024-12-17 03:42:00.642970: Epoch 79
2024-12-17 03:42:00.643840: Current learning rate: 0.0051
2024-12-17 03:43:29.788570: Validation loss did not improve from -0.57330. Patience: 57/50
2024-12-17 03:43:29.789709: train_loss -0.7442
2024-12-17 03:43:29.790693: val_loss -0.5546
2024-12-17 03:43:29.791620: Pseudo dice [0.7856]
2024-12-17 03:43:29.792595: Epoch time: 89.15 s
2024-12-17 03:43:31.505818: 
2024-12-17 03:43:31.507560: Epoch 80
2024-12-17 03:43:31.508499: Current learning rate: 0.00504
2024-12-17 03:45:00.618058: Validation loss did not improve from -0.57330. Patience: 58/50
2024-12-17 03:45:00.619120: train_loss -0.7428
2024-12-17 03:45:00.619927: val_loss -0.5008
2024-12-17 03:45:00.620685: Pseudo dice [0.7632]
2024-12-17 03:45:00.621356: Epoch time: 89.11 s
2024-12-17 03:45:01.942700: 
2024-12-17 03:45:01.944770: Epoch 81
2024-12-17 03:45:01.945473: Current learning rate: 0.00497
2024-12-17 03:46:31.333656: Validation loss did not improve from -0.57330. Patience: 59/50
2024-12-17 03:46:31.334697: train_loss -0.7453
2024-12-17 03:46:31.335468: val_loss -0.5276
2024-12-17 03:46:31.336093: Pseudo dice [0.7662]
2024-12-17 03:46:31.336674: Epoch time: 89.39 s
2024-12-17 03:46:32.633019: 
2024-12-17 03:46:32.634583: Epoch 82
2024-12-17 03:46:32.635283: Current learning rate: 0.00491
2024-12-17 03:48:02.042274: Validation loss did not improve from -0.57330. Patience: 60/50
2024-12-17 03:48:02.043358: train_loss -0.7487
2024-12-17 03:48:02.044291: val_loss -0.5537
2024-12-17 03:48:02.045121: Pseudo dice [0.7805]
2024-12-17 03:48:02.045880: Epoch time: 89.41 s
2024-12-17 03:48:03.693468: 
2024-12-17 03:48:03.695553: Epoch 83
2024-12-17 03:48:03.696426: Current learning rate: 0.00484
2024-12-17 03:49:33.123499: Validation loss did not improve from -0.57330. Patience: 61/50
2024-12-17 03:49:33.124762: train_loss -0.7499
2024-12-17 03:49:33.126067: val_loss -0.5473
2024-12-17 03:49:33.126967: Pseudo dice [0.775]
2024-12-17 03:49:33.127901: Epoch time: 89.43 s
2024-12-17 03:49:34.334466: 
2024-12-17 03:49:34.336166: Epoch 84
2024-12-17 03:49:34.337103: Current learning rate: 0.00478
2024-12-17 03:51:03.781438: Validation loss did not improve from -0.57330. Patience: 62/50
2024-12-17 03:51:03.783474: train_loss -0.7471
2024-12-17 03:51:03.785883: val_loss -0.5561
2024-12-17 03:51:03.786782: Pseudo dice [0.7764]
2024-12-17 03:51:03.788333: Epoch time: 89.45 s
2024-12-17 03:51:05.379057: 
2024-12-17 03:51:05.380909: Epoch 85
2024-12-17 03:51:05.381794: Current learning rate: 0.00471
2024-12-17 03:52:34.575622: Validation loss did not improve from -0.57330. Patience: 63/50
2024-12-17 03:52:34.577229: train_loss -0.7536
2024-12-17 03:52:34.578169: val_loss -0.5386
2024-12-17 03:52:34.578808: Pseudo dice [0.7802]
2024-12-17 03:52:34.579728: Epoch time: 89.2 s
2024-12-17 03:52:35.809686: 
2024-12-17 03:52:35.811313: Epoch 86
2024-12-17 03:52:35.812330: Current learning rate: 0.00465
2024-12-17 03:54:04.895455: Validation loss did not improve from -0.57330. Patience: 64/50
2024-12-17 03:54:04.896846: train_loss -0.7516
2024-12-17 03:54:04.897640: val_loss -0.5171
2024-12-17 03:54:04.898306: Pseudo dice [0.7609]
2024-12-17 03:54:04.899084: Epoch time: 89.09 s
2024-12-17 03:54:06.117977: 
2024-12-17 03:54:06.120099: Epoch 87
2024-12-17 03:54:06.120863: Current learning rate: 0.00458
2024-12-17 03:55:35.234422: Validation loss did not improve from -0.57330. Patience: 65/50
2024-12-17 03:55:35.235660: train_loss -0.752
2024-12-17 03:55:35.236496: val_loss -0.5285
2024-12-17 03:55:35.237170: Pseudo dice [0.7727]
2024-12-17 03:55:35.237849: Epoch time: 89.12 s
2024-12-17 03:55:36.444334: 
2024-12-17 03:55:36.445859: Epoch 88
2024-12-17 03:55:36.446627: Current learning rate: 0.00452
2024-12-17 03:57:05.518605: Validation loss did not improve from -0.57330. Patience: 66/50
2024-12-17 03:57:05.519709: train_loss -0.7534
2024-12-17 03:57:05.520594: val_loss -0.5558
2024-12-17 03:57:05.521307: Pseudo dice [0.7821]
2024-12-17 03:57:05.522162: Epoch time: 89.08 s
2024-12-17 03:57:06.747105: 
2024-12-17 03:57:06.748741: Epoch 89
2024-12-17 03:57:06.749798: Current learning rate: 0.00445
2024-12-17 03:58:35.882864: Validation loss did not improve from -0.57330. Patience: 67/50
2024-12-17 03:58:35.883764: train_loss -0.7548
2024-12-17 03:58:35.884517: val_loss -0.5572
2024-12-17 03:58:35.885195: Pseudo dice [0.7844]
2024-12-17 03:58:35.885907: Epoch time: 89.14 s
2024-12-17 03:58:37.493436: 
2024-12-17 03:58:37.494449: Epoch 90
2024-12-17 03:58:37.495177: Current learning rate: 0.00438
2024-12-17 04:00:06.571719: Validation loss did not improve from -0.57330. Patience: 68/50
2024-12-17 04:00:06.573041: train_loss -0.7561
2024-12-17 04:00:06.574033: val_loss -0.5454
2024-12-17 04:00:06.574932: Pseudo dice [0.7818]
2024-12-17 04:00:06.575603: Epoch time: 89.08 s
2024-12-17 04:00:07.803743: 
2024-12-17 04:00:07.805623: Epoch 91
2024-12-17 04:00:07.806298: Current learning rate: 0.00432
2024-12-17 04:01:36.914896: Validation loss did not improve from -0.57330. Patience: 69/50
2024-12-17 04:01:36.916212: train_loss -0.7545
2024-12-17 04:01:36.917427: val_loss -0.5348
2024-12-17 04:01:36.918119: Pseudo dice [0.7766]
2024-12-17 04:01:36.918715: Epoch time: 89.11 s
2024-12-17 04:01:38.139239: 
2024-12-17 04:01:38.141203: Epoch 92
2024-12-17 04:01:38.141976: Current learning rate: 0.00425
2024-12-17 04:03:07.337919: Validation loss did not improve from -0.57330. Patience: 70/50
2024-12-17 04:03:07.339009: train_loss -0.7569
2024-12-17 04:03:07.339970: val_loss -0.4742
2024-12-17 04:03:07.340770: Pseudo dice [0.7513]
2024-12-17 04:03:07.341639: Epoch time: 89.2 s
2024-12-17 04:03:08.609704: 
2024-12-17 04:03:08.611819: Epoch 93
2024-12-17 04:03:08.612742: Current learning rate: 0.00419
2024-12-17 04:04:38.044678: Validation loss did not improve from -0.57330. Patience: 71/50
2024-12-17 04:04:38.046005: train_loss -0.7593
2024-12-17 04:04:38.046776: val_loss -0.5448
2024-12-17 04:04:38.047408: Pseudo dice [0.7776]
2024-12-17 04:04:38.048004: Epoch time: 89.44 s
2024-12-17 04:04:39.265215: 
2024-12-17 04:04:39.267136: Epoch 94
2024-12-17 04:04:39.267893: Current learning rate: 0.00412
2024-12-17 04:06:08.570800: Validation loss improved from -0.57330 to -0.57412! Patience: 71/50
2024-12-17 04:06:08.571926: train_loss -0.7551
2024-12-17 04:06:08.573020: val_loss -0.5741
2024-12-17 04:06:08.573719: Pseudo dice [0.792]
2024-12-17 04:06:08.574413: Epoch time: 89.31 s
2024-12-17 04:06:10.978921: 
2024-12-17 04:06:10.980685: Epoch 95
2024-12-17 04:06:10.981628: Current learning rate: 0.00405
2024-12-17 04:07:40.039495: Validation loss did not improve from -0.57412. Patience: 1/50
2024-12-17 04:07:40.040646: train_loss -0.7579
2024-12-17 04:07:40.041535: val_loss -0.5187
2024-12-17 04:07:40.042264: Pseudo dice [0.7708]
2024-12-17 04:07:40.042874: Epoch time: 89.06 s
2024-12-17 04:07:41.297724: 
2024-12-17 04:07:41.299405: Epoch 96
2024-12-17 04:07:41.300318: Current learning rate: 0.00399
2024-12-17 04:09:10.402493: Validation loss did not improve from -0.57412. Patience: 2/50
2024-12-17 04:09:10.403570: train_loss -0.7603
2024-12-17 04:09:10.404452: val_loss -0.5133
2024-12-17 04:09:10.405191: Pseudo dice [0.761]
2024-12-17 04:09:10.405963: Epoch time: 89.11 s
2024-12-17 04:09:11.668135: 
2024-12-17 04:09:11.669849: Epoch 97
2024-12-17 04:09:11.670650: Current learning rate: 0.00392
2024-12-17 04:10:40.842840: Validation loss did not improve from -0.57412. Patience: 3/50
2024-12-17 04:10:40.844112: train_loss -0.7617
2024-12-17 04:10:40.845045: val_loss -0.5529
2024-12-17 04:10:40.845804: Pseudo dice [0.7845]
2024-12-17 04:10:40.846414: Epoch time: 89.18 s
2024-12-17 04:10:42.118443: 
2024-12-17 04:10:42.120025: Epoch 98
2024-12-17 04:10:42.120838: Current learning rate: 0.00385
2024-12-17 04:12:11.327080: Validation loss did not improve from -0.57412. Patience: 4/50
2024-12-17 04:12:11.328526: train_loss -0.7585
2024-12-17 04:12:11.329309: val_loss -0.5019
2024-12-17 04:12:11.330022: Pseudo dice [0.7606]
2024-12-17 04:12:11.330761: Epoch time: 89.21 s
2024-12-17 04:12:12.564235: 
2024-12-17 04:12:12.565058: Epoch 99
2024-12-17 04:12:12.565838: Current learning rate: 0.00379
2024-12-17 04:13:41.680863: Validation loss did not improve from -0.57412. Patience: 5/50
2024-12-17 04:13:41.682086: train_loss -0.76
2024-12-17 04:13:41.683194: val_loss -0.5069
2024-12-17 04:13:41.683860: Pseudo dice [0.7574]
2024-12-17 04:13:41.684705: Epoch time: 89.12 s
2024-12-17 04:13:43.312208: 
2024-12-17 04:13:43.313724: Epoch 100
2024-12-17 04:13:43.314499: Current learning rate: 0.00372
2024-12-17 04:15:12.387677: Validation loss did not improve from -0.57412. Patience: 6/50
2024-12-17 04:15:12.388537: train_loss -0.7639
2024-12-17 04:15:12.389345: val_loss -0.5506
2024-12-17 04:15:12.390172: Pseudo dice [0.7806]
2024-12-17 04:15:12.390892: Epoch time: 89.08 s
2024-12-17 04:15:13.643111: 
2024-12-17 04:15:13.644852: Epoch 101
2024-12-17 04:15:13.645538: Current learning rate: 0.00365
2024-12-17 04:16:42.691246: Validation loss did not improve from -0.57412. Patience: 7/50
2024-12-17 04:16:42.692269: train_loss -0.7666
2024-12-17 04:16:42.693007: val_loss -0.4994
2024-12-17 04:16:42.693802: Pseudo dice [0.7654]
2024-12-17 04:16:42.694629: Epoch time: 89.05 s
2024-12-17 04:16:43.986154: 
2024-12-17 04:16:43.988616: Epoch 102
2024-12-17 04:16:43.989375: Current learning rate: 0.00359
2024-12-17 04:18:13.001472: Validation loss did not improve from -0.57412. Patience: 8/50
2024-12-17 04:18:13.002792: train_loss -0.7686
2024-12-17 04:18:13.004387: val_loss -0.5494
2024-12-17 04:18:13.005519: Pseudo dice [0.7736]
2024-12-17 04:18:13.006624: Epoch time: 89.02 s
2024-12-17 04:18:14.281729: 
2024-12-17 04:18:14.283620: Epoch 103
2024-12-17 04:18:14.284667: Current learning rate: 0.00352
2024-12-17 04:19:43.589290: Validation loss did not improve from -0.57412. Patience: 9/50
2024-12-17 04:19:43.590497: train_loss -0.7676
2024-12-17 04:19:43.591406: val_loss -0.5439
2024-12-17 04:19:43.592097: Pseudo dice [0.7819]
2024-12-17 04:19:43.592729: Epoch time: 89.31 s
2024-12-17 04:19:44.827197: 
2024-12-17 04:19:44.828851: Epoch 104
2024-12-17 04:19:44.829650: Current learning rate: 0.00345
2024-12-17 04:21:14.220104: Validation loss did not improve from -0.57412. Patience: 10/50
2024-12-17 04:21:14.221005: train_loss -0.7651
2024-12-17 04:21:14.221997: val_loss -0.5391
2024-12-17 04:21:14.222928: Pseudo dice [0.7834]
2024-12-17 04:21:14.223837: Epoch time: 89.39 s
2024-12-17 04:21:15.867417: 
2024-12-17 04:21:15.868800: Epoch 105
2024-12-17 04:21:15.869807: Current learning rate: 0.00338
2024-12-17 04:22:45.129541: Validation loss did not improve from -0.57412. Patience: 11/50
2024-12-17 04:22:45.130860: train_loss -0.7698
2024-12-17 04:22:45.131818: val_loss -0.5308
2024-12-17 04:22:45.132513: Pseudo dice [0.7686]
2024-12-17 04:22:45.133137: Epoch time: 89.26 s
2024-12-17 04:22:46.831968: 
2024-12-17 04:22:46.833617: Epoch 106
2024-12-17 04:22:46.834668: Current learning rate: 0.00332
2024-12-17 04:24:16.204262: Validation loss did not improve from -0.57412. Patience: 12/50
2024-12-17 04:24:16.205316: train_loss -0.7688
2024-12-17 04:24:16.206076: val_loss -0.5355
2024-12-17 04:24:16.206767: Pseudo dice [0.7723]
2024-12-17 04:24:16.207578: Epoch time: 89.37 s
2024-12-17 04:24:17.462213: 
2024-12-17 04:24:17.464206: Epoch 107
2024-12-17 04:24:17.464960: Current learning rate: 0.00325
2024-12-17 04:25:46.761512: Validation loss did not improve from -0.57412. Patience: 13/50
2024-12-17 04:25:46.762770: train_loss -0.7711
2024-12-17 04:25:46.763815: val_loss -0.552
2024-12-17 04:25:46.764712: Pseudo dice [0.7762]
2024-12-17 04:25:46.765842: Epoch time: 89.3 s
2024-12-17 04:25:48.012069: 
2024-12-17 04:25:48.013824: Epoch 108
2024-12-17 04:25:48.014685: Current learning rate: 0.00318
2024-12-17 04:27:17.339668: Validation loss did not improve from -0.57412. Patience: 14/50
2024-12-17 04:27:17.340843: train_loss -0.7688
2024-12-17 04:27:17.341845: val_loss -0.5418
2024-12-17 04:27:17.342630: Pseudo dice [0.777]
2024-12-17 04:27:17.343363: Epoch time: 89.33 s
2024-12-17 04:27:18.619349: 
2024-12-17 04:27:18.620720: Epoch 109
2024-12-17 04:27:18.621529: Current learning rate: 0.00311
2024-12-17 04:28:48.033212: Validation loss did not improve from -0.57412. Patience: 15/50
2024-12-17 04:28:48.033957: train_loss -0.7716
2024-12-17 04:28:48.034647: val_loss -0.5147
2024-12-17 04:28:48.035453: Pseudo dice [0.7678]
2024-12-17 04:28:48.036312: Epoch time: 89.42 s
2024-12-17 04:28:49.696907: 
2024-12-17 04:28:49.698765: Epoch 110
2024-12-17 04:28:49.699665: Current learning rate: 0.00304
2024-12-17 04:30:19.020752: Validation loss did not improve from -0.57412. Patience: 16/50
2024-12-17 04:30:19.021878: train_loss -0.767
2024-12-17 04:30:19.022847: val_loss -0.5096
2024-12-17 04:30:19.023773: Pseudo dice [0.764]
2024-12-17 04:30:19.024618: Epoch time: 89.33 s
2024-12-17 04:30:20.273299: 
2024-12-17 04:30:20.274760: Epoch 111
2024-12-17 04:30:20.275521: Current learning rate: 0.00297
2024-12-17 04:31:49.774888: Validation loss did not improve from -0.57412. Patience: 17/50
2024-12-17 04:31:49.776129: train_loss -0.7677
2024-12-17 04:31:49.777068: val_loss -0.5505
2024-12-17 04:31:49.777807: Pseudo dice [0.7844]
2024-12-17 04:31:49.778569: Epoch time: 89.5 s
2024-12-17 04:31:51.025865: 
2024-12-17 04:31:51.027616: Epoch 112
2024-12-17 04:31:51.028414: Current learning rate: 0.00291
2024-12-17 04:33:20.410077: Validation loss did not improve from -0.57412. Patience: 18/50
2024-12-17 04:33:20.411145: train_loss -0.7704
2024-12-17 04:33:20.412168: val_loss -0.5329
2024-12-17 04:33:20.412926: Pseudo dice [0.7751]
2024-12-17 04:33:20.413774: Epoch time: 89.39 s
2024-12-17 04:33:21.669219: 
2024-12-17 04:33:21.670828: Epoch 113
2024-12-17 04:33:21.671640: Current learning rate: 0.00284
2024-12-17 04:34:51.079770: Validation loss did not improve from -0.57412. Patience: 19/50
2024-12-17 04:34:51.080995: train_loss -0.7729
2024-12-17 04:34:51.082060: val_loss -0.5559
2024-12-17 04:34:51.082786: Pseudo dice [0.7867]
2024-12-17 04:34:51.083613: Epoch time: 89.41 s
2024-12-17 04:34:52.350085: 
2024-12-17 04:34:52.351803: Epoch 114
2024-12-17 04:34:52.352740: Current learning rate: 0.00277
2024-12-17 04:36:21.575991: Validation loss did not improve from -0.57412. Patience: 20/50
2024-12-17 04:36:21.576864: train_loss -0.7696
2024-12-17 04:36:21.577749: val_loss -0.5523
2024-12-17 04:36:21.578500: Pseudo dice [0.7827]
2024-12-17 04:36:21.579131: Epoch time: 89.23 s
2024-12-17 04:36:23.234924: 
2024-12-17 04:36:23.236122: Epoch 115
2024-12-17 04:36:23.236841: Current learning rate: 0.0027
2024-12-17 04:37:52.488485: Validation loss did not improve from -0.57412. Patience: 21/50
2024-12-17 04:37:52.489593: train_loss -0.7684
2024-12-17 04:37:52.490436: val_loss -0.5589
2024-12-17 04:37:52.491093: Pseudo dice [0.7871]
2024-12-17 04:37:52.491946: Epoch time: 89.26 s
2024-12-17 04:37:52.492573: Yayy! New best EMA pseudo Dice: 0.7769
2024-12-17 04:37:54.139107: 
2024-12-17 04:37:54.140637: Epoch 116
2024-12-17 04:37:54.141623: Current learning rate: 0.00263
2024-12-17 04:39:23.323240: Validation loss did not improve from -0.57412. Patience: 22/50
2024-12-17 04:39:23.326478: train_loss -0.7737
2024-12-17 04:39:23.327472: val_loss -0.5498
2024-12-17 04:39:23.328230: Pseudo dice [0.7764]
2024-12-17 04:39:23.329164: Epoch time: 89.19 s
2024-12-17 04:39:25.050612: 
2024-12-17 04:39:25.052396: Epoch 117
2024-12-17 04:39:25.053272: Current learning rate: 0.00256
2024-12-17 04:40:54.028330: Validation loss did not improve from -0.57412. Patience: 23/50
2024-12-17 04:40:54.029480: train_loss -0.773
2024-12-17 04:40:54.030299: val_loss -0.5553
2024-12-17 04:40:54.031105: Pseudo dice [0.7768]
2024-12-17 04:40:54.031851: Epoch time: 88.98 s
2024-12-17 04:40:55.349149: 
2024-12-17 04:40:55.351178: Epoch 118
2024-12-17 04:40:55.351917: Current learning rate: 0.00249
2024-12-17 04:42:24.329414: Validation loss did not improve from -0.57412. Patience: 24/50
2024-12-17 04:42:24.330855: train_loss -0.7743
2024-12-17 04:42:24.332029: val_loss -0.5001
2024-12-17 04:42:24.332888: Pseudo dice [0.7609]
2024-12-17 04:42:24.333685: Epoch time: 88.98 s
2024-12-17 04:42:25.609680: 
2024-12-17 04:42:25.610973: Epoch 119
2024-12-17 04:42:25.611817: Current learning rate: 0.00242
2024-12-17 04:43:54.613146: Validation loss did not improve from -0.57412. Patience: 25/50
2024-12-17 04:43:54.614636: train_loss -0.7705
2024-12-17 04:43:54.615680: val_loss -0.5439
2024-12-17 04:43:54.616508: Pseudo dice [0.7745]
2024-12-17 04:43:54.617257: Epoch time: 89.01 s
2024-12-17 04:43:56.307043: 
2024-12-17 04:43:56.308477: Epoch 120
2024-12-17 04:43:56.309153: Current learning rate: 0.00235
2024-12-17 04:45:25.240386: Validation loss did not improve from -0.57412. Patience: 26/50
2024-12-17 04:45:25.241593: train_loss -0.7776
2024-12-17 04:45:25.242465: val_loss -0.5441
2024-12-17 04:45:25.243090: Pseudo dice [0.7752]
2024-12-17 04:45:25.243759: Epoch time: 88.94 s
2024-12-17 04:45:26.540032: 
2024-12-17 04:45:26.541750: Epoch 121
2024-12-17 04:45:26.542735: Current learning rate: 0.00228
2024-12-17 04:46:55.563683: Validation loss did not improve from -0.57412. Patience: 27/50
2024-12-17 04:46:55.564742: train_loss -0.7777
2024-12-17 04:46:55.565762: val_loss -0.5605
2024-12-17 04:46:55.566532: Pseudo dice [0.7897]
2024-12-17 04:46:55.567396: Epoch time: 89.03 s
2024-12-17 04:46:56.885966: 
2024-12-17 04:46:56.887497: Epoch 122
2024-12-17 04:46:56.888397: Current learning rate: 0.00221
2024-12-17 04:48:26.165919: Validation loss did not improve from -0.57412. Patience: 28/50
2024-12-17 04:48:26.167186: train_loss -0.7805
2024-12-17 04:48:26.168149: val_loss -0.5298
2024-12-17 04:48:26.168790: Pseudo dice [0.7654]
2024-12-17 04:48:26.169416: Epoch time: 89.28 s
2024-12-17 04:48:27.473023: 
2024-12-17 04:48:27.474730: Epoch 123
2024-12-17 04:48:27.475418: Current learning rate: 0.00214
2024-12-17 04:49:56.777761: Validation loss did not improve from -0.57412. Patience: 29/50
2024-12-17 04:49:56.778699: train_loss -0.7775
2024-12-17 04:49:56.779562: val_loss -0.5392
2024-12-17 04:49:56.780366: Pseudo dice [0.7738]
2024-12-17 04:49:56.781193: Epoch time: 89.31 s
2024-12-17 04:49:58.133555: 
2024-12-17 04:49:58.135077: Epoch 124
2024-12-17 04:49:58.135754: Current learning rate: 0.00207
2024-12-17 04:51:27.409282: Validation loss did not improve from -0.57412. Patience: 30/50
2024-12-17 04:51:27.410628: train_loss -0.7805
2024-12-17 04:51:27.411798: val_loss -0.5439
2024-12-17 04:51:27.412828: Pseudo dice [0.7845]
2024-12-17 04:51:27.413807: Epoch time: 89.28 s
2024-12-17 04:51:29.132447: 
2024-12-17 04:51:29.134057: Epoch 125
2024-12-17 04:51:29.134865: Current learning rate: 0.00199
2024-12-17 04:52:58.420184: Validation loss did not improve from -0.57412. Patience: 31/50
2024-12-17 04:52:58.421682: train_loss -0.7812
2024-12-17 04:52:58.422559: val_loss -0.4863
2024-12-17 04:52:58.423220: Pseudo dice [0.7565]
2024-12-17 04:52:58.423889: Epoch time: 89.29 s
2024-12-17 04:52:59.744771: 
2024-12-17 04:52:59.746297: Epoch 126
2024-12-17 04:52:59.747145: Current learning rate: 0.00192
2024-12-17 04:54:29.058940: Validation loss did not improve from -0.57412. Patience: 32/50
2024-12-17 04:54:29.060161: train_loss -0.7802
2024-12-17 04:54:29.061073: val_loss -0.5399
2024-12-17 04:54:29.061660: Pseudo dice [0.7822]
2024-12-17 04:54:29.062306: Epoch time: 89.32 s
2024-12-17 04:54:30.379861: 
2024-12-17 04:54:30.381798: Epoch 127
2024-12-17 04:54:30.382656: Current learning rate: 0.00185
2024-12-17 04:55:59.684788: Validation loss did not improve from -0.57412. Patience: 33/50
2024-12-17 04:55:59.685870: train_loss -0.7834
2024-12-17 04:55:59.686783: val_loss -0.5591
2024-12-17 04:55:59.687627: Pseudo dice [0.778]
2024-12-17 04:55:59.688422: Epoch time: 89.31 s
2024-12-17 04:56:01.480688: 
2024-12-17 04:56:01.483321: Epoch 128
2024-12-17 04:56:01.484601: Current learning rate: 0.00178
2024-12-17 04:57:30.783645: Validation loss did not improve from -0.57412. Patience: 34/50
2024-12-17 04:57:30.784650: train_loss -0.7827
2024-12-17 04:57:30.785588: val_loss -0.521
2024-12-17 04:57:30.786265: Pseudo dice [0.7774]
2024-12-17 04:57:30.787074: Epoch time: 89.3 s
2024-12-17 04:57:32.118143: 
2024-12-17 04:57:32.119921: Epoch 129
2024-12-17 04:57:32.120945: Current learning rate: 0.0017
2024-12-17 04:59:01.409536: Validation loss did not improve from -0.57412. Patience: 35/50
2024-12-17 04:59:01.410878: train_loss -0.7858
2024-12-17 04:59:01.412243: val_loss -0.5312
2024-12-17 04:59:01.413245: Pseudo dice [0.7745]
2024-12-17 04:59:01.414255: Epoch time: 89.29 s
2024-12-17 04:59:03.059858: 
2024-12-17 04:59:03.062157: Epoch 130
2024-12-17 04:59:03.063168: Current learning rate: 0.00163
2024-12-17 05:00:32.412475: Validation loss did not improve from -0.57412. Patience: 36/50
2024-12-17 05:00:32.413356: train_loss -0.7816
2024-12-17 05:00:32.414308: val_loss -0.5315
2024-12-17 05:00:32.414975: Pseudo dice [0.7706]
2024-12-17 05:00:32.415628: Epoch time: 89.35 s
2024-12-17 05:00:33.742676: 
2024-12-17 05:00:33.744818: Epoch 131
2024-12-17 05:00:33.745527: Current learning rate: 0.00156
2024-12-17 05:02:03.312203: Validation loss did not improve from -0.57412. Patience: 37/50
2024-12-17 05:02:03.313409: train_loss -0.7852
2024-12-17 05:02:03.314463: val_loss -0.5148
2024-12-17 05:02:03.315663: Pseudo dice [0.7585]
2024-12-17 05:02:03.316571: Epoch time: 89.57 s
2024-12-17 05:02:04.576548: 
2024-12-17 05:02:04.578748: Epoch 132
2024-12-17 05:02:04.579871: Current learning rate: 0.00148
2024-12-17 05:03:34.177023: Validation loss did not improve from -0.57412. Patience: 38/50
2024-12-17 05:03:34.177972: train_loss -0.7803
2024-12-17 05:03:34.178650: val_loss -0.5141
2024-12-17 05:03:34.179255: Pseudo dice [0.7628]
2024-12-17 05:03:34.179918: Epoch time: 89.6 s
2024-12-17 05:03:35.448443: 
2024-12-17 05:03:35.449623: Epoch 133
2024-12-17 05:03:35.450437: Current learning rate: 0.00141
2024-12-17 05:05:04.951610: Validation loss did not improve from -0.57412. Patience: 39/50
2024-12-17 05:05:04.953172: train_loss -0.7866
2024-12-17 05:05:04.954373: val_loss -0.5115
2024-12-17 05:05:04.955123: Pseudo dice [0.7626]
2024-12-17 05:05:04.955908: Epoch time: 89.51 s
2024-12-17 05:05:06.254447: 
2024-12-17 05:05:06.256729: Epoch 134
2024-12-17 05:05:06.257551: Current learning rate: 0.00133
2024-12-17 05:06:35.712879: Validation loss did not improve from -0.57412. Patience: 40/50
2024-12-17 05:06:35.714044: train_loss -0.7862
2024-12-17 05:06:35.715264: val_loss -0.5196
2024-12-17 05:06:35.716192: Pseudo dice [0.7715]
2024-12-17 05:06:35.717012: Epoch time: 89.46 s
2024-12-17 05:06:37.361107: 
2024-12-17 05:06:37.362721: Epoch 135
2024-12-17 05:06:37.363524: Current learning rate: 0.00126
2024-12-17 05:08:06.733809: Validation loss did not improve from -0.57412. Patience: 41/50
2024-12-17 05:08:06.734661: train_loss -0.7842
2024-12-17 05:08:06.735364: val_loss -0.4934
2024-12-17 05:08:06.736028: Pseudo dice [0.7632]
2024-12-17 05:08:06.736820: Epoch time: 89.37 s
2024-12-17 05:08:08.045193: 
2024-12-17 05:08:08.046675: Epoch 136
2024-12-17 05:08:08.047402: Current learning rate: 0.00118
2024-12-17 05:09:37.489615: Validation loss did not improve from -0.57412. Patience: 42/50
2024-12-17 05:09:37.490598: train_loss -0.7856
2024-12-17 05:09:37.491615: val_loss -0.503
2024-12-17 05:09:37.492385: Pseudo dice [0.7648]
2024-12-17 05:09:37.493002: Epoch time: 89.45 s
2024-12-17 05:09:38.799402: 
2024-12-17 05:09:38.800901: Epoch 137
2024-12-17 05:09:38.801634: Current learning rate: 0.00111
2024-12-17 05:11:08.220278: Validation loss did not improve from -0.57412. Patience: 43/50
2024-12-17 05:11:08.221559: train_loss -0.7849
2024-12-17 05:11:08.222385: val_loss -0.5436
2024-12-17 05:11:08.223261: Pseudo dice [0.7839]
2024-12-17 05:11:08.223955: Epoch time: 89.42 s
2024-12-17 05:11:09.519723: 
2024-12-17 05:11:09.521138: Epoch 138
2024-12-17 05:11:09.521942: Current learning rate: 0.00103
2024-12-17 05:12:38.902280: Validation loss did not improve from -0.57412. Patience: 44/50
2024-12-17 05:12:38.903670: train_loss -0.7872
2024-12-17 05:12:38.904695: val_loss -0.5433
2024-12-17 05:12:38.905343: Pseudo dice [0.7764]
2024-12-17 05:12:38.906093: Epoch time: 89.39 s
2024-12-17 05:12:40.665665: 
2024-12-17 05:12:40.667402: Epoch 139
2024-12-17 05:12:40.668236: Current learning rate: 0.00095
2024-12-17 05:14:10.075814: Validation loss did not improve from -0.57412. Patience: 45/50
2024-12-17 05:14:10.076995: train_loss -0.787
2024-12-17 05:14:10.077832: val_loss -0.5286
2024-12-17 05:14:10.078487: Pseudo dice [0.7708]
2024-12-17 05:14:10.079128: Epoch time: 89.41 s
2024-12-17 05:14:11.752574: 
2024-12-17 05:14:11.754551: Epoch 140
2024-12-17 05:14:11.755299: Current learning rate: 0.00087
2024-12-17 05:15:41.109805: Validation loss did not improve from -0.57412. Patience: 46/50
2024-12-17 05:15:41.110871: train_loss -0.7863
2024-12-17 05:15:41.111795: val_loss -0.5634
2024-12-17 05:15:41.112573: Pseudo dice [0.7893]
2024-12-17 05:15:41.113405: Epoch time: 89.36 s
2024-12-17 05:15:42.402984: 
2024-12-17 05:15:42.404493: Epoch 141
2024-12-17 05:15:42.405306: Current learning rate: 0.00079
2024-12-17 05:17:11.904533: Validation loss did not improve from -0.57412. Patience: 47/50
2024-12-17 05:17:11.905612: train_loss -0.7872
2024-12-17 05:17:11.906908: val_loss -0.5241
2024-12-17 05:17:11.908082: Pseudo dice [0.7756]
2024-12-17 05:17:11.908824: Epoch time: 89.5 s
2024-12-17 05:17:13.225527: 
2024-12-17 05:17:13.227298: Epoch 142
2024-12-17 05:17:13.228133: Current learning rate: 0.00071
2024-12-17 05:18:42.689828: Validation loss did not improve from -0.57412. Patience: 48/50
2024-12-17 05:18:42.691114: train_loss -0.7883
2024-12-17 05:18:42.692058: val_loss -0.5222
2024-12-17 05:18:42.692692: Pseudo dice [0.7765]
2024-12-17 05:18:42.693584: Epoch time: 89.47 s
2024-12-17 05:18:44.017421: 
2024-12-17 05:18:44.018774: Epoch 143
2024-12-17 05:18:44.019418: Current learning rate: 0.00063
2024-12-17 05:20:13.318063: Validation loss did not improve from -0.57412. Patience: 49/50
2024-12-17 05:20:13.319185: train_loss -0.7877
2024-12-17 05:20:13.320233: val_loss -0.5485
2024-12-17 05:20:13.321118: Pseudo dice [0.7809]
2024-12-17 05:20:13.321872: Epoch time: 89.3 s
2024-12-17 05:20:14.656610: 
2024-12-17 05:20:14.658357: Epoch 144
2024-12-17 05:20:14.659274: Current learning rate: 0.00055
2024-12-17 05:21:43.977070: Validation loss improved from -0.57412 to -0.58174! Patience: 49/50
2024-12-17 05:21:43.978241: train_loss -0.784
2024-12-17 05:21:43.979076: val_loss -0.5817
2024-12-17 05:21:43.979694: Pseudo dice [0.7974]
2024-12-17 05:21:43.980448: Epoch time: 89.32 s
2024-12-17 05:21:44.377462: Yayy! New best EMA pseudo Dice: 0.7769
2024-12-17 05:21:45.997977: 
2024-12-17 05:21:45.999629: Epoch 145
2024-12-17 05:21:46.000480: Current learning rate: 0.00047
2024-12-17 05:23:15.247618: Validation loss did not improve from -0.58174. Patience: 1/50
2024-12-17 05:23:15.248603: train_loss -0.7888
2024-12-17 05:23:15.249536: val_loss -0.5704
2024-12-17 05:23:15.250160: Pseudo dice [0.7929]
2024-12-17 05:23:15.250981: Epoch time: 89.25 s
2024-12-17 05:23:15.251796: Yayy! New best EMA pseudo Dice: 0.7785
2024-12-17 05:23:16.977565: 
2024-12-17 05:23:16.978972: Epoch 146
2024-12-17 05:23:16.979855: Current learning rate: 0.00038
2024-12-17 05:24:46.338725: Validation loss did not improve from -0.58174. Patience: 2/50
2024-12-17 05:24:46.339971: train_loss -0.7902
2024-12-17 05:24:46.341215: val_loss -0.5439
2024-12-17 05:24:46.342396: Pseudo dice [0.7778]
2024-12-17 05:24:46.343250: Epoch time: 89.36 s
2024-12-17 05:24:47.640140: 
2024-12-17 05:24:47.641831: Epoch 147
2024-12-17 05:24:47.642768: Current learning rate: 0.0003
2024-12-17 05:26:16.997093: Validation loss did not improve from -0.58174. Patience: 3/50
2024-12-17 05:26:16.998200: train_loss -0.7881
2024-12-17 05:26:16.998897: val_loss -0.5332
2024-12-17 05:26:16.999490: Pseudo dice [0.7797]
2024-12-17 05:26:17.000244: Epoch time: 89.36 s
2024-12-17 05:26:17.000819: Yayy! New best EMA pseudo Dice: 0.7786
2024-12-17 05:26:18.646613: 
2024-12-17 05:26:18.648560: Epoch 148
2024-12-17 05:26:18.649297: Current learning rate: 0.00021
2024-12-17 05:27:48.013343: Validation loss did not improve from -0.58174. Patience: 4/50
2024-12-17 05:27:48.014387: train_loss -0.7907
2024-12-17 05:27:48.015223: val_loss -0.5133
2024-12-17 05:27:48.016100: Pseudo dice [0.7698]
2024-12-17 05:27:48.016826: Epoch time: 89.37 s
2024-12-17 05:27:49.319221: 
2024-12-17 05:27:49.321072: Epoch 149
2024-12-17 05:27:49.321766: Current learning rate: 0.00011
2024-12-17 05:29:18.756182: Validation loss did not improve from -0.58174. Patience: 5/50
2024-12-17 05:29:18.757124: train_loss -0.7902
2024-12-17 05:29:18.758053: val_loss -0.5115
2024-12-17 05:29:18.758668: Pseudo dice [0.7662]
2024-12-17 05:29:18.759344: Epoch time: 89.44 s
2024-12-17 05:29:20.946940: Training done.
2024-12-17 01:41:48.835337: unpacking done...
2024-12-17 01:41:49.065609: Unable to plot network architecture: nnUNet_compile is enabled!
2024-12-17 01:41:49.121617: 
2024-12-17 01:41:49.123729: Epoch 0
2024-12-17 01:41:49.125196: Current learning rate: 0.01
2024-12-17 01:44:22.501584: Validation loss improved from 1000.00000 to -0.43107! Patience: 0/50
2024-12-17 01:44:22.502462: train_loss -0.2983
2024-12-17 01:44:22.503371: val_loss -0.4311
2024-12-17 01:44:22.504203: Pseudo dice [0.675]
2024-12-17 01:44:22.505161: Epoch time: 153.38 s
2024-12-17 01:44:22.505891: Yayy! New best EMA pseudo Dice: 0.675
2024-12-17 01:44:24.095495: 
2024-12-17 01:44:24.097337: Epoch 1
2024-12-17 01:44:24.098387: Current learning rate: 0.00994
2024-12-17 01:45:52.470541: Validation loss improved from -0.43107 to -0.43533! Patience: 0/50
2024-12-17 01:45:52.471621: train_loss -0.4685
2024-12-17 01:45:52.473125: val_loss -0.4353
2024-12-17 01:45:52.474178: Pseudo dice [0.674]
2024-12-17 01:45:52.475088: Epoch time: 88.38 s
2024-12-17 01:45:53.706143: 
2024-12-17 01:45:53.708297: Epoch 2
2024-12-17 01:45:53.709167: Current learning rate: 0.00988
2024-12-17 01:47:22.843917: Validation loss improved from -0.43533 to -0.46681! Patience: 0/50
2024-12-17 01:47:22.844883: train_loss -0.4936
2024-12-17 01:47:22.845730: val_loss -0.4668
2024-12-17 01:47:22.846370: Pseudo dice [0.6918]
2024-12-17 01:47:22.847057: Epoch time: 89.14 s
2024-12-17 01:47:22.847890: Yayy! New best EMA pseudo Dice: 0.6766
2024-12-17 01:47:24.583230: 
2024-12-17 01:47:24.584945: Epoch 3
2024-12-17 01:47:24.585752: Current learning rate: 0.00982
2024-12-17 01:48:53.810834: Validation loss improved from -0.46681 to -0.47132! Patience: 0/50
2024-12-17 01:48:53.812192: train_loss -0.5251
2024-12-17 01:48:53.813841: val_loss -0.4713
2024-12-17 01:48:53.814919: Pseudo dice [0.6984]
2024-12-17 01:48:53.816058: Epoch time: 89.23 s
2024-12-17 01:48:53.817033: Yayy! New best EMA pseudo Dice: 0.6788
2024-12-17 01:48:55.409359: 
2024-12-17 01:48:55.411094: Epoch 4
2024-12-17 01:48:55.412184: Current learning rate: 0.00976
2024-12-17 01:50:24.608848: Validation loss did not improve from -0.47132. Patience: 1/50
2024-12-17 01:50:24.610099: train_loss -0.5264
2024-12-17 01:50:24.610960: val_loss -0.4709
2024-12-17 01:50:24.611733: Pseudo dice [0.6977]
2024-12-17 01:50:24.612553: Epoch time: 89.2 s
2024-12-17 01:50:24.960678: Yayy! New best EMA pseudo Dice: 0.6807
2024-12-17 01:50:26.618530: 
2024-12-17 01:50:26.620011: Epoch 5
2024-12-17 01:50:26.620724: Current learning rate: 0.0097
2024-12-17 01:51:55.778334: Validation loss did not improve from -0.47132. Patience: 2/50
2024-12-17 01:51:55.779605: train_loss -0.5499
2024-12-17 01:51:55.780798: val_loss -0.4454
2024-12-17 01:51:55.781606: Pseudo dice [0.6601]
2024-12-17 01:51:55.782361: Epoch time: 89.16 s
2024-12-17 01:51:57.054499: 
2024-12-17 01:51:57.056243: Epoch 6
2024-12-17 01:51:57.057230: Current learning rate: 0.00964
2024-12-17 01:53:26.223582: Validation loss improved from -0.47132 to -0.47568! Patience: 2/50
2024-12-17 01:53:26.224806: train_loss -0.5628
2024-12-17 01:53:26.226011: val_loss -0.4757
2024-12-17 01:53:26.226797: Pseudo dice [0.6999]
2024-12-17 01:53:26.227569: Epoch time: 89.17 s
2024-12-17 01:53:26.228381: Yayy! New best EMA pseudo Dice: 0.6807
2024-12-17 01:53:27.872421: 
2024-12-17 01:53:27.874351: Epoch 7
2024-12-17 01:53:27.875308: Current learning rate: 0.00958
2024-12-17 01:54:57.105384: Validation loss improved from -0.47568 to -0.49734! Patience: 0/50
2024-12-17 01:54:57.106406: train_loss -0.5597
2024-12-17 01:54:57.107241: val_loss -0.4973
2024-12-17 01:54:57.108057: Pseudo dice [0.71]
2024-12-17 01:54:57.108765: Epoch time: 89.24 s
2024-12-17 01:54:57.109475: Yayy! New best EMA pseudo Dice: 0.6837
2024-12-17 01:54:59.132787: 
2024-12-17 01:54:59.134984: Epoch 8
2024-12-17 01:54:59.135790: Current learning rate: 0.00952
2024-12-17 01:56:28.803185: Validation loss did not improve from -0.49734. Patience: 1/50
2024-12-17 01:56:28.804341: train_loss -0.5728
2024-12-17 01:56:28.805369: val_loss -0.4815
2024-12-17 01:56:28.806172: Pseudo dice [0.7025]
2024-12-17 01:56:28.807044: Epoch time: 89.67 s
2024-12-17 01:56:28.807889: Yayy! New best EMA pseudo Dice: 0.6856
2024-12-17 01:56:30.455766: 
2024-12-17 01:56:30.457635: Epoch 9
2024-12-17 01:56:30.459001: Current learning rate: 0.00946
2024-12-17 01:58:00.029477: Validation loss improved from -0.49734 to -0.51408! Patience: 1/50
2024-12-17 01:58:00.030670: train_loss -0.5777
2024-12-17 01:58:00.031705: val_loss -0.5141
2024-12-17 01:58:00.032549: Pseudo dice [0.7169]
2024-12-17 01:58:00.033489: Epoch time: 89.58 s
2024-12-17 01:58:00.439022: Yayy! New best EMA pseudo Dice: 0.6887
2024-12-17 01:58:01.980018: 
2024-12-17 01:58:01.981723: Epoch 10
2024-12-17 01:58:01.982648: Current learning rate: 0.0094
2024-12-17 01:59:31.693308: Validation loss did not improve from -0.51408. Patience: 1/50
2024-12-17 01:59:31.694208: train_loss -0.5868
2024-12-17 01:59:31.695487: val_loss -0.4684
2024-12-17 01:59:31.696604: Pseudo dice [0.6979]
2024-12-17 01:59:31.697582: Epoch time: 89.72 s
2024-12-17 01:59:31.698625: Yayy! New best EMA pseudo Dice: 0.6896
2024-12-17 01:59:33.295733: 
2024-12-17 01:59:33.297789: Epoch 11
2024-12-17 01:59:33.298968: Current learning rate: 0.00934
2024-12-17 02:01:03.011956: Validation loss improved from -0.51408 to -0.54144! Patience: 1/50
2024-12-17 02:01:03.013073: train_loss -0.595
2024-12-17 02:01:03.014115: val_loss -0.5414
2024-12-17 02:01:03.014812: Pseudo dice [0.7475]
2024-12-17 02:01:03.015661: Epoch time: 89.72 s
2024-12-17 02:01:03.016500: Yayy! New best EMA pseudo Dice: 0.6954
2024-12-17 02:01:04.631404: 
2024-12-17 02:01:04.632832: Epoch 12
2024-12-17 02:01:04.633851: Current learning rate: 0.00928
2024-12-17 02:02:34.367473: Validation loss did not improve from -0.54144. Patience: 1/50
2024-12-17 02:02:34.368670: train_loss -0.6016
2024-12-17 02:02:34.369503: val_loss -0.5208
2024-12-17 02:02:34.370281: Pseudo dice [0.7255]
2024-12-17 02:02:34.370983: Epoch time: 89.74 s
2024-12-17 02:02:34.371648: Yayy! New best EMA pseudo Dice: 0.6984
2024-12-17 02:02:36.010397: 
2024-12-17 02:02:36.012418: Epoch 13
2024-12-17 02:02:36.013290: Current learning rate: 0.00922
2024-12-17 02:04:05.681164: Validation loss did not improve from -0.54144. Patience: 2/50
2024-12-17 02:04:05.682424: train_loss -0.6119
2024-12-17 02:04:05.683428: val_loss -0.5224
2024-12-17 02:04:05.684222: Pseudo dice [0.734]
2024-12-17 02:04:05.685063: Epoch time: 89.67 s
2024-12-17 02:04:05.685888: Yayy! New best EMA pseudo Dice: 0.702
2024-12-17 02:04:07.312685: 
2024-12-17 02:04:07.314573: Epoch 14
2024-12-17 02:04:07.315465: Current learning rate: 0.00916
2024-12-17 02:05:37.004845: Validation loss improved from -0.54144 to -0.54366! Patience: 2/50
2024-12-17 02:05:37.005911: train_loss -0.6128
2024-12-17 02:05:37.006973: val_loss -0.5437
2024-12-17 02:05:37.007893: Pseudo dice [0.7384]
2024-12-17 02:05:37.008609: Epoch time: 89.69 s
2024-12-17 02:05:37.374177: Yayy! New best EMA pseudo Dice: 0.7056
2024-12-17 02:05:38.992482: 
2024-12-17 02:05:38.994182: Epoch 15
2024-12-17 02:05:38.994887: Current learning rate: 0.0091
2024-12-17 02:07:08.769659: Validation loss did not improve from -0.54366. Patience: 1/50
2024-12-17 02:07:08.770941: train_loss -0.6012
2024-12-17 02:07:08.771883: val_loss -0.4879
2024-12-17 02:07:08.772934: Pseudo dice [0.7126]
2024-12-17 02:07:08.774115: Epoch time: 89.78 s
2024-12-17 02:07:08.774811: Yayy! New best EMA pseudo Dice: 0.7063
2024-12-17 02:07:10.425191: 
2024-12-17 02:07:10.426632: Epoch 16
2024-12-17 02:07:10.427355: Current learning rate: 0.00903
2024-12-17 02:08:40.453930: Validation loss did not improve from -0.54366. Patience: 2/50
2024-12-17 02:08:40.455085: train_loss -0.6147
2024-12-17 02:08:40.456119: val_loss -0.5161
2024-12-17 02:08:40.456814: Pseudo dice [0.7258]
2024-12-17 02:08:40.457628: Epoch time: 90.03 s
2024-12-17 02:08:40.458498: Yayy! New best EMA pseudo Dice: 0.7083
2024-12-17 02:08:42.103903: 
2024-12-17 02:08:42.105548: Epoch 17
2024-12-17 02:08:42.106353: Current learning rate: 0.00897
2024-12-17 02:10:12.166696: Validation loss did not improve from -0.54366. Patience: 3/50
2024-12-17 02:10:12.167933: train_loss -0.6053
2024-12-17 02:10:12.168854: val_loss -0.5372
2024-12-17 02:10:12.169641: Pseudo dice [0.7266]
2024-12-17 02:10:12.170417: Epoch time: 90.06 s
2024-12-17 02:10:12.171236: Yayy! New best EMA pseudo Dice: 0.7101
2024-12-17 02:10:13.810704: 
2024-12-17 02:10:13.812399: Epoch 18
2024-12-17 02:10:13.813232: Current learning rate: 0.00891
2024-12-17 02:11:43.816153: Validation loss improved from -0.54366 to -0.55181! Patience: 3/50
2024-12-17 02:11:43.817354: train_loss -0.6319
2024-12-17 02:11:43.818468: val_loss -0.5518
2024-12-17 02:11:43.819283: Pseudo dice [0.7459]
2024-12-17 02:11:43.820082: Epoch time: 90.01 s
2024-12-17 02:11:43.820805: Yayy! New best EMA pseudo Dice: 0.7137
2024-12-17 02:11:45.843156: 
2024-12-17 02:11:45.844879: Epoch 19
2024-12-17 02:11:45.846084: Current learning rate: 0.00885
2024-12-17 02:13:15.805326: Validation loss did not improve from -0.55181. Patience: 1/50
2024-12-17 02:13:15.806553: train_loss -0.6324
2024-12-17 02:13:15.807513: val_loss -0.4781
2024-12-17 02:13:15.808335: Pseudo dice [0.7064]
2024-12-17 02:13:15.809068: Epoch time: 89.96 s
2024-12-17 02:13:17.458463: 
2024-12-17 02:13:17.460303: Epoch 20
2024-12-17 02:13:17.461131: Current learning rate: 0.00879
2024-12-17 02:14:47.434755: Validation loss did not improve from -0.55181. Patience: 2/50
2024-12-17 02:14:47.436186: train_loss -0.6276
2024-12-17 02:14:47.437414: val_loss -0.5341
2024-12-17 02:14:47.438105: Pseudo dice [0.7288]
2024-12-17 02:14:47.438759: Epoch time: 89.98 s
2024-12-17 02:14:47.439465: Yayy! New best EMA pseudo Dice: 0.7145
2024-12-17 02:14:49.144484: 
2024-12-17 02:14:49.146420: Epoch 21
2024-12-17 02:14:49.147313: Current learning rate: 0.00873
2024-12-17 02:16:19.166074: Validation loss improved from -0.55181 to -0.57666! Patience: 2/50
2024-12-17 02:16:19.167384: train_loss -0.6296
2024-12-17 02:16:19.168377: val_loss -0.5767
2024-12-17 02:16:19.169200: Pseudo dice [0.761]
2024-12-17 02:16:19.169962: Epoch time: 90.02 s
2024-12-17 02:16:19.170737: Yayy! New best EMA pseudo Dice: 0.7192
2024-12-17 02:16:20.805347: 
2024-12-17 02:16:20.807358: Epoch 22
2024-12-17 02:16:20.808142: Current learning rate: 0.00867
2024-12-17 02:17:50.847821: Validation loss did not improve from -0.57666. Patience: 1/50
2024-12-17 02:17:50.848630: train_loss -0.6406
2024-12-17 02:17:50.849567: val_loss -0.5437
2024-12-17 02:17:50.850352: Pseudo dice [0.7419]
2024-12-17 02:17:50.851112: Epoch time: 90.04 s
2024-12-17 02:17:50.851922: Yayy! New best EMA pseudo Dice: 0.7214
2024-12-17 02:17:52.484478: 
2024-12-17 02:17:52.486442: Epoch 23
2024-12-17 02:17:52.487545: Current learning rate: 0.00861
2024-12-17 02:19:22.545660: Validation loss improved from -0.57666 to -0.57718! Patience: 1/50
2024-12-17 02:19:22.546873: train_loss -0.6475
2024-12-17 02:19:22.547807: val_loss -0.5772
2024-12-17 02:19:22.548627: Pseudo dice [0.757]
2024-12-17 02:19:22.549359: Epoch time: 90.06 s
2024-12-17 02:19:22.550159: Yayy! New best EMA pseudo Dice: 0.725
2024-12-17 02:19:24.169577: 
2024-12-17 02:19:24.170992: Epoch 24
2024-12-17 02:19:24.171700: Current learning rate: 0.00855
2024-12-17 02:20:54.599876: Validation loss did not improve from -0.57718. Patience: 1/50
2024-12-17 02:20:54.600952: train_loss -0.6406
2024-12-17 02:20:54.602029: val_loss -0.5405
2024-12-17 02:20:54.602806: Pseudo dice [0.7336]
2024-12-17 02:20:54.603490: Epoch time: 90.43 s
2024-12-17 02:20:54.989949: Yayy! New best EMA pseudo Dice: 0.7259
2024-12-17 02:20:56.569736: 
2024-12-17 02:20:56.571094: Epoch 25
2024-12-17 02:20:56.571821: Current learning rate: 0.00849
2024-12-17 02:22:26.790023: Validation loss did not improve from -0.57718. Patience: 2/50
2024-12-17 02:22:26.791493: train_loss -0.6484
2024-12-17 02:22:26.792691: val_loss -0.5493
2024-12-17 02:22:26.793768: Pseudo dice [0.7447]
2024-12-17 02:22:26.794906: Epoch time: 90.22 s
2024-12-17 02:22:26.795937: Yayy! New best EMA pseudo Dice: 0.7277
2024-12-17 02:22:28.393064: 
2024-12-17 02:22:28.394949: Epoch 26
2024-12-17 02:22:28.396194: Current learning rate: 0.00843
2024-12-17 02:23:58.741004: Validation loss did not improve from -0.57718. Patience: 3/50
2024-12-17 02:23:58.742053: train_loss -0.6402
2024-12-17 02:23:58.743059: val_loss -0.5253
2024-12-17 02:23:58.743794: Pseudo dice [0.7329]
2024-12-17 02:23:58.744500: Epoch time: 90.35 s
2024-12-17 02:23:58.745369: Yayy! New best EMA pseudo Dice: 0.7283
2024-12-17 02:24:00.430811: 
2024-12-17 02:24:00.432436: Epoch 27
2024-12-17 02:24:00.433246: Current learning rate: 0.00836
2024-12-17 02:25:30.735005: Validation loss did not improve from -0.57718. Patience: 4/50
2024-12-17 02:25:30.736036: train_loss -0.6499
2024-12-17 02:25:30.736866: val_loss -0.5629
2024-12-17 02:25:30.737633: Pseudo dice [0.7533]
2024-12-17 02:25:30.738459: Epoch time: 90.31 s
2024-12-17 02:25:30.739185: Yayy! New best EMA pseudo Dice: 0.7308
2024-12-17 02:25:32.328994: 
2024-12-17 02:25:32.330097: Epoch 28
2024-12-17 02:25:32.330829: Current learning rate: 0.0083
2024-12-17 02:27:02.644852: Validation loss did not improve from -0.57718. Patience: 5/50
2024-12-17 02:27:02.645798: train_loss -0.6635
2024-12-17 02:27:02.646976: val_loss -0.5749
2024-12-17 02:27:02.647940: Pseudo dice [0.7585]
2024-12-17 02:27:02.648850: Epoch time: 90.32 s
2024-12-17 02:27:02.649716: Yayy! New best EMA pseudo Dice: 0.7335
2024-12-17 02:27:04.304968: 
2024-12-17 02:27:04.306844: Epoch 29
2024-12-17 02:27:04.308011: Current learning rate: 0.00824
2024-12-17 02:28:34.706651: Validation loss did not improve from -0.57718. Patience: 6/50
2024-12-17 02:28:34.707689: train_loss -0.6608
2024-12-17 02:28:34.708575: val_loss -0.5418
2024-12-17 02:28:34.709302: Pseudo dice [0.7401]
2024-12-17 02:28:34.710029: Epoch time: 90.4 s
2024-12-17 02:28:35.097776: Yayy! New best EMA pseudo Dice: 0.7342
2024-12-17 02:28:37.099452: 
2024-12-17 02:28:37.101417: Epoch 30
2024-12-17 02:28:37.102078: Current learning rate: 0.00818
2024-12-17 02:30:07.315496: Validation loss did not improve from -0.57718. Patience: 7/50
2024-12-17 02:30:07.317072: train_loss -0.6648
2024-12-17 02:30:07.318236: val_loss -0.5548
2024-12-17 02:30:07.319277: Pseudo dice [0.7466]
2024-12-17 02:30:07.320361: Epoch time: 90.22 s
2024-12-17 02:30:07.321111: Yayy! New best EMA pseudo Dice: 0.7354
2024-12-17 02:30:08.933186: 
2024-12-17 02:30:08.934673: Epoch 31
2024-12-17 02:30:08.935563: Current learning rate: 0.00812
2024-12-17 02:31:39.223019: Validation loss did not improve from -0.57718. Patience: 8/50
2024-12-17 02:31:39.224440: train_loss -0.6663
2024-12-17 02:31:39.225582: val_loss -0.5606
2024-12-17 02:31:39.226289: Pseudo dice [0.7525]
2024-12-17 02:31:39.227055: Epoch time: 90.29 s
2024-12-17 02:31:39.227727: Yayy! New best EMA pseudo Dice: 0.7371
2024-12-17 02:31:40.863389: 
2024-12-17 02:31:40.865112: Epoch 32
2024-12-17 02:31:40.865983: Current learning rate: 0.00806
2024-12-17 02:33:11.490121: Validation loss did not improve from -0.57718. Patience: 9/50
2024-12-17 02:33:11.491255: train_loss -0.6648
2024-12-17 02:33:11.492255: val_loss -0.5746
2024-12-17 02:33:11.492917: Pseudo dice [0.7571]
2024-12-17 02:33:11.493718: Epoch time: 90.63 s
2024-12-17 02:33:11.494320: Yayy! New best EMA pseudo Dice: 0.7391
2024-12-17 02:33:13.131955: 
2024-12-17 02:33:13.133653: Epoch 33
2024-12-17 02:33:13.134500: Current learning rate: 0.008
2024-12-17 02:34:43.757869: Validation loss did not improve from -0.57718. Patience: 10/50
2024-12-17 02:34:43.758906: train_loss -0.6709
2024-12-17 02:34:43.759774: val_loss -0.5499
2024-12-17 02:34:43.760502: Pseudo dice [0.7375]
2024-12-17 02:34:43.761377: Epoch time: 90.63 s
2024-12-17 02:34:45.064834: 
2024-12-17 02:34:45.066668: Epoch 34
2024-12-17 02:34:45.067672: Current learning rate: 0.00793
2024-12-17 02:36:15.433152: Validation loss did not improve from -0.57718. Patience: 11/50
2024-12-17 02:36:15.434542: train_loss -0.6715
2024-12-17 02:36:15.435749: val_loss -0.5634
2024-12-17 02:36:15.436656: Pseudo dice [0.7541]
2024-12-17 02:36:15.437571: Epoch time: 90.37 s
2024-12-17 02:36:15.818653: Yayy! New best EMA pseudo Dice: 0.7405
2024-12-17 02:36:17.471870: 
2024-12-17 02:36:17.473533: Epoch 35
2024-12-17 02:36:17.474494: Current learning rate: 0.00787
2024-12-17 02:37:47.714880: Validation loss improved from -0.57718 to -0.58253! Patience: 11/50
2024-12-17 02:37:47.715917: train_loss -0.6656
2024-12-17 02:37:47.716687: val_loss -0.5825
2024-12-17 02:37:47.717463: Pseudo dice [0.7553]
2024-12-17 02:37:47.718290: Epoch time: 90.24 s
2024-12-17 02:37:47.719068: Yayy! New best EMA pseudo Dice: 0.742
2024-12-17 02:37:49.370506: 
2024-12-17 02:37:49.372357: Epoch 36
2024-12-17 02:37:49.373273: Current learning rate: 0.00781
2024-12-17 02:39:19.619979: Validation loss did not improve from -0.58253. Patience: 1/50
2024-12-17 02:39:19.621125: train_loss -0.6722
2024-12-17 02:39:19.622011: val_loss -0.5632
2024-12-17 02:39:19.622745: Pseudo dice [0.7449]
2024-12-17 02:39:19.623509: Epoch time: 90.25 s
2024-12-17 02:39:19.624191: Yayy! New best EMA pseudo Dice: 0.7423
2024-12-17 02:39:21.255476: 
2024-12-17 02:39:21.256742: Epoch 37
2024-12-17 02:39:21.257394: Current learning rate: 0.00775
2024-12-17 02:40:51.443712: Validation loss did not improve from -0.58253. Patience: 2/50
2024-12-17 02:40:51.445145: train_loss -0.6788
2024-12-17 02:40:51.446374: val_loss -0.5774
2024-12-17 02:40:51.447345: Pseudo dice [0.7641]
2024-12-17 02:40:51.448191: Epoch time: 90.19 s
2024-12-17 02:40:51.449112: Yayy! New best EMA pseudo Dice: 0.7445
2024-12-17 02:40:53.164871: 
2024-12-17 02:40:53.166659: Epoch 38
2024-12-17 02:40:53.167587: Current learning rate: 0.00769
2024-12-17 02:42:23.413336: Validation loss did not improve from -0.58253. Patience: 3/50
2024-12-17 02:42:23.414420: train_loss -0.6877
2024-12-17 02:42:23.415288: val_loss -0.5706
2024-12-17 02:42:23.415974: Pseudo dice [0.7528]
2024-12-17 02:42:23.416733: Epoch time: 90.25 s
2024-12-17 02:42:23.417377: Yayy! New best EMA pseudo Dice: 0.7453
2024-12-17 02:42:25.089699: 
2024-12-17 02:42:25.091372: Epoch 39
2024-12-17 02:42:25.092200: Current learning rate: 0.00763
2024-12-17 02:43:55.340689: Validation loss did not improve from -0.58253. Patience: 4/50
2024-12-17 02:43:55.341710: train_loss -0.6891
2024-12-17 02:43:55.342653: val_loss -0.5439
2024-12-17 02:43:55.343435: Pseudo dice [0.7392]
2024-12-17 02:43:55.344356: Epoch time: 90.25 s
2024-12-17 02:43:57.417161: 
2024-12-17 02:43:57.418968: Epoch 40
2024-12-17 02:43:57.419754: Current learning rate: 0.00756
2024-12-17 02:45:27.661270: Validation loss did not improve from -0.58253. Patience: 5/50
2024-12-17 02:45:27.662472: train_loss -0.6806
2024-12-17 02:45:27.663363: val_loss -0.5813
2024-12-17 02:45:27.664191: Pseudo dice [0.7627]
2024-12-17 02:45:27.665041: Epoch time: 90.25 s
2024-12-17 02:45:27.665799: Yayy! New best EMA pseudo Dice: 0.7465
2024-12-17 02:45:29.369931: 
2024-12-17 02:45:29.371921: Epoch 41
2024-12-17 02:45:29.373008: Current learning rate: 0.0075
2024-12-17 02:46:59.682348: Validation loss did not improve from -0.58253. Patience: 6/50
2024-12-17 02:46:59.750710: train_loss -0.6814
2024-12-17 02:46:59.751828: val_loss -0.5406
2024-12-17 02:46:59.752577: Pseudo dice [0.7407]
2024-12-17 02:46:59.753296: Epoch time: 90.38 s
2024-12-17 02:47:01.015071: 
2024-12-17 02:47:01.016900: Epoch 42
2024-12-17 02:47:01.017986: Current learning rate: 0.00744
2024-12-17 02:48:31.517083: Validation loss did not improve from -0.58253. Patience: 7/50
2024-12-17 02:48:31.518184: train_loss -0.6904
2024-12-17 02:48:31.519065: val_loss -0.5727
2024-12-17 02:48:31.519867: Pseudo dice [0.7567]
2024-12-17 02:48:31.520694: Epoch time: 90.5 s
2024-12-17 02:48:31.521603: Yayy! New best EMA pseudo Dice: 0.747
2024-12-17 02:48:33.135970: 
2024-12-17 02:48:33.137596: Epoch 43
2024-12-17 02:48:33.138452: Current learning rate: 0.00738
2024-12-17 02:50:03.643601: Validation loss did not improve from -0.58253. Patience: 8/50
2024-12-17 02:50:03.644605: train_loss -0.6916
2024-12-17 02:50:03.645525: val_loss -0.5483
2024-12-17 02:50:03.646264: Pseudo dice [0.7504]
2024-12-17 02:50:03.646937: Epoch time: 90.51 s
2024-12-17 02:50:03.647723: Yayy! New best EMA pseudo Dice: 0.7473
2024-12-17 02:50:05.230546: 
2024-12-17 02:50:05.232377: Epoch 44
2024-12-17 02:50:05.233151: Current learning rate: 0.00732
2024-12-17 02:51:35.450779: Validation loss did not improve from -0.58253. Patience: 9/50
2024-12-17 02:51:35.451822: train_loss -0.6924
2024-12-17 02:51:35.452783: val_loss -0.5443
2024-12-17 02:51:35.453590: Pseudo dice [0.7389]
2024-12-17 02:51:35.454381: Epoch time: 90.22 s
2024-12-17 02:51:37.028519: 
2024-12-17 02:51:37.030180: Epoch 45
2024-12-17 02:51:37.031027: Current learning rate: 0.00725
2024-12-17 02:53:07.312153: Validation loss did not improve from -0.58253. Patience: 10/50
2024-12-17 02:53:07.313109: train_loss -0.6889
2024-12-17 02:53:07.314250: val_loss -0.5177
2024-12-17 02:53:07.315165: Pseudo dice [0.7283]
2024-12-17 02:53:07.316042: Epoch time: 90.29 s
2024-12-17 02:53:08.531491: 
2024-12-17 02:53:08.532555: Epoch 46
2024-12-17 02:53:08.533449: Current learning rate: 0.00719
2024-12-17 02:54:38.902214: Validation loss did not improve from -0.58253. Patience: 11/50
2024-12-17 02:54:38.903526: train_loss -0.6889
2024-12-17 02:54:38.904781: val_loss -0.5748
2024-12-17 02:54:38.905699: Pseudo dice [0.7597]
2024-12-17 02:54:38.906819: Epoch time: 90.37 s
2024-12-17 02:54:40.126203: 
2024-12-17 02:54:40.127750: Epoch 47
2024-12-17 02:54:40.128661: Current learning rate: 0.00713
2024-12-17 02:56:10.397732: Validation loss did not improve from -0.58253. Patience: 12/50
2024-12-17 02:56:10.398638: train_loss -0.6947
2024-12-17 02:56:10.399515: val_loss -0.5689
2024-12-17 02:56:10.400237: Pseudo dice [0.7547]
2024-12-17 02:56:10.401010: Epoch time: 90.27 s
2024-12-17 02:56:11.623351: 
2024-12-17 02:56:11.624924: Epoch 48
2024-12-17 02:56:11.625775: Current learning rate: 0.00707
2024-12-17 02:57:41.878631: Validation loss did not improve from -0.58253. Patience: 13/50
2024-12-17 02:57:41.879996: train_loss -0.6985
2024-12-17 02:57:41.880985: val_loss -0.5521
2024-12-17 02:57:41.881805: Pseudo dice [0.7388]
2024-12-17 02:57:41.882601: Epoch time: 90.26 s
2024-12-17 02:57:43.131155: 
2024-12-17 02:57:43.133193: Epoch 49
2024-12-17 02:57:43.134174: Current learning rate: 0.007
2024-12-17 02:59:13.355551: Validation loss did not improve from -0.58253. Patience: 14/50
2024-12-17 02:59:13.356744: train_loss -0.705
2024-12-17 02:59:13.357778: val_loss -0.5161
2024-12-17 02:59:13.358559: Pseudo dice [0.7303]
2024-12-17 02:59:13.359256: Epoch time: 90.23 s
2024-12-17 02:59:15.055072: 
2024-12-17 02:59:15.056712: Epoch 50
2024-12-17 02:59:15.057516: Current learning rate: 0.00694
2024-12-17 03:00:45.346745: Validation loss did not improve from -0.58253. Patience: 15/50
2024-12-17 03:00:45.347968: train_loss -0.7012
2024-12-17 03:00:45.348774: val_loss -0.5513
2024-12-17 03:00:45.349564: Pseudo dice [0.7511]
2024-12-17 03:00:45.350225: Epoch time: 90.29 s
2024-12-17 03:00:46.937962: 
2024-12-17 03:00:46.939307: Epoch 51
2024-12-17 03:00:46.939994: Current learning rate: 0.00688
2024-12-17 03:02:17.279782: Validation loss did not improve from -0.58253. Patience: 16/50
2024-12-17 03:02:17.280635: train_loss -0.7038
2024-12-17 03:02:17.281542: val_loss -0.5734
2024-12-17 03:02:17.282290: Pseudo dice [0.7612]
2024-12-17 03:02:17.283081: Epoch time: 90.34 s
2024-12-17 03:02:18.530925: 
2024-12-17 03:02:18.532689: Epoch 52
2024-12-17 03:02:18.533587: Current learning rate: 0.00682
2024-12-17 03:03:49.039381: Validation loss did not improve from -0.58253. Patience: 17/50
2024-12-17 03:03:49.040669: train_loss -0.7089
2024-12-17 03:03:49.042027: val_loss -0.5747
2024-12-17 03:03:49.043421: Pseudo dice [0.756]
2024-12-17 03:03:49.044447: Epoch time: 90.51 s
2024-12-17 03:03:49.045415: Yayy! New best EMA pseudo Dice: 0.7478
2024-12-17 03:03:50.701172: 
2024-12-17 03:03:50.702933: Epoch 53
2024-12-17 03:03:50.704329: Current learning rate: 0.00675
2024-12-17 03:05:21.010816: Validation loss did not improve from -0.58253. Patience: 18/50
2024-12-17 03:05:21.011841: train_loss -0.7024
2024-12-17 03:05:21.012834: val_loss -0.5502
2024-12-17 03:05:21.013675: Pseudo dice [0.7416]
2024-12-17 03:05:21.014674: Epoch time: 90.31 s
2024-12-17 03:05:22.269689: 
2024-12-17 03:05:22.271506: Epoch 54
2024-12-17 03:05:22.272486: Current learning rate: 0.00669
2024-12-17 03:06:52.608459: Validation loss did not improve from -0.58253. Patience: 19/50
2024-12-17 03:06:52.609675: train_loss -0.7103
2024-12-17 03:06:52.610798: val_loss -0.5513
2024-12-17 03:06:52.611648: Pseudo dice [0.7444]
2024-12-17 03:06:52.612523: Epoch time: 90.34 s
2024-12-17 03:06:54.224519: 
2024-12-17 03:06:54.226507: Epoch 55
2024-12-17 03:06:54.227354: Current learning rate: 0.00663
2024-12-17 03:08:24.537265: Validation loss did not improve from -0.58253. Patience: 20/50
2024-12-17 03:08:24.538368: train_loss -0.7068
2024-12-17 03:08:24.539209: val_loss -0.5592
2024-12-17 03:08:24.539962: Pseudo dice [0.7506]
2024-12-17 03:08:24.540786: Epoch time: 90.31 s
2024-12-17 03:08:25.735902: 
2024-12-17 03:08:25.738011: Epoch 56
2024-12-17 03:08:25.738783: Current learning rate: 0.00657
2024-12-17 03:09:56.019315: Validation loss did not improve from -0.58253. Patience: 21/50
2024-12-17 03:09:56.020652: train_loss -0.7137
2024-12-17 03:09:56.021872: val_loss -0.5662
2024-12-17 03:09:56.022578: Pseudo dice [0.7604]
2024-12-17 03:09:56.023389: Epoch time: 90.29 s
2024-12-17 03:09:56.024133: Yayy! New best EMA pseudo Dice: 0.7486
2024-12-17 03:09:57.688918: 
2024-12-17 03:09:57.690439: Epoch 57
2024-12-17 03:09:57.691092: Current learning rate: 0.0065
2024-12-17 03:11:27.942583: Validation loss did not improve from -0.58253. Patience: 22/50
2024-12-17 03:11:27.943838: train_loss -0.7117
2024-12-17 03:11:27.944858: val_loss -0.5143
2024-12-17 03:11:27.945644: Pseudo dice [0.7325]
2024-12-17 03:11:27.946330: Epoch time: 90.26 s
2024-12-17 03:11:29.155373: 
2024-12-17 03:11:29.156915: Epoch 58
2024-12-17 03:11:29.157748: Current learning rate: 0.00644
2024-12-17 03:12:59.363883: Validation loss did not improve from -0.58253. Patience: 23/50
2024-12-17 03:12:59.365125: train_loss -0.7136
2024-12-17 03:12:59.366282: val_loss -0.5801
2024-12-17 03:12:59.367074: Pseudo dice [0.7669]
2024-12-17 03:12:59.367728: Epoch time: 90.21 s
2024-12-17 03:12:59.368415: Yayy! New best EMA pseudo Dice: 0.7489
2024-12-17 03:13:01.023454: 
2024-12-17 03:13:01.025126: Epoch 59
2024-12-17 03:13:01.026014: Current learning rate: 0.00638
2024-12-17 03:14:31.191046: Validation loss did not improve from -0.58253. Patience: 24/50
2024-12-17 03:14:31.192104: train_loss -0.7132
2024-12-17 03:14:31.193071: val_loss -0.5694
2024-12-17 03:14:31.194028: Pseudo dice [0.759]
2024-12-17 03:14:31.195033: Epoch time: 90.17 s
2024-12-17 03:14:31.575315: Yayy! New best EMA pseudo Dice: 0.7499
2024-12-17 03:14:33.205426: 
2024-12-17 03:14:33.207331: Epoch 60
2024-12-17 03:14:33.208194: Current learning rate: 0.00631
2024-12-17 03:16:03.563045: Validation loss did not improve from -0.58253. Patience: 25/50
2024-12-17 03:16:03.563772: train_loss -0.7126
2024-12-17 03:16:03.564761: val_loss -0.5458
2024-12-17 03:16:03.565536: Pseudo dice [0.753]
2024-12-17 03:16:03.566433: Epoch time: 90.36 s
2024-12-17 03:16:03.567206: Yayy! New best EMA pseudo Dice: 0.7503
2024-12-17 03:16:05.196070: 
2024-12-17 03:16:05.196994: Epoch 61
2024-12-17 03:16:05.197882: Current learning rate: 0.00625
2024-12-17 03:17:35.718198: Validation loss did not improve from -0.58253. Patience: 26/50
2024-12-17 03:17:35.719077: train_loss -0.7174
2024-12-17 03:17:35.719965: val_loss -0.578
2024-12-17 03:17:35.720784: Pseudo dice [0.7603]
2024-12-17 03:17:35.721769: Epoch time: 90.52 s
2024-12-17 03:17:35.722542: Yayy! New best EMA pseudo Dice: 0.7513
2024-12-17 03:17:37.768188: 
2024-12-17 03:17:37.770036: Epoch 62
2024-12-17 03:17:37.771196: Current learning rate: 0.00619
2024-12-17 03:19:08.027963: Validation loss did not improve from -0.58253. Patience: 27/50
2024-12-17 03:19:08.029037: train_loss -0.7138
2024-12-17 03:19:08.029967: val_loss -0.5544
2024-12-17 03:19:08.030728: Pseudo dice [0.7486]
2024-12-17 03:19:08.031514: Epoch time: 90.26 s
2024-12-17 03:19:09.284435: 
2024-12-17 03:19:09.285939: Epoch 63
2024-12-17 03:19:09.286894: Current learning rate: 0.00612
2024-12-17 03:20:39.413374: Validation loss did not improve from -0.58253. Patience: 28/50
2024-12-17 03:20:39.414629: train_loss -0.7169
2024-12-17 03:20:39.415798: val_loss -0.5678
2024-12-17 03:20:39.416873: Pseudo dice [0.7656]
2024-12-17 03:20:39.417877: Epoch time: 90.13 s
2024-12-17 03:20:39.418639: Yayy! New best EMA pseudo Dice: 0.7525
2024-12-17 03:20:41.022843: 
2024-12-17 03:20:41.024031: Epoch 64
2024-12-17 03:20:41.024857: Current learning rate: 0.00606
2024-12-17 03:22:11.069893: Validation loss did not improve from -0.58253. Patience: 29/50
2024-12-17 03:22:11.070673: train_loss -0.7206
2024-12-17 03:22:11.071568: val_loss -0.5518
2024-12-17 03:22:11.072223: Pseudo dice [0.749]
2024-12-17 03:22:11.072886: Epoch time: 90.05 s
2024-12-17 03:22:12.724851: 
2024-12-17 03:22:12.726223: Epoch 65
2024-12-17 03:22:12.727019: Current learning rate: 0.006
2024-12-17 03:23:42.949182: Validation loss improved from -0.58253 to -0.58569! Patience: 29/50
2024-12-17 03:23:42.950406: train_loss -0.7213
2024-12-17 03:23:42.951102: val_loss -0.5857
2024-12-17 03:23:42.951847: Pseudo dice [0.7645]
2024-12-17 03:23:42.952574: Epoch time: 90.23 s
2024-12-17 03:23:42.953280: Yayy! New best EMA pseudo Dice: 0.7533
2024-12-17 03:23:44.601432: 
2024-12-17 03:23:44.602881: Epoch 66
2024-12-17 03:23:44.603631: Current learning rate: 0.00593
2024-12-17 03:25:14.863671: Validation loss did not improve from -0.58569. Patience: 1/50
2024-12-17 03:25:14.864816: train_loss -0.7241
2024-12-17 03:25:14.865637: val_loss -0.5736
2024-12-17 03:25:14.866535: Pseudo dice [0.7631]
2024-12-17 03:25:14.867260: Epoch time: 90.26 s
2024-12-17 03:25:14.868013: Yayy! New best EMA pseudo Dice: 0.7543
2024-12-17 03:25:16.543235: 
2024-12-17 03:25:16.545053: Epoch 67
2024-12-17 03:25:16.545966: Current learning rate: 0.00587
2024-12-17 03:26:46.735805: Validation loss did not improve from -0.58569. Patience: 2/50
2024-12-17 03:26:46.737033: train_loss -0.7219
2024-12-17 03:26:46.738049: val_loss -0.5557
2024-12-17 03:26:46.738741: Pseudo dice [0.7484]
2024-12-17 03:26:46.739468: Epoch time: 90.19 s
2024-12-17 03:26:47.969940: 
2024-12-17 03:26:47.971357: Epoch 68
2024-12-17 03:26:47.972099: Current learning rate: 0.00581
2024-12-17 03:28:18.184976: Validation loss did not improve from -0.58569. Patience: 3/50
2024-12-17 03:28:18.186016: train_loss -0.7195
2024-12-17 03:28:18.187188: val_loss -0.5638
2024-12-17 03:28:18.188033: Pseudo dice [0.757]
2024-12-17 03:28:18.188750: Epoch time: 90.22 s
2024-12-17 03:28:19.418818: 
2024-12-17 03:28:19.420753: Epoch 69
2024-12-17 03:28:19.421666: Current learning rate: 0.00574
2024-12-17 03:29:49.693546: Validation loss did not improve from -0.58569. Patience: 4/50
2024-12-17 03:29:49.694793: train_loss -0.7262
2024-12-17 03:29:49.695849: val_loss -0.529
2024-12-17 03:29:49.696742: Pseudo dice [0.7322]
2024-12-17 03:29:49.697470: Epoch time: 90.28 s
2024-12-17 03:29:51.322145: 
2024-12-17 03:29:51.323585: Epoch 70
2024-12-17 03:29:51.324403: Current learning rate: 0.00568
2024-12-17 03:31:21.660519: Validation loss did not improve from -0.58569. Patience: 5/50
2024-12-17 03:31:21.661735: train_loss -0.7245
2024-12-17 03:31:21.662580: val_loss -0.5513
2024-12-17 03:31:21.663539: Pseudo dice [0.7471]
2024-12-17 03:31:21.664299: Epoch time: 90.34 s
2024-12-17 03:31:22.939850: 
2024-12-17 03:31:22.941550: Epoch 71
2024-12-17 03:31:22.942527: Current learning rate: 0.00562
2024-12-17 03:32:53.541953: Validation loss did not improve from -0.58569. Patience: 6/50
2024-12-17 03:32:53.543129: train_loss -0.7305
2024-12-17 03:32:53.545356: val_loss -0.5661
2024-12-17 03:32:53.546250: Pseudo dice [0.7599]
2024-12-17 03:32:53.547620: Epoch time: 90.6 s
2024-12-17 03:32:55.228719: 
2024-12-17 03:32:55.230291: Epoch 72
2024-12-17 03:32:55.231183: Current learning rate: 0.00555
2024-12-17 03:34:25.837394: Validation loss did not improve from -0.58569. Patience: 7/50
2024-12-17 03:34:25.840784: train_loss -0.7314
2024-12-17 03:34:25.842136: val_loss -0.5729
2024-12-17 03:34:25.842958: Pseudo dice [0.7582]
2024-12-17 03:34:25.844311: Epoch time: 90.61 s
2024-12-17 03:34:27.127271: 
2024-12-17 03:34:27.128597: Epoch 73
2024-12-17 03:34:27.129335: Current learning rate: 0.00549
2024-12-17 03:35:57.714735: Validation loss did not improve from -0.58569. Patience: 8/50
2024-12-17 03:35:57.715712: train_loss -0.7339
2024-12-17 03:35:57.716612: val_loss -0.5643
2024-12-17 03:35:57.717483: Pseudo dice [0.7487]
2024-12-17 03:35:57.718267: Epoch time: 90.59 s
2024-12-17 03:35:58.986252: 
2024-12-17 03:35:58.988234: Epoch 74
2024-12-17 03:35:58.989332: Current learning rate: 0.00542
2024-12-17 03:37:29.525450: Validation loss did not improve from -0.58569. Patience: 9/50
2024-12-17 03:37:29.526551: train_loss -0.7374
2024-12-17 03:37:29.527879: val_loss -0.5684
2024-12-17 03:37:29.528804: Pseudo dice [0.7521]
2024-12-17 03:37:29.529796: Epoch time: 90.54 s
2024-12-17 03:37:31.181505: 
2024-12-17 03:37:31.183323: Epoch 75
2024-12-17 03:37:31.184265: Current learning rate: 0.00536
2024-12-17 03:39:01.602094: Validation loss did not improve from -0.58569. Patience: 10/50
2024-12-17 03:39:01.603234: train_loss -0.732
2024-12-17 03:39:01.604529: val_loss -0.5422
2024-12-17 03:39:01.605531: Pseudo dice [0.745]
2024-12-17 03:39:01.606324: Epoch time: 90.42 s
2024-12-17 03:39:02.846403: 
2024-12-17 03:39:02.848342: Epoch 76
2024-12-17 03:39:02.849302: Current learning rate: 0.00529
2024-12-17 03:40:33.241181: Validation loss did not improve from -0.58569. Patience: 11/50
2024-12-17 03:40:33.242172: train_loss -0.7313
2024-12-17 03:40:33.243119: val_loss -0.5575
2024-12-17 03:40:33.243843: Pseudo dice [0.7548]
2024-12-17 03:40:33.244964: Epoch time: 90.4 s
2024-12-17 03:40:34.555435: 
2024-12-17 03:40:34.556979: Epoch 77
2024-12-17 03:40:34.557853: Current learning rate: 0.00523
2024-12-17 03:42:04.935545: Validation loss did not improve from -0.58569. Patience: 12/50
2024-12-17 03:42:04.936682: train_loss -0.7324
2024-12-17 03:42:04.937835: val_loss -0.5805
2024-12-17 03:42:04.938595: Pseudo dice [0.7628]
2024-12-17 03:42:04.939421: Epoch time: 90.38 s
2024-12-17 03:42:06.209716: 
2024-12-17 03:42:06.211413: Epoch 78
2024-12-17 03:42:06.212375: Current learning rate: 0.00517
2024-12-17 03:43:36.546844: Validation loss did not improve from -0.58569. Patience: 13/50
2024-12-17 03:43:36.548109: train_loss -0.7301
2024-12-17 03:43:36.548965: val_loss -0.5506
2024-12-17 03:43:36.549782: Pseudo dice [0.7457]
2024-12-17 03:43:36.550605: Epoch time: 90.34 s
2024-12-17 03:43:37.833188: 
2024-12-17 03:43:37.835007: Epoch 79
2024-12-17 03:43:37.836140: Current learning rate: 0.0051
2024-12-17 03:45:08.258737: Validation loss did not improve from -0.58569. Patience: 14/50
2024-12-17 03:45:08.260079: train_loss -0.7364
2024-12-17 03:45:08.261319: val_loss -0.5541
2024-12-17 03:45:08.262021: Pseudo dice [0.7555]
2024-12-17 03:45:08.262761: Epoch time: 90.43 s
2024-12-17 03:45:09.959007: 
2024-12-17 03:45:09.960608: Epoch 80
2024-12-17 03:45:09.961331: Current learning rate: 0.00504
2024-12-17 03:46:40.301520: Validation loss did not improve from -0.58569. Patience: 15/50
2024-12-17 03:46:40.302742: train_loss -0.7417
2024-12-17 03:46:40.303604: val_loss -0.5627
2024-12-17 03:46:40.304410: Pseudo dice [0.7638]
2024-12-17 03:46:40.305191: Epoch time: 90.34 s
2024-12-17 03:46:41.583958: 
2024-12-17 03:46:41.585627: Epoch 81
2024-12-17 03:46:41.586435: Current learning rate: 0.00497
2024-12-17 03:48:11.857027: Validation loss did not improve from -0.58569. Patience: 16/50
2024-12-17 03:48:11.857802: train_loss -0.7382
2024-12-17 03:48:11.858680: val_loss -0.5522
2024-12-17 03:48:11.859346: Pseudo dice [0.7518]
2024-12-17 03:48:11.860081: Epoch time: 90.27 s
2024-12-17 03:48:13.205575: 
2024-12-17 03:48:13.207513: Epoch 82
2024-12-17 03:48:13.208368: Current learning rate: 0.00491
2024-12-17 03:49:43.587075: Validation loss did not improve from -0.58569. Patience: 17/50
2024-12-17 03:49:43.587906: train_loss -0.7343
2024-12-17 03:49:43.588782: val_loss -0.5607
2024-12-17 03:49:43.589576: Pseudo dice [0.7459]
2024-12-17 03:49:43.590341: Epoch time: 90.38 s
2024-12-17 03:49:45.181590: 
2024-12-17 03:49:45.183017: Epoch 83
2024-12-17 03:49:45.183781: Current learning rate: 0.00484
2024-12-17 03:51:15.611683: Validation loss did not improve from -0.58569. Patience: 18/50
2024-12-17 03:51:15.613037: train_loss -0.7362
2024-12-17 03:51:15.614027: val_loss -0.5545
2024-12-17 03:51:15.614791: Pseudo dice [0.7518]
2024-12-17 03:51:15.615573: Epoch time: 90.43 s
2024-12-17 03:51:16.844529: 
2024-12-17 03:51:16.846000: Epoch 84
2024-12-17 03:51:16.846978: Current learning rate: 0.00478
2024-12-17 03:52:47.549116: Validation loss did not improve from -0.58569. Patience: 19/50
2024-12-17 03:52:47.550151: train_loss -0.7419
2024-12-17 03:52:47.551338: val_loss -0.5783
2024-12-17 03:52:47.552161: Pseudo dice [0.7646]
2024-12-17 03:52:47.552974: Epoch time: 90.71 s
2024-12-17 03:52:49.116979: 
2024-12-17 03:52:49.118960: Epoch 85
2024-12-17 03:52:49.119845: Current learning rate: 0.00471
2024-12-17 03:54:19.691939: Validation loss did not improve from -0.58569. Patience: 20/50
2024-12-17 03:54:19.693005: train_loss -0.7391
2024-12-17 03:54:19.693842: val_loss -0.5661
2024-12-17 03:54:19.694720: Pseudo dice [0.7548]
2024-12-17 03:54:19.695560: Epoch time: 90.58 s
2024-12-17 03:54:20.924178: 
2024-12-17 03:54:20.926256: Epoch 86
2024-12-17 03:54:20.927019: Current learning rate: 0.00465
2024-12-17 03:55:50.916352: Validation loss did not improve from -0.58569. Patience: 21/50
2024-12-17 03:55:50.917456: train_loss -0.7463
2024-12-17 03:55:50.918378: val_loss -0.5497
2024-12-17 03:55:50.919082: Pseudo dice [0.7488]
2024-12-17 03:55:50.919887: Epoch time: 89.99 s
2024-12-17 03:55:52.120719: 
2024-12-17 03:55:52.122321: Epoch 87
2024-12-17 03:55:52.123113: Current learning rate: 0.00458
2024-12-17 03:57:22.146266: Validation loss did not improve from -0.58569. Patience: 22/50
2024-12-17 03:57:22.147615: train_loss -0.7437
2024-12-17 03:57:22.148441: val_loss -0.5483
2024-12-17 03:57:22.149213: Pseudo dice [0.7481]
2024-12-17 03:57:22.150119: Epoch time: 90.03 s
2024-12-17 03:57:23.362690: 
2024-12-17 03:57:23.364941: Epoch 88
2024-12-17 03:57:23.365905: Current learning rate: 0.00452
2024-12-17 03:58:53.339815: Validation loss did not improve from -0.58569. Patience: 23/50
2024-12-17 03:58:53.340654: train_loss -0.7444
2024-12-17 03:58:53.341562: val_loss -0.5695
2024-12-17 03:58:53.342377: Pseudo dice [0.7621]
2024-12-17 03:58:53.343289: Epoch time: 89.98 s
2024-12-17 03:58:54.559740: 
2024-12-17 03:58:54.561412: Epoch 89
2024-12-17 03:58:54.562256: Current learning rate: 0.00445
2024-12-17 04:00:24.507998: Validation loss did not improve from -0.58569. Patience: 24/50
2024-12-17 04:00:24.509255: train_loss -0.7434
2024-12-17 04:00:24.510231: val_loss -0.552
2024-12-17 04:00:24.510921: Pseudo dice [0.7605]
2024-12-17 04:00:24.511658: Epoch time: 89.95 s
2024-12-17 04:00:24.892613: Yayy! New best EMA pseudo Dice: 0.7545
2024-12-17 04:00:26.482178: 
2024-12-17 04:00:26.483861: Epoch 90
2024-12-17 04:00:26.485073: Current learning rate: 0.00438
2024-12-17 04:01:56.431484: Validation loss did not improve from -0.58569. Patience: 25/50
2024-12-17 04:01:56.432487: train_loss -0.7433
2024-12-17 04:01:56.433415: val_loss -0.5622
2024-12-17 04:01:56.434375: Pseudo dice [0.7541]
2024-12-17 04:01:56.435317: Epoch time: 89.95 s
2024-12-17 04:01:57.620304: 
2024-12-17 04:01:57.621789: Epoch 91
2024-12-17 04:01:57.622596: Current learning rate: 0.00432
2024-12-17 04:03:27.613132: Validation loss improved from -0.58569 to -0.59455! Patience: 25/50
2024-12-17 04:03:27.614520: train_loss -0.7433
2024-12-17 04:03:27.615840: val_loss -0.5945
2024-12-17 04:03:27.616596: Pseudo dice [0.7714]
2024-12-17 04:03:27.617296: Epoch time: 90.0 s
2024-12-17 04:03:27.618082: Yayy! New best EMA pseudo Dice: 0.7562
2024-12-17 04:03:29.231455: 
2024-12-17 04:03:29.232846: Epoch 92
2024-12-17 04:03:29.233695: Current learning rate: 0.00425
2024-12-17 04:04:59.227107: Validation loss improved from -0.59455 to -0.60681! Patience: 0/50
2024-12-17 04:04:59.228168: train_loss -0.7456
2024-12-17 04:04:59.229255: val_loss -0.6068
2024-12-17 04:04:59.230470: Pseudo dice [0.7772]
2024-12-17 04:04:59.231571: Epoch time: 90.0 s
2024-12-17 04:04:59.232627: Yayy! New best EMA pseudo Dice: 0.7583
2024-12-17 04:05:00.826962: 
2024-12-17 04:05:00.828514: Epoch 93
2024-12-17 04:05:00.829334: Current learning rate: 0.00419
2024-12-17 04:06:30.761130: Validation loss did not improve from -0.60681. Patience: 1/50
2024-12-17 04:06:30.762327: train_loss -0.7516
2024-12-17 04:06:30.763246: val_loss -0.5684
2024-12-17 04:06:30.763985: Pseudo dice [0.7584]
2024-12-17 04:06:30.764625: Epoch time: 89.94 s
2024-12-17 04:06:30.765447: Yayy! New best EMA pseudo Dice: 0.7583
2024-12-17 04:06:32.731143: 
2024-12-17 04:06:32.732753: Epoch 94
2024-12-17 04:06:32.733496: Current learning rate: 0.00412
2024-12-17 04:08:02.872868: Validation loss did not improve from -0.60681. Patience: 2/50
2024-12-17 04:08:02.873992: train_loss -0.7498
2024-12-17 04:08:02.874972: val_loss -0.5649
2024-12-17 04:08:02.875853: Pseudo dice [0.7544]
2024-12-17 04:08:02.876606: Epoch time: 90.14 s
2024-12-17 04:08:04.468566: 
2024-12-17 04:08:04.470039: Epoch 95
2024-12-17 04:08:04.470876: Current learning rate: 0.00405
2024-12-17 04:09:34.743291: Validation loss did not improve from -0.60681. Patience: 3/50
2024-12-17 04:09:34.744380: train_loss -0.7516
2024-12-17 04:09:34.745213: val_loss -0.5717
2024-12-17 04:09:34.745948: Pseudo dice [0.7559]
2024-12-17 04:09:34.746795: Epoch time: 90.28 s
2024-12-17 04:09:35.947701: 
2024-12-17 04:09:35.949277: Epoch 96
2024-12-17 04:09:35.950008: Current learning rate: 0.00399
2024-12-17 04:11:06.262138: Validation loss did not improve from -0.60681. Patience: 4/50
2024-12-17 04:11:06.263565: train_loss -0.7532
2024-12-17 04:11:06.264948: val_loss -0.5357
2024-12-17 04:11:06.265877: Pseudo dice [0.742]
2024-12-17 04:11:06.266767: Epoch time: 90.32 s
2024-12-17 04:11:07.537509: 
2024-12-17 04:11:07.539027: Epoch 97
2024-12-17 04:11:07.539899: Current learning rate: 0.00392
2024-12-17 04:12:37.830099: Validation loss did not improve from -0.60681. Patience: 5/50
2024-12-17 04:12:37.831933: train_loss -0.7541
2024-12-17 04:12:37.832972: val_loss -0.5423
2024-12-17 04:12:37.833723: Pseudo dice [0.7361]
2024-12-17 04:12:37.834513: Epoch time: 90.3 s
2024-12-17 04:12:39.056283: 
2024-12-17 04:12:39.057482: Epoch 98
2024-12-17 04:12:39.058268: Current learning rate: 0.00385
2024-12-17 04:14:09.272258: Validation loss did not improve from -0.60681. Patience: 6/50
2024-12-17 04:14:09.273632: train_loss -0.7531
2024-12-17 04:14:09.274943: val_loss -0.5505
2024-12-17 04:14:09.275741: Pseudo dice [0.7507]
2024-12-17 04:14:09.276612: Epoch time: 90.22 s
2024-12-17 04:14:10.531220: 
2024-12-17 04:14:10.532857: Epoch 99
2024-12-17 04:14:10.533624: Current learning rate: 0.00379
2024-12-17 04:15:40.757460: Validation loss did not improve from -0.60681. Patience: 7/50
2024-12-17 04:15:40.758526: train_loss -0.7572
2024-12-17 04:15:40.759679: val_loss -0.542
2024-12-17 04:15:40.760602: Pseudo dice [0.7416]
2024-12-17 04:15:40.761527: Epoch time: 90.23 s
2024-12-17 04:15:42.377087: 
2024-12-17 04:15:42.379074: Epoch 100
2024-12-17 04:15:42.379931: Current learning rate: 0.00372
2024-12-17 04:17:12.546485: Validation loss did not improve from -0.60681. Patience: 8/50
2024-12-17 04:17:12.547815: train_loss -0.7608
2024-12-17 04:17:12.548829: val_loss -0.5523
2024-12-17 04:17:12.549734: Pseudo dice [0.753]
2024-12-17 04:17:12.550527: Epoch time: 90.17 s
2024-12-17 04:17:13.834191: 
2024-12-17 04:17:13.835711: Epoch 101
2024-12-17 04:17:13.836677: Current learning rate: 0.00365
2024-12-17 04:18:43.980243: Validation loss did not improve from -0.60681. Patience: 9/50
2024-12-17 04:18:43.981207: train_loss -0.7569
2024-12-17 04:18:43.982152: val_loss -0.5708
2024-12-17 04:18:43.982809: Pseudo dice [0.7524]
2024-12-17 04:18:43.983630: Epoch time: 90.15 s
2024-12-17 04:18:45.273232: 
2024-12-17 04:18:45.275024: Epoch 102
2024-12-17 04:18:45.276038: Current learning rate: 0.00359
2024-12-17 04:20:15.616435: Validation loss did not improve from -0.60681. Patience: 10/50
2024-12-17 04:20:15.617788: train_loss -0.7537
2024-12-17 04:20:15.618661: val_loss -0.5535
2024-12-17 04:20:15.619546: Pseudo dice [0.7542]
2024-12-17 04:20:15.620266: Epoch time: 90.35 s
2024-12-17 04:20:16.856072: 
2024-12-17 04:20:16.857404: Epoch 103
2024-12-17 04:20:16.858186: Current learning rate: 0.00352
2024-12-17 04:21:47.247520: Validation loss did not improve from -0.60681. Patience: 11/50
2024-12-17 04:21:47.248407: train_loss -0.7586
2024-12-17 04:21:47.249177: val_loss -0.5756
2024-12-17 04:21:47.249810: Pseudo dice [0.762]
2024-12-17 04:21:47.250534: Epoch time: 90.39 s
2024-12-17 04:21:48.519874: 
2024-12-17 04:21:48.521439: Epoch 104
2024-12-17 04:21:48.522231: Current learning rate: 0.00345
2024-12-17 04:23:19.030012: Validation loss did not improve from -0.60681. Patience: 12/50
2024-12-17 04:23:19.031141: train_loss -0.7535
2024-12-17 04:23:19.032240: val_loss -0.5867
2024-12-17 04:23:19.033156: Pseudo dice [0.7682]
2024-12-17 04:23:19.033988: Epoch time: 90.51 s
2024-12-17 04:23:21.067125: 
2024-12-17 04:23:21.068847: Epoch 105
2024-12-17 04:23:21.069597: Current learning rate: 0.00338
2024-12-17 04:24:51.450458: Validation loss did not improve from -0.60681. Patience: 13/50
2024-12-17 04:24:51.451649: train_loss -0.7574
2024-12-17 04:24:51.452574: val_loss -0.5465
2024-12-17 04:24:51.453247: Pseudo dice [0.7517]
2024-12-17 04:24:51.454083: Epoch time: 90.39 s
2024-12-17 04:24:52.698390: 
2024-12-17 04:24:52.700137: Epoch 106
2024-12-17 04:24:52.700984: Current learning rate: 0.00332
2024-12-17 04:26:23.074498: Validation loss did not improve from -0.60681. Patience: 14/50
2024-12-17 04:26:23.075646: train_loss -0.7585
2024-12-17 04:26:23.076649: val_loss -0.5346
2024-12-17 04:26:23.077378: Pseudo dice [0.7432]
2024-12-17 04:26:23.078138: Epoch time: 90.38 s
2024-12-17 04:26:24.329060: 
2024-12-17 04:26:24.331154: Epoch 107
2024-12-17 04:26:24.331985: Current learning rate: 0.00325
2024-12-17 04:27:54.734544: Validation loss did not improve from -0.60681. Patience: 15/50
2024-12-17 04:27:54.735660: train_loss -0.7643
2024-12-17 04:27:54.736572: val_loss -0.5791
2024-12-17 04:27:54.737346: Pseudo dice [0.7626]
2024-12-17 04:27:54.738097: Epoch time: 90.41 s
2024-12-17 04:27:56.021763: 
2024-12-17 04:27:56.023338: Epoch 108
2024-12-17 04:27:56.024286: Current learning rate: 0.00318
2024-12-17 04:29:26.472582: Validation loss did not improve from -0.60681. Patience: 16/50
2024-12-17 04:29:26.473951: train_loss -0.7614
2024-12-17 04:29:26.475007: val_loss -0.5594
2024-12-17 04:29:26.475765: Pseudo dice [0.7549]
2024-12-17 04:29:26.476453: Epoch time: 90.45 s
2024-12-17 04:29:27.767907: 
2024-12-17 04:29:27.769415: Epoch 109
2024-12-17 04:29:27.770344: Current learning rate: 0.00311
2024-12-17 04:30:58.257567: Validation loss did not improve from -0.60681. Patience: 17/50
2024-12-17 04:30:58.258569: train_loss -0.7646
2024-12-17 04:30:58.259378: val_loss -0.5666
2024-12-17 04:30:58.260028: Pseudo dice [0.7538]
2024-12-17 04:30:58.260691: Epoch time: 90.49 s
2024-12-17 04:30:59.877950: 
2024-12-17 04:30:59.879561: Epoch 110
2024-12-17 04:30:59.880265: Current learning rate: 0.00304
2024-12-17 04:32:30.502098: Validation loss did not improve from -0.60681. Patience: 18/50
2024-12-17 04:32:30.503086: train_loss -0.7662
2024-12-17 04:32:30.504091: val_loss -0.5657
2024-12-17 04:32:30.504828: Pseudo dice [0.7546]
2024-12-17 04:32:30.505579: Epoch time: 90.63 s
2024-12-17 04:32:31.754939: 
2024-12-17 04:32:31.756679: Epoch 111
2024-12-17 04:32:31.757488: Current learning rate: 0.00297
2024-12-17 04:34:02.574623: Validation loss did not improve from -0.60681. Patience: 19/50
2024-12-17 04:34:02.576159: train_loss -0.7674
2024-12-17 04:34:02.577120: val_loss -0.5868
2024-12-17 04:34:02.577804: Pseudo dice [0.7673]
2024-12-17 04:34:02.578633: Epoch time: 90.82 s
2024-12-17 04:34:03.777585: 
2024-12-17 04:34:03.779039: Epoch 112
2024-12-17 04:34:03.779890: Current learning rate: 0.00291
2024-12-17 04:35:34.551984: Validation loss did not improve from -0.60681. Patience: 20/50
2024-12-17 04:35:34.553243: train_loss -0.7683
2024-12-17 04:35:34.554333: val_loss -0.5621
2024-12-17 04:35:34.555157: Pseudo dice [0.7522]
2024-12-17 04:35:34.555908: Epoch time: 90.78 s
2024-12-17 04:35:35.832683: 
2024-12-17 04:35:35.834785: Epoch 113
2024-12-17 04:35:35.835532: Current learning rate: 0.00284
2024-12-17 04:37:06.443352: Validation loss did not improve from -0.60681. Patience: 21/50
2024-12-17 04:37:06.444515: train_loss -0.767
2024-12-17 04:37:06.445485: val_loss -0.5795
2024-12-17 04:37:06.446181: Pseudo dice [0.7702]
2024-12-17 04:37:06.446937: Epoch time: 90.61 s
2024-12-17 04:37:07.685292: 
2024-12-17 04:37:07.687295: Epoch 114
2024-12-17 04:37:07.688183: Current learning rate: 0.00277
2024-12-17 04:38:38.328782: Validation loss did not improve from -0.60681. Patience: 22/50
2024-12-17 04:38:38.331450: train_loss -0.7681
2024-12-17 04:38:38.332264: val_loss -0.5706
2024-12-17 04:38:38.332963: Pseudo dice [0.7641]
2024-12-17 04:38:38.333717: Epoch time: 90.65 s
2024-12-17 04:38:39.997122: 
2024-12-17 04:38:39.999067: Epoch 115
2024-12-17 04:38:40.000027: Current learning rate: 0.0027
2024-12-17 04:40:10.423394: Validation loss did not improve from -0.60681. Patience: 23/50
2024-12-17 04:40:10.424829: train_loss -0.7706
2024-12-17 04:40:10.426044: val_loss -0.5713
2024-12-17 04:40:10.427150: Pseudo dice [0.7572]
2024-12-17 04:40:10.428201: Epoch time: 90.43 s
2024-12-17 04:40:11.692726: 
2024-12-17 04:40:11.694592: Epoch 116
2024-12-17 04:40:11.695542: Current learning rate: 0.00263
2024-12-17 04:41:42.050756: Validation loss did not improve from -0.60681. Patience: 24/50
2024-12-17 04:41:42.051857: train_loss -0.7657
2024-12-17 04:41:42.053385: val_loss -0.5645
2024-12-17 04:41:42.054637: Pseudo dice [0.7558]
2024-12-17 04:41:42.055380: Epoch time: 90.36 s
2024-12-17 04:41:43.733272: 
2024-12-17 04:41:43.735324: Epoch 117
2024-12-17 04:41:43.736185: Current learning rate: 0.00256
2024-12-17 04:43:14.008218: Validation loss did not improve from -0.60681. Patience: 25/50
2024-12-17 04:43:14.009189: train_loss -0.767
2024-12-17 04:43:14.010060: val_loss -0.5532
2024-12-17 04:43:14.010843: Pseudo dice [0.747]
2024-12-17 04:43:14.011720: Epoch time: 90.28 s
2024-12-17 04:43:15.256367: 
2024-12-17 04:43:15.257917: Epoch 118
2024-12-17 04:43:15.258793: Current learning rate: 0.00249
2024-12-17 04:44:45.639945: Validation loss did not improve from -0.60681. Patience: 26/50
2024-12-17 04:44:45.641153: train_loss -0.7704
2024-12-17 04:44:45.642128: val_loss -0.5441
2024-12-17 04:44:45.642874: Pseudo dice [0.7482]
2024-12-17 04:44:45.643616: Epoch time: 90.39 s
2024-12-17 04:44:46.925676: 
2024-12-17 04:44:46.927762: Epoch 119
2024-12-17 04:44:46.928535: Current learning rate: 0.00242
2024-12-17 04:46:17.301530: Validation loss did not improve from -0.60681. Patience: 27/50
2024-12-17 04:46:17.302805: train_loss -0.7731
2024-12-17 04:46:17.303701: val_loss -0.5791
2024-12-17 04:46:17.304444: Pseudo dice [0.7668]
2024-12-17 04:46:17.305286: Epoch time: 90.38 s
2024-12-17 04:46:18.927514: 
2024-12-17 04:46:18.929181: Epoch 120
2024-12-17 04:46:18.930140: Current learning rate: 0.00235
2024-12-17 04:47:49.411033: Validation loss did not improve from -0.60681. Patience: 28/50
2024-12-17 04:47:49.412323: train_loss -0.7704
2024-12-17 04:47:49.413727: val_loss -0.545
2024-12-17 04:47:49.414827: Pseudo dice [0.7482]
2024-12-17 04:47:49.415905: Epoch time: 90.49 s
2024-12-17 04:47:50.694974: 
2024-12-17 04:47:50.696929: Epoch 121
2024-12-17 04:47:50.698142: Current learning rate: 0.00228
2024-12-17 04:49:21.416769: Validation loss did not improve from -0.60681. Patience: 29/50
2024-12-17 04:49:21.417678: train_loss -0.7725
2024-12-17 04:49:21.418579: val_loss -0.539
2024-12-17 04:49:21.419230: Pseudo dice [0.7445]
2024-12-17 04:49:21.419940: Epoch time: 90.72 s
2024-12-17 04:49:22.681009: 
2024-12-17 04:49:22.682297: Epoch 122
2024-12-17 04:49:22.683043: Current learning rate: 0.00221
2024-12-17 04:50:53.194481: Validation loss did not improve from -0.60681. Patience: 30/50
2024-12-17 04:50:53.195238: train_loss -0.7722
2024-12-17 04:50:53.196117: val_loss -0.5836
2024-12-17 04:50:53.196966: Pseudo dice [0.7718]
2024-12-17 04:50:53.197636: Epoch time: 90.52 s
2024-12-17 04:50:54.435272: 
2024-12-17 04:50:54.437525: Epoch 123
2024-12-17 04:50:54.438480: Current learning rate: 0.00214
2024-12-17 04:52:24.869538: Validation loss did not improve from -0.60681. Patience: 31/50
2024-12-17 04:52:24.870777: train_loss -0.7705
2024-12-17 04:52:24.871886: val_loss -0.5653
2024-12-17 04:52:24.872650: Pseudo dice [0.7597]
2024-12-17 04:52:24.873433: Epoch time: 90.44 s
2024-12-17 04:52:26.147699: 
2024-12-17 04:52:26.149412: Epoch 124
2024-12-17 04:52:26.150099: Current learning rate: 0.00207
2024-12-17 04:53:56.585451: Validation loss did not improve from -0.60681. Patience: 32/50
2024-12-17 04:53:56.586629: train_loss -0.7736
2024-12-17 04:53:56.587534: val_loss -0.596
2024-12-17 04:53:56.588311: Pseudo dice [0.7721]
2024-12-17 04:53:56.589028: Epoch time: 90.44 s
2024-12-17 04:53:58.224589: 
2024-12-17 04:53:58.226295: Epoch 125
2024-12-17 04:53:58.227358: Current learning rate: 0.00199
2024-12-17 04:55:28.369524: Validation loss did not improve from -0.60681. Patience: 33/50
2024-12-17 04:55:28.372087: train_loss -0.7748
2024-12-17 04:55:28.373558: val_loss -0.5553
2024-12-17 04:55:28.374312: Pseudo dice [0.7498]
2024-12-17 04:55:28.375489: Epoch time: 90.15 s
2024-12-17 04:55:29.643719: 
2024-12-17 04:55:29.645463: Epoch 126
2024-12-17 04:55:29.646386: Current learning rate: 0.00192
2024-12-17 04:56:59.895158: Validation loss did not improve from -0.60681. Patience: 34/50
2024-12-17 04:56:59.896496: train_loss -0.7752
2024-12-17 04:56:59.897593: val_loss -0.5528
2024-12-17 04:56:59.898471: Pseudo dice [0.7489]
2024-12-17 04:56:59.899135: Epoch time: 90.25 s
2024-12-17 04:57:01.536097: 
2024-12-17 04:57:01.538236: Epoch 127
2024-12-17 04:57:01.538986: Current learning rate: 0.00185
2024-12-17 04:58:31.770022: Validation loss did not improve from -0.60681. Patience: 35/50
2024-12-17 04:58:31.771505: train_loss -0.7762
2024-12-17 04:58:31.772310: val_loss -0.5894
2024-12-17 04:58:31.772962: Pseudo dice [0.7706]
2024-12-17 04:58:31.773751: Epoch time: 90.24 s
2024-12-17 04:58:33.058626: 
2024-12-17 04:58:33.060208: Epoch 128
2024-12-17 04:58:33.061147: Current learning rate: 0.00178
2024-12-17 05:00:03.033499: Validation loss did not improve from -0.60681. Patience: 36/50
2024-12-17 05:00:03.034953: train_loss -0.777
2024-12-17 05:00:03.036093: val_loss -0.5672
2024-12-17 05:00:03.036900: Pseudo dice [0.7558]
2024-12-17 05:00:03.037582: Epoch time: 89.98 s
2024-12-17 05:00:04.267768: 
2024-12-17 05:00:04.269296: Epoch 129
2024-12-17 05:00:04.270071: Current learning rate: 0.0017
2024-12-17 05:01:34.218087: Validation loss did not improve from -0.60681. Patience: 37/50
2024-12-17 05:01:34.218843: train_loss -0.7767
2024-12-17 05:01:34.219754: val_loss -0.5748
2024-12-17 05:01:34.220419: Pseudo dice [0.7657]
2024-12-17 05:01:34.221106: Epoch time: 89.95 s
2024-12-17 05:01:34.561092: Yayy! New best EMA pseudo Dice: 0.7585
2024-12-17 05:01:36.163670: 
2024-12-17 05:01:36.165564: Epoch 130
2024-12-17 05:01:36.166527: Current learning rate: 0.00163
2024-12-17 05:03:06.041455: Validation loss did not improve from -0.60681. Patience: 38/50
2024-12-17 05:03:06.042577: train_loss -0.7772
2024-12-17 05:03:06.043592: val_loss -0.5806
2024-12-17 05:03:06.044370: Pseudo dice [0.7709]
2024-12-17 05:03:06.045314: Epoch time: 89.88 s
2024-12-17 05:03:06.046056: Yayy! New best EMA pseudo Dice: 0.7598
2024-12-17 05:03:07.693556: 
2024-12-17 05:03:07.695029: Epoch 131
2024-12-17 05:03:07.695836: Current learning rate: 0.00156
2024-12-17 05:04:37.622690: Validation loss did not improve from -0.60681. Patience: 39/50
2024-12-17 05:04:37.624128: train_loss -0.773
2024-12-17 05:04:37.625245: val_loss -0.5649
2024-12-17 05:04:37.625984: Pseudo dice [0.755]
2024-12-17 05:04:37.626828: Epoch time: 89.93 s
2024-12-17 05:04:38.884750: 
2024-12-17 05:04:38.886477: Epoch 132
2024-12-17 05:04:38.887239: Current learning rate: 0.00148
2024-12-17 05:06:08.919297: Validation loss did not improve from -0.60681. Patience: 40/50
2024-12-17 05:06:08.920375: train_loss -0.7743
2024-12-17 05:06:08.921290: val_loss -0.562
2024-12-17 05:06:08.922054: Pseudo dice [0.7605]
2024-12-17 05:06:08.922805: Epoch time: 90.04 s
2024-12-17 05:06:10.164771: 
2024-12-17 05:06:10.166922: Epoch 133
2024-12-17 05:06:10.167950: Current learning rate: 0.00141
2024-12-17 05:07:40.286976: Validation loss did not improve from -0.60681. Patience: 41/50
2024-12-17 05:07:40.288242: train_loss -0.7768
2024-12-17 05:07:40.288952: val_loss -0.5745
2024-12-17 05:07:40.289591: Pseudo dice [0.7641]
2024-12-17 05:07:40.290301: Epoch time: 90.12 s
2024-12-17 05:07:40.290955: Yayy! New best EMA pseudo Dice: 0.7599
2024-12-17 05:07:41.888516: 
2024-12-17 05:07:41.890104: Epoch 134
2024-12-17 05:07:41.890876: Current learning rate: 0.00133
2024-12-17 05:09:11.892507: Validation loss did not improve from -0.60681. Patience: 42/50
2024-12-17 05:09:11.893953: train_loss -0.7784
2024-12-17 05:09:11.895384: val_loss -0.566
2024-12-17 05:09:11.896173: Pseudo dice [0.7568]
2024-12-17 05:09:11.896976: Epoch time: 90.01 s
2024-12-17 05:09:13.559847: 
2024-12-17 05:09:13.561963: Epoch 135
2024-12-17 05:09:13.563230: Current learning rate: 0.00126
2024-12-17 05:10:43.556679: Validation loss did not improve from -0.60681. Patience: 43/50
2024-12-17 05:10:43.557905: train_loss -0.7833
2024-12-17 05:10:43.558896: val_loss -0.5658
2024-12-17 05:10:43.559748: Pseudo dice [0.755]
2024-12-17 05:10:43.560596: Epoch time: 90.0 s
2024-12-17 05:10:44.810078: 
2024-12-17 05:10:44.811942: Epoch 136
2024-12-17 05:10:44.812681: Current learning rate: 0.00118
2024-12-17 05:12:15.026153: Validation loss did not improve from -0.60681. Patience: 44/50
2024-12-17 05:12:15.027066: train_loss -0.7776
2024-12-17 05:12:15.027974: val_loss -0.5726
2024-12-17 05:12:15.028850: Pseudo dice [0.7605]
2024-12-17 05:12:15.029625: Epoch time: 90.22 s
2024-12-17 05:12:16.310538: 
2024-12-17 05:12:16.312212: Epoch 137
2024-12-17 05:12:16.313029: Current learning rate: 0.00111
2024-12-17 05:13:46.635984: Validation loss did not improve from -0.60681. Patience: 45/50
2024-12-17 05:13:46.636973: train_loss -0.7822
2024-12-17 05:13:46.637656: val_loss -0.5747
2024-12-17 05:13:46.638435: Pseudo dice [0.7537]
2024-12-17 05:13:46.639133: Epoch time: 90.33 s
2024-12-17 05:13:48.337737: 
2024-12-17 05:13:48.338949: Epoch 138
2024-12-17 05:13:48.339633: Current learning rate: 0.00103
2024-12-17 05:15:18.651003: Validation loss did not improve from -0.60681. Patience: 46/50
2024-12-17 05:15:18.652218: train_loss -0.7814
2024-12-17 05:15:18.653360: val_loss -0.5486
2024-12-17 05:15:18.654164: Pseudo dice [0.7516]
2024-12-17 05:15:18.655087: Epoch time: 90.32 s
2024-12-17 05:15:19.923454: 
2024-12-17 05:15:19.926024: Epoch 139
2024-12-17 05:15:19.926992: Current learning rate: 0.00095
2024-12-17 05:16:50.195019: Validation loss did not improve from -0.60681. Patience: 47/50
2024-12-17 05:16:50.196167: train_loss -0.7833
2024-12-17 05:16:50.197324: val_loss -0.5756
2024-12-17 05:16:50.198280: Pseudo dice [0.7605]
2024-12-17 05:16:50.199177: Epoch time: 90.27 s
2024-12-17 05:16:51.863137: 
2024-12-17 05:16:51.865004: Epoch 140
2024-12-17 05:16:51.865967: Current learning rate: 0.00087
2024-12-17 05:18:22.088475: Validation loss did not improve from -0.60681. Patience: 48/50
2024-12-17 05:18:22.089617: train_loss -0.7836
2024-12-17 05:18:22.090694: val_loss -0.5538
2024-12-17 05:18:22.091745: Pseudo dice [0.7518]
2024-12-17 05:18:22.092758: Epoch time: 90.23 s
2024-12-17 05:18:23.342681: 
2024-12-17 05:18:23.344077: Epoch 141
2024-12-17 05:18:23.344801: Current learning rate: 0.00079
2024-12-17 05:19:53.532377: Validation loss did not improve from -0.60681. Patience: 49/50
2024-12-17 05:19:53.533643: train_loss -0.7865
2024-12-17 05:19:53.534870: val_loss -0.5789
2024-12-17 05:19:53.535754: Pseudo dice [0.7722]
2024-12-17 05:19:53.536655: Epoch time: 90.19 s
2024-12-17 05:19:54.820043: 
2024-12-17 05:19:54.822024: Epoch 142
2024-12-17 05:19:54.822954: Current learning rate: 0.00071
2024-12-17 05:21:24.865662: Validation loss did not improve from -0.60681. Patience: 50/50
2024-12-17 05:21:24.867058: train_loss -0.7818
2024-12-17 05:21:24.868358: val_loss -0.5682
2024-12-17 05:21:24.869192: Pseudo dice [0.7658]
2024-12-17 05:21:24.870167: Epoch time: 90.05 s
2024-12-17 05:21:26.176950: 
2024-12-17 05:21:26.178839: Epoch 143
2024-12-17 05:21:26.179832: Current learning rate: 0.00063
2024-12-17 05:22:56.292300: Validation loss did not improve from -0.60681. Patience: 51/50
2024-12-17 05:22:56.293484: train_loss -0.7836
2024-12-17 05:22:56.294351: val_loss -0.5695
2024-12-17 05:22:56.295148: Pseudo dice [0.7656]
2024-12-17 05:22:56.295850: Epoch time: 90.12 s
2024-12-17 05:22:56.296657: Yayy! New best EMA pseudo Dice: 0.7603
2024-12-17 05:22:57.968875: 
2024-12-17 05:22:57.970897: Epoch 144
2024-12-17 05:22:57.971804: Current learning rate: 0.00055
2024-12-17 05:24:28.457997: Validation loss did not improve from -0.60681. Patience: 52/50
2024-12-17 05:24:28.459055: train_loss -0.7833
2024-12-17 05:24:28.460108: val_loss -0.5779
2024-12-17 05:24:28.461053: Pseudo dice [0.7609]
2024-12-17 05:24:28.462017: Epoch time: 90.49 s
2024-12-17 05:24:28.872119: Yayy! New best EMA pseudo Dice: 0.7604
2024-12-17 05:24:30.513957: 
2024-12-17 05:24:30.516019: Epoch 145
2024-12-17 05:24:30.516961: Current learning rate: 0.00047
2024-12-17 05:26:01.009958: Validation loss did not improve from -0.60681. Patience: 53/50
2024-12-17 05:26:01.010790: train_loss -0.7847
2024-12-17 05:26:01.012167: val_loss -0.5598
2024-12-17 05:26:01.013218: Pseudo dice [0.7599]
2024-12-17 05:26:01.014223: Epoch time: 90.5 s
2024-12-17 05:26:02.307731: 
2024-12-17 05:26:02.309333: Epoch 146
2024-12-17 05:26:02.310395: Current learning rate: 0.00038
2024-12-17 05:27:32.885398: Validation loss did not improve from -0.60681. Patience: 54/50
2024-12-17 05:27:32.886798: train_loss -0.7857
2024-12-17 05:27:32.888035: val_loss -0.5827
2024-12-17 05:27:32.888780: Pseudo dice [0.7712]
2024-12-17 05:27:32.889584: Epoch time: 90.58 s
2024-12-17 05:27:32.890441: Yayy! New best EMA pseudo Dice: 0.7614
2024-12-17 05:27:34.536613: 
2024-12-17 05:27:34.538660: Epoch 147
2024-12-17 05:27:34.539620: Current learning rate: 0.0003
2024-12-17 05:29:05.070631: Validation loss did not improve from -0.60681. Patience: 55/50
2024-12-17 05:29:05.072090: train_loss -0.784
2024-12-17 05:29:05.072934: val_loss -0.5615
2024-12-17 05:29:05.073621: Pseudo dice [0.7586]
2024-12-17 05:29:05.074364: Epoch time: 90.54 s
2024-12-17 05:29:06.730515: 
2024-12-17 05:29:06.732205: Epoch 148
2024-12-17 05:29:06.733197: Current learning rate: 0.00021
2024-12-17 05:30:37.253065: Validation loss did not improve from -0.60681. Patience: 56/50
2024-12-17 05:30:37.254221: train_loss -0.7846
2024-12-17 05:30:37.255422: val_loss -0.552
2024-12-17 05:30:37.256335: Pseudo dice [0.7491]
2024-12-17 05:30:37.257237: Epoch time: 90.52 s
2024-12-17 05:30:38.484637: 
2024-12-17 05:30:38.486389: Epoch 149
2024-12-17 05:30:38.487598: Current learning rate: 0.00011
2024-12-17 05:32:08.781277: Validation loss did not improve from -0.60681. Patience: 57/50
2024-12-17 05:32:08.782616: train_loss -0.7832
2024-12-17 05:32:08.783820: val_loss -0.5864
2024-12-17 05:32:08.784581: Pseudo dice [0.7685]
2024-12-17 05:32:08.785280: Epoch time: 90.3 s
2024-12-17 05:32:10.412807: Training done.
2024-12-17 05:29:21.239520: Using splits from existing split file: /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset309_Sohee_Calcium_OCT_CrossValidation/splits_final.json
2024-12-17 05:29:21.242530: The split file contains 5 splits.
2024-12-17 05:29:21.243430: Desired fold for training: 0
2024-12-17 05:29:21.244190: This split has 6 training and 2 validation cases.
2024-12-17 05:29:21.244936: predicting 106-002
2024-12-17 05:29:21.268064: 106-002, shape torch.Size([1, 539, 498, 498]), rank 0
2024-12-17 05:31:44.044105: predicting 706-005
2024-12-17 05:31:44.070524: 706-005, shape torch.Size([1, 375, 498, 498]), rank 0
2024-12-17 05:33:29.288850: Validation complete
2024-12-17 05:33:29.289338: Mean Validation Dice:  0.7797454103808763
2024-12-17 05:32:10.555474: Using splits from existing split file: /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset309_Sohee_Calcium_OCT_CrossValidation/splits_final.json
2024-12-17 05:32:10.556992: The split file contains 5 splits.
2024-12-17 05:32:10.557929: Desired fold for training: 1
2024-12-17 05:32:10.558538: This split has 6 training and 2 validation cases.
2024-12-17 05:32:10.559305: predicting 101-019
2024-12-17 05:32:10.569174: 101-019, shape torch.Size([1, 375, 498, 498]), rank 0
2024-12-17 05:33:47.968410: predicting 401-004
2024-12-17 05:33:47.995469: 401-004, shape torch.Size([1, 375, 498, 498]), rank 0
2024-12-17 05:35:36.744779: Validation complete
2024-12-17 05:35:36.746210: Mean Validation Dice:  0.7544705074657965

############################
INFO: You are using the old nnU-Net default plans. We have updated our recommendations. Please consider using those instead! Read more here: https://github.com/MIC-DKFZ/nnUNet/blob/master/documentation/resenc_presets.md
############################

Using device: cuda:0

#######################################################################
Please cite the following paper when using nnU-Net:
Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.
#######################################################################

2024-12-17 05:35:42.755295: Using torch.compile...

############################
INFO: You are using the old nnU-Net default plans. We have updated our recommendations. Please consider using those instead! Read more here: https://github.com/MIC-DKFZ/nnUNet/blob/master/documentation/resenc_presets.md
############################

Using device: cuda:0

#######################################################################
Please cite the following paper when using nnU-Net:
Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.
#######################################################################

2024-12-17 05:35:42.762171: Using torch.compile...
/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
################### Loading pretrained weights from file  /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_results/Dataset300_Lumen_and_Wall_OCT/nnUNetTrainer__nnUNetPreprocessPlans__3d_fullres/fold_all/checkpoint_best.pth ###################
Below is the list of overlapping blocks in pretrained model and nnUNet architecture:
encoder.stages.0.0.convs.0.conv.weight shape torch.Size([32, 1, 3, 3, 3])
encoder.stages.0.0.convs.0.conv.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.norm.weight shape torch.Size([32])
encoder.stages.0.0.convs.0.norm.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.0.weight shape torch.Size([32, 1, 3, 3, 3])
encoder.stages.0.0.convs.0.all_modules.0.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.1.weight shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.1.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
encoder.stages.0.0.convs.1.conv.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.norm.weight shape torch.Size([32])
encoder.stages.0.0.convs.1.norm.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
encoder.stages.0.0.convs.1.all_modules.0.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.1.weight shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.1.bias shape torch.Size([32])
encoder.stages.1.0.convs.0.conv.weight shape torch.Size([64, 32, 3, 3, 3])
encoder.stages.1.0.convs.0.conv.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.norm.weight shape torch.Size([64])
encoder.stages.1.0.convs.0.norm.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.0.weight shape torch.Size([64, 32, 3, 3, 3])
encoder.stages.1.0.convs.0.all_modules.0.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.1.weight shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.1.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
encoder.stages.1.0.convs.1.conv.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.norm.weight shape torch.Size([64])
encoder.stages.1.0.convs.1.norm.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
encoder.stages.1.0.convs.1.all_modules.0.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.1.weight shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.1.bias shape torch.Size([64])
encoder.stages.2.0.convs.0.conv.weight shape torch.Size([128, 64, 3, 3, 3])
encoder.stages.2.0.convs.0.conv.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.norm.weight shape torch.Size([128])
encoder.stages.2.0.convs.0.norm.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.0.weight shape torch.Size([128, 64, 3, 3, 3])
encoder.stages.2.0.convs.0.all_modules.0.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.1.weight shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.1.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
encoder.stages.2.0.convs.1.conv.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.norm.weight shape torch.Size([128])
encoder.stages.2.0.convs.1.norm.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
encoder.stages.2.0.convs.1.all_modules.0.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.1.weight shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.1.bias shape torch.Size([128])
encoder.stages.3.0.convs.0.conv.weight shape torch.Size([256, 128, 3, 3, 3])
encoder.stages.3.0.convs.0.conv.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.norm.weight shape torch.Size([256])
encoder.stages.3.0.convs.0.norm.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.0.weight shape torch.Size([256, 128, 3, 3, 3])
encoder.stages.3.0.convs.0.all_modules.0.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.1.weight shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.1.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
encoder.stages.3.0.convs.1.conv.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.norm.weight shape torch.Size([256])
encoder.stages.3.0.convs.1.norm.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
encoder.stages.3.0.convs.1.all_modules.0.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.1.weight shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.1.bias shape torch.Size([256])
encoder.stages.4.0.convs.0.conv.weight shape torch.Size([320, 256, 3, 3, 3])
encoder.stages.4.0.convs.0.conv.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.norm.weight shape torch.Size([320])
encoder.stages.4.0.convs.0.norm.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.0.weight shape torch.Size([320, 256, 3, 3, 3])
encoder.stages.4.0.convs.0.all_modules.0.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.1.weight shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.1.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.4.0.convs.1.conv.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.norm.weight shape torch.Size([320])
encoder.stages.4.0.convs.1.norm.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.4.0.convs.1.all_modules.0.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.1.weight shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.1.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.0.conv.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.norm.weight shape torch.Size([320])
encoder.stages.5.0.convs.0.norm.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.0.all_modules.0.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.1.weight shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.1.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.1.conv.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.norm.weight shape torch.Size([320])
encoder.stages.5.0.convs.1.norm.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.1.all_modules.0.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.1.weight shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.0.0.convs.0.conv.weight shape torch.Size([32, 1, 3, 3, 3])
decoder.encoder.stages.0.0.convs.0.conv.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.norm.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.norm.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.0.weight shape torch.Size([32, 1, 3, 3, 3])
decoder.encoder.stages.0.0.convs.0.all_modules.0.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.1.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.1.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.encoder.stages.0.0.convs.1.conv.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.norm.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.norm.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.encoder.stages.0.0.convs.1.all_modules.0.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.1.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.1.bias shape torch.Size([32])
decoder.encoder.stages.1.0.convs.0.conv.weight shape torch.Size([64, 32, 3, 3, 3])
decoder.encoder.stages.1.0.convs.0.conv.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.norm.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.norm.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.0.weight shape torch.Size([64, 32, 3, 3, 3])
decoder.encoder.stages.1.0.convs.0.all_modules.0.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.1.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.1.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.encoder.stages.1.0.convs.1.conv.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.norm.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.norm.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.encoder.stages.1.0.convs.1.all_modules.0.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.1.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.1.bias shape torch.Size([64])
decoder.encoder.stages.2.0.convs.0.conv.weight shape torch.Size([128, 64, 3, 3, 3])
decoder.encoder.stages.2.0.convs.0.conv.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.norm.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.norm.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.0.weight shape torch.Size([128, 64, 3, 3, 3])
decoder.encoder.stages.2.0.convs.0.all_modules.0.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.1.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.1.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.encoder.stages.2.0.convs.1.conv.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.norm.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.norm.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.encoder.stages.2.0.convs.1.all_modules.0.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.1.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.1.bias shape torch.Size([128])
decoder.encoder.stages.3.0.convs.0.conv.weight shape torch.Size([256, 128, 3, 3, 3])
decoder.encoder.stages.3.0.convs.0.conv.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.norm.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.norm.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.0.weight shape torch.Size([256, 128, 3, 3, 3])
decoder.encoder.stages.3.0.convs.0.all_modules.0.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.1.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.1.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.encoder.stages.3.0.convs.1.conv.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.norm.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.norm.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.encoder.stages.3.0.convs.1.all_modules.0.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.1.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.1.bias shape torch.Size([256])
decoder.encoder.stages.4.0.convs.0.conv.weight shape torch.Size([320, 256, 3, 3, 3])
decoder.encoder.stages.4.0.convs.0.conv.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.norm.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.norm.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.0.weight shape torch.Size([320, 256, 3, 3, 3])
decoder.encoder.stages.4.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.4.0.convs.1.conv.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.norm.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.norm.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.4.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.0.conv.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.norm.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.norm.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.1.conv.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.norm.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.norm.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.stages.0.convs.0.conv.weight shape torch.Size([320, 640, 3, 3, 3])
decoder.stages.0.convs.0.conv.bias shape torch.Size([320])
decoder.stages.0.convs.0.norm.weight shape torch.Size([320])
decoder.stages.0.convs.0.norm.bias shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.0.weight shape torch.Size([320, 640, 3, 3, 3])
decoder.stages.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.stages.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.stages.0.convs.1.conv.bias shape torch.Size([320])
decoder.stages.0.convs.1.norm.weight shape torch.Size([320])
decoder.stages.0.convs.1.norm.bias shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.stages.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.stages.1.convs.0.conv.weight shape torch.Size([256, 512, 3, 3, 3])
decoder.stages.1.convs.0.conv.bias shape torch.Size([256])
decoder.stages.1.convs.0.norm.weight shape torch.Size([256])
decoder.stages.1.convs.0.norm.bias shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.0.weight shape torch.Size([256, 512, 3, 3, 3])
decoder.stages.1.convs.0.all_modules.0.bias shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.1.weight shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.1.bias shape torch.Size([256])
decoder.stages.1.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.stages.1.convs.1.conv.bias shape torch.Size([256])
decoder.stages.1.convs.1.norm.weight shape torch.Size([256])
decoder.stages.1.convs.1.norm.bias shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.stages.1.convs.1.all_modules.0.bias shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.1.weight shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.1.bias shape torch.Size([256])
decoder.stages.2.convs.0.conv.weight shape torch.Size([128, 256, 3, 3, 3])
decoder.stages.2.convs.0.conv.bias shape torch.Size([128])
decoder.stages.2.convs.0.norm.weight shape torch.Size([128])
decoder.stages.2.convs.0.norm.bias shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.0.weight shape torch.Size([128, 256, 3, 3, 3])
decoder.stages.2.convs.0.all_modules.0.bias shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.1.weight shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.1.bias shape torch.Size([128])
decoder.stages.2.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.stages.2.convs.1.conv.bias shape torch.Size([128])
decoder.stages.2.convs.1.norm.weight shape torch.Size([128])
decoder.stages.2.convs.1.norm.bias shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.stages.2.convs.1.all_modules.0.bias shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.1.weight shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.1.bias shape torch.Size([128])
decoder.stages.3.convs.0.conv.weight shape torch.Size([64, 128, 3, 3, 3])
decoder.stages.3.convs.0.conv.bias shape torch.Size([64])
decoder.stages.3.convs.0.norm.weight shape torch.Size([64])
decoder.stages.3.convs.0.norm.bias shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.0.weight shape torch.Size([64, 128, 3, 3, 3])
decoder.stages.3.convs.0.all_modules.0.bias shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.1.weight shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.1.bias shape torch.Size([64])
decoder.stages.3.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.stages.3.convs.1.conv.bias shape torch.Size([64])
decoder.stages.3.convs.1.norm.weight shape torch.Size([64])
decoder.stages.3.convs.1.norm.bias shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.stages.3.convs.1.all_modules.0.bias shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.1.weight shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.1.bias shape torch.Size([64])
decoder.stages.4.convs.0.conv.weight shape torch.Size([32, 64, 3, 3, 3])
decoder.stages.4.convs.0.conv.bias shape torch.Size([32])
decoder.stages.4.convs.0.norm.weight shape torch.Size([32])
decoder.stages.4.convs.0.norm.bias shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.0.weight shape torch.Size([32, 64, 3, 3, 3])
decoder.stages.4.convs.0.all_modules.0.bias shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.1.weight shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.1.bias shape torch.Size([32])
decoder.stages.4.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.stages.4.convs.1.conv.bias shape torch.Size([32])
decoder.stages.4.convs.1.norm.weight shape torch.Size([32])
decoder.stages.4.convs.1.norm.bias shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.stages.4.convs.1.all_modules.0.bias shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.1.weight shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.1.bias shape torch.Size([32])
decoder.transpconvs.0.weight shape torch.Size([320, 320, 1, 2, 2])
decoder.transpconvs.0.bias shape torch.Size([320])
decoder.transpconvs.1.weight shape torch.Size([320, 256, 2, 2, 2])
decoder.transpconvs.1.bias shape torch.Size([256])
decoder.transpconvs.2.weight shape torch.Size([256, 128, 2, 2, 2])
decoder.transpconvs.2.bias shape torch.Size([128])
decoder.transpconvs.3.weight shape torch.Size([128, 64, 2, 2, 2])
decoder.transpconvs.3.bias shape torch.Size([64])
decoder.transpconvs.4.weight shape torch.Size([64, 32, 2, 2, 2])
decoder.transpconvs.4.bias shape torch.Size([32])
################### Done ###################
2024-12-17 05:35:46.264915: do_dummy_2d_data_aug: True
2024-12-17 05:35:46.267294: Using splits from existing split file: /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset309_Sohee_Calcium_OCT_CrossValidation/splits_final.json
2024-12-17 05:35:46.269415: The split file contains 5 splits.
2024-12-17 05:35:46.270271: Desired fold for training: 2
2024-12-17 05:35:46.271237: This split has 6 training and 2 validation cases.
################### Loading pretrained weights from file  /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_results/Dataset300_Lumen_and_Wall_OCT/nnUNetTrainer__nnUNetPreprocessPlans__3d_fullres/fold_all/checkpoint_best.pth ###################
Below is the list of overlapping blocks in pretrained model and nnUNet architecture:
encoder.stages.0.0.convs.0.conv.weight shape torch.Size([32, 1, 3, 3, 3])
encoder.stages.0.0.convs.0.conv.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.norm.weight shape torch.Size([32])
encoder.stages.0.0.convs.0.norm.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.0.weight shape torch.Size([32, 1, 3, 3, 3])
encoder.stages.0.0.convs.0.all_modules.0.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.1.weight shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.1.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
encoder.stages.0.0.convs.1.conv.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.norm.weight shape torch.Size([32])
encoder.stages.0.0.convs.1.norm.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
encoder.stages.0.0.convs.1.all_modules.0.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.1.weight shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.1.bias shape torch.Size([32])
encoder.stages.1.0.convs.0.conv.weight shape torch.Size([64, 32, 3, 3, 3])
encoder.stages.1.0.convs.0.conv.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.norm.weight shape torch.Size([64])
encoder.stages.1.0.convs.0.norm.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.0.weight shape torch.Size([64, 32, 3, 3, 3])
encoder.stages.1.0.convs.0.all_modules.0.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.1.weight shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.1.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
encoder.stages.1.0.convs.1.conv.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.norm.weight shape torch.Size([64])
encoder.stages.1.0.convs.1.norm.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
encoder.stages.1.0.convs.1.all_modules.0.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.1.weight shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.1.bias shape torch.Size([64])
encoder.stages.2.0.convs.0.conv.weight shape torch.Size([128, 64, 3, 3, 3])
encoder.stages.2.0.convs.0.conv.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.norm.weight shape torch.Size([128])
encoder.stages.2.0.convs.0.norm.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.0.weight shape torch.Size([128, 64, 3, 3, 3])
encoder.stages.2.0.convs.0.all_modules.0.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.1.weight shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.1.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
encoder.stages.2.0.convs.1.conv.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.norm.weight shape torch.Size([128])
encoder.stages.2.0.convs.1.norm.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
encoder.stages.2.0.convs.1.all_modules.0.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.1.weight shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.1.bias shape torch.Size([128])
encoder.stages.3.0.convs.0.conv.weight shape torch.Size([256, 128, 3, 3, 3])
encoder.stages.3.0.convs.0.conv.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.norm.weight shape torch.Size([256])
encoder.stages.3.0.convs.0.norm.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.0.weight shape torch.Size([256, 128, 3, 3, 3])
encoder.stages.3.0.convs.0.all_modules.0.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.1.weight shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.1.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
encoder.stages.3.0.convs.1.conv.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.norm.weight shape torch.Size([256])
encoder.stages.3.0.convs.1.norm.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
encoder.stages.3.0.convs.1.all_modules.0.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.1.weight shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.1.bias shape torch.Size([256])
encoder.stages.4.0.convs.0.conv.weight shape torch.Size([320, 256, 3, 3, 3])
encoder.stages.4.0.convs.0.conv.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.norm.weight shape torch.Size([320])
encoder.stages.4.0.convs.0.norm.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.0.weight shape torch.Size([320, 256, 3, 3, 3])
encoder.stages.4.0.convs.0.all_modules.0.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.1.weight shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.1.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.4.0.convs.1.conv.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.norm.weight shape torch.Size([320])
encoder.stages.4.0.convs.1.norm.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.4.0.convs.1.all_modules.0.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.1.weight shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.1.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.0.conv.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.norm.weight shape torch.Size([320])
encoder.stages.5.0.convs.0.norm.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.0.all_modules.0.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.1.weight shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.1.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.1.conv.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.norm.weight shape torch.Size([320])
encoder.stages.5.0.convs.1.norm.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.1.all_modules.0.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.1.weight shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.0.0.convs.0.conv.weight shape torch.Size([32, 1, 3, 3, 3])
decoder.encoder.stages.0.0.convs.0.conv.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.norm.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.norm.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.0.weight shape torch.Size([32, 1, 3, 3, 3])
decoder.encoder.stages.0.0.convs.0.all_modules.0.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.1.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.1.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.encoder.stages.0.0.convs.1.conv.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.norm.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.norm.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.encoder.stages.0.0.convs.1.all_modules.0.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.1.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.1.bias shape torch.Size([32])
decoder.encoder.stages.1.0.convs.0.conv.weight shape torch.Size([64, 32, 3, 3, 3])
decoder.encoder.stages.1.0.convs.0.conv.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.norm.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.norm.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.0.weight shape torch.Size([64, 32, 3, 3, 3])
decoder.encoder.stages.1.0.convs.0.all_modules.0.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.1.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.1.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.encoder.stages.1.0.convs.1.conv.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.norm.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.norm.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.encoder.stages.1.0.convs.1.all_modules.0.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.1.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.1.bias shape torch.Size([64])
decoder.encoder.stages.2.0.convs.0.conv.weight shape torch.Size([128, 64, 3, 3, 3])
decoder.encoder.stages.2.0.convs.0.conv.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.norm.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.norm.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.0.weight shape torch.Size([128, 64, 3, 3, 3])
decoder.encoder.stages.2.0.convs.0.all_modules.0.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.1.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.1.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.encoder.stages.2.0.convs.1.conv.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.norm.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.norm.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.encoder.stages.2.0.convs.1.all_modules.0.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.1.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.1.bias shape torch.Size([128])
decoder.encoder.stages.3.0.convs.0.conv.weight shape torch.Size([256, 128, 3, 3, 3])
decoder.encoder.stages.3.0.convs.0.conv.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.norm.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.norm.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.0.weight shape torch.Size([256, 128, 3, 3, 3])
decoder.encoder.stages.3.0.convs.0.all_modules.0.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.1.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.1.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.encoder.stages.3.0.convs.1.conv.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.norm.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.norm.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.encoder.stages.3.0.convs.1.all_modules.0.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.1.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.1.bias shape torch.Size([256])
decoder.encoder.stages.4.0.convs.0.conv.weight shape torch.Size([320, 256, 3, 3, 3])
decoder.encoder.stages.4.0.convs.0.conv.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.norm.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.norm.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.0.weight shape torch.Size([320, 256, 3, 3, 3])
decoder.encoder.stages.4.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.4.0.convs.1.conv.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.norm.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.norm.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.4.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.0.conv.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.norm.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.norm.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.1.conv.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.norm.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.norm.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.stages.0.convs.0.conv.weight shape torch.Size([320, 640, 3, 3, 3])
decoder.stages.0.convs.0.conv.bias shape torch.Size([320])
decoder.stages.0.convs.0.norm.weight shape torch.Size([320])
decoder.stages.0.convs.0.norm.bias shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.0.weight shape torch.Size([320, 640, 3, 3, 3])
decoder.stages.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.stages.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.stages.0.convs.1.conv.bias shape torch.Size([320])
decoder.stages.0.convs.1.norm.weight shape torch.Size([320])
decoder.stages.0.convs.1.norm.bias shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.stages.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.stages.1.convs.0.conv.weight shape torch.Size([256, 512, 3, 3, 3])
decoder.stages.1.convs.0.conv.bias shape torch.Size([256])
decoder.stages.1.convs.0.norm.weight shape torch.Size([256])
decoder.stages.1.convs.0.norm.bias shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.0.weight shape torch.Size([256, 512, 3, 3, 3])
decoder.stages.1.convs.0.all_modules.0.bias shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.1.weight shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.1.bias shape torch.Size([256])
decoder.stages.1.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.stages.1.convs.1.conv.bias shape torch.Size([256])
decoder.stages.1.convs.1.norm.weight shape torch.Size([256])
decoder.stages.1.convs.1.norm.bias shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.stages.1.convs.1.all_modules.0.bias shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.1.weight shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.1.bias shape torch.Size([256])
decoder.stages.2.convs.0.conv.weight shape torch.Size([128, 256, 3, 3, 3])
decoder.stages.2.convs.0.conv.bias shape torch.Size([128])
decoder.stages.2.convs.0.norm.weight shape torch.Size([128])
decoder.stages.2.convs.0.norm.bias shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.0.weight shape torch.Size([128, 256, 3, 3, 3])
decoder.stages.2.convs.0.all_modules.0.bias shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.1.weight shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.1.bias shape torch.Size([128])
decoder.stages.2.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.stages.2.convs.1.conv.bias shape torch.Size([128])
decoder.stages.2.convs.1.norm.weight shape torch.Size([128])
decoder.stages.2.convs.1.norm.bias shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.stages.2.convs.1.all_modules.0.bias shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.1.weight shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.1.bias shape torch.Size([128])
decoder.stages.3.convs.0.conv.weight shape torch.Size([64, 128, 3, 3, 3])
decoder.stages.3.convs.0.conv.bias shape torch.Size([64])
decoder.stages.3.convs.0.norm.weight shape torch.Size([64])
decoder.stages.3.convs.0.norm.bias shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.0.weight shape torch.Size([64, 128, 3, 3, 3])
decoder.stages.3.convs.0.all_modules.0.bias shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.1.weight shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.1.bias shape torch.Size([64])
decoder.stages.3.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.stages.3.convs.1.conv.bias shape torch.Size([64])
decoder.stages.3.convs.1.norm.weight shape torch.Size([64])
decoder.stages.3.convs.1.norm.bias shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.stages.3.convs.1.all_modules.0.bias shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.1.weight shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.1.bias shape torch.Size([64])
decoder.stages.4.convs.0.conv.weight shape torch.Size([32, 64, 3, 3, 3])
decoder.stages.4.convs.0.conv.bias shape torch.Size([32])
decoder.stages.4.convs.0.norm.weight shape torch.Size([32])
decoder.stages.4.convs.0.norm.bias shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.0.weight shape torch.Size([32, 64, 3, 3, 3])
decoder.stages.4.convs.0.all_modules.0.bias shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.1.weight shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.1.bias shape torch.Size([32])
decoder.stages.4.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.stages.4.convs.1.conv.bias shape torch.Size([32])
decoder.stages.4.convs.1.norm.weight shape torch.Size([32])
decoder.stages.4.convs.1.norm.bias shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.stages.4.convs.1.all_modules.0.bias shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.1.weight shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.1.bias shape torch.Size([32])
decoder.transpconvs.0.weight shape torch.Size([320, 320, 1, 2, 2])
decoder.transpconvs.0.bias shape torch.Size([320])
decoder.transpconvs.1.weight shape torch.Size([320, 256, 2, 2, 2])
decoder.transpconvs.1.bias shape torch.Size([256])
decoder.transpconvs.2.weight shape torch.Size([256, 128, 2, 2, 2])
decoder.transpconvs.2.bias shape torch.Size([128])
decoder.transpconvs.3.weight shape torch.Size([128, 64, 2, 2, 2])
decoder.transpconvs.3.bias shape torch.Size([64])
decoder.transpconvs.4.weight shape torch.Size([64, 32, 2, 2, 2])
decoder.transpconvs.4.bias shape torch.Size([32])
################### Done ###################
2024-12-17 05:35:46.266295: do_dummy_2d_data_aug: True
2024-12-17 05:35:46.268289: Using splits from existing split file: /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset309_Sohee_Calcium_OCT_CrossValidation/splits_final.json
2024-12-17 05:35:46.269977: The split file contains 5 splits.
2024-12-17 05:35:46.270991: Desired fold for training: 3
2024-12-17 05:35:46.272018: This split has 7 training and 1 validation cases.
using pin_memory on device 0
using pin_memory on device 0
using pin_memory on device 0

This is the configuration used by this training:
Configuration name: 3d_32x160x128_b10
 {'data_identifier': 'nnUNetPlans_3d_fullres', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 10, 'patch_size': [32, 160, 128], 'median_image_size_in_voxels': [375.0, 498.0, 498.0], 'spacing': [1.0, 1.0, 1.0], 'normalization_schemes': ['ZScoreNormalization'], 'use_mask_for_norm': [False], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'architecture': {'network_class_name': 'dynamic_network_architectures.architectures.unet.PlainConvUNet', 'arch_kwargs': {'n_stages': 6, 'features_per_stage': [32, 64, 128, 256, 320, 320], 'conv_op': 'torch.nn.modules.conv.Conv3d', 'kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]], 'strides': [[1, 1, 1], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [1, 2, 2]], 'n_conv_per_stage': [2, 2, 2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2, 2, 2], 'conv_bias': True, 'norm_op': 'torch.nn.modules.instancenorm.InstanceNorm3d', 'norm_op_kwargs': {'eps': 1e-05, 'affine': True}, 'dropout_op': None, 'dropout_op_kwargs': None, 'nonlin': 'torch.nn.LeakyReLU', 'nonlin_kwargs': {'inplace': True}, 'deep_supervision': True}, '_kw_requires_import': ['conv_op', 'norm_op', 'dropout_op', 'nonlin']}, 'batch_dice': True, 'inherits_from': '3d_fullres'} 

These are the global plan.json settings:
 {'dataset_name': 'Dataset309_Sohee_Calcium_OCT_CrossValidation', 'plans_name': 'nnUNetPlans', 'original_median_spacing_after_transp': [1.0, 1.0, 1.0], 'original_median_shape_after_transp': [375, 498, 498], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 0.4977976381778717, 'mean': 0.13507525622844696, 'median': 0.09599608182907104, 'min': 0.0, 'percentile_00_5': 0.014754901640117168, 'percentile_99_5': 0.4977976381778717, 'std': 0.12025152146816254}}} 

2024-12-17 05:35:48.747240: unpacking dataset...
using pin_memory on device 0

This is the configuration used by this training:
Configuration name: 3d_32x160x128_b10
 {'data_identifier': 'nnUNetPlans_3d_fullres', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 10, 'patch_size': [32, 160, 128], 'median_image_size_in_voxels': [375.0, 498.0, 498.0], 'spacing': [1.0, 1.0, 1.0], 'normalization_schemes': ['ZScoreNormalization'], 'use_mask_for_norm': [False], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'architecture': {'network_class_name': 'dynamic_network_architectures.architectures.unet.PlainConvUNet', 'arch_kwargs': {'n_stages': 6, 'features_per_stage': [32, 64, 128, 256, 320, 320], 'conv_op': 'torch.nn.modules.conv.Conv3d', 'kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]], 'strides': [[1, 1, 1], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [1, 2, 2]], 'n_conv_per_stage': [2, 2, 2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2, 2, 2], 'conv_bias': True, 'norm_op': 'torch.nn.modules.instancenorm.InstanceNorm3d', 'norm_op_kwargs': {'eps': 1e-05, 'affine': True}, 'dropout_op': None, 'dropout_op_kwargs': None, 'nonlin': 'torch.nn.LeakyReLU', 'nonlin_kwargs': {'inplace': True}, 'deep_supervision': True}, '_kw_requires_import': ['conv_op', 'norm_op', 'dropout_op', 'nonlin']}, 'batch_dice': True, 'inherits_from': '3d_fullres'} 

These are the global plan.json settings:
 {'dataset_name': 'Dataset309_Sohee_Calcium_OCT_CrossValidation', 'plans_name': 'nnUNetPlans', 'original_median_spacing_after_transp': [1.0, 1.0, 1.0], 'original_median_shape_after_transp': [375, 498, 498], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 0.4977976381778717, 'mean': 0.13507525622844696, 'median': 0.09599608182907104, 'min': 0.0, 'percentile_00_5': 0.014754901640117168, 'percentile_99_5': 0.4977976381778717, 'std': 0.12025152146816254}}} 

2024-12-17 05:35:48.752918: unpacking dataset...
2024-12-17 05:35:53.045411: unpacking done...
2024-12-17 05:35:53.136880: Unable to plot network architecture: nnUNet_compile is enabled!
2024-12-17 05:35:53.239770: 
2024-12-17 05:35:53.241304: Epoch 0
2024-12-17 05:35:53.242457: Current learning rate: 0.01
2024-12-17 05:38:06.186152: Validation loss improved from 1000.00000 to -0.46947! Patience: 0/50
2024-12-17 05:38:06.187138: train_loss -0.3226
2024-12-17 05:38:06.187947: val_loss -0.4695
2024-12-17 05:38:06.188610: Pseudo dice [0.6911]
2024-12-17 05:38:06.189375: Epoch time: 132.95 s
2024-12-17 05:38:06.190121: Yayy! New best EMA pseudo Dice: 0.6911
2024-12-17 05:38:07.684520: 
2024-12-17 05:38:07.685888: Epoch 1
2024-12-17 05:38:07.686741: Current learning rate: 0.00994
2024-12-17 05:39:34.470787: Validation loss improved from -0.46947 to -0.47873! Patience: 0/50
2024-12-17 05:39:34.471861: train_loss -0.4606
2024-12-17 05:39:34.472761: val_loss -0.4787
2024-12-17 05:39:34.473587: Pseudo dice [0.6929]
2024-12-17 05:39:34.474319: Epoch time: 86.79 s
2024-12-17 05:39:34.474983: Yayy! New best EMA pseudo Dice: 0.6913
2024-12-17 05:39:36.114199: 
2024-12-17 05:39:36.115884: Epoch 2
2024-12-17 05:39:36.116704: Current learning rate: 0.00988
2024-12-17 05:41:03.602217: Validation loss did not improve from -0.47873. Patience: 1/50
2024-12-17 05:41:03.603175: train_loss -0.4934
2024-12-17 05:41:03.604108: val_loss -0.4632
2024-12-17 05:41:03.604751: Pseudo dice [0.691]
2024-12-17 05:41:03.605620: Epoch time: 87.49 s
2024-12-17 05:41:04.921251: 
2024-12-17 05:41:04.923408: Epoch 3
2024-12-17 05:41:04.924426: Current learning rate: 0.00982
2024-12-17 05:42:32.543703: Validation loss improved from -0.47873 to -0.47952! Patience: 1/50
2024-12-17 05:42:32.544955: train_loss -0.5103
2024-12-17 05:42:32.545947: val_loss -0.4795
2024-12-17 05:42:32.546770: Pseudo dice [0.6865]
2024-12-17 05:42:32.547605: Epoch time: 87.63 s
2024-12-17 05:42:33.803974: 
2024-12-17 05:42:33.805412: Epoch 4
2024-12-17 05:42:33.806189: Current learning rate: 0.00976
2024-12-17 05:44:01.455490: Validation loss improved from -0.47952 to -0.52059! Patience: 0/50
2024-12-17 05:44:01.456768: train_loss -0.5225
2024-12-17 05:44:01.457658: val_loss -0.5206
2024-12-17 05:44:01.458443: Pseudo dice [0.7291]
2024-12-17 05:44:01.459118: Epoch time: 87.65 s
2024-12-17 05:44:01.785757: Yayy! New best EMA pseudo Dice: 0.6946
2024-12-17 05:44:03.390924: 
2024-12-17 05:44:03.392583: Epoch 5
2024-12-17 05:44:03.393478: Current learning rate: 0.0097
2024-12-17 05:45:31.027099: Validation loss improved from -0.52059 to -0.54684! Patience: 0/50
2024-12-17 05:45:31.028266: train_loss -0.537
2024-12-17 05:45:31.029163: val_loss -0.5468
2024-12-17 05:45:31.029927: Pseudo dice [0.7414]
2024-12-17 05:45:31.030563: Epoch time: 87.64 s
2024-12-17 05:45:31.031328: Yayy! New best EMA pseudo Dice: 0.6993
2024-12-17 05:45:32.543192: 
2024-12-17 05:45:32.544678: Epoch 6
2024-12-17 05:45:32.545388: Current learning rate: 0.00964
2024-12-17 05:47:00.251066: Validation loss did not improve from -0.54684. Patience: 1/50
2024-12-17 05:47:00.252396: train_loss -0.5487
2024-12-17 05:47:00.253304: val_loss -0.5025
2024-12-17 05:47:00.254102: Pseudo dice [0.7039]
2024-12-17 05:47:00.254946: Epoch time: 87.71 s
2024-12-17 05:47:00.255770: Yayy! New best EMA pseudo Dice: 0.6998
2024-12-17 05:47:01.800670: 
2024-12-17 05:47:01.802220: Epoch 7
2024-12-17 05:47:01.803246: Current learning rate: 0.00958
2024-12-17 05:48:29.463053: Validation loss improved from -0.54684 to -0.54751! Patience: 1/50
2024-12-17 05:48:29.464135: train_loss -0.5597
2024-12-17 05:48:29.464935: val_loss -0.5475
2024-12-17 05:48:29.465637: Pseudo dice [0.7429]
2024-12-17 05:48:29.466570: Epoch time: 87.66 s
2024-12-17 05:48:29.467292: Yayy! New best EMA pseudo Dice: 0.7041
2024-12-17 05:48:31.032553: 
2024-12-17 05:48:31.034056: Epoch 8
2024-12-17 05:48:31.034872: Current learning rate: 0.00952
2024-12-17 05:49:59.120891: Validation loss did not improve from -0.54751. Patience: 1/50
2024-12-17 05:49:59.121981: train_loss -0.5648
2024-12-17 05:49:59.122941: val_loss -0.5356
2024-12-17 05:49:59.123677: Pseudo dice [0.7361]
2024-12-17 05:49:59.124434: Epoch time: 88.09 s
2024-12-17 05:49:59.125177: Yayy! New best EMA pseudo Dice: 0.7073
2024-12-17 05:50:01.099429: 
2024-12-17 05:50:01.100983: Epoch 9
2024-12-17 05:50:01.101791: Current learning rate: 0.00946
2024-12-17 05:51:29.125395: Validation loss did not improve from -0.54751. Patience: 2/50
2024-12-17 05:51:29.126291: train_loss -0.58
2024-12-17 05:51:29.127167: val_loss -0.5371
2024-12-17 05:51:29.128000: Pseudo dice [0.7332]
2024-12-17 05:51:29.128827: Epoch time: 88.03 s
2024-12-17 05:51:29.498549: Yayy! New best EMA pseudo Dice: 0.7099
2024-12-17 05:51:31.019814: 
2024-12-17 05:51:31.022072: Epoch 10
2024-12-17 05:51:31.023100: Current learning rate: 0.0094
2024-12-17 05:52:59.029763: Validation loss did not improve from -0.54751. Patience: 3/50
2024-12-17 05:52:59.031226: train_loss -0.5858
2024-12-17 05:52:59.032372: val_loss -0.5434
2024-12-17 05:52:59.033296: Pseudo dice [0.7351]
2024-12-17 05:52:59.034037: Epoch time: 88.01 s
2024-12-17 05:52:59.034729: Yayy! New best EMA pseudo Dice: 0.7124
2024-12-17 05:53:00.603667: 
2024-12-17 05:53:00.605479: Epoch 11
2024-12-17 05:53:00.606443: Current learning rate: 0.00934
2024-12-17 05:54:28.715547: Validation loss did not improve from -0.54751. Patience: 4/50
2024-12-17 05:54:28.717105: train_loss -0.5745
2024-12-17 05:54:28.718137: val_loss -0.5431
2024-12-17 05:54:28.718830: Pseudo dice [0.7372]
2024-12-17 05:54:28.719656: Epoch time: 88.11 s
2024-12-17 05:54:28.720455: Yayy! New best EMA pseudo Dice: 0.7149
2024-12-17 05:54:30.286690: 
2024-12-17 05:54:30.288607: Epoch 12
2024-12-17 05:54:30.289348: Current learning rate: 0.00928
2024-12-17 05:55:58.359822: Validation loss did not improve from -0.54751. Patience: 5/50
2024-12-17 05:55:58.361054: train_loss -0.5933
2024-12-17 05:55:58.362049: val_loss -0.5322
2024-12-17 05:55:58.362764: Pseudo dice [0.7248]
2024-12-17 05:55:58.363403: Epoch time: 88.08 s
2024-12-17 05:55:58.364126: Yayy! New best EMA pseudo Dice: 0.7159
2024-12-17 05:55:59.934501: 
2024-12-17 05:55:59.935997: Epoch 13
2024-12-17 05:55:59.936799: Current learning rate: 0.00922
2024-12-17 05:57:27.939424: Validation loss did not improve from -0.54751. Patience: 6/50
2024-12-17 05:57:27.940627: train_loss -0.6017
2024-12-17 05:57:27.941726: val_loss -0.5196
2024-12-17 05:57:27.942684: Pseudo dice [0.7185]
2024-12-17 05:57:27.943447: Epoch time: 88.01 s
2024-12-17 05:57:27.944347: Yayy! New best EMA pseudo Dice: 0.7161
2024-12-17 05:57:29.543759: 
2024-12-17 05:57:29.545685: Epoch 14
2024-12-17 05:57:29.546691: Current learning rate: 0.00916
2024-12-17 05:58:57.503736: Validation loss did not improve from -0.54751. Patience: 7/50
2024-12-17 05:58:57.504859: train_loss -0.6168
2024-12-17 05:58:57.505899: val_loss -0.5444
2024-12-17 05:58:57.506624: Pseudo dice [0.7385]
2024-12-17 05:58:57.507432: Epoch time: 87.96 s
2024-12-17 05:58:57.875041: Yayy! New best EMA pseudo Dice: 0.7184
2024-12-17 05:58:59.431016: 
2024-12-17 05:58:59.433052: Epoch 15
2024-12-17 05:58:59.433950: Current learning rate: 0.0091
2024-12-17 06:00:27.391361: Validation loss improved from -0.54751 to -0.58644! Patience: 7/50
2024-12-17 06:00:27.392619: train_loss -0.6221
2024-12-17 06:00:27.393664: val_loss -0.5864
2024-12-17 06:00:27.394647: Pseudo dice [0.7642]
2024-12-17 06:00:27.395387: Epoch time: 87.96 s
2024-12-17 06:00:27.396038: Yayy! New best EMA pseudo Dice: 0.723
2024-12-17 06:00:29.093522: 
2024-12-17 06:00:29.095135: Epoch 16
2024-12-17 06:00:29.095922: Current learning rate: 0.00903
2024-12-17 06:01:57.384414: Validation loss did not improve from -0.58644. Patience: 1/50
2024-12-17 06:01:57.385569: train_loss -0.6133
2024-12-17 06:01:57.386478: val_loss -0.5581
2024-12-17 06:01:57.387219: Pseudo dice [0.7482]
2024-12-17 06:01:57.388033: Epoch time: 88.29 s
2024-12-17 06:01:57.388814: Yayy! New best EMA pseudo Dice: 0.7255
2024-12-17 06:01:58.990161: 
2024-12-17 06:01:58.991797: Epoch 17
2024-12-17 06:01:58.992750: Current learning rate: 0.00897
2024-12-17 06:03:27.323099: Validation loss did not improve from -0.58644. Patience: 2/50
2024-12-17 06:03:27.324515: train_loss -0.6247
2024-12-17 06:03:27.325566: val_loss -0.571
2024-12-17 06:03:27.326311: Pseudo dice [0.7552]
2024-12-17 06:03:27.327005: Epoch time: 88.34 s
2024-12-17 06:03:27.327637: Yayy! New best EMA pseudo Dice: 0.7285
2024-12-17 06:03:28.989918: 
2024-12-17 06:03:28.991509: Epoch 18
2024-12-17 06:03:28.992336: Current learning rate: 0.00891
2024-12-17 06:04:57.262184: Validation loss did not improve from -0.58644. Patience: 3/50
2024-12-17 06:04:57.263482: train_loss -0.6215
2024-12-17 06:04:57.264260: val_loss -0.575
2024-12-17 06:04:57.264870: Pseudo dice [0.7484]
2024-12-17 06:04:57.265573: Epoch time: 88.27 s
2024-12-17 06:04:57.266211: Yayy! New best EMA pseudo Dice: 0.7305
2024-12-17 06:04:59.186747: 
2024-12-17 06:04:59.188006: Epoch 19
2024-12-17 06:04:59.188707: Current learning rate: 0.00885
2024-12-17 06:06:27.568985: Validation loss improved from -0.58644 to -0.60126! Patience: 3/50
2024-12-17 06:06:27.569951: train_loss -0.6319
2024-12-17 06:06:27.570873: val_loss -0.6013
2024-12-17 06:06:27.571648: Pseudo dice [0.7709]
2024-12-17 06:06:27.572395: Epoch time: 88.38 s
2024-12-17 06:06:27.939326: Yayy! New best EMA pseudo Dice: 0.7345
2024-12-17 06:06:29.513021: 
2024-12-17 06:06:29.514375: Epoch 20
2024-12-17 06:06:29.515123: Current learning rate: 0.00879
2024-12-17 06:07:57.741811: Validation loss did not improve from -0.60126. Patience: 1/50
2024-12-17 06:07:57.742923: train_loss -0.6486
2024-12-17 06:07:57.743867: val_loss -0.5664
2024-12-17 06:07:57.744668: Pseudo dice [0.7457]
2024-12-17 06:07:57.745418: Epoch time: 88.23 s
2024-12-17 06:07:57.746273: Yayy! New best EMA pseudo Dice: 0.7356
2024-12-17 06:07:59.375272: 
2024-12-17 06:07:59.377081: Epoch 21
2024-12-17 06:07:59.377923: Current learning rate: 0.00873
2024-12-17 06:09:27.652992: Validation loss did not improve from -0.60126. Patience: 2/50
2024-12-17 06:09:27.654054: train_loss -0.6435
2024-12-17 06:09:27.654996: val_loss -0.5912
2024-12-17 06:09:27.655747: Pseudo dice [0.7633]
2024-12-17 06:09:27.656576: Epoch time: 88.28 s
2024-12-17 06:09:27.657317: Yayy! New best EMA pseudo Dice: 0.7384
2024-12-17 06:09:29.176158: 
2024-12-17 06:09:29.178095: Epoch 22
2024-12-17 06:09:29.178993: Current learning rate: 0.00867
2024-12-17 06:10:57.566661: Validation loss did not improve from -0.60126. Patience: 3/50
2024-12-17 06:10:57.567989: train_loss -0.6408
2024-12-17 06:10:57.569066: val_loss -0.5777
2024-12-17 06:10:57.570152: Pseudo dice [0.7575]
2024-12-17 06:10:57.571116: Epoch time: 88.39 s
2024-12-17 06:10:57.571935: Yayy! New best EMA pseudo Dice: 0.7403
2024-12-17 06:10:59.104834: 
2024-12-17 06:10:59.106583: Epoch 23
2024-12-17 06:10:59.107551: Current learning rate: 0.00861
2024-12-17 06:12:27.428316: Validation loss did not improve from -0.60126. Patience: 4/50
2024-12-17 06:12:27.429371: train_loss -0.6469
2024-12-17 06:12:27.430458: val_loss -0.5873
2024-12-17 06:12:27.431389: Pseudo dice [0.7626]
2024-12-17 06:12:27.432261: Epoch time: 88.33 s
2024-12-17 06:12:27.433041: Yayy! New best EMA pseudo Dice: 0.7425
2024-12-17 06:12:29.004315: 
2024-12-17 06:12:29.006061: Epoch 24
2024-12-17 06:12:29.007185: Current learning rate: 0.00855
2024-12-17 06:13:57.594538: Validation loss did not improve from -0.60126. Patience: 5/50
2024-12-17 06:13:57.595736: train_loss -0.6397
2024-12-17 06:13:57.596816: val_loss -0.5892
2024-12-17 06:13:57.597684: Pseudo dice [0.7614]
2024-12-17 06:13:57.598561: Epoch time: 88.59 s
2024-12-17 06:13:57.970099: Yayy! New best EMA pseudo Dice: 0.7444
2024-12-17 06:13:59.479120: 
2024-12-17 06:13:59.480652: Epoch 25
2024-12-17 06:13:59.481600: Current learning rate: 0.00849
2024-12-17 06:15:27.975615: Validation loss did not improve from -0.60126. Patience: 6/50
2024-12-17 06:15:27.977026: train_loss -0.6531
2024-12-17 06:15:27.978219: val_loss -0.5759
2024-12-17 06:15:27.978960: Pseudo dice [0.7527]
2024-12-17 06:15:27.979770: Epoch time: 88.5 s
2024-12-17 06:15:27.980522: Yayy! New best EMA pseudo Dice: 0.7452
2024-12-17 06:15:29.545183: 
2024-12-17 06:15:29.546844: Epoch 26
2024-12-17 06:15:29.547716: Current learning rate: 0.00843
2024-12-17 06:16:58.043280: Validation loss did not improve from -0.60126. Patience: 7/50
2024-12-17 06:16:58.044380: train_loss -0.6599
2024-12-17 06:16:58.045383: val_loss -0.5908
2024-12-17 06:16:58.046073: Pseudo dice [0.7649]
2024-12-17 06:16:58.046834: Epoch time: 88.5 s
2024-12-17 06:16:58.047495: Yayy! New best EMA pseudo Dice: 0.7472
2024-12-17 06:16:59.609052: 
2024-12-17 06:16:59.610232: Epoch 27
2024-12-17 06:16:59.610894: Current learning rate: 0.00836
2024-12-17 06:18:28.178949: Validation loss did not improve from -0.60126. Patience: 8/50
2024-12-17 06:18:28.179795: train_loss -0.66
2024-12-17 06:18:28.180789: val_loss -0.5649
2024-12-17 06:18:28.181621: Pseudo dice [0.7496]
2024-12-17 06:18:28.182385: Epoch time: 88.57 s
2024-12-17 06:18:28.183174: Yayy! New best EMA pseudo Dice: 0.7474
2024-12-17 06:18:29.750146: 
2024-12-17 06:18:29.751539: Epoch 28
2024-12-17 06:18:29.752364: Current learning rate: 0.0083
2024-12-17 06:19:58.277359: Validation loss did not improve from -0.60126. Patience: 9/50
2024-12-17 06:19:58.278656: train_loss -0.6648
2024-12-17 06:19:58.279948: val_loss -0.6011
2024-12-17 06:19:58.281015: Pseudo dice [0.771]
2024-12-17 06:19:58.282038: Epoch time: 88.53 s
2024-12-17 06:19:58.283068: Yayy! New best EMA pseudo Dice: 0.7498
2024-12-17 06:19:59.866995: 
2024-12-17 06:19:59.868930: Epoch 29
2024-12-17 06:19:59.870012: Current learning rate: 0.00824
2024-12-17 06:21:28.450174: Validation loss did not improve from -0.60126. Patience: 10/50
2024-12-17 06:21:28.451221: train_loss -0.6638
2024-12-17 06:21:28.452161: val_loss -0.5852
2024-12-17 06:21:28.453073: Pseudo dice [0.763]
2024-12-17 06:21:28.453999: Epoch time: 88.59 s
2024-12-17 06:21:28.831552: Yayy! New best EMA pseudo Dice: 0.7511
2024-12-17 06:21:30.703478: 
2024-12-17 06:21:30.705366: Epoch 30
2024-12-17 06:21:30.706028: Current learning rate: 0.00818
2024-12-17 06:22:59.164077: Validation loss did not improve from -0.60126. Patience: 11/50
2024-12-17 06:22:59.165244: train_loss -0.6678
2024-12-17 06:22:59.166431: val_loss -0.586
2024-12-17 06:22:59.167199: Pseudo dice [0.7605]
2024-12-17 06:22:59.168075: Epoch time: 88.46 s
2024-12-17 06:22:59.168814: Yayy! New best EMA pseudo Dice: 0.7521
2024-12-17 06:23:00.738150: 
2024-12-17 06:23:00.739741: Epoch 31
2024-12-17 06:23:00.740502: Current learning rate: 0.00812
2024-12-17 06:24:29.322409: Validation loss did not improve from -0.60126. Patience: 12/50
2024-12-17 06:24:29.323404: train_loss -0.6664
2024-12-17 06:24:29.324379: val_loss -0.5617
2024-12-17 06:24:29.325179: Pseudo dice [0.7452]
2024-12-17 06:24:29.325964: Epoch time: 88.59 s
2024-12-17 06:24:30.545764: 
2024-12-17 06:24:30.547402: Epoch 32
2024-12-17 06:24:30.548230: Current learning rate: 0.00806
2024-12-17 06:25:59.247391: Validation loss did not improve from -0.60126. Patience: 13/50
2024-12-17 06:25:59.248493: train_loss -0.6701
2024-12-17 06:25:59.249304: val_loss -0.5933
2024-12-17 06:25:59.249966: Pseudo dice [0.7672]
2024-12-17 06:25:59.250731: Epoch time: 88.7 s
2024-12-17 06:25:59.251462: Yayy! New best EMA pseudo Dice: 0.753
2024-12-17 06:26:00.840567: 
2024-12-17 06:26:00.842154: Epoch 33
2024-12-17 06:26:00.842949: Current learning rate: 0.008
2024-12-17 06:27:29.459672: Validation loss did not improve from -0.60126. Patience: 14/50
2024-12-17 06:27:29.460813: train_loss -0.6716
2024-12-17 06:27:29.461918: val_loss -0.5721
2024-12-17 06:27:29.462711: Pseudo dice [0.7589]
2024-12-17 06:27:29.463562: Epoch time: 88.62 s
2024-12-17 06:27:29.464351: Yayy! New best EMA pseudo Dice: 0.7536
2024-12-17 06:27:31.045163: 
2024-12-17 06:27:31.046311: Epoch 34
2024-12-17 06:27:31.047256: Current learning rate: 0.00793
2024-12-17 06:28:59.475799: Validation loss did not improve from -0.60126. Patience: 15/50
2024-12-17 06:28:59.476994: train_loss -0.6726
2024-12-17 06:28:59.478153: val_loss -0.5692
2024-12-17 06:28:59.479087: Pseudo dice [0.7516]
2024-12-17 06:28:59.480005: Epoch time: 88.43 s
2024-12-17 06:29:01.083695: 
2024-12-17 06:29:01.085489: Epoch 35
2024-12-17 06:29:01.086874: Current learning rate: 0.00787
2024-12-17 06:30:29.423790: Validation loss did not improve from -0.60126. Patience: 16/50
2024-12-17 06:30:29.424727: train_loss -0.6763
2024-12-17 06:30:29.425653: val_loss -0.5758
2024-12-17 06:30:29.426418: Pseudo dice [0.7518]
2024-12-17 06:30:29.427490: Epoch time: 88.34 s
2024-12-17 06:30:30.679955: 
2024-12-17 06:30:30.681967: Epoch 36
2024-12-17 06:30:30.682736: Current learning rate: 0.00781
2024-12-17 06:31:58.957112: Validation loss did not improve from -0.60126. Patience: 17/50
2024-12-17 06:31:58.958311: train_loss -0.6793
2024-12-17 06:31:58.959112: val_loss -0.5984
2024-12-17 06:31:58.959820: Pseudo dice [0.7648]
2024-12-17 06:31:58.960531: Epoch time: 88.28 s
2024-12-17 06:31:58.961296: Yayy! New best EMA pseudo Dice: 0.7544
2024-12-17 06:32:00.555746: 
2024-12-17 06:32:00.557597: Epoch 37
2024-12-17 06:32:00.558415: Current learning rate: 0.00775
2024-12-17 06:33:28.988713: Validation loss did not improve from -0.60126. Patience: 18/50
2024-12-17 06:33:28.989711: train_loss -0.6859
2024-12-17 06:33:28.990592: val_loss -0.5925
2024-12-17 06:33:28.991430: Pseudo dice [0.7653]
2024-12-17 06:33:28.992385: Epoch time: 88.43 s
2024-12-17 06:33:28.993071: Yayy! New best EMA pseudo Dice: 0.7555
2024-12-17 06:33:30.564723: 
2024-12-17 06:33:30.566487: Epoch 38
2024-12-17 06:33:30.567524: Current learning rate: 0.00769
2024-12-17 06:34:59.033597: Validation loss did not improve from -0.60126. Patience: 19/50
2024-12-17 06:34:59.034829: train_loss -0.6739
2024-12-17 06:34:59.037154: val_loss -0.5911
2024-12-17 06:34:59.037989: Pseudo dice [0.765]
2024-12-17 06:34:59.039320: Epoch time: 88.47 s
2024-12-17 06:34:59.040167: Yayy! New best EMA pseudo Dice: 0.7564
2024-12-17 06:35:00.725572: 
2024-12-17 06:35:00.727765: Epoch 39
2024-12-17 06:35:00.728534: Current learning rate: 0.00763
2024-12-17 06:36:29.089679: Validation loss did not improve from -0.60126. Patience: 20/50
2024-12-17 06:36:29.091059: train_loss -0.6806
2024-12-17 06:36:29.092251: val_loss -0.592
2024-12-17 06:36:29.093141: Pseudo dice [0.7667]
2024-12-17 06:36:29.093860: Epoch time: 88.37 s
2024-12-17 06:36:29.462802: Yayy! New best EMA pseudo Dice: 0.7574
2024-12-17 06:36:31.077423: 
2024-12-17 06:36:31.079230: Epoch 40
2024-12-17 06:36:31.080084: Current learning rate: 0.00756
2024-12-17 06:37:59.560506: Validation loss did not improve from -0.60126. Patience: 21/50
2024-12-17 06:37:59.561518: train_loss -0.6744
2024-12-17 06:37:59.562343: val_loss -0.5952
2024-12-17 06:37:59.563121: Pseudo dice [0.7687]
2024-12-17 06:37:59.563941: Epoch time: 88.49 s
2024-12-17 06:37:59.564536: Yayy! New best EMA pseudo Dice: 0.7586
2024-12-17 06:38:01.476038: 
2024-12-17 06:38:01.477317: Epoch 41
2024-12-17 06:38:01.477996: Current learning rate: 0.0075
2024-12-17 06:39:30.019418: Validation loss did not improve from -0.60126. Patience: 22/50
2024-12-17 06:39:30.020516: train_loss -0.6829
2024-12-17 06:39:30.021391: val_loss -0.5765
2024-12-17 06:39:30.022125: Pseudo dice [0.7617]
2024-12-17 06:39:30.022821: Epoch time: 88.55 s
2024-12-17 06:39:30.023490: Yayy! New best EMA pseudo Dice: 0.7589
2024-12-17 06:39:31.534454: 
2024-12-17 06:39:31.536108: Epoch 42
2024-12-17 06:39:31.536938: Current learning rate: 0.00744
2024-12-17 06:41:00.155954: Validation loss did not improve from -0.60126. Patience: 23/50
2024-12-17 06:41:00.173132: train_loss -0.6817
2024-12-17 06:41:00.173976: val_loss -0.5812
2024-12-17 06:41:00.174807: Pseudo dice [0.7572]
2024-12-17 06:41:00.175598: Epoch time: 88.64 s
2024-12-17 06:41:01.364851: 
2024-12-17 06:41:01.365968: Epoch 43
2024-12-17 06:41:01.366954: Current learning rate: 0.00738
2024-12-17 06:42:30.055859: Validation loss did not improve from -0.60126. Patience: 24/50
2024-12-17 06:42:30.057284: train_loss -0.6896
2024-12-17 06:42:30.058609: val_loss -0.5699
2024-12-17 06:42:30.059578: Pseudo dice [0.7516]
2024-12-17 06:42:30.060432: Epoch time: 88.69 s
2024-12-17 06:42:31.284268: 
2024-12-17 06:42:31.285430: Epoch 44
2024-12-17 06:42:31.286256: Current learning rate: 0.00732
2024-12-17 06:43:59.889538: Validation loss improved from -0.60126 to -0.60995! Patience: 24/50
2024-12-17 06:43:59.890786: train_loss -0.6871
2024-12-17 06:43:59.891835: val_loss -0.6099
2024-12-17 06:43:59.892766: Pseudo dice [0.7783]
2024-12-17 06:43:59.893650: Epoch time: 88.61 s
2024-12-17 06:44:00.266047: Yayy! New best EMA pseudo Dice: 0.76
2024-12-17 06:44:01.788133: 
2024-12-17 06:44:01.790004: Epoch 45
2024-12-17 06:44:01.790980: Current learning rate: 0.00725
2024-12-17 06:45:30.259983: Validation loss did not improve from -0.60995. Patience: 1/50
2024-12-17 06:45:30.261280: train_loss -0.6952
2024-12-17 06:45:30.262283: val_loss -0.6063
2024-12-17 06:45:30.262965: Pseudo dice [0.7719]
2024-12-17 06:45:30.263629: Epoch time: 88.47 s
2024-12-17 06:45:30.264312: Yayy! New best EMA pseudo Dice: 0.7612
2024-12-17 06:45:31.779379: 
2024-12-17 06:45:31.781087: Epoch 46
2024-12-17 06:45:31.781778: Current learning rate: 0.00719
2024-12-17 06:47:00.403887: Validation loss did not improve from -0.60995. Patience: 2/50
2024-12-17 06:47:00.405178: train_loss -0.6881
2024-12-17 06:47:00.406202: val_loss -0.5897
2024-12-17 06:47:00.406965: Pseudo dice [0.7658]
2024-12-17 06:47:00.407664: Epoch time: 88.63 s
2024-12-17 06:47:00.408334: Yayy! New best EMA pseudo Dice: 0.7617
2024-12-17 06:47:01.936552: 
2024-12-17 06:47:01.938343: Epoch 47
2024-12-17 06:47:01.939109: Current learning rate: 0.00713
2024-12-17 06:48:30.670012: Validation loss did not improve from -0.60995. Patience: 3/50
2024-12-17 06:48:30.671353: train_loss -0.6943
2024-12-17 06:48:30.672682: val_loss -0.5953
2024-12-17 06:48:30.673535: Pseudo dice [0.7736]
2024-12-17 06:48:30.674339: Epoch time: 88.74 s
2024-12-17 06:48:30.675120: Yayy! New best EMA pseudo Dice: 0.7629
2024-12-17 06:48:32.223655: 
2024-12-17 06:48:32.225371: Epoch 48
2024-12-17 06:48:32.226285: Current learning rate: 0.00707
2024-12-17 06:50:00.940286: Validation loss did not improve from -0.60995. Patience: 4/50
2024-12-17 06:50:00.941444: train_loss -0.7026
2024-12-17 06:50:00.942653: val_loss -0.6081
2024-12-17 06:50:00.943640: Pseudo dice [0.7805]
2024-12-17 06:50:00.944617: Epoch time: 88.72 s
2024-12-17 06:50:00.945562: Yayy! New best EMA pseudo Dice: 0.7646
2024-12-17 06:50:02.511588: 
2024-12-17 06:50:02.513746: Epoch 49
2024-12-17 06:50:02.514752: Current learning rate: 0.007
2024-12-17 06:51:31.340539: Validation loss did not improve from -0.60995. Patience: 5/50
2024-12-17 06:51:31.341616: train_loss -0.702
2024-12-17 06:51:31.342441: val_loss -0.5969
2024-12-17 06:51:31.343170: Pseudo dice [0.775]
2024-12-17 06:51:31.344132: Epoch time: 88.83 s
2024-12-17 06:51:31.717039: Yayy! New best EMA pseudo Dice: 0.7657
2024-12-17 06:51:33.277580: 
2024-12-17 06:51:33.279054: Epoch 50
2024-12-17 06:51:33.279885: Current learning rate: 0.00694
2024-12-17 06:53:02.016810: Validation loss did not improve from -0.60995. Patience: 6/50
2024-12-17 06:53:02.018126: train_loss -0.7037
2024-12-17 06:53:02.018851: val_loss -0.5844
2024-12-17 06:53:02.019610: Pseudo dice [0.767]
2024-12-17 06:53:02.020366: Epoch time: 88.74 s
2024-12-17 06:53:02.021027: Yayy! New best EMA pseudo Dice: 0.7658
2024-12-17 06:53:04.343067: 
2024-12-17 06:53:04.345402: Epoch 51
2024-12-17 06:53:04.346286: Current learning rate: 0.00688
2024-12-17 06:54:32.847262: Validation loss did not improve from -0.60995. Patience: 7/50
2024-12-17 06:54:32.848365: train_loss -0.6999
2024-12-17 06:54:32.849370: val_loss -0.5246
2024-12-17 06:54:32.850157: Pseudo dice [0.7325]
2024-12-17 06:54:32.850793: Epoch time: 88.51 s
2024-12-17 06:54:34.084760: 
2024-12-17 06:54:34.086521: Epoch 52
2024-12-17 06:54:34.087242: Current learning rate: 0.00682
2024-12-17 06:56:02.645455: Validation loss improved from -0.60995 to -0.61050! Patience: 7/50
2024-12-17 06:56:02.646515: train_loss -0.7066
2024-12-17 06:56:02.647566: val_loss -0.6105
2024-12-17 06:56:02.648462: Pseudo dice [0.7731]
2024-12-17 06:56:02.649230: Epoch time: 88.56 s
2024-12-17 06:56:03.891591: 
2024-12-17 06:56:03.893355: Epoch 53
2024-12-17 06:56:03.894200: Current learning rate: 0.00675
2024-12-17 06:57:32.405145: Validation loss did not improve from -0.61050. Patience: 1/50
2024-12-17 06:57:32.406531: train_loss -0.7096
2024-12-17 06:57:32.407579: val_loss -0.5957
2024-12-17 06:57:32.408317: Pseudo dice [0.7701]
2024-12-17 06:57:32.408992: Epoch time: 88.52 s
2024-12-17 06:57:33.632754: 
2024-12-17 06:57:33.634280: Epoch 54
2024-12-17 06:57:33.634908: Current learning rate: 0.00669
2024-12-17 06:59:02.092994: Validation loss did not improve from -0.61050. Patience: 2/50
2024-12-17 06:59:02.094136: train_loss -0.7033
2024-12-17 06:59:02.095138: val_loss -0.5732
2024-12-17 06:59:02.095908: Pseudo dice [0.7612]
2024-12-17 06:59:02.096762: Epoch time: 88.46 s
2024-12-17 06:59:03.712114: 
2024-12-17 06:59:03.713973: Epoch 55
2024-12-17 06:59:03.714803: Current learning rate: 0.00663
2024-12-17 07:00:32.057422: Validation loss improved from -0.61050 to -0.61144! Patience: 2/50
2024-12-17 07:00:32.058303: train_loss -0.71
2024-12-17 07:00:32.059171: val_loss -0.6114
2024-12-17 07:00:32.060184: Pseudo dice [0.7786]
2024-12-17 07:00:32.061174: Epoch time: 88.35 s
2024-12-17 07:00:33.328799: 
2024-12-17 07:00:33.330647: Epoch 56
2024-12-17 07:00:33.331655: Current learning rate: 0.00657
2024-12-17 07:02:01.841092: Validation loss did not improve from -0.61144. Patience: 1/50
2024-12-17 07:02:01.842392: train_loss -0.7143
2024-12-17 07:02:01.843350: val_loss -0.5805
2024-12-17 07:02:01.844306: Pseudo dice [0.7606]
2024-12-17 07:02:01.845348: Epoch time: 88.51 s
2024-12-17 07:02:03.081475: 
2024-12-17 07:02:03.083117: Epoch 57
2024-12-17 07:02:03.084197: Current learning rate: 0.0065
2024-12-17 07:03:31.402453: Validation loss did not improve from -0.61144. Patience: 2/50
2024-12-17 07:03:31.403724: train_loss -0.7125
2024-12-17 07:03:31.404600: val_loss -0.5717
2024-12-17 07:03:31.405265: Pseudo dice [0.7583]
2024-12-17 07:03:31.405930: Epoch time: 88.32 s
2024-12-17 07:03:32.621547: 
2024-12-17 07:03:32.623328: Epoch 58
2024-12-17 07:03:32.624155: Current learning rate: 0.00644
2024-12-17 07:05:01.195863: Validation loss did not improve from -0.61144. Patience: 3/50
2024-12-17 07:05:01.196928: train_loss -0.7093
2024-12-17 07:05:01.197669: val_loss -0.5507
2024-12-17 07:05:01.198330: Pseudo dice [0.7478]
2024-12-17 07:05:01.199001: Epoch time: 88.58 s
2024-12-17 07:05:02.412462: 
2024-12-17 07:05:02.414184: Epoch 59
2024-12-17 07:05:02.414994: Current learning rate: 0.00638
2024-12-17 07:06:31.089388: Validation loss did not improve from -0.61144. Patience: 4/50
2024-12-17 07:06:31.090726: train_loss -0.7155
2024-12-17 07:06:31.091819: val_loss -0.6028
2024-12-17 07:06:31.092649: Pseudo dice [0.7766]
2024-12-17 07:06:31.093561: Epoch time: 88.68 s
2024-12-17 07:06:32.662808: 
2024-12-17 07:06:32.664332: Epoch 60
2024-12-17 07:06:32.665126: Current learning rate: 0.00631
2024-12-17 07:08:01.313876: Validation loss did not improve from -0.61144. Patience: 5/50
2024-12-17 07:08:01.314938: train_loss -0.7188
2024-12-17 07:08:01.315872: val_loss -0.6037
2024-12-17 07:08:01.316726: Pseudo dice [0.7705]
2024-12-17 07:08:01.317626: Epoch time: 88.65 s
2024-12-17 07:08:02.513505: 
2024-12-17 07:08:02.515377: Epoch 61
2024-12-17 07:08:02.516203: Current learning rate: 0.00625
2024-12-17 07:09:31.255675: Validation loss did not improve from -0.61144. Patience: 6/50
2024-12-17 07:09:31.256906: train_loss -0.7107
2024-12-17 07:09:31.257990: val_loss -0.6081
2024-12-17 07:09:31.258704: Pseudo dice [0.7788]
2024-12-17 07:09:31.259620: Epoch time: 88.74 s
2024-12-17 07:09:31.260319: Yayy! New best EMA pseudo Dice: 0.7661
2024-12-17 07:09:33.284351: 
2024-12-17 07:09:33.286321: Epoch 62
2024-12-17 07:09:33.287207: Current learning rate: 0.00619
2024-12-17 07:11:01.911777: Validation loss did not improve from -0.61144. Patience: 7/50
2024-12-17 07:11:01.912853: train_loss -0.7119
2024-12-17 07:11:01.913697: val_loss -0.5517
2024-12-17 07:11:01.914395: Pseudo dice [0.7447]
2024-12-17 07:11:01.915267: Epoch time: 88.63 s
2024-12-17 07:11:03.189041: 
2024-12-17 07:11:03.190814: Epoch 63
2024-12-17 07:11:03.191704: Current learning rate: 0.00612
2024-12-17 07:12:31.774051: Validation loss did not improve from -0.61144. Patience: 8/50
2024-12-17 07:12:31.775192: train_loss -0.7223
2024-12-17 07:12:31.776154: val_loss -0.5826
2024-12-17 07:12:31.776906: Pseudo dice [0.7661]
2024-12-17 07:12:31.777695: Epoch time: 88.59 s
2024-12-17 07:12:32.998200: 
2024-12-17 07:12:32.999912: Epoch 64
2024-12-17 07:12:33.001045: Current learning rate: 0.00606
2024-12-17 07:14:01.737125: Validation loss did not improve from -0.61144. Patience: 9/50
2024-12-17 07:14:01.737949: train_loss -0.7289
2024-12-17 07:14:01.738812: val_loss -0.6098
2024-12-17 07:14:01.739610: Pseudo dice [0.7743]
2024-12-17 07:14:01.740527: Epoch time: 88.74 s
2024-12-17 07:14:03.332520: 
2024-12-17 07:14:03.334374: Epoch 65
2024-12-17 07:14:03.335418: Current learning rate: 0.006
2024-12-17 07:15:32.065268: Validation loss did not improve from -0.61144. Patience: 10/50
2024-12-17 07:15:32.066429: train_loss -0.7236
2024-12-17 07:15:32.067363: val_loss -0.5778
2024-12-17 07:15:32.068112: Pseudo dice [0.7601]
2024-12-17 07:15:32.068903: Epoch time: 88.73 s
2024-12-17 07:15:33.358087: 
2024-12-17 07:15:33.359793: Epoch 66
2024-12-17 07:15:33.360600: Current learning rate: 0.00593
2024-12-17 07:17:02.059581: Validation loss improved from -0.61144 to -0.61205! Patience: 10/50
2024-12-17 07:17:02.060658: train_loss -0.7285
2024-12-17 07:17:02.061408: val_loss -0.612
2024-12-17 07:17:02.062071: Pseudo dice [0.7817]
2024-12-17 07:17:02.062745: Epoch time: 88.7 s
2024-12-17 07:17:02.063452: Yayy! New best EMA pseudo Dice: 0.7663
2024-12-17 07:17:03.732537: 
2024-12-17 07:17:03.734403: Epoch 67
2024-12-17 07:17:03.735330: Current learning rate: 0.00587
2024-12-17 07:18:32.465920: Validation loss did not improve from -0.61205. Patience: 1/50
2024-12-17 07:18:32.466961: train_loss -0.7246
2024-12-17 07:18:32.467896: val_loss -0.603
2024-12-17 07:18:32.468673: Pseudo dice [0.7736]
2024-12-17 07:18:32.469348: Epoch time: 88.74 s
2024-12-17 07:18:32.470001: Yayy! New best EMA pseudo Dice: 0.7671
2024-12-17 07:18:34.102961: 
2024-12-17 07:18:34.104417: Epoch 68
2024-12-17 07:18:34.105332: Current learning rate: 0.00581
2024-12-17 07:20:02.882418: Validation loss did not improve from -0.61205. Patience: 2/50
2024-12-17 07:20:02.883567: train_loss -0.7256
2024-12-17 07:20:02.884453: val_loss -0.6033
2024-12-17 07:20:02.885097: Pseudo dice [0.7718]
2024-12-17 07:20:02.885849: Epoch time: 88.78 s
2024-12-17 07:20:02.886578: Yayy! New best EMA pseudo Dice: 0.7675
2024-12-17 07:20:04.627314: 
2024-12-17 07:20:04.629041: Epoch 69
2024-12-17 07:20:04.630041: Current learning rate: 0.00574
2024-12-17 07:21:33.452879: Validation loss did not improve from -0.61205. Patience: 3/50
2024-12-17 07:21:33.454251: train_loss -0.7288
2024-12-17 07:21:33.455419: val_loss -0.5925
2024-12-17 07:21:33.456315: Pseudo dice [0.7727]
2024-12-17 07:21:33.457276: Epoch time: 88.83 s
2024-12-17 07:21:33.826911: Yayy! New best EMA pseudo Dice: 0.7681
2024-12-17 07:21:35.454960: 
2024-12-17 07:21:35.456907: Epoch 70
2024-12-17 07:21:35.457832: Current learning rate: 0.00568
2024-12-17 07:23:04.048634: Validation loss did not improve from -0.61205. Patience: 4/50
2024-12-17 07:23:04.049739: train_loss -0.7262
2024-12-17 07:23:04.050702: val_loss -0.601
2024-12-17 07:23:04.051392: Pseudo dice [0.7772]
2024-12-17 07:23:04.052171: Epoch time: 88.6 s
2024-12-17 07:23:04.052888: Yayy! New best EMA pseudo Dice: 0.769
2024-12-17 07:23:05.681007: 
2024-12-17 07:23:05.682578: Epoch 71
2024-12-17 07:23:05.683458: Current learning rate: 0.00562
2024-12-17 07:24:34.200805: Validation loss did not improve from -0.61205. Patience: 5/50
2024-12-17 07:24:34.202017: train_loss -0.7349
2024-12-17 07:24:34.203241: val_loss -0.5721
2024-12-17 07:24:34.204169: Pseudo dice [0.7538]
2024-12-17 07:24:34.205068: Epoch time: 88.52 s
2024-12-17 07:24:35.421724: 
2024-12-17 07:24:35.422679: Epoch 72
2024-12-17 07:24:35.423521: Current learning rate: 0.00555
2024-12-17 07:26:03.933800: Validation loss did not improve from -0.61205. Patience: 6/50
2024-12-17 07:26:03.935023: train_loss -0.7329
2024-12-17 07:26:03.936098: val_loss -0.6081
2024-12-17 07:26:03.936926: Pseudo dice [0.777]
2024-12-17 07:26:03.937708: Epoch time: 88.51 s
2024-12-17 07:26:05.540772: 
2024-12-17 07:26:05.542146: Epoch 73
2024-12-17 07:26:05.542945: Current learning rate: 0.00549
2024-12-17 07:27:34.101897: Validation loss did not improve from -0.61205. Patience: 7/50
2024-12-17 07:27:34.103185: train_loss -0.7338
2024-12-17 07:27:34.103987: val_loss -0.5878
2024-12-17 07:27:34.104794: Pseudo dice [0.7637]
2024-12-17 07:27:34.105560: Epoch time: 88.56 s
2024-12-17 07:27:35.330689: 
2024-12-17 07:27:35.332451: Epoch 74
2024-12-17 07:27:35.333241: Current learning rate: 0.00542
2024-12-17 07:29:03.622457: Validation loss did not improve from -0.61205. Patience: 8/50
2024-12-17 07:29:03.623546: train_loss -0.7327
2024-12-17 07:29:03.624607: val_loss -0.5801
2024-12-17 07:29:03.625419: Pseudo dice [0.7632]
2024-12-17 07:29:03.626213: Epoch time: 88.29 s
2024-12-17 07:29:05.227594: 
2024-12-17 07:29:05.228940: Epoch 75
2024-12-17 07:29:05.229755: Current learning rate: 0.00536
2024-12-17 07:30:33.488795: Validation loss did not improve from -0.61205. Patience: 9/50
2024-12-17 07:30:33.489869: train_loss -0.7319
2024-12-17 07:30:33.491289: val_loss -0.6057
2024-12-17 07:30:33.492155: Pseudo dice [0.7773]
2024-12-17 07:30:33.492972: Epoch time: 88.26 s
2024-12-17 07:30:34.727942: 
2024-12-17 07:30:34.729120: Epoch 76
2024-12-17 07:30:34.730061: Current learning rate: 0.00529
2024-12-17 07:32:02.943814: Validation loss improved from -0.61205 to -0.62615! Patience: 9/50
2024-12-17 07:32:02.944827: train_loss -0.734
2024-12-17 07:32:02.945771: val_loss -0.6262
2024-12-17 07:32:02.946627: Pseudo dice [0.7853]
2024-12-17 07:32:02.947408: Epoch time: 88.22 s
2024-12-17 07:32:02.948147: Yayy! New best EMA pseudo Dice: 0.7701
2024-12-17 07:32:04.514088: 
2024-12-17 07:32:04.515360: Epoch 77
2024-12-17 07:32:04.516184: Current learning rate: 0.00523
2024-12-17 07:33:32.698865: Validation loss did not improve from -0.62615. Patience: 1/50
2024-12-17 07:33:32.700118: train_loss -0.7388
2024-12-17 07:33:32.701156: val_loss -0.5966
2024-12-17 07:33:32.702187: Pseudo dice [0.7707]
2024-12-17 07:33:32.702979: Epoch time: 88.19 s
2024-12-17 07:33:32.703725: Yayy! New best EMA pseudo Dice: 0.7702
2024-12-17 07:33:34.294785: 
2024-12-17 07:33:34.296314: Epoch 78
2024-12-17 07:33:34.297100: Current learning rate: 0.00517
2024-12-17 07:35:02.566036: Validation loss did not improve from -0.62615. Patience: 2/50
2024-12-17 07:35:02.567081: train_loss -0.7362
2024-12-17 07:35:02.568131: val_loss -0.5812
2024-12-17 07:35:02.568932: Pseudo dice [0.7666]
2024-12-17 07:35:02.569696: Epoch time: 88.27 s
2024-12-17 07:35:03.869746: 
2024-12-17 07:35:03.871247: Epoch 79
2024-12-17 07:35:03.872097: Current learning rate: 0.0051
2024-12-17 07:36:32.192497: Validation loss did not improve from -0.62615. Patience: 3/50
2024-12-17 07:36:32.193719: train_loss -0.7377
2024-12-17 07:36:32.194821: val_loss -0.5897
2024-12-17 07:36:32.195529: Pseudo dice [0.7664]
2024-12-17 07:36:32.196271: Epoch time: 88.33 s
2024-12-17 07:36:33.775979: 
2024-12-17 07:36:33.776909: Epoch 80
2024-12-17 07:36:33.777647: Current learning rate: 0.00504
2024-12-17 07:38:02.034588: Validation loss did not improve from -0.62615. Patience: 4/50
2024-12-17 07:38:02.035869: train_loss -0.7409
2024-12-17 07:38:02.037078: val_loss -0.5747
2024-12-17 07:38:02.038039: Pseudo dice [0.7621]
2024-12-17 07:38:02.039003: Epoch time: 88.26 s
2024-12-17 07:38:03.310062: 
2024-12-17 07:38:03.311585: Epoch 81
2024-12-17 07:38:03.312585: Current learning rate: 0.00497
2024-12-17 07:39:31.332477: Validation loss did not improve from -0.62615. Patience: 5/50
2024-12-17 07:39:31.333854: train_loss -0.7388
2024-12-17 07:39:31.335112: val_loss -0.5755
2024-12-17 07:39:31.335915: Pseudo dice [0.7573]
2024-12-17 07:39:31.336663: Epoch time: 88.02 s
2024-12-17 07:39:32.672241: 
2024-12-17 07:39:32.673847: Epoch 82
2024-12-17 07:39:32.674711: Current learning rate: 0.00491
2024-12-17 07:41:01.117339: Validation loss did not improve from -0.62615. Patience: 6/50
2024-12-17 07:41:01.119534: train_loss -0.7369
2024-12-17 07:41:01.120529: val_loss -0.5723
2024-12-17 07:41:01.121334: Pseudo dice [0.7585]
2024-12-17 07:41:01.122150: Epoch time: 88.45 s
2024-12-17 07:41:02.323832: 
2024-12-17 07:41:02.325448: Epoch 83
2024-12-17 07:41:02.326323: Current learning rate: 0.00484
2024-12-17 07:42:30.710469: Validation loss did not improve from -0.62615. Patience: 7/50
2024-12-17 07:42:30.711711: train_loss -0.7427
2024-12-17 07:42:30.712553: val_loss -0.5782
2024-12-17 07:42:30.713342: Pseudo dice [0.7602]
2024-12-17 07:42:30.714022: Epoch time: 88.39 s
2024-12-17 07:42:32.296273: 
2024-12-17 07:42:32.297542: Epoch 84
2024-12-17 07:42:32.298326: Current learning rate: 0.00478
2024-12-17 07:44:00.626004: Validation loss did not improve from -0.62615. Patience: 8/50
2024-12-17 07:44:00.626834: train_loss -0.7353
2024-12-17 07:44:00.627661: val_loss -0.6135
2024-12-17 07:44:00.628424: Pseudo dice [0.7803]
2024-12-17 07:44:00.629111: Epoch time: 88.33 s
2024-12-17 07:44:02.155740: 
2024-12-17 07:44:02.157411: Epoch 85
2024-12-17 07:44:02.158227: Current learning rate: 0.00471
2024-12-17 07:45:30.538391: Validation loss did not improve from -0.62615. Patience: 9/50
2024-12-17 07:45:30.539428: train_loss -0.7426
2024-12-17 07:45:30.540350: val_loss -0.6016
2024-12-17 07:45:30.541064: Pseudo dice [0.7733]
2024-12-17 07:45:30.541852: Epoch time: 88.38 s
2024-12-17 07:45:31.705537: 
2024-12-17 07:45:31.707131: Epoch 86
2024-12-17 07:45:31.707857: Current learning rate: 0.00465
2024-12-17 07:47:00.060837: Validation loss improved from -0.62615 to -0.64085! Patience: 9/50
2024-12-17 07:47:00.062139: train_loss -0.7422
2024-12-17 07:47:00.063262: val_loss -0.6408
2024-12-17 07:47:00.064217: Pseudo dice [0.7953]
2024-12-17 07:47:00.065243: Epoch time: 88.36 s
2024-12-17 07:47:00.066139: Yayy! New best EMA pseudo Dice: 0.7708
2024-12-17 07:47:01.596349: 
2024-12-17 07:47:01.597822: Epoch 87
2024-12-17 07:47:01.598788: Current learning rate: 0.00458
2024-12-17 07:48:29.868670: Validation loss did not improve from -0.64085. Patience: 1/50
2024-12-17 07:48:29.869601: train_loss -0.7464
2024-12-17 07:48:29.870557: val_loss -0.5928
2024-12-17 07:48:29.871178: Pseudo dice [0.7691]
2024-12-17 07:48:29.871826: Epoch time: 88.27 s
2024-12-17 07:48:31.057184: 
2024-12-17 07:48:31.058556: Epoch 88
2024-12-17 07:48:31.059265: Current learning rate: 0.00452
2024-12-17 07:49:59.356021: Validation loss did not improve from -0.64085. Patience: 2/50
2024-12-17 07:49:59.356835: train_loss -0.7485
2024-12-17 07:49:59.357769: val_loss -0.6127
2024-12-17 07:49:59.358523: Pseudo dice [0.7811]
2024-12-17 07:49:59.359330: Epoch time: 88.3 s
2024-12-17 07:49:59.359982: Yayy! New best EMA pseudo Dice: 0.7717
2024-12-17 07:50:00.909363: 
2024-12-17 07:50:00.911006: Epoch 89
2024-12-17 07:50:00.911937: Current learning rate: 0.00445
2024-12-17 07:51:29.220186: Validation loss did not improve from -0.64085. Patience: 3/50
2024-12-17 07:51:29.221344: train_loss -0.7487
2024-12-17 07:51:29.222088: val_loss -0.5831
2024-12-17 07:51:29.222823: Pseudo dice [0.7601]
2024-12-17 07:51:29.223639: Epoch time: 88.31 s
2024-12-17 07:51:30.750163: 
2024-12-17 07:51:30.751584: Epoch 90
2024-12-17 07:51:30.752287: Current learning rate: 0.00438
2024-12-17 07:52:59.222638: Validation loss did not improve from -0.64085. Patience: 4/50
2024-12-17 07:52:59.223679: train_loss -0.7515
2024-12-17 07:52:59.224833: val_loss -0.5953
2024-12-17 07:52:59.225598: Pseudo dice [0.7739]
2024-12-17 07:52:59.226218: Epoch time: 88.47 s
2024-12-17 07:53:00.389589: 
2024-12-17 07:53:00.391294: Epoch 91
2024-12-17 07:53:00.392108: Current learning rate: 0.00432
2024-12-17 07:54:29.070179: Validation loss did not improve from -0.64085. Patience: 5/50
2024-12-17 07:54:29.071031: train_loss -0.7505
2024-12-17 07:54:29.071784: val_loss -0.5771
2024-12-17 07:54:29.072410: Pseudo dice [0.7566]
2024-12-17 07:54:29.073254: Epoch time: 88.68 s
2024-12-17 07:54:30.254953: 
2024-12-17 07:54:30.256555: Epoch 92
2024-12-17 07:54:30.257481: Current learning rate: 0.00425
2024-12-17 07:55:58.656923: Validation loss did not improve from -0.64085. Patience: 6/50
2024-12-17 07:55:58.658290: train_loss -0.7525
2024-12-17 07:55:58.659273: val_loss -0.6069
2024-12-17 07:55:58.659988: Pseudo dice [0.7714]
2024-12-17 07:55:58.660685: Epoch time: 88.4 s
2024-12-17 07:55:59.934862: 
2024-12-17 07:55:59.936433: Epoch 93
2024-12-17 07:55:59.937177: Current learning rate: 0.00419
2024-12-17 07:57:28.326295: Validation loss did not improve from -0.64085. Patience: 7/50
2024-12-17 07:57:28.327378: train_loss -0.748
2024-12-17 07:57:28.328202: val_loss -0.6064
2024-12-17 07:57:28.329094: Pseudo dice [0.7737]
2024-12-17 07:57:28.329953: Epoch time: 88.39 s
2024-12-17 07:57:29.509629: 
2024-12-17 07:57:29.511606: Epoch 94
2024-12-17 07:57:29.512433: Current learning rate: 0.00412
2024-12-17 07:58:57.974585: Validation loss did not improve from -0.64085. Patience: 8/50
2024-12-17 07:58:57.975696: train_loss -0.751
2024-12-17 07:58:57.976925: val_loss -0.6131
2024-12-17 07:58:57.977945: Pseudo dice [0.78]
2024-12-17 07:58:57.978891: Epoch time: 88.47 s
2024-12-17 07:58:59.553468: 
2024-12-17 07:58:59.555495: Epoch 95
2024-12-17 07:58:59.556366: Current learning rate: 0.00405
2024-12-17 08:00:27.816331: Validation loss did not improve from -0.64085. Patience: 9/50
2024-12-17 08:00:27.817537: train_loss -0.7545
2024-12-17 08:00:27.818439: val_loss -0.6114
2024-12-17 08:00:27.819204: Pseudo dice [0.7754]
2024-12-17 08:00:27.819918: Epoch time: 88.26 s
2024-12-17 08:00:29.413668: 
2024-12-17 08:00:29.415376: Epoch 96
2024-12-17 08:00:29.416291: Current learning rate: 0.00399
2024-12-17 08:01:57.691357: Validation loss did not improve from -0.64085. Patience: 10/50
2024-12-17 08:01:57.692067: train_loss -0.7498
2024-12-17 08:01:57.693049: val_loss -0.5927
2024-12-17 08:01:57.693800: Pseudo dice [0.7752]
2024-12-17 08:01:57.694605: Epoch time: 88.28 s
2024-12-17 08:01:57.695331: Yayy! New best EMA pseudo Dice: 0.7718
2024-12-17 08:01:59.355906: 
2024-12-17 08:01:59.357530: Epoch 97
2024-12-17 08:01:59.358427: Current learning rate: 0.00392
2024-12-17 08:03:27.748867: Validation loss did not improve from -0.64085. Patience: 11/50
2024-12-17 08:03:27.750312: train_loss -0.7523
2024-12-17 08:03:27.751251: val_loss -0.599
2024-12-17 08:03:27.752028: Pseudo dice [0.7705]
2024-12-17 08:03:27.752795: Epoch time: 88.4 s
2024-12-17 08:03:28.979807: 
2024-12-17 08:03:28.981848: Epoch 98
2024-12-17 08:03:28.982537: Current learning rate: 0.00385
2024-12-17 08:04:57.410088: Validation loss did not improve from -0.64085. Patience: 12/50
2024-12-17 08:04:57.411040: train_loss -0.7511
2024-12-17 08:04:57.411837: val_loss -0.5871
2024-12-17 08:04:57.412594: Pseudo dice [0.77]
2024-12-17 08:04:57.413280: Epoch time: 88.43 s
2024-12-17 08:04:58.662654: 
2024-12-17 08:04:58.664543: Epoch 99
2024-12-17 08:04:58.665332: Current learning rate: 0.00379
2024-12-17 08:06:27.079619: Validation loss did not improve from -0.64085. Patience: 13/50
2024-12-17 08:06:27.080790: train_loss -0.7507
2024-12-17 08:06:27.081614: val_loss -0.6322
2024-12-17 08:06:27.082356: Pseudo dice [0.7916]
2024-12-17 08:06:27.083115: Epoch time: 88.42 s
2024-12-17 08:06:27.448521: Yayy! New best EMA pseudo Dice: 0.7735
2024-12-17 08:06:29.011437: 
2024-12-17 08:06:29.013466: Epoch 100
2024-12-17 08:06:29.014241: Current learning rate: 0.00372
2024-12-17 08:07:57.645067: Validation loss did not improve from -0.64085. Patience: 14/50
2024-12-17 08:07:57.646271: train_loss -0.7619
2024-12-17 08:07:57.647207: val_loss -0.6119
2024-12-17 08:07:57.648117: Pseudo dice [0.7834]
2024-12-17 08:07:57.648812: Epoch time: 88.64 s
2024-12-17 08:07:57.649616: Yayy! New best EMA pseudo Dice: 0.7745
2024-12-17 08:07:59.261420: 
2024-12-17 08:07:59.263410: Epoch 101
2024-12-17 08:07:59.264292: Current learning rate: 0.00365
2024-12-17 08:09:27.870671: Validation loss did not improve from -0.64085. Patience: 15/50
2024-12-17 08:09:27.871706: train_loss -0.7621
2024-12-17 08:09:27.872732: val_loss -0.6038
2024-12-17 08:09:27.873476: Pseudo dice [0.7825]
2024-12-17 08:09:27.874206: Epoch time: 88.61 s
2024-12-17 08:09:27.874967: Yayy! New best EMA pseudo Dice: 0.7753
2024-12-17 08:09:29.428729: 
2024-12-17 08:09:29.430016: Epoch 102
2024-12-17 08:09:29.430806: Current learning rate: 0.00359
2024-12-17 08:10:58.001304: Validation loss did not improve from -0.64085. Patience: 16/50
2024-12-17 08:10:58.002501: train_loss -0.7572
2024-12-17 08:10:58.003758: val_loss -0.6248
2024-12-17 08:10:58.004657: Pseudo dice [0.7901]
2024-12-17 08:10:58.005481: Epoch time: 88.57 s
2024-12-17 08:10:58.006401: Yayy! New best EMA pseudo Dice: 0.7768
2024-12-17 08:10:59.603099: 
2024-12-17 08:10:59.604706: Epoch 103
2024-12-17 08:10:59.605663: Current learning rate: 0.00352
2024-12-17 08:12:28.165073: Validation loss did not improve from -0.64085. Patience: 17/50
2024-12-17 08:12:28.166211: train_loss -0.7579
2024-12-17 08:12:28.166950: val_loss -0.6225
2024-12-17 08:12:28.167679: Pseudo dice [0.7811]
2024-12-17 08:12:28.168445: Epoch time: 88.56 s
2024-12-17 08:12:28.169117: Yayy! New best EMA pseudo Dice: 0.7772
2024-12-17 08:12:29.761220: 
2024-12-17 08:12:29.762992: Epoch 104
2024-12-17 08:12:29.763974: Current learning rate: 0.00345
2024-12-17 08:13:58.284359: Validation loss did not improve from -0.64085. Patience: 18/50
2024-12-17 08:13:58.285625: train_loss -0.7582
2024-12-17 08:13:58.286754: val_loss -0.578
2024-12-17 08:13:58.287561: Pseudo dice [0.7577]
2024-12-17 08:13:58.288465: Epoch time: 88.53 s
2024-12-17 08:13:59.906455: 
2024-12-17 08:13:59.908469: Epoch 105
2024-12-17 08:13:59.909530: Current learning rate: 0.00338
2024-12-17 08:15:28.445059: Validation loss did not improve from -0.64085. Patience: 19/50
2024-12-17 08:15:28.446288: train_loss -0.7601
2024-12-17 08:15:28.447407: val_loss -0.5884
2024-12-17 08:15:28.448421: Pseudo dice [0.7696]
2024-12-17 08:15:28.449280: Epoch time: 88.54 s
2024-12-17 08:15:29.985188: 
2024-12-17 08:15:29.987093: Epoch 106
2024-12-17 08:15:29.988058: Current learning rate: 0.00332
2024-12-17 08:16:58.567483: Validation loss did not improve from -0.64085. Patience: 20/50
2024-12-17 08:16:58.568428: train_loss -0.7591
2024-12-17 08:16:58.569296: val_loss -0.5634
2024-12-17 08:16:58.570102: Pseudo dice [0.7524]
2024-12-17 08:16:58.570913: Epoch time: 88.58 s
2024-12-17 08:16:59.777396: 
2024-12-17 08:16:59.778839: Epoch 107
2024-12-17 08:16:59.779759: Current learning rate: 0.00325
2024-12-17 08:18:28.313754: Validation loss did not improve from -0.64085. Patience: 21/50
2024-12-17 08:18:28.315028: train_loss -0.7614
2024-12-17 08:18:28.316075: val_loss -0.5943
2024-12-17 08:18:28.316856: Pseudo dice [0.7704]
2024-12-17 08:18:28.317760: Epoch time: 88.54 s
2024-12-17 08:18:29.548995: 
2024-12-17 08:18:29.550862: Epoch 108
2024-12-17 08:18:29.551652: Current learning rate: 0.00318
2024-12-17 08:19:58.306643: Validation loss did not improve from -0.64085. Patience: 22/50
2024-12-17 08:19:58.307666: train_loss -0.7621
2024-12-17 08:19:58.308510: val_loss -0.5761
2024-12-17 08:19:58.309182: Pseudo dice [0.7513]
2024-12-17 08:19:58.309912: Epoch time: 88.76 s
2024-12-17 08:19:59.529619: 
2024-12-17 08:19:59.531230: Epoch 109
2024-12-17 08:19:59.531961: Current learning rate: 0.00311
2024-12-17 08:21:28.258452: Validation loss did not improve from -0.64085. Patience: 23/50
2024-12-17 08:21:28.259759: train_loss -0.7697
2024-12-17 08:21:28.261019: val_loss -0.6042
2024-12-17 08:21:28.261884: Pseudo dice [0.7811]
2024-12-17 08:21:28.262826: Epoch time: 88.73 s
2024-12-17 08:21:29.804257: 
2024-12-17 08:21:29.805967: Epoch 110
2024-12-17 08:21:29.806748: Current learning rate: 0.00304
2024-12-17 08:22:58.305995: Validation loss did not improve from -0.64085. Patience: 24/50
2024-12-17 08:22:58.306905: train_loss -0.7634
2024-12-17 08:22:58.307910: val_loss -0.5864
2024-12-17 08:22:58.308673: Pseudo dice [0.7632]
2024-12-17 08:22:58.309444: Epoch time: 88.5 s
2024-12-17 08:22:59.507474: 
2024-12-17 08:22:59.508710: Epoch 111
2024-12-17 08:22:59.509683: Current learning rate: 0.00297
2024-12-17 08:24:27.936305: Validation loss did not improve from -0.64085. Patience: 25/50
2024-12-17 08:24:27.937453: train_loss -0.7674
2024-12-17 08:24:27.938634: val_loss -0.6019
2024-12-17 08:24:27.939543: Pseudo dice [0.7804]
2024-12-17 08:24:27.940668: Epoch time: 88.43 s
2024-12-17 08:24:29.162536: 
2024-12-17 08:24:29.164434: Epoch 112
2024-12-17 08:24:29.165226: Current learning rate: 0.00291
2024-12-17 08:25:57.663739: Validation loss did not improve from -0.64085. Patience: 26/50
2024-12-17 08:25:57.664896: train_loss -0.7689
2024-12-17 08:25:57.665878: val_loss -0.5861
2024-12-17 08:25:57.666744: Pseudo dice [0.7654]
2024-12-17 08:25:57.667442: Epoch time: 88.5 s
2024-12-17 08:25:58.940691: 
2024-12-17 08:25:58.942071: Epoch 113
2024-12-17 08:25:58.942928: Current learning rate: 0.00284
2024-12-17 08:27:27.528603: Validation loss did not improve from -0.64085. Patience: 27/50
2024-12-17 08:27:27.529620: train_loss -0.7671
2024-12-17 08:27:27.530644: val_loss -0.5711
2024-12-17 08:27:27.531500: Pseudo dice [0.7615]
2024-12-17 08:27:27.532175: Epoch time: 88.59 s
2024-12-17 08:27:28.736445: 
2024-12-17 08:27:28.738139: Epoch 114
2024-12-17 08:27:28.738830: Current learning rate: 0.00277
2024-12-17 08:28:57.259080: Validation loss did not improve from -0.64085. Patience: 28/50
2024-12-17 08:28:57.260020: train_loss -0.7669
2024-12-17 08:28:57.261006: val_loss -0.6091
2024-12-17 08:28:57.262048: Pseudo dice [0.7744]
2024-12-17 08:28:57.263013: Epoch time: 88.52 s
2024-12-17 08:28:58.796654: 
2024-12-17 08:28:58.798228: Epoch 115
2024-12-17 08:28:58.799254: Current learning rate: 0.0027
2024-12-17 08:30:27.273403: Validation loss did not improve from -0.64085. Patience: 29/50
2024-12-17 08:30:27.274533: train_loss -0.7703
2024-12-17 08:30:27.275503: val_loss -0.5776
2024-12-17 08:30:27.276481: Pseudo dice [0.7593]
2024-12-17 08:30:27.277341: Epoch time: 88.48 s
2024-12-17 08:30:28.492145: 
2024-12-17 08:30:28.494063: Epoch 116
2024-12-17 08:30:28.495109: Current learning rate: 0.00263
2024-12-17 08:31:56.990525: Validation loss did not improve from -0.64085. Patience: 30/50
2024-12-17 08:31:56.991851: train_loss -0.7719
2024-12-17 08:31:56.992789: val_loss -0.6108
2024-12-17 08:31:56.993487: Pseudo dice [0.7873]
2024-12-17 08:31:56.994226: Epoch time: 88.5 s
2024-12-17 08:31:58.220643: 
2024-12-17 08:31:58.222117: Epoch 117
2024-12-17 08:31:58.222885: Current learning rate: 0.00256
2024-12-17 08:33:26.721064: Validation loss did not improve from -0.64085. Patience: 31/50
2024-12-17 08:33:26.722049: train_loss -0.77
2024-12-17 08:33:26.723274: val_loss -0.585
2024-12-17 08:33:26.724192: Pseudo dice [0.7726]
2024-12-17 08:33:26.725281: Epoch time: 88.5 s
2024-12-17 08:33:28.270533: 
2024-12-17 08:33:28.272253: Epoch 118
2024-12-17 08:33:28.273079: Current learning rate: 0.00249
2024-12-17 08:34:56.917824: Validation loss did not improve from -0.64085. Patience: 32/50
2024-12-17 08:34:56.918664: train_loss -0.7673
2024-12-17 08:34:56.919516: val_loss -0.5785
2024-12-17 08:34:56.920279: Pseudo dice [0.7664]
2024-12-17 08:34:56.921072: Epoch time: 88.65 s
2024-12-17 08:34:58.184191: 
2024-12-17 08:34:58.186115: Epoch 119
2024-12-17 08:34:58.186922: Current learning rate: 0.00242
2024-12-17 08:36:26.816329: Validation loss did not improve from -0.64085. Patience: 33/50
2024-12-17 08:36:26.817486: train_loss -0.7739
2024-12-17 08:36:26.818465: val_loss -0.5922
2024-12-17 08:36:26.819222: Pseudo dice [0.7727]
2024-12-17 08:36:26.820128: Epoch time: 88.63 s
2024-12-17 08:36:28.473749: 
2024-12-17 08:36:28.475208: Epoch 120
2024-12-17 08:36:28.476104: Current learning rate: 0.00235
2024-12-17 08:37:56.935411: Validation loss did not improve from -0.64085. Patience: 34/50
2024-12-17 08:37:56.936127: train_loss -0.7699
2024-12-17 08:37:56.937016: val_loss -0.5805
2024-12-17 08:37:56.937744: Pseudo dice [0.762]
2024-12-17 08:37:56.938425: Epoch time: 88.46 s
2024-12-17 08:37:58.213508: 
2024-12-17 08:37:58.215263: Epoch 121
2024-12-17 08:37:58.216034: Current learning rate: 0.00228
2024-12-17 08:39:26.747225: Validation loss did not improve from -0.64085. Patience: 35/50
2024-12-17 08:39:26.748445: train_loss -0.769
2024-12-17 08:39:26.749325: val_loss -0.6306
2024-12-17 08:39:26.750128: Pseudo dice [0.7943]
2024-12-17 08:39:26.750812: Epoch time: 88.54 s
2024-12-17 08:39:27.999701: 
2024-12-17 08:39:28.001413: Epoch 122
2024-12-17 08:39:28.002431: Current learning rate: 0.00221
2024-12-17 08:40:56.640123: Validation loss did not improve from -0.64085. Patience: 36/50
2024-12-17 08:40:56.641290: train_loss -0.7734
2024-12-17 08:40:56.642250: val_loss -0.5955
2024-12-17 08:40:56.643162: Pseudo dice [0.7702]
2024-12-17 08:40:56.643885: Epoch time: 88.64 s
2024-12-17 08:40:57.896185: 
2024-12-17 08:40:57.897708: Epoch 123
2024-12-17 08:40:57.898570: Current learning rate: 0.00214
2024-12-17 08:42:26.565218: Validation loss did not improve from -0.64085. Patience: 37/50
2024-12-17 08:42:26.565896: train_loss -0.7732
2024-12-17 08:42:26.566710: val_loss -0.6055
2024-12-17 08:42:26.567431: Pseudo dice [0.778]
2024-12-17 08:42:26.568109: Epoch time: 88.67 s
2024-12-17 08:42:27.817560: 
2024-12-17 08:42:27.818639: Epoch 124
2024-12-17 08:42:27.819512: Current learning rate: 0.00207
2024-12-17 08:43:56.515861: Validation loss did not improve from -0.64085. Patience: 38/50
2024-12-17 08:43:56.517135: train_loss -0.7753
2024-12-17 08:43:56.518148: val_loss -0.6053
2024-12-17 08:43:56.518937: Pseudo dice [0.777]
2024-12-17 08:43:56.519750: Epoch time: 88.7 s
2024-12-17 08:43:58.151491: 
2024-12-17 08:43:58.153224: Epoch 125
2024-12-17 08:43:58.154045: Current learning rate: 0.00199
2024-12-17 08:45:26.887539: Validation loss did not improve from -0.64085. Patience: 39/50
2024-12-17 08:45:26.890498: train_loss -0.7735
2024-12-17 08:45:26.892478: val_loss -0.602
2024-12-17 08:45:26.893453: Pseudo dice [0.7669]
2024-12-17 08:45:26.895269: Epoch time: 88.74 s
2024-12-17 08:45:28.169223: 
2024-12-17 08:45:28.170930: Epoch 126
2024-12-17 08:45:28.171719: Current learning rate: 0.00192
2024-12-17 08:46:56.658835: Validation loss did not improve from -0.64085. Patience: 40/50
2024-12-17 08:46:56.659909: train_loss -0.7772
2024-12-17 08:46:56.660742: val_loss -0.6059
2024-12-17 08:46:56.661571: Pseudo dice [0.7686]
2024-12-17 08:46:56.662317: Epoch time: 88.49 s
2024-12-17 08:46:57.915836: 
2024-12-17 08:46:57.917432: Epoch 127
2024-12-17 08:46:57.918189: Current learning rate: 0.00185
2024-12-17 08:48:26.337197: Validation loss did not improve from -0.64085. Patience: 41/50
2024-12-17 08:48:26.338395: train_loss -0.7763
2024-12-17 08:48:26.339291: val_loss -0.6088
2024-12-17 08:48:26.340038: Pseudo dice [0.775]
2024-12-17 08:48:26.340714: Epoch time: 88.42 s
2024-12-17 08:48:27.653452: 
2024-12-17 08:48:27.655117: Epoch 128
2024-12-17 08:48:27.655796: Current learning rate: 0.00178
2024-12-17 08:49:56.010475: Validation loss did not improve from -0.64085. Patience: 42/50
2024-12-17 08:49:56.011719: train_loss -0.7802
2024-12-17 08:49:56.013212: val_loss -0.5651
2024-12-17 08:49:56.014049: Pseudo dice [0.7467]
2024-12-17 08:49:56.014927: Epoch time: 88.36 s
2024-12-17 08:49:57.556881: 
2024-12-17 08:49:57.558619: Epoch 129
2024-12-17 08:49:57.559376: Current learning rate: 0.0017
2024-12-17 08:51:25.963439: Validation loss did not improve from -0.64085. Patience: 43/50
2024-12-17 08:51:25.967092: train_loss -0.7752
2024-12-17 08:51:25.968645: val_loss -0.5761
2024-12-17 08:51:25.969562: Pseudo dice [0.7563]
2024-12-17 08:51:25.970394: Epoch time: 88.41 s
2024-12-17 08:51:27.593176: 
2024-12-17 08:51:27.595171: Epoch 130
2024-12-17 08:51:27.596009: Current learning rate: 0.00163
2024-12-17 08:52:55.931679: Validation loss did not improve from -0.64085. Patience: 44/50
2024-12-17 08:52:55.932854: train_loss -0.7771
2024-12-17 08:52:55.933803: val_loss -0.6061
2024-12-17 08:52:55.934633: Pseudo dice [0.773]
2024-12-17 08:52:55.935344: Epoch time: 88.34 s
2024-12-17 08:52:57.190218: 
2024-12-17 08:52:57.192210: Epoch 131
2024-12-17 08:52:57.193154: Current learning rate: 0.00156
2024-12-17 08:54:25.553263: Validation loss did not improve from -0.64085. Patience: 45/50
2024-12-17 08:54:25.554424: train_loss -0.7767
2024-12-17 08:54:25.555283: val_loss -0.5826
2024-12-17 08:54:25.556123: Pseudo dice [0.7618]
2024-12-17 08:54:25.556892: Epoch time: 88.37 s
2024-12-17 08:54:26.787074: 
2024-12-17 08:54:26.788736: Epoch 132
2024-12-17 08:54:26.789477: Current learning rate: 0.00148
2024-12-17 08:55:55.122251: Validation loss did not improve from -0.64085. Patience: 46/50
2024-12-17 08:55:55.123546: train_loss -0.7793
2024-12-17 08:55:55.124451: val_loss -0.5837
2024-12-17 08:55:55.125414: Pseudo dice [0.7665]
2024-12-17 08:55:55.126381: Epoch time: 88.34 s
2024-12-17 08:55:56.386771: 
2024-12-17 08:55:56.388758: Epoch 133
2024-12-17 08:55:56.389750: Current learning rate: 0.00141
2024-12-17 08:57:24.704854: Validation loss did not improve from -0.64085. Patience: 47/50
2024-12-17 08:57:24.705750: train_loss -0.7831
2024-12-17 08:57:24.706652: val_loss -0.6195
2024-12-17 08:57:24.707347: Pseudo dice [0.7821]
2024-12-17 08:57:24.708012: Epoch time: 88.32 s
2024-12-17 08:57:25.951327: 
2024-12-17 08:57:25.952582: Epoch 134
2024-12-17 08:57:25.953298: Current learning rate: 0.00133
2024-12-17 08:58:54.472823: Validation loss did not improve from -0.64085. Patience: 48/50
2024-12-17 08:58:54.473700: train_loss -0.7824
2024-12-17 08:58:54.474651: val_loss -0.5989
2024-12-17 08:58:54.475548: Pseudo dice [0.7698]
2024-12-17 08:58:54.476356: Epoch time: 88.52 s
2024-12-17 08:58:56.132348: 
2024-12-17 08:58:56.134423: Epoch 135
2024-12-17 08:58:56.135224: Current learning rate: 0.00126
2024-12-17 09:00:24.694680: Validation loss did not improve from -0.64085. Patience: 49/50
2024-12-17 09:00:24.695440: train_loss -0.7798
2024-12-17 09:00:24.696373: val_loss -0.5731
2024-12-17 09:00:24.697189: Pseudo dice [0.7553]
2024-12-17 09:00:24.698029: Epoch time: 88.56 s
2024-12-17 09:00:25.939027: 
2024-12-17 09:00:25.940570: Epoch 136
2024-12-17 09:00:25.941537: Current learning rate: 0.00118
2024-12-17 09:01:54.518592: Validation loss did not improve from -0.64085. Patience: 50/50
2024-12-17 09:01:54.519680: train_loss -0.7785
2024-12-17 09:01:54.520687: val_loss -0.6204
2024-12-17 09:01:54.521783: Pseudo dice [0.7806]
2024-12-17 09:01:54.522708: Epoch time: 88.58 s
2024-12-17 09:01:55.818187: 
2024-12-17 09:01:55.820190: Epoch 137
2024-12-17 09:01:55.821221: Current learning rate: 0.00111
2024-12-17 09:03:24.514970: Validation loss did not improve from -0.64085. Patience: 51/50
2024-12-17 09:03:24.515875: train_loss -0.7834
2024-12-17 09:03:24.516691: val_loss -0.5882
2024-12-17 09:03:24.517436: Pseudo dice [0.7663]
2024-12-17 09:03:24.518254: Epoch time: 88.7 s
2024-12-17 09:03:25.764666: 
2024-12-17 09:03:25.766350: Epoch 138
2024-12-17 09:03:25.767041: Current learning rate: 0.00103
2024-12-17 09:04:54.405129: Validation loss did not improve from -0.64085. Patience: 52/50
2024-12-17 09:04:54.406151: train_loss -0.7801
2024-12-17 09:04:54.407134: val_loss -0.6088
2024-12-17 09:04:54.407895: Pseudo dice [0.7822]
2024-12-17 09:04:54.408606: Epoch time: 88.64 s
2024-12-17 09:04:55.706400: 
2024-12-17 09:04:55.708050: Epoch 139
2024-12-17 09:04:55.708991: Current learning rate: 0.00095
2024-12-17 09:06:24.331264: Validation loss did not improve from -0.64085. Patience: 53/50
2024-12-17 09:06:24.332567: train_loss -0.7803
2024-12-17 09:06:24.333515: val_loss -0.616
2024-12-17 09:06:24.334254: Pseudo dice [0.7834]
2024-12-17 09:06:24.334990: Epoch time: 88.63 s
2024-12-17 09:06:26.333294: 
2024-12-17 09:06:26.335381: Epoch 140
2024-12-17 09:06:26.336082: Current learning rate: 0.00087
2024-12-17 09:07:54.956742: Validation loss did not improve from -0.64085. Patience: 54/50
2024-12-17 09:07:54.957819: train_loss -0.7806
2024-12-17 09:07:54.958695: val_loss -0.6114
2024-12-17 09:07:54.959501: Pseudo dice [0.7772]
2024-12-17 09:07:54.960234: Epoch time: 88.63 s
2024-12-17 09:07:56.252924: 
2024-12-17 09:07:56.254586: Epoch 141
2024-12-17 09:07:56.255265: Current learning rate: 0.00079
2024-12-17 09:09:24.846205: Validation loss did not improve from -0.64085. Patience: 55/50
2024-12-17 09:09:24.847198: train_loss -0.783
2024-12-17 09:09:24.848136: val_loss -0.6108
2024-12-17 09:09:24.848908: Pseudo dice [0.7798]
2024-12-17 09:09:24.849684: Epoch time: 88.6 s
2024-12-17 09:09:26.081287: 
2024-12-17 09:09:26.082793: Epoch 142
2024-12-17 09:09:26.083692: Current learning rate: 0.00071
2024-12-17 09:10:54.835132: Validation loss did not improve from -0.64085. Patience: 56/50
2024-12-17 09:10:54.835963: train_loss -0.787
2024-12-17 09:10:54.836978: val_loss -0.5738
2024-12-17 09:10:54.837841: Pseudo dice [0.7575]
2024-12-17 09:10:54.838751: Epoch time: 88.76 s
2024-12-17 09:10:56.111376: 
2024-12-17 09:10:56.112961: Epoch 143
2024-12-17 09:10:56.113744: Current learning rate: 0.00063
2024-12-17 09:12:24.926940: Validation loss did not improve from -0.64085. Patience: 57/50
2024-12-17 09:12:24.928047: train_loss -0.7823
2024-12-17 09:12:24.929080: val_loss -0.6008
2024-12-17 09:12:24.929898: Pseudo dice [0.7765]
2024-12-17 09:12:24.930701: Epoch time: 88.82 s
2024-12-17 09:12:26.178448: 
2024-12-17 09:12:26.180393: Epoch 144
2024-12-17 09:12:26.181316: Current learning rate: 0.00055
2024-12-17 09:13:54.882693: Validation loss did not improve from -0.64085. Patience: 58/50
2024-12-17 09:13:54.883847: train_loss -0.7852
2024-12-17 09:13:54.885188: val_loss -0.5904
2024-12-17 09:13:54.886167: Pseudo dice [0.7695]
2024-12-17 09:13:54.887220: Epoch time: 88.71 s
2024-12-17 09:13:56.502497: 
2024-12-17 09:13:56.504313: Epoch 145
2024-12-17 09:13:56.505368: Current learning rate: 0.00047
2024-12-17 09:15:25.245852: Validation loss did not improve from -0.64085. Patience: 59/50
2024-12-17 09:15:25.246915: train_loss -0.7859
2024-12-17 09:15:25.247694: val_loss -0.5932
2024-12-17 09:15:25.248488: Pseudo dice [0.7697]
2024-12-17 09:15:25.249401: Epoch time: 88.75 s
2024-12-17 09:15:26.538105: 
2024-12-17 09:15:26.539628: Epoch 146
2024-12-17 09:15:26.540484: Current learning rate: 0.00038
2024-12-17 09:16:55.225845: Validation loss did not improve from -0.64085. Patience: 60/50
2024-12-17 09:16:55.227073: train_loss -0.7808
2024-12-17 09:16:55.228056: val_loss -0.5995
2024-12-17 09:16:55.228738: Pseudo dice [0.7735]
2024-12-17 09:16:55.229475: Epoch time: 88.69 s
2024-12-17 09:16:56.500214: 
2024-12-17 09:16:56.501830: Epoch 147
2024-12-17 09:16:56.502596: Current learning rate: 0.0003
2024-12-17 09:18:25.041744: Validation loss did not improve from -0.64085. Patience: 61/50
2024-12-17 09:18:25.043152: train_loss -0.7842
2024-12-17 09:18:25.044304: val_loss -0.6056
2024-12-17 09:18:25.045058: Pseudo dice [0.7821]
2024-12-17 09:18:25.045789: Epoch time: 88.54 s
2024-12-17 09:18:26.353625: 
2024-12-17 09:18:26.355863: Epoch 148
2024-12-17 09:18:26.356736: Current learning rate: 0.00021
2024-12-17 09:19:54.731884: Validation loss did not improve from -0.64085. Patience: 62/50
2024-12-17 09:19:54.732696: train_loss -0.7864
2024-12-17 09:19:54.733626: val_loss -0.5897
2024-12-17 09:19:54.734352: Pseudo dice [0.7681]
2024-12-17 09:19:54.735054: Epoch time: 88.38 s
2024-12-17 09:19:56.015652: 
2024-12-17 09:19:56.017258: Epoch 149
2024-12-17 09:19:56.018032: Current learning rate: 0.00011
2024-12-17 09:21:24.448724: Validation loss did not improve from -0.64085. Patience: 63/50
2024-12-17 09:21:24.449962: train_loss -0.785
2024-12-17 09:21:24.450777: val_loss -0.5733
2024-12-17 09:21:24.451430: Pseudo dice [0.7661]
2024-12-17 09:21:24.452186: Epoch time: 88.44 s
2024-12-17 09:21:26.189704: Training done.
2024-12-17 09:21:26.358442: Using splits from existing split file: /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset309_Sohee_Calcium_OCT_CrossValidation/splits_final.json
2024-12-17 09:21:26.377097: The split file contains 5 splits.
2024-12-17 09:21:26.377862: Desired fold for training: 2
2024-12-17 09:21:26.378474: This split has 6 training and 2 validation cases.
2024-12-17 09:21:26.379360: predicting 101-044
2024-12-17 09:21:26.388166: 101-044, shape torch.Size([1, 404, 498, 498]), rank 0
2024-12-17 09:23:15.082059: predicting 704-003
2024-12-17 09:23:15.110222: 704-003, shape torch.Size([1, 375, 498, 498]), rank 0
2024-12-17 09:25:08.858609: Validation complete
2024-12-17 09:25:08.859593: Mean Validation Dice:  0.7516744251353542
2024-12-17 05:35:52.879313: unpacking done...
2024-12-17 05:35:53.137610: Unable to plot network architecture: nnUNet_compile is enabled!
2024-12-17 05:35:53.234437: 
2024-12-17 05:35:53.236716: Epoch 0
2024-12-17 05:35:53.238304: Current learning rate: 0.01
2024-12-17 05:38:08.389967: Validation loss improved from 1000.00000 to -0.34950! Patience: 0/50
2024-12-17 05:38:08.391130: train_loss -0.3154
2024-12-17 05:38:08.391900: val_loss -0.3495
2024-12-17 05:38:08.392539: Pseudo dice [0.6463]
2024-12-17 05:38:08.393239: Epoch time: 135.16 s
2024-12-17 05:38:08.394005: Yayy! New best EMA pseudo Dice: 0.6463
2024-12-17 05:38:09.864561: 
2024-12-17 05:38:09.866240: Epoch 1
2024-12-17 05:38:09.867082: Current learning rate: 0.00994
2024-12-17 05:39:38.491090: Validation loss improved from -0.34950 to -0.36589! Patience: 0/50
2024-12-17 05:39:38.492334: train_loss -0.491
2024-12-17 05:39:38.493533: val_loss -0.3659
2024-12-17 05:39:38.494443: Pseudo dice [0.6325]
2024-12-17 05:39:38.495264: Epoch time: 88.63 s
2024-12-17 05:39:39.764068: 
2024-12-17 05:39:39.765794: Epoch 2
2024-12-17 05:39:39.766898: Current learning rate: 0.00988
2024-12-17 05:41:08.874942: Validation loss improved from -0.36589 to -0.42247! Patience: 0/50
2024-12-17 05:41:08.876177: train_loss -0.5162
2024-12-17 05:41:08.877009: val_loss -0.4225
2024-12-17 05:41:08.877800: Pseudo dice [0.68]
2024-12-17 05:41:08.878563: Epoch time: 89.11 s
2024-12-17 05:41:08.879377: Yayy! New best EMA pseudo Dice: 0.6484
2024-12-17 05:41:10.545397: 
2024-12-17 05:41:10.547136: Epoch 3
2024-12-17 05:41:10.548153: Current learning rate: 0.00982
2024-12-17 05:42:39.707049: Validation loss did not improve from -0.42247. Patience: 1/50
2024-12-17 05:42:39.708366: train_loss -0.5491
2024-12-17 05:42:39.709127: val_loss -0.3827
2024-12-17 05:42:39.709788: Pseudo dice [0.6586]
2024-12-17 05:42:39.710473: Epoch time: 89.16 s
2024-12-17 05:42:39.711090: Yayy! New best EMA pseudo Dice: 0.6494
2024-12-17 05:42:41.299778: 
2024-12-17 05:42:41.301498: Epoch 4
2024-12-17 05:42:41.302206: Current learning rate: 0.00976
2024-12-17 05:44:10.489593: Validation loss did not improve from -0.42247. Patience: 2/50
2024-12-17 05:44:10.490739: train_loss -0.5338
2024-12-17 05:44:10.491858: val_loss -0.3459
2024-12-17 05:44:10.492567: Pseudo dice [0.6404]
2024-12-17 05:44:10.493445: Epoch time: 89.19 s
2024-12-17 05:44:12.084141: 
2024-12-17 05:44:12.086367: Epoch 5
2024-12-17 05:44:12.087309: Current learning rate: 0.0097
2024-12-17 05:45:41.113713: Validation loss did not improve from -0.42247. Patience: 3/50
2024-12-17 05:45:41.115105: train_loss -0.5602
2024-12-17 05:45:41.116593: val_loss -0.4063
2024-12-17 05:45:41.117499: Pseudo dice [0.6589]
2024-12-17 05:45:41.118368: Epoch time: 89.03 s
2024-12-17 05:45:41.119336: Yayy! New best EMA pseudo Dice: 0.6496
2024-12-17 05:45:42.717512: 
2024-12-17 05:45:42.719186: Epoch 6
2024-12-17 05:45:42.720014: Current learning rate: 0.00964
2024-12-17 05:47:11.869639: Validation loss improved from -0.42247 to -0.42779! Patience: 3/50
2024-12-17 05:47:11.870880: train_loss -0.5634
2024-12-17 05:47:11.871959: val_loss -0.4278
2024-12-17 05:47:11.872836: Pseudo dice [0.6778]
2024-12-17 05:47:11.873506: Epoch time: 89.15 s
2024-12-17 05:47:11.874131: Yayy! New best EMA pseudo Dice: 0.6524
2024-12-17 05:47:13.444117: 
2024-12-17 05:47:13.445614: Epoch 7
2024-12-17 05:47:13.446357: Current learning rate: 0.00958
2024-12-17 05:48:42.463587: Validation loss did not improve from -0.42779. Patience: 1/50
2024-12-17 05:48:42.464854: train_loss -0.5882
2024-12-17 05:48:42.465965: val_loss -0.3177
2024-12-17 05:48:42.466750: Pseudo dice [0.6238]
2024-12-17 05:48:42.467585: Epoch time: 89.02 s
2024-12-17 05:48:44.139179: 
2024-12-17 05:48:44.140974: Epoch 8
2024-12-17 05:48:44.142129: Current learning rate: 0.00952
2024-12-17 05:50:13.554914: Validation loss did not improve from -0.42779. Patience: 2/50
2024-12-17 05:50:13.555991: train_loss -0.584
2024-12-17 05:50:13.556943: val_loss -0.3547
2024-12-17 05:50:13.557609: Pseudo dice [0.6459]
2024-12-17 05:50:13.558437: Epoch time: 89.42 s
2024-12-17 05:50:14.811002: 
2024-12-17 05:50:14.813029: Epoch 9
2024-12-17 05:50:14.813908: Current learning rate: 0.00946
2024-12-17 05:51:44.243770: Validation loss improved from -0.42779 to -0.44539! Patience: 2/50
2024-12-17 05:51:44.244858: train_loss -0.5801
2024-12-17 05:51:44.245752: val_loss -0.4454
2024-12-17 05:51:44.246603: Pseudo dice [0.6988]
2024-12-17 05:51:44.247396: Epoch time: 89.43 s
2024-12-17 05:51:44.614628: Yayy! New best EMA pseudo Dice: 0.6541
2024-12-17 05:51:46.140280: 
2024-12-17 05:51:46.141881: Epoch 10
2024-12-17 05:51:46.142953: Current learning rate: 0.0094
2024-12-17 05:53:15.796705: Validation loss improved from -0.44539 to -0.46158! Patience: 0/50
2024-12-17 05:53:15.797816: train_loss -0.5867
2024-12-17 05:53:15.798583: val_loss -0.4616
2024-12-17 05:53:15.799245: Pseudo dice [0.6979]
2024-12-17 05:53:15.799891: Epoch time: 89.66 s
2024-12-17 05:53:15.800517: Yayy! New best EMA pseudo Dice: 0.6585
2024-12-17 05:53:17.352474: 
2024-12-17 05:53:17.354150: Epoch 11
2024-12-17 05:53:17.354853: Current learning rate: 0.00934
2024-12-17 05:54:46.879260: Validation loss did not improve from -0.46158. Patience: 1/50
2024-12-17 05:54:46.880586: train_loss -0.5986
2024-12-17 05:54:46.881895: val_loss -0.3438
2024-12-17 05:54:46.883061: Pseudo dice [0.6376]
2024-12-17 05:54:46.884154: Epoch time: 89.53 s
2024-12-17 05:54:48.116683: 
2024-12-17 05:54:48.118297: Epoch 12
2024-12-17 05:54:48.119463: Current learning rate: 0.00928
2024-12-17 05:56:17.666060: Validation loss did not improve from -0.46158. Patience: 2/50
2024-12-17 05:56:17.667469: train_loss -0.6101
2024-12-17 05:56:17.668617: val_loss -0.4226
2024-12-17 05:56:17.669415: Pseudo dice [0.6845]
2024-12-17 05:56:17.670210: Epoch time: 89.55 s
2024-12-17 05:56:17.670919: Yayy! New best EMA pseudo Dice: 0.6592
2024-12-17 05:56:19.267180: 
2024-12-17 05:56:19.268813: Epoch 13
2024-12-17 05:56:19.269872: Current learning rate: 0.00922
2024-12-17 05:57:48.830425: Validation loss did not improve from -0.46158. Patience: 3/50
2024-12-17 05:57:48.831766: train_loss -0.6193
2024-12-17 05:57:48.833061: val_loss -0.3476
2024-12-17 05:57:48.834029: Pseudo dice [0.6338]
2024-12-17 05:57:48.835048: Epoch time: 89.57 s
2024-12-17 05:57:50.076600: 
2024-12-17 05:57:50.078232: Epoch 14
2024-12-17 05:57:50.079094: Current learning rate: 0.00916
2024-12-17 05:59:19.701739: Validation loss improved from -0.46158 to -0.49864! Patience: 3/50
2024-12-17 05:59:19.703040: train_loss -0.632
2024-12-17 05:59:19.703983: val_loss -0.4986
2024-12-17 05:59:19.704904: Pseudo dice [0.7354]
2024-12-17 05:59:19.705815: Epoch time: 89.63 s
2024-12-17 05:59:20.077983: Yayy! New best EMA pseudo Dice: 0.6646
2024-12-17 05:59:21.684233: 
2024-12-17 05:59:21.685235: Epoch 15
2024-12-17 05:59:21.685947: Current learning rate: 0.0091
2024-12-17 06:00:51.286676: Validation loss did not improve from -0.49864. Patience: 1/50
2024-12-17 06:00:51.287959: train_loss -0.6141
2024-12-17 06:00:51.288865: val_loss -0.4948
2024-12-17 06:00:51.289663: Pseudo dice [0.7242]
2024-12-17 06:00:51.290427: Epoch time: 89.6 s
2024-12-17 06:00:51.291137: Yayy! New best EMA pseudo Dice: 0.6705
2024-12-17 06:00:52.888058: 
2024-12-17 06:00:52.889303: Epoch 16
2024-12-17 06:00:52.890105: Current learning rate: 0.00903
2024-12-17 06:02:22.790838: Validation loss did not improve from -0.49864. Patience: 2/50
2024-12-17 06:02:22.792260: train_loss -0.629
2024-12-17 06:02:22.793353: val_loss -0.4867
2024-12-17 06:02:22.794187: Pseudo dice [0.7264]
2024-12-17 06:02:22.794883: Epoch time: 89.91 s
2024-12-17 06:02:22.795596: Yayy! New best EMA pseudo Dice: 0.6761
2024-12-17 06:02:24.400006: 
2024-12-17 06:02:24.401633: Epoch 17
2024-12-17 06:02:24.402529: Current learning rate: 0.00897
2024-12-17 06:03:54.361416: Validation loss did not improve from -0.49864. Patience: 3/50
2024-12-17 06:03:54.362699: train_loss -0.6358
2024-12-17 06:03:54.363752: val_loss -0.4548
2024-12-17 06:03:54.364591: Pseudo dice [0.7068]
2024-12-17 06:03:54.365370: Epoch time: 89.96 s
2024-12-17 06:03:54.366174: Yayy! New best EMA pseudo Dice: 0.6792
2024-12-17 06:03:56.021898: 
2024-12-17 06:03:56.023791: Epoch 18
2024-12-17 06:03:56.024590: Current learning rate: 0.00891
2024-12-17 06:05:25.834327: Validation loss did not improve from -0.49864. Patience: 4/50
2024-12-17 06:05:25.835322: train_loss -0.6477
2024-12-17 06:05:25.836190: val_loss -0.4698
2024-12-17 06:05:25.836987: Pseudo dice [0.722]
2024-12-17 06:05:25.837947: Epoch time: 89.81 s
2024-12-17 06:05:25.838745: Yayy! New best EMA pseudo Dice: 0.6835
2024-12-17 06:05:27.861850: 
2024-12-17 06:05:27.863935: Epoch 19
2024-12-17 06:05:27.864786: Current learning rate: 0.00885
2024-12-17 06:06:57.710993: Validation loss did not improve from -0.49864. Patience: 5/50
2024-12-17 06:06:57.712146: train_loss -0.637
2024-12-17 06:06:57.713164: val_loss -0.4577
2024-12-17 06:06:57.713951: Pseudo dice [0.7058]
2024-12-17 06:06:57.714734: Epoch time: 89.85 s
2024-12-17 06:06:58.088578: Yayy! New best EMA pseudo Dice: 0.6857
2024-12-17 06:06:59.718341: 
2024-12-17 06:06:59.719534: Epoch 20
2024-12-17 06:06:59.720334: Current learning rate: 0.00879
2024-12-17 06:08:29.271107: Validation loss did not improve from -0.49864. Patience: 6/50
2024-12-17 06:08:29.272375: train_loss -0.6456
2024-12-17 06:08:29.273452: val_loss -0.4304
2024-12-17 06:08:29.274217: Pseudo dice [0.6886]
2024-12-17 06:08:29.275003: Epoch time: 89.55 s
2024-12-17 06:08:29.275807: Yayy! New best EMA pseudo Dice: 0.686
2024-12-17 06:08:30.954580: 
2024-12-17 06:08:30.955928: Epoch 21
2024-12-17 06:08:30.956818: Current learning rate: 0.00873
2024-12-17 06:10:00.528882: Validation loss did not improve from -0.49864. Patience: 7/50
2024-12-17 06:10:00.529613: train_loss -0.6522
2024-12-17 06:10:00.530497: val_loss -0.4603
2024-12-17 06:10:00.531528: Pseudo dice [0.7152]
2024-12-17 06:10:00.532449: Epoch time: 89.58 s
2024-12-17 06:10:00.533398: Yayy! New best EMA pseudo Dice: 0.6889
2024-12-17 06:10:02.078233: 
2024-12-17 06:10:02.079834: Epoch 22
2024-12-17 06:10:02.080858: Current learning rate: 0.00867
2024-12-17 06:11:31.593236: Validation loss did not improve from -0.49864. Patience: 8/50
2024-12-17 06:11:31.594374: train_loss -0.6526
2024-12-17 06:11:31.595129: val_loss -0.4878
2024-12-17 06:11:31.595792: Pseudo dice [0.7245]
2024-12-17 06:11:31.596413: Epoch time: 89.52 s
2024-12-17 06:11:31.597044: Yayy! New best EMA pseudo Dice: 0.6925
2024-12-17 06:11:33.145331: 
2024-12-17 06:11:33.146912: Epoch 23
2024-12-17 06:11:33.147701: Current learning rate: 0.00861
2024-12-17 06:13:02.636918: Validation loss did not improve from -0.49864. Patience: 9/50
2024-12-17 06:13:02.638161: train_loss -0.6531
2024-12-17 06:13:02.639126: val_loss -0.4979
2024-12-17 06:13:02.639825: Pseudo dice [0.7254]
2024-12-17 06:13:02.640706: Epoch time: 89.49 s
2024-12-17 06:13:02.641449: Yayy! New best EMA pseudo Dice: 0.6958
2024-12-17 06:13:04.244255: 
2024-12-17 06:13:04.246391: Epoch 24
2024-12-17 06:13:04.247155: Current learning rate: 0.00855
2024-12-17 06:14:33.889835: Validation loss did not improve from -0.49864. Patience: 10/50
2024-12-17 06:14:33.890862: train_loss -0.6453
2024-12-17 06:14:33.891779: val_loss -0.4398
2024-12-17 06:14:33.892532: Pseudo dice [0.6788]
2024-12-17 06:14:33.893215: Epoch time: 89.65 s
2024-12-17 06:14:35.548891: 
2024-12-17 06:14:35.550468: Epoch 25
2024-12-17 06:14:35.551292: Current learning rate: 0.00849
2024-12-17 06:16:05.105376: Validation loss did not improve from -0.49864. Patience: 11/50
2024-12-17 06:16:05.106636: train_loss -0.6553
2024-12-17 06:16:05.107468: val_loss -0.4799
2024-12-17 06:16:05.108231: Pseudo dice [0.7166]
2024-12-17 06:16:05.108944: Epoch time: 89.56 s
2024-12-17 06:16:05.109639: Yayy! New best EMA pseudo Dice: 0.6963
2024-12-17 06:16:06.681069: 
2024-12-17 06:16:06.682943: Epoch 26
2024-12-17 06:16:06.683820: Current learning rate: 0.00843
2024-12-17 06:17:36.240815: Validation loss did not improve from -0.49864. Patience: 12/50
2024-12-17 06:17:36.242071: train_loss -0.6555
2024-12-17 06:17:36.243114: val_loss -0.4875
2024-12-17 06:17:36.243891: Pseudo dice [0.7254]
2024-12-17 06:17:36.244638: Epoch time: 89.56 s
2024-12-17 06:17:36.245401: Yayy! New best EMA pseudo Dice: 0.6992
2024-12-17 06:17:37.861721: 
2024-12-17 06:17:37.863257: Epoch 27
2024-12-17 06:17:37.864003: Current learning rate: 0.00836
2024-12-17 06:19:07.452907: Validation loss did not improve from -0.49864. Patience: 13/50
2024-12-17 06:19:07.453730: train_loss -0.6564
2024-12-17 06:19:07.454662: val_loss -0.4167
2024-12-17 06:19:07.455421: Pseudo dice [0.6748]
2024-12-17 06:19:07.456199: Epoch time: 89.59 s
2024-12-17 06:19:08.666909: 
2024-12-17 06:19:08.668553: Epoch 28
2024-12-17 06:19:08.669325: Current learning rate: 0.0083
2024-12-17 06:20:38.515776: Validation loss did not improve from -0.49864. Patience: 14/50
2024-12-17 06:20:38.516917: train_loss -0.6588
2024-12-17 06:20:38.517917: val_loss -0.3513
2024-12-17 06:20:38.518646: Pseudo dice [0.6484]
2024-12-17 06:20:38.519484: Epoch time: 89.85 s
2024-12-17 06:20:40.069024: 
2024-12-17 06:20:40.070726: Epoch 29
2024-12-17 06:20:40.071636: Current learning rate: 0.00824
2024-12-17 06:22:10.060061: Validation loss did not improve from -0.49864. Patience: 15/50
2024-12-17 06:22:10.060945: train_loss -0.6621
2024-12-17 06:22:10.061937: val_loss -0.4516
2024-12-17 06:22:10.062907: Pseudo dice [0.7066]
2024-12-17 06:22:10.063683: Epoch time: 89.99 s
2024-12-17 06:22:11.649105: 
2024-12-17 06:22:11.650844: Epoch 30
2024-12-17 06:22:11.651711: Current learning rate: 0.00818
2024-12-17 06:23:41.666727: Validation loss did not improve from -0.49864. Patience: 16/50
2024-12-17 06:23:41.667854: train_loss -0.6666
2024-12-17 06:23:41.668898: val_loss -0.4522
2024-12-17 06:23:41.669893: Pseudo dice [0.6991]
2024-12-17 06:23:41.670844: Epoch time: 90.02 s
2024-12-17 06:23:42.933840: 
2024-12-17 06:23:42.935990: Epoch 31
2024-12-17 06:23:42.936833: Current learning rate: 0.00812
2024-12-17 06:25:12.992412: Validation loss did not improve from -0.49864. Patience: 17/50
2024-12-17 06:25:12.993671: train_loss -0.6675
2024-12-17 06:25:12.994619: val_loss -0.4642
2024-12-17 06:25:12.995393: Pseudo dice [0.7004]
2024-12-17 06:25:12.996043: Epoch time: 90.06 s
2024-12-17 06:25:14.254081: 
2024-12-17 06:25:14.255419: Epoch 32
2024-12-17 06:25:14.256189: Current learning rate: 0.00806
2024-12-17 06:26:44.124661: Validation loss did not improve from -0.49864. Patience: 18/50
2024-12-17 06:26:44.125808: train_loss -0.673
2024-12-17 06:26:44.126640: val_loss -0.2292
2024-12-17 06:26:44.127488: Pseudo dice [0.5782]
2024-12-17 06:26:44.128174: Epoch time: 89.87 s
2024-12-17 06:26:45.375385: 
2024-12-17 06:26:45.376951: Epoch 33
2024-12-17 06:26:45.377635: Current learning rate: 0.008
2024-12-17 06:28:15.208151: Validation loss did not improve from -0.49864. Patience: 19/50
2024-12-17 06:28:15.209287: train_loss -0.6822
2024-12-17 06:28:15.210161: val_loss -0.4563
2024-12-17 06:28:15.210848: Pseudo dice [0.7136]
2024-12-17 06:28:15.211599: Epoch time: 89.83 s
2024-12-17 06:28:16.478365: 
2024-12-17 06:28:16.480084: Epoch 34
2024-12-17 06:28:16.480943: Current learning rate: 0.00793
2024-12-17 06:29:46.305248: Validation loss did not improve from -0.49864. Patience: 20/50
2024-12-17 06:29:46.306498: train_loss -0.6776
2024-12-17 06:29:46.307668: val_loss -0.471
2024-12-17 06:29:46.308567: Pseudo dice [0.719]
2024-12-17 06:29:46.309480: Epoch time: 89.83 s
2024-12-17 06:29:48.117384: 
2024-12-17 06:29:48.119964: Epoch 35
2024-12-17 06:29:48.121569: Current learning rate: 0.00787
2024-12-17 06:31:17.877547: Validation loss did not improve from -0.49864. Patience: 21/50
2024-12-17 06:31:17.878466: train_loss -0.6834
2024-12-17 06:31:17.879356: val_loss -0.454
2024-12-17 06:31:17.880265: Pseudo dice [0.7018]
2024-12-17 06:31:17.881130: Epoch time: 89.77 s
2024-12-17 06:31:19.138125: 
2024-12-17 06:31:19.139584: Epoch 36
2024-12-17 06:31:19.140349: Current learning rate: 0.00781
2024-12-17 06:32:49.075440: Validation loss did not improve from -0.49864. Patience: 22/50
2024-12-17 06:32:49.078327: train_loss -0.6885
2024-12-17 06:32:49.079328: val_loss -0.4682
2024-12-17 06:32:49.080312: Pseudo dice [0.7226]
2024-12-17 06:32:49.081307: Epoch time: 89.94 s
2024-12-17 06:32:50.382268: 
2024-12-17 06:32:50.384805: Epoch 37
2024-12-17 06:32:50.386056: Current learning rate: 0.00775
2024-12-17 06:34:20.530197: Validation loss did not improve from -0.49864. Patience: 23/50
2024-12-17 06:34:20.531207: train_loss -0.6902
2024-12-17 06:34:20.532478: val_loss -0.4373
2024-12-17 06:34:20.533498: Pseudo dice [0.6897]
2024-12-17 06:34:20.534484: Epoch time: 90.15 s
2024-12-17 06:34:21.822029: 
2024-12-17 06:34:21.823850: Epoch 38
2024-12-17 06:34:21.824883: Current learning rate: 0.00769
2024-12-17 06:35:51.986481: Validation loss did not improve from -0.49864. Patience: 24/50
2024-12-17 06:35:51.987339: train_loss -0.6926
2024-12-17 06:35:51.988372: val_loss -0.3887
2024-12-17 06:35:51.989464: Pseudo dice [0.673]
2024-12-17 06:35:51.990478: Epoch time: 90.17 s
2024-12-17 06:35:53.276203: 
2024-12-17 06:35:53.277986: Epoch 39
2024-12-17 06:35:53.278775: Current learning rate: 0.00763
2024-12-17 06:37:23.449342: Validation loss did not improve from -0.49864. Patience: 25/50
2024-12-17 06:37:23.450422: train_loss -0.6901
2024-12-17 06:37:23.451239: val_loss -0.4125
2024-12-17 06:37:23.452085: Pseudo dice [0.6783]
2024-12-17 06:37:23.452765: Epoch time: 90.18 s
2024-12-17 06:37:25.467842: 
2024-12-17 06:37:25.469221: Epoch 40
2024-12-17 06:37:25.470155: Current learning rate: 0.00756
2024-12-17 06:38:55.478123: Validation loss improved from -0.49864 to -0.50477! Patience: 25/50
2024-12-17 06:38:55.479094: train_loss -0.6917
2024-12-17 06:38:55.480170: val_loss -0.5048
2024-12-17 06:38:55.480911: Pseudo dice [0.7365]
2024-12-17 06:38:55.481738: Epoch time: 90.01 s
2024-12-17 06:38:56.772863: 
2024-12-17 06:38:56.774540: Epoch 41
2024-12-17 06:38:56.775352: Current learning rate: 0.0075
2024-12-17 06:40:26.915506: Validation loss did not improve from -0.50477. Patience: 1/50
2024-12-17 06:40:26.916782: train_loss -0.6953
2024-12-17 06:40:26.917948: val_loss -0.4522
2024-12-17 06:40:26.918797: Pseudo dice [0.6952]
2024-12-17 06:40:26.919582: Epoch time: 90.15 s
2024-12-17 06:40:28.098659: 
2024-12-17 06:40:28.100442: Epoch 42
2024-12-17 06:40:28.101439: Current learning rate: 0.00744
2024-12-17 06:41:58.141118: Validation loss did not improve from -0.50477. Patience: 2/50
2024-12-17 06:41:58.143593: train_loss -0.694
2024-12-17 06:41:58.144826: val_loss -0.4258
2024-12-17 06:41:58.145732: Pseudo dice [0.6815]
2024-12-17 06:41:58.146561: Epoch time: 90.05 s
2024-12-17 06:41:59.361880: 
2024-12-17 06:41:59.363566: Epoch 43
2024-12-17 06:41:59.364375: Current learning rate: 0.00738
2024-12-17 06:43:29.440182: Validation loss did not improve from -0.50477. Patience: 3/50
2024-12-17 06:43:29.441815: train_loss -0.6969
2024-12-17 06:43:29.443521: val_loss -0.4405
2024-12-17 06:43:29.444347: Pseudo dice [0.6813]
2024-12-17 06:43:29.445187: Epoch time: 90.08 s
2024-12-17 06:43:30.657321: 
2024-12-17 06:43:30.658736: Epoch 44
2024-12-17 06:43:30.659699: Current learning rate: 0.00732
2024-12-17 06:45:00.728228: Validation loss did not improve from -0.50477. Patience: 4/50
2024-12-17 06:45:00.729352: train_loss -0.6907
2024-12-17 06:45:00.730439: val_loss -0.2666
2024-12-17 06:45:00.731405: Pseudo dice [0.6077]
2024-12-17 06:45:00.732336: Epoch time: 90.07 s
2024-12-17 06:45:02.282618: 
2024-12-17 06:45:02.284369: Epoch 45
2024-12-17 06:45:02.285132: Current learning rate: 0.00725
2024-12-17 06:46:32.316001: Validation loss did not improve from -0.50477. Patience: 5/50
2024-12-17 06:46:32.317199: train_loss -0.6979
2024-12-17 06:46:32.318525: val_loss -0.4639
2024-12-17 06:46:32.319459: Pseudo dice [0.7173]
2024-12-17 06:46:32.320394: Epoch time: 90.04 s
2024-12-17 06:46:33.525937: 
2024-12-17 06:46:33.527474: Epoch 46
2024-12-17 06:46:33.528306: Current learning rate: 0.00719
2024-12-17 06:48:03.618053: Validation loss did not improve from -0.50477. Patience: 6/50
2024-12-17 06:48:03.619102: train_loss -0.6944
2024-12-17 06:48:03.619996: val_loss -0.4444
2024-12-17 06:48:03.620843: Pseudo dice [0.7079]
2024-12-17 06:48:03.621576: Epoch time: 90.09 s
2024-12-17 06:48:04.835016: 
2024-12-17 06:48:04.836722: Epoch 47
2024-12-17 06:48:04.837469: Current learning rate: 0.00713
2024-12-17 06:49:34.954536: Validation loss did not improve from -0.50477. Patience: 7/50
2024-12-17 06:49:34.955738: train_loss -0.7003
2024-12-17 06:49:34.956969: val_loss -0.4535
2024-12-17 06:49:34.957911: Pseudo dice [0.7063]
2024-12-17 06:49:34.958846: Epoch time: 90.12 s
2024-12-17 06:49:36.176203: 
2024-12-17 06:49:36.178059: Epoch 48
2024-12-17 06:49:36.178931: Current learning rate: 0.00707
2024-12-17 06:51:06.341894: Validation loss did not improve from -0.50477. Patience: 8/50
2024-12-17 06:51:06.343178: train_loss -0.701
2024-12-17 06:51:06.344059: val_loss -0.4169
2024-12-17 06:51:06.344824: Pseudo dice [0.6814]
2024-12-17 06:51:06.345653: Epoch time: 90.17 s
2024-12-17 06:51:07.564397: 
2024-12-17 06:51:07.565977: Epoch 49
2024-12-17 06:51:07.566879: Current learning rate: 0.007
2024-12-17 06:52:37.829739: Validation loss did not improve from -0.50477. Patience: 9/50
2024-12-17 06:52:37.830965: train_loss -0.7043
2024-12-17 06:52:37.831857: val_loss -0.3594
2024-12-17 06:52:37.832895: Pseudo dice [0.6661]
2024-12-17 06:52:37.833850: Epoch time: 90.27 s
2024-12-17 06:52:39.468038: 
2024-12-17 06:52:39.469437: Epoch 50
2024-12-17 06:52:39.470542: Current learning rate: 0.00694
2024-12-17 06:54:09.645312: Validation loss did not improve from -0.50477. Patience: 10/50
2024-12-17 06:54:09.646407: train_loss -0.7099
2024-12-17 06:54:09.647272: val_loss -0.4528
2024-12-17 06:54:09.648106: Pseudo dice [0.7054]
2024-12-17 06:54:09.648761: Epoch time: 90.18 s
2024-12-17 06:54:11.181330: 
2024-12-17 06:54:11.183091: Epoch 51
2024-12-17 06:54:11.183998: Current learning rate: 0.00688
2024-12-17 06:55:41.298010: Validation loss did not improve from -0.50477. Patience: 11/50
2024-12-17 06:55:41.299186: train_loss -0.7095
2024-12-17 06:55:41.300075: val_loss -0.3886
2024-12-17 06:55:41.300859: Pseudo dice [0.6718]
2024-12-17 06:55:41.301668: Epoch time: 90.12 s
2024-12-17 06:55:42.561465: 
2024-12-17 06:55:42.563692: Epoch 52
2024-12-17 06:55:42.564441: Current learning rate: 0.00682
2024-12-17 06:57:12.881892: Validation loss did not improve from -0.50477. Patience: 12/50
2024-12-17 06:57:12.883054: train_loss -0.7099
2024-12-17 06:57:12.883738: val_loss -0.478
2024-12-17 06:57:12.884507: Pseudo dice [0.7202]
2024-12-17 06:57:12.885184: Epoch time: 90.32 s
2024-12-17 06:57:14.113149: 
2024-12-17 06:57:14.114837: Epoch 53
2024-12-17 06:57:14.115766: Current learning rate: 0.00675
2024-12-17 06:58:44.484525: Validation loss did not improve from -0.50477. Patience: 13/50
2024-12-17 06:58:44.485686: train_loss -0.7068
2024-12-17 06:58:44.486663: val_loss -0.4374
2024-12-17 06:58:44.487454: Pseudo dice [0.6885]
2024-12-17 06:58:44.488179: Epoch time: 90.37 s
2024-12-17 06:58:45.729425: 
2024-12-17 06:58:45.731211: Epoch 54
2024-12-17 06:58:45.732122: Current learning rate: 0.00669
2024-12-17 07:00:16.076352: Validation loss did not improve from -0.50477. Patience: 14/50
2024-12-17 07:00:16.077520: train_loss -0.7105
2024-12-17 07:00:16.078833: val_loss -0.4212
2024-12-17 07:00:16.079795: Pseudo dice [0.6922]
2024-12-17 07:00:16.080715: Epoch time: 90.35 s
2024-12-17 07:00:17.677123: 
2024-12-17 07:00:17.678894: Epoch 55
2024-12-17 07:00:17.679757: Current learning rate: 0.00663
2024-12-17 07:01:48.179769: Validation loss did not improve from -0.50477. Patience: 15/50
2024-12-17 07:01:48.180865: train_loss -0.7088
2024-12-17 07:01:48.181699: val_loss -0.4825
2024-12-17 07:01:48.182454: Pseudo dice [0.7263]
2024-12-17 07:01:48.183281: Epoch time: 90.5 s
2024-12-17 07:01:49.414412: 
2024-12-17 07:01:49.415668: Epoch 56
2024-12-17 07:01:49.416394: Current learning rate: 0.00657
2024-12-17 07:03:19.827210: Validation loss did not improve from -0.50477. Patience: 16/50
2024-12-17 07:03:19.828423: train_loss -0.7188
2024-12-17 07:03:19.829438: val_loss -0.418
2024-12-17 07:03:19.830302: Pseudo dice [0.6963]
2024-12-17 07:03:19.831203: Epoch time: 90.42 s
2024-12-17 07:03:21.107066: 
2024-12-17 07:03:21.108850: Epoch 57
2024-12-17 07:03:21.109809: Current learning rate: 0.0065
2024-12-17 07:04:51.516681: Validation loss did not improve from -0.50477. Patience: 17/50
2024-12-17 07:04:51.518054: train_loss -0.7102
2024-12-17 07:04:51.519300: val_loss -0.3593
2024-12-17 07:04:51.520325: Pseudo dice [0.66]
2024-12-17 07:04:51.521219: Epoch time: 90.41 s
2024-12-17 07:04:52.787872: 
2024-12-17 07:04:52.789351: Epoch 58
2024-12-17 07:04:52.790278: Current learning rate: 0.00644
2024-12-17 07:06:23.280424: Validation loss did not improve from -0.50477. Patience: 18/50
2024-12-17 07:06:23.281716: train_loss -0.7161
2024-12-17 07:06:23.282614: val_loss -0.4217
2024-12-17 07:06:23.283255: Pseudo dice [0.6987]
2024-12-17 07:06:23.283944: Epoch time: 90.49 s
2024-12-17 07:06:24.515680: 
2024-12-17 07:06:24.516931: Epoch 59
2024-12-17 07:06:24.517669: Current learning rate: 0.00638
2024-12-17 07:07:54.927249: Validation loss did not improve from -0.50477. Patience: 19/50
2024-12-17 07:07:54.928377: train_loss -0.7178
2024-12-17 07:07:54.929207: val_loss -0.3881
2024-12-17 07:07:54.930052: Pseudo dice [0.6675]
2024-12-17 07:07:54.930839: Epoch time: 90.41 s
2024-12-17 07:07:56.542992: 
2024-12-17 07:07:56.544599: Epoch 60
2024-12-17 07:07:56.545394: Current learning rate: 0.00631
2024-12-17 07:09:27.075015: Validation loss did not improve from -0.50477. Patience: 20/50
2024-12-17 07:09:27.075815: train_loss -0.7192
2024-12-17 07:09:27.076830: val_loss -0.4171
2024-12-17 07:09:27.077824: Pseudo dice [0.6977]
2024-12-17 07:09:27.078875: Epoch time: 90.53 s
2024-12-17 07:09:28.349220: 
2024-12-17 07:09:28.350291: Epoch 61
2024-12-17 07:09:28.351173: Current learning rate: 0.00625
2024-12-17 07:10:58.883697: Validation loss did not improve from -0.50477. Patience: 21/50
2024-12-17 07:10:58.885021: train_loss -0.7154
2024-12-17 07:10:58.886205: val_loss -0.3905
2024-12-17 07:10:58.887124: Pseudo dice [0.6606]
2024-12-17 07:10:58.888117: Epoch time: 90.54 s
2024-12-17 07:11:00.522478: 
2024-12-17 07:11:00.524308: Epoch 62
2024-12-17 07:11:00.525097: Current learning rate: 0.00619
2024-12-17 07:12:30.664560: Validation loss did not improve from -0.50477. Patience: 22/50
2024-12-17 07:12:30.665909: train_loss -0.7174
2024-12-17 07:12:30.667063: val_loss -0.316
2024-12-17 07:12:30.667938: Pseudo dice [0.6164]
2024-12-17 07:12:30.668602: Epoch time: 90.14 s
2024-12-17 07:12:31.919742: 
2024-12-17 07:12:31.921483: Epoch 63
2024-12-17 07:12:31.922278: Current learning rate: 0.00612
2024-12-17 07:14:02.148963: Validation loss did not improve from -0.50477. Patience: 23/50
2024-12-17 07:14:02.150376: train_loss -0.7112
2024-12-17 07:14:02.151961: val_loss -0.4836
2024-12-17 07:14:02.152999: Pseudo dice [0.7151]
2024-12-17 07:14:02.154104: Epoch time: 90.23 s
2024-12-17 07:14:03.372288: 
2024-12-17 07:14:03.373736: Epoch 64
2024-12-17 07:14:03.374795: Current learning rate: 0.00606
2024-12-17 07:15:33.724082: Validation loss did not improve from -0.50477. Patience: 24/50
2024-12-17 07:15:33.725289: train_loss -0.7169
2024-12-17 07:15:33.726263: val_loss -0.4131
2024-12-17 07:15:33.727101: Pseudo dice [0.692]
2024-12-17 07:15:33.728135: Epoch time: 90.35 s
2024-12-17 07:15:35.358477: 
2024-12-17 07:15:35.359956: Epoch 65
2024-12-17 07:15:35.360965: Current learning rate: 0.006
2024-12-17 07:17:05.589485: Validation loss did not improve from -0.50477. Patience: 25/50
2024-12-17 07:17:05.590502: train_loss -0.7232
2024-12-17 07:17:05.591481: val_loss -0.3532
2024-12-17 07:17:05.592119: Pseudo dice [0.6515]
2024-12-17 07:17:05.592786: Epoch time: 90.23 s
2024-12-17 07:17:06.863135: 
2024-12-17 07:17:06.864176: Epoch 66
2024-12-17 07:17:06.864880: Current learning rate: 0.00593
2024-12-17 07:18:37.115604: Validation loss did not improve from -0.50477. Patience: 26/50
2024-12-17 07:18:37.116542: train_loss -0.7323
2024-12-17 07:18:37.117612: val_loss -0.5041
2024-12-17 07:18:37.118423: Pseudo dice [0.7415]
2024-12-17 07:18:37.119175: Epoch time: 90.25 s
2024-12-17 07:18:38.373494: 
2024-12-17 07:18:38.375490: Epoch 67
2024-12-17 07:18:38.376510: Current learning rate: 0.00587
2024-12-17 07:20:08.603114: Validation loss did not improve from -0.50477. Patience: 27/50
2024-12-17 07:20:08.604224: train_loss -0.7246
2024-12-17 07:20:08.605223: val_loss -0.4514
2024-12-17 07:20:08.606158: Pseudo dice [0.7023]
2024-12-17 07:20:08.607067: Epoch time: 90.23 s
2024-12-17 07:20:09.853070: 
2024-12-17 07:20:09.854883: Epoch 68
2024-12-17 07:20:09.855856: Current learning rate: 0.00581
2024-12-17 07:21:40.161312: Validation loss did not improve from -0.50477. Patience: 28/50
2024-12-17 07:21:40.162551: train_loss -0.7292
2024-12-17 07:21:40.163528: val_loss -0.4198
2024-12-17 07:21:40.164212: Pseudo dice [0.6852]
2024-12-17 07:21:40.164903: Epoch time: 90.31 s
2024-12-17 07:21:41.435771: 
2024-12-17 07:21:41.437443: Epoch 69
2024-12-17 07:21:41.438323: Current learning rate: 0.00574
2024-12-17 07:23:11.694763: Validation loss did not improve from -0.50477. Patience: 29/50
2024-12-17 07:23:11.695957: train_loss -0.7284
2024-12-17 07:23:11.696921: val_loss -0.4137
2024-12-17 07:23:11.697919: Pseudo dice [0.6914]
2024-12-17 07:23:11.698834: Epoch time: 90.26 s
2024-12-17 07:23:13.299911: 
2024-12-17 07:23:13.301877: Epoch 70
2024-12-17 07:23:13.302835: Current learning rate: 0.00568
2024-12-17 07:24:43.822051: Validation loss did not improve from -0.50477. Patience: 30/50
2024-12-17 07:24:43.823540: train_loss -0.7327
2024-12-17 07:24:43.824728: val_loss -0.4841
2024-12-17 07:24:43.825461: Pseudo dice [0.7246]
2024-12-17 07:24:43.826138: Epoch time: 90.52 s
2024-12-17 07:24:45.045024: 
2024-12-17 07:24:45.046586: Epoch 71
2024-12-17 07:24:45.047505: Current learning rate: 0.00562
2024-12-17 07:26:15.584041: Validation loss did not improve from -0.50477. Patience: 31/50
2024-12-17 07:26:15.585164: train_loss -0.7321
2024-12-17 07:26:15.586347: val_loss -0.4472
2024-12-17 07:26:15.587198: Pseudo dice [0.7055]
2024-12-17 07:26:15.588141: Epoch time: 90.54 s
2024-12-17 07:26:16.864460: 
2024-12-17 07:26:16.866344: Epoch 72
2024-12-17 07:26:16.867220: Current learning rate: 0.00555
2024-12-17 07:27:47.612748: Validation loss did not improve from -0.50477. Patience: 32/50
2024-12-17 07:27:47.614067: train_loss -0.7291
2024-12-17 07:27:47.615078: val_loss -0.4765
2024-12-17 07:27:47.615833: Pseudo dice [0.7206]
2024-12-17 07:27:47.616536: Epoch time: 90.75 s
2024-12-17 07:27:48.840880: 
2024-12-17 07:27:48.842384: Epoch 73
2024-12-17 07:27:48.843138: Current learning rate: 0.00549
2024-12-17 07:29:19.366279: Validation loss did not improve from -0.50477. Patience: 33/50
2024-12-17 07:29:19.367672: train_loss -0.7271
2024-12-17 07:29:19.368845: val_loss -0.3635
2024-12-17 07:29:19.369770: Pseudo dice [0.6517]
2024-12-17 07:29:19.370621: Epoch time: 90.53 s
2024-12-17 07:29:20.641700: 
2024-12-17 07:29:20.643546: Epoch 74
2024-12-17 07:29:20.644476: Current learning rate: 0.00542
2024-12-17 07:30:51.284150: Validation loss did not improve from -0.50477. Patience: 34/50
2024-12-17 07:30:51.285293: train_loss -0.7352
2024-12-17 07:30:51.286161: val_loss -0.4441
2024-12-17 07:30:51.286948: Pseudo dice [0.7001]
2024-12-17 07:30:51.287802: Epoch time: 90.64 s
2024-12-17 07:30:52.917006: 
2024-12-17 07:30:52.918580: Epoch 75
2024-12-17 07:30:52.919336: Current learning rate: 0.00536
2024-12-17 07:32:23.450897: Validation loss did not improve from -0.50477. Patience: 35/50
2024-12-17 07:32:23.452113: train_loss -0.7345
2024-12-17 07:32:23.452980: val_loss -0.4463
2024-12-17 07:32:23.453681: Pseudo dice [0.7084]
2024-12-17 07:32:23.454467: Epoch time: 90.54 s
2024-12-17 07:32:24.728426: 
2024-12-17 07:32:24.730057: Epoch 76
2024-12-17 07:32:24.730798: Current learning rate: 0.00529
2024-12-17 07:33:55.174989: Validation loss did not improve from -0.50477. Patience: 36/50
2024-12-17 07:33:55.176441: train_loss -0.735
2024-12-17 07:33:55.177644: val_loss -0.4977
2024-12-17 07:33:55.178318: Pseudo dice [0.7226]
2024-12-17 07:33:55.178940: Epoch time: 90.45 s
2024-12-17 07:33:56.462677: 
2024-12-17 07:33:56.464305: Epoch 77
2024-12-17 07:33:56.465225: Current learning rate: 0.00523
2024-12-17 07:35:26.874777: Validation loss did not improve from -0.50477. Patience: 37/50
2024-12-17 07:35:26.875852: train_loss -0.7461
2024-12-17 07:35:26.876742: val_loss -0.3537
2024-12-17 07:35:26.877478: Pseudo dice [0.6687]
2024-12-17 07:35:26.878275: Epoch time: 90.41 s
2024-12-17 07:35:28.131506: 
2024-12-17 07:35:28.133084: Epoch 78
2024-12-17 07:35:28.133909: Current learning rate: 0.00517
2024-12-17 07:36:58.453279: Validation loss did not improve from -0.50477. Patience: 38/50
2024-12-17 07:36:58.454589: train_loss -0.7361
2024-12-17 07:36:58.455667: val_loss -0.4256
2024-12-17 07:36:58.456512: Pseudo dice [0.6967]
2024-12-17 07:36:58.457259: Epoch time: 90.32 s
2024-12-17 07:36:59.721909: 
2024-12-17 07:36:59.723421: Epoch 79
2024-12-17 07:36:59.724266: Current learning rate: 0.0051
2024-12-17 07:38:29.939189: Validation loss did not improve from -0.50477. Patience: 39/50
2024-12-17 07:38:29.940393: train_loss -0.7346
2024-12-17 07:38:29.941350: val_loss -0.3933
2024-12-17 07:38:29.942109: Pseudo dice [0.6929]
2024-12-17 07:38:29.943108: Epoch time: 90.22 s
2024-12-17 07:38:31.573597: 
2024-12-17 07:38:31.575407: Epoch 80
2024-12-17 07:38:31.576301: Current learning rate: 0.00504
2024-12-17 07:40:01.777136: Validation loss did not improve from -0.50477. Patience: 40/50
2024-12-17 07:40:01.778267: train_loss -0.7372
2024-12-17 07:40:01.780438: val_loss -0.4621
2024-12-17 07:40:01.781337: Pseudo dice [0.6983]
2024-12-17 07:40:01.782493: Epoch time: 90.21 s
2024-12-17 07:40:03.104155: 
2024-12-17 07:40:03.105579: Epoch 81
2024-12-17 07:40:03.106467: Current learning rate: 0.00497
2024-12-17 07:41:33.217404: Validation loss did not improve from -0.50477. Patience: 41/50
2024-12-17 07:41:33.218452: train_loss -0.7376
2024-12-17 07:41:33.219323: val_loss -0.4744
2024-12-17 07:41:33.220175: Pseudo dice [0.7047]
2024-12-17 07:41:33.221015: Epoch time: 90.12 s
2024-12-17 07:41:34.569035: 
2024-12-17 07:41:34.570715: Epoch 82
2024-12-17 07:41:34.571712: Current learning rate: 0.00491
2024-12-17 07:43:04.612671: Validation loss did not improve from -0.50477. Patience: 42/50
2024-12-17 07:43:04.613655: train_loss -0.7401
2024-12-17 07:43:04.614531: val_loss -0.414
2024-12-17 07:43:04.615315: Pseudo dice [0.6897]
2024-12-17 07:43:04.616024: Epoch time: 90.05 s
2024-12-17 07:43:06.158978: 
2024-12-17 07:43:06.160665: Epoch 83
2024-12-17 07:43:06.161453: Current learning rate: 0.00484
2024-12-17 07:44:36.161319: Validation loss did not improve from -0.50477. Patience: 43/50
2024-12-17 07:44:36.162601: train_loss -0.7456
2024-12-17 07:44:36.163591: val_loss -0.3835
2024-12-17 07:44:36.164300: Pseudo dice [0.6635]
2024-12-17 07:44:36.165031: Epoch time: 90.0 s
2024-12-17 07:44:37.348202: 
2024-12-17 07:44:37.349792: Epoch 84
2024-12-17 07:44:37.350646: Current learning rate: 0.00478
2024-12-17 07:46:07.395669: Validation loss did not improve from -0.50477. Patience: 44/50
2024-12-17 07:46:07.406014: train_loss -0.7454
2024-12-17 07:46:07.408239: val_loss -0.3814
2024-12-17 07:46:07.409099: Pseudo dice [0.6841]
2024-12-17 07:46:07.410097: Epoch time: 90.06 s
2024-12-17 07:46:08.981695: 
2024-12-17 07:46:08.983789: Epoch 85
2024-12-17 07:46:08.984827: Current learning rate: 0.00471
2024-12-17 07:47:39.135192: Validation loss did not improve from -0.50477. Patience: 45/50
2024-12-17 07:47:39.136504: train_loss -0.7393
2024-12-17 07:47:39.137516: val_loss -0.3749
2024-12-17 07:47:39.138226: Pseudo dice [0.6703]
2024-12-17 07:47:39.138975: Epoch time: 90.16 s
2024-12-17 07:47:40.319777: 
2024-12-17 07:47:40.321675: Epoch 86
2024-12-17 07:47:40.322589: Current learning rate: 0.00465
2024-12-17 07:49:10.617001: Validation loss did not improve from -0.50477. Patience: 46/50
2024-12-17 07:49:10.618210: train_loss -0.7471
2024-12-17 07:49:10.619051: val_loss -0.4142
2024-12-17 07:49:10.619855: Pseudo dice [0.6845]
2024-12-17 07:49:10.620522: Epoch time: 90.3 s
2024-12-17 07:49:11.894073: 
2024-12-17 07:49:11.895717: Epoch 87
2024-12-17 07:49:11.896687: Current learning rate: 0.00458
2024-12-17 07:50:42.109490: Validation loss did not improve from -0.50477. Patience: 47/50
2024-12-17 07:50:42.110618: train_loss -0.748
2024-12-17 07:50:42.111580: val_loss -0.4612
2024-12-17 07:50:42.112298: Pseudo dice [0.7206]
2024-12-17 07:50:42.112935: Epoch time: 90.22 s
2024-12-17 07:50:43.345461: 
2024-12-17 07:50:43.347639: Epoch 88
2024-12-17 07:50:43.348447: Current learning rate: 0.00452
2024-12-17 07:52:13.599350: Validation loss did not improve from -0.50477. Patience: 48/50
2024-12-17 07:52:13.600711: train_loss -0.7473
2024-12-17 07:52:13.601789: val_loss -0.4173
2024-12-17 07:52:13.602586: Pseudo dice [0.6849]
2024-12-17 07:52:13.603633: Epoch time: 90.26 s
2024-12-17 07:52:14.809153: 
2024-12-17 07:52:14.811004: Epoch 89
2024-12-17 07:52:14.812106: Current learning rate: 0.00445
2024-12-17 07:53:45.058048: Validation loss did not improve from -0.50477. Patience: 49/50
2024-12-17 07:53:45.059135: train_loss -0.7453
2024-12-17 07:53:45.060217: val_loss -0.4731
2024-12-17 07:53:45.061254: Pseudo dice [0.7253]
2024-12-17 07:53:45.062065: Epoch time: 90.25 s
2024-12-17 07:53:46.603969: 
2024-12-17 07:53:46.605519: Epoch 90
2024-12-17 07:53:46.606598: Current learning rate: 0.00438
2024-12-17 07:55:16.832373: Validation loss did not improve from -0.50477. Patience: 50/50
2024-12-17 07:55:16.833464: train_loss -0.7488
2024-12-17 07:55:16.834677: val_loss -0.4253
2024-12-17 07:55:16.835821: Pseudo dice [0.6977]
2024-12-17 07:55:16.836916: Epoch time: 90.23 s
2024-12-17 07:55:18.011598: 
2024-12-17 07:55:18.013597: Epoch 91
2024-12-17 07:55:18.014837: Current learning rate: 0.00432
2024-12-17 07:56:48.306565: Validation loss did not improve from -0.50477. Patience: 51/50
2024-12-17 07:56:48.307786: train_loss -0.7531
2024-12-17 07:56:48.308745: val_loss -0.4893
2024-12-17 07:56:48.309533: Pseudo dice [0.7339]
2024-12-17 07:56:48.310347: Epoch time: 90.3 s
2024-12-17 07:56:49.500365: 
2024-12-17 07:56:49.502092: Epoch 92
2024-12-17 07:56:49.503014: Current learning rate: 0.00425
2024-12-17 07:58:19.816759: Validation loss did not improve from -0.50477. Patience: 52/50
2024-12-17 07:58:19.819291: train_loss -0.7533
2024-12-17 07:58:19.820396: val_loss -0.4928
2024-12-17 07:58:19.821153: Pseudo dice [0.7309]
2024-12-17 07:58:19.821939: Epoch time: 90.32 s
2024-12-17 07:58:19.822634: Yayy! New best EMA pseudo Dice: 0.702
2024-12-17 07:58:21.765849: 
2024-12-17 07:58:21.767591: Epoch 93
2024-12-17 07:58:21.768517: Current learning rate: 0.00419
2024-12-17 07:59:51.700336: Validation loss did not improve from -0.50477. Patience: 53/50
2024-12-17 07:59:51.701669: train_loss -0.7486
2024-12-17 07:59:51.702724: val_loss -0.437
2024-12-17 07:59:51.703524: Pseudo dice [0.6929]
2024-12-17 07:59:51.704377: Epoch time: 89.94 s
2024-12-17 07:59:53.693622: 
2024-12-17 07:59:53.695103: Epoch 94
2024-12-17 07:59:53.695968: Current learning rate: 0.00412
2024-12-17 08:01:23.566754: Validation loss did not improve from -0.50477. Patience: 54/50
2024-12-17 08:01:23.567851: train_loss -0.7541
2024-12-17 08:01:23.568728: val_loss -0.3811
2024-12-17 08:01:23.569449: Pseudo dice [0.6644]
2024-12-17 08:01:23.570126: Epoch time: 89.88 s
2024-12-17 08:01:25.118730: 
2024-12-17 08:01:25.120286: Epoch 95
2024-12-17 08:01:25.121139: Current learning rate: 0.00405
2024-12-17 08:02:55.062662: Validation loss did not improve from -0.50477. Patience: 55/50
2024-12-17 08:02:55.063885: train_loss -0.7519
2024-12-17 08:02:55.064792: val_loss -0.4055
2024-12-17 08:02:55.065654: Pseudo dice [0.6896]
2024-12-17 08:02:55.066407: Epoch time: 89.95 s
2024-12-17 08:02:56.295904: 
2024-12-17 08:02:56.297396: Epoch 96
2024-12-17 08:02:56.298137: Current learning rate: 0.00399
2024-12-17 08:04:26.328256: Validation loss did not improve from -0.50477. Patience: 56/50
2024-12-17 08:04:26.329463: train_loss -0.7532
2024-12-17 08:04:26.330339: val_loss -0.3589
2024-12-17 08:04:26.331173: Pseudo dice [0.6703]
2024-12-17 08:04:26.331851: Epoch time: 90.03 s
2024-12-17 08:04:27.566060: 
2024-12-17 08:04:27.567275: Epoch 97
2024-12-17 08:04:27.568085: Current learning rate: 0.00392
2024-12-17 08:05:57.714666: Validation loss did not improve from -0.50477. Patience: 57/50
2024-12-17 08:05:57.716052: train_loss -0.7558
2024-12-17 08:05:57.716980: val_loss -0.4697
2024-12-17 08:05:57.717649: Pseudo dice [0.7245]
2024-12-17 08:05:57.718351: Epoch time: 90.15 s
2024-12-17 08:05:58.937007: 
2024-12-17 08:05:58.938497: Epoch 98
2024-12-17 08:05:58.939309: Current learning rate: 0.00385
2024-12-17 08:07:28.963689: Validation loss did not improve from -0.50477. Patience: 58/50
2024-12-17 08:07:28.964868: train_loss -0.7566
2024-12-17 08:07:28.965817: val_loss -0.4313
2024-12-17 08:07:28.966721: Pseudo dice [0.7023]
2024-12-17 08:07:28.967699: Epoch time: 90.03 s
2024-12-17 08:07:30.235633: 
2024-12-17 08:07:30.237273: Epoch 99
2024-12-17 08:07:30.238295: Current learning rate: 0.00379
2024-12-17 08:09:00.281871: Validation loss did not improve from -0.50477. Patience: 59/50
2024-12-17 08:09:00.283066: train_loss -0.7644
2024-12-17 08:09:00.284252: val_loss -0.4598
2024-12-17 08:09:00.284911: Pseudo dice [0.7238]
2024-12-17 08:09:00.285703: Epoch time: 90.05 s
2024-12-17 08:09:01.864070: 
2024-12-17 08:09:01.865741: Epoch 100
2024-12-17 08:09:01.866560: Current learning rate: 0.00372
2024-12-17 08:10:31.869895: Validation loss did not improve from -0.50477. Patience: 60/50
2024-12-17 08:10:31.870835: train_loss -0.7585
2024-12-17 08:10:31.871704: val_loss -0.4545
2024-12-17 08:10:31.872553: Pseudo dice [0.7244]
2024-12-17 08:10:31.873340: Epoch time: 90.01 s
2024-12-17 08:10:31.874105: Yayy! New best EMA pseudo Dice: 0.7026
2024-12-17 08:10:33.464709: 
2024-12-17 08:10:33.465965: Epoch 101
2024-12-17 08:10:33.466764: Current learning rate: 0.00365
2024-12-17 08:12:03.693081: Validation loss did not improve from -0.50477. Patience: 61/50
2024-12-17 08:12:03.694168: train_loss -0.7573
2024-12-17 08:12:03.695137: val_loss -0.3963
2024-12-17 08:12:03.695941: Pseudo dice [0.6794]
2024-12-17 08:12:03.696768: Epoch time: 90.23 s
2024-12-17 08:12:04.921601: 
2024-12-17 08:12:04.922982: Epoch 102
2024-12-17 08:12:04.923755: Current learning rate: 0.00359
2024-12-17 08:13:35.270852: Validation loss did not improve from -0.50477. Patience: 62/50
2024-12-17 08:13:35.272056: train_loss -0.7594
2024-12-17 08:13:35.273520: val_loss -0.4162
2024-12-17 08:13:35.274650: Pseudo dice [0.699]
2024-12-17 08:13:35.275703: Epoch time: 90.35 s
2024-12-17 08:13:36.541616: 
2024-12-17 08:13:36.543568: Epoch 103
2024-12-17 08:13:36.544334: Current learning rate: 0.00352
2024-12-17 08:15:06.803562: Validation loss did not improve from -0.50477. Patience: 63/50
2024-12-17 08:15:06.804877: train_loss -0.7657
2024-12-17 08:15:06.805749: val_loss -0.4925
2024-12-17 08:15:06.806562: Pseudo dice [0.7364]
2024-12-17 08:15:06.807348: Epoch time: 90.26 s
2024-12-17 08:15:06.808121: Yayy! New best EMA pseudo Dice: 0.7038
2024-12-17 08:15:08.391233: 
2024-12-17 08:15:08.393582: Epoch 104
2024-12-17 08:15:08.394323: Current learning rate: 0.00345
2024-12-17 08:16:38.655391: Validation loss did not improve from -0.50477. Patience: 64/50
2024-12-17 08:16:38.656336: train_loss -0.7587
2024-12-17 08:16:38.657693: val_loss -0.4262
2024-12-17 08:16:38.658664: Pseudo dice [0.6891]
2024-12-17 08:16:38.659761: Epoch time: 90.27 s
2024-12-17 08:16:40.598474: 
2024-12-17 08:16:40.600406: Epoch 105
2024-12-17 08:16:40.601323: Current learning rate: 0.00338
2024-12-17 08:18:10.937087: Validation loss did not improve from -0.50477. Patience: 65/50
2024-12-17 08:18:10.938293: train_loss -0.7609
2024-12-17 08:18:10.939030: val_loss -0.47
2024-12-17 08:18:10.939693: Pseudo dice [0.7219]
2024-12-17 08:18:10.940375: Epoch time: 90.34 s
2024-12-17 08:18:10.941056: Yayy! New best EMA pseudo Dice: 0.7043
2024-12-17 08:18:12.555177: 
2024-12-17 08:18:12.556801: Epoch 106
2024-12-17 08:18:12.557580: Current learning rate: 0.00332
2024-12-17 08:19:42.780333: Validation loss did not improve from -0.50477. Patience: 66/50
2024-12-17 08:19:42.781380: train_loss -0.7587
2024-12-17 08:19:42.782290: val_loss -0.4508
2024-12-17 08:19:42.782988: Pseudo dice [0.7036]
2024-12-17 08:19:42.783832: Epoch time: 90.23 s
2024-12-17 08:19:43.982562: 
2024-12-17 08:19:43.984168: Epoch 107
2024-12-17 08:19:43.985002: Current learning rate: 0.00325
2024-12-17 08:21:14.163732: Validation loss did not improve from -0.50477. Patience: 67/50
2024-12-17 08:21:14.164955: train_loss -0.7606
2024-12-17 08:21:14.166073: val_loss -0.4494
2024-12-17 08:21:14.167160: Pseudo dice [0.7122]
2024-12-17 08:21:14.168052: Epoch time: 90.18 s
2024-12-17 08:21:14.169035: Yayy! New best EMA pseudo Dice: 0.705
2024-12-17 08:21:15.742398: 
2024-12-17 08:21:15.743906: Epoch 108
2024-12-17 08:21:15.744956: Current learning rate: 0.00318
2024-12-17 08:22:45.946038: Validation loss did not improve from -0.50477. Patience: 68/50
2024-12-17 08:22:45.947347: train_loss -0.7639
2024-12-17 08:22:45.948461: val_loss -0.3708
2024-12-17 08:22:45.949398: Pseudo dice [0.6606]
2024-12-17 08:22:45.950234: Epoch time: 90.21 s
2024-12-17 08:22:47.160657: 
2024-12-17 08:22:47.161918: Epoch 109
2024-12-17 08:22:47.162785: Current learning rate: 0.00311
2024-12-17 08:24:17.597473: Validation loss did not improve from -0.50477. Patience: 69/50
2024-12-17 08:24:17.598451: train_loss -0.7622
2024-12-17 08:24:17.599267: val_loss -0.4266
2024-12-17 08:24:17.599991: Pseudo dice [0.7047]
2024-12-17 08:24:17.600806: Epoch time: 90.44 s
2024-12-17 08:24:19.160630: 
2024-12-17 08:24:19.161921: Epoch 110
2024-12-17 08:24:19.162721: Current learning rate: 0.00304
2024-12-17 08:25:49.209532: Validation loss did not improve from -0.50477. Patience: 70/50
2024-12-17 08:25:49.210520: train_loss -0.7659
2024-12-17 08:25:49.211429: val_loss -0.3515
2024-12-17 08:25:49.212379: Pseudo dice [0.666]
2024-12-17 08:25:49.213113: Epoch time: 90.05 s
2024-12-17 08:25:50.469430: 
2024-12-17 08:25:50.471099: Epoch 111
2024-12-17 08:25:50.471880: Current learning rate: 0.00297
2024-12-17 08:27:20.418779: Validation loss did not improve from -0.50477. Patience: 71/50
2024-12-17 08:27:20.419984: train_loss -0.7652
2024-12-17 08:27:20.420838: val_loss -0.4648
2024-12-17 08:27:20.421542: Pseudo dice [0.7113]
2024-12-17 08:27:20.422318: Epoch time: 89.95 s
2024-12-17 08:27:21.668977: 
2024-12-17 08:27:21.670424: Epoch 112
2024-12-17 08:27:21.671129: Current learning rate: 0.00291
2024-12-17 08:28:51.625262: Validation loss did not improve from -0.50477. Patience: 72/50
2024-12-17 08:28:51.626577: train_loss -0.7626
2024-12-17 08:28:51.628122: val_loss -0.4067
2024-12-17 08:28:51.629107: Pseudo dice [0.6963]
2024-12-17 08:28:51.629992: Epoch time: 89.96 s
2024-12-17 08:28:52.871768: 
2024-12-17 08:28:52.873452: Epoch 113
2024-12-17 08:28:52.874353: Current learning rate: 0.00284
2024-12-17 08:30:22.819930: Validation loss did not improve from -0.50477. Patience: 73/50
2024-12-17 08:30:22.820758: train_loss -0.7663
2024-12-17 08:30:22.821960: val_loss -0.4375
2024-12-17 08:30:22.822702: Pseudo dice [0.7047]
2024-12-17 08:30:22.823404: Epoch time: 89.95 s
2024-12-17 08:30:24.103472: 
2024-12-17 08:30:24.105283: Epoch 114
2024-12-17 08:30:24.105930: Current learning rate: 0.00277
2024-12-17 08:31:54.125259: Validation loss did not improve from -0.50477. Patience: 74/50
2024-12-17 08:31:54.126287: train_loss -0.7691
2024-12-17 08:31:54.127293: val_loss -0.3816
2024-12-17 08:31:54.128098: Pseudo dice [0.6691]
2024-12-17 08:31:54.128956: Epoch time: 90.02 s
2024-12-17 08:31:55.762014: 
2024-12-17 08:31:55.763695: Epoch 115
2024-12-17 08:31:55.764618: Current learning rate: 0.0027
2024-12-17 08:33:25.724479: Validation loss did not improve from -0.50477. Patience: 75/50
2024-12-17 08:33:25.725589: train_loss -0.7712
2024-12-17 08:33:25.726390: val_loss -0.4125
2024-12-17 08:33:25.727259: Pseudo dice [0.7012]
2024-12-17 08:33:25.728307: Epoch time: 89.96 s
2024-12-17 08:33:27.709301: 
2024-12-17 08:33:27.711193: Epoch 116
2024-12-17 08:33:27.712207: Current learning rate: 0.00263
2024-12-17 08:34:57.791438: Validation loss did not improve from -0.50477. Patience: 76/50
2024-12-17 08:34:57.792366: train_loss -0.7664
2024-12-17 08:34:57.793297: val_loss -0.3893
2024-12-17 08:34:57.794114: Pseudo dice [0.682]
2024-12-17 08:34:57.794981: Epoch time: 90.09 s
2024-12-17 08:34:59.089935: 
2024-12-17 08:34:59.091903: Epoch 117
2024-12-17 08:34:59.092895: Current learning rate: 0.00256
2024-12-17 08:36:29.089765: Validation loss did not improve from -0.50477. Patience: 77/50
2024-12-17 08:36:29.091619: train_loss -0.7729
2024-12-17 08:36:29.093063: val_loss -0.436
2024-12-17 08:36:29.093769: Pseudo dice [0.7092]
2024-12-17 08:36:29.094558: Epoch time: 90.0 s
2024-12-17 08:36:30.340057: 
2024-12-17 08:36:30.341899: Epoch 118
2024-12-17 08:36:30.342593: Current learning rate: 0.00249
2024-12-17 08:38:00.472397: Validation loss did not improve from -0.50477. Patience: 78/50
2024-12-17 08:38:00.473398: train_loss -0.7744
2024-12-17 08:38:00.474247: val_loss -0.4357
2024-12-17 08:38:00.474837: Pseudo dice [0.7114]
2024-12-17 08:38:00.475483: Epoch time: 90.13 s
2024-12-17 08:38:01.706697: 
2024-12-17 08:38:01.708443: Epoch 119
2024-12-17 08:38:01.709208: Current learning rate: 0.00242
2024-12-17 08:39:32.079967: Validation loss did not improve from -0.50477. Patience: 79/50
2024-12-17 08:39:32.080641: train_loss -0.7733
2024-12-17 08:39:32.081341: val_loss -0.4118
2024-12-17 08:39:32.082108: Pseudo dice [0.6965]
2024-12-17 08:39:32.082803: Epoch time: 90.37 s
2024-12-17 08:39:33.692094: 
2024-12-17 08:39:33.693396: Epoch 120
2024-12-17 08:39:33.694064: Current learning rate: 0.00235
2024-12-17 08:41:04.120213: Validation loss did not improve from -0.50477. Patience: 80/50
2024-12-17 08:41:04.121478: train_loss -0.7707
2024-12-17 08:41:04.122357: val_loss -0.3622
2024-12-17 08:41:04.123294: Pseudo dice [0.6706]
2024-12-17 08:41:04.124044: Epoch time: 90.43 s
2024-12-17 08:41:05.409864: 
2024-12-17 08:41:05.411684: Epoch 121
2024-12-17 08:41:05.412511: Current learning rate: 0.00228
2024-12-17 08:42:35.863979: Validation loss did not improve from -0.50477. Patience: 81/50
2024-12-17 08:42:35.865164: train_loss -0.7764
2024-12-17 08:42:35.866086: val_loss -0.3673
2024-12-17 08:42:35.866874: Pseudo dice [0.6797]
2024-12-17 08:42:35.867826: Epoch time: 90.46 s
2024-12-17 08:42:37.126736: 
2024-12-17 08:42:37.128477: Epoch 122
2024-12-17 08:42:37.129277: Current learning rate: 0.00221
2024-12-17 08:44:07.683047: Validation loss did not improve from -0.50477. Patience: 82/50
2024-12-17 08:44:07.684219: train_loss -0.7725
2024-12-17 08:44:07.684977: val_loss -0.4295
2024-12-17 08:44:07.685676: Pseudo dice [0.7064]
2024-12-17 08:44:07.686406: Epoch time: 90.56 s
2024-12-17 08:44:08.915810: 
2024-12-17 08:44:08.917543: Epoch 123
2024-12-17 08:44:08.918339: Current learning rate: 0.00214
2024-12-17 08:45:39.464787: Validation loss did not improve from -0.50477. Patience: 83/50
2024-12-17 08:45:39.466333: train_loss -0.7732
2024-12-17 08:45:39.467557: val_loss -0.3353
2024-12-17 08:45:39.468446: Pseudo dice [0.6639]
2024-12-17 08:45:39.469208: Epoch time: 90.55 s
2024-12-17 08:45:40.775557: 
2024-12-17 08:45:40.777296: Epoch 124
2024-12-17 08:45:40.778128: Current learning rate: 0.00207
2024-12-17 08:47:10.947603: Validation loss did not improve from -0.50477. Patience: 84/50
2024-12-17 08:47:10.948673: train_loss -0.771
2024-12-17 08:47:10.949645: val_loss -0.385
2024-12-17 08:47:10.950471: Pseudo dice [0.6841]
2024-12-17 08:47:10.951215: Epoch time: 90.17 s
2024-12-17 08:47:12.586314: 
2024-12-17 08:47:12.588182: Epoch 125
2024-12-17 08:47:12.589155: Current learning rate: 0.00199
2024-12-17 08:48:42.606921: Validation loss did not improve from -0.50477. Patience: 85/50
2024-12-17 08:48:42.608045: train_loss -0.7752
2024-12-17 08:48:42.608959: val_loss -0.4273
2024-12-17 08:48:42.609633: Pseudo dice [0.7004]
2024-12-17 08:48:42.610457: Epoch time: 90.02 s
2024-12-17 08:48:44.369711: 
2024-12-17 08:48:44.370836: Epoch 126
2024-12-17 08:48:44.371680: Current learning rate: 0.00192
2024-12-17 08:50:14.294387: Validation loss did not improve from -0.50477. Patience: 86/50
2024-12-17 08:50:14.295657: train_loss -0.7782
2024-12-17 08:50:14.296828: val_loss -0.4531
2024-12-17 08:50:14.297832: Pseudo dice [0.7102]
2024-12-17 08:50:14.298794: Epoch time: 89.93 s
2024-12-17 08:50:15.598042: 
2024-12-17 08:50:15.599592: Epoch 127
2024-12-17 08:50:15.600618: Current learning rate: 0.00185
2024-12-17 08:51:45.595970: Validation loss did not improve from -0.50477. Patience: 87/50
2024-12-17 08:51:45.597401: train_loss -0.7762
2024-12-17 08:51:45.598595: val_loss -0.3146
2024-12-17 08:51:45.599468: Pseudo dice [0.6314]
2024-12-17 08:51:45.600474: Epoch time: 90.0 s
2024-12-17 08:51:46.877118: 
2024-12-17 08:51:46.878661: Epoch 128
2024-12-17 08:51:46.879604: Current learning rate: 0.00178
2024-12-17 08:53:16.877623: Validation loss did not improve from -0.50477. Patience: 88/50
2024-12-17 08:53:16.878577: train_loss -0.7726
2024-12-17 08:53:16.879428: val_loss -0.3885
2024-12-17 08:53:16.880204: Pseudo dice [0.6855]
2024-12-17 08:53:16.880858: Epoch time: 90.0 s
2024-12-17 08:53:18.145933: 
2024-12-17 08:53:18.147624: Epoch 129
2024-12-17 08:53:18.148370: Current learning rate: 0.0017
2024-12-17 08:54:48.117796: Validation loss did not improve from -0.50477. Patience: 89/50
2024-12-17 08:54:48.118695: train_loss -0.7799
2024-12-17 08:54:48.119642: val_loss -0.4232
2024-12-17 08:54:48.120468: Pseudo dice [0.6982]
2024-12-17 08:54:48.121290: Epoch time: 89.97 s
2024-12-17 08:54:49.733373: 
2024-12-17 08:54:49.735098: Epoch 130
2024-12-17 08:54:49.735827: Current learning rate: 0.00163
2024-12-17 08:56:19.715189: Validation loss did not improve from -0.50477. Patience: 90/50
2024-12-17 08:56:19.715730: train_loss -0.7792
2024-12-17 08:56:19.716523: val_loss -0.4249
2024-12-17 08:56:19.717325: Pseudo dice [0.7018]
2024-12-17 08:56:19.718188: Epoch time: 89.98 s
2024-12-17 08:56:20.964159: 
2024-12-17 08:56:20.965734: Epoch 131
2024-12-17 08:56:20.966730: Current learning rate: 0.00156
2024-12-17 08:57:51.005081: Validation loss did not improve from -0.50477. Patience: 91/50
2024-12-17 08:57:51.005988: train_loss -0.7774
2024-12-17 08:57:51.006803: val_loss -0.4436
2024-12-17 08:57:51.007495: Pseudo dice [0.7036]
2024-12-17 08:57:51.008255: Epoch time: 90.04 s
2024-12-17 08:57:52.254776: 
2024-12-17 08:57:52.256381: Epoch 132
2024-12-17 08:57:52.257098: Current learning rate: 0.00148
2024-12-17 08:59:22.513577: Validation loss did not improve from -0.50477. Patience: 92/50
2024-12-17 08:59:22.514508: train_loss -0.7761
2024-12-17 08:59:22.515288: val_loss -0.4743
2024-12-17 08:59:22.516091: Pseudo dice [0.7247]
2024-12-17 08:59:22.516937: Epoch time: 90.26 s
2024-12-17 08:59:23.828760: 
2024-12-17 08:59:23.830304: Epoch 133
2024-12-17 08:59:23.831107: Current learning rate: 0.00141
2024-12-17 09:00:54.159959: Validation loss did not improve from -0.50477. Patience: 93/50
2024-12-17 09:00:54.160996: train_loss -0.7805
2024-12-17 09:00:54.161911: val_loss -0.4438
2024-12-17 09:00:54.162718: Pseudo dice [0.7134]
2024-12-17 09:00:54.163441: Epoch time: 90.33 s
2024-12-17 09:00:55.383781: 
2024-12-17 09:00:55.385381: Epoch 134
2024-12-17 09:00:55.386578: Current learning rate: 0.00133
2024-12-17 09:02:25.688882: Validation loss did not improve from -0.50477. Patience: 94/50
2024-12-17 09:02:25.690050: train_loss -0.7787
2024-12-17 09:02:25.690933: val_loss -0.4603
2024-12-17 09:02:25.691611: Pseudo dice [0.7181]
2024-12-17 09:02:25.692326: Epoch time: 90.31 s
2024-12-17 09:02:27.345867: 
2024-12-17 09:02:27.347339: Epoch 135
2024-12-17 09:02:27.348283: Current learning rate: 0.00126
2024-12-17 09:03:57.637876: Validation loss did not improve from -0.50477. Patience: 95/50
2024-12-17 09:03:57.640728: train_loss -0.7797
2024-12-17 09:03:57.641843: val_loss -0.3896
2024-12-17 09:03:57.642664: Pseudo dice [0.6833]
2024-12-17 09:03:57.643411: Epoch time: 90.3 s
2024-12-17 09:03:58.924324: 
2024-12-17 09:03:58.926124: Epoch 136
2024-12-17 09:03:58.927047: Current learning rate: 0.00118
2024-12-17 09:05:29.253731: Validation loss did not improve from -0.50477. Patience: 96/50
2024-12-17 09:05:29.254873: train_loss -0.7807
2024-12-17 09:05:29.256180: val_loss -0.4005
2024-12-17 09:05:29.256934: Pseudo dice [0.6877]
2024-12-17 09:05:29.257788: Epoch time: 90.33 s
2024-12-17 09:05:31.227669: 
2024-12-17 09:05:31.229223: Epoch 137
2024-12-17 09:05:31.230134: Current learning rate: 0.00111
2024-12-17 09:07:01.643780: Validation loss did not improve from -0.50477. Patience: 97/50
2024-12-17 09:07:01.644903: train_loss -0.7792
2024-12-17 09:07:01.645828: val_loss -0.4303
2024-12-17 09:07:01.646497: Pseudo dice [0.7157]
2024-12-17 09:07:01.647243: Epoch time: 90.42 s
2024-12-17 09:07:02.912792: 
2024-12-17 09:07:02.914405: Epoch 138
2024-12-17 09:07:02.915118: Current learning rate: 0.00103
2024-12-17 09:08:33.290089: Validation loss did not improve from -0.50477. Patience: 98/50
2024-12-17 09:08:33.291280: train_loss -0.7814
2024-12-17 09:08:33.292208: val_loss -0.413
2024-12-17 09:08:33.292861: Pseudo dice [0.6961]
2024-12-17 09:08:33.293555: Epoch time: 90.38 s
2024-12-17 09:08:34.535345: 
2024-12-17 09:08:34.536830: Epoch 139
2024-12-17 09:08:34.537564: Current learning rate: 0.00095
2024-12-17 09:10:04.902929: Validation loss did not improve from -0.50477. Patience: 99/50
2024-12-17 09:10:04.904109: train_loss -0.7806
2024-12-17 09:10:04.904983: val_loss -0.3973
2024-12-17 09:10:04.905699: Pseudo dice [0.6936]
2024-12-17 09:10:04.906492: Epoch time: 90.37 s
2024-12-17 09:10:06.537825: 
2024-12-17 09:10:06.539514: Epoch 140
2024-12-17 09:10:06.540485: Current learning rate: 0.00087
2024-12-17 09:11:37.265742: Validation loss did not improve from -0.50477. Patience: 100/50
2024-12-17 09:11:37.266976: train_loss -0.7788
2024-12-17 09:11:37.268081: val_loss -0.429
2024-12-17 09:11:37.268888: Pseudo dice [0.7052]
2024-12-17 09:11:37.269655: Epoch time: 90.73 s
2024-12-17 09:11:38.771748: 
2024-12-17 09:11:38.773721: Epoch 141
2024-12-17 09:11:38.774601: Current learning rate: 0.00079
2024-12-17 09:13:09.442538: Validation loss did not improve from -0.50477. Patience: 101/50
2024-12-17 09:13:09.443906: train_loss -0.7792
2024-12-17 09:13:09.444885: val_loss -0.4131
2024-12-17 09:13:09.445603: Pseudo dice [0.6937]
2024-12-17 09:13:09.446289: Epoch time: 90.67 s
2024-12-17 09:13:10.727112: 
2024-12-17 09:13:10.728492: Epoch 142
2024-12-17 09:13:10.729436: Current learning rate: 0.00071
2024-12-17 09:14:41.211923: Validation loss did not improve from -0.50477. Patience: 102/50
2024-12-17 09:14:41.213372: train_loss -0.7825
2024-12-17 09:14:41.214480: val_loss -0.4092
2024-12-17 09:14:41.215298: Pseudo dice [0.698]
2024-12-17 09:14:41.216132: Epoch time: 90.49 s
2024-12-17 09:14:42.490311: 
2024-12-17 09:14:42.492230: Epoch 143
2024-12-17 09:14:42.493183: Current learning rate: 0.00063
2024-12-17 09:16:12.952708: Validation loss did not improve from -0.50477. Patience: 103/50
2024-12-17 09:16:12.953979: train_loss -0.7845
2024-12-17 09:16:12.954961: val_loss -0.4275
2024-12-17 09:16:12.955772: Pseudo dice [0.7049]
2024-12-17 09:16:12.956520: Epoch time: 90.46 s
2024-12-17 09:16:14.250185: 
2024-12-17 09:16:14.251908: Epoch 144
2024-12-17 09:16:14.252640: Current learning rate: 0.00055
2024-12-17 09:17:44.707149: Validation loss did not improve from -0.50477. Patience: 104/50
2024-12-17 09:17:44.708174: train_loss -0.7836
2024-12-17 09:17:44.709114: val_loss -0.44
2024-12-17 09:17:44.709907: Pseudo dice [0.7088]
2024-12-17 09:17:44.710607: Epoch time: 90.46 s
2024-12-17 09:17:46.349470: 
2024-12-17 09:17:46.350937: Epoch 145
2024-12-17 09:17:46.351666: Current learning rate: 0.00047
2024-12-17 09:19:16.642761: Validation loss did not improve from -0.50477. Patience: 105/50
2024-12-17 09:19:16.643945: train_loss -0.7843
2024-12-17 09:19:16.644901: val_loss -0.4684
2024-12-17 09:19:16.645597: Pseudo dice [0.7206]
2024-12-17 09:19:16.646457: Epoch time: 90.3 s
2024-12-17 09:19:17.897569: 
2024-12-17 09:19:17.899095: Epoch 146
2024-12-17 09:19:17.899931: Current learning rate: 0.00038
2024-12-17 09:20:48.167426: Validation loss did not improve from -0.50477. Patience: 106/50
2024-12-17 09:20:48.168650: train_loss -0.7839
2024-12-17 09:20:48.169490: val_loss -0.4378
2024-12-17 09:20:48.170322: Pseudo dice [0.7134]
2024-12-17 09:20:48.171121: Epoch time: 90.27 s
2024-12-17 09:20:49.435103: 
2024-12-17 09:20:49.436866: Epoch 147
2024-12-17 09:20:49.437732: Current learning rate: 0.0003
2024-12-17 09:22:19.912659: Validation loss did not improve from -0.50477. Patience: 107/50
2024-12-17 09:22:19.913961: train_loss -0.7849
2024-12-17 09:22:19.914931: val_loss -0.4432
2024-12-17 09:22:19.915571: Pseudo dice [0.7083]
2024-12-17 09:22:19.916344: Epoch time: 90.48 s
2024-12-17 09:22:21.673331: 
2024-12-17 09:22:21.675264: Epoch 148
2024-12-17 09:22:21.676105: Current learning rate: 0.00021
2024-12-17 09:23:51.996053: Validation loss did not improve from -0.50477. Patience: 108/50
2024-12-17 09:23:51.997334: train_loss -0.786
2024-12-17 09:23:51.998271: val_loss -0.4226
2024-12-17 09:23:51.998907: Pseudo dice [0.695]
2024-12-17 09:23:51.999569: Epoch time: 90.33 s
2024-12-17 09:23:53.227640: 
2024-12-17 09:23:53.229196: Epoch 149
2024-12-17 09:23:53.230290: Current learning rate: 0.00011
2024-12-17 09:25:23.574017: Validation loss did not improve from -0.50477. Patience: 109/50
2024-12-17 09:25:23.575069: train_loss -0.7865
2024-12-17 09:25:23.576044: val_loss -0.4165
2024-12-17 09:25:23.576892: Pseudo dice [0.6905]
2024-12-17 09:25:23.577827: Epoch time: 90.35 s
2024-12-17 09:25:25.285413: Training done.
2024-12-17 09:25:25.413054: Using splits from existing split file: /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset309_Sohee_Calcium_OCT_CrossValidation/splits_final.json
2024-12-17 09:25:25.415009: The split file contains 5 splits.
2024-12-17 09:25:25.415990: Desired fold for training: 3
2024-12-17 09:25:25.416854: This split has 7 training and 1 validation cases.
2024-12-17 09:25:25.418112: predicting 701-013
2024-12-17 09:25:25.428789: 701-013, shape torch.Size([1, 375, 498, 498]), rank 0
2024-12-17 09:27:25.269150: Validation complete
2024-12-17 09:27:25.270347: Mean Validation Dice:  0.7000132784707112

############################
INFO: You are using the old nnU-Net default plans. We have updated our recommendations. Please consider using those instead! Read more here: https://github.com/MIC-DKFZ/nnUNet/blob/master/documentation/resenc_presets.md
############################

Using device: cuda:0

#######################################################################
Please cite the following paper when using nnU-Net:
Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.
#######################################################################

2024-12-17 09:27:31.284837: Using torch.compile...
/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
################### Loading pretrained weights from file  /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_results/Dataset300_Lumen_and_Wall_OCT/nnUNetTrainer__nnUNetPreprocessPlans__3d_fullres/fold_all/checkpoint_best.pth ###################
Below is the list of overlapping blocks in pretrained model and nnUNet architecture:
encoder.stages.0.0.convs.0.conv.weight shape torch.Size([32, 1, 3, 3, 3])
encoder.stages.0.0.convs.0.conv.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.norm.weight shape torch.Size([32])
encoder.stages.0.0.convs.0.norm.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.0.weight shape torch.Size([32, 1, 3, 3, 3])
encoder.stages.0.0.convs.0.all_modules.0.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.1.weight shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.1.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
encoder.stages.0.0.convs.1.conv.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.norm.weight shape torch.Size([32])
encoder.stages.0.0.convs.1.norm.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
encoder.stages.0.0.convs.1.all_modules.0.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.1.weight shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.1.bias shape torch.Size([32])
encoder.stages.1.0.convs.0.conv.weight shape torch.Size([64, 32, 3, 3, 3])
encoder.stages.1.0.convs.0.conv.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.norm.weight shape torch.Size([64])
encoder.stages.1.0.convs.0.norm.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.0.weight shape torch.Size([64, 32, 3, 3, 3])
encoder.stages.1.0.convs.0.all_modules.0.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.1.weight shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.1.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
encoder.stages.1.0.convs.1.conv.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.norm.weight shape torch.Size([64])
encoder.stages.1.0.convs.1.norm.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
encoder.stages.1.0.convs.1.all_modules.0.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.1.weight shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.1.bias shape torch.Size([64])
encoder.stages.2.0.convs.0.conv.weight shape torch.Size([128, 64, 3, 3, 3])
encoder.stages.2.0.convs.0.conv.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.norm.weight shape torch.Size([128])
encoder.stages.2.0.convs.0.norm.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.0.weight shape torch.Size([128, 64, 3, 3, 3])
encoder.stages.2.0.convs.0.all_modules.0.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.1.weight shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.1.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
encoder.stages.2.0.convs.1.conv.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.norm.weight shape torch.Size([128])
encoder.stages.2.0.convs.1.norm.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
encoder.stages.2.0.convs.1.all_modules.0.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.1.weight shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.1.bias shape torch.Size([128])
encoder.stages.3.0.convs.0.conv.weight shape torch.Size([256, 128, 3, 3, 3])
encoder.stages.3.0.convs.0.conv.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.norm.weight shape torch.Size([256])
encoder.stages.3.0.convs.0.norm.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.0.weight shape torch.Size([256, 128, 3, 3, 3])
encoder.stages.3.0.convs.0.all_modules.0.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.1.weight shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.1.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
encoder.stages.3.0.convs.1.conv.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.norm.weight shape torch.Size([256])
encoder.stages.3.0.convs.1.norm.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
encoder.stages.3.0.convs.1.all_modules.0.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.1.weight shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.1.bias shape torch.Size([256])
encoder.stages.4.0.convs.0.conv.weight shape torch.Size([320, 256, 3, 3, 3])
encoder.stages.4.0.convs.0.conv.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.norm.weight shape torch.Size([320])
encoder.stages.4.0.convs.0.norm.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.0.weight shape torch.Size([320, 256, 3, 3, 3])
encoder.stages.4.0.convs.0.all_modules.0.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.1.weight shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.1.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.4.0.convs.1.conv.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.norm.weight shape torch.Size([320])
encoder.stages.4.0.convs.1.norm.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.4.0.convs.1.all_modules.0.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.1.weight shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.1.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.0.conv.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.norm.weight shape torch.Size([320])
encoder.stages.5.0.convs.0.norm.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.0.all_modules.0.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.1.weight shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.1.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.1.conv.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.norm.weight shape torch.Size([320])
encoder.stages.5.0.convs.1.norm.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.1.all_modules.0.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.1.weight shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.0.0.convs.0.conv.weight shape torch.Size([32, 1, 3, 3, 3])
decoder.encoder.stages.0.0.convs.0.conv.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.norm.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.norm.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.0.weight shape torch.Size([32, 1, 3, 3, 3])
decoder.encoder.stages.0.0.convs.0.all_modules.0.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.1.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.1.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.encoder.stages.0.0.convs.1.conv.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.norm.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.norm.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.encoder.stages.0.0.convs.1.all_modules.0.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.1.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.1.bias shape torch.Size([32])
decoder.encoder.stages.1.0.convs.0.conv.weight shape torch.Size([64, 32, 3, 3, 3])
decoder.encoder.stages.1.0.convs.0.conv.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.norm.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.norm.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.0.weight shape torch.Size([64, 32, 3, 3, 3])
decoder.encoder.stages.1.0.convs.0.all_modules.0.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.1.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.1.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.encoder.stages.1.0.convs.1.conv.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.norm.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.norm.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.encoder.stages.1.0.convs.1.all_modules.0.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.1.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.1.bias shape torch.Size([64])
decoder.encoder.stages.2.0.convs.0.conv.weight shape torch.Size([128, 64, 3, 3, 3])
decoder.encoder.stages.2.0.convs.0.conv.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.norm.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.norm.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.0.weight shape torch.Size([128, 64, 3, 3, 3])
decoder.encoder.stages.2.0.convs.0.all_modules.0.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.1.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.1.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.encoder.stages.2.0.convs.1.conv.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.norm.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.norm.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.encoder.stages.2.0.convs.1.all_modules.0.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.1.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.1.bias shape torch.Size([128])
decoder.encoder.stages.3.0.convs.0.conv.weight shape torch.Size([256, 128, 3, 3, 3])
decoder.encoder.stages.3.0.convs.0.conv.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.norm.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.norm.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.0.weight shape torch.Size([256, 128, 3, 3, 3])
decoder.encoder.stages.3.0.convs.0.all_modules.0.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.1.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.1.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.encoder.stages.3.0.convs.1.conv.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.norm.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.norm.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.encoder.stages.3.0.convs.1.all_modules.0.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.1.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.1.bias shape torch.Size([256])
decoder.encoder.stages.4.0.convs.0.conv.weight shape torch.Size([320, 256, 3, 3, 3])
decoder.encoder.stages.4.0.convs.0.conv.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.norm.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.norm.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.0.weight shape torch.Size([320, 256, 3, 3, 3])
decoder.encoder.stages.4.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.4.0.convs.1.conv.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.norm.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.norm.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.4.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.0.conv.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.norm.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.norm.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.1.conv.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.norm.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.norm.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.stages.0.convs.0.conv.weight shape torch.Size([320, 640, 3, 3, 3])
decoder.stages.0.convs.0.conv.bias shape torch.Size([320])
decoder.stages.0.convs.0.norm.weight shape torch.Size([320])
decoder.stages.0.convs.0.norm.bias shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.0.weight shape torch.Size([320, 640, 3, 3, 3])
decoder.stages.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.stages.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.stages.0.convs.1.conv.bias shape torch.Size([320])
decoder.stages.0.convs.1.norm.weight shape torch.Size([320])
decoder.stages.0.convs.1.norm.bias shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.stages.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.stages.1.convs.0.conv.weight shape torch.Size([256, 512, 3, 3, 3])
decoder.stages.1.convs.0.conv.bias shape torch.Size([256])
decoder.stages.1.convs.0.norm.weight shape torch.Size([256])
decoder.stages.1.convs.0.norm.bias shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.0.weight shape torch.Size([256, 512, 3, 3, 3])
decoder.stages.1.convs.0.all_modules.0.bias shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.1.weight shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.1.bias shape torch.Size([256])
decoder.stages.1.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.stages.1.convs.1.conv.bias shape torch.Size([256])
decoder.stages.1.convs.1.norm.weight shape torch.Size([256])
decoder.stages.1.convs.1.norm.bias shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.stages.1.convs.1.all_modules.0.bias shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.1.weight shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.1.bias shape torch.Size([256])
decoder.stages.2.convs.0.conv.weight shape torch.Size([128, 256, 3, 3, 3])
decoder.stages.2.convs.0.conv.bias shape torch.Size([128])
decoder.stages.2.convs.0.norm.weight shape torch.Size([128])
decoder.stages.2.convs.0.norm.bias shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.0.weight shape torch.Size([128, 256, 3, 3, 3])
decoder.stages.2.convs.0.all_modules.0.bias shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.1.weight shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.1.bias shape torch.Size([128])
decoder.stages.2.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.stages.2.convs.1.conv.bias shape torch.Size([128])
decoder.stages.2.convs.1.norm.weight shape torch.Size([128])
decoder.stages.2.convs.1.norm.bias shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.stages.2.convs.1.all_modules.0.bias shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.1.weight shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.1.bias shape torch.Size([128])
decoder.stages.3.convs.0.conv.weight shape torch.Size([64, 128, 3, 3, 3])
decoder.stages.3.convs.0.conv.bias shape torch.Size([64])
decoder.stages.3.convs.0.norm.weight shape torch.Size([64])
decoder.stages.3.convs.0.norm.bias shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.0.weight shape torch.Size([64, 128, 3, 3, 3])
decoder.stages.3.convs.0.all_modules.0.bias shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.1.weight shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.1.bias shape torch.Size([64])
decoder.stages.3.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.stages.3.convs.1.conv.bias shape torch.Size([64])
decoder.stages.3.convs.1.norm.weight shape torch.Size([64])
decoder.stages.3.convs.1.norm.bias shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.stages.3.convs.1.all_modules.0.bias shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.1.weight shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.1.bias shape torch.Size([64])
decoder.stages.4.convs.0.conv.weight shape torch.Size([32, 64, 3, 3, 3])
decoder.stages.4.convs.0.conv.bias shape torch.Size([32])
decoder.stages.4.convs.0.norm.weight shape torch.Size([32])
decoder.stages.4.convs.0.norm.bias shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.0.weight shape torch.Size([32, 64, 3, 3, 3])
decoder.stages.4.convs.0.all_modules.0.bias shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.1.weight shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.1.bias shape torch.Size([32])
decoder.stages.4.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.stages.4.convs.1.conv.bias shape torch.Size([32])
decoder.stages.4.convs.1.norm.weight shape torch.Size([32])
decoder.stages.4.convs.1.norm.bias shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.stages.4.convs.1.all_modules.0.bias shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.1.weight shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.1.bias shape torch.Size([32])
decoder.transpconvs.0.weight shape torch.Size([320, 320, 1, 2, 2])
decoder.transpconvs.0.bias shape torch.Size([320])
decoder.transpconvs.1.weight shape torch.Size([320, 256, 2, 2, 2])
decoder.transpconvs.1.bias shape torch.Size([256])
decoder.transpconvs.2.weight shape torch.Size([256, 128, 2, 2, 2])
decoder.transpconvs.2.bias shape torch.Size([128])
decoder.transpconvs.3.weight shape torch.Size([128, 64, 2, 2, 2])
decoder.transpconvs.3.bias shape torch.Size([64])
decoder.transpconvs.4.weight shape torch.Size([64, 32, 2, 2, 2])
decoder.transpconvs.4.bias shape torch.Size([32])
################### Done ###################
2024-12-17 09:27:38.284170: do_dummy_2d_data_aug: True
2024-12-17 09:27:38.286794: Using splits from existing split file: /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset309_Sohee_Calcium_OCT_CrossValidation/splits_final.json
2024-12-17 09:27:38.288875: The split file contains 5 splits.
2024-12-17 09:27:38.289925: Desired fold for training: 4
2024-12-17 09:27:38.290687: This split has 7 training and 1 validation cases.
using pin_memory on device 0
using pin_memory on device 0

This is the configuration used by this training:
Configuration name: 3d_32x160x128_b10
 {'data_identifier': 'nnUNetPlans_3d_fullres', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 10, 'patch_size': [32, 160, 128], 'median_image_size_in_voxels': [375.0, 498.0, 498.0], 'spacing': [1.0, 1.0, 1.0], 'normalization_schemes': ['ZScoreNormalization'], 'use_mask_for_norm': [False], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'architecture': {'network_class_name': 'dynamic_network_architectures.architectures.unet.PlainConvUNet', 'arch_kwargs': {'n_stages': 6, 'features_per_stage': [32, 64, 128, 256, 320, 320], 'conv_op': 'torch.nn.modules.conv.Conv3d', 'kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]], 'strides': [[1, 1, 1], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [1, 2, 2]], 'n_conv_per_stage': [2, 2, 2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2, 2, 2], 'conv_bias': True, 'norm_op': 'torch.nn.modules.instancenorm.InstanceNorm3d', 'norm_op_kwargs': {'eps': 1e-05, 'affine': True}, 'dropout_op': None, 'dropout_op_kwargs': None, 'nonlin': 'torch.nn.LeakyReLU', 'nonlin_kwargs': {'inplace': True}, 'deep_supervision': True}, '_kw_requires_import': ['conv_op', 'norm_op', 'dropout_op', 'nonlin']}, 'batch_dice': True, 'inherits_from': '3d_fullres'} 

These are the global plan.json settings:
 {'dataset_name': 'Dataset309_Sohee_Calcium_OCT_CrossValidation', 'plans_name': 'nnUNetPlans', 'original_median_spacing_after_transp': [1.0, 1.0, 1.0], 'original_median_shape_after_transp': [375, 498, 498], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 0.4977976381778717, 'mean': 0.13507525622844696, 'median': 0.09599608182907104, 'min': 0.0, 'percentile_00_5': 0.014754901640117168, 'percentile_99_5': 0.4977976381778717, 'std': 0.12025152146816254}}} 

2024-12-17 09:27:40.507459: unpacking dataset...
2024-12-17 09:27:44.330465: unpacking done...
2024-12-17 09:27:44.632824: Unable to plot network architecture: nnUNet_compile is enabled!
2024-12-17 09:27:44.702000: 
2024-12-17 09:27:44.703957: Epoch 0
2024-12-17 09:27:44.705285: Current learning rate: 0.01
2024-12-17 09:29:57.117925: Validation loss improved from 1000.00000 to -0.32848! Patience: 0/50
2024-12-17 09:29:57.119107: train_loss -0.3139
2024-12-17 09:29:57.119871: val_loss -0.3285
2024-12-17 09:29:57.120505: Pseudo dice [0.6439]
2024-12-17 09:29:57.121328: Epoch time: 132.42 s
2024-12-17 09:29:57.122044: Yayy! New best EMA pseudo Dice: 0.6439
2024-12-17 09:29:58.612117: 
2024-12-17 09:29:58.614314: Epoch 1
2024-12-17 09:29:58.615709: Current learning rate: 0.00994
2024-12-17 09:31:24.661724: Validation loss improved from -0.32848 to -0.40114! Patience: 0/50
2024-12-17 09:31:24.662733: train_loss -0.4743
2024-12-17 09:31:24.663576: val_loss -0.4011
2024-12-17 09:31:24.664371: Pseudo dice [0.6663]
2024-12-17 09:31:24.665287: Epoch time: 86.05 s
2024-12-17 09:31:24.665934: Yayy! New best EMA pseudo Dice: 0.6462
2024-12-17 09:31:26.169002: 
2024-12-17 09:31:26.170917: Epoch 2
2024-12-17 09:31:26.171975: Current learning rate: 0.00988
2024-12-17 09:32:52.227224: Validation loss improved from -0.40114 to -0.41186! Patience: 0/50
2024-12-17 09:32:52.228745: train_loss -0.5212
2024-12-17 09:32:52.230114: val_loss -0.4119
2024-12-17 09:32:52.230791: Pseudo dice [0.673]
2024-12-17 09:32:52.231643: Epoch time: 86.06 s
2024-12-17 09:32:52.232354: Yayy! New best EMA pseudo Dice: 0.6488
2024-12-17 09:32:53.785697: 
2024-12-17 09:32:53.787948: Epoch 3
2024-12-17 09:32:53.788697: Current learning rate: 0.00982
2024-12-17 09:34:19.864828: Validation loss improved from -0.41186 to -0.47017! Patience: 0/50
2024-12-17 09:34:19.865706: train_loss -0.5231
2024-12-17 09:34:19.866654: val_loss -0.4702
2024-12-17 09:34:19.867604: Pseudo dice [0.7036]
2024-12-17 09:34:19.868534: Epoch time: 86.08 s
2024-12-17 09:34:19.869508: Yayy! New best EMA pseudo Dice: 0.6543
2024-12-17 09:34:21.409594: 
2024-12-17 09:34:21.411932: Epoch 4
2024-12-17 09:34:21.413374: Current learning rate: 0.00976
2024-12-17 09:35:47.461295: Validation loss did not improve from -0.47017. Patience: 1/50
2024-12-17 09:35:47.462471: train_loss -0.5488
2024-12-17 09:35:47.463672: val_loss -0.4315
2024-12-17 09:35:47.464813: Pseudo dice [0.695]
2024-12-17 09:35:47.465766: Epoch time: 86.05 s
2024-12-17 09:35:47.828088: Yayy! New best EMA pseudo Dice: 0.6584
2024-12-17 09:35:49.396911: 
2024-12-17 09:35:49.399072: Epoch 5
2024-12-17 09:35:49.400203: Current learning rate: 0.0097
2024-12-17 09:37:15.501883: Validation loss did not improve from -0.47017. Patience: 2/50
2024-12-17 09:37:15.503110: train_loss -0.5536
2024-12-17 09:37:15.504740: val_loss -0.4334
2024-12-17 09:37:15.505810: Pseudo dice [0.6861]
2024-12-17 09:37:15.507196: Epoch time: 86.11 s
2024-12-17 09:37:15.508159: Yayy! New best EMA pseudo Dice: 0.6612
2024-12-17 09:37:16.953069: 
2024-12-17 09:37:16.955084: Epoch 6
2024-12-17 09:37:16.956649: Current learning rate: 0.00964
2024-12-17 09:38:43.026618: Validation loss did not improve from -0.47017. Patience: 3/50
2024-12-17 09:38:43.027674: train_loss -0.5645
2024-12-17 09:38:43.028873: val_loss -0.3869
2024-12-17 09:38:43.029767: Pseudo dice [0.677]
2024-12-17 09:38:43.030714: Epoch time: 86.08 s
2024-12-17 09:38:43.031547: Yayy! New best EMA pseudo Dice: 0.6627
2024-12-17 09:38:44.549363: 
2024-12-17 09:38:44.551732: Epoch 7
2024-12-17 09:38:44.552521: Current learning rate: 0.00958
2024-12-17 09:40:10.602424: Validation loss did not improve from -0.47017. Patience: 4/50
2024-12-17 09:40:10.603793: train_loss -0.5686
2024-12-17 09:40:10.605090: val_loss -0.4497
2024-12-17 09:40:10.605935: Pseudo dice [0.683]
2024-12-17 09:40:10.606658: Epoch time: 86.06 s
2024-12-17 09:40:10.607517: Yayy! New best EMA pseudo Dice: 0.6648
2024-12-17 09:40:12.511091: 
2024-12-17 09:40:12.513332: Epoch 8
2024-12-17 09:40:12.514420: Current learning rate: 0.00952
2024-12-17 09:41:38.618678: Validation loss did not improve from -0.47017. Patience: 5/50
2024-12-17 09:41:38.620043: train_loss -0.5776
2024-12-17 09:41:38.621580: val_loss -0.4133
2024-12-17 09:41:38.622401: Pseudo dice [0.6808]
2024-12-17 09:41:38.623271: Epoch time: 86.11 s
2024-12-17 09:41:38.624297: Yayy! New best EMA pseudo Dice: 0.6664
2024-12-17 09:41:40.196845: 
2024-12-17 09:41:40.198420: Epoch 9
2024-12-17 09:41:40.199809: Current learning rate: 0.00946
2024-12-17 09:43:06.456338: Validation loss improved from -0.47017 to -0.48689! Patience: 5/50
2024-12-17 09:43:06.457819: train_loss -0.5892
2024-12-17 09:43:06.459080: val_loss -0.4869
2024-12-17 09:43:06.459741: Pseudo dice [0.7096]
2024-12-17 09:43:06.460528: Epoch time: 86.26 s
2024-12-17 09:43:06.816871: Yayy! New best EMA pseudo Dice: 0.6707
2024-12-17 09:43:08.256747: 
2024-12-17 09:43:08.258551: Epoch 10
2024-12-17 09:43:08.260007: Current learning rate: 0.0094
2024-12-17 09:44:34.383577: Validation loss did not improve from -0.48689. Patience: 1/50
2024-12-17 09:44:34.384844: train_loss -0.6056
2024-12-17 09:44:34.385744: val_loss -0.4689
2024-12-17 09:44:34.386526: Pseudo dice [0.7025]
2024-12-17 09:44:34.387362: Epoch time: 86.13 s
2024-12-17 09:44:34.388086: Yayy! New best EMA pseudo Dice: 0.6739
2024-12-17 09:44:35.872930: 
2024-12-17 09:44:35.875063: Epoch 11
2024-12-17 09:44:35.876327: Current learning rate: 0.00934
2024-12-17 09:46:01.973016: Validation loss improved from -0.48689 to -0.49329! Patience: 1/50
2024-12-17 09:46:01.974200: train_loss -0.6106
2024-12-17 09:46:01.975148: val_loss -0.4933
2024-12-17 09:46:01.976066: Pseudo dice [0.7181]
2024-12-17 09:46:01.976952: Epoch time: 86.1 s
2024-12-17 09:46:01.977939: Yayy! New best EMA pseudo Dice: 0.6783
2024-12-17 09:46:03.438658: 
2024-12-17 09:46:03.440459: Epoch 12
2024-12-17 09:46:03.441604: Current learning rate: 0.00928
2024-12-17 09:47:29.602865: Validation loss did not improve from -0.49329. Patience: 1/50
2024-12-17 09:47:29.604227: train_loss -0.6084
2024-12-17 09:47:29.605163: val_loss -0.4797
2024-12-17 09:47:29.606028: Pseudo dice [0.7018]
2024-12-17 09:47:29.606668: Epoch time: 86.17 s
2024-12-17 09:47:29.607331: Yayy! New best EMA pseudo Dice: 0.6807
2024-12-17 09:47:31.111373: 
2024-12-17 09:47:31.113254: Epoch 13
2024-12-17 09:47:31.114449: Current learning rate: 0.00922
2024-12-17 09:48:57.338171: Validation loss improved from -0.49329 to -0.49592! Patience: 1/50
2024-12-17 09:48:57.339589: train_loss -0.6118
2024-12-17 09:48:57.340640: val_loss -0.4959
2024-12-17 09:48:57.341576: Pseudo dice [0.7199]
2024-12-17 09:48:57.342413: Epoch time: 86.23 s
2024-12-17 09:48:57.343308: Yayy! New best EMA pseudo Dice: 0.6846
2024-12-17 09:48:58.839401: 
2024-12-17 09:48:58.841962: Epoch 14
2024-12-17 09:48:58.843366: Current learning rate: 0.00916
2024-12-17 09:50:25.033801: Validation loss did not improve from -0.49592. Patience: 1/50
2024-12-17 09:50:25.035031: train_loss -0.6252
2024-12-17 09:50:25.036252: val_loss -0.4912
2024-12-17 09:50:25.037549: Pseudo dice [0.721]
2024-12-17 09:50:25.038347: Epoch time: 86.2 s
2024-12-17 09:50:25.387871: Yayy! New best EMA pseudo Dice: 0.6882
2024-12-17 09:50:26.873041: 
2024-12-17 09:50:26.875045: Epoch 15
2024-12-17 09:50:26.876570: Current learning rate: 0.0091
2024-12-17 09:51:53.115483: Validation loss did not improve from -0.49592. Patience: 2/50
2024-12-17 09:51:53.116795: train_loss -0.6172
2024-12-17 09:51:53.118179: val_loss -0.4885
2024-12-17 09:51:53.119290: Pseudo dice [0.7136]
2024-12-17 09:51:53.120376: Epoch time: 86.24 s
2024-12-17 09:51:53.121350: Yayy! New best EMA pseudo Dice: 0.6908
2024-12-17 09:51:54.606085: 
2024-12-17 09:51:54.608414: Epoch 16
2024-12-17 09:51:54.609276: Current learning rate: 0.00903
2024-12-17 09:53:21.122581: Validation loss did not improve from -0.49592. Patience: 3/50
2024-12-17 09:53:21.123854: train_loss -0.6199
2024-12-17 09:53:21.124921: val_loss -0.4917
2024-12-17 09:53:21.125571: Pseudo dice [0.7189]
2024-12-17 09:53:21.126202: Epoch time: 86.52 s
2024-12-17 09:53:21.126806: Yayy! New best EMA pseudo Dice: 0.6936
2024-12-17 09:53:22.679032: 
2024-12-17 09:53:22.681051: Epoch 17
2024-12-17 09:53:22.682355: Current learning rate: 0.00897
2024-12-17 09:54:49.336370: Validation loss improved from -0.49592 to -0.49692! Patience: 3/50
2024-12-17 09:54:49.337828: train_loss -0.6345
2024-12-17 09:54:49.338896: val_loss -0.4969
2024-12-17 09:54:49.339921: Pseudo dice [0.7182]
2024-12-17 09:54:49.340824: Epoch time: 86.66 s
2024-12-17 09:54:49.341594: Yayy! New best EMA pseudo Dice: 0.696
2024-12-17 09:54:50.870657: 
2024-12-17 09:54:50.873771: Epoch 18
2024-12-17 09:54:50.875184: Current learning rate: 0.00891
2024-12-17 09:56:17.524201: Validation loss improved from -0.49692 to -0.50531! Patience: 0/50
2024-12-17 09:56:17.525305: train_loss -0.6303
2024-12-17 09:56:17.526286: val_loss -0.5053
2024-12-17 09:56:17.526942: Pseudo dice [0.7384]
2024-12-17 09:56:17.527729: Epoch time: 86.66 s
2024-12-17 09:56:17.528913: Yayy! New best EMA pseudo Dice: 0.7003
2024-12-17 09:56:19.382231: 
2024-12-17 09:56:19.384704: Epoch 19
2024-12-17 09:56:19.385666: Current learning rate: 0.00885
2024-12-17 09:57:46.048981: Validation loss did not improve from -0.50531. Patience: 1/50
2024-12-17 09:57:46.050053: train_loss -0.6379
2024-12-17 09:57:46.051345: val_loss -0.4648
2024-12-17 09:57:46.052321: Pseudo dice [0.6848]
2024-12-17 09:57:46.053090: Epoch time: 86.67 s
2024-12-17 09:57:47.627507: 
2024-12-17 09:57:47.629362: Epoch 20
2024-12-17 09:57:47.630647: Current learning rate: 0.00879
2024-12-17 09:59:14.271424: Validation loss did not improve from -0.50531. Patience: 2/50
2024-12-17 09:59:14.272816: train_loss -0.6329
2024-12-17 09:59:14.274018: val_loss -0.4835
2024-12-17 09:59:14.274733: Pseudo dice [0.7057]
2024-12-17 09:59:14.275576: Epoch time: 86.65 s
2024-12-17 09:59:15.504608: 
2024-12-17 09:59:15.506525: Epoch 21
2024-12-17 09:59:15.507672: Current learning rate: 0.00873
2024-12-17 10:00:42.086575: Validation loss did not improve from -0.50531. Patience: 3/50
2024-12-17 10:00:42.087906: train_loss -0.649
2024-12-17 10:00:42.088970: val_loss -0.4907
2024-12-17 10:00:42.089949: Pseudo dice [0.7147]
2024-12-17 10:00:42.090858: Epoch time: 86.58 s
2024-12-17 10:00:42.091598: Yayy! New best EMA pseudo Dice: 0.7009
2024-12-17 10:00:43.599005: 
2024-12-17 10:00:43.601684: Epoch 22
2024-12-17 10:00:43.602569: Current learning rate: 0.00867
2024-12-17 10:02:10.158952: Validation loss did not improve from -0.50531. Patience: 4/50
2024-12-17 10:02:10.160223: train_loss -0.6539
2024-12-17 10:02:10.161234: val_loss -0.467
2024-12-17 10:02:10.162180: Pseudo dice [0.6883]
2024-12-17 10:02:10.163168: Epoch time: 86.56 s
2024-12-17 10:02:11.280244: 
2024-12-17 10:02:11.282463: Epoch 23
2024-12-17 10:02:11.283535: Current learning rate: 0.00861
2024-12-17 10:03:37.986613: Validation loss improved from -0.50531 to -0.51966! Patience: 4/50
2024-12-17 10:03:37.987912: train_loss -0.6545
2024-12-17 10:03:37.989077: val_loss -0.5197
2024-12-17 10:03:37.990411: Pseudo dice [0.7302]
2024-12-17 10:03:37.991742: Epoch time: 86.71 s
2024-12-17 10:03:37.992906: Yayy! New best EMA pseudo Dice: 0.7027
2024-12-17 10:03:39.508742: 
2024-12-17 10:03:39.510833: Epoch 24
2024-12-17 10:03:39.512102: Current learning rate: 0.00855
2024-12-17 10:05:06.622038: Validation loss did not improve from -0.51966. Patience: 1/50
2024-12-17 10:05:06.623332: train_loss -0.6616
2024-12-17 10:05:06.624296: val_loss -0.4667
2024-12-17 10:05:06.625054: Pseudo dice [0.7053]
2024-12-17 10:05:06.625730: Epoch time: 87.12 s
2024-12-17 10:05:06.987465: Yayy! New best EMA pseudo Dice: 0.703
2024-12-17 10:05:08.488662: 
2024-12-17 10:05:08.490874: Epoch 25
2024-12-17 10:05:08.491997: Current learning rate: 0.00849
2024-12-17 10:06:35.479537: Validation loss did not improve from -0.51966. Patience: 2/50
2024-12-17 10:06:35.480805: train_loss -0.6671
2024-12-17 10:06:35.481871: val_loss -0.4674
2024-12-17 10:06:35.482632: Pseudo dice [0.7097]
2024-12-17 10:06:35.483349: Epoch time: 86.99 s
2024-12-17 10:06:35.483985: Yayy! New best EMA pseudo Dice: 0.7037
2024-12-17 10:06:36.980923: 
2024-12-17 10:06:36.983724: Epoch 26
2024-12-17 10:06:36.985060: Current learning rate: 0.00843
2024-12-17 10:08:04.036763: Validation loss did not improve from -0.51966. Patience: 3/50
2024-12-17 10:08:04.037659: train_loss -0.6569
2024-12-17 10:08:04.038641: val_loss -0.4952
2024-12-17 10:08:04.039518: Pseudo dice [0.714]
2024-12-17 10:08:04.040210: Epoch time: 87.06 s
2024-12-17 10:08:04.041097: Yayy! New best EMA pseudo Dice: 0.7047
2024-12-17 10:08:05.545370: 
2024-12-17 10:08:05.547562: Epoch 27
2024-12-17 10:08:05.548700: Current learning rate: 0.00836
2024-12-17 10:09:32.518330: Validation loss did not improve from -0.51966. Patience: 4/50
2024-12-17 10:09:32.519547: train_loss -0.6673
2024-12-17 10:09:32.520557: val_loss -0.5157
2024-12-17 10:09:32.521446: Pseudo dice [0.7199]
2024-12-17 10:09:32.522114: Epoch time: 86.98 s
2024-12-17 10:09:32.522767: Yayy! New best EMA pseudo Dice: 0.7062
2024-12-17 10:09:34.025617: 
2024-12-17 10:09:34.028103: Epoch 28
2024-12-17 10:09:34.028830: Current learning rate: 0.0083
2024-12-17 10:11:00.855999: Validation loss did not improve from -0.51966. Patience: 5/50
2024-12-17 10:11:00.857419: train_loss -0.6617
2024-12-17 10:11:00.858949: val_loss -0.4676
2024-12-17 10:11:00.860180: Pseudo dice [0.7114]
2024-12-17 10:11:00.861008: Epoch time: 86.83 s
2024-12-17 10:11:00.861747: Yayy! New best EMA pseudo Dice: 0.7067
2024-12-17 10:11:02.712818: 
2024-12-17 10:11:02.715060: Epoch 29
2024-12-17 10:11:02.716144: Current learning rate: 0.00824
2024-12-17 10:12:29.724447: Validation loss did not improve from -0.51966. Patience: 6/50
2024-12-17 10:12:29.725518: train_loss -0.6592
2024-12-17 10:12:29.726741: val_loss -0.4781
2024-12-17 10:12:29.727879: Pseudo dice [0.7185]
2024-12-17 10:12:29.728877: Epoch time: 87.01 s
2024-12-17 10:12:30.084910: Yayy! New best EMA pseudo Dice: 0.7079
2024-12-17 10:12:31.607479: 
2024-12-17 10:12:31.609011: Epoch 30
2024-12-17 10:12:31.610090: Current learning rate: 0.00818
2024-12-17 10:13:58.609453: Validation loss did not improve from -0.51966. Patience: 7/50
2024-12-17 10:13:58.610770: train_loss -0.6618
2024-12-17 10:13:58.611889: val_loss -0.4825
2024-12-17 10:13:58.612973: Pseudo dice [0.7145]
2024-12-17 10:13:58.614043: Epoch time: 87.0 s
2024-12-17 10:13:58.615067: Yayy! New best EMA pseudo Dice: 0.7086
2024-12-17 10:14:00.086764: 
2024-12-17 10:14:00.088724: Epoch 31
2024-12-17 10:14:00.089680: Current learning rate: 0.00812
2024-12-17 10:15:27.069117: Validation loss did not improve from -0.51966. Patience: 8/50
2024-12-17 10:15:27.070439: train_loss -0.6691
2024-12-17 10:15:27.071455: val_loss -0.5079
2024-12-17 10:15:27.072142: Pseudo dice [0.7216]
2024-12-17 10:15:27.072813: Epoch time: 86.98 s
2024-12-17 10:15:27.073598: Yayy! New best EMA pseudo Dice: 0.7099
2024-12-17 10:15:28.602100: 
2024-12-17 10:15:28.604484: Epoch 32
2024-12-17 10:15:28.605772: Current learning rate: 0.00806
2024-12-17 10:16:55.965887: Validation loss did not improve from -0.51966. Patience: 9/50
2024-12-17 10:16:55.967212: train_loss -0.6756
2024-12-17 10:16:55.968693: val_loss -0.499
2024-12-17 10:16:55.969622: Pseudo dice [0.72]
2024-12-17 10:16:55.970365: Epoch time: 87.37 s
2024-12-17 10:16:55.971482: Yayy! New best EMA pseudo Dice: 0.7109
2024-12-17 10:16:57.512461: 
2024-12-17 10:16:57.514647: Epoch 33
2024-12-17 10:16:57.515674: Current learning rate: 0.008
2024-12-17 10:18:24.715737: Validation loss did not improve from -0.51966. Patience: 10/50
2024-12-17 10:18:24.716926: train_loss -0.6735
2024-12-17 10:18:24.717969: val_loss -0.4805
2024-12-17 10:18:24.718863: Pseudo dice [0.6993]
2024-12-17 10:18:24.719796: Epoch time: 87.21 s
2024-12-17 10:18:25.891549: 
2024-12-17 10:18:25.893644: Epoch 34
2024-12-17 10:18:25.894778: Current learning rate: 0.00793
2024-12-17 10:19:52.824568: Validation loss improved from -0.51966 to -0.54227! Patience: 10/50
2024-12-17 10:19:52.826078: train_loss -0.6785
2024-12-17 10:19:52.827290: val_loss -0.5423
2024-12-17 10:19:52.828003: Pseudo dice [0.7365]
2024-12-17 10:19:52.828758: Epoch time: 86.94 s
2024-12-17 10:19:53.186684: Yayy! New best EMA pseudo Dice: 0.7124
2024-12-17 10:19:54.738153: 
2024-12-17 10:19:54.740466: Epoch 35
2024-12-17 10:19:54.742013: Current learning rate: 0.00787
2024-12-17 10:21:21.338005: Validation loss did not improve from -0.54227. Patience: 1/50
2024-12-17 10:21:21.339212: train_loss -0.6813
2024-12-17 10:21:21.340356: val_loss -0.5056
2024-12-17 10:21:21.341195: Pseudo dice [0.7199]
2024-12-17 10:21:21.342024: Epoch time: 86.6 s
2024-12-17 10:21:21.343199: Yayy! New best EMA pseudo Dice: 0.7131
2024-12-17 10:21:22.881708: 
2024-12-17 10:21:22.883436: Epoch 36
2024-12-17 10:21:22.884432: Current learning rate: 0.00781
2024-12-17 10:22:49.554305: Validation loss did not improve from -0.54227. Patience: 2/50
2024-12-17 10:22:49.555526: train_loss -0.6899
2024-12-17 10:22:49.556725: val_loss -0.4862
2024-12-17 10:22:49.557650: Pseudo dice [0.7107]
2024-12-17 10:22:49.558575: Epoch time: 86.67 s
2024-12-17 10:22:50.760507: 
2024-12-17 10:22:50.762450: Epoch 37
2024-12-17 10:22:50.763690: Current learning rate: 0.00775
2024-12-17 10:24:17.424109: Validation loss did not improve from -0.54227. Patience: 3/50
2024-12-17 10:24:17.425628: train_loss -0.6824
2024-12-17 10:24:17.427063: val_loss -0.5081
2024-12-17 10:24:17.427985: Pseudo dice [0.7238]
2024-12-17 10:24:17.428945: Epoch time: 86.67 s
2024-12-17 10:24:17.429602: Yayy! New best EMA pseudo Dice: 0.714
2024-12-17 10:24:18.956252: 
2024-12-17 10:24:18.958068: Epoch 38
2024-12-17 10:24:18.958954: Current learning rate: 0.00769
2024-12-17 10:25:45.650441: Validation loss did not improve from -0.54227. Patience: 4/50
2024-12-17 10:25:45.651751: train_loss -0.6898
2024-12-17 10:25:45.652836: val_loss -0.4573
2024-12-17 10:25:45.653565: Pseudo dice [0.7135]
2024-12-17 10:25:45.654283: Epoch time: 86.7 s
2024-12-17 10:25:47.160958: 
2024-12-17 10:25:47.163157: Epoch 39
2024-12-17 10:25:47.164337: Current learning rate: 0.00763
2024-12-17 10:27:14.377003: Validation loss did not improve from -0.54227. Patience: 5/50
2024-12-17 10:27:14.378254: train_loss -0.674
2024-12-17 10:27:14.379419: val_loss -0.4846
2024-12-17 10:27:14.380468: Pseudo dice [0.7041]
2024-12-17 10:27:14.381176: Epoch time: 87.22 s
2024-12-17 10:27:15.940128: 
2024-12-17 10:27:15.942338: Epoch 40
2024-12-17 10:27:15.943535: Current learning rate: 0.00756
2024-12-17 10:28:42.427358: Validation loss did not improve from -0.54227. Patience: 6/50
2024-12-17 10:28:42.428274: train_loss -0.6875
2024-12-17 10:28:42.429498: val_loss -0.5108
2024-12-17 10:28:42.430581: Pseudo dice [0.7308]
2024-12-17 10:28:42.431796: Epoch time: 86.49 s
2024-12-17 10:28:42.432583: Yayy! New best EMA pseudo Dice: 0.7147
2024-12-17 10:28:44.011013: 
2024-12-17 10:28:44.013648: Epoch 41
2024-12-17 10:28:44.014732: Current learning rate: 0.0075
2024-12-17 10:30:10.602031: Validation loss did not improve from -0.54227. Patience: 7/50
2024-12-17 10:30:10.603394: train_loss -0.6872
2024-12-17 10:30:10.604762: val_loss -0.4728
2024-12-17 10:30:10.605709: Pseudo dice [0.7138]
2024-12-17 10:30:10.606668: Epoch time: 86.59 s
2024-12-17 10:30:11.740788: 
2024-12-17 10:30:11.742701: Epoch 42
2024-12-17 10:30:11.743921: Current learning rate: 0.00744
2024-12-17 10:31:38.552119: Validation loss did not improve from -0.54227. Patience: 8/50
2024-12-17 10:31:38.553432: train_loss -0.6911
2024-12-17 10:31:38.554744: val_loss -0.4919
2024-12-17 10:31:38.555691: Pseudo dice [0.7205]
2024-12-17 10:31:38.556456: Epoch time: 86.81 s
2024-12-17 10:31:38.557215: Yayy! New best EMA pseudo Dice: 0.7152
2024-12-17 10:31:40.058515: 
2024-12-17 10:31:40.060694: Epoch 43
2024-12-17 10:31:40.061919: Current learning rate: 0.00738
2024-12-17 10:33:07.209226: Validation loss did not improve from -0.54227. Patience: 9/50
2024-12-17 10:33:07.212564: train_loss -0.6941
2024-12-17 10:33:07.214031: val_loss -0.4847
2024-12-17 10:33:07.215047: Pseudo dice [0.7204]
2024-12-17 10:33:07.216039: Epoch time: 87.15 s
2024-12-17 10:33:07.216826: Yayy! New best EMA pseudo Dice: 0.7157
2024-12-17 10:33:08.853122: 
2024-12-17 10:33:08.855246: Epoch 44
2024-12-17 10:33:08.856583: Current learning rate: 0.00732
2024-12-17 10:34:35.611512: Validation loss did not improve from -0.54227. Patience: 10/50
2024-12-17 10:34:35.612434: train_loss -0.6981
2024-12-17 10:34:35.615217: val_loss -0.4808
2024-12-17 10:34:35.616422: Pseudo dice [0.7069]
2024-12-17 10:34:35.617460: Epoch time: 86.76 s
2024-12-17 10:34:37.160157: 
2024-12-17 10:34:37.161974: Epoch 45
2024-12-17 10:34:37.163398: Current learning rate: 0.00725
2024-12-17 10:36:03.909811: Validation loss did not improve from -0.54227. Patience: 11/50
2024-12-17 10:36:03.911078: train_loss -0.6969
2024-12-17 10:36:03.912215: val_loss -0.5221
2024-12-17 10:36:03.913082: Pseudo dice [0.7326]
2024-12-17 10:36:03.914076: Epoch time: 86.75 s
2024-12-17 10:36:03.914926: Yayy! New best EMA pseudo Dice: 0.7166
2024-12-17 10:36:05.438561: 
2024-12-17 10:36:05.440860: Epoch 46
2024-12-17 10:36:05.442410: Current learning rate: 0.00719
2024-12-17 10:37:32.026556: Validation loss did not improve from -0.54227. Patience: 12/50
2024-12-17 10:37:32.027998: train_loss -0.7067
2024-12-17 10:37:32.028959: val_loss -0.5075
2024-12-17 10:37:32.029736: Pseudo dice [0.7127]
2024-12-17 10:37:32.030417: Epoch time: 86.59 s
2024-12-17 10:37:33.171641: 
2024-12-17 10:37:33.174142: Epoch 47
2024-12-17 10:37:33.175067: Current learning rate: 0.00713
2024-12-17 10:38:59.809453: Validation loss did not improve from -0.54227. Patience: 13/50
2024-12-17 10:38:59.810472: train_loss -0.7068
2024-12-17 10:38:59.811427: val_loss -0.4997
2024-12-17 10:38:59.812299: Pseudo dice [0.7213]
2024-12-17 10:38:59.813035: Epoch time: 86.64 s
2024-12-17 10:38:59.813694: Yayy! New best EMA pseudo Dice: 0.7167
2024-12-17 10:39:01.308208: 
2024-12-17 10:39:01.311110: Epoch 48
2024-12-17 10:39:01.312957: Current learning rate: 0.00707
2024-12-17 10:40:27.908569: Validation loss did not improve from -0.54227. Patience: 14/50
2024-12-17 10:40:27.909916: train_loss -0.7041
2024-12-17 10:40:27.911230: val_loss -0.5043
2024-12-17 10:40:27.912049: Pseudo dice [0.7208]
2024-12-17 10:40:27.912812: Epoch time: 86.6 s
2024-12-17 10:40:27.913579: Yayy! New best EMA pseudo Dice: 0.7171
2024-12-17 10:40:29.476192: 
2024-12-17 10:40:29.478109: Epoch 49
2024-12-17 10:40:29.479217: Current learning rate: 0.007
2024-12-17 10:41:56.213886: Validation loss did not improve from -0.54227. Patience: 15/50
2024-12-17 10:41:56.215063: train_loss -0.703
2024-12-17 10:41:56.216069: val_loss -0.504
2024-12-17 10:41:56.217055: Pseudo dice [0.7278]
2024-12-17 10:41:56.217790: Epoch time: 86.74 s
2024-12-17 10:41:56.578975: Yayy! New best EMA pseudo Dice: 0.7182
2024-12-17 10:41:58.967436: 
2024-12-17 10:41:58.969444: Epoch 50
2024-12-17 10:41:58.970464: Current learning rate: 0.00694
2024-12-17 10:43:25.617857: Validation loss did not improve from -0.54227. Patience: 16/50
2024-12-17 10:43:25.619143: train_loss -0.7066
2024-12-17 10:43:25.620035: val_loss -0.5093
2024-12-17 10:43:25.620964: Pseudo dice [0.7339]
2024-12-17 10:43:25.621762: Epoch time: 86.65 s
2024-12-17 10:43:25.622512: Yayy! New best EMA pseudo Dice: 0.7198
2024-12-17 10:43:27.132609: 
2024-12-17 10:43:27.134842: Epoch 51
2024-12-17 10:43:27.135929: Current learning rate: 0.00688
2024-12-17 10:44:54.012618: Validation loss did not improve from -0.54227. Patience: 17/50
2024-12-17 10:44:54.013898: train_loss -0.7096
2024-12-17 10:44:54.015199: val_loss -0.5214
2024-12-17 10:44:54.016152: Pseudo dice [0.7334]
2024-12-17 10:44:54.017072: Epoch time: 86.88 s
2024-12-17 10:44:54.017912: Yayy! New best EMA pseudo Dice: 0.7211
2024-12-17 10:44:55.550467: 
2024-12-17 10:44:55.552304: Epoch 52
2024-12-17 10:44:55.553501: Current learning rate: 0.00682
2024-12-17 10:46:22.747083: Validation loss did not improve from -0.54227. Patience: 18/50
2024-12-17 10:46:22.748274: train_loss -0.7086
2024-12-17 10:46:22.749312: val_loss -0.5198
2024-12-17 10:46:22.750261: Pseudo dice [0.7334]
2024-12-17 10:46:22.751077: Epoch time: 87.2 s
2024-12-17 10:46:22.752013: Yayy! New best EMA pseudo Dice: 0.7224
2024-12-17 10:46:24.251843: 
2024-12-17 10:46:24.253721: Epoch 53
2024-12-17 10:46:24.254915: Current learning rate: 0.00675
2024-12-17 10:47:51.404434: Validation loss did not improve from -0.54227. Patience: 19/50
2024-12-17 10:47:51.405838: train_loss -0.7125
2024-12-17 10:47:51.407105: val_loss -0.5312
2024-12-17 10:47:51.408223: Pseudo dice [0.7251]
2024-12-17 10:47:51.409280: Epoch time: 87.15 s
2024-12-17 10:47:51.410279: Yayy! New best EMA pseudo Dice: 0.7226
2024-12-17 10:47:52.916606: 
2024-12-17 10:47:52.918928: Epoch 54
2024-12-17 10:47:52.920271: Current learning rate: 0.00669
2024-12-17 10:49:20.130638: Validation loss did not improve from -0.54227. Patience: 20/50
2024-12-17 10:49:20.131836: train_loss -0.7058
2024-12-17 10:49:20.132850: val_loss -0.4371
2024-12-17 10:49:20.133520: Pseudo dice [0.6783]
2024-12-17 10:49:20.134451: Epoch time: 87.22 s
2024-12-17 10:49:21.684002: 
2024-12-17 10:49:21.685852: Epoch 55
2024-12-17 10:49:21.686742: Current learning rate: 0.00663
2024-12-17 10:50:48.934716: Validation loss did not improve from -0.54227. Patience: 21/50
2024-12-17 10:50:48.936006: train_loss -0.7041
2024-12-17 10:50:48.936994: val_loss -0.4837
2024-12-17 10:50:48.938010: Pseudo dice [0.7186]
2024-12-17 10:50:48.938783: Epoch time: 87.25 s
2024-12-17 10:50:50.123148: 
2024-12-17 10:50:50.125360: Epoch 56
2024-12-17 10:50:50.126644: Current learning rate: 0.00657
2024-12-17 10:52:17.355509: Validation loss did not improve from -0.54227. Patience: 22/50
2024-12-17 10:52:17.356776: train_loss -0.7049
2024-12-17 10:52:17.357985: val_loss -0.4985
2024-12-17 10:52:17.358963: Pseudo dice [0.7256]
2024-12-17 10:52:17.359850: Epoch time: 87.23 s
2024-12-17 10:52:18.579410: 
2024-12-17 10:52:18.581252: Epoch 57
2024-12-17 10:52:18.582348: Current learning rate: 0.0065
2024-12-17 10:53:45.693538: Validation loss did not improve from -0.54227. Patience: 23/50
2024-12-17 10:53:45.694930: train_loss -0.7104
2024-12-17 10:53:45.696237: val_loss -0.5309
2024-12-17 10:53:45.697148: Pseudo dice [0.7333]
2024-12-17 10:53:45.697953: Epoch time: 87.12 s
2024-12-17 10:53:46.878902: 
2024-12-17 10:53:46.880713: Epoch 58
2024-12-17 10:53:46.881895: Current learning rate: 0.00644
2024-12-17 10:55:14.031304: Validation loss did not improve from -0.54227. Patience: 24/50
2024-12-17 10:55:14.032292: train_loss -0.715
2024-12-17 10:55:14.033456: val_loss -0.4934
2024-12-17 10:55:14.034546: Pseudo dice [0.7149]
2024-12-17 10:55:14.035465: Epoch time: 87.15 s
2024-12-17 10:55:15.206706: 
2024-12-17 10:55:15.208384: Epoch 59
2024-12-17 10:55:15.209564: Current learning rate: 0.00638
2024-12-17 10:56:42.671024: Validation loss did not improve from -0.54227. Patience: 25/50
2024-12-17 10:56:42.672307: train_loss -0.7163
2024-12-17 10:56:42.673305: val_loss -0.5234
2024-12-17 10:56:42.674008: Pseudo dice [0.7306]
2024-12-17 10:56:42.674739: Epoch time: 87.47 s
2024-12-17 10:56:44.320714: 
2024-12-17 10:56:44.322628: Epoch 60
2024-12-17 10:56:44.323779: Current learning rate: 0.00631
2024-12-17 10:58:11.768546: Validation loss did not improve from -0.54227. Patience: 26/50
2024-12-17 10:58:11.769694: train_loss -0.7242
2024-12-17 10:58:11.770885: val_loss -0.5071
2024-12-17 10:58:11.771854: Pseudo dice [0.7198]
2024-12-17 10:58:11.772706: Epoch time: 87.45 s
2024-12-17 10:58:13.286908: 
2024-12-17 10:58:13.288549: Epoch 61
2024-12-17 10:58:13.289796: Current learning rate: 0.00625
2024-12-17 10:59:40.905571: Validation loss did not improve from -0.54227. Patience: 27/50
2024-12-17 10:59:40.906805: train_loss -0.7255
2024-12-17 10:59:40.908425: val_loss -0.4637
2024-12-17 10:59:40.909634: Pseudo dice [0.7039]
2024-12-17 10:59:40.910860: Epoch time: 87.62 s
2024-12-17 10:59:42.084825: 
2024-12-17 10:59:42.087074: Epoch 62
2024-12-17 10:59:42.088289: Current learning rate: 0.00619
2024-12-17 11:01:09.615262: Validation loss did not improve from -0.54227. Patience: 28/50
2024-12-17 11:01:09.616464: train_loss -0.7243
2024-12-17 11:01:09.617579: val_loss -0.483
2024-12-17 11:01:09.618530: Pseudo dice [0.7189]
2024-12-17 11:01:09.619489: Epoch time: 87.53 s
2024-12-17 11:01:10.922920: 
2024-12-17 11:01:10.925334: Epoch 63
2024-12-17 11:01:10.926487: Current learning rate: 0.00612
2024-12-17 11:02:38.377525: Validation loss did not improve from -0.54227. Patience: 29/50
2024-12-17 11:02:38.378595: train_loss -0.7206
2024-12-17 11:02:38.379382: val_loss -0.4894
2024-12-17 11:02:38.380069: Pseudo dice [0.716]
2024-12-17 11:02:38.380820: Epoch time: 87.46 s
2024-12-17 11:02:39.583586: 
2024-12-17 11:02:39.585728: Epoch 64
2024-12-17 11:02:39.586896: Current learning rate: 0.00606
2024-12-17 11:04:07.119725: Validation loss did not improve from -0.54227. Patience: 30/50
2024-12-17 11:04:07.121386: train_loss -0.7215
2024-12-17 11:04:07.122643: val_loss -0.4903
2024-12-17 11:04:07.123382: Pseudo dice [0.7173]
2024-12-17 11:04:07.124074: Epoch time: 87.54 s
2024-12-17 11:04:08.644354: 
2024-12-17 11:04:08.646512: Epoch 65
2024-12-17 11:04:08.647635: Current learning rate: 0.006
2024-12-17 11:05:35.871525: Validation loss did not improve from -0.54227. Patience: 31/50
2024-12-17 11:05:35.872846: train_loss -0.7242
2024-12-17 11:05:35.874020: val_loss -0.5197
2024-12-17 11:05:35.874901: Pseudo dice [0.7334]
2024-12-17 11:05:35.875831: Epoch time: 87.23 s
2024-12-17 11:05:37.071001: 
2024-12-17 11:05:37.073366: Epoch 66
2024-12-17 11:05:37.074397: Current learning rate: 0.00593
2024-12-17 11:07:04.050302: Validation loss did not improve from -0.54227. Patience: 32/50
2024-12-17 11:07:04.051496: train_loss -0.7241
2024-12-17 11:07:04.052715: val_loss -0.525
2024-12-17 11:07:04.053596: Pseudo dice [0.734]
2024-12-17 11:07:04.054449: Epoch time: 86.98 s
2024-12-17 11:07:05.269370: 
2024-12-17 11:07:05.271277: Epoch 67
2024-12-17 11:07:05.272193: Current learning rate: 0.00587
2024-12-17 11:08:32.288571: Validation loss did not improve from -0.54227. Patience: 33/50
2024-12-17 11:08:32.289958: train_loss -0.725
2024-12-17 11:08:32.291454: val_loss -0.5015
2024-12-17 11:08:32.292513: Pseudo dice [0.7209]
2024-12-17 11:08:32.293579: Epoch time: 87.02 s
2024-12-17 11:08:33.468997: 
2024-12-17 11:08:33.471457: Epoch 68
2024-12-17 11:08:33.472411: Current learning rate: 0.00581
2024-12-17 11:10:00.535305: Validation loss did not improve from -0.54227. Patience: 34/50
2024-12-17 11:10:00.536571: train_loss -0.7281
2024-12-17 11:10:00.537732: val_loss -0.506
2024-12-17 11:10:00.538801: Pseudo dice [0.7186]
2024-12-17 11:10:00.539620: Epoch time: 87.07 s
2024-12-17 11:10:01.730162: 
2024-12-17 11:10:01.732452: Epoch 69
2024-12-17 11:10:01.733941: Current learning rate: 0.00574
2024-12-17 11:11:28.726023: Validation loss did not improve from -0.54227. Patience: 35/50
2024-12-17 11:11:28.727436: train_loss -0.7252
2024-12-17 11:11:28.728541: val_loss -0.5043
2024-12-17 11:11:28.729402: Pseudo dice [0.7167]
2024-12-17 11:11:28.730449: Epoch time: 87.0 s
2024-12-17 11:11:30.265941: 
2024-12-17 11:11:30.268008: Epoch 70
2024-12-17 11:11:30.269043: Current learning rate: 0.00568
2024-12-17 11:12:57.260229: Validation loss did not improve from -0.54227. Patience: 36/50
2024-12-17 11:12:57.261263: train_loss -0.7334
2024-12-17 11:12:57.262386: val_loss -0.5071
2024-12-17 11:12:57.263148: Pseudo dice [0.7272]
2024-12-17 11:12:57.263928: Epoch time: 87.0 s
2024-12-17 11:12:58.742892: 
2024-12-17 11:12:58.744505: Epoch 71
2024-12-17 11:12:58.745313: Current learning rate: 0.00562
2024-12-17 11:14:25.936823: Validation loss did not improve from -0.54227. Patience: 37/50
2024-12-17 11:14:25.937881: train_loss -0.7321
2024-12-17 11:14:25.938573: val_loss -0.5152
2024-12-17 11:14:25.939408: Pseudo dice [0.7291]
2024-12-17 11:14:25.940050: Epoch time: 87.2 s
2024-12-17 11:14:27.165800: 
2024-12-17 11:14:27.167728: Epoch 72
2024-12-17 11:14:27.168797: Current learning rate: 0.00555
2024-12-17 11:15:54.159817: Validation loss did not improve from -0.54227. Patience: 38/50
2024-12-17 11:15:54.161492: train_loss -0.7337
2024-12-17 11:15:54.163265: val_loss -0.49
2024-12-17 11:15:54.164183: Pseudo dice [0.7162]
2024-12-17 11:15:54.165188: Epoch time: 87.0 s
2024-12-17 11:15:55.360172: 
2024-12-17 11:15:55.362064: Epoch 73
2024-12-17 11:15:55.363025: Current learning rate: 0.00549
2024-12-17 11:17:22.616604: Validation loss did not improve from -0.54227. Patience: 39/50
2024-12-17 11:17:22.617981: train_loss -0.7308
2024-12-17 11:17:22.619215: val_loss -0.535
2024-12-17 11:17:22.620561: Pseudo dice [0.7411]
2024-12-17 11:17:22.621838: Epoch time: 87.26 s
2024-12-17 11:17:22.623128: Yayy! New best EMA pseudo Dice: 0.7235
2024-12-17 11:17:24.162646: 
2024-12-17 11:17:24.164559: Epoch 74
2024-12-17 11:17:24.165366: Current learning rate: 0.00542
2024-12-17 11:18:51.531095: Validation loss did not improve from -0.54227. Patience: 40/50
2024-12-17 11:18:51.532388: train_loss -0.7333
2024-12-17 11:18:51.533743: val_loss -0.5119
2024-12-17 11:18:51.534801: Pseudo dice [0.7304]
2024-12-17 11:18:51.535817: Epoch time: 87.37 s
2024-12-17 11:18:51.896322: Yayy! New best EMA pseudo Dice: 0.7242
2024-12-17 11:18:53.403928: 
2024-12-17 11:18:53.405883: Epoch 75
2024-12-17 11:18:53.407372: Current learning rate: 0.00536
2024-12-17 11:20:20.532655: Validation loss did not improve from -0.54227. Patience: 41/50
2024-12-17 11:20:20.533591: train_loss -0.7332
2024-12-17 11:20:20.534647: val_loss -0.5031
2024-12-17 11:20:20.535597: Pseudo dice [0.7264]
2024-12-17 11:20:20.536376: Epoch time: 87.13 s
2024-12-17 11:20:20.536983: Yayy! New best EMA pseudo Dice: 0.7244
2024-12-17 11:20:22.072880: 
2024-12-17 11:20:22.074888: Epoch 76
2024-12-17 11:20:22.075950: Current learning rate: 0.00529
2024-12-17 11:21:48.957938: Validation loss did not improve from -0.54227. Patience: 42/50
2024-12-17 11:21:48.958847: train_loss -0.739
2024-12-17 11:21:48.959893: val_loss -0.5189
2024-12-17 11:21:48.960877: Pseudo dice [0.7259]
2024-12-17 11:21:48.961642: Epoch time: 86.89 s
2024-12-17 11:21:48.962559: Yayy! New best EMA pseudo Dice: 0.7246
2024-12-17 11:21:50.501867: 
2024-12-17 11:21:50.504204: Epoch 77
2024-12-17 11:21:50.505599: Current learning rate: 0.00523
2024-12-17 11:23:17.698626: Validation loss did not improve from -0.54227. Patience: 43/50
2024-12-17 11:23:17.699929: train_loss -0.7334
2024-12-17 11:23:17.701437: val_loss -0.4902
2024-12-17 11:23:17.702654: Pseudo dice [0.7115]
2024-12-17 11:23:17.703353: Epoch time: 87.2 s
2024-12-17 11:23:18.897187: 
2024-12-17 11:23:18.899091: Epoch 78
2024-12-17 11:23:18.900238: Current learning rate: 0.00517
2024-12-17 11:24:46.036477: Validation loss did not improve from -0.54227. Patience: 44/50
2024-12-17 11:24:46.037860: train_loss -0.7387
2024-12-17 11:24:46.038783: val_loss -0.5058
2024-12-17 11:24:46.039448: Pseudo dice [0.7237]
2024-12-17 11:24:46.040330: Epoch time: 87.14 s
2024-12-17 11:24:47.211382: 
2024-12-17 11:24:47.213350: Epoch 79
2024-12-17 11:24:47.214622: Current learning rate: 0.0051
2024-12-17 11:26:14.282131: Validation loss did not improve from -0.54227. Patience: 45/50
2024-12-17 11:26:14.283132: train_loss -0.7356
2024-12-17 11:26:14.284288: val_loss -0.4967
2024-12-17 11:26:14.285210: Pseudo dice [0.7277]
2024-12-17 11:26:14.286213: Epoch time: 87.07 s
2024-12-17 11:26:15.810191: 
2024-12-17 11:26:15.811925: Epoch 80
2024-12-17 11:26:15.812927: Current learning rate: 0.00504
2024-12-17 11:27:43.009758: Validation loss did not improve from -0.54227. Patience: 46/50
2024-12-17 11:27:43.011189: train_loss -0.7347
2024-12-17 11:27:43.012311: val_loss -0.4834
2024-12-17 11:27:43.013316: Pseudo dice [0.7115]
2024-12-17 11:27:43.014336: Epoch time: 87.2 s
2024-12-17 11:27:44.220784: 
2024-12-17 11:27:44.223043: Epoch 81
2024-12-17 11:27:44.224170: Current learning rate: 0.00497
2024-12-17 11:29:11.512534: Validation loss did not improve from -0.54227. Patience: 47/50
2024-12-17 11:29:11.513598: train_loss -0.7354
2024-12-17 11:29:11.514838: val_loss -0.4803
2024-12-17 11:29:11.515755: Pseudo dice [0.7238]
2024-12-17 11:29:11.516655: Epoch time: 87.29 s
2024-12-17 11:29:13.034851: 
2024-12-17 11:29:13.036976: Epoch 82
2024-12-17 11:29:13.038534: Current learning rate: 0.00491
2024-12-17 11:30:40.403157: Validation loss did not improve from -0.54227. Patience: 48/50
2024-12-17 11:30:40.404465: train_loss -0.7423
2024-12-17 11:30:40.405652: val_loss -0.4947
2024-12-17 11:30:40.406532: Pseudo dice [0.7175]
2024-12-17 11:30:40.407207: Epoch time: 87.37 s
2024-12-17 11:30:41.526576: 
2024-12-17 11:30:41.528680: Epoch 83
2024-12-17 11:30:41.529807: Current learning rate: 0.00484
2024-12-17 11:32:14.206766: Validation loss did not improve from -0.54227. Patience: 49/50
2024-12-17 11:32:14.209766: train_loss -0.7484
2024-12-17 11:32:14.212919: val_loss -0.5225
2024-12-17 11:32:14.214093: Pseudo dice [0.7403]
2024-12-17 11:32:14.215356: Epoch time: 92.68 s
2024-12-17 11:32:15.562924: 
2024-12-17 11:32:15.564645: Epoch 84
2024-12-17 11:32:15.565485: Current learning rate: 0.00478
2024-12-17 11:33:42.861877: Validation loss did not improve from -0.54227. Patience: 50/50
2024-12-17 11:33:42.863247: train_loss -0.7448
2024-12-17 11:33:42.864323: val_loss -0.5338
2024-12-17 11:33:42.865260: Pseudo dice [0.7384]
2024-12-17 11:33:42.866137: Epoch time: 87.3 s
2024-12-17 11:33:43.245827: Yayy! New best EMA pseudo Dice: 0.7254
2024-12-17 11:33:44.824924: 
2024-12-17 11:33:44.827199: Epoch 85
2024-12-17 11:33:44.828153: Current learning rate: 0.00471
2024-12-17 11:35:12.267585: Validation loss did not improve from -0.54227. Patience: 51/50
2024-12-17 11:35:12.269209: train_loss -0.7464
2024-12-17 11:35:12.270344: val_loss -0.5037
2024-12-17 11:35:12.271376: Pseudo dice [0.7254]
2024-12-17 11:35:12.272395: Epoch time: 87.45 s
2024-12-17 11:35:13.413645: 
2024-12-17 11:35:13.415126: Epoch 86
2024-12-17 11:35:13.415826: Current learning rate: 0.00465
2024-12-17 11:36:40.767636: Validation loss did not improve from -0.54227. Patience: 52/50
2024-12-17 11:36:40.769145: train_loss -0.7458
2024-12-17 11:36:40.770674: val_loss -0.5422
2024-12-17 11:36:40.771812: Pseudo dice [0.7492]
2024-12-17 11:36:40.772891: Epoch time: 87.36 s
2024-12-17 11:36:40.773840: Yayy! New best EMA pseudo Dice: 0.7278
2024-12-17 11:36:42.266335: 
2024-12-17 11:36:42.268505: Epoch 87
2024-12-17 11:36:42.269676: Current learning rate: 0.00458
2024-12-17 11:38:09.676819: Validation loss did not improve from -0.54227. Patience: 53/50
2024-12-17 11:38:09.678656: train_loss -0.7476
2024-12-17 11:38:09.680357: val_loss -0.5078
2024-12-17 11:38:09.681122: Pseudo dice [0.7317]
2024-12-17 11:38:09.681848: Epoch time: 87.41 s
2024-12-17 11:38:09.682576: Yayy! New best EMA pseudo Dice: 0.7282
2024-12-17 11:38:11.232083: 
2024-12-17 11:38:11.234082: Epoch 88
2024-12-17 11:38:11.235207: Current learning rate: 0.00452
2024-12-17 11:39:38.480321: Validation loss did not improve from -0.54227. Patience: 54/50
2024-12-17 11:39:38.481552: train_loss -0.7441
2024-12-17 11:39:38.482738: val_loss -0.5135
2024-12-17 11:39:38.483801: Pseudo dice [0.7271]
2024-12-17 11:39:38.484753: Epoch time: 87.25 s
2024-12-17 11:39:39.675571: 
2024-12-17 11:39:39.677831: Epoch 89
2024-12-17 11:39:39.678959: Current learning rate: 0.00445
2024-12-17 11:41:06.667118: Validation loss did not improve from -0.54227. Patience: 55/50
2024-12-17 11:41:06.668219: train_loss -0.748
2024-12-17 11:41:06.669528: val_loss -0.5408
2024-12-17 11:41:06.670691: Pseudo dice [0.7446]
2024-12-17 11:41:06.671713: Epoch time: 86.99 s
2024-12-17 11:41:07.050562: Yayy! New best EMA pseudo Dice: 0.7297
2024-12-17 11:41:08.524690: 
2024-12-17 11:41:08.526501: Epoch 90
2024-12-17 11:41:08.527646: Current learning rate: 0.00438
2024-12-17 11:42:35.311682: Validation loss did not improve from -0.54227. Patience: 56/50
2024-12-17 11:42:35.313247: train_loss -0.7478
2024-12-17 11:42:35.314910: val_loss -0.5369
2024-12-17 11:42:35.315909: Pseudo dice [0.7443]
2024-12-17 11:42:35.317132: Epoch time: 86.79 s
2024-12-17 11:42:35.318331: Yayy! New best EMA pseudo Dice: 0.7312
2024-12-17 11:42:36.847589: 
2024-12-17 11:42:36.849214: Epoch 91
2024-12-17 11:42:36.850376: Current learning rate: 0.00432
2024-12-17 11:44:03.663356: Validation loss did not improve from -0.54227. Patience: 57/50
2024-12-17 11:44:03.664905: train_loss -0.7502
2024-12-17 11:44:03.666144: val_loss -0.5116
2024-12-17 11:44:03.667117: Pseudo dice [0.7269]
2024-12-17 11:44:03.668165: Epoch time: 86.82 s
2024-12-17 11:44:04.786877: 
2024-12-17 11:44:04.788997: Epoch 92
2024-12-17 11:44:04.790235: Current learning rate: 0.00425
2024-12-17 11:45:31.620410: Validation loss did not improve from -0.54227. Patience: 58/50
2024-12-17 11:45:31.621484: train_loss -0.7509
2024-12-17 11:45:31.622649: val_loss -0.5323
2024-12-17 11:45:31.623393: Pseudo dice [0.7386]
2024-12-17 11:45:31.624295: Epoch time: 86.84 s
2024-12-17 11:45:31.625211: Yayy! New best EMA pseudo Dice: 0.7315
2024-12-17 11:45:34.096044: 
2024-12-17 11:45:34.097638: Epoch 93
2024-12-17 11:45:34.098886: Current learning rate: 0.00419
2024-12-17 11:47:00.848563: Validation loss did not improve from -0.54227. Patience: 59/50
2024-12-17 11:47:00.849577: train_loss -0.7505
2024-12-17 11:47:00.850593: val_loss -0.5003
2024-12-17 11:47:00.851542: Pseudo dice [0.7263]
2024-12-17 11:47:00.852522: Epoch time: 86.75 s
2024-12-17 11:47:02.003044: 
2024-12-17 11:47:02.004908: Epoch 94
2024-12-17 11:47:02.005892: Current learning rate: 0.00412
2024-12-17 11:48:29.011374: Validation loss did not improve from -0.54227. Patience: 60/50
2024-12-17 11:48:29.013091: train_loss -0.752
2024-12-17 11:48:29.014392: val_loss -0.4987
2024-12-17 11:48:29.015249: Pseudo dice [0.7317]
2024-12-17 11:48:29.016052: Epoch time: 87.01 s
2024-12-17 11:48:30.519984: 
2024-12-17 11:48:30.522203: Epoch 95
2024-12-17 11:48:30.523271: Current learning rate: 0.00405
2024-12-17 11:49:57.209178: Validation loss did not improve from -0.54227. Patience: 61/50
2024-12-17 11:49:57.210433: train_loss -0.7551
2024-12-17 11:49:57.211419: val_loss -0.4946
2024-12-17 11:49:57.212262: Pseudo dice [0.7273]
2024-12-17 11:49:57.212908: Epoch time: 86.69 s
2024-12-17 11:49:58.334243: 
2024-12-17 11:49:58.336646: Epoch 96
2024-12-17 11:49:58.337676: Current learning rate: 0.00399
2024-12-17 11:51:25.083650: Validation loss did not improve from -0.54227. Patience: 62/50
2024-12-17 11:51:25.084437: train_loss -0.7569
2024-12-17 11:51:25.085595: val_loss -0.5292
2024-12-17 11:51:25.086935: Pseudo dice [0.7406]
2024-12-17 11:51:25.088209: Epoch time: 86.75 s
2024-12-17 11:51:25.089401: Yayy! New best EMA pseudo Dice: 0.7317
2024-12-17 11:51:26.571960: 
2024-12-17 11:51:26.574761: Epoch 97
2024-12-17 11:51:26.576466: Current learning rate: 0.00392
2024-12-17 11:52:53.444902: Validation loss did not improve from -0.54227. Patience: 63/50
2024-12-17 11:52:53.446189: train_loss -0.7563
2024-12-17 11:52:53.447345: val_loss -0.541
2024-12-17 11:52:53.448621: Pseudo dice [0.7449]
2024-12-17 11:52:53.449927: Epoch time: 86.88 s
2024-12-17 11:52:53.451025: Yayy! New best EMA pseudo Dice: 0.733
2024-12-17 11:52:54.891313: 
2024-12-17 11:52:54.894318: Epoch 98
2024-12-17 11:52:54.896456: Current learning rate: 0.00385
2024-12-17 11:54:21.911745: Validation loss did not improve from -0.54227. Patience: 64/50
2024-12-17 11:54:21.913350: train_loss -0.7551
2024-12-17 11:54:21.915432: val_loss -0.4856
2024-12-17 11:54:21.916450: Pseudo dice [0.7262]
2024-12-17 11:54:21.917432: Epoch time: 87.02 s
2024-12-17 11:54:23.016867: 
2024-12-17 11:54:23.019288: Epoch 99
2024-12-17 11:54:23.020649: Current learning rate: 0.00379
2024-12-17 11:55:50.128390: Validation loss did not improve from -0.54227. Patience: 65/50
2024-12-17 11:55:50.129235: train_loss -0.7578
2024-12-17 11:55:50.130504: val_loss -0.5269
2024-12-17 11:55:50.131579: Pseudo dice [0.7342]
2024-12-17 11:55:50.132342: Epoch time: 87.11 s
2024-12-17 11:55:51.646544: 
2024-12-17 11:55:51.648339: Epoch 100
2024-12-17 11:55:51.649479: Current learning rate: 0.00372
2024-12-17 11:57:18.727762: Validation loss did not improve from -0.54227. Patience: 66/50
2024-12-17 11:57:18.728727: train_loss -0.7579
2024-12-17 11:57:18.729997: val_loss -0.4989
2024-12-17 11:57:18.730991: Pseudo dice [0.7273]
2024-12-17 11:57:18.731761: Epoch time: 87.08 s
2024-12-17 11:57:19.856675: 
2024-12-17 11:57:19.858648: Epoch 101
2024-12-17 11:57:19.859881: Current learning rate: 0.00365
2024-12-17 11:58:46.976948: Validation loss did not improve from -0.54227. Patience: 67/50
2024-12-17 11:58:46.978391: train_loss -0.7604
2024-12-17 11:58:46.979812: val_loss -0.5253
2024-12-17 11:58:46.980987: Pseudo dice [0.7326]
2024-12-17 11:58:46.982027: Epoch time: 87.12 s
2024-12-17 11:58:48.137255: 
2024-12-17 11:58:48.139258: Epoch 102
2024-12-17 11:58:48.140576: Current learning rate: 0.00359
2024-12-17 12:00:15.267343: Validation loss did not improve from -0.54227. Patience: 68/50
2024-12-17 12:00:15.268522: train_loss -0.7567
2024-12-17 12:00:15.269636: val_loss -0.4844
2024-12-17 12:00:15.270609: Pseudo dice [0.715]
2024-12-17 12:00:15.271452: Epoch time: 87.13 s
2024-12-17 12:00:16.425045: 
2024-12-17 12:00:16.426761: Epoch 103
2024-12-17 12:00:16.427966: Current learning rate: 0.00352
2024-12-17 12:01:43.679753: Validation loss did not improve from -0.54227. Patience: 69/50
2024-12-17 12:01:43.680739: train_loss -0.7596
2024-12-17 12:01:43.681852: val_loss -0.5219
2024-12-17 12:01:43.682845: Pseudo dice [0.7339]
2024-12-17 12:01:43.683783: Epoch time: 87.26 s
2024-12-17 12:01:44.807208: 
2024-12-17 12:01:44.809345: Epoch 104
2024-12-17 12:01:44.810897: Current learning rate: 0.00345
2024-12-17 12:03:11.976230: Validation loss did not improve from -0.54227. Patience: 70/50
2024-12-17 12:03:11.977217: train_loss -0.7617
2024-12-17 12:03:11.978366: val_loss -0.5075
2024-12-17 12:03:11.979156: Pseudo dice [0.7297]
2024-12-17 12:03:11.979961: Epoch time: 87.17 s
2024-12-17 12:03:13.972356: 
2024-12-17 12:03:13.974806: Epoch 105
2024-12-17 12:03:13.976081: Current learning rate: 0.00338
2024-12-17 12:04:41.344896: Validation loss did not improve from -0.54227. Patience: 71/50
2024-12-17 12:04:41.345865: train_loss -0.7591
2024-12-17 12:04:41.347011: val_loss -0.4923
2024-12-17 12:04:41.348085: Pseudo dice [0.7168]
2024-12-17 12:04:41.349166: Epoch time: 87.37 s
2024-12-17 12:04:42.518227: 
2024-12-17 12:04:42.520022: Epoch 106
2024-12-17 12:04:42.521412: Current learning rate: 0.00332
2024-12-17 12:06:09.971150: Validation loss did not improve from -0.54227. Patience: 72/50
2024-12-17 12:06:09.972082: train_loss -0.7625
2024-12-17 12:06:09.973024: val_loss -0.4852
2024-12-17 12:06:09.973747: Pseudo dice [0.7332]
2024-12-17 12:06:09.974528: Epoch time: 87.45 s
2024-12-17 12:06:11.172719: 
2024-12-17 12:06:11.174575: Epoch 107
2024-12-17 12:06:11.175563: Current learning rate: 0.00325
2024-12-17 12:07:38.687618: Validation loss did not improve from -0.54227. Patience: 73/50
2024-12-17 12:07:38.688583: train_loss -0.7654
2024-12-17 12:07:38.689699: val_loss -0.4928
2024-12-17 12:07:38.690559: Pseudo dice [0.7221]
2024-12-17 12:07:38.691465: Epoch time: 87.52 s
2024-12-17 12:07:39.874727: 
2024-12-17 12:07:39.876625: Epoch 108
2024-12-17 12:07:39.877881: Current learning rate: 0.00318
2024-12-17 12:09:07.115281: Validation loss did not improve from -0.54227. Patience: 74/50
2024-12-17 12:09:07.116237: train_loss -0.7628
2024-12-17 12:09:07.117463: val_loss -0.5096
2024-12-17 12:09:07.118477: Pseudo dice [0.7259]
2024-12-17 12:09:07.119251: Epoch time: 87.24 s
2024-12-17 12:09:08.267080: 
2024-12-17 12:09:08.268950: Epoch 109
2024-12-17 12:09:08.269950: Current learning rate: 0.00311
2024-12-17 12:10:35.471795: Validation loss did not improve from -0.54227. Patience: 75/50
2024-12-17 12:10:35.472747: train_loss -0.7596
2024-12-17 12:10:35.473718: val_loss -0.5312
2024-12-17 12:10:35.474782: Pseudo dice [0.7439]
2024-12-17 12:10:35.475599: Epoch time: 87.21 s
2024-12-17 12:10:36.972747: 
2024-12-17 12:10:36.974234: Epoch 110
2024-12-17 12:10:36.975252: Current learning rate: 0.00304
2024-12-17 12:12:03.829472: Validation loss did not improve from -0.54227. Patience: 76/50
2024-12-17 12:12:03.830540: train_loss -0.7662
2024-12-17 12:12:03.831661: val_loss -0.5079
2024-12-17 12:12:03.832773: Pseudo dice [0.7256]
2024-12-17 12:12:03.833868: Epoch time: 86.86 s
2024-12-17 12:12:04.996153: 
2024-12-17 12:12:04.997825: Epoch 111
2024-12-17 12:12:04.998651: Current learning rate: 0.00297
2024-12-17 12:13:31.928526: Validation loss did not improve from -0.54227. Patience: 77/50
2024-12-17 12:13:31.930064: train_loss -0.7649
2024-12-17 12:13:31.931860: val_loss -0.5349
2024-12-17 12:13:31.933061: Pseudo dice [0.7369]
2024-12-17 12:13:31.933916: Epoch time: 86.93 s
2024-12-17 12:13:33.067567: 
2024-12-17 12:13:33.069639: Epoch 112
2024-12-17 12:13:33.070899: Current learning rate: 0.00291
2024-12-17 12:15:00.006844: Validation loss did not improve from -0.54227. Patience: 78/50
2024-12-17 12:15:00.007792: train_loss -0.7682
2024-12-17 12:15:00.008659: val_loss -0.521
2024-12-17 12:15:00.009470: Pseudo dice [0.7303]
2024-12-17 12:15:00.010285: Epoch time: 86.94 s
2024-12-17 12:15:01.164967: 
2024-12-17 12:15:01.166760: Epoch 113
2024-12-17 12:15:01.167929: Current learning rate: 0.00284
2024-12-17 12:16:28.078894: Validation loss did not improve from -0.54227. Patience: 79/50
2024-12-17 12:16:28.080315: train_loss -0.765
2024-12-17 12:16:28.081419: val_loss -0.5151
2024-12-17 12:16:28.082461: Pseudo dice [0.7248]
2024-12-17 12:16:28.083496: Epoch time: 86.92 s
2024-12-17 12:16:29.251688: 
2024-12-17 12:16:29.253435: Epoch 114
2024-12-17 12:16:29.254457: Current learning rate: 0.00277
2024-12-17 12:17:56.271149: Validation loss did not improve from -0.54227. Patience: 80/50
2024-12-17 12:17:56.272257: train_loss -0.7671
2024-12-17 12:17:56.273470: val_loss -0.5119
2024-12-17 12:17:56.274621: Pseudo dice [0.7314]
2024-12-17 12:17:56.275743: Epoch time: 87.02 s
2024-12-17 12:17:57.792727: 
2024-12-17 12:17:57.794695: Epoch 115
2024-12-17 12:17:57.796014: Current learning rate: 0.0027
2024-12-17 12:19:24.731597: Validation loss did not improve from -0.54227. Patience: 81/50
2024-12-17 12:19:24.733030: train_loss -0.7729
2024-12-17 12:19:24.734683: val_loss -0.5251
2024-12-17 12:19:24.736202: Pseudo dice [0.7369]
2024-12-17 12:19:24.737580: Epoch time: 86.94 s
2024-12-17 12:19:26.251360: 
2024-12-17 12:19:26.253224: Epoch 116
2024-12-17 12:19:26.254430: Current learning rate: 0.00263
2024-12-17 12:20:53.187400: Validation loss did not improve from -0.54227. Patience: 82/50
2024-12-17 12:20:53.188877: train_loss -0.7669
2024-12-17 12:20:53.190276: val_loss -0.4967
2024-12-17 12:20:53.191331: Pseudo dice [0.7253]
2024-12-17 12:20:53.192288: Epoch time: 86.94 s
2024-12-17 12:20:54.357560: 
2024-12-17 12:20:54.359875: Epoch 117
2024-12-17 12:20:54.361128: Current learning rate: 0.00256
2024-12-17 12:22:21.417969: Validation loss did not improve from -0.54227. Patience: 83/50
2024-12-17 12:22:21.419266: train_loss -0.7687
2024-12-17 12:22:21.420730: val_loss -0.5251
2024-12-17 12:22:21.421992: Pseudo dice [0.7413]
2024-12-17 12:22:21.423169: Epoch time: 87.06 s
2024-12-17 12:22:22.613282: 
2024-12-17 12:22:22.615833: Epoch 118
2024-12-17 12:22:22.617107: Current learning rate: 0.00249
2024-12-17 12:23:49.964946: Validation loss did not improve from -0.54227. Patience: 84/50
2024-12-17 12:23:49.965948: train_loss -0.7734
2024-12-17 12:23:49.967113: val_loss -0.5142
2024-12-17 12:23:49.967944: Pseudo dice [0.7349]
2024-12-17 12:23:49.968790: Epoch time: 87.35 s
2024-12-17 12:23:51.152676: 
2024-12-17 12:23:51.154766: Epoch 119
2024-12-17 12:23:51.156198: Current learning rate: 0.00242
2024-12-17 12:25:18.446153: Validation loss did not improve from -0.54227. Patience: 85/50
2024-12-17 12:25:18.447494: train_loss -0.7717
2024-12-17 12:25:18.448596: val_loss -0.4989
2024-12-17 12:25:18.449694: Pseudo dice [0.721]
2024-12-17 12:25:18.450617: Epoch time: 87.3 s
2024-12-17 12:25:19.993205: 
2024-12-17 12:25:19.995407: Epoch 120
2024-12-17 12:25:19.996489: Current learning rate: 0.00235
2024-12-17 12:26:47.337387: Validation loss did not improve from -0.54227. Patience: 86/50
2024-12-17 12:26:47.338453: train_loss -0.7723
2024-12-17 12:26:47.339446: val_loss -0.5081
2024-12-17 12:26:47.340559: Pseudo dice [0.7333]
2024-12-17 12:26:47.341385: Epoch time: 87.35 s
2024-12-17 12:26:48.531537: 
2024-12-17 12:26:48.533347: Epoch 121
2024-12-17 12:26:48.534405: Current learning rate: 0.00228
2024-12-17 12:28:15.801428: Validation loss did not improve from -0.54227. Patience: 87/50
2024-12-17 12:28:15.803038: train_loss -0.7719
2024-12-17 12:28:15.804633: val_loss -0.5171
2024-12-17 12:28:15.805552: Pseudo dice [0.7376]
2024-12-17 12:28:15.806460: Epoch time: 87.27 s
2024-12-17 12:28:16.988811: 
2024-12-17 12:28:16.990920: Epoch 122
2024-12-17 12:28:16.992182: Current learning rate: 0.00221
2024-12-17 12:29:44.408455: Validation loss did not improve from -0.54227. Patience: 88/50
2024-12-17 12:29:44.409849: train_loss -0.7751
2024-12-17 12:29:44.411221: val_loss -0.479
2024-12-17 12:29:44.411941: Pseudo dice [0.7122]
2024-12-17 12:29:44.412753: Epoch time: 87.42 s
2024-12-17 12:29:45.606599: 
2024-12-17 12:29:45.608457: Epoch 123
2024-12-17 12:29:45.609470: Current learning rate: 0.00214
2024-12-17 12:31:12.852607: Validation loss did not improve from -0.54227. Patience: 89/50
2024-12-17 12:31:12.853945: train_loss -0.7769
2024-12-17 12:31:12.855179: val_loss -0.4857
2024-12-17 12:31:12.856261: Pseudo dice [0.7249]
2024-12-17 12:31:12.857175: Epoch time: 87.25 s
2024-12-17 12:31:14.056695: 
2024-12-17 12:31:14.059009: Epoch 124
2024-12-17 12:31:14.060260: Current learning rate: 0.00207
2024-12-17 12:32:41.399998: Validation loss did not improve from -0.54227. Patience: 90/50
2024-12-17 12:32:41.401471: train_loss -0.7787
2024-12-17 12:32:41.402835: val_loss -0.5208
2024-12-17 12:32:41.403492: Pseudo dice [0.7348]
2024-12-17 12:32:41.404159: Epoch time: 87.35 s
2024-12-17 12:32:42.961350: 
2024-12-17 12:32:42.963659: Epoch 125
2024-12-17 12:32:42.964925: Current learning rate: 0.00199
2024-12-17 12:34:10.551726: Validation loss improved from -0.54227 to -0.54357! Patience: 90/50
2024-12-17 12:34:10.552854: train_loss -0.7761
2024-12-17 12:34:10.553639: val_loss -0.5436
2024-12-17 12:34:10.554353: Pseudo dice [0.7431]
2024-12-17 12:34:10.555154: Epoch time: 87.59 s
2024-12-17 12:34:11.765717: 
2024-12-17 12:34:11.768169: Epoch 126
2024-12-17 12:34:11.769023: Current learning rate: 0.00192
2024-12-17 12:35:39.332496: Validation loss did not improve from -0.54357. Patience: 1/50
2024-12-17 12:35:39.333795: train_loss -0.7766
2024-12-17 12:35:39.334817: val_loss -0.5243
2024-12-17 12:35:39.335609: Pseudo dice [0.7363]
2024-12-17 12:35:39.336564: Epoch time: 87.57 s
2024-12-17 12:35:40.839083: 
2024-12-17 12:35:40.841131: Epoch 127
2024-12-17 12:35:40.842091: Current learning rate: 0.00185
2024-12-17 12:37:08.404878: Validation loss did not improve from -0.54357. Patience: 2/50
2024-12-17 12:37:08.407577: train_loss -0.7804
2024-12-17 12:37:08.409990: val_loss -0.5028
2024-12-17 12:37:08.410942: Pseudo dice [0.724]
2024-12-17 12:37:08.412680: Epoch time: 87.57 s
2024-12-17 12:37:09.634298: 
2024-12-17 12:37:09.636136: Epoch 128
2024-12-17 12:37:09.637279: Current learning rate: 0.00178
2024-12-17 12:38:37.279873: Validation loss did not improve from -0.54357. Patience: 3/50
2024-12-17 12:38:37.281065: train_loss -0.7769
2024-12-17 12:38:37.282489: val_loss -0.5218
2024-12-17 12:38:37.283571: Pseudo dice [0.7355]
2024-12-17 12:38:37.284538: Epoch time: 87.65 s
2024-12-17 12:38:38.455726: 
2024-12-17 12:38:38.457967: Epoch 129
2024-12-17 12:38:38.459379: Current learning rate: 0.0017
2024-12-17 12:40:06.081398: Validation loss did not improve from -0.54357. Patience: 4/50
2024-12-17 12:40:06.082816: train_loss -0.7758
2024-12-17 12:40:06.084193: val_loss -0.5069
2024-12-17 12:40:06.085355: Pseudo dice [0.7268]
2024-12-17 12:40:06.086507: Epoch time: 87.63 s
2024-12-17 12:40:07.612989: 
2024-12-17 12:40:07.614760: Epoch 130
2024-12-17 12:40:07.615919: Current learning rate: 0.00163
2024-12-17 12:41:35.298758: Validation loss did not improve from -0.54357. Patience: 5/50
2024-12-17 12:41:35.300945: train_loss -0.772
2024-12-17 12:41:35.302072: val_loss -0.5365
2024-12-17 12:41:35.302879: Pseudo dice [0.7413]
2024-12-17 12:41:35.303666: Epoch time: 87.69 s
2024-12-17 12:41:36.604439: 
2024-12-17 12:41:36.606669: Epoch 131
2024-12-17 12:41:36.607843: Current learning rate: 0.00156
2024-12-17 12:43:06.257602: Validation loss did not improve from -0.54357. Patience: 6/50
2024-12-17 12:43:06.259488: train_loss -0.7781
2024-12-17 12:43:06.260617: val_loss -0.5086
2024-12-17 12:43:06.261407: Pseudo dice [0.7238]
2024-12-17 12:43:06.262361: Epoch time: 89.66 s
2024-12-17 12:43:07.668097: 
2024-12-17 12:43:07.669566: Epoch 132
2024-12-17 12:43:07.670439: Current learning rate: 0.00148
2024-12-17 12:44:41.100767: Validation loss did not improve from -0.54357. Patience: 7/50
2024-12-17 12:44:41.101961: train_loss -0.7769
2024-12-17 12:44:41.102795: val_loss -0.535
2024-12-17 12:44:41.103573: Pseudo dice [0.7444]
2024-12-17 12:44:41.104536: Epoch time: 93.44 s
2024-12-17 12:44:42.501841: 
2024-12-17 12:44:42.503177: Epoch 133
2024-12-17 12:44:42.504089: Current learning rate: 0.00141
2024-12-17 12:46:28.450932: Validation loss did not improve from -0.54357. Patience: 8/50
2024-12-17 12:46:28.452069: train_loss -0.783
2024-12-17 12:46:28.452899: val_loss -0.4841
2024-12-17 12:46:28.453690: Pseudo dice [0.7157]
2024-12-17 12:46:28.454477: Epoch time: 105.95 s
2024-12-17 12:46:29.931656: 
2024-12-17 12:46:29.932977: Epoch 134
2024-12-17 12:46:29.933783: Current learning rate: 0.00133
2024-12-17 12:48:44.900980: Validation loss did not improve from -0.54357. Patience: 9/50
2024-12-17 12:48:44.902108: train_loss -0.7794
2024-12-17 12:48:44.903013: val_loss -0.5328
2024-12-17 12:48:44.903679: Pseudo dice [0.7404]
2024-12-17 12:48:44.904491: Epoch time: 134.97 s
2024-12-17 12:48:46.707961: 
2024-12-17 12:48:46.709343: Epoch 135
2024-12-17 12:48:46.710088: Current learning rate: 0.00126
2024-12-17 12:51:07.447096: Validation loss did not improve from -0.54357. Patience: 10/50
2024-12-17 12:51:07.448168: train_loss -0.7844
2024-12-17 12:51:07.449017: val_loss -0.525
2024-12-17 12:51:07.449843: Pseudo dice [0.7376]
2024-12-17 12:51:07.450663: Epoch time: 140.74 s
2024-12-17 12:51:08.956026: 
2024-12-17 12:51:08.957332: Epoch 136
2024-12-17 12:51:08.958632: Current learning rate: 0.00118
2024-12-17 12:54:15.057476: Validation loss did not improve from -0.54357. Patience: 11/50
2024-12-17 12:54:15.058762: train_loss -0.78
2024-12-17 12:54:15.059747: val_loss -0.5141
2024-12-17 12:54:15.060547: Pseudo dice [0.7261]
2024-12-17 12:54:15.061461: Epoch time: 186.1 s
2024-12-17 12:54:16.498699: 
2024-12-17 12:54:16.499744: Epoch 137
2024-12-17 12:54:16.500423: Current learning rate: 0.00111
2024-12-17 12:57:34.385337: Validation loss did not improve from -0.54357. Patience: 12/50
2024-12-17 12:57:34.386107: train_loss -0.7815
2024-12-17 12:57:34.387059: val_loss -0.5137
2024-12-17 12:57:34.387936: Pseudo dice [0.7316]
2024-12-17 12:57:34.388663: Epoch time: 197.89 s
2024-12-17 12:57:36.349382: 
2024-12-17 12:57:36.350891: Epoch 138
2024-12-17 12:57:36.351830: Current learning rate: 0.00103
2024-12-17 13:01:11.058733: Validation loss did not improve from -0.54357. Patience: 13/50
2024-12-17 13:01:11.059619: train_loss -0.7796
2024-12-17 13:01:11.060458: val_loss -0.5107
2024-12-17 13:01:11.061161: Pseudo dice [0.7265]
2024-12-17 13:01:11.061766: Epoch time: 214.71 s
2024-12-17 13:01:12.492199: 
2024-12-17 13:01:12.493854: Epoch 139
2024-12-17 13:01:12.494870: Current learning rate: 0.00095
2024-12-17 13:04:55.156715: Validation loss did not improve from -0.54357. Patience: 14/50
2024-12-17 13:04:55.157478: train_loss -0.7815
2024-12-17 13:04:55.158319: val_loss -0.5375
2024-12-17 13:04:55.158957: Pseudo dice [0.7424]
2024-12-17 13:04:55.159624: Epoch time: 222.67 s
2024-12-17 13:04:56.993655: 
2024-12-17 13:04:56.995031: Epoch 140
2024-12-17 13:04:56.995845: Current learning rate: 0.00087
2024-12-17 13:08:30.763179: Validation loss did not improve from -0.54357. Patience: 15/50
2024-12-17 13:08:30.764227: train_loss -0.7829
2024-12-17 13:08:30.765114: val_loss -0.5165
2024-12-17 13:08:30.765932: Pseudo dice [0.7358]
2024-12-17 13:08:30.766607: Epoch time: 213.77 s
2024-12-17 13:08:32.124991: 
2024-12-17 13:08:32.126275: Epoch 141
2024-12-17 13:08:32.127129: Current learning rate: 0.00079
2024-12-17 13:12:03.191368: Validation loss did not improve from -0.54357. Patience: 16/50
2024-12-17 13:12:03.192355: train_loss -0.7842
2024-12-17 13:12:03.193282: val_loss -0.535
2024-12-17 13:12:03.194005: Pseudo dice [0.746]
2024-12-17 13:12:03.194706: Epoch time: 211.07 s
2024-12-17 13:12:03.195373: Yayy! New best EMA pseudo Dice: 0.734
2024-12-17 13:12:05.064254: 
2024-12-17 13:12:05.065490: Epoch 142
2024-12-17 13:12:05.066203: Current learning rate: 0.00071
2024-12-17 13:15:45.361362: Validation loss did not improve from -0.54357. Patience: 17/50
2024-12-17 13:15:45.362371: train_loss -0.7856
2024-12-17 13:15:45.363168: val_loss -0.5113
2024-12-17 13:15:45.363945: Pseudo dice [0.729]
2024-12-17 13:15:45.364644: Epoch time: 220.3 s
2024-12-17 13:15:46.780285: 
2024-12-17 13:15:46.781714: Epoch 143
2024-12-17 13:15:46.782475: Current learning rate: 0.00063
2024-12-17 13:19:29.067838: Validation loss did not improve from -0.54357. Patience: 18/50
2024-12-17 13:19:29.068728: train_loss -0.7842
2024-12-17 13:19:29.069681: val_loss -0.4993
2024-12-17 13:19:29.070496: Pseudo dice [0.7303]
2024-12-17 13:19:29.071183: Epoch time: 222.29 s
2024-12-17 13:19:30.543445: 
2024-12-17 13:19:30.544746: Epoch 144
2024-12-17 13:19:30.545632: Current learning rate: 0.00055
2024-12-17 13:23:17.044726: Validation loss did not improve from -0.54357. Patience: 19/50
2024-12-17 13:23:17.045786: train_loss -0.7862
2024-12-17 13:23:17.046713: val_loss -0.5013
2024-12-17 13:23:17.047379: Pseudo dice [0.727]
2024-12-17 13:23:17.048131: Epoch time: 226.5 s
2024-12-17 13:23:18.981064: 
2024-12-17 13:23:18.982412: Epoch 145
2024-12-17 13:23:18.983187: Current learning rate: 0.00047
2024-12-17 13:27:05.595969: Validation loss did not improve from -0.54357. Patience: 20/50
2024-12-17 13:27:05.596949: train_loss -0.7877
2024-12-17 13:27:05.597856: val_loss -0.5203
2024-12-17 13:27:05.598527: Pseudo dice [0.7352]
2024-12-17 13:27:05.599414: Epoch time: 226.62 s
2024-12-17 13:27:07.145050: 
2024-12-17 13:27:07.146498: Epoch 146
2024-12-17 13:27:07.147357: Current learning rate: 0.00038
2024-12-17 13:31:01.037513: Validation loss did not improve from -0.54357. Patience: 21/50
2024-12-17 13:31:01.038446: train_loss -0.7846
2024-12-17 13:31:01.039281: val_loss -0.5243
2024-12-17 13:31:01.040030: Pseudo dice [0.7418]
2024-12-17 13:31:01.040837: Epoch time: 233.89 s
2024-12-17 13:31:02.593079: 
2024-12-17 13:31:02.594421: Epoch 147
2024-12-17 13:31:02.595258: Current learning rate: 0.0003
2024-12-17 13:34:54.811581: Validation loss did not improve from -0.54357. Patience: 22/50
2024-12-17 13:34:54.812660: train_loss -0.7858
2024-12-17 13:34:54.813560: val_loss -0.5132
2024-12-17 13:34:54.814246: Pseudo dice [0.7354]
2024-12-17 13:34:54.814905: Epoch time: 232.22 s
2024-12-17 13:34:56.269994: 
2024-12-17 13:34:56.271079: Epoch 148
2024-12-17 13:34:56.271805: Current learning rate: 0.00021
2024-12-17 13:38:51.382761: Validation loss did not improve from -0.54357. Patience: 23/50
2024-12-17 13:38:51.383749: train_loss -0.7857
2024-12-17 13:38:51.384615: val_loss -0.513
2024-12-17 13:38:51.385375: Pseudo dice [0.7335]
2024-12-17 13:38:51.386004: Epoch time: 235.12 s
2024-12-17 13:38:53.319762: 
2024-12-17 13:38:53.321128: Epoch 149
2024-12-17 13:38:53.322224: Current learning rate: 0.00011
2024-12-17 13:42:49.705017: Validation loss did not improve from -0.54357. Patience: 24/50
2024-12-17 13:42:49.707437: train_loss -0.7874
2024-12-17 13:42:49.709033: val_loss -0.5091
2024-12-17 13:42:49.710029: Pseudo dice [0.7319]
2024-12-17 13:42:49.711125: Epoch time: 236.39 s
2024-12-17 13:42:51.842083: Training done.
2024-12-17 13:42:52.151737: Using splits from existing split file: /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset309_Sohee_Calcium_OCT_CrossValidation/splits_final.json
2024-12-17 13:42:52.165643: The split file contains 5 splits.
2024-12-17 13:42:52.166575: Desired fold for training: 4
2024-12-17 13:42:52.167277: This split has 7 training and 1 validation cases.
2024-12-17 13:42:52.168164: predicting 101-045
2024-12-17 13:42:52.204770: 101-045, shape torch.Size([1, 375, 498, 498]), rank 0
2024-12-17 13:45:22.845230: Validation complete
2024-12-17 13:45:22.846429: Mean Validation Dice:  0.723639372500889
