/home/gridsan/nchutisilp/.conda/envs/nnunet/bin/python
Starting training with CONFIG=3d_32x160x128_b10, DATASET_ID=308, TRAINER=nnUNetTrainerScaleAnalysis20

############################
INFO: You are using the old nnU-Net default plans. We have updated our recommendations. Please consider using those instead! Read more here: https://github.com/MIC-DKFZ/nnUNet/blob/master/documentation/resenc_presets.md
############################

Using device: cuda:0

#######################################################################
Please cite the following paper when using nnU-Net:
Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.
#######################################################################

2024-12-20 17:52:38.220820: Using torch.compile...

############################
INFO: You are using the old nnU-Net default plans. We have updated our recommendations. Please consider using those instead! Read more here: https://github.com/MIC-DKFZ/nnUNet/blob/master/documentation/resenc_presets.md
############################

Using device: cuda:0

#######################################################################
Please cite the following paper when using nnU-Net:
Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.
#######################################################################

2024-12-20 17:52:38.233216: Using torch.compile...
/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
################### Loading pretrained weights from file  /home/gridsan/nchutisilp/projects/OpenAI-CLIP/clip_pretrained_nnUNet.pt ###################
Below is the list of overlapping blocks in pretrained model and nnUNet architecture:
encoder.stages.0.0.convs.0.conv.weight shape torch.Size([32, 1, 3, 3, 3])
encoder.stages.0.0.convs.0.conv.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.norm.weight shape torch.Size([32])
encoder.stages.0.0.convs.0.norm.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.0.weight shape torch.Size([32, 1, 3, 3, 3])
encoder.stages.0.0.convs.0.all_modules.0.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.1.weight shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.1.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
encoder.stages.0.0.convs.1.conv.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.norm.weight shape torch.Size([32])
encoder.stages.0.0.convs.1.norm.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
encoder.stages.0.0.convs.1.all_modules.0.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.1.weight shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.1.bias shape torch.Size([32])
encoder.stages.1.0.convs.0.conv.weight shape torch.Size([64, 32, 3, 3, 3])
encoder.stages.1.0.convs.0.conv.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.norm.weight shape torch.Size([64])
encoder.stages.1.0.convs.0.norm.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.0.weight shape torch.Size([64, 32, 3, 3, 3])
encoder.stages.1.0.convs.0.all_modules.0.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.1.weight shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.1.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
encoder.stages.1.0.convs.1.conv.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.norm.weight shape torch.Size([64])
encoder.stages.1.0.convs.1.norm.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
encoder.stages.1.0.convs.1.all_modules.0.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.1.weight shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.1.bias shape torch.Size([64])
encoder.stages.2.0.convs.0.conv.weight shape torch.Size([128, 64, 3, 3, 3])
encoder.stages.2.0.convs.0.conv.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.norm.weight shape torch.Size([128])
encoder.stages.2.0.convs.0.norm.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.0.weight shape torch.Size([128, 64, 3, 3, 3])
encoder.stages.2.0.convs.0.all_modules.0.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.1.weight shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.1.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
encoder.stages.2.0.convs.1.conv.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.norm.weight shape torch.Size([128])
encoder.stages.2.0.convs.1.norm.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
encoder.stages.2.0.convs.1.all_modules.0.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.1.weight shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.1.bias shape torch.Size([128])
encoder.stages.3.0.convs.0.conv.weight shape torch.Size([256, 128, 3, 3, 3])
encoder.stages.3.0.convs.0.conv.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.norm.weight shape torch.Size([256])
encoder.stages.3.0.convs.0.norm.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.0.weight shape torch.Size([256, 128, 3, 3, 3])
encoder.stages.3.0.convs.0.all_modules.0.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.1.weight shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.1.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
encoder.stages.3.0.convs.1.conv.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.norm.weight shape torch.Size([256])
encoder.stages.3.0.convs.1.norm.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
encoder.stages.3.0.convs.1.all_modules.0.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.1.weight shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.1.bias shape torch.Size([256])
encoder.stages.4.0.convs.0.conv.weight shape torch.Size([320, 256, 3, 3, 3])
encoder.stages.4.0.convs.0.conv.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.norm.weight shape torch.Size([320])
encoder.stages.4.0.convs.0.norm.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.0.weight shape torch.Size([320, 256, 3, 3, 3])
encoder.stages.4.0.convs.0.all_modules.0.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.1.weight shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.1.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.4.0.convs.1.conv.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.norm.weight shape torch.Size([320])
encoder.stages.4.0.convs.1.norm.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.4.0.convs.1.all_modules.0.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.1.weight shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.1.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.0.conv.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.norm.weight shape torch.Size([320])
encoder.stages.5.0.convs.0.norm.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.0.all_modules.0.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.1.weight shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.1.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.1.conv.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.norm.weight shape torch.Size([320])
encoder.stages.5.0.convs.1.norm.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.1.all_modules.0.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.1.weight shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.0.0.convs.0.conv.weight shape torch.Size([32, 1, 3, 3, 3])
decoder.encoder.stages.0.0.convs.0.conv.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.norm.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.norm.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.0.weight shape torch.Size([32, 1, 3, 3, 3])
decoder.encoder.stages.0.0.convs.0.all_modules.0.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.1.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.1.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.encoder.stages.0.0.convs.1.conv.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.norm.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.norm.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.encoder.stages.0.0.convs.1.all_modules.0.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.1.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.1.bias shape torch.Size([32])
decoder.encoder.stages.1.0.convs.0.conv.weight shape torch.Size([64, 32, 3, 3, 3])
decoder.encoder.stages.1.0.convs.0.conv.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.norm.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.norm.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.0.weight shape torch.Size([64, 32, 3, 3, 3])
decoder.encoder.stages.1.0.convs.0.all_modules.0.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.1.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.1.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.encoder.stages.1.0.convs.1.conv.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.norm.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.norm.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.encoder.stages.1.0.convs.1.all_modules.0.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.1.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.1.bias shape torch.Size([64])
decoder.encoder.stages.2.0.convs.0.conv.weight shape torch.Size([128, 64, 3, 3, 3])
decoder.encoder.stages.2.0.convs.0.conv.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.norm.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.norm.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.0.weight shape torch.Size([128, 64, 3, 3, 3])
decoder.encoder.stages.2.0.convs.0.all_modules.0.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.1.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.1.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.encoder.stages.2.0.convs.1.conv.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.norm.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.norm.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.encoder.stages.2.0.convs.1.all_modules.0.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.1.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.1.bias shape torch.Size([128])
decoder.encoder.stages.3.0.convs.0.conv.weight shape torch.Size([256, 128, 3, 3, 3])
decoder.encoder.stages.3.0.convs.0.conv.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.norm.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.norm.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.0.weight shape torch.Size([256, 128, 3, 3, 3])
decoder.encoder.stages.3.0.convs.0.all_modules.0.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.1.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.1.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.encoder.stages.3.0.convs.1.conv.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.norm.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.norm.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.encoder.stages.3.0.convs.1.all_modules.0.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.1.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.1.bias shape torch.Size([256])
decoder.encoder.stages.4.0.convs.0.conv.weight shape torch.Size([320, 256, 3, 3, 3])
decoder.encoder.stages.4.0.convs.0.conv.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.norm.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.norm.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.0.weight shape torch.Size([320, 256, 3, 3, 3])
decoder.encoder.stages.4.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.4.0.convs.1.conv.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.norm.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.norm.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.4.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.0.conv.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.norm.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.norm.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.1.conv.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.norm.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.norm.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.stages.0.convs.0.conv.weight shape torch.Size([320, 640, 3, 3, 3])
decoder.stages.0.convs.0.conv.bias shape torch.Size([320])
decoder.stages.0.convs.0.norm.weight shape torch.Size([320])
decoder.stages.0.convs.0.norm.bias shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.0.weight shape torch.Size([320, 640, 3, 3, 3])
decoder.stages.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.stages.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.stages.0.convs.1.conv.bias shape torch.Size([320])
decoder.stages.0.convs.1.norm.weight shape torch.Size([320])
decoder.stages.0.convs.1.norm.bias shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.stages.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.stages.1.convs.0.conv.weight shape torch.Size([256, 512, 3, 3, 3])
decoder.stages.1.convs.0.conv.bias shape torch.Size([256])
decoder.stages.1.convs.0.norm.weight shape torch.Size([256])
decoder.stages.1.convs.0.norm.bias shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.0.weight shape torch.Size([256, 512, 3, 3, 3])
decoder.stages.1.convs.0.all_modules.0.bias shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.1.weight shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.1.bias shape torch.Size([256])
decoder.stages.1.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.stages.1.convs.1.conv.bias shape torch.Size([256])
decoder.stages.1.convs.1.norm.weight shape torch.Size([256])
decoder.stages.1.convs.1.norm.bias shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.stages.1.convs.1.all_modules.0.bias shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.1.weight shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.1.bias shape torch.Size([256])
decoder.stages.2.convs.0.conv.weight shape torch.Size([128, 256, 3, 3, 3])
decoder.stages.2.convs.0.conv.bias shape torch.Size([128])
decoder.stages.2.convs.0.norm.weight shape torch.Size([128])
decoder.stages.2.convs.0.norm.bias shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.0.weight shape torch.Size([128, 256, 3, 3, 3])
decoder.stages.2.convs.0.all_modules.0.bias shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.1.weight shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.1.bias shape torch.Size([128])
decoder.stages.2.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.stages.2.convs.1.conv.bias shape torch.Size([128])
decoder.stages.2.convs.1.norm.weight shape torch.Size([128])
decoder.stages.2.convs.1.norm.bias shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.stages.2.convs.1.all_modules.0.bias shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.1.weight shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.1.bias shape torch.Size([128])
decoder.stages.3.convs.0.conv.weight shape torch.Size([64, 128, 3, 3, 3])
decoder.stages.3.convs.0.conv.bias shape torch.Size([64])
decoder.stages.3.convs.0.norm.weight shape torch.Size([64])
decoder.stages.3.convs.0.norm.bias shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.0.weight shape torch.Size([64, 128, 3, 3, 3])
decoder.stages.3.convs.0.all_modules.0.bias shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.1.weight shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.1.bias shape torch.Size([64])
decoder.stages.3.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.stages.3.convs.1.conv.bias shape torch.Size([64])
decoder.stages.3.convs.1.norm.weight shape torch.Size([64])
decoder.stages.3.convs.1.norm.bias shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.stages.3.convs.1.all_modules.0.bias shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.1.weight shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.1.bias shape torch.Size([64])
decoder.stages.4.convs.0.conv.weight shape torch.Size([32, 64, 3, 3, 3])
decoder.stages.4.convs.0.conv.bias shape torch.Size([32])
decoder.stages.4.convs.0.norm.weight shape torch.Size([32])
decoder.stages.4.convs.0.norm.bias shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.0.weight shape torch.Size([32, 64, 3, 3, 3])
decoder.stages.4.convs.0.all_modules.0.bias shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.1.weight shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.1.bias shape torch.Size([32])
decoder.stages.4.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.stages.4.convs.1.conv.bias shape torch.Size([32])
decoder.stages.4.convs.1.norm.weight shape torch.Size([32])
decoder.stages.4.convs.1.norm.bias shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.stages.4.convs.1.all_modules.0.bias shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.1.weight shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.1.bias shape torch.Size([32])
decoder.transpconvs.0.weight shape torch.Size([320, 320, 1, 2, 2])
decoder.transpconvs.0.bias shape torch.Size([320])
decoder.transpconvs.1.weight shape torch.Size([320, 256, 2, 2, 2])
decoder.transpconvs.1.bias shape torch.Size([256])
decoder.transpconvs.2.weight shape torch.Size([256, 128, 2, 2, 2])
decoder.transpconvs.2.bias shape torch.Size([128])
decoder.transpconvs.3.weight shape torch.Size([128, 64, 2, 2, 2])
decoder.transpconvs.3.bias shape torch.Size([64])
decoder.transpconvs.4.weight shape torch.Size([64, 32, 2, 2, 2])
decoder.transpconvs.4.bias shape torch.Size([32])
################### Done ###################
2024-12-20 17:52:40.565061: do_dummy_2d_data_aug: True
2024-12-20 17:52:40.565909: Using splits from existing split file: /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset308_Sohee_Calcium_OCT_CrossValidation/splits_final_20.json
2024-12-20 17:52:40.567054: The split file contains 5 splits.
2024-12-20 17:52:40.567849: Desired fold for training: 1
2024-12-20 17:52:40.568831: This split has 1 training and 7 validation cases.
################### Loading pretrained weights from file  /home/gridsan/nchutisilp/projects/OpenAI-CLIP/clip_pretrained_nnUNet.pt ###################
Below is the list of overlapping blocks in pretrained model and nnUNet architecture:
encoder.stages.0.0.convs.0.conv.weight shape torch.Size([32, 1, 3, 3, 3])
encoder.stages.0.0.convs.0.conv.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.norm.weight shape torch.Size([32])
encoder.stages.0.0.convs.0.norm.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.0.weight shape torch.Size([32, 1, 3, 3, 3])
encoder.stages.0.0.convs.0.all_modules.0.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.1.weight shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.1.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
encoder.stages.0.0.convs.1.conv.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.norm.weight shape torch.Size([32])
encoder.stages.0.0.convs.1.norm.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
encoder.stages.0.0.convs.1.all_modules.0.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.1.weight shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.1.bias shape torch.Size([32])
encoder.stages.1.0.convs.0.conv.weight shape torch.Size([64, 32, 3, 3, 3])
encoder.stages.1.0.convs.0.conv.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.norm.weight shape torch.Size([64])
encoder.stages.1.0.convs.0.norm.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.0.weight shape torch.Size([64, 32, 3, 3, 3])
encoder.stages.1.0.convs.0.all_modules.0.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.1.weight shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.1.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
encoder.stages.1.0.convs.1.conv.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.norm.weight shape torch.Size([64])
encoder.stages.1.0.convs.1.norm.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
encoder.stages.1.0.convs.1.all_modules.0.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.1.weight shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.1.bias shape torch.Size([64])
encoder.stages.2.0.convs.0.conv.weight shape torch.Size([128, 64, 3, 3, 3])
encoder.stages.2.0.convs.0.conv.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.norm.weight shape torch.Size([128])
encoder.stages.2.0.convs.0.norm.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.0.weight shape torch.Size([128, 64, 3, 3, 3])
encoder.stages.2.0.convs.0.all_modules.0.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.1.weight shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.1.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
encoder.stages.2.0.convs.1.conv.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.norm.weight shape torch.Size([128])
encoder.stages.2.0.convs.1.norm.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
encoder.stages.2.0.convs.1.all_modules.0.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.1.weight shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.1.bias shape torch.Size([128])
encoder.stages.3.0.convs.0.conv.weight shape torch.Size([256, 128, 3, 3, 3])
encoder.stages.3.0.convs.0.conv.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.norm.weight shape torch.Size([256])
encoder.stages.3.0.convs.0.norm.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.0.weight shape torch.Size([256, 128, 3, 3, 3])
encoder.stages.3.0.convs.0.all_modules.0.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.1.weight shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.1.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
encoder.stages.3.0.convs.1.conv.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.norm.weight shape torch.Size([256])
encoder.stages.3.0.convs.1.norm.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
encoder.stages.3.0.convs.1.all_modules.0.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.1.weight shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.1.bias shape torch.Size([256])
encoder.stages.4.0.convs.0.conv.weight shape torch.Size([320, 256, 3, 3, 3])
encoder.stages.4.0.convs.0.conv.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.norm.weight shape torch.Size([320])
encoder.stages.4.0.convs.0.norm.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.0.weight shape torch.Size([320, 256, 3, 3, 3])
encoder.stages.4.0.convs.0.all_modules.0.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.1.weight shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.1.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.4.0.convs.1.conv.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.norm.weight shape torch.Size([320])
encoder.stages.4.0.convs.1.norm.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.4.0.convs.1.all_modules.0.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.1.weight shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.1.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.0.conv.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.norm.weight shape torch.Size([320])
encoder.stages.5.0.convs.0.norm.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.0.all_modules.0.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.1.weight shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.1.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.1.conv.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.norm.weight shape torch.Size([320])
encoder.stages.5.0.convs.1.norm.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.1.all_modules.0.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.1.weight shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.0.0.convs.0.conv.weight shape torch.Size([32, 1, 3, 3, 3])
decoder.encoder.stages.0.0.convs.0.conv.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.norm.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.norm.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.0.weight shape torch.Size([32, 1, 3, 3, 3])
decoder.encoder.stages.0.0.convs.0.all_modules.0.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.1.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.1.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.encoder.stages.0.0.convs.1.conv.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.norm.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.norm.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.encoder.stages.0.0.convs.1.all_modules.0.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.1.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.1.bias shape torch.Size([32])
decoder.encoder.stages.1.0.convs.0.conv.weight shape torch.Size([64, 32, 3, 3, 3])
decoder.encoder.stages.1.0.convs.0.conv.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.norm.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.norm.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.0.weight shape torch.Size([64, 32, 3, 3, 3])
decoder.encoder.stages.1.0.convs.0.all_modules.0.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.1.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.1.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.encoder.stages.1.0.convs.1.conv.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.norm.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.norm.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.encoder.stages.1.0.convs.1.all_modules.0.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.1.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.1.bias shape torch.Size([64])
decoder.encoder.stages.2.0.convs.0.conv.weight shape torch.Size([128, 64, 3, 3, 3])
decoder.encoder.stages.2.0.convs.0.conv.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.norm.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.norm.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.0.weight shape torch.Size([128, 64, 3, 3, 3])
decoder.encoder.stages.2.0.convs.0.all_modules.0.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.1.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.1.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.encoder.stages.2.0.convs.1.conv.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.norm.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.norm.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.encoder.stages.2.0.convs.1.all_modules.0.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.1.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.1.bias shape torch.Size([128])
decoder.encoder.stages.3.0.convs.0.conv.weight shape torch.Size([256, 128, 3, 3, 3])
decoder.encoder.stages.3.0.convs.0.conv.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.norm.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.norm.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.0.weight shape torch.Size([256, 128, 3, 3, 3])
decoder.encoder.stages.3.0.convs.0.all_modules.0.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.1.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.1.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.encoder.stages.3.0.convs.1.conv.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.norm.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.norm.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.encoder.stages.3.0.convs.1.all_modules.0.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.1.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.1.bias shape torch.Size([256])
decoder.encoder.stages.4.0.convs.0.conv.weight shape torch.Size([320, 256, 3, 3, 3])
decoder.encoder.stages.4.0.convs.0.conv.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.norm.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.norm.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.0.weight shape torch.Size([320, 256, 3, 3, 3])
decoder.encoder.stages.4.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.4.0.convs.1.conv.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.norm.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.norm.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.4.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.0.conv.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.norm.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.norm.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.1.conv.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.norm.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.norm.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.stages.0.convs.0.conv.weight shape torch.Size([320, 640, 3, 3, 3])
decoder.stages.0.convs.0.conv.bias shape torch.Size([320])
decoder.stages.0.convs.0.norm.weight shape torch.Size([320])
decoder.stages.0.convs.0.norm.bias shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.0.weight shape torch.Size([320, 640, 3, 3, 3])
decoder.stages.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.stages.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.stages.0.convs.1.conv.bias shape torch.Size([320])
decoder.stages.0.convs.1.norm.weight shape torch.Size([320])
decoder.stages.0.convs.1.norm.bias shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.stages.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.stages.1.convs.0.conv.weight shape torch.Size([256, 512, 3, 3, 3])
decoder.stages.1.convs.0.conv.bias shape torch.Size([256])
decoder.stages.1.convs.0.norm.weight shape torch.Size([256])
decoder.stages.1.convs.0.norm.bias shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.0.weight shape torch.Size([256, 512, 3, 3, 3])
decoder.stages.1.convs.0.all_modules.0.bias shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.1.weight shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.1.bias shape torch.Size([256])
decoder.stages.1.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.stages.1.convs.1.conv.bias shape torch.Size([256])
decoder.stages.1.convs.1.norm.weight shape torch.Size([256])
decoder.stages.1.convs.1.norm.bias shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.stages.1.convs.1.all_modules.0.bias shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.1.weight shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.1.bias shape torch.Size([256])
decoder.stages.2.convs.0.conv.weight shape torch.Size([128, 256, 3, 3, 3])
decoder.stages.2.convs.0.conv.bias shape torch.Size([128])
decoder.stages.2.convs.0.norm.weight shape torch.Size([128])
decoder.stages.2.convs.0.norm.bias shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.0.weight shape torch.Size([128, 256, 3, 3, 3])
decoder.stages.2.convs.0.all_modules.0.bias shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.1.weight shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.1.bias shape torch.Size([128])
decoder.stages.2.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.stages.2.convs.1.conv.bias shape torch.Size([128])
decoder.stages.2.convs.1.norm.weight shape torch.Size([128])
decoder.stages.2.convs.1.norm.bias shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.stages.2.convs.1.all_modules.0.bias shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.1.weight shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.1.bias shape torch.Size([128])
decoder.stages.3.convs.0.conv.weight shape torch.Size([64, 128, 3, 3, 3])
decoder.stages.3.convs.0.conv.bias shape torch.Size([64])
decoder.stages.3.convs.0.norm.weight shape torch.Size([64])
decoder.stages.3.convs.0.norm.bias shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.0.weight shape torch.Size([64, 128, 3, 3, 3])
decoder.stages.3.convs.0.all_modules.0.bias shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.1.weight shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.1.bias shape torch.Size([64])
decoder.stages.3.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.stages.3.convs.1.conv.bias shape torch.Size([64])
decoder.stages.3.convs.1.norm.weight shape torch.Size([64])
decoder.stages.3.convs.1.norm.bias shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.stages.3.convs.1.all_modules.0.bias shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.1.weight shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.1.bias shape torch.Size([64])
decoder.stages.4.convs.0.conv.weight shape torch.Size([32, 64, 3, 3, 3])
decoder.stages.4.convs.0.conv.bias shape torch.Size([32])
decoder.stages.4.convs.0.norm.weight shape torch.Size([32])
decoder.stages.4.convs.0.norm.bias shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.0.weight shape torch.Size([32, 64, 3, 3, 3])
decoder.stages.4.convs.0.all_modules.0.bias shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.1.weight shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.1.bias shape torch.Size([32])
decoder.stages.4.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.stages.4.convs.1.conv.bias shape torch.Size([32])
decoder.stages.4.convs.1.norm.weight shape torch.Size([32])
decoder.stages.4.convs.1.norm.bias shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.stages.4.convs.1.all_modules.0.bias shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.1.weight shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.1.bias shape torch.Size([32])
decoder.transpconvs.0.weight shape torch.Size([320, 320, 1, 2, 2])
decoder.transpconvs.0.bias shape torch.Size([320])
decoder.transpconvs.1.weight shape torch.Size([320, 256, 2, 2, 2])
decoder.transpconvs.1.bias shape torch.Size([256])
decoder.transpconvs.2.weight shape torch.Size([256, 128, 2, 2, 2])
decoder.transpconvs.2.bias shape torch.Size([128])
decoder.transpconvs.3.weight shape torch.Size([128, 64, 2, 2, 2])
decoder.transpconvs.3.bias shape torch.Size([64])
decoder.transpconvs.4.weight shape torch.Size([64, 32, 2, 2, 2])
decoder.transpconvs.4.bias shape torch.Size([32])
################### Done ###################
2024-12-20 17:52:40.521390: do_dummy_2d_data_aug: True
2024-12-20 17:52:40.546449: Using splits from existing split file: /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset308_Sohee_Calcium_OCT_CrossValidation/splits_final_20.json
2024-12-20 17:52:40.563769: The split file contains 5 splits.
2024-12-20 17:52:40.565257: Desired fold for training: 0
2024-12-20 17:52:40.566088: This split has 1 training and 7 validation cases.
using pin_memory on device 0
using pin_memory on device 0
using pin_memory on device 0

This is the configuration used by this training:
Configuration name: 3d_32x160x128_b10
 {'data_identifier': 'nnUNetPlans_3d_fullres', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 10, 'patch_size': [32, 160, 128], 'median_image_size_in_voxels': [375.0, 498.0, 498.0], 'spacing': [1.0, 1.0, 1.0], 'normalization_schemes': ['ZScoreNormalization'], 'use_mask_for_norm': [False], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'architecture': {'network_class_name': 'dynamic_network_architectures.architectures.unet.PlainConvUNet', 'arch_kwargs': {'n_stages': 6, 'features_per_stage': [32, 64, 128, 256, 320, 320], 'conv_op': 'torch.nn.modules.conv.Conv3d', 'kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]], 'strides': [[1, 1, 1], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [1, 2, 2]], 'n_conv_per_stage': [2, 2, 2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2, 2, 2], 'conv_bias': True, 'norm_op': 'torch.nn.modules.instancenorm.InstanceNorm3d', 'norm_op_kwargs': {'eps': 1e-05, 'affine': True}, 'dropout_op': None, 'dropout_op_kwargs': None, 'nonlin': 'torch.nn.LeakyReLU', 'nonlin_kwargs': {'inplace': True}, 'deep_supervision': True}, '_kw_requires_import': ['conv_op', 'norm_op', 'dropout_op', 'nonlin']}, 'batch_dice': True, 'inherits_from': '3d_fullres'} 

These are the global plan.json settings:
 {'dataset_name': 'Dataset308_Sohee_Calcium_OCT_CrossValidation', 'plans_name': 'nnUNetPlans', 'original_median_spacing_after_transp': [1.0, 1.0, 1.0], 'original_median_shape_after_transp': [375, 498, 498], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 0.4977976381778717, 'mean': 0.13507525622844696, 'median': 0.09599608182907104, 'min': 0.0, 'percentile_00_5': 0.014754901640117168, 'percentile_99_5': 0.4977976381778717, 'std': 0.12025152146816254}}} 

2024-12-20 17:53:02.840928: unpacking dataset...
using pin_memory on device 0

This is the configuration used by this training:
Configuration name: 3d_32x160x128_b10
 {'data_identifier': 'nnUNetPlans_3d_fullres', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 10, 'patch_size': [32, 160, 128], 'median_image_size_in_voxels': [375.0, 498.0, 498.0], 'spacing': [1.0, 1.0, 1.0], 'normalization_schemes': ['ZScoreNormalization'], 'use_mask_for_norm': [False], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'architecture': {'network_class_name': 'dynamic_network_architectures.architectures.unet.PlainConvUNet', 'arch_kwargs': {'n_stages': 6, 'features_per_stage': [32, 64, 128, 256, 320, 320], 'conv_op': 'torch.nn.modules.conv.Conv3d', 'kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]], 'strides': [[1, 1, 1], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [1, 2, 2]], 'n_conv_per_stage': [2, 2, 2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2, 2, 2], 'conv_bias': True, 'norm_op': 'torch.nn.modules.instancenorm.InstanceNorm3d', 'norm_op_kwargs': {'eps': 1e-05, 'affine': True}, 'dropout_op': None, 'dropout_op_kwargs': None, 'nonlin': 'torch.nn.LeakyReLU', 'nonlin_kwargs': {'inplace': True}, 'deep_supervision': True}, '_kw_requires_import': ['conv_op', 'norm_op', 'dropout_op', 'nonlin']}, 'batch_dice': True, 'inherits_from': '3d_fullres'} 

These are the global plan.json settings:
 {'dataset_name': 'Dataset308_Sohee_Calcium_OCT_CrossValidation', 'plans_name': 'nnUNetPlans', 'original_median_spacing_after_transp': [1.0, 1.0, 1.0], 'original_median_shape_after_transp': [375, 498, 498], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 0.4977976381778717, 'mean': 0.13507525622844696, 'median': 0.09599608182907104, 'min': 0.0, 'percentile_00_5': 0.014754901640117168, 'percentile_99_5': 0.4977976381778717, 'std': 0.12025152146816254}}} 

2024-12-20 17:53:03.958549: unpacking dataset...
2024-12-20 17:53:07.358257: unpacking done...
2024-12-20 17:53:07.620167: Unable to plot network architecture: nnUNet_compile is enabled!
2024-12-20 17:53:07.661265: 
2024-12-20 17:53:07.662591: Epoch 0
2024-12-20 17:53:07.663893: Current learning rate: 0.01
2024-12-20 17:58:11.272556: Validation loss improved from 1000.00000 to -0.07983! Patience: 0/50
2024-12-20 17:58:11.273590: train_loss -0.1135
2024-12-20 17:58:11.274580: val_loss -0.0798
2024-12-20 17:58:11.275313: Pseudo dice [0.4506]
2024-12-20 17:58:11.276084: Epoch time: 303.61 s
2024-12-20 17:58:11.276749: Yayy! New best EMA pseudo Dice: 0.4506
2024-12-20 17:58:12.851109: 
2024-12-20 17:58:12.852416: Epoch 1
2024-12-20 17:58:12.853329: Current learning rate: 0.00994
2024-12-20 18:02:10.291579: Validation loss improved from -0.07983 to -0.10659! Patience: 0/50
2024-12-20 18:02:10.292538: train_loss -0.343
2024-12-20 18:02:10.293366: val_loss -0.1066
2024-12-20 18:02:10.294212: Pseudo dice [0.4717]
2024-12-20 18:02:10.295036: Epoch time: 237.44 s
2024-12-20 18:02:10.295861: Yayy! New best EMA pseudo Dice: 0.4527
2024-12-20 18:02:12.176178: 
2024-12-20 18:02:12.177407: Epoch 2
2024-12-20 18:02:12.178252: Current learning rate: 0.00988
2024-12-20 18:06:11.678029: Validation loss did not improve from -0.10659. Patience: 1/50
2024-12-20 18:06:11.679721: train_loss -0.435
2024-12-20 18:06:11.680939: val_loss 0.0339
2024-12-20 18:06:11.681720: Pseudo dice [0.3915]
2024-12-20 18:06:11.682533: Epoch time: 239.5 s
2024-12-20 18:06:13.288692: 
2024-12-20 18:06:13.289918: Epoch 3
2024-12-20 18:06:13.290673: Current learning rate: 0.00982
2024-12-20 18:10:12.085521: Validation loss improved from -0.10659 to -0.25922! Patience: 1/50
2024-12-20 18:10:12.086598: train_loss -0.4894
2024-12-20 18:10:12.087749: val_loss -0.2592
2024-12-20 18:10:12.088973: Pseudo dice [0.5978]
2024-12-20 18:10:12.090080: Epoch time: 238.8 s
2024-12-20 18:10:12.091207: Yayy! New best EMA pseudo Dice: 0.4617
2024-12-20 18:10:13.978348: 
2024-12-20 18:10:13.979879: Epoch 4
2024-12-20 18:10:13.981035: Current learning rate: 0.00976
2024-12-20 18:14:17.002513: Validation loss did not improve from -0.25922. Patience: 1/50
2024-12-20 18:14:17.003463: train_loss -0.5374
2024-12-20 18:14:17.004289: val_loss -0.1828
2024-12-20 18:14:17.009506: Pseudo dice [0.5433]
2024-12-20 18:14:17.010200: Epoch time: 243.03 s
2024-12-20 18:14:17.384819: Yayy! New best EMA pseudo Dice: 0.4699
2024-12-20 18:14:19.412107: 
2024-12-20 18:14:19.413455: Epoch 5
2024-12-20 18:14:19.414561: Current learning rate: 0.0097
2024-12-20 18:18:19.397461: Validation loss did not improve from -0.25922. Patience: 2/50
2024-12-20 18:18:19.398414: train_loss -0.5865
2024-12-20 18:18:19.399474: val_loss -0.2009
2024-12-20 18:18:19.400297: Pseudo dice [0.5741]
2024-12-20 18:18:19.401101: Epoch time: 239.99 s
2024-12-20 18:18:19.401879: Yayy! New best EMA pseudo Dice: 0.4803
2024-12-20 18:18:21.217884: 
2024-12-20 18:18:21.219202: Epoch 6
2024-12-20 18:18:21.219978: Current learning rate: 0.00964
2024-12-20 18:22:27.888202: Validation loss did not improve from -0.25922. Patience: 3/50
2024-12-20 18:22:27.889188: train_loss -0.6165
2024-12-20 18:22:27.890236: val_loss -0.123
2024-12-20 18:22:27.891203: Pseudo dice [0.5498]
2024-12-20 18:22:27.892297: Epoch time: 246.67 s
2024-12-20 18:22:27.893196: Yayy! New best EMA pseudo Dice: 0.4872
2024-12-20 18:22:29.680979: 
2024-12-20 18:22:29.682395: Epoch 7
2024-12-20 18:22:29.683353: Current learning rate: 0.00958
2024-12-20 18:26:36.854001: Validation loss did not improve from -0.25922. Patience: 4/50
2024-12-20 18:26:36.855085: train_loss -0.6444
2024-12-20 18:26:36.855972: val_loss -0.1742
2024-12-20 18:26:36.856929: Pseudo dice [0.5604]
2024-12-20 18:26:36.857906: Epoch time: 247.18 s
2024-12-20 18:26:36.858724: Yayy! New best EMA pseudo Dice: 0.4946
2024-12-20 18:26:39.181495: 
2024-12-20 18:26:39.182913: Epoch 8
2024-12-20 18:26:39.183951: Current learning rate: 0.00952
2024-12-20 18:30:44.392057: Validation loss did not improve from -0.25922. Patience: 5/50
2024-12-20 18:30:44.393162: train_loss -0.6774
2024-12-20 18:30:44.394061: val_loss -0.1157
2024-12-20 18:30:44.395014: Pseudo dice [0.5102]
2024-12-20 18:30:44.396021: Epoch time: 245.21 s
2024-12-20 18:30:44.396863: Yayy! New best EMA pseudo Dice: 0.4961
2024-12-20 18:30:46.351904: 
2024-12-20 18:30:46.353577: Epoch 9
2024-12-20 18:30:46.354664: Current learning rate: 0.00946
2024-12-20 18:34:48.088408: Validation loss did not improve from -0.25922. Patience: 6/50
2024-12-20 18:34:48.089337: train_loss -0.6918
2024-12-20 18:34:48.090340: val_loss -0.2228
2024-12-20 18:34:48.091098: Pseudo dice [0.5897]
2024-12-20 18:34:48.091905: Epoch time: 241.74 s
2024-12-20 18:34:48.507560: Yayy! New best EMA pseudo Dice: 0.5055
2024-12-20 18:34:50.236333: 
2024-12-20 18:34:50.237546: Epoch 10
2024-12-20 18:34:50.238496: Current learning rate: 0.0094
2024-12-20 18:38:08.171669: Validation loss did not improve from -0.25922. Patience: 7/50
2024-12-20 18:38:08.172504: train_loss -0.7097
2024-12-20 18:38:08.173325: val_loss -0.1638
2024-12-20 18:38:08.174051: Pseudo dice [0.5794]
2024-12-20 18:38:08.174783: Epoch time: 197.94 s
2024-12-20 18:38:08.175604: Yayy! New best EMA pseudo Dice: 0.5129
2024-12-20 18:38:09.991343: 
2024-12-20 18:38:09.992568: Epoch 11
2024-12-20 18:38:09.993339: Current learning rate: 0.00934
2024-12-20 18:41:54.958878: Validation loss did not improve from -0.25922. Patience: 8/50
2024-12-20 18:41:54.959607: train_loss -0.7189
2024-12-20 18:41:54.960505: val_loss -0.2121
2024-12-20 18:41:54.961317: Pseudo dice [0.5754]
2024-12-20 18:41:54.962233: Epoch time: 224.97 s
2024-12-20 18:41:54.962984: Yayy! New best EMA pseudo Dice: 0.5191
2024-12-20 18:41:56.786644: 
2024-12-20 18:41:56.787916: Epoch 12
2024-12-20 18:41:56.788816: Current learning rate: 0.00928
2024-12-20 18:45:32.039431: Validation loss did not improve from -0.25922. Patience: 9/50
2024-12-20 18:45:32.040484: train_loss -0.7291
2024-12-20 18:45:32.041286: val_loss -0.1905
2024-12-20 18:45:32.042099: Pseudo dice [0.5792]
2024-12-20 18:45:32.042925: Epoch time: 215.26 s
2024-12-20 18:45:32.043616: Yayy! New best EMA pseudo Dice: 0.5251
2024-12-20 18:45:33.827194: 
2024-12-20 18:45:33.828331: Epoch 13
2024-12-20 18:45:33.829118: Current learning rate: 0.00922
2024-12-20 18:48:57.075698: Validation loss did not improve from -0.25922. Patience: 10/50
2024-12-20 18:48:57.076689: train_loss -0.7362
2024-12-20 18:48:57.077446: val_loss -0.1385
2024-12-20 18:48:57.078139: Pseudo dice [0.5504]
2024-12-20 18:48:57.078881: Epoch time: 203.25 s
2024-12-20 18:48:57.079596: Yayy! New best EMA pseudo Dice: 0.5277
2024-12-20 18:48:58.913285: 
2024-12-20 18:48:58.914576: Epoch 14
2024-12-20 18:48:58.915427: Current learning rate: 0.00916
2024-12-20 18:52:38.527603: Validation loss did not improve from -0.25922. Patience: 11/50
2024-12-20 18:52:38.528669: train_loss -0.7464
2024-12-20 18:52:38.529599: val_loss -0.0955
2024-12-20 18:52:38.530560: Pseudo dice [0.5351]
2024-12-20 18:52:38.531403: Epoch time: 219.62 s
2024-12-20 18:52:38.979012: Yayy! New best EMA pseudo Dice: 0.5284
2024-12-20 18:52:40.801744: 
2024-12-20 18:52:40.803176: Epoch 15
2024-12-20 18:52:40.804161: Current learning rate: 0.0091
2024-12-20 18:56:21.990970: Validation loss did not improve from -0.25922. Patience: 12/50
2024-12-20 18:56:21.991941: train_loss -0.7564
2024-12-20 18:56:21.992913: val_loss -0.1525
2024-12-20 18:56:21.993795: Pseudo dice [0.5686]
2024-12-20 18:56:21.994678: Epoch time: 221.19 s
2024-12-20 18:56:21.995654: Yayy! New best EMA pseudo Dice: 0.5324
2024-12-20 18:56:23.850558: 
2024-12-20 18:56:23.852551: Epoch 16
2024-12-20 18:56:23.853649: Current learning rate: 0.00903
2024-12-20 18:59:51.655326: Validation loss did not improve from -0.25922. Patience: 13/50
2024-12-20 18:59:51.656864: train_loss -0.762
2024-12-20 18:59:51.657902: val_loss -0.1259
2024-12-20 18:59:51.658715: Pseudo dice [0.5454]
2024-12-20 18:59:51.659478: Epoch time: 207.81 s
2024-12-20 18:59:51.660134: Yayy! New best EMA pseudo Dice: 0.5337
2024-12-20 18:59:53.552108: 
2024-12-20 18:59:53.553623: Epoch 17
2024-12-20 18:59:53.554494: Current learning rate: 0.00897
2024-12-20 19:03:38.953060: Validation loss did not improve from -0.25922. Patience: 14/50
2024-12-20 19:03:38.953962: train_loss -0.7606
2024-12-20 19:03:38.954704: val_loss -0.0021
2024-12-20 19:03:38.955365: Pseudo dice [0.4983]
2024-12-20 19:03:38.956272: Epoch time: 225.4 s
2024-12-20 19:03:40.423875: 
2024-12-20 19:03:40.425199: Epoch 18
2024-12-20 19:03:40.426017: Current learning rate: 0.00891
2024-12-20 19:06:58.497286: Validation loss did not improve from -0.25922. Patience: 15/50
2024-12-20 19:06:58.498273: train_loss -0.7682
2024-12-20 19:06:58.499331: val_loss -0.179
2024-12-20 19:06:58.500208: Pseudo dice [0.58]
2024-12-20 19:06:58.501197: Epoch time: 198.08 s
2024-12-20 19:06:58.502142: Yayy! New best EMA pseudo Dice: 0.5352
2024-12-20 19:07:01.160228: 
2024-12-20 19:07:01.161390: Epoch 19
2024-12-20 19:07:01.162245: Current learning rate: 0.00885
2024-12-20 19:10:40.728530: Validation loss did not improve from -0.25922. Patience: 16/50
2024-12-20 19:10:40.729657: train_loss -0.7722
2024-12-20 19:10:40.730539: val_loss -0.1258
2024-12-20 19:10:40.731327: Pseudo dice [0.5721]
2024-12-20 19:10:40.732209: Epoch time: 219.57 s
2024-12-20 19:10:41.097209: Yayy! New best EMA pseudo Dice: 0.5389
2024-12-20 19:10:43.028489: 
2024-12-20 19:10:43.030195: Epoch 20
2024-12-20 19:10:43.031389: Current learning rate: 0.00879
2024-12-20 19:14:23.644769: Validation loss did not improve from -0.25922. Patience: 17/50
2024-12-20 19:14:23.645841: train_loss -0.7744
2024-12-20 19:14:23.646721: val_loss -0.1494
2024-12-20 19:14:23.647441: Pseudo dice [0.5846]
2024-12-20 19:14:23.648150: Epoch time: 220.62 s
2024-12-20 19:14:23.648790: Yayy! New best EMA pseudo Dice: 0.5434
2024-12-20 19:14:25.458201: 
2024-12-20 19:14:25.459743: Epoch 21
2024-12-20 19:14:25.460547: Current learning rate: 0.00873
2024-12-20 19:17:44.349355: Validation loss did not improve from -0.25922. Patience: 18/50
2024-12-20 19:17:44.350370: train_loss -0.7862
2024-12-20 19:17:44.351360: val_loss -0.1569
2024-12-20 19:17:44.352089: Pseudo dice [0.5761]
2024-12-20 19:17:44.352906: Epoch time: 198.89 s
2024-12-20 19:17:44.353639: Yayy! New best EMA pseudo Dice: 0.5467
2024-12-20 19:17:46.184647: 
2024-12-20 19:17:46.186099: Epoch 22
2024-12-20 19:17:46.186852: Current learning rate: 0.00867
2024-12-20 19:21:30.998995: Validation loss did not improve from -0.25922. Patience: 19/50
2024-12-20 19:21:30.999994: train_loss -0.7933
2024-12-20 19:21:31.001029: val_loss -0.0145
2024-12-20 19:21:31.001844: Pseudo dice [0.5192]
2024-12-20 19:21:31.002720: Epoch time: 224.82 s
2024-12-20 19:21:32.444191: 
2024-12-20 19:21:32.445521: Epoch 23
2024-12-20 19:21:32.446363: Current learning rate: 0.00861
2024-12-20 19:25:05.365902: Validation loss did not improve from -0.25922. Patience: 20/50
2024-12-20 19:25:05.366797: train_loss -0.7954
2024-12-20 19:25:05.367659: val_loss -0.0811
2024-12-20 19:25:05.368489: Pseudo dice [0.5573]
2024-12-20 19:25:05.369173: Epoch time: 212.92 s
2024-12-20 19:25:06.720154: 
2024-12-20 19:25:06.721455: Epoch 24
2024-12-20 19:25:06.722257: Current learning rate: 0.00855
2024-12-20 19:28:31.292496: Validation loss did not improve from -0.25922. Patience: 21/50
2024-12-20 19:28:31.293339: train_loss -0.7956
2024-12-20 19:28:31.294111: val_loss -0.1466
2024-12-20 19:28:31.294765: Pseudo dice [0.5737]
2024-12-20 19:28:31.295466: Epoch time: 204.57 s
2024-12-20 19:28:31.724303: Yayy! New best EMA pseudo Dice: 0.5481
2024-12-20 19:28:33.614092: 
2024-12-20 19:28:33.615616: Epoch 25
2024-12-20 19:28:33.616656: Current learning rate: 0.00849
2024-12-20 19:32:15.245122: Validation loss did not improve from -0.25922. Patience: 22/50
2024-12-20 19:32:15.246848: train_loss -0.7996
2024-12-20 19:32:15.247970: val_loss -0.1306
2024-12-20 19:32:15.248787: Pseudo dice [0.5586]
2024-12-20 19:32:15.249631: Epoch time: 221.63 s
2024-12-20 19:32:15.250434: Yayy! New best EMA pseudo Dice: 0.5492
2024-12-20 19:32:17.136808: 
2024-12-20 19:32:17.138591: Epoch 26
2024-12-20 19:32:17.139490: Current learning rate: 0.00843
2024-12-20 19:35:57.115368: Validation loss did not improve from -0.25922. Patience: 23/50
2024-12-20 19:35:57.116140: train_loss -0.8053
2024-12-20 19:35:57.117104: val_loss -0.0972
2024-12-20 19:35:57.117985: Pseudo dice [0.5711]
2024-12-20 19:35:57.118802: Epoch time: 219.98 s
2024-12-20 19:35:57.119522: Yayy! New best EMA pseudo Dice: 0.5514
2024-12-20 19:35:58.926098: 
2024-12-20 19:35:58.927096: Epoch 27
2024-12-20 19:35:58.927803: Current learning rate: 0.00836
2024-12-20 19:39:29.713473: Validation loss did not improve from -0.25922. Patience: 24/50
2024-12-20 19:39:29.714579: train_loss -0.8097
2024-12-20 19:39:29.715518: val_loss -0.0216
2024-12-20 19:39:29.716341: Pseudo dice [0.5299]
2024-12-20 19:39:29.717193: Epoch time: 210.79 s
2024-12-20 19:39:31.147643: 
2024-12-20 19:39:31.149539: Epoch 28
2024-12-20 19:39:31.150520: Current learning rate: 0.0083
2024-12-20 19:43:14.427230: Validation loss did not improve from -0.25922. Patience: 25/50
2024-12-20 19:43:14.428274: train_loss -0.8073
2024-12-20 19:43:14.428985: val_loss -0.0118
2024-12-20 19:43:14.429620: Pseudo dice [0.5562]
2024-12-20 19:43:14.430304: Epoch time: 223.28 s
2024-12-20 19:43:15.801532: 
2024-12-20 19:43:15.803277: Epoch 29
2024-12-20 19:43:15.804103: Current learning rate: 0.00824
2024-12-20 19:46:44.912021: Validation loss did not improve from -0.25922. Patience: 26/50
2024-12-20 19:46:44.913617: train_loss -0.8115
2024-12-20 19:46:44.914433: val_loss -0.0352
2024-12-20 19:46:44.915108: Pseudo dice [0.5271]
2024-12-20 19:46:44.915906: Epoch time: 209.11 s
2024-12-20 19:46:47.103092: 
2024-12-20 19:46:47.104598: Epoch 30
2024-12-20 19:46:47.105458: Current learning rate: 0.00818
2024-12-20 19:50:11.540683: Validation loss did not improve from -0.25922. Patience: 27/50
2024-12-20 19:50:11.541690: train_loss -0.8183
2024-12-20 19:50:11.542428: val_loss -0.1213
2024-12-20 19:50:11.543125: Pseudo dice [0.5757]
2024-12-20 19:50:11.543941: Epoch time: 204.44 s
2024-12-20 19:50:12.917564: 
2024-12-20 19:50:12.918858: Epoch 31
2024-12-20 19:50:12.919764: Current learning rate: 0.00812
2024-12-20 19:53:51.008692: Validation loss did not improve from -0.25922. Patience: 28/50
2024-12-20 19:53:51.009756: train_loss -0.8183
2024-12-20 19:53:51.010579: val_loss -0.0589
2024-12-20 19:53:51.011449: Pseudo dice [0.5476]
2024-12-20 19:53:51.012227: Epoch time: 218.09 s
2024-12-20 19:53:52.448390: 
2024-12-20 19:53:52.449878: Epoch 32
2024-12-20 19:53:52.450818: Current learning rate: 0.00806
2024-12-20 19:57:18.865836: Validation loss did not improve from -0.25922. Patience: 29/50
2024-12-20 19:57:18.866879: train_loss -0.8171
2024-12-20 19:57:18.868257: val_loss -0.1386
2024-12-20 19:57:18.869380: Pseudo dice [0.5807]
2024-12-20 19:57:18.870427: Epoch time: 206.42 s
2024-12-20 19:57:18.871509: Yayy! New best EMA pseudo Dice: 0.5532
2024-12-20 19:57:20.717094: 
2024-12-20 19:57:20.718741: Epoch 33
2024-12-20 19:57:20.719739: Current learning rate: 0.008
2024-12-20 20:00:46.835943: Validation loss did not improve from -0.25922. Patience: 30/50
2024-12-20 20:00:46.859182: train_loss -0.8245
2024-12-20 20:00:46.860119: val_loss -0.0895
2024-12-20 20:00:46.860953: Pseudo dice [0.5674]
2024-12-20 20:00:46.861769: Epoch time: 206.14 s
2024-12-20 20:00:46.862494: Yayy! New best EMA pseudo Dice: 0.5546
2024-12-20 20:00:48.712704: 
2024-12-20 20:00:48.713989: Epoch 34
2024-12-20 20:00:48.714761: Current learning rate: 0.00793
2024-12-20 20:04:27.713059: Validation loss did not improve from -0.25922. Patience: 31/50
2024-12-20 20:04:27.714208: train_loss -0.8215
2024-12-20 20:04:27.715389: val_loss -0.0063
2024-12-20 20:04:27.716146: Pseudo dice [0.534]
2024-12-20 20:04:27.716999: Epoch time: 219.0 s
2024-12-20 20:04:29.560035: 
2024-12-20 20:04:29.561371: Epoch 35
2024-12-20 20:04:29.562093: Current learning rate: 0.00787
2024-12-20 20:08:03.056041: Validation loss did not improve from -0.25922. Patience: 32/50
2024-12-20 20:08:03.056909: train_loss -0.8244
2024-12-20 20:08:03.057841: val_loss -0.0489
2024-12-20 20:08:03.058693: Pseudo dice [0.5573]
2024-12-20 20:08:03.059538: Epoch time: 213.5 s
2024-12-20 20:08:04.552368: 
2024-12-20 20:08:04.554036: Epoch 36
2024-12-20 20:08:04.554955: Current learning rate: 0.00781
2024-12-20 20:11:42.403000: Validation loss did not improve from -0.25922. Patience: 33/50
2024-12-20 20:11:42.404981: train_loss -0.8307
2024-12-20 20:11:42.406043: val_loss 0.0023
2024-12-20 20:11:42.406773: Pseudo dice [0.5403]
2024-12-20 20:11:42.407695: Epoch time: 217.85 s
2024-12-20 20:11:43.943778: 
2024-12-20 20:11:43.945227: Epoch 37
2024-12-20 20:11:43.946093: Current learning rate: 0.00775
2024-12-20 20:15:21.044488: Validation loss did not improve from -0.25922. Patience: 34/50
2024-12-20 20:15:21.047170: train_loss -0.8329
2024-12-20 20:15:21.048061: val_loss -0.0405
2024-12-20 20:15:21.048945: Pseudo dice [0.5304]
2024-12-20 20:15:21.049762: Epoch time: 217.1 s
2024-12-20 20:15:22.507135: 
2024-12-20 20:15:22.508657: Epoch 38
2024-12-20 20:15:22.509587: Current learning rate: 0.00769
2024-12-20 20:18:41.316086: Validation loss did not improve from -0.25922. Patience: 35/50
2024-12-20 20:18:41.317283: train_loss -0.8298
2024-12-20 20:18:41.318293: val_loss 0.0177
2024-12-20 20:18:41.319361: Pseudo dice [0.5277]
2024-12-20 20:18:41.320246: Epoch time: 198.81 s
2024-12-20 20:18:42.763928: 
2024-12-20 20:18:42.765351: Epoch 39
2024-12-20 20:18:42.766279: Current learning rate: 0.00763
2024-12-20 20:22:20.864796: Validation loss did not improve from -0.25922. Patience: 36/50
2024-12-20 20:22:20.865866: train_loss -0.8313
2024-12-20 20:22:20.866678: val_loss -0.0287
2024-12-20 20:22:20.867392: Pseudo dice [0.554]
2024-12-20 20:22:20.868158: Epoch time: 218.1 s
2024-12-20 20:22:22.725492: 
2024-12-20 20:22:22.726700: Epoch 40
2024-12-20 20:22:22.727450: Current learning rate: 0.00756
2024-12-20 20:25:57.534022: Validation loss did not improve from -0.25922. Patience: 37/50
2024-12-20 20:25:57.535059: train_loss -0.8336
2024-12-20 20:25:57.535935: val_loss -0.0417
2024-12-20 20:25:57.536623: Pseudo dice [0.5656]
2024-12-20 20:25:57.537298: Epoch time: 214.81 s
2024-12-20 20:25:59.505277: 
2024-12-20 20:25:59.506709: Epoch 41
2024-12-20 20:25:59.507720: Current learning rate: 0.0075
2024-12-20 20:29:38.273599: Validation loss did not improve from -0.25922. Patience: 38/50
2024-12-20 20:29:38.274668: train_loss -0.8355
2024-12-20 20:29:38.275752: val_loss -0.0304
2024-12-20 20:29:38.276869: Pseudo dice [0.5559]
2024-12-20 20:29:38.277709: Epoch time: 218.77 s
2024-12-20 20:29:39.624178: 
2024-12-20 20:29:39.625664: Epoch 42
2024-12-20 20:29:39.626685: Current learning rate: 0.00744
2024-12-20 20:33:27.473506: Validation loss did not improve from -0.25922. Patience: 39/50
2024-12-20 20:33:27.474716: train_loss -0.8355
2024-12-20 20:33:27.475664: val_loss -0.0841
2024-12-20 20:33:27.476521: Pseudo dice [0.5779]
2024-12-20 20:33:27.477472: Epoch time: 227.85 s
2024-12-20 20:33:28.960264: 
2024-12-20 20:33:28.961711: Epoch 43
2024-12-20 20:33:28.962573: Current learning rate: 0.00738
2024-12-20 20:37:12.610567: Validation loss did not improve from -0.25922. Patience: 40/50
2024-12-20 20:37:12.611660: train_loss -0.8404
2024-12-20 20:37:12.612483: val_loss -0.0125
2024-12-20 20:37:12.613226: Pseudo dice [0.5383]
2024-12-20 20:37:12.613983: Epoch time: 223.65 s
2024-12-20 20:37:14.013884: 
2024-12-20 20:37:14.015250: Epoch 44
2024-12-20 20:37:14.016105: Current learning rate: 0.00732
2024-12-20 20:40:58.712876: Validation loss did not improve from -0.25922. Patience: 41/50
2024-12-20 20:40:58.713963: train_loss -0.8416
2024-12-20 20:40:58.714813: val_loss -0.0269
2024-12-20 20:40:58.715528: Pseudo dice [0.5609]
2024-12-20 20:40:58.716221: Epoch time: 224.7 s
2024-12-20 20:41:00.542011: 
2024-12-20 20:41:00.543080: Epoch 45
2024-12-20 20:41:00.543839: Current learning rate: 0.00725
2024-12-20 20:44:35.932907: Validation loss did not improve from -0.25922. Patience: 42/50
2024-12-20 20:44:35.933909: train_loss -0.8391
2024-12-20 20:44:35.934717: val_loss -0.0035
2024-12-20 20:44:35.935405: Pseudo dice [0.5552]
2024-12-20 20:44:35.936215: Epoch time: 215.39 s
2024-12-20 20:44:37.303145: 
2024-12-20 20:44:37.304522: Epoch 46
2024-12-20 20:44:37.305323: Current learning rate: 0.00719
2024-12-20 20:48:28.327894: Validation loss did not improve from -0.25922. Patience: 43/50
2024-12-20 20:48:28.328906: train_loss -0.8429
2024-12-20 20:48:28.329846: val_loss -0.0097
2024-12-20 20:48:28.330572: Pseudo dice [0.5411]
2024-12-20 20:48:28.331269: Epoch time: 231.03 s
2024-12-20 20:48:29.665429: 
2024-12-20 20:48:29.666792: Epoch 47
2024-12-20 20:48:29.667711: Current learning rate: 0.00713
2024-12-20 20:52:09.237662: Validation loss did not improve from -0.25922. Patience: 44/50
2024-12-20 20:52:09.238687: train_loss -0.8447
2024-12-20 20:52:09.239554: val_loss -0.0394
2024-12-20 20:52:09.240484: Pseudo dice [0.5605]
2024-12-20 20:52:09.241190: Epoch time: 219.57 s
2024-12-20 20:52:10.662750: 
2024-12-20 20:52:10.664064: Epoch 48
2024-12-20 20:52:10.664906: Current learning rate: 0.00707
2024-12-20 20:55:53.346976: Validation loss did not improve from -0.25922. Patience: 45/50
2024-12-20 20:55:53.347947: train_loss -0.8465
2024-12-20 20:55:53.348734: val_loss 0.0283
2024-12-20 20:55:53.349410: Pseudo dice [0.5486]
2024-12-20 20:55:53.350139: Epoch time: 222.69 s
2024-12-20 20:55:54.723576: 
2024-12-20 20:55:54.725082: Epoch 49
2024-12-20 20:55:54.725841: Current learning rate: 0.007
2024-12-20 20:59:33.926960: Validation loss did not improve from -0.25922. Patience: 46/50
2024-12-20 20:59:33.928998: train_loss -0.8456
2024-12-20 20:59:33.930258: val_loss -0.0321
2024-12-20 20:59:33.931144: Pseudo dice [0.5599]
2024-12-20 20:59:33.932045: Epoch time: 219.21 s
2024-12-20 20:59:35.773606: 
2024-12-20 20:59:35.775007: Epoch 50
2024-12-20 20:59:35.776080: Current learning rate: 0.00694
2024-12-20 21:03:00.524475: Validation loss did not improve from -0.25922. Patience: 47/50
2024-12-20 21:03:00.525587: train_loss -0.8489
2024-12-20 21:03:00.526551: val_loss -0.0005
2024-12-20 21:03:00.527405: Pseudo dice [0.556]
2024-12-20 21:03:00.528262: Epoch time: 204.75 s
2024-12-20 21:03:02.010856: 
2024-12-20 21:03:02.012249: Epoch 51
2024-12-20 21:03:02.013175: Current learning rate: 0.00688
2024-12-20 21:06:51.609416: Validation loss did not improve from -0.25922. Patience: 48/50
2024-12-20 21:06:51.611432: train_loss -0.8473
2024-12-20 21:06:51.613644: val_loss -0.001
2024-12-20 21:06:51.614881: Pseudo dice [0.5556]
2024-12-20 21:06:51.616435: Epoch time: 229.6 s
2024-12-20 21:06:53.697907: 
2024-12-20 21:06:53.699490: Epoch 52
2024-12-20 21:06:53.700574: Current learning rate: 0.00682
2024-12-20 21:10:47.411034: Validation loss did not improve from -0.25922. Patience: 49/50
2024-12-20 21:10:47.412440: train_loss -0.8485
2024-12-20 21:10:47.413545: val_loss 0.0908
2024-12-20 21:10:47.414334: Pseudo dice [0.5202]
2024-12-20 21:10:47.415229: Epoch time: 233.72 s
2024-12-20 21:10:48.839907: 
2024-12-20 21:10:48.841230: Epoch 53
2024-12-20 21:10:48.842042: Current learning rate: 0.00675
2024-12-20 21:14:36.496419: Validation loss did not improve from -0.25922. Patience: 50/50
2024-12-20 21:14:36.497507: train_loss -0.8508
2024-12-20 21:14:36.498269: val_loss -0.0953
2024-12-20 21:14:36.498996: Pseudo dice [0.5737]
2024-12-20 21:14:36.499792: Epoch time: 227.66 s
2024-12-20 21:14:37.883204: 
2024-12-20 21:14:37.884538: Epoch 54
2024-12-20 21:14:37.885352: Current learning rate: 0.00669
2024-12-20 21:18:08.172707: Validation loss did not improve from -0.25922. Patience: 51/50
2024-12-20 21:18:08.173759: train_loss -0.8534
2024-12-20 21:18:08.174567: val_loss 0.0185
2024-12-20 21:18:08.175274: Pseudo dice [0.5359]
2024-12-20 21:18:08.175970: Epoch time: 210.29 s
2024-12-20 21:18:09.949415: 
2024-12-20 21:18:09.950784: Epoch 55
2024-12-20 21:18:09.951580: Current learning rate: 0.00663
2024-12-20 21:22:05.600476: Validation loss did not improve from -0.25922. Patience: 52/50
2024-12-20 21:22:05.601554: train_loss -0.8544
2024-12-20 21:22:05.602670: val_loss 0.0228
2024-12-20 21:22:05.603738: Pseudo dice [0.5412]
2024-12-20 21:22:05.604620: Epoch time: 235.65 s
2024-12-20 21:22:07.015552: 
2024-12-20 21:22:07.017053: Epoch 56
2024-12-20 21:22:07.018117: Current learning rate: 0.00657
2024-12-20 21:26:01.551389: Validation loss did not improve from -0.25922. Patience: 53/50
2024-12-20 21:26:01.552383: train_loss -0.856
2024-12-20 21:26:01.553205: val_loss -0.0142
2024-12-20 21:26:01.553999: Pseudo dice [0.5521]
2024-12-20 21:26:01.554733: Epoch time: 234.54 s
2024-12-20 21:26:02.987371: 
2024-12-20 21:26:02.988480: Epoch 57
2024-12-20 21:26:02.989493: Current learning rate: 0.0065
2024-12-20 21:29:46.576835: Validation loss did not improve from -0.25922. Patience: 54/50
2024-12-20 21:29:46.577795: train_loss -0.856
2024-12-20 21:29:46.578799: val_loss -0.0084
2024-12-20 21:29:46.579662: Pseudo dice [0.5613]
2024-12-20 21:29:46.580444: Epoch time: 223.59 s
2024-12-20 21:29:47.960052: 
2024-12-20 21:29:47.961202: Epoch 58
2024-12-20 21:29:47.962199: Current learning rate: 0.00644
2024-12-20 21:33:40.092242: Validation loss did not improve from -0.25922. Patience: 55/50
2024-12-20 21:33:40.093325: train_loss -0.8587
2024-12-20 21:33:40.094177: val_loss 0.058
2024-12-20 21:33:40.094957: Pseudo dice [0.5263]
2024-12-20 21:33:40.095746: Epoch time: 232.13 s
2024-12-20 21:33:41.534883: 
2024-12-20 21:33:41.536207: Epoch 59
2024-12-20 21:33:41.537315: Current learning rate: 0.00638
2024-12-20 21:37:34.706525: Validation loss did not improve from -0.25922. Patience: 56/50
2024-12-20 21:37:34.707369: train_loss -0.8602
2024-12-20 21:37:34.708129: val_loss -0.0144
2024-12-20 21:37:34.708790: Pseudo dice [0.5557]
2024-12-20 21:37:34.709569: Epoch time: 233.17 s
2024-12-20 21:37:36.527686: 
2024-12-20 21:37:36.529135: Epoch 60
2024-12-20 21:37:36.529965: Current learning rate: 0.00631
2024-12-20 21:41:14.090140: Validation loss did not improve from -0.25922. Patience: 57/50
2024-12-20 21:41:14.091297: train_loss -0.8599
2024-12-20 21:41:14.092237: val_loss 0.0633
2024-12-20 21:41:14.092973: Pseudo dice [0.5374]
2024-12-20 21:41:14.093700: Epoch time: 217.56 s
2024-12-20 21:41:15.502256: 
2024-12-20 21:41:15.503560: Epoch 61
2024-12-20 21:41:15.504297: Current learning rate: 0.00625
2024-12-20 21:44:56.829168: Validation loss did not improve from -0.25922. Patience: 58/50
2024-12-20 21:44:56.830376: train_loss -0.8606
2024-12-20 21:44:56.831303: val_loss 0.0019
2024-12-20 21:44:56.832013: Pseudo dice [0.5578]
2024-12-20 21:44:56.832767: Epoch time: 221.33 s
2024-12-20 21:44:58.269700: 
2024-12-20 21:44:58.270825: Epoch 62
2024-12-20 21:44:58.271609: Current learning rate: 0.00619
2024-12-20 21:48:31.525797: Validation loss did not improve from -0.25922. Patience: 59/50
2024-12-20 21:48:31.527422: train_loss -0.8622
2024-12-20 21:48:31.528465: val_loss 0.1469
2024-12-20 21:48:31.529243: Pseudo dice [0.4841]
2024-12-20 21:48:31.529928: Epoch time: 213.26 s
2024-12-20 21:48:33.474042: 
2024-12-20 21:48:33.475411: Epoch 63
2024-12-20 21:48:33.476191: Current learning rate: 0.00612
2024-12-20 21:51:53.048124: Validation loss did not improve from -0.25922. Patience: 60/50
2024-12-20 21:51:53.049685: train_loss -0.8627
2024-12-20 21:51:53.050733: val_loss -0.0619
2024-12-20 21:51:53.051567: Pseudo dice [0.554]
2024-12-20 21:51:53.052310: Epoch time: 199.58 s
2024-12-20 21:51:54.491626: 
2024-12-20 21:51:54.492915: Epoch 64
2024-12-20 21:51:54.493754: Current learning rate: 0.00606
2024-12-20 21:55:46.214813: Validation loss did not improve from -0.25922. Patience: 61/50
2024-12-20 21:55:46.215917: train_loss -0.8624
2024-12-20 21:55:46.216738: val_loss 0.0778
2024-12-20 21:55:46.217420: Pseudo dice [0.5215]
2024-12-20 21:55:46.218187: Epoch time: 231.73 s
2024-12-20 21:55:48.075485: 
2024-12-20 21:55:48.076974: Epoch 65
2024-12-20 21:55:48.077809: Current learning rate: 0.006
2024-12-20 21:59:03.756269: Validation loss did not improve from -0.25922. Patience: 62/50
2024-12-20 21:59:03.757355: train_loss -0.8647
2024-12-20 21:59:03.758175: val_loss 0.0998
2024-12-20 21:59:03.759010: Pseudo dice [0.5076]
2024-12-20 21:59:03.759818: Epoch time: 195.68 s
2024-12-20 21:59:05.165592: 
2024-12-20 21:59:05.167054: Epoch 66
2024-12-20 21:59:05.167791: Current learning rate: 0.00593
2024-12-20 22:02:50.479551: Validation loss did not improve from -0.25922. Patience: 63/50
2024-12-20 22:02:50.480556: train_loss -0.8667
2024-12-20 22:02:50.481409: val_loss 0.0898
2024-12-20 22:02:50.482255: Pseudo dice [0.5265]
2024-12-20 22:02:50.483254: Epoch time: 225.32 s
2024-12-20 22:02:51.874629: 
2024-12-20 22:02:51.875812: Epoch 67
2024-12-20 22:02:51.876600: Current learning rate: 0.00587
2024-12-20 22:06:30.634402: Validation loss did not improve from -0.25922. Patience: 64/50
2024-12-20 22:06:30.635349: train_loss -0.8645
2024-12-20 22:06:30.636251: val_loss 0.1076
2024-12-20 22:06:30.637073: Pseudo dice [0.5245]
2024-12-20 22:06:30.637874: Epoch time: 218.76 s
2024-12-20 22:06:32.011884: 
2024-12-20 22:06:32.013219: Epoch 68
2024-12-20 22:06:32.014028: Current learning rate: 0.00581
2024-12-20 22:10:07.101489: Validation loss did not improve from -0.25922. Patience: 65/50
2024-12-20 22:10:07.102638: train_loss -0.8629
2024-12-20 22:10:07.103443: val_loss 0.0249
2024-12-20 22:10:07.104247: Pseudo dice [0.5586]
2024-12-20 22:10:07.105008: Epoch time: 215.09 s
2024-12-20 22:10:08.565543: 
2024-12-20 22:10:08.567481: Epoch 69
2024-12-20 22:10:08.568319: Current learning rate: 0.00574
2024-12-20 22:13:54.166908: Validation loss did not improve from -0.25922. Patience: 66/50
2024-12-20 22:13:54.168883: train_loss -0.8643
2024-12-20 22:13:54.170811: val_loss 0.0601
2024-12-20 22:13:54.171790: Pseudo dice [0.5363]
2024-12-20 22:13:54.173002: Epoch time: 225.6 s
2024-12-20 22:13:56.071092: 
2024-12-20 22:13:56.072286: Epoch 70
2024-12-20 22:13:56.073234: Current learning rate: 0.00568
2024-12-20 22:17:35.682286: Validation loss did not improve from -0.25922. Patience: 67/50
2024-12-20 22:17:35.683861: train_loss -0.8662
2024-12-20 22:17:35.684732: val_loss 0.0722
2024-12-20 22:17:35.685503: Pseudo dice [0.5349]
2024-12-20 22:17:35.686311: Epoch time: 219.61 s
2024-12-20 22:17:37.104306: 
2024-12-20 22:17:37.105746: Epoch 71
2024-12-20 22:17:37.106694: Current learning rate: 0.00562
2024-12-20 22:20:55.859595: Validation loss did not improve from -0.25922. Patience: 68/50
2024-12-20 22:20:55.860504: train_loss -0.8725
2024-12-20 22:20:55.861355: val_loss 0.0031
2024-12-20 22:20:55.862220: Pseudo dice [0.5479]
2024-12-20 22:20:55.863162: Epoch time: 198.76 s
2024-12-20 22:20:57.263943: 
2024-12-20 22:20:57.265342: Epoch 72
2024-12-20 22:20:57.266117: Current learning rate: 0.00555
2024-12-20 22:24:37.868482: Validation loss did not improve from -0.25922. Patience: 69/50
2024-12-20 22:24:37.869246: train_loss -0.8736
2024-12-20 22:24:37.870015: val_loss -0.0121
2024-12-20 22:24:37.870757: Pseudo dice [0.5619]
2024-12-20 22:24:37.871399: Epoch time: 220.61 s
2024-12-20 22:24:39.340291: 
2024-12-20 22:24:39.341737: Epoch 73
2024-12-20 22:24:39.342620: Current learning rate: 0.00549
2024-12-20 22:28:29.361605: Validation loss did not improve from -0.25922. Patience: 70/50
2024-12-20 22:28:29.362831: train_loss -0.8698
2024-12-20 22:28:29.363698: val_loss 0.0438
2024-12-20 22:28:29.364591: Pseudo dice [0.535]
2024-12-20 22:28:29.365334: Epoch time: 230.02 s
2024-12-20 22:28:30.858317: 
2024-12-20 22:28:30.859634: Epoch 74
2024-12-20 22:28:30.860435: Current learning rate: 0.00542
2024-12-20 22:32:04.626100: Validation loss did not improve from -0.25922. Patience: 71/50
2024-12-20 22:32:04.627254: train_loss -0.8702
2024-12-20 22:32:04.628197: val_loss 0.1656
2024-12-20 22:32:04.628856: Pseudo dice [0.5368]
2024-12-20 22:32:04.629632: Epoch time: 213.77 s
2024-12-20 22:32:07.323085: 
2024-12-20 22:32:07.324457: Epoch 75
2024-12-20 22:32:07.325378: Current learning rate: 0.00536
2024-12-20 22:35:52.400808: Validation loss did not improve from -0.25922. Patience: 72/50
2024-12-20 22:35:52.401964: train_loss -0.8732
2024-12-20 22:35:52.402753: val_loss 0.0382
2024-12-20 22:35:52.403476: Pseudo dice [0.5515]
2024-12-20 22:35:52.404272: Epoch time: 225.08 s
2024-12-20 22:35:53.945003: 
2024-12-20 22:35:53.946500: Epoch 76
2024-12-20 22:35:53.947350: Current learning rate: 0.00529
2024-12-20 22:39:59.632909: Validation loss did not improve from -0.25922. Patience: 73/50
2024-12-20 22:39:59.634150: train_loss -0.8731
2024-12-20 22:39:59.635161: val_loss -0.0074
2024-12-20 22:39:59.636085: Pseudo dice [0.5651]
2024-12-20 22:39:59.636961: Epoch time: 245.69 s
2024-12-20 22:40:01.154236: 
2024-12-20 22:40:01.155633: Epoch 77
2024-12-20 22:40:01.156416: Current learning rate: 0.00523
2024-12-20 22:43:55.204077: Validation loss did not improve from -0.25922. Patience: 74/50
2024-12-20 22:43:55.205016: train_loss -0.8731
2024-12-20 22:43:55.205863: val_loss 0.0251
2024-12-20 22:43:55.206654: Pseudo dice [0.5611]
2024-12-20 22:43:55.207326: Epoch time: 234.05 s
2024-12-20 22:43:56.672709: 
2024-12-20 22:43:56.674088: Epoch 78
2024-12-20 22:43:56.674901: Current learning rate: 0.00517
2024-12-20 22:47:46.037739: Validation loss did not improve from -0.25922. Patience: 75/50
2024-12-20 22:47:46.038749: train_loss -0.8753
2024-12-20 22:47:46.039717: val_loss 0.0899
2024-12-20 22:47:46.040751: Pseudo dice [0.5307]
2024-12-20 22:47:46.041747: Epoch time: 229.37 s
2024-12-20 22:47:47.527165: 
2024-12-20 22:47:47.528172: Epoch 79
2024-12-20 22:47:47.528865: Current learning rate: 0.0051
2024-12-20 22:51:39.153706: Validation loss did not improve from -0.25922. Patience: 76/50
2024-12-20 22:51:39.154733: train_loss -0.8746
2024-12-20 22:51:39.155798: val_loss 0.0388
2024-12-20 22:51:39.156724: Pseudo dice [0.5529]
2024-12-20 22:51:39.157590: Epoch time: 231.63 s
2024-12-20 22:51:41.076911: 
2024-12-20 22:51:41.078496: Epoch 80
2024-12-20 22:51:41.079468: Current learning rate: 0.00504
2024-12-20 22:55:27.613806: Validation loss did not improve from -0.25922. Patience: 77/50
2024-12-20 22:55:27.614962: train_loss -0.8784
2024-12-20 22:55:27.616040: val_loss 0.1247
2024-12-20 22:55:27.616903: Pseudo dice [0.5234]
2024-12-20 22:55:27.617732: Epoch time: 226.54 s
2024-12-20 22:55:29.092208: 
2024-12-20 22:55:29.093466: Epoch 81
2024-12-20 22:55:29.094191: Current learning rate: 0.00497
2024-12-20 22:59:14.232022: Validation loss did not improve from -0.25922. Patience: 78/50
2024-12-20 22:59:14.233187: train_loss -0.8769
2024-12-20 22:59:14.234101: val_loss 0.059
2024-12-20 22:59:14.234880: Pseudo dice [0.5439]
2024-12-20 22:59:14.235579: Epoch time: 225.14 s
2024-12-20 22:59:15.702497: 
2024-12-20 22:59:15.703950: Epoch 82
2024-12-20 22:59:15.704871: Current learning rate: 0.00491
2024-12-20 23:03:10.553906: Validation loss did not improve from -0.25922. Patience: 79/50
2024-12-20 23:03:10.554965: train_loss -0.879
2024-12-20 23:03:10.555968: val_loss 0.0618
2024-12-20 23:03:10.556825: Pseudo dice [0.532]
2024-12-20 23:03:10.557632: Epoch time: 234.85 s
2024-12-20 23:03:11.981947: 
2024-12-20 23:03:11.983334: Epoch 83
2024-12-20 23:03:11.984394: Current learning rate: 0.00484
2024-12-20 23:07:15.565278: Validation loss did not improve from -0.25922. Patience: 80/50
2024-12-20 23:07:15.567258: train_loss -0.8776
2024-12-20 23:07:15.568306: val_loss 0.139
2024-12-20 23:07:15.569076: Pseudo dice [0.5379]
2024-12-20 23:07:15.569920: Epoch time: 243.59 s
2024-12-20 23:07:16.938458: 
2024-12-20 23:07:16.940031: Epoch 84
2024-12-20 23:07:16.940957: Current learning rate: 0.00478
2024-12-20 23:11:03.314632: Validation loss did not improve from -0.25922. Patience: 81/50
2024-12-20 23:11:03.315412: train_loss -0.8789
2024-12-20 23:11:03.316201: val_loss 0.1241
2024-12-20 23:11:03.316961: Pseudo dice [0.5262]
2024-12-20 23:11:03.317674: Epoch time: 226.38 s
2024-12-20 23:11:05.494007: 
2024-12-20 23:11:05.494956: Epoch 85
2024-12-20 23:11:05.495965: Current learning rate: 0.00471
2024-12-20 23:14:52.575733: Validation loss did not improve from -0.25922. Patience: 82/50
2024-12-20 23:14:52.576693: train_loss -0.8841
2024-12-20 23:14:52.577889: val_loss 0.0923
2024-12-20 23:14:52.578867: Pseudo dice [0.5287]
2024-12-20 23:14:52.579932: Epoch time: 227.08 s
2024-12-20 23:14:53.923428: 
2024-12-20 23:14:53.924905: Epoch 86
2024-12-20 23:14:53.925931: Current learning rate: 0.00465
2024-12-20 23:18:42.436900: Validation loss did not improve from -0.25922. Patience: 83/50
2024-12-20 23:18:42.438781: train_loss -0.8808
2024-12-20 23:18:42.439858: val_loss 0.1284
2024-12-20 23:18:42.440656: Pseudo dice [0.5364]
2024-12-20 23:18:42.441553: Epoch time: 228.52 s
2024-12-20 23:18:43.856259: 
2024-12-20 23:18:43.857474: Epoch 87
2024-12-20 23:18:43.858243: Current learning rate: 0.00458
2024-12-20 23:22:34.943360: Validation loss did not improve from -0.25922. Patience: 84/50
2024-12-20 23:22:34.944492: train_loss -0.8821
2024-12-20 23:22:34.945395: val_loss 0.1388
2024-12-20 23:22:34.946082: Pseudo dice [0.5252]
2024-12-20 23:22:34.946910: Epoch time: 231.09 s
2024-12-20 23:22:36.280710: 
2024-12-20 23:22:36.282048: Epoch 88
2024-12-20 23:22:36.282794: Current learning rate: 0.00452
2024-12-20 23:25:44.310189: Validation loss did not improve from -0.25922. Patience: 85/50
2024-12-20 23:25:44.311243: train_loss -0.8825
2024-12-20 23:25:44.312090: val_loss 0.1201
2024-12-20 23:25:44.312873: Pseudo dice [0.5326]
2024-12-20 23:25:44.313566: Epoch time: 188.03 s
2024-12-20 23:25:45.731352: 
2024-12-20 23:25:45.732800: Epoch 89
2024-12-20 23:25:45.733555: Current learning rate: 0.00445
2024-12-20 23:29:19.475453: Validation loss did not improve from -0.25922. Patience: 86/50
2024-12-20 23:29:19.476494: train_loss -0.8832
2024-12-20 23:29:19.477431: val_loss 0.1987
2024-12-20 23:29:19.478138: Pseudo dice [0.5137]
2024-12-20 23:29:19.478914: Epoch time: 213.75 s
2024-12-20 23:29:21.224314: 
2024-12-20 23:29:21.225598: Epoch 90
2024-12-20 23:29:21.226457: Current learning rate: 0.00438
2024-12-20 23:33:01.208794: Validation loss did not improve from -0.25922. Patience: 87/50
2024-12-20 23:33:01.209934: train_loss -0.8804
2024-12-20 23:33:01.220245: val_loss 0.151
2024-12-20 23:33:01.221043: Pseudo dice [0.4883]
2024-12-20 23:33:01.221787: Epoch time: 219.99 s
2024-12-20 23:33:02.592482: 
2024-12-20 23:33:02.593911: Epoch 91
2024-12-20 23:33:02.594629: Current learning rate: 0.00432
2024-12-20 23:36:33.071900: Validation loss did not improve from -0.25922. Patience: 88/50
2024-12-20 23:36:33.073004: train_loss -0.8828
2024-12-20 23:36:33.074174: val_loss 0.0746
2024-12-20 23:36:33.075300: Pseudo dice [0.5314]
2024-12-20 23:36:33.076373: Epoch time: 210.48 s
2024-12-20 23:36:34.442874: 
2024-12-20 23:36:34.444042: Epoch 92
2024-12-20 23:36:34.444934: Current learning rate: 0.00425
2024-12-20 23:40:21.216853: Validation loss did not improve from -0.25922. Patience: 89/50
2024-12-20 23:40:21.217857: train_loss -0.8847
2024-12-20 23:40:21.218871: val_loss 0.1611
2024-12-20 23:40:21.219809: Pseudo dice [0.5144]
2024-12-20 23:40:21.220571: Epoch time: 226.78 s
2024-12-20 23:40:22.571262: 
2024-12-20 23:40:22.572699: Epoch 93
2024-12-20 23:40:22.573579: Current learning rate: 0.00419
2024-12-20 23:43:43.881899: Validation loss did not improve from -0.25922. Patience: 90/50
2024-12-20 23:43:43.882976: train_loss -0.8851
2024-12-20 23:43:43.883770: val_loss 0.1359
2024-12-20 23:43:43.884545: Pseudo dice [0.5223]
2024-12-20 23:43:43.885377: Epoch time: 201.31 s
2024-12-20 23:43:45.230108: 
2024-12-20 23:43:45.231556: Epoch 94
2024-12-20 23:43:45.232641: Current learning rate: 0.00412
2024-12-20 23:47:19.327905: Validation loss did not improve from -0.25922. Patience: 91/50
2024-12-20 23:47:19.328677: train_loss -0.8855
2024-12-20 23:47:19.329451: val_loss 0.1047
2024-12-20 23:47:19.330149: Pseudo dice [0.5388]
2024-12-20 23:47:19.330866: Epoch time: 214.1 s
2024-12-20 23:47:21.154455: 
2024-12-20 23:47:21.155833: Epoch 95
2024-12-20 23:47:21.156669: Current learning rate: 0.00405
2024-12-20 23:51:04.270801: Validation loss did not improve from -0.25922. Patience: 92/50
2024-12-20 23:51:04.271734: train_loss -0.8887
2024-12-20 23:51:04.272533: val_loss 0.1519
2024-12-20 23:51:04.273448: Pseudo dice [0.5298]
2024-12-20 23:51:04.274251: Epoch time: 223.12 s
2024-12-20 23:51:05.681495: 
2024-12-20 23:51:05.682734: Epoch 96
2024-12-20 23:51:05.683602: Current learning rate: 0.00399
2024-12-20 23:54:31.432026: Validation loss did not improve from -0.25922. Patience: 93/50
2024-12-20 23:54:31.433065: train_loss -0.8886
2024-12-20 23:54:31.434543: val_loss 0.1643
2024-12-20 23:54:31.435716: Pseudo dice [0.5151]
2024-12-20 23:54:31.436841: Epoch time: 205.75 s
2024-12-20 23:54:33.442668: 
2024-12-20 23:54:33.444188: Epoch 97
2024-12-20 23:54:33.445430: Current learning rate: 0.00392
2024-12-20 23:58:12.391286: Validation loss did not improve from -0.25922. Patience: 94/50
2024-12-20 23:58:12.392398: train_loss -0.8891
2024-12-20 23:58:12.393442: val_loss 0.0796
2024-12-20 23:58:12.394327: Pseudo dice [0.5382]
2024-12-20 23:58:12.395250: Epoch time: 218.95 s
2024-12-20 23:58:13.918301: 
2024-12-20 23:58:13.919785: Epoch 98
2024-12-20 23:58:13.920801: Current learning rate: 0.00385
2024-12-21 00:01:55.719599: Validation loss did not improve from -0.25922. Patience: 95/50
2024-12-21 00:01:55.720782: train_loss -0.8893
2024-12-21 00:01:55.722030: val_loss 0.0617
2024-12-21 00:01:55.723004: Pseudo dice [0.5427]
2024-12-21 00:01:55.723920: Epoch time: 221.8 s
2024-12-21 00:01:57.127330: 
2024-12-21 00:01:57.128815: Epoch 99
2024-12-21 00:01:57.129686: Current learning rate: 0.00379
2024-12-21 00:05:18.804850: Validation loss did not improve from -0.25922. Patience: 96/50
2024-12-21 00:05:18.805871: train_loss -0.8898
2024-12-21 00:05:18.806724: val_loss 0.0907
2024-12-21 00:05:18.807418: Pseudo dice [0.5286]
2024-12-21 00:05:18.808149: Epoch time: 201.68 s
2024-12-21 00:05:20.645737: 
2024-12-21 00:05:20.646809: Epoch 100
2024-12-21 00:05:20.647640: Current learning rate: 0.00372
2024-12-21 00:08:59.707937: Validation loss did not improve from -0.25922. Patience: 97/50
2024-12-21 00:08:59.708946: train_loss -0.8893
2024-12-21 00:08:59.709879: val_loss 0.0829
2024-12-21 00:08:59.710679: Pseudo dice [0.5407]
2024-12-21 00:08:59.711544: Epoch time: 219.06 s
2024-12-21 00:09:01.209854: 
2024-12-21 00:09:01.211315: Epoch 101
2024-12-21 00:09:01.212250: Current learning rate: 0.00365
2024-12-21 00:12:34.602186: Validation loss did not improve from -0.25922. Patience: 98/50
2024-12-21 00:12:34.603143: train_loss -0.8902
2024-12-21 00:12:34.603902: val_loss 0.1341
2024-12-21 00:12:34.604611: Pseudo dice [0.5463]
2024-12-21 00:12:34.605395: Epoch time: 213.39 s
2024-12-21 00:12:36.212821: 
2024-12-21 00:12:36.214065: Epoch 102
2024-12-21 00:12:36.214967: Current learning rate: 0.00359
2024-12-21 00:16:12.400063: Validation loss did not improve from -0.25922. Patience: 99/50
2024-12-21 00:16:12.401019: train_loss -0.8908
2024-12-21 00:16:12.402086: val_loss 0.1119
2024-12-21 00:16:12.402976: Pseudo dice [0.5482]
2024-12-21 00:16:12.403822: Epoch time: 216.19 s
2024-12-21 00:16:13.797693: 
2024-12-21 00:16:13.799097: Epoch 103
2024-12-21 00:16:13.800089: Current learning rate: 0.00352
2024-12-21 00:19:53.729146: Validation loss did not improve from -0.25922. Patience: 100/50
2024-12-21 00:19:53.730228: train_loss -0.8924
2024-12-21 00:19:53.731394: val_loss 0.1212
2024-12-21 00:19:53.732255: Pseudo dice [0.5354]
2024-12-21 00:19:53.733098: Epoch time: 219.93 s
2024-12-21 00:19:55.109119: 
2024-12-21 00:19:55.110655: Epoch 104
2024-12-21 00:19:55.111665: Current learning rate: 0.00345
2024-12-21 00:23:31.194400: Validation loss did not improve from -0.25922. Patience: 101/50
2024-12-21 00:23:31.195746: train_loss -0.8912
2024-12-21 00:23:31.196848: val_loss 0.0375
2024-12-21 00:23:31.197771: Pseudo dice [0.566]
2024-12-21 00:23:31.198782: Epoch time: 216.09 s
2024-12-21 00:23:32.999899: 
2024-12-21 00:23:33.001325: Epoch 105
2024-12-21 00:23:33.002302: Current learning rate: 0.00338
2024-12-21 00:27:11.644121: Validation loss did not improve from -0.25922. Patience: 102/50
2024-12-21 00:27:11.645329: train_loss -0.8939
2024-12-21 00:27:11.646283: val_loss 0.1165
2024-12-21 00:27:11.647042: Pseudo dice [0.555]
2024-12-21 00:27:11.647756: Epoch time: 218.65 s
2024-12-21 00:27:13.034164: 
2024-12-21 00:27:13.035628: Epoch 106
2024-12-21 00:27:13.036496: Current learning rate: 0.00332
2024-12-21 00:30:57.840087: Validation loss did not improve from -0.25922. Patience: 103/50
2024-12-21 00:30:57.840924: train_loss -0.8917
2024-12-21 00:30:57.841963: val_loss 0.1213
2024-12-21 00:30:57.842898: Pseudo dice [0.5441]
2024-12-21 00:30:57.843742: Epoch time: 224.81 s
2024-12-21 00:30:59.324736: 
2024-12-21 00:30:59.326111: Epoch 107
2024-12-21 00:30:59.327115: Current learning rate: 0.00325
2024-12-21 00:34:18.025464: Validation loss did not improve from -0.25922. Patience: 104/50
2024-12-21 00:34:18.026600: train_loss -0.8936
2024-12-21 00:34:18.027565: val_loss 0.1899
2024-12-21 00:34:18.028398: Pseudo dice [0.522]
2024-12-21 00:34:18.029175: Epoch time: 198.7 s
2024-12-21 00:34:19.991743: 
2024-12-21 00:34:19.993056: Epoch 108
2024-12-21 00:34:19.993910: Current learning rate: 0.00318
2024-12-21 00:37:58.651429: Validation loss did not improve from -0.25922. Patience: 105/50
2024-12-21 00:37:58.652478: train_loss -0.8953
2024-12-21 00:37:58.653508: val_loss 0.2331
2024-12-21 00:37:58.654420: Pseudo dice [0.4994]
2024-12-21 00:37:58.655248: Epoch time: 218.66 s
2024-12-21 00:38:00.088132: 
2024-12-21 00:38:00.089742: Epoch 109
2024-12-21 00:38:00.090664: Current learning rate: 0.00311
2024-12-21 00:41:36.529484: Validation loss did not improve from -0.25922. Patience: 106/50
2024-12-21 00:41:36.530607: train_loss -0.8966
2024-12-21 00:41:36.531814: val_loss 0.0743
2024-12-21 00:41:36.532769: Pseudo dice [0.5605]
2024-12-21 00:41:36.533862: Epoch time: 216.44 s
2024-12-21 00:41:38.608839: 
2024-12-21 00:41:38.610458: Epoch 110
2024-12-21 00:41:38.611386: Current learning rate: 0.00304
2024-12-21 00:45:06.788545: Validation loss did not improve from -0.25922. Patience: 107/50
2024-12-21 00:45:06.790988: train_loss -0.8947
2024-12-21 00:45:06.791961: val_loss 0.0811
2024-12-21 00:45:06.792858: Pseudo dice [0.5548]
2024-12-21 00:45:06.793726: Epoch time: 208.18 s
2024-12-21 00:45:08.194681: 
2024-12-21 00:45:08.196162: Epoch 111
2024-12-21 00:45:08.197112: Current learning rate: 0.00297
2024-12-21 00:48:50.047207: Validation loss did not improve from -0.25922. Patience: 108/50
2024-12-21 00:48:50.048293: train_loss -0.8948
2024-12-21 00:48:50.049084: val_loss 0.1736
2024-12-21 00:48:50.049799: Pseudo dice [0.5232]
2024-12-21 00:48:50.050503: Epoch time: 221.85 s
2024-12-21 00:48:51.483015: 
2024-12-21 00:48:51.484429: Epoch 112
2024-12-21 00:48:51.485213: Current learning rate: 0.00291
2024-12-21 00:52:25.479338: Validation loss did not improve from -0.25922. Patience: 109/50
2024-12-21 00:52:25.481515: train_loss -0.8973
2024-12-21 00:52:25.482826: val_loss 0.1468
2024-12-21 00:52:25.483601: Pseudo dice [0.5254]
2024-12-21 00:52:25.484413: Epoch time: 214.0 s
2024-12-21 00:52:26.903033: 
2024-12-21 00:52:26.904498: Epoch 113
2024-12-21 00:52:26.905387: Current learning rate: 0.00284
2024-12-21 00:56:02.683229: Validation loss did not improve from -0.25922. Patience: 110/50
2024-12-21 00:56:02.684181: train_loss -0.8963
2024-12-21 00:56:02.685095: val_loss 0.0594
2024-12-21 00:56:02.685779: Pseudo dice [0.563]
2024-12-21 00:56:02.686467: Epoch time: 215.78 s
2024-12-21 00:56:04.096944: 
2024-12-21 00:56:04.098254: Epoch 114
2024-12-21 00:56:04.099121: Current learning rate: 0.00277
2024-12-21 00:59:50.776079: Validation loss did not improve from -0.25922. Patience: 111/50
2024-12-21 00:59:50.777069: train_loss -0.8981
2024-12-21 00:59:50.777868: val_loss 0.1041
2024-12-21 00:59:50.778592: Pseudo dice [0.5399]
2024-12-21 00:59:50.779389: Epoch time: 226.68 s
2024-12-21 00:59:52.653564: 
2024-12-21 00:59:52.654951: Epoch 115
2024-12-21 00:59:52.656024: Current learning rate: 0.0027
2024-12-21 01:03:37.731567: Validation loss did not improve from -0.25922. Patience: 112/50
2024-12-21 01:03:37.732579: train_loss -0.8991
2024-12-21 01:03:37.733485: val_loss 0.1564
2024-12-21 01:03:37.734206: Pseudo dice [0.5299]
2024-12-21 01:03:37.734974: Epoch time: 225.08 s
2024-12-21 01:03:39.159428: 
2024-12-21 01:03:39.160827: Epoch 116
2024-12-21 01:03:39.161682: Current learning rate: 0.00263
2024-12-21 01:07:23.948870: Validation loss did not improve from -0.25922. Patience: 113/50
2024-12-21 01:07:23.949828: train_loss -0.9003
2024-12-21 01:07:23.950583: val_loss 0.1584
2024-12-21 01:07:23.951360: Pseudo dice [0.5376]
2024-12-21 01:07:23.952446: Epoch time: 224.79 s
2024-12-21 01:07:25.433401: 
2024-12-21 01:07:25.434675: Epoch 117
2024-12-21 01:07:25.435503: Current learning rate: 0.00256
2024-12-21 01:11:02.685896: Validation loss did not improve from -0.25922. Patience: 114/50
2024-12-21 01:11:02.686790: train_loss -0.8986
2024-12-21 01:11:02.687729: val_loss 0.0957
2024-12-21 01:11:02.688479: Pseudo dice [0.5604]
2024-12-21 01:11:02.689490: Epoch time: 217.25 s
2024-12-21 01:11:04.067729: 
2024-12-21 01:11:04.068941: Epoch 118
2024-12-21 01:11:04.069723: Current learning rate: 0.00249
2024-12-21 01:14:23.518761: Validation loss did not improve from -0.25922. Patience: 115/50
2024-12-21 01:14:23.519933: train_loss -0.8993
2024-12-21 01:14:23.520858: val_loss 0.1896
2024-12-21 01:14:23.521590: Pseudo dice [0.5316]
2024-12-21 01:14:23.522516: Epoch time: 199.45 s
2024-12-21 01:14:25.469399: 
2024-12-21 01:14:25.470529: Epoch 119
2024-12-21 01:14:25.471335: Current learning rate: 0.00242
2024-12-21 01:18:01.632072: Validation loss did not improve from -0.25922. Patience: 116/50
2024-12-21 01:18:01.633058: train_loss -0.9017
2024-12-21 01:18:01.633918: val_loss 0.1558
2024-12-21 01:18:01.634756: Pseudo dice [0.5149]
2024-12-21 01:18:01.635673: Epoch time: 216.17 s
2024-12-21 01:18:03.438170: 
2024-12-21 01:18:03.439553: Epoch 120
2024-12-21 01:18:03.440401: Current learning rate: 0.00235
2024-12-21 01:21:38.252573: Validation loss did not improve from -0.25922. Patience: 117/50
2024-12-21 01:21:38.253582: train_loss -0.9012
2024-12-21 01:21:38.254551: val_loss 0.1434
2024-12-21 01:21:38.255219: Pseudo dice [0.5198]
2024-12-21 01:21:38.255952: Epoch time: 214.82 s
2024-12-21 01:21:39.675509: 
2024-12-21 01:21:39.676983: Epoch 121
2024-12-21 01:21:39.677848: Current learning rate: 0.00228
2024-12-21 01:25:11.409462: Validation loss did not improve from -0.25922. Patience: 118/50
2024-12-21 01:25:11.410719: train_loss -0.9007
2024-12-21 01:25:11.411507: val_loss 0.1379
2024-12-21 01:25:11.412205: Pseudo dice [0.5231]
2024-12-21 01:25:11.413030: Epoch time: 211.74 s
2024-12-21 01:25:12.936237: 
2024-12-21 01:25:12.937929: Epoch 122
2024-12-21 01:25:12.938710: Current learning rate: 0.00221
2024-12-21 01:28:51.045842: Validation loss did not improve from -0.25922. Patience: 119/50
2024-12-21 01:28:51.047857: train_loss -0.9007
2024-12-21 01:28:51.049387: val_loss 0.1397
2024-12-21 01:28:51.050441: Pseudo dice [0.5256]
2024-12-21 01:28:51.051772: Epoch time: 218.12 s
2024-12-21 01:28:52.564335: 
2024-12-21 01:28:52.565834: Epoch 123
2024-12-21 01:28:52.566821: Current learning rate: 0.00214
2024-12-21 01:32:31.711124: Validation loss did not improve from -0.25922. Patience: 120/50
2024-12-21 01:32:31.712248: train_loss -0.9037
2024-12-21 01:32:31.713260: val_loss 0.1285
2024-12-21 01:32:31.714053: Pseudo dice [0.5568]
2024-12-21 01:32:31.714820: Epoch time: 219.15 s
2024-12-21 01:32:33.180371: 
2024-12-21 01:32:33.181733: Epoch 124
2024-12-21 01:32:33.182622: Current learning rate: 0.00207
2024-12-21 01:36:01.923516: Validation loss did not improve from -0.25922. Patience: 121/50
2024-12-21 01:36:01.924648: train_loss -0.9036
2024-12-21 01:36:01.925518: val_loss 0.1151
2024-12-21 01:36:01.926246: Pseudo dice [0.5392]
2024-12-21 01:36:01.927053: Epoch time: 208.75 s
2024-12-21 01:36:03.766977: 
2024-12-21 01:36:03.768216: Epoch 125
2024-12-21 01:36:03.769096: Current learning rate: 0.00199
2024-12-21 01:39:57.148564: Validation loss did not improve from -0.25922. Patience: 122/50
2024-12-21 01:39:57.149677: train_loss -0.9035
2024-12-21 01:39:57.150634: val_loss 0.2237
2024-12-21 01:39:57.151542: Pseudo dice [0.5054]
2024-12-21 01:39:57.152501: Epoch time: 233.38 s
2024-12-21 01:39:58.600033: 
2024-12-21 01:39:58.601363: Epoch 126
2024-12-21 01:39:58.602401: Current learning rate: 0.00192
2024-12-21 01:43:46.071400: Validation loss did not improve from -0.25922. Patience: 123/50
2024-12-21 01:43:46.072461: train_loss -0.9031
2024-12-21 01:43:46.073497: val_loss 0.2391
2024-12-21 01:43:46.074515: Pseudo dice [0.4909]
2024-12-21 01:43:46.075445: Epoch time: 227.47 s
2024-12-21 01:43:47.489453: 
2024-12-21 01:43:47.491030: Epoch 127
2024-12-21 01:43:47.492020: Current learning rate: 0.00185
2024-12-21 01:47:29.590441: Validation loss did not improve from -0.25922. Patience: 124/50
2024-12-21 01:47:29.591554: train_loss -0.9034
2024-12-21 01:47:29.592632: val_loss 0.1668
2024-12-21 01:47:29.593680: Pseudo dice [0.5148]
2024-12-21 01:47:29.594714: Epoch time: 222.1 s
2024-12-21 01:47:31.082003: 
2024-12-21 01:47:31.083607: Epoch 128
2024-12-21 01:47:31.084545: Current learning rate: 0.00178
2024-12-21 01:51:16.876690: Validation loss did not improve from -0.25922. Patience: 125/50
2024-12-21 01:51:16.877699: train_loss -0.9056
2024-12-21 01:51:16.878543: val_loss 0.1044
2024-12-21 01:51:16.879358: Pseudo dice [0.5498]
2024-12-21 01:51:16.880220: Epoch time: 225.8 s
2024-12-21 01:51:18.295719: 
2024-12-21 01:51:18.297259: Epoch 129
2024-12-21 01:51:18.298316: Current learning rate: 0.0017
2024-12-21 01:54:58.325268: Validation loss did not improve from -0.25922. Patience: 126/50
2024-12-21 01:54:58.326318: train_loss -0.905
2024-12-21 01:54:58.327140: val_loss 0.1543
2024-12-21 01:54:58.327949: Pseudo dice [0.5269]
2024-12-21 01:54:58.328775: Epoch time: 220.03 s
2024-12-21 01:55:00.531095: 
2024-12-21 01:55:00.532465: Epoch 130
2024-12-21 01:55:00.533261: Current learning rate: 0.00163
2024-12-21 01:58:35.823893: Validation loss did not improve from -0.25922. Patience: 127/50
2024-12-21 01:58:35.825007: train_loss -0.9052
2024-12-21 01:58:35.826010: val_loss 0.1729
2024-12-21 01:58:35.826837: Pseudo dice [0.5276]
2024-12-21 01:58:35.827647: Epoch time: 215.29 s
2024-12-21 01:58:37.245082: 
2024-12-21 01:58:37.246540: Epoch 131
2024-12-21 01:58:37.247333: Current learning rate: 0.00156
2024-12-21 02:02:13.109926: Validation loss did not improve from -0.25922. Patience: 128/50
2024-12-21 02:02:13.111231: train_loss -0.9046
2024-12-21 02:02:13.112262: val_loss 0.1304
2024-12-21 02:02:13.112971: Pseudo dice [0.5297]
2024-12-21 02:02:13.113679: Epoch time: 215.87 s
2024-12-21 02:02:14.662239: 
2024-12-21 02:02:14.700450: Epoch 132
2024-12-21 02:02:14.701409: Current learning rate: 0.00148
2024-12-21 02:05:28.876128: Validation loss did not improve from -0.25922. Patience: 129/50
2024-12-21 02:05:28.877211: train_loss -0.906
2024-12-21 02:05:28.878191: val_loss 0.2295
2024-12-21 02:05:28.878944: Pseudo dice [0.5074]
2024-12-21 02:05:28.879658: Epoch time: 194.22 s
2024-12-21 02:05:30.320356: 
2024-12-21 02:05:30.321664: Epoch 133
2024-12-21 02:05:30.322582: Current learning rate: 0.00141
2024-12-21 02:09:05.282600: Validation loss did not improve from -0.25922. Patience: 130/50
2024-12-21 02:09:05.283573: train_loss -0.9064
2024-12-21 02:09:05.284519: val_loss 0.1133
2024-12-21 02:09:05.285345: Pseudo dice [0.5484]
2024-12-21 02:09:05.286085: Epoch time: 214.96 s
2024-12-21 02:09:06.713292: 
2024-12-21 02:09:06.714699: Epoch 134
2024-12-21 02:09:06.715513: Current learning rate: 0.00133
2024-12-21 02:12:53.244290: Validation loss did not improve from -0.25922. Patience: 131/50
2024-12-21 02:12:53.245308: train_loss -0.905
2024-12-21 02:12:53.246165: val_loss 0.1328
2024-12-21 02:12:53.246966: Pseudo dice [0.5435]
2024-12-21 02:12:53.247913: Epoch time: 226.53 s
2024-12-21 02:12:55.193402: 
2024-12-21 02:12:55.194793: Epoch 135
2024-12-21 02:12:55.195637: Current learning rate: 0.00126
2024-12-21 02:16:29.093624: Validation loss did not improve from -0.25922. Patience: 132/50
2024-12-21 02:16:29.094745: train_loss -0.9062
2024-12-21 02:16:29.095749: val_loss 0.1813
2024-12-21 02:16:29.103373: Pseudo dice [0.5276]
2024-12-21 02:16:29.104348: Epoch time: 213.9 s
2024-12-21 02:16:30.576177: 
2024-12-21 02:16:30.577561: Epoch 136
2024-12-21 02:16:30.578547: Current learning rate: 0.00118
2024-12-21 02:20:16.910683: Validation loss did not improve from -0.25922. Patience: 133/50
2024-12-21 02:20:16.911703: train_loss -0.9056
2024-12-21 02:20:16.912688: val_loss 0.2546
2024-12-21 02:20:16.913596: Pseudo dice [0.5214]
2024-12-21 02:20:16.914425: Epoch time: 226.34 s
2024-12-21 02:20:18.403810: 
2024-12-21 02:20:18.404940: Epoch 137
2024-12-21 02:20:18.405697: Current learning rate: 0.00111
2024-12-21 02:24:02.655018: Validation loss did not improve from -0.25922. Patience: 134/50
2024-12-21 02:24:02.656100: train_loss -0.9066
2024-12-21 02:24:02.657116: val_loss 0.0997
2024-12-21 02:24:02.658109: Pseudo dice [0.5536]
2024-12-21 02:24:02.658960: Epoch time: 224.25 s
2024-12-21 02:24:04.250553: 
2024-12-21 02:24:04.251951: Epoch 138
2024-12-21 02:24:04.252830: Current learning rate: 0.00103
2024-12-21 02:27:42.272557: Validation loss did not improve from -0.25922. Patience: 135/50
2024-12-21 02:27:42.273587: train_loss -0.9079
2024-12-21 02:27:42.274470: val_loss 0.1811
2024-12-21 02:27:42.275337: Pseudo dice [0.5214]
2024-12-21 02:27:42.276108: Epoch time: 218.02 s
2024-12-21 02:27:43.745233: 
2024-12-21 02:27:43.747515: Epoch 139
2024-12-21 02:27:43.748518: Current learning rate: 0.00095
2024-12-21 02:31:20.482506: Validation loss did not improve from -0.25922. Patience: 136/50
2024-12-21 02:31:20.506826: train_loss -0.9082
2024-12-21 02:31:20.508959: val_loss 0.1839
2024-12-21 02:31:20.509780: Pseudo dice [0.5303]
2024-12-21 02:31:20.510774: Epoch time: 216.76 s
2024-12-21 02:31:22.527454: 
2024-12-21 02:31:22.528695: Epoch 140
2024-12-21 02:31:22.529389: Current learning rate: 0.00087
2024-12-21 02:34:36.977834: Validation loss did not improve from -0.25922. Patience: 137/50
2024-12-21 02:34:36.978988: train_loss -0.9082
2024-12-21 02:34:36.979940: val_loss 0.1882
2024-12-21 02:34:36.980710: Pseudo dice [0.5077]
2024-12-21 02:34:36.981437: Epoch time: 194.45 s
2024-12-21 02:34:38.970233: 
2024-12-21 02:34:38.971680: Epoch 141
2024-12-21 02:34:38.972352: Current learning rate: 0.00079
2024-12-21 02:38:24.467130: Validation loss did not improve from -0.25922. Patience: 138/50
2024-12-21 02:38:24.468153: train_loss -0.9083
2024-12-21 02:38:24.468926: val_loss 0.1815
2024-12-21 02:38:24.469658: Pseudo dice [0.5239]
2024-12-21 02:38:24.470291: Epoch time: 225.5 s
2024-12-21 02:38:25.895187: 
2024-12-21 02:38:25.896731: Epoch 142
2024-12-21 02:38:25.897442: Current learning rate: 0.00071
2024-12-21 02:42:03.503777: Validation loss did not improve from -0.25922. Patience: 139/50
2024-12-21 02:42:03.506305: train_loss -0.9084
2024-12-21 02:42:03.507127: val_loss 0.1716
2024-12-21 02:42:03.507798: Pseudo dice [0.5389]
2024-12-21 02:42:03.508646: Epoch time: 217.61 s
2024-12-21 02:42:04.955478: 
2024-12-21 02:42:04.956813: Epoch 143
2024-12-21 02:42:04.957645: Current learning rate: 0.00063
2024-12-21 02:45:35.136269: Validation loss did not improve from -0.25922. Patience: 140/50
2024-12-21 02:45:35.137298: train_loss -0.9098
2024-12-21 02:45:35.138008: val_loss 0.1889
2024-12-21 02:45:35.138744: Pseudo dice [0.5088]
2024-12-21 02:45:35.139534: Epoch time: 210.18 s
2024-12-21 02:45:36.566185: 
2024-12-21 02:45:36.567566: Epoch 144
2024-12-21 02:45:36.568381: Current learning rate: 0.00055
2024-12-21 02:49:05.759422: Validation loss did not improve from -0.25922. Patience: 141/50
2024-12-21 02:49:05.781665: train_loss -0.9082
2024-12-21 02:49:05.783075: val_loss 0.1212
2024-12-21 02:49:05.783781: Pseudo dice [0.5339]
2024-12-21 02:49:05.784555: Epoch time: 209.22 s
2024-12-21 02:49:07.830642: 
2024-12-21 02:49:07.832073: Epoch 145
2024-12-21 02:49:07.833014: Current learning rate: 0.00047
2024-12-21 02:52:22.748001: Validation loss did not improve from -0.25922. Patience: 142/50
2024-12-21 02:52:22.748998: train_loss -0.9085
2024-12-21 02:52:22.749871: val_loss 0.1313
2024-12-21 02:52:22.750525: Pseudo dice [0.5406]
2024-12-21 02:52:22.751318: Epoch time: 194.92 s
2024-12-21 02:52:24.206861: 
2024-12-21 02:52:24.208193: Epoch 146
2024-12-21 02:52:24.208892: Current learning rate: 0.00038
2024-12-21 02:55:30.725956: Validation loss did not improve from -0.25922. Patience: 143/50
2024-12-21 02:55:30.727025: train_loss -0.9093
2024-12-21 02:55:30.727846: val_loss 0.219
2024-12-21 02:55:30.728558: Pseudo dice [0.5173]
2024-12-21 02:55:30.729450: Epoch time: 186.52 s
2024-12-21 02:55:32.198223: 
2024-12-21 02:55:32.199503: Epoch 147
2024-12-21 02:55:32.200338: Current learning rate: 0.0003
2024-12-21 02:58:27.667337: Validation loss did not improve from -0.25922. Patience: 144/50
2024-12-21 02:58:27.668369: train_loss -0.9097
2024-12-21 02:58:27.669158: val_loss 0.1961
2024-12-21 02:58:27.669842: Pseudo dice [0.5203]
2024-12-21 02:58:27.670453: Epoch time: 175.47 s
2024-12-21 02:58:29.104790: 
2024-12-21 02:58:29.106014: Epoch 148
2024-12-21 02:58:29.106822: Current learning rate: 0.00021
2024-12-21 03:01:38.807585: Validation loss did not improve from -0.25922. Patience: 145/50
2024-12-21 03:01:38.808552: train_loss -0.9096
2024-12-21 03:01:38.809676: val_loss 0.2306
2024-12-21 03:01:38.810878: Pseudo dice [0.4998]
2024-12-21 03:01:38.812050: Epoch time: 189.71 s
2024-12-21 03:01:40.208254: 
2024-12-21 03:01:40.209830: Epoch 149
2024-12-21 03:01:40.210858: Current learning rate: 0.00011
2024-12-21 03:05:15.595937: Validation loss did not improve from -0.25922. Patience: 146/50
2024-12-21 03:05:15.596958: train_loss -0.9084
2024-12-21 03:05:15.597890: val_loss 0.2604
2024-12-21 03:05:15.598681: Pseudo dice [0.5116]
2024-12-21 03:05:15.599460: Epoch time: 215.39 s
2024-12-21 03:05:17.466358: Training done.
2024-12-21 03:05:17.690892: Using splits from existing split file: /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset308_Sohee_Calcium_OCT_CrossValidation/splits_final_20.json
2024-12-21 03:05:17.710147: The split file contains 5 splits.
2024-12-21 03:05:17.710953: Desired fold for training: 0
2024-12-21 03:05:17.711787: This split has 1 training and 7 validation cases.
2024-12-21 03:05:17.712867: predicting 101-019
2024-12-21 03:05:17.741969: 101-019, shape torch.Size([1, 375, 498, 498]), rank 0
2024-12-21 03:07:55.463443: predicting 101-044
2024-12-21 03:07:55.519042: 101-044, shape torch.Size([1, 404, 498, 498]), rank 0
2024-12-21 03:09:55.739316: predicting 101-045
2024-12-21 03:09:55.760101: 101-045, shape torch.Size([1, 375, 498, 498]), rank 0
2024-12-21 03:12:00.500424: predicting 106-002
2024-12-21 03:12:00.534980: 106-002, shape torch.Size([1, 539, 498, 498]), rank 0
2024-12-21 03:15:10.283774: predicting 701-013
2024-12-21 03:15:10.329187: 701-013, shape torch.Size([1, 375, 498, 498]), rank 0
2024-12-21 03:17:18.645141: predicting 704-003
2024-12-21 03:17:18.679425: 704-003, shape torch.Size([1, 375, 498, 498]), rank 0
2024-12-21 03:19:13.515836: predicting 706-005
2024-12-21 03:19:13.553288: 706-005, shape torch.Size([1, 375, 498, 498]), rank 0
2024-12-21 03:21:29.890847: Validation complete
2024-12-21 03:21:29.891619: Mean Validation Dice:  0.5162723676745814
2024-12-20 17:53:08.358311: unpacking done...
2024-12-20 17:53:08.365826: Unable to plot network architecture: nnUNet_compile is enabled!
2024-12-20 17:53:08.411579: 
2024-12-20 17:53:08.413049: Epoch 0
2024-12-20 17:53:08.414290: Current learning rate: 0.01
2024-12-20 17:58:09.124066: Validation loss improved from 1000.00000 to -0.09164! Patience: 0/50
2024-12-20 17:58:09.125265: train_loss -0.192
2024-12-20 17:58:09.126459: val_loss -0.0916
2024-12-20 17:58:09.127401: Pseudo dice [0.5175]
2024-12-20 17:58:09.128412: Epoch time: 300.72 s
2024-12-20 17:58:09.129334: Yayy! New best EMA pseudo Dice: 0.5175
2024-12-20 17:58:10.781221: 
2024-12-20 17:58:10.782571: Epoch 1
2024-12-20 17:58:10.783318: Current learning rate: 0.00994
2024-12-20 18:02:15.333893: Validation loss improved from -0.09164 to -0.14984! Patience: 0/50
2024-12-20 18:02:15.335091: train_loss -0.4321
2024-12-20 18:02:15.336273: val_loss -0.1498
2024-12-20 18:02:15.337096: Pseudo dice [0.546]
2024-12-20 18:02:15.338052: Epoch time: 244.56 s
2024-12-20 18:02:15.338969: Yayy! New best EMA pseudo Dice: 0.5204
2024-12-20 18:02:17.168076: 
2024-12-20 18:02:17.170281: Epoch 2
2024-12-20 18:02:17.171381: Current learning rate: 0.00988
2024-12-20 18:06:34.413161: Validation loss improved from -0.14984 to -0.21331! Patience: 0/50
2024-12-20 18:06:34.414134: train_loss -0.5039
2024-12-20 18:06:34.414963: val_loss -0.2133
2024-12-20 18:06:34.415916: Pseudo dice [0.5787]
2024-12-20 18:06:34.416746: Epoch time: 257.25 s
2024-12-20 18:06:34.417701: Yayy! New best EMA pseudo Dice: 0.5262
2024-12-20 18:06:36.312630: 
2024-12-20 18:06:36.314309: Epoch 3
2024-12-20 18:06:36.315084: Current learning rate: 0.00982
2024-12-20 18:10:53.705255: Validation loss did not improve from -0.21331. Patience: 1/50
2024-12-20 18:10:53.706394: train_loss -0.5255
2024-12-20 18:10:53.707478: val_loss -0.1911
2024-12-20 18:10:53.708485: Pseudo dice [0.5702]
2024-12-20 18:10:53.709228: Epoch time: 257.39 s
2024-12-20 18:10:53.710064: Yayy! New best EMA pseudo Dice: 0.5306
2024-12-20 18:10:55.521441: 
2024-12-20 18:10:55.522666: Epoch 4
2024-12-20 18:10:55.529901: Current learning rate: 0.00976
2024-12-20 18:15:10.185719: Validation loss did not improve from -0.21331. Patience: 2/50
2024-12-20 18:15:10.186667: train_loss -0.5711
2024-12-20 18:15:10.187678: val_loss -0.1694
2024-12-20 18:15:10.188675: Pseudo dice [0.5588]
2024-12-20 18:15:10.189669: Epoch time: 254.67 s
2024-12-20 18:15:10.546778: Yayy! New best EMA pseudo Dice: 0.5334
2024-12-20 18:15:12.479799: 
2024-12-20 18:15:12.481368: Epoch 5
2024-12-20 18:15:12.482552: Current learning rate: 0.0097
2024-12-20 18:19:14.541597: Validation loss improved from -0.21331 to -0.22483! Patience: 2/50
2024-12-20 18:19:14.542647: train_loss -0.5896
2024-12-20 18:19:14.543619: val_loss -0.2248
2024-12-20 18:19:14.544368: Pseudo dice [0.5958]
2024-12-20 18:19:14.545197: Epoch time: 242.07 s
2024-12-20 18:19:14.545948: Yayy! New best EMA pseudo Dice: 0.5396
2024-12-20 18:19:16.326082: 
2024-12-20 18:19:16.327354: Epoch 6
2024-12-20 18:19:16.328278: Current learning rate: 0.00964
2024-12-20 18:23:11.999554: Validation loss did not improve from -0.22483. Patience: 1/50
2024-12-20 18:23:12.000556: train_loss -0.6132
2024-12-20 18:23:12.001359: val_loss -0.2213
2024-12-20 18:23:12.002257: Pseudo dice [0.5927]
2024-12-20 18:23:12.003071: Epoch time: 235.68 s
2024-12-20 18:23:12.003828: Yayy! New best EMA pseudo Dice: 0.545
2024-12-20 18:23:13.841702: 
2024-12-20 18:23:13.843148: Epoch 7
2024-12-20 18:23:13.844077: Current learning rate: 0.00958
2024-12-20 18:27:15.152017: Validation loss did not improve from -0.22483. Patience: 2/50
2024-12-20 18:27:15.153039: train_loss -0.6143
2024-12-20 18:27:15.153867: val_loss -0.2149
2024-12-20 18:27:15.154585: Pseudo dice [0.6027]
2024-12-20 18:27:15.155337: Epoch time: 241.31 s
2024-12-20 18:27:15.156123: Yayy! New best EMA pseudo Dice: 0.5507
2024-12-20 18:27:17.494170: 
2024-12-20 18:27:17.495466: Epoch 8
2024-12-20 18:27:17.496227: Current learning rate: 0.00952
2024-12-20 18:31:01.647102: Validation loss did not improve from -0.22483. Patience: 3/50
2024-12-20 18:31:01.648007: train_loss -0.6213
2024-12-20 18:31:01.648870: val_loss -0.209
2024-12-20 18:31:01.649652: Pseudo dice [0.5723]
2024-12-20 18:31:01.650545: Epoch time: 224.16 s
2024-12-20 18:31:01.651403: Yayy! New best EMA pseudo Dice: 0.5529
2024-12-20 18:31:03.698474: 
2024-12-20 18:31:03.700024: Epoch 9
2024-12-20 18:31:03.701045: Current learning rate: 0.00946
2024-12-20 18:34:59.899022: Validation loss did not improve from -0.22483. Patience: 4/50
2024-12-20 18:34:59.899794: train_loss -0.6452
2024-12-20 18:34:59.900560: val_loss -0.1926
2024-12-20 18:34:59.901293: Pseudo dice [0.5599]
2024-12-20 18:34:59.902012: Epoch time: 236.2 s
2024-12-20 18:35:00.330366: Yayy! New best EMA pseudo Dice: 0.5536
2024-12-20 18:35:02.145595: 
2024-12-20 18:35:02.147378: Epoch 10
2024-12-20 18:35:02.148294: Current learning rate: 0.0094
2024-12-20 18:38:46.129042: Validation loss did not improve from -0.22483. Patience: 5/50
2024-12-20 18:38:46.130148: train_loss -0.6667
2024-12-20 18:38:46.131170: val_loss -0.162
2024-12-20 18:38:46.131997: Pseudo dice [0.5668]
2024-12-20 18:38:46.132771: Epoch time: 223.99 s
2024-12-20 18:38:46.133481: Yayy! New best EMA pseudo Dice: 0.5549
2024-12-20 18:38:47.987791: 
2024-12-20 18:38:47.989572: Epoch 11
2024-12-20 18:38:47.990929: Current learning rate: 0.00934
2024-12-20 18:42:33.121453: Validation loss did not improve from -0.22483. Patience: 6/50
2024-12-20 18:42:33.122637: train_loss -0.6787
2024-12-20 18:42:33.123521: val_loss -0.2042
2024-12-20 18:42:33.124322: Pseudo dice [0.5836]
2024-12-20 18:42:33.125132: Epoch time: 225.14 s
2024-12-20 18:42:33.125945: Yayy! New best EMA pseudo Dice: 0.5578
2024-12-20 18:42:34.962649: 
2024-12-20 18:42:34.981128: Epoch 12
2024-12-20 18:42:34.982221: Current learning rate: 0.00928
2024-12-20 18:46:39.513363: Validation loss did not improve from -0.22483. Patience: 7/50
2024-12-20 18:46:39.514493: train_loss -0.6843
2024-12-20 18:46:39.515443: val_loss -0.1556
2024-12-20 18:46:39.516189: Pseudo dice [0.5782]
2024-12-20 18:46:39.517003: Epoch time: 244.55 s
2024-12-20 18:46:39.517784: Yayy! New best EMA pseudo Dice: 0.5598
2024-12-20 18:46:41.410380: 
2024-12-20 18:46:41.412126: Epoch 13
2024-12-20 18:46:41.413029: Current learning rate: 0.00922
2024-12-20 18:50:53.190430: Validation loss improved from -0.22483 to -0.25117! Patience: 7/50
2024-12-20 18:50:53.191441: train_loss -0.7013
2024-12-20 18:50:53.192281: val_loss -0.2512
2024-12-20 18:50:53.193019: Pseudo dice [0.6117]
2024-12-20 18:50:53.193796: Epoch time: 251.78 s
2024-12-20 18:50:53.194458: Yayy! New best EMA pseudo Dice: 0.565
2024-12-20 18:50:55.103189: 
2024-12-20 18:50:55.104889: Epoch 14
2024-12-20 18:50:55.105891: Current learning rate: 0.00916
2024-12-20 18:55:08.051456: Validation loss did not improve from -0.25117. Patience: 1/50
2024-12-20 18:55:08.053042: train_loss -0.6906
2024-12-20 18:55:08.054156: val_loss -0.2213
2024-12-20 18:55:08.055019: Pseudo dice [0.5837]
2024-12-20 18:55:08.055840: Epoch time: 252.95 s
2024-12-20 18:55:08.480211: Yayy! New best EMA pseudo Dice: 0.5669
2024-12-20 18:55:10.287606: 
2024-12-20 18:55:10.288822: Epoch 15
2024-12-20 18:55:10.289830: Current learning rate: 0.0091
2024-12-20 18:59:18.625045: Validation loss did not improve from -0.25117. Patience: 2/50
2024-12-20 18:59:18.644262: train_loss -0.7027
2024-12-20 18:59:18.646098: val_loss -0.2495
2024-12-20 18:59:18.647091: Pseudo dice [0.6104]
2024-12-20 18:59:18.648053: Epoch time: 248.36 s
2024-12-20 18:59:18.648878: Yayy! New best EMA pseudo Dice: 0.5712
2024-12-20 18:59:20.562409: 
2024-12-20 18:59:20.563808: Epoch 16
2024-12-20 18:59:20.564573: Current learning rate: 0.00903
2024-12-20 19:03:26.322495: Validation loss improved from -0.25117 to -0.26026! Patience: 2/50
2024-12-20 19:03:26.323502: train_loss -0.7168
2024-12-20 19:03:26.324399: val_loss -0.2603
2024-12-20 19:03:26.325035: Pseudo dice [0.6184]
2024-12-20 19:03:26.325766: Epoch time: 245.76 s
2024-12-20 19:03:26.326423: Yayy! New best EMA pseudo Dice: 0.5759
2024-12-20 19:03:28.110032: 
2024-12-20 19:03:28.111297: Epoch 17
2024-12-20 19:03:28.112146: Current learning rate: 0.00897
2024-12-20 19:07:02.802134: Validation loss did not improve from -0.26026. Patience: 1/50
2024-12-20 19:07:02.803054: train_loss -0.721
2024-12-20 19:07:02.803911: val_loss -0.2175
2024-12-20 19:07:02.804793: Pseudo dice [0.5811]
2024-12-20 19:07:02.805615: Epoch time: 214.69 s
2024-12-20 19:07:02.806413: Yayy! New best EMA pseudo Dice: 0.5765
2024-12-20 19:07:04.656567: 
2024-12-20 19:07:04.657794: Epoch 18
2024-12-20 19:07:04.658660: Current learning rate: 0.00891
2024-12-20 19:10:53.166592: Validation loss did not improve from -0.26026. Patience: 2/50
2024-12-20 19:10:53.167570: train_loss -0.7298
2024-12-20 19:10:53.168361: val_loss -0.1889
2024-12-20 19:10:53.169107: Pseudo dice [0.5917]
2024-12-20 19:10:53.169881: Epoch time: 228.51 s
2024-12-20 19:10:53.170670: Yayy! New best EMA pseudo Dice: 0.578
2024-12-20 19:10:55.478730: 
2024-12-20 19:10:55.479803: Epoch 19
2024-12-20 19:10:55.480551: Current learning rate: 0.00885
2024-12-20 19:15:02.571823: Validation loss did not improve from -0.26026. Patience: 3/50
2024-12-20 19:15:02.572810: train_loss -0.7353
2024-12-20 19:15:02.573731: val_loss -0.2349
2024-12-20 19:15:02.574699: Pseudo dice [0.5985]
2024-12-20 19:15:02.575574: Epoch time: 247.1 s
2024-12-20 19:15:03.024968: Yayy! New best EMA pseudo Dice: 0.58
2024-12-20 19:15:04.900151: 
2024-12-20 19:15:04.901487: Epoch 20
2024-12-20 19:15:04.902500: Current learning rate: 0.00879
2024-12-20 19:19:14.939942: Validation loss did not improve from -0.26026. Patience: 4/50
2024-12-20 19:19:14.941061: train_loss -0.7402
2024-12-20 19:19:14.941977: val_loss -0.2557
2024-12-20 19:19:14.942793: Pseudo dice [0.6299]
2024-12-20 19:19:14.943481: Epoch time: 250.04 s
2024-12-20 19:19:14.944198: Yayy! New best EMA pseudo Dice: 0.585
2024-12-20 19:19:16.826734: 
2024-12-20 19:19:16.828552: Epoch 21
2024-12-20 19:19:16.829690: Current learning rate: 0.00873
2024-12-20 19:23:30.193993: Validation loss did not improve from -0.26026. Patience: 5/50
2024-12-20 19:23:30.195239: train_loss -0.7456
2024-12-20 19:23:30.196090: val_loss -0.1896
2024-12-20 19:23:30.196887: Pseudo dice [0.5855]
2024-12-20 19:23:30.197627: Epoch time: 253.37 s
2024-12-20 19:23:30.198268: Yayy! New best EMA pseudo Dice: 0.5851
2024-12-20 19:23:32.005134: 
2024-12-20 19:23:32.006397: Epoch 22
2024-12-20 19:23:32.007211: Current learning rate: 0.00867
2024-12-20 19:27:42.685522: Validation loss did not improve from -0.26026. Patience: 6/50
2024-12-20 19:27:42.686587: train_loss -0.7455
2024-12-20 19:27:42.687369: val_loss -0.2368
2024-12-20 19:27:42.688106: Pseudo dice [0.6054]
2024-12-20 19:27:42.688832: Epoch time: 250.68 s
2024-12-20 19:27:42.689524: Yayy! New best EMA pseudo Dice: 0.5871
2024-12-20 19:27:44.528174: 
2024-12-20 19:27:44.529660: Epoch 23
2024-12-20 19:27:44.530676: Current learning rate: 0.00861
2024-12-20 19:31:48.633096: Validation loss did not improve from -0.26026. Patience: 7/50
2024-12-20 19:31:48.635390: train_loss -0.756
2024-12-20 19:31:48.636465: val_loss -0.2206
2024-12-20 19:31:48.637346: Pseudo dice [0.5977]
2024-12-20 19:31:48.638133: Epoch time: 244.11 s
2024-12-20 19:31:48.638885: Yayy! New best EMA pseudo Dice: 0.5882
2024-12-20 19:31:50.435995: 
2024-12-20 19:31:50.437327: Epoch 24
2024-12-20 19:31:50.438103: Current learning rate: 0.00855
2024-12-20 19:35:42.206486: Validation loss did not improve from -0.26026. Patience: 8/50
2024-12-20 19:35:42.207520: train_loss -0.7675
2024-12-20 19:35:42.208596: val_loss -0.1698
2024-12-20 19:35:42.209362: Pseudo dice [0.5633]
2024-12-20 19:35:42.210044: Epoch time: 231.77 s
2024-12-20 19:35:44.016363: 
2024-12-20 19:35:44.017404: Epoch 25
2024-12-20 19:35:44.018126: Current learning rate: 0.00849
2024-12-20 19:39:24.369416: Validation loss did not improve from -0.26026. Patience: 9/50
2024-12-20 19:39:24.370337: train_loss -0.7652
2024-12-20 19:39:24.371211: val_loss -0.2258
2024-12-20 19:39:24.372002: Pseudo dice [0.6146]
2024-12-20 19:39:24.372719: Epoch time: 220.35 s
2024-12-20 19:39:24.373437: Yayy! New best EMA pseudo Dice: 0.5886
2024-12-20 19:39:26.094584: 
2024-12-20 19:39:26.095809: Epoch 26
2024-12-20 19:39:26.096553: Current learning rate: 0.00843
2024-12-20 19:43:21.730128: Validation loss did not improve from -0.26026. Patience: 10/50
2024-12-20 19:43:21.731149: train_loss -0.7685
2024-12-20 19:43:21.732053: val_loss -0.2137
2024-12-20 19:43:21.732842: Pseudo dice [0.5927]
2024-12-20 19:43:21.733772: Epoch time: 235.64 s
2024-12-20 19:43:21.734679: Yayy! New best EMA pseudo Dice: 0.589
2024-12-20 19:43:23.581541: 
2024-12-20 19:43:23.582776: Epoch 27
2024-12-20 19:43:23.583683: Current learning rate: 0.00836
2024-12-20 19:47:26.111331: Validation loss did not improve from -0.26026. Patience: 11/50
2024-12-20 19:47:26.112278: train_loss -0.7673
2024-12-20 19:47:26.113172: val_loss -0.1804
2024-12-20 19:47:26.119620: Pseudo dice [0.5816]
2024-12-20 19:47:26.120470: Epoch time: 242.53 s
2024-12-20 19:47:27.503404: 
2024-12-20 19:47:27.504624: Epoch 28
2024-12-20 19:47:27.505359: Current learning rate: 0.0083
2024-12-20 19:51:25.803030: Validation loss did not improve from -0.26026. Patience: 12/50
2024-12-20 19:51:25.804070: train_loss -0.769
2024-12-20 19:51:25.804901: val_loss -0.1594
2024-12-20 19:51:25.805601: Pseudo dice [0.5781]
2024-12-20 19:51:25.806321: Epoch time: 238.3 s
2024-12-20 19:51:27.641956: 
2024-12-20 19:51:27.643372: Epoch 29
2024-12-20 19:51:27.644114: Current learning rate: 0.00824
2024-12-20 19:55:41.673096: Validation loss did not improve from -0.26026. Patience: 13/50
2024-12-20 19:55:41.674107: train_loss -0.774
2024-12-20 19:55:41.675015: val_loss -0.2139
2024-12-20 19:55:41.675820: Pseudo dice [0.623]
2024-12-20 19:55:41.676699: Epoch time: 254.03 s
2024-12-20 19:55:42.131124: Yayy! New best EMA pseudo Dice: 0.5908
2024-12-20 19:55:43.955842: 
2024-12-20 19:55:43.957324: Epoch 30
2024-12-20 19:55:43.958386: Current learning rate: 0.00818
2024-12-20 19:59:59.057973: Validation loss did not improve from -0.26026. Patience: 14/50
2024-12-20 19:59:59.058901: train_loss -0.771
2024-12-20 19:59:59.059771: val_loss -0.1719
2024-12-20 19:59:59.060813: Pseudo dice [0.5919]
2024-12-20 19:59:59.061871: Epoch time: 255.1 s
2024-12-20 19:59:59.062932: Yayy! New best EMA pseudo Dice: 0.5909
2024-12-20 20:00:00.863031: 
2024-12-20 20:00:00.864515: Epoch 31
2024-12-20 20:00:00.865573: Current learning rate: 0.00812
2024-12-20 20:04:07.305113: Validation loss did not improve from -0.26026. Patience: 15/50
2024-12-20 20:04:07.307261: train_loss -0.7698
2024-12-20 20:04:07.309604: val_loss -0.1063
2024-12-20 20:04:07.310618: Pseudo dice [0.5367]
2024-12-20 20:04:07.311818: Epoch time: 246.45 s
2024-12-20 20:04:08.814135: 
2024-12-20 20:04:08.815523: Epoch 32
2024-12-20 20:04:08.816337: Current learning rate: 0.00806
2024-12-20 20:08:00.953435: Validation loss did not improve from -0.26026. Patience: 16/50
2024-12-20 20:08:00.954396: train_loss -0.779
2024-12-20 20:08:00.955217: val_loss -0.181
2024-12-20 20:08:00.955953: Pseudo dice [0.5885]
2024-12-20 20:08:00.956805: Epoch time: 232.14 s
2024-12-20 20:08:02.391106: 
2024-12-20 20:08:02.392321: Epoch 33
2024-12-20 20:08:02.393097: Current learning rate: 0.008
2024-12-20 20:11:47.747484: Validation loss did not improve from -0.26026. Patience: 17/50
2024-12-20 20:11:47.748563: train_loss -0.7839
2024-12-20 20:11:47.749414: val_loss -0.2252
2024-12-20 20:11:47.750199: Pseudo dice [0.6277]
2024-12-20 20:11:47.751043: Epoch time: 225.36 s
2024-12-20 20:11:49.270863: 
2024-12-20 20:11:49.272470: Epoch 34
2024-12-20 20:11:49.273442: Current learning rate: 0.00793
2024-12-20 20:15:46.307017: Validation loss did not improve from -0.26026. Patience: 18/50
2024-12-20 20:15:46.308650: train_loss -0.7873
2024-12-20 20:15:46.309685: val_loss -0.2167
2024-12-20 20:15:46.310481: Pseudo dice [0.613]
2024-12-20 20:15:46.311230: Epoch time: 237.04 s
2024-12-20 20:15:46.666124: Yayy! New best EMA pseudo Dice: 0.5923
2024-12-20 20:15:48.532527: 
2024-12-20 20:15:48.533591: Epoch 35
2024-12-20 20:15:48.534274: Current learning rate: 0.00787
2024-12-20 20:19:38.545074: Validation loss did not improve from -0.26026. Patience: 19/50
2024-12-20 20:19:38.546286: train_loss -0.7849
2024-12-20 20:19:38.547196: val_loss -0.1919
2024-12-20 20:19:38.547879: Pseudo dice [0.5858]
2024-12-20 20:19:38.548535: Epoch time: 230.02 s
2024-12-20 20:19:40.060023: 
2024-12-20 20:19:40.061368: Epoch 36
2024-12-20 20:19:40.062093: Current learning rate: 0.00781
2024-12-20 20:23:33.411175: Validation loss did not improve from -0.26026. Patience: 20/50
2024-12-20 20:23:33.412162: train_loss -0.7867
2024-12-20 20:23:33.412952: val_loss -0.2441
2024-12-20 20:23:33.413686: Pseudo dice [0.6175]
2024-12-20 20:23:33.414423: Epoch time: 233.35 s
2024-12-20 20:23:33.415206: Yayy! New best EMA pseudo Dice: 0.5942
2024-12-20 20:23:35.336441: 
2024-12-20 20:23:35.337696: Epoch 37
2024-12-20 20:23:35.338684: Current learning rate: 0.00775
2024-12-20 20:27:51.356090: Validation loss did not improve from -0.26026. Patience: 21/50
2024-12-20 20:27:51.357053: train_loss -0.7953
2024-12-20 20:27:51.357920: val_loss -0.2578
2024-12-20 20:27:51.358613: Pseudo dice [0.6219]
2024-12-20 20:27:51.359350: Epoch time: 256.02 s
2024-12-20 20:27:51.360024: Yayy! New best EMA pseudo Dice: 0.597
2024-12-20 20:27:53.259004: 
2024-12-20 20:27:53.260441: Epoch 38
2024-12-20 20:27:53.261395: Current learning rate: 0.00769
2024-12-20 20:32:05.364376: Validation loss did not improve from -0.26026. Patience: 22/50
2024-12-20 20:32:05.365506: train_loss -0.7929
2024-12-20 20:32:05.366457: val_loss -0.2381
2024-12-20 20:32:05.367234: Pseudo dice [0.6129]
2024-12-20 20:32:05.368033: Epoch time: 252.11 s
2024-12-20 20:32:05.368834: Yayy! New best EMA pseudo Dice: 0.5986
2024-12-20 20:32:07.239494: 
2024-12-20 20:32:07.240659: Epoch 39
2024-12-20 20:32:07.241391: Current learning rate: 0.00763
2024-12-20 20:36:12.499197: Validation loss did not improve from -0.26026. Patience: 23/50
2024-12-20 20:36:12.500310: train_loss -0.7948
2024-12-20 20:36:12.501355: val_loss -0.1913
2024-12-20 20:36:12.502118: Pseudo dice [0.6123]
2024-12-20 20:36:12.503002: Epoch time: 245.26 s
2024-12-20 20:36:12.989711: Yayy! New best EMA pseudo Dice: 0.6
2024-12-20 20:36:15.449103: 
2024-12-20 20:36:15.450589: Epoch 40
2024-12-20 20:36:15.451338: Current learning rate: 0.00756
2024-12-20 20:40:29.639424: Validation loss did not improve from -0.26026. Patience: 24/50
2024-12-20 20:40:29.640523: train_loss -0.7971
2024-12-20 20:40:29.641487: val_loss -0.208
2024-12-20 20:40:29.642430: Pseudo dice [0.6129]
2024-12-20 20:40:29.643196: Epoch time: 254.19 s
2024-12-20 20:40:29.643931: Yayy! New best EMA pseudo Dice: 0.6013
2024-12-20 20:40:31.494616: 
2024-12-20 20:40:31.496090: Epoch 41
2024-12-20 20:40:31.497033: Current learning rate: 0.0075
2024-12-20 20:44:32.888516: Validation loss improved from -0.26026 to -0.26436! Patience: 24/50
2024-12-20 20:44:32.889400: train_loss -0.7948
2024-12-20 20:44:32.890190: val_loss -0.2644
2024-12-20 20:44:32.890966: Pseudo dice [0.6359]
2024-12-20 20:44:32.891610: Epoch time: 241.4 s
2024-12-20 20:44:32.892310: Yayy! New best EMA pseudo Dice: 0.6047
2024-12-20 20:44:34.712267: 
2024-12-20 20:44:34.713558: Epoch 42
2024-12-20 20:44:34.714299: Current learning rate: 0.00744
2024-12-20 20:48:34.848449: Validation loss did not improve from -0.26436. Patience: 1/50
2024-12-20 20:48:34.849453: train_loss -0.8049
2024-12-20 20:48:34.850217: val_loss -0.2158
2024-12-20 20:48:34.851038: Pseudo dice [0.6024]
2024-12-20 20:48:34.851738: Epoch time: 240.14 s
2024-12-20 20:48:36.296529: 
2024-12-20 20:48:36.299235: Epoch 43
2024-12-20 20:48:36.300114: Current learning rate: 0.00738
2024-12-20 20:52:27.361996: Validation loss did not improve from -0.26436. Patience: 2/50
2024-12-20 20:52:27.363193: train_loss -0.8061
2024-12-20 20:52:27.363965: val_loss -0.225
2024-12-20 20:52:27.364624: Pseudo dice [0.618]
2024-12-20 20:52:27.365336: Epoch time: 231.07 s
2024-12-20 20:52:27.365977: Yayy! New best EMA pseudo Dice: 0.6058
2024-12-20 20:52:29.184647: 
2024-12-20 20:52:29.187325: Epoch 44
2024-12-20 20:52:29.188278: Current learning rate: 0.00732
2024-12-20 20:56:29.758081: Validation loss did not improve from -0.26436. Patience: 3/50
2024-12-20 20:56:29.759159: train_loss -0.8073
2024-12-20 20:56:29.760139: val_loss -0.1878
2024-12-20 20:56:29.760946: Pseudo dice [0.6206]
2024-12-20 20:56:29.761690: Epoch time: 240.58 s
2024-12-20 20:56:30.193463: Yayy! New best EMA pseudo Dice: 0.6073
2024-12-20 20:56:31.973544: 
2024-12-20 20:56:31.974872: Epoch 45
2024-12-20 20:56:31.978077: Current learning rate: 0.00725
2024-12-20 21:00:42.177255: Validation loss did not improve from -0.26436. Patience: 4/50
2024-12-20 21:00:42.178207: train_loss -0.8102
2024-12-20 21:00:42.179279: val_loss -0.2087
2024-12-20 21:00:42.180252: Pseudo dice [0.6184]
2024-12-20 21:00:42.181204: Epoch time: 250.21 s
2024-12-20 21:00:42.182103: Yayy! New best EMA pseudo Dice: 0.6084
2024-12-20 21:00:43.938174: 
2024-12-20 21:00:43.939724: Epoch 46
2024-12-20 21:00:43.940723: Current learning rate: 0.00719
2024-12-20 21:04:59.041507: Validation loss did not improve from -0.26436. Patience: 5/50
2024-12-20 21:04:59.042658: train_loss -0.811
2024-12-20 21:04:59.043420: val_loss -0.2165
2024-12-20 21:04:59.044061: Pseudo dice [0.6254]
2024-12-20 21:04:59.044693: Epoch time: 255.11 s
2024-12-20 21:04:59.045490: Yayy! New best EMA pseudo Dice: 0.6101
2024-12-20 21:05:00.833397: 
2024-12-20 21:05:00.834924: Epoch 47
2024-12-20 21:05:00.835678: Current learning rate: 0.00713
2024-12-20 21:09:05.387084: Validation loss did not improve from -0.26436. Patience: 6/50
2024-12-20 21:09:05.389019: train_loss -0.8123
2024-12-20 21:09:05.390623: val_loss -0.2148
2024-12-20 21:09:05.391452: Pseudo dice [0.6044]
2024-12-20 21:09:05.392356: Epoch time: 244.56 s
2024-12-20 21:09:06.829056: 
2024-12-20 21:09:06.830524: Epoch 48
2024-12-20 21:09:06.831456: Current learning rate: 0.00707
2024-12-20 21:13:17.108593: Validation loss did not improve from -0.26436. Patience: 7/50
2024-12-20 21:13:17.111240: train_loss -0.8112
2024-12-20 21:13:17.112391: val_loss -0.2432
2024-12-20 21:13:17.113190: Pseudo dice [0.6286]
2024-12-20 21:13:17.114131: Epoch time: 250.28 s
2024-12-20 21:13:17.119156: Yayy! New best EMA pseudo Dice: 0.6115
2024-12-20 21:13:18.938175: 
2024-12-20 21:13:18.939621: Epoch 49
2024-12-20 21:13:18.940618: Current learning rate: 0.007
2024-12-20 21:17:32.474216: Validation loss did not improve from -0.26436. Patience: 8/50
2024-12-20 21:17:32.477689: train_loss -0.8127
2024-12-20 21:17:32.478633: val_loss -0.183
2024-12-20 21:17:32.479376: Pseudo dice [0.6079]
2024-12-20 21:17:32.480110: Epoch time: 253.54 s
2024-12-20 21:17:34.258006: 
2024-12-20 21:17:34.259461: Epoch 50
2024-12-20 21:17:34.260360: Current learning rate: 0.00694
2024-12-20 21:21:46.443309: Validation loss did not improve from -0.26436. Patience: 9/50
2024-12-20 21:21:46.444255: train_loss -0.8152
2024-12-20 21:21:46.444997: val_loss -0.1566
2024-12-20 21:21:46.445642: Pseudo dice [0.583]
2024-12-20 21:21:46.446392: Epoch time: 252.19 s
2024-12-20 21:21:48.369298: 
2024-12-20 21:21:48.370710: Epoch 51
2024-12-20 21:21:48.371536: Current learning rate: 0.00688
2024-12-20 21:26:01.583997: Validation loss did not improve from -0.26436. Patience: 10/50
2024-12-20 21:26:01.585177: train_loss -0.8203
2024-12-20 21:26:01.586198: val_loss -0.1684
2024-12-20 21:26:01.587126: Pseudo dice [0.617]
2024-12-20 21:26:01.588065: Epoch time: 253.22 s
2024-12-20 21:26:03.006142: 
2024-12-20 21:26:03.007296: Epoch 52
2024-12-20 21:26:03.008299: Current learning rate: 0.00682
2024-12-20 21:29:50.469227: Validation loss did not improve from -0.26436. Patience: 11/50
2024-12-20 21:29:50.470452: train_loss -0.8178
2024-12-20 21:29:50.471507: val_loss -0.2212
2024-12-20 21:29:50.472434: Pseudo dice [0.6266]
2024-12-20 21:29:50.473234: Epoch time: 227.47 s
2024-12-20 21:29:51.927577: 
2024-12-20 21:29:51.929616: Epoch 53
2024-12-20 21:29:51.930568: Current learning rate: 0.00675
2024-12-20 21:33:55.094748: Validation loss did not improve from -0.26436. Patience: 12/50
2024-12-20 21:33:55.095788: train_loss -0.8221
2024-12-20 21:33:55.096894: val_loss -0.2383
2024-12-20 21:33:55.097728: Pseudo dice [0.6217]
2024-12-20 21:33:55.098502: Epoch time: 243.17 s
2024-12-20 21:33:55.099230: Yayy! New best EMA pseudo Dice: 0.612
2024-12-20 21:33:56.939578: 
2024-12-20 21:33:56.940827: Epoch 54
2024-12-20 21:33:56.941545: Current learning rate: 0.00669
2024-12-20 21:38:11.905261: Validation loss did not improve from -0.26436. Patience: 13/50
2024-12-20 21:38:11.906419: train_loss -0.8228
2024-12-20 21:38:11.907233: val_loss -0.2301
2024-12-20 21:38:11.907923: Pseudo dice [0.6396]
2024-12-20 21:38:11.908665: Epoch time: 254.97 s
2024-12-20 21:38:12.361941: Yayy! New best EMA pseudo Dice: 0.6147
2024-12-20 21:38:14.144885: 
2024-12-20 21:38:14.146284: Epoch 55
2024-12-20 21:38:14.147127: Current learning rate: 0.00663
2024-12-20 21:42:31.658062: Validation loss did not improve from -0.26436. Patience: 14/50
2024-12-20 21:42:31.658931: train_loss -0.8194
2024-12-20 21:42:31.659733: val_loss -0.1795
2024-12-20 21:42:31.660352: Pseudo dice [0.6097]
2024-12-20 21:42:31.660983: Epoch time: 257.52 s
2024-12-20 21:42:33.071158: 
2024-12-20 21:42:33.072503: Epoch 56
2024-12-20 21:42:33.073277: Current learning rate: 0.00657
2024-12-20 21:46:52.268659: Validation loss did not improve from -0.26436. Patience: 15/50
2024-12-20 21:46:52.269721: train_loss -0.8227
2024-12-20 21:46:52.270519: val_loss -0.2376
2024-12-20 21:46:52.271273: Pseudo dice [0.6414]
2024-12-20 21:46:52.271989: Epoch time: 259.2 s
2024-12-20 21:46:52.272714: Yayy! New best EMA pseudo Dice: 0.617
2024-12-20 21:46:54.065409: 
2024-12-20 21:46:54.066741: Epoch 57
2024-12-20 21:46:54.067573: Current learning rate: 0.0065
2024-12-20 21:51:12.760152: Validation loss improved from -0.26436 to -0.26694! Patience: 15/50
2024-12-20 21:51:12.761189: train_loss -0.8261
2024-12-20 21:51:12.762156: val_loss -0.2669
2024-12-20 21:51:12.762896: Pseudo dice [0.6432]
2024-12-20 21:51:12.763711: Epoch time: 258.7 s
2024-12-20 21:51:12.764520: Yayy! New best EMA pseudo Dice: 0.6196
2024-12-20 21:51:14.566776: 
2024-12-20 21:51:14.568129: Epoch 58
2024-12-20 21:51:14.569002: Current learning rate: 0.00644
2024-12-20 21:55:18.212297: Validation loss did not improve from -0.26694. Patience: 1/50
2024-12-20 21:55:18.214006: train_loss -0.8244
2024-12-20 21:55:18.214931: val_loss -0.195
2024-12-20 21:55:18.215691: Pseudo dice [0.6103]
2024-12-20 21:55:18.216545: Epoch time: 243.65 s
2024-12-20 21:55:19.765175: 
2024-12-20 21:55:19.766618: Epoch 59
2024-12-20 21:55:19.767404: Current learning rate: 0.00638
2024-12-20 21:58:49.857232: Validation loss did not improve from -0.26694. Patience: 2/50
2024-12-20 21:58:49.858537: train_loss -0.827
2024-12-20 21:58:49.859390: val_loss -0.2151
2024-12-20 21:58:49.860148: Pseudo dice [0.6278]
2024-12-20 21:58:49.860991: Epoch time: 210.09 s
2024-12-20 21:58:51.795299: 
2024-12-20 21:58:51.797678: Epoch 60
2024-12-20 21:58:51.798601: Current learning rate: 0.00631
2024-12-20 22:02:46.576996: Validation loss did not improve from -0.26694. Patience: 3/50
2024-12-20 22:02:46.577983: train_loss -0.8328
2024-12-20 22:02:46.578969: val_loss -0.1807
2024-12-20 22:02:46.579694: Pseudo dice [0.6145]
2024-12-20 22:02:46.580398: Epoch time: 234.78 s
2024-12-20 22:02:48.343871: 
2024-12-20 22:02:48.345323: Epoch 61
2024-12-20 22:02:48.346242: Current learning rate: 0.00625
2024-12-20 22:06:46.671386: Validation loss did not improve from -0.26694. Patience: 4/50
2024-12-20 22:06:46.672433: train_loss -0.8312
2024-12-20 22:06:46.673463: val_loss -0.2005
2024-12-20 22:06:46.674556: Pseudo dice [0.6155]
2024-12-20 22:06:46.675620: Epoch time: 238.33 s
2024-12-20 22:06:48.105660: 
2024-12-20 22:06:48.107215: Epoch 62
2024-12-20 22:06:48.108341: Current learning rate: 0.00619
2024-12-20 22:10:44.963036: Validation loss did not improve from -0.26694. Patience: 5/50
2024-12-20 22:10:44.964168: train_loss -0.8299
2024-12-20 22:10:44.965377: val_loss -0.2224
2024-12-20 22:10:44.966318: Pseudo dice [0.6312]
2024-12-20 22:10:44.967281: Epoch time: 236.86 s
2024-12-20 22:10:44.968209: Yayy! New best EMA pseudo Dice: 0.62
2024-12-20 22:10:46.822156: 
2024-12-20 22:10:46.823767: Epoch 63
2024-12-20 22:10:46.824697: Current learning rate: 0.00612
2024-12-20 22:14:56.589647: Validation loss did not improve from -0.26694. Patience: 6/50
2024-12-20 22:14:56.591172: train_loss -0.8331
2024-12-20 22:14:56.592393: val_loss -0.2225
2024-12-20 22:14:56.593464: Pseudo dice [0.6351]
2024-12-20 22:14:56.594490: Epoch time: 249.77 s
2024-12-20 22:14:56.595530: Yayy! New best EMA pseudo Dice: 0.6215
2024-12-20 22:14:58.527423: 
2024-12-20 22:14:58.528930: Epoch 64
2024-12-20 22:14:58.529999: Current learning rate: 0.00606
2024-12-20 22:19:13.730853: Validation loss did not improve from -0.26694. Patience: 7/50
2024-12-20 22:19:13.731854: train_loss -0.8342
2024-12-20 22:19:13.732640: val_loss -0.2162
2024-12-20 22:19:13.733332: Pseudo dice [0.6326]
2024-12-20 22:19:13.734110: Epoch time: 255.21 s
2024-12-20 22:19:14.083616: Yayy! New best EMA pseudo Dice: 0.6226
2024-12-20 22:19:15.921377: 
2024-12-20 22:19:15.922915: Epoch 65
2024-12-20 22:19:15.924098: Current learning rate: 0.006
2024-12-20 22:23:34.779343: Validation loss did not improve from -0.26694. Patience: 8/50
2024-12-20 22:23:34.782436: train_loss -0.8381
2024-12-20 22:23:34.783305: val_loss -0.2147
2024-12-20 22:23:34.784033: Pseudo dice [0.6335]
2024-12-20 22:23:34.784832: Epoch time: 258.87 s
2024-12-20 22:23:34.785628: Yayy! New best EMA pseudo Dice: 0.6237
2024-12-20 22:23:36.642738: 
2024-12-20 22:23:36.644021: Epoch 66
2024-12-20 22:23:36.644702: Current learning rate: 0.00593
2024-12-20 22:27:55.018022: Validation loss did not improve from -0.26694. Patience: 9/50
2024-12-20 22:27:55.019090: train_loss -0.8349
2024-12-20 22:27:55.019917: val_loss -0.1671
2024-12-20 22:27:55.020668: Pseudo dice [0.6073]
2024-12-20 22:27:55.021490: Epoch time: 258.38 s
2024-12-20 22:27:56.481263: 
2024-12-20 22:27:56.482671: Epoch 67
2024-12-20 22:27:56.483497: Current learning rate: 0.00587
2024-12-20 22:31:45.485332: Validation loss did not improve from -0.26694. Patience: 10/50
2024-12-20 22:31:45.486382: train_loss -0.8348
2024-12-20 22:31:45.487216: val_loss -0.2061
2024-12-20 22:31:45.488027: Pseudo dice [0.6332]
2024-12-20 22:31:45.488819: Epoch time: 229.01 s
2024-12-20 22:31:46.917677: 
2024-12-20 22:31:46.918868: Epoch 68
2024-12-20 22:31:46.919632: Current learning rate: 0.00581
2024-12-20 22:35:42.669260: Validation loss did not improve from -0.26694. Patience: 11/50
2024-12-20 22:35:42.670329: train_loss -0.8391
2024-12-20 22:35:42.671127: val_loss -0.1832
2024-12-20 22:35:42.671899: Pseudo dice [0.6248]
2024-12-20 22:35:42.672563: Epoch time: 235.75 s
2024-12-20 22:35:44.122212: 
2024-12-20 22:35:44.123695: Epoch 69
2024-12-20 22:35:44.124540: Current learning rate: 0.00574
2024-12-20 22:39:45.144597: Validation loss did not improve from -0.26694. Patience: 12/50
2024-12-20 22:39:45.145611: train_loss -0.8376
2024-12-20 22:39:45.146574: val_loss -0.1894
2024-12-20 22:39:45.147313: Pseudo dice [0.615]
2024-12-20 22:39:45.148076: Epoch time: 241.02 s
2024-12-20 22:39:46.995237: 
2024-12-20 22:39:46.996616: Epoch 70
2024-12-20 22:39:46.997396: Current learning rate: 0.00568
2024-12-20 22:43:28.193554: Validation loss did not improve from -0.26694. Patience: 13/50
2024-12-20 22:43:28.194604: train_loss -0.8434
2024-12-20 22:43:28.195523: val_loss -0.2118
2024-12-20 22:43:28.196290: Pseudo dice [0.6493]
2024-12-20 22:43:28.197069: Epoch time: 221.2 s
2024-12-20 22:43:28.197811: Yayy! New best EMA pseudo Dice: 0.6252
2024-12-20 22:43:30.562401: 
2024-12-20 22:43:30.563978: Epoch 71
2024-12-20 22:43:30.564718: Current learning rate: 0.00562
2024-12-20 22:47:22.993279: Validation loss did not improve from -0.26694. Patience: 14/50
2024-12-20 22:47:22.994349: train_loss -0.8396
2024-12-20 22:47:22.995379: val_loss -0.1768
2024-12-20 22:47:22.996545: Pseudo dice [0.6366]
2024-12-20 22:47:22.997475: Epoch time: 232.43 s
2024-12-20 22:47:22.998377: Yayy! New best EMA pseudo Dice: 0.6263
2024-12-20 22:47:24.852991: 
2024-12-20 22:47:24.854344: Epoch 72
2024-12-20 22:47:24.855192: Current learning rate: 0.00555
2024-12-20 22:51:20.455558: Validation loss did not improve from -0.26694. Patience: 15/50
2024-12-20 22:51:20.457095: train_loss -0.8436
2024-12-20 22:51:20.458569: val_loss -0.1508
2024-12-20 22:51:20.459978: Pseudo dice [0.5984]
2024-12-20 22:51:20.461409: Epoch time: 235.61 s
2024-12-20 22:51:21.890801: 
2024-12-20 22:51:21.892401: Epoch 73
2024-12-20 22:51:21.893538: Current learning rate: 0.00549
2024-12-20 22:55:20.922662: Validation loss did not improve from -0.26694. Patience: 16/50
2024-12-20 22:55:20.923772: train_loss -0.8424
2024-12-20 22:55:20.924608: val_loss -0.1495
2024-12-20 22:55:20.925423: Pseudo dice [0.6195]
2024-12-20 22:55:20.926185: Epoch time: 239.03 s
2024-12-20 22:55:22.369588: 
2024-12-20 22:55:22.371023: Epoch 74
2024-12-20 22:55:22.371886: Current learning rate: 0.00542
2024-12-20 22:59:20.398620: Validation loss did not improve from -0.26694. Patience: 17/50
2024-12-20 22:59:20.399642: train_loss -0.8449
2024-12-20 22:59:20.400451: val_loss -0.1815
2024-12-20 22:59:20.401200: Pseudo dice [0.6235]
2024-12-20 22:59:20.401993: Epoch time: 238.03 s
2024-12-20 22:59:22.309522: 
2024-12-20 22:59:22.310815: Epoch 75
2024-12-20 22:59:22.311566: Current learning rate: 0.00536
2024-12-20 23:03:29.880881: Validation loss did not improve from -0.26694. Patience: 18/50
2024-12-20 23:03:29.881956: train_loss -0.846
2024-12-20 23:03:29.882862: val_loss -0.1623
2024-12-20 23:03:29.883682: Pseudo dice [0.6101]
2024-12-20 23:03:29.884591: Epoch time: 247.57 s
2024-12-20 23:03:31.343927: 
2024-12-20 23:03:31.345365: Epoch 76
2024-12-20 23:03:31.346198: Current learning rate: 0.00529
2024-12-20 23:07:42.811022: Validation loss did not improve from -0.26694. Patience: 19/50
2024-12-20 23:07:42.812090: train_loss -0.8463
2024-12-20 23:07:42.812927: val_loss -0.1227
2024-12-20 23:07:42.813615: Pseudo dice [0.6114]
2024-12-20 23:07:42.814417: Epoch time: 251.47 s
2024-12-20 23:07:44.243667: 
2024-12-20 23:07:44.245312: Epoch 77
2024-12-20 23:07:44.246430: Current learning rate: 0.00523
2024-12-20 23:11:58.042855: Validation loss did not improve from -0.26694. Patience: 20/50
2024-12-20 23:11:58.044262: train_loss -0.8438
2024-12-20 23:11:58.045365: val_loss -0.2044
2024-12-20 23:11:58.046216: Pseudo dice [0.6343]
2024-12-20 23:11:58.047267: Epoch time: 253.8 s
2024-12-20 23:11:59.503096: 
2024-12-20 23:11:59.504623: Epoch 78
2024-12-20 23:11:59.505622: Current learning rate: 0.00517
2024-12-20 23:16:05.775175: Validation loss did not improve from -0.26694. Patience: 21/50
2024-12-20 23:16:05.779412: train_loss -0.8457
2024-12-20 23:16:05.781747: val_loss -0.1637
2024-12-20 23:16:05.782675: Pseudo dice [0.621]
2024-12-20 23:16:05.783658: Epoch time: 246.28 s
2024-12-20 23:16:07.309393: 
2024-12-20 23:16:07.310661: Epoch 79
2024-12-20 23:16:07.311513: Current learning rate: 0.0051
2024-12-20 23:20:09.467551: Validation loss did not improve from -0.26694. Patience: 22/50
2024-12-20 23:20:09.468875: train_loss -0.8461
2024-12-20 23:20:09.470259: val_loss -0.2059
2024-12-20 23:20:09.471482: Pseudo dice [0.6188]
2024-12-20 23:20:09.472563: Epoch time: 242.16 s
2024-12-20 23:20:11.326480: 
2024-12-20 23:20:11.327948: Epoch 80
2024-12-20 23:20:11.328882: Current learning rate: 0.00504
2024-12-20 23:24:20.448930: Validation loss did not improve from -0.26694. Patience: 23/50
2024-12-20 23:24:20.450009: train_loss -0.8487
2024-12-20 23:24:20.451104: val_loss -0.1911
2024-12-20 23:24:20.451972: Pseudo dice [0.6105]
2024-12-20 23:24:20.452802: Epoch time: 249.12 s
2024-12-20 23:24:21.911043: 
2024-12-20 23:24:21.912093: Epoch 81
2024-12-20 23:24:21.912880: Current learning rate: 0.00497
2024-12-20 23:28:48.697522: Validation loss did not improve from -0.26694. Patience: 24/50
2024-12-20 23:28:48.700943: train_loss -0.851
2024-12-20 23:28:48.702091: val_loss -0.1592
2024-12-20 23:28:48.702910: Pseudo dice [0.6132]
2024-12-20 23:28:48.703808: Epoch time: 266.79 s
2024-12-20 23:28:50.605572: 
2024-12-20 23:28:50.607030: Epoch 82
2024-12-20 23:28:50.607929: Current learning rate: 0.00491
2024-12-20 23:32:34.738043: Validation loss did not improve from -0.26694. Patience: 25/50
2024-12-20 23:32:34.739108: train_loss -0.852
2024-12-20 23:32:34.740181: val_loss -0.1602
2024-12-20 23:32:34.741338: Pseudo dice [0.6256]
2024-12-20 23:32:34.742382: Epoch time: 224.13 s
2024-12-20 23:32:36.174261: 
2024-12-20 23:32:36.175752: Epoch 83
2024-12-20 23:32:36.176753: Current learning rate: 0.00484
2024-12-20 23:36:14.097834: Validation loss did not improve from -0.26694. Patience: 26/50
2024-12-20 23:36:14.099028: train_loss -0.8534
2024-12-20 23:36:14.099938: val_loss -0.2221
2024-12-20 23:36:14.100655: Pseudo dice [0.639]
2024-12-20 23:36:14.101475: Epoch time: 217.93 s
2024-12-20 23:36:15.469836: 
2024-12-20 23:36:15.471105: Epoch 84
2024-12-20 23:36:15.471881: Current learning rate: 0.00478
2024-12-20 23:40:01.258625: Validation loss did not improve from -0.26694. Patience: 27/50
2024-12-20 23:40:01.259742: train_loss -0.854
2024-12-20 23:40:01.260639: val_loss -0.2101
2024-12-20 23:40:01.261703: Pseudo dice [0.6353]
2024-12-20 23:40:01.262639: Epoch time: 225.79 s
2024-12-20 23:40:03.053814: 
2024-12-20 23:40:03.054973: Epoch 85
2024-12-20 23:40:03.055683: Current learning rate: 0.00471
2024-12-20 23:43:20.304329: Validation loss did not improve from -0.26694. Patience: 28/50
2024-12-20 23:43:20.305389: train_loss -0.8558
2024-12-20 23:43:20.306361: val_loss -0.181
2024-12-20 23:43:20.307226: Pseudo dice [0.6261]
2024-12-20 23:43:20.308014: Epoch time: 197.25 s
2024-12-20 23:43:21.722466: 
2024-12-20 23:43:21.723871: Epoch 86
2024-12-20 23:43:21.724647: Current learning rate: 0.00465
2024-12-20 23:47:09.669389: Validation loss did not improve from -0.26694. Patience: 29/50
2024-12-20 23:47:09.670481: train_loss -0.8564
2024-12-20 23:47:09.671317: val_loss -0.1923
2024-12-20 23:47:09.672169: Pseudo dice [0.6297]
2024-12-20 23:47:09.673014: Epoch time: 227.95 s
2024-12-20 23:47:11.034113: 
2024-12-20 23:47:11.035353: Epoch 87
2024-12-20 23:47:11.036047: Current learning rate: 0.00458
2024-12-20 23:50:59.809931: Validation loss did not improve from -0.26694. Patience: 30/50
2024-12-20 23:50:59.811030: train_loss -0.8568
2024-12-20 23:50:59.811756: val_loss -0.1561
2024-12-20 23:50:59.812447: Pseudo dice [0.6046]
2024-12-20 23:50:59.813275: Epoch time: 228.78 s
2024-12-20 23:51:01.233348: 
2024-12-20 23:51:01.234796: Epoch 88
2024-12-20 23:51:01.235720: Current learning rate: 0.00452
2024-12-20 23:54:21.399831: Validation loss did not improve from -0.26694. Patience: 31/50
2024-12-20 23:54:21.400662: train_loss -0.856
2024-12-20 23:54:21.401724: val_loss -0.1862
2024-12-20 23:54:21.402808: Pseudo dice [0.6154]
2024-12-20 23:54:21.403841: Epoch time: 200.17 s
2024-12-20 23:54:22.794283: 
2024-12-20 23:54:22.795884: Epoch 89
2024-12-20 23:54:22.797106: Current learning rate: 0.00445
2024-12-20 23:58:09.114455: Validation loss did not improve from -0.26694. Patience: 32/50
2024-12-20 23:58:09.115511: train_loss -0.859
2024-12-20 23:58:09.116460: val_loss -0.1363
2024-12-20 23:58:09.117232: Pseudo dice [0.6164]
2024-12-20 23:58:09.118037: Epoch time: 226.32 s
2024-12-20 23:58:10.859277: 
2024-12-20 23:58:10.860632: Epoch 90
2024-12-20 23:58:10.861493: Current learning rate: 0.00438
2024-12-21 00:02:10.172713: Validation loss did not improve from -0.26694. Patience: 33/50
2024-12-21 00:02:10.173768: train_loss -0.8591
2024-12-21 00:02:10.174712: val_loss -0.1643
2024-12-21 00:02:10.175538: Pseudo dice [0.6282]
2024-12-21 00:02:10.176382: Epoch time: 239.32 s
2024-12-21 00:02:11.594579: 
2024-12-21 00:02:11.596050: Epoch 91
2024-12-21 00:02:11.597054: Current learning rate: 0.00432
2024-12-21 00:05:54.124067: Validation loss did not improve from -0.26694. Patience: 34/50
2024-12-21 00:05:54.125045: train_loss -0.8609
2024-12-21 00:05:54.125830: val_loss -0.1437
2024-12-21 00:05:54.126561: Pseudo dice [0.6162]
2024-12-21 00:05:54.127338: Epoch time: 222.53 s
2024-12-21 00:05:55.517138: 
2024-12-21 00:05:55.518465: Epoch 92
2024-12-21 00:05:55.519165: Current learning rate: 0.00425
2024-12-21 00:09:59.456812: Validation loss did not improve from -0.26694. Patience: 35/50
2024-12-21 00:09:59.457923: train_loss -0.8649
2024-12-21 00:09:59.458875: val_loss -0.1305
2024-12-21 00:09:59.459704: Pseudo dice [0.6117]
2024-12-21 00:09:59.460436: Epoch time: 243.94 s
2024-12-21 00:10:01.281106: 
2024-12-21 00:10:01.282453: Epoch 93
2024-12-21 00:10:01.283231: Current learning rate: 0.00419
2024-12-21 00:14:16.855710: Validation loss did not improve from -0.26694. Patience: 36/50
2024-12-21 00:14:16.856813: train_loss -0.8638
2024-12-21 00:14:16.857606: val_loss -0.1346
2024-12-21 00:14:16.858254: Pseudo dice [0.6295]
2024-12-21 00:14:16.859086: Epoch time: 255.58 s
2024-12-21 00:14:18.235314: 
2024-12-21 00:14:18.237497: Epoch 94
2024-12-21 00:14:18.238519: Current learning rate: 0.00412
2024-12-21 00:18:43.053545: Validation loss did not improve from -0.26694. Patience: 37/50
2024-12-21 00:18:43.055039: train_loss -0.8625
2024-12-21 00:18:43.056077: val_loss -0.1358
2024-12-21 00:18:43.056833: Pseudo dice [0.6219]
2024-12-21 00:18:43.057717: Epoch time: 264.82 s
2024-12-21 00:18:44.973353: 
2024-12-21 00:18:44.974715: Epoch 95
2024-12-21 00:18:44.975471: Current learning rate: 0.00405
2024-12-21 00:22:50.235622: Validation loss did not improve from -0.26694. Patience: 38/50
2024-12-21 00:22:50.239198: train_loss -0.8642
2024-12-21 00:22:50.241148: val_loss -0.1207
2024-12-21 00:22:50.242069: Pseudo dice [0.5983]
2024-12-21 00:22:50.243197: Epoch time: 245.27 s
2024-12-21 00:22:51.733156: 
2024-12-21 00:22:51.734646: Epoch 96
2024-12-21 00:22:51.735518: Current learning rate: 0.00399
2024-12-21 00:26:56.151647: Validation loss did not improve from -0.26694. Patience: 39/50
2024-12-21 00:26:56.152942: train_loss -0.8656
2024-12-21 00:26:56.154026: val_loss -0.1378
2024-12-21 00:26:56.154770: Pseudo dice [0.6183]
2024-12-21 00:26:56.155648: Epoch time: 244.42 s
2024-12-21 00:26:57.773229: 
2024-12-21 00:26:57.774657: Epoch 97
2024-12-21 00:26:57.775789: Current learning rate: 0.00392
2024-12-21 00:30:51.088134: Validation loss did not improve from -0.26694. Patience: 40/50
2024-12-21 00:30:51.089169: train_loss -0.8671
2024-12-21 00:30:51.090024: val_loss -0.1463
2024-12-21 00:30:51.090748: Pseudo dice [0.6122]
2024-12-21 00:30:51.091608: Epoch time: 233.32 s
2024-12-21 00:30:52.487937: 
2024-12-21 00:30:52.489617: Epoch 98
2024-12-21 00:30:52.490514: Current learning rate: 0.00385
2024-12-21 00:34:14.298177: Validation loss did not improve from -0.26694. Patience: 41/50
2024-12-21 00:34:14.301757: train_loss -0.8684
2024-12-21 00:34:14.302728: val_loss -0.1431
2024-12-21 00:34:14.303513: Pseudo dice [0.6248]
2024-12-21 00:34:14.304383: Epoch time: 201.82 s
2024-12-21 00:34:15.711236: 
2024-12-21 00:34:15.713233: Epoch 99
2024-12-21 00:34:15.714031: Current learning rate: 0.00379
2024-12-21 00:38:15.535025: Validation loss did not improve from -0.26694. Patience: 42/50
2024-12-21 00:38:15.537374: train_loss -0.8697
2024-12-21 00:38:15.538679: val_loss -0.1366
2024-12-21 00:38:15.539639: Pseudo dice [0.6122]
2024-12-21 00:38:15.540606: Epoch time: 239.83 s
2024-12-21 00:38:17.449767: 
2024-12-21 00:38:17.451205: Epoch 100
2024-12-21 00:38:17.452098: Current learning rate: 0.00372
2024-12-21 00:42:19.685758: Validation loss did not improve from -0.26694. Patience: 43/50
2024-12-21 00:42:19.686788: train_loss -0.8695
2024-12-21 00:42:19.687604: val_loss -0.1533
2024-12-21 00:42:19.688345: Pseudo dice [0.629]
2024-12-21 00:42:19.689072: Epoch time: 242.24 s
2024-12-21 00:42:21.095396: 
2024-12-21 00:42:21.097090: Epoch 101
2024-12-21 00:42:21.097852: Current learning rate: 0.00365
2024-12-21 00:46:17.615071: Validation loss did not improve from -0.26694. Patience: 44/50
2024-12-21 00:46:17.616231: train_loss -0.8708
2024-12-21 00:46:17.616961: val_loss -0.1778
2024-12-21 00:46:17.617800: Pseudo dice [0.6344]
2024-12-21 00:46:17.618602: Epoch time: 236.52 s
2024-12-21 00:46:19.059650: 
2024-12-21 00:46:19.061006: Epoch 102
2024-12-21 00:46:19.061812: Current learning rate: 0.00359
2024-12-21 00:50:29.592771: Validation loss did not improve from -0.26694. Patience: 45/50
2024-12-21 00:50:29.593956: train_loss -0.8715
2024-12-21 00:50:29.594811: val_loss -0.1318
2024-12-21 00:50:29.595639: Pseudo dice [0.6207]
2024-12-21 00:50:29.596498: Epoch time: 250.54 s
2024-12-21 00:50:31.084068: 
2024-12-21 00:50:31.128673: Epoch 103
2024-12-21 00:50:31.140951: Current learning rate: 0.00352
2024-12-21 00:54:49.578684: Validation loss did not improve from -0.26694. Patience: 46/50
2024-12-21 00:54:49.579828: train_loss -0.871
2024-12-21 00:54:49.580799: val_loss -0.1214
2024-12-21 00:54:49.581496: Pseudo dice [0.6279]
2024-12-21 00:54:49.582257: Epoch time: 258.5 s
2024-12-21 00:54:51.414165: 
2024-12-21 00:54:51.415497: Epoch 104
2024-12-21 00:54:51.416287: Current learning rate: 0.00345
2024-12-21 00:59:06.761616: Validation loss did not improve from -0.26694. Patience: 47/50
2024-12-21 00:59:06.763296: train_loss -0.872
2024-12-21 00:59:06.764302: val_loss -0.1299
2024-12-21 00:59:06.765054: Pseudo dice [0.5989]
2024-12-21 00:59:06.765878: Epoch time: 255.35 s
2024-12-21 00:59:08.663427: 
2024-12-21 00:59:08.665217: Epoch 105
2024-12-21 00:59:08.666396: Current learning rate: 0.00338
2024-12-21 01:03:13.534045: Validation loss did not improve from -0.26694. Patience: 48/50
2024-12-21 01:03:13.535028: train_loss -0.8721
2024-12-21 01:03:13.535806: val_loss -0.1405
2024-12-21 01:03:13.536486: Pseudo dice [0.6243]
2024-12-21 01:03:13.537197: Epoch time: 244.88 s
2024-12-21 01:03:14.954664: 
2024-12-21 01:03:14.955983: Epoch 106
2024-12-21 01:03:14.956747: Current learning rate: 0.00332
2024-12-21 01:07:12.956189: Validation loss did not improve from -0.26694. Patience: 49/50
2024-12-21 01:07:12.957111: train_loss -0.8749
2024-12-21 01:07:12.958059: val_loss -0.1199
2024-12-21 01:07:12.958718: Pseudo dice [0.6177]
2024-12-21 01:07:12.959493: Epoch time: 238.0 s
2024-12-21 01:07:14.486680: 
2024-12-21 01:07:14.488123: Epoch 107
2024-12-21 01:07:14.488923: Current learning rate: 0.00325
2024-12-21 01:11:05.554490: Validation loss did not improve from -0.26694. Patience: 50/50
2024-12-21 01:11:05.555521: train_loss -0.8756
2024-12-21 01:11:05.556258: val_loss -0.143
2024-12-21 01:11:05.557069: Pseudo dice [0.6141]
2024-12-21 01:11:05.557899: Epoch time: 231.07 s
2024-12-21 01:11:07.007267: 
2024-12-21 01:11:07.008174: Epoch 108
2024-12-21 01:11:07.008904: Current learning rate: 0.00318
2024-12-21 01:14:55.637169: Validation loss did not improve from -0.26694. Patience: 51/50
2024-12-21 01:14:55.638246: train_loss -0.8792
2024-12-21 01:14:55.638989: val_loss -0.1249
2024-12-21 01:14:55.639683: Pseudo dice [0.6104]
2024-12-21 01:14:55.640421: Epoch time: 228.63 s
2024-12-21 01:14:57.063203: 
2024-12-21 01:14:57.064604: Epoch 109
2024-12-21 01:14:57.065394: Current learning rate: 0.00311
2024-12-21 01:18:45.661528: Validation loss did not improve from -0.26694. Patience: 52/50
2024-12-21 01:18:45.662617: train_loss -0.8785
2024-12-21 01:18:45.663583: val_loss -0.1953
2024-12-21 01:18:45.664360: Pseudo dice [0.651]
2024-12-21 01:18:45.665134: Epoch time: 228.6 s
2024-12-21 01:18:47.492271: 
2024-12-21 01:18:47.493604: Epoch 110
2024-12-21 01:18:47.494461: Current learning rate: 0.00304
2024-12-21 01:22:52.628439: Validation loss did not improve from -0.26694. Patience: 53/50
2024-12-21 01:22:52.629602: train_loss -0.8784
2024-12-21 01:22:52.630605: val_loss -0.1545
2024-12-21 01:22:52.631442: Pseudo dice [0.6325]
2024-12-21 01:22:52.632355: Epoch time: 245.14 s
2024-12-21 01:22:54.125159: 
2024-12-21 01:22:54.198568: Epoch 111
2024-12-21 01:22:54.199753: Current learning rate: 0.00297
2024-12-21 01:27:08.119998: Validation loss did not improve from -0.26694. Patience: 54/50
2024-12-21 01:27:08.121688: train_loss -0.8785
2024-12-21 01:27:08.123595: val_loss -0.1134
2024-12-21 01:27:08.124447: Pseudo dice [0.5886]
2024-12-21 01:27:08.125891: Epoch time: 254.0 s
2024-12-21 01:27:09.570057: 
2024-12-21 01:27:09.571452: Epoch 112
2024-12-21 01:27:09.572277: Current learning rate: 0.00291
2024-12-21 01:31:30.410178: Validation loss did not improve from -0.26694. Patience: 55/50
2024-12-21 01:31:30.411618: train_loss -0.8809
2024-12-21 01:31:30.412547: val_loss -0.0973
2024-12-21 01:31:30.413334: Pseudo dice [0.6225]
2024-12-21 01:31:30.414105: Epoch time: 260.84 s
2024-12-21 01:31:31.851059: 
2024-12-21 01:31:31.851951: Epoch 113
2024-12-21 01:31:31.852772: Current learning rate: 0.00284
2024-12-21 01:35:33.090378: Validation loss did not improve from -0.26694. Patience: 56/50
2024-12-21 01:35:33.091533: train_loss -0.8839
2024-12-21 01:35:33.092294: val_loss -0.1593
2024-12-21 01:35:33.093063: Pseudo dice [0.6307]
2024-12-21 01:35:33.093826: Epoch time: 241.24 s
2024-12-21 01:35:34.539502: 
2024-12-21 01:35:34.540745: Epoch 114
2024-12-21 01:35:34.541507: Current learning rate: 0.00277
2024-12-21 01:39:33.761111: Validation loss did not improve from -0.26694. Patience: 57/50
2024-12-21 01:39:33.764538: train_loss -0.8827
2024-12-21 01:39:33.765480: val_loss -0.1403
2024-12-21 01:39:33.766240: Pseudo dice [0.6262]
2024-12-21 01:39:33.767008: Epoch time: 239.23 s
2024-12-21 01:39:36.061968: 
2024-12-21 01:39:36.063251: Epoch 115
2024-12-21 01:39:36.064159: Current learning rate: 0.0027
2024-12-21 01:43:28.552616: Validation loss did not improve from -0.26694. Patience: 58/50
2024-12-21 01:43:28.553761: train_loss -0.8812
2024-12-21 01:43:28.554631: val_loss -0.0562
2024-12-21 01:43:28.555415: Pseudo dice [0.597]
2024-12-21 01:43:28.556152: Epoch time: 232.49 s
2024-12-21 01:43:29.984244: 
2024-12-21 01:43:29.985496: Epoch 116
2024-12-21 01:43:29.986279: Current learning rate: 0.00263
2024-12-21 01:47:17.156629: Validation loss did not improve from -0.26694. Patience: 59/50
2024-12-21 01:47:17.157532: train_loss -0.8838
2024-12-21 01:47:17.158560: val_loss -0.1416
2024-12-21 01:47:17.159513: Pseudo dice [0.6328]
2024-12-21 01:47:17.160530: Epoch time: 227.17 s
2024-12-21 01:47:18.756526: 
2024-12-21 01:47:18.757935: Epoch 117
2024-12-21 01:47:18.758831: Current learning rate: 0.00256
2024-12-21 01:51:20.639878: Validation loss did not improve from -0.26694. Patience: 60/50
2024-12-21 01:51:20.640895: train_loss -0.8851
2024-12-21 01:51:20.641962: val_loss -0.0907
2024-12-21 01:51:20.642993: Pseudo dice [0.6073]
2024-12-21 01:51:20.643963: Epoch time: 241.89 s
2024-12-21 01:51:22.138427: 
2024-12-21 01:51:22.140496: Epoch 118
2024-12-21 01:51:22.141531: Current learning rate: 0.00249
2024-12-21 01:55:30.602308: Validation loss did not improve from -0.26694. Patience: 61/50
2024-12-21 01:55:30.604074: train_loss -0.8873
2024-12-21 01:55:30.605460: val_loss -0.1169
2024-12-21 01:55:30.606417: Pseudo dice [0.622]
2024-12-21 01:55:30.607452: Epoch time: 248.47 s
2024-12-21 01:55:32.020766: 
2024-12-21 01:55:32.022801: Epoch 119
2024-12-21 01:55:32.024079: Current learning rate: 0.00242
2024-12-21 01:59:32.489142: Validation loss did not improve from -0.26694. Patience: 62/50
2024-12-21 01:59:32.490197: train_loss -0.8877
2024-12-21 01:59:32.491024: val_loss -0.1452
2024-12-21 01:59:32.491806: Pseudo dice [0.623]
2024-12-21 01:59:32.492703: Epoch time: 240.47 s
2024-12-21 01:59:34.384805: 
2024-12-21 01:59:34.386729: Epoch 120
2024-12-21 01:59:34.387829: Current learning rate: 0.00235
2024-12-21 02:03:41.424275: Validation loss did not improve from -0.26694. Patience: 63/50
2024-12-21 02:03:41.425931: train_loss -0.8871
2024-12-21 02:03:41.427230: val_loss -0.1145
2024-12-21 02:03:41.428253: Pseudo dice [0.6152]
2024-12-21 02:03:41.429166: Epoch time: 247.04 s
2024-12-21 02:03:42.872097: 
2024-12-21 02:03:42.873578: Epoch 121
2024-12-21 02:03:42.874609: Current learning rate: 0.00228
2024-12-21 02:07:56.194479: Validation loss did not improve from -0.26694. Patience: 64/50
2024-12-21 02:07:56.195397: train_loss -0.8881
2024-12-21 02:07:56.196359: val_loss -0.1134
2024-12-21 02:07:56.197278: Pseudo dice [0.6218]
2024-12-21 02:07:56.198078: Epoch time: 253.32 s
2024-12-21 02:07:57.580464: 
2024-12-21 02:07:57.581913: Epoch 122
2024-12-21 02:07:57.582807: Current learning rate: 0.00221
2024-12-21 02:12:11.596364: Validation loss did not improve from -0.26694. Patience: 65/50
2024-12-21 02:12:11.597680: train_loss -0.8909
2024-12-21 02:12:11.598624: val_loss -0.1166
2024-12-21 02:12:11.599468: Pseudo dice [0.6211]
2024-12-21 02:12:11.600420: Epoch time: 254.02 s
2024-12-21 02:12:13.010964: 
2024-12-21 02:12:13.012511: Epoch 123
2024-12-21 02:12:13.013611: Current learning rate: 0.00214
2024-12-21 02:16:11.397244: Validation loss did not improve from -0.26694. Patience: 66/50
2024-12-21 02:16:11.398360: train_loss -0.8899
2024-12-21 02:16:11.399290: val_loss -0.1158
2024-12-21 02:16:11.400187: Pseudo dice [0.6216]
2024-12-21 02:16:11.401094: Epoch time: 238.39 s
2024-12-21 02:16:12.824134: 
2024-12-21 02:16:12.825399: Epoch 124
2024-12-21 02:16:12.826464: Current learning rate: 0.00207
2024-12-21 02:20:10.249576: Validation loss did not improve from -0.26694. Patience: 67/50
2024-12-21 02:20:10.250852: train_loss -0.8911
2024-12-21 02:20:10.251994: val_loss -0.1096
2024-12-21 02:20:10.252882: Pseudo dice [0.6187]
2024-12-21 02:20:10.253896: Epoch time: 237.43 s
2024-12-21 02:20:12.078083: 
2024-12-21 02:20:12.079349: Epoch 125
2024-12-21 02:20:12.080090: Current learning rate: 0.00199
2024-12-21 02:24:03.908760: Validation loss did not improve from -0.26694. Patience: 68/50
2024-12-21 02:24:03.909924: train_loss -0.891
2024-12-21 02:24:03.910899: val_loss -0.1296
2024-12-21 02:24:03.911675: Pseudo dice [0.6452]
2024-12-21 02:24:03.912471: Epoch time: 231.83 s
2024-12-21 02:24:05.950863: 
2024-12-21 02:24:05.952237: Epoch 126
2024-12-21 02:24:05.953021: Current learning rate: 0.00192
2024-12-21 02:27:57.444732: Validation loss did not improve from -0.26694. Patience: 69/50
2024-12-21 02:27:57.445507: train_loss -0.8889
2024-12-21 02:27:57.446716: val_loss -0.1253
2024-12-21 02:27:57.447803: Pseudo dice [0.6269]
2024-12-21 02:27:57.448871: Epoch time: 231.5 s
2024-12-21 02:27:58.921678: 
2024-12-21 02:27:58.923147: Epoch 127
2024-12-21 02:27:58.924149: Current learning rate: 0.00185
2024-12-21 02:31:59.171418: Validation loss did not improve from -0.26694. Patience: 70/50
2024-12-21 02:31:59.172397: train_loss -0.8903
2024-12-21 02:31:59.173221: val_loss -0.1173
2024-12-21 02:31:59.174092: Pseudo dice [0.6181]
2024-12-21 02:31:59.174912: Epoch time: 240.25 s
2024-12-21 02:32:00.609552: 
2024-12-21 02:32:00.611046: Epoch 128
2024-12-21 02:32:00.612036: Current learning rate: 0.00178
2024-12-21 02:36:28.175630: Validation loss did not improve from -0.26694. Patience: 71/50
2024-12-21 02:36:28.176895: train_loss -0.891
2024-12-21 02:36:28.177697: val_loss -0.0974
2024-12-21 02:36:28.178349: Pseudo dice [0.6162]
2024-12-21 02:36:28.179043: Epoch time: 267.57 s
2024-12-21 02:36:29.644880: 
2024-12-21 02:36:29.646297: Epoch 129
2024-12-21 02:36:29.647099: Current learning rate: 0.0017
2024-12-21 02:40:43.486401: Validation loss did not improve from -0.26694. Patience: 72/50
2024-12-21 02:40:43.487541: train_loss -0.8932
2024-12-21 02:40:43.488479: val_loss -0.142
2024-12-21 02:40:43.489140: Pseudo dice [0.6331]
2024-12-21 02:40:43.489906: Epoch time: 253.84 s
2024-12-21 02:40:45.406576: 
2024-12-21 02:40:45.408087: Epoch 130
2024-12-21 02:40:45.408982: Current learning rate: 0.00163
2024-12-21 02:44:34.165415: Validation loss did not improve from -0.26694. Patience: 73/50
2024-12-21 02:44:34.167101: train_loss -0.8922
2024-12-21 02:44:34.167856: val_loss -0.1037
2024-12-21 02:44:34.168520: Pseudo dice [0.6257]
2024-12-21 02:44:34.169317: Epoch time: 228.76 s
2024-12-21 02:44:35.624553: 
2024-12-21 02:44:35.626002: Epoch 131
2024-12-21 02:44:35.626827: Current learning rate: 0.00156
2024-12-21 02:48:00.421442: Validation loss did not improve from -0.26694. Patience: 74/50
2024-12-21 02:48:00.422601: train_loss -0.8932
2024-12-21 02:48:00.423444: val_loss -0.1493
2024-12-21 02:48:00.424244: Pseudo dice [0.625]
2024-12-21 02:48:00.424923: Epoch time: 204.8 s
2024-12-21 02:48:01.838745: 
2024-12-21 02:48:01.840112: Epoch 132
2024-12-21 02:48:01.840910: Current learning rate: 0.00148
2024-12-21 02:51:36.929173: Validation loss did not improve from -0.26694. Patience: 75/50
2024-12-21 02:51:36.930218: train_loss -0.8925
2024-12-21 02:51:36.931072: val_loss -0.0952
2024-12-21 02:51:36.931886: Pseudo dice [0.6149]
2024-12-21 02:51:36.932695: Epoch time: 215.09 s
2024-12-21 02:51:38.328907: 
2024-12-21 02:51:38.330203: Epoch 133
2024-12-21 02:51:38.330964: Current learning rate: 0.00141
2024-12-21 02:55:11.259507: Validation loss did not improve from -0.26694. Patience: 76/50
2024-12-21 02:55:11.260496: train_loss -0.8969
2024-12-21 02:55:11.261466: val_loss -0.1391
2024-12-21 02:55:11.262338: Pseudo dice [0.6327]
2024-12-21 02:55:11.263128: Epoch time: 212.93 s
2024-12-21 02:55:12.675346: 
2024-12-21 02:55:12.676669: Epoch 134
2024-12-21 02:55:12.677389: Current learning rate: 0.00133
2024-12-21 02:58:25.299529: Validation loss did not improve from -0.26694. Patience: 77/50
2024-12-21 02:58:25.300785: train_loss -0.8954
2024-12-21 02:58:25.301636: val_loss -0.1316
2024-12-21 02:58:25.302439: Pseudo dice [0.625]
2024-12-21 02:58:25.303187: Epoch time: 192.63 s
2024-12-21 02:58:27.141370: 
2024-12-21 02:58:27.142651: Epoch 135
2024-12-21 02:58:27.143414: Current learning rate: 0.00126
2024-12-21 03:01:43.194233: Validation loss did not improve from -0.26694. Patience: 78/50
2024-12-21 03:01:43.195516: train_loss -0.8983
2024-12-21 03:01:43.196473: val_loss -0.1182
2024-12-21 03:01:43.197535: Pseudo dice [0.6368]
2024-12-21 03:01:43.198553: Epoch time: 196.06 s
2024-12-21 03:01:44.683046: 
2024-12-21 03:01:44.684155: Epoch 136
2024-12-21 03:01:44.684997: Current learning rate: 0.00118
2024-12-21 03:05:25.861187: Validation loss did not improve from -0.26694. Patience: 79/50
2024-12-21 03:05:25.862035: train_loss -0.8964
2024-12-21 03:05:25.862855: val_loss -0.048
2024-12-21 03:05:25.863577: Pseudo dice [0.6139]
2024-12-21 03:05:25.864376: Epoch time: 221.18 s
2024-12-21 03:05:27.946533: 
2024-12-21 03:05:27.948050: Epoch 137
2024-12-21 03:05:27.949158: Current learning rate: 0.00111
2024-12-21 03:08:41.397446: Validation loss did not improve from -0.26694. Patience: 80/50
2024-12-21 03:08:41.398371: train_loss -0.8967
2024-12-21 03:08:41.399191: val_loss -0.1142
2024-12-21 03:08:41.399967: Pseudo dice [0.6254]
2024-12-21 03:08:41.401036: Epoch time: 193.45 s
2024-12-21 03:08:42.800007: 
2024-12-21 03:08:42.801471: Epoch 138
2024-12-21 03:08:42.802235: Current learning rate: 0.00103
2024-12-21 03:11:48.927530: Validation loss did not improve from -0.26694. Patience: 81/50
2024-12-21 03:11:48.928555: train_loss -0.8967
2024-12-21 03:11:48.929463: val_loss -0.0884
2024-12-21 03:11:48.930129: Pseudo dice [0.6161]
2024-12-21 03:11:48.930930: Epoch time: 186.13 s
2024-12-21 03:11:50.324556: 
2024-12-21 03:11:50.325900: Epoch 139
2024-12-21 03:11:50.326574: Current learning rate: 0.00095
2024-12-21 03:15:04.399860: Validation loss did not improve from -0.26694. Patience: 82/50
2024-12-21 03:15:04.400810: train_loss -0.8985
2024-12-21 03:15:04.401904: val_loss -0.1045
2024-12-21 03:15:04.402942: Pseudo dice [0.6163]
2024-12-21 03:15:04.403948: Epoch time: 194.08 s
2024-12-21 03:15:06.190781: 
2024-12-21 03:15:06.192364: Epoch 140
2024-12-21 03:15:06.193414: Current learning rate: 0.00087
2024-12-21 03:18:15.553775: Validation loss did not improve from -0.26694. Patience: 83/50
2024-12-21 03:18:15.554842: train_loss -0.8969
2024-12-21 03:18:15.555661: val_loss -0.0847
2024-12-21 03:18:15.556292: Pseudo dice [0.6174]
2024-12-21 03:18:15.557041: Epoch time: 189.37 s
2024-12-21 03:18:16.975876: 
2024-12-21 03:18:16.977213: Epoch 141
2024-12-21 03:18:16.977998: Current learning rate: 0.00079
2024-12-21 03:21:27.650417: Validation loss did not improve from -0.26694. Patience: 84/50
2024-12-21 03:21:27.651253: train_loss -0.897
2024-12-21 03:21:27.652440: val_loss -0.108
2024-12-21 03:21:27.653496: Pseudo dice [0.6108]
2024-12-21 03:21:27.654922: Epoch time: 190.68 s
2024-12-21 03:21:29.077443: 
2024-12-21 03:21:29.078833: Epoch 142
2024-12-21 03:21:29.079582: Current learning rate: 0.00071
2024-12-21 03:24:37.594392: Validation loss did not improve from -0.26694. Patience: 85/50
2024-12-21 03:24:37.595316: train_loss -0.8998
2024-12-21 03:24:37.596069: val_loss -0.0769
2024-12-21 03:24:37.596751: Pseudo dice [0.6026]
2024-12-21 03:24:37.597425: Epoch time: 188.52 s
2024-12-21 03:24:39.037476: 
2024-12-21 03:24:39.038778: Epoch 143
2024-12-21 03:24:39.039603: Current learning rate: 0.00063
2024-12-21 03:27:49.315780: Validation loss did not improve from -0.26694. Patience: 86/50
2024-12-21 03:27:49.317117: train_loss -0.8989
2024-12-21 03:27:49.318190: val_loss -0.0518
2024-12-21 03:27:49.319195: Pseudo dice [0.6066]
2024-12-21 03:27:49.319932: Epoch time: 190.28 s
2024-12-21 03:27:50.726779: 
2024-12-21 03:27:50.728216: Epoch 144
2024-12-21 03:27:50.729553: Current learning rate: 0.00055
2024-12-21 03:31:02.929225: Validation loss did not improve from -0.26694. Patience: 87/50
2024-12-21 03:31:02.930181: train_loss -0.8995
2024-12-21 03:31:02.931110: val_loss -0.1043
2024-12-21 03:31:02.931833: Pseudo dice [0.6249]
2024-12-21 03:31:02.932607: Epoch time: 192.2 s
2024-12-21 03:31:04.702533: 
2024-12-21 03:31:04.703864: Epoch 145
2024-12-21 03:31:04.704595: Current learning rate: 0.00047
2024-12-21 03:34:17.543004: Validation loss did not improve from -0.26694. Patience: 88/50
2024-12-21 03:34:17.544130: train_loss -0.9001
2024-12-21 03:34:17.545258: val_loss -0.1113
2024-12-21 03:34:17.546132: Pseudo dice [0.6246]
2024-12-21 03:34:17.547032: Epoch time: 192.84 s
2024-12-21 03:34:18.999953: 
2024-12-21 03:34:19.001373: Epoch 146
2024-12-21 03:34:19.002218: Current learning rate: 0.00038
2024-12-21 03:37:26.468089: Validation loss did not improve from -0.26694. Patience: 89/50
2024-12-21 03:37:26.468974: train_loss -0.8996
2024-12-21 03:37:26.469747: val_loss -0.1051
2024-12-21 03:37:26.470503: Pseudo dice [0.6144]
2024-12-21 03:37:26.471274: Epoch time: 187.47 s
2024-12-21 03:37:28.257502: 
2024-12-21 03:37:28.258829: Epoch 147
2024-12-21 03:37:28.259654: Current learning rate: 0.0003
2024-12-21 03:40:39.008450: Validation loss did not improve from -0.26694. Patience: 90/50
2024-12-21 03:40:39.009558: train_loss -0.9003
2024-12-21 03:40:39.010255: val_loss -0.1029
2024-12-21 03:40:39.010966: Pseudo dice [0.6072]
2024-12-21 03:40:39.011676: Epoch time: 190.75 s
2024-12-21 03:40:40.456984: 
2024-12-21 03:40:40.458494: Epoch 148
2024-12-21 03:40:40.459441: Current learning rate: 0.00021
2024-12-21 03:43:55.895079: Validation loss did not improve from -0.26694. Patience: 91/50
2024-12-21 03:43:55.895997: train_loss -0.901
2024-12-21 03:43:55.896890: val_loss -0.0732
2024-12-21 03:43:55.897742: Pseudo dice [0.6262]
2024-12-21 03:43:55.898692: Epoch time: 195.44 s
2024-12-21 03:43:57.310224: 
2024-12-21 03:43:57.311397: Epoch 149
2024-12-21 03:43:57.312151: Current learning rate: 0.00011
2024-12-21 03:47:04.563331: Validation loss did not improve from -0.26694. Patience: 92/50
2024-12-21 03:47:04.564306: train_loss -0.8995
2024-12-21 03:47:04.565161: val_loss -0.1015
2024-12-21 03:47:04.565823: Pseudo dice [0.6172]
2024-12-21 03:47:04.566495: Epoch time: 187.26 s
2024-12-21 03:47:06.383187: Training done.
2024-12-21 03:47:06.500216: Using splits from existing split file: /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset308_Sohee_Calcium_OCT_CrossValidation/splits_final_20.json
2024-12-21 03:47:06.502082: The split file contains 5 splits.
2024-12-21 03:47:06.502946: Desired fold for training: 1
2024-12-21 03:47:06.503875: This split has 1 training and 7 validation cases.
2024-12-21 03:47:06.505367: predicting 101-019
2024-12-21 03:47:06.532396: 101-019, shape torch.Size([1, 375, 498, 498]), rank 0
2024-12-21 03:49:35.770551: predicting 101-045
2024-12-21 03:49:35.786359: 101-045, shape torch.Size([1, 375, 498, 498]), rank 0
2024-12-21 03:51:50.689257: predicting 106-002
2024-12-21 03:51:50.705583: 106-002, shape torch.Size([1, 539, 498, 498]), rank 0
2024-12-21 03:55:06.466931: predicting 401-004
2024-12-21 03:55:06.489486: 401-004, shape torch.Size([1, 375, 498, 498]), rank 0
2024-12-21 03:57:10.719864: predicting 701-013
2024-12-21 03:57:10.737136: 701-013, shape torch.Size([1, 375, 498, 498]), rank 0
2024-12-21 03:59:26.187700: predicting 704-003
2024-12-21 03:59:26.203055: 704-003, shape torch.Size([1, 375, 498, 498]), rank 0
2024-12-21 04:01:24.767755: predicting 706-005
2024-12-21 04:01:24.785003: 706-005, shape torch.Size([1, 375, 498, 498]), rank 0
2024-12-21 04:03:48.220276: Validation complete
2024-12-21 04:03:48.221073: Mean Validation Dice:  0.5879837527939135

############################
INFO: You are using the old nnU-Net default plans. We have updated our recommendations. Please consider using those instead! Read more here: https://github.com/MIC-DKFZ/nnUNet/blob/master/documentation/resenc_presets.md
############################

Using device: cuda:0

#######################################################################
Please cite the following paper when using nnU-Net:
Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.
#######################################################################

2024-12-21 04:03:54.245344: Using torch.compile...

############################
INFO: You are using the old nnU-Net default plans. We have updated our recommendations. Please consider using those instead! Read more here: https://github.com/MIC-DKFZ/nnUNet/blob/master/documentation/resenc_presets.md
############################

Using device: cuda:0

#######################################################################
Please cite the following paper when using nnU-Net:
Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.
#######################################################################

2024-12-21 04:03:54.247750: Using torch.compile...
/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
################### Loading pretrained weights from file  /home/gridsan/nchutisilp/projects/OpenAI-CLIP/clip_pretrained_nnUNet.pt ###################
Below is the list of overlapping blocks in pretrained model and nnUNet architecture:
encoder.stages.0.0.convs.0.conv.weight shape torch.Size([32, 1, 3, 3, 3])
encoder.stages.0.0.convs.0.conv.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.norm.weight shape torch.Size([32])
encoder.stages.0.0.convs.0.norm.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.0.weight shape torch.Size([32, 1, 3, 3, 3])
encoder.stages.0.0.convs.0.all_modules.0.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.1.weight shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.1.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
encoder.stages.0.0.convs.1.conv.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.norm.weight shape torch.Size([32])
encoder.stages.0.0.convs.1.norm.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
encoder.stages.0.0.convs.1.all_modules.0.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.1.weight shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.1.bias shape torch.Size([32])
encoder.stages.1.0.convs.0.conv.weight shape torch.Size([64, 32, 3, 3, 3])
encoder.stages.1.0.convs.0.conv.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.norm.weight shape torch.Size([64])
encoder.stages.1.0.convs.0.norm.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.0.weight shape torch.Size([64, 32, 3, 3, 3])
encoder.stages.1.0.convs.0.all_modules.0.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.1.weight shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.1.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
encoder.stages.1.0.convs.1.conv.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.norm.weight shape torch.Size([64])
encoder.stages.1.0.convs.1.norm.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
encoder.stages.1.0.convs.1.all_modules.0.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.1.weight shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.1.bias shape torch.Size([64])
encoder.stages.2.0.convs.0.conv.weight shape torch.Size([128, 64, 3, 3, 3])
encoder.stages.2.0.convs.0.conv.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.norm.weight shape torch.Size([128])
encoder.stages.2.0.convs.0.norm.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.0.weight shape torch.Size([128, 64, 3, 3, 3])
encoder.stages.2.0.convs.0.all_modules.0.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.1.weight shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.1.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
encoder.stages.2.0.convs.1.conv.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.norm.weight shape torch.Size([128])
encoder.stages.2.0.convs.1.norm.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
encoder.stages.2.0.convs.1.all_modules.0.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.1.weight shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.1.bias shape torch.Size([128])
encoder.stages.3.0.convs.0.conv.weight shape torch.Size([256, 128, 3, 3, 3])
encoder.stages.3.0.convs.0.conv.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.norm.weight shape torch.Size([256])
encoder.stages.3.0.convs.0.norm.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.0.weight shape torch.Size([256, 128, 3, 3, 3])
encoder.stages.3.0.convs.0.all_modules.0.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.1.weight shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.1.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
encoder.stages.3.0.convs.1.conv.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.norm.weight shape torch.Size([256])
encoder.stages.3.0.convs.1.norm.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
encoder.stages.3.0.convs.1.all_modules.0.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.1.weight shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.1.bias shape torch.Size([256])
encoder.stages.4.0.convs.0.conv.weight shape torch.Size([320, 256, 3, 3, 3])
encoder.stages.4.0.convs.0.conv.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.norm.weight shape torch.Size([320])
encoder.stages.4.0.convs.0.norm.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.0.weight shape torch.Size([320, 256, 3, 3, 3])
encoder.stages.4.0.convs.0.all_modules.0.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.1.weight shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.1.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.4.0.convs.1.conv.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.norm.weight shape torch.Size([320])
encoder.stages.4.0.convs.1.norm.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.4.0.convs.1.all_modules.0.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.1.weight shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.1.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.0.conv.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.norm.weight shape torch.Size([320])
encoder.stages.5.0.convs.0.norm.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.0.all_modules.0.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.1.weight shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.1.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.1.conv.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.norm.weight shape torch.Size([320])
encoder.stages.5.0.convs.1.norm.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.1.all_modules.0.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.1.weight shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.0.0.convs.0.conv.weight shape torch.Size([32, 1, 3, 3, 3])
decoder.encoder.stages.0.0.convs.0.conv.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.norm.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.norm.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.0.weight shape torch.Size([32, 1, 3, 3, 3])
decoder.encoder.stages.0.0.convs.0.all_modules.0.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.1.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.1.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.encoder.stages.0.0.convs.1.conv.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.norm.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.norm.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.encoder.stages.0.0.convs.1.all_modules.0.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.1.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.1.bias shape torch.Size([32])
decoder.encoder.stages.1.0.convs.0.conv.weight shape torch.Size([64, 32, 3, 3, 3])
decoder.encoder.stages.1.0.convs.0.conv.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.norm.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.norm.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.0.weight shape torch.Size([64, 32, 3, 3, 3])
decoder.encoder.stages.1.0.convs.0.all_modules.0.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.1.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.1.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.encoder.stages.1.0.convs.1.conv.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.norm.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.norm.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.encoder.stages.1.0.convs.1.all_modules.0.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.1.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.1.bias shape torch.Size([64])
decoder.encoder.stages.2.0.convs.0.conv.weight shape torch.Size([128, 64, 3, 3, 3])
decoder.encoder.stages.2.0.convs.0.conv.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.norm.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.norm.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.0.weight shape torch.Size([128, 64, 3, 3, 3])
decoder.encoder.stages.2.0.convs.0.all_modules.0.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.1.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.1.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.encoder.stages.2.0.convs.1.conv.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.norm.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.norm.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.encoder.stages.2.0.convs.1.all_modules.0.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.1.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.1.bias shape torch.Size([128])
decoder.encoder.stages.3.0.convs.0.conv.weight shape torch.Size([256, 128, 3, 3, 3])
decoder.encoder.stages.3.0.convs.0.conv.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.norm.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.norm.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.0.weight shape torch.Size([256, 128, 3, 3, 3])
decoder.encoder.stages.3.0.convs.0.all_modules.0.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.1.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.1.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.encoder.stages.3.0.convs.1.conv.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.norm.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.norm.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.encoder.stages.3.0.convs.1.all_modules.0.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.1.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.1.bias shape torch.Size([256])
decoder.encoder.stages.4.0.convs.0.conv.weight shape torch.Size([320, 256, 3, 3, 3])
decoder.encoder.stages.4.0.convs.0.conv.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.norm.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.norm.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.0.weight shape torch.Size([320, 256, 3, 3, 3])
decoder.encoder.stages.4.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.4.0.convs.1.conv.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.norm.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.norm.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.4.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.0.conv.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.norm.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.norm.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.1.conv.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.norm.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.norm.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.stages.0.convs.0.conv.weight shape torch.Size([320, 640, 3, 3, 3])
decoder.stages.0.convs.0.conv.bias shape torch.Size([320])
decoder.stages.0.convs.0.norm.weight shape torch.Size([320])
decoder.stages.0.convs.0.norm.bias shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.0.weight shape torch.Size([320, 640, 3, 3, 3])
decoder.stages.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.stages.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.stages.0.convs.1.conv.bias shape torch.Size([320])
decoder.stages.0.convs.1.norm.weight shape torch.Size([320])
decoder.stages.0.convs.1.norm.bias shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.stages.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.stages.1.convs.0.conv.weight shape torch.Size([256, 512, 3, 3, 3])
decoder.stages.1.convs.0.conv.bias shape torch.Size([256])
decoder.stages.1.convs.0.norm.weight shape torch.Size([256])
decoder.stages.1.convs.0.norm.bias shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.0.weight shape torch.Size([256, 512, 3, 3, 3])
decoder.stages.1.convs.0.all_modules.0.bias shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.1.weight shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.1.bias shape torch.Size([256])
decoder.stages.1.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.stages.1.convs.1.conv.bias shape torch.Size([256])
decoder.stages.1.convs.1.norm.weight shape torch.Size([256])
decoder.stages.1.convs.1.norm.bias shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.stages.1.convs.1.all_modules.0.bias shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.1.weight shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.1.bias shape torch.Size([256])
decoder.stages.2.convs.0.conv.weight shape torch.Size([128, 256, 3, 3, 3])
decoder.stages.2.convs.0.conv.bias shape torch.Size([128])
decoder.stages.2.convs.0.norm.weight shape torch.Size([128])
decoder.stages.2.convs.0.norm.bias shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.0.weight shape torch.Size([128, 256, 3, 3, 3])
decoder.stages.2.convs.0.all_modules.0.bias shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.1.weight shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.1.bias shape torch.Size([128])
decoder.stages.2.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.stages.2.convs.1.conv.bias shape torch.Size([128])
decoder.stages.2.convs.1.norm.weight shape torch.Size([128])
decoder.stages.2.convs.1.norm.bias shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.stages.2.convs.1.all_modules.0.bias shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.1.weight shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.1.bias shape torch.Size([128])
decoder.stages.3.convs.0.conv.weight shape torch.Size([64, 128, 3, 3, 3])
decoder.stages.3.convs.0.conv.bias shape torch.Size([64])
decoder.stages.3.convs.0.norm.weight shape torch.Size([64])
decoder.stages.3.convs.0.norm.bias shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.0.weight shape torch.Size([64, 128, 3, 3, 3])
decoder.stages.3.convs.0.all_modules.0.bias shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.1.weight shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.1.bias shape torch.Size([64])
decoder.stages.3.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.stages.3.convs.1.conv.bias shape torch.Size([64])
decoder.stages.3.convs.1.norm.weight shape torch.Size([64])
decoder.stages.3.convs.1.norm.bias shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.stages.3.convs.1.all_modules.0.bias shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.1.weight shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.1.bias shape torch.Size([64])
decoder.stages.4.convs.0.conv.weight shape torch.Size([32, 64, 3, 3, 3])
decoder.stages.4.convs.0.conv.bias shape torch.Size([32])
decoder.stages.4.convs.0.norm.weight shape torch.Size([32])
decoder.stages.4.convs.0.norm.bias shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.0.weight shape torch.Size([32, 64, 3, 3, 3])
decoder.stages.4.convs.0.all_modules.0.bias shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.1.weight shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.1.bias shape torch.Size([32])
decoder.stages.4.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.stages.4.convs.1.conv.bias shape torch.Size([32])
decoder.stages.4.convs.1.norm.weight shape torch.Size([32])
decoder.stages.4.convs.1.norm.bias shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.stages.4.convs.1.all_modules.0.bias shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.1.weight shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.1.bias shape torch.Size([32])
decoder.transpconvs.0.weight shape torch.Size([320, 320, 1, 2, 2])
decoder.transpconvs.0.bias shape torch.Size([320])
decoder.transpconvs.1.weight shape torch.Size([320, 256, 2, 2, 2])
decoder.transpconvs.1.bias shape torch.Size([256])
decoder.transpconvs.2.weight shape torch.Size([256, 128, 2, 2, 2])
decoder.transpconvs.2.bias shape torch.Size([128])
decoder.transpconvs.3.weight shape torch.Size([128, 64, 2, 2, 2])
decoder.transpconvs.3.bias shape torch.Size([64])
decoder.transpconvs.4.weight shape torch.Size([64, 32, 2, 2, 2])
decoder.transpconvs.4.bias shape torch.Size([32])
################### Done ###################
2024-12-21 04:03:56.253178: do_dummy_2d_data_aug: True
2024-12-21 04:03:56.254806: Using splits from existing split file: /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset308_Sohee_Calcium_OCT_CrossValidation/splits_final_20.json
2024-12-21 04:03:56.257029: The split file contains 5 splits.
2024-12-21 04:03:56.258049: Desired fold for training: 3
2024-12-21 04:03:56.259022: This split has 1 training and 7 validation cases.
################### Loading pretrained weights from file  /home/gridsan/nchutisilp/projects/OpenAI-CLIP/clip_pretrained_nnUNet.pt ###################
Below is the list of overlapping blocks in pretrained model and nnUNet architecture:
encoder.stages.0.0.convs.0.conv.weight shape torch.Size([32, 1, 3, 3, 3])
encoder.stages.0.0.convs.0.conv.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.norm.weight shape torch.Size([32])
encoder.stages.0.0.convs.0.norm.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.0.weight shape torch.Size([32, 1, 3, 3, 3])
encoder.stages.0.0.convs.0.all_modules.0.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.1.weight shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.1.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
encoder.stages.0.0.convs.1.conv.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.norm.weight shape torch.Size([32])
encoder.stages.0.0.convs.1.norm.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
encoder.stages.0.0.convs.1.all_modules.0.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.1.weight shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.1.bias shape torch.Size([32])
encoder.stages.1.0.convs.0.conv.weight shape torch.Size([64, 32, 3, 3, 3])
encoder.stages.1.0.convs.0.conv.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.norm.weight shape torch.Size([64])
encoder.stages.1.0.convs.0.norm.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.0.weight shape torch.Size([64, 32, 3, 3, 3])
encoder.stages.1.0.convs.0.all_modules.0.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.1.weight shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.1.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
encoder.stages.1.0.convs.1.conv.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.norm.weight shape torch.Size([64])
encoder.stages.1.0.convs.1.norm.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
encoder.stages.1.0.convs.1.all_modules.0.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.1.weight shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.1.bias shape torch.Size([64])
encoder.stages.2.0.convs.0.conv.weight shape torch.Size([128, 64, 3, 3, 3])
encoder.stages.2.0.convs.0.conv.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.norm.weight shape torch.Size([128])
encoder.stages.2.0.convs.0.norm.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.0.weight shape torch.Size([128, 64, 3, 3, 3])
encoder.stages.2.0.convs.0.all_modules.0.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.1.weight shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.1.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
encoder.stages.2.0.convs.1.conv.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.norm.weight shape torch.Size([128])
encoder.stages.2.0.convs.1.norm.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
encoder.stages.2.0.convs.1.all_modules.0.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.1.weight shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.1.bias shape torch.Size([128])
encoder.stages.3.0.convs.0.conv.weight shape torch.Size([256, 128, 3, 3, 3])
encoder.stages.3.0.convs.0.conv.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.norm.weight shape torch.Size([256])
encoder.stages.3.0.convs.0.norm.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.0.weight shape torch.Size([256, 128, 3, 3, 3])
encoder.stages.3.0.convs.0.all_modules.0.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.1.weight shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.1.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
encoder.stages.3.0.convs.1.conv.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.norm.weight shape torch.Size([256])
encoder.stages.3.0.convs.1.norm.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
encoder.stages.3.0.convs.1.all_modules.0.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.1.weight shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.1.bias shape torch.Size([256])
encoder.stages.4.0.convs.0.conv.weight shape torch.Size([320, 256, 3, 3, 3])
encoder.stages.4.0.convs.0.conv.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.norm.weight shape torch.Size([320])
encoder.stages.4.0.convs.0.norm.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.0.weight shape torch.Size([320, 256, 3, 3, 3])
encoder.stages.4.0.convs.0.all_modules.0.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.1.weight shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.1.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.4.0.convs.1.conv.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.norm.weight shape torch.Size([320])
encoder.stages.4.0.convs.1.norm.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.4.0.convs.1.all_modules.0.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.1.weight shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.1.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.0.conv.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.norm.weight shape torch.Size([320])
encoder.stages.5.0.convs.0.norm.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.0.all_modules.0.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.1.weight shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.1.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.1.conv.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.norm.weight shape torch.Size([320])
encoder.stages.5.0.convs.1.norm.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.1.all_modules.0.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.1.weight shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.0.0.convs.0.conv.weight shape torch.Size([32, 1, 3, 3, 3])
decoder.encoder.stages.0.0.convs.0.conv.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.norm.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.norm.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.0.weight shape torch.Size([32, 1, 3, 3, 3])
decoder.encoder.stages.0.0.convs.0.all_modules.0.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.1.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.1.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.encoder.stages.0.0.convs.1.conv.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.norm.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.norm.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.encoder.stages.0.0.convs.1.all_modules.0.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.1.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.1.bias shape torch.Size([32])
decoder.encoder.stages.1.0.convs.0.conv.weight shape torch.Size([64, 32, 3, 3, 3])
decoder.encoder.stages.1.0.convs.0.conv.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.norm.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.norm.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.0.weight shape torch.Size([64, 32, 3, 3, 3])
decoder.encoder.stages.1.0.convs.0.all_modules.0.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.1.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.1.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.encoder.stages.1.0.convs.1.conv.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.norm.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.norm.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.encoder.stages.1.0.convs.1.all_modules.0.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.1.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.1.bias shape torch.Size([64])
decoder.encoder.stages.2.0.convs.0.conv.weight shape torch.Size([128, 64, 3, 3, 3])
decoder.encoder.stages.2.0.convs.0.conv.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.norm.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.norm.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.0.weight shape torch.Size([128, 64, 3, 3, 3])
decoder.encoder.stages.2.0.convs.0.all_modules.0.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.1.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.1.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.encoder.stages.2.0.convs.1.conv.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.norm.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.norm.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.encoder.stages.2.0.convs.1.all_modules.0.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.1.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.1.bias shape torch.Size([128])
decoder.encoder.stages.3.0.convs.0.conv.weight shape torch.Size([256, 128, 3, 3, 3])
decoder.encoder.stages.3.0.convs.0.conv.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.norm.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.norm.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.0.weight shape torch.Size([256, 128, 3, 3, 3])
decoder.encoder.stages.3.0.convs.0.all_modules.0.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.1.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.1.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.encoder.stages.3.0.convs.1.conv.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.norm.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.norm.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.encoder.stages.3.0.convs.1.all_modules.0.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.1.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.1.bias shape torch.Size([256])
decoder.encoder.stages.4.0.convs.0.conv.weight shape torch.Size([320, 256, 3, 3, 3])
decoder.encoder.stages.4.0.convs.0.conv.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.norm.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.norm.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.0.weight shape torch.Size([320, 256, 3, 3, 3])
decoder.encoder.stages.4.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.4.0.convs.1.conv.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.norm.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.norm.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.4.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.0.conv.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.norm.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.norm.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.1.conv.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.norm.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.norm.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.stages.0.convs.0.conv.weight shape torch.Size([320, 640, 3, 3, 3])
decoder.stages.0.convs.0.conv.bias shape torch.Size([320])
decoder.stages.0.convs.0.norm.weight shape torch.Size([320])
decoder.stages.0.convs.0.norm.bias shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.0.weight shape torch.Size([320, 640, 3, 3, 3])
decoder.stages.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.stages.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.stages.0.convs.1.conv.bias shape torch.Size([320])
decoder.stages.0.convs.1.norm.weight shape torch.Size([320])
decoder.stages.0.convs.1.norm.bias shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.stages.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.stages.1.convs.0.conv.weight shape torch.Size([256, 512, 3, 3, 3])
decoder.stages.1.convs.0.conv.bias shape torch.Size([256])
decoder.stages.1.convs.0.norm.weight shape torch.Size([256])
decoder.stages.1.convs.0.norm.bias shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.0.weight shape torch.Size([256, 512, 3, 3, 3])
decoder.stages.1.convs.0.all_modules.0.bias shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.1.weight shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.1.bias shape torch.Size([256])
decoder.stages.1.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.stages.1.convs.1.conv.bias shape torch.Size([256])
decoder.stages.1.convs.1.norm.weight shape torch.Size([256])
decoder.stages.1.convs.1.norm.bias shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.stages.1.convs.1.all_modules.0.bias shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.1.weight shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.1.bias shape torch.Size([256])
decoder.stages.2.convs.0.conv.weight shape torch.Size([128, 256, 3, 3, 3])
decoder.stages.2.convs.0.conv.bias shape torch.Size([128])
decoder.stages.2.convs.0.norm.weight shape torch.Size([128])
decoder.stages.2.convs.0.norm.bias shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.0.weight shape torch.Size([128, 256, 3, 3, 3])
decoder.stages.2.convs.0.all_modules.0.bias shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.1.weight shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.1.bias shape torch.Size([128])
decoder.stages.2.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.stages.2.convs.1.conv.bias shape torch.Size([128])
decoder.stages.2.convs.1.norm.weight shape torch.Size([128])
decoder.stages.2.convs.1.norm.bias shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.stages.2.convs.1.all_modules.0.bias shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.1.weight shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.1.bias shape torch.Size([128])
decoder.stages.3.convs.0.conv.weight shape torch.Size([64, 128, 3, 3, 3])
decoder.stages.3.convs.0.conv.bias shape torch.Size([64])
decoder.stages.3.convs.0.norm.weight shape torch.Size([64])
decoder.stages.3.convs.0.norm.bias shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.0.weight shape torch.Size([64, 128, 3, 3, 3])
decoder.stages.3.convs.0.all_modules.0.bias shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.1.weight shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.1.bias shape torch.Size([64])
decoder.stages.3.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.stages.3.convs.1.conv.bias shape torch.Size([64])
decoder.stages.3.convs.1.norm.weight shape torch.Size([64])
decoder.stages.3.convs.1.norm.bias shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.stages.3.convs.1.all_modules.0.bias shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.1.weight shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.1.bias shape torch.Size([64])
decoder.stages.4.convs.0.conv.weight shape torch.Size([32, 64, 3, 3, 3])
decoder.stages.4.convs.0.conv.bias shape torch.Size([32])
decoder.stages.4.convs.0.norm.weight shape torch.Size([32])
decoder.stages.4.convs.0.norm.bias shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.0.weight shape torch.Size([32, 64, 3, 3, 3])
decoder.stages.4.convs.0.all_modules.0.bias shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.1.weight shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.1.bias shape torch.Size([32])
decoder.stages.4.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.stages.4.convs.1.conv.bias shape torch.Size([32])
decoder.stages.4.convs.1.norm.weight shape torch.Size([32])
decoder.stages.4.convs.1.norm.bias shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.stages.4.convs.1.all_modules.0.bias shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.1.weight shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.1.bias shape torch.Size([32])
decoder.transpconvs.0.weight shape torch.Size([320, 320, 1, 2, 2])
decoder.transpconvs.0.bias shape torch.Size([320])
decoder.transpconvs.1.weight shape torch.Size([320, 256, 2, 2, 2])
decoder.transpconvs.1.bias shape torch.Size([256])
decoder.transpconvs.2.weight shape torch.Size([256, 128, 2, 2, 2])
decoder.transpconvs.2.bias shape torch.Size([128])
decoder.transpconvs.3.weight shape torch.Size([128, 64, 2, 2, 2])
decoder.transpconvs.3.bias shape torch.Size([64])
decoder.transpconvs.4.weight shape torch.Size([64, 32, 2, 2, 2])
decoder.transpconvs.4.bias shape torch.Size([32])
################### Done ###################
2024-12-21 04:03:56.270077: do_dummy_2d_data_aug: True
2024-12-21 04:03:56.271217: Using splits from existing split file: /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset308_Sohee_Calcium_OCT_CrossValidation/splits_final_20.json
2024-12-21 04:03:56.272740: The split file contains 5 splits.
2024-12-21 04:03:56.273736: Desired fold for training: 2
2024-12-21 04:03:56.274974: This split has 1 training and 7 validation cases.
using pin_memory on device 0
using pin_memory on device 0
using pin_memory on device 0

This is the configuration used by this training:
Configuration name: 3d_32x160x128_b10
 {'data_identifier': 'nnUNetPlans_3d_fullres', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 10, 'patch_size': [32, 160, 128], 'median_image_size_in_voxels': [375.0, 498.0, 498.0], 'spacing': [1.0, 1.0, 1.0], 'normalization_schemes': ['ZScoreNormalization'], 'use_mask_for_norm': [False], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'architecture': {'network_class_name': 'dynamic_network_architectures.architectures.unet.PlainConvUNet', 'arch_kwargs': {'n_stages': 6, 'features_per_stage': [32, 64, 128, 256, 320, 320], 'conv_op': 'torch.nn.modules.conv.Conv3d', 'kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]], 'strides': [[1, 1, 1], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [1, 2, 2]], 'n_conv_per_stage': [2, 2, 2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2, 2, 2], 'conv_bias': True, 'norm_op': 'torch.nn.modules.instancenorm.InstanceNorm3d', 'norm_op_kwargs': {'eps': 1e-05, 'affine': True}, 'dropout_op': None, 'dropout_op_kwargs': None, 'nonlin': 'torch.nn.LeakyReLU', 'nonlin_kwargs': {'inplace': True}, 'deep_supervision': True}, '_kw_requires_import': ['conv_op', 'norm_op', 'dropout_op', 'nonlin']}, 'batch_dice': True, 'inherits_from': '3d_fullres'} 

These are the global plan.json settings:
 {'dataset_name': 'Dataset308_Sohee_Calcium_OCT_CrossValidation', 'plans_name': 'nnUNetPlans', 'original_median_spacing_after_transp': [1.0, 1.0, 1.0], 'original_median_shape_after_transp': [375, 498, 498], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 0.4977976381778717, 'mean': 0.13507525622844696, 'median': 0.09599608182907104, 'min': 0.0, 'percentile_00_5': 0.014754901640117168, 'percentile_99_5': 0.4977976381778717, 'std': 0.12025152146816254}}} 

2024-12-21 04:04:23.227820: unpacking dataset...
using pin_memory on device 0

This is the configuration used by this training:
Configuration name: 3d_32x160x128_b10
 {'data_identifier': 'nnUNetPlans_3d_fullres', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 10, 'patch_size': [32, 160, 128], 'median_image_size_in_voxels': [375.0, 498.0, 498.0], 'spacing': [1.0, 1.0, 1.0], 'normalization_schemes': ['ZScoreNormalization'], 'use_mask_for_norm': [False], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'architecture': {'network_class_name': 'dynamic_network_architectures.architectures.unet.PlainConvUNet', 'arch_kwargs': {'n_stages': 6, 'features_per_stage': [32, 64, 128, 256, 320, 320], 'conv_op': 'torch.nn.modules.conv.Conv3d', 'kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]], 'strides': [[1, 1, 1], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [1, 2, 2]], 'n_conv_per_stage': [2, 2, 2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2, 2, 2], 'conv_bias': True, 'norm_op': 'torch.nn.modules.instancenorm.InstanceNorm3d', 'norm_op_kwargs': {'eps': 1e-05, 'affine': True}, 'dropout_op': None, 'dropout_op_kwargs': None, 'nonlin': 'torch.nn.LeakyReLU', 'nonlin_kwargs': {'inplace': True}, 'deep_supervision': True}, '_kw_requires_import': ['conv_op', 'norm_op', 'dropout_op', 'nonlin']}, 'batch_dice': True, 'inherits_from': '3d_fullres'} 

These are the global plan.json settings:
 {'dataset_name': 'Dataset308_Sohee_Calcium_OCT_CrossValidation', 'plans_name': 'nnUNetPlans', 'original_median_spacing_after_transp': [1.0, 1.0, 1.0], 'original_median_shape_after_transp': [375, 498, 498], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 0.4977976381778717, 'mean': 0.13507525622844696, 'median': 0.09599608182907104, 'min': 0.0, 'percentile_00_5': 0.014754901640117168, 'percentile_99_5': 0.4977976381778717, 'std': 0.12025152146816254}}} 

2024-12-21 04:04:24.029967: unpacking dataset...
2024-12-21 04:04:27.804726: unpacking done...
2024-12-21 04:04:28.027765: Unable to plot network architecture: nnUNet_compile is enabled!
2024-12-21 04:04:28.074749: 
2024-12-21 04:04:28.076200: Epoch 0
2024-12-21 04:04:28.077627: Current learning rate: 0.01
2024-12-21 04:10:56.922557: Validation loss improved from 1000.00000 to -0.08098! Patience: 0/50
2024-12-21 04:10:56.923652: train_loss -0.0978
2024-12-21 04:10:56.924399: val_loss -0.081
2024-12-21 04:10:56.925097: Pseudo dice [0.46]
2024-12-21 04:10:56.925894: Epoch time: 388.85 s
2024-12-21 04:10:56.926637: Yayy! New best EMA pseudo Dice: 0.46
2024-12-21 04:10:58.496250: 
2024-12-21 04:10:58.497649: Epoch 1
2024-12-21 04:10:58.498507: Current learning rate: 0.00994
2024-12-21 04:16:38.360833: Validation loss did not improve from -0.08098. Patience: 1/50
2024-12-21 04:16:38.361892: train_loss -0.3136
2024-12-21 04:16:38.362733: val_loss -0.0614
2024-12-21 04:16:38.363457: Pseudo dice [0.4416]
2024-12-21 04:16:38.364082: Epoch time: 339.87 s
2024-12-21 04:16:39.822878: 
2024-12-21 04:16:39.824389: Epoch 2
2024-12-21 04:16:39.825137: Current learning rate: 0.00988
2024-12-21 04:20:10.630280: Validation loss improved from -0.08098 to -0.15812! Patience: 1/50
2024-12-21 04:20:10.631330: train_loss -0.4078
2024-12-21 04:20:10.632168: val_loss -0.1581
2024-12-21 04:20:10.632934: Pseudo dice [0.5295]
2024-12-21 04:20:10.633797: Epoch time: 210.81 s
2024-12-21 04:20:10.634533: Yayy! New best EMA pseudo Dice: 0.4653
2024-12-21 04:20:12.479845: 
2024-12-21 04:20:12.481045: Epoch 3
2024-12-21 04:20:12.481831: Current learning rate: 0.00982
2024-12-21 04:23:29.197706: Validation loss did not improve from -0.15812. Patience: 1/50
2024-12-21 04:23:29.198582: train_loss -0.4772
2024-12-21 04:23:29.199413: val_loss -0.1431
2024-12-21 04:23:29.200209: Pseudo dice [0.5359]
2024-12-21 04:23:29.201147: Epoch time: 196.72 s
2024-12-21 04:23:29.201943: Yayy! New best EMA pseudo Dice: 0.4723
2024-12-21 04:23:31.038541: 
2024-12-21 04:23:31.039544: Epoch 4
2024-12-21 04:23:31.040586: Current learning rate: 0.00976
2024-12-21 04:26:43.569826: Validation loss improved from -0.15812 to -0.17875! Patience: 1/50
2024-12-21 04:26:43.570843: train_loss -0.5155
2024-12-21 04:26:43.571816: val_loss -0.1788
2024-12-21 04:26:43.572634: Pseudo dice [0.5408]
2024-12-21 04:26:43.573553: Epoch time: 192.53 s
2024-12-21 04:26:43.949626: Yayy! New best EMA pseudo Dice: 0.4792
2024-12-21 04:26:45.848033: 
2024-12-21 04:26:45.849361: Epoch 5
2024-12-21 04:26:45.850197: Current learning rate: 0.0097
2024-12-21 04:30:14.432499: Validation loss did not improve from -0.17875. Patience: 1/50
2024-12-21 04:30:14.433549: train_loss -0.5576
2024-12-21 04:30:14.434541: val_loss -0.1462
2024-12-21 04:30:14.435351: Pseudo dice [0.5523]
2024-12-21 04:30:14.436340: Epoch time: 208.59 s
2024-12-21 04:30:14.437171: Yayy! New best EMA pseudo Dice: 0.4865
2024-12-21 04:30:16.233830: 
2024-12-21 04:30:16.235091: Epoch 6
2024-12-21 04:30:16.236115: Current learning rate: 0.00964
2024-12-21 04:33:32.465915: Validation loss did not improve from -0.17875. Patience: 2/50
2024-12-21 04:33:32.466994: train_loss -0.5792
2024-12-21 04:33:32.468019: val_loss -0.1399
2024-12-21 04:33:32.469068: Pseudo dice [0.5146]
2024-12-21 04:33:32.469958: Epoch time: 196.23 s
2024-12-21 04:33:32.470933: Yayy! New best EMA pseudo Dice: 0.4893
2024-12-21 04:33:34.284344: 
2024-12-21 04:33:34.285809: Epoch 7
2024-12-21 04:33:34.286737: Current learning rate: 0.00958
2024-12-21 04:37:13.995234: Validation loss did not improve from -0.17875. Patience: 3/50
2024-12-21 04:37:13.996242: train_loss -0.6017
2024-12-21 04:37:13.997099: val_loss -0.1718
2024-12-21 04:37:13.997920: Pseudo dice [0.5593]
2024-12-21 04:37:13.998668: Epoch time: 219.71 s
2024-12-21 04:37:13.999393: Yayy! New best EMA pseudo Dice: 0.4963
2024-12-21 04:37:16.252097: 
2024-12-21 04:37:16.253500: Epoch 8
2024-12-21 04:37:16.254351: Current learning rate: 0.00952
2024-12-21 04:40:34.745352: Validation loss improved from -0.17875 to -0.26856! Patience: 3/50
2024-12-21 04:40:34.746324: train_loss -0.6171
2024-12-21 04:40:34.747175: val_loss -0.2686
2024-12-21 04:40:34.747941: Pseudo dice [0.6021]
2024-12-21 04:40:34.748716: Epoch time: 198.5 s
2024-12-21 04:40:34.749547: Yayy! New best EMA pseudo Dice: 0.5069
2024-12-21 04:40:36.618912: 
2024-12-21 04:40:36.620187: Epoch 9
2024-12-21 04:40:36.620894: Current learning rate: 0.00946
2024-12-21 04:44:11.842073: Validation loss did not improve from -0.26856. Patience: 1/50
2024-12-21 04:44:11.843092: train_loss -0.6453
2024-12-21 04:44:11.844001: val_loss -0.2177
2024-12-21 04:44:11.844817: Pseudo dice [0.5855]
2024-12-21 04:44:11.845712: Epoch time: 215.23 s
2024-12-21 04:44:12.253168: Yayy! New best EMA pseudo Dice: 0.5147
2024-12-21 04:44:13.949405: 
2024-12-21 04:44:13.950811: Epoch 10
2024-12-21 04:44:13.951992: Current learning rate: 0.0094
2024-12-21 04:47:28.135680: Validation loss did not improve from -0.26856. Patience: 2/50
2024-12-21 04:47:28.136565: train_loss -0.6491
2024-12-21 04:47:28.137554: val_loss -0.1685
2024-12-21 04:47:28.138417: Pseudo dice [0.5552]
2024-12-21 04:47:28.139423: Epoch time: 194.19 s
2024-12-21 04:47:28.140530: Yayy! New best EMA pseudo Dice: 0.5188
2024-12-21 04:47:29.950475: 
2024-12-21 04:47:29.951881: Epoch 11
2024-12-21 04:47:29.952780: Current learning rate: 0.00934
2024-12-21 04:51:04.336180: Validation loss did not improve from -0.26856. Patience: 3/50
2024-12-21 04:51:04.337270: train_loss -0.6581
2024-12-21 04:51:04.338266: val_loss -0.2665
2024-12-21 04:51:04.339111: Pseudo dice [0.623]
2024-12-21 04:51:04.340048: Epoch time: 214.39 s
2024-12-21 04:51:04.340990: Yayy! New best EMA pseudo Dice: 0.5292
2024-12-21 04:51:06.213979: 
2024-12-21 04:51:06.215447: Epoch 12
2024-12-21 04:51:06.216446: Current learning rate: 0.00928
2024-12-21 04:54:25.098574: Validation loss did not improve from -0.26856. Patience: 4/50
2024-12-21 04:54:25.099327: train_loss -0.6705
2024-12-21 04:54:25.100306: val_loss -0.2215
2024-12-21 04:54:25.101066: Pseudo dice [0.6102]
2024-12-21 04:54:25.101870: Epoch time: 198.89 s
2024-12-21 04:54:25.102627: Yayy! New best EMA pseudo Dice: 0.5373
2024-12-21 04:54:26.957137: 
2024-12-21 04:54:26.958251: Epoch 13
2024-12-21 04:54:26.959127: Current learning rate: 0.00922
2024-12-21 04:58:00.911539: Validation loss improved from -0.26856 to -0.29375! Patience: 4/50
2024-12-21 04:58:00.912489: train_loss -0.6832
2024-12-21 04:58:00.913397: val_loss -0.2938
2024-12-21 04:58:00.914340: Pseudo dice [0.623]
2024-12-21 04:58:00.915241: Epoch time: 213.96 s
2024-12-21 04:58:00.916186: Yayy! New best EMA pseudo Dice: 0.5459
2024-12-21 04:58:02.721771: 
2024-12-21 04:58:02.723105: Epoch 14
2024-12-21 04:58:02.724092: Current learning rate: 0.00916
2024-12-21 05:01:12.937702: Validation loss did not improve from -0.29375. Patience: 1/50
2024-12-21 05:01:12.938706: train_loss -0.692
2024-12-21 05:01:12.939482: val_loss -0.2398
2024-12-21 05:01:12.940134: Pseudo dice [0.5961]
2024-12-21 05:01:12.940780: Epoch time: 190.22 s
2024-12-21 05:01:13.354211: Yayy! New best EMA pseudo Dice: 0.5509
2024-12-21 05:01:15.149416: 
2024-12-21 05:01:15.150707: Epoch 15
2024-12-21 05:01:15.151410: Current learning rate: 0.0091
2024-12-21 05:04:54.518890: Validation loss did not improve from -0.29375. Patience: 2/50
2024-12-21 05:04:54.519889: train_loss -0.6996
2024-12-21 05:04:54.520847: val_loss -0.2924
2024-12-21 05:04:54.521821: Pseudo dice [0.6238]
2024-12-21 05:04:54.522621: Epoch time: 219.37 s
2024-12-21 05:04:54.523554: Yayy! New best EMA pseudo Dice: 0.5582
2024-12-21 05:04:56.316515: 
2024-12-21 05:04:56.318010: Epoch 16
2024-12-21 05:04:56.319019: Current learning rate: 0.00903
2024-12-21 05:08:11.587275: Validation loss did not improve from -0.29375. Patience: 3/50
2024-12-21 05:08:11.588395: train_loss -0.7112
2024-12-21 05:08:11.589159: val_loss -0.2362
2024-12-21 05:08:11.589842: Pseudo dice [0.6032]
2024-12-21 05:08:11.590532: Epoch time: 195.27 s
2024-12-21 05:08:11.591298: Yayy! New best EMA pseudo Dice: 0.5627
2024-12-21 05:08:13.469903: 
2024-12-21 05:08:13.471198: Epoch 17
2024-12-21 05:08:13.471855: Current learning rate: 0.00897
2024-12-21 05:11:43.883364: Validation loss did not improve from -0.29375. Patience: 4/50
2024-12-21 05:11:43.885184: train_loss -0.7132
2024-12-21 05:11:43.886959: val_loss -0.2131
2024-12-21 05:11:43.887685: Pseudo dice [0.5929]
2024-12-21 05:11:43.888510: Epoch time: 210.42 s
2024-12-21 05:11:43.889297: Yayy! New best EMA pseudo Dice: 0.5657
2024-12-21 05:11:45.800169: 
2024-12-21 05:11:45.801293: Epoch 18
2024-12-21 05:11:45.802092: Current learning rate: 0.00891
2024-12-21 05:15:00.267880: Validation loss did not improve from -0.29375. Patience: 5/50
2024-12-21 05:15:00.268962: train_loss -0.7173
2024-12-21 05:15:00.269751: val_loss -0.2166
2024-12-21 05:15:00.270451: Pseudo dice [0.5845]
2024-12-21 05:15:00.271147: Epoch time: 194.47 s
2024-12-21 05:15:00.271827: Yayy! New best EMA pseudo Dice: 0.5676
2024-12-21 05:15:02.531521: 
2024-12-21 05:15:02.532928: Epoch 19
2024-12-21 05:15:02.533761: Current learning rate: 0.00885
2024-12-21 05:18:31.213549: Validation loss did not improve from -0.29375. Patience: 6/50
2024-12-21 05:18:31.215318: train_loss -0.7278
2024-12-21 05:18:31.216335: val_loss -0.2107
2024-12-21 05:18:31.217150: Pseudo dice [0.5816]
2024-12-21 05:18:31.217890: Epoch time: 208.69 s
2024-12-21 05:18:31.616215: Yayy! New best EMA pseudo Dice: 0.569
2024-12-21 05:18:33.468999: 
2024-12-21 05:18:33.470386: Epoch 20
2024-12-21 05:18:33.471111: Current learning rate: 0.00879
2024-12-21 05:21:48.935388: Validation loss did not improve from -0.29375. Patience: 7/50
2024-12-21 05:21:48.936217: train_loss -0.7359
2024-12-21 05:21:48.937054: val_loss -0.1947
2024-12-21 05:21:48.937767: Pseudo dice [0.5906]
2024-12-21 05:21:48.938559: Epoch time: 195.47 s
2024-12-21 05:21:48.939225: Yayy! New best EMA pseudo Dice: 0.5711
2024-12-21 05:21:50.806003: 
2024-12-21 05:21:50.813058: Epoch 21
2024-12-21 05:21:50.813938: Current learning rate: 0.00873
2024-12-21 05:25:24.870208: Validation loss did not improve from -0.29375. Patience: 8/50
2024-12-21 05:25:24.871340: train_loss -0.7382
2024-12-21 05:25:24.872130: val_loss -0.1701
2024-12-21 05:25:24.872820: Pseudo dice [0.5707]
2024-12-21 05:25:24.873523: Epoch time: 214.07 s
2024-12-21 05:25:26.244494: 
2024-12-21 05:25:26.245836: Epoch 22
2024-12-21 05:25:26.246598: Current learning rate: 0.00867
2024-12-21 05:28:51.819915: Validation loss did not improve from -0.29375. Patience: 9/50
2024-12-21 05:28:51.820977: train_loss -0.7446
2024-12-21 05:28:51.821921: val_loss -0.2754
2024-12-21 05:28:51.822725: Pseudo dice [0.6402]
2024-12-21 05:28:51.823697: Epoch time: 205.58 s
2024-12-21 05:28:51.824491: Yayy! New best EMA pseudo Dice: 0.578
2024-12-21 05:28:53.690846: 
2024-12-21 05:28:53.692390: Epoch 23
2024-12-21 05:28:53.693525: Current learning rate: 0.00861
2024-12-21 05:32:25.886087: Validation loss did not improve from -0.29375. Patience: 10/50
2024-12-21 05:32:25.887146: train_loss -0.7437
2024-12-21 05:32:25.887982: val_loss -0.1249
2024-12-21 05:32:25.888708: Pseudo dice [0.556]
2024-12-21 05:32:25.889507: Epoch time: 212.2 s
2024-12-21 05:32:27.280165: 
2024-12-21 05:32:27.281440: Epoch 24
2024-12-21 05:32:27.282206: Current learning rate: 0.00855
2024-12-21 05:35:47.824212: Validation loss did not improve from -0.29375. Patience: 11/50
2024-12-21 05:35:47.825295: train_loss -0.7542
2024-12-21 05:35:47.826081: val_loss -0.251
2024-12-21 05:35:47.826776: Pseudo dice [0.6204]
2024-12-21 05:35:47.827545: Epoch time: 200.55 s
2024-12-21 05:35:48.221793: Yayy! New best EMA pseudo Dice: 0.5803
2024-12-21 05:35:49.964472: 
2024-12-21 05:35:49.967248: Epoch 25
2024-12-21 05:35:49.968066: Current learning rate: 0.00849
2024-12-21 05:39:29.923375: Validation loss did not improve from -0.29375. Patience: 12/50
2024-12-21 05:39:29.924343: train_loss -0.7575
2024-12-21 05:39:29.925198: val_loss -0.2037
2024-12-21 05:39:29.925920: Pseudo dice [0.6056]
2024-12-21 05:39:29.926649: Epoch time: 219.96 s
2024-12-21 05:39:29.927427: Yayy! New best EMA pseudo Dice: 0.5828
2024-12-21 05:39:31.760718: 
2024-12-21 05:39:31.762162: Epoch 26
2024-12-21 05:39:31.763038: Current learning rate: 0.00843
2024-12-21 05:42:54.356526: Validation loss did not improve from -0.29375. Patience: 13/50
2024-12-21 05:42:54.357602: train_loss -0.7622
2024-12-21 05:42:54.358335: val_loss -0.2566
2024-12-21 05:42:54.359171: Pseudo dice [0.6184]
2024-12-21 05:42:54.359869: Epoch time: 202.6 s
2024-12-21 05:42:54.360548: Yayy! New best EMA pseudo Dice: 0.5864
2024-12-21 05:42:56.212099: 
2024-12-21 05:42:56.213552: Epoch 27
2024-12-21 05:42:56.214389: Current learning rate: 0.00836
2024-12-21 05:46:25.700773: Validation loss did not improve from -0.29375. Patience: 14/50
2024-12-21 05:46:25.701833: train_loss -0.7613
2024-12-21 05:46:25.702845: val_loss -0.1584
2024-12-21 05:46:25.703713: Pseudo dice [0.5747]
2024-12-21 05:46:25.704644: Epoch time: 209.49 s
2024-12-21 05:46:27.072424: 
2024-12-21 05:46:27.073924: Epoch 28
2024-12-21 05:46:27.074970: Current learning rate: 0.0083
2024-12-21 05:49:40.178101: Validation loss did not improve from -0.29375. Patience: 15/50
2024-12-21 05:49:40.179036: train_loss -0.7651
2024-12-21 05:49:40.179926: val_loss -0.1854
2024-12-21 05:49:40.180646: Pseudo dice [0.5966]
2024-12-21 05:49:40.181352: Epoch time: 193.11 s
2024-12-21 05:49:42.235044: 
2024-12-21 05:49:42.236469: Epoch 29
2024-12-21 05:49:42.237399: Current learning rate: 0.00824
2024-12-21 05:53:19.714945: Validation loss did not improve from -0.29375. Patience: 16/50
2024-12-21 05:53:19.716040: train_loss -0.7681
2024-12-21 05:53:19.716882: val_loss -0.2049
2024-12-21 05:53:19.717551: Pseudo dice [0.5827]
2024-12-21 05:53:19.718229: Epoch time: 217.48 s
2024-12-21 05:53:21.555636: 
2024-12-21 05:53:21.557038: Epoch 30
2024-12-21 05:53:21.557868: Current learning rate: 0.00818
2024-12-21 05:56:39.210351: Validation loss did not improve from -0.29375. Patience: 17/50
2024-12-21 05:56:39.211276: train_loss -0.7712
2024-12-21 05:56:39.212213: val_loss -0.2796
2024-12-21 05:56:39.212919: Pseudo dice [0.6257]
2024-12-21 05:56:39.213627: Epoch time: 197.66 s
2024-12-21 05:56:39.214361: Yayy! New best EMA pseudo Dice: 0.5899
2024-12-21 05:56:41.062102: 
2024-12-21 05:56:41.063547: Epoch 31
2024-12-21 05:56:41.064272: Current learning rate: 0.00812
2024-12-21 06:00:17.816103: Validation loss did not improve from -0.29375. Patience: 18/50
2024-12-21 06:00:17.817099: train_loss -0.7733
2024-12-21 06:00:17.817897: val_loss -0.1195
2024-12-21 06:00:17.818503: Pseudo dice [0.5838]
2024-12-21 06:00:17.819179: Epoch time: 216.76 s
2024-12-21 06:00:19.296666: 
2024-12-21 06:00:19.298025: Epoch 32
2024-12-21 06:00:19.298687: Current learning rate: 0.00806
2024-12-21 06:03:36.183633: Validation loss did not improve from -0.29375. Patience: 19/50
2024-12-21 06:03:36.184601: train_loss -0.7751
2024-12-21 06:03:36.185357: val_loss -0.16
2024-12-21 06:03:36.186033: Pseudo dice [0.5707]
2024-12-21 06:03:36.186806: Epoch time: 196.89 s
2024-12-21 06:03:37.656460: 
2024-12-21 06:03:37.658072: Epoch 33
2024-12-21 06:03:37.659186: Current learning rate: 0.008
2024-12-21 06:07:14.697400: Validation loss did not improve from -0.29375. Patience: 20/50
2024-12-21 06:07:14.698387: train_loss -0.7797
2024-12-21 06:07:14.699373: val_loss -0.1698
2024-12-21 06:07:14.700309: Pseudo dice [0.5956]
2024-12-21 06:07:14.701163: Epoch time: 217.04 s
2024-12-21 06:07:16.130398: 
2024-12-21 06:07:16.131926: Epoch 34
2024-12-21 06:07:16.132733: Current learning rate: 0.00793
2024-12-21 06:10:38.988840: Validation loss did not improve from -0.29375. Patience: 21/50
2024-12-21 06:10:38.989777: train_loss -0.7791
2024-12-21 06:10:38.990720: val_loss -0.1951
2024-12-21 06:10:38.991491: Pseudo dice [0.6049]
2024-12-21 06:10:38.992229: Epoch time: 202.86 s
2024-12-21 06:10:40.866136: 
2024-12-21 06:10:40.867346: Epoch 35
2024-12-21 06:10:40.868241: Current learning rate: 0.00787
2024-12-21 06:14:14.997260: Validation loss did not improve from -0.29375. Patience: 22/50
2024-12-21 06:14:14.999350: train_loss -0.7851
2024-12-21 06:14:15.000522: val_loss -0.0608
2024-12-21 06:14:15.001384: Pseudo dice [0.5277]
2024-12-21 06:14:15.002483: Epoch time: 214.13 s
2024-12-21 06:14:16.474332: 
2024-12-21 06:14:16.475699: Epoch 36
2024-12-21 06:14:16.476542: Current learning rate: 0.00781
2024-12-21 06:17:38.517602: Validation loss did not improve from -0.29375. Patience: 23/50
2024-12-21 06:17:38.518968: train_loss -0.7903
2024-12-21 06:17:38.519855: val_loss -0.1775
2024-12-21 06:17:38.520522: Pseudo dice [0.6085]
2024-12-21 06:17:38.521374: Epoch time: 202.05 s
2024-12-21 06:17:39.995819: 
2024-12-21 06:17:39.996955: Epoch 37
2024-12-21 06:17:39.998004: Current learning rate: 0.00775
2024-12-21 06:21:15.687472: Validation loss did not improve from -0.29375. Patience: 24/50
2024-12-21 06:21:15.691382: train_loss -0.791
2024-12-21 06:21:15.693169: val_loss -0.0344
2024-12-21 06:21:15.694057: Pseudo dice [0.5453]
2024-12-21 06:21:15.695060: Epoch time: 215.7 s
2024-12-21 06:21:17.178520: 
2024-12-21 06:21:17.179809: Epoch 38
2024-12-21 06:21:17.180516: Current learning rate: 0.00769
2024-12-21 06:24:37.187712: Validation loss did not improve from -0.29375. Patience: 25/50
2024-12-21 06:24:37.188703: train_loss -0.7931
2024-12-21 06:24:37.189580: val_loss -0.1437
2024-12-21 06:24:37.190417: Pseudo dice [0.5638]
2024-12-21 06:24:37.191103: Epoch time: 200.01 s
2024-12-21 06:24:39.185962: 
2024-12-21 06:24:39.187505: Epoch 39
2024-12-21 06:24:39.188310: Current learning rate: 0.00763
2024-12-21 06:28:17.428109: Validation loss did not improve from -0.29375. Patience: 26/50
2024-12-21 06:28:17.429142: train_loss -0.7934
2024-12-21 06:28:17.432167: val_loss -0.2454
2024-12-21 06:28:17.432852: Pseudo dice [0.6298]
2024-12-21 06:28:17.433486: Epoch time: 218.24 s
2024-12-21 06:28:19.286159: 
2024-12-21 06:28:19.287567: Epoch 40
2024-12-21 06:28:19.288306: Current learning rate: 0.00756
2024-12-21 06:31:33.681472: Validation loss did not improve from -0.29375. Patience: 27/50
2024-12-21 06:31:33.682388: train_loss -0.7912
2024-12-21 06:31:33.683331: val_loss -0.1841
2024-12-21 06:31:33.684299: Pseudo dice [0.5764]
2024-12-21 06:31:33.685292: Epoch time: 194.4 s
2024-12-21 06:31:35.184671: 
2024-12-21 06:31:35.185626: Epoch 41
2024-12-21 06:31:35.186392: Current learning rate: 0.0075
2024-12-21 06:35:08.229967: Validation loss did not improve from -0.29375. Patience: 28/50
2024-12-21 06:35:08.231006: train_loss -0.7974
2024-12-21 06:35:08.232340: val_loss -0.203
2024-12-21 06:35:08.233481: Pseudo dice [0.6137]
2024-12-21 06:35:08.234598: Epoch time: 213.05 s
2024-12-21 06:35:09.602991: 
2024-12-21 06:35:09.604482: Epoch 42
2024-12-21 06:35:09.605617: Current learning rate: 0.00744
2024-12-21 06:38:21.638860: Validation loss did not improve from -0.29375. Patience: 29/50
2024-12-21 06:38:21.639958: train_loss -0.8004
2024-12-21 06:38:21.640902: val_loss -0.1367
2024-12-21 06:38:21.641648: Pseudo dice [0.5889]
2024-12-21 06:38:21.642358: Epoch time: 192.04 s
2024-12-21 06:38:23.035975: 
2024-12-21 06:38:23.037402: Epoch 43
2024-12-21 06:38:23.038228: Current learning rate: 0.00738
2024-12-21 06:41:56.978406: Validation loss did not improve from -0.29375. Patience: 30/50
2024-12-21 06:41:56.979552: train_loss -0.8008
2024-12-21 06:41:56.980436: val_loss -0.1482
2024-12-21 06:41:56.981178: Pseudo dice [0.5714]
2024-12-21 06:41:56.982031: Epoch time: 213.94 s
2024-12-21 06:41:58.375479: 
2024-12-21 06:41:58.376842: Epoch 44
2024-12-21 06:41:58.377674: Current learning rate: 0.00732
2024-12-21 06:45:19.220953: Validation loss did not improve from -0.29375. Patience: 31/50
2024-12-21 06:45:19.221952: train_loss -0.8005
2024-12-21 06:45:19.222705: val_loss -0.19
2024-12-21 06:45:19.223522: Pseudo dice [0.613]
2024-12-21 06:45:19.224184: Epoch time: 200.85 s
2024-12-21 06:45:21.077211: 
2024-12-21 06:45:21.078620: Epoch 45
2024-12-21 06:45:21.079425: Current learning rate: 0.00725
2024-12-21 06:48:56.200444: Validation loss did not improve from -0.29375. Patience: 32/50
2024-12-21 06:48:56.201299: train_loss -0.8018
2024-12-21 06:48:56.202051: val_loss -0.1872
2024-12-21 06:48:56.202713: Pseudo dice [0.5934]
2024-12-21 06:48:56.203532: Epoch time: 215.13 s
2024-12-21 06:48:57.622128: 
2024-12-21 06:48:57.623581: Epoch 46
2024-12-21 06:48:57.624419: Current learning rate: 0.00719
2024-12-21 06:52:06.817378: Validation loss did not improve from -0.29375. Patience: 33/50
2024-12-21 06:52:06.818357: train_loss -0.8039
2024-12-21 06:52:06.819191: val_loss -0.1728
2024-12-21 06:52:06.819968: Pseudo dice [0.5902]
2024-12-21 06:52:06.820797: Epoch time: 189.2 s
2024-12-21 06:52:08.221462: 
2024-12-21 06:52:08.222900: Epoch 47
2024-12-21 06:52:08.223770: Current learning rate: 0.00713
2024-12-21 06:56:01.460432: Validation loss did not improve from -0.29375. Patience: 34/50
2024-12-21 06:56:01.461447: train_loss -0.8033
2024-12-21 06:56:01.462242: val_loss -0.1372
2024-12-21 06:56:01.462903: Pseudo dice [0.59]
2024-12-21 06:56:01.463595: Epoch time: 233.24 s
2024-12-21 06:56:02.929764: 
2024-12-21 06:56:02.931017: Epoch 48
2024-12-21 06:56:02.931732: Current learning rate: 0.00707
2024-12-21 06:59:22.760947: Validation loss did not improve from -0.29375. Patience: 35/50
2024-12-21 06:59:22.761820: train_loss -0.807
2024-12-21 06:59:22.762768: val_loss -0.1441
2024-12-21 06:59:22.763584: Pseudo dice [0.5907]
2024-12-21 06:59:22.764531: Epoch time: 199.83 s
2024-12-21 06:59:24.576687: 
2024-12-21 06:59:24.578081: Epoch 49
2024-12-21 06:59:24.579103: Current learning rate: 0.007
2024-12-21 07:02:56.226483: Validation loss did not improve from -0.29375. Patience: 36/50
2024-12-21 07:02:56.227503: train_loss -0.8095
2024-12-21 07:02:56.228449: val_loss -0.1535
2024-12-21 07:02:56.229276: Pseudo dice [0.6112]
2024-12-21 07:02:56.230036: Epoch time: 211.65 s
2024-12-21 07:02:56.626653: Yayy! New best EMA pseudo Dice: 0.5915
2024-12-21 07:02:58.465098: 
2024-12-21 07:02:58.466534: Epoch 50
2024-12-21 07:02:58.467437: Current learning rate: 0.00694
2024-12-21 07:06:15.367897: Validation loss did not improve from -0.29375. Patience: 37/50
2024-12-21 07:06:15.369043: train_loss -0.8091
2024-12-21 07:06:15.369813: val_loss -0.1907
2024-12-21 07:06:15.370476: Pseudo dice [0.6065]
2024-12-21 07:06:15.371185: Epoch time: 196.91 s
2024-12-21 07:06:15.371833: Yayy! New best EMA pseudo Dice: 0.593
2024-12-21 07:06:17.192822: 
2024-12-21 07:06:17.194123: Epoch 51
2024-12-21 07:06:17.194852: Current learning rate: 0.00688
2024-12-21 07:09:55.386667: Validation loss did not improve from -0.29375. Patience: 38/50
2024-12-21 07:09:55.387719: train_loss -0.8121
2024-12-21 07:09:55.388531: val_loss -0.2325
2024-12-21 07:09:55.389133: Pseudo dice [0.6326]
2024-12-21 07:09:55.389820: Epoch time: 218.2 s
2024-12-21 07:09:55.390518: Yayy! New best EMA pseudo Dice: 0.597
2024-12-21 07:09:57.235500: 
2024-12-21 07:09:57.236781: Epoch 52
2024-12-21 07:09:57.237522: Current learning rate: 0.00682
2024-12-21 07:13:20.178534: Validation loss did not improve from -0.29375. Patience: 39/50
2024-12-21 07:13:20.179384: train_loss -0.8131
2024-12-21 07:13:20.180135: val_loss -0.2015
2024-12-21 07:13:20.180788: Pseudo dice [0.623]
2024-12-21 07:13:20.181480: Epoch time: 202.95 s
2024-12-21 07:13:20.182078: Yayy! New best EMA pseudo Dice: 0.5996
2024-12-21 07:13:21.970689: 
2024-12-21 07:13:21.971910: Epoch 53
2024-12-21 07:13:21.972705: Current learning rate: 0.00675
2024-12-21 07:16:56.613889: Validation loss did not improve from -0.29375. Patience: 40/50
2024-12-21 07:16:56.614993: train_loss -0.812
2024-12-21 07:16:56.615800: val_loss -0.0976
2024-12-21 07:16:56.616637: Pseudo dice [0.5757]
2024-12-21 07:16:56.617386: Epoch time: 214.65 s
2024-12-21 07:16:58.083072: 
2024-12-21 07:16:58.084264: Epoch 54
2024-12-21 07:16:58.085071: Current learning rate: 0.00669
2024-12-21 07:20:21.946779: Validation loss did not improve from -0.29375. Patience: 41/50
2024-12-21 07:20:21.948710: train_loss -0.8177
2024-12-21 07:20:21.949970: val_loss -0.1245
2024-12-21 07:20:21.950726: Pseudo dice [0.5869]
2024-12-21 07:20:21.951973: Epoch time: 203.87 s
2024-12-21 07:20:23.824016: 
2024-12-21 07:20:23.825598: Epoch 55
2024-12-21 07:20:23.826443: Current learning rate: 0.00663
2024-12-21 07:23:55.567283: Validation loss did not improve from -0.29375. Patience: 42/50
2024-12-21 07:23:55.568574: train_loss -0.8186
2024-12-21 07:23:55.569443: val_loss -0.1445
2024-12-21 07:23:55.570128: Pseudo dice [0.5801]
2024-12-21 07:23:55.570836: Epoch time: 211.75 s
2024-12-21 07:23:57.026435: 
2024-12-21 07:23:57.027922: Epoch 56
2024-12-21 07:23:57.029356: Current learning rate: 0.00657
2024-12-21 07:27:16.719889: Validation loss did not improve from -0.29375. Patience: 43/50
2024-12-21 07:27:16.723720: train_loss -0.819
2024-12-21 07:27:16.725503: val_loss -0.1639
2024-12-21 07:27:16.726519: Pseudo dice [0.5952]
2024-12-21 07:27:16.727519: Epoch time: 199.7 s
2024-12-21 07:27:18.212685: 
2024-12-21 07:27:18.214082: Epoch 57
2024-12-21 07:27:18.214907: Current learning rate: 0.0065
2024-12-21 07:30:58.147763: Validation loss did not improve from -0.29375. Patience: 44/50
2024-12-21 07:30:58.148748: train_loss -0.8198
2024-12-21 07:30:58.149694: val_loss -0.0535
2024-12-21 07:30:58.150471: Pseudo dice [0.5474]
2024-12-21 07:30:58.151300: Epoch time: 219.94 s
2024-12-21 07:30:59.556496: 
2024-12-21 07:30:59.557842: Epoch 58
2024-12-21 07:30:59.558704: Current learning rate: 0.00644
2024-12-21 07:34:17.666146: Validation loss did not improve from -0.29375. Patience: 45/50
2024-12-21 07:34:17.667304: train_loss -0.8216
2024-12-21 07:34:17.668226: val_loss -0.0921
2024-12-21 07:34:17.669034: Pseudo dice [0.5726]
2024-12-21 07:34:17.669736: Epoch time: 198.11 s
2024-12-21 07:34:19.107438: 
2024-12-21 07:34:19.108749: Epoch 59
2024-12-21 07:34:19.109657: Current learning rate: 0.00638
2024-12-21 07:37:57.389422: Validation loss did not improve from -0.29375. Patience: 46/50
2024-12-21 07:37:57.390425: train_loss -0.8236
2024-12-21 07:37:57.391232: val_loss -0.1242
2024-12-21 07:37:57.392044: Pseudo dice [0.5943]
2024-12-21 07:37:57.392892: Epoch time: 218.28 s
2024-12-21 07:38:00.299520: 
2024-12-21 07:38:00.300922: Epoch 60
2024-12-21 07:38:00.301811: Current learning rate: 0.00631
2024-12-21 07:41:20.200755: Validation loss did not improve from -0.29375. Patience: 47/50
2024-12-21 07:41:20.201765: train_loss -0.8219
2024-12-21 07:41:20.202548: val_loss -0.0465
2024-12-21 07:41:20.203294: Pseudo dice [0.5427]
2024-12-21 07:41:20.204028: Epoch time: 199.9 s
2024-12-21 07:41:21.633416: 
2024-12-21 07:41:21.634716: Epoch 61
2024-12-21 07:41:21.635549: Current learning rate: 0.00625
2024-12-21 07:44:53.644627: Validation loss did not improve from -0.29375. Patience: 48/50
2024-12-21 07:44:53.645712: train_loss -0.8238
2024-12-21 07:44:53.646849: val_loss -0.1602
2024-12-21 07:44:53.647937: Pseudo dice [0.5983]
2024-12-21 07:44:53.648994: Epoch time: 212.01 s
2024-12-21 07:44:55.135481: 
2024-12-21 07:44:55.137180: Epoch 62
2024-12-21 07:44:55.138206: Current learning rate: 0.00619
2024-12-21 07:48:11.331385: Validation loss did not improve from -0.29375. Patience: 49/50
2024-12-21 07:48:11.332191: train_loss -0.8253
2024-12-21 07:48:11.332916: val_loss -0.0567
2024-12-21 07:48:11.333641: Pseudo dice [0.5572]
2024-12-21 07:48:11.334416: Epoch time: 196.2 s
2024-12-21 07:48:12.839621: 
2024-12-21 07:48:12.840756: Epoch 63
2024-12-21 07:48:12.841607: Current learning rate: 0.00612
2024-12-21 07:51:51.933338: Validation loss did not improve from -0.29375. Patience: 50/50
2024-12-21 07:51:51.934367: train_loss -0.8267
2024-12-21 07:51:51.935369: val_loss -0.2059
2024-12-21 07:51:51.936091: Pseudo dice [0.6287]
2024-12-21 07:51:51.936887: Epoch time: 219.1 s
2024-12-21 07:51:53.438008: 
2024-12-21 07:51:53.439465: Epoch 64
2024-12-21 07:51:53.440281: Current learning rate: 0.00606
2024-12-21 07:55:13.139416: Validation loss did not improve from -0.29375. Patience: 51/50
2024-12-21 07:55:13.140466: train_loss -0.8284
2024-12-21 07:55:13.141298: val_loss -0.1379
2024-12-21 07:55:13.141997: Pseudo dice [0.5961]
2024-12-21 07:55:13.142692: Epoch time: 199.7 s
2024-12-21 07:55:15.069172: 
2024-12-21 07:55:15.070594: Epoch 65
2024-12-21 07:55:15.072669: Current learning rate: 0.006
2024-12-21 07:58:47.649817: Validation loss did not improve from -0.29375. Patience: 52/50
2024-12-21 07:58:47.650691: train_loss -0.8311
2024-12-21 07:58:47.651507: val_loss -0.1503
2024-12-21 07:58:47.652302: Pseudo dice [0.5918]
2024-12-21 07:58:47.653155: Epoch time: 212.58 s
2024-12-21 07:58:49.176171: 
2024-12-21 07:58:49.177579: Epoch 66
2024-12-21 07:58:49.178408: Current learning rate: 0.00593
2024-12-21 08:02:07.209322: Validation loss did not improve from -0.29375. Patience: 53/50
2024-12-21 08:02:07.210371: train_loss -0.8292
2024-12-21 08:02:07.211253: val_loss -0.1122
2024-12-21 08:02:07.212014: Pseudo dice [0.5761]
2024-12-21 08:02:07.212852: Epoch time: 198.04 s
2024-12-21 08:02:08.680998: 
2024-12-21 08:02:08.682515: Epoch 67
2024-12-21 08:02:08.683438: Current learning rate: 0.00587
2024-12-21 08:05:56.637332: Validation loss did not improve from -0.29375. Patience: 54/50
2024-12-21 08:05:56.638440: train_loss -0.8294
2024-12-21 08:05:56.639321: val_loss 0.0194
2024-12-21 08:05:56.640092: Pseudo dice [0.525]
2024-12-21 08:05:56.640865: Epoch time: 227.96 s
2024-12-21 08:05:58.145998: 
2024-12-21 08:05:58.147485: Epoch 68
2024-12-21 08:05:58.148263: Current learning rate: 0.00581
2024-12-21 08:09:17.032315: Validation loss did not improve from -0.29375. Patience: 55/50
2024-12-21 08:09:17.033563: train_loss -0.8326
2024-12-21 08:09:17.034633: val_loss -0.179
2024-12-21 08:09:17.035616: Pseudo dice [0.6101]
2024-12-21 08:09:17.036508: Epoch time: 198.89 s
2024-12-21 08:09:18.522194: 
2024-12-21 08:09:18.523675: Epoch 69
2024-12-21 08:09:18.524520: Current learning rate: 0.00574
2024-12-21 08:12:58.276535: Validation loss did not improve from -0.29375. Patience: 56/50
2024-12-21 08:12:58.277626: train_loss -0.834
2024-12-21 08:12:58.278421: val_loss -0.12
2024-12-21 08:12:58.279134: Pseudo dice [0.5817]
2024-12-21 08:12:58.279822: Epoch time: 219.76 s
2024-12-21 08:13:00.725240: 
2024-12-21 08:13:00.726479: Epoch 70
2024-12-21 08:13:00.727217: Current learning rate: 0.00568
2024-12-21 08:16:22.997204: Validation loss did not improve from -0.29375. Patience: 57/50
2024-12-21 08:16:22.998210: train_loss -0.8348
2024-12-21 08:16:22.999115: val_loss -0.0932
2024-12-21 08:16:22.999943: Pseudo dice [0.579]
2024-12-21 08:16:23.000798: Epoch time: 202.27 s
2024-12-21 08:16:24.434422: 
2024-12-21 08:16:24.435778: Epoch 71
2024-12-21 08:16:24.436508: Current learning rate: 0.00562
2024-12-21 08:20:03.705899: Validation loss did not improve from -0.29375. Patience: 58/50
2024-12-21 08:20:03.707030: train_loss -0.8356
2024-12-21 08:20:03.707815: val_loss -0.0775
2024-12-21 08:20:03.708490: Pseudo dice [0.5929]
2024-12-21 08:20:03.709161: Epoch time: 219.27 s
2024-12-21 08:20:05.132251: 
2024-12-21 08:20:05.133650: Epoch 72
2024-12-21 08:20:05.134412: Current learning rate: 0.00555
2024-12-21 08:23:17.961531: Validation loss did not improve from -0.29375. Patience: 59/50
2024-12-21 08:23:17.962516: train_loss -0.8365
2024-12-21 08:23:17.963298: val_loss -0.0194
2024-12-21 08:23:17.963970: Pseudo dice [0.5367]
2024-12-21 08:23:17.964649: Epoch time: 192.83 s
2024-12-21 08:23:19.406300: 
2024-12-21 08:23:19.407558: Epoch 73
2024-12-21 08:23:19.408325: Current learning rate: 0.00549
2024-12-21 08:26:54.871854: Validation loss did not improve from -0.29375. Patience: 60/50
2024-12-21 08:26:54.873298: train_loss -0.837
2024-12-21 08:26:54.874582: val_loss -0.0861
2024-12-21 08:26:54.875498: Pseudo dice [0.5709]
2024-12-21 08:26:54.876500: Epoch time: 215.47 s
2024-12-21 08:26:56.405343: 
2024-12-21 08:26:56.407452: Epoch 74
2024-12-21 08:26:56.408672: Current learning rate: 0.00542
2024-12-21 08:30:14.946630: Validation loss did not improve from -0.29375. Patience: 61/50
2024-12-21 08:30:14.948364: train_loss -0.8396
2024-12-21 08:30:14.949399: val_loss -0.152
2024-12-21 08:30:14.950409: Pseudo dice [0.614]
2024-12-21 08:30:14.951304: Epoch time: 198.54 s
2024-12-21 08:30:16.890973: 
2024-12-21 08:30:16.892395: Epoch 75
2024-12-21 08:30:16.893358: Current learning rate: 0.00536
2024-12-21 08:33:56.362571: Validation loss did not improve from -0.29375. Patience: 62/50
2024-12-21 08:33:56.364147: train_loss -0.839
2024-12-21 08:33:56.365332: val_loss -0.1035
2024-12-21 08:33:56.366203: Pseudo dice [0.5884]
2024-12-21 08:33:56.367025: Epoch time: 219.47 s
2024-12-21 08:33:57.838691: 
2024-12-21 08:33:57.839974: Epoch 76
2024-12-21 08:33:57.840659: Current learning rate: 0.00529
2024-12-21 08:37:11.593906: Validation loss did not improve from -0.29375. Patience: 63/50
2024-12-21 08:37:11.595061: train_loss -0.8396
2024-12-21 08:37:11.596050: val_loss -0.1096
2024-12-21 08:37:11.596849: Pseudo dice [0.5989]
2024-12-21 08:37:11.597597: Epoch time: 193.76 s
2024-12-21 08:37:13.052304: 
2024-12-21 08:37:13.053751: Epoch 77
2024-12-21 08:37:13.054600: Current learning rate: 0.00523
2024-12-21 08:40:45.960757: Validation loss did not improve from -0.29375. Patience: 64/50
2024-12-21 08:40:45.962060: train_loss -0.8412
2024-12-21 08:40:45.962993: val_loss -0.1262
2024-12-21 08:40:45.963730: Pseudo dice [0.5936]
2024-12-21 08:40:45.964619: Epoch time: 212.91 s
2024-12-21 08:40:47.486850: 
2024-12-21 08:40:47.488078: Epoch 78
2024-12-21 08:40:47.489022: Current learning rate: 0.00517
2024-12-21 08:44:05.242989: Validation loss did not improve from -0.29375. Patience: 65/50
2024-12-21 08:44:05.244046: train_loss -0.8427
2024-12-21 08:44:05.244891: val_loss -0.0687
2024-12-21 08:44:05.245768: Pseudo dice [0.5754]
2024-12-21 08:44:05.246601: Epoch time: 197.76 s
2024-12-21 08:44:06.806455: 
2024-12-21 08:44:06.808136: Epoch 79
2024-12-21 08:44:06.809229: Current learning rate: 0.0051
2024-12-21 08:47:47.623451: Validation loss did not improve from -0.29375. Patience: 66/50
2024-12-21 08:47:47.624452: train_loss -0.8444
2024-12-21 08:47:47.625324: val_loss -0.0949
2024-12-21 08:47:47.626065: Pseudo dice [0.5768]
2024-12-21 08:47:47.626878: Epoch time: 220.82 s
2024-12-21 08:47:50.324691: 
2024-12-21 08:47:50.325894: Epoch 80
2024-12-21 08:47:50.326635: Current learning rate: 0.00504
2024-12-21 08:51:06.510896: Validation loss did not improve from -0.29375. Patience: 67/50
2024-12-21 08:51:06.511889: train_loss -0.8457
2024-12-21 08:51:06.512797: val_loss -0.0795
2024-12-21 08:51:06.513592: Pseudo dice [0.5819]
2024-12-21 08:51:06.514408: Epoch time: 196.19 s
2024-12-21 08:51:08.025579: 
2024-12-21 08:51:08.026769: Epoch 81
2024-12-21 08:51:08.027454: Current learning rate: 0.00497
2024-12-21 08:54:41.435414: Validation loss did not improve from -0.29375. Patience: 68/50
2024-12-21 08:54:41.436493: train_loss -0.8452
2024-12-21 08:54:41.437266: val_loss -0.1503
2024-12-21 08:54:41.438148: Pseudo dice [0.6165]
2024-12-21 08:54:41.438858: Epoch time: 213.41 s
2024-12-21 08:54:42.967000: 
2024-12-21 08:54:42.968382: Epoch 82
2024-12-21 08:54:42.969220: Current learning rate: 0.00491
2024-12-21 08:57:55.756831: Validation loss did not improve from -0.29375. Patience: 69/50
2024-12-21 08:57:55.757611: train_loss -0.8458
2024-12-21 08:57:55.767015: val_loss -0.0969
2024-12-21 08:57:55.768014: Pseudo dice [0.5851]
2024-12-21 08:57:55.768742: Epoch time: 192.79 s
2024-12-21 08:57:57.190607: 
2024-12-21 08:57:57.191778: Epoch 83
2024-12-21 08:57:57.192506: Current learning rate: 0.00484
2024-12-21 09:01:38.123827: Validation loss did not improve from -0.29375. Patience: 70/50
2024-12-21 09:01:38.124733: train_loss -0.8481
2024-12-21 09:01:38.125718: val_loss -0.1592
2024-12-21 09:01:38.126501: Pseudo dice [0.6041]
2024-12-21 09:01:38.127286: Epoch time: 220.94 s
2024-12-21 09:01:39.518923: 
2024-12-21 09:01:39.520419: Epoch 84
2024-12-21 09:01:39.521341: Current learning rate: 0.00478
2024-12-21 09:05:00.117090: Validation loss did not improve from -0.29375. Patience: 71/50
2024-12-21 09:05:00.118174: train_loss -0.8479
2024-12-21 09:05:00.119104: val_loss -0.1113
2024-12-21 09:05:00.119823: Pseudo dice [0.6019]
2024-12-21 09:05:00.120493: Epoch time: 200.6 s
2024-12-21 09:05:01.919095: 
2024-12-21 09:05:01.920497: Epoch 85
2024-12-21 09:05:01.921400: Current learning rate: 0.00471
2024-12-21 09:08:35.258777: Validation loss did not improve from -0.29375. Patience: 72/50
2024-12-21 09:08:35.259791: train_loss -0.8482
2024-12-21 09:08:35.260790: val_loss -0.1389
2024-12-21 09:08:35.261542: Pseudo dice [0.6011]
2024-12-21 09:08:35.262313: Epoch time: 213.34 s
2024-12-21 09:08:36.657718: 
2024-12-21 09:08:36.659007: Epoch 86
2024-12-21 09:08:36.659780: Current learning rate: 0.00465
2024-12-21 09:12:00.923857: Validation loss did not improve from -0.29375. Patience: 73/50
2024-12-21 09:12:00.924891: train_loss -0.8485
2024-12-21 09:12:00.925797: val_loss -0.0893
2024-12-21 09:12:00.926654: Pseudo dice [0.6012]
2024-12-21 09:12:00.927634: Epoch time: 204.27 s
2024-12-21 09:12:02.300737: 
2024-12-21 09:12:02.302304: Epoch 87
2024-12-21 09:12:02.303322: Current learning rate: 0.00458
2024-12-21 09:15:29.636709: Validation loss did not improve from -0.29375. Patience: 74/50
2024-12-21 09:15:29.637661: train_loss -0.8514
2024-12-21 09:15:29.638460: val_loss -0.093
2024-12-21 09:15:29.639276: Pseudo dice [0.593]
2024-12-21 09:15:29.640071: Epoch time: 207.34 s
2024-12-21 09:15:31.004613: 
2024-12-21 09:15:31.006054: Epoch 88
2024-12-21 09:15:31.006839: Current learning rate: 0.00452
2024-12-21 09:18:53.589875: Validation loss did not improve from -0.29375. Patience: 75/50
2024-12-21 09:18:53.590857: train_loss -0.8509
2024-12-21 09:18:53.591671: val_loss -0.1379
2024-12-21 09:18:53.592412: Pseudo dice [0.6107]
2024-12-21 09:18:53.593135: Epoch time: 202.59 s
2024-12-21 09:18:54.957995: 
2024-12-21 09:18:54.958845: Epoch 89
2024-12-21 09:18:54.959575: Current learning rate: 0.00445
2024-12-21 09:22:24.901029: Validation loss did not improve from -0.29375. Patience: 76/50
2024-12-21 09:22:24.901988: train_loss -0.8513
2024-12-21 09:22:24.902774: val_loss -0.0814
2024-12-21 09:22:24.903470: Pseudo dice [0.5826]
2024-12-21 09:22:24.904146: Epoch time: 209.95 s
2024-12-21 09:22:26.701674: 
2024-12-21 09:22:26.702917: Epoch 90
2024-12-21 09:22:26.703678: Current learning rate: 0.00438
2024-12-21 09:25:45.056557: Validation loss did not improve from -0.29375. Patience: 77/50
2024-12-21 09:25:45.057547: train_loss -0.8534
2024-12-21 09:25:45.058584: val_loss -0.1274
2024-12-21 09:25:45.085647: Pseudo dice [0.6106]
2024-12-21 09:25:45.086768: Epoch time: 198.36 s
2024-12-21 09:25:46.504136: 
2024-12-21 09:25:46.505363: Epoch 91
2024-12-21 09:25:46.506227: Current learning rate: 0.00432
2024-12-21 09:29:19.858211: Validation loss did not improve from -0.29375. Patience: 78/50
2024-12-21 09:29:19.859299: train_loss -0.854
2024-12-21 09:29:19.860187: val_loss -0.1182
2024-12-21 09:29:19.860948: Pseudo dice [0.6057]
2024-12-21 09:29:19.861706: Epoch time: 213.36 s
2024-12-21 09:29:21.255147: 
2024-12-21 09:29:21.256540: Epoch 92
2024-12-21 09:29:21.257453: Current learning rate: 0.00425
2024-12-21 09:32:34.892165: Validation loss did not improve from -0.29375. Patience: 79/50
2024-12-21 09:32:34.893545: train_loss -0.8555
2024-12-21 09:32:34.894479: val_loss -0.1447
2024-12-21 09:32:34.895339: Pseudo dice [0.6107]
2024-12-21 09:32:34.896081: Epoch time: 193.64 s
2024-12-21 09:32:36.333012: 
2024-12-21 09:32:36.334339: Epoch 93
2024-12-21 09:32:36.335145: Current learning rate: 0.00419
2024-12-21 09:35:52.001766: Validation loss did not improve from -0.29375. Patience: 80/50
2024-12-21 09:35:52.002908: train_loss -0.8547
2024-12-21 09:35:52.003711: val_loss -0.1098
2024-12-21 09:35:52.004374: Pseudo dice [0.596]
2024-12-21 09:35:52.005016: Epoch time: 195.67 s
2024-12-21 09:35:53.450712: 
2024-12-21 09:35:53.452105: Epoch 94
2024-12-21 09:35:53.452940: Current learning rate: 0.00412
2024-12-21 09:39:09.537013: Validation loss did not improve from -0.29375. Patience: 81/50
2024-12-21 09:39:09.538744: train_loss -0.8556
2024-12-21 09:39:09.539756: val_loss -0.0927
2024-12-21 09:39:09.540560: Pseudo dice [0.5964]
2024-12-21 09:39:09.541384: Epoch time: 196.09 s
2024-12-21 09:39:11.357597: 
2024-12-21 09:39:11.358844: Epoch 95
2024-12-21 09:39:11.359607: Current learning rate: 0.00405
2024-12-21 09:42:43.924792: Validation loss did not improve from -0.29375. Patience: 82/50
2024-12-21 09:42:43.925530: train_loss -0.8532
2024-12-21 09:42:43.926322: val_loss -0.0292
2024-12-21 09:42:43.927139: Pseudo dice [0.5691]
2024-12-21 09:42:43.928009: Epoch time: 212.57 s
2024-12-21 09:42:45.310973: 
2024-12-21 09:42:45.312173: Epoch 96
2024-12-21 09:42:45.312935: Current learning rate: 0.00399
2024-12-21 09:46:03.223329: Validation loss did not improve from -0.29375. Patience: 83/50
2024-12-21 09:46:03.224335: train_loss -0.8562
2024-12-21 09:46:03.225209: val_loss -0.0964
2024-12-21 09:46:03.226035: Pseudo dice [0.5875]
2024-12-21 09:46:03.226865: Epoch time: 197.91 s
2024-12-21 09:46:04.616452: 
2024-12-21 09:46:04.617702: Epoch 97
2024-12-21 09:46:04.618523: Current learning rate: 0.00392
2024-12-21 09:49:44.651664: Validation loss did not improve from -0.29375. Patience: 84/50
2024-12-21 09:49:44.652575: train_loss -0.858
2024-12-21 09:49:44.653459: val_loss -0.0203
2024-12-21 09:49:44.654110: Pseudo dice [0.5612]
2024-12-21 09:49:44.654914: Epoch time: 220.04 s
2024-12-21 09:49:46.050616: 
2024-12-21 09:49:46.052085: Epoch 98
2024-12-21 09:49:46.052857: Current learning rate: 0.00385
2024-12-21 09:52:59.817149: Validation loss did not improve from -0.29375. Patience: 85/50
2024-12-21 09:52:59.818259: train_loss -0.8602
2024-12-21 09:52:59.819221: val_loss -0.0988
2024-12-21 09:52:59.820160: Pseudo dice [0.6086]
2024-12-21 09:52:59.820946: Epoch time: 193.77 s
2024-12-21 09:53:01.259181: 
2024-12-21 09:53:01.260434: Epoch 99
2024-12-21 09:53:01.261232: Current learning rate: 0.00379
2024-12-21 09:56:20.238846: Validation loss did not improve from -0.29375. Patience: 86/50
2024-12-21 09:56:20.239888: train_loss -0.8589
2024-12-21 09:56:20.240761: val_loss 0.037
2024-12-21 09:56:20.241568: Pseudo dice [0.537]
2024-12-21 09:56:20.242454: Epoch time: 198.98 s
2024-12-21 09:56:22.147986: 
2024-12-21 09:56:22.149500: Epoch 100
2024-12-21 09:56:22.150454: Current learning rate: 0.00372
2024-12-21 09:59:37.614730: Validation loss did not improve from -0.29375. Patience: 87/50
2024-12-21 09:59:37.615658: train_loss -0.8609
2024-12-21 09:59:37.616453: val_loss -0.1009
2024-12-21 09:59:37.617174: Pseudo dice [0.6002]
2024-12-21 09:59:37.617918: Epoch time: 195.47 s
2024-12-21 09:59:39.039705: 
2024-12-21 09:59:39.041103: Epoch 101
2024-12-21 09:59:39.041849: Current learning rate: 0.00365
2024-12-21 10:02:56.013941: Validation loss did not improve from -0.29375. Patience: 88/50
2024-12-21 10:02:56.015357: train_loss -0.8626
2024-12-21 10:02:56.016593: val_loss -0.0403
2024-12-21 10:02:56.017363: Pseudo dice [0.5827]
2024-12-21 10:02:56.018149: Epoch time: 196.98 s
2024-12-21 10:02:58.006495: 
2024-12-21 10:02:58.007939: Epoch 102
2024-12-21 10:02:58.008799: Current learning rate: 0.00359
2024-12-21 10:06:25.806918: Validation loss did not improve from -0.29375. Patience: 89/50
2024-12-21 10:06:25.807937: train_loss -0.861
2024-12-21 10:06:25.808759: val_loss -0.084
2024-12-21 10:06:25.809480: Pseudo dice [0.5836]
2024-12-21 10:06:25.810274: Epoch time: 207.8 s
2024-12-21 10:06:27.331091: 
2024-12-21 10:06:27.332417: Epoch 103
2024-12-21 10:06:27.333209: Current learning rate: 0.00352
2024-12-21 10:09:41.164317: Validation loss did not improve from -0.29375. Patience: 90/50
2024-12-21 10:09:41.165364: train_loss -0.8607
2024-12-21 10:09:41.166385: val_loss -0.1132
2024-12-21 10:09:41.167299: Pseudo dice [0.586]
2024-12-21 10:09:41.168365: Epoch time: 193.84 s
2024-12-21 10:09:42.658874: 
2024-12-21 10:09:42.660469: Epoch 104
2024-12-21 10:09:42.661433: Current learning rate: 0.00345
2024-12-21 10:13:10.716967: Validation loss did not improve from -0.29375. Patience: 91/50
2024-12-21 10:13:10.718133: train_loss -0.863
2024-12-21 10:13:10.718867: val_loss -0.1
2024-12-21 10:13:10.719553: Pseudo dice [0.5979]
2024-12-21 10:13:10.720294: Epoch time: 208.06 s
2024-12-21 10:13:12.541683: 
2024-12-21 10:13:12.543521: Epoch 105
2024-12-21 10:13:12.544396: Current learning rate: 0.00338
2024-12-21 10:16:22.959412: Validation loss did not improve from -0.29375. Patience: 92/50
2024-12-21 10:16:22.960385: train_loss -0.8635
2024-12-21 10:16:22.961190: val_loss -0.0121
2024-12-21 10:16:22.961888: Pseudo dice [0.5748]
2024-12-21 10:16:22.962677: Epoch time: 190.42 s
2024-12-21 10:16:24.441757: 
2024-12-21 10:16:24.444322: Epoch 106
2024-12-21 10:16:24.445667: Current learning rate: 0.00332
2024-12-21 10:19:55.441476: Validation loss did not improve from -0.29375. Patience: 93/50
2024-12-21 10:19:55.442813: train_loss -0.864
2024-12-21 10:19:55.443827: val_loss -0.0824
2024-12-21 10:19:55.444527: Pseudo dice [0.5924]
2024-12-21 10:19:55.445440: Epoch time: 211.0 s
2024-12-21 10:19:56.906606: 
2024-12-21 10:19:56.907989: Epoch 107
2024-12-21 10:19:56.908728: Current learning rate: 0.00325
2024-12-21 10:23:15.496368: Validation loss did not improve from -0.29375. Patience: 94/50
2024-12-21 10:23:15.497611: train_loss -0.8644
2024-12-21 10:23:15.498654: val_loss -0.0928
2024-12-21 10:23:15.499618: Pseudo dice [0.5884]
2024-12-21 10:23:15.500454: Epoch time: 198.59 s
2024-12-21 10:23:16.963031: 
2024-12-21 10:23:16.964205: Epoch 108
2024-12-21 10:23:16.965050: Current learning rate: 0.00318
2024-12-21 10:26:51.252790: Validation loss did not improve from -0.29375. Patience: 95/50
2024-12-21 10:26:51.253853: train_loss -0.8657
2024-12-21 10:26:51.254693: val_loss -0.0404
2024-12-21 10:26:51.255356: Pseudo dice [0.5623]
2024-12-21 10:26:51.256028: Epoch time: 214.29 s
2024-12-21 10:26:52.711741: 
2024-12-21 10:26:52.713117: Epoch 109
2024-12-21 10:26:52.713874: Current learning rate: 0.00311
2024-12-21 10:30:07.915978: Validation loss did not improve from -0.29375. Patience: 96/50
2024-12-21 10:30:07.917019: train_loss -0.8678
2024-12-21 10:30:07.917934: val_loss -0.1071
2024-12-21 10:30:07.918824: Pseudo dice [0.6028]
2024-12-21 10:30:07.919675: Epoch time: 195.21 s
2024-12-21 10:30:09.784203: 
2024-12-21 10:30:09.785529: Epoch 110
2024-12-21 10:30:09.786380: Current learning rate: 0.00304
2024-12-21 10:33:41.607889: Validation loss did not improve from -0.29375. Patience: 97/50
2024-12-21 10:33:41.608825: train_loss -0.8652
2024-12-21 10:33:41.609873: val_loss -0.043
2024-12-21 10:33:41.610899: Pseudo dice [0.5616]
2024-12-21 10:33:41.611752: Epoch time: 211.83 s
2024-12-21 10:33:43.079246: 
2024-12-21 10:33:43.080777: Epoch 111
2024-12-21 10:33:43.081930: Current learning rate: 0.00297
2024-12-21 10:36:58.446077: Validation loss did not improve from -0.29375. Patience: 98/50
2024-12-21 10:36:58.447937: train_loss -0.8698
2024-12-21 10:36:58.448836: val_loss -0.0756
2024-12-21 10:36:58.449654: Pseudo dice [0.5837]
2024-12-21 10:36:58.450575: Epoch time: 195.37 s
2024-12-21 10:36:59.899649: 
2024-12-21 10:36:59.900795: Epoch 112
2024-12-21 10:36:59.901608: Current learning rate: 0.00291
2024-12-21 10:40:32.834500: Validation loss did not improve from -0.29375. Patience: 99/50
2024-12-21 10:40:32.835689: train_loss -0.8692
2024-12-21 10:40:32.836563: val_loss -0.113
2024-12-21 10:40:32.837353: Pseudo dice [0.6032]
2024-12-21 10:40:32.838177: Epoch time: 212.94 s
2024-12-21 10:40:34.859455: 
2024-12-21 10:40:34.860929: Epoch 113
2024-12-21 10:40:34.861907: Current learning rate: 0.00284
2024-12-21 10:43:55.639400: Validation loss did not improve from -0.29375. Patience: 100/50
2024-12-21 10:43:55.642536: train_loss -0.8705
2024-12-21 10:43:55.644328: val_loss -0.1366
2024-12-21 10:43:55.645224: Pseudo dice [0.6121]
2024-12-21 10:43:55.646272: Epoch time: 200.78 s
2024-12-21 10:43:57.138324: 
2024-12-21 10:43:57.139645: Epoch 114
2024-12-21 10:43:57.140438: Current learning rate: 0.00277
2024-12-21 10:47:24.592862: Validation loss did not improve from -0.29375. Patience: 101/50
2024-12-21 10:47:24.593995: train_loss -0.8691
2024-12-21 10:47:24.595004: val_loss -0.0787
2024-12-21 10:47:24.595889: Pseudo dice [0.5999]
2024-12-21 10:47:24.596743: Epoch time: 207.46 s
2024-12-21 10:47:26.531074: 
2024-12-21 10:47:26.532792: Epoch 115
2024-12-21 10:47:26.533757: Current learning rate: 0.0027
2024-12-21 10:50:43.061567: Validation loss did not improve from -0.29375. Patience: 102/50
2024-12-21 10:50:43.062710: train_loss -0.8697
2024-12-21 10:50:43.063532: val_loss -0.0013
2024-12-21 10:50:43.064293: Pseudo dice [0.5547]
2024-12-21 10:50:43.065073: Epoch time: 196.53 s
2024-12-21 10:50:44.548088: 
2024-12-21 10:50:44.549482: Epoch 116
2024-12-21 10:50:44.550229: Current learning rate: 0.00263
2024-12-21 10:54:21.936157: Validation loss did not improve from -0.29375. Patience: 103/50
2024-12-21 10:54:21.937191: train_loss -0.8704
2024-12-21 10:54:21.938065: val_loss -0.1051
2024-12-21 10:54:21.938731: Pseudo dice [0.6051]
2024-12-21 10:54:21.939444: Epoch time: 217.39 s
2024-12-21 10:54:23.410058: 
2024-12-21 10:54:23.411385: Epoch 117
2024-12-21 10:54:23.412142: Current learning rate: 0.00256
2024-12-21 10:57:36.800259: Validation loss did not improve from -0.29375. Patience: 104/50
2024-12-21 10:57:36.801016: train_loss -0.8708
2024-12-21 10:57:36.802114: val_loss -0.0509
2024-12-21 10:57:36.802827: Pseudo dice [0.5837]
2024-12-21 10:57:36.803589: Epoch time: 193.39 s
2024-12-21 10:57:38.251331: 
2024-12-21 10:57:38.252470: Epoch 118
2024-12-21 10:57:38.253282: Current learning rate: 0.00249
2024-12-21 11:01:09.369184: Validation loss did not improve from -0.29375. Patience: 105/50
2024-12-21 11:01:09.369938: train_loss -0.8733
2024-12-21 11:01:09.370787: val_loss -0.0888
2024-12-21 11:01:09.371470: Pseudo dice [0.5953]
2024-12-21 11:01:09.372204: Epoch time: 211.12 s
2024-12-21 11:01:10.888844: 
2024-12-21 11:01:10.890246: Epoch 119
2024-12-21 11:01:10.891038: Current learning rate: 0.00242
2024-12-21 11:04:30.551854: Validation loss did not improve from -0.29375. Patience: 106/50
2024-12-21 11:04:30.553003: train_loss -0.8719
2024-12-21 11:04:30.553914: val_loss -0.0674
2024-12-21 11:04:30.554682: Pseudo dice [0.5786]
2024-12-21 11:04:30.555476: Epoch time: 199.67 s
2024-12-21 11:04:32.408019: 
2024-12-21 11:04:32.409573: Epoch 120
2024-12-21 11:04:32.410469: Current learning rate: 0.00235
2024-12-21 11:08:15.489574: Validation loss did not improve from -0.29375. Patience: 107/50
2024-12-21 11:08:15.491153: train_loss -0.8731
2024-12-21 11:08:15.492357: val_loss -0.0883
2024-12-21 11:08:15.493293: Pseudo dice [0.5981]
2024-12-21 11:08:15.494236: Epoch time: 223.08 s
2024-12-21 11:08:17.031591: 
2024-12-21 11:08:17.032946: Epoch 121
2024-12-21 11:08:17.033972: Current learning rate: 0.00228
2024-12-21 11:11:28.125969: Validation loss did not improve from -0.29375. Patience: 108/50
2024-12-21 11:11:28.127425: train_loss -0.8741
2024-12-21 11:11:28.128642: val_loss -0.0997
2024-12-21 11:11:28.129622: Pseudo dice [0.5965]
2024-12-21 11:11:28.130534: Epoch time: 191.1 s
2024-12-21 11:11:29.617110: 
2024-12-21 11:11:29.618574: Epoch 122
2024-12-21 11:11:29.619588: Current learning rate: 0.00221
2024-12-21 11:15:06.030985: Validation loss did not improve from -0.29375. Patience: 109/50
2024-12-21 11:15:06.032015: train_loss -0.8745
2024-12-21 11:15:06.032873: val_loss -0.0539
2024-12-21 11:15:06.033561: Pseudo dice [0.5838]
2024-12-21 11:15:06.034402: Epoch time: 216.42 s
2024-12-21 11:15:07.465467: 
2024-12-21 11:15:07.466666: Epoch 123
2024-12-21 11:15:07.467405: Current learning rate: 0.00214
2024-12-21 11:18:27.271363: Validation loss did not improve from -0.29375. Patience: 110/50
2024-12-21 11:18:27.272285: train_loss -0.8738
2024-12-21 11:18:27.273151: val_loss -0.1078
2024-12-21 11:18:27.273892: Pseudo dice [0.6063]
2024-12-21 11:18:27.274788: Epoch time: 199.81 s
2024-12-21 11:18:29.185748: 
2024-12-21 11:18:29.187288: Epoch 124
2024-12-21 11:18:29.188047: Current learning rate: 0.00207
2024-12-21 11:22:04.561171: Validation loss did not improve from -0.29375. Patience: 111/50
2024-12-21 11:22:04.562160: train_loss -0.8744
2024-12-21 11:22:04.563013: val_loss -0.0726
2024-12-21 11:22:04.563857: Pseudo dice [0.593]
2024-12-21 11:22:04.564656: Epoch time: 215.38 s
2024-12-21 11:22:06.446234: 
2024-12-21 11:22:06.447600: Epoch 125
2024-12-21 11:22:06.448492: Current learning rate: 0.00199
2024-12-21 11:25:29.005165: Validation loss did not improve from -0.29375. Patience: 112/50
2024-12-21 11:25:29.006129: train_loss -0.8759
2024-12-21 11:25:29.006982: val_loss -0.01
2024-12-21 11:25:29.007811: Pseudo dice [0.5657]
2024-12-21 11:25:29.008588: Epoch time: 202.56 s
2024-12-21 11:25:30.456389: 
2024-12-21 11:25:30.457760: Epoch 126
2024-12-21 11:25:30.458591: Current learning rate: 0.00192
2024-12-21 11:29:10.174307: Validation loss did not improve from -0.29375. Patience: 113/50
2024-12-21 11:29:10.175239: train_loss -0.8768
2024-12-21 11:29:10.175997: val_loss -0.0076
2024-12-21 11:29:10.176680: Pseudo dice [0.5721]
2024-12-21 11:29:10.177383: Epoch time: 219.72 s
2024-12-21 11:29:11.708131: 
2024-12-21 11:29:11.709488: Epoch 127
2024-12-21 11:29:11.710286: Current learning rate: 0.00185
2024-12-21 11:32:24.107273: Validation loss did not improve from -0.29375. Patience: 114/50
2024-12-21 11:32:24.108123: train_loss -0.879
2024-12-21 11:32:24.109139: val_loss -0.0562
2024-12-21 11:32:24.110260: Pseudo dice [0.5963]
2024-12-21 11:32:24.111359: Epoch time: 192.4 s
2024-12-21 11:32:25.578041: 
2024-12-21 11:32:25.579305: Epoch 128
2024-12-21 11:32:25.580181: Current learning rate: 0.00178
2024-12-21 11:35:58.862149: Validation loss did not improve from -0.29375. Patience: 115/50
2024-12-21 11:35:58.863025: train_loss -0.8786
2024-12-21 11:35:58.863825: val_loss -0.1075
2024-12-21 11:35:58.864578: Pseudo dice [0.6037]
2024-12-21 11:35:58.865459: Epoch time: 213.29 s
2024-12-21 11:36:00.301690: 
2024-12-21 11:36:00.302883: Epoch 129
2024-12-21 11:36:00.303687: Current learning rate: 0.0017
2024-12-21 11:39:16.978931: Validation loss did not improve from -0.29375. Patience: 116/50
2024-12-21 11:39:16.979930: train_loss -0.8776
2024-12-21 11:39:16.980729: val_loss -0.0586
2024-12-21 11:39:16.981411: Pseudo dice [0.5883]
2024-12-21 11:39:16.982074: Epoch time: 196.68 s
2024-12-21 11:39:18.887383: 
2024-12-21 11:39:18.888435: Epoch 130
2024-12-21 11:39:18.889212: Current learning rate: 0.00163
2024-12-21 11:43:04.226238: Validation loss did not improve from -0.29375. Patience: 117/50
2024-12-21 11:43:04.227406: train_loss -0.8784
2024-12-21 11:43:04.228188: val_loss -0.0779
2024-12-21 11:43:04.228981: Pseudo dice [0.5944]
2024-12-21 11:43:04.229905: Epoch time: 225.34 s
2024-12-21 11:43:05.730260: 
2024-12-21 11:43:05.731354: Epoch 131
2024-12-21 11:43:05.732339: Current learning rate: 0.00156
2024-12-21 11:46:21.007828: Validation loss did not improve from -0.29375. Patience: 118/50
2024-12-21 11:46:21.009763: train_loss -0.8798
2024-12-21 11:46:21.011576: val_loss -0.0747
2024-12-21 11:46:21.012473: Pseudo dice [0.5992]
2024-12-21 11:46:21.013444: Epoch time: 195.28 s
2024-12-21 11:46:22.471578: 
2024-12-21 11:46:22.472549: Epoch 132
2024-12-21 11:46:22.473472: Current learning rate: 0.00148
2024-12-21 11:50:09.348144: Validation loss did not improve from -0.29375. Patience: 119/50
2024-12-21 11:50:09.349572: train_loss -0.8786
2024-12-21 11:50:09.350579: val_loss -0.0816
2024-12-21 11:50:09.351341: Pseudo dice [0.5983]
2024-12-21 11:50:09.352226: Epoch time: 226.88 s
2024-12-21 11:50:10.854453: 
2024-12-21 11:50:10.855375: Epoch 133
2024-12-21 11:50:10.856230: Current learning rate: 0.00141
2024-12-21 11:53:29.942550: Validation loss did not improve from -0.29375. Patience: 120/50
2024-12-21 11:53:29.943781: train_loss -0.8811
2024-12-21 11:53:29.944749: val_loss -0.0613
2024-12-21 11:53:29.965677: Pseudo dice [0.5832]
2024-12-21 11:53:29.966877: Epoch time: 199.09 s
2024-12-21 11:53:31.943320: 
2024-12-21 11:53:31.944749: Epoch 134
2024-12-21 11:53:31.945843: Current learning rate: 0.00133
2024-12-21 11:57:07.941649: Validation loss did not improve from -0.29375. Patience: 121/50
2024-12-21 11:57:07.942466: train_loss -0.8812
2024-12-21 11:57:07.943202: val_loss -0.136
2024-12-21 11:57:07.943871: Pseudo dice [0.6084]
2024-12-21 11:57:07.944530: Epoch time: 216.0 s
2024-12-21 11:57:09.903390: 
2024-12-21 11:57:09.904801: Epoch 135
2024-12-21 11:57:09.905563: Current learning rate: 0.00126
2024-12-21 12:00:25.816050: Validation loss did not improve from -0.29375. Patience: 122/50
2024-12-21 12:00:25.816827: train_loss -0.8812
2024-12-21 12:00:25.817566: val_loss -0.0738
2024-12-21 12:00:25.818287: Pseudo dice [0.6141]
2024-12-21 12:00:25.819018: Epoch time: 195.91 s
2024-12-21 12:00:27.332355: 
2024-12-21 12:00:27.333255: Epoch 136
2024-12-21 12:00:27.334023: Current learning rate: 0.00118
2024-12-21 12:03:56.986963: Validation loss did not improve from -0.29375. Patience: 123/50
2024-12-21 12:03:56.987710: train_loss -0.8811
2024-12-21 12:03:56.988579: val_loss -0.068
2024-12-21 12:03:56.989417: Pseudo dice [0.5924]
2024-12-21 12:03:56.990124: Epoch time: 209.66 s
2024-12-21 12:03:58.523336: 
2024-12-21 12:03:58.524336: Epoch 137
2024-12-21 12:03:58.525155: Current learning rate: 0.00111
2024-12-21 12:07:09.366309: Validation loss did not improve from -0.29375. Patience: 124/50
2024-12-21 12:07:09.367304: train_loss -0.8813
2024-12-21 12:07:09.368404: val_loss -0.0856
2024-12-21 12:07:09.369544: Pseudo dice [0.6074]
2024-12-21 12:07:09.370625: Epoch time: 190.85 s
2024-12-21 12:07:10.839012: 
2024-12-21 12:07:10.839967: Epoch 138
2024-12-21 12:07:10.840959: Current learning rate: 0.00103
2024-12-21 12:10:46.307529: Validation loss did not improve from -0.29375. Patience: 125/50
2024-12-21 12:10:46.308609: train_loss -0.882
2024-12-21 12:10:46.309444: val_loss -0.0969
2024-12-21 12:10:46.310152: Pseudo dice [0.5963]
2024-12-21 12:10:46.311032: Epoch time: 215.47 s
2024-12-21 12:10:47.819942: 
2024-12-21 12:10:47.821347: Epoch 139
2024-12-21 12:10:47.822255: Current learning rate: 0.00095
2024-12-21 12:14:08.163120: Validation loss did not improve from -0.29375. Patience: 126/50
2024-12-21 12:14:08.164523: train_loss -0.8821
2024-12-21 12:14:08.165404: val_loss -0.0652
2024-12-21 12:14:08.166168: Pseudo dice [0.5867]
2024-12-21 12:14:08.166902: Epoch time: 200.35 s
2024-12-21 12:14:10.124218: 
2024-12-21 12:14:10.125588: Epoch 140
2024-12-21 12:14:10.126405: Current learning rate: 0.00087
2024-12-21 12:17:48.036726: Validation loss did not improve from -0.29375. Patience: 127/50
2024-12-21 12:17:48.037789: train_loss -0.8832
2024-12-21 12:17:48.038598: val_loss -0.0147
2024-12-21 12:17:48.039306: Pseudo dice [0.5722]
2024-12-21 12:17:48.040095: Epoch time: 217.92 s
2024-12-21 12:17:49.604664: 
2024-12-21 12:17:49.605575: Epoch 141
2024-12-21 12:17:49.606297: Current learning rate: 0.00079
2024-12-21 12:20:59.771843: Validation loss did not improve from -0.29375. Patience: 128/50
2024-12-21 12:20:59.772690: train_loss -0.883
2024-12-21 12:20:59.773560: val_loss -0.059
2024-12-21 12:20:59.774398: Pseudo dice [0.5842]
2024-12-21 12:20:59.775284: Epoch time: 190.17 s
2024-12-21 12:21:01.306709: 
2024-12-21 12:21:01.307957: Epoch 142
2024-12-21 12:21:01.308820: Current learning rate: 0.00071
2024-12-21 12:24:38.960180: Validation loss did not improve from -0.29375. Patience: 129/50
2024-12-21 12:24:38.960864: train_loss -0.881
2024-12-21 12:24:38.961799: val_loss -0.0845
2024-12-21 12:24:38.962651: Pseudo dice [0.5958]
2024-12-21 12:24:38.963587: Epoch time: 217.66 s
2024-12-21 12:24:40.469574: 
2024-12-21 12:24:40.470774: Epoch 143
2024-12-21 12:24:40.471691: Current learning rate: 0.00063
2024-12-21 12:27:57.078858: Validation loss did not improve from -0.29375. Patience: 130/50
2024-12-21 12:27:57.079849: train_loss -0.8833
2024-12-21 12:27:57.080683: val_loss -0.0595
2024-12-21 12:27:57.081487: Pseudo dice [0.5862]
2024-12-21 12:27:57.082341: Epoch time: 196.61 s
2024-12-21 12:27:58.562585: 
2024-12-21 12:27:58.563604: Epoch 144
2024-12-21 12:27:58.564402: Current learning rate: 0.00055
2024-12-21 12:31:44.124356: Validation loss did not improve from -0.29375. Patience: 131/50
2024-12-21 12:31:44.125298: train_loss -0.8837
2024-12-21 12:31:44.126076: val_loss -0.0164
2024-12-21 12:31:44.126773: Pseudo dice [0.5867]
2024-12-21 12:31:44.127467: Epoch time: 225.56 s
2024-12-21 12:31:46.445737: 
2024-12-21 12:31:46.446695: Epoch 145
2024-12-21 12:31:46.447386: Current learning rate: 0.00047
2024-12-21 12:35:02.813577: Validation loss did not improve from -0.29375. Patience: 132/50
2024-12-21 12:35:02.814342: train_loss -0.8838
2024-12-21 12:35:02.815434: val_loss -0.1138
2024-12-21 12:35:02.816426: Pseudo dice [0.6045]
2024-12-21 12:35:02.817369: Epoch time: 196.37 s
2024-12-21 12:35:04.279375: 
2024-12-21 12:35:04.280489: Epoch 146
2024-12-21 12:35:04.281207: Current learning rate: 0.00038
2024-12-21 12:38:57.056086: Validation loss did not improve from -0.29375. Patience: 133/50
2024-12-21 12:38:57.057072: train_loss -0.8852
2024-12-21 12:38:57.057976: val_loss -0.0601
2024-12-21 12:38:57.058763: Pseudo dice [0.5877]
2024-12-21 12:38:57.059540: Epoch time: 232.78 s
2024-12-21 12:38:58.557926: 
2024-12-21 12:38:58.559260: Epoch 147
2024-12-21 12:38:58.560317: Current learning rate: 0.0003
2024-12-21 12:42:12.156300: Validation loss did not improve from -0.29375. Patience: 134/50
2024-12-21 12:42:12.157011: train_loss -0.8856
2024-12-21 12:42:12.157781: val_loss -0.0479
2024-12-21 12:42:12.158490: Pseudo dice [0.5923]
2024-12-21 12:42:12.159194: Epoch time: 193.6 s
2024-12-21 12:42:13.658934: 
2024-12-21 12:42:13.659939: Epoch 148
2024-12-21 12:42:13.660776: Current learning rate: 0.00021
2024-12-21 12:45:54.820506: Validation loss did not improve from -0.29375. Patience: 135/50
2024-12-21 12:45:54.822142: train_loss -0.8837
2024-12-21 12:45:54.823245: val_loss -0.0983
2024-12-21 12:45:54.824189: Pseudo dice [0.6092]
2024-12-21 12:45:54.825299: Epoch time: 221.16 s
2024-12-21 12:45:56.411948: 
2024-12-21 12:45:56.412992: Epoch 149
2024-12-21 12:45:56.413990: Current learning rate: 0.00011
2024-12-21 12:49:12.369408: Validation loss did not improve from -0.29375. Patience: 136/50
2024-12-21 12:49:12.394602: train_loss -0.8863
2024-12-21 12:49:12.395596: val_loss -0.0196
2024-12-21 12:49:12.396553: Pseudo dice [0.5821]
2024-12-21 12:49:12.439000: Epoch time: 195.98 s
2024-12-21 12:49:14.487679: Training done.
2024-12-21 04:04:28.569175: unpacking done...
2024-12-21 04:04:28.577084: Unable to plot network architecture: nnUNet_compile is enabled!
2024-12-21 04:04:28.614882: 
2024-12-21 04:04:28.616113: Epoch 0
2024-12-21 04:04:28.617170: Current learning rate: 0.01
2024-12-21 04:08:53.686482: Validation loss improved from 1000.00000 to -0.14618! Patience: 0/50
2024-12-21 04:08:53.687546: train_loss -0.1639
2024-12-21 04:08:53.688504: val_loss -0.1462
2024-12-21 04:08:53.689233: Pseudo dice [0.5282]
2024-12-21 04:08:53.690046: Epoch time: 265.07 s
2024-12-21 04:08:53.690700: Yayy! New best EMA pseudo Dice: 0.5282
2024-12-21 04:08:55.367589: 
2024-12-21 04:08:55.368955: Epoch 1
2024-12-21 04:08:55.369831: Current learning rate: 0.00994
2024-12-21 04:12:31.829041: Validation loss improved from -0.14618 to -0.16576! Patience: 0/50
2024-12-21 04:12:31.830079: train_loss -0.3215
2024-12-21 04:12:31.831135: val_loss -0.1658
2024-12-21 04:12:31.831943: Pseudo dice [0.5226]
2024-12-21 04:12:31.832716: Epoch time: 216.46 s
2024-12-21 04:12:33.207452: 
2024-12-21 04:12:33.208932: Epoch 2
2024-12-21 04:12:33.210006: Current learning rate: 0.00988
2024-12-21 04:16:13.636696: Validation loss did not improve from -0.16576. Patience: 1/50
2024-12-21 04:16:13.637889: train_loss -0.381
2024-12-21 04:16:13.638762: val_loss -0.1625
2024-12-21 04:16:13.639596: Pseudo dice [0.5354]
2024-12-21 04:16:13.640380: Epoch time: 220.43 s
2024-12-21 04:16:13.641337: Yayy! New best EMA pseudo Dice: 0.5284
2024-12-21 04:16:15.601869: 
2024-12-21 04:16:15.603243: Epoch 3
2024-12-21 04:16:15.604201: Current learning rate: 0.00982
2024-12-21 04:20:14.468470: Validation loss improved from -0.16576 to -0.23261! Patience: 1/50
2024-12-21 04:20:14.469430: train_loss -0.4042
2024-12-21 04:20:14.470457: val_loss -0.2326
2024-12-21 04:20:14.471209: Pseudo dice [0.5576]
2024-12-21 04:20:14.471998: Epoch time: 238.87 s
2024-12-21 04:20:14.472799: Yayy! New best EMA pseudo Dice: 0.5313
2024-12-21 04:20:16.356680: 
2024-12-21 04:20:16.358067: Epoch 4
2024-12-21 04:20:16.358789: Current learning rate: 0.00976
2024-12-21 04:23:41.211308: Validation loss did not improve from -0.23261. Patience: 1/50
2024-12-21 04:23:41.212248: train_loss -0.4398
2024-12-21 04:23:41.213102: val_loss -0.2253
2024-12-21 04:23:41.213810: Pseudo dice [0.577]
2024-12-21 04:23:41.214684: Epoch time: 204.86 s
2024-12-21 04:23:41.547191: Yayy! New best EMA pseudo Dice: 0.5359
2024-12-21 04:23:43.384305: 
2024-12-21 04:23:43.385501: Epoch 5
2024-12-21 04:23:43.386409: Current learning rate: 0.0097
2024-12-21 04:27:38.764649: Validation loss improved from -0.23261 to -0.27301! Patience: 1/50
2024-12-21 04:27:38.765566: train_loss -0.4621
2024-12-21 04:27:38.766450: val_loss -0.273
2024-12-21 04:27:38.767383: Pseudo dice [0.6065]
2024-12-21 04:27:38.768203: Epoch time: 235.38 s
2024-12-21 04:27:38.769131: Yayy! New best EMA pseudo Dice: 0.543
2024-12-21 04:27:40.548895: 
2024-12-21 04:27:40.550287: Epoch 6
2024-12-21 04:27:40.551091: Current learning rate: 0.00964
2024-12-21 04:30:57.016298: Validation loss did not improve from -0.27301. Patience: 1/50
2024-12-21 04:30:57.017378: train_loss -0.4899
2024-12-21 04:30:57.018296: val_loss -0.2518
2024-12-21 04:30:57.019016: Pseudo dice [0.5992]
2024-12-21 04:30:57.019845: Epoch time: 196.47 s
2024-12-21 04:30:57.020539: Yayy! New best EMA pseudo Dice: 0.5486
2024-12-21 04:30:58.828729: 
2024-12-21 04:30:58.830260: Epoch 7
2024-12-21 04:30:58.830983: Current learning rate: 0.00958
2024-12-21 04:34:16.538002: Validation loss did not improve from -0.27301. Patience: 2/50
2024-12-21 04:34:16.539160: train_loss -0.5213
2024-12-21 04:34:16.540184: val_loss -0.2505
2024-12-21 04:34:16.541094: Pseudo dice [0.5885]
2024-12-21 04:34:16.541935: Epoch time: 197.71 s
2024-12-21 04:34:16.542894: Yayy! New best EMA pseudo Dice: 0.5526
2024-12-21 04:34:18.424826: 
2024-12-21 04:34:18.426408: Epoch 8
2024-12-21 04:34:18.427542: Current learning rate: 0.00952
2024-12-21 04:37:43.997059: Validation loss improved from -0.27301 to -0.29561! Patience: 2/50
2024-12-21 04:37:43.997959: train_loss -0.5396
2024-12-21 04:37:43.998788: val_loss -0.2956
2024-12-21 04:37:43.999516: Pseudo dice [0.6122]
2024-12-21 04:37:44.000240: Epoch time: 205.57 s
2024-12-21 04:37:44.001015: Yayy! New best EMA pseudo Dice: 0.5585
2024-12-21 04:37:46.282896: 
2024-12-21 04:37:46.284148: Epoch 9
2024-12-21 04:37:46.285017: Current learning rate: 0.00946
2024-12-21 04:41:23.153074: Validation loss improved from -0.29561 to -0.30819! Patience: 0/50
2024-12-21 04:41:23.154091: train_loss -0.5714
2024-12-21 04:41:23.155143: val_loss -0.3082
2024-12-21 04:41:23.155944: Pseudo dice [0.618]
2024-12-21 04:41:23.156848: Epoch time: 216.87 s
2024-12-21 04:41:23.584102: Yayy! New best EMA pseudo Dice: 0.5645
2024-12-21 04:41:25.352140: 
2024-12-21 04:41:25.353494: Epoch 10
2024-12-21 04:41:25.354371: Current learning rate: 0.0094
2024-12-21 04:44:52.696892: Validation loss did not improve from -0.30819. Patience: 1/50
2024-12-21 04:44:52.697895: train_loss -0.5887
2024-12-21 04:44:52.698686: val_loss -0.2387
2024-12-21 04:44:52.699407: Pseudo dice [0.6016]
2024-12-21 04:44:52.700183: Epoch time: 207.35 s
2024-12-21 04:44:52.701049: Yayy! New best EMA pseudo Dice: 0.5682
2024-12-21 04:44:54.506425: 
2024-12-21 04:44:54.507932: Epoch 11
2024-12-21 04:44:54.508798: Current learning rate: 0.00934
2024-12-21 04:48:20.349446: Validation loss did not improve from -0.30819. Patience: 2/50
2024-12-21 04:48:20.350564: train_loss -0.5906
2024-12-21 04:48:20.351393: val_loss -0.2741
2024-12-21 04:48:20.352192: Pseudo dice [0.6001]
2024-12-21 04:48:20.353034: Epoch time: 205.85 s
2024-12-21 04:48:20.353787: Yayy! New best EMA pseudo Dice: 0.5714
2024-12-21 04:48:22.089950: 
2024-12-21 04:48:22.091155: Epoch 12
2024-12-21 04:48:22.092000: Current learning rate: 0.00928
2024-12-21 04:51:45.575115: Validation loss did not improve from -0.30819. Patience: 3/50
2024-12-21 04:51:45.576254: train_loss -0.599
2024-12-21 04:51:45.577027: val_loss -0.2721
2024-12-21 04:51:45.577870: Pseudo dice [0.6065]
2024-12-21 04:51:45.578630: Epoch time: 203.49 s
2024-12-21 04:51:45.579325: Yayy! New best EMA pseudo Dice: 0.5749
2024-12-21 04:51:47.406924: 
2024-12-21 04:51:47.408072: Epoch 13
2024-12-21 04:51:47.408890: Current learning rate: 0.00922
2024-12-21 04:55:18.347989: Validation loss did not improve from -0.30819. Patience: 4/50
2024-12-21 04:55:18.349032: train_loss -0.6193
2024-12-21 04:55:18.349945: val_loss -0.2884
2024-12-21 04:55:18.350725: Pseudo dice [0.6161]
2024-12-21 04:55:18.351565: Epoch time: 210.94 s
2024-12-21 04:55:18.352254: Yayy! New best EMA pseudo Dice: 0.579
2024-12-21 04:55:20.135253: 
2024-12-21 04:55:20.136519: Epoch 14
2024-12-21 04:55:20.137327: Current learning rate: 0.00916
2024-12-21 04:58:46.485417: Validation loss did not improve from -0.30819. Patience: 5/50
2024-12-21 04:58:46.486612: train_loss -0.6345
2024-12-21 04:58:46.487913: val_loss -0.2716
2024-12-21 04:58:46.488971: Pseudo dice [0.6063]
2024-12-21 04:58:46.490082: Epoch time: 206.35 s
2024-12-21 04:58:46.937694: Yayy! New best EMA pseudo Dice: 0.5818
2024-12-21 04:58:48.792746: 
2024-12-21 04:58:48.794391: Epoch 15
2024-12-21 04:58:48.795523: Current learning rate: 0.0091
2024-12-21 05:02:23.870910: Validation loss did not improve from -0.30819. Patience: 6/50
2024-12-21 05:02:23.872062: train_loss -0.6487
2024-12-21 05:02:23.873215: val_loss -0.2249
2024-12-21 05:02:23.874277: Pseudo dice [0.5712]
2024-12-21 05:02:23.875233: Epoch time: 215.08 s
2024-12-21 05:02:25.267505: 
2024-12-21 05:02:25.268975: Epoch 16
2024-12-21 05:02:25.270129: Current learning rate: 0.00903
2024-12-21 05:05:52.954857: Validation loss did not improve from -0.30819. Patience: 7/50
2024-12-21 05:05:52.955872: train_loss -0.6559
2024-12-21 05:05:52.956716: val_loss -0.2393
2024-12-21 05:05:52.957555: Pseudo dice [0.617]
2024-12-21 05:05:52.958499: Epoch time: 207.69 s
2024-12-21 05:05:52.959347: Yayy! New best EMA pseudo Dice: 0.5843
2024-12-21 05:05:54.864871: 
2024-12-21 05:05:54.866379: Epoch 17
2024-12-21 05:05:54.867265: Current learning rate: 0.00897
2024-12-21 05:09:26.222431: Validation loss did not improve from -0.30819. Patience: 8/50
2024-12-21 05:09:26.341150: train_loss -0.6698
2024-12-21 05:09:26.342331: val_loss -0.208
2024-12-21 05:09:26.343450: Pseudo dice [0.5838]
2024-12-21 05:09:26.344288: Epoch time: 211.48 s
2024-12-21 05:09:28.187154: 
2024-12-21 05:09:28.188506: Epoch 18
2024-12-21 05:09:28.189282: Current learning rate: 0.00891
2024-12-21 05:12:54.940017: Validation loss did not improve from -0.30819. Patience: 9/50
2024-12-21 05:12:54.940905: train_loss -0.6718
2024-12-21 05:12:54.941820: val_loss -0.2559
2024-12-21 05:12:54.942695: Pseudo dice [0.6022]
2024-12-21 05:12:54.943508: Epoch time: 206.76 s
2024-12-21 05:12:54.944443: Yayy! New best EMA pseudo Dice: 0.5861
2024-12-21 05:12:57.282535: 
2024-12-21 05:12:57.283904: Epoch 19
2024-12-21 05:12:57.284760: Current learning rate: 0.00885
2024-12-21 05:16:25.121977: Validation loss did not improve from -0.30819. Patience: 10/50
2024-12-21 05:16:25.123740: train_loss -0.6882
2024-12-21 05:16:25.125332: val_loss -0.2054
2024-12-21 05:16:25.126091: Pseudo dice [0.5884]
2024-12-21 05:16:25.126978: Epoch time: 207.84 s
2024-12-21 05:16:25.535633: Yayy! New best EMA pseudo Dice: 0.5863
2024-12-21 05:16:27.455582: 
2024-12-21 05:16:27.456848: Epoch 20
2024-12-21 05:16:27.457731: Current learning rate: 0.00879
2024-12-21 05:19:56.172672: Validation loss did not improve from -0.30819. Patience: 11/50
2024-12-21 05:19:56.173930: train_loss -0.686
2024-12-21 05:19:56.175301: val_loss -0.2203
2024-12-21 05:19:56.176587: Pseudo dice [0.6107]
2024-12-21 05:19:56.177920: Epoch time: 208.72 s
2024-12-21 05:19:56.179034: Yayy! New best EMA pseudo Dice: 0.5887
2024-12-21 05:19:58.072909: 
2024-12-21 05:19:58.074193: Epoch 21
2024-12-21 05:19:58.075034: Current learning rate: 0.00873
2024-12-21 05:23:25.484273: Validation loss did not improve from -0.30819. Patience: 12/50
2024-12-21 05:23:25.485178: train_loss -0.6906
2024-12-21 05:23:25.486135: val_loss -0.2496
2024-12-21 05:23:25.487046: Pseudo dice [0.6122]
2024-12-21 05:23:25.487990: Epoch time: 207.41 s
2024-12-21 05:23:25.488961: Yayy! New best EMA pseudo Dice: 0.5911
2024-12-21 05:23:27.297795: 
2024-12-21 05:23:27.299472: Epoch 22
2024-12-21 05:23:27.300581: Current learning rate: 0.00867
2024-12-21 05:26:46.565206: Validation loss did not improve from -0.30819. Patience: 13/50
2024-12-21 05:26:46.566540: train_loss -0.695
2024-12-21 05:26:46.567510: val_loss -0.2586
2024-12-21 05:26:46.572565: Pseudo dice [0.6078]
2024-12-21 05:26:46.573480: Epoch time: 199.27 s
2024-12-21 05:26:46.574249: Yayy! New best EMA pseudo Dice: 0.5928
2024-12-21 05:26:48.305552: 
2024-12-21 05:26:48.306756: Epoch 23
2024-12-21 05:26:48.307507: Current learning rate: 0.00861
2024-12-21 05:30:20.254759: Validation loss did not improve from -0.30819. Patience: 14/50
2024-12-21 05:30:20.255861: train_loss -0.717
2024-12-21 05:30:20.256781: val_loss -0.2113
2024-12-21 05:30:20.257574: Pseudo dice [0.5961]
2024-12-21 05:30:20.258389: Epoch time: 211.95 s
2024-12-21 05:30:20.259294: Yayy! New best EMA pseudo Dice: 0.5931
2024-12-21 05:30:22.079820: 
2024-12-21 05:30:22.080880: Epoch 24
2024-12-21 05:30:22.081602: Current learning rate: 0.00855
2024-12-21 05:33:54.859945: Validation loss did not improve from -0.30819. Patience: 15/50
2024-12-21 05:33:54.860999: train_loss -0.7134
2024-12-21 05:33:54.861687: val_loss -0.281
2024-12-21 05:33:54.862377: Pseudo dice [0.6306]
2024-12-21 05:33:54.863178: Epoch time: 212.78 s
2024-12-21 05:33:55.277272: Yayy! New best EMA pseudo Dice: 0.5968
2024-12-21 05:33:57.088914: 
2024-12-21 05:33:57.089975: Epoch 25
2024-12-21 05:33:57.090775: Current learning rate: 0.00849
2024-12-21 05:37:28.112459: Validation loss did not improve from -0.30819. Patience: 16/50
2024-12-21 05:37:28.113379: train_loss -0.7171
2024-12-21 05:37:28.114299: val_loss -0.2193
2024-12-21 05:37:28.114977: Pseudo dice [0.6073]
2024-12-21 05:37:28.115709: Epoch time: 211.03 s
2024-12-21 05:37:28.116518: Yayy! New best EMA pseudo Dice: 0.5979
2024-12-21 05:37:29.886657: 
2024-12-21 05:37:29.887849: Epoch 26
2024-12-21 05:37:29.888637: Current learning rate: 0.00843
2024-12-21 05:40:53.658252: Validation loss did not improve from -0.30819. Patience: 17/50
2024-12-21 05:40:53.659331: train_loss -0.7282
2024-12-21 05:40:53.660180: val_loss -0.2518
2024-12-21 05:40:53.660826: Pseudo dice [0.6351]
2024-12-21 05:40:53.661502: Epoch time: 203.77 s
2024-12-21 05:40:53.662331: Yayy! New best EMA pseudo Dice: 0.6016
2024-12-21 05:40:55.523357: 
2024-12-21 05:40:55.524656: Epoch 27
2024-12-21 05:40:55.525436: Current learning rate: 0.00836
2024-12-21 05:44:22.184461: Validation loss did not improve from -0.30819. Patience: 18/50
2024-12-21 05:44:22.185548: train_loss -0.7362
2024-12-21 05:44:22.186387: val_loss -0.2934
2024-12-21 05:44:22.187163: Pseudo dice [0.6388]
2024-12-21 05:44:22.187937: Epoch time: 206.66 s
2024-12-21 05:44:22.188597: Yayy! New best EMA pseudo Dice: 0.6053
2024-12-21 05:44:24.000872: 
2024-12-21 05:44:24.002448: Epoch 28
2024-12-21 05:44:24.003256: Current learning rate: 0.0083
2024-12-21 05:47:55.779449: Validation loss did not improve from -0.30819. Patience: 19/50
2024-12-21 05:47:55.780660: train_loss -0.7341
2024-12-21 05:47:55.781683: val_loss -0.2422
2024-12-21 05:47:55.782435: Pseudo dice [0.619]
2024-12-21 05:47:55.783262: Epoch time: 211.78 s
2024-12-21 05:47:55.783933: Yayy! New best EMA pseudo Dice: 0.6067
2024-12-21 05:47:57.640174: 
2024-12-21 05:47:57.641636: Epoch 29
2024-12-21 05:47:57.642486: Current learning rate: 0.00824
2024-12-21 05:51:23.009569: Validation loss did not improve from -0.30819. Patience: 20/50
2024-12-21 05:51:23.010673: train_loss -0.7422
2024-12-21 05:51:23.011579: val_loss -0.2642
2024-12-21 05:51:23.012373: Pseudo dice [0.6282]
2024-12-21 05:51:23.013145: Epoch time: 205.37 s
2024-12-21 05:51:23.458265: Yayy! New best EMA pseudo Dice: 0.6088
2024-12-21 05:51:25.803590: 
2024-12-21 05:51:25.805044: Epoch 30
2024-12-21 05:51:25.805872: Current learning rate: 0.00818
2024-12-21 05:54:47.458897: Validation loss did not improve from -0.30819. Patience: 21/50
2024-12-21 05:54:47.459970: train_loss -0.7376
2024-12-21 05:54:47.460740: val_loss -0.2434
2024-12-21 05:54:47.461383: Pseudo dice [0.6246]
2024-12-21 05:54:47.462169: Epoch time: 201.66 s
2024-12-21 05:54:47.462772: Yayy! New best EMA pseudo Dice: 0.6104
2024-12-21 05:54:49.272749: 
2024-12-21 05:54:49.274143: Epoch 31
2024-12-21 05:54:49.274992: Current learning rate: 0.00812
2024-12-21 05:58:12.195056: Validation loss did not improve from -0.30819. Patience: 22/50
2024-12-21 05:58:12.195913: train_loss -0.7408
2024-12-21 05:58:12.196645: val_loss -0.2388
2024-12-21 05:58:12.197354: Pseudo dice [0.6022]
2024-12-21 05:58:12.198199: Epoch time: 202.92 s
2024-12-21 05:58:13.601795: 
2024-12-21 05:58:13.603092: Epoch 32
2024-12-21 05:58:13.603967: Current learning rate: 0.00806
2024-12-21 06:01:43.668652: Validation loss did not improve from -0.30819. Patience: 23/50
2024-12-21 06:01:43.669760: train_loss -0.7501
2024-12-21 06:01:43.670640: val_loss -0.2624
2024-12-21 06:01:43.671492: Pseudo dice [0.6291]
2024-12-21 06:01:43.672307: Epoch time: 210.07 s
2024-12-21 06:01:43.672961: Yayy! New best EMA pseudo Dice: 0.6115
2024-12-21 06:01:45.530942: 
2024-12-21 06:01:45.532218: Epoch 33
2024-12-21 06:01:45.533202: Current learning rate: 0.008
2024-12-21 06:05:09.807853: Validation loss did not improve from -0.30819. Patience: 24/50
2024-12-21 06:05:09.808951: train_loss -0.7581
2024-12-21 06:05:09.821111: val_loss -0.2251
2024-12-21 06:05:09.821885: Pseudo dice [0.6185]
2024-12-21 06:05:09.822606: Epoch time: 204.28 s
2024-12-21 06:05:09.823348: Yayy! New best EMA pseudo Dice: 0.6122
2024-12-21 06:05:11.653053: 
2024-12-21 06:05:11.654316: Epoch 34
2024-12-21 06:05:11.655177: Current learning rate: 0.00793
2024-12-21 06:08:40.855397: Validation loss did not improve from -0.30819. Patience: 25/50
2024-12-21 06:08:40.856349: train_loss -0.7613
2024-12-21 06:08:40.857117: val_loss -0.247
2024-12-21 06:08:40.857839: Pseudo dice [0.6258]
2024-12-21 06:08:40.858651: Epoch time: 209.2 s
2024-12-21 06:08:41.286723: Yayy! New best EMA pseudo Dice: 0.6136
2024-12-21 06:08:43.152286: 
2024-12-21 06:08:43.153577: Epoch 35
2024-12-21 06:08:43.154300: Current learning rate: 0.00787
2024-12-21 06:12:15.632560: Validation loss did not improve from -0.30819. Patience: 26/50
2024-12-21 06:12:15.633721: train_loss -0.759
2024-12-21 06:12:15.634533: val_loss -0.2475
2024-12-21 06:12:15.635221: Pseudo dice [0.6096]
2024-12-21 06:12:15.635999: Epoch time: 212.48 s
2024-12-21 06:12:17.144930: 
2024-12-21 06:12:17.146668: Epoch 36
2024-12-21 06:12:17.147538: Current learning rate: 0.00781
2024-12-21 06:15:44.123188: Validation loss did not improve from -0.30819. Patience: 27/50
2024-12-21 06:15:44.128003: train_loss -0.7556
2024-12-21 06:15:44.137126: val_loss -0.2437
2024-12-21 06:15:44.138382: Pseudo dice [0.6393]
2024-12-21 06:15:44.139403: Epoch time: 206.98 s
2024-12-21 06:15:44.140210: Yayy! New best EMA pseudo Dice: 0.6158
2024-12-21 06:15:45.987829: 
2024-12-21 06:15:45.989040: Epoch 37
2024-12-21 06:15:45.989791: Current learning rate: 0.00775
2024-12-21 06:19:17.971078: Validation loss did not improve from -0.30819. Patience: 28/50
2024-12-21 06:19:17.972057: train_loss -0.7642
2024-12-21 06:19:17.973059: val_loss -0.2654
2024-12-21 06:19:17.973951: Pseudo dice [0.633]
2024-12-21 06:19:17.974936: Epoch time: 211.99 s
2024-12-21 06:19:17.975786: Yayy! New best EMA pseudo Dice: 0.6175
2024-12-21 06:19:19.827026: 
2024-12-21 06:19:19.828327: Epoch 38
2024-12-21 06:19:19.829582: Current learning rate: 0.00769
2024-12-21 06:22:48.618594: Validation loss did not improve from -0.30819. Patience: 29/50
2024-12-21 06:22:48.619628: train_loss -0.7719
2024-12-21 06:22:48.620502: val_loss -0.2225
2024-12-21 06:22:48.621287: Pseudo dice [0.6192]
2024-12-21 06:22:48.622015: Epoch time: 208.79 s
2024-12-21 06:22:48.622717: Yayy! New best EMA pseudo Dice: 0.6177
2024-12-21 06:22:50.507785: 
2024-12-21 06:22:50.509120: Epoch 39
2024-12-21 06:22:50.510006: Current learning rate: 0.00763
2024-12-21 06:26:18.939481: Validation loss did not improve from -0.30819. Patience: 30/50
2024-12-21 06:26:18.940384: train_loss -0.7695
2024-12-21 06:26:18.941175: val_loss -0.2142
2024-12-21 06:26:18.941863: Pseudo dice [0.6024]
2024-12-21 06:26:18.942526: Epoch time: 208.43 s
2024-12-21 06:26:21.300014: 
2024-12-21 06:26:21.301455: Epoch 40
2024-12-21 06:26:21.302268: Current learning rate: 0.00756
2024-12-21 06:29:47.957878: Validation loss did not improve from -0.30819. Patience: 31/50
2024-12-21 06:29:47.958928: train_loss -0.7742
2024-12-21 06:29:47.960009: val_loss -0.2823
2024-12-21 06:29:47.960826: Pseudo dice [0.6498]
2024-12-21 06:29:47.962107: Epoch time: 206.66 s
2024-12-21 06:29:47.962960: Yayy! New best EMA pseudo Dice: 0.6195
2024-12-21 06:29:49.822579: 
2024-12-21 06:29:49.823820: Epoch 41
2024-12-21 06:29:49.824711: Current learning rate: 0.0075
2024-12-21 06:33:16.627141: Validation loss did not improve from -0.30819. Patience: 32/50
2024-12-21 06:33:16.628265: train_loss -0.7755
2024-12-21 06:33:16.629181: val_loss -0.267
2024-12-21 06:33:16.629977: Pseudo dice [0.6444]
2024-12-21 06:33:16.630753: Epoch time: 206.81 s
2024-12-21 06:33:16.631458: Yayy! New best EMA pseudo Dice: 0.622
2024-12-21 06:33:18.449892: 
2024-12-21 06:33:18.451240: Epoch 42
2024-12-21 06:33:18.452075: Current learning rate: 0.00744
2024-12-21 06:36:57.878929: Validation loss did not improve from -0.30819. Patience: 33/50
2024-12-21 06:36:57.879924: train_loss -0.7806
2024-12-21 06:36:57.880692: val_loss -0.1915
2024-12-21 06:36:57.881477: Pseudo dice [0.636]
2024-12-21 06:36:57.882304: Epoch time: 219.43 s
2024-12-21 06:36:57.882958: Yayy! New best EMA pseudo Dice: 0.6234
2024-12-21 06:36:59.648665: 
2024-12-21 06:36:59.650042: Epoch 43
2024-12-21 06:36:59.650836: Current learning rate: 0.00738
2024-12-21 06:40:34.293848: Validation loss did not improve from -0.30819. Patience: 34/50
2024-12-21 06:40:34.294943: train_loss -0.781
2024-12-21 06:40:34.295938: val_loss -0.2448
2024-12-21 06:40:34.296887: Pseudo dice [0.6349]
2024-12-21 06:40:34.297901: Epoch time: 214.65 s
2024-12-21 06:40:34.298798: Yayy! New best EMA pseudo Dice: 0.6246
2024-12-21 06:40:36.124542: 
2024-12-21 06:40:36.126161: Epoch 44
2024-12-21 06:40:36.127252: Current learning rate: 0.00732
2024-12-21 06:44:00.047942: Validation loss did not improve from -0.30819. Patience: 35/50
2024-12-21 06:44:00.049149: train_loss -0.7797
2024-12-21 06:44:00.050087: val_loss -0.2207
2024-12-21 06:44:00.050739: Pseudo dice [0.6374]
2024-12-21 06:44:00.051464: Epoch time: 203.93 s
2024-12-21 06:44:00.465443: Yayy! New best EMA pseudo Dice: 0.6258
2024-12-21 06:44:02.273750: 
2024-12-21 06:44:02.274968: Epoch 45
2024-12-21 06:44:02.276047: Current learning rate: 0.00725
2024-12-21 06:47:29.893540: Validation loss did not improve from -0.30819. Patience: 36/50
2024-12-21 06:47:29.894565: train_loss -0.7803
2024-12-21 06:47:29.895533: val_loss -0.2444
2024-12-21 06:47:29.896297: Pseudo dice [0.6297]
2024-12-21 06:47:29.897255: Epoch time: 207.62 s
2024-12-21 06:47:29.898126: Yayy! New best EMA pseudo Dice: 0.6262
2024-12-21 06:47:31.673060: 
2024-12-21 06:47:31.673943: Epoch 46
2024-12-21 06:47:31.674683: Current learning rate: 0.00719
2024-12-21 06:50:58.629056: Validation loss did not improve from -0.30819. Patience: 37/50
2024-12-21 06:50:58.630025: train_loss -0.7898
2024-12-21 06:50:58.630979: val_loss -0.1826
2024-12-21 06:50:58.631732: Pseudo dice [0.623]
2024-12-21 06:50:58.632473: Epoch time: 206.96 s
2024-12-21 06:50:59.978013: 
2024-12-21 06:50:59.979471: Epoch 47
2024-12-21 06:50:59.980399: Current learning rate: 0.00713
2024-12-21 06:54:31.608448: Validation loss did not improve from -0.30819. Patience: 38/50
2024-12-21 06:54:31.609519: train_loss -0.7898
2024-12-21 06:54:31.610331: val_loss -0.2502
2024-12-21 06:54:31.611212: Pseudo dice [0.635]
2024-12-21 06:54:31.611956: Epoch time: 211.63 s
2024-12-21 06:54:31.612660: Yayy! New best EMA pseudo Dice: 0.6268
2024-12-21 06:54:33.427770: 
2024-12-21 06:54:33.429311: Epoch 48
2024-12-21 06:54:33.430186: Current learning rate: 0.00707
2024-12-21 06:57:53.200283: Validation loss did not improve from -0.30819. Patience: 39/50
2024-12-21 06:57:53.201383: train_loss -0.7915
2024-12-21 06:57:53.202425: val_loss -0.2021
2024-12-21 06:57:53.203325: Pseudo dice [0.6313]
2024-12-21 06:57:53.204122: Epoch time: 199.78 s
2024-12-21 06:57:53.204943: Yayy! New best EMA pseudo Dice: 0.6273
2024-12-21 06:57:55.004528: 
2024-12-21 06:57:55.005989: Epoch 49
2024-12-21 06:57:55.006815: Current learning rate: 0.007
2024-12-21 07:01:25.093559: Validation loss did not improve from -0.30819. Patience: 40/50
2024-12-21 07:01:25.094885: train_loss -0.7935
2024-12-21 07:01:25.096020: val_loss -0.2033
2024-12-21 07:01:25.097061: Pseudo dice [0.6252]
2024-12-21 07:01:25.098099: Epoch time: 210.09 s
2024-12-21 07:01:26.960820: 
2024-12-21 07:01:26.962211: Epoch 50
2024-12-21 07:01:26.962955: Current learning rate: 0.00694
2024-12-21 07:04:55.392750: Validation loss did not improve from -0.30819. Patience: 41/50
2024-12-21 07:04:55.393822: train_loss -0.7936
2024-12-21 07:04:55.394953: val_loss -0.2235
2024-12-21 07:04:55.395855: Pseudo dice [0.6288]
2024-12-21 07:04:55.396868: Epoch time: 208.43 s
2024-12-21 07:04:57.187160: 
2024-12-21 07:04:57.194446: Epoch 51
2024-12-21 07:04:57.195473: Current learning rate: 0.00688
2024-12-21 07:08:21.372554: Validation loss did not improve from -0.30819. Patience: 42/50
2024-12-21 07:08:21.373638: train_loss -0.7904
2024-12-21 07:08:21.374630: val_loss -0.2668
2024-12-21 07:08:21.375434: Pseudo dice [0.6508]
2024-12-21 07:08:21.376318: Epoch time: 204.19 s
2024-12-21 07:08:21.377121: Yayy! New best EMA pseudo Dice: 0.6296
2024-12-21 07:08:23.158863: 
2024-12-21 07:08:23.160117: Epoch 52
2024-12-21 07:08:23.161049: Current learning rate: 0.00682
2024-12-21 07:11:53.207290: Validation loss did not improve from -0.30819. Patience: 43/50
2024-12-21 07:11:53.208405: train_loss -0.7972
2024-12-21 07:11:53.209629: val_loss -0.2021
2024-12-21 07:11:53.210604: Pseudo dice [0.6348]
2024-12-21 07:11:53.211711: Epoch time: 210.05 s
2024-12-21 07:11:53.212814: Yayy! New best EMA pseudo Dice: 0.6301
2024-12-21 07:11:55.136142: 
2024-12-21 07:11:55.137657: Epoch 53
2024-12-21 07:11:55.138784: Current learning rate: 0.00675
2024-12-21 07:15:25.938025: Validation loss did not improve from -0.30819. Patience: 44/50
2024-12-21 07:15:25.939117: train_loss -0.7972
2024-12-21 07:15:25.940016: val_loss -0.2386
2024-12-21 07:15:25.940862: Pseudo dice [0.6449]
2024-12-21 07:15:25.941799: Epoch time: 210.8 s
2024-12-21 07:15:25.942553: Yayy! New best EMA pseudo Dice: 0.6316
2024-12-21 07:15:27.761940: 
2024-12-21 07:15:27.763231: Epoch 54
2024-12-21 07:15:27.764097: Current learning rate: 0.00669
2024-12-21 07:18:55.459953: Validation loss did not improve from -0.30819. Patience: 45/50
2024-12-21 07:18:55.461074: train_loss -0.798
2024-12-21 07:18:55.462254: val_loss -0.2321
2024-12-21 07:18:55.463227: Pseudo dice [0.6422]
2024-12-21 07:18:55.464110: Epoch time: 207.7 s
2024-12-21 07:18:55.878899: Yayy! New best EMA pseudo Dice: 0.6327
2024-12-21 07:18:57.674495: 
2024-12-21 07:18:57.675474: Epoch 55
2024-12-21 07:18:57.676178: Current learning rate: 0.00663
2024-12-21 07:22:20.896592: Validation loss did not improve from -0.30819. Patience: 46/50
2024-12-21 07:22:20.897904: train_loss -0.8011
2024-12-21 07:22:20.898859: val_loss -0.2372
2024-12-21 07:22:20.899681: Pseudo dice [0.6328]
2024-12-21 07:22:20.900393: Epoch time: 203.22 s
2024-12-21 07:22:20.901191: Yayy! New best EMA pseudo Dice: 0.6327
2024-12-21 07:22:22.813836: 
2024-12-21 07:22:22.815166: Epoch 56
2024-12-21 07:22:22.815959: Current learning rate: 0.00657
2024-12-21 07:25:55.970425: Validation loss did not improve from -0.30819. Patience: 47/50
2024-12-21 07:25:55.971312: train_loss -0.8046
2024-12-21 07:25:55.972718: val_loss -0.1983
2024-12-21 07:25:55.973890: Pseudo dice [0.6254]
2024-12-21 07:25:55.975131: Epoch time: 213.16 s
2024-12-21 07:25:57.407384: 
2024-12-21 07:25:57.408765: Epoch 57
2024-12-21 07:25:57.409734: Current learning rate: 0.0065
2024-12-21 07:29:27.196438: Validation loss did not improve from -0.30819. Patience: 48/50
2024-12-21 07:29:27.197591: train_loss -0.8032
2024-12-21 07:29:27.198427: val_loss -0.2165
2024-12-21 07:29:27.199244: Pseudo dice [0.6286]
2024-12-21 07:29:27.200061: Epoch time: 209.79 s
2024-12-21 07:29:28.654525: 
2024-12-21 07:29:28.655890: Epoch 58
2024-12-21 07:29:28.656778: Current learning rate: 0.00644
2024-12-21 07:32:50.353839: Validation loss did not improve from -0.30819. Patience: 49/50
2024-12-21 07:32:50.354676: train_loss -0.8011
2024-12-21 07:32:50.355610: val_loss -0.1943
2024-12-21 07:32:50.356436: Pseudo dice [0.6348]
2024-12-21 07:32:50.357189: Epoch time: 201.7 s
2024-12-21 07:32:51.757030: 
2024-12-21 07:32:51.757869: Epoch 59
2024-12-21 07:32:51.758645: Current learning rate: 0.00638
2024-12-21 07:36:16.510395: Validation loss did not improve from -0.30819. Patience: 50/50
2024-12-21 07:36:16.511410: train_loss -0.8092
2024-12-21 07:36:16.512504: val_loss -0.2325
2024-12-21 07:36:16.513510: Pseudo dice [0.6351]
2024-12-21 07:36:16.514508: Epoch time: 204.76 s
2024-12-21 07:36:18.360924: 
2024-12-21 07:36:18.362375: Epoch 60
2024-12-21 07:36:18.363263: Current learning rate: 0.00631
2024-12-21 07:39:42.480896: Validation loss did not improve from -0.30819. Patience: 51/50
2024-12-21 07:39:42.481949: train_loss -0.8096
2024-12-21 07:39:42.482779: val_loss -0.2316
2024-12-21 07:39:42.483541: Pseudo dice [0.6422]
2024-12-21 07:39:42.484210: Epoch time: 204.12 s
2024-12-21 07:39:42.485024: Yayy! New best EMA pseudo Dice: 0.6332
2024-12-21 07:39:44.401328: 
2024-12-21 07:39:44.402746: Epoch 61
2024-12-21 07:39:44.403540: Current learning rate: 0.00625
2024-12-21 07:43:15.008618: Validation loss did not improve from -0.30819. Patience: 52/50
2024-12-21 07:43:15.009643: train_loss -0.8111
2024-12-21 07:43:15.010463: val_loss -0.181
2024-12-21 07:43:15.011283: Pseudo dice [0.6286]
2024-12-21 07:43:15.012014: Epoch time: 210.61 s
2024-12-21 07:43:16.860952: 
2024-12-21 07:43:16.862518: Epoch 62
2024-12-21 07:43:16.863349: Current learning rate: 0.00619
2024-12-21 07:46:38.008586: Validation loss did not improve from -0.30819. Patience: 53/50
2024-12-21 07:46:38.009475: train_loss -0.8077
2024-12-21 07:46:38.010539: val_loss -0.1886
2024-12-21 07:46:38.011492: Pseudo dice [0.6213]
2024-12-21 07:46:38.012408: Epoch time: 201.15 s
2024-12-21 07:46:39.505883: 
2024-12-21 07:46:39.507189: Epoch 63
2024-12-21 07:46:39.508210: Current learning rate: 0.00612
2024-12-21 07:50:05.153571: Validation loss did not improve from -0.30819. Patience: 54/50
2024-12-21 07:50:05.154554: train_loss -0.8116
2024-12-21 07:50:05.155532: val_loss -0.2213
2024-12-21 07:50:05.156328: Pseudo dice [0.6412]
2024-12-21 07:50:05.157138: Epoch time: 205.65 s
2024-12-21 07:50:06.641621: 
2024-12-21 07:50:06.642730: Epoch 64
2024-12-21 07:50:06.643523: Current learning rate: 0.00606
2024-12-21 07:53:35.499552: Validation loss did not improve from -0.30819. Patience: 55/50
2024-12-21 07:53:35.500475: train_loss -0.817
2024-12-21 07:53:35.501476: val_loss -0.2032
2024-12-21 07:53:35.502246: Pseudo dice [0.6382]
2024-12-21 07:53:35.502950: Epoch time: 208.86 s
2024-12-21 07:53:37.345761: 
2024-12-21 07:53:37.347242: Epoch 65
2024-12-21 07:53:37.347962: Current learning rate: 0.006
2024-12-21 07:56:59.278673: Validation loss did not improve from -0.30819. Patience: 56/50
2024-12-21 07:56:59.279569: train_loss -0.8192
2024-12-21 07:56:59.280434: val_loss -0.1953
2024-12-21 07:56:59.281182: Pseudo dice [0.6384]
2024-12-21 07:56:59.281976: Epoch time: 201.93 s
2024-12-21 07:56:59.282713: Yayy! New best EMA pseudo Dice: 0.6337
2024-12-21 07:57:01.190901: 
2024-12-21 07:57:01.192322: Epoch 66
2024-12-21 07:57:01.193273: Current learning rate: 0.00593
2024-12-21 08:00:26.991543: Validation loss did not improve from -0.30819. Patience: 57/50
2024-12-21 08:00:26.993179: train_loss -0.8183
2024-12-21 08:00:26.994146: val_loss -0.2098
2024-12-21 08:00:26.994886: Pseudo dice [0.6408]
2024-12-21 08:00:26.995595: Epoch time: 205.8 s
2024-12-21 08:00:26.996424: Yayy! New best EMA pseudo Dice: 0.6344
2024-12-21 08:00:28.861190: 
2024-12-21 08:00:28.862603: Epoch 67
2024-12-21 08:00:28.863451: Current learning rate: 0.00587
2024-12-21 08:03:48.426600: Validation loss did not improve from -0.30819. Patience: 58/50
2024-12-21 08:03:48.427687: train_loss -0.8169
2024-12-21 08:03:48.428687: val_loss -0.2445
2024-12-21 08:03:48.429610: Pseudo dice [0.6481]
2024-12-21 08:03:48.430499: Epoch time: 199.57 s
2024-12-21 08:03:48.431308: Yayy! New best EMA pseudo Dice: 0.6358
2024-12-21 08:03:50.343970: 
2024-12-21 08:03:50.345447: Epoch 68
2024-12-21 08:03:50.346479: Current learning rate: 0.00581
2024-12-21 08:07:18.447997: Validation loss did not improve from -0.30819. Patience: 59/50
2024-12-21 08:07:18.448833: train_loss -0.8167
2024-12-21 08:07:18.449653: val_loss -0.1863
2024-12-21 08:07:18.450399: Pseudo dice [0.6225]
2024-12-21 08:07:18.451120: Epoch time: 208.11 s
2024-12-21 08:07:19.864107: 
2024-12-21 08:07:19.865053: Epoch 69
2024-12-21 08:07:19.865861: Current learning rate: 0.00574
2024-12-21 08:10:47.496534: Validation loss did not improve from -0.30819. Patience: 60/50
2024-12-21 08:10:47.497714: train_loss -0.8211
2024-12-21 08:10:47.498528: val_loss -0.2239
2024-12-21 08:10:47.499319: Pseudo dice [0.6437]
2024-12-21 08:10:47.500039: Epoch time: 207.63 s
2024-12-21 08:10:49.333039: 
2024-12-21 08:10:49.334357: Epoch 70
2024-12-21 08:10:49.335227: Current learning rate: 0.00568
2024-12-21 08:14:17.725643: Validation loss did not improve from -0.30819. Patience: 61/50
2024-12-21 08:14:17.726351: train_loss -0.8224
2024-12-21 08:14:17.727195: val_loss -0.1709
2024-12-21 08:14:17.727971: Pseudo dice [0.6172]
2024-12-21 08:14:17.728854: Epoch time: 208.39 s
2024-12-21 08:14:19.185165: 
2024-12-21 08:14:19.186144: Epoch 71
2024-12-21 08:14:19.186906: Current learning rate: 0.00562
2024-12-21 08:17:42.671232: Validation loss did not improve from -0.30819. Patience: 62/50
2024-12-21 08:17:42.672194: train_loss -0.8213
2024-12-21 08:17:42.673145: val_loss -0.1912
2024-12-21 08:17:42.673995: Pseudo dice [0.6303]
2024-12-21 08:17:42.674958: Epoch time: 203.49 s
2024-12-21 08:17:44.566170: 
2024-12-21 08:17:44.567442: Epoch 72
2024-12-21 08:17:44.568271: Current learning rate: 0.00555
2024-12-21 08:21:13.331965: Validation loss did not improve from -0.30819. Patience: 63/50
2024-12-21 08:21:13.333004: train_loss -0.8242
2024-12-21 08:21:13.333870: val_loss -0.2599
2024-12-21 08:21:13.334637: Pseudo dice [0.662]
2024-12-21 08:21:13.335413: Epoch time: 208.77 s
2024-12-21 08:21:13.336102: Yayy! New best EMA pseudo Dice: 0.6361
2024-12-21 08:21:15.154100: 
2024-12-21 08:21:15.155432: Epoch 73
2024-12-21 08:21:15.156221: Current learning rate: 0.00549
2024-12-21 08:24:39.497624: Validation loss did not improve from -0.30819. Patience: 64/50
2024-12-21 08:24:39.499894: train_loss -0.8253
2024-12-21 08:24:39.501062: val_loss -0.251
2024-12-21 08:24:39.501953: Pseudo dice [0.6622]
2024-12-21 08:24:39.503061: Epoch time: 204.35 s
2024-12-21 08:24:39.504245: Yayy! New best EMA pseudo Dice: 0.6387
2024-12-21 08:24:41.378937: 
2024-12-21 08:24:41.380460: Epoch 74
2024-12-21 08:24:41.381373: Current learning rate: 0.00542
2024-12-21 08:28:10.957335: Validation loss did not improve from -0.30819. Patience: 65/50
2024-12-21 08:28:10.958417: train_loss -0.8294
2024-12-21 08:28:10.959348: val_loss -0.2007
2024-12-21 08:28:10.960234: Pseudo dice [0.6363]
2024-12-21 08:28:10.961221: Epoch time: 209.58 s
2024-12-21 08:28:12.854427: 
2024-12-21 08:28:12.855969: Epoch 75
2024-12-21 08:28:12.857045: Current learning rate: 0.00536
2024-12-21 08:31:36.819743: Validation loss did not improve from -0.30819. Patience: 66/50
2024-12-21 08:31:36.822714: train_loss -0.8296
2024-12-21 08:31:36.824791: val_loss -0.173
2024-12-21 08:31:36.826148: Pseudo dice [0.6324]
2024-12-21 08:31:36.827263: Epoch time: 203.97 s
2024-12-21 08:31:38.363467: 
2024-12-21 08:31:38.364781: Epoch 76
2024-12-21 08:31:38.365721: Current learning rate: 0.00529
2024-12-21 08:35:04.923637: Validation loss did not improve from -0.30819. Patience: 67/50
2024-12-21 08:35:04.924513: train_loss -0.8301
2024-12-21 08:35:04.925348: val_loss -0.2011
2024-12-21 08:35:04.926120: Pseudo dice [0.6397]
2024-12-21 08:35:04.926914: Epoch time: 206.56 s
2024-12-21 08:35:06.342607: 
2024-12-21 08:35:06.343970: Epoch 77
2024-12-21 08:35:06.344807: Current learning rate: 0.00523
2024-12-21 08:38:34.692024: Validation loss did not improve from -0.30819. Patience: 68/50
2024-12-21 08:38:34.693032: train_loss -0.8274
2024-12-21 08:38:34.693860: val_loss -0.1746
2024-12-21 08:38:34.694619: Pseudo dice [0.636]
2024-12-21 08:38:34.695357: Epoch time: 208.35 s
2024-12-21 08:38:36.159535: 
2024-12-21 08:38:36.160922: Epoch 78
2024-12-21 08:38:36.161778: Current learning rate: 0.00517
2024-12-21 08:42:06.574506: Validation loss did not improve from -0.30819. Patience: 69/50
2024-12-21 08:42:06.575430: train_loss -0.829
2024-12-21 08:42:06.576182: val_loss -0.1639
2024-12-21 08:42:06.577049: Pseudo dice [0.6286]
2024-12-21 08:42:06.577781: Epoch time: 210.42 s
2024-12-21 08:42:08.108611: 
2024-12-21 08:42:08.110121: Epoch 79
2024-12-21 08:42:08.110958: Current learning rate: 0.0051
2024-12-21 08:45:26.946210: Validation loss did not improve from -0.30819. Patience: 70/50
2024-12-21 08:45:26.947348: train_loss -0.8327
2024-12-21 08:45:26.948509: val_loss -0.194
2024-12-21 08:45:26.949518: Pseudo dice [0.6407]
2024-12-21 08:45:26.950614: Epoch time: 198.84 s
2024-12-21 08:45:28.820022: 
2024-12-21 08:45:28.821343: Epoch 80
2024-12-21 08:45:28.822126: Current learning rate: 0.00504
2024-12-21 08:48:56.481409: Validation loss did not improve from -0.30819. Patience: 71/50
2024-12-21 08:48:56.482409: train_loss -0.8343
2024-12-21 08:48:56.483330: val_loss -0.1916
2024-12-21 08:48:56.484431: Pseudo dice [0.6372]
2024-12-21 08:48:56.485335: Epoch time: 207.66 s
2024-12-21 08:48:57.954662: 
2024-12-21 08:48:57.956147: Epoch 81
2024-12-21 08:48:57.956955: Current learning rate: 0.00497
2024-12-21 08:52:23.649190: Validation loss did not improve from -0.30819. Patience: 72/50
2024-12-21 08:52:23.650192: train_loss -0.8347
2024-12-21 08:52:23.651325: val_loss -0.1934
2024-12-21 08:52:23.652325: Pseudo dice [0.6496]
2024-12-21 08:52:23.653447: Epoch time: 205.7 s
2024-12-21 08:52:25.103297: 
2024-12-21 08:52:25.105088: Epoch 82
2024-12-21 08:52:25.106172: Current learning rate: 0.00491
2024-12-21 08:55:56.607031: Validation loss did not improve from -0.30819. Patience: 73/50
2024-12-21 08:55:56.608035: train_loss -0.8374
2024-12-21 08:55:56.608918: val_loss -0.2245
2024-12-21 08:55:56.609714: Pseudo dice [0.6456]
2024-12-21 08:55:56.610447: Epoch time: 211.51 s
2024-12-21 08:55:56.611244: Yayy! New best EMA pseudo Dice: 0.6392
2024-12-21 08:55:58.749433: 
2024-12-21 08:55:58.750687: Epoch 83
2024-12-21 08:55:58.751520: Current learning rate: 0.00484
2024-12-21 08:59:21.146125: Validation loss did not improve from -0.30819. Patience: 74/50
2024-12-21 08:59:21.147164: train_loss -0.8391
2024-12-21 08:59:21.147983: val_loss -0.171
2024-12-21 08:59:21.148797: Pseudo dice [0.6343]
2024-12-21 08:59:21.149550: Epoch time: 202.4 s
2024-12-21 08:59:22.515191: 
2024-12-21 08:59:22.516751: Epoch 84
2024-12-21 08:59:22.517661: Current learning rate: 0.00478
2024-12-21 09:02:56.078766: Validation loss did not improve from -0.30819. Patience: 75/50
2024-12-21 09:02:56.079870: train_loss -0.8375
2024-12-21 09:02:56.080721: val_loss -0.1901
2024-12-21 09:02:56.081635: Pseudo dice [0.6441]
2024-12-21 09:02:56.082427: Epoch time: 213.57 s
2024-12-21 09:02:56.505266: Yayy! New best EMA pseudo Dice: 0.6393
2024-12-21 09:02:58.263503: 
2024-12-21 09:02:58.264509: Epoch 85
2024-12-21 09:02:58.265304: Current learning rate: 0.00471
2024-12-21 09:06:19.728811: Validation loss did not improve from -0.30819. Patience: 76/50
2024-12-21 09:06:19.729804: train_loss -0.8412
2024-12-21 09:06:19.730813: val_loss -0.1786
2024-12-21 09:06:19.731850: Pseudo dice [0.6297]
2024-12-21 09:06:19.732828: Epoch time: 201.47 s
2024-12-21 09:06:21.101560: 
2024-12-21 09:06:21.102732: Epoch 86
2024-12-21 09:06:21.103639: Current learning rate: 0.00465
2024-12-21 09:09:41.225470: Validation loss did not improve from -0.30819. Patience: 77/50
2024-12-21 09:09:41.226510: train_loss -0.8396
2024-12-21 09:09:41.227394: val_loss -0.1582
2024-12-21 09:09:41.228118: Pseudo dice [0.6299]
2024-12-21 09:09:41.228948: Epoch time: 200.13 s
2024-12-21 09:09:42.595280: 
2024-12-21 09:09:42.596900: Epoch 87
2024-12-21 09:09:42.597903: Current learning rate: 0.00458
2024-12-21 09:13:11.106480: Validation loss did not improve from -0.30819. Patience: 78/50
2024-12-21 09:13:11.107528: train_loss -0.8376
2024-12-21 09:13:11.108500: val_loss -0.2122
2024-12-21 09:13:11.109328: Pseudo dice [0.6479]
2024-12-21 09:13:11.110253: Epoch time: 208.51 s
2024-12-21 09:13:12.506401: 
2024-12-21 09:13:12.507588: Epoch 88
2024-12-21 09:13:12.508283: Current learning rate: 0.00452
2024-12-21 09:16:45.701985: Validation loss did not improve from -0.30819. Patience: 79/50
2024-12-21 09:16:45.703033: train_loss -0.8412
2024-12-21 09:16:45.703975: val_loss -0.1631
2024-12-21 09:16:45.704934: Pseudo dice [0.6369]
2024-12-21 09:16:45.705773: Epoch time: 213.2 s
2024-12-21 09:16:47.055573: 
2024-12-21 09:16:47.056837: Epoch 89
2024-12-21 09:16:47.057627: Current learning rate: 0.00445
2024-12-21 09:20:10.227454: Validation loss did not improve from -0.30819. Patience: 80/50
2024-12-21 09:20:10.228368: train_loss -0.8429
2024-12-21 09:20:10.229367: val_loss -0.1683
2024-12-21 09:20:10.230241: Pseudo dice [0.6456]
2024-12-21 09:20:10.230945: Epoch time: 203.17 s
2024-12-21 09:20:12.160397: 
2024-12-21 09:20:12.161811: Epoch 90
2024-12-21 09:20:12.162734: Current learning rate: 0.00438
2024-12-21 09:23:38.656804: Validation loss did not improve from -0.30819. Patience: 81/50
2024-12-21 09:23:38.657832: train_loss -0.8424
2024-12-21 09:23:38.658786: val_loss -0.1921
2024-12-21 09:23:38.659582: Pseudo dice [0.6464]
2024-12-21 09:23:38.660356: Epoch time: 206.5 s
2024-12-21 09:23:38.661190: Yayy! New best EMA pseudo Dice: 0.6398
2024-12-21 09:23:40.435845: 
2024-12-21 09:23:40.437219: Epoch 91
2024-12-21 09:23:40.438288: Current learning rate: 0.00432
2024-12-21 09:27:04.883744: Validation loss did not improve from -0.30819. Patience: 82/50
2024-12-21 09:27:04.884967: train_loss -0.8436
2024-12-21 09:27:04.885895: val_loss -0.1833
2024-12-21 09:27:04.886760: Pseudo dice [0.6372]
2024-12-21 09:27:04.887454: Epoch time: 204.45 s
2024-12-21 09:27:06.281899: 
2024-12-21 09:27:06.283094: Epoch 92
2024-12-21 09:27:06.283897: Current learning rate: 0.00425
2024-12-21 09:30:35.641805: Validation loss did not improve from -0.30819. Patience: 83/50
2024-12-21 09:30:35.643692: train_loss -0.8411
2024-12-21 09:30:35.645117: val_loss -0.1789
2024-12-21 09:30:35.645957: Pseudo dice [0.6207]
2024-12-21 09:30:35.646942: Epoch time: 209.36 s
2024-12-21 09:30:37.107962: 
2024-12-21 09:30:37.109338: Epoch 93
2024-12-21 09:30:37.110154: Current learning rate: 0.00419
2024-12-21 09:34:42.785645: Validation loss did not improve from -0.30819. Patience: 84/50
2024-12-21 09:34:42.786923: train_loss -0.843
2024-12-21 09:34:42.787886: val_loss -0.1505
2024-12-21 09:34:42.788606: Pseudo dice [0.6302]
2024-12-21 09:34:42.789332: Epoch time: 245.68 s
2024-12-21 09:34:44.616802: 
2024-12-21 09:34:44.618177: Epoch 94
2024-12-21 09:34:44.618981: Current learning rate: 0.00412
2024-12-21 09:38:17.033667: Validation loss did not improve from -0.30819. Patience: 85/50
2024-12-21 09:38:17.036628: train_loss -0.8464
2024-12-21 09:38:17.038698: val_loss -0.2049
2024-12-21 09:38:17.039799: Pseudo dice [0.6437]
2024-12-21 09:38:17.040993: Epoch time: 212.42 s
2024-12-21 09:38:18.918334: 
2024-12-21 09:38:18.919782: Epoch 95
2024-12-21 09:38:18.920591: Current learning rate: 0.00405
2024-12-21 09:42:14.105370: Validation loss did not improve from -0.30819. Patience: 86/50
2024-12-21 09:42:14.107246: train_loss -0.8458
2024-12-21 09:42:14.108291: val_loss -0.1577
2024-12-21 09:42:14.109137: Pseudo dice [0.6287]
2024-12-21 09:42:14.109902: Epoch time: 235.19 s
2024-12-21 09:42:15.494604: 
2024-12-21 09:42:15.495984: Epoch 96
2024-12-21 09:42:15.496850: Current learning rate: 0.00399
2024-12-21 09:45:36.906274: Validation loss did not improve from -0.30819. Patience: 87/50
2024-12-21 09:45:36.907208: train_loss -0.8476
2024-12-21 09:45:36.907937: val_loss -0.1629
2024-12-21 09:45:36.908769: Pseudo dice [0.6229]
2024-12-21 09:45:36.909580: Epoch time: 201.41 s
2024-12-21 09:45:38.356644: 
2024-12-21 09:45:38.357848: Epoch 97
2024-12-21 09:45:38.358867: Current learning rate: 0.00392
2024-12-21 09:48:58.182975: Validation loss did not improve from -0.30819. Patience: 88/50
2024-12-21 09:48:58.183906: train_loss -0.8483
2024-12-21 09:48:58.185029: val_loss -0.1781
2024-12-21 09:48:58.185805: Pseudo dice [0.6321]
2024-12-21 09:48:58.186651: Epoch time: 199.83 s
2024-12-21 09:48:59.619521: 
2024-12-21 09:48:59.620700: Epoch 98
2024-12-21 09:48:59.621556: Current learning rate: 0.00385
2024-12-21 09:52:25.519531: Validation loss did not improve from -0.30819. Patience: 89/50
2024-12-21 09:52:25.520585: train_loss -0.8499
2024-12-21 09:52:25.521443: val_loss -0.1935
2024-12-21 09:52:25.522162: Pseudo dice [0.6535]
2024-12-21 09:52:25.522867: Epoch time: 205.9 s
2024-12-21 09:52:26.933996: 
2024-12-21 09:52:26.935415: Epoch 99
2024-12-21 09:52:26.936343: Current learning rate: 0.00379
2024-12-21 09:55:49.680039: Validation loss did not improve from -0.30819. Patience: 90/50
2024-12-21 09:55:49.680983: train_loss -0.8507
2024-12-21 09:55:49.681837: val_loss -0.161
2024-12-21 09:55:49.682567: Pseudo dice [0.6334]
2024-12-21 09:55:49.683352: Epoch time: 202.75 s
2024-12-21 09:55:51.535882: 
2024-12-21 09:55:51.537207: Epoch 100
2024-12-21 09:55:51.538108: Current learning rate: 0.00372
2024-12-21 09:59:31.293771: Validation loss did not improve from -0.30819. Patience: 91/50
2024-12-21 09:59:31.294936: train_loss -0.8515
2024-12-21 09:59:31.295858: val_loss -0.2084
2024-12-21 09:59:31.296507: Pseudo dice [0.643]
2024-12-21 09:59:31.297277: Epoch time: 219.76 s
2024-12-21 09:59:32.701596: 
2024-12-21 09:59:32.702984: Epoch 101
2024-12-21 09:59:32.704253: Current learning rate: 0.00365
2024-12-21 10:02:56.073890: Validation loss did not improve from -0.30819. Patience: 92/50
2024-12-21 10:02:56.074946: train_loss -0.852
2024-12-21 10:02:56.075848: val_loss -0.1615
2024-12-21 10:02:56.076699: Pseudo dice [0.6365]
2024-12-21 10:02:56.077589: Epoch time: 203.37 s
2024-12-21 10:02:57.440227: 
2024-12-21 10:02:57.441421: Epoch 102
2024-12-21 10:02:57.442399: Current learning rate: 0.00359
2024-12-21 10:06:23.247768: Validation loss did not improve from -0.30819. Patience: 93/50
2024-12-21 10:06:23.248912: train_loss -0.8512
2024-12-21 10:06:23.249955: val_loss -0.1658
2024-12-21 10:06:23.250771: Pseudo dice [0.6254]
2024-12-21 10:06:23.251622: Epoch time: 205.81 s
2024-12-21 10:06:24.630737: 
2024-12-21 10:06:24.632157: Epoch 103
2024-12-21 10:06:24.632977: Current learning rate: 0.00352
2024-12-21 10:09:48.664544: Validation loss did not improve from -0.30819. Patience: 94/50
2024-12-21 10:09:48.665757: train_loss -0.8542
2024-12-21 10:09:48.666641: val_loss -0.1234
2024-12-21 10:09:48.667734: Pseudo dice [0.621]
2024-12-21 10:09:48.668582: Epoch time: 204.04 s
2024-12-21 10:09:50.100730: 
2024-12-21 10:09:50.102040: Epoch 104
2024-12-21 10:09:50.102919: Current learning rate: 0.00345
2024-12-21 10:13:17.104704: Validation loss did not improve from -0.30819. Patience: 95/50
2024-12-21 10:13:17.105586: train_loss -0.8532
2024-12-21 10:13:17.106479: val_loss -0.1653
2024-12-21 10:13:17.107243: Pseudo dice [0.6486]
2024-12-21 10:13:17.108092: Epoch time: 207.01 s
2024-12-21 10:13:19.331402: 
2024-12-21 10:13:19.332878: Epoch 105
2024-12-21 10:13:19.333933: Current learning rate: 0.00338
2024-12-21 10:16:45.854214: Validation loss did not improve from -0.30819. Patience: 96/50
2024-12-21 10:16:45.855309: train_loss -0.8547
2024-12-21 10:16:45.856392: val_loss -0.174
2024-12-21 10:16:45.857324: Pseudo dice [0.6397]
2024-12-21 10:16:45.858133: Epoch time: 206.53 s
2024-12-21 10:16:47.291324: 
2024-12-21 10:16:47.292691: Epoch 106
2024-12-21 10:16:47.293671: Current learning rate: 0.00332
2024-12-21 10:20:13.505965: Validation loss did not improve from -0.30819. Patience: 97/50
2024-12-21 10:20:13.506880: train_loss -0.8547
2024-12-21 10:20:13.507720: val_loss -0.1543
2024-12-21 10:20:13.508562: Pseudo dice [0.6363]
2024-12-21 10:20:13.509375: Epoch time: 206.22 s
2024-12-21 10:20:14.890136: 
2024-12-21 10:20:14.891324: Epoch 107
2024-12-21 10:20:14.892191: Current learning rate: 0.00325
2024-12-21 10:23:32.044149: Validation loss did not improve from -0.30819. Patience: 98/50
2024-12-21 10:23:32.045010: train_loss -0.8571
2024-12-21 10:23:32.046064: val_loss -0.1377
2024-12-21 10:23:32.047288: Pseudo dice [0.6401]
2024-12-21 10:23:32.048122: Epoch time: 197.16 s
2024-12-21 10:23:33.468392: 
2024-12-21 10:23:33.469970: Epoch 108
2024-12-21 10:23:33.471226: Current learning rate: 0.00318
2024-12-21 10:27:05.338104: Validation loss did not improve from -0.30819. Patience: 99/50
2024-12-21 10:27:05.339118: train_loss -0.8568
2024-12-21 10:27:05.340066: val_loss -0.1722
2024-12-21 10:27:05.341006: Pseudo dice [0.6432]
2024-12-21 10:27:05.341900: Epoch time: 211.87 s
2024-12-21 10:27:06.751344: 
2024-12-21 10:27:06.752839: Epoch 109
2024-12-21 10:27:06.753807: Current learning rate: 0.00311
2024-12-21 10:30:32.306207: Validation loss did not improve from -0.30819. Patience: 100/50
2024-12-21 10:30:32.307220: train_loss -0.8581
2024-12-21 10:30:32.308125: val_loss -0.1599
2024-12-21 10:30:32.308899: Pseudo dice [0.6345]
2024-12-21 10:30:32.309704: Epoch time: 205.56 s
2024-12-21 10:30:34.118664: 
2024-12-21 10:30:34.119680: Epoch 110
2024-12-21 10:30:34.120493: Current learning rate: 0.00304
2024-12-21 10:34:01.712209: Validation loss did not improve from -0.30819. Patience: 101/50
2024-12-21 10:34:01.713326: train_loss -0.8574
2024-12-21 10:34:01.714099: val_loss -0.1877
2024-12-21 10:34:01.714771: Pseudo dice [0.6501]
2024-12-21 10:34:01.715589: Epoch time: 207.6 s
2024-12-21 10:34:03.126101: 
2024-12-21 10:34:03.127604: Epoch 111
2024-12-21 10:34:03.128441: Current learning rate: 0.00297
2024-12-21 10:37:24.923955: Validation loss did not improve from -0.30819. Patience: 102/50
2024-12-21 10:37:24.925399: train_loss -0.8597
2024-12-21 10:37:24.926528: val_loss -0.1214
2024-12-21 10:37:24.927594: Pseudo dice [0.6304]
2024-12-21 10:37:24.928533: Epoch time: 201.8 s
2024-12-21 10:37:26.346389: 
2024-12-21 10:37:26.347870: Epoch 112
2024-12-21 10:37:26.348908: Current learning rate: 0.00291
2024-12-21 10:40:54.311669: Validation loss did not improve from -0.30819. Patience: 103/50
2024-12-21 10:40:54.312622: train_loss -0.8582
2024-12-21 10:40:54.313430: val_loss -0.1507
2024-12-21 10:40:54.314142: Pseudo dice [0.6425]
2024-12-21 10:40:54.314973: Epoch time: 207.97 s
2024-12-21 10:40:55.768653: 
2024-12-21 10:40:55.770045: Epoch 113
2024-12-21 10:40:55.770787: Current learning rate: 0.00284
2024-12-21 10:44:19.281197: Validation loss did not improve from -0.30819. Patience: 104/50
2024-12-21 10:44:19.282197: train_loss -0.8591
2024-12-21 10:44:19.283097: val_loss -0.1133
2024-12-21 10:44:19.283981: Pseudo dice [0.631]
2024-12-21 10:44:19.284887: Epoch time: 203.51 s
2024-12-21 10:44:20.765786: 
2024-12-21 10:44:20.767269: Epoch 114
2024-12-21 10:44:20.768278: Current learning rate: 0.00277
2024-12-21 10:47:56.493459: Validation loss did not improve from -0.30819. Patience: 105/50
2024-12-21 10:47:56.494555: train_loss -0.8601
2024-12-21 10:47:56.495562: val_loss -0.1671
2024-12-21 10:47:56.496499: Pseudo dice [0.6439]
2024-12-21 10:47:56.497451: Epoch time: 215.73 s
2024-12-21 10:47:58.334635: 
2024-12-21 10:47:58.336061: Epoch 115
2024-12-21 10:47:58.337214: Current learning rate: 0.0027
2024-12-21 10:51:25.210312: Validation loss did not improve from -0.30819. Patience: 106/50
2024-12-21 10:51:25.211204: train_loss -0.8615
2024-12-21 10:51:25.212052: val_loss -0.1278
2024-12-21 10:51:25.212841: Pseudo dice [0.636]
2024-12-21 10:51:25.213715: Epoch time: 206.88 s
2024-12-21 10:51:27.105642: 
2024-12-21 10:51:27.107085: Epoch 116
2024-12-21 10:51:27.108153: Current learning rate: 0.00263
2024-12-21 10:54:47.844088: Validation loss did not improve from -0.30819. Patience: 107/50
2024-12-21 10:54:47.844818: train_loss -0.8609
2024-12-21 10:54:47.845594: val_loss -0.1381
2024-12-21 10:54:47.846268: Pseudo dice [0.6327]
2024-12-21 10:54:47.846955: Epoch time: 200.74 s
2024-12-21 10:54:49.292281: 
2024-12-21 10:54:49.293649: Epoch 117
2024-12-21 10:54:49.294491: Current learning rate: 0.00256
2024-12-21 10:58:13.808140: Validation loss did not improve from -0.30819. Patience: 108/50
2024-12-21 10:58:13.809241: train_loss -0.8623
2024-12-21 10:58:13.810146: val_loss -0.1434
2024-12-21 10:58:13.811000: Pseudo dice [0.6327]
2024-12-21 10:58:13.811706: Epoch time: 204.52 s
2024-12-21 10:58:15.298129: 
2024-12-21 10:58:15.299322: Epoch 118
2024-12-21 10:58:15.300094: Current learning rate: 0.00249
2024-12-21 11:01:45.594280: Validation loss did not improve from -0.30819. Patience: 109/50
2024-12-21 11:01:45.595290: train_loss -0.8633
2024-12-21 11:01:45.596322: val_loss -0.1167
2024-12-21 11:01:45.597380: Pseudo dice [0.6261]
2024-12-21 11:01:45.598555: Epoch time: 210.3 s
2024-12-21 11:01:47.052032: 
2024-12-21 11:01:47.053383: Epoch 119
2024-12-21 11:01:47.054317: Current learning rate: 0.00242
2024-12-21 11:05:02.699099: Validation loss did not improve from -0.30819. Patience: 110/50
2024-12-21 11:05:02.700023: train_loss -0.8632
2024-12-21 11:05:02.700912: val_loss -0.1528
2024-12-21 11:05:02.701620: Pseudo dice [0.6352]
2024-12-21 11:05:02.702290: Epoch time: 195.65 s
2024-12-21 11:05:04.677513: 
2024-12-21 11:05:04.678970: Epoch 120
2024-12-21 11:05:04.679848: Current learning rate: 0.00235
2024-12-21 11:08:30.914605: Validation loss did not improve from -0.30819. Patience: 111/50
2024-12-21 11:08:30.915516: train_loss -0.8651
2024-12-21 11:08:30.916333: val_loss -0.1418
2024-12-21 11:08:30.917047: Pseudo dice [0.6306]
2024-12-21 11:08:30.917934: Epoch time: 206.24 s
2024-12-21 11:08:32.359285: 
2024-12-21 11:08:32.360687: Epoch 121
2024-12-21 11:08:32.361454: Current learning rate: 0.00228
2024-12-21 11:12:02.119792: Validation loss did not improve from -0.30819. Patience: 112/50
2024-12-21 11:12:02.120887: train_loss -0.8638
2024-12-21 11:12:02.121769: val_loss -0.1152
2024-12-21 11:12:02.122554: Pseudo dice [0.633]
2024-12-21 11:12:02.123378: Epoch time: 209.76 s
2024-12-21 11:12:03.536544: 
2024-12-21 11:12:03.537865: Epoch 122
2024-12-21 11:12:03.538720: Current learning rate: 0.00221
2024-12-21 11:15:37.413994: Validation loss did not improve from -0.30819. Patience: 113/50
2024-12-21 11:15:37.415050: train_loss -0.8666
2024-12-21 11:15:37.415818: val_loss -0.1951
2024-12-21 11:15:37.416512: Pseudo dice [0.6381]
2024-12-21 11:15:37.417286: Epoch time: 213.88 s
2024-12-21 11:15:38.936203: 
2024-12-21 11:15:38.937544: Epoch 123
2024-12-21 11:15:38.938404: Current learning rate: 0.00214
2024-12-21 11:19:11.698199: Validation loss did not improve from -0.30819. Patience: 114/50
2024-12-21 11:19:11.699171: train_loss -0.8665
2024-12-21 11:19:11.700134: val_loss -0.1999
2024-12-21 11:19:11.701153: Pseudo dice [0.6531]
2024-12-21 11:19:11.702028: Epoch time: 212.76 s
2024-12-21 11:19:13.173636: 
2024-12-21 11:19:13.174683: Epoch 124
2024-12-21 11:19:13.175653: Current learning rate: 0.00207
2024-12-21 11:22:39.243467: Validation loss did not improve from -0.30819. Patience: 115/50
2024-12-21 11:22:39.244439: train_loss -0.8668
2024-12-21 11:22:39.245296: val_loss -0.1533
2024-12-21 11:22:39.246098: Pseudo dice [0.6485]
2024-12-21 11:22:39.246797: Epoch time: 206.07 s
2024-12-21 11:22:41.120055: 
2024-12-21 11:22:41.121527: Epoch 125
2024-12-21 11:22:41.122360: Current learning rate: 0.00199
2024-12-21 11:25:56.841803: Validation loss did not improve from -0.30819. Patience: 116/50
2024-12-21 11:25:56.842488: train_loss -0.8674
2024-12-21 11:25:56.843349: val_loss -0.153
2024-12-21 11:25:56.844202: Pseudo dice [0.647]
2024-12-21 11:25:56.845059: Epoch time: 195.72 s
2024-12-21 11:25:58.728423: 
2024-12-21 11:25:58.729412: Epoch 126
2024-12-21 11:25:58.730268: Current learning rate: 0.00192
2024-12-21 11:29:32.886837: Validation loss did not improve from -0.30819. Patience: 117/50
2024-12-21 11:29:32.887521: train_loss -0.8669
2024-12-21 11:29:32.888344: val_loss -0.1688
2024-12-21 11:29:32.889030: Pseudo dice [0.6457]
2024-12-21 11:29:32.889857: Epoch time: 214.16 s
2024-12-21 11:29:34.379111: 
2024-12-21 11:29:34.380171: Epoch 127
2024-12-21 11:29:34.380956: Current learning rate: 0.00185
2024-12-21 11:33:09.416806: Validation loss did not improve from -0.30819. Patience: 118/50
2024-12-21 11:33:09.417900: train_loss -0.8684
2024-12-21 11:33:09.418981: val_loss -0.1297
2024-12-21 11:33:09.419990: Pseudo dice [0.6433]
2024-12-21 11:33:09.421101: Epoch time: 215.04 s
2024-12-21 11:33:09.422069: Yayy! New best EMA pseudo Dice: 0.6401
2024-12-21 11:33:11.293979: 
2024-12-21 11:33:11.295205: Epoch 128
2024-12-21 11:33:11.296270: Current learning rate: 0.00178
2024-12-21 11:36:48.769124: Validation loss did not improve from -0.30819. Patience: 119/50
2024-12-21 11:36:48.769930: train_loss -0.8665
2024-12-21 11:36:48.770815: val_loss -0.1145
2024-12-21 11:36:48.771623: Pseudo dice [0.6324]
2024-12-21 11:36:48.772377: Epoch time: 217.48 s
2024-12-21 11:36:50.211217: 
2024-12-21 11:36:50.212314: Epoch 129
2024-12-21 11:36:50.213039: Current learning rate: 0.0017
2024-12-21 11:40:14.872153: Validation loss did not improve from -0.30819. Patience: 120/50
2024-12-21 11:40:14.873916: train_loss -0.8709
2024-12-21 11:40:14.875463: val_loss -0.1266
2024-12-21 11:40:14.876596: Pseudo dice [0.6344]
2024-12-21 11:40:14.878067: Epoch time: 204.66 s
2024-12-21 11:40:16.741775: 
2024-12-21 11:40:16.742790: Epoch 130
2024-12-21 11:40:16.743747: Current learning rate: 0.00163
2024-12-21 11:43:31.797938: Validation loss did not improve from -0.30819. Patience: 121/50
2024-12-21 11:43:31.798654: train_loss -0.8698
2024-12-21 11:43:31.799519: val_loss -0.1448
2024-12-21 11:43:31.800279: Pseudo dice [0.641]
2024-12-21 11:43:31.801021: Epoch time: 195.06 s
2024-12-21 11:43:33.199609: 
2024-12-21 11:43:33.200928: Epoch 131
2024-12-21 11:43:33.201794: Current learning rate: 0.00156
2024-12-21 11:46:57.835810: Validation loss did not improve from -0.30819. Patience: 122/50
2024-12-21 11:46:57.836885: train_loss -0.8706
2024-12-21 11:46:57.837729: val_loss -0.1297
2024-12-21 11:46:57.838473: Pseudo dice [0.6394]
2024-12-21 11:46:57.839325: Epoch time: 204.64 s
2024-12-21 11:46:59.227803: 
2024-12-21 11:46:59.229328: Epoch 132
2024-12-21 11:46:59.230231: Current learning rate: 0.00148
2024-12-21 11:50:20.551078: Validation loss did not improve from -0.30819. Patience: 123/50
2024-12-21 11:50:20.551768: train_loss -0.8698
2024-12-21 11:50:20.552699: val_loss -0.1381
2024-12-21 11:50:20.553633: Pseudo dice [0.642]
2024-12-21 11:50:20.554407: Epoch time: 201.33 s
2024-12-21 11:50:21.997185: 
2024-12-21 11:50:21.998636: Epoch 133
2024-12-21 11:50:21.999603: Current learning rate: 0.00141
2024-12-21 11:53:41.163647: Validation loss did not improve from -0.30819. Patience: 124/50
2024-12-21 11:53:41.164602: train_loss -0.8711
2024-12-21 11:53:41.165497: val_loss -0.1436
2024-12-21 11:53:41.166398: Pseudo dice [0.6335]
2024-12-21 11:53:41.167332: Epoch time: 199.17 s
2024-12-21 11:53:42.662530: 
2024-12-21 11:53:42.663714: Epoch 134
2024-12-21 11:53:42.664570: Current learning rate: 0.00133
2024-12-21 11:57:08.408350: Validation loss did not improve from -0.30819. Patience: 125/50
2024-12-21 11:57:08.409263: train_loss -0.87
2024-12-21 11:57:08.410159: val_loss -0.1173
2024-12-21 11:57:08.410809: Pseudo dice [0.6303]
2024-12-21 11:57:08.411582: Epoch time: 205.75 s
2024-12-21 11:57:10.261739: 
2024-12-21 11:57:10.263036: Epoch 135
2024-12-21 11:57:10.263796: Current learning rate: 0.00126
2024-12-21 12:00:42.580478: Validation loss did not improve from -0.30819. Patience: 126/50
2024-12-21 12:00:42.581405: train_loss -0.871
2024-12-21 12:00:42.582315: val_loss -0.1549
2024-12-21 12:00:42.583054: Pseudo dice [0.6501]
2024-12-21 12:00:42.583847: Epoch time: 212.32 s
2024-12-21 12:00:44.027881: 
2024-12-21 12:00:44.028774: Epoch 136
2024-12-21 12:00:44.029546: Current learning rate: 0.00118
2024-12-21 12:04:21.926706: Validation loss did not improve from -0.30819. Patience: 127/50
2024-12-21 12:04:21.927397: train_loss -0.8723
2024-12-21 12:04:21.928204: val_loss -0.1326
2024-12-21 12:04:21.928984: Pseudo dice [0.6398]
2024-12-21 12:04:21.929782: Epoch time: 217.9 s
2024-12-21 12:04:23.785868: 
2024-12-21 12:04:23.787151: Epoch 137
2024-12-21 12:04:23.788013: Current learning rate: 0.00111
2024-12-21 12:07:48.369160: Validation loss did not improve from -0.30819. Patience: 128/50
2024-12-21 12:07:48.369919: train_loss -0.8728
2024-12-21 12:07:48.370859: val_loss -0.1473
2024-12-21 12:07:48.371777: Pseudo dice [0.636]
2024-12-21 12:07:48.372785: Epoch time: 204.59 s
2024-12-21 12:07:49.835349: 
2024-12-21 12:07:49.836712: Epoch 138
2024-12-21 12:07:49.837797: Current learning rate: 0.00103
2024-12-21 12:11:17.565214: Validation loss did not improve from -0.30819. Patience: 129/50
2024-12-21 12:11:17.566244: train_loss -0.8732
2024-12-21 12:11:17.567125: val_loss -0.1697
2024-12-21 12:11:17.567929: Pseudo dice [0.636]
2024-12-21 12:11:17.568795: Epoch time: 207.73 s
2024-12-21 12:11:19.033731: 
2024-12-21 12:11:19.035025: Epoch 139
2024-12-21 12:11:19.035894: Current learning rate: 0.00095
2024-12-21 12:14:41.633625: Validation loss did not improve from -0.30819. Patience: 130/50
2024-12-21 12:14:41.634615: train_loss -0.8736
2024-12-21 12:14:41.635365: val_loss -0.143
2024-12-21 12:14:41.636178: Pseudo dice [0.6415]
2024-12-21 12:14:41.637007: Epoch time: 202.6 s
2024-12-21 12:14:43.522988: 
2024-12-21 12:14:43.524137: Epoch 140
2024-12-21 12:14:43.524905: Current learning rate: 0.00087
2024-12-21 12:18:11.914222: Validation loss did not improve from -0.30819. Patience: 131/50
2024-12-21 12:18:11.915279: train_loss -0.8722
2024-12-21 12:18:11.916454: val_loss -0.1188
2024-12-21 12:18:11.917501: Pseudo dice [0.6398]
2024-12-21 12:18:11.918570: Epoch time: 208.39 s
2024-12-21 12:18:13.369515: 
2024-12-21 12:18:13.370819: Epoch 141
2024-12-21 12:18:13.371828: Current learning rate: 0.00079
2024-12-21 12:21:41.831800: Validation loss did not improve from -0.30819. Patience: 132/50
2024-12-21 12:21:41.832749: train_loss -0.8748
2024-12-21 12:21:41.833583: val_loss -0.1613
2024-12-21 12:21:41.834437: Pseudo dice [0.6397]
2024-12-21 12:21:41.835277: Epoch time: 208.46 s
2024-12-21 12:21:43.279480: 
2024-12-21 12:21:43.280738: Epoch 142
2024-12-21 12:21:43.281568: Current learning rate: 0.00071
2024-12-21 12:25:22.209625: Validation loss did not improve from -0.30819. Patience: 133/50
2024-12-21 12:25:22.210553: train_loss -0.8735
2024-12-21 12:25:22.211468: val_loss -0.1292
2024-12-21 12:25:22.212414: Pseudo dice [0.6369]
2024-12-21 12:25:22.213177: Epoch time: 218.93 s
2024-12-21 12:25:23.661461: 
2024-12-21 12:25:23.662640: Epoch 143
2024-12-21 12:25:23.663491: Current learning rate: 0.00063
2024-12-21 12:28:45.513568: Validation loss did not improve from -0.30819. Patience: 134/50
2024-12-21 12:28:45.514567: train_loss -0.8729
2024-12-21 12:28:45.515592: val_loss -0.1427
2024-12-21 12:28:45.516417: Pseudo dice [0.6303]
2024-12-21 12:28:45.517229: Epoch time: 201.85 s
2024-12-21 12:28:47.018317: 
2024-12-21 12:28:47.019669: Epoch 144
2024-12-21 12:28:47.020635: Current learning rate: 0.00055
2024-12-21 12:32:21.543065: Validation loss did not improve from -0.30819. Patience: 135/50
2024-12-21 12:32:21.543890: train_loss -0.8756
2024-12-21 12:32:21.544816: val_loss -0.1207
2024-12-21 12:32:21.545583: Pseudo dice [0.6236]
2024-12-21 12:32:21.546322: Epoch time: 214.53 s
2024-12-21 12:32:23.398482: 
2024-12-21 12:32:23.400716: Epoch 145
2024-12-21 12:32:23.401591: Current learning rate: 0.00047
2024-12-21 12:35:50.428124: Validation loss did not improve from -0.30819. Patience: 136/50
2024-12-21 12:35:50.429136: train_loss -0.8764
2024-12-21 12:35:50.429884: val_loss -0.1458
2024-12-21 12:35:50.430682: Pseudo dice [0.6389]
2024-12-21 12:35:50.431538: Epoch time: 207.03 s
2024-12-21 12:35:51.929448: 
2024-12-21 12:35:51.930311: Epoch 146
2024-12-21 12:35:51.931118: Current learning rate: 0.00038
2024-12-21 12:39:15.527761: Validation loss did not improve from -0.30819. Patience: 137/50
2024-12-21 12:39:15.528589: train_loss -0.8769
2024-12-21 12:39:15.529413: val_loss -0.1582
2024-12-21 12:39:15.530112: Pseudo dice [0.6511]
2024-12-21 12:39:15.530839: Epoch time: 203.6 s
2024-12-21 12:39:16.987812: 
2024-12-21 12:39:16.989169: Epoch 147
2024-12-21 12:39:16.989890: Current learning rate: 0.0003
2024-12-21 12:42:44.639523: Validation loss did not improve from -0.30819. Patience: 138/50
2024-12-21 12:42:44.640332: train_loss -0.8747
2024-12-21 12:42:44.641158: val_loss -0.0937
2024-12-21 12:42:44.641894: Pseudo dice [0.6249]
2024-12-21 12:42:44.642546: Epoch time: 207.65 s
2024-12-21 12:42:46.540208: 
2024-12-21 12:42:46.541363: Epoch 148
2024-12-21 12:42:46.542112: Current learning rate: 0.00021
2024-12-21 12:46:08.034086: Validation loss did not improve from -0.30819. Patience: 139/50
2024-12-21 12:46:08.034997: train_loss -0.8762
2024-12-21 12:46:08.035770: val_loss -0.1188
2024-12-21 12:46:08.036506: Pseudo dice [0.6292]
2024-12-21 12:46:08.037219: Epoch time: 201.5 s
2024-12-21 12:46:09.490019: 
2024-12-21 12:46:09.491233: Epoch 149
2024-12-21 12:46:09.492009: Current learning rate: 0.00011
2024-12-21 12:49:37.614020: Validation loss did not improve from -0.30819. Patience: 140/50
2024-12-21 12:49:37.614708: train_loss -0.877
2024-12-21 12:49:37.615767: val_loss -0.1305
2024-12-21 12:49:37.616629: Pseudo dice [0.6473]
2024-12-21 12:49:37.617503: Epoch time: 208.13 s
2024-12-21 12:49:39.475653: Training done.
2024-12-21 12:49:15.069662: Using splits from existing split file: /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset308_Sohee_Calcium_OCT_CrossValidation/splits_final_20.json
2024-12-21 12:49:15.086952: The split file contains 5 splits.
2024-12-21 12:49:15.087939: Desired fold for training: 3
2024-12-21 12:49:15.088619: This split has 1 training and 7 validation cases.
2024-12-21 12:49:15.089656: predicting 101-044
2024-12-21 12:49:15.247119: 101-044, shape torch.Size([1, 404, 498, 498]), rank 0
2024-12-21 12:51:31.419424: predicting 101-045
2024-12-21 12:51:31.443988: 101-045, shape torch.Size([1, 375, 498, 498]), rank 0
2024-12-21 12:53:34.658406: predicting 106-002
2024-12-21 12:53:34.677414: 106-002, shape torch.Size([1, 539, 498, 498]), rank 0
2024-12-21 12:56:41.574687: predicting 401-004
2024-12-21 12:56:41.592482: 401-004, shape torch.Size([1, 375, 498, 498]), rank 0
2024-12-21 12:58:42.937261: predicting 701-013
2024-12-21 12:58:42.964634: 701-013, shape torch.Size([1, 375, 498, 498]), rank 0
2024-12-21 13:00:56.461704: predicting 704-003
2024-12-21 13:00:56.476984: 704-003, shape torch.Size([1, 375, 498, 498]), rank 0
2024-12-21 13:02:53.588835: predicting 706-005
2024-12-21 13:02:53.608260: 706-005, shape torch.Size([1, 375, 498, 498]), rank 0
2024-12-21 12:49:39.600934: Using splits from existing split file: /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset308_Sohee_Calcium_OCT_CrossValidation/splits_final_20.json
2024-12-21 12:49:39.603004: The split file contains 5 splits.
2024-12-21 12:49:39.604223: Desired fold for training: 2
2024-12-21 12:49:39.605256: This split has 1 training and 7 validation cases.
2024-12-21 12:49:39.606583: predicting 101-019
2024-12-21 12:49:39.643721: 101-019, shape torch.Size([1, 375, 498, 498]), rank 0
2024-12-21 12:52:01.859232: predicting 101-044
2024-12-21 12:52:01.882380: 101-044, shape torch.Size([1, 404, 498, 498]), rank 0
2024-12-21 12:54:14.907491: predicting 101-045
2024-12-21 12:54:14.927112: 101-045, shape torch.Size([1, 375, 498, 498]), rank 0
2024-12-21 12:56:14.252903: predicting 106-002
2024-12-21 12:56:14.277907: 106-002, shape torch.Size([1, 539, 498, 498]), rank 0
2024-12-21 12:59:17.951003: predicting 401-004
2024-12-21 12:59:17.968415: 401-004, shape torch.Size([1, 375, 498, 498]), rank 0
2024-12-21 13:01:14.207184: predicting 704-003
2024-12-21 13:01:14.256155: 704-003, shape torch.Size([1, 375, 498, 498]), rank 0
2024-12-21 13:02:53.562535: predicting 706-005
2024-12-21 13:02:53.579708: 706-005, shape torch.Size([1, 375, 498, 498]), rank 0
2024-12-21 13:05:17.425982: Validation complete
2024-12-21 13:05:17.426870: Mean Validation Dice:  0.5790314578966697
2024-12-21 13:05:17.501179: Validation complete
2024-12-21 13:05:17.502721: Mean Validation Dice:  0.5849687096543313

############################
INFO: You are using the old nnU-Net default plans. We have updated our recommendations. Please consider using those instead! Read more here: https://github.com/MIC-DKFZ/nnUNet/blob/master/documentation/resenc_presets.md
############################

Using device: cuda:0

#######################################################################
Please cite the following paper when using nnU-Net:
Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.
#######################################################################

2024-12-21 13:05:39.172627: Using torch.compile...
/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
################### Loading pretrained weights from file  /home/gridsan/nchutisilp/projects/OpenAI-CLIP/clip_pretrained_nnUNet.pt ###################
Below is the list of overlapping blocks in pretrained model and nnUNet architecture:
encoder.stages.0.0.convs.0.conv.weight shape torch.Size([32, 1, 3, 3, 3])
encoder.stages.0.0.convs.0.conv.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.norm.weight shape torch.Size([32])
encoder.stages.0.0.convs.0.norm.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.0.weight shape torch.Size([32, 1, 3, 3, 3])
encoder.stages.0.0.convs.0.all_modules.0.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.1.weight shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.1.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
encoder.stages.0.0.convs.1.conv.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.norm.weight shape torch.Size([32])
encoder.stages.0.0.convs.1.norm.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
encoder.stages.0.0.convs.1.all_modules.0.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.1.weight shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.1.bias shape torch.Size([32])
encoder.stages.1.0.convs.0.conv.weight shape torch.Size([64, 32, 3, 3, 3])
encoder.stages.1.0.convs.0.conv.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.norm.weight shape torch.Size([64])
encoder.stages.1.0.convs.0.norm.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.0.weight shape torch.Size([64, 32, 3, 3, 3])
encoder.stages.1.0.convs.0.all_modules.0.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.1.weight shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.1.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
encoder.stages.1.0.convs.1.conv.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.norm.weight shape torch.Size([64])
encoder.stages.1.0.convs.1.norm.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
encoder.stages.1.0.convs.1.all_modules.0.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.1.weight shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.1.bias shape torch.Size([64])
encoder.stages.2.0.convs.0.conv.weight shape torch.Size([128, 64, 3, 3, 3])
encoder.stages.2.0.convs.0.conv.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.norm.weight shape torch.Size([128])
encoder.stages.2.0.convs.0.norm.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.0.weight shape torch.Size([128, 64, 3, 3, 3])
encoder.stages.2.0.convs.0.all_modules.0.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.1.weight shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.1.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
encoder.stages.2.0.convs.1.conv.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.norm.weight shape torch.Size([128])
encoder.stages.2.0.convs.1.norm.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
encoder.stages.2.0.convs.1.all_modules.0.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.1.weight shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.1.bias shape torch.Size([128])
encoder.stages.3.0.convs.0.conv.weight shape torch.Size([256, 128, 3, 3, 3])
encoder.stages.3.0.convs.0.conv.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.norm.weight shape torch.Size([256])
encoder.stages.3.0.convs.0.norm.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.0.weight shape torch.Size([256, 128, 3, 3, 3])
encoder.stages.3.0.convs.0.all_modules.0.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.1.weight shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.1.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
encoder.stages.3.0.convs.1.conv.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.norm.weight shape torch.Size([256])
encoder.stages.3.0.convs.1.norm.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
encoder.stages.3.0.convs.1.all_modules.0.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.1.weight shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.1.bias shape torch.Size([256])
encoder.stages.4.0.convs.0.conv.weight shape torch.Size([320, 256, 3, 3, 3])
encoder.stages.4.0.convs.0.conv.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.norm.weight shape torch.Size([320])
encoder.stages.4.0.convs.0.norm.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.0.weight shape torch.Size([320, 256, 3, 3, 3])
encoder.stages.4.0.convs.0.all_modules.0.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.1.weight shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.1.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.4.0.convs.1.conv.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.norm.weight shape torch.Size([320])
encoder.stages.4.0.convs.1.norm.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.4.0.convs.1.all_modules.0.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.1.weight shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.1.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.0.conv.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.norm.weight shape torch.Size([320])
encoder.stages.5.0.convs.0.norm.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.0.all_modules.0.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.1.weight shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.1.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.1.conv.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.norm.weight shape torch.Size([320])
encoder.stages.5.0.convs.1.norm.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.1.all_modules.0.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.1.weight shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.0.0.convs.0.conv.weight shape torch.Size([32, 1, 3, 3, 3])
decoder.encoder.stages.0.0.convs.0.conv.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.norm.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.norm.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.0.weight shape torch.Size([32, 1, 3, 3, 3])
decoder.encoder.stages.0.0.convs.0.all_modules.0.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.1.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.1.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.encoder.stages.0.0.convs.1.conv.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.norm.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.norm.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.encoder.stages.0.0.convs.1.all_modules.0.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.1.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.1.bias shape torch.Size([32])
decoder.encoder.stages.1.0.convs.0.conv.weight shape torch.Size([64, 32, 3, 3, 3])
decoder.encoder.stages.1.0.convs.0.conv.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.norm.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.norm.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.0.weight shape torch.Size([64, 32, 3, 3, 3])
decoder.encoder.stages.1.0.convs.0.all_modules.0.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.1.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.1.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.encoder.stages.1.0.convs.1.conv.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.norm.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.norm.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.encoder.stages.1.0.convs.1.all_modules.0.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.1.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.1.bias shape torch.Size([64])
decoder.encoder.stages.2.0.convs.0.conv.weight shape torch.Size([128, 64, 3, 3, 3])
decoder.encoder.stages.2.0.convs.0.conv.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.norm.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.norm.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.0.weight shape torch.Size([128, 64, 3, 3, 3])
decoder.encoder.stages.2.0.convs.0.all_modules.0.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.1.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.1.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.encoder.stages.2.0.convs.1.conv.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.norm.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.norm.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.encoder.stages.2.0.convs.1.all_modules.0.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.1.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.1.bias shape torch.Size([128])
decoder.encoder.stages.3.0.convs.0.conv.weight shape torch.Size([256, 128, 3, 3, 3])
decoder.encoder.stages.3.0.convs.0.conv.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.norm.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.norm.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.0.weight shape torch.Size([256, 128, 3, 3, 3])
decoder.encoder.stages.3.0.convs.0.all_modules.0.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.1.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.1.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.encoder.stages.3.0.convs.1.conv.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.norm.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.norm.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.encoder.stages.3.0.convs.1.all_modules.0.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.1.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.1.bias shape torch.Size([256])
decoder.encoder.stages.4.0.convs.0.conv.weight shape torch.Size([320, 256, 3, 3, 3])
decoder.encoder.stages.4.0.convs.0.conv.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.norm.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.norm.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.0.weight shape torch.Size([320, 256, 3, 3, 3])
decoder.encoder.stages.4.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.4.0.convs.1.conv.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.norm.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.norm.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.4.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.0.conv.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.norm.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.norm.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.1.conv.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.norm.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.norm.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.stages.0.convs.0.conv.weight shape torch.Size([320, 640, 3, 3, 3])
decoder.stages.0.convs.0.conv.bias shape torch.Size([320])
decoder.stages.0.convs.0.norm.weight shape torch.Size([320])
decoder.stages.0.convs.0.norm.bias shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.0.weight shape torch.Size([320, 640, 3, 3, 3])
decoder.stages.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.stages.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.stages.0.convs.1.conv.bias shape torch.Size([320])
decoder.stages.0.convs.1.norm.weight shape torch.Size([320])
decoder.stages.0.convs.1.norm.bias shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.stages.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.stages.1.convs.0.conv.weight shape torch.Size([256, 512, 3, 3, 3])
decoder.stages.1.convs.0.conv.bias shape torch.Size([256])
decoder.stages.1.convs.0.norm.weight shape torch.Size([256])
decoder.stages.1.convs.0.norm.bias shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.0.weight shape torch.Size([256, 512, 3, 3, 3])
decoder.stages.1.convs.0.all_modules.0.bias shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.1.weight shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.1.bias shape torch.Size([256])
decoder.stages.1.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.stages.1.convs.1.conv.bias shape torch.Size([256])
decoder.stages.1.convs.1.norm.weight shape torch.Size([256])
decoder.stages.1.convs.1.norm.bias shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.stages.1.convs.1.all_modules.0.bias shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.1.weight shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.1.bias shape torch.Size([256])
decoder.stages.2.convs.0.conv.weight shape torch.Size([128, 256, 3, 3, 3])
decoder.stages.2.convs.0.conv.bias shape torch.Size([128])
decoder.stages.2.convs.0.norm.weight shape torch.Size([128])
decoder.stages.2.convs.0.norm.bias shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.0.weight shape torch.Size([128, 256, 3, 3, 3])
decoder.stages.2.convs.0.all_modules.0.bias shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.1.weight shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.1.bias shape torch.Size([128])
decoder.stages.2.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.stages.2.convs.1.conv.bias shape torch.Size([128])
decoder.stages.2.convs.1.norm.weight shape torch.Size([128])
decoder.stages.2.convs.1.norm.bias shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.stages.2.convs.1.all_modules.0.bias shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.1.weight shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.1.bias shape torch.Size([128])
decoder.stages.3.convs.0.conv.weight shape torch.Size([64, 128, 3, 3, 3])
decoder.stages.3.convs.0.conv.bias shape torch.Size([64])
decoder.stages.3.convs.0.norm.weight shape torch.Size([64])
decoder.stages.3.convs.0.norm.bias shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.0.weight shape torch.Size([64, 128, 3, 3, 3])
decoder.stages.3.convs.0.all_modules.0.bias shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.1.weight shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.1.bias shape torch.Size([64])
decoder.stages.3.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.stages.3.convs.1.conv.bias shape torch.Size([64])
decoder.stages.3.convs.1.norm.weight shape torch.Size([64])
decoder.stages.3.convs.1.norm.bias shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.stages.3.convs.1.all_modules.0.bias shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.1.weight shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.1.bias shape torch.Size([64])
decoder.stages.4.convs.0.conv.weight shape torch.Size([32, 64, 3, 3, 3])
decoder.stages.4.convs.0.conv.bias shape torch.Size([32])
decoder.stages.4.convs.0.norm.weight shape torch.Size([32])
decoder.stages.4.convs.0.norm.bias shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.0.weight shape torch.Size([32, 64, 3, 3, 3])
decoder.stages.4.convs.0.all_modules.0.bias shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.1.weight shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.1.bias shape torch.Size([32])
decoder.stages.4.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.stages.4.convs.1.conv.bias shape torch.Size([32])
decoder.stages.4.convs.1.norm.weight shape torch.Size([32])
decoder.stages.4.convs.1.norm.bias shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.stages.4.convs.1.all_modules.0.bias shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.1.weight shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.1.bias shape torch.Size([32])
decoder.transpconvs.0.weight shape torch.Size([320, 320, 1, 2, 2])
decoder.transpconvs.0.bias shape torch.Size([320])
decoder.transpconvs.1.weight shape torch.Size([320, 256, 2, 2, 2])
decoder.transpconvs.1.bias shape torch.Size([256])
decoder.transpconvs.2.weight shape torch.Size([256, 128, 2, 2, 2])
decoder.transpconvs.2.bias shape torch.Size([128])
decoder.transpconvs.3.weight shape torch.Size([128, 64, 2, 2, 2])
decoder.transpconvs.3.bias shape torch.Size([64])
decoder.transpconvs.4.weight shape torch.Size([64, 32, 2, 2, 2])
decoder.transpconvs.4.bias shape torch.Size([32])
################### Done ###################
2024-12-21 13:05:58.823667: do_dummy_2d_data_aug: True
2024-12-21 13:05:58.825654: Using splits from existing split file: /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset308_Sohee_Calcium_OCT_CrossValidation/splits_final_20.json
2024-12-21 13:05:58.827891: The split file contains 5 splits.
2024-12-21 13:05:58.829136: Desired fold for training: 4
2024-12-21 13:05:58.830339: This split has 1 training and 7 validation cases.
using pin_memory on device 0
using pin_memory on device 0

This is the configuration used by this training:
Configuration name: 3d_32x160x128_b10
 {'data_identifier': 'nnUNetPlans_3d_fullres', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 10, 'patch_size': [32, 160, 128], 'median_image_size_in_voxels': [375.0, 498.0, 498.0], 'spacing': [1.0, 1.0, 1.0], 'normalization_schemes': ['ZScoreNormalization'], 'use_mask_for_norm': [False], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'architecture': {'network_class_name': 'dynamic_network_architectures.architectures.unet.PlainConvUNet', 'arch_kwargs': {'n_stages': 6, 'features_per_stage': [32, 64, 128, 256, 320, 320], 'conv_op': 'torch.nn.modules.conv.Conv3d', 'kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]], 'strides': [[1, 1, 1], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [1, 2, 2]], 'n_conv_per_stage': [2, 2, 2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2, 2, 2], 'conv_bias': True, 'norm_op': 'torch.nn.modules.instancenorm.InstanceNorm3d', 'norm_op_kwargs': {'eps': 1e-05, 'affine': True}, 'dropout_op': None, 'dropout_op_kwargs': None, 'nonlin': 'torch.nn.LeakyReLU', 'nonlin_kwargs': {'inplace': True}, 'deep_supervision': True}, '_kw_requires_import': ['conv_op', 'norm_op', 'dropout_op', 'nonlin']}, 'batch_dice': True, 'inherits_from': '3d_fullres'} 

These are the global plan.json settings:
 {'dataset_name': 'Dataset308_Sohee_Calcium_OCT_CrossValidation', 'plans_name': 'nnUNetPlans', 'original_median_spacing_after_transp': [1.0, 1.0, 1.0], 'original_median_shape_after_transp': [375, 498, 498], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 0.4977976381778717, 'mean': 0.13507525622844696, 'median': 0.09599608182907104, 'min': 0.0, 'percentile_00_5': 0.014754901640117168, 'percentile_99_5': 0.4977976381778717, 'std': 0.12025152146816254}}} 

2024-12-21 13:06:32.389027: unpacking dataset...
2024-12-21 13:06:36.823406: unpacking done...
2024-12-21 13:06:37.056774: Unable to plot network architecture: nnUNet_compile is enabled!
2024-12-21 13:06:37.229849: 
2024-12-21 13:06:37.230582: Epoch 0
2024-12-21 13:06:37.231314: Current learning rate: 0.01
2024-12-21 13:13:43.999220: Validation loss improved from 1000.00000 to -0.17685! Patience: 0/50
2024-12-21 13:13:44.000092: train_loss -0.1446
2024-12-21 13:13:44.000995: val_loss -0.1768
2024-12-21 13:13:44.001660: Pseudo dice [0.5176]
2024-12-21 13:13:44.002344: Epoch time: 426.77 s
2024-12-21 13:13:44.003108: Yayy! New best EMA pseudo Dice: 0.5176
2024-12-21 13:13:46.483407: 
2024-12-21 13:13:46.484676: Epoch 1
2024-12-21 13:13:46.485466: Current learning rate: 0.00994
2024-12-21 13:19:21.873797: Validation loss improved from -0.17685 to -0.22761! Patience: 0/50
2024-12-21 13:19:21.874550: train_loss -0.3161
2024-12-21 13:19:21.875432: val_loss -0.2276
2024-12-21 13:19:21.876277: Pseudo dice [0.5841]
2024-12-21 13:19:21.877067: Epoch time: 335.39 s
2024-12-21 13:19:21.877953: Yayy! New best EMA pseudo Dice: 0.5243
2024-12-21 13:19:23.647959: 
2024-12-21 13:19:23.649396: Epoch 2
2024-12-21 13:19:23.650374: Current learning rate: 0.00988
2024-12-21 13:24:53.278207: Validation loss did not improve from -0.22761. Patience: 1/50
2024-12-21 13:24:53.278859: train_loss -0.3854
2024-12-21 13:24:53.279629: val_loss -0.1679
2024-12-21 13:24:53.280257: Pseudo dice [0.5619]
2024-12-21 13:24:53.280919: Epoch time: 329.63 s
2024-12-21 13:24:53.281623: Yayy! New best EMA pseudo Dice: 0.5281
2024-12-21 13:24:55.131669: 
2024-12-21 13:24:55.133029: Epoch 3
2024-12-21 13:24:55.133811: Current learning rate: 0.00982
2024-12-21 13:30:35.594487: Validation loss improved from -0.22761 to -0.29819! Patience: 1/50
2024-12-21 13:30:35.595348: train_loss -0.4409
2024-12-21 13:30:35.596110: val_loss -0.2982
2024-12-21 13:30:35.596868: Pseudo dice [0.6114]
2024-12-21 13:30:35.597617: Epoch time: 340.46 s
2024-12-21 13:30:35.598553: Yayy! New best EMA pseudo Dice: 0.5364
2024-12-21 13:30:37.349914: 
2024-12-21 13:30:37.351368: Epoch 4
2024-12-21 13:30:37.352090: Current learning rate: 0.00976
2024-12-21 13:36:14.178088: Validation loss improved from -0.29819 to -0.30698! Patience: 0/50
2024-12-21 13:36:14.179030: train_loss -0.493
2024-12-21 13:36:14.179874: val_loss -0.307
2024-12-21 13:36:14.180575: Pseudo dice [0.5961]
2024-12-21 13:36:14.181261: Epoch time: 336.83 s
2024-12-21 13:36:14.511117: Yayy! New best EMA pseudo Dice: 0.5424
2024-12-21 13:36:16.320803: 
2024-12-21 13:36:16.322362: Epoch 5
2024-12-21 13:36:16.323375: Current learning rate: 0.0097
2024-12-21 13:42:03.576496: Validation loss improved from -0.30698 to -0.34013! Patience: 0/50
2024-12-21 13:42:03.577278: train_loss -0.5125
2024-12-21 13:42:03.578379: val_loss -0.3401
2024-12-21 13:42:03.579426: Pseudo dice [0.6331]
2024-12-21 13:42:03.580430: Epoch time: 347.26 s
2024-12-21 13:42:03.581361: Yayy! New best EMA pseudo Dice: 0.5514
2024-12-21 13:42:05.319918: 
2024-12-21 13:42:05.321203: Epoch 6
2024-12-21 13:42:05.322258: Current learning rate: 0.00964
2024-12-21 13:47:26.250198: Validation loss did not improve from -0.34013. Patience: 1/50
2024-12-21 13:47:26.251072: train_loss -0.5502
2024-12-21 13:47:26.251826: val_loss -0.3314
2024-12-21 13:47:26.252468: Pseudo dice [0.6469]
2024-12-21 13:47:26.253177: Epoch time: 320.93 s
2024-12-21 13:47:26.253795: Yayy! New best EMA pseudo Dice: 0.561
2024-12-21 13:47:28.015796: 
2024-12-21 13:47:28.017006: Epoch 7
2024-12-21 13:47:28.017774: Current learning rate: 0.00958
2024-12-21 13:53:10.722457: Validation loss did not improve from -0.34013. Patience: 2/50
2024-12-21 13:53:10.723426: train_loss -0.5653
2024-12-21 13:53:10.724294: val_loss -0.3095
2024-12-21 13:53:10.725058: Pseudo dice [0.6236]
2024-12-21 13:53:10.726006: Epoch time: 342.71 s
2024-12-21 13:53:10.727062: Yayy! New best EMA pseudo Dice: 0.5673
2024-12-21 13:53:12.835882: 
2024-12-21 13:53:12.837030: Epoch 8
2024-12-21 13:53:12.837796: Current learning rate: 0.00952
2024-12-21 13:59:02.776323: Validation loss improved from -0.34013 to -0.34555! Patience: 2/50
2024-12-21 13:59:02.776982: train_loss -0.5733
2024-12-21 13:59:02.777762: val_loss -0.3456
2024-12-21 13:59:02.778467: Pseudo dice [0.6492]
2024-12-21 13:59:02.779336: Epoch time: 349.94 s
2024-12-21 13:59:02.780106: Yayy! New best EMA pseudo Dice: 0.5755
2024-12-21 13:59:04.545956: 
2024-12-21 13:59:04.547029: Epoch 9
2024-12-21 13:59:04.547691: Current learning rate: 0.00946
2024-12-21 14:04:39.531886: Validation loss did not improve from -0.34555. Patience: 1/50
2024-12-21 14:04:39.532904: train_loss -0.5899
2024-12-21 14:04:39.533725: val_loss -0.3455
2024-12-21 14:04:39.534572: Pseudo dice [0.6375]
2024-12-21 14:04:39.535287: Epoch time: 334.99 s
2024-12-21 14:04:39.944368: Yayy! New best EMA pseudo Dice: 0.5817
2024-12-21 14:04:41.721700: 
2024-12-21 14:04:41.722977: Epoch 10
2024-12-21 14:04:41.723846: Current learning rate: 0.0094
2024-12-21 14:10:12.237185: Validation loss improved from -0.34555 to -0.38688! Patience: 1/50
2024-12-21 14:10:12.237940: train_loss -0.6095
2024-12-21 14:10:12.238674: val_loss -0.3869
2024-12-21 14:10:12.239520: Pseudo dice [0.6625]
2024-12-21 14:10:12.240249: Epoch time: 330.52 s
2024-12-21 14:10:12.241252: Yayy! New best EMA pseudo Dice: 0.5897
2024-12-21 14:10:14.055513: 
2024-12-21 14:10:14.056764: Epoch 11
2024-12-21 14:10:14.057664: Current learning rate: 0.00934
2024-12-21 14:15:41.317841: Validation loss did not improve from -0.38688. Patience: 1/50
2024-12-21 14:15:41.320689: train_loss -0.6206
2024-12-21 14:15:41.322749: val_loss -0.3476
2024-12-21 14:15:41.323527: Pseudo dice [0.632]
2024-12-21 14:15:41.324634: Epoch time: 327.27 s
2024-12-21 14:15:41.325726: Yayy! New best EMA pseudo Dice: 0.594
2024-12-21 14:15:43.080035: 
2024-12-21 14:15:43.081397: Epoch 12
2024-12-21 14:15:43.082116: Current learning rate: 0.00928
2024-12-21 14:21:15.790860: Validation loss did not improve from -0.38688. Patience: 2/50
2024-12-21 14:21:15.792513: train_loss -0.6335
2024-12-21 14:21:15.793705: val_loss -0.362
2024-12-21 14:21:15.794549: Pseudo dice [0.6468]
2024-12-21 14:21:15.795389: Epoch time: 332.71 s
2024-12-21 14:21:15.796182: Yayy! New best EMA pseudo Dice: 0.5992
2024-12-21 14:21:17.549213: 
2024-12-21 14:21:17.550818: Epoch 13
2024-12-21 14:21:17.552129: Current learning rate: 0.00922
2024-12-21 14:27:14.224009: Validation loss improved from -0.38688 to -0.39233! Patience: 2/50
2024-12-21 14:27:14.224665: train_loss -0.633
2024-12-21 14:27:14.225365: val_loss -0.3923
2024-12-21 14:27:14.226151: Pseudo dice [0.6631]
2024-12-21 14:27:14.226840: Epoch time: 356.68 s
2024-12-21 14:27:14.227566: Yayy! New best EMA pseudo Dice: 0.6056
2024-12-21 14:27:16.080508: 
2024-12-21 14:27:16.081380: Epoch 14
2024-12-21 14:27:16.082163: Current learning rate: 0.00916
2024-12-21 14:32:51.196233: Validation loss did not improve from -0.39233. Patience: 1/50
2024-12-21 14:32:51.197005: train_loss -0.6484
2024-12-21 14:32:51.197980: val_loss -0.3476
2024-12-21 14:32:51.198698: Pseudo dice [0.6504]
2024-12-21 14:32:51.199375: Epoch time: 335.12 s
2024-12-21 14:32:51.587020: Yayy! New best EMA pseudo Dice: 0.6101
2024-12-21 14:32:53.333553: 
2024-12-21 14:32:53.334890: Epoch 15
2024-12-21 14:32:53.335695: Current learning rate: 0.0091
2024-12-21 14:38:33.211907: Validation loss did not improve from -0.39233. Patience: 2/50
2024-12-21 14:38:33.212794: train_loss -0.6608
2024-12-21 14:38:33.213703: val_loss -0.3747
2024-12-21 14:38:33.214397: Pseudo dice [0.6609]
2024-12-21 14:38:33.215242: Epoch time: 339.88 s
2024-12-21 14:38:33.216155: Yayy! New best EMA pseudo Dice: 0.6152
2024-12-21 14:38:34.970392: 
2024-12-21 14:38:34.971341: Epoch 16
2024-12-21 14:38:34.972108: Current learning rate: 0.00903
2024-12-21 14:44:33.655828: Validation loss did not improve from -0.39233. Patience: 3/50
2024-12-21 14:44:33.656458: train_loss -0.6511
2024-12-21 14:44:33.657387: val_loss -0.3656
2024-12-21 14:44:33.658371: Pseudo dice [0.6507]
2024-12-21 14:44:33.659558: Epoch time: 358.69 s
2024-12-21 14:44:33.660752: Yayy! New best EMA pseudo Dice: 0.6187
2024-12-21 14:44:35.470527: 
2024-12-21 14:44:35.471963: Epoch 17
2024-12-21 14:44:35.473021: Current learning rate: 0.00897
2024-12-21 14:50:28.111865: Validation loss did not improve from -0.39233. Patience: 4/50
2024-12-21 14:50:28.112739: train_loss -0.6583
2024-12-21 14:50:28.113818: val_loss -0.3252
2024-12-21 14:50:28.114676: Pseudo dice [0.6257]
2024-12-21 14:50:28.115636: Epoch time: 352.64 s
2024-12-21 14:50:28.116569: Yayy! New best EMA pseudo Dice: 0.6194
2024-12-21 14:50:30.937741: 
2024-12-21 14:50:30.938881: Epoch 18
2024-12-21 14:50:30.939727: Current learning rate: 0.00891
2024-12-21 14:56:32.306146: Validation loss did not improve from -0.39233. Patience: 5/50
2024-12-21 14:56:32.307065: train_loss -0.6624
2024-12-21 14:56:32.307981: val_loss -0.3663
2024-12-21 14:56:32.308618: Pseudo dice [0.6539]
2024-12-21 14:56:32.309472: Epoch time: 361.37 s
2024-12-21 14:56:32.310117: Yayy! New best EMA pseudo Dice: 0.6229
2024-12-21 14:56:34.106685: 
2024-12-21 14:56:34.107907: Epoch 19
2024-12-21 14:56:34.108613: Current learning rate: 0.00885
2024-12-21 15:02:08.995622: Validation loss did not improve from -0.39233. Patience: 6/50
2024-12-21 15:02:08.996539: train_loss -0.6839
2024-12-21 15:02:08.997340: val_loss -0.3898
2024-12-21 15:02:08.998017: Pseudo dice [0.6523]
2024-12-21 15:02:08.998816: Epoch time: 334.89 s
2024-12-21 15:02:09.401547: Yayy! New best EMA pseudo Dice: 0.6258
2024-12-21 15:02:11.187607: 
2024-12-21 15:02:11.188484: Epoch 20
2024-12-21 15:02:11.189342: Current learning rate: 0.00879
2024-12-21 15:07:39.780305: Validation loss did not improve from -0.39233. Patience: 7/50
2024-12-21 15:07:39.784531: train_loss -0.6811
2024-12-21 15:07:39.785944: val_loss -0.3819
2024-12-21 15:07:39.786721: Pseudo dice [0.6647]
2024-12-21 15:07:39.787684: Epoch time: 328.6 s
2024-12-21 15:07:39.788582: Yayy! New best EMA pseudo Dice: 0.6297
2024-12-21 15:07:41.670276: 
2024-12-21 15:07:41.671445: Epoch 21
2024-12-21 15:07:41.672453: Current learning rate: 0.00873
2024-12-21 15:13:19.984311: Validation loss improved from -0.39233 to -0.39701! Patience: 7/50
2024-12-21 15:13:19.985239: train_loss -0.6896
2024-12-21 15:13:19.986160: val_loss -0.397
2024-12-21 15:13:19.986845: Pseudo dice [0.6777]
2024-12-21 15:13:19.987587: Epoch time: 338.32 s
2024-12-21 15:13:19.988348: Yayy! New best EMA pseudo Dice: 0.6345
2024-12-21 15:13:21.684553: 
2024-12-21 15:13:21.685557: Epoch 22
2024-12-21 15:13:21.686282: Current learning rate: 0.00867
2024-12-21 15:19:02.058331: Validation loss did not improve from -0.39701. Patience: 1/50
2024-12-21 15:19:02.059177: train_loss -0.6979
2024-12-21 15:19:02.060871: val_loss -0.3796
2024-12-21 15:19:02.061655: Pseudo dice [0.6714]
2024-12-21 15:19:02.062689: Epoch time: 340.38 s
2024-12-21 15:19:02.063307: Yayy! New best EMA pseudo Dice: 0.6382
2024-12-21 15:19:03.808786: 
2024-12-21 15:19:03.809751: Epoch 23
2024-12-21 15:19:03.810514: Current learning rate: 0.00861
2024-12-21 15:24:44.775065: Validation loss did not improve from -0.39701. Patience: 2/50
2024-12-21 15:24:44.775867: train_loss -0.7008
2024-12-21 15:24:44.776593: val_loss -0.3878
2024-12-21 15:24:44.777226: Pseudo dice [0.6711]
2024-12-21 15:24:44.777922: Epoch time: 340.97 s
2024-12-21 15:24:44.778664: Yayy! New best EMA pseudo Dice: 0.6415
2024-12-21 15:24:46.543257: 
2024-12-21 15:24:46.544231: Epoch 24
2024-12-21 15:24:46.545029: Current learning rate: 0.00855
2024-12-21 15:29:33.724785: Validation loss did not improve from -0.39701. Patience: 3/50
2024-12-21 15:29:33.725478: train_loss -0.7018
2024-12-21 15:29:33.726390: val_loss -0.3581
2024-12-21 15:29:33.727407: Pseudo dice [0.6534]
2024-12-21 15:29:33.728220: Epoch time: 287.18 s
2024-12-21 15:29:34.151832: Yayy! New best EMA pseudo Dice: 0.6427
2024-12-21 15:29:35.888438: 
2024-12-21 15:29:35.889324: Epoch 25
2024-12-21 15:29:35.890198: Current learning rate: 0.00849
2024-12-21 15:32:14.576694: Validation loss did not improve from -0.39701. Patience: 4/50
2024-12-21 15:32:14.577692: train_loss -0.694
2024-12-21 15:32:14.578516: val_loss -0.381
2024-12-21 15:32:14.579201: Pseudo dice [0.6593]
2024-12-21 15:32:14.580050: Epoch time: 158.69 s
2024-12-21 15:32:14.580896: Yayy! New best EMA pseudo Dice: 0.6443
2024-12-21 15:32:16.360315: 
2024-12-21 15:32:16.361658: Epoch 26
2024-12-21 15:32:16.362428: Current learning rate: 0.00843
2024-12-21 15:34:56.456086: Validation loss did not improve from -0.39701. Patience: 5/50
2024-12-21 15:34:56.457071: train_loss -0.7119
2024-12-21 15:34:56.457888: val_loss -0.3816
2024-12-21 15:34:56.458689: Pseudo dice [0.6699]
2024-12-21 15:34:56.459492: Epoch time: 160.1 s
2024-12-21 15:34:56.460290: Yayy! New best EMA pseudo Dice: 0.6469
2024-12-21 15:34:58.223530: 
2024-12-21 15:34:58.224681: Epoch 27
2024-12-21 15:34:58.225400: Current learning rate: 0.00836
2024-12-21 15:37:40.605150: Validation loss improved from -0.39701 to -0.42195! Patience: 5/50
2024-12-21 15:37:40.607348: train_loss -0.7184
2024-12-21 15:37:40.608254: val_loss -0.4219
2024-12-21 15:37:40.608940: Pseudo dice [0.6862]
2024-12-21 15:37:40.609753: Epoch time: 162.38 s
2024-12-21 15:37:40.610544: Yayy! New best EMA pseudo Dice: 0.6508
2024-12-21 15:37:42.432047: 
2024-12-21 15:37:42.433241: Epoch 28
2024-12-21 15:37:42.434054: Current learning rate: 0.0083
2024-12-21 15:40:14.500515: Validation loss did not improve from -0.42195. Patience: 1/50
2024-12-21 15:40:14.501334: train_loss -0.7212
2024-12-21 15:40:14.502216: val_loss -0.3794
2024-12-21 15:40:14.503129: Pseudo dice [0.6586]
2024-12-21 15:40:14.504236: Epoch time: 152.07 s
2024-12-21 15:40:14.505238: Yayy! New best EMA pseudo Dice: 0.6516
2024-12-21 15:40:16.855363: 
2024-12-21 15:40:16.857418: Epoch 29
2024-12-21 15:40:16.858682: Current learning rate: 0.00824
2024-12-21 15:42:49.893793: Validation loss did not improve from -0.42195. Patience: 2/50
2024-12-21 15:42:49.895098: train_loss -0.7287
2024-12-21 15:42:49.896064: val_loss -0.3775
2024-12-21 15:42:49.896972: Pseudo dice [0.668]
2024-12-21 15:42:49.897953: Epoch time: 153.04 s
2024-12-21 15:42:50.345844: Yayy! New best EMA pseudo Dice: 0.6532
2024-12-21 15:42:52.150519: 
2024-12-21 15:42:52.151670: Epoch 30
2024-12-21 15:42:52.152582: Current learning rate: 0.00818
2024-12-21 15:45:27.565962: Validation loss did not improve from -0.42195. Patience: 3/50
2024-12-21 15:45:27.567134: train_loss -0.7227
2024-12-21 15:45:27.568106: val_loss -0.4091
2024-12-21 15:45:27.568919: Pseudo dice [0.6807]
2024-12-21 15:45:27.569657: Epoch time: 155.42 s
2024-12-21 15:45:27.570337: Yayy! New best EMA pseudo Dice: 0.656
2024-12-21 15:45:29.429301: 
2024-12-21 15:45:29.430703: Epoch 31
2024-12-21 15:45:29.431661: Current learning rate: 0.00812
2024-12-21 15:48:04.765023: Validation loss did not improve from -0.42195. Patience: 4/50
2024-12-21 15:48:04.765857: train_loss -0.7317
2024-12-21 15:48:04.766709: val_loss -0.3458
2024-12-21 15:48:04.767642: Pseudo dice [0.6645]
2024-12-21 15:48:04.768532: Epoch time: 155.34 s
2024-12-21 15:48:04.769401: Yayy! New best EMA pseudo Dice: 0.6568
2024-12-21 15:48:06.541236: 
2024-12-21 15:48:06.542499: Epoch 32
2024-12-21 15:48:06.543206: Current learning rate: 0.00806
2024-12-21 15:50:43.407995: Validation loss did not improve from -0.42195. Patience: 5/50
2024-12-21 15:50:43.408947: train_loss -0.7363
2024-12-21 15:50:43.409817: val_loss -0.3955
2024-12-21 15:50:43.410631: Pseudo dice [0.6698]
2024-12-21 15:50:43.411359: Epoch time: 156.87 s
2024-12-21 15:50:43.411985: Yayy! New best EMA pseudo Dice: 0.6581
2024-12-21 15:50:45.145324: 
2024-12-21 15:50:45.146682: Epoch 33
2024-12-21 15:50:45.147467: Current learning rate: 0.008
2024-12-21 15:53:18.096187: Validation loss did not improve from -0.42195. Patience: 6/50
2024-12-21 15:53:18.097276: train_loss -0.7382
2024-12-21 15:53:18.098228: val_loss -0.4188
2024-12-21 15:53:18.099041: Pseudo dice [0.6793]
2024-12-21 15:53:18.099949: Epoch time: 152.95 s
2024-12-21 15:53:18.100791: Yayy! New best EMA pseudo Dice: 0.6602
2024-12-21 15:53:19.814777: 
2024-12-21 15:53:19.816114: Epoch 34
2024-12-21 15:53:19.817098: Current learning rate: 0.00793
2024-12-21 15:55:55.624467: Validation loss did not improve from -0.42195. Patience: 7/50
2024-12-21 15:55:55.626266: train_loss -0.7423
2024-12-21 15:55:55.627543: val_loss -0.3925
2024-12-21 15:55:55.628475: Pseudo dice [0.6719]
2024-12-21 15:55:55.629382: Epoch time: 155.81 s
2024-12-21 15:55:56.037160: Yayy! New best EMA pseudo Dice: 0.6614
2024-12-21 15:55:57.974907: 
2024-12-21 15:55:57.976561: Epoch 35
2024-12-21 15:55:57.977582: Current learning rate: 0.00787
2024-12-21 15:58:33.360935: Validation loss did not improve from -0.42195. Patience: 8/50
2024-12-21 15:58:33.362062: train_loss -0.7451
2024-12-21 15:58:33.363077: val_loss -0.4009
2024-12-21 15:58:33.363871: Pseudo dice [0.6903]
2024-12-21 15:58:33.364664: Epoch time: 155.39 s
2024-12-21 15:58:33.365416: Yayy! New best EMA pseudo Dice: 0.6643
2024-12-21 15:58:35.103562: 
2024-12-21 15:58:35.105229: Epoch 36
2024-12-21 15:58:35.106100: Current learning rate: 0.00781
2024-12-21 16:01:09.896742: Validation loss improved from -0.42195 to -0.42254! Patience: 8/50
2024-12-21 16:01:09.897788: train_loss -0.7486
2024-12-21 16:01:09.898985: val_loss -0.4225
2024-12-21 16:01:09.899930: Pseudo dice [0.6953]
2024-12-21 16:01:09.900872: Epoch time: 154.8 s
2024-12-21 16:01:09.901645: Yayy! New best EMA pseudo Dice: 0.6674
2024-12-21 16:01:11.646744: 
2024-12-21 16:01:11.648173: Epoch 37
2024-12-21 16:01:11.649003: Current learning rate: 0.00775
2024-12-21 16:03:46.618515: Validation loss did not improve from -0.42254. Patience: 1/50
2024-12-21 16:03:46.619296: train_loss -0.7465
2024-12-21 16:03:46.620090: val_loss -0.3528
2024-12-21 16:03:46.620883: Pseudo dice [0.6718]
2024-12-21 16:03:46.621757: Epoch time: 154.97 s
2024-12-21 16:03:46.622453: Yayy! New best EMA pseudo Dice: 0.6678
2024-12-21 16:03:48.499293: 
2024-12-21 16:03:48.500761: Epoch 38
2024-12-21 16:03:48.501700: Current learning rate: 0.00769
2024-12-21 16:06:21.921314: Validation loss did not improve from -0.42254. Patience: 2/50
2024-12-21 16:06:21.922829: train_loss -0.7534
2024-12-21 16:06:21.924097: val_loss -0.4217
2024-12-21 16:06:21.925082: Pseudo dice [0.6914]
2024-12-21 16:06:21.926274: Epoch time: 153.43 s
2024-12-21 16:06:21.927201: Yayy! New best EMA pseudo Dice: 0.6702
2024-12-21 16:06:24.345008: 
2024-12-21 16:06:24.346771: Epoch 39
2024-12-21 16:06:24.347696: Current learning rate: 0.00763
2024-12-21 16:09:01.333537: Validation loss did not improve from -0.42254. Patience: 3/50
2024-12-21 16:09:01.337853: train_loss -0.7556
2024-12-21 16:09:01.340206: val_loss -0.4221
2024-12-21 16:09:01.341020: Pseudo dice [0.6868]
2024-12-21 16:09:01.342372: Epoch time: 156.99 s
2024-12-21 16:09:01.755752: Yayy! New best EMA pseudo Dice: 0.6719
2024-12-21 16:09:03.580475: 
2024-12-21 16:09:03.581806: Epoch 40
2024-12-21 16:09:03.582583: Current learning rate: 0.00756
2024-12-21 16:11:37.794421: Validation loss did not improve from -0.42254. Patience: 4/50
2024-12-21 16:11:37.795487: train_loss -0.7583
2024-12-21 16:11:37.796681: val_loss -0.3326
2024-12-21 16:11:37.797387: Pseudo dice [0.6414]
2024-12-21 16:11:37.798095: Epoch time: 154.22 s
2024-12-21 16:11:39.244702: 
2024-12-21 16:11:39.246001: Epoch 41
2024-12-21 16:11:39.246720: Current learning rate: 0.0075
2024-12-21 16:14:15.761348: Validation loss did not improve from -0.42254. Patience: 5/50
2024-12-21 16:14:15.762284: train_loss -0.7522
2024-12-21 16:14:15.763052: val_loss -0.3653
2024-12-21 16:14:15.763852: Pseudo dice [0.6633]
2024-12-21 16:14:15.764525: Epoch time: 156.52 s
2024-12-21 16:14:17.082357: 
2024-12-21 16:14:17.083575: Epoch 42
2024-12-21 16:14:17.084412: Current learning rate: 0.00744
2024-12-21 16:16:52.205605: Validation loss did not improve from -0.42254. Patience: 6/50
2024-12-21 16:16:52.206445: train_loss -0.7641
2024-12-21 16:16:52.207210: val_loss -0.3791
2024-12-21 16:16:52.207964: Pseudo dice [0.6709]
2024-12-21 16:16:52.208655: Epoch time: 155.13 s
2024-12-21 16:16:53.553856: 
2024-12-21 16:16:53.555478: Epoch 43
2024-12-21 16:16:53.556203: Current learning rate: 0.00738
2024-12-21 16:19:29.694156: Validation loss did not improve from -0.42254. Patience: 7/50
2024-12-21 16:19:29.694912: train_loss -0.7613
2024-12-21 16:19:29.695964: val_loss -0.3779
2024-12-21 16:19:29.696810: Pseudo dice [0.6681]
2024-12-21 16:19:29.697654: Epoch time: 156.14 s
2024-12-21 16:19:31.059831: 
2024-12-21 16:19:31.061236: Epoch 44
2024-12-21 16:19:31.062129: Current learning rate: 0.00732
2024-12-21 16:22:06.998431: Validation loss did not improve from -0.42254. Patience: 8/50
2024-12-21 16:22:07.000545: train_loss -0.763
2024-12-21 16:22:07.001912: val_loss -0.3709
2024-12-21 16:22:07.002751: Pseudo dice [0.672]
2024-12-21 16:22:07.003697: Epoch time: 155.94 s
2024-12-21 16:22:08.748953: 
2024-12-21 16:22:08.750093: Epoch 45
2024-12-21 16:22:08.750872: Current learning rate: 0.00725
2024-12-21 16:24:46.792048: Validation loss did not improve from -0.42254. Patience: 9/50
2024-12-21 16:24:46.792940: train_loss -0.7638
2024-12-21 16:24:46.795477: val_loss -0.3997
2024-12-21 16:24:46.796549: Pseudo dice [0.6803]
2024-12-21 16:24:46.798271: Epoch time: 158.05 s
2024-12-21 16:24:48.103305: 
2024-12-21 16:24:48.105163: Epoch 46
2024-12-21 16:24:48.106139: Current learning rate: 0.00719
2024-12-21 16:27:22.711076: Validation loss did not improve from -0.42254. Patience: 10/50
2024-12-21 16:27:22.712164: train_loss -0.7703
2024-12-21 16:27:22.713391: val_loss -0.3582
2024-12-21 16:27:22.714185: Pseudo dice [0.6618]
2024-12-21 16:27:22.714974: Epoch time: 154.61 s
2024-12-21 16:27:24.058164: 
2024-12-21 16:27:24.059750: Epoch 47
2024-12-21 16:27:24.060836: Current learning rate: 0.00713
2024-12-21 16:30:00.979198: Validation loss did not improve from -0.42254. Patience: 11/50
2024-12-21 16:30:00.980126: train_loss -0.7692
2024-12-21 16:30:00.981337: val_loss -0.4049
2024-12-21 16:30:00.982296: Pseudo dice [0.6961]
2024-12-21 16:30:00.983250: Epoch time: 156.92 s
2024-12-21 16:30:02.301901: 
2024-12-21 16:30:02.303635: Epoch 48
2024-12-21 16:30:02.304549: Current learning rate: 0.00707
2024-12-21 16:32:40.176645: Validation loss did not improve from -0.42254. Patience: 12/50
2024-12-21 16:32:40.177706: train_loss -0.771
2024-12-21 16:32:40.178833: val_loss -0.3716
2024-12-21 16:32:40.179935: Pseudo dice [0.6631]
2024-12-21 16:32:40.180940: Epoch time: 157.88 s
2024-12-21 16:32:41.596172: 
2024-12-21 16:32:41.597402: Epoch 49
2024-12-21 16:32:41.598075: Current learning rate: 0.007
2024-12-21 16:34:32.571590: Validation loss did not improve from -0.42254. Patience: 13/50
2024-12-21 16:34:32.573709: train_loss -0.7779
2024-12-21 16:34:32.574854: val_loss -0.3557
2024-12-21 16:34:32.575881: Pseudo dice [0.6667]
2024-12-21 16:34:32.577009: Epoch time: 110.98 s
2024-12-21 16:34:35.119849: 
2024-12-21 16:34:35.121735: Epoch 50
2024-12-21 16:34:35.123033: Current learning rate: 0.00694
2024-12-21 16:36:05.349930: Validation loss did not improve from -0.42254. Patience: 14/50
2024-12-21 16:36:05.350960: train_loss -0.7745
2024-12-21 16:36:05.351910: val_loss -0.3703
2024-12-21 16:36:05.352744: Pseudo dice [0.6618]
2024-12-21 16:36:05.353486: Epoch time: 90.23 s
2024-12-21 16:36:06.747206: 
2024-12-21 16:36:06.748243: Epoch 51
2024-12-21 16:36:06.749110: Current learning rate: 0.00688
2024-12-21 16:37:37.085240: Validation loss did not improve from -0.42254. Patience: 15/50
2024-12-21 16:37:37.085987: train_loss -0.7782
2024-12-21 16:37:37.086711: val_loss -0.3799
2024-12-21 16:37:37.087512: Pseudo dice [0.6801]
2024-12-21 16:37:37.088333: Epoch time: 90.34 s
2024-12-21 16:37:38.414547: 
2024-12-21 16:37:38.415779: Epoch 52
2024-12-21 16:37:38.416764: Current learning rate: 0.00682
2024-12-21 16:39:06.855960: Validation loss did not improve from -0.42254. Patience: 16/50
2024-12-21 16:39:06.856957: train_loss -0.7737
2024-12-21 16:39:06.857805: val_loss -0.4054
2024-12-21 16:39:06.858552: Pseudo dice [0.693]
2024-12-21 16:39:06.859195: Epoch time: 88.44 s
2024-12-21 16:39:06.859934: Yayy! New best EMA pseudo Dice: 0.6729
2024-12-21 16:39:08.565154: 
2024-12-21 16:39:08.567029: Epoch 53
2024-12-21 16:39:08.568064: Current learning rate: 0.00675
2024-12-21 16:40:38.684550: Validation loss did not improve from -0.42254. Patience: 17/50
2024-12-21 16:40:38.685586: train_loss -0.7787
2024-12-21 16:40:38.686506: val_loss -0.3479
2024-12-21 16:40:38.687140: Pseudo dice [0.6565]
2024-12-21 16:40:38.687769: Epoch time: 90.12 s
2024-12-21 16:40:39.995141: 
2024-12-21 16:40:39.996992: Epoch 54
2024-12-21 16:40:39.997768: Current learning rate: 0.00669
2024-12-21 16:42:08.077253: Validation loss did not improve from -0.42254. Patience: 18/50
2024-12-21 16:42:08.078277: train_loss -0.7785
2024-12-21 16:42:08.079359: val_loss -0.3775
2024-12-21 16:42:08.080111: Pseudo dice [0.6741]
2024-12-21 16:42:08.080902: Epoch time: 88.08 s
2024-12-21 16:42:09.692394: 
2024-12-21 16:42:09.694518: Epoch 55
2024-12-21 16:42:09.695758: Current learning rate: 0.00663
2024-12-21 16:43:40.362038: Validation loss did not improve from -0.42254. Patience: 19/50
2024-12-21 16:43:40.362718: train_loss -0.7783
2024-12-21 16:43:40.363729: val_loss -0.4114
2024-12-21 16:43:40.364862: Pseudo dice [0.692]
2024-12-21 16:43:40.365837: Epoch time: 90.67 s
2024-12-21 16:43:40.366837: Yayy! New best EMA pseudo Dice: 0.6736
2024-12-21 16:43:41.989869: 
2024-12-21 16:43:41.990989: Epoch 56
2024-12-21 16:43:41.992038: Current learning rate: 0.00657
2024-12-21 16:45:09.714971: Validation loss did not improve from -0.42254. Patience: 20/50
2024-12-21 16:45:09.716116: train_loss -0.7799
2024-12-21 16:45:09.717425: val_loss -0.4028
2024-12-21 16:45:09.718581: Pseudo dice [0.6918]
2024-12-21 16:45:09.719488: Epoch time: 87.73 s
2024-12-21 16:45:09.720553: Yayy! New best EMA pseudo Dice: 0.6754
2024-12-21 16:45:11.356099: 
2024-12-21 16:45:11.358418: Epoch 57
2024-12-21 16:45:11.359732: Current learning rate: 0.0065
2024-12-21 16:46:39.150435: Validation loss did not improve from -0.42254. Patience: 21/50
2024-12-21 16:46:39.151013: train_loss -0.7835
2024-12-21 16:46:39.151860: val_loss -0.3247
2024-12-21 16:46:39.152565: Pseudo dice [0.6383]
2024-12-21 16:46:39.153315: Epoch time: 87.8 s
2024-12-21 16:46:40.374491: 
2024-12-21 16:46:40.376456: Epoch 58
2024-12-21 16:46:40.377213: Current learning rate: 0.00644
2024-12-21 16:48:08.127400: Validation loss did not improve from -0.42254. Patience: 22/50
2024-12-21 16:48:08.128103: train_loss -0.7878
2024-12-21 16:48:08.128927: val_loss -0.3593
2024-12-21 16:48:08.129657: Pseudo dice [0.6707]
2024-12-21 16:48:08.130260: Epoch time: 87.75 s
2024-12-21 16:48:09.406421: 
2024-12-21 16:48:09.408613: Epoch 59
2024-12-21 16:48:09.409632: Current learning rate: 0.00638
2024-12-21 16:49:37.262821: Validation loss did not improve from -0.42254. Patience: 23/50
2024-12-21 16:49:37.263962: train_loss -0.7887
2024-12-21 16:49:37.265155: val_loss -0.3635
2024-12-21 16:49:37.266035: Pseudo dice [0.6728]
2024-12-21 16:49:37.266974: Epoch time: 87.86 s
2024-12-21 16:49:39.258932: 
2024-12-21 16:49:39.260148: Epoch 60
2024-12-21 16:49:39.260916: Current learning rate: 0.00631
2024-12-21 16:51:06.938874: Validation loss did not improve from -0.42254. Patience: 24/50
2024-12-21 16:51:06.939723: train_loss -0.7946
2024-12-21 16:51:06.940534: val_loss -0.3626
2024-12-21 16:51:06.941132: Pseudo dice [0.667]
2024-12-21 16:51:06.941771: Epoch time: 87.68 s
2024-12-21 16:51:08.177070: 
2024-12-21 16:51:08.179273: Epoch 61
2024-12-21 16:51:08.180007: Current learning rate: 0.00625
2024-12-21 16:52:35.928297: Validation loss did not improve from -0.42254. Patience: 25/50
2024-12-21 16:52:35.929216: train_loss -0.7896
2024-12-21 16:52:35.929993: val_loss -0.3487
2024-12-21 16:52:35.930599: Pseudo dice [0.6482]
2024-12-21 16:52:35.931218: Epoch time: 87.75 s
2024-12-21 16:52:37.167655: 
2024-12-21 16:52:37.168833: Epoch 62
2024-12-21 16:52:37.169649: Current learning rate: 0.00619
2024-12-21 16:54:05.019822: Validation loss did not improve from -0.42254. Patience: 26/50
2024-12-21 16:54:05.020530: train_loss -0.7831
2024-12-21 16:54:05.021797: val_loss -0.3647
2024-12-21 16:54:05.022674: Pseudo dice [0.6784]
2024-12-21 16:54:05.023649: Epoch time: 87.85 s
2024-12-21 16:54:06.249163: 
2024-12-21 16:54:06.250337: Epoch 63
2024-12-21 16:54:06.251391: Current learning rate: 0.00612
2024-12-21 16:55:34.094342: Validation loss did not improve from -0.42254. Patience: 27/50
2024-12-21 16:55:34.095200: train_loss -0.7916
2024-12-21 16:55:34.096061: val_loss -0.4054
2024-12-21 16:55:34.096755: Pseudo dice [0.6785]
2024-12-21 16:55:34.097479: Epoch time: 87.85 s
2024-12-21 16:55:35.318132: 
2024-12-21 16:55:35.319879: Epoch 64
2024-12-21 16:55:35.320642: Current learning rate: 0.00606
2024-12-21 16:57:03.320166: Validation loss did not improve from -0.42254. Patience: 28/50
2024-12-21 16:57:03.321517: train_loss -0.7936
2024-12-21 16:57:03.322904: val_loss -0.3739
2024-12-21 16:57:03.323711: Pseudo dice [0.6781]
2024-12-21 16:57:03.324726: Epoch time: 88.0 s
2024-12-21 16:57:04.906838: 
2024-12-21 16:57:04.908561: Epoch 65
2024-12-21 16:57:04.909683: Current learning rate: 0.006
2024-12-21 16:58:33.063555: Validation loss did not improve from -0.42254. Patience: 29/50
2024-12-21 16:58:33.064869: train_loss -0.794
2024-12-21 16:58:33.066199: val_loss -0.3774
2024-12-21 16:58:33.067054: Pseudo dice [0.6764]
2024-12-21 16:58:33.067842: Epoch time: 88.16 s
2024-12-21 16:58:34.319326: 
2024-12-21 16:58:34.320975: Epoch 66
2024-12-21 16:58:34.321942: Current learning rate: 0.00593
2024-12-21 17:00:02.382679: Validation loss did not improve from -0.42254. Patience: 30/50
2024-12-21 17:00:02.383228: train_loss -0.7989
2024-12-21 17:00:02.383869: val_loss -0.3646
2024-12-21 17:00:02.384499: Pseudo dice [0.6651]
2024-12-21 17:00:02.385583: Epoch time: 88.06 s
2024-12-21 17:00:03.662711: 
2024-12-21 17:00:03.664701: Epoch 67
2024-12-21 17:00:03.666098: Current learning rate: 0.00587
2024-12-21 17:01:31.698146: Validation loss did not improve from -0.42254. Patience: 31/50
2024-12-21 17:01:31.699184: train_loss -0.7972
2024-12-21 17:01:31.699917: val_loss -0.3524
2024-12-21 17:01:31.700665: Pseudo dice [0.6835]
2024-12-21 17:01:31.701543: Epoch time: 88.04 s
2024-12-21 17:01:32.972295: 
2024-12-21 17:01:32.974104: Epoch 68
2024-12-21 17:01:32.974945: Current learning rate: 0.00581
2024-12-21 17:03:01.012603: Validation loss did not improve from -0.42254. Patience: 32/50
2024-12-21 17:03:01.014067: train_loss -0.8008
2024-12-21 17:03:01.015532: val_loss -0.3765
2024-12-21 17:03:01.016633: Pseudo dice [0.68]
2024-12-21 17:03:01.017515: Epoch time: 88.04 s
2024-12-21 17:03:02.227538: 
2024-12-21 17:03:02.229421: Epoch 69
2024-12-21 17:03:02.230355: Current learning rate: 0.00574
2024-12-21 17:04:30.249769: Validation loss did not improve from -0.42254. Patience: 33/50
2024-12-21 17:04:30.250972: train_loss -0.7979
2024-12-21 17:04:30.252228: val_loss -0.3614
2024-12-21 17:04:30.253206: Pseudo dice [0.671]
2024-12-21 17:04:30.254236: Epoch time: 88.02 s
2024-12-21 17:04:32.439974: 
2024-12-21 17:04:32.441715: Epoch 70
2024-12-21 17:04:32.442741: Current learning rate: 0.00568
2024-12-21 17:06:00.358647: Validation loss did not improve from -0.42254. Patience: 34/50
2024-12-21 17:06:00.359509: train_loss -0.7969
2024-12-21 17:06:00.360317: val_loss -0.3822
2024-12-21 17:06:00.361124: Pseudo dice [0.6731]
2024-12-21 17:06:00.361747: Epoch time: 87.92 s
2024-12-21 17:06:01.585924: 
2024-12-21 17:06:01.587271: Epoch 71
2024-12-21 17:06:01.588401: Current learning rate: 0.00562
2024-12-21 17:07:29.691218: Validation loss did not improve from -0.42254. Patience: 35/50
2024-12-21 17:07:29.691911: train_loss -0.8023
2024-12-21 17:07:29.692611: val_loss -0.3894
2024-12-21 17:07:29.693383: Pseudo dice [0.6822]
2024-12-21 17:07:29.694175: Epoch time: 88.11 s
2024-12-21 17:07:30.915174: 
2024-12-21 17:07:30.917314: Epoch 72
2024-12-21 17:07:30.918082: Current learning rate: 0.00555
2024-12-21 17:08:59.146230: Validation loss did not improve from -0.42254. Patience: 36/50
2024-12-21 17:08:59.147127: train_loss -0.7997
2024-12-21 17:08:59.148525: val_loss -0.3585
2024-12-21 17:08:59.149295: Pseudo dice [0.6854]
2024-12-21 17:08:59.149906: Epoch time: 88.23 s
2024-12-21 17:09:00.396827: 
2024-12-21 17:09:00.398772: Epoch 73
2024-12-21 17:09:00.399915: Current learning rate: 0.00549
2024-12-21 17:10:28.698560: Validation loss did not improve from -0.42254. Patience: 37/50
2024-12-21 17:10:28.699463: train_loss -0.805
2024-12-21 17:10:28.700281: val_loss -0.3999
2024-12-21 17:10:28.700978: Pseudo dice [0.6871]
2024-12-21 17:10:28.701839: Epoch time: 88.3 s
2024-12-21 17:10:28.702716: Yayy! New best EMA pseudo Dice: 0.6763
2024-12-21 17:10:30.301697: 
2024-12-21 17:10:30.303401: Epoch 74
2024-12-21 17:10:30.304452: Current learning rate: 0.00542
2024-12-21 17:11:58.746620: Validation loss did not improve from -0.42254. Patience: 38/50
2024-12-21 17:11:58.747764: train_loss -0.8068
2024-12-21 17:11:58.748969: val_loss -0.3569
2024-12-21 17:11:58.750025: Pseudo dice [0.6677]
2024-12-21 17:11:58.750994: Epoch time: 88.45 s
2024-12-21 17:12:00.407279: 
2024-12-21 17:12:00.409853: Epoch 75
2024-12-21 17:12:00.411005: Current learning rate: 0.00536
2024-12-21 17:13:28.731624: Validation loss did not improve from -0.42254. Patience: 39/50
2024-12-21 17:13:28.735546: train_loss -0.807
2024-12-21 17:13:28.737224: val_loss -0.3726
2024-12-21 17:13:28.738232: Pseudo dice [0.6847]
2024-12-21 17:13:28.739301: Epoch time: 88.33 s
2024-12-21 17:13:28.740812: Yayy! New best EMA pseudo Dice: 0.6764
2024-12-21 17:13:30.322680: 
2024-12-21 17:13:30.325649: Epoch 76
2024-12-21 17:13:30.327317: Current learning rate: 0.00529
2024-12-21 17:14:58.640712: Validation loss did not improve from -0.42254. Patience: 40/50
2024-12-21 17:14:58.642206: train_loss -0.81
2024-12-21 17:14:58.643465: val_loss -0.3667
2024-12-21 17:14:58.644904: Pseudo dice [0.6749]
2024-12-21 17:14:58.646304: Epoch time: 88.32 s
2024-12-21 17:14:59.872444: 
2024-12-21 17:14:59.874244: Epoch 77
2024-12-21 17:14:59.875533: Current learning rate: 0.00523
2024-12-21 17:16:28.133986: Validation loss did not improve from -0.42254. Patience: 41/50
2024-12-21 17:16:28.135158: train_loss -0.8121
2024-12-21 17:16:28.136402: val_loss -0.3774
2024-12-21 17:16:28.137031: Pseudo dice [0.693]
2024-12-21 17:16:28.137681: Epoch time: 88.26 s
2024-12-21 17:16:28.138611: Yayy! New best EMA pseudo Dice: 0.6779
2024-12-21 17:16:29.750296: 
2024-12-21 17:16:29.752201: Epoch 78
2024-12-21 17:16:29.753387: Current learning rate: 0.00517
2024-12-21 17:17:57.959084: Validation loss did not improve from -0.42254. Patience: 42/50
2024-12-21 17:17:57.960091: train_loss -0.8084
2024-12-21 17:17:57.960938: val_loss -0.3788
2024-12-21 17:17:57.961668: Pseudo dice [0.6799]
2024-12-21 17:17:57.962545: Epoch time: 88.21 s
2024-12-21 17:17:57.963258: Yayy! New best EMA pseudo Dice: 0.6781
2024-12-21 17:17:59.585480: 
2024-12-21 17:17:59.587675: Epoch 79
2024-12-21 17:17:59.588643: Current learning rate: 0.0051
2024-12-21 17:19:27.633597: Validation loss did not improve from -0.42254. Patience: 43/50
2024-12-21 17:19:27.634211: train_loss -0.8067
2024-12-21 17:19:27.634933: val_loss -0.3389
2024-12-21 17:19:27.635545: Pseudo dice [0.6662]
2024-12-21 17:19:27.636399: Epoch time: 88.05 s
2024-12-21 17:19:29.282830: 
2024-12-21 17:19:29.284738: Epoch 80
2024-12-21 17:19:29.286019: Current learning rate: 0.00504
2024-12-21 17:20:57.293473: Validation loss did not improve from -0.42254. Patience: 44/50
2024-12-21 17:20:57.294458: train_loss -0.8113
2024-12-21 17:20:57.295241: val_loss -0.3625
2024-12-21 17:20:57.295938: Pseudo dice [0.6752]
2024-12-21 17:20:57.296663: Epoch time: 88.01 s
2024-12-21 17:20:58.952759: 
2024-12-21 17:20:58.954165: Epoch 81
2024-12-21 17:20:58.955211: Current learning rate: 0.00497
2024-12-21 17:22:26.971150: Validation loss did not improve from -0.42254. Patience: 45/50
2024-12-21 17:22:26.971874: train_loss -0.8149
2024-12-21 17:22:26.972728: val_loss -0.3691
2024-12-21 17:22:26.973385: Pseudo dice [0.6835]
2024-12-21 17:22:26.974122: Epoch time: 88.02 s
2024-12-21 17:22:28.254625: 
2024-12-21 17:22:28.256306: Epoch 82
2024-12-21 17:22:28.257480: Current learning rate: 0.00491
2024-12-21 17:23:56.115485: Validation loss did not improve from -0.42254. Patience: 46/50
2024-12-21 17:23:56.116432: train_loss -0.8155
2024-12-21 17:23:56.117628: val_loss -0.3674
2024-12-21 17:23:56.118757: Pseudo dice [0.6687]
2024-12-21 17:23:56.119815: Epoch time: 87.86 s
2024-12-21 17:23:57.315253: 
2024-12-21 17:23:57.316559: Epoch 83
2024-12-21 17:23:57.317398: Current learning rate: 0.00484
2024-12-21 17:25:25.314092: Validation loss did not improve from -0.42254. Patience: 47/50
2024-12-21 17:25:25.314909: train_loss -0.8122
2024-12-21 17:25:25.315867: val_loss -0.3452
2024-12-21 17:25:25.316607: Pseudo dice [0.667]
2024-12-21 17:25:25.317227: Epoch time: 88.0 s
2024-12-21 17:25:26.478334: 
2024-12-21 17:25:26.480191: Epoch 84
2024-12-21 17:25:26.481454: Current learning rate: 0.00478
2024-12-21 17:26:54.517851: Validation loss did not improve from -0.42254. Patience: 48/50
2024-12-21 17:26:54.519554: train_loss -0.8143
2024-12-21 17:26:54.520631: val_loss -0.3518
2024-12-21 17:26:54.521342: Pseudo dice [0.6714]
2024-12-21 17:26:54.522000: Epoch time: 88.04 s
2024-12-21 17:26:56.085915: 
2024-12-21 17:26:56.087906: Epoch 85
2024-12-21 17:26:56.089526: Current learning rate: 0.00471
2024-12-21 17:28:24.025404: Validation loss did not improve from -0.42254. Patience: 49/50
2024-12-21 17:28:24.026232: train_loss -0.8146
2024-12-21 17:28:24.029044: val_loss -0.3986
2024-12-21 17:28:24.030440: Pseudo dice [0.697]
2024-12-21 17:28:24.032037: Epoch time: 87.94 s
2024-12-21 17:28:25.231385: 
2024-12-21 17:28:25.233467: Epoch 86
2024-12-21 17:28:25.234817: Current learning rate: 0.00465
2024-12-21 17:29:53.247919: Validation loss did not improve from -0.42254. Patience: 50/50
2024-12-21 17:29:53.248842: train_loss -0.8153
2024-12-21 17:29:53.250146: val_loss -0.3951
2024-12-21 17:29:53.251238: Pseudo dice [0.692]
2024-12-21 17:29:53.252246: Epoch time: 88.02 s
2024-12-21 17:29:53.253395: Yayy! New best EMA pseudo Dice: 0.6788
2024-12-21 17:29:54.779933: 
2024-12-21 17:29:54.781626: Epoch 87
2024-12-21 17:29:54.782650: Current learning rate: 0.00458
2024-12-21 17:31:23.017244: Validation loss did not improve from -0.42254. Patience: 51/50
2024-12-21 17:31:23.017922: train_loss -0.8175
2024-12-21 17:31:23.018704: val_loss -0.3152
2024-12-21 17:31:23.019480: Pseudo dice [0.6489]
2024-12-21 17:31:23.020297: Epoch time: 88.24 s
2024-12-21 17:31:24.195211: 
2024-12-21 17:31:24.197255: Epoch 88
2024-12-21 17:31:24.198494: Current learning rate: 0.00452
2024-12-21 17:32:52.339196: Validation loss did not improve from -0.42254. Patience: 52/50
2024-12-21 17:32:52.339986: train_loss -0.8165
2024-12-21 17:32:52.341271: val_loss -0.384
2024-12-21 17:32:52.342446: Pseudo dice [0.6906]
2024-12-21 17:32:52.343594: Epoch time: 88.15 s
2024-12-21 17:32:53.535064: 
2024-12-21 17:32:53.537259: Epoch 89
2024-12-21 17:32:53.538248: Current learning rate: 0.00445
2024-12-21 17:34:21.758612: Validation loss did not improve from -0.42254. Patience: 53/50
2024-12-21 17:34:21.759212: train_loss -0.8157
2024-12-21 17:34:21.759846: val_loss -0.3517
2024-12-21 17:34:21.760828: Pseudo dice [0.6784]
2024-12-21 17:34:21.761623: Epoch time: 88.23 s
2024-12-21 17:34:23.246895: 
2024-12-21 17:34:23.249037: Epoch 90
2024-12-21 17:34:23.250211: Current learning rate: 0.00438
2024-12-21 17:35:51.436929: Validation loss did not improve from -0.42254. Patience: 54/50
2024-12-21 17:35:51.438005: train_loss -0.8185
2024-12-21 17:35:51.438927: val_loss -0.4029
2024-12-21 17:35:51.439789: Pseudo dice [0.6816]
2024-12-21 17:35:51.440457: Epoch time: 88.19 s
2024-12-21 17:35:52.625681: 
2024-12-21 17:35:52.627101: Epoch 91
2024-12-21 17:35:52.628213: Current learning rate: 0.00432
2024-12-21 17:37:20.879810: Validation loss did not improve from -0.42254. Patience: 55/50
2024-12-21 17:37:20.880528: train_loss -0.8188
2024-12-21 17:37:20.881464: val_loss -0.3649
2024-12-21 17:37:20.882199: Pseudo dice [0.6798]
2024-12-21 17:37:20.882939: Epoch time: 88.26 s
2024-12-21 17:37:22.382245: 
2024-12-21 17:37:22.383778: Epoch 92
2024-12-21 17:37:22.384721: Current learning rate: 0.00425
2024-12-21 17:38:50.704791: Validation loss did not improve from -0.42254. Patience: 56/50
2024-12-21 17:38:50.705918: train_loss -0.819
2024-12-21 17:38:50.707588: val_loss -0.3697
2024-12-21 17:38:50.708938: Pseudo dice [0.6783]
2024-12-21 17:38:50.709707: Epoch time: 88.32 s
2024-12-21 17:38:51.915440: 
2024-12-21 17:38:51.917150: Epoch 93
2024-12-21 17:38:51.918376: Current learning rate: 0.00419
2024-12-21 17:40:20.138542: Validation loss did not improve from -0.42254. Patience: 57/50
2024-12-21 17:40:20.139375: train_loss -0.8222
2024-12-21 17:40:20.140716: val_loss -0.354
2024-12-21 17:40:20.141849: Pseudo dice [0.6805]
2024-12-21 17:40:20.143066: Epoch time: 88.22 s
2024-12-21 17:40:21.421151: 
2024-12-21 17:40:21.423874: Epoch 94
2024-12-21 17:40:21.425143: Current learning rate: 0.00412
2024-12-21 17:41:49.678158: Validation loss did not improve from -0.42254. Patience: 58/50
2024-12-21 17:41:49.679290: train_loss -0.8226
2024-12-21 17:41:49.680173: val_loss -0.3298
2024-12-21 17:41:49.680862: Pseudo dice [0.6641]
2024-12-21 17:41:49.681461: Epoch time: 88.26 s
2024-12-21 17:41:51.260977: 
2024-12-21 17:41:51.262573: Epoch 95
2024-12-21 17:41:51.263348: Current learning rate: 0.00405
2024-12-21 17:43:19.682490: Validation loss did not improve from -0.42254. Patience: 59/50
2024-12-21 17:43:19.683479: train_loss -0.8213
2024-12-21 17:43:19.684433: val_loss -0.367
2024-12-21 17:43:19.685108: Pseudo dice [0.6778]
2024-12-21 17:43:19.685823: Epoch time: 88.42 s
2024-12-21 17:43:20.893408: 
2024-12-21 17:43:20.895276: Epoch 96
2024-12-21 17:43:20.896773: Current learning rate: 0.00399
2024-12-21 17:44:49.292620: Validation loss did not improve from -0.42254. Patience: 60/50
2024-12-21 17:44:49.294039: train_loss -0.8232
2024-12-21 17:44:49.295247: val_loss -0.3785
2024-12-21 17:44:49.296195: Pseudo dice [0.6786]
2024-12-21 17:44:49.297287: Epoch time: 88.4 s
2024-12-21 17:44:50.507234: 
2024-12-21 17:44:50.508684: Epoch 97
2024-12-21 17:44:50.509459: Current learning rate: 0.00392
2024-12-21 17:46:18.840552: Validation loss did not improve from -0.42254. Patience: 61/50
2024-12-21 17:46:18.841214: train_loss -0.8233
2024-12-21 17:46:18.842067: val_loss -0.404
2024-12-21 17:46:18.842655: Pseudo dice [0.7065]
2024-12-21 17:46:18.843340: Epoch time: 88.34 s
2024-12-21 17:46:18.843993: Yayy! New best EMA pseudo Dice: 0.6801
2024-12-21 17:46:20.456321: 
2024-12-21 17:46:20.457943: Epoch 98
2024-12-21 17:46:20.459363: Current learning rate: 0.00385
2024-12-21 17:47:48.758782: Validation loss did not improve from -0.42254. Patience: 62/50
2024-12-21 17:47:48.759713: train_loss -0.8232
2024-12-21 17:47:48.760909: val_loss -0.3932
2024-12-21 17:47:48.761583: Pseudo dice [0.6868]
2024-12-21 17:47:48.762454: Epoch time: 88.3 s
2024-12-21 17:47:48.763333: Yayy! New best EMA pseudo Dice: 0.6807
2024-12-21 17:47:50.344747: 
2024-12-21 17:47:50.346479: Epoch 99
2024-12-21 17:47:50.347662: Current learning rate: 0.00379
2024-12-21 17:49:18.575066: Validation loss did not improve from -0.42254. Patience: 63/50
2024-12-21 17:49:18.576227: train_loss -0.824
2024-12-21 17:49:18.577259: val_loss -0.3893
2024-12-21 17:49:18.577997: Pseudo dice [0.6788]
2024-12-21 17:49:18.579163: Epoch time: 88.23 s
2024-12-21 17:49:20.144581: 
2024-12-21 17:49:20.146185: Epoch 100
2024-12-21 17:49:20.147354: Current learning rate: 0.00372
2024-12-21 17:50:48.336651: Validation loss did not improve from -0.42254. Patience: 64/50
2024-12-21 17:50:48.337332: train_loss -0.8234
2024-12-21 17:50:48.338069: val_loss -0.3806
2024-12-21 17:50:48.338833: Pseudo dice [0.6784]
2024-12-21 17:50:48.339565: Epoch time: 88.19 s
2024-12-21 17:50:49.534217: 
2024-12-21 17:50:49.536419: Epoch 101
2024-12-21 17:50:49.537719: Current learning rate: 0.00365
2024-12-21 17:52:17.766532: Validation loss did not improve from -0.42254. Patience: 65/50
2024-12-21 17:52:17.767265: train_loss -0.8244
2024-12-21 17:52:17.768604: val_loss -0.3622
2024-12-21 17:52:17.770024: Pseudo dice [0.6768]
2024-12-21 17:52:17.771058: Epoch time: 88.23 s
2024-12-21 17:52:18.937337: 
2024-12-21 17:52:18.939341: Epoch 102
2024-12-21 17:52:18.940516: Current learning rate: 0.00359
2024-12-21 17:53:47.050465: Validation loss did not improve from -0.42254. Patience: 66/50
2024-12-21 17:53:47.051953: train_loss -0.8263
2024-12-21 17:53:47.053175: val_loss -0.3512
2024-12-21 17:53:47.054343: Pseudo dice [0.6677]
2024-12-21 17:53:47.055185: Epoch time: 88.12 s
2024-12-21 17:53:48.614563: 
2024-12-21 17:53:48.616315: Epoch 103
2024-12-21 17:53:48.617107: Current learning rate: 0.00352
2024-12-21 17:55:16.664859: Validation loss did not improve from -0.42254. Patience: 67/50
2024-12-21 17:55:16.665756: train_loss -0.8255
2024-12-21 17:55:16.666465: val_loss -0.3705
2024-12-21 17:55:16.667174: Pseudo dice [0.6774]
2024-12-21 17:55:16.667927: Epoch time: 88.05 s
2024-12-21 17:55:17.858573: 
2024-12-21 17:55:17.860594: Epoch 104
2024-12-21 17:55:17.861679: Current learning rate: 0.00345
2024-12-21 17:56:45.786036: Validation loss did not improve from -0.42254. Patience: 68/50
2024-12-21 17:56:45.787064: train_loss -0.8268
2024-12-21 17:56:45.788025: val_loss -0.3583
2024-12-21 17:56:45.789133: Pseudo dice [0.6849]
2024-12-21 17:56:45.790375: Epoch time: 87.93 s
2024-12-21 17:56:47.373574: 
2024-12-21 17:56:47.375171: Epoch 105
2024-12-21 17:56:47.376032: Current learning rate: 0.00338
2024-12-21 17:58:15.190391: Validation loss did not improve from -0.42254. Patience: 69/50
2024-12-21 17:58:15.191493: train_loss -0.8253
2024-12-21 17:58:15.192364: val_loss -0.3772
2024-12-21 17:58:15.193175: Pseudo dice [0.6849]
2024-12-21 17:58:15.193978: Epoch time: 87.82 s
2024-12-21 17:58:16.369622: 
2024-12-21 17:58:16.371819: Epoch 106
2024-12-21 17:58:16.373246: Current learning rate: 0.00332
2024-12-21 17:59:44.281502: Validation loss did not improve from -0.42254. Patience: 70/50
2024-12-21 17:59:44.282198: train_loss -0.8259
2024-12-21 17:59:44.283226: val_loss -0.3779
2024-12-21 17:59:44.284188: Pseudo dice [0.674]
2024-12-21 17:59:44.285000: Epoch time: 87.91 s
2024-12-21 17:59:45.465613: 
2024-12-21 17:59:45.467144: Epoch 107
2024-12-21 17:59:45.468302: Current learning rate: 0.00325
2024-12-21 18:01:13.377841: Validation loss did not improve from -0.42254. Patience: 71/50
2024-12-21 18:01:13.378496: train_loss -0.8285
2024-12-21 18:01:13.379283: val_loss -0.3875
2024-12-21 18:01:13.380077: Pseudo dice [0.688]
2024-12-21 18:01:13.380848: Epoch time: 87.91 s
2024-12-21 18:01:14.673214: 
2024-12-21 18:01:14.675275: Epoch 108
2024-12-21 18:01:14.676599: Current learning rate: 0.00318
2024-12-21 18:02:42.649614: Validation loss did not improve from -0.42254. Patience: 72/50
2024-12-21 18:02:42.650405: train_loss -0.8291
2024-12-21 18:02:42.651382: val_loss -0.3519
2024-12-21 18:02:42.652003: Pseudo dice [0.6759]
2024-12-21 18:02:42.652764: Epoch time: 87.98 s
2024-12-21 18:02:43.850340: 
2024-12-21 18:02:43.852297: Epoch 109
2024-12-21 18:02:43.853566: Current learning rate: 0.00311
2024-12-21 18:04:11.770311: Validation loss did not improve from -0.42254. Patience: 73/50
2024-12-21 18:04:11.771676: train_loss -0.8278
2024-12-21 18:04:11.773006: val_loss -0.3725
2024-12-21 18:04:11.773654: Pseudo dice [0.6752]
2024-12-21 18:04:11.774321: Epoch time: 87.92 s
2024-12-21 18:04:13.382638: 
2024-12-21 18:04:13.385066: Epoch 110
2024-12-21 18:04:13.385975: Current learning rate: 0.00304
2024-12-21 18:05:41.298788: Validation loss did not improve from -0.42254. Patience: 74/50
2024-12-21 18:05:41.300056: train_loss -0.8295
2024-12-21 18:05:41.300863: val_loss -0.3562
2024-12-21 18:05:41.301578: Pseudo dice [0.6872]
2024-12-21 18:05:41.302233: Epoch time: 87.92 s
2024-12-21 18:05:42.590130: 
2024-12-21 18:05:42.591805: Epoch 111
2024-12-21 18:05:42.592804: Current learning rate: 0.00297
2024-12-21 18:07:10.506878: Validation loss did not improve from -0.42254. Patience: 75/50
2024-12-21 18:07:10.508232: train_loss -0.8311
2024-12-21 18:07:10.509654: val_loss -0.3493
2024-12-21 18:07:10.510954: Pseudo dice [0.6808]
2024-12-21 18:07:10.512008: Epoch time: 87.92 s
2024-12-21 18:07:11.726928: 
2024-12-21 18:07:11.728573: Epoch 112
2024-12-21 18:07:11.729465: Current learning rate: 0.00291
2024-12-21 18:08:39.827984: Validation loss did not improve from -0.42254. Patience: 76/50
2024-12-21 18:08:39.828974: train_loss -0.8303
2024-12-21 18:08:39.829813: val_loss -0.357
2024-12-21 18:08:39.830451: Pseudo dice [0.6785]
2024-12-21 18:08:39.831088: Epoch time: 88.1 s
2024-12-21 18:08:41.405417: 
2024-12-21 18:08:41.407033: Epoch 113
2024-12-21 18:08:41.407932: Current learning rate: 0.00284
2024-12-21 18:10:09.434235: Validation loss did not improve from -0.42254. Patience: 77/50
2024-12-21 18:10:09.435432: train_loss -0.8297
2024-12-21 18:10:09.436430: val_loss -0.3515
2024-12-21 18:10:09.437433: Pseudo dice [0.6749]
2024-12-21 18:10:09.438450: Epoch time: 88.03 s
2024-12-21 18:10:10.636289: 
2024-12-21 18:10:10.637830: Epoch 114
2024-12-21 18:10:10.638609: Current learning rate: 0.00277
2024-12-21 18:11:38.655711: Validation loss did not improve from -0.42254. Patience: 78/50
2024-12-21 18:11:38.656595: train_loss -0.8298
2024-12-21 18:11:38.657518: val_loss -0.3612
2024-12-21 18:11:38.658244: Pseudo dice [0.6775]
2024-12-21 18:11:38.659009: Epoch time: 88.02 s
2024-12-21 18:11:40.215321: 
2024-12-21 18:11:40.217059: Epoch 115
2024-12-21 18:11:40.218489: Current learning rate: 0.0027
2024-12-21 18:13:07.931451: Validation loss did not improve from -0.42254. Patience: 79/50
2024-12-21 18:13:07.932311: train_loss -0.833
2024-12-21 18:13:07.933078: val_loss -0.3332
2024-12-21 18:13:07.933875: Pseudo dice [0.6743]
2024-12-21 18:13:07.934501: Epoch time: 87.72 s
2024-12-21 18:13:09.151700: 
2024-12-21 18:13:09.153516: Epoch 116
2024-12-21 18:13:09.154616: Current learning rate: 0.00263
2024-12-21 18:14:36.911574: Validation loss did not improve from -0.42254. Patience: 80/50
2024-12-21 18:14:36.912649: train_loss -0.8334
2024-12-21 18:14:36.913853: val_loss -0.3755
2024-12-21 18:14:36.914830: Pseudo dice [0.6922]
2024-12-21 18:14:36.915433: Epoch time: 87.76 s
2024-12-21 18:14:38.156068: 
2024-12-21 18:14:38.158541: Epoch 117
2024-12-21 18:14:38.159468: Current learning rate: 0.00256
2024-12-21 18:16:06.147069: Validation loss did not improve from -0.42254. Patience: 81/50
2024-12-21 18:16:06.147986: train_loss -0.8333
2024-12-21 18:16:06.148865: val_loss -0.371
2024-12-21 18:16:06.149467: Pseudo dice [0.6961]
2024-12-21 18:16:06.150066: Epoch time: 87.99 s
2024-12-21 18:16:06.150725: Yayy! New best EMA pseudo Dice: 0.6817
2024-12-21 18:16:07.760374: 
2024-12-21 18:16:07.761650: Epoch 118
2024-12-21 18:16:07.762652: Current learning rate: 0.00249
2024-12-21 18:17:35.681310: Validation loss did not improve from -0.42254. Patience: 82/50
2024-12-21 18:17:35.682250: train_loss -0.8314
2024-12-21 18:17:35.683429: val_loss -0.3862
2024-12-21 18:17:35.684225: Pseudo dice [0.6938]
2024-12-21 18:17:35.684915: Epoch time: 87.92 s
2024-12-21 18:17:35.685582: Yayy! New best EMA pseudo Dice: 0.6829
2024-12-21 18:17:37.285516: 
2024-12-21 18:17:37.287701: Epoch 119
2024-12-21 18:17:37.288827: Current learning rate: 0.00242
2024-12-21 18:19:05.061575: Validation loss did not improve from -0.42254. Patience: 83/50
2024-12-21 18:19:05.066065: train_loss -0.8348
2024-12-21 18:19:05.066797: val_loss -0.3548
2024-12-21 18:19:05.067494: Pseudo dice [0.6747]
2024-12-21 18:19:05.068599: Epoch time: 87.78 s
2024-12-21 18:19:06.642747: 
2024-12-21 18:19:06.644284: Epoch 120
2024-12-21 18:19:06.645540: Current learning rate: 0.00235
2024-12-21 18:20:34.387133: Validation loss did not improve from -0.42254. Patience: 84/50
2024-12-21 18:20:34.388326: train_loss -0.8341
2024-12-21 18:20:34.389472: val_loss -0.3624
2024-12-21 18:20:34.390152: Pseudo dice [0.6784]
2024-12-21 18:20:34.390944: Epoch time: 87.75 s
2024-12-21 18:20:35.576097: 
2024-12-21 18:20:35.577643: Epoch 121
2024-12-21 18:20:35.578808: Current learning rate: 0.00228
2024-12-21 18:22:03.228124: Validation loss did not improve from -0.42254. Patience: 85/50
2024-12-21 18:22:03.229331: train_loss -0.8376
2024-12-21 18:22:03.230187: val_loss -0.361
2024-12-21 18:22:03.230849: Pseudo dice [0.6778]
2024-12-21 18:22:03.232099: Epoch time: 87.65 s
2024-12-21 18:22:04.461315: 
2024-12-21 18:22:04.463503: Epoch 122
2024-12-21 18:22:04.464860: Current learning rate: 0.00221
2024-12-21 18:23:32.324371: Validation loss did not improve from -0.42254. Patience: 86/50
2024-12-21 18:23:32.325639: train_loss -0.8349
2024-12-21 18:23:32.326781: val_loss -0.3847
2024-12-21 18:23:32.327873: Pseudo dice [0.689]
2024-12-21 18:23:32.328790: Epoch time: 87.87 s
2024-12-21 18:23:33.585539: 
2024-12-21 18:23:33.587555: Epoch 123
2024-12-21 18:23:33.588367: Current learning rate: 0.00214
2024-12-21 18:25:01.741500: Validation loss did not improve from -0.42254. Patience: 87/50
2024-12-21 18:25:01.742826: train_loss -0.8359
2024-12-21 18:25:01.744465: val_loss -0.3626
2024-12-21 18:25:01.745682: Pseudo dice [0.6838]
2024-12-21 18:25:01.746961: Epoch time: 88.16 s
2024-12-21 18:25:03.525633: 
2024-12-21 18:25:03.527617: Epoch 124
2024-12-21 18:25:03.528593: Current learning rate: 0.00207
2024-12-21 18:26:31.492773: Validation loss did not improve from -0.42254. Patience: 88/50
2024-12-21 18:26:31.494080: train_loss -0.8357
2024-12-21 18:26:31.495215: val_loss -0.3703
2024-12-21 18:26:31.495915: Pseudo dice [0.693]
2024-12-21 18:26:31.497000: Epoch time: 87.97 s
2024-12-21 18:26:31.857151: Yayy! New best EMA pseudo Dice: 0.6833
2024-12-21 18:26:33.388209: 
2024-12-21 18:26:33.390568: Epoch 125
2024-12-21 18:26:33.391592: Current learning rate: 0.00199
2024-12-21 18:28:01.383226: Validation loss did not improve from -0.42254. Patience: 89/50
2024-12-21 18:28:01.384367: train_loss -0.8346
2024-12-21 18:28:01.385575: val_loss -0.3405
2024-12-21 18:28:01.386420: Pseudo dice [0.6778]
2024-12-21 18:28:01.387539: Epoch time: 88.0 s
2024-12-21 18:28:02.630729: 
2024-12-21 18:28:02.633074: Epoch 126
2024-12-21 18:28:02.634505: Current learning rate: 0.00192
2024-12-21 18:29:30.788505: Validation loss did not improve from -0.42254. Patience: 90/50
2024-12-21 18:29:30.789416: train_loss -0.8369
2024-12-21 18:29:30.790169: val_loss -0.3647
2024-12-21 18:29:30.791136: Pseudo dice [0.6927]
2024-12-21 18:29:30.791924: Epoch time: 88.16 s
2024-12-21 18:29:30.793000: Yayy! New best EMA pseudo Dice: 0.6838
2024-12-21 18:29:32.354593: 
2024-12-21 18:29:32.356472: Epoch 127
2024-12-21 18:29:32.357816: Current learning rate: 0.00185
2024-12-21 18:31:00.428736: Validation loss did not improve from -0.42254. Patience: 91/50
2024-12-21 18:31:00.429424: train_loss -0.8376
2024-12-21 18:31:00.430287: val_loss -0.3259
2024-12-21 18:31:00.431146: Pseudo dice [0.6644]
2024-12-21 18:31:00.431901: Epoch time: 88.08 s
2024-12-21 18:31:01.666955: 
2024-12-21 18:31:01.669229: Epoch 128
2024-12-21 18:31:01.670406: Current learning rate: 0.00178
2024-12-21 18:32:29.797940: Validation loss did not improve from -0.42254. Patience: 92/50
2024-12-21 18:32:29.799299: train_loss -0.8378
2024-12-21 18:32:29.800479: val_loss -0.3678
2024-12-21 18:32:29.801183: Pseudo dice [0.6878]
2024-12-21 18:32:29.801943: Epoch time: 88.13 s
2024-12-21 18:32:31.056493: 
2024-12-21 18:32:31.058621: Epoch 129
2024-12-21 18:32:31.059614: Current learning rate: 0.0017
2024-12-21 18:33:59.187727: Validation loss did not improve from -0.42254. Patience: 93/50
2024-12-21 18:33:59.188977: train_loss -0.8389
2024-12-21 18:33:59.190582: val_loss -0.337
2024-12-21 18:33:59.191799: Pseudo dice [0.6685]
2024-12-21 18:33:59.193574: Epoch time: 88.13 s
2024-12-21 18:34:00.807443: 
2024-12-21 18:34:00.809002: Epoch 130
2024-12-21 18:34:00.809927: Current learning rate: 0.00163
2024-12-21 18:35:28.981698: Validation loss did not improve from -0.42254. Patience: 94/50
2024-12-21 18:35:28.982871: train_loss -0.836
2024-12-21 18:35:28.984104: val_loss -0.3493
2024-12-21 18:35:28.985388: Pseudo dice [0.6779]
2024-12-21 18:35:28.986516: Epoch time: 88.18 s
2024-12-21 18:35:30.212936: 
2024-12-21 18:35:30.214472: Epoch 131
2024-12-21 18:35:30.215431: Current learning rate: 0.00156
2024-12-21 18:36:58.536747: Validation loss did not improve from -0.42254. Patience: 95/50
2024-12-21 18:36:58.538018: train_loss -0.8372
2024-12-21 18:36:58.539209: val_loss -0.3526
2024-12-21 18:36:58.539952: Pseudo dice [0.6778]
2024-12-21 18:36:58.540704: Epoch time: 88.33 s
2024-12-21 18:36:59.807544: 
2024-12-21 18:36:59.809332: Epoch 132
2024-12-21 18:36:59.810634: Current learning rate: 0.00148
2024-12-21 18:38:28.084329: Validation loss did not improve from -0.42254. Patience: 96/50
2024-12-21 18:38:28.085425: train_loss -0.8378
2024-12-21 18:38:28.086266: val_loss -0.322
2024-12-21 18:38:28.086987: Pseudo dice [0.6537]
2024-12-21 18:38:28.087786: Epoch time: 88.28 s
2024-12-21 18:38:29.331195: 
2024-12-21 18:38:29.333278: Epoch 133
2024-12-21 18:38:29.334116: Current learning rate: 0.00141
2024-12-21 18:39:57.657209: Validation loss did not improve from -0.42254. Patience: 97/50
2024-12-21 18:39:57.658264: train_loss -0.8378
2024-12-21 18:39:57.659421: val_loss -0.368
2024-12-21 18:39:57.660367: Pseudo dice [0.6886]
2024-12-21 18:39:57.661402: Epoch time: 88.33 s
2024-12-21 18:39:58.909168: 
2024-12-21 18:39:58.911413: Epoch 134
2024-12-21 18:39:58.912695: Current learning rate: 0.00133
2024-12-21 18:41:27.203010: Validation loss did not improve from -0.42254. Patience: 98/50
2024-12-21 18:41:27.203884: train_loss -0.839
2024-12-21 18:41:27.204832: val_loss -0.3498
2024-12-21 18:41:27.206166: Pseudo dice [0.6741]
2024-12-21 18:41:27.207080: Epoch time: 88.3 s
2024-12-21 18:41:29.189472: 
2024-12-21 18:41:29.191749: Epoch 135
2024-12-21 18:41:29.193350: Current learning rate: 0.00126
2024-12-21 18:42:57.546836: Validation loss did not improve from -0.42254. Patience: 99/50
2024-12-21 18:42:57.547792: train_loss -0.838
2024-12-21 18:42:57.548857: val_loss -0.3864
2024-12-21 18:42:57.549690: Pseudo dice [0.6915]
2024-12-21 18:42:57.550549: Epoch time: 88.36 s
2024-12-21 18:42:58.794502: 
2024-12-21 18:42:58.796351: Epoch 136
2024-12-21 18:42:58.797378: Current learning rate: 0.00118
2024-12-21 18:44:27.100605: Validation loss did not improve from -0.42254. Patience: 100/50
2024-12-21 18:44:27.101868: train_loss -0.8404
2024-12-21 18:44:27.103126: val_loss -0.3555
2024-12-21 18:44:27.104262: Pseudo dice [0.6772]
2024-12-21 18:44:27.105245: Epoch time: 88.31 s
2024-12-21 18:44:28.306114: 
2024-12-21 18:44:28.307924: Epoch 137
2024-12-21 18:44:28.308942: Current learning rate: 0.00111
2024-12-21 18:45:56.593604: Validation loss did not improve from -0.42254. Patience: 101/50
2024-12-21 18:45:56.594300: train_loss -0.8406
2024-12-21 18:45:56.594959: val_loss -0.3722
2024-12-21 18:45:56.595739: Pseudo dice [0.6881]
2024-12-21 18:45:56.596397: Epoch time: 88.29 s
2024-12-21 18:45:57.839799: 
2024-12-21 18:45:57.841795: Epoch 138
2024-12-21 18:45:57.842781: Current learning rate: 0.00103
2024-12-21 18:47:26.213719: Validation loss did not improve from -0.42254. Patience: 102/50
2024-12-21 18:47:26.214419: train_loss -0.8403
2024-12-21 18:47:26.215415: val_loss -0.3628
2024-12-21 18:47:26.216454: Pseudo dice [0.6804]
2024-12-21 18:47:26.217472: Epoch time: 88.38 s
2024-12-21 18:47:27.464367: 
2024-12-21 18:47:27.466590: Epoch 139
2024-12-21 18:47:27.467787: Current learning rate: 0.00095
2024-12-21 18:48:55.851483: Validation loss did not improve from -0.42254. Patience: 103/50
2024-12-21 18:48:55.852594: train_loss -0.8403
2024-12-21 18:48:55.853831: val_loss -0.3484
2024-12-21 18:48:55.854917: Pseudo dice [0.674]
2024-12-21 18:48:55.855980: Epoch time: 88.39 s
2024-12-21 18:48:57.481955: 
2024-12-21 18:48:57.483831: Epoch 140
2024-12-21 18:48:57.484962: Current learning rate: 0.00087
2024-12-21 18:50:25.595279: Validation loss did not improve from -0.42254. Patience: 104/50
2024-12-21 18:50:25.596187: train_loss -0.8413
2024-12-21 18:50:25.597157: val_loss -0.3591
2024-12-21 18:50:25.597958: Pseudo dice [0.6805]
2024-12-21 18:50:25.598755: Epoch time: 88.12 s
2024-12-21 18:50:26.863435: 
2024-12-21 18:50:26.864902: Epoch 141
2024-12-21 18:50:26.865886: Current learning rate: 0.00079
2024-12-21 18:51:55.048235: Validation loss did not improve from -0.42254. Patience: 105/50
2024-12-21 18:51:55.049177: train_loss -0.8395
2024-12-21 18:51:55.050025: val_loss -0.3403
2024-12-21 18:51:55.050661: Pseudo dice [0.6798]
2024-12-21 18:51:55.051281: Epoch time: 88.19 s
2024-12-21 18:51:56.310804: 
2024-12-21 18:51:56.313222: Epoch 142
2024-12-21 18:51:56.314047: Current learning rate: 0.00071
2024-12-21 18:53:24.514345: Validation loss did not improve from -0.42254. Patience: 106/50
2024-12-21 18:53:24.514989: train_loss -0.8414
2024-12-21 18:53:24.515794: val_loss -0.3295
2024-12-21 18:53:24.516697: Pseudo dice [0.6799]
2024-12-21 18:53:24.517484: Epoch time: 88.21 s
2024-12-21 18:53:25.752948: 
2024-12-21 18:53:25.754771: Epoch 143
2024-12-21 18:53:25.755931: Current learning rate: 0.00063
2024-12-21 18:54:53.800407: Validation loss did not improve from -0.42254. Patience: 107/50
2024-12-21 18:54:53.801513: train_loss -0.8426
2024-12-21 18:54:53.802438: val_loss -0.3194
2024-12-21 18:54:53.803332: Pseudo dice [0.665]
2024-12-21 18:54:53.804130: Epoch time: 88.05 s
2024-12-21 18:54:55.055000: 
2024-12-21 18:54:55.057029: Epoch 144
2024-12-21 18:54:55.058093: Current learning rate: 0.00055
2024-12-21 18:56:23.183758: Validation loss did not improve from -0.42254. Patience: 108/50
2024-12-21 18:56:23.184869: train_loss -0.842
2024-12-21 18:56:23.185891: val_loss -0.3613
2024-12-21 18:56:23.186894: Pseudo dice [0.6971]
2024-12-21 18:56:23.187762: Epoch time: 88.13 s
2024-12-21 18:56:25.100169: 
2024-12-21 18:56:25.101790: Epoch 145
2024-12-21 18:56:25.102839: Current learning rate: 0.00047
2024-12-21 18:57:53.277866: Validation loss did not improve from -0.42254. Patience: 109/50
2024-12-21 18:57:53.279161: train_loss -0.8427
2024-12-21 18:57:53.280473: val_loss -0.3205
2024-12-21 18:57:53.281878: Pseudo dice [0.6634]
2024-12-21 18:57:53.283064: Epoch time: 88.18 s
2024-12-21 18:57:54.519536: 
2024-12-21 18:57:54.521323: Epoch 146
2024-12-21 18:57:54.522153: Current learning rate: 0.00038
2024-12-21 18:59:22.645559: Validation loss did not improve from -0.42254. Patience: 110/50
2024-12-21 18:59:22.646420: train_loss -0.841
2024-12-21 18:59:22.647646: val_loss -0.3559
2024-12-21 18:59:22.648541: Pseudo dice [0.6809]
2024-12-21 18:59:22.649515: Epoch time: 88.13 s
2024-12-21 18:59:23.895780: 
2024-12-21 18:59:23.897430: Epoch 147
2024-12-21 18:59:23.898689: Current learning rate: 0.0003
2024-12-21 19:00:52.041274: Validation loss did not improve from -0.42254. Patience: 111/50
2024-12-21 19:00:52.042617: train_loss -0.8427
2024-12-21 19:00:52.043899: val_loss -0.3461
2024-12-21 19:00:52.044823: Pseudo dice [0.6931]
2024-12-21 19:00:52.045489: Epoch time: 88.15 s
2024-12-21 19:00:53.319526: 
2024-12-21 19:00:53.321226: Epoch 148
2024-12-21 19:00:53.322121: Current learning rate: 0.00021
2024-12-21 19:02:21.631273: Validation loss did not improve from -0.42254. Patience: 112/50
2024-12-21 19:02:21.632263: train_loss -0.8422
2024-12-21 19:02:21.633615: val_loss -0.339
2024-12-21 19:02:21.634483: Pseudo dice [0.6742]
2024-12-21 19:02:21.635477: Epoch time: 88.31 s
2024-12-21 19:02:22.876220: 
2024-12-21 19:02:22.878295: Epoch 149
2024-12-21 19:02:22.879433: Current learning rate: 0.00011
2024-12-21 19:03:51.178114: Validation loss did not improve from -0.42254. Patience: 113/50
2024-12-21 19:03:51.178885: train_loss -0.8432
2024-12-21 19:03:51.180208: val_loss -0.3597
2024-12-21 19:03:51.181268: Pseudo dice [0.6946]
2024-12-21 19:03:51.182432: Epoch time: 88.3 s
2024-12-21 19:03:52.814953: Training done.
2024-12-21 19:03:53.044386: Using splits from existing split file: /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset308_Sohee_Calcium_OCT_CrossValidation/splits_final_20.json
2024-12-21 19:03:53.048142: The split file contains 5 splits.
2024-12-21 19:03:53.049417: Desired fold for training: 4
2024-12-21 19:03:53.050838: This split has 1 training and 7 validation cases.
2024-12-21 19:03:53.052417: predicting 101-019
2024-12-21 19:03:53.090733: 101-019, shape torch.Size([1, 375, 498, 498]), rank 0
2024-12-21 19:05:37.331360: predicting 101-044
2024-12-21 19:05:37.361388: 101-044, shape torch.Size([1, 404, 498, 498]), rank 0
2024-12-21 19:07:14.584695: predicting 101-045
2024-12-21 19:07:14.615095: 101-045, shape torch.Size([1, 375, 498, 498]), rank 0
2024-12-21 19:08:43.106241: predicting 401-004
2024-12-21 19:08:43.137612: 401-004, shape torch.Size([1, 375, 498, 498]), rank 0
2024-12-21 19:10:14.425024: predicting 701-013
2024-12-21 19:10:14.454744: 701-013, shape torch.Size([1, 375, 498, 498]), rank 0
2024-12-21 19:11:43.331956: predicting 704-003
2024-12-21 19:11:43.469892: 704-003, shape torch.Size([1, 375, 498, 498]), rank 0
2024-12-21 19:13:13.058582: predicting 706-005
2024-12-21 19:13:13.085986: 706-005, shape torch.Size([1, 375, 498, 498]), rank 0
2024-12-21 19:15:09.400462: Validation complete
2024-12-21 19:15:09.401847: Mean Validation Dice:  0.6486145373891311
