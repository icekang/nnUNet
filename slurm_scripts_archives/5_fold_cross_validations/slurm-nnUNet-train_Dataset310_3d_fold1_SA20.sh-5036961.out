/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/bin/python
Starting training with CONFIG=3d_32x160x128_b10, DATASET_ID=310, TRAINER=nnUNetTrainerScaleAnalysis20
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/nnunetv2/training/nnUNetTrainer/nnUNetTrainer.py:169: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  self.grad_scaler = GradScaler() if self.device.type == 'cuda' else None

############################
INFO: You are using the old nnU-Net default plans. We have updated our recommendations. Please consider using those instead! Read more here: https://github.com/MIC-DKFZ/nnUNet/blob/master/documentation/resenc_presets.md
############################

Using device: cuda:0

#######################################################################
Please cite the following paper when using nnU-Net:
Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.
#######################################################################

2025-10-05 12:49:44.562555: do_dummy_2d_data_aug: True
2025-10-05 12:49:44.562943: Using splits from existing split file: /nfs/erelab001/shared/Computational_Group/Naravich/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset310_nnInteractive_Calcium_OCT_CrossValidation/splits_final_20.json
2025-10-05 12:49:44.563241: The split file contains 5 splits.
2025-10-05 12:49:44.563355: Desired fold for training: 1
2025-10-05 12:49:44.563489: This split has 1 training and 7 validation cases.
using pin_memory on device 0
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/fft_conv_pytorch/fft_conv.py:139: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:306.)
  output = output[crop_slices].contiguous()
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/fft_conv_pytorch/fft_conv.py:139: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:306.)
  output = output[crop_slices].contiguous()
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/fft_conv_pytorch/fft_conv.py:139: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:306.)
  output = output[crop_slices].contiguous()
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/fft_conv_pytorch/fft_conv.py:139: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:306.)
  output = output[crop_slices].contiguous()
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/fft_conv_pytorch/fft_conv.py:139: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:306.)
  output = output[crop_slices].contiguous()
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/fft_conv_pytorch/fft_conv.py:139: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:306.)
  output = output[crop_slices].contiguous()
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/fft_conv_pytorch/fft_conv.py:139: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:306.)
  output = output[crop_slices].contiguous()
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/fft_conv_pytorch/fft_conv.py:139: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:306.)
  output = output[crop_slices].contiguous()
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/fft_conv_pytorch/fft_conv.py:139: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:306.)
  output = output[crop_slices].contiguous()
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/fft_conv_pytorch/fft_conv.py:139: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:306.)
  output = output[crop_slices].contiguous()
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/fft_conv_pytorch/fft_conv.py:139: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:306.)
  output = output[crop_slices].contiguous()
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/fft_conv_pytorch/fft_conv.py:139: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:306.)
  output = output[crop_slices].contiguous()
using pin_memory on device 0
2025-10-05 12:49:49.301028: Using torch.compile...

This is the configuration used by this training:
Configuration name: 3d_32x160x128_b10
 {'data_identifier': 'nnUNetPlans_3d_fullres', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 10, 'patch_size': [32, 160, 128], 'median_image_size_in_voxels': [375.0, 498.0, 498.0], 'spacing': [1.0, 1.0, 1.0], 'normalization_schemes': ['ZScoreNormalization'], 'use_mask_for_norm': [False], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'architecture': {'network_class_name': 'dynamic_network_architectures.architectures.unet.PlainConvUNet', 'arch_kwargs': {'n_stages': 6, 'features_per_stage': [32, 64, 128, 256, 320, 320], 'conv_op': 'torch.nn.modules.conv.Conv3d', 'kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]], 'strides': [[1, 1, 1], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [1, 2, 2]], 'n_conv_per_stage': [2, 2, 2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2, 2, 2], 'conv_bias': True, 'norm_op': 'torch.nn.modules.instancenorm.InstanceNorm3d', 'norm_op_kwargs': {'eps': 1e-05, 'affine': True}, 'dropout_op': None, 'dropout_op_kwargs': None, 'nonlin': 'torch.nn.LeakyReLU', 'nonlin_kwargs': {'inplace': True}, 'deep_supervision': True}, '_kw_requires_import': ['conv_op', 'norm_op', 'dropout_op', 'nonlin']}, 'batch_dice': True, 'inherits_from': '3d_fullres'} 

These are the global plan.json settings:
 {'dataset_name': 'Dataset310_nnInteractive_Calcium_OCT_CrossValidation', 'plans_name': 'nnUNetPlans', 'original_median_spacing_after_transp': [1.0, 1.0, 1.0], 'original_median_shape_after_transp': [375, 498, 498], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 0.4977976381778717, 'mean': 0.1394134759902954, 'median': 0.09849607944488525, 'min': 0.0, 'percentile_00_5': 0.015305490233004093, 'percentile_99_5': 0.4977976381778717, 'std': 0.121165432035923}}} 

2025-10-05 12:49:51.038822: unpacking dataset...
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/torch/_inductor/lowering.py:7095: UserWarning: 
Online softmax is disabled on the fly since Inductor decides to
split the reduction. Cut an issue to PyTorch if this is an
important use case and you want to speed it up with online
softmax.

  warnings.warn(
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/torch/_inductor/lowering.py:7095: UserWarning: 
Online softmax is disabled on the fly since Inductor decides to
split the reduction. Cut an issue to PyTorch if this is an
important use case and you want to speed it up with online
softmax.

  warnings.warn(
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/torch/_inductor/lowering.py:7095: UserWarning: 
Online softmax is disabled on the fly since Inductor decides to
split the reduction. Cut an issue to PyTorch if this is an
important use case and you want to speed it up with online
softmax.

  warnings.warn(
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/torch/_inductor/lowering.py:7095: UserWarning: 
Online softmax is disabled on the fly since Inductor decides to
split the reduction. Cut an issue to PyTorch if this is an
important use case and you want to speed it up with online
softmax.

  warnings.warn(
2025-10-05 12:49:55.197653: unpacking done...
2025-10-05 12:49:55.200022: Unable to plot network architecture: nnUNet_compile is enabled!
2025-10-05 12:49:55.207152: 
2025-10-05 12:49:55.207326: Epoch 0
2025-10-05 12:49:55.207528: Current learning rate: 0.01
2025-10-05 12:51:14.750130: Validation loss improved from 1000.00000 to 0.24104! Patience: 0/50
2025-10-05 12:51:14.750651: train_loss -0.2502
2025-10-05 12:51:14.750842: val_loss 0.241
2025-10-05 12:51:14.750988: Pseudo dice [np.float32(0.3595)]
2025-10-05 12:51:14.751173: Epoch time: 79.54 s
2025-10-05 12:51:14.751307: Yayy! New best EMA pseudo Dice: 0.359499990940094
2025-10-05 12:51:15.684878: 
2025-10-05 12:51:15.685201: Epoch 1
2025-10-05 12:51:15.685445: Current learning rate: 0.00994
2025-10-05 12:52:01.357326: Validation loss improved from 0.24104 to 0.01316! Patience: 0/50
2025-10-05 12:52:01.357953: train_loss -0.4709
2025-10-05 12:52:01.358216: val_loss 0.0132
2025-10-05 12:52:01.358433: Pseudo dice [np.float32(0.4797)]
2025-10-05 12:52:01.358617: Epoch time: 45.67 s
2025-10-05 12:52:01.358838: Yayy! New best EMA pseudo Dice: 0.3714999854564667
2025-10-05 12:52:02.477588: 
2025-10-05 12:52:02.477919: Epoch 2
2025-10-05 12:52:02.478182: Current learning rate: 0.00988
2025-10-05 12:52:48.192291: Validation loss improved from 0.01316 to -0.01651! Patience: 0/50
2025-10-05 12:52:48.192858: train_loss -0.5238
2025-10-05 12:52:48.193013: val_loss -0.0165
2025-10-05 12:52:48.193139: Pseudo dice [np.float32(0.5571)]
2025-10-05 12:52:48.193275: Epoch time: 45.72 s
2025-10-05 12:52:48.193414: Yayy! New best EMA pseudo Dice: 0.39010000228881836
2025-10-05 12:52:49.305738: 
2025-10-05 12:52:49.306008: Epoch 3
2025-10-05 12:52:49.306230: Current learning rate: 0.00982
2025-10-05 12:53:35.008680: Validation loss improved from -0.01651 to -0.07666! Patience: 0/50
2025-10-05 12:53:35.009124: train_loss -0.5841
2025-10-05 12:53:35.009272: val_loss -0.0767
2025-10-05 12:53:35.009415: Pseudo dice [np.float32(0.5525)]
2025-10-05 12:53:35.009572: Epoch time: 45.7 s
2025-10-05 12:53:35.009699: Yayy! New best EMA pseudo Dice: 0.40630000829696655
2025-10-05 12:53:36.088647: 
2025-10-05 12:53:36.088959: Epoch 4
2025-10-05 12:53:36.089152: Current learning rate: 0.00976
2025-10-05 12:54:21.821162: Validation loss did not improve from -0.07666. Patience: 1/50
2025-10-05 12:54:21.821779: train_loss -0.6478
2025-10-05 12:54:21.821994: val_loss -0.0733
2025-10-05 12:54:21.822148: Pseudo dice [np.float32(0.577)]
2025-10-05 12:54:21.822434: Epoch time: 45.73 s
2025-10-05 12:54:22.245519: Yayy! New best EMA pseudo Dice: 0.42340001463890076
2025-10-05 12:54:23.293064: 
2025-10-05 12:54:23.293479: Epoch 5
2025-10-05 12:54:23.293805: Current learning rate: 0.0097
2025-10-05 12:55:09.011560: Validation loss did not improve from -0.07666. Patience: 2/50
2025-10-05 12:55:09.012238: train_loss -0.6757
2025-10-05 12:55:09.012618: val_loss 0.0697
2025-10-05 12:55:09.012874: Pseudo dice [np.float32(0.469)]
2025-10-05 12:55:09.013119: Epoch time: 45.72 s
2025-10-05 12:55:09.013380: Yayy! New best EMA pseudo Dice: 0.4278999865055084
2025-10-05 12:55:10.184191: 
2025-10-05 12:55:10.184510: Epoch 6
2025-10-05 12:55:10.184785: Current learning rate: 0.00964
2025-10-05 12:55:55.904237: Validation loss did not improve from -0.07666. Patience: 3/50
2025-10-05 12:55:55.905117: train_loss -0.6838
2025-10-05 12:55:55.905381: val_loss -0.0628
2025-10-05 12:55:55.905598: Pseudo dice [np.float32(0.5284)]
2025-10-05 12:55:55.905823: Epoch time: 45.72 s
2025-10-05 12:55:55.906050: Yayy! New best EMA pseudo Dice: 0.43799999356269836
2025-10-05 12:55:56.987752: 
2025-10-05 12:55:56.988089: Epoch 7
2025-10-05 12:55:56.988309: Current learning rate: 0.00958
2025-10-05 12:56:42.734395: Validation loss did not improve from -0.07666. Patience: 4/50
2025-10-05 12:56:42.734868: train_loss -0.7071
2025-10-05 12:56:42.735042: val_loss 0.0112
2025-10-05 12:56:42.735242: Pseudo dice [np.float32(0.5113)]
2025-10-05 12:56:42.735405: Epoch time: 45.75 s
2025-10-05 12:56:42.735546: Yayy! New best EMA pseudo Dice: 0.44530001282691956
2025-10-05 12:56:43.806748: 
2025-10-05 12:56:43.807067: Epoch 8
2025-10-05 12:56:43.807265: Current learning rate: 0.00952
2025-10-05 12:57:29.583314: Validation loss did not improve from -0.07666. Patience: 5/50
2025-10-05 12:57:29.583865: train_loss -0.7279
2025-10-05 12:57:29.584109: val_loss -0.019
2025-10-05 12:57:29.584290: Pseudo dice [np.float32(0.5144)]
2025-10-05 12:57:29.584481: Epoch time: 45.78 s
2025-10-05 12:57:29.584657: Yayy! New best EMA pseudo Dice: 0.4521999955177307
2025-10-05 12:57:30.653432: 
2025-10-05 12:57:30.653816: Epoch 9
2025-10-05 12:57:30.654108: Current learning rate: 0.00946
2025-10-05 12:58:16.436802: Validation loss did not improve from -0.07666. Patience: 6/50
2025-10-05 12:58:16.437247: train_loss -0.7478
2025-10-05 12:58:16.437403: val_loss 0.0118
2025-10-05 12:58:16.437581: Pseudo dice [np.float32(0.5371)]
2025-10-05 12:58:16.437729: Epoch time: 45.78 s
2025-10-05 12:58:16.978049: Yayy! New best EMA pseudo Dice: 0.46070000529289246
2025-10-05 12:58:18.106169: 
2025-10-05 12:58:18.106435: Epoch 10
2025-10-05 12:58:18.106668: Current learning rate: 0.0094
2025-10-05 12:59:03.872654: Validation loss improved from -0.07666 to -0.11245! Patience: 6/50
2025-10-05 12:59:03.873474: train_loss -0.7623
2025-10-05 12:59:03.873673: val_loss -0.1124
2025-10-05 12:59:03.873832: Pseudo dice [np.float32(0.5771)]
2025-10-05 12:59:03.873972: Epoch time: 45.77 s
2025-10-05 12:59:03.874165: Yayy! New best EMA pseudo Dice: 0.4724000096321106
2025-10-05 12:59:04.958250: 
2025-10-05 12:59:04.958547: Epoch 11
2025-10-05 12:59:04.958751: Current learning rate: 0.00934
2025-10-05 12:59:50.720986: Validation loss did not improve from -0.11245. Patience: 1/50
2025-10-05 12:59:50.721471: train_loss -0.7819
2025-10-05 12:59:50.721672: val_loss 0.0159
2025-10-05 12:59:50.721806: Pseudo dice [np.float32(0.5483)]
2025-10-05 12:59:50.721977: Epoch time: 45.76 s
2025-10-05 12:59:50.722131: Yayy! New best EMA pseudo Dice: 0.4799000024795532
2025-10-05 12:59:52.168121: 
2025-10-05 12:59:52.168389: Epoch 12
2025-10-05 12:59:52.168574: Current learning rate: 0.00928
2025-10-05 13:00:38.006038: Validation loss did not improve from -0.11245. Patience: 2/50
2025-10-05 13:00:38.006773: train_loss -0.7836
2025-10-05 13:00:38.006989: val_loss -0.0315
2025-10-05 13:00:38.007136: Pseudo dice [np.float32(0.5637)]
2025-10-05 13:00:38.007367: Epoch time: 45.84 s
2025-10-05 13:00:38.007630: Yayy! New best EMA pseudo Dice: 0.48829999566078186
2025-10-05 13:00:39.118310: 
2025-10-05 13:00:39.118760: Epoch 13
2025-10-05 13:00:39.119032: Current learning rate: 0.00922
2025-10-05 13:01:24.939209: Validation loss did not improve from -0.11245. Patience: 3/50
2025-10-05 13:01:24.939639: train_loss -0.8016
2025-10-05 13:01:24.939830: val_loss 0.0201
2025-10-05 13:01:24.940011: Pseudo dice [np.float32(0.5196)]
2025-10-05 13:01:24.940191: Epoch time: 45.82 s
2025-10-05 13:01:24.940354: Yayy! New best EMA pseudo Dice: 0.49140000343322754
2025-10-05 13:01:26.030234: 
2025-10-05 13:01:26.030614: Epoch 14
2025-10-05 13:01:26.030837: Current learning rate: 0.00916
2025-10-05 13:02:11.821280: Validation loss did not improve from -0.11245. Patience: 4/50
2025-10-05 13:02:11.821914: train_loss -0.815
2025-10-05 13:02:11.822084: val_loss 0.0304
2025-10-05 13:02:11.822222: Pseudo dice [np.float32(0.5623)]
2025-10-05 13:02:11.822384: Epoch time: 45.79 s
2025-10-05 13:02:12.297462: Yayy! New best EMA pseudo Dice: 0.4984999895095825
2025-10-05 13:02:13.362068: 
2025-10-05 13:02:13.362765: Epoch 15
2025-10-05 13:02:13.363029: Current learning rate: 0.0091
2025-10-05 13:02:59.110600: Validation loss did not improve from -0.11245. Patience: 5/50
2025-10-05 13:02:59.111101: train_loss -0.8189
2025-10-05 13:02:59.111306: val_loss 0.0496
2025-10-05 13:02:59.111457: Pseudo dice [np.float32(0.5256)]
2025-10-05 13:02:59.111653: Epoch time: 45.75 s
2025-10-05 13:02:59.111903: Yayy! New best EMA pseudo Dice: 0.5012000203132629
2025-10-05 13:03:00.255998: 
2025-10-05 13:03:00.256451: Epoch 16
2025-10-05 13:03:00.256793: Current learning rate: 0.00903
2025-10-05 13:03:46.067750: Validation loss did not improve from -0.11245. Patience: 6/50
2025-10-05 13:03:46.068940: train_loss -0.8222
2025-10-05 13:03:46.069262: val_loss 0.0199
2025-10-05 13:03:46.069401: Pseudo dice [np.float32(0.5401)]
2025-10-05 13:03:46.069675: Epoch time: 45.81 s
2025-10-05 13:03:46.069964: Yayy! New best EMA pseudo Dice: 0.5051000118255615
2025-10-05 13:03:47.193174: 
2025-10-05 13:03:47.193481: Epoch 17
2025-10-05 13:03:47.193678: Current learning rate: 0.00897
2025-10-05 13:04:32.958195: Validation loss did not improve from -0.11245. Patience: 7/50
2025-10-05 13:04:32.958659: train_loss -0.8313
2025-10-05 13:04:32.958838: val_loss 0.0489
2025-10-05 13:04:32.958990: Pseudo dice [np.float32(0.542)]
2025-10-05 13:04:32.959129: Epoch time: 45.77 s
2025-10-05 13:04:32.959301: Yayy! New best EMA pseudo Dice: 0.5088000297546387
2025-10-05 13:04:34.057377: 
2025-10-05 13:04:34.057671: Epoch 18
2025-10-05 13:04:34.057899: Current learning rate: 0.00891
2025-10-05 13:05:19.843395: Validation loss did not improve from -0.11245. Patience: 8/50
2025-10-05 13:05:19.843952: train_loss -0.837
2025-10-05 13:05:19.844103: val_loss 0.0661
2025-10-05 13:05:19.844262: Pseudo dice [np.float32(0.5329)]
2025-10-05 13:05:19.844399: Epoch time: 45.79 s
2025-10-05 13:05:19.844542: Yayy! New best EMA pseudo Dice: 0.5112000107765198
2025-10-05 13:05:20.969903: 
2025-10-05 13:05:20.970210: Epoch 19
2025-10-05 13:05:20.970456: Current learning rate: 0.00885
2025-10-05 13:06:06.766716: Validation loss did not improve from -0.11245. Patience: 9/50
2025-10-05 13:06:06.767114: train_loss -0.8497
2025-10-05 13:06:06.767279: val_loss 0.0791
2025-10-05 13:06:06.767407: Pseudo dice [np.float32(0.5273)]
2025-10-05 13:06:06.767549: Epoch time: 45.8 s
2025-10-05 13:06:07.239552: Yayy! New best EMA pseudo Dice: 0.5127999782562256
2025-10-05 13:06:08.337510: 
2025-10-05 13:06:08.337830: Epoch 20
2025-10-05 13:06:08.338014: Current learning rate: 0.00879
2025-10-05 13:06:54.152114: Validation loss did not improve from -0.11245. Patience: 10/50
2025-10-05 13:06:54.152875: train_loss -0.8542
2025-10-05 13:06:54.153062: val_loss 0.1871
2025-10-05 13:06:54.153296: Pseudo dice [np.float32(0.4704)]
2025-10-05 13:06:54.153484: Epoch time: 45.82 s
2025-10-05 13:06:54.789473: 
2025-10-05 13:06:54.789824: Epoch 21
2025-10-05 13:06:54.790072: Current learning rate: 0.00873
2025-10-05 13:07:40.567978: Validation loss did not improve from -0.11245. Patience: 11/50
2025-10-05 13:07:40.568453: train_loss -0.8499
2025-10-05 13:07:40.568712: val_loss 0.0669
2025-10-05 13:07:40.568960: Pseudo dice [np.float32(0.5508)]
2025-10-05 13:07:40.569227: Epoch time: 45.78 s
2025-10-05 13:07:41.183573: 
2025-10-05 13:07:41.183933: Epoch 22
2025-10-05 13:07:41.184207: Current learning rate: 0.00867
2025-10-05 13:08:26.976799: Validation loss did not improve from -0.11245. Patience: 12/50
2025-10-05 13:08:26.977492: train_loss -0.8562
2025-10-05 13:08:26.977656: val_loss 0.1252
2025-10-05 13:08:26.977815: Pseudo dice [np.float32(0.5231)]
2025-10-05 13:08:26.977977: Epoch time: 45.79 s
2025-10-05 13:08:26.978107: Yayy! New best EMA pseudo Dice: 0.5138000249862671
2025-10-05 13:08:28.067085: 
2025-10-05 13:08:28.067444: Epoch 23
2025-10-05 13:08:28.067695: Current learning rate: 0.00861
2025-10-05 13:09:13.870206: Validation loss did not improve from -0.11245. Patience: 13/50
2025-10-05 13:09:13.870614: train_loss -0.8599
2025-10-05 13:09:13.870794: val_loss 0.1431
2025-10-05 13:09:13.870960: Pseudo dice [np.float32(0.5406)]
2025-10-05 13:09:13.871101: Epoch time: 45.8 s
2025-10-05 13:09:13.871255: Yayy! New best EMA pseudo Dice: 0.5164999961853027
2025-10-05 13:09:14.957613: 
2025-10-05 13:09:14.957944: Epoch 24
2025-10-05 13:09:14.958179: Current learning rate: 0.00855
2025-10-05 13:10:00.777205: Validation loss did not improve from -0.11245. Patience: 14/50
2025-10-05 13:10:00.777901: train_loss -0.8689
2025-10-05 13:10:00.778081: val_loss 0.0017
2025-10-05 13:10:00.778217: Pseudo dice [np.float32(0.5815)]
2025-10-05 13:10:00.778378: Epoch time: 45.82 s
2025-10-05 13:10:01.250807: Yayy! New best EMA pseudo Dice: 0.5230000019073486
2025-10-05 13:10:02.320795: 
2025-10-05 13:10:02.321056: Epoch 25
2025-10-05 13:10:02.321260: Current learning rate: 0.00849
2025-10-05 13:10:48.159578: Validation loss did not improve from -0.11245. Patience: 15/50
2025-10-05 13:10:48.160170: train_loss -0.8717
2025-10-05 13:10:48.160390: val_loss 0.2448
2025-10-05 13:10:48.160579: Pseudo dice [np.float32(0.4549)]
2025-10-05 13:10:48.160839: Epoch time: 45.84 s
2025-10-05 13:10:48.786863: 
2025-10-05 13:10:48.787189: Epoch 26
2025-10-05 13:10:48.787427: Current learning rate: 0.00843
2025-10-05 13:11:34.635566: Validation loss did not improve from -0.11245. Patience: 16/50
2025-10-05 13:11:34.636183: train_loss -0.8701
2025-10-05 13:11:34.636572: val_loss 0.1458
2025-10-05 13:11:34.636706: Pseudo dice [np.float32(0.5118)]
2025-10-05 13:11:34.636858: Epoch time: 45.85 s
2025-10-05 13:11:35.265989: 
2025-10-05 13:11:35.266308: Epoch 27
2025-10-05 13:11:35.266490: Current learning rate: 0.00836
2025-10-05 13:12:21.070931: Validation loss did not improve from -0.11245. Patience: 17/50
2025-10-05 13:12:21.071355: train_loss -0.8795
2025-10-05 13:12:21.071520: val_loss 0.0797
2025-10-05 13:12:21.071662: Pseudo dice [np.float32(0.5409)]
2025-10-05 13:12:21.071807: Epoch time: 45.81 s
2025-10-05 13:12:22.169198: 
2025-10-05 13:12:22.169732: Epoch 28
2025-10-05 13:12:22.169931: Current learning rate: 0.0083
2025-10-05 13:13:08.006200: Validation loss did not improve from -0.11245. Patience: 18/50
2025-10-05 13:13:08.006876: train_loss -0.8801
2025-10-05 13:13:08.007057: val_loss 0.0769
2025-10-05 13:13:08.007258: Pseudo dice [np.float32(0.5588)]
2025-10-05 13:13:08.007503: Epoch time: 45.84 s
2025-10-05 13:13:08.638260: 
2025-10-05 13:13:08.638726: Epoch 29
2025-10-05 13:13:08.639035: Current learning rate: 0.00824
2025-10-05 13:13:54.506967: Validation loss did not improve from -0.11245. Patience: 19/50
2025-10-05 13:13:54.507420: train_loss -0.8759
2025-10-05 13:13:54.507605: val_loss 0.0935
2025-10-05 13:13:54.507733: Pseudo dice [np.float32(0.5403)]
2025-10-05 13:13:54.507886: Epoch time: 45.87 s
2025-10-05 13:13:54.945122: Yayy! New best EMA pseudo Dice: 0.5241000056266785
2025-10-05 13:13:56.018625: 
2025-10-05 13:13:56.018979: Epoch 30
2025-10-05 13:13:56.019224: Current learning rate: 0.00818
2025-10-05 13:14:41.866845: Validation loss did not improve from -0.11245. Patience: 20/50
2025-10-05 13:14:41.867661: train_loss -0.8867
2025-10-05 13:14:41.867934: val_loss 0.1392
2025-10-05 13:14:41.868117: Pseudo dice [np.float32(0.5191)]
2025-10-05 13:14:41.868369: Epoch time: 45.85 s
2025-10-05 13:14:42.512360: 
2025-10-05 13:14:42.512686: Epoch 31
2025-10-05 13:14:42.512886: Current learning rate: 0.00812
2025-10-05 13:15:28.425559: Validation loss did not improve from -0.11245. Patience: 21/50
2025-10-05 13:15:28.426047: train_loss -0.8852
2025-10-05 13:15:28.426314: val_loss 0.1748
2025-10-05 13:15:28.426471: Pseudo dice [np.float32(0.5018)]
2025-10-05 13:15:28.426619: Epoch time: 45.91 s
2025-10-05 13:15:29.061981: 
2025-10-05 13:15:29.062199: Epoch 32
2025-10-05 13:15:29.062374: Current learning rate: 0.00806
2025-10-05 13:16:14.931286: Validation loss did not improve from -0.11245. Patience: 22/50
2025-10-05 13:16:14.932157: train_loss -0.8873
2025-10-05 13:16:14.932685: val_loss 0.0959
2025-10-05 13:16:14.933288: Pseudo dice [np.float32(0.529)]
2025-10-05 13:16:14.933718: Epoch time: 45.87 s
2025-10-05 13:16:15.578157: 
2025-10-05 13:16:15.578627: Epoch 33
2025-10-05 13:16:15.578982: Current learning rate: 0.008
2025-10-05 13:17:01.484850: Validation loss did not improve from -0.11245. Patience: 23/50
2025-10-05 13:17:01.485266: train_loss -0.8905
2025-10-05 13:17:01.485431: val_loss 0.0841
2025-10-05 13:17:01.485611: Pseudo dice [np.float32(0.5645)]
2025-10-05 13:17:01.485781: Epoch time: 45.91 s
2025-10-05 13:17:01.485921: Yayy! New best EMA pseudo Dice: 0.5264000296592712
2025-10-05 13:17:02.597386: 
2025-10-05 13:17:02.597634: Epoch 34
2025-10-05 13:17:02.597885: Current learning rate: 0.00793
2025-10-05 13:17:48.497026: Validation loss did not improve from -0.11245. Patience: 24/50
2025-10-05 13:17:48.497865: train_loss -0.8929
2025-10-05 13:17:48.498055: val_loss 0.202
2025-10-05 13:17:48.498204: Pseudo dice [np.float32(0.4907)]
2025-10-05 13:17:48.498358: Epoch time: 45.9 s
2025-10-05 13:17:49.558578: 
2025-10-05 13:17:49.558903: Epoch 35
2025-10-05 13:17:49.559087: Current learning rate: 0.00787
2025-10-05 13:18:35.432641: Validation loss did not improve from -0.11245. Patience: 25/50
2025-10-05 13:18:35.433112: train_loss -0.8965
2025-10-05 13:18:35.433298: val_loss 0.1159
2025-10-05 13:18:35.433482: Pseudo dice [np.float32(0.563)]
2025-10-05 13:18:35.433630: Epoch time: 45.88 s
2025-10-05 13:18:35.433763: Yayy! New best EMA pseudo Dice: 0.5268999934196472
2025-10-05 13:18:36.512387: 
2025-10-05 13:18:36.512722: Epoch 36
2025-10-05 13:18:36.512931: Current learning rate: 0.00781
2025-10-05 13:19:22.353356: Validation loss did not improve from -0.11245. Patience: 26/50
2025-10-05 13:19:22.354044: train_loss -0.9044
2025-10-05 13:19:22.354286: val_loss 0.1083
2025-10-05 13:19:22.354514: Pseudo dice [np.float32(0.5309)]
2025-10-05 13:19:22.354739: Epoch time: 45.84 s
2025-10-05 13:19:22.354867: Yayy! New best EMA pseudo Dice: 0.5273000001907349
2025-10-05 13:19:23.528990: 
2025-10-05 13:19:23.529252: Epoch 37
2025-10-05 13:19:23.529521: Current learning rate: 0.00775
2025-10-05 13:20:09.411651: Validation loss did not improve from -0.11245. Patience: 27/50
2025-10-05 13:20:09.412079: train_loss -0.9053
2025-10-05 13:20:09.412243: val_loss 0.17
2025-10-05 13:20:09.412392: Pseudo dice [np.float32(0.5257)]
2025-10-05 13:20:09.412590: Epoch time: 45.88 s
2025-10-05 13:20:10.048207: 
2025-10-05 13:20:10.048661: Epoch 38
2025-10-05 13:20:10.049002: Current learning rate: 0.00769
2025-10-05 13:20:55.925384: Validation loss did not improve from -0.11245. Patience: 28/50
2025-10-05 13:20:55.925947: train_loss -0.9074
2025-10-05 13:20:55.926154: val_loss 0.1494
2025-10-05 13:20:55.926345: Pseudo dice [np.float32(0.5444)]
2025-10-05 13:20:55.926523: Epoch time: 45.88 s
2025-10-05 13:20:55.926657: Yayy! New best EMA pseudo Dice: 0.5288000106811523
2025-10-05 13:20:57.024455: 
2025-10-05 13:20:57.024853: Epoch 39
2025-10-05 13:20:57.025125: Current learning rate: 0.00763
2025-10-05 13:21:42.904060: Validation loss did not improve from -0.11245. Patience: 29/50
2025-10-05 13:21:42.904518: train_loss -0.9008
2025-10-05 13:21:42.904696: val_loss 0.1562
2025-10-05 13:21:42.904890: Pseudo dice [np.float32(0.5554)]
2025-10-05 13:21:42.905035: Epoch time: 45.88 s
2025-10-05 13:21:43.350374: Yayy! New best EMA pseudo Dice: 0.531499981880188
2025-10-05 13:21:44.439497: 
2025-10-05 13:21:44.439844: Epoch 40
2025-10-05 13:21:44.440133: Current learning rate: 0.00756
2025-10-05 13:22:30.228401: Validation loss did not improve from -0.11245. Patience: 30/50
2025-10-05 13:22:30.229050: train_loss -0.9046
2025-10-05 13:22:30.229244: val_loss 0.1687
2025-10-05 13:22:30.229483: Pseudo dice [np.float32(0.5265)]
2025-10-05 13:22:30.229664: Epoch time: 45.79 s
2025-10-05 13:22:30.868493: 
2025-10-05 13:22:30.868850: Epoch 41
2025-10-05 13:22:30.869075: Current learning rate: 0.0075
2025-10-05 13:23:16.642466: Validation loss did not improve from -0.11245. Patience: 31/50
2025-10-05 13:23:16.642865: train_loss -0.9087
2025-10-05 13:23:16.643091: val_loss 0.1274
2025-10-05 13:23:16.643232: Pseudo dice [np.float32(0.5623)]
2025-10-05 13:23:16.643536: Epoch time: 45.78 s
2025-10-05 13:23:16.643775: Yayy! New best EMA pseudo Dice: 0.5340999960899353
2025-10-05 13:23:17.720490: 
2025-10-05 13:23:17.720830: Epoch 42
2025-10-05 13:23:17.721136: Current learning rate: 0.00744
2025-10-05 13:24:03.438303: Validation loss did not improve from -0.11245. Patience: 32/50
2025-10-05 13:24:03.439039: train_loss -0.9139
2025-10-05 13:24:03.439217: val_loss 0.1626
2025-10-05 13:24:03.439365: Pseudo dice [np.float32(0.5674)]
2025-10-05 13:24:03.439536: Epoch time: 45.72 s
2025-10-05 13:24:03.439672: Yayy! New best EMA pseudo Dice: 0.5375000238418579
2025-10-05 13:24:04.540289: 
2025-10-05 13:24:04.540599: Epoch 43
2025-10-05 13:24:04.540861: Current learning rate: 0.00738
2025-10-05 13:24:50.291518: Validation loss did not improve from -0.11245. Patience: 33/50
2025-10-05 13:24:50.292064: train_loss -0.9115
2025-10-05 13:24:50.292338: val_loss 0.1077
2025-10-05 13:24:50.292613: Pseudo dice [np.float32(0.5542)]
2025-10-05 13:24:50.292867: Epoch time: 45.75 s
2025-10-05 13:24:50.293059: Yayy! New best EMA pseudo Dice: 0.5390999913215637
2025-10-05 13:24:52.006627: 
2025-10-05 13:24:52.006926: Epoch 44
2025-10-05 13:24:52.007128: Current learning rate: 0.00732
2025-10-05 13:25:37.790810: Validation loss did not improve from -0.11245. Patience: 34/50
2025-10-05 13:25:37.791552: train_loss -0.9178
2025-10-05 13:25:37.791758: val_loss 0.1992
2025-10-05 13:25:37.791923: Pseudo dice [np.float32(0.55)]
2025-10-05 13:25:37.792087: Epoch time: 45.79 s
2025-10-05 13:25:38.247998: Yayy! New best EMA pseudo Dice: 0.5401999950408936
2025-10-05 13:25:39.299042: 
2025-10-05 13:25:39.299375: Epoch 45
2025-10-05 13:25:39.299588: Current learning rate: 0.00725
2025-10-05 13:26:25.098472: Validation loss did not improve from -0.11245. Patience: 35/50
2025-10-05 13:26:25.098936: train_loss -0.9198
2025-10-05 13:26:25.099130: val_loss 0.1352
2025-10-05 13:26:25.099257: Pseudo dice [np.float32(0.557)]
2025-10-05 13:26:25.099411: Epoch time: 45.8 s
2025-10-05 13:26:25.099533: Yayy! New best EMA pseudo Dice: 0.5418999791145325
2025-10-05 13:26:26.184911: 
2025-10-05 13:26:26.185260: Epoch 46
2025-10-05 13:26:26.185507: Current learning rate: 0.00719
2025-10-05 13:27:11.969169: Validation loss did not improve from -0.11245. Patience: 36/50
2025-10-05 13:27:11.969805: train_loss -0.9177
2025-10-05 13:27:11.969980: val_loss 0.1286
2025-10-05 13:27:11.970108: Pseudo dice [np.float32(0.5756)]
2025-10-05 13:27:11.970249: Epoch time: 45.79 s
2025-10-05 13:27:11.970450: Yayy! New best EMA pseudo Dice: 0.5453000068664551
2025-10-05 13:27:13.037761: 
2025-10-05 13:27:13.038127: Epoch 47
2025-10-05 13:27:13.038353: Current learning rate: 0.00713
2025-10-05 13:27:58.805475: Validation loss did not improve from -0.11245. Patience: 37/50
2025-10-05 13:27:58.806051: train_loss -0.9151
2025-10-05 13:27:58.806409: val_loss 0.1721
2025-10-05 13:27:58.806699: Pseudo dice [np.float32(0.5344)]
2025-10-05 13:27:58.806948: Epoch time: 45.77 s
2025-10-05 13:27:59.436229: 
2025-10-05 13:27:59.436536: Epoch 48
2025-10-05 13:27:59.436781: Current learning rate: 0.00707
2025-10-05 13:28:45.285344: Validation loss did not improve from -0.11245. Patience: 38/50
2025-10-05 13:28:45.286111: train_loss -0.9132
2025-10-05 13:28:45.286352: val_loss 0.1429
2025-10-05 13:28:45.286553: Pseudo dice [np.float32(0.5601)]
2025-10-05 13:28:45.286776: Epoch time: 45.85 s
2025-10-05 13:28:45.286976: Yayy! New best EMA pseudo Dice: 0.545799970626831
2025-10-05 13:28:46.453382: 
2025-10-05 13:28:46.453848: Epoch 49
2025-10-05 13:28:46.454166: Current learning rate: 0.007
2025-10-05 13:29:32.198521: Validation loss did not improve from -0.11245. Patience: 39/50
2025-10-05 13:29:32.199028: train_loss -0.9171
2025-10-05 13:29:32.199226: val_loss 0.1792
2025-10-05 13:29:32.199386: Pseudo dice [np.float32(0.5575)]
2025-10-05 13:29:32.199601: Epoch time: 45.75 s
2025-10-05 13:29:32.644669: Yayy! New best EMA pseudo Dice: 0.546999990940094
2025-10-05 13:29:33.707601: 
2025-10-05 13:29:33.707978: Epoch 50
2025-10-05 13:29:33.708220: Current learning rate: 0.00694
2025-10-05 13:30:19.559886: Validation loss did not improve from -0.11245. Patience: 40/50
2025-10-05 13:30:19.560570: train_loss -0.9208
2025-10-05 13:30:19.560760: val_loss 0.1632
2025-10-05 13:30:19.560915: Pseudo dice [np.float32(0.5524)]
2025-10-05 13:30:19.561113: Epoch time: 45.85 s
2025-10-05 13:30:19.561258: Yayy! New best EMA pseudo Dice: 0.5475000143051147
2025-10-05 13:30:20.649426: 
2025-10-05 13:30:20.649722: Epoch 51
2025-10-05 13:30:20.649998: Current learning rate: 0.00688
2025-10-05 13:31:06.454163: Validation loss did not improve from -0.11245. Patience: 41/50
2025-10-05 13:31:06.454757: train_loss -0.9171
2025-10-05 13:31:06.455106: val_loss 0.2161
2025-10-05 13:31:06.455384: Pseudo dice [np.float32(0.5451)]
2025-10-05 13:31:06.455864: Epoch time: 45.81 s
2025-10-05 13:31:07.089531: 
2025-10-05 13:31:07.089921: Epoch 52
2025-10-05 13:31:07.090135: Current learning rate: 0.00682
2025-10-05 13:31:52.890582: Validation loss did not improve from -0.11245. Patience: 42/50
2025-10-05 13:31:52.891319: train_loss -0.9209
2025-10-05 13:31:52.891519: val_loss 0.1392
2025-10-05 13:31:52.891685: Pseudo dice [np.float32(0.5467)]
2025-10-05 13:31:52.891871: Epoch time: 45.8 s
2025-10-05 13:31:53.519083: 
2025-10-05 13:31:53.519415: Epoch 53
2025-10-05 13:31:53.519664: Current learning rate: 0.00675
2025-10-05 13:32:39.335912: Validation loss did not improve from -0.11245. Patience: 43/50
2025-10-05 13:32:39.336343: train_loss -0.9202
2025-10-05 13:32:39.336512: val_loss 0.1064
2025-10-05 13:32:39.336674: Pseudo dice [np.float32(0.5667)]
2025-10-05 13:32:39.336830: Epoch time: 45.82 s
2025-10-05 13:32:39.336965: Yayy! New best EMA pseudo Dice: 0.5491999983787537
2025-10-05 13:32:40.422689: 
2025-10-05 13:32:40.422987: Epoch 54
2025-10-05 13:32:40.423234: Current learning rate: 0.00669
2025-10-05 13:33:26.237709: Validation loss did not improve from -0.11245. Patience: 44/50
2025-10-05 13:33:26.238378: train_loss -0.9219
2025-10-05 13:33:26.238557: val_loss 0.1554
2025-10-05 13:33:26.238706: Pseudo dice [np.float32(0.5681)]
2025-10-05 13:33:26.238860: Epoch time: 45.82 s
2025-10-05 13:33:26.702106: Yayy! New best EMA pseudo Dice: 0.5511000156402588
2025-10-05 13:33:27.784405: 
2025-10-05 13:33:27.784700: Epoch 55
2025-10-05 13:33:27.784902: Current learning rate: 0.00663
2025-10-05 13:34:13.615555: Validation loss did not improve from -0.11245. Patience: 45/50
2025-10-05 13:34:13.616164: train_loss -0.9246
2025-10-05 13:34:13.616406: val_loss 0.2163
2025-10-05 13:34:13.616564: Pseudo dice [np.float32(0.5117)]
2025-10-05 13:34:13.616772: Epoch time: 45.83 s
2025-10-05 13:34:14.252688: 
2025-10-05 13:34:14.252994: Epoch 56
2025-10-05 13:34:14.253307: Current learning rate: 0.00657
2025-10-05 13:35:00.091571: Validation loss did not improve from -0.11245. Patience: 46/50
2025-10-05 13:35:00.092225: train_loss -0.9259
2025-10-05 13:35:00.092417: val_loss 0.2032
2025-10-05 13:35:00.092544: Pseudo dice [np.float32(0.5595)]
2025-10-05 13:35:00.092679: Epoch time: 45.84 s
2025-10-05 13:35:00.726859: 
2025-10-05 13:35:00.727127: Epoch 57
2025-10-05 13:35:00.727310: Current learning rate: 0.0065
2025-10-05 13:35:46.599952: Validation loss did not improve from -0.11245. Patience: 47/50
2025-10-05 13:35:46.600363: train_loss -0.9252
2025-10-05 13:35:46.600520: val_loss 0.1482
2025-10-05 13:35:46.600665: Pseudo dice [np.float32(0.5886)]
2025-10-05 13:35:46.600840: Epoch time: 45.87 s
2025-10-05 13:35:46.600992: Yayy! New best EMA pseudo Dice: 0.5523999929428101
2025-10-05 13:35:47.694012: 
2025-10-05 13:35:47.694326: Epoch 58
2025-10-05 13:35:47.694533: Current learning rate: 0.00644
2025-10-05 13:36:33.536409: Validation loss did not improve from -0.11245. Patience: 48/50
2025-10-05 13:36:33.537111: train_loss -0.9265
2025-10-05 13:36:33.537278: val_loss 0.276
2025-10-05 13:36:33.537441: Pseudo dice [np.float32(0.5078)]
2025-10-05 13:36:33.537600: Epoch time: 45.84 s
2025-10-05 13:36:34.176634: 
2025-10-05 13:36:34.176995: Epoch 59
2025-10-05 13:36:34.177211: Current learning rate: 0.00638
2025-10-05 13:37:20.029426: Validation loss did not improve from -0.11245. Patience: 49/50
2025-10-05 13:37:20.029825: train_loss -0.9303
2025-10-05 13:37:20.030028: val_loss 0.1505
2025-10-05 13:37:20.030249: Pseudo dice [np.float32(0.5653)]
2025-10-05 13:37:20.030440: Epoch time: 45.85 s
2025-10-05 13:37:21.635352: 
2025-10-05 13:37:21.635762: Epoch 60
2025-10-05 13:37:21.636085: Current learning rate: 0.00631
2025-10-05 13:38:07.492780: Validation loss did not improve from -0.11245. Patience: 50/50
2025-10-05 13:38:07.493688: train_loss -0.9314
2025-10-05 13:38:07.493916: val_loss 0.2588
2025-10-05 13:38:07.494134: Pseudo dice [np.float32(0.5212)]
2025-10-05 13:38:07.494362: Epoch time: 45.86 s
2025-10-05 13:38:08.130731: 
2025-10-05 13:38:08.131124: Epoch 61
2025-10-05 13:38:08.131314: Current learning rate: 0.00625
2025-10-05 13:38:54.010133: Validation loss did not improve from -0.11245. Patience: 51/50
2025-10-05 13:38:54.010600: train_loss -0.9299
2025-10-05 13:38:54.010780: val_loss 0.2566
2025-10-05 13:38:54.010998: Pseudo dice [np.float32(0.5399)]
2025-10-05 13:38:54.011235: Epoch time: 45.88 s
2025-10-05 13:38:54.648790: 
2025-10-05 13:38:54.649083: Epoch 62
2025-10-05 13:38:54.649296: Current learning rate: 0.00619
2025-10-05 13:39:40.515588: Validation loss did not improve from -0.11245. Patience: 52/50
2025-10-05 13:39:40.516265: train_loss -0.9296
2025-10-05 13:39:40.516430: val_loss 0.1463
2025-10-05 13:39:40.516558: Pseudo dice [np.float32(0.5528)]
2025-10-05 13:39:40.516694: Epoch time: 45.87 s
2025-10-05 13:39:41.157651: 
2025-10-05 13:39:41.158016: Epoch 63
2025-10-05 13:39:41.158219: Current learning rate: 0.00612
2025-10-05 13:40:27.029344: Validation loss did not improve from -0.11245. Patience: 53/50
2025-10-05 13:40:27.029964: train_loss -0.9307
2025-10-05 13:40:27.030206: val_loss 0.2472
2025-10-05 13:40:27.030378: Pseudo dice [np.float32(0.5264)]
2025-10-05 13:40:27.030558: Epoch time: 45.87 s
2025-10-05 13:40:27.688171: 
2025-10-05 13:40:27.688413: Epoch 64
2025-10-05 13:40:27.688594: Current learning rate: 0.00606
2025-10-05 13:41:13.538481: Validation loss did not improve from -0.11245. Patience: 54/50
2025-10-05 13:41:13.539090: train_loss -0.9315
2025-10-05 13:41:13.539241: val_loss 0.2292
2025-10-05 13:41:13.539363: Pseudo dice [np.float32(0.5609)]
2025-10-05 13:41:13.539495: Epoch time: 45.85 s
2025-10-05 13:41:14.625751: 
2025-10-05 13:41:14.626104: Epoch 65
2025-10-05 13:41:14.626321: Current learning rate: 0.006
2025-10-05 13:42:00.526328: Validation loss did not improve from -0.11245. Patience: 55/50
2025-10-05 13:42:00.526795: train_loss -0.9332
2025-10-05 13:42:00.526994: val_loss 0.2468
2025-10-05 13:42:00.527157: Pseudo dice [np.float32(0.5545)]
2025-10-05 13:42:00.527364: Epoch time: 45.9 s
2025-10-05 13:42:01.174423: 
2025-10-05 13:42:01.174675: Epoch 66
2025-10-05 13:42:01.174877: Current learning rate: 0.00593
2025-10-05 13:42:47.045029: Validation loss did not improve from -0.11245. Patience: 56/50
2025-10-05 13:42:47.045768: train_loss -0.9328
2025-10-05 13:42:47.046040: val_loss 0.3056
2025-10-05 13:42:47.046187: Pseudo dice [np.float32(0.5403)]
2025-10-05 13:42:47.046397: Epoch time: 45.87 s
2025-10-05 13:42:47.691517: 
2025-10-05 13:42:47.691824: Epoch 67
2025-10-05 13:42:47.692024: Current learning rate: 0.00587
2025-10-05 13:43:33.497967: Validation loss did not improve from -0.11245. Patience: 57/50
2025-10-05 13:43:33.498479: train_loss -0.9332
2025-10-05 13:43:33.498709: val_loss 0.2028
2025-10-05 13:43:33.498944: Pseudo dice [np.float32(0.5522)]
2025-10-05 13:43:33.499188: Epoch time: 45.81 s
2025-10-05 13:43:34.136578: 
2025-10-05 13:43:34.136897: Epoch 68
2025-10-05 13:43:34.137161: Current learning rate: 0.00581
2025-10-05 13:44:20.086297: Validation loss did not improve from -0.11245. Patience: 58/50
2025-10-05 13:44:20.087045: train_loss -0.9327
2025-10-05 13:44:20.087296: val_loss 0.2734
2025-10-05 13:44:20.087557: Pseudo dice [np.float32(0.5484)]
2025-10-05 13:44:20.087749: Epoch time: 45.95 s
2025-10-05 13:44:20.717870: 
2025-10-05 13:44:20.718137: Epoch 69
2025-10-05 13:44:20.718435: Current learning rate: 0.00574
2025-10-05 13:45:06.769516: Validation loss did not improve from -0.11245. Patience: 59/50
2025-10-05 13:45:06.769991: train_loss -0.9346
2025-10-05 13:45:06.770224: val_loss 0.2316
2025-10-05 13:45:06.770504: Pseudo dice [np.float32(0.5531)]
2025-10-05 13:45:06.770758: Epoch time: 46.05 s
2025-10-05 13:45:07.848714: 
2025-10-05 13:45:07.849105: Epoch 70
2025-10-05 13:45:07.849348: Current learning rate: 0.00568
2025-10-05 13:45:53.907460: Validation loss did not improve from -0.11245. Patience: 60/50
2025-10-05 13:45:53.908138: train_loss -0.9355
2025-10-05 13:45:53.908329: val_loss 0.243
2025-10-05 13:45:53.908497: Pseudo dice [np.float32(0.5321)]
2025-10-05 13:45:53.908685: Epoch time: 46.06 s
2025-10-05 13:45:54.539798: 
2025-10-05 13:45:54.540027: Epoch 71
2025-10-05 13:45:54.540199: Current learning rate: 0.00562
2025-10-05 13:46:40.605049: Validation loss did not improve from -0.11245. Patience: 61/50
2025-10-05 13:46:40.605426: train_loss -0.9358
2025-10-05 13:46:40.605603: val_loss 0.1486
2025-10-05 13:46:40.605749: Pseudo dice [np.float32(0.5632)]
2025-10-05 13:46:40.605900: Epoch time: 46.07 s
2025-10-05 13:46:41.240530: 
2025-10-05 13:46:41.240822: Epoch 72
2025-10-05 13:46:41.240993: Current learning rate: 0.00555
2025-10-05 13:47:27.220456: Validation loss did not improve from -0.11245. Patience: 62/50
2025-10-05 13:47:27.221083: train_loss -0.9364
2025-10-05 13:47:27.221251: val_loss 0.2502
2025-10-05 13:47:27.221387: Pseudo dice [np.float32(0.5164)]
2025-10-05 13:47:27.221520: Epoch time: 45.98 s
2025-10-05 13:47:27.851173: 
2025-10-05 13:47:27.851426: Epoch 73
2025-10-05 13:47:27.851607: Current learning rate: 0.00549
2025-10-05 13:48:13.880766: Validation loss did not improve from -0.11245. Patience: 63/50
2025-10-05 13:48:13.881255: train_loss -0.9358
2025-10-05 13:48:13.881526: val_loss 0.1631
2025-10-05 13:48:13.881845: Pseudo dice [np.float32(0.5572)]
2025-10-05 13:48:13.882192: Epoch time: 46.03 s
2025-10-05 13:48:14.517107: 
2025-10-05 13:48:14.517370: Epoch 74
2025-10-05 13:48:14.517576: Current learning rate: 0.00542
2025-10-05 13:49:00.527318: Validation loss did not improve from -0.11245. Patience: 64/50
2025-10-05 13:49:00.527962: train_loss -0.9362
2025-10-05 13:49:00.528117: val_loss 0.1887
2025-10-05 13:49:00.528267: Pseudo dice [np.float32(0.5552)]
2025-10-05 13:49:00.528436: Epoch time: 46.01 s
2025-10-05 13:49:01.622414: 
2025-10-05 13:49:01.623083: Epoch 75
2025-10-05 13:49:01.623571: Current learning rate: 0.00536
2025-10-05 13:49:47.700375: Validation loss did not improve from -0.11245. Patience: 65/50
2025-10-05 13:49:47.700835: train_loss -0.9349
2025-10-05 13:49:47.701061: val_loss 0.1856
2025-10-05 13:49:47.701263: Pseudo dice [np.float32(0.5613)]
2025-10-05 13:49:47.701490: Epoch time: 46.08 s
2025-10-05 13:49:48.721512: 
2025-10-05 13:49:48.721878: Epoch 76
2025-10-05 13:49:48.722117: Current learning rate: 0.00529
2025-10-05 13:50:34.893090: Validation loss did not improve from -0.11245. Patience: 66/50
2025-10-05 13:50:34.893805: train_loss -0.9384
2025-10-05 13:50:34.893998: val_loss 0.26
2025-10-05 13:50:34.894157: Pseudo dice [np.float32(0.5544)]
2025-10-05 13:50:34.894301: Epoch time: 46.17 s
2025-10-05 13:50:35.539562: 
2025-10-05 13:50:35.539883: Epoch 77
2025-10-05 13:50:35.540101: Current learning rate: 0.00523
2025-10-05 13:51:21.641632: Validation loss did not improve from -0.11245. Patience: 67/50
2025-10-05 13:51:21.642138: train_loss -0.9396
2025-10-05 13:51:21.642353: val_loss 0.2374
2025-10-05 13:51:21.642575: Pseudo dice [np.float32(0.539)]
2025-10-05 13:51:21.642742: Epoch time: 46.1 s
2025-10-05 13:51:22.288403: 
2025-10-05 13:51:22.288795: Epoch 78
2025-10-05 13:51:22.288985: Current learning rate: 0.00517
2025-10-05 13:52:08.331888: Validation loss did not improve from -0.11245. Patience: 68/50
2025-10-05 13:52:08.332590: train_loss -0.9416
2025-10-05 13:52:08.332890: val_loss 0.2337
2025-10-05 13:52:08.333055: Pseudo dice [np.float32(0.5402)]
2025-10-05 13:52:08.333241: Epoch time: 46.04 s
2025-10-05 13:52:08.979464: 
2025-10-05 13:52:08.979762: Epoch 79
2025-10-05 13:52:08.979949: Current learning rate: 0.0051
2025-10-05 13:52:54.981817: Validation loss did not improve from -0.11245. Patience: 69/50
2025-10-05 13:52:54.982388: train_loss -0.9409
2025-10-05 13:52:54.982612: val_loss 0.2391
2025-10-05 13:52:54.982792: Pseudo dice [np.float32(0.5489)]
2025-10-05 13:52:54.983102: Epoch time: 46.0 s
2025-10-05 13:52:56.114238: 
2025-10-05 13:52:56.114604: Epoch 80
2025-10-05 13:52:56.114843: Current learning rate: 0.00504
2025-10-05 13:53:42.160424: Validation loss did not improve from -0.11245. Patience: 70/50
2025-10-05 13:53:42.161160: train_loss -0.9423
2025-10-05 13:53:42.161338: val_loss 0.1982
2025-10-05 13:53:42.161507: Pseudo dice [np.float32(0.5626)]
2025-10-05 13:53:42.161665: Epoch time: 46.05 s
2025-10-05 13:53:42.812008: 
2025-10-05 13:53:42.812351: Epoch 81
2025-10-05 13:53:42.812541: Current learning rate: 0.00497
2025-10-05 13:54:28.777218: Validation loss did not improve from -0.11245. Patience: 71/50
2025-10-05 13:54:28.777742: train_loss -0.9403
2025-10-05 13:54:28.777975: val_loss 0.2034
2025-10-05 13:54:28.778154: Pseudo dice [np.float32(0.5622)]
2025-10-05 13:54:28.778391: Epoch time: 45.97 s
2025-10-05 13:54:29.419763: 
2025-10-05 13:54:29.420059: Epoch 82
2025-10-05 13:54:29.420261: Current learning rate: 0.00491
2025-10-05 13:55:15.397842: Validation loss did not improve from -0.11245. Patience: 72/50
2025-10-05 13:55:15.398614: train_loss -0.941
2025-10-05 13:55:15.398832: val_loss 0.2686
2025-10-05 13:55:15.399092: Pseudo dice [np.float32(0.5519)]
2025-10-05 13:55:15.399347: Epoch time: 45.98 s
2025-10-05 13:55:16.026944: 
2025-10-05 13:55:16.027220: Epoch 83
2025-10-05 13:55:16.027403: Current learning rate: 0.00484
2025-10-05 13:56:02.032053: Validation loss did not improve from -0.11245. Patience: 73/50
2025-10-05 13:56:02.032572: train_loss -0.9397
2025-10-05 13:56:02.032812: val_loss 0.2
2025-10-05 13:56:02.033051: Pseudo dice [np.float32(0.5514)]
2025-10-05 13:56:02.033321: Epoch time: 46.01 s
2025-10-05 13:56:02.667852: 
2025-10-05 13:56:02.668128: Epoch 84
2025-10-05 13:56:02.668308: Current learning rate: 0.00478
2025-10-05 13:56:48.653707: Validation loss did not improve from -0.11245. Patience: 74/50
2025-10-05 13:56:48.654440: train_loss -0.9415
2025-10-05 13:56:48.654588: val_loss 0.2632
2025-10-05 13:56:48.654731: Pseudo dice [np.float32(0.566)]
2025-10-05 13:56:48.654883: Epoch time: 45.99 s
2025-10-05 13:56:49.766436: 
2025-10-05 13:56:49.766749: Epoch 85
2025-10-05 13:56:49.766946: Current learning rate: 0.00471
2025-10-05 13:57:35.723238: Validation loss did not improve from -0.11245. Patience: 75/50
2025-10-05 13:57:35.723694: train_loss -0.943
2025-10-05 13:57:35.723861: val_loss 0.2526
2025-10-05 13:57:35.724068: Pseudo dice [np.float32(0.5586)]
2025-10-05 13:57:35.724273: Epoch time: 45.96 s
2025-10-05 13:57:35.724414: Yayy! New best EMA pseudo Dice: 0.5526999831199646
2025-10-05 13:57:36.832484: 
2025-10-05 13:57:36.832815: Epoch 86
2025-10-05 13:57:36.833070: Current learning rate: 0.00465
2025-10-05 13:58:22.727056: Validation loss did not improve from -0.11245. Patience: 76/50
2025-10-05 13:58:22.727894: train_loss -0.9423
2025-10-05 13:58:22.728284: val_loss 0.1774
2025-10-05 13:58:22.728494: Pseudo dice [np.float32(0.5543)]
2025-10-05 13:58:22.728713: Epoch time: 45.9 s
2025-10-05 13:58:22.728901: Yayy! New best EMA pseudo Dice: 0.5527999997138977
2025-10-05 13:58:23.845870: 
2025-10-05 13:58:23.846227: Epoch 87
2025-10-05 13:58:23.846550: Current learning rate: 0.00458
2025-10-05 13:59:09.784846: Validation loss did not improve from -0.11245. Patience: 77/50
2025-10-05 13:59:09.785345: train_loss -0.9426
2025-10-05 13:59:09.785534: val_loss 0.1957
2025-10-05 13:59:09.785693: Pseudo dice [np.float32(0.5614)]
2025-10-05 13:59:09.785873: Epoch time: 45.94 s
2025-10-05 13:59:09.786010: Yayy! New best EMA pseudo Dice: 0.5536999702453613
2025-10-05 13:59:10.899102: 
2025-10-05 13:59:10.899354: Epoch 88
2025-10-05 13:59:10.899559: Current learning rate: 0.00452
2025-10-05 13:59:56.811356: Validation loss did not improve from -0.11245. Patience: 78/50
2025-10-05 13:59:56.812163: train_loss -0.9422
2025-10-05 13:59:56.812453: val_loss 0.2721
2025-10-05 13:59:56.812662: Pseudo dice [np.float32(0.5528)]
2025-10-05 13:59:56.812888: Epoch time: 45.91 s
2025-10-05 13:59:57.444809: 
2025-10-05 13:59:57.445193: Epoch 89
2025-10-05 13:59:57.445379: Current learning rate: 0.00445
2025-10-05 14:00:43.375515: Validation loss did not improve from -0.11245. Patience: 79/50
2025-10-05 14:00:43.375980: train_loss -0.9456
2025-10-05 14:00:43.376156: val_loss 0.1903
2025-10-05 14:00:43.376287: Pseudo dice [np.float32(0.5635)]
2025-10-05 14:00:43.376575: Epoch time: 45.93 s
2025-10-05 14:00:43.878682: Yayy! New best EMA pseudo Dice: 0.5546000003814697
2025-10-05 14:00:45.048008: 
2025-10-05 14:00:45.048259: Epoch 90
2025-10-05 14:00:45.048439: Current learning rate: 0.00438
2025-10-05 14:01:31.002246: Validation loss did not improve from -0.11245. Patience: 80/50
2025-10-05 14:01:31.003150: train_loss -0.9452
2025-10-05 14:01:31.003374: val_loss 0.1878
2025-10-05 14:01:31.003608: Pseudo dice [np.float32(0.5827)]
2025-10-05 14:01:31.003844: Epoch time: 45.96 s
2025-10-05 14:01:31.004017: Yayy! New best EMA pseudo Dice: 0.5573999881744385
2025-10-05 14:01:32.142235: 
2025-10-05 14:01:32.142625: Epoch 91
2025-10-05 14:01:32.142871: Current learning rate: 0.00432
2025-10-05 14:02:18.078307: Validation loss did not improve from -0.11245. Patience: 81/50
2025-10-05 14:02:18.078837: train_loss -0.9442
2025-10-05 14:02:18.079019: val_loss 0.2498
2025-10-05 14:02:18.079176: Pseudo dice [np.float32(0.5659)]
2025-10-05 14:02:18.079312: Epoch time: 45.94 s
2025-10-05 14:02:18.079457: Yayy! New best EMA pseudo Dice: 0.5583000183105469
2025-10-05 14:02:19.676653: 
2025-10-05 14:02:19.676993: Epoch 92
2025-10-05 14:02:19.677187: Current learning rate: 0.00425
2025-10-05 14:03:05.590743: Validation loss did not improve from -0.11245. Patience: 82/50
2025-10-05 14:03:05.591414: train_loss -0.944
2025-10-05 14:03:05.591567: val_loss 0.3291
2025-10-05 14:03:05.591707: Pseudo dice [np.float32(0.5257)]
2025-10-05 14:03:05.591855: Epoch time: 45.92 s
2025-10-05 14:03:06.228227: 
2025-10-05 14:03:06.228523: Epoch 93
2025-10-05 14:03:06.228710: Current learning rate: 0.00419
2025-10-05 14:03:52.196055: Validation loss did not improve from -0.11245. Patience: 83/50
2025-10-05 14:03:52.196573: train_loss -0.9457
2025-10-05 14:03:52.196742: val_loss 0.2178
2025-10-05 14:03:52.196893: Pseudo dice [np.float32(0.5804)]
2025-10-05 14:03:52.197060: Epoch time: 45.97 s
2025-10-05 14:03:52.826841: 
2025-10-05 14:03:52.827235: Epoch 94
2025-10-05 14:03:52.827436: Current learning rate: 0.00412
2025-10-05 14:04:38.781696: Validation loss did not improve from -0.11245. Patience: 84/50
2025-10-05 14:04:38.782385: train_loss -0.9456
2025-10-05 14:04:38.782609: val_loss 0.2265
2025-10-05 14:04:38.782753: Pseudo dice [np.float32(0.5721)]
2025-10-05 14:04:38.782910: Epoch time: 45.96 s
2025-10-05 14:04:39.270031: Yayy! New best EMA pseudo Dice: 0.5590000152587891
2025-10-05 14:04:40.374232: 
2025-10-05 14:04:40.374552: Epoch 95
2025-10-05 14:04:40.374835: Current learning rate: 0.00405
2025-10-05 14:05:26.293766: Validation loss did not improve from -0.11245. Patience: 85/50
2025-10-05 14:05:26.294302: train_loss -0.9478
2025-10-05 14:05:26.294522: val_loss 0.2777
2025-10-05 14:05:26.294699: Pseudo dice [np.float32(0.5589)]
2025-10-05 14:05:26.294866: Epoch time: 45.92 s
2025-10-05 14:05:26.931077: 
2025-10-05 14:05:26.931447: Epoch 96
2025-10-05 14:05:26.931688: Current learning rate: 0.00399
2025-10-05 14:06:12.880134: Validation loss did not improve from -0.11245. Patience: 86/50
2025-10-05 14:06:12.880894: train_loss -0.9461
2025-10-05 14:06:12.881087: val_loss 0.2814
2025-10-05 14:06:12.881246: Pseudo dice [np.float32(0.5399)]
2025-10-05 14:06:12.881439: Epoch time: 45.95 s
2025-10-05 14:06:13.516251: 
2025-10-05 14:06:13.516614: Epoch 97
2025-10-05 14:06:13.516837: Current learning rate: 0.00392
2025-10-05 14:06:59.509162: Validation loss did not improve from -0.11245. Patience: 87/50
2025-10-05 14:06:59.509626: train_loss -0.9482
2025-10-05 14:06:59.509819: val_loss 0.3726
2025-10-05 14:06:59.509976: Pseudo dice [np.float32(0.523)]
2025-10-05 14:06:59.510207: Epoch time: 45.99 s
2025-10-05 14:07:00.154853: 
2025-10-05 14:07:00.155221: Epoch 98
2025-10-05 14:07:00.155429: Current learning rate: 0.00385
2025-10-05 14:07:46.131306: Validation loss did not improve from -0.11245. Patience: 88/50
2025-10-05 14:07:46.132023: train_loss -0.9492
2025-10-05 14:07:46.132233: val_loss 0.1849
2025-10-05 14:07:46.132396: Pseudo dice [np.float32(0.5639)]
2025-10-05 14:07:46.132536: Epoch time: 45.98 s
2025-10-05 14:07:46.770715: 
2025-10-05 14:07:46.771023: Epoch 99
2025-10-05 14:07:46.771243: Current learning rate: 0.00379
2025-10-05 14:08:32.740269: Validation loss did not improve from -0.11245. Patience: 89/50
2025-10-05 14:08:32.740793: train_loss -0.9476
2025-10-05 14:08:32.740969: val_loss 0.2534
2025-10-05 14:08:32.741104: Pseudo dice [np.float32(0.5343)]
2025-10-05 14:08:32.741248: Epoch time: 45.97 s
2025-10-05 14:08:33.869266: 
2025-10-05 14:08:33.869612: Epoch 100
2025-10-05 14:08:33.869811: Current learning rate: 0.00372
2025-10-05 14:09:19.891774: Validation loss did not improve from -0.11245. Patience: 90/50
2025-10-05 14:09:19.892394: train_loss -0.9484
2025-10-05 14:09:19.892578: val_loss 0.2596
2025-10-05 14:09:19.892729: Pseudo dice [np.float32(0.553)]
2025-10-05 14:09:19.892901: Epoch time: 46.02 s
2025-10-05 14:09:20.534081: 
2025-10-05 14:09:20.534412: Epoch 101
2025-10-05 14:09:20.534722: Current learning rate: 0.00365
2025-10-05 14:10:06.496394: Validation loss did not improve from -0.11245. Patience: 91/50
2025-10-05 14:10:06.496842: train_loss -0.9491
2025-10-05 14:10:06.497018: val_loss 0.2346
2025-10-05 14:10:06.497149: Pseudo dice [np.float32(0.575)]
2025-10-05 14:10:06.497332: Epoch time: 45.96 s
2025-10-05 14:10:07.142569: 
2025-10-05 14:10:07.142873: Epoch 102
2025-10-05 14:10:07.143081: Current learning rate: 0.00359
2025-10-05 14:10:53.120039: Validation loss did not improve from -0.11245. Patience: 92/50
2025-10-05 14:10:53.120732: train_loss -0.9498
2025-10-05 14:10:53.120895: val_loss 0.233
2025-10-05 14:10:53.121051: Pseudo dice [np.float32(0.5618)]
2025-10-05 14:10:53.121206: Epoch time: 45.98 s
2025-10-05 14:10:53.756971: 
2025-10-05 14:10:53.757354: Epoch 103
2025-10-05 14:10:53.757581: Current learning rate: 0.00352
2025-10-05 14:11:39.740589: Validation loss did not improve from -0.11245. Patience: 93/50
2025-10-05 14:11:39.741080: train_loss -0.9498
2025-10-05 14:11:39.741275: val_loss 0.3058
2025-10-05 14:11:39.741427: Pseudo dice [np.float32(0.5405)]
2025-10-05 14:11:39.741580: Epoch time: 45.98 s
2025-10-05 14:11:40.378395: 
2025-10-05 14:11:40.378670: Epoch 104
2025-10-05 14:11:40.378919: Current learning rate: 0.00345
2025-10-05 14:12:26.311143: Validation loss did not improve from -0.11245. Patience: 94/50
2025-10-05 14:12:26.311996: train_loss -0.95
2025-10-05 14:12:26.312305: val_loss 0.2912
2025-10-05 14:12:26.312598: Pseudo dice [np.float32(0.5323)]
2025-10-05 14:12:26.312904: Epoch time: 45.93 s
2025-10-05 14:12:27.495029: 
2025-10-05 14:12:27.495384: Epoch 105
2025-10-05 14:12:27.495606: Current learning rate: 0.00338
2025-10-05 14:13:13.536154: Validation loss did not improve from -0.11245. Patience: 95/50
2025-10-05 14:13:13.536645: train_loss -0.9493
2025-10-05 14:13:13.536848: val_loss 0.2347
2025-10-05 14:13:13.537032: Pseudo dice [np.float32(0.552)]
2025-10-05 14:13:13.537240: Epoch time: 46.04 s
2025-10-05 14:13:14.176555: 
2025-10-05 14:13:14.176917: Epoch 106
2025-10-05 14:13:14.177148: Current learning rate: 0.00332
2025-10-05 14:14:00.223287: Validation loss did not improve from -0.11245. Patience: 96/50
2025-10-05 14:14:00.224049: train_loss -0.949
2025-10-05 14:14:00.224311: val_loss 0.2931
2025-10-05 14:14:00.224556: Pseudo dice [np.float32(0.5363)]
2025-10-05 14:14:00.224782: Epoch time: 46.05 s
2025-10-05 14:14:00.869065: 
2025-10-05 14:14:00.869420: Epoch 107
2025-10-05 14:14:00.869698: Current learning rate: 0.00325
2025-10-05 14:14:46.901470: Validation loss did not improve from -0.11245. Patience: 97/50
2025-10-05 14:14:46.901966: train_loss -0.9507
2025-10-05 14:14:46.902186: val_loss 0.2962
2025-10-05 14:14:46.902419: Pseudo dice [np.float32(0.5432)]
2025-10-05 14:14:46.902611: Epoch time: 46.03 s
2025-10-05 14:14:48.024431: 
2025-10-05 14:14:48.024772: Epoch 108
2025-10-05 14:14:48.025016: Current learning rate: 0.00318
2025-10-05 14:15:34.183137: Validation loss did not improve from -0.11245. Patience: 98/50
2025-10-05 14:15:34.183784: train_loss -0.9512
2025-10-05 14:15:34.183961: val_loss 0.2587
2025-10-05 14:15:34.184083: Pseudo dice [np.float32(0.5757)]
2025-10-05 14:15:34.184242: Epoch time: 46.16 s
2025-10-05 14:15:34.826308: 
2025-10-05 14:15:34.826597: Epoch 109
2025-10-05 14:15:34.826951: Current learning rate: 0.00311
2025-10-05 14:16:20.917529: Validation loss did not improve from -0.11245. Patience: 99/50
2025-10-05 14:16:20.918050: train_loss -0.9512
2025-10-05 14:16:20.918244: val_loss 0.297
2025-10-05 14:16:20.918394: Pseudo dice [np.float32(0.5593)]
2025-10-05 14:16:20.918557: Epoch time: 46.09 s
2025-10-05 14:16:22.009895: 
2025-10-05 14:16:22.010186: Epoch 110
2025-10-05 14:16:22.010373: Current learning rate: 0.00304
2025-10-05 14:17:08.069782: Validation loss did not improve from -0.11245. Patience: 100/50
2025-10-05 14:17:08.070550: train_loss -0.9506
2025-10-05 14:17:08.070771: val_loss 0.2453
2025-10-05 14:17:08.070922: Pseudo dice [np.float32(0.5651)]
2025-10-05 14:17:08.071148: Epoch time: 46.06 s
2025-10-05 14:17:08.719901: 
2025-10-05 14:17:08.720289: Epoch 111
2025-10-05 14:17:08.720514: Current learning rate: 0.00297
2025-10-05 14:17:54.762713: Validation loss did not improve from -0.11245. Patience: 101/50
2025-10-05 14:17:54.763217: train_loss -0.9528
2025-10-05 14:17:54.763395: val_loss 0.2626
2025-10-05 14:17:54.763522: Pseudo dice [np.float32(0.5629)]
2025-10-05 14:17:54.763669: Epoch time: 46.04 s
2025-10-05 14:17:55.413913: 
2025-10-05 14:17:55.414266: Epoch 112
2025-10-05 14:17:55.414479: Current learning rate: 0.00291
2025-10-05 14:18:41.420275: Validation loss did not improve from -0.11245. Patience: 102/50
2025-10-05 14:18:41.420945: train_loss -0.9522
2025-10-05 14:18:41.421134: val_loss 0.2675
2025-10-05 14:18:41.421300: Pseudo dice [np.float32(0.5578)]
2025-10-05 14:18:41.421441: Epoch time: 46.01 s
2025-10-05 14:18:42.058686: 
2025-10-05 14:18:42.059087: Epoch 113
2025-10-05 14:18:42.059332: Current learning rate: 0.00284
2025-10-05 14:19:28.013268: Validation loss did not improve from -0.11245. Patience: 103/50
2025-10-05 14:19:28.013747: train_loss -0.9516
2025-10-05 14:19:28.013935: val_loss 0.2383
2025-10-05 14:19:28.014094: Pseudo dice [np.float32(0.5473)]
2025-10-05 14:19:28.014273: Epoch time: 45.96 s
2025-10-05 14:19:28.645932: 
2025-10-05 14:19:28.646298: Epoch 114
2025-10-05 14:19:28.646487: Current learning rate: 0.00277
2025-10-05 14:20:14.563909: Validation loss did not improve from -0.11245. Patience: 104/50
2025-10-05 14:20:14.564808: train_loss -0.9508
2025-10-05 14:20:14.565032: val_loss 0.3109
2025-10-05 14:20:14.565239: Pseudo dice [np.float32(0.561)]
2025-10-05 14:20:14.565443: Epoch time: 45.92 s
2025-10-05 14:20:15.747615: 
2025-10-05 14:20:15.748044: Epoch 115
2025-10-05 14:20:15.748378: Current learning rate: 0.0027
2025-10-05 14:21:01.664869: Validation loss did not improve from -0.11245. Patience: 105/50
2025-10-05 14:21:01.665358: train_loss -0.9514
2025-10-05 14:21:01.665525: val_loss 0.2268
2025-10-05 14:21:01.665655: Pseudo dice [np.float32(0.557)]
2025-10-05 14:21:01.665834: Epoch time: 45.92 s
2025-10-05 14:21:02.299123: 
2025-10-05 14:21:02.299405: Epoch 116
2025-10-05 14:21:02.299634: Current learning rate: 0.00263
2025-10-05 14:21:48.144729: Validation loss did not improve from -0.11245. Patience: 106/50
2025-10-05 14:21:48.145346: train_loss -0.9531
2025-10-05 14:21:48.145521: val_loss 0.3845
2025-10-05 14:21:48.145655: Pseudo dice [np.float32(0.5538)]
2025-10-05 14:21:48.145814: Epoch time: 45.85 s
2025-10-05 14:21:48.788221: 
2025-10-05 14:21:48.788505: Epoch 117
2025-10-05 14:21:48.788690: Current learning rate: 0.00256
2025-10-05 14:22:34.668943: Validation loss did not improve from -0.11245. Patience: 107/50
2025-10-05 14:22:34.669365: train_loss -0.9526
2025-10-05 14:22:34.669577: val_loss 0.2731
2025-10-05 14:22:34.669733: Pseudo dice [np.float32(0.5717)]
2025-10-05 14:22:34.669928: Epoch time: 45.88 s
2025-10-05 14:22:35.304982: 
2025-10-05 14:22:35.305287: Epoch 118
2025-10-05 14:22:35.305488: Current learning rate: 0.00249
2025-10-05 14:23:21.216886: Validation loss did not improve from -0.11245. Patience: 108/50
2025-10-05 14:23:21.217566: train_loss -0.9521
2025-10-05 14:23:21.217722: val_loss 0.2703
2025-10-05 14:23:21.217852: Pseudo dice [np.float32(0.5469)]
2025-10-05 14:23:21.218002: Epoch time: 45.91 s
2025-10-05 14:23:21.862789: 
2025-10-05 14:23:21.863124: Epoch 119
2025-10-05 14:23:21.863305: Current learning rate: 0.00242
2025-10-05 14:24:07.753624: Validation loss did not improve from -0.11245. Patience: 109/50
2025-10-05 14:24:07.754095: train_loss -0.9535
2025-10-05 14:24:07.754267: val_loss 0.3507
2025-10-05 14:24:07.754406: Pseudo dice [np.float32(0.5455)]
2025-10-05 14:24:07.754542: Epoch time: 45.89 s
2025-10-05 14:24:08.862964: 
2025-10-05 14:24:08.863272: Epoch 120
2025-10-05 14:24:08.863478: Current learning rate: 0.00235
2025-10-05 14:24:54.715426: Validation loss did not improve from -0.11245. Patience: 110/50
2025-10-05 14:24:54.716021: train_loss -0.9538
2025-10-05 14:24:54.716192: val_loss 0.3468
2025-10-05 14:24:54.716337: Pseudo dice [np.float32(0.5473)]
2025-10-05 14:24:54.716490: Epoch time: 45.85 s
2025-10-05 14:24:55.350519: 
2025-10-05 14:24:55.350842: Epoch 121
2025-10-05 14:24:55.351026: Current learning rate: 0.00228
2025-10-05 14:25:41.200089: Validation loss did not improve from -0.11245. Patience: 111/50
2025-10-05 14:25:41.200617: train_loss -0.9542
2025-10-05 14:25:41.200875: val_loss 0.3135
2025-10-05 14:25:41.201074: Pseudo dice [np.float32(0.5346)]
2025-10-05 14:25:41.201241: Epoch time: 45.85 s
2025-10-05 14:25:41.845919: 
2025-10-05 14:25:41.846151: Epoch 122
2025-10-05 14:25:41.846437: Current learning rate: 0.00221
2025-10-05 14:26:27.711160: Validation loss did not improve from -0.11245. Patience: 112/50
2025-10-05 14:26:27.711819: train_loss -0.9538
2025-10-05 14:26:27.712067: val_loss 0.2315
2025-10-05 14:26:27.712240: Pseudo dice [np.float32(0.5555)]
2025-10-05 14:26:27.712399: Epoch time: 45.87 s
2025-10-05 14:26:28.701428: 
2025-10-05 14:26:28.701818: Epoch 123
2025-10-05 14:26:28.702049: Current learning rate: 0.00214
2025-10-05 14:27:14.619696: Validation loss did not improve from -0.11245. Patience: 113/50
2025-10-05 14:27:14.620148: train_loss -0.955
2025-10-05 14:27:14.620322: val_loss 0.2853
2025-10-05 14:27:14.620446: Pseudo dice [np.float32(0.5613)]
2025-10-05 14:27:14.620611: Epoch time: 45.92 s
2025-10-05 14:27:15.261854: 
2025-10-05 14:27:15.262136: Epoch 124
2025-10-05 14:27:15.262313: Current learning rate: 0.00207
2025-10-05 14:28:01.181140: Validation loss did not improve from -0.11245. Patience: 114/50
2025-10-05 14:28:01.181807: train_loss -0.9542
2025-10-05 14:28:01.181959: val_loss 0.2581
2025-10-05 14:28:01.182102: Pseudo dice [np.float32(0.5755)]
2025-10-05 14:28:01.182296: Epoch time: 45.92 s
2025-10-05 14:28:02.274251: 
2025-10-05 14:28:02.274684: Epoch 125
2025-10-05 14:28:02.274891: Current learning rate: 0.00199
2025-10-05 14:28:48.168150: Validation loss did not improve from -0.11245. Patience: 115/50
2025-10-05 14:28:48.168819: train_loss -0.9543
2025-10-05 14:28:48.169163: val_loss 0.2569
2025-10-05 14:28:48.169392: Pseudo dice [np.float32(0.555)]
2025-10-05 14:28:48.169599: Epoch time: 45.9 s
2025-10-05 14:28:48.822260: 
2025-10-05 14:28:48.822631: Epoch 126
2025-10-05 14:28:48.822837: Current learning rate: 0.00192
2025-10-05 14:29:34.708124: Validation loss did not improve from -0.11245. Patience: 116/50
2025-10-05 14:29:34.708770: train_loss -0.9555
2025-10-05 14:29:34.708949: val_loss 0.2376
2025-10-05 14:29:34.709154: Pseudo dice [np.float32(0.558)]
2025-10-05 14:29:34.709324: Epoch time: 45.89 s
2025-10-05 14:29:35.345658: 
2025-10-05 14:29:35.346024: Epoch 127
2025-10-05 14:29:35.346215: Current learning rate: 0.00185
2025-10-05 14:30:21.230155: Validation loss did not improve from -0.11245. Patience: 117/50
2025-10-05 14:30:21.230631: train_loss -0.9558
2025-10-05 14:30:21.230880: val_loss 0.3043
2025-10-05 14:30:21.231042: Pseudo dice [np.float32(0.5768)]
2025-10-05 14:30:21.231188: Epoch time: 45.89 s
2025-10-05 14:30:21.867783: 
2025-10-05 14:30:21.868054: Epoch 128
2025-10-05 14:30:21.868205: Current learning rate: 0.00178
2025-10-05 14:31:07.761882: Validation loss did not improve from -0.11245. Patience: 118/50
2025-10-05 14:31:07.762803: train_loss -0.9548
2025-10-05 14:31:07.763078: val_loss 0.348
2025-10-05 14:31:07.763349: Pseudo dice [np.float32(0.5343)]
2025-10-05 14:31:07.763709: Epoch time: 45.9 s
2025-10-05 14:31:08.410742: 
2025-10-05 14:31:08.411011: Epoch 129
2025-10-05 14:31:08.411196: Current learning rate: 0.0017
2025-10-05 14:31:54.296556: Validation loss did not improve from -0.11245. Patience: 119/50
2025-10-05 14:31:54.296935: train_loss -0.9554
2025-10-05 14:31:54.297115: val_loss 0.3254
2025-10-05 14:31:54.297260: Pseudo dice [np.float32(0.5453)]
2025-10-05 14:31:54.297436: Epoch time: 45.89 s
2025-10-05 14:31:55.387205: 
2025-10-05 14:31:55.387604: Epoch 130
2025-10-05 14:31:55.387841: Current learning rate: 0.00163
2025-10-05 14:32:41.224458: Validation loss did not improve from -0.11245. Patience: 120/50
2025-10-05 14:32:41.225142: train_loss -0.9572
2025-10-05 14:32:41.225344: val_loss 0.3363
2025-10-05 14:32:41.225503: Pseudo dice [np.float32(0.5327)]
2025-10-05 14:32:41.225694: Epoch time: 45.84 s
2025-10-05 14:32:41.862472: 
2025-10-05 14:32:41.862813: Epoch 131
2025-10-05 14:32:41.863005: Current learning rate: 0.00156
2025-10-05 14:33:27.714567: Validation loss did not improve from -0.11245. Patience: 121/50
2025-10-05 14:33:27.714985: train_loss -0.9565
2025-10-05 14:33:27.715136: val_loss 0.3056
2025-10-05 14:33:27.715287: Pseudo dice [np.float32(0.5593)]
2025-10-05 14:33:27.715561: Epoch time: 45.85 s
2025-10-05 14:33:28.356705: 
2025-10-05 14:33:28.357174: Epoch 132
2025-10-05 14:33:28.357494: Current learning rate: 0.00148
2025-10-05 14:34:14.247692: Validation loss did not improve from -0.11245. Patience: 122/50
2025-10-05 14:34:14.248501: train_loss -0.9563
2025-10-05 14:34:14.248751: val_loss 0.3355
2025-10-05 14:34:14.248949: Pseudo dice [np.float32(0.5563)]
2025-10-05 14:34:14.249147: Epoch time: 45.89 s
2025-10-05 14:34:14.880903: 
2025-10-05 14:34:14.881311: Epoch 133
2025-10-05 14:34:14.881588: Current learning rate: 0.00141
2025-10-05 14:35:00.693797: Validation loss did not improve from -0.11245. Patience: 123/50
2025-10-05 14:35:00.694238: train_loss -0.9567
2025-10-05 14:35:00.694421: val_loss 0.3066
2025-10-05 14:35:00.694561: Pseudo dice [np.float32(0.5368)]
2025-10-05 14:35:00.694699: Epoch time: 45.81 s
2025-10-05 14:35:01.326040: 
2025-10-05 14:35:01.326411: Epoch 134
2025-10-05 14:35:01.326634: Current learning rate: 0.00133
2025-10-05 14:35:47.131535: Validation loss did not improve from -0.11245. Patience: 124/50
2025-10-05 14:35:47.132337: train_loss -0.9563
2025-10-05 14:35:47.132594: val_loss 0.304
2025-10-05 14:35:47.132841: Pseudo dice [np.float32(0.5346)]
2025-10-05 14:35:47.133126: Epoch time: 45.81 s
2025-10-05 14:35:48.239659: 
2025-10-05 14:35:48.240007: Epoch 135
2025-10-05 14:35:48.240341: Current learning rate: 0.00126
2025-10-05 14:36:34.072079: Validation loss did not improve from -0.11245. Patience: 125/50
2025-10-05 14:36:34.072538: train_loss -0.9561
2025-10-05 14:36:34.072695: val_loss 0.3332
2025-10-05 14:36:34.072874: Pseudo dice [np.float32(0.5369)]
2025-10-05 14:36:34.073050: Epoch time: 45.83 s
2025-10-05 14:36:34.714914: 
2025-10-05 14:36:34.715280: Epoch 136
2025-10-05 14:36:34.715513: Current learning rate: 0.00118
2025-10-05 14:37:20.518146: Validation loss did not improve from -0.11245. Patience: 126/50
2025-10-05 14:37:20.518841: train_loss -0.9576
2025-10-05 14:37:20.519010: val_loss 0.3453
2025-10-05 14:37:20.519145: Pseudo dice [np.float32(0.5543)]
2025-10-05 14:37:20.519339: Epoch time: 45.8 s
2025-10-05 14:37:21.154254: 
2025-10-05 14:37:21.154659: Epoch 137
2025-10-05 14:37:21.154868: Current learning rate: 0.00111
2025-10-05 14:38:06.949233: Validation loss did not improve from -0.11245. Patience: 127/50
2025-10-05 14:38:06.949701: train_loss -0.9569
2025-10-05 14:38:06.950000: val_loss 0.3367
2025-10-05 14:38:06.950144: Pseudo dice [np.float32(0.5607)]
2025-10-05 14:38:06.950295: Epoch time: 45.8 s
2025-10-05 14:38:07.940364: 
2025-10-05 14:38:07.940750: Epoch 138
2025-10-05 14:38:07.940965: Current learning rate: 0.00103
2025-10-05 14:38:53.741077: Validation loss did not improve from -0.11245. Patience: 128/50
2025-10-05 14:38:53.741889: train_loss -0.9571
2025-10-05 14:38:53.742059: val_loss 0.3504
2025-10-05 14:38:53.742272: Pseudo dice [np.float32(0.53)]
2025-10-05 14:38:53.742432: Epoch time: 45.8 s
2025-10-05 14:38:54.378127: 
2025-10-05 14:38:54.378699: Epoch 139
2025-10-05 14:38:54.378974: Current learning rate: 0.00095
2025-10-05 14:39:40.206223: Validation loss did not improve from -0.11245. Patience: 129/50
2025-10-05 14:39:40.206751: train_loss -0.9571
2025-10-05 14:39:40.206968: val_loss 0.2879
2025-10-05 14:39:40.207135: Pseudo dice [np.float32(0.5644)]
2025-10-05 14:39:40.207309: Epoch time: 45.83 s
2025-10-05 14:39:41.294177: 
2025-10-05 14:39:41.294644: Epoch 140
2025-10-05 14:39:41.294896: Current learning rate: 0.00087
2025-10-05 14:40:27.086922: Validation loss did not improve from -0.11245. Patience: 130/50
2025-10-05 14:40:27.087793: train_loss -0.9574
2025-10-05 14:40:27.088012: val_loss 0.359
2025-10-05 14:40:27.088190: Pseudo dice [np.float32(0.5432)]
2025-10-05 14:40:27.088344: Epoch time: 45.79 s
2025-10-05 14:40:27.741178: 
2025-10-05 14:40:27.741595: Epoch 141
2025-10-05 14:40:27.741844: Current learning rate: 0.00079
2025-10-05 14:41:13.638444: Validation loss did not improve from -0.11245. Patience: 131/50
2025-10-05 14:41:13.638916: train_loss -0.9573
2025-10-05 14:41:13.639116: val_loss 0.3174
2025-10-05 14:41:13.639277: Pseudo dice [np.float32(0.5616)]
2025-10-05 14:41:13.639506: Epoch time: 45.9 s
2025-10-05 14:41:14.286853: 
2025-10-05 14:41:14.287259: Epoch 142
2025-10-05 14:41:14.287508: Current learning rate: 0.00071
2025-10-05 14:42:00.136304: Validation loss did not improve from -0.11245. Patience: 132/50
2025-10-05 14:42:00.136842: train_loss -0.9572
2025-10-05 14:42:00.136994: val_loss 0.3208
2025-10-05 14:42:00.137124: Pseudo dice [np.float32(0.564)]
2025-10-05 14:42:00.137273: Epoch time: 45.85 s
2025-10-05 14:42:00.776090: 
2025-10-05 14:42:00.776423: Epoch 143
2025-10-05 14:42:00.776616: Current learning rate: 0.00063
2025-10-05 14:42:46.635337: Validation loss did not improve from -0.11245. Patience: 133/50
2025-10-05 14:42:46.635793: train_loss -0.9584
2025-10-05 14:42:46.635983: val_loss 0.2893
2025-10-05 14:42:46.636118: Pseudo dice [np.float32(0.5759)]
2025-10-05 14:42:46.636306: Epoch time: 45.86 s
2025-10-05 14:42:47.279910: 
2025-10-05 14:42:47.280233: Epoch 144
2025-10-05 14:42:47.280433: Current learning rate: 0.00055
2025-10-05 14:43:33.151446: Validation loss did not improve from -0.11245. Patience: 134/50
2025-10-05 14:43:33.152046: train_loss -0.958
2025-10-05 14:43:33.152248: val_loss 0.3118
2025-10-05 14:43:33.152411: Pseudo dice [np.float32(0.5457)]
2025-10-05 14:43:33.152609: Epoch time: 45.87 s
2025-10-05 14:43:34.257678: 
2025-10-05 14:43:34.258039: Epoch 145
2025-10-05 14:43:34.258255: Current learning rate: 0.00047
2025-10-05 14:44:20.124893: Validation loss did not improve from -0.11245. Patience: 135/50
2025-10-05 14:44:20.125281: train_loss -0.9575
2025-10-05 14:44:20.125441: val_loss 0.3197
2025-10-05 14:44:20.125586: Pseudo dice [np.float32(0.559)]
2025-10-05 14:44:20.125787: Epoch time: 45.87 s
2025-10-05 14:44:20.771266: 
2025-10-05 14:44:20.771555: Epoch 146
2025-10-05 14:44:20.771743: Current learning rate: 0.00038
2025-10-05 14:45:06.608088: Validation loss did not improve from -0.11245. Patience: 136/50
2025-10-05 14:45:06.608705: train_loss -0.9592
2025-10-05 14:45:06.608882: val_loss 0.3386
2025-10-05 14:45:06.609057: Pseudo dice [np.float32(0.5461)]
2025-10-05 14:45:06.609214: Epoch time: 45.84 s
2025-10-05 14:45:07.251949: 
2025-10-05 14:45:07.252289: Epoch 147
2025-10-05 14:45:07.252466: Current learning rate: 0.0003
2025-10-05 14:45:53.074508: Validation loss did not improve from -0.11245. Patience: 137/50
2025-10-05 14:45:53.074914: train_loss -0.9581
2025-10-05 14:45:53.075067: val_loss 0.2665
2025-10-05 14:45:53.075213: Pseudo dice [np.float32(0.5782)]
2025-10-05 14:45:53.075351: Epoch time: 45.82 s
2025-10-05 14:45:53.713032: 
2025-10-05 14:45:53.713382: Epoch 148
2025-10-05 14:45:53.713615: Current learning rate: 0.00021
2025-10-05 14:46:39.532560: Validation loss did not improve from -0.11245. Patience: 138/50
2025-10-05 14:46:39.533137: train_loss -0.9594
2025-10-05 14:46:39.533288: val_loss 0.3651
2025-10-05 14:46:39.533430: Pseudo dice [np.float32(0.5576)]
2025-10-05 14:46:39.533566: Epoch time: 45.82 s
2025-10-05 14:46:40.176860: 
2025-10-05 14:46:40.177134: Epoch 149
2025-10-05 14:46:40.177308: Current learning rate: 0.00011
2025-10-05 14:47:26.062201: Validation loss did not improve from -0.11245. Patience: 139/50
2025-10-05 14:47:26.062618: train_loss -0.9593
2025-10-05 14:47:26.062788: val_loss 0.3183
2025-10-05 14:47:26.063003: Pseudo dice [np.float32(0.5549)]
2025-10-05 14:47:26.063177: Epoch time: 45.89 s
2025-10-05 14:47:27.245298: Training done.
2025-10-05 14:47:27.259362: Using splits from existing split file: /nfs/erelab001/shared/Computational_Group/Naravich/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset310_nnInteractive_Calcium_OCT_CrossValidation/splits_final_20.json
2025-10-05 14:47:27.259693: The split file contains 5 splits.
2025-10-05 14:47:27.260098: Desired fold for training: 1
2025-10-05 14:47:27.260376: This split has 1 training and 7 validation cases.
2025-10-05 14:47:27.260700: predicting 101-019
2025-10-05 14:47:27.264069: 101-019, shape torch.Size([1, 375, 498, 498]), rank 0
2025-10-05 14:48:13.816788: predicting 101-045
2025-10-05 14:48:13.826753: 101-045, shape torch.Size([1, 375, 498, 498]), rank 0
2025-10-05 14:48:47.574294: predicting 106-002
2025-10-05 14:48:47.584883: 106-002, shape torch.Size([1, 539, 498, 498]), rank 0
2025-10-05 14:49:35.542242: predicting 401-004
2025-10-05 14:49:35.555999: 401-004, shape torch.Size([1, 375, 498, 498]), rank 0
2025-10-05 14:50:09.417006: predicting 701-013
2025-10-05 14:50:09.427214: 701-013, shape torch.Size([1, 375, 498, 498]), rank 0
2025-10-05 14:50:43.272911: predicting 704-003
2025-10-05 14:50:43.283429: 704-003, shape torch.Size([1, 375, 498, 498]), rank 0
2025-10-05 14:51:17.066037: predicting 706-005
2025-10-05 14:51:17.076269: 706-005, shape torch.Size([1, 375, 498, 498]), rank 0
2025-10-05 14:52:04.367359: Validation complete
2025-10-05 14:52:04.367667: Mean Validation Dice:  0.5156835627567884
Finished training fold 1 saving to /nfs/erelab001/shared/Computational_Group/Naravich/datasets/nnUNet_Datasets/nnUNet_results/Dataset310_nnInteractive_Calcium_OCT_CrossValidation/nnUNetTrainerScaleAnalysis20__nnUNetPlans__3d_32x160x128_b10/fold_1_No_Pretrained
