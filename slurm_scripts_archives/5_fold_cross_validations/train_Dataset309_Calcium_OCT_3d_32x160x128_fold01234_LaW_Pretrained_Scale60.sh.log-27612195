/home/gridsan/nchutisilp/.conda/envs/nnunet/bin/python
Starting training with CONFIG=3d_32x160x128_b10, DATASET_ID=309, TRAINER=nnUNetTrainerScaleAnalysis60

############################
INFO: You are using the old nnU-Net default plans. We have updated our recommendations. Please consider using those instead! Read more here: https://github.com/MIC-DKFZ/nnUNet/blob/master/documentation/resenc_presets.md
############################

Using device: cuda:0

#######################################################################
Please cite the following paper when using nnU-Net:
Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.
#######################################################################

2024-12-17 13:50:08.629629: Using torch.compile...

############################
INFO: You are using the old nnU-Net default plans. We have updated our recommendations. Please consider using those instead! Read more here: https://github.com/MIC-DKFZ/nnUNet/blob/master/documentation/resenc_presets.md
############################

Using device: cuda:0

#######################################################################
Please cite the following paper when using nnU-Net:
Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.
#######################################################################

2024-12-17 13:50:08.629807: Using torch.compile...
/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
################### Loading pretrained weights from file  /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_results/Dataset300_Lumen_and_Wall_OCT/nnUNetTrainer__nnUNetPreprocessPlans__3d_fullres/fold_all/checkpoint_best.pth ###################
Below is the list of overlapping blocks in pretrained model and nnUNet architecture:
encoder.stages.0.0.convs.0.conv.weight shape torch.Size([32, 1, 3, 3, 3])
encoder.stages.0.0.convs.0.conv.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.norm.weight shape torch.Size([32])
encoder.stages.0.0.convs.0.norm.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.0.weight shape torch.Size([32, 1, 3, 3, 3])
encoder.stages.0.0.convs.0.all_modules.0.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.1.weight shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.1.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
encoder.stages.0.0.convs.1.conv.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.norm.weight shape torch.Size([32])
encoder.stages.0.0.convs.1.norm.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
encoder.stages.0.0.convs.1.all_modules.0.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.1.weight shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.1.bias shape torch.Size([32])
encoder.stages.1.0.convs.0.conv.weight shape torch.Size([64, 32, 3, 3, 3])
encoder.stages.1.0.convs.0.conv.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.norm.weight shape torch.Size([64])
encoder.stages.1.0.convs.0.norm.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.0.weight shape torch.Size([64, 32, 3, 3, 3])
encoder.stages.1.0.convs.0.all_modules.0.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.1.weight shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.1.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
encoder.stages.1.0.convs.1.conv.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.norm.weight shape torch.Size([64])
encoder.stages.1.0.convs.1.norm.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
encoder.stages.1.0.convs.1.all_modules.0.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.1.weight shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.1.bias shape torch.Size([64])
encoder.stages.2.0.convs.0.conv.weight shape torch.Size([128, 64, 3, 3, 3])
encoder.stages.2.0.convs.0.conv.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.norm.weight shape torch.Size([128])
encoder.stages.2.0.convs.0.norm.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.0.weight shape torch.Size([128, 64, 3, 3, 3])
encoder.stages.2.0.convs.0.all_modules.0.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.1.weight shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.1.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
encoder.stages.2.0.convs.1.conv.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.norm.weight shape torch.Size([128])
encoder.stages.2.0.convs.1.norm.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
encoder.stages.2.0.convs.1.all_modules.0.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.1.weight shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.1.bias shape torch.Size([128])
encoder.stages.3.0.convs.0.conv.weight shape torch.Size([256, 128, 3, 3, 3])
encoder.stages.3.0.convs.0.conv.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.norm.weight shape torch.Size([256])
encoder.stages.3.0.convs.0.norm.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.0.weight shape torch.Size([256, 128, 3, 3, 3])
encoder.stages.3.0.convs.0.all_modules.0.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.1.weight shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.1.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
encoder.stages.3.0.convs.1.conv.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.norm.weight shape torch.Size([256])
encoder.stages.3.0.convs.1.norm.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
encoder.stages.3.0.convs.1.all_modules.0.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.1.weight shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.1.bias shape torch.Size([256])
encoder.stages.4.0.convs.0.conv.weight shape torch.Size([320, 256, 3, 3, 3])
encoder.stages.4.0.convs.0.conv.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.norm.weight shape torch.Size([320])
encoder.stages.4.0.convs.0.norm.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.0.weight shape torch.Size([320, 256, 3, 3, 3])
encoder.stages.4.0.convs.0.all_modules.0.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.1.weight shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.1.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.4.0.convs.1.conv.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.norm.weight shape torch.Size([320])
encoder.stages.4.0.convs.1.norm.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.4.0.convs.1.all_modules.0.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.1.weight shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.1.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.0.conv.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.norm.weight shape torch.Size([320])
encoder.stages.5.0.convs.0.norm.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.0.all_modules.0.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.1.weight shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.1.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.1.conv.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.norm.weight shape torch.Size([320])
encoder.stages.5.0.convs.1.norm.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.1.all_modules.0.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.1.weight shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.0.0.convs.0.conv.weight shape torch.Size([32, 1, 3, 3, 3])
decoder.encoder.stages.0.0.convs.0.conv.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.norm.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.norm.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.0.weight shape torch.Size([32, 1, 3, 3, 3])
decoder.encoder.stages.0.0.convs.0.all_modules.0.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.1.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.1.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.encoder.stages.0.0.convs.1.conv.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.norm.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.norm.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.encoder.stages.0.0.convs.1.all_modules.0.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.1.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.1.bias shape torch.Size([32])
decoder.encoder.stages.1.0.convs.0.conv.weight shape torch.Size([64, 32, 3, 3, 3])
decoder.encoder.stages.1.0.convs.0.conv.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.norm.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.norm.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.0.weight shape torch.Size([64, 32, 3, 3, 3])
decoder.encoder.stages.1.0.convs.0.all_modules.0.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.1.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.1.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.encoder.stages.1.0.convs.1.conv.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.norm.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.norm.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.encoder.stages.1.0.convs.1.all_modules.0.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.1.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.1.bias shape torch.Size([64])
decoder.encoder.stages.2.0.convs.0.conv.weight shape torch.Size([128, 64, 3, 3, 3])
decoder.encoder.stages.2.0.convs.0.conv.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.norm.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.norm.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.0.weight shape torch.Size([128, 64, 3, 3, 3])
decoder.encoder.stages.2.0.convs.0.all_modules.0.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.1.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.1.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.encoder.stages.2.0.convs.1.conv.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.norm.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.norm.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.encoder.stages.2.0.convs.1.all_modules.0.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.1.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.1.bias shape torch.Size([128])
decoder.encoder.stages.3.0.convs.0.conv.weight shape torch.Size([256, 128, 3, 3, 3])
decoder.encoder.stages.3.0.convs.0.conv.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.norm.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.norm.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.0.weight shape torch.Size([256, 128, 3, 3, 3])
decoder.encoder.stages.3.0.convs.0.all_modules.0.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.1.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.1.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.encoder.stages.3.0.convs.1.conv.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.norm.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.norm.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.encoder.stages.3.0.convs.1.all_modules.0.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.1.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.1.bias shape torch.Size([256])
decoder.encoder.stages.4.0.convs.0.conv.weight shape torch.Size([320, 256, 3, 3, 3])
decoder.encoder.stages.4.0.convs.0.conv.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.norm.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.norm.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.0.weight shape torch.Size([320, 256, 3, 3, 3])
decoder.encoder.stages.4.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.4.0.convs.1.conv.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.norm.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.norm.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.4.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.0.conv.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.norm.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.norm.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.1.conv.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.norm.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.norm.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.stages.0.convs.0.conv.weight shape torch.Size([320, 640, 3, 3, 3])
decoder.stages.0.convs.0.conv.bias shape torch.Size([320])
decoder.stages.0.convs.0.norm.weight shape torch.Size([320])
decoder.stages.0.convs.0.norm.bias shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.0.weight shape torch.Size([320, 640, 3, 3, 3])
decoder.stages.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.stages.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.stages.0.convs.1.conv.bias shape torch.Size([320])
decoder.stages.0.convs.1.norm.weight shape torch.Size([320])
decoder.stages.0.convs.1.norm.bias shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.stages.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.stages.1.convs.0.conv.weight shape torch.Size([256, 512, 3, 3, 3])
decoder.stages.1.convs.0.conv.bias shape torch.Size([256])
decoder.stages.1.convs.0.norm.weight shape torch.Size([256])
decoder.stages.1.convs.0.norm.bias shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.0.weight shape torch.Size([256, 512, 3, 3, 3])
decoder.stages.1.convs.0.all_modules.0.bias shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.1.weight shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.1.bias shape torch.Size([256])
decoder.stages.1.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.stages.1.convs.1.conv.bias shape torch.Size([256])
decoder.stages.1.convs.1.norm.weight shape torch.Size([256])
decoder.stages.1.convs.1.norm.bias shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.stages.1.convs.1.all_modules.0.bias shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.1.weight shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.1.bias shape torch.Size([256])
decoder.stages.2.convs.0.conv.weight shape torch.Size([128, 256, 3, 3, 3])
decoder.stages.2.convs.0.conv.bias shape torch.Size([128])
decoder.stages.2.convs.0.norm.weight shape torch.Size([128])
decoder.stages.2.convs.0.norm.bias shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.0.weight shape torch.Size([128, 256, 3, 3, 3])
decoder.stages.2.convs.0.all_modules.0.bias shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.1.weight shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.1.bias shape torch.Size([128])
decoder.stages.2.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.stages.2.convs.1.conv.bias shape torch.Size([128])
decoder.stages.2.convs.1.norm.weight shape torch.Size([128])
decoder.stages.2.convs.1.norm.bias shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.stages.2.convs.1.all_modules.0.bias shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.1.weight shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.1.bias shape torch.Size([128])
decoder.stages.3.convs.0.conv.weight shape torch.Size([64, 128, 3, 3, 3])
decoder.stages.3.convs.0.conv.bias shape torch.Size([64])
decoder.stages.3.convs.0.norm.weight shape torch.Size([64])
decoder.stages.3.convs.0.norm.bias shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.0.weight shape torch.Size([64, 128, 3, 3, 3])
decoder.stages.3.convs.0.all_modules.0.bias shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.1.weight shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.1.bias shape torch.Size([64])
decoder.stages.3.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.stages.3.convs.1.conv.bias shape torch.Size([64])
decoder.stages.3.convs.1.norm.weight shape torch.Size([64])
decoder.stages.3.convs.1.norm.bias shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.stages.3.convs.1.all_modules.0.bias shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.1.weight shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.1.bias shape torch.Size([64])
decoder.stages.4.convs.0.conv.weight shape torch.Size([32, 64, 3, 3, 3])
decoder.stages.4.convs.0.conv.bias shape torch.Size([32])
decoder.stages.4.convs.0.norm.weight shape torch.Size([32])
decoder.stages.4.convs.0.norm.bias shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.0.weight shape torch.Size([32, 64, 3, 3, 3])
decoder.stages.4.convs.0.all_modules.0.bias shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.1.weight shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.1.bias shape torch.Size([32])
decoder.stages.4.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.stages.4.convs.1.conv.bias shape torch.Size([32])
decoder.stages.4.convs.1.norm.weight shape torch.Size([32])
decoder.stages.4.convs.1.norm.bias shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.stages.4.convs.1.all_modules.0.bias shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.1.weight shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.1.bias shape torch.Size([32])
decoder.transpconvs.0.weight shape torch.Size([320, 320, 1, 2, 2])
decoder.transpconvs.0.bias shape torch.Size([320])
decoder.transpconvs.1.weight shape torch.Size([320, 256, 2, 2, 2])
decoder.transpconvs.1.bias shape torch.Size([256])
decoder.transpconvs.2.weight shape torch.Size([256, 128, 2, 2, 2])
decoder.transpconvs.2.bias shape torch.Size([128])
decoder.transpconvs.3.weight shape torch.Size([128, 64, 2, 2, 2])
decoder.transpconvs.3.bias shape torch.Size([64])
decoder.transpconvs.4.weight shape torch.Size([64, 32, 2, 2, 2])
decoder.transpconvs.4.bias shape torch.Size([32])
################### Done ###################
2024-12-17 13:50:10.900821: do_dummy_2d_data_aug: True
2024-12-17 13:50:10.953241: Using splits from existing split file: /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset309_Sohee_Calcium_OCT_CrossValidation/splits_final_60.json
2024-12-17 13:50:10.966999: The split file contains 5 splits.
2024-12-17 13:50:10.968256: Desired fold for training: 1
2024-12-17 13:50:10.968955: This split has 4 training and 5 validation cases.
################### Loading pretrained weights from file  /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_results/Dataset300_Lumen_and_Wall_OCT/nnUNetTrainer__nnUNetPreprocessPlans__3d_fullres/fold_all/checkpoint_best.pth ###################
Below is the list of overlapping blocks in pretrained model and nnUNet architecture:
encoder.stages.0.0.convs.0.conv.weight shape torch.Size([32, 1, 3, 3, 3])
encoder.stages.0.0.convs.0.conv.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.norm.weight shape torch.Size([32])
encoder.stages.0.0.convs.0.norm.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.0.weight shape torch.Size([32, 1, 3, 3, 3])
encoder.stages.0.0.convs.0.all_modules.0.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.1.weight shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.1.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
encoder.stages.0.0.convs.1.conv.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.norm.weight shape torch.Size([32])
encoder.stages.0.0.convs.1.norm.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
encoder.stages.0.0.convs.1.all_modules.0.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.1.weight shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.1.bias shape torch.Size([32])
encoder.stages.1.0.convs.0.conv.weight shape torch.Size([64, 32, 3, 3, 3])
encoder.stages.1.0.convs.0.conv.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.norm.weight shape torch.Size([64])
encoder.stages.1.0.convs.0.norm.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.0.weight shape torch.Size([64, 32, 3, 3, 3])
encoder.stages.1.0.convs.0.all_modules.0.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.1.weight shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.1.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
encoder.stages.1.0.convs.1.conv.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.norm.weight shape torch.Size([64])
encoder.stages.1.0.convs.1.norm.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
encoder.stages.1.0.convs.1.all_modules.0.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.1.weight shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.1.bias shape torch.Size([64])
encoder.stages.2.0.convs.0.conv.weight shape torch.Size([128, 64, 3, 3, 3])
encoder.stages.2.0.convs.0.conv.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.norm.weight shape torch.Size([128])
encoder.stages.2.0.convs.0.norm.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.0.weight shape torch.Size([128, 64, 3, 3, 3])
encoder.stages.2.0.convs.0.all_modules.0.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.1.weight shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.1.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
encoder.stages.2.0.convs.1.conv.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.norm.weight shape torch.Size([128])
encoder.stages.2.0.convs.1.norm.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
encoder.stages.2.0.convs.1.all_modules.0.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.1.weight shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.1.bias shape torch.Size([128])
encoder.stages.3.0.convs.0.conv.weight shape torch.Size([256, 128, 3, 3, 3])
encoder.stages.3.0.convs.0.conv.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.norm.weight shape torch.Size([256])
encoder.stages.3.0.convs.0.norm.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.0.weight shape torch.Size([256, 128, 3, 3, 3])
encoder.stages.3.0.convs.0.all_modules.0.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.1.weight shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.1.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
encoder.stages.3.0.convs.1.conv.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.norm.weight shape torch.Size([256])
encoder.stages.3.0.convs.1.norm.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
encoder.stages.3.0.convs.1.all_modules.0.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.1.weight shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.1.bias shape torch.Size([256])
encoder.stages.4.0.convs.0.conv.weight shape torch.Size([320, 256, 3, 3, 3])
encoder.stages.4.0.convs.0.conv.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.norm.weight shape torch.Size([320])
encoder.stages.4.0.convs.0.norm.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.0.weight shape torch.Size([320, 256, 3, 3, 3])
encoder.stages.4.0.convs.0.all_modules.0.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.1.weight shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.1.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.4.0.convs.1.conv.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.norm.weight shape torch.Size([320])
encoder.stages.4.0.convs.1.norm.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.4.0.convs.1.all_modules.0.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.1.weight shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.1.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.0.conv.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.norm.weight shape torch.Size([320])
encoder.stages.5.0.convs.0.norm.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.0.all_modules.0.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.1.weight shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.1.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.1.conv.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.norm.weight shape torch.Size([320])
encoder.stages.5.0.convs.1.norm.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.1.all_modules.0.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.1.weight shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.0.0.convs.0.conv.weight shape torch.Size([32, 1, 3, 3, 3])
decoder.encoder.stages.0.0.convs.0.conv.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.norm.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.norm.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.0.weight shape torch.Size([32, 1, 3, 3, 3])
decoder.encoder.stages.0.0.convs.0.all_modules.0.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.1.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.1.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.encoder.stages.0.0.convs.1.conv.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.norm.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.norm.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.encoder.stages.0.0.convs.1.all_modules.0.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.1.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.1.bias shape torch.Size([32])
decoder.encoder.stages.1.0.convs.0.conv.weight shape torch.Size([64, 32, 3, 3, 3])
decoder.encoder.stages.1.0.convs.0.conv.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.norm.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.norm.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.0.weight shape torch.Size([64, 32, 3, 3, 3])
decoder.encoder.stages.1.0.convs.0.all_modules.0.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.1.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.1.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.encoder.stages.1.0.convs.1.conv.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.norm.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.norm.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.encoder.stages.1.0.convs.1.all_modules.0.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.1.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.1.bias shape torch.Size([64])
decoder.encoder.stages.2.0.convs.0.conv.weight shape torch.Size([128, 64, 3, 3, 3])
decoder.encoder.stages.2.0.convs.0.conv.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.norm.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.norm.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.0.weight shape torch.Size([128, 64, 3, 3, 3])
decoder.encoder.stages.2.0.convs.0.all_modules.0.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.1.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.1.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.encoder.stages.2.0.convs.1.conv.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.norm.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.norm.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.encoder.stages.2.0.convs.1.all_modules.0.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.1.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.1.bias shape torch.Size([128])
decoder.encoder.stages.3.0.convs.0.conv.weight shape torch.Size([256, 128, 3, 3, 3])
decoder.encoder.stages.3.0.convs.0.conv.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.norm.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.norm.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.0.weight shape torch.Size([256, 128, 3, 3, 3])
decoder.encoder.stages.3.0.convs.0.all_modules.0.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.1.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.1.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.encoder.stages.3.0.convs.1.conv.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.norm.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.norm.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.encoder.stages.3.0.convs.1.all_modules.0.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.1.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.1.bias shape torch.Size([256])
decoder.encoder.stages.4.0.convs.0.conv.weight shape torch.Size([320, 256, 3, 3, 3])
decoder.encoder.stages.4.0.convs.0.conv.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.norm.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.norm.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.0.weight shape torch.Size([320, 256, 3, 3, 3])
decoder.encoder.stages.4.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.4.0.convs.1.conv.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.norm.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.norm.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.4.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.0.conv.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.norm.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.norm.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.1.conv.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.norm.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.norm.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.stages.0.convs.0.conv.weight shape torch.Size([320, 640, 3, 3, 3])
decoder.stages.0.convs.0.conv.bias shape torch.Size([320])
decoder.stages.0.convs.0.norm.weight shape torch.Size([320])
decoder.stages.0.convs.0.norm.bias shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.0.weight shape torch.Size([320, 640, 3, 3, 3])
decoder.stages.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.stages.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.stages.0.convs.1.conv.bias shape torch.Size([320])
decoder.stages.0.convs.1.norm.weight shape torch.Size([320])
decoder.stages.0.convs.1.norm.bias shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.stages.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.stages.1.convs.0.conv.weight shape torch.Size([256, 512, 3, 3, 3])
decoder.stages.1.convs.0.conv.bias shape torch.Size([256])
decoder.stages.1.convs.0.norm.weight shape torch.Size([256])
decoder.stages.1.convs.0.norm.bias shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.0.weight shape torch.Size([256, 512, 3, 3, 3])
decoder.stages.1.convs.0.all_modules.0.bias shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.1.weight shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.1.bias shape torch.Size([256])
decoder.stages.1.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.stages.1.convs.1.conv.bias shape torch.Size([256])
decoder.stages.1.convs.1.norm.weight shape torch.Size([256])
decoder.stages.1.convs.1.norm.bias shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.stages.1.convs.1.all_modules.0.bias shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.1.weight shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.1.bias shape torch.Size([256])
decoder.stages.2.convs.0.conv.weight shape torch.Size([128, 256, 3, 3, 3])
decoder.stages.2.convs.0.conv.bias shape torch.Size([128])
decoder.stages.2.convs.0.norm.weight shape torch.Size([128])
decoder.stages.2.convs.0.norm.bias shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.0.weight shape torch.Size([128, 256, 3, 3, 3])
decoder.stages.2.convs.0.all_modules.0.bias shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.1.weight shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.1.bias shape torch.Size([128])
decoder.stages.2.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.stages.2.convs.1.conv.bias shape torch.Size([128])
decoder.stages.2.convs.1.norm.weight shape torch.Size([128])
decoder.stages.2.convs.1.norm.bias shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.stages.2.convs.1.all_modules.0.bias shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.1.weight shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.1.bias shape torch.Size([128])
decoder.stages.3.convs.0.conv.weight shape torch.Size([64, 128, 3, 3, 3])
decoder.stages.3.convs.0.conv.bias shape torch.Size([64])
decoder.stages.3.convs.0.norm.weight shape torch.Size([64])
decoder.stages.3.convs.0.norm.bias shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.0.weight shape torch.Size([64, 128, 3, 3, 3])
decoder.stages.3.convs.0.all_modules.0.bias shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.1.weight shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.1.bias shape torch.Size([64])
decoder.stages.3.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.stages.3.convs.1.conv.bias shape torch.Size([64])
decoder.stages.3.convs.1.norm.weight shape torch.Size([64])
decoder.stages.3.convs.1.norm.bias shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.stages.3.convs.1.all_modules.0.bias shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.1.weight shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.1.bias shape torch.Size([64])
decoder.stages.4.convs.0.conv.weight shape torch.Size([32, 64, 3, 3, 3])
decoder.stages.4.convs.0.conv.bias shape torch.Size([32])
decoder.stages.4.convs.0.norm.weight shape torch.Size([32])
decoder.stages.4.convs.0.norm.bias shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.0.weight shape torch.Size([32, 64, 3, 3, 3])
decoder.stages.4.convs.0.all_modules.0.bias shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.1.weight shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.1.bias shape torch.Size([32])
decoder.stages.4.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.stages.4.convs.1.conv.bias shape torch.Size([32])
decoder.stages.4.convs.1.norm.weight shape torch.Size([32])
decoder.stages.4.convs.1.norm.bias shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.stages.4.convs.1.all_modules.0.bias shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.1.weight shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.1.bias shape torch.Size([32])
decoder.transpconvs.0.weight shape torch.Size([320, 320, 1, 2, 2])
decoder.transpconvs.0.bias shape torch.Size([320])
decoder.transpconvs.1.weight shape torch.Size([320, 256, 2, 2, 2])
decoder.transpconvs.1.bias shape torch.Size([256])
decoder.transpconvs.2.weight shape torch.Size([256, 128, 2, 2, 2])
decoder.transpconvs.2.bias shape torch.Size([128])
decoder.transpconvs.3.weight shape torch.Size([128, 64, 2, 2, 2])
decoder.transpconvs.3.bias shape torch.Size([64])
decoder.transpconvs.4.weight shape torch.Size([64, 32, 2, 2, 2])
decoder.transpconvs.4.bias shape torch.Size([32])
################### Done ###################
2024-12-17 13:50:10.900820: do_dummy_2d_data_aug: True
2024-12-17 13:50:10.953177: Using splits from existing split file: /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset309_Sohee_Calcium_OCT_CrossValidation/splits_final_60.json
2024-12-17 13:50:10.966363: The split file contains 5 splits.
2024-12-17 13:50:10.968156: Desired fold for training: 0
2024-12-17 13:50:10.968751: This split has 4 training and 4 validation cases.
using pin_memory on device 0
using pin_memory on device 0
using pin_memory on device 0

This is the configuration used by this training:
Configuration name: 3d_32x160x128_b10
 {'data_identifier': 'nnUNetPlans_3d_fullres', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 10, 'patch_size': [32, 160, 128], 'median_image_size_in_voxels': [375.0, 498.0, 498.0], 'spacing': [1.0, 1.0, 1.0], 'normalization_schemes': ['ZScoreNormalization'], 'use_mask_for_norm': [False], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'architecture': {'network_class_name': 'dynamic_network_architectures.architectures.unet.PlainConvUNet', 'arch_kwargs': {'n_stages': 6, 'features_per_stage': [32, 64, 128, 256, 320, 320], 'conv_op': 'torch.nn.modules.conv.Conv3d', 'kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]], 'strides': [[1, 1, 1], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [1, 2, 2]], 'n_conv_per_stage': [2, 2, 2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2, 2, 2], 'conv_bias': True, 'norm_op': 'torch.nn.modules.instancenorm.InstanceNorm3d', 'norm_op_kwargs': {'eps': 1e-05, 'affine': True}, 'dropout_op': None, 'dropout_op_kwargs': None, 'nonlin': 'torch.nn.LeakyReLU', 'nonlin_kwargs': {'inplace': True}, 'deep_supervision': True}, '_kw_requires_import': ['conv_op', 'norm_op', 'dropout_op', 'nonlin']}, 'batch_dice': True, 'inherits_from': '3d_fullres'} 

These are the global plan.json settings:
 {'dataset_name': 'Dataset309_Sohee_Calcium_OCT_CrossValidation', 'plans_name': 'nnUNetPlans', 'original_median_spacing_after_transp': [1.0, 1.0, 1.0], 'original_median_shape_after_transp': [375, 498, 498], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 0.4977976381778717, 'mean': 0.13507525622844696, 'median': 0.09599608182907104, 'min': 0.0, 'percentile_00_5': 0.014754901640117168, 'percentile_99_5': 0.4977976381778717, 'std': 0.12025152146816254}}} 

2024-12-17 13:50:31.163611: unpacking dataset...
using pin_memory on device 0

This is the configuration used by this training:
Configuration name: 3d_32x160x128_b10
 {'data_identifier': 'nnUNetPlans_3d_fullres', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 10, 'patch_size': [32, 160, 128], 'median_image_size_in_voxels': [375.0, 498.0, 498.0], 'spacing': [1.0, 1.0, 1.0], 'normalization_schemes': ['ZScoreNormalization'], 'use_mask_for_norm': [False], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'architecture': {'network_class_name': 'dynamic_network_architectures.architectures.unet.PlainConvUNet', 'arch_kwargs': {'n_stages': 6, 'features_per_stage': [32, 64, 128, 256, 320, 320], 'conv_op': 'torch.nn.modules.conv.Conv3d', 'kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]], 'strides': [[1, 1, 1], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [1, 2, 2]], 'n_conv_per_stage': [2, 2, 2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2, 2, 2], 'conv_bias': True, 'norm_op': 'torch.nn.modules.instancenorm.InstanceNorm3d', 'norm_op_kwargs': {'eps': 1e-05, 'affine': True}, 'dropout_op': None, 'dropout_op_kwargs': None, 'nonlin': 'torch.nn.LeakyReLU', 'nonlin_kwargs': {'inplace': True}, 'deep_supervision': True}, '_kw_requires_import': ['conv_op', 'norm_op', 'dropout_op', 'nonlin']}, 'batch_dice': True, 'inherits_from': '3d_fullres'} 

These are the global plan.json settings:
 {'dataset_name': 'Dataset309_Sohee_Calcium_OCT_CrossValidation', 'plans_name': 'nnUNetPlans', 'original_median_spacing_after_transp': [1.0, 1.0, 1.0], 'original_median_shape_after_transp': [375, 498, 498], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 0.4977976381778717, 'mean': 0.13507525622844696, 'median': 0.09599608182907104, 'min': 0.0, 'percentile_00_5': 0.014754901640117168, 'percentile_99_5': 0.4977976381778717, 'std': 0.12025152146816254}}} 

2024-12-17 13:50:31.344031: unpacking dataset...
2024-12-17 13:50:37.401558: unpacking done...
2024-12-17 13:50:37.470894: Unable to plot network architecture: nnUNet_compile is enabled!
2024-12-17 13:50:37.529958: 
2024-12-17 13:50:37.530914: Epoch 0
2024-12-17 13:50:37.531698: Current learning rate: 0.01
2024-12-17 13:55:32.451103: Validation loss improved from 1000.00000 to -0.35916! Patience: 0/50
2024-12-17 13:55:32.462535: train_loss -0.2915
2024-12-17 13:55:32.463677: val_loss -0.3592
2024-12-17 13:55:32.464527: Pseudo dice [0.6612]
2024-12-17 13:55:32.465334: Epoch time: 294.92 s
2024-12-17 13:55:32.466256: Yayy! New best EMA pseudo Dice: 0.6612
2024-12-17 13:55:34.249566: 
2024-12-17 13:55:34.250743: Epoch 1
2024-12-17 13:55:34.251792: Current learning rate: 0.00994
2024-12-17 13:59:05.547678: Validation loss improved from -0.35916 to -0.40723! Patience: 0/50
2024-12-17 13:59:05.548664: train_loss -0.4846
2024-12-17 13:59:05.549699: val_loss -0.4072
2024-12-17 13:59:05.550479: Pseudo dice [0.6915]
2024-12-17 13:59:05.551281: Epoch time: 211.3 s
2024-12-17 13:59:05.552039: Yayy! New best EMA pseudo Dice: 0.6642
2024-12-17 13:59:07.329268: 
2024-12-17 13:59:07.330399: Epoch 2
2024-12-17 13:59:07.331285: Current learning rate: 0.00988
2024-12-17 14:02:38.233084: Validation loss improved from -0.40723 to -0.46065! Patience: 0/50
2024-12-17 14:02:38.233884: train_loss -0.5449
2024-12-17 14:02:38.234873: val_loss -0.4606
2024-12-17 14:02:38.235780: Pseudo dice [0.7118]
2024-12-17 14:02:38.236758: Epoch time: 210.91 s
2024-12-17 14:02:38.237759: Yayy! New best EMA pseudo Dice: 0.669
2024-12-17 14:02:40.052138: 
2024-12-17 14:02:40.053186: Epoch 3
2024-12-17 14:02:40.054137: Current learning rate: 0.00982
2024-12-17 14:06:26.391449: Validation loss did not improve from -0.46065. Patience: 1/50
2024-12-17 14:06:26.392233: train_loss -0.563
2024-12-17 14:06:26.393046: val_loss -0.4162
2024-12-17 14:06:26.393831: Pseudo dice [0.7]
2024-12-17 14:06:26.394548: Epoch time: 226.34 s
2024-12-17 14:06:26.395346: Yayy! New best EMA pseudo Dice: 0.6721
2024-12-17 14:06:28.207485: 
2024-12-17 14:06:28.208489: Epoch 4
2024-12-17 14:06:28.209187: Current learning rate: 0.00976
2024-12-17 14:10:04.638327: Validation loss did not improve from -0.46065. Patience: 2/50
2024-12-17 14:10:04.639112: train_loss -0.5767
2024-12-17 14:10:04.639901: val_loss -0.4284
2024-12-17 14:10:04.640632: Pseudo dice [0.7054]
2024-12-17 14:10:04.641515: Epoch time: 216.43 s
2024-12-17 14:10:04.985527: Yayy! New best EMA pseudo Dice: 0.6754
2024-12-17 14:10:06.794288: 
2024-12-17 14:10:06.795296: Epoch 5
2024-12-17 14:10:06.796079: Current learning rate: 0.0097
2024-12-17 14:13:49.449936: Validation loss improved from -0.46065 to -0.46327! Patience: 2/50
2024-12-17 14:13:49.450554: train_loss -0.6039
2024-12-17 14:13:49.451350: val_loss -0.4633
2024-12-17 14:13:49.452089: Pseudo dice [0.7199]
2024-12-17 14:13:49.452912: Epoch time: 222.66 s
2024-12-17 14:13:49.453789: Yayy! New best EMA pseudo Dice: 0.6799
2024-12-17 14:13:51.270098: 
2024-12-17 14:13:51.271257: Epoch 6
2024-12-17 14:13:51.272189: Current learning rate: 0.00964
2024-12-17 14:17:31.888355: Validation loss did not improve from -0.46327. Patience: 1/50
2024-12-17 14:17:31.889018: train_loss -0.6083
2024-12-17 14:17:31.889737: val_loss -0.4625
2024-12-17 14:17:31.890345: Pseudo dice [0.7197]
2024-12-17 14:17:31.890952: Epoch time: 220.62 s
2024-12-17 14:17:31.891569: Yayy! New best EMA pseudo Dice: 0.6839
2024-12-17 14:17:33.678606: 
2024-12-17 14:17:33.679925: Epoch 7
2024-12-17 14:17:33.680981: Current learning rate: 0.00958
2024-12-17 14:21:17.525667: Validation loss improved from -0.46327 to -0.51301! Patience: 1/50
2024-12-17 14:21:17.526557: train_loss -0.6331
2024-12-17 14:21:17.527294: val_loss -0.513
2024-12-17 14:21:17.527912: Pseudo dice [0.7391]
2024-12-17 14:21:17.528714: Epoch time: 223.85 s
2024-12-17 14:21:17.529315: Yayy! New best EMA pseudo Dice: 0.6894
2024-12-17 14:21:19.447074: 
2024-12-17 14:21:19.448467: Epoch 8
2024-12-17 14:21:19.449269: Current learning rate: 0.00952
2024-12-17 14:25:07.344475: Validation loss did not improve from -0.51301. Patience: 1/50
2024-12-17 14:25:07.345491: train_loss -0.6379
2024-12-17 14:25:07.346331: val_loss -0.4591
2024-12-17 14:25:07.347286: Pseudo dice [0.7099]
2024-12-17 14:25:07.348063: Epoch time: 227.9 s
2024-12-17 14:25:07.348933: Yayy! New best EMA pseudo Dice: 0.6914
2024-12-17 14:25:09.678806: 
2024-12-17 14:25:09.680079: Epoch 9
2024-12-17 14:25:09.680837: Current learning rate: 0.00946
2024-12-17 14:28:58.219827: Validation loss did not improve from -0.51301. Patience: 2/50
2024-12-17 14:28:58.220687: train_loss -0.6586
2024-12-17 14:28:58.221550: val_loss -0.4525
2024-12-17 14:28:58.222355: Pseudo dice [0.7121]
2024-12-17 14:28:58.223149: Epoch time: 228.54 s
2024-12-17 14:28:58.657625: Yayy! New best EMA pseudo Dice: 0.6935
2024-12-17 14:29:00.405716: 
2024-12-17 14:29:00.407118: Epoch 10
2024-12-17 14:29:00.408010: Current learning rate: 0.0094
2024-12-17 14:32:22.548408: Validation loss did not improve from -0.51301. Patience: 3/50
2024-12-17 14:32:22.549250: train_loss -0.6689
2024-12-17 14:32:22.550042: val_loss -0.4778
2024-12-17 14:32:22.550693: Pseudo dice [0.728]
2024-12-17 14:32:22.551426: Epoch time: 202.14 s
2024-12-17 14:32:22.552120: Yayy! New best EMA pseudo Dice: 0.697
2024-12-17 14:32:24.333852: 
2024-12-17 14:32:24.335004: Epoch 11
2024-12-17 14:32:24.335858: Current learning rate: 0.00934
2024-12-17 14:36:03.356768: Validation loss did not improve from -0.51301. Patience: 4/50
2024-12-17 14:36:03.357608: train_loss -0.6638
2024-12-17 14:36:03.358549: val_loss -0.4662
2024-12-17 14:36:03.359318: Pseudo dice [0.7159]
2024-12-17 14:36:03.359911: Epoch time: 219.03 s
2024-12-17 14:36:03.360571: Yayy! New best EMA pseudo Dice: 0.6988
2024-12-17 14:36:05.218537: 
2024-12-17 14:36:05.219421: Epoch 12
2024-12-17 14:36:05.220170: Current learning rate: 0.00928
2024-12-17 14:39:59.890728: Validation loss did not improve from -0.51301. Patience: 5/50
2024-12-17 14:39:59.891780: train_loss -0.6744
2024-12-17 14:39:59.892613: val_loss -0.4954
2024-12-17 14:39:59.893360: Pseudo dice [0.73]
2024-12-17 14:39:59.894039: Epoch time: 234.67 s
2024-12-17 14:39:59.894708: Yayy! New best EMA pseudo Dice: 0.702
2024-12-17 14:40:01.711658: 
2024-12-17 14:40:01.712839: Epoch 13
2024-12-17 14:40:01.713558: Current learning rate: 0.00922
2024-12-17 14:43:47.661697: Validation loss did not improve from -0.51301. Patience: 6/50
2024-12-17 14:43:47.662736: train_loss -0.6804
2024-12-17 14:43:47.663446: val_loss -0.4694
2024-12-17 14:43:47.664245: Pseudo dice [0.7229]
2024-12-17 14:43:47.664923: Epoch time: 225.95 s
2024-12-17 14:43:47.665581: Yayy! New best EMA pseudo Dice: 0.7041
2024-12-17 14:43:49.460400: 
2024-12-17 14:43:49.461641: Epoch 14
2024-12-17 14:43:49.462459: Current learning rate: 0.00916
2024-12-17 14:47:28.315096: Validation loss did not improve from -0.51301. Patience: 7/50
2024-12-17 14:47:28.315891: train_loss -0.6862
2024-12-17 14:47:28.316581: val_loss -0.4499
2024-12-17 14:47:28.317195: Pseudo dice [0.7049]
2024-12-17 14:47:28.317855: Epoch time: 218.86 s
2024-12-17 14:47:28.740304: Yayy! New best EMA pseudo Dice: 0.7041
2024-12-17 14:47:30.574653: 
2024-12-17 14:47:30.575800: Epoch 15
2024-12-17 14:47:30.576612: Current learning rate: 0.0091
2024-12-17 14:51:16.568666: Validation loss did not improve from -0.51301. Patience: 8/50
2024-12-17 14:51:16.569612: train_loss -0.693
2024-12-17 14:51:16.570511: val_loss -0.4615
2024-12-17 14:51:16.571300: Pseudo dice [0.7185]
2024-12-17 14:51:16.572138: Epoch time: 226.0 s
2024-12-17 14:51:16.572945: Yayy! New best EMA pseudo Dice: 0.7056
2024-12-17 14:51:18.385142: 
2024-12-17 14:51:18.386216: Epoch 16
2024-12-17 14:51:18.387008: Current learning rate: 0.00903
2024-12-17 14:54:50.702115: Validation loss did not improve from -0.51301. Patience: 9/50
2024-12-17 14:54:50.703057: train_loss -0.7006
2024-12-17 14:54:50.703695: val_loss -0.4865
2024-12-17 14:54:50.704387: Pseudo dice [0.7263]
2024-12-17 14:54:50.705011: Epoch time: 212.32 s
2024-12-17 14:54:50.705565: Yayy! New best EMA pseudo Dice: 0.7077
2024-12-17 14:54:52.530094: 
2024-12-17 14:54:52.531297: Epoch 17
2024-12-17 14:54:52.532356: Current learning rate: 0.00897
2024-12-17 14:58:39.164135: Validation loss did not improve from -0.51301. Patience: 10/50
2024-12-17 14:58:39.165125: train_loss -0.7053
2024-12-17 14:58:39.166027: val_loss -0.4781
2024-12-17 14:58:39.166681: Pseudo dice [0.7178]
2024-12-17 14:58:39.167294: Epoch time: 226.64 s
2024-12-17 14:58:39.167918: Yayy! New best EMA pseudo Dice: 0.7087
2024-12-17 14:58:40.973739: 
2024-12-17 14:58:40.974967: Epoch 18
2024-12-17 14:58:40.975792: Current learning rate: 0.00891
2024-12-17 15:02:16.216601: Validation loss did not improve from -0.51301. Patience: 11/50
2024-12-17 15:02:16.217559: train_loss -0.7053
2024-12-17 15:02:16.218504: val_loss -0.5116
2024-12-17 15:02:16.219271: Pseudo dice [0.7387]
2024-12-17 15:02:16.219908: Epoch time: 215.25 s
2024-12-17 15:02:16.220552: Yayy! New best EMA pseudo Dice: 0.7117
2024-12-17 15:02:18.554419: 
2024-12-17 15:02:18.555459: Epoch 19
2024-12-17 15:02:18.556211: Current learning rate: 0.00885
2024-12-17 15:05:56.776231: Validation loss did not improve from -0.51301. Patience: 12/50
2024-12-17 15:05:56.777006: train_loss -0.7176
2024-12-17 15:05:56.777671: val_loss -0.4784
2024-12-17 15:05:56.778341: Pseudo dice [0.716]
2024-12-17 15:05:56.778980: Epoch time: 218.22 s
2024-12-17 15:05:57.173081: Yayy! New best EMA pseudo Dice: 0.7121
2024-12-17 15:05:58.946881: 
2024-12-17 15:05:58.948068: Epoch 20
2024-12-17 15:05:58.948738: Current learning rate: 0.00879
2024-12-17 15:09:45.425791: Validation loss did not improve from -0.51301. Patience: 13/50
2024-12-17 15:09:45.426669: train_loss -0.7157
2024-12-17 15:09:45.427485: val_loss -0.4581
2024-12-17 15:09:45.428307: Pseudo dice [0.7164]
2024-12-17 15:09:45.429093: Epoch time: 226.48 s
2024-12-17 15:09:45.429758: Yayy! New best EMA pseudo Dice: 0.7125
2024-12-17 15:09:47.232318: 
2024-12-17 15:09:47.233422: Epoch 21
2024-12-17 15:09:47.234221: Current learning rate: 0.00873
2024-12-17 15:13:41.378093: Validation loss did not improve from -0.51301. Patience: 14/50
2024-12-17 15:13:41.378908: train_loss -0.7106
2024-12-17 15:13:41.379816: val_loss -0.4854
2024-12-17 15:13:41.380769: Pseudo dice [0.728]
2024-12-17 15:13:41.381609: Epoch time: 234.15 s
2024-12-17 15:13:41.382500: Yayy! New best EMA pseudo Dice: 0.7141
2024-12-17 15:13:43.074895: 
2024-12-17 15:13:43.076046: Epoch 22
2024-12-17 15:13:43.077094: Current learning rate: 0.00867
2024-12-17 15:17:42.362318: Validation loss did not improve from -0.51301. Patience: 15/50
2024-12-17 15:17:42.363259: train_loss -0.7184
2024-12-17 15:17:42.364274: val_loss -0.4958
2024-12-17 15:17:42.365163: Pseudo dice [0.7381]
2024-12-17 15:17:42.366056: Epoch time: 239.29 s
2024-12-17 15:17:42.366996: Yayy! New best EMA pseudo Dice: 0.7165
2024-12-17 15:17:44.076479: 
2024-12-17 15:17:44.077760: Epoch 23
2024-12-17 15:17:44.078702: Current learning rate: 0.00861
2024-12-17 15:21:40.679087: Validation loss did not improve from -0.51301. Patience: 16/50
2024-12-17 15:21:40.680057: train_loss -0.7183
2024-12-17 15:21:40.680929: val_loss -0.4957
2024-12-17 15:21:40.681544: Pseudo dice [0.7359]
2024-12-17 15:21:40.682248: Epoch time: 236.6 s
2024-12-17 15:21:40.682882: Yayy! New best EMA pseudo Dice: 0.7184
2024-12-17 15:21:42.561746: 
2024-12-17 15:21:42.563166: Epoch 24
2024-12-17 15:21:42.564051: Current learning rate: 0.00855
2024-12-17 15:25:40.204479: Validation loss improved from -0.51301 to -0.51921! Patience: 16/50
2024-12-17 15:25:40.205393: train_loss -0.7315
2024-12-17 15:25:40.206360: val_loss -0.5192
2024-12-17 15:25:40.207124: Pseudo dice [0.7504]
2024-12-17 15:25:40.207986: Epoch time: 237.65 s
2024-12-17 15:25:40.620427: Yayy! New best EMA pseudo Dice: 0.7216
2024-12-17 15:25:42.413795: 
2024-12-17 15:25:42.414759: Epoch 25
2024-12-17 15:25:42.415387: Current learning rate: 0.00849
2024-12-17 15:29:44.285842: Validation loss did not improve from -0.51921. Patience: 1/50
2024-12-17 15:29:44.286795: train_loss -0.7332
2024-12-17 15:29:44.287721: val_loss -0.4669
2024-12-17 15:29:44.288375: Pseudo dice [0.7252]
2024-12-17 15:29:44.289185: Epoch time: 241.87 s
2024-12-17 15:29:44.289862: Yayy! New best EMA pseudo Dice: 0.722
2024-12-17 15:29:46.038508: 
2024-12-17 15:29:46.039597: Epoch 26
2024-12-17 15:29:46.040305: Current learning rate: 0.00843
2024-12-17 15:33:45.275512: Validation loss did not improve from -0.51921. Patience: 2/50
2024-12-17 15:33:45.276383: train_loss -0.7316
2024-12-17 15:33:45.277183: val_loss -0.5072
2024-12-17 15:33:45.277879: Pseudo dice [0.7409]
2024-12-17 15:33:45.278694: Epoch time: 239.24 s
2024-12-17 15:33:45.279506: Yayy! New best EMA pseudo Dice: 0.7239
2024-12-17 15:33:47.039443: 
2024-12-17 15:33:47.040738: Epoch 27
2024-12-17 15:33:47.041534: Current learning rate: 0.00836
2024-12-17 15:37:51.576936: Validation loss did not improve from -0.51921. Patience: 3/50
2024-12-17 15:37:51.577804: train_loss -0.7324
2024-12-17 15:37:51.578783: val_loss -0.5095
2024-12-17 15:37:51.579633: Pseudo dice [0.7442]
2024-12-17 15:37:51.580455: Epoch time: 244.54 s
2024-12-17 15:37:51.581318: Yayy! New best EMA pseudo Dice: 0.7259
2024-12-17 15:37:53.307075: 
2024-12-17 15:37:53.308247: Epoch 28
2024-12-17 15:37:53.309247: Current learning rate: 0.0083
2024-12-17 15:41:53.494894: Validation loss did not improve from -0.51921. Patience: 4/50
2024-12-17 15:41:53.495734: train_loss -0.7423
2024-12-17 15:41:53.496433: val_loss -0.4557
2024-12-17 15:41:53.497086: Pseudo dice [0.7114]
2024-12-17 15:41:53.497858: Epoch time: 240.19 s
2024-12-17 15:41:54.833440: 
2024-12-17 15:41:54.834512: Epoch 29
2024-12-17 15:41:54.835447: Current learning rate: 0.00824
2024-12-17 15:45:56.221797: Validation loss did not improve from -0.51921. Patience: 5/50
2024-12-17 15:45:56.222680: train_loss -0.7452
2024-12-17 15:45:56.223598: val_loss -0.4742
2024-12-17 15:45:56.224281: Pseudo dice [0.7231]
2024-12-17 15:45:56.224919: Epoch time: 241.39 s
2024-12-17 15:45:58.156491: 
2024-12-17 15:45:58.157828: Epoch 30
2024-12-17 15:45:58.158704: Current learning rate: 0.00818
2024-12-17 15:49:47.279445: Validation loss did not improve from -0.51921. Patience: 6/50
2024-12-17 15:49:47.280215: train_loss -0.7357
2024-12-17 15:49:47.281199: val_loss -0.4953
2024-12-17 15:49:47.281980: Pseudo dice [0.7354]
2024-12-17 15:49:47.282774: Epoch time: 229.12 s
2024-12-17 15:49:48.655273: 
2024-12-17 15:49:48.656770: Epoch 31
2024-12-17 15:49:48.657795: Current learning rate: 0.00812
2024-12-17 15:53:53.071808: Validation loss did not improve from -0.51921. Patience: 7/50
2024-12-17 15:53:53.072618: train_loss -0.7417
2024-12-17 15:53:53.073543: val_loss -0.5008
2024-12-17 15:53:53.074509: Pseudo dice [0.7303]
2024-12-17 15:53:53.075362: Epoch time: 244.42 s
2024-12-17 15:53:53.076183: Yayy! New best EMA pseudo Dice: 0.7259
2024-12-17 15:53:54.881970: 
2024-12-17 15:53:54.882761: Epoch 32
2024-12-17 15:53:54.883520: Current learning rate: 0.00806
2024-12-17 15:57:48.982297: Validation loss did not improve from -0.51921. Patience: 8/50
2024-12-17 15:57:48.983035: train_loss -0.7486
2024-12-17 15:57:48.983745: val_loss -0.4665
2024-12-17 15:57:48.984400: Pseudo dice [0.7274]
2024-12-17 15:57:48.985023: Epoch time: 234.1 s
2024-12-17 15:57:48.985650: Yayy! New best EMA pseudo Dice: 0.7261
2024-12-17 15:57:50.752188: 
2024-12-17 15:57:50.753386: Epoch 33
2024-12-17 15:57:50.754017: Current learning rate: 0.008
2024-12-17 16:02:08.771373: Validation loss did not improve from -0.51921. Patience: 9/50
2024-12-17 16:02:08.775395: train_loss -0.7529
2024-12-17 16:02:08.777633: val_loss -0.4473
2024-12-17 16:02:08.778361: Pseudo dice [0.7117]
2024-12-17 16:02:08.779637: Epoch time: 258.02 s
2024-12-17 16:02:10.206283: 
2024-12-17 16:02:10.207468: Epoch 34
2024-12-17 16:02:10.208134: Current learning rate: 0.00793
2024-12-17 16:06:26.627504: Validation loss did not improve from -0.51921. Patience: 10/50
2024-12-17 16:06:26.628215: train_loss -0.7495
2024-12-17 16:06:26.629046: val_loss -0.5126
2024-12-17 16:06:26.629762: Pseudo dice [0.746]
2024-12-17 16:06:26.630399: Epoch time: 256.42 s
2024-12-17 16:06:27.028353: Yayy! New best EMA pseudo Dice: 0.7268
2024-12-17 16:06:28.873329: 
2024-12-17 16:06:28.874436: Epoch 35
2024-12-17 16:06:28.875391: Current learning rate: 0.00787
2024-12-17 16:10:55.390552: Validation loss did not improve from -0.51921. Patience: 11/50
2024-12-17 16:10:55.391504: train_loss -0.7547
2024-12-17 16:10:55.392225: val_loss -0.4741
2024-12-17 16:10:55.392945: Pseudo dice [0.7348]
2024-12-17 16:10:55.393638: Epoch time: 266.52 s
2024-12-17 16:10:55.394318: Yayy! New best EMA pseudo Dice: 0.7276
2024-12-17 16:10:57.356976: 
2024-12-17 16:10:57.358945: Epoch 36
2024-12-17 16:10:57.359935: Current learning rate: 0.00781
2024-12-17 16:15:00.477825: Validation loss did not improve from -0.51921. Patience: 12/50
2024-12-17 16:15:00.478840: train_loss -0.758
2024-12-17 16:15:00.479615: val_loss -0.4685
2024-12-17 16:15:00.480186: Pseudo dice [0.7226]
2024-12-17 16:15:00.480816: Epoch time: 243.12 s
2024-12-17 16:15:01.948840: 
2024-12-17 16:15:01.949948: Epoch 37
2024-12-17 16:15:01.950810: Current learning rate: 0.00775
2024-12-17 16:19:10.200435: Validation loss did not improve from -0.51921. Patience: 13/50
2024-12-17 16:19:10.201383: train_loss -0.7583
2024-12-17 16:19:10.202297: val_loss -0.4547
2024-12-17 16:19:10.202973: Pseudo dice [0.7255]
2024-12-17 16:19:10.203740: Epoch time: 248.25 s
2024-12-17 16:19:11.667507: 
2024-12-17 16:19:11.668700: Epoch 38
2024-12-17 16:19:11.669728: Current learning rate: 0.00769
2024-12-17 16:23:29.175614: Validation loss did not improve from -0.51921. Patience: 14/50
2024-12-17 16:23:29.176458: train_loss -0.7568
2024-12-17 16:23:29.177146: val_loss -0.4858
2024-12-17 16:23:29.177754: Pseudo dice [0.7337]
2024-12-17 16:23:29.178391: Epoch time: 257.51 s
2024-12-17 16:23:29.179049: Yayy! New best EMA pseudo Dice: 0.7276
2024-12-17 16:23:31.098437: 
2024-12-17 16:23:31.099678: Epoch 39
2024-12-17 16:23:31.100766: Current learning rate: 0.00763
2024-12-17 16:27:25.641240: Validation loss did not improve from -0.51921. Patience: 15/50
2024-12-17 16:27:25.642598: train_loss -0.7626
2024-12-17 16:27:25.643554: val_loss -0.5008
2024-12-17 16:27:25.644254: Pseudo dice [0.7389]
2024-12-17 16:27:25.645077: Epoch time: 234.55 s
2024-12-17 16:27:26.091502: Yayy! New best EMA pseudo Dice: 0.7287
2024-12-17 16:27:30.529803: 
2024-12-17 16:27:30.531105: Epoch 40
2024-12-17 16:27:30.531971: Current learning rate: 0.00756
2024-12-17 16:31:26.985744: Validation loss did not improve from -0.51921. Patience: 16/50
2024-12-17 16:31:26.986656: train_loss -0.7654
2024-12-17 16:31:26.987514: val_loss -0.4741
2024-12-17 16:31:26.988220: Pseudo dice [0.7267]
2024-12-17 16:31:26.988872: Epoch time: 236.46 s
2024-12-17 16:31:28.432463: 
2024-12-17 16:31:28.433665: Epoch 41
2024-12-17 16:31:28.434354: Current learning rate: 0.0075
2024-12-17 16:35:18.527181: Validation loss did not improve from -0.51921. Patience: 17/50
2024-12-17 16:35:18.528068: train_loss -0.7651
2024-12-17 16:35:18.529007: val_loss -0.4697
2024-12-17 16:35:18.530080: Pseudo dice [0.7335]
2024-12-17 16:35:18.530893: Epoch time: 230.1 s
2024-12-17 16:35:18.531746: Yayy! New best EMA pseudo Dice: 0.729
2024-12-17 16:35:20.350449: 
2024-12-17 16:35:20.351941: Epoch 42
2024-12-17 16:35:20.352981: Current learning rate: 0.00744
2024-12-17 16:39:25.530149: Validation loss did not improve from -0.51921. Patience: 18/50
2024-12-17 16:39:25.531226: train_loss -0.7675
2024-12-17 16:39:25.532216: val_loss -0.4635
2024-12-17 16:39:25.533112: Pseudo dice [0.7247]
2024-12-17 16:39:25.533911: Epoch time: 245.18 s
2024-12-17 16:39:26.926084: 
2024-12-17 16:39:26.927050: Epoch 43
2024-12-17 16:39:26.927777: Current learning rate: 0.00738
2024-12-17 16:43:19.411376: Validation loss did not improve from -0.51921. Patience: 19/50
2024-12-17 16:43:19.412451: train_loss -0.7719
2024-12-17 16:43:19.413274: val_loss -0.4789
2024-12-17 16:43:19.414052: Pseudo dice [0.7254]
2024-12-17 16:43:19.414919: Epoch time: 232.49 s
2024-12-17 16:43:20.793038: 
2024-12-17 16:43:20.794136: Epoch 44
2024-12-17 16:43:20.795008: Current learning rate: 0.00732
2024-12-17 16:47:19.325038: Validation loss did not improve from -0.51921. Patience: 20/50
2024-12-17 16:47:19.325934: train_loss -0.7681
2024-12-17 16:47:19.326821: val_loss -0.4652
2024-12-17 16:47:19.327619: Pseudo dice [0.7327]
2024-12-17 16:47:19.328406: Epoch time: 238.53 s
2024-12-17 16:47:21.201101: 
2024-12-17 16:47:21.202217: Epoch 45
2024-12-17 16:47:21.203127: Current learning rate: 0.00725
2024-12-17 16:50:52.925028: Validation loss did not improve from -0.51921. Patience: 21/50
2024-12-17 16:50:52.926197: train_loss -0.771
2024-12-17 16:50:52.927138: val_loss -0.4824
2024-12-17 16:50:52.927916: Pseudo dice [0.7294]
2024-12-17 16:50:52.928669: Epoch time: 211.73 s
2024-12-17 16:50:54.319165: 
2024-12-17 16:50:54.320348: Epoch 46
2024-12-17 16:50:54.321195: Current learning rate: 0.00719
2024-12-17 16:54:37.449619: Validation loss did not improve from -0.51921. Patience: 22/50
2024-12-17 16:54:37.450397: train_loss -0.7685
2024-12-17 16:54:37.451519: val_loss -0.4806
2024-12-17 16:54:37.452412: Pseudo dice [0.7299]
2024-12-17 16:54:37.453313: Epoch time: 223.13 s
2024-12-17 16:54:38.802751: 
2024-12-17 16:54:38.803954: Epoch 47
2024-12-17 16:54:38.804887: Current learning rate: 0.00713
2024-12-17 16:58:26.767673: Validation loss did not improve from -0.51921. Patience: 23/50
2024-12-17 16:58:26.768488: train_loss -0.7692
2024-12-17 16:58:26.769248: val_loss -0.4904
2024-12-17 16:58:26.769980: Pseudo dice [0.7403]
2024-12-17 16:58:26.770811: Epoch time: 227.97 s
2024-12-17 16:58:26.771482: Yayy! New best EMA pseudo Dice: 0.73
2024-12-17 16:58:28.653559: 
2024-12-17 16:58:28.654614: Epoch 48
2024-12-17 16:58:28.655339: Current learning rate: 0.00707
2024-12-17 17:02:31.738792: Validation loss did not improve from -0.51921. Patience: 24/50
2024-12-17 17:02:31.739556: train_loss -0.7712
2024-12-17 17:02:31.740272: val_loss -0.4548
2024-12-17 17:02:31.740952: Pseudo dice [0.7177]
2024-12-17 17:02:31.741757: Epoch time: 243.09 s
2024-12-17 17:02:33.133155: 
2024-12-17 17:02:33.134264: Epoch 49
2024-12-17 17:02:33.134979: Current learning rate: 0.007
2024-12-17 17:06:49.786519: Validation loss did not improve from -0.51921. Patience: 25/50
2024-12-17 17:06:49.790210: train_loss -0.7712
2024-12-17 17:06:49.792111: val_loss -0.5162
2024-12-17 17:06:49.792817: Pseudo dice [0.7482]
2024-12-17 17:06:49.793793: Epoch time: 256.66 s
2024-12-17 17:06:50.169055: Yayy! New best EMA pseudo Dice: 0.7307
2024-12-17 17:06:51.971439: 
2024-12-17 17:06:51.972873: Epoch 50
2024-12-17 17:06:51.973593: Current learning rate: 0.00694
2024-12-17 17:10:46.086752: Validation loss did not improve from -0.51921. Patience: 26/50
2024-12-17 17:10:46.087738: train_loss -0.7761
2024-12-17 17:10:46.089047: val_loss -0.4766
2024-12-17 17:10:46.090191: Pseudo dice [0.7233]
2024-12-17 17:10:46.091415: Epoch time: 234.12 s
2024-12-17 17:10:47.480411: 
2024-12-17 17:10:47.481700: Epoch 51
2024-12-17 17:10:47.482655: Current learning rate: 0.00688
2024-12-17 17:14:46.903735: Validation loss did not improve from -0.51921. Patience: 27/50
2024-12-17 17:14:46.904572: train_loss -0.7783
2024-12-17 17:14:46.905406: val_loss -0.4588
2024-12-17 17:14:46.906077: Pseudo dice [0.7255]
2024-12-17 17:14:46.906762: Epoch time: 239.43 s
2024-12-17 17:14:49.159259: 
2024-12-17 17:14:49.160660: Epoch 52
2024-12-17 17:14:49.161695: Current learning rate: 0.00682
2024-12-17 17:18:36.025406: Validation loss did not improve from -0.51921. Patience: 28/50
2024-12-17 17:18:36.026239: train_loss -0.7748
2024-12-17 17:18:36.027097: val_loss -0.4895
2024-12-17 17:18:36.027926: Pseudo dice [0.7427]
2024-12-17 17:18:36.028737: Epoch time: 226.87 s
2024-12-17 17:18:36.029580: Yayy! New best EMA pseudo Dice: 0.7309
2024-12-17 17:18:37.898098: 
2024-12-17 17:18:37.899469: Epoch 53
2024-12-17 17:18:37.900644: Current learning rate: 0.00675
2024-12-17 17:22:34.351640: Validation loss did not improve from -0.51921. Patience: 29/50
2024-12-17 17:22:34.352528: train_loss -0.7831
2024-12-17 17:22:34.353421: val_loss -0.4938
2024-12-17 17:22:34.354081: Pseudo dice [0.7425]
2024-12-17 17:22:34.354771: Epoch time: 236.46 s
2024-12-17 17:22:34.355768: Yayy! New best EMA pseudo Dice: 0.732
2024-12-17 17:22:36.293327: 
2024-12-17 17:22:36.294504: Epoch 54
2024-12-17 17:22:36.295223: Current learning rate: 0.00669
2024-12-17 17:26:27.153831: Validation loss did not improve from -0.51921. Patience: 30/50
2024-12-17 17:26:27.154682: train_loss -0.7836
2024-12-17 17:26:27.155552: val_loss -0.4801
2024-12-17 17:26:27.156618: Pseudo dice [0.7237]
2024-12-17 17:26:27.157560: Epoch time: 230.86 s
2024-12-17 17:26:29.001282: 
2024-12-17 17:26:29.002241: Epoch 55
2024-12-17 17:26:29.002950: Current learning rate: 0.00663
2024-12-17 17:30:48.833812: Validation loss did not improve from -0.51921. Patience: 31/50
2024-12-17 17:30:48.834678: train_loss -0.7834
2024-12-17 17:30:48.835707: val_loss -0.4503
2024-12-17 17:30:48.836736: Pseudo dice [0.7237]
2024-12-17 17:30:48.837598: Epoch time: 259.83 s
2024-12-17 17:30:50.286684: 
2024-12-17 17:30:50.287800: Epoch 56
2024-12-17 17:30:50.288574: Current learning rate: 0.00657
2024-12-17 17:34:59.738298: Validation loss did not improve from -0.51921. Patience: 32/50
2024-12-17 17:34:59.739087: train_loss -0.7821
2024-12-17 17:34:59.739950: val_loss -0.4869
2024-12-17 17:34:59.740669: Pseudo dice [0.7296]
2024-12-17 17:34:59.741402: Epoch time: 249.45 s
2024-12-17 17:35:01.180025: 
2024-12-17 17:35:01.181044: Epoch 57
2024-12-17 17:35:01.181818: Current learning rate: 0.0065
2024-12-17 17:39:10.717540: Validation loss did not improve from -0.51921. Patience: 33/50
2024-12-17 17:39:10.718443: train_loss -0.7858
2024-12-17 17:39:10.719553: val_loss -0.4929
2024-12-17 17:39:10.720659: Pseudo dice [0.7456]
2024-12-17 17:39:10.721764: Epoch time: 249.54 s
2024-12-17 17:39:12.171967: 
2024-12-17 17:39:12.173353: Epoch 58
2024-12-17 17:39:12.174271: Current learning rate: 0.00644
2024-12-17 17:43:24.001398: Validation loss did not improve from -0.51921. Patience: 34/50
2024-12-17 17:43:24.002432: train_loss -0.7863
2024-12-17 17:43:24.003267: val_loss -0.5119
2024-12-17 17:43:24.004247: Pseudo dice [0.7514]
2024-12-17 17:43:24.005222: Epoch time: 251.83 s
2024-12-17 17:43:24.006343: Yayy! New best EMA pseudo Dice: 0.7338
2024-12-17 17:43:25.985517: 
2024-12-17 17:43:25.986971: Epoch 59
2024-12-17 17:43:25.987750: Current learning rate: 0.00638
2024-12-17 17:47:32.061224: Validation loss did not improve from -0.51921. Patience: 35/50
2024-12-17 17:47:32.062086: train_loss -0.7844
2024-12-17 17:47:32.062891: val_loss -0.4821
2024-12-17 17:47:32.063745: Pseudo dice [0.7348]
2024-12-17 17:47:32.064637: Epoch time: 246.08 s
2024-12-17 17:47:32.516125: Yayy! New best EMA pseudo Dice: 0.7339
2024-12-17 17:47:34.347855: 
2024-12-17 17:47:34.348902: Epoch 60
2024-12-17 17:47:34.349693: Current learning rate: 0.00631
2024-12-17 17:51:28.992727: Validation loss did not improve from -0.51921. Patience: 36/50
2024-12-17 17:51:28.993637: train_loss -0.7839
2024-12-17 17:51:28.994488: val_loss -0.4501
2024-12-17 17:51:28.995238: Pseudo dice [0.718]
2024-12-17 17:51:28.996017: Epoch time: 234.65 s
2024-12-17 17:51:30.489712: 
2024-12-17 17:51:30.491098: Epoch 61
2024-12-17 17:51:30.491966: Current learning rate: 0.00625
2024-12-17 17:55:36.228120: Validation loss did not improve from -0.51921. Patience: 37/50
2024-12-17 17:55:36.228809: train_loss -0.7883
2024-12-17 17:55:36.229578: val_loss -0.4944
2024-12-17 17:55:36.230232: Pseudo dice [0.7403]
2024-12-17 17:55:36.231100: Epoch time: 245.74 s
2024-12-17 17:55:37.710515: 
2024-12-17 17:55:37.711559: Epoch 62
2024-12-17 17:55:37.712259: Current learning rate: 0.00619
2024-12-17 17:59:29.493853: Validation loss did not improve from -0.51921. Patience: 38/50
2024-12-17 17:59:29.494844: train_loss -0.7904
2024-12-17 17:59:29.495694: val_loss -0.4806
2024-12-17 17:59:29.496452: Pseudo dice [0.7366]
2024-12-17 17:59:29.497234: Epoch time: 231.79 s
2024-12-17 17:59:31.556150: 
2024-12-17 17:59:31.557449: Epoch 63
2024-12-17 17:59:31.558359: Current learning rate: 0.00612
2024-12-17 18:03:24.547530: Validation loss did not improve from -0.51921. Patience: 39/50
2024-12-17 18:03:24.548432: train_loss -0.7913
2024-12-17 18:03:24.549203: val_loss -0.4798
2024-12-17 18:03:24.549887: Pseudo dice [0.7347]
2024-12-17 18:03:24.550673: Epoch time: 232.99 s
2024-12-17 18:03:26.017482: 
2024-12-17 18:03:26.018405: Epoch 64
2024-12-17 18:03:26.019139: Current learning rate: 0.00606
2024-12-17 18:07:14.997299: Validation loss did not improve from -0.51921. Patience: 40/50
2024-12-17 18:07:14.998107: train_loss -0.7918
2024-12-17 18:07:14.998941: val_loss -0.4675
2024-12-17 18:07:14.999690: Pseudo dice [0.7227]
2024-12-17 18:07:15.000395: Epoch time: 228.98 s
2024-12-17 18:07:16.890347: 
2024-12-17 18:07:16.892135: Epoch 65
2024-12-17 18:07:16.893090: Current learning rate: 0.006
2024-12-17 18:11:05.100006: Validation loss did not improve from -0.51921. Patience: 41/50
2024-12-17 18:11:05.102348: train_loss -0.7929
2024-12-17 18:11:05.103484: val_loss -0.4843
2024-12-17 18:11:05.104203: Pseudo dice [0.7313]
2024-12-17 18:11:05.104966: Epoch time: 228.22 s
2024-12-17 18:11:06.573457: 
2024-12-17 18:11:06.576167: Epoch 66
2024-12-17 18:11:06.577350: Current learning rate: 0.00593
2024-12-17 18:14:58.892565: Validation loss did not improve from -0.51921. Patience: 42/50
2024-12-17 18:14:58.893579: train_loss -0.7927
2024-12-17 18:14:58.894447: val_loss -0.4433
2024-12-17 18:14:58.895200: Pseudo dice [0.7192]
2024-12-17 18:14:58.895957: Epoch time: 232.32 s
2024-12-17 18:15:00.345262: 
2024-12-17 18:15:00.346347: Epoch 67
2024-12-17 18:15:00.347124: Current learning rate: 0.00587
2024-12-17 18:19:04.104927: Validation loss did not improve from -0.51921. Patience: 43/50
2024-12-17 18:19:04.105894: train_loss -0.7944
2024-12-17 18:19:04.107050: val_loss -0.4744
2024-12-17 18:19:04.108124: Pseudo dice [0.7342]
2024-12-17 18:19:04.109075: Epoch time: 243.76 s
2024-12-17 18:19:05.627017: 
2024-12-17 18:19:05.628393: Epoch 68
2024-12-17 18:19:05.629530: Current learning rate: 0.00581
2024-12-17 18:23:00.426328: Validation loss did not improve from -0.51921. Patience: 44/50
2024-12-17 18:23:00.427687: train_loss -0.7953
2024-12-17 18:23:00.428494: val_loss -0.4581
2024-12-17 18:23:00.429160: Pseudo dice [0.726]
2024-12-17 18:23:00.429960: Epoch time: 234.8 s
2024-12-17 18:23:01.894349: 
2024-12-17 18:23:01.895439: Epoch 69
2024-12-17 18:23:01.896214: Current learning rate: 0.00574
2024-12-17 18:27:05.037203: Validation loss did not improve from -0.51921. Patience: 45/50
2024-12-17 18:27:05.038194: train_loss -0.7961
2024-12-17 18:27:05.039048: val_loss -0.4882
2024-12-17 18:27:05.039835: Pseudo dice [0.7288]
2024-12-17 18:27:05.040583: Epoch time: 243.15 s
2024-12-17 18:27:06.972068: 
2024-12-17 18:27:06.973160: Epoch 70
2024-12-17 18:27:06.973890: Current learning rate: 0.00568
2024-12-17 18:31:12.880550: Validation loss did not improve from -0.51921. Patience: 46/50
2024-12-17 18:31:12.881486: train_loss -0.7978
2024-12-17 18:31:12.882488: val_loss -0.4953
2024-12-17 18:31:12.883444: Pseudo dice [0.742]
2024-12-17 18:31:12.884382: Epoch time: 245.91 s
2024-12-17 18:31:14.379203: 
2024-12-17 18:31:14.380487: Epoch 71
2024-12-17 18:31:14.381567: Current learning rate: 0.00562
2024-12-17 18:34:56.666929: Validation loss did not improve from -0.51921. Patience: 47/50
2024-12-17 18:34:56.667972: train_loss -0.7999
2024-12-17 18:34:56.668676: val_loss -0.4277
2024-12-17 18:34:56.669294: Pseudo dice [0.7076]
2024-12-17 18:34:56.669972: Epoch time: 222.29 s
2024-12-17 18:34:58.171380: 
2024-12-17 18:34:58.172968: Epoch 72
2024-12-17 18:34:58.174480: Current learning rate: 0.00555
2024-12-17 18:39:09.045614: Validation loss did not improve from -0.51921. Patience: 48/50
2024-12-17 18:39:09.046810: train_loss -0.799
2024-12-17 18:39:09.047718: val_loss -0.4464
2024-12-17 18:39:09.048566: Pseudo dice [0.7232]
2024-12-17 18:39:09.049438: Epoch time: 250.88 s
2024-12-17 18:39:10.495493: 
2024-12-17 18:39:10.496776: Epoch 73
2024-12-17 18:39:10.497499: Current learning rate: 0.00549
2024-12-17 18:42:49.166592: Validation loss improved from -0.51921 to -0.52431! Patience: 48/50
2024-12-17 18:42:49.167602: train_loss -0.8029
2024-12-17 18:42:49.168758: val_loss -0.5243
2024-12-17 18:42:49.169523: Pseudo dice [0.7546]
2024-12-17 18:42:49.170259: Epoch time: 218.67 s
2024-12-17 18:42:51.022747: 
2024-12-17 18:42:51.023996: Epoch 74
2024-12-17 18:42:51.024816: Current learning rate: 0.00542
2024-12-17 18:46:57.752878: Validation loss did not improve from -0.52431. Patience: 1/50
2024-12-17 18:46:57.753829: train_loss -0.8018
2024-12-17 18:46:57.754616: val_loss -0.4728
2024-12-17 18:46:57.755229: Pseudo dice [0.7299]
2024-12-17 18:46:57.755885: Epoch time: 246.73 s
2024-12-17 18:46:59.621504: 
2024-12-17 18:46:59.622687: Epoch 75
2024-12-17 18:46:59.623407: Current learning rate: 0.00536
2024-12-17 18:51:07.713406: Validation loss did not improve from -0.52431. Patience: 2/50
2024-12-17 18:51:07.714391: train_loss -0.8003
2024-12-17 18:51:07.715356: val_loss -0.4711
2024-12-17 18:51:07.716074: Pseudo dice [0.746]
2024-12-17 18:51:07.716808: Epoch time: 248.09 s
2024-12-17 18:51:09.118520: 
2024-12-17 18:51:09.119978: Epoch 76
2024-12-17 18:51:09.120782: Current learning rate: 0.00529
2024-12-17 18:55:06.263682: Validation loss did not improve from -0.52431. Patience: 3/50
2024-12-17 18:55:06.264500: train_loss -0.8031
2024-12-17 18:55:06.265579: val_loss -0.4982
2024-12-17 18:55:06.266403: Pseudo dice [0.7485]
2024-12-17 18:55:06.267378: Epoch time: 237.15 s
2024-12-17 18:55:06.268154: Yayy! New best EMA pseudo Dice: 0.7343
2024-12-17 18:55:08.099723: 
2024-12-17 18:55:08.100762: Epoch 77
2024-12-17 18:55:08.101553: Current learning rate: 0.00523
2024-12-17 18:59:14.744900: Validation loss did not improve from -0.52431. Patience: 4/50
2024-12-17 18:59:14.745959: train_loss -0.8014
2024-12-17 18:59:14.746681: val_loss -0.4564
2024-12-17 18:59:14.747500: Pseudo dice [0.7326]
2024-12-17 18:59:14.748448: Epoch time: 246.65 s
2024-12-17 18:59:16.273899: 
2024-12-17 18:59:16.275029: Epoch 78
2024-12-17 18:59:16.275788: Current learning rate: 0.00517
2024-12-17 19:03:09.464761: Validation loss did not improve from -0.52431. Patience: 5/50
2024-12-17 19:03:09.465727: train_loss -0.8016
2024-12-17 19:03:09.466514: val_loss -0.4978
2024-12-17 19:03:09.467259: Pseudo dice [0.7455]
2024-12-17 19:03:09.467944: Epoch time: 233.19 s
2024-12-17 19:03:09.468588: Yayy! New best EMA pseudo Dice: 0.7352
2024-12-17 19:03:11.317329: 
2024-12-17 19:03:11.318385: Epoch 79
2024-12-17 19:03:11.319136: Current learning rate: 0.0051
2024-12-17 19:06:52.752793: Validation loss did not improve from -0.52431. Patience: 6/50
2024-12-17 19:06:52.753674: train_loss -0.803
2024-12-17 19:06:52.754363: val_loss -0.506
2024-12-17 19:06:52.755052: Pseudo dice [0.7534]
2024-12-17 19:06:52.755790: Epoch time: 221.44 s
2024-12-17 19:06:53.180652: Yayy! New best EMA pseudo Dice: 0.737
2024-12-17 19:06:55.078943: 
2024-12-17 19:06:55.080148: Epoch 80
2024-12-17 19:06:55.080881: Current learning rate: 0.00504
2024-12-17 19:11:09.426321: Validation loss did not improve from -0.52431. Patience: 7/50
2024-12-17 19:11:09.427302: train_loss -0.805
2024-12-17 19:11:09.428203: val_loss -0.501
2024-12-17 19:11:09.428920: Pseudo dice [0.7432]
2024-12-17 19:11:09.429617: Epoch time: 254.35 s
2024-12-17 19:11:09.430295: Yayy! New best EMA pseudo Dice: 0.7377
2024-12-17 19:11:11.274570: 
2024-12-17 19:11:11.275912: Epoch 81
2024-12-17 19:11:11.277083: Current learning rate: 0.00497
2024-12-17 19:15:14.343694: Validation loss did not improve from -0.52431. Patience: 8/50
2024-12-17 19:15:14.344593: train_loss -0.8059
2024-12-17 19:15:14.345327: val_loss -0.4773
2024-12-17 19:15:14.345967: Pseudo dice [0.7346]
2024-12-17 19:15:14.346667: Epoch time: 243.07 s
2024-12-17 19:15:15.771259: 
2024-12-17 19:15:15.772290: Epoch 82
2024-12-17 19:15:15.772976: Current learning rate: 0.00491
2024-12-17 19:19:09.530680: Validation loss did not improve from -0.52431. Patience: 9/50
2024-12-17 19:19:09.534427: train_loss -0.803
2024-12-17 19:19:09.535710: val_loss -0.4485
2024-12-17 19:19:09.536515: Pseudo dice [0.7262]
2024-12-17 19:19:09.537228: Epoch time: 233.76 s
2024-12-17 19:19:10.884236: 
2024-12-17 19:19:10.885470: Epoch 83
2024-12-17 19:19:10.886253: Current learning rate: 0.00484
2024-12-17 19:23:08.291134: Validation loss did not improve from -0.52431. Patience: 10/50
2024-12-17 19:23:08.292274: train_loss -0.8049
2024-12-17 19:23:08.293138: val_loss -0.4618
2024-12-17 19:23:08.293824: Pseudo dice [0.7282]
2024-12-17 19:23:08.294439: Epoch time: 237.41 s
2024-12-17 19:23:09.635597: 
2024-12-17 19:23:09.636717: Epoch 84
2024-12-17 19:23:09.637439: Current learning rate: 0.00478
2024-12-17 19:26:57.645509: Validation loss did not improve from -0.52431. Patience: 11/50
2024-12-17 19:26:57.646941: train_loss -0.8075
2024-12-17 19:26:57.647907: val_loss -0.46
2024-12-17 19:26:57.648588: Pseudo dice [0.7238]
2024-12-17 19:26:57.649250: Epoch time: 228.01 s
2024-12-17 19:27:00.349232: 
2024-12-17 19:27:00.350391: Epoch 85
2024-12-17 19:27:00.351208: Current learning rate: 0.00471
2024-12-17 19:31:05.278535: Validation loss did not improve from -0.52431. Patience: 12/50
2024-12-17 19:31:05.279488: train_loss -0.8079
2024-12-17 19:31:05.280301: val_loss -0.4725
2024-12-17 19:31:05.280946: Pseudo dice [0.735]
2024-12-17 19:31:05.281560: Epoch time: 244.93 s
2024-12-17 19:31:06.625168: 
2024-12-17 19:31:06.626553: Epoch 86
2024-12-17 19:31:06.627385: Current learning rate: 0.00465
2024-12-17 19:35:11.244107: Validation loss did not improve from -0.52431. Patience: 13/50
2024-12-17 19:35:11.245164: train_loss -0.8099
2024-12-17 19:35:11.246067: val_loss -0.4868
2024-12-17 19:35:11.246779: Pseudo dice [0.7391]
2024-12-17 19:35:11.247536: Epoch time: 244.62 s
2024-12-17 19:35:12.664176: 
2024-12-17 19:35:12.665528: Epoch 87
2024-12-17 19:35:12.666700: Current learning rate: 0.00458
2024-12-17 19:39:19.322816: Validation loss did not improve from -0.52431. Patience: 14/50
2024-12-17 19:39:19.323710: train_loss -0.8071
2024-12-17 19:39:19.324408: val_loss -0.4782
2024-12-17 19:39:19.325028: Pseudo dice [0.7353]
2024-12-17 19:39:19.325749: Epoch time: 246.66 s
2024-12-17 19:39:20.678402: 
2024-12-17 19:39:20.679748: Epoch 88
2024-12-17 19:39:20.680373: Current learning rate: 0.00452
2024-12-17 19:43:22.102149: Validation loss did not improve from -0.52431. Patience: 15/50
2024-12-17 19:43:22.103419: train_loss -0.807
2024-12-17 19:43:22.104316: val_loss -0.5005
2024-12-17 19:43:22.105142: Pseudo dice [0.7433]
2024-12-17 19:43:22.105889: Epoch time: 241.43 s
2024-12-17 19:43:23.482409: 
2024-12-17 19:43:23.483820: Epoch 89
2024-12-17 19:43:23.484651: Current learning rate: 0.00445
2024-12-17 19:47:33.780256: Validation loss did not improve from -0.52431. Patience: 16/50
2024-12-17 19:47:33.781211: train_loss -0.8107
2024-12-17 19:47:33.782208: val_loss -0.4526
2024-12-17 19:47:33.782972: Pseudo dice [0.7208]
2024-12-17 19:47:33.783767: Epoch time: 250.3 s
2024-12-17 19:47:35.662481: 
2024-12-17 19:47:35.663530: Epoch 90
2024-12-17 19:47:35.664206: Current learning rate: 0.00438
2024-12-17 19:51:48.992773: Validation loss did not improve from -0.52431. Patience: 17/50
2024-12-17 19:51:48.993834: train_loss -0.8111
2024-12-17 19:51:48.994550: val_loss -0.4909
2024-12-17 19:51:48.995213: Pseudo dice [0.7416]
2024-12-17 19:51:48.995905: Epoch time: 253.33 s
2024-12-17 19:51:50.345392: 
2024-12-17 19:51:50.346801: Epoch 91
2024-12-17 19:51:50.347619: Current learning rate: 0.00432
2024-12-17 19:56:00.618011: Validation loss did not improve from -0.52431. Patience: 18/50
2024-12-17 19:56:00.619011: train_loss -0.8114
2024-12-17 19:56:00.619803: val_loss -0.4721
2024-12-17 19:56:00.620386: Pseudo dice [0.7391]
2024-12-17 19:56:00.620991: Epoch time: 250.27 s
2024-12-17 19:56:01.947703: 
2024-12-17 19:56:01.949044: Epoch 92
2024-12-17 19:56:01.949658: Current learning rate: 0.00425
2024-12-17 20:00:11.889201: Validation loss did not improve from -0.52431. Patience: 19/50
2024-12-17 20:00:11.890005: train_loss -0.8108
2024-12-17 20:00:11.890790: val_loss -0.4811
2024-12-17 20:00:11.891462: Pseudo dice [0.7305]
2024-12-17 20:00:11.892196: Epoch time: 249.94 s
2024-12-17 20:00:13.257781: 
2024-12-17 20:00:13.258991: Epoch 93
2024-12-17 20:00:13.259821: Current learning rate: 0.00419
2024-12-17 20:04:14.472705: Validation loss did not improve from -0.52431. Patience: 20/50
2024-12-17 20:04:14.473559: train_loss -0.8129
2024-12-17 20:04:14.474322: val_loss -0.4624
2024-12-17 20:04:14.475055: Pseudo dice [0.7311]
2024-12-17 20:04:14.475775: Epoch time: 241.22 s
2024-12-17 20:04:15.790300: 
2024-12-17 20:04:15.791778: Epoch 94
2024-12-17 20:04:15.792645: Current learning rate: 0.00412
2024-12-17 20:07:58.963273: Validation loss did not improve from -0.52431. Patience: 21/50
2024-12-17 20:07:58.964072: train_loss -0.8109
2024-12-17 20:07:58.964935: val_loss -0.4667
2024-12-17 20:07:58.965874: Pseudo dice [0.729]
2024-12-17 20:07:58.966710: Epoch time: 223.18 s
2024-12-17 20:08:00.736573: 
2024-12-17 20:08:00.737784: Epoch 95
2024-12-17 20:08:00.738396: Current learning rate: 0.00405
2024-12-17 20:11:52.946020: Validation loss did not improve from -0.52431. Patience: 22/50
2024-12-17 20:11:52.946824: train_loss -0.8134
2024-12-17 20:11:52.947788: val_loss -0.4675
2024-12-17 20:11:52.948735: Pseudo dice [0.7352]
2024-12-17 20:11:52.949638: Epoch time: 232.21 s
2024-12-17 20:11:54.300038: 
2024-12-17 20:11:54.301343: Epoch 96
2024-12-17 20:11:54.302256: Current learning rate: 0.00399
2024-12-17 20:15:58.763222: Validation loss did not improve from -0.52431. Patience: 23/50
2024-12-17 20:15:58.764248: train_loss -0.8147
2024-12-17 20:15:58.764997: val_loss -0.4431
2024-12-17 20:15:58.765668: Pseudo dice [0.7258]
2024-12-17 20:15:58.766263: Epoch time: 244.47 s
2024-12-17 20:16:00.683841: 
2024-12-17 20:16:00.685161: Epoch 97
2024-12-17 20:16:00.685895: Current learning rate: 0.00392
2024-12-17 20:20:15.143761: Validation loss did not improve from -0.52431. Patience: 24/50
2024-12-17 20:20:15.145184: train_loss -0.8177
2024-12-17 20:20:15.145978: val_loss -0.4743
2024-12-17 20:20:15.146763: Pseudo dice [0.7399]
2024-12-17 20:20:15.147626: Epoch time: 254.46 s
2024-12-17 20:20:16.487267: 
2024-12-17 20:20:16.488323: Epoch 98
2024-12-17 20:20:16.489032: Current learning rate: 0.00385
2024-12-17 20:24:36.168559: Validation loss did not improve from -0.52431. Patience: 25/50
2024-12-17 20:24:36.169540: train_loss -0.8161
2024-12-17 20:24:36.170218: val_loss -0.4987
2024-12-17 20:24:36.170804: Pseudo dice [0.7433]
2024-12-17 20:24:36.171566: Epoch time: 259.68 s
2024-12-17 20:24:37.511586: 
2024-12-17 20:24:37.512656: Epoch 99
2024-12-17 20:24:37.513427: Current learning rate: 0.00379
2024-12-17 20:28:43.853749: Validation loss did not improve from -0.52431. Patience: 26/50
2024-12-17 20:28:43.854946: train_loss -0.8162
2024-12-17 20:28:43.855973: val_loss -0.4269
2024-12-17 20:28:43.856950: Pseudo dice [0.7146]
2024-12-17 20:28:43.857932: Epoch time: 246.34 s
2024-12-17 20:28:45.712779: 
2024-12-17 20:28:45.713856: Epoch 100
2024-12-17 20:28:45.714634: Current learning rate: 0.00372
2024-12-17 20:33:03.921080: Validation loss did not improve from -0.52431. Patience: 27/50
2024-12-17 20:33:03.922077: train_loss -0.8176
2024-12-17 20:33:03.922940: val_loss -0.4606
2024-12-17 20:33:03.923757: Pseudo dice [0.727]
2024-12-17 20:33:03.924660: Epoch time: 258.21 s
2024-12-17 20:33:05.287989: 
2024-12-17 20:33:05.289201: Epoch 101
2024-12-17 20:33:05.289953: Current learning rate: 0.00365
2024-12-17 20:37:20.859756: Validation loss did not improve from -0.52431. Patience: 28/50
2024-12-17 20:37:20.860964: train_loss -0.8183
2024-12-17 20:37:20.861841: val_loss -0.4633
2024-12-17 20:37:20.862607: Pseudo dice [0.7352]
2024-12-17 20:37:20.863532: Epoch time: 255.57 s
2024-12-17 20:37:22.347186: 
2024-12-17 20:37:22.348357: Epoch 102
2024-12-17 20:37:22.349150: Current learning rate: 0.00359
2024-12-17 20:41:55.641577: Validation loss did not improve from -0.52431. Patience: 29/50
2024-12-17 20:41:55.643853: train_loss -0.8162
2024-12-17 20:41:55.644700: val_loss -0.4887
2024-12-17 20:41:55.645585: Pseudo dice [0.7495]
2024-12-17 20:41:55.646435: Epoch time: 273.3 s
2024-12-17 20:41:57.023267: 
2024-12-17 20:41:57.024160: Epoch 103
2024-12-17 20:41:57.024869: Current learning rate: 0.00352
2024-12-17 20:46:03.876457: Validation loss did not improve from -0.52431. Patience: 30/50
2024-12-17 20:46:03.877352: train_loss -0.8153
2024-12-17 20:46:03.878191: val_loss -0.4793
2024-12-17 20:46:03.879000: Pseudo dice [0.7356]
2024-12-17 20:46:03.879708: Epoch time: 246.86 s
2024-12-17 20:46:05.235411: 
2024-12-17 20:46:05.236201: Epoch 104
2024-12-17 20:46:05.236974: Current learning rate: 0.00345
2024-12-17 20:50:18.308804: Validation loss did not improve from -0.52431. Patience: 31/50
2024-12-17 20:50:18.309744: train_loss -0.814
2024-12-17 20:50:18.310497: val_loss -0.4671
2024-12-17 20:50:18.311119: Pseudo dice [0.7342]
2024-12-17 20:50:18.311788: Epoch time: 253.08 s
2024-12-17 20:50:20.103301: 
2024-12-17 20:50:20.104423: Epoch 105
2024-12-17 20:50:20.105190: Current learning rate: 0.00338
2024-12-17 20:54:32.990544: Validation loss did not improve from -0.52431. Patience: 32/50
2024-12-17 20:54:32.991446: train_loss -0.8169
2024-12-17 20:54:32.992230: val_loss -0.4523
2024-12-17 20:54:32.992969: Pseudo dice [0.7259]
2024-12-17 20:54:32.993573: Epoch time: 252.89 s
2024-12-17 20:54:34.348652: 
2024-12-17 20:54:34.349828: Epoch 106
2024-12-17 20:54:34.350512: Current learning rate: 0.00332
2024-12-17 20:58:52.226459: Validation loss did not improve from -0.52431. Patience: 33/50
2024-12-17 20:58:52.227278: train_loss -0.8193
2024-12-17 20:58:52.228068: val_loss -0.4608
2024-12-17 20:58:52.228784: Pseudo dice [0.7325]
2024-12-17 20:58:52.229458: Epoch time: 257.88 s
2024-12-17 20:58:53.650825: 
2024-12-17 20:58:53.651958: Epoch 107
2024-12-17 20:58:53.652590: Current learning rate: 0.00325
2024-12-17 21:03:08.910877: Validation loss did not improve from -0.52431. Patience: 34/50
2024-12-17 21:03:08.911694: train_loss -0.8183
2024-12-17 21:03:08.912466: val_loss -0.4605
2024-12-17 21:03:08.913212: Pseudo dice [0.7248]
2024-12-17 21:03:08.913911: Epoch time: 255.26 s
2024-12-17 21:03:11.372146: 
2024-12-17 21:03:11.373276: Epoch 108
2024-12-17 21:03:11.373980: Current learning rate: 0.00318
2024-12-17 21:05:54.841995: Validation loss did not improve from -0.52431. Patience: 35/50
2024-12-17 21:05:54.843110: train_loss -0.8226
2024-12-17 21:05:54.843815: val_loss -0.5063
2024-12-17 21:05:54.844445: Pseudo dice [0.755]
2024-12-17 21:05:54.845062: Epoch time: 163.47 s
2024-12-17 21:05:56.239907: 
2024-12-17 21:05:56.240895: Epoch 109
2024-12-17 21:05:56.241663: Current learning rate: 0.00311
2024-12-17 21:07:59.309310: Validation loss did not improve from -0.52431. Patience: 36/50
2024-12-17 21:07:59.310252: train_loss -0.8203
2024-12-17 21:07:59.311072: val_loss -0.5028
2024-12-17 21:07:59.311753: Pseudo dice [0.7468]
2024-12-17 21:07:59.312388: Epoch time: 123.07 s
2024-12-17 21:08:01.217582: 
2024-12-17 21:08:01.218678: Epoch 110
2024-12-17 21:08:01.219440: Current learning rate: 0.00304
2024-12-17 21:10:07.861637: Validation loss did not improve from -0.52431. Patience: 37/50
2024-12-17 21:10:07.862645: train_loss -0.8196
2024-12-17 21:10:07.863348: val_loss -0.4605
2024-12-17 21:10:07.869388: Pseudo dice [0.7275]
2024-12-17 21:10:07.870470: Epoch time: 126.65 s
2024-12-17 21:10:09.269449: 
2024-12-17 21:10:09.270571: Epoch 111
2024-12-17 21:10:09.271488: Current learning rate: 0.00297
2024-12-17 21:12:23.417569: Validation loss did not improve from -0.52431. Patience: 38/50
2024-12-17 21:12:23.418412: train_loss -0.8244
2024-12-17 21:12:23.419151: val_loss -0.478
2024-12-17 21:12:23.419815: Pseudo dice [0.7292]
2024-12-17 21:12:23.420476: Epoch time: 134.15 s
2024-12-17 21:12:24.802081: 
2024-12-17 21:12:24.803294: Epoch 112
2024-12-17 21:12:24.804104: Current learning rate: 0.00291
2024-12-17 21:14:28.926446: Validation loss did not improve from -0.52431. Patience: 39/50
2024-12-17 21:14:28.927309: train_loss -0.8255
2024-12-17 21:14:28.928211: val_loss -0.4683
2024-12-17 21:14:28.929051: Pseudo dice [0.7352]
2024-12-17 21:14:28.929940: Epoch time: 124.13 s
2024-12-17 21:14:30.338447: 
2024-12-17 21:14:30.339714: Epoch 113
2024-12-17 21:14:30.340369: Current learning rate: 0.00284
2024-12-17 21:16:26.782201: Validation loss did not improve from -0.52431. Patience: 40/50
2024-12-17 21:16:26.783301: train_loss -0.8257
2024-12-17 21:16:26.784070: val_loss -0.484
2024-12-17 21:16:26.784912: Pseudo dice [0.7452]
2024-12-17 21:16:26.785701: Epoch time: 116.45 s
2024-12-17 21:16:28.185400: 
2024-12-17 21:16:28.186576: Epoch 114
2024-12-17 21:16:28.187315: Current learning rate: 0.00277
2024-12-17 21:18:37.852662: Validation loss did not improve from -0.52431. Patience: 41/50
2024-12-17 21:18:37.853766: train_loss -0.8237
2024-12-17 21:18:37.854611: val_loss -0.4515
2024-12-17 21:18:37.855502: Pseudo dice [0.7252]
2024-12-17 21:18:37.856457: Epoch time: 129.67 s
2024-12-17 21:18:39.667388: 
2024-12-17 21:18:39.668498: Epoch 115
2024-12-17 21:18:39.669388: Current learning rate: 0.0027
2024-12-17 21:20:42.517730: Validation loss did not improve from -0.52431. Patience: 42/50
2024-12-17 21:20:42.518903: train_loss -0.8234
2024-12-17 21:20:42.519703: val_loss -0.4622
2024-12-17 21:20:42.520562: Pseudo dice [0.7309]
2024-12-17 21:20:42.521348: Epoch time: 122.85 s
2024-12-17 21:20:43.974035: 
2024-12-17 21:20:43.975216: Epoch 116
2024-12-17 21:20:43.976116: Current learning rate: 0.00263
2024-12-17 21:23:19.718097: Validation loss did not improve from -0.52431. Patience: 43/50
2024-12-17 21:23:19.718998: train_loss -0.8248
2024-12-17 21:23:19.719720: val_loss -0.4467
2024-12-17 21:23:19.720351: Pseudo dice [0.7308]
2024-12-17 21:23:19.721132: Epoch time: 155.75 s
2024-12-17 21:23:21.160820: 
2024-12-17 21:23:21.161528: Epoch 117
2024-12-17 21:23:21.162171: Current learning rate: 0.00256
2024-12-17 21:29:25.844564: Validation loss did not improve from -0.52431. Patience: 44/50
2024-12-17 21:29:25.845925: train_loss -0.8223
2024-12-17 21:29:25.846774: val_loss -0.4647
2024-12-17 21:29:25.847533: Pseudo dice [0.7349]
2024-12-17 21:29:25.848253: Epoch time: 364.69 s
2024-12-17 21:29:27.279393: 
2024-12-17 21:29:27.280710: Epoch 118
2024-12-17 21:29:27.281401: Current learning rate: 0.00249
2024-12-17 21:35:31.964590: Validation loss did not improve from -0.52431. Patience: 45/50
2024-12-17 21:35:31.966377: train_loss -0.8231
2024-12-17 21:35:31.967137: val_loss -0.4541
2024-12-17 21:35:31.967763: Pseudo dice [0.7302]
2024-12-17 21:35:31.968614: Epoch time: 364.69 s
2024-12-17 21:35:34.233884: 
2024-12-17 21:35:34.235100: Epoch 119
2024-12-17 21:35:34.235908: Current learning rate: 0.00242
2024-12-17 21:43:04.686589: Validation loss did not improve from -0.52431. Patience: 46/50
2024-12-17 21:43:04.687579: train_loss -0.8284
2024-12-17 21:43:04.688676: val_loss -0.4879
2024-12-17 21:43:04.689680: Pseudo dice [0.741]
2024-12-17 21:43:04.690668: Epoch time: 450.46 s
2024-12-17 21:43:06.513446: 
2024-12-17 21:43:06.514738: Epoch 120
2024-12-17 21:43:06.515643: Current learning rate: 0.00235
2024-12-17 21:50:33.115202: Validation loss did not improve from -0.52431. Patience: 47/50
2024-12-17 21:50:33.116565: train_loss -0.8262
2024-12-17 21:50:33.117440: val_loss -0.4751
2024-12-17 21:50:33.118190: Pseudo dice [0.739]
2024-12-17 21:50:33.118862: Epoch time: 446.6 s
2024-12-17 21:50:34.581485: 
2024-12-17 21:50:34.582740: Epoch 121
2024-12-17 21:50:34.583525: Current learning rate: 0.00228
2024-12-17 21:57:16.855999: Validation loss did not improve from -0.52431. Patience: 48/50
2024-12-17 21:57:16.856890: train_loss -0.829
2024-12-17 21:57:16.857608: val_loss -0.4862
2024-12-17 21:57:16.858232: Pseudo dice [0.7387]
2024-12-17 21:57:16.858922: Epoch time: 402.28 s
2024-12-17 21:57:18.310780: 
2024-12-17 21:57:18.311952: Epoch 122
2024-12-17 21:57:18.312854: Current learning rate: 0.00221
2024-12-17 22:04:45.749331: Validation loss did not improve from -0.52431. Patience: 49/50
2024-12-17 22:04:45.750287: train_loss -0.827
2024-12-17 22:04:45.751086: val_loss -0.4734
2024-12-17 22:04:45.751824: Pseudo dice [0.746]
2024-12-17 22:04:45.752569: Epoch time: 447.44 s
2024-12-17 22:04:47.223155: 
2024-12-17 22:04:47.224165: Epoch 123
2024-12-17 22:04:47.224815: Current learning rate: 0.00214
2024-12-17 22:12:45.188500: Validation loss did not improve from -0.52431. Patience: 50/50
2024-12-17 22:12:45.189264: train_loss -0.8252
2024-12-17 22:12:45.190242: val_loss -0.4795
2024-12-17 22:12:45.191106: Pseudo dice [0.7415]
2024-12-17 22:12:45.191963: Epoch time: 477.97 s
2024-12-17 22:12:46.606736: 
2024-12-17 22:12:46.607777: Epoch 124
2024-12-17 22:12:46.608463: Current learning rate: 0.00207
2024-12-17 22:21:10.253709: Validation loss did not improve from -0.52431. Patience: 51/50
2024-12-17 22:21:10.254691: train_loss -0.8266
2024-12-17 22:21:10.255491: val_loss -0.4581
2024-12-17 22:21:10.256195: Pseudo dice [0.7394]
2024-12-17 22:21:10.256894: Epoch time: 503.65 s
2024-12-17 22:21:12.114978: 
2024-12-17 22:21:12.116114: Epoch 125
2024-12-17 22:21:12.117009: Current learning rate: 0.00199
2024-12-17 22:29:29.079265: Validation loss did not improve from -0.52431. Patience: 52/50
2024-12-17 22:29:29.080231: train_loss -0.8275
2024-12-17 22:29:29.081251: val_loss -0.4688
2024-12-17 22:29:29.082193: Pseudo dice [0.7412]
2024-12-17 22:29:29.083068: Epoch time: 496.97 s
2024-12-17 22:29:30.526329: 
2024-12-17 22:29:30.528063: Epoch 126
2024-12-17 22:29:30.529146: Current learning rate: 0.00192
2024-12-17 22:37:46.155615: Validation loss did not improve from -0.52431. Patience: 53/50
2024-12-17 22:37:46.156586: train_loss -0.8292
2024-12-17 22:37:46.157316: val_loss -0.4705
2024-12-17 22:37:46.158029: Pseudo dice [0.7368]
2024-12-17 22:37:46.158611: Epoch time: 495.63 s
2024-12-17 22:37:47.555948: 
2024-12-17 22:37:47.557063: Epoch 127
2024-12-17 22:37:47.557747: Current learning rate: 0.00185
2024-12-17 22:46:02.660719: Validation loss did not improve from -0.52431. Patience: 54/50
2024-12-17 22:46:02.662429: train_loss -0.8286
2024-12-17 22:46:02.663478: val_loss -0.4382
2024-12-17 22:46:02.664247: Pseudo dice [0.7315]
2024-12-17 22:46:02.665106: Epoch time: 495.11 s
2024-12-17 22:46:04.102717: 
2024-12-17 22:46:04.103791: Epoch 128
2024-12-17 22:46:04.104481: Current learning rate: 0.00178
2024-12-17 22:54:18.717403: Validation loss did not improve from -0.52431. Patience: 55/50
2024-12-17 22:54:18.718368: train_loss -0.8312
2024-12-17 22:54:18.719183: val_loss -0.4488
2024-12-17 22:54:18.719937: Pseudo dice [0.7254]
2024-12-17 22:54:18.720681: Epoch time: 494.62 s
2024-12-17 22:54:20.200793: 
2024-12-17 22:54:20.202188: Epoch 129
2024-12-17 22:54:20.203034: Current learning rate: 0.0017
2024-12-17 23:02:24.140080: Validation loss did not improve from -0.52431. Patience: 56/50
2024-12-17 23:02:24.142881: train_loss -0.8292
2024-12-17 23:02:24.144120: val_loss -0.4587
2024-12-17 23:02:24.144962: Pseudo dice [0.7391]
2024-12-17 23:02:24.145844: Epoch time: 483.94 s
2024-12-17 23:02:27.606463: 
2024-12-17 23:02:27.607762: Epoch 130
2024-12-17 23:02:27.608564: Current learning rate: 0.00163
2024-12-17 23:10:45.254071: Validation loss did not improve from -0.52431. Patience: 57/50
2024-12-17 23:10:45.254815: train_loss -0.8287
2024-12-17 23:10:45.255729: val_loss -0.4644
2024-12-17 23:10:45.256513: Pseudo dice [0.7329]
2024-12-17 23:10:45.257395: Epoch time: 497.65 s
2024-12-17 23:10:46.701369: 
2024-12-17 23:10:46.703674: Epoch 131
2024-12-17 23:10:46.704658: Current learning rate: 0.00156
2024-12-17 23:19:02.756929: Validation loss did not improve from -0.52431. Patience: 58/50
2024-12-17 23:19:02.758202: train_loss -0.8307
2024-12-17 23:19:02.759059: val_loss -0.4675
2024-12-17 23:19:02.759828: Pseudo dice [0.739]
2024-12-17 23:19:02.760529: Epoch time: 496.06 s
2024-12-17 23:19:04.306483: 
2024-12-17 23:19:04.307428: Epoch 132
2024-12-17 23:19:04.308063: Current learning rate: 0.00148
2024-12-17 23:27:36.752090: Validation loss did not improve from -0.52431. Patience: 59/50
2024-12-17 23:27:36.752969: train_loss -0.8311
2024-12-17 23:27:36.753724: val_loss -0.4744
2024-12-17 23:27:36.754344: Pseudo dice [0.7373]
2024-12-17 23:27:36.754989: Epoch time: 512.45 s
2024-12-17 23:27:38.177787: 
2024-12-17 23:27:38.179294: Epoch 133
2024-12-17 23:27:38.180393: Current learning rate: 0.00141
2024-12-17 23:36:07.735876: Validation loss did not improve from -0.52431. Patience: 60/50
2024-12-17 23:36:07.739556: train_loss -0.8312
2024-12-17 23:36:07.741409: val_loss -0.4705
2024-12-17 23:36:07.742290: Pseudo dice [0.7404]
2024-12-17 23:36:07.743646: Epoch time: 509.56 s
2024-12-17 23:36:09.173276: 
2024-12-17 23:36:09.174315: Epoch 134
2024-12-17 23:36:09.175102: Current learning rate: 0.00133
2024-12-17 23:44:42.020427: Validation loss did not improve from -0.52431. Patience: 61/50
2024-12-17 23:44:42.021530: train_loss -0.8318
2024-12-17 23:44:42.022665: val_loss -0.4612
2024-12-17 23:44:42.023877: Pseudo dice [0.729]
2024-12-17 23:44:42.025012: Epoch time: 512.85 s
2024-12-17 23:44:43.781990: 
2024-12-17 23:44:43.783384: Epoch 135
2024-12-17 23:44:43.784413: Current learning rate: 0.00126
2024-12-17 23:53:24.367403: Validation loss did not improve from -0.52431. Patience: 62/50
2024-12-17 23:53:24.368379: train_loss -0.8332
2024-12-17 23:53:24.369132: val_loss -0.4839
2024-12-17 23:53:24.369777: Pseudo dice [0.7441]
2024-12-17 23:53:24.370376: Epoch time: 520.59 s
2024-12-17 23:53:25.811113: 
2024-12-17 23:53:25.812348: Epoch 136
2024-12-17 23:53:25.813013: Current learning rate: 0.00118
2024-12-18 00:01:22.286440: Validation loss did not improve from -0.52431. Patience: 63/50
2024-12-18 00:01:22.287486: train_loss -0.8318
2024-12-18 00:01:22.288162: val_loss -0.4766
2024-12-18 00:01:22.288846: Pseudo dice [0.7458]
2024-12-18 00:01:22.289453: Epoch time: 476.48 s
2024-12-18 00:01:23.790469: 
2024-12-18 00:01:23.791917: Epoch 137
2024-12-18 00:01:23.792736: Current learning rate: 0.00111
2024-12-18 00:09:29.265871: Validation loss did not improve from -0.52431. Patience: 64/50
2024-12-18 00:09:29.266884: train_loss -0.8313
2024-12-18 00:09:29.267582: val_loss -0.4545
2024-12-18 00:09:29.268137: Pseudo dice [0.7393]
2024-12-18 00:09:29.268768: Epoch time: 485.48 s
2024-12-18 00:09:29.269311: Yayy! New best EMA pseudo Dice: 0.7377
2024-12-18 00:09:31.447297: 
2024-12-18 00:09:31.448520: Epoch 138
2024-12-18 00:09:31.449518: Current learning rate: 0.00103
2024-12-18 00:17:57.525224: Validation loss did not improve from -0.52431. Patience: 65/50
2024-12-18 00:17:57.527613: train_loss -0.8341
2024-12-18 00:17:57.528439: val_loss -0.444
2024-12-18 00:17:57.529183: Pseudo dice [0.727]
2024-12-18 00:17:57.529981: Epoch time: 506.08 s
2024-12-18 00:17:59.067663: 
2024-12-18 00:17:59.068818: Epoch 139
2024-12-18 00:17:59.069509: Current learning rate: 0.00095
2024-12-18 00:26:02.322057: Validation loss did not improve from -0.52431. Patience: 66/50
2024-12-18 00:26:02.322982: train_loss -0.8323
2024-12-18 00:26:02.323999: val_loss -0.4821
2024-12-18 00:26:02.324801: Pseudo dice [0.7405]
2024-12-18 00:26:02.325645: Epoch time: 483.26 s
2024-12-18 00:26:04.374268: 
2024-12-18 00:26:04.375652: Epoch 140
2024-12-18 00:26:04.376388: Current learning rate: 0.00087
2024-12-18 00:34:24.639328: Validation loss did not improve from -0.52431. Patience: 67/50
2024-12-18 00:34:24.640321: train_loss -0.835
2024-12-18 00:34:24.641299: val_loss -0.5028
2024-12-18 00:34:24.642133: Pseudo dice [0.7463]
2024-12-18 00:34:24.642918: Epoch time: 500.27 s
2024-12-18 00:34:24.643610: Yayy! New best EMA pseudo Dice: 0.738
2024-12-18 00:34:26.701720: 
2024-12-18 00:34:26.703036: Epoch 141
2024-12-18 00:34:26.703793: Current learning rate: 0.00079
2024-12-18 00:42:05.397280: Validation loss did not improve from -0.52431. Patience: 68/50
2024-12-18 00:42:05.398505: train_loss -0.8341
2024-12-18 00:42:05.399321: val_loss -0.4941
2024-12-18 00:42:05.400176: Pseudo dice [0.7539]
2024-12-18 00:42:05.401092: Epoch time: 458.7 s
2024-12-18 00:42:05.402001: Yayy! New best EMA pseudo Dice: 0.7396
2024-12-18 00:42:07.874707: 
2024-12-18 00:42:07.875897: Epoch 142
2024-12-18 00:42:07.876877: Current learning rate: 0.00071
2024-12-18 00:50:30.107293: Validation loss did not improve from -0.52431. Patience: 69/50
2024-12-18 00:50:30.108649: train_loss -0.8335
2024-12-18 00:50:30.109563: val_loss -0.4614
2024-12-18 00:50:30.110227: Pseudo dice [0.7231]
2024-12-18 00:50:30.110924: Epoch time: 502.23 s
2024-12-18 00:50:31.696534: 
2024-12-18 00:50:31.697895: Epoch 143
2024-12-18 00:50:31.698621: Current learning rate: 0.00063
2024-12-18 00:58:47.884767: Validation loss did not improve from -0.52431. Patience: 70/50
2024-12-18 00:58:47.886316: train_loss -0.8351
2024-12-18 00:58:47.887172: val_loss -0.4969
2024-12-18 00:58:47.887926: Pseudo dice [0.7489]
2024-12-18 00:58:47.888646: Epoch time: 496.19 s
2024-12-18 00:58:49.435400: 
2024-12-18 00:58:49.436693: Epoch 144
2024-12-18 00:58:49.437419: Current learning rate: 0.00055
2024-12-18 01:07:09.407551: Validation loss did not improve from -0.52431. Patience: 71/50
2024-12-18 01:07:09.409055: train_loss -0.8307
2024-12-18 01:07:09.409942: val_loss -0.4552
2024-12-18 01:07:09.410938: Pseudo dice [0.7397]
2024-12-18 01:07:09.411933: Epoch time: 499.97 s
2024-12-18 01:07:11.362756: 
2024-12-18 01:07:11.364303: Epoch 145
2024-12-18 01:07:11.365358: Current learning rate: 0.00047
2024-12-18 01:15:18.471868: Validation loss did not improve from -0.52431. Patience: 72/50
2024-12-18 01:15:18.472851: train_loss -0.8327
2024-12-18 01:15:18.474618: val_loss -0.4701
2024-12-18 01:15:18.475363: Pseudo dice [0.7387]
2024-12-18 01:15:18.476200: Epoch time: 487.11 s
2024-12-18 01:15:20.012543: 
2024-12-18 01:15:20.013829: Epoch 146
2024-12-18 01:15:20.014542: Current learning rate: 0.00038
2024-12-18 01:23:30.649796: Validation loss did not improve from -0.52431. Patience: 73/50
2024-12-18 01:23:30.650621: train_loss -0.8357
2024-12-18 01:23:30.651319: val_loss -0.4521
2024-12-18 01:23:30.652065: Pseudo dice [0.7337]
2024-12-18 01:23:30.652745: Epoch time: 490.64 s
2024-12-18 01:23:32.170306: 
2024-12-18 01:23:32.172052: Epoch 147
2024-12-18 01:23:32.173150: Current learning rate: 0.0003
2024-12-18 01:31:48.347399: Validation loss did not improve from -0.52431. Patience: 74/50
2024-12-18 01:31:48.348494: train_loss -0.836
2024-12-18 01:31:48.349353: val_loss -0.4733
2024-12-18 01:31:48.350231: Pseudo dice [0.7398]
2024-12-18 01:31:48.351054: Epoch time: 496.18 s
2024-12-18 01:31:49.978881: 
2024-12-18 01:31:49.980148: Epoch 148
2024-12-18 01:31:49.980919: Current learning rate: 0.00021
2024-12-18 01:40:05.968129: Validation loss did not improve from -0.52431. Patience: 75/50
2024-12-18 01:40:05.969128: train_loss -0.8336
2024-12-18 01:40:05.969955: val_loss -0.4917
2024-12-18 01:40:05.970693: Pseudo dice [0.7512]
2024-12-18 01:40:05.971408: Epoch time: 495.99 s
2024-12-18 01:40:05.972253: Yayy! New best EMA pseudo Dice: 0.7399
2024-12-18 01:40:07.968537: 
2024-12-18 01:40:07.969919: Epoch 149
2024-12-18 01:40:07.971131: Current learning rate: 0.00011
2024-12-18 01:48:50.835020: Validation loss did not improve from -0.52431. Patience: 76/50
2024-12-18 01:48:50.837781: train_loss -0.8342
2024-12-18 01:48:50.838573: val_loss -0.4627
2024-12-18 01:48:50.839269: Pseudo dice [0.7333]
2024-12-18 01:48:50.840067: Epoch time: 522.87 s
2024-12-18 01:48:52.933953: Training done.
2024-12-18 01:48:53.274238: Using splits from existing split file: /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset309_Sohee_Calcium_OCT_CrossValidation/splits_final_60.json
2024-12-18 01:48:53.295619: The split file contains 5 splits.
2024-12-18 01:48:53.296716: Desired fold for training: 1
2024-12-18 01:48:53.297439: This split has 4 training and 5 validation cases.
2024-12-18 01:48:53.298367: predicting 101-019
2024-12-18 01:48:53.343276: 101-019, shape torch.Size([1, 375, 498, 498]), rank 0
2024-12-18 01:51:29.375621: predicting 101-045
2024-12-18 01:51:29.391917: 101-045, shape torch.Size([1, 375, 498, 498]), rank 0
2024-12-18 01:53:31.092485: predicting 106-002
2024-12-18 01:53:31.164388: 106-002, shape torch.Size([1, 539, 498, 498]), rank 0
2024-12-18 01:56:05.321734: predicting 704-003
2024-12-18 01:56:05.339527: 704-003, shape torch.Size([1, 375, 498, 498]), rank 0
2024-12-18 01:58:06.913491: predicting 706-005
2024-12-18 01:58:06.930027: 706-005, shape torch.Size([1, 375, 498, 498]), rank 0
2024-12-18 02:00:33.625053: Validation complete
2024-12-18 02:00:33.625716: Mean Validation Dice:  0.7354076296969428
2024-12-17 13:50:37.401599: unpacking done...
2024-12-17 13:50:37.468549: Unable to plot network architecture: nnUNet_compile is enabled!
2024-12-17 13:50:37.532366: 
2024-12-17 13:50:37.533061: Epoch 0
2024-12-17 13:50:37.533825: Current learning rate: 0.01
2024-12-17 13:55:41.885090: Validation loss improved from 1000.00000 to -0.40719! Patience: 0/50
2024-12-17 13:55:41.885885: train_loss -0.3314
2024-12-17 13:55:41.886817: val_loss -0.4072
2024-12-17 13:55:41.887688: Pseudo dice [0.6749]
2024-12-17 13:55:41.888523: Epoch time: 304.35 s
2024-12-17 13:55:41.889433: Yayy! New best EMA pseudo Dice: 0.6749
2024-12-17 13:55:43.587096: 
2024-12-17 13:55:43.588365: Epoch 1
2024-12-17 13:55:43.589281: Current learning rate: 0.00994
2024-12-17 13:58:57.610951: Validation loss improved from -0.40719 to -0.43024! Patience: 0/50
2024-12-17 13:58:57.612031: train_loss -0.5026
2024-12-17 13:58:57.613052: val_loss -0.4302
2024-12-17 13:58:57.613941: Pseudo dice [0.6753]
2024-12-17 13:58:57.614919: Epoch time: 194.03 s
2024-12-17 13:58:57.615808: Yayy! New best EMA pseudo Dice: 0.6749
2024-12-17 13:58:59.500789: 
2024-12-17 13:58:59.501848: Epoch 2
2024-12-17 13:58:59.502838: Current learning rate: 0.00988
2024-12-17 14:02:40.567896: Validation loss did not improve from -0.43024. Patience: 1/50
2024-12-17 14:02:40.568977: train_loss -0.5308
2024-12-17 14:02:40.569854: val_loss -0.4166
2024-12-17 14:02:40.570683: Pseudo dice [0.6707]
2024-12-17 14:02:40.571546: Epoch time: 221.07 s
2024-12-17 14:02:42.141195: 
2024-12-17 14:02:42.142393: Epoch 3
2024-12-17 14:02:42.143460: Current learning rate: 0.00982
2024-12-17 14:06:18.825652: Validation loss improved from -0.43024 to -0.47133! Patience: 1/50
2024-12-17 14:06:18.826449: train_loss -0.5619
2024-12-17 14:06:18.827302: val_loss -0.4713
2024-12-17 14:06:18.827927: Pseudo dice [0.7155]
2024-12-17 14:06:18.828563: Epoch time: 216.69 s
2024-12-17 14:06:18.829175: Yayy! New best EMA pseudo Dice: 0.6786
2024-12-17 14:06:20.707884: 
2024-12-17 14:06:20.708901: Epoch 4
2024-12-17 14:06:20.709743: Current learning rate: 0.00976
2024-12-17 14:10:15.796486: Validation loss did not improve from -0.47133. Patience: 1/50
2024-12-17 14:10:15.797258: train_loss -0.5847
2024-12-17 14:10:15.798121: val_loss -0.456
2024-12-17 14:10:15.798964: Pseudo dice [0.7024]
2024-12-17 14:10:15.799948: Epoch time: 235.09 s
2024-12-17 14:10:16.163237: Yayy! New best EMA pseudo Dice: 0.681
2024-12-17 14:10:18.119528: 
2024-12-17 14:10:18.120512: Epoch 5
2024-12-17 14:10:18.121205: Current learning rate: 0.0097
2024-12-17 14:14:03.744821: Validation loss improved from -0.47133 to -0.47809! Patience: 1/50
2024-12-17 14:14:03.745680: train_loss -0.6004
2024-12-17 14:14:03.746488: val_loss -0.4781
2024-12-17 14:14:03.747116: Pseudo dice [0.7154]
2024-12-17 14:14:03.747753: Epoch time: 225.63 s
2024-12-17 14:14:03.748452: Yayy! New best EMA pseudo Dice: 0.6844
2024-12-17 14:14:05.694703: 
2024-12-17 14:14:05.695920: Epoch 6
2024-12-17 14:14:05.696794: Current learning rate: 0.00964
2024-12-17 14:17:52.536119: Validation loss improved from -0.47809 to -0.48473! Patience: 0/50
2024-12-17 14:17:52.537101: train_loss -0.6123
2024-12-17 14:17:52.538035: val_loss -0.4847
2024-12-17 14:17:52.538894: Pseudo dice [0.721]
2024-12-17 14:17:52.539762: Epoch time: 226.84 s
2024-12-17 14:17:52.540616: Yayy! New best EMA pseudo Dice: 0.6881
2024-12-17 14:17:54.473067: 
2024-12-17 14:17:54.474447: Epoch 7
2024-12-17 14:17:54.475279: Current learning rate: 0.00958
2024-12-17 14:21:53.375678: Validation loss improved from -0.48473 to -0.48838! Patience: 0/50
2024-12-17 14:21:53.376656: train_loss -0.6147
2024-12-17 14:21:53.377602: val_loss -0.4884
2024-12-17 14:21:53.378337: Pseudo dice [0.7267]
2024-12-17 14:21:53.379091: Epoch time: 238.91 s
2024-12-17 14:21:53.379977: Yayy! New best EMA pseudo Dice: 0.6919
2024-12-17 14:21:55.356075: 
2024-12-17 14:21:55.357268: Epoch 8
2024-12-17 14:21:55.358117: Current learning rate: 0.00952
2024-12-17 14:25:53.714268: Validation loss did not improve from -0.48838. Patience: 1/50
2024-12-17 14:25:53.715343: train_loss -0.6323
2024-12-17 14:25:53.716137: val_loss -0.4635
2024-12-17 14:25:53.716869: Pseudo dice [0.7015]
2024-12-17 14:25:53.717745: Epoch time: 238.36 s
2024-12-17 14:25:53.718498: Yayy! New best EMA pseudo Dice: 0.6929
2024-12-17 14:25:56.206056: 
2024-12-17 14:25:56.207575: Epoch 9
2024-12-17 14:25:56.208602: Current learning rate: 0.00946
2024-12-17 14:29:43.564157: Validation loss improved from -0.48838 to -0.51987! Patience: 1/50
2024-12-17 14:29:43.565169: train_loss -0.6386
2024-12-17 14:29:43.566155: val_loss -0.5199
2024-12-17 14:29:43.567122: Pseudo dice [0.7405]
2024-12-17 14:29:43.567949: Epoch time: 227.36 s
2024-12-17 14:29:44.003358: Yayy! New best EMA pseudo Dice: 0.6976
2024-12-17 14:29:45.869456: 
2024-12-17 14:29:45.870531: Epoch 10
2024-12-17 14:29:45.871581: Current learning rate: 0.0094
2024-12-17 14:33:47.302571: Validation loss did not improve from -0.51987. Patience: 1/50
2024-12-17 14:33:47.303556: train_loss -0.6464
2024-12-17 14:33:47.304667: val_loss -0.4431
2024-12-17 14:33:47.305711: Pseudo dice [0.6931]
2024-12-17 14:33:47.306617: Epoch time: 241.44 s
2024-12-17 14:33:48.743588: 
2024-12-17 14:33:48.744667: Epoch 11
2024-12-17 14:33:48.745436: Current learning rate: 0.00934
2024-12-17 14:37:39.914853: Validation loss did not improve from -0.51987. Patience: 2/50
2024-12-17 14:37:39.915899: train_loss -0.6519
2024-12-17 14:37:39.916847: val_loss -0.4753
2024-12-17 14:37:39.917734: Pseudo dice [0.7143]
2024-12-17 14:37:39.918549: Epoch time: 231.17 s
2024-12-17 14:37:39.919364: Yayy! New best EMA pseudo Dice: 0.6989
2024-12-17 14:37:41.725992: 
2024-12-17 14:37:41.727162: Epoch 12
2024-12-17 14:37:41.728090: Current learning rate: 0.00928
2024-12-17 14:41:33.625478: Validation loss did not improve from -0.51987. Patience: 3/50
2024-12-17 14:41:33.626465: train_loss -0.6602
2024-12-17 14:41:33.627423: val_loss -0.4914
2024-12-17 14:41:33.628219: Pseudo dice [0.7198]
2024-12-17 14:41:33.628973: Epoch time: 231.9 s
2024-12-17 14:41:33.629695: Yayy! New best EMA pseudo Dice: 0.701
2024-12-17 14:41:35.520280: 
2024-12-17 14:41:35.521533: Epoch 13
2024-12-17 14:41:35.522327: Current learning rate: 0.00922
2024-12-17 14:45:26.597883: Validation loss did not improve from -0.51987. Patience: 4/50
2024-12-17 14:45:26.598990: train_loss -0.6595
2024-12-17 14:45:26.599906: val_loss -0.5053
2024-12-17 14:45:26.600627: Pseudo dice [0.7366]
2024-12-17 14:45:26.601360: Epoch time: 231.08 s
2024-12-17 14:45:26.602156: Yayy! New best EMA pseudo Dice: 0.7045
2024-12-17 14:45:28.454713: 
2024-12-17 14:45:28.455901: Epoch 14
2024-12-17 14:45:28.456661: Current learning rate: 0.00916
2024-12-17 14:49:17.921993: Validation loss did not improve from -0.51987. Patience: 5/50
2024-12-17 14:49:17.922919: train_loss -0.6514
2024-12-17 14:49:17.923707: val_loss -0.511
2024-12-17 14:49:17.924438: Pseudo dice [0.73]
2024-12-17 14:49:17.925316: Epoch time: 229.47 s
2024-12-17 14:49:18.360667: Yayy! New best EMA pseudo Dice: 0.7071
2024-12-17 14:49:20.278095: 
2024-12-17 14:49:20.279072: Epoch 15
2024-12-17 14:49:20.279763: Current learning rate: 0.0091
2024-12-17 14:53:11.946190: Validation loss did not improve from -0.51987. Patience: 6/50
2024-12-17 14:53:11.947202: train_loss -0.6721
2024-12-17 14:53:11.948119: val_loss -0.5158
2024-12-17 14:53:11.948931: Pseudo dice [0.7358]
2024-12-17 14:53:11.949933: Epoch time: 231.67 s
2024-12-17 14:53:11.950908: Yayy! New best EMA pseudo Dice: 0.71
2024-12-17 14:53:13.858029: 
2024-12-17 14:53:13.859138: Epoch 16
2024-12-17 14:53:13.860131: Current learning rate: 0.00903
2024-12-17 14:57:09.956934: Validation loss improved from -0.51987 to -0.52322! Patience: 6/50
2024-12-17 14:57:09.992785: train_loss -0.6789
2024-12-17 14:57:09.993536: val_loss -0.5232
2024-12-17 14:57:09.994257: Pseudo dice [0.737]
2024-12-17 14:57:09.995363: Epoch time: 236.14 s
2024-12-17 14:57:09.996177: Yayy! New best EMA pseudo Dice: 0.7127
2024-12-17 14:57:12.038131: 
2024-12-17 14:57:12.039155: Epoch 17
2024-12-17 14:57:12.040004: Current learning rate: 0.00897
2024-12-17 15:01:17.912638: Validation loss did not improve from -0.52322. Patience: 1/50
2024-12-17 15:01:17.914284: train_loss -0.6836
2024-12-17 15:01:17.915228: val_loss -0.5221
2024-12-17 15:01:17.915986: Pseudo dice [0.7425]
2024-12-17 15:01:17.916809: Epoch time: 245.88 s
2024-12-17 15:01:17.917598: Yayy! New best EMA pseudo Dice: 0.7157
2024-12-17 15:01:19.889343: 
2024-12-17 15:01:19.890566: Epoch 18
2024-12-17 15:01:19.891301: Current learning rate: 0.00891
2024-12-17 15:05:26.693226: Validation loss did not improve from -0.52322. Patience: 2/50
2024-12-17 15:05:26.694267: train_loss -0.6826
2024-12-17 15:05:26.695096: val_loss -0.4992
2024-12-17 15:05:26.695809: Pseudo dice [0.7299]
2024-12-17 15:05:26.696547: Epoch time: 246.81 s
2024-12-17 15:05:26.697304: Yayy! New best EMA pseudo Dice: 0.7171
2024-12-17 15:05:29.030428: 
2024-12-17 15:05:29.031614: Epoch 19
2024-12-17 15:05:29.032300: Current learning rate: 0.00885
2024-12-17 15:09:37.811773: Validation loss did not improve from -0.52322. Patience: 3/50
2024-12-17 15:09:37.812759: train_loss -0.6855
2024-12-17 15:09:37.813566: val_loss -0.4823
2024-12-17 15:09:37.814362: Pseudo dice [0.7134]
2024-12-17 15:09:37.815085: Epoch time: 248.78 s
2024-12-17 15:09:39.648969: 
2024-12-17 15:09:39.650195: Epoch 20
2024-12-17 15:09:39.650943: Current learning rate: 0.00879
2024-12-17 15:13:39.916635: Validation loss did not improve from -0.52322. Patience: 4/50
2024-12-17 15:13:39.917635: train_loss -0.6897
2024-12-17 15:13:39.918760: val_loss -0.507
2024-12-17 15:13:39.919913: Pseudo dice [0.7321]
2024-12-17 15:13:39.921050: Epoch time: 240.27 s
2024-12-17 15:13:39.922140: Yayy! New best EMA pseudo Dice: 0.7183
2024-12-17 15:13:41.846987: 
2024-12-17 15:13:41.848435: Epoch 21
2024-12-17 15:13:41.849512: Current learning rate: 0.00873
2024-12-17 15:17:50.237858: Validation loss did not improve from -0.52322. Patience: 5/50
2024-12-17 15:17:50.238667: train_loss -0.7031
2024-12-17 15:17:50.239474: val_loss -0.515
2024-12-17 15:17:50.240137: Pseudo dice [0.7339]
2024-12-17 15:17:50.240948: Epoch time: 248.39 s
2024-12-17 15:17:50.241593: Yayy! New best EMA pseudo Dice: 0.7198
2024-12-17 15:17:52.194062: 
2024-12-17 15:17:52.195045: Epoch 22
2024-12-17 15:17:52.195807: Current learning rate: 0.00867
2024-12-17 15:21:51.695445: Validation loss did not improve from -0.52322. Patience: 6/50
2024-12-17 15:21:51.696498: train_loss -0.7097
2024-12-17 15:21:51.697331: val_loss -0.5032
2024-12-17 15:21:51.698117: Pseudo dice [0.7177]
2024-12-17 15:21:51.698940: Epoch time: 239.5 s
2024-12-17 15:21:53.167378: 
2024-12-17 15:21:53.168603: Epoch 23
2024-12-17 15:21:53.169376: Current learning rate: 0.00861
2024-12-17 15:25:47.712702: Validation loss did not improve from -0.52322. Patience: 7/50
2024-12-17 15:25:47.713760: train_loss -0.6998
2024-12-17 15:25:47.714747: val_loss -0.5011
2024-12-17 15:25:47.715621: Pseudo dice [0.7235]
2024-12-17 15:25:47.716566: Epoch time: 234.55 s
2024-12-17 15:25:47.717481: Yayy! New best EMA pseudo Dice: 0.72
2024-12-17 15:25:49.634079: 
2024-12-17 15:25:49.635313: Epoch 24
2024-12-17 15:25:49.636067: Current learning rate: 0.00855
2024-12-17 15:29:45.617838: Validation loss did not improve from -0.52322. Patience: 8/50
2024-12-17 15:29:45.618678: train_loss -0.7102
2024-12-17 15:29:45.619439: val_loss -0.487
2024-12-17 15:29:45.620141: Pseudo dice [0.7182]
2024-12-17 15:29:45.620860: Epoch time: 235.99 s
2024-12-17 15:29:47.506631: 
2024-12-17 15:29:47.507859: Epoch 25
2024-12-17 15:29:47.508830: Current learning rate: 0.00849
2024-12-17 15:33:47.143816: Validation loss did not improve from -0.52322. Patience: 9/50
2024-12-17 15:33:47.144717: train_loss -0.71
2024-12-17 15:33:47.145456: val_loss -0.5164
2024-12-17 15:33:47.146222: Pseudo dice [0.7369]
2024-12-17 15:33:47.146996: Epoch time: 239.64 s
2024-12-17 15:33:47.147697: Yayy! New best EMA pseudo Dice: 0.7215
2024-12-17 15:33:48.976371: 
2024-12-17 15:33:48.977441: Epoch 26
2024-12-17 15:33:48.978273: Current learning rate: 0.00843
2024-12-17 15:37:55.002840: Validation loss did not improve from -0.52322. Patience: 10/50
2024-12-17 15:37:55.003726: train_loss -0.712
2024-12-17 15:37:55.004623: val_loss -0.4909
2024-12-17 15:37:55.005390: Pseudo dice [0.7169]
2024-12-17 15:37:55.006186: Epoch time: 246.03 s
2024-12-17 15:37:56.448489: 
2024-12-17 15:37:56.449637: Epoch 27
2024-12-17 15:37:56.450595: Current learning rate: 0.00836
2024-12-17 15:42:01.669305: Validation loss did not improve from -0.52322. Patience: 11/50
2024-12-17 15:42:01.670241: train_loss -0.7129
2024-12-17 15:42:01.671284: val_loss -0.4647
2024-12-17 15:42:01.672235: Pseudo dice [0.7072]
2024-12-17 15:42:01.673199: Epoch time: 245.22 s
2024-12-17 15:42:03.142940: 
2024-12-17 15:42:03.144064: Epoch 28
2024-12-17 15:42:03.144925: Current learning rate: 0.0083
2024-12-17 15:46:11.548973: Validation loss did not improve from -0.52322. Patience: 12/50
2024-12-17 15:46:11.550090: train_loss -0.7127
2024-12-17 15:46:11.550978: val_loss -0.5063
2024-12-17 15:46:11.551758: Pseudo dice [0.7355]
2024-12-17 15:46:11.552654: Epoch time: 248.41 s
2024-12-17 15:46:13.455748: 
2024-12-17 15:46:13.456915: Epoch 29
2024-12-17 15:46:13.457769: Current learning rate: 0.00824
2024-12-17 15:50:18.019597: Validation loss did not improve from -0.52322. Patience: 13/50
2024-12-17 15:50:18.020649: train_loss -0.7181
2024-12-17 15:50:18.021618: val_loss -0.5136
2024-12-17 15:50:18.022499: Pseudo dice [0.7332]
2024-12-17 15:50:18.023319: Epoch time: 244.57 s
2024-12-17 15:50:18.482030: Yayy! New best EMA pseudo Dice: 0.7225
2024-12-17 15:50:20.420410: 
2024-12-17 15:50:20.421599: Epoch 30
2024-12-17 15:50:20.422348: Current learning rate: 0.00818
2024-12-17 15:54:27.557914: Validation loss did not improve from -0.52322. Patience: 14/50
2024-12-17 15:54:27.558863: train_loss -0.7169
2024-12-17 15:54:27.559736: val_loss -0.5138
2024-12-17 15:54:27.560538: Pseudo dice [0.7359]
2024-12-17 15:54:27.561423: Epoch time: 247.14 s
2024-12-17 15:54:27.562161: Yayy! New best EMA pseudo Dice: 0.7238
2024-12-17 15:54:29.444341: 
2024-12-17 15:54:29.445488: Epoch 31
2024-12-17 15:54:29.446233: Current learning rate: 0.00812
2024-12-17 15:58:44.195536: Validation loss improved from -0.52322 to -0.54851! Patience: 14/50
2024-12-17 15:58:44.196470: train_loss -0.7238
2024-12-17 15:58:44.197343: val_loss -0.5485
2024-12-17 15:58:44.198116: Pseudo dice [0.7489]
2024-12-17 15:58:44.198961: Epoch time: 254.75 s
2024-12-17 15:58:44.199702: Yayy! New best EMA pseudo Dice: 0.7263
2024-12-17 15:58:46.168978: 
2024-12-17 15:58:46.170119: Epoch 32
2024-12-17 15:58:46.170899: Current learning rate: 0.00806
2024-12-17 16:02:53.374985: Validation loss did not improve from -0.54851. Patience: 1/50
2024-12-17 16:02:53.376002: train_loss -0.7226
2024-12-17 16:02:53.377005: val_loss -0.5174
2024-12-17 16:02:53.377927: Pseudo dice [0.7306]
2024-12-17 16:02:53.379002: Epoch time: 247.21 s
2024-12-17 16:02:53.379915: Yayy! New best EMA pseudo Dice: 0.7267
2024-12-17 16:02:55.260524: 
2024-12-17 16:02:55.262028: Epoch 33
2024-12-17 16:02:55.263070: Current learning rate: 0.008
2024-12-17 16:07:01.230262: Validation loss did not improve from -0.54851. Patience: 2/50
2024-12-17 16:07:01.231150: train_loss -0.7284
2024-12-17 16:07:01.232086: val_loss -0.5346
2024-12-17 16:07:01.232889: Pseudo dice [0.7441]
2024-12-17 16:07:01.233687: Epoch time: 245.97 s
2024-12-17 16:07:01.234421: Yayy! New best EMA pseudo Dice: 0.7285
2024-12-17 16:07:03.242981: 
2024-12-17 16:07:03.244055: Epoch 34
2024-12-17 16:07:03.244794: Current learning rate: 0.00793
2024-12-17 16:11:19.921004: Validation loss did not improve from -0.54851. Patience: 3/50
2024-12-17 16:11:19.921858: train_loss -0.7238
2024-12-17 16:11:19.922607: val_loss -0.5217
2024-12-17 16:11:19.923325: Pseudo dice [0.7386]
2024-12-17 16:11:19.924047: Epoch time: 256.68 s
2024-12-17 16:11:20.356388: Yayy! New best EMA pseudo Dice: 0.7295
2024-12-17 16:11:22.336390: 
2024-12-17 16:11:22.337530: Epoch 35
2024-12-17 16:11:22.338428: Current learning rate: 0.00787
2024-12-17 16:15:34.878919: Validation loss did not improve from -0.54851. Patience: 4/50
2024-12-17 16:15:34.879831: train_loss -0.7261
2024-12-17 16:15:34.880673: val_loss -0.518
2024-12-17 16:15:34.881501: Pseudo dice [0.7402]
2024-12-17 16:15:34.882279: Epoch time: 252.54 s
2024-12-17 16:15:34.882926: Yayy! New best EMA pseudo Dice: 0.7306
2024-12-17 16:15:36.869802: 
2024-12-17 16:15:36.871046: Epoch 36
2024-12-17 16:15:36.871938: Current learning rate: 0.00781
2024-12-17 16:19:57.444012: Validation loss did not improve from -0.54851. Patience: 5/50
2024-12-17 16:19:57.444949: train_loss -0.7291
2024-12-17 16:19:57.445673: val_loss -0.536
2024-12-17 16:19:57.446387: Pseudo dice [0.7431]
2024-12-17 16:19:57.447084: Epoch time: 260.58 s
2024-12-17 16:19:57.447696: Yayy! New best EMA pseudo Dice: 0.7318
2024-12-17 16:19:59.477715: 
2024-12-17 16:19:59.479108: Epoch 37
2024-12-17 16:19:59.479900: Current learning rate: 0.00775
2024-12-17 16:24:14.970016: Validation loss did not improve from -0.54851. Patience: 6/50
2024-12-17 16:24:14.970902: train_loss -0.7353
2024-12-17 16:24:14.971689: val_loss -0.4578
2024-12-17 16:24:14.972483: Pseudo dice [0.7037]
2024-12-17 16:24:14.973241: Epoch time: 255.49 s
2024-12-17 16:24:16.509204: 
2024-12-17 16:24:16.510231: Epoch 38
2024-12-17 16:24:16.511008: Current learning rate: 0.00769
2024-12-17 16:28:32.300413: Validation loss did not improve from -0.54851. Patience: 7/50
2024-12-17 16:28:32.301124: train_loss -0.7327
2024-12-17 16:28:32.301862: val_loss -0.5306
2024-12-17 16:28:32.302502: Pseudo dice [0.7507]
2024-12-17 16:28:32.303194: Epoch time: 255.79 s
2024-12-17 16:28:34.201460: 
2024-12-17 16:28:34.202605: Epoch 39
2024-12-17 16:28:34.203302: Current learning rate: 0.00763
2024-12-17 16:32:50.744763: Validation loss did not improve from -0.54851. Patience: 8/50
2024-12-17 16:32:50.745528: train_loss -0.7365
2024-12-17 16:32:50.746296: val_loss -0.5107
2024-12-17 16:32:50.746972: Pseudo dice [0.7393]
2024-12-17 16:32:50.747747: Epoch time: 256.55 s
2024-12-17 16:32:51.191255: Yayy! New best EMA pseudo Dice: 0.732
2024-12-17 16:32:53.224865: 
2024-12-17 16:32:53.226112: Epoch 40
2024-12-17 16:32:53.227090: Current learning rate: 0.00756
2024-12-17 16:37:14.914427: Validation loss did not improve from -0.54851. Patience: 9/50
2024-12-17 16:37:14.915316: train_loss -0.7395
2024-12-17 16:37:14.916322: val_loss -0.5233
2024-12-17 16:37:14.917110: Pseudo dice [0.7395]
2024-12-17 16:37:14.917875: Epoch time: 261.69 s
2024-12-17 16:37:14.918617: Yayy! New best EMA pseudo Dice: 0.7327
2024-12-17 16:37:16.957001: 
2024-12-17 16:37:16.958230: Epoch 41
2024-12-17 16:37:16.959110: Current learning rate: 0.0075
2024-12-17 16:41:28.572210: Validation loss did not improve from -0.54851. Patience: 10/50
2024-12-17 16:41:28.573173: train_loss -0.741
2024-12-17 16:41:28.574180: val_loss -0.4969
2024-12-17 16:41:28.574983: Pseudo dice [0.726]
2024-12-17 16:41:28.575687: Epoch time: 251.62 s
2024-12-17 16:41:29.981321: 
2024-12-17 16:41:29.982687: Epoch 42
2024-12-17 16:41:29.983943: Current learning rate: 0.00744
2024-12-17 16:45:44.558726: Validation loss did not improve from -0.54851. Patience: 11/50
2024-12-17 16:45:44.559586: train_loss -0.7443
2024-12-17 16:45:44.560405: val_loss -0.4883
2024-12-17 16:45:44.561131: Pseudo dice [0.7261]
2024-12-17 16:45:44.561837: Epoch time: 254.58 s
2024-12-17 16:45:45.974314: 
2024-12-17 16:45:45.975433: Epoch 43
2024-12-17 16:45:45.976308: Current learning rate: 0.00738
2024-12-17 16:49:56.411910: Validation loss did not improve from -0.54851. Patience: 12/50
2024-12-17 16:49:56.412787: train_loss -0.7451
2024-12-17 16:49:56.413674: val_loss -0.4342
2024-12-17 16:49:56.414278: Pseudo dice [0.7004]
2024-12-17 16:49:56.414915: Epoch time: 250.44 s
2024-12-17 16:49:57.823173: 
2024-12-17 16:49:57.824248: Epoch 44
2024-12-17 16:49:57.825016: Current learning rate: 0.00732
2024-12-17 16:53:58.023755: Validation loss did not improve from -0.54851. Patience: 13/50
2024-12-17 16:53:58.024573: train_loss -0.7425
2024-12-17 16:53:58.025492: val_loss -0.514
2024-12-17 16:53:58.026284: Pseudo dice [0.7359]
2024-12-17 16:53:58.026988: Epoch time: 240.2 s
2024-12-17 16:53:59.916532: 
2024-12-17 16:53:59.917511: Epoch 45
2024-12-17 16:53:59.918324: Current learning rate: 0.00725
2024-12-17 16:58:14.017645: Validation loss did not improve from -0.54851. Patience: 14/50
2024-12-17 16:58:14.018408: train_loss -0.7399
2024-12-17 16:58:14.019542: val_loss -0.4758
2024-12-17 16:58:14.020404: Pseudo dice [0.7251]
2024-12-17 16:58:14.021314: Epoch time: 254.1 s
2024-12-17 16:58:15.411249: 
2024-12-17 16:58:15.412720: Epoch 46
2024-12-17 16:58:15.413778: Current learning rate: 0.00719
2024-12-17 17:02:30.496111: Validation loss did not improve from -0.54851. Patience: 15/50
2024-12-17 17:02:30.496814: train_loss -0.7473
2024-12-17 17:02:30.497578: val_loss -0.5287
2024-12-17 17:02:30.498496: Pseudo dice [0.7461]
2024-12-17 17:02:30.499305: Epoch time: 255.09 s
2024-12-17 17:02:31.926441: 
2024-12-17 17:02:31.927261: Epoch 47
2024-12-17 17:02:31.927933: Current learning rate: 0.00713
2024-12-17 17:06:55.117780: Validation loss did not improve from -0.54851. Patience: 16/50
2024-12-17 17:06:55.119076: train_loss -0.7431
2024-12-17 17:06:55.119824: val_loss -0.4643
2024-12-17 17:06:55.120467: Pseudo dice [0.7106]
2024-12-17 17:06:55.121131: Epoch time: 263.19 s
2024-12-17 17:06:56.544555: 
2024-12-17 17:06:56.545730: Epoch 48
2024-12-17 17:06:56.546408: Current learning rate: 0.00707
2024-12-17 17:10:59.998492: Validation loss did not improve from -0.54851. Patience: 17/50
2024-12-17 17:10:59.999673: train_loss -0.7478
2024-12-17 17:11:00.000744: val_loss -0.4966
2024-12-17 17:11:00.001649: Pseudo dice [0.7327]
2024-12-17 17:11:00.002665: Epoch time: 243.46 s
2024-12-17 17:11:01.556352: 
2024-12-17 17:11:01.557431: Epoch 49
2024-12-17 17:11:01.558296: Current learning rate: 0.007
2024-12-17 17:15:06.828480: Validation loss did not improve from -0.54851. Patience: 18/50
2024-12-17 17:15:06.829566: train_loss -0.7444
2024-12-17 17:15:06.830490: val_loss -0.4837
2024-12-17 17:15:06.831295: Pseudo dice [0.7202]
2024-12-17 17:15:06.832119: Epoch time: 245.27 s
2024-12-17 17:15:09.196932: 
2024-12-17 17:15:09.198038: Epoch 50
2024-12-17 17:15:09.198844: Current learning rate: 0.00694
2024-12-17 17:19:19.428801: Validation loss did not improve from -0.54851. Patience: 19/50
2024-12-17 17:19:19.429541: train_loss -0.751
2024-12-17 17:19:19.430307: val_loss -0.4933
2024-12-17 17:19:19.431007: Pseudo dice [0.7365]
2024-12-17 17:19:19.431766: Epoch time: 250.23 s
2024-12-17 17:19:20.876122: 
2024-12-17 17:19:20.877007: Epoch 51
2024-12-17 17:19:20.877780: Current learning rate: 0.00688
2024-12-17 17:23:28.048426: Validation loss did not improve from -0.54851. Patience: 20/50
2024-12-17 17:23:28.049251: train_loss -0.7536
2024-12-17 17:23:28.050043: val_loss -0.4725
2024-12-17 17:23:28.050754: Pseudo dice [0.7177]
2024-12-17 17:23:28.051439: Epoch time: 247.17 s
2024-12-17 17:23:29.462324: 
2024-12-17 17:23:29.463356: Epoch 52
2024-12-17 17:23:29.464219: Current learning rate: 0.00682
2024-12-17 17:27:57.342461: Validation loss did not improve from -0.54851. Patience: 21/50
2024-12-17 17:27:57.344178: train_loss -0.758
2024-12-17 17:27:57.345683: val_loss -0.5004
2024-12-17 17:27:57.346656: Pseudo dice [0.7352]
2024-12-17 17:27:57.347571: Epoch time: 267.88 s
2024-12-17 17:27:58.807650: 
2024-12-17 17:27:58.808810: Epoch 53
2024-12-17 17:27:58.809539: Current learning rate: 0.00675
2024-12-17 17:32:14.351263: Validation loss did not improve from -0.54851. Patience: 22/50
2024-12-17 17:32:14.352104: train_loss -0.7559
2024-12-17 17:32:14.352933: val_loss -0.4953
2024-12-17 17:32:14.353579: Pseudo dice [0.7257]
2024-12-17 17:32:14.354235: Epoch time: 255.55 s
2024-12-17 17:32:15.798541: 
2024-12-17 17:32:15.799567: Epoch 54
2024-12-17 17:32:15.800240: Current learning rate: 0.00669
2024-12-17 17:36:29.188141: Validation loss did not improve from -0.54851. Patience: 23/50
2024-12-17 17:36:29.189127: train_loss -0.7508
2024-12-17 17:36:29.190180: val_loss -0.4983
2024-12-17 17:36:29.191215: Pseudo dice [0.7324]
2024-12-17 17:36:29.192127: Epoch time: 253.39 s
2024-12-17 17:36:31.074465: 
2024-12-17 17:36:31.075797: Epoch 55
2024-12-17 17:36:31.076829: Current learning rate: 0.00663
2024-12-17 17:40:40.182712: Validation loss did not improve from -0.54851. Patience: 24/50
2024-12-17 17:40:40.183595: train_loss -0.756
2024-12-17 17:40:40.184407: val_loss -0.4955
2024-12-17 17:40:40.185177: Pseudo dice [0.7317]
2024-12-17 17:40:40.185987: Epoch time: 249.11 s
2024-12-17 17:40:41.607078: 
2024-12-17 17:40:41.608285: Epoch 56
2024-12-17 17:40:41.609123: Current learning rate: 0.00657
2024-12-17 17:44:59.868922: Validation loss did not improve from -0.54851. Patience: 25/50
2024-12-17 17:44:59.869938: train_loss -0.7606
2024-12-17 17:44:59.870949: val_loss -0.5204
2024-12-17 17:44:59.871900: Pseudo dice [0.7479]
2024-12-17 17:44:59.872817: Epoch time: 258.26 s
2024-12-17 17:45:01.291276: 
2024-12-17 17:45:01.292672: Epoch 57
2024-12-17 17:45:01.293817: Current learning rate: 0.0065
2024-12-17 17:49:05.038246: Validation loss did not improve from -0.54851. Patience: 26/50
2024-12-17 17:49:05.039072: train_loss -0.7612
2024-12-17 17:49:05.039808: val_loss -0.5046
2024-12-17 17:49:05.040553: Pseudo dice [0.73]
2024-12-17 17:49:05.041251: Epoch time: 243.75 s
2024-12-17 17:49:06.516904: 
2024-12-17 17:49:06.517889: Epoch 58
2024-12-17 17:49:06.518657: Current learning rate: 0.00644
2024-12-17 17:53:19.686348: Validation loss did not improve from -0.54851. Patience: 27/50
2024-12-17 17:53:19.687392: train_loss -0.7588
2024-12-17 17:53:19.688232: val_loss -0.4869
2024-12-17 17:53:19.688894: Pseudo dice [0.721]
2024-12-17 17:53:19.689555: Epoch time: 253.17 s
2024-12-17 17:53:21.213938: 
2024-12-17 17:53:21.215103: Epoch 59
2024-12-17 17:53:21.215890: Current learning rate: 0.00638
2024-12-17 17:57:28.691564: Validation loss did not improve from -0.54851. Patience: 28/50
2024-12-17 17:57:28.692690: train_loss -0.7612
2024-12-17 17:57:28.693518: val_loss -0.5335
2024-12-17 17:57:28.694364: Pseudo dice [0.744]
2024-12-17 17:57:28.695300: Epoch time: 247.48 s
2024-12-17 17:57:30.668550: 
2024-12-17 17:57:30.669712: Epoch 60
2024-12-17 17:57:30.671150: Current learning rate: 0.00631
2024-12-17 18:01:47.500792: Validation loss did not improve from -0.54851. Patience: 29/50
2024-12-17 18:01:47.501745: train_loss -0.7655
2024-12-17 18:01:47.502514: val_loss -0.5083
2024-12-17 18:01:47.503247: Pseudo dice [0.7298]
2024-12-17 18:01:47.503993: Epoch time: 256.83 s
2024-12-17 18:01:49.445490: 
2024-12-17 18:01:49.446683: Epoch 61
2024-12-17 18:01:49.447374: Current learning rate: 0.00625
2024-12-17 18:06:01.565312: Validation loss did not improve from -0.54851. Patience: 30/50
2024-12-17 18:06:01.566215: train_loss -0.7698
2024-12-17 18:06:01.567116: val_loss -0.5151
2024-12-17 18:06:01.567800: Pseudo dice [0.7448]
2024-12-17 18:06:01.568481: Epoch time: 252.12 s
2024-12-17 18:06:03.034245: 
2024-12-17 18:06:03.035595: Epoch 62
2024-12-17 18:06:03.036402: Current learning rate: 0.00619
2024-12-17 18:10:14.519256: Validation loss did not improve from -0.54851. Patience: 31/50
2024-12-17 18:10:14.521648: train_loss -0.7669
2024-12-17 18:10:14.522590: val_loss -0.51
2024-12-17 18:10:14.523293: Pseudo dice [0.7369]
2024-12-17 18:10:14.524369: Epoch time: 251.49 s
2024-12-17 18:10:14.525280: Yayy! New best EMA pseudo Dice: 0.7329
2024-12-17 18:10:16.504211: 
2024-12-17 18:10:16.505124: Epoch 63
2024-12-17 18:10:16.505927: Current learning rate: 0.00612
2024-12-17 18:14:29.950940: Validation loss did not improve from -0.54851. Patience: 32/50
2024-12-17 18:14:29.952352: train_loss -0.7687
2024-12-17 18:14:29.953435: val_loss -0.5222
2024-12-17 18:14:29.954213: Pseudo dice [0.7355]
2024-12-17 18:14:29.955039: Epoch time: 253.45 s
2024-12-17 18:14:29.955849: Yayy! New best EMA pseudo Dice: 0.7331
2024-12-17 18:14:31.919508: 
2024-12-17 18:14:31.920841: Epoch 64
2024-12-17 18:14:31.921827: Current learning rate: 0.00606
2024-12-17 18:18:48.224648: Validation loss did not improve from -0.54851. Patience: 33/50
2024-12-17 18:18:48.225843: train_loss -0.7699
2024-12-17 18:18:48.226615: val_loss -0.504
2024-12-17 18:18:48.227430: Pseudo dice [0.7267]
2024-12-17 18:18:48.228224: Epoch time: 256.31 s
2024-12-17 18:18:50.120083: 
2024-12-17 18:18:50.121072: Epoch 65
2024-12-17 18:18:50.121866: Current learning rate: 0.006
2024-12-17 18:23:13.580065: Validation loss did not improve from -0.54851. Patience: 34/50
2024-12-17 18:23:13.580963: train_loss -0.7705
2024-12-17 18:23:13.581921: val_loss -0.5047
2024-12-17 18:23:13.582580: Pseudo dice [0.7432]
2024-12-17 18:23:13.583307: Epoch time: 263.46 s
2024-12-17 18:23:13.584084: Yayy! New best EMA pseudo Dice: 0.7336
2024-12-17 18:23:15.579112: 
2024-12-17 18:23:15.580443: Epoch 66
2024-12-17 18:23:15.581136: Current learning rate: 0.00593
2024-12-17 18:27:32.131773: Validation loss did not improve from -0.54851. Patience: 35/50
2024-12-17 18:27:32.132780: train_loss -0.7677
2024-12-17 18:27:32.133602: val_loss -0.5035
2024-12-17 18:27:32.134371: Pseudo dice [0.7327]
2024-12-17 18:27:32.135158: Epoch time: 256.56 s
2024-12-17 18:27:33.655537: 
2024-12-17 18:27:33.656982: Epoch 67
2024-12-17 18:27:33.657746: Current learning rate: 0.00587
2024-12-17 18:31:49.455790: Validation loss did not improve from -0.54851. Patience: 36/50
2024-12-17 18:31:49.456721: train_loss -0.7685
2024-12-17 18:31:49.457485: val_loss -0.5008
2024-12-17 18:31:49.458365: Pseudo dice [0.7309]
2024-12-17 18:31:49.459222: Epoch time: 255.8 s
2024-12-17 18:31:50.906615: 
2024-12-17 18:31:50.907924: Epoch 68
2024-12-17 18:31:50.908711: Current learning rate: 0.00581
2024-12-17 18:36:00.123735: Validation loss did not improve from -0.54851. Patience: 37/50
2024-12-17 18:36:00.124620: train_loss -0.7689
2024-12-17 18:36:00.125493: val_loss -0.4902
2024-12-17 18:36:00.126294: Pseudo dice [0.7263]
2024-12-17 18:36:00.127161: Epoch time: 249.22 s
2024-12-17 18:36:01.652172: 
2024-12-17 18:36:01.653502: Epoch 69
2024-12-17 18:36:01.654365: Current learning rate: 0.00574
2024-12-17 18:40:23.397868: Validation loss did not improve from -0.54851. Patience: 38/50
2024-12-17 18:40:23.398876: train_loss -0.772
2024-12-17 18:40:23.399651: val_loss -0.4384
2024-12-17 18:40:23.400368: Pseudo dice [0.704]
2024-12-17 18:40:23.401170: Epoch time: 261.75 s
2024-12-17 18:40:25.376887: 
2024-12-17 18:40:25.377967: Epoch 70
2024-12-17 18:40:25.378767: Current learning rate: 0.00568
2024-12-17 18:44:38.615027: Validation loss did not improve from -0.54851. Patience: 39/50
2024-12-17 18:44:38.615977: train_loss -0.7734
2024-12-17 18:44:38.616823: val_loss -0.5015
2024-12-17 18:44:38.617682: Pseudo dice [0.7446]
2024-12-17 18:44:38.618556: Epoch time: 253.24 s
2024-12-17 18:44:40.087416: 
2024-12-17 18:44:40.088589: Epoch 71
2024-12-17 18:44:40.089597: Current learning rate: 0.00562
2024-12-17 18:49:00.299724: Validation loss did not improve from -0.54851. Patience: 40/50
2024-12-17 18:49:00.300394: train_loss -0.7765
2024-12-17 18:49:00.301131: val_loss -0.4664
2024-12-17 18:49:00.301730: Pseudo dice [0.7277]
2024-12-17 18:49:00.302409: Epoch time: 260.21 s
2024-12-17 18:49:02.153960: 
2024-12-17 18:49:02.155057: Epoch 72
2024-12-17 18:49:02.155733: Current learning rate: 0.00555
2024-12-17 18:53:18.256096: Validation loss did not improve from -0.54851. Patience: 41/50
2024-12-17 18:53:18.257212: train_loss -0.7732
2024-12-17 18:53:18.258020: val_loss -0.4956
2024-12-17 18:53:18.258817: Pseudo dice [0.7405]
2024-12-17 18:53:18.259551: Epoch time: 256.1 s
2024-12-17 18:53:19.767194: 
2024-12-17 18:53:19.768412: Epoch 73
2024-12-17 18:53:19.769359: Current learning rate: 0.00549
2024-12-17 18:57:37.235130: Validation loss did not improve from -0.54851. Patience: 42/50
2024-12-17 18:57:37.236020: train_loss -0.7736
2024-12-17 18:57:37.237020: val_loss -0.5125
2024-12-17 18:57:37.238055: Pseudo dice [0.7381]
2024-12-17 18:57:37.239035: Epoch time: 257.47 s
2024-12-17 18:57:38.735414: 
2024-12-17 18:57:38.736767: Epoch 74
2024-12-17 18:57:38.737790: Current learning rate: 0.00542
2024-12-17 19:02:00.724285: Validation loss did not improve from -0.54851. Patience: 43/50
2024-12-17 19:02:00.725260: train_loss -0.7791
2024-12-17 19:02:00.726233: val_loss -0.486
2024-12-17 19:02:00.727011: Pseudo dice [0.7237]
2024-12-17 19:02:00.728034: Epoch time: 261.99 s
2024-12-17 19:02:02.657543: 
2024-12-17 19:02:02.658731: Epoch 75
2024-12-17 19:02:02.659539: Current learning rate: 0.00536
2024-12-17 19:06:14.260584: Validation loss did not improve from -0.54851. Patience: 44/50
2024-12-17 19:06:14.261560: train_loss -0.78
2024-12-17 19:06:14.262326: val_loss -0.5097
2024-12-17 19:06:14.263038: Pseudo dice [0.7419]
2024-12-17 19:06:14.263707: Epoch time: 251.61 s
2024-12-17 19:06:15.720191: 
2024-12-17 19:06:15.721159: Epoch 76
2024-12-17 19:06:15.721895: Current learning rate: 0.00529
2024-12-17 19:10:43.522143: Validation loss did not improve from -0.54851. Patience: 45/50
2024-12-17 19:10:43.523114: train_loss -0.7745
2024-12-17 19:10:43.524013: val_loss -0.5121
2024-12-17 19:10:43.524664: Pseudo dice [0.738]
2024-12-17 19:10:43.525430: Epoch time: 267.8 s
2024-12-17 19:10:44.989626: 
2024-12-17 19:10:44.990918: Epoch 77
2024-12-17 19:10:44.992205: Current learning rate: 0.00523
2024-12-17 19:15:07.865083: Validation loss did not improve from -0.54851. Patience: 46/50
2024-12-17 19:15:07.866555: train_loss -0.7778
2024-12-17 19:15:07.868140: val_loss -0.4993
2024-12-17 19:15:07.868817: Pseudo dice [0.7349]
2024-12-17 19:15:07.869695: Epoch time: 262.88 s
2024-12-17 19:15:09.390483: 
2024-12-17 19:15:09.391643: Epoch 78
2024-12-17 19:15:09.392451: Current learning rate: 0.00517
2024-12-17 19:19:28.801455: Validation loss did not improve from -0.54851. Patience: 47/50
2024-12-17 19:19:28.802638: train_loss -0.7778
2024-12-17 19:19:28.803429: val_loss -0.5035
2024-12-17 19:19:28.804085: Pseudo dice [0.7305]
2024-12-17 19:19:28.804821: Epoch time: 259.41 s
2024-12-17 19:19:30.337031: 
2024-12-17 19:19:30.338115: Epoch 79
2024-12-17 19:19:30.338901: Current learning rate: 0.0051
2024-12-17 19:23:52.080066: Validation loss did not improve from -0.54851. Patience: 48/50
2024-12-17 19:23:52.080759: train_loss -0.7799
2024-12-17 19:23:52.081383: val_loss -0.495
2024-12-17 19:23:52.082008: Pseudo dice [0.7222]
2024-12-17 19:23:52.082630: Epoch time: 261.75 s
2024-12-17 19:23:53.995917: 
2024-12-17 19:23:53.997245: Epoch 80
2024-12-17 19:23:53.998070: Current learning rate: 0.00504
2024-12-17 19:28:13.880613: Validation loss did not improve from -0.54851. Patience: 49/50
2024-12-17 19:28:13.882347: train_loss -0.7804
2024-12-17 19:28:13.883335: val_loss -0.5234
2024-12-17 19:28:13.884160: Pseudo dice [0.7493]
2024-12-17 19:28:13.884842: Epoch time: 259.89 s
2024-12-17 19:28:13.885520: Yayy! New best EMA pseudo Dice: 0.7337
2024-12-17 19:28:15.846885: 
2024-12-17 19:28:15.848071: Epoch 81
2024-12-17 19:28:15.848901: Current learning rate: 0.00497
2024-12-17 19:32:38.487009: Validation loss did not improve from -0.54851. Patience: 50/50
2024-12-17 19:32:38.487955: train_loss -0.781
2024-12-17 19:32:38.489005: val_loss -0.4969
2024-12-17 19:32:38.490007: Pseudo dice [0.7316]
2024-12-17 19:32:38.490883: Epoch time: 262.64 s
2024-12-17 19:32:40.370795: 
2024-12-17 19:32:40.372089: Epoch 82
2024-12-17 19:32:40.373045: Current learning rate: 0.00491
2024-12-17 19:36:58.232131: Validation loss did not improve from -0.54851. Patience: 51/50
2024-12-17 19:36:58.233010: train_loss -0.7822
2024-12-17 19:36:58.233775: val_loss -0.5034
2024-12-17 19:36:58.234464: Pseudo dice [0.7355]
2024-12-17 19:36:58.235361: Epoch time: 257.86 s
2024-12-17 19:36:59.714781: 
2024-12-17 19:36:59.715858: Epoch 83
2024-12-17 19:36:59.716685: Current learning rate: 0.00484
2024-12-17 19:41:17.610253: Validation loss did not improve from -0.54851. Patience: 52/50
2024-12-17 19:41:17.611313: train_loss -0.7866
2024-12-17 19:41:17.612245: val_loss -0.51
2024-12-17 19:41:17.613074: Pseudo dice [0.7396]
2024-12-17 19:41:17.613982: Epoch time: 257.9 s
2024-12-17 19:41:17.614856: Yayy! New best EMA pseudo Dice: 0.7343
2024-12-17 19:41:19.522204: 
2024-12-17 19:41:19.523574: Epoch 84
2024-12-17 19:41:19.524419: Current learning rate: 0.00478
2024-12-17 19:45:34.062157: Validation loss did not improve from -0.54851. Patience: 53/50
2024-12-17 19:45:34.063147: train_loss -0.7854
2024-12-17 19:45:34.063887: val_loss -0.4581
2024-12-17 19:45:34.064585: Pseudo dice [0.7042]
2024-12-17 19:45:34.065327: Epoch time: 254.54 s
2024-12-17 19:45:36.065761: 
2024-12-17 19:45:36.067226: Epoch 85
2024-12-17 19:45:36.068233: Current learning rate: 0.00471
2024-12-17 19:49:50.838736: Validation loss did not improve from -0.54851. Patience: 54/50
2024-12-17 19:49:50.839683: train_loss -0.7847
2024-12-17 19:49:50.840450: val_loss -0.4905
2024-12-17 19:49:50.841077: Pseudo dice [0.7308]
2024-12-17 19:49:50.841702: Epoch time: 254.78 s
2024-12-17 19:49:52.333888: 
2024-12-17 19:49:52.335188: Epoch 86
2024-12-17 19:49:52.336004: Current learning rate: 0.00465
2024-12-17 19:54:23.243947: Validation loss did not improve from -0.54851. Patience: 55/50
2024-12-17 19:54:23.245050: train_loss -0.7823
2024-12-17 19:54:23.246103: val_loss -0.5015
2024-12-17 19:54:23.246898: Pseudo dice [0.7379]
2024-12-17 19:54:23.247747: Epoch time: 270.91 s
2024-12-17 19:54:24.705997: 
2024-12-17 19:54:24.707293: Epoch 87
2024-12-17 19:54:24.708149: Current learning rate: 0.00458
2024-12-17 19:58:42.424587: Validation loss did not improve from -0.54851. Patience: 56/50
2024-12-17 19:58:42.425590: train_loss -0.7849
2024-12-17 19:58:42.426506: val_loss -0.5405
2024-12-17 19:58:42.427267: Pseudo dice [0.7525]
2024-12-17 19:58:42.428084: Epoch time: 257.72 s
2024-12-17 19:58:43.818940: 
2024-12-17 19:58:43.820284: Epoch 88
2024-12-17 19:58:43.821391: Current learning rate: 0.00452
2024-12-17 20:03:05.219823: Validation loss did not improve from -0.54851. Patience: 57/50
2024-12-17 20:03:05.220772: train_loss -0.787
2024-12-17 20:03:05.221649: val_loss -0.5277
2024-12-17 20:03:05.222503: Pseudo dice [0.7521]
2024-12-17 20:03:05.223301: Epoch time: 261.4 s
2024-12-17 20:03:05.224066: Yayy! New best EMA pseudo Dice: 0.7358
2024-12-17 20:03:07.192816: 
2024-12-17 20:03:07.194168: Epoch 89
2024-12-17 20:03:07.195053: Current learning rate: 0.00445
2024-12-17 20:07:07.529015: Validation loss did not improve from -0.54851. Patience: 58/50
2024-12-17 20:07:07.529820: train_loss -0.7891
2024-12-17 20:07:07.530576: val_loss -0.4986
2024-12-17 20:07:07.531278: Pseudo dice [0.7317]
2024-12-17 20:07:07.531972: Epoch time: 240.34 s
2024-12-17 20:07:09.425102: 
2024-12-17 20:07:09.426172: Epoch 90
2024-12-17 20:07:09.426888: Current learning rate: 0.00438
2024-12-17 20:11:19.143685: Validation loss did not improve from -0.54851. Patience: 59/50
2024-12-17 20:11:19.144696: train_loss -0.7888
2024-12-17 20:11:19.145572: val_loss -0.5061
2024-12-17 20:11:19.146293: Pseudo dice [0.7344]
2024-12-17 20:11:19.147108: Epoch time: 249.72 s
2024-12-17 20:11:20.674365: 
2024-12-17 20:11:20.675862: Epoch 91
2024-12-17 20:11:20.676602: Current learning rate: 0.00432
2024-12-17 20:15:43.765601: Validation loss did not improve from -0.54851. Patience: 60/50
2024-12-17 20:15:43.766392: train_loss -0.7882
2024-12-17 20:15:43.767258: val_loss -0.4672
2024-12-17 20:15:43.767894: Pseudo dice [0.7136]
2024-12-17 20:15:43.768544: Epoch time: 263.09 s
2024-12-17 20:15:45.145577: 
2024-12-17 20:15:45.146987: Epoch 92
2024-12-17 20:15:45.148012: Current learning rate: 0.00425
2024-12-17 20:20:17.467060: Validation loss did not improve from -0.54851. Patience: 61/50
2024-12-17 20:20:17.468266: train_loss -0.7895
2024-12-17 20:20:17.469097: val_loss -0.4996
2024-12-17 20:20:17.469897: Pseudo dice [0.7358]
2024-12-17 20:20:17.470675: Epoch time: 272.32 s
2024-12-17 20:20:19.457396: 
2024-12-17 20:20:19.458707: Epoch 93
2024-12-17 20:20:19.459511: Current learning rate: 0.00419
2024-12-17 20:24:33.931099: Validation loss did not improve from -0.54851. Patience: 62/50
2024-12-17 20:24:33.933136: train_loss -0.7884
2024-12-17 20:24:33.934163: val_loss -0.4901
2024-12-17 20:24:33.935095: Pseudo dice [0.7258]
2024-12-17 20:24:33.935989: Epoch time: 254.48 s
2024-12-17 20:24:35.314296: 
2024-12-17 20:24:35.315417: Epoch 94
2024-12-17 20:24:35.316205: Current learning rate: 0.00412
2024-12-17 20:28:38.574492: Validation loss did not improve from -0.54851. Patience: 63/50
2024-12-17 20:28:38.575538: train_loss -0.7892
2024-12-17 20:28:38.576275: val_loss -0.4946
2024-12-17 20:28:38.576962: Pseudo dice [0.7342]
2024-12-17 20:28:38.577791: Epoch time: 243.26 s
2024-12-17 20:28:40.398208: 
2024-12-17 20:28:40.399469: Epoch 95
2024-12-17 20:28:40.400432: Current learning rate: 0.00405
2024-12-17 20:32:51.668235: Validation loss did not improve from -0.54851. Patience: 64/50
2024-12-17 20:32:51.669975: train_loss -0.7896
2024-12-17 20:32:51.671064: val_loss -0.5379
2024-12-17 20:32:51.671896: Pseudo dice [0.7554]
2024-12-17 20:32:51.672707: Epoch time: 251.27 s
2024-12-17 20:32:53.061360: 
2024-12-17 20:32:53.062729: Epoch 96
2024-12-17 20:32:53.063522: Current learning rate: 0.00399
2024-12-17 20:37:05.556540: Validation loss did not improve from -0.54851. Patience: 65/50
2024-12-17 20:37:05.557548: train_loss -0.7899
2024-12-17 20:37:05.558431: val_loss -0.4713
2024-12-17 20:37:05.559350: Pseudo dice [0.7232]
2024-12-17 20:37:05.560303: Epoch time: 252.5 s
2024-12-17 20:37:07.003983: 
2024-12-17 20:37:07.005204: Epoch 97
2024-12-17 20:37:07.005906: Current learning rate: 0.00392
2024-12-17 20:41:44.693454: Validation loss did not improve from -0.54851. Patience: 66/50
2024-12-17 20:41:44.694408: train_loss -0.7925
2024-12-17 20:41:44.695210: val_loss -0.4467
2024-12-17 20:41:44.695920: Pseudo dice [0.7088]
2024-12-17 20:41:44.696682: Epoch time: 277.69 s
2024-12-17 20:41:46.090227: 
2024-12-17 20:41:46.091555: Epoch 98
2024-12-17 20:41:46.092485: Current learning rate: 0.00385
2024-12-17 20:45:43.718314: Validation loss did not improve from -0.54851. Patience: 67/50
2024-12-17 20:45:43.719757: train_loss -0.7936
2024-12-17 20:45:43.720664: val_loss -0.5282
2024-12-17 20:45:43.721403: Pseudo dice [0.7516]
2024-12-17 20:45:43.722057: Epoch time: 237.63 s
2024-12-17 20:45:45.164297: 
2024-12-17 20:45:45.165267: Epoch 99
2024-12-17 20:45:45.165937: Current learning rate: 0.00379
2024-12-17 20:50:09.232732: Validation loss did not improve from -0.54851. Patience: 68/50
2024-12-17 20:50:09.234480: train_loss -0.7922
2024-12-17 20:50:09.235566: val_loss -0.4992
2024-12-17 20:50:09.236644: Pseudo dice [0.7283]
2024-12-17 20:50:09.237442: Epoch time: 264.07 s
2024-12-17 20:50:11.065054: 
2024-12-17 20:50:11.066335: Epoch 100
2024-12-17 20:50:11.067316: Current learning rate: 0.00372
2024-12-17 20:54:45.723427: Validation loss did not improve from -0.54851. Patience: 69/50
2024-12-17 20:54:45.724491: train_loss -0.793
2024-12-17 20:54:45.725332: val_loss -0.4786
2024-12-17 20:54:45.726146: Pseudo dice [0.7219]
2024-12-17 20:54:45.726840: Epoch time: 274.66 s
2024-12-17 20:54:47.206621: 
2024-12-17 20:54:47.208024: Epoch 101
2024-12-17 20:54:47.208931: Current learning rate: 0.00365
2024-12-17 20:59:10.319553: Validation loss did not improve from -0.54851. Patience: 70/50
2024-12-17 20:59:10.320429: train_loss -0.7976
2024-12-17 20:59:10.321158: val_loss -0.4672
2024-12-17 20:59:10.321877: Pseudo dice [0.712]
2024-12-17 20:59:10.322516: Epoch time: 263.12 s
2024-12-17 20:59:11.830406: 
2024-12-17 20:59:11.831662: Epoch 102
2024-12-17 20:59:11.832609: Current learning rate: 0.00359
2024-12-17 21:03:45.754031: Validation loss did not improve from -0.54851. Patience: 71/50
2024-12-17 21:03:45.755243: train_loss -0.7947
2024-12-17 21:03:45.756093: val_loss -0.5148
2024-12-17 21:03:45.756880: Pseudo dice [0.7409]
2024-12-17 21:03:45.757881: Epoch time: 273.93 s
2024-12-17 21:03:47.206590: 
2024-12-17 21:03:47.207742: Epoch 103
2024-12-17 21:03:47.208624: Current learning rate: 0.00352
2024-12-17 21:06:25.040980: Validation loss did not improve from -0.54851. Patience: 72/50
2024-12-17 21:06:25.042035: train_loss -0.798
2024-12-17 21:06:25.042725: val_loss -0.5261
2024-12-17 21:06:25.043345: Pseudo dice [0.7431]
2024-12-17 21:06:25.043970: Epoch time: 157.84 s
2024-12-17 21:06:26.981893: 
2024-12-17 21:06:26.983123: Epoch 104
2024-12-17 21:06:26.983906: Current learning rate: 0.00345
2024-12-17 21:09:08.610127: Validation loss did not improve from -0.54851. Patience: 73/50
2024-12-17 21:09:08.610889: train_loss -0.7977
2024-12-17 21:09:08.611678: val_loss -0.5032
2024-12-17 21:09:08.612370: Pseudo dice [0.735]
2024-12-17 21:09:08.613135: Epoch time: 161.63 s
2024-12-17 21:09:10.527800: 
2024-12-17 21:09:10.529096: Epoch 105
2024-12-17 21:09:10.529889: Current learning rate: 0.00338
2024-12-17 21:11:40.852406: Validation loss did not improve from -0.54851. Patience: 74/50
2024-12-17 21:11:40.853398: train_loss -0.7951
2024-12-17 21:11:40.854340: val_loss -0.4936
2024-12-17 21:11:40.854962: Pseudo dice [0.7324]
2024-12-17 21:11:40.855615: Epoch time: 150.33 s
2024-12-17 21:11:42.350184: 
2024-12-17 21:11:42.351444: Epoch 106
2024-12-17 21:11:42.352371: Current learning rate: 0.00332
2024-12-17 21:14:18.047236: Validation loss did not improve from -0.54851. Patience: 75/50
2024-12-17 21:14:18.048362: train_loss -0.7963
2024-12-17 21:14:18.049370: val_loss -0.4961
2024-12-17 21:14:18.050215: Pseudo dice [0.7359]
2024-12-17 21:14:18.051083: Epoch time: 155.7 s
2024-12-17 21:14:19.483537: 
2024-12-17 21:14:19.484716: Epoch 107
2024-12-17 21:14:19.485465: Current learning rate: 0.00325
2024-12-17 21:17:12.633096: Validation loss did not improve from -0.54851. Patience: 76/50
2024-12-17 21:17:12.634021: train_loss -0.8008
2024-12-17 21:17:12.634844: val_loss -0.48
2024-12-17 21:17:12.635506: Pseudo dice [0.73]
2024-12-17 21:17:12.636142: Epoch time: 173.15 s
2024-12-17 21:17:14.025826: 
2024-12-17 21:17:14.027117: Epoch 108
2024-12-17 21:17:14.027825: Current learning rate: 0.00318
2024-12-17 21:19:28.553011: Validation loss did not improve from -0.54851. Patience: 77/50
2024-12-17 21:19:28.553984: train_loss -0.797
2024-12-17 21:19:28.554964: val_loss -0.499
2024-12-17 21:19:28.555883: Pseudo dice [0.7354]
2024-12-17 21:19:28.556670: Epoch time: 134.53 s
2024-12-17 21:19:30.117463: 
2024-12-17 21:19:30.118881: Epoch 109
2024-12-17 21:19:30.119681: Current learning rate: 0.00311
2024-12-17 21:22:37.912187: Validation loss did not improve from -0.54851. Patience: 78/50
2024-12-17 21:22:37.913168: train_loss -0.7982
2024-12-17 21:22:37.913996: val_loss -0.522
2024-12-17 21:22:37.914623: Pseudo dice [0.7505]
2024-12-17 21:22:37.915311: Epoch time: 187.8 s
2024-12-17 21:22:39.838337: 
2024-12-17 21:22:39.839540: Epoch 110
2024-12-17 21:22:39.840240: Current learning rate: 0.00304
2024-12-17 21:27:05.190573: Validation loss did not improve from -0.54851. Patience: 79/50
2024-12-17 21:27:05.193210: train_loss -0.7973
2024-12-17 21:27:05.194893: val_loss -0.4955
2024-12-17 21:27:05.195539: Pseudo dice [0.729]
2024-12-17 21:27:05.196600: Epoch time: 265.36 s
2024-12-17 21:27:06.704667: 
2024-12-17 21:27:06.705772: Epoch 111
2024-12-17 21:27:06.706618: Current learning rate: 0.00297
2024-12-17 21:32:04.805406: Validation loss did not improve from -0.54851. Patience: 80/50
2024-12-17 21:32:04.806462: train_loss -0.7999
2024-12-17 21:32:04.807160: val_loss -0.4911
2024-12-17 21:32:04.807818: Pseudo dice [0.7323]
2024-12-17 21:32:04.808473: Epoch time: 298.1 s
2024-12-17 21:32:06.323833: 
2024-12-17 21:32:06.324801: Epoch 112
2024-12-17 21:32:06.325478: Current learning rate: 0.00291
2024-12-17 21:37:57.345490: Validation loss did not improve from -0.54851. Patience: 81/50
2024-12-17 21:37:57.347755: train_loss -0.8019
2024-12-17 21:37:57.349057: val_loss -0.4973
2024-12-17 21:37:57.349763: Pseudo dice [0.7306]
2024-12-17 21:37:57.350489: Epoch time: 351.02 s
2024-12-17 21:37:58.899317: 
2024-12-17 21:37:58.900655: Epoch 113
2024-12-17 21:37:58.901429: Current learning rate: 0.00284
2024-12-17 21:44:26.712716: Validation loss did not improve from -0.54851. Patience: 82/50
2024-12-17 21:44:26.713645: train_loss -0.8034
2024-12-17 21:44:26.714410: val_loss -0.4984
2024-12-17 21:44:26.715080: Pseudo dice [0.7339]
2024-12-17 21:44:26.715746: Epoch time: 387.82 s
2024-12-17 21:44:28.182300: 
2024-12-17 21:44:28.183298: Epoch 114
2024-12-17 21:44:28.184022: Current learning rate: 0.00277
2024-12-17 21:51:12.137382: Validation loss did not improve from -0.54851. Patience: 83/50
2024-12-17 21:51:12.138332: train_loss -0.8042
2024-12-17 21:51:12.139305: val_loss -0.4955
2024-12-17 21:51:12.140206: Pseudo dice [0.728]
2024-12-17 21:51:12.140986: Epoch time: 403.96 s
2024-12-17 21:51:13.929631: 
2024-12-17 21:51:13.931066: Epoch 115
2024-12-17 21:51:13.931863: Current learning rate: 0.0027
2024-12-17 21:56:56.790777: Validation loss did not improve from -0.54851. Patience: 84/50
2024-12-17 21:56:56.791859: train_loss -0.8056
2024-12-17 21:56:56.792812: val_loss -0.4646
2024-12-17 21:56:56.793633: Pseudo dice [0.7201]
2024-12-17 21:56:56.794416: Epoch time: 342.86 s
2024-12-17 21:56:58.885301: 
2024-12-17 21:56:58.886523: Epoch 116
2024-12-17 21:56:58.887181: Current learning rate: 0.00263
2024-12-17 22:03:38.593307: Validation loss did not improve from -0.54851. Patience: 85/50
2024-12-17 22:03:38.594097: train_loss -0.8032
2024-12-17 22:03:38.594994: val_loss -0.4959
2024-12-17 22:03:38.595774: Pseudo dice [0.726]
2024-12-17 22:03:38.596375: Epoch time: 399.71 s
2024-12-17 22:03:40.145617: 
2024-12-17 22:03:40.146951: Epoch 117
2024-12-17 22:03:40.147671: Current learning rate: 0.00256
2024-12-17 22:10:33.672219: Validation loss did not improve from -0.54851. Patience: 86/50
2024-12-17 22:10:33.673287: train_loss -0.7988
2024-12-17 22:10:33.673976: val_loss -0.5005
2024-12-17 22:10:33.674744: Pseudo dice [0.7373]
2024-12-17 22:10:33.675361: Epoch time: 413.53 s
2024-12-17 22:10:35.165345: 
2024-12-17 22:10:35.166497: Epoch 118
2024-12-17 22:10:35.167201: Current learning rate: 0.00249
2024-12-17 22:18:03.362796: Validation loss did not improve from -0.54851. Patience: 87/50
2024-12-17 22:18:03.363695: train_loss -0.8012
2024-12-17 22:18:03.364542: val_loss -0.4785
2024-12-17 22:18:03.365300: Pseudo dice [0.7264]
2024-12-17 22:18:03.365963: Epoch time: 448.2 s
2024-12-17 22:18:04.806925: 
2024-12-17 22:18:04.808353: Epoch 119
2024-12-17 22:18:04.809452: Current learning rate: 0.00242
2024-12-17 22:25:52.281051: Validation loss did not improve from -0.54851. Patience: 88/50
2024-12-17 22:25:52.281870: train_loss -0.8024
2024-12-17 22:25:52.282665: val_loss -0.49
2024-12-17 22:25:52.283336: Pseudo dice [0.7265]
2024-12-17 22:25:52.284004: Epoch time: 467.48 s
2024-12-17 22:25:54.196626: 
2024-12-17 22:25:54.197803: Epoch 120
2024-12-17 22:25:54.198530: Current learning rate: 0.00235
2024-12-17 22:33:29.296051: Validation loss did not improve from -0.54851. Patience: 89/50
2024-12-17 22:33:29.298561: train_loss -0.8031
2024-12-17 22:33:29.300375: val_loss -0.4987
2024-12-17 22:33:29.301390: Pseudo dice [0.7377]
2024-12-17 22:33:29.302702: Epoch time: 455.1 s
2024-12-17 22:33:30.798924: 
2024-12-17 22:33:30.800326: Epoch 121
2024-12-17 22:33:30.801510: Current learning rate: 0.00228
2024-12-17 22:40:50.585130: Validation loss did not improve from -0.54851. Patience: 90/50
2024-12-17 22:40:50.587223: train_loss -0.8041
2024-12-17 22:40:50.588181: val_loss -0.4992
2024-12-17 22:40:50.588996: Pseudo dice [0.7342]
2024-12-17 22:40:50.589718: Epoch time: 439.79 s
2024-12-17 22:40:52.063414: 
2024-12-17 22:40:52.064378: Epoch 122
2024-12-17 22:40:52.065057: Current learning rate: 0.00221
2024-12-17 22:48:36.769480: Validation loss did not improve from -0.54851. Patience: 91/50
2024-12-17 22:48:36.770404: train_loss -0.807
2024-12-17 22:48:36.771156: val_loss -0.5371
2024-12-17 22:48:36.771818: Pseudo dice [0.7484]
2024-12-17 22:48:36.772557: Epoch time: 464.71 s
2024-12-17 22:48:38.186571: 
2024-12-17 22:48:38.187808: Epoch 123
2024-12-17 22:48:38.188650: Current learning rate: 0.00214
2024-12-17 22:56:22.456958: Validation loss did not improve from -0.54851. Patience: 92/50
2024-12-17 22:56:22.458312: train_loss -0.8035
2024-12-17 22:56:22.459131: val_loss -0.5069
2024-12-17 22:56:22.459934: Pseudo dice [0.7316]
2024-12-17 22:56:22.460709: Epoch time: 464.27 s
2024-12-17 22:56:23.925019: 
2024-12-17 22:56:23.926442: Epoch 124
2024-12-17 22:56:23.927585: Current learning rate: 0.00207
2024-12-17 23:03:23.796751: Validation loss did not improve from -0.54851. Patience: 93/50
2024-12-17 23:03:23.797715: train_loss -0.8064
2024-12-17 23:03:23.798902: val_loss -0.5213
2024-12-17 23:03:23.799940: Pseudo dice [0.7466]
2024-12-17 23:03:23.800852: Epoch time: 419.87 s
2024-12-17 23:03:25.604698: 
2024-12-17 23:03:25.605973: Epoch 125
2024-12-17 23:03:25.606681: Current learning rate: 0.00199
2024-12-17 23:10:42.000671: Validation loss did not improve from -0.54851. Patience: 94/50
2024-12-17 23:10:42.001634: train_loss -0.8109
2024-12-17 23:10:42.002334: val_loss -0.5018
2024-12-17 23:10:42.003007: Pseudo dice [0.7346]
2024-12-17 23:10:42.003731: Epoch time: 436.4 s
2024-12-17 23:10:43.891014: 
2024-12-17 23:10:43.892216: Epoch 126
2024-12-17 23:10:43.893016: Current learning rate: 0.00192
2024-12-17 23:18:27.541587: Validation loss did not improve from -0.54851. Patience: 95/50
2024-12-17 23:18:27.542732: train_loss -0.8042
2024-12-17 23:18:27.543788: val_loss -0.4994
2024-12-17 23:18:27.544737: Pseudo dice [0.7272]
2024-12-17 23:18:27.545528: Epoch time: 463.65 s
2024-12-17 23:18:29.080995: 
2024-12-17 23:18:29.082307: Epoch 127
2024-12-17 23:18:29.083082: Current learning rate: 0.00185
2024-12-17 23:26:30.461670: Validation loss did not improve from -0.54851. Patience: 96/50
2024-12-17 23:26:30.462670: train_loss -0.8096
2024-12-17 23:26:30.463552: val_loss -0.4793
2024-12-17 23:26:30.464389: Pseudo dice [0.7289]
2024-12-17 23:26:30.465121: Epoch time: 481.38 s
2024-12-17 23:26:31.972641: 
2024-12-17 23:26:31.974012: Epoch 128
2024-12-17 23:26:31.974855: Current learning rate: 0.00178
2024-12-17 23:33:46.484960: Validation loss did not improve from -0.54851. Patience: 97/50
2024-12-17 23:33:46.485863: train_loss -0.8076
2024-12-17 23:33:46.486757: val_loss -0.5179
2024-12-17 23:33:46.487456: Pseudo dice [0.7372]
2024-12-17 23:33:46.488283: Epoch time: 434.51 s
2024-12-17 23:33:48.037346: 
2024-12-17 23:33:48.038568: Epoch 129
2024-12-17 23:33:48.039450: Current learning rate: 0.0017
2024-12-17 23:41:24.256340: Validation loss did not improve from -0.54851. Patience: 98/50
2024-12-17 23:41:24.257617: train_loss -0.8102
2024-12-17 23:41:24.258379: val_loss -0.494
2024-12-17 23:41:24.259137: Pseudo dice [0.7287]
2024-12-17 23:41:24.260096: Epoch time: 456.22 s
2024-12-17 23:41:26.121144: 
2024-12-17 23:41:26.122660: Epoch 130
2024-12-17 23:41:26.123821: Current learning rate: 0.00163
2024-12-17 23:49:26.717562: Validation loss did not improve from -0.54851. Patience: 99/50
2024-12-17 23:49:26.719823: train_loss -0.8095
2024-12-17 23:49:26.721150: val_loss -0.5092
2024-12-17 23:49:26.721820: Pseudo dice [0.7383]
2024-12-17 23:49:26.722538: Epoch time: 480.6 s
2024-12-17 23:49:28.187331: 
2024-12-17 23:49:28.188493: Epoch 131
2024-12-17 23:49:28.189303: Current learning rate: 0.00156
2024-12-17 23:56:46.598170: Validation loss did not improve from -0.54851. Patience: 100/50
2024-12-17 23:56:46.599210: train_loss -0.81
2024-12-17 23:56:46.600025: val_loss -0.4978
2024-12-17 23:56:46.600709: Pseudo dice [0.7288]
2024-12-17 23:56:46.601403: Epoch time: 438.41 s
2024-12-17 23:56:48.056795: 
2024-12-17 23:56:48.057810: Epoch 132
2024-12-17 23:56:48.058528: Current learning rate: 0.00148
2024-12-18 00:04:21.379171: Validation loss did not improve from -0.54851. Patience: 101/50
2024-12-18 00:04:21.380295: train_loss -0.8072
2024-12-18 00:04:21.381061: val_loss -0.5157
2024-12-18 00:04:21.381877: Pseudo dice [0.7461]
2024-12-18 00:04:21.382530: Epoch time: 453.32 s
2024-12-18 00:04:22.905748: 
2024-12-18 00:04:22.907066: Epoch 133
2024-12-18 00:04:22.907933: Current learning rate: 0.00141
2024-12-18 00:11:37.901155: Validation loss did not improve from -0.54851. Patience: 102/50
2024-12-18 00:11:37.902075: train_loss -0.8109
2024-12-18 00:11:37.902873: val_loss -0.5144
2024-12-18 00:11:37.903652: Pseudo dice [0.748]
2024-12-18 00:11:37.904383: Epoch time: 435.0 s
2024-12-18 00:11:37.904971: Yayy! New best EMA pseudo Dice: 0.7359
2024-12-18 00:11:39.916718: 
2024-12-18 00:11:39.918086: Epoch 134
2024-12-18 00:11:39.919098: Current learning rate: 0.00133
2024-12-18 00:18:44.161124: Validation loss did not improve from -0.54851. Patience: 103/50
2024-12-18 00:18:44.162390: train_loss -0.8105
2024-12-18 00:18:44.163424: val_loss -0.4989
2024-12-18 00:18:44.164424: Pseudo dice [0.7422]
2024-12-18 00:18:44.165326: Epoch time: 424.25 s
2024-12-18 00:18:44.641922: Yayy! New best EMA pseudo Dice: 0.7365
2024-12-18 00:18:46.595379: 
2024-12-18 00:18:46.596642: Epoch 135
2024-12-18 00:18:46.597551: Current learning rate: 0.00126
2024-12-18 00:26:18.947073: Validation loss did not improve from -0.54851. Patience: 104/50
2024-12-18 00:26:18.948186: train_loss -0.8095
2024-12-18 00:26:18.948929: val_loss -0.5104
2024-12-18 00:26:18.949749: Pseudo dice [0.7371]
2024-12-18 00:26:18.950471: Epoch time: 452.35 s
2024-12-18 00:26:18.951251: Yayy! New best EMA pseudo Dice: 0.7366
2024-12-18 00:26:21.046455: 
2024-12-18 00:26:21.047748: Epoch 136
2024-12-18 00:26:21.048505: Current learning rate: 0.00118
2024-12-18 00:33:49.933534: Validation loss did not improve from -0.54851. Patience: 105/50
2024-12-18 00:33:49.934581: train_loss -0.8084
2024-12-18 00:33:49.935438: val_loss -0.4939
2024-12-18 00:33:49.936203: Pseudo dice [0.7314]
2024-12-18 00:33:49.936966: Epoch time: 448.89 s
2024-12-18 00:33:51.494652: 
2024-12-18 00:33:51.495793: Epoch 137
2024-12-18 00:33:51.496483: Current learning rate: 0.00111
2024-12-18 00:41:13.533810: Validation loss did not improve from -0.54851. Patience: 106/50
2024-12-18 00:41:13.536348: train_loss -0.8099
2024-12-18 00:41:13.537330: val_loss -0.5019
2024-12-18 00:41:13.538071: Pseudo dice [0.733]
2024-12-18 00:41:13.539081: Epoch time: 442.04 s
2024-12-18 00:41:16.600294: 
2024-12-18 00:41:16.601480: Epoch 138
2024-12-18 00:41:16.602212: Current learning rate: 0.00103
2024-12-18 00:48:56.979544: Validation loss did not improve from -0.54851. Patience: 107/50
2024-12-18 00:48:56.980725: train_loss -0.8106
2024-12-18 00:48:56.981453: val_loss -0.5185
2024-12-18 00:48:56.982059: Pseudo dice [0.7474]
2024-12-18 00:48:56.982885: Epoch time: 460.38 s
2024-12-18 00:48:56.983640: Yayy! New best EMA pseudo Dice: 0.7369
2024-12-18 00:48:58.827943: 
2024-12-18 00:48:58.829438: Epoch 139
2024-12-18 00:48:58.830452: Current learning rate: 0.00095
2024-12-18 00:56:49.432729: Validation loss did not improve from -0.54851. Patience: 108/50
2024-12-18 00:56:49.434815: train_loss -0.8112
2024-12-18 00:56:49.436116: val_loss -0.5199
2024-12-18 00:56:49.437127: Pseudo dice [0.7514]
2024-12-18 00:56:49.438169: Epoch time: 470.61 s
2024-12-18 00:56:49.867382: Yayy! New best EMA pseudo Dice: 0.7384
2024-12-18 00:56:51.941638: 
2024-12-18 00:56:51.943104: Epoch 140
2024-12-18 00:56:51.944217: Current learning rate: 0.00087
2024-12-18 01:04:14.005845: Validation loss did not improve from -0.54851. Patience: 109/50
2024-12-18 01:04:14.006627: train_loss -0.8136
2024-12-18 01:04:14.007635: val_loss -0.5189
2024-12-18 01:04:14.008548: Pseudo dice [0.7459]
2024-12-18 01:04:14.009568: Epoch time: 442.07 s
2024-12-18 01:04:14.010461: Yayy! New best EMA pseudo Dice: 0.7391
2024-12-18 01:04:16.035332: 
2024-12-18 01:04:16.036471: Epoch 141
2024-12-18 01:04:16.037333: Current learning rate: 0.00079
2024-12-18 01:11:39.539429: Validation loss did not improve from -0.54851. Patience: 110/50
2024-12-18 01:11:39.540329: train_loss -0.8136
2024-12-18 01:11:39.541209: val_loss -0.5094
2024-12-18 01:11:39.541882: Pseudo dice [0.7366]
2024-12-18 01:11:39.542564: Epoch time: 443.51 s
2024-12-18 01:11:41.120875: 
2024-12-18 01:11:41.122196: Epoch 142
2024-12-18 01:11:41.123001: Current learning rate: 0.00071
2024-12-18 01:19:00.777842: Validation loss did not improve from -0.54851. Patience: 111/50
2024-12-18 01:19:00.778777: train_loss -0.8138
2024-12-18 01:19:00.779771: val_loss -0.5018
2024-12-18 01:19:00.780601: Pseudo dice [0.7351]
2024-12-18 01:19:00.781427: Epoch time: 439.66 s
2024-12-18 01:19:02.259701: 
2024-12-18 01:19:02.260837: Epoch 143
2024-12-18 01:19:02.261562: Current learning rate: 0.00063
2024-12-18 01:26:59.107267: Validation loss did not improve from -0.54851. Patience: 112/50
2024-12-18 01:26:59.108410: train_loss -0.8117
2024-12-18 01:26:59.109180: val_loss -0.4945
2024-12-18 01:26:59.109849: Pseudo dice [0.737]
2024-12-18 01:26:59.110538: Epoch time: 476.85 s
2024-12-18 01:27:00.569385: 
2024-12-18 01:27:00.570466: Epoch 144
2024-12-18 01:27:00.571170: Current learning rate: 0.00055
2024-12-18 01:34:52.393176: Validation loss did not improve from -0.54851. Patience: 113/50
2024-12-18 01:34:52.394062: train_loss -0.813
2024-12-18 01:34:52.394871: val_loss -0.5064
2024-12-18 01:34:52.395875: Pseudo dice [0.7381]
2024-12-18 01:34:52.396876: Epoch time: 471.83 s
2024-12-18 01:34:54.352794: 
2024-12-18 01:34:54.354058: Epoch 145
2024-12-18 01:34:54.354858: Current learning rate: 0.00047
2024-12-18 01:42:51.227394: Validation loss did not improve from -0.54851. Patience: 114/50
2024-12-18 01:42:51.228271: train_loss -0.8151
2024-12-18 01:42:51.229035: val_loss -0.4887
2024-12-18 01:42:51.229721: Pseudo dice [0.7294]
2024-12-18 01:42:51.230553: Epoch time: 476.88 s
2024-12-18 01:42:52.775301: 
2024-12-18 01:42:52.776722: Epoch 146
2024-12-18 01:42:52.777841: Current learning rate: 0.00038
2024-12-18 01:50:07.109177: Validation loss did not improve from -0.54851. Patience: 115/50
2024-12-18 01:50:07.110187: train_loss -0.8139
2024-12-18 01:50:07.110908: val_loss -0.4986
2024-12-18 01:50:07.111541: Pseudo dice [0.7391]
2024-12-18 01:50:07.112237: Epoch time: 434.34 s
2024-12-18 01:50:08.571042: 
2024-12-18 01:50:08.571998: Epoch 147
2024-12-18 01:50:08.572854: Current learning rate: 0.0003
2024-12-18 01:57:36.233987: Validation loss did not improve from -0.54851. Patience: 116/50
2024-12-18 01:57:36.235037: train_loss -0.8127
2024-12-18 01:57:36.235781: val_loss -0.5157
2024-12-18 01:57:36.236387: Pseudo dice [0.7464]
2024-12-18 01:57:36.237179: Epoch time: 447.67 s
2024-12-18 01:57:37.672517: 
2024-12-18 01:57:37.674381: Epoch 148
2024-12-18 01:57:37.675251: Current learning rate: 0.00021
2024-12-18 02:04:45.658857: Validation loss did not improve from -0.54851. Patience: 117/50
2024-12-18 02:04:45.659735: train_loss -0.815
2024-12-18 02:04:45.660511: val_loss -0.495
2024-12-18 02:04:45.661112: Pseudo dice [0.7275]
2024-12-18 02:04:45.661862: Epoch time: 427.99 s
2024-12-18 02:04:47.541306: 
2024-12-18 02:04:47.542649: Epoch 149
2024-12-18 02:04:47.543488: Current learning rate: 0.00011
2024-12-18 02:12:09.991581: Validation loss did not improve from -0.54851. Patience: 118/50
2024-12-18 02:12:09.992536: train_loss -0.8145
2024-12-18 02:12:09.993511: val_loss -0.5094
2024-12-18 02:12:09.994509: Pseudo dice [0.7335]
2024-12-18 02:12:09.995437: Epoch time: 442.45 s
2024-12-18 02:12:12.004987: Training done.
2024-12-18 02:12:12.143819: Using splits from existing split file: /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset309_Sohee_Calcium_OCT_CrossValidation/splits_final_60.json
2024-12-18 02:12:12.145778: The split file contains 5 splits.
2024-12-18 02:12:12.146673: Desired fold for training: 0
2024-12-18 02:12:12.147478: This split has 4 training and 4 validation cases.
2024-12-18 02:12:12.148710: predicting 101-045
2024-12-18 02:12:12.227026: 101-045, shape torch.Size([1, 375, 498, 498]), rank 0
2024-12-18 02:14:00.753054: predicting 701-013
2024-12-18 02:14:00.770324: 701-013, shape torch.Size([1, 375, 498, 498]), rank 0
2024-12-18 02:16:19.887872: predicting 704-003
2024-12-18 02:16:19.903318: 704-003, shape torch.Size([1, 375, 498, 498]), rank 0
2024-12-18 02:18:17.515159: predicting 706-005
2024-12-18 02:18:17.529999: 706-005, shape torch.Size([1, 375, 498, 498]), rank 0
2024-12-18 02:20:30.880209: Validation complete
2024-12-18 02:20:30.881305: Mean Validation Dice:  0.7378873285379197

############################
INFO: You are using the old nnU-Net default plans. We have updated our recommendations. Please consider using those instead! Read more here: https://github.com/MIC-DKFZ/nnUNet/blob/master/documentation/resenc_presets.md
############################

Using device: cuda:0

#######################################################################
Please cite the following paper when using nnU-Net:
Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.
#######################################################################

2024-12-18 02:20:38.676240: Using torch.compile...

############################
INFO: You are using the old nnU-Net default plans. We have updated our recommendations. Please consider using those instead! Read more here: https://github.com/MIC-DKFZ/nnUNet/blob/master/documentation/resenc_presets.md
############################

Using device: cuda:0

#######################################################################
Please cite the following paper when using nnU-Net:
Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.
#######################################################################

2024-12-18 02:20:38.676997: Using torch.compile...
/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
################### Loading pretrained weights from file  /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_results/Dataset300_Lumen_and_Wall_OCT/nnUNetTrainer__nnUNetPreprocessPlans__3d_fullres/fold_all/checkpoint_best.pth ###################
Below is the list of overlapping blocks in pretrained model and nnUNet architecture:
encoder.stages.0.0.convs.0.conv.weight shape torch.Size([32, 1, 3, 3, 3])
encoder.stages.0.0.convs.0.conv.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.norm.weight shape torch.Size([32])
encoder.stages.0.0.convs.0.norm.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.0.weight shape torch.Size([32, 1, 3, 3, 3])
encoder.stages.0.0.convs.0.all_modules.0.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.1.weight shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.1.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
encoder.stages.0.0.convs.1.conv.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.norm.weight shape torch.Size([32])
encoder.stages.0.0.convs.1.norm.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
encoder.stages.0.0.convs.1.all_modules.0.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.1.weight shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.1.bias shape torch.Size([32])
encoder.stages.1.0.convs.0.conv.weight shape torch.Size([64, 32, 3, 3, 3])
encoder.stages.1.0.convs.0.conv.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.norm.weight shape torch.Size([64])
encoder.stages.1.0.convs.0.norm.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.0.weight shape torch.Size([64, 32, 3, 3, 3])
encoder.stages.1.0.convs.0.all_modules.0.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.1.weight shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.1.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
encoder.stages.1.0.convs.1.conv.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.norm.weight shape torch.Size([64])
encoder.stages.1.0.convs.1.norm.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
encoder.stages.1.0.convs.1.all_modules.0.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.1.weight shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.1.bias shape torch.Size([64])
encoder.stages.2.0.convs.0.conv.weight shape torch.Size([128, 64, 3, 3, 3])
encoder.stages.2.0.convs.0.conv.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.norm.weight shape torch.Size([128])
encoder.stages.2.0.convs.0.norm.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.0.weight shape torch.Size([128, 64, 3, 3, 3])
encoder.stages.2.0.convs.0.all_modules.0.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.1.weight shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.1.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
encoder.stages.2.0.convs.1.conv.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.norm.weight shape torch.Size([128])
encoder.stages.2.0.convs.1.norm.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
encoder.stages.2.0.convs.1.all_modules.0.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.1.weight shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.1.bias shape torch.Size([128])
encoder.stages.3.0.convs.0.conv.weight shape torch.Size([256, 128, 3, 3, 3])
encoder.stages.3.0.convs.0.conv.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.norm.weight shape torch.Size([256])
encoder.stages.3.0.convs.0.norm.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.0.weight shape torch.Size([256, 128, 3, 3, 3])
encoder.stages.3.0.convs.0.all_modules.0.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.1.weight shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.1.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
encoder.stages.3.0.convs.1.conv.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.norm.weight shape torch.Size([256])
encoder.stages.3.0.convs.1.norm.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
encoder.stages.3.0.convs.1.all_modules.0.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.1.weight shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.1.bias shape torch.Size([256])
encoder.stages.4.0.convs.0.conv.weight shape torch.Size([320, 256, 3, 3, 3])
encoder.stages.4.0.convs.0.conv.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.norm.weight shape torch.Size([320])
encoder.stages.4.0.convs.0.norm.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.0.weight shape torch.Size([320, 256, 3, 3, 3])
encoder.stages.4.0.convs.0.all_modules.0.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.1.weight shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.1.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.4.0.convs.1.conv.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.norm.weight shape torch.Size([320])
encoder.stages.4.0.convs.1.norm.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.4.0.convs.1.all_modules.0.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.1.weight shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.1.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.0.conv.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.norm.weight shape torch.Size([320])
encoder.stages.5.0.convs.0.norm.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.0.all_modules.0.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.1.weight shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.1.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.1.conv.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.norm.weight shape torch.Size([320])
encoder.stages.5.0.convs.1.norm.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.1.all_modules.0.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.1.weight shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.0.0.convs.0.conv.weight shape torch.Size([32, 1, 3, 3, 3])
decoder.encoder.stages.0.0.convs.0.conv.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.norm.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.norm.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.0.weight shape torch.Size([32, 1, 3, 3, 3])
decoder.encoder.stages.0.0.convs.0.all_modules.0.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.1.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.1.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.encoder.stages.0.0.convs.1.conv.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.norm.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.norm.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.encoder.stages.0.0.convs.1.all_modules.0.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.1.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.1.bias shape torch.Size([32])
decoder.encoder.stages.1.0.convs.0.conv.weight shape torch.Size([64, 32, 3, 3, 3])
decoder.encoder.stages.1.0.convs.0.conv.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.norm.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.norm.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.0.weight shape torch.Size([64, 32, 3, 3, 3])
decoder.encoder.stages.1.0.convs.0.all_modules.0.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.1.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.1.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.encoder.stages.1.0.convs.1.conv.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.norm.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.norm.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.encoder.stages.1.0.convs.1.all_modules.0.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.1.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.1.bias shape torch.Size([64])
decoder.encoder.stages.2.0.convs.0.conv.weight shape torch.Size([128, 64, 3, 3, 3])
decoder.encoder.stages.2.0.convs.0.conv.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.norm.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.norm.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.0.weight shape torch.Size([128, 64, 3, 3, 3])
decoder.encoder.stages.2.0.convs.0.all_modules.0.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.1.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.1.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.encoder.stages.2.0.convs.1.conv.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.norm.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.norm.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.encoder.stages.2.0.convs.1.all_modules.0.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.1.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.1.bias shape torch.Size([128])
decoder.encoder.stages.3.0.convs.0.conv.weight shape torch.Size([256, 128, 3, 3, 3])
decoder.encoder.stages.3.0.convs.0.conv.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.norm.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.norm.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.0.weight shape torch.Size([256, 128, 3, 3, 3])
decoder.encoder.stages.3.0.convs.0.all_modules.0.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.1.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.1.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.encoder.stages.3.0.convs.1.conv.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.norm.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.norm.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.encoder.stages.3.0.convs.1.all_modules.0.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.1.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.1.bias shape torch.Size([256])
decoder.encoder.stages.4.0.convs.0.conv.weight shape torch.Size([320, 256, 3, 3, 3])
decoder.encoder.stages.4.0.convs.0.conv.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.norm.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.norm.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.0.weight shape torch.Size([320, 256, 3, 3, 3])
decoder.encoder.stages.4.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.4.0.convs.1.conv.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.norm.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.norm.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.4.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.0.conv.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.norm.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.norm.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.1.conv.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.norm.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.norm.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.stages.0.convs.0.conv.weight shape torch.Size([320, 640, 3, 3, 3])
decoder.stages.0.convs.0.conv.bias shape torch.Size([320])
decoder.stages.0.convs.0.norm.weight shape torch.Size([320])
decoder.stages.0.convs.0.norm.bias shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.0.weight shape torch.Size([320, 640, 3, 3, 3])
decoder.stages.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.stages.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.stages.0.convs.1.conv.bias shape torch.Size([320])
decoder.stages.0.convs.1.norm.weight shape torch.Size([320])
decoder.stages.0.convs.1.norm.bias shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.stages.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.stages.1.convs.0.conv.weight shape torch.Size([256, 512, 3, 3, 3])
decoder.stages.1.convs.0.conv.bias shape torch.Size([256])
decoder.stages.1.convs.0.norm.weight shape torch.Size([256])
decoder.stages.1.convs.0.norm.bias shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.0.weight shape torch.Size([256, 512, 3, 3, 3])
decoder.stages.1.convs.0.all_modules.0.bias shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.1.weight shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.1.bias shape torch.Size([256])
decoder.stages.1.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.stages.1.convs.1.conv.bias shape torch.Size([256])
decoder.stages.1.convs.1.norm.weight shape torch.Size([256])
decoder.stages.1.convs.1.norm.bias shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.stages.1.convs.1.all_modules.0.bias shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.1.weight shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.1.bias shape torch.Size([256])
decoder.stages.2.convs.0.conv.weight shape torch.Size([128, 256, 3, 3, 3])
decoder.stages.2.convs.0.conv.bias shape torch.Size([128])
decoder.stages.2.convs.0.norm.weight shape torch.Size([128])
decoder.stages.2.convs.0.norm.bias shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.0.weight shape torch.Size([128, 256, 3, 3, 3])
decoder.stages.2.convs.0.all_modules.0.bias shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.1.weight shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.1.bias shape torch.Size([128])
decoder.stages.2.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.stages.2.convs.1.conv.bias shape torch.Size([128])
decoder.stages.2.convs.1.norm.weight shape torch.Size([128])
decoder.stages.2.convs.1.norm.bias shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.stages.2.convs.1.all_modules.0.bias shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.1.weight shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.1.bias shape torch.Size([128])
decoder.stages.3.convs.0.conv.weight shape torch.Size([64, 128, 3, 3, 3])
decoder.stages.3.convs.0.conv.bias shape torch.Size([64])
decoder.stages.3.convs.0.norm.weight shape torch.Size([64])
decoder.stages.3.convs.0.norm.bias shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.0.weight shape torch.Size([64, 128, 3, 3, 3])
decoder.stages.3.convs.0.all_modules.0.bias shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.1.weight shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.1.bias shape torch.Size([64])
decoder.stages.3.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.stages.3.convs.1.conv.bias shape torch.Size([64])
decoder.stages.3.convs.1.norm.weight shape torch.Size([64])
decoder.stages.3.convs.1.norm.bias shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.stages.3.convs.1.all_modules.0.bias shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.1.weight shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.1.bias shape torch.Size([64])
decoder.stages.4.convs.0.conv.weight shape torch.Size([32, 64, 3, 3, 3])
decoder.stages.4.convs.0.conv.bias shape torch.Size([32])
decoder.stages.4.convs.0.norm.weight shape torch.Size([32])
decoder.stages.4.convs.0.norm.bias shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.0.weight shape torch.Size([32, 64, 3, 3, 3])
decoder.stages.4.convs.0.all_modules.0.bias shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.1.weight shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.1.bias shape torch.Size([32])
decoder.stages.4.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.stages.4.convs.1.conv.bias shape torch.Size([32])
decoder.stages.4.convs.1.norm.weight shape torch.Size([32])
decoder.stages.4.convs.1.norm.bias shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.stages.4.convs.1.all_modules.0.bias shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.1.weight shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.1.bias shape torch.Size([32])
decoder.transpconvs.0.weight shape torch.Size([320, 320, 1, 2, 2])
decoder.transpconvs.0.bias shape torch.Size([320])
decoder.transpconvs.1.weight shape torch.Size([320, 256, 2, 2, 2])
decoder.transpconvs.1.bias shape torch.Size([256])
decoder.transpconvs.2.weight shape torch.Size([256, 128, 2, 2, 2])
decoder.transpconvs.2.bias shape torch.Size([128])
decoder.transpconvs.3.weight shape torch.Size([128, 64, 2, 2, 2])
decoder.transpconvs.3.bias shape torch.Size([64])
decoder.transpconvs.4.weight shape torch.Size([64, 32, 2, 2, 2])
decoder.transpconvs.4.bias shape torch.Size([32])
################### Done ###################
2024-12-18 02:21:01.379103: do_dummy_2d_data_aug: True
2024-12-18 02:21:01.380464: Using splits from existing split file: /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset309_Sohee_Calcium_OCT_CrossValidation/splits_final_60.json
2024-12-18 02:21:01.382360: The split file contains 5 splits.
2024-12-18 02:21:01.383413: Desired fold for training: 2
2024-12-18 02:21:01.384353: This split has 4 training and 4 validation cases.
################### Loading pretrained weights from file  /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_results/Dataset300_Lumen_and_Wall_OCT/nnUNetTrainer__nnUNetPreprocessPlans__3d_fullres/fold_all/checkpoint_best.pth ###################
Below is the list of overlapping blocks in pretrained model and nnUNet architecture:
encoder.stages.0.0.convs.0.conv.weight shape torch.Size([32, 1, 3, 3, 3])
encoder.stages.0.0.convs.0.conv.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.norm.weight shape torch.Size([32])
encoder.stages.0.0.convs.0.norm.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.0.weight shape torch.Size([32, 1, 3, 3, 3])
encoder.stages.0.0.convs.0.all_modules.0.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.1.weight shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.1.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
encoder.stages.0.0.convs.1.conv.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.norm.weight shape torch.Size([32])
encoder.stages.0.0.convs.1.norm.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
encoder.stages.0.0.convs.1.all_modules.0.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.1.weight shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.1.bias shape torch.Size([32])
encoder.stages.1.0.convs.0.conv.weight shape torch.Size([64, 32, 3, 3, 3])
encoder.stages.1.0.convs.0.conv.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.norm.weight shape torch.Size([64])
encoder.stages.1.0.convs.0.norm.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.0.weight shape torch.Size([64, 32, 3, 3, 3])
encoder.stages.1.0.convs.0.all_modules.0.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.1.weight shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.1.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
encoder.stages.1.0.convs.1.conv.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.norm.weight shape torch.Size([64])
encoder.stages.1.0.convs.1.norm.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
encoder.stages.1.0.convs.1.all_modules.0.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.1.weight shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.1.bias shape torch.Size([64])
encoder.stages.2.0.convs.0.conv.weight shape torch.Size([128, 64, 3, 3, 3])
encoder.stages.2.0.convs.0.conv.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.norm.weight shape torch.Size([128])
encoder.stages.2.0.convs.0.norm.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.0.weight shape torch.Size([128, 64, 3, 3, 3])
encoder.stages.2.0.convs.0.all_modules.0.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.1.weight shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.1.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
encoder.stages.2.0.convs.1.conv.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.norm.weight shape torch.Size([128])
encoder.stages.2.0.convs.1.norm.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
encoder.stages.2.0.convs.1.all_modules.0.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.1.weight shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.1.bias shape torch.Size([128])
encoder.stages.3.0.convs.0.conv.weight shape torch.Size([256, 128, 3, 3, 3])
encoder.stages.3.0.convs.0.conv.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.norm.weight shape torch.Size([256])
encoder.stages.3.0.convs.0.norm.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.0.weight shape torch.Size([256, 128, 3, 3, 3])
encoder.stages.3.0.convs.0.all_modules.0.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.1.weight shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.1.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
encoder.stages.3.0.convs.1.conv.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.norm.weight shape torch.Size([256])
encoder.stages.3.0.convs.1.norm.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
encoder.stages.3.0.convs.1.all_modules.0.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.1.weight shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.1.bias shape torch.Size([256])
encoder.stages.4.0.convs.0.conv.weight shape torch.Size([320, 256, 3, 3, 3])
encoder.stages.4.0.convs.0.conv.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.norm.weight shape torch.Size([320])
encoder.stages.4.0.convs.0.norm.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.0.weight shape torch.Size([320, 256, 3, 3, 3])
encoder.stages.4.0.convs.0.all_modules.0.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.1.weight shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.1.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.4.0.convs.1.conv.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.norm.weight shape torch.Size([320])
encoder.stages.4.0.convs.1.norm.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.4.0.convs.1.all_modules.0.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.1.weight shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.1.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.0.conv.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.norm.weight shape torch.Size([320])
encoder.stages.5.0.convs.0.norm.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.0.all_modules.0.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.1.weight shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.1.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.1.conv.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.norm.weight shape torch.Size([320])
encoder.stages.5.0.convs.1.norm.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.1.all_modules.0.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.1.weight shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.0.0.convs.0.conv.weight shape torch.Size([32, 1, 3, 3, 3])
decoder.encoder.stages.0.0.convs.0.conv.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.norm.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.norm.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.0.weight shape torch.Size([32, 1, 3, 3, 3])
decoder.encoder.stages.0.0.convs.0.all_modules.0.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.1.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.1.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.encoder.stages.0.0.convs.1.conv.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.norm.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.norm.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.encoder.stages.0.0.convs.1.all_modules.0.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.1.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.1.bias shape torch.Size([32])
decoder.encoder.stages.1.0.convs.0.conv.weight shape torch.Size([64, 32, 3, 3, 3])
decoder.encoder.stages.1.0.convs.0.conv.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.norm.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.norm.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.0.weight shape torch.Size([64, 32, 3, 3, 3])
decoder.encoder.stages.1.0.convs.0.all_modules.0.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.1.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.1.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.encoder.stages.1.0.convs.1.conv.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.norm.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.norm.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.encoder.stages.1.0.convs.1.all_modules.0.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.1.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.1.bias shape torch.Size([64])
decoder.encoder.stages.2.0.convs.0.conv.weight shape torch.Size([128, 64, 3, 3, 3])
decoder.encoder.stages.2.0.convs.0.conv.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.norm.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.norm.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.0.weight shape torch.Size([128, 64, 3, 3, 3])
decoder.encoder.stages.2.0.convs.0.all_modules.0.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.1.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.1.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.encoder.stages.2.0.convs.1.conv.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.norm.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.norm.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.encoder.stages.2.0.convs.1.all_modules.0.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.1.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.1.bias shape torch.Size([128])
decoder.encoder.stages.3.0.convs.0.conv.weight shape torch.Size([256, 128, 3, 3, 3])
decoder.encoder.stages.3.0.convs.0.conv.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.norm.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.norm.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.0.weight shape torch.Size([256, 128, 3, 3, 3])
decoder.encoder.stages.3.0.convs.0.all_modules.0.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.1.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.1.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.encoder.stages.3.0.convs.1.conv.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.norm.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.norm.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.encoder.stages.3.0.convs.1.all_modules.0.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.1.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.1.bias shape torch.Size([256])
decoder.encoder.stages.4.0.convs.0.conv.weight shape torch.Size([320, 256, 3, 3, 3])
decoder.encoder.stages.4.0.convs.0.conv.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.norm.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.norm.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.0.weight shape torch.Size([320, 256, 3, 3, 3])
decoder.encoder.stages.4.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.4.0.convs.1.conv.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.norm.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.norm.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.4.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.0.conv.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.norm.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.norm.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.1.conv.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.norm.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.norm.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.stages.0.convs.0.conv.weight shape torch.Size([320, 640, 3, 3, 3])
decoder.stages.0.convs.0.conv.bias shape torch.Size([320])
decoder.stages.0.convs.0.norm.weight shape torch.Size([320])
decoder.stages.0.convs.0.norm.bias shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.0.weight shape torch.Size([320, 640, 3, 3, 3])
decoder.stages.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.stages.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.stages.0.convs.1.conv.bias shape torch.Size([320])
decoder.stages.0.convs.1.norm.weight shape torch.Size([320])
decoder.stages.0.convs.1.norm.bias shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.stages.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.stages.1.convs.0.conv.weight shape torch.Size([256, 512, 3, 3, 3])
decoder.stages.1.convs.0.conv.bias shape torch.Size([256])
decoder.stages.1.convs.0.norm.weight shape torch.Size([256])
decoder.stages.1.convs.0.norm.bias shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.0.weight shape torch.Size([256, 512, 3, 3, 3])
decoder.stages.1.convs.0.all_modules.0.bias shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.1.weight shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.1.bias shape torch.Size([256])
decoder.stages.1.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.stages.1.convs.1.conv.bias shape torch.Size([256])
decoder.stages.1.convs.1.norm.weight shape torch.Size([256])
decoder.stages.1.convs.1.norm.bias shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.stages.1.convs.1.all_modules.0.bias shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.1.weight shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.1.bias shape torch.Size([256])
decoder.stages.2.convs.0.conv.weight shape torch.Size([128, 256, 3, 3, 3])
decoder.stages.2.convs.0.conv.bias shape torch.Size([128])
decoder.stages.2.convs.0.norm.weight shape torch.Size([128])
decoder.stages.2.convs.0.norm.bias shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.0.weight shape torch.Size([128, 256, 3, 3, 3])
decoder.stages.2.convs.0.all_modules.0.bias shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.1.weight shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.1.bias shape torch.Size([128])
decoder.stages.2.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.stages.2.convs.1.conv.bias shape torch.Size([128])
decoder.stages.2.convs.1.norm.weight shape torch.Size([128])
decoder.stages.2.convs.1.norm.bias shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.stages.2.convs.1.all_modules.0.bias shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.1.weight shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.1.bias shape torch.Size([128])
decoder.stages.3.convs.0.conv.weight shape torch.Size([64, 128, 3, 3, 3])
decoder.stages.3.convs.0.conv.bias shape torch.Size([64])
decoder.stages.3.convs.0.norm.weight shape torch.Size([64])
decoder.stages.3.convs.0.norm.bias shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.0.weight shape torch.Size([64, 128, 3, 3, 3])
decoder.stages.3.convs.0.all_modules.0.bias shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.1.weight shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.1.bias shape torch.Size([64])
decoder.stages.3.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.stages.3.convs.1.conv.bias shape torch.Size([64])
decoder.stages.3.convs.1.norm.weight shape torch.Size([64])
decoder.stages.3.convs.1.norm.bias shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.stages.3.convs.1.all_modules.0.bias shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.1.weight shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.1.bias shape torch.Size([64])
decoder.stages.4.convs.0.conv.weight shape torch.Size([32, 64, 3, 3, 3])
decoder.stages.4.convs.0.conv.bias shape torch.Size([32])
decoder.stages.4.convs.0.norm.weight shape torch.Size([32])
decoder.stages.4.convs.0.norm.bias shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.0.weight shape torch.Size([32, 64, 3, 3, 3])
decoder.stages.4.convs.0.all_modules.0.bias shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.1.weight shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.1.bias shape torch.Size([32])
decoder.stages.4.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.stages.4.convs.1.conv.bias shape torch.Size([32])
decoder.stages.4.convs.1.norm.weight shape torch.Size([32])
decoder.stages.4.convs.1.norm.bias shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.stages.4.convs.1.all_modules.0.bias shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.1.weight shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.1.bias shape torch.Size([32])
decoder.transpconvs.0.weight shape torch.Size([320, 320, 1, 2, 2])
decoder.transpconvs.0.bias shape torch.Size([320])
decoder.transpconvs.1.weight shape torch.Size([320, 256, 2, 2, 2])
decoder.transpconvs.1.bias shape torch.Size([256])
decoder.transpconvs.2.weight shape torch.Size([256, 128, 2, 2, 2])
decoder.transpconvs.2.bias shape torch.Size([128])
decoder.transpconvs.3.weight shape torch.Size([128, 64, 2, 2, 2])
decoder.transpconvs.3.bias shape torch.Size([64])
decoder.transpconvs.4.weight shape torch.Size([64, 32, 2, 2, 2])
decoder.transpconvs.4.bias shape torch.Size([32])
################### Done ###################
2024-12-18 02:21:01.379003: do_dummy_2d_data_aug: True
2024-12-18 02:21:01.380775: Using splits from existing split file: /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset309_Sohee_Calcium_OCT_CrossValidation/splits_final_60.json
2024-12-18 02:21:01.383057: The split file contains 5 splits.
2024-12-18 02:21:01.384177: Desired fold for training: 3
2024-12-18 02:21:01.385233: This split has 4 training and 5 validation cases.
using pin_memory on device 0
using pin_memory on device 0
using pin_memory on device 0

This is the configuration used by this training:
Configuration name: 3d_32x160x128_b10
 {'data_identifier': 'nnUNetPlans_3d_fullres', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 10, 'patch_size': [32, 160, 128], 'median_image_size_in_voxels': [375.0, 498.0, 498.0], 'spacing': [1.0, 1.0, 1.0], 'normalization_schemes': ['ZScoreNormalization'], 'use_mask_for_norm': [False], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'architecture': {'network_class_name': 'dynamic_network_architectures.architectures.unet.PlainConvUNet', 'arch_kwargs': {'n_stages': 6, 'features_per_stage': [32, 64, 128, 256, 320, 320], 'conv_op': 'torch.nn.modules.conv.Conv3d', 'kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]], 'strides': [[1, 1, 1], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [1, 2, 2]], 'n_conv_per_stage': [2, 2, 2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2, 2, 2], 'conv_bias': True, 'norm_op': 'torch.nn.modules.instancenorm.InstanceNorm3d', 'norm_op_kwargs': {'eps': 1e-05, 'affine': True}, 'dropout_op': None, 'dropout_op_kwargs': None, 'nonlin': 'torch.nn.LeakyReLU', 'nonlin_kwargs': {'inplace': True}, 'deep_supervision': True}, '_kw_requires_import': ['conv_op', 'norm_op', 'dropout_op', 'nonlin']}, 'batch_dice': True, 'inherits_from': '3d_fullres'} 

These are the global plan.json settings:
 {'dataset_name': 'Dataset309_Sohee_Calcium_OCT_CrossValidation', 'plans_name': 'nnUNetPlans', 'original_median_spacing_after_transp': [1.0, 1.0, 1.0], 'original_median_shape_after_transp': [375, 498, 498], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 0.4977976381778717, 'mean': 0.13507525622844696, 'median': 0.09599608182907104, 'min': 0.0, 'percentile_00_5': 0.014754901640117168, 'percentile_99_5': 0.4977976381778717, 'std': 0.12025152146816254}}} 

2024-12-18 02:21:28.882962: unpacking dataset...
using pin_memory on device 0

This is the configuration used by this training:
Configuration name: 3d_32x160x128_b10
 {'data_identifier': 'nnUNetPlans_3d_fullres', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 10, 'patch_size': [32, 160, 128], 'median_image_size_in_voxels': [375.0, 498.0, 498.0], 'spacing': [1.0, 1.0, 1.0], 'normalization_schemes': ['ZScoreNormalization'], 'use_mask_for_norm': [False], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'architecture': {'network_class_name': 'dynamic_network_architectures.architectures.unet.PlainConvUNet', 'arch_kwargs': {'n_stages': 6, 'features_per_stage': [32, 64, 128, 256, 320, 320], 'conv_op': 'torch.nn.modules.conv.Conv3d', 'kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]], 'strides': [[1, 1, 1], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [1, 2, 2]], 'n_conv_per_stage': [2, 2, 2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2, 2, 2], 'conv_bias': True, 'norm_op': 'torch.nn.modules.instancenorm.InstanceNorm3d', 'norm_op_kwargs': {'eps': 1e-05, 'affine': True}, 'dropout_op': None, 'dropout_op_kwargs': None, 'nonlin': 'torch.nn.LeakyReLU', 'nonlin_kwargs': {'inplace': True}, 'deep_supervision': True}, '_kw_requires_import': ['conv_op', 'norm_op', 'dropout_op', 'nonlin']}, 'batch_dice': True, 'inherits_from': '3d_fullres'} 

These are the global plan.json settings:
 {'dataset_name': 'Dataset309_Sohee_Calcium_OCT_CrossValidation', 'plans_name': 'nnUNetPlans', 'original_median_spacing_after_transp': [1.0, 1.0, 1.0], 'original_median_shape_after_transp': [375, 498, 498], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 0.4977976381778717, 'mean': 0.13507525622844696, 'median': 0.09599608182907104, 'min': 0.0, 'percentile_00_5': 0.014754901640117168, 'percentile_99_5': 0.4977976381778717, 'std': 0.12025152146816254}}} 

2024-12-18 02:21:33.276715: unpacking dataset...
2024-12-18 02:21:33.305303: unpacking done...
2024-12-18 02:21:33.655225: Unable to plot network architecture: nnUNet_compile is enabled!
2024-12-18 02:21:33.876790: 
2024-12-18 02:21:33.878132: Epoch 0
2024-12-18 02:21:33.879055: Current learning rate: 0.01
2024-12-18 02:30:18.731404: Validation loss improved from 1000.00000 to -0.38404! Patience: 0/50
2024-12-18 02:30:18.733451: train_loss -0.3413
2024-12-18 02:30:18.734687: val_loss -0.384
2024-12-18 02:30:18.735496: Pseudo dice [0.6757]
2024-12-18 02:30:18.736284: Epoch time: 524.86 s
2024-12-18 02:30:18.737139: Yayy! New best EMA pseudo Dice: 0.6757
2024-12-18 02:30:20.489789: 
2024-12-18 02:30:20.491131: Epoch 1
2024-12-18 02:30:20.492029: Current learning rate: 0.00994
2024-12-18 02:37:28.124753: Validation loss improved from -0.38404 to -0.39687! Patience: 0/50
2024-12-18 02:37:28.125697: train_loss -0.5083
2024-12-18 02:37:28.126702: val_loss -0.3969
2024-12-18 02:37:28.127689: Pseudo dice [0.6801]
2024-12-18 02:37:28.128727: Epoch time: 427.64 s
2024-12-18 02:37:28.129782: Yayy! New best EMA pseudo Dice: 0.6762
2024-12-18 02:37:30.010307: 
2024-12-18 02:37:30.011909: Epoch 2
2024-12-18 02:37:30.012918: Current learning rate: 0.00988
2024-12-18 02:44:15.663707: Validation loss improved from -0.39687 to -0.41001! Patience: 0/50
2024-12-18 02:44:15.664510: train_loss -0.5534
2024-12-18 02:44:15.665713: val_loss -0.41
2024-12-18 02:44:15.667036: Pseudo dice [0.6869]
2024-12-18 02:44:15.668445: Epoch time: 405.66 s
2024-12-18 02:44:15.669607: Yayy! New best EMA pseudo Dice: 0.6772
2024-12-18 02:44:17.554444: 
2024-12-18 02:44:17.555572: Epoch 3
2024-12-18 02:44:17.556306: Current learning rate: 0.00982
2024-12-18 02:51:24.975875: Validation loss improved from -0.41001 to -0.44445! Patience: 0/50
2024-12-18 02:51:24.976846: train_loss -0.577
2024-12-18 02:51:24.977622: val_loss -0.4444
2024-12-18 02:51:24.978339: Pseudo dice [0.7001]
2024-12-18 02:51:24.979097: Epoch time: 427.42 s
2024-12-18 02:51:24.979810: Yayy! New best EMA pseudo Dice: 0.6795
2024-12-18 02:51:26.799345: 
2024-12-18 02:51:26.800631: Epoch 4
2024-12-18 02:51:26.801465: Current learning rate: 0.00976
2024-12-18 02:57:52.193705: Validation loss did not improve from -0.44445. Patience: 1/50
2024-12-18 02:57:52.194649: train_loss -0.5891
2024-12-18 02:57:52.195401: val_loss -0.3496
2024-12-18 02:57:52.196053: Pseudo dice [0.6683]
2024-12-18 02:57:52.196765: Epoch time: 385.4 s
2024-12-18 02:57:54.019259: 
2024-12-18 02:57:54.020768: Epoch 5
2024-12-18 02:57:54.021923: Current learning rate: 0.0097
2024-12-18 03:05:08.581260: Validation loss did not improve from -0.44445. Patience: 2/50
2024-12-18 03:05:08.582188: train_loss -0.615
2024-12-18 03:05:08.583046: val_loss -0.4063
2024-12-18 03:05:08.583870: Pseudo dice [0.6824]
2024-12-18 03:05:08.584564: Epoch time: 434.56 s
2024-12-18 03:05:10.011158: 
2024-12-18 03:05:10.012515: Epoch 6
2024-12-18 03:05:10.013585: Current learning rate: 0.00964
2024-12-18 03:11:57.991088: Validation loss improved from -0.44445 to -0.45868! Patience: 2/50
2024-12-18 03:11:57.991917: train_loss -0.6203
2024-12-18 03:11:57.992703: val_loss -0.4587
2024-12-18 03:11:57.993548: Pseudo dice [0.7062]
2024-12-18 03:11:57.994387: Epoch time: 407.98 s
2024-12-18 03:11:57.995121: Yayy! New best EMA pseudo Dice: 0.6815
2024-12-18 03:11:59.816689: 
2024-12-18 03:11:59.817982: Epoch 7
2024-12-18 03:11:59.818703: Current learning rate: 0.00958
2024-12-18 03:18:54.341761: Validation loss improved from -0.45868 to -0.47068! Patience: 0/50
2024-12-18 03:18:54.342731: train_loss -0.635
2024-12-18 03:18:54.343504: val_loss -0.4707
2024-12-18 03:18:54.344295: Pseudo dice [0.7252]
2024-12-18 03:18:54.345094: Epoch time: 414.53 s
2024-12-18 03:18:54.345824: Yayy! New best EMA pseudo Dice: 0.6859
2024-12-18 03:18:56.592340: 
2024-12-18 03:18:56.593704: Epoch 8
2024-12-18 03:18:56.594413: Current learning rate: 0.00952
2024-12-18 03:25:33.838884: Validation loss did not improve from -0.47068. Patience: 1/50
2024-12-18 03:25:33.839811: train_loss -0.6375
2024-12-18 03:25:33.840672: val_loss -0.426
2024-12-18 03:25:33.841388: Pseudo dice [0.705]
2024-12-18 03:25:33.842193: Epoch time: 397.25 s
2024-12-18 03:25:33.842880: Yayy! New best EMA pseudo Dice: 0.6878
2024-12-18 03:25:35.870501: 
2024-12-18 03:25:35.871775: Epoch 9
2024-12-18 03:25:35.872507: Current learning rate: 0.00946
2024-12-18 03:32:49.880633: Validation loss improved from -0.47068 to -0.50513! Patience: 1/50
2024-12-18 03:32:49.881704: train_loss -0.6466
2024-12-18 03:32:49.882597: val_loss -0.5051
2024-12-18 03:32:49.883373: Pseudo dice [0.7402]
2024-12-18 03:32:49.884100: Epoch time: 434.01 s
2024-12-18 03:32:50.302581: Yayy! New best EMA pseudo Dice: 0.6931
2024-12-18 03:32:52.031737: 
2024-12-18 03:32:52.032991: Epoch 10
2024-12-18 03:32:52.033768: Current learning rate: 0.0094
2024-12-18 03:39:32.635281: Validation loss did not improve from -0.50513. Patience: 1/50
2024-12-18 03:39:32.635926: train_loss -0.6522
2024-12-18 03:39:32.636645: val_loss -0.4163
2024-12-18 03:39:32.637265: Pseudo dice [0.695]
2024-12-18 03:39:32.637869: Epoch time: 400.61 s
2024-12-18 03:39:32.638494: Yayy! New best EMA pseudo Dice: 0.6933
2024-12-18 03:39:34.423232: 
2024-12-18 03:39:34.424539: Epoch 11
2024-12-18 03:39:34.425614: Current learning rate: 0.00934
2024-12-18 03:46:43.175418: Validation loss did not improve from -0.50513. Patience: 2/50
2024-12-18 03:46:43.176316: train_loss -0.662
2024-12-18 03:46:43.177265: val_loss -0.4486
2024-12-18 03:46:43.178053: Pseudo dice [0.7142]
2024-12-18 03:46:43.178757: Epoch time: 428.75 s
2024-12-18 03:46:43.179546: Yayy! New best EMA pseudo Dice: 0.6953
2024-12-18 03:46:45.053924: 
2024-12-18 03:46:45.055021: Epoch 12
2024-12-18 03:46:45.055850: Current learning rate: 0.00928
2024-12-18 03:53:22.512572: Validation loss did not improve from -0.50513. Patience: 3/50
2024-12-18 03:53:22.513659: train_loss -0.6703
2024-12-18 03:53:22.514659: val_loss -0.465
2024-12-18 03:53:22.515633: Pseudo dice [0.7244]
2024-12-18 03:53:22.516493: Epoch time: 397.46 s
2024-12-18 03:53:22.517381: Yayy! New best EMA pseudo Dice: 0.6983
2024-12-18 03:53:24.419124: 
2024-12-18 03:53:24.420559: Epoch 13
2024-12-18 03:53:24.421551: Current learning rate: 0.00922
2024-12-18 03:59:41.583575: Validation loss did not improve from -0.50513. Patience: 4/50
2024-12-18 03:59:41.584630: train_loss -0.6734
2024-12-18 03:59:41.585644: val_loss -0.4717
2024-12-18 03:59:41.586495: Pseudo dice [0.7182]
2024-12-18 03:59:41.587257: Epoch time: 377.17 s
2024-12-18 03:59:41.588042: Yayy! New best EMA pseudo Dice: 0.7002
2024-12-18 03:59:43.423650: 
2024-12-18 03:59:43.424802: Epoch 14
2024-12-18 03:59:43.425532: Current learning rate: 0.00916
2024-12-18 04:06:36.858259: Validation loss did not improve from -0.50513. Patience: 5/50
2024-12-18 04:06:36.859202: train_loss -0.6783
2024-12-18 04:06:36.859980: val_loss -0.4576
2024-12-18 04:06:36.860672: Pseudo dice [0.7227]
2024-12-18 04:06:36.861405: Epoch time: 413.44 s
2024-12-18 04:06:37.278275: Yayy! New best EMA pseudo Dice: 0.7025
2024-12-18 04:06:39.173708: 
2024-12-18 04:06:39.175103: Epoch 15
2024-12-18 04:06:39.175960: Current learning rate: 0.0091
2024-12-18 04:13:34.638904: Validation loss did not improve from -0.50513. Patience: 6/50
2024-12-18 04:13:34.639739: train_loss -0.6809
2024-12-18 04:13:34.640641: val_loss -0.4545
2024-12-18 04:13:34.641471: Pseudo dice [0.7258]
2024-12-18 04:13:34.642364: Epoch time: 415.47 s
2024-12-18 04:13:34.643265: Yayy! New best EMA pseudo Dice: 0.7048
2024-12-18 04:13:36.565611: 
2024-12-18 04:13:36.566768: Epoch 16
2024-12-18 04:13:36.567535: Current learning rate: 0.00903
2024-12-18 04:20:43.934366: Validation loss did not improve from -0.50513. Patience: 7/50
2024-12-18 04:20:43.935319: train_loss -0.6897
2024-12-18 04:20:43.936074: val_loss -0.4768
2024-12-18 04:20:43.936769: Pseudo dice [0.722]
2024-12-18 04:20:43.937495: Epoch time: 427.37 s
2024-12-18 04:20:43.938214: Yayy! New best EMA pseudo Dice: 0.7065
2024-12-18 04:20:46.076783: 
2024-12-18 04:20:46.078136: Epoch 17
2024-12-18 04:20:46.078766: Current learning rate: 0.00897
2024-12-18 04:27:29.135118: Validation loss did not improve from -0.50513. Patience: 8/50
2024-12-18 04:27:29.136362: train_loss -0.6917
2024-12-18 04:27:29.137183: val_loss -0.432
2024-12-18 04:27:29.137889: Pseudo dice [0.7078]
2024-12-18 04:27:29.138664: Epoch time: 403.06 s
2024-12-18 04:27:29.139364: Yayy! New best EMA pseudo Dice: 0.7067
2024-12-18 04:27:31.470408: 
2024-12-18 04:27:31.471642: Epoch 18
2024-12-18 04:27:31.472416: Current learning rate: 0.00891
2024-12-18 04:34:26.539642: Validation loss did not improve from -0.50513. Patience: 9/50
2024-12-18 04:34:26.541991: train_loss -0.6958
2024-12-18 04:34:26.543044: val_loss -0.4221
2024-12-18 04:34:26.543940: Pseudo dice [0.6984]
2024-12-18 04:34:26.544954: Epoch time: 415.07 s
2024-12-18 04:34:27.997817: 
2024-12-18 04:34:27.999055: Epoch 19
2024-12-18 04:34:27.999893: Current learning rate: 0.00885
2024-12-18 04:40:52.192510: Validation loss did not improve from -0.50513. Patience: 10/50
2024-12-18 04:40:52.193601: train_loss -0.7068
2024-12-18 04:40:52.194605: val_loss -0.4823
2024-12-18 04:40:52.195546: Pseudo dice [0.7324]
2024-12-18 04:40:52.196466: Epoch time: 384.2 s
2024-12-18 04:40:52.572138: Yayy! New best EMA pseudo Dice: 0.7085
2024-12-18 04:40:54.403970: 
2024-12-18 04:40:54.405625: Epoch 20
2024-12-18 04:40:54.406744: Current learning rate: 0.00879
2024-12-18 04:47:38.511437: Validation loss did not improve from -0.50513. Patience: 11/50
2024-12-18 04:47:38.512627: train_loss -0.7121
2024-12-18 04:47:38.513433: val_loss -0.4623
2024-12-18 04:47:38.514049: Pseudo dice [0.7244]
2024-12-18 04:47:38.514729: Epoch time: 404.11 s
2024-12-18 04:47:38.515385: Yayy! New best EMA pseudo Dice: 0.7101
2024-12-18 04:47:40.465519: 
2024-12-18 04:47:40.466825: Epoch 21
2024-12-18 04:47:40.467626: Current learning rate: 0.00873
2024-12-18 04:54:21.042741: Validation loss did not improve from -0.50513. Patience: 12/50
2024-12-18 04:54:21.043779: train_loss -0.7176
2024-12-18 04:54:21.044807: val_loss -0.4843
2024-12-18 04:54:21.045694: Pseudo dice [0.7233]
2024-12-18 04:54:21.046665: Epoch time: 400.58 s
2024-12-18 04:54:21.047523: Yayy! New best EMA pseudo Dice: 0.7114
2024-12-18 04:54:22.893684: 
2024-12-18 04:54:22.895269: Epoch 22
2024-12-18 04:54:22.896337: Current learning rate: 0.00867
2024-12-18 05:01:00.043729: Validation loss did not improve from -0.50513. Patience: 13/50
2024-12-18 05:01:00.044648: train_loss -0.7175
2024-12-18 05:01:00.045399: val_loss -0.4687
2024-12-18 05:01:00.046091: Pseudo dice [0.7299]
2024-12-18 05:01:00.046731: Epoch time: 397.15 s
2024-12-18 05:01:00.047376: Yayy! New best EMA pseudo Dice: 0.7133
2024-12-18 05:01:01.878996: 
2024-12-18 05:01:01.880134: Epoch 23
2024-12-18 05:01:01.880800: Current learning rate: 0.00861
2024-12-18 05:08:18.320847: Validation loss did not improve from -0.50513. Patience: 14/50
2024-12-18 05:08:18.322256: train_loss -0.7233
2024-12-18 05:08:18.323394: val_loss -0.4436
2024-12-18 05:08:18.324348: Pseudo dice [0.71]
2024-12-18 05:08:18.325512: Epoch time: 436.44 s
2024-12-18 05:08:19.822453: 
2024-12-18 05:08:19.823786: Epoch 24
2024-12-18 05:08:19.824808: Current learning rate: 0.00855
2024-12-18 05:15:15.231268: Validation loss did not improve from -0.50513. Patience: 15/50
2024-12-18 05:15:15.232039: train_loss -0.7212
2024-12-18 05:15:15.232820: val_loss -0.4711
2024-12-18 05:15:15.233554: Pseudo dice [0.7222]
2024-12-18 05:15:15.234302: Epoch time: 415.41 s
2024-12-18 05:15:15.645566: Yayy! New best EMA pseudo Dice: 0.7139
2024-12-18 05:15:17.505995: 
2024-12-18 05:15:17.507420: Epoch 25
2024-12-18 05:15:17.508126: Current learning rate: 0.00849
2024-12-18 05:22:19.151250: Validation loss did not improve from -0.50513. Patience: 16/50
2024-12-18 05:22:19.152142: train_loss -0.723
2024-12-18 05:22:19.153265: val_loss -0.455
2024-12-18 05:22:19.154144: Pseudo dice [0.7191]
2024-12-18 05:22:19.155093: Epoch time: 421.65 s
2024-12-18 05:22:19.156042: Yayy! New best EMA pseudo Dice: 0.7144
2024-12-18 05:22:21.016089: 
2024-12-18 05:22:21.017317: Epoch 26
2024-12-18 05:22:21.018301: Current learning rate: 0.00843
2024-12-18 05:29:03.857702: Validation loss did not improve from -0.50513. Patience: 17/50
2024-12-18 05:29:03.858689: train_loss -0.7265
2024-12-18 05:29:03.859668: val_loss -0.4658
2024-12-18 05:29:03.860426: Pseudo dice [0.7282]
2024-12-18 05:29:03.861103: Epoch time: 402.84 s
2024-12-18 05:29:03.861820: Yayy! New best EMA pseudo Dice: 0.7158
2024-12-18 05:29:05.750412: 
2024-12-18 05:29:05.751854: Epoch 27
2024-12-18 05:29:05.752651: Current learning rate: 0.00836
2024-12-18 05:35:51.128140: Validation loss did not improve from -0.50513. Patience: 18/50
2024-12-18 05:35:51.129166: train_loss -0.7221
2024-12-18 05:35:51.130195: val_loss -0.4654
2024-12-18 05:35:51.131011: Pseudo dice [0.7201]
2024-12-18 05:35:51.131840: Epoch time: 405.38 s
2024-12-18 05:35:51.132620: Yayy! New best EMA pseudo Dice: 0.7162
2024-12-18 05:35:52.968613: 
2024-12-18 05:35:52.969854: Epoch 28
2024-12-18 05:35:52.970650: Current learning rate: 0.0083
2024-12-18 05:42:44.660167: Validation loss did not improve from -0.50513. Patience: 19/50
2024-12-18 05:42:44.661610: train_loss -0.7253
2024-12-18 05:42:44.662612: val_loss -0.4334
2024-12-18 05:42:44.663354: Pseudo dice [0.7069]
2024-12-18 05:42:44.663993: Epoch time: 411.69 s
2024-12-18 05:42:46.521314: 
2024-12-18 05:42:46.522663: Epoch 29
2024-12-18 05:42:46.523590: Current learning rate: 0.00824
2024-12-18 05:49:59.389091: Validation loss did not improve from -0.50513. Patience: 20/50
2024-12-18 05:49:59.390251: train_loss -0.7332
2024-12-18 05:49:59.391866: val_loss -0.4579
2024-12-18 05:49:59.392788: Pseudo dice [0.7233]
2024-12-18 05:49:59.393696: Epoch time: 432.87 s
2024-12-18 05:50:01.316212: 
2024-12-18 05:50:01.317447: Epoch 30
2024-12-18 05:50:01.318383: Current learning rate: 0.00818
2024-12-18 05:57:26.232280: Validation loss did not improve from -0.50513. Patience: 21/50
2024-12-18 05:57:26.233374: train_loss -0.7356
2024-12-18 05:57:26.234366: val_loss -0.4228
2024-12-18 05:57:26.235398: Pseudo dice [0.7111]
2024-12-18 05:57:26.236272: Epoch time: 444.92 s
2024-12-18 05:57:27.716675: 
2024-12-18 05:57:27.718052: Epoch 31
2024-12-18 05:57:27.718871: Current learning rate: 0.00812
2024-12-18 06:04:46.895263: Validation loss did not improve from -0.50513. Patience: 22/50
2024-12-18 06:04:46.896424: train_loss -0.7341
2024-12-18 06:04:46.897423: val_loss -0.4846
2024-12-18 06:04:46.898218: Pseudo dice [0.7362]
2024-12-18 06:04:46.899049: Epoch time: 439.18 s
2024-12-18 06:04:46.899873: Yayy! New best EMA pseudo Dice: 0.7176
2024-12-18 06:04:48.771939: 
2024-12-18 06:04:48.773014: Epoch 32
2024-12-18 06:04:48.773860: Current learning rate: 0.00806
2024-12-18 06:11:48.861327: Validation loss did not improve from -0.50513. Patience: 23/50
2024-12-18 06:11:48.862233: train_loss -0.7395
2024-12-18 06:11:48.863107: val_loss -0.457
2024-12-18 06:11:48.863840: Pseudo dice [0.7226]
2024-12-18 06:11:48.864681: Epoch time: 420.09 s
2024-12-18 06:11:48.865516: Yayy! New best EMA pseudo Dice: 0.7181
2024-12-18 06:11:50.790048: 
2024-12-18 06:11:50.790922: Epoch 33
2024-12-18 06:11:50.791667: Current learning rate: 0.008
2024-12-18 06:18:28.084513: Validation loss did not improve from -0.50513. Patience: 24/50
2024-12-18 06:18:28.085544: train_loss -0.7431
2024-12-18 06:18:28.086535: val_loss -0.4408
2024-12-18 06:18:28.087407: Pseudo dice [0.7163]
2024-12-18 06:18:28.089491: Epoch time: 397.3 s
2024-12-18 06:18:29.671952: 
2024-12-18 06:18:29.673188: Epoch 34
2024-12-18 06:18:29.673915: Current learning rate: 0.00793
2024-12-18 06:25:23.213520: Validation loss did not improve from -0.50513. Patience: 25/50
2024-12-18 06:25:23.214586: train_loss -0.7462
2024-12-18 06:25:23.215430: val_loss -0.4543
2024-12-18 06:25:23.216211: Pseudo dice [0.723]
2024-12-18 06:25:23.216977: Epoch time: 413.54 s
2024-12-18 06:25:23.622678: Yayy! New best EMA pseudo Dice: 0.7185
2024-12-18 06:25:25.636784: 
2024-12-18 06:25:25.638200: Epoch 35
2024-12-18 06:25:25.639018: Current learning rate: 0.00787
2024-12-18 06:32:21.375859: Validation loss did not improve from -0.50513. Patience: 26/50
2024-12-18 06:32:21.376931: train_loss -0.7445
2024-12-18 06:32:21.377940: val_loss -0.4649
2024-12-18 06:32:21.378625: Pseudo dice [0.717]
2024-12-18 06:32:21.379272: Epoch time: 415.74 s
2024-12-18 06:32:22.836690: 
2024-12-18 06:32:22.837664: Epoch 36
2024-12-18 06:32:22.838326: Current learning rate: 0.00781
2024-12-18 06:39:23.298001: Validation loss did not improve from -0.50513. Patience: 27/50
2024-12-18 06:39:23.299154: train_loss -0.7497
2024-12-18 06:39:23.299896: val_loss -0.4646
2024-12-18 06:39:23.300557: Pseudo dice [0.725]
2024-12-18 06:39:23.301231: Epoch time: 420.46 s
2024-12-18 06:39:23.301850: Yayy! New best EMA pseudo Dice: 0.719
2024-12-18 06:39:25.254649: 
2024-12-18 06:39:25.256258: Epoch 37
2024-12-18 06:39:25.256986: Current learning rate: 0.00775
2024-12-18 06:46:23.441311: Validation loss did not improve from -0.50513. Patience: 28/50
2024-12-18 06:46:23.445683: train_loss -0.752
2024-12-18 06:46:23.446774: val_loss -0.4554
2024-12-18 06:46:23.447751: Pseudo dice [0.7279]
2024-12-18 06:46:23.448566: Epoch time: 418.2 s
2024-12-18 06:46:23.449397: Yayy! New best EMA pseudo Dice: 0.7199
2024-12-18 06:46:25.315539: 
2024-12-18 06:46:25.317238: Epoch 38
2024-12-18 06:46:25.318155: Current learning rate: 0.00769
2024-12-18 06:53:03.863946: Validation loss did not improve from -0.50513. Patience: 29/50
2024-12-18 06:53:03.866311: train_loss -0.7457
2024-12-18 06:53:03.868069: val_loss -0.4601
2024-12-18 06:53:03.868888: Pseudo dice [0.7235]
2024-12-18 06:53:03.870167: Epoch time: 398.55 s
2024-12-18 06:53:03.871004: Yayy! New best EMA pseudo Dice: 0.7202
2024-12-18 06:53:06.225260: 
2024-12-18 06:53:06.226283: Epoch 39
2024-12-18 06:53:06.226975: Current learning rate: 0.00763
2024-12-18 07:00:02.276179: Validation loss did not improve from -0.50513. Patience: 30/50
2024-12-18 07:00:02.277150: train_loss -0.7532
2024-12-18 07:00:02.277900: val_loss -0.4874
2024-12-18 07:00:02.278555: Pseudo dice [0.7356]
2024-12-18 07:00:02.279354: Epoch time: 416.05 s
2024-12-18 07:00:02.658035: Yayy! New best EMA pseudo Dice: 0.7218
2024-12-18 07:00:04.690661: 
2024-12-18 07:00:04.691966: Epoch 40
2024-12-18 07:00:04.692826: Current learning rate: 0.00756
2024-12-18 07:06:49.890201: Validation loss did not improve from -0.50513. Patience: 31/50
2024-12-18 07:06:49.891146: train_loss -0.7526
2024-12-18 07:06:49.891881: val_loss -0.4327
2024-12-18 07:06:49.892503: Pseudo dice [0.7119]
2024-12-18 07:06:49.893106: Epoch time: 405.2 s
2024-12-18 07:06:51.314495: 
2024-12-18 07:06:51.315663: Epoch 41
2024-12-18 07:06:51.316351: Current learning rate: 0.0075
2024-12-18 07:14:20.627281: Validation loss did not improve from -0.50513. Patience: 32/50
2024-12-18 07:14:20.628197: train_loss -0.7562
2024-12-18 07:14:20.629379: val_loss -0.4671
2024-12-18 07:14:20.630641: Pseudo dice [0.7324]
2024-12-18 07:14:20.631718: Epoch time: 449.32 s
2024-12-18 07:14:20.632777: Yayy! New best EMA pseudo Dice: 0.7219
2024-12-18 07:14:22.608089: 
2024-12-18 07:14:22.609660: Epoch 42
2024-12-18 07:14:22.610690: Current learning rate: 0.00744
2024-12-18 07:21:02.269538: Validation loss did not improve from -0.50513. Patience: 33/50
2024-12-18 07:21:02.271429: train_loss -0.7569
2024-12-18 07:21:02.272296: val_loss -0.4374
2024-12-18 07:21:02.272973: Pseudo dice [0.7224]
2024-12-18 07:21:02.274037: Epoch time: 399.66 s
2024-12-18 07:21:02.275108: Yayy! New best EMA pseudo Dice: 0.722
2024-12-18 07:21:04.025945: 
2024-12-18 07:21:04.027327: Epoch 43
2024-12-18 07:21:04.028289: Current learning rate: 0.00738
2024-12-18 07:28:08.151527: Validation loss did not improve from -0.50513. Patience: 34/50
2024-12-18 07:28:08.152524: train_loss -0.7614
2024-12-18 07:28:08.153437: val_loss -0.4811
2024-12-18 07:28:08.154253: Pseudo dice [0.7286]
2024-12-18 07:28:08.155172: Epoch time: 424.13 s
2024-12-18 07:28:08.155998: Yayy! New best EMA pseudo Dice: 0.7227
2024-12-18 07:28:09.932366: 
2024-12-18 07:28:09.933536: Epoch 44
2024-12-18 07:28:09.934327: Current learning rate: 0.00732
2024-12-18 07:35:11.144459: Validation loss did not improve from -0.50513. Patience: 35/50
2024-12-18 07:35:11.145303: train_loss -0.7623
2024-12-18 07:35:11.146190: val_loss -0.4469
2024-12-18 07:35:11.147000: Pseudo dice [0.7293]
2024-12-18 07:35:11.147951: Epoch time: 421.21 s
2024-12-18 07:35:11.539649: Yayy! New best EMA pseudo Dice: 0.7233
2024-12-18 07:35:13.311672: 
2024-12-18 07:35:13.312917: Epoch 45
2024-12-18 07:35:13.313621: Current learning rate: 0.00725
2024-12-18 07:42:18.752312: Validation loss did not improve from -0.50513. Patience: 36/50
2024-12-18 07:42:18.753400: train_loss -0.765
2024-12-18 07:42:18.754207: val_loss -0.4831
2024-12-18 07:42:18.755072: Pseudo dice [0.7326]
2024-12-18 07:42:18.755826: Epoch time: 425.44 s
2024-12-18 07:42:18.756513: Yayy! New best EMA pseudo Dice: 0.7242
2024-12-18 07:42:20.626180: 
2024-12-18 07:42:20.627441: Epoch 46
2024-12-18 07:42:20.628249: Current learning rate: 0.00719
2024-12-18 07:49:29.338970: Validation loss did not improve from -0.50513. Patience: 37/50
2024-12-18 07:49:29.340701: train_loss -0.762
2024-12-18 07:49:29.341850: val_loss -0.4439
2024-12-18 07:49:29.342777: Pseudo dice [0.7286]
2024-12-18 07:49:29.343699: Epoch time: 428.72 s
2024-12-18 07:49:29.344585: Yayy! New best EMA pseudo Dice: 0.7247
2024-12-18 07:49:31.162091: 
2024-12-18 07:49:31.163371: Epoch 47
2024-12-18 07:49:31.164208: Current learning rate: 0.00713
2024-12-18 07:56:11.932886: Validation loss did not improve from -0.50513. Patience: 38/50
2024-12-18 07:56:11.933843: train_loss -0.767
2024-12-18 07:56:11.934768: val_loss -0.4398
2024-12-18 07:56:11.935588: Pseudo dice [0.7204]
2024-12-18 07:56:11.936379: Epoch time: 400.77 s
2024-12-18 07:56:13.401552: 
2024-12-18 07:56:13.403092: Epoch 48
2024-12-18 07:56:13.404034: Current learning rate: 0.00707
2024-12-18 08:03:28.638940: Validation loss did not improve from -0.50513. Patience: 39/50
2024-12-18 08:03:28.639859: train_loss -0.7676
2024-12-18 08:03:28.640709: val_loss -0.4768
2024-12-18 08:03:28.641362: Pseudo dice [0.7344]
2024-12-18 08:03:28.642195: Epoch time: 435.24 s
2024-12-18 08:03:28.642849: Yayy! New best EMA pseudo Dice: 0.7253
2024-12-18 08:03:30.517364: 
2024-12-18 08:03:30.518487: Epoch 49
2024-12-18 08:03:30.519243: Current learning rate: 0.007
2024-12-18 08:10:11.776162: Validation loss did not improve from -0.50513. Patience: 40/50
2024-12-18 08:10:11.777030: train_loss -0.7646
2024-12-18 08:10:11.778007: val_loss -0.4526
2024-12-18 08:10:11.778919: Pseudo dice [0.7292]
2024-12-18 08:10:11.779887: Epoch time: 401.26 s
2024-12-18 08:10:12.192290: Yayy! New best EMA pseudo Dice: 0.7257
2024-12-18 08:10:14.693382: 
2024-12-18 08:10:14.694829: Epoch 50
2024-12-18 08:10:14.695792: Current learning rate: 0.00694
2024-12-18 08:17:30.957713: Validation loss did not improve from -0.50513. Patience: 41/50
2024-12-18 08:17:30.958667: train_loss -0.7704
2024-12-18 08:17:30.959385: val_loss -0.4064
2024-12-18 08:17:30.960013: Pseudo dice [0.71]
2024-12-18 08:17:30.960652: Epoch time: 436.27 s
2024-12-18 08:17:32.334627: 
2024-12-18 08:17:32.335738: Epoch 51
2024-12-18 08:17:32.336499: Current learning rate: 0.00688
2024-12-18 08:24:11.033550: Validation loss did not improve from -0.50513. Patience: 42/50
2024-12-18 08:24:11.035074: train_loss -0.7711
2024-12-18 08:24:11.035990: val_loss -0.4583
2024-12-18 08:24:11.036746: Pseudo dice [0.7326]
2024-12-18 08:24:11.037621: Epoch time: 398.7 s
2024-12-18 08:24:12.461792: 
2024-12-18 08:24:12.462670: Epoch 52
2024-12-18 08:24:12.463311: Current learning rate: 0.00682
2024-12-18 08:31:17.213092: Validation loss did not improve from -0.50513. Patience: 43/50
2024-12-18 08:31:17.214043: train_loss -0.7716
2024-12-18 08:31:17.214955: val_loss -0.4603
2024-12-18 08:31:17.215744: Pseudo dice [0.7325]
2024-12-18 08:31:17.216496: Epoch time: 424.75 s
2024-12-18 08:31:17.217158: Yayy! New best EMA pseudo Dice: 0.7257
2024-12-18 08:31:19.005154: 
2024-12-18 08:31:19.006422: Epoch 53
2024-12-18 08:31:19.007086: Current learning rate: 0.00675
2024-12-18 08:38:24.036724: Validation loss did not improve from -0.50513. Patience: 44/50
2024-12-18 08:38:24.037575: train_loss -0.7748
2024-12-18 08:38:24.038364: val_loss -0.4793
2024-12-18 08:38:24.039369: Pseudo dice [0.7356]
2024-12-18 08:38:24.040178: Epoch time: 425.03 s
2024-12-18 08:38:24.041021: Yayy! New best EMA pseudo Dice: 0.7267
2024-12-18 08:38:25.810814: 
2024-12-18 08:38:25.811639: Epoch 54
2024-12-18 08:38:25.812327: Current learning rate: 0.00669
2024-12-18 08:45:57.934715: Validation loss did not improve from -0.50513. Patience: 45/50
2024-12-18 08:45:57.935714: train_loss -0.7704
2024-12-18 08:45:57.936545: val_loss -0.4678
2024-12-18 08:45:57.937211: Pseudo dice [0.7299]
2024-12-18 08:45:57.937844: Epoch time: 452.13 s
2024-12-18 08:45:58.344378: Yayy! New best EMA pseudo Dice: 0.727
2024-12-18 08:46:00.236052: 
2024-12-18 08:46:00.237192: Epoch 55
2024-12-18 08:46:00.238142: Current learning rate: 0.00663
2024-12-18 08:53:23.110974: Validation loss did not improve from -0.50513. Patience: 46/50
2024-12-18 08:53:23.113214: train_loss -0.7697
2024-12-18 08:53:23.114031: val_loss -0.4667
2024-12-18 08:53:23.114869: Pseudo dice [0.7275]
2024-12-18 08:53:23.115669: Epoch time: 442.88 s
2024-12-18 08:53:23.116334: Yayy! New best EMA pseudo Dice: 0.7271
2024-12-18 08:53:25.019247: 
2024-12-18 08:53:25.020384: Epoch 56
2024-12-18 08:53:25.021102: Current learning rate: 0.00657
2024-12-18 09:01:00.693161: Validation loss did not improve from -0.50513. Patience: 47/50
2024-12-18 09:01:00.694265: train_loss -0.7761
2024-12-18 09:01:00.695403: val_loss -0.4413
2024-12-18 09:01:00.696398: Pseudo dice [0.7203]
2024-12-18 09:01:00.697386: Epoch time: 455.68 s
2024-12-18 09:01:02.101309: 
2024-12-18 09:01:02.102693: Epoch 57
2024-12-18 09:01:02.103700: Current learning rate: 0.0065
2024-12-18 09:08:04.690414: Validation loss did not improve from -0.50513. Patience: 48/50
2024-12-18 09:08:04.691213: train_loss -0.7758
2024-12-18 09:08:04.692142: val_loss -0.4632
2024-12-18 09:08:04.692762: Pseudo dice [0.7345]
2024-12-18 09:08:04.693404: Epoch time: 422.59 s
2024-12-18 09:08:04.694044: Yayy! New best EMA pseudo Dice: 0.7272
2024-12-18 09:08:06.598572: 
2024-12-18 09:08:06.599906: Epoch 58
2024-12-18 09:08:06.600627: Current learning rate: 0.00644
2024-12-18 09:15:21.770827: Validation loss did not improve from -0.50513. Patience: 49/50
2024-12-18 09:15:21.771693: train_loss -0.7774
2024-12-18 09:15:21.772472: val_loss -0.4746
2024-12-18 09:15:21.773162: Pseudo dice [0.7358]
2024-12-18 09:15:21.773906: Epoch time: 435.17 s
2024-12-18 09:15:21.774597: Yayy! New best EMA pseudo Dice: 0.728
2024-12-18 09:15:23.577313: 
2024-12-18 09:15:23.578918: Epoch 59
2024-12-18 09:15:23.579771: Current learning rate: 0.00638
2024-12-18 09:22:26.446525: Validation loss did not improve from -0.50513. Patience: 50/50
2024-12-18 09:22:26.447527: train_loss -0.7751
2024-12-18 09:22:26.448614: val_loss -0.422
2024-12-18 09:22:26.449375: Pseudo dice [0.7139]
2024-12-18 09:22:26.450147: Epoch time: 422.87 s
2024-12-18 09:22:28.264988: 
2024-12-18 09:22:28.266854: Epoch 60
2024-12-18 09:22:28.267832: Current learning rate: 0.00631
2024-12-18 09:29:11.562110: Validation loss did not improve from -0.50513. Patience: 51/50
2024-12-18 09:29:11.563067: train_loss -0.774
2024-12-18 09:29:11.563875: val_loss -0.4495
2024-12-18 09:29:11.564513: Pseudo dice [0.7239]
2024-12-18 09:29:11.565278: Epoch time: 403.3 s
2024-12-18 09:29:13.754251: 
2024-12-18 09:29:13.755681: Epoch 61
2024-12-18 09:29:13.756646: Current learning rate: 0.00625
2024-12-18 09:35:28.658087: Validation loss did not improve from -0.50513. Patience: 52/50
2024-12-18 09:35:28.659037: train_loss -0.7809
2024-12-18 09:35:28.660049: val_loss -0.4611
2024-12-18 09:35:28.661111: Pseudo dice [0.7261]
2024-12-18 09:35:28.662078: Epoch time: 374.91 s
2024-12-18 09:35:30.073862: 
2024-12-18 09:35:30.075275: Epoch 62
2024-12-18 09:35:30.076108: Current learning rate: 0.00619
2024-12-18 09:41:25.090814: Validation loss did not improve from -0.50513. Patience: 53/50
2024-12-18 09:41:25.091870: train_loss -0.7812
2024-12-18 09:41:25.092817: val_loss -0.4537
2024-12-18 09:41:25.093761: Pseudo dice [0.722]
2024-12-18 09:41:25.094897: Epoch time: 355.02 s
2024-12-18 09:41:26.490136: 
2024-12-18 09:41:26.491491: Epoch 63
2024-12-18 09:41:26.492429: Current learning rate: 0.00612
2024-12-18 09:47:46.824810: Validation loss did not improve from -0.50513. Patience: 54/50
2024-12-18 09:47:46.825764: train_loss -0.7792
2024-12-18 09:47:46.826546: val_loss -0.4604
2024-12-18 09:47:46.827178: Pseudo dice [0.7348]
2024-12-18 09:47:46.827866: Epoch time: 380.34 s
2024-12-18 09:47:48.266642: 
2024-12-18 09:47:48.267777: Epoch 64
2024-12-18 09:47:48.268412: Current learning rate: 0.00606
2024-12-18 09:54:38.569911: Validation loss did not improve from -0.50513. Patience: 55/50
2024-12-18 09:54:38.570921: train_loss -0.7839
2024-12-18 09:54:38.571691: val_loss -0.4446
2024-12-18 09:54:38.572275: Pseudo dice [0.7209]
2024-12-18 09:54:38.572919: Epoch time: 410.31 s
2024-12-18 09:54:40.372656: 
2024-12-18 09:54:40.373829: Epoch 65
2024-12-18 09:54:40.374626: Current learning rate: 0.006
2024-12-18 10:01:20.104609: Validation loss did not improve from -0.50513. Patience: 56/50
2024-12-18 10:01:20.105914: train_loss -0.7815
2024-12-18 10:01:20.106768: val_loss -0.434
2024-12-18 10:01:20.107708: Pseudo dice [0.7179]
2024-12-18 10:01:20.108694: Epoch time: 399.73 s
2024-12-18 10:01:21.592490: 
2024-12-18 10:01:21.593861: Epoch 66
2024-12-18 10:01:21.594740: Current learning rate: 0.00593
2024-12-18 10:07:54.329951: Validation loss did not improve from -0.50513. Patience: 57/50
2024-12-18 10:07:54.331580: train_loss -0.7871
2024-12-18 10:07:54.332569: val_loss -0.4542
2024-12-18 10:07:54.333282: Pseudo dice [0.7298]
2024-12-18 10:07:54.334011: Epoch time: 392.74 s
2024-12-18 10:07:55.964472: 
2024-12-18 10:07:55.965873: Epoch 67
2024-12-18 10:07:55.966721: Current learning rate: 0.00587
2024-12-18 10:14:21.311036: Validation loss did not improve from -0.50513. Patience: 58/50
2024-12-18 10:14:21.312018: train_loss -0.7849
2024-12-18 10:14:21.312805: val_loss -0.4438
2024-12-18 10:14:21.313499: Pseudo dice [0.7214]
2024-12-18 10:14:21.314222: Epoch time: 385.35 s
2024-12-18 10:14:22.815735: 
2024-12-18 10:14:22.817204: Epoch 68
2024-12-18 10:14:22.818217: Current learning rate: 0.00581
2024-12-18 10:20:32.999180: Validation loss did not improve from -0.50513. Patience: 59/50
2024-12-18 10:20:33.000389: train_loss -0.7882
2024-12-18 10:20:33.001644: val_loss -0.4565
2024-12-18 10:20:33.002620: Pseudo dice [0.7315]
2024-12-18 10:20:33.003552: Epoch time: 370.19 s
2024-12-18 10:20:34.550434: 
2024-12-18 10:20:34.551622: Epoch 69
2024-12-18 10:20:34.552348: Current learning rate: 0.00574
2024-12-18 10:26:55.739966: Validation loss did not improve from -0.50513. Patience: 60/50
2024-12-18 10:26:55.740796: train_loss -0.7825
2024-12-18 10:26:55.741709: val_loss -0.4518
2024-12-18 10:26:55.742582: Pseudo dice [0.7258]
2024-12-18 10:26:55.743363: Epoch time: 381.19 s
2024-12-18 10:26:57.729605: 
2024-12-18 10:26:57.730883: Epoch 70
2024-12-18 10:26:57.731772: Current learning rate: 0.00568
2024-12-18 10:33:12.942125: Validation loss did not improve from -0.50513. Patience: 61/50
2024-12-18 10:33:12.943562: train_loss -0.7848
2024-12-18 10:33:12.944309: val_loss -0.453
2024-12-18 10:33:12.944998: Pseudo dice [0.7315]
2024-12-18 10:33:12.945826: Epoch time: 375.22 s
2024-12-18 10:33:14.459252: 
2024-12-18 10:33:14.460600: Epoch 71
2024-12-18 10:33:14.461350: Current learning rate: 0.00562
2024-12-18 10:39:36.979371: Validation loss did not improve from -0.50513. Patience: 62/50
2024-12-18 10:39:36.980440: train_loss -0.7913
2024-12-18 10:39:36.981323: val_loss -0.4543
2024-12-18 10:39:36.982106: Pseudo dice [0.725]
2024-12-18 10:39:36.982919: Epoch time: 382.52 s
2024-12-18 10:39:39.031671: 
2024-12-18 10:39:39.032986: Epoch 72
2024-12-18 10:39:39.033751: Current learning rate: 0.00555
2024-12-18 10:46:06.292486: Validation loss did not improve from -0.50513. Patience: 63/50
2024-12-18 10:46:06.293585: train_loss -0.7906
2024-12-18 10:46:06.294728: val_loss -0.4805
2024-12-18 10:46:06.295675: Pseudo dice [0.7435]
2024-12-18 10:46:06.296723: Epoch time: 387.26 s
2024-12-18 10:46:06.297608: Yayy! New best EMA pseudo Dice: 0.7281
2024-12-18 10:46:08.197051: 
2024-12-18 10:46:08.198128: Epoch 73
2024-12-18 10:46:08.198914: Current learning rate: 0.00549
2024-12-18 10:52:51.230543: Validation loss did not improve from -0.50513. Patience: 64/50
2024-12-18 10:52:51.231635: train_loss -0.7908
2024-12-18 10:52:51.232598: val_loss -0.4483
2024-12-18 10:52:51.233345: Pseudo dice [0.7297]
2024-12-18 10:52:51.234157: Epoch time: 403.04 s
2024-12-18 10:52:51.234969: Yayy! New best EMA pseudo Dice: 0.7283
2024-12-18 10:52:53.041330: 
2024-12-18 10:52:53.042835: Epoch 74
2024-12-18 10:52:53.043866: Current learning rate: 0.00542
2024-12-18 10:59:09.196373: Validation loss did not improve from -0.50513. Patience: 65/50
2024-12-18 10:59:09.197392: train_loss -0.789
2024-12-18 10:59:09.198165: val_loss -0.4626
2024-12-18 10:59:09.198799: Pseudo dice [0.7292]
2024-12-18 10:59:09.199459: Epoch time: 376.16 s
2024-12-18 10:59:09.640871: Yayy! New best EMA pseudo Dice: 0.7283
2024-12-18 10:59:11.483120: 
2024-12-18 10:59:11.484233: Epoch 75
2024-12-18 10:59:11.485001: Current learning rate: 0.00536
2024-12-18 11:05:16.856533: Validation loss did not improve from -0.50513. Patience: 66/50
2024-12-18 11:05:16.859627: train_loss -0.7941
2024-12-18 11:05:16.860709: val_loss -0.4741
2024-12-18 11:05:16.861498: Pseudo dice [0.7416]
2024-12-18 11:05:16.862316: Epoch time: 365.38 s
2024-12-18 11:05:16.863028: Yayy! New best EMA pseudo Dice: 0.7297
2024-12-18 11:05:18.888695: 
2024-12-18 11:05:18.889799: Epoch 76
2024-12-18 11:05:18.890514: Current learning rate: 0.00529
2024-12-18 11:11:33.095472: Validation loss did not improve from -0.50513. Patience: 67/50
2024-12-18 11:11:33.096564: train_loss -0.7939
2024-12-18 11:11:33.098241: val_loss -0.4675
2024-12-18 11:11:33.099015: Pseudo dice [0.7382]
2024-12-18 11:11:33.099849: Epoch time: 374.21 s
2024-12-18 11:11:33.100592: Yayy! New best EMA pseudo Dice: 0.7305
2024-12-18 11:11:34.921599: 
2024-12-18 11:11:34.923060: Epoch 77
2024-12-18 11:11:34.923826: Current learning rate: 0.00523
2024-12-18 11:17:58.349220: Validation loss did not improve from -0.50513. Patience: 68/50
2024-12-18 11:17:58.350283: train_loss -0.7933
2024-12-18 11:17:58.351422: val_loss -0.4645
2024-12-18 11:17:58.352094: Pseudo dice [0.7303]
2024-12-18 11:17:58.352791: Epoch time: 383.43 s
2024-12-18 11:17:59.987739: 
2024-12-18 11:17:59.989079: Epoch 78
2024-12-18 11:17:59.989903: Current learning rate: 0.00517
2024-12-18 11:24:17.526523: Validation loss did not improve from -0.50513. Patience: 69/50
2024-12-18 11:24:17.527391: train_loss -0.7949
2024-12-18 11:24:17.528133: val_loss -0.4447
2024-12-18 11:24:17.528985: Pseudo dice [0.7241]
2024-12-18 11:24:17.529851: Epoch time: 377.54 s
2024-12-18 11:24:19.057971: 
2024-12-18 11:24:19.059403: Epoch 79
2024-12-18 11:24:19.060441: Current learning rate: 0.0051
2024-12-18 11:30:40.850389: Validation loss did not improve from -0.50513. Patience: 70/50
2024-12-18 11:30:40.851331: train_loss -0.7947
2024-12-18 11:30:40.852170: val_loss -0.4564
2024-12-18 11:30:40.852956: Pseudo dice [0.7318]
2024-12-18 11:30:40.853798: Epoch time: 381.79 s
2024-12-18 11:30:42.852426: 
2024-12-18 11:30:42.853478: Epoch 80
2024-12-18 11:30:42.854157: Current learning rate: 0.00504
2024-12-18 11:37:17.238837: Validation loss did not improve from -0.50513. Patience: 71/50
2024-12-18 11:37:17.239827: train_loss -0.7937
2024-12-18 11:37:17.241017: val_loss -0.4663
2024-12-18 11:37:17.241900: Pseudo dice [0.734]
2024-12-18 11:37:17.242775: Epoch time: 394.39 s
2024-12-18 11:37:18.779817: 
2024-12-18 11:37:18.781335: Epoch 81
2024-12-18 11:37:18.782474: Current learning rate: 0.00497
2024-12-18 11:43:56.189356: Validation loss did not improve from -0.50513. Patience: 72/50
2024-12-18 11:43:56.190211: train_loss -0.7954
2024-12-18 11:43:56.191211: val_loss -0.4323
2024-12-18 11:43:56.192040: Pseudo dice [0.725]
2024-12-18 11:43:56.192754: Epoch time: 397.41 s
2024-12-18 11:43:58.152947: 
2024-12-18 11:43:58.154045: Epoch 82
2024-12-18 11:43:58.154755: Current learning rate: 0.00491
2024-12-18 11:50:22.609446: Validation loss did not improve from -0.50513. Patience: 73/50
2024-12-18 11:50:22.610337: train_loss -0.7961
2024-12-18 11:50:22.611145: val_loss -0.4438
2024-12-18 11:50:22.611829: Pseudo dice [0.7211]
2024-12-18 11:50:22.612483: Epoch time: 384.46 s
2024-12-18 11:50:24.049821: 
2024-12-18 11:50:24.051081: Epoch 83
2024-12-18 11:50:24.051857: Current learning rate: 0.00484
2024-12-18 11:56:28.791057: Validation loss did not improve from -0.50513. Patience: 74/50
2024-12-18 11:56:28.792182: train_loss -0.797
2024-12-18 11:56:28.793290: val_loss -0.4445
2024-12-18 11:56:28.794186: Pseudo dice [0.7288]
2024-12-18 11:56:28.795140: Epoch time: 364.74 s
2024-12-18 11:56:30.225220: 
2024-12-18 11:56:30.226583: Epoch 84
2024-12-18 11:56:30.227348: Current learning rate: 0.00478
2024-12-18 12:03:08.764565: Validation loss did not improve from -0.50513. Patience: 75/50
2024-12-18 12:03:08.765540: train_loss -0.7996
2024-12-18 12:03:08.766458: val_loss -0.4537
2024-12-18 12:03:08.767101: Pseudo dice [0.7296]
2024-12-18 12:03:08.767741: Epoch time: 398.54 s
2024-12-18 12:03:10.705999: 
2024-12-18 12:03:10.707155: Epoch 85
2024-12-18 12:03:10.707904: Current learning rate: 0.00471
2024-12-18 12:09:12.368694: Validation loss did not improve from -0.50513. Patience: 76/50
2024-12-18 12:09:12.385324: train_loss -0.8
2024-12-18 12:09:12.386380: val_loss -0.4531
2024-12-18 12:09:12.387247: Pseudo dice [0.7314]
2024-12-18 12:09:12.387961: Epoch time: 361.68 s
2024-12-18 12:09:13.868162: 
2024-12-18 12:09:13.869343: Epoch 86
2024-12-18 12:09:13.870044: Current learning rate: 0.00465
2024-12-18 12:15:40.122783: Validation loss did not improve from -0.50513. Patience: 77/50
2024-12-18 12:15:40.123653: train_loss -0.7977
2024-12-18 12:15:40.124482: val_loss -0.4267
2024-12-18 12:15:40.125135: Pseudo dice [0.7257]
2024-12-18 12:15:40.125854: Epoch time: 386.26 s
2024-12-18 12:15:41.609520: 
2024-12-18 12:15:41.610652: Epoch 87
2024-12-18 12:15:41.611347: Current learning rate: 0.00458
2024-12-18 12:21:49.285224: Validation loss did not improve from -0.50513. Patience: 78/50
2024-12-18 12:21:49.286062: train_loss -0.8028
2024-12-18 12:21:49.287031: val_loss -0.4352
2024-12-18 12:21:49.287771: Pseudo dice [0.7229]
2024-12-18 12:21:49.288486: Epoch time: 367.68 s
2024-12-18 12:21:50.718909: 
2024-12-18 12:21:50.720120: Epoch 88
2024-12-18 12:21:50.721095: Current learning rate: 0.00452
2024-12-18 12:28:22.932560: Validation loss did not improve from -0.50513. Patience: 79/50
2024-12-18 12:28:22.933318: train_loss -0.797
2024-12-18 12:28:22.934128: val_loss -0.4596
2024-12-18 12:28:22.934752: Pseudo dice [0.736]
2024-12-18 12:28:22.935546: Epoch time: 392.22 s
2024-12-18 12:28:24.334034: 
2024-12-18 12:28:24.334863: Epoch 89
2024-12-18 12:28:24.335670: Current learning rate: 0.00445
2024-12-18 12:34:49.913441: Validation loss did not improve from -0.50513. Patience: 80/50
2024-12-18 12:34:49.914452: train_loss -0.8
2024-12-18 12:34:49.915349: val_loss -0.4694
2024-12-18 12:34:49.916142: Pseudo dice [0.7253]
2024-12-18 12:34:49.916784: Epoch time: 385.58 s
2024-12-18 12:34:51.806104: 
2024-12-18 12:34:51.807449: Epoch 90
2024-12-18 12:34:51.808358: Current learning rate: 0.00438
2024-12-18 12:41:08.874058: Validation loss did not improve from -0.50513. Patience: 81/50
2024-12-18 12:41:08.875030: train_loss -0.8006
2024-12-18 12:41:08.875943: val_loss -0.4398
2024-12-18 12:41:08.876621: Pseudo dice [0.7278]
2024-12-18 12:41:08.877293: Epoch time: 377.07 s
2024-12-18 12:41:10.293060: 
2024-12-18 12:41:10.294415: Epoch 91
2024-12-18 12:41:10.295273: Current learning rate: 0.00432
2024-12-18 12:47:34.819260: Validation loss did not improve from -0.50513. Patience: 82/50
2024-12-18 12:47:34.820335: train_loss -0.806
2024-12-18 12:47:34.821337: val_loss -0.4569
2024-12-18 12:47:34.822044: Pseudo dice [0.7336]
2024-12-18 12:47:34.822791: Epoch time: 384.53 s
2024-12-18 12:47:36.234540: 
2024-12-18 12:47:36.235864: Epoch 92
2024-12-18 12:47:36.236613: Current learning rate: 0.00425
2024-12-18 12:54:22.351062: Validation loss did not improve from -0.50513. Patience: 83/50
2024-12-18 12:54:22.352086: train_loss -0.8025
2024-12-18 12:54:22.353030: val_loss -0.4249
2024-12-18 12:54:22.353756: Pseudo dice [0.7146]
2024-12-18 12:54:22.354445: Epoch time: 406.12 s
2024-12-18 12:54:23.805225: 
2024-12-18 12:54:23.806392: Epoch 93
2024-12-18 12:54:23.807129: Current learning rate: 0.00419
2024-12-18 13:00:46.294270: Validation loss did not improve from -0.50513. Patience: 84/50
2024-12-18 13:00:46.294981: train_loss -0.805
2024-12-18 13:00:46.295730: val_loss -0.4117
2024-12-18 13:00:46.296521: Pseudo dice [0.708]
2024-12-18 13:00:46.297229: Epoch time: 382.49 s
2024-12-18 13:00:48.109826: 
2024-12-18 13:00:48.111229: Epoch 94
2024-12-18 13:00:48.111968: Current learning rate: 0.00412
2024-12-18 13:05:52.785661: Validation loss did not improve from -0.50513. Patience: 85/50
2024-12-18 13:05:52.786518: train_loss -0.8026
2024-12-18 13:05:52.787303: val_loss -0.4243
2024-12-18 13:05:52.787986: Pseudo dice [0.719]
2024-12-18 13:05:52.788738: Epoch time: 304.68 s
2024-12-18 13:05:54.617728: 
2024-12-18 13:05:54.619076: Epoch 95
2024-12-18 13:05:54.619914: Current learning rate: 0.00405
2024-12-18 13:11:18.677009: Validation loss did not improve from -0.50513. Patience: 86/50
2024-12-18 13:11:18.677819: train_loss -0.8032
2024-12-18 13:11:18.678705: val_loss -0.4559
2024-12-18 13:11:18.679481: Pseudo dice [0.7361]
2024-12-18 13:11:18.680204: Epoch time: 324.06 s
2024-12-18 13:11:20.151766: 
2024-12-18 13:11:20.152787: Epoch 96
2024-12-18 13:11:20.153664: Current learning rate: 0.00399
2024-12-18 13:17:20.187978: Validation loss did not improve from -0.50513. Patience: 87/50
2024-12-18 13:17:20.191415: train_loss -0.8074
2024-12-18 13:17:20.192366: val_loss -0.4333
2024-12-18 13:17:20.192974: Pseudo dice [0.7258]
2024-12-18 13:17:20.193811: Epoch time: 360.04 s
2024-12-18 13:17:21.714678: 
2024-12-18 13:17:21.715906: Epoch 97
2024-12-18 13:17:21.716584: Current learning rate: 0.00392
2024-12-18 13:23:02.500383: Validation loss did not improve from -0.50513. Patience: 88/50
2024-12-18 13:23:02.501214: train_loss -0.8061
2024-12-18 13:23:02.502096: val_loss -0.4551
2024-12-18 13:23:02.502956: Pseudo dice [0.7267]
2024-12-18 13:23:02.503745: Epoch time: 340.79 s
2024-12-18 13:23:03.985874: 
2024-12-18 13:23:03.987011: Epoch 98
2024-12-18 13:23:03.987739: Current learning rate: 0.00385
2024-12-18 13:29:29.922512: Validation loss did not improve from -0.50513. Patience: 89/50
2024-12-18 13:29:29.923775: train_loss -0.8077
2024-12-18 13:29:29.924867: val_loss -0.4205
2024-12-18 13:29:29.925651: Pseudo dice [0.7202]
2024-12-18 13:29:29.926448: Epoch time: 385.94 s
2024-12-18 13:29:31.340062: 
2024-12-18 13:29:31.341425: Epoch 99
2024-12-18 13:29:31.342357: Current learning rate: 0.00379
2024-12-18 13:36:19.392135: Validation loss did not improve from -0.50513. Patience: 90/50
2024-12-18 13:36:19.393298: train_loss -0.8063
2024-12-18 13:36:19.394190: val_loss -0.4564
2024-12-18 13:36:19.395280: Pseudo dice [0.7348]
2024-12-18 13:36:19.396290: Epoch time: 408.05 s
2024-12-18 13:36:21.274444: 
2024-12-18 13:36:21.275400: Epoch 100
2024-12-18 13:36:21.276107: Current learning rate: 0.00372
2024-12-18 13:42:59.226779: Validation loss did not improve from -0.50513. Patience: 91/50
2024-12-18 13:42:59.227505: train_loss -0.8091
2024-12-18 13:42:59.228336: val_loss -0.4382
2024-12-18 13:42:59.229181: Pseudo dice [0.7267]
2024-12-18 13:42:59.229920: Epoch time: 397.96 s
2024-12-18 13:43:00.688354: 
2024-12-18 13:43:00.689717: Epoch 101
2024-12-18 13:43:00.690464: Current learning rate: 0.00365
2024-12-18 13:49:15.320550: Validation loss did not improve from -0.50513. Patience: 92/50
2024-12-18 13:49:15.322487: train_loss -0.8104
2024-12-18 13:49:15.323464: val_loss -0.445
2024-12-18 13:49:15.324271: Pseudo dice [0.7276]
2024-12-18 13:49:15.325269: Epoch time: 374.64 s
2024-12-18 13:49:16.840642: 
2024-12-18 13:49:16.842011: Epoch 102
2024-12-18 13:49:16.842754: Current learning rate: 0.00359
2024-12-18 13:55:54.576504: Validation loss did not improve from -0.50513. Patience: 93/50
2024-12-18 13:55:54.577389: train_loss -0.8093
2024-12-18 13:55:54.578160: val_loss -0.4577
2024-12-18 13:55:54.578896: Pseudo dice [0.7296]
2024-12-18 13:55:54.579722: Epoch time: 397.74 s
2024-12-18 13:55:56.021499: 
2024-12-18 13:55:56.022563: Epoch 103
2024-12-18 13:55:56.023355: Current learning rate: 0.00352
2024-12-18 14:02:03.870765: Validation loss did not improve from -0.50513. Patience: 94/50
2024-12-18 14:02:03.871746: train_loss -0.8101
2024-12-18 14:02:03.872926: val_loss -0.3994
2024-12-18 14:02:03.873841: Pseudo dice [0.7153]
2024-12-18 14:02:03.874744: Epoch time: 367.85 s
2024-12-18 14:02:05.402411: 
2024-12-18 14:02:05.403500: Epoch 104
2024-12-18 14:02:05.404236: Current learning rate: 0.00345
2024-12-18 14:07:37.854378: Validation loss did not improve from -0.50513. Patience: 95/50
2024-12-18 14:07:37.855314: train_loss -0.8098
2024-12-18 14:07:37.856229: val_loss -0.448
2024-12-18 14:07:37.856946: Pseudo dice [0.7302]
2024-12-18 14:07:37.857725: Epoch time: 332.45 s
2024-12-18 14:07:40.557568: 
2024-12-18 14:07:40.558878: Epoch 105
2024-12-18 14:07:40.559758: Current learning rate: 0.00338
2024-12-18 14:13:55.030107: Validation loss did not improve from -0.50513. Patience: 96/50
2024-12-18 14:13:55.030927: train_loss -0.8089
2024-12-18 14:13:55.031795: val_loss -0.4386
2024-12-18 14:13:55.032475: Pseudo dice [0.7267]
2024-12-18 14:13:55.033231: Epoch time: 374.47 s
2024-12-18 14:13:56.497486: 
2024-12-18 14:13:56.498772: Epoch 106
2024-12-18 14:13:56.499687: Current learning rate: 0.00332
2024-12-18 14:19:03.121564: Validation loss did not improve from -0.50513. Patience: 97/50
2024-12-18 14:19:03.124200: train_loss -0.8115
2024-12-18 14:19:03.125884: val_loss -0.4214
2024-12-18 14:19:03.126643: Pseudo dice [0.7252]
2024-12-18 14:19:03.127468: Epoch time: 306.63 s
2024-12-18 14:19:04.620373: 
2024-12-18 14:19:04.621662: Epoch 107
2024-12-18 14:19:04.622691: Current learning rate: 0.00325
2024-12-18 14:24:14.437737: Validation loss did not improve from -0.50513. Patience: 98/50
2024-12-18 14:24:14.438797: train_loss -0.8154
2024-12-18 14:24:14.439738: val_loss -0.432
2024-12-18 14:24:14.440573: Pseudo dice [0.7183]
2024-12-18 14:24:14.441434: Epoch time: 309.82 s
2024-12-18 14:24:15.838428: 
2024-12-18 14:24:15.839574: Epoch 108
2024-12-18 14:24:15.840450: Current learning rate: 0.00318
2024-12-18 14:28:59.755479: Validation loss did not improve from -0.50513. Patience: 99/50
2024-12-18 14:28:59.756385: train_loss -0.8126
2024-12-18 14:28:59.757302: val_loss -0.4493
2024-12-18 14:28:59.757957: Pseudo dice [0.7218]
2024-12-18 14:28:59.758661: Epoch time: 283.92 s
2024-12-18 14:29:01.195656: 
2024-12-18 14:29:01.196653: Epoch 109
2024-12-18 14:29:01.197386: Current learning rate: 0.00311
2024-12-18 14:33:54.185536: Validation loss did not improve from -0.50513. Patience: 100/50
2024-12-18 14:33:54.186385: train_loss -0.8157
2024-12-18 14:33:54.187492: val_loss -0.4429
2024-12-18 14:33:54.188591: Pseudo dice [0.7235]
2024-12-18 14:33:54.189555: Epoch time: 292.99 s
2024-12-18 14:33:55.983212: 
2024-12-18 14:33:55.984493: Epoch 110
2024-12-18 14:33:55.985543: Current learning rate: 0.00304
2024-12-18 14:38:51.956567: Validation loss did not improve from -0.50513. Patience: 101/50
2024-12-18 14:38:51.957594: train_loss -0.8145
2024-12-18 14:38:51.958392: val_loss -0.4339
2024-12-18 14:38:51.959201: Pseudo dice [0.7326]
2024-12-18 14:38:51.959967: Epoch time: 295.98 s
2024-12-18 14:38:53.413856: 
2024-12-18 14:38:53.415037: Epoch 111
2024-12-18 14:38:53.415855: Current learning rate: 0.00297
2024-12-18 14:43:17.821538: Validation loss did not improve from -0.50513. Patience: 102/50
2024-12-18 14:43:17.822485: train_loss -0.8138
2024-12-18 14:43:17.823517: val_loss -0.4834
2024-12-18 14:43:17.824415: Pseudo dice [0.7455]
2024-12-18 14:43:17.825318: Epoch time: 264.41 s
2024-12-18 14:43:19.271463: 
2024-12-18 14:43:19.272536: Epoch 112
2024-12-18 14:43:19.273170: Current learning rate: 0.00291
2024-12-18 14:48:50.444805: Validation loss did not improve from -0.50513. Patience: 103/50
2024-12-18 14:48:50.445964: train_loss -0.8147
2024-12-18 14:48:50.446894: val_loss -0.4191
2024-12-18 14:48:50.447592: Pseudo dice [0.7158]
2024-12-18 14:48:50.448262: Epoch time: 331.18 s
2024-12-18 14:48:51.878608: 
2024-12-18 14:48:51.879692: Epoch 113
2024-12-18 14:48:51.880374: Current learning rate: 0.00284
2024-12-18 14:53:37.600812: Validation loss did not improve from -0.50513. Patience: 104/50
2024-12-18 14:53:37.602090: train_loss -0.816
2024-12-18 14:53:37.603101: val_loss -0.4286
2024-12-18 14:53:37.603966: Pseudo dice [0.7208]
2024-12-18 14:53:37.604921: Epoch time: 285.72 s
2024-12-18 14:53:39.098419: 
2024-12-18 14:53:39.099586: Epoch 114
2024-12-18 14:53:39.100508: Current learning rate: 0.00277
2024-12-18 14:57:55.127239: Validation loss did not improve from -0.50513. Patience: 105/50
2024-12-18 14:57:55.128443: train_loss -0.8161
2024-12-18 14:57:55.129542: val_loss -0.4362
2024-12-18 14:57:55.130504: Pseudo dice [0.7271]
2024-12-18 14:57:55.131385: Epoch time: 256.03 s
2024-12-18 14:57:57.300431: 
2024-12-18 14:57:57.301579: Epoch 115
2024-12-18 14:57:57.302305: Current learning rate: 0.0027
2024-12-18 15:03:00.117547: Validation loss did not improve from -0.50513. Patience: 106/50
2024-12-18 15:03:00.118429: train_loss -0.8171
2024-12-18 15:03:00.119339: val_loss -0.462
2024-12-18 15:03:00.120124: Pseudo dice [0.7419]
2024-12-18 15:03:00.120954: Epoch time: 302.82 s
2024-12-18 15:03:01.561875: 
2024-12-18 15:03:01.563027: Epoch 116
2024-12-18 15:03:01.563888: Current learning rate: 0.00263
2024-12-18 15:07:45.486932: Validation loss did not improve from -0.50513. Patience: 107/50
2024-12-18 15:07:45.487828: train_loss -0.8152
2024-12-18 15:07:45.488871: val_loss -0.4621
2024-12-18 15:07:45.489556: Pseudo dice [0.7385]
2024-12-18 15:07:45.490251: Epoch time: 283.93 s
2024-12-18 15:07:47.576664: 
2024-12-18 15:07:47.578094: Epoch 117
2024-12-18 15:07:47.579066: Current learning rate: 0.00256
2024-12-18 15:12:18.144277: Validation loss did not improve from -0.50513. Patience: 108/50
2024-12-18 15:12:18.145500: train_loss -0.8166
2024-12-18 15:12:18.146559: val_loss -0.4303
2024-12-18 15:12:18.147472: Pseudo dice [0.7265]
2024-12-18 15:12:18.148351: Epoch time: 270.57 s
2024-12-18 15:12:19.612957: 
2024-12-18 15:12:19.614294: Epoch 118
2024-12-18 15:12:19.615061: Current learning rate: 0.00249
2024-12-18 15:17:25.031859: Validation loss did not improve from -0.50513. Patience: 109/50
2024-12-18 15:17:25.032599: train_loss -0.8182
2024-12-18 15:17:25.033353: val_loss -0.4332
2024-12-18 15:17:25.034051: Pseudo dice [0.7231]
2024-12-18 15:17:25.034892: Epoch time: 305.42 s
2024-12-18 15:17:26.508703: 
2024-12-18 15:17:26.509680: Epoch 119
2024-12-18 15:17:26.510363: Current learning rate: 0.00242
2024-12-18 15:22:15.136495: Validation loss did not improve from -0.50513. Patience: 110/50
2024-12-18 15:22:15.138301: train_loss -0.8166
2024-12-18 15:22:15.140505: val_loss -0.4551
2024-12-18 15:22:15.141387: Pseudo dice [0.7337]
2024-12-18 15:22:15.142417: Epoch time: 288.63 s
2024-12-18 15:22:17.085396: 
2024-12-18 15:22:17.086646: Epoch 120
2024-12-18 15:22:17.087436: Current learning rate: 0.00235
2024-12-18 15:26:45.129784: Validation loss did not improve from -0.50513. Patience: 111/50
2024-12-18 15:26:45.131059: train_loss -0.8201
2024-12-18 15:26:45.131916: val_loss -0.4314
2024-12-18 15:26:45.132622: Pseudo dice [0.7231]
2024-12-18 15:26:45.133264: Epoch time: 268.05 s
2024-12-18 15:26:46.631956: 
2024-12-18 15:26:46.632910: Epoch 121
2024-12-18 15:26:46.633511: Current learning rate: 0.00228
2024-12-18 15:31:53.259849: Validation loss did not improve from -0.50513. Patience: 112/50
2024-12-18 15:31:53.260750: train_loss -0.8186
2024-12-18 15:31:53.261644: val_loss -0.4405
2024-12-18 15:31:53.262456: Pseudo dice [0.7206]
2024-12-18 15:31:53.263305: Epoch time: 306.63 s
2024-12-18 15:31:54.742511: 
2024-12-18 15:31:54.743601: Epoch 122
2024-12-18 15:31:54.744453: Current learning rate: 0.00221
2024-12-18 15:36:30.034303: Validation loss did not improve from -0.50513. Patience: 113/50
2024-12-18 15:36:30.035178: train_loss -0.8186
2024-12-18 15:36:30.036017: val_loss -0.4443
2024-12-18 15:36:30.036782: Pseudo dice [0.7247]
2024-12-18 15:36:30.037496: Epoch time: 275.29 s
2024-12-18 15:36:31.536771: 
2024-12-18 15:36:31.537910: Epoch 123
2024-12-18 15:36:31.538777: Current learning rate: 0.00214
2024-12-18 15:41:04.030780: Validation loss did not improve from -0.50513. Patience: 114/50
2024-12-18 15:41:04.031643: train_loss -0.8175
2024-12-18 15:41:04.032399: val_loss -0.4706
2024-12-18 15:41:04.033120: Pseudo dice [0.7443]
2024-12-18 15:41:04.033753: Epoch time: 272.5 s
2024-12-18 15:41:05.477519: 
2024-12-18 15:41:05.478830: Epoch 124
2024-12-18 15:41:05.479803: Current learning rate: 0.00207
2024-12-18 15:46:18.973723: Validation loss did not improve from -0.50513. Patience: 115/50
2024-12-18 15:46:18.974713: train_loss -0.8196
2024-12-18 15:46:18.975656: val_loss -0.4306
2024-12-18 15:46:18.976432: Pseudo dice [0.7315]
2024-12-18 15:46:18.977286: Epoch time: 313.5 s
2024-12-18 15:46:20.959275: 
2024-12-18 15:46:20.960447: Epoch 125
2024-12-18 15:46:20.961153: Current learning rate: 0.00199
2024-12-18 15:51:04.874091: Validation loss did not improve from -0.50513. Patience: 116/50
2024-12-18 15:51:04.874902: train_loss -0.822
2024-12-18 15:51:04.875565: val_loss -0.4108
2024-12-18 15:51:04.876132: Pseudo dice [0.7192]
2024-12-18 15:51:04.876874: Epoch time: 283.92 s
2024-12-18 15:51:06.412123: 
2024-12-18 15:51:06.413313: Epoch 126
2024-12-18 15:51:06.414098: Current learning rate: 0.00192
2024-12-18 15:55:57.304879: Validation loss did not improve from -0.50513. Patience: 117/50
2024-12-18 15:55:57.305734: train_loss -0.8187
2024-12-18 15:55:57.306608: val_loss -0.3908
2024-12-18 15:55:57.307531: Pseudo dice [0.7075]
2024-12-18 15:55:57.308375: Epoch time: 290.89 s
2024-12-18 15:55:59.161899: 
2024-12-18 15:55:59.163056: Epoch 127
2024-12-18 15:55:59.163813: Current learning rate: 0.00185
2024-12-18 16:00:56.658822: Validation loss did not improve from -0.50513. Patience: 118/50
2024-12-18 16:00:56.659675: train_loss -0.8237
2024-12-18 16:00:56.660616: val_loss -0.4033
2024-12-18 16:00:56.661331: Pseudo dice [0.7202]
2024-12-18 16:00:56.662103: Epoch time: 297.5 s
2024-12-18 16:00:58.162038: 
2024-12-18 16:00:58.163231: Epoch 128
2024-12-18 16:00:58.163929: Current learning rate: 0.00178
2024-12-18 16:05:33.291969: Validation loss did not improve from -0.50513. Patience: 119/50
2024-12-18 16:05:33.293020: train_loss -0.8217
2024-12-18 16:05:33.293813: val_loss -0.4255
2024-12-18 16:05:33.294576: Pseudo dice [0.7237]
2024-12-18 16:05:33.295251: Epoch time: 275.13 s
2024-12-18 16:05:34.761235: 
2024-12-18 16:05:34.762143: Epoch 129
2024-12-18 16:05:34.763044: Current learning rate: 0.0017
2024-12-18 16:10:24.411274: Validation loss did not improve from -0.50513. Patience: 120/50
2024-12-18 16:10:24.412026: train_loss -0.8222
2024-12-18 16:10:24.412929: val_loss -0.4531
2024-12-18 16:10:24.413723: Pseudo dice [0.7316]
2024-12-18 16:10:24.414631: Epoch time: 289.65 s
2024-12-18 16:10:26.380815: 
2024-12-18 16:10:26.381947: Epoch 130
2024-12-18 16:10:26.382625: Current learning rate: 0.00163
2024-12-18 16:16:03.663896: Validation loss did not improve from -0.50513. Patience: 121/50
2024-12-18 16:16:03.664637: train_loss -0.8213
2024-12-18 16:16:03.665434: val_loss -0.4102
2024-12-18 16:16:03.666068: Pseudo dice [0.7238]
2024-12-18 16:16:03.666670: Epoch time: 337.29 s
2024-12-18 16:16:05.090642: 
2024-12-18 16:16:05.091381: Epoch 131
2024-12-18 16:16:05.092000: Current learning rate: 0.00156
2024-12-18 16:21:23.135345: Validation loss did not improve from -0.50513. Patience: 122/50
2024-12-18 16:21:23.136214: train_loss -0.821
2024-12-18 16:21:23.137110: val_loss -0.4501
2024-12-18 16:21:23.137809: Pseudo dice [0.724]
2024-12-18 16:21:23.138523: Epoch time: 318.05 s
2024-12-18 16:21:24.602551: 
2024-12-18 16:21:24.603888: Epoch 132
2024-12-18 16:21:24.604895: Current learning rate: 0.00148
2024-12-18 16:26:43.531446: Validation loss did not improve from -0.50513. Patience: 123/50
2024-12-18 16:26:43.532534: train_loss -0.8237
2024-12-18 16:26:43.534443: val_loss -0.4114
2024-12-18 16:26:43.535227: Pseudo dice [0.7266]
2024-12-18 16:26:43.536311: Epoch time: 318.93 s
2024-12-18 16:26:45.168711: 
2024-12-18 16:26:45.169549: Epoch 133
2024-12-18 16:26:45.170363: Current learning rate: 0.00141
2024-12-18 16:34:41.062752: Validation loss did not improve from -0.50513. Patience: 124/50
2024-12-18 16:34:41.063565: train_loss -0.8228
2024-12-18 16:34:41.064456: val_loss -0.4462
2024-12-18 16:34:41.065323: Pseudo dice [0.7276]
2024-12-18 16:34:41.066299: Epoch time: 475.9 s
2024-12-18 16:34:42.537856: 
2024-12-18 16:34:42.538989: Epoch 134
2024-12-18 16:34:42.539697: Current learning rate: 0.00133
2024-12-18 16:42:19.710237: Validation loss did not improve from -0.50513. Patience: 125/50
2024-12-18 16:42:19.711061: train_loss -0.8222
2024-12-18 16:42:19.711847: val_loss -0.4677
2024-12-18 16:42:19.712533: Pseudo dice [0.7343]
2024-12-18 16:42:19.713299: Epoch time: 457.17 s
2024-12-18 16:42:21.701221: 
2024-12-18 16:42:21.702409: Epoch 135
2024-12-18 16:42:21.703233: Current learning rate: 0.00126
2024-12-18 16:50:02.560462: Validation loss did not improve from -0.50513. Patience: 126/50
2024-12-18 16:50:02.561446: train_loss -0.8236
2024-12-18 16:50:02.562229: val_loss -0.4167
2024-12-18 16:50:02.562960: Pseudo dice [0.7151]
2024-12-18 16:50:02.563684: Epoch time: 460.86 s
2024-12-18 16:50:04.057972: 
2024-12-18 16:50:04.059053: Epoch 136
2024-12-18 16:50:04.059782: Current learning rate: 0.00118
2024-12-18 16:57:19.740728: Validation loss did not improve from -0.50513. Patience: 127/50
2024-12-18 16:57:19.741696: train_loss -0.8228
2024-12-18 16:57:19.742476: val_loss -0.4307
2024-12-18 16:57:19.743252: Pseudo dice [0.7207]
2024-12-18 16:57:19.743939: Epoch time: 435.69 s
2024-12-18 16:57:21.261274: 
2024-12-18 16:57:21.262466: Epoch 137
2024-12-18 16:57:21.263368: Current learning rate: 0.00111
2024-12-18 17:04:22.626212: Validation loss did not improve from -0.50513. Patience: 128/50
2024-12-18 17:04:22.627179: train_loss -0.8255
2024-12-18 17:04:22.628129: val_loss -0.4091
2024-12-18 17:04:22.628874: Pseudo dice [0.715]
2024-12-18 17:04:22.629661: Epoch time: 421.37 s
2024-12-18 17:04:24.165310: 
2024-12-18 17:04:24.166240: Epoch 138
2024-12-18 17:04:24.167041: Current learning rate: 0.00103
2024-12-18 17:11:08.060588: Validation loss did not improve from -0.50513. Patience: 129/50
2024-12-18 17:11:08.061933: train_loss -0.8268
2024-12-18 17:11:08.062946: val_loss -0.4339
2024-12-18 17:11:08.063746: Pseudo dice [0.7269]
2024-12-18 17:11:08.064519: Epoch time: 403.9 s
2024-12-18 17:11:09.669669: 
2024-12-18 17:11:09.670823: Epoch 139
2024-12-18 17:11:09.671633: Current learning rate: 0.00095
2024-12-18 17:17:40.585362: Validation loss did not improve from -0.50513. Patience: 130/50
2024-12-18 17:17:40.586257: train_loss -0.8278
2024-12-18 17:17:40.587032: val_loss -0.3966
2024-12-18 17:17:40.587737: Pseudo dice [0.7065]
2024-12-18 17:17:40.588459: Epoch time: 390.92 s
2024-12-18 17:17:42.601454: 
2024-12-18 17:17:42.602729: Epoch 140
2024-12-18 17:17:42.603700: Current learning rate: 0.00087
2024-12-18 17:25:07.619758: Validation loss did not improve from -0.50513. Patience: 131/50
2024-12-18 17:25:07.620616: train_loss -0.8252
2024-12-18 17:25:07.621545: val_loss -0.4421
2024-12-18 17:25:07.622525: Pseudo dice [0.7299]
2024-12-18 17:25:07.623450: Epoch time: 445.02 s
2024-12-18 17:25:09.111828: 
2024-12-18 17:25:09.112754: Epoch 141
2024-12-18 17:25:09.113563: Current learning rate: 0.00079
2024-12-18 17:32:11.881319: Validation loss did not improve from -0.50513. Patience: 132/50
2024-12-18 17:32:11.882838: train_loss -0.8256
2024-12-18 17:32:11.883637: val_loss -0.4352
2024-12-18 17:32:11.884429: Pseudo dice [0.7312]
2024-12-18 17:32:11.885084: Epoch time: 422.77 s
2024-12-18 17:32:13.361164: 
2024-12-18 17:32:13.362223: Epoch 142
2024-12-18 17:32:13.362921: Current learning rate: 0.00071
2024-12-18 17:38:21.196533: Validation loss did not improve from -0.50513. Patience: 133/50
2024-12-18 17:38:21.197373: train_loss -0.8262
2024-12-18 17:38:21.198057: val_loss -0.4273
2024-12-18 17:38:21.198723: Pseudo dice [0.7256]
2024-12-18 17:38:21.199431: Epoch time: 367.84 s
2024-12-18 17:38:22.683424: 
2024-12-18 17:38:22.684666: Epoch 143
2024-12-18 17:38:22.685401: Current learning rate: 0.00063
2024-12-18 17:45:28.187584: Validation loss did not improve from -0.50513. Patience: 134/50
2024-12-18 17:45:28.188617: train_loss -0.8275
2024-12-18 17:45:28.189649: val_loss -0.4243
2024-12-18 17:45:28.190468: Pseudo dice [0.7214]
2024-12-18 17:45:28.191234: Epoch time: 425.51 s
2024-12-18 17:45:29.675389: 
2024-12-18 17:45:29.677184: Epoch 144
2024-12-18 17:45:29.677989: Current learning rate: 0.00055
2024-12-18 17:52:20.228585: Validation loss did not improve from -0.50513. Patience: 135/50
2024-12-18 17:52:20.230700: train_loss -0.8253
2024-12-18 17:52:20.231725: val_loss -0.4582
2024-12-18 17:52:20.232434: Pseudo dice [0.7357]
2024-12-18 17:52:20.233227: Epoch time: 410.56 s
2024-12-18 17:52:22.147485: 
2024-12-18 17:52:22.148751: Epoch 145
2024-12-18 17:52:22.149577: Current learning rate: 0.00047
2024-12-18 17:57:46.804253: Validation loss did not improve from -0.50513. Patience: 136/50
2024-12-18 17:57:46.805217: train_loss -0.8279
2024-12-18 17:57:46.806054: val_loss -0.4186
2024-12-18 17:57:46.806825: Pseudo dice [0.7155]
2024-12-18 17:57:46.807577: Epoch time: 324.66 s
2024-12-18 17:57:48.215571: 
2024-12-18 17:57:48.216514: Epoch 146
2024-12-18 17:57:48.217349: Current learning rate: 0.00038
2024-12-18 18:03:12.827358: Validation loss did not improve from -0.50513. Patience: 137/50
2024-12-18 18:03:12.827991: train_loss -0.8263
2024-12-18 18:03:12.828684: val_loss -0.4195
2024-12-18 18:03:12.829350: Pseudo dice [0.7227]
2024-12-18 18:03:12.830011: Epoch time: 324.61 s
2024-12-18 18:03:14.299283: 
2024-12-18 18:03:14.300131: Epoch 147
2024-12-18 18:03:14.300872: Current learning rate: 0.0003
2024-12-18 18:09:13.449802: Validation loss did not improve from -0.50513. Patience: 138/50
2024-12-18 18:09:13.451144: train_loss -0.8277
2024-12-18 18:09:13.452004: val_loss -0.4245
2024-12-18 18:09:13.452753: Pseudo dice [0.7222]
2024-12-18 18:09:13.453492: Epoch time: 359.15 s
2024-12-18 18:09:14.993196: 
2024-12-18 18:09:14.994255: Epoch 148
2024-12-18 18:09:14.995075: Current learning rate: 0.00021
2024-12-18 18:16:24.034725: Validation loss did not improve from -0.50513. Patience: 139/50
2024-12-18 18:16:24.035862: train_loss -0.8275
2024-12-18 18:16:24.036618: val_loss -0.4556
2024-12-18 18:16:24.037350: Pseudo dice [0.727]
2024-12-18 18:16:24.038129: Epoch time: 429.04 s
2024-12-18 18:16:26.543386: 
2024-12-18 18:16:26.544346: Epoch 149
2024-12-18 18:16:26.545101: Current learning rate: 0.00011
2024-12-18 18:23:34.341228: Validation loss did not improve from -0.50513. Patience: 140/50
2024-12-18 18:23:34.342241: train_loss -0.8278
2024-12-18 18:23:34.343205: val_loss -0.4094
2024-12-18 18:23:34.344012: Pseudo dice [0.7198]
2024-12-18 18:23:34.344765: Epoch time: 427.8 s
2024-12-18 18:23:36.321893: Training done.
2024-12-18 18:23:36.886852: Using splits from existing split file: /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset309_Sohee_Calcium_OCT_CrossValidation/splits_final_60.json
2024-12-18 18:23:36.906933: The split file contains 5 splits.
2024-12-18 18:23:36.907988: Desired fold for training: 3
2024-12-18 18:23:36.908844: This split has 4 training and 5 validation cases.
2024-12-18 18:23:36.910010: predicting 101-045
2024-12-18 18:23:36.959600: 101-045, shape torch.Size([1, 375, 498, 498]), rank 0
2024-12-18 18:25:53.832238: predicting 106-002
2024-12-18 18:25:53.866883: 106-002, shape torch.Size([1, 539, 498, 498]), rank 0
2024-12-18 18:28:50.649923: predicting 401-004
2024-12-18 18:28:50.711718: 401-004, shape torch.Size([1, 375, 498, 498]), rank 0
2024-12-18 18:30:38.275686: predicting 704-003
2024-12-18 18:30:38.324576: 704-003, shape torch.Size([1, 375, 498, 498]), rank 0
2024-12-18 18:32:23.001712: predicting 706-005
2024-12-18 18:32:23.019784: 706-005, shape torch.Size([1, 375, 498, 498]), rank 0
2024-12-18 18:34:48.596106: Validation complete
2024-12-18 18:34:48.596582: Mean Validation Dice:  0.6939977889610197
2024-12-18 02:21:37.494784: unpacking done...
2024-12-18 02:21:37.502892: Unable to plot network architecture: nnUNet_compile is enabled!
2024-12-18 02:21:37.556458: 
2024-12-18 02:21:37.557598: Epoch 0
2024-12-18 02:21:37.558494: Current learning rate: 0.01
2024-12-18 02:29:57.024718: Validation loss improved from 1000.00000 to -0.40690! Patience: 0/50
2024-12-18 02:29:57.026720: train_loss -0.3116
2024-12-18 02:29:57.027880: val_loss -0.4069
2024-12-18 02:29:57.028835: Pseudo dice [0.6712]
2024-12-18 02:29:57.029723: Epoch time: 499.47 s
2024-12-18 02:29:57.030411: Yayy! New best EMA pseudo Dice: 0.6712
2024-12-18 02:29:59.399200: 
2024-12-18 02:29:59.400731: Epoch 1
2024-12-18 02:29:59.401780: Current learning rate: 0.00994
2024-12-18 02:36:52.084156: Validation loss improved from -0.40690 to -0.46665! Patience: 0/50
2024-12-18 02:36:52.085216: train_loss -0.4668
2024-12-18 02:36:52.085946: val_loss -0.4667
2024-12-18 02:36:52.086534: Pseudo dice [0.687]
2024-12-18 02:36:52.087173: Epoch time: 412.69 s
2024-12-18 02:36:52.087837: Yayy! New best EMA pseudo Dice: 0.6728
2024-12-18 02:36:53.902944: 
2024-12-18 02:36:53.904108: Epoch 2
2024-12-18 02:36:53.904862: Current learning rate: 0.00988
2024-12-18 02:43:42.950490: Validation loss improved from -0.46665 to -0.49094! Patience: 0/50
2024-12-18 02:43:42.951531: train_loss -0.5147
2024-12-18 02:43:42.952454: val_loss -0.4909
2024-12-18 02:43:42.953204: Pseudo dice [0.7232]
2024-12-18 02:43:42.954056: Epoch time: 409.05 s
2024-12-18 02:43:42.954924: Yayy! New best EMA pseudo Dice: 0.6778
2024-12-18 02:43:44.886657: 
2024-12-18 02:43:44.888043: Epoch 3
2024-12-18 02:43:44.888934: Current learning rate: 0.00982
2024-12-18 02:50:35.473161: Validation loss did not improve from -0.49094. Patience: 1/50
2024-12-18 02:50:35.474139: train_loss -0.5461
2024-12-18 02:50:35.475048: val_loss -0.4869
2024-12-18 02:50:35.475992: Pseudo dice [0.7109]
2024-12-18 02:50:35.476982: Epoch time: 410.59 s
2024-12-18 02:50:35.478165: Yayy! New best EMA pseudo Dice: 0.6811
2024-12-18 02:50:37.309587: 
2024-12-18 02:50:37.311262: Epoch 4
2024-12-18 02:50:37.312434: Current learning rate: 0.00976
2024-12-18 02:57:18.561217: Validation loss improved from -0.49094 to -0.49167! Patience: 1/50
2024-12-18 02:57:18.562313: train_loss -0.5522
2024-12-18 02:57:18.563409: val_loss -0.4917
2024-12-18 02:57:18.564609: Pseudo dice [0.7239]
2024-12-18 02:57:18.565576: Epoch time: 401.25 s
2024-12-18 02:57:18.949400: Yayy! New best EMA pseudo Dice: 0.6854
2024-12-18 02:57:20.855348: 
2024-12-18 02:57:20.856774: Epoch 5
2024-12-18 02:57:20.857890: Current learning rate: 0.0097
2024-12-18 03:04:33.347560: Validation loss did not improve from -0.49167. Patience: 1/50
2024-12-18 03:04:33.348391: train_loss -0.5755
2024-12-18 03:04:33.349161: val_loss -0.4866
2024-12-18 03:04:33.350156: Pseudo dice [0.7007]
2024-12-18 03:04:33.351075: Epoch time: 432.49 s
2024-12-18 03:04:33.352032: Yayy! New best EMA pseudo Dice: 0.6869
2024-12-18 03:04:35.209259: 
2024-12-18 03:04:35.211038: Epoch 6
2024-12-18 03:04:35.211797: Current learning rate: 0.00964
2024-12-18 03:11:26.283262: Validation loss improved from -0.49167 to -0.52743! Patience: 1/50
2024-12-18 03:11:26.284067: train_loss -0.5861
2024-12-18 03:11:26.284765: val_loss -0.5274
2024-12-18 03:11:26.285391: Pseudo dice [0.7344]
2024-12-18 03:11:26.286044: Epoch time: 411.08 s
2024-12-18 03:11:26.286866: Yayy! New best EMA pseudo Dice: 0.6917
2024-12-18 03:11:28.146115: 
2024-12-18 03:11:28.147772: Epoch 7
2024-12-18 03:11:28.148634: Current learning rate: 0.00958
2024-12-18 03:18:20.228805: Validation loss did not improve from -0.52743. Patience: 1/50
2024-12-18 03:18:20.229843: train_loss -0.5875
2024-12-18 03:18:20.230990: val_loss -0.4732
2024-12-18 03:18:20.231893: Pseudo dice [0.691]
2024-12-18 03:18:20.232891: Epoch time: 412.09 s
2024-12-18 03:18:22.339537: 
2024-12-18 03:18:22.341094: Epoch 8
2024-12-18 03:18:22.341959: Current learning rate: 0.00952
2024-12-18 03:25:14.117653: Validation loss did not improve from -0.52743. Patience: 2/50
2024-12-18 03:25:14.118709: train_loss -0.6053
2024-12-18 03:25:14.119571: val_loss -0.4909
2024-12-18 03:25:14.120309: Pseudo dice [0.7187]
2024-12-18 03:25:14.120985: Epoch time: 411.78 s
2024-12-18 03:25:14.121673: Yayy! New best EMA pseudo Dice: 0.6943
2024-12-18 03:25:16.038202: 
2024-12-18 03:25:16.039214: Epoch 9
2024-12-18 03:25:16.040067: Current learning rate: 0.00946
2024-12-18 03:31:40.776978: Validation loss did not improve from -0.52743. Patience: 3/50
2024-12-18 03:31:40.780780: train_loss -0.6072
2024-12-18 03:31:40.781899: val_loss -0.5234
2024-12-18 03:31:40.782602: Pseudo dice [0.7369]
2024-12-18 03:31:40.783338: Epoch time: 384.74 s
2024-12-18 03:31:41.205596: Yayy! New best EMA pseudo Dice: 0.6986
2024-12-18 03:31:43.113413: 
2024-12-18 03:31:43.114824: Epoch 10
2024-12-18 03:31:43.115578: Current learning rate: 0.0094
2024-12-18 03:38:57.915174: Validation loss improved from -0.52743 to -0.52958! Patience: 3/50
2024-12-18 03:38:57.916100: train_loss -0.6188
2024-12-18 03:38:57.917897: val_loss -0.5296
2024-12-18 03:38:57.918627: Pseudo dice [0.7414]
2024-12-18 03:38:57.919420: Epoch time: 434.8 s
2024-12-18 03:38:57.920078: Yayy! New best EMA pseudo Dice: 0.7029
2024-12-18 03:38:59.845584: 
2024-12-18 03:38:59.846948: Epoch 11
2024-12-18 03:38:59.847798: Current learning rate: 0.00934
2024-12-18 03:46:08.620745: Validation loss improved from -0.52958 to -0.55215! Patience: 0/50
2024-12-18 03:46:08.621763: train_loss -0.632
2024-12-18 03:46:08.622755: val_loss -0.5521
2024-12-18 03:46:08.623761: Pseudo dice [0.7487]
2024-12-18 03:46:08.624627: Epoch time: 428.78 s
2024-12-18 03:46:08.625490: Yayy! New best EMA pseudo Dice: 0.7075
2024-12-18 03:46:10.515146: 
2024-12-18 03:46:10.516462: Epoch 12
2024-12-18 03:46:10.517344: Current learning rate: 0.00928
2024-12-18 03:52:50.779093: Validation loss did not improve from -0.55215. Patience: 1/50
2024-12-18 03:52:50.780064: train_loss -0.6431
2024-12-18 03:52:50.780830: val_loss -0.5328
2024-12-18 03:52:50.781559: Pseudo dice [0.7429]
2024-12-18 03:52:50.782330: Epoch time: 400.27 s
2024-12-18 03:52:50.782980: Yayy! New best EMA pseudo Dice: 0.711
2024-12-18 03:52:52.832577: 
2024-12-18 03:52:52.833884: Epoch 13
2024-12-18 03:52:52.834586: Current learning rate: 0.00922
2024-12-18 03:59:22.333512: Validation loss did not improve from -0.55215. Patience: 2/50
2024-12-18 03:59:22.334500: train_loss -0.6441
2024-12-18 03:59:22.335208: val_loss -0.507
2024-12-18 03:59:22.335911: Pseudo dice [0.7211]
2024-12-18 03:59:22.336616: Epoch time: 389.5 s
2024-12-18 03:59:22.337332: Yayy! New best EMA pseudo Dice: 0.712
2024-12-18 03:59:24.270632: 
2024-12-18 03:59:24.271986: Epoch 14
2024-12-18 03:59:24.272884: Current learning rate: 0.00916
2024-12-18 04:06:31.835247: Validation loss did not improve from -0.55215. Patience: 3/50
2024-12-18 04:06:31.836951: train_loss -0.6461
2024-12-18 04:06:31.837925: val_loss -0.5296
2024-12-18 04:06:31.838681: Pseudo dice [0.7299]
2024-12-18 04:06:31.839690: Epoch time: 427.57 s
2024-12-18 04:06:32.249186: Yayy! New best EMA pseudo Dice: 0.7138
2024-12-18 04:06:34.111221: 
2024-12-18 04:06:34.112490: Epoch 15
2024-12-18 04:06:34.113376: Current learning rate: 0.0091
2024-12-18 04:13:15.625398: Validation loss did not improve from -0.55215. Patience: 4/50
2024-12-18 04:13:15.626355: train_loss -0.6505
2024-12-18 04:13:15.627061: val_loss -0.4957
2024-12-18 04:13:15.628018: Pseudo dice [0.7249]
2024-12-18 04:13:15.628700: Epoch time: 401.52 s
2024-12-18 04:13:15.629434: Yayy! New best EMA pseudo Dice: 0.7149
2024-12-18 04:13:17.437977: 
2024-12-18 04:13:17.439329: Epoch 16
2024-12-18 04:13:17.440234: Current learning rate: 0.00903
2024-12-18 04:20:20.772754: Validation loss did not improve from -0.55215. Patience: 5/50
2024-12-18 04:20:20.773739: train_loss -0.6433
2024-12-18 04:20:20.774461: val_loss -0.5457
2024-12-18 04:20:20.775123: Pseudo dice [0.7457]
2024-12-18 04:20:20.775788: Epoch time: 423.34 s
2024-12-18 04:20:20.776477: Yayy! New best EMA pseudo Dice: 0.718
2024-12-18 04:20:22.734244: 
2024-12-18 04:20:22.735418: Epoch 17
2024-12-18 04:20:22.736261: Current learning rate: 0.00897
2024-12-18 04:27:20.927358: Validation loss did not improve from -0.55215. Patience: 6/50
2024-12-18 04:27:20.928115: train_loss -0.6527
2024-12-18 04:27:20.928912: val_loss -0.5236
2024-12-18 04:27:20.929602: Pseudo dice [0.7302]
2024-12-18 04:27:20.930391: Epoch time: 418.2 s
2024-12-18 04:27:20.931056: Yayy! New best EMA pseudo Dice: 0.7192
2024-12-18 04:27:23.932114: 
2024-12-18 04:27:23.933303: Epoch 18
2024-12-18 04:27:23.934062: Current learning rate: 0.00891
2024-12-18 04:34:19.262193: Validation loss did not improve from -0.55215. Patience: 7/50
2024-12-18 04:34:19.266281: train_loss -0.665
2024-12-18 04:34:19.267507: val_loss -0.5515
2024-12-18 04:34:19.268224: Pseudo dice [0.75]
2024-12-18 04:34:19.268933: Epoch time: 415.33 s
2024-12-18 04:34:19.269655: Yayy! New best EMA pseudo Dice: 0.7223
2024-12-18 04:34:21.309112: 
2024-12-18 04:34:21.310562: Epoch 19
2024-12-18 04:34:21.311721: Current learning rate: 0.00885
2024-12-18 04:41:17.118813: Validation loss improved from -0.55215 to -0.55798! Patience: 7/50
2024-12-18 04:41:17.120392: train_loss -0.6611
2024-12-18 04:41:17.121341: val_loss -0.558
2024-12-18 04:41:17.122061: Pseudo dice [0.7564]
2024-12-18 04:41:17.122821: Epoch time: 415.81 s
2024-12-18 04:41:17.526330: Yayy! New best EMA pseudo Dice: 0.7257
2024-12-18 04:41:19.510363: 
2024-12-18 04:41:19.511487: Epoch 20
2024-12-18 04:41:19.512288: Current learning rate: 0.00879
2024-12-18 04:48:13.143501: Validation loss did not improve from -0.55798. Patience: 1/50
2024-12-18 04:48:13.144579: train_loss -0.6701
2024-12-18 04:48:13.145385: val_loss -0.557
2024-12-18 04:48:13.146209: Pseudo dice [0.759]
2024-12-18 04:48:13.147299: Epoch time: 413.64 s
2024-12-18 04:48:13.148334: Yayy! New best EMA pseudo Dice: 0.729
2024-12-18 04:48:15.102570: 
2024-12-18 04:48:15.103805: Epoch 21
2024-12-18 04:48:15.104551: Current learning rate: 0.00873
2024-12-18 04:55:23.601084: Validation loss did not improve from -0.55798. Patience: 2/50
2024-12-18 04:55:23.602371: train_loss -0.6696
2024-12-18 04:55:23.603235: val_loss -0.5276
2024-12-18 04:55:23.603933: Pseudo dice [0.7411]
2024-12-18 04:55:23.604659: Epoch time: 428.5 s
2024-12-18 04:55:23.605480: Yayy! New best EMA pseudo Dice: 0.7302
2024-12-18 04:55:25.448570: 
2024-12-18 04:55:25.449847: Epoch 22
2024-12-18 04:55:25.450569: Current learning rate: 0.00867
2024-12-18 05:02:39.207527: Validation loss did not improve from -0.55798. Patience: 3/50
2024-12-18 05:02:39.208534: train_loss -0.6671
2024-12-18 05:02:39.209635: val_loss -0.5311
2024-12-18 05:02:39.210603: Pseudo dice [0.7398]
2024-12-18 05:02:39.211756: Epoch time: 433.76 s
2024-12-18 05:02:39.212877: Yayy! New best EMA pseudo Dice: 0.7312
2024-12-18 05:02:41.175139: 
2024-12-18 05:02:41.176581: Epoch 23
2024-12-18 05:02:41.177490: Current learning rate: 0.00861
2024-12-18 05:09:57.202735: Validation loss did not improve from -0.55798. Patience: 4/50
2024-12-18 05:09:57.203698: train_loss -0.679
2024-12-18 05:09:57.204568: val_loss -0.5284
2024-12-18 05:09:57.205407: Pseudo dice [0.7401]
2024-12-18 05:09:57.206147: Epoch time: 436.03 s
2024-12-18 05:09:57.206900: Yayy! New best EMA pseudo Dice: 0.7321
2024-12-18 05:09:59.037395: 
2024-12-18 05:09:59.038880: Epoch 24
2024-12-18 05:09:59.039925: Current learning rate: 0.00855
2024-12-18 05:16:42.329740: Validation loss did not improve from -0.55798. Patience: 5/50
2024-12-18 05:16:42.330738: train_loss -0.6829
2024-12-18 05:16:42.331545: val_loss -0.5406
2024-12-18 05:16:42.332233: Pseudo dice [0.7472]
2024-12-18 05:16:42.332976: Epoch time: 403.29 s
2024-12-18 05:16:42.746485: Yayy! New best EMA pseudo Dice: 0.7336
2024-12-18 05:16:44.659342: 
2024-12-18 05:16:44.660835: Epoch 25
2024-12-18 05:16:44.661663: Current learning rate: 0.00849
2024-12-18 05:23:11.439699: Validation loss improved from -0.55798 to -0.55922! Patience: 5/50
2024-12-18 05:23:11.440661: train_loss -0.6839
2024-12-18 05:23:11.441512: val_loss -0.5592
2024-12-18 05:23:11.442435: Pseudo dice [0.7534]
2024-12-18 05:23:11.443209: Epoch time: 386.78 s
2024-12-18 05:23:11.444007: Yayy! New best EMA pseudo Dice: 0.7356
2024-12-18 05:23:13.312519: 
2024-12-18 05:23:13.313764: Epoch 26
2024-12-18 05:23:13.314551: Current learning rate: 0.00843
2024-12-18 05:29:40.647204: Validation loss improved from -0.55922 to -0.58277! Patience: 0/50
2024-12-18 05:29:40.648155: train_loss -0.6883
2024-12-18 05:29:40.648916: val_loss -0.5828
2024-12-18 05:29:40.649621: Pseudo dice [0.7715]
2024-12-18 05:29:40.650443: Epoch time: 387.34 s
2024-12-18 05:29:40.651174: Yayy! New best EMA pseudo Dice: 0.7392
2024-12-18 05:29:42.665132: 
2024-12-18 05:29:42.666570: Epoch 27
2024-12-18 05:29:42.667454: Current learning rate: 0.00836
2024-12-18 05:36:26.415738: Validation loss did not improve from -0.58277. Patience: 1/50
2024-12-18 05:36:26.417970: train_loss -0.6961
2024-12-18 05:36:26.419228: val_loss -0.5656
2024-12-18 05:36:26.420035: Pseudo dice [0.76]
2024-12-18 05:36:26.420860: Epoch time: 403.75 s
2024-12-18 05:36:26.421601: Yayy! New best EMA pseudo Dice: 0.7413
2024-12-18 05:36:28.340114: 
2024-12-18 05:36:28.341113: Epoch 28
2024-12-18 05:36:28.341963: Current learning rate: 0.0083
2024-12-18 05:43:23.206785: Validation loss did not improve from -0.58277. Patience: 2/50
2024-12-18 05:43:23.207868: train_loss -0.6952
2024-12-18 05:43:23.208595: val_loss -0.5688
2024-12-18 05:43:23.209309: Pseudo dice [0.7523]
2024-12-18 05:43:23.210101: Epoch time: 414.87 s
2024-12-18 05:43:23.210815: Yayy! New best EMA pseudo Dice: 0.7424
2024-12-18 05:43:25.466579: 
2024-12-18 05:43:25.467865: Epoch 29
2024-12-18 05:43:25.468666: Current learning rate: 0.00824
2024-12-18 05:50:35.226736: Validation loss did not improve from -0.58277. Patience: 3/50
2024-12-18 05:50:35.227714: train_loss -0.6947
2024-12-18 05:50:35.228483: val_loss -0.5401
2024-12-18 05:50:35.229146: Pseudo dice [0.7486]
2024-12-18 05:50:35.229907: Epoch time: 429.76 s
2024-12-18 05:50:35.604352: Yayy! New best EMA pseudo Dice: 0.743
2024-12-18 05:50:37.561589: 
2024-12-18 05:50:37.562963: Epoch 30
2024-12-18 05:50:37.563916: Current learning rate: 0.00818
2024-12-18 05:57:31.530740: Validation loss did not improve from -0.58277. Patience: 4/50
2024-12-18 05:57:31.531782: train_loss -0.6997
2024-12-18 05:57:31.532748: val_loss -0.5489
2024-12-18 05:57:31.533718: Pseudo dice [0.7432]
2024-12-18 05:57:31.534717: Epoch time: 413.97 s
2024-12-18 05:57:31.535658: Yayy! New best EMA pseudo Dice: 0.743
2024-12-18 05:57:33.526522: 
2024-12-18 05:57:33.527993: Epoch 31
2024-12-18 05:57:33.529011: Current learning rate: 0.00812
2024-12-18 06:04:15.374310: Validation loss did not improve from -0.58277. Patience: 5/50
2024-12-18 06:04:15.375249: train_loss -0.6976
2024-12-18 06:04:15.376044: val_loss -0.5506
2024-12-18 06:04:15.376772: Pseudo dice [0.7563]
2024-12-18 06:04:15.377470: Epoch time: 401.85 s
2024-12-18 06:04:15.378055: Yayy! New best EMA pseudo Dice: 0.7443
2024-12-18 06:04:17.324609: 
2024-12-18 06:04:17.325845: Epoch 32
2024-12-18 06:04:17.326614: Current learning rate: 0.00806
2024-12-18 06:10:46.178905: Validation loss did not improve from -0.58277. Patience: 6/50
2024-12-18 06:10:46.179956: train_loss -0.7
2024-12-18 06:10:46.180823: val_loss -0.5653
2024-12-18 06:10:46.181647: Pseudo dice [0.7612]
2024-12-18 06:10:46.182478: Epoch time: 388.86 s
2024-12-18 06:10:46.183190: Yayy! New best EMA pseudo Dice: 0.746
2024-12-18 06:10:48.177865: 
2024-12-18 06:10:48.179155: Epoch 33
2024-12-18 06:10:48.179974: Current learning rate: 0.008
2024-12-18 06:17:53.807004: Validation loss did not improve from -0.58277. Patience: 7/50
2024-12-18 06:17:53.808276: train_loss -0.7006
2024-12-18 06:17:53.809128: val_loss -0.561
2024-12-18 06:17:53.809972: Pseudo dice [0.7629]
2024-12-18 06:17:53.810978: Epoch time: 425.63 s
2024-12-18 06:17:53.811930: Yayy! New best EMA pseudo Dice: 0.7477
2024-12-18 06:17:55.808675: 
2024-12-18 06:17:55.809920: Epoch 34
2024-12-18 06:17:55.810791: Current learning rate: 0.00793
2024-12-18 06:25:02.643052: Validation loss did not improve from -0.58277. Patience: 8/50
2024-12-18 06:25:02.644148: train_loss -0.7112
2024-12-18 06:25:02.645168: val_loss -0.5747
2024-12-18 06:25:02.645947: Pseudo dice [0.767]
2024-12-18 06:25:02.646567: Epoch time: 426.84 s
2024-12-18 06:25:03.056458: Yayy! New best EMA pseudo Dice: 0.7496
2024-12-18 06:25:04.928813: 
2024-12-18 06:25:04.930181: Epoch 35
2024-12-18 06:25:04.930968: Current learning rate: 0.00787
2024-12-18 06:32:24.411439: Validation loss did not improve from -0.58277. Patience: 9/50
2024-12-18 06:32:24.413180: train_loss -0.716
2024-12-18 06:32:24.414231: val_loss -0.5658
2024-12-18 06:32:24.415109: Pseudo dice [0.7522]
2024-12-18 06:32:24.415932: Epoch time: 439.49 s
2024-12-18 06:32:24.416738: Yayy! New best EMA pseudo Dice: 0.7499
2024-12-18 06:32:26.353636: 
2024-12-18 06:32:26.354820: Epoch 36
2024-12-18 06:32:26.355552: Current learning rate: 0.00781
2024-12-18 06:39:36.331034: Validation loss did not improve from -0.58277. Patience: 10/50
2024-12-18 06:39:36.332033: train_loss -0.7015
2024-12-18 06:39:36.332809: val_loss -0.5351
2024-12-18 06:39:36.333507: Pseudo dice [0.7398]
2024-12-18 06:39:36.334210: Epoch time: 429.98 s
2024-12-18 06:39:37.869902: 
2024-12-18 06:39:37.871225: Epoch 37
2024-12-18 06:39:37.872190: Current learning rate: 0.00775
2024-12-18 06:46:48.814771: Validation loss did not improve from -0.58277. Patience: 11/50
2024-12-18 06:46:48.816192: train_loss -0.7104
2024-12-18 06:46:48.816993: val_loss -0.5458
2024-12-18 06:46:48.817656: Pseudo dice [0.7438]
2024-12-18 06:46:48.818363: Epoch time: 430.95 s
2024-12-18 06:46:50.280648: 
2024-12-18 06:46:50.282000: Epoch 38
2024-12-18 06:46:50.282939: Current learning rate: 0.00769
2024-12-18 06:53:36.650430: Validation loss did not improve from -0.58277. Patience: 12/50
2024-12-18 06:53:36.651455: train_loss -0.7138
2024-12-18 06:53:36.652623: val_loss -0.5753
2024-12-18 06:53:36.653582: Pseudo dice [0.7695]
2024-12-18 06:53:36.654531: Epoch time: 406.37 s
2024-12-18 06:53:36.655382: Yayy! New best EMA pseudo Dice: 0.7505
2024-12-18 06:53:39.071398: 
2024-12-18 06:53:39.072778: Epoch 39
2024-12-18 06:53:39.073536: Current learning rate: 0.00763
2024-12-18 07:00:30.056095: Validation loss did not improve from -0.58277. Patience: 13/50
2024-12-18 07:00:30.057125: train_loss -0.7174
2024-12-18 07:00:30.057907: val_loss -0.5452
2024-12-18 07:00:30.058919: Pseudo dice [0.7473]
2024-12-18 07:00:30.059765: Epoch time: 410.99 s
2024-12-18 07:00:31.889784: 
2024-12-18 07:00:31.891180: Epoch 40
2024-12-18 07:00:31.892023: Current learning rate: 0.00756
2024-12-18 07:07:12.815091: Validation loss did not improve from -0.58277. Patience: 14/50
2024-12-18 07:07:12.816223: train_loss -0.7184
2024-12-18 07:07:12.817344: val_loss -0.556
2024-12-18 07:07:12.818238: Pseudo dice [0.7603]
2024-12-18 07:07:12.819186: Epoch time: 400.93 s
2024-12-18 07:07:12.820122: Yayy! New best EMA pseudo Dice: 0.7512
2024-12-18 07:07:14.816863: 
2024-12-18 07:07:14.818088: Epoch 41
2024-12-18 07:07:14.818835: Current learning rate: 0.0075
2024-12-18 07:14:24.418560: Validation loss did not improve from -0.58277. Patience: 15/50
2024-12-18 07:14:24.419640: train_loss -0.7232
2024-12-18 07:14:24.420683: val_loss -0.5547
2024-12-18 07:14:24.421679: Pseudo dice [0.7562]
2024-12-18 07:14:24.422599: Epoch time: 429.6 s
2024-12-18 07:14:24.423335: Yayy! New best EMA pseudo Dice: 0.7517
2024-12-18 07:14:26.211216: 
2024-12-18 07:14:26.212603: Epoch 42
2024-12-18 07:14:26.213490: Current learning rate: 0.00744
2024-12-18 07:21:12.883291: Validation loss did not improve from -0.58277. Patience: 16/50
2024-12-18 07:21:12.884312: train_loss -0.7225
2024-12-18 07:21:12.885279: val_loss -0.5456
2024-12-18 07:21:12.885949: Pseudo dice [0.7453]
2024-12-18 07:21:12.886739: Epoch time: 406.67 s
2024-12-18 07:21:14.364314: 
2024-12-18 07:21:14.365579: Epoch 43
2024-12-18 07:21:14.366379: Current learning rate: 0.00738
2024-12-18 07:28:35.927729: Validation loss did not improve from -0.58277. Patience: 17/50
2024-12-18 07:28:35.928790: train_loss -0.7255
2024-12-18 07:28:35.929642: val_loss -0.5414
2024-12-18 07:28:35.930444: Pseudo dice [0.7488]
2024-12-18 07:28:35.931206: Epoch time: 441.57 s
2024-12-18 07:28:37.292828: 
2024-12-18 07:28:37.294194: Epoch 44
2024-12-18 07:28:37.295027: Current learning rate: 0.00732
2024-12-18 07:35:28.144163: Validation loss did not improve from -0.58277. Patience: 18/50
2024-12-18 07:35:28.145056: train_loss -0.7254
2024-12-18 07:35:28.145923: val_loss -0.5691
2024-12-18 07:35:28.146713: Pseudo dice [0.7592]
2024-12-18 07:35:28.147542: Epoch time: 410.85 s
2024-12-18 07:35:29.974153: 
2024-12-18 07:35:29.975498: Epoch 45
2024-12-18 07:35:29.976329: Current learning rate: 0.00725
2024-12-18 07:42:03.969554: Validation loss did not improve from -0.58277. Patience: 19/50
2024-12-18 07:42:03.970519: train_loss -0.7321
2024-12-18 07:42:03.971362: val_loss -0.5683
2024-12-18 07:42:03.972190: Pseudo dice [0.7599]
2024-12-18 07:42:03.972989: Epoch time: 394.0 s
2024-12-18 07:42:03.973669: Yayy! New best EMA pseudo Dice: 0.7525
2024-12-18 07:42:05.723475: 
2024-12-18 07:42:05.724578: Epoch 46
2024-12-18 07:42:05.725306: Current learning rate: 0.00719
2024-12-18 07:49:12.190132: Validation loss did not improve from -0.58277. Patience: 20/50
2024-12-18 07:49:12.193310: train_loss -0.7319
2024-12-18 07:49:12.194319: val_loss -0.5447
2024-12-18 07:49:12.194881: Pseudo dice [0.7461]
2024-12-18 07:49:12.195501: Epoch time: 426.47 s
2024-12-18 07:49:13.569309: 
2024-12-18 07:49:13.571201: Epoch 47
2024-12-18 07:49:13.572114: Current learning rate: 0.00713
2024-12-18 07:56:02.898585: Validation loss did not improve from -0.58277. Patience: 21/50
2024-12-18 07:56:02.900051: train_loss -0.7374
2024-12-18 07:56:02.901419: val_loss -0.5752
2024-12-18 07:56:02.902122: Pseudo dice [0.7651]
2024-12-18 07:56:02.902966: Epoch time: 409.33 s
2024-12-18 07:56:02.903598: Yayy! New best EMA pseudo Dice: 0.7532
2024-12-18 07:56:04.694409: 
2024-12-18 07:56:04.695630: Epoch 48
2024-12-18 07:56:04.696455: Current learning rate: 0.00707
2024-12-18 08:03:16.710610: Validation loss did not improve from -0.58277. Patience: 22/50
2024-12-18 08:03:16.711625: train_loss -0.7365
2024-12-18 08:03:16.712619: val_loss -0.5463
2024-12-18 08:03:16.713454: Pseudo dice [0.7477]
2024-12-18 08:03:16.714268: Epoch time: 432.02 s
2024-12-18 08:03:18.183761: 
2024-12-18 08:03:18.184894: Epoch 49
2024-12-18 08:03:18.185539: Current learning rate: 0.007
2024-12-18 08:10:23.942906: Validation loss did not improve from -0.58277. Patience: 23/50
2024-12-18 08:10:23.943958: train_loss -0.7374
2024-12-18 08:10:23.944714: val_loss -0.5559
2024-12-18 08:10:23.945553: Pseudo dice [0.7559]
2024-12-18 08:10:23.946252: Epoch time: 425.76 s
2024-12-18 08:10:26.321709: 
2024-12-18 08:10:26.322928: Epoch 50
2024-12-18 08:10:26.323703: Current learning rate: 0.00694
2024-12-18 08:17:56.461197: Validation loss did not improve from -0.58277. Patience: 24/50
2024-12-18 08:17:56.461978: train_loss -0.741
2024-12-18 08:17:56.462889: val_loss -0.5538
2024-12-18 08:17:56.463951: Pseudo dice [0.7484]
2024-12-18 08:17:56.464843: Epoch time: 450.14 s
2024-12-18 08:17:57.863686: 
2024-12-18 08:17:57.864713: Epoch 51
2024-12-18 08:17:57.865336: Current learning rate: 0.00688
2024-12-18 08:24:30.197007: Validation loss did not improve from -0.58277. Patience: 25/50
2024-12-18 08:24:30.197970: train_loss -0.7369
2024-12-18 08:24:30.198738: val_loss -0.5326
2024-12-18 08:24:30.199400: Pseudo dice [0.7408]
2024-12-18 08:24:30.200066: Epoch time: 392.34 s
2024-12-18 08:24:31.541181: 
2024-12-18 08:24:31.542267: Epoch 52
2024-12-18 08:24:31.543118: Current learning rate: 0.00682
2024-12-18 08:31:22.825842: Validation loss did not improve from -0.58277. Patience: 26/50
2024-12-18 08:31:22.826937: train_loss -0.7328
2024-12-18 08:31:22.827816: val_loss -0.5417
2024-12-18 08:31:22.828595: Pseudo dice [0.7409]
2024-12-18 08:31:22.829307: Epoch time: 411.29 s
2024-12-18 08:31:24.334800: 
2024-12-18 08:31:24.336170: Epoch 53
2024-12-18 08:31:24.336989: Current learning rate: 0.00675
2024-12-18 08:38:31.890287: Validation loss did not improve from -0.58277. Patience: 27/50
2024-12-18 08:38:31.892565: train_loss -0.7406
2024-12-18 08:38:31.893981: val_loss -0.5503
2024-12-18 08:38:31.895025: Pseudo dice [0.7537]
2024-12-18 08:38:31.896161: Epoch time: 427.56 s
2024-12-18 08:38:33.497608: 
2024-12-18 08:38:33.498862: Epoch 54
2024-12-18 08:38:33.499975: Current learning rate: 0.00669
2024-12-18 08:45:46.873403: Validation loss did not improve from -0.58277. Patience: 28/50
2024-12-18 08:45:46.874371: train_loss -0.7457
2024-12-18 08:45:46.875115: val_loss -0.5564
2024-12-18 08:45:46.875840: Pseudo dice [0.7509]
2024-12-18 08:45:46.876514: Epoch time: 433.38 s
2024-12-18 08:45:48.705784: 
2024-12-18 08:45:48.707249: Epoch 55
2024-12-18 08:45:48.708047: Current learning rate: 0.00663
2024-12-18 08:52:55.616744: Validation loss did not improve from -0.58277. Patience: 29/50
2024-12-18 08:52:55.618617: train_loss -0.7452
2024-12-18 08:52:55.619332: val_loss -0.5495
2024-12-18 08:52:55.619978: Pseudo dice [0.7542]
2024-12-18 08:52:55.620577: Epoch time: 426.91 s
2024-12-18 08:52:57.072920: 
2024-12-18 08:52:57.074253: Epoch 56
2024-12-18 08:52:57.075089: Current learning rate: 0.00657
2024-12-18 08:59:59.161066: Validation loss did not improve from -0.58277. Patience: 30/50
2024-12-18 08:59:59.162903: train_loss -0.7479
2024-12-18 08:59:59.164328: val_loss -0.5571
2024-12-18 08:59:59.164933: Pseudo dice [0.7606]
2024-12-18 08:59:59.165793: Epoch time: 422.09 s
2024-12-18 09:00:00.625088: 
2024-12-18 09:00:00.626439: Epoch 57
2024-12-18 09:00:00.627260: Current learning rate: 0.0065
2024-12-18 09:06:31.465292: Validation loss did not improve from -0.58277. Patience: 31/50
2024-12-18 09:06:31.466692: train_loss -0.7485
2024-12-18 09:06:31.467918: val_loss -0.5744
2024-12-18 09:06:31.468706: Pseudo dice [0.7641]
2024-12-18 09:06:31.469412: Epoch time: 390.84 s
2024-12-18 09:06:31.470137: Yayy! New best EMA pseudo Dice: 0.7532
2024-12-18 09:06:33.448212: 
2024-12-18 09:06:33.449631: Epoch 58
2024-12-18 09:06:33.450591: Current learning rate: 0.00644
2024-12-18 09:13:11.959395: Validation loss did not improve from -0.58277. Patience: 32/50
2024-12-18 09:13:11.960304: train_loss -0.749
2024-12-18 09:13:11.961129: val_loss -0.5787
2024-12-18 09:13:11.961733: Pseudo dice [0.7694]
2024-12-18 09:13:11.962331: Epoch time: 398.51 s
2024-12-18 09:13:11.962945: Yayy! New best EMA pseudo Dice: 0.7548
2024-12-18 09:13:13.862952: 
2024-12-18 09:13:13.864194: Epoch 59
2024-12-18 09:13:13.864922: Current learning rate: 0.00638
2024-12-18 09:20:51.154222: Validation loss did not improve from -0.58277. Patience: 33/50
2024-12-18 09:20:51.155947: train_loss -0.7495
2024-12-18 09:20:51.156847: val_loss -0.5659
2024-12-18 09:20:51.157702: Pseudo dice [0.7592]
2024-12-18 09:20:51.158483: Epoch time: 457.29 s
2024-12-18 09:20:51.545182: Yayy! New best EMA pseudo Dice: 0.7552
2024-12-18 09:20:53.460536: 
2024-12-18 09:20:53.461931: Epoch 60
2024-12-18 09:20:53.462910: Current learning rate: 0.00631
2024-12-18 09:28:05.686836: Validation loss did not improve from -0.58277. Patience: 34/50
2024-12-18 09:28:05.688330: train_loss -0.7452
2024-12-18 09:28:05.689495: val_loss -0.5357
2024-12-18 09:28:05.690443: Pseudo dice [0.7398]
2024-12-18 09:28:05.691557: Epoch time: 432.23 s
2024-12-18 09:28:07.154528: 
2024-12-18 09:28:07.155714: Epoch 61
2024-12-18 09:28:07.156355: Current learning rate: 0.00625
2024-12-18 09:34:29.217392: Validation loss improved from -0.58277 to -0.58399! Patience: 34/50
2024-12-18 09:34:29.218337: train_loss -0.7495
2024-12-18 09:34:29.219180: val_loss -0.584
2024-12-18 09:34:29.219898: Pseudo dice [0.7614]
2024-12-18 09:34:29.220731: Epoch time: 382.07 s
2024-12-18 09:34:31.205595: 
2024-12-18 09:34:31.207071: Epoch 62
2024-12-18 09:34:31.207898: Current learning rate: 0.00619
2024-12-18 09:40:27.798810: Validation loss did not improve from -0.58399. Patience: 1/50
2024-12-18 09:40:27.799880: train_loss -0.7517
2024-12-18 09:40:27.800874: val_loss -0.5535
2024-12-18 09:40:27.802029: Pseudo dice [0.7629]
2024-12-18 09:40:27.803100: Epoch time: 356.6 s
2024-12-18 09:40:27.804178: Yayy! New best EMA pseudo Dice: 0.7553
2024-12-18 09:40:29.786631: 
2024-12-18 09:40:29.787760: Epoch 63
2024-12-18 09:40:29.788756: Current learning rate: 0.00612
2024-12-18 09:46:35.591199: Validation loss did not improve from -0.58399. Patience: 2/50
2024-12-18 09:46:35.592063: train_loss -0.7565
2024-12-18 09:46:35.592801: val_loss -0.5832
2024-12-18 09:46:35.593409: Pseudo dice [0.7695]
2024-12-18 09:46:35.594165: Epoch time: 365.81 s
2024-12-18 09:46:35.594815: Yayy! New best EMA pseudo Dice: 0.7567
2024-12-18 09:46:37.523837: 
2024-12-18 09:46:37.525059: Epoch 64
2024-12-18 09:46:37.525899: Current learning rate: 0.00606
2024-12-18 09:53:14.096071: Validation loss did not improve from -0.58399. Patience: 3/50
2024-12-18 09:53:14.097071: train_loss -0.7586
2024-12-18 09:53:14.097960: val_loss -0.5494
2024-12-18 09:53:14.098612: Pseudo dice [0.7485]
2024-12-18 09:53:14.099310: Epoch time: 396.57 s
2024-12-18 09:53:16.094848: 
2024-12-18 09:53:16.096214: Epoch 65
2024-12-18 09:53:16.097060: Current learning rate: 0.006
2024-12-18 09:59:44.805240: Validation loss did not improve from -0.58399. Patience: 4/50
2024-12-18 09:59:44.820225: train_loss -0.7518
2024-12-18 09:59:44.821052: val_loss -0.5571
2024-12-18 09:59:44.821760: Pseudo dice [0.7548]
2024-12-18 09:59:44.822401: Epoch time: 388.73 s
2024-12-18 09:59:46.326389: 
2024-12-18 09:59:46.327349: Epoch 66
2024-12-18 09:59:46.328019: Current learning rate: 0.00593
2024-12-18 10:06:01.778315: Validation loss did not improve from -0.58399. Patience: 5/50
2024-12-18 10:06:01.779200: train_loss -0.7558
2024-12-18 10:06:01.780325: val_loss -0.5586
2024-12-18 10:06:01.781133: Pseudo dice [0.7545]
2024-12-18 10:06:01.781965: Epoch time: 375.45 s
2024-12-18 10:06:03.266644: 
2024-12-18 10:06:03.267960: Epoch 67
2024-12-18 10:06:03.269063: Current learning rate: 0.00587
2024-12-18 10:11:48.282633: Validation loss did not improve from -0.58399. Patience: 6/50
2024-12-18 10:11:48.292748: train_loss -0.7616
2024-12-18 10:11:48.301561: val_loss -0.5601
2024-12-18 10:11:48.302690: Pseudo dice [0.7649]
2024-12-18 10:11:48.303695: Epoch time: 345.02 s
2024-12-18 10:11:50.044360: 
2024-12-18 10:11:50.045548: Epoch 68
2024-12-18 10:11:50.046363: Current learning rate: 0.00581
2024-12-18 10:17:50.128500: Validation loss did not improve from -0.58399. Patience: 7/50
2024-12-18 10:17:50.129458: train_loss -0.7568
2024-12-18 10:17:50.130336: val_loss -0.5673
2024-12-18 10:17:50.131113: Pseudo dice [0.7584]
2024-12-18 10:17:50.132038: Epoch time: 360.09 s
2024-12-18 10:17:50.132853: Yayy! New best EMA pseudo Dice: 0.7568
2024-12-18 10:17:52.062536: 
2024-12-18 10:17:52.063691: Epoch 69
2024-12-18 10:17:52.064487: Current learning rate: 0.00574
2024-12-18 10:24:01.306982: Validation loss did not improve from -0.58399. Patience: 8/50
2024-12-18 10:24:01.308005: train_loss -0.7544
2024-12-18 10:24:01.308745: val_loss -0.5446
2024-12-18 10:24:01.309453: Pseudo dice [0.7595]
2024-12-18 10:24:01.310115: Epoch time: 369.25 s
2024-12-18 10:24:01.784227: Yayy! New best EMA pseudo Dice: 0.757
2024-12-18 10:24:03.645244: 
2024-12-18 10:24:03.646373: Epoch 70
2024-12-18 10:24:03.647190: Current learning rate: 0.00568
2024-12-18 10:30:06.155471: Validation loss did not improve from -0.58399. Patience: 9/50
2024-12-18 10:30:06.156493: train_loss -0.7586
2024-12-18 10:30:06.157382: val_loss -0.5677
2024-12-18 10:30:06.158155: Pseudo dice [0.7611]
2024-12-18 10:30:06.158931: Epoch time: 362.51 s
2024-12-18 10:30:06.159767: Yayy! New best EMA pseudo Dice: 0.7574
2024-12-18 10:30:08.053799: 
2024-12-18 10:30:08.054993: Epoch 71
2024-12-18 10:30:08.055842: Current learning rate: 0.00562
2024-12-18 10:36:17.942355: Validation loss improved from -0.58399 to -0.59207! Patience: 9/50
2024-12-18 10:36:17.943035: train_loss -0.7626
2024-12-18 10:36:17.943886: val_loss -0.5921
2024-12-18 10:36:17.944640: Pseudo dice [0.7714]
2024-12-18 10:36:17.945423: Epoch time: 369.89 s
2024-12-18 10:36:17.946360: Yayy! New best EMA pseudo Dice: 0.7588
2024-12-18 10:36:20.651807: 
2024-12-18 10:36:20.653007: Epoch 72
2024-12-18 10:36:20.654098: Current learning rate: 0.00555
2024-12-18 10:42:14.340281: Validation loss did not improve from -0.59207. Patience: 1/50
2024-12-18 10:42:14.341075: train_loss -0.768
2024-12-18 10:42:14.341774: val_loss -0.5449
2024-12-18 10:42:14.342547: Pseudo dice [0.7552]
2024-12-18 10:42:14.343262: Epoch time: 353.69 s
2024-12-18 10:42:15.805251: 
2024-12-18 10:42:15.806598: Epoch 73
2024-12-18 10:42:15.807740: Current learning rate: 0.00549
2024-12-18 10:48:19.097617: Validation loss did not improve from -0.59207. Patience: 2/50
2024-12-18 10:48:19.098491: train_loss -0.759
2024-12-18 10:48:19.099351: val_loss -0.5624
2024-12-18 10:48:19.100109: Pseudo dice [0.7611]
2024-12-18 10:48:19.100825: Epoch time: 363.29 s
2024-12-18 10:48:20.527738: 
2024-12-18 10:48:20.529027: Epoch 74
2024-12-18 10:48:20.529782: Current learning rate: 0.00542
2024-12-18 10:54:37.190791: Validation loss did not improve from -0.59207. Patience: 3/50
2024-12-18 10:54:37.191825: train_loss -0.7673
2024-12-18 10:54:37.192601: val_loss -0.5541
2024-12-18 10:54:37.193308: Pseudo dice [0.7629]
2024-12-18 10:54:37.194009: Epoch time: 376.67 s
2024-12-18 10:54:37.582994: Yayy! New best EMA pseudo Dice: 0.7591
2024-12-18 10:54:39.428722: 
2024-12-18 10:54:39.430192: Epoch 75
2024-12-18 10:54:39.431267: Current learning rate: 0.00536
2024-12-18 10:59:59.496773: Validation loss did not improve from -0.59207. Patience: 4/50
2024-12-18 10:59:59.497633: train_loss -0.7634
2024-12-18 10:59:59.498358: val_loss -0.5666
2024-12-18 10:59:59.499163: Pseudo dice [0.7643]
2024-12-18 10:59:59.499992: Epoch time: 320.07 s
2024-12-18 10:59:59.500623: Yayy! New best EMA pseudo Dice: 0.7597
2024-12-18 11:00:01.358557: 
2024-12-18 11:00:01.359627: Epoch 76
2024-12-18 11:00:01.360415: Current learning rate: 0.00529
2024-12-18 11:05:45.614102: Validation loss did not improve from -0.59207. Patience: 5/50
2024-12-18 11:05:45.615055: train_loss -0.7652
2024-12-18 11:05:45.616096: val_loss -0.5818
2024-12-18 11:05:45.617001: Pseudo dice [0.7645]
2024-12-18 11:05:45.617973: Epoch time: 344.26 s
2024-12-18 11:05:45.618904: Yayy! New best EMA pseudo Dice: 0.7601
2024-12-18 11:05:47.419309: 
2024-12-18 11:05:47.420627: Epoch 77
2024-12-18 11:05:47.421486: Current learning rate: 0.00523
2024-12-18 11:12:10.972909: Validation loss did not improve from -0.59207. Patience: 6/50
2024-12-18 11:12:10.974705: train_loss -0.7674
2024-12-18 11:12:10.975742: val_loss -0.5702
2024-12-18 11:12:10.976396: Pseudo dice [0.7593]
2024-12-18 11:12:10.977132: Epoch time: 383.56 s
2024-12-18 11:12:12.511587: 
2024-12-18 11:12:12.513488: Epoch 78
2024-12-18 11:12:12.514688: Current learning rate: 0.00517
2024-12-18 11:18:42.331427: Validation loss did not improve from -0.59207. Patience: 7/50
2024-12-18 11:18:42.332422: train_loss -0.7698
2024-12-18 11:18:42.333195: val_loss -0.5872
2024-12-18 11:18:42.333860: Pseudo dice [0.7719]
2024-12-18 11:18:42.334507: Epoch time: 389.83 s
2024-12-18 11:18:42.335142: Yayy! New best EMA pseudo Dice: 0.7612
2024-12-18 11:18:44.175010: 
2024-12-18 11:18:44.176664: Epoch 79
2024-12-18 11:18:44.177765: Current learning rate: 0.0051
2024-12-18 11:25:04.074548: Validation loss did not improve from -0.59207. Patience: 8/50
2024-12-18 11:25:04.075541: train_loss -0.7741
2024-12-18 11:25:04.076199: val_loss -0.5848
2024-12-18 11:25:04.076746: Pseudo dice [0.7716]
2024-12-18 11:25:04.077604: Epoch time: 379.9 s
2024-12-18 11:25:04.496941: Yayy! New best EMA pseudo Dice: 0.7623
2024-12-18 11:25:06.305718: 
2024-12-18 11:25:06.306921: Epoch 80
2024-12-18 11:25:06.307667: Current learning rate: 0.00504
2024-12-18 11:31:11.717491: Validation loss did not improve from -0.59207. Patience: 9/50
2024-12-18 11:31:11.718314: train_loss -0.7717
2024-12-18 11:31:11.719556: val_loss -0.5573
2024-12-18 11:31:11.720603: Pseudo dice [0.7507]
2024-12-18 11:31:11.721619: Epoch time: 365.41 s
2024-12-18 11:31:13.137096: 
2024-12-18 11:31:13.138471: Epoch 81
2024-12-18 11:31:13.139439: Current learning rate: 0.00497
2024-12-18 11:37:15.698299: Validation loss did not improve from -0.59207. Patience: 10/50
2024-12-18 11:37:15.699226: train_loss -0.7731
2024-12-18 11:37:15.700198: val_loss -0.5687
2024-12-18 11:37:15.701089: Pseudo dice [0.7675]
2024-12-18 11:37:15.701835: Epoch time: 362.56 s
2024-12-18 11:37:17.112434: 
2024-12-18 11:37:17.113868: Epoch 82
2024-12-18 11:37:17.114901: Current learning rate: 0.00491
2024-12-18 11:43:37.999519: Validation loss did not improve from -0.59207. Patience: 11/50
2024-12-18 11:43:38.000936: train_loss -0.7733
2024-12-18 11:43:38.001612: val_loss -0.5805
2024-12-18 11:43:38.002187: Pseudo dice [0.771]
2024-12-18 11:43:38.003010: Epoch time: 380.89 s
2024-12-18 11:43:38.003813: Yayy! New best EMA pseudo Dice: 0.7627
2024-12-18 11:43:40.350186: 
2024-12-18 11:43:40.351378: Epoch 83
2024-12-18 11:43:40.352031: Current learning rate: 0.00484
2024-12-18 11:49:56.443001: Validation loss did not improve from -0.59207. Patience: 12/50
2024-12-18 11:49:56.443898: train_loss -0.7682
2024-12-18 11:49:56.444673: val_loss -0.5542
2024-12-18 11:49:56.445442: Pseudo dice [0.7605]
2024-12-18 11:49:56.446123: Epoch time: 376.09 s
2024-12-18 11:49:57.852447: 
2024-12-18 11:49:57.853886: Epoch 84
2024-12-18 11:49:57.854770: Current learning rate: 0.00478
2024-12-18 11:56:08.171987: Validation loss did not improve from -0.59207. Patience: 13/50
2024-12-18 11:56:08.173183: train_loss -0.7737
2024-12-18 11:56:08.173985: val_loss -0.5718
2024-12-18 11:56:08.174623: Pseudo dice [0.7741]
2024-12-18 11:56:08.175218: Epoch time: 370.32 s
2024-12-18 11:56:08.610242: Yayy! New best EMA pseudo Dice: 0.7636
2024-12-18 11:56:10.408805: 
2024-12-18 11:56:10.410163: Epoch 85
2024-12-18 11:56:10.411211: Current learning rate: 0.00471
2024-12-18 12:02:42.963205: Validation loss did not improve from -0.59207. Patience: 14/50
2024-12-18 12:02:42.964011: train_loss -0.774
2024-12-18 12:02:42.964754: val_loss -0.5597
2024-12-18 12:02:42.965319: Pseudo dice [0.7681]
2024-12-18 12:02:42.965912: Epoch time: 392.56 s
2024-12-18 12:02:42.966515: Yayy! New best EMA pseudo Dice: 0.7641
2024-12-18 12:02:44.747066: 
2024-12-18 12:02:44.748500: Epoch 86
2024-12-18 12:02:44.749162: Current learning rate: 0.00465
2024-12-18 12:09:07.100089: Validation loss did not improve from -0.59207. Patience: 15/50
2024-12-18 12:09:07.102140: train_loss -0.7755
2024-12-18 12:09:07.102987: val_loss -0.5649
2024-12-18 12:09:07.103645: Pseudo dice [0.7556]
2024-12-18 12:09:07.104295: Epoch time: 382.36 s
2024-12-18 12:09:08.457670: 
2024-12-18 12:09:08.458772: Epoch 87
2024-12-18 12:09:08.459549: Current learning rate: 0.00458
2024-12-18 12:15:38.832834: Validation loss did not improve from -0.59207. Patience: 16/50
2024-12-18 12:15:38.833822: train_loss -0.776
2024-12-18 12:15:38.834826: val_loss -0.5911
2024-12-18 12:15:38.835573: Pseudo dice [0.7742]
2024-12-18 12:15:38.836322: Epoch time: 390.38 s
2024-12-18 12:15:38.837167: Yayy! New best EMA pseudo Dice: 0.7643
2024-12-18 12:15:40.694436: 
2024-12-18 12:15:40.695831: Epoch 88
2024-12-18 12:15:40.696628: Current learning rate: 0.00452
2024-12-18 12:21:55.288692: Validation loss did not improve from -0.59207. Patience: 17/50
2024-12-18 12:21:55.289508: train_loss -0.7745
2024-12-18 12:21:55.290197: val_loss -0.5467
2024-12-18 12:21:55.290921: Pseudo dice [0.7544]
2024-12-18 12:21:55.291661: Epoch time: 374.6 s
2024-12-18 12:21:56.642074: 
2024-12-18 12:21:56.643206: Epoch 89
2024-12-18 12:21:56.643842: Current learning rate: 0.00445
2024-12-18 12:28:19.401266: Validation loss did not improve from -0.59207. Patience: 18/50
2024-12-18 12:28:19.402389: train_loss -0.777
2024-12-18 12:28:19.403127: val_loss -0.5597
2024-12-18 12:28:19.403733: Pseudo dice [0.7636]
2024-12-18 12:28:19.404325: Epoch time: 382.76 s
2024-12-18 12:28:21.291659: 
2024-12-18 12:28:21.293280: Epoch 90
2024-12-18 12:28:21.294157: Current learning rate: 0.00438
2024-12-18 12:34:37.618703: Validation loss did not improve from -0.59207. Patience: 19/50
2024-12-18 12:34:37.620204: train_loss -0.7754
2024-12-18 12:34:37.621055: val_loss -0.5684
2024-12-18 12:34:37.621703: Pseudo dice [0.7623]
2024-12-18 12:34:37.622341: Epoch time: 376.33 s
2024-12-18 12:34:39.048874: 
2024-12-18 12:34:39.049968: Epoch 91
2024-12-18 12:34:39.050782: Current learning rate: 0.00432
2024-12-18 12:40:53.350568: Validation loss did not improve from -0.59207. Patience: 20/50
2024-12-18 12:40:53.351532: train_loss -0.7798
2024-12-18 12:40:53.352205: val_loss -0.5592
2024-12-18 12:40:53.352967: Pseudo dice [0.7569]
2024-12-18 12:40:53.353563: Epoch time: 374.3 s
2024-12-18 12:40:54.690396: 
2024-12-18 12:40:54.691616: Epoch 92
2024-12-18 12:40:54.692288: Current learning rate: 0.00425
2024-12-18 12:47:19.007420: Validation loss did not improve from -0.59207. Patience: 21/50
2024-12-18 12:47:19.009068: train_loss -0.778
2024-12-18 12:47:19.010002: val_loss -0.5628
2024-12-18 12:47:19.010800: Pseudo dice [0.766]
2024-12-18 12:47:19.011695: Epoch time: 384.32 s
2024-12-18 12:47:20.428349: 
2024-12-18 12:47:20.429713: Epoch 93
2024-12-18 12:47:20.430419: Current learning rate: 0.00419
2024-12-18 12:53:58.544545: Validation loss did not improve from -0.59207. Patience: 22/50
2024-12-18 12:53:58.545516: train_loss -0.781
2024-12-18 12:53:58.546308: val_loss -0.5528
2024-12-18 12:53:58.547009: Pseudo dice [0.7557]
2024-12-18 12:53:58.547686: Epoch time: 398.12 s
2024-12-18 12:54:00.550781: 
2024-12-18 12:54:00.551926: Epoch 94
2024-12-18 12:54:00.552740: Current learning rate: 0.00412
2024-12-18 13:00:32.913130: Validation loss did not improve from -0.59207. Patience: 23/50
2024-12-18 13:00:32.914122: train_loss -0.782
2024-12-18 13:00:32.914831: val_loss -0.5661
2024-12-18 13:00:32.915498: Pseudo dice [0.7682]
2024-12-18 13:00:32.916196: Epoch time: 392.36 s
2024-12-18 13:00:34.763026: 
2024-12-18 13:00:34.763986: Epoch 95
2024-12-18 13:00:34.764665: Current learning rate: 0.00405
2024-12-18 13:05:50.721263: Validation loss did not improve from -0.59207. Patience: 24/50
2024-12-18 13:05:50.722169: train_loss -0.7802
2024-12-18 13:05:50.722929: val_loss -0.5576
2024-12-18 13:05:50.723635: Pseudo dice [0.7575]
2024-12-18 13:05:50.724435: Epoch time: 315.96 s
2024-12-18 13:05:52.085363: 
2024-12-18 13:05:52.086470: Epoch 96
2024-12-18 13:05:52.087206: Current learning rate: 0.00399
2024-12-18 13:11:14.402723: Validation loss did not improve from -0.59207. Patience: 25/50
2024-12-18 13:11:14.404575: train_loss -0.7847
2024-12-18 13:11:14.406329: val_loss -0.5591
2024-12-18 13:11:14.407180: Pseudo dice [0.7575]
2024-12-18 13:11:14.408078: Epoch time: 322.32 s
2024-12-18 13:11:15.810201: 
2024-12-18 13:11:15.811343: Epoch 97
2024-12-18 13:11:15.812048: Current learning rate: 0.00392
2024-12-18 13:17:46.136935: Validation loss did not improve from -0.59207. Patience: 26/50
2024-12-18 13:17:46.137887: train_loss -0.785
2024-12-18 13:17:46.138716: val_loss -0.5651
2024-12-18 13:17:46.139432: Pseudo dice [0.7635]
2024-12-18 13:17:46.140204: Epoch time: 390.33 s
2024-12-18 13:17:47.568278: 
2024-12-18 13:17:47.569420: Epoch 98
2024-12-18 13:17:47.570258: Current learning rate: 0.00385
2024-12-18 13:24:26.672023: Validation loss did not improve from -0.59207. Patience: 27/50
2024-12-18 13:24:26.672967: train_loss -0.7859
2024-12-18 13:24:26.673906: val_loss -0.5686
2024-12-18 13:24:26.674680: Pseudo dice [0.7609]
2024-12-18 13:24:26.675562: Epoch time: 399.11 s
2024-12-18 13:24:28.128578: 
2024-12-18 13:24:28.129698: Epoch 99
2024-12-18 13:24:28.130417: Current learning rate: 0.00379
2024-12-18 13:30:37.127135: Validation loss did not improve from -0.59207. Patience: 28/50
2024-12-18 13:30:37.128387: train_loss -0.7817
2024-12-18 13:30:37.129579: val_loss -0.5654
2024-12-18 13:30:37.130348: Pseudo dice [0.7663]
2024-12-18 13:30:37.131095: Epoch time: 369.0 s
2024-12-18 13:30:38.915605: 
2024-12-18 13:30:38.916928: Epoch 100
2024-12-18 13:30:38.917768: Current learning rate: 0.00372
2024-12-18 13:37:15.035785: Validation loss did not improve from -0.59207. Patience: 29/50
2024-12-18 13:37:15.036859: train_loss -0.7851
2024-12-18 13:37:15.037685: val_loss -0.5393
2024-12-18 13:37:15.038420: Pseudo dice [0.7473]
2024-12-18 13:37:15.039225: Epoch time: 396.12 s
2024-12-18 13:37:16.450362: 
2024-12-18 13:37:16.451674: Epoch 101
2024-12-18 13:37:16.452576: Current learning rate: 0.00365
2024-12-18 13:43:37.325013: Validation loss did not improve from -0.59207. Patience: 30/50
2024-12-18 13:43:37.325937: train_loss -0.7853
2024-12-18 13:43:37.326893: val_loss -0.5714
2024-12-18 13:43:37.327569: Pseudo dice [0.7671]
2024-12-18 13:43:37.328337: Epoch time: 380.88 s
2024-12-18 13:43:38.798049: 
2024-12-18 13:43:38.799308: Epoch 102
2024-12-18 13:43:38.800122: Current learning rate: 0.00359
2024-12-18 13:50:14.852089: Validation loss improved from -0.59207 to -0.59554! Patience: 30/50
2024-12-18 13:50:14.853135: train_loss -0.7863
2024-12-18 13:50:14.854084: val_loss -0.5955
2024-12-18 13:50:14.854966: Pseudo dice [0.7771]
2024-12-18 13:50:14.855850: Epoch time: 396.06 s
2024-12-18 13:50:16.223496: 
2024-12-18 13:50:16.224634: Epoch 103
2024-12-18 13:50:16.225365: Current learning rate: 0.00352
2024-12-18 13:56:43.298707: Validation loss did not improve from -0.59554. Patience: 1/50
2024-12-18 13:56:43.299601: train_loss -0.7826
2024-12-18 13:56:43.300452: val_loss -0.5663
2024-12-18 13:56:43.301259: Pseudo dice [0.7592]
2024-12-18 13:56:43.302088: Epoch time: 387.08 s
2024-12-18 13:56:44.772625: 
2024-12-18 13:56:44.774002: Epoch 104
2024-12-18 13:56:44.774790: Current learning rate: 0.00345
2024-12-18 14:02:41.822606: Validation loss did not improve from -0.59554. Patience: 2/50
2024-12-18 14:02:41.823524: train_loss -0.784
2024-12-18 14:02:41.824652: val_loss -0.5576
2024-12-18 14:02:41.825554: Pseudo dice [0.771]
2024-12-18 14:02:41.826476: Epoch time: 357.05 s
2024-12-18 14:02:43.643954: 
2024-12-18 14:02:43.644901: Epoch 105
2024-12-18 14:02:43.645654: Current learning rate: 0.00338
2024-12-18 14:08:27.712698: Validation loss did not improve from -0.59554. Patience: 3/50
2024-12-18 14:08:27.713652: train_loss -0.7853
2024-12-18 14:08:27.714727: val_loss -0.5541
2024-12-18 14:08:27.715480: Pseudo dice [0.7568]
2024-12-18 14:08:27.716229: Epoch time: 344.07 s
2024-12-18 14:08:29.563789: 
2024-12-18 14:08:29.564863: Epoch 106
2024-12-18 14:08:29.565631: Current learning rate: 0.00332
2024-12-18 14:14:39.941838: Validation loss did not improve from -0.59554. Patience: 4/50
2024-12-18 14:14:39.942479: train_loss -0.7887
2024-12-18 14:14:39.943221: val_loss -0.5638
2024-12-18 14:14:39.943940: Pseudo dice [0.7597]
2024-12-18 14:14:39.944639: Epoch time: 370.38 s
2024-12-18 14:14:41.412191: 
2024-12-18 14:14:41.413369: Epoch 107
2024-12-18 14:14:41.414093: Current learning rate: 0.00325
2024-12-18 14:21:03.416388: Validation loss did not improve from -0.59554. Patience: 5/50
2024-12-18 14:21:03.418562: train_loss -0.7888
2024-12-18 14:21:03.419549: val_loss -0.5529
2024-12-18 14:21:03.420330: Pseudo dice [0.7612]
2024-12-18 14:21:03.421192: Epoch time: 382.01 s
2024-12-18 14:21:04.905750: 
2024-12-18 14:21:04.906802: Epoch 108
2024-12-18 14:21:04.907618: Current learning rate: 0.00318
2024-12-18 14:27:26.670473: Validation loss did not improve from -0.59554. Patience: 6/50
2024-12-18 14:27:26.671334: train_loss -0.7894
2024-12-18 14:27:26.672028: val_loss -0.5665
2024-12-18 14:27:26.672753: Pseudo dice [0.7685]
2024-12-18 14:27:26.673552: Epoch time: 381.77 s
2024-12-18 14:27:28.075147: 
2024-12-18 14:27:28.076121: Epoch 109
2024-12-18 14:27:28.077084: Current learning rate: 0.00311
2024-12-18 14:33:32.707479: Validation loss did not improve from -0.59554. Patience: 7/50
2024-12-18 14:33:32.708429: train_loss -0.7923
2024-12-18 14:33:32.709458: val_loss -0.5808
2024-12-18 14:33:32.710506: Pseudo dice [0.7758]
2024-12-18 14:33:32.711385: Epoch time: 364.63 s
2024-12-18 14:33:34.515604: 
2024-12-18 14:33:34.516775: Epoch 110
2024-12-18 14:33:34.517643: Current learning rate: 0.00304
2024-12-18 14:39:28.149210: Validation loss did not improve from -0.59554. Patience: 8/50
2024-12-18 14:39:28.150062: train_loss -0.791
2024-12-18 14:39:28.150911: val_loss -0.5542
2024-12-18 14:39:28.151613: Pseudo dice [0.7625]
2024-12-18 14:39:28.152392: Epoch time: 353.64 s
2024-12-18 14:39:29.660578: 
2024-12-18 14:39:29.661912: Epoch 111
2024-12-18 14:39:29.662798: Current learning rate: 0.00297
2024-12-18 14:45:36.473084: Validation loss did not improve from -0.59554. Patience: 9/50
2024-12-18 14:45:36.474027: train_loss -0.7907
2024-12-18 14:45:36.474769: val_loss -0.5816
2024-12-18 14:45:36.475537: Pseudo dice [0.7746]
2024-12-18 14:45:36.476281: Epoch time: 366.81 s
2024-12-18 14:45:36.476988: Yayy! New best EMA pseudo Dice: 0.7651
2024-12-18 14:45:38.365756: 
2024-12-18 14:45:38.367184: Epoch 112
2024-12-18 14:45:38.368058: Current learning rate: 0.00291
2024-12-18 14:51:49.800748: Validation loss did not improve from -0.59554. Patience: 10/50
2024-12-18 14:51:49.801726: train_loss -0.7898
2024-12-18 14:51:49.802602: val_loss -0.551
2024-12-18 14:51:49.803433: Pseudo dice [0.7569]
2024-12-18 14:51:49.804231: Epoch time: 371.44 s
2024-12-18 14:51:51.284255: 
2024-12-18 14:51:51.285585: Epoch 113
2024-12-18 14:51:51.286410: Current learning rate: 0.00284
2024-12-18 14:57:37.461772: Validation loss did not improve from -0.59554. Patience: 11/50
2024-12-18 14:57:37.462501: train_loss -0.7923
2024-12-18 14:57:37.463310: val_loss -0.5472
2024-12-18 14:57:37.464030: Pseudo dice [0.7613]
2024-12-18 14:57:37.464799: Epoch time: 346.18 s
2024-12-18 14:57:38.902801: 
2024-12-18 14:57:38.903881: Epoch 114
2024-12-18 14:57:38.904607: Current learning rate: 0.00277
2024-12-18 15:03:58.954156: Validation loss did not improve from -0.59554. Patience: 12/50
2024-12-18 15:03:58.955134: train_loss -0.7927
2024-12-18 15:03:58.955966: val_loss -0.568
2024-12-18 15:03:58.956726: Pseudo dice [0.7624]
2024-12-18 15:03:58.957462: Epoch time: 380.05 s
2024-12-18 15:04:00.725722: 
2024-12-18 15:04:00.726881: Epoch 115
2024-12-18 15:04:00.727747: Current learning rate: 0.0027
2024-12-18 15:10:06.905699: Validation loss did not improve from -0.59554. Patience: 13/50
2024-12-18 15:10:06.906753: train_loss -0.7945
2024-12-18 15:10:06.907589: val_loss -0.5783
2024-12-18 15:10:06.908463: Pseudo dice [0.7759]
2024-12-18 15:10:06.909248: Epoch time: 366.18 s
2024-12-18 15:10:08.487623: 
2024-12-18 15:10:08.489072: Epoch 116
2024-12-18 15:10:08.490050: Current learning rate: 0.00263
2024-12-18 15:16:19.991649: Validation loss did not improve from -0.59554. Patience: 14/50
2024-12-18 15:16:19.992607: train_loss -0.7949
2024-12-18 15:16:19.993619: val_loss -0.5553
2024-12-18 15:16:19.994371: Pseudo dice [0.7642]
2024-12-18 15:16:19.995122: Epoch time: 371.51 s
2024-12-18 15:16:21.967157: 
2024-12-18 15:16:21.968205: Epoch 117
2024-12-18 15:16:21.969048: Current learning rate: 0.00256
2024-12-18 15:22:37.839301: Validation loss did not improve from -0.59554. Patience: 15/50
2024-12-18 15:22:37.840938: train_loss -0.793
2024-12-18 15:22:37.841903: val_loss -0.5263
2024-12-18 15:22:37.842843: Pseudo dice [0.7441]
2024-12-18 15:22:37.843672: Epoch time: 375.87 s
2024-12-18 15:22:39.409162: 
2024-12-18 15:22:39.410513: Epoch 118
2024-12-18 15:22:39.411400: Current learning rate: 0.00249
2024-12-18 15:28:41.253934: Validation loss did not improve from -0.59554. Patience: 16/50
2024-12-18 15:28:41.255075: train_loss -0.7926
2024-12-18 15:28:41.255892: val_loss -0.5641
2024-12-18 15:28:41.256494: Pseudo dice [0.7666]
2024-12-18 15:28:41.257108: Epoch time: 361.85 s
2024-12-18 15:28:42.708468: 
2024-12-18 15:28:42.709642: Epoch 119
2024-12-18 15:28:42.710458: Current learning rate: 0.00242
2024-12-18 15:34:42.663341: Validation loss did not improve from -0.59554. Patience: 17/50
2024-12-18 15:34:42.664042: train_loss -0.7982
2024-12-18 15:34:42.664920: val_loss -0.5554
2024-12-18 15:34:42.665810: Pseudo dice [0.7614]
2024-12-18 15:34:42.666733: Epoch time: 359.96 s
2024-12-18 15:34:44.700401: 
2024-12-18 15:34:44.702034: Epoch 120
2024-12-18 15:34:44.702755: Current learning rate: 0.00235
2024-12-18 15:40:40.629562: Validation loss did not improve from -0.59554. Patience: 18/50
2024-12-18 15:40:40.630594: train_loss -0.7972
2024-12-18 15:40:40.631486: val_loss -0.5499
2024-12-18 15:40:40.632348: Pseudo dice [0.7597]
2024-12-18 15:40:40.632972: Epoch time: 355.93 s
2024-12-18 15:40:42.074962: 
2024-12-18 15:40:42.076328: Epoch 121
2024-12-18 15:40:42.077117: Current learning rate: 0.00228
2024-12-18 15:46:55.271486: Validation loss did not improve from -0.59554. Patience: 19/50
2024-12-18 15:46:55.272971: train_loss -0.7976
2024-12-18 15:46:55.274175: val_loss -0.5658
2024-12-18 15:46:55.275200: Pseudo dice [0.7684]
2024-12-18 15:46:55.276261: Epoch time: 373.2 s
2024-12-18 15:46:56.745654: 
2024-12-18 15:46:56.746949: Epoch 122
2024-12-18 15:46:56.747888: Current learning rate: 0.00221
2024-12-18 15:53:15.274795: Validation loss did not improve from -0.59554. Patience: 20/50
2024-12-18 15:53:15.275525: train_loss -0.7961
2024-12-18 15:53:15.276347: val_loss -0.5543
2024-12-18 15:53:15.277177: Pseudo dice [0.7613]
2024-12-18 15:53:15.278005: Epoch time: 378.53 s
2024-12-18 15:53:16.714890: 
2024-12-18 15:53:16.716082: Epoch 123
2024-12-18 15:53:16.716907: Current learning rate: 0.00214
2024-12-18 15:59:43.507357: Validation loss did not improve from -0.59554. Patience: 21/50
2024-12-18 15:59:43.509083: train_loss -0.7981
2024-12-18 15:59:43.509928: val_loss -0.5473
2024-12-18 15:59:43.510766: Pseudo dice [0.7637]
2024-12-18 15:59:43.511659: Epoch time: 386.8 s
2024-12-18 15:59:44.996305: 
2024-12-18 15:59:44.997667: Epoch 124
2024-12-18 15:59:44.998693: Current learning rate: 0.00207
2024-12-18 16:05:40.423341: Validation loss did not improve from -0.59554. Patience: 22/50
2024-12-18 16:05:40.424262: train_loss -0.7957
2024-12-18 16:05:40.425135: val_loss -0.5708
2024-12-18 16:05:40.425806: Pseudo dice [0.7678]
2024-12-18 16:05:40.426538: Epoch time: 355.43 s
2024-12-18 16:05:42.333687: 
2024-12-18 16:05:42.334966: Epoch 125
2024-12-18 16:05:42.335870: Current learning rate: 0.00199
2024-12-18 16:11:02.777372: Validation loss did not improve from -0.59554. Patience: 23/50
2024-12-18 16:11:02.778241: train_loss -0.7988
2024-12-18 16:11:02.779015: val_loss -0.5514
2024-12-18 16:11:02.779856: Pseudo dice [0.7645]
2024-12-18 16:11:02.780696: Epoch time: 320.45 s
2024-12-18 16:11:04.207677: 
2024-12-18 16:11:04.208811: Epoch 126
2024-12-18 16:11:04.209490: Current learning rate: 0.00192
2024-12-18 16:16:32.334913: Validation loss did not improve from -0.59554. Patience: 24/50
2024-12-18 16:16:32.335910: train_loss -0.799
2024-12-18 16:16:32.336679: val_loss -0.561
2024-12-18 16:16:32.337263: Pseudo dice [0.7624]
2024-12-18 16:16:32.337936: Epoch time: 328.13 s
2024-12-18 16:16:33.765102: 
2024-12-18 16:16:33.766258: Epoch 127
2024-12-18 16:16:33.767032: Current learning rate: 0.00185
2024-12-18 16:22:00.156427: Validation loss did not improve from -0.59554. Patience: 25/50
2024-12-18 16:22:00.157264: train_loss -0.8
2024-12-18 16:22:00.158161: val_loss -0.5338
2024-12-18 16:22:00.158876: Pseudo dice [0.7505]
2024-12-18 16:22:00.159695: Epoch time: 326.39 s
2024-12-18 16:22:02.521371: 
2024-12-18 16:22:02.522535: Epoch 128
2024-12-18 16:22:02.523269: Current learning rate: 0.00178
2024-12-18 16:28:01.318573: Validation loss did not improve from -0.59554. Patience: 26/50
2024-12-18 16:28:01.350155: train_loss -0.8027
2024-12-18 16:28:01.351249: val_loss -0.5823
2024-12-18 16:28:01.351992: Pseudo dice [0.7812]
2024-12-18 16:28:01.352734: Epoch time: 358.83 s
2024-12-18 16:28:02.852297: 
2024-12-18 16:28:02.853481: Epoch 129
2024-12-18 16:28:02.854146: Current learning rate: 0.0017
2024-12-18 16:35:36.293086: Validation loss did not improve from -0.59554. Patience: 27/50
2024-12-18 16:35:36.294149: train_loss -0.8022
2024-12-18 16:35:36.295049: val_loss -0.5666
2024-12-18 16:35:36.295719: Pseudo dice [0.7662]
2024-12-18 16:35:36.296459: Epoch time: 453.44 s
2024-12-18 16:35:38.200323: 
2024-12-18 16:35:38.201434: Epoch 130
2024-12-18 16:35:38.202076: Current learning rate: 0.00163
2024-12-18 16:42:58.708027: Validation loss did not improve from -0.59554. Patience: 28/50
2024-12-18 16:42:58.708898: train_loss -0.803
2024-12-18 16:42:58.709552: val_loss -0.5531
2024-12-18 16:42:58.710134: Pseudo dice [0.7633]
2024-12-18 16:42:58.710851: Epoch time: 440.51 s
2024-12-18 16:43:00.081483: 
2024-12-18 16:43:00.082575: Epoch 131
2024-12-18 16:43:00.083537: Current learning rate: 0.00156
2024-12-18 16:49:57.995544: Validation loss did not improve from -0.59554. Patience: 29/50
2024-12-18 16:49:57.996865: train_loss -0.7999
2024-12-18 16:49:57.997980: val_loss -0.5506
2024-12-18 16:49:57.998848: Pseudo dice [0.7562]
2024-12-18 16:49:57.999994: Epoch time: 417.92 s
2024-12-18 16:49:59.420558: 
2024-12-18 16:49:59.421705: Epoch 132
2024-12-18 16:49:59.422583: Current learning rate: 0.00148
2024-12-18 16:57:02.527615: Validation loss did not improve from -0.59554. Patience: 30/50
2024-12-18 16:57:02.528248: train_loss -0.8022
2024-12-18 16:57:02.528964: val_loss -0.5551
2024-12-18 16:57:02.529676: Pseudo dice [0.7646]
2024-12-18 16:57:02.530351: Epoch time: 423.11 s
2024-12-18 16:57:03.892961: 
2024-12-18 16:57:03.894069: Epoch 133
2024-12-18 16:57:03.894733: Current learning rate: 0.00141
2024-12-18 17:03:50.259788: Validation loss did not improve from -0.59554. Patience: 31/50
2024-12-18 17:03:50.261044: train_loss -0.801
2024-12-18 17:03:50.261819: val_loss -0.5887
2024-12-18 17:03:50.262589: Pseudo dice [0.7767]
2024-12-18 17:03:50.263349: Epoch time: 406.37 s
2024-12-18 17:03:51.672201: 
2024-12-18 17:03:51.673407: Epoch 134
2024-12-18 17:03:51.674170: Current learning rate: 0.00133
2024-12-18 17:10:18.253217: Validation loss did not improve from -0.59554. Patience: 32/50
2024-12-18 17:10:18.254237: train_loss -0.8017
2024-12-18 17:10:18.255201: val_loss -0.5491
2024-12-18 17:10:18.255972: Pseudo dice [0.7589]
2024-12-18 17:10:18.256761: Epoch time: 386.58 s
2024-12-18 17:10:20.089336: 
2024-12-18 17:10:20.090803: Epoch 135
2024-12-18 17:10:20.091749: Current learning rate: 0.00126
2024-12-18 17:16:43.272856: Validation loss did not improve from -0.59554. Patience: 33/50
2024-12-18 17:16:43.273759: train_loss -0.8006
2024-12-18 17:16:43.274605: val_loss -0.5717
2024-12-18 17:16:43.275303: Pseudo dice [0.7714]
2024-12-18 17:16:43.276042: Epoch time: 383.19 s
2024-12-18 17:16:44.709031: 
2024-12-18 17:16:44.710096: Epoch 136
2024-12-18 17:16:44.710782: Current learning rate: 0.00118
2024-12-18 17:23:09.491890: Validation loss did not improve from -0.59554. Patience: 34/50
2024-12-18 17:23:09.492690: train_loss -0.8018
2024-12-18 17:23:09.493730: val_loss -0.5566
2024-12-18 17:23:09.494300: Pseudo dice [0.7608]
2024-12-18 17:23:09.494913: Epoch time: 384.78 s
2024-12-18 17:23:10.939012: 
2024-12-18 17:23:10.939922: Epoch 137
2024-12-18 17:23:10.940569: Current learning rate: 0.00111
2024-12-18 17:29:36.710006: Validation loss did not improve from -0.59554. Patience: 35/50
2024-12-18 17:29:36.711830: train_loss -0.8064
2024-12-18 17:29:36.712796: val_loss -0.562
2024-12-18 17:29:36.713701: Pseudo dice [0.7678]
2024-12-18 17:29:36.714658: Epoch time: 385.77 s
2024-12-18 17:29:38.930835: 
2024-12-18 17:29:38.933002: Epoch 138
2024-12-18 17:29:38.934235: Current learning rate: 0.00103
2024-12-18 17:35:34.136689: Validation loss improved from -0.59554 to -0.59914! Patience: 35/50
2024-12-18 17:35:34.138016: train_loss -0.8033
2024-12-18 17:35:34.138801: val_loss -0.5991
2024-12-18 17:35:34.139444: Pseudo dice [0.7814]
2024-12-18 17:35:34.140224: Epoch time: 355.21 s
2024-12-18 17:35:34.140938: Yayy! New best EMA pseudo Dice: 0.7666
2024-12-18 17:35:36.289716: 
2024-12-18 17:35:36.290930: Epoch 139
2024-12-18 17:35:36.291652: Current learning rate: 0.00095
2024-12-18 17:42:02.719031: Validation loss did not improve from -0.59914. Patience: 1/50
2024-12-18 17:42:02.719904: train_loss -0.8046
2024-12-18 17:42:02.720715: val_loss -0.5753
2024-12-18 17:42:02.721398: Pseudo dice [0.7726]
2024-12-18 17:42:02.722071: Epoch time: 386.43 s
2024-12-18 17:42:03.164557: Yayy! New best EMA pseudo Dice: 0.7672
2024-12-18 17:42:05.090840: 
2024-12-18 17:42:05.091771: Epoch 140
2024-12-18 17:42:05.092520: Current learning rate: 0.00087
2024-12-18 17:48:15.631931: Validation loss did not improve from -0.59914. Patience: 2/50
2024-12-18 17:48:15.632995: train_loss -0.8062
2024-12-18 17:48:15.633973: val_loss -0.558
2024-12-18 17:48:15.634948: Pseudo dice [0.7648]
2024-12-18 17:48:15.635780: Epoch time: 370.54 s
2024-12-18 17:48:17.147178: 
2024-12-18 17:48:17.148451: Epoch 141
2024-12-18 17:48:17.149273: Current learning rate: 0.00079
2024-12-18 17:54:17.706349: Validation loss did not improve from -0.59914. Patience: 3/50
2024-12-18 17:54:17.707206: train_loss -0.8053
2024-12-18 17:54:17.708148: val_loss -0.5497
2024-12-18 17:54:17.708906: Pseudo dice [0.7621]
2024-12-18 17:54:17.709529: Epoch time: 360.56 s
2024-12-18 17:54:19.174837: 
2024-12-18 17:54:19.176131: Epoch 142
2024-12-18 17:54:19.177087: Current learning rate: 0.00071
2024-12-18 17:59:30.472683: Validation loss did not improve from -0.59914. Patience: 4/50
2024-12-18 17:59:30.473593: train_loss -0.8056
2024-12-18 17:59:30.474723: val_loss -0.5581
2024-12-18 17:59:30.475922: Pseudo dice [0.7677]
2024-12-18 17:59:30.477125: Epoch time: 311.3 s
2024-12-18 17:59:31.960144: 
2024-12-18 17:59:31.961291: Epoch 143
2024-12-18 17:59:31.962017: Current learning rate: 0.00063
2024-12-18 18:04:55.082078: Validation loss did not improve from -0.59914. Patience: 5/50
2024-12-18 18:04:55.083120: train_loss -0.8059
2024-12-18 18:04:55.083967: val_loss -0.5582
2024-12-18 18:04:55.084694: Pseudo dice [0.7669]
2024-12-18 18:04:55.085446: Epoch time: 323.12 s
2024-12-18 18:04:56.584371: 
2024-12-18 18:04:56.585561: Epoch 144
2024-12-18 18:04:56.586276: Current learning rate: 0.00055
2024-12-18 18:11:14.206986: Validation loss did not improve from -0.59914. Patience: 6/50
2024-12-18 18:11:14.207821: train_loss -0.8043
2024-12-18 18:11:14.208526: val_loss -0.5523
2024-12-18 18:11:14.209255: Pseudo dice [0.7634]
2024-12-18 18:11:14.209966: Epoch time: 377.62 s
2024-12-18 18:11:16.101025: 
2024-12-18 18:11:16.102042: Epoch 145
2024-12-18 18:11:16.102812: Current learning rate: 0.00047
2024-12-18 18:17:57.600853: Validation loss did not improve from -0.59914. Patience: 7/50
2024-12-18 18:17:57.602012: train_loss -0.8066
2024-12-18 18:17:57.602970: val_loss -0.5928
2024-12-18 18:17:57.603734: Pseudo dice [0.7844]
2024-12-18 18:17:57.604647: Epoch time: 401.5 s
2024-12-18 18:17:57.605442: Yayy! New best EMA pseudo Dice: 0.7681
2024-12-18 18:17:59.491925: 
2024-12-18 18:17:59.492936: Epoch 146
2024-12-18 18:17:59.493718: Current learning rate: 0.00038
2024-12-18 18:24:34.489188: Validation loss did not improve from -0.59914. Patience: 8/50
2024-12-18 18:24:34.489973: train_loss -0.8046
2024-12-18 18:24:34.490721: val_loss -0.5754
2024-12-18 18:24:34.491469: Pseudo dice [0.7709]
2024-12-18 18:24:34.492079: Epoch time: 395.0 s
2024-12-18 18:24:34.492672: Yayy! New best EMA pseudo Dice: 0.7684
2024-12-18 18:24:36.381883: 
2024-12-18 18:24:36.382950: Epoch 147
2024-12-18 18:24:36.383875: Current learning rate: 0.0003
2024-12-18 18:30:59.285133: Validation loss did not improve from -0.59914. Patience: 9/50
2024-12-18 18:30:59.285952: train_loss -0.8053
2024-12-18 18:30:59.286892: val_loss -0.5748
2024-12-18 18:30:59.287622: Pseudo dice [0.7718]
2024-12-18 18:30:59.288252: Epoch time: 382.91 s
2024-12-18 18:30:59.288992: Yayy! New best EMA pseudo Dice: 0.7687
2024-12-18 18:31:01.175471: 
2024-12-18 18:31:01.176655: Epoch 148
2024-12-18 18:31:01.177329: Current learning rate: 0.00021
2024-12-18 18:37:13.181220: Validation loss did not improve from -0.59914. Patience: 10/50
2024-12-18 18:37:13.182020: train_loss -0.8046
2024-12-18 18:37:13.182795: val_loss -0.5796
2024-12-18 18:37:13.183542: Pseudo dice [0.7717]
2024-12-18 18:37:13.184306: Epoch time: 372.01 s
2024-12-18 18:37:13.184917: Yayy! New best EMA pseudo Dice: 0.769
2024-12-18 18:37:15.343997: 
2024-12-18 18:37:15.345026: Epoch 149
2024-12-18 18:37:15.346084: Current learning rate: 0.00011
2024-12-18 18:43:47.259311: Validation loss did not improve from -0.59914. Patience: 11/50
2024-12-18 18:43:47.261225: train_loss -0.806
2024-12-18 18:43:47.262438: val_loss -0.5763
2024-12-18 18:43:47.263527: Pseudo dice [0.767]
2024-12-18 18:43:47.264328: Epoch time: 391.92 s
2024-12-18 18:43:49.245434: Training done.
2024-12-18 18:43:49.355372: Using splits from existing split file: /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset309_Sohee_Calcium_OCT_CrossValidation/splits_final_60.json
2024-12-18 18:43:49.357173: The split file contains 5 splits.
2024-12-18 18:43:49.357910: Desired fold for training: 2
2024-12-18 18:43:49.358592: This split has 4 training and 4 validation cases.
2024-12-18 18:43:49.359509: predicting 101-044
2024-12-18 18:43:49.418989: 101-044, shape torch.Size([1, 404, 498, 498]), rank 0
2024-12-18 18:46:11.465731: predicting 101-045
2024-12-18 18:46:11.489501: 101-045, shape torch.Size([1, 375, 498, 498]), rank 0
2024-12-18 18:48:13.591371: predicting 704-003
2024-12-18 18:48:13.605737: 704-003, shape torch.Size([1, 375, 498, 498]), rank 0
2024-12-18 18:50:12.164131: predicting 706-005
2024-12-18 18:50:12.178359: 706-005, shape torch.Size([1, 375, 498, 498]), rank 0
2024-12-18 18:52:31.987812: Validation complete
2024-12-18 18:52:31.988524: Mean Validation Dice:  0.7543815442604305

############################
INFO: You are using the old nnU-Net default plans. We have updated our recommendations. Please consider using those instead! Read more here: https://github.com/MIC-DKFZ/nnUNet/blob/master/documentation/resenc_presets.md
############################

Using device: cuda:0

#######################################################################
Please cite the following paper when using nnU-Net:
Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.
#######################################################################

2024-12-18 18:52:38.436731: Using torch.compile...
/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
################### Loading pretrained weights from file  /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_results/Dataset300_Lumen_and_Wall_OCT/nnUNetTrainer__nnUNetPreprocessPlans__3d_fullres/fold_all/checkpoint_best.pth ###################
Below is the list of overlapping blocks in pretrained model and nnUNet architecture:
encoder.stages.0.0.convs.0.conv.weight shape torch.Size([32, 1, 3, 3, 3])
encoder.stages.0.0.convs.0.conv.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.norm.weight shape torch.Size([32])
encoder.stages.0.0.convs.0.norm.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.0.weight shape torch.Size([32, 1, 3, 3, 3])
encoder.stages.0.0.convs.0.all_modules.0.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.1.weight shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.1.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
encoder.stages.0.0.convs.1.conv.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.norm.weight shape torch.Size([32])
encoder.stages.0.0.convs.1.norm.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
encoder.stages.0.0.convs.1.all_modules.0.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.1.weight shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.1.bias shape torch.Size([32])
encoder.stages.1.0.convs.0.conv.weight shape torch.Size([64, 32, 3, 3, 3])
encoder.stages.1.0.convs.0.conv.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.norm.weight shape torch.Size([64])
encoder.stages.1.0.convs.0.norm.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.0.weight shape torch.Size([64, 32, 3, 3, 3])
encoder.stages.1.0.convs.0.all_modules.0.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.1.weight shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.1.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
encoder.stages.1.0.convs.1.conv.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.norm.weight shape torch.Size([64])
encoder.stages.1.0.convs.1.norm.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
encoder.stages.1.0.convs.1.all_modules.0.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.1.weight shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.1.bias shape torch.Size([64])
encoder.stages.2.0.convs.0.conv.weight shape torch.Size([128, 64, 3, 3, 3])
encoder.stages.2.0.convs.0.conv.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.norm.weight shape torch.Size([128])
encoder.stages.2.0.convs.0.norm.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.0.weight shape torch.Size([128, 64, 3, 3, 3])
encoder.stages.2.0.convs.0.all_modules.0.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.1.weight shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.1.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
encoder.stages.2.0.convs.1.conv.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.norm.weight shape torch.Size([128])
encoder.stages.2.0.convs.1.norm.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
encoder.stages.2.0.convs.1.all_modules.0.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.1.weight shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.1.bias shape torch.Size([128])
encoder.stages.3.0.convs.0.conv.weight shape torch.Size([256, 128, 3, 3, 3])
encoder.stages.3.0.convs.0.conv.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.norm.weight shape torch.Size([256])
encoder.stages.3.0.convs.0.norm.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.0.weight shape torch.Size([256, 128, 3, 3, 3])
encoder.stages.3.0.convs.0.all_modules.0.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.1.weight shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.1.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
encoder.stages.3.0.convs.1.conv.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.norm.weight shape torch.Size([256])
encoder.stages.3.0.convs.1.norm.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
encoder.stages.3.0.convs.1.all_modules.0.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.1.weight shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.1.bias shape torch.Size([256])
encoder.stages.4.0.convs.0.conv.weight shape torch.Size([320, 256, 3, 3, 3])
encoder.stages.4.0.convs.0.conv.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.norm.weight shape torch.Size([320])
encoder.stages.4.0.convs.0.norm.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.0.weight shape torch.Size([320, 256, 3, 3, 3])
encoder.stages.4.0.convs.0.all_modules.0.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.1.weight shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.1.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.4.0.convs.1.conv.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.norm.weight shape torch.Size([320])
encoder.stages.4.0.convs.1.norm.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.4.0.convs.1.all_modules.0.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.1.weight shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.1.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.0.conv.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.norm.weight shape torch.Size([320])
encoder.stages.5.0.convs.0.norm.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.0.all_modules.0.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.1.weight shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.1.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.1.conv.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.norm.weight shape torch.Size([320])
encoder.stages.5.0.convs.1.norm.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.1.all_modules.0.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.1.weight shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.0.0.convs.0.conv.weight shape torch.Size([32, 1, 3, 3, 3])
decoder.encoder.stages.0.0.convs.0.conv.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.norm.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.norm.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.0.weight shape torch.Size([32, 1, 3, 3, 3])
decoder.encoder.stages.0.0.convs.0.all_modules.0.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.1.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.1.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.encoder.stages.0.0.convs.1.conv.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.norm.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.norm.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.encoder.stages.0.0.convs.1.all_modules.0.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.1.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.1.bias shape torch.Size([32])
decoder.encoder.stages.1.0.convs.0.conv.weight shape torch.Size([64, 32, 3, 3, 3])
decoder.encoder.stages.1.0.convs.0.conv.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.norm.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.norm.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.0.weight shape torch.Size([64, 32, 3, 3, 3])
decoder.encoder.stages.1.0.convs.0.all_modules.0.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.1.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.1.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.encoder.stages.1.0.convs.1.conv.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.norm.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.norm.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.encoder.stages.1.0.convs.1.all_modules.0.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.1.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.1.bias shape torch.Size([64])
decoder.encoder.stages.2.0.convs.0.conv.weight shape torch.Size([128, 64, 3, 3, 3])
decoder.encoder.stages.2.0.convs.0.conv.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.norm.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.norm.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.0.weight shape torch.Size([128, 64, 3, 3, 3])
decoder.encoder.stages.2.0.convs.0.all_modules.0.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.1.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.1.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.encoder.stages.2.0.convs.1.conv.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.norm.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.norm.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.encoder.stages.2.0.convs.1.all_modules.0.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.1.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.1.bias shape torch.Size([128])
decoder.encoder.stages.3.0.convs.0.conv.weight shape torch.Size([256, 128, 3, 3, 3])
decoder.encoder.stages.3.0.convs.0.conv.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.norm.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.norm.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.0.weight shape torch.Size([256, 128, 3, 3, 3])
decoder.encoder.stages.3.0.convs.0.all_modules.0.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.1.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.1.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.encoder.stages.3.0.convs.1.conv.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.norm.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.norm.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.encoder.stages.3.0.convs.1.all_modules.0.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.1.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.1.bias shape torch.Size([256])
decoder.encoder.stages.4.0.convs.0.conv.weight shape torch.Size([320, 256, 3, 3, 3])
decoder.encoder.stages.4.0.convs.0.conv.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.norm.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.norm.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.0.weight shape torch.Size([320, 256, 3, 3, 3])
decoder.encoder.stages.4.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.4.0.convs.1.conv.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.norm.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.norm.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.4.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.0.conv.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.norm.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.norm.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.1.conv.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.norm.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.norm.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.stages.0.convs.0.conv.weight shape torch.Size([320, 640, 3, 3, 3])
decoder.stages.0.convs.0.conv.bias shape torch.Size([320])
decoder.stages.0.convs.0.norm.weight shape torch.Size([320])
decoder.stages.0.convs.0.norm.bias shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.0.weight shape torch.Size([320, 640, 3, 3, 3])
decoder.stages.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.stages.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.stages.0.convs.1.conv.bias shape torch.Size([320])
decoder.stages.0.convs.1.norm.weight shape torch.Size([320])
decoder.stages.0.convs.1.norm.bias shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.stages.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.stages.1.convs.0.conv.weight shape torch.Size([256, 512, 3, 3, 3])
decoder.stages.1.convs.0.conv.bias shape torch.Size([256])
decoder.stages.1.convs.0.norm.weight shape torch.Size([256])
decoder.stages.1.convs.0.norm.bias shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.0.weight shape torch.Size([256, 512, 3, 3, 3])
decoder.stages.1.convs.0.all_modules.0.bias shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.1.weight shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.1.bias shape torch.Size([256])
decoder.stages.1.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.stages.1.convs.1.conv.bias shape torch.Size([256])
decoder.stages.1.convs.1.norm.weight shape torch.Size([256])
decoder.stages.1.convs.1.norm.bias shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.stages.1.convs.1.all_modules.0.bias shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.1.weight shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.1.bias shape torch.Size([256])
decoder.stages.2.convs.0.conv.weight shape torch.Size([128, 256, 3, 3, 3])
decoder.stages.2.convs.0.conv.bias shape torch.Size([128])
decoder.stages.2.convs.0.norm.weight shape torch.Size([128])
decoder.stages.2.convs.0.norm.bias shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.0.weight shape torch.Size([128, 256, 3, 3, 3])
decoder.stages.2.convs.0.all_modules.0.bias shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.1.weight shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.1.bias shape torch.Size([128])
decoder.stages.2.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.stages.2.convs.1.conv.bias shape torch.Size([128])
decoder.stages.2.convs.1.norm.weight shape torch.Size([128])
decoder.stages.2.convs.1.norm.bias shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.stages.2.convs.1.all_modules.0.bias shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.1.weight shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.1.bias shape torch.Size([128])
decoder.stages.3.convs.0.conv.weight shape torch.Size([64, 128, 3, 3, 3])
decoder.stages.3.convs.0.conv.bias shape torch.Size([64])
decoder.stages.3.convs.0.norm.weight shape torch.Size([64])
decoder.stages.3.convs.0.norm.bias shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.0.weight shape torch.Size([64, 128, 3, 3, 3])
decoder.stages.3.convs.0.all_modules.0.bias shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.1.weight shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.1.bias shape torch.Size([64])
decoder.stages.3.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.stages.3.convs.1.conv.bias shape torch.Size([64])
decoder.stages.3.convs.1.norm.weight shape torch.Size([64])
decoder.stages.3.convs.1.norm.bias shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.stages.3.convs.1.all_modules.0.bias shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.1.weight shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.1.bias shape torch.Size([64])
decoder.stages.4.convs.0.conv.weight shape torch.Size([32, 64, 3, 3, 3])
decoder.stages.4.convs.0.conv.bias shape torch.Size([32])
decoder.stages.4.convs.0.norm.weight shape torch.Size([32])
decoder.stages.4.convs.0.norm.bias shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.0.weight shape torch.Size([32, 64, 3, 3, 3])
decoder.stages.4.convs.0.all_modules.0.bias shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.1.weight shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.1.bias shape torch.Size([32])
decoder.stages.4.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.stages.4.convs.1.conv.bias shape torch.Size([32])
decoder.stages.4.convs.1.norm.weight shape torch.Size([32])
decoder.stages.4.convs.1.norm.bias shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.stages.4.convs.1.all_modules.0.bias shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.1.weight shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.1.bias shape torch.Size([32])
decoder.transpconvs.0.weight shape torch.Size([320, 320, 1, 2, 2])
decoder.transpconvs.0.bias shape torch.Size([320])
decoder.transpconvs.1.weight shape torch.Size([320, 256, 2, 2, 2])
decoder.transpconvs.1.bias shape torch.Size([256])
decoder.transpconvs.2.weight shape torch.Size([256, 128, 2, 2, 2])
decoder.transpconvs.2.bias shape torch.Size([128])
decoder.transpconvs.3.weight shape torch.Size([128, 64, 2, 2, 2])
decoder.transpconvs.3.bias shape torch.Size([64])
decoder.transpconvs.4.weight shape torch.Size([64, 32, 2, 2, 2])
decoder.transpconvs.4.bias shape torch.Size([32])
################### Done ###################
2024-12-18 18:52:44.535614: do_dummy_2d_data_aug: True
2024-12-18 18:52:44.537273: Using splits from existing split file: /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset309_Sohee_Calcium_OCT_CrossValidation/splits_final_60.json
2024-12-18 18:52:44.539383: The split file contains 5 splits.
2024-12-18 18:52:44.540659: Desired fold for training: 4
2024-12-18 18:52:44.541975: This split has 4 training and 4 validation cases.
using pin_memory on device 0
using pin_memory on device 0

This is the configuration used by this training:
Configuration name: 3d_32x160x128_b10
 {'data_identifier': 'nnUNetPlans_3d_fullres', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 10, 'patch_size': [32, 160, 128], 'median_image_size_in_voxels': [375.0, 498.0, 498.0], 'spacing': [1.0, 1.0, 1.0], 'normalization_schemes': ['ZScoreNormalization'], 'use_mask_for_norm': [False], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'architecture': {'network_class_name': 'dynamic_network_architectures.architectures.unet.PlainConvUNet', 'arch_kwargs': {'n_stages': 6, 'features_per_stage': [32, 64, 128, 256, 320, 320], 'conv_op': 'torch.nn.modules.conv.Conv3d', 'kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]], 'strides': [[1, 1, 1], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [1, 2, 2]], 'n_conv_per_stage': [2, 2, 2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2, 2, 2], 'conv_bias': True, 'norm_op': 'torch.nn.modules.instancenorm.InstanceNorm3d', 'norm_op_kwargs': {'eps': 1e-05, 'affine': True}, 'dropout_op': None, 'dropout_op_kwargs': None, 'nonlin': 'torch.nn.LeakyReLU', 'nonlin_kwargs': {'inplace': True}, 'deep_supervision': True}, '_kw_requires_import': ['conv_op', 'norm_op', 'dropout_op', 'nonlin']}, 'batch_dice': True, 'inherits_from': '3d_fullres'} 

These are the global plan.json settings:
 {'dataset_name': 'Dataset309_Sohee_Calcium_OCT_CrossValidation', 'plans_name': 'nnUNetPlans', 'original_median_spacing_after_transp': [1.0, 1.0, 1.0], 'original_median_shape_after_transp': [375, 498, 498], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 0.4977976381778717, 'mean': 0.13507525622844696, 'median': 0.09599608182907104, 'min': 0.0, 'percentile_00_5': 0.014754901640117168, 'percentile_99_5': 0.4977976381778717, 'std': 0.12025152146816254}}} 

2024-12-18 18:53:12.237256: unpacking dataset...
2024-12-18 18:53:16.561254: unpacking done...
2024-12-18 18:53:17.008613: Unable to plot network architecture: nnUNet_compile is enabled!
2024-12-18 18:53:17.104053: 
2024-12-18 18:53:17.105090: Epoch 0
2024-12-18 18:53:17.105898: Current learning rate: 0.01
2024-12-18 19:00:28.797661: Validation loss improved from 1000.00000 to -0.43553! Patience: 0/50
2024-12-18 19:00:28.798482: train_loss -0.3096
2024-12-18 19:00:28.799194: val_loss -0.4355
2024-12-18 19:00:28.799810: Pseudo dice [0.6804]
2024-12-18 19:00:28.800419: Epoch time: 431.7 s
2024-12-18 19:00:28.801041: Yayy! New best EMA pseudo Dice: 0.6804
2024-12-18 19:00:30.823294: 
2024-12-18 19:00:30.824686: Epoch 1
2024-12-18 19:00:30.825355: Current learning rate: 0.00994
2024-12-18 19:07:08.945656: Validation loss improved from -0.43553 to -0.46715! Patience: 0/50
2024-12-18 19:07:08.946473: train_loss -0.4775
2024-12-18 19:07:08.947759: val_loss -0.4672
2024-12-18 19:07:08.948627: Pseudo dice [0.7038]
2024-12-18 19:07:08.949502: Epoch time: 398.13 s
2024-12-18 19:07:08.950445: Yayy! New best EMA pseudo Dice: 0.6828
2024-12-18 19:07:10.720688: 
2024-12-18 19:07:10.721838: Epoch 2
2024-12-18 19:07:10.722594: Current learning rate: 0.00988
2024-12-18 19:13:19.322267: Validation loss improved from -0.46715 to -0.48905! Patience: 0/50
2024-12-18 19:13:19.323214: train_loss -0.5055
2024-12-18 19:13:19.324133: val_loss -0.489
2024-12-18 19:13:19.324805: Pseudo dice [0.7184]
2024-12-18 19:13:19.325435: Epoch time: 368.6 s
2024-12-18 19:13:19.326072: Yayy! New best EMA pseudo Dice: 0.6863
2024-12-18 19:13:21.187992: 
2024-12-18 19:13:21.189458: Epoch 3
2024-12-18 19:13:21.190447: Current learning rate: 0.00982
2024-12-18 19:19:50.028146: Validation loss did not improve from -0.48905. Patience: 1/50
2024-12-18 19:19:50.029136: train_loss -0.5255
2024-12-18 19:19:50.029783: val_loss -0.4637
2024-12-18 19:19:50.030362: Pseudo dice [0.6861]
2024-12-18 19:19:50.030972: Epoch time: 388.84 s
2024-12-18 19:19:51.414273: 
2024-12-18 19:19:51.415511: Epoch 4
2024-12-18 19:19:51.416254: Current learning rate: 0.00976
2024-12-18 19:26:10.977832: Validation loss did not improve from -0.48905. Patience: 2/50
2024-12-18 19:26:10.978906: train_loss -0.5478
2024-12-18 19:26:10.979695: val_loss -0.475
2024-12-18 19:26:10.980678: Pseudo dice [0.7047]
2024-12-18 19:26:10.981362: Epoch time: 379.57 s
2024-12-18 19:26:11.339833: Yayy! New best EMA pseudo Dice: 0.6881
2024-12-18 19:26:13.290883: 
2024-12-18 19:26:13.292112: Epoch 5
2024-12-18 19:26:13.293046: Current learning rate: 0.0097
2024-12-18 19:32:43.999274: Validation loss improved from -0.48905 to -0.51157! Patience: 2/50
2024-12-18 19:32:44.000467: train_loss -0.5814
2024-12-18 19:32:44.001373: val_loss -0.5116
2024-12-18 19:32:44.002126: Pseudo dice [0.7246]
2024-12-18 19:32:44.002922: Epoch time: 390.71 s
2024-12-18 19:32:44.003746: Yayy! New best EMA pseudo Dice: 0.6918
2024-12-18 19:32:45.768538: 
2024-12-18 19:32:45.770274: Epoch 6
2024-12-18 19:32:45.771357: Current learning rate: 0.00964
2024-12-18 19:39:06.417645: Validation loss did not improve from -0.51157. Patience: 1/50
2024-12-18 19:39:06.418589: train_loss -0.5767
2024-12-18 19:39:06.419786: val_loss -0.511
2024-12-18 19:39:06.420611: Pseudo dice [0.7292]
2024-12-18 19:39:06.421626: Epoch time: 380.65 s
2024-12-18 19:39:06.422411: Yayy! New best EMA pseudo Dice: 0.6955
2024-12-18 19:39:08.282126: 
2024-12-18 19:39:08.283322: Epoch 7
2024-12-18 19:39:08.284069: Current learning rate: 0.00958
2024-12-18 19:45:25.895305: Validation loss did not improve from -0.51157. Patience: 2/50
2024-12-18 19:45:25.896196: train_loss -0.5813
2024-12-18 19:45:25.897068: val_loss -0.501
2024-12-18 19:45:25.897961: Pseudo dice [0.7204]
2024-12-18 19:45:25.898676: Epoch time: 377.62 s
2024-12-18 19:45:25.899367: Yayy! New best EMA pseudo Dice: 0.698
2024-12-18 19:45:28.100755: 
2024-12-18 19:45:28.101641: Epoch 8
2024-12-18 19:45:28.102366: Current learning rate: 0.00952
2024-12-18 19:52:04.328773: Validation loss improved from -0.51157 to -0.52086! Patience: 2/50
2024-12-18 19:52:04.330684: train_loss -0.6018
2024-12-18 19:52:04.331754: val_loss -0.5209
2024-12-18 19:52:04.332631: Pseudo dice [0.7287]
2024-12-18 19:52:04.333527: Epoch time: 396.23 s
2024-12-18 19:52:04.334339: Yayy! New best EMA pseudo Dice: 0.7011
2024-12-18 19:52:06.163814: 
2024-12-18 19:52:06.165397: Epoch 9
2024-12-18 19:52:06.166359: Current learning rate: 0.00946
2024-12-18 19:58:46.598412: Validation loss did not improve from -0.52086. Patience: 1/50
2024-12-18 19:58:46.601350: train_loss -0.6097
2024-12-18 19:58:46.602092: val_loss -0.5049
2024-12-18 19:58:46.602689: Pseudo dice [0.7264]
2024-12-18 19:58:46.603476: Epoch time: 400.44 s
2024-12-18 19:58:47.003417: Yayy! New best EMA pseudo Dice: 0.7036
2024-12-18 19:58:48.720900: 
2024-12-18 19:58:48.722249: Epoch 10
2024-12-18 19:58:48.723604: Current learning rate: 0.0094
2024-12-18 20:05:22.654180: Validation loss did not improve from -0.52086. Patience: 2/50
2024-12-18 20:05:22.655159: train_loss -0.6143
2024-12-18 20:05:22.657022: val_loss -0.4625
2024-12-18 20:05:22.657733: Pseudo dice [0.7014]
2024-12-18 20:05:22.658544: Epoch time: 393.94 s
2024-12-18 20:05:24.019166: 
2024-12-18 20:05:24.020252: Epoch 11
2024-12-18 20:05:24.020849: Current learning rate: 0.00934
2024-12-18 20:11:38.389624: Validation loss did not improve from -0.52086. Patience: 3/50
2024-12-18 20:11:38.390493: train_loss -0.622
2024-12-18 20:11:38.391496: val_loss -0.5118
2024-12-18 20:11:38.392154: Pseudo dice [0.7276]
2024-12-18 20:11:38.393146: Epoch time: 374.37 s
2024-12-18 20:11:38.394036: Yayy! New best EMA pseudo Dice: 0.7058
2024-12-18 20:11:40.114998: 
2024-12-18 20:11:40.116155: Epoch 12
2024-12-18 20:11:40.116833: Current learning rate: 0.00928
2024-12-18 20:18:04.479213: Validation loss improved from -0.52086 to -0.52733! Patience: 3/50
2024-12-18 20:18:04.480153: train_loss -0.6336
2024-12-18 20:18:04.480830: val_loss -0.5273
2024-12-18 20:18:04.481656: Pseudo dice [0.7351]
2024-12-18 20:18:04.482400: Epoch time: 384.37 s
2024-12-18 20:18:04.483119: Yayy! New best EMA pseudo Dice: 0.7087
2024-12-18 20:18:06.265883: 
2024-12-18 20:18:06.266946: Epoch 13
2024-12-18 20:18:06.267558: Current learning rate: 0.00922
2024-12-18 20:24:28.674276: Validation loss improved from -0.52733 to -0.52850! Patience: 0/50
2024-12-18 20:24:28.675273: train_loss -0.6336
2024-12-18 20:24:28.676008: val_loss -0.5285
2024-12-18 20:24:28.676636: Pseudo dice [0.7318]
2024-12-18 20:24:28.677314: Epoch time: 382.41 s
2024-12-18 20:24:28.678377: Yayy! New best EMA pseudo Dice: 0.711
2024-12-18 20:24:30.444352: 
2024-12-18 20:24:30.445560: Epoch 14
2024-12-18 20:24:30.446204: Current learning rate: 0.00916
2024-12-18 20:30:52.406010: Validation loss improved from -0.52850 to -0.53780! Patience: 0/50
2024-12-18 20:30:52.407082: train_loss -0.6377
2024-12-18 20:30:52.407729: val_loss -0.5378
2024-12-18 20:30:52.408442: Pseudo dice [0.7324]
2024-12-18 20:30:52.409079: Epoch time: 381.96 s
2024-12-18 20:30:52.820551: Yayy! New best EMA pseudo Dice: 0.7132
2024-12-18 20:30:54.602891: 
2024-12-18 20:30:54.603976: Epoch 15
2024-12-18 20:30:54.604619: Current learning rate: 0.0091
2024-12-18 20:37:34.238677: Validation loss did not improve from -0.53780. Patience: 1/50
2024-12-18 20:37:34.252330: train_loss -0.6468
2024-12-18 20:37:34.253451: val_loss -0.509
2024-12-18 20:37:34.254330: Pseudo dice [0.7127]
2024-12-18 20:37:34.255170: Epoch time: 399.65 s
2024-12-18 20:37:35.851733: 
2024-12-18 20:37:35.853032: Epoch 16
2024-12-18 20:37:35.854118: Current learning rate: 0.00903
2024-12-18 20:44:15.758632: Validation loss did not improve from -0.53780. Patience: 2/50
2024-12-18 20:44:15.759589: train_loss -0.6453
2024-12-18 20:44:15.760314: val_loss -0.4996
2024-12-18 20:44:15.761000: Pseudo dice [0.7184]
2024-12-18 20:44:15.761673: Epoch time: 399.91 s
2024-12-18 20:44:15.762299: Yayy! New best EMA pseudo Dice: 0.7137
2024-12-18 20:44:17.589916: 
2024-12-18 20:44:17.591053: Epoch 17
2024-12-18 20:44:17.591701: Current learning rate: 0.00897
2024-12-18 20:50:36.092944: Validation loss did not improve from -0.53780. Patience: 3/50
2024-12-18 20:50:36.094093: train_loss -0.6466
2024-12-18 20:50:36.094952: val_loss -0.5257
2024-12-18 20:50:36.095761: Pseudo dice [0.741]
2024-12-18 20:50:36.096473: Epoch time: 378.51 s
2024-12-18 20:50:36.097264: Yayy! New best EMA pseudo Dice: 0.7164
2024-12-18 20:50:38.448247: 
2024-12-18 20:50:38.449517: Epoch 18
2024-12-18 20:50:38.450373: Current learning rate: 0.00891
2024-12-18 20:57:03.713548: Validation loss did not improve from -0.53780. Patience: 4/50
2024-12-18 20:57:03.714220: train_loss -0.6528
2024-12-18 20:57:03.715423: val_loss -0.5147
2024-12-18 20:57:03.716338: Pseudo dice [0.7264]
2024-12-18 20:57:03.717129: Epoch time: 385.27 s
2024-12-18 20:57:03.717813: Yayy! New best EMA pseudo Dice: 0.7174
2024-12-18 20:57:05.556904: 
2024-12-18 20:57:05.558435: Epoch 19
2024-12-18 20:57:05.559127: Current learning rate: 0.00885
2024-12-18 21:03:28.561368: Validation loss improved from -0.53780 to -0.54364! Patience: 4/50
2024-12-18 21:03:28.562420: train_loss -0.6649
2024-12-18 21:03:28.563272: val_loss -0.5436
2024-12-18 21:03:28.563918: Pseudo dice [0.7505]
2024-12-18 21:03:28.564496: Epoch time: 383.01 s
2024-12-18 21:03:28.911845: Yayy! New best EMA pseudo Dice: 0.7207
2024-12-18 21:03:30.852555: 
2024-12-18 21:03:30.853740: Epoch 20
2024-12-18 21:03:30.854415: Current learning rate: 0.00879
2024-12-18 21:10:20.930142: Validation loss improved from -0.54364 to -0.57470! Patience: 0/50
2024-12-18 21:10:20.931203: train_loss -0.6624
2024-12-18 21:10:20.932412: val_loss -0.5747
2024-12-18 21:10:20.933511: Pseudo dice [0.7656]
2024-12-18 21:10:20.934496: Epoch time: 410.08 s
2024-12-18 21:10:20.935441: Yayy! New best EMA pseudo Dice: 0.7252
2024-12-18 21:10:22.776060: 
2024-12-18 21:10:22.777215: Epoch 21
2024-12-18 21:10:22.777855: Current learning rate: 0.00873
2024-12-18 21:16:40.130447: Validation loss did not improve from -0.57470. Patience: 1/50
2024-12-18 21:16:40.150914: train_loss -0.6706
2024-12-18 21:16:40.151991: val_loss -0.56
2024-12-18 21:16:40.152613: Pseudo dice [0.7532]
2024-12-18 21:16:40.153228: Epoch time: 377.36 s
2024-12-18 21:16:40.153769: Yayy! New best EMA pseudo Dice: 0.728
2024-12-18 21:16:41.971688: 
2024-12-18 21:16:41.972705: Epoch 22
2024-12-18 21:16:41.973759: Current learning rate: 0.00867
2024-12-18 21:22:53.729696: Validation loss did not improve from -0.57470. Patience: 2/50
2024-12-18 21:22:53.730826: train_loss -0.6643
2024-12-18 21:22:53.731794: val_loss -0.5546
2024-12-18 21:22:53.732784: Pseudo dice [0.7525]
2024-12-18 21:22:53.733613: Epoch time: 371.76 s
2024-12-18 21:22:53.734394: Yayy! New best EMA pseudo Dice: 0.7304
2024-12-18 21:22:55.457926: 
2024-12-18 21:22:55.459542: Epoch 23
2024-12-18 21:22:55.460990: Current learning rate: 0.00861
2024-12-18 21:29:37.053198: Validation loss did not improve from -0.57470. Patience: 3/50
2024-12-18 21:29:37.054278: train_loss -0.6691
2024-12-18 21:29:37.055207: val_loss -0.5363
2024-12-18 21:29:37.056201: Pseudo dice [0.7332]
2024-12-18 21:29:37.057211: Epoch time: 401.6 s
2024-12-18 21:29:37.058153: Yayy! New best EMA pseudo Dice: 0.7307
2024-12-18 21:29:38.789332: 
2024-12-18 21:29:38.790577: Epoch 24
2024-12-18 21:29:38.791557: Current learning rate: 0.00855
2024-12-18 21:36:15.183845: Validation loss did not improve from -0.57470. Patience: 4/50
2024-12-18 21:36:15.184689: train_loss -0.6798
2024-12-18 21:36:15.185463: val_loss -0.5408
2024-12-18 21:36:15.186015: Pseudo dice [0.7375]
2024-12-18 21:36:15.186639: Epoch time: 396.4 s
2024-12-18 21:36:15.580611: Yayy! New best EMA pseudo Dice: 0.7314
2024-12-18 21:36:17.391962: 
2024-12-18 21:36:17.393116: Epoch 25
2024-12-18 21:36:17.393751: Current learning rate: 0.00849
2024-12-18 21:42:58.638466: Validation loss did not improve from -0.57470. Patience: 5/50
2024-12-18 21:42:58.642604: train_loss -0.6782
2024-12-18 21:42:58.643742: val_loss -0.5388
2024-12-18 21:42:58.644475: Pseudo dice [0.7366]
2024-12-18 21:42:58.645379: Epoch time: 401.25 s
2024-12-18 21:42:58.646200: Yayy! New best EMA pseudo Dice: 0.7319
2024-12-18 21:43:00.419284: 
2024-12-18 21:43:00.420469: Epoch 26
2024-12-18 21:43:00.421248: Current learning rate: 0.00843
2024-12-18 21:49:23.202767: Validation loss did not improve from -0.57470. Patience: 6/50
2024-12-18 21:49:23.203623: train_loss -0.6688
2024-12-18 21:49:23.204733: val_loss -0.5567
2024-12-18 21:49:23.205969: Pseudo dice [0.7471]
2024-12-18 21:49:23.207000: Epoch time: 382.79 s
2024-12-18 21:49:23.207833: Yayy! New best EMA pseudo Dice: 0.7334
2024-12-18 21:49:24.977592: 
2024-12-18 21:49:24.979223: Epoch 27
2024-12-18 21:49:24.980246: Current learning rate: 0.00836
2024-12-18 21:55:39.644917: Validation loss did not improve from -0.57470. Patience: 7/50
2024-12-18 21:55:39.645576: train_loss -0.6841
2024-12-18 21:55:39.646766: val_loss -0.5552
2024-12-18 21:55:39.647379: Pseudo dice [0.7487]
2024-12-18 21:55:39.648101: Epoch time: 374.67 s
2024-12-18 21:55:39.648977: Yayy! New best EMA pseudo Dice: 0.735
2024-12-18 21:55:42.174812: 
2024-12-18 21:55:42.175864: Epoch 28
2024-12-18 21:55:42.176508: Current learning rate: 0.0083
2024-12-18 22:02:20.406109: Validation loss did not improve from -0.57470. Patience: 8/50
2024-12-18 22:02:20.407055: train_loss -0.6903
2024-12-18 22:02:20.407843: val_loss -0.5346
2024-12-18 22:02:20.408779: Pseudo dice [0.7416]
2024-12-18 22:02:20.409464: Epoch time: 398.23 s
2024-12-18 22:02:20.410391: Yayy! New best EMA pseudo Dice: 0.7356
2024-12-18 22:02:22.176016: 
2024-12-18 22:02:22.177456: Epoch 29
2024-12-18 22:02:22.178394: Current learning rate: 0.00824
2024-12-18 22:08:56.174958: Validation loss did not improve from -0.57470. Patience: 9/50
2024-12-18 22:08:56.176289: train_loss -0.6994
2024-12-18 22:08:56.177060: val_loss -0.5282
2024-12-18 22:08:56.177716: Pseudo dice [0.7337]
2024-12-18 22:08:56.178341: Epoch time: 394.0 s
2024-12-18 22:08:57.991070: 
2024-12-18 22:08:57.992163: Epoch 30
2024-12-18 22:08:57.992779: Current learning rate: 0.00818
2024-12-18 22:15:16.001347: Validation loss did not improve from -0.57470. Patience: 10/50
2024-12-18 22:15:16.002396: train_loss -0.695
2024-12-18 22:15:16.003563: val_loss -0.5556
2024-12-18 22:15:16.004175: Pseudo dice [0.7478]
2024-12-18 22:15:16.004745: Epoch time: 378.01 s
2024-12-18 22:15:16.005466: Yayy! New best EMA pseudo Dice: 0.7367
2024-12-18 22:15:17.838755: 
2024-12-18 22:15:17.839905: Epoch 31
2024-12-18 22:15:17.840596: Current learning rate: 0.00812
2024-12-18 22:21:45.456935: Validation loss did not improve from -0.57470. Patience: 11/50
2024-12-18 22:21:45.458398: train_loss -0.7028
2024-12-18 22:21:45.459223: val_loss -0.5577
2024-12-18 22:21:45.460020: Pseudo dice [0.7461]
2024-12-18 22:21:45.460846: Epoch time: 387.62 s
2024-12-18 22:21:45.461586: Yayy! New best EMA pseudo Dice: 0.7376
2024-12-18 22:21:47.294629: 
2024-12-18 22:21:47.295715: Epoch 32
2024-12-18 22:21:47.296351: Current learning rate: 0.00806
2024-12-18 22:28:17.277814: Validation loss did not improve from -0.57470. Patience: 12/50
2024-12-18 22:28:17.278750: train_loss -0.7004
2024-12-18 22:28:17.279927: val_loss -0.5541
2024-12-18 22:28:17.280748: Pseudo dice [0.7572]
2024-12-18 22:28:17.281776: Epoch time: 389.99 s
2024-12-18 22:28:17.282444: Yayy! New best EMA pseudo Dice: 0.7396
2024-12-18 22:28:19.083520: 
2024-12-18 22:28:19.084619: Epoch 33
2024-12-18 22:28:19.085404: Current learning rate: 0.008
2024-12-18 22:34:40.776246: Validation loss did not improve from -0.57470. Patience: 13/50
2024-12-18 22:34:40.777288: train_loss -0.7065
2024-12-18 22:34:40.778266: val_loss -0.5649
2024-12-18 22:34:40.779100: Pseudo dice [0.7521]
2024-12-18 22:34:40.779969: Epoch time: 381.7 s
2024-12-18 22:34:40.781106: Yayy! New best EMA pseudo Dice: 0.7408
2024-12-18 22:34:42.545239: 
2024-12-18 22:34:42.546357: Epoch 34
2024-12-18 22:34:42.547177: Current learning rate: 0.00793
2024-12-18 22:41:20.506169: Validation loss did not improve from -0.57470. Patience: 14/50
2024-12-18 22:41:20.507180: train_loss -0.7036
2024-12-18 22:41:20.507852: val_loss -0.5369
2024-12-18 22:41:20.508628: Pseudo dice [0.7407]
2024-12-18 22:41:20.509787: Epoch time: 397.96 s
2024-12-18 22:41:22.362252: 
2024-12-18 22:41:22.363405: Epoch 35
2024-12-18 22:41:22.364912: Current learning rate: 0.00787
2024-12-18 22:48:19.668752: Validation loss did not improve from -0.57470. Patience: 15/50
2024-12-18 22:48:19.677943: train_loss -0.7014
2024-12-18 22:48:19.679495: val_loss -0.5737
2024-12-18 22:48:19.680486: Pseudo dice [0.7616]
2024-12-18 22:48:19.681767: Epoch time: 417.31 s
2024-12-18 22:48:19.682823: Yayy! New best EMA pseudo Dice: 0.7429
2024-12-18 22:48:21.666003: 
2024-12-18 22:48:21.668002: Epoch 36
2024-12-18 22:48:21.669066: Current learning rate: 0.00781
2024-12-18 22:54:50.259889: Validation loss did not improve from -0.57470. Patience: 16/50
2024-12-18 22:54:50.260808: train_loss -0.7024
2024-12-18 22:54:50.261622: val_loss -0.5506
2024-12-18 22:54:50.262367: Pseudo dice [0.7453]
2024-12-18 22:54:50.263100: Epoch time: 388.6 s
2024-12-18 22:54:50.263802: Yayy! New best EMA pseudo Dice: 0.7431
2024-12-18 22:54:52.344924: 
2024-12-18 22:54:52.345881: Epoch 37
2024-12-18 22:54:52.346560: Current learning rate: 0.00775
2024-12-18 23:02:06.315158: Validation loss did not improve from -0.57470. Patience: 17/50
2024-12-18 23:02:06.316183: train_loss -0.7086
2024-12-18 23:02:06.317756: val_loss -0.5349
2024-12-18 23:02:06.318512: Pseudo dice [0.7382]
2024-12-18 23:02:06.319456: Epoch time: 433.97 s
2024-12-18 23:02:08.663112: 
2024-12-18 23:02:08.665009: Epoch 38
2024-12-18 23:02:08.666005: Current learning rate: 0.00769
2024-12-18 23:08:19.517199: Validation loss did not improve from -0.57470. Patience: 18/50
2024-12-18 23:08:19.518801: train_loss -0.7089
2024-12-18 23:08:19.519744: val_loss -0.5473
2024-12-18 23:08:19.520366: Pseudo dice [0.7486]
2024-12-18 23:08:19.521158: Epoch time: 370.86 s
2024-12-18 23:08:19.521802: Yayy! New best EMA pseudo Dice: 0.7432
2024-12-18 23:08:21.353902: 
2024-12-18 23:08:21.356176: Epoch 39
2024-12-18 23:08:21.357514: Current learning rate: 0.00763
2024-12-18 23:14:52.432400: Validation loss did not improve from -0.57470. Patience: 19/50
2024-12-18 23:14:52.433748: train_loss -0.7127
2024-12-18 23:14:52.434627: val_loss -0.5459
2024-12-18 23:14:52.435366: Pseudo dice [0.7501]
2024-12-18 23:14:52.436171: Epoch time: 391.08 s
2024-12-18 23:14:52.920912: Yayy! New best EMA pseudo Dice: 0.7439
2024-12-18 23:14:54.758549: 
2024-12-18 23:14:54.759962: Epoch 40
2024-12-18 23:14:54.760961: Current learning rate: 0.00756
2024-12-18 23:21:23.727376: Validation loss did not improve from -0.57470. Patience: 20/50
2024-12-18 23:21:23.728574: train_loss -0.7088
2024-12-18 23:21:23.729310: val_loss -0.5504
2024-12-18 23:21:23.729929: Pseudo dice [0.7522]
2024-12-18 23:21:23.730648: Epoch time: 388.97 s
2024-12-18 23:21:23.731294: Yayy! New best EMA pseudo Dice: 0.7447
2024-12-18 23:21:25.621332: 
2024-12-18 23:21:25.622844: Epoch 41
2024-12-18 23:21:25.623571: Current learning rate: 0.0075
2024-12-18 23:28:16.613067: Validation loss did not improve from -0.57470. Patience: 21/50
2024-12-18 23:28:16.613737: train_loss -0.7148
2024-12-18 23:28:16.614919: val_loss -0.5641
2024-12-18 23:28:16.615910: Pseudo dice [0.7669]
2024-12-18 23:28:16.616756: Epoch time: 410.99 s
2024-12-18 23:28:16.617488: Yayy! New best EMA pseudo Dice: 0.747
2024-12-18 23:28:18.369071: 
2024-12-18 23:28:18.370788: Epoch 42
2024-12-18 23:28:18.371685: Current learning rate: 0.00744
2024-12-18 23:35:10.850601: Validation loss did not improve from -0.57470. Patience: 22/50
2024-12-18 23:35:10.851565: train_loss -0.71
2024-12-18 23:35:10.852258: val_loss -0.542
2024-12-18 23:35:10.852958: Pseudo dice [0.7451]
2024-12-18 23:35:10.853681: Epoch time: 412.48 s
2024-12-18 23:35:12.204291: 
2024-12-18 23:35:12.205967: Epoch 43
2024-12-18 23:35:12.206812: Current learning rate: 0.00738
2024-12-18 23:42:04.665029: Validation loss improved from -0.57470 to -0.58887! Patience: 22/50
2024-12-18 23:42:04.665918: train_loss -0.7125
2024-12-18 23:42:04.666779: val_loss -0.5889
2024-12-18 23:42:04.667753: Pseudo dice [0.7711]
2024-12-18 23:42:04.668606: Epoch time: 412.46 s
2024-12-18 23:42:04.669227: Yayy! New best EMA pseudo Dice: 0.7492
2024-12-18 23:42:06.422729: 
2024-12-18 23:42:06.424404: Epoch 44
2024-12-18 23:42:06.425207: Current learning rate: 0.00732
2024-12-18 23:48:45.607098: Validation loss did not improve from -0.58887. Patience: 1/50
2024-12-18 23:48:45.607938: train_loss -0.7199
2024-12-18 23:48:45.608657: val_loss -0.5304
2024-12-18 23:48:45.609222: Pseudo dice [0.7313]
2024-12-18 23:48:45.609823: Epoch time: 399.19 s
2024-12-18 23:48:47.284342: 
2024-12-18 23:48:47.285617: Epoch 45
2024-12-18 23:48:47.286266: Current learning rate: 0.00725
2024-12-18 23:55:44.519310: Validation loss did not improve from -0.58887. Patience: 2/50
2024-12-18 23:55:44.523074: train_loss -0.7242
2024-12-18 23:55:44.524182: val_loss -0.5555
2024-12-18 23:55:44.524869: Pseudo dice [0.7509]
2024-12-18 23:55:44.525716: Epoch time: 417.24 s
2024-12-18 23:55:45.841343: 
2024-12-18 23:55:45.843114: Epoch 46
2024-12-18 23:55:45.843820: Current learning rate: 0.00719
2024-12-19 00:02:56.232230: Validation loss did not improve from -0.58887. Patience: 3/50
2024-12-19 00:02:56.232862: train_loss -0.7188
2024-12-19 00:02:56.233717: val_loss -0.5642
2024-12-19 00:02:56.234813: Pseudo dice [0.7647]
2024-12-19 00:02:56.235973: Epoch time: 430.39 s
2024-12-19 00:02:56.236676: Yayy! New best EMA pseudo Dice: 0.7494
2024-12-19 00:02:57.948450: 
2024-12-19 00:02:57.949926: Epoch 47
2024-12-19 00:02:57.950694: Current learning rate: 0.00713
2024-12-19 00:09:18.167631: Validation loss did not improve from -0.58887. Patience: 4/50
2024-12-19 00:09:18.168650: train_loss -0.7191
2024-12-19 00:09:18.169971: val_loss -0.5737
2024-12-19 00:09:18.170623: Pseudo dice [0.7653]
2024-12-19 00:09:18.171685: Epoch time: 380.22 s
2024-12-19 00:09:18.172479: Yayy! New best EMA pseudo Dice: 0.751
2024-12-19 00:09:19.869515: 
2024-12-19 00:09:19.870618: Epoch 48
2024-12-19 00:09:19.871690: Current learning rate: 0.00707
2024-12-19 00:16:11.246507: Validation loss did not improve from -0.58887. Patience: 5/50
2024-12-19 00:16:11.247563: train_loss -0.7241
2024-12-19 00:16:11.248668: val_loss -0.5689
2024-12-19 00:16:11.249428: Pseudo dice [0.7668]
2024-12-19 00:16:11.250049: Epoch time: 411.38 s
2024-12-19 00:16:11.250715: Yayy! New best EMA pseudo Dice: 0.7526
2024-12-19 00:16:13.884453: 
2024-12-19 00:16:13.885808: Epoch 49
2024-12-19 00:16:13.886422: Current learning rate: 0.007
2024-12-19 00:23:12.612002: Validation loss did not improve from -0.58887. Patience: 6/50
2024-12-19 00:23:12.612753: train_loss -0.7351
2024-12-19 00:23:12.613416: val_loss -0.5453
2024-12-19 00:23:12.614039: Pseudo dice [0.7517]
2024-12-19 00:23:12.614742: Epoch time: 418.73 s
2024-12-19 00:23:14.315822: 
2024-12-19 00:23:14.317005: Epoch 50
2024-12-19 00:23:14.317636: Current learning rate: 0.00694
2024-12-19 00:30:10.662768: Validation loss did not improve from -0.58887. Patience: 7/50
2024-12-19 00:30:10.663709: train_loss -0.7282
2024-12-19 00:30:10.664635: val_loss -0.5719
2024-12-19 00:30:10.665456: Pseudo dice [0.7623]
2024-12-19 00:30:10.666481: Epoch time: 416.35 s
2024-12-19 00:30:10.667451: Yayy! New best EMA pseudo Dice: 0.7535
2024-12-19 00:30:12.422179: 
2024-12-19 00:30:12.423697: Epoch 51
2024-12-19 00:30:12.424613: Current learning rate: 0.00688
2024-12-19 00:37:02.479114: Validation loss did not improve from -0.58887. Patience: 8/50
2024-12-19 00:37:02.479935: train_loss -0.7325
2024-12-19 00:37:02.480725: val_loss -0.5441
2024-12-19 00:37:02.481441: Pseudo dice [0.7445]
2024-12-19 00:37:02.482208: Epoch time: 410.06 s
2024-12-19 00:37:03.819527: 
2024-12-19 00:37:03.820689: Epoch 52
2024-12-19 00:37:03.821400: Current learning rate: 0.00682
2024-12-19 00:43:27.545372: Validation loss did not improve from -0.58887. Patience: 9/50
2024-12-19 00:43:27.546003: train_loss -0.738
2024-12-19 00:43:27.546660: val_loss -0.5746
2024-12-19 00:43:27.547277: Pseudo dice [0.7645]
2024-12-19 00:43:27.547887: Epoch time: 383.73 s
2024-12-19 00:43:27.548556: Yayy! New best EMA pseudo Dice: 0.7538
2024-12-19 00:43:29.343671: 
2024-12-19 00:43:29.344785: Epoch 53
2024-12-19 00:43:29.345497: Current learning rate: 0.00675
2024-12-19 00:50:06.027626: Validation loss did not improve from -0.58887. Patience: 10/50
2024-12-19 00:50:06.028282: train_loss -0.7406
2024-12-19 00:50:06.029057: val_loss -0.5676
2024-12-19 00:50:06.029742: Pseudo dice [0.7646]
2024-12-19 00:50:06.030492: Epoch time: 396.69 s
2024-12-19 00:50:06.031264: Yayy! New best EMA pseudo Dice: 0.7549
2024-12-19 00:50:07.770471: 
2024-12-19 00:50:07.771571: Epoch 54
2024-12-19 00:50:07.772210: Current learning rate: 0.00669
2024-12-19 00:56:39.657257: Validation loss did not improve from -0.58887. Patience: 11/50
2024-12-19 00:56:39.660547: train_loss -0.7403
2024-12-19 00:56:39.661398: val_loss -0.5753
2024-12-19 00:56:39.662105: Pseudo dice [0.7666]
2024-12-19 00:56:39.663332: Epoch time: 391.89 s
2024-12-19 00:56:40.066582: Yayy! New best EMA pseudo Dice: 0.756
2024-12-19 00:56:41.819017: 
2024-12-19 00:56:41.820609: Epoch 55
2024-12-19 00:56:41.821930: Current learning rate: 0.00663
2024-12-19 01:03:02.397967: Validation loss did not improve from -0.58887. Patience: 12/50
2024-12-19 01:03:02.398724: train_loss -0.7342
2024-12-19 01:03:02.399453: val_loss -0.5604
2024-12-19 01:03:02.400207: Pseudo dice [0.7505]
2024-12-19 01:03:02.401024: Epoch time: 380.58 s
2024-12-19 01:03:03.722183: 
2024-12-19 01:03:03.723353: Epoch 56
2024-12-19 01:03:03.724137: Current learning rate: 0.00657
2024-12-19 01:09:46.355535: Validation loss did not improve from -0.58887. Patience: 13/50
2024-12-19 01:09:46.356443: train_loss -0.7374
2024-12-19 01:09:46.357373: val_loss -0.5721
2024-12-19 01:09:46.358111: Pseudo dice [0.7619]
2024-12-19 01:09:46.358847: Epoch time: 402.64 s
2024-12-19 01:09:46.359543: Yayy! New best EMA pseudo Dice: 0.7561
2024-12-19 01:09:48.155460: 
2024-12-19 01:09:48.156497: Epoch 57
2024-12-19 01:09:48.157253: Current learning rate: 0.0065
2024-12-19 01:16:27.024982: Validation loss did not improve from -0.58887. Patience: 14/50
2024-12-19 01:16:27.026202: train_loss -0.7414
2024-12-19 01:16:27.027623: val_loss -0.5609
2024-12-19 01:16:27.028229: Pseudo dice [0.7652]
2024-12-19 01:16:27.029031: Epoch time: 398.87 s
2024-12-19 01:16:27.029648: Yayy! New best EMA pseudo Dice: 0.757
2024-12-19 01:16:28.764144: 
2024-12-19 01:16:28.765353: Epoch 58
2024-12-19 01:16:28.765973: Current learning rate: 0.00644
2024-12-19 01:23:07.742330: Validation loss did not improve from -0.58887. Patience: 15/50
2024-12-19 01:23:07.743170: train_loss -0.7451
2024-12-19 01:23:07.743956: val_loss -0.5711
2024-12-19 01:23:07.744592: Pseudo dice [0.762]
2024-12-19 01:23:07.745292: Epoch time: 398.98 s
2024-12-19 01:23:07.745881: Yayy! New best EMA pseudo Dice: 0.7575
2024-12-19 01:23:09.506448: 
2024-12-19 01:23:09.507915: Epoch 59
2024-12-19 01:23:09.509139: Current learning rate: 0.00638
2024-12-19 01:29:45.603081: Validation loss did not improve from -0.58887. Patience: 16/50
2024-12-19 01:29:45.603852: train_loss -0.7419
2024-12-19 01:29:45.604647: val_loss -0.5514
2024-12-19 01:29:45.605861: Pseudo dice [0.7538]
2024-12-19 01:29:45.606764: Epoch time: 396.1 s
2024-12-19 01:29:50.340674: 
2024-12-19 01:29:50.342026: Epoch 60
2024-12-19 01:29:50.343375: Current learning rate: 0.00631
2024-12-19 01:36:21.173524: Validation loss did not improve from -0.58887. Patience: 17/50
2024-12-19 01:36:21.174473: train_loss -0.7446
2024-12-19 01:36:21.175121: val_loss -0.5659
2024-12-19 01:36:21.175733: Pseudo dice [0.7613]
2024-12-19 01:36:21.176344: Epoch time: 390.83 s
2024-12-19 01:36:21.176861: Yayy! New best EMA pseudo Dice: 0.7576
2024-12-19 01:36:23.031625: 
2024-12-19 01:36:23.032865: Epoch 61
2024-12-19 01:36:23.033493: Current learning rate: 0.00625
2024-12-19 01:43:16.890034: Validation loss did not improve from -0.58887. Patience: 18/50
2024-12-19 01:43:16.891056: train_loss -0.7469
2024-12-19 01:43:16.891869: val_loss -0.5366
2024-12-19 01:43:16.892560: Pseudo dice [0.7435]
2024-12-19 01:43:16.893111: Epoch time: 413.86 s
2024-12-19 01:43:18.229783: 
2024-12-19 01:43:18.230944: Epoch 62
2024-12-19 01:43:18.231713: Current learning rate: 0.00619
2024-12-19 01:49:53.708816: Validation loss did not improve from -0.58887. Patience: 19/50
2024-12-19 01:49:53.709653: train_loss -0.7448
2024-12-19 01:49:53.710258: val_loss -0.5617
2024-12-19 01:49:53.710824: Pseudo dice [0.756]
2024-12-19 01:49:53.711418: Epoch time: 395.48 s
2024-12-19 01:49:55.064370: 
2024-12-19 01:49:55.065895: Epoch 63
2024-12-19 01:49:55.066544: Current learning rate: 0.00612
2024-12-19 01:56:16.096624: Validation loss did not improve from -0.58887. Patience: 20/50
2024-12-19 01:56:16.097524: train_loss -0.7456
2024-12-19 01:56:16.098173: val_loss -0.5559
2024-12-19 01:56:16.099082: Pseudo dice [0.7459]
2024-12-19 01:56:16.099953: Epoch time: 381.03 s
2024-12-19 01:56:17.512532: 
2024-12-19 01:56:17.513910: Epoch 64
2024-12-19 01:56:17.514648: Current learning rate: 0.00606
2024-12-19 02:02:51.237618: Validation loss did not improve from -0.58887. Patience: 21/50
2024-12-19 02:02:51.241242: train_loss -0.7451
2024-12-19 02:02:51.242265: val_loss -0.5376
2024-12-19 02:02:51.242916: Pseudo dice [0.7466]
2024-12-19 02:02:51.243766: Epoch time: 393.73 s
2024-12-19 02:02:53.064643: 
2024-12-19 02:02:53.065986: Epoch 65
2024-12-19 02:02:53.067030: Current learning rate: 0.006
2024-12-19 02:09:06.429175: Validation loss did not improve from -0.58887. Patience: 22/50
2024-12-19 02:09:06.430097: train_loss -0.7455
2024-12-19 02:09:06.430876: val_loss -0.5699
2024-12-19 02:09:06.431507: Pseudo dice [0.7596]
2024-12-19 02:09:06.432168: Epoch time: 373.37 s
2024-12-19 02:09:07.836437: 
2024-12-19 02:09:07.837501: Epoch 66
2024-12-19 02:09:07.838197: Current learning rate: 0.00593
2024-12-19 02:15:32.994524: Validation loss did not improve from -0.58887. Patience: 23/50
2024-12-19 02:15:32.995406: train_loss -0.7509
2024-12-19 02:15:32.996090: val_loss -0.5532
2024-12-19 02:15:32.996666: Pseudo dice [0.7515]
2024-12-19 02:15:32.997257: Epoch time: 385.16 s
2024-12-19 02:15:34.348851: 
2024-12-19 02:15:34.349946: Epoch 67
2024-12-19 02:15:34.350642: Current learning rate: 0.00587
2024-12-19 02:21:48.949253: Validation loss did not improve from -0.58887. Patience: 24/50
2024-12-19 02:21:48.950624: train_loss -0.7562
2024-12-19 02:21:48.951617: val_loss -0.5326
2024-12-19 02:21:48.952418: Pseudo dice [0.7457]
2024-12-19 02:21:48.953090: Epoch time: 374.6 s
2024-12-19 02:21:50.315780: 
2024-12-19 02:21:50.317065: Epoch 68
2024-12-19 02:21:50.318029: Current learning rate: 0.00581
2024-12-19 02:28:24.174252: Validation loss did not improve from -0.58887. Patience: 25/50
2024-12-19 02:28:24.175130: train_loss -0.7485
2024-12-19 02:28:24.176004: val_loss -0.579
2024-12-19 02:28:24.176736: Pseudo dice [0.767]
2024-12-19 02:28:24.177774: Epoch time: 393.86 s
2024-12-19 02:28:25.578745: 
2024-12-19 02:28:25.579675: Epoch 69
2024-12-19 02:28:25.580552: Current learning rate: 0.00574
2024-12-19 02:34:48.127818: Validation loss did not improve from -0.58887. Patience: 26/50
2024-12-19 02:34:48.128790: train_loss -0.7548
2024-12-19 02:34:48.129823: val_loss -0.5395
2024-12-19 02:34:48.130480: Pseudo dice [0.7464]
2024-12-19 02:34:48.131204: Epoch time: 382.55 s
2024-12-19 02:34:50.044902: 
2024-12-19 02:34:50.046618: Epoch 70
2024-12-19 02:34:50.047504: Current learning rate: 0.00568
2024-12-19 02:41:33.418364: Validation loss did not improve from -0.58887. Patience: 27/50
2024-12-19 02:41:33.419382: train_loss -0.7543
2024-12-19 02:41:33.420257: val_loss -0.5536
2024-12-19 02:41:33.421031: Pseudo dice [0.7582]
2024-12-19 02:41:33.421724: Epoch time: 403.38 s
2024-12-19 02:41:36.308277: 
2024-12-19 02:41:36.309934: Epoch 71
2024-12-19 02:41:36.310914: Current learning rate: 0.00562
2024-12-19 02:48:18.952826: Validation loss did not improve from -0.58887. Patience: 28/50
2024-12-19 02:48:18.953775: train_loss -0.7579
2024-12-19 02:48:18.954627: val_loss -0.5572
2024-12-19 02:48:18.955317: Pseudo dice [0.7558]
2024-12-19 02:48:18.956008: Epoch time: 402.65 s
2024-12-19 02:48:20.478220: 
2024-12-19 02:48:20.479550: Epoch 72
2024-12-19 02:48:20.480434: Current learning rate: 0.00555
2024-12-19 02:55:06.275312: Validation loss did not improve from -0.58887. Patience: 29/50
2024-12-19 02:55:06.276311: train_loss -0.7562
2024-12-19 02:55:06.277422: val_loss -0.5425
2024-12-19 02:55:06.278190: Pseudo dice [0.7499]
2024-12-19 02:55:06.278952: Epoch time: 405.8 s
2024-12-19 02:55:07.736253: 
2024-12-19 02:55:07.751988: Epoch 73
2024-12-19 02:55:07.752773: Current learning rate: 0.00549
2024-12-19 03:01:53.538708: Validation loss did not improve from -0.58887. Patience: 30/50
2024-12-19 03:01:53.539347: train_loss -0.7565
2024-12-19 03:01:53.540027: val_loss -0.5388
2024-12-19 03:01:53.540817: Pseudo dice [0.7424]
2024-12-19 03:01:53.541468: Epoch time: 405.8 s
2024-12-19 03:01:54.964152: 
2024-12-19 03:01:54.965335: Epoch 74
2024-12-19 03:01:54.966062: Current learning rate: 0.00542
2024-12-19 03:08:57.797955: Validation loss did not improve from -0.58887. Patience: 31/50
2024-12-19 03:08:57.801337: train_loss -0.7577
2024-12-19 03:08:57.802497: val_loss -0.5813
2024-12-19 03:08:57.803428: Pseudo dice [0.7679]
2024-12-19 03:08:57.804355: Epoch time: 422.84 s
2024-12-19 03:08:59.641431: 
2024-12-19 03:08:59.642698: Epoch 75
2024-12-19 03:08:59.643487: Current learning rate: 0.00536
2024-12-19 03:15:26.037187: Validation loss did not improve from -0.58887. Patience: 32/50
2024-12-19 03:15:26.038908: train_loss -0.7603
2024-12-19 03:15:26.039917: val_loss -0.5585
2024-12-19 03:15:26.040548: Pseudo dice [0.7561]
2024-12-19 03:15:26.041539: Epoch time: 386.4 s
2024-12-19 03:15:27.521687: 
2024-12-19 03:15:27.522613: Epoch 76
2024-12-19 03:15:27.523316: Current learning rate: 0.00529
2024-12-19 03:22:04.938157: Validation loss did not improve from -0.58887. Patience: 33/50
2024-12-19 03:22:04.939018: train_loss -0.7624
2024-12-19 03:22:04.940642: val_loss -0.564
2024-12-19 03:22:04.941765: Pseudo dice [0.7618]
2024-12-19 03:22:04.942813: Epoch time: 397.42 s
2024-12-19 03:22:06.375571: 
2024-12-19 03:22:06.376618: Epoch 77
2024-12-19 03:22:06.377405: Current learning rate: 0.00523
2024-12-19 03:28:44.612396: Validation loss did not improve from -0.58887. Patience: 34/50
2024-12-19 03:28:44.613934: train_loss -0.7637
2024-12-19 03:28:44.614805: val_loss -0.5731
2024-12-19 03:28:44.615411: Pseudo dice [0.7613]
2024-12-19 03:28:44.616100: Epoch time: 398.24 s
2024-12-19 03:28:46.009648: 
2024-12-19 03:28:46.011006: Epoch 78
2024-12-19 03:28:46.011943: Current learning rate: 0.00517
2024-12-19 03:35:09.843052: Validation loss did not improve from -0.58887. Patience: 35/50
2024-12-19 03:35:09.843983: train_loss -0.7626
2024-12-19 03:35:09.845245: val_loss -0.5527
2024-12-19 03:35:09.846021: Pseudo dice [0.7516]
2024-12-19 03:35:09.846926: Epoch time: 383.84 s
2024-12-19 03:35:11.229810: 
2024-12-19 03:35:11.231461: Epoch 79
2024-12-19 03:35:11.232307: Current learning rate: 0.0051
2024-12-19 03:41:31.232399: Validation loss did not improve from -0.58887. Patience: 36/50
2024-12-19 03:41:31.233222: train_loss -0.7645
2024-12-19 03:41:31.233833: val_loss -0.5557
2024-12-19 03:41:31.234364: Pseudo dice [0.7577]
2024-12-19 03:41:31.234900: Epoch time: 380.0 s
2024-12-19 03:41:33.033045: 
2024-12-19 03:41:33.034115: Epoch 80
2024-12-19 03:41:33.034739: Current learning rate: 0.00504
2024-12-19 03:47:53.947775: Validation loss did not improve from -0.58887. Patience: 37/50
2024-12-19 03:47:53.948785: train_loss -0.764
2024-12-19 03:47:53.949669: val_loss -0.5331
2024-12-19 03:47:53.950287: Pseudo dice [0.7532]
2024-12-19 03:47:53.951221: Epoch time: 380.92 s
2024-12-19 03:47:55.996017: 
2024-12-19 03:47:55.997263: Epoch 81
2024-12-19 03:47:55.997975: Current learning rate: 0.00497
2024-12-19 03:54:28.809639: Validation loss did not improve from -0.58887. Patience: 38/50
2024-12-19 03:54:28.810425: train_loss -0.7683
2024-12-19 03:54:28.811193: val_loss -0.556
2024-12-19 03:54:28.811886: Pseudo dice [0.7624]
2024-12-19 03:54:28.812529: Epoch time: 392.82 s
2024-12-19 03:54:30.224188: 
2024-12-19 03:54:30.225377: Epoch 82
2024-12-19 03:54:30.226008: Current learning rate: 0.00491
2024-12-19 04:00:43.561342: Validation loss did not improve from -0.58887. Patience: 39/50
2024-12-19 04:00:43.562325: train_loss -0.7674
2024-12-19 04:00:43.563095: val_loss -0.5654
2024-12-19 04:00:43.563798: Pseudo dice [0.7645]
2024-12-19 04:00:43.564412: Epoch time: 373.34 s
2024-12-19 04:00:44.852662: 
2024-12-19 04:00:44.853901: Epoch 83
2024-12-19 04:00:44.854609: Current learning rate: 0.00484
2024-12-19 04:07:07.092506: Validation loss did not improve from -0.58887. Patience: 40/50
2024-12-19 04:07:07.093761: train_loss -0.7638
2024-12-19 04:07:07.094530: val_loss -0.5545
2024-12-19 04:07:07.095098: Pseudo dice [0.7531]
2024-12-19 04:07:07.095698: Epoch time: 382.24 s
2024-12-19 04:07:08.412757: 
2024-12-19 04:07:08.414082: Epoch 84
2024-12-19 04:07:08.414819: Current learning rate: 0.00478
2024-12-19 04:13:30.654393: Validation loss did not improve from -0.58887. Patience: 41/50
2024-12-19 04:13:30.657835: train_loss -0.7692
2024-12-19 04:13:30.658826: val_loss -0.5386
2024-12-19 04:13:30.659432: Pseudo dice [0.7528]
2024-12-19 04:13:30.660229: Epoch time: 382.25 s
2024-12-19 04:13:32.355106: 
2024-12-19 04:13:32.356114: Epoch 85
2024-12-19 04:13:32.356835: Current learning rate: 0.00471
2024-12-19 04:19:48.353423: Validation loss did not improve from -0.58887. Patience: 42/50
2024-12-19 04:19:48.354433: train_loss -0.7679
2024-12-19 04:19:48.355319: val_loss -0.5695
2024-12-19 04:19:48.355947: Pseudo dice [0.7636]
2024-12-19 04:19:48.356584: Epoch time: 376.0 s
2024-12-19 04:19:49.660901: 
2024-12-19 04:19:49.661782: Epoch 86
2024-12-19 04:19:49.662410: Current learning rate: 0.00465
2024-12-19 04:26:08.035117: Validation loss did not improve from -0.58887. Patience: 43/50
2024-12-19 04:26:08.036112: train_loss -0.7691
2024-12-19 04:26:08.037774: val_loss -0.5675
2024-12-19 04:26:08.038756: Pseudo dice [0.7594]
2024-12-19 04:26:08.040008: Epoch time: 378.38 s
2024-12-19 04:26:09.346165: 
2024-12-19 04:26:09.347365: Epoch 87
2024-12-19 04:26:09.348070: Current learning rate: 0.00458
2024-12-19 04:32:19.036023: Validation loss did not improve from -0.58887. Patience: 44/50
2024-12-19 04:32:19.037192: train_loss -0.767
2024-12-19 04:32:19.038031: val_loss -0.564
2024-12-19 04:32:19.038594: Pseudo dice [0.7621]
2024-12-19 04:32:19.039229: Epoch time: 369.69 s
2024-12-19 04:32:19.039807: Yayy! New best EMA pseudo Dice: 0.7577
2024-12-19 04:32:20.748294: 
2024-12-19 04:32:20.749536: Epoch 88
2024-12-19 04:32:20.750254: Current learning rate: 0.00452
2024-12-19 04:37:55.162677: Validation loss did not improve from -0.58887. Patience: 45/50
2024-12-19 04:37:55.163529: train_loss -0.7687
2024-12-19 04:37:55.164152: val_loss -0.5431
2024-12-19 04:37:55.164804: Pseudo dice [0.7476]
2024-12-19 04:37:55.165399: Epoch time: 334.42 s
2024-12-19 04:37:56.513951: 
2024-12-19 04:37:56.515156: Epoch 89
2024-12-19 04:37:56.516303: Current learning rate: 0.00445
2024-12-19 04:43:45.514914: Validation loss did not improve from -0.58887. Patience: 46/50
2024-12-19 04:43:45.515810: train_loss -0.772
2024-12-19 04:43:45.516556: val_loss -0.5681
2024-12-19 04:43:45.517150: Pseudo dice [0.7649]
2024-12-19 04:43:45.517781: Epoch time: 349.0 s
2024-12-19 04:43:47.223948: 
2024-12-19 04:43:47.225495: Epoch 90
2024-12-19 04:43:47.226271: Current learning rate: 0.00438
2024-12-19 04:49:42.695288: Validation loss did not improve from -0.58887. Patience: 47/50
2024-12-19 04:49:42.695999: train_loss -0.7715
2024-12-19 04:49:42.696902: val_loss -0.5445
2024-12-19 04:49:42.697806: Pseudo dice [0.7454]
2024-12-19 04:49:42.698657: Epoch time: 355.47 s
2024-12-19 04:49:44.000681: 
2024-12-19 04:49:44.001865: Epoch 91
2024-12-19 04:49:44.002525: Current learning rate: 0.00432
2024-12-19 04:55:39.432552: Validation loss did not improve from -0.58887. Patience: 48/50
2024-12-19 04:55:39.433204: train_loss -0.7671
2024-12-19 04:55:39.434049: val_loss -0.5672
2024-12-19 04:55:39.434641: Pseudo dice [0.7557]
2024-12-19 04:55:39.435261: Epoch time: 355.43 s
2024-12-19 04:55:40.802266: 
2024-12-19 04:55:40.803556: Epoch 92
2024-12-19 04:55:40.804310: Current learning rate: 0.00425
2024-12-19 05:01:19.782291: Validation loss did not improve from -0.58887. Patience: 49/50
2024-12-19 05:01:19.783440: train_loss -0.7693
2024-12-19 05:01:19.784372: val_loss -0.5398
2024-12-19 05:01:19.784991: Pseudo dice [0.7426]
2024-12-19 05:01:19.785595: Epoch time: 338.98 s
2024-12-19 05:01:22.121863: 
2024-12-19 05:01:22.122863: Epoch 93
2024-12-19 05:01:22.123520: Current learning rate: 0.00419
2024-12-19 05:07:15.614196: Validation loss did not improve from -0.58887. Patience: 50/50
2024-12-19 05:07:15.614986: train_loss -0.7705
2024-12-19 05:07:15.615638: val_loss -0.5569
2024-12-19 05:07:15.616215: Pseudo dice [0.7582]
2024-12-19 05:07:15.616820: Epoch time: 353.49 s
2024-12-19 05:07:16.977283: 
2024-12-19 05:07:16.978320: Epoch 94
2024-12-19 05:07:16.978991: Current learning rate: 0.00412
2024-12-19 05:13:04.996057: Validation loss did not improve from -0.58887. Patience: 51/50
2024-12-19 05:13:04.996924: train_loss -0.7734
2024-12-19 05:13:04.997627: val_loss -0.5227
2024-12-19 05:13:04.998238: Pseudo dice [0.7423]
2024-12-19 05:13:04.998893: Epoch time: 348.02 s
2024-12-19 05:13:06.787371: 
2024-12-19 05:13:06.788462: Epoch 95
2024-12-19 05:13:06.789119: Current learning rate: 0.00405
2024-12-19 05:18:37.238222: Validation loss did not improve from -0.58887. Patience: 52/50
2024-12-19 05:18:37.242313: train_loss -0.7753
2024-12-19 05:18:37.243432: val_loss -0.582
2024-12-19 05:18:37.244044: Pseudo dice [0.7693]
2024-12-19 05:18:37.244862: Epoch time: 330.45 s
2024-12-19 05:18:38.584428: 
2024-12-19 05:18:38.585465: Epoch 96
2024-12-19 05:18:38.586108: Current learning rate: 0.00399
2024-12-19 05:24:35.984011: Validation loss did not improve from -0.58887. Patience: 53/50
2024-12-19 05:24:35.984873: train_loss -0.7757
2024-12-19 05:24:35.985681: val_loss -0.547
2024-12-19 05:24:35.986334: Pseudo dice [0.755]
2024-12-19 05:24:35.986930: Epoch time: 357.4 s
2024-12-19 05:24:37.389281: 
2024-12-19 05:24:37.390690: Epoch 97
2024-12-19 05:24:37.391547: Current learning rate: 0.00392
2024-12-19 05:29:52.626066: Validation loss did not improve from -0.58887. Patience: 54/50
2024-12-19 05:29:52.627005: train_loss -0.7786
2024-12-19 05:29:52.627914: val_loss -0.5602
2024-12-19 05:29:52.628609: Pseudo dice [0.7604]
2024-12-19 05:29:52.629210: Epoch time: 315.24 s
2024-12-19 05:29:53.970954: 
2024-12-19 05:29:53.972120: Epoch 98
2024-12-19 05:29:53.973386: Current learning rate: 0.00385
2024-12-19 05:35:13.783444: Validation loss did not improve from -0.58887. Patience: 55/50
2024-12-19 05:35:13.784850: train_loss -0.7793
2024-12-19 05:35:13.786579: val_loss -0.5581
2024-12-19 05:35:13.787539: Pseudo dice [0.7622]
2024-12-19 05:35:13.788305: Epoch time: 319.82 s
2024-12-19 05:35:15.135531: 
2024-12-19 05:35:15.136698: Epoch 99
2024-12-19 05:35:15.137313: Current learning rate: 0.00379
2024-12-19 05:40:19.308082: Validation loss did not improve from -0.58887. Patience: 56/50
2024-12-19 05:40:19.309479: train_loss -0.778
2024-12-19 05:40:19.310652: val_loss -0.5679
2024-12-19 05:40:19.311275: Pseudo dice [0.7579]
2024-12-19 05:40:19.312001: Epoch time: 304.18 s
2024-12-19 05:40:21.181017: 
2024-12-19 05:40:21.182019: Epoch 100
2024-12-19 05:40:21.182814: Current learning rate: 0.00372
2024-12-19 05:45:38.743973: Validation loss did not improve from -0.58887. Patience: 57/50
2024-12-19 05:45:38.744714: train_loss -0.7802
2024-12-19 05:45:38.745567: val_loss -0.5773
2024-12-19 05:45:38.746259: Pseudo dice [0.7681]
2024-12-19 05:45:38.746919: Epoch time: 317.56 s
2024-12-19 05:45:38.747773: Yayy! New best EMA pseudo Dice: 0.7578
2024-12-19 05:45:40.446713: 
2024-12-19 05:45:40.448071: Epoch 101
2024-12-19 05:45:40.448824: Current learning rate: 0.00365
2024-12-19 05:50:41.649401: Validation loss did not improve from -0.58887. Patience: 58/50
2024-12-19 05:50:41.650224: train_loss -0.7769
2024-12-19 05:50:41.651107: val_loss -0.545
2024-12-19 05:50:41.651749: Pseudo dice [0.7544]
2024-12-19 05:50:41.652351: Epoch time: 301.2 s
2024-12-19 05:50:42.991863: 
2024-12-19 05:50:42.992944: Epoch 102
2024-12-19 05:50:42.993632: Current learning rate: 0.00359
2024-12-19 05:55:55.411275: Validation loss did not improve from -0.58887. Patience: 59/50
2024-12-19 05:55:55.412029: train_loss -0.7777
2024-12-19 05:55:55.412642: val_loss -0.569
2024-12-19 05:55:55.413267: Pseudo dice [0.7668]
2024-12-19 05:55:55.413857: Epoch time: 312.42 s
2024-12-19 05:55:55.414442: Yayy! New best EMA pseudo Dice: 0.7584
2024-12-19 05:55:57.131803: 
2024-12-19 05:55:57.132874: Epoch 103
2024-12-19 05:55:57.133654: Current learning rate: 0.00352
2024-12-19 06:00:51.041122: Validation loss did not improve from -0.58887. Patience: 60/50
2024-12-19 06:00:51.042024: train_loss -0.7845
2024-12-19 06:00:51.042881: val_loss -0.5496
2024-12-19 06:00:51.043634: Pseudo dice [0.7523]
2024-12-19 06:00:51.044282: Epoch time: 293.91 s
2024-12-19 06:00:53.352439: 
2024-12-19 06:00:53.353781: Epoch 104
2024-12-19 06:00:53.354501: Current learning rate: 0.00345
2024-12-19 06:05:59.401685: Validation loss did not improve from -0.58887. Patience: 61/50
2024-12-19 06:05:59.402597: train_loss -0.7823
2024-12-19 06:05:59.403371: val_loss -0.5779
2024-12-19 06:05:59.404024: Pseudo dice [0.7721]
2024-12-19 06:05:59.404651: Epoch time: 306.05 s
2024-12-19 06:05:59.840312: Yayy! New best EMA pseudo Dice: 0.7592
2024-12-19 06:06:01.585014: 
2024-12-19 06:06:01.585946: Epoch 105
2024-12-19 06:06:01.586655: Current learning rate: 0.00338
2024-12-19 06:10:54.447481: Validation loss did not improve from -0.58887. Patience: 62/50
2024-12-19 06:10:54.448434: train_loss -0.7861
2024-12-19 06:10:54.449183: val_loss -0.5568
2024-12-19 06:10:54.449990: Pseudo dice [0.7581]
2024-12-19 06:10:54.450688: Epoch time: 292.86 s
2024-12-19 06:10:55.805434: 
2024-12-19 06:10:55.807007: Epoch 106
2024-12-19 06:10:55.808233: Current learning rate: 0.00332
2024-12-19 06:16:06.905666: Validation loss did not improve from -0.58887. Patience: 63/50
2024-12-19 06:16:06.906564: train_loss -0.7847
2024-12-19 06:16:06.907253: val_loss -0.5481
2024-12-19 06:16:06.907934: Pseudo dice [0.751]
2024-12-19 06:16:06.908567: Epoch time: 311.1 s
2024-12-19 06:16:08.264537: 
2024-12-19 06:16:08.265631: Epoch 107
2024-12-19 06:16:08.266238: Current learning rate: 0.00325
2024-12-19 06:21:24.156010: Validation loss did not improve from -0.58887. Patience: 64/50
2024-12-19 06:21:24.159697: train_loss -0.7843
2024-12-19 06:21:24.160903: val_loss -0.5696
2024-12-19 06:21:24.161834: Pseudo dice [0.7669]
2024-12-19 06:21:24.162975: Epoch time: 315.9 s
2024-12-19 06:21:25.568906: 
2024-12-19 06:21:25.570202: Epoch 108
2024-12-19 06:21:25.571060: Current learning rate: 0.00318
2024-12-19 06:26:43.385483: Validation loss did not improve from -0.58887. Patience: 65/50
2024-12-19 06:26:43.386405: train_loss -0.7841
2024-12-19 06:26:43.387436: val_loss -0.5634
2024-12-19 06:26:43.388163: Pseudo dice [0.7594]
2024-12-19 06:26:43.389011: Epoch time: 317.82 s
2024-12-19 06:26:44.731457: 
2024-12-19 06:26:44.732594: Epoch 109
2024-12-19 06:26:44.733470: Current learning rate: 0.00311
2024-12-19 06:32:10.800413: Validation loss did not improve from -0.58887. Patience: 66/50
2024-12-19 06:32:10.801422: train_loss -0.7821
2024-12-19 06:32:10.802117: val_loss -0.5462
2024-12-19 06:32:10.802880: Pseudo dice [0.7517]
2024-12-19 06:32:10.803652: Epoch time: 326.07 s
2024-12-19 06:32:12.587703: 
2024-12-19 06:32:12.588912: Epoch 110
2024-12-19 06:32:12.589614: Current learning rate: 0.00304
2024-12-19 06:37:31.579332: Validation loss did not improve from -0.58887. Patience: 67/50
2024-12-19 06:37:31.580301: train_loss -0.7855
2024-12-19 06:37:31.581573: val_loss -0.5487
2024-12-19 06:37:31.582132: Pseudo dice [0.7573]
2024-12-19 06:37:31.582901: Epoch time: 318.99 s
2024-12-19 06:37:32.944084: 
2024-12-19 06:37:32.945195: Epoch 111
2024-12-19 06:37:32.945819: Current learning rate: 0.00297
2024-12-19 06:40:58.754380: Validation loss did not improve from -0.58887. Patience: 68/50
2024-12-19 06:40:58.755369: train_loss -0.7904
2024-12-19 06:40:58.756378: val_loss -0.555
2024-12-19 06:40:58.757183: Pseudo dice [0.7605]
2024-12-19 06:40:58.757985: Epoch time: 205.81 s
2024-12-19 06:41:00.196921: 
2024-12-19 06:41:00.198406: Epoch 112
2024-12-19 06:41:00.199380: Current learning rate: 0.00291
2024-12-19 06:43:58.151529: Validation loss did not improve from -0.58887. Patience: 69/50
2024-12-19 06:43:58.152460: train_loss -0.7867
2024-12-19 06:43:58.153516: val_loss -0.5254
2024-12-19 06:43:58.154366: Pseudo dice [0.7493]
2024-12-19 06:43:58.155143: Epoch time: 177.96 s
2024-12-19 06:43:59.529029: 
2024-12-19 06:43:59.530622: Epoch 113
2024-12-19 06:43:59.531646: Current learning rate: 0.00284
2024-12-19 06:47:05.791883: Validation loss did not improve from -0.58887. Patience: 70/50
2024-12-19 06:47:05.792862: train_loss -0.7895
2024-12-19 06:47:05.793896: val_loss -0.5418
2024-12-19 06:47:05.794857: Pseudo dice [0.7477]
2024-12-19 06:47:05.795758: Epoch time: 186.26 s
2024-12-19 06:47:07.218098: 
2024-12-19 06:47:07.219619: Epoch 114
2024-12-19 06:47:07.220637: Current learning rate: 0.00277
2024-12-19 06:50:15.883972: Validation loss did not improve from -0.58887. Patience: 71/50
2024-12-19 06:50:15.884774: train_loss -0.7854
2024-12-19 06:50:15.885611: val_loss -0.5782
2024-12-19 06:50:15.886381: Pseudo dice [0.7758]
2024-12-19 06:50:15.887229: Epoch time: 188.67 s
2024-12-19 06:50:18.128397: 
2024-12-19 06:50:18.129656: Epoch 115
2024-12-19 06:50:18.130356: Current learning rate: 0.0027
2024-12-19 06:53:55.740092: Validation loss did not improve from -0.58887. Patience: 72/50
2024-12-19 06:53:55.740767: train_loss -0.7893
2024-12-19 06:53:55.741444: val_loss -0.5682
2024-12-19 06:53:55.742116: Pseudo dice [0.764]
2024-12-19 06:53:55.742705: Epoch time: 217.61 s
2024-12-19 06:53:57.127638: 
2024-12-19 06:53:57.128821: Epoch 116
2024-12-19 06:53:57.129501: Current learning rate: 0.00263
2024-12-19 06:57:20.850874: Validation loss did not improve from -0.58887. Patience: 73/50
2024-12-19 06:57:20.852055: train_loss -0.7872
2024-12-19 06:57:20.852929: val_loss -0.5814
2024-12-19 06:57:20.853750: Pseudo dice [0.769]
2024-12-19 06:57:20.854504: Epoch time: 203.73 s
2024-12-19 06:57:20.855193: Yayy! New best EMA pseudo Dice: 0.7601
2024-12-19 06:57:22.624383: 
2024-12-19 06:57:22.625925: Epoch 117
2024-12-19 06:57:22.626615: Current learning rate: 0.00256
2024-12-19 07:01:01.948837: Validation loss did not improve from -0.58887. Patience: 74/50
2024-12-19 07:01:01.949794: train_loss -0.7881
2024-12-19 07:01:01.950576: val_loss -0.5471
2024-12-19 07:01:01.951374: Pseudo dice [0.7532]
2024-12-19 07:01:01.952110: Epoch time: 219.33 s
2024-12-19 07:01:03.342744: 
2024-12-19 07:01:03.344264: Epoch 118
2024-12-19 07:01:03.345255: Current learning rate: 0.00249
2024-12-19 07:04:11.540416: Validation loss did not improve from -0.58887. Patience: 75/50
2024-12-19 07:04:11.541214: train_loss -0.7903
2024-12-19 07:04:11.542102: val_loss -0.5624
2024-12-19 07:04:11.542938: Pseudo dice [0.7609]
2024-12-19 07:04:11.543753: Epoch time: 188.2 s
2024-12-19 07:04:13.013172: 
2024-12-19 07:04:13.014184: Epoch 119
2024-12-19 07:04:13.014936: Current learning rate: 0.00242
2024-12-19 07:07:41.738848: Validation loss did not improve from -0.58887. Patience: 76/50
2024-12-19 07:07:41.740204: train_loss -0.7913
2024-12-19 07:07:41.741141: val_loss -0.5615
2024-12-19 07:07:41.741918: Pseudo dice [0.7647]
2024-12-19 07:07:41.742766: Epoch time: 208.73 s
2024-12-19 07:07:43.568029: 
2024-12-19 07:07:43.569075: Epoch 120
2024-12-19 07:07:43.569732: Current learning rate: 0.00235
2024-12-19 07:11:20.234596: Validation loss did not improve from -0.58887. Patience: 77/50
2024-12-19 07:11:20.235479: train_loss -0.7937
2024-12-19 07:11:20.236426: val_loss -0.5553
2024-12-19 07:11:20.237471: Pseudo dice [0.7531]
2024-12-19 07:11:20.238338: Epoch time: 216.67 s
2024-12-19 07:11:21.732634: 
2024-12-19 07:11:21.734140: Epoch 121
2024-12-19 07:11:21.735357: Current learning rate: 0.00228
2024-12-19 07:15:12.175802: Validation loss did not improve from -0.58887. Patience: 78/50
2024-12-19 07:15:12.176587: train_loss -0.7924
2024-12-19 07:15:12.177320: val_loss -0.5562
2024-12-19 07:15:12.177916: Pseudo dice [0.7621]
2024-12-19 07:15:12.178634: Epoch time: 230.45 s
2024-12-19 07:15:13.648549: 
2024-12-19 07:15:13.649733: Epoch 122
2024-12-19 07:15:13.650457: Current learning rate: 0.00221
2024-12-19 07:18:46.664695: Validation loss did not improve from -0.58887. Patience: 79/50
2024-12-19 07:18:46.665847: train_loss -0.7944
2024-12-19 07:18:46.666698: val_loss -0.5371
2024-12-19 07:18:46.667433: Pseudo dice [0.7462]
2024-12-19 07:18:46.668149: Epoch time: 213.02 s
2024-12-19 07:18:48.187752: 
2024-12-19 07:18:48.189328: Epoch 123
2024-12-19 07:18:48.190356: Current learning rate: 0.00214
2024-12-19 07:22:27.853957: Validation loss did not improve from -0.58887. Patience: 80/50
2024-12-19 07:22:27.854892: train_loss -0.7943
2024-12-19 07:22:27.855924: val_loss -0.5577
2024-12-19 07:22:27.856683: Pseudo dice [0.7599]
2024-12-19 07:22:27.857501: Epoch time: 219.67 s
2024-12-19 07:22:29.304010: 
2024-12-19 07:22:29.305307: Epoch 124
2024-12-19 07:22:29.306105: Current learning rate: 0.00207
2024-12-19 07:26:03.793944: Validation loss did not improve from -0.58887. Patience: 81/50
2024-12-19 07:26:03.797884: train_loss -0.7942
2024-12-19 07:26:03.799120: val_loss -0.5242
2024-12-19 07:26:03.800003: Pseudo dice [0.7326]
2024-12-19 07:26:03.801097: Epoch time: 214.49 s
2024-12-19 07:26:06.188024: 
2024-12-19 07:26:06.189538: Epoch 125
2024-12-19 07:26:06.190550: Current learning rate: 0.00199
2024-12-19 07:29:30.650619: Validation loss did not improve from -0.58887. Patience: 82/50
2024-12-19 07:29:30.651625: train_loss -0.796
2024-12-19 07:29:30.652351: val_loss -0.5607
2024-12-19 07:29:30.653000: Pseudo dice [0.7584]
2024-12-19 07:29:30.653748: Epoch time: 204.46 s
2024-12-19 07:29:32.074372: 
2024-12-19 07:29:32.076039: Epoch 126
2024-12-19 07:29:32.076756: Current learning rate: 0.00192
2024-12-19 07:33:02.199731: Validation loss did not improve from -0.58887. Patience: 83/50
2024-12-19 07:33:02.200713: train_loss -0.7961
2024-12-19 07:33:02.201427: val_loss -0.5248
2024-12-19 07:33:02.202144: Pseudo dice [0.7508]
2024-12-19 07:33:02.202749: Epoch time: 210.13 s
2024-12-19 07:33:03.645390: 
2024-12-19 07:33:03.646327: Epoch 127
2024-12-19 07:33:03.646925: Current learning rate: 0.00185
2024-12-19 07:36:41.129872: Validation loss did not improve from -0.58887. Patience: 84/50
2024-12-19 07:36:41.130746: train_loss -0.7927
2024-12-19 07:36:41.131570: val_loss -0.5597
2024-12-19 07:36:41.132175: Pseudo dice [0.7592]
2024-12-19 07:36:41.132776: Epoch time: 217.49 s
2024-12-19 07:36:42.529024: 
2024-12-19 07:36:42.530189: Epoch 128
2024-12-19 07:36:42.530841: Current learning rate: 0.00178
2024-12-19 07:40:02.678662: Validation loss did not improve from -0.58887. Patience: 85/50
2024-12-19 07:40:02.679544: train_loss -0.7966
2024-12-19 07:40:02.680365: val_loss -0.5483
2024-12-19 07:40:02.681004: Pseudo dice [0.7462]
2024-12-19 07:40:02.681641: Epoch time: 200.15 s
2024-12-19 07:40:04.076330: 
2024-12-19 07:40:04.077271: Epoch 129
2024-12-19 07:40:04.077945: Current learning rate: 0.0017
2024-12-19 07:43:44.709052: Validation loss did not improve from -0.58887. Patience: 86/50
2024-12-19 07:43:44.709997: train_loss -0.7964
2024-12-19 07:43:44.711042: val_loss -0.5586
2024-12-19 07:43:44.711987: Pseudo dice [0.7586]
2024-12-19 07:43:44.712856: Epoch time: 220.64 s
2024-12-19 07:43:46.440631: 
2024-12-19 07:43:46.442291: Epoch 130
2024-12-19 07:43:46.443280: Current learning rate: 0.00163
2024-12-19 07:47:23.135030: Validation loss did not improve from -0.58887. Patience: 87/50
2024-12-19 07:47:23.136335: train_loss -0.7971
2024-12-19 07:47:23.137097: val_loss -0.5614
2024-12-19 07:47:23.137925: Pseudo dice [0.762]
2024-12-19 07:47:23.138772: Epoch time: 216.7 s
2024-12-19 07:47:24.557488: 
2024-12-19 07:47:24.558817: Epoch 131
2024-12-19 07:47:24.559664: Current learning rate: 0.00156
2024-12-19 07:50:21.545118: Validation loss did not improve from -0.58887. Patience: 88/50
2024-12-19 07:50:21.546083: train_loss -0.7981
2024-12-19 07:50:21.546978: val_loss -0.5476
2024-12-19 07:50:21.547814: Pseudo dice [0.7615]
2024-12-19 07:50:21.548735: Epoch time: 176.99 s
2024-12-19 07:50:23.102803: 
2024-12-19 07:50:23.104690: Epoch 132
2024-12-19 07:50:23.106354: Current learning rate: 0.00148
2024-12-19 07:53:58.911790: Validation loss did not improve from -0.58887. Patience: 89/50
2024-12-19 07:53:58.912628: train_loss -0.7972
2024-12-19 07:53:58.913442: val_loss -0.5768
2024-12-19 07:53:58.914072: Pseudo dice [0.7708]
2024-12-19 07:53:58.914857: Epoch time: 215.81 s
2024-12-19 07:54:00.354818: 
2024-12-19 07:54:00.355966: Epoch 133
2024-12-19 07:54:00.356684: Current learning rate: 0.00141
2024-12-19 07:57:38.996741: Validation loss did not improve from -0.58887. Patience: 90/50
2024-12-19 07:57:38.997768: train_loss -0.8002
2024-12-19 07:57:38.998684: val_loss -0.5477
2024-12-19 07:57:38.999573: Pseudo dice [0.7557]
2024-12-19 07:57:39.000366: Epoch time: 218.64 s
2024-12-19 07:57:40.482615: 
2024-12-19 07:57:40.483894: Epoch 134
2024-12-19 07:57:40.484812: Current learning rate: 0.00133
2024-12-19 08:00:58.255203: Validation loss did not improve from -0.58887. Patience: 91/50
2024-12-19 08:00:58.256141: train_loss -0.7992
2024-12-19 08:00:58.256907: val_loss -0.5498
2024-12-19 08:00:58.257761: Pseudo dice [0.7558]
2024-12-19 08:00:58.258479: Epoch time: 197.77 s
2024-12-19 08:01:00.108664: 
2024-12-19 08:01:00.109813: Epoch 135
2024-12-19 08:01:00.110530: Current learning rate: 0.00126
2024-12-19 08:04:40.691960: Validation loss did not improve from -0.58887. Patience: 92/50
2024-12-19 08:04:40.692777: train_loss -0.8002
2024-12-19 08:04:40.693565: val_loss -0.5579
2024-12-19 08:04:40.694214: Pseudo dice [0.7607]
2024-12-19 08:04:40.694876: Epoch time: 220.59 s
2024-12-19 08:04:42.714981: 
2024-12-19 08:04:42.716829: Epoch 136
2024-12-19 08:04:42.717589: Current learning rate: 0.00118
2024-12-19 08:07:59.508967: Validation loss did not improve from -0.58887. Patience: 93/50
2024-12-19 08:07:59.509700: train_loss -0.7991
2024-12-19 08:07:59.510521: val_loss -0.5428
2024-12-19 08:07:59.511296: Pseudo dice [0.7505]
2024-12-19 08:07:59.512014: Epoch time: 196.8 s
2024-12-19 08:08:00.955276: 
2024-12-19 08:08:00.957082: Epoch 137
2024-12-19 08:08:00.957898: Current learning rate: 0.00111
2024-12-19 08:11:36.099436: Validation loss did not improve from -0.58887. Patience: 94/50
2024-12-19 08:11:36.101221: train_loss -0.8017
2024-12-19 08:11:36.102181: val_loss -0.5385
2024-12-19 08:11:36.102791: Pseudo dice [0.7519]
2024-12-19 08:11:36.103450: Epoch time: 215.15 s
2024-12-19 08:11:37.481270: 
2024-12-19 08:11:37.483034: Epoch 138
2024-12-19 08:11:37.483832: Current learning rate: 0.00103
2024-12-19 08:15:14.704531: Validation loss did not improve from -0.58887. Patience: 95/50
2024-12-19 08:15:14.705574: train_loss -0.8016
2024-12-19 08:15:14.706344: val_loss -0.5448
2024-12-19 08:15:14.706925: Pseudo dice [0.7605]
2024-12-19 08:15:14.707542: Epoch time: 217.23 s
2024-12-19 08:15:16.111712: 
2024-12-19 08:15:16.113341: Epoch 139
2024-12-19 08:15:16.114246: Current learning rate: 0.00095
2024-12-19 08:18:24.960011: Validation loss did not improve from -0.58887. Patience: 96/50
2024-12-19 08:18:24.960912: train_loss -0.8012
2024-12-19 08:18:24.961750: val_loss -0.558
2024-12-19 08:18:24.962503: Pseudo dice [0.764]
2024-12-19 08:18:24.963148: Epoch time: 188.85 s
2024-12-19 08:18:26.938289: 
2024-12-19 08:18:26.940002: Epoch 140
2024-12-19 08:18:26.941011: Current learning rate: 0.00087
2024-12-19 08:21:59.991833: Validation loss did not improve from -0.58887. Patience: 97/50
2024-12-19 08:21:59.992617: train_loss -0.8026
2024-12-19 08:21:59.993402: val_loss -0.5315
2024-12-19 08:21:59.994095: Pseudo dice [0.743]
2024-12-19 08:21:59.994762: Epoch time: 213.06 s
2024-12-19 08:22:01.463244: 
2024-12-19 08:22:01.466002: Epoch 141
2024-12-19 08:22:01.467820: Current learning rate: 0.00079
2024-12-19 08:25:45.868787: Validation loss did not improve from -0.58887. Patience: 98/50
2024-12-19 08:25:45.870232: train_loss -0.8013
2024-12-19 08:25:45.871111: val_loss -0.5739
2024-12-19 08:25:45.871884: Pseudo dice [0.7683]
2024-12-19 08:25:45.872745: Epoch time: 224.41 s
2024-12-19 08:25:47.310544: 
2024-12-19 08:25:47.312206: Epoch 142
2024-12-19 08:25:47.312938: Current learning rate: 0.00071
2024-12-19 08:29:02.351979: Validation loss did not improve from -0.58887. Patience: 99/50
2024-12-19 08:29:02.353369: train_loss -0.8033
2024-12-19 08:29:02.354215: val_loss -0.5763
2024-12-19 08:29:02.355021: Pseudo dice [0.7707]
2024-12-19 08:29:02.355824: Epoch time: 195.04 s
2024-12-19 08:29:03.893779: 
2024-12-19 08:29:03.895486: Epoch 143
2024-12-19 08:29:03.896281: Current learning rate: 0.00063
2024-12-19 08:32:38.324309: Validation loss did not improve from -0.58887. Patience: 100/50
2024-12-19 08:32:38.328172: train_loss -0.8011
2024-12-19 08:32:38.329378: val_loss -0.5565
2024-12-19 08:32:38.330245: Pseudo dice [0.7621]
2024-12-19 08:32:38.331052: Epoch time: 214.44 s
2024-12-19 08:32:39.780304: 
2024-12-19 08:32:39.781866: Epoch 144
2024-12-19 08:32:39.782846: Current learning rate: 0.00055
2024-12-19 08:36:11.957327: Validation loss did not improve from -0.58887. Patience: 101/50
2024-12-19 08:36:11.958167: train_loss -0.8028
2024-12-19 08:36:11.958827: val_loss -0.563
2024-12-19 08:36:11.959487: Pseudo dice [0.7639]
2024-12-19 08:36:11.960156: Epoch time: 212.18 s
2024-12-19 08:36:13.809539: 
2024-12-19 08:36:13.811339: Epoch 145
2024-12-19 08:36:13.812107: Current learning rate: 0.00047
2024-12-19 08:39:37.248980: Validation loss did not improve from -0.58887. Patience: 102/50
2024-12-19 08:39:37.249863: train_loss -0.8002
2024-12-19 08:39:37.250725: val_loss -0.5683
2024-12-19 08:39:37.251710: Pseudo dice [0.7653]
2024-12-19 08:39:37.252390: Epoch time: 203.44 s
2024-12-19 08:39:37.252956: Yayy! New best EMA pseudo Dice: 0.7601
2024-12-19 08:39:39.052009: 
2024-12-19 08:39:39.053079: Epoch 146
2024-12-19 08:39:39.053772: Current learning rate: 0.00038
2024-12-19 08:43:14.508440: Validation loss did not improve from -0.58887. Patience: 103/50
2024-12-19 08:43:14.509518: train_loss -0.8023
2024-12-19 08:43:14.510532: val_loss -0.5147
2024-12-19 08:43:14.511206: Pseudo dice [0.739]
2024-12-19 08:43:14.511984: Epoch time: 215.46 s
2024-12-19 08:43:16.586303: 
2024-12-19 08:43:16.587769: Epoch 147
2024-12-19 08:43:16.588717: Current learning rate: 0.0003
2024-12-19 08:46:45.137836: Validation loss did not improve from -0.58887. Patience: 104/50
2024-12-19 08:46:45.138841: train_loss -0.8062
2024-12-19 08:46:45.140613: val_loss -0.5384
2024-12-19 08:46:45.141622: Pseudo dice [0.7509]
2024-12-19 08:46:45.142658: Epoch time: 208.55 s
2024-12-19 08:46:46.612649: 
2024-12-19 08:46:46.613794: Epoch 148
2024-12-19 08:46:46.614800: Current learning rate: 0.00021
2024-12-19 08:50:21.378407: Validation loss did not improve from -0.58887. Patience: 105/50
2024-12-19 08:50:21.379329: train_loss -0.8036
2024-12-19 08:50:21.380119: val_loss -0.5577
2024-12-19 08:50:21.380857: Pseudo dice [0.7643]
2024-12-19 08:50:21.381544: Epoch time: 214.77 s
2024-12-19 08:50:22.831321: 
2024-12-19 08:50:22.832846: Epoch 149
2024-12-19 08:50:22.833563: Current learning rate: 0.00011
2024-12-19 08:54:04.613591: Validation loss did not improve from -0.58887. Patience: 106/50
2024-12-19 08:54:04.615115: train_loss -0.807
2024-12-19 08:54:04.616036: val_loss -0.5738
2024-12-19 08:54:04.616793: Pseudo dice [0.7706]
2024-12-19 08:54:04.617465: Epoch time: 221.79 s
2024-12-19 08:54:06.600813: Training done.
2024-12-19 08:54:06.913729: Using splits from existing split file: /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset309_Sohee_Calcium_OCT_CrossValidation/splits_final_60.json
2024-12-19 08:54:06.937221: The split file contains 5 splits.
2024-12-19 08:54:06.938701: Desired fold for training: 4
2024-12-19 08:54:06.940257: This split has 4 training and 4 validation cases.
2024-12-19 08:54:06.941987: predicting 101-044
2024-12-19 08:54:07.032665: 101-044, shape torch.Size([1, 404, 498, 498]), rank 0
2024-12-19 08:55:59.576955: predicting 101-045
2024-12-19 08:55:59.677899: 101-045, shape torch.Size([1, 375, 498, 498]), rank 0
2024-12-19 08:57:32.319153: predicting 401-004
2024-12-19 08:57:32.338077: 401-004, shape torch.Size([1, 375, 498, 498]), rank 0
2024-12-19 08:59:34.379347: predicting 706-005
2024-12-19 08:59:34.407295: 706-005, shape torch.Size([1, 375, 498, 498]), rank 0
2024-12-19 09:01:50.911856: Validation complete
2024-12-19 09:01:50.912795: Mean Validation Dice:  0.7522218009788615
