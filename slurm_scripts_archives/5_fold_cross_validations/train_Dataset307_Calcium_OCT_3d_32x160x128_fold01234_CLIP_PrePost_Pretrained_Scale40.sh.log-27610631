/home/gridsan/nchutisilp/.conda/envs/nnunet/bin/python
Starting training with CONFIG=3d_32x160x128_b10, DATASET_ID=307, TRAINER=nnUNetTrainerScaleAnalysis40

############################
INFO: You are using the old nnU-Net default plans. We have updated our recommendations. Please consider using those instead! Read more here: https://github.com/MIC-DKFZ/nnUNet/blob/master/documentation/resenc_presets.md
############################

Using device: cuda:0

#######################################################################
Please cite the following paper when using nnU-Net:
Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.
#######################################################################

2024-12-13 18:18:57.713894: Using torch.compile...

############################
INFO: You are using the old nnU-Net default plans. We have updated our recommendations. Please consider using those instead! Read more here: https://github.com/MIC-DKFZ/nnUNet/blob/master/documentation/resenc_presets.md
############################

Using device: cuda:0

#######################################################################
Please cite the following paper when using nnU-Net:
Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.
#######################################################################

2024-12-13 18:18:57.711169: Using torch.compile...
/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
################### Loading pretrained weights from file  /home/gridsan/nchutisilp/projects/OpenAI-CLIP/clip_preivl_poststent_pretrained_nnUNet.pt ###################
Below is the list of overlapping blocks in pretrained model and nnUNet architecture:
encoder.stages.0.0.convs.0.conv.weight shape torch.Size([32, 1, 3, 3, 3])
encoder.stages.0.0.convs.0.conv.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.norm.weight shape torch.Size([32])
encoder.stages.0.0.convs.0.norm.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.0.weight shape torch.Size([32, 1, 3, 3, 3])
encoder.stages.0.0.convs.0.all_modules.0.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.1.weight shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.1.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
encoder.stages.0.0.convs.1.conv.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.norm.weight shape torch.Size([32])
encoder.stages.0.0.convs.1.norm.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
encoder.stages.0.0.convs.1.all_modules.0.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.1.weight shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.1.bias shape torch.Size([32])
encoder.stages.1.0.convs.0.conv.weight shape torch.Size([64, 32, 3, 3, 3])
encoder.stages.1.0.convs.0.conv.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.norm.weight shape torch.Size([64])
encoder.stages.1.0.convs.0.norm.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.0.weight shape torch.Size([64, 32, 3, 3, 3])
encoder.stages.1.0.convs.0.all_modules.0.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.1.weight shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.1.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
encoder.stages.1.0.convs.1.conv.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.norm.weight shape torch.Size([64])
encoder.stages.1.0.convs.1.norm.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
encoder.stages.1.0.convs.1.all_modules.0.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.1.weight shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.1.bias shape torch.Size([64])
encoder.stages.2.0.convs.0.conv.weight shape torch.Size([128, 64, 3, 3, 3])
encoder.stages.2.0.convs.0.conv.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.norm.weight shape torch.Size([128])
encoder.stages.2.0.convs.0.norm.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.0.weight shape torch.Size([128, 64, 3, 3, 3])
encoder.stages.2.0.convs.0.all_modules.0.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.1.weight shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.1.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
encoder.stages.2.0.convs.1.conv.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.norm.weight shape torch.Size([128])
encoder.stages.2.0.convs.1.norm.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
encoder.stages.2.0.convs.1.all_modules.0.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.1.weight shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.1.bias shape torch.Size([128])
encoder.stages.3.0.convs.0.conv.weight shape torch.Size([256, 128, 3, 3, 3])
encoder.stages.3.0.convs.0.conv.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.norm.weight shape torch.Size([256])
encoder.stages.3.0.convs.0.norm.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.0.weight shape torch.Size([256, 128, 3, 3, 3])
encoder.stages.3.0.convs.0.all_modules.0.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.1.weight shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.1.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
encoder.stages.3.0.convs.1.conv.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.norm.weight shape torch.Size([256])
encoder.stages.3.0.convs.1.norm.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
encoder.stages.3.0.convs.1.all_modules.0.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.1.weight shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.1.bias shape torch.Size([256])
encoder.stages.4.0.convs.0.conv.weight shape torch.Size([320, 256, 3, 3, 3])
encoder.stages.4.0.convs.0.conv.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.norm.weight shape torch.Size([320])
encoder.stages.4.0.convs.0.norm.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.0.weight shape torch.Size([320, 256, 3, 3, 3])
encoder.stages.4.0.convs.0.all_modules.0.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.1.weight shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.1.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.4.0.convs.1.conv.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.norm.weight shape torch.Size([320])
encoder.stages.4.0.convs.1.norm.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.4.0.convs.1.all_modules.0.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.1.weight shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.1.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.0.conv.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.norm.weight shape torch.Size([320])
encoder.stages.5.0.convs.0.norm.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.0.all_modules.0.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.1.weight shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.1.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.1.conv.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.norm.weight shape torch.Size([320])
encoder.stages.5.0.convs.1.norm.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.1.all_modules.0.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.1.weight shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.0.0.convs.0.conv.weight shape torch.Size([32, 1, 3, 3, 3])
decoder.encoder.stages.0.0.convs.0.conv.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.norm.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.norm.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.0.weight shape torch.Size([32, 1, 3, 3, 3])
decoder.encoder.stages.0.0.convs.0.all_modules.0.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.1.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.1.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.encoder.stages.0.0.convs.1.conv.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.norm.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.norm.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.encoder.stages.0.0.convs.1.all_modules.0.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.1.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.1.bias shape torch.Size([32])
decoder.encoder.stages.1.0.convs.0.conv.weight shape torch.Size([64, 32, 3, 3, 3])
decoder.encoder.stages.1.0.convs.0.conv.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.norm.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.norm.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.0.weight shape torch.Size([64, 32, 3, 3, 3])
decoder.encoder.stages.1.0.convs.0.all_modules.0.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.1.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.1.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.encoder.stages.1.0.convs.1.conv.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.norm.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.norm.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.encoder.stages.1.0.convs.1.all_modules.0.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.1.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.1.bias shape torch.Size([64])
decoder.encoder.stages.2.0.convs.0.conv.weight shape torch.Size([128, 64, 3, 3, 3])
decoder.encoder.stages.2.0.convs.0.conv.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.norm.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.norm.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.0.weight shape torch.Size([128, 64, 3, 3, 3])
decoder.encoder.stages.2.0.convs.0.all_modules.0.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.1.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.1.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.encoder.stages.2.0.convs.1.conv.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.norm.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.norm.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.encoder.stages.2.0.convs.1.all_modules.0.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.1.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.1.bias shape torch.Size([128])
decoder.encoder.stages.3.0.convs.0.conv.weight shape torch.Size([256, 128, 3, 3, 3])
decoder.encoder.stages.3.0.convs.0.conv.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.norm.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.norm.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.0.weight shape torch.Size([256, 128, 3, 3, 3])
decoder.encoder.stages.3.0.convs.0.all_modules.0.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.1.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.1.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.encoder.stages.3.0.convs.1.conv.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.norm.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.norm.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.encoder.stages.3.0.convs.1.all_modules.0.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.1.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.1.bias shape torch.Size([256])
decoder.encoder.stages.4.0.convs.0.conv.weight shape torch.Size([320, 256, 3, 3, 3])
decoder.encoder.stages.4.0.convs.0.conv.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.norm.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.norm.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.0.weight shape torch.Size([320, 256, 3, 3, 3])
decoder.encoder.stages.4.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.4.0.convs.1.conv.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.norm.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.norm.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.4.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.0.conv.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.norm.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.norm.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.1.conv.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.norm.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.norm.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.stages.0.convs.0.conv.weight shape torch.Size([320, 640, 3, 3, 3])
decoder.stages.0.convs.0.conv.bias shape torch.Size([320])
decoder.stages.0.convs.0.norm.weight shape torch.Size([320])
decoder.stages.0.convs.0.norm.bias shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.0.weight shape torch.Size([320, 640, 3, 3, 3])
decoder.stages.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.stages.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.stages.0.convs.1.conv.bias shape torch.Size([320])
decoder.stages.0.convs.1.norm.weight shape torch.Size([320])
decoder.stages.0.convs.1.norm.bias shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.stages.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.stages.1.convs.0.conv.weight shape torch.Size([256, 512, 3, 3, 3])
decoder.stages.1.convs.0.conv.bias shape torch.Size([256])
decoder.stages.1.convs.0.norm.weight shape torch.Size([256])
decoder.stages.1.convs.0.norm.bias shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.0.weight shape torch.Size([256, 512, 3, 3, 3])
decoder.stages.1.convs.0.all_modules.0.bias shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.1.weight shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.1.bias shape torch.Size([256])
decoder.stages.1.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.stages.1.convs.1.conv.bias shape torch.Size([256])
decoder.stages.1.convs.1.norm.weight shape torch.Size([256])
decoder.stages.1.convs.1.norm.bias shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.stages.1.convs.1.all_modules.0.bias shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.1.weight shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.1.bias shape torch.Size([256])
decoder.stages.2.convs.0.conv.weight shape torch.Size([128, 256, 3, 3, 3])
decoder.stages.2.convs.0.conv.bias shape torch.Size([128])
decoder.stages.2.convs.0.norm.weight shape torch.Size([128])
decoder.stages.2.convs.0.norm.bias shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.0.weight shape torch.Size([128, 256, 3, 3, 3])
decoder.stages.2.convs.0.all_modules.0.bias shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.1.weight shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.1.bias shape torch.Size([128])
decoder.stages.2.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.stages.2.convs.1.conv.bias shape torch.Size([128])
decoder.stages.2.convs.1.norm.weight shape torch.Size([128])
decoder.stages.2.convs.1.norm.bias shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.stages.2.convs.1.all_modules.0.bias shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.1.weight shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.1.bias shape torch.Size([128])
decoder.stages.3.convs.0.conv.weight shape torch.Size([64, 128, 3, 3, 3])
decoder.stages.3.convs.0.conv.bias shape torch.Size([64])
decoder.stages.3.convs.0.norm.weight shape torch.Size([64])
decoder.stages.3.convs.0.norm.bias shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.0.weight shape torch.Size([64, 128, 3, 3, 3])
decoder.stages.3.convs.0.all_modules.0.bias shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.1.weight shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.1.bias shape torch.Size([64])
decoder.stages.3.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.stages.3.convs.1.conv.bias shape torch.Size([64])
decoder.stages.3.convs.1.norm.weight shape torch.Size([64])
decoder.stages.3.convs.1.norm.bias shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.stages.3.convs.1.all_modules.0.bias shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.1.weight shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.1.bias shape torch.Size([64])
decoder.stages.4.convs.0.conv.weight shape torch.Size([32, 64, 3, 3, 3])
decoder.stages.4.convs.0.conv.bias shape torch.Size([32])
decoder.stages.4.convs.0.norm.weight shape torch.Size([32])
decoder.stages.4.convs.0.norm.bias shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.0.weight shape torch.Size([32, 64, 3, 3, 3])
decoder.stages.4.convs.0.all_modules.0.bias shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.1.weight shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.1.bias shape torch.Size([32])
decoder.stages.4.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.stages.4.convs.1.conv.bias shape torch.Size([32])
decoder.stages.4.convs.1.norm.weight shape torch.Size([32])
decoder.stages.4.convs.1.norm.bias shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.stages.4.convs.1.all_modules.0.bias shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.1.weight shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.1.bias shape torch.Size([32])
decoder.transpconvs.0.weight shape torch.Size([320, 320, 1, 2, 2])
decoder.transpconvs.0.bias shape torch.Size([320])
decoder.transpconvs.1.weight shape torch.Size([320, 256, 2, 2, 2])
decoder.transpconvs.1.bias shape torch.Size([256])
decoder.transpconvs.2.weight shape torch.Size([256, 128, 2, 2, 2])
decoder.transpconvs.2.bias shape torch.Size([128])
decoder.transpconvs.3.weight shape torch.Size([128, 64, 2, 2, 2])
decoder.transpconvs.3.bias shape torch.Size([64])
decoder.transpconvs.4.weight shape torch.Size([64, 32, 2, 2, 2])
decoder.transpconvs.4.bias shape torch.Size([32])
################### Done ###################
2024-12-13 18:18:59.790141: do_dummy_2d_data_aug: True
2024-12-13 18:18:59.791744: /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset307_Sohee_Calcium_OCT_CrossValidation/nnUNetPlans_3d_fullres
2024-12-13 18:18:59.793836: Creating new 5-fold cross-validation split...
2024-12-13 18:18:59.801399: Desired fold for training: 1
2024-12-13 18:18:59.802345: This split has 3 training and 6 validation cases.
################### Loading pretrained weights from file  /home/gridsan/nchutisilp/projects/OpenAI-CLIP/clip_preivl_poststent_pretrained_nnUNet.pt ###################
Below is the list of overlapping blocks in pretrained model and nnUNet architecture:
encoder.stages.0.0.convs.0.conv.weight shape torch.Size([32, 1, 3, 3, 3])
encoder.stages.0.0.convs.0.conv.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.norm.weight shape torch.Size([32])
encoder.stages.0.0.convs.0.norm.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.0.weight shape torch.Size([32, 1, 3, 3, 3])
encoder.stages.0.0.convs.0.all_modules.0.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.1.weight shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.1.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
encoder.stages.0.0.convs.1.conv.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.norm.weight shape torch.Size([32])
encoder.stages.0.0.convs.1.norm.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
encoder.stages.0.0.convs.1.all_modules.0.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.1.weight shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.1.bias shape torch.Size([32])
encoder.stages.1.0.convs.0.conv.weight shape torch.Size([64, 32, 3, 3, 3])
encoder.stages.1.0.convs.0.conv.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.norm.weight shape torch.Size([64])
encoder.stages.1.0.convs.0.norm.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.0.weight shape torch.Size([64, 32, 3, 3, 3])
encoder.stages.1.0.convs.0.all_modules.0.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.1.weight shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.1.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
encoder.stages.1.0.convs.1.conv.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.norm.weight shape torch.Size([64])
encoder.stages.1.0.convs.1.norm.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
encoder.stages.1.0.convs.1.all_modules.0.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.1.weight shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.1.bias shape torch.Size([64])
encoder.stages.2.0.convs.0.conv.weight shape torch.Size([128, 64, 3, 3, 3])
encoder.stages.2.0.convs.0.conv.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.norm.weight shape torch.Size([128])
encoder.stages.2.0.convs.0.norm.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.0.weight shape torch.Size([128, 64, 3, 3, 3])
encoder.stages.2.0.convs.0.all_modules.0.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.1.weight shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.1.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
encoder.stages.2.0.convs.1.conv.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.norm.weight shape torch.Size([128])
encoder.stages.2.0.convs.1.norm.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
encoder.stages.2.0.convs.1.all_modules.0.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.1.weight shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.1.bias shape torch.Size([128])
encoder.stages.3.0.convs.0.conv.weight shape torch.Size([256, 128, 3, 3, 3])
encoder.stages.3.0.convs.0.conv.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.norm.weight shape torch.Size([256])
encoder.stages.3.0.convs.0.norm.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.0.weight shape torch.Size([256, 128, 3, 3, 3])
encoder.stages.3.0.convs.0.all_modules.0.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.1.weight shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.1.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
encoder.stages.3.0.convs.1.conv.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.norm.weight shape torch.Size([256])
encoder.stages.3.0.convs.1.norm.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
encoder.stages.3.0.convs.1.all_modules.0.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.1.weight shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.1.bias shape torch.Size([256])
encoder.stages.4.0.convs.0.conv.weight shape torch.Size([320, 256, 3, 3, 3])
encoder.stages.4.0.convs.0.conv.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.norm.weight shape torch.Size([320])
encoder.stages.4.0.convs.0.norm.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.0.weight shape torch.Size([320, 256, 3, 3, 3])
encoder.stages.4.0.convs.0.all_modules.0.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.1.weight shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.1.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.4.0.convs.1.conv.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.norm.weight shape torch.Size([320])
encoder.stages.4.0.convs.1.norm.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.4.0.convs.1.all_modules.0.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.1.weight shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.1.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.0.conv.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.norm.weight shape torch.Size([320])
encoder.stages.5.0.convs.0.norm.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.0.all_modules.0.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.1.weight shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.1.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.1.conv.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.norm.weight shape torch.Size([320])
encoder.stages.5.0.convs.1.norm.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.1.all_modules.0.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.1.weight shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.0.0.convs.0.conv.weight shape torch.Size([32, 1, 3, 3, 3])
decoder.encoder.stages.0.0.convs.0.conv.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.norm.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.norm.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.0.weight shape torch.Size([32, 1, 3, 3, 3])
decoder.encoder.stages.0.0.convs.0.all_modules.0.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.1.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.1.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.encoder.stages.0.0.convs.1.conv.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.norm.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.norm.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.encoder.stages.0.0.convs.1.all_modules.0.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.1.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.1.bias shape torch.Size([32])
decoder.encoder.stages.1.0.convs.0.conv.weight shape torch.Size([64, 32, 3, 3, 3])
decoder.encoder.stages.1.0.convs.0.conv.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.norm.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.norm.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.0.weight shape torch.Size([64, 32, 3, 3, 3])
decoder.encoder.stages.1.0.convs.0.all_modules.0.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.1.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.1.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.encoder.stages.1.0.convs.1.conv.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.norm.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.norm.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.encoder.stages.1.0.convs.1.all_modules.0.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.1.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.1.bias shape torch.Size([64])
decoder.encoder.stages.2.0.convs.0.conv.weight shape torch.Size([128, 64, 3, 3, 3])
decoder.encoder.stages.2.0.convs.0.conv.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.norm.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.norm.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.0.weight shape torch.Size([128, 64, 3, 3, 3])
decoder.encoder.stages.2.0.convs.0.all_modules.0.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.1.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.1.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.encoder.stages.2.0.convs.1.conv.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.norm.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.norm.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.encoder.stages.2.0.convs.1.all_modules.0.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.1.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.1.bias shape torch.Size([128])
decoder.encoder.stages.3.0.convs.0.conv.weight shape torch.Size([256, 128, 3, 3, 3])
decoder.encoder.stages.3.0.convs.0.conv.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.norm.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.norm.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.0.weight shape torch.Size([256, 128, 3, 3, 3])
decoder.encoder.stages.3.0.convs.0.all_modules.0.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.1.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.1.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.encoder.stages.3.0.convs.1.conv.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.norm.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.norm.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.encoder.stages.3.0.convs.1.all_modules.0.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.1.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.1.bias shape torch.Size([256])
decoder.encoder.stages.4.0.convs.0.conv.weight shape torch.Size([320, 256, 3, 3, 3])
decoder.encoder.stages.4.0.convs.0.conv.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.norm.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.norm.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.0.weight shape torch.Size([320, 256, 3, 3, 3])
decoder.encoder.stages.4.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.4.0.convs.1.conv.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.norm.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.norm.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.4.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.0.conv.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.norm.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.norm.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.1.conv.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.norm.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.norm.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.stages.0.convs.0.conv.weight shape torch.Size([320, 640, 3, 3, 3])
decoder.stages.0.convs.0.conv.bias shape torch.Size([320])
decoder.stages.0.convs.0.norm.weight shape torch.Size([320])
decoder.stages.0.convs.0.norm.bias shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.0.weight shape torch.Size([320, 640, 3, 3, 3])
decoder.stages.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.stages.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.stages.0.convs.1.conv.bias shape torch.Size([320])
decoder.stages.0.convs.1.norm.weight shape torch.Size([320])
decoder.stages.0.convs.1.norm.bias shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.stages.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.stages.1.convs.0.conv.weight shape torch.Size([256, 512, 3, 3, 3])
decoder.stages.1.convs.0.conv.bias shape torch.Size([256])
decoder.stages.1.convs.0.norm.weight shape torch.Size([256])
decoder.stages.1.convs.0.norm.bias shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.0.weight shape torch.Size([256, 512, 3, 3, 3])
decoder.stages.1.convs.0.all_modules.0.bias shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.1.weight shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.1.bias shape torch.Size([256])
decoder.stages.1.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.stages.1.convs.1.conv.bias shape torch.Size([256])
decoder.stages.1.convs.1.norm.weight shape torch.Size([256])
decoder.stages.1.convs.1.norm.bias shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.stages.1.convs.1.all_modules.0.bias shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.1.weight shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.1.bias shape torch.Size([256])
decoder.stages.2.convs.0.conv.weight shape torch.Size([128, 256, 3, 3, 3])
decoder.stages.2.convs.0.conv.bias shape torch.Size([128])
decoder.stages.2.convs.0.norm.weight shape torch.Size([128])
decoder.stages.2.convs.0.norm.bias shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.0.weight shape torch.Size([128, 256, 3, 3, 3])
decoder.stages.2.convs.0.all_modules.0.bias shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.1.weight shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.1.bias shape torch.Size([128])
decoder.stages.2.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.stages.2.convs.1.conv.bias shape torch.Size([128])
decoder.stages.2.convs.1.norm.weight shape torch.Size([128])
decoder.stages.2.convs.1.norm.bias shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.stages.2.convs.1.all_modules.0.bias shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.1.weight shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.1.bias shape torch.Size([128])
decoder.stages.3.convs.0.conv.weight shape torch.Size([64, 128, 3, 3, 3])
decoder.stages.3.convs.0.conv.bias shape torch.Size([64])
decoder.stages.3.convs.0.norm.weight shape torch.Size([64])
decoder.stages.3.convs.0.norm.bias shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.0.weight shape torch.Size([64, 128, 3, 3, 3])
decoder.stages.3.convs.0.all_modules.0.bias shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.1.weight shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.1.bias shape torch.Size([64])
decoder.stages.3.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.stages.3.convs.1.conv.bias shape torch.Size([64])
decoder.stages.3.convs.1.norm.weight shape torch.Size([64])
decoder.stages.3.convs.1.norm.bias shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.stages.3.convs.1.all_modules.0.bias shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.1.weight shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.1.bias shape torch.Size([64])
decoder.stages.4.convs.0.conv.weight shape torch.Size([32, 64, 3, 3, 3])
decoder.stages.4.convs.0.conv.bias shape torch.Size([32])
decoder.stages.4.convs.0.norm.weight shape torch.Size([32])
decoder.stages.4.convs.0.norm.bias shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.0.weight shape torch.Size([32, 64, 3, 3, 3])
decoder.stages.4.convs.0.all_modules.0.bias shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.1.weight shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.1.bias shape torch.Size([32])
decoder.stages.4.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.stages.4.convs.1.conv.bias shape torch.Size([32])
decoder.stages.4.convs.1.norm.weight shape torch.Size([32])
decoder.stages.4.convs.1.norm.bias shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.stages.4.convs.1.all_modules.0.bias shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.1.weight shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.1.bias shape torch.Size([32])
decoder.transpconvs.0.weight shape torch.Size([320, 320, 1, 2, 2])
decoder.transpconvs.0.bias shape torch.Size([320])
decoder.transpconvs.1.weight shape torch.Size([320, 256, 2, 2, 2])
decoder.transpconvs.1.bias shape torch.Size([256])
decoder.transpconvs.2.weight shape torch.Size([256, 128, 2, 2, 2])
decoder.transpconvs.2.bias shape torch.Size([128])
decoder.transpconvs.3.weight shape torch.Size([128, 64, 2, 2, 2])
decoder.transpconvs.3.bias shape torch.Size([64])
decoder.transpconvs.4.weight shape torch.Size([64, 32, 2, 2, 2])
decoder.transpconvs.4.bias shape torch.Size([32])
################### Done ###################
2024-12-13 18:18:59.790139: do_dummy_2d_data_aug: True
2024-12-13 18:18:59.792116: /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset307_Sohee_Calcium_OCT_CrossValidation/nnUNetPlans_3d_fullres
2024-12-13 18:18:59.793668: Creating new 5-fold cross-validation split...
2024-12-13 18:18:59.798058: Desired fold for training: 0
2024-12-13 18:18:59.798993: This split has 3 training and 5 validation cases.
using pin_memory on device 0
using pin_memory on device 0

This is the configuration used by this training:
Configuration name: 3d_32x160x128_b10
 {'data_identifier': 'nnUNetPlans_3d_fullres', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 10, 'patch_size': [32, 160, 128], 'median_image_size_in_voxels': [375.0, 498.0, 498.0], 'spacing': [1.0, 1.0, 1.0], 'normalization_schemes': ['ZScoreNormalization'], 'use_mask_for_norm': [False], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'architecture': {'network_class_name': 'dynamic_network_architectures.architectures.unet.PlainConvUNet', 'arch_kwargs': {'n_stages': 6, 'features_per_stage': [32, 64, 128, 256, 320, 320], 'conv_op': 'torch.nn.modules.conv.Conv3d', 'kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]], 'strides': [[1, 1, 1], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [1, 2, 2]], 'n_conv_per_stage': [2, 2, 2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2, 2, 2], 'conv_bias': True, 'norm_op': 'torch.nn.modules.instancenorm.InstanceNorm3d', 'norm_op_kwargs': {'eps': 1e-05, 'affine': True}, 'dropout_op': None, 'dropout_op_kwargs': None, 'nonlin': 'torch.nn.LeakyReLU', 'nonlin_kwargs': {'inplace': True}, 'deep_supervision': True}, '_kw_requires_import': ['conv_op', 'norm_op', 'dropout_op', 'nonlin']}, 'batch_dice': True, 'inherits_from': '3d_fullres'} 

These are the global plan.json settings:
 {'dataset_name': 'Dataset307_Sohee_Calcium_OCT_CrossValidation', 'plans_name': 'nnUNetPlans', 'original_median_spacing_after_transp': [1.0, 1.0, 1.0], 'original_median_shape_after_transp': [375, 498, 498], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 0.4977976381778717, 'mean': 0.13507525622844696, 'median': 0.09599608182907104, 'min': 0.0, 'percentile_00_5': 0.014754901640117168, 'percentile_99_5': 0.4977976381778717, 'std': 0.12025152146816254}}} 

2024-12-13 18:19:02.280937: unpacking dataset...
using pin_memory on device 0
using pin_memory on device 0

This is the configuration used by this training:
Configuration name: 3d_32x160x128_b10
 {'data_identifier': 'nnUNetPlans_3d_fullres', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 10, 'patch_size': [32, 160, 128], 'median_image_size_in_voxels': [375.0, 498.0, 498.0], 'spacing': [1.0, 1.0, 1.0], 'normalization_schemes': ['ZScoreNormalization'], 'use_mask_for_norm': [False], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'architecture': {'network_class_name': 'dynamic_network_architectures.architectures.unet.PlainConvUNet', 'arch_kwargs': {'n_stages': 6, 'features_per_stage': [32, 64, 128, 256, 320, 320], 'conv_op': 'torch.nn.modules.conv.Conv3d', 'kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]], 'strides': [[1, 1, 1], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [1, 2, 2]], 'n_conv_per_stage': [2, 2, 2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2, 2, 2], 'conv_bias': True, 'norm_op': 'torch.nn.modules.instancenorm.InstanceNorm3d', 'norm_op_kwargs': {'eps': 1e-05, 'affine': True}, 'dropout_op': None, 'dropout_op_kwargs': None, 'nonlin': 'torch.nn.LeakyReLU', 'nonlin_kwargs': {'inplace': True}, 'deep_supervision': True}, '_kw_requires_import': ['conv_op', 'norm_op', 'dropout_op', 'nonlin']}, 'batch_dice': True, 'inherits_from': '3d_fullres'} 

These are the global plan.json settings:
 {'dataset_name': 'Dataset307_Sohee_Calcium_OCT_CrossValidation', 'plans_name': 'nnUNetPlans', 'original_median_spacing_after_transp': [1.0, 1.0, 1.0], 'original_median_shape_after_transp': [375, 498, 498], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 0.4977976381778717, 'mean': 0.13507525622844696, 'median': 0.09599608182907104, 'min': 0.0, 'percentile_00_5': 0.014754901640117168, 'percentile_99_5': 0.4977976381778717, 'std': 0.12025152146816254}}} 

2024-12-13 18:19:03.474714: unpacking dataset...
2024-12-13 18:19:08.007014: unpacking done...
2024-12-13 18:19:08.014860: Unable to plot network architecture: nnUNet_compile is enabled!
2024-12-13 18:19:08.055996: 
2024-12-13 18:19:08.057007: Epoch 0
2024-12-13 18:19:08.057853: Current learning rate: 0.01
2024-12-13 18:21:46.930962: Validation loss improved from 1000.00000 to -0.21292! Patience: 0/50
2024-12-13 18:21:46.932557: train_loss -0.0621
2024-12-13 18:21:46.933445: val_loss -0.2129
2024-12-13 18:21:46.934210: Pseudo dice [0.5466]
2024-12-13 18:21:46.935058: Epoch time: 158.88 s
2024-12-13 18:21:46.935836: Yayy! New best EMA pseudo Dice: 0.5466
2024-12-13 18:21:48.666897: 
2024-12-13 18:21:48.668454: Epoch 1
2024-12-13 18:21:48.669512: Current learning rate: 0.00994
2024-12-13 18:23:26.212114: Validation loss improved from -0.21292 to -0.23899! Patience: 0/50
2024-12-13 18:23:26.213081: train_loss -0.2143
2024-12-13 18:23:26.214020: val_loss -0.239
2024-12-13 18:23:26.214719: Pseudo dice [0.5579]
2024-12-13 18:23:26.215365: Epoch time: 97.55 s
2024-12-13 18:23:26.216020: Yayy! New best EMA pseudo Dice: 0.5478
2024-12-13 18:23:28.120618: 
2024-12-13 18:23:28.122394: Epoch 2
2024-12-13 18:23:28.123591: Current learning rate: 0.00988
2024-12-13 18:25:28.409924: Validation loss did not improve from -0.23899. Patience: 1/50
2024-12-13 18:25:28.411070: train_loss -0.2663
2024-12-13 18:25:28.411832: val_loss -0.2359
2024-12-13 18:25:28.412571: Pseudo dice [0.5781]
2024-12-13 18:25:28.413368: Epoch time: 120.29 s
2024-12-13 18:25:28.414090: Yayy! New best EMA pseudo Dice: 0.5508
2024-12-13 18:25:30.435890: 
2024-12-13 18:25:30.437459: Epoch 3
2024-12-13 18:25:30.438226: Current learning rate: 0.00982
2024-12-13 18:27:44.400210: Validation loss improved from -0.23899 to -0.28722! Patience: 1/50
2024-12-13 18:27:44.401254: train_loss -0.307
2024-12-13 18:27:44.402337: val_loss -0.2872
2024-12-13 18:27:44.403430: Pseudo dice [0.5943]
2024-12-13 18:27:44.404355: Epoch time: 133.97 s
2024-12-13 18:27:44.405276: Yayy! New best EMA pseudo Dice: 0.5551
2024-12-13 18:27:46.287801: 
2024-12-13 18:27:46.289057: Epoch 4
2024-12-13 18:27:46.290048: Current learning rate: 0.00976
2024-12-13 18:30:02.797224: Validation loss improved from -0.28722 to -0.29820! Patience: 0/50
2024-12-13 18:30:02.798277: train_loss -0.3513
2024-12-13 18:30:02.799103: val_loss -0.2982
2024-12-13 18:30:02.799895: Pseudo dice [0.6123]
2024-12-13 18:30:02.800680: Epoch time: 136.51 s
2024-12-13 18:30:03.154875: Yayy! New best EMA pseudo Dice: 0.5609
2024-12-13 18:30:05.125214: 
2024-12-13 18:30:05.126713: Epoch 5
2024-12-13 18:30:05.127563: Current learning rate: 0.0097
2024-12-13 18:32:34.503936: Validation loss improved from -0.29820 to -0.34773! Patience: 0/50
2024-12-13 18:32:34.504983: train_loss -0.3985
2024-12-13 18:32:34.505827: val_loss -0.3477
2024-12-13 18:32:34.506619: Pseudo dice [0.6465]
2024-12-13 18:32:34.507384: Epoch time: 149.38 s
2024-12-13 18:32:34.508134: Yayy! New best EMA pseudo Dice: 0.5694
2024-12-13 18:32:36.274840: 
2024-12-13 18:32:36.276078: Epoch 6
2024-12-13 18:32:36.276884: Current learning rate: 0.00964
2024-12-13 18:35:06.357437: Validation loss improved from -0.34773 to -0.36516! Patience: 0/50
2024-12-13 18:35:06.358302: train_loss -0.4038
2024-12-13 18:35:06.359215: val_loss -0.3652
2024-12-13 18:35:06.360074: Pseudo dice [0.6563]
2024-12-13 18:35:06.360866: Epoch time: 150.08 s
2024-12-13 18:35:06.361644: Yayy! New best EMA pseudo Dice: 0.5781
2024-12-13 18:35:08.309741: 
2024-12-13 18:35:08.310945: Epoch 7
2024-12-13 18:35:08.311629: Current learning rate: 0.00958
2024-12-13 18:37:50.583416: Validation loss did not improve from -0.36516. Patience: 1/50
2024-12-13 18:37:50.584333: train_loss -0.426
2024-12-13 18:37:50.585223: val_loss -0.3295
2024-12-13 18:37:50.585961: Pseudo dice [0.6294]
2024-12-13 18:37:50.586837: Epoch time: 162.28 s
2024-12-13 18:37:50.587621: Yayy! New best EMA pseudo Dice: 0.5832
2024-12-13 18:37:53.173793: 
2024-12-13 18:37:53.174901: Epoch 8
2024-12-13 18:37:53.175622: Current learning rate: 0.00952
2024-12-13 18:40:16.724454: Validation loss did not improve from -0.36516. Patience: 2/50
2024-12-13 18:40:16.725448: train_loss -0.452
2024-12-13 18:40:16.726403: val_loss -0.3572
2024-12-13 18:40:16.727221: Pseudo dice [0.6318]
2024-12-13 18:40:16.728091: Epoch time: 143.55 s
2024-12-13 18:40:16.728875: Yayy! New best EMA pseudo Dice: 0.5881
2024-12-13 18:40:18.610391: 
2024-12-13 18:40:18.611742: Epoch 9
2024-12-13 18:40:18.612441: Current learning rate: 0.00946
2024-12-13 18:42:59.737067: Validation loss did not improve from -0.36516. Patience: 3/50
2024-12-13 18:42:59.738186: train_loss -0.461
2024-12-13 18:42:59.739082: val_loss -0.3419
2024-12-13 18:42:59.739836: Pseudo dice [0.6125]
2024-12-13 18:42:59.740575: Epoch time: 161.13 s
2024-12-13 18:43:00.125608: Yayy! New best EMA pseudo Dice: 0.5905
2024-12-13 18:43:01.902023: 
2024-12-13 18:43:01.903273: Epoch 10
2024-12-13 18:43:01.904017: Current learning rate: 0.0094
2024-12-13 18:46:29.762878: Validation loss improved from -0.36516 to -0.39422! Patience: 3/50
2024-12-13 18:46:29.763868: train_loss -0.4894
2024-12-13 18:46:29.764670: val_loss -0.3942
2024-12-13 18:46:29.765364: Pseudo dice [0.667]
2024-12-13 18:46:29.766009: Epoch time: 207.86 s
2024-12-13 18:46:29.766639: Yayy! New best EMA pseudo Dice: 0.5982
2024-12-13 18:46:31.557901: 
2024-12-13 18:46:31.559717: Epoch 11
2024-12-13 18:46:31.560620: Current learning rate: 0.00934
2024-12-13 18:52:31.960423: Validation loss did not improve from -0.39422. Patience: 1/50
2024-12-13 18:52:31.961295: train_loss -0.4978
2024-12-13 18:52:31.962255: val_loss -0.3648
2024-12-13 18:52:31.963083: Pseudo dice [0.6365]
2024-12-13 18:52:31.963908: Epoch time: 360.4 s
2024-12-13 18:52:31.964635: Yayy! New best EMA pseudo Dice: 0.602
2024-12-13 18:52:33.734765: 
2024-12-13 18:52:33.736045: Epoch 12
2024-12-13 18:52:33.736840: Current learning rate: 0.00928
2024-12-13 18:58:47.360142: Validation loss did not improve from -0.39422. Patience: 2/50
2024-12-13 18:58:47.360900: train_loss -0.5426
2024-12-13 18:58:47.361709: val_loss -0.3855
2024-12-13 18:58:47.362411: Pseudo dice [0.6574]
2024-12-13 18:58:47.363060: Epoch time: 373.63 s
2024-12-13 18:58:47.363752: Yayy! New best EMA pseudo Dice: 0.6076
2024-12-13 18:58:49.142835: 
2024-12-13 18:58:49.144121: Epoch 13
2024-12-13 18:58:49.145025: Current learning rate: 0.00922
2024-12-13 19:05:13.145812: Validation loss did not improve from -0.39422. Patience: 3/50
2024-12-13 19:05:13.146749: train_loss -0.5425
2024-12-13 19:05:13.147558: val_loss -0.389
2024-12-13 19:05:13.148268: Pseudo dice [0.6734]
2024-12-13 19:05:13.148997: Epoch time: 384.01 s
2024-12-13 19:05:13.149774: Yayy! New best EMA pseudo Dice: 0.6141
2024-12-13 19:05:14.984861: 
2024-12-13 19:05:14.986069: Epoch 14
2024-12-13 19:05:14.987079: Current learning rate: 0.00916
2024-12-13 19:12:00.999290: Validation loss did not improve from -0.39422. Patience: 4/50
2024-12-13 19:12:01.000413: train_loss -0.5569
2024-12-13 19:12:01.001331: val_loss -0.3705
2024-12-13 19:12:01.002064: Pseudo dice [0.6534]
2024-12-13 19:12:01.002844: Epoch time: 406.02 s
2024-12-13 19:12:01.408698: Yayy! New best EMA pseudo Dice: 0.6181
2024-12-13 19:12:03.253814: 
2024-12-13 19:12:03.255048: Epoch 15
2024-12-13 19:12:03.255863: Current learning rate: 0.0091
2024-12-13 19:18:33.958238: Validation loss improved from -0.39422 to -0.39881! Patience: 4/50
2024-12-13 19:18:33.959300: train_loss -0.5641
2024-12-13 19:18:33.960090: val_loss -0.3988
2024-12-13 19:18:33.960850: Pseudo dice [0.6706]
2024-12-13 19:18:33.961784: Epoch time: 390.71 s
2024-12-13 19:18:33.962559: Yayy! New best EMA pseudo Dice: 0.6233
2024-12-13 19:18:35.770656: 
2024-12-13 19:18:35.772110: Epoch 16
2024-12-13 19:18:35.772990: Current learning rate: 0.00903
2024-12-13 19:25:18.430324: Validation loss improved from -0.39881 to -0.41129! Patience: 0/50
2024-12-13 19:25:18.432251: train_loss -0.5872
2024-12-13 19:25:18.433415: val_loss -0.4113
2024-12-13 19:25:18.434198: Pseudo dice [0.6867]
2024-12-13 19:25:18.435249: Epoch time: 402.66 s
2024-12-13 19:25:18.436178: Yayy! New best EMA pseudo Dice: 0.6297
2024-12-13 19:25:20.296047: 
2024-12-13 19:25:20.297405: Epoch 17
2024-12-13 19:25:20.298222: Current learning rate: 0.00897
2024-12-13 19:31:47.813703: Validation loss did not improve from -0.41129. Patience: 1/50
2024-12-13 19:31:47.814977: train_loss -0.589
2024-12-13 19:31:47.816124: val_loss -0.3988
2024-12-13 19:31:47.817138: Pseudo dice [0.6636]
2024-12-13 19:31:47.818107: Epoch time: 387.52 s
2024-12-13 19:31:47.818954: Yayy! New best EMA pseudo Dice: 0.633
2024-12-13 19:31:50.106251: 
2024-12-13 19:31:50.107692: Epoch 18
2024-12-13 19:31:50.108747: Current learning rate: 0.00891
2024-12-13 19:38:37.209689: Validation loss did not improve from -0.41129. Patience: 2/50
2024-12-13 19:38:37.210742: train_loss -0.6083
2024-12-13 19:38:37.211516: val_loss -0.377
2024-12-13 19:38:37.212180: Pseudo dice [0.652]
2024-12-13 19:38:37.212971: Epoch time: 407.11 s
2024-12-13 19:38:37.213658: Yayy! New best EMA pseudo Dice: 0.6349
2024-12-13 19:38:39.042082: 
2024-12-13 19:38:39.043422: Epoch 19
2024-12-13 19:38:39.044214: Current learning rate: 0.00885
2024-12-13 19:45:24.672606: Validation loss did not improve from -0.41129. Patience: 3/50
2024-12-13 19:45:24.673733: train_loss -0.605
2024-12-13 19:45:24.674543: val_loss -0.3317
2024-12-13 19:45:24.675275: Pseudo dice [0.6261]
2024-12-13 19:45:24.676015: Epoch time: 405.63 s
2024-12-13 19:45:26.452573: 
2024-12-13 19:45:26.454095: Epoch 20
2024-12-13 19:45:26.455071: Current learning rate: 0.00879
2024-12-13 19:52:25.651741: Validation loss did not improve from -0.41129. Patience: 4/50
2024-12-13 19:52:25.652655: train_loss -0.616
2024-12-13 19:52:25.653566: val_loss -0.3681
2024-12-13 19:52:25.654224: Pseudo dice [0.6523]
2024-12-13 19:52:25.654903: Epoch time: 419.2 s
2024-12-13 19:52:25.655659: Yayy! New best EMA pseudo Dice: 0.6359
2024-12-13 19:52:27.499828: 
2024-12-13 19:52:27.501259: Epoch 21
2024-12-13 19:52:27.501978: Current learning rate: 0.00873
2024-12-13 19:59:34.935035: Validation loss improved from -0.41129 to -0.41452! Patience: 4/50
2024-12-13 19:59:34.936058: train_loss -0.6312
2024-12-13 19:59:34.936710: val_loss -0.4145
2024-12-13 19:59:34.937347: Pseudo dice [0.6844]
2024-12-13 19:59:34.937981: Epoch time: 427.44 s
2024-12-13 19:59:34.938575: Yayy! New best EMA pseudo Dice: 0.6407
2024-12-13 19:59:36.688217: 
2024-12-13 19:59:36.689590: Epoch 22
2024-12-13 19:59:36.690368: Current learning rate: 0.00867
2024-12-13 20:06:42.938613: Validation loss improved from -0.41452 to -0.43820! Patience: 0/50
2024-12-13 20:06:42.939620: train_loss -0.6416
2024-12-13 20:06:42.940312: val_loss -0.4382
2024-12-13 20:06:42.941054: Pseudo dice [0.6923]
2024-12-13 20:06:42.941747: Epoch time: 426.25 s
2024-12-13 20:06:42.942374: Yayy! New best EMA pseudo Dice: 0.6459
2024-12-13 20:06:44.682201: 
2024-12-13 20:06:44.683243: Epoch 23
2024-12-13 20:06:44.683905: Current learning rate: 0.00861
2024-12-13 20:14:03.239282: Validation loss did not improve from -0.43820. Patience: 1/50
2024-12-13 20:14:03.240315: train_loss -0.6353
2024-12-13 20:14:03.241155: val_loss -0.4325
2024-12-13 20:14:03.241841: Pseudo dice [0.6882]
2024-12-13 20:14:03.242570: Epoch time: 438.56 s
2024-12-13 20:14:03.243286: Yayy! New best EMA pseudo Dice: 0.6501
2024-12-13 20:14:05.025982: 
2024-12-13 20:14:05.027404: Epoch 24
2024-12-13 20:14:05.028292: Current learning rate: 0.00855
2024-12-13 20:20:47.745582: Validation loss did not improve from -0.43820. Patience: 2/50
2024-12-13 20:20:47.746615: train_loss -0.6429
2024-12-13 20:20:47.747535: val_loss -0.3825
2024-12-13 20:20:47.748534: Pseudo dice [0.6777]
2024-12-13 20:20:47.749352: Epoch time: 402.72 s
2024-12-13 20:20:48.134358: Yayy! New best EMA pseudo Dice: 0.6529
2024-12-13 20:20:49.926539: 
2024-12-13 20:20:49.927863: Epoch 25
2024-12-13 20:20:49.928964: Current learning rate: 0.00849
2024-12-13 20:27:49.332243: Validation loss did not improve from -0.43820. Patience: 3/50
2024-12-13 20:27:49.333262: train_loss -0.6468
2024-12-13 20:27:49.335051: val_loss -0.4145
2024-12-13 20:27:49.336025: Pseudo dice [0.6805]
2024-12-13 20:27:49.337096: Epoch time: 419.41 s
2024-12-13 20:27:49.338019: Yayy! New best EMA pseudo Dice: 0.6556
2024-12-13 20:27:51.107984: 
2024-12-13 20:27:51.109440: Epoch 26
2024-12-13 20:27:51.110365: Current learning rate: 0.00843
2024-12-13 20:34:56.255650: Validation loss did not improve from -0.43820. Patience: 4/50
2024-12-13 20:34:56.256967: train_loss -0.657
2024-12-13 20:34:56.257786: val_loss -0.4007
2024-12-13 20:34:56.258619: Pseudo dice [0.6579]
2024-12-13 20:34:56.259394: Epoch time: 425.15 s
2024-12-13 20:34:56.260164: Yayy! New best EMA pseudo Dice: 0.6559
2024-12-13 20:34:58.065573: 
2024-12-13 20:34:58.066831: Epoch 27
2024-12-13 20:34:58.067587: Current learning rate: 0.00836
2024-12-13 20:41:50.422193: Validation loss did not improve from -0.43820. Patience: 5/50
2024-12-13 20:41:50.423225: train_loss -0.6601
2024-12-13 20:41:50.424162: val_loss -0.3665
2024-12-13 20:41:50.425135: Pseudo dice [0.6641]
2024-12-13 20:41:50.426105: Epoch time: 412.36 s
2024-12-13 20:41:50.427054: Yayy! New best EMA pseudo Dice: 0.6567
2024-12-13 20:41:52.240003: 
2024-12-13 20:41:52.241297: Epoch 28
2024-12-13 20:41:52.242285: Current learning rate: 0.0083
2024-12-13 20:50:14.240896: Validation loss did not improve from -0.43820. Patience: 6/50
2024-12-13 20:50:14.242239: train_loss -0.6713
2024-12-13 20:50:14.243107: val_loss -0.3496
2024-12-13 20:50:14.243882: Pseudo dice [0.6547]
2024-12-13 20:50:14.244570: Epoch time: 502.0 s
2024-12-13 20:50:16.101818: 
2024-12-13 20:50:16.102964: Epoch 29
2024-12-13 20:50:16.103867: Current learning rate: 0.00824
2024-12-13 20:59:11.524115: Validation loss did not improve from -0.43820. Patience: 7/50
2024-12-13 20:59:11.524989: train_loss -0.6789
2024-12-13 20:59:11.526009: val_loss -0.4043
2024-12-13 20:59:11.526857: Pseudo dice [0.6814]
2024-12-13 20:59:11.527591: Epoch time: 535.42 s
2024-12-13 20:59:11.965188: Yayy! New best EMA pseudo Dice: 0.659
2024-12-13 20:59:13.860246: 
2024-12-13 20:59:13.861752: Epoch 30
2024-12-13 20:59:13.862800: Current learning rate: 0.00818
2024-12-13 21:08:08.020252: Validation loss did not improve from -0.43820. Patience: 8/50
2024-12-13 21:08:08.021341: train_loss -0.6905
2024-12-13 21:08:08.022098: val_loss -0.4271
2024-12-13 21:08:08.022819: Pseudo dice [0.6924]
2024-12-13 21:08:08.023614: Epoch time: 534.16 s
2024-12-13 21:08:08.024373: Yayy! New best EMA pseudo Dice: 0.6623
2024-12-13 21:08:09.857998: 
2024-12-13 21:08:09.859285: Epoch 31
2024-12-13 21:08:09.860015: Current learning rate: 0.00812
2024-12-13 21:17:09.607271: Validation loss did not improve from -0.43820. Patience: 9/50
2024-12-13 21:17:09.608412: train_loss -0.6898
2024-12-13 21:17:09.609448: val_loss -0.4067
2024-12-13 21:17:09.610498: Pseudo dice [0.6907]
2024-12-13 21:17:09.611560: Epoch time: 539.75 s
2024-12-13 21:17:09.612530: Yayy! New best EMA pseudo Dice: 0.6652
2024-12-13 21:17:11.496639: 
2024-12-13 21:17:11.497701: Epoch 32
2024-12-13 21:17:11.498444: Current learning rate: 0.00806
2024-12-13 21:26:59.256842: Validation loss did not improve from -0.43820. Patience: 10/50
2024-12-13 21:26:59.257796: train_loss -0.6945
2024-12-13 21:26:59.258773: val_loss -0.3247
2024-12-13 21:26:59.259477: Pseudo dice [0.6265]
2024-12-13 21:26:59.260162: Epoch time: 587.76 s
2024-12-13 21:27:00.673770: 
2024-12-13 21:27:00.675110: Epoch 33
2024-12-13 21:27:00.676001: Current learning rate: 0.008
2024-12-13 21:36:07.936949: Validation loss did not improve from -0.43820. Patience: 11/50
2024-12-13 21:36:07.942030: train_loss -0.6926
2024-12-13 21:36:07.943341: val_loss -0.4079
2024-12-13 21:36:07.944211: Pseudo dice [0.6788]
2024-12-13 21:36:07.945198: Epoch time: 547.27 s
2024-12-13 21:36:09.418978: 
2024-12-13 21:36:09.420323: Epoch 34
2024-12-13 21:36:09.421143: Current learning rate: 0.00793
2024-12-13 21:45:01.669667: Validation loss improved from -0.43820 to -0.45426! Patience: 11/50
2024-12-13 21:45:01.670866: train_loss -0.698
2024-12-13 21:45:01.671873: val_loss -0.4543
2024-12-13 21:45:01.672881: Pseudo dice [0.7058]
2024-12-13 21:45:01.673676: Epoch time: 532.25 s
2024-12-13 21:45:02.094869: Yayy! New best EMA pseudo Dice: 0.6673
2024-12-13 21:45:03.874754: 
2024-12-13 21:45:03.876072: Epoch 35
2024-12-13 21:45:03.876788: Current learning rate: 0.00787
2024-12-13 21:54:00.396850: Validation loss did not improve from -0.45426. Patience: 1/50
2024-12-13 21:54:00.397836: train_loss -0.7026
2024-12-13 21:54:00.398508: val_loss -0.3515
2024-12-13 21:54:00.399105: Pseudo dice [0.6518]
2024-12-13 21:54:00.399702: Epoch time: 536.52 s
2024-12-13 21:54:01.836247: 
2024-12-13 21:54:01.837626: Epoch 36
2024-12-13 21:54:01.838421: Current learning rate: 0.00781
2024-12-13 22:03:44.413201: Validation loss did not improve from -0.45426. Patience: 2/50
2024-12-13 22:03:44.414159: train_loss -0.7099
2024-12-13 22:03:44.415055: val_loss -0.4128
2024-12-13 22:03:44.415876: Pseudo dice [0.6788]
2024-12-13 22:03:44.416636: Epoch time: 582.58 s
2024-12-13 22:03:45.854829: 
2024-12-13 22:03:45.856238: Epoch 37
2024-12-13 22:03:45.857180: Current learning rate: 0.00775
2024-12-13 22:13:14.081188: Validation loss did not improve from -0.45426. Patience: 3/50
2024-12-13 22:13:14.082188: train_loss -0.7032
2024-12-13 22:13:14.083080: val_loss -0.4068
2024-12-13 22:13:14.083978: Pseudo dice [0.6742]
2024-12-13 22:13:14.084707: Epoch time: 568.23 s
2024-12-13 22:13:14.085378: Yayy! New best EMA pseudo Dice: 0.6678
2024-12-13 22:13:16.002332: 
2024-12-13 22:13:16.003793: Epoch 38
2024-12-13 22:13:16.004612: Current learning rate: 0.00769
2024-12-13 22:22:42.181074: Validation loss did not improve from -0.45426. Patience: 4/50
2024-12-13 22:22:42.182080: train_loss -0.7076
2024-12-13 22:22:42.183013: val_loss -0.3973
2024-12-13 22:22:42.183859: Pseudo dice [0.6935]
2024-12-13 22:22:42.184685: Epoch time: 566.18 s
2024-12-13 22:22:42.185453: Yayy! New best EMA pseudo Dice: 0.6704
2024-12-13 22:22:44.372321: 
2024-12-13 22:22:44.373611: Epoch 39
2024-12-13 22:22:44.374413: Current learning rate: 0.00763
2024-12-13 22:31:06.521971: Validation loss did not improve from -0.45426. Patience: 5/50
2024-12-13 22:31:06.522998: train_loss -0.7203
2024-12-13 22:31:06.523766: val_loss -0.4246
2024-12-13 22:31:06.524493: Pseudo dice [0.6862]
2024-12-13 22:31:06.525191: Epoch time: 502.15 s
2024-12-13 22:31:06.986300: Yayy! New best EMA pseudo Dice: 0.6719
2024-12-13 22:31:08.877923: 
2024-12-13 22:31:08.879154: Epoch 40
2024-12-13 22:31:08.879906: Current learning rate: 0.00756
2024-12-13 22:39:35.030559: Validation loss did not improve from -0.45426. Patience: 6/50
2024-12-13 22:39:35.035114: train_loss -0.7259
2024-12-13 22:39:35.036570: val_loss -0.4377
2024-12-13 22:39:35.037338: Pseudo dice [0.6963]
2024-12-13 22:39:35.038516: Epoch time: 506.16 s
2024-12-13 22:39:35.039473: Yayy! New best EMA pseudo Dice: 0.6744
2024-12-13 22:39:36.933070: 
2024-12-13 22:39:36.934467: Epoch 41
2024-12-13 22:39:36.935310: Current learning rate: 0.0075
2024-12-13 22:48:30.667981: Validation loss did not improve from -0.45426. Patience: 7/50
2024-12-13 22:48:30.668963: train_loss -0.7267
2024-12-13 22:48:30.669723: val_loss -0.3664
2024-12-13 22:48:30.670491: Pseudo dice [0.6673]
2024-12-13 22:48:30.671158: Epoch time: 533.74 s
2024-12-13 22:48:32.033620: 
2024-12-13 22:48:32.034878: Epoch 42
2024-12-13 22:48:32.035747: Current learning rate: 0.00744
2024-12-13 22:57:08.430934: Validation loss did not improve from -0.45426. Patience: 8/50
2024-12-13 22:57:08.432005: train_loss -0.7273
2024-12-13 22:57:08.432837: val_loss -0.3856
2024-12-13 22:57:08.433584: Pseudo dice [0.6655]
2024-12-13 22:57:08.434282: Epoch time: 516.4 s
2024-12-13 22:57:09.803464: 
2024-12-13 22:57:09.804615: Epoch 43
2024-12-13 22:57:09.805384: Current learning rate: 0.00738
2024-12-13 23:06:04.176681: Validation loss did not improve from -0.45426. Patience: 9/50
2024-12-13 23:06:04.177672: train_loss -0.7273
2024-12-13 23:06:04.178488: val_loss -0.3849
2024-12-13 23:06:04.179126: Pseudo dice [0.6803]
2024-12-13 23:06:04.179776: Epoch time: 534.38 s
2024-12-13 23:06:05.579504: 
2024-12-13 23:06:05.580830: Epoch 44
2024-12-13 23:06:05.581496: Current learning rate: 0.00732
2024-12-13 23:15:12.849506: Validation loss did not improve from -0.45426. Patience: 10/50
2024-12-13 23:15:12.850504: train_loss -0.7366
2024-12-13 23:15:12.851456: val_loss -0.4214
2024-12-13 23:15:12.852215: Pseudo dice [0.6924]
2024-12-13 23:15:12.852978: Epoch time: 547.27 s
2024-12-13 23:15:13.185992: Yayy! New best EMA pseudo Dice: 0.6755
2024-12-13 23:15:14.963278: 
2024-12-13 23:15:14.964551: Epoch 45
2024-12-13 23:15:14.965272: Current learning rate: 0.00725
2024-12-13 23:24:29.438806: Validation loss did not improve from -0.45426. Patience: 11/50
2024-12-13 23:24:29.439842: train_loss -0.7375
2024-12-13 23:24:29.440854: val_loss -0.4236
2024-12-13 23:24:29.441604: Pseudo dice [0.6938]
2024-12-13 23:24:29.442491: Epoch time: 554.48 s
2024-12-13 23:24:29.443291: Yayy! New best EMA pseudo Dice: 0.6773
2024-12-13 23:24:31.284051: 
2024-12-13 23:24:31.285376: Epoch 46
2024-12-13 23:24:31.286231: Current learning rate: 0.00719
2024-12-13 23:34:06.738146: Validation loss did not improve from -0.45426. Patience: 12/50
2024-12-13 23:34:06.739112: train_loss -0.7438
2024-12-13 23:34:06.739802: val_loss -0.4198
2024-12-13 23:34:06.740592: Pseudo dice [0.6971]
2024-12-13 23:34:06.741251: Epoch time: 575.46 s
2024-12-13 23:34:06.741969: Yayy! New best EMA pseudo Dice: 0.6793
2024-12-13 23:34:08.533085: 
2024-12-13 23:34:08.534576: Epoch 47
2024-12-13 23:34:08.535291: Current learning rate: 0.00713
2024-12-13 23:43:15.332403: Validation loss did not improve from -0.45426. Patience: 13/50
2024-12-13 23:43:15.333215: train_loss -0.7436
2024-12-13 23:43:15.335048: val_loss -0.3823
2024-12-13 23:43:15.336020: Pseudo dice [0.6736]
2024-12-13 23:43:15.337228: Epoch time: 546.8 s
2024-12-13 23:43:16.717668: 
2024-12-13 23:43:16.719108: Epoch 48
2024-12-13 23:43:16.720074: Current learning rate: 0.00707
2024-12-13 23:52:09.944625: Validation loss did not improve from -0.45426. Patience: 14/50
2024-12-13 23:52:09.946328: train_loss -0.7427
2024-12-13 23:52:09.947258: val_loss -0.4031
2024-12-13 23:52:09.948088: Pseudo dice [0.6789]
2024-12-13 23:52:09.948933: Epoch time: 533.23 s
2024-12-13 23:52:11.364255: 
2024-12-13 23:52:11.365721: Epoch 49
2024-12-13 23:52:11.366575: Current learning rate: 0.007
2024-12-14 00:01:50.518426: Validation loss did not improve from -0.45426. Patience: 15/50
2024-12-14 00:01:50.519448: train_loss -0.7495
2024-12-14 00:01:50.520451: val_loss -0.4132
2024-12-14 00:01:50.521310: Pseudo dice [0.6834]
2024-12-14 00:01:50.522215: Epoch time: 579.16 s
2024-12-14 00:01:52.750706: 
2024-12-14 00:01:52.752037: Epoch 50
2024-12-14 00:01:52.753029: Current learning rate: 0.00694
2024-12-14 00:11:08.145078: Validation loss did not improve from -0.45426. Patience: 16/50
2024-12-14 00:11:08.146698: train_loss -0.7531
2024-12-14 00:11:08.147700: val_loss -0.4062
2024-12-14 00:11:08.148416: Pseudo dice [0.6915]
2024-12-14 00:11:08.149095: Epoch time: 555.4 s
2024-12-14 00:11:08.149802: Yayy! New best EMA pseudo Dice: 0.6804
2024-12-14 00:11:09.961313: 
2024-12-14 00:11:09.962659: Epoch 51
2024-12-14 00:11:09.963499: Current learning rate: 0.00688
2024-12-14 00:20:27.709468: Validation loss did not improve from -0.45426. Patience: 17/50
2024-12-14 00:20:27.710566: train_loss -0.7483
2024-12-14 00:20:27.711380: val_loss -0.4092
2024-12-14 00:20:27.712134: Pseudo dice [0.6994]
2024-12-14 00:20:27.712879: Epoch time: 557.75 s
2024-12-14 00:20:27.713612: Yayy! New best EMA pseudo Dice: 0.6823
2024-12-14 00:20:29.580033: 
2024-12-14 00:20:29.581361: Epoch 52
2024-12-14 00:20:29.582084: Current learning rate: 0.00682
2024-12-14 00:29:29.463015: Validation loss did not improve from -0.45426. Patience: 18/50
2024-12-14 00:29:29.463961: train_loss -0.755
2024-12-14 00:29:29.464730: val_loss -0.446
2024-12-14 00:29:29.465351: Pseudo dice [0.697]
2024-12-14 00:29:29.466020: Epoch time: 539.88 s
2024-12-14 00:29:29.466622: Yayy! New best EMA pseudo Dice: 0.6838
2024-12-14 00:29:31.259914: 
2024-12-14 00:29:31.261103: Epoch 53
2024-12-14 00:29:31.261852: Current learning rate: 0.00675
2024-12-14 00:38:08.934000: Validation loss did not improve from -0.45426. Patience: 19/50
2024-12-14 00:38:08.935126: train_loss -0.76
2024-12-14 00:38:08.935955: val_loss -0.4029
2024-12-14 00:38:08.936724: Pseudo dice [0.6942]
2024-12-14 00:38:08.937334: Epoch time: 517.68 s
2024-12-14 00:38:08.937969: Yayy! New best EMA pseudo Dice: 0.6848
2024-12-14 00:38:10.767222: 
2024-12-14 00:38:10.768573: Epoch 54
2024-12-14 00:38:10.769272: Current learning rate: 0.00669
2024-12-14 00:47:12.999696: Validation loss did not improve from -0.45426. Patience: 20/50
2024-12-14 00:47:13.000603: train_loss -0.7645
2024-12-14 00:47:13.001300: val_loss -0.4538
2024-12-14 00:47:13.002087: Pseudo dice [0.7126]
2024-12-14 00:47:13.002918: Epoch time: 542.23 s
2024-12-14 00:47:13.444105: Yayy! New best EMA pseudo Dice: 0.6876
2024-12-14 00:47:15.205921: 
2024-12-14 00:47:15.207076: Epoch 55
2024-12-14 00:47:15.207837: Current learning rate: 0.00663
2024-12-14 00:57:10.441918: Validation loss did not improve from -0.45426. Patience: 21/50
2024-12-14 00:57:10.443799: train_loss -0.7677
2024-12-14 00:57:10.444740: val_loss -0.4048
2024-12-14 00:57:10.445526: Pseudo dice [0.6929]
2024-12-14 00:57:10.446375: Epoch time: 595.24 s
2024-12-14 00:57:10.447195: Yayy! New best EMA pseudo Dice: 0.6881
2024-12-14 00:57:12.488701: 
2024-12-14 00:57:12.490040: Epoch 56
2024-12-14 00:57:12.490882: Current learning rate: 0.00657
2024-12-14 01:06:43.547506: Validation loss did not improve from -0.45426. Patience: 22/50
2024-12-14 01:06:43.548501: train_loss -0.7642
2024-12-14 01:06:43.549524: val_loss -0.3598
2024-12-14 01:06:43.550433: Pseudo dice [0.6833]
2024-12-14 01:06:43.551455: Epoch time: 571.06 s
2024-12-14 01:06:45.026067: 
2024-12-14 01:06:45.027596: Epoch 57
2024-12-14 01:06:45.028600: Current learning rate: 0.0065
2024-12-14 01:15:18.304054: Validation loss did not improve from -0.45426. Patience: 23/50
2024-12-14 01:15:18.305075: train_loss -0.7655
2024-12-14 01:15:18.306268: val_loss -0.3992
2024-12-14 01:15:18.307360: Pseudo dice [0.6919]
2024-12-14 01:15:18.308461: Epoch time: 513.28 s
2024-12-14 01:15:19.746619: 
2024-12-14 01:15:19.747974: Epoch 58
2024-12-14 01:15:19.749099: Current learning rate: 0.00644
2024-12-14 01:24:18.377328: Validation loss did not improve from -0.45426. Patience: 24/50
2024-12-14 01:24:18.378304: train_loss -0.7677
2024-12-14 01:24:18.378987: val_loss -0.3899
2024-12-14 01:24:18.379633: Pseudo dice [0.6876]
2024-12-14 01:24:18.380258: Epoch time: 538.63 s
2024-12-14 01:24:19.820799: 
2024-12-14 01:24:19.822183: Epoch 59
2024-12-14 01:24:19.823014: Current learning rate: 0.00638
2024-12-14 01:33:25.713129: Validation loss improved from -0.45426 to -0.45446! Patience: 24/50
2024-12-14 01:33:25.714159: train_loss -0.7586
2024-12-14 01:33:25.714949: val_loss -0.4545
2024-12-14 01:33:25.715585: Pseudo dice [0.7088]
2024-12-14 01:33:25.716308: Epoch time: 545.89 s
2024-12-14 01:33:26.190934: Yayy! New best EMA pseudo Dice: 0.6901
2024-12-14 01:33:29.042535: 
2024-12-14 01:33:29.043922: Epoch 60
2024-12-14 01:33:29.044633: Current learning rate: 0.00631
2024-12-14 01:43:14.177574: Validation loss did not improve from -0.45446. Patience: 1/50
2024-12-14 01:43:14.178540: train_loss -0.7736
2024-12-14 01:43:14.179397: val_loss -0.4364
2024-12-14 01:43:14.180169: Pseudo dice [0.7054]
2024-12-14 01:43:14.180832: Epoch time: 585.14 s
2024-12-14 01:43:14.181420: Yayy! New best EMA pseudo Dice: 0.6916
2024-12-14 01:43:15.998672: 
2024-12-14 01:43:15.999989: Epoch 61
2024-12-14 01:43:16.000690: Current learning rate: 0.00625
2024-12-14 01:52:14.909264: Validation loss did not improve from -0.45446. Patience: 2/50
2024-12-14 01:52:14.910333: train_loss -0.763
2024-12-14 01:52:14.911324: val_loss -0.3861
2024-12-14 01:52:14.912215: Pseudo dice [0.6814]
2024-12-14 01:52:14.913192: Epoch time: 538.91 s
2024-12-14 01:52:16.341447: 
2024-12-14 01:52:16.342898: Epoch 62
2024-12-14 01:52:16.343957: Current learning rate: 0.00619
2024-12-14 02:00:58.664901: Validation loss did not improve from -0.45446. Patience: 3/50
2024-12-14 02:00:58.669961: train_loss -0.7638
2024-12-14 02:00:58.671753: val_loss -0.4368
2024-12-14 02:00:58.672443: Pseudo dice [0.7116]
2024-12-14 02:00:58.673350: Epoch time: 522.33 s
2024-12-14 02:00:58.674251: Yayy! New best EMA pseudo Dice: 0.6927
2024-12-14 02:01:00.529773: 
2024-12-14 02:01:00.530956: Epoch 63
2024-12-14 02:01:00.531660: Current learning rate: 0.00612
2024-12-14 02:10:06.378358: Validation loss did not improve from -0.45446. Patience: 4/50
2024-12-14 02:10:06.379730: train_loss -0.769
2024-12-14 02:10:06.380748: val_loss -0.4168
2024-12-14 02:10:06.381467: Pseudo dice [0.6961]
2024-12-14 02:10:06.382238: Epoch time: 545.85 s
2024-12-14 02:10:06.382987: Yayy! New best EMA pseudo Dice: 0.693
2024-12-14 02:10:08.200946: 
2024-12-14 02:10:08.202175: Epoch 64
2024-12-14 02:10:08.202972: Current learning rate: 0.00606
2024-12-14 02:19:33.682996: Validation loss did not improve from -0.45446. Patience: 5/50
2024-12-14 02:19:33.684066: train_loss -0.7776
2024-12-14 02:19:33.684863: val_loss -0.4056
2024-12-14 02:19:33.685600: Pseudo dice [0.6987]
2024-12-14 02:19:33.686357: Epoch time: 565.48 s
2024-12-14 02:19:34.004844: Yayy! New best EMA pseudo Dice: 0.6936
2024-12-14 02:19:35.833809: 
2024-12-14 02:19:35.835045: Epoch 65
2024-12-14 02:19:35.835888: Current learning rate: 0.006
2024-12-14 02:29:00.868258: Validation loss did not improve from -0.45446. Patience: 6/50
2024-12-14 02:29:00.869189: train_loss -0.7815
2024-12-14 02:29:00.870157: val_loss -0.3751
2024-12-14 02:29:00.871080: Pseudo dice [0.6812]
2024-12-14 02:29:00.871956: Epoch time: 565.04 s
2024-12-14 02:29:02.296417: 
2024-12-14 02:29:02.297951: Epoch 66
2024-12-14 02:29:02.298871: Current learning rate: 0.00593
2024-12-14 02:38:31.447944: Validation loss did not improve from -0.45446. Patience: 7/50
2024-12-14 02:38:31.448830: train_loss -0.7818
2024-12-14 02:38:31.449951: val_loss -0.3662
2024-12-14 02:38:31.450910: Pseudo dice [0.6832]
2024-12-14 02:38:31.451873: Epoch time: 569.15 s
2024-12-14 02:38:32.892013: 
2024-12-14 02:38:32.893482: Epoch 67
2024-12-14 02:38:32.894557: Current learning rate: 0.00587
2024-12-14 02:47:38.373590: Validation loss did not improve from -0.45446. Patience: 8/50
2024-12-14 02:47:38.374593: train_loss -0.786
2024-12-14 02:47:38.375553: val_loss -0.3884
2024-12-14 02:47:38.376493: Pseudo dice [0.6964]
2024-12-14 02:47:38.377353: Epoch time: 545.48 s
2024-12-14 02:47:39.856198: 
2024-12-14 02:47:39.857398: Epoch 68
2024-12-14 02:47:39.858193: Current learning rate: 0.00581
2024-12-14 02:57:19.855922: Validation loss did not improve from -0.45446. Patience: 9/50
2024-12-14 02:57:19.856912: train_loss -0.7928
2024-12-14 02:57:19.858904: val_loss -0.439
2024-12-14 02:57:19.859675: Pseudo dice [0.7113]
2024-12-14 02:57:19.860793: Epoch time: 580.0 s
2024-12-14 02:57:19.861557: Yayy! New best EMA pseudo Dice: 0.6939
2024-12-14 02:57:21.808706: 
2024-12-14 02:57:21.810146: Epoch 69
2024-12-14 02:57:21.810894: Current learning rate: 0.00574
2024-12-14 03:06:46.256362: Validation loss did not improve from -0.45446. Patience: 10/50
2024-12-14 03:06:46.260413: train_loss -0.7848
2024-12-14 03:06:46.261311: val_loss -0.4035
2024-12-14 03:06:46.261947: Pseudo dice [0.686]
2024-12-14 03:06:46.262717: Epoch time: 564.45 s
2024-12-14 03:06:48.557897: 
2024-12-14 03:06:48.559266: Epoch 70
2024-12-14 03:06:48.559955: Current learning rate: 0.00568
2024-12-14 03:15:50.518737: Validation loss did not improve from -0.45446. Patience: 11/50
2024-12-14 03:15:50.519841: train_loss -0.7864
2024-12-14 03:15:50.520756: val_loss -0.4088
2024-12-14 03:15:50.521498: Pseudo dice [0.6879]
2024-12-14 03:15:50.522309: Epoch time: 541.96 s
2024-12-14 03:15:51.977709: 
2024-12-14 03:15:51.979033: Epoch 71
2024-12-14 03:15:51.979990: Current learning rate: 0.00562
2024-12-14 03:25:21.393122: Validation loss did not improve from -0.45446. Patience: 12/50
2024-12-14 03:25:21.394056: train_loss -0.7885
2024-12-14 03:25:21.394984: val_loss -0.4338
2024-12-14 03:25:21.395926: Pseudo dice [0.7055]
2024-12-14 03:25:21.396810: Epoch time: 569.42 s
2024-12-14 03:25:22.856538: 
2024-12-14 03:25:22.857849: Epoch 72
2024-12-14 03:25:22.858752: Current learning rate: 0.00555
2024-12-14 03:35:00.222166: Validation loss did not improve from -0.45446. Patience: 13/50
2024-12-14 03:35:00.223114: train_loss -0.7905
2024-12-14 03:35:00.223883: val_loss -0.4118
2024-12-14 03:35:00.224541: Pseudo dice [0.6939]
2024-12-14 03:35:00.225205: Epoch time: 577.37 s
2024-12-14 03:35:01.669771: 
2024-12-14 03:35:01.670991: Epoch 73
2024-12-14 03:35:01.671631: Current learning rate: 0.00549
2024-12-14 03:43:57.180829: Validation loss did not improve from -0.45446. Patience: 14/50
2024-12-14 03:43:57.181799: train_loss -0.7878
2024-12-14 03:43:57.182680: val_loss -0.414
2024-12-14 03:43:57.183487: Pseudo dice [0.6962]
2024-12-14 03:43:57.184424: Epoch time: 535.51 s
2024-12-14 03:43:57.185216: Yayy! New best EMA pseudo Dice: 0.6941
2024-12-14 03:43:59.040955: 
2024-12-14 03:43:59.042777: Epoch 74
2024-12-14 03:43:59.043622: Current learning rate: 0.00542
2024-12-14 03:53:14.355379: Validation loss did not improve from -0.45446. Patience: 15/50
2024-12-14 03:53:14.356361: train_loss -0.7893
2024-12-14 03:53:14.357158: val_loss -0.403
2024-12-14 03:53:14.357846: Pseudo dice [0.6859]
2024-12-14 03:53:14.358458: Epoch time: 555.32 s
2024-12-14 03:53:16.186999: 
2024-12-14 03:53:16.188287: Epoch 75
2024-12-14 03:53:16.189294: Current learning rate: 0.00536
2024-12-14 04:02:21.410836: Validation loss did not improve from -0.45446. Patience: 16/50
2024-12-14 04:02:21.411789: train_loss -0.7905
2024-12-14 04:02:21.412628: val_loss -0.384
2024-12-14 04:02:21.413448: Pseudo dice [0.678]
2024-12-14 04:02:21.414298: Epoch time: 545.23 s
2024-12-14 04:02:22.846270: 
2024-12-14 04:02:22.847509: Epoch 76
2024-12-14 04:02:22.848199: Current learning rate: 0.00529
2024-12-14 04:11:59.885666: Validation loss did not improve from -0.45446. Patience: 17/50
2024-12-14 04:11:59.887826: train_loss -0.7974
2024-12-14 04:11:59.888918: val_loss -0.3713
2024-12-14 04:11:59.889930: Pseudo dice [0.6776]
2024-12-14 04:11:59.890711: Epoch time: 577.04 s
2024-12-14 04:12:01.364980: 
2024-12-14 04:12:01.366536: Epoch 77
2024-12-14 04:12:01.367358: Current learning rate: 0.00523
2024-12-14 04:21:26.654898: Validation loss did not improve from -0.45446. Patience: 18/50
2024-12-14 04:21:26.656039: train_loss -0.7966
2024-12-14 04:21:26.657110: val_loss -0.3809
2024-12-14 04:21:26.658013: Pseudo dice [0.6862]
2024-12-14 04:21:26.658819: Epoch time: 565.29 s
2024-12-14 04:21:28.151596: 
2024-12-14 04:21:28.153006: Epoch 78
2024-12-14 04:21:28.154215: Current learning rate: 0.00517
2024-12-14 04:30:28.562616: Validation loss did not improve from -0.45446. Patience: 19/50
2024-12-14 04:30:28.563574: train_loss -0.796
2024-12-14 04:30:28.564391: val_loss -0.3543
2024-12-14 04:30:28.565119: Pseudo dice [0.677]
2024-12-14 04:30:28.565774: Epoch time: 540.41 s
2024-12-14 04:30:30.032661: 
2024-12-14 04:30:30.033781: Epoch 79
2024-12-14 04:30:30.034458: Current learning rate: 0.0051
2024-12-14 04:40:02.585887: Validation loss did not improve from -0.45446. Patience: 20/50
2024-12-14 04:40:02.586557: train_loss -0.7979
2024-12-14 04:40:02.587541: val_loss -0.4535
2024-12-14 04:40:02.588522: Pseudo dice [0.7211]
2024-12-14 04:40:02.589307: Epoch time: 572.56 s
2024-12-14 04:40:04.519269: 
2024-12-14 04:40:04.520824: Epoch 80
2024-12-14 04:40:04.522267: Current learning rate: 0.00504
2024-12-14 04:49:01.616799: Validation loss did not improve from -0.45446. Patience: 21/50
2024-12-14 04:49:01.617824: train_loss -0.7996
2024-12-14 04:49:01.618697: val_loss -0.3526
2024-12-14 04:49:01.619531: Pseudo dice [0.6893]
2024-12-14 04:49:01.620821: Epoch time: 537.1 s
2024-12-14 04:49:03.504499: 
2024-12-14 04:49:03.505902: Epoch 81
2024-12-14 04:49:03.506999: Current learning rate: 0.00497
2024-12-14 04:58:34.088902: Validation loss did not improve from -0.45446. Patience: 22/50
2024-12-14 04:58:34.089774: train_loss -0.7977
2024-12-14 04:58:34.090631: val_loss -0.3696
2024-12-14 04:58:34.091456: Pseudo dice [0.6763]
2024-12-14 04:58:34.092250: Epoch time: 570.59 s
2024-12-14 04:58:35.600672: 
2024-12-14 04:58:35.601968: Epoch 82
2024-12-14 04:58:35.602754: Current learning rate: 0.00491
2024-12-14 05:08:17.195562: Validation loss did not improve from -0.45446. Patience: 23/50
2024-12-14 05:08:17.196462: train_loss -0.7937
2024-12-14 05:08:17.197996: val_loss -0.3978
2024-12-14 05:08:17.198938: Pseudo dice [0.7055]
2024-12-14 05:08:17.199811: Epoch time: 581.6 s
2024-12-14 05:08:18.574207: 
2024-12-14 05:08:18.575082: Epoch 83
2024-12-14 05:08:18.575764: Current learning rate: 0.00484
2024-12-14 05:17:27.852403: Validation loss did not improve from -0.45446. Patience: 24/50
2024-12-14 05:17:27.880480: train_loss -0.8023
2024-12-14 05:17:27.881622: val_loss -0.3719
2024-12-14 05:17:27.882341: Pseudo dice [0.6821]
2024-12-14 05:17:27.883313: Epoch time: 549.28 s
2024-12-14 05:17:29.420064: 
2024-12-14 05:17:29.421430: Epoch 84
2024-12-14 05:17:29.422201: Current learning rate: 0.00478
2024-12-14 05:26:46.429697: Validation loss did not improve from -0.45446. Patience: 25/50
2024-12-14 05:26:46.430833: train_loss -0.7972
2024-12-14 05:26:46.431707: val_loss -0.4228
2024-12-14 05:26:46.432765: Pseudo dice [0.7041]
2024-12-14 05:26:46.433746: Epoch time: 557.01 s
2024-12-14 05:26:48.217038: 
2024-12-14 05:26:48.218635: Epoch 85
2024-12-14 05:26:48.219802: Current learning rate: 0.00471
2024-12-14 05:35:58.133983: Validation loss did not improve from -0.45446. Patience: 26/50
2024-12-14 05:35:58.134881: train_loss -0.7974
2024-12-14 05:35:58.135813: val_loss -0.381
2024-12-14 05:35:58.136502: Pseudo dice [0.6938]
2024-12-14 05:35:58.137172: Epoch time: 549.92 s
2024-12-14 05:35:59.561616: 
2024-12-14 05:35:59.562631: Epoch 86
2024-12-14 05:35:59.563492: Current learning rate: 0.00465
2024-12-14 05:45:22.139129: Validation loss did not improve from -0.45446. Patience: 27/50
2024-12-14 05:45:22.140218: train_loss -0.7995
2024-12-14 05:45:22.140947: val_loss -0.4044
2024-12-14 05:45:22.141695: Pseudo dice [0.7027]
2024-12-14 05:45:22.142535: Epoch time: 562.58 s
2024-12-14 05:45:23.548047: 
2024-12-14 05:45:23.549459: Epoch 87
2024-12-14 05:45:23.550366: Current learning rate: 0.00458
2024-12-14 05:55:14.114023: Validation loss did not improve from -0.45446. Patience: 28/50
2024-12-14 05:55:14.114813: train_loss -0.8008
2024-12-14 05:55:14.115718: val_loss -0.3954
2024-12-14 05:55:14.116489: Pseudo dice [0.689]
2024-12-14 05:55:14.117276: Epoch time: 590.57 s
2024-12-14 05:55:15.497134: 
2024-12-14 05:55:15.498455: Epoch 88
2024-12-14 05:55:15.499449: Current learning rate: 0.00452
2024-12-14 06:04:55.661535: Validation loss did not improve from -0.45446. Patience: 29/50
2024-12-14 06:04:55.662597: train_loss -0.8019
2024-12-14 06:04:55.663450: val_loss -0.3773
2024-12-14 06:04:55.664094: Pseudo dice [0.6828]
2024-12-14 06:04:55.664883: Epoch time: 580.17 s
2024-12-14 06:04:57.030545: 
2024-12-14 06:04:57.032027: Epoch 89
2024-12-14 06:04:57.033016: Current learning rate: 0.00445
2024-12-14 06:14:20.684425: Validation loss did not improve from -0.45446. Patience: 30/50
2024-12-14 06:14:20.685629: train_loss -0.8014
2024-12-14 06:14:20.687461: val_loss -0.3876
2024-12-14 06:14:20.688822: Pseudo dice [0.6895]
2024-12-14 06:14:20.690108: Epoch time: 563.66 s
2024-12-14 06:14:22.467478: 
2024-12-14 06:14:22.469018: Epoch 90
2024-12-14 06:14:22.470041: Current learning rate: 0.00438
2024-12-14 06:23:19.142800: Validation loss did not improve from -0.45446. Patience: 31/50
2024-12-14 06:23:19.145499: train_loss -0.809
2024-12-14 06:23:19.147472: val_loss -0.3867
2024-12-14 06:23:19.148209: Pseudo dice [0.6933]
2024-12-14 06:23:19.148886: Epoch time: 536.68 s
2024-12-14 06:23:20.533002: 
2024-12-14 06:23:20.534594: Epoch 91
2024-12-14 06:23:20.535600: Current learning rate: 0.00432
2024-12-14 06:32:41.322513: Validation loss did not improve from -0.45446. Patience: 32/50
2024-12-14 06:32:41.323765: train_loss -0.8055
2024-12-14 06:32:41.325000: val_loss -0.3295
2024-12-14 06:32:41.325795: Pseudo dice [0.6618]
2024-12-14 06:32:41.326482: Epoch time: 560.79 s
2024-12-14 06:32:43.274978: 
2024-12-14 06:32:43.276334: Epoch 92
2024-12-14 06:32:43.277230: Current learning rate: 0.00425
2024-12-14 06:42:23.239958: Validation loss did not improve from -0.45446. Patience: 33/50
2024-12-14 06:42:23.240797: train_loss -0.8064
2024-12-14 06:42:23.241537: val_loss -0.3966
2024-12-14 06:42:23.242320: Pseudo dice [0.6979]
2024-12-14 06:42:23.243003: Epoch time: 579.97 s
2024-12-14 06:42:24.631492: 
2024-12-14 06:42:24.632899: Epoch 93
2024-12-14 06:42:24.633728: Current learning rate: 0.00419
2024-12-14 06:51:09.528665: Validation loss did not improve from -0.45446. Patience: 34/50
2024-12-14 06:51:09.529604: train_loss -0.8101
2024-12-14 06:51:09.530514: val_loss -0.409
2024-12-14 06:51:09.531293: Pseudo dice [0.7099]
2024-12-14 06:51:09.531964: Epoch time: 524.9 s
2024-12-14 06:51:10.912493: 
2024-12-14 06:51:10.913781: Epoch 94
2024-12-14 06:51:10.914510: Current learning rate: 0.00412
2024-12-14 07:00:57.675985: Validation loss did not improve from -0.45446. Patience: 35/50
2024-12-14 07:00:57.677010: train_loss -0.8104
2024-12-14 07:00:57.677733: val_loss -0.4189
2024-12-14 07:00:57.678485: Pseudo dice [0.697]
2024-12-14 07:00:57.679170: Epoch time: 586.77 s
2024-12-14 07:00:59.521497: 
2024-12-14 07:00:59.522576: Epoch 95
2024-12-14 07:00:59.523216: Current learning rate: 0.00405
2024-12-14 07:10:23.258383: Validation loss did not improve from -0.45446. Patience: 36/50
2024-12-14 07:10:23.259373: train_loss -0.8125
2024-12-14 07:10:23.260105: val_loss -0.388
2024-12-14 07:10:23.260907: Pseudo dice [0.6968]
2024-12-14 07:10:23.261588: Epoch time: 563.74 s
2024-12-14 07:10:24.653669: 
2024-12-14 07:10:24.655079: Epoch 96
2024-12-14 07:10:24.655907: Current learning rate: 0.00399
2024-12-14 07:19:10.533045: Validation loss did not improve from -0.45446. Patience: 37/50
2024-12-14 07:19:10.534085: train_loss -0.8139
2024-12-14 07:19:10.534839: val_loss -0.3774
2024-12-14 07:19:10.535622: Pseudo dice [0.6894]
2024-12-14 07:19:10.536323: Epoch time: 525.88 s
2024-12-14 07:19:11.946419: 
2024-12-14 07:19:11.947618: Epoch 97
2024-12-14 07:19:11.948278: Current learning rate: 0.00392
2024-12-14 07:28:33.105824: Validation loss did not improve from -0.45446. Patience: 38/50
2024-12-14 07:28:33.107741: train_loss -0.8111
2024-12-14 07:28:33.108876: val_loss -0.4095
2024-12-14 07:28:33.109659: Pseudo dice [0.7006]
2024-12-14 07:28:33.110444: Epoch time: 561.16 s
2024-12-14 07:28:34.573607: 
2024-12-14 07:28:34.574923: Epoch 98
2024-12-14 07:28:34.575697: Current learning rate: 0.00385
2024-12-14 07:37:35.009219: Validation loss improved from -0.45446 to -0.45602! Patience: 38/50
2024-12-14 07:37:35.010043: train_loss -0.8111
2024-12-14 07:37:35.010816: val_loss -0.456
2024-12-14 07:37:35.011602: Pseudo dice [0.7134]
2024-12-14 07:37:35.012335: Epoch time: 540.44 s
2024-12-14 07:37:35.012977: Yayy! New best EMA pseudo Dice: 0.6952
2024-12-14 07:37:36.823540: 
2024-12-14 07:37:36.824888: Epoch 99
2024-12-14 07:37:36.825561: Current learning rate: 0.00379
2024-12-14 07:46:24.688859: Validation loss did not improve from -0.45602. Patience: 1/50
2024-12-14 07:46:24.689831: train_loss -0.8158
2024-12-14 07:46:24.690547: val_loss -0.4227
2024-12-14 07:46:24.691176: Pseudo dice [0.7154]
2024-12-14 07:46:24.691846: Epoch time: 527.87 s
2024-12-14 07:46:25.059520: Yayy! New best EMA pseudo Dice: 0.6972
2024-12-14 07:46:26.899873: 
2024-12-14 07:46:26.901043: Epoch 100
2024-12-14 07:46:26.901839: Current learning rate: 0.00372
2024-12-14 07:55:37.297117: Validation loss did not improve from -0.45602. Patience: 2/50
2024-12-14 07:55:37.298122: train_loss -0.8161
2024-12-14 07:55:37.298910: val_loss -0.3539
2024-12-14 07:55:37.299536: Pseudo dice [0.6848]
2024-12-14 07:55:37.300177: Epoch time: 550.4 s
2024-12-14 07:55:38.705519: 
2024-12-14 07:55:38.706822: Epoch 101
2024-12-14 07:55:38.707499: Current learning rate: 0.00365
2024-12-14 08:05:15.224909: Validation loss did not improve from -0.45602. Patience: 3/50
2024-12-14 08:05:15.225964: train_loss -0.8194
2024-12-14 08:05:15.226895: val_loss -0.3509
2024-12-14 08:05:15.227629: Pseudo dice [0.681]
2024-12-14 08:05:15.228264: Epoch time: 576.52 s
2024-12-14 08:05:16.633835: 
2024-12-14 08:05:16.635224: Epoch 102
2024-12-14 08:05:16.635967: Current learning rate: 0.00359
2024-12-14 08:15:01.749404: Validation loss did not improve from -0.45602. Patience: 4/50
2024-12-14 08:15:01.750396: train_loss -0.8151
2024-12-14 08:15:01.751236: val_loss -0.3514
2024-12-14 08:15:01.751873: Pseudo dice [0.6715]
2024-12-14 08:15:01.752581: Epoch time: 585.12 s
2024-12-14 08:15:03.507172: 
2024-12-14 08:15:03.508595: Epoch 103
2024-12-14 08:15:03.509452: Current learning rate: 0.00352
2024-12-14 08:24:19.704755: Validation loss did not improve from -0.45602. Patience: 5/50
2024-12-14 08:24:19.705830: train_loss -0.8177
2024-12-14 08:24:19.706654: val_loss -0.3749
2024-12-14 08:24:19.707348: Pseudo dice [0.683]
2024-12-14 08:24:19.708186: Epoch time: 556.2 s
2024-12-14 08:24:21.112140: 
2024-12-14 08:24:21.113475: Epoch 104
2024-12-14 08:24:21.114297: Current learning rate: 0.00345
2024-12-14 08:33:46.485709: Validation loss did not improve from -0.45602. Patience: 6/50
2024-12-14 08:33:46.487916: train_loss -0.8177
2024-12-14 08:33:46.489756: val_loss -0.3803
2024-12-14 08:33:46.490869: Pseudo dice [0.7011]
2024-12-14 08:33:46.491827: Epoch time: 565.38 s
2024-12-14 08:33:48.374554: 
2024-12-14 08:33:48.376060: Epoch 105
2024-12-14 08:33:48.377311: Current learning rate: 0.00338
2024-12-14 08:43:22.495440: Validation loss did not improve from -0.45602. Patience: 7/50
2024-12-14 08:43:22.496786: train_loss -0.8192
2024-12-14 08:43:22.497553: val_loss -0.3809
2024-12-14 08:43:22.498229: Pseudo dice [0.6881]
2024-12-14 08:43:22.499012: Epoch time: 574.12 s
2024-12-14 08:43:23.939941: 
2024-12-14 08:43:23.941130: Epoch 106
2024-12-14 08:43:23.941822: Current learning rate: 0.00332
2024-12-14 08:52:06.801162: Validation loss did not improve from -0.45602. Patience: 8/50
2024-12-14 08:52:06.802207: train_loss -0.8227
2024-12-14 08:52:06.803002: val_loss -0.4172
2024-12-14 08:52:06.803666: Pseudo dice [0.7081]
2024-12-14 08:52:06.804428: Epoch time: 522.86 s
2024-12-14 08:52:08.211320: 
2024-12-14 08:52:08.212568: Epoch 107
2024-12-14 08:52:08.213238: Current learning rate: 0.00325
2024-12-14 09:02:19.136470: Validation loss did not improve from -0.45602. Patience: 9/50
2024-12-14 09:02:19.137314: train_loss -0.8208
2024-12-14 09:02:19.138187: val_loss -0.3958
2024-12-14 09:02:19.139001: Pseudo dice [0.7059]
2024-12-14 09:02:19.139942: Epoch time: 610.93 s
2024-12-14 09:02:20.567518: 
2024-12-14 09:02:20.568917: Epoch 108
2024-12-14 09:02:20.569688: Current learning rate: 0.00318
2024-12-14 09:12:01.391333: Validation loss did not improve from -0.45602. Patience: 10/50
2024-12-14 09:12:01.392325: train_loss -0.8216
2024-12-14 09:12:01.393115: val_loss -0.3768
2024-12-14 09:12:01.393786: Pseudo dice [0.696]
2024-12-14 09:12:01.394439: Epoch time: 580.83 s
2024-12-14 09:12:02.836156: 
2024-12-14 09:12:02.837496: Epoch 109
2024-12-14 09:12:02.838208: Current learning rate: 0.00311
2024-12-14 09:20:56.458937: Validation loss did not improve from -0.45602. Patience: 11/50
2024-12-14 09:20:56.459838: train_loss -0.8212
2024-12-14 09:20:56.460687: val_loss -0.4407
2024-12-14 09:20:56.461569: Pseudo dice [0.7166]
2024-12-14 09:20:56.462222: Epoch time: 533.62 s
2024-12-14 09:20:58.397621: 
2024-12-14 09:20:58.398912: Epoch 110
2024-12-14 09:20:58.399612: Current learning rate: 0.00304
2024-12-14 09:30:07.452581: Validation loss did not improve from -0.45602. Patience: 12/50
2024-12-14 09:30:07.454970: train_loss -0.821
2024-12-14 09:30:07.455908: val_loss -0.385
2024-12-14 09:30:07.456640: Pseudo dice [0.6873]
2024-12-14 09:30:07.457631: Epoch time: 549.06 s
2024-12-14 09:30:08.882573: 
2024-12-14 09:30:08.884125: Epoch 111
2024-12-14 09:30:08.885318: Current learning rate: 0.00297
2024-12-14 09:39:24.066984: Validation loss did not improve from -0.45602. Patience: 13/50
2024-12-14 09:39:24.068078: train_loss -0.8244
2024-12-14 09:39:24.068888: val_loss -0.3636
2024-12-14 09:39:24.069547: Pseudo dice [0.6943]
2024-12-14 09:39:24.070179: Epoch time: 555.19 s
2024-12-14 09:39:25.493029: 
2024-12-14 09:39:25.494327: Epoch 112
2024-12-14 09:39:25.495060: Current learning rate: 0.00291
2024-12-14 09:48:53.224917: Validation loss did not improve from -0.45602. Patience: 14/50
2024-12-14 09:48:53.225735: train_loss -0.8226
2024-12-14 09:48:53.226526: val_loss -0.4087
2024-12-14 09:48:53.227248: Pseudo dice [0.7113]
2024-12-14 09:48:53.227920: Epoch time: 567.73 s
2024-12-14 09:48:53.228570: Yayy! New best EMA pseudo Dice: 0.6974
2024-12-14 09:48:55.052761: 
2024-12-14 09:48:55.054197: Epoch 113
2024-12-14 09:48:55.055034: Current learning rate: 0.00284
2024-12-14 09:58:27.937104: Validation loss did not improve from -0.45602. Patience: 15/50
2024-12-14 09:58:27.938041: train_loss -0.8233
2024-12-14 09:58:27.938937: val_loss -0.3762
2024-12-14 09:58:27.939811: Pseudo dice [0.6876]
2024-12-14 09:58:27.940632: Epoch time: 572.89 s
2024-12-14 09:58:29.697566: 
2024-12-14 09:58:29.698764: Epoch 114
2024-12-14 09:58:29.699748: Current learning rate: 0.00277
2024-12-14 10:07:28.847700: Validation loss did not improve from -0.45602. Patience: 16/50
2024-12-14 10:07:28.848686: train_loss -0.8255
2024-12-14 10:07:28.849600: val_loss -0.3578
2024-12-14 10:07:28.850784: Pseudo dice [0.6876]
2024-12-14 10:07:28.851782: Epoch time: 539.15 s
2024-12-14 10:07:30.613823: 
2024-12-14 10:07:30.615162: Epoch 115
2024-12-14 10:07:30.616082: Current learning rate: 0.0027
2024-12-14 10:16:42.819185: Validation loss did not improve from -0.45602. Patience: 17/50
2024-12-14 10:16:42.820145: train_loss -0.8261
2024-12-14 10:16:42.820974: val_loss -0.3139
2024-12-14 10:16:42.821630: Pseudo dice [0.6576]
2024-12-14 10:16:42.822383: Epoch time: 552.21 s
2024-12-14 10:16:44.244193: 
2024-12-14 10:16:44.245264: Epoch 116
2024-12-14 10:16:44.245979: Current learning rate: 0.00263
2024-12-14 10:26:06.036947: Validation loss did not improve from -0.45602. Patience: 18/50
2024-12-14 10:26:06.038015: train_loss -0.8299
2024-12-14 10:26:06.038875: val_loss -0.3777
2024-12-14 10:26:06.039587: Pseudo dice [0.6861]
2024-12-14 10:26:06.040373: Epoch time: 561.8 s
2024-12-14 10:26:07.488993: 
2024-12-14 10:26:07.490498: Epoch 117
2024-12-14 10:26:07.491248: Current learning rate: 0.00256
2024-12-14 10:35:24.337842: Validation loss did not improve from -0.45602. Patience: 19/50
2024-12-14 10:35:24.340211: train_loss -0.8283
2024-12-14 10:35:24.341394: val_loss -0.3791
2024-12-14 10:35:24.342056: Pseudo dice [0.6971]
2024-12-14 10:35:24.343014: Epoch time: 556.85 s
2024-12-14 10:35:25.789591: 
2024-12-14 10:35:25.790692: Epoch 118
2024-12-14 10:35:25.791404: Current learning rate: 0.00249
2024-12-14 10:45:19.523658: Validation loss did not improve from -0.45602. Patience: 20/50
2024-12-14 10:45:19.524898: train_loss -0.826
2024-12-14 10:45:19.527387: val_loss -0.3896
2024-12-14 10:45:19.528301: Pseudo dice [0.6988]
2024-12-14 10:45:19.529126: Epoch time: 593.74 s
2024-12-14 10:45:20.966132: 
2024-12-14 10:45:20.967556: Epoch 119
2024-12-14 10:45:20.968322: Current learning rate: 0.00242
2024-12-14 10:54:18.295951: Validation loss did not improve from -0.45602. Patience: 21/50
2024-12-14 10:54:18.297184: train_loss -0.8288
2024-12-14 10:54:18.297881: val_loss -0.391
2024-12-14 10:54:18.298551: Pseudo dice [0.703]
2024-12-14 10:54:18.299248: Epoch time: 537.33 s
2024-12-14 10:54:20.183087: 
2024-12-14 10:54:20.184423: Epoch 120
2024-12-14 10:54:20.185118: Current learning rate: 0.00235
2024-12-14 11:03:56.959569: Validation loss did not improve from -0.45602. Patience: 22/50
2024-12-14 11:03:56.960517: train_loss -0.829
2024-12-14 11:03:56.961702: val_loss -0.3798
2024-12-14 11:03:56.962632: Pseudo dice [0.7013]
2024-12-14 11:03:56.963595: Epoch time: 576.78 s
2024-12-14 11:03:58.431016: 
2024-12-14 11:03:58.432340: Epoch 121
2024-12-14 11:03:58.433140: Current learning rate: 0.00228
2024-12-14 11:12:49.592574: Validation loss did not improve from -0.45602. Patience: 23/50
2024-12-14 11:12:49.593525: train_loss -0.8283
2024-12-14 11:12:49.594697: val_loss -0.3415
2024-12-14 11:12:49.595785: Pseudo dice [0.6822]
2024-12-14 11:12:49.596767: Epoch time: 531.16 s
2024-12-14 11:12:51.041524: 
2024-12-14 11:12:51.042823: Epoch 122
2024-12-14 11:12:51.043940: Current learning rate: 0.00221
2024-12-14 11:22:36.914175: Validation loss did not improve from -0.45602. Patience: 24/50
2024-12-14 11:22:36.915103: train_loss -0.8311
2024-12-14 11:22:36.915973: val_loss -0.3331
2024-12-14 11:22:36.916708: Pseudo dice [0.6804]
2024-12-14 11:22:36.917527: Epoch time: 585.87 s
2024-12-14 11:22:38.357520: 
2024-12-14 11:22:38.358799: Epoch 123
2024-12-14 11:22:38.359737: Current learning rate: 0.00214
2024-12-14 11:32:44.676478: Validation loss did not improve from -0.45602. Patience: 25/50
2024-12-14 11:32:44.677545: train_loss -0.83
2024-12-14 11:32:44.678393: val_loss -0.3794
2024-12-14 11:32:44.679200: Pseudo dice [0.6928]
2024-12-14 11:32:44.679876: Epoch time: 606.32 s
2024-12-14 11:32:47.122254: 
2024-12-14 11:32:47.123395: Epoch 124
2024-12-14 11:32:47.124134: Current learning rate: 0.00207
2024-12-14 11:42:30.762383: Validation loss did not improve from -0.45602. Patience: 26/50
2024-12-14 11:42:30.767963: train_loss -0.8317
2024-12-14 11:42:30.768986: val_loss -0.3873
2024-12-14 11:42:30.769649: Pseudo dice [0.6933]
2024-12-14 11:42:30.770538: Epoch time: 583.65 s
2024-12-14 11:42:32.610161: 
2024-12-14 11:42:32.611250: Epoch 125
2024-12-14 11:42:32.611996: Current learning rate: 0.00199
2024-12-14 11:51:54.662097: Validation loss did not improve from -0.45602. Patience: 27/50
2024-12-14 11:51:54.663047: train_loss -0.8298
2024-12-14 11:51:54.663851: val_loss -0.3719
2024-12-14 11:51:54.664561: Pseudo dice [0.6998]
2024-12-14 11:51:54.665446: Epoch time: 562.05 s
2024-12-14 11:51:56.174377: 
2024-12-14 11:51:56.175650: Epoch 126
2024-12-14 11:51:56.176470: Current learning rate: 0.00192
2024-12-14 12:00:44.428253: Validation loss did not improve from -0.45602. Patience: 28/50
2024-12-14 12:00:44.429291: train_loss -0.8347
2024-12-14 12:00:44.430193: val_loss -0.4004
2024-12-14 12:00:44.430992: Pseudo dice [0.7045]
2024-12-14 12:00:44.431710: Epoch time: 528.26 s
2024-12-14 12:00:45.948601: 
2024-12-14 12:00:45.949933: Epoch 127
2024-12-14 12:00:45.950784: Current learning rate: 0.00185
2024-12-14 12:10:15.358439: Validation loss did not improve from -0.45602. Patience: 29/50
2024-12-14 12:10:15.359420: train_loss -0.8307
2024-12-14 12:10:15.360203: val_loss -0.3611
2024-12-14 12:10:15.361045: Pseudo dice [0.6882]
2024-12-14 12:10:15.361877: Epoch time: 569.41 s
2024-12-14 12:10:16.822616: 
2024-12-14 12:10:16.823677: Epoch 128
2024-12-14 12:10:16.824443: Current learning rate: 0.00178
2024-12-14 12:20:15.772579: Validation loss did not improve from -0.45602. Patience: 30/50
2024-12-14 12:20:15.773637: train_loss -0.8341
2024-12-14 12:20:15.774318: val_loss -0.3877
2024-12-14 12:20:15.774943: Pseudo dice [0.7002]
2024-12-14 12:20:15.775583: Epoch time: 598.95 s
2024-12-14 12:20:17.210762: 
2024-12-14 12:20:17.212198: Epoch 129
2024-12-14 12:20:17.212967: Current learning rate: 0.0017
2024-12-14 12:29:41.436293: Validation loss did not improve from -0.45602. Patience: 31/50
2024-12-14 12:29:41.437097: train_loss -0.8334
2024-12-14 12:29:41.437854: val_loss -0.3552
2024-12-14 12:29:41.438535: Pseudo dice [0.6954]
2024-12-14 12:29:41.439276: Epoch time: 564.23 s
2024-12-14 12:29:43.396760: 
2024-12-14 12:29:43.398021: Epoch 130
2024-12-14 12:29:43.398810: Current learning rate: 0.00163
2024-12-14 12:38:42.101270: Validation loss did not improve from -0.45602. Patience: 32/50
2024-12-14 12:38:42.102311: train_loss -0.8363
2024-12-14 12:38:42.103030: val_loss -0.3743
2024-12-14 12:38:42.103703: Pseudo dice [0.6941]
2024-12-14 12:38:42.104461: Epoch time: 538.71 s
2024-12-14 12:38:43.548194: 
2024-12-14 12:38:43.549553: Epoch 131
2024-12-14 12:38:43.550310: Current learning rate: 0.00156
2024-12-14 12:47:57.507208: Validation loss did not improve from -0.45602. Patience: 33/50
2024-12-14 12:47:57.511688: train_loss -0.8339
2024-12-14 12:47:57.512940: val_loss -0.4238
2024-12-14 12:47:57.513915: Pseudo dice [0.7158]
2024-12-14 12:47:57.515075: Epoch time: 553.96 s
2024-12-14 12:47:58.940548: 
2024-12-14 12:47:58.941893: Epoch 132
2024-12-14 12:47:58.942768: Current learning rate: 0.00148
2024-12-14 12:57:05.995475: Validation loss did not improve from -0.45602. Patience: 34/50
2024-12-14 12:57:05.996923: train_loss -0.8346
2024-12-14 12:57:05.997837: val_loss -0.3835
2024-12-14 12:57:05.998571: Pseudo dice [0.6924]
2024-12-14 12:57:05.999218: Epoch time: 547.06 s
2024-12-14 12:57:07.452262: 
2024-12-14 12:57:07.453693: Epoch 133
2024-12-14 12:57:07.454508: Current learning rate: 0.00141
2024-12-14 13:05:50.225886: Validation loss did not improve from -0.45602. Patience: 35/50
2024-12-14 13:05:50.227224: train_loss -0.8363
2024-12-14 13:05:50.228161: val_loss -0.3635
2024-12-14 13:05:50.228913: Pseudo dice [0.6993]
2024-12-14 13:05:50.229814: Epoch time: 522.78 s
2024-12-14 13:05:51.691466: 
2024-12-14 13:05:51.692748: Epoch 134
2024-12-14 13:05:51.693667: Current learning rate: 0.00133
2024-12-14 13:14:00.845322: Validation loss did not improve from -0.45602. Patience: 36/50
2024-12-14 13:14:00.846410: train_loss -0.8356
2024-12-14 13:14:00.847269: val_loss -0.3571
2024-12-14 13:14:00.847965: Pseudo dice [0.6925]
2024-12-14 13:14:00.848750: Epoch time: 489.16 s
2024-12-14 13:14:03.181276: 
2024-12-14 13:14:03.182586: Epoch 135
2024-12-14 13:14:03.183331: Current learning rate: 0.00126
2024-12-14 13:22:32.952648: Validation loss did not improve from -0.45602. Patience: 37/50
2024-12-14 13:22:32.953654: train_loss -0.8352
2024-12-14 13:22:32.954472: val_loss -0.3837
2024-12-14 13:22:32.955188: Pseudo dice [0.6999]
2024-12-14 13:22:32.955950: Epoch time: 509.77 s
2024-12-14 13:22:34.412017: 
2024-12-14 13:22:34.413293: Epoch 136
2024-12-14 13:22:34.414010: Current learning rate: 0.00118
2024-12-14 13:31:11.725119: Validation loss did not improve from -0.45602. Patience: 38/50
2024-12-14 13:31:11.726114: train_loss -0.8367
2024-12-14 13:31:11.727021: val_loss -0.3857
2024-12-14 13:31:11.727667: Pseudo dice [0.7022]
2024-12-14 13:31:11.728315: Epoch time: 517.32 s
2024-12-14 13:31:13.189906: 
2024-12-14 13:31:13.190828: Epoch 137
2024-12-14 13:31:13.191489: Current learning rate: 0.00111
2024-12-14 13:39:39.089974: Validation loss did not improve from -0.45602. Patience: 39/50
2024-12-14 13:39:39.090976: train_loss -0.8372
2024-12-14 13:39:39.091722: val_loss -0.3823
2024-12-14 13:39:39.092398: Pseudo dice [0.7003]
2024-12-14 13:39:39.093085: Epoch time: 505.9 s
2024-12-14 13:39:40.550855: 
2024-12-14 13:39:40.552147: Epoch 138
2024-12-14 13:39:40.552827: Current learning rate: 0.00103
2024-12-14 13:48:11.204969: Validation loss did not improve from -0.45602. Patience: 40/50
2024-12-14 13:48:11.205879: train_loss -0.8378
2024-12-14 13:48:11.206698: val_loss -0.3237
2024-12-14 13:48:11.207522: Pseudo dice [0.6819]
2024-12-14 13:48:11.208372: Epoch time: 510.66 s
2024-12-14 13:48:12.654519: 
2024-12-14 13:48:12.655998: Epoch 139
2024-12-14 13:48:12.656774: Current learning rate: 0.00095
2024-12-14 13:56:50.314022: Validation loss did not improve from -0.45602. Patience: 41/50
2024-12-14 13:56:50.315972: train_loss -0.836
2024-12-14 13:56:50.316933: val_loss -0.4057
2024-12-14 13:56:50.317898: Pseudo dice [0.7069]
2024-12-14 13:56:50.318940: Epoch time: 517.66 s
2024-12-14 13:56:52.124304: 
2024-12-14 13:56:52.125715: Epoch 140
2024-12-14 13:56:52.126528: Current learning rate: 0.00087
2024-12-14 14:05:22.897296: Validation loss did not improve from -0.45602. Patience: 42/50
2024-12-14 14:05:22.898303: train_loss -0.8389
2024-12-14 14:05:22.899375: val_loss -0.3985
2024-12-14 14:05:22.900370: Pseudo dice [0.7045]
2024-12-14 14:05:22.901435: Epoch time: 510.77 s
2024-12-14 14:05:22.902426: Yayy! New best EMA pseudo Dice: 0.6976
2024-12-14 14:05:24.740390: 
2024-12-14 14:05:24.741996: Epoch 141
2024-12-14 14:05:24.743020: Current learning rate: 0.00079
2024-12-14 14:14:00.058069: Validation loss did not improve from -0.45602. Patience: 43/50
2024-12-14 14:14:00.058824: train_loss -0.8388
2024-12-14 14:14:00.059743: val_loss -0.3814
2024-12-14 14:14:00.060562: Pseudo dice [0.7022]
2024-12-14 14:14:00.061324: Epoch time: 515.32 s
2024-12-14 14:14:00.062116: Yayy! New best EMA pseudo Dice: 0.6981
2024-12-14 14:14:01.904205: 
2024-12-14 14:14:01.905528: Epoch 142
2024-12-14 14:14:01.906212: Current learning rate: 0.00071
2024-12-14 14:22:41.510276: Validation loss did not improve from -0.45602. Patience: 44/50
2024-12-14 14:22:41.511211: train_loss -0.839
2024-12-14 14:22:41.512086: val_loss -0.4128
2024-12-14 14:22:41.512912: Pseudo dice [0.7116]
2024-12-14 14:22:41.513794: Epoch time: 519.61 s
2024-12-14 14:22:41.514777: Yayy! New best EMA pseudo Dice: 0.6994
2024-12-14 14:22:43.386997: 
2024-12-14 14:22:43.388402: Epoch 143
2024-12-14 14:22:43.389305: Current learning rate: 0.00063
2024-12-14 14:31:58.255792: Validation loss did not improve from -0.45602. Patience: 45/50
2024-12-14 14:31:58.256453: train_loss -0.8389
2024-12-14 14:31:58.257489: val_loss -0.3749
2024-12-14 14:31:58.258471: Pseudo dice [0.6982]
2024-12-14 14:31:58.259427: Epoch time: 554.87 s
2024-12-14 14:31:59.709225: 
2024-12-14 14:31:59.710613: Epoch 144
2024-12-14 14:31:59.711645: Current learning rate: 0.00055
2024-12-14 14:41:16.859911: Validation loss did not improve from -0.45602. Patience: 46/50
2024-12-14 14:41:16.860963: train_loss -0.8396
2024-12-14 14:41:16.861824: val_loss -0.3992
2024-12-14 14:41:16.862532: Pseudo dice [0.7056]
2024-12-14 14:41:16.863324: Epoch time: 557.15 s
2024-12-14 14:41:17.304155: Yayy! New best EMA pseudo Dice: 0.6999
2024-12-14 14:41:19.166265: 
2024-12-14 14:41:19.167577: Epoch 145
2024-12-14 14:41:19.168435: Current learning rate: 0.00047
2024-12-14 14:50:46.482513: Validation loss did not improve from -0.45602. Patience: 47/50
2024-12-14 14:50:46.483185: train_loss -0.837
2024-12-14 14:50:46.483955: val_loss -0.3687
2024-12-14 14:50:46.484600: Pseudo dice [0.7011]
2024-12-14 14:50:46.485237: Epoch time: 567.32 s
2024-12-14 14:50:46.485926: Yayy! New best EMA pseudo Dice: 0.7001
2024-12-14 14:50:49.244007: 
2024-12-14 14:50:49.245180: Epoch 146
2024-12-14 14:50:49.245950: Current learning rate: 0.00038
2024-12-14 14:58:55.592474: Validation loss did not improve from -0.45602. Patience: 48/50
2024-12-14 14:58:55.601195: train_loss -0.8405
2024-12-14 14:58:55.603838: val_loss -0.3448
2024-12-14 14:58:55.604744: Pseudo dice [0.689]
2024-12-14 14:58:55.605906: Epoch time: 486.35 s
2024-12-14 14:58:57.410441: 
2024-12-14 14:58:57.411602: Epoch 147
2024-12-14 14:58:57.412379: Current learning rate: 0.0003
2024-12-14 15:06:30.321424: Validation loss did not improve from -0.45602. Patience: 49/50
2024-12-14 15:06:30.323023: train_loss -0.8409
2024-12-14 15:06:30.323842: val_loss -0.3923
2024-12-14 15:06:30.324514: Pseudo dice [0.7059]
2024-12-14 15:06:30.325207: Epoch time: 452.91 s
2024-12-14 15:06:31.803505: 
2024-12-14 15:06:31.804867: Epoch 148
2024-12-14 15:06:31.805534: Current learning rate: 0.00021
2024-12-14 15:14:17.196311: Validation loss did not improve from -0.45602. Patience: 50/50
2024-12-14 15:14:17.197102: train_loss -0.8389
2024-12-14 15:14:17.197771: val_loss -0.3557
2024-12-14 15:14:17.198439: Pseudo dice [0.6957]
2024-12-14 15:14:17.199104: Epoch time: 465.39 s
2024-12-14 15:14:18.714759: 
2024-12-14 15:14:18.716116: Epoch 149
2024-12-14 15:14:18.716814: Current learning rate: 0.00011
2024-12-14 15:21:32.124192: Validation loss did not improve from -0.45602. Patience: 51/50
2024-12-14 15:21:32.125176: train_loss -0.8395
2024-12-14 15:21:32.125938: val_loss -0.3652
2024-12-14 15:21:32.126593: Pseudo dice [0.7018]
2024-12-14 15:21:32.127344: Epoch time: 433.41 s
2024-12-14 15:21:33.991980: Training done.
2024-12-14 15:21:34.195388: Using splits from existing split file: /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset307_Sohee_Calcium_OCT_CrossValidation/splits_final_40.json
2024-12-14 15:21:34.217712: The split file contains 5 splits.
2024-12-14 15:21:34.218535: Desired fold for training: 1
2024-12-14 15:21:34.219270: This split has 3 training and 6 validation cases.
2024-12-14 15:21:34.220235: predicting 101-019
2024-12-14 15:21:34.325105: 101-019, shape torch.Size([1, 375, 498, 498]), rank 0
2024-12-14 15:23:35.100801: predicting 101-044
2024-12-14 15:23:35.139242: 101-044, shape torch.Size([1, 404, 498, 498]), rank 0
2024-12-14 15:25:46.315954: predicting 101-045
2024-12-14 15:25:46.330121: 101-045, shape torch.Size([1, 375, 498, 498]), rank 0
2024-12-14 15:27:39.383427: predicting 106-002
2024-12-14 15:27:39.399853: 106-002, shape torch.Size([1, 539, 498, 498]), rank 0
2024-12-14 15:30:33.073125: predicting 704-003
2024-12-14 15:30:33.086691: 704-003, shape torch.Size([1, 375, 498, 498]), rank 0
2024-12-14 15:32:24.511578: predicting 706-005
2024-12-14 15:32:24.548621: 706-005, shape torch.Size([1, 375, 498, 498]), rank 0
2024-12-14 15:34:37.301813: Validation complete
2024-12-14 15:34:37.302466: Mean Validation Dice:  0.6888401039727835
2024-12-13 18:19:08.007407: unpacking done...
2024-12-13 18:19:08.018121: Unable to plot network architecture: nnUNet_compile is enabled!
2024-12-13 18:19:08.054333: 
2024-12-13 18:19:08.055754: Epoch 0
2024-12-13 18:19:08.056818: Current learning rate: 0.01
2024-12-13 18:21:46.356600: Validation loss improved from 1000.00000 to -0.18306! Patience: 0/50
2024-12-13 18:21:46.357777: train_loss -0.0958
2024-12-13 18:21:46.358679: val_loss -0.1831
2024-12-13 18:21:46.359282: Pseudo dice [0.5356]
2024-12-13 18:21:46.359913: Epoch time: 158.3 s
2024-12-13 18:21:46.360455: Yayy! New best EMA pseudo Dice: 0.5356
2024-12-13 18:21:47.916334: 
2024-12-13 18:21:47.917530: Epoch 1
2024-12-13 18:21:47.918564: Current learning rate: 0.00994
2024-12-13 18:24:02.789447: Validation loss improved from -0.18306 to -0.22654! Patience: 0/50
2024-12-13 18:24:02.790362: train_loss -0.2512
2024-12-13 18:24:02.791238: val_loss -0.2265
2024-12-13 18:24:02.792016: Pseudo dice [0.5714]
2024-12-13 18:24:02.792850: Epoch time: 134.88 s
2024-12-13 18:24:02.794234: Yayy! New best EMA pseudo Dice: 0.5392
2024-12-13 18:24:04.580078: 
2024-12-13 18:24:04.581308: Epoch 2
2024-12-13 18:24:04.582150: Current learning rate: 0.00988
2024-12-13 18:27:15.709800: Validation loss improved from -0.22654 to -0.32621! Patience: 0/50
2024-12-13 18:27:15.710495: train_loss -0.3441
2024-12-13 18:27:15.711257: val_loss -0.3262
2024-12-13 18:27:15.712033: Pseudo dice [0.6201]
2024-12-13 18:27:15.712857: Epoch time: 191.13 s
2024-12-13 18:27:15.713675: Yayy! New best EMA pseudo Dice: 0.5473
2024-12-13 18:27:17.533373: 
2024-12-13 18:27:17.534834: Epoch 3
2024-12-13 18:27:17.535698: Current learning rate: 0.00982
2024-12-13 18:30:31.174576: Validation loss did not improve from -0.32621. Patience: 1/50
2024-12-13 18:30:31.175460: train_loss -0.3845
2024-12-13 18:30:31.176318: val_loss -0.3218
2024-12-13 18:30:31.177008: Pseudo dice [0.625]
2024-12-13 18:30:31.177778: Epoch time: 193.64 s
2024-12-13 18:30:31.178617: Yayy! New best EMA pseudo Dice: 0.5551
2024-12-13 18:30:32.955256: 
2024-12-13 18:30:32.956226: Epoch 4
2024-12-13 18:30:32.957035: Current learning rate: 0.00976
2024-12-13 18:34:17.690112: Validation loss improved from -0.32621 to -0.33151! Patience: 1/50
2024-12-13 18:34:17.691012: train_loss -0.4288
2024-12-13 18:34:17.691867: val_loss -0.3315
2024-12-13 18:34:17.692728: Pseudo dice [0.6445]
2024-12-13 18:34:17.693451: Epoch time: 224.74 s
2024-12-13 18:34:18.019382: Yayy! New best EMA pseudo Dice: 0.564
2024-12-13 18:34:19.838433: 
2024-12-13 18:34:19.839765: Epoch 5
2024-12-13 18:34:19.840603: Current learning rate: 0.0097
2024-12-13 18:37:56.824570: Validation loss improved from -0.33151 to -0.33490! Patience: 0/50
2024-12-13 18:37:56.825707: train_loss -0.4482
2024-12-13 18:37:56.826638: val_loss -0.3349
2024-12-13 18:37:56.827456: Pseudo dice [0.6356]
2024-12-13 18:37:56.828110: Epoch time: 216.99 s
2024-12-13 18:37:56.828802: Yayy! New best EMA pseudo Dice: 0.5712
2024-12-13 18:37:58.650506: 
2024-12-13 18:37:58.651789: Epoch 6
2024-12-13 18:37:58.652555: Current learning rate: 0.00964
2024-12-13 18:41:41.434705: Validation loss improved from -0.33490 to -0.33732! Patience: 0/50
2024-12-13 18:41:41.435792: train_loss -0.4616
2024-12-13 18:41:41.436506: val_loss -0.3373
2024-12-13 18:41:41.437175: Pseudo dice [0.6519]
2024-12-13 18:41:41.437875: Epoch time: 222.79 s
2024-12-13 18:41:41.438621: Yayy! New best EMA pseudo Dice: 0.5793
2024-12-13 18:41:43.236036: 
2024-12-13 18:41:43.237228: Epoch 7
2024-12-13 18:41:43.237962: Current learning rate: 0.00958
2024-12-13 18:46:11.005997: Validation loss did not improve from -0.33732. Patience: 1/50
2024-12-13 18:46:11.007059: train_loss -0.4934
2024-12-13 18:46:11.007823: val_loss -0.323
2024-12-13 18:46:11.008520: Pseudo dice [0.6359]
2024-12-13 18:46:11.009207: Epoch time: 267.77 s
2024-12-13 18:46:11.009911: Yayy! New best EMA pseudo Dice: 0.5849
2024-12-13 18:46:12.867857: 
2024-12-13 18:46:12.869038: Epoch 8
2024-12-13 18:46:12.869807: Current learning rate: 0.00952
2024-12-13 18:52:21.721365: Validation loss improved from -0.33732 to -0.39768! Patience: 1/50
2024-12-13 18:52:21.722450: train_loss -0.5121
2024-12-13 18:52:21.723197: val_loss -0.3977
2024-12-13 18:52:21.723908: Pseudo dice [0.6856]
2024-12-13 18:52:21.724663: Epoch time: 368.86 s
2024-12-13 18:52:21.725379: Yayy! New best EMA pseudo Dice: 0.595
2024-12-13 18:52:24.016731: 
2024-12-13 18:52:24.018140: Epoch 9
2024-12-13 18:52:24.018884: Current learning rate: 0.00946
2024-12-13 18:58:36.228767: Validation loss did not improve from -0.39768. Patience: 1/50
2024-12-13 18:58:36.229716: train_loss -0.5186
2024-12-13 18:58:36.230581: val_loss -0.3658
2024-12-13 18:58:36.231376: Pseudo dice [0.6597]
2024-12-13 18:58:36.232236: Epoch time: 372.21 s
2024-12-13 18:58:36.622023: Yayy! New best EMA pseudo Dice: 0.6015
2024-12-13 18:58:38.406590: 
2024-12-13 18:58:38.408048: Epoch 10
2024-12-13 18:58:38.408869: Current learning rate: 0.0094
2024-12-13 19:05:31.581814: Validation loss improved from -0.39768 to -0.42227! Patience: 1/50
2024-12-13 19:05:31.582867: train_loss -0.5348
2024-12-13 19:05:31.583703: val_loss -0.4223
2024-12-13 19:05:31.584463: Pseudo dice [0.6877]
2024-12-13 19:05:31.585117: Epoch time: 413.18 s
2024-12-13 19:05:31.585862: Yayy! New best EMA pseudo Dice: 0.6101
2024-12-13 19:05:33.386202: 
2024-12-13 19:05:33.387357: Epoch 11
2024-12-13 19:05:33.388021: Current learning rate: 0.00934
2024-12-13 19:12:02.816254: Validation loss did not improve from -0.42227. Patience: 1/50
2024-12-13 19:12:02.817276: train_loss -0.5524
2024-12-13 19:12:02.818096: val_loss -0.3521
2024-12-13 19:12:02.818841: Pseudo dice [0.6583]
2024-12-13 19:12:02.819513: Epoch time: 389.43 s
2024-12-13 19:12:02.820243: Yayy! New best EMA pseudo Dice: 0.6149
2024-12-13 19:12:04.628178: 
2024-12-13 19:12:04.629346: Epoch 12
2024-12-13 19:12:04.630117: Current learning rate: 0.00928
2024-12-13 19:18:51.732631: Validation loss did not improve from -0.42227. Patience: 2/50
2024-12-13 19:18:51.733565: train_loss -0.5665
2024-12-13 19:18:51.734447: val_loss -0.3592
2024-12-13 19:18:51.735144: Pseudo dice [0.6418]
2024-12-13 19:18:51.736004: Epoch time: 407.11 s
2024-12-13 19:18:51.736687: Yayy! New best EMA pseudo Dice: 0.6176
2024-12-13 19:18:53.567269: 
2024-12-13 19:18:53.568373: Epoch 13
2024-12-13 19:18:53.569101: Current learning rate: 0.00922
2024-12-13 19:25:44.739019: Validation loss did not improve from -0.42227. Patience: 3/50
2024-12-13 19:25:44.740304: train_loss -0.5794
2024-12-13 19:25:44.741098: val_loss -0.4126
2024-12-13 19:25:44.741781: Pseudo dice [0.6876]
2024-12-13 19:25:44.742577: Epoch time: 411.17 s
2024-12-13 19:25:44.743386: Yayy! New best EMA pseudo Dice: 0.6246
2024-12-13 19:25:46.625737: 
2024-12-13 19:25:46.626867: Epoch 14
2024-12-13 19:25:46.627614: Current learning rate: 0.00916
2024-12-13 19:32:21.712770: Validation loss improved from -0.42227 to -0.43095! Patience: 3/50
2024-12-13 19:32:21.713737: train_loss -0.6016
2024-12-13 19:32:21.714477: val_loss -0.431
2024-12-13 19:32:21.715191: Pseudo dice [0.6917]
2024-12-13 19:32:21.715932: Epoch time: 395.09 s
2024-12-13 19:32:22.089537: Yayy! New best EMA pseudo Dice: 0.6313
2024-12-13 19:32:23.932181: 
2024-12-13 19:32:23.933723: Epoch 15
2024-12-13 19:32:23.934769: Current learning rate: 0.0091
2024-12-13 19:39:43.873018: Validation loss did not improve from -0.43095. Patience: 1/50
2024-12-13 19:39:43.874056: train_loss -0.5987
2024-12-13 19:39:43.874780: val_loss -0.3887
2024-12-13 19:39:43.875430: Pseudo dice [0.6661]
2024-12-13 19:39:43.876084: Epoch time: 439.94 s
2024-12-13 19:39:43.876756: Yayy! New best EMA pseudo Dice: 0.6348
2024-12-13 19:39:45.686023: 
2024-12-13 19:39:45.686908: Epoch 16
2024-12-13 19:39:45.687829: Current learning rate: 0.00903
2024-12-13 19:46:32.049247: Validation loss did not improve from -0.43095. Patience: 2/50
2024-12-13 19:46:32.050271: train_loss -0.5988
2024-12-13 19:46:32.051177: val_loss -0.4204
2024-12-13 19:46:32.052148: Pseudo dice [0.6923]
2024-12-13 19:46:32.052984: Epoch time: 406.37 s
2024-12-13 19:46:32.053877: Yayy! New best EMA pseudo Dice: 0.6405
2024-12-13 19:46:33.993025: 
2024-12-13 19:46:33.994313: Epoch 17
2024-12-13 19:46:33.995233: Current learning rate: 0.00897
2024-12-13 19:53:39.438102: Validation loss did not improve from -0.43095. Patience: 3/50
2024-12-13 19:53:39.438998: train_loss -0.608
2024-12-13 19:53:39.439900: val_loss -0.4061
2024-12-13 19:53:39.440768: Pseudo dice [0.6778]
2024-12-13 19:53:39.441750: Epoch time: 425.45 s
2024-12-13 19:53:39.442623: Yayy! New best EMA pseudo Dice: 0.6443
2024-12-13 19:53:41.308723: 
2024-12-13 19:53:41.310105: Epoch 18
2024-12-13 19:53:41.310938: Current learning rate: 0.00891
2024-12-13 20:01:03.973230: Validation loss did not improve from -0.43095. Patience: 4/50
2024-12-13 20:01:03.974200: train_loss -0.6266
2024-12-13 20:01:03.974967: val_loss -0.3666
2024-12-13 20:01:03.975614: Pseudo dice [0.668]
2024-12-13 20:01:03.976284: Epoch time: 442.67 s
2024-12-13 20:01:03.976911: Yayy! New best EMA pseudo Dice: 0.6466
2024-12-13 20:01:06.173881: 
2024-12-13 20:01:06.175107: Epoch 19
2024-12-13 20:01:06.175810: Current learning rate: 0.00885
2024-12-13 20:08:15.588003: Validation loss did not improve from -0.43095. Patience: 5/50
2024-12-13 20:08:15.589066: train_loss -0.6315
2024-12-13 20:08:15.589880: val_loss -0.4223
2024-12-13 20:08:15.590622: Pseudo dice [0.6974]
2024-12-13 20:08:15.591384: Epoch time: 429.42 s
2024-12-13 20:08:16.015271: Yayy! New best EMA pseudo Dice: 0.6517
2024-12-13 20:08:17.856583: 
2024-12-13 20:08:17.857978: Epoch 20
2024-12-13 20:08:17.858761: Current learning rate: 0.00879
2024-12-13 20:15:23.301135: Validation loss did not improve from -0.43095. Patience: 6/50
2024-12-13 20:15:23.302140: train_loss -0.6331
2024-12-13 20:15:23.303169: val_loss -0.3915
2024-12-13 20:15:23.304025: Pseudo dice [0.6736]
2024-12-13 20:15:23.304719: Epoch time: 425.45 s
2024-12-13 20:15:23.305414: Yayy! New best EMA pseudo Dice: 0.6539
2024-12-13 20:15:25.210748: 
2024-12-13 20:15:25.211964: Epoch 21
2024-12-13 20:15:25.212750: Current learning rate: 0.00873
2024-12-13 20:22:40.790873: Validation loss improved from -0.43095 to -0.46988! Patience: 6/50
2024-12-13 20:22:40.791830: train_loss -0.6395
2024-12-13 20:22:40.792737: val_loss -0.4699
2024-12-13 20:22:40.793525: Pseudo dice [0.7153]
2024-12-13 20:22:40.794333: Epoch time: 435.58 s
2024-12-13 20:22:40.795143: Yayy! New best EMA pseudo Dice: 0.66
2024-12-13 20:22:42.540424: 
2024-12-13 20:22:42.541745: Epoch 22
2024-12-13 20:22:42.542551: Current learning rate: 0.00867
2024-12-13 20:29:41.113237: Validation loss did not improve from -0.46988. Patience: 1/50
2024-12-13 20:29:41.116978: train_loss -0.6443
2024-12-13 20:29:41.118268: val_loss -0.3368
2024-12-13 20:29:41.118930: Pseudo dice [0.6485]
2024-12-13 20:29:41.119605: Epoch time: 418.58 s
2024-12-13 20:29:42.559677: 
2024-12-13 20:29:42.560950: Epoch 23
2024-12-13 20:29:42.561815: Current learning rate: 0.00861
2024-12-13 20:36:51.329843: Validation loss did not improve from -0.46988. Patience: 2/50
2024-12-13 20:36:51.330920: train_loss -0.6427
2024-12-13 20:36:51.331893: val_loss -0.4078
2024-12-13 20:36:51.332683: Pseudo dice [0.685]
2024-12-13 20:36:51.333489: Epoch time: 428.77 s
2024-12-13 20:36:51.334232: Yayy! New best EMA pseudo Dice: 0.6615
2024-12-13 20:36:53.029558: 
2024-12-13 20:36:53.031028: Epoch 24
2024-12-13 20:36:53.031775: Current learning rate: 0.00855
2024-12-13 20:44:42.560556: Validation loss did not improve from -0.46988. Patience: 3/50
2024-12-13 20:44:42.561720: train_loss -0.6563
2024-12-13 20:44:42.562518: val_loss -0.4078
2024-12-13 20:44:42.563241: Pseudo dice [0.677]
2024-12-13 20:44:42.564036: Epoch time: 469.53 s
2024-12-13 20:44:42.903967: Yayy! New best EMA pseudo Dice: 0.663
2024-12-13 20:44:44.666116: 
2024-12-13 20:44:44.667577: Epoch 25
2024-12-13 20:44:44.668353: Current learning rate: 0.00849
2024-12-13 20:53:37.165492: Validation loss did not improve from -0.46988. Patience: 4/50
2024-12-13 20:53:37.166439: train_loss -0.6663
2024-12-13 20:53:37.167278: val_loss -0.3559
2024-12-13 20:53:37.168054: Pseudo dice [0.665]
2024-12-13 20:53:37.168741: Epoch time: 532.5 s
2024-12-13 20:53:37.169468: Yayy! New best EMA pseudo Dice: 0.6632
2024-12-13 20:53:39.075945: 
2024-12-13 20:53:39.077382: Epoch 26
2024-12-13 20:53:39.078101: Current learning rate: 0.00843
2024-12-13 21:02:52.089655: Validation loss did not improve from -0.46988. Patience: 5/50
2024-12-13 21:02:52.090815: train_loss -0.6558
2024-12-13 21:02:52.091601: val_loss -0.4385
2024-12-13 21:02:52.092499: Pseudo dice [0.7024]
2024-12-13 21:02:52.093444: Epoch time: 553.02 s
2024-12-13 21:02:52.094427: Yayy! New best EMA pseudo Dice: 0.6672
2024-12-13 21:02:53.882848: 
2024-12-13 21:02:53.884279: Epoch 27
2024-12-13 21:02:53.885307: Current learning rate: 0.00836
2024-12-13 21:12:13.333742: Validation loss did not improve from -0.46988. Patience: 6/50
2024-12-13 21:12:13.334806: train_loss -0.6715
2024-12-13 21:12:13.335684: val_loss -0.3974
2024-12-13 21:12:13.336330: Pseudo dice [0.6723]
2024-12-13 21:12:13.337119: Epoch time: 559.45 s
2024-12-13 21:12:13.337855: Yayy! New best EMA pseudo Dice: 0.6677
2024-12-13 21:12:15.139855: 
2024-12-13 21:12:15.141248: Epoch 28
2024-12-13 21:12:15.142097: Current learning rate: 0.0083
2024-12-13 21:21:26.433725: Validation loss did not improve from -0.46988. Patience: 7/50
2024-12-13 21:21:26.434728: train_loss -0.6775
2024-12-13 21:21:26.435738: val_loss -0.443
2024-12-13 21:21:26.436720: Pseudo dice [0.6942]
2024-12-13 21:21:26.437698: Epoch time: 551.3 s
2024-12-13 21:21:26.438648: Yayy! New best EMA pseudo Dice: 0.6703
2024-12-13 21:21:28.251062: 
2024-12-13 21:21:28.252505: Epoch 29
2024-12-13 21:21:28.253365: Current learning rate: 0.00824
2024-12-13 21:31:08.764671: Validation loss did not improve from -0.46988. Patience: 8/50
2024-12-13 21:31:08.765617: train_loss -0.6866
2024-12-13 21:31:08.767087: val_loss -0.4082
2024-12-13 21:31:08.767962: Pseudo dice [0.686]
2024-12-13 21:31:08.769012: Epoch time: 580.52 s
2024-12-13 21:31:09.531652: Yayy! New best EMA pseudo Dice: 0.6719
2024-12-13 21:31:11.393270: 
2024-12-13 21:31:11.394622: Epoch 30
2024-12-13 21:31:11.395616: Current learning rate: 0.00818
2024-12-13 21:40:15.335607: Validation loss did not improve from -0.46988. Patience: 9/50
2024-12-13 21:40:15.336829: train_loss -0.6865
2024-12-13 21:40:15.337543: val_loss -0.3325
2024-12-13 21:40:15.338242: Pseudo dice [0.645]
2024-12-13 21:40:15.338978: Epoch time: 543.94 s
2024-12-13 21:40:16.801693: 
2024-12-13 21:40:16.803165: Epoch 31
2024-12-13 21:40:16.803968: Current learning rate: 0.00812
2024-12-13 21:49:35.787110: Validation loss did not improve from -0.46988. Patience: 10/50
2024-12-13 21:49:35.788130: train_loss -0.6903
2024-12-13 21:49:35.788988: val_loss -0.4318
2024-12-13 21:49:35.789791: Pseudo dice [0.7104]
2024-12-13 21:49:35.790494: Epoch time: 558.99 s
2024-12-13 21:49:35.791215: Yayy! New best EMA pseudo Dice: 0.6733
2024-12-13 21:49:37.639021: 
2024-12-13 21:49:37.640370: Epoch 32
2024-12-13 21:49:37.641198: Current learning rate: 0.00806
2024-12-13 21:59:12.584986: Validation loss did not improve from -0.46988. Patience: 11/50
2024-12-13 21:59:12.586058: train_loss -0.6892
2024-12-13 21:59:12.587001: val_loss -0.3997
2024-12-13 21:59:12.587787: Pseudo dice [0.7032]
2024-12-13 21:59:12.588508: Epoch time: 574.95 s
2024-12-13 21:59:12.589283: Yayy! New best EMA pseudo Dice: 0.6763
2024-12-13 21:59:14.429475: 
2024-12-13 21:59:14.430821: Epoch 33
2024-12-13 21:59:14.431616: Current learning rate: 0.008
2024-12-13 22:08:58.097421: Validation loss did not improve from -0.46988. Patience: 12/50
2024-12-13 22:08:58.098352: train_loss -0.6915
2024-12-13 22:08:58.099151: val_loss -0.4248
2024-12-13 22:08:58.099824: Pseudo dice [0.6942]
2024-12-13 22:08:58.100507: Epoch time: 583.67 s
2024-12-13 22:08:58.101198: Yayy! New best EMA pseudo Dice: 0.6781
2024-12-13 22:09:00.017622: 
2024-12-13 22:09:00.018976: Epoch 34
2024-12-13 22:09:00.019750: Current learning rate: 0.00793
2024-12-13 22:18:26.938232: Validation loss did not improve from -0.46988. Patience: 13/50
2024-12-13 22:18:26.939056: train_loss -0.7028
2024-12-13 22:18:26.939757: val_loss -0.3863
2024-12-13 22:18:26.940441: Pseudo dice [0.6814]
2024-12-13 22:18:26.941345: Epoch time: 566.92 s
2024-12-13 22:18:27.327540: Yayy! New best EMA pseudo Dice: 0.6784
2024-12-13 22:18:29.192731: 
2024-12-13 22:18:29.194077: Epoch 35
2024-12-13 22:18:29.194817: Current learning rate: 0.00787
2024-12-13 22:27:51.950668: Validation loss did not improve from -0.46988. Patience: 14/50
2024-12-13 22:27:51.951550: train_loss -0.697
2024-12-13 22:27:51.952243: val_loss -0.4047
2024-12-13 22:27:51.952867: Pseudo dice [0.6942]
2024-12-13 22:27:51.953613: Epoch time: 562.76 s
2024-12-13 22:27:51.954341: Yayy! New best EMA pseudo Dice: 0.68
2024-12-13 22:27:53.855162: 
2024-12-13 22:27:53.856373: Epoch 36
2024-12-13 22:27:53.857190: Current learning rate: 0.00781
2024-12-13 22:37:16.654655: Validation loss did not improve from -0.46988. Patience: 15/50
2024-12-13 22:37:16.655654: train_loss -0.7043
2024-12-13 22:37:16.657596: val_loss -0.3761
2024-12-13 22:37:16.658615: Pseudo dice [0.6718]
2024-12-13 22:37:16.659506: Epoch time: 562.8 s
2024-12-13 22:37:18.092017: 
2024-12-13 22:37:18.093629: Epoch 37
2024-12-13 22:37:18.094559: Current learning rate: 0.00775
2024-12-13 22:47:01.724771: Validation loss did not improve from -0.46988. Patience: 16/50
2024-12-13 22:47:01.725950: train_loss -0.7059
2024-12-13 22:47:01.726917: val_loss -0.3864
2024-12-13 22:47:01.727608: Pseudo dice [0.6868]
2024-12-13 22:47:01.728367: Epoch time: 583.64 s
2024-12-13 22:47:03.191344: 
2024-12-13 22:47:03.192757: Epoch 38
2024-12-13 22:47:03.193792: Current learning rate: 0.00769
2024-12-13 22:56:20.949513: Validation loss did not improve from -0.46988. Patience: 17/50
2024-12-13 22:56:20.950519: train_loss -0.7129
2024-12-13 22:56:20.951490: val_loss -0.4106
2024-12-13 22:56:20.952222: Pseudo dice [0.7003]
2024-12-13 22:56:20.953021: Epoch time: 557.76 s
2024-12-13 22:56:20.953806: Yayy! New best EMA pseudo Dice: 0.682
2024-12-13 22:56:22.763145: 
2024-12-13 22:56:22.764499: Epoch 39
2024-12-13 22:56:22.765274: Current learning rate: 0.00763
2024-12-13 23:05:54.678550: Validation loss did not improve from -0.46988. Patience: 18/50
2024-12-13 23:05:54.679509: train_loss -0.7143
2024-12-13 23:05:54.680283: val_loss -0.3923
2024-12-13 23:05:54.680957: Pseudo dice [0.6798]
2024-12-13 23:05:54.681676: Epoch time: 571.92 s
2024-12-13 23:05:57.175744: 
2024-12-13 23:05:57.177222: Epoch 40
2024-12-13 23:05:57.177935: Current learning rate: 0.00756
2024-12-13 23:15:26.286877: Validation loss did not improve from -0.46988. Patience: 19/50
2024-12-13 23:15:26.287806: train_loss -0.7173
2024-12-13 23:15:26.288659: val_loss -0.4003
2024-12-13 23:15:26.289359: Pseudo dice [0.6847]
2024-12-13 23:15:26.290113: Epoch time: 569.11 s
2024-12-13 23:15:26.290865: Yayy! New best EMA pseudo Dice: 0.6821
2024-12-13 23:15:28.162052: 
2024-12-13 23:15:28.163303: Epoch 41
2024-12-13 23:15:28.164057: Current learning rate: 0.0075
2024-12-13 23:25:01.642804: Validation loss did not improve from -0.46988. Patience: 20/50
2024-12-13 23:25:01.643761: train_loss -0.7192
2024-12-13 23:25:01.644500: val_loss -0.3937
2024-12-13 23:25:01.645321: Pseudo dice [0.6885]
2024-12-13 23:25:01.646101: Epoch time: 573.48 s
2024-12-13 23:25:01.646924: Yayy! New best EMA pseudo Dice: 0.6827
2024-12-13 23:25:03.430858: 
2024-12-13 23:25:03.432324: Epoch 42
2024-12-13 23:25:03.433138: Current learning rate: 0.00744
2024-12-13 23:34:50.704788: Validation loss did not improve from -0.46988. Patience: 21/50
2024-12-13 23:34:50.706203: train_loss -0.7234
2024-12-13 23:34:50.707167: val_loss -0.4111
2024-12-13 23:34:50.707811: Pseudo dice [0.6917]
2024-12-13 23:34:50.708471: Epoch time: 587.28 s
2024-12-13 23:34:50.709125: Yayy! New best EMA pseudo Dice: 0.6836
2024-12-13 23:34:52.472622: 
2024-12-13 23:34:52.473909: Epoch 43
2024-12-13 23:34:52.474626: Current learning rate: 0.00738
2024-12-13 23:44:39.960517: Validation loss did not improve from -0.46988. Patience: 22/50
2024-12-13 23:44:39.964935: train_loss -0.7267
2024-12-13 23:44:39.966156: val_loss -0.4326
2024-12-13 23:44:39.966843: Pseudo dice [0.705]
2024-12-13 23:44:39.967685: Epoch time: 587.49 s
2024-12-13 23:44:39.968506: Yayy! New best EMA pseudo Dice: 0.6857
2024-12-13 23:44:41.742882: 
2024-12-13 23:44:41.744299: Epoch 44
2024-12-13 23:44:41.745076: Current learning rate: 0.00732
2024-12-13 23:54:21.705877: Validation loss did not improve from -0.46988. Patience: 23/50
2024-12-13 23:54:21.707108: train_loss -0.7286
2024-12-13 23:54:21.708009: val_loss -0.3418
2024-12-13 23:54:21.708771: Pseudo dice [0.6662]
2024-12-13 23:54:21.709577: Epoch time: 579.97 s
2024-12-13 23:54:23.480409: 
2024-12-13 23:54:23.481740: Epoch 45
2024-12-13 23:54:23.482705: Current learning rate: 0.00725
2024-12-14 00:04:08.061929: Validation loss did not improve from -0.46988. Patience: 24/50
2024-12-14 00:04:08.062736: train_loss -0.7285
2024-12-14 00:04:08.063504: val_loss -0.3377
2024-12-14 00:04:08.064218: Pseudo dice [0.6583]
2024-12-14 00:04:08.064956: Epoch time: 584.58 s
2024-12-14 00:04:09.412642: 
2024-12-14 00:04:09.414160: Epoch 46
2024-12-14 00:04:09.415000: Current learning rate: 0.00719
2024-12-14 00:13:58.106092: Validation loss did not improve from -0.46988. Patience: 25/50
2024-12-14 00:13:58.107126: train_loss -0.7285
2024-12-14 00:13:58.107911: val_loss -0.3892
2024-12-14 00:13:58.108650: Pseudo dice [0.6903]
2024-12-14 00:13:58.109414: Epoch time: 588.7 s
2024-12-14 00:13:59.512993: 
2024-12-14 00:13:59.514403: Epoch 47
2024-12-14 00:13:59.515212: Current learning rate: 0.00713
2024-12-14 00:23:31.228475: Validation loss did not improve from -0.46988. Patience: 26/50
2024-12-14 00:23:31.229491: train_loss -0.7299
2024-12-14 00:23:31.230386: val_loss -0.3593
2024-12-14 00:23:31.231221: Pseudo dice [0.6712]
2024-12-14 00:23:31.231955: Epoch time: 571.72 s
2024-12-14 00:23:32.613818: 
2024-12-14 00:23:32.615039: Epoch 48
2024-12-14 00:23:32.615784: Current learning rate: 0.00707
2024-12-14 00:33:34.953665: Validation loss did not improve from -0.46988. Patience: 27/50
2024-12-14 00:33:34.954563: train_loss -0.7313
2024-12-14 00:33:34.955307: val_loss -0.3752
2024-12-14 00:33:34.955976: Pseudo dice [0.6755]
2024-12-14 00:33:34.956842: Epoch time: 602.34 s
2024-12-14 00:33:36.417174: 
2024-12-14 00:33:36.418470: Epoch 49
2024-12-14 00:33:36.419204: Current learning rate: 0.007
2024-12-14 00:43:35.418662: Validation loss did not improve from -0.46988. Patience: 28/50
2024-12-14 00:43:35.419502: train_loss -0.7394
2024-12-14 00:43:35.420851: val_loss -0.3316
2024-12-14 00:43:35.421597: Pseudo dice [0.651]
2024-12-14 00:43:35.422480: Epoch time: 599.0 s
2024-12-14 00:43:37.574131: 
2024-12-14 00:43:37.575302: Epoch 50
2024-12-14 00:43:37.576075: Current learning rate: 0.00694
2024-12-14 00:53:19.556796: Validation loss did not improve from -0.46988. Patience: 29/50
2024-12-14 00:53:19.579622: train_loss -0.7315
2024-12-14 00:53:19.580863: val_loss -0.3619
2024-12-14 00:53:19.581571: Pseudo dice [0.676]
2024-12-14 00:53:19.582436: Epoch time: 582.01 s
2024-12-14 00:53:21.196953: 
2024-12-14 00:53:21.198118: Epoch 51
2024-12-14 00:53:21.198858: Current learning rate: 0.00688
2024-12-14 01:03:09.342244: Validation loss did not improve from -0.46988. Patience: 30/50
2024-12-14 01:03:09.343956: train_loss -0.744
2024-12-14 01:03:09.344954: val_loss -0.3345
2024-12-14 01:03:09.345832: Pseudo dice [0.6684]
2024-12-14 01:03:09.346558: Epoch time: 588.15 s
2024-12-14 01:03:10.745978: 
2024-12-14 01:03:10.747249: Epoch 52
2024-12-14 01:03:10.748151: Current learning rate: 0.00682
2024-12-14 01:12:36.835146: Validation loss did not improve from -0.46988. Patience: 31/50
2024-12-14 01:12:36.836153: train_loss -0.7449
2024-12-14 01:12:36.837003: val_loss -0.3323
2024-12-14 01:12:36.837823: Pseudo dice [0.655]
2024-12-14 01:12:36.838618: Epoch time: 566.09 s
2024-12-14 01:12:38.246035: 
2024-12-14 01:12:38.247403: Epoch 53
2024-12-14 01:12:38.248081: Current learning rate: 0.00675
2024-12-14 01:22:19.665173: Validation loss did not improve from -0.46988. Patience: 32/50
2024-12-14 01:22:19.666211: train_loss -0.7476
2024-12-14 01:22:19.667131: val_loss -0.4142
2024-12-14 01:22:19.667937: Pseudo dice [0.6993]
2024-12-14 01:22:19.668881: Epoch time: 581.42 s
2024-12-14 01:22:21.053145: 
2024-12-14 01:22:21.054435: Epoch 54
2024-12-14 01:22:21.055365: Current learning rate: 0.00669
2024-12-14 01:32:29.446493: Validation loss did not improve from -0.46988. Patience: 33/50
2024-12-14 01:32:29.447518: train_loss -0.7493
2024-12-14 01:32:29.448299: val_loss -0.3997
2024-12-14 01:32:29.449039: Pseudo dice [0.687]
2024-12-14 01:32:29.449680: Epoch time: 608.4 s
2024-12-14 01:32:31.309780: 
2024-12-14 01:32:31.311434: Epoch 55
2024-12-14 01:32:31.312247: Current learning rate: 0.00663
2024-12-14 01:42:20.648582: Validation loss did not improve from -0.46988. Patience: 34/50
2024-12-14 01:42:20.649544: train_loss -0.7497
2024-12-14 01:42:20.650267: val_loss -0.357
2024-12-14 01:42:20.650992: Pseudo dice [0.6683]
2024-12-14 01:42:20.651772: Epoch time: 589.34 s
2024-12-14 01:42:22.055301: 
2024-12-14 01:42:22.056426: Epoch 56
2024-12-14 01:42:22.057198: Current learning rate: 0.00657
2024-12-14 01:51:51.138967: Validation loss did not improve from -0.46988. Patience: 35/50
2024-12-14 01:51:51.140041: train_loss -0.7578
2024-12-14 01:51:51.142195: val_loss -0.3561
2024-12-14 01:51:51.143517: Pseudo dice [0.6831]
2024-12-14 01:51:51.144677: Epoch time: 569.09 s
2024-12-14 01:51:52.593276: 
2024-12-14 01:51:52.594764: Epoch 57
2024-12-14 01:51:52.595720: Current learning rate: 0.0065
2024-12-14 02:01:19.834153: Validation loss did not improve from -0.46988. Patience: 36/50
2024-12-14 02:01:19.837466: train_loss -0.7565
2024-12-14 02:01:19.839077: val_loss -0.3693
2024-12-14 02:01:19.840136: Pseudo dice [0.6769]
2024-12-14 02:01:19.841194: Epoch time: 567.25 s
2024-12-14 02:01:21.246682: 
2024-12-14 02:01:21.248137: Epoch 58
2024-12-14 02:01:21.249258: Current learning rate: 0.00644
2024-12-14 02:10:56.510190: Validation loss did not improve from -0.46988. Patience: 37/50
2024-12-14 02:10:56.511199: train_loss -0.7619
2024-12-14 02:10:56.512002: val_loss -0.3662
2024-12-14 02:10:56.512689: Pseudo dice [0.6877]
2024-12-14 02:10:56.513501: Epoch time: 575.27 s
2024-12-14 02:10:57.954784: 
2024-12-14 02:10:57.956126: Epoch 59
2024-12-14 02:10:57.956872: Current learning rate: 0.00638
2024-12-14 02:20:34.560447: Validation loss did not improve from -0.46988. Patience: 38/50
2024-12-14 02:20:34.561495: train_loss -0.755
2024-12-14 02:20:34.562386: val_loss -0.3733
2024-12-14 02:20:34.563094: Pseudo dice [0.6792]
2024-12-14 02:20:34.563938: Epoch time: 576.61 s
2024-12-14 02:20:36.315222: 
2024-12-14 02:20:36.316664: Epoch 60
2024-12-14 02:20:36.317718: Current learning rate: 0.00631
2024-12-14 02:29:47.661449: Validation loss did not improve from -0.46988. Patience: 39/50
2024-12-14 02:29:47.662429: train_loss -0.7559
2024-12-14 02:29:47.663573: val_loss -0.3477
2024-12-14 02:29:47.664442: Pseudo dice [0.6727]
2024-12-14 02:29:47.665394: Epoch time: 551.35 s
2024-12-14 02:29:49.609948: 
2024-12-14 02:29:49.610893: Epoch 61
2024-12-14 02:29:49.611726: Current learning rate: 0.00625
2024-12-14 02:39:51.246284: Validation loss did not improve from -0.46988. Patience: 40/50
2024-12-14 02:39:51.247091: train_loss -0.7609
2024-12-14 02:39:51.248057: val_loss -0.3801
2024-12-14 02:39:51.248898: Pseudo dice [0.6822]
2024-12-14 02:39:51.249706: Epoch time: 601.64 s
2024-12-14 02:39:52.663593: 
2024-12-14 02:39:52.665014: Epoch 62
2024-12-14 02:39:52.665973: Current learning rate: 0.00619
2024-12-14 02:49:57.123173: Validation loss did not improve from -0.46988. Patience: 41/50
2024-12-14 02:49:57.124184: train_loss -0.7636
2024-12-14 02:49:57.125062: val_loss -0.4058
2024-12-14 02:49:57.125834: Pseudo dice [0.6998]
2024-12-14 02:49:57.126677: Epoch time: 604.46 s
2024-12-14 02:49:58.548651: 
2024-12-14 02:49:58.549988: Epoch 63
2024-12-14 02:49:58.550814: Current learning rate: 0.00612
2024-12-14 02:59:19.575223: Validation loss did not improve from -0.46988. Patience: 42/50
2024-12-14 02:59:19.577574: train_loss -0.7676
2024-12-14 02:59:19.579271: val_loss -0.4008
2024-12-14 02:59:19.580304: Pseudo dice [0.6977]
2024-12-14 02:59:19.581420: Epoch time: 561.03 s
2024-12-14 02:59:21.040213: 
2024-12-14 02:59:21.041566: Epoch 64
2024-12-14 02:59:21.042559: Current learning rate: 0.00606
2024-12-14 03:09:34.380226: Validation loss did not improve from -0.46988. Patience: 43/50
2024-12-14 03:09:34.381368: train_loss -0.7711
2024-12-14 03:09:34.382334: val_loss -0.3381
2024-12-14 03:09:34.383196: Pseudo dice [0.6615]
2024-12-14 03:09:34.384112: Epoch time: 613.34 s
2024-12-14 03:09:36.180780: 
2024-12-14 03:09:36.182111: Epoch 65
2024-12-14 03:09:36.182927: Current learning rate: 0.006
2024-12-14 03:19:32.149308: Validation loss did not improve from -0.46988. Patience: 44/50
2024-12-14 03:19:32.150207: train_loss -0.7652
2024-12-14 03:19:32.151111: val_loss -0.4167
2024-12-14 03:19:32.151949: Pseudo dice [0.7065]
2024-12-14 03:19:32.152794: Epoch time: 595.97 s
2024-12-14 03:19:33.579923: 
2024-12-14 03:19:33.581264: Epoch 66
2024-12-14 03:19:33.582089: Current learning rate: 0.00593
2024-12-14 03:29:26.033193: Validation loss did not improve from -0.46988. Patience: 45/50
2024-12-14 03:29:26.034173: train_loss -0.7648
2024-12-14 03:29:26.035128: val_loss -0.3753
2024-12-14 03:29:26.035965: Pseudo dice [0.6803]
2024-12-14 03:29:26.036970: Epoch time: 592.46 s
2024-12-14 03:29:27.497467: 
2024-12-14 03:29:27.499084: Epoch 67
2024-12-14 03:29:27.500043: Current learning rate: 0.00587
2024-12-14 03:39:04.770824: Validation loss did not improve from -0.46988. Patience: 46/50
2024-12-14 03:39:04.771862: train_loss -0.7694
2024-12-14 03:39:04.772709: val_loss -0.3896
2024-12-14 03:39:04.773437: Pseudo dice [0.7035]
2024-12-14 03:39:04.774189: Epoch time: 577.28 s
2024-12-14 03:39:06.211838: 
2024-12-14 03:39:06.213083: Epoch 68
2024-12-14 03:39:06.213883: Current learning rate: 0.00581
2024-12-14 03:48:52.323392: Validation loss did not improve from -0.46988. Patience: 47/50
2024-12-14 03:48:52.324470: train_loss -0.7675
2024-12-14 03:48:52.325315: val_loss -0.3715
2024-12-14 03:48:52.326049: Pseudo dice [0.6888]
2024-12-14 03:48:52.326854: Epoch time: 586.11 s
2024-12-14 03:48:53.759985: 
2024-12-14 03:48:53.761222: Epoch 69
2024-12-14 03:48:53.762255: Current learning rate: 0.00574
2024-12-14 03:58:58.001318: Validation loss did not improve from -0.46988. Patience: 48/50
2024-12-14 03:58:58.002010: train_loss -0.7739
2024-12-14 03:58:58.003510: val_loss -0.3739
2024-12-14 03:58:58.004255: Pseudo dice [0.6846]
2024-12-14 03:58:58.005118: Epoch time: 604.24 s
2024-12-14 03:58:59.837635: 
2024-12-14 03:58:59.839033: Epoch 70
2024-12-14 03:58:59.839744: Current learning rate: 0.00568
2024-12-14 04:08:51.626346: Validation loss did not improve from -0.46988. Patience: 49/50
2024-12-14 04:08:51.629535: train_loss -0.7731
2024-12-14 04:08:51.630519: val_loss -0.347
2024-12-14 04:08:51.631183: Pseudo dice [0.6639]
2024-12-14 04:08:51.631959: Epoch time: 591.79 s
2024-12-14 04:08:53.127043: 
2024-12-14 04:08:53.128131: Epoch 71
2024-12-14 04:08:53.128901: Current learning rate: 0.00562
2024-12-14 04:18:42.028406: Validation loss did not improve from -0.46988. Patience: 50/50
2024-12-14 04:18:42.029921: train_loss -0.7717
2024-12-14 04:18:42.030946: val_loss -0.3738
2024-12-14 04:18:42.031587: Pseudo dice [0.687]
2024-12-14 04:18:42.032259: Epoch time: 588.9 s
2024-12-14 04:18:44.331534: 
2024-12-14 04:18:44.332673: Epoch 72
2024-12-14 04:18:44.333614: Current learning rate: 0.00555
2024-12-14 04:28:27.358348: Validation loss did not improve from -0.46988. Patience: 51/50
2024-12-14 04:28:27.359382: train_loss -0.7754
2024-12-14 04:28:27.360116: val_loss -0.3929
2024-12-14 04:28:27.360878: Pseudo dice [0.6993]
2024-12-14 04:28:27.361814: Epoch time: 583.03 s
2024-12-14 04:28:28.825592: 
2024-12-14 04:28:28.827346: Epoch 73
2024-12-14 04:28:28.828168: Current learning rate: 0.00549
2024-12-14 04:38:30.763995: Validation loss did not improve from -0.46988. Patience: 52/50
2024-12-14 04:38:30.764915: train_loss -0.7783
2024-12-14 04:38:30.765748: val_loss -0.3218
2024-12-14 04:38:30.766446: Pseudo dice [0.655]
2024-12-14 04:38:30.767242: Epoch time: 601.94 s
2024-12-14 04:38:32.208461: 
2024-12-14 04:38:32.209942: Epoch 74
2024-12-14 04:38:32.210748: Current learning rate: 0.00542
2024-12-14 04:48:22.555748: Validation loss did not improve from -0.46988. Patience: 53/50
2024-12-14 04:48:22.556870: train_loss -0.7804
2024-12-14 04:48:22.557892: val_loss -0.3905
2024-12-14 04:48:22.558705: Pseudo dice [0.6814]
2024-12-14 04:48:22.559541: Epoch time: 590.35 s
2024-12-14 04:48:24.417389: 
2024-12-14 04:48:24.418786: Epoch 75
2024-12-14 04:48:24.419738: Current learning rate: 0.00536
2024-12-14 04:58:38.340796: Validation loss did not improve from -0.46988. Patience: 54/50
2024-12-14 04:58:38.341756: train_loss -0.7816
2024-12-14 04:58:38.342600: val_loss -0.2934
2024-12-14 04:58:38.343470: Pseudo dice [0.6534]
2024-12-14 04:58:38.344257: Epoch time: 613.93 s
2024-12-14 04:58:39.804997: 
2024-12-14 04:58:39.806405: Epoch 76
2024-12-14 04:58:39.807242: Current learning rate: 0.00529
2024-12-14 05:08:40.108124: Validation loss did not improve from -0.46988. Patience: 55/50
2024-12-14 05:08:40.109091: train_loss -0.7783
2024-12-14 05:08:40.110003: val_loss -0.4378
2024-12-14 05:08:40.110807: Pseudo dice [0.7008]
2024-12-14 05:08:40.111632: Epoch time: 600.31 s
2024-12-14 05:08:41.573265: 
2024-12-14 05:08:41.574591: Epoch 77
2024-12-14 05:08:41.575337: Current learning rate: 0.00523
2024-12-14 05:18:22.090089: Validation loss did not improve from -0.46988. Patience: 56/50
2024-12-14 05:18:22.091244: train_loss -0.7809
2024-12-14 05:18:22.092245: val_loss -0.337
2024-12-14 05:18:22.093354: Pseudo dice [0.6629]
2024-12-14 05:18:22.094227: Epoch time: 580.52 s
2024-12-14 05:18:23.553230: 
2024-12-14 05:18:23.554799: Epoch 78
2024-12-14 05:18:23.555747: Current learning rate: 0.00517
2024-12-14 05:27:52.918460: Validation loss did not improve from -0.46988. Patience: 57/50
2024-12-14 05:27:52.919482: train_loss -0.7768
2024-12-14 05:27:52.920539: val_loss -0.4093
2024-12-14 05:27:52.921648: Pseudo dice [0.7106]
2024-12-14 05:27:52.922494: Epoch time: 569.37 s
2024-12-14 05:27:54.406944: 
2024-12-14 05:27:54.408420: Epoch 79
2024-12-14 05:27:54.409336: Current learning rate: 0.0051
2024-12-14 05:37:55.323016: Validation loss did not improve from -0.46988. Patience: 58/50
2024-12-14 05:37:55.323926: train_loss -0.7808
2024-12-14 05:37:55.324951: val_loss -0.4124
2024-12-14 05:37:55.325808: Pseudo dice [0.6981]
2024-12-14 05:37:55.326848: Epoch time: 600.92 s
2024-12-14 05:37:57.117026: 
2024-12-14 05:37:57.118436: Epoch 80
2024-12-14 05:37:57.119501: Current learning rate: 0.00504
2024-12-14 05:47:48.821939: Validation loss did not improve from -0.46988. Patience: 59/50
2024-12-14 05:47:48.822770: train_loss -0.7836
2024-12-14 05:47:48.823516: val_loss -0.3774
2024-12-14 05:47:48.824236: Pseudo dice [0.6816]
2024-12-14 05:47:48.824894: Epoch time: 591.71 s
2024-12-14 05:47:50.288256: 
2024-12-14 05:47:50.289596: Epoch 81
2024-12-14 05:47:50.290408: Current learning rate: 0.00497
2024-12-14 05:58:10.705965: Validation loss did not improve from -0.46988. Patience: 60/50
2024-12-14 05:58:10.706916: train_loss -0.7874
2024-12-14 05:58:10.708000: val_loss -0.39
2024-12-14 05:58:10.708971: Pseudo dice [0.6946]
2024-12-14 05:58:10.709855: Epoch time: 620.42 s
2024-12-14 05:58:12.196133: 
2024-12-14 05:58:12.197465: Epoch 82
2024-12-14 05:58:12.198522: Current learning rate: 0.00491
2024-12-14 06:08:20.394365: Validation loss did not improve from -0.46988. Patience: 61/50
2024-12-14 06:08:20.395249: train_loss -0.789
2024-12-14 06:08:20.395946: val_loss -0.4335
2024-12-14 06:08:20.396616: Pseudo dice [0.7127]
2024-12-14 06:08:20.397254: Epoch time: 608.2 s
2024-12-14 06:08:20.398008: Yayy! New best EMA pseudo Dice: 0.6877
2024-12-14 06:08:22.592245: 
2024-12-14 06:08:22.593396: Epoch 83
2024-12-14 06:08:22.594134: Current learning rate: 0.00484
2024-12-14 06:18:18.940961: Validation loss did not improve from -0.46988. Patience: 62/50
2024-12-14 06:18:18.944082: train_loss -0.7879
2024-12-14 06:18:18.945099: val_loss -0.3911
2024-12-14 06:18:18.945767: Pseudo dice [0.6987]
2024-12-14 06:18:18.946681: Epoch time: 596.35 s
2024-12-14 06:18:18.947605: Yayy! New best EMA pseudo Dice: 0.6888
2024-12-14 06:18:20.686544: 
2024-12-14 06:18:20.688030: Epoch 84
2024-12-14 06:18:20.688936: Current learning rate: 0.00478
2024-12-14 06:27:59.828018: Validation loss did not improve from -0.46988. Patience: 63/50
2024-12-14 06:27:59.829182: train_loss -0.7888
2024-12-14 06:27:59.830054: val_loss -0.3557
2024-12-14 06:27:59.830911: Pseudo dice [0.676]
2024-12-14 06:27:59.832010: Epoch time: 579.14 s
2024-12-14 06:28:01.539023: 
2024-12-14 06:28:01.540558: Epoch 85
2024-12-14 06:28:01.541786: Current learning rate: 0.00471
2024-12-14 06:37:54.291690: Validation loss did not improve from -0.46988. Patience: 64/50
2024-12-14 06:37:54.292772: train_loss -0.7901
2024-12-14 06:37:54.293810: val_loss -0.379
2024-12-14 06:37:54.294810: Pseudo dice [0.6876]
2024-12-14 06:37:54.295824: Epoch time: 592.75 s
2024-12-14 06:37:55.695467: 
2024-12-14 06:37:55.696942: Epoch 86
2024-12-14 06:37:55.697991: Current learning rate: 0.00465
2024-12-14 06:47:58.412996: Validation loss did not improve from -0.46988. Patience: 65/50
2024-12-14 06:47:58.413965: train_loss -0.7872
2024-12-14 06:47:58.414696: val_loss -0.3922
2024-12-14 06:47:58.415360: Pseudo dice [0.6876]
2024-12-14 06:47:58.416145: Epoch time: 602.72 s
2024-12-14 06:47:59.789528: 
2024-12-14 06:47:59.790387: Epoch 87
2024-12-14 06:47:59.791050: Current learning rate: 0.00458
2024-12-14 06:57:43.851433: Validation loss did not improve from -0.46988. Patience: 66/50
2024-12-14 06:57:43.852408: train_loss -0.7934
2024-12-14 06:57:43.853567: val_loss -0.3428
2024-12-14 06:57:43.854386: Pseudo dice [0.687]
2024-12-14 06:57:43.855360: Epoch time: 584.06 s
2024-12-14 06:57:45.221888: 
2024-12-14 06:57:45.223262: Epoch 88
2024-12-14 06:57:45.224115: Current learning rate: 0.00452
2024-12-14 07:07:27.243319: Validation loss did not improve from -0.46988. Patience: 67/50
2024-12-14 07:07:27.244435: train_loss -0.7955
2024-12-14 07:07:27.245258: val_loss -0.3235
2024-12-14 07:07:27.246163: Pseudo dice [0.6789]
2024-12-14 07:07:27.247035: Epoch time: 582.02 s
2024-12-14 07:07:28.627303: 
2024-12-14 07:07:28.628774: Epoch 89
2024-12-14 07:07:28.629662: Current learning rate: 0.00445
2024-12-14 07:17:25.813059: Validation loss did not improve from -0.46988. Patience: 68/50
2024-12-14 07:17:25.814239: train_loss -0.7947
2024-12-14 07:17:25.816186: val_loss -0.37
2024-12-14 07:17:25.817025: Pseudo dice [0.6871]
2024-12-14 07:17:25.817971: Epoch time: 597.19 s
2024-12-14 07:17:27.640030: 
2024-12-14 07:17:27.641212: Epoch 90
2024-12-14 07:17:27.641945: Current learning rate: 0.00438
2024-12-14 07:27:21.308172: Validation loss did not improve from -0.46988. Patience: 69/50
2024-12-14 07:27:21.313880: train_loss -0.7893
2024-12-14 07:27:21.316322: val_loss -0.3485
2024-12-14 07:27:21.317317: Pseudo dice [0.6799]
2024-12-14 07:27:21.318720: Epoch time: 593.67 s
2024-12-14 07:27:22.736410: 
2024-12-14 07:27:22.737702: Epoch 91
2024-12-14 07:27:22.738483: Current learning rate: 0.00432
2024-12-14 07:36:59.253768: Validation loss did not improve from -0.46988. Patience: 70/50
2024-12-14 07:36:59.255346: train_loss -0.792
2024-12-14 07:36:59.256262: val_loss -0.2931
2024-12-14 07:36:59.256915: Pseudo dice [0.6632]
2024-12-14 07:36:59.257574: Epoch time: 576.52 s
2024-12-14 07:37:00.634648: 
2024-12-14 07:37:00.635829: Epoch 92
2024-12-14 07:37:00.636493: Current learning rate: 0.00425
2024-12-14 07:46:49.757918: Validation loss did not improve from -0.46988. Patience: 71/50
2024-12-14 07:46:49.759032: train_loss -0.7925
2024-12-14 07:46:49.760041: val_loss -0.2599
2024-12-14 07:46:49.760927: Pseudo dice [0.6332]
2024-12-14 07:46:49.761743: Epoch time: 589.13 s
2024-12-14 07:46:51.149721: 
2024-12-14 07:46:51.151276: Epoch 93
2024-12-14 07:46:51.152194: Current learning rate: 0.00419
2024-12-14 07:56:59.909796: Validation loss did not improve from -0.46988. Patience: 72/50
2024-12-14 07:56:59.910842: train_loss -0.7978
2024-12-14 07:56:59.911813: val_loss -0.3672
2024-12-14 07:56:59.912677: Pseudo dice [0.6773]
2024-12-14 07:56:59.913718: Epoch time: 608.76 s
2024-12-14 07:57:01.770555: 
2024-12-14 07:57:01.771721: Epoch 94
2024-12-14 07:57:01.772524: Current learning rate: 0.00412
2024-12-14 08:06:48.842340: Validation loss did not improve from -0.46988. Patience: 73/50
2024-12-14 08:06:48.843395: train_loss -0.7975
2024-12-14 08:06:48.844404: val_loss -0.3793
2024-12-14 08:06:48.845211: Pseudo dice [0.6848]
2024-12-14 08:06:48.846101: Epoch time: 587.07 s
2024-12-14 08:06:50.599822: 
2024-12-14 08:06:50.601336: Epoch 95
2024-12-14 08:06:50.602257: Current learning rate: 0.00405
2024-12-14 08:16:36.338731: Validation loss did not improve from -0.46988. Patience: 74/50
2024-12-14 08:16:36.339690: train_loss -0.7987
2024-12-14 08:16:36.340540: val_loss -0.4086
2024-12-14 08:16:36.341315: Pseudo dice [0.7069]
2024-12-14 08:16:36.342018: Epoch time: 585.74 s
2024-12-14 08:16:37.719143: 
2024-12-14 08:16:37.720453: Epoch 96
2024-12-14 08:16:37.721229: Current learning rate: 0.00399
2024-12-14 08:26:50.089244: Validation loss did not improve from -0.46988. Patience: 75/50
2024-12-14 08:26:50.092988: train_loss -0.7956
2024-12-14 08:26:50.094201: val_loss -0.3845
2024-12-14 08:26:50.094848: Pseudo dice [0.6952]
2024-12-14 08:26:50.095699: Epoch time: 612.37 s
2024-12-14 08:26:51.506522: 
2024-12-14 08:26:51.507694: Epoch 97
2024-12-14 08:26:51.508499: Current learning rate: 0.00392
2024-12-14 08:36:52.217044: Validation loss did not improve from -0.46988. Patience: 76/50
2024-12-14 08:36:52.218463: train_loss -0.7998
2024-12-14 08:36:52.219772: val_loss -0.3149
2024-12-14 08:36:52.220538: Pseudo dice [0.6561]
2024-12-14 08:36:52.221428: Epoch time: 600.71 s
2024-12-14 08:36:53.657393: 
2024-12-14 08:36:53.658689: Epoch 98
2024-12-14 08:36:53.659592: Current learning rate: 0.00385
2024-12-14 08:47:01.267550: Validation loss did not improve from -0.46988. Patience: 77/50
2024-12-14 08:47:01.268579: train_loss -0.7988
2024-12-14 08:47:01.269534: val_loss -0.3517
2024-12-14 08:47:01.270336: Pseudo dice [0.6691]
2024-12-14 08:47:01.271221: Epoch time: 607.61 s
2024-12-14 08:47:02.698135: 
2024-12-14 08:47:02.699732: Epoch 99
2024-12-14 08:47:02.700713: Current learning rate: 0.00379
2024-12-14 08:57:10.448090: Validation loss did not improve from -0.46988. Patience: 78/50
2024-12-14 08:57:10.449140: train_loss -0.8014
2024-12-14 08:57:10.450383: val_loss -0.3316
2024-12-14 08:57:10.451432: Pseudo dice [0.6665]
2024-12-14 08:57:10.452437: Epoch time: 607.75 s
2024-12-14 08:57:12.207932: 
2024-12-14 08:57:12.209301: Epoch 100
2024-12-14 08:57:12.210186: Current learning rate: 0.00372
2024-12-14 09:06:58.789424: Validation loss did not improve from -0.46988. Patience: 79/50
2024-12-14 09:06:58.790545: train_loss -0.804
2024-12-14 09:06:58.791460: val_loss -0.4097
2024-12-14 09:06:58.792276: Pseudo dice [0.7107]
2024-12-14 09:06:58.793130: Epoch time: 586.58 s
2024-12-14 09:07:00.216064: 
2024-12-14 09:07:00.217314: Epoch 101
2024-12-14 09:07:00.218240: Current learning rate: 0.00365
2024-12-14 09:16:49.965287: Validation loss did not improve from -0.46988. Patience: 80/50
2024-12-14 09:16:49.966347: train_loss -0.8034
2024-12-14 09:16:49.967110: val_loss -0.3704
2024-12-14 09:16:49.967808: Pseudo dice [0.6934]
2024-12-14 09:16:49.968467: Epoch time: 589.75 s
2024-12-14 09:16:51.389826: 
2024-12-14 09:16:51.391025: Epoch 102
2024-12-14 09:16:51.391701: Current learning rate: 0.00359
2024-12-14 09:27:02.984320: Validation loss did not improve from -0.46988. Patience: 81/50
2024-12-14 09:27:02.985382: train_loss -0.8051
2024-12-14 09:27:02.986142: val_loss -0.3228
2024-12-14 09:27:02.986814: Pseudo dice [0.6645]
2024-12-14 09:27:02.987492: Epoch time: 611.6 s
2024-12-14 09:27:04.447956: 
2024-12-14 09:27:04.449402: Epoch 103
2024-12-14 09:27:04.450269: Current learning rate: 0.00352
2024-12-14 09:37:07.251645: Validation loss did not improve from -0.46988. Patience: 82/50
2024-12-14 09:37:07.255449: train_loss -0.8033
2024-12-14 09:37:07.257022: val_loss -0.3728
2024-12-14 09:37:07.257876: Pseudo dice [0.6835]
2024-12-14 09:37:07.258850: Epoch time: 602.81 s
2024-12-14 09:37:08.697806: 
2024-12-14 09:37:08.699174: Epoch 104
2024-12-14 09:37:08.699976: Current learning rate: 0.00345
2024-12-14 09:47:28.674209: Validation loss did not improve from -0.46988. Patience: 83/50
2024-12-14 09:47:28.676098: train_loss -0.8038
2024-12-14 09:47:28.676857: val_loss -0.3583
2024-12-14 09:47:28.677453: Pseudo dice [0.6772]
2024-12-14 09:47:28.678187: Epoch time: 619.98 s
2024-12-14 09:47:30.977215: 
2024-12-14 09:47:30.978581: Epoch 105
2024-12-14 09:47:30.979324: Current learning rate: 0.00338
2024-12-14 09:57:41.685449: Validation loss did not improve from -0.46988. Patience: 84/50
2024-12-14 09:57:41.686368: train_loss -0.8011
2024-12-14 09:57:41.687185: val_loss -0.3868
2024-12-14 09:57:41.688075: Pseudo dice [0.7003]
2024-12-14 09:57:41.688755: Epoch time: 610.71 s
2024-12-14 09:57:43.092702: 
2024-12-14 09:57:43.093890: Epoch 106
2024-12-14 09:57:43.094702: Current learning rate: 0.00332
2024-12-14 10:07:34.063877: Validation loss did not improve from -0.46988. Patience: 85/50
2024-12-14 10:07:34.066074: train_loss -0.8048
2024-12-14 10:07:34.067290: val_loss -0.3455
2024-12-14 10:07:34.068341: Pseudo dice [0.6828]
2024-12-14 10:07:34.069411: Epoch time: 590.97 s
2024-12-14 10:07:35.510506: 
2024-12-14 10:07:35.512002: Epoch 107
2024-12-14 10:07:35.512935: Current learning rate: 0.00325
2024-12-14 10:17:27.701547: Validation loss did not improve from -0.46988. Patience: 86/50
2024-12-14 10:17:27.702588: train_loss -0.8062
2024-12-14 10:17:27.703450: val_loss -0.3665
2024-12-14 10:17:27.704288: Pseudo dice [0.6886]
2024-12-14 10:17:27.705104: Epoch time: 592.19 s
2024-12-14 10:17:29.119567: 
2024-12-14 10:17:29.121317: Epoch 108
2024-12-14 10:17:29.122492: Current learning rate: 0.00318
2024-12-14 10:27:33.941701: Validation loss did not improve from -0.46988. Patience: 87/50
2024-12-14 10:27:33.942593: train_loss -0.8104
2024-12-14 10:27:33.943410: val_loss -0.3258
2024-12-14 10:27:33.944087: Pseudo dice [0.666]
2024-12-14 10:27:33.944901: Epoch time: 604.82 s
2024-12-14 10:27:35.359405: 
2024-12-14 10:27:35.360748: Epoch 109
2024-12-14 10:27:35.361652: Current learning rate: 0.00311
2024-12-14 10:37:59.743330: Validation loss did not improve from -0.46988. Patience: 88/50
2024-12-14 10:37:59.745679: train_loss -0.808
2024-12-14 10:37:59.746545: val_loss -0.339
2024-12-14 10:37:59.747328: Pseudo dice [0.6897]
2024-12-14 10:37:59.747996: Epoch time: 624.39 s
2024-12-14 10:38:01.572740: 
2024-12-14 10:38:01.574091: Epoch 110
2024-12-14 10:38:01.574899: Current learning rate: 0.00304
2024-12-14 10:48:00.504339: Validation loss did not improve from -0.46988. Patience: 89/50
2024-12-14 10:48:00.505623: train_loss -0.8084
2024-12-14 10:48:00.506606: val_loss -0.3528
2024-12-14 10:48:00.507282: Pseudo dice [0.6903]
2024-12-14 10:48:00.507965: Epoch time: 598.93 s
2024-12-14 10:48:01.905164: 
2024-12-14 10:48:01.906524: Epoch 111
2024-12-14 10:48:01.907258: Current learning rate: 0.00297
2024-12-14 10:58:07.640725: Validation loss did not improve from -0.46988. Patience: 90/50
2024-12-14 10:58:07.641716: train_loss -0.8077
2024-12-14 10:58:07.642569: val_loss -0.3413
2024-12-14 10:58:07.643438: Pseudo dice [0.6761]
2024-12-14 10:58:07.644218: Epoch time: 605.74 s
2024-12-14 10:58:09.064742: 
2024-12-14 10:58:09.066057: Epoch 112
2024-12-14 10:58:09.066844: Current learning rate: 0.00291
2024-12-14 11:08:15.993135: Validation loss did not improve from -0.46988. Patience: 91/50
2024-12-14 11:08:15.994000: train_loss -0.8084
2024-12-14 11:08:15.994961: val_loss -0.3627
2024-12-14 11:08:15.995860: Pseudo dice [0.6845]
2024-12-14 11:08:15.996721: Epoch time: 606.93 s
2024-12-14 11:08:17.436055: 
2024-12-14 11:08:17.437581: Epoch 113
2024-12-14 11:08:17.438613: Current learning rate: 0.00284
2024-12-14 11:18:21.865202: Validation loss did not improve from -0.46988. Patience: 92/50
2024-12-14 11:18:21.866199: train_loss -0.8077
2024-12-14 11:18:21.866974: val_loss -0.401
2024-12-14 11:18:21.867720: Pseudo dice [0.7096]
2024-12-14 11:18:21.868458: Epoch time: 604.43 s
2024-12-14 11:18:23.267394: 
2024-12-14 11:18:23.268644: Epoch 114
2024-12-14 11:18:23.269629: Current learning rate: 0.00277
2024-12-14 11:28:28.341789: Validation loss did not improve from -0.46988. Patience: 93/50
2024-12-14 11:28:28.342807: train_loss -0.8116
2024-12-14 11:28:28.343712: val_loss -0.3366
2024-12-14 11:28:28.344387: Pseudo dice [0.6756]
2024-12-14 11:28:28.345058: Epoch time: 605.08 s
2024-12-14 11:28:30.215871: 
2024-12-14 11:28:30.217238: Epoch 115
2024-12-14 11:28:30.218164: Current learning rate: 0.0027
2024-12-14 11:38:44.272057: Validation loss did not improve from -0.46988. Patience: 94/50
2024-12-14 11:38:44.273081: train_loss -0.8114
2024-12-14 11:38:44.274357: val_loss -0.3526
2024-12-14 11:38:44.275422: Pseudo dice [0.68]
2024-12-14 11:38:44.276424: Epoch time: 614.06 s
2024-12-14 11:38:46.127087: 
2024-12-14 11:38:46.128495: Epoch 116
2024-12-14 11:38:46.129569: Current learning rate: 0.00263
2024-12-14 11:49:07.666337: Validation loss did not improve from -0.46988. Patience: 95/50
2024-12-14 11:49:07.667587: train_loss -0.8113
2024-12-14 11:49:07.669552: val_loss -0.3649
2024-12-14 11:49:07.670503: Pseudo dice [0.6883]
2024-12-14 11:49:07.671618: Epoch time: 621.54 s
2024-12-14 11:49:09.146688: 
2024-12-14 11:49:09.148112: Epoch 117
2024-12-14 11:49:09.149033: Current learning rate: 0.00256
2024-12-14 11:59:44.942713: Validation loss did not improve from -0.46988. Patience: 96/50
2024-12-14 11:59:44.943841: train_loss -0.8121
2024-12-14 11:59:44.944760: val_loss -0.3498
2024-12-14 11:59:44.945464: Pseudo dice [0.6727]
2024-12-14 11:59:44.946255: Epoch time: 635.8 s
2024-12-14 11:59:46.387336: 
2024-12-14 11:59:46.388812: Epoch 118
2024-12-14 11:59:46.389848: Current learning rate: 0.00249
2024-12-14 12:10:10.731737: Validation loss did not improve from -0.46988. Patience: 97/50
2024-12-14 12:10:10.732716: train_loss -0.8142
2024-12-14 12:10:10.733788: val_loss -0.369
2024-12-14 12:10:10.734862: Pseudo dice [0.6938]
2024-12-14 12:10:10.735672: Epoch time: 624.35 s
2024-12-14 12:10:12.326956: 
2024-12-14 12:10:12.328291: Epoch 119
2024-12-14 12:10:12.329169: Current learning rate: 0.00242
2024-12-14 12:20:46.355428: Validation loss did not improve from -0.46988. Patience: 98/50
2024-12-14 12:20:46.356367: train_loss -0.8154
2024-12-14 12:20:46.357632: val_loss -0.393
2024-12-14 12:20:46.358343: Pseudo dice [0.7012]
2024-12-14 12:20:46.358980: Epoch time: 634.03 s
2024-12-14 12:20:48.210759: 
2024-12-14 12:20:48.212140: Epoch 120
2024-12-14 12:20:48.212992: Current learning rate: 0.00235
2024-12-14 12:30:53.394372: Validation loss did not improve from -0.46988. Patience: 99/50
2024-12-14 12:30:53.395309: train_loss -0.8109
2024-12-14 12:30:53.396257: val_loss -0.4131
2024-12-14 12:30:53.397040: Pseudo dice [0.7037]
2024-12-14 12:30:53.397965: Epoch time: 605.19 s
2024-12-14 12:30:54.852628: 
2024-12-14 12:30:54.854092: Epoch 121
2024-12-14 12:30:54.855277: Current learning rate: 0.00228
2024-12-14 12:40:49.428519: Validation loss did not improve from -0.46988. Patience: 100/50
2024-12-14 12:40:49.429588: train_loss -0.8146
2024-12-14 12:40:49.430524: val_loss -0.3408
2024-12-14 12:40:49.431461: Pseudo dice [0.6953]
2024-12-14 12:40:49.432509: Epoch time: 594.58 s
2024-12-14 12:40:50.879888: 
2024-12-14 12:40:50.881386: Epoch 122
2024-12-14 12:40:50.882780: Current learning rate: 0.00221
2024-12-14 12:50:33.957392: Validation loss did not improve from -0.46988. Patience: 101/50
2024-12-14 12:50:33.958477: train_loss -0.8159
2024-12-14 12:50:33.959740: val_loss -0.2842
2024-12-14 12:50:33.960465: Pseudo dice [0.6516]
2024-12-14 12:50:33.961333: Epoch time: 583.08 s
2024-12-14 12:50:35.418560: 
2024-12-14 12:50:35.419558: Epoch 123
2024-12-14 12:50:35.420243: Current learning rate: 0.00214
2024-12-14 12:59:26.013573: Validation loss did not improve from -0.46988. Patience: 102/50
2024-12-14 12:59:26.014509: train_loss -0.8151
2024-12-14 12:59:26.015250: val_loss -0.3739
2024-12-14 12:59:26.015966: Pseudo dice [0.6982]
2024-12-14 12:59:26.016759: Epoch time: 530.6 s
2024-12-14 12:59:27.480599: 
2024-12-14 12:59:27.481823: Epoch 124
2024-12-14 12:59:27.482589: Current learning rate: 0.00207
2024-12-14 13:08:12.874787: Validation loss did not improve from -0.46988. Patience: 103/50
2024-12-14 13:08:12.875823: train_loss -0.8147
2024-12-14 13:08:12.876809: val_loss -0.3269
2024-12-14 13:08:12.877641: Pseudo dice [0.6708]
2024-12-14 13:08:12.878419: Epoch time: 525.4 s
2024-12-14 13:08:14.743976: 
2024-12-14 13:08:14.744974: Epoch 125
2024-12-14 13:08:14.745750: Current learning rate: 0.00199
2024-12-14 13:17:12.442013: Validation loss did not improve from -0.46988. Patience: 104/50
2024-12-14 13:17:12.442725: train_loss -0.8166
2024-12-14 13:17:12.443407: val_loss -0.3696
2024-12-14 13:17:12.443979: Pseudo dice [0.6899]
2024-12-14 13:17:12.444613: Epoch time: 537.7 s
2024-12-14 13:17:13.865612: 
2024-12-14 13:17:13.866741: Epoch 126
2024-12-14 13:17:13.867373: Current learning rate: 0.00192
2024-12-14 13:26:36.662401: Validation loss did not improve from -0.46988. Patience: 105/50
2024-12-14 13:26:36.663186: train_loss -0.8179
2024-12-14 13:26:36.664171: val_loss -0.3198
2024-12-14 13:26:36.665247: Pseudo dice [0.6699]
2024-12-14 13:26:36.666369: Epoch time: 562.8 s
2024-12-14 13:26:38.449840: 
2024-12-14 13:26:38.451214: Epoch 127
2024-12-14 13:26:38.452158: Current learning rate: 0.00185
2024-12-14 13:35:50.500353: Validation loss did not improve from -0.46988. Patience: 106/50
2024-12-14 13:35:50.501309: train_loss -0.8186
2024-12-14 13:35:50.502252: val_loss -0.3456
2024-12-14 13:35:50.503117: Pseudo dice [0.6867]
2024-12-14 13:35:50.504086: Epoch time: 552.05 s
2024-12-14 13:35:51.945419: 
2024-12-14 13:35:51.946924: Epoch 128
2024-12-14 13:35:51.947835: Current learning rate: 0.00178
2024-12-14 13:44:41.709165: Validation loss did not improve from -0.46988. Patience: 107/50
2024-12-14 13:44:41.709866: train_loss -0.8192
2024-12-14 13:44:41.710656: val_loss -0.3593
2024-12-14 13:44:41.711314: Pseudo dice [0.6962]
2024-12-14 13:44:41.712004: Epoch time: 529.77 s
2024-12-14 13:44:43.175696: 
2024-12-14 13:44:43.177203: Epoch 129
2024-12-14 13:44:43.177925: Current learning rate: 0.0017
2024-12-14 13:53:38.566185: Validation loss did not improve from -0.46988. Patience: 108/50
2024-12-14 13:53:38.569876: train_loss -0.8184
2024-12-14 13:53:38.571930: val_loss -0.3396
2024-12-14 13:53:38.572809: Pseudo dice [0.6718]
2024-12-14 13:53:38.573805: Epoch time: 535.39 s
2024-12-14 13:53:40.471066: 
2024-12-14 13:53:40.472249: Epoch 130
2024-12-14 13:53:40.472996: Current learning rate: 0.00163
2024-12-14 14:02:33.941754: Validation loss did not improve from -0.46988. Patience: 109/50
2024-12-14 14:02:33.943368: train_loss -0.8182
2024-12-14 14:02:33.944341: val_loss -0.3926
2024-12-14 14:02:33.945162: Pseudo dice [0.7102]
2024-12-14 14:02:33.945954: Epoch time: 533.47 s
2024-12-14 14:02:35.387331: 
2024-12-14 14:02:35.388258: Epoch 131
2024-12-14 14:02:35.389021: Current learning rate: 0.00156
2024-12-14 14:11:33.949567: Validation loss did not improve from -0.46988. Patience: 110/50
2024-12-14 14:11:33.950592: train_loss -0.8193
2024-12-14 14:11:33.951406: val_loss -0.358
2024-12-14 14:11:33.952110: Pseudo dice [0.6822]
2024-12-14 14:11:33.952787: Epoch time: 538.56 s
2024-12-14 14:11:35.416804: 
2024-12-14 14:11:35.417738: Epoch 132
2024-12-14 14:11:35.418370: Current learning rate: 0.00148
2024-12-14 14:20:36.405222: Validation loss did not improve from -0.46988. Patience: 111/50
2024-12-14 14:20:36.406215: train_loss -0.8205
2024-12-14 14:20:36.407135: val_loss -0.3526
2024-12-14 14:20:36.407829: Pseudo dice [0.6695]
2024-12-14 14:20:36.408590: Epoch time: 540.99 s
2024-12-14 14:20:37.894957: 
2024-12-14 14:20:37.896283: Epoch 133
2024-12-14 14:20:37.896997: Current learning rate: 0.00141
2024-12-14 14:28:45.626945: Validation loss did not improve from -0.46988. Patience: 112/50
2024-12-14 14:28:45.627904: train_loss -0.8174
2024-12-14 14:28:45.628698: val_loss -0.3706
2024-12-14 14:28:45.629398: Pseudo dice [0.6981]
2024-12-14 14:28:45.630086: Epoch time: 487.73 s
2024-12-14 14:28:47.051597: 
2024-12-14 14:28:47.052923: Epoch 134
2024-12-14 14:28:47.053962: Current learning rate: 0.00133
2024-12-14 14:36:44.820036: Validation loss did not improve from -0.46988. Patience: 113/50
2024-12-14 14:36:44.821049: train_loss -0.821
2024-12-14 14:36:44.821890: val_loss -0.3425
2024-12-14 14:36:44.822618: Pseudo dice [0.6833]
2024-12-14 14:36:44.823401: Epoch time: 477.77 s
2024-12-14 14:36:46.713433: 
2024-12-14 14:36:46.714637: Epoch 135
2024-12-14 14:36:46.715464: Current learning rate: 0.00126
2024-12-14 14:45:08.640811: Validation loss did not improve from -0.46988. Patience: 114/50
2024-12-14 14:45:08.641895: train_loss -0.8193
2024-12-14 14:45:08.643078: val_loss -0.3539
2024-12-14 14:45:08.644243: Pseudo dice [0.6832]
2024-12-14 14:45:08.645382: Epoch time: 501.93 s
2024-12-14 14:45:10.083016: 
2024-12-14 14:45:10.084476: Epoch 136
2024-12-14 14:45:10.085466: Current learning rate: 0.00118
2024-12-14 14:52:35.266204: Validation loss did not improve from -0.46988. Patience: 115/50
2024-12-14 14:52:35.266951: train_loss -0.8219
2024-12-14 14:52:35.267787: val_loss -0.3399
2024-12-14 14:52:35.268446: Pseudo dice [0.6839]
2024-12-14 14:52:35.269058: Epoch time: 445.19 s
2024-12-14 14:52:36.747304: 
2024-12-14 14:52:36.748460: Epoch 137
2024-12-14 14:52:36.749190: Current learning rate: 0.00111
2024-12-14 14:59:02.575761: Validation loss did not improve from -0.46988. Patience: 116/50
2024-12-14 14:59:02.576970: train_loss -0.8224
2024-12-14 14:59:02.577784: val_loss -0.3607
2024-12-14 14:59:02.578570: Pseudo dice [0.682]
2024-12-14 14:59:02.579334: Epoch time: 385.83 s
2024-12-14 14:59:04.473523: 
2024-12-14 14:59:04.474972: Epoch 138
2024-12-14 14:59:04.475740: Current learning rate: 0.00103
2024-12-14 15:05:11.106508: Validation loss did not improve from -0.46988. Patience: 117/50
2024-12-14 15:05:11.107544: train_loss -0.8242
2024-12-14 15:05:11.108594: val_loss -0.3418
2024-12-14 15:05:11.109601: Pseudo dice [0.6904]
2024-12-14 15:05:11.110424: Epoch time: 366.63 s
2024-12-14 15:05:12.579405: 
2024-12-14 15:05:12.580821: Epoch 139
2024-12-14 15:05:12.581747: Current learning rate: 0.00095
2024-12-14 15:11:41.673031: Validation loss did not improve from -0.46988. Patience: 118/50
2024-12-14 15:11:41.674163: train_loss -0.8218
2024-12-14 15:11:41.674906: val_loss -0.3609
2024-12-14 15:11:41.675579: Pseudo dice [0.6885]
2024-12-14 15:11:41.676237: Epoch time: 389.1 s
2024-12-14 15:11:43.572512: 
2024-12-14 15:11:43.573655: Epoch 140
2024-12-14 15:11:43.574605: Current learning rate: 0.00087
2024-12-14 15:18:21.840481: Validation loss did not improve from -0.46988. Patience: 119/50
2024-12-14 15:18:21.841543: train_loss -0.8206
2024-12-14 15:18:21.842277: val_loss -0.3437
2024-12-14 15:18:21.842926: Pseudo dice [0.6912]
2024-12-14 15:18:21.843606: Epoch time: 398.27 s
2024-12-14 15:18:23.371194: 
2024-12-14 15:18:23.372379: Epoch 141
2024-12-14 15:18:23.373061: Current learning rate: 0.00079
2024-12-14 15:24:59.327513: Validation loss did not improve from -0.46988. Patience: 120/50
2024-12-14 15:24:59.329130: train_loss -0.8226
2024-12-14 15:24:59.330498: val_loss -0.3392
2024-12-14 15:24:59.331311: Pseudo dice [0.6824]
2024-12-14 15:24:59.332208: Epoch time: 395.96 s
2024-12-14 15:25:00.768036: 
2024-12-14 15:25:00.769380: Epoch 142
2024-12-14 15:25:00.770225: Current learning rate: 0.00071
2024-12-14 15:31:24.552591: Validation loss did not improve from -0.46988. Patience: 121/50
2024-12-14 15:31:24.553572: train_loss -0.8203
2024-12-14 15:31:24.554532: val_loss -0.3577
2024-12-14 15:31:24.555444: Pseudo dice [0.6853]
2024-12-14 15:31:24.556398: Epoch time: 383.79 s
2024-12-14 15:31:26.032878: 
2024-12-14 15:31:26.034199: Epoch 143
2024-12-14 15:31:26.035183: Current learning rate: 0.00063
2024-12-14 15:37:29.832926: Validation loss did not improve from -0.46988. Patience: 122/50
2024-12-14 15:37:29.834193: train_loss -0.8251
2024-12-14 15:37:29.835258: val_loss -0.3571
2024-12-14 15:37:29.835999: Pseudo dice [0.687]
2024-12-14 15:37:29.836774: Epoch time: 363.8 s
2024-12-14 15:37:31.269768: 
2024-12-14 15:37:31.271304: Epoch 144
2024-12-14 15:37:31.272354: Current learning rate: 0.00055
2024-12-14 15:43:34.332010: Validation loss did not improve from -0.46988. Patience: 123/50
2024-12-14 15:43:34.333082: train_loss -0.8249
2024-12-14 15:43:34.333931: val_loss -0.3363
2024-12-14 15:43:34.334688: Pseudo dice [0.6729]
2024-12-14 15:43:34.335364: Epoch time: 363.06 s
2024-12-14 15:43:36.126590: 
2024-12-14 15:43:36.127566: Epoch 145
2024-12-14 15:43:36.128386: Current learning rate: 0.00047
2024-12-14 15:49:50.199813: Validation loss did not improve from -0.46988. Patience: 124/50
2024-12-14 15:49:50.200850: train_loss -0.8262
2024-12-14 15:49:50.201586: val_loss -0.3515
2024-12-14 15:49:50.202262: Pseudo dice [0.6825]
2024-12-14 15:49:50.202997: Epoch time: 374.08 s
2024-12-14 15:49:51.636593: 
2024-12-14 15:49:51.638041: Epoch 146
2024-12-14 15:49:51.638895: Current learning rate: 0.00038
2024-12-14 15:55:59.239559: Validation loss did not improve from -0.46988. Patience: 125/50
2024-12-14 15:55:59.240593: train_loss -0.8248
2024-12-14 15:55:59.241659: val_loss -0.3053
2024-12-14 15:55:59.242576: Pseudo dice [0.6664]
2024-12-14 15:55:59.243592: Epoch time: 367.61 s
2024-12-14 15:56:00.665468: 
2024-12-14 15:56:00.667427: Epoch 147
2024-12-14 15:56:00.668775: Current learning rate: 0.0003
2024-12-14 16:02:15.170972: Validation loss did not improve from -0.46988. Patience: 126/50
2024-12-14 16:02:15.171976: train_loss -0.8242
2024-12-14 16:02:15.172670: val_loss -0.3556
2024-12-14 16:02:15.173645: Pseudo dice [0.6803]
2024-12-14 16:02:15.174263: Epoch time: 374.51 s
2024-12-14 16:02:16.985922: 
2024-12-14 16:02:16.987344: Epoch 148
2024-12-14 16:02:16.988224: Current learning rate: 0.00021
2024-12-14 16:08:14.985482: Validation loss did not improve from -0.46988. Patience: 127/50
2024-12-14 16:08:14.986334: train_loss -0.8251
2024-12-14 16:08:14.987138: val_loss -0.3771
2024-12-14 16:08:14.987819: Pseudo dice [0.6936]
2024-12-14 16:08:14.988487: Epoch time: 358.0 s
2024-12-14 16:08:16.480424: 
2024-12-14 16:08:16.481764: Epoch 149
2024-12-14 16:08:16.482497: Current learning rate: 0.00011
2024-12-14 16:14:13.082567: Validation loss did not improve from -0.46988. Patience: 128/50
2024-12-14 16:14:13.083649: train_loss -0.8223
2024-12-14 16:14:13.084401: val_loss -0.3859
2024-12-14 16:14:13.085042: Pseudo dice [0.7097]
2024-12-14 16:14:13.085724: Epoch time: 356.6 s
2024-12-14 16:14:14.947651: Training done.
2024-12-14 16:14:15.059938: Using splits from existing split file: /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset307_Sohee_Calcium_OCT_CrossValidation/splits_final_40.json
2024-12-14 16:14:15.062076: The split file contains 5 splits.
2024-12-14 16:14:15.063018: Desired fold for training: 0
2024-12-14 16:14:15.063714: This split has 3 training and 5 validation cases.
2024-12-14 16:14:15.064642: predicting 101-045
2024-12-14 16:14:15.129660: 101-045, shape torch.Size([1, 375, 498, 498]), rank 0
2024-12-14 16:15:55.232628: predicting 106-002
2024-12-14 16:15:55.252703: 106-002, shape torch.Size([1, 539, 498, 498]), rank 0
2024-12-14 16:18:46.689876: predicting 701-013
2024-12-14 16:18:46.717748: 701-013, shape torch.Size([1, 375, 498, 498]), rank 0
2024-12-14 16:20:52.754928: predicting 704-003
2024-12-14 16:20:52.769682: 704-003, shape torch.Size([1, 375, 498, 498]), rank 0
2024-12-14 16:22:46.476309: predicting 706-005
2024-12-14 16:22:46.493688: 706-005, shape torch.Size([1, 375, 498, 498]), rank 0
2024-12-14 16:25:02.387321: Validation complete
2024-12-14 16:25:02.388730: Mean Validation Dice:  0.6867334575337433

############################
INFO: You are using the old nnU-Net default plans. We have updated our recommendations. Please consider using those instead! Read more here: https://github.com/MIC-DKFZ/nnUNet/blob/master/documentation/resenc_presets.md
############################

Using device: cuda:0

#######################################################################
Please cite the following paper when using nnU-Net:
Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.
#######################################################################

2024-12-14 16:25:09.992385: Using torch.compile...

############################
INFO: You are using the old nnU-Net default plans. We have updated our recommendations. Please consider using those instead! Read more here: https://github.com/MIC-DKFZ/nnUNet/blob/master/documentation/resenc_presets.md
############################

Using device: cuda:0

#######################################################################
Please cite the following paper when using nnU-Net:
Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.
#######################################################################

2024-12-14 16:25:09.963624: Using torch.compile...
/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
################### Loading pretrained weights from file  /home/gridsan/nchutisilp/projects/OpenAI-CLIP/clip_preivl_poststent_pretrained_nnUNet.pt ###################
Below is the list of overlapping blocks in pretrained model and nnUNet architecture:
encoder.stages.0.0.convs.0.conv.weight shape torch.Size([32, 1, 3, 3, 3])
encoder.stages.0.0.convs.0.conv.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.norm.weight shape torch.Size([32])
encoder.stages.0.0.convs.0.norm.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.0.weight shape torch.Size([32, 1, 3, 3, 3])
encoder.stages.0.0.convs.0.all_modules.0.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.1.weight shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.1.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
encoder.stages.0.0.convs.1.conv.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.norm.weight shape torch.Size([32])
encoder.stages.0.0.convs.1.norm.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
encoder.stages.0.0.convs.1.all_modules.0.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.1.weight shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.1.bias shape torch.Size([32])
encoder.stages.1.0.convs.0.conv.weight shape torch.Size([64, 32, 3, 3, 3])
encoder.stages.1.0.convs.0.conv.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.norm.weight shape torch.Size([64])
encoder.stages.1.0.convs.0.norm.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.0.weight shape torch.Size([64, 32, 3, 3, 3])
encoder.stages.1.0.convs.0.all_modules.0.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.1.weight shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.1.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
encoder.stages.1.0.convs.1.conv.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.norm.weight shape torch.Size([64])
encoder.stages.1.0.convs.1.norm.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
encoder.stages.1.0.convs.1.all_modules.0.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.1.weight shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.1.bias shape torch.Size([64])
encoder.stages.2.0.convs.0.conv.weight shape torch.Size([128, 64, 3, 3, 3])
encoder.stages.2.0.convs.0.conv.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.norm.weight shape torch.Size([128])
encoder.stages.2.0.convs.0.norm.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.0.weight shape torch.Size([128, 64, 3, 3, 3])
encoder.stages.2.0.convs.0.all_modules.0.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.1.weight shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.1.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
encoder.stages.2.0.convs.1.conv.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.norm.weight shape torch.Size([128])
encoder.stages.2.0.convs.1.norm.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
encoder.stages.2.0.convs.1.all_modules.0.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.1.weight shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.1.bias shape torch.Size([128])
encoder.stages.3.0.convs.0.conv.weight shape torch.Size([256, 128, 3, 3, 3])
encoder.stages.3.0.convs.0.conv.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.norm.weight shape torch.Size([256])
encoder.stages.3.0.convs.0.norm.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.0.weight shape torch.Size([256, 128, 3, 3, 3])
encoder.stages.3.0.convs.0.all_modules.0.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.1.weight shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.1.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
encoder.stages.3.0.convs.1.conv.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.norm.weight shape torch.Size([256])
encoder.stages.3.0.convs.1.norm.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
encoder.stages.3.0.convs.1.all_modules.0.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.1.weight shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.1.bias shape torch.Size([256])
encoder.stages.4.0.convs.0.conv.weight shape torch.Size([320, 256, 3, 3, 3])
encoder.stages.4.0.convs.0.conv.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.norm.weight shape torch.Size([320])
encoder.stages.4.0.convs.0.norm.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.0.weight shape torch.Size([320, 256, 3, 3, 3])
encoder.stages.4.0.convs.0.all_modules.0.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.1.weight shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.1.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.4.0.convs.1.conv.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.norm.weight shape torch.Size([320])
encoder.stages.4.0.convs.1.norm.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.4.0.convs.1.all_modules.0.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.1.weight shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.1.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.0.conv.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.norm.weight shape torch.Size([320])
encoder.stages.5.0.convs.0.norm.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.0.all_modules.0.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.1.weight shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.1.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.1.conv.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.norm.weight shape torch.Size([320])
encoder.stages.5.0.convs.1.norm.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.1.all_modules.0.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.1.weight shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.0.0.convs.0.conv.weight shape torch.Size([32, 1, 3, 3, 3])
decoder.encoder.stages.0.0.convs.0.conv.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.norm.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.norm.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.0.weight shape torch.Size([32, 1, 3, 3, 3])
decoder.encoder.stages.0.0.convs.0.all_modules.0.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.1.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.1.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.encoder.stages.0.0.convs.1.conv.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.norm.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.norm.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.encoder.stages.0.0.convs.1.all_modules.0.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.1.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.1.bias shape torch.Size([32])
decoder.encoder.stages.1.0.convs.0.conv.weight shape torch.Size([64, 32, 3, 3, 3])
decoder.encoder.stages.1.0.convs.0.conv.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.norm.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.norm.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.0.weight shape torch.Size([64, 32, 3, 3, 3])
decoder.encoder.stages.1.0.convs.0.all_modules.0.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.1.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.1.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.encoder.stages.1.0.convs.1.conv.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.norm.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.norm.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.encoder.stages.1.0.convs.1.all_modules.0.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.1.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.1.bias shape torch.Size([64])
decoder.encoder.stages.2.0.convs.0.conv.weight shape torch.Size([128, 64, 3, 3, 3])
decoder.encoder.stages.2.0.convs.0.conv.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.norm.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.norm.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.0.weight shape torch.Size([128, 64, 3, 3, 3])
decoder.encoder.stages.2.0.convs.0.all_modules.0.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.1.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.1.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.encoder.stages.2.0.convs.1.conv.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.norm.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.norm.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.encoder.stages.2.0.convs.1.all_modules.0.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.1.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.1.bias shape torch.Size([128])
decoder.encoder.stages.3.0.convs.0.conv.weight shape torch.Size([256, 128, 3, 3, 3])
decoder.encoder.stages.3.0.convs.0.conv.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.norm.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.norm.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.0.weight shape torch.Size([256, 128, 3, 3, 3])
decoder.encoder.stages.3.0.convs.0.all_modules.0.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.1.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.1.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.encoder.stages.3.0.convs.1.conv.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.norm.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.norm.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.encoder.stages.3.0.convs.1.all_modules.0.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.1.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.1.bias shape torch.Size([256])
decoder.encoder.stages.4.0.convs.0.conv.weight shape torch.Size([320, 256, 3, 3, 3])
decoder.encoder.stages.4.0.convs.0.conv.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.norm.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.norm.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.0.weight shape torch.Size([320, 256, 3, 3, 3])
decoder.encoder.stages.4.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.4.0.convs.1.conv.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.norm.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.norm.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.4.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.0.conv.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.norm.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.norm.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.1.conv.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.norm.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.norm.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.stages.0.convs.0.conv.weight shape torch.Size([320, 640, 3, 3, 3])
decoder.stages.0.convs.0.conv.bias shape torch.Size([320])
decoder.stages.0.convs.0.norm.weight shape torch.Size([320])
decoder.stages.0.convs.0.norm.bias shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.0.weight shape torch.Size([320, 640, 3, 3, 3])
decoder.stages.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.stages.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.stages.0.convs.1.conv.bias shape torch.Size([320])
decoder.stages.0.convs.1.norm.weight shape torch.Size([320])
decoder.stages.0.convs.1.norm.bias shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.stages.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.stages.1.convs.0.conv.weight shape torch.Size([256, 512, 3, 3, 3])
decoder.stages.1.convs.0.conv.bias shape torch.Size([256])
decoder.stages.1.convs.0.norm.weight shape torch.Size([256])
decoder.stages.1.convs.0.norm.bias shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.0.weight shape torch.Size([256, 512, 3, 3, 3])
decoder.stages.1.convs.0.all_modules.0.bias shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.1.weight shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.1.bias shape torch.Size([256])
decoder.stages.1.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.stages.1.convs.1.conv.bias shape torch.Size([256])
decoder.stages.1.convs.1.norm.weight shape torch.Size([256])
decoder.stages.1.convs.1.norm.bias shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.stages.1.convs.1.all_modules.0.bias shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.1.weight shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.1.bias shape torch.Size([256])
decoder.stages.2.convs.0.conv.weight shape torch.Size([128, 256, 3, 3, 3])
decoder.stages.2.convs.0.conv.bias shape torch.Size([128])
decoder.stages.2.convs.0.norm.weight shape torch.Size([128])
decoder.stages.2.convs.0.norm.bias shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.0.weight shape torch.Size([128, 256, 3, 3, 3])
decoder.stages.2.convs.0.all_modules.0.bias shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.1.weight shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.1.bias shape torch.Size([128])
decoder.stages.2.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.stages.2.convs.1.conv.bias shape torch.Size([128])
decoder.stages.2.convs.1.norm.weight shape torch.Size([128])
decoder.stages.2.convs.1.norm.bias shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.stages.2.convs.1.all_modules.0.bias shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.1.weight shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.1.bias shape torch.Size([128])
decoder.stages.3.convs.0.conv.weight shape torch.Size([64, 128, 3, 3, 3])
decoder.stages.3.convs.0.conv.bias shape torch.Size([64])
decoder.stages.3.convs.0.norm.weight shape torch.Size([64])
decoder.stages.3.convs.0.norm.bias shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.0.weight shape torch.Size([64, 128, 3, 3, 3])
decoder.stages.3.convs.0.all_modules.0.bias shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.1.weight shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.1.bias shape torch.Size([64])
decoder.stages.3.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.stages.3.convs.1.conv.bias shape torch.Size([64])
decoder.stages.3.convs.1.norm.weight shape torch.Size([64])
decoder.stages.3.convs.1.norm.bias shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.stages.3.convs.1.all_modules.0.bias shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.1.weight shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.1.bias shape torch.Size([64])
decoder.stages.4.convs.0.conv.weight shape torch.Size([32, 64, 3, 3, 3])
decoder.stages.4.convs.0.conv.bias shape torch.Size([32])
decoder.stages.4.convs.0.norm.weight shape torch.Size([32])
decoder.stages.4.convs.0.norm.bias shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.0.weight shape torch.Size([32, 64, 3, 3, 3])
decoder.stages.4.convs.0.all_modules.0.bias shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.1.weight shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.1.bias shape torch.Size([32])
decoder.stages.4.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.stages.4.convs.1.conv.bias shape torch.Size([32])
decoder.stages.4.convs.1.norm.weight shape torch.Size([32])
decoder.stages.4.convs.1.norm.bias shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.stages.4.convs.1.all_modules.0.bias shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.1.weight shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.1.bias shape torch.Size([32])
decoder.transpconvs.0.weight shape torch.Size([320, 320, 1, 2, 2])
decoder.transpconvs.0.bias shape torch.Size([320])
decoder.transpconvs.1.weight shape torch.Size([320, 256, 2, 2, 2])
decoder.transpconvs.1.bias shape torch.Size([256])
decoder.transpconvs.2.weight shape torch.Size([256, 128, 2, 2, 2])
decoder.transpconvs.2.bias shape torch.Size([128])
decoder.transpconvs.3.weight shape torch.Size([128, 64, 2, 2, 2])
decoder.transpconvs.3.bias shape torch.Size([64])
decoder.transpconvs.4.weight shape torch.Size([64, 32, 2, 2, 2])
decoder.transpconvs.4.bias shape torch.Size([32])
################### Done ###################
2024-12-14 16:25:29.051090: do_dummy_2d_data_aug: True
2024-12-14 16:25:29.052652: Using splits from existing split file: /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset307_Sohee_Calcium_OCT_CrossValidation/splits_final_40.json
2024-12-14 16:25:29.055266: The split file contains 5 splits.
2024-12-14 16:25:29.056158: Desired fold for training: 3
2024-12-14 16:25:29.057047: This split has 3 training and 6 validation cases.
################### Loading pretrained weights from file  /home/gridsan/nchutisilp/projects/OpenAI-CLIP/clip_preivl_poststent_pretrained_nnUNet.pt ###################
Below is the list of overlapping blocks in pretrained model and nnUNet architecture:
encoder.stages.0.0.convs.0.conv.weight shape torch.Size([32, 1, 3, 3, 3])
encoder.stages.0.0.convs.0.conv.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.norm.weight shape torch.Size([32])
encoder.stages.0.0.convs.0.norm.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.0.weight shape torch.Size([32, 1, 3, 3, 3])
encoder.stages.0.0.convs.0.all_modules.0.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.1.weight shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.1.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
encoder.stages.0.0.convs.1.conv.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.norm.weight shape torch.Size([32])
encoder.stages.0.0.convs.1.norm.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
encoder.stages.0.0.convs.1.all_modules.0.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.1.weight shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.1.bias shape torch.Size([32])
encoder.stages.1.0.convs.0.conv.weight shape torch.Size([64, 32, 3, 3, 3])
encoder.stages.1.0.convs.0.conv.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.norm.weight shape torch.Size([64])
encoder.stages.1.0.convs.0.norm.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.0.weight shape torch.Size([64, 32, 3, 3, 3])
encoder.stages.1.0.convs.0.all_modules.0.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.1.weight shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.1.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
encoder.stages.1.0.convs.1.conv.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.norm.weight shape torch.Size([64])
encoder.stages.1.0.convs.1.norm.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
encoder.stages.1.0.convs.1.all_modules.0.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.1.weight shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.1.bias shape torch.Size([64])
encoder.stages.2.0.convs.0.conv.weight shape torch.Size([128, 64, 3, 3, 3])
encoder.stages.2.0.convs.0.conv.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.norm.weight shape torch.Size([128])
encoder.stages.2.0.convs.0.norm.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.0.weight shape torch.Size([128, 64, 3, 3, 3])
encoder.stages.2.0.convs.0.all_modules.0.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.1.weight shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.1.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
encoder.stages.2.0.convs.1.conv.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.norm.weight shape torch.Size([128])
encoder.stages.2.0.convs.1.norm.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
encoder.stages.2.0.convs.1.all_modules.0.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.1.weight shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.1.bias shape torch.Size([128])
encoder.stages.3.0.convs.0.conv.weight shape torch.Size([256, 128, 3, 3, 3])
encoder.stages.3.0.convs.0.conv.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.norm.weight shape torch.Size([256])
encoder.stages.3.0.convs.0.norm.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.0.weight shape torch.Size([256, 128, 3, 3, 3])
encoder.stages.3.0.convs.0.all_modules.0.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.1.weight shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.1.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
encoder.stages.3.0.convs.1.conv.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.norm.weight shape torch.Size([256])
encoder.stages.3.0.convs.1.norm.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
encoder.stages.3.0.convs.1.all_modules.0.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.1.weight shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.1.bias shape torch.Size([256])
encoder.stages.4.0.convs.0.conv.weight shape torch.Size([320, 256, 3, 3, 3])
encoder.stages.4.0.convs.0.conv.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.norm.weight shape torch.Size([320])
encoder.stages.4.0.convs.0.norm.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.0.weight shape torch.Size([320, 256, 3, 3, 3])
encoder.stages.4.0.convs.0.all_modules.0.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.1.weight shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.1.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.4.0.convs.1.conv.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.norm.weight shape torch.Size([320])
encoder.stages.4.0.convs.1.norm.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.4.0.convs.1.all_modules.0.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.1.weight shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.1.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.0.conv.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.norm.weight shape torch.Size([320])
encoder.stages.5.0.convs.0.norm.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.0.all_modules.0.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.1.weight shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.1.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.1.conv.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.norm.weight shape torch.Size([320])
encoder.stages.5.0.convs.1.norm.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.1.all_modules.0.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.1.weight shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.0.0.convs.0.conv.weight shape torch.Size([32, 1, 3, 3, 3])
decoder.encoder.stages.0.0.convs.0.conv.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.norm.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.norm.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.0.weight shape torch.Size([32, 1, 3, 3, 3])
decoder.encoder.stages.0.0.convs.0.all_modules.0.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.1.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.1.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.encoder.stages.0.0.convs.1.conv.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.norm.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.norm.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.encoder.stages.0.0.convs.1.all_modules.0.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.1.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.1.bias shape torch.Size([32])
decoder.encoder.stages.1.0.convs.0.conv.weight shape torch.Size([64, 32, 3, 3, 3])
decoder.encoder.stages.1.0.convs.0.conv.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.norm.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.norm.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.0.weight shape torch.Size([64, 32, 3, 3, 3])
decoder.encoder.stages.1.0.convs.0.all_modules.0.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.1.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.1.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.encoder.stages.1.0.convs.1.conv.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.norm.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.norm.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.encoder.stages.1.0.convs.1.all_modules.0.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.1.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.1.bias shape torch.Size([64])
decoder.encoder.stages.2.0.convs.0.conv.weight shape torch.Size([128, 64, 3, 3, 3])
decoder.encoder.stages.2.0.convs.0.conv.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.norm.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.norm.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.0.weight shape torch.Size([128, 64, 3, 3, 3])
decoder.encoder.stages.2.0.convs.0.all_modules.0.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.1.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.1.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.encoder.stages.2.0.convs.1.conv.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.norm.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.norm.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.encoder.stages.2.0.convs.1.all_modules.0.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.1.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.1.bias shape torch.Size([128])
decoder.encoder.stages.3.0.convs.0.conv.weight shape torch.Size([256, 128, 3, 3, 3])
decoder.encoder.stages.3.0.convs.0.conv.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.norm.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.norm.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.0.weight shape torch.Size([256, 128, 3, 3, 3])
decoder.encoder.stages.3.0.convs.0.all_modules.0.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.1.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.1.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.encoder.stages.3.0.convs.1.conv.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.norm.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.norm.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.encoder.stages.3.0.convs.1.all_modules.0.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.1.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.1.bias shape torch.Size([256])
decoder.encoder.stages.4.0.convs.0.conv.weight shape torch.Size([320, 256, 3, 3, 3])
decoder.encoder.stages.4.0.convs.0.conv.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.norm.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.norm.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.0.weight shape torch.Size([320, 256, 3, 3, 3])
decoder.encoder.stages.4.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.4.0.convs.1.conv.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.norm.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.norm.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.4.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.0.conv.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.norm.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.norm.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.1.conv.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.norm.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.norm.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.stages.0.convs.0.conv.weight shape torch.Size([320, 640, 3, 3, 3])
decoder.stages.0.convs.0.conv.bias shape torch.Size([320])
decoder.stages.0.convs.0.norm.weight shape torch.Size([320])
decoder.stages.0.convs.0.norm.bias shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.0.weight shape torch.Size([320, 640, 3, 3, 3])
decoder.stages.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.stages.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.stages.0.convs.1.conv.bias shape torch.Size([320])
decoder.stages.0.convs.1.norm.weight shape torch.Size([320])
decoder.stages.0.convs.1.norm.bias shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.stages.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.stages.1.convs.0.conv.weight shape torch.Size([256, 512, 3, 3, 3])
decoder.stages.1.convs.0.conv.bias shape torch.Size([256])
decoder.stages.1.convs.0.norm.weight shape torch.Size([256])
decoder.stages.1.convs.0.norm.bias shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.0.weight shape torch.Size([256, 512, 3, 3, 3])
decoder.stages.1.convs.0.all_modules.0.bias shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.1.weight shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.1.bias shape torch.Size([256])
decoder.stages.1.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.stages.1.convs.1.conv.bias shape torch.Size([256])
decoder.stages.1.convs.1.norm.weight shape torch.Size([256])
decoder.stages.1.convs.1.norm.bias shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.stages.1.convs.1.all_modules.0.bias shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.1.weight shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.1.bias shape torch.Size([256])
decoder.stages.2.convs.0.conv.weight shape torch.Size([128, 256, 3, 3, 3])
decoder.stages.2.convs.0.conv.bias shape torch.Size([128])
decoder.stages.2.convs.0.norm.weight shape torch.Size([128])
decoder.stages.2.convs.0.norm.bias shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.0.weight shape torch.Size([128, 256, 3, 3, 3])
decoder.stages.2.convs.0.all_modules.0.bias shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.1.weight shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.1.bias shape torch.Size([128])
decoder.stages.2.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.stages.2.convs.1.conv.bias shape torch.Size([128])
decoder.stages.2.convs.1.norm.weight shape torch.Size([128])
decoder.stages.2.convs.1.norm.bias shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.stages.2.convs.1.all_modules.0.bias shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.1.weight shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.1.bias shape torch.Size([128])
decoder.stages.3.convs.0.conv.weight shape torch.Size([64, 128, 3, 3, 3])
decoder.stages.3.convs.0.conv.bias shape torch.Size([64])
decoder.stages.3.convs.0.norm.weight shape torch.Size([64])
decoder.stages.3.convs.0.norm.bias shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.0.weight shape torch.Size([64, 128, 3, 3, 3])
decoder.stages.3.convs.0.all_modules.0.bias shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.1.weight shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.1.bias shape torch.Size([64])
decoder.stages.3.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.stages.3.convs.1.conv.bias shape torch.Size([64])
decoder.stages.3.convs.1.norm.weight shape torch.Size([64])
decoder.stages.3.convs.1.norm.bias shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.stages.3.convs.1.all_modules.0.bias shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.1.weight shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.1.bias shape torch.Size([64])
decoder.stages.4.convs.0.conv.weight shape torch.Size([32, 64, 3, 3, 3])
decoder.stages.4.convs.0.conv.bias shape torch.Size([32])
decoder.stages.4.convs.0.norm.weight shape torch.Size([32])
decoder.stages.4.convs.0.norm.bias shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.0.weight shape torch.Size([32, 64, 3, 3, 3])
decoder.stages.4.convs.0.all_modules.0.bias shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.1.weight shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.1.bias shape torch.Size([32])
decoder.stages.4.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.stages.4.convs.1.conv.bias shape torch.Size([32])
decoder.stages.4.convs.1.norm.weight shape torch.Size([32])
decoder.stages.4.convs.1.norm.bias shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.stages.4.convs.1.all_modules.0.bias shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.1.weight shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.1.bias shape torch.Size([32])
decoder.transpconvs.0.weight shape torch.Size([320, 320, 1, 2, 2])
decoder.transpconvs.0.bias shape torch.Size([320])
decoder.transpconvs.1.weight shape torch.Size([320, 256, 2, 2, 2])
decoder.transpconvs.1.bias shape torch.Size([256])
decoder.transpconvs.2.weight shape torch.Size([256, 128, 2, 2, 2])
decoder.transpconvs.2.bias shape torch.Size([128])
decoder.transpconvs.3.weight shape torch.Size([128, 64, 2, 2, 2])
decoder.transpconvs.3.bias shape torch.Size([64])
decoder.transpconvs.4.weight shape torch.Size([64, 32, 2, 2, 2])
decoder.transpconvs.4.bias shape torch.Size([32])
################### Done ###################
2024-12-14 16:25:29.051136: do_dummy_2d_data_aug: True
2024-12-14 16:25:29.052648: Using splits from existing split file: /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset307_Sohee_Calcium_OCT_CrossValidation/splits_final_40.json
2024-12-14 16:25:29.054940: The split file contains 5 splits.
2024-12-14 16:25:29.055974: Desired fold for training: 2
2024-12-14 16:25:29.056793: This split has 3 training and 5 validation cases.
using pin_memory on device 0
using pin_memory on device 0
using pin_memory on device 0

This is the configuration used by this training:
Configuration name: 3d_32x160x128_b10
 {'data_identifier': 'nnUNetPlans_3d_fullres', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 10, 'patch_size': [32, 160, 128], 'median_image_size_in_voxels': [375.0, 498.0, 498.0], 'spacing': [1.0, 1.0, 1.0], 'normalization_schemes': ['ZScoreNormalization'], 'use_mask_for_norm': [False], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'architecture': {'network_class_name': 'dynamic_network_architectures.architectures.unet.PlainConvUNet', 'arch_kwargs': {'n_stages': 6, 'features_per_stage': [32, 64, 128, 256, 320, 320], 'conv_op': 'torch.nn.modules.conv.Conv3d', 'kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]], 'strides': [[1, 1, 1], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [1, 2, 2]], 'n_conv_per_stage': [2, 2, 2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2, 2, 2], 'conv_bias': True, 'norm_op': 'torch.nn.modules.instancenorm.InstanceNorm3d', 'norm_op_kwargs': {'eps': 1e-05, 'affine': True}, 'dropout_op': None, 'dropout_op_kwargs': None, 'nonlin': 'torch.nn.LeakyReLU', 'nonlin_kwargs': {'inplace': True}, 'deep_supervision': True}, '_kw_requires_import': ['conv_op', 'norm_op', 'dropout_op', 'nonlin']}, 'batch_dice': True, 'inherits_from': '3d_fullres'} 

These are the global plan.json settings:
 {'dataset_name': 'Dataset307_Sohee_Calcium_OCT_CrossValidation', 'plans_name': 'nnUNetPlans', 'original_median_spacing_after_transp': [1.0, 1.0, 1.0], 'original_median_shape_after_transp': [375, 498, 498], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 0.4977976381778717, 'mean': 0.13507525622844696, 'median': 0.09599608182907104, 'min': 0.0, 'percentile_00_5': 0.014754901640117168, 'percentile_99_5': 0.4977976381778717, 'std': 0.12025152146816254}}} 

2024-12-14 16:25:49.516257: unpacking dataset...
using pin_memory on device 0

This is the configuration used by this training:
Configuration name: 3d_32x160x128_b10
 {'data_identifier': 'nnUNetPlans_3d_fullres', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 10, 'patch_size': [32, 160, 128], 'median_image_size_in_voxels': [375.0, 498.0, 498.0], 'spacing': [1.0, 1.0, 1.0], 'normalization_schemes': ['ZScoreNormalization'], 'use_mask_for_norm': [False], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'architecture': {'network_class_name': 'dynamic_network_architectures.architectures.unet.PlainConvUNet', 'arch_kwargs': {'n_stages': 6, 'features_per_stage': [32, 64, 128, 256, 320, 320], 'conv_op': 'torch.nn.modules.conv.Conv3d', 'kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]], 'strides': [[1, 1, 1], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [1, 2, 2]], 'n_conv_per_stage': [2, 2, 2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2, 2, 2], 'conv_bias': True, 'norm_op': 'torch.nn.modules.instancenorm.InstanceNorm3d', 'norm_op_kwargs': {'eps': 1e-05, 'affine': True}, 'dropout_op': None, 'dropout_op_kwargs': None, 'nonlin': 'torch.nn.LeakyReLU', 'nonlin_kwargs': {'inplace': True}, 'deep_supervision': True}, '_kw_requires_import': ['conv_op', 'norm_op', 'dropout_op', 'nonlin']}, 'batch_dice': True, 'inherits_from': '3d_fullres'} 

These are the global plan.json settings:
 {'dataset_name': 'Dataset307_Sohee_Calcium_OCT_CrossValidation', 'plans_name': 'nnUNetPlans', 'original_median_spacing_after_transp': [1.0, 1.0, 1.0], 'original_median_shape_after_transp': [375, 498, 498], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 0.4977976381778717, 'mean': 0.13507525622844696, 'median': 0.09599608182907104, 'min': 0.0, 'percentile_00_5': 0.014754901640117168, 'percentile_99_5': 0.4977976381778717, 'std': 0.12025152146816254}}} 

2024-12-14 16:25:50.040961: unpacking dataset...
2024-12-14 16:25:54.304633: unpacking done...
2024-12-14 16:25:54.311844: Unable to plot network architecture: nnUNet_compile is enabled!
2024-12-14 16:25:54.389274: 
2024-12-14 16:25:54.390316: Epoch 0
2024-12-14 16:25:54.391164: Current learning rate: 0.01
2024-12-14 16:29:45.788642: Validation loss improved from 1000.00000 to -0.20715! Patience: 0/50
2024-12-14 16:29:45.789766: train_loss -0.0717
2024-12-14 16:29:45.790557: val_loss -0.2071
2024-12-14 16:29:45.791330: Pseudo dice [0.5634]
2024-12-14 16:29:45.792081: Epoch time: 231.4 s
2024-12-14 16:29:45.792705: Yayy! New best EMA pseudo Dice: 0.5634
2024-12-14 16:29:47.924257: 
2024-12-14 16:29:47.925563: Epoch 1
2024-12-14 16:29:47.926436: Current learning rate: 0.00994
2024-12-14 16:32:20.865349: Validation loss improved from -0.20715 to -0.26544! Patience: 0/50
2024-12-14 16:32:20.866831: train_loss -0.2389
2024-12-14 16:32:20.868103: val_loss -0.2654
2024-12-14 16:32:20.869133: Pseudo dice [0.5907]
2024-12-14 16:32:20.870231: Epoch time: 152.94 s
2024-12-14 16:32:20.871132: Yayy! New best EMA pseudo Dice: 0.5662
2024-12-14 16:32:22.656746: 
2024-12-14 16:32:22.658202: Epoch 2
2024-12-14 16:32:22.659408: Current learning rate: 0.00988
2024-12-14 16:34:42.644610: Validation loss improved from -0.26544 to -0.28314! Patience: 0/50
2024-12-14 16:34:42.645664: train_loss -0.3008
2024-12-14 16:34:42.646625: val_loss -0.2831
2024-12-14 16:34:42.647513: Pseudo dice [0.5901]
2024-12-14 16:34:42.648405: Epoch time: 139.99 s
2024-12-14 16:34:42.649320: Yayy! New best EMA pseudo Dice: 0.5686
2024-12-14 16:34:44.538960: 
2024-12-14 16:34:44.540301: Epoch 3
2024-12-14 16:34:44.541107: Current learning rate: 0.00982
2024-12-14 16:37:52.348242: Validation loss improved from -0.28314 to -0.33921! Patience: 0/50
2024-12-14 16:37:52.349327: train_loss -0.3366
2024-12-14 16:37:52.350060: val_loss -0.3392
2024-12-14 16:37:52.350691: Pseudo dice [0.6346]
2024-12-14 16:37:52.351429: Epoch time: 187.81 s
2024-12-14 16:37:52.352056: Yayy! New best EMA pseudo Dice: 0.5752
2024-12-14 16:37:54.153663: 
2024-12-14 16:37:54.154997: Epoch 4
2024-12-14 16:37:54.155807: Current learning rate: 0.00976
2024-12-14 16:41:52.783295: Validation loss did not improve from -0.33921. Patience: 1/50
2024-12-14 16:41:52.784259: train_loss -0.3621
2024-12-14 16:41:52.785129: val_loss -0.3011
2024-12-14 16:41:52.785967: Pseudo dice [0.6098]
2024-12-14 16:41:52.786786: Epoch time: 238.63 s
2024-12-14 16:41:53.146542: Yayy! New best EMA pseudo Dice: 0.5786
2024-12-14 16:41:54.959135: 
2024-12-14 16:41:54.960010: Epoch 5
2024-12-14 16:41:54.960716: Current learning rate: 0.0097
2024-12-14 16:46:15.248687: Validation loss improved from -0.33921 to -0.35539! Patience: 1/50
2024-12-14 16:46:15.249676: train_loss -0.4035
2024-12-14 16:46:15.250618: val_loss -0.3554
2024-12-14 16:46:15.251296: Pseudo dice [0.6379]
2024-12-14 16:46:15.251960: Epoch time: 260.29 s
2024-12-14 16:46:15.252606: Yayy! New best EMA pseudo Dice: 0.5846
2024-12-14 16:46:17.023208: 
2024-12-14 16:46:17.024596: Epoch 6
2024-12-14 16:46:17.025512: Current learning rate: 0.00964
2024-12-14 16:50:04.675308: Validation loss did not improve from -0.35539. Patience: 1/50
2024-12-14 16:50:04.676244: train_loss -0.4082
2024-12-14 16:50:04.677030: val_loss -0.3535
2024-12-14 16:50:04.677761: Pseudo dice [0.6454]
2024-12-14 16:50:04.678447: Epoch time: 227.65 s
2024-12-14 16:50:04.679199: Yayy! New best EMA pseudo Dice: 0.5906
2024-12-14 16:50:06.437213: 
2024-12-14 16:50:06.438559: Epoch 7
2024-12-14 16:50:06.439332: Current learning rate: 0.00958
2024-12-14 16:54:30.695135: Validation loss improved from -0.35539 to -0.38838! Patience: 1/50
2024-12-14 16:54:30.696160: train_loss -0.4454
2024-12-14 16:54:30.696877: val_loss -0.3884
2024-12-14 16:54:30.697512: Pseudo dice [0.6566]
2024-12-14 16:54:30.698156: Epoch time: 264.26 s
2024-12-14 16:54:30.698834: Yayy! New best EMA pseudo Dice: 0.5972
2024-12-14 16:54:32.494122: 
2024-12-14 16:54:32.495272: Epoch 8
2024-12-14 16:54:32.495968: Current learning rate: 0.00952
2024-12-14 16:58:57.942536: Validation loss improved from -0.38838 to -0.39771! Patience: 0/50
2024-12-14 16:58:57.943413: train_loss -0.4712
2024-12-14 16:58:57.944174: val_loss -0.3977
2024-12-14 16:58:57.944773: Pseudo dice [0.6662]
2024-12-14 16:58:57.945434: Epoch time: 265.45 s
2024-12-14 16:58:57.946053: Yayy! New best EMA pseudo Dice: 0.6041
2024-12-14 16:59:00.067030: 
2024-12-14 16:59:00.068406: Epoch 9
2024-12-14 16:59:00.069584: Current learning rate: 0.00946
2024-12-14 17:02:47.718672: Validation loss did not improve from -0.39771. Patience: 1/50
2024-12-14 17:02:47.719702: train_loss -0.4853
2024-12-14 17:02:47.720770: val_loss -0.3258
2024-12-14 17:02:47.721701: Pseudo dice [0.6177]
2024-12-14 17:02:47.722620: Epoch time: 227.65 s
2024-12-14 17:02:48.127573: Yayy! New best EMA pseudo Dice: 0.6055
2024-12-14 17:02:49.828028: 
2024-12-14 17:02:49.829297: Epoch 10
2024-12-14 17:02:49.830044: Current learning rate: 0.0094
2024-12-14 17:07:12.026641: Validation loss improved from -0.39771 to -0.41095! Patience: 1/50
2024-12-14 17:07:12.027550: train_loss -0.4814
2024-12-14 17:07:12.028302: val_loss -0.4109
2024-12-14 17:07:12.028938: Pseudo dice [0.6798]
2024-12-14 17:07:12.029579: Epoch time: 262.2 s
2024-12-14 17:07:12.030187: Yayy! New best EMA pseudo Dice: 0.6129
2024-12-14 17:07:13.750461: 
2024-12-14 17:07:13.751910: Epoch 11
2024-12-14 17:07:13.752733: Current learning rate: 0.00934
2024-12-14 17:11:16.865507: Validation loss did not improve from -0.41095. Patience: 1/50
2024-12-14 17:11:16.866608: train_loss -0.498
2024-12-14 17:11:16.867603: val_loss -0.3924
2024-12-14 17:11:16.868532: Pseudo dice [0.6704]
2024-12-14 17:11:16.869470: Epoch time: 243.12 s
2024-12-14 17:11:16.870322: Yayy! New best EMA pseudo Dice: 0.6187
2024-12-14 17:11:18.580716: 
2024-12-14 17:11:18.582110: Epoch 12
2024-12-14 17:11:18.582881: Current learning rate: 0.00928
2024-12-14 17:15:48.061617: Validation loss improved from -0.41095 to -0.42910! Patience: 1/50
2024-12-14 17:15:48.062596: train_loss -0.5164
2024-12-14 17:15:48.063378: val_loss -0.4291
2024-12-14 17:15:48.064054: Pseudo dice [0.6801]
2024-12-14 17:15:48.064697: Epoch time: 269.48 s
2024-12-14 17:15:48.065489: Yayy! New best EMA pseudo Dice: 0.6248
2024-12-14 17:15:49.865667: 
2024-12-14 17:15:49.867051: Epoch 13
2024-12-14 17:15:49.867857: Current learning rate: 0.00922
2024-12-14 17:19:07.360387: Validation loss improved from -0.42910 to -0.44947! Patience: 0/50
2024-12-14 17:19:07.361280: train_loss -0.5335
2024-12-14 17:19:07.362422: val_loss -0.4495
2024-12-14 17:19:07.363400: Pseudo dice [0.6937]
2024-12-14 17:19:07.364392: Epoch time: 197.5 s
2024-12-14 17:19:07.365326: Yayy! New best EMA pseudo Dice: 0.6317
2024-12-14 17:19:09.176584: 
2024-12-14 17:19:09.178256: Epoch 14
2024-12-14 17:19:09.179466: Current learning rate: 0.00916
2024-12-14 17:23:01.883535: Validation loss did not improve from -0.44947. Patience: 1/50
2024-12-14 17:23:01.884623: train_loss -0.5442
2024-12-14 17:23:01.885411: val_loss -0.4066
2024-12-14 17:23:01.886201: Pseudo dice [0.6679]
2024-12-14 17:23:01.886853: Epoch time: 232.71 s
2024-12-14 17:23:02.299608: Yayy! New best EMA pseudo Dice: 0.6353
2024-12-14 17:23:04.115376: 
2024-12-14 17:23:04.116779: Epoch 15
2024-12-14 17:23:04.117629: Current learning rate: 0.0091
2024-12-14 17:27:22.901391: Validation loss did not improve from -0.44947. Patience: 2/50
2024-12-14 17:27:22.902160: train_loss -0.5548
2024-12-14 17:27:22.902859: val_loss -0.3909
2024-12-14 17:27:22.903558: Pseudo dice [0.66]
2024-12-14 17:27:22.904245: Epoch time: 258.79 s
2024-12-14 17:27:22.905166: Yayy! New best EMA pseudo Dice: 0.6378
2024-12-14 17:27:24.667394: 
2024-12-14 17:27:24.668670: Epoch 16
2024-12-14 17:27:24.669391: Current learning rate: 0.00903
2024-12-14 17:32:11.906632: Validation loss did not improve from -0.44947. Patience: 3/50
2024-12-14 17:32:11.921062: train_loss -0.5575
2024-12-14 17:32:11.922863: val_loss -0.4349
2024-12-14 17:32:11.923712: Pseudo dice [0.6865]
2024-12-14 17:32:11.924616: Epoch time: 287.25 s
2024-12-14 17:32:11.925339: Yayy! New best EMA pseudo Dice: 0.6427
2024-12-14 17:32:13.835736: 
2024-12-14 17:32:13.837293: Epoch 17
2024-12-14 17:32:13.838177: Current learning rate: 0.00897
2024-12-14 17:36:53.523284: Validation loss did not improve from -0.44947. Patience: 4/50
2024-12-14 17:36:53.527235: train_loss -0.5632
2024-12-14 17:36:53.528412: val_loss -0.3534
2024-12-14 17:36:53.529028: Pseudo dice [0.64]
2024-12-14 17:36:53.529796: Epoch time: 279.69 s
2024-12-14 17:36:54.947768: 
2024-12-14 17:36:54.949148: Epoch 18
2024-12-14 17:36:54.949959: Current learning rate: 0.00891
2024-12-14 17:41:43.566783: Validation loss improved from -0.44947 to -0.45016! Patience: 4/50
2024-12-14 17:41:43.567719: train_loss -0.5629
2024-12-14 17:41:43.568496: val_loss -0.4502
2024-12-14 17:41:43.569191: Pseudo dice [0.6958]
2024-12-14 17:41:43.569979: Epoch time: 288.62 s
2024-12-14 17:41:43.570799: Yayy! New best EMA pseudo Dice: 0.6477
2024-12-14 17:41:45.827589: 
2024-12-14 17:41:45.828919: Epoch 19
2024-12-14 17:41:45.829806: Current learning rate: 0.00885
2024-12-14 17:46:35.176019: Validation loss did not improve from -0.45016. Patience: 1/50
2024-12-14 17:46:35.176990: train_loss -0.5705
2024-12-14 17:46:35.177783: val_loss -0.4258
2024-12-14 17:46:35.178453: Pseudo dice [0.6868]
2024-12-14 17:46:35.179213: Epoch time: 289.35 s
2024-12-14 17:46:35.700640: Yayy! New best EMA pseudo Dice: 0.6516
2024-12-14 17:46:37.511043: 
2024-12-14 17:46:37.512284: Epoch 20
2024-12-14 17:46:37.513119: Current learning rate: 0.00879
2024-12-14 17:52:24.260360: Validation loss did not improve from -0.45016. Patience: 2/50
2024-12-14 17:52:24.261323: train_loss -0.5844
2024-12-14 17:52:24.262189: val_loss -0.4011
2024-12-14 17:52:24.262851: Pseudo dice [0.6665]
2024-12-14 17:52:24.263649: Epoch time: 346.75 s
2024-12-14 17:52:24.264380: Yayy! New best EMA pseudo Dice: 0.6531
2024-12-14 17:52:26.104204: 
2024-12-14 17:52:26.105542: Epoch 21
2024-12-14 17:52:26.106344: Current learning rate: 0.00873
2024-12-14 17:58:09.477958: Validation loss did not improve from -0.45016. Patience: 3/50
2024-12-14 17:58:09.478856: train_loss -0.5902
2024-12-14 17:58:09.479696: val_loss -0.4336
2024-12-14 17:58:09.480521: Pseudo dice [0.6864]
2024-12-14 17:58:09.481354: Epoch time: 343.38 s
2024-12-14 17:58:09.482046: Yayy! New best EMA pseudo Dice: 0.6564
2024-12-14 17:58:11.239286: 
2024-12-14 17:58:11.240547: Epoch 22
2024-12-14 17:58:11.241374: Current learning rate: 0.00867
2024-12-14 18:03:20.801375: Validation loss did not improve from -0.45016. Patience: 4/50
2024-12-14 18:03:20.802458: train_loss -0.5961
2024-12-14 18:03:20.803265: val_loss -0.4032
2024-12-14 18:03:20.803909: Pseudo dice [0.6754]
2024-12-14 18:03:20.804674: Epoch time: 309.56 s
2024-12-14 18:03:20.805422: Yayy! New best EMA pseudo Dice: 0.6583
2024-12-14 18:03:22.552029: 
2024-12-14 18:03:22.553211: Epoch 23
2024-12-14 18:03:22.553854: Current learning rate: 0.00861
2024-12-14 18:09:03.673907: Validation loss did not improve from -0.45016. Patience: 5/50
2024-12-14 18:09:03.674966: train_loss -0.6064
2024-12-14 18:09:03.675747: val_loss -0.3679
2024-12-14 18:09:03.676402: Pseudo dice [0.6298]
2024-12-14 18:09:03.677089: Epoch time: 341.12 s
2024-12-14 18:09:05.060401: 
2024-12-14 18:09:05.061645: Epoch 24
2024-12-14 18:09:05.062283: Current learning rate: 0.00855
2024-12-14 18:14:38.551466: Validation loss did not improve from -0.45016. Patience: 6/50
2024-12-14 18:14:38.552482: train_loss -0.6188
2024-12-14 18:14:38.553250: val_loss -0.4167
2024-12-14 18:14:38.553991: Pseudo dice [0.68]
2024-12-14 18:14:38.554577: Epoch time: 333.49 s
2024-12-14 18:14:40.400354: 
2024-12-14 18:14:40.401879: Epoch 25
2024-12-14 18:14:40.402560: Current learning rate: 0.00849
2024-12-14 18:20:07.208963: Validation loss did not improve from -0.45016. Patience: 7/50
2024-12-14 18:20:07.210147: train_loss -0.6107
2024-12-14 18:20:07.211144: val_loss -0.3387
2024-12-14 18:20:07.211947: Pseudo dice [0.6413]
2024-12-14 18:20:07.212922: Epoch time: 326.81 s
2024-12-14 18:20:08.726453: 
2024-12-14 18:20:08.727751: Epoch 26
2024-12-14 18:20:08.728605: Current learning rate: 0.00843
2024-12-14 18:24:56.056687: Validation loss did not improve from -0.45016. Patience: 8/50
2024-12-14 18:24:56.057615: train_loss -0.6114
2024-12-14 18:24:56.058545: val_loss -0.4414
2024-12-14 18:24:56.059512: Pseudo dice [0.6992]
2024-12-14 18:24:56.060296: Epoch time: 287.33 s
2024-12-14 18:24:56.061092: Yayy! New best EMA pseudo Dice: 0.6606
2024-12-14 18:24:57.858496: 
2024-12-14 18:24:57.859952: Epoch 27
2024-12-14 18:24:57.861050: Current learning rate: 0.00836
2024-12-14 18:30:45.772905: Validation loss improved from -0.45016 to -0.47055! Patience: 8/50
2024-12-14 18:30:45.774031: train_loss -0.6275
2024-12-14 18:30:45.774962: val_loss -0.4706
2024-12-14 18:30:45.775617: Pseudo dice [0.7077]
2024-12-14 18:30:45.776240: Epoch time: 347.92 s
2024-12-14 18:30:45.777034: Yayy! New best EMA pseudo Dice: 0.6653
2024-12-14 18:30:47.585531: 
2024-12-14 18:30:47.586911: Epoch 28
2024-12-14 18:30:47.587726: Current learning rate: 0.0083
2024-12-14 18:36:22.323570: Validation loss did not improve from -0.47055. Patience: 1/50
2024-12-14 18:36:22.324748: train_loss -0.625
2024-12-14 18:36:22.326750: val_loss -0.4061
2024-12-14 18:36:22.327939: Pseudo dice [0.6679]
2024-12-14 18:36:22.329288: Epoch time: 334.74 s
2024-12-14 18:36:22.330321: Yayy! New best EMA pseudo Dice: 0.6656
2024-12-14 18:36:24.523565: 
2024-12-14 18:36:24.524816: Epoch 29
2024-12-14 18:36:24.525801: Current learning rate: 0.00824
2024-12-14 18:41:40.863152: Validation loss did not improve from -0.47055. Patience: 2/50
2024-12-14 18:41:40.864189: train_loss -0.6334
2024-12-14 18:41:40.865214: val_loss -0.4079
2024-12-14 18:41:40.866041: Pseudo dice [0.6651]
2024-12-14 18:41:40.866948: Epoch time: 316.34 s
2024-12-14 18:41:42.645284: 
2024-12-14 18:41:42.646671: Epoch 30
2024-12-14 18:41:42.647692: Current learning rate: 0.00818
2024-12-14 18:47:06.140448: Validation loss did not improve from -0.47055. Patience: 3/50
2024-12-14 18:47:06.141474: train_loss -0.6355
2024-12-14 18:47:06.142468: val_loss -0.4119
2024-12-14 18:47:06.143361: Pseudo dice [0.6732]
2024-12-14 18:47:06.144106: Epoch time: 323.5 s
2024-12-14 18:47:06.144715: Yayy! New best EMA pseudo Dice: 0.6663
2024-12-14 18:47:07.935044: 
2024-12-14 18:47:07.936407: Epoch 31
2024-12-14 18:47:07.937096: Current learning rate: 0.00812
2024-12-14 18:52:50.754901: Validation loss did not improve from -0.47055. Patience: 4/50
2024-12-14 18:52:50.755830: train_loss -0.6485
2024-12-14 18:52:50.756734: val_loss -0.4241
2024-12-14 18:52:50.757568: Pseudo dice [0.6855]
2024-12-14 18:52:50.758482: Epoch time: 342.82 s
2024-12-14 18:52:50.759315: Yayy! New best EMA pseudo Dice: 0.6682
2024-12-14 18:52:52.533902: 
2024-12-14 18:52:52.535145: Epoch 32
2024-12-14 18:52:52.536124: Current learning rate: 0.00806
2024-12-14 18:58:36.986008: Validation loss did not improve from -0.47055. Patience: 5/50
2024-12-14 18:58:36.987157: train_loss -0.658
2024-12-14 18:58:36.988117: val_loss -0.4149
2024-12-14 18:58:36.988985: Pseudo dice [0.6822]
2024-12-14 18:58:36.989750: Epoch time: 344.45 s
2024-12-14 18:58:36.990473: Yayy! New best EMA pseudo Dice: 0.6696
2024-12-14 18:58:38.784957: 
2024-12-14 18:58:38.786291: Epoch 33
2024-12-14 18:58:38.787045: Current learning rate: 0.008
2024-12-14 19:05:00.984534: Validation loss did not improve from -0.47055. Patience: 6/50
2024-12-14 19:05:00.985543: train_loss -0.6592
2024-12-14 19:05:00.986634: val_loss -0.4165
2024-12-14 19:05:00.987660: Pseudo dice [0.6855]
2024-12-14 19:05:00.988662: Epoch time: 382.2 s
2024-12-14 19:05:00.989485: Yayy! New best EMA pseudo Dice: 0.6712
2024-12-14 19:05:02.803224: 
2024-12-14 19:05:02.804702: Epoch 34
2024-12-14 19:05:02.805730: Current learning rate: 0.00793
2024-12-14 19:11:56.815685: Validation loss did not improve from -0.47055. Patience: 7/50
2024-12-14 19:11:56.816635: train_loss -0.6575
2024-12-14 19:11:56.818026: val_loss -0.4591
2024-12-14 19:11:56.819066: Pseudo dice [0.7089]
2024-12-14 19:11:56.820126: Epoch time: 414.01 s
2024-12-14 19:11:57.213492: Yayy! New best EMA pseudo Dice: 0.675
2024-12-14 19:11:59.025186: 
2024-12-14 19:11:59.026601: Epoch 35
2024-12-14 19:11:59.027575: Current learning rate: 0.00787
2024-12-14 19:19:03.647205: Validation loss did not improve from -0.47055. Patience: 8/50
2024-12-14 19:19:03.648202: train_loss -0.6643
2024-12-14 19:19:03.649008: val_loss -0.4702
2024-12-14 19:19:03.649741: Pseudo dice [0.7096]
2024-12-14 19:19:03.650429: Epoch time: 424.62 s
2024-12-14 19:19:03.651106: Yayy! New best EMA pseudo Dice: 0.6784
2024-12-14 19:19:05.482379: 
2024-12-14 19:19:05.483742: Epoch 36
2024-12-14 19:19:05.484590: Current learning rate: 0.00781
2024-12-14 19:25:48.408072: Validation loss improved from -0.47055 to -0.48766! Patience: 8/50
2024-12-14 19:25:48.409188: train_loss -0.6521
2024-12-14 19:25:48.409941: val_loss -0.4877
2024-12-14 19:25:48.410694: Pseudo dice [0.7278]
2024-12-14 19:25:48.411334: Epoch time: 402.93 s
2024-12-14 19:25:48.411959: Yayy! New best EMA pseudo Dice: 0.6834
2024-12-14 19:25:50.208165: 
2024-12-14 19:25:50.209279: Epoch 37
2024-12-14 19:25:50.210008: Current learning rate: 0.00775
2024-12-14 19:32:36.944791: Validation loss did not improve from -0.48766. Patience: 1/50
2024-12-14 19:32:36.945740: train_loss -0.6691
2024-12-14 19:32:36.946554: val_loss -0.4612
2024-12-14 19:32:36.947340: Pseudo dice [0.7126]
2024-12-14 19:32:36.948015: Epoch time: 406.74 s
2024-12-14 19:32:36.948646: Yayy! New best EMA pseudo Dice: 0.6863
2024-12-14 19:32:38.765441: 
2024-12-14 19:32:38.766867: Epoch 38
2024-12-14 19:32:38.767671: Current learning rate: 0.00769
2024-12-14 19:39:22.249276: Validation loss did not improve from -0.48766. Patience: 2/50
2024-12-14 19:39:22.250295: train_loss -0.6746
2024-12-14 19:39:22.251199: val_loss -0.487
2024-12-14 19:39:22.252016: Pseudo dice [0.7229]
2024-12-14 19:39:22.252794: Epoch time: 403.49 s
2024-12-14 19:39:22.253597: Yayy! New best EMA pseudo Dice: 0.6899
2024-12-14 19:39:24.097986: 
2024-12-14 19:39:24.099543: Epoch 39
2024-12-14 19:39:24.100500: Current learning rate: 0.00763
2024-12-14 19:46:07.551237: Validation loss did not improve from -0.48766. Patience: 3/50
2024-12-14 19:46:07.555304: train_loss -0.6719
2024-12-14 19:46:07.557535: val_loss -0.4053
2024-12-14 19:46:07.558489: Pseudo dice [0.6827]
2024-12-14 19:46:07.559472: Epoch time: 403.46 s
2024-12-14 19:46:09.904531: 
2024-12-14 19:46:09.905975: Epoch 40
2024-12-14 19:46:09.906889: Current learning rate: 0.00756
2024-12-14 19:53:17.934973: Validation loss did not improve from -0.48766. Patience: 4/50
2024-12-14 19:53:17.936178: train_loss -0.6817
2024-12-14 19:53:17.937149: val_loss -0.4832
2024-12-14 19:53:17.938067: Pseudo dice [0.7273]
2024-12-14 19:53:17.939001: Epoch time: 428.03 s
2024-12-14 19:53:17.939923: Yayy! New best EMA pseudo Dice: 0.693
2024-12-14 19:53:19.761556: 
2024-12-14 19:53:19.763094: Epoch 41
2024-12-14 19:53:19.764131: Current learning rate: 0.0075
2024-12-14 20:00:11.250095: Validation loss did not improve from -0.48766. Patience: 5/50
2024-12-14 20:00:11.251217: train_loss -0.6834
2024-12-14 20:00:11.252086: val_loss -0.4294
2024-12-14 20:00:11.252866: Pseudo dice [0.6968]
2024-12-14 20:00:11.253602: Epoch time: 411.49 s
2024-12-14 20:00:11.254269: Yayy! New best EMA pseudo Dice: 0.6934
2024-12-14 20:00:13.058656: 
2024-12-14 20:00:13.059883: Epoch 42
2024-12-14 20:00:13.060575: Current learning rate: 0.00744
2024-12-14 20:07:15.427204: Validation loss did not improve from -0.48766. Patience: 6/50
2024-12-14 20:07:15.428420: train_loss -0.6801
2024-12-14 20:07:15.429401: val_loss -0.4261
2024-12-14 20:07:15.430314: Pseudo dice [0.689]
2024-12-14 20:07:15.431165: Epoch time: 422.37 s
2024-12-14 20:07:16.865220: 
2024-12-14 20:07:16.866987: Epoch 43
2024-12-14 20:07:16.867849: Current learning rate: 0.00738
2024-12-14 20:13:59.604105: Validation loss did not improve from -0.48766. Patience: 7/50
2024-12-14 20:13:59.605013: train_loss -0.6859
2024-12-14 20:13:59.605726: val_loss -0.4467
2024-12-14 20:13:59.606395: Pseudo dice [0.7046]
2024-12-14 20:13:59.607224: Epoch time: 402.74 s
2024-12-14 20:13:59.608011: Yayy! New best EMA pseudo Dice: 0.6941
2024-12-14 20:14:01.350312: 
2024-12-14 20:14:01.351829: Epoch 44
2024-12-14 20:14:01.352600: Current learning rate: 0.00732
2024-12-14 20:21:00.528525: Validation loss did not improve from -0.48766. Patience: 8/50
2024-12-14 20:21:00.529537: train_loss -0.6918
2024-12-14 20:21:00.530411: val_loss -0.3781
2024-12-14 20:21:00.531185: Pseudo dice [0.6632]
2024-12-14 20:21:00.531890: Epoch time: 419.18 s
2024-12-14 20:21:02.262564: 
2024-12-14 20:21:02.263743: Epoch 45
2024-12-14 20:21:02.264547: Current learning rate: 0.00725
2024-12-14 20:28:00.014242: Validation loss did not improve from -0.48766. Patience: 9/50
2024-12-14 20:28:00.015240: train_loss -0.6951
2024-12-14 20:28:00.016045: val_loss -0.4408
2024-12-14 20:28:00.016741: Pseudo dice [0.6884]
2024-12-14 20:28:00.017430: Epoch time: 417.75 s
2024-12-14 20:28:01.377814: 
2024-12-14 20:28:01.379180: Epoch 46
2024-12-14 20:28:01.379926: Current learning rate: 0.00719
2024-12-14 20:34:49.226247: Validation loss did not improve from -0.48766. Patience: 10/50
2024-12-14 20:34:49.227128: train_loss -0.6998
2024-12-14 20:34:49.227991: val_loss -0.4444
2024-12-14 20:34:49.228749: Pseudo dice [0.7126]
2024-12-14 20:34:49.229583: Epoch time: 407.85 s
2024-12-14 20:34:50.603891: 
2024-12-14 20:34:50.605074: Epoch 47
2024-12-14 20:34:50.605911: Current learning rate: 0.00713
2024-12-14 20:41:49.694593: Validation loss did not improve from -0.48766. Patience: 11/50
2024-12-14 20:41:49.695496: train_loss -0.7045
2024-12-14 20:41:49.696338: val_loss -0.3961
2024-12-14 20:41:49.697031: Pseudo dice [0.6864]
2024-12-14 20:41:49.697813: Epoch time: 419.09 s
2024-12-14 20:41:51.061247: 
2024-12-14 20:41:51.062361: Epoch 48
2024-12-14 20:41:51.063051: Current learning rate: 0.00707
2024-12-14 20:48:39.138632: Validation loss did not improve from -0.48766. Patience: 12/50
2024-12-14 20:48:39.140710: train_loss -0.7058
2024-12-14 20:48:39.142330: val_loss -0.3579
2024-12-14 20:48:39.143330: Pseudo dice [0.6569]
2024-12-14 20:48:39.144283: Epoch time: 408.08 s
2024-12-14 20:48:40.582248: 
2024-12-14 20:48:40.583517: Epoch 49
2024-12-14 20:48:40.584261: Current learning rate: 0.007
2024-12-14 20:55:34.725127: Validation loss did not improve from -0.48766. Patience: 13/50
2024-12-14 20:55:34.726144: train_loss -0.701
2024-12-14 20:55:34.726842: val_loss -0.4014
2024-12-14 20:55:34.727610: Pseudo dice [0.6746]
2024-12-14 20:55:34.728215: Epoch time: 414.15 s
2024-12-14 20:55:37.031987: 
2024-12-14 20:55:37.033263: Epoch 50
2024-12-14 20:55:37.034041: Current learning rate: 0.00694
2024-12-14 21:02:33.294471: Validation loss did not improve from -0.48766. Patience: 14/50
2024-12-14 21:02:33.295389: train_loss -0.7107
2024-12-14 21:02:33.296257: val_loss -0.4425
2024-12-14 21:02:33.297189: Pseudo dice [0.6953]
2024-12-14 21:02:33.297988: Epoch time: 416.26 s
2024-12-14 21:02:34.669155: 
2024-12-14 21:02:34.670624: Epoch 51
2024-12-14 21:02:34.671678: Current learning rate: 0.00688
2024-12-14 21:09:38.515199: Validation loss did not improve from -0.48766. Patience: 15/50
2024-12-14 21:09:38.516225: train_loss -0.703
2024-12-14 21:09:38.516998: val_loss -0.4372
2024-12-14 21:09:38.517773: Pseudo dice [0.7006]
2024-12-14 21:09:38.518458: Epoch time: 423.85 s
2024-12-14 21:09:39.974883: 
2024-12-14 21:09:39.976182: Epoch 52
2024-12-14 21:09:39.976917: Current learning rate: 0.00682
2024-12-14 21:16:26.596020: Validation loss did not improve from -0.48766. Patience: 16/50
2024-12-14 21:16:26.597024: train_loss -0.7187
2024-12-14 21:16:26.598048: val_loss -0.3714
2024-12-14 21:16:26.599041: Pseudo dice [0.6623]
2024-12-14 21:16:26.600082: Epoch time: 406.62 s
2024-12-14 21:16:28.042594: 
2024-12-14 21:16:28.044173: Epoch 53
2024-12-14 21:16:28.045174: Current learning rate: 0.00675
2024-12-14 21:23:31.856381: Validation loss did not improve from -0.48766. Patience: 17/50
2024-12-14 21:23:31.857352: train_loss -0.7142
2024-12-14 21:23:31.858346: val_loss -0.4394
2024-12-14 21:23:31.859410: Pseudo dice [0.6944]
2024-12-14 21:23:31.860444: Epoch time: 423.82 s
2024-12-14 21:23:33.262847: 
2024-12-14 21:23:33.264436: Epoch 54
2024-12-14 21:23:33.265530: Current learning rate: 0.00669
2024-12-14 21:30:23.062294: Validation loss did not improve from -0.48766. Patience: 18/50
2024-12-14 21:30:23.063306: train_loss -0.7213
2024-12-14 21:30:23.064204: val_loss -0.3185
2024-12-14 21:30:23.064867: Pseudo dice [0.6479]
2024-12-14 21:30:23.065582: Epoch time: 409.8 s
2024-12-14 21:30:24.950918: 
2024-12-14 21:30:24.952338: Epoch 55
2024-12-14 21:30:24.953021: Current learning rate: 0.00663
2024-12-14 21:37:20.534929: Validation loss did not improve from -0.48766. Patience: 19/50
2024-12-14 21:37:20.535877: train_loss -0.7221
2024-12-14 21:37:20.536677: val_loss -0.4441
2024-12-14 21:37:20.537334: Pseudo dice [0.7041]
2024-12-14 21:37:20.538138: Epoch time: 415.59 s
2024-12-14 21:37:21.931521: 
2024-12-14 21:37:21.932789: Epoch 56
2024-12-14 21:37:21.933540: Current learning rate: 0.00657
2024-12-14 21:44:28.127816: Validation loss did not improve from -0.48766. Patience: 20/50
2024-12-14 21:44:28.128681: train_loss -0.723
2024-12-14 21:44:28.129445: val_loss -0.4582
2024-12-14 21:44:28.130197: Pseudo dice [0.7141]
2024-12-14 21:44:28.130860: Epoch time: 426.2 s
2024-12-14 21:44:29.517264: 
2024-12-14 21:44:29.518663: Epoch 57
2024-12-14 21:44:29.519456: Current learning rate: 0.0065
2024-12-14 21:51:15.362817: Validation loss did not improve from -0.48766. Patience: 21/50
2024-12-14 21:51:15.364764: train_loss -0.7255
2024-12-14 21:51:15.366449: val_loss -0.4
2024-12-14 21:51:15.367295: Pseudo dice [0.6769]
2024-12-14 21:51:15.368189: Epoch time: 405.85 s
2024-12-14 21:51:16.792563: 
2024-12-14 21:51:16.793710: Epoch 58
2024-12-14 21:51:16.794461: Current learning rate: 0.00644
2024-12-14 21:57:57.138346: Validation loss did not improve from -0.48766. Patience: 22/50
2024-12-14 21:57:57.140408: train_loss -0.7287
2024-12-14 21:57:57.141961: val_loss -0.4207
2024-12-14 21:57:57.142680: Pseudo dice [0.7079]
2024-12-14 21:57:57.143514: Epoch time: 400.35 s
2024-12-14 21:57:58.574157: 
2024-12-14 21:57:58.575453: Epoch 59
2024-12-14 21:57:58.576396: Current learning rate: 0.00638
2024-12-14 22:04:54.546350: Validation loss did not improve from -0.48766. Patience: 23/50
2024-12-14 22:04:54.547317: train_loss -0.7283
2024-12-14 22:04:54.548364: val_loss -0.3801
2024-12-14 22:04:54.549355: Pseudo dice [0.672]
2024-12-14 22:04:54.550276: Epoch time: 415.97 s
2024-12-14 22:04:56.349888: 
2024-12-14 22:04:56.351331: Epoch 60
2024-12-14 22:04:56.352390: Current learning rate: 0.00631
2024-12-14 22:11:59.015837: Validation loss did not improve from -0.48766. Patience: 24/50
2024-12-14 22:11:59.016802: train_loss -0.7384
2024-12-14 22:11:59.017667: val_loss -0.4453
2024-12-14 22:11:59.018437: Pseudo dice [0.712]
2024-12-14 22:11:59.019090: Epoch time: 422.67 s
2024-12-14 22:12:01.001438: 
2024-12-14 22:12:01.002865: Epoch 61
2024-12-14 22:12:01.003891: Current learning rate: 0.00625
2024-12-14 22:19:03.302018: Validation loss did not improve from -0.48766. Patience: 25/50
2024-12-14 22:19:03.302826: train_loss -0.7371
2024-12-14 22:19:03.303590: val_loss -0.437
2024-12-14 22:19:03.304265: Pseudo dice [0.7091]
2024-12-14 22:19:03.304878: Epoch time: 422.3 s
2024-12-14 22:19:04.725192: 
2024-12-14 22:19:04.726475: Epoch 62
2024-12-14 22:19:04.727425: Current learning rate: 0.00619
2024-12-14 22:25:49.037655: Validation loss did not improve from -0.48766. Patience: 26/50
2024-12-14 22:25:49.038316: train_loss -0.7355
2024-12-14 22:25:49.039172: val_loss -0.4352
2024-12-14 22:25:49.039932: Pseudo dice [0.6957]
2024-12-14 22:25:49.040562: Epoch time: 404.31 s
2024-12-14 22:25:50.449670: 
2024-12-14 22:25:50.450918: Epoch 63
2024-12-14 22:25:50.451671: Current learning rate: 0.00612
2024-12-14 22:32:37.468381: Validation loss did not improve from -0.48766. Patience: 27/50
2024-12-14 22:32:37.469322: train_loss -0.7316
2024-12-14 22:32:37.470077: val_loss -0.4649
2024-12-14 22:32:37.470719: Pseudo dice [0.7176]
2024-12-14 22:32:37.471511: Epoch time: 407.02 s
2024-12-14 22:32:37.472531: Yayy! New best EMA pseudo Dice: 0.6948
2024-12-14 22:32:39.290351: 
2024-12-14 22:32:39.291633: Epoch 64
2024-12-14 22:32:39.292533: Current learning rate: 0.00606
2024-12-14 22:39:50.415759: Validation loss did not improve from -0.48766. Patience: 28/50
2024-12-14 22:39:50.416906: train_loss -0.7367
2024-12-14 22:39:50.417916: val_loss -0.4049
2024-12-14 22:39:50.418756: Pseudo dice [0.6923]
2024-12-14 22:39:50.419604: Epoch time: 431.13 s
2024-12-14 22:39:52.337047: 
2024-12-14 22:39:52.338385: Epoch 65
2024-12-14 22:39:52.339274: Current learning rate: 0.006
2024-12-14 22:46:34.006469: Validation loss did not improve from -0.48766. Patience: 29/50
2024-12-14 22:46:34.009099: train_loss -0.7367
2024-12-14 22:46:34.009977: val_loss -0.4348
2024-12-14 22:46:34.010841: Pseudo dice [0.6906]
2024-12-14 22:46:34.011662: Epoch time: 401.67 s
2024-12-14 22:46:35.457676: 
2024-12-14 22:46:35.459286: Epoch 66
2024-12-14 22:46:35.460312: Current learning rate: 0.00593
2024-12-14 22:53:22.562486: Validation loss did not improve from -0.48766. Patience: 30/50
2024-12-14 22:53:22.563616: train_loss -0.7418
2024-12-14 22:53:22.564324: val_loss -0.4239
2024-12-14 22:53:22.564951: Pseudo dice [0.6919]
2024-12-14 22:53:22.565577: Epoch time: 407.11 s
2024-12-14 22:53:23.993049: 
2024-12-14 22:53:23.994591: Epoch 67
2024-12-14 22:53:23.995490: Current learning rate: 0.00587
2024-12-14 22:59:58.309515: Validation loss did not improve from -0.48766. Patience: 31/50
2024-12-14 22:59:58.311836: train_loss -0.7473
2024-12-14 22:59:58.312779: val_loss -0.4305
2024-12-14 22:59:58.313453: Pseudo dice [0.6925]
2024-12-14 22:59:58.314140: Epoch time: 394.32 s
2024-12-14 22:59:59.743972: 
2024-12-14 22:59:59.745324: Epoch 68
2024-12-14 22:59:59.746025: Current learning rate: 0.00581
2024-12-14 23:07:08.899082: Validation loss did not improve from -0.48766. Patience: 32/50
2024-12-14 23:07:08.900213: train_loss -0.741
2024-12-14 23:07:08.901011: val_loss -0.3966
2024-12-14 23:07:08.901711: Pseudo dice [0.6885]
2024-12-14 23:07:08.902404: Epoch time: 429.16 s
2024-12-14 23:07:10.338870: 
2024-12-14 23:07:10.340189: Epoch 69
2024-12-14 23:07:10.340954: Current learning rate: 0.00574
2024-12-14 23:14:11.637012: Validation loss did not improve from -0.48766. Patience: 33/50
2024-12-14 23:14:11.637934: train_loss -0.7496
2024-12-14 23:14:11.638645: val_loss -0.4228
2024-12-14 23:14:11.639346: Pseudo dice [0.7006]
2024-12-14 23:14:11.640115: Epoch time: 421.3 s
2024-12-14 23:14:13.418995: 
2024-12-14 23:14:13.420012: Epoch 70
2024-12-14 23:14:13.420696: Current learning rate: 0.00568
2024-12-14 23:21:19.921951: Validation loss did not improve from -0.48766. Patience: 34/50
2024-12-14 23:21:19.922803: train_loss -0.7502
2024-12-14 23:21:19.923679: val_loss -0.4028
2024-12-14 23:21:19.924420: Pseudo dice [0.6917]
2024-12-14 23:21:19.925118: Epoch time: 426.5 s
2024-12-14 23:21:21.774990: 
2024-12-14 23:21:21.776080: Epoch 71
2024-12-14 23:21:21.776781: Current learning rate: 0.00562
2024-12-14 23:28:10.907800: Validation loss did not improve from -0.48766. Patience: 35/50
2024-12-14 23:28:10.908553: train_loss -0.7523
2024-12-14 23:28:10.909275: val_loss -0.4026
2024-12-14 23:28:10.909950: Pseudo dice [0.6867]
2024-12-14 23:28:10.910668: Epoch time: 409.13 s
2024-12-14 23:28:12.423000: 
2024-12-14 23:28:12.424176: Epoch 72
2024-12-14 23:28:12.425065: Current learning rate: 0.00555
2024-12-14 23:35:34.154497: Validation loss did not improve from -0.48766. Patience: 36/50
2024-12-14 23:35:34.155330: train_loss -0.7548
2024-12-14 23:35:34.156245: val_loss -0.4418
2024-12-14 23:35:34.156913: Pseudo dice [0.6947]
2024-12-14 23:35:34.157551: Epoch time: 441.73 s
2024-12-14 23:35:35.576150: 
2024-12-14 23:35:35.577468: Epoch 73
2024-12-14 23:35:35.578319: Current learning rate: 0.00549
2024-12-14 23:43:10.646757: Validation loss did not improve from -0.48766. Patience: 37/50
2024-12-14 23:43:10.647623: train_loss -0.7563
2024-12-14 23:43:10.648355: val_loss -0.4102
2024-12-14 23:43:10.649008: Pseudo dice [0.6933]
2024-12-14 23:43:10.649704: Epoch time: 455.07 s
2024-12-14 23:43:12.076989: 
2024-12-14 23:43:12.077983: Epoch 74
2024-12-14 23:43:12.078882: Current learning rate: 0.00542
2024-12-14 23:50:30.580816: Validation loss did not improve from -0.48766. Patience: 38/50
2024-12-14 23:50:30.581560: train_loss -0.7576
2024-12-14 23:50:30.582603: val_loss -0.4402
2024-12-14 23:50:30.583532: Pseudo dice [0.7207]
2024-12-14 23:50:30.584399: Epoch time: 438.51 s
2024-12-14 23:50:30.989324: Yayy! New best EMA pseudo Dice: 0.696
2024-12-14 23:50:32.819926: 
2024-12-14 23:50:32.821326: Epoch 75
2024-12-14 23:50:32.822273: Current learning rate: 0.00536
2024-12-14 23:57:57.531658: Validation loss did not improve from -0.48766. Patience: 39/50
2024-12-14 23:57:57.532483: train_loss -0.758
2024-12-14 23:57:57.533592: val_loss -0.4386
2024-12-14 23:57:57.534425: Pseudo dice [0.7044]
2024-12-14 23:57:57.535071: Epoch time: 444.71 s
2024-12-14 23:57:57.535909: Yayy! New best EMA pseudo Dice: 0.6968
2024-12-14 23:57:59.343369: 
2024-12-14 23:57:59.344256: Epoch 76
2024-12-14 23:57:59.344938: Current learning rate: 0.00529
2024-12-15 00:05:12.543755: Validation loss did not improve from -0.48766. Patience: 40/50
2024-12-15 00:05:12.545309: train_loss -0.7577
2024-12-15 00:05:12.546095: val_loss -0.3547
2024-12-15 00:05:12.546813: Pseudo dice [0.6714]
2024-12-15 00:05:12.547558: Epoch time: 433.2 s
2024-12-15 00:05:14.001226: 
2024-12-15 00:05:14.002081: Epoch 77
2024-12-15 00:05:14.002818: Current learning rate: 0.00523
2024-12-15 00:12:48.957307: Validation loss did not improve from -0.48766. Patience: 41/50
2024-12-15 00:12:48.958716: train_loss -0.758
2024-12-15 00:12:48.959794: val_loss -0.4319
2024-12-15 00:12:48.960440: Pseudo dice [0.7025]
2024-12-15 00:12:48.961173: Epoch time: 454.96 s
2024-12-15 00:12:50.492810: 
2024-12-15 00:12:50.493578: Epoch 78
2024-12-15 00:12:50.494277: Current learning rate: 0.00517
2024-12-15 00:20:01.008156: Validation loss did not improve from -0.48766. Patience: 42/50
2024-12-15 00:20:01.009228: train_loss -0.7584
2024-12-15 00:20:01.009948: val_loss -0.4561
2024-12-15 00:20:01.010621: Pseudo dice [0.7148]
2024-12-15 00:20:01.011359: Epoch time: 430.52 s
2024-12-15 00:20:01.011931: Yayy! New best EMA pseudo Dice: 0.6971
2024-12-15 00:20:02.804369: 
2024-12-15 00:20:02.805508: Epoch 79
2024-12-15 00:20:02.806254: Current learning rate: 0.0051
2024-12-15 00:27:01.392367: Validation loss did not improve from -0.48766. Patience: 43/50
2024-12-15 00:27:01.393037: train_loss -0.7583
2024-12-15 00:27:01.393840: val_loss -0.395
2024-12-15 00:27:01.394516: Pseudo dice [0.7004]
2024-12-15 00:27:01.395245: Epoch time: 418.59 s
2024-12-15 00:27:01.785119: Yayy! New best EMA pseudo Dice: 0.6974
2024-12-15 00:27:03.620140: 
2024-12-15 00:27:03.621125: Epoch 80
2024-12-15 00:27:03.621769: Current learning rate: 0.00504
2024-12-15 00:33:57.365371: Validation loss did not improve from -0.48766. Patience: 44/50
2024-12-15 00:33:57.366138: train_loss -0.7606
2024-12-15 00:33:57.367021: val_loss -0.4469
2024-12-15 00:33:57.367765: Pseudo dice [0.712]
2024-12-15 00:33:57.368537: Epoch time: 413.75 s
2024-12-15 00:33:57.369296: Yayy! New best EMA pseudo Dice: 0.6989
2024-12-15 00:33:59.204824: 
2024-12-15 00:33:59.205749: Epoch 81
2024-12-15 00:33:59.206552: Current learning rate: 0.00497
2024-12-15 00:41:13.253822: Validation loss did not improve from -0.48766. Patience: 45/50
2024-12-15 00:41:13.254558: train_loss -0.7615
2024-12-15 00:41:13.255435: val_loss -0.429
2024-12-15 00:41:13.256196: Pseudo dice [0.7042]
2024-12-15 00:41:13.256798: Epoch time: 434.05 s
2024-12-15 00:41:13.257439: Yayy! New best EMA pseudo Dice: 0.6994
2024-12-15 00:41:15.466948: 
2024-12-15 00:41:15.467804: Epoch 82
2024-12-15 00:41:15.468587: Current learning rate: 0.00491
2024-12-15 00:48:06.877705: Validation loss did not improve from -0.48766. Patience: 46/50
2024-12-15 00:48:06.878384: train_loss -0.7658
2024-12-15 00:48:06.879161: val_loss -0.3713
2024-12-15 00:48:06.879917: Pseudo dice [0.6872]
2024-12-15 00:48:06.880647: Epoch time: 411.41 s
2024-12-15 00:48:08.237338: 
2024-12-15 00:48:08.238436: Epoch 83
2024-12-15 00:48:08.239440: Current learning rate: 0.00484
2024-12-15 00:54:58.013386: Validation loss did not improve from -0.48766. Patience: 47/50
2024-12-15 00:54:58.014190: train_loss -0.7644
2024-12-15 00:54:58.015046: val_loss -0.4676
2024-12-15 00:54:58.015819: Pseudo dice [0.7085]
2024-12-15 00:54:58.016658: Epoch time: 409.78 s
2024-12-15 00:54:59.367621: 
2024-12-15 00:54:59.368673: Epoch 84
2024-12-15 00:54:59.369556: Current learning rate: 0.00478
2024-12-15 01:01:50.877093: Validation loss did not improve from -0.48766. Patience: 48/50
2024-12-15 01:01:50.877908: train_loss -0.7692
2024-12-15 01:01:50.878802: val_loss -0.4308
2024-12-15 01:01:50.879601: Pseudo dice [0.7033]
2024-12-15 01:01:50.880402: Epoch time: 411.51 s
2024-12-15 01:01:51.329540: Yayy! New best EMA pseudo Dice: 0.6996
2024-12-15 01:01:53.082422: 
2024-12-15 01:01:53.083622: Epoch 85
2024-12-15 01:01:53.084581: Current learning rate: 0.00471
2024-12-15 01:08:51.620304: Validation loss did not improve from -0.48766. Patience: 49/50
2024-12-15 01:08:51.621699: train_loss -0.7639
2024-12-15 01:08:51.623214: val_loss -0.3451
2024-12-15 01:08:51.623947: Pseudo dice [0.6678]
2024-12-15 01:08:51.625179: Epoch time: 418.54 s
2024-12-15 01:08:53.064569: 
2024-12-15 01:08:53.065569: Epoch 86
2024-12-15 01:08:53.066347: Current learning rate: 0.00465
2024-12-15 01:15:37.654112: Validation loss did not improve from -0.48766. Patience: 50/50
2024-12-15 01:15:37.655974: train_loss -0.7677
2024-12-15 01:15:37.656839: val_loss -0.4225
2024-12-15 01:15:37.657475: Pseudo dice [0.7085]
2024-12-15 01:15:37.658239: Epoch time: 404.59 s
2024-12-15 01:15:39.024750: 
2024-12-15 01:15:39.025779: Epoch 87
2024-12-15 01:15:39.026402: Current learning rate: 0.00458
2024-12-15 01:22:31.909316: Validation loss did not improve from -0.48766. Patience: 51/50
2024-12-15 01:22:31.910356: train_loss -0.7699
2024-12-15 01:22:31.911308: val_loss -0.4
2024-12-15 01:22:31.912079: Pseudo dice [0.7005]
2024-12-15 01:22:31.912867: Epoch time: 412.89 s
2024-12-15 01:22:33.274602: 
2024-12-15 01:22:33.276131: Epoch 88
2024-12-15 01:22:33.277052: Current learning rate: 0.00452
2024-12-15 01:29:34.704447: Validation loss did not improve from -0.48766. Patience: 52/50
2024-12-15 01:29:34.705171: train_loss -0.7752
2024-12-15 01:29:34.705947: val_loss -0.3963
2024-12-15 01:29:34.706739: Pseudo dice [0.6919]
2024-12-15 01:29:34.707387: Epoch time: 421.43 s
2024-12-15 01:29:36.142956: 
2024-12-15 01:29:36.143952: Epoch 89
2024-12-15 01:29:36.144867: Current learning rate: 0.00445
2024-12-15 01:36:47.163797: Validation loss did not improve from -0.48766. Patience: 53/50
2024-12-15 01:36:47.164773: train_loss -0.777
2024-12-15 01:36:47.165536: val_loss -0.4104
2024-12-15 01:36:47.166226: Pseudo dice [0.7011]
2024-12-15 01:36:47.166904: Epoch time: 431.02 s
2024-12-15 01:36:48.871224: 
2024-12-15 01:36:48.872481: Epoch 90
2024-12-15 01:36:48.873337: Current learning rate: 0.00438
2024-12-15 01:43:45.293411: Validation loss did not improve from -0.48766. Patience: 54/50
2024-12-15 01:43:45.294132: train_loss -0.7756
2024-12-15 01:43:45.294910: val_loss -0.4429
2024-12-15 01:43:45.295664: Pseudo dice [0.7098]
2024-12-15 01:43:45.296455: Epoch time: 416.42 s
2024-12-15 01:43:46.635419: 
2024-12-15 01:43:46.636586: Epoch 91
2024-12-15 01:43:46.637574: Current learning rate: 0.00432
2024-12-15 01:50:42.813898: Validation loss did not improve from -0.48766. Patience: 55/50
2024-12-15 01:50:42.814647: train_loss -0.7759
2024-12-15 01:50:42.815413: val_loss -0.4255
2024-12-15 01:50:42.816338: Pseudo dice [0.7026]
2024-12-15 01:50:42.817021: Epoch time: 416.18 s
2024-12-15 01:50:44.207967: 
2024-12-15 01:50:44.209175: Epoch 92
2024-12-15 01:50:44.209889: Current learning rate: 0.00425
2024-12-15 01:57:50.007891: Validation loss did not improve from -0.48766. Patience: 56/50
2024-12-15 01:57:50.008597: train_loss -0.7767
2024-12-15 01:57:50.009318: val_loss -0.4605
2024-12-15 01:57:50.010024: Pseudo dice [0.7188]
2024-12-15 01:57:50.010652: Epoch time: 425.8 s
2024-12-15 01:57:50.011246: Yayy! New best EMA pseudo Dice: 0.7012
2024-12-15 01:57:51.798238: 
2024-12-15 01:57:51.799072: Epoch 93
2024-12-15 01:57:51.799785: Current learning rate: 0.00419
2024-12-15 02:04:44.012422: Validation loss did not improve from -0.48766. Patience: 57/50
2024-12-15 02:04:44.013357: train_loss -0.7782
2024-12-15 02:04:44.014523: val_loss -0.3544
2024-12-15 02:04:44.015174: Pseudo dice [0.672]
2024-12-15 02:04:44.015941: Epoch time: 412.22 s
2024-12-15 02:04:45.932245: 
2024-12-15 02:04:45.933734: Epoch 94
2024-12-15 02:04:45.934629: Current learning rate: 0.00412
2024-12-15 02:11:39.929959: Validation loss did not improve from -0.48766. Patience: 58/50
2024-12-15 02:11:39.931025: train_loss -0.783
2024-12-15 02:11:39.931845: val_loss -0.4088
2024-12-15 02:11:39.932770: Pseudo dice [0.7007]
2024-12-15 02:11:39.933589: Epoch time: 414.0 s
2024-12-15 02:11:41.659724: 
2024-12-15 02:11:41.660957: Epoch 95
2024-12-15 02:11:41.661868: Current learning rate: 0.00405
2024-12-15 02:18:29.777055: Validation loss did not improve from -0.48766. Patience: 59/50
2024-12-15 02:18:29.781483: train_loss -0.7808
2024-12-15 02:18:29.782691: val_loss -0.4135
2024-12-15 02:18:29.783663: Pseudo dice [0.6953]
2024-12-15 02:18:29.784735: Epoch time: 408.12 s
2024-12-15 02:18:31.220350: 
2024-12-15 02:18:31.221706: Epoch 96
2024-12-15 02:18:31.222691: Current learning rate: 0.00399
2024-12-15 02:25:11.101298: Validation loss did not improve from -0.48766. Patience: 60/50
2024-12-15 02:25:11.102262: train_loss -0.7807
2024-12-15 02:25:11.103245: val_loss -0.3988
2024-12-15 02:25:11.104112: Pseudo dice [0.6803]
2024-12-15 02:25:11.105050: Epoch time: 399.88 s
2024-12-15 02:25:12.569564: 
2024-12-15 02:25:12.570669: Epoch 97
2024-12-15 02:25:12.571500: Current learning rate: 0.00392
2024-12-15 02:32:06.856733: Validation loss did not improve from -0.48766. Patience: 61/50
2024-12-15 02:32:06.857697: train_loss -0.7837
2024-12-15 02:32:06.858805: val_loss -0.366
2024-12-15 02:32:06.859747: Pseudo dice [0.6715]
2024-12-15 02:32:06.860624: Epoch time: 414.29 s
2024-12-15 02:32:08.237012: 
2024-12-15 02:32:08.237913: Epoch 98
2024-12-15 02:32:08.238703: Current learning rate: 0.00385
2024-12-15 02:39:07.073983: Validation loss did not improve from -0.48766. Patience: 62/50
2024-12-15 02:39:07.074723: train_loss -0.7822
2024-12-15 02:39:07.075469: val_loss -0.4049
2024-12-15 02:39:07.076180: Pseudo dice [0.6988]
2024-12-15 02:39:07.076880: Epoch time: 418.84 s
2024-12-15 02:39:08.522243: 
2024-12-15 02:39:08.523385: Epoch 99
2024-12-15 02:39:08.524100: Current learning rate: 0.00379
2024-12-15 02:46:10.283492: Validation loss did not improve from -0.48766. Patience: 63/50
2024-12-15 02:46:10.284786: train_loss -0.7871
2024-12-15 02:46:10.285599: val_loss -0.407
2024-12-15 02:46:10.286376: Pseudo dice [0.696]
2024-12-15 02:46:10.287227: Epoch time: 421.76 s
2024-12-15 02:46:12.083581: 
2024-12-15 02:46:12.084609: Epoch 100
2024-12-15 02:46:12.085415: Current learning rate: 0.00372
2024-12-15 02:53:07.711060: Validation loss did not improve from -0.48766. Patience: 64/50
2024-12-15 02:53:07.711861: train_loss -0.788
2024-12-15 02:53:07.712584: val_loss -0.4072
2024-12-15 02:53:07.713234: Pseudo dice [0.7044]
2024-12-15 02:53:07.713867: Epoch time: 415.63 s
2024-12-15 02:53:09.096729: 
2024-12-15 02:53:09.097546: Epoch 101
2024-12-15 02:53:09.098275: Current learning rate: 0.00365
2024-12-15 03:00:08.295905: Validation loss did not improve from -0.48766. Patience: 65/50
2024-12-15 03:00:08.296524: train_loss -0.7882
2024-12-15 03:00:08.297392: val_loss -0.4045
2024-12-15 03:00:08.298110: Pseudo dice [0.6931]
2024-12-15 03:00:08.298887: Epoch time: 419.2 s
2024-12-15 03:00:09.687233: 
2024-12-15 03:00:09.688288: Epoch 102
2024-12-15 03:00:09.689229: Current learning rate: 0.00359
2024-12-15 03:07:06.838204: Validation loss did not improve from -0.48766. Patience: 66/50
2024-12-15 03:07:06.839246: train_loss -0.7868
2024-12-15 03:07:06.840244: val_loss -0.3491
2024-12-15 03:07:06.841267: Pseudo dice [0.667]
2024-12-15 03:07:06.842178: Epoch time: 417.15 s
2024-12-15 03:07:08.320550: 
2024-12-15 03:07:08.321949: Epoch 103
2024-12-15 03:07:08.322914: Current learning rate: 0.00352
2024-12-15 03:13:56.851275: Validation loss did not improve from -0.48766. Patience: 67/50
2024-12-15 03:13:56.851925: train_loss -0.7837
2024-12-15 03:13:56.852617: val_loss -0.4416
2024-12-15 03:13:56.853314: Pseudo dice [0.7132]
2024-12-15 03:13:56.854027: Epoch time: 408.53 s
2024-12-15 03:13:58.232636: 
2024-12-15 03:13:58.233569: Epoch 104
2024-12-15 03:13:58.234166: Current learning rate: 0.00345
2024-12-15 03:20:56.853972: Validation loss did not improve from -0.48766. Patience: 68/50
2024-12-15 03:20:56.857648: train_loss -0.7834
2024-12-15 03:20:56.859311: val_loss -0.4548
2024-12-15 03:20:56.860221: Pseudo dice [0.7155]
2024-12-15 03:20:56.861158: Epoch time: 418.63 s
2024-12-15 03:20:59.008384: 
2024-12-15 03:20:59.009262: Epoch 105
2024-12-15 03:20:59.009959: Current learning rate: 0.00338
2024-12-15 03:28:22.931159: Validation loss did not improve from -0.48766. Patience: 69/50
2024-12-15 03:28:22.932618: train_loss -0.788
2024-12-15 03:28:22.933943: val_loss -0.4349
2024-12-15 03:28:22.934888: Pseudo dice [0.712]
2024-12-15 03:28:22.935859: Epoch time: 443.93 s
2024-12-15 03:28:24.344765: 
2024-12-15 03:28:24.345803: Epoch 106
2024-12-15 03:28:24.346684: Current learning rate: 0.00332
2024-12-15 03:36:00.420769: Validation loss did not improve from -0.48766. Patience: 70/50
2024-12-15 03:36:00.421679: train_loss -0.7901
2024-12-15 03:36:00.422410: val_loss -0.4202
2024-12-15 03:36:00.423086: Pseudo dice [0.7011]
2024-12-15 03:36:00.423757: Epoch time: 456.08 s
2024-12-15 03:36:01.796922: 
2024-12-15 03:36:01.798279: Epoch 107
2024-12-15 03:36:01.799015: Current learning rate: 0.00325
2024-12-15 03:43:31.438605: Validation loss did not improve from -0.48766. Patience: 71/50
2024-12-15 03:43:31.439589: train_loss -0.7881
2024-12-15 03:43:31.440439: val_loss -0.3613
2024-12-15 03:43:31.441067: Pseudo dice [0.6686]
2024-12-15 03:43:31.441673: Epoch time: 449.64 s
2024-12-15 03:43:32.820508: 
2024-12-15 03:43:32.821542: Epoch 108
2024-12-15 03:43:32.822233: Current learning rate: 0.00318
2024-12-15 03:50:41.301202: Validation loss did not improve from -0.48766. Patience: 72/50
2024-12-15 03:50:41.302240: train_loss -0.7886
2024-12-15 03:50:41.303047: val_loss -0.4407
2024-12-15 03:50:41.303708: Pseudo dice [0.7148]
2024-12-15 03:50:41.304528: Epoch time: 428.48 s
2024-12-15 03:50:42.688355: 
2024-12-15 03:50:42.689504: Epoch 109
2024-12-15 03:50:42.690248: Current learning rate: 0.00311
2024-12-15 03:58:05.056468: Validation loss did not improve from -0.48766. Patience: 73/50
2024-12-15 03:58:05.057431: train_loss -0.7921
2024-12-15 03:58:05.058382: val_loss -0.3464
2024-12-15 03:58:05.059140: Pseudo dice [0.6742]
2024-12-15 03:58:05.059861: Epoch time: 442.37 s
2024-12-15 03:58:06.859332: 
2024-12-15 03:58:06.860751: Epoch 110
2024-12-15 03:58:06.861789: Current learning rate: 0.00304
2024-12-15 04:05:05.655127: Validation loss did not improve from -0.48766. Patience: 74/50
2024-12-15 04:05:05.656099: train_loss -0.7918
2024-12-15 04:05:05.656811: val_loss -0.398
2024-12-15 04:05:05.657485: Pseudo dice [0.6927]
2024-12-15 04:05:05.658192: Epoch time: 418.8 s
2024-12-15 04:05:07.032102: 
2024-12-15 04:05:07.033518: Epoch 111
2024-12-15 04:05:07.034274: Current learning rate: 0.00297
2024-12-15 04:12:20.245819: Validation loss did not improve from -0.48766. Patience: 75/50
2024-12-15 04:12:20.246780: train_loss -0.7937
2024-12-15 04:12:20.247566: val_loss -0.4075
2024-12-15 04:12:20.248211: Pseudo dice [0.6961]
2024-12-15 04:12:20.248851: Epoch time: 433.22 s
2024-12-15 04:12:21.648710: 
2024-12-15 04:12:21.650105: Epoch 112
2024-12-15 04:12:21.650851: Current learning rate: 0.00291
2024-12-15 04:19:35.724139: Validation loss did not improve from -0.48766. Patience: 76/50
2024-12-15 04:19:35.724964: train_loss -0.7964
2024-12-15 04:19:35.725714: val_loss -0.4336
2024-12-15 04:19:35.726367: Pseudo dice [0.7016]
2024-12-15 04:19:35.727056: Epoch time: 434.08 s
2024-12-15 04:19:37.100098: 
2024-12-15 04:19:37.101075: Epoch 113
2024-12-15 04:19:37.101825: Current learning rate: 0.00284
2024-12-15 04:26:23.428631: Validation loss did not improve from -0.48766. Patience: 77/50
2024-12-15 04:26:23.434144: train_loss -0.7968
2024-12-15 04:26:23.435697: val_loss -0.4437
2024-12-15 04:26:23.436388: Pseudo dice [0.7095]
2024-12-15 04:26:23.437410: Epoch time: 406.33 s
2024-12-15 04:26:24.857054: 
2024-12-15 04:26:24.858468: Epoch 114
2024-12-15 04:26:24.859395: Current learning rate: 0.00277
2024-12-15 04:33:33.495434: Validation loss did not improve from -0.48766. Patience: 78/50
2024-12-15 04:33:33.496327: train_loss -0.7946
2024-12-15 04:33:33.497132: val_loss -0.3949
2024-12-15 04:33:33.497851: Pseudo dice [0.6971]
2024-12-15 04:33:33.498544: Epoch time: 428.64 s
2024-12-15 04:33:35.260556: 
2024-12-15 04:33:35.261917: Epoch 115
2024-12-15 04:33:35.262763: Current learning rate: 0.0027
2024-12-15 04:40:49.594815: Validation loss did not improve from -0.48766. Patience: 79/50
2024-12-15 04:40:49.595557: train_loss -0.7944
2024-12-15 04:40:49.596488: val_loss -0.3859
2024-12-15 04:40:49.597389: Pseudo dice [0.6899]
2024-12-15 04:40:49.598166: Epoch time: 434.34 s
2024-12-15 04:40:51.637909: 
2024-12-15 04:40:51.639318: Epoch 116
2024-12-15 04:40:51.640157: Current learning rate: 0.00263
2024-12-15 04:48:03.197726: Validation loss did not improve from -0.48766. Patience: 80/50
2024-12-15 04:48:03.198622: train_loss -0.7973
2024-12-15 04:48:03.199326: val_loss -0.4265
2024-12-15 04:48:03.199958: Pseudo dice [0.7135]
2024-12-15 04:48:03.200595: Epoch time: 431.56 s
2024-12-15 04:48:04.615015: 
2024-12-15 04:48:04.616230: Epoch 117
2024-12-15 04:48:04.616971: Current learning rate: 0.00256
2024-12-15 04:54:45.527184: Validation loss did not improve from -0.48766. Patience: 81/50
2024-12-15 04:54:45.528262: train_loss -0.7932
2024-12-15 04:54:45.529043: val_loss -0.4117
2024-12-15 04:54:45.529733: Pseudo dice [0.6949]
2024-12-15 04:54:45.530392: Epoch time: 400.91 s
2024-12-15 04:54:46.996118: 
2024-12-15 04:54:46.997561: Epoch 118
2024-12-15 04:54:46.998501: Current learning rate: 0.00249
2024-12-15 05:01:59.208783: Validation loss did not improve from -0.48766. Patience: 82/50
2024-12-15 05:01:59.209656: train_loss -0.7949
2024-12-15 05:01:59.210420: val_loss -0.4139
2024-12-15 05:01:59.211053: Pseudo dice [0.6953]
2024-12-15 05:01:59.211712: Epoch time: 432.21 s
2024-12-15 05:02:00.666556: 
2024-12-15 05:02:00.667953: Epoch 119
2024-12-15 05:02:00.669036: Current learning rate: 0.00242
2024-12-15 05:08:48.142823: Validation loss did not improve from -0.48766. Patience: 83/50
2024-12-15 05:08:48.143965: train_loss -0.7983
2024-12-15 05:08:48.144917: val_loss -0.396
2024-12-15 05:08:48.145628: Pseudo dice [0.7019]
2024-12-15 05:08:48.146431: Epoch time: 407.48 s
2024-12-15 05:08:50.014591: 
2024-12-15 05:08:50.015974: Epoch 120
2024-12-15 05:08:50.016921: Current learning rate: 0.00235
2024-12-15 05:15:30.040655: Validation loss did not improve from -0.48766. Patience: 84/50
2024-12-15 05:15:30.041659: train_loss -0.8016
2024-12-15 05:15:30.042391: val_loss -0.4087
2024-12-15 05:15:30.042960: Pseudo dice [0.6999]
2024-12-15 05:15:30.043704: Epoch time: 400.03 s
2024-12-15 05:15:31.447656: 
2024-12-15 05:15:31.449183: Epoch 121
2024-12-15 05:15:31.449992: Current learning rate: 0.00228
2024-12-15 05:22:22.558273: Validation loss did not improve from -0.48766. Patience: 85/50
2024-12-15 05:22:22.559402: train_loss -0.8003
2024-12-15 05:22:22.560150: val_loss -0.379
2024-12-15 05:22:22.560773: Pseudo dice [0.6959]
2024-12-15 05:22:22.561521: Epoch time: 411.11 s
2024-12-15 05:22:23.970794: 
2024-12-15 05:22:23.979340: Epoch 122
2024-12-15 05:22:23.980225: Current learning rate: 0.00221
2024-12-15 05:29:22.443987: Validation loss did not improve from -0.48766. Patience: 86/50
2024-12-15 05:29:22.445837: train_loss -0.8018
2024-12-15 05:29:22.447464: val_loss -0.3752
2024-12-15 05:29:22.448276: Pseudo dice [0.6812]
2024-12-15 05:29:22.449048: Epoch time: 418.48 s
2024-12-15 05:29:23.885280: 
2024-12-15 05:29:23.886713: Epoch 123
2024-12-15 05:29:23.887504: Current learning rate: 0.00214
2024-12-15 05:36:16.182385: Validation loss did not improve from -0.48766. Patience: 87/50
2024-12-15 05:36:16.184207: train_loss -0.8017
2024-12-15 05:36:16.185329: val_loss -0.4199
2024-12-15 05:36:16.186053: Pseudo dice [0.6953]
2024-12-15 05:36:16.187145: Epoch time: 412.3 s
2024-12-15 05:36:17.620496: 
2024-12-15 05:36:17.621893: Epoch 124
2024-12-15 05:36:17.622581: Current learning rate: 0.00207
2024-12-15 05:43:06.232866: Validation loss did not improve from -0.48766. Patience: 88/50
2024-12-15 05:43:06.233865: train_loss -0.8007
2024-12-15 05:43:06.234874: val_loss -0.4104
2024-12-15 05:43:06.235904: Pseudo dice [0.6995]
2024-12-15 05:43:06.236912: Epoch time: 408.61 s
2024-12-15 05:43:08.023336: 
2024-12-15 05:43:08.024817: Epoch 125
2024-12-15 05:43:08.025943: Current learning rate: 0.00199
2024-12-15 05:50:02.435228: Validation loss did not improve from -0.48766. Patience: 89/50
2024-12-15 05:50:02.436352: train_loss -0.804
2024-12-15 05:50:02.437419: val_loss -0.3673
2024-12-15 05:50:02.438301: Pseudo dice [0.6776]
2024-12-15 05:50:02.439250: Epoch time: 414.41 s
2024-12-15 05:50:04.017639: 
2024-12-15 05:50:04.018925: Epoch 126
2024-12-15 05:50:04.020059: Current learning rate: 0.00192
2024-12-15 05:57:01.571796: Validation loss did not improve from -0.48766. Patience: 90/50
2024-12-15 05:57:01.572850: train_loss -0.8026
2024-12-15 05:57:01.573664: val_loss -0.3967
2024-12-15 05:57:01.574467: Pseudo dice [0.6985]
2024-12-15 05:57:01.575272: Epoch time: 417.56 s
2024-12-15 05:57:03.492503: 
2024-12-15 05:57:03.493861: Epoch 127
2024-12-15 05:57:03.494615: Current learning rate: 0.00185
2024-12-15 06:04:04.704983: Validation loss did not improve from -0.48766. Patience: 91/50
2024-12-15 06:04:04.706058: train_loss -0.8014
2024-12-15 06:04:04.707023: val_loss -0.3649
2024-12-15 06:04:04.707911: Pseudo dice [0.6873]
2024-12-15 06:04:04.708710: Epoch time: 421.21 s
2024-12-15 06:04:06.126110: 
2024-12-15 06:04:06.127714: Epoch 128
2024-12-15 06:04:06.128719: Current learning rate: 0.00178
2024-12-15 06:10:53.190888: Validation loss did not improve from -0.48766. Patience: 92/50
2024-12-15 06:10:53.191965: train_loss -0.8029
2024-12-15 06:10:53.192731: val_loss -0.4086
2024-12-15 06:10:53.193403: Pseudo dice [0.7007]
2024-12-15 06:10:53.194181: Epoch time: 407.07 s
2024-12-15 06:10:54.580314: 
2024-12-15 06:10:54.581661: Epoch 129
2024-12-15 06:10:54.583033: Current learning rate: 0.0017
2024-12-15 06:17:47.932997: Validation loss did not improve from -0.48766. Patience: 93/50
2024-12-15 06:17:47.934053: train_loss -0.8049
2024-12-15 06:17:47.934728: val_loss -0.3894
2024-12-15 06:17:47.935377: Pseudo dice [0.6992]
2024-12-15 06:17:47.936119: Epoch time: 413.36 s
2024-12-15 06:17:49.712384: 
2024-12-15 06:17:49.713558: Epoch 130
2024-12-15 06:17:49.714289: Current learning rate: 0.00163
2024-12-15 06:24:39.984548: Validation loss did not improve from -0.48766. Patience: 94/50
2024-12-15 06:24:39.985625: train_loss -0.8046
2024-12-15 06:24:39.986665: val_loss -0.3512
2024-12-15 06:24:39.987642: Pseudo dice [0.6665]
2024-12-15 06:24:39.988549: Epoch time: 410.27 s
2024-12-15 06:24:41.389454: 
2024-12-15 06:24:41.390764: Epoch 131
2024-12-15 06:24:41.391567: Current learning rate: 0.00156
2024-12-15 06:31:42.392308: Validation loss did not improve from -0.48766. Patience: 95/50
2024-12-15 06:31:42.393342: train_loss -0.805
2024-12-15 06:31:42.395514: val_loss -0.3833
2024-12-15 06:31:42.396486: Pseudo dice [0.6819]
2024-12-15 06:31:42.397420: Epoch time: 421.01 s
2024-12-15 06:31:43.802570: 
2024-12-15 06:31:43.804074: Epoch 132
2024-12-15 06:31:43.804886: Current learning rate: 0.00148
2024-12-15 06:38:26.347165: Validation loss did not improve from -0.48766. Patience: 96/50
2024-12-15 06:38:26.352945: train_loss -0.8019
2024-12-15 06:38:26.354129: val_loss -0.4017
2024-12-15 06:38:26.355080: Pseudo dice [0.6953]
2024-12-15 06:38:26.356080: Epoch time: 402.55 s
2024-12-15 06:38:27.802664: 
2024-12-15 06:38:27.804053: Epoch 133
2024-12-15 06:38:27.804961: Current learning rate: 0.00141
2024-12-15 06:45:20.715153: Validation loss did not improve from -0.48766. Patience: 97/50
2024-12-15 06:45:20.716125: train_loss -0.8071
2024-12-15 06:45:20.716918: val_loss -0.4208
2024-12-15 06:45:20.717857: Pseudo dice [0.7089]
2024-12-15 06:45:20.718849: Epoch time: 412.91 s
2024-12-15 06:45:22.123667: 
2024-12-15 06:45:22.124735: Epoch 134
2024-12-15 06:45:22.125511: Current learning rate: 0.00133
2024-12-15 06:52:26.744230: Validation loss did not improve from -0.48766. Patience: 98/50
2024-12-15 06:52:26.745210: train_loss -0.8066
2024-12-15 06:52:26.745922: val_loss -0.3872
2024-12-15 06:52:26.746683: Pseudo dice [0.6933]
2024-12-15 06:52:26.747332: Epoch time: 424.62 s
2024-12-15 06:52:28.599565: 
2024-12-15 06:52:28.600724: Epoch 135
2024-12-15 06:52:28.601411: Current learning rate: 0.00126
2024-12-15 06:59:44.758072: Validation loss did not improve from -0.48766. Patience: 99/50
2024-12-15 06:59:44.759323: train_loss -0.8078
2024-12-15 06:59:44.760279: val_loss -0.4238
2024-12-15 06:59:44.761180: Pseudo dice [0.7098]
2024-12-15 06:59:44.762006: Epoch time: 436.16 s
2024-12-15 06:59:46.225034: 
2024-12-15 06:59:46.226118: Epoch 136
2024-12-15 06:59:46.226995: Current learning rate: 0.00118
2024-12-15 07:06:52.873607: Validation loss did not improve from -0.48766. Patience: 100/50
2024-12-15 07:06:52.874384: train_loss -0.8074
2024-12-15 07:06:52.875319: val_loss -0.3735
2024-12-15 07:06:52.876041: Pseudo dice [0.6888]
2024-12-15 07:06:52.876759: Epoch time: 426.65 s
2024-12-15 07:06:54.300068: 
2024-12-15 07:06:54.301445: Epoch 137
2024-12-15 07:06:54.302262: Current learning rate: 0.00111
2024-12-15 07:14:02.395030: Validation loss did not improve from -0.48766. Patience: 101/50
2024-12-15 07:14:02.396090: train_loss -0.8075
2024-12-15 07:14:02.396978: val_loss -0.4365
2024-12-15 07:14:02.397832: Pseudo dice [0.7084]
2024-12-15 07:14:02.398661: Epoch time: 428.1 s
2024-12-15 07:14:04.204010: 
2024-12-15 07:14:04.205429: Epoch 138
2024-12-15 07:14:04.206319: Current learning rate: 0.00103
2024-12-15 07:20:56.634423: Validation loss did not improve from -0.48766. Patience: 102/50
2024-12-15 07:20:56.635559: train_loss -0.8088
2024-12-15 07:20:56.636409: val_loss -0.399
2024-12-15 07:20:56.637249: Pseudo dice [0.7049]
2024-12-15 07:20:56.638089: Epoch time: 412.43 s
2024-12-15 07:20:58.050477: 
2024-12-15 07:20:58.051940: Epoch 139
2024-12-15 07:20:58.052936: Current learning rate: 0.00095
2024-12-15 07:28:06.764000: Validation loss did not improve from -0.48766. Patience: 103/50
2024-12-15 07:28:06.765007: train_loss -0.8086
2024-12-15 07:28:06.765789: val_loss -0.3907
2024-12-15 07:28:06.766612: Pseudo dice [0.6932]
2024-12-15 07:28:06.767390: Epoch time: 428.72 s
2024-12-15 07:28:08.678123: 
2024-12-15 07:28:08.679295: Epoch 140
2024-12-15 07:28:08.680075: Current learning rate: 0.00087
2024-12-15 07:35:21.807017: Validation loss did not improve from -0.48766. Patience: 104/50
2024-12-15 07:35:21.807937: train_loss -0.8119
2024-12-15 07:35:21.809540: val_loss -0.3927
2024-12-15 07:35:21.810239: Pseudo dice [0.7044]
2024-12-15 07:35:21.811041: Epoch time: 433.13 s
2024-12-15 07:35:23.231997: 
2024-12-15 07:35:23.233391: Epoch 141
2024-12-15 07:35:23.234118: Current learning rate: 0.00079
2024-12-15 07:42:42.044189: Validation loss did not improve from -0.48766. Patience: 105/50
2024-12-15 07:42:42.048768: train_loss -0.8094
2024-12-15 07:42:42.049556: val_loss -0.3824
2024-12-15 07:42:42.050212: Pseudo dice [0.6988]
2024-12-15 07:42:42.050931: Epoch time: 438.82 s
2024-12-15 07:42:43.497075: 
2024-12-15 07:42:43.498266: Epoch 142
2024-12-15 07:42:43.498974: Current learning rate: 0.00071
2024-12-15 07:49:53.157303: Validation loss did not improve from -0.48766. Patience: 106/50
2024-12-15 07:49:53.158339: train_loss -0.811
2024-12-15 07:49:53.159165: val_loss -0.4101
2024-12-15 07:49:53.159993: Pseudo dice [0.7046]
2024-12-15 07:49:53.160920: Epoch time: 429.66 s
2024-12-15 07:49:54.586488: 
2024-12-15 07:49:54.587967: Epoch 143
2024-12-15 07:49:54.588776: Current learning rate: 0.00063
2024-12-15 07:56:42.362923: Validation loss did not improve from -0.48766. Patience: 107/50
2024-12-15 07:56:42.363796: train_loss -0.8084
2024-12-15 07:56:42.364645: val_loss -0.3861
2024-12-15 07:56:42.365442: Pseudo dice [0.689]
2024-12-15 07:56:42.366229: Epoch time: 407.78 s
2024-12-15 07:56:43.884385: 
2024-12-15 07:56:43.885492: Epoch 144
2024-12-15 07:56:43.886281: Current learning rate: 0.00055
2024-12-15 08:03:52.607775: Validation loss did not improve from -0.48766. Patience: 108/50
2024-12-15 08:03:52.608809: train_loss -0.8129
2024-12-15 08:03:52.609823: val_loss -0.3684
2024-12-15 08:03:52.610906: Pseudo dice [0.6869]
2024-12-15 08:03:52.612371: Epoch time: 428.73 s
2024-12-15 08:03:54.392369: 
2024-12-15 08:03:54.393881: Epoch 145
2024-12-15 08:03:54.394721: Current learning rate: 0.00047
2024-12-15 08:10:40.897557: Validation loss did not improve from -0.48766. Patience: 109/50
2024-12-15 08:10:40.898523: train_loss -0.808
2024-12-15 08:10:40.899254: val_loss -0.3904
2024-12-15 08:10:40.900009: Pseudo dice [0.6985]
2024-12-15 08:10:40.900796: Epoch time: 406.51 s
2024-12-15 08:10:42.354850: 
2024-12-15 08:10:42.356226: Epoch 146
2024-12-15 08:10:42.357055: Current learning rate: 0.00038
2024-12-15 08:17:44.119328: Validation loss did not improve from -0.48766. Patience: 110/50
2024-12-15 08:17:44.120287: train_loss -0.8104
2024-12-15 08:17:44.121124: val_loss -0.3535
2024-12-15 08:17:44.121831: Pseudo dice [0.6738]
2024-12-15 08:17:44.122539: Epoch time: 421.77 s
2024-12-15 08:17:45.550768: 
2024-12-15 08:17:45.551979: Epoch 147
2024-12-15 08:17:45.552639: Current learning rate: 0.0003
2024-12-15 08:24:34.605969: Validation loss did not improve from -0.48766. Patience: 111/50
2024-12-15 08:24:34.606951: train_loss -0.8114
2024-12-15 08:24:34.607749: val_loss -0.4009
2024-12-15 08:24:34.608908: Pseudo dice [0.6977]
2024-12-15 08:24:34.609885: Epoch time: 409.06 s
2024-12-15 08:24:36.039706: 
2024-12-15 08:24:36.040904: Epoch 148
2024-12-15 08:24:36.041753: Current learning rate: 0.00021
2024-12-15 08:31:33.917739: Validation loss did not improve from -0.48766. Patience: 112/50
2024-12-15 08:31:33.918763: train_loss -0.8129
2024-12-15 08:31:33.919546: val_loss -0.3773
2024-12-15 08:31:33.920210: Pseudo dice [0.6952]
2024-12-15 08:31:33.920813: Epoch time: 417.88 s
2024-12-15 08:31:35.975991: 
2024-12-15 08:31:35.977396: Epoch 149
2024-12-15 08:31:35.978206: Current learning rate: 0.00011
2024-12-15 08:38:54.222580: Validation loss did not improve from -0.48766. Patience: 113/50
2024-12-15 08:38:54.223516: train_loss -0.8152
2024-12-15 08:38:54.224461: val_loss -0.4182
2024-12-15 08:38:54.225301: Pseudo dice [0.7019]
2024-12-15 08:38:54.226095: Epoch time: 438.25 s
2024-12-15 08:38:56.132886: Training done.
2024-12-15 08:38:56.499525: Using splits from existing split file: /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset307_Sohee_Calcium_OCT_CrossValidation/splits_final_40.json
2024-12-15 08:38:56.512712: The split file contains 5 splits.
2024-12-15 08:38:56.513666: Desired fold for training: 2
2024-12-15 08:38:56.514539: This split has 3 training and 5 validation cases.
2024-12-15 08:38:56.515712: predicting 101-044
2024-12-15 08:38:56.549612: 101-044, shape torch.Size([1, 404, 498, 498]), rank 0
2024-12-15 08:41:56.140629: predicting 106-002
2024-12-15 08:41:56.162366: 106-002, shape torch.Size([1, 539, 498, 498]), rank 0
2024-12-15 08:45:06.763067: predicting 401-004
2024-12-15 08:45:06.781772: 401-004, shape torch.Size([1, 375, 498, 498]), rank 0
2024-12-15 08:47:19.278674: predicting 701-013
2024-12-15 08:47:19.301572: 701-013, shape torch.Size([1, 375, 498, 498]), rank 0
2024-12-15 08:49:32.528717: predicting 704-003
2024-12-15 08:49:32.542228: 704-003, shape torch.Size([1, 375, 498, 498]), rank 0
2024-12-15 08:52:00.836754: Validation complete
2024-12-15 08:52:00.837359: Mean Validation Dice:  0.677513817601769
2024-12-14 16:25:53.894398: unpacking done...
2024-12-14 16:25:54.089010: Unable to plot network architecture: nnUNet_compile is enabled!
2024-12-14 16:25:54.291045: 
2024-12-14 16:25:54.292221: Epoch 0
2024-12-14 16:25:54.293400: Current learning rate: 0.01
2024-12-14 16:30:44.913671: Validation loss improved from 1000.00000 to -0.13887! Patience: 0/50
2024-12-14 16:30:44.914879: train_loss -0.1185
2024-12-14 16:30:44.915889: val_loss -0.1389
2024-12-14 16:30:44.916832: Pseudo dice [0.4857]
2024-12-14 16:30:44.917857: Epoch time: 290.63 s
2024-12-14 16:30:44.918764: Yayy! New best EMA pseudo Dice: 0.4857
2024-12-14 16:30:46.623807: 
2024-12-14 16:30:46.625229: Epoch 1
2024-12-14 16:30:46.626159: Current learning rate: 0.00994
2024-12-14 16:33:36.593820: Validation loss improved from -0.13887 to -0.16241! Patience: 0/50
2024-12-14 16:33:36.594953: train_loss -0.2769
2024-12-14 16:33:36.595747: val_loss -0.1624
2024-12-14 16:33:36.596413: Pseudo dice [0.546]
2024-12-14 16:33:36.597160: Epoch time: 169.97 s
2024-12-14 16:33:36.597927: Yayy! New best EMA pseudo Dice: 0.4918
2024-12-14 16:33:38.502668: 
2024-12-14 16:33:38.504247: Epoch 2
2024-12-14 16:33:38.505306: Current learning rate: 0.00988
2024-12-14 16:36:54.469413: Validation loss improved from -0.16241 to -0.25791! Patience: 0/50
2024-12-14 16:36:54.470512: train_loss -0.3422
2024-12-14 16:36:54.471411: val_loss -0.2579
2024-12-14 16:36:54.472383: Pseudo dice [0.6019]
2024-12-14 16:36:54.473445: Epoch time: 195.97 s
2024-12-14 16:36:54.474467: Yayy! New best EMA pseudo Dice: 0.5028
2024-12-14 16:36:56.406256: 
2024-12-14 16:36:56.407543: Epoch 3
2024-12-14 16:36:56.408609: Current learning rate: 0.00982
2024-12-14 16:42:33.647625: Validation loss did not improve from -0.25791. Patience: 1/50
2024-12-14 16:42:33.648728: train_loss -0.3744
2024-12-14 16:42:33.649655: val_loss -0.2075
2024-12-14 16:42:33.650525: Pseudo dice [0.5842]
2024-12-14 16:42:33.651356: Epoch time: 337.24 s
2024-12-14 16:42:33.652036: Yayy! New best EMA pseudo Dice: 0.5109
2024-12-14 16:42:35.577343: 
2024-12-14 16:42:35.578924: Epoch 4
2024-12-14 16:42:35.579734: Current learning rate: 0.00976
2024-12-14 16:48:06.933318: Validation loss did not improve from -0.25791. Patience: 2/50
2024-12-14 16:48:06.934572: train_loss -0.4051
2024-12-14 16:48:06.935447: val_loss -0.2203
2024-12-14 16:48:06.936157: Pseudo dice [0.5726]
2024-12-14 16:48:06.936875: Epoch time: 331.36 s
2024-12-14 16:48:07.351235: Yayy! New best EMA pseudo Dice: 0.5171
2024-12-14 16:48:09.367566: 
2024-12-14 16:48:09.369168: Epoch 5
2024-12-14 16:48:09.370009: Current learning rate: 0.0097
2024-12-14 16:53:45.182616: Validation loss improved from -0.25791 to -0.30034! Patience: 2/50
2024-12-14 16:53:45.183739: train_loss -0.442
2024-12-14 16:53:45.184552: val_loss -0.3003
2024-12-14 16:53:45.185236: Pseudo dice [0.6223]
2024-12-14 16:53:45.186004: Epoch time: 335.82 s
2024-12-14 16:53:45.186804: Yayy! New best EMA pseudo Dice: 0.5276
2024-12-14 16:53:46.989454: 
2024-12-14 16:53:46.991365: Epoch 6
2024-12-14 16:53:46.992100: Current learning rate: 0.00964
2024-12-14 16:59:21.662952: Validation loss improved from -0.30034 to -0.32549! Patience: 0/50
2024-12-14 16:59:21.664095: train_loss -0.4686
2024-12-14 16:59:21.665207: val_loss -0.3255
2024-12-14 16:59:21.666012: Pseudo dice [0.6316]
2024-12-14 16:59:21.666815: Epoch time: 334.68 s
2024-12-14 16:59:21.667546: Yayy! New best EMA pseudo Dice: 0.538
2024-12-14 16:59:23.587892: 
2024-12-14 16:59:23.589119: Epoch 7
2024-12-14 16:59:23.589897: Current learning rate: 0.00958
2024-12-14 17:05:09.818726: Validation loss did not improve from -0.32549. Patience: 1/50
2024-12-14 17:05:09.819928: train_loss -0.4882
2024-12-14 17:05:09.820829: val_loss -0.316
2024-12-14 17:05:09.821624: Pseudo dice [0.6377]
2024-12-14 17:05:09.822399: Epoch time: 346.23 s
2024-12-14 17:05:09.823070: Yayy! New best EMA pseudo Dice: 0.548
2024-12-14 17:05:11.820078: 
2024-12-14 17:05:11.825792: Epoch 8
2024-12-14 17:05:11.828375: Current learning rate: 0.00952
2024-12-14 17:11:19.930329: Validation loss improved from -0.32549 to -0.33354! Patience: 1/50
2024-12-14 17:11:19.932358: train_loss -0.5025
2024-12-14 17:11:19.933391: val_loss -0.3335
2024-12-14 17:11:19.934214: Pseudo dice [0.6329]
2024-12-14 17:11:19.934958: Epoch time: 368.11 s
2024-12-14 17:11:19.935636: Yayy! New best EMA pseudo Dice: 0.5565
2024-12-14 17:11:22.481329: 
2024-12-14 17:11:22.482575: Epoch 9
2024-12-14 17:11:22.483344: Current learning rate: 0.00946
2024-12-14 17:16:47.314481: Validation loss improved from -0.33354 to -0.37761! Patience: 0/50
2024-12-14 17:16:47.315518: train_loss -0.5245
2024-12-14 17:16:47.316277: val_loss -0.3776
2024-12-14 17:16:47.317080: Pseudo dice [0.6592]
2024-12-14 17:16:47.317834: Epoch time: 324.84 s
2024-12-14 17:16:47.778849: Yayy! New best EMA pseudo Dice: 0.5667
2024-12-14 17:16:49.670465: 
2024-12-14 17:16:49.672021: Epoch 10
2024-12-14 17:16:49.672833: Current learning rate: 0.0094
2024-12-14 17:22:09.279027: Validation loss did not improve from -0.37761. Patience: 1/50
2024-12-14 17:22:09.280151: train_loss -0.5403
2024-12-14 17:22:09.280905: val_loss -0.3255
2024-12-14 17:22:09.281539: Pseudo dice [0.6382]
2024-12-14 17:22:09.282281: Epoch time: 319.61 s
2024-12-14 17:22:09.282880: Yayy! New best EMA pseudo Dice: 0.5739
2024-12-14 17:22:11.048728: 
2024-12-14 17:22:11.050020: Epoch 11
2024-12-14 17:22:11.050682: Current learning rate: 0.00934
2024-12-14 17:27:33.339113: Validation loss did not improve from -0.37761. Patience: 2/50
2024-12-14 17:27:33.340158: train_loss -0.5416
2024-12-14 17:27:33.340983: val_loss -0.3379
2024-12-14 17:27:33.341764: Pseudo dice [0.646]
2024-12-14 17:27:33.342499: Epoch time: 322.29 s
2024-12-14 17:27:33.343226: Yayy! New best EMA pseudo Dice: 0.5811
2024-12-14 17:27:35.163776: 
2024-12-14 17:27:35.165140: Epoch 12
2024-12-14 17:27:35.166029: Current learning rate: 0.00928
2024-12-14 17:32:31.102217: Validation loss did not improve from -0.37761. Patience: 3/50
2024-12-14 17:32:31.103323: train_loss -0.5487
2024-12-14 17:32:31.104164: val_loss -0.301
2024-12-14 17:32:31.104936: Pseudo dice [0.6308]
2024-12-14 17:32:31.105628: Epoch time: 295.94 s
2024-12-14 17:32:31.106390: Yayy! New best EMA pseudo Dice: 0.5861
2024-12-14 17:32:32.919843: 
2024-12-14 17:32:32.920943: Epoch 13
2024-12-14 17:32:32.921674: Current learning rate: 0.00922
2024-12-14 17:37:16.687370: Validation loss did not improve from -0.37761. Patience: 4/50
2024-12-14 17:37:16.688362: train_loss -0.5497
2024-12-14 17:37:16.689264: val_loss -0.3637
2024-12-14 17:37:16.689982: Pseudo dice [0.6481]
2024-12-14 17:37:16.690701: Epoch time: 283.77 s
2024-12-14 17:37:16.691426: Yayy! New best EMA pseudo Dice: 0.5923
2024-12-14 17:37:18.619326: 
2024-12-14 17:37:18.620513: Epoch 14
2024-12-14 17:37:18.621316: Current learning rate: 0.00916
2024-12-14 17:43:18.555092: Validation loss improved from -0.37761 to -0.38949! Patience: 4/50
2024-12-14 17:43:18.556243: train_loss -0.5643
2024-12-14 17:43:18.556999: val_loss -0.3895
2024-12-14 17:43:18.557752: Pseudo dice [0.6637]
2024-12-14 17:43:18.558514: Epoch time: 359.94 s
2024-12-14 17:43:18.913025: Yayy! New best EMA pseudo Dice: 0.5994
2024-12-14 17:43:20.786574: 
2024-12-14 17:43:20.788118: Epoch 15
2024-12-14 17:43:20.789133: Current learning rate: 0.0091
2024-12-14 17:50:14.401115: Validation loss improved from -0.38949 to -0.39132! Patience: 0/50
2024-12-14 17:50:14.402235: train_loss -0.5786
2024-12-14 17:50:14.403113: val_loss -0.3913
2024-12-14 17:50:14.403913: Pseudo dice [0.6743]
2024-12-14 17:50:14.404601: Epoch time: 413.62 s
2024-12-14 17:50:14.405382: Yayy! New best EMA pseudo Dice: 0.6069
2024-12-14 17:50:16.492467: 
2024-12-14 17:50:16.494413: Epoch 16
2024-12-14 17:50:16.495332: Current learning rate: 0.00903
2024-12-14 17:57:54.431933: Validation loss improved from -0.39132 to -0.41111! Patience: 0/50
2024-12-14 17:57:54.433131: train_loss -0.597
2024-12-14 17:57:54.433853: val_loss -0.4111
2024-12-14 17:57:54.434550: Pseudo dice [0.6918]
2024-12-14 17:57:54.435244: Epoch time: 457.95 s
2024-12-14 17:57:54.435970: Yayy! New best EMA pseudo Dice: 0.6154
2024-12-14 17:57:56.288287: 
2024-12-14 17:57:56.290213: Epoch 17
2024-12-14 17:57:56.291284: Current learning rate: 0.00897
2024-12-14 18:05:28.740387: Validation loss did not improve from -0.41111. Patience: 1/50
2024-12-14 18:05:28.741620: train_loss -0.5955
2024-12-14 18:05:28.742415: val_loss -0.3978
2024-12-14 18:05:28.743129: Pseudo dice [0.6783]
2024-12-14 18:05:28.743892: Epoch time: 452.45 s
2024-12-14 18:05:28.744658: Yayy! New best EMA pseudo Dice: 0.6217
2024-12-14 18:05:30.575823: 
2024-12-14 18:05:30.577264: Epoch 18
2024-12-14 18:05:30.578124: Current learning rate: 0.00891
2024-12-14 18:12:42.709077: Validation loss did not improve from -0.41111. Patience: 2/50
2024-12-14 18:12:42.710246: train_loss -0.6072
2024-12-14 18:12:42.711015: val_loss -0.3711
2024-12-14 18:12:42.711620: Pseudo dice [0.6554]
2024-12-14 18:12:42.712287: Epoch time: 432.14 s
2024-12-14 18:12:42.712888: Yayy! New best EMA pseudo Dice: 0.6251
2024-12-14 18:12:45.173260: 
2024-12-14 18:12:45.174602: Epoch 19
2024-12-14 18:12:45.175339: Current learning rate: 0.00885
2024-12-14 18:19:58.064714: Validation loss did not improve from -0.41111. Patience: 3/50
2024-12-14 18:19:58.065666: train_loss -0.6113
2024-12-14 18:19:58.066481: val_loss -0.3254
2024-12-14 18:19:58.067228: Pseudo dice [0.6434]
2024-12-14 18:19:58.067933: Epoch time: 432.89 s
2024-12-14 18:19:58.481839: Yayy! New best EMA pseudo Dice: 0.6269
2024-12-14 18:20:00.260697: 
2024-12-14 18:20:00.262140: Epoch 20
2024-12-14 18:20:00.263075: Current learning rate: 0.00879
2024-12-14 18:26:47.537137: Validation loss did not improve from -0.41111. Patience: 4/50
2024-12-14 18:26:47.538223: train_loss -0.6297
2024-12-14 18:26:47.539049: val_loss -0.3485
2024-12-14 18:26:47.539762: Pseudo dice [0.662]
2024-12-14 18:26:47.540514: Epoch time: 407.28 s
2024-12-14 18:26:47.541217: Yayy! New best EMA pseudo Dice: 0.6304
2024-12-14 18:26:49.408733: 
2024-12-14 18:26:49.410204: Epoch 21
2024-12-14 18:26:49.410975: Current learning rate: 0.00873
2024-12-14 18:34:25.408215: Validation loss did not improve from -0.41111. Patience: 5/50
2024-12-14 18:34:25.409118: train_loss -0.6237
2024-12-14 18:34:25.409936: val_loss -0.3747
2024-12-14 18:34:25.410635: Pseudo dice [0.6611]
2024-12-14 18:34:25.411384: Epoch time: 456.0 s
2024-12-14 18:34:25.412219: Yayy! New best EMA pseudo Dice: 0.6335
2024-12-14 18:34:27.150287: 
2024-12-14 18:34:27.151738: Epoch 22
2024-12-14 18:34:27.152572: Current learning rate: 0.00867
2024-12-14 18:40:57.543869: Validation loss did not improve from -0.41111. Patience: 6/50
2024-12-14 18:40:57.585953: train_loss -0.637
2024-12-14 18:40:57.587271: val_loss -0.3834
2024-12-14 18:40:57.588164: Pseudo dice [0.6691]
2024-12-14 18:40:57.591450: Epoch time: 390.44 s
2024-12-14 18:40:57.592715: Yayy! New best EMA pseudo Dice: 0.637
2024-12-14 18:40:59.409188: 
2024-12-14 18:40:59.410506: Epoch 23
2024-12-14 18:40:59.411334: Current learning rate: 0.00861
2024-12-14 18:48:35.735905: Validation loss did not improve from -0.41111. Patience: 7/50
2024-12-14 18:48:35.737431: train_loss -0.6442
2024-12-14 18:48:35.738539: val_loss -0.3416
2024-12-14 18:48:35.739367: Pseudo dice [0.6469]
2024-12-14 18:48:35.740152: Epoch time: 456.33 s
2024-12-14 18:48:35.740830: Yayy! New best EMA pseudo Dice: 0.638
2024-12-14 18:48:37.509094: 
2024-12-14 18:48:37.510359: Epoch 24
2024-12-14 18:48:37.511099: Current learning rate: 0.00855
2024-12-14 18:56:04.940497: Validation loss did not improve from -0.41111. Patience: 8/50
2024-12-14 18:56:04.941797: train_loss -0.6381
2024-12-14 18:56:04.942913: val_loss -0.407
2024-12-14 18:56:04.943856: Pseudo dice [0.6832]
2024-12-14 18:56:04.944703: Epoch time: 447.43 s
2024-12-14 18:56:05.368745: Yayy! New best EMA pseudo Dice: 0.6425
2024-12-14 18:56:07.229916: 
2024-12-14 18:56:07.231360: Epoch 25
2024-12-14 18:56:07.232230: Current learning rate: 0.00849
2024-12-14 19:04:48.583151: Validation loss did not improve from -0.41111. Patience: 9/50
2024-12-14 19:04:48.584955: train_loss -0.6464
2024-12-14 19:04:48.586032: val_loss -0.332
2024-12-14 19:04:48.586723: Pseudo dice [0.6396]
2024-12-14 19:04:48.587617: Epoch time: 521.36 s
2024-12-14 19:04:49.962817: 
2024-12-14 19:04:49.964047: Epoch 26
2024-12-14 19:04:49.964913: Current learning rate: 0.00843
2024-12-14 19:13:35.719873: Validation loss did not improve from -0.41111. Patience: 10/50
2024-12-14 19:13:35.720834: train_loss -0.66
2024-12-14 19:13:35.721764: val_loss -0.3719
2024-12-14 19:13:35.722445: Pseudo dice [0.6773]
2024-12-14 19:13:35.723132: Epoch time: 525.76 s
2024-12-14 19:13:35.723793: Yayy! New best EMA pseudo Dice: 0.6458
2024-12-14 19:13:37.527984: 
2024-12-14 19:13:37.529556: Epoch 27
2024-12-14 19:13:37.530622: Current learning rate: 0.00836
2024-12-14 19:22:25.913681: Validation loss did not improve from -0.41111. Patience: 11/50
2024-12-14 19:22:25.914780: train_loss -0.6695
2024-12-14 19:22:25.915737: val_loss -0.3351
2024-12-14 19:22:25.916414: Pseudo dice [0.6575]
2024-12-14 19:22:25.917206: Epoch time: 528.39 s
2024-12-14 19:22:25.917871: Yayy! New best EMA pseudo Dice: 0.6469
2024-12-14 19:22:27.721624: 
2024-12-14 19:22:27.722990: Epoch 28
2024-12-14 19:22:27.723778: Current learning rate: 0.0083
2024-12-14 19:31:31.485068: Validation loss improved from -0.41111 to -0.42616! Patience: 11/50
2024-12-14 19:31:31.485996: train_loss -0.6663
2024-12-14 19:31:31.486860: val_loss -0.4262
2024-12-14 19:31:31.487600: Pseudo dice [0.6965]
2024-12-14 19:31:31.488309: Epoch time: 543.77 s
2024-12-14 19:31:31.489109: Yayy! New best EMA pseudo Dice: 0.6519
2024-12-14 19:31:33.272084: 
2024-12-14 19:31:33.273398: Epoch 29
2024-12-14 19:31:33.274248: Current learning rate: 0.00824
2024-12-14 19:40:20.671722: Validation loss did not improve from -0.42616. Patience: 1/50
2024-12-14 19:40:20.673047: train_loss -0.6772
2024-12-14 19:40:20.673977: val_loss -0.3595
2024-12-14 19:40:20.674854: Pseudo dice [0.6714]
2024-12-14 19:40:20.675718: Epoch time: 527.4 s
2024-12-14 19:40:21.079400: Yayy! New best EMA pseudo Dice: 0.6538
2024-12-14 19:40:22.900865: 
2024-12-14 19:40:22.902375: Epoch 30
2024-12-14 19:40:22.903471: Current learning rate: 0.00818
2024-12-14 19:49:07.401402: Validation loss did not improve from -0.42616. Patience: 2/50
2024-12-14 19:49:07.402738: train_loss -0.6821
2024-12-14 19:49:07.403743: val_loss -0.3679
2024-12-14 19:49:07.404512: Pseudo dice [0.6659]
2024-12-14 19:49:07.405235: Epoch time: 524.5 s
2024-12-14 19:49:07.406060: Yayy! New best EMA pseudo Dice: 0.6551
2024-12-14 19:49:09.838217: 
2024-12-14 19:49:09.839947: Epoch 31
2024-12-14 19:49:09.840926: Current learning rate: 0.00812
2024-12-14 19:58:05.383448: Validation loss did not improve from -0.42616. Patience: 3/50
2024-12-14 19:58:05.384685: train_loss -0.686
2024-12-14 19:58:05.385669: val_loss -0.365
2024-12-14 19:58:05.386613: Pseudo dice [0.6741]
2024-12-14 19:58:05.387382: Epoch time: 535.55 s
2024-12-14 19:58:05.388167: Yayy! New best EMA pseudo Dice: 0.657
2024-12-14 19:58:07.356803: 
2024-12-14 19:58:07.358216: Epoch 32
2024-12-14 19:58:07.359044: Current learning rate: 0.00806
2024-12-14 20:06:35.342020: Validation loss did not improve from -0.42616. Patience: 4/50
2024-12-14 20:06:35.343075: train_loss -0.6813
2024-12-14 20:06:35.344278: val_loss -0.3798
2024-12-14 20:06:35.345022: Pseudo dice [0.68]
2024-12-14 20:06:35.345640: Epoch time: 507.99 s
2024-12-14 20:06:35.346257: Yayy! New best EMA pseudo Dice: 0.6593
2024-12-14 20:06:37.240400: 
2024-12-14 20:06:37.241934: Epoch 33
2024-12-14 20:06:37.242617: Current learning rate: 0.008
2024-12-14 20:15:23.003036: Validation loss did not improve from -0.42616. Patience: 5/50
2024-12-14 20:15:23.004276: train_loss -0.6796
2024-12-14 20:15:23.005280: val_loss -0.4139
2024-12-14 20:15:23.006169: Pseudo dice [0.6802]
2024-12-14 20:15:23.006899: Epoch time: 525.77 s
2024-12-14 20:15:23.007613: Yayy! New best EMA pseudo Dice: 0.6614
2024-12-14 20:15:24.890851: 
2024-12-14 20:15:24.892959: Epoch 34
2024-12-14 20:15:24.893814: Current learning rate: 0.00793
2024-12-14 20:24:33.457863: Validation loss did not improve from -0.42616. Patience: 6/50
2024-12-14 20:24:33.459189: train_loss -0.6923
2024-12-14 20:24:33.460638: val_loss -0.3877
2024-12-14 20:24:33.461437: Pseudo dice [0.678]
2024-12-14 20:24:33.462201: Epoch time: 548.57 s
2024-12-14 20:24:33.890945: Yayy! New best EMA pseudo Dice: 0.663
2024-12-14 20:24:35.770886: 
2024-12-14 20:24:35.772108: Epoch 35
2024-12-14 20:24:35.772957: Current learning rate: 0.00787
2024-12-14 20:33:30.634908: Validation loss did not improve from -0.42616. Patience: 7/50
2024-12-14 20:33:30.636369: train_loss -0.7013
2024-12-14 20:33:30.637580: val_loss -0.3427
2024-12-14 20:33:30.638482: Pseudo dice [0.6585]
2024-12-14 20:33:30.639585: Epoch time: 534.87 s
2024-12-14 20:33:32.225627: 
2024-12-14 20:33:32.227436: Epoch 36
2024-12-14 20:33:32.228598: Current learning rate: 0.00781
2024-12-14 20:42:37.297981: Validation loss did not improve from -0.42616. Patience: 8/50
2024-12-14 20:42:37.299349: train_loss -0.7037
2024-12-14 20:42:37.300536: val_loss -0.3821
2024-12-14 20:42:37.301522: Pseudo dice [0.6811]
2024-12-14 20:42:37.302412: Epoch time: 545.07 s
2024-12-14 20:42:37.303317: Yayy! New best EMA pseudo Dice: 0.6644
2024-12-14 20:42:39.198156: 
2024-12-14 20:42:39.199657: Epoch 37
2024-12-14 20:42:39.200475: Current learning rate: 0.00775
2024-12-14 20:51:44.230574: Validation loss did not improve from -0.42616. Patience: 9/50
2024-12-14 20:51:44.235662: train_loss -0.7068
2024-12-14 20:51:44.236924: val_loss -0.411
2024-12-14 20:51:44.237607: Pseudo dice [0.6867]
2024-12-14 20:51:44.238318: Epoch time: 545.04 s
2024-12-14 20:51:44.239106: Yayy! New best EMA pseudo Dice: 0.6667
2024-12-14 20:51:46.136693: 
2024-12-14 20:51:46.138087: Epoch 38
2024-12-14 20:51:46.138784: Current learning rate: 0.00769
2024-12-14 21:00:36.071544: Validation loss did not improve from -0.42616. Patience: 10/50
2024-12-14 21:00:36.072679: train_loss -0.7126
2024-12-14 21:00:36.073550: val_loss -0.3692
2024-12-14 21:00:36.074377: Pseudo dice [0.684]
2024-12-14 21:00:36.075330: Epoch time: 529.94 s
2024-12-14 21:00:36.076081: Yayy! New best EMA pseudo Dice: 0.6684
2024-12-14 21:00:37.942382: 
2024-12-14 21:00:37.944011: Epoch 39
2024-12-14 21:00:37.944992: Current learning rate: 0.00763
2024-12-14 21:09:30.543349: Validation loss did not improve from -0.42616. Patience: 11/50
2024-12-14 21:09:30.544419: train_loss -0.7197
2024-12-14 21:09:30.545317: val_loss -0.3561
2024-12-14 21:09:30.546656: Pseudo dice [0.6628]
2024-12-14 21:09:30.547710: Epoch time: 532.6 s
2024-12-14 21:09:32.314256: 
2024-12-14 21:09:32.315254: Epoch 40
2024-12-14 21:09:32.315987: Current learning rate: 0.00756
2024-12-14 21:18:11.191749: Validation loss did not improve from -0.42616. Patience: 12/50
2024-12-14 21:18:11.192716: train_loss -0.7133
2024-12-14 21:18:11.193619: val_loss -0.3838
2024-12-14 21:18:11.194310: Pseudo dice [0.673]
2024-12-14 21:18:11.194984: Epoch time: 518.88 s
2024-12-14 21:18:13.088586: 
2024-12-14 21:18:13.090086: Epoch 41
2024-12-14 21:18:13.090813: Current learning rate: 0.0075
2024-12-14 21:27:06.242808: Validation loss did not improve from -0.42616. Patience: 13/50
2024-12-14 21:27:06.243917: train_loss -0.7154
2024-12-14 21:27:06.244704: val_loss -0.339
2024-12-14 21:27:06.245377: Pseudo dice [0.6645]
2024-12-14 21:27:06.246201: Epoch time: 533.16 s
2024-12-14 21:27:07.634803: 
2024-12-14 21:27:07.636271: Epoch 42
2024-12-14 21:27:07.637034: Current learning rate: 0.00744
2024-12-14 21:36:03.109529: Validation loss did not improve from -0.42616. Patience: 14/50
2024-12-14 21:36:03.110770: train_loss -0.727
2024-12-14 21:36:03.111961: val_loss -0.3808
2024-12-14 21:36:03.112877: Pseudo dice [0.6757]
2024-12-14 21:36:03.113855: Epoch time: 535.48 s
2024-12-14 21:36:03.114815: Yayy! New best EMA pseudo Dice: 0.6687
2024-12-14 21:36:04.906062: 
2024-12-14 21:36:04.907483: Epoch 43
2024-12-14 21:36:04.908302: Current learning rate: 0.00738
2024-12-14 21:45:09.751842: Validation loss did not improve from -0.42616. Patience: 15/50
2024-12-14 21:45:09.752909: train_loss -0.732
2024-12-14 21:45:09.753825: val_loss -0.3618
2024-12-14 21:45:09.754488: Pseudo dice [0.6749]
2024-12-14 21:45:09.755138: Epoch time: 544.85 s
2024-12-14 21:45:09.755894: Yayy! New best EMA pseudo Dice: 0.6693
2024-12-14 21:45:11.535817: 
2024-12-14 21:45:11.536890: Epoch 44
2024-12-14 21:45:11.537568: Current learning rate: 0.00732
2024-12-14 21:54:22.174141: Validation loss did not improve from -0.42616. Patience: 16/50
2024-12-14 21:54:22.176093: train_loss -0.7315
2024-12-14 21:54:22.176904: val_loss -0.3561
2024-12-14 21:54:22.177624: Pseudo dice [0.6667]
2024-12-14 21:54:22.178430: Epoch time: 550.64 s
2024-12-14 21:54:24.110565: 
2024-12-14 21:54:24.111941: Epoch 45
2024-12-14 21:54:24.112857: Current learning rate: 0.00725
2024-12-14 22:03:18.777846: Validation loss did not improve from -0.42616. Patience: 17/50
2024-12-14 22:03:18.779909: train_loss -0.7337
2024-12-14 22:03:18.780933: val_loss -0.4015
2024-12-14 22:03:18.781705: Pseudo dice [0.6905]
2024-12-14 22:03:18.782382: Epoch time: 534.67 s
2024-12-14 22:03:18.783151: Yayy! New best EMA pseudo Dice: 0.6712
2024-12-14 22:03:20.529948: 
2024-12-14 22:03:20.531359: Epoch 46
2024-12-14 22:03:20.532155: Current learning rate: 0.00719
2024-12-14 22:12:36.703598: Validation loss did not improve from -0.42616. Patience: 18/50
2024-12-14 22:12:36.704673: train_loss -0.7372
2024-12-14 22:12:36.705583: val_loss -0.4102
2024-12-14 22:12:36.706523: Pseudo dice [0.6967]
2024-12-14 22:12:36.707489: Epoch time: 556.18 s
2024-12-14 22:12:36.708406: Yayy! New best EMA pseudo Dice: 0.6738
2024-12-14 22:12:38.491808: 
2024-12-14 22:12:38.493322: Epoch 47
2024-12-14 22:12:38.494353: Current learning rate: 0.00713
2024-12-14 22:21:38.013344: Validation loss did not improve from -0.42616. Patience: 19/50
2024-12-14 22:21:38.014418: train_loss -0.7387
2024-12-14 22:21:38.015456: val_loss -0.3682
2024-12-14 22:21:38.016506: Pseudo dice [0.6782]
2024-12-14 22:21:38.017489: Epoch time: 539.52 s
2024-12-14 22:21:38.018453: Yayy! New best EMA pseudo Dice: 0.6742
2024-12-14 22:21:39.821224: 
2024-12-14 22:21:39.822528: Epoch 48
2024-12-14 22:21:39.823403: Current learning rate: 0.00707
2024-12-14 22:30:32.094505: Validation loss did not improve from -0.42616. Patience: 20/50
2024-12-14 22:30:32.095660: train_loss -0.7438
2024-12-14 22:30:32.096581: val_loss -0.4013
2024-12-14 22:30:32.097264: Pseudo dice [0.6946]
2024-12-14 22:30:32.098074: Epoch time: 532.28 s
2024-12-14 22:30:32.098879: Yayy! New best EMA pseudo Dice: 0.6763
2024-12-14 22:30:34.075869: 
2024-12-14 22:30:34.078103: Epoch 49
2024-12-14 22:30:34.079622: Current learning rate: 0.007
2024-12-14 22:39:47.351899: Validation loss did not improve from -0.42616. Patience: 21/50
2024-12-14 22:39:47.352908: train_loss -0.7383
2024-12-14 22:39:47.353872: val_loss -0.3767
2024-12-14 22:39:47.354620: Pseudo dice [0.6812]
2024-12-14 22:39:47.355392: Epoch time: 553.28 s
2024-12-14 22:39:47.794557: Yayy! New best EMA pseudo Dice: 0.6768
2024-12-14 22:39:49.682760: 
2024-12-14 22:39:49.684230: Epoch 50
2024-12-14 22:39:49.685032: Current learning rate: 0.00694
2024-12-14 22:49:10.459067: Validation loss did not improve from -0.42616. Patience: 22/50
2024-12-14 22:49:10.460270: train_loss -0.7448
2024-12-14 22:49:10.461196: val_loss -0.3744
2024-12-14 22:49:10.461888: Pseudo dice [0.6876]
2024-12-14 22:49:10.462652: Epoch time: 560.78 s
2024-12-14 22:49:10.463391: Yayy! New best EMA pseudo Dice: 0.6778
2024-12-14 22:49:12.407583: 
2024-12-14 22:49:12.408972: Epoch 51
2024-12-14 22:49:12.409885: Current learning rate: 0.00688
2024-12-14 22:58:07.461111: Validation loss did not improve from -0.42616. Patience: 23/50
2024-12-14 22:58:07.462431: train_loss -0.7457
2024-12-14 22:58:07.464075: val_loss -0.3801
2024-12-14 22:58:07.464993: Pseudo dice [0.6813]
2024-12-14 22:58:07.465929: Epoch time: 535.06 s
2024-12-14 22:58:07.466679: Yayy! New best EMA pseudo Dice: 0.6782
2024-12-14 22:58:09.959405: 
2024-12-14 22:58:09.960380: Epoch 52
2024-12-14 22:58:09.961024: Current learning rate: 0.00682
2024-12-14 23:07:01.794306: Validation loss did not improve from -0.42616. Patience: 24/50
2024-12-14 23:07:01.797012: train_loss -0.7526
2024-12-14 23:07:01.798218: val_loss -0.3363
2024-12-14 23:07:01.799609: Pseudo dice [0.6673]
2024-12-14 23:07:01.800715: Epoch time: 531.84 s
2024-12-14 23:07:03.315127: 
2024-12-14 23:07:03.316515: Epoch 53
2024-12-14 23:07:03.317254: Current learning rate: 0.00675
2024-12-14 23:16:15.260834: Validation loss did not improve from -0.42616. Patience: 25/50
2024-12-14 23:16:15.261719: train_loss -0.7513
2024-12-14 23:16:15.262619: val_loss -0.3728
2024-12-14 23:16:15.263384: Pseudo dice [0.6789]
2024-12-14 23:16:15.264217: Epoch time: 551.95 s
2024-12-14 23:16:16.748472: 
2024-12-14 23:16:16.749439: Epoch 54
2024-12-14 23:16:16.750190: Current learning rate: 0.00669
2024-12-14 23:25:37.390541: Validation loss did not improve from -0.42616. Patience: 26/50
2024-12-14 23:25:37.391512: train_loss -0.7507
2024-12-14 23:25:37.392680: val_loss -0.3429
2024-12-14 23:25:37.393621: Pseudo dice [0.6681]
2024-12-14 23:25:37.394612: Epoch time: 560.64 s
2024-12-14 23:25:39.401404: 
2024-12-14 23:25:39.402631: Epoch 55
2024-12-14 23:25:39.403741: Current learning rate: 0.00663
2024-12-14 23:35:30.066057: Validation loss did not improve from -0.42616. Patience: 27/50
2024-12-14 23:35:30.067134: train_loss -0.7493
2024-12-14 23:35:30.067988: val_loss -0.3696
2024-12-14 23:35:30.068713: Pseudo dice [0.6892]
2024-12-14 23:35:30.069328: Epoch time: 590.67 s
2024-12-14 23:35:31.587837: 
2024-12-14 23:35:31.589044: Epoch 56
2024-12-14 23:35:31.590181: Current learning rate: 0.00657
2024-12-14 23:45:06.275709: Validation loss did not improve from -0.42616. Patience: 28/50
2024-12-14 23:45:06.276496: train_loss -0.7499
2024-12-14 23:45:06.277562: val_loss -0.3525
2024-12-14 23:45:06.278560: Pseudo dice [0.6723]
2024-12-14 23:45:06.279502: Epoch time: 574.69 s
2024-12-14 23:45:07.751417: 
2024-12-14 23:45:07.752937: Epoch 57
2024-12-14 23:45:07.754056: Current learning rate: 0.0065
2024-12-14 23:54:40.385934: Validation loss did not improve from -0.42616. Patience: 29/50
2024-12-14 23:54:40.387888: train_loss -0.7562
2024-12-14 23:54:40.388664: val_loss -0.3301
2024-12-14 23:54:40.389301: Pseudo dice [0.6657]
2024-12-14 23:54:40.390025: Epoch time: 572.64 s
2024-12-14 23:54:41.819418: 
2024-12-14 23:54:41.820529: Epoch 58
2024-12-14 23:54:41.821222: Current learning rate: 0.00644
2024-12-15 00:04:37.733206: Validation loss did not improve from -0.42616. Patience: 30/50
2024-12-15 00:04:37.735682: train_loss -0.759
2024-12-15 00:04:37.737947: val_loss -0.3854
2024-12-15 00:04:37.738887: Pseudo dice [0.695]
2024-12-15 00:04:37.739908: Epoch time: 595.92 s
2024-12-15 00:04:39.176860: 
2024-12-15 00:04:39.177974: Epoch 59
2024-12-15 00:04:39.178919: Current learning rate: 0.00638
2024-12-15 00:14:21.872734: Validation loss did not improve from -0.42616. Patience: 31/50
2024-12-15 00:14:21.874172: train_loss -0.7644
2024-12-15 00:14:21.874898: val_loss -0.3489
2024-12-15 00:14:21.875571: Pseudo dice [0.6757]
2024-12-15 00:14:21.876249: Epoch time: 582.7 s
2024-12-15 00:14:23.670333: 
2024-12-15 00:14:23.671455: Epoch 60
2024-12-15 00:14:23.672190: Current learning rate: 0.00631
2024-12-15 00:23:50.601524: Validation loss did not improve from -0.42616. Patience: 32/50
2024-12-15 00:23:50.602556: train_loss -0.7606
2024-12-15 00:23:50.603415: val_loss -0.3774
2024-12-15 00:23:50.604057: Pseudo dice [0.6818]
2024-12-15 00:23:50.604735: Epoch time: 566.93 s
2024-12-15 00:23:52.057865: 
2024-12-15 00:23:52.059204: Epoch 61
2024-12-15 00:23:52.060099: Current learning rate: 0.00625
2024-12-15 00:33:26.913352: Validation loss did not improve from -0.42616. Patience: 33/50
2024-12-15 00:33:26.914361: train_loss -0.761
2024-12-15 00:33:26.915105: val_loss -0.3997
2024-12-15 00:33:26.915781: Pseudo dice [0.6879]
2024-12-15 00:33:26.916497: Epoch time: 574.86 s
2024-12-15 00:33:26.917120: Yayy! New best EMA pseudo Dice: 0.6791
2024-12-15 00:33:28.696978: 
2024-12-15 00:33:28.698212: Epoch 62
2024-12-15 00:33:28.698928: Current learning rate: 0.00619
2024-12-15 00:42:58.740988: Validation loss did not improve from -0.42616. Patience: 34/50
2024-12-15 00:42:58.741929: train_loss -0.7657
2024-12-15 00:42:58.742881: val_loss -0.4183
2024-12-15 00:42:58.743589: Pseudo dice [0.6976]
2024-12-15 00:42:58.744330: Epoch time: 570.05 s
2024-12-15 00:42:58.745009: Yayy! New best EMA pseudo Dice: 0.6809
2024-12-15 00:43:01.104686: 
2024-12-15 00:43:01.105743: Epoch 63
2024-12-15 00:43:01.106483: Current learning rate: 0.00612
2024-12-15 00:51:54.615614: Validation loss did not improve from -0.42616. Patience: 35/50
2024-12-15 00:51:54.616380: train_loss -0.7657
2024-12-15 00:51:54.617314: val_loss -0.3384
2024-12-15 00:51:54.618089: Pseudo dice [0.6826]
2024-12-15 00:51:54.618877: Epoch time: 533.51 s
2024-12-15 00:51:54.619739: Yayy! New best EMA pseudo Dice: 0.6811
2024-12-15 00:51:56.492689: 
2024-12-15 00:51:56.494203: Epoch 64
2024-12-15 00:51:56.495283: Current learning rate: 0.00606
2024-12-15 01:00:31.956589: Validation loss did not improve from -0.42616. Patience: 36/50
2024-12-15 01:00:31.957375: train_loss -0.7662
2024-12-15 01:00:31.958163: val_loss -0.3845
2024-12-15 01:00:31.958910: Pseudo dice [0.6859]
2024-12-15 01:00:31.959589: Epoch time: 515.47 s
2024-12-15 01:00:32.354735: Yayy! New best EMA pseudo Dice: 0.6816
2024-12-15 01:00:34.151165: 
2024-12-15 01:00:34.152048: Epoch 65
2024-12-15 01:00:34.152700: Current learning rate: 0.006
2024-12-15 01:09:23.781623: Validation loss did not improve from -0.42616. Patience: 37/50
2024-12-15 01:09:23.784868: train_loss -0.7695
2024-12-15 01:09:23.785607: val_loss -0.4116
2024-12-15 01:09:23.786298: Pseudo dice [0.7097]
2024-12-15 01:09:23.787130: Epoch time: 529.63 s
2024-12-15 01:09:23.787888: Yayy! New best EMA pseudo Dice: 0.6844
2024-12-15 01:09:25.641486: 
2024-12-15 01:09:25.642710: Epoch 66
2024-12-15 01:09:25.643757: Current learning rate: 0.00593
2024-12-15 01:18:04.407789: Validation loss did not improve from -0.42616. Patience: 38/50
2024-12-15 01:18:04.408726: train_loss -0.7684
2024-12-15 01:18:04.409469: val_loss -0.405
2024-12-15 01:18:04.410167: Pseudo dice [0.6931]
2024-12-15 01:18:04.410794: Epoch time: 518.77 s
2024-12-15 01:18:04.411525: Yayy! New best EMA pseudo Dice: 0.6853
2024-12-15 01:18:06.239813: 
2024-12-15 01:18:06.240729: Epoch 67
2024-12-15 01:18:06.241365: Current learning rate: 0.00587
2024-12-15 01:26:23.887874: Validation loss did not improve from -0.42616. Patience: 39/50
2024-12-15 01:26:23.888670: train_loss -0.7727
2024-12-15 01:26:23.889706: val_loss -0.3886
2024-12-15 01:26:23.890590: Pseudo dice [0.6861]
2024-12-15 01:26:23.891562: Epoch time: 497.65 s
2024-12-15 01:26:23.892556: Yayy! New best EMA pseudo Dice: 0.6853
2024-12-15 01:26:25.794325: 
2024-12-15 01:26:25.795535: Epoch 68
2024-12-15 01:26:25.796584: Current learning rate: 0.00581
2024-12-15 01:35:19.116562: Validation loss did not improve from -0.42616. Patience: 40/50
2024-12-15 01:35:19.118013: train_loss -0.7703
2024-12-15 01:35:19.118987: val_loss -0.3213
2024-12-15 01:35:19.119901: Pseudo dice [0.6728]
2024-12-15 01:35:19.120931: Epoch time: 533.33 s
2024-12-15 01:35:20.619592: 
2024-12-15 01:35:20.621007: Epoch 69
2024-12-15 01:35:20.621992: Current learning rate: 0.00574
2024-12-15 01:43:56.871271: Validation loss did not improve from -0.42616. Patience: 41/50
2024-12-15 01:43:56.872459: train_loss -0.7702
2024-12-15 01:43:56.873534: val_loss -0.3321
2024-12-15 01:43:56.874539: Pseudo dice [0.6727]
2024-12-15 01:43:56.875561: Epoch time: 516.25 s
2024-12-15 01:43:58.743761: 
2024-12-15 01:43:58.745207: Epoch 70
2024-12-15 01:43:58.746469: Current learning rate: 0.00568
2024-12-15 01:52:45.308413: Validation loss did not improve from -0.42616. Patience: 42/50
2024-12-15 01:52:45.309276: train_loss -0.7676
2024-12-15 01:52:45.309997: val_loss -0.3933
2024-12-15 01:52:45.310779: Pseudo dice [0.6891]
2024-12-15 01:52:45.311484: Epoch time: 526.57 s
2024-12-15 01:52:46.747127: 
2024-12-15 01:52:46.748162: Epoch 71
2024-12-15 01:52:46.748885: Current learning rate: 0.00562
2024-12-15 02:01:46.778750: Validation loss did not improve from -0.42616. Patience: 43/50
2024-12-15 02:01:46.779678: train_loss -0.7724
2024-12-15 02:01:46.780376: val_loss -0.392
2024-12-15 02:01:46.781225: Pseudo dice [0.6934]
2024-12-15 02:01:46.781988: Epoch time: 540.03 s
2024-12-15 02:01:48.232366: 
2024-12-15 02:01:48.233619: Epoch 72
2024-12-15 02:01:48.234415: Current learning rate: 0.00555
2024-12-15 02:10:57.889750: Validation loss did not improve from -0.42616. Patience: 44/50
2024-12-15 02:10:57.890657: train_loss -0.779
2024-12-15 02:10:57.891984: val_loss -0.3428
2024-12-15 02:10:57.892708: Pseudo dice [0.677]
2024-12-15 02:10:57.893572: Epoch time: 549.66 s
2024-12-15 02:10:59.677086: 
2024-12-15 02:10:59.677992: Epoch 73
2024-12-15 02:10:59.678699: Current learning rate: 0.00549
2024-12-15 02:20:04.289874: Validation loss did not improve from -0.42616. Patience: 45/50
2024-12-15 02:20:04.290885: train_loss -0.7786
2024-12-15 02:20:04.291991: val_loss -0.3561
2024-12-15 02:20:04.292946: Pseudo dice [0.6814]
2024-12-15 02:20:04.294053: Epoch time: 544.62 s
2024-12-15 02:20:05.748556: 
2024-12-15 02:20:05.749755: Epoch 74
2024-12-15 02:20:05.750620: Current learning rate: 0.00542
2024-12-15 02:29:08.408255: Validation loss did not improve from -0.42616. Patience: 46/50
2024-12-15 02:29:08.408930: train_loss -0.7831
2024-12-15 02:29:08.409693: val_loss -0.3653
2024-12-15 02:29:08.410413: Pseudo dice [0.6839]
2024-12-15 02:29:08.411059: Epoch time: 542.66 s
2024-12-15 02:29:10.216985: 
2024-12-15 02:29:10.218015: Epoch 75
2024-12-15 02:29:10.218673: Current learning rate: 0.00536
2024-12-15 02:38:16.660973: Validation loss did not improve from -0.42616. Patience: 47/50
2024-12-15 02:38:16.661638: train_loss -0.7826
2024-12-15 02:38:16.662619: val_loss -0.3712
2024-12-15 02:38:16.663505: Pseudo dice [0.6907]
2024-12-15 02:38:16.664340: Epoch time: 546.45 s
2024-12-15 02:38:18.095118: 
2024-12-15 02:38:18.096138: Epoch 76
2024-12-15 02:38:18.096899: Current learning rate: 0.00529
2024-12-15 02:47:02.129706: Validation loss did not improve from -0.42616. Patience: 48/50
2024-12-15 02:47:02.131000: train_loss -0.7858
2024-12-15 02:47:02.131959: val_loss -0.3995
2024-12-15 02:47:02.132787: Pseudo dice [0.6899]
2024-12-15 02:47:02.133697: Epoch time: 524.04 s
2024-12-15 02:47:03.700097: 
2024-12-15 02:47:03.701665: Epoch 77
2024-12-15 02:47:03.702709: Current learning rate: 0.00523
2024-12-15 02:55:57.101621: Validation loss did not improve from -0.42616. Patience: 49/50
2024-12-15 02:55:57.102694: train_loss -0.7842
2024-12-15 02:55:57.103772: val_loss -0.3788
2024-12-15 02:55:57.104719: Pseudo dice [0.6914]
2024-12-15 02:55:57.105530: Epoch time: 533.41 s
2024-12-15 02:55:57.106337: Yayy! New best EMA pseudo Dice: 0.6855
2024-12-15 02:55:59.071465: 
2024-12-15 02:55:59.072968: Epoch 78
2024-12-15 02:55:59.073758: Current learning rate: 0.00517
2024-12-15 03:04:50.256104: Validation loss did not improve from -0.42616. Patience: 50/50
2024-12-15 03:04:50.257244: train_loss -0.7854
2024-12-15 03:04:50.258173: val_loss -0.3861
2024-12-15 03:04:50.258897: Pseudo dice [0.6928]
2024-12-15 03:04:50.259630: Epoch time: 531.19 s
2024-12-15 03:04:50.260418: Yayy! New best EMA pseudo Dice: 0.6862
2024-12-15 03:04:52.182152: 
2024-12-15 03:04:52.183550: Epoch 79
2024-12-15 03:04:52.184263: Current learning rate: 0.0051
2024-12-15 03:13:33.566123: Validation loss did not improve from -0.42616. Patience: 51/50
2024-12-15 03:13:33.566805: train_loss -0.7861
2024-12-15 03:13:33.567608: val_loss -0.3421
2024-12-15 03:13:33.568372: Pseudo dice [0.6839]
2024-12-15 03:13:33.569067: Epoch time: 521.39 s
2024-12-15 03:13:35.460085: 
2024-12-15 03:13:35.461240: Epoch 80
2024-12-15 03:13:35.462099: Current learning rate: 0.00504
2024-12-15 03:22:42.197616: Validation loss did not improve from -0.42616. Patience: 52/50
2024-12-15 03:22:42.199132: train_loss -0.7858
2024-12-15 03:22:42.200199: val_loss -0.3555
2024-12-15 03:22:42.201014: Pseudo dice [0.6805]
2024-12-15 03:22:42.202134: Epoch time: 546.74 s
2024-12-15 03:22:43.715933: 
2024-12-15 03:22:43.717100: Epoch 81
2024-12-15 03:22:43.717872: Current learning rate: 0.00497
2024-12-15 03:32:32.850910: Validation loss did not improve from -0.42616. Patience: 53/50
2024-12-15 03:32:32.852103: train_loss -0.7905
2024-12-15 03:32:32.852946: val_loss -0.3545
2024-12-15 03:32:32.853777: Pseudo dice [0.681]
2024-12-15 03:32:32.854757: Epoch time: 589.14 s
2024-12-15 03:32:34.362186: 
2024-12-15 03:32:34.363707: Epoch 82
2024-12-15 03:32:34.364730: Current learning rate: 0.00491
2024-12-15 03:41:47.045620: Validation loss did not improve from -0.42616. Patience: 54/50
2024-12-15 03:41:47.046541: train_loss -0.7905
2024-12-15 03:41:47.047398: val_loss -0.3694
2024-12-15 03:41:47.048229: Pseudo dice [0.69]
2024-12-15 03:41:47.049037: Epoch time: 552.69 s
2024-12-15 03:41:48.508253: 
2024-12-15 03:41:48.509737: Epoch 83
2024-12-15 03:41:48.510591: Current learning rate: 0.00484
2024-12-15 03:50:53.537585: Validation loss did not improve from -0.42616. Patience: 55/50
2024-12-15 03:50:53.539486: train_loss -0.788
2024-12-15 03:50:53.540601: val_loss -0.3564
2024-12-15 03:50:53.541472: Pseudo dice [0.7007]
2024-12-15 03:50:53.542379: Epoch time: 545.03 s
2024-12-15 03:50:53.543211: Yayy! New best EMA pseudo Dice: 0.687
2024-12-15 03:50:55.910830: 
2024-12-15 03:50:55.912336: Epoch 84
2024-12-15 03:50:55.913426: Current learning rate: 0.00478
2024-12-15 04:00:15.687313: Validation loss did not improve from -0.42616. Patience: 56/50
2024-12-15 04:00:15.688329: train_loss -0.7904
2024-12-15 04:00:15.689179: val_loss -0.3393
2024-12-15 04:00:15.689904: Pseudo dice [0.6704]
2024-12-15 04:00:15.690683: Epoch time: 559.78 s
2024-12-15 04:00:17.542877: 
2024-12-15 04:00:17.544421: Epoch 85
2024-12-15 04:00:17.545256: Current learning rate: 0.00471
2024-12-15 04:09:17.324824: Validation loss did not improve from -0.42616. Patience: 57/50
2024-12-15 04:09:17.325786: train_loss -0.7931
2024-12-15 04:09:17.326651: val_loss -0.3334
2024-12-15 04:09:17.327404: Pseudo dice [0.681]
2024-12-15 04:09:17.328174: Epoch time: 539.78 s
2024-12-15 04:09:18.785599: 
2024-12-15 04:09:18.786999: Epoch 86
2024-12-15 04:09:18.787850: Current learning rate: 0.00465
2024-12-15 04:18:33.215076: Validation loss did not improve from -0.42616. Patience: 58/50
2024-12-15 04:18:33.216119: train_loss -0.7938
2024-12-15 04:18:33.217091: val_loss -0.3759
2024-12-15 04:18:33.217847: Pseudo dice [0.6925]
2024-12-15 04:18:33.218503: Epoch time: 554.43 s
2024-12-15 04:18:34.705881: 
2024-12-15 04:18:34.707305: Epoch 87
2024-12-15 04:18:34.708165: Current learning rate: 0.00458
2024-12-15 04:27:44.094256: Validation loss did not improve from -0.42616. Patience: 59/50
2024-12-15 04:27:44.095932: train_loss -0.7977
2024-12-15 04:27:44.096968: val_loss -0.3623
2024-12-15 04:27:44.097754: Pseudo dice [0.697]
2024-12-15 04:27:44.098626: Epoch time: 549.39 s
2024-12-15 04:27:45.559308: 
2024-12-15 04:27:45.560718: Epoch 88
2024-12-15 04:27:45.561484: Current learning rate: 0.00452
2024-12-15 04:36:36.650192: Validation loss did not improve from -0.42616. Patience: 60/50
2024-12-15 04:36:36.651522: train_loss -0.7972
2024-12-15 04:36:36.652262: val_loss -0.3791
2024-12-15 04:36:36.653035: Pseudo dice [0.6997]
2024-12-15 04:36:36.653738: Epoch time: 531.09 s
2024-12-15 04:36:36.654484: Yayy! New best EMA pseudo Dice: 0.6881
2024-12-15 04:36:38.485789: 
2024-12-15 04:36:38.487037: Epoch 89
2024-12-15 04:36:38.487745: Current learning rate: 0.00445
2024-12-15 04:45:32.826852: Validation loss did not improve from -0.42616. Patience: 61/50
2024-12-15 04:45:32.827962: train_loss -0.7936
2024-12-15 04:45:32.828743: val_loss -0.3698
2024-12-15 04:45:32.829393: Pseudo dice [0.684]
2024-12-15 04:45:32.830101: Epoch time: 534.34 s
2024-12-15 04:45:34.553052: 
2024-12-15 04:45:34.554343: Epoch 90
2024-12-15 04:45:34.555049: Current learning rate: 0.00438
2024-12-15 04:54:03.340451: Validation loss did not improve from -0.42616. Patience: 62/50
2024-12-15 04:54:03.341653: train_loss -0.796
2024-12-15 04:54:03.342549: val_loss -0.3417
2024-12-15 04:54:03.343360: Pseudo dice [0.6839]
2024-12-15 04:54:03.344051: Epoch time: 508.79 s
2024-12-15 04:54:04.719769: 
2024-12-15 04:54:04.721882: Epoch 91
2024-12-15 04:54:04.722856: Current learning rate: 0.00432
2024-12-15 05:02:59.066848: Validation loss did not improve from -0.42616. Patience: 63/50
2024-12-15 05:02:59.067882: train_loss -0.7986
2024-12-15 05:02:59.068945: val_loss -0.3269
2024-12-15 05:02:59.069822: Pseudo dice [0.6817]
2024-12-15 05:02:59.070741: Epoch time: 534.35 s
2024-12-15 05:03:00.548432: 
2024-12-15 05:03:00.550185: Epoch 92
2024-12-15 05:03:00.551322: Current learning rate: 0.00425
2024-12-15 05:11:41.743213: Validation loss did not improve from -0.42616. Patience: 64/50
2024-12-15 05:11:41.744280: train_loss -0.7993
2024-12-15 05:11:41.745030: val_loss -0.3564
2024-12-15 05:11:41.745725: Pseudo dice [0.6889]
2024-12-15 05:11:41.746363: Epoch time: 521.2 s
2024-12-15 05:11:43.118144: 
2024-12-15 05:11:43.119737: Epoch 93
2024-12-15 05:11:43.120515: Current learning rate: 0.00419
2024-12-15 05:20:32.836092: Validation loss did not improve from -0.42616. Patience: 65/50
2024-12-15 05:20:32.837135: train_loss -0.7986
2024-12-15 05:20:32.837947: val_loss -0.374
2024-12-15 05:20:32.838682: Pseudo dice [0.6892]
2024-12-15 05:20:32.839453: Epoch time: 529.72 s
2024-12-15 05:20:34.218743: 
2024-12-15 05:20:34.219677: Epoch 94
2024-12-15 05:20:34.220407: Current learning rate: 0.00412
2024-12-15 05:29:44.727859: Validation loss did not improve from -0.42616. Patience: 66/50
2024-12-15 05:29:44.729670: train_loss -0.7995
2024-12-15 05:29:44.730852: val_loss -0.3675
2024-12-15 05:29:44.731877: Pseudo dice [0.6948]
2024-12-15 05:29:44.732853: Epoch time: 550.51 s
2024-12-15 05:29:47.091266: 
2024-12-15 05:29:47.092716: Epoch 95
2024-12-15 05:29:47.093606: Current learning rate: 0.00405
2024-12-15 05:38:44.940112: Validation loss did not improve from -0.42616. Patience: 67/50
2024-12-15 05:38:44.941437: train_loss -0.8002
2024-12-15 05:38:44.942274: val_loss -0.3743
2024-12-15 05:38:44.943207: Pseudo dice [0.6915]
2024-12-15 05:38:44.943980: Epoch time: 537.85 s
2024-12-15 05:38:44.944799: Yayy! New best EMA pseudo Dice: 0.6883
2024-12-15 05:38:46.689165: 
2024-12-15 05:38:46.690867: Epoch 96
2024-12-15 05:38:46.691974: Current learning rate: 0.00399
2024-12-15 05:47:48.228297: Validation loss did not improve from -0.42616. Patience: 68/50
2024-12-15 05:47:48.229617: train_loss -0.8033
2024-12-15 05:47:48.230462: val_loss -0.3468
2024-12-15 05:47:48.231137: Pseudo dice [0.6872]
2024-12-15 05:47:48.231824: Epoch time: 541.54 s
2024-12-15 05:47:49.662374: 
2024-12-15 05:47:49.663620: Epoch 97
2024-12-15 05:47:49.664321: Current learning rate: 0.00392
2024-12-15 05:56:49.494448: Validation loss did not improve from -0.42616. Patience: 69/50
2024-12-15 05:56:49.495664: train_loss -0.8062
2024-12-15 05:56:49.496658: val_loss -0.3465
2024-12-15 05:56:49.497578: Pseudo dice [0.6941]
2024-12-15 05:56:49.498405: Epoch time: 539.83 s
2024-12-15 05:56:49.499220: Yayy! New best EMA pseudo Dice: 0.6888
2024-12-15 05:56:51.270293: 
2024-12-15 05:56:51.271849: Epoch 98
2024-12-15 05:56:51.272944: Current learning rate: 0.00385
2024-12-15 06:05:50.819410: Validation loss did not improve from -0.42616. Patience: 70/50
2024-12-15 06:05:50.820438: train_loss -0.8039
2024-12-15 06:05:50.821232: val_loss -0.3684
2024-12-15 06:05:50.821971: Pseudo dice [0.6931]
2024-12-15 06:05:50.822704: Epoch time: 539.55 s
2024-12-15 06:05:50.823433: Yayy! New best EMA pseudo Dice: 0.6892
2024-12-15 06:05:52.744589: 
2024-12-15 06:05:52.746016: Epoch 99
2024-12-15 06:05:52.746924: Current learning rate: 0.00379
2024-12-15 06:14:52.078883: Validation loss did not improve from -0.42616. Patience: 71/50
2024-12-15 06:14:52.079858: train_loss -0.8058
2024-12-15 06:14:52.080704: val_loss -0.3884
2024-12-15 06:14:52.081615: Pseudo dice [0.7004]
2024-12-15 06:14:52.082441: Epoch time: 539.34 s
2024-12-15 06:14:52.477664: Yayy! New best EMA pseudo Dice: 0.6903
2024-12-15 06:14:54.281903: 
2024-12-15 06:14:54.284111: Epoch 100
2024-12-15 06:14:54.285619: Current learning rate: 0.00372
2024-12-15 06:24:06.326847: Validation loss did not improve from -0.42616. Patience: 72/50
2024-12-15 06:24:06.327828: train_loss -0.8039
2024-12-15 06:24:06.329112: val_loss -0.368
2024-12-15 06:24:06.330011: Pseudo dice [0.684]
2024-12-15 06:24:06.330692: Epoch time: 552.05 s
2024-12-15 06:24:07.739549: 
2024-12-15 06:24:07.740985: Epoch 101
2024-12-15 06:24:07.741747: Current learning rate: 0.00365
2024-12-15 06:32:55.879365: Validation loss did not improve from -0.42616. Patience: 73/50
2024-12-15 06:32:55.880266: train_loss -0.8079
2024-12-15 06:32:55.881019: val_loss -0.3482
2024-12-15 06:32:55.881707: Pseudo dice [0.6942]
2024-12-15 06:32:55.882412: Epoch time: 528.14 s
2024-12-15 06:32:57.264777: 
2024-12-15 06:32:57.266137: Epoch 102
2024-12-15 06:32:57.266937: Current learning rate: 0.00359
2024-12-15 06:41:42.575234: Validation loss did not improve from -0.42616. Patience: 74/50
2024-12-15 06:41:42.576443: train_loss -0.8042
2024-12-15 06:41:42.577419: val_loss -0.3421
2024-12-15 06:41:42.578480: Pseudo dice [0.6721]
2024-12-15 06:41:42.579467: Epoch time: 525.31 s
2024-12-15 06:41:44.002496: 
2024-12-15 06:41:44.004207: Epoch 103
2024-12-15 06:41:44.005292: Current learning rate: 0.00352
2024-12-15 06:50:44.900532: Validation loss did not improve from -0.42616. Patience: 75/50
2024-12-15 06:50:44.901718: train_loss -0.8087
2024-12-15 06:50:44.902639: val_loss -0.3416
2024-12-15 06:50:44.903427: Pseudo dice [0.6744]
2024-12-15 06:50:44.904201: Epoch time: 540.9 s
2024-12-15 06:50:46.426168: 
2024-12-15 06:50:46.427485: Epoch 104
2024-12-15 06:50:46.428207: Current learning rate: 0.00345
2024-12-15 06:59:58.072324: Validation loss did not improve from -0.42616. Patience: 76/50
2024-12-15 06:59:58.073398: train_loss -0.807
2024-12-15 06:59:58.074562: val_loss -0.3537
2024-12-15 06:59:58.075460: Pseudo dice [0.6892]
2024-12-15 06:59:58.076389: Epoch time: 551.65 s
2024-12-15 07:00:00.014469: 
2024-12-15 07:00:00.016330: Epoch 105
2024-12-15 07:00:00.017380: Current learning rate: 0.00338
2024-12-15 07:08:58.114742: Validation loss did not improve from -0.42616. Patience: 77/50
2024-12-15 07:08:58.115728: train_loss -0.8069
2024-12-15 07:08:58.116624: val_loss -0.3364
2024-12-15 07:08:58.117519: Pseudo dice [0.6831]
2024-12-15 07:08:58.118481: Epoch time: 538.1 s
2024-12-15 07:09:00.314259: 
2024-12-15 07:09:00.315724: Epoch 106
2024-12-15 07:09:00.316529: Current learning rate: 0.00332
2024-12-15 07:17:51.047026: Validation loss did not improve from -0.42616. Patience: 78/50
2024-12-15 07:17:51.048145: train_loss -0.8078
2024-12-15 07:17:51.049079: val_loss -0.3517
2024-12-15 07:17:51.049918: Pseudo dice [0.6796]
2024-12-15 07:17:51.050812: Epoch time: 530.74 s
2024-12-15 07:17:52.576164: 
2024-12-15 07:17:52.577584: Epoch 107
2024-12-15 07:17:52.578426: Current learning rate: 0.00325
2024-12-15 07:26:49.156832: Validation loss did not improve from -0.42616. Patience: 79/50
2024-12-15 07:26:49.157967: train_loss -0.8086
2024-12-15 07:26:49.158815: val_loss -0.3519
2024-12-15 07:26:49.159469: Pseudo dice [0.6807]
2024-12-15 07:26:49.160171: Epoch time: 536.58 s
2024-12-15 07:26:50.596733: 
2024-12-15 07:26:50.598110: Epoch 108
2024-12-15 07:26:50.598937: Current learning rate: 0.00318
2024-12-15 07:35:47.176286: Validation loss did not improve from -0.42616. Patience: 80/50
2024-12-15 07:35:47.178454: train_loss -0.813
2024-12-15 07:35:47.179524: val_loss -0.3647
2024-12-15 07:35:47.180205: Pseudo dice [0.6926]
2024-12-15 07:35:47.181185: Epoch time: 536.58 s
2024-12-15 07:35:48.619886: 
2024-12-15 07:35:48.621336: Epoch 109
2024-12-15 07:35:48.622174: Current learning rate: 0.00311
2024-12-15 07:44:31.182727: Validation loss did not improve from -0.42616. Patience: 81/50
2024-12-15 07:44:31.183807: train_loss -0.8118
2024-12-15 07:44:31.184777: val_loss -0.3064
2024-12-15 07:44:31.185620: Pseudo dice [0.6727]
2024-12-15 07:44:31.186469: Epoch time: 522.57 s
2024-12-15 07:44:32.967376: 
2024-12-15 07:44:32.968836: Epoch 110
2024-12-15 07:44:32.969672: Current learning rate: 0.00304
2024-12-15 07:53:39.169189: Validation loss did not improve from -0.42616. Patience: 82/50
2024-12-15 07:53:39.170469: train_loss -0.8117
2024-12-15 07:53:39.171329: val_loss -0.3714
2024-12-15 07:53:39.172302: Pseudo dice [0.7032]
2024-12-15 07:53:39.173191: Epoch time: 546.2 s
2024-12-15 07:53:40.645566: 
2024-12-15 07:53:40.646924: Epoch 111
2024-12-15 07:53:40.647784: Current learning rate: 0.00297
2024-12-15 08:02:23.062076: Validation loss did not improve from -0.42616. Patience: 83/50
2024-12-15 08:02:23.063044: train_loss -0.8107
2024-12-15 08:02:23.063875: val_loss -0.394
2024-12-15 08:02:23.064617: Pseudo dice [0.705]
2024-12-15 08:02:23.065443: Epoch time: 522.42 s
2024-12-15 08:02:24.446282: 
2024-12-15 08:02:24.447423: Epoch 112
2024-12-15 08:02:24.448210: Current learning rate: 0.00291
2024-12-15 08:11:00.618736: Validation loss did not improve from -0.42616. Patience: 84/50
2024-12-15 08:11:00.620154: train_loss -0.8131
2024-12-15 08:11:00.621345: val_loss -0.3578
2024-12-15 08:11:00.622429: Pseudo dice [0.6959]
2024-12-15 08:11:00.623434: Epoch time: 516.18 s
2024-12-15 08:11:02.007377: 
2024-12-15 08:11:02.009042: Epoch 113
2024-12-15 08:11:02.010160: Current learning rate: 0.00284
2024-12-15 08:19:51.773312: Validation loss did not improve from -0.42616. Patience: 85/50
2024-12-15 08:19:51.774290: train_loss -0.8158
2024-12-15 08:19:51.775090: val_loss -0.3388
2024-12-15 08:19:51.775805: Pseudo dice [0.682]
2024-12-15 08:19:51.776575: Epoch time: 529.77 s
2024-12-15 08:19:53.171116: 
2024-12-15 08:19:53.172606: Epoch 114
2024-12-15 08:19:53.173429: Current learning rate: 0.00277
2024-12-15 08:28:35.617253: Validation loss did not improve from -0.42616. Patience: 86/50
2024-12-15 08:28:35.618285: train_loss -0.8156
2024-12-15 08:28:35.619053: val_loss -0.388
2024-12-15 08:28:35.619648: Pseudo dice [0.7022]
2024-12-15 08:28:35.620284: Epoch time: 522.45 s
2024-12-15 08:28:37.409209: 
2024-12-15 08:28:37.410620: Epoch 115
2024-12-15 08:28:37.411437: Current learning rate: 0.0027
2024-12-15 08:37:32.912021: Validation loss did not improve from -0.42616. Patience: 87/50
2024-12-15 08:37:32.913122: train_loss -0.8165
2024-12-15 08:37:32.914126: val_loss -0.3378
2024-12-15 08:37:32.914908: Pseudo dice [0.6879]
2024-12-15 08:37:32.915737: Epoch time: 535.51 s
2024-12-15 08:37:34.330154: 
2024-12-15 08:37:34.331740: Epoch 116
2024-12-15 08:37:34.332703: Current learning rate: 0.00263
2024-12-15 08:45:56.072634: Validation loss did not improve from -0.42616. Patience: 88/50
2024-12-15 08:45:56.073859: train_loss -0.816
2024-12-15 08:45:56.074730: val_loss -0.3695
2024-12-15 08:45:56.075848: Pseudo dice [0.6972]
2024-12-15 08:45:56.076854: Epoch time: 501.75 s
2024-12-15 08:45:56.077846: Yayy! New best EMA pseudo Dice: 0.6905
2024-12-15 08:45:58.421443: 
2024-12-15 08:45:58.423417: Epoch 117
2024-12-15 08:45:58.424385: Current learning rate: 0.00256
2024-12-15 08:54:16.009811: Validation loss did not improve from -0.42616. Patience: 89/50
2024-12-15 08:54:16.010873: train_loss -0.8186
2024-12-15 08:54:16.011881: val_loss -0.388
2024-12-15 08:54:16.012764: Pseudo dice [0.7069]
2024-12-15 08:54:16.013752: Epoch time: 497.59 s
2024-12-15 08:54:16.014789: Yayy! New best EMA pseudo Dice: 0.6921
2024-12-15 08:54:17.902025: 
2024-12-15 08:54:17.903615: Epoch 118
2024-12-15 08:54:17.904666: Current learning rate: 0.00249
2024-12-15 09:02:27.131824: Validation loss did not improve from -0.42616. Patience: 90/50
2024-12-15 09:02:27.133041: train_loss -0.8161
2024-12-15 09:02:27.134056: val_loss -0.3436
2024-12-15 09:02:27.135006: Pseudo dice [0.6896]
2024-12-15 09:02:27.135971: Epoch time: 489.23 s
2024-12-15 09:02:28.540733: 
2024-12-15 09:02:28.542159: Epoch 119
2024-12-15 09:02:28.543039: Current learning rate: 0.00242
2024-12-15 09:10:30.646643: Validation loss did not improve from -0.42616. Patience: 91/50
2024-12-15 09:10:30.647709: train_loss -0.8185
2024-12-15 09:10:30.648598: val_loss -0.3841
2024-12-15 09:10:30.649399: Pseudo dice [0.7012]
2024-12-15 09:10:30.650061: Epoch time: 482.11 s
2024-12-15 09:10:31.064637: Yayy! New best EMA pseudo Dice: 0.6928
2024-12-15 09:10:32.881727: 
2024-12-15 09:10:32.883641: Epoch 120
2024-12-15 09:10:32.885197: Current learning rate: 0.00235
2024-12-15 09:18:18.565870: Validation loss did not improve from -0.42616. Patience: 92/50
2024-12-15 09:18:18.567354: train_loss -0.8162
2024-12-15 09:18:18.568494: val_loss -0.3229
2024-12-15 09:18:18.569474: Pseudo dice [0.6755]
2024-12-15 09:18:18.570586: Epoch time: 465.69 s
2024-12-15 09:18:20.001069: 
2024-12-15 09:18:20.002572: Epoch 121
2024-12-15 09:18:20.003658: Current learning rate: 0.00228
2024-12-15 09:25:59.969863: Validation loss did not improve from -0.42616. Patience: 93/50
2024-12-15 09:25:59.970820: train_loss -0.8204
2024-12-15 09:25:59.971632: val_loss -0.3932
2024-12-15 09:25:59.972282: Pseudo dice [0.7021]
2024-12-15 09:25:59.972968: Epoch time: 459.97 s
2024-12-15 09:26:01.376996: 
2024-12-15 09:26:01.378255: Epoch 122
2024-12-15 09:26:01.379120: Current learning rate: 0.00221
2024-12-15 09:33:48.700142: Validation loss did not improve from -0.42616. Patience: 94/50
2024-12-15 09:33:48.701054: train_loss -0.8209
2024-12-15 09:33:48.702141: val_loss -0.3574
2024-12-15 09:33:48.703011: Pseudo dice [0.6776]
2024-12-15 09:33:48.704152: Epoch time: 467.33 s
2024-12-15 09:33:50.095321: 
2024-12-15 09:33:50.096944: Epoch 123
2024-12-15 09:33:50.098354: Current learning rate: 0.00214
2024-12-15 09:41:29.538326: Validation loss did not improve from -0.42616. Patience: 95/50
2024-12-15 09:41:29.539347: train_loss -0.8187
2024-12-15 09:41:29.540081: val_loss -0.3608
2024-12-15 09:41:29.540797: Pseudo dice [0.6911]
2024-12-15 09:41:29.541381: Epoch time: 459.45 s
2024-12-15 09:41:30.922907: 
2024-12-15 09:41:30.924077: Epoch 124
2024-12-15 09:41:30.924716: Current learning rate: 0.00207
2024-12-15 09:49:28.279738: Validation loss did not improve from -0.42616. Patience: 96/50
2024-12-15 09:49:28.280824: train_loss -0.8218
2024-12-15 09:49:28.281714: val_loss -0.3207
2024-12-15 09:49:28.282414: Pseudo dice [0.6766]
2024-12-15 09:49:28.283178: Epoch time: 477.36 s
2024-12-15 09:49:30.076585: 
2024-12-15 09:49:30.077949: Epoch 125
2024-12-15 09:49:30.078714: Current learning rate: 0.00199
2024-12-15 09:57:40.784360: Validation loss did not improve from -0.42616. Patience: 97/50
2024-12-15 09:57:40.788230: train_loss -0.8221
2024-12-15 09:57:40.790240: val_loss -0.343
2024-12-15 09:57:40.791057: Pseudo dice [0.6809]
2024-12-15 09:57:40.792748: Epoch time: 490.71 s
2024-12-15 09:57:42.216361: 
2024-12-15 09:57:42.217903: Epoch 126
2024-12-15 09:57:42.218698: Current learning rate: 0.00192
2024-12-15 10:05:48.365052: Validation loss did not improve from -0.42616. Patience: 98/50
2024-12-15 10:05:48.366021: train_loss -0.821
2024-12-15 10:05:48.366956: val_loss -0.3666
2024-12-15 10:05:48.367865: Pseudo dice [0.6987]
2024-12-15 10:05:48.368533: Epoch time: 486.15 s
2024-12-15 10:05:50.262774: 
2024-12-15 10:05:50.264441: Epoch 127
2024-12-15 10:05:50.265471: Current learning rate: 0.00185
2024-12-15 10:14:22.557668: Validation loss did not improve from -0.42616. Patience: 99/50
2024-12-15 10:14:22.558628: train_loss -0.822
2024-12-15 10:14:22.559542: val_loss -0.3459
2024-12-15 10:14:22.560697: Pseudo dice [0.6794]
2024-12-15 10:14:22.561630: Epoch time: 512.3 s
2024-12-15 10:14:23.941721: 
2024-12-15 10:14:23.943043: Epoch 128
2024-12-15 10:14:23.943848: Current learning rate: 0.00178
2024-12-15 10:22:07.559486: Validation loss did not improve from -0.42616. Patience: 100/50
2024-12-15 10:22:07.560462: train_loss -0.8213
2024-12-15 10:22:07.561312: val_loss -0.3686
2024-12-15 10:22:07.562166: Pseudo dice [0.7036]
2024-12-15 10:22:07.563141: Epoch time: 463.62 s
2024-12-15 10:22:08.929648: 
2024-12-15 10:22:08.930967: Epoch 129
2024-12-15 10:22:08.931720: Current learning rate: 0.0017
2024-12-15 10:30:13.053221: Validation loss did not improve from -0.42616. Patience: 101/50
2024-12-15 10:30:13.054123: train_loss -0.8204
2024-12-15 10:30:13.054879: val_loss -0.3507
2024-12-15 10:30:13.055525: Pseudo dice [0.6903]
2024-12-15 10:30:13.056324: Epoch time: 484.13 s
2024-12-15 10:30:14.940365: 
2024-12-15 10:30:14.941665: Epoch 130
2024-12-15 10:30:14.942546: Current learning rate: 0.00163
2024-12-15 10:38:33.868947: Validation loss did not improve from -0.42616. Patience: 102/50
2024-12-15 10:38:33.869924: train_loss -0.8247
2024-12-15 10:38:33.870656: val_loss -0.331
2024-12-15 10:38:33.871358: Pseudo dice [0.6802]
2024-12-15 10:38:33.871995: Epoch time: 498.93 s
2024-12-15 10:38:35.241864: 
2024-12-15 10:38:35.243092: Epoch 131
2024-12-15 10:38:35.243859: Current learning rate: 0.00156
2024-12-15 10:45:47.970268: Validation loss did not improve from -0.42616. Patience: 103/50
2024-12-15 10:45:47.971303: train_loss -0.8226
2024-12-15 10:45:47.972076: val_loss -0.3003
2024-12-15 10:45:47.972743: Pseudo dice [0.6667]
2024-12-15 10:45:47.973460: Epoch time: 432.73 s
2024-12-15 10:45:49.340804: 
2024-12-15 10:45:49.342125: Epoch 132
2024-12-15 10:45:49.342854: Current learning rate: 0.00148
2024-12-15 10:53:32.329563: Validation loss did not improve from -0.42616. Patience: 104/50
2024-12-15 10:53:32.332185: train_loss -0.8268
2024-12-15 10:53:32.333223: val_loss -0.3178
2024-12-15 10:53:32.333911: Pseudo dice [0.6766]
2024-12-15 10:53:32.334930: Epoch time: 462.99 s
2024-12-15 10:53:33.716141: 
2024-12-15 10:53:33.717366: Epoch 133
2024-12-15 10:53:33.718164: Current learning rate: 0.00141
2024-12-15 11:01:29.994876: Validation loss did not improve from -0.42616. Patience: 105/50
2024-12-15 11:01:29.996972: train_loss -0.8261
2024-12-15 11:01:29.998907: val_loss -0.3208
2024-12-15 11:01:29.999716: Pseudo dice [0.6756]
2024-12-15 11:01:30.000715: Epoch time: 476.28 s
2024-12-15 11:01:31.389712: 
2024-12-15 11:01:31.390860: Epoch 134
2024-12-15 11:01:31.391733: Current learning rate: 0.00133
2024-12-15 11:09:44.694624: Validation loss did not improve from -0.42616. Patience: 106/50
2024-12-15 11:09:44.697234: train_loss -0.8269
2024-12-15 11:09:44.697962: val_loss -0.319
2024-12-15 11:09:44.698663: Pseudo dice [0.6851]
2024-12-15 11:09:44.699643: Epoch time: 493.31 s
2024-12-15 11:09:46.519387: 
2024-12-15 11:09:46.520733: Epoch 135
2024-12-15 11:09:46.521678: Current learning rate: 0.00126
2024-12-15 11:17:59.473005: Validation loss did not improve from -0.42616. Patience: 107/50
2024-12-15 11:17:59.474134: train_loss -0.8279
2024-12-15 11:17:59.475016: val_loss -0.319
2024-12-15 11:17:59.475816: Pseudo dice [0.6825]
2024-12-15 11:17:59.476581: Epoch time: 492.96 s
2024-12-15 11:18:00.972453: 
2024-12-15 11:18:00.974086: Epoch 136
2024-12-15 11:18:00.975006: Current learning rate: 0.00118
2024-12-15 11:26:16.603664: Validation loss did not improve from -0.42616. Patience: 108/50
2024-12-15 11:26:16.604935: train_loss -0.8244
2024-12-15 11:26:16.605736: val_loss -0.3603
2024-12-15 11:26:16.606420: Pseudo dice [0.7002]
2024-12-15 11:26:16.607084: Epoch time: 495.63 s
2024-12-15 11:26:18.023132: 
2024-12-15 11:26:18.024460: Epoch 137
2024-12-15 11:26:18.025218: Current learning rate: 0.00111
2024-12-15 11:33:45.047264: Validation loss did not improve from -0.42616. Patience: 109/50
2024-12-15 11:33:45.048348: train_loss -0.8258
2024-12-15 11:33:45.049177: val_loss -0.3658
2024-12-15 11:33:45.049953: Pseudo dice [0.6908]
2024-12-15 11:33:45.050779: Epoch time: 447.03 s
2024-12-15 11:33:47.450228: 
2024-12-15 11:33:47.451598: Epoch 138
2024-12-15 11:33:47.452396: Current learning rate: 0.00103
2024-12-15 11:41:28.048792: Validation loss did not improve from -0.42616. Patience: 110/50
2024-12-15 11:41:28.050129: train_loss -0.8265
2024-12-15 11:41:28.050973: val_loss -0.3089
2024-12-15 11:41:28.051653: Pseudo dice [0.6766]
2024-12-15 11:41:28.052515: Epoch time: 460.6 s
2024-12-15 11:41:29.470933: 
2024-12-15 11:41:29.472289: Epoch 139
2024-12-15 11:41:29.473178: Current learning rate: 0.00095
2024-12-15 11:49:02.137613: Validation loss did not improve from -0.42616. Patience: 111/50
2024-12-15 11:49:02.138541: train_loss -0.8256
2024-12-15 11:49:02.139377: val_loss -0.3351
2024-12-15 11:49:02.140049: Pseudo dice [0.6965]
2024-12-15 11:49:02.140803: Epoch time: 452.67 s
2024-12-15 11:49:03.968704: 
2024-12-15 11:49:03.970016: Epoch 140
2024-12-15 11:49:03.971061: Current learning rate: 0.00087
2024-12-15 11:57:19.769149: Validation loss did not improve from -0.42616. Patience: 112/50
2024-12-15 11:57:19.770167: train_loss -0.8264
2024-12-15 11:57:19.771113: val_loss -0.3501
2024-12-15 11:57:19.772195: Pseudo dice [0.6967]
2024-12-15 11:57:19.773152: Epoch time: 495.8 s
2024-12-15 11:57:21.159447: 
2024-12-15 11:57:21.160867: Epoch 141
2024-12-15 11:57:21.161823: Current learning rate: 0.00079
2024-12-15 12:05:34.987128: Validation loss did not improve from -0.42616. Patience: 113/50
2024-12-15 12:05:34.989804: train_loss -0.8284
2024-12-15 12:05:34.992079: val_loss -0.3583
2024-12-15 12:05:34.992859: Pseudo dice [0.6958]
2024-12-15 12:05:34.993874: Epoch time: 493.83 s
2024-12-15 12:05:36.448834: 
2024-12-15 12:05:36.450301: Epoch 142
2024-12-15 12:05:36.451065: Current learning rate: 0.00071
2024-12-15 12:13:24.636648: Validation loss did not improve from -0.42616. Patience: 114/50
2024-12-15 12:13:24.639065: train_loss -0.8284
2024-12-15 12:13:24.639969: val_loss -0.3776
2024-12-15 12:13:24.640924: Pseudo dice [0.7052]
2024-12-15 12:13:24.641999: Epoch time: 468.19 s
2024-12-15 12:13:26.045865: 
2024-12-15 12:13:26.047190: Epoch 143
2024-12-15 12:13:26.047874: Current learning rate: 0.00063
2024-12-15 12:21:39.156647: Validation loss did not improve from -0.42616. Patience: 115/50
2024-12-15 12:21:39.158759: train_loss -0.8308
2024-12-15 12:21:39.160161: val_loss -0.3961
2024-12-15 12:21:39.160872: Pseudo dice [0.7058]
2024-12-15 12:21:39.161514: Epoch time: 493.11 s
2024-12-15 12:21:40.564664: 
2024-12-15 12:21:40.565938: Epoch 144
2024-12-15 12:21:40.566609: Current learning rate: 0.00055
2024-12-15 12:29:41.460978: Validation loss did not improve from -0.42616. Patience: 116/50
2024-12-15 12:29:41.462092: train_loss -0.8291
2024-12-15 12:29:41.462942: val_loss -0.3066
2024-12-15 12:29:41.463809: Pseudo dice [0.6813]
2024-12-15 12:29:41.464723: Epoch time: 480.9 s
2024-12-15 12:29:43.294205: 
2024-12-15 12:29:43.295779: Epoch 145
2024-12-15 12:29:43.296737: Current learning rate: 0.00047
2024-12-15 12:37:55.571831: Validation loss did not improve from -0.42616. Patience: 117/50
2024-12-15 12:37:55.572633: train_loss -0.8287
2024-12-15 12:37:55.573368: val_loss -0.3563
2024-12-15 12:37:55.574052: Pseudo dice [0.6921]
2024-12-15 12:37:55.574728: Epoch time: 492.28 s
2024-12-15 12:37:56.977859: 
2024-12-15 12:37:56.979074: Epoch 146
2024-12-15 12:37:56.980002: Current learning rate: 0.00038
2024-12-15 12:44:42.925683: Validation loss did not improve from -0.42616. Patience: 118/50
2024-12-15 12:44:42.926708: train_loss -0.8301
2024-12-15 12:44:42.927454: val_loss -0.33
2024-12-15 12:44:42.928100: Pseudo dice [0.6849]
2024-12-15 12:44:42.928859: Epoch time: 405.95 s
2024-12-15 12:44:44.354009: 
2024-12-15 12:44:44.355718: Epoch 147
2024-12-15 12:44:44.356528: Current learning rate: 0.0003
2024-12-15 12:51:19.199258: Validation loss did not improve from -0.42616. Patience: 119/50
2024-12-15 12:51:19.200275: train_loss -0.8285
2024-12-15 12:51:19.201303: val_loss -0.3466
2024-12-15 12:51:19.202251: Pseudo dice [0.6884]
2024-12-15 12:51:19.203417: Epoch time: 394.85 s
2024-12-15 12:51:20.603994: 
2024-12-15 12:51:20.605496: Epoch 148
2024-12-15 12:51:20.606444: Current learning rate: 0.00021
2024-12-15 12:58:06.963120: Validation loss did not improve from -0.42616. Patience: 120/50
2024-12-15 12:58:06.964015: train_loss -0.8293
2024-12-15 12:58:06.964949: val_loss -0.3465
2024-12-15 12:58:06.965620: Pseudo dice [0.69]
2024-12-15 12:58:06.966417: Epoch time: 406.36 s
2024-12-15 12:58:08.880766: 
2024-12-15 12:58:08.882365: Epoch 149
2024-12-15 12:58:08.883280: Current learning rate: 0.00011
2024-12-15 13:04:27.304252: Validation loss did not improve from -0.42616. Patience: 121/50
2024-12-15 13:04:27.306523: train_loss -0.8299
2024-12-15 13:04:27.308428: val_loss -0.3426
2024-12-15 13:04:27.309739: Pseudo dice [0.6859]
2024-12-15 13:04:27.311151: Epoch time: 378.43 s
2024-12-15 13:04:29.227602: Training done.
2024-12-15 13:04:29.417618: Using splits from existing split file: /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset307_Sohee_Calcium_OCT_CrossValidation/splits_final_40.json
2024-12-15 13:04:29.420484: The split file contains 5 splits.
2024-12-15 13:04:29.421744: Desired fold for training: 3
2024-12-15 13:04:29.422800: This split has 3 training and 6 validation cases.
2024-12-15 13:04:29.424273: predicting 101-044
2024-12-15 13:04:29.456919: 101-044, shape torch.Size([1, 404, 498, 498]), rank 0
2024-12-15 13:07:08.315027: predicting 101-045
2024-12-15 13:07:08.336305: 101-045, shape torch.Size([1, 375, 498, 498]), rank 0
2024-12-15 13:09:15.300225: predicting 106-002
2024-12-15 13:09:15.324020: 106-002, shape torch.Size([1, 539, 498, 498]), rank 0
2024-12-15 13:12:30.748320: predicting 401-004
2024-12-15 13:12:30.763095: 401-004, shape torch.Size([1, 375, 498, 498]), rank 0
2024-12-15 13:14:39.049629: predicting 704-003
2024-12-15 13:14:39.063165: 704-003, shape torch.Size([1, 375, 498, 498]), rank 0
2024-12-15 13:16:42.390192: predicting 706-005
2024-12-15 13:16:42.402062: 706-005, shape torch.Size([1, 375, 498, 498]), rank 0
2024-12-15 13:19:17.846047: Validation complete
2024-12-15 13:19:17.847161: Mean Validation Dice:  0.6627968286657103

############################
INFO: You are using the old nnU-Net default plans. We have updated our recommendations. Please consider using those instead! Read more here: https://github.com/MIC-DKFZ/nnUNet/blob/master/documentation/resenc_presets.md
############################

Using device: cuda:0

#######################################################################
Please cite the following paper when using nnU-Net:
Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.
#######################################################################

2024-12-15 13:19:25.923388: Using torch.compile...
/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
################### Loading pretrained weights from file  /home/gridsan/nchutisilp/projects/OpenAI-CLIP/clip_preivl_poststent_pretrained_nnUNet.pt ###################
Below is the list of overlapping blocks in pretrained model and nnUNet architecture:
encoder.stages.0.0.convs.0.conv.weight shape torch.Size([32, 1, 3, 3, 3])
encoder.stages.0.0.convs.0.conv.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.norm.weight shape torch.Size([32])
encoder.stages.0.0.convs.0.norm.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.0.weight shape torch.Size([32, 1, 3, 3, 3])
encoder.stages.0.0.convs.0.all_modules.0.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.1.weight shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.1.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
encoder.stages.0.0.convs.1.conv.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.norm.weight shape torch.Size([32])
encoder.stages.0.0.convs.1.norm.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
encoder.stages.0.0.convs.1.all_modules.0.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.1.weight shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.1.bias shape torch.Size([32])
encoder.stages.1.0.convs.0.conv.weight shape torch.Size([64, 32, 3, 3, 3])
encoder.stages.1.0.convs.0.conv.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.norm.weight shape torch.Size([64])
encoder.stages.1.0.convs.0.norm.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.0.weight shape torch.Size([64, 32, 3, 3, 3])
encoder.stages.1.0.convs.0.all_modules.0.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.1.weight shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.1.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
encoder.stages.1.0.convs.1.conv.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.norm.weight shape torch.Size([64])
encoder.stages.1.0.convs.1.norm.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
encoder.stages.1.0.convs.1.all_modules.0.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.1.weight shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.1.bias shape torch.Size([64])
encoder.stages.2.0.convs.0.conv.weight shape torch.Size([128, 64, 3, 3, 3])
encoder.stages.2.0.convs.0.conv.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.norm.weight shape torch.Size([128])
encoder.stages.2.0.convs.0.norm.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.0.weight shape torch.Size([128, 64, 3, 3, 3])
encoder.stages.2.0.convs.0.all_modules.0.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.1.weight shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.1.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
encoder.stages.2.0.convs.1.conv.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.norm.weight shape torch.Size([128])
encoder.stages.2.0.convs.1.norm.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
encoder.stages.2.0.convs.1.all_modules.0.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.1.weight shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.1.bias shape torch.Size([128])
encoder.stages.3.0.convs.0.conv.weight shape torch.Size([256, 128, 3, 3, 3])
encoder.stages.3.0.convs.0.conv.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.norm.weight shape torch.Size([256])
encoder.stages.3.0.convs.0.norm.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.0.weight shape torch.Size([256, 128, 3, 3, 3])
encoder.stages.3.0.convs.0.all_modules.0.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.1.weight shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.1.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
encoder.stages.3.0.convs.1.conv.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.norm.weight shape torch.Size([256])
encoder.stages.3.0.convs.1.norm.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
encoder.stages.3.0.convs.1.all_modules.0.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.1.weight shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.1.bias shape torch.Size([256])
encoder.stages.4.0.convs.0.conv.weight shape torch.Size([320, 256, 3, 3, 3])
encoder.stages.4.0.convs.0.conv.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.norm.weight shape torch.Size([320])
encoder.stages.4.0.convs.0.norm.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.0.weight shape torch.Size([320, 256, 3, 3, 3])
encoder.stages.4.0.convs.0.all_modules.0.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.1.weight shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.1.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.4.0.convs.1.conv.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.norm.weight shape torch.Size([320])
encoder.stages.4.0.convs.1.norm.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.4.0.convs.1.all_modules.0.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.1.weight shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.1.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.0.conv.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.norm.weight shape torch.Size([320])
encoder.stages.5.0.convs.0.norm.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.0.all_modules.0.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.1.weight shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.1.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.1.conv.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.norm.weight shape torch.Size([320])
encoder.stages.5.0.convs.1.norm.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.1.all_modules.0.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.1.weight shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.0.0.convs.0.conv.weight shape torch.Size([32, 1, 3, 3, 3])
decoder.encoder.stages.0.0.convs.0.conv.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.norm.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.norm.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.0.weight shape torch.Size([32, 1, 3, 3, 3])
decoder.encoder.stages.0.0.convs.0.all_modules.0.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.1.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.1.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.encoder.stages.0.0.convs.1.conv.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.norm.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.norm.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.encoder.stages.0.0.convs.1.all_modules.0.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.1.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.1.bias shape torch.Size([32])
decoder.encoder.stages.1.0.convs.0.conv.weight shape torch.Size([64, 32, 3, 3, 3])
decoder.encoder.stages.1.0.convs.0.conv.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.norm.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.norm.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.0.weight shape torch.Size([64, 32, 3, 3, 3])
decoder.encoder.stages.1.0.convs.0.all_modules.0.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.1.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.1.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.encoder.stages.1.0.convs.1.conv.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.norm.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.norm.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.encoder.stages.1.0.convs.1.all_modules.0.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.1.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.1.bias shape torch.Size([64])
decoder.encoder.stages.2.0.convs.0.conv.weight shape torch.Size([128, 64, 3, 3, 3])
decoder.encoder.stages.2.0.convs.0.conv.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.norm.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.norm.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.0.weight shape torch.Size([128, 64, 3, 3, 3])
decoder.encoder.stages.2.0.convs.0.all_modules.0.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.1.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.1.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.encoder.stages.2.0.convs.1.conv.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.norm.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.norm.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.encoder.stages.2.0.convs.1.all_modules.0.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.1.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.1.bias shape torch.Size([128])
decoder.encoder.stages.3.0.convs.0.conv.weight shape torch.Size([256, 128, 3, 3, 3])
decoder.encoder.stages.3.0.convs.0.conv.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.norm.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.norm.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.0.weight shape torch.Size([256, 128, 3, 3, 3])
decoder.encoder.stages.3.0.convs.0.all_modules.0.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.1.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.1.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.encoder.stages.3.0.convs.1.conv.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.norm.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.norm.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.encoder.stages.3.0.convs.1.all_modules.0.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.1.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.1.bias shape torch.Size([256])
decoder.encoder.stages.4.0.convs.0.conv.weight shape torch.Size([320, 256, 3, 3, 3])
decoder.encoder.stages.4.0.convs.0.conv.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.norm.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.norm.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.0.weight shape torch.Size([320, 256, 3, 3, 3])
decoder.encoder.stages.4.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.4.0.convs.1.conv.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.norm.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.norm.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.4.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.0.conv.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.norm.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.norm.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.1.conv.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.norm.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.norm.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.stages.0.convs.0.conv.weight shape torch.Size([320, 640, 3, 3, 3])
decoder.stages.0.convs.0.conv.bias shape torch.Size([320])
decoder.stages.0.convs.0.norm.weight shape torch.Size([320])
decoder.stages.0.convs.0.norm.bias shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.0.weight shape torch.Size([320, 640, 3, 3, 3])
decoder.stages.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.stages.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.stages.0.convs.1.conv.bias shape torch.Size([320])
decoder.stages.0.convs.1.norm.weight shape torch.Size([320])
decoder.stages.0.convs.1.norm.bias shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.stages.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.stages.1.convs.0.conv.weight shape torch.Size([256, 512, 3, 3, 3])
decoder.stages.1.convs.0.conv.bias shape torch.Size([256])
decoder.stages.1.convs.0.norm.weight shape torch.Size([256])
decoder.stages.1.convs.0.norm.bias shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.0.weight shape torch.Size([256, 512, 3, 3, 3])
decoder.stages.1.convs.0.all_modules.0.bias shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.1.weight shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.1.bias shape torch.Size([256])
decoder.stages.1.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.stages.1.convs.1.conv.bias shape torch.Size([256])
decoder.stages.1.convs.1.norm.weight shape torch.Size([256])
decoder.stages.1.convs.1.norm.bias shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.stages.1.convs.1.all_modules.0.bias shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.1.weight shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.1.bias shape torch.Size([256])
decoder.stages.2.convs.0.conv.weight shape torch.Size([128, 256, 3, 3, 3])
decoder.stages.2.convs.0.conv.bias shape torch.Size([128])
decoder.stages.2.convs.0.norm.weight shape torch.Size([128])
decoder.stages.2.convs.0.norm.bias shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.0.weight shape torch.Size([128, 256, 3, 3, 3])
decoder.stages.2.convs.0.all_modules.0.bias shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.1.weight shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.1.bias shape torch.Size([128])
decoder.stages.2.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.stages.2.convs.1.conv.bias shape torch.Size([128])
decoder.stages.2.convs.1.norm.weight shape torch.Size([128])
decoder.stages.2.convs.1.norm.bias shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.stages.2.convs.1.all_modules.0.bias shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.1.weight shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.1.bias shape torch.Size([128])
decoder.stages.3.convs.0.conv.weight shape torch.Size([64, 128, 3, 3, 3])
decoder.stages.3.convs.0.conv.bias shape torch.Size([64])
decoder.stages.3.convs.0.norm.weight shape torch.Size([64])
decoder.stages.3.convs.0.norm.bias shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.0.weight shape torch.Size([64, 128, 3, 3, 3])
decoder.stages.3.convs.0.all_modules.0.bias shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.1.weight shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.1.bias shape torch.Size([64])
decoder.stages.3.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.stages.3.convs.1.conv.bias shape torch.Size([64])
decoder.stages.3.convs.1.norm.weight shape torch.Size([64])
decoder.stages.3.convs.1.norm.bias shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.stages.3.convs.1.all_modules.0.bias shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.1.weight shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.1.bias shape torch.Size([64])
decoder.stages.4.convs.0.conv.weight shape torch.Size([32, 64, 3, 3, 3])
decoder.stages.4.convs.0.conv.bias shape torch.Size([32])
decoder.stages.4.convs.0.norm.weight shape torch.Size([32])
decoder.stages.4.convs.0.norm.bias shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.0.weight shape torch.Size([32, 64, 3, 3, 3])
decoder.stages.4.convs.0.all_modules.0.bias shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.1.weight shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.1.bias shape torch.Size([32])
decoder.stages.4.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.stages.4.convs.1.conv.bias shape torch.Size([32])
decoder.stages.4.convs.1.norm.weight shape torch.Size([32])
decoder.stages.4.convs.1.norm.bias shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.stages.4.convs.1.all_modules.0.bias shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.1.weight shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.1.bias shape torch.Size([32])
decoder.transpconvs.0.weight shape torch.Size([320, 320, 1, 2, 2])
decoder.transpconvs.0.bias shape torch.Size([320])
decoder.transpconvs.1.weight shape torch.Size([320, 256, 2, 2, 2])
decoder.transpconvs.1.bias shape torch.Size([256])
decoder.transpconvs.2.weight shape torch.Size([256, 128, 2, 2, 2])
decoder.transpconvs.2.bias shape torch.Size([128])
decoder.transpconvs.3.weight shape torch.Size([128, 64, 2, 2, 2])
decoder.transpconvs.3.bias shape torch.Size([64])
decoder.transpconvs.4.weight shape torch.Size([64, 32, 2, 2, 2])
decoder.transpconvs.4.bias shape torch.Size([32])
################### Done ###################
2024-12-15 13:19:28.546455: do_dummy_2d_data_aug: True
2024-12-15 13:19:28.548136: Using splits from existing split file: /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset307_Sohee_Calcium_OCT_CrossValidation/splits_final_40.json
2024-12-15 13:19:28.550535: The split file contains 5 splits.
2024-12-15 13:19:28.551908: Desired fold for training: 4
2024-12-15 13:19:28.553087: This split has 3 training and 5 validation cases.
using pin_memory on device 0
using pin_memory on device 0

This is the configuration used by this training:
Configuration name: 3d_32x160x128_b10
 {'data_identifier': 'nnUNetPlans_3d_fullres', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 10, 'patch_size': [32, 160, 128], 'median_image_size_in_voxels': [375.0, 498.0, 498.0], 'spacing': [1.0, 1.0, 1.0], 'normalization_schemes': ['ZScoreNormalization'], 'use_mask_for_norm': [False], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'architecture': {'network_class_name': 'dynamic_network_architectures.architectures.unet.PlainConvUNet', 'arch_kwargs': {'n_stages': 6, 'features_per_stage': [32, 64, 128, 256, 320, 320], 'conv_op': 'torch.nn.modules.conv.Conv3d', 'kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]], 'strides': [[1, 1, 1], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [1, 2, 2]], 'n_conv_per_stage': [2, 2, 2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2, 2, 2], 'conv_bias': True, 'norm_op': 'torch.nn.modules.instancenorm.InstanceNorm3d', 'norm_op_kwargs': {'eps': 1e-05, 'affine': True}, 'dropout_op': None, 'dropout_op_kwargs': None, 'nonlin': 'torch.nn.LeakyReLU', 'nonlin_kwargs': {'inplace': True}, 'deep_supervision': True}, '_kw_requires_import': ['conv_op', 'norm_op', 'dropout_op', 'nonlin']}, 'batch_dice': True, 'inherits_from': '3d_fullres'} 

These are the global plan.json settings:
 {'dataset_name': 'Dataset307_Sohee_Calcium_OCT_CrossValidation', 'plans_name': 'nnUNetPlans', 'original_median_spacing_after_transp': [1.0, 1.0, 1.0], 'original_median_shape_after_transp': [375, 498, 498], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 0.4977976381778717, 'mean': 0.13507525622844696, 'median': 0.09599608182907104, 'min': 0.0, 'percentile_00_5': 0.014754901640117168, 'percentile_99_5': 0.4977976381778717, 'std': 0.12025152146816254}}} 

2024-12-15 13:19:59.417102: unpacking dataset...
2024-12-15 13:20:03.729457: unpacking done...
2024-12-15 13:20:03.928260: Unable to plot network architecture: nnUNet_compile is enabled!
2024-12-15 13:20:04.010859: 
2024-12-15 13:20:04.011878: Epoch 0
2024-12-15 13:20:04.012923: Current learning rate: 0.01
2024-12-15 13:27:38.448618: Validation loss improved from 1000.00000 to -0.18581! Patience: 0/50
2024-12-15 13:27:38.449866: train_loss -0.072
2024-12-15 13:27:38.450917: val_loss -0.1858
2024-12-15 13:27:38.451907: Pseudo dice [0.5278]
2024-12-15 13:27:38.452828: Epoch time: 454.44 s
2024-12-15 13:27:38.453798: Yayy! New best EMA pseudo Dice: 0.5278
2024-12-15 13:27:39.987146: 
2024-12-15 13:27:39.988464: Epoch 1
2024-12-15 13:27:39.989444: Current learning rate: 0.00994
2024-12-15 13:34:36.259369: Validation loss improved from -0.18581 to -0.19623! Patience: 0/50
2024-12-15 13:34:36.260385: train_loss -0.234
2024-12-15 13:34:36.261790: val_loss -0.1962
2024-12-15 13:34:36.262614: Pseudo dice [0.5243]
2024-12-15 13:34:36.263727: Epoch time: 416.27 s
2024-12-15 13:34:37.576116: 
2024-12-15 13:34:37.577259: Epoch 2
2024-12-15 13:34:37.578042: Current learning rate: 0.00988
2024-12-15 13:41:38.163347: Validation loss improved from -0.19623 to -0.20907! Patience: 0/50
2024-12-15 13:41:38.164140: train_loss -0.2689
2024-12-15 13:41:38.165151: val_loss -0.2091
2024-12-15 13:41:38.166082: Pseudo dice [0.5475]
2024-12-15 13:41:38.167279: Epoch time: 420.59 s
2024-12-15 13:41:38.168372: Yayy! New best EMA pseudo Dice: 0.5294
2024-12-15 13:41:39.931805: 
2024-12-15 13:41:39.933177: Epoch 3
2024-12-15 13:41:39.933976: Current learning rate: 0.00982
2024-12-15 13:48:45.671781: Validation loss improved from -0.20907 to -0.26969! Patience: 0/50
2024-12-15 13:48:45.672824: train_loss -0.2917
2024-12-15 13:48:45.673678: val_loss -0.2697
2024-12-15 13:48:45.674481: Pseudo dice [0.5912]
2024-12-15 13:48:45.675128: Epoch time: 425.74 s
2024-12-15 13:48:45.675772: Yayy! New best EMA pseudo Dice: 0.5356
2024-12-15 13:48:47.395318: 
2024-12-15 13:48:47.396636: Epoch 4
2024-12-15 13:48:47.397454: Current learning rate: 0.00976
2024-12-15 13:56:10.448903: Validation loss improved from -0.26969 to -0.30262! Patience: 0/50
2024-12-15 13:56:10.449998: train_loss -0.3533
2024-12-15 13:56:10.450826: val_loss -0.3026
2024-12-15 13:56:10.451502: Pseudo dice [0.6046]
2024-12-15 13:56:10.452156: Epoch time: 443.06 s
2024-12-15 13:56:10.833607: Yayy! New best EMA pseudo Dice: 0.5425
2024-12-15 13:56:12.573772: 
2024-12-15 13:56:12.575148: Epoch 5
2024-12-15 13:56:12.576072: Current learning rate: 0.0097
2024-12-15 14:03:23.991937: Validation loss improved from -0.30262 to -0.31770! Patience: 0/50
2024-12-15 14:03:23.992912: train_loss -0.3664
2024-12-15 14:03:23.993797: val_loss -0.3177
2024-12-15 14:03:23.994570: Pseudo dice [0.6139]
2024-12-15 14:03:23.995275: Epoch time: 431.42 s
2024-12-15 14:03:23.995953: Yayy! New best EMA pseudo Dice: 0.5496
2024-12-15 14:03:25.710721: 
2024-12-15 14:03:25.712151: Epoch 6
2024-12-15 14:03:25.713489: Current learning rate: 0.00964
2024-12-15 14:10:40.563481: Validation loss improved from -0.31770 to -0.32160! Patience: 0/50
2024-12-15 14:10:40.565236: train_loss -0.397
2024-12-15 14:10:40.566240: val_loss -0.3216
2024-12-15 14:10:40.566945: Pseudo dice [0.6249]
2024-12-15 14:10:40.567577: Epoch time: 434.86 s
2024-12-15 14:10:40.568349: Yayy! New best EMA pseudo Dice: 0.5572
2024-12-15 14:10:42.268679: 
2024-12-15 14:10:42.270150: Epoch 7
2024-12-15 14:10:42.270922: Current learning rate: 0.00958
2024-12-15 14:18:22.044683: Validation loss improved from -0.32160 to -0.38437! Patience: 0/50
2024-12-15 14:18:22.045588: train_loss -0.4277
2024-12-15 14:18:22.046360: val_loss -0.3844
2024-12-15 14:18:22.047141: Pseudo dice [0.6563]
2024-12-15 14:18:22.048132: Epoch time: 459.78 s
2024-12-15 14:18:22.049362: Yayy! New best EMA pseudo Dice: 0.5671
2024-12-15 14:18:23.749372: 
2024-12-15 14:18:23.750663: Epoch 8
2024-12-15 14:18:23.751480: Current learning rate: 0.00952
2024-12-15 14:25:59.675005: Validation loss did not improve from -0.38437. Patience: 1/50
2024-12-15 14:25:59.677557: train_loss -0.4338
2024-12-15 14:25:59.678943: val_loss -0.3053
2024-12-15 14:25:59.679683: Pseudo dice [0.6124]
2024-12-15 14:25:59.680571: Epoch time: 455.93 s
2024-12-15 14:25:59.681298: Yayy! New best EMA pseudo Dice: 0.5716
2024-12-15 14:26:01.892470: 
2024-12-15 14:26:01.893589: Epoch 9
2024-12-15 14:26:01.894463: Current learning rate: 0.00946
2024-12-15 14:33:26.567170: Validation loss did not improve from -0.38437. Patience: 2/50
2024-12-15 14:33:26.568300: train_loss -0.4487
2024-12-15 14:33:26.569246: val_loss -0.3637
2024-12-15 14:33:26.569869: Pseudo dice [0.6341]
2024-12-15 14:33:26.570599: Epoch time: 444.68 s
2024-12-15 14:33:26.922682: Yayy! New best EMA pseudo Dice: 0.5779
2024-12-15 14:33:28.578570: 
2024-12-15 14:33:28.579762: Epoch 10
2024-12-15 14:33:28.580439: Current learning rate: 0.0094
2024-12-15 14:41:04.429123: Validation loss did not improve from -0.38437. Patience: 3/50
2024-12-15 14:41:04.430595: train_loss -0.4664
2024-12-15 14:41:04.431868: val_loss -0.3709
2024-12-15 14:41:04.432811: Pseudo dice [0.6365]
2024-12-15 14:41:04.433606: Epoch time: 455.85 s
2024-12-15 14:41:04.434589: Yayy! New best EMA pseudo Dice: 0.5837
2024-12-15 14:41:06.122361: 
2024-12-15 14:41:06.123606: Epoch 11
2024-12-15 14:41:06.124559: Current learning rate: 0.00934
2024-12-15 14:48:31.697224: Validation loss improved from -0.38437 to -0.39375! Patience: 3/50
2024-12-15 14:48:31.698019: train_loss -0.4928
2024-12-15 14:48:31.698797: val_loss -0.3937
2024-12-15 14:48:31.699459: Pseudo dice [0.6447]
2024-12-15 14:48:31.700289: Epoch time: 445.58 s
2024-12-15 14:48:31.701113: Yayy! New best EMA pseudo Dice: 0.5898
2024-12-15 14:48:33.406841: 
2024-12-15 14:48:33.408277: Epoch 12
2024-12-15 14:48:33.409344: Current learning rate: 0.00928
2024-12-15 14:55:59.758102: Validation loss improved from -0.39375 to -0.42090! Patience: 0/50
2024-12-15 14:55:59.759153: train_loss -0.4833
2024-12-15 14:55:59.760124: val_loss -0.4209
2024-12-15 14:55:59.760854: Pseudo dice [0.6746]
2024-12-15 14:55:59.761526: Epoch time: 446.35 s
2024-12-15 14:55:59.762214: Yayy! New best EMA pseudo Dice: 0.5983
2024-12-15 14:56:01.473483: 
2024-12-15 14:56:01.475011: Epoch 13
2024-12-15 14:56:01.476123: Current learning rate: 0.00922
2024-12-15 15:02:48.905719: Validation loss improved from -0.42090 to -0.42181! Patience: 0/50
2024-12-15 15:02:48.906830: train_loss -0.5085
2024-12-15 15:02:48.907583: val_loss -0.4218
2024-12-15 15:02:48.908214: Pseudo dice [0.664]
2024-12-15 15:02:48.909218: Epoch time: 407.43 s
2024-12-15 15:02:48.909977: Yayy! New best EMA pseudo Dice: 0.6049
2024-12-15 15:02:50.603934: 
2024-12-15 15:02:50.604990: Epoch 14
2024-12-15 15:02:50.605717: Current learning rate: 0.00916
2024-12-15 15:10:15.607703: Validation loss improved from -0.42181 to -0.42955! Patience: 0/50
2024-12-15 15:10:15.609491: train_loss -0.5283
2024-12-15 15:10:15.610909: val_loss -0.4296
2024-12-15 15:10:15.612113: Pseudo dice [0.6738]
2024-12-15 15:10:15.613205: Epoch time: 445.01 s
2024-12-15 15:10:15.938564: Yayy! New best EMA pseudo Dice: 0.6118
2024-12-15 15:10:17.594546: 
2024-12-15 15:10:17.595857: Epoch 15
2024-12-15 15:10:17.596652: Current learning rate: 0.0091
2024-12-15 15:16:54.000705: Validation loss did not improve from -0.42955. Patience: 1/50
2024-12-15 15:16:54.004294: train_loss -0.5351
2024-12-15 15:16:54.007315: val_loss -0.4022
2024-12-15 15:16:54.008129: Pseudo dice [0.6655]
2024-12-15 15:16:54.009275: Epoch time: 396.41 s
2024-12-15 15:16:54.010223: Yayy! New best EMA pseudo Dice: 0.6171
2024-12-15 15:16:55.748603: 
2024-12-15 15:16:55.749866: Epoch 16
2024-12-15 15:16:55.750633: Current learning rate: 0.00903
2024-12-15 15:23:24.285268: Validation loss did not improve from -0.42955. Patience: 2/50
2024-12-15 15:23:24.286281: train_loss -0.5459
2024-12-15 15:23:24.286989: val_loss -0.4219
2024-12-15 15:23:24.287622: Pseudo dice [0.6677]
2024-12-15 15:23:24.288329: Epoch time: 388.54 s
2024-12-15 15:23:24.288984: Yayy! New best EMA pseudo Dice: 0.6222
2024-12-15 15:23:26.028544: 
2024-12-15 15:23:26.029757: Epoch 17
2024-12-15 15:23:26.030622: Current learning rate: 0.00897
2024-12-15 15:28:30.471125: Validation loss improved from -0.42955 to -0.45035! Patience: 2/50
2024-12-15 15:28:30.472049: train_loss -0.5388
2024-12-15 15:28:30.472942: val_loss -0.4503
2024-12-15 15:28:30.473610: Pseudo dice [0.6905]
2024-12-15 15:28:30.474365: Epoch time: 304.44 s
2024-12-15 15:28:30.475186: Yayy! New best EMA pseudo Dice: 0.629
2024-12-15 15:28:32.221595: 
2024-12-15 15:28:32.223136: Epoch 18
2024-12-15 15:28:32.224347: Current learning rate: 0.00891
2024-12-15 15:33:51.893630: Validation loss did not improve from -0.45035. Patience: 1/50
2024-12-15 15:33:51.896760: train_loss -0.5467
2024-12-15 15:33:51.897701: val_loss -0.446
2024-12-15 15:33:51.898482: Pseudo dice [0.6865]
2024-12-15 15:33:51.899176: Epoch time: 319.68 s
2024-12-15 15:33:51.899771: Yayy! New best EMA pseudo Dice: 0.6348
2024-12-15 15:33:54.295215: 
2024-12-15 15:33:54.296643: Epoch 19
2024-12-15 15:33:54.297424: Current learning rate: 0.00885
2024-12-15 15:39:05.687436: Validation loss improved from -0.45035 to -0.47411! Patience: 1/50
2024-12-15 15:39:05.688464: train_loss -0.5584
2024-12-15 15:39:05.689272: val_loss -0.4741
2024-12-15 15:39:05.690064: Pseudo dice [0.7052]
2024-12-15 15:39:05.690757: Epoch time: 311.39 s
2024-12-15 15:39:06.027356: Yayy! New best EMA pseudo Dice: 0.6418
2024-12-15 15:39:07.770167: 
2024-12-15 15:39:07.771267: Epoch 20
2024-12-15 15:39:07.771954: Current learning rate: 0.00879
2024-12-15 15:44:38.475202: Validation loss did not improve from -0.47411. Patience: 1/50
2024-12-15 15:44:38.476576: train_loss -0.5737
2024-12-15 15:44:38.477309: val_loss -0.4694
2024-12-15 15:44:38.478019: Pseudo dice [0.702]
2024-12-15 15:44:38.478976: Epoch time: 330.71 s
2024-12-15 15:44:38.479751: Yayy! New best EMA pseudo Dice: 0.6478
2024-12-15 15:44:40.229902: 
2024-12-15 15:44:40.231298: Epoch 21
2024-12-15 15:44:40.232014: Current learning rate: 0.00873
2024-12-15 15:50:03.599961: Validation loss did not improve from -0.47411. Patience: 2/50
2024-12-15 15:50:03.600847: train_loss -0.5784
2024-12-15 15:50:03.601723: val_loss -0.4047
2024-12-15 15:50:03.602474: Pseudo dice [0.6812]
2024-12-15 15:50:03.603649: Epoch time: 323.37 s
2024-12-15 15:50:03.604505: Yayy! New best EMA pseudo Dice: 0.6512
2024-12-15 15:50:05.293204: 
2024-12-15 15:50:05.294472: Epoch 22
2024-12-15 15:50:05.295189: Current learning rate: 0.00867
2024-12-15 15:55:09.205572: Validation loss did not improve from -0.47411. Patience: 3/50
2024-12-15 15:55:09.206621: train_loss -0.5846
2024-12-15 15:55:09.207363: val_loss -0.4613
2024-12-15 15:55:09.208025: Pseudo dice [0.6974]
2024-12-15 15:55:09.208683: Epoch time: 303.91 s
2024-12-15 15:55:09.209396: Yayy! New best EMA pseudo Dice: 0.6558
2024-12-15 15:55:10.929520: 
2024-12-15 15:55:10.930697: Epoch 23
2024-12-15 15:55:10.931729: Current learning rate: 0.00861
2024-12-15 16:00:07.465837: Validation loss improved from -0.47411 to -0.48112! Patience: 3/50
2024-12-15 16:00:07.466942: train_loss -0.5813
2024-12-15 16:00:07.467853: val_loss -0.4811
2024-12-15 16:00:07.468580: Pseudo dice [0.7005]
2024-12-15 16:00:07.469250: Epoch time: 296.54 s
2024-12-15 16:00:07.469947: Yayy! New best EMA pseudo Dice: 0.6603
2024-12-15 16:00:09.158235: 
2024-12-15 16:00:09.159327: Epoch 24
2024-12-15 16:00:09.160084: Current learning rate: 0.00855
2024-12-15 16:05:14.683147: Validation loss did not improve from -0.48112. Patience: 1/50
2024-12-15 16:05:14.684099: train_loss -0.5984
2024-12-15 16:05:14.684803: val_loss -0.427
2024-12-15 16:05:14.685517: Pseudo dice [0.6798]
2024-12-15 16:05:14.686170: Epoch time: 305.53 s
2024-12-15 16:05:15.083132: Yayy! New best EMA pseudo Dice: 0.6622
2024-12-15 16:05:16.778486: 
2024-12-15 16:05:16.779950: Epoch 25
2024-12-15 16:05:16.780730: Current learning rate: 0.00849
2024-12-15 16:10:13.610973: Validation loss did not improve from -0.48112. Patience: 2/50
2024-12-15 16:10:13.611784: train_loss -0.5932
2024-12-15 16:10:13.612499: val_loss -0.4575
2024-12-15 16:10:13.613213: Pseudo dice [0.6998]
2024-12-15 16:10:13.613867: Epoch time: 296.83 s
2024-12-15 16:10:13.614481: Yayy! New best EMA pseudo Dice: 0.666
2024-12-15 16:10:15.324651: 
2024-12-15 16:10:15.325890: Epoch 26
2024-12-15 16:10:15.326715: Current learning rate: 0.00843
2024-12-15 16:14:56.385227: Validation loss did not improve from -0.48112. Patience: 3/50
2024-12-15 16:14:56.385970: train_loss -0.594
2024-12-15 16:14:56.386661: val_loss -0.4191
2024-12-15 16:14:56.387280: Pseudo dice [0.6709]
2024-12-15 16:14:56.387975: Epoch time: 281.06 s
2024-12-15 16:14:56.388830: Yayy! New best EMA pseudo Dice: 0.6665
2024-12-15 16:14:58.108885: 
2024-12-15 16:14:58.110327: Epoch 27
2024-12-15 16:14:58.111250: Current learning rate: 0.00836
2024-12-15 16:19:20.658046: Validation loss did not improve from -0.48112. Patience: 4/50
2024-12-15 16:19:20.659078: train_loss -0.6054
2024-12-15 16:19:20.660208: val_loss -0.4731
2024-12-15 16:19:20.661223: Pseudo dice [0.7073]
2024-12-15 16:19:20.662304: Epoch time: 262.55 s
2024-12-15 16:19:20.663082: Yayy! New best EMA pseudo Dice: 0.6706
2024-12-15 16:19:22.372118: 
2024-12-15 16:19:22.373606: Epoch 28
2024-12-15 16:19:22.374326: Current learning rate: 0.0083
2024-12-15 16:23:35.617829: Validation loss did not improve from -0.48112. Patience: 5/50
2024-12-15 16:23:35.620017: train_loss -0.61
2024-12-15 16:23:35.621661: val_loss -0.4513
2024-12-15 16:23:35.622483: Pseudo dice [0.6895]
2024-12-15 16:23:35.623771: Epoch time: 253.25 s
2024-12-15 16:23:35.624577: Yayy! New best EMA pseudo Dice: 0.6725
2024-12-15 16:23:37.745486: 
2024-12-15 16:23:37.746504: Epoch 29
2024-12-15 16:23:37.747283: Current learning rate: 0.00824
2024-12-15 16:27:37.738401: Validation loss did not improve from -0.48112. Patience: 6/50
2024-12-15 16:27:37.739029: train_loss -0.6155
2024-12-15 16:27:37.739669: val_loss -0.4648
2024-12-15 16:27:37.740366: Pseudo dice [0.7021]
2024-12-15 16:27:37.741088: Epoch time: 239.99 s
2024-12-15 16:27:38.166992: Yayy! New best EMA pseudo Dice: 0.6754
2024-12-15 16:27:39.930229: 
2024-12-15 16:27:39.930982: Epoch 30
2024-12-15 16:27:39.931623: Current learning rate: 0.00818
2024-12-15 16:31:55.405612: Validation loss improved from -0.48112 to -0.50596! Patience: 6/50
2024-12-15 16:31:55.406587: train_loss -0.6199
2024-12-15 16:31:55.407386: val_loss -0.506
2024-12-15 16:31:55.408270: Pseudo dice [0.7205]
2024-12-15 16:31:55.409285: Epoch time: 255.48 s
2024-12-15 16:31:55.410002: Yayy! New best EMA pseudo Dice: 0.6799
2024-12-15 16:31:57.181130: 
2024-12-15 16:31:57.182550: Epoch 31
2024-12-15 16:31:57.183415: Current learning rate: 0.00812
2024-12-15 16:36:13.801503: Validation loss did not improve from -0.50596. Patience: 1/50
2024-12-15 16:36:13.804780: train_loss -0.6271
2024-12-15 16:36:13.806033: val_loss -0.4874
2024-12-15 16:36:13.806724: Pseudo dice [0.7097]
2024-12-15 16:36:13.807577: Epoch time: 256.62 s
2024-12-15 16:36:13.808254: Yayy! New best EMA pseudo Dice: 0.6829
2024-12-15 16:36:15.564238: 
2024-12-15 16:36:15.565636: Epoch 32
2024-12-15 16:36:15.566781: Current learning rate: 0.00806
2024-12-15 16:40:39.612311: Validation loss did not improve from -0.50596. Patience: 2/50
2024-12-15 16:40:39.613003: train_loss -0.6258
2024-12-15 16:40:39.613976: val_loss -0.4339
2024-12-15 16:40:39.615052: Pseudo dice [0.6872]
2024-12-15 16:40:39.616207: Epoch time: 264.05 s
2024-12-15 16:40:39.617348: Yayy! New best EMA pseudo Dice: 0.6833
2024-12-15 16:40:41.388820: 
2024-12-15 16:40:41.389808: Epoch 33
2024-12-15 16:40:41.390846: Current learning rate: 0.008
2024-12-15 16:45:09.778666: Validation loss did not improve from -0.50596. Patience: 3/50
2024-12-15 16:45:09.779623: train_loss -0.6357
2024-12-15 16:45:09.780707: val_loss -0.4527
2024-12-15 16:45:09.781871: Pseudo dice [0.7022]
2024-12-15 16:45:09.782965: Epoch time: 268.39 s
2024-12-15 16:45:09.783979: Yayy! New best EMA pseudo Dice: 0.6852
2024-12-15 16:45:11.561649: 
2024-12-15 16:45:11.563001: Epoch 34
2024-12-15 16:45:11.564091: Current learning rate: 0.00793
2024-12-15 16:50:03.585939: Validation loss did not improve from -0.50596. Patience: 4/50
2024-12-15 16:50:03.586773: train_loss -0.6372
2024-12-15 16:50:03.587630: val_loss -0.4574
2024-12-15 16:50:03.588273: Pseudo dice [0.7006]
2024-12-15 16:50:03.588936: Epoch time: 292.03 s
2024-12-15 16:50:03.975468: Yayy! New best EMA pseudo Dice: 0.6868
2024-12-15 16:50:05.722714: 
2024-12-15 16:50:05.724676: Epoch 35
2024-12-15 16:50:05.725440: Current learning rate: 0.00787
2024-12-15 16:54:57.788827: Validation loss did not improve from -0.50596. Patience: 5/50
2024-12-15 16:54:57.789773: train_loss -0.6493
2024-12-15 16:54:57.790619: val_loss -0.4487
2024-12-15 16:54:57.791341: Pseudo dice [0.7074]
2024-12-15 16:54:57.792181: Epoch time: 292.07 s
2024-12-15 16:54:57.793124: Yayy! New best EMA pseudo Dice: 0.6888
2024-12-15 16:54:59.597193: 
2024-12-15 16:54:59.598461: Epoch 36
2024-12-15 16:54:59.599243: Current learning rate: 0.00781
2024-12-15 16:59:59.330023: Validation loss did not improve from -0.50596. Patience: 6/50
2024-12-15 16:59:59.330896: train_loss -0.649
2024-12-15 16:59:59.331635: val_loss -0.4835
2024-12-15 16:59:59.332265: Pseudo dice [0.7086]
2024-12-15 16:59:59.332935: Epoch time: 299.73 s
2024-12-15 16:59:59.333530: Yayy! New best EMA pseudo Dice: 0.6908
2024-12-15 17:00:01.139530: 
2024-12-15 17:00:01.140832: Epoch 37
2024-12-15 17:00:01.141535: Current learning rate: 0.00775
2024-12-15 17:05:16.447562: Validation loss did not improve from -0.50596. Patience: 7/50
2024-12-15 17:05:16.448296: train_loss -0.6424
2024-12-15 17:05:16.449041: val_loss -0.4566
2024-12-15 17:05:16.449824: Pseudo dice [0.6984]
2024-12-15 17:05:16.450470: Epoch time: 315.31 s
2024-12-15 17:05:16.451370: Yayy! New best EMA pseudo Dice: 0.6916
2024-12-15 17:05:18.284104: 
2024-12-15 17:05:18.285019: Epoch 38
2024-12-15 17:05:18.285765: Current learning rate: 0.00769
2024-12-15 17:10:37.875793: Validation loss did not improve from -0.50596. Patience: 8/50
2024-12-15 17:10:37.876743: train_loss -0.6549
2024-12-15 17:10:37.877651: val_loss -0.4893
2024-12-15 17:10:37.878639: Pseudo dice [0.7187]
2024-12-15 17:10:37.879529: Epoch time: 319.59 s
2024-12-15 17:10:37.880521: Yayy! New best EMA pseudo Dice: 0.6943
2024-12-15 17:10:39.652062: 
2024-12-15 17:10:39.653548: Epoch 39
2024-12-15 17:10:39.654780: Current learning rate: 0.00763
2024-12-15 17:15:46.592517: Validation loss did not improve from -0.50596. Patience: 9/50
2024-12-15 17:15:46.593211: train_loss -0.6444
2024-12-15 17:15:46.593961: val_loss -0.4678
2024-12-15 17:15:46.594618: Pseudo dice [0.7072]
2024-12-15 17:15:46.595230: Epoch time: 306.94 s
2024-12-15 17:15:47.338464: Yayy! New best EMA pseudo Dice: 0.6956
2024-12-15 17:15:49.151307: 
2024-12-15 17:15:49.152158: Epoch 40
2024-12-15 17:15:49.152829: Current learning rate: 0.00756
2024-12-15 17:21:18.890277: Validation loss did not improve from -0.50596. Patience: 10/50
2024-12-15 17:21:18.890881: train_loss -0.658
2024-12-15 17:21:18.891786: val_loss -0.5019
2024-12-15 17:21:18.892575: Pseudo dice [0.7225]
2024-12-15 17:21:18.893441: Epoch time: 329.74 s
2024-12-15 17:21:18.894723: Yayy! New best EMA pseudo Dice: 0.6983
2024-12-15 17:21:20.716363: 
2024-12-15 17:21:20.717343: Epoch 41
2024-12-15 17:21:20.718369: Current learning rate: 0.0075
2024-12-15 17:26:34.830439: Validation loss did not improve from -0.50596. Patience: 11/50
2024-12-15 17:26:34.832278: train_loss -0.6625
2024-12-15 17:26:34.833961: val_loss -0.4999
2024-12-15 17:26:34.834567: Pseudo dice [0.7124]
2024-12-15 17:26:34.835610: Epoch time: 314.12 s
2024-12-15 17:26:34.836414: Yayy! New best EMA pseudo Dice: 0.6997
2024-12-15 17:26:36.593263: 
2024-12-15 17:26:36.594477: Epoch 42
2024-12-15 17:26:36.595387: Current learning rate: 0.00744
2024-12-15 17:31:49.846482: Validation loss did not improve from -0.50596. Patience: 12/50
2024-12-15 17:31:49.847322: train_loss -0.6698
2024-12-15 17:31:49.848033: val_loss -0.4891
2024-12-15 17:31:49.848691: Pseudo dice [0.7187]
2024-12-15 17:31:49.849481: Epoch time: 313.26 s
2024-12-15 17:31:49.850260: Yayy! New best EMA pseudo Dice: 0.7016
2024-12-15 17:31:51.621489: 
2024-12-15 17:31:51.622391: Epoch 43
2024-12-15 17:31:51.623211: Current learning rate: 0.00738
2024-12-15 17:36:50.064510: Validation loss did not improve from -0.50596. Patience: 13/50
2024-12-15 17:36:50.065170: train_loss -0.6765
2024-12-15 17:36:50.065921: val_loss -0.4717
2024-12-15 17:36:50.066842: Pseudo dice [0.7011]
2024-12-15 17:36:50.067482: Epoch time: 298.44 s
2024-12-15 17:36:51.372339: 
2024-12-15 17:36:51.373403: Epoch 44
2024-12-15 17:36:51.374078: Current learning rate: 0.00732
2024-12-15 17:41:44.855639: Validation loss did not improve from -0.50596. Patience: 14/50
2024-12-15 17:41:44.857888: train_loss -0.6702
2024-12-15 17:41:44.858620: val_loss -0.4704
2024-12-15 17:41:44.859283: Pseudo dice [0.7066]
2024-12-15 17:41:44.859924: Epoch time: 293.49 s
2024-12-15 17:41:45.344226: Yayy! New best EMA pseudo Dice: 0.702
2024-12-15 17:41:47.090517: 
2024-12-15 17:41:47.091440: Epoch 45
2024-12-15 17:41:47.092196: Current learning rate: 0.00725
2024-12-15 17:46:48.272529: Validation loss did not improve from -0.50596. Patience: 15/50
2024-12-15 17:46:48.273255: train_loss -0.6744
2024-12-15 17:46:48.274137: val_loss -0.5009
2024-12-15 17:46:48.274935: Pseudo dice [0.7154]
2024-12-15 17:46:48.275627: Epoch time: 301.18 s
2024-12-15 17:46:48.276280: Yayy! New best EMA pseudo Dice: 0.7034
2024-12-15 17:46:50.017656: 
2024-12-15 17:46:50.018618: Epoch 46
2024-12-15 17:46:50.019273: Current learning rate: 0.00719
2024-12-15 17:51:47.017899: Validation loss did not improve from -0.50596. Patience: 16/50
2024-12-15 17:51:47.018627: train_loss -0.6727
2024-12-15 17:51:47.019601: val_loss -0.4768
2024-12-15 17:51:47.020338: Pseudo dice [0.7025]
2024-12-15 17:51:47.021044: Epoch time: 297.0 s
2024-12-15 17:51:48.346452: 
2024-12-15 17:51:48.347685: Epoch 47
2024-12-15 17:51:48.348464: Current learning rate: 0.00713
2024-12-15 17:55:55.451863: Validation loss did not improve from -0.50596. Patience: 17/50
2024-12-15 17:55:55.452579: train_loss -0.6689
2024-12-15 17:55:55.453478: val_loss -0.5014
2024-12-15 17:55:55.454174: Pseudo dice [0.7279]
2024-12-15 17:55:55.455066: Epoch time: 247.11 s
2024-12-15 17:55:55.455885: Yayy! New best EMA pseudo Dice: 0.7057
2024-12-15 17:55:57.147382: 
2024-12-15 17:55:57.148486: Epoch 48
2024-12-15 17:55:57.149268: Current learning rate: 0.00707
2024-12-15 18:00:09.778646: Validation loss improved from -0.50596 to -0.52137! Patience: 17/50
2024-12-15 18:00:09.779480: train_loss -0.6791
2024-12-15 18:00:09.780271: val_loss -0.5214
2024-12-15 18:00:09.781024: Pseudo dice [0.7255]
2024-12-15 18:00:09.781930: Epoch time: 252.63 s
2024-12-15 18:00:09.782766: Yayy! New best EMA pseudo Dice: 0.7077
2024-12-15 18:00:11.523261: 
2024-12-15 18:00:11.524281: Epoch 49
2024-12-15 18:00:11.525311: Current learning rate: 0.007
2024-12-15 18:04:16.646144: Validation loss did not improve from -0.52137. Patience: 1/50
2024-12-15 18:04:16.646950: train_loss -0.6868
2024-12-15 18:04:16.647600: val_loss -0.5097
2024-12-15 18:04:16.648219: Pseudo dice [0.7352]
2024-12-15 18:04:16.648912: Epoch time: 245.12 s
2024-12-15 18:04:17.037722: Yayy! New best EMA pseudo Dice: 0.7105
2024-12-15 18:04:20.028603: 
2024-12-15 18:04:20.029540: Epoch 50
2024-12-15 18:04:20.030230: Current learning rate: 0.00694
2024-12-15 18:08:26.078422: Validation loss did not improve from -0.52137. Patience: 2/50
2024-12-15 18:08:26.079181: train_loss -0.686
2024-12-15 18:08:26.079951: val_loss -0.482
2024-12-15 18:08:26.080688: Pseudo dice [0.7116]
2024-12-15 18:08:26.081471: Epoch time: 246.05 s
2024-12-15 18:08:26.082146: Yayy! New best EMA pseudo Dice: 0.7106
2024-12-15 18:08:27.784668: 
2024-12-15 18:08:27.785627: Epoch 51
2024-12-15 18:08:27.786315: Current learning rate: 0.00688
2024-12-15 18:12:22.137518: Validation loss did not improve from -0.52137. Patience: 3/50
2024-12-15 18:12:22.138160: train_loss -0.6952
2024-12-15 18:12:22.138867: val_loss -0.5043
2024-12-15 18:12:22.139566: Pseudo dice [0.7194]
2024-12-15 18:12:22.140357: Epoch time: 234.35 s
2024-12-15 18:12:22.141284: Yayy! New best EMA pseudo Dice: 0.7115
2024-12-15 18:12:23.871757: 
2024-12-15 18:12:23.872547: Epoch 52
2024-12-15 18:12:23.873238: Current learning rate: 0.00682
2024-12-15 18:16:48.403990: Validation loss did not improve from -0.52137. Patience: 4/50
2024-12-15 18:16:48.404942: train_loss -0.6947
2024-12-15 18:16:48.405735: val_loss -0.4993
2024-12-15 18:16:48.406452: Pseudo dice [0.7282]
2024-12-15 18:16:48.407221: Epoch time: 264.53 s
2024-12-15 18:16:48.408017: Yayy! New best EMA pseudo Dice: 0.7131
2024-12-15 18:16:50.119405: 
2024-12-15 18:16:50.120559: Epoch 53
2024-12-15 18:16:50.121419: Current learning rate: 0.00675
2024-12-15 18:21:30.188782: Validation loss did not improve from -0.52137. Patience: 5/50
2024-12-15 18:21:30.189711: train_loss -0.6886
2024-12-15 18:21:30.190517: val_loss -0.4933
2024-12-15 18:21:30.191250: Pseudo dice [0.7129]
2024-12-15 18:21:30.192060: Epoch time: 280.07 s
2024-12-15 18:21:31.577220: 
2024-12-15 18:21:31.578453: Epoch 54
2024-12-15 18:21:31.579319: Current learning rate: 0.00669
2024-12-15 18:26:21.007219: Validation loss did not improve from -0.52137. Patience: 6/50
2024-12-15 18:26:21.008203: train_loss -0.688
2024-12-15 18:26:21.008960: val_loss -0.489
2024-12-15 18:26:21.009639: Pseudo dice [0.7088]
2024-12-15 18:26:21.010391: Epoch time: 289.43 s
2024-12-15 18:26:22.719705: 
2024-12-15 18:26:22.720852: Epoch 55
2024-12-15 18:26:22.721567: Current learning rate: 0.00663
2024-12-15 18:31:09.769629: Validation loss did not improve from -0.52137. Patience: 7/50
2024-12-15 18:31:09.771227: train_loss -0.6944
2024-12-15 18:31:09.773284: val_loss -0.4901
2024-12-15 18:31:09.774206: Pseudo dice [0.7241]
2024-12-15 18:31:09.775229: Epoch time: 287.05 s
2024-12-15 18:31:09.776302: Yayy! New best EMA pseudo Dice: 0.7138
2024-12-15 18:31:11.885334: 
2024-12-15 18:31:11.886402: Epoch 56
2024-12-15 18:31:11.887281: Current learning rate: 0.00657
2024-12-15 18:36:08.299222: Validation loss did not improve from -0.52137. Patience: 8/50
2024-12-15 18:36:08.300798: train_loss -0.7011
2024-12-15 18:36:08.301715: val_loss -0.4846
2024-12-15 18:36:08.302436: Pseudo dice [0.7199]
2024-12-15 18:36:08.303178: Epoch time: 296.42 s
2024-12-15 18:36:08.304212: Yayy! New best EMA pseudo Dice: 0.7144
2024-12-15 18:36:10.048209: 
2024-12-15 18:36:10.049369: Epoch 57
2024-12-15 18:36:10.050081: Current learning rate: 0.0065
2024-12-15 18:41:11.383693: Validation loss did not improve from -0.52137. Patience: 9/50
2024-12-15 18:41:11.384510: train_loss -0.7006
2024-12-15 18:41:11.385397: val_loss -0.5164
2024-12-15 18:41:11.386275: Pseudo dice [0.7221]
2024-12-15 18:41:11.387115: Epoch time: 301.34 s
2024-12-15 18:41:11.387913: Yayy! New best EMA pseudo Dice: 0.7152
2024-12-15 18:41:13.103057: 
2024-12-15 18:41:13.104115: Epoch 58
2024-12-15 18:41:13.105085: Current learning rate: 0.00644
2024-12-15 18:46:14.655530: Validation loss did not improve from -0.52137. Patience: 10/50
2024-12-15 18:46:14.657485: train_loss -0.7052
2024-12-15 18:46:14.658236: val_loss -0.5174
2024-12-15 18:46:14.658953: Pseudo dice [0.7303]
2024-12-15 18:46:14.659598: Epoch time: 301.56 s
2024-12-15 18:46:14.660263: Yayy! New best EMA pseudo Dice: 0.7167
2024-12-15 18:46:16.432817: 
2024-12-15 18:46:16.433664: Epoch 59
2024-12-15 18:46:16.434354: Current learning rate: 0.00638
2024-12-15 18:51:33.140917: Validation loss did not improve from -0.52137. Patience: 11/50
2024-12-15 18:51:33.141654: train_loss -0.7022
2024-12-15 18:51:33.142400: val_loss -0.4903
2024-12-15 18:51:33.143149: Pseudo dice [0.7161]
2024-12-15 18:51:33.143918: Epoch time: 316.71 s
2024-12-15 18:51:34.947506: 
2024-12-15 18:51:34.948490: Epoch 60
2024-12-15 18:51:34.949142: Current learning rate: 0.00631
2024-12-15 18:56:53.482023: Validation loss did not improve from -0.52137. Patience: 12/50
2024-12-15 18:56:53.482807: train_loss -0.7107
2024-12-15 18:56:53.483573: val_loss -0.5094
2024-12-15 18:56:53.484328: Pseudo dice [0.7343]
2024-12-15 18:56:53.484965: Epoch time: 318.54 s
2024-12-15 18:56:53.485658: Yayy! New best EMA pseudo Dice: 0.7184
2024-12-15 18:56:55.841506: 
2024-12-15 18:56:55.842733: Epoch 61
2024-12-15 18:56:55.843928: Current learning rate: 0.00625
2024-12-15 19:02:20.380049: Validation loss did not improve from -0.52137. Patience: 13/50
2024-12-15 19:02:20.380762: train_loss -0.7058
2024-12-15 19:02:20.381513: val_loss -0.4697
2024-12-15 19:02:20.382259: Pseudo dice [0.7117]
2024-12-15 19:02:20.383054: Epoch time: 324.54 s
2024-12-15 19:02:21.746743: 
2024-12-15 19:02:21.747710: Epoch 62
2024-12-15 19:02:21.748385: Current learning rate: 0.00619
2024-12-15 19:07:44.921699: Validation loss did not improve from -0.52137. Patience: 14/50
2024-12-15 19:07:44.922394: train_loss -0.7105
2024-12-15 19:07:44.923051: val_loss -0.4919
2024-12-15 19:07:44.923687: Pseudo dice [0.7159]
2024-12-15 19:07:44.924499: Epoch time: 323.18 s
2024-12-15 19:07:46.265924: 
2024-12-15 19:07:46.266717: Epoch 63
2024-12-15 19:07:46.267317: Current learning rate: 0.00612
2024-12-15 19:13:09.773821: Validation loss did not improve from -0.52137. Patience: 15/50
2024-12-15 19:13:09.774826: train_loss -0.7085
2024-12-15 19:13:09.775569: val_loss -0.5185
2024-12-15 19:13:09.776353: Pseudo dice [0.7341]
2024-12-15 19:13:09.777775: Epoch time: 323.51 s
2024-12-15 19:13:09.778800: Yayy! New best EMA pseudo Dice: 0.7192
2024-12-15 19:13:11.529613: 
2024-12-15 19:13:11.531151: Epoch 64
2024-12-15 19:13:11.532155: Current learning rate: 0.00606
2024-12-15 19:18:16.911422: Validation loss did not improve from -0.52137. Patience: 16/50
2024-12-15 19:18:16.912153: train_loss -0.7197
2024-12-15 19:18:16.912915: val_loss -0.4779
2024-12-15 19:18:16.913601: Pseudo dice [0.7133]
2024-12-15 19:18:16.914370: Epoch time: 305.38 s
2024-12-15 19:18:18.690796: 
2024-12-15 19:18:18.692053: Epoch 65
2024-12-15 19:18:18.692947: Current learning rate: 0.006
2024-12-15 19:23:12.149012: Validation loss did not improve from -0.52137. Patience: 17/50
2024-12-15 19:23:12.149949: train_loss -0.7196
2024-12-15 19:23:12.151096: val_loss -0.4906
2024-12-15 19:23:12.152211: Pseudo dice [0.7197]
2024-12-15 19:23:12.153183: Epoch time: 293.46 s
2024-12-15 19:23:13.503217: 
2024-12-15 19:23:13.504716: Epoch 66
2024-12-15 19:23:13.505704: Current learning rate: 0.00593
2024-12-15 19:28:15.894994: Validation loss did not improve from -0.52137. Patience: 18/50
2024-12-15 19:28:15.895821: train_loss -0.7167
2024-12-15 19:28:15.897119: val_loss -0.5122
2024-12-15 19:28:15.898125: Pseudo dice [0.7347]
2024-12-15 19:28:15.899014: Epoch time: 302.39 s
2024-12-15 19:28:15.900087: Yayy! New best EMA pseudo Dice: 0.7203
2024-12-15 19:28:17.660463: 
2024-12-15 19:28:17.661435: Epoch 67
2024-12-15 19:28:17.662293: Current learning rate: 0.00587
2024-12-15 19:33:16.672946: Validation loss did not improve from -0.52137. Patience: 19/50
2024-12-15 19:33:16.673565: train_loss -0.7241
2024-12-15 19:33:16.674263: val_loss -0.479
2024-12-15 19:33:16.675053: Pseudo dice [0.7149]
2024-12-15 19:33:16.675697: Epoch time: 299.01 s
2024-12-15 19:33:18.039817: 
2024-12-15 19:33:18.040766: Epoch 68
2024-12-15 19:33:18.041552: Current learning rate: 0.00581
2024-12-15 19:37:30.309532: Validation loss did not improve from -0.52137. Patience: 20/50
2024-12-15 19:37:30.311650: train_loss -0.7247
2024-12-15 19:37:30.313890: val_loss -0.4907
2024-12-15 19:37:30.314666: Pseudo dice [0.7229]
2024-12-15 19:37:30.316041: Epoch time: 252.27 s
2024-12-15 19:37:31.713355: 
2024-12-15 19:37:31.714506: Epoch 69
2024-12-15 19:37:31.715647: Current learning rate: 0.00574
2024-12-15 19:41:28.336054: Validation loss did not improve from -0.52137. Patience: 21/50
2024-12-15 19:41:28.337182: train_loss -0.7264
2024-12-15 19:41:28.337951: val_loss -0.4991
2024-12-15 19:41:28.338664: Pseudo dice [0.7139]
2024-12-15 19:41:28.339331: Epoch time: 236.63 s
2024-12-15 19:41:30.161338: 
2024-12-15 19:41:30.162390: Epoch 70
2024-12-15 19:41:30.163124: Current learning rate: 0.00568
2024-12-15 19:45:35.254536: Validation loss did not improve from -0.52137. Patience: 22/50
2024-12-15 19:45:35.255206: train_loss -0.7226
2024-12-15 19:45:35.255993: val_loss -0.5082
2024-12-15 19:45:35.256667: Pseudo dice [0.7241]
2024-12-15 19:45:35.257527: Epoch time: 245.1 s
2024-12-15 19:45:36.617265: 
2024-12-15 19:45:36.618286: Epoch 71
2024-12-15 19:45:36.619366: Current learning rate: 0.00562
2024-12-15 19:49:38.490851: Validation loss did not improve from -0.52137. Patience: 23/50
2024-12-15 19:49:38.491564: train_loss -0.7296
2024-12-15 19:49:38.492383: val_loss -0.5046
2024-12-15 19:49:38.493112: Pseudo dice [0.7216]
2024-12-15 19:49:38.493790: Epoch time: 241.88 s
2024-12-15 19:49:40.582070: 
2024-12-15 19:49:40.583322: Epoch 72
2024-12-15 19:49:40.584292: Current learning rate: 0.00555
2024-12-15 19:53:56.095105: Validation loss did not improve from -0.52137. Patience: 24/50
2024-12-15 19:53:56.097121: train_loss -0.7283
2024-12-15 19:53:56.097848: val_loss -0.4709
2024-12-15 19:53:56.098423: Pseudo dice [0.717]
2024-12-15 19:53:56.099013: Epoch time: 255.52 s
2024-12-15 19:53:57.449628: 
2024-12-15 19:53:57.450501: Epoch 73
2024-12-15 19:53:57.451167: Current learning rate: 0.00549
2024-12-15 19:58:36.216487: Validation loss did not improve from -0.52137. Patience: 25/50
2024-12-15 19:58:36.217351: train_loss -0.7273
2024-12-15 19:58:36.218224: val_loss -0.4811
2024-12-15 19:58:36.219117: Pseudo dice [0.7204]
2024-12-15 19:58:36.219900: Epoch time: 278.77 s
2024-12-15 19:58:37.580313: 
2024-12-15 19:58:37.581351: Epoch 74
2024-12-15 19:58:37.582313: Current learning rate: 0.00542
2024-12-15 20:03:24.542609: Validation loss did not improve from -0.52137. Patience: 26/50
2024-12-15 20:03:24.543273: train_loss -0.7285
2024-12-15 20:03:24.543982: val_loss -0.5106
2024-12-15 20:03:24.544645: Pseudo dice [0.7324]
2024-12-15 20:03:24.545363: Epoch time: 286.96 s
2024-12-15 20:03:25.071591: Yayy! New best EMA pseudo Dice: 0.7211
2024-12-15 20:03:26.812973: 
2024-12-15 20:03:26.814096: Epoch 75
2024-12-15 20:03:26.814936: Current learning rate: 0.00536
2024-12-15 20:08:21.949713: Validation loss did not improve from -0.52137. Patience: 27/50
2024-12-15 20:08:21.950526: train_loss -0.7262
2024-12-15 20:08:21.951477: val_loss -0.4955
2024-12-15 20:08:21.952175: Pseudo dice [0.714]
2024-12-15 20:08:21.952888: Epoch time: 295.14 s
2024-12-15 20:08:23.299035: 
2024-12-15 20:08:23.300457: Epoch 76
2024-12-15 20:08:23.301577: Current learning rate: 0.00529
2024-12-15 20:13:31.676434: Validation loss did not improve from -0.52137. Patience: 28/50
2024-12-15 20:13:31.677692: train_loss -0.7316
2024-12-15 20:13:31.678436: val_loss -0.4912
2024-12-15 20:13:31.679096: Pseudo dice [0.7231]
2024-12-15 20:13:31.679740: Epoch time: 308.38 s
2024-12-15 20:13:33.044538: 
2024-12-15 20:13:33.045889: Epoch 77
2024-12-15 20:13:33.046675: Current learning rate: 0.00523
2024-12-15 20:18:29.514819: Validation loss did not improve from -0.52137. Patience: 29/50
2024-12-15 20:18:29.515721: train_loss -0.732
2024-12-15 20:18:29.516711: val_loss -0.4937
2024-12-15 20:18:29.517699: Pseudo dice [0.7202]
2024-12-15 20:18:29.518736: Epoch time: 296.47 s
2024-12-15 20:18:30.906225: 
2024-12-15 20:18:30.907790: Epoch 78
2024-12-15 20:18:30.908802: Current learning rate: 0.00517
2024-12-15 20:23:39.930447: Validation loss did not improve from -0.52137. Patience: 30/50
2024-12-15 20:23:39.931452: train_loss -0.7355
2024-12-15 20:23:39.932724: val_loss -0.4984
2024-12-15 20:23:39.933683: Pseudo dice [0.7277]
2024-12-15 20:23:39.934484: Epoch time: 309.03 s
2024-12-15 20:23:39.935395: Yayy! New best EMA pseudo Dice: 0.7213
2024-12-15 20:23:41.693373: 
2024-12-15 20:23:41.694356: Epoch 79
2024-12-15 20:23:41.695186: Current learning rate: 0.0051
2024-12-15 20:29:08.391282: Validation loss did not improve from -0.52137. Patience: 31/50
2024-12-15 20:29:08.392061: train_loss -0.7346
2024-12-15 20:29:08.392968: val_loss -0.4817
2024-12-15 20:29:08.393704: Pseudo dice [0.7168]
2024-12-15 20:29:08.394384: Epoch time: 326.7 s
2024-12-15 20:29:10.201151: 
2024-12-15 20:29:10.202367: Epoch 80
2024-12-15 20:29:10.203032: Current learning rate: 0.00504
2024-12-15 20:34:21.353867: Validation loss did not improve from -0.52137. Patience: 32/50
2024-12-15 20:34:21.354784: train_loss -0.7426
2024-12-15 20:34:21.355572: val_loss -0.5087
2024-12-15 20:34:21.356314: Pseudo dice [0.7369]
2024-12-15 20:34:21.357395: Epoch time: 311.15 s
2024-12-15 20:34:21.358133: Yayy! New best EMA pseudo Dice: 0.7225
2024-12-15 20:34:23.141474: 
2024-12-15 20:34:23.142608: Epoch 81
2024-12-15 20:34:23.143438: Current learning rate: 0.00497
2024-12-15 20:39:56.784947: Validation loss improved from -0.52137 to -0.53466! Patience: 32/50
2024-12-15 20:39:56.785891: train_loss -0.7441
2024-12-15 20:39:56.786870: val_loss -0.5347
2024-12-15 20:39:56.787771: Pseudo dice [0.7404]
2024-12-15 20:39:56.789110: Epoch time: 333.65 s
2024-12-15 20:39:56.789981: Yayy! New best EMA pseudo Dice: 0.7243
2024-12-15 20:39:58.557770: 
2024-12-15 20:39:58.558911: Epoch 82
2024-12-15 20:39:58.559698: Current learning rate: 0.00491
2024-12-15 20:45:34.121115: Validation loss did not improve from -0.53466. Patience: 1/50
2024-12-15 20:45:34.123588: train_loss -0.7474
2024-12-15 20:45:34.125460: val_loss -0.4724
2024-12-15 20:45:34.126331: Pseudo dice [0.7103]
2024-12-15 20:45:34.127488: Epoch time: 335.57 s
2024-12-15 20:45:35.807609: 
2024-12-15 20:45:35.808987: Epoch 83
2024-12-15 20:45:35.810064: Current learning rate: 0.00484
2024-12-15 20:50:58.094644: Validation loss did not improve from -0.53466. Patience: 2/50
2024-12-15 20:50:58.095721: train_loss -0.7432
2024-12-15 20:50:58.096748: val_loss -0.5238
2024-12-15 20:50:58.097623: Pseudo dice [0.7331]
2024-12-15 20:50:58.098706: Epoch time: 322.29 s
2024-12-15 20:50:59.424581: 
2024-12-15 20:50:59.425877: Epoch 84
2024-12-15 20:50:59.426922: Current learning rate: 0.00478
2024-12-15 20:55:57.080029: Validation loss did not improve from -0.53466. Patience: 3/50
2024-12-15 20:55:57.082342: train_loss -0.7523
2024-12-15 20:55:57.083808: val_loss -0.4395
2024-12-15 20:55:57.085057: Pseudo dice [0.698]
2024-12-15 20:55:57.086037: Epoch time: 297.66 s
2024-12-15 20:55:58.797988: 
2024-12-15 20:55:58.799464: Epoch 85
2024-12-15 20:55:58.800397: Current learning rate: 0.00471
2024-12-15 21:01:07.386398: Validation loss did not improve from -0.53466. Patience: 4/50
2024-12-15 21:01:07.387291: train_loss -0.7476
2024-12-15 21:01:07.388036: val_loss -0.4867
2024-12-15 21:01:07.388914: Pseudo dice [0.7221]
2024-12-15 21:01:07.389657: Epoch time: 308.59 s
2024-12-15 21:01:08.696775: 
2024-12-15 21:01:08.698092: Epoch 86
2024-12-15 21:01:08.698946: Current learning rate: 0.00465
2024-12-15 21:06:13.314356: Validation loss did not improve from -0.53466. Patience: 5/50
2024-12-15 21:06:13.315423: train_loss -0.749
2024-12-15 21:06:13.316203: val_loss -0.5082
2024-12-15 21:06:13.316981: Pseudo dice [0.736]
2024-12-15 21:06:13.317747: Epoch time: 304.62 s
2024-12-15 21:06:14.656253: 
2024-12-15 21:06:14.657707: Epoch 87
2024-12-15 21:06:14.658593: Current learning rate: 0.00458
2024-12-15 21:11:15.252616: Validation loss did not improve from -0.53466. Patience: 6/50
2024-12-15 21:11:15.253535: train_loss -0.7511
2024-12-15 21:11:15.254264: val_loss -0.5182
2024-12-15 21:11:15.254960: Pseudo dice [0.7404]
2024-12-15 21:11:15.255624: Epoch time: 300.6 s
2024-12-15 21:11:15.256189: Yayy! New best EMA pseudo Dice: 0.7246
2024-12-15 21:11:16.952703: 
2024-12-15 21:11:16.954005: Epoch 88
2024-12-15 21:11:16.954740: Current learning rate: 0.00452
2024-12-15 21:16:06.484151: Validation loss did not improve from -0.53466. Patience: 7/50
2024-12-15 21:16:06.484850: train_loss -0.7533
2024-12-15 21:16:06.485496: val_loss -0.5153
2024-12-15 21:16:06.486097: Pseudo dice [0.7341]
2024-12-15 21:16:06.486734: Epoch time: 289.53 s
2024-12-15 21:16:06.487313: Yayy! New best EMA pseudo Dice: 0.7256
2024-12-15 21:16:08.191078: 
2024-12-15 21:16:08.192220: Epoch 89
2024-12-15 21:16:08.192905: Current learning rate: 0.00445
2024-12-15 21:21:35.569504: Validation loss did not improve from -0.53466. Patience: 8/50
2024-12-15 21:21:35.570858: train_loss -0.7521
2024-12-15 21:21:35.571720: val_loss -0.5078
2024-12-15 21:21:35.572373: Pseudo dice [0.7357]
2024-12-15 21:21:35.572993: Epoch time: 327.38 s
2024-12-15 21:21:35.960912: Yayy! New best EMA pseudo Dice: 0.7266
2024-12-15 21:21:37.638638: 
2024-12-15 21:21:37.639959: Epoch 90
2024-12-15 21:21:37.640630: Current learning rate: 0.00438
2024-12-15 21:26:51.356135: Validation loss did not improve from -0.53466. Patience: 9/50
2024-12-15 21:26:51.357295: train_loss -0.7503
2024-12-15 21:26:51.357998: val_loss -0.5297
2024-12-15 21:26:51.358573: Pseudo dice [0.7385]
2024-12-15 21:26:51.359229: Epoch time: 313.72 s
2024-12-15 21:26:51.359945: Yayy! New best EMA pseudo Dice: 0.7278
2024-12-15 21:26:53.031553: 
2024-12-15 21:26:53.032917: Epoch 91
2024-12-15 21:26:53.033642: Current learning rate: 0.00432
2024-12-15 21:32:27.756513: Validation loss did not improve from -0.53466. Patience: 10/50
2024-12-15 21:32:27.757441: train_loss -0.7545
2024-12-15 21:32:27.758111: val_loss -0.533
2024-12-15 21:32:27.758739: Pseudo dice [0.7435]
2024-12-15 21:32:27.759489: Epoch time: 334.73 s
2024-12-15 21:32:27.760635: Yayy! New best EMA pseudo Dice: 0.7293
2024-12-15 21:32:29.448567: 
2024-12-15 21:32:29.449612: Epoch 92
2024-12-15 21:32:29.450372: Current learning rate: 0.00425
2024-12-15 21:37:41.911924: Validation loss did not improve from -0.53466. Patience: 11/50
2024-12-15 21:37:41.913001: train_loss -0.7557
2024-12-15 21:37:41.913986: val_loss -0.5196
2024-12-15 21:37:41.914872: Pseudo dice [0.7383]
2024-12-15 21:37:41.915798: Epoch time: 312.47 s
2024-12-15 21:37:41.916677: Yayy! New best EMA pseudo Dice: 0.7302
2024-12-15 21:37:43.603830: 
2024-12-15 21:37:43.605235: Epoch 93
2024-12-15 21:37:43.606089: Current learning rate: 0.00419
2024-12-15 21:42:51.915311: Validation loss did not improve from -0.53466. Patience: 12/50
2024-12-15 21:42:51.916305: train_loss -0.7616
2024-12-15 21:42:51.917138: val_loss -0.5319
2024-12-15 21:42:51.917871: Pseudo dice [0.7486]
2024-12-15 21:42:51.918628: Epoch time: 308.31 s
2024-12-15 21:42:51.919277: Yayy! New best EMA pseudo Dice: 0.7321
2024-12-15 21:42:53.995219: 
2024-12-15 21:42:53.996664: Epoch 94
2024-12-15 21:42:53.997422: Current learning rate: 0.00412
2024-12-15 21:48:00.768214: Validation loss did not improve from -0.53466. Patience: 13/50
2024-12-15 21:48:00.770967: train_loss -0.761
2024-12-15 21:48:00.773196: val_loss -0.5161
2024-12-15 21:48:00.774120: Pseudo dice [0.7299]
2024-12-15 21:48:00.775277: Epoch time: 306.78 s
2024-12-15 21:48:02.458903: 
2024-12-15 21:48:02.460132: Epoch 95
2024-12-15 21:48:02.460871: Current learning rate: 0.00405
2024-12-15 21:53:19.114955: Validation loss did not improve from -0.53466. Patience: 14/50
2024-12-15 21:53:19.116267: train_loss -0.7589
2024-12-15 21:53:19.117093: val_loss -0.5217
2024-12-15 21:53:19.117780: Pseudo dice [0.7342]
2024-12-15 21:53:19.118651: Epoch time: 316.66 s
2024-12-15 21:53:19.119509: Yayy! New best EMA pseudo Dice: 0.7321
2024-12-15 21:53:20.828667: 
2024-12-15 21:53:20.829707: Epoch 96
2024-12-15 21:53:20.830516: Current learning rate: 0.00399
2024-12-15 21:58:45.029261: Validation loss did not improve from -0.53466. Patience: 15/50
2024-12-15 21:58:45.030330: train_loss -0.7617
2024-12-15 21:58:45.031477: val_loss -0.4631
2024-12-15 21:58:45.032587: Pseudo dice [0.712]
2024-12-15 21:58:45.033672: Epoch time: 324.2 s
2024-12-15 21:58:46.398598: 
2024-12-15 21:58:46.400111: Epoch 97
2024-12-15 21:58:46.401049: Current learning rate: 0.00392
2024-12-15 22:04:15.425130: Validation loss did not improve from -0.53466. Patience: 16/50
2024-12-15 22:04:15.427398: train_loss -0.7613
2024-12-15 22:04:15.428503: val_loss -0.494
2024-12-15 22:04:15.429535: Pseudo dice [0.7218]
2024-12-15 22:04:15.430615: Epoch time: 329.03 s
2024-12-15 22:04:16.805413: 
2024-12-15 22:04:16.806927: Epoch 98
2024-12-15 22:04:16.807924: Current learning rate: 0.00385
2024-12-15 22:09:26.586602: Validation loss did not improve from -0.53466. Patience: 17/50
2024-12-15 22:09:26.587558: train_loss -0.7653
2024-12-15 22:09:26.588414: val_loss -0.5127
2024-12-15 22:09:26.589121: Pseudo dice [0.733]
2024-12-15 22:09:26.589935: Epoch time: 309.78 s
2024-12-15 22:09:27.928879: 
2024-12-15 22:09:27.930051: Epoch 99
2024-12-15 22:09:27.931098: Current learning rate: 0.00379
2024-12-15 22:14:39.140240: Validation loss did not improve from -0.53466. Patience: 18/50
2024-12-15 22:14:39.141151: train_loss -0.7614
2024-12-15 22:14:39.142030: val_loss -0.5054
2024-12-15 22:14:39.142943: Pseudo dice [0.7305]
2024-12-15 22:14:39.143821: Epoch time: 311.21 s
2024-12-15 22:14:40.917469: 
2024-12-15 22:14:40.918489: Epoch 100
2024-12-15 22:14:40.919152: Current learning rate: 0.00372
2024-12-15 22:20:10.716121: Validation loss did not improve from -0.53466. Patience: 19/50
2024-12-15 22:20:10.717009: train_loss -0.7655
2024-12-15 22:20:10.717700: val_loss -0.5205
2024-12-15 22:20:10.718437: Pseudo dice [0.7385]
2024-12-15 22:20:10.719217: Epoch time: 329.8 s
2024-12-15 22:20:12.047211: 
2024-12-15 22:20:12.048558: Epoch 101
2024-12-15 22:20:12.049384: Current learning rate: 0.00365
2024-12-15 22:25:17.910665: Validation loss did not improve from -0.53466. Patience: 20/50
2024-12-15 22:25:17.911854: train_loss -0.7638
2024-12-15 22:25:17.912848: val_loss -0.5063
2024-12-15 22:25:17.913609: Pseudo dice [0.731]
2024-12-15 22:25:17.914714: Epoch time: 305.87 s
2024-12-15 22:25:19.300437: 
2024-12-15 22:25:19.301451: Epoch 102
2024-12-15 22:25:19.302210: Current learning rate: 0.00359
2024-12-15 22:30:33.764737: Validation loss did not improve from -0.53466. Patience: 21/50
2024-12-15 22:30:33.765499: train_loss -0.7693
2024-12-15 22:30:33.766335: val_loss -0.5243
2024-12-15 22:30:33.767085: Pseudo dice [0.7442]
2024-12-15 22:30:33.767789: Epoch time: 314.47 s
2024-12-15 22:30:35.092021: 
2024-12-15 22:30:35.093462: Epoch 103
2024-12-15 22:30:35.094334: Current learning rate: 0.00352
2024-12-15 22:36:07.422973: Validation loss did not improve from -0.53466. Patience: 22/50
2024-12-15 22:36:07.423808: train_loss -0.7679
2024-12-15 22:36:07.424557: val_loss -0.4917
2024-12-15 22:36:07.425177: Pseudo dice [0.727]
2024-12-15 22:36:07.425934: Epoch time: 332.33 s
2024-12-15 22:36:08.758532: 
2024-12-15 22:36:08.759743: Epoch 104
2024-12-15 22:36:08.760575: Current learning rate: 0.00345
2024-12-15 22:41:47.100197: Validation loss did not improve from -0.53466. Patience: 23/50
2024-12-15 22:41:47.101167: train_loss -0.7644
2024-12-15 22:41:47.102219: val_loss -0.4894
2024-12-15 22:41:47.103208: Pseudo dice [0.7224]
2024-12-15 22:41:47.104011: Epoch time: 338.34 s
2024-12-15 22:41:48.882159: 
2024-12-15 22:41:48.883303: Epoch 105
2024-12-15 22:41:48.883976: Current learning rate: 0.00338
2024-12-15 22:47:21.634731: Validation loss did not improve from -0.53466. Patience: 24/50
2024-12-15 22:47:21.635593: train_loss -0.7689
2024-12-15 22:47:21.636385: val_loss -0.5157
2024-12-15 22:47:21.637099: Pseudo dice [0.7295]
2024-12-15 22:47:21.637907: Epoch time: 332.75 s
2024-12-15 22:47:22.963097: 
2024-12-15 22:47:22.964150: Epoch 106
2024-12-15 22:47:22.964893: Current learning rate: 0.00332
2024-12-15 22:52:45.603015: Validation loss did not improve from -0.53466. Patience: 25/50
2024-12-15 22:52:45.604737: train_loss -0.7708
2024-12-15 22:52:45.606390: val_loss -0.5148
2024-12-15 22:52:45.607036: Pseudo dice [0.7439]
2024-12-15 22:52:45.608162: Epoch time: 322.64 s
2024-12-15 22:52:46.985434: 
2024-12-15 22:52:46.986832: Epoch 107
2024-12-15 22:52:46.987554: Current learning rate: 0.00325
2024-12-15 22:57:52.004802: Validation loss did not improve from -0.53466. Patience: 26/50
2024-12-15 22:57:52.005875: train_loss -0.769
2024-12-15 22:57:52.007253: val_loss -0.4716
2024-12-15 22:57:52.008049: Pseudo dice [0.713]
2024-12-15 22:57:52.008650: Epoch time: 305.02 s
2024-12-15 22:57:53.343307: 
2024-12-15 22:57:53.344483: Epoch 108
2024-12-15 22:57:53.345640: Current learning rate: 0.00318
2024-12-15 23:03:03.249316: Validation loss did not improve from -0.53466. Patience: 27/50
2024-12-15 23:03:03.250270: train_loss -0.7724
2024-12-15 23:03:03.251133: val_loss -0.4945
2024-12-15 23:03:03.251936: Pseudo dice [0.7319]
2024-12-15 23:03:03.253296: Epoch time: 309.91 s
2024-12-15 23:03:04.668201: 
2024-12-15 23:03:04.669432: Epoch 109
2024-12-15 23:03:04.670361: Current learning rate: 0.00311
2024-12-15 23:08:23.793687: Validation loss did not improve from -0.53466. Patience: 28/50
2024-12-15 23:08:23.796423: train_loss -0.7732
2024-12-15 23:08:23.797215: val_loss -0.5174
2024-12-15 23:08:23.798076: Pseudo dice [0.7373]
2024-12-15 23:08:23.798911: Epoch time: 319.13 s
2024-12-15 23:08:25.532310: 
2024-12-15 23:08:25.533684: Epoch 110
2024-12-15 23:08:25.535054: Current learning rate: 0.00304
2024-12-15 23:12:28.681010: Validation loss did not improve from -0.53466. Patience: 29/50
2024-12-15 23:12:28.681877: train_loss -0.7756
2024-12-15 23:12:28.682750: val_loss -0.5026
2024-12-15 23:12:28.683632: Pseudo dice [0.7304]
2024-12-15 23:12:28.684624: Epoch time: 243.15 s
2024-12-15 23:12:30.027905: 
2024-12-15 23:12:30.029504: Epoch 111
2024-12-15 23:12:30.030991: Current learning rate: 0.00297
2024-12-15 23:16:46.297845: Validation loss did not improve from -0.53466. Patience: 30/50
2024-12-15 23:16:46.298877: train_loss -0.7733
2024-12-15 23:16:46.299631: val_loss -0.5241
2024-12-15 23:16:46.300228: Pseudo dice [0.7383]
2024-12-15 23:16:46.300879: Epoch time: 256.27 s
2024-12-15 23:16:47.638917: 
2024-12-15 23:16:47.640447: Epoch 112
2024-12-15 23:16:47.641257: Current learning rate: 0.00291
2024-12-15 23:20:49.960117: Validation loss did not improve from -0.53466. Patience: 31/50
2024-12-15 23:20:49.961087: train_loss -0.7749
2024-12-15 23:20:49.962074: val_loss -0.499
2024-12-15 23:20:49.963023: Pseudo dice [0.726]
2024-12-15 23:20:49.963971: Epoch time: 242.32 s
2024-12-15 23:20:51.341693: 
2024-12-15 23:20:51.342861: Epoch 113
2024-12-15 23:20:51.343752: Current learning rate: 0.00284
2024-12-15 23:24:42.157686: Validation loss did not improve from -0.53466. Patience: 32/50
2024-12-15 23:24:42.158849: train_loss -0.7778
2024-12-15 23:24:42.159575: val_loss -0.5055
2024-12-15 23:24:42.160266: Pseudo dice [0.7344]
2024-12-15 23:24:42.161001: Epoch time: 230.82 s
2024-12-15 23:24:43.505574: 
2024-12-15 23:24:43.506647: Epoch 114
2024-12-15 23:24:43.507608: Current learning rate: 0.00277
2024-12-15 23:28:25.363147: Validation loss did not improve from -0.53466. Patience: 33/50
2024-12-15 23:28:25.363919: train_loss -0.7769
2024-12-15 23:28:25.364641: val_loss -0.5237
2024-12-15 23:28:25.365409: Pseudo dice [0.7399]
2024-12-15 23:28:25.366079: Epoch time: 221.86 s
2024-12-15 23:28:25.783450: Yayy! New best EMA pseudo Dice: 0.7322
2024-12-15 23:28:27.488783: 
2024-12-15 23:28:27.490114: Epoch 115
2024-12-15 23:28:27.490946: Current learning rate: 0.0027
2024-12-15 23:31:54.947255: Validation loss did not improve from -0.53466. Patience: 34/50
2024-12-15 23:31:54.948186: train_loss -0.7769
2024-12-15 23:31:54.948956: val_loss -0.5058
2024-12-15 23:31:54.949645: Pseudo dice [0.7356]
2024-12-15 23:31:54.950274: Epoch time: 207.46 s
2024-12-15 23:31:54.950886: Yayy! New best EMA pseudo Dice: 0.7325
2024-12-15 23:31:56.698174: 
2024-12-15 23:31:56.699852: Epoch 116
2024-12-15 23:31:56.701031: Current learning rate: 0.00263
2024-12-15 23:35:25.923597: Validation loss did not improve from -0.53466. Patience: 35/50
2024-12-15 23:35:25.924639: train_loss -0.7793
2024-12-15 23:35:25.925565: val_loss -0.4944
2024-12-15 23:35:25.926243: Pseudo dice [0.7251]
2024-12-15 23:35:25.926919: Epoch time: 209.23 s
2024-12-15 23:35:27.902880: 
2024-12-15 23:35:27.904398: Epoch 117
2024-12-15 23:35:27.905282: Current learning rate: 0.00256
2024-12-15 23:38:49.282113: Validation loss did not improve from -0.53466. Patience: 36/50
2024-12-15 23:38:49.283066: train_loss -0.7796
2024-12-15 23:38:49.283725: val_loss -0.4985
2024-12-15 23:38:49.284339: Pseudo dice [0.731]
2024-12-15 23:38:49.284994: Epoch time: 201.38 s
2024-12-15 23:38:50.670325: 
2024-12-15 23:38:50.672026: Epoch 118
2024-12-15 23:38:50.672790: Current learning rate: 0.00249
2024-12-15 23:42:15.016843: Validation loss did not improve from -0.53466. Patience: 37/50
2024-12-15 23:42:15.017665: train_loss -0.783
2024-12-15 23:42:15.018532: val_loss -0.5233
2024-12-15 23:42:15.019433: Pseudo dice [0.7345]
2024-12-15 23:42:15.020144: Epoch time: 204.35 s
2024-12-15 23:42:16.419687: 
2024-12-15 23:42:16.420696: Epoch 119
2024-12-15 23:42:16.421373: Current learning rate: 0.00242
2024-12-15 23:45:37.227680: Validation loss did not improve from -0.53466. Patience: 38/50
2024-12-15 23:45:37.228585: train_loss -0.784
2024-12-15 23:45:37.229258: val_loss -0.49
2024-12-15 23:45:37.229985: Pseudo dice [0.721]
2024-12-15 23:45:37.230752: Epoch time: 200.81 s
2024-12-15 23:45:39.004578: 
2024-12-15 23:45:39.005932: Epoch 120
2024-12-15 23:45:39.006668: Current learning rate: 0.00235
2024-12-15 23:48:58.484509: Validation loss did not improve from -0.53466. Patience: 39/50
2024-12-15 23:48:58.485526: train_loss -0.7857
2024-12-15 23:48:58.486408: val_loss -0.5045
2024-12-15 23:48:58.487204: Pseudo dice [0.7327]
2024-12-15 23:48:58.488072: Epoch time: 199.48 s
2024-12-15 23:48:59.893997: 
2024-12-15 23:48:59.895184: Epoch 121
2024-12-15 23:48:59.895950: Current learning rate: 0.00228
2024-12-15 23:52:28.383704: Validation loss did not improve from -0.53466. Patience: 40/50
2024-12-15 23:52:28.385295: train_loss -0.7836
2024-12-15 23:52:28.386825: val_loss -0.4928
2024-12-15 23:52:28.387574: Pseudo dice [0.7272]
2024-12-15 23:52:28.388291: Epoch time: 208.49 s
2024-12-15 23:52:29.752461: 
2024-12-15 23:52:29.753597: Epoch 122
2024-12-15 23:52:29.754380: Current learning rate: 0.00221
2024-12-15 23:55:55.846305: Validation loss did not improve from -0.53466. Patience: 41/50
2024-12-15 23:55:55.848296: train_loss -0.7827
2024-12-15 23:55:55.849983: val_loss -0.5039
2024-12-15 23:55:55.850806: Pseudo dice [0.7336]
2024-12-15 23:55:55.851828: Epoch time: 206.1 s
2024-12-15 23:55:57.234495: 
2024-12-15 23:55:57.235950: Epoch 123
2024-12-15 23:55:57.236799: Current learning rate: 0.00214
2024-12-15 23:59:21.973519: Validation loss did not improve from -0.53466. Patience: 42/50
2024-12-15 23:59:21.974693: train_loss -0.7865
2024-12-15 23:59:21.976155: val_loss -0.5006
2024-12-15 23:59:21.977386: Pseudo dice [0.7312]
2024-12-15 23:59:21.978556: Epoch time: 204.74 s
2024-12-15 23:59:23.356276: 
2024-12-15 23:59:23.357455: Epoch 124
2024-12-15 23:59:23.358205: Current learning rate: 0.00207
2024-12-16 00:02:41.136828: Validation loss did not improve from -0.53466. Patience: 43/50
2024-12-16 00:02:41.138347: train_loss -0.7861
2024-12-16 00:02:41.139511: val_loss -0.5132
2024-12-16 00:02:41.140247: Pseudo dice [0.7418]
2024-12-16 00:02:41.140898: Epoch time: 197.78 s
2024-12-16 00:02:42.878480: 
2024-12-16 00:02:42.879970: Epoch 125
2024-12-16 00:02:42.880700: Current learning rate: 0.00199
2024-12-16 00:06:09.994475: Validation loss did not improve from -0.53466. Patience: 44/50
2024-12-16 00:06:09.995657: train_loss -0.7865
2024-12-16 00:06:09.996473: val_loss -0.4968
2024-12-16 00:06:09.997171: Pseudo dice [0.7222]
2024-12-16 00:06:09.997768: Epoch time: 207.12 s
2024-12-16 00:06:11.390345: 
2024-12-16 00:06:11.391669: Epoch 126
2024-12-16 00:06:11.392373: Current learning rate: 0.00192
2024-12-16 00:09:33.813178: Validation loss did not improve from -0.53466. Patience: 45/50
2024-12-16 00:09:33.814176: train_loss -0.7845
2024-12-16 00:09:33.815045: val_loss -0.5121
2024-12-16 00:09:33.815809: Pseudo dice [0.7348]
2024-12-16 00:09:33.816502: Epoch time: 202.43 s
2024-12-16 00:09:35.763431: 
2024-12-16 00:09:35.764845: Epoch 127
2024-12-16 00:09:35.765594: Current learning rate: 0.00185
2024-12-16 00:12:59.151629: Validation loss did not improve from -0.53466. Patience: 46/50
2024-12-16 00:12:59.153777: train_loss -0.7866
2024-12-16 00:12:59.154646: val_loss -0.5253
2024-12-16 00:12:59.155312: Pseudo dice [0.7517]
2024-12-16 00:12:59.156052: Epoch time: 203.39 s
2024-12-16 00:12:59.156739: Yayy! New best EMA pseudo Dice: 0.7335
2024-12-16 00:13:02.009307: 
2024-12-16 00:13:02.010870: Epoch 128
2024-12-16 00:13:02.011714: Current learning rate: 0.00178
2024-12-16 00:16:16.085320: Validation loss did not improve from -0.53466. Patience: 47/50
2024-12-16 00:16:16.086673: train_loss -0.7906
2024-12-16 00:16:16.087851: val_loss -0.5198
2024-12-16 00:16:16.088841: Pseudo dice [0.7358]
2024-12-16 00:16:16.089830: Epoch time: 194.08 s
2024-12-16 00:16:16.090538: Yayy! New best EMA pseudo Dice: 0.7337
2024-12-16 00:16:19.011739: 
2024-12-16 00:16:19.012838: Epoch 129
2024-12-16 00:16:19.013579: Current learning rate: 0.0017
2024-12-16 00:19:34.040146: Validation loss did not improve from -0.53466. Patience: 48/50
2024-12-16 00:19:34.041065: train_loss -0.7883
2024-12-16 00:19:34.041811: val_loss -0.5137
2024-12-16 00:19:34.042450: Pseudo dice [0.7462]
2024-12-16 00:19:34.043196: Epoch time: 195.03 s
2024-12-16 00:19:34.478108: Yayy! New best EMA pseudo Dice: 0.735
2024-12-16 00:19:36.228734: 
2024-12-16 00:19:36.229982: Epoch 130
2024-12-16 00:19:36.230789: Current learning rate: 0.00163
2024-12-16 00:23:01.408379: Validation loss did not improve from -0.53466. Patience: 49/50
2024-12-16 00:23:01.409356: train_loss -0.789
2024-12-16 00:23:01.410296: val_loss -0.5112
2024-12-16 00:23:01.411083: Pseudo dice [0.7356]
2024-12-16 00:23:01.411864: Epoch time: 205.18 s
2024-12-16 00:23:01.412833: Yayy! New best EMA pseudo Dice: 0.735
2024-12-16 00:23:03.151031: 
2024-12-16 00:23:03.152502: Epoch 131
2024-12-16 00:23:03.153430: Current learning rate: 0.00156
2024-12-16 00:26:25.634237: Validation loss did not improve from -0.53466. Patience: 50/50
2024-12-16 00:26:25.635293: train_loss -0.7887
2024-12-16 00:26:25.636139: val_loss -0.5092
2024-12-16 00:26:25.636936: Pseudo dice [0.733]
2024-12-16 00:26:25.637724: Epoch time: 202.49 s
2024-12-16 00:26:27.030669: 
2024-12-16 00:26:27.032002: Epoch 132
2024-12-16 00:26:27.032817: Current learning rate: 0.00148
2024-12-16 00:29:55.676688: Validation loss did not improve from -0.53466. Patience: 51/50
2024-12-16 00:29:55.677499: train_loss -0.7924
2024-12-16 00:29:55.678278: val_loss -0.5231
2024-12-16 00:29:55.678949: Pseudo dice [0.7411]
2024-12-16 00:29:55.679704: Epoch time: 208.65 s
2024-12-16 00:29:55.680331: Yayy! New best EMA pseudo Dice: 0.7354
2024-12-16 00:29:57.426948: 
2024-12-16 00:29:57.428315: Epoch 133
2024-12-16 00:29:57.429112: Current learning rate: 0.00141
2024-12-16 00:33:19.149177: Validation loss did not improve from -0.53466. Patience: 52/50
2024-12-16 00:33:19.150178: train_loss -0.79
2024-12-16 00:33:19.150949: val_loss -0.5283
2024-12-16 00:33:19.151704: Pseudo dice [0.7444]
2024-12-16 00:33:19.152327: Epoch time: 201.72 s
2024-12-16 00:33:19.152918: Yayy! New best EMA pseudo Dice: 0.7363
2024-12-16 00:33:20.875648: 
2024-12-16 00:33:20.876804: Epoch 134
2024-12-16 00:33:20.877482: Current learning rate: 0.00133
2024-12-16 00:36:40.275485: Validation loss did not improve from -0.53466. Patience: 53/50
2024-12-16 00:36:40.276374: train_loss -0.7907
2024-12-16 00:36:40.277435: val_loss -0.4979
2024-12-16 00:36:40.278609: Pseudo dice [0.7295]
2024-12-16 00:36:40.279913: Epoch time: 199.4 s
2024-12-16 00:36:42.065092: 
2024-12-16 00:36:42.066158: Epoch 135
2024-12-16 00:36:42.067203: Current learning rate: 0.00126
2024-12-16 00:40:03.899808: Validation loss did not improve from -0.53466. Patience: 54/50
2024-12-16 00:40:03.900596: train_loss -0.7915
2024-12-16 00:40:03.901503: val_loss -0.4954
2024-12-16 00:40:03.902260: Pseudo dice [0.7332]
2024-12-16 00:40:03.903062: Epoch time: 201.84 s
2024-12-16 00:40:05.289032: 
2024-12-16 00:40:05.290196: Epoch 136
2024-12-16 00:40:05.290994: Current learning rate: 0.00118
2024-12-16 00:43:39.661436: Validation loss did not improve from -0.53466. Patience: 55/50
2024-12-16 00:43:39.662446: train_loss -0.7948
2024-12-16 00:43:39.663467: val_loss -0.4971
2024-12-16 00:43:39.664432: Pseudo dice [0.7342]
2024-12-16 00:43:39.665252: Epoch time: 214.37 s
2024-12-16 00:43:41.080425: 
2024-12-16 00:43:41.081754: Epoch 137
2024-12-16 00:43:41.082659: Current learning rate: 0.00111
2024-12-16 00:46:57.534922: Validation loss did not improve from -0.53466. Patience: 56/50
2024-12-16 00:46:57.535935: train_loss -0.7945
2024-12-16 00:46:57.536737: val_loss -0.5249
2024-12-16 00:46:57.537833: Pseudo dice [0.74]
2024-12-16 00:46:57.538761: Epoch time: 196.46 s
2024-12-16 00:46:59.444468: 
2024-12-16 00:46:59.445812: Epoch 138
2024-12-16 00:46:59.446772: Current learning rate: 0.00103
2024-12-16 00:50:26.566817: Validation loss did not improve from -0.53466. Patience: 57/50
2024-12-16 00:50:26.567809: train_loss -0.7948
2024-12-16 00:50:26.568779: val_loss -0.5112
2024-12-16 00:50:26.569431: Pseudo dice [0.7444]
2024-12-16 00:50:26.570105: Epoch time: 207.12 s
2024-12-16 00:50:26.570863: Yayy! New best EMA pseudo Dice: 0.7366
2024-12-16 00:50:28.311659: 
2024-12-16 00:50:28.312949: Epoch 139
2024-12-16 00:50:28.313627: Current learning rate: 0.00095
2024-12-16 00:53:35.809043: Validation loss did not improve from -0.53466. Patience: 58/50
2024-12-16 00:53:35.809810: train_loss -0.7939
2024-12-16 00:53:35.810477: val_loss -0.5107
2024-12-16 00:53:35.811119: Pseudo dice [0.7381]
2024-12-16 00:53:35.811741: Epoch time: 187.5 s
2024-12-16 00:53:36.207735: Yayy! New best EMA pseudo Dice: 0.7368
2024-12-16 00:53:37.950598: 
2024-12-16 00:53:37.952013: Epoch 140
2024-12-16 00:53:37.952887: Current learning rate: 0.00087
2024-12-16 00:55:44.417430: Validation loss did not improve from -0.53466. Patience: 59/50
2024-12-16 00:55:44.418694: train_loss -0.7923
2024-12-16 00:55:44.419981: val_loss -0.4952
2024-12-16 00:55:44.421286: Pseudo dice [0.7208]
2024-12-16 00:55:44.422079: Epoch time: 126.47 s
2024-12-16 00:55:45.861356: 
2024-12-16 00:55:45.862656: Epoch 141
2024-12-16 00:55:45.863445: Current learning rate: 0.00079
2024-12-16 00:57:22.444925: Validation loss did not improve from -0.53466. Patience: 60/50
2024-12-16 00:57:22.445891: train_loss -0.7911
2024-12-16 00:57:22.446780: val_loss -0.4843
2024-12-16 00:57:22.447604: Pseudo dice [0.7312]
2024-12-16 00:57:22.448298: Epoch time: 96.59 s
2024-12-16 00:57:23.854576: 
2024-12-16 00:57:23.856288: Epoch 142
2024-12-16 00:57:23.857122: Current learning rate: 0.00071
2024-12-16 00:58:58.792825: Validation loss did not improve from -0.53466. Patience: 61/50
2024-12-16 00:58:58.793767: train_loss -0.7942
2024-12-16 00:58:58.794929: val_loss -0.5026
2024-12-16 00:58:58.796085: Pseudo dice [0.7277]
2024-12-16 00:58:58.797151: Epoch time: 94.94 s
2024-12-16 00:59:00.169277: 
2024-12-16 00:59:00.170771: Epoch 143
2024-12-16 00:59:00.171826: Current learning rate: 0.00063
2024-12-16 01:00:33.265227: Validation loss did not improve from -0.53466. Patience: 62/50
2024-12-16 01:00:33.266545: train_loss -0.7978
2024-12-16 01:00:33.267586: val_loss -0.5075
2024-12-16 01:00:33.268311: Pseudo dice [0.7332]
2024-12-16 01:00:33.268984: Epoch time: 93.1 s
2024-12-16 01:00:34.696563: 
2024-12-16 01:00:34.698479: Epoch 144
2024-12-16 01:00:34.699377: Current learning rate: 0.00055
2024-12-16 01:02:10.926550: Validation loss did not improve from -0.53466. Patience: 63/50
2024-12-16 01:02:10.928864: train_loss -0.7969
2024-12-16 01:02:10.931128: val_loss -0.5106
2024-12-16 01:02:10.931950: Pseudo dice [0.746]
2024-12-16 01:02:10.932906: Epoch time: 96.23 s
2024-12-16 01:02:12.772530: 
2024-12-16 01:02:12.774126: Epoch 145
2024-12-16 01:02:12.774828: Current learning rate: 0.00047
2024-12-16 01:03:44.760862: Validation loss did not improve from -0.53466. Patience: 64/50
2024-12-16 01:03:44.763229: train_loss -0.7948
2024-12-16 01:03:44.764360: val_loss -0.5016
2024-12-16 01:03:44.765029: Pseudo dice [0.7453]
2024-12-16 01:03:44.765777: Epoch time: 91.99 s
2024-12-16 01:03:46.091586: 
2024-12-16 01:03:46.093094: Epoch 146
2024-12-16 01:03:46.093886: Current learning rate: 0.00038
2024-12-16 01:05:18.114608: Validation loss did not improve from -0.53466. Patience: 65/50
2024-12-16 01:05:18.115855: train_loss -0.7977
2024-12-16 01:05:18.117391: val_loss -0.4953
2024-12-16 01:05:18.118481: Pseudo dice [0.7297]
2024-12-16 01:05:18.119634: Epoch time: 92.03 s
2024-12-16 01:05:19.455394: 
2024-12-16 01:05:19.457248: Epoch 147
2024-12-16 01:05:19.458317: Current learning rate: 0.0003
2024-12-16 01:06:51.739144: Validation loss did not improve from -0.53466. Patience: 66/50
2024-12-16 01:06:51.740117: train_loss -0.799
2024-12-16 01:06:51.741111: val_loss -0.5204
2024-12-16 01:06:51.741780: Pseudo dice [0.7451]
2024-12-16 01:06:51.742649: Epoch time: 92.29 s
2024-12-16 01:06:53.088317: 
2024-12-16 01:06:53.089912: Epoch 148
2024-12-16 01:06:53.090847: Current learning rate: 0.00021
2024-12-16 01:08:25.516829: Validation loss did not improve from -0.53466. Patience: 67/50
2024-12-16 01:08:25.518042: train_loss -0.7991
2024-12-16 01:08:25.518950: val_loss -0.4996
2024-12-16 01:08:25.519759: Pseudo dice [0.7319]
2024-12-16 01:08:25.520605: Epoch time: 92.43 s
2024-12-16 01:08:27.444389: 
2024-12-16 01:08:27.446179: Epoch 149
2024-12-16 01:08:27.447411: Current learning rate: 0.00011
2024-12-16 01:09:59.606431: Validation loss did not improve from -0.53466. Patience: 68/50
2024-12-16 01:09:59.607787: train_loss -0.7968
2024-12-16 01:09:59.608705: val_loss -0.5311
2024-12-16 01:09:59.609454: Pseudo dice [0.7502]
2024-12-16 01:09:59.610246: Epoch time: 92.16 s
2024-12-16 01:09:59.611035: Yayy! New best EMA pseudo Dice: 0.7375
2024-12-16 01:10:01.814423: Training done.
2024-12-16 01:10:02.021880: Using splits from existing split file: /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset307_Sohee_Calcium_OCT_CrossValidation/splits_final_40.json
2024-12-16 01:10:02.036855: The split file contains 5 splits.
2024-12-16 01:10:02.037980: Desired fold for training: 4
2024-12-16 01:10:02.039004: This split has 3 training and 5 validation cases.
2024-12-16 01:10:02.040256: predicting 101-044
2024-12-16 01:10:02.135720: 101-044, shape torch.Size([1, 404, 498, 498]), rank 0
2024-12-16 01:11:49.132783: predicting 101-045
2024-12-16 01:11:49.219017: 101-045, shape torch.Size([1, 375, 498, 498]), rank 0
2024-12-16 01:13:15.949726: predicting 401-004
2024-12-16 01:13:16.046170: 401-004, shape torch.Size([1, 375, 498, 498]), rank 0
2024-12-16 01:14:43.906792: predicting 704-003
2024-12-16 01:14:43.990909: 704-003, shape torch.Size([1, 375, 498, 498]), rank 0
2024-12-16 01:16:10.816167: predicting 706-005
2024-12-16 01:16:10.908695: 706-005, shape torch.Size([1, 375, 498, 498]), rank 0
2024-12-16 01:18:03.715261: Validation complete
2024-12-16 01:18:03.716816: Mean Validation Dice:  0.7168445240585162
