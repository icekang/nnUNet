/home/gridsan/nchutisilp/.conda/envs/nnunet/bin/python
Starting training with CONFIG=3d_32x160x128_b10, DATASET_ID=307, TRAINER=nnUNetTrainer

############################
INFO: You are using the old nnU-Net default plans. We have updated our recommendations. Please consider using those instead! Read more here: https://github.com/MIC-DKFZ/nnUNet/blob/master/documentation/resenc_presets.md
############################

Using device: cuda:0

#######################################################################
Please cite the following paper when using nnU-Net:
Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.
#######################################################################

2024-12-17 00:59:42.709195: Using torch.compile...

############################
INFO: You are using the old nnU-Net default plans. We have updated our recommendations. Please consider using those instead! Read more here: https://github.com/MIC-DKFZ/nnUNet/blob/master/documentation/resenc_presets.md
############################

Using device: cuda:0

#######################################################################
Please cite the following paper when using nnU-Net:
Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.
#######################################################################

2024-12-17 00:59:42.713091: Using torch.compile...
/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
################### Loading pretrained weights from file  /home/gridsan/nchutisilp/projects/OpenAI-CLIP/clip_preivl_poststent_pretrained_nnUNet.pt ###################
Below is the list of overlapping blocks in pretrained model and nnUNet architecture:
encoder.stages.0.0.convs.0.conv.weight shape torch.Size([32, 1, 3, 3, 3])
encoder.stages.0.0.convs.0.conv.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.norm.weight shape torch.Size([32])
encoder.stages.0.0.convs.0.norm.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.0.weight shape torch.Size([32, 1, 3, 3, 3])
encoder.stages.0.0.convs.0.all_modules.0.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.1.weight shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.1.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
encoder.stages.0.0.convs.1.conv.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.norm.weight shape torch.Size([32])
encoder.stages.0.0.convs.1.norm.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
encoder.stages.0.0.convs.1.all_modules.0.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.1.weight shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.1.bias shape torch.Size([32])
encoder.stages.1.0.convs.0.conv.weight shape torch.Size([64, 32, 3, 3, 3])
encoder.stages.1.0.convs.0.conv.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.norm.weight shape torch.Size([64])
encoder.stages.1.0.convs.0.norm.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.0.weight shape torch.Size([64, 32, 3, 3, 3])
encoder.stages.1.0.convs.0.all_modules.0.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.1.weight shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.1.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
encoder.stages.1.0.convs.1.conv.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.norm.weight shape torch.Size([64])
encoder.stages.1.0.convs.1.norm.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
encoder.stages.1.0.convs.1.all_modules.0.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.1.weight shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.1.bias shape torch.Size([64])
encoder.stages.2.0.convs.0.conv.weight shape torch.Size([128, 64, 3, 3, 3])
encoder.stages.2.0.convs.0.conv.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.norm.weight shape torch.Size([128])
encoder.stages.2.0.convs.0.norm.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.0.weight shape torch.Size([128, 64, 3, 3, 3])
encoder.stages.2.0.convs.0.all_modules.0.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.1.weight shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.1.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
encoder.stages.2.0.convs.1.conv.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.norm.weight shape torch.Size([128])
encoder.stages.2.0.convs.1.norm.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
encoder.stages.2.0.convs.1.all_modules.0.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.1.weight shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.1.bias shape torch.Size([128])
encoder.stages.3.0.convs.0.conv.weight shape torch.Size([256, 128, 3, 3, 3])
encoder.stages.3.0.convs.0.conv.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.norm.weight shape torch.Size([256])
encoder.stages.3.0.convs.0.norm.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.0.weight shape torch.Size([256, 128, 3, 3, 3])
encoder.stages.3.0.convs.0.all_modules.0.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.1.weight shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.1.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
encoder.stages.3.0.convs.1.conv.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.norm.weight shape torch.Size([256])
encoder.stages.3.0.convs.1.norm.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
encoder.stages.3.0.convs.1.all_modules.0.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.1.weight shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.1.bias shape torch.Size([256])
encoder.stages.4.0.convs.0.conv.weight shape torch.Size([320, 256, 3, 3, 3])
encoder.stages.4.0.convs.0.conv.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.norm.weight shape torch.Size([320])
encoder.stages.4.0.convs.0.norm.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.0.weight shape torch.Size([320, 256, 3, 3, 3])
encoder.stages.4.0.convs.0.all_modules.0.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.1.weight shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.1.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.4.0.convs.1.conv.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.norm.weight shape torch.Size([320])
encoder.stages.4.0.convs.1.norm.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.4.0.convs.1.all_modules.0.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.1.weight shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.1.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.0.conv.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.norm.weight shape torch.Size([320])
encoder.stages.5.0.convs.0.norm.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.0.all_modules.0.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.1.weight shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.1.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.1.conv.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.norm.weight shape torch.Size([320])
encoder.stages.5.0.convs.1.norm.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.1.all_modules.0.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.1.weight shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.0.0.convs.0.conv.weight shape torch.Size([32, 1, 3, 3, 3])
decoder.encoder.stages.0.0.convs.0.conv.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.norm.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.norm.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.0.weight shape torch.Size([32, 1, 3, 3, 3])
decoder.encoder.stages.0.0.convs.0.all_modules.0.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.1.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.1.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.encoder.stages.0.0.convs.1.conv.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.norm.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.norm.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.encoder.stages.0.0.convs.1.all_modules.0.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.1.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.1.bias shape torch.Size([32])
decoder.encoder.stages.1.0.convs.0.conv.weight shape torch.Size([64, 32, 3, 3, 3])
decoder.encoder.stages.1.0.convs.0.conv.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.norm.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.norm.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.0.weight shape torch.Size([64, 32, 3, 3, 3])
decoder.encoder.stages.1.0.convs.0.all_modules.0.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.1.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.1.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.encoder.stages.1.0.convs.1.conv.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.norm.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.norm.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.encoder.stages.1.0.convs.1.all_modules.0.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.1.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.1.bias shape torch.Size([64])
decoder.encoder.stages.2.0.convs.0.conv.weight shape torch.Size([128, 64, 3, 3, 3])
decoder.encoder.stages.2.0.convs.0.conv.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.norm.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.norm.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.0.weight shape torch.Size([128, 64, 3, 3, 3])
decoder.encoder.stages.2.0.convs.0.all_modules.0.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.1.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.1.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.encoder.stages.2.0.convs.1.conv.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.norm.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.norm.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.encoder.stages.2.0.convs.1.all_modules.0.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.1.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.1.bias shape torch.Size([128])
decoder.encoder.stages.3.0.convs.0.conv.weight shape torch.Size([256, 128, 3, 3, 3])
decoder.encoder.stages.3.0.convs.0.conv.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.norm.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.norm.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.0.weight shape torch.Size([256, 128, 3, 3, 3])
decoder.encoder.stages.3.0.convs.0.all_modules.0.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.1.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.1.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.encoder.stages.3.0.convs.1.conv.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.norm.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.norm.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.encoder.stages.3.0.convs.1.all_modules.0.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.1.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.1.bias shape torch.Size([256])
decoder.encoder.stages.4.0.convs.0.conv.weight shape torch.Size([320, 256, 3, 3, 3])
decoder.encoder.stages.4.0.convs.0.conv.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.norm.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.norm.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.0.weight shape torch.Size([320, 256, 3, 3, 3])
decoder.encoder.stages.4.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.4.0.convs.1.conv.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.norm.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.norm.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.4.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.0.conv.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.norm.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.norm.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.1.conv.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.norm.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.norm.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.stages.0.convs.0.conv.weight shape torch.Size([320, 640, 3, 3, 3])
decoder.stages.0.convs.0.conv.bias shape torch.Size([320])
decoder.stages.0.convs.0.norm.weight shape torch.Size([320])
decoder.stages.0.convs.0.norm.bias shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.0.weight shape torch.Size([320, 640, 3, 3, 3])
decoder.stages.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.stages.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.stages.0.convs.1.conv.bias shape torch.Size([320])
decoder.stages.0.convs.1.norm.weight shape torch.Size([320])
decoder.stages.0.convs.1.norm.bias shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.stages.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.stages.1.convs.0.conv.weight shape torch.Size([256, 512, 3, 3, 3])
decoder.stages.1.convs.0.conv.bias shape torch.Size([256])
decoder.stages.1.convs.0.norm.weight shape torch.Size([256])
decoder.stages.1.convs.0.norm.bias shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.0.weight shape torch.Size([256, 512, 3, 3, 3])
decoder.stages.1.convs.0.all_modules.0.bias shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.1.weight shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.1.bias shape torch.Size([256])
decoder.stages.1.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.stages.1.convs.1.conv.bias shape torch.Size([256])
decoder.stages.1.convs.1.norm.weight shape torch.Size([256])
decoder.stages.1.convs.1.norm.bias shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.stages.1.convs.1.all_modules.0.bias shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.1.weight shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.1.bias shape torch.Size([256])
decoder.stages.2.convs.0.conv.weight shape torch.Size([128, 256, 3, 3, 3])
decoder.stages.2.convs.0.conv.bias shape torch.Size([128])
decoder.stages.2.convs.0.norm.weight shape torch.Size([128])
decoder.stages.2.convs.0.norm.bias shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.0.weight shape torch.Size([128, 256, 3, 3, 3])
decoder.stages.2.convs.0.all_modules.0.bias shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.1.weight shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.1.bias shape torch.Size([128])
decoder.stages.2.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.stages.2.convs.1.conv.bias shape torch.Size([128])
decoder.stages.2.convs.1.norm.weight shape torch.Size([128])
decoder.stages.2.convs.1.norm.bias shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.stages.2.convs.1.all_modules.0.bias shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.1.weight shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.1.bias shape torch.Size([128])
decoder.stages.3.convs.0.conv.weight shape torch.Size([64, 128, 3, 3, 3])
decoder.stages.3.convs.0.conv.bias shape torch.Size([64])
decoder.stages.3.convs.0.norm.weight shape torch.Size([64])
decoder.stages.3.convs.0.norm.bias shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.0.weight shape torch.Size([64, 128, 3, 3, 3])
decoder.stages.3.convs.0.all_modules.0.bias shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.1.weight shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.1.bias shape torch.Size([64])
decoder.stages.3.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.stages.3.convs.1.conv.bias shape torch.Size([64])
decoder.stages.3.convs.1.norm.weight shape torch.Size([64])
decoder.stages.3.convs.1.norm.bias shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.stages.3.convs.1.all_modules.0.bias shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.1.weight shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.1.bias shape torch.Size([64])
decoder.stages.4.convs.0.conv.weight shape torch.Size([32, 64, 3, 3, 3])
decoder.stages.4.convs.0.conv.bias shape torch.Size([32])
decoder.stages.4.convs.0.norm.weight shape torch.Size([32])
decoder.stages.4.convs.0.norm.bias shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.0.weight shape torch.Size([32, 64, 3, 3, 3])
decoder.stages.4.convs.0.all_modules.0.bias shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.1.weight shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.1.bias shape torch.Size([32])
decoder.stages.4.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.stages.4.convs.1.conv.bias shape torch.Size([32])
decoder.stages.4.convs.1.norm.weight shape torch.Size([32])
decoder.stages.4.convs.1.norm.bias shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.stages.4.convs.1.all_modules.0.bias shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.1.weight shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.1.bias shape torch.Size([32])
decoder.transpconvs.0.weight shape torch.Size([320, 320, 1, 2, 2])
decoder.transpconvs.0.bias shape torch.Size([320])
decoder.transpconvs.1.weight shape torch.Size([320, 256, 2, 2, 2])
decoder.transpconvs.1.bias shape torch.Size([256])
decoder.transpconvs.2.weight shape torch.Size([256, 128, 2, 2, 2])
decoder.transpconvs.2.bias shape torch.Size([128])
decoder.transpconvs.3.weight shape torch.Size([128, 64, 2, 2, 2])
decoder.transpconvs.3.bias shape torch.Size([64])
decoder.transpconvs.4.weight shape torch.Size([64, 32, 2, 2, 2])
decoder.transpconvs.4.bias shape torch.Size([32])
################### Done ###################
2024-12-17 00:59:45.336218: do_dummy_2d_data_aug: True
2024-12-17 00:59:45.379344: Using splits from existing split file: /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset307_Sohee_Calcium_OCT_CrossValidation/splits_final.json
2024-12-17 00:59:45.403382: The split file contains 5 splits.
2024-12-17 00:59:45.405569: Desired fold for training: 1
2024-12-17 00:59:45.406629: This split has 6 training and 2 validation cases.
################### Loading pretrained weights from file  /home/gridsan/nchutisilp/projects/OpenAI-CLIP/clip_preivl_poststent_pretrained_nnUNet.pt ###################
Below is the list of overlapping blocks in pretrained model and nnUNet architecture:
encoder.stages.0.0.convs.0.conv.weight shape torch.Size([32, 1, 3, 3, 3])
encoder.stages.0.0.convs.0.conv.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.norm.weight shape torch.Size([32])
encoder.stages.0.0.convs.0.norm.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.0.weight shape torch.Size([32, 1, 3, 3, 3])
encoder.stages.0.0.convs.0.all_modules.0.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.1.weight shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.1.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
encoder.stages.0.0.convs.1.conv.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.norm.weight shape torch.Size([32])
encoder.stages.0.0.convs.1.norm.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
encoder.stages.0.0.convs.1.all_modules.0.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.1.weight shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.1.bias shape torch.Size([32])
encoder.stages.1.0.convs.0.conv.weight shape torch.Size([64, 32, 3, 3, 3])
encoder.stages.1.0.convs.0.conv.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.norm.weight shape torch.Size([64])
encoder.stages.1.0.convs.0.norm.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.0.weight shape torch.Size([64, 32, 3, 3, 3])
encoder.stages.1.0.convs.0.all_modules.0.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.1.weight shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.1.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
encoder.stages.1.0.convs.1.conv.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.norm.weight shape torch.Size([64])
encoder.stages.1.0.convs.1.norm.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
encoder.stages.1.0.convs.1.all_modules.0.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.1.weight shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.1.bias shape torch.Size([64])
encoder.stages.2.0.convs.0.conv.weight shape torch.Size([128, 64, 3, 3, 3])
encoder.stages.2.0.convs.0.conv.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.norm.weight shape torch.Size([128])
encoder.stages.2.0.convs.0.norm.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.0.weight shape torch.Size([128, 64, 3, 3, 3])
encoder.stages.2.0.convs.0.all_modules.0.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.1.weight shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.1.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
encoder.stages.2.0.convs.1.conv.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.norm.weight shape torch.Size([128])
encoder.stages.2.0.convs.1.norm.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
encoder.stages.2.0.convs.1.all_modules.0.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.1.weight shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.1.bias shape torch.Size([128])
encoder.stages.3.0.convs.0.conv.weight shape torch.Size([256, 128, 3, 3, 3])
encoder.stages.3.0.convs.0.conv.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.norm.weight shape torch.Size([256])
encoder.stages.3.0.convs.0.norm.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.0.weight shape torch.Size([256, 128, 3, 3, 3])
encoder.stages.3.0.convs.0.all_modules.0.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.1.weight shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.1.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
encoder.stages.3.0.convs.1.conv.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.norm.weight shape torch.Size([256])
encoder.stages.3.0.convs.1.norm.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
encoder.stages.3.0.convs.1.all_modules.0.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.1.weight shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.1.bias shape torch.Size([256])
encoder.stages.4.0.convs.0.conv.weight shape torch.Size([320, 256, 3, 3, 3])
encoder.stages.4.0.convs.0.conv.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.norm.weight shape torch.Size([320])
encoder.stages.4.0.convs.0.norm.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.0.weight shape torch.Size([320, 256, 3, 3, 3])
encoder.stages.4.0.convs.0.all_modules.0.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.1.weight shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.1.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.4.0.convs.1.conv.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.norm.weight shape torch.Size([320])
encoder.stages.4.0.convs.1.norm.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.4.0.convs.1.all_modules.0.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.1.weight shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.1.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.0.conv.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.norm.weight shape torch.Size([320])
encoder.stages.5.0.convs.0.norm.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.0.all_modules.0.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.1.weight shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.1.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.1.conv.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.norm.weight shape torch.Size([320])
encoder.stages.5.0.convs.1.norm.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.1.all_modules.0.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.1.weight shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.0.0.convs.0.conv.weight shape torch.Size([32, 1, 3, 3, 3])
decoder.encoder.stages.0.0.convs.0.conv.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.norm.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.norm.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.0.weight shape torch.Size([32, 1, 3, 3, 3])
decoder.encoder.stages.0.0.convs.0.all_modules.0.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.1.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.1.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.encoder.stages.0.0.convs.1.conv.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.norm.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.norm.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.encoder.stages.0.0.convs.1.all_modules.0.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.1.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.1.bias shape torch.Size([32])
decoder.encoder.stages.1.0.convs.0.conv.weight shape torch.Size([64, 32, 3, 3, 3])
decoder.encoder.stages.1.0.convs.0.conv.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.norm.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.norm.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.0.weight shape torch.Size([64, 32, 3, 3, 3])
decoder.encoder.stages.1.0.convs.0.all_modules.0.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.1.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.1.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.encoder.stages.1.0.convs.1.conv.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.norm.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.norm.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.encoder.stages.1.0.convs.1.all_modules.0.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.1.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.1.bias shape torch.Size([64])
decoder.encoder.stages.2.0.convs.0.conv.weight shape torch.Size([128, 64, 3, 3, 3])
decoder.encoder.stages.2.0.convs.0.conv.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.norm.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.norm.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.0.weight shape torch.Size([128, 64, 3, 3, 3])
decoder.encoder.stages.2.0.convs.0.all_modules.0.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.1.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.1.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.encoder.stages.2.0.convs.1.conv.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.norm.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.norm.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.encoder.stages.2.0.convs.1.all_modules.0.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.1.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.1.bias shape torch.Size([128])
decoder.encoder.stages.3.0.convs.0.conv.weight shape torch.Size([256, 128, 3, 3, 3])
decoder.encoder.stages.3.0.convs.0.conv.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.norm.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.norm.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.0.weight shape torch.Size([256, 128, 3, 3, 3])
decoder.encoder.stages.3.0.convs.0.all_modules.0.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.1.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.1.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.encoder.stages.3.0.convs.1.conv.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.norm.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.norm.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.encoder.stages.3.0.convs.1.all_modules.0.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.1.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.1.bias shape torch.Size([256])
decoder.encoder.stages.4.0.convs.0.conv.weight shape torch.Size([320, 256, 3, 3, 3])
decoder.encoder.stages.4.0.convs.0.conv.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.norm.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.norm.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.0.weight shape torch.Size([320, 256, 3, 3, 3])
decoder.encoder.stages.4.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.4.0.convs.1.conv.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.norm.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.norm.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.4.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.0.conv.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.norm.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.norm.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.1.conv.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.norm.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.norm.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.stages.0.convs.0.conv.weight shape torch.Size([320, 640, 3, 3, 3])
decoder.stages.0.convs.0.conv.bias shape torch.Size([320])
decoder.stages.0.convs.0.norm.weight shape torch.Size([320])
decoder.stages.0.convs.0.norm.bias shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.0.weight shape torch.Size([320, 640, 3, 3, 3])
decoder.stages.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.stages.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.stages.0.convs.1.conv.bias shape torch.Size([320])
decoder.stages.0.convs.1.norm.weight shape torch.Size([320])
decoder.stages.0.convs.1.norm.bias shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.stages.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.stages.1.convs.0.conv.weight shape torch.Size([256, 512, 3, 3, 3])
decoder.stages.1.convs.0.conv.bias shape torch.Size([256])
decoder.stages.1.convs.0.norm.weight shape torch.Size([256])
decoder.stages.1.convs.0.norm.bias shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.0.weight shape torch.Size([256, 512, 3, 3, 3])
decoder.stages.1.convs.0.all_modules.0.bias shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.1.weight shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.1.bias shape torch.Size([256])
decoder.stages.1.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.stages.1.convs.1.conv.bias shape torch.Size([256])
decoder.stages.1.convs.1.norm.weight shape torch.Size([256])
decoder.stages.1.convs.1.norm.bias shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.stages.1.convs.1.all_modules.0.bias shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.1.weight shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.1.bias shape torch.Size([256])
decoder.stages.2.convs.0.conv.weight shape torch.Size([128, 256, 3, 3, 3])
decoder.stages.2.convs.0.conv.bias shape torch.Size([128])
decoder.stages.2.convs.0.norm.weight shape torch.Size([128])
decoder.stages.2.convs.0.norm.bias shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.0.weight shape torch.Size([128, 256, 3, 3, 3])
decoder.stages.2.convs.0.all_modules.0.bias shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.1.weight shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.1.bias shape torch.Size([128])
decoder.stages.2.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.stages.2.convs.1.conv.bias shape torch.Size([128])
decoder.stages.2.convs.1.norm.weight shape torch.Size([128])
decoder.stages.2.convs.1.norm.bias shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.stages.2.convs.1.all_modules.0.bias shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.1.weight shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.1.bias shape torch.Size([128])
decoder.stages.3.convs.0.conv.weight shape torch.Size([64, 128, 3, 3, 3])
decoder.stages.3.convs.0.conv.bias shape torch.Size([64])
decoder.stages.3.convs.0.norm.weight shape torch.Size([64])
decoder.stages.3.convs.0.norm.bias shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.0.weight shape torch.Size([64, 128, 3, 3, 3])
decoder.stages.3.convs.0.all_modules.0.bias shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.1.weight shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.1.bias shape torch.Size([64])
decoder.stages.3.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.stages.3.convs.1.conv.bias shape torch.Size([64])
decoder.stages.3.convs.1.norm.weight shape torch.Size([64])
decoder.stages.3.convs.1.norm.bias shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.stages.3.convs.1.all_modules.0.bias shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.1.weight shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.1.bias shape torch.Size([64])
decoder.stages.4.convs.0.conv.weight shape torch.Size([32, 64, 3, 3, 3])
decoder.stages.4.convs.0.conv.bias shape torch.Size([32])
decoder.stages.4.convs.0.norm.weight shape torch.Size([32])
decoder.stages.4.convs.0.norm.bias shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.0.weight shape torch.Size([32, 64, 3, 3, 3])
decoder.stages.4.convs.0.all_modules.0.bias shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.1.weight shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.1.bias shape torch.Size([32])
decoder.stages.4.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.stages.4.convs.1.conv.bias shape torch.Size([32])
decoder.stages.4.convs.1.norm.weight shape torch.Size([32])
decoder.stages.4.convs.1.norm.bias shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.stages.4.convs.1.all_modules.0.bias shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.1.weight shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.1.bias shape torch.Size([32])
decoder.transpconvs.0.weight shape torch.Size([320, 320, 1, 2, 2])
decoder.transpconvs.0.bias shape torch.Size([320])
decoder.transpconvs.1.weight shape torch.Size([320, 256, 2, 2, 2])
decoder.transpconvs.1.bias shape torch.Size([256])
decoder.transpconvs.2.weight shape torch.Size([256, 128, 2, 2, 2])
decoder.transpconvs.2.bias shape torch.Size([128])
decoder.transpconvs.3.weight shape torch.Size([128, 64, 2, 2, 2])
decoder.transpconvs.3.bias shape torch.Size([64])
decoder.transpconvs.4.weight shape torch.Size([64, 32, 2, 2, 2])
decoder.transpconvs.4.bias shape torch.Size([32])
################### Done ###################
2024-12-17 00:59:45.305642: do_dummy_2d_data_aug: True
2024-12-17 00:59:45.379400: Using splits from existing split file: /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset307_Sohee_Calcium_OCT_CrossValidation/splits_final.json
2024-12-17 00:59:45.403998: The split file contains 5 splits.
2024-12-17 00:59:45.405759: Desired fold for training: 0
2024-12-17 00:59:45.406810: This split has 6 training and 2 validation cases.
using pin_memory on device 0
using pin_memory on device 0
using pin_memory on device 0

This is the configuration used by this training:
Configuration name: 3d_32x160x128_b10
 {'data_identifier': 'nnUNetPlans_3d_fullres', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 10, 'patch_size': [32, 160, 128], 'median_image_size_in_voxels': [375.0, 498.0, 498.0], 'spacing': [1.0, 1.0, 1.0], 'normalization_schemes': ['ZScoreNormalization'], 'use_mask_for_norm': [False], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'architecture': {'network_class_name': 'dynamic_network_architectures.architectures.unet.PlainConvUNet', 'arch_kwargs': {'n_stages': 6, 'features_per_stage': [32, 64, 128, 256, 320, 320], 'conv_op': 'torch.nn.modules.conv.Conv3d', 'kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]], 'strides': [[1, 1, 1], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [1, 2, 2]], 'n_conv_per_stage': [2, 2, 2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2, 2, 2], 'conv_bias': True, 'norm_op': 'torch.nn.modules.instancenorm.InstanceNorm3d', 'norm_op_kwargs': {'eps': 1e-05, 'affine': True}, 'dropout_op': None, 'dropout_op_kwargs': None, 'nonlin': 'torch.nn.LeakyReLU', 'nonlin_kwargs': {'inplace': True}, 'deep_supervision': True}, '_kw_requires_import': ['conv_op', 'norm_op', 'dropout_op', 'nonlin']}, 'batch_dice': True, 'inherits_from': '3d_fullres'} 

These are the global plan.json settings:
 {'dataset_name': 'Dataset307_Sohee_Calcium_OCT_CrossValidation', 'plans_name': 'nnUNetPlans', 'original_median_spacing_after_transp': [1.0, 1.0, 1.0], 'original_median_shape_after_transp': [375, 498, 498], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 0.4977976381778717, 'mean': 0.13507525622844696, 'median': 0.09599608182907104, 'min': 0.0, 'percentile_00_5': 0.014754901640117168, 'percentile_99_5': 0.4977976381778717, 'std': 0.12025152146816254}}} 

2024-12-17 00:59:53.113712: unpacking dataset...
using pin_memory on device 0

This is the configuration used by this training:
Configuration name: 3d_32x160x128_b10
 {'data_identifier': 'nnUNetPlans_3d_fullres', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 10, 'patch_size': [32, 160, 128], 'median_image_size_in_voxels': [375.0, 498.0, 498.0], 'spacing': [1.0, 1.0, 1.0], 'normalization_schemes': ['ZScoreNormalization'], 'use_mask_for_norm': [False], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'architecture': {'network_class_name': 'dynamic_network_architectures.architectures.unet.PlainConvUNet', 'arch_kwargs': {'n_stages': 6, 'features_per_stage': [32, 64, 128, 256, 320, 320], 'conv_op': 'torch.nn.modules.conv.Conv3d', 'kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]], 'strides': [[1, 1, 1], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [1, 2, 2]], 'n_conv_per_stage': [2, 2, 2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2, 2, 2], 'conv_bias': True, 'norm_op': 'torch.nn.modules.instancenorm.InstanceNorm3d', 'norm_op_kwargs': {'eps': 1e-05, 'affine': True}, 'dropout_op': None, 'dropout_op_kwargs': None, 'nonlin': 'torch.nn.LeakyReLU', 'nonlin_kwargs': {'inplace': True}, 'deep_supervision': True}, '_kw_requires_import': ['conv_op', 'norm_op', 'dropout_op', 'nonlin']}, 'batch_dice': True, 'inherits_from': '3d_fullres'} 

These are the global plan.json settings:
 {'dataset_name': 'Dataset307_Sohee_Calcium_OCT_CrossValidation', 'plans_name': 'nnUNetPlans', 'original_median_spacing_after_transp': [1.0, 1.0, 1.0], 'original_median_shape_after_transp': [375, 498, 498], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 0.4977976381778717, 'mean': 0.13507525622844696, 'median': 0.09599608182907104, 'min': 0.0, 'percentile_00_5': 0.014754901640117168, 'percentile_99_5': 0.4977976381778717, 'std': 0.12025152146816254}}} 

2024-12-17 00:59:54.368737: unpacking dataset...
2024-12-17 00:59:57.405172: unpacking done...
2024-12-17 00:59:57.830425: Unable to plot network architecture: nnUNet_compile is enabled!
2024-12-17 00:59:57.900316: 
2024-12-17 00:59:57.902079: Epoch 0
2024-12-17 00:59:57.903193: Current learning rate: 0.01
2024-12-17 01:02:37.137367: Validation loss improved from 1000.00000 to -0.16149! Patience: 0/50
2024-12-17 01:02:37.138424: train_loss -0.1087
2024-12-17 01:02:37.139635: val_loss -0.1615
2024-12-17 01:02:37.140505: Pseudo dice [0.502]
2024-12-17 01:02:37.141263: Epoch time: 159.24 s
2024-12-17 01:02:37.141991: Yayy! New best EMA pseudo Dice: 0.502
2024-12-17 01:02:39.400967: 
2024-12-17 01:02:39.402108: Epoch 1
2024-12-17 01:02:39.403236: Current learning rate: 0.00994
2024-12-17 01:04:08.308700: Validation loss improved from -0.16149 to -0.20500! Patience: 0/50
2024-12-17 01:04:08.309893: train_loss -0.2578
2024-12-17 01:04:08.310678: val_loss -0.205
2024-12-17 01:04:08.311342: Pseudo dice [0.5528]
2024-12-17 01:04:08.312301: Epoch time: 88.91 s
2024-12-17 01:04:08.313219: Yayy! New best EMA pseudo Dice: 0.507
2024-12-17 01:04:09.953440: 
2024-12-17 01:04:09.955207: Epoch 2
2024-12-17 01:04:09.955944: Current learning rate: 0.00988
2024-12-17 01:05:38.445037: Validation loss improved from -0.20500 to -0.20963! Patience: 0/50
2024-12-17 01:05:38.446052: train_loss -0.3123
2024-12-17 01:05:38.446944: val_loss -0.2096
2024-12-17 01:05:38.447762: Pseudo dice [0.5544]
2024-12-17 01:05:38.448589: Epoch time: 88.49 s
2024-12-17 01:05:38.449327: Yayy! New best EMA pseudo Dice: 0.5118
2024-12-17 01:05:40.079162: 
2024-12-17 01:05:40.080636: Epoch 3
2024-12-17 01:05:40.081649: Current learning rate: 0.00982
2024-12-17 01:07:08.486793: Validation loss improved from -0.20963 to -0.26299! Patience: 0/50
2024-12-17 01:07:08.488010: train_loss -0.3321
2024-12-17 01:07:08.488727: val_loss -0.263
2024-12-17 01:07:08.489413: Pseudo dice [0.5639]
2024-12-17 01:07:08.490109: Epoch time: 88.41 s
2024-12-17 01:07:08.491121: Yayy! New best EMA pseudo Dice: 0.517
2024-12-17 01:07:10.121704: 
2024-12-17 01:07:10.123349: Epoch 4
2024-12-17 01:07:10.124276: Current learning rate: 0.00976
2024-12-17 01:08:38.882686: Validation loss improved from -0.26299 to -0.30182! Patience: 0/50
2024-12-17 01:08:38.883916: train_loss -0.3657
2024-12-17 01:08:38.885502: val_loss -0.3018
2024-12-17 01:08:38.886386: Pseudo dice [0.6044]
2024-12-17 01:08:38.887149: Epoch time: 88.76 s
2024-12-17 01:08:39.266076: Yayy! New best EMA pseudo Dice: 0.5257
2024-12-17 01:08:40.939869: 
2024-12-17 01:08:40.942016: Epoch 5
2024-12-17 01:08:40.943165: Current learning rate: 0.0097
2024-12-17 01:10:09.580812: Validation loss did not improve from -0.30182. Patience: 1/50
2024-12-17 01:10:09.583305: train_loss -0.3894
2024-12-17 01:10:09.584401: val_loss -0.3018
2024-12-17 01:10:09.585269: Pseudo dice [0.5948]
2024-12-17 01:10:09.586123: Epoch time: 88.64 s
2024-12-17 01:10:09.586868: Yayy! New best EMA pseudo Dice: 0.5326
2024-12-17 01:10:11.276436: 
2024-12-17 01:10:11.278055: Epoch 6
2024-12-17 01:10:11.278951: Current learning rate: 0.00964
2024-12-17 01:11:39.832473: Validation loss improved from -0.30182 to -0.32246! Patience: 1/50
2024-12-17 01:11:39.833514: train_loss -0.4107
2024-12-17 01:11:39.835163: val_loss -0.3225
2024-12-17 01:11:39.836252: Pseudo dice [0.612]
2024-12-17 01:11:39.837249: Epoch time: 88.56 s
2024-12-17 01:11:39.838189: Yayy! New best EMA pseudo Dice: 0.5406
2024-12-17 01:11:41.452410: 
2024-12-17 01:11:41.454290: Epoch 7
2024-12-17 01:11:41.455435: Current learning rate: 0.00958
2024-12-17 01:13:10.046464: Validation loss did not improve from -0.32246. Patience: 1/50
2024-12-17 01:13:10.047795: train_loss -0.4136
2024-12-17 01:13:10.049459: val_loss -0.3117
2024-12-17 01:13:10.050680: Pseudo dice [0.6144]
2024-12-17 01:13:10.051728: Epoch time: 88.6 s
2024-12-17 01:13:10.052624: Yayy! New best EMA pseudo Dice: 0.548
2024-12-17 01:13:12.271188: 
2024-12-17 01:13:12.273504: Epoch 8
2024-12-17 01:13:12.274536: Current learning rate: 0.00952
2024-12-17 01:14:41.094313: Validation loss improved from -0.32246 to -0.41585! Patience: 1/50
2024-12-17 01:14:41.095333: train_loss -0.4393
2024-12-17 01:14:41.096191: val_loss -0.4159
2024-12-17 01:14:41.097018: Pseudo dice [0.6614]
2024-12-17 01:14:41.097843: Epoch time: 88.83 s
2024-12-17 01:14:41.098541: Yayy! New best EMA pseudo Dice: 0.5593
2024-12-17 01:14:42.816875: 
2024-12-17 01:14:42.818350: Epoch 9
2024-12-17 01:14:42.819223: Current learning rate: 0.00946
2024-12-17 01:16:11.677362: Validation loss did not improve from -0.41585. Patience: 1/50
2024-12-17 01:16:11.678448: train_loss -0.4433
2024-12-17 01:16:11.679625: val_loss -0.3817
2024-12-17 01:16:11.680506: Pseudo dice [0.644]
2024-12-17 01:16:11.681404: Epoch time: 88.86 s
2024-12-17 01:16:12.055535: Yayy! New best EMA pseudo Dice: 0.5678
2024-12-17 01:16:13.689296: 
2024-12-17 01:16:13.690610: Epoch 10
2024-12-17 01:16:13.691632: Current learning rate: 0.0094
2024-12-17 01:17:42.497272: Validation loss did not improve from -0.41585. Patience: 2/50
2024-12-17 01:17:42.498251: train_loss -0.4488
2024-12-17 01:17:42.499285: val_loss -0.4134
2024-12-17 01:17:42.500190: Pseudo dice [0.6488]
2024-12-17 01:17:42.501066: Epoch time: 88.81 s
2024-12-17 01:17:42.502067: Yayy! New best EMA pseudo Dice: 0.5759
2024-12-17 01:17:44.135455: 
2024-12-17 01:17:44.136987: Epoch 11
2024-12-17 01:17:44.137960: Current learning rate: 0.00934
2024-12-17 01:19:12.889217: Validation loss improved from -0.41585 to -0.42783! Patience: 2/50
2024-12-17 01:19:12.890284: train_loss -0.4691
2024-12-17 01:19:12.891215: val_loss -0.4278
2024-12-17 01:19:12.891893: Pseudo dice [0.6759]
2024-12-17 01:19:12.892787: Epoch time: 88.76 s
2024-12-17 01:19:12.893623: Yayy! New best EMA pseudo Dice: 0.5859
2024-12-17 01:19:14.536625: 
2024-12-17 01:19:14.538126: Epoch 12
2024-12-17 01:19:14.538848: Current learning rate: 0.00928
2024-12-17 01:20:43.404537: Validation loss improved from -0.42783 to -0.45086! Patience: 0/50
2024-12-17 01:20:43.405615: train_loss -0.4759
2024-12-17 01:20:43.406607: val_loss -0.4509
2024-12-17 01:20:43.407435: Pseudo dice [0.6865]
2024-12-17 01:20:43.408225: Epoch time: 88.87 s
2024-12-17 01:20:43.408994: Yayy! New best EMA pseudo Dice: 0.5959
2024-12-17 01:20:45.116600: 
2024-12-17 01:20:45.117758: Epoch 13
2024-12-17 01:20:45.118543: Current learning rate: 0.00922
2024-12-17 01:22:13.989354: Validation loss improved from -0.45086 to -0.47647! Patience: 0/50
2024-12-17 01:22:13.990269: train_loss -0.4953
2024-12-17 01:22:13.991106: val_loss -0.4765
2024-12-17 01:22:13.991831: Pseudo dice [0.6899]
2024-12-17 01:22:13.992620: Epoch time: 88.87 s
2024-12-17 01:22:13.993254: Yayy! New best EMA pseudo Dice: 0.6053
2024-12-17 01:22:15.705758: 
2024-12-17 01:22:15.707238: Epoch 14
2024-12-17 01:22:15.708327: Current learning rate: 0.00916
2024-12-17 01:23:44.529602: Validation loss did not improve from -0.47647. Patience: 1/50
2024-12-17 01:23:44.530694: train_loss -0.507
2024-12-17 01:23:44.531758: val_loss -0.4458
2024-12-17 01:23:44.532599: Pseudo dice [0.6884]
2024-12-17 01:23:44.533460: Epoch time: 88.83 s
2024-12-17 01:23:44.912288: Yayy! New best EMA pseudo Dice: 0.6136
2024-12-17 01:23:46.525420: 
2024-12-17 01:23:46.526747: Epoch 15
2024-12-17 01:23:46.527749: Current learning rate: 0.0091
2024-12-17 01:25:15.333817: Validation loss did not improve from -0.47647. Patience: 2/50
2024-12-17 01:25:15.334867: train_loss -0.5109
2024-12-17 01:25:15.335695: val_loss -0.4344
2024-12-17 01:25:15.336486: Pseudo dice [0.6687]
2024-12-17 01:25:15.337301: Epoch time: 88.81 s
2024-12-17 01:25:15.337934: Yayy! New best EMA pseudo Dice: 0.6191
2024-12-17 01:25:16.992971: 
2024-12-17 01:25:16.994575: Epoch 16
2024-12-17 01:25:16.995465: Current learning rate: 0.00903
2024-12-17 01:26:46.116210: Validation loss improved from -0.47647 to -0.49010! Patience: 2/50
2024-12-17 01:26:46.117297: train_loss -0.5236
2024-12-17 01:26:46.118255: val_loss -0.4901
2024-12-17 01:26:46.119015: Pseudo dice [0.7104]
2024-12-17 01:26:46.119778: Epoch time: 89.13 s
2024-12-17 01:26:46.120550: Yayy! New best EMA pseudo Dice: 0.6283
2024-12-17 01:26:47.771857: 
2024-12-17 01:26:47.773571: Epoch 17
2024-12-17 01:26:47.774451: Current learning rate: 0.00897
2024-12-17 01:28:16.770373: Validation loss did not improve from -0.49010. Patience: 1/50
2024-12-17 01:28:16.771540: train_loss -0.5178
2024-12-17 01:28:16.772604: val_loss -0.4355
2024-12-17 01:28:16.773285: Pseudo dice [0.6674]
2024-12-17 01:28:16.774007: Epoch time: 89.0 s
2024-12-17 01:28:16.774842: Yayy! New best EMA pseudo Dice: 0.6322
2024-12-17 01:28:18.787356: 
2024-12-17 01:28:18.788648: Epoch 18
2024-12-17 01:28:18.789423: Current learning rate: 0.00891
2024-12-17 01:29:47.559014: Validation loss did not improve from -0.49010. Patience: 2/50
2024-12-17 01:29:47.560134: train_loss -0.5222
2024-12-17 01:29:47.561037: val_loss -0.4612
2024-12-17 01:29:47.561883: Pseudo dice [0.6867]
2024-12-17 01:29:47.562684: Epoch time: 88.77 s
2024-12-17 01:29:47.563669: Yayy! New best EMA pseudo Dice: 0.6376
2024-12-17 01:29:49.165632: 
2024-12-17 01:29:49.167552: Epoch 19
2024-12-17 01:29:49.168385: Current learning rate: 0.00885
2024-12-17 01:31:17.939741: Validation loss did not improve from -0.49010. Patience: 3/50
2024-12-17 01:31:17.940780: train_loss -0.5367
2024-12-17 01:31:17.941687: val_loss -0.4716
2024-12-17 01:31:17.942472: Pseudo dice [0.6979]
2024-12-17 01:31:17.943222: Epoch time: 88.78 s
2024-12-17 01:31:18.327229: Yayy! New best EMA pseudo Dice: 0.6437
2024-12-17 01:31:19.917549: 
2024-12-17 01:31:19.919018: Epoch 20
2024-12-17 01:31:19.920011: Current learning rate: 0.00879
2024-12-17 01:32:48.596905: Validation loss did not improve from -0.49010. Patience: 4/50
2024-12-17 01:32:48.598293: train_loss -0.5484
2024-12-17 01:32:48.599493: val_loss -0.458
2024-12-17 01:32:48.600230: Pseudo dice [0.6864]
2024-12-17 01:32:48.601055: Epoch time: 88.68 s
2024-12-17 01:32:48.601764: Yayy! New best EMA pseudo Dice: 0.6479
2024-12-17 01:32:50.223524: 
2024-12-17 01:32:50.224704: Epoch 21
2024-12-17 01:32:50.225559: Current learning rate: 0.00873
2024-12-17 01:34:18.961505: Validation loss did not improve from -0.49010. Patience: 5/50
2024-12-17 01:34:18.962665: train_loss -0.5467
2024-12-17 01:34:18.963515: val_loss -0.4405
2024-12-17 01:34:18.964277: Pseudo dice [0.6743]
2024-12-17 01:34:18.965061: Epoch time: 88.74 s
2024-12-17 01:34:18.965823: Yayy! New best EMA pseudo Dice: 0.6506
2024-12-17 01:34:20.492748: 
2024-12-17 01:34:20.494282: Epoch 22
2024-12-17 01:34:20.495085: Current learning rate: 0.00867
2024-12-17 01:35:49.396979: Validation loss did not improve from -0.49010. Patience: 6/50
2024-12-17 01:35:49.397863: train_loss -0.5489
2024-12-17 01:35:49.398932: val_loss -0.4768
2024-12-17 01:35:49.399768: Pseudo dice [0.7026]
2024-12-17 01:35:49.400701: Epoch time: 88.91 s
2024-12-17 01:35:49.401489: Yayy! New best EMA pseudo Dice: 0.6558
2024-12-17 01:35:50.946647: 
2024-12-17 01:35:50.948277: Epoch 23
2024-12-17 01:35:50.948966: Current learning rate: 0.00861
2024-12-17 01:37:19.775442: Validation loss did not improve from -0.49010. Patience: 7/50
2024-12-17 01:37:19.776177: train_loss -0.5575
2024-12-17 01:37:19.777029: val_loss -0.4699
2024-12-17 01:37:19.777855: Pseudo dice [0.6991]
2024-12-17 01:37:19.778749: Epoch time: 88.83 s
2024-12-17 01:37:19.779536: Yayy! New best EMA pseudo Dice: 0.6601
2024-12-17 01:37:21.286645: 
2024-12-17 01:37:21.288506: Epoch 24
2024-12-17 01:37:21.289525: Current learning rate: 0.00855
2024-12-17 01:38:50.082687: Validation loss did not improve from -0.49010. Patience: 8/50
2024-12-17 01:38:50.083909: train_loss -0.5599
2024-12-17 01:38:50.084856: val_loss -0.427
2024-12-17 01:38:50.085675: Pseudo dice [0.6655]
2024-12-17 01:38:50.086601: Epoch time: 88.8 s
2024-12-17 01:38:50.464520: Yayy! New best EMA pseudo Dice: 0.6606
2024-12-17 01:38:51.993166: 
2024-12-17 01:38:51.994996: Epoch 25
2024-12-17 01:38:51.995939: Current learning rate: 0.00849
2024-12-17 01:40:20.839205: Validation loss did not improve from -0.49010. Patience: 9/50
2024-12-17 01:40:20.839898: train_loss -0.5615
2024-12-17 01:40:20.840759: val_loss -0.468
2024-12-17 01:40:20.841541: Pseudo dice [0.7036]
2024-12-17 01:40:20.842180: Epoch time: 88.85 s
2024-12-17 01:40:20.842814: Yayy! New best EMA pseudo Dice: 0.6649
2024-12-17 01:40:22.403276: 
2024-12-17 01:40:22.405219: Epoch 26
2024-12-17 01:40:22.405996: Current learning rate: 0.00843
2024-12-17 01:41:51.354969: Validation loss improved from -0.49010 to -0.49688! Patience: 9/50
2024-12-17 01:41:51.355876: train_loss -0.5734
2024-12-17 01:41:51.356748: val_loss -0.4969
2024-12-17 01:41:51.357544: Pseudo dice [0.7145]
2024-12-17 01:41:51.358332: Epoch time: 88.95 s
2024-12-17 01:41:51.359053: Yayy! New best EMA pseudo Dice: 0.6699
2024-12-17 01:41:52.902344: 
2024-12-17 01:41:52.903964: Epoch 27
2024-12-17 01:41:52.904792: Current learning rate: 0.00836
2024-12-17 01:43:21.901764: Validation loss did not improve from -0.49688. Patience: 1/50
2024-12-17 01:43:21.902982: train_loss -0.5746
2024-12-17 01:43:21.904143: val_loss -0.4696
2024-12-17 01:43:21.904940: Pseudo dice [0.7025]
2024-12-17 01:43:21.905771: Epoch time: 89.0 s
2024-12-17 01:43:21.906449: Yayy! New best EMA pseudo Dice: 0.6732
2024-12-17 01:43:23.480284: 
2024-12-17 01:43:23.481485: Epoch 28
2024-12-17 01:43:23.482385: Current learning rate: 0.0083
2024-12-17 01:44:52.496330: Validation loss did not improve from -0.49688. Patience: 2/50
2024-12-17 01:44:52.497362: train_loss -0.585
2024-12-17 01:44:52.498425: val_loss -0.4876
2024-12-17 01:44:52.499175: Pseudo dice [0.7033]
2024-12-17 01:44:52.499867: Epoch time: 89.02 s
2024-12-17 01:44:52.500517: Yayy! New best EMA pseudo Dice: 0.6762
2024-12-17 01:44:54.456856: 
2024-12-17 01:44:54.458555: Epoch 29
2024-12-17 01:44:54.459431: Current learning rate: 0.00824
2024-12-17 01:46:23.412267: Validation loss did not improve from -0.49688. Patience: 3/50
2024-12-17 01:46:23.413509: train_loss -0.5886
2024-12-17 01:46:23.414564: val_loss -0.4603
2024-12-17 01:46:23.415562: Pseudo dice [0.6897]
2024-12-17 01:46:23.416394: Epoch time: 88.96 s
2024-12-17 01:46:23.812918: Yayy! New best EMA pseudo Dice: 0.6775
2024-12-17 01:46:25.424501: 
2024-12-17 01:46:25.426485: Epoch 30
2024-12-17 01:46:25.427343: Current learning rate: 0.00818
2024-12-17 01:47:54.415084: Validation loss improved from -0.49688 to -0.51751! Patience: 3/50
2024-12-17 01:47:54.415814: train_loss -0.5858
2024-12-17 01:47:54.416738: val_loss -0.5175
2024-12-17 01:47:54.417551: Pseudo dice [0.7205]
2024-12-17 01:47:54.418200: Epoch time: 88.99 s
2024-12-17 01:47:54.418974: Yayy! New best EMA pseudo Dice: 0.6818
2024-12-17 01:47:56.096913: 
2024-12-17 01:47:56.098265: Epoch 31
2024-12-17 01:47:56.099113: Current learning rate: 0.00812
2024-12-17 01:49:25.161383: Validation loss did not improve from -0.51751. Patience: 1/50
2024-12-17 01:49:25.162575: train_loss -0.5793
2024-12-17 01:49:25.163802: val_loss -0.4892
2024-12-17 01:49:25.164677: Pseudo dice [0.7102]
2024-12-17 01:49:25.165558: Epoch time: 89.07 s
2024-12-17 01:49:25.166466: Yayy! New best EMA pseudo Dice: 0.6847
2024-12-17 01:49:26.770908: 
2024-12-17 01:49:26.772039: Epoch 32
2024-12-17 01:49:26.772875: Current learning rate: 0.00806
2024-12-17 01:50:55.788241: Validation loss improved from -0.51751 to -0.53434! Patience: 1/50
2024-12-17 01:50:55.789428: train_loss -0.6005
2024-12-17 01:50:55.790215: val_loss -0.5343
2024-12-17 01:50:55.790987: Pseudo dice [0.7304]
2024-12-17 01:50:55.791589: Epoch time: 89.02 s
2024-12-17 01:50:55.792362: Yayy! New best EMA pseudo Dice: 0.6892
2024-12-17 01:50:57.376279: 
2024-12-17 01:50:57.378141: Epoch 33
2024-12-17 01:50:57.379024: Current learning rate: 0.008
2024-12-17 01:52:26.254918: Validation loss did not improve from -0.53434. Patience: 1/50
2024-12-17 01:52:26.255959: train_loss -0.6018
2024-12-17 01:52:26.257061: val_loss -0.514
2024-12-17 01:52:26.257816: Pseudo dice [0.7236]
2024-12-17 01:52:26.258674: Epoch time: 88.88 s
2024-12-17 01:52:26.259522: Yayy! New best EMA pseudo Dice: 0.6927
2024-12-17 01:52:27.837730: 
2024-12-17 01:52:27.839346: Epoch 34
2024-12-17 01:52:27.840192: Current learning rate: 0.00793
2024-12-17 01:53:56.604038: Validation loss did not improve from -0.53434. Patience: 2/50
2024-12-17 01:53:56.605459: train_loss -0.5949
2024-12-17 01:53:56.606262: val_loss -0.4986
2024-12-17 01:53:56.607041: Pseudo dice [0.7219]
2024-12-17 01:53:56.607801: Epoch time: 88.77 s
2024-12-17 01:53:56.978916: Yayy! New best EMA pseudo Dice: 0.6956
2024-12-17 01:53:58.568658: 
2024-12-17 01:53:58.570030: Epoch 35
2024-12-17 01:53:58.570853: Current learning rate: 0.00787
2024-12-17 01:55:27.259283: Validation loss did not improve from -0.53434. Patience: 3/50
2024-12-17 01:55:27.260317: train_loss -0.6023
2024-12-17 01:55:27.261275: val_loss -0.5229
2024-12-17 01:55:27.262103: Pseudo dice [0.7291]
2024-12-17 01:55:27.262910: Epoch time: 88.69 s
2024-12-17 01:55:27.263682: Yayy! New best EMA pseudo Dice: 0.6989
2024-12-17 01:55:28.836750: 
2024-12-17 01:55:28.838805: Epoch 36
2024-12-17 01:55:28.839759: Current learning rate: 0.00781
2024-12-17 01:56:57.589674: Validation loss did not improve from -0.53434. Patience: 4/50
2024-12-17 01:56:57.591017: train_loss -0.6125
2024-12-17 01:56:57.591987: val_loss -0.5036
2024-12-17 01:56:57.592714: Pseudo dice [0.7189]
2024-12-17 01:56:57.593383: Epoch time: 88.76 s
2024-12-17 01:56:57.594050: Yayy! New best EMA pseudo Dice: 0.7009
2024-12-17 01:56:59.162219: 
2024-12-17 01:56:59.163598: Epoch 37
2024-12-17 01:56:59.164384: Current learning rate: 0.00775
2024-12-17 01:58:27.937114: Validation loss did not improve from -0.53434. Patience: 5/50
2024-12-17 01:58:27.938137: train_loss -0.6092
2024-12-17 01:58:27.939295: val_loss -0.4897
2024-12-17 01:58:27.940088: Pseudo dice [0.7098]
2024-12-17 01:58:27.940766: Epoch time: 88.78 s
2024-12-17 01:58:27.941449: Yayy! New best EMA pseudo Dice: 0.7018
2024-12-17 01:58:29.531443: 
2024-12-17 01:58:29.533227: Epoch 38
2024-12-17 01:58:29.533997: Current learning rate: 0.00769
2024-12-17 01:59:58.290500: Validation loss improved from -0.53434 to -0.53885! Patience: 5/50
2024-12-17 01:59:58.291708: train_loss -0.6194
2024-12-17 01:59:58.292854: val_loss -0.5388
2024-12-17 01:59:58.293835: Pseudo dice [0.7413]
2024-12-17 01:59:58.294543: Epoch time: 88.76 s
2024-12-17 01:59:58.295350: Yayy! New best EMA pseudo Dice: 0.7058
2024-12-17 01:59:59.912344: 
2024-12-17 01:59:59.914156: Epoch 39
2024-12-17 01:59:59.914917: Current learning rate: 0.00763
2024-12-17 02:01:28.889361: Validation loss did not improve from -0.53885. Patience: 1/50
2024-12-17 02:01:28.890265: train_loss -0.627
2024-12-17 02:01:28.891160: val_loss -0.4847
2024-12-17 02:01:28.891865: Pseudo dice [0.7005]
2024-12-17 02:01:28.892546: Epoch time: 88.98 s
2024-12-17 02:01:30.542762: 
2024-12-17 02:01:30.544592: Epoch 40
2024-12-17 02:01:30.545384: Current learning rate: 0.00756
2024-12-17 02:02:59.309729: Validation loss did not improve from -0.53885. Patience: 2/50
2024-12-17 02:02:59.311614: train_loss -0.6226
2024-12-17 02:02:59.313257: val_loss -0.5154
2024-12-17 02:02:59.313925: Pseudo dice [0.7218]
2024-12-17 02:02:59.314610: Epoch time: 88.77 s
2024-12-17 02:02:59.315254: Yayy! New best EMA pseudo Dice: 0.7069
2024-12-17 02:03:00.989968: 
2024-12-17 02:03:00.991344: Epoch 41
2024-12-17 02:03:00.992198: Current learning rate: 0.0075
2024-12-17 02:04:29.900871: Validation loss did not improve from -0.53885. Patience: 3/50
2024-12-17 02:04:29.901627: train_loss -0.6229
2024-12-17 02:04:29.902852: val_loss -0.511
2024-12-17 02:04:29.903703: Pseudo dice [0.7235]
2024-12-17 02:04:29.904574: Epoch time: 88.91 s
2024-12-17 02:04:29.905352: Yayy! New best EMA pseudo Dice: 0.7086
2024-12-17 02:04:31.427160: 
2024-12-17 02:04:31.428936: Epoch 42
2024-12-17 02:04:31.429835: Current learning rate: 0.00744
2024-12-17 02:06:00.792193: Validation loss did not improve from -0.53885. Patience: 4/50
2024-12-17 02:06:00.807688: train_loss -0.6339
2024-12-17 02:06:00.854922: val_loss -0.494
2024-12-17 02:06:00.855838: Pseudo dice [0.711]
2024-12-17 02:06:00.883178: Epoch time: 89.38 s
2024-12-17 02:06:00.884602: Yayy! New best EMA pseudo Dice: 0.7088
2024-12-17 02:06:02.675348: 
2024-12-17 02:06:02.676739: Epoch 43
2024-12-17 02:06:02.677445: Current learning rate: 0.00738
2024-12-17 02:07:31.658408: Validation loss did not improve from -0.53885. Patience: 5/50
2024-12-17 02:07:31.659240: train_loss -0.6365
2024-12-17 02:07:31.660127: val_loss -0.5237
2024-12-17 02:07:31.661057: Pseudo dice [0.7358]
2024-12-17 02:07:31.661874: Epoch time: 88.98 s
2024-12-17 02:07:31.662637: Yayy! New best EMA pseudo Dice: 0.7115
2024-12-17 02:07:33.225453: 
2024-12-17 02:07:33.227271: Epoch 44
2024-12-17 02:07:33.228125: Current learning rate: 0.00732
2024-12-17 02:09:02.206794: Validation loss did not improve from -0.53885. Patience: 6/50
2024-12-17 02:09:02.207916: train_loss -0.6396
2024-12-17 02:09:02.208863: val_loss -0.536
2024-12-17 02:09:02.209893: Pseudo dice [0.7392]
2024-12-17 02:09:02.210648: Epoch time: 88.98 s
2024-12-17 02:09:02.586657: Yayy! New best EMA pseudo Dice: 0.7143
2024-12-17 02:09:04.099848: 
2024-12-17 02:09:04.101777: Epoch 45
2024-12-17 02:09:04.102500: Current learning rate: 0.00725
2024-12-17 02:10:33.078577: Validation loss did not improve from -0.53885. Patience: 7/50
2024-12-17 02:10:33.079859: train_loss -0.6294
2024-12-17 02:10:33.080734: val_loss -0.5093
2024-12-17 02:10:33.081469: Pseudo dice [0.7147]
2024-12-17 02:10:33.082129: Epoch time: 88.98 s
2024-12-17 02:10:33.082718: Yayy! New best EMA pseudo Dice: 0.7143
2024-12-17 02:10:34.623035: 
2024-12-17 02:10:34.624694: Epoch 46
2024-12-17 02:10:34.625445: Current learning rate: 0.00719
2024-12-17 02:12:03.677246: Validation loss did not improve from -0.53885. Patience: 8/50
2024-12-17 02:12:03.678220: train_loss -0.6334
2024-12-17 02:12:03.678995: val_loss -0.5266
2024-12-17 02:12:03.679742: Pseudo dice [0.7348]
2024-12-17 02:12:03.680485: Epoch time: 89.06 s
2024-12-17 02:12:03.681122: Yayy! New best EMA pseudo Dice: 0.7164
2024-12-17 02:12:05.213611: 
2024-12-17 02:12:05.215342: Epoch 47
2024-12-17 02:12:05.216127: Current learning rate: 0.00713
2024-12-17 02:13:34.268638: Validation loss did not improve from -0.53885. Patience: 9/50
2024-12-17 02:13:34.269758: train_loss -0.6411
2024-12-17 02:13:34.270599: val_loss -0.5066
2024-12-17 02:13:34.271441: Pseudo dice [0.7168]
2024-12-17 02:13:34.272258: Epoch time: 89.06 s
2024-12-17 02:13:34.272989: Yayy! New best EMA pseudo Dice: 0.7164
2024-12-17 02:13:35.824340: 
2024-12-17 02:13:35.825709: Epoch 48
2024-12-17 02:13:35.826524: Current learning rate: 0.00707
2024-12-17 02:15:04.842062: Validation loss did not improve from -0.53885. Patience: 10/50
2024-12-17 02:15:04.843208: train_loss -0.6439
2024-12-17 02:15:04.844161: val_loss -0.5234
2024-12-17 02:15:04.845072: Pseudo dice [0.7251]
2024-12-17 02:15:04.845837: Epoch time: 89.02 s
2024-12-17 02:15:04.846671: Yayy! New best EMA pseudo Dice: 0.7173
2024-12-17 02:15:06.397179: 
2024-12-17 02:15:06.398968: Epoch 49
2024-12-17 02:15:06.399799: Current learning rate: 0.007
2024-12-17 02:16:35.497655: Validation loss did not improve from -0.53885. Patience: 11/50
2024-12-17 02:16:35.498620: train_loss -0.6437
2024-12-17 02:16:35.499492: val_loss -0.5102
2024-12-17 02:16:35.500266: Pseudo dice [0.719]
2024-12-17 02:16:35.500934: Epoch time: 89.1 s
2024-12-17 02:16:35.869196: Yayy! New best EMA pseudo Dice: 0.7175
2024-12-17 02:16:37.990490: 
2024-12-17 02:16:37.991936: Epoch 50
2024-12-17 02:16:37.992842: Current learning rate: 0.00694
2024-12-17 02:18:07.291993: Validation loss improved from -0.53885 to -0.53890! Patience: 11/50
2024-12-17 02:18:07.292990: train_loss -0.6586
2024-12-17 02:18:07.293921: val_loss -0.5389
2024-12-17 02:18:07.294630: Pseudo dice [0.7412]
2024-12-17 02:18:07.295260: Epoch time: 89.3 s
2024-12-17 02:18:07.295880: Yayy! New best EMA pseudo Dice: 0.7198
2024-12-17 02:18:08.871487: 
2024-12-17 02:18:08.872700: Epoch 51
2024-12-17 02:18:08.873396: Current learning rate: 0.00688
2024-12-17 02:19:38.086375: Validation loss did not improve from -0.53890. Patience: 1/50
2024-12-17 02:19:38.087182: train_loss -0.6481
2024-12-17 02:19:38.088002: val_loss -0.526
2024-12-17 02:19:38.088710: Pseudo dice [0.7268]
2024-12-17 02:19:38.089511: Epoch time: 89.22 s
2024-12-17 02:19:38.090511: Yayy! New best EMA pseudo Dice: 0.7205
2024-12-17 02:19:39.667814: 
2024-12-17 02:19:39.669491: Epoch 52
2024-12-17 02:19:39.670267: Current learning rate: 0.00682
2024-12-17 02:21:08.786418: Validation loss did not improve from -0.53890. Patience: 2/50
2024-12-17 02:21:08.787274: train_loss -0.6511
2024-12-17 02:21:08.788127: val_loss -0.5312
2024-12-17 02:21:08.788969: Pseudo dice [0.7362]
2024-12-17 02:21:08.789678: Epoch time: 89.12 s
2024-12-17 02:21:08.790487: Yayy! New best EMA pseudo Dice: 0.7221
2024-12-17 02:21:10.385042: 
2024-12-17 02:21:10.386489: Epoch 53
2024-12-17 02:21:10.387524: Current learning rate: 0.00675
2024-12-17 02:22:39.406247: Validation loss did not improve from -0.53890. Patience: 3/50
2024-12-17 02:22:39.407267: train_loss -0.6535
2024-12-17 02:22:39.408133: val_loss -0.5389
2024-12-17 02:22:39.408828: Pseudo dice [0.7402]
2024-12-17 02:22:39.409631: Epoch time: 89.02 s
2024-12-17 02:22:39.410232: Yayy! New best EMA pseudo Dice: 0.7239
2024-12-17 02:22:40.957483: 
2024-12-17 02:22:40.959430: Epoch 54
2024-12-17 02:22:40.960128: Current learning rate: 0.00669
2024-12-17 02:24:09.936513: Validation loss did not improve from -0.53890. Patience: 4/50
2024-12-17 02:24:09.937670: train_loss -0.6592
2024-12-17 02:24:09.939112: val_loss -0.5267
2024-12-17 02:24:09.940173: Pseudo dice [0.728]
2024-12-17 02:24:09.941111: Epoch time: 88.98 s
2024-12-17 02:24:10.315130: Yayy! New best EMA pseudo Dice: 0.7243
2024-12-17 02:24:11.872582: 
2024-12-17 02:24:11.874358: Epoch 55
2024-12-17 02:24:11.875285: Current learning rate: 0.00663
2024-12-17 02:25:40.848434: Validation loss improved from -0.53890 to -0.55502! Patience: 4/50
2024-12-17 02:25:40.849630: train_loss -0.6614
2024-12-17 02:25:40.850552: val_loss -0.555
2024-12-17 02:25:40.851321: Pseudo dice [0.7498]
2024-12-17 02:25:40.852332: Epoch time: 88.98 s
2024-12-17 02:25:40.853244: Yayy! New best EMA pseudo Dice: 0.7269
2024-12-17 02:25:42.434366: 
2024-12-17 02:25:42.435458: Epoch 56
2024-12-17 02:25:42.436429: Current learning rate: 0.00657
2024-12-17 02:27:11.304011: Validation loss did not improve from -0.55502. Patience: 1/50
2024-12-17 02:27:11.304943: train_loss -0.6568
2024-12-17 02:27:11.305822: val_loss -0.5328
2024-12-17 02:27:11.306522: Pseudo dice [0.7357]
2024-12-17 02:27:11.307269: Epoch time: 88.87 s
2024-12-17 02:27:11.307934: Yayy! New best EMA pseudo Dice: 0.7277
2024-12-17 02:27:12.850355: 
2024-12-17 02:27:12.851916: Epoch 57
2024-12-17 02:27:12.852706: Current learning rate: 0.0065
2024-12-17 02:28:41.554818: Validation loss did not improve from -0.55502. Patience: 2/50
2024-12-17 02:28:41.556145: train_loss -0.6608
2024-12-17 02:28:41.556986: val_loss -0.5127
2024-12-17 02:28:41.557910: Pseudo dice [0.7323]
2024-12-17 02:28:41.558743: Epoch time: 88.71 s
2024-12-17 02:28:41.559444: Yayy! New best EMA pseudo Dice: 0.7282
2024-12-17 02:28:43.113283: 
2024-12-17 02:28:43.114742: Epoch 58
2024-12-17 02:28:43.115481: Current learning rate: 0.00644
2024-12-17 02:30:11.845154: Validation loss did not improve from -0.55502. Patience: 3/50
2024-12-17 02:30:11.846304: train_loss -0.6607
2024-12-17 02:30:11.847291: val_loss -0.5375
2024-12-17 02:30:11.848188: Pseudo dice [0.7383]
2024-12-17 02:30:11.848986: Epoch time: 88.73 s
2024-12-17 02:30:11.849824: Yayy! New best EMA pseudo Dice: 0.7292
2024-12-17 02:30:13.424169: 
2024-12-17 02:30:13.425882: Epoch 59
2024-12-17 02:30:13.426763: Current learning rate: 0.00638
2024-12-17 02:31:42.100137: Validation loss did not improve from -0.55502. Patience: 4/50
2024-12-17 02:31:42.101211: train_loss -0.673
2024-12-17 02:31:42.102294: val_loss -0.513
2024-12-17 02:31:42.103424: Pseudo dice [0.7187]
2024-12-17 02:31:42.104369: Epoch time: 88.68 s
2024-12-17 02:31:43.672189: 
2024-12-17 02:31:43.673922: Epoch 60
2024-12-17 02:31:43.674936: Current learning rate: 0.00631
2024-12-17 02:33:12.292442: Validation loss did not improve from -0.55502. Patience: 5/50
2024-12-17 02:33:12.293480: train_loss -0.6677
2024-12-17 02:33:12.294375: val_loss -0.5118
2024-12-17 02:33:12.295112: Pseudo dice [0.7205]
2024-12-17 02:33:12.295899: Epoch time: 88.62 s
2024-12-17 02:33:13.939161: 
2024-12-17 02:33:13.940874: Epoch 61
2024-12-17 02:33:13.941694: Current learning rate: 0.00625
2024-12-17 02:34:42.641697: Validation loss did not improve from -0.55502. Patience: 6/50
2024-12-17 02:34:42.642857: train_loss -0.665
2024-12-17 02:34:42.643995: val_loss -0.532
2024-12-17 02:34:42.644803: Pseudo dice [0.7325]
2024-12-17 02:34:42.645599: Epoch time: 88.7 s
2024-12-17 02:34:43.849711: 
2024-12-17 02:34:43.850943: Epoch 62
2024-12-17 02:34:43.851892: Current learning rate: 0.00619
2024-12-17 02:36:12.552228: Validation loss did not improve from -0.55502. Patience: 7/50
2024-12-17 02:36:12.553249: train_loss -0.6704
2024-12-17 02:36:12.554292: val_loss -0.5206
2024-12-17 02:36:12.554929: Pseudo dice [0.7346]
2024-12-17 02:36:12.555645: Epoch time: 88.7 s
2024-12-17 02:36:13.816129: 
2024-12-17 02:36:13.818056: Epoch 63
2024-12-17 02:36:13.818938: Current learning rate: 0.00612
2024-12-17 02:37:42.457052: Validation loss did not improve from -0.55502. Patience: 8/50
2024-12-17 02:37:42.457771: train_loss -0.6646
2024-12-17 02:37:42.458843: val_loss -0.5533
2024-12-17 02:37:42.459646: Pseudo dice [0.7544]
2024-12-17 02:37:42.460528: Epoch time: 88.64 s
2024-12-17 02:37:42.461369: Yayy! New best EMA pseudo Dice: 0.7311
2024-12-17 02:37:44.065019: 
2024-12-17 02:37:44.066391: Epoch 64
2024-12-17 02:37:44.067225: Current learning rate: 0.00606
2024-12-17 02:39:12.946076: Validation loss did not improve from -0.55502. Patience: 9/50
2024-12-17 02:39:12.947547: train_loss -0.6717
2024-12-17 02:39:12.948717: val_loss -0.5503
2024-12-17 02:39:12.949457: Pseudo dice [0.7417]
2024-12-17 02:39:12.950128: Epoch time: 88.88 s
2024-12-17 02:39:13.324223: Yayy! New best EMA pseudo Dice: 0.7322
2024-12-17 02:39:14.895059: 
2024-12-17 02:39:14.896364: Epoch 65
2024-12-17 02:39:14.897167: Current learning rate: 0.006
2024-12-17 02:40:43.884252: Validation loss did not improve from -0.55502. Patience: 10/50
2024-12-17 02:40:43.885508: train_loss -0.6785
2024-12-17 02:40:43.886396: val_loss -0.5297
2024-12-17 02:40:43.887228: Pseudo dice [0.7287]
2024-12-17 02:40:43.888043: Epoch time: 88.99 s
2024-12-17 02:40:45.125339: 
2024-12-17 02:40:45.127158: Epoch 66
2024-12-17 02:40:45.128074: Current learning rate: 0.00593
2024-12-17 02:42:14.094202: Validation loss did not improve from -0.55502. Patience: 11/50
2024-12-17 02:42:14.095371: train_loss -0.6744
2024-12-17 02:42:14.096333: val_loss -0.5442
2024-12-17 02:42:14.097412: Pseudo dice [0.747]
2024-12-17 02:42:14.098381: Epoch time: 88.97 s
2024-12-17 02:42:14.099342: Yayy! New best EMA pseudo Dice: 0.7334
2024-12-17 02:42:15.703532: 
2024-12-17 02:42:15.705568: Epoch 67
2024-12-17 02:42:15.706506: Current learning rate: 0.00587
2024-12-17 02:43:44.627914: Validation loss did not improve from -0.55502. Patience: 12/50
2024-12-17 02:43:44.629274: train_loss -0.686
2024-12-17 02:43:44.630233: val_loss -0.5274
2024-12-17 02:43:44.631193: Pseudo dice [0.7339]
2024-12-17 02:43:44.632148: Epoch time: 88.93 s
2024-12-17 02:43:44.632968: Yayy! New best EMA pseudo Dice: 0.7334
2024-12-17 02:43:46.216862: 
2024-12-17 02:43:46.218496: Epoch 68
2024-12-17 02:43:46.219433: Current learning rate: 0.00581
2024-12-17 02:45:15.220826: Validation loss did not improve from -0.55502. Patience: 13/50
2024-12-17 02:45:15.221956: train_loss -0.6824
2024-12-17 02:45:15.223096: val_loss -0.5296
2024-12-17 02:45:15.224034: Pseudo dice [0.7324]
2024-12-17 02:45:15.224809: Epoch time: 89.01 s
2024-12-17 02:45:16.458100: 
2024-12-17 02:45:16.459855: Epoch 69
2024-12-17 02:45:16.460643: Current learning rate: 0.00574
2024-12-17 02:46:45.413316: Validation loss did not improve from -0.55502. Patience: 14/50
2024-12-17 02:46:45.414234: train_loss -0.6847
2024-12-17 02:46:45.415079: val_loss -0.502
2024-12-17 02:46:45.415964: Pseudo dice [0.7234]
2024-12-17 02:46:45.416818: Epoch time: 88.96 s
2024-12-17 02:46:47.031781: 
2024-12-17 02:46:47.033586: Epoch 70
2024-12-17 02:46:47.034419: Current learning rate: 0.00568
2024-12-17 02:48:16.046216: Validation loss improved from -0.55502 to -0.56856! Patience: 14/50
2024-12-17 02:48:16.047578: train_loss -0.6789
2024-12-17 02:48:16.048699: val_loss -0.5686
2024-12-17 02:48:16.049410: Pseudo dice [0.755]
2024-12-17 02:48:16.050202: Epoch time: 89.02 s
2024-12-17 02:48:16.050992: Yayy! New best EMA pseudo Dice: 0.7346
2024-12-17 02:48:17.663063: 
2024-12-17 02:48:17.664465: Epoch 71
2024-12-17 02:48:17.665281: Current learning rate: 0.00562
2024-12-17 02:49:46.567303: Validation loss did not improve from -0.56856. Patience: 1/50
2024-12-17 02:49:46.568253: train_loss -0.6836
2024-12-17 02:49:46.569335: val_loss -0.5499
2024-12-17 02:49:46.570153: Pseudo dice [0.7489]
2024-12-17 02:49:46.571095: Epoch time: 88.91 s
2024-12-17 02:49:46.571975: Yayy! New best EMA pseudo Dice: 0.736
2024-12-17 02:49:48.567577: 
2024-12-17 02:49:48.569049: Epoch 72
2024-12-17 02:49:48.569903: Current learning rate: 0.00555
2024-12-17 02:51:17.623813: Validation loss did not improve from -0.56856. Patience: 2/50
2024-12-17 02:51:17.624936: train_loss -0.6962
2024-12-17 02:51:17.625747: val_loss -0.549
2024-12-17 02:51:17.626433: Pseudo dice [0.7511]
2024-12-17 02:51:17.627063: Epoch time: 89.06 s
2024-12-17 02:51:17.627773: Yayy! New best EMA pseudo Dice: 0.7375
2024-12-17 02:51:19.209408: 
2024-12-17 02:51:19.211120: Epoch 73
2024-12-17 02:51:19.212069: Current learning rate: 0.00549
2024-12-17 02:52:48.385180: Validation loss did not improve from -0.56856. Patience: 3/50
2024-12-17 02:52:48.386128: train_loss -0.6897
2024-12-17 02:52:48.387028: val_loss -0.5306
2024-12-17 02:52:48.387878: Pseudo dice [0.7365]
2024-12-17 02:52:48.388744: Epoch time: 89.18 s
2024-12-17 02:52:49.614108: 
2024-12-17 02:52:49.615607: Epoch 74
2024-12-17 02:52:49.616442: Current learning rate: 0.00542
2024-12-17 02:54:18.823393: Validation loss improved from -0.56856 to -0.57072! Patience: 3/50
2024-12-17 02:54:18.824655: train_loss -0.6875
2024-12-17 02:54:18.825864: val_loss -0.5707
2024-12-17 02:54:18.826765: Pseudo dice [0.7558]
2024-12-17 02:54:18.827644: Epoch time: 89.21 s
2024-12-17 02:54:19.194955: Yayy! New best EMA pseudo Dice: 0.7393
2024-12-17 02:54:20.834958: 
2024-12-17 02:54:20.836780: Epoch 75
2024-12-17 02:54:20.837726: Current learning rate: 0.00536
2024-12-17 02:55:50.033286: Validation loss did not improve from -0.57072. Patience: 1/50
2024-12-17 02:55:50.034259: train_loss -0.6875
2024-12-17 02:55:50.035075: val_loss -0.5676
2024-12-17 02:55:50.035862: Pseudo dice [0.7575]
2024-12-17 02:55:50.036740: Epoch time: 89.2 s
2024-12-17 02:55:50.037600: Yayy! New best EMA pseudo Dice: 0.7411
2024-12-17 02:55:51.643144: 
2024-12-17 02:55:51.645049: Epoch 76
2024-12-17 02:55:51.645861: Current learning rate: 0.00529
2024-12-17 02:57:20.841860: Validation loss did not improve from -0.57072. Patience: 2/50
2024-12-17 02:57:20.842828: train_loss -0.6919
2024-12-17 02:57:20.843810: val_loss -0.541
2024-12-17 02:57:20.844609: Pseudo dice [0.7394]
2024-12-17 02:57:20.845276: Epoch time: 89.2 s
2024-12-17 02:57:22.091361: 
2024-12-17 02:57:22.092627: Epoch 77
2024-12-17 02:57:22.093325: Current learning rate: 0.00523
2024-12-17 02:58:51.465365: Validation loss did not improve from -0.57072. Patience: 3/50
2024-12-17 02:58:51.466467: train_loss -0.6863
2024-12-17 02:58:51.467693: val_loss -0.5209
2024-12-17 02:58:51.468793: Pseudo dice [0.7268]
2024-12-17 02:58:51.470083: Epoch time: 89.38 s
2024-12-17 02:58:52.755968: 
2024-12-17 02:58:52.757613: Epoch 78
2024-12-17 02:58:52.758715: Current learning rate: 0.00517
2024-12-17 03:00:21.986170: Validation loss improved from -0.57072 to -0.58233! Patience: 3/50
2024-12-17 03:00:21.987471: train_loss -0.6956
2024-12-17 03:00:21.988856: val_loss -0.5823
2024-12-17 03:00:21.989706: Pseudo dice [0.7584]
2024-12-17 03:00:21.990506: Epoch time: 89.23 s
2024-12-17 03:00:21.991177: Yayy! New best EMA pseudo Dice: 0.7414
2024-12-17 03:00:23.619216: 
2024-12-17 03:00:23.620706: Epoch 79
2024-12-17 03:00:23.621681: Current learning rate: 0.0051
2024-12-17 03:01:52.881871: Validation loss did not improve from -0.58233. Patience: 1/50
2024-12-17 03:01:52.882721: train_loss -0.7021
2024-12-17 03:01:52.883686: val_loss -0.5643
2024-12-17 03:01:52.884540: Pseudo dice [0.7522]
2024-12-17 03:01:52.885494: Epoch time: 89.26 s
2024-12-17 03:01:53.253706: Yayy! New best EMA pseudo Dice: 0.7425
2024-12-17 03:01:54.977524: 
2024-12-17 03:01:54.979339: Epoch 80
2024-12-17 03:01:54.980266: Current learning rate: 0.00504
2024-12-17 03:03:23.873232: Validation loss did not improve from -0.58233. Patience: 2/50
2024-12-17 03:03:23.874375: train_loss -0.6981
2024-12-17 03:03:23.875457: val_loss -0.5555
2024-12-17 03:03:23.876263: Pseudo dice [0.75]
2024-12-17 03:03:23.877190: Epoch time: 88.9 s
2024-12-17 03:03:23.878103: Yayy! New best EMA pseudo Dice: 0.7432
2024-12-17 03:03:25.474792: 
2024-12-17 03:03:25.476571: Epoch 81
2024-12-17 03:03:25.477512: Current learning rate: 0.00497
2024-12-17 03:04:54.299256: Validation loss did not improve from -0.58233. Patience: 3/50
2024-12-17 03:04:54.300385: train_loss -0.6985
2024-12-17 03:04:54.301353: val_loss -0.5545
2024-12-17 03:04:54.302138: Pseudo dice [0.7459]
2024-12-17 03:04:54.302921: Epoch time: 88.83 s
2024-12-17 03:04:54.303771: Yayy! New best EMA pseudo Dice: 0.7435
2024-12-17 03:04:56.266689: 
2024-12-17 03:04:56.268481: Epoch 82
2024-12-17 03:04:56.269205: Current learning rate: 0.00491
2024-12-17 03:06:25.286680: Validation loss did not improve from -0.58233. Patience: 4/50
2024-12-17 03:06:25.288008: train_loss -0.7008
2024-12-17 03:06:25.289411: val_loss -0.5583
2024-12-17 03:06:25.290203: Pseudo dice [0.7528]
2024-12-17 03:06:25.290974: Epoch time: 89.02 s
2024-12-17 03:06:25.291907: Yayy! New best EMA pseudo Dice: 0.7444
2024-12-17 03:06:26.831734: 
2024-12-17 03:06:26.833247: Epoch 83
2024-12-17 03:06:26.833955: Current learning rate: 0.00484
2024-12-17 03:07:56.078162: Validation loss did not improve from -0.58233. Patience: 5/50
2024-12-17 03:07:56.080927: train_loss -0.7069
2024-12-17 03:07:56.082322: val_loss -0.5488
2024-12-17 03:07:56.083071: Pseudo dice [0.7509]
2024-12-17 03:07:56.083848: Epoch time: 89.25 s
2024-12-17 03:07:56.084629: Yayy! New best EMA pseudo Dice: 0.7451
2024-12-17 03:07:57.666806: 
2024-12-17 03:07:57.668648: Epoch 84
2024-12-17 03:07:57.669509: Current learning rate: 0.00478
2024-12-17 03:09:26.598997: Validation loss did not improve from -0.58233. Patience: 6/50
2024-12-17 03:09:26.600159: train_loss -0.7026
2024-12-17 03:09:26.601147: val_loss -0.5293
2024-12-17 03:09:26.601915: Pseudo dice [0.7407]
2024-12-17 03:09:26.602584: Epoch time: 88.93 s
2024-12-17 03:09:28.163435: 
2024-12-17 03:09:28.165156: Epoch 85
2024-12-17 03:09:28.165960: Current learning rate: 0.00471
2024-12-17 03:10:57.147839: Validation loss did not improve from -0.58233. Patience: 7/50
2024-12-17 03:10:57.149442: train_loss -0.7038
2024-12-17 03:10:57.150614: val_loss -0.5638
2024-12-17 03:10:57.151537: Pseudo dice [0.7484]
2024-12-17 03:10:57.152447: Epoch time: 88.99 s
2024-12-17 03:10:58.335236: 
2024-12-17 03:10:58.336789: Epoch 86
2024-12-17 03:10:58.337486: Current learning rate: 0.00465
2024-12-17 03:12:27.212843: Validation loss did not improve from -0.58233. Patience: 8/50
2024-12-17 03:12:27.214119: train_loss -0.7054
2024-12-17 03:12:27.215299: val_loss -0.5399
2024-12-17 03:12:27.215976: Pseudo dice [0.7367]
2024-12-17 03:12:27.216741: Epoch time: 88.88 s
2024-12-17 03:12:28.414210: 
2024-12-17 03:12:28.416229: Epoch 87
2024-12-17 03:12:28.417400: Current learning rate: 0.00458
2024-12-17 03:13:57.063996: Validation loss did not improve from -0.58233. Patience: 9/50
2024-12-17 03:13:57.064933: train_loss -0.7107
2024-12-17 03:13:57.065938: val_loss -0.5762
2024-12-17 03:13:57.066588: Pseudo dice [0.7586]
2024-12-17 03:13:57.067364: Epoch time: 88.65 s
2024-12-17 03:13:57.068130: Yayy! New best EMA pseudo Dice: 0.7456
2024-12-17 03:13:58.582675: 
2024-12-17 03:13:58.584593: Epoch 88
2024-12-17 03:13:58.585488: Current learning rate: 0.00452
2024-12-17 03:15:27.231518: Validation loss did not improve from -0.58233. Patience: 10/50
2024-12-17 03:15:27.232682: train_loss -0.7037
2024-12-17 03:15:27.233539: val_loss -0.5123
2024-12-17 03:15:27.234335: Pseudo dice [0.737]
2024-12-17 03:15:27.234963: Epoch time: 88.65 s
2024-12-17 03:15:28.543268: 
2024-12-17 03:15:28.545067: Epoch 89
2024-12-17 03:15:28.545835: Current learning rate: 0.00445
2024-12-17 03:16:57.154773: Validation loss did not improve from -0.58233. Patience: 11/50
2024-12-17 03:16:57.155899: train_loss -0.7079
2024-12-17 03:16:57.156853: val_loss -0.5477
2024-12-17 03:16:57.157726: Pseudo dice [0.7436]
2024-12-17 03:16:57.158383: Epoch time: 88.61 s
2024-12-17 03:16:58.725423: 
2024-12-17 03:16:58.726896: Epoch 90
2024-12-17 03:16:58.727840: Current learning rate: 0.00438
2024-12-17 03:18:27.250397: Validation loss did not improve from -0.58233. Patience: 12/50
2024-12-17 03:18:27.251328: train_loss -0.7149
2024-12-17 03:18:27.252154: val_loss -0.54
2024-12-17 03:18:27.252984: Pseudo dice [0.7396]
2024-12-17 03:18:27.253947: Epoch time: 88.53 s
2024-12-17 03:18:28.413029: 
2024-12-17 03:18:28.414932: Epoch 91
2024-12-17 03:18:28.415635: Current learning rate: 0.00432
2024-12-17 03:19:57.007167: Validation loss did not improve from -0.58233. Patience: 13/50
2024-12-17 03:19:57.008540: train_loss -0.71
2024-12-17 03:19:57.009484: val_loss -0.5267
2024-12-17 03:19:57.010201: Pseudo dice [0.7451]
2024-12-17 03:19:57.010990: Epoch time: 88.6 s
2024-12-17 03:19:58.177270: 
2024-12-17 03:19:58.178709: Epoch 92
2024-12-17 03:19:58.179512: Current learning rate: 0.00425
2024-12-17 03:21:26.744318: Validation loss did not improve from -0.58233. Patience: 14/50
2024-12-17 03:21:26.745452: train_loss -0.7064
2024-12-17 03:21:26.746453: val_loss -0.5248
2024-12-17 03:21:26.747306: Pseudo dice [0.7287]
2024-12-17 03:21:26.748044: Epoch time: 88.57 s
2024-12-17 03:21:28.282461: 
2024-12-17 03:21:28.284340: Epoch 93
2024-12-17 03:21:28.285166: Current learning rate: 0.00419
2024-12-17 03:22:56.804612: Validation loss did not improve from -0.58233. Patience: 15/50
2024-12-17 03:22:56.805594: train_loss -0.7096
2024-12-17 03:22:56.806563: val_loss -0.5259
2024-12-17 03:22:56.807329: Pseudo dice [0.7443]
2024-12-17 03:22:56.808038: Epoch time: 88.52 s
2024-12-17 03:22:58.000809: 
2024-12-17 03:22:58.002321: Epoch 94
2024-12-17 03:22:58.003052: Current learning rate: 0.00412
2024-12-17 03:24:26.801285: Validation loss did not improve from -0.58233. Patience: 16/50
2024-12-17 03:24:26.802307: train_loss -0.7118
2024-12-17 03:24:26.803231: val_loss -0.5221
2024-12-17 03:24:26.804011: Pseudo dice [0.7285]
2024-12-17 03:24:26.804715: Epoch time: 88.8 s
2024-12-17 03:24:28.349382: 
2024-12-17 03:24:28.350623: Epoch 95
2024-12-17 03:24:28.351380: Current learning rate: 0.00405
2024-12-17 03:25:57.175283: Validation loss did not improve from -0.58233. Patience: 17/50
2024-12-17 03:25:57.176686: train_loss -0.7127
2024-12-17 03:25:57.177773: val_loss -0.5362
2024-12-17 03:25:57.178577: Pseudo dice [0.745]
2024-12-17 03:25:57.179324: Epoch time: 88.83 s
2024-12-17 03:25:58.384990: 
2024-12-17 03:25:58.386563: Epoch 96
2024-12-17 03:25:58.387566: Current learning rate: 0.00399
2024-12-17 03:27:27.295132: Validation loss did not improve from -0.58233. Patience: 18/50
2024-12-17 03:27:27.295885: train_loss -0.7176
2024-12-17 03:27:27.296727: val_loss -0.5584
2024-12-17 03:27:27.297411: Pseudo dice [0.7524]
2024-12-17 03:27:27.298106: Epoch time: 88.91 s
2024-12-17 03:27:28.530830: 
2024-12-17 03:27:28.532406: Epoch 97
2024-12-17 03:27:28.533077: Current learning rate: 0.00392
2024-12-17 03:28:57.475466: Validation loss did not improve from -0.58233. Patience: 19/50
2024-12-17 03:28:57.476566: train_loss -0.7239
2024-12-17 03:28:57.477501: val_loss -0.5235
2024-12-17 03:28:57.478350: Pseudo dice [0.7332]
2024-12-17 03:28:57.479244: Epoch time: 88.95 s
2024-12-17 03:28:58.721210: 
2024-12-17 03:28:58.722690: Epoch 98
2024-12-17 03:28:58.723513: Current learning rate: 0.00385
2024-12-17 03:30:27.606814: Validation loss did not improve from -0.58233. Patience: 20/50
2024-12-17 03:30:27.607881: train_loss -0.7145
2024-12-17 03:30:27.608830: val_loss -0.5215
2024-12-17 03:30:27.609562: Pseudo dice [0.7251]
2024-12-17 03:30:27.610266: Epoch time: 88.89 s
2024-12-17 03:30:28.862041: 
2024-12-17 03:30:28.863943: Epoch 99
2024-12-17 03:30:28.864722: Current learning rate: 0.00379
2024-12-17 03:31:57.664894: Validation loss did not improve from -0.58233. Patience: 21/50
2024-12-17 03:31:57.665975: train_loss -0.7218
2024-12-17 03:31:57.666912: val_loss -0.5355
2024-12-17 03:31:57.667653: Pseudo dice [0.7388]
2024-12-17 03:31:57.668432: Epoch time: 88.8 s
2024-12-17 03:31:59.249558: 
2024-12-17 03:31:59.250997: Epoch 100
2024-12-17 03:31:59.251774: Current learning rate: 0.00372
2024-12-17 03:33:28.122659: Validation loss did not improve from -0.58233. Patience: 22/50
2024-12-17 03:33:28.123979: train_loss -0.7211
2024-12-17 03:33:28.124998: val_loss -0.5727
2024-12-17 03:33:28.125696: Pseudo dice [0.7636]
2024-12-17 03:33:28.126375: Epoch time: 88.88 s
2024-12-17 03:33:29.319372: 
2024-12-17 03:33:29.321016: Epoch 101
2024-12-17 03:33:29.321859: Current learning rate: 0.00365
2024-12-17 03:34:58.253185: Validation loss did not improve from -0.58233. Patience: 23/50
2024-12-17 03:34:58.254602: train_loss -0.7208
2024-12-17 03:34:58.255956: val_loss -0.5151
2024-12-17 03:34:58.256913: Pseudo dice [0.7379]
2024-12-17 03:34:58.257941: Epoch time: 88.94 s
2024-12-17 03:34:59.463053: 
2024-12-17 03:34:59.464879: Epoch 102
2024-12-17 03:34:59.465990: Current learning rate: 0.00359
2024-12-17 03:36:28.568093: Validation loss did not improve from -0.58233. Patience: 24/50
2024-12-17 03:36:28.569027: train_loss -0.7306
2024-12-17 03:36:28.569834: val_loss -0.5518
2024-12-17 03:36:28.570533: Pseudo dice [0.7509]
2024-12-17 03:36:28.571292: Epoch time: 89.11 s
2024-12-17 03:36:30.096489: 
2024-12-17 03:36:30.098811: Epoch 103
2024-12-17 03:36:30.099851: Current learning rate: 0.00352
2024-12-17 03:37:59.364356: Validation loss did not improve from -0.58233. Patience: 25/50
2024-12-17 03:37:59.365147: train_loss -0.7213
2024-12-17 03:37:59.366038: val_loss -0.5657
2024-12-17 03:37:59.366724: Pseudo dice [0.761]
2024-12-17 03:37:59.367542: Epoch time: 89.27 s
2024-12-17 03:38:00.580947: 
2024-12-17 03:38:00.582906: Epoch 104
2024-12-17 03:38:00.583686: Current learning rate: 0.00345
2024-12-17 03:39:29.760304: Validation loss did not improve from -0.58233. Patience: 26/50
2024-12-17 03:39:29.761313: train_loss -0.7323
2024-12-17 03:39:29.762301: val_loss -0.5553
2024-12-17 03:39:29.763155: Pseudo dice [0.7461]
2024-12-17 03:39:29.763988: Epoch time: 89.18 s
2024-12-17 03:39:31.318014: 
2024-12-17 03:39:31.319407: Epoch 105
2024-12-17 03:39:31.320361: Current learning rate: 0.00338
2024-12-17 03:41:00.429990: Validation loss did not improve from -0.58233. Patience: 27/50
2024-12-17 03:41:00.431276: train_loss -0.7308
2024-12-17 03:41:00.432326: val_loss -0.5415
2024-12-17 03:41:00.433253: Pseudo dice [0.7449]
2024-12-17 03:41:00.434005: Epoch time: 89.11 s
2024-12-17 03:41:01.618580: 
2024-12-17 03:41:01.619903: Epoch 106
2024-12-17 03:41:01.620719: Current learning rate: 0.00332
2024-12-17 03:42:30.926836: Validation loss did not improve from -0.58233. Patience: 28/50
2024-12-17 03:42:30.927825: train_loss -0.7248
2024-12-17 03:42:30.928795: val_loss -0.5315
2024-12-17 03:42:30.929595: Pseudo dice [0.7375]
2024-12-17 03:42:30.930439: Epoch time: 89.31 s
2024-12-17 03:42:32.171380: 
2024-12-17 03:42:32.172479: Epoch 107
2024-12-17 03:42:32.173322: Current learning rate: 0.00325
2024-12-17 03:44:01.426697: Validation loss did not improve from -0.58233. Patience: 29/50
2024-12-17 03:44:01.427705: train_loss -0.7323
2024-12-17 03:44:01.428534: val_loss -0.5423
2024-12-17 03:44:01.429260: Pseudo dice [0.741]
2024-12-17 03:44:01.430050: Epoch time: 89.26 s
2024-12-17 03:44:02.653693: 
2024-12-17 03:44:02.655492: Epoch 108
2024-12-17 03:44:02.656426: Current learning rate: 0.00318
2024-12-17 03:45:31.953643: Validation loss did not improve from -0.58233. Patience: 30/50
2024-12-17 03:45:31.954644: train_loss -0.7332
2024-12-17 03:45:31.955965: val_loss -0.5484
2024-12-17 03:45:31.957063: Pseudo dice [0.751]
2024-12-17 03:45:31.957908: Epoch time: 89.3 s
2024-12-17 03:45:33.175447: 
2024-12-17 03:45:33.177123: Epoch 109
2024-12-17 03:45:33.177878: Current learning rate: 0.00311
2024-12-17 03:47:02.428771: Validation loss did not improve from -0.58233. Patience: 31/50
2024-12-17 03:47:02.429872: train_loss -0.7401
2024-12-17 03:47:02.430945: val_loss -0.5455
2024-12-17 03:47:02.431994: Pseudo dice [0.7465]
2024-12-17 03:47:02.433076: Epoch time: 89.26 s
2024-12-17 03:47:04.003354: 
2024-12-17 03:47:04.004850: Epoch 110
2024-12-17 03:47:04.005923: Current learning rate: 0.00304
2024-12-17 03:48:33.213624: Validation loss did not improve from -0.58233. Patience: 32/50
2024-12-17 03:48:33.214561: train_loss -0.7367
2024-12-17 03:48:33.215504: val_loss -0.5626
2024-12-17 03:48:33.216195: Pseudo dice [0.7474]
2024-12-17 03:48:33.217045: Epoch time: 89.21 s
2024-12-17 03:48:34.438365: 
2024-12-17 03:48:34.440005: Epoch 111
2024-12-17 03:48:34.440767: Current learning rate: 0.00297
2024-12-17 03:50:03.770877: Validation loss did not improve from -0.58233. Patience: 33/50
2024-12-17 03:50:03.772181: train_loss -0.7362
2024-12-17 03:50:03.773049: val_loss -0.5578
2024-12-17 03:50:03.773786: Pseudo dice [0.7608]
2024-12-17 03:50:03.774566: Epoch time: 89.33 s
2024-12-17 03:50:03.775369: Yayy! New best EMA pseudo Dice: 0.7465
2024-12-17 03:50:05.306603: 
2024-12-17 03:50:05.308176: Epoch 112
2024-12-17 03:50:05.309243: Current learning rate: 0.00291
2024-12-17 03:51:34.528068: Validation loss did not improve from -0.58233. Patience: 34/50
2024-12-17 03:51:34.529417: train_loss -0.7353
2024-12-17 03:51:34.530394: val_loss -0.5766
2024-12-17 03:51:34.531201: Pseudo dice [0.7561]
2024-12-17 03:51:34.531882: Epoch time: 89.22 s
2024-12-17 03:51:34.532647: Yayy! New best EMA pseudo Dice: 0.7475
2024-12-17 03:51:36.118577: 
2024-12-17 03:51:36.119995: Epoch 113
2024-12-17 03:51:36.120893: Current learning rate: 0.00284
2024-12-17 03:53:05.302363: Validation loss did not improve from -0.58233. Patience: 35/50
2024-12-17 03:53:05.303535: train_loss -0.739
2024-12-17 03:53:05.304341: val_loss -0.5363
2024-12-17 03:53:05.305212: Pseudo dice [0.7413]
2024-12-17 03:53:05.305944: Epoch time: 89.19 s
2024-12-17 03:53:06.857454: 
2024-12-17 03:53:06.858797: Epoch 114
2024-12-17 03:53:06.859536: Current learning rate: 0.00277
2024-12-17 03:54:36.121997: Validation loss did not improve from -0.58233. Patience: 36/50
2024-12-17 03:54:36.123162: train_loss -0.7324
2024-12-17 03:54:36.124069: val_loss -0.551
2024-12-17 03:54:36.124804: Pseudo dice [0.75]
2024-12-17 03:54:36.125470: Epoch time: 89.27 s
2024-12-17 03:54:37.711056: 
2024-12-17 03:54:37.712751: Epoch 115
2024-12-17 03:54:37.713900: Current learning rate: 0.0027
2024-12-17 03:56:06.681410: Validation loss did not improve from -0.58233. Patience: 37/50
2024-12-17 03:56:06.682087: train_loss -0.7434
2024-12-17 03:56:06.683053: val_loss -0.5519
2024-12-17 03:56:06.683851: Pseudo dice [0.7541]
2024-12-17 03:56:06.684679: Epoch time: 88.97 s
2024-12-17 03:56:06.685541: Yayy! New best EMA pseudo Dice: 0.7479
2024-12-17 03:56:08.272380: 
2024-12-17 03:56:08.274128: Epoch 116
2024-12-17 03:56:08.275026: Current learning rate: 0.00263
2024-12-17 03:57:37.143067: Validation loss did not improve from -0.58233. Patience: 38/50
2024-12-17 03:57:37.144251: train_loss -0.7419
2024-12-17 03:57:37.145201: val_loss -0.5543
2024-12-17 03:57:37.145904: Pseudo dice [0.7486]
2024-12-17 03:57:37.146568: Epoch time: 88.87 s
2024-12-17 03:57:37.147233: Yayy! New best EMA pseudo Dice: 0.748
2024-12-17 03:57:38.747052: 
2024-12-17 03:57:38.748639: Epoch 117
2024-12-17 03:57:38.749543: Current learning rate: 0.00256
2024-12-17 03:59:07.616211: Validation loss did not improve from -0.58233. Patience: 39/50
2024-12-17 03:59:07.617471: train_loss -0.7456
2024-12-17 03:59:07.618444: val_loss -0.5586
2024-12-17 03:59:07.619496: Pseudo dice [0.7457]
2024-12-17 03:59:07.620172: Epoch time: 88.87 s
2024-12-17 03:59:08.866446: 
2024-12-17 03:59:08.868155: Epoch 118
2024-12-17 03:59:08.868891: Current learning rate: 0.00249
2024-12-17 04:00:37.831501: Validation loss did not improve from -0.58233. Patience: 40/50
2024-12-17 04:00:37.832483: train_loss -0.7423
2024-12-17 04:00:37.833614: val_loss -0.5625
2024-12-17 04:00:37.834492: Pseudo dice [0.7545]
2024-12-17 04:00:37.835176: Epoch time: 88.97 s
2024-12-17 04:00:37.835823: Yayy! New best EMA pseudo Dice: 0.7484
2024-12-17 04:00:39.487393: 
2024-12-17 04:00:39.489168: Epoch 119
2024-12-17 04:00:39.489975: Current learning rate: 0.00242
2024-12-17 04:02:08.426336: Validation loss did not improve from -0.58233. Patience: 41/50
2024-12-17 04:02:08.427383: train_loss -0.7421
2024-12-17 04:02:08.428268: val_loss -0.5355
2024-12-17 04:02:08.428975: Pseudo dice [0.742]
2024-12-17 04:02:08.429706: Epoch time: 88.94 s
2024-12-17 04:02:10.025602: 
2024-12-17 04:02:10.027149: Epoch 120
2024-12-17 04:02:10.027939: Current learning rate: 0.00235
2024-12-17 04:03:38.957687: Validation loss did not improve from -0.58233. Patience: 42/50
2024-12-17 04:03:38.959195: train_loss -0.7418
2024-12-17 04:03:38.960448: val_loss -0.5592
2024-12-17 04:03:38.961191: Pseudo dice [0.7506]
2024-12-17 04:03:38.962005: Epoch time: 88.93 s
2024-12-17 04:03:40.279073: 
2024-12-17 04:03:40.280934: Epoch 121
2024-12-17 04:03:40.282357: Current learning rate: 0.00228
2024-12-17 04:05:09.195812: Validation loss improved from -0.58233 to -0.58994! Patience: 42/50
2024-12-17 04:05:09.196465: train_loss -0.7489
2024-12-17 04:05:09.197232: val_loss -0.5899
2024-12-17 04:05:09.197865: Pseudo dice [0.7644]
2024-12-17 04:05:09.198558: Epoch time: 88.92 s
2024-12-17 04:05:09.199417: Yayy! New best EMA pseudo Dice: 0.7497
2024-12-17 04:05:10.847811: 
2024-12-17 04:05:10.849637: Epoch 122
2024-12-17 04:05:10.850442: Current learning rate: 0.00221
2024-12-17 04:06:39.726382: Validation loss did not improve from -0.58994. Patience: 1/50
2024-12-17 04:06:39.727812: train_loss -0.7493
2024-12-17 04:06:39.728975: val_loss -0.5553
2024-12-17 04:06:39.729810: Pseudo dice [0.7492]
2024-12-17 04:06:39.730684: Epoch time: 88.88 s
2024-12-17 04:06:40.965417: 
2024-12-17 04:06:40.967163: Epoch 123
2024-12-17 04:06:40.968083: Current learning rate: 0.00214
2024-12-17 04:08:10.093083: Validation loss did not improve from -0.58994. Patience: 2/50
2024-12-17 04:08:10.094311: train_loss -0.7487
2024-12-17 04:08:10.095277: val_loss -0.5534
2024-12-17 04:08:10.096102: Pseudo dice [0.7567]
2024-12-17 04:08:10.097187: Epoch time: 89.13 s
2024-12-17 04:08:10.098005: Yayy! New best EMA pseudo Dice: 0.7504
2024-12-17 04:08:12.064297: 
2024-12-17 04:08:12.066374: Epoch 124
2024-12-17 04:08:12.067204: Current learning rate: 0.00207
2024-12-17 04:09:41.322844: Validation loss did not improve from -0.58994. Patience: 3/50
2024-12-17 04:09:41.323840: train_loss -0.7509
2024-12-17 04:09:41.324928: val_loss -0.5555
2024-12-17 04:09:41.325612: Pseudo dice [0.7523]
2024-12-17 04:09:41.326373: Epoch time: 89.26 s
2024-12-17 04:09:41.685570: Yayy! New best EMA pseudo Dice: 0.7505
2024-12-17 04:09:43.279174: 
2024-12-17 04:09:43.281092: Epoch 125
2024-12-17 04:09:43.282310: Current learning rate: 0.00199
2024-12-17 04:11:12.547210: Validation loss did not improve from -0.58994. Patience: 4/50
2024-12-17 04:11:12.548295: train_loss -0.7477
2024-12-17 04:11:12.549417: val_loss -0.5484
2024-12-17 04:11:12.550442: Pseudo dice [0.7432]
2024-12-17 04:11:12.551169: Epoch time: 89.27 s
2024-12-17 04:11:13.807590: 
2024-12-17 04:11:13.809983: Epoch 126
2024-12-17 04:11:13.810823: Current learning rate: 0.00192
2024-12-17 04:12:43.070683: Validation loss did not improve from -0.58994. Patience: 5/50
2024-12-17 04:12:43.073436: train_loss -0.7565
2024-12-17 04:12:43.074428: val_loss -0.5615
2024-12-17 04:12:43.075238: Pseudo dice [0.7545]
2024-12-17 04:12:43.076066: Epoch time: 89.27 s
2024-12-17 04:12:44.356295: 
2024-12-17 04:12:44.357544: Epoch 127
2024-12-17 04:12:44.358334: Current learning rate: 0.00185
2024-12-17 04:14:13.525645: Validation loss did not improve from -0.58994. Patience: 6/50
2024-12-17 04:14:13.527125: train_loss -0.7531
2024-12-17 04:14:13.528250: val_loss -0.5523
2024-12-17 04:14:13.529283: Pseudo dice [0.7534]
2024-12-17 04:14:13.530159: Epoch time: 89.17 s
2024-12-17 04:14:13.531137: Yayy! New best EMA pseudo Dice: 0.7506
2024-12-17 04:14:15.245405: 
2024-12-17 04:14:15.247295: Epoch 128
2024-12-17 04:14:15.248162: Current learning rate: 0.00178
2024-12-17 04:15:44.463969: Validation loss did not improve from -0.58994. Patience: 7/50
2024-12-17 04:15:44.465276: train_loss -0.7519
2024-12-17 04:15:44.466183: val_loss -0.578
2024-12-17 04:15:44.467103: Pseudo dice [0.7621]
2024-12-17 04:15:44.467956: Epoch time: 89.22 s
2024-12-17 04:15:44.468595: Yayy! New best EMA pseudo Dice: 0.7517
2024-12-17 04:15:46.049476: 
2024-12-17 04:15:46.050829: Epoch 129
2024-12-17 04:15:46.051753: Current learning rate: 0.0017
2024-12-17 04:17:15.198644: Validation loss did not improve from -0.58994. Patience: 8/50
2024-12-17 04:17:15.199553: train_loss -0.7568
2024-12-17 04:17:15.200366: val_loss -0.5692
2024-12-17 04:17:15.201154: Pseudo dice [0.7596]
2024-12-17 04:17:15.201925: Epoch time: 89.15 s
2024-12-17 04:17:15.559949: Yayy! New best EMA pseudo Dice: 0.7525
2024-12-17 04:17:17.145772: 
2024-12-17 04:17:17.147043: Epoch 130
2024-12-17 04:17:17.147789: Current learning rate: 0.00163
2024-12-17 04:18:46.263534: Validation loss did not improve from -0.58994. Patience: 9/50
2024-12-17 04:18:46.264726: train_loss -0.7523
2024-12-17 04:18:46.265478: val_loss -0.5536
2024-12-17 04:18:46.266429: Pseudo dice [0.7509]
2024-12-17 04:18:46.267421: Epoch time: 89.12 s
2024-12-17 04:18:47.499904: 
2024-12-17 04:18:47.501501: Epoch 131
2024-12-17 04:18:47.502273: Current learning rate: 0.00156
2024-12-17 04:20:16.805459: Validation loss did not improve from -0.58994. Patience: 10/50
2024-12-17 04:20:16.806655: train_loss -0.7604
2024-12-17 04:20:16.807618: val_loss -0.5709
2024-12-17 04:20:16.808389: Pseudo dice [0.7636]
2024-12-17 04:20:16.809288: Epoch time: 89.31 s
2024-12-17 04:20:16.809975: Yayy! New best EMA pseudo Dice: 0.7535
2024-12-17 04:20:18.472047: 
2024-12-17 04:20:18.473400: Epoch 132
2024-12-17 04:20:18.474346: Current learning rate: 0.00148
2024-12-17 04:21:47.814887: Validation loss did not improve from -0.58994. Patience: 11/50
2024-12-17 04:21:47.815935: train_loss -0.7554
2024-12-17 04:21:47.817009: val_loss -0.5726
2024-12-17 04:21:47.817759: Pseudo dice [0.7597]
2024-12-17 04:21:47.818513: Epoch time: 89.34 s
2024-12-17 04:21:47.819170: Yayy! New best EMA pseudo Dice: 0.7541
2024-12-17 04:21:49.432760: 
2024-12-17 04:21:49.434127: Epoch 133
2024-12-17 04:21:49.434914: Current learning rate: 0.00141
2024-12-17 04:23:18.626324: Validation loss did not improve from -0.58994. Patience: 12/50
2024-12-17 04:23:18.627471: train_loss -0.7529
2024-12-17 04:23:18.628674: val_loss -0.5401
2024-12-17 04:23:18.629635: Pseudo dice [0.7486]
2024-12-17 04:23:18.630670: Epoch time: 89.2 s
2024-12-17 04:23:19.849394: 
2024-12-17 04:23:19.851072: Epoch 134
2024-12-17 04:23:19.851969: Current learning rate: 0.00133
2024-12-17 04:24:48.937828: Validation loss did not improve from -0.58994. Patience: 13/50
2024-12-17 04:24:48.939435: train_loss -0.7552
2024-12-17 04:24:48.940672: val_loss -0.5448
2024-12-17 04:24:48.941751: Pseudo dice [0.749]
2024-12-17 04:24:48.942779: Epoch time: 89.09 s
2024-12-17 04:24:51.031914: 
2024-12-17 04:24:51.034052: Epoch 135
2024-12-17 04:24:51.034837: Current learning rate: 0.00126
2024-12-17 04:26:20.150074: Validation loss did not improve from -0.58994. Patience: 14/50
2024-12-17 04:26:20.151176: train_loss -0.7597
2024-12-17 04:26:20.152353: val_loss -0.5663
2024-12-17 04:26:20.153361: Pseudo dice [0.7652]
2024-12-17 04:26:20.154411: Epoch time: 89.12 s
2024-12-17 04:26:20.155434: Yayy! New best EMA pseudo Dice: 0.7543
2024-12-17 04:26:21.847601: 
2024-12-17 04:26:21.849718: Epoch 136
2024-12-17 04:26:21.850498: Current learning rate: 0.00118
2024-12-17 04:27:50.997456: Validation loss did not improve from -0.58994. Patience: 15/50
2024-12-17 04:27:50.998745: train_loss -0.7598
2024-12-17 04:27:51.000101: val_loss -0.5606
2024-12-17 04:27:51.001008: Pseudo dice [0.7566]
2024-12-17 04:27:51.001979: Epoch time: 89.15 s
2024-12-17 04:27:51.002826: Yayy! New best EMA pseudo Dice: 0.7545
2024-12-17 04:27:52.584520: 
2024-12-17 04:27:52.586344: Epoch 137
2024-12-17 04:27:52.587114: Current learning rate: 0.00111
2024-12-17 04:29:21.746958: Validation loss did not improve from -0.58994. Patience: 16/50
2024-12-17 04:29:21.748092: train_loss -0.7626
2024-12-17 04:29:21.749213: val_loss -0.568
2024-12-17 04:29:21.750116: Pseudo dice [0.7556]
2024-12-17 04:29:21.751126: Epoch time: 89.16 s
2024-12-17 04:29:21.752108: Yayy! New best EMA pseudo Dice: 0.7546
2024-12-17 04:29:23.404406: 
2024-12-17 04:29:23.405775: Epoch 138
2024-12-17 04:29:23.406767: Current learning rate: 0.00103
2024-12-17 04:30:52.606563: Validation loss did not improve from -0.58994. Patience: 17/50
2024-12-17 04:30:52.607556: train_loss -0.7625
2024-12-17 04:30:52.608840: val_loss -0.5679
2024-12-17 04:30:52.609877: Pseudo dice [0.7552]
2024-12-17 04:30:52.611203: Epoch time: 89.2 s
2024-12-17 04:30:52.612170: Yayy! New best EMA pseudo Dice: 0.7547
2024-12-17 04:30:54.235671: 
2024-12-17 04:30:54.236756: Epoch 139
2024-12-17 04:30:54.237533: Current learning rate: 0.00095
2024-12-17 04:32:23.444232: Validation loss did not improve from -0.58994. Patience: 18/50
2024-12-17 04:32:23.445544: train_loss -0.7629
2024-12-17 04:32:23.446354: val_loss -0.5542
2024-12-17 04:32:23.447162: Pseudo dice [0.7601]
2024-12-17 04:32:23.447855: Epoch time: 89.21 s
2024-12-17 04:32:23.808071: Yayy! New best EMA pseudo Dice: 0.7552
2024-12-17 04:32:25.403922: 
2024-12-17 04:32:25.405381: Epoch 140
2024-12-17 04:32:25.406078: Current learning rate: 0.00087
2024-12-17 04:33:54.610547: Validation loss did not improve from -0.58994. Patience: 19/50
2024-12-17 04:33:54.611576: train_loss -0.7631
2024-12-17 04:33:54.612530: val_loss -0.5536
2024-12-17 04:33:54.613234: Pseudo dice [0.7445]
2024-12-17 04:33:54.613915: Epoch time: 89.21 s
2024-12-17 04:33:55.874894: 
2024-12-17 04:33:55.877119: Epoch 141
2024-12-17 04:33:55.877877: Current learning rate: 0.00079
2024-12-17 04:35:25.265023: Validation loss did not improve from -0.58994. Patience: 20/50
2024-12-17 04:35:25.266272: train_loss -0.7636
2024-12-17 04:35:25.267382: val_loss -0.5294
2024-12-17 04:35:25.268207: Pseudo dice [0.7357]
2024-12-17 04:35:25.268982: Epoch time: 89.39 s
2024-12-17 04:35:26.534879: 
2024-12-17 04:35:26.537149: Epoch 142
2024-12-17 04:35:26.538158: Current learning rate: 0.00071
2024-12-17 04:36:56.006000: Validation loss did not improve from -0.58994. Patience: 21/50
2024-12-17 04:36:56.007263: train_loss -0.7638
2024-12-17 04:36:56.008534: val_loss -0.5877
2024-12-17 04:36:56.009352: Pseudo dice [0.7704]
2024-12-17 04:36:56.010084: Epoch time: 89.47 s
2024-12-17 04:36:57.295356: 
2024-12-17 04:36:57.297067: Epoch 143
2024-12-17 04:36:57.297962: Current learning rate: 0.00063
2024-12-17 04:38:26.842486: Validation loss did not improve from -0.58994. Patience: 22/50
2024-12-17 04:38:26.843465: train_loss -0.7641
2024-12-17 04:38:26.844605: val_loss -0.5564
2024-12-17 04:38:26.845721: Pseudo dice [0.7593]
2024-12-17 04:38:26.846580: Epoch time: 89.55 s
2024-12-17 04:38:28.114879: 
2024-12-17 04:38:28.116509: Epoch 144
2024-12-17 04:38:28.117284: Current learning rate: 0.00055
2024-12-17 04:39:57.556537: Validation loss did not improve from -0.58994. Patience: 23/50
2024-12-17 04:39:57.557417: train_loss -0.7629
2024-12-17 04:39:57.558339: val_loss -0.5435
2024-12-17 04:39:57.559153: Pseudo dice [0.7415]
2024-12-17 04:39:57.559819: Epoch time: 89.44 s
2024-12-17 04:39:59.169343: 
2024-12-17 04:39:59.170881: Epoch 145
2024-12-17 04:39:59.171624: Current learning rate: 0.00047
2024-12-17 04:41:28.495525: Validation loss did not improve from -0.58994. Patience: 24/50
2024-12-17 04:41:28.496512: train_loss -0.7685
2024-12-17 04:41:28.497474: val_loss -0.5564
2024-12-17 04:41:28.498324: Pseudo dice [0.7501]
2024-12-17 04:41:28.499197: Epoch time: 89.33 s
2024-12-17 04:41:30.162738: 
2024-12-17 04:41:30.165164: Epoch 146
2024-12-17 04:41:30.166158: Current learning rate: 0.00038
2024-12-17 04:42:59.471009: Validation loss did not improve from -0.58994. Patience: 25/50
2024-12-17 04:42:59.472249: train_loss -0.7657
2024-12-17 04:42:59.473166: val_loss -0.5419
2024-12-17 04:42:59.473822: Pseudo dice [0.7464]
2024-12-17 04:42:59.474605: Epoch time: 89.31 s
2024-12-17 04:43:00.792922: 
2024-12-17 04:43:00.794481: Epoch 147
2024-12-17 04:43:00.795423: Current learning rate: 0.0003
2024-12-17 04:44:30.115033: Validation loss did not improve from -0.58994. Patience: 26/50
2024-12-17 04:44:30.116258: train_loss -0.766
2024-12-17 04:44:30.117445: val_loss -0.5397
2024-12-17 04:44:30.118298: Pseudo dice [0.742]
2024-12-17 04:44:30.119296: Epoch time: 89.32 s
2024-12-17 04:44:31.383781: 
2024-12-17 04:44:31.385472: Epoch 148
2024-12-17 04:44:31.386515: Current learning rate: 0.00021
2024-12-17 04:46:00.794811: Validation loss did not improve from -0.58994. Patience: 27/50
2024-12-17 04:46:00.795618: train_loss -0.7667
2024-12-17 04:46:00.796599: val_loss -0.5466
2024-12-17 04:46:00.797483: Pseudo dice [0.7467]
2024-12-17 04:46:00.798309: Epoch time: 89.41 s
2024-12-17 04:46:02.054269: 
2024-12-17 04:46:02.056125: Epoch 149
2024-12-17 04:46:02.056988: Current learning rate: 0.00011
2024-12-17 04:47:31.647818: Validation loss did not improve from -0.58994. Patience: 28/50
2024-12-17 04:47:31.649221: train_loss -0.7659
2024-12-17 04:47:31.650305: val_loss -0.5367
2024-12-17 04:47:31.651131: Pseudo dice [0.743]
2024-12-17 04:47:31.651947: Epoch time: 89.6 s
2024-12-17 04:47:33.422943: Training done.
2024-12-17 00:59:58.555417: unpacking done...
2024-12-17 00:59:58.564957: Unable to plot network architecture: nnUNet_compile is enabled!
2024-12-17 00:59:58.686160: 
2024-12-17 00:59:58.687742: Epoch 0
2024-12-17 00:59:58.689145: Current learning rate: 0.01
2024-12-17 01:02:37.112153: Validation loss improved from 1000.00000 to -0.14415! Patience: 0/50
2024-12-17 01:02:37.113506: train_loss -0.086
2024-12-17 01:02:37.114940: val_loss -0.1441
2024-12-17 01:02:37.116023: Pseudo dice [0.5281]
2024-12-17 01:02:37.117064: Epoch time: 158.43 s
2024-12-17 01:02:37.117983: Yayy! New best EMA pseudo Dice: 0.5281
2024-12-17 01:02:39.397343: 
2024-12-17 01:02:39.399556: Epoch 1
2024-12-17 01:02:39.400780: Current learning rate: 0.00994
2024-12-17 01:04:07.448511: Validation loss improved from -0.14415 to -0.20754! Patience: 0/50
2024-12-17 01:04:07.449651: train_loss -0.2192
2024-12-17 01:04:07.450529: val_loss -0.2075
2024-12-17 01:04:07.451377: Pseudo dice [0.579]
2024-12-17 01:04:07.452204: Epoch time: 88.05 s
2024-12-17 01:04:07.452851: Yayy! New best EMA pseudo Dice: 0.5332
2024-12-17 01:04:09.062173: 
2024-12-17 01:04:09.064055: Epoch 2
2024-12-17 01:04:09.065154: Current learning rate: 0.00988
2024-12-17 01:05:37.870496: Validation loss improved from -0.20754 to -0.25775! Patience: 0/50
2024-12-17 01:05:37.871750: train_loss -0.2692
2024-12-17 01:05:37.872884: val_loss -0.2577
2024-12-17 01:05:37.873769: Pseudo dice [0.6153]
2024-12-17 01:05:37.874637: Epoch time: 88.81 s
2024-12-17 01:05:37.875412: Yayy! New best EMA pseudo Dice: 0.5414
2024-12-17 01:05:39.556538: 
2024-12-17 01:05:39.558026: Epoch 3
2024-12-17 01:05:39.559047: Current learning rate: 0.00982
2024-12-17 01:07:08.565492: Validation loss improved from -0.25775 to -0.26676! Patience: 0/50
2024-12-17 01:07:08.566476: train_loss -0.3089
2024-12-17 01:07:08.567448: val_loss -0.2668
2024-12-17 01:07:08.568436: Pseudo dice [0.6257]
2024-12-17 01:07:08.569293: Epoch time: 89.01 s
2024-12-17 01:07:08.570141: Yayy! New best EMA pseudo Dice: 0.5499
2024-12-17 01:07:10.162170: 
2024-12-17 01:07:10.163749: Epoch 4
2024-12-17 01:07:10.164500: Current learning rate: 0.00976
2024-12-17 01:08:39.217865: Validation loss improved from -0.26676 to -0.30235! Patience: 0/50
2024-12-17 01:08:39.218753: train_loss -0.3271
2024-12-17 01:08:39.219947: val_loss -0.3023
2024-12-17 01:08:39.220901: Pseudo dice [0.6125]
2024-12-17 01:08:39.221876: Epoch time: 89.06 s
2024-12-17 01:08:39.576436: Yayy! New best EMA pseudo Dice: 0.5561
2024-12-17 01:08:41.194096: 
2024-12-17 01:08:41.196193: Epoch 5
2024-12-17 01:08:41.197086: Current learning rate: 0.0097
2024-12-17 01:10:10.392343: Validation loss improved from -0.30235 to -0.34608! Patience: 0/50
2024-12-17 01:10:10.393582: train_loss -0.3449
2024-12-17 01:10:10.395071: val_loss -0.3461
2024-12-17 01:10:10.396192: Pseudo dice [0.6566]
2024-12-17 01:10:10.397367: Epoch time: 89.2 s
2024-12-17 01:10:10.398246: Yayy! New best EMA pseudo Dice: 0.5662
2024-12-17 01:10:11.941432: 
2024-12-17 01:10:11.942789: Epoch 6
2024-12-17 01:10:11.943480: Current learning rate: 0.00964
2024-12-17 01:11:41.096949: Validation loss improved from -0.34608 to -0.36816! Patience: 0/50
2024-12-17 01:11:41.098067: train_loss -0.3801
2024-12-17 01:11:41.099309: val_loss -0.3682
2024-12-17 01:11:41.100550: Pseudo dice [0.6648]
2024-12-17 01:11:41.101607: Epoch time: 89.16 s
2024-12-17 01:11:41.102584: Yayy! New best EMA pseudo Dice: 0.576
2024-12-17 01:11:42.626768: 
2024-12-17 01:11:42.628069: Epoch 7
2024-12-17 01:11:42.628851: Current learning rate: 0.00958
2024-12-17 01:13:11.728252: Validation loss improved from -0.36816 to -0.39835! Patience: 0/50
2024-12-17 01:13:11.728931: train_loss -0.402
2024-12-17 01:13:11.729850: val_loss -0.3984
2024-12-17 01:13:11.730701: Pseudo dice [0.6971]
2024-12-17 01:13:11.731807: Epoch time: 89.1 s
2024-12-17 01:13:11.732755: Yayy! New best EMA pseudo Dice: 0.5881
2024-12-17 01:13:13.685032: 
2024-12-17 01:13:13.686561: Epoch 8
2024-12-17 01:13:13.687822: Current learning rate: 0.00952
2024-12-17 01:14:43.149830: Validation loss improved from -0.39835 to -0.42784! Patience: 0/50
2024-12-17 01:14:43.150954: train_loss -0.4367
2024-12-17 01:14:43.152009: val_loss -0.4278
2024-12-17 01:14:43.152831: Pseudo dice [0.7102]
2024-12-17 01:14:43.153671: Epoch time: 89.47 s
2024-12-17 01:14:43.154558: Yayy! New best EMA pseudo Dice: 0.6003
2024-12-17 01:14:44.838468: 
2024-12-17 01:14:44.839684: Epoch 9
2024-12-17 01:14:44.840419: Current learning rate: 0.00946
2024-12-17 01:16:14.308731: Validation loss did not improve from -0.42784. Patience: 1/50
2024-12-17 01:16:14.309794: train_loss -0.4399
2024-12-17 01:16:14.310742: val_loss -0.29
2024-12-17 01:16:14.311471: Pseudo dice [0.6136]
2024-12-17 01:16:14.312243: Epoch time: 89.47 s
2024-12-17 01:16:14.644417: Yayy! New best EMA pseudo Dice: 0.6017
2024-12-17 01:16:16.180479: 
2024-12-17 01:16:16.182045: Epoch 10
2024-12-17 01:16:16.182893: Current learning rate: 0.0094
2024-12-17 01:17:45.690477: Validation loss did not improve from -0.42784. Patience: 2/50
2024-12-17 01:17:45.691769: train_loss -0.4377
2024-12-17 01:17:45.692657: val_loss -0.3585
2024-12-17 01:17:45.693336: Pseudo dice [0.6739]
2024-12-17 01:17:45.694113: Epoch time: 89.51 s
2024-12-17 01:17:45.694896: Yayy! New best EMA pseudo Dice: 0.6089
2024-12-17 01:17:47.282512: 
2024-12-17 01:17:47.284037: Epoch 11
2024-12-17 01:17:47.284768: Current learning rate: 0.00934
2024-12-17 01:19:16.718785: Validation loss did not improve from -0.42784. Patience: 3/50
2024-12-17 01:19:16.719786: train_loss -0.4658
2024-12-17 01:19:16.720836: val_loss -0.4135
2024-12-17 01:19:16.721633: Pseudo dice [0.6937]
2024-12-17 01:19:16.722528: Epoch time: 89.44 s
2024-12-17 01:19:16.723373: Yayy! New best EMA pseudo Dice: 0.6174
2024-12-17 01:19:18.281209: 
2024-12-17 01:19:18.282732: Epoch 12
2024-12-17 01:19:18.283632: Current learning rate: 0.00928
2024-12-17 01:20:47.726350: Validation loss improved from -0.42784 to -0.46304! Patience: 3/50
2024-12-17 01:20:47.727526: train_loss -0.4796
2024-12-17 01:20:47.728482: val_loss -0.463
2024-12-17 01:20:47.729215: Pseudo dice [0.7257]
2024-12-17 01:20:47.730062: Epoch time: 89.45 s
2024-12-17 01:20:47.730729: Yayy! New best EMA pseudo Dice: 0.6282
2024-12-17 01:20:49.329776: 
2024-12-17 01:20:49.331256: Epoch 13
2024-12-17 01:20:49.332135: Current learning rate: 0.00922
2024-12-17 01:22:18.735142: Validation loss did not improve from -0.46304. Patience: 1/50
2024-12-17 01:22:18.736369: train_loss -0.4779
2024-12-17 01:22:18.737255: val_loss -0.4581
2024-12-17 01:22:18.738026: Pseudo dice [0.726]
2024-12-17 01:22:18.738830: Epoch time: 89.41 s
2024-12-17 01:22:18.739503: Yayy! New best EMA pseudo Dice: 0.638
2024-12-17 01:22:20.338068: 
2024-12-17 01:22:20.339768: Epoch 14
2024-12-17 01:22:20.340534: Current learning rate: 0.00916
2024-12-17 01:23:49.668297: Validation loss did not improve from -0.46304. Patience: 2/50
2024-12-17 01:23:49.669385: train_loss -0.5069
2024-12-17 01:23:49.670572: val_loss -0.447
2024-12-17 01:23:49.671688: Pseudo dice [0.7144]
2024-12-17 01:23:49.672471: Epoch time: 89.33 s
2024-12-17 01:23:50.048090: Yayy! New best EMA pseudo Dice: 0.6456
2024-12-17 01:23:51.678657: 
2024-12-17 01:23:51.679651: Epoch 15
2024-12-17 01:23:51.680401: Current learning rate: 0.0091
2024-12-17 01:25:21.005656: Validation loss did not improve from -0.46304. Patience: 3/50
2024-12-17 01:25:21.006537: train_loss -0.5175
2024-12-17 01:25:21.007540: val_loss -0.4024
2024-12-17 01:25:21.008628: Pseudo dice [0.7047]
2024-12-17 01:25:21.009711: Epoch time: 89.33 s
2024-12-17 01:25:21.010569: Yayy! New best EMA pseudo Dice: 0.6515
2024-12-17 01:25:22.576044: 
2024-12-17 01:25:22.577912: Epoch 16
2024-12-17 01:25:22.579036: Current learning rate: 0.00903
2024-12-17 01:26:52.209000: Validation loss did not improve from -0.46304. Patience: 4/50
2024-12-17 01:26:52.210034: train_loss -0.5055
2024-12-17 01:26:52.210815: val_loss -0.4333
2024-12-17 01:26:52.211466: Pseudo dice [0.6929]
2024-12-17 01:26:52.212229: Epoch time: 89.63 s
2024-12-17 01:26:52.212814: Yayy! New best EMA pseudo Dice: 0.6557
2024-12-17 01:26:53.828082: 
2024-12-17 01:26:53.829189: Epoch 17
2024-12-17 01:26:53.829978: Current learning rate: 0.00897
2024-12-17 01:28:23.468158: Validation loss improved from -0.46304 to -0.47119! Patience: 4/50
2024-12-17 01:28:23.469255: train_loss -0.5155
2024-12-17 01:28:23.470109: val_loss -0.4712
2024-12-17 01:28:23.470975: Pseudo dice [0.7292]
2024-12-17 01:28:23.471909: Epoch time: 89.64 s
2024-12-17 01:28:23.472666: Yayy! New best EMA pseudo Dice: 0.663
2024-12-17 01:28:25.090857: 
2024-12-17 01:28:25.092426: Epoch 18
2024-12-17 01:28:25.093460: Current learning rate: 0.00891
2024-12-17 01:29:54.733802: Validation loss did not improve from -0.47119. Patience: 1/50
2024-12-17 01:29:54.734895: train_loss -0.5452
2024-12-17 01:29:54.735761: val_loss -0.4645
2024-12-17 01:29:54.736528: Pseudo dice [0.73]
2024-12-17 01:29:54.737160: Epoch time: 89.64 s
2024-12-17 01:29:54.738044: Yayy! New best EMA pseudo Dice: 0.6697
2024-12-17 01:29:56.883153: 
2024-12-17 01:29:56.884361: Epoch 19
2024-12-17 01:29:56.885300: Current learning rate: 0.00885
2024-12-17 01:31:26.482952: Validation loss did not improve from -0.47119. Patience: 2/50
2024-12-17 01:31:26.483855: train_loss -0.5333
2024-12-17 01:31:26.484745: val_loss -0.4647
2024-12-17 01:31:26.485470: Pseudo dice [0.7257]
2024-12-17 01:31:26.486248: Epoch time: 89.6 s
2024-12-17 01:31:26.819167: Yayy! New best EMA pseudo Dice: 0.6753
2024-12-17 01:31:28.463415: 
2024-12-17 01:31:28.465345: Epoch 20
2024-12-17 01:31:28.466307: Current learning rate: 0.00879
2024-12-17 01:32:58.166793: Validation loss improved from -0.47119 to -0.49121! Patience: 2/50
2024-12-17 01:32:58.167842: train_loss -0.5413
2024-12-17 01:32:58.169150: val_loss -0.4912
2024-12-17 01:32:58.170200: Pseudo dice [0.7344]
2024-12-17 01:32:58.171279: Epoch time: 89.71 s
2024-12-17 01:32:58.172156: Yayy! New best EMA pseudo Dice: 0.6812
2024-12-17 01:32:59.814180: 
2024-12-17 01:32:59.815891: Epoch 21
2024-12-17 01:32:59.817007: Current learning rate: 0.00873
2024-12-17 01:34:29.456535: Validation loss did not improve from -0.49121. Patience: 1/50
2024-12-17 01:34:29.457733: train_loss -0.5365
2024-12-17 01:34:29.458626: val_loss -0.4716
2024-12-17 01:34:29.459358: Pseudo dice [0.7338]
2024-12-17 01:34:29.460039: Epoch time: 89.64 s
2024-12-17 01:34:29.460890: Yayy! New best EMA pseudo Dice: 0.6865
2024-12-17 01:34:31.029078: 
2024-12-17 01:34:31.030713: Epoch 22
2024-12-17 01:34:31.031516: Current learning rate: 0.00867
2024-12-17 01:36:00.638437: Validation loss did not improve from -0.49121. Patience: 2/50
2024-12-17 01:36:00.639314: train_loss -0.5575
2024-12-17 01:36:00.640211: val_loss -0.3966
2024-12-17 01:36:00.641139: Pseudo dice [0.7005]
2024-12-17 01:36:00.642055: Epoch time: 89.61 s
2024-12-17 01:36:00.642842: Yayy! New best EMA pseudo Dice: 0.6879
2024-12-17 01:36:02.210386: 
2024-12-17 01:36:02.212252: Epoch 23
2024-12-17 01:36:02.213212: Current learning rate: 0.00861
2024-12-17 01:37:31.434797: Validation loss improved from -0.49121 to -0.50159! Patience: 2/50
2024-12-17 01:37:31.435533: train_loss -0.5456
2024-12-17 01:37:31.436356: val_loss -0.5016
2024-12-17 01:37:31.437108: Pseudo dice [0.7464]
2024-12-17 01:37:31.437892: Epoch time: 89.23 s
2024-12-17 01:37:31.438728: Yayy! New best EMA pseudo Dice: 0.6937
2024-12-17 01:37:32.975268: 
2024-12-17 01:37:32.976861: Epoch 24
2024-12-17 01:37:32.977749: Current learning rate: 0.00855
2024-12-17 01:39:02.184754: Validation loss did not improve from -0.50159. Patience: 1/50
2024-12-17 01:39:02.185959: train_loss -0.5549
2024-12-17 01:39:02.186960: val_loss -0.4999
2024-12-17 01:39:02.187654: Pseudo dice [0.7545]
2024-12-17 01:39:02.188311: Epoch time: 89.21 s
2024-12-17 01:39:02.531356: Yayy! New best EMA pseudo Dice: 0.6998
2024-12-17 01:39:04.084869: 
2024-12-17 01:39:04.086459: Epoch 25
2024-12-17 01:39:04.087346: Current learning rate: 0.00849
2024-12-17 01:40:33.206335: Validation loss did not improve from -0.50159. Patience: 2/50
2024-12-17 01:40:33.207247: train_loss -0.5644
2024-12-17 01:40:33.208334: val_loss -0.4937
2024-12-17 01:40:33.209318: Pseudo dice [0.7403]
2024-12-17 01:40:33.210105: Epoch time: 89.12 s
2024-12-17 01:40:33.210989: Yayy! New best EMA pseudo Dice: 0.7038
2024-12-17 01:40:34.775628: 
2024-12-17 01:40:34.777067: Epoch 26
2024-12-17 01:40:34.778106: Current learning rate: 0.00843
2024-12-17 01:42:03.985128: Validation loss improved from -0.50159 to -0.50291! Patience: 2/50
2024-12-17 01:42:03.985922: train_loss -0.5712
2024-12-17 01:42:03.987110: val_loss -0.5029
2024-12-17 01:42:03.987999: Pseudo dice [0.7549]
2024-12-17 01:42:03.988869: Epoch time: 89.21 s
2024-12-17 01:42:03.989641: Yayy! New best EMA pseudo Dice: 0.709
2024-12-17 01:42:05.565857: 
2024-12-17 01:42:05.567587: Epoch 27
2024-12-17 01:42:05.568664: Current learning rate: 0.00836
2024-12-17 01:43:34.826663: Validation loss did not improve from -0.50291. Patience: 1/50
2024-12-17 01:43:34.827845: train_loss -0.5557
2024-12-17 01:43:34.828593: val_loss -0.4668
2024-12-17 01:43:34.829266: Pseudo dice [0.7198]
2024-12-17 01:43:34.830042: Epoch time: 89.26 s
2024-12-17 01:43:34.830672: Yayy! New best EMA pseudo Dice: 0.71
2024-12-17 01:43:36.394520: 
2024-12-17 01:43:36.396203: Epoch 28
2024-12-17 01:43:36.397029: Current learning rate: 0.0083
2024-12-17 01:45:05.622432: Validation loss improved from -0.50291 to -0.51413! Patience: 1/50
2024-12-17 01:45:05.623327: train_loss -0.5809
2024-12-17 01:45:05.624237: val_loss -0.5141
2024-12-17 01:45:05.624964: Pseudo dice [0.7575]
2024-12-17 01:45:05.625665: Epoch time: 89.23 s
2024-12-17 01:45:05.626373: Yayy! New best EMA pseudo Dice: 0.7148
2024-12-17 01:45:07.524236: 
2024-12-17 01:45:07.525775: Epoch 29
2024-12-17 01:45:07.526629: Current learning rate: 0.00824
2024-12-17 01:46:36.635579: Validation loss did not improve from -0.51413. Patience: 1/50
2024-12-17 01:46:36.636523: train_loss -0.5723
2024-12-17 01:46:36.637440: val_loss -0.4913
2024-12-17 01:46:36.638322: Pseudo dice [0.7466]
2024-12-17 01:46:36.639172: Epoch time: 89.11 s
2024-12-17 01:46:36.977647: Yayy! New best EMA pseudo Dice: 0.718
2024-12-17 01:46:38.541989: 
2024-12-17 01:46:38.543405: Epoch 30
2024-12-17 01:46:38.544175: Current learning rate: 0.00818
2024-12-17 01:48:07.692587: Validation loss improved from -0.51413 to -0.51458! Patience: 1/50
2024-12-17 01:48:07.693661: train_loss -0.5863
2024-12-17 01:48:07.694382: val_loss -0.5146
2024-12-17 01:48:07.695031: Pseudo dice [0.7559]
2024-12-17 01:48:07.695868: Epoch time: 89.15 s
2024-12-17 01:48:07.696686: Yayy! New best EMA pseudo Dice: 0.7218
2024-12-17 01:48:09.306490: 
2024-12-17 01:48:09.308060: Epoch 31
2024-12-17 01:48:09.308804: Current learning rate: 0.00812
2024-12-17 01:49:38.438517: Validation loss did not improve from -0.51458. Patience: 1/50
2024-12-17 01:49:38.439621: train_loss -0.6009
2024-12-17 01:49:38.440541: val_loss -0.4547
2024-12-17 01:49:38.441340: Pseudo dice [0.7079]
2024-12-17 01:49:38.442094: Epoch time: 89.13 s
2024-12-17 01:49:39.747479: 
2024-12-17 01:49:39.749387: Epoch 32
2024-12-17 01:49:39.750185: Current learning rate: 0.00806
2024-12-17 01:51:08.898990: Validation loss improved from -0.51458 to -0.52664! Patience: 1/50
2024-12-17 01:51:08.899770: train_loss -0.5814
2024-12-17 01:51:08.900605: val_loss -0.5266
2024-12-17 01:51:08.901562: Pseudo dice [0.7654]
2024-12-17 01:51:08.902386: Epoch time: 89.15 s
2024-12-17 01:51:08.903127: Yayy! New best EMA pseudo Dice: 0.7249
2024-12-17 01:51:10.498148: 
2024-12-17 01:51:10.499503: Epoch 33
2024-12-17 01:51:10.500242: Current learning rate: 0.008
2024-12-17 01:52:39.676882: Validation loss did not improve from -0.52664. Patience: 1/50
2024-12-17 01:52:39.677903: train_loss -0.601
2024-12-17 01:52:39.678833: val_loss -0.4758
2024-12-17 01:52:39.679609: Pseudo dice [0.7257]
2024-12-17 01:52:39.680235: Epoch time: 89.18 s
2024-12-17 01:52:39.680923: Yayy! New best EMA pseudo Dice: 0.725
2024-12-17 01:52:41.266738: 
2024-12-17 01:52:41.268473: Epoch 34
2024-12-17 01:52:41.269384: Current learning rate: 0.00793
2024-12-17 01:54:10.495250: Validation loss did not improve from -0.52664. Patience: 2/50
2024-12-17 01:54:10.496345: train_loss -0.5856
2024-12-17 01:54:10.497617: val_loss -0.4404
2024-12-17 01:54:10.498500: Pseudo dice [0.7215]
2024-12-17 01:54:10.499490: Epoch time: 89.23 s
2024-12-17 01:54:12.114663: 
2024-12-17 01:54:12.116417: Epoch 35
2024-12-17 01:54:12.117523: Current learning rate: 0.00787
2024-12-17 01:55:41.282411: Validation loss improved from -0.52664 to -0.52971! Patience: 2/50
2024-12-17 01:55:41.283680: train_loss -0.6072
2024-12-17 01:55:41.284739: val_loss -0.5297
2024-12-17 01:55:41.285565: Pseudo dice [0.7678]
2024-12-17 01:55:41.286426: Epoch time: 89.17 s
2024-12-17 01:55:41.287188: Yayy! New best EMA pseudo Dice: 0.7289
2024-12-17 01:55:42.940512: 
2024-12-17 01:55:42.941936: Epoch 36
2024-12-17 01:55:42.942665: Current learning rate: 0.00781
2024-12-17 01:57:12.120490: Validation loss did not improve from -0.52971. Patience: 1/50
2024-12-17 01:57:12.121470: train_loss -0.5835
2024-12-17 01:57:12.122417: val_loss -0.5058
2024-12-17 01:57:12.123228: Pseudo dice [0.7537]
2024-12-17 01:57:12.123970: Epoch time: 89.18 s
2024-12-17 01:57:12.124736: Yayy! New best EMA pseudo Dice: 0.7314
2024-12-17 01:57:13.742008: 
2024-12-17 01:57:13.743742: Epoch 37
2024-12-17 01:57:13.744580: Current learning rate: 0.00775
2024-12-17 01:58:42.978630: Validation loss did not improve from -0.52971. Patience: 2/50
2024-12-17 01:58:42.979708: train_loss -0.6109
2024-12-17 01:58:42.980606: val_loss -0.5263
2024-12-17 01:58:42.981478: Pseudo dice [0.7594]
2024-12-17 01:58:42.982337: Epoch time: 89.24 s
2024-12-17 01:58:42.983038: Yayy! New best EMA pseudo Dice: 0.7342
2024-12-17 01:58:44.587090: 
2024-12-17 01:58:44.588727: Epoch 38
2024-12-17 01:58:44.589662: Current learning rate: 0.00769
2024-12-17 02:00:13.672999: Validation loss did not improve from -0.52971. Patience: 3/50
2024-12-17 02:00:13.674108: train_loss -0.62
2024-12-17 02:00:13.675068: val_loss -0.5093
2024-12-17 02:00:13.675748: Pseudo dice [0.7602]
2024-12-17 02:00:13.676515: Epoch time: 89.09 s
2024-12-17 02:00:13.677338: Yayy! New best EMA pseudo Dice: 0.7368
2024-12-17 02:00:15.310946: 
2024-12-17 02:00:15.312480: Epoch 39
2024-12-17 02:00:15.313223: Current learning rate: 0.00763
2024-12-17 02:01:44.705766: Validation loss did not improve from -0.52971. Patience: 4/50
2024-12-17 02:01:44.706926: train_loss -0.6292
2024-12-17 02:01:44.708063: val_loss -0.5228
2024-12-17 02:01:44.708948: Pseudo dice [0.7639]
2024-12-17 02:01:44.709785: Epoch time: 89.4 s
2024-12-17 02:01:45.061389: Yayy! New best EMA pseudo Dice: 0.7395
2024-12-17 02:01:47.088474: 
2024-12-17 02:01:47.090090: Epoch 40
2024-12-17 02:01:47.090785: Current learning rate: 0.00756
2024-12-17 02:03:16.490861: Validation loss improved from -0.52971 to -0.54528! Patience: 4/50
2024-12-17 02:03:16.491950: train_loss -0.6212
2024-12-17 02:03:16.492767: val_loss -0.5453
2024-12-17 02:03:16.493525: Pseudo dice [0.7752]
2024-12-17 02:03:16.494215: Epoch time: 89.4 s
2024-12-17 02:03:16.494858: Yayy! New best EMA pseudo Dice: 0.7431
2024-12-17 02:03:18.205369: 
2024-12-17 02:03:18.207122: Epoch 41
2024-12-17 02:03:18.208005: Current learning rate: 0.0075
2024-12-17 02:04:47.676494: Validation loss did not improve from -0.54528. Patience: 1/50
2024-12-17 02:04:47.678030: train_loss -0.6244
2024-12-17 02:04:47.678889: val_loss -0.4497
2024-12-17 02:04:47.679602: Pseudo dice [0.7234]
2024-12-17 02:04:47.680310: Epoch time: 89.47 s
2024-12-17 02:04:48.869429: 
2024-12-17 02:04:48.870632: Epoch 42
2024-12-17 02:04:48.871470: Current learning rate: 0.00744
2024-12-17 02:06:18.530251: Validation loss did not improve from -0.54528. Patience: 2/50
2024-12-17 02:06:18.531282: train_loss -0.6191
2024-12-17 02:06:18.532444: val_loss -0.5226
2024-12-17 02:06:18.533238: Pseudo dice [0.7597]
2024-12-17 02:06:18.534017: Epoch time: 89.66 s
2024-12-17 02:06:19.789440: 
2024-12-17 02:06:19.791014: Epoch 43
2024-12-17 02:06:19.791857: Current learning rate: 0.00738
2024-12-17 02:07:49.373988: Validation loss did not improve from -0.54528. Patience: 3/50
2024-12-17 02:07:49.374783: train_loss -0.6233
2024-12-17 02:07:49.375502: val_loss -0.4984
2024-12-17 02:07:49.376176: Pseudo dice [0.753]
2024-12-17 02:07:49.377047: Epoch time: 89.59 s
2024-12-17 02:07:49.378090: Yayy! New best EMA pseudo Dice: 0.744
2024-12-17 02:07:50.911036: 
2024-12-17 02:07:50.912813: Epoch 44
2024-12-17 02:07:50.913576: Current learning rate: 0.00732
2024-12-17 02:09:20.342420: Validation loss did not improve from -0.54528. Patience: 4/50
2024-12-17 02:09:20.343600: train_loss -0.6281
2024-12-17 02:09:20.344587: val_loss -0.5069
2024-12-17 02:09:20.345364: Pseudo dice [0.7569]
2024-12-17 02:09:20.346155: Epoch time: 89.43 s
2024-12-17 02:09:20.705979: Yayy! New best EMA pseudo Dice: 0.7453
2024-12-17 02:09:22.226154: 
2024-12-17 02:09:22.227607: Epoch 45
2024-12-17 02:09:22.228376: Current learning rate: 0.00725
2024-12-17 02:10:51.694533: Validation loss did not improve from -0.54528. Patience: 5/50
2024-12-17 02:10:51.695834: train_loss -0.6277
2024-12-17 02:10:51.696866: val_loss -0.4647
2024-12-17 02:10:51.697681: Pseudo dice [0.7203]
2024-12-17 02:10:51.698462: Epoch time: 89.47 s
2024-12-17 02:10:52.882004: 
2024-12-17 02:10:52.883851: Epoch 46
2024-12-17 02:10:52.884596: Current learning rate: 0.00719
2024-12-17 02:12:22.375620: Validation loss did not improve from -0.54528. Patience: 6/50
2024-12-17 02:12:22.376832: train_loss -0.6331
2024-12-17 02:12:22.377838: val_loss -0.4758
2024-12-17 02:12:22.378540: Pseudo dice [0.7322]
2024-12-17 02:12:22.379252: Epoch time: 89.5 s
2024-12-17 02:12:23.541558: 
2024-12-17 02:12:23.543301: Epoch 47
2024-12-17 02:12:23.544237: Current learning rate: 0.00713
2024-12-17 02:13:53.274236: Validation loss did not improve from -0.54528. Patience: 7/50
2024-12-17 02:13:53.275337: train_loss -0.6371
2024-12-17 02:13:53.276343: val_loss -0.5065
2024-12-17 02:13:53.277184: Pseudo dice [0.7472]
2024-12-17 02:13:53.277900: Epoch time: 89.73 s
2024-12-17 02:13:54.458364: 
2024-12-17 02:13:54.460062: Epoch 48
2024-12-17 02:13:54.460865: Current learning rate: 0.00707
2024-12-17 02:15:24.253795: Validation loss did not improve from -0.54528. Patience: 8/50
2024-12-17 02:15:24.255174: train_loss -0.6437
2024-12-17 02:15:24.256334: val_loss -0.4923
2024-12-17 02:15:24.257303: Pseudo dice [0.747]
2024-12-17 02:15:24.258172: Epoch time: 89.8 s
2024-12-17 02:15:25.525311: 
2024-12-17 02:15:25.527137: Epoch 49
2024-12-17 02:15:25.528052: Current learning rate: 0.007
2024-12-17 02:16:55.236928: Validation loss did not improve from -0.54528. Patience: 9/50
2024-12-17 02:16:55.238238: train_loss -0.6409
2024-12-17 02:16:55.239194: val_loss -0.5228
2024-12-17 02:16:55.239883: Pseudo dice [0.7708]
2024-12-17 02:16:55.240535: Epoch time: 89.71 s
2024-12-17 02:16:55.628938: Yayy! New best EMA pseudo Dice: 0.7455
2024-12-17 02:16:57.190662: 
2024-12-17 02:16:57.191815: Epoch 50
2024-12-17 02:16:57.192560: Current learning rate: 0.00694
2024-12-17 02:18:26.885022: Validation loss did not improve from -0.54528. Patience: 10/50
2024-12-17 02:18:26.886158: train_loss -0.6494
2024-12-17 02:18:26.887100: val_loss -0.4876
2024-12-17 02:18:26.887867: Pseudo dice [0.7416]
2024-12-17 02:18:26.888611: Epoch time: 89.7 s
2024-12-17 02:18:28.434645: 
2024-12-17 02:18:28.436157: Epoch 51
2024-12-17 02:18:28.437008: Current learning rate: 0.00688
2024-12-17 02:19:58.205586: Validation loss did not improve from -0.54528. Patience: 11/50
2024-12-17 02:19:58.206839: train_loss -0.6515
2024-12-17 02:19:58.207749: val_loss -0.5144
2024-12-17 02:19:58.208546: Pseudo dice [0.7581]
2024-12-17 02:19:58.209282: Epoch time: 89.77 s
2024-12-17 02:19:58.209983: Yayy! New best EMA pseudo Dice: 0.7464
2024-12-17 02:19:59.798198: 
2024-12-17 02:19:59.799613: Epoch 52
2024-12-17 02:19:59.800420: Current learning rate: 0.00682
2024-12-17 02:21:29.594362: Validation loss did not improve from -0.54528. Patience: 12/50
2024-12-17 02:21:29.595430: train_loss -0.648
2024-12-17 02:21:29.596401: val_loss -0.4559
2024-12-17 02:21:29.597245: Pseudo dice [0.7302]
2024-12-17 02:21:29.598082: Epoch time: 89.8 s
2024-12-17 02:21:30.788741: 
2024-12-17 02:21:30.790417: Epoch 53
2024-12-17 02:21:30.791342: Current learning rate: 0.00675
2024-12-17 02:23:00.546203: Validation loss did not improve from -0.54528. Patience: 13/50
2024-12-17 02:23:00.547276: train_loss -0.648
2024-12-17 02:23:00.548054: val_loss -0.477
2024-12-17 02:23:00.548944: Pseudo dice [0.7263]
2024-12-17 02:23:00.549795: Epoch time: 89.76 s
2024-12-17 02:23:01.750578: 
2024-12-17 02:23:01.751915: Epoch 54
2024-12-17 02:23:01.752688: Current learning rate: 0.00669
2024-12-17 02:24:31.512255: Validation loss did not improve from -0.54528. Patience: 14/50
2024-12-17 02:24:31.513297: train_loss -0.6593
2024-12-17 02:24:31.514297: val_loss -0.506
2024-12-17 02:24:31.515135: Pseudo dice [0.754]
2024-12-17 02:24:31.516099: Epoch time: 89.76 s
2024-12-17 02:24:33.099407: 
2024-12-17 02:24:33.100537: Epoch 55
2024-12-17 02:24:33.101314: Current learning rate: 0.00663
2024-12-17 02:26:02.940285: Validation loss did not improve from -0.54528. Patience: 15/50
2024-12-17 02:26:02.941277: train_loss -0.653
2024-12-17 02:26:02.942239: val_loss -0.4711
2024-12-17 02:26:02.943045: Pseudo dice [0.7341]
2024-12-17 02:26:02.943823: Epoch time: 89.84 s
2024-12-17 02:26:04.138237: 
2024-12-17 02:26:04.139917: Epoch 56
2024-12-17 02:26:04.140638: Current learning rate: 0.00657
2024-12-17 02:27:33.775166: Validation loss did not improve from -0.54528. Patience: 16/50
2024-12-17 02:27:33.776185: train_loss -0.65
2024-12-17 02:27:33.777149: val_loss -0.4931
2024-12-17 02:27:33.778080: Pseudo dice [0.7503]
2024-12-17 02:27:33.779143: Epoch time: 89.64 s
2024-12-17 02:27:34.990443: 
2024-12-17 02:27:34.991778: Epoch 57
2024-12-17 02:27:34.992657: Current learning rate: 0.0065
2024-12-17 02:29:04.517050: Validation loss did not improve from -0.54528. Patience: 17/50
2024-12-17 02:29:04.518409: train_loss -0.6508
2024-12-17 02:29:04.519499: val_loss -0.5126
2024-12-17 02:29:04.520267: Pseudo dice [0.7506]
2024-12-17 02:29:04.521065: Epoch time: 89.53 s
2024-12-17 02:29:05.710881: 
2024-12-17 02:29:05.713073: Epoch 58
2024-12-17 02:29:05.713875: Current learning rate: 0.00644
2024-12-17 02:30:35.291094: Validation loss did not improve from -0.54528. Patience: 18/50
2024-12-17 02:30:35.291997: train_loss -0.6506
2024-12-17 02:30:35.292865: val_loss -0.4825
2024-12-17 02:30:35.293689: Pseudo dice [0.7412]
2024-12-17 02:30:35.294456: Epoch time: 89.58 s
2024-12-17 02:30:36.537426: 
2024-12-17 02:30:36.539069: Epoch 59
2024-12-17 02:30:36.539892: Current learning rate: 0.00638
2024-12-17 02:32:06.093673: Validation loss did not improve from -0.54528. Patience: 19/50
2024-12-17 02:32:06.094846: train_loss -0.6572
2024-12-17 02:32:06.095900: val_loss -0.5285
2024-12-17 02:32:06.096823: Pseudo dice [0.7693]
2024-12-17 02:32:06.097918: Epoch time: 89.56 s
2024-12-17 02:32:06.464901: Yayy! New best EMA pseudo Dice: 0.7467
2024-12-17 02:32:08.084692: 
2024-12-17 02:32:08.085927: Epoch 60
2024-12-17 02:32:08.086685: Current learning rate: 0.00631
2024-12-17 02:33:37.468282: Validation loss did not improve from -0.54528. Patience: 20/50
2024-12-17 02:33:37.469324: train_loss -0.6681
2024-12-17 02:33:37.470213: val_loss -0.4904
2024-12-17 02:33:37.470969: Pseudo dice [0.7511]
2024-12-17 02:33:37.471848: Epoch time: 89.39 s
2024-12-17 02:33:37.472550: Yayy! New best EMA pseudo Dice: 0.7471
2024-12-17 02:33:39.048953: 
2024-12-17 02:33:39.049918: Epoch 61
2024-12-17 02:33:39.050627: Current learning rate: 0.00625
2024-12-17 02:35:08.490459: Validation loss did not improve from -0.54528. Patience: 21/50
2024-12-17 02:35:08.491534: train_loss -0.6634
2024-12-17 02:35:08.492422: val_loss -0.4935
2024-12-17 02:35:08.493158: Pseudo dice [0.7546]
2024-12-17 02:35:08.493948: Epoch time: 89.44 s
2024-12-17 02:35:08.494790: Yayy! New best EMA pseudo Dice: 0.7479
2024-12-17 02:35:10.395826: 
2024-12-17 02:35:10.397478: Epoch 62
2024-12-17 02:35:10.398381: Current learning rate: 0.00619
2024-12-17 02:36:39.856604: Validation loss did not improve from -0.54528. Patience: 22/50
2024-12-17 02:36:39.857687: train_loss -0.6731
2024-12-17 02:36:39.858832: val_loss -0.4801
2024-12-17 02:36:39.859540: Pseudo dice [0.7377]
2024-12-17 02:36:39.860239: Epoch time: 89.46 s
2024-12-17 02:36:41.121006: 
2024-12-17 02:36:41.122662: Epoch 63
2024-12-17 02:36:41.123450: Current learning rate: 0.00612
2024-12-17 02:38:10.576471: Validation loss did not improve from -0.54528. Patience: 23/50
2024-12-17 02:38:10.577658: train_loss -0.6687
2024-12-17 02:38:10.578651: val_loss -0.4567
2024-12-17 02:38:10.579520: Pseudo dice [0.7167]
2024-12-17 02:38:10.580225: Epoch time: 89.46 s
2024-12-17 02:38:11.858087: 
2024-12-17 02:38:11.859500: Epoch 64
2024-12-17 02:38:11.860250: Current learning rate: 0.00606
2024-12-17 02:39:41.468730: Validation loss did not improve from -0.54528. Patience: 24/50
2024-12-17 02:39:41.469915: train_loss -0.673
2024-12-17 02:39:41.470795: val_loss -0.5353
2024-12-17 02:39:41.471588: Pseudo dice [0.7643]
2024-12-17 02:39:41.472542: Epoch time: 89.61 s
2024-12-17 02:39:43.070327: 
2024-12-17 02:39:43.071914: Epoch 65
2024-12-17 02:39:43.072698: Current learning rate: 0.006
2024-12-17 02:41:12.801029: Validation loss did not improve from -0.54528. Patience: 25/50
2024-12-17 02:41:12.802260: train_loss -0.675
2024-12-17 02:41:12.803554: val_loss -0.5028
2024-12-17 02:41:12.804640: Pseudo dice [0.7607]
2024-12-17 02:41:12.805476: Epoch time: 89.73 s
2024-12-17 02:41:14.096621: 
2024-12-17 02:41:14.099019: Epoch 66
2024-12-17 02:41:14.099990: Current learning rate: 0.00593
2024-12-17 02:42:43.845856: Validation loss did not improve from -0.54528. Patience: 26/50
2024-12-17 02:42:43.847111: train_loss -0.6733
2024-12-17 02:42:43.847964: val_loss -0.5123
2024-12-17 02:42:43.848736: Pseudo dice [0.7658]
2024-12-17 02:42:43.849478: Epoch time: 89.75 s
2024-12-17 02:42:43.850090: Yayy! New best EMA pseudo Dice: 0.7492
2024-12-17 02:42:45.433622: 
2024-12-17 02:42:45.434986: Epoch 67
2024-12-17 02:42:45.436026: Current learning rate: 0.00587
2024-12-17 02:44:15.115631: Validation loss did not improve from -0.54528. Patience: 27/50
2024-12-17 02:44:15.116702: train_loss -0.6742
2024-12-17 02:44:15.117728: val_loss -0.495
2024-12-17 02:44:15.118479: Pseudo dice [0.7438]
2024-12-17 02:44:15.119106: Epoch time: 89.68 s
2024-12-17 02:44:16.428324: 
2024-12-17 02:44:16.429791: Epoch 68
2024-12-17 02:44:16.430597: Current learning rate: 0.00581
2024-12-17 02:45:46.158424: Validation loss did not improve from -0.54528. Patience: 28/50
2024-12-17 02:45:46.159421: train_loss -0.6749
2024-12-17 02:45:46.160341: val_loss -0.4838
2024-12-17 02:45:46.161179: Pseudo dice [0.7483]
2024-12-17 02:45:46.162114: Epoch time: 89.73 s
2024-12-17 02:45:47.417490: 
2024-12-17 02:45:47.419421: Epoch 69
2024-12-17 02:45:47.420208: Current learning rate: 0.00574
2024-12-17 02:47:17.025661: Validation loss did not improve from -0.54528. Patience: 29/50
2024-12-17 02:47:17.026922: train_loss -0.6839
2024-12-17 02:47:17.027693: val_loss -0.5194
2024-12-17 02:47:17.028407: Pseudo dice [0.7671]
2024-12-17 02:47:17.029189: Epoch time: 89.61 s
2024-12-17 02:47:17.385591: Yayy! New best EMA pseudo Dice: 0.7505
2024-12-17 02:47:18.989390: 
2024-12-17 02:47:18.990682: Epoch 70
2024-12-17 02:47:18.991451: Current learning rate: 0.00568
2024-12-17 02:48:48.318748: Validation loss did not improve from -0.54528. Patience: 30/50
2024-12-17 02:48:48.319758: train_loss -0.6752
2024-12-17 02:48:48.320999: val_loss -0.5248
2024-12-17 02:48:48.321930: Pseudo dice [0.764]
2024-12-17 02:48:48.322928: Epoch time: 89.33 s
2024-12-17 02:48:48.324103: Yayy! New best EMA pseudo Dice: 0.7518
2024-12-17 02:48:49.923114: 
2024-12-17 02:48:49.924933: Epoch 71
2024-12-17 02:48:49.925784: Current learning rate: 0.00562
2024-12-17 02:50:19.264622: Validation loss did not improve from -0.54528. Patience: 31/50
2024-12-17 02:50:19.265815: train_loss -0.6781
2024-12-17 02:50:19.266737: val_loss -0.5397
2024-12-17 02:50:19.267504: Pseudo dice [0.7698]
2024-12-17 02:50:19.268167: Epoch time: 89.34 s
2024-12-17 02:50:19.268819: Yayy! New best EMA pseudo Dice: 0.7536
2024-12-17 02:50:21.177769: 
2024-12-17 02:50:21.179233: Epoch 72
2024-12-17 02:50:21.179951: Current learning rate: 0.00555
2024-12-17 02:51:50.615572: Validation loss did not improve from -0.54528. Patience: 32/50
2024-12-17 02:51:50.617114: train_loss -0.6876
2024-12-17 02:51:50.618081: val_loss -0.4416
2024-12-17 02:51:50.618787: Pseudo dice [0.717]
2024-12-17 02:51:50.619541: Epoch time: 89.44 s
2024-12-17 02:51:51.856535: 
2024-12-17 02:51:51.858109: Epoch 73
2024-12-17 02:51:51.859008: Current learning rate: 0.00549
2024-12-17 02:53:21.255151: Validation loss did not improve from -0.54528. Patience: 33/50
2024-12-17 02:53:21.256200: train_loss -0.6907
2024-12-17 02:53:21.257275: val_loss -0.4982
2024-12-17 02:53:21.257994: Pseudo dice [0.7409]
2024-12-17 02:53:21.258862: Epoch time: 89.4 s
2024-12-17 02:53:22.523528: 
2024-12-17 02:53:22.525216: Epoch 74
2024-12-17 02:53:22.526048: Current learning rate: 0.00542
2024-12-17 02:54:51.955909: Validation loss did not improve from -0.54528. Patience: 34/50
2024-12-17 02:54:51.957113: train_loss -0.6922
2024-12-17 02:54:51.958348: val_loss -0.4443
2024-12-17 02:54:51.959168: Pseudo dice [0.7305]
2024-12-17 02:54:51.959786: Epoch time: 89.43 s
2024-12-17 02:54:53.557729: 
2024-12-17 02:54:53.559643: Epoch 75
2024-12-17 02:54:53.560458: Current learning rate: 0.00536
2024-12-17 02:56:22.938683: Validation loss did not improve from -0.54528. Patience: 35/50
2024-12-17 02:56:22.939696: train_loss -0.6913
2024-12-17 02:56:22.940437: val_loss -0.4903
2024-12-17 02:56:22.941217: Pseudo dice [0.7452]
2024-12-17 02:56:22.941949: Epoch time: 89.38 s
2024-12-17 02:56:24.165192: 
2024-12-17 02:56:24.166478: Epoch 76
2024-12-17 02:56:24.167201: Current learning rate: 0.00529
2024-12-17 02:57:53.657637: Validation loss did not improve from -0.54528. Patience: 36/50
2024-12-17 02:57:53.658933: train_loss -0.6885
2024-12-17 02:57:53.660059: val_loss -0.4808
2024-12-17 02:57:53.661012: Pseudo dice [0.7556]
2024-12-17 02:57:53.661929: Epoch time: 89.49 s
2024-12-17 02:57:54.956493: 
2024-12-17 02:57:54.958370: Epoch 77
2024-12-17 02:57:54.959372: Current learning rate: 0.00523
2024-12-17 02:59:24.452682: Validation loss did not improve from -0.54528. Patience: 37/50
2024-12-17 02:59:24.453785: train_loss -0.6905
2024-12-17 02:59:24.454718: val_loss -0.5085
2024-12-17 02:59:24.455487: Pseudo dice [0.7578]
2024-12-17 02:59:24.456435: Epoch time: 89.5 s
2024-12-17 02:59:25.735043: 
2024-12-17 02:59:25.736370: Epoch 78
2024-12-17 02:59:25.737358: Current learning rate: 0.00517
2024-12-17 03:00:55.355048: Validation loss did not improve from -0.54528. Patience: 38/50
2024-12-17 03:00:55.356274: train_loss -0.6941
2024-12-17 03:00:55.357217: val_loss -0.4811
2024-12-17 03:00:55.358014: Pseudo dice [0.7509]
2024-12-17 03:00:55.358760: Epoch time: 89.62 s
2024-12-17 03:00:56.691619: 
2024-12-17 03:00:56.693361: Epoch 79
2024-12-17 03:00:56.694293: Current learning rate: 0.0051
2024-12-17 03:02:26.230581: Validation loss did not improve from -0.54528. Patience: 39/50
2024-12-17 03:02:26.231894: train_loss -0.6972
2024-12-17 03:02:26.232832: val_loss -0.5143
2024-12-17 03:02:26.233740: Pseudo dice [0.7559]
2024-12-17 03:02:26.234463: Epoch time: 89.54 s
2024-12-17 03:02:27.895892: 
2024-12-17 03:02:27.897331: Epoch 80
2024-12-17 03:02:27.898096: Current learning rate: 0.00504
2024-12-17 03:03:57.480747: Validation loss did not improve from -0.54528. Patience: 40/50
2024-12-17 03:03:57.481906: train_loss -0.7041
2024-12-17 03:03:57.482917: val_loss -0.4718
2024-12-17 03:03:57.483598: Pseudo dice [0.738]
2024-12-17 03:03:57.484337: Epoch time: 89.59 s
2024-12-17 03:03:58.758659: 
2024-12-17 03:03:58.760525: Epoch 81
2024-12-17 03:03:58.761372: Current learning rate: 0.00497
2024-12-17 03:05:28.232770: Validation loss did not improve from -0.54528. Patience: 41/50
2024-12-17 03:05:28.233767: train_loss -0.7012
2024-12-17 03:05:28.235057: val_loss -0.4744
2024-12-17 03:05:28.235834: Pseudo dice [0.7457]
2024-12-17 03:05:28.237026: Epoch time: 89.48 s
2024-12-17 03:05:29.487297: 
2024-12-17 03:05:29.488851: Epoch 82
2024-12-17 03:05:29.489735: Current learning rate: 0.00491
2024-12-17 03:06:58.973543: Validation loss did not improve from -0.54528. Patience: 42/50
2024-12-17 03:06:58.975851: train_loss -0.6997
2024-12-17 03:06:58.978021: val_loss -0.5195
2024-12-17 03:06:58.978851: Pseudo dice [0.7634]
2024-12-17 03:06:58.980220: Epoch time: 89.49 s
2024-12-17 03:07:00.541294: 
2024-12-17 03:07:00.543068: Epoch 83
2024-12-17 03:07:00.543884: Current learning rate: 0.00484
2024-12-17 03:08:30.208950: Validation loss did not improve from -0.54528. Patience: 43/50
2024-12-17 03:08:30.210777: train_loss -0.6981
2024-12-17 03:08:30.212023: val_loss -0.513
2024-12-17 03:08:30.212946: Pseudo dice [0.7626]
2024-12-17 03:08:30.213838: Epoch time: 89.67 s
2024-12-17 03:08:31.403270: 
2024-12-17 03:08:31.404896: Epoch 84
2024-12-17 03:08:31.405784: Current learning rate: 0.00478
2024-12-17 03:10:00.830644: Validation loss did not improve from -0.54528. Patience: 44/50
2024-12-17 03:10:00.832081: train_loss -0.7038
2024-12-17 03:10:00.833039: val_loss -0.5384
2024-12-17 03:10:00.833708: Pseudo dice [0.7715]
2024-12-17 03:10:00.834447: Epoch time: 89.43 s
2024-12-17 03:10:02.433703: 
2024-12-17 03:10:02.435627: Epoch 85
2024-12-17 03:10:02.436389: Current learning rate: 0.00471
2024-12-17 03:11:31.897160: Validation loss did not improve from -0.54528. Patience: 45/50
2024-12-17 03:11:31.898268: train_loss -0.6981
2024-12-17 03:11:31.899044: val_loss -0.5038
2024-12-17 03:11:31.899808: Pseudo dice [0.7616]
2024-12-17 03:11:31.900725: Epoch time: 89.47 s
2024-12-17 03:11:31.901635: Yayy! New best EMA pseudo Dice: 0.754
2024-12-17 03:11:33.477451: 
2024-12-17 03:11:33.479119: Epoch 86
2024-12-17 03:11:33.479985: Current learning rate: 0.00465
2024-12-17 03:13:02.918394: Validation loss did not improve from -0.54528. Patience: 46/50
2024-12-17 03:13:02.920000: train_loss -0.7088
2024-12-17 03:13:02.920977: val_loss -0.5346
2024-12-17 03:13:02.921838: Pseudo dice [0.7702]
2024-12-17 03:13:02.922783: Epoch time: 89.44 s
2024-12-17 03:13:02.923626: Yayy! New best EMA pseudo Dice: 0.7556
2024-12-17 03:13:04.447007: 
2024-12-17 03:13:04.448736: Epoch 87
2024-12-17 03:13:04.449895: Current learning rate: 0.00458
2024-12-17 03:14:34.080043: Validation loss did not improve from -0.54528. Patience: 47/50
2024-12-17 03:14:34.081101: train_loss -0.7131
2024-12-17 03:14:34.082126: val_loss -0.4652
2024-12-17 03:14:34.083024: Pseudo dice [0.733]
2024-12-17 03:14:34.083857: Epoch time: 89.63 s
2024-12-17 03:14:35.282844: 
2024-12-17 03:14:35.284523: Epoch 88
2024-12-17 03:14:35.285432: Current learning rate: 0.00452
2024-12-17 03:16:04.976896: Validation loss did not improve from -0.54528. Patience: 48/50
2024-12-17 03:16:04.978209: train_loss -0.7077
2024-12-17 03:16:04.979236: val_loss -0.5242
2024-12-17 03:16:04.979873: Pseudo dice [0.7683]
2024-12-17 03:16:04.980479: Epoch time: 89.7 s
2024-12-17 03:16:06.180408: 
2024-12-17 03:16:06.182038: Epoch 89
2024-12-17 03:16:06.182868: Current learning rate: 0.00445
2024-12-17 03:17:35.933376: Validation loss did not improve from -0.54528. Patience: 49/50
2024-12-17 03:17:35.934337: train_loss -0.7139
2024-12-17 03:17:35.935454: val_loss -0.4922
2024-12-17 03:17:35.936366: Pseudo dice [0.7594]
2024-12-17 03:17:35.937258: Epoch time: 89.75 s
2024-12-17 03:17:37.517298: 
2024-12-17 03:17:37.519674: Epoch 90
2024-12-17 03:17:37.520778: Current learning rate: 0.00438
2024-12-17 03:19:07.201265: Validation loss did not improve from -0.54528. Patience: 50/50
2024-12-17 03:19:07.202193: train_loss -0.7121
2024-12-17 03:19:07.203104: val_loss -0.5212
2024-12-17 03:19:07.203909: Pseudo dice [0.7659]
2024-12-17 03:19:07.204587: Epoch time: 89.69 s
2024-12-17 03:19:07.205355: Yayy! New best EMA pseudo Dice: 0.7563
2024-12-17 03:19:08.825928: 
2024-12-17 03:19:08.827261: Epoch 91
2024-12-17 03:19:08.828067: Current learning rate: 0.00432
2024-12-17 03:20:38.585026: Validation loss did not improve from -0.54528. Patience: 51/50
2024-12-17 03:20:38.585723: train_loss -0.7085
2024-12-17 03:20:38.586630: val_loss -0.5154
2024-12-17 03:20:38.587445: Pseudo dice [0.7678]
2024-12-17 03:20:38.588219: Epoch time: 89.76 s
2024-12-17 03:20:38.588981: Yayy! New best EMA pseudo Dice: 0.7575
2024-12-17 03:20:40.113885: 
2024-12-17 03:20:40.115546: Epoch 92
2024-12-17 03:20:40.116346: Current learning rate: 0.00425
2024-12-17 03:22:09.882419: Validation loss did not improve from -0.54528. Patience: 52/50
2024-12-17 03:22:09.884449: train_loss -0.7131
2024-12-17 03:22:09.886063: val_loss -0.5312
2024-12-17 03:22:09.886819: Pseudo dice [0.7682]
2024-12-17 03:22:09.887449: Epoch time: 89.77 s
2024-12-17 03:22:09.888138: Yayy! New best EMA pseudo Dice: 0.7586
2024-12-17 03:22:11.595339: 
2024-12-17 03:22:11.596922: Epoch 93
2024-12-17 03:22:11.597807: Current learning rate: 0.00419
2024-12-17 03:23:41.297797: Validation loss did not improve from -0.54528. Patience: 53/50
2024-12-17 03:23:41.299067: train_loss -0.7239
2024-12-17 03:23:41.300299: val_loss -0.5213
2024-12-17 03:23:41.301274: Pseudo dice [0.7636]
2024-12-17 03:23:41.302380: Epoch time: 89.7 s
2024-12-17 03:23:41.303339: Yayy! New best EMA pseudo Dice: 0.7591
2024-12-17 03:23:43.209085: 
2024-12-17 03:23:43.210643: Epoch 94
2024-12-17 03:23:43.211586: Current learning rate: 0.00412
2024-12-17 03:25:12.995896: Validation loss did not improve from -0.54528. Patience: 54/50
2024-12-17 03:25:12.997808: train_loss -0.7131
2024-12-17 03:25:12.999145: val_loss -0.5106
2024-12-17 03:25:12.999881: Pseudo dice [0.7665]
2024-12-17 03:25:13.000662: Epoch time: 89.79 s
2024-12-17 03:25:13.383630: Yayy! New best EMA pseudo Dice: 0.7598
2024-12-17 03:25:14.951021: 
2024-12-17 03:25:14.952799: Epoch 95
2024-12-17 03:25:14.953679: Current learning rate: 0.00405
2024-12-17 03:26:44.843513: Validation loss did not improve from -0.54528. Patience: 55/50
2024-12-17 03:26:44.844916: train_loss -0.7179
2024-12-17 03:26:44.845856: val_loss -0.5259
2024-12-17 03:26:44.846511: Pseudo dice [0.7668]
2024-12-17 03:26:44.847188: Epoch time: 89.89 s
2024-12-17 03:26:44.847865: Yayy! New best EMA pseudo Dice: 0.7605
2024-12-17 03:26:46.450918: 
2024-12-17 03:26:46.452765: Epoch 96
2024-12-17 03:26:46.453526: Current learning rate: 0.00399
2024-12-17 03:28:16.298014: Validation loss did not improve from -0.54528. Patience: 56/50
2024-12-17 03:28:16.299112: train_loss -0.7199
2024-12-17 03:28:16.299989: val_loss -0.5094
2024-12-17 03:28:16.300638: Pseudo dice [0.7574]
2024-12-17 03:28:16.301473: Epoch time: 89.85 s
2024-12-17 03:28:17.582934: 
2024-12-17 03:28:17.584857: Epoch 97
2024-12-17 03:28:17.585780: Current learning rate: 0.00392
2024-12-17 03:29:47.317433: Validation loss did not improve from -0.54528. Patience: 57/50
2024-12-17 03:29:47.318625: train_loss -0.7255
2024-12-17 03:29:47.319455: val_loss -0.4709
2024-12-17 03:29:47.320234: Pseudo dice [0.7436]
2024-12-17 03:29:47.321028: Epoch time: 89.74 s
2024-12-17 03:29:48.568031: 
2024-12-17 03:29:48.569606: Epoch 98
2024-12-17 03:29:48.570342: Current learning rate: 0.00385
2024-12-17 03:31:18.249643: Validation loss did not improve from -0.54528. Patience: 58/50
2024-12-17 03:31:18.250834: train_loss -0.7249
2024-12-17 03:31:18.251996: val_loss -0.5221
2024-12-17 03:31:18.252806: Pseudo dice [0.768]
2024-12-17 03:31:18.253556: Epoch time: 89.68 s
2024-12-17 03:31:19.506442: 
2024-12-17 03:31:19.508376: Epoch 99
2024-12-17 03:31:19.509226: Current learning rate: 0.00379
2024-12-17 03:32:49.257252: Validation loss did not improve from -0.54528. Patience: 59/50
2024-12-17 03:32:49.258308: train_loss -0.7273
2024-12-17 03:32:49.259165: val_loss -0.445
2024-12-17 03:32:49.260027: Pseudo dice [0.7317]
2024-12-17 03:32:49.260728: Epoch time: 89.75 s
2024-12-17 03:32:50.866886: 
2024-12-17 03:32:50.868398: Epoch 100
2024-12-17 03:32:50.869076: Current learning rate: 0.00372
2024-12-17 03:34:20.584715: Validation loss did not improve from -0.54528. Patience: 60/50
2024-12-17 03:34:20.585990: train_loss -0.7232
2024-12-17 03:34:20.587220: val_loss -0.5121
2024-12-17 03:34:20.588208: Pseudo dice [0.769]
2024-12-17 03:34:20.589535: Epoch time: 89.72 s
2024-12-17 03:34:21.825958: 
2024-12-17 03:34:21.827820: Epoch 101
2024-12-17 03:34:21.828849: Current learning rate: 0.00365
2024-12-17 03:35:51.495703: Validation loss improved from -0.54528 to -0.54826! Patience: 60/50
2024-12-17 03:35:51.496696: train_loss -0.7255
2024-12-17 03:35:51.497708: val_loss -0.5483
2024-12-17 03:35:51.498410: Pseudo dice [0.7782]
2024-12-17 03:35:51.499251: Epoch time: 89.67 s
2024-12-17 03:35:52.718640: 
2024-12-17 03:35:52.720244: Epoch 102
2024-12-17 03:35:52.721206: Current learning rate: 0.00359
2024-12-17 03:37:22.479599: Validation loss did not improve from -0.54826. Patience: 1/50
2024-12-17 03:37:22.480686: train_loss -0.7343
2024-12-17 03:37:22.481452: val_loss -0.4875
2024-12-17 03:37:22.482159: Pseudo dice [0.7497]
2024-12-17 03:37:22.482827: Epoch time: 89.76 s
2024-12-17 03:37:23.709080: 
2024-12-17 03:37:23.710678: Epoch 103
2024-12-17 03:37:23.711409: Current learning rate: 0.00352
2024-12-17 03:38:53.048671: Validation loss did not improve from -0.54826. Patience: 2/50
2024-12-17 03:38:53.049519: train_loss -0.7342
2024-12-17 03:38:53.050287: val_loss -0.5322
2024-12-17 03:38:53.051187: Pseudo dice [0.7651]
2024-12-17 03:38:53.051849: Epoch time: 89.34 s
2024-12-17 03:38:54.292984: 
2024-12-17 03:38:54.294488: Epoch 104
2024-12-17 03:38:54.295345: Current learning rate: 0.00345
2024-12-17 03:40:23.688292: Validation loss did not improve from -0.54826. Patience: 3/50
2024-12-17 03:40:23.689233: train_loss -0.7317
2024-12-17 03:40:23.690058: val_loss -0.529
2024-12-17 03:40:23.690753: Pseudo dice [0.7721]
2024-12-17 03:40:23.691425: Epoch time: 89.4 s
2024-12-17 03:40:24.059836: Yayy! New best EMA pseudo Dice: 0.7608
2024-12-17 03:40:25.638180: 
2024-12-17 03:40:25.639540: Epoch 105
2024-12-17 03:40:25.640320: Current learning rate: 0.00338
2024-12-17 03:41:55.251200: Validation loss did not improve from -0.54826. Patience: 4/50
2024-12-17 03:41:55.252377: train_loss -0.7323
2024-12-17 03:41:55.253269: val_loss -0.4518
2024-12-17 03:41:55.254083: Pseudo dice [0.7357]
2024-12-17 03:41:55.254800: Epoch time: 89.62 s
2024-12-17 03:41:56.500644: 
2024-12-17 03:41:56.502280: Epoch 106
2024-12-17 03:41:56.502953: Current learning rate: 0.00332
2024-12-17 03:43:25.879448: Validation loss did not improve from -0.54826. Patience: 5/50
2024-12-17 03:43:25.880440: train_loss -0.7362
2024-12-17 03:43:25.881421: val_loss -0.5063
2024-12-17 03:43:25.882235: Pseudo dice [0.7661]
2024-12-17 03:43:25.883073: Epoch time: 89.38 s
2024-12-17 03:43:27.152513: 
2024-12-17 03:43:27.154517: Epoch 107
2024-12-17 03:43:27.155411: Current learning rate: 0.00325
2024-12-17 03:44:56.569608: Validation loss did not improve from -0.54826. Patience: 6/50
2024-12-17 03:44:56.570779: train_loss -0.7337
2024-12-17 03:44:56.571829: val_loss -0.5
2024-12-17 03:44:56.572787: Pseudo dice [0.7621]
2024-12-17 03:44:56.573752: Epoch time: 89.42 s
2024-12-17 03:44:57.796724: 
2024-12-17 03:44:57.797857: Epoch 108
2024-12-17 03:44:57.798580: Current learning rate: 0.00318
2024-12-17 03:46:27.314461: Validation loss did not improve from -0.54826. Patience: 7/50
2024-12-17 03:46:27.315365: train_loss -0.7308
2024-12-17 03:46:27.316085: val_loss -0.4879
2024-12-17 03:46:27.316778: Pseudo dice [0.7531]
2024-12-17 03:46:27.317573: Epoch time: 89.52 s
2024-12-17 03:46:28.612820: 
2024-12-17 03:46:28.614287: Epoch 109
2024-12-17 03:46:28.614977: Current learning rate: 0.00311
2024-12-17 03:47:57.923985: Validation loss improved from -0.54826 to -0.55686! Patience: 7/50
2024-12-17 03:47:57.925257: train_loss -0.7382
2024-12-17 03:47:57.926197: val_loss -0.5569
2024-12-17 03:47:57.926886: Pseudo dice [0.7764]
2024-12-17 03:47:57.927556: Epoch time: 89.31 s
2024-12-17 03:47:59.488383: 
2024-12-17 03:47:59.490016: Epoch 110
2024-12-17 03:47:59.490771: Current learning rate: 0.00304
2024-12-17 03:49:28.809897: Validation loss did not improve from -0.55686. Patience: 1/50
2024-12-17 03:49:28.811085: train_loss -0.7391
2024-12-17 03:49:28.812249: val_loss -0.4805
2024-12-17 03:49:28.813113: Pseudo dice [0.7566]
2024-12-17 03:49:28.813918: Epoch time: 89.32 s
2024-12-17 03:49:30.097890: 
2024-12-17 03:49:30.099565: Epoch 111
2024-12-17 03:49:30.100301: Current learning rate: 0.00297
2024-12-17 03:50:59.809296: Validation loss did not improve from -0.55686. Patience: 2/50
2024-12-17 03:50:59.810440: train_loss -0.7394
2024-12-17 03:50:59.811289: val_loss -0.4862
2024-12-17 03:50:59.812202: Pseudo dice [0.7475]
2024-12-17 03:50:59.812845: Epoch time: 89.71 s
2024-12-17 03:51:01.076161: 
2024-12-17 03:51:01.077711: Epoch 112
2024-12-17 03:51:01.078382: Current learning rate: 0.00291
2024-12-17 03:52:31.232642: Validation loss did not improve from -0.55686. Patience: 3/50
2024-12-17 03:52:31.233754: train_loss -0.7411
2024-12-17 03:52:31.234714: val_loss -0.4842
2024-12-17 03:52:31.235524: Pseudo dice [0.7498]
2024-12-17 03:52:31.236256: Epoch time: 90.16 s
2024-12-17 03:52:32.564078: 
2024-12-17 03:52:32.565580: Epoch 113
2024-12-17 03:52:32.566437: Current learning rate: 0.00284
2024-12-17 03:54:02.585001: Validation loss did not improve from -0.55686. Patience: 4/50
2024-12-17 03:54:02.587578: train_loss -0.7399
2024-12-17 03:54:02.588752: val_loss -0.4927
2024-12-17 03:54:02.589597: Pseudo dice [0.752]
2024-12-17 03:54:02.590512: Epoch time: 90.02 s
2024-12-17 03:54:03.887809: 
2024-12-17 03:54:03.889794: Epoch 114
2024-12-17 03:54:03.890834: Current learning rate: 0.00277
2024-12-17 03:55:33.821156: Validation loss did not improve from -0.55686. Patience: 5/50
2024-12-17 03:55:33.822350: train_loss -0.744
2024-12-17 03:55:33.823327: val_loss -0.5174
2024-12-17 03:55:33.824225: Pseudo dice [0.7721]
2024-12-17 03:55:33.825121: Epoch time: 89.94 s
2024-12-17 03:55:35.534419: 
2024-12-17 03:55:35.536314: Epoch 115
2024-12-17 03:55:35.537080: Current learning rate: 0.0027
2024-12-17 03:57:05.329421: Validation loss did not improve from -0.55686. Patience: 6/50
2024-12-17 03:57:05.330792: train_loss -0.7461
2024-12-17 03:57:05.331615: val_loss -0.5522
2024-12-17 03:57:05.332333: Pseudo dice [0.7781]
2024-12-17 03:57:05.333052: Epoch time: 89.8 s
2024-12-17 03:57:07.109548: 
2024-12-17 03:57:07.110814: Epoch 116
2024-12-17 03:57:07.111573: Current learning rate: 0.00263
2024-12-17 03:58:36.872955: Validation loss did not improve from -0.55686. Patience: 7/50
2024-12-17 03:58:36.874244: train_loss -0.747
2024-12-17 03:58:36.875241: val_loss -0.4965
2024-12-17 03:58:36.875875: Pseudo dice [0.7623]
2024-12-17 03:58:36.876529: Epoch time: 89.77 s
2024-12-17 03:58:36.877120: Yayy! New best EMA pseudo Dice: 0.7609
2024-12-17 03:58:38.615125: 
2024-12-17 03:58:38.616056: Epoch 117
2024-12-17 03:58:38.616768: Current learning rate: 0.00256
2024-12-17 04:00:08.401813: Validation loss did not improve from -0.55686. Patience: 8/50
2024-12-17 04:00:08.402665: train_loss -0.7505
2024-12-17 04:00:08.403406: val_loss -0.5003
2024-12-17 04:00:08.404140: Pseudo dice [0.7596]
2024-12-17 04:00:08.404803: Epoch time: 89.79 s
2024-12-17 04:00:09.742015: 
2024-12-17 04:00:09.743528: Epoch 118
2024-12-17 04:00:09.744263: Current learning rate: 0.00249
2024-12-17 04:01:39.602740: Validation loss did not improve from -0.55686. Patience: 9/50
2024-12-17 04:01:39.603920: train_loss -0.7474
2024-12-17 04:01:39.604605: val_loss -0.4852
2024-12-17 04:01:39.605335: Pseudo dice [0.7453]
2024-12-17 04:01:39.606162: Epoch time: 89.86 s
2024-12-17 04:01:40.885152: 
2024-12-17 04:01:40.887050: Epoch 119
2024-12-17 04:01:40.887878: Current learning rate: 0.00242
2024-12-17 04:03:10.720248: Validation loss did not improve from -0.55686. Patience: 10/50
2024-12-17 04:03:10.721459: train_loss -0.7488
2024-12-17 04:03:10.722232: val_loss -0.4635
2024-12-17 04:03:10.723114: Pseudo dice [0.7439]
2024-12-17 04:03:10.723723: Epoch time: 89.84 s
2024-12-17 04:03:12.348208: 
2024-12-17 04:03:12.350065: Epoch 120
2024-12-17 04:03:12.350868: Current learning rate: 0.00235
2024-12-17 04:04:42.171751: Validation loss did not improve from -0.55686. Patience: 11/50
2024-12-17 04:04:42.172843: train_loss -0.7489
2024-12-17 04:04:42.173672: val_loss -0.4969
2024-12-17 04:04:42.174592: Pseudo dice [0.7561]
2024-12-17 04:04:42.175314: Epoch time: 89.83 s
2024-12-17 04:04:43.504590: 
2024-12-17 04:04:43.505946: Epoch 121
2024-12-17 04:04:43.506749: Current learning rate: 0.00228
2024-12-17 04:06:13.365715: Validation loss did not improve from -0.55686. Patience: 12/50
2024-12-17 04:06:13.366832: train_loss -0.7475
2024-12-17 04:06:13.367863: val_loss -0.4955
2024-12-17 04:06:13.368693: Pseudo dice [0.7675]
2024-12-17 04:06:13.369524: Epoch time: 89.86 s
2024-12-17 04:06:14.720066: 
2024-12-17 04:06:14.721288: Epoch 122
2024-12-17 04:06:14.722026: Current learning rate: 0.00221
2024-12-17 04:07:44.707164: Validation loss did not improve from -0.55686. Patience: 13/50
2024-12-17 04:07:44.708149: train_loss -0.7532
2024-12-17 04:07:44.709113: val_loss -0.5078
2024-12-17 04:07:44.709865: Pseudo dice [0.7659]
2024-12-17 04:07:44.710610: Epoch time: 89.99 s
2024-12-17 04:07:45.987835: 
2024-12-17 04:07:45.989457: Epoch 123
2024-12-17 04:07:45.990262: Current learning rate: 0.00214
2024-12-17 04:09:16.201313: Validation loss did not improve from -0.55686. Patience: 14/50
2024-12-17 04:09:16.202482: train_loss -0.7499
2024-12-17 04:09:16.203383: val_loss -0.4588
2024-12-17 04:09:16.204126: Pseudo dice [0.7427]
2024-12-17 04:09:16.204794: Epoch time: 90.22 s
2024-12-17 04:09:17.572767: 
2024-12-17 04:09:17.574469: Epoch 124
2024-12-17 04:09:17.575307: Current learning rate: 0.00207
2024-12-17 04:10:47.704769: Validation loss did not improve from -0.55686. Patience: 15/50
2024-12-17 04:10:47.706006: train_loss -0.7523
2024-12-17 04:10:47.706955: val_loss -0.4824
2024-12-17 04:10:47.707767: Pseudo dice [0.7579]
2024-12-17 04:10:47.708714: Epoch time: 90.13 s
2024-12-17 04:10:49.398375: 
2024-12-17 04:10:49.400007: Epoch 125
2024-12-17 04:10:49.400710: Current learning rate: 0.00199
2024-12-17 04:12:19.681411: Validation loss did not improve from -0.55686. Patience: 16/50
2024-12-17 04:12:19.683123: train_loss -0.7552
2024-12-17 04:12:19.684786: val_loss -0.5258
2024-12-17 04:12:19.685554: Pseudo dice [0.774]
2024-12-17 04:12:19.686581: Epoch time: 90.29 s
2024-12-17 04:12:21.009727: 
2024-12-17 04:12:21.010834: Epoch 126
2024-12-17 04:12:21.011665: Current learning rate: 0.00192
2024-12-17 04:13:51.237814: Validation loss did not improve from -0.55686. Patience: 17/50
2024-12-17 04:13:51.240450: train_loss -0.7488
2024-12-17 04:13:51.241563: val_loss -0.4814
2024-12-17 04:13:51.242462: Pseudo dice [0.7552]
2024-12-17 04:13:51.243168: Epoch time: 90.23 s
2024-12-17 04:13:53.206459: 
2024-12-17 04:13:53.208276: Epoch 127
2024-12-17 04:13:53.209014: Current learning rate: 0.00185
2024-12-17 04:15:23.368319: Validation loss did not improve from -0.55686. Patience: 18/50
2024-12-17 04:15:23.369826: train_loss -0.7542
2024-12-17 04:15:23.370806: val_loss -0.4827
2024-12-17 04:15:23.371465: Pseudo dice [0.7519]
2024-12-17 04:15:23.372245: Epoch time: 90.16 s
2024-12-17 04:15:24.692302: 
2024-12-17 04:15:24.693983: Epoch 128
2024-12-17 04:15:24.694959: Current learning rate: 0.00178
2024-12-17 04:16:54.738294: Validation loss did not improve from -0.55686. Patience: 19/50
2024-12-17 04:16:54.739539: train_loss -0.7529
2024-12-17 04:16:54.740393: val_loss -0.4409
2024-12-17 04:16:54.741187: Pseudo dice [0.7286]
2024-12-17 04:16:54.741929: Epoch time: 90.05 s
2024-12-17 04:16:56.079238: 
2024-12-17 04:16:56.080955: Epoch 129
2024-12-17 04:16:56.081717: Current learning rate: 0.0017
2024-12-17 04:18:26.118737: Validation loss did not improve from -0.55686. Patience: 20/50
2024-12-17 04:18:26.120490: train_loss -0.7521
2024-12-17 04:18:26.121268: val_loss -0.4775
2024-12-17 04:18:26.122004: Pseudo dice [0.746]
2024-12-17 04:18:26.122601: Epoch time: 90.04 s
2024-12-17 04:18:27.769643: 
2024-12-17 04:18:27.771101: Epoch 130
2024-12-17 04:18:27.771946: Current learning rate: 0.00163
2024-12-17 04:19:57.874599: Validation loss did not improve from -0.55686. Patience: 21/50
2024-12-17 04:19:57.875777: train_loss -0.755
2024-12-17 04:19:57.876713: val_loss -0.4684
2024-12-17 04:19:57.877384: Pseudo dice [0.7477]
2024-12-17 04:19:57.878120: Epoch time: 90.11 s
2024-12-17 04:19:59.143336: 
2024-12-17 04:19:59.145561: Epoch 131
2024-12-17 04:19:59.146581: Current learning rate: 0.00156
2024-12-17 04:21:29.541305: Validation loss did not improve from -0.55686. Patience: 22/50
2024-12-17 04:21:29.542584: train_loss -0.7588
2024-12-17 04:21:29.543398: val_loss -0.4741
2024-12-17 04:21:29.544350: Pseudo dice [0.7523]
2024-12-17 04:21:29.545211: Epoch time: 90.4 s
2024-12-17 04:21:30.812462: 
2024-12-17 04:21:30.814373: Epoch 132
2024-12-17 04:21:30.815182: Current learning rate: 0.00148
2024-12-17 04:23:01.032413: Validation loss did not improve from -0.55686. Patience: 23/50
2024-12-17 04:23:01.034457: train_loss -0.7603
2024-12-17 04:23:01.035602: val_loss -0.5356
2024-12-17 04:23:01.036336: Pseudo dice [0.774]
2024-12-17 04:23:01.037100: Epoch time: 90.22 s
2024-12-17 04:23:02.316664: 
2024-12-17 04:23:02.318325: Epoch 133
2024-12-17 04:23:02.319414: Current learning rate: 0.00141
2024-12-17 04:24:32.592154: Validation loss did not improve from -0.55686. Patience: 24/50
2024-12-17 04:24:32.593379: train_loss -0.7615
2024-12-17 04:24:32.594097: val_loss -0.4899
2024-12-17 04:24:32.594872: Pseudo dice [0.75]
2024-12-17 04:24:32.595626: Epoch time: 90.28 s
2024-12-17 04:24:33.875658: 
2024-12-17 04:24:33.877225: Epoch 134
2024-12-17 04:24:33.877941: Current learning rate: 0.00133
2024-12-17 04:26:04.049511: Validation loss did not improve from -0.55686. Patience: 25/50
2024-12-17 04:26:04.050732: train_loss -0.7606
2024-12-17 04:26:04.051896: val_loss -0.4897
2024-12-17 04:26:04.052740: Pseudo dice [0.7521]
2024-12-17 04:26:04.053455: Epoch time: 90.18 s
2024-12-17 04:26:05.726097: 
2024-12-17 04:26:05.728076: Epoch 135
2024-12-17 04:26:05.728913: Current learning rate: 0.00126
2024-12-17 04:27:35.759883: Validation loss did not improve from -0.55686. Patience: 26/50
2024-12-17 04:27:35.761383: train_loss -0.7572
2024-12-17 04:27:35.762556: val_loss -0.5125
2024-12-17 04:27:35.763208: Pseudo dice [0.7702]
2024-12-17 04:27:35.763963: Epoch time: 90.04 s
2024-12-17 04:27:37.125635: 
2024-12-17 04:27:37.127335: Epoch 136
2024-12-17 04:27:37.128254: Current learning rate: 0.00118
2024-12-17 04:29:07.133129: Validation loss did not improve from -0.55686. Patience: 27/50
2024-12-17 04:29:07.133964: train_loss -0.7584
2024-12-17 04:29:07.134628: val_loss -0.5048
2024-12-17 04:29:07.135268: Pseudo dice [0.7648]
2024-12-17 04:29:07.135831: Epoch time: 90.01 s
2024-12-17 04:29:08.478299: 
2024-12-17 04:29:08.479234: Epoch 137
2024-12-17 04:29:08.479869: Current learning rate: 0.00111
2024-12-17 04:30:38.529808: Validation loss did not improve from -0.55686. Patience: 28/50
2024-12-17 04:30:38.530924: train_loss -0.7625
2024-12-17 04:30:38.531804: val_loss -0.4751
2024-12-17 04:30:38.532547: Pseudo dice [0.7512]
2024-12-17 04:30:38.533342: Epoch time: 90.05 s
2024-12-17 04:30:40.355037: 
2024-12-17 04:30:40.356762: Epoch 138
2024-12-17 04:30:40.357550: Current learning rate: 0.00103
2024-12-17 04:32:10.387925: Validation loss did not improve from -0.55686. Patience: 29/50
2024-12-17 04:32:10.389295: train_loss -0.7687
2024-12-17 04:32:10.390087: val_loss -0.4929
2024-12-17 04:32:10.390801: Pseudo dice [0.747]
2024-12-17 04:32:10.391559: Epoch time: 90.04 s
2024-12-17 04:32:11.703530: 
2024-12-17 04:32:11.705677: Epoch 139
2024-12-17 04:32:11.706741: Current learning rate: 0.00095
2024-12-17 04:33:41.785564: Validation loss did not improve from -0.55686. Patience: 30/50
2024-12-17 04:33:41.786735: train_loss -0.7609
2024-12-17 04:33:41.787923: val_loss -0.4977
2024-12-17 04:33:41.788725: Pseudo dice [0.7548]
2024-12-17 04:33:41.789414: Epoch time: 90.08 s
2024-12-17 04:33:43.455128: 
2024-12-17 04:33:43.457167: Epoch 140
2024-12-17 04:33:43.458076: Current learning rate: 0.00087
2024-12-17 04:35:13.551762: Validation loss did not improve from -0.55686. Patience: 31/50
2024-12-17 04:35:13.552706: train_loss -0.7645
2024-12-17 04:35:13.553484: val_loss -0.4967
2024-12-17 04:35:13.554163: Pseudo dice [0.7616]
2024-12-17 04:35:13.554854: Epoch time: 90.1 s
2024-12-17 04:35:14.839929: 
2024-12-17 04:35:14.841715: Epoch 141
2024-12-17 04:35:14.842485: Current learning rate: 0.00079
2024-12-17 04:36:44.991808: Validation loss did not improve from -0.55686. Patience: 32/50
2024-12-17 04:36:44.993055: train_loss -0.7599
2024-12-17 04:36:44.994334: val_loss -0.5385
2024-12-17 04:36:44.995062: Pseudo dice [0.7732]
2024-12-17 04:36:44.995775: Epoch time: 90.15 s
2024-12-17 04:36:46.290701: 
2024-12-17 04:36:46.292512: Epoch 142
2024-12-17 04:36:46.293400: Current learning rate: 0.00071
2024-12-17 04:38:16.604556: Validation loss did not improve from -0.55686. Patience: 33/50
2024-12-17 04:38:16.605756: train_loss -0.765
2024-12-17 04:38:16.606793: val_loss -0.5182
2024-12-17 04:38:16.607612: Pseudo dice [0.7682]
2024-12-17 04:38:16.608417: Epoch time: 90.32 s
2024-12-17 04:38:17.963167: 
2024-12-17 04:38:17.965103: Epoch 143
2024-12-17 04:38:17.965944: Current learning rate: 0.00063
2024-12-17 04:39:48.435136: Validation loss did not improve from -0.55686. Patience: 34/50
2024-12-17 04:39:48.436435: train_loss -0.7705
2024-12-17 04:39:48.437373: val_loss -0.5115
2024-12-17 04:39:48.438125: Pseudo dice [0.77]
2024-12-17 04:39:48.438907: Epoch time: 90.47 s
2024-12-17 04:39:49.806071: 
2024-12-17 04:39:49.807700: Epoch 144
2024-12-17 04:39:49.808456: Current learning rate: 0.00055
2024-12-17 04:41:20.183867: Validation loss did not improve from -0.55686. Patience: 35/50
2024-12-17 04:41:20.184919: train_loss -0.7645
2024-12-17 04:41:20.185761: val_loss -0.4906
2024-12-17 04:41:20.186521: Pseudo dice [0.7559]
2024-12-17 04:41:20.187217: Epoch time: 90.38 s
2024-12-17 04:41:21.937145: 
2024-12-17 04:41:21.938919: Epoch 145
2024-12-17 04:41:21.939845: Current learning rate: 0.00047
2024-12-17 04:42:52.231326: Validation loss did not improve from -0.55686. Patience: 36/50
2024-12-17 04:42:52.232388: train_loss -0.77
2024-12-17 04:42:52.233258: val_loss -0.5246
2024-12-17 04:42:52.234074: Pseudo dice [0.7761]
2024-12-17 04:42:52.234851: Epoch time: 90.3 s
2024-12-17 04:42:52.235870: Yayy! New best EMA pseudo Dice: 0.7612
2024-12-17 04:42:53.913465: 
2024-12-17 04:42:53.914939: Epoch 146
2024-12-17 04:42:53.915749: Current learning rate: 0.00038
2024-12-17 04:44:23.884995: Validation loss did not improve from -0.55686. Patience: 37/50
2024-12-17 04:44:23.886215: train_loss -0.7685
2024-12-17 04:44:23.887185: val_loss -0.526
2024-12-17 04:44:23.887998: Pseudo dice [0.7747]
2024-12-17 04:44:23.888664: Epoch time: 89.97 s
2024-12-17 04:44:23.889526: Yayy! New best EMA pseudo Dice: 0.7626
2024-12-17 04:44:25.566397: 
2024-12-17 04:44:25.568609: Epoch 147
2024-12-17 04:44:25.569546: Current learning rate: 0.0003
2024-12-17 04:45:55.587022: Validation loss did not improve from -0.55686. Patience: 38/50
2024-12-17 04:45:55.588079: train_loss -0.7683
2024-12-17 04:45:55.589037: val_loss -0.5032
2024-12-17 04:45:55.589952: Pseudo dice [0.7513]
2024-12-17 04:45:55.590610: Epoch time: 90.02 s
2024-12-17 04:45:56.934647: 
2024-12-17 04:45:56.936318: Epoch 148
2024-12-17 04:45:56.937214: Current learning rate: 0.00021
2024-12-17 04:47:27.265697: Validation loss did not improve from -0.55686. Patience: 39/50
2024-12-17 04:47:27.266815: train_loss -0.7677
2024-12-17 04:47:27.267991: val_loss -0.5176
2024-12-17 04:47:27.268969: Pseudo dice [0.7731]
2024-12-17 04:47:27.269908: Epoch time: 90.33 s
2024-12-17 04:47:27.270807: Yayy! New best EMA pseudo Dice: 0.7626
2024-12-17 04:47:28.978087: 
2024-12-17 04:47:28.979677: Epoch 149
2024-12-17 04:47:28.980623: Current learning rate: 0.00011
2024-12-17 04:48:58.816075: Validation loss did not improve from -0.55686. Patience: 40/50
2024-12-17 04:48:58.817295: train_loss -0.7673
2024-12-17 04:48:58.818406: val_loss -0.5096
2024-12-17 04:48:58.819444: Pseudo dice [0.7733]
2024-12-17 04:48:58.820215: Epoch time: 89.84 s
2024-12-17 04:48:58.821089: Yayy! New best EMA pseudo Dice: 0.7637
2024-12-17 04:49:00.831133: Training done.
2024-12-17 04:47:33.874267: Using splits from existing split file: /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset307_Sohee_Calcium_OCT_CrossValidation/splits_final.json
2024-12-17 04:47:33.896699: The split file contains 5 splits.
2024-12-17 04:47:33.897860: Desired fold for training: 1
2024-12-17 04:47:33.898761: This split has 6 training and 2 validation cases.
2024-12-17 04:47:33.899725: predicting 101-019
2024-12-17 04:47:33.910151: 101-019, shape torch.Size([1, 375, 498, 498]), rank 0
2024-12-17 04:49:26.321239: predicting 401-004
2024-12-17 04:49:26.346177: 401-004, shape torch.Size([1, 375, 498, 498]), rank 0
2024-12-17 04:51:16.168633: Validation complete
2024-12-17 04:51:16.169717: Mean Validation Dice:  0.7420881020686648
2024-12-17 04:49:00.963620: Using splits from existing split file: /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset307_Sohee_Calcium_OCT_CrossValidation/splits_final.json
2024-12-17 04:49:00.965693: The split file contains 5 splits.
2024-12-17 04:49:00.966711: Desired fold for training: 0
2024-12-17 04:49:00.967599: This split has 6 training and 2 validation cases.
2024-12-17 04:49:00.968738: predicting 106-002
2024-12-17 04:49:00.981601: 106-002, shape torch.Size([1, 539, 498, 498]), rank 0
2024-12-17 04:51:17.465183: predicting 706-005
2024-12-17 04:51:17.496230: 706-005, shape torch.Size([1, 375, 498, 498]), rank 0
2024-12-17 04:53:08.512164: Validation complete
2024-12-17 04:53:08.513070: Mean Validation Dice:  0.759408345420812

############################
INFO: You are using the old nnU-Net default plans. We have updated our recommendations. Please consider using those instead! Read more here: https://github.com/MIC-DKFZ/nnUNet/blob/master/documentation/resenc_presets.md
############################

Using device: cuda:0

#######################################################################
Please cite the following paper when using nnU-Net:
Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.
#######################################################################

2024-12-17 04:53:15.098248: Using torch.compile...

############################
INFO: You are using the old nnU-Net default plans. We have updated our recommendations. Please consider using those instead! Read more here: https://github.com/MIC-DKFZ/nnUNet/blob/master/documentation/resenc_presets.md
############################

Using device: cuda:0

#######################################################################
Please cite the following paper when using nnU-Net:
Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.
#######################################################################

2024-12-17 04:53:15.095949: Using torch.compile...
/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
################### Loading pretrained weights from file  /home/gridsan/nchutisilp/projects/OpenAI-CLIP/clip_preivl_poststent_pretrained_nnUNet.pt ###################
Below is the list of overlapping blocks in pretrained model and nnUNet architecture:
encoder.stages.0.0.convs.0.conv.weight shape torch.Size([32, 1, 3, 3, 3])
encoder.stages.0.0.convs.0.conv.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.norm.weight shape torch.Size([32])
encoder.stages.0.0.convs.0.norm.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.0.weight shape torch.Size([32, 1, 3, 3, 3])
encoder.stages.0.0.convs.0.all_modules.0.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.1.weight shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.1.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
encoder.stages.0.0.convs.1.conv.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.norm.weight shape torch.Size([32])
encoder.stages.0.0.convs.1.norm.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
encoder.stages.0.0.convs.1.all_modules.0.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.1.weight shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.1.bias shape torch.Size([32])
encoder.stages.1.0.convs.0.conv.weight shape torch.Size([64, 32, 3, 3, 3])
encoder.stages.1.0.convs.0.conv.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.norm.weight shape torch.Size([64])
encoder.stages.1.0.convs.0.norm.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.0.weight shape torch.Size([64, 32, 3, 3, 3])
encoder.stages.1.0.convs.0.all_modules.0.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.1.weight shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.1.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
encoder.stages.1.0.convs.1.conv.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.norm.weight shape torch.Size([64])
encoder.stages.1.0.convs.1.norm.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
encoder.stages.1.0.convs.1.all_modules.0.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.1.weight shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.1.bias shape torch.Size([64])
encoder.stages.2.0.convs.0.conv.weight shape torch.Size([128, 64, 3, 3, 3])
encoder.stages.2.0.convs.0.conv.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.norm.weight shape torch.Size([128])
encoder.stages.2.0.convs.0.norm.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.0.weight shape torch.Size([128, 64, 3, 3, 3])
encoder.stages.2.0.convs.0.all_modules.0.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.1.weight shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.1.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
encoder.stages.2.0.convs.1.conv.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.norm.weight shape torch.Size([128])
encoder.stages.2.0.convs.1.norm.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
encoder.stages.2.0.convs.1.all_modules.0.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.1.weight shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.1.bias shape torch.Size([128])
encoder.stages.3.0.convs.0.conv.weight shape torch.Size([256, 128, 3, 3, 3])
encoder.stages.3.0.convs.0.conv.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.norm.weight shape torch.Size([256])
encoder.stages.3.0.convs.0.norm.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.0.weight shape torch.Size([256, 128, 3, 3, 3])
encoder.stages.3.0.convs.0.all_modules.0.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.1.weight shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.1.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
encoder.stages.3.0.convs.1.conv.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.norm.weight shape torch.Size([256])
encoder.stages.3.0.convs.1.norm.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
encoder.stages.3.0.convs.1.all_modules.0.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.1.weight shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.1.bias shape torch.Size([256])
encoder.stages.4.0.convs.0.conv.weight shape torch.Size([320, 256, 3, 3, 3])
encoder.stages.4.0.convs.0.conv.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.norm.weight shape torch.Size([320])
encoder.stages.4.0.convs.0.norm.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.0.weight shape torch.Size([320, 256, 3, 3, 3])
encoder.stages.4.0.convs.0.all_modules.0.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.1.weight shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.1.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.4.0.convs.1.conv.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.norm.weight shape torch.Size([320])
encoder.stages.4.0.convs.1.norm.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.4.0.convs.1.all_modules.0.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.1.weight shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.1.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.0.conv.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.norm.weight shape torch.Size([320])
encoder.stages.5.0.convs.0.norm.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.0.all_modules.0.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.1.weight shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.1.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.1.conv.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.norm.weight shape torch.Size([320])
encoder.stages.5.0.convs.1.norm.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.1.all_modules.0.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.1.weight shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.0.0.convs.0.conv.weight shape torch.Size([32, 1, 3, 3, 3])
decoder.encoder.stages.0.0.convs.0.conv.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.norm.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.norm.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.0.weight shape torch.Size([32, 1, 3, 3, 3])
decoder.encoder.stages.0.0.convs.0.all_modules.0.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.1.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.1.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.encoder.stages.0.0.convs.1.conv.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.norm.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.norm.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.encoder.stages.0.0.convs.1.all_modules.0.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.1.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.1.bias shape torch.Size([32])
decoder.encoder.stages.1.0.convs.0.conv.weight shape torch.Size([64, 32, 3, 3, 3])
decoder.encoder.stages.1.0.convs.0.conv.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.norm.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.norm.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.0.weight shape torch.Size([64, 32, 3, 3, 3])
decoder.encoder.stages.1.0.convs.0.all_modules.0.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.1.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.1.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.encoder.stages.1.0.convs.1.conv.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.norm.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.norm.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.encoder.stages.1.0.convs.1.all_modules.0.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.1.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.1.bias shape torch.Size([64])
decoder.encoder.stages.2.0.convs.0.conv.weight shape torch.Size([128, 64, 3, 3, 3])
decoder.encoder.stages.2.0.convs.0.conv.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.norm.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.norm.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.0.weight shape torch.Size([128, 64, 3, 3, 3])
decoder.encoder.stages.2.0.convs.0.all_modules.0.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.1.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.1.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.encoder.stages.2.0.convs.1.conv.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.norm.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.norm.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.encoder.stages.2.0.convs.1.all_modules.0.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.1.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.1.bias shape torch.Size([128])
decoder.encoder.stages.3.0.convs.0.conv.weight shape torch.Size([256, 128, 3, 3, 3])
decoder.encoder.stages.3.0.convs.0.conv.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.norm.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.norm.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.0.weight shape torch.Size([256, 128, 3, 3, 3])
decoder.encoder.stages.3.0.convs.0.all_modules.0.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.1.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.1.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.encoder.stages.3.0.convs.1.conv.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.norm.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.norm.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.encoder.stages.3.0.convs.1.all_modules.0.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.1.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.1.bias shape torch.Size([256])
decoder.encoder.stages.4.0.convs.0.conv.weight shape torch.Size([320, 256, 3, 3, 3])
decoder.encoder.stages.4.0.convs.0.conv.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.norm.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.norm.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.0.weight shape torch.Size([320, 256, 3, 3, 3])
decoder.encoder.stages.4.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.4.0.convs.1.conv.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.norm.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.norm.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.4.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.0.conv.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.norm.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.norm.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.1.conv.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.norm.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.norm.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.stages.0.convs.0.conv.weight shape torch.Size([320, 640, 3, 3, 3])
decoder.stages.0.convs.0.conv.bias shape torch.Size([320])
decoder.stages.0.convs.0.norm.weight shape torch.Size([320])
decoder.stages.0.convs.0.norm.bias shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.0.weight shape torch.Size([320, 640, 3, 3, 3])
decoder.stages.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.stages.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.stages.0.convs.1.conv.bias shape torch.Size([320])
decoder.stages.0.convs.1.norm.weight shape torch.Size([320])
decoder.stages.0.convs.1.norm.bias shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.stages.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.stages.1.convs.0.conv.weight shape torch.Size([256, 512, 3, 3, 3])
decoder.stages.1.convs.0.conv.bias shape torch.Size([256])
decoder.stages.1.convs.0.norm.weight shape torch.Size([256])
decoder.stages.1.convs.0.norm.bias shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.0.weight shape torch.Size([256, 512, 3, 3, 3])
decoder.stages.1.convs.0.all_modules.0.bias shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.1.weight shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.1.bias shape torch.Size([256])
decoder.stages.1.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.stages.1.convs.1.conv.bias shape torch.Size([256])
decoder.stages.1.convs.1.norm.weight shape torch.Size([256])
decoder.stages.1.convs.1.norm.bias shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.stages.1.convs.1.all_modules.0.bias shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.1.weight shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.1.bias shape torch.Size([256])
decoder.stages.2.convs.0.conv.weight shape torch.Size([128, 256, 3, 3, 3])
decoder.stages.2.convs.0.conv.bias shape torch.Size([128])
decoder.stages.2.convs.0.norm.weight shape torch.Size([128])
decoder.stages.2.convs.0.norm.bias shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.0.weight shape torch.Size([128, 256, 3, 3, 3])
decoder.stages.2.convs.0.all_modules.0.bias shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.1.weight shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.1.bias shape torch.Size([128])
decoder.stages.2.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.stages.2.convs.1.conv.bias shape torch.Size([128])
decoder.stages.2.convs.1.norm.weight shape torch.Size([128])
decoder.stages.2.convs.1.norm.bias shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.stages.2.convs.1.all_modules.0.bias shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.1.weight shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.1.bias shape torch.Size([128])
decoder.stages.3.convs.0.conv.weight shape torch.Size([64, 128, 3, 3, 3])
decoder.stages.3.convs.0.conv.bias shape torch.Size([64])
decoder.stages.3.convs.0.norm.weight shape torch.Size([64])
decoder.stages.3.convs.0.norm.bias shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.0.weight shape torch.Size([64, 128, 3, 3, 3])
decoder.stages.3.convs.0.all_modules.0.bias shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.1.weight shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.1.bias shape torch.Size([64])
decoder.stages.3.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.stages.3.convs.1.conv.bias shape torch.Size([64])
decoder.stages.3.convs.1.norm.weight shape torch.Size([64])
decoder.stages.3.convs.1.norm.bias shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.stages.3.convs.1.all_modules.0.bias shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.1.weight shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.1.bias shape torch.Size([64])
decoder.stages.4.convs.0.conv.weight shape torch.Size([32, 64, 3, 3, 3])
decoder.stages.4.convs.0.conv.bias shape torch.Size([32])
decoder.stages.4.convs.0.norm.weight shape torch.Size([32])
decoder.stages.4.convs.0.norm.bias shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.0.weight shape torch.Size([32, 64, 3, 3, 3])
decoder.stages.4.convs.0.all_modules.0.bias shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.1.weight shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.1.bias shape torch.Size([32])
decoder.stages.4.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.stages.4.convs.1.conv.bias shape torch.Size([32])
decoder.stages.4.convs.1.norm.weight shape torch.Size([32])
decoder.stages.4.convs.1.norm.bias shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.stages.4.convs.1.all_modules.0.bias shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.1.weight shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.1.bias shape torch.Size([32])
decoder.transpconvs.0.weight shape torch.Size([320, 320, 1, 2, 2])
decoder.transpconvs.0.bias shape torch.Size([320])
decoder.transpconvs.1.weight shape torch.Size([320, 256, 2, 2, 2])
decoder.transpconvs.1.bias shape torch.Size([256])
decoder.transpconvs.2.weight shape torch.Size([256, 128, 2, 2, 2])
decoder.transpconvs.2.bias shape torch.Size([128])
decoder.transpconvs.3.weight shape torch.Size([128, 64, 2, 2, 2])
decoder.transpconvs.3.bias shape torch.Size([64])
decoder.transpconvs.4.weight shape torch.Size([64, 32, 2, 2, 2])
decoder.transpconvs.4.bias shape torch.Size([32])
################### Done ###################
2024-12-17 04:53:21.312784: do_dummy_2d_data_aug: True
2024-12-17 04:53:21.314971: Using splits from existing split file: /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset307_Sohee_Calcium_OCT_CrossValidation/splits_final.json
2024-12-17 04:53:21.316288: The split file contains 5 splits.
2024-12-17 04:53:21.317069: Desired fold for training: 2
2024-12-17 04:53:21.317984: This split has 6 training and 2 validation cases.
################### Loading pretrained weights from file  /home/gridsan/nchutisilp/projects/OpenAI-CLIP/clip_preivl_poststent_pretrained_nnUNet.pt ###################
Below is the list of overlapping blocks in pretrained model and nnUNet architecture:
encoder.stages.0.0.convs.0.conv.weight shape torch.Size([32, 1, 3, 3, 3])
encoder.stages.0.0.convs.0.conv.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.norm.weight shape torch.Size([32])
encoder.stages.0.0.convs.0.norm.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.0.weight shape torch.Size([32, 1, 3, 3, 3])
encoder.stages.0.0.convs.0.all_modules.0.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.1.weight shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.1.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
encoder.stages.0.0.convs.1.conv.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.norm.weight shape torch.Size([32])
encoder.stages.0.0.convs.1.norm.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
encoder.stages.0.0.convs.1.all_modules.0.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.1.weight shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.1.bias shape torch.Size([32])
encoder.stages.1.0.convs.0.conv.weight shape torch.Size([64, 32, 3, 3, 3])
encoder.stages.1.0.convs.0.conv.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.norm.weight shape torch.Size([64])
encoder.stages.1.0.convs.0.norm.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.0.weight shape torch.Size([64, 32, 3, 3, 3])
encoder.stages.1.0.convs.0.all_modules.0.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.1.weight shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.1.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
encoder.stages.1.0.convs.1.conv.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.norm.weight shape torch.Size([64])
encoder.stages.1.0.convs.1.norm.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
encoder.stages.1.0.convs.1.all_modules.0.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.1.weight shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.1.bias shape torch.Size([64])
encoder.stages.2.0.convs.0.conv.weight shape torch.Size([128, 64, 3, 3, 3])
encoder.stages.2.0.convs.0.conv.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.norm.weight shape torch.Size([128])
encoder.stages.2.0.convs.0.norm.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.0.weight shape torch.Size([128, 64, 3, 3, 3])
encoder.stages.2.0.convs.0.all_modules.0.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.1.weight shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.1.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
encoder.stages.2.0.convs.1.conv.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.norm.weight shape torch.Size([128])
encoder.stages.2.0.convs.1.norm.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
encoder.stages.2.0.convs.1.all_modules.0.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.1.weight shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.1.bias shape torch.Size([128])
encoder.stages.3.0.convs.0.conv.weight shape torch.Size([256, 128, 3, 3, 3])
encoder.stages.3.0.convs.0.conv.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.norm.weight shape torch.Size([256])
encoder.stages.3.0.convs.0.norm.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.0.weight shape torch.Size([256, 128, 3, 3, 3])
encoder.stages.3.0.convs.0.all_modules.0.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.1.weight shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.1.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
encoder.stages.3.0.convs.1.conv.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.norm.weight shape torch.Size([256])
encoder.stages.3.0.convs.1.norm.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
encoder.stages.3.0.convs.1.all_modules.0.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.1.weight shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.1.bias shape torch.Size([256])
encoder.stages.4.0.convs.0.conv.weight shape torch.Size([320, 256, 3, 3, 3])
encoder.stages.4.0.convs.0.conv.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.norm.weight shape torch.Size([320])
encoder.stages.4.0.convs.0.norm.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.0.weight shape torch.Size([320, 256, 3, 3, 3])
encoder.stages.4.0.convs.0.all_modules.0.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.1.weight shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.1.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.4.0.convs.1.conv.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.norm.weight shape torch.Size([320])
encoder.stages.4.0.convs.1.norm.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.4.0.convs.1.all_modules.0.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.1.weight shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.1.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.0.conv.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.norm.weight shape torch.Size([320])
encoder.stages.5.0.convs.0.norm.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.0.all_modules.0.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.1.weight shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.1.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.1.conv.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.norm.weight shape torch.Size([320])
encoder.stages.5.0.convs.1.norm.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.1.all_modules.0.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.1.weight shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.0.0.convs.0.conv.weight shape torch.Size([32, 1, 3, 3, 3])
decoder.encoder.stages.0.0.convs.0.conv.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.norm.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.norm.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.0.weight shape torch.Size([32, 1, 3, 3, 3])
decoder.encoder.stages.0.0.convs.0.all_modules.0.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.1.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.1.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.encoder.stages.0.0.convs.1.conv.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.norm.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.norm.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.encoder.stages.0.0.convs.1.all_modules.0.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.1.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.1.bias shape torch.Size([32])
decoder.encoder.stages.1.0.convs.0.conv.weight shape torch.Size([64, 32, 3, 3, 3])
decoder.encoder.stages.1.0.convs.0.conv.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.norm.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.norm.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.0.weight shape torch.Size([64, 32, 3, 3, 3])
decoder.encoder.stages.1.0.convs.0.all_modules.0.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.1.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.1.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.encoder.stages.1.0.convs.1.conv.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.norm.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.norm.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.encoder.stages.1.0.convs.1.all_modules.0.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.1.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.1.bias shape torch.Size([64])
decoder.encoder.stages.2.0.convs.0.conv.weight shape torch.Size([128, 64, 3, 3, 3])
decoder.encoder.stages.2.0.convs.0.conv.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.norm.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.norm.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.0.weight shape torch.Size([128, 64, 3, 3, 3])
decoder.encoder.stages.2.0.convs.0.all_modules.0.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.1.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.1.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.encoder.stages.2.0.convs.1.conv.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.norm.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.norm.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.encoder.stages.2.0.convs.1.all_modules.0.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.1.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.1.bias shape torch.Size([128])
decoder.encoder.stages.3.0.convs.0.conv.weight shape torch.Size([256, 128, 3, 3, 3])
decoder.encoder.stages.3.0.convs.0.conv.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.norm.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.norm.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.0.weight shape torch.Size([256, 128, 3, 3, 3])
decoder.encoder.stages.3.0.convs.0.all_modules.0.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.1.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.1.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.encoder.stages.3.0.convs.1.conv.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.norm.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.norm.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.encoder.stages.3.0.convs.1.all_modules.0.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.1.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.1.bias shape torch.Size([256])
decoder.encoder.stages.4.0.convs.0.conv.weight shape torch.Size([320, 256, 3, 3, 3])
decoder.encoder.stages.4.0.convs.0.conv.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.norm.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.norm.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.0.weight shape torch.Size([320, 256, 3, 3, 3])
decoder.encoder.stages.4.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.4.0.convs.1.conv.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.norm.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.norm.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.4.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.0.conv.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.norm.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.norm.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.1.conv.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.norm.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.norm.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.stages.0.convs.0.conv.weight shape torch.Size([320, 640, 3, 3, 3])
decoder.stages.0.convs.0.conv.bias shape torch.Size([320])
decoder.stages.0.convs.0.norm.weight shape torch.Size([320])
decoder.stages.0.convs.0.norm.bias shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.0.weight shape torch.Size([320, 640, 3, 3, 3])
decoder.stages.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.stages.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.stages.0.convs.1.conv.bias shape torch.Size([320])
decoder.stages.0.convs.1.norm.weight shape torch.Size([320])
decoder.stages.0.convs.1.norm.bias shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.stages.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.stages.1.convs.0.conv.weight shape torch.Size([256, 512, 3, 3, 3])
decoder.stages.1.convs.0.conv.bias shape torch.Size([256])
decoder.stages.1.convs.0.norm.weight shape torch.Size([256])
decoder.stages.1.convs.0.norm.bias shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.0.weight shape torch.Size([256, 512, 3, 3, 3])
decoder.stages.1.convs.0.all_modules.0.bias shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.1.weight shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.1.bias shape torch.Size([256])
decoder.stages.1.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.stages.1.convs.1.conv.bias shape torch.Size([256])
decoder.stages.1.convs.1.norm.weight shape torch.Size([256])
decoder.stages.1.convs.1.norm.bias shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.stages.1.convs.1.all_modules.0.bias shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.1.weight shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.1.bias shape torch.Size([256])
decoder.stages.2.convs.0.conv.weight shape torch.Size([128, 256, 3, 3, 3])
decoder.stages.2.convs.0.conv.bias shape torch.Size([128])
decoder.stages.2.convs.0.norm.weight shape torch.Size([128])
decoder.stages.2.convs.0.norm.bias shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.0.weight shape torch.Size([128, 256, 3, 3, 3])
decoder.stages.2.convs.0.all_modules.0.bias shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.1.weight shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.1.bias shape torch.Size([128])
decoder.stages.2.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.stages.2.convs.1.conv.bias shape torch.Size([128])
decoder.stages.2.convs.1.norm.weight shape torch.Size([128])
decoder.stages.2.convs.1.norm.bias shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.stages.2.convs.1.all_modules.0.bias shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.1.weight shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.1.bias shape torch.Size([128])
decoder.stages.3.convs.0.conv.weight shape torch.Size([64, 128, 3, 3, 3])
decoder.stages.3.convs.0.conv.bias shape torch.Size([64])
decoder.stages.3.convs.0.norm.weight shape torch.Size([64])
decoder.stages.3.convs.0.norm.bias shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.0.weight shape torch.Size([64, 128, 3, 3, 3])
decoder.stages.3.convs.0.all_modules.0.bias shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.1.weight shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.1.bias shape torch.Size([64])
decoder.stages.3.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.stages.3.convs.1.conv.bias shape torch.Size([64])
decoder.stages.3.convs.1.norm.weight shape torch.Size([64])
decoder.stages.3.convs.1.norm.bias shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.stages.3.convs.1.all_modules.0.bias shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.1.weight shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.1.bias shape torch.Size([64])
decoder.stages.4.convs.0.conv.weight shape torch.Size([32, 64, 3, 3, 3])
decoder.stages.4.convs.0.conv.bias shape torch.Size([32])
decoder.stages.4.convs.0.norm.weight shape torch.Size([32])
decoder.stages.4.convs.0.norm.bias shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.0.weight shape torch.Size([32, 64, 3, 3, 3])
decoder.stages.4.convs.0.all_modules.0.bias shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.1.weight shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.1.bias shape torch.Size([32])
decoder.stages.4.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.stages.4.convs.1.conv.bias shape torch.Size([32])
decoder.stages.4.convs.1.norm.weight shape torch.Size([32])
decoder.stages.4.convs.1.norm.bias shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.stages.4.convs.1.all_modules.0.bias shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.1.weight shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.1.bias shape torch.Size([32])
decoder.transpconvs.0.weight shape torch.Size([320, 320, 1, 2, 2])
decoder.transpconvs.0.bias shape torch.Size([320])
decoder.transpconvs.1.weight shape torch.Size([320, 256, 2, 2, 2])
decoder.transpconvs.1.bias shape torch.Size([256])
decoder.transpconvs.2.weight shape torch.Size([256, 128, 2, 2, 2])
decoder.transpconvs.2.bias shape torch.Size([128])
decoder.transpconvs.3.weight shape torch.Size([128, 64, 2, 2, 2])
decoder.transpconvs.3.bias shape torch.Size([64])
decoder.transpconvs.4.weight shape torch.Size([64, 32, 2, 2, 2])
decoder.transpconvs.4.bias shape torch.Size([32])
################### Done ###################
2024-12-17 04:53:21.312932: do_dummy_2d_data_aug: True
2024-12-17 04:53:21.315237: Using splits from existing split file: /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset307_Sohee_Calcium_OCT_CrossValidation/splits_final.json
2024-12-17 04:53:21.316786: The split file contains 5 splits.
2024-12-17 04:53:21.317748: Desired fold for training: 3
2024-12-17 04:53:21.318571: This split has 7 training and 1 validation cases.
using pin_memory on device 0
using pin_memory on device 0
using pin_memory on device 0

This is the configuration used by this training:
Configuration name: 3d_32x160x128_b10
 {'data_identifier': 'nnUNetPlans_3d_fullres', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 10, 'patch_size': [32, 160, 128], 'median_image_size_in_voxels': [375.0, 498.0, 498.0], 'spacing': [1.0, 1.0, 1.0], 'normalization_schemes': ['ZScoreNormalization'], 'use_mask_for_norm': [False], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'architecture': {'network_class_name': 'dynamic_network_architectures.architectures.unet.PlainConvUNet', 'arch_kwargs': {'n_stages': 6, 'features_per_stage': [32, 64, 128, 256, 320, 320], 'conv_op': 'torch.nn.modules.conv.Conv3d', 'kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]], 'strides': [[1, 1, 1], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [1, 2, 2]], 'n_conv_per_stage': [2, 2, 2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2, 2, 2], 'conv_bias': True, 'norm_op': 'torch.nn.modules.instancenorm.InstanceNorm3d', 'norm_op_kwargs': {'eps': 1e-05, 'affine': True}, 'dropout_op': None, 'dropout_op_kwargs': None, 'nonlin': 'torch.nn.LeakyReLU', 'nonlin_kwargs': {'inplace': True}, 'deep_supervision': True}, '_kw_requires_import': ['conv_op', 'norm_op', 'dropout_op', 'nonlin']}, 'batch_dice': True, 'inherits_from': '3d_fullres'} 

These are the global plan.json settings:
 {'dataset_name': 'Dataset307_Sohee_Calcium_OCT_CrossValidation', 'plans_name': 'nnUNetPlans', 'original_median_spacing_after_transp': [1.0, 1.0, 1.0], 'original_median_shape_after_transp': [375, 498, 498], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 0.4977976381778717, 'mean': 0.13507525622844696, 'median': 0.09599608182907104, 'min': 0.0, 'percentile_00_5': 0.014754901640117168, 'percentile_99_5': 0.4977976381778717, 'std': 0.12025152146816254}}} 

2024-12-17 04:53:22.582844: unpacking dataset...
using pin_memory on device 0

This is the configuration used by this training:
Configuration name: 3d_32x160x128_b10
 {'data_identifier': 'nnUNetPlans_3d_fullres', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 10, 'patch_size': [32, 160, 128], 'median_image_size_in_voxels': [375.0, 498.0, 498.0], 'spacing': [1.0, 1.0, 1.0], 'normalization_schemes': ['ZScoreNormalization'], 'use_mask_for_norm': [False], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'architecture': {'network_class_name': 'dynamic_network_architectures.architectures.unet.PlainConvUNet', 'arch_kwargs': {'n_stages': 6, 'features_per_stage': [32, 64, 128, 256, 320, 320], 'conv_op': 'torch.nn.modules.conv.Conv3d', 'kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]], 'strides': [[1, 1, 1], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [1, 2, 2]], 'n_conv_per_stage': [2, 2, 2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2, 2, 2], 'conv_bias': True, 'norm_op': 'torch.nn.modules.instancenorm.InstanceNorm3d', 'norm_op_kwargs': {'eps': 1e-05, 'affine': True}, 'dropout_op': None, 'dropout_op_kwargs': None, 'nonlin': 'torch.nn.LeakyReLU', 'nonlin_kwargs': {'inplace': True}, 'deep_supervision': True}, '_kw_requires_import': ['conv_op', 'norm_op', 'dropout_op', 'nonlin']}, 'batch_dice': True, 'inherits_from': '3d_fullres'} 

These are the global plan.json settings:
 {'dataset_name': 'Dataset307_Sohee_Calcium_OCT_CrossValidation', 'plans_name': 'nnUNetPlans', 'original_median_spacing_after_transp': [1.0, 1.0, 1.0], 'original_median_shape_after_transp': [375, 498, 498], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 0.4977976381778717, 'mean': 0.13507525622844696, 'median': 0.09599608182907104, 'min': 0.0, 'percentile_00_5': 0.014754901640117168, 'percentile_99_5': 0.4977976381778717, 'std': 0.12025152146816254}}} 

2024-12-17 04:53:22.779276: unpacking dataset...
2024-12-17 04:53:27.004165: unpacking done...
2024-12-17 04:53:27.012496: Unable to plot network architecture: nnUNet_compile is enabled!
2024-12-17 04:53:27.120493: 
2024-12-17 04:53:27.121827: Epoch 0
2024-12-17 04:53:27.123249: Current learning rate: 0.01
2024-12-17 04:55:47.073524: Validation loss improved from 1000.00000 to -0.19292! Patience: 0/50
2024-12-17 04:55:47.074627: train_loss -0.0791
2024-12-17 04:55:47.075813: val_loss -0.1929
2024-12-17 04:55:47.076786: Pseudo dice [0.5131]
2024-12-17 04:55:47.077704: Epoch time: 139.96 s
2024-12-17 04:55:47.078697: Yayy! New best EMA pseudo Dice: 0.5131
2024-12-17 04:55:48.673805: 
2024-12-17 04:55:48.675473: Epoch 1
2024-12-17 04:55:48.677059: Current learning rate: 0.00994
2024-12-17 04:57:17.328342: Validation loss did not improve from -0.19292. Patience: 1/50
2024-12-17 04:57:17.329598: train_loss -0.2388
2024-12-17 04:57:17.330811: val_loss -0.0355
2024-12-17 04:57:17.331766: Pseudo dice [0.3359]
2024-12-17 04:57:17.332741: Epoch time: 88.66 s
2024-12-17 04:57:18.640155: 
2024-12-17 04:57:18.642033: Epoch 2
2024-12-17 04:57:18.643225: Current learning rate: 0.00988
2024-12-17 04:58:47.893006: Validation loss improved from -0.19292 to -0.27322! Patience: 1/50
2024-12-17 04:58:47.894206: train_loss -0.2979
2024-12-17 04:58:47.895040: val_loss -0.2732
2024-12-17 04:58:47.895769: Pseudo dice [0.5965]
2024-12-17 04:58:47.896593: Epoch time: 89.26 s
2024-12-17 04:58:49.209964: 
2024-12-17 04:58:49.211898: Epoch 3
2024-12-17 04:58:49.212879: Current learning rate: 0.00982
2024-12-17 05:00:18.586071: Validation loss improved from -0.27322 to -0.29177! Patience: 0/50
2024-12-17 05:00:18.587282: train_loss -0.3079
2024-12-17 05:00:18.588376: val_loss -0.2918
2024-12-17 05:00:18.589301: Pseudo dice [0.5849]
2024-12-17 05:00:18.590091: Epoch time: 89.38 s
2024-12-17 05:00:18.590875: Yayy! New best EMA pseudo Dice: 0.5134
2024-12-17 05:00:20.256540: 
2024-12-17 05:00:20.258266: Epoch 4
2024-12-17 05:00:20.259465: Current learning rate: 0.00976
2024-12-17 05:01:49.581182: Validation loss improved from -0.29177 to -0.36551! Patience: 0/50
2024-12-17 05:01:49.582621: train_loss -0.3568
2024-12-17 05:01:49.583535: val_loss -0.3655
2024-12-17 05:01:49.584278: Pseudo dice [0.6639]
2024-12-17 05:01:49.585147: Epoch time: 89.33 s
2024-12-17 05:01:50.204659: Yayy! New best EMA pseudo Dice: 0.5285
2024-12-17 05:01:51.868518: 
2024-12-17 05:01:51.870297: Epoch 5
2024-12-17 05:01:51.871100: Current learning rate: 0.0097
2024-12-17 05:03:21.208836: Validation loss did not improve from -0.36551. Patience: 1/50
2024-12-17 05:03:21.210030: train_loss -0.3712
2024-12-17 05:03:21.210956: val_loss -0.2322
2024-12-17 05:03:21.211739: Pseudo dice [0.5766]
2024-12-17 05:03:21.212521: Epoch time: 89.34 s
2024-12-17 05:03:21.213211: Yayy! New best EMA pseudo Dice: 0.5333
2024-12-17 05:03:22.761915: 
2024-12-17 05:03:22.763876: Epoch 6
2024-12-17 05:03:22.764925: Current learning rate: 0.00964
2024-12-17 05:04:52.088511: Validation loss did not improve from -0.36551. Patience: 2/50
2024-12-17 05:04:52.089654: train_loss -0.4038
2024-12-17 05:04:52.090535: val_loss -0.302
2024-12-17 05:04:52.091330: Pseudo dice [0.6157]
2024-12-17 05:04:52.092137: Epoch time: 89.33 s
2024-12-17 05:04:52.092795: Yayy! New best EMA pseudo Dice: 0.5415
2024-12-17 05:04:53.750166: 
2024-12-17 05:04:53.751662: Epoch 7
2024-12-17 05:04:53.752700: Current learning rate: 0.00958
2024-12-17 05:06:22.999688: Validation loss did not improve from -0.36551. Patience: 3/50
2024-12-17 05:06:23.000775: train_loss -0.4158
2024-12-17 05:06:23.001961: val_loss -0.3415
2024-12-17 05:06:23.002811: Pseudo dice [0.6536]
2024-12-17 05:06:23.003739: Epoch time: 89.25 s
2024-12-17 05:06:23.004617: Yayy! New best EMA pseudo Dice: 0.5528
2024-12-17 05:06:24.962935: 
2024-12-17 05:06:24.965004: Epoch 8
2024-12-17 05:06:24.965957: Current learning rate: 0.00952
2024-12-17 05:07:54.621162: Validation loss did not improve from -0.36551. Patience: 4/50
2024-12-17 05:07:54.622359: train_loss -0.4332
2024-12-17 05:07:54.623602: val_loss -0.2493
2024-12-17 05:07:54.624732: Pseudo dice [0.5754]
2024-12-17 05:07:54.625972: Epoch time: 89.66 s
2024-12-17 05:07:54.626960: Yayy! New best EMA pseudo Dice: 0.555
2024-12-17 05:07:56.287036: 
2024-12-17 05:07:56.288635: Epoch 9
2024-12-17 05:07:56.289667: Current learning rate: 0.00946
2024-12-17 05:09:25.956911: Validation loss did not improve from -0.36551. Patience: 5/50
2024-12-17 05:09:25.958049: train_loss -0.4579
2024-12-17 05:09:25.959250: val_loss -0.301
2024-12-17 05:09:25.959970: Pseudo dice [0.6054]
2024-12-17 05:09:25.960804: Epoch time: 89.67 s
2024-12-17 05:09:26.313763: Yayy! New best EMA pseudo Dice: 0.5601
2024-12-17 05:09:27.846641: 
2024-12-17 05:09:27.851341: Epoch 10
2024-12-17 05:09:27.852434: Current learning rate: 0.0094
2024-12-17 05:10:57.493677: Validation loss did not improve from -0.36551. Patience: 6/50
2024-12-17 05:10:57.494885: train_loss -0.4703
2024-12-17 05:10:57.495754: val_loss -0.1878
2024-12-17 05:10:57.496413: Pseudo dice [0.5174]
2024-12-17 05:10:57.497171: Epoch time: 89.65 s
2024-12-17 05:10:58.750107: 
2024-12-17 05:10:58.751704: Epoch 11
2024-12-17 05:10:58.752570: Current learning rate: 0.00934
2024-12-17 05:12:28.286487: Validation loss did not improve from -0.36551. Patience: 7/50
2024-12-17 05:12:28.287756: train_loss -0.4851
2024-12-17 05:12:28.288970: val_loss -0.3176
2024-12-17 05:12:28.289855: Pseudo dice [0.6236]
2024-12-17 05:12:28.290820: Epoch time: 89.54 s
2024-12-17 05:12:28.291704: Yayy! New best EMA pseudo Dice: 0.5626
2024-12-17 05:12:29.877825: 
2024-12-17 05:12:29.879308: Epoch 12
2024-12-17 05:12:29.880233: Current learning rate: 0.00928
2024-12-17 05:13:59.443151: Validation loss did not improve from -0.36551. Patience: 8/50
2024-12-17 05:13:59.444474: train_loss -0.4783
2024-12-17 05:13:59.445354: val_loss -0.2809
2024-12-17 05:13:59.446079: Pseudo dice [0.5982]
2024-12-17 05:13:59.446949: Epoch time: 89.57 s
2024-12-17 05:13:59.447832: Yayy! New best EMA pseudo Dice: 0.5661
2024-12-17 05:14:01.070276: 
2024-12-17 05:14:01.072440: Epoch 13
2024-12-17 05:14:01.073577: Current learning rate: 0.00922
2024-12-17 05:15:30.590155: Validation loss improved from -0.36551 to -0.40590! Patience: 8/50
2024-12-17 05:15:30.591176: train_loss -0.5051
2024-12-17 05:15:30.592000: val_loss -0.4059
2024-12-17 05:15:30.592687: Pseudo dice [0.6814]
2024-12-17 05:15:30.593361: Epoch time: 89.52 s
2024-12-17 05:15:30.594110: Yayy! New best EMA pseudo Dice: 0.5777
2024-12-17 05:15:32.224371: 
2024-12-17 05:15:32.225635: Epoch 14
2024-12-17 05:15:32.226398: Current learning rate: 0.00916
2024-12-17 05:17:01.792171: Validation loss did not improve from -0.40590. Patience: 1/50
2024-12-17 05:17:01.793452: train_loss -0.4953
2024-12-17 05:17:01.794709: val_loss -0.3137
2024-12-17 05:17:01.795650: Pseudo dice [0.6287]
2024-12-17 05:17:01.796556: Epoch time: 89.57 s
2024-12-17 05:17:02.154616: Yayy! New best EMA pseudo Dice: 0.5828
2024-12-17 05:17:03.739734: 
2024-12-17 05:17:03.741245: Epoch 15
2024-12-17 05:17:03.742268: Current learning rate: 0.0091
2024-12-17 05:18:33.274091: Validation loss did not improve from -0.40590. Patience: 2/50
2024-12-17 05:18:33.275056: train_loss -0.5171
2024-12-17 05:18:33.275845: val_loss -0.3057
2024-12-17 05:18:33.276611: Pseudo dice [0.6131]
2024-12-17 05:18:33.277261: Epoch time: 89.54 s
2024-12-17 05:18:33.278034: Yayy! New best EMA pseudo Dice: 0.5858
2024-12-17 05:18:34.871014: 
2024-12-17 05:18:34.872323: Epoch 16
2024-12-17 05:18:34.873349: Current learning rate: 0.00903
2024-12-17 05:20:04.653205: Validation loss did not improve from -0.40590. Patience: 3/50
2024-12-17 05:20:04.654340: train_loss -0.5185
2024-12-17 05:20:04.655555: val_loss -0.3316
2024-12-17 05:20:04.656575: Pseudo dice [0.6411]
2024-12-17 05:20:04.657533: Epoch time: 89.78 s
2024-12-17 05:20:04.658405: Yayy! New best EMA pseudo Dice: 0.5913
2024-12-17 05:20:06.298102: 
2024-12-17 05:20:06.300042: Epoch 17
2024-12-17 05:20:06.301322: Current learning rate: 0.00897
2024-12-17 05:21:36.039325: Validation loss did not improve from -0.40590. Patience: 4/50
2024-12-17 05:21:36.040239: train_loss -0.5366
2024-12-17 05:21:36.041189: val_loss -0.3816
2024-12-17 05:21:36.041937: Pseudo dice [0.6656]
2024-12-17 05:21:36.042602: Epoch time: 89.74 s
2024-12-17 05:21:36.043354: Yayy! New best EMA pseudo Dice: 0.5988
2024-12-17 05:21:37.628811: 
2024-12-17 05:21:37.630420: Epoch 18
2024-12-17 05:21:37.631406: Current learning rate: 0.00891
2024-12-17 05:23:07.381970: Validation loss improved from -0.40590 to -0.41233! Patience: 4/50
2024-12-17 05:23:07.383028: train_loss -0.534
2024-12-17 05:23:07.383764: val_loss -0.4123
2024-12-17 05:23:07.384631: Pseudo dice [0.6788]
2024-12-17 05:23:07.385382: Epoch time: 89.76 s
2024-12-17 05:23:07.386043: Yayy! New best EMA pseudo Dice: 0.6068
2024-12-17 05:23:09.294346: 
2024-12-17 05:23:09.296227: Epoch 19
2024-12-17 05:23:09.297081: Current learning rate: 0.00885
2024-12-17 05:24:39.039124: Validation loss did not improve from -0.41233. Patience: 1/50
2024-12-17 05:24:39.040129: train_loss -0.5473
2024-12-17 05:24:39.041342: val_loss -0.3683
2024-12-17 05:24:39.042186: Pseudo dice [0.6558]
2024-12-17 05:24:39.043022: Epoch time: 89.75 s
2024-12-17 05:24:39.393804: Yayy! New best EMA pseudo Dice: 0.6117
2024-12-17 05:24:41.036830: 
2024-12-17 05:24:41.038885: Epoch 20
2024-12-17 05:24:41.039757: Current learning rate: 0.00879
2024-12-17 05:26:10.715411: Validation loss improved from -0.41233 to -0.44201! Patience: 1/50
2024-12-17 05:26:10.716879: train_loss -0.5566
2024-12-17 05:26:10.717851: val_loss -0.442
2024-12-17 05:26:10.718640: Pseudo dice [0.7077]
2024-12-17 05:26:10.719366: Epoch time: 89.68 s
2024-12-17 05:26:10.720070: Yayy! New best EMA pseudo Dice: 0.6213
2024-12-17 05:26:12.336413: 
2024-12-17 05:26:12.337930: Epoch 21
2024-12-17 05:26:12.338641: Current learning rate: 0.00873
2024-12-17 05:27:42.049333: Validation loss did not improve from -0.44201. Patience: 1/50
2024-12-17 05:27:42.050574: train_loss -0.5629
2024-12-17 05:27:42.051533: val_loss -0.3521
2024-12-17 05:27:42.052317: Pseudo dice [0.6488]
2024-12-17 05:27:42.053233: Epoch time: 89.72 s
2024-12-17 05:27:42.053878: Yayy! New best EMA pseudo Dice: 0.624
2024-12-17 05:27:43.570389: 
2024-12-17 05:27:43.571391: Epoch 22
2024-12-17 05:27:43.572279: Current learning rate: 0.00867
2024-12-17 05:29:13.383633: Validation loss did not improve from -0.44201. Patience: 2/50
2024-12-17 05:29:13.384670: train_loss -0.5531
2024-12-17 05:29:13.385478: val_loss -0.4292
2024-12-17 05:29:13.386173: Pseudo dice [0.6948]
2024-12-17 05:29:13.386777: Epoch time: 89.82 s
2024-12-17 05:29:13.387420: Yayy! New best EMA pseudo Dice: 0.6311
2024-12-17 05:29:14.940849: 
2024-12-17 05:29:14.942589: Epoch 23
2024-12-17 05:29:14.943403: Current learning rate: 0.00861
2024-12-17 05:30:44.811042: Validation loss did not improve from -0.44201. Patience: 3/50
2024-12-17 05:30:44.812380: train_loss -0.5681
2024-12-17 05:30:44.813244: val_loss -0.3152
2024-12-17 05:30:44.813982: Pseudo dice [0.6338]
2024-12-17 05:30:44.814669: Epoch time: 89.87 s
2024-12-17 05:30:44.815287: Yayy! New best EMA pseudo Dice: 0.6314
2024-12-17 05:30:46.317556: 
2024-12-17 05:30:46.319283: Epoch 24
2024-12-17 05:30:46.320332: Current learning rate: 0.00855
2024-12-17 05:32:16.275267: Validation loss did not improve from -0.44201. Patience: 4/50
2024-12-17 05:32:16.276694: train_loss -0.5686
2024-12-17 05:32:16.277850: val_loss -0.3745
2024-12-17 05:32:16.278587: Pseudo dice [0.6548]
2024-12-17 05:32:16.279365: Epoch time: 89.96 s
2024-12-17 05:32:16.618056: Yayy! New best EMA pseudo Dice: 0.6337
2024-12-17 05:32:18.132735: 
2024-12-17 05:32:18.133824: Epoch 25
2024-12-17 05:32:18.134638: Current learning rate: 0.00849
2024-12-17 05:33:48.114469: Validation loss did not improve from -0.44201. Patience: 5/50
2024-12-17 05:33:48.115643: train_loss -0.5765
2024-12-17 05:33:48.116594: val_loss -0.3792
2024-12-17 05:33:48.117303: Pseudo dice [0.6496]
2024-12-17 05:33:48.118116: Epoch time: 89.98 s
2024-12-17 05:33:48.118738: Yayy! New best EMA pseudo Dice: 0.6353
2024-12-17 05:33:49.672125: 
2024-12-17 05:33:49.673506: Epoch 26
2024-12-17 05:33:49.674265: Current learning rate: 0.00843
2024-12-17 05:35:19.643693: Validation loss did not improve from -0.44201. Patience: 6/50
2024-12-17 05:35:19.644971: train_loss -0.5649
2024-12-17 05:35:19.646071: val_loss -0.3901
2024-12-17 05:35:19.647186: Pseudo dice [0.6678]
2024-12-17 05:35:19.648205: Epoch time: 89.97 s
2024-12-17 05:35:19.649076: Yayy! New best EMA pseudo Dice: 0.6386
2024-12-17 05:35:21.205487: 
2024-12-17 05:35:21.206636: Epoch 27
2024-12-17 05:35:21.207752: Current learning rate: 0.00836
2024-12-17 05:36:51.192188: Validation loss did not improve from -0.44201. Patience: 7/50
2024-12-17 05:36:51.193558: train_loss -0.5874
2024-12-17 05:36:51.194959: val_loss -0.3619
2024-12-17 05:36:51.196064: Pseudo dice [0.6519]
2024-12-17 05:36:51.197156: Epoch time: 89.99 s
2024-12-17 05:36:51.198260: Yayy! New best EMA pseudo Dice: 0.6399
2024-12-17 05:36:52.844804: 
2024-12-17 05:36:52.846753: Epoch 28
2024-12-17 05:36:52.848172: Current learning rate: 0.0083
2024-12-17 05:38:22.823111: Validation loss did not improve from -0.44201. Patience: 8/50
2024-12-17 05:38:22.824473: train_loss -0.5873
2024-12-17 05:38:22.825470: val_loss -0.3461
2024-12-17 05:38:22.826192: Pseudo dice [0.6397]
2024-12-17 05:38:22.827033: Epoch time: 89.98 s
2024-12-17 05:38:24.367281: 
2024-12-17 05:38:24.368882: Epoch 29
2024-12-17 05:38:24.369610: Current learning rate: 0.00824
2024-12-17 05:39:54.137079: Validation loss did not improve from -0.44201. Patience: 9/50
2024-12-17 05:39:54.138054: train_loss -0.5959
2024-12-17 05:39:54.139065: val_loss -0.3227
2024-12-17 05:39:54.139790: Pseudo dice [0.6223]
2024-12-17 05:39:54.140481: Epoch time: 89.77 s
2024-12-17 05:39:55.695881: 
2024-12-17 05:39:55.697327: Epoch 30
2024-12-17 05:39:55.698121: Current learning rate: 0.00818
2024-12-17 05:41:25.335755: Validation loss did not improve from -0.44201. Patience: 10/50
2024-12-17 05:41:25.336781: train_loss -0.5952
2024-12-17 05:41:25.337774: val_loss -0.3836
2024-12-17 05:41:25.338392: Pseudo dice [0.6672]
2024-12-17 05:41:25.338991: Epoch time: 89.64 s
2024-12-17 05:41:25.339701: Yayy! New best EMA pseudo Dice: 0.641
2024-12-17 05:41:26.954834: 
2024-12-17 05:41:26.956308: Epoch 31
2024-12-17 05:41:26.957000: Current learning rate: 0.00812
2024-12-17 05:42:56.533706: Validation loss did not improve from -0.44201. Patience: 11/50
2024-12-17 05:42:56.534481: train_loss -0.6017
2024-12-17 05:42:56.535420: val_loss -0.4418
2024-12-17 05:42:56.536128: Pseudo dice [0.7055]
2024-12-17 05:42:56.536841: Epoch time: 89.58 s
2024-12-17 05:42:56.537525: Yayy! New best EMA pseudo Dice: 0.6475
2024-12-17 05:42:58.161835: 
2024-12-17 05:42:58.163466: Epoch 32
2024-12-17 05:42:58.164680: Current learning rate: 0.00806
2024-12-17 05:44:27.762193: Validation loss did not improve from -0.44201. Patience: 12/50
2024-12-17 05:44:27.763469: train_loss -0.606
2024-12-17 05:44:27.764782: val_loss -0.3827
2024-12-17 05:44:27.765593: Pseudo dice [0.6631]
2024-12-17 05:44:27.766398: Epoch time: 89.6 s
2024-12-17 05:44:27.767204: Yayy! New best EMA pseudo Dice: 0.649
2024-12-17 05:44:29.365868: 
2024-12-17 05:44:29.367294: Epoch 33
2024-12-17 05:44:29.368114: Current learning rate: 0.008
2024-12-17 05:45:59.003464: Validation loss did not improve from -0.44201. Patience: 13/50
2024-12-17 05:45:59.004719: train_loss -0.605
2024-12-17 05:45:59.005656: val_loss -0.3827
2024-12-17 05:45:59.006483: Pseudo dice [0.6631]
2024-12-17 05:45:59.007188: Epoch time: 89.64 s
2024-12-17 05:45:59.007974: Yayy! New best EMA pseudo Dice: 0.6504
2024-12-17 05:46:00.580279: 
2024-12-17 05:46:00.582182: Epoch 34
2024-12-17 05:46:00.582911: Current learning rate: 0.00793
2024-12-17 05:47:30.248035: Validation loss did not improve from -0.44201. Patience: 14/50
2024-12-17 05:47:30.249265: train_loss -0.6092
2024-12-17 05:47:30.250566: val_loss -0.4136
2024-12-17 05:47:30.251502: Pseudo dice [0.685]
2024-12-17 05:47:30.252377: Epoch time: 89.67 s
2024-12-17 05:47:30.612077: Yayy! New best EMA pseudo Dice: 0.6539
2024-12-17 05:47:32.234329: 
2024-12-17 05:47:32.236072: Epoch 35
2024-12-17 05:47:32.237135: Current learning rate: 0.00787
2024-12-17 05:49:01.823664: Validation loss did not improve from -0.44201. Patience: 15/50
2024-12-17 05:49:01.824601: train_loss -0.6077
2024-12-17 05:49:01.825597: val_loss -0.4221
2024-12-17 05:49:01.826605: Pseudo dice [0.6931]
2024-12-17 05:49:01.827668: Epoch time: 89.59 s
2024-12-17 05:49:01.828651: Yayy! New best EMA pseudo Dice: 0.6578
2024-12-17 05:49:03.443053: 
2024-12-17 05:49:03.444592: Epoch 36
2024-12-17 05:49:03.445781: Current learning rate: 0.00781
2024-12-17 05:50:33.122487: Validation loss did not improve from -0.44201. Patience: 16/50
2024-12-17 05:50:33.123560: train_loss -0.6068
2024-12-17 05:50:33.124582: val_loss -0.3632
2024-12-17 05:50:33.125232: Pseudo dice [0.6563]
2024-12-17 05:50:33.125983: Epoch time: 89.68 s
2024-12-17 05:50:34.384860: 
2024-12-17 05:50:34.386863: Epoch 37
2024-12-17 05:50:34.387626: Current learning rate: 0.00775
2024-12-17 05:52:04.135295: Validation loss did not improve from -0.44201. Patience: 17/50
2024-12-17 05:52:04.136508: train_loss -0.6151
2024-12-17 05:52:04.137456: val_loss -0.4153
2024-12-17 05:52:04.138352: Pseudo dice [0.6838]
2024-12-17 05:52:04.139029: Epoch time: 89.75 s
2024-12-17 05:52:04.139822: Yayy! New best EMA pseudo Dice: 0.6603
2024-12-17 05:52:05.715938: 
2024-12-17 05:52:05.717304: Epoch 38
2024-12-17 05:52:05.718022: Current learning rate: 0.00769
2024-12-17 05:53:35.405627: Validation loss did not improve from -0.44201. Patience: 18/50
2024-12-17 05:53:35.406822: train_loss -0.6153
2024-12-17 05:53:35.407851: val_loss -0.2858
2024-12-17 05:53:35.408645: Pseudo dice [0.6169]
2024-12-17 05:53:35.409544: Epoch time: 89.69 s
2024-12-17 05:53:36.949161: 
2024-12-17 05:53:36.951086: Epoch 39
2024-12-17 05:53:36.951931: Current learning rate: 0.00763
2024-12-17 05:55:06.618447: Validation loss improved from -0.44201 to -0.47055! Patience: 18/50
2024-12-17 05:55:06.619726: train_loss -0.6198
2024-12-17 05:55:06.620633: val_loss -0.4705
2024-12-17 05:55:06.621475: Pseudo dice [0.7109]
2024-12-17 05:55:06.622295: Epoch time: 89.67 s
2024-12-17 05:55:06.982644: Yayy! New best EMA pseudo Dice: 0.6614
2024-12-17 05:55:08.619831: 
2024-12-17 05:55:08.620967: Epoch 40
2024-12-17 05:55:08.621932: Current learning rate: 0.00756
2024-12-17 05:56:38.287629: Validation loss did not improve from -0.47055. Patience: 1/50
2024-12-17 05:56:38.289026: train_loss -0.6224
2024-12-17 05:56:38.289912: val_loss -0.3895
2024-12-17 05:56:38.290608: Pseudo dice [0.6763]
2024-12-17 05:56:38.291369: Epoch time: 89.67 s
2024-12-17 05:56:38.292138: Yayy! New best EMA pseudo Dice: 0.6629
2024-12-17 05:56:39.940400: 
2024-12-17 05:56:39.942089: Epoch 41
2024-12-17 05:56:39.942882: Current learning rate: 0.0075
2024-12-17 05:58:09.600456: Validation loss did not improve from -0.47055. Patience: 2/50
2024-12-17 05:58:09.601725: train_loss -0.6254
2024-12-17 05:58:09.602740: val_loss -0.3457
2024-12-17 05:58:09.603469: Pseudo dice [0.6354]
2024-12-17 05:58:09.604102: Epoch time: 89.66 s
2024-12-17 05:58:10.803208: 
2024-12-17 05:58:10.804937: Epoch 42
2024-12-17 05:58:10.806105: Current learning rate: 0.00744
2024-12-17 05:59:40.562535: Validation loss did not improve from -0.47055. Patience: 3/50
2024-12-17 05:59:40.567134: train_loss -0.6258
2024-12-17 05:59:40.569913: val_loss -0.4551
2024-12-17 05:59:40.571030: Pseudo dice [0.7176]
2024-12-17 05:59:40.571959: Epoch time: 89.76 s
2024-12-17 05:59:40.572763: Yayy! New best EMA pseudo Dice: 0.6659
2024-12-17 05:59:42.174333: 
2024-12-17 05:59:42.176046: Epoch 43
2024-12-17 05:59:42.176908: Current learning rate: 0.00738
2024-12-17 06:01:11.797769: Validation loss did not improve from -0.47055. Patience: 4/50
2024-12-17 06:01:11.799181: train_loss -0.6241
2024-12-17 06:01:11.800408: val_loss -0.403
2024-12-17 06:01:11.801455: Pseudo dice [0.6868]
2024-12-17 06:01:11.802456: Epoch time: 89.63 s
2024-12-17 06:01:11.803470: Yayy! New best EMA pseudo Dice: 0.668
2024-12-17 06:01:13.359461: 
2024-12-17 06:01:13.361045: Epoch 44
2024-12-17 06:01:13.361759: Current learning rate: 0.00732
2024-12-17 06:02:43.108117: Validation loss did not improve from -0.47055. Patience: 5/50
2024-12-17 06:02:43.109488: train_loss -0.6354
2024-12-17 06:02:43.110778: val_loss -0.4083
2024-12-17 06:02:43.112032: Pseudo dice [0.6907]
2024-12-17 06:02:43.113048: Epoch time: 89.75 s
2024-12-17 06:02:43.466334: Yayy! New best EMA pseudo Dice: 0.6703
2024-12-17 06:02:44.981069: 
2024-12-17 06:02:44.982740: Epoch 45
2024-12-17 06:02:44.983813: Current learning rate: 0.00725
2024-12-17 06:04:14.716346: Validation loss did not improve from -0.47055. Patience: 6/50
2024-12-17 06:04:14.717569: train_loss -0.6435
2024-12-17 06:04:14.718672: val_loss -0.3863
2024-12-17 06:04:14.719463: Pseudo dice [0.6627]
2024-12-17 06:04:14.720088: Epoch time: 89.74 s
2024-12-17 06:04:15.921200: 
2024-12-17 06:04:15.923014: Epoch 46
2024-12-17 06:04:15.924258: Current learning rate: 0.00719
2024-12-17 06:05:45.761509: Validation loss did not improve from -0.47055. Patience: 7/50
2024-12-17 06:05:45.762748: train_loss -0.6445
2024-12-17 06:05:45.763746: val_loss -0.4161
2024-12-17 06:05:45.764481: Pseudo dice [0.6703]
2024-12-17 06:05:45.765296: Epoch time: 89.84 s
2024-12-17 06:05:46.931699: 
2024-12-17 06:05:46.933530: Epoch 47
2024-12-17 06:05:46.934399: Current learning rate: 0.00713
2024-12-17 06:07:16.885430: Validation loss did not improve from -0.47055. Patience: 8/50
2024-12-17 06:07:16.886604: train_loss -0.6399
2024-12-17 06:07:16.887761: val_loss -0.2435
2024-12-17 06:07:16.888558: Pseudo dice [0.6009]
2024-12-17 06:07:16.889335: Epoch time: 89.96 s
2024-12-17 06:07:18.091411: 
2024-12-17 06:07:18.093547: Epoch 48
2024-12-17 06:07:18.094549: Current learning rate: 0.00707
2024-12-17 06:08:47.963580: Validation loss did not improve from -0.47055. Patience: 9/50
2024-12-17 06:08:47.965013: train_loss -0.6422
2024-12-17 06:08:47.966008: val_loss -0.4036
2024-12-17 06:08:47.966735: Pseudo dice [0.6777]
2024-12-17 06:08:47.967523: Epoch time: 89.87 s
2024-12-17 06:08:49.241937: 
2024-12-17 06:08:49.243591: Epoch 49
2024-12-17 06:08:49.244417: Current learning rate: 0.007
2024-12-17 06:10:18.929131: Validation loss did not improve from -0.47055. Patience: 10/50
2024-12-17 06:10:18.930183: train_loss -0.6541
2024-12-17 06:10:18.931152: val_loss -0.4433
2024-12-17 06:10:18.931998: Pseudo dice [0.6958]
2024-12-17 06:10:18.932932: Epoch time: 89.69 s
2024-12-17 06:10:21.346609: 
2024-12-17 06:10:21.348695: Epoch 50
2024-12-17 06:10:21.350045: Current learning rate: 0.00694
2024-12-17 06:11:50.926215: Validation loss did not improve from -0.47055. Patience: 11/50
2024-12-17 06:11:50.927648: train_loss -0.6506
2024-12-17 06:11:50.928611: val_loss -0.4127
2024-12-17 06:11:50.929437: Pseudo dice [0.6854]
2024-12-17 06:11:50.930166: Epoch time: 89.58 s
2024-12-17 06:11:52.127434: 
2024-12-17 06:11:52.128508: Epoch 51
2024-12-17 06:11:52.129382: Current learning rate: 0.00688
2024-12-17 06:13:21.836238: Validation loss did not improve from -0.47055. Patience: 12/50
2024-12-17 06:13:21.837526: train_loss -0.6458
2024-12-17 06:13:21.838486: val_loss -0.3997
2024-12-17 06:13:21.839214: Pseudo dice [0.6727]
2024-12-17 06:13:21.839817: Epoch time: 89.71 s
2024-12-17 06:13:23.044614: 
2024-12-17 06:13:23.046559: Epoch 52
2024-12-17 06:13:23.047662: Current learning rate: 0.00682
2024-12-17 06:14:52.733625: Validation loss improved from -0.47055 to -0.48799! Patience: 12/50
2024-12-17 06:14:52.734925: train_loss -0.6511
2024-12-17 06:14:52.736065: val_loss -0.488
2024-12-17 06:14:52.736780: Pseudo dice [0.7167]
2024-12-17 06:14:52.737576: Epoch time: 89.69 s
2024-12-17 06:14:52.738253: Yayy! New best EMA pseudo Dice: 0.6742
2024-12-17 06:14:54.373704: 
2024-12-17 06:14:54.375173: Epoch 53
2024-12-17 06:14:54.376066: Current learning rate: 0.00675
2024-12-17 06:16:23.963030: Validation loss did not improve from -0.48799. Patience: 1/50
2024-12-17 06:16:23.964478: train_loss -0.6477
2024-12-17 06:16:23.965404: val_loss -0.3629
2024-12-17 06:16:23.966071: Pseudo dice [0.664]
2024-12-17 06:16:23.966702: Epoch time: 89.59 s
2024-12-17 06:16:25.206887: 
2024-12-17 06:16:25.209271: Epoch 54
2024-12-17 06:16:25.210617: Current learning rate: 0.00669
2024-12-17 06:17:54.723078: Validation loss did not improve from -0.48799. Patience: 2/50
2024-12-17 06:17:54.724264: train_loss -0.6561
2024-12-17 06:17:54.725242: val_loss -0.2111
2024-12-17 06:17:54.725998: Pseudo dice [0.5744]
2024-12-17 06:17:54.726673: Epoch time: 89.52 s
2024-12-17 06:17:56.307863: 
2024-12-17 06:17:56.308976: Epoch 55
2024-12-17 06:17:56.309882: Current learning rate: 0.00663
2024-12-17 06:19:25.945275: Validation loss did not improve from -0.48799. Patience: 3/50
2024-12-17 06:19:25.946060: train_loss -0.6483
2024-12-17 06:19:25.947343: val_loss -0.3866
2024-12-17 06:19:25.948408: Pseudo dice [0.6852]
2024-12-17 06:19:25.949439: Epoch time: 89.64 s
2024-12-17 06:19:27.186965: 
2024-12-17 06:19:27.189029: Epoch 56
2024-12-17 06:19:27.190173: Current learning rate: 0.00657
2024-12-17 06:20:57.022154: Validation loss did not improve from -0.48799. Patience: 4/50
2024-12-17 06:20:57.023401: train_loss -0.6488
2024-12-17 06:20:57.024380: val_loss -0.4464
2024-12-17 06:20:57.025232: Pseudo dice [0.7097]
2024-12-17 06:20:57.026031: Epoch time: 89.84 s
2024-12-17 06:20:58.272853: 
2024-12-17 06:20:58.274555: Epoch 57
2024-12-17 06:20:58.275871: Current learning rate: 0.0065
2024-12-17 06:22:28.234408: Validation loss did not improve from -0.48799. Patience: 5/50
2024-12-17 06:22:28.235825: train_loss -0.6531
2024-12-17 06:22:28.236713: val_loss -0.4743
2024-12-17 06:22:28.237418: Pseudo dice [0.7258]
2024-12-17 06:22:28.238138: Epoch time: 89.96 s
2024-12-17 06:22:28.238925: Yayy! New best EMA pseudo Dice: 0.6755
2024-12-17 06:22:29.848895: 
2024-12-17 06:22:29.850859: Epoch 58
2024-12-17 06:22:29.852060: Current learning rate: 0.00644
2024-12-17 06:23:59.744440: Validation loss did not improve from -0.48799. Patience: 6/50
2024-12-17 06:23:59.745835: train_loss -0.6647
2024-12-17 06:23:59.746820: val_loss -0.4544
2024-12-17 06:23:59.747467: Pseudo dice [0.7101]
2024-12-17 06:23:59.748232: Epoch time: 89.9 s
2024-12-17 06:23:59.748962: Yayy! New best EMA pseudo Dice: 0.679
2024-12-17 06:24:01.354193: 
2024-12-17 06:24:01.355951: Epoch 59
2024-12-17 06:24:01.357038: Current learning rate: 0.00638
2024-12-17 06:25:31.278673: Validation loss did not improve from -0.48799. Patience: 7/50
2024-12-17 06:25:31.279634: train_loss -0.6638
2024-12-17 06:25:31.280476: val_loss -0.442
2024-12-17 06:25:31.281329: Pseudo dice [0.7041]
2024-12-17 06:25:31.282218: Epoch time: 89.93 s
2024-12-17 06:25:31.644278: Yayy! New best EMA pseudo Dice: 0.6815
2024-12-17 06:25:33.240895: 
2024-12-17 06:25:33.241965: Epoch 60
2024-12-17 06:25:33.242953: Current learning rate: 0.00631
2024-12-17 06:27:03.069361: Validation loss did not improve from -0.48799. Patience: 8/50
2024-12-17 06:27:03.070326: train_loss -0.6666
2024-12-17 06:27:03.071705: val_loss -0.44
2024-12-17 06:27:03.072716: Pseudo dice [0.7062]
2024-12-17 06:27:03.073776: Epoch time: 89.83 s
2024-12-17 06:27:03.074739: Yayy! New best EMA pseudo Dice: 0.684
2024-12-17 06:27:04.981997: 
2024-12-17 06:27:04.984179: Epoch 61
2024-12-17 06:27:04.985461: Current learning rate: 0.00625
2024-12-17 06:28:34.771822: Validation loss did not improve from -0.48799. Patience: 9/50
2024-12-17 06:28:34.773005: train_loss -0.6606
2024-12-17 06:28:34.773949: val_loss -0.4724
2024-12-17 06:28:34.774787: Pseudo dice [0.7189]
2024-12-17 06:28:34.775579: Epoch time: 89.79 s
2024-12-17 06:28:34.776274: Yayy! New best EMA pseudo Dice: 0.6875
2024-12-17 06:28:36.425975: 
2024-12-17 06:28:36.427340: Epoch 62
2024-12-17 06:28:36.428098: Current learning rate: 0.00619
2024-12-17 06:30:06.179351: Validation loss did not improve from -0.48799. Patience: 10/50
2024-12-17 06:30:06.180699: train_loss -0.67
2024-12-17 06:30:06.181892: val_loss -0.4474
2024-12-17 06:30:06.182764: Pseudo dice [0.7058]
2024-12-17 06:30:06.183745: Epoch time: 89.76 s
2024-12-17 06:30:06.184621: Yayy! New best EMA pseudo Dice: 0.6893
2024-12-17 06:30:07.830590: 
2024-12-17 06:30:07.832510: Epoch 63
2024-12-17 06:30:07.833753: Current learning rate: 0.00612
2024-12-17 06:31:37.551051: Validation loss did not improve from -0.48799. Patience: 11/50
2024-12-17 06:31:37.552159: train_loss -0.6735
2024-12-17 06:31:37.553330: val_loss -0.4634
2024-12-17 06:31:37.554363: Pseudo dice [0.7153]
2024-12-17 06:31:37.555288: Epoch time: 89.72 s
2024-12-17 06:31:37.556138: Yayy! New best EMA pseudo Dice: 0.6919
2024-12-17 06:31:39.180624: 
2024-12-17 06:31:39.182396: Epoch 64
2024-12-17 06:31:39.183324: Current learning rate: 0.00606
2024-12-17 06:33:09.045676: Validation loss did not improve from -0.48799. Patience: 12/50
2024-12-17 06:33:09.046979: train_loss -0.6555
2024-12-17 06:33:09.048206: val_loss -0.3964
2024-12-17 06:33:09.049239: Pseudo dice [0.6749]
2024-12-17 06:33:09.050246: Epoch time: 89.87 s
2024-12-17 06:33:10.659321: 
2024-12-17 06:33:10.661132: Epoch 65
2024-12-17 06:33:10.662212: Current learning rate: 0.006
2024-12-17 06:34:40.457376: Validation loss did not improve from -0.48799. Patience: 13/50
2024-12-17 06:34:40.458526: train_loss -0.6675
2024-12-17 06:34:40.459548: val_loss -0.4771
2024-12-17 06:34:40.460405: Pseudo dice [0.7172]
2024-12-17 06:34:40.461173: Epoch time: 89.8 s
2024-12-17 06:34:40.461879: Yayy! New best EMA pseudo Dice: 0.6929
2024-12-17 06:34:42.046245: 
2024-12-17 06:34:42.047786: Epoch 66
2024-12-17 06:34:42.048866: Current learning rate: 0.00593
2024-12-17 06:36:11.917241: Validation loss did not improve from -0.48799. Patience: 14/50
2024-12-17 06:36:11.918542: train_loss -0.6805
2024-12-17 06:36:11.919648: val_loss -0.4119
2024-12-17 06:36:11.920320: Pseudo dice [0.6762]
2024-12-17 06:36:11.921014: Epoch time: 89.87 s
2024-12-17 06:36:13.203720: 
2024-12-17 06:36:13.205663: Epoch 67
2024-12-17 06:36:13.206579: Current learning rate: 0.00587
2024-12-17 06:37:43.059634: Validation loss did not improve from -0.48799. Patience: 15/50
2024-12-17 06:37:43.060658: train_loss -0.6769
2024-12-17 06:37:43.061527: val_loss -0.3515
2024-12-17 06:37:43.062283: Pseudo dice [0.6576]
2024-12-17 06:37:43.063048: Epoch time: 89.86 s
2024-12-17 06:37:44.311576: 
2024-12-17 06:37:44.313133: Epoch 68
2024-12-17 06:37:44.313856: Current learning rate: 0.00581
2024-12-17 06:39:14.158471: Validation loss did not improve from -0.48799. Patience: 16/50
2024-12-17 06:39:14.159674: train_loss -0.6724
2024-12-17 06:39:14.160533: val_loss -0.3455
2024-12-17 06:39:14.161342: Pseudo dice [0.6442]
2024-12-17 06:39:14.162117: Epoch time: 89.85 s
2024-12-17 06:39:15.430646: 
2024-12-17 06:39:15.432501: Epoch 69
2024-12-17 06:39:15.433318: Current learning rate: 0.00574
2024-12-17 06:40:45.259360: Validation loss did not improve from -0.48799. Patience: 17/50
2024-12-17 06:40:45.260591: train_loss -0.6813
2024-12-17 06:40:45.261777: val_loss -0.4483
2024-12-17 06:40:45.262544: Pseudo dice [0.7045]
2024-12-17 06:40:45.263384: Epoch time: 89.83 s
2024-12-17 06:40:46.910096: 
2024-12-17 06:40:46.911537: Epoch 70
2024-12-17 06:40:46.912257: Current learning rate: 0.00568
2024-12-17 06:42:16.704387: Validation loss did not improve from -0.48799. Patience: 18/50
2024-12-17 06:42:16.705452: train_loss -0.6826
2024-12-17 06:42:16.706608: val_loss -0.3965
2024-12-17 06:42:16.707338: Pseudo dice [0.6738]
2024-12-17 06:42:16.708078: Epoch time: 89.8 s
2024-12-17 06:42:18.302269: 
2024-12-17 06:42:18.303998: Epoch 71
2024-12-17 06:42:18.304846: Current learning rate: 0.00562
2024-12-17 06:43:48.082978: Validation loss did not improve from -0.48799. Patience: 19/50
2024-12-17 06:43:48.084108: train_loss -0.6853
2024-12-17 06:43:48.084910: val_loss -0.3553
2024-12-17 06:43:48.085594: Pseudo dice [0.6553]
2024-12-17 06:43:48.086358: Epoch time: 89.78 s
2024-12-17 06:43:49.328761: 
2024-12-17 06:43:49.330643: Epoch 72
2024-12-17 06:43:49.331820: Current learning rate: 0.00555
2024-12-17 06:45:19.173129: Validation loss did not improve from -0.48799. Patience: 20/50
2024-12-17 06:45:19.174339: train_loss -0.6858
2024-12-17 06:45:19.175159: val_loss -0.3993
2024-12-17 06:45:19.175891: Pseudo dice [0.6783]
2024-12-17 06:45:19.176785: Epoch time: 89.85 s
2024-12-17 06:45:20.445308: 
2024-12-17 06:45:20.446489: Epoch 73
2024-12-17 06:45:20.447287: Current learning rate: 0.00549
2024-12-17 06:46:50.386394: Validation loss did not improve from -0.48799. Patience: 21/50
2024-12-17 06:46:50.387642: train_loss -0.6839
2024-12-17 06:46:50.388763: val_loss -0.3867
2024-12-17 06:46:50.389665: Pseudo dice [0.6607]
2024-12-17 06:46:50.390583: Epoch time: 89.94 s
2024-12-17 06:46:51.645484: 
2024-12-17 06:46:51.646664: Epoch 74
2024-12-17 06:46:51.647541: Current learning rate: 0.00542
2024-12-17 06:48:21.508947: Validation loss did not improve from -0.48799. Patience: 22/50
2024-12-17 06:48:21.509996: train_loss -0.6918
2024-12-17 06:48:21.510812: val_loss -0.3901
2024-12-17 06:48:21.511673: Pseudo dice [0.6693]
2024-12-17 06:48:21.512369: Epoch time: 89.87 s
2024-12-17 06:48:23.135933: 
2024-12-17 06:48:23.137776: Epoch 75
2024-12-17 06:48:23.138843: Current learning rate: 0.00536
2024-12-17 06:49:52.938422: Validation loss did not improve from -0.48799. Patience: 23/50
2024-12-17 06:49:52.939543: train_loss -0.6882
2024-12-17 06:49:52.940406: val_loss -0.3952
2024-12-17 06:49:52.941188: Pseudo dice [0.6798]
2024-12-17 06:49:52.941993: Epoch time: 89.8 s
2024-12-17 06:49:54.212466: 
2024-12-17 06:49:54.214451: Epoch 76
2024-12-17 06:49:54.215678: Current learning rate: 0.00529
2024-12-17 06:51:23.826213: Validation loss did not improve from -0.48799. Patience: 24/50
2024-12-17 06:51:23.827567: train_loss -0.6906
2024-12-17 06:51:23.828465: val_loss -0.3842
2024-12-17 06:51:23.829364: Pseudo dice [0.6714]
2024-12-17 06:51:23.830092: Epoch time: 89.62 s
2024-12-17 06:51:25.133270: 
2024-12-17 06:51:25.135143: Epoch 77
2024-12-17 06:51:25.136240: Current learning rate: 0.00523
2024-12-17 06:52:54.709968: Validation loss did not improve from -0.48799. Patience: 25/50
2024-12-17 06:52:54.711376: train_loss -0.6924
2024-12-17 06:52:54.712403: val_loss -0.3445
2024-12-17 06:52:54.713184: Pseudo dice [0.643]
2024-12-17 06:52:54.713889: Epoch time: 89.58 s
2024-12-17 06:52:55.984919: 
2024-12-17 06:52:55.987328: Epoch 78
2024-12-17 06:52:55.988236: Current learning rate: 0.00517
2024-12-17 06:54:25.537580: Validation loss did not improve from -0.48799. Patience: 26/50
2024-12-17 06:54:25.538974: train_loss -0.6993
2024-12-17 06:54:25.540035: val_loss -0.4222
2024-12-17 06:54:25.540832: Pseudo dice [0.6954]
2024-12-17 06:54:25.541688: Epoch time: 89.56 s
2024-12-17 06:54:26.809028: 
2024-12-17 06:54:26.810747: Epoch 79
2024-12-17 06:54:26.811881: Current learning rate: 0.0051
2024-12-17 06:55:56.429941: Validation loss did not improve from -0.48799. Patience: 27/50
2024-12-17 06:55:56.431135: train_loss -0.6934
2024-12-17 06:55:56.432123: val_loss -0.2569
2024-12-17 06:55:56.432940: Pseudo dice [0.5947]
2024-12-17 06:55:56.433873: Epoch time: 89.62 s
2024-12-17 06:55:58.136131: 
2024-12-17 06:55:58.138034: Epoch 80
2024-12-17 06:55:58.139637: Current learning rate: 0.00504
2024-12-17 06:57:27.752921: Validation loss did not improve from -0.48799. Patience: 28/50
2024-12-17 06:57:27.754070: train_loss -0.694
2024-12-17 06:57:27.755001: val_loss -0.3795
2024-12-17 06:57:27.755756: Pseudo dice [0.6761]
2024-12-17 06:57:27.756433: Epoch time: 89.62 s
2024-12-17 06:57:29.080516: 
2024-12-17 06:57:29.082442: Epoch 81
2024-12-17 06:57:29.083277: Current learning rate: 0.00497
2024-12-17 06:58:58.675447: Validation loss did not improve from -0.48799. Patience: 29/50
2024-12-17 06:58:58.676664: train_loss -0.6992
2024-12-17 06:58:58.677488: val_loss -0.2962
2024-12-17 06:58:58.678233: Pseudo dice [0.6094]
2024-12-17 06:58:58.678879: Epoch time: 89.6 s
2024-12-17 06:59:00.260916: 
2024-12-17 06:59:00.262501: Epoch 82
2024-12-17 06:59:00.263272: Current learning rate: 0.00491
2024-12-17 07:00:29.884778: Validation loss did not improve from -0.48799. Patience: 30/50
2024-12-17 07:00:29.889301: train_loss -0.6962
2024-12-17 07:00:29.891095: val_loss -0.4114
2024-12-17 07:00:29.891868: Pseudo dice [0.6844]
2024-12-17 07:00:29.892790: Epoch time: 89.63 s
2024-12-17 07:00:31.127707: 
2024-12-17 07:00:31.129367: Epoch 83
2024-12-17 07:00:31.130030: Current learning rate: 0.00484
2024-12-17 07:02:00.688315: Validation loss did not improve from -0.48799. Patience: 31/50
2024-12-17 07:02:00.689799: train_loss -0.7057
2024-12-17 07:02:00.690750: val_loss -0.3158
2024-12-17 07:02:00.691383: Pseudo dice [0.635]
2024-12-17 07:02:00.692055: Epoch time: 89.56 s
2024-12-17 07:02:01.895507: 
2024-12-17 07:02:01.897368: Epoch 84
2024-12-17 07:02:01.898062: Current learning rate: 0.00478
2024-12-17 07:03:31.765191: Validation loss did not improve from -0.48799. Patience: 32/50
2024-12-17 07:03:31.766518: train_loss -0.6975
2024-12-17 07:03:31.768920: val_loss -0.2267
2024-12-17 07:03:31.769846: Pseudo dice [0.5921]
2024-12-17 07:03:31.770995: Epoch time: 89.87 s
2024-12-17 07:03:33.392757: 
2024-12-17 07:03:33.394187: Epoch 85
2024-12-17 07:03:33.395206: Current learning rate: 0.00471
2024-12-17 07:05:03.325529: Validation loss did not improve from -0.48799. Patience: 33/50
2024-12-17 07:05:03.326993: train_loss -0.6961
2024-12-17 07:05:03.327961: val_loss -0.4551
2024-12-17 07:05:03.328933: Pseudo dice [0.7023]
2024-12-17 07:05:03.329724: Epoch time: 89.94 s
2024-12-17 07:05:04.550240: 
2024-12-17 07:05:04.552105: Epoch 86
2024-12-17 07:05:04.552955: Current learning rate: 0.00465
2024-12-17 07:06:34.459524: Validation loss did not improve from -0.48799. Patience: 34/50
2024-12-17 07:06:34.460709: train_loss -0.6988
2024-12-17 07:06:34.461647: val_loss -0.2772
2024-12-17 07:06:34.462338: Pseudo dice [0.6206]
2024-12-17 07:06:34.463014: Epoch time: 89.91 s
2024-12-17 07:06:35.644027: 
2024-12-17 07:06:35.645422: Epoch 87
2024-12-17 07:06:35.646328: Current learning rate: 0.00458
2024-12-17 07:08:05.497277: Validation loss did not improve from -0.48799. Patience: 35/50
2024-12-17 07:08:05.498400: train_loss -0.7016
2024-12-17 07:08:05.499394: val_loss -0.361
2024-12-17 07:08:05.500121: Pseudo dice [0.6515]
2024-12-17 07:08:05.500955: Epoch time: 89.86 s
2024-12-17 07:08:06.719541: 
2024-12-17 07:08:06.721348: Epoch 88
2024-12-17 07:08:06.722583: Current learning rate: 0.00452
2024-12-17 07:09:36.581572: Validation loss did not improve from -0.48799. Patience: 36/50
2024-12-17 07:09:36.583106: train_loss -0.7027
2024-12-17 07:09:36.584088: val_loss -0.3105
2024-12-17 07:09:36.584923: Pseudo dice [0.6293]
2024-12-17 07:09:36.585720: Epoch time: 89.86 s
2024-12-17 07:09:37.773196: 
2024-12-17 07:09:37.774878: Epoch 89
2024-12-17 07:09:37.775727: Current learning rate: 0.00445
2024-12-17 07:11:07.565472: Validation loss did not improve from -0.48799. Patience: 37/50
2024-12-17 07:11:07.566672: train_loss -0.7108
2024-12-17 07:11:07.567734: val_loss -0.3816
2024-12-17 07:11:07.569131: Pseudo dice [0.6824]
2024-12-17 07:11:07.570406: Epoch time: 89.79 s
2024-12-17 07:11:09.100563: 
2024-12-17 07:11:09.102725: Epoch 90
2024-12-17 07:11:09.103901: Current learning rate: 0.00438
2024-12-17 07:12:38.917459: Validation loss did not improve from -0.48799. Patience: 38/50
2024-12-17 07:12:38.918606: train_loss -0.7017
2024-12-17 07:12:38.919791: val_loss -0.4282
2024-12-17 07:12:38.920557: Pseudo dice [0.6935]
2024-12-17 07:12:38.921302: Epoch time: 89.82 s
2024-12-17 07:12:40.140051: 
2024-12-17 07:12:40.141638: Epoch 91
2024-12-17 07:12:40.142492: Current learning rate: 0.00432
2024-12-17 07:14:10.142049: Validation loss did not improve from -0.48799. Patience: 39/50
2024-12-17 07:14:10.143620: train_loss -0.7084
2024-12-17 07:14:10.145010: val_loss -0.3406
2024-12-17 07:14:10.145937: Pseudo dice [0.6594]
2024-12-17 07:14:10.146666: Epoch time: 90.0 s
2024-12-17 07:14:11.666123: 
2024-12-17 07:14:11.667146: Epoch 92
2024-12-17 07:14:11.667976: Current learning rate: 0.00425
2024-12-17 07:15:41.586589: Validation loss did not improve from -0.48799. Patience: 40/50
2024-12-17 07:15:41.587592: train_loss -0.7141
2024-12-17 07:15:41.588459: val_loss -0.2604
2024-12-17 07:15:41.589209: Pseudo dice [0.6094]
2024-12-17 07:15:41.589874: Epoch time: 89.92 s
2024-12-17 07:15:42.767141: 
2024-12-17 07:15:42.769100: Epoch 93
2024-12-17 07:15:42.769950: Current learning rate: 0.00419
2024-12-17 07:17:12.640373: Validation loss did not improve from -0.48799. Patience: 41/50
2024-12-17 07:17:12.641488: train_loss -0.7118
2024-12-17 07:17:12.642494: val_loss -0.3533
2024-12-17 07:17:12.643272: Pseudo dice [0.6565]
2024-12-17 07:17:12.644022: Epoch time: 89.88 s
2024-12-17 07:17:13.867707: 
2024-12-17 07:17:13.869602: Epoch 94
2024-12-17 07:17:13.870579: Current learning rate: 0.00412
2024-12-17 07:18:43.775578: Validation loss did not improve from -0.48799. Patience: 42/50
2024-12-17 07:18:43.776695: train_loss -0.7093
2024-12-17 07:18:43.777768: val_loss -0.4366
2024-12-17 07:18:43.778681: Pseudo dice [0.708]
2024-12-17 07:18:43.779402: Epoch time: 89.91 s
2024-12-17 07:18:45.331080: 
2024-12-17 07:18:45.332757: Epoch 95
2024-12-17 07:18:45.333888: Current learning rate: 0.00405
2024-12-17 07:20:15.221275: Validation loss did not improve from -0.48799. Patience: 43/50
2024-12-17 07:20:15.222500: train_loss -0.7086
2024-12-17 07:20:15.223600: val_loss -0.3529
2024-12-17 07:20:15.224428: Pseudo dice [0.6535]
2024-12-17 07:20:15.225087: Epoch time: 89.89 s
2024-12-17 07:20:16.686006: 
2024-12-17 07:20:16.687837: Epoch 96
2024-12-17 07:20:16.688961: Current learning rate: 0.00399
2024-12-17 07:21:46.359637: Validation loss did not improve from -0.48799. Patience: 44/50
2024-12-17 07:21:46.360726: train_loss -0.7115
2024-12-17 07:21:46.362087: val_loss -0.4244
2024-12-17 07:21:46.362941: Pseudo dice [0.7004]
2024-12-17 07:21:46.363605: Epoch time: 89.68 s
2024-12-17 07:21:47.580531: 
2024-12-17 07:21:47.582318: Epoch 97
2024-12-17 07:21:47.583432: Current learning rate: 0.00392
2024-12-17 07:23:17.121915: Validation loss did not improve from -0.48799. Patience: 45/50
2024-12-17 07:23:17.123141: train_loss -0.7193
2024-12-17 07:23:17.124413: val_loss -0.3733
2024-12-17 07:23:17.125200: Pseudo dice [0.6748]
2024-12-17 07:23:17.125922: Epoch time: 89.54 s
2024-12-17 07:23:18.365043: 
2024-12-17 07:23:18.367050: Epoch 98
2024-12-17 07:23:18.367748: Current learning rate: 0.00385
2024-12-17 07:24:47.983098: Validation loss did not improve from -0.48799. Patience: 46/50
2024-12-17 07:24:47.984566: train_loss -0.7202
2024-12-17 07:24:47.986137: val_loss -0.3719
2024-12-17 07:24:47.987238: Pseudo dice [0.6682]
2024-12-17 07:24:47.988369: Epoch time: 89.62 s
2024-12-17 07:24:49.201438: 
2024-12-17 07:24:49.203527: Epoch 99
2024-12-17 07:24:49.204932: Current learning rate: 0.00379
2024-12-17 07:26:18.923072: Validation loss did not improve from -0.48799. Patience: 47/50
2024-12-17 07:26:18.923760: train_loss -0.7135
2024-12-17 07:26:18.924699: val_loss -0.3972
2024-12-17 07:26:18.925391: Pseudo dice [0.6765]
2024-12-17 07:26:18.926235: Epoch time: 89.72 s
2024-12-17 07:26:20.552241: 
2024-12-17 07:26:20.553402: Epoch 100
2024-12-17 07:26:20.554358: Current learning rate: 0.00372
2024-12-17 07:27:50.209565: Validation loss did not improve from -0.48799. Patience: 48/50
2024-12-17 07:27:50.210733: train_loss -0.7212
2024-12-17 07:27:50.211704: val_loss -0.4589
2024-12-17 07:27:50.212707: Pseudo dice [0.7144]
2024-12-17 07:27:50.213502: Epoch time: 89.66 s
2024-12-17 07:27:51.437963: 
2024-12-17 07:27:51.439547: Epoch 101
2024-12-17 07:27:51.440725: Current learning rate: 0.00365
2024-12-17 07:29:20.981031: Validation loss did not improve from -0.48799. Patience: 49/50
2024-12-17 07:29:20.982471: train_loss -0.722
2024-12-17 07:29:20.983404: val_loss -0.3955
2024-12-17 07:29:20.984183: Pseudo dice [0.6719]
2024-12-17 07:29:20.984937: Epoch time: 89.55 s
2024-12-17 07:29:22.243508: 
2024-12-17 07:29:22.245338: Epoch 102
2024-12-17 07:29:22.246330: Current learning rate: 0.00359
2024-12-17 07:30:51.915515: Validation loss did not improve from -0.48799. Patience: 50/50
2024-12-17 07:30:51.916891: train_loss -0.7223
2024-12-17 07:30:51.918147: val_loss -0.3002
2024-12-17 07:30:51.918904: Pseudo dice [0.6244]
2024-12-17 07:30:51.920349: Epoch time: 89.67 s
2024-12-17 07:30:53.464176: 
2024-12-17 07:30:53.466385: Epoch 103
2024-12-17 07:30:53.467472: Current learning rate: 0.00352
2024-12-17 07:32:23.110448: Validation loss did not improve from -0.48799. Patience: 51/50
2024-12-17 07:32:23.111542: train_loss -0.718
2024-12-17 07:32:23.112730: val_loss -0.3975
2024-12-17 07:32:23.113680: Pseudo dice [0.6922]
2024-12-17 07:32:23.114663: Epoch time: 89.65 s
2024-12-17 07:32:24.352833: 
2024-12-17 07:32:24.354590: Epoch 104
2024-12-17 07:32:24.356123: Current learning rate: 0.00345
2024-12-17 07:33:54.265062: Validation loss did not improve from -0.48799. Patience: 52/50
2024-12-17 07:33:54.266263: train_loss -0.7213
2024-12-17 07:33:54.267241: val_loss -0.3208
2024-12-17 07:33:54.268066: Pseudo dice [0.6492]
2024-12-17 07:33:54.268834: Epoch time: 89.91 s
2024-12-17 07:33:55.850834: 
2024-12-17 07:33:55.852935: Epoch 105
2024-12-17 07:33:55.853785: Current learning rate: 0.00338
2024-12-17 07:35:25.728029: Validation loss did not improve from -0.48799. Patience: 53/50
2024-12-17 07:35:25.729846: train_loss -0.7258
2024-12-17 07:35:25.730820: val_loss -0.3634
2024-12-17 07:35:25.731620: Pseudo dice [0.6622]
2024-12-17 07:35:25.732381: Epoch time: 89.88 s
2024-12-17 07:35:26.969613: 
2024-12-17 07:35:26.970766: Epoch 106
2024-12-17 07:35:26.971589: Current learning rate: 0.00332
2024-12-17 07:36:56.856845: Validation loss did not improve from -0.48799. Patience: 54/50
2024-12-17 07:36:56.857972: train_loss -0.7258
2024-12-17 07:36:56.858957: val_loss -0.3841
2024-12-17 07:36:56.859838: Pseudo dice [0.6734]
2024-12-17 07:36:56.860890: Epoch time: 89.89 s
2024-12-17 07:36:58.088012: 
2024-12-17 07:36:58.089724: Epoch 107
2024-12-17 07:36:58.090842: Current learning rate: 0.00325
2024-12-17 07:38:27.968714: Validation loss did not improve from -0.48799. Patience: 55/50
2024-12-17 07:38:27.969650: train_loss -0.7263
2024-12-17 07:38:27.970444: val_loss -0.3505
2024-12-17 07:38:27.971093: Pseudo dice [0.6544]
2024-12-17 07:38:27.971794: Epoch time: 89.88 s
2024-12-17 07:38:29.184578: 
2024-12-17 07:38:29.186749: Epoch 108
2024-12-17 07:38:29.187812: Current learning rate: 0.00318
2024-12-17 07:39:59.070887: Validation loss did not improve from -0.48799. Patience: 56/50
2024-12-17 07:39:59.072102: train_loss -0.7255
2024-12-17 07:39:59.073021: val_loss -0.3325
2024-12-17 07:39:59.073774: Pseudo dice [0.6441]
2024-12-17 07:39:59.074586: Epoch time: 89.89 s
2024-12-17 07:40:00.274496: 
2024-12-17 07:40:00.276580: Epoch 109
2024-12-17 07:40:00.277695: Current learning rate: 0.00311
2024-12-17 07:41:30.073505: Validation loss did not improve from -0.48799. Patience: 57/50
2024-12-17 07:41:30.074625: train_loss -0.7305
2024-12-17 07:41:30.075521: val_loss -0.38
2024-12-17 07:41:30.076344: Pseudo dice [0.6719]
2024-12-17 07:41:30.077189: Epoch time: 89.8 s
2024-12-17 07:41:31.683402: 
2024-12-17 07:41:31.684631: Epoch 110
2024-12-17 07:41:31.685436: Current learning rate: 0.00304
2024-12-17 07:43:01.431321: Validation loss did not improve from -0.48799. Patience: 58/50
2024-12-17 07:43:01.432729: train_loss -0.7286
2024-12-17 07:43:01.434194: val_loss -0.3839
2024-12-17 07:43:01.434935: Pseudo dice [0.6653]
2024-12-17 07:43:01.435777: Epoch time: 89.75 s
2024-12-17 07:43:02.645704: 
2024-12-17 07:43:02.647896: Epoch 111
2024-12-17 07:43:02.648930: Current learning rate: 0.00297
2024-12-17 07:44:32.440683: Validation loss did not improve from -0.48799. Patience: 59/50
2024-12-17 07:44:32.441863: train_loss -0.7295
2024-12-17 07:44:32.442905: val_loss -0.3364
2024-12-17 07:44:32.443833: Pseudo dice [0.6463]
2024-12-17 07:44:32.444504: Epoch time: 89.8 s
2024-12-17 07:44:33.670224: 
2024-12-17 07:44:33.671986: Epoch 112
2024-12-17 07:44:33.673179: Current learning rate: 0.00291
2024-12-17 07:46:03.664407: Validation loss did not improve from -0.48799. Patience: 60/50
2024-12-17 07:46:03.665469: train_loss -0.7367
2024-12-17 07:46:03.666488: val_loss -0.3443
2024-12-17 07:46:03.667253: Pseudo dice [0.6614]
2024-12-17 07:46:03.668029: Epoch time: 90.0 s
2024-12-17 07:46:04.879897: 
2024-12-17 07:46:04.881781: Epoch 113
2024-12-17 07:46:04.882748: Current learning rate: 0.00284
2024-12-17 07:47:34.965447: Validation loss did not improve from -0.48799. Patience: 61/50
2024-12-17 07:47:34.966886: train_loss -0.7339
2024-12-17 07:47:34.968409: val_loss -0.324
2024-12-17 07:47:34.969678: Pseudo dice [0.6322]
2024-12-17 07:47:34.970665: Epoch time: 90.09 s
2024-12-17 07:47:36.522020: 
2024-12-17 07:47:36.524397: Epoch 114
2024-12-17 07:47:36.525439: Current learning rate: 0.00277
2024-12-17 07:49:06.443553: Validation loss did not improve from -0.48799. Patience: 62/50
2024-12-17 07:49:06.444289: train_loss -0.7362
2024-12-17 07:49:06.445075: val_loss -0.3412
2024-12-17 07:49:06.445842: Pseudo dice [0.6578]
2024-12-17 07:49:06.446574: Epoch time: 89.92 s
2024-12-17 07:49:08.018791: 
2024-12-17 07:49:08.020681: Epoch 115
2024-12-17 07:49:08.021784: Current learning rate: 0.0027
2024-12-17 07:50:37.921386: Validation loss did not improve from -0.48799. Patience: 63/50
2024-12-17 07:50:37.922673: train_loss -0.7336
2024-12-17 07:50:37.923863: val_loss -0.4202
2024-12-17 07:50:37.924816: Pseudo dice [0.6877]
2024-12-17 07:50:37.925838: Epoch time: 89.9 s
2024-12-17 07:50:39.160981: 
2024-12-17 07:50:39.162953: Epoch 116
2024-12-17 07:50:39.164069: Current learning rate: 0.00263
2024-12-17 07:52:08.863240: Validation loss did not improve from -0.48799. Patience: 64/50
2024-12-17 07:52:08.864528: train_loss -0.7341
2024-12-17 07:52:08.865546: val_loss -0.4152
2024-12-17 07:52:08.866306: Pseudo dice [0.6867]
2024-12-17 07:52:08.867062: Epoch time: 89.7 s
2024-12-17 07:52:10.116751: 
2024-12-17 07:52:10.118495: Epoch 117
2024-12-17 07:52:10.119714: Current learning rate: 0.00256
2024-12-17 07:53:39.893742: Validation loss did not improve from -0.48799. Patience: 65/50
2024-12-17 07:53:39.895107: train_loss -0.7336
2024-12-17 07:53:39.896208: val_loss -0.376
2024-12-17 07:53:39.896967: Pseudo dice [0.6681]
2024-12-17 07:53:39.897687: Epoch time: 89.78 s
2024-12-17 07:53:41.191298: 
2024-12-17 07:53:41.193091: Epoch 118
2024-12-17 07:53:41.194230: Current learning rate: 0.00249
2024-12-17 07:55:10.985159: Validation loss did not improve from -0.48799. Patience: 66/50
2024-12-17 07:55:10.986239: train_loss -0.7394
2024-12-17 07:55:10.987212: val_loss -0.2883
2024-12-17 07:55:10.987922: Pseudo dice [0.6263]
2024-12-17 07:55:10.988633: Epoch time: 89.8 s
2024-12-17 07:55:12.223356: 
2024-12-17 07:55:12.225312: Epoch 119
2024-12-17 07:55:12.226398: Current learning rate: 0.00242
2024-12-17 07:56:42.004139: Validation loss did not improve from -0.48799. Patience: 67/50
2024-12-17 07:56:42.005447: train_loss -0.7377
2024-12-17 07:56:42.006568: val_loss -0.3395
2024-12-17 07:56:42.007515: Pseudo dice [0.6583]
2024-12-17 07:56:42.008535: Epoch time: 89.78 s
2024-12-17 07:56:43.642047: 
2024-12-17 07:56:43.644080: Epoch 120
2024-12-17 07:56:43.645312: Current learning rate: 0.00235
2024-12-17 07:58:13.379594: Validation loss did not improve from -0.48799. Patience: 68/50
2024-12-17 07:58:13.380605: train_loss -0.743
2024-12-17 07:58:13.381551: val_loss -0.3717
2024-12-17 07:58:13.382246: Pseudo dice [0.6753]
2024-12-17 07:58:13.383179: Epoch time: 89.74 s
2024-12-17 07:58:14.627503: 
2024-12-17 07:58:14.629503: Epoch 121
2024-12-17 07:58:14.630589: Current learning rate: 0.00228
2024-12-17 07:59:44.400311: Validation loss did not improve from -0.48799. Patience: 69/50
2024-12-17 07:59:44.401511: train_loss -0.7436
2024-12-17 07:59:44.402507: val_loss -0.3136
2024-12-17 07:59:44.403475: Pseudo dice [0.631]
2024-12-17 07:59:44.404351: Epoch time: 89.77 s
2024-12-17 07:59:45.656297: 
2024-12-17 07:59:45.657854: Epoch 122
2024-12-17 07:59:45.659034: Current learning rate: 0.00221
2024-12-17 08:01:15.405607: Validation loss did not improve from -0.48799. Patience: 70/50
2024-12-17 08:01:15.407132: train_loss -0.7403
2024-12-17 08:01:15.408160: val_loss -0.3306
2024-12-17 08:01:15.408964: Pseudo dice [0.6521]
2024-12-17 08:01:15.409704: Epoch time: 89.75 s
2024-12-17 08:01:16.676348: 
2024-12-17 08:01:16.678260: Epoch 123
2024-12-17 08:01:16.679506: Current learning rate: 0.00214
2024-12-17 08:02:46.580492: Validation loss did not improve from -0.48799. Patience: 71/50
2024-12-17 08:02:46.581796: train_loss -0.737
2024-12-17 08:02:46.582659: val_loss -0.3038
2024-12-17 08:02:46.583475: Pseudo dice [0.6405]
2024-12-17 08:02:46.584343: Epoch time: 89.91 s
2024-12-17 08:02:47.843449: 
2024-12-17 08:02:47.845275: Epoch 124
2024-12-17 08:02:47.846285: Current learning rate: 0.00207
2024-12-17 08:04:17.854155: Validation loss did not improve from -0.48799. Patience: 72/50
2024-12-17 08:04:17.855414: train_loss -0.741
2024-12-17 08:04:17.856587: val_loss -0.2612
2024-12-17 08:04:17.857357: Pseudo dice [0.6133]
2024-12-17 08:04:17.858160: Epoch time: 90.01 s
2024-12-17 08:04:19.834130: 
2024-12-17 08:04:19.835774: Epoch 125
2024-12-17 08:04:19.836768: Current learning rate: 0.00199
2024-12-17 08:05:49.862374: Validation loss did not improve from -0.48799. Patience: 73/50
2024-12-17 08:05:49.865020: train_loss -0.743
2024-12-17 08:05:49.866364: val_loss -0.3905
2024-12-17 08:05:49.867276: Pseudo dice [0.6832]
2024-12-17 08:05:49.868242: Epoch time: 90.03 s
2024-12-17 08:05:51.134029: 
2024-12-17 08:05:51.135621: Epoch 126
2024-12-17 08:05:51.136807: Current learning rate: 0.00192
2024-12-17 08:07:21.108677: Validation loss did not improve from -0.48799. Patience: 74/50
2024-12-17 08:07:21.109579: train_loss -0.7469
2024-12-17 08:07:21.110512: val_loss -0.4017
2024-12-17 08:07:21.111179: Pseudo dice [0.6979]
2024-12-17 08:07:21.111845: Epoch time: 89.98 s
2024-12-17 08:07:22.352399: 
2024-12-17 08:07:22.354416: Epoch 127
2024-12-17 08:07:22.355231: Current learning rate: 0.00185
2024-12-17 08:08:52.179865: Validation loss did not improve from -0.48799. Patience: 75/50
2024-12-17 08:08:52.181602: train_loss -0.7444
2024-12-17 08:08:52.184222: val_loss -0.3493
2024-12-17 08:08:52.185423: Pseudo dice [0.661]
2024-12-17 08:08:52.186885: Epoch time: 89.83 s
2024-12-17 08:08:53.519880: 
2024-12-17 08:08:53.521937: Epoch 128
2024-12-17 08:08:53.522770: Current learning rate: 0.00178
2024-12-17 08:10:23.410955: Validation loss did not improve from -0.48799. Patience: 76/50
2024-12-17 08:10:23.412237: train_loss -0.744
2024-12-17 08:10:23.413349: val_loss -0.4048
2024-12-17 08:10:23.414356: Pseudo dice [0.6913]
2024-12-17 08:10:23.415319: Epoch time: 89.89 s
2024-12-17 08:10:24.673122: 
2024-12-17 08:10:24.675021: Epoch 129
2024-12-17 08:10:24.675823: Current learning rate: 0.0017
2024-12-17 08:11:54.441541: Validation loss did not improve from -0.48799. Patience: 77/50
2024-12-17 08:11:54.442940: train_loss -0.7477
2024-12-17 08:11:54.444167: val_loss -0.3817
2024-12-17 08:11:54.444996: Pseudo dice [0.6775]
2024-12-17 08:11:54.445826: Epoch time: 89.77 s
2024-12-17 08:11:56.051900: 
2024-12-17 08:11:56.053612: Epoch 130
2024-12-17 08:11:56.054672: Current learning rate: 0.00163
2024-12-17 08:13:25.826909: Validation loss did not improve from -0.48799. Patience: 78/50
2024-12-17 08:13:25.828265: train_loss -0.7528
2024-12-17 08:13:25.829406: val_loss -0.4068
2024-12-17 08:13:25.830063: Pseudo dice [0.6957]
2024-12-17 08:13:25.830859: Epoch time: 89.78 s
2024-12-17 08:13:27.078745: 
2024-12-17 08:13:27.080371: Epoch 131
2024-12-17 08:13:27.081100: Current learning rate: 0.00156
2024-12-17 08:14:56.821302: Validation loss did not improve from -0.48799. Patience: 79/50
2024-12-17 08:14:56.822564: train_loss -0.7496
2024-12-17 08:14:56.823725: val_loss -0.4019
2024-12-17 08:14:56.824480: Pseudo dice [0.6749]
2024-12-17 08:14:56.825476: Epoch time: 89.75 s
2024-12-17 08:14:58.081672: 
2024-12-17 08:14:58.084205: Epoch 132
2024-12-17 08:14:58.085125: Current learning rate: 0.00148
2024-12-17 08:16:27.862255: Validation loss did not improve from -0.48799. Patience: 80/50
2024-12-17 08:16:27.863670: train_loss -0.7545
2024-12-17 08:16:27.864695: val_loss -0.3154
2024-12-17 08:16:27.865440: Pseudo dice [0.6439]
2024-12-17 08:16:27.866195: Epoch time: 89.78 s
2024-12-17 08:16:29.123226: 
2024-12-17 08:16:29.124943: Epoch 133
2024-12-17 08:16:29.125885: Current learning rate: 0.00141
2024-12-17 08:17:58.928596: Validation loss did not improve from -0.48799. Patience: 81/50
2024-12-17 08:17:58.929765: train_loss -0.7554
2024-12-17 08:17:58.930737: val_loss -0.2638
2024-12-17 08:17:58.931512: Pseudo dice [0.6139]
2024-12-17 08:17:58.932424: Epoch time: 89.81 s
2024-12-17 08:18:00.153509: 
2024-12-17 08:18:00.155450: Epoch 134
2024-12-17 08:18:00.156231: Current learning rate: 0.00133
2024-12-17 08:19:29.900375: Validation loss did not improve from -0.48799. Patience: 82/50
2024-12-17 08:19:29.901458: train_loss -0.7532
2024-12-17 08:19:29.902358: val_loss -0.29
2024-12-17 08:19:29.903186: Pseudo dice [0.6279]
2024-12-17 08:19:29.903878: Epoch time: 89.75 s
2024-12-17 08:19:31.850128: 
2024-12-17 08:19:31.851626: Epoch 135
2024-12-17 08:19:31.852695: Current learning rate: 0.00126
2024-12-17 08:21:01.811946: Validation loss did not improve from -0.48799. Patience: 83/50
2024-12-17 08:21:01.813254: train_loss -0.7552
2024-12-17 08:21:01.814366: val_loss -0.3319
2024-12-17 08:21:01.815223: Pseudo dice [0.6475]
2024-12-17 08:21:01.815866: Epoch time: 89.96 s
2024-12-17 08:21:03.075632: 
2024-12-17 08:21:03.077939: Epoch 136
2024-12-17 08:21:03.078960: Current learning rate: 0.00118
2024-12-17 08:22:32.973176: Validation loss did not improve from -0.48799. Patience: 84/50
2024-12-17 08:22:32.974403: train_loss -0.749
2024-12-17 08:22:32.975220: val_loss -0.3199
2024-12-17 08:22:32.976027: Pseudo dice [0.6371]
2024-12-17 08:22:32.976841: Epoch time: 89.9 s
2024-12-17 08:22:34.252969: 
2024-12-17 08:22:34.254813: Epoch 137
2024-12-17 08:22:34.255782: Current learning rate: 0.00111
2024-12-17 08:24:04.217199: Validation loss did not improve from -0.48799. Patience: 85/50
2024-12-17 08:24:04.218475: train_loss -0.7531
2024-12-17 08:24:04.219747: val_loss -0.3066
2024-12-17 08:24:04.220590: Pseudo dice [0.6275]
2024-12-17 08:24:04.221543: Epoch time: 89.97 s
2024-12-17 08:24:05.536241: 
2024-12-17 08:24:05.537947: Epoch 138
2024-12-17 08:24:05.538884: Current learning rate: 0.00103
2024-12-17 08:25:35.478590: Validation loss did not improve from -0.48799. Patience: 86/50
2024-12-17 08:25:35.479734: train_loss -0.7558
2024-12-17 08:25:35.480552: val_loss -0.3201
2024-12-17 08:25:35.481184: Pseudo dice [0.6428]
2024-12-17 08:25:35.481895: Epoch time: 89.94 s
2024-12-17 08:25:36.748425: 
2024-12-17 08:25:36.750574: Epoch 139
2024-12-17 08:25:36.751714: Current learning rate: 0.00095
2024-12-17 08:27:06.447552: Validation loss did not improve from -0.48799. Patience: 87/50
2024-12-17 08:27:06.448819: train_loss -0.7555
2024-12-17 08:27:06.449989: val_loss -0.3723
2024-12-17 08:27:06.450736: Pseudo dice [0.6688]
2024-12-17 08:27:06.451508: Epoch time: 89.7 s
2024-12-17 08:27:08.136602: 
2024-12-17 08:27:08.138326: Epoch 140
2024-12-17 08:27:08.139452: Current learning rate: 0.00087
2024-12-17 08:28:37.810152: Validation loss did not improve from -0.48799. Patience: 88/50
2024-12-17 08:28:37.811545: train_loss -0.7614
2024-12-17 08:28:37.812376: val_loss -0.3232
2024-12-17 08:28:37.813063: Pseudo dice [0.6342]
2024-12-17 08:28:37.813781: Epoch time: 89.68 s
2024-12-17 08:28:39.062825: 
2024-12-17 08:28:39.064908: Epoch 141
2024-12-17 08:28:39.065762: Current learning rate: 0.00079
2024-12-17 08:30:08.765936: Validation loss did not improve from -0.48799. Patience: 89/50
2024-12-17 08:30:08.767284: train_loss -0.7576
2024-12-17 08:30:08.768485: val_loss -0.3724
2024-12-17 08:30:08.769206: Pseudo dice [0.6687]
2024-12-17 08:30:08.769905: Epoch time: 89.71 s
2024-12-17 08:30:10.140329: 
2024-12-17 08:30:10.142634: Epoch 142
2024-12-17 08:30:10.143548: Current learning rate: 0.00071
2024-12-17 08:31:39.827734: Validation loss did not improve from -0.48799. Patience: 90/50
2024-12-17 08:31:39.828931: train_loss -0.7623
2024-12-17 08:31:39.829843: val_loss -0.3342
2024-12-17 08:31:39.830529: Pseudo dice [0.6518]
2024-12-17 08:31:39.831213: Epoch time: 89.69 s
2024-12-17 08:31:41.096758: 
2024-12-17 08:31:41.099002: Epoch 143
2024-12-17 08:31:41.099855: Current learning rate: 0.00063
2024-12-17 08:33:10.790581: Validation loss did not improve from -0.48799. Patience: 91/50
2024-12-17 08:33:10.791989: train_loss -0.7569
2024-12-17 08:33:10.793090: val_loss -0.3496
2024-12-17 08:33:10.793868: Pseudo dice [0.6583]
2024-12-17 08:33:10.794798: Epoch time: 89.7 s
2024-12-17 08:33:12.146074: 
2024-12-17 08:33:12.147956: Epoch 144
2024-12-17 08:33:12.148952: Current learning rate: 0.00055
2024-12-17 08:34:41.926390: Validation loss did not improve from -0.48799. Patience: 92/50
2024-12-17 08:34:41.927560: train_loss -0.7579
2024-12-17 08:34:41.928709: val_loss -0.3168
2024-12-17 08:34:41.929406: Pseudo dice [0.637]
2024-12-17 08:34:41.930139: Epoch time: 89.78 s
2024-12-17 08:34:43.907191: 
2024-12-17 08:34:43.909171: Epoch 145
2024-12-17 08:34:43.910179: Current learning rate: 0.00047
2024-12-17 08:36:13.629814: Validation loss did not improve from -0.48799. Patience: 93/50
2024-12-17 08:36:13.630844: train_loss -0.7605
2024-12-17 08:36:13.631701: val_loss -0.34
2024-12-17 08:36:13.632407: Pseudo dice [0.6427]
2024-12-17 08:36:13.633290: Epoch time: 89.72 s
2024-12-17 08:36:15.007116: 
2024-12-17 08:36:15.008722: Epoch 146
2024-12-17 08:36:15.009608: Current learning rate: 0.00038
2024-12-17 08:37:44.810652: Validation loss did not improve from -0.48799. Patience: 94/50
2024-12-17 08:37:44.812043: train_loss -0.7589
2024-12-17 08:37:44.812973: val_loss -0.3631
2024-12-17 08:37:44.813767: Pseudo dice [0.6656]
2024-12-17 08:37:44.814392: Epoch time: 89.81 s
2024-12-17 08:37:46.156916: 
2024-12-17 08:37:46.158876: Epoch 147
2024-12-17 08:37:46.159696: Current learning rate: 0.0003
2024-12-17 08:39:16.110066: Validation loss did not improve from -0.48799. Patience: 95/50
2024-12-17 08:39:16.111087: train_loss -0.7614
2024-12-17 08:39:16.111964: val_loss -0.3147
2024-12-17 08:39:16.112739: Pseudo dice [0.6311]
2024-12-17 08:39:16.113413: Epoch time: 89.96 s
2024-12-17 08:39:17.441374: 
2024-12-17 08:39:17.442422: Epoch 148
2024-12-17 08:39:17.443168: Current learning rate: 0.00021
2024-12-17 08:40:47.528988: Validation loss did not improve from -0.48799. Patience: 96/50
2024-12-17 08:40:47.530885: train_loss -0.7647
2024-12-17 08:40:47.532068: val_loss -0.3347
2024-12-17 08:40:47.532831: Pseudo dice [0.6539]
2024-12-17 08:40:47.533565: Epoch time: 90.09 s
2024-12-17 08:40:48.860411: 
2024-12-17 08:40:48.862251: Epoch 149
2024-12-17 08:40:48.863153: Current learning rate: 0.00011
2024-12-17 08:42:18.848938: Validation loss did not improve from -0.48799. Patience: 97/50
2024-12-17 08:42:18.849725: train_loss -0.7614
2024-12-17 08:42:18.850492: val_loss -0.3397
2024-12-17 08:42:18.851131: Pseudo dice [0.6566]
2024-12-17 08:42:18.851885: Epoch time: 89.99 s
2024-12-17 08:42:20.599957: Training done.
2024-12-17 04:53:26.622916: unpacking done...
2024-12-17 04:53:26.892329: Unable to plot network architecture: nnUNet_compile is enabled!
2024-12-17 04:53:27.069736: 
2024-12-17 04:53:27.071424: Epoch 0
2024-12-17 04:53:27.073171: Current learning rate: 0.01
2024-12-17 04:55:46.935479: Validation loss improved from 1000.00000 to -0.23352! Patience: 0/50
2024-12-17 04:55:46.936682: train_loss -0.0946
2024-12-17 04:55:46.937681: val_loss -0.2335
2024-12-17 04:55:46.938726: Pseudo dice [0.5455]
2024-12-17 04:55:46.939720: Epoch time: 139.87 s
2024-12-17 04:55:46.940453: Yayy! New best EMA pseudo Dice: 0.5455
2024-12-17 04:55:48.670507: 
2024-12-17 04:55:48.672564: Epoch 1
2024-12-17 04:55:48.673799: Current learning rate: 0.00994
2024-12-17 04:57:17.267897: Validation loss improved from -0.23352 to -0.25379! Patience: 0/50
2024-12-17 04:57:17.269066: train_loss -0.2195
2024-12-17 04:57:17.269980: val_loss -0.2538
2024-12-17 04:57:17.270796: Pseudo dice [0.5229]
2024-12-17 04:57:17.271575: Epoch time: 88.6 s
2024-12-17 04:57:18.467319: 
2024-12-17 04:57:18.469091: Epoch 2
2024-12-17 04:57:18.470320: Current learning rate: 0.00988
2024-12-17 04:58:47.820780: Validation loss improved from -0.25379 to -0.32256! Patience: 0/50
2024-12-17 04:58:47.821833: train_loss -0.2844
2024-12-17 04:58:47.822650: val_loss -0.3226
2024-12-17 04:58:47.823377: Pseudo dice [0.6107]
2024-12-17 04:58:47.824041: Epoch time: 89.36 s
2024-12-17 04:58:47.824754: Yayy! New best EMA pseudo Dice: 0.55
2024-12-17 04:58:49.414718: 
2024-12-17 04:58:49.416467: Epoch 3
2024-12-17 04:58:49.417267: Current learning rate: 0.00982
2024-12-17 05:00:18.999645: Validation loss improved from -0.32256 to -0.33183! Patience: 0/50
2024-12-17 05:00:19.000612: train_loss -0.3135
2024-12-17 05:00:19.001656: val_loss -0.3318
2024-12-17 05:00:19.002717: Pseudo dice [0.6182]
2024-12-17 05:00:19.003412: Epoch time: 89.59 s
2024-12-17 05:00:19.004354: Yayy! New best EMA pseudo Dice: 0.5568
2024-12-17 05:00:20.569401: 
2024-12-17 05:00:20.570596: Epoch 4
2024-12-17 05:00:20.571737: Current learning rate: 0.00976
2024-12-17 05:01:50.185220: Validation loss improved from -0.33183 to -0.39397! Patience: 0/50
2024-12-17 05:01:50.186466: train_loss -0.3564
2024-12-17 05:01:50.187595: val_loss -0.394
2024-12-17 05:01:50.189375: Pseudo dice [0.6424]
2024-12-17 05:01:50.190253: Epoch time: 89.62 s
2024-12-17 05:01:50.643970: Yayy! New best EMA pseudo Dice: 0.5654
2024-12-17 05:01:52.214121: 
2024-12-17 05:01:52.216020: Epoch 5
2024-12-17 05:01:52.216735: Current learning rate: 0.0097
2024-12-17 05:03:21.907228: Validation loss improved from -0.39397 to -0.39919! Patience: 0/50
2024-12-17 05:03:21.908589: train_loss -0.389
2024-12-17 05:03:21.909493: val_loss -0.3992
2024-12-17 05:03:21.910160: Pseudo dice [0.6482]
2024-12-17 05:03:21.910885: Epoch time: 89.7 s
2024-12-17 05:03:21.911479: Yayy! New best EMA pseudo Dice: 0.5737
2024-12-17 05:03:23.434824: 
2024-12-17 05:03:23.436544: Epoch 6
2024-12-17 05:03:23.437380: Current learning rate: 0.00964
2024-12-17 05:04:53.132575: Validation loss improved from -0.39919 to -0.45562! Patience: 0/50
2024-12-17 05:04:53.133794: train_loss -0.4143
2024-12-17 05:04:53.134975: val_loss -0.4556
2024-12-17 05:04:53.136335: Pseudo dice [0.6896]
2024-12-17 05:04:53.137119: Epoch time: 89.7 s
2024-12-17 05:04:53.137883: Yayy! New best EMA pseudo Dice: 0.5852
2024-12-17 05:04:54.687268: 
2024-12-17 05:04:54.688958: Epoch 7
2024-12-17 05:04:54.690074: Current learning rate: 0.00958
2024-12-17 05:06:24.345051: Validation loss did not improve from -0.45562. Patience: 1/50
2024-12-17 05:06:24.346220: train_loss -0.4091
2024-12-17 05:06:24.347167: val_loss -0.4302
2024-12-17 05:06:24.347816: Pseudo dice [0.6637]
2024-12-17 05:06:24.348567: Epoch time: 89.66 s
2024-12-17 05:06:24.349335: Yayy! New best EMA pseudo Dice: 0.5931
2024-12-17 05:06:25.932130: 
2024-12-17 05:06:25.934378: Epoch 8
2024-12-17 05:06:25.935437: Current learning rate: 0.00952
2024-12-17 05:07:56.093602: Validation loss did not improve from -0.45562. Patience: 2/50
2024-12-17 05:07:56.094745: train_loss -0.4372
2024-12-17 05:07:56.095403: val_loss -0.446
2024-12-17 05:07:56.096078: Pseudo dice [0.6817]
2024-12-17 05:07:56.096694: Epoch time: 90.16 s
2024-12-17 05:07:56.097261: Yayy! New best EMA pseudo Dice: 0.602
2024-12-17 05:07:57.711002: 
2024-12-17 05:07:57.712693: Epoch 9
2024-12-17 05:07:57.713376: Current learning rate: 0.00946
2024-12-17 05:09:27.632225: Validation loss improved from -0.45562 to -0.47693! Patience: 2/50
2024-12-17 05:09:27.633156: train_loss -0.4454
2024-12-17 05:09:27.634148: val_loss -0.4769
2024-12-17 05:09:27.634856: Pseudo dice [0.6856]
2024-12-17 05:09:27.635526: Epoch time: 89.92 s
2024-12-17 05:09:27.975476: Yayy! New best EMA pseudo Dice: 0.6103
2024-12-17 05:09:29.519292: 
2024-12-17 05:09:29.520888: Epoch 10
2024-12-17 05:09:29.522178: Current learning rate: 0.0094
2024-12-17 05:10:59.488144: Validation loss did not improve from -0.47693. Patience: 1/50
2024-12-17 05:10:59.489277: train_loss -0.4513
2024-12-17 05:10:59.490032: val_loss -0.4484
2024-12-17 05:10:59.490682: Pseudo dice [0.6798]
2024-12-17 05:10:59.491298: Epoch time: 89.97 s
2024-12-17 05:10:59.491902: Yayy! New best EMA pseudo Dice: 0.6173
2024-12-17 05:11:01.088448: 
2024-12-17 05:11:01.090132: Epoch 11
2024-12-17 05:11:01.091614: Current learning rate: 0.00934
2024-12-17 05:12:30.947786: Validation loss did not improve from -0.47693. Patience: 2/50
2024-12-17 05:12:30.948694: train_loss -0.473
2024-12-17 05:12:30.949436: val_loss -0.4466
2024-12-17 05:12:30.950080: Pseudo dice [0.6828]
2024-12-17 05:12:30.950712: Epoch time: 89.86 s
2024-12-17 05:12:30.951394: Yayy! New best EMA pseudo Dice: 0.6238
2024-12-17 05:12:32.532429: 
2024-12-17 05:12:32.534078: Epoch 12
2024-12-17 05:12:32.534894: Current learning rate: 0.00928
2024-12-17 05:14:02.373383: Validation loss improved from -0.47693 to -0.51083! Patience: 2/50
2024-12-17 05:14:02.374528: train_loss -0.4722
2024-12-17 05:14:02.375396: val_loss -0.5108
2024-12-17 05:14:02.376186: Pseudo dice [0.7112]
2024-12-17 05:14:02.376881: Epoch time: 89.84 s
2024-12-17 05:14:02.377565: Yayy! New best EMA pseudo Dice: 0.6326
2024-12-17 05:14:04.017588: 
2024-12-17 05:14:04.019395: Epoch 13
2024-12-17 05:14:04.020210: Current learning rate: 0.00922
2024-12-17 05:15:33.909778: Validation loss did not improve from -0.51083. Patience: 1/50
2024-12-17 05:15:33.910573: train_loss -0.472
2024-12-17 05:15:33.911539: val_loss -0.5028
2024-12-17 05:15:33.912149: Pseudo dice [0.7108]
2024-12-17 05:15:33.912791: Epoch time: 89.89 s
2024-12-17 05:15:33.913497: Yayy! New best EMA pseudo Dice: 0.6404
2024-12-17 05:15:35.529953: 
2024-12-17 05:15:35.531641: Epoch 14
2024-12-17 05:15:35.532386: Current learning rate: 0.00916
2024-12-17 05:17:05.388056: Validation loss did not improve from -0.51083. Patience: 2/50
2024-12-17 05:17:05.388697: train_loss -0.4911
2024-12-17 05:17:05.389537: val_loss -0.4667
2024-12-17 05:17:05.390331: Pseudo dice [0.6921]
2024-12-17 05:17:05.391078: Epoch time: 89.86 s
2024-12-17 05:17:05.735423: Yayy! New best EMA pseudo Dice: 0.6456
2024-12-17 05:17:07.350767: 
2024-12-17 05:17:07.353759: Epoch 15
2024-12-17 05:17:07.354877: Current learning rate: 0.0091
2024-12-17 05:18:37.064137: Validation loss did not improve from -0.51083. Patience: 3/50
2024-12-17 05:18:37.065158: train_loss -0.5175
2024-12-17 05:18:37.066091: val_loss -0.5077
2024-12-17 05:18:37.066796: Pseudo dice [0.7197]
2024-12-17 05:18:37.067471: Epoch time: 89.72 s
2024-12-17 05:18:37.068222: Yayy! New best EMA pseudo Dice: 0.653
2024-12-17 05:18:38.647855: 
2024-12-17 05:18:38.649617: Epoch 16
2024-12-17 05:18:38.650442: Current learning rate: 0.00903
2024-12-17 05:20:08.688346: Validation loss improved from -0.51083 to -0.52475! Patience: 3/50
2024-12-17 05:20:08.689111: train_loss -0.5215
2024-12-17 05:20:08.689899: val_loss -0.5248
2024-12-17 05:20:08.690694: Pseudo dice [0.7272]
2024-12-17 05:20:08.691523: Epoch time: 90.04 s
2024-12-17 05:20:08.692391: Yayy! New best EMA pseudo Dice: 0.6604
2024-12-17 05:20:10.307080: 
2024-12-17 05:20:10.308979: Epoch 17
2024-12-17 05:20:10.309927: Current learning rate: 0.00897
2024-12-17 05:21:40.286185: Validation loss did not improve from -0.52475. Patience: 1/50
2024-12-17 05:21:40.287175: train_loss -0.5082
2024-12-17 05:21:40.288078: val_loss -0.5145
2024-12-17 05:21:40.288839: Pseudo dice [0.7161]
2024-12-17 05:21:40.289585: Epoch time: 89.98 s
2024-12-17 05:21:40.290293: Yayy! New best EMA pseudo Dice: 0.666
2024-12-17 05:21:41.906414: 
2024-12-17 05:21:41.908178: Epoch 18
2024-12-17 05:21:41.908941: Current learning rate: 0.00891
2024-12-17 05:23:11.800884: Validation loss did not improve from -0.52475. Patience: 2/50
2024-12-17 05:23:11.801946: train_loss -0.5159
2024-12-17 05:23:11.802707: val_loss -0.4857
2024-12-17 05:23:11.803303: Pseudo dice [0.6997]
2024-12-17 05:23:11.803887: Epoch time: 89.9 s
2024-12-17 05:23:11.804550: Yayy! New best EMA pseudo Dice: 0.6693
2024-12-17 05:23:13.773349: 
2024-12-17 05:23:13.774976: Epoch 19
2024-12-17 05:23:13.775732: Current learning rate: 0.00885
2024-12-17 05:24:43.774549: Validation loss improved from -0.52475 to -0.52925! Patience: 2/50
2024-12-17 05:24:43.775519: train_loss -0.5286
2024-12-17 05:24:43.776506: val_loss -0.5293
2024-12-17 05:24:43.777116: Pseudo dice [0.719]
2024-12-17 05:24:43.777839: Epoch time: 90.0 s
2024-12-17 05:24:44.134458: Yayy! New best EMA pseudo Dice: 0.6743
2024-12-17 05:24:45.802050: 
2024-12-17 05:24:45.803517: Epoch 20
2024-12-17 05:24:45.804166: Current learning rate: 0.00879
2024-12-17 05:26:15.817096: Validation loss did not improve from -0.52925. Patience: 1/50
2024-12-17 05:26:15.818058: train_loss -0.5414
2024-12-17 05:26:15.819079: val_loss -0.5227
2024-12-17 05:26:15.819829: Pseudo dice [0.7195]
2024-12-17 05:26:15.820616: Epoch time: 90.02 s
2024-12-17 05:26:15.821319: Yayy! New best EMA pseudo Dice: 0.6788
2024-12-17 05:26:17.447183: 
2024-12-17 05:26:17.448406: Epoch 21
2024-12-17 05:26:17.449155: Current learning rate: 0.00873
2024-12-17 05:27:47.463498: Validation loss did not improve from -0.52925. Patience: 2/50
2024-12-17 05:27:47.464592: train_loss -0.5479
2024-12-17 05:27:47.465384: val_loss -0.5071
2024-12-17 05:27:47.466056: Pseudo dice [0.7124]
2024-12-17 05:27:47.466767: Epoch time: 90.02 s
2024-12-17 05:27:47.467438: Yayy! New best EMA pseudo Dice: 0.6822
2024-12-17 05:27:49.007310: 
2024-12-17 05:27:49.008296: Epoch 22
2024-12-17 05:27:49.009075: Current learning rate: 0.00867
2024-12-17 05:29:19.051330: Validation loss improved from -0.52925 to -0.53396! Patience: 2/50
2024-12-17 05:29:19.052588: train_loss -0.5524
2024-12-17 05:29:19.053472: val_loss -0.534
2024-12-17 05:29:19.054218: Pseudo dice [0.73]
2024-12-17 05:29:19.054871: Epoch time: 90.05 s
2024-12-17 05:29:19.055507: Yayy! New best EMA pseudo Dice: 0.687
2024-12-17 05:29:20.575632: 
2024-12-17 05:29:20.577676: Epoch 23
2024-12-17 05:29:20.578491: Current learning rate: 0.00861
2024-12-17 05:30:50.581203: Validation loss did not improve from -0.53396. Patience: 1/50
2024-12-17 05:30:50.581963: train_loss -0.5463
2024-12-17 05:30:50.582780: val_loss -0.5129
2024-12-17 05:30:50.583429: Pseudo dice [0.7112]
2024-12-17 05:30:50.584084: Epoch time: 90.01 s
2024-12-17 05:30:50.584665: Yayy! New best EMA pseudo Dice: 0.6894
2024-12-17 05:30:52.108720: 
2024-12-17 05:30:52.110430: Epoch 24
2024-12-17 05:30:52.111385: Current learning rate: 0.00855
2024-12-17 05:32:22.323639: Validation loss improved from -0.53396 to -0.56633! Patience: 1/50
2024-12-17 05:32:22.324462: train_loss -0.5527
2024-12-17 05:32:22.325700: val_loss -0.5663
2024-12-17 05:32:22.326397: Pseudo dice [0.7463]
2024-12-17 05:32:22.327398: Epoch time: 90.22 s
2024-12-17 05:32:22.684345: Yayy! New best EMA pseudo Dice: 0.6951
2024-12-17 05:32:24.287924: 
2024-12-17 05:32:24.289513: Epoch 25
2024-12-17 05:32:24.290337: Current learning rate: 0.00849
2024-12-17 05:33:54.276767: Validation loss did not improve from -0.56633. Patience: 1/50
2024-12-17 05:33:54.278174: train_loss -0.5555
2024-12-17 05:33:54.279095: val_loss -0.5179
2024-12-17 05:33:54.279782: Pseudo dice [0.7211]
2024-12-17 05:33:54.280401: Epoch time: 89.99 s
2024-12-17 05:33:54.281024: Yayy! New best EMA pseudo Dice: 0.6977
2024-12-17 05:33:55.863121: 
2024-12-17 05:33:55.864837: Epoch 26
2024-12-17 05:33:55.865495: Current learning rate: 0.00843
2024-12-17 05:35:25.848327: Validation loss did not improve from -0.56633. Patience: 2/50
2024-12-17 05:35:25.849608: train_loss -0.5614
2024-12-17 05:35:25.850469: val_loss -0.5319
2024-12-17 05:35:25.851169: Pseudo dice [0.7214]
2024-12-17 05:35:25.851883: Epoch time: 89.99 s
2024-12-17 05:35:25.852793: Yayy! New best EMA pseudo Dice: 0.7
2024-12-17 05:35:27.409041: 
2024-12-17 05:35:27.410537: Epoch 27
2024-12-17 05:35:27.411287: Current learning rate: 0.00836
2024-12-17 05:36:57.308342: Validation loss did not improve from -0.56633. Patience: 3/50
2024-12-17 05:36:57.309499: train_loss -0.5695
2024-12-17 05:36:57.310406: val_loss -0.5314
2024-12-17 05:36:57.311230: Pseudo dice [0.7305]
2024-12-17 05:36:57.312206: Epoch time: 89.9 s
2024-12-17 05:36:57.312995: Yayy! New best EMA pseudo Dice: 0.7031
2024-12-17 05:36:58.904866: 
2024-12-17 05:36:58.907311: Epoch 28
2024-12-17 05:36:58.908168: Current learning rate: 0.0083
2024-12-17 05:38:28.611594: Validation loss did not improve from -0.56633. Patience: 4/50
2024-12-17 05:38:28.612782: train_loss -0.5687
2024-12-17 05:38:28.613663: val_loss -0.5278
2024-12-17 05:38:28.614334: Pseudo dice [0.721]
2024-12-17 05:38:28.615034: Epoch time: 89.71 s
2024-12-17 05:38:28.615825: Yayy! New best EMA pseudo Dice: 0.7049
2024-12-17 05:38:30.499661: 
2024-12-17 05:38:30.501475: Epoch 29
2024-12-17 05:38:30.502343: Current learning rate: 0.00824
2024-12-17 05:40:00.097628: Validation loss did not improve from -0.56633. Patience: 5/50
2024-12-17 05:40:00.098468: train_loss -0.5747
2024-12-17 05:40:00.099402: val_loss -0.5163
2024-12-17 05:40:00.100021: Pseudo dice [0.7166]
2024-12-17 05:40:00.100742: Epoch time: 89.6 s
2024-12-17 05:40:00.440284: Yayy! New best EMA pseudo Dice: 0.7061
2024-12-17 05:40:02.013451: 
2024-12-17 05:40:02.015353: Epoch 30
2024-12-17 05:40:02.016372: Current learning rate: 0.00818
2024-12-17 05:41:31.599322: Validation loss did not improve from -0.56633. Patience: 6/50
2024-12-17 05:41:31.600389: train_loss -0.5771
2024-12-17 05:41:31.601435: val_loss -0.5251
2024-12-17 05:41:31.602173: Pseudo dice [0.7213]
2024-12-17 05:41:31.602862: Epoch time: 89.59 s
2024-12-17 05:41:31.603604: Yayy! New best EMA pseudo Dice: 0.7076
2024-12-17 05:41:33.201859: 
2024-12-17 05:41:33.203533: Epoch 31
2024-12-17 05:41:33.204358: Current learning rate: 0.00812
2024-12-17 05:43:02.751198: Validation loss did not improve from -0.56633. Patience: 7/50
2024-12-17 05:43:02.752189: train_loss -0.5882
2024-12-17 05:43:02.752969: val_loss -0.5542
2024-12-17 05:43:02.753686: Pseudo dice [0.7428]
2024-12-17 05:43:02.754315: Epoch time: 89.55 s
2024-12-17 05:43:02.754946: Yayy! New best EMA pseudo Dice: 0.7111
2024-12-17 05:43:04.317468: 
2024-12-17 05:43:04.318572: Epoch 32
2024-12-17 05:43:04.319443: Current learning rate: 0.00806
2024-12-17 05:44:33.908005: Validation loss did not improve from -0.56633. Patience: 8/50
2024-12-17 05:44:33.909153: train_loss -0.5908
2024-12-17 05:44:33.909949: val_loss -0.5294
2024-12-17 05:44:33.910705: Pseudo dice [0.73]
2024-12-17 05:44:33.911483: Epoch time: 89.59 s
2024-12-17 05:44:33.912110: Yayy! New best EMA pseudo Dice: 0.713
2024-12-17 05:44:35.527424: 
2024-12-17 05:44:35.528267: Epoch 33
2024-12-17 05:44:35.528980: Current learning rate: 0.008
2024-12-17 05:46:05.152178: Validation loss did not improve from -0.56633. Patience: 9/50
2024-12-17 05:46:05.153008: train_loss -0.5798
2024-12-17 05:46:05.153718: val_loss -0.5074
2024-12-17 05:46:05.154399: Pseudo dice [0.7076]
2024-12-17 05:46:05.155104: Epoch time: 89.63 s
2024-12-17 05:46:06.394317: 
2024-12-17 05:46:06.395794: Epoch 34
2024-12-17 05:46:06.396545: Current learning rate: 0.00793
2024-12-17 05:47:36.010568: Validation loss did not improve from -0.56633. Patience: 10/50
2024-12-17 05:47:36.011875: train_loss -0.597
2024-12-17 05:47:36.012745: val_loss -0.5387
2024-12-17 05:47:36.013587: Pseudo dice [0.7269]
2024-12-17 05:47:36.014547: Epoch time: 89.62 s
2024-12-17 05:47:36.357994: Yayy! New best EMA pseudo Dice: 0.7139
2024-12-17 05:47:37.919081: 
2024-12-17 05:47:37.920445: Epoch 35
2024-12-17 05:47:37.921444: Current learning rate: 0.00787
2024-12-17 05:49:07.478285: Validation loss did not improve from -0.56633. Patience: 11/50
2024-12-17 05:49:07.479427: train_loss -0.5941
2024-12-17 05:49:07.480318: val_loss -0.5617
2024-12-17 05:49:07.481030: Pseudo dice [0.7474]
2024-12-17 05:49:07.481692: Epoch time: 89.56 s
2024-12-17 05:49:07.482423: Yayy! New best EMA pseudo Dice: 0.7173
2024-12-17 05:49:09.057476: 
2024-12-17 05:49:09.059850: Epoch 36
2024-12-17 05:49:09.060769: Current learning rate: 0.00781
2024-12-17 05:50:38.860795: Validation loss did not improve from -0.56633. Patience: 12/50
2024-12-17 05:50:38.861818: train_loss -0.5975
2024-12-17 05:50:38.862572: val_loss -0.5214
2024-12-17 05:50:38.863244: Pseudo dice [0.7226]
2024-12-17 05:50:38.863988: Epoch time: 89.81 s
2024-12-17 05:50:38.864772: Yayy! New best EMA pseudo Dice: 0.7178
2024-12-17 05:50:40.470976: 
2024-12-17 05:50:40.472812: Epoch 37
2024-12-17 05:50:40.474016: Current learning rate: 0.00775
2024-12-17 05:52:10.349685: Validation loss improved from -0.56633 to -0.57181! Patience: 12/50
2024-12-17 05:52:10.350720: train_loss -0.6009
2024-12-17 05:52:10.351533: val_loss -0.5718
2024-12-17 05:52:10.352354: Pseudo dice [0.752]
2024-12-17 05:52:10.352980: Epoch time: 89.88 s
2024-12-17 05:52:10.353557: Yayy! New best EMA pseudo Dice: 0.7212
2024-12-17 05:52:11.957497: 
2024-12-17 05:52:11.959834: Epoch 38
2024-12-17 05:52:11.960664: Current learning rate: 0.00769
2024-12-17 05:53:41.827466: Validation loss did not improve from -0.57181. Patience: 1/50
2024-12-17 05:53:41.828409: train_loss -0.6169
2024-12-17 05:53:41.829645: val_loss -0.558
2024-12-17 05:53:41.830424: Pseudo dice [0.7421]
2024-12-17 05:53:41.831264: Epoch time: 89.87 s
2024-12-17 05:53:41.832133: Yayy! New best EMA pseudo Dice: 0.7233
2024-12-17 05:53:43.422643: 
2024-12-17 05:53:43.425128: Epoch 39
2024-12-17 05:53:43.426005: Current learning rate: 0.00763
2024-12-17 05:55:13.349838: Validation loss did not improve from -0.57181. Patience: 2/50
2024-12-17 05:55:13.351077: train_loss -0.6123
2024-12-17 05:55:13.351965: val_loss -0.5509
2024-12-17 05:55:13.352674: Pseudo dice [0.7422]
2024-12-17 05:55:13.353335: Epoch time: 89.93 s
2024-12-17 05:55:13.699236: Yayy! New best EMA pseudo Dice: 0.7252
2024-12-17 05:55:15.684760: 
2024-12-17 05:55:15.686313: Epoch 40
2024-12-17 05:55:15.687080: Current learning rate: 0.00756
2024-12-17 05:56:45.496851: Validation loss did not improve from -0.57181. Patience: 3/50
2024-12-17 05:56:45.497850: train_loss -0.6164
2024-12-17 05:56:45.498593: val_loss -0.5696
2024-12-17 05:56:45.499229: Pseudo dice [0.7527]
2024-12-17 05:56:45.499897: Epoch time: 89.81 s
2024-12-17 05:56:45.500621: Yayy! New best EMA pseudo Dice: 0.7279
2024-12-17 05:56:47.144977: 
2024-12-17 05:56:47.146280: Epoch 41
2024-12-17 05:56:47.147145: Current learning rate: 0.0075
2024-12-17 05:58:17.173279: Validation loss did not improve from -0.57181. Patience: 4/50
2024-12-17 05:58:17.174285: train_loss -0.6206
2024-12-17 05:58:17.175186: val_loss -0.5486
2024-12-17 05:58:17.175812: Pseudo dice [0.7397]
2024-12-17 05:58:17.176502: Epoch time: 90.03 s
2024-12-17 05:58:17.177104: Yayy! New best EMA pseudo Dice: 0.7291
2024-12-17 05:58:18.727371: 
2024-12-17 05:58:18.729002: Epoch 42
2024-12-17 05:58:18.729899: Current learning rate: 0.00744
2024-12-17 05:59:48.685035: Validation loss did not improve from -0.57181. Patience: 5/50
2024-12-17 05:59:48.686210: train_loss -0.6208
2024-12-17 05:59:48.687083: val_loss -0.5447
2024-12-17 05:59:48.687710: Pseudo dice [0.7315]
2024-12-17 05:59:48.688315: Epoch time: 89.96 s
2024-12-17 05:59:48.688884: Yayy! New best EMA pseudo Dice: 0.7294
2024-12-17 05:59:50.265604: 
2024-12-17 05:59:50.267684: Epoch 43
2024-12-17 05:59:50.268396: Current learning rate: 0.00738
2024-12-17 06:01:20.225264: Validation loss did not improve from -0.57181. Patience: 6/50
2024-12-17 06:01:20.226384: train_loss -0.6172
2024-12-17 06:01:20.227348: val_loss -0.547
2024-12-17 06:01:20.228298: Pseudo dice [0.7404]
2024-12-17 06:01:20.229336: Epoch time: 89.96 s
2024-12-17 06:01:20.230049: Yayy! New best EMA pseudo Dice: 0.7305
2024-12-17 06:01:21.772747: 
2024-12-17 06:01:21.774572: Epoch 44
2024-12-17 06:01:21.775295: Current learning rate: 0.00732
2024-12-17 06:02:51.678549: Validation loss did not improve from -0.57181. Patience: 7/50
2024-12-17 06:02:51.679694: train_loss -0.6065
2024-12-17 06:02:51.680394: val_loss -0.5453
2024-12-17 06:02:51.681065: Pseudo dice [0.7315]
2024-12-17 06:02:51.681683: Epoch time: 89.91 s
2024-12-17 06:02:52.056551: Yayy! New best EMA pseudo Dice: 0.7306
2024-12-17 06:02:53.662130: 
2024-12-17 06:02:53.663880: Epoch 45
2024-12-17 06:02:53.664758: Current learning rate: 0.00725
2024-12-17 06:04:23.516605: Validation loss improved from -0.57181 to -0.58253! Patience: 7/50
2024-12-17 06:04:23.517733: train_loss -0.6181
2024-12-17 06:04:23.518519: val_loss -0.5825
2024-12-17 06:04:23.519206: Pseudo dice [0.757]
2024-12-17 06:04:23.519919: Epoch time: 89.86 s
2024-12-17 06:04:23.520519: Yayy! New best EMA pseudo Dice: 0.7332
2024-12-17 06:04:25.114977: 
2024-12-17 06:04:25.116495: Epoch 46
2024-12-17 06:04:25.117141: Current learning rate: 0.00719
2024-12-17 06:05:55.096211: Validation loss improved from -0.58253 to -0.60569! Patience: 0/50
2024-12-17 06:05:55.097040: train_loss -0.6159
2024-12-17 06:05:55.098048: val_loss -0.6057
2024-12-17 06:05:55.098945: Pseudo dice [0.7737]
2024-12-17 06:05:55.099672: Epoch time: 89.98 s
2024-12-17 06:05:55.100325: Yayy! New best EMA pseudo Dice: 0.7373
2024-12-17 06:05:56.643684: 
2024-12-17 06:05:56.645439: Epoch 47
2024-12-17 06:05:56.646167: Current learning rate: 0.00713
2024-12-17 06:07:26.545597: Validation loss did not improve from -0.60569. Patience: 1/50
2024-12-17 06:07:26.546829: train_loss -0.6233
2024-12-17 06:07:26.547810: val_loss -0.5557
2024-12-17 06:07:26.548489: Pseudo dice [0.7477]
2024-12-17 06:07:26.549335: Epoch time: 89.9 s
2024-12-17 06:07:26.549993: Yayy! New best EMA pseudo Dice: 0.7383
2024-12-17 06:07:28.116440: 
2024-12-17 06:07:28.117997: Epoch 48
2024-12-17 06:07:28.118758: Current learning rate: 0.00707
2024-12-17 06:08:58.003402: Validation loss did not improve from -0.60569. Patience: 2/50
2024-12-17 06:08:58.005017: train_loss -0.6353
2024-12-17 06:08:58.006495: val_loss -0.5571
2024-12-17 06:08:58.007482: Pseudo dice [0.7434]
2024-12-17 06:08:58.008190: Epoch time: 89.89 s
2024-12-17 06:08:58.008951: Yayy! New best EMA pseudo Dice: 0.7388
2024-12-17 06:08:59.579800: 
2024-12-17 06:08:59.581671: Epoch 49
2024-12-17 06:08:59.582757: Current learning rate: 0.007
2024-12-17 06:10:29.299842: Validation loss did not improve from -0.60569. Patience: 3/50
2024-12-17 06:10:29.301230: train_loss -0.6363
2024-12-17 06:10:29.302237: val_loss -0.5559
2024-12-17 06:10:29.302891: Pseudo dice [0.7393]
2024-12-17 06:10:29.303712: Epoch time: 89.72 s
2024-12-17 06:10:29.665460: Yayy! New best EMA pseudo Dice: 0.7389
2024-12-17 06:10:31.612582: 
2024-12-17 06:10:31.614616: Epoch 50
2024-12-17 06:10:31.615556: Current learning rate: 0.00694
2024-12-17 06:12:01.129342: Validation loss did not improve from -0.60569. Patience: 4/50
2024-12-17 06:12:01.130295: train_loss -0.6349
2024-12-17 06:12:01.131173: val_loss -0.5556
2024-12-17 06:12:01.132063: Pseudo dice [0.7512]
2024-12-17 06:12:01.133382: Epoch time: 89.52 s
2024-12-17 06:12:01.134277: Yayy! New best EMA pseudo Dice: 0.7401
2024-12-17 06:12:02.701108: 
2024-12-17 06:12:02.702420: Epoch 51
2024-12-17 06:12:02.703133: Current learning rate: 0.00688
2024-12-17 06:13:32.357440: Validation loss did not improve from -0.60569. Patience: 5/50
2024-12-17 06:13:32.358508: train_loss -0.6324
2024-12-17 06:13:32.359364: val_loss -0.5286
2024-12-17 06:13:32.360035: Pseudo dice [0.7245]
2024-12-17 06:13:32.360891: Epoch time: 89.66 s
2024-12-17 06:13:33.582922: 
2024-12-17 06:13:33.585294: Epoch 52
2024-12-17 06:13:33.586129: Current learning rate: 0.00682
2024-12-17 06:15:03.265679: Validation loss did not improve from -0.60569. Patience: 6/50
2024-12-17 06:15:03.266865: train_loss -0.6415
2024-12-17 06:15:03.267654: val_loss -0.5654
2024-12-17 06:15:03.268241: Pseudo dice [0.7473]
2024-12-17 06:15:03.268854: Epoch time: 89.69 s
2024-12-17 06:15:04.490792: 
2024-12-17 06:15:04.491924: Epoch 53
2024-12-17 06:15:04.492667: Current learning rate: 0.00675
2024-12-17 06:16:34.167097: Validation loss did not improve from -0.60569. Patience: 7/50
2024-12-17 06:16:34.168384: train_loss -0.6341
2024-12-17 06:16:34.169396: val_loss -0.5872
2024-12-17 06:16:34.170187: Pseudo dice [0.7583]
2024-12-17 06:16:34.170955: Epoch time: 89.68 s
2024-12-17 06:16:34.171771: Yayy! New best EMA pseudo Dice: 0.7413
2024-12-17 06:16:35.794080: 
2024-12-17 06:16:35.796185: Epoch 54
2024-12-17 06:16:35.797181: Current learning rate: 0.00669
2024-12-17 06:18:05.347381: Validation loss did not improve from -0.60569. Patience: 8/50
2024-12-17 06:18:05.348433: train_loss -0.6418
2024-12-17 06:18:05.349296: val_loss -0.5517
2024-12-17 06:18:05.349955: Pseudo dice [0.7332]
2024-12-17 06:18:05.350556: Epoch time: 89.56 s
2024-12-17 06:18:06.947316: 
2024-12-17 06:18:06.948941: Epoch 55
2024-12-17 06:18:06.949661: Current learning rate: 0.00663
2024-12-17 06:19:36.640348: Validation loss did not improve from -0.60569. Patience: 9/50
2024-12-17 06:19:36.641486: train_loss -0.6418
2024-12-17 06:19:36.642755: val_loss -0.5576
2024-12-17 06:19:36.643548: Pseudo dice [0.7533]
2024-12-17 06:19:36.644322: Epoch time: 89.69 s
2024-12-17 06:19:36.645068: Yayy! New best EMA pseudo Dice: 0.7418
2024-12-17 06:19:38.178755: 
2024-12-17 06:19:38.179944: Epoch 56
2024-12-17 06:19:38.180602: Current learning rate: 0.00657
2024-12-17 06:21:07.683149: Validation loss did not improve from -0.60569. Patience: 10/50
2024-12-17 06:21:07.684186: train_loss -0.6425
2024-12-17 06:21:07.684971: val_loss -0.5896
2024-12-17 06:21:07.685574: Pseudo dice [0.7568]
2024-12-17 06:21:07.686176: Epoch time: 89.51 s
2024-12-17 06:21:07.686733: Yayy! New best EMA pseudo Dice: 0.7433
2024-12-17 06:21:09.230861: 
2024-12-17 06:21:09.232984: Epoch 57
2024-12-17 06:21:09.233763: Current learning rate: 0.0065
2024-12-17 06:22:39.053750: Validation loss did not improve from -0.60569. Patience: 11/50
2024-12-17 06:22:39.055107: train_loss -0.6453
2024-12-17 06:22:39.056003: val_loss -0.5773
2024-12-17 06:22:39.056757: Pseudo dice [0.7554]
2024-12-17 06:22:39.057522: Epoch time: 89.83 s
2024-12-17 06:22:39.058205: Yayy! New best EMA pseudo Dice: 0.7445
2024-12-17 06:22:40.622253: 
2024-12-17 06:22:40.624081: Epoch 58
2024-12-17 06:22:40.624871: Current learning rate: 0.00644
2024-12-17 06:24:10.502101: Validation loss did not improve from -0.60569. Patience: 12/50
2024-12-17 06:24:10.502926: train_loss -0.6547
2024-12-17 06:24:10.503769: val_loss -0.5732
2024-12-17 06:24:10.504537: Pseudo dice [0.7593]
2024-12-17 06:24:10.505225: Epoch time: 89.88 s
2024-12-17 06:24:10.505903: Yayy! New best EMA pseudo Dice: 0.746
2024-12-17 06:24:12.088250: 
2024-12-17 06:24:12.090095: Epoch 59
2024-12-17 06:24:12.091011: Current learning rate: 0.00638
2024-12-17 06:25:41.951317: Validation loss did not improve from -0.60569. Patience: 13/50
2024-12-17 06:25:41.952597: train_loss -0.6546
2024-12-17 06:25:41.953489: val_loss -0.5546
2024-12-17 06:25:41.954116: Pseudo dice [0.7379]
2024-12-17 06:25:41.954849: Epoch time: 89.87 s
2024-12-17 06:25:43.587195: 
2024-12-17 06:25:43.588941: Epoch 60
2024-12-17 06:25:43.589763: Current learning rate: 0.00631
2024-12-17 06:27:13.415626: Validation loss did not improve from -0.60569. Patience: 14/50
2024-12-17 06:27:13.416661: train_loss -0.6644
2024-12-17 06:27:13.417553: val_loss -0.573
2024-12-17 06:27:13.418236: Pseudo dice [0.754]
2024-12-17 06:27:13.418867: Epoch time: 89.83 s
2024-12-17 06:27:13.419579: Yayy! New best EMA pseudo Dice: 0.746
2024-12-17 06:27:15.322279: 
2024-12-17 06:27:15.323860: Epoch 61
2024-12-17 06:27:15.325143: Current learning rate: 0.00625
2024-12-17 06:28:45.129364: Validation loss did not improve from -0.60569. Patience: 15/50
2024-12-17 06:28:45.130175: train_loss -0.6637
2024-12-17 06:28:45.131181: val_loss -0.5921
2024-12-17 06:28:45.132235: Pseudo dice [0.763]
2024-12-17 06:28:45.132954: Epoch time: 89.81 s
2024-12-17 06:28:45.133603: Yayy! New best EMA pseudo Dice: 0.7477
2024-12-17 06:28:46.695875: 
2024-12-17 06:28:46.697371: Epoch 62
2024-12-17 06:28:46.698092: Current learning rate: 0.00619
2024-12-17 06:30:16.460370: Validation loss did not improve from -0.60569. Patience: 16/50
2024-12-17 06:30:16.461419: train_loss -0.662
2024-12-17 06:30:16.462155: val_loss -0.5556
2024-12-17 06:30:16.462791: Pseudo dice [0.7428]
2024-12-17 06:30:16.463384: Epoch time: 89.77 s
2024-12-17 06:30:17.725541: 
2024-12-17 06:30:17.726987: Epoch 63
2024-12-17 06:30:17.727884: Current learning rate: 0.00612
2024-12-17 06:31:47.524344: Validation loss did not improve from -0.60569. Patience: 17/50
2024-12-17 06:31:47.525590: train_loss -0.6666
2024-12-17 06:31:47.526436: val_loss -0.566
2024-12-17 06:31:47.527157: Pseudo dice [0.7414]
2024-12-17 06:31:47.527854: Epoch time: 89.8 s
2024-12-17 06:31:48.753290: 
2024-12-17 06:31:48.755018: Epoch 64
2024-12-17 06:31:48.756035: Current learning rate: 0.00606
2024-12-17 06:33:18.561428: Validation loss did not improve from -0.60569. Patience: 18/50
2024-12-17 06:33:18.562673: train_loss -0.6657
2024-12-17 06:33:18.563659: val_loss -0.5643
2024-12-17 06:33:18.564403: Pseudo dice [0.7498]
2024-12-17 06:33:18.565269: Epoch time: 89.81 s
2024-12-17 06:33:20.170787: 
2024-12-17 06:33:20.172490: Epoch 65
2024-12-17 06:33:20.173402: Current learning rate: 0.006
2024-12-17 06:34:50.131459: Validation loss did not improve from -0.60569. Patience: 19/50
2024-12-17 06:34:50.132610: train_loss -0.6565
2024-12-17 06:34:50.133373: val_loss -0.5843
2024-12-17 06:34:50.134126: Pseudo dice [0.7641]
2024-12-17 06:34:50.134768: Epoch time: 89.96 s
2024-12-17 06:34:50.135364: Yayy! New best EMA pseudo Dice: 0.7487
2024-12-17 06:34:51.751689: 
2024-12-17 06:34:51.753119: Epoch 66
2024-12-17 06:34:51.753981: Current learning rate: 0.00593
2024-12-17 06:36:21.844339: Validation loss did not improve from -0.60569. Patience: 20/50
2024-12-17 06:36:21.845695: train_loss -0.6636
2024-12-17 06:36:21.846620: val_loss -0.5513
2024-12-17 06:36:21.847266: Pseudo dice [0.7398]
2024-12-17 06:36:21.847945: Epoch time: 90.09 s
2024-12-17 06:36:23.081907: 
2024-12-17 06:36:23.083577: Epoch 67
2024-12-17 06:36:23.084372: Current learning rate: 0.00587
2024-12-17 06:37:53.083469: Validation loss did not improve from -0.60569. Patience: 21/50
2024-12-17 06:37:53.084336: train_loss -0.6697
2024-12-17 06:37:53.085337: val_loss -0.5883
2024-12-17 06:37:53.086162: Pseudo dice [0.764]
2024-12-17 06:37:53.087175: Epoch time: 90.0 s
2024-12-17 06:37:53.087978: Yayy! New best EMA pseudo Dice: 0.7494
2024-12-17 06:37:54.656586: 
2024-12-17 06:37:54.658252: Epoch 68
2024-12-17 06:37:54.659622: Current learning rate: 0.00581
2024-12-17 06:39:24.643223: Validation loss did not improve from -0.60569. Patience: 22/50
2024-12-17 06:39:24.644314: train_loss -0.6782
2024-12-17 06:39:24.645123: val_loss -0.5823
2024-12-17 06:39:24.645888: Pseudo dice [0.7572]
2024-12-17 06:39:24.646760: Epoch time: 89.99 s
2024-12-17 06:39:24.647524: Yayy! New best EMA pseudo Dice: 0.7502
2024-12-17 06:39:26.243341: 
2024-12-17 06:39:26.244967: Epoch 69
2024-12-17 06:39:26.245755: Current learning rate: 0.00574
2024-12-17 06:40:56.257282: Validation loss did not improve from -0.60569. Patience: 23/50
2024-12-17 06:40:56.258485: train_loss -0.6788
2024-12-17 06:40:56.259407: val_loss -0.5869
2024-12-17 06:40:56.260034: Pseudo dice [0.7619]
2024-12-17 06:40:56.260805: Epoch time: 90.02 s
2024-12-17 06:40:56.628150: Yayy! New best EMA pseudo Dice: 0.7514
2024-12-17 06:40:58.228303: 
2024-12-17 06:40:58.229940: Epoch 70
2024-12-17 06:40:58.230653: Current learning rate: 0.00568
2024-12-17 06:42:28.147316: Validation loss did not improve from -0.60569. Patience: 24/50
2024-12-17 06:42:28.148207: train_loss -0.6804
2024-12-17 06:42:28.148916: val_loss -0.5617
2024-12-17 06:42:28.149528: Pseudo dice [0.7463]
2024-12-17 06:42:28.150097: Epoch time: 89.92 s
2024-12-17 06:42:29.398646: 
2024-12-17 06:42:29.400152: Epoch 71
2024-12-17 06:42:29.401155: Current learning rate: 0.00562
2024-12-17 06:43:59.531705: Validation loss did not improve from -0.60569. Patience: 25/50
2024-12-17 06:43:59.533077: train_loss -0.6725
2024-12-17 06:43:59.533961: val_loss -0.6038
2024-12-17 06:43:59.534618: Pseudo dice [0.77]
2024-12-17 06:43:59.535263: Epoch time: 90.14 s
2024-12-17 06:43:59.535960: Yayy! New best EMA pseudo Dice: 0.7528
2024-12-17 06:44:01.455699: 
2024-12-17 06:44:01.457903: Epoch 72
2024-12-17 06:44:01.458622: Current learning rate: 0.00555
2024-12-17 06:45:31.388283: Validation loss did not improve from -0.60569. Patience: 26/50
2024-12-17 06:45:31.389171: train_loss -0.6817
2024-12-17 06:45:31.389885: val_loss -0.5784
2024-12-17 06:45:31.390549: Pseudo dice [0.76]
2024-12-17 06:45:31.391188: Epoch time: 89.93 s
2024-12-17 06:45:31.391769: Yayy! New best EMA pseudo Dice: 0.7535
2024-12-17 06:45:32.959544: 
2024-12-17 06:45:32.961477: Epoch 73
2024-12-17 06:45:32.962174: Current learning rate: 0.00549
2024-12-17 06:47:03.046759: Validation loss did not improve from -0.60569. Patience: 27/50
2024-12-17 06:47:03.047904: train_loss -0.6839
2024-12-17 06:47:03.048910: val_loss -0.5899
2024-12-17 06:47:03.049664: Pseudo dice [0.7685]
2024-12-17 06:47:03.050384: Epoch time: 90.09 s
2024-12-17 06:47:03.050976: Yayy! New best EMA pseudo Dice: 0.755
2024-12-17 06:47:04.722970: 
2024-12-17 06:47:04.724650: Epoch 74
2024-12-17 06:47:04.725491: Current learning rate: 0.00542
2024-12-17 06:48:34.660425: Validation loss improved from -0.60569 to -0.61136! Patience: 27/50
2024-12-17 06:48:34.661719: train_loss -0.6791
2024-12-17 06:48:34.662562: val_loss -0.6114
2024-12-17 06:48:34.663192: Pseudo dice [0.7744]
2024-12-17 06:48:34.663818: Epoch time: 89.94 s
2024-12-17 06:48:35.030871: Yayy! New best EMA pseudo Dice: 0.7569
2024-12-17 06:48:36.635227: 
2024-12-17 06:48:36.636344: Epoch 75
2024-12-17 06:48:36.637192: Current learning rate: 0.00536
2024-12-17 06:50:06.661352: Validation loss did not improve from -0.61136. Patience: 1/50
2024-12-17 06:50:06.662439: train_loss -0.6797
2024-12-17 06:50:06.663324: val_loss -0.5984
2024-12-17 06:50:06.664120: Pseudo dice [0.7738]
2024-12-17 06:50:06.664878: Epoch time: 90.03 s
2024-12-17 06:50:06.665504: Yayy! New best EMA pseudo Dice: 0.7586
2024-12-17 06:50:08.304296: 
2024-12-17 06:50:08.305905: Epoch 76
2024-12-17 06:50:08.306874: Current learning rate: 0.00529
2024-12-17 06:51:38.282923: Validation loss did not improve from -0.61136. Patience: 2/50
2024-12-17 06:51:38.284187: train_loss -0.6866
2024-12-17 06:51:38.284971: val_loss -0.5877
2024-12-17 06:51:38.285572: Pseudo dice [0.7609]
2024-12-17 06:51:38.286316: Epoch time: 89.98 s
2024-12-17 06:51:38.287077: Yayy! New best EMA pseudo Dice: 0.7588
2024-12-17 06:51:39.890941: 
2024-12-17 06:51:39.892885: Epoch 77
2024-12-17 06:51:39.893795: Current learning rate: 0.00523
2024-12-17 06:53:09.977618: Validation loss did not improve from -0.61136. Patience: 3/50
2024-12-17 06:53:09.978867: train_loss -0.6909
2024-12-17 06:53:09.979726: val_loss -0.5633
2024-12-17 06:53:09.980400: Pseudo dice [0.7562]
2024-12-17 06:53:09.981057: Epoch time: 90.09 s
2024-12-17 06:53:11.240402: 
2024-12-17 06:53:11.241824: Epoch 78
2024-12-17 06:53:11.242535: Current learning rate: 0.00517
2024-12-17 06:54:41.496647: Validation loss did not improve from -0.61136. Patience: 4/50
2024-12-17 06:54:41.497800: train_loss -0.6911
2024-12-17 06:54:41.498850: val_loss -0.5997
2024-12-17 06:54:41.499682: Pseudo dice [0.767]
2024-12-17 06:54:41.500671: Epoch time: 90.26 s
2024-12-17 06:54:41.501546: Yayy! New best EMA pseudo Dice: 0.7594
2024-12-17 06:54:43.208546: 
2024-12-17 06:54:43.210255: Epoch 79
2024-12-17 06:54:43.211333: Current learning rate: 0.0051
2024-12-17 06:56:13.232657: Validation loss did not improve from -0.61136. Patience: 5/50
2024-12-17 06:56:13.233785: train_loss -0.6884
2024-12-17 06:56:13.234632: val_loss -0.5896
2024-12-17 06:56:13.235370: Pseudo dice [0.7697]
2024-12-17 06:56:13.236256: Epoch time: 90.03 s
2024-12-17 06:56:13.609576: Yayy! New best EMA pseudo Dice: 0.7604
2024-12-17 06:56:15.253496: 
2024-12-17 06:56:15.255609: Epoch 80
2024-12-17 06:56:15.256335: Current learning rate: 0.00504
2024-12-17 06:57:45.341166: Validation loss did not improve from -0.61136. Patience: 6/50
2024-12-17 06:57:45.342114: train_loss -0.694
2024-12-17 06:57:45.342994: val_loss -0.6017
2024-12-17 06:57:45.343768: Pseudo dice [0.7716]
2024-12-17 06:57:45.344483: Epoch time: 90.09 s
2024-12-17 06:57:45.345161: Yayy! New best EMA pseudo Dice: 0.7616
2024-12-17 06:57:46.970078: 
2024-12-17 06:57:46.971271: Epoch 81
2024-12-17 06:57:46.971861: Current learning rate: 0.00497
2024-12-17 06:59:17.009887: Validation loss did not improve from -0.61136. Patience: 7/50
2024-12-17 06:59:17.011246: train_loss -0.6972
2024-12-17 06:59:17.012057: val_loss -0.6001
2024-12-17 06:59:17.013046: Pseudo dice [0.7618]
2024-12-17 06:59:17.013832: Epoch time: 90.04 s
2024-12-17 06:59:17.014813: Yayy! New best EMA pseudo Dice: 0.7616
2024-12-17 06:59:19.016282: 
2024-12-17 06:59:19.017844: Epoch 82
2024-12-17 06:59:19.018507: Current learning rate: 0.00491
2024-12-17 07:00:49.343102: Validation loss did not improve from -0.61136. Patience: 8/50
2024-12-17 07:00:49.343951: train_loss -0.6962
2024-12-17 07:00:49.344716: val_loss -0.6068
2024-12-17 07:00:49.345643: Pseudo dice [0.7755]
2024-12-17 07:00:49.346255: Epoch time: 90.33 s
2024-12-17 07:00:49.346870: Yayy! New best EMA pseudo Dice: 0.763
2024-12-17 07:00:50.912436: 
2024-12-17 07:00:50.914061: Epoch 83
2024-12-17 07:00:50.914863: Current learning rate: 0.00484
2024-12-17 07:02:20.972547: Validation loss did not improve from -0.61136. Patience: 9/50
2024-12-17 07:02:20.973485: train_loss -0.6976
2024-12-17 07:02:20.974421: val_loss -0.5843
2024-12-17 07:02:20.975162: Pseudo dice [0.7567]
2024-12-17 07:02:20.975934: Epoch time: 90.06 s
2024-12-17 07:02:22.208125: 
2024-12-17 07:02:22.209271: Epoch 84
2024-12-17 07:02:22.209938: Current learning rate: 0.00478
2024-12-17 07:03:52.291928: Validation loss did not improve from -0.61136. Patience: 10/50
2024-12-17 07:03:52.294047: train_loss -0.6991
2024-12-17 07:03:52.294876: val_loss -0.5958
2024-12-17 07:03:52.295516: Pseudo dice [0.7616]
2024-12-17 07:03:52.296425: Epoch time: 90.09 s
2024-12-17 07:03:53.837993: 
2024-12-17 07:03:53.839523: Epoch 85
2024-12-17 07:03:53.840223: Current learning rate: 0.00471
2024-12-17 07:05:23.867564: Validation loss did not improve from -0.61136. Patience: 11/50
2024-12-17 07:05:23.868337: train_loss -0.7003
2024-12-17 07:05:23.869277: val_loss -0.6108
2024-12-17 07:05:23.870179: Pseudo dice [0.7757]
2024-12-17 07:05:23.871127: Epoch time: 90.03 s
2024-12-17 07:05:23.871889: Yayy! New best EMA pseudo Dice: 0.7636
2024-12-17 07:05:25.449086: 
2024-12-17 07:05:25.450417: Epoch 86
2024-12-17 07:05:25.451393: Current learning rate: 0.00465
2024-12-17 07:06:55.456853: Validation loss did not improve from -0.61136. Patience: 12/50
2024-12-17 07:06:55.457883: train_loss -0.7006
2024-12-17 07:06:55.458801: val_loss -0.571
2024-12-17 07:06:55.459408: Pseudo dice [0.7502]
2024-12-17 07:06:55.460026: Epoch time: 90.01 s
2024-12-17 07:06:56.636356: 
2024-12-17 07:06:56.638110: Epoch 87
2024-12-17 07:06:56.638777: Current learning rate: 0.00458
2024-12-17 07:08:26.653439: Validation loss did not improve from -0.61136. Patience: 13/50
2024-12-17 07:08:26.654689: train_loss -0.7022
2024-12-17 07:08:26.655936: val_loss -0.5647
2024-12-17 07:08:26.656606: Pseudo dice [0.7484]
2024-12-17 07:08:26.657248: Epoch time: 90.02 s
2024-12-17 07:08:27.862217: 
2024-12-17 07:08:27.863990: Epoch 88
2024-12-17 07:08:27.864931: Current learning rate: 0.00452
2024-12-17 07:09:57.951626: Validation loss did not improve from -0.61136. Patience: 14/50
2024-12-17 07:09:57.952834: train_loss -0.7038
2024-12-17 07:09:57.953793: val_loss -0.5998
2024-12-17 07:09:57.954448: Pseudo dice [0.777]
2024-12-17 07:09:57.955144: Epoch time: 90.09 s
2024-12-17 07:09:59.153054: 
2024-12-17 07:09:59.154068: Epoch 89
2024-12-17 07:09:59.154687: Current learning rate: 0.00445
2024-12-17 07:11:29.146547: Validation loss did not improve from -0.61136. Patience: 15/50
2024-12-17 07:11:29.147751: train_loss -0.7038
2024-12-17 07:11:29.148833: val_loss -0.5737
2024-12-17 07:11:29.149438: Pseudo dice [0.7557]
2024-12-17 07:11:29.150067: Epoch time: 90.0 s
2024-12-17 07:11:30.741771: 
2024-12-17 07:11:30.743083: Epoch 90
2024-12-17 07:11:30.743763: Current learning rate: 0.00438
2024-12-17 07:13:00.774513: Validation loss did not improve from -0.61136. Patience: 16/50
2024-12-17 07:13:00.775667: train_loss -0.7119
2024-12-17 07:13:00.776877: val_loss -0.5814
2024-12-17 07:13:00.777902: Pseudo dice [0.758]
2024-12-17 07:13:00.778764: Epoch time: 90.03 s
2024-12-17 07:13:01.974606: 
2024-12-17 07:13:01.976307: Epoch 91
2024-12-17 07:13:01.977183: Current learning rate: 0.00432
2024-12-17 07:14:32.225164: Validation loss did not improve from -0.61136. Patience: 17/50
2024-12-17 07:14:32.226100: train_loss -0.7025
2024-12-17 07:14:32.227094: val_loss -0.5951
2024-12-17 07:14:32.227933: Pseudo dice [0.7663]
2024-12-17 07:14:32.228662: Epoch time: 90.25 s
2024-12-17 07:14:33.423444: 
2024-12-17 07:14:33.425458: Epoch 92
2024-12-17 07:14:33.426547: Current learning rate: 0.00425
2024-12-17 07:16:03.558908: Validation loss did not improve from -0.61136. Patience: 18/50
2024-12-17 07:16:03.559932: train_loss -0.7036
2024-12-17 07:16:03.560727: val_loss -0.5835
2024-12-17 07:16:03.561517: Pseudo dice [0.7626]
2024-12-17 07:16:03.562207: Epoch time: 90.14 s
2024-12-17 07:16:05.174512: 
2024-12-17 07:16:05.175984: Epoch 93
2024-12-17 07:16:05.176770: Current learning rate: 0.00419
2024-12-17 07:17:35.220940: Validation loss did not improve from -0.61136. Patience: 19/50
2024-12-17 07:17:35.221966: train_loss -0.7142
2024-12-17 07:17:35.222945: val_loss -0.5764
2024-12-17 07:17:35.223656: Pseudo dice [0.7593]
2024-12-17 07:17:35.224364: Epoch time: 90.05 s
2024-12-17 07:17:36.400161: 
2024-12-17 07:17:36.401880: Epoch 94
2024-12-17 07:17:36.402858: Current learning rate: 0.00412
2024-12-17 07:19:06.446193: Validation loss improved from -0.61136 to -0.61675! Patience: 19/50
2024-12-17 07:19:06.447393: train_loss -0.7157
2024-12-17 07:19:06.448255: val_loss -0.6167
2024-12-17 07:19:06.448956: Pseudo dice [0.7843]
2024-12-17 07:19:06.449692: Epoch time: 90.05 s
2024-12-17 07:19:06.816973: Yayy! New best EMA pseudo Dice: 0.764
2024-12-17 07:19:08.366762: 
2024-12-17 07:19:08.368402: Epoch 95
2024-12-17 07:19:08.369142: Current learning rate: 0.00405
2024-12-17 07:20:38.450425: Validation loss did not improve from -0.61675. Patience: 1/50
2024-12-17 07:20:38.451597: train_loss -0.7186
2024-12-17 07:20:38.452668: val_loss -0.5919
2024-12-17 07:20:38.453621: Pseudo dice [0.7675]
2024-12-17 07:20:38.454552: Epoch time: 90.09 s
2024-12-17 07:20:38.455450: Yayy! New best EMA pseudo Dice: 0.7643
2024-12-17 07:20:40.021024: 
2024-12-17 07:20:40.022656: Epoch 96
2024-12-17 07:20:40.023587: Current learning rate: 0.00399
2024-12-17 07:22:10.293297: Validation loss did not improve from -0.61675. Patience: 2/50
2024-12-17 07:22:10.294407: train_loss -0.7134
2024-12-17 07:22:10.295308: val_loss -0.5994
2024-12-17 07:22:10.296054: Pseudo dice [0.7693]
2024-12-17 07:22:10.296756: Epoch time: 90.27 s
2024-12-17 07:22:10.297619: Yayy! New best EMA pseudo Dice: 0.7648
2024-12-17 07:22:11.894842: 
2024-12-17 07:22:11.896434: Epoch 97
2024-12-17 07:22:11.897377: Current learning rate: 0.00392
2024-12-17 07:23:41.738199: Validation loss did not improve from -0.61675. Patience: 3/50
2024-12-17 07:23:41.739577: train_loss -0.716
2024-12-17 07:23:41.740354: val_loss -0.5744
2024-12-17 07:23:41.741063: Pseudo dice [0.758]
2024-12-17 07:23:41.741693: Epoch time: 89.85 s
2024-12-17 07:23:42.944027: 
2024-12-17 07:23:42.945820: Epoch 98
2024-12-17 07:23:42.946601: Current learning rate: 0.00385
2024-12-17 07:25:12.769675: Validation loss did not improve from -0.61675. Patience: 4/50
2024-12-17 07:25:12.770877: train_loss -0.7209
2024-12-17 07:25:12.771835: val_loss -0.5863
2024-12-17 07:25:12.772594: Pseudo dice [0.7627]
2024-12-17 07:25:12.773309: Epoch time: 89.83 s
2024-12-17 07:25:13.989564: 
2024-12-17 07:25:13.990954: Epoch 99
2024-12-17 07:25:13.991706: Current learning rate: 0.00379
2024-12-17 07:26:43.822206: Validation loss improved from -0.61675 to -0.62468! Patience: 4/50
2024-12-17 07:26:43.823380: train_loss -0.7218
2024-12-17 07:26:43.824683: val_loss -0.6247
2024-12-17 07:26:43.825493: Pseudo dice [0.7849]
2024-12-17 07:26:43.826117: Epoch time: 89.83 s
2024-12-17 07:26:44.214348: Yayy! New best EMA pseudo Dice: 0.7661
2024-12-17 07:26:45.786670: 
2024-12-17 07:26:45.788241: Epoch 100
2024-12-17 07:26:45.788974: Current learning rate: 0.00372
2024-12-17 07:28:15.606425: Validation loss did not improve from -0.62468. Patience: 1/50
2024-12-17 07:28:15.607721: train_loss -0.7215
2024-12-17 07:28:15.608870: val_loss -0.6129
2024-12-17 07:28:15.609596: Pseudo dice [0.7823]
2024-12-17 07:28:15.610382: Epoch time: 89.82 s
2024-12-17 07:28:15.611131: Yayy! New best EMA pseudo Dice: 0.7677
2024-12-17 07:28:17.148248: 
2024-12-17 07:28:17.149819: Epoch 101
2024-12-17 07:28:17.150579: Current learning rate: 0.00365
2024-12-17 07:29:46.948504: Validation loss did not improve from -0.62468. Patience: 2/50
2024-12-17 07:29:46.949483: train_loss -0.7256
2024-12-17 07:29:46.950185: val_loss -0.5905
2024-12-17 07:29:46.950930: Pseudo dice [0.7701]
2024-12-17 07:29:46.951567: Epoch time: 89.8 s
2024-12-17 07:29:46.952178: Yayy! New best EMA pseudo Dice: 0.7679
2024-12-17 07:29:48.465214: 
2024-12-17 07:29:48.467507: Epoch 102
2024-12-17 07:29:48.468204: Current learning rate: 0.00359
2024-12-17 07:31:18.277105: Validation loss did not improve from -0.62468. Patience: 3/50
2024-12-17 07:31:18.277999: train_loss -0.7213
2024-12-17 07:31:18.279047: val_loss -0.5978
2024-12-17 07:31:18.279687: Pseudo dice [0.7719]
2024-12-17 07:31:18.280239: Epoch time: 89.81 s
2024-12-17 07:31:18.281022: Yayy! New best EMA pseudo Dice: 0.7683
2024-12-17 07:31:19.872703: 
2024-12-17 07:31:19.874756: Epoch 103
2024-12-17 07:31:19.875519: Current learning rate: 0.00352
2024-12-17 07:32:49.708192: Validation loss did not improve from -0.62468. Patience: 4/50
2024-12-17 07:32:49.708951: train_loss -0.7187
2024-12-17 07:32:49.709706: val_loss -0.6078
2024-12-17 07:32:49.710448: Pseudo dice [0.7702]
2024-12-17 07:32:49.711073: Epoch time: 89.84 s
2024-12-17 07:32:49.711722: Yayy! New best EMA pseudo Dice: 0.7685
2024-12-17 07:32:51.627498: 
2024-12-17 07:32:51.629119: Epoch 104
2024-12-17 07:32:51.629942: Current learning rate: 0.00345
2024-12-17 07:34:21.674284: Validation loss did not improve from -0.62468. Patience: 5/50
2024-12-17 07:34:21.675720: train_loss -0.7261
2024-12-17 07:34:21.676635: val_loss -0.5991
2024-12-17 07:34:21.677275: Pseudo dice [0.7701]
2024-12-17 07:34:21.677998: Epoch time: 90.05 s
2024-12-17 07:34:22.053635: Yayy! New best EMA pseudo Dice: 0.7687
2024-12-17 07:34:23.676180: 
2024-12-17 07:34:23.677486: Epoch 105
2024-12-17 07:34:23.678230: Current learning rate: 0.00338
2024-12-17 07:35:53.682121: Validation loss did not improve from -0.62468. Patience: 6/50
2024-12-17 07:35:53.683384: train_loss -0.7264
2024-12-17 07:35:53.684271: val_loss -0.5821
2024-12-17 07:35:53.685128: Pseudo dice [0.7612]
2024-12-17 07:35:53.685815: Epoch time: 90.01 s
2024-12-17 07:35:54.893670: 
2024-12-17 07:35:54.895267: Epoch 106
2024-12-17 07:35:54.896168: Current learning rate: 0.00332
2024-12-17 07:37:24.920878: Validation loss did not improve from -0.62468. Patience: 7/50
2024-12-17 07:37:24.921732: train_loss -0.7278
2024-12-17 07:37:24.922420: val_loss -0.5944
2024-12-17 07:37:24.923031: Pseudo dice [0.7678]
2024-12-17 07:37:24.923807: Epoch time: 90.03 s
2024-12-17 07:37:26.141077: 
2024-12-17 07:37:26.142434: Epoch 107
2024-12-17 07:37:26.143217: Current learning rate: 0.00325
2024-12-17 07:38:56.242166: Validation loss did not improve from -0.62468. Patience: 8/50
2024-12-17 07:38:56.243358: train_loss -0.7322
2024-12-17 07:38:56.244253: val_loss -0.6044
2024-12-17 07:38:56.244895: Pseudo dice [0.7736]
2024-12-17 07:38:56.245550: Epoch time: 90.1 s
2024-12-17 07:38:57.495614: 
2024-12-17 07:38:57.497438: Epoch 108
2024-12-17 07:38:57.498295: Current learning rate: 0.00318
2024-12-17 07:40:27.569908: Validation loss did not improve from -0.62468. Patience: 9/50
2024-12-17 07:40:27.571776: train_loss -0.7286
2024-12-17 07:40:27.572824: val_loss -0.5796
2024-12-17 07:40:27.573522: Pseudo dice [0.7577]
2024-12-17 07:40:27.574218: Epoch time: 90.08 s
2024-12-17 07:40:28.845252: 
2024-12-17 07:40:28.847226: Epoch 109
2024-12-17 07:40:28.848345: Current learning rate: 0.00311
2024-12-17 07:41:58.856340: Validation loss did not improve from -0.62468. Patience: 10/50
2024-12-17 07:41:58.857367: train_loss -0.7339
2024-12-17 07:41:58.858191: val_loss -0.5632
2024-12-17 07:41:58.858825: Pseudo dice [0.7484]
2024-12-17 07:41:58.859649: Epoch time: 90.01 s
2024-12-17 07:42:00.505391: 
2024-12-17 07:42:00.506770: Epoch 110
2024-12-17 07:42:00.507721: Current learning rate: 0.00304
2024-12-17 07:43:30.495629: Validation loss did not improve from -0.62468. Patience: 11/50
2024-12-17 07:43:30.496930: train_loss -0.7287
2024-12-17 07:43:30.497897: val_loss -0.5967
2024-12-17 07:43:30.498565: Pseudo dice [0.7691]
2024-12-17 07:43:30.499293: Epoch time: 89.99 s
2024-12-17 07:43:31.727954: 
2024-12-17 07:43:31.729414: Epoch 111
2024-12-17 07:43:31.730091: Current learning rate: 0.00297
2024-12-17 07:45:01.721522: Validation loss did not improve from -0.62468. Patience: 12/50
2024-12-17 07:45:01.722637: train_loss -0.7347
2024-12-17 07:45:01.723582: val_loss -0.6065
2024-12-17 07:45:01.724270: Pseudo dice [0.7775]
2024-12-17 07:45:01.725045: Epoch time: 90.0 s
2024-12-17 07:45:02.938221: 
2024-12-17 07:45:02.939821: Epoch 112
2024-12-17 07:45:02.940508: Current learning rate: 0.00291
2024-12-17 07:46:33.049299: Validation loss did not improve from -0.62468. Patience: 13/50
2024-12-17 07:46:33.050289: train_loss -0.7354
2024-12-17 07:46:33.051339: val_loss -0.6094
2024-12-17 07:46:33.052630: Pseudo dice [0.7806]
2024-12-17 07:46:33.053496: Epoch time: 90.11 s
2024-12-17 07:46:34.268529: 
2024-12-17 07:46:34.270205: Epoch 113
2024-12-17 07:46:34.271067: Current learning rate: 0.00284
2024-12-17 07:48:04.415975: Validation loss did not improve from -0.62468. Patience: 14/50
2024-12-17 07:48:04.416959: train_loss -0.7357
2024-12-17 07:48:04.417772: val_loss -0.6021
2024-12-17 07:48:04.418410: Pseudo dice [0.7749]
2024-12-17 07:48:04.419194: Epoch time: 90.15 s
2024-12-17 07:48:04.420051: Yayy! New best EMA pseudo Dice: 0.769
2024-12-17 07:48:06.021338: 
2024-12-17 07:48:06.023134: Epoch 114
2024-12-17 07:48:06.024163: Current learning rate: 0.00277
2024-12-17 07:49:36.168949: Validation loss did not improve from -0.62468. Patience: 15/50
2024-12-17 07:49:36.169815: train_loss -0.7447
2024-12-17 07:49:36.170687: val_loss -0.5846
2024-12-17 07:49:36.171371: Pseudo dice [0.7697]
2024-12-17 07:49:36.172037: Epoch time: 90.15 s
2024-12-17 07:49:36.542690: Yayy! New best EMA pseudo Dice: 0.7691
2024-12-17 07:49:38.540347: 
2024-12-17 07:49:38.541658: Epoch 115
2024-12-17 07:49:38.542656: Current learning rate: 0.0027
2024-12-17 07:51:08.611135: Validation loss did not improve from -0.62468. Patience: 16/50
2024-12-17 07:51:08.612118: train_loss -0.7416
2024-12-17 07:51:08.613213: val_loss -0.5927
2024-12-17 07:51:08.614013: Pseudo dice [0.7692]
2024-12-17 07:51:08.614786: Epoch time: 90.07 s
2024-12-17 07:51:08.615481: Yayy! New best EMA pseudo Dice: 0.7691
2024-12-17 07:51:10.256200: 
2024-12-17 07:51:10.257061: Epoch 116
2024-12-17 07:51:10.257747: Current learning rate: 0.00263
2024-12-17 07:52:40.334372: Validation loss did not improve from -0.62468. Patience: 17/50
2024-12-17 07:52:40.335652: train_loss -0.7394
2024-12-17 07:52:40.336465: val_loss -0.5981
2024-12-17 07:52:40.337144: Pseudo dice [0.773]
2024-12-17 07:52:40.337865: Epoch time: 90.08 s
2024-12-17 07:52:40.338543: Yayy! New best EMA pseudo Dice: 0.7695
2024-12-17 07:52:41.967620: 
2024-12-17 07:52:41.969207: Epoch 117
2024-12-17 07:52:41.969864: Current learning rate: 0.00256
2024-12-17 07:54:12.067395: Validation loss improved from -0.62468 to -0.63299! Patience: 17/50
2024-12-17 07:54:12.068386: train_loss -0.7394
2024-12-17 07:54:12.069371: val_loss -0.633
2024-12-17 07:54:12.070145: Pseudo dice [0.7895]
2024-12-17 07:54:12.070812: Epoch time: 90.1 s
2024-12-17 07:54:12.071501: Yayy! New best EMA pseudo Dice: 0.7715
2024-12-17 07:54:13.669742: 
2024-12-17 07:54:13.671394: Epoch 118
2024-12-17 07:54:13.672039: Current learning rate: 0.00249
2024-12-17 07:55:43.789006: Validation loss did not improve from -0.63299. Patience: 1/50
2024-12-17 07:55:43.789971: train_loss -0.7404
2024-12-17 07:55:43.790819: val_loss -0.6156
2024-12-17 07:55:43.791552: Pseudo dice [0.7823]
2024-12-17 07:55:43.792315: Epoch time: 90.12 s
2024-12-17 07:55:43.793029: Yayy! New best EMA pseudo Dice: 0.7726
2024-12-17 07:55:45.377908: 
2024-12-17 07:55:45.379536: Epoch 119
2024-12-17 07:55:45.380477: Current learning rate: 0.00242
2024-12-17 07:57:15.703233: Validation loss did not improve from -0.63299. Patience: 2/50
2024-12-17 07:57:15.704164: train_loss -0.7398
2024-12-17 07:57:15.705415: val_loss -0.5963
2024-12-17 07:57:15.706390: Pseudo dice [0.7639]
2024-12-17 07:57:15.707309: Epoch time: 90.33 s
2024-12-17 07:57:17.321165: 
2024-12-17 07:57:17.323270: Epoch 120
2024-12-17 07:57:17.324178: Current learning rate: 0.00235
2024-12-17 07:58:47.582378: Validation loss did not improve from -0.63299. Patience: 3/50
2024-12-17 07:58:47.583544: train_loss -0.7476
2024-12-17 07:58:47.584637: val_loss -0.6146
2024-12-17 07:58:47.585441: Pseudo dice [0.7834]
2024-12-17 07:58:47.586093: Epoch time: 90.26 s
2024-12-17 07:58:47.586797: Yayy! New best EMA pseudo Dice: 0.7729
2024-12-17 07:58:49.311577: 
2024-12-17 07:58:49.313390: Epoch 121
2024-12-17 07:58:49.314121: Current learning rate: 0.00228
2024-12-17 08:00:19.382137: Validation loss did not improve from -0.63299. Patience: 4/50
2024-12-17 08:00:19.383394: train_loss -0.7483
2024-12-17 08:00:19.384214: val_loss -0.5957
2024-12-17 08:00:19.384933: Pseudo dice [0.7705]
2024-12-17 08:00:19.385724: Epoch time: 90.07 s
2024-12-17 08:00:20.687328: 
2024-12-17 08:00:20.689173: Epoch 122
2024-12-17 08:00:20.689944: Current learning rate: 0.00221
2024-12-17 08:01:50.723689: Validation loss did not improve from -0.63299. Patience: 5/50
2024-12-17 08:01:50.724924: train_loss -0.7452
2024-12-17 08:01:50.726043: val_loss -0.6088
2024-12-17 08:01:50.726879: Pseudo dice [0.7793]
2024-12-17 08:01:50.727761: Epoch time: 90.04 s
2024-12-17 08:01:50.728511: Yayy! New best EMA pseudo Dice: 0.7733
2024-12-17 08:01:52.416931: 
2024-12-17 08:01:52.418365: Epoch 123
2024-12-17 08:01:52.419326: Current learning rate: 0.00214
2024-12-17 08:03:22.545147: Validation loss did not improve from -0.63299. Patience: 6/50
2024-12-17 08:03:22.546306: train_loss -0.7425
2024-12-17 08:03:22.547144: val_loss -0.6106
2024-12-17 08:03:22.547744: Pseudo dice [0.7837]
2024-12-17 08:03:22.548381: Epoch time: 90.13 s
2024-12-17 08:03:22.549088: Yayy! New best EMA pseudo Dice: 0.7744
2024-12-17 08:03:24.150577: 
2024-12-17 08:03:24.152035: Epoch 124
2024-12-17 08:03:24.152728: Current learning rate: 0.00207
2024-12-17 08:04:54.395017: Validation loss did not improve from -0.63299. Patience: 7/50
2024-12-17 08:04:54.399027: train_loss -0.7465
2024-12-17 08:04:54.400640: val_loss -0.6021
2024-12-17 08:04:54.401512: Pseudo dice [0.768]
2024-12-17 08:04:54.402190: Epoch time: 90.25 s
2024-12-17 08:04:56.066866: 
2024-12-17 08:04:56.068631: Epoch 125
2024-12-17 08:04:56.069383: Current learning rate: 0.00199
2024-12-17 08:06:26.265454: Validation loss did not improve from -0.63299. Patience: 8/50
2024-12-17 08:06:26.266880: train_loss -0.7464
2024-12-17 08:06:26.267751: val_loss -0.6118
2024-12-17 08:06:26.268445: Pseudo dice [0.7781]
2024-12-17 08:06:26.269190: Epoch time: 90.2 s
2024-12-17 08:06:28.183834: 
2024-12-17 08:06:28.185559: Epoch 126
2024-12-17 08:06:28.186296: Current learning rate: 0.00192
2024-12-17 08:07:58.338466: Validation loss did not improve from -0.63299. Patience: 9/50
2024-12-17 08:07:58.339799: train_loss -0.7486
2024-12-17 08:07:58.340630: val_loss -0.611
2024-12-17 08:07:58.341336: Pseudo dice [0.7841]
2024-12-17 08:07:58.342124: Epoch time: 90.16 s
2024-12-17 08:07:58.342813: Yayy! New best EMA pseudo Dice: 0.7752
2024-12-17 08:07:59.966634: 
2024-12-17 08:07:59.968076: Epoch 127
2024-12-17 08:07:59.968765: Current learning rate: 0.00185
2024-12-17 08:09:30.183129: Validation loss did not improve from -0.63299. Patience: 10/50
2024-12-17 08:09:30.185432: train_loss -0.7523
2024-12-17 08:09:30.186120: val_loss -0.6145
2024-12-17 08:09:30.186841: Pseudo dice [0.7788]
2024-12-17 08:09:30.187530: Epoch time: 90.22 s
2024-12-17 08:09:30.188217: Yayy! New best EMA pseudo Dice: 0.7755
2024-12-17 08:09:31.759233: 
2024-12-17 08:09:31.760413: Epoch 128
2024-12-17 08:09:31.761207: Current learning rate: 0.00178
2024-12-17 08:11:02.059101: Validation loss did not improve from -0.63299. Patience: 11/50
2024-12-17 08:11:02.060075: train_loss -0.7519
2024-12-17 08:11:02.060965: val_loss -0.608
2024-12-17 08:11:02.061752: Pseudo dice [0.7806]
2024-12-17 08:11:02.062661: Epoch time: 90.3 s
2024-12-17 08:11:02.063407: Yayy! New best EMA pseudo Dice: 0.776
2024-12-17 08:11:03.691149: 
2024-12-17 08:11:03.692995: Epoch 129
2024-12-17 08:11:03.693643: Current learning rate: 0.0017
2024-12-17 08:12:33.803208: Validation loss did not improve from -0.63299. Patience: 12/50
2024-12-17 08:12:33.804129: train_loss -0.7524
2024-12-17 08:12:33.804976: val_loss -0.5882
2024-12-17 08:12:33.805701: Pseudo dice [0.766]
2024-12-17 08:12:33.806365: Epoch time: 90.11 s
2024-12-17 08:12:35.387098: 
2024-12-17 08:12:35.388900: Epoch 130
2024-12-17 08:12:35.389726: Current learning rate: 0.00163
2024-12-17 08:14:05.502023: Validation loss did not improve from -0.63299. Patience: 13/50
2024-12-17 08:14:05.503281: train_loss -0.7507
2024-12-17 08:14:05.504220: val_loss -0.5902
2024-12-17 08:14:05.505005: Pseudo dice [0.7692]
2024-12-17 08:14:05.505801: Epoch time: 90.12 s
2024-12-17 08:14:06.732808: 
2024-12-17 08:14:06.734411: Epoch 131
2024-12-17 08:14:06.735082: Current learning rate: 0.00156
2024-12-17 08:15:36.820641: Validation loss did not improve from -0.63299. Patience: 14/50
2024-12-17 08:15:36.821683: train_loss -0.7526
2024-12-17 08:15:36.822515: val_loss -0.6225
2024-12-17 08:15:36.823270: Pseudo dice [0.783]
2024-12-17 08:15:36.824039: Epoch time: 90.09 s
2024-12-17 08:15:38.075501: 
2024-12-17 08:15:38.076624: Epoch 132
2024-12-17 08:15:38.077296: Current learning rate: 0.00148
2024-12-17 08:17:08.248168: Validation loss did not improve from -0.63299. Patience: 15/50
2024-12-17 08:17:08.249482: train_loss -0.7534
2024-12-17 08:17:08.250339: val_loss -0.6082
2024-12-17 08:17:08.251077: Pseudo dice [0.7822]
2024-12-17 08:17:08.251814: Epoch time: 90.17 s
2024-12-17 08:17:09.530093: 
2024-12-17 08:17:09.531794: Epoch 133
2024-12-17 08:17:09.532635: Current learning rate: 0.00141
2024-12-17 08:18:39.702595: Validation loss did not improve from -0.63299. Patience: 16/50
2024-12-17 08:18:39.703780: train_loss -0.7592
2024-12-17 08:18:39.704869: val_loss -0.5941
2024-12-17 08:18:39.705795: Pseudo dice [0.7747]
2024-12-17 08:18:39.706650: Epoch time: 90.17 s
2024-12-17 08:18:40.973744: 
2024-12-17 08:18:40.975102: Epoch 134
2024-12-17 08:18:40.975859: Current learning rate: 0.00133
2024-12-17 08:20:11.138375: Validation loss did not improve from -0.63299. Patience: 17/50
2024-12-17 08:20:11.139381: train_loss -0.7571
2024-12-17 08:20:11.140121: val_loss -0.5886
2024-12-17 08:20:11.140884: Pseudo dice [0.7627]
2024-12-17 08:20:11.141539: Epoch time: 90.17 s
2024-12-17 08:20:12.769582: 
2024-12-17 08:20:12.771123: Epoch 135
2024-12-17 08:20:12.772157: Current learning rate: 0.00126
2024-12-17 08:21:42.763780: Validation loss did not improve from -0.63299. Patience: 18/50
2024-12-17 08:21:42.764663: train_loss -0.7546
2024-12-17 08:21:42.765475: val_loss -0.6044
2024-12-17 08:21:42.766172: Pseudo dice [0.783]
2024-12-17 08:21:42.767020: Epoch time: 90.0 s
2024-12-17 08:21:44.392563: 
2024-12-17 08:21:44.394069: Epoch 136
2024-12-17 08:21:44.394989: Current learning rate: 0.00118
2024-12-17 08:23:14.670396: Validation loss did not improve from -0.63299. Patience: 19/50
2024-12-17 08:23:14.671320: train_loss -0.7542
2024-12-17 08:23:14.672001: val_loss -0.6082
2024-12-17 08:23:14.672827: Pseudo dice [0.7786]
2024-12-17 08:23:14.673599: Epoch time: 90.28 s
2024-12-17 08:23:15.976518: 
2024-12-17 08:23:15.978202: Epoch 137
2024-12-17 08:23:15.979182: Current learning rate: 0.00111
2024-12-17 08:24:46.105640: Validation loss did not improve from -0.63299. Patience: 20/50
2024-12-17 08:24:46.106921: train_loss -0.7606
2024-12-17 08:24:46.107863: val_loss -0.5974
2024-12-17 08:24:46.108569: Pseudo dice [0.7781]
2024-12-17 08:24:46.109244: Epoch time: 90.13 s
2024-12-17 08:24:47.340887: 
2024-12-17 08:24:47.342272: Epoch 138
2024-12-17 08:24:47.343147: Current learning rate: 0.00103
2024-12-17 08:26:17.465314: Validation loss did not improve from -0.63299. Patience: 21/50
2024-12-17 08:26:17.466249: train_loss -0.7614
2024-12-17 08:26:17.467219: val_loss -0.5945
2024-12-17 08:26:17.468061: Pseudo dice [0.764]
2024-12-17 08:26:17.468902: Epoch time: 90.13 s
2024-12-17 08:26:18.739906: 
2024-12-17 08:26:18.741840: Epoch 139
2024-12-17 08:26:18.742806: Current learning rate: 0.00095
2024-12-17 08:27:48.789727: Validation loss did not improve from -0.63299. Patience: 22/50
2024-12-17 08:27:48.790604: train_loss -0.7625
2024-12-17 08:27:48.791328: val_loss -0.6043
2024-12-17 08:27:48.791894: Pseudo dice [0.7773]
2024-12-17 08:27:48.792586: Epoch time: 90.05 s
2024-12-17 08:27:50.427754: 
2024-12-17 08:27:50.429040: Epoch 140
2024-12-17 08:27:50.429716: Current learning rate: 0.00087
2024-12-17 08:29:20.444454: Validation loss did not improve from -0.63299. Patience: 23/50
2024-12-17 08:29:20.445848: train_loss -0.7636
2024-12-17 08:29:20.446757: val_loss -0.6069
2024-12-17 08:29:20.447422: Pseudo dice [0.782]
2024-12-17 08:29:20.448074: Epoch time: 90.02 s
2024-12-17 08:29:21.720373: 
2024-12-17 08:29:21.722097: Epoch 141
2024-12-17 08:29:21.723114: Current learning rate: 0.00079
2024-12-17 08:30:51.819847: Validation loss did not improve from -0.63299. Patience: 24/50
2024-12-17 08:30:51.820789: train_loss -0.763
2024-12-17 08:30:51.821597: val_loss -0.6037
2024-12-17 08:30:51.822416: Pseudo dice [0.7752]
2024-12-17 08:30:51.823181: Epoch time: 90.1 s
2024-12-17 08:30:53.066201: 
2024-12-17 08:30:53.067684: Epoch 142
2024-12-17 08:30:53.068333: Current learning rate: 0.00071
2024-12-17 08:32:23.098592: Validation loss did not improve from -0.63299. Patience: 25/50
2024-12-17 08:32:23.099887: train_loss -0.7612
2024-12-17 08:32:23.100776: val_loss -0.6095
2024-12-17 08:32:23.101475: Pseudo dice [0.7801]
2024-12-17 08:32:23.102155: Epoch time: 90.04 s
2024-12-17 08:32:23.102923: Yayy! New best EMA pseudo Dice: 0.7761
2024-12-17 08:32:24.745460: 
2024-12-17 08:32:24.746807: Epoch 143
2024-12-17 08:32:24.747571: Current learning rate: 0.00063
2024-12-17 08:33:54.724728: Validation loss did not improve from -0.63299. Patience: 26/50
2024-12-17 08:33:54.725420: train_loss -0.7648
2024-12-17 08:33:54.726156: val_loss -0.6162
2024-12-17 08:33:54.727036: Pseudo dice [0.7837]
2024-12-17 08:33:54.727910: Epoch time: 89.98 s
2024-12-17 08:33:54.728718: Yayy! New best EMA pseudo Dice: 0.7769
2024-12-17 08:33:56.358661: 
2024-12-17 08:33:56.360259: Epoch 144
2024-12-17 08:33:56.361555: Current learning rate: 0.00055
2024-12-17 08:35:26.461640: Validation loss did not improve from -0.63299. Patience: 27/50
2024-12-17 08:35:26.462704: train_loss -0.7654
2024-12-17 08:35:26.463430: val_loss -0.6314
2024-12-17 08:35:26.464158: Pseudo dice [0.7935]
2024-12-17 08:35:26.464954: Epoch time: 90.11 s
2024-12-17 08:35:26.831417: Yayy! New best EMA pseudo Dice: 0.7785
2024-12-17 08:35:28.468472: 
2024-12-17 08:35:28.470098: Epoch 145
2024-12-17 08:35:28.470889: Current learning rate: 0.00047
2024-12-17 08:36:58.578783: Validation loss did not improve from -0.63299. Patience: 28/50
2024-12-17 08:36:58.579816: train_loss -0.7634
2024-12-17 08:36:58.580663: val_loss -0.6141
2024-12-17 08:36:58.581415: Pseudo dice [0.7807]
2024-12-17 08:36:58.582135: Epoch time: 90.11 s
2024-12-17 08:36:58.582913: Yayy! New best EMA pseudo Dice: 0.7787
2024-12-17 08:37:00.218171: 
2024-12-17 08:37:00.219652: Epoch 146
2024-12-17 08:37:00.220380: Current learning rate: 0.00038
2024-12-17 08:38:30.487518: Validation loss did not improve from -0.63299. Patience: 29/50
2024-12-17 08:38:30.488564: train_loss -0.7641
2024-12-17 08:38:30.489421: val_loss -0.5983
2024-12-17 08:38:30.490113: Pseudo dice [0.7756]
2024-12-17 08:38:30.490724: Epoch time: 90.27 s
2024-12-17 08:38:32.174916: 
2024-12-17 08:38:32.177332: Epoch 147
2024-12-17 08:38:32.178352: Current learning rate: 0.0003
2024-12-17 08:40:02.483565: Validation loss did not improve from -0.63299. Patience: 30/50
2024-12-17 08:40:02.484648: train_loss -0.7657
2024-12-17 08:40:02.485563: val_loss -0.5976
2024-12-17 08:40:02.486520: Pseudo dice [0.7733]
2024-12-17 08:40:02.487283: Epoch time: 90.31 s
2024-12-17 08:40:03.767183: 
2024-12-17 08:40:03.769099: Epoch 148
2024-12-17 08:40:03.769962: Current learning rate: 0.00021
2024-12-17 08:41:33.997579: Validation loss did not improve from -0.63299. Patience: 31/50
2024-12-17 08:41:33.998639: train_loss -0.768
2024-12-17 08:41:33.999551: val_loss -0.5953
2024-12-17 08:41:34.000198: Pseudo dice [0.7694]
2024-12-17 08:41:34.000882: Epoch time: 90.23 s
2024-12-17 08:41:35.303806: 
2024-12-17 08:41:35.305248: Epoch 149
2024-12-17 08:41:35.305961: Current learning rate: 0.00011
2024-12-17 08:43:05.661589: Validation loss did not improve from -0.63299. Patience: 32/50
2024-12-17 08:43:05.663138: train_loss -0.7615
2024-12-17 08:43:05.664788: val_loss -0.6173
2024-12-17 08:43:05.665728: Pseudo dice [0.7863]
2024-12-17 08:43:05.666837: Epoch time: 90.36 s
2024-12-17 08:43:07.308272: Training done.
2024-12-17 08:42:20.884772: Using splits from existing split file: /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset307_Sohee_Calcium_OCT_CrossValidation/splits_final.json
2024-12-17 08:42:20.887279: The split file contains 5 splits.
2024-12-17 08:42:20.888112: Desired fold for training: 3
2024-12-17 08:42:20.888853: This split has 7 training and 1 validation cases.
2024-12-17 08:42:20.889669: predicting 701-013
2024-12-17 08:42:20.898660: 701-013, shape torch.Size([1, 375, 498, 498]), rank 0
2024-12-17 08:45:53.577558: Validation complete
2024-12-17 08:45:53.578862: Mean Validation Dice:  0.6713115939079219
2024-12-17 08:43:07.415105: Using splits from existing split file: /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset307_Sohee_Calcium_OCT_CrossValidation/splits_final.json
2024-12-17 08:43:07.416638: The split file contains 5 splits.
2024-12-17 08:43:07.417335: Desired fold for training: 2
2024-12-17 08:43:07.417932: This split has 6 training and 2 validation cases.
2024-12-17 08:43:07.418688: predicting 101-044
2024-12-17 08:43:07.426509: 101-044, shape torch.Size([1, 404, 498, 498]), rank 0
2024-12-17 08:44:53.399586: predicting 704-003
2024-12-17 08:44:53.434490: 704-003, shape torch.Size([1, 375, 498, 498]), rank 0
2024-12-17 08:46:43.860693: Validation complete
2024-12-17 08:46:43.862322: Mean Validation Dice:  0.760312494834149

############################
INFO: You are using the old nnU-Net default plans. We have updated our recommendations. Please consider using those instead! Read more here: https://github.com/MIC-DKFZ/nnUNet/blob/master/documentation/resenc_presets.md
############################

Using device: cuda:0

#######################################################################
Please cite the following paper when using nnU-Net:
Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.
#######################################################################

2024-12-17 08:46:50.805432: Using torch.compile...
/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
################### Loading pretrained weights from file  /home/gridsan/nchutisilp/projects/OpenAI-CLIP/clip_preivl_poststent_pretrained_nnUNet.pt ###################
Below is the list of overlapping blocks in pretrained model and nnUNet architecture:
encoder.stages.0.0.convs.0.conv.weight shape torch.Size([32, 1, 3, 3, 3])
encoder.stages.0.0.convs.0.conv.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.norm.weight shape torch.Size([32])
encoder.stages.0.0.convs.0.norm.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.0.weight shape torch.Size([32, 1, 3, 3, 3])
encoder.stages.0.0.convs.0.all_modules.0.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.1.weight shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.1.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
encoder.stages.0.0.convs.1.conv.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.norm.weight shape torch.Size([32])
encoder.stages.0.0.convs.1.norm.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
encoder.stages.0.0.convs.1.all_modules.0.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.1.weight shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.1.bias shape torch.Size([32])
encoder.stages.1.0.convs.0.conv.weight shape torch.Size([64, 32, 3, 3, 3])
encoder.stages.1.0.convs.0.conv.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.norm.weight shape torch.Size([64])
encoder.stages.1.0.convs.0.norm.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.0.weight shape torch.Size([64, 32, 3, 3, 3])
encoder.stages.1.0.convs.0.all_modules.0.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.1.weight shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.1.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
encoder.stages.1.0.convs.1.conv.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.norm.weight shape torch.Size([64])
encoder.stages.1.0.convs.1.norm.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
encoder.stages.1.0.convs.1.all_modules.0.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.1.weight shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.1.bias shape torch.Size([64])
encoder.stages.2.0.convs.0.conv.weight shape torch.Size([128, 64, 3, 3, 3])
encoder.stages.2.0.convs.0.conv.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.norm.weight shape torch.Size([128])
encoder.stages.2.0.convs.0.norm.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.0.weight shape torch.Size([128, 64, 3, 3, 3])
encoder.stages.2.0.convs.0.all_modules.0.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.1.weight shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.1.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
encoder.stages.2.0.convs.1.conv.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.norm.weight shape torch.Size([128])
encoder.stages.2.0.convs.1.norm.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
encoder.stages.2.0.convs.1.all_modules.0.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.1.weight shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.1.bias shape torch.Size([128])
encoder.stages.3.0.convs.0.conv.weight shape torch.Size([256, 128, 3, 3, 3])
encoder.stages.3.0.convs.0.conv.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.norm.weight shape torch.Size([256])
encoder.stages.3.0.convs.0.norm.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.0.weight shape torch.Size([256, 128, 3, 3, 3])
encoder.stages.3.0.convs.0.all_modules.0.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.1.weight shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.1.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
encoder.stages.3.0.convs.1.conv.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.norm.weight shape torch.Size([256])
encoder.stages.3.0.convs.1.norm.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
encoder.stages.3.0.convs.1.all_modules.0.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.1.weight shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.1.bias shape torch.Size([256])
encoder.stages.4.0.convs.0.conv.weight shape torch.Size([320, 256, 3, 3, 3])
encoder.stages.4.0.convs.0.conv.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.norm.weight shape torch.Size([320])
encoder.stages.4.0.convs.0.norm.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.0.weight shape torch.Size([320, 256, 3, 3, 3])
encoder.stages.4.0.convs.0.all_modules.0.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.1.weight shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.1.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.4.0.convs.1.conv.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.norm.weight shape torch.Size([320])
encoder.stages.4.0.convs.1.norm.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.4.0.convs.1.all_modules.0.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.1.weight shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.1.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.0.conv.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.norm.weight shape torch.Size([320])
encoder.stages.5.0.convs.0.norm.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.0.all_modules.0.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.1.weight shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.1.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.1.conv.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.norm.weight shape torch.Size([320])
encoder.stages.5.0.convs.1.norm.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.1.all_modules.0.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.1.weight shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.0.0.convs.0.conv.weight shape torch.Size([32, 1, 3, 3, 3])
decoder.encoder.stages.0.0.convs.0.conv.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.norm.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.norm.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.0.weight shape torch.Size([32, 1, 3, 3, 3])
decoder.encoder.stages.0.0.convs.0.all_modules.0.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.1.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.1.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.encoder.stages.0.0.convs.1.conv.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.norm.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.norm.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.encoder.stages.0.0.convs.1.all_modules.0.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.1.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.1.bias shape torch.Size([32])
decoder.encoder.stages.1.0.convs.0.conv.weight shape torch.Size([64, 32, 3, 3, 3])
decoder.encoder.stages.1.0.convs.0.conv.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.norm.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.norm.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.0.weight shape torch.Size([64, 32, 3, 3, 3])
decoder.encoder.stages.1.0.convs.0.all_modules.0.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.1.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.1.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.encoder.stages.1.0.convs.1.conv.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.norm.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.norm.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.encoder.stages.1.0.convs.1.all_modules.0.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.1.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.1.bias shape torch.Size([64])
decoder.encoder.stages.2.0.convs.0.conv.weight shape torch.Size([128, 64, 3, 3, 3])
decoder.encoder.stages.2.0.convs.0.conv.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.norm.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.norm.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.0.weight shape torch.Size([128, 64, 3, 3, 3])
decoder.encoder.stages.2.0.convs.0.all_modules.0.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.1.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.1.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.encoder.stages.2.0.convs.1.conv.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.norm.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.norm.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.encoder.stages.2.0.convs.1.all_modules.0.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.1.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.1.bias shape torch.Size([128])
decoder.encoder.stages.3.0.convs.0.conv.weight shape torch.Size([256, 128, 3, 3, 3])
decoder.encoder.stages.3.0.convs.0.conv.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.norm.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.norm.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.0.weight shape torch.Size([256, 128, 3, 3, 3])
decoder.encoder.stages.3.0.convs.0.all_modules.0.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.1.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.1.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.encoder.stages.3.0.convs.1.conv.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.norm.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.norm.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.encoder.stages.3.0.convs.1.all_modules.0.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.1.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.1.bias shape torch.Size([256])
decoder.encoder.stages.4.0.convs.0.conv.weight shape torch.Size([320, 256, 3, 3, 3])
decoder.encoder.stages.4.0.convs.0.conv.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.norm.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.norm.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.0.weight shape torch.Size([320, 256, 3, 3, 3])
decoder.encoder.stages.4.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.4.0.convs.1.conv.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.norm.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.norm.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.4.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.0.conv.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.norm.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.norm.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.1.conv.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.norm.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.norm.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.stages.0.convs.0.conv.weight shape torch.Size([320, 640, 3, 3, 3])
decoder.stages.0.convs.0.conv.bias shape torch.Size([320])
decoder.stages.0.convs.0.norm.weight shape torch.Size([320])
decoder.stages.0.convs.0.norm.bias shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.0.weight shape torch.Size([320, 640, 3, 3, 3])
decoder.stages.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.stages.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.stages.0.convs.1.conv.bias shape torch.Size([320])
decoder.stages.0.convs.1.norm.weight shape torch.Size([320])
decoder.stages.0.convs.1.norm.bias shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.stages.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.stages.1.convs.0.conv.weight shape torch.Size([256, 512, 3, 3, 3])
decoder.stages.1.convs.0.conv.bias shape torch.Size([256])
decoder.stages.1.convs.0.norm.weight shape torch.Size([256])
decoder.stages.1.convs.0.norm.bias shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.0.weight shape torch.Size([256, 512, 3, 3, 3])
decoder.stages.1.convs.0.all_modules.0.bias shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.1.weight shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.1.bias shape torch.Size([256])
decoder.stages.1.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.stages.1.convs.1.conv.bias shape torch.Size([256])
decoder.stages.1.convs.1.norm.weight shape torch.Size([256])
decoder.stages.1.convs.1.norm.bias shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.stages.1.convs.1.all_modules.0.bias shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.1.weight shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.1.bias shape torch.Size([256])
decoder.stages.2.convs.0.conv.weight shape torch.Size([128, 256, 3, 3, 3])
decoder.stages.2.convs.0.conv.bias shape torch.Size([128])
decoder.stages.2.convs.0.norm.weight shape torch.Size([128])
decoder.stages.2.convs.0.norm.bias shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.0.weight shape torch.Size([128, 256, 3, 3, 3])
decoder.stages.2.convs.0.all_modules.0.bias shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.1.weight shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.1.bias shape torch.Size([128])
decoder.stages.2.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.stages.2.convs.1.conv.bias shape torch.Size([128])
decoder.stages.2.convs.1.norm.weight shape torch.Size([128])
decoder.stages.2.convs.1.norm.bias shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.stages.2.convs.1.all_modules.0.bias shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.1.weight shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.1.bias shape torch.Size([128])
decoder.stages.3.convs.0.conv.weight shape torch.Size([64, 128, 3, 3, 3])
decoder.stages.3.convs.0.conv.bias shape torch.Size([64])
decoder.stages.3.convs.0.norm.weight shape torch.Size([64])
decoder.stages.3.convs.0.norm.bias shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.0.weight shape torch.Size([64, 128, 3, 3, 3])
decoder.stages.3.convs.0.all_modules.0.bias shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.1.weight shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.1.bias shape torch.Size([64])
decoder.stages.3.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.stages.3.convs.1.conv.bias shape torch.Size([64])
decoder.stages.3.convs.1.norm.weight shape torch.Size([64])
decoder.stages.3.convs.1.norm.bias shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.stages.3.convs.1.all_modules.0.bias shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.1.weight shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.1.bias shape torch.Size([64])
decoder.stages.4.convs.0.conv.weight shape torch.Size([32, 64, 3, 3, 3])
decoder.stages.4.convs.0.conv.bias shape torch.Size([32])
decoder.stages.4.convs.0.norm.weight shape torch.Size([32])
decoder.stages.4.convs.0.norm.bias shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.0.weight shape torch.Size([32, 64, 3, 3, 3])
decoder.stages.4.convs.0.all_modules.0.bias shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.1.weight shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.1.bias shape torch.Size([32])
decoder.stages.4.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.stages.4.convs.1.conv.bias shape torch.Size([32])
decoder.stages.4.convs.1.norm.weight shape torch.Size([32])
decoder.stages.4.convs.1.norm.bias shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.stages.4.convs.1.all_modules.0.bias shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.1.weight shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.1.bias shape torch.Size([32])
decoder.transpconvs.0.weight shape torch.Size([320, 320, 1, 2, 2])
decoder.transpconvs.0.bias shape torch.Size([320])
decoder.transpconvs.1.weight shape torch.Size([320, 256, 2, 2, 2])
decoder.transpconvs.1.bias shape torch.Size([256])
decoder.transpconvs.2.weight shape torch.Size([256, 128, 2, 2, 2])
decoder.transpconvs.2.bias shape torch.Size([128])
decoder.transpconvs.3.weight shape torch.Size([128, 64, 2, 2, 2])
decoder.transpconvs.3.bias shape torch.Size([64])
decoder.transpconvs.4.weight shape torch.Size([64, 32, 2, 2, 2])
decoder.transpconvs.4.bias shape torch.Size([32])
################### Done ###################
2024-12-17 08:47:06.289850: do_dummy_2d_data_aug: True
2024-12-17 08:47:06.293011: Using splits from existing split file: /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset307_Sohee_Calcium_OCT_CrossValidation/splits_final.json
2024-12-17 08:47:06.295895: The split file contains 5 splits.
2024-12-17 08:47:06.297172: Desired fold for training: 4
2024-12-17 08:47:06.298388: This split has 7 training and 1 validation cases.
using pin_memory on device 0
using pin_memory on device 0

This is the configuration used by this training:
Configuration name: 3d_32x160x128_b10
 {'data_identifier': 'nnUNetPlans_3d_fullres', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 10, 'patch_size': [32, 160, 128], 'median_image_size_in_voxels': [375.0, 498.0, 498.0], 'spacing': [1.0, 1.0, 1.0], 'normalization_schemes': ['ZScoreNormalization'], 'use_mask_for_norm': [False], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'architecture': {'network_class_name': 'dynamic_network_architectures.architectures.unet.PlainConvUNet', 'arch_kwargs': {'n_stages': 6, 'features_per_stage': [32, 64, 128, 256, 320, 320], 'conv_op': 'torch.nn.modules.conv.Conv3d', 'kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]], 'strides': [[1, 1, 1], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [1, 2, 2]], 'n_conv_per_stage': [2, 2, 2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2, 2, 2], 'conv_bias': True, 'norm_op': 'torch.nn.modules.instancenorm.InstanceNorm3d', 'norm_op_kwargs': {'eps': 1e-05, 'affine': True}, 'dropout_op': None, 'dropout_op_kwargs': None, 'nonlin': 'torch.nn.LeakyReLU', 'nonlin_kwargs': {'inplace': True}, 'deep_supervision': True}, '_kw_requires_import': ['conv_op', 'norm_op', 'dropout_op', 'nonlin']}, 'batch_dice': True, 'inherits_from': '3d_fullres'} 

These are the global plan.json settings:
 {'dataset_name': 'Dataset307_Sohee_Calcium_OCT_CrossValidation', 'plans_name': 'nnUNetPlans', 'original_median_spacing_after_transp': [1.0, 1.0, 1.0], 'original_median_shape_after_transp': [375, 498, 498], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 0.4977976381778717, 'mean': 0.13507525622844696, 'median': 0.09599608182907104, 'min': 0.0, 'percentile_00_5': 0.014754901640117168, 'percentile_99_5': 0.4977976381778717, 'std': 0.12025152146816254}}} 

2024-12-17 08:47:09.261930: unpacking dataset...
2024-12-17 08:47:12.821182: unpacking done...
2024-12-17 08:47:13.100043: Unable to plot network architecture: nnUNet_compile is enabled!
2024-12-17 08:47:13.160726: 
2024-12-17 08:47:13.163078: Epoch 0
2024-12-17 08:47:13.165002: Current learning rate: 0.01
2024-12-17 08:49:56.497035: Validation loss improved from 1000.00000 to -0.25550! Patience: 0/50
2024-12-17 08:49:56.498127: train_loss -0.1078
2024-12-17 08:49:56.499330: val_loss -0.2555
2024-12-17 08:49:56.500374: Pseudo dice [0.5805]
2024-12-17 08:49:56.501214: Epoch time: 163.34 s
2024-12-17 08:49:56.502083: Yayy! New best EMA pseudo Dice: 0.5805
2024-12-17 08:49:58.349498: 
2024-12-17 08:49:58.351360: Epoch 1
2024-12-17 08:49:58.352366: Current learning rate: 0.00994
2024-12-17 08:51:26.324807: Validation loss did not improve from -0.25550. Patience: 1/50
2024-12-17 08:51:26.326157: train_loss -0.2515
2024-12-17 08:51:26.327396: val_loss -0.2509
2024-12-17 08:51:26.328131: Pseudo dice [0.575]
2024-12-17 08:51:26.328859: Epoch time: 87.98 s
2024-12-17 08:51:27.534613: 
2024-12-17 08:51:27.536347: Epoch 2
2024-12-17 08:51:27.537242: Current learning rate: 0.00988
2024-12-17 08:52:55.572457: Validation loss improved from -0.25550 to -0.32149! Patience: 1/50
2024-12-17 08:52:55.573976: train_loss -0.3059
2024-12-17 08:52:55.575295: val_loss -0.3215
2024-12-17 08:52:55.576255: Pseudo dice [0.6241]
2024-12-17 08:52:55.577069: Epoch time: 88.04 s
2024-12-17 08:52:55.577795: Yayy! New best EMA pseudo Dice: 0.5844
2024-12-17 08:52:57.125146: 
2024-12-17 08:52:57.127080: Epoch 3
2024-12-17 08:52:57.128371: Current learning rate: 0.00982
2024-12-17 08:54:25.195165: Validation loss improved from -0.32149 to -0.35830! Patience: 0/50
2024-12-17 08:54:25.196200: train_loss -0.3412
2024-12-17 08:54:25.197551: val_loss -0.3583
2024-12-17 08:54:25.198333: Pseudo dice [0.624]
2024-12-17 08:54:25.199213: Epoch time: 88.07 s
2024-12-17 08:54:25.200008: Yayy! New best EMA pseudo Dice: 0.5883
2024-12-17 08:54:26.715836: 
2024-12-17 08:54:26.718311: Epoch 4
2024-12-17 08:54:26.719628: Current learning rate: 0.00976
2024-12-17 08:55:54.640991: Validation loss improved from -0.35830 to -0.36405! Patience: 0/50
2024-12-17 08:55:54.642396: train_loss -0.3812
2024-12-17 08:55:54.643590: val_loss -0.364
2024-12-17 08:55:54.644480: Pseudo dice [0.6241]
2024-12-17 08:55:54.645199: Epoch time: 87.93 s
2024-12-17 08:55:54.964579: Yayy! New best EMA pseudo Dice: 0.5919
2024-12-17 08:55:56.537369: 
2024-12-17 08:55:56.539489: Epoch 5
2024-12-17 08:55:56.540282: Current learning rate: 0.0097
2024-12-17 08:57:24.290438: Validation loss did not improve from -0.36405. Patience: 1/50
2024-12-17 08:57:24.291593: train_loss -0.4224
2024-12-17 08:57:24.292622: val_loss -0.3463
2024-12-17 08:57:24.293491: Pseudo dice [0.6361]
2024-12-17 08:57:24.294292: Epoch time: 87.76 s
2024-12-17 08:57:24.294991: Yayy! New best EMA pseudo Dice: 0.5963
2024-12-17 08:57:25.754118: 
2024-12-17 08:57:25.756296: Epoch 6
2024-12-17 08:57:25.757660: Current learning rate: 0.00964
2024-12-17 08:58:53.565645: Validation loss improved from -0.36405 to -0.43239! Patience: 1/50
2024-12-17 08:58:53.567425: train_loss -0.4302
2024-12-17 08:58:53.568800: val_loss -0.4324
2024-12-17 08:58:53.569961: Pseudo dice [0.6761]
2024-12-17 08:58:53.571067: Epoch time: 87.81 s
2024-12-17 08:58:53.571900: Yayy! New best EMA pseudo Dice: 0.6043
2024-12-17 08:58:55.068884: 
2024-12-17 08:58:55.070826: Epoch 7
2024-12-17 08:58:55.072237: Current learning rate: 0.00958
2024-12-17 09:00:22.838662: Validation loss did not improve from -0.43239. Patience: 1/50
2024-12-17 09:00:22.839718: train_loss -0.4422
2024-12-17 09:00:22.840739: val_loss -0.3704
2024-12-17 09:00:22.841610: Pseudo dice [0.6525]
2024-12-17 09:00:22.842535: Epoch time: 87.77 s
2024-12-17 09:00:22.843341: Yayy! New best EMA pseudo Dice: 0.6091
2024-12-17 09:00:24.696934: 
2024-12-17 09:00:24.699178: Epoch 8
2024-12-17 09:00:24.700014: Current learning rate: 0.00952
2024-12-17 09:01:52.804737: Validation loss did not improve from -0.43239. Patience: 2/50
2024-12-17 09:01:52.806049: train_loss -0.4524
2024-12-17 09:01:52.807207: val_loss -0.4076
2024-12-17 09:01:52.807879: Pseudo dice [0.6776]
2024-12-17 09:01:52.808529: Epoch time: 88.11 s
2024-12-17 09:01:52.809173: Yayy! New best EMA pseudo Dice: 0.616
2024-12-17 09:01:54.381668: 
2024-12-17 09:01:54.383965: Epoch 9
2024-12-17 09:01:54.385031: Current learning rate: 0.00946
2024-12-17 09:03:22.620654: Validation loss did not improve from -0.43239. Patience: 3/50
2024-12-17 09:03:22.621700: train_loss -0.4662
2024-12-17 09:03:22.622932: val_loss -0.4058
2024-12-17 09:03:22.623829: Pseudo dice [0.6726]
2024-12-17 09:03:22.624690: Epoch time: 88.24 s
2024-12-17 09:03:22.957635: Yayy! New best EMA pseudo Dice: 0.6216
2024-12-17 09:03:24.431236: 
2024-12-17 09:03:24.433707: Epoch 10
2024-12-17 09:03:24.435244: Current learning rate: 0.0094
2024-12-17 09:04:52.571549: Validation loss improved from -0.43239 to -0.43641! Patience: 3/50
2024-12-17 09:04:52.572851: train_loss -0.4685
2024-12-17 09:04:52.574279: val_loss -0.4364
2024-12-17 09:04:52.575310: Pseudo dice [0.6881]
2024-12-17 09:04:52.576213: Epoch time: 88.14 s
2024-12-17 09:04:52.577477: Yayy! New best EMA pseudo Dice: 0.6283
2024-12-17 09:04:54.077888: 
2024-12-17 09:04:54.080122: Epoch 11
2024-12-17 09:04:54.081348: Current learning rate: 0.00934
2024-12-17 09:06:22.086087: Validation loss improved from -0.43641 to -0.44863! Patience: 0/50
2024-12-17 09:06:22.087452: train_loss -0.4872
2024-12-17 09:06:22.088756: val_loss -0.4486
2024-12-17 09:06:22.089764: Pseudo dice [0.6913]
2024-12-17 09:06:22.090666: Epoch time: 88.01 s
2024-12-17 09:06:22.091602: Yayy! New best EMA pseudo Dice: 0.6346
2024-12-17 09:06:23.592473: 
2024-12-17 09:06:23.594956: Epoch 12
2024-12-17 09:06:23.596365: Current learning rate: 0.00928
2024-12-17 09:07:51.756850: Validation loss did not improve from -0.44863. Patience: 1/50
2024-12-17 09:07:51.758067: train_loss -0.4968
2024-12-17 09:07:51.759223: val_loss -0.4259
2024-12-17 09:07:51.760146: Pseudo dice [0.6791]
2024-12-17 09:07:51.760871: Epoch time: 88.17 s
2024-12-17 09:07:51.761622: Yayy! New best EMA pseudo Dice: 0.639
2024-12-17 09:07:53.280077: 
2024-12-17 09:07:53.282140: Epoch 13
2024-12-17 09:07:53.283195: Current learning rate: 0.00922
2024-12-17 09:09:21.463501: Validation loss did not improve from -0.44863. Patience: 2/50
2024-12-17 09:09:21.464851: train_loss -0.5143
2024-12-17 09:09:21.465887: val_loss -0.4481
2024-12-17 09:09:21.466646: Pseudo dice [0.6891]
2024-12-17 09:09:21.467467: Epoch time: 88.19 s
2024-12-17 09:09:21.468266: Yayy! New best EMA pseudo Dice: 0.644
2024-12-17 09:09:22.957247: 
2024-12-17 09:09:22.959112: Epoch 14
2024-12-17 09:09:22.959866: Current learning rate: 0.00916
2024-12-17 09:10:51.019456: Validation loss did not improve from -0.44863. Patience: 3/50
2024-12-17 09:10:51.020709: train_loss -0.5155
2024-12-17 09:10:51.021690: val_loss -0.4226
2024-12-17 09:10:51.022661: Pseudo dice [0.6844]
2024-12-17 09:10:51.023432: Epoch time: 88.06 s
2024-12-17 09:10:51.352038: Yayy! New best EMA pseudo Dice: 0.6481
2024-12-17 09:10:52.877434: 
2024-12-17 09:10:52.879811: Epoch 15
2024-12-17 09:10:52.880928: Current learning rate: 0.0091
2024-12-17 09:12:21.145223: Validation loss improved from -0.44863 to -0.49239! Patience: 3/50
2024-12-17 09:12:21.146310: train_loss -0.4952
2024-12-17 09:12:21.147633: val_loss -0.4924
2024-12-17 09:12:21.148643: Pseudo dice [0.716]
2024-12-17 09:12:21.149671: Epoch time: 88.27 s
2024-12-17 09:12:21.150488: Yayy! New best EMA pseudo Dice: 0.6549
2024-12-17 09:12:22.694002: 
2024-12-17 09:12:22.696363: Epoch 16
2024-12-17 09:12:22.697471: Current learning rate: 0.00903
2024-12-17 09:13:51.116869: Validation loss did not improve from -0.49239. Patience: 1/50
2024-12-17 09:13:51.118390: train_loss -0.5166
2024-12-17 09:13:51.119947: val_loss -0.4209
2024-12-17 09:13:51.120918: Pseudo dice [0.68]
2024-12-17 09:13:51.121921: Epoch time: 88.43 s
2024-12-17 09:13:51.122740: Yayy! New best EMA pseudo Dice: 0.6574
2024-12-17 09:13:52.673588: 
2024-12-17 09:13:52.675668: Epoch 17
2024-12-17 09:13:52.676767: Current learning rate: 0.00897
2024-12-17 09:15:20.920813: Validation loss did not improve from -0.49239. Patience: 2/50
2024-12-17 09:15:20.922093: train_loss -0.5401
2024-12-17 09:15:20.922762: val_loss -0.46
2024-12-17 09:15:20.923650: Pseudo dice [0.6957]
2024-12-17 09:15:20.924607: Epoch time: 88.25 s
2024-12-17 09:15:20.925599: Yayy! New best EMA pseudo Dice: 0.6612
2024-12-17 09:15:22.450126: 
2024-12-17 09:15:22.451951: Epoch 18
2024-12-17 09:15:22.452688: Current learning rate: 0.00891
2024-12-17 09:16:50.828000: Validation loss did not improve from -0.49239. Patience: 3/50
2024-12-17 09:16:50.829657: train_loss -0.5413
2024-12-17 09:16:50.830562: val_loss -0.4443
2024-12-17 09:16:50.831652: Pseudo dice [0.6915]
2024-12-17 09:16:50.832405: Epoch time: 88.38 s
2024-12-17 09:16:50.833385: Yayy! New best EMA pseudo Dice: 0.6642
2024-12-17 09:16:52.654086: 
2024-12-17 09:16:52.656112: Epoch 19
2024-12-17 09:16:52.657264: Current learning rate: 0.00885
2024-12-17 09:18:20.921518: Validation loss did not improve from -0.49239. Patience: 4/50
2024-12-17 09:18:20.922504: train_loss -0.5428
2024-12-17 09:18:20.923301: val_loss -0.4761
2024-12-17 09:18:20.924035: Pseudo dice [0.7015]
2024-12-17 09:18:20.924716: Epoch time: 88.27 s
2024-12-17 09:18:21.262989: Yayy! New best EMA pseudo Dice: 0.668
2024-12-17 09:18:22.795808: 
2024-12-17 09:18:22.797716: Epoch 20
2024-12-17 09:18:22.798572: Current learning rate: 0.00879
2024-12-17 09:19:51.001697: Validation loss did not improve from -0.49239. Patience: 5/50
2024-12-17 09:19:51.003165: train_loss -0.5528
2024-12-17 09:19:51.004441: val_loss -0.442
2024-12-17 09:19:51.005069: Pseudo dice [0.6926]
2024-12-17 09:19:51.005935: Epoch time: 88.21 s
2024-12-17 09:19:51.006938: Yayy! New best EMA pseudo Dice: 0.6704
2024-12-17 09:19:52.552690: 
2024-12-17 09:19:52.555465: Epoch 21
2024-12-17 09:19:52.556433: Current learning rate: 0.00873
2024-12-17 09:21:20.793727: Validation loss did not improve from -0.49239. Patience: 6/50
2024-12-17 09:21:20.794872: train_loss -0.5611
2024-12-17 09:21:20.795916: val_loss -0.4148
2024-12-17 09:21:20.796820: Pseudo dice [0.6741]
2024-12-17 09:21:20.797898: Epoch time: 88.24 s
2024-12-17 09:21:20.798745: Yayy! New best EMA pseudo Dice: 0.6708
2024-12-17 09:21:22.256215: 
2024-12-17 09:21:22.257854: Epoch 22
2024-12-17 09:21:22.258739: Current learning rate: 0.00867
2024-12-17 09:22:50.472311: Validation loss did not improve from -0.49239. Patience: 7/50
2024-12-17 09:22:50.473536: train_loss -0.5683
2024-12-17 09:22:50.474772: val_loss -0.4903
2024-12-17 09:22:50.475694: Pseudo dice [0.7017]
2024-12-17 09:22:50.476472: Epoch time: 88.22 s
2024-12-17 09:22:50.477211: Yayy! New best EMA pseudo Dice: 0.6739
2024-12-17 09:22:51.925964: 
2024-12-17 09:22:51.928181: Epoch 23
2024-12-17 09:22:51.929514: Current learning rate: 0.00861
2024-12-17 09:24:20.145822: Validation loss did not improve from -0.49239. Patience: 8/50
2024-12-17 09:24:20.146890: train_loss -0.5768
2024-12-17 09:24:20.147591: val_loss -0.4063
2024-12-17 09:24:20.148329: Pseudo dice [0.6979]
2024-12-17 09:24:20.149181: Epoch time: 88.22 s
2024-12-17 09:24:20.150017: Yayy! New best EMA pseudo Dice: 0.6763
2024-12-17 09:24:21.630052: 
2024-12-17 09:24:21.632794: Epoch 24
2024-12-17 09:24:21.634057: Current learning rate: 0.00855
2024-12-17 09:25:50.140852: Validation loss did not improve from -0.49239. Patience: 9/50
2024-12-17 09:25:50.142209: train_loss -0.5635
2024-12-17 09:25:50.143327: val_loss -0.4745
2024-12-17 09:25:50.144095: Pseudo dice [0.7104]
2024-12-17 09:25:50.144989: Epoch time: 88.51 s
2024-12-17 09:25:50.502261: Yayy! New best EMA pseudo Dice: 0.6797
2024-12-17 09:25:51.961617: 
2024-12-17 09:25:51.964174: Epoch 25
2024-12-17 09:25:51.965729: Current learning rate: 0.00849
2024-12-17 09:27:20.409741: Validation loss did not improve from -0.49239. Patience: 10/50
2024-12-17 09:27:20.411027: train_loss -0.5774
2024-12-17 09:27:20.412322: val_loss -0.3988
2024-12-17 09:27:20.413335: Pseudo dice [0.6791]
2024-12-17 09:27:20.414345: Epoch time: 88.45 s
2024-12-17 09:27:21.592444: 
2024-12-17 09:27:21.595074: Epoch 26
2024-12-17 09:27:21.596460: Current learning rate: 0.00843
2024-12-17 09:28:50.035228: Validation loss did not improve from -0.49239. Patience: 11/50
2024-12-17 09:28:50.036477: train_loss -0.5668
2024-12-17 09:28:50.037822: val_loss -0.4377
2024-12-17 09:28:50.038601: Pseudo dice [0.6838]
2024-12-17 09:28:50.039376: Epoch time: 88.44 s
2024-12-17 09:28:50.040233: Yayy! New best EMA pseudo Dice: 0.6801
2024-12-17 09:28:51.532731: 
2024-12-17 09:28:51.534939: Epoch 27
2024-12-17 09:28:51.536249: Current learning rate: 0.00836
2024-12-17 09:30:19.876716: Validation loss did not improve from -0.49239. Patience: 12/50
2024-12-17 09:30:19.878167: train_loss -0.5824
2024-12-17 09:30:19.879259: val_loss -0.4732
2024-12-17 09:30:19.880384: Pseudo dice [0.7099]
2024-12-17 09:30:19.881527: Epoch time: 88.35 s
2024-12-17 09:30:19.882714: Yayy! New best EMA pseudo Dice: 0.683
2024-12-17 09:30:21.348158: 
2024-12-17 09:30:21.350250: Epoch 28
2024-12-17 09:30:21.351358: Current learning rate: 0.0083
2024-12-17 09:31:49.652436: Validation loss did not improve from -0.49239. Patience: 13/50
2024-12-17 09:31:49.653399: train_loss -0.584
2024-12-17 09:31:49.654235: val_loss -0.4312
2024-12-17 09:31:49.655189: Pseudo dice [0.6821]
2024-12-17 09:31:49.656226: Epoch time: 88.31 s
2024-12-17 09:31:51.159288: 
2024-12-17 09:31:51.161686: Epoch 29
2024-12-17 09:31:51.163185: Current learning rate: 0.00824
2024-12-17 09:33:19.407457: Validation loss did not improve from -0.49239. Patience: 14/50
2024-12-17 09:33:19.408805: train_loss -0.5897
2024-12-17 09:33:19.409671: val_loss -0.49
2024-12-17 09:33:19.410465: Pseudo dice [0.7244]
2024-12-17 09:33:19.411204: Epoch time: 88.25 s
2024-12-17 09:33:19.761225: Yayy! New best EMA pseudo Dice: 0.6871
2024-12-17 09:33:21.264621: 
2024-12-17 09:33:21.266600: Epoch 30
2024-12-17 09:33:21.268088: Current learning rate: 0.00818
2024-12-17 09:34:49.694306: Validation loss did not improve from -0.49239. Patience: 15/50
2024-12-17 09:34:49.695512: train_loss -0.5995
2024-12-17 09:34:49.696516: val_loss -0.4747
2024-12-17 09:34:49.697277: Pseudo dice [0.7061]
2024-12-17 09:34:49.698191: Epoch time: 88.43 s
2024-12-17 09:34:49.699033: Yayy! New best EMA pseudo Dice: 0.689
2024-12-17 09:34:51.192518: 
2024-12-17 09:34:51.194957: Epoch 31
2024-12-17 09:34:51.196378: Current learning rate: 0.00812
2024-12-17 09:36:19.437516: Validation loss did not improve from -0.49239. Patience: 16/50
2024-12-17 09:36:19.438723: train_loss -0.6014
2024-12-17 09:36:19.440068: val_loss -0.481
2024-12-17 09:36:19.440857: Pseudo dice [0.7159]
2024-12-17 09:36:19.441713: Epoch time: 88.25 s
2024-12-17 09:36:19.442730: Yayy! New best EMA pseudo Dice: 0.6917
2024-12-17 09:36:20.956961: 
2024-12-17 09:36:20.959211: Epoch 32
2024-12-17 09:36:20.960564: Current learning rate: 0.00806
2024-12-17 09:37:49.193968: Validation loss did not improve from -0.49239. Patience: 17/50
2024-12-17 09:37:49.195078: train_loss -0.5952
2024-12-17 09:37:49.196295: val_loss -0.4473
2024-12-17 09:37:49.197212: Pseudo dice [0.6941]
2024-12-17 09:37:49.197962: Epoch time: 88.24 s
2024-12-17 09:37:49.198785: Yayy! New best EMA pseudo Dice: 0.6919
2024-12-17 09:37:50.707390: 
2024-12-17 09:37:50.709124: Epoch 33
2024-12-17 09:37:50.710295: Current learning rate: 0.008
2024-12-17 09:39:18.980989: Validation loss did not improve from -0.49239. Patience: 18/50
2024-12-17 09:39:18.981952: train_loss -0.6081
2024-12-17 09:39:18.982772: val_loss -0.4838
2024-12-17 09:39:18.983738: Pseudo dice [0.7177]
2024-12-17 09:39:18.984744: Epoch time: 88.28 s
2024-12-17 09:39:18.985542: Yayy! New best EMA pseudo Dice: 0.6945
2024-12-17 09:39:20.528566: 
2024-12-17 09:39:20.530285: Epoch 34
2024-12-17 09:39:20.531254: Current learning rate: 0.00793
2024-12-17 09:40:48.932179: Validation loss improved from -0.49239 to -0.49388! Patience: 18/50
2024-12-17 09:40:48.933348: train_loss -0.6089
2024-12-17 09:40:48.935137: val_loss -0.4939
2024-12-17 09:40:48.936139: Pseudo dice [0.719]
2024-12-17 09:40:48.937249: Epoch time: 88.41 s
2024-12-17 09:40:49.279430: Yayy! New best EMA pseudo Dice: 0.697
2024-12-17 09:40:50.818203: 
2024-12-17 09:40:50.820573: Epoch 35
2024-12-17 09:40:50.821742: Current learning rate: 0.00787
2024-12-17 09:42:19.097919: Validation loss improved from -0.49388 to -0.50145! Patience: 0/50
2024-12-17 09:42:19.099016: train_loss -0.6099
2024-12-17 09:42:19.100356: val_loss -0.5014
2024-12-17 09:42:19.101489: Pseudo dice [0.729]
2024-12-17 09:42:19.102735: Epoch time: 88.28 s
2024-12-17 09:42:19.103992: Yayy! New best EMA pseudo Dice: 0.7002
2024-12-17 09:42:20.640649: 
2024-12-17 09:42:20.643116: Epoch 36
2024-12-17 09:42:20.644643: Current learning rate: 0.00781
2024-12-17 09:43:48.910888: Validation loss did not improve from -0.50145. Patience: 1/50
2024-12-17 09:43:48.912158: train_loss -0.6238
2024-12-17 09:43:48.914547: val_loss -0.4384
2024-12-17 09:43:48.915410: Pseudo dice [0.6918]
2024-12-17 09:43:48.916764: Epoch time: 88.27 s
2024-12-17 09:43:50.148816: 
2024-12-17 09:43:50.150989: Epoch 37
2024-12-17 09:43:50.152384: Current learning rate: 0.00775
2024-12-17 09:45:18.362557: Validation loss did not improve from -0.50145. Patience: 2/50
2024-12-17 09:45:18.363956: train_loss -0.6196
2024-12-17 09:45:18.364830: val_loss -0.4874
2024-12-17 09:45:18.365795: Pseudo dice [0.7125]
2024-12-17 09:45:18.366906: Epoch time: 88.22 s
2024-12-17 09:45:18.367678: Yayy! New best EMA pseudo Dice: 0.7006
2024-12-17 09:45:19.895727: 
2024-12-17 09:45:19.897641: Epoch 38
2024-12-17 09:45:19.898661: Current learning rate: 0.00769
2024-12-17 09:46:48.067636: Validation loss did not improve from -0.50145. Patience: 3/50
2024-12-17 09:46:48.069118: train_loss -0.6315
2024-12-17 09:46:48.070350: val_loss -0.4924
2024-12-17 09:46:48.071470: Pseudo dice [0.7133]
2024-12-17 09:46:48.072433: Epoch time: 88.17 s
2024-12-17 09:46:48.073542: Yayy! New best EMA pseudo Dice: 0.7019
2024-12-17 09:46:50.133139: 
2024-12-17 09:46:50.135191: Epoch 39
2024-12-17 09:46:50.136441: Current learning rate: 0.00763
2024-12-17 09:48:18.420983: Validation loss did not improve from -0.50145. Patience: 4/50
2024-12-17 09:48:18.422460: train_loss -0.6185
2024-12-17 09:48:18.423632: val_loss -0.4529
2024-12-17 09:48:18.424591: Pseudo dice [0.7038]
2024-12-17 09:48:18.425447: Epoch time: 88.29 s
2024-12-17 09:48:18.764985: Yayy! New best EMA pseudo Dice: 0.7021
2024-12-17 09:48:20.328370: 
2024-12-17 09:48:20.330564: Epoch 40
2024-12-17 09:48:20.331784: Current learning rate: 0.00756
2024-12-17 09:49:48.830300: Validation loss did not improve from -0.50145. Patience: 5/50
2024-12-17 09:49:48.831661: train_loss -0.6288
2024-12-17 09:49:48.832832: val_loss -0.4671
2024-12-17 09:49:48.833810: Pseudo dice [0.7117]
2024-12-17 09:49:48.834504: Epoch time: 88.5 s
2024-12-17 09:49:48.835329: Yayy! New best EMA pseudo Dice: 0.7031
2024-12-17 09:49:50.401605: 
2024-12-17 09:49:50.403841: Epoch 41
2024-12-17 09:49:50.405075: Current learning rate: 0.0075
2024-12-17 09:51:18.724172: Validation loss improved from -0.50145 to -0.51229! Patience: 5/50
2024-12-17 09:51:18.725276: train_loss -0.6358
2024-12-17 09:51:18.726040: val_loss -0.5123
2024-12-17 09:51:18.726853: Pseudo dice [0.7393]
2024-12-17 09:51:18.727869: Epoch time: 88.32 s
2024-12-17 09:51:18.728789: Yayy! New best EMA pseudo Dice: 0.7067
2024-12-17 09:51:20.221600: 
2024-12-17 09:51:20.224062: Epoch 42
2024-12-17 09:51:20.225549: Current learning rate: 0.00744
2024-12-17 09:52:48.775248: Validation loss improved from -0.51229 to -0.53167! Patience: 0/50
2024-12-17 09:52:48.779549: train_loss -0.6417
2024-12-17 09:52:48.780728: val_loss -0.5317
2024-12-17 09:52:48.781517: Pseudo dice [0.7381]
2024-12-17 09:52:48.782269: Epoch time: 88.56 s
2024-12-17 09:52:48.782976: Yayy! New best EMA pseudo Dice: 0.7098
2024-12-17 09:52:50.297785: 
2024-12-17 09:52:50.299834: Epoch 43
2024-12-17 09:52:50.300783: Current learning rate: 0.00738
2024-12-17 09:54:19.173698: Validation loss did not improve from -0.53167. Patience: 1/50
2024-12-17 09:54:19.176193: train_loss -0.6342
2024-12-17 09:54:19.179235: val_loss -0.512
2024-12-17 09:54:19.180477: Pseudo dice [0.7183]
2024-12-17 09:54:19.181774: Epoch time: 88.88 s
2024-12-17 09:54:19.182709: Yayy! New best EMA pseudo Dice: 0.7107
2024-12-17 09:54:20.980307: 
2024-12-17 09:54:20.982802: Epoch 44
2024-12-17 09:54:20.984030: Current learning rate: 0.00732
2024-12-17 09:55:49.447293: Validation loss did not improve from -0.53167. Patience: 2/50
2024-12-17 09:55:49.448308: train_loss -0.6356
2024-12-17 09:55:49.449053: val_loss -0.4744
2024-12-17 09:55:49.449867: Pseudo dice [0.7109]
2024-12-17 09:55:49.450645: Epoch time: 88.47 s
2024-12-17 09:55:49.793762: Yayy! New best EMA pseudo Dice: 0.7107
2024-12-17 09:55:51.233232: 
2024-12-17 09:55:51.235523: Epoch 45
2024-12-17 09:55:51.236724: Current learning rate: 0.00725
2024-12-17 09:57:19.728379: Validation loss did not improve from -0.53167. Patience: 3/50
2024-12-17 09:57:19.729505: train_loss -0.6465
2024-12-17 09:57:19.730516: val_loss -0.4667
2024-12-17 09:57:19.731390: Pseudo dice [0.7048]
2024-12-17 09:57:19.732254: Epoch time: 88.5 s
2024-12-17 09:57:20.888359: 
2024-12-17 09:57:20.890134: Epoch 46
2024-12-17 09:57:20.891424: Current learning rate: 0.00719
2024-12-17 09:58:49.420462: Validation loss did not improve from -0.53167. Patience: 4/50
2024-12-17 09:58:49.421849: train_loss -0.6397
2024-12-17 09:58:49.422877: val_loss -0.4999
2024-12-17 09:58:49.423879: Pseudo dice [0.725]
2024-12-17 09:58:49.424938: Epoch time: 88.53 s
2024-12-17 09:58:49.425718: Yayy! New best EMA pseudo Dice: 0.7116
2024-12-17 09:58:50.923635: 
2024-12-17 09:58:50.925484: Epoch 47
2024-12-17 09:58:50.926339: Current learning rate: 0.00713
2024-12-17 10:00:19.412556: Validation loss did not improve from -0.53167. Patience: 5/50
2024-12-17 10:00:19.413702: train_loss -0.6519
2024-12-17 10:00:19.414956: val_loss -0.484
2024-12-17 10:00:19.415606: Pseudo dice [0.7089]
2024-12-17 10:00:19.416242: Epoch time: 88.49 s
2024-12-17 10:00:20.559779: 
2024-12-17 10:00:20.561990: Epoch 48
2024-12-17 10:00:20.562942: Current learning rate: 0.00707
2024-12-17 10:01:49.123012: Validation loss did not improve from -0.53167. Patience: 6/50
2024-12-17 10:01:49.124196: train_loss -0.6418
2024-12-17 10:01:49.125712: val_loss -0.4822
2024-12-17 10:01:49.126759: Pseudo dice [0.7157]
2024-12-17 10:01:49.127917: Epoch time: 88.57 s
2024-12-17 10:01:49.129168: Yayy! New best EMA pseudo Dice: 0.7118
2024-12-17 10:01:50.618093: 
2024-12-17 10:01:50.619923: Epoch 49
2024-12-17 10:01:50.621165: Current learning rate: 0.007
2024-12-17 10:03:19.248331: Validation loss did not improve from -0.53167. Patience: 7/50
2024-12-17 10:03:19.249387: train_loss -0.6489
2024-12-17 10:03:19.250312: val_loss -0.4751
2024-12-17 10:03:19.251201: Pseudo dice [0.7055]
2024-12-17 10:03:19.252017: Epoch time: 88.63 s
2024-12-17 10:03:21.597520: 
2024-12-17 10:03:21.599322: Epoch 50
2024-12-17 10:03:21.600680: Current learning rate: 0.00694
2024-12-17 10:04:50.113806: Validation loss did not improve from -0.53167. Patience: 8/50
2024-12-17 10:04:50.115241: train_loss -0.6381
2024-12-17 10:04:50.116460: val_loss -0.4762
2024-12-17 10:04:50.117352: Pseudo dice [0.7079]
2024-12-17 10:04:50.118150: Epoch time: 88.52 s
2024-12-17 10:04:51.267262: 
2024-12-17 10:04:51.269115: Epoch 51
2024-12-17 10:04:51.270569: Current learning rate: 0.00688
2024-12-17 10:06:19.949498: Validation loss did not improve from -0.53167. Patience: 9/50
2024-12-17 10:06:19.950783: train_loss -0.6521
2024-12-17 10:06:19.952165: val_loss -0.4934
2024-12-17 10:06:19.953243: Pseudo dice [0.7191]
2024-12-17 10:06:19.954282: Epoch time: 88.68 s
2024-12-17 10:06:21.143154: 
2024-12-17 10:06:21.145583: Epoch 52
2024-12-17 10:06:21.146570: Current learning rate: 0.00682
2024-12-17 10:07:49.715529: Validation loss did not improve from -0.53167. Patience: 10/50
2024-12-17 10:07:49.716833: train_loss -0.6562
2024-12-17 10:07:49.717760: val_loss -0.5144
2024-12-17 10:07:49.718767: Pseudo dice [0.7357]
2024-12-17 10:07:49.719829: Epoch time: 88.57 s
2024-12-17 10:07:49.720797: Yayy! New best EMA pseudo Dice: 0.714
2024-12-17 10:07:51.200365: 
2024-12-17 10:07:51.202312: Epoch 53
2024-12-17 10:07:51.203304: Current learning rate: 0.00675
2024-12-17 10:09:19.657126: Validation loss did not improve from -0.53167. Patience: 11/50
2024-12-17 10:09:19.658439: train_loss -0.6593
2024-12-17 10:09:19.659861: val_loss -0.5062
2024-12-17 10:09:19.660764: Pseudo dice [0.7217]
2024-12-17 10:09:19.661535: Epoch time: 88.46 s
2024-12-17 10:09:19.662370: Yayy! New best EMA pseudo Dice: 0.7148
2024-12-17 10:09:21.168033: 
2024-12-17 10:09:21.170249: Epoch 54
2024-12-17 10:09:21.171762: Current learning rate: 0.00669
2024-12-17 10:10:49.694784: Validation loss did not improve from -0.53167. Patience: 12/50
2024-12-17 10:10:49.696207: train_loss -0.6598
2024-12-17 10:10:49.697308: val_loss -0.4725
2024-12-17 10:10:49.698364: Pseudo dice [0.7013]
2024-12-17 10:10:49.699494: Epoch time: 88.53 s
2024-12-17 10:10:51.226307: 
2024-12-17 10:10:51.228426: Epoch 55
2024-12-17 10:10:51.229424: Current learning rate: 0.00663
2024-12-17 10:12:19.726749: Validation loss did not improve from -0.53167. Patience: 13/50
2024-12-17 10:12:19.727731: train_loss -0.6669
2024-12-17 10:12:19.728689: val_loss -0.4969
2024-12-17 10:12:19.729518: Pseudo dice [0.7272]
2024-12-17 10:12:19.730469: Epoch time: 88.5 s
2024-12-17 10:12:19.731177: Yayy! New best EMA pseudo Dice: 0.7148
2024-12-17 10:12:21.234201: 
2024-12-17 10:12:21.236035: Epoch 56
2024-12-17 10:12:21.237005: Current learning rate: 0.00657
2024-12-17 10:13:49.722256: Validation loss did not improve from -0.53167. Patience: 14/50
2024-12-17 10:13:49.723453: train_loss -0.6693
2024-12-17 10:13:49.724634: val_loss -0.5034
2024-12-17 10:13:49.725731: Pseudo dice [0.7301]
2024-12-17 10:13:49.726460: Epoch time: 88.49 s
2024-12-17 10:13:49.727184: Yayy! New best EMA pseudo Dice: 0.7164
2024-12-17 10:13:51.243728: 
2024-12-17 10:13:51.245836: Epoch 57
2024-12-17 10:13:51.246965: Current learning rate: 0.0065
2024-12-17 10:15:19.753564: Validation loss did not improve from -0.53167. Patience: 15/50
2024-12-17 10:15:19.755111: train_loss -0.6696
2024-12-17 10:15:19.756535: val_loss -0.5002
2024-12-17 10:15:19.757347: Pseudo dice [0.7209]
2024-12-17 10:15:19.758152: Epoch time: 88.51 s
2024-12-17 10:15:19.759026: Yayy! New best EMA pseudo Dice: 0.7168
2024-12-17 10:15:21.250313: 
2024-12-17 10:15:21.252950: Epoch 58
2024-12-17 10:15:21.254215: Current learning rate: 0.00644
2024-12-17 10:16:49.524862: Validation loss did not improve from -0.53167. Patience: 16/50
2024-12-17 10:16:49.526029: train_loss -0.6748
2024-12-17 10:16:49.526941: val_loss -0.4932
2024-12-17 10:16:49.527870: Pseudo dice [0.7164]
2024-12-17 10:16:49.528748: Epoch time: 88.28 s
2024-12-17 10:16:50.721032: 
2024-12-17 10:16:50.723040: Epoch 59
2024-12-17 10:16:50.724371: Current learning rate: 0.00638
2024-12-17 10:18:19.145557: Validation loss did not improve from -0.53167. Patience: 17/50
2024-12-17 10:18:19.146495: train_loss -0.6754
2024-12-17 10:18:19.147737: val_loss -0.4756
2024-12-17 10:18:19.148705: Pseudo dice [0.7042]
2024-12-17 10:18:19.149600: Epoch time: 88.43 s
2024-12-17 10:18:20.700038: 
2024-12-17 10:18:20.702092: Epoch 60
2024-12-17 10:18:20.703249: Current learning rate: 0.00631
2024-12-17 10:19:49.010744: Validation loss did not improve from -0.53167. Patience: 18/50
2024-12-17 10:19:49.012132: train_loss -0.6817
2024-12-17 10:19:49.013328: val_loss -0.5015
2024-12-17 10:19:49.014309: Pseudo dice [0.7194]
2024-12-17 10:19:49.015267: Epoch time: 88.31 s
2024-12-17 10:19:50.536299: 
2024-12-17 10:19:50.538447: Epoch 61
2024-12-17 10:19:50.539656: Current learning rate: 0.00625
2024-12-17 10:21:18.832757: Validation loss did not improve from -0.53167. Patience: 19/50
2024-12-17 10:21:18.834076: train_loss -0.6777
2024-12-17 10:21:18.835435: val_loss -0.519
2024-12-17 10:21:18.836326: Pseudo dice [0.7299]
2024-12-17 10:21:18.837108: Epoch time: 88.3 s
2024-12-17 10:21:18.837887: Yayy! New best EMA pseudo Dice: 0.7173
2024-12-17 10:21:20.348135: 
2024-12-17 10:21:20.350317: Epoch 62
2024-12-17 10:21:20.351562: Current learning rate: 0.00619
2024-12-17 10:22:48.617527: Validation loss did not improve from -0.53167. Patience: 20/50
2024-12-17 10:22:48.618645: train_loss -0.6762
2024-12-17 10:22:48.619776: val_loss -0.4475
2024-12-17 10:22:48.620747: Pseudo dice [0.7038]
2024-12-17 10:22:48.621636: Epoch time: 88.27 s
2024-12-17 10:22:49.818323: 
2024-12-17 10:22:49.820718: Epoch 63
2024-12-17 10:22:49.821555: Current learning rate: 0.00612
2024-12-17 10:24:18.104630: Validation loss improved from -0.53167 to -0.54789! Patience: 20/50
2024-12-17 10:24:18.105724: train_loss -0.6878
2024-12-17 10:24:18.106987: val_loss -0.5479
2024-12-17 10:24:18.108056: Pseudo dice [0.7448]
2024-12-17 10:24:18.108913: Epoch time: 88.29 s
2024-12-17 10:24:18.109683: Yayy! New best EMA pseudo Dice: 0.7188
2024-12-17 10:24:19.688985: 
2024-12-17 10:24:19.691109: Epoch 64
2024-12-17 10:24:19.692577: Current learning rate: 0.00606
2024-12-17 10:25:47.976270: Validation loss did not improve from -0.54789. Patience: 1/50
2024-12-17 10:25:47.977553: train_loss -0.6841
2024-12-17 10:25:47.979110: val_loss -0.508
2024-12-17 10:25:47.979952: Pseudo dice [0.729]
2024-12-17 10:25:47.980619: Epoch time: 88.29 s
2024-12-17 10:25:48.329779: Yayy! New best EMA pseudo Dice: 0.7198
2024-12-17 10:25:49.871859: 
2024-12-17 10:25:49.874030: Epoch 65
2024-12-17 10:25:49.875164: Current learning rate: 0.006
2024-12-17 10:27:18.096995: Validation loss did not improve from -0.54789. Patience: 2/50
2024-12-17 10:27:18.098285: train_loss -0.6855
2024-12-17 10:27:18.099672: val_loss -0.5125
2024-12-17 10:27:18.100811: Pseudo dice [0.7226]
2024-12-17 10:27:18.101654: Epoch time: 88.23 s
2024-12-17 10:27:18.102585: Yayy! New best EMA pseudo Dice: 0.7201
2024-12-17 10:27:19.633379: 
2024-12-17 10:27:19.635348: Epoch 66
2024-12-17 10:27:19.636302: Current learning rate: 0.00593
2024-12-17 10:28:48.055369: Validation loss did not improve from -0.54789. Patience: 3/50
2024-12-17 10:28:48.056536: train_loss -0.6881
2024-12-17 10:28:48.057914: val_loss -0.5453
2024-12-17 10:28:48.058820: Pseudo dice [0.746]
2024-12-17 10:28:48.059688: Epoch time: 88.42 s
2024-12-17 10:28:48.060441: Yayy! New best EMA pseudo Dice: 0.7227
2024-12-17 10:28:49.595339: 
2024-12-17 10:28:49.597200: Epoch 67
2024-12-17 10:28:49.598040: Current learning rate: 0.00587
2024-12-17 10:30:18.090303: Validation loss did not improve from -0.54789. Patience: 4/50
2024-12-17 10:30:18.091345: train_loss -0.6791
2024-12-17 10:30:18.092367: val_loss -0.501
2024-12-17 10:30:18.093121: Pseudo dice [0.7175]
2024-12-17 10:30:18.093888: Epoch time: 88.5 s
2024-12-17 10:30:19.296515: 
2024-12-17 10:30:19.298396: Epoch 68
2024-12-17 10:30:19.299229: Current learning rate: 0.00581
2024-12-17 10:31:47.699976: Validation loss did not improve from -0.54789. Patience: 5/50
2024-12-17 10:31:47.701384: train_loss -0.6815
2024-12-17 10:31:47.702936: val_loss -0.5006
2024-12-17 10:31:47.703856: Pseudo dice [0.7189]
2024-12-17 10:31:47.704779: Epoch time: 88.41 s
2024-12-17 10:31:48.919636: 
2024-12-17 10:31:48.921661: Epoch 69
2024-12-17 10:31:48.922662: Current learning rate: 0.00574
2024-12-17 10:33:17.436355: Validation loss did not improve from -0.54789. Patience: 6/50
2024-12-17 10:33:17.437475: train_loss -0.6901
2024-12-17 10:33:17.438367: val_loss -0.5318
2024-12-17 10:33:17.439039: Pseudo dice [0.7433]
2024-12-17 10:33:17.440097: Epoch time: 88.52 s
2024-12-17 10:33:17.777138: Yayy! New best EMA pseudo Dice: 0.724
2024-12-17 10:33:19.326569: 
2024-12-17 10:33:19.328851: Epoch 70
2024-12-17 10:33:19.330094: Current learning rate: 0.00568
2024-12-17 10:34:47.833933: Validation loss did not improve from -0.54789. Patience: 7/50
2024-12-17 10:34:47.834980: train_loss -0.6946
2024-12-17 10:34:47.836107: val_loss -0.4608
2024-12-17 10:34:47.837228: Pseudo dice [0.7157]
2024-12-17 10:34:47.837949: Epoch time: 88.51 s
2024-12-17 10:34:49.370504: 
2024-12-17 10:34:49.372678: Epoch 71
2024-12-17 10:34:49.373716: Current learning rate: 0.00562
2024-12-17 10:36:17.787420: Validation loss did not improve from -0.54789. Patience: 8/50
2024-12-17 10:36:17.788380: train_loss -0.6915
2024-12-17 10:36:17.789275: val_loss -0.4898
2024-12-17 10:36:17.790079: Pseudo dice [0.723]
2024-12-17 10:36:17.790859: Epoch time: 88.42 s
2024-12-17 10:36:19.000320: 
2024-12-17 10:36:19.001768: Epoch 72
2024-12-17 10:36:19.002494: Current learning rate: 0.00555
2024-12-17 10:37:47.482857: Validation loss did not improve from -0.54789. Patience: 9/50
2024-12-17 10:37:47.484307: train_loss -0.6886
2024-12-17 10:37:47.485496: val_loss -0.5348
2024-12-17 10:37:47.486530: Pseudo dice [0.7371]
2024-12-17 10:37:47.487267: Epoch time: 88.48 s
2024-12-17 10:37:47.488077: Yayy! New best EMA pseudo Dice: 0.7246
2024-12-17 10:37:49.061189: 
2024-12-17 10:37:49.063543: Epoch 73
2024-12-17 10:37:49.064779: Current learning rate: 0.00549
2024-12-17 10:39:17.606922: Validation loss did not improve from -0.54789. Patience: 10/50
2024-12-17 10:39:17.608236: train_loss -0.6887
2024-12-17 10:39:17.609185: val_loss -0.5371
2024-12-17 10:39:17.610236: Pseudo dice [0.7438]
2024-12-17 10:39:17.611077: Epoch time: 88.55 s
2024-12-17 10:39:17.611792: Yayy! New best EMA pseudo Dice: 0.7265
2024-12-17 10:39:19.171180: 
2024-12-17 10:39:19.174458: Epoch 74
2024-12-17 10:39:19.176411: Current learning rate: 0.00542
2024-12-17 10:40:47.645303: Validation loss did not improve from -0.54789. Patience: 11/50
2024-12-17 10:40:47.646389: train_loss -0.6971
2024-12-17 10:40:47.647628: val_loss -0.5306
2024-12-17 10:40:47.648803: Pseudo dice [0.7271]
2024-12-17 10:40:47.649810: Epoch time: 88.48 s
2024-12-17 10:40:47.994061: Yayy! New best EMA pseudo Dice: 0.7265
2024-12-17 10:40:49.596152: 
2024-12-17 10:40:49.597735: Epoch 75
2024-12-17 10:40:49.598782: Current learning rate: 0.00536
2024-12-17 10:42:18.077774: Validation loss did not improve from -0.54789. Patience: 12/50
2024-12-17 10:42:18.079005: train_loss -0.6931
2024-12-17 10:42:18.079935: val_loss -0.5322
2024-12-17 10:42:18.080595: Pseudo dice [0.7358]
2024-12-17 10:42:18.081279: Epoch time: 88.48 s
2024-12-17 10:42:18.082071: Yayy! New best EMA pseudo Dice: 0.7275
2024-12-17 10:42:19.637724: 
2024-12-17 10:42:19.639821: Epoch 76
2024-12-17 10:42:19.640663: Current learning rate: 0.00529
2024-12-17 10:43:48.038574: Validation loss did not improve from -0.54789. Patience: 13/50
2024-12-17 10:43:48.039917: train_loss -0.698
2024-12-17 10:43:48.041140: val_loss -0.4776
2024-12-17 10:43:48.041896: Pseudo dice [0.7039]
2024-12-17 10:43:48.043116: Epoch time: 88.4 s
2024-12-17 10:43:49.259688: 
2024-12-17 10:43:49.261088: Epoch 77
2024-12-17 10:43:49.262208: Current learning rate: 0.00523
2024-12-17 10:45:17.679154: Validation loss did not improve from -0.54789. Patience: 14/50
2024-12-17 10:45:17.680484: train_loss -0.698
2024-12-17 10:45:17.681648: val_loss -0.5249
2024-12-17 10:45:17.682497: Pseudo dice [0.7384]
2024-12-17 10:45:17.683372: Epoch time: 88.42 s
2024-12-17 10:45:18.930346: 
2024-12-17 10:45:18.932471: Epoch 78
2024-12-17 10:45:18.933605: Current learning rate: 0.00517
2024-12-17 10:46:47.353802: Validation loss did not improve from -0.54789. Patience: 15/50
2024-12-17 10:46:47.355473: train_loss -0.7068
2024-12-17 10:46:47.356734: val_loss -0.4706
2024-12-17 10:46:47.357756: Pseudo dice [0.7172]
2024-12-17 10:46:47.358813: Epoch time: 88.43 s
2024-12-17 10:46:48.589123: 
2024-12-17 10:46:48.590755: Epoch 79
2024-12-17 10:46:48.591693: Current learning rate: 0.0051
2024-12-17 10:48:17.111421: Validation loss did not improve from -0.54789. Patience: 16/50
2024-12-17 10:48:17.112958: train_loss -0.704
2024-12-17 10:48:17.114306: val_loss -0.497
2024-12-17 10:48:17.115272: Pseudo dice [0.7235]
2024-12-17 10:48:17.116248: Epoch time: 88.52 s
2024-12-17 10:48:18.689811: 
2024-12-17 10:48:18.692072: Epoch 80
2024-12-17 10:48:18.693140: Current learning rate: 0.00504
2024-12-17 10:49:47.164346: Validation loss did not improve from -0.54789. Patience: 17/50
2024-12-17 10:49:47.165411: train_loss -0.7092
2024-12-17 10:49:47.166764: val_loss -0.5161
2024-12-17 10:49:47.167971: Pseudo dice [0.7328]
2024-12-17 10:49:47.169322: Epoch time: 88.48 s
2024-12-17 10:49:48.795717: 
2024-12-17 10:49:48.797995: Epoch 81
2024-12-17 10:49:48.799082: Current learning rate: 0.00497
2024-12-17 10:51:17.349652: Validation loss did not improve from -0.54789. Patience: 18/50
2024-12-17 10:51:17.350726: train_loss -0.6998
2024-12-17 10:51:17.351724: val_loss -0.4919
2024-12-17 10:51:17.352710: Pseudo dice [0.7237]
2024-12-17 10:51:17.353728: Epoch time: 88.56 s
2024-12-17 10:51:18.605335: 
2024-12-17 10:51:18.607355: Epoch 82
2024-12-17 10:51:18.608728: Current learning rate: 0.00491
2024-12-17 10:52:47.269268: Validation loss did not improve from -0.54789. Patience: 19/50
2024-12-17 10:52:47.270370: train_loss -0.7096
2024-12-17 10:52:47.271515: val_loss -0.5132
2024-12-17 10:52:47.272340: Pseudo dice [0.731]
2024-12-17 10:52:47.273276: Epoch time: 88.67 s
2024-12-17 10:52:48.457589: 
2024-12-17 10:52:48.460102: Epoch 83
2024-12-17 10:52:48.461652: Current learning rate: 0.00484
2024-12-17 10:54:17.361675: Validation loss did not improve from -0.54789. Patience: 20/50
2024-12-17 10:54:17.362893: train_loss -0.7084
2024-12-17 10:54:17.363845: val_loss -0.5166
2024-12-17 10:54:17.364681: Pseudo dice [0.726]
2024-12-17 10:54:17.365442: Epoch time: 88.91 s
2024-12-17 10:54:18.543163: 
2024-12-17 10:54:18.545114: Epoch 84
2024-12-17 10:54:18.546412: Current learning rate: 0.00478
2024-12-17 10:55:47.639835: Validation loss did not improve from -0.54789. Patience: 21/50
2024-12-17 10:55:47.656491: train_loss -0.7113
2024-12-17 10:55:47.658209: val_loss -0.5434
2024-12-17 10:55:47.659054: Pseudo dice [0.7445]
2024-12-17 10:55:47.660470: Epoch time: 89.11 s
2024-12-17 10:55:48.043911: Yayy! New best EMA pseudo Dice: 0.7281
2024-12-17 10:55:49.685864: 
2024-12-17 10:55:49.687947: Epoch 85
2024-12-17 10:55:49.689255: Current learning rate: 0.00471
2024-12-17 10:57:18.201007: Validation loss did not improve from -0.54789. Patience: 22/50
2024-12-17 10:57:18.202501: train_loss -0.7161
2024-12-17 10:57:18.203820: val_loss -0.483
2024-12-17 10:57:18.205007: Pseudo dice [0.7188]
2024-12-17 10:57:18.206063: Epoch time: 88.52 s
2024-12-17 10:57:19.464529: 
2024-12-17 10:57:19.468155: Epoch 86
2024-12-17 10:57:19.469616: Current learning rate: 0.00465
2024-12-17 10:58:50.713294: Validation loss did not improve from -0.54789. Patience: 23/50
2024-12-17 10:58:50.714544: train_loss -0.7138
2024-12-17 10:58:50.715809: val_loss -0.5455
2024-12-17 10:58:50.716743: Pseudo dice [0.7392]
2024-12-17 10:58:50.717639: Epoch time: 91.25 s
2024-12-17 10:58:50.718810: Yayy! New best EMA pseudo Dice: 0.7284
2024-12-17 10:58:52.589182: 
2024-12-17 10:58:52.591169: Epoch 87
2024-12-17 10:58:52.592460: Current learning rate: 0.00458
2024-12-17 11:00:21.272510: Validation loss did not improve from -0.54789. Patience: 24/50
2024-12-17 11:00:21.275096: train_loss -0.7129
2024-12-17 11:00:21.277680: val_loss -0.5217
2024-12-17 11:00:21.278510: Pseudo dice [0.7297]
2024-12-17 11:00:21.279496: Epoch time: 88.69 s
2024-12-17 11:00:21.280221: Yayy! New best EMA pseudo Dice: 0.7285
2024-12-17 11:00:23.048282: 
2024-12-17 11:00:23.050570: Epoch 88
2024-12-17 11:00:23.051609: Current learning rate: 0.00452
2024-12-17 11:01:51.550818: Validation loss did not improve from -0.54789. Patience: 25/50
2024-12-17 11:01:51.551898: train_loss -0.7056
2024-12-17 11:01:51.553456: val_loss -0.5278
2024-12-17 11:01:51.554388: Pseudo dice [0.738]
2024-12-17 11:01:51.555208: Epoch time: 88.5 s
2024-12-17 11:01:51.556037: Yayy! New best EMA pseudo Dice: 0.7295
2024-12-17 11:01:53.075516: 
2024-12-17 11:01:53.077858: Epoch 89
2024-12-17 11:01:53.079350: Current learning rate: 0.00445
2024-12-17 11:03:21.596353: Validation loss did not improve from -0.54789. Patience: 26/50
2024-12-17 11:03:21.597846: train_loss -0.705
2024-12-17 11:03:21.599273: val_loss -0.5198
2024-12-17 11:03:21.600286: Pseudo dice [0.7401]
2024-12-17 11:03:21.601294: Epoch time: 88.52 s
2024-12-17 11:03:21.936281: Yayy! New best EMA pseudo Dice: 0.7305
2024-12-17 11:03:23.448445: 
2024-12-17 11:03:23.450275: Epoch 90
2024-12-17 11:03:23.451184: Current learning rate: 0.00438
2024-12-17 11:04:52.024970: Validation loss did not improve from -0.54789. Patience: 27/50
2024-12-17 11:04:52.026273: train_loss -0.717
2024-12-17 11:04:52.027579: val_loss -0.5073
2024-12-17 11:04:52.028632: Pseudo dice [0.728]
2024-12-17 11:04:52.029683: Epoch time: 88.58 s
2024-12-17 11:04:53.213973: 
2024-12-17 11:04:53.216678: Epoch 91
2024-12-17 11:04:53.218243: Current learning rate: 0.00432
2024-12-17 11:06:21.900568: Validation loss did not improve from -0.54789. Patience: 28/50
2024-12-17 11:06:21.901684: train_loss -0.7214
2024-12-17 11:06:21.902879: val_loss -0.5203
2024-12-17 11:06:21.903575: Pseudo dice [0.7414]
2024-12-17 11:06:21.904570: Epoch time: 88.69 s
2024-12-17 11:06:21.905542: Yayy! New best EMA pseudo Dice: 0.7314
2024-12-17 11:06:23.760265: 
2024-12-17 11:06:23.762604: Epoch 92
2024-12-17 11:06:23.764085: Current learning rate: 0.00425
2024-12-17 11:07:52.343517: Validation loss did not improve from -0.54789. Patience: 29/50
2024-12-17 11:07:52.344831: train_loss -0.7227
2024-12-17 11:07:52.345741: val_loss -0.4941
2024-12-17 11:07:52.346479: Pseudo dice [0.7217]
2024-12-17 11:07:52.347238: Epoch time: 88.59 s
2024-12-17 11:07:53.526747: 
2024-12-17 11:07:53.528876: Epoch 93
2024-12-17 11:07:53.529917: Current learning rate: 0.00419
2024-12-17 11:09:22.259773: Validation loss did not improve from -0.54789. Patience: 30/50
2024-12-17 11:09:22.260891: train_loss -0.7206
2024-12-17 11:09:22.262182: val_loss -0.5418
2024-12-17 11:09:22.263137: Pseudo dice [0.7407]
2024-12-17 11:09:22.263928: Epoch time: 88.74 s
2024-12-17 11:09:22.264823: Yayy! New best EMA pseudo Dice: 0.7315
2024-12-17 11:09:23.760108: 
2024-12-17 11:09:23.762540: Epoch 94
2024-12-17 11:09:23.764447: Current learning rate: 0.00412
2024-12-17 11:10:52.328515: Validation loss did not improve from -0.54789. Patience: 31/50
2024-12-17 11:10:52.329653: train_loss -0.7244
2024-12-17 11:10:52.330785: val_loss -0.5191
2024-12-17 11:10:52.331814: Pseudo dice [0.7344]
2024-12-17 11:10:52.332763: Epoch time: 88.57 s
2024-12-17 11:10:52.688362: Yayy! New best EMA pseudo Dice: 0.7317
2024-12-17 11:10:54.193544: 
2024-12-17 11:10:54.195845: Epoch 95
2024-12-17 11:10:54.197031: Current learning rate: 0.00405
2024-12-17 11:12:22.613462: Validation loss did not improve from -0.54789. Patience: 32/50
2024-12-17 11:12:22.614622: train_loss -0.7206
2024-12-17 11:12:22.615799: val_loss -0.4926
2024-12-17 11:12:22.616447: Pseudo dice [0.7104]
2024-12-17 11:12:22.617287: Epoch time: 88.42 s
2024-12-17 11:12:23.782071: 
2024-12-17 11:12:23.784630: Epoch 96
2024-12-17 11:12:23.785550: Current learning rate: 0.00399
2024-12-17 11:13:52.210071: Validation loss did not improve from -0.54789. Patience: 33/50
2024-12-17 11:13:52.211427: train_loss -0.7258
2024-12-17 11:13:52.212381: val_loss -0.5202
2024-12-17 11:13:52.213163: Pseudo dice [0.7257]
2024-12-17 11:13:52.213965: Epoch time: 88.43 s
2024-12-17 11:13:53.429824: 
2024-12-17 11:13:53.431483: Epoch 97
2024-12-17 11:13:53.432309: Current learning rate: 0.00392
2024-12-17 11:15:21.804950: Validation loss did not improve from -0.54789. Patience: 34/50
2024-12-17 11:15:21.806276: train_loss -0.7238
2024-12-17 11:15:21.807507: val_loss -0.5158
2024-12-17 11:15:21.808431: Pseudo dice [0.7261]
2024-12-17 11:15:21.809464: Epoch time: 88.38 s
2024-12-17 11:15:22.960734: 
2024-12-17 11:15:22.962534: Epoch 98
2024-12-17 11:15:22.963876: Current learning rate: 0.00385
2024-12-17 11:16:51.364207: Validation loss did not improve from -0.54789. Patience: 35/50
2024-12-17 11:16:51.365540: train_loss -0.7211
2024-12-17 11:16:51.366904: val_loss -0.4878
2024-12-17 11:16:51.367850: Pseudo dice [0.7236]
2024-12-17 11:16:51.368794: Epoch time: 88.41 s
2024-12-17 11:16:52.519553: 
2024-12-17 11:16:52.521487: Epoch 99
2024-12-17 11:16:52.522670: Current learning rate: 0.00379
2024-12-17 11:18:21.004572: Validation loss did not improve from -0.54789. Patience: 36/50
2024-12-17 11:18:21.005757: train_loss -0.7265
2024-12-17 11:18:21.007041: val_loss -0.5228
2024-12-17 11:18:21.007840: Pseudo dice [0.7362]
2024-12-17 11:18:21.008855: Epoch time: 88.49 s
2024-12-17 11:18:22.504194: 
2024-12-17 11:18:22.506566: Epoch 100
2024-12-17 11:18:22.507837: Current learning rate: 0.00372
2024-12-17 11:19:50.950375: Validation loss did not improve from -0.54789. Patience: 37/50
2024-12-17 11:19:50.951946: train_loss -0.7336
2024-12-17 11:19:50.953497: val_loss -0.51
2024-12-17 11:19:50.954408: Pseudo dice [0.7237]
2024-12-17 11:19:50.955367: Epoch time: 88.45 s
2024-12-17 11:19:52.113604: 
2024-12-17 11:19:52.116665: Epoch 101
2024-12-17 11:19:52.118371: Current learning rate: 0.00365
2024-12-17 11:21:20.642918: Validation loss did not improve from -0.54789. Patience: 38/50
2024-12-17 11:21:20.644198: train_loss -0.7281
2024-12-17 11:21:20.645099: val_loss -0.5142
2024-12-17 11:21:20.646038: Pseudo dice [0.742]
2024-12-17 11:21:20.646686: Epoch time: 88.53 s
2024-12-17 11:21:21.809521: 
2024-12-17 11:21:21.811469: Epoch 102
2024-12-17 11:21:21.812442: Current learning rate: 0.00359
2024-12-17 11:22:50.228392: Validation loss did not improve from -0.54789. Patience: 39/50
2024-12-17 11:22:50.229315: train_loss -0.7391
2024-12-17 11:22:50.230351: val_loss -0.5041
2024-12-17 11:22:50.231163: Pseudo dice [0.7254]
2024-12-17 11:22:50.231868: Epoch time: 88.42 s
2024-12-17 11:22:51.855484: 
2024-12-17 11:22:51.857512: Epoch 103
2024-12-17 11:22:51.858648: Current learning rate: 0.00352
2024-12-17 11:24:19.982486: Validation loss did not improve from -0.54789. Patience: 40/50
2024-12-17 11:24:19.983467: train_loss -0.736
2024-12-17 11:24:19.984385: val_loss -0.4968
2024-12-17 11:24:19.985371: Pseudo dice [0.7321]
2024-12-17 11:24:19.986169: Epoch time: 88.13 s
2024-12-17 11:24:21.191231: 
2024-12-17 11:24:21.193213: Epoch 104
2024-12-17 11:24:21.194405: Current learning rate: 0.00345
2024-12-17 11:25:49.356259: Validation loss did not improve from -0.54789. Patience: 41/50
2024-12-17 11:25:49.357674: train_loss -0.7323
2024-12-17 11:25:49.359010: val_loss -0.5261
2024-12-17 11:25:49.360028: Pseudo dice [0.7387]
2024-12-17 11:25:49.360853: Epoch time: 88.17 s
2024-12-17 11:25:50.899034: 
2024-12-17 11:25:50.900759: Epoch 105
2024-12-17 11:25:50.901785: Current learning rate: 0.00338
2024-12-17 11:27:19.279868: Validation loss did not improve from -0.54789. Patience: 42/50
2024-12-17 11:27:19.281253: train_loss -0.7399
2024-12-17 11:27:19.282679: val_loss -0.5143
2024-12-17 11:27:19.283841: Pseudo dice [0.7292]
2024-12-17 11:27:19.285169: Epoch time: 88.38 s
2024-12-17 11:27:20.426060: 
2024-12-17 11:27:20.428524: Epoch 106
2024-12-17 11:27:20.429532: Current learning rate: 0.00332
2024-12-17 11:28:48.702064: Validation loss did not improve from -0.54789. Patience: 43/50
2024-12-17 11:28:48.703545: train_loss -0.7315
2024-12-17 11:28:48.704709: val_loss -0.5018
2024-12-17 11:28:48.705808: Pseudo dice [0.7247]
2024-12-17 11:28:48.707033: Epoch time: 88.28 s
2024-12-17 11:28:49.902575: 
2024-12-17 11:28:49.904677: Epoch 107
2024-12-17 11:28:49.905925: Current learning rate: 0.00325
2024-12-17 11:30:18.094582: Validation loss did not improve from -0.54789. Patience: 44/50
2024-12-17 11:30:18.095525: train_loss -0.7339
2024-12-17 11:30:18.096471: val_loss -0.5335
2024-12-17 11:30:18.097364: Pseudo dice [0.7395]
2024-12-17 11:30:18.098132: Epoch time: 88.19 s
2024-12-17 11:30:19.334451: 
2024-12-17 11:30:19.336722: Epoch 108
2024-12-17 11:30:19.338081: Current learning rate: 0.00318
2024-12-17 11:31:47.632770: Validation loss did not improve from -0.54789. Patience: 45/50
2024-12-17 11:31:47.634096: train_loss -0.7346
2024-12-17 11:31:47.635395: val_loss -0.5009
2024-12-17 11:31:47.636529: Pseudo dice [0.7283]
2024-12-17 11:31:47.637805: Epoch time: 88.3 s
2024-12-17 11:31:48.789578: 
2024-12-17 11:31:48.791209: Epoch 109
2024-12-17 11:31:48.792566: Current learning rate: 0.00311
2024-12-17 11:33:17.182486: Validation loss did not improve from -0.54789. Patience: 46/50
2024-12-17 11:33:17.183781: train_loss -0.7291
2024-12-17 11:33:17.185154: val_loss -0.4856
2024-12-17 11:33:17.186236: Pseudo dice [0.7181]
2024-12-17 11:33:17.186965: Epoch time: 88.4 s
2024-12-17 11:33:18.706573: 
2024-12-17 11:33:18.708498: Epoch 110
2024-12-17 11:33:18.710194: Current learning rate: 0.00304
2024-12-17 11:34:47.030461: Validation loss did not improve from -0.54789. Patience: 47/50
2024-12-17 11:34:47.031434: train_loss -0.7349
2024-12-17 11:34:47.032656: val_loss -0.5127
2024-12-17 11:34:47.033601: Pseudo dice [0.7284]
2024-12-17 11:34:47.034528: Epoch time: 88.33 s
2024-12-17 11:34:48.180835: 
2024-12-17 11:34:48.182981: Epoch 111
2024-12-17 11:34:48.184230: Current learning rate: 0.00297
2024-12-17 11:36:16.734940: Validation loss did not improve from -0.54789. Patience: 48/50
2024-12-17 11:36:16.736282: train_loss -0.7399
2024-12-17 11:36:16.737496: val_loss -0.502
2024-12-17 11:36:16.738482: Pseudo dice [0.7181]
2024-12-17 11:36:16.739632: Epoch time: 88.56 s
2024-12-17 11:36:17.924504: 
2024-12-17 11:36:17.927322: Epoch 112
2024-12-17 11:36:17.929077: Current learning rate: 0.00291
2024-12-17 11:37:46.391367: Validation loss did not improve from -0.54789. Patience: 49/50
2024-12-17 11:37:46.392923: train_loss -0.7403
2024-12-17 11:37:46.393687: val_loss -0.4943
2024-12-17 11:37:46.394339: Pseudo dice [0.7259]
2024-12-17 11:37:46.395046: Epoch time: 88.47 s
2024-12-17 11:37:47.914953: 
2024-12-17 11:37:47.916883: Epoch 113
2024-12-17 11:37:47.918009: Current learning rate: 0.00284
2024-12-17 11:39:16.249084: Validation loss did not improve from -0.54789. Patience: 50/50
2024-12-17 11:39:16.250149: train_loss -0.7375
2024-12-17 11:39:16.251115: val_loss -0.5387
2024-12-17 11:39:16.252334: Pseudo dice [0.7435]
2024-12-17 11:39:16.253358: Epoch time: 88.34 s
2024-12-17 11:39:17.442292: 
2024-12-17 11:39:17.444306: Epoch 114
2024-12-17 11:39:17.445419: Current learning rate: 0.00277
2024-12-17 11:40:45.702754: Validation loss did not improve from -0.54789. Patience: 51/50
2024-12-17 11:40:45.704237: train_loss -0.741
2024-12-17 11:40:45.704998: val_loss -0.4989
2024-12-17 11:40:45.706007: Pseudo dice [0.7232]
2024-12-17 11:40:45.706848: Epoch time: 88.26 s
2024-12-17 11:40:47.263730: 
2024-12-17 11:40:47.265846: Epoch 115
2024-12-17 11:40:47.267003: Current learning rate: 0.0027
2024-12-17 11:42:15.581273: Validation loss did not improve from -0.54789. Patience: 52/50
2024-12-17 11:42:15.582606: train_loss -0.7485
2024-12-17 11:42:15.584062: val_loss -0.5164
2024-12-17 11:42:15.585289: Pseudo dice [0.7362]
2024-12-17 11:42:15.585991: Epoch time: 88.32 s
2024-12-17 11:42:16.776352: 
2024-12-17 11:42:16.778800: Epoch 116
2024-12-17 11:42:16.779889: Current learning rate: 0.00263
2024-12-17 11:43:44.974479: Validation loss did not improve from -0.54789. Patience: 53/50
2024-12-17 11:43:44.975507: train_loss -0.7494
2024-12-17 11:43:44.976821: val_loss -0.5109
2024-12-17 11:43:44.977979: Pseudo dice [0.7318]
2024-12-17 11:43:44.979030: Epoch time: 88.2 s
2024-12-17 11:43:46.179169: 
2024-12-17 11:43:46.181160: Epoch 117
2024-12-17 11:43:46.182222: Current learning rate: 0.00256
2024-12-17 11:45:14.460669: Validation loss did not improve from -0.54789. Patience: 54/50
2024-12-17 11:45:14.461628: train_loss -0.7398
2024-12-17 11:45:14.462614: val_loss -0.5268
2024-12-17 11:45:14.463646: Pseudo dice [0.7383]
2024-12-17 11:45:14.464813: Epoch time: 88.28 s
2024-12-17 11:45:15.664567: 
2024-12-17 11:45:15.666694: Epoch 118
2024-12-17 11:45:15.667930: Current learning rate: 0.00249
2024-12-17 11:46:43.956455: Validation loss did not improve from -0.54789. Patience: 55/50
2024-12-17 11:46:43.957712: train_loss -0.7471
2024-12-17 11:46:43.958987: val_loss -0.5287
2024-12-17 11:46:43.959977: Pseudo dice [0.7404]
2024-12-17 11:46:43.960591: Epoch time: 88.29 s
2024-12-17 11:46:45.162396: 
2024-12-17 11:46:45.164808: Epoch 119
2024-12-17 11:46:45.166167: Current learning rate: 0.00242
2024-12-17 11:48:13.366420: Validation loss did not improve from -0.54789. Patience: 56/50
2024-12-17 11:48:13.367745: train_loss -0.7497
2024-12-17 11:48:13.368917: val_loss -0.4679
2024-12-17 11:48:13.369763: Pseudo dice [0.7053]
2024-12-17 11:48:13.370607: Epoch time: 88.21 s
2024-12-17 11:48:14.942528: 
2024-12-17 11:48:14.944711: Epoch 120
2024-12-17 11:48:14.945973: Current learning rate: 0.00235
2024-12-17 11:49:43.202351: Validation loss did not improve from -0.54789. Patience: 57/50
2024-12-17 11:49:43.203386: train_loss -0.7504
2024-12-17 11:49:43.204133: val_loss -0.502
2024-12-17 11:49:43.204756: Pseudo dice [0.719]
2024-12-17 11:49:43.205402: Epoch time: 88.26 s
2024-12-17 11:49:44.464921: 
2024-12-17 11:49:44.466813: Epoch 121
2024-12-17 11:49:44.467801: Current learning rate: 0.00228
2024-12-17 11:51:12.883111: Validation loss did not improve from -0.54789. Patience: 58/50
2024-12-17 11:51:12.884318: train_loss -0.7501
2024-12-17 11:51:12.885718: val_loss -0.506
2024-12-17 11:51:12.887056: Pseudo dice [0.728]
2024-12-17 11:51:12.888028: Epoch time: 88.42 s
2024-12-17 11:51:14.118687: 
2024-12-17 11:51:14.120386: Epoch 122
2024-12-17 11:51:14.121740: Current learning rate: 0.00221
2024-12-17 11:52:42.593545: Validation loss did not improve from -0.54789. Patience: 59/50
2024-12-17 11:52:42.594942: train_loss -0.7535
2024-12-17 11:52:42.596055: val_loss -0.5179
2024-12-17 11:52:42.596812: Pseudo dice [0.7351]
2024-12-17 11:52:42.597748: Epoch time: 88.48 s
2024-12-17 11:52:43.787116: 
2024-12-17 11:52:43.789294: Epoch 123
2024-12-17 11:52:43.790387: Current learning rate: 0.00214
2024-12-17 11:54:12.266287: Validation loss did not improve from -0.54789. Patience: 60/50
2024-12-17 11:54:12.267471: train_loss -0.7495
2024-12-17 11:54:12.268306: val_loss -0.5117
2024-12-17 11:54:12.269065: Pseudo dice [0.7295]
2024-12-17 11:54:12.270005: Epoch time: 88.48 s
2024-12-17 11:54:13.800407: 
2024-12-17 11:54:13.802412: Epoch 124
2024-12-17 11:54:13.803795: Current learning rate: 0.00207
2024-12-17 11:55:42.284735: Validation loss did not improve from -0.54789. Patience: 61/50
2024-12-17 11:55:42.285827: train_loss -0.7539
2024-12-17 11:55:42.286884: val_loss -0.5035
2024-12-17 11:55:42.287980: Pseudo dice [0.7231]
2024-12-17 11:55:42.289014: Epoch time: 88.49 s
2024-12-17 11:55:43.840768: 
2024-12-17 11:55:43.842884: Epoch 125
2024-12-17 11:55:43.844286: Current learning rate: 0.00199
2024-12-17 11:57:12.240771: Validation loss did not improve from -0.54789. Patience: 62/50
2024-12-17 11:57:12.242299: train_loss -0.7554
2024-12-17 11:57:12.243887: val_loss -0.5149
2024-12-17 11:57:12.245013: Pseudo dice [0.7396]
2024-12-17 11:57:12.246120: Epoch time: 88.4 s
2024-12-17 11:57:13.454782: 
2024-12-17 11:57:13.457211: Epoch 126
2024-12-17 11:57:13.458737: Current learning rate: 0.00192
2024-12-17 11:58:42.121813: Validation loss did not improve from -0.54789. Patience: 63/50
2024-12-17 11:58:42.159536: train_loss -0.7569
2024-12-17 11:58:42.160839: val_loss -0.5156
2024-12-17 11:58:42.161982: Pseudo dice [0.7308]
2024-12-17 11:58:42.162803: Epoch time: 88.71 s
2024-12-17 11:58:43.514306: 
2024-12-17 11:58:43.515916: Epoch 127
2024-12-17 11:58:43.517036: Current learning rate: 0.00185
2024-12-17 12:00:11.724098: Validation loss did not improve from -0.54789. Patience: 64/50
2024-12-17 12:00:11.725091: train_loss -0.7521
2024-12-17 12:00:11.726146: val_loss -0.4928
2024-12-17 12:00:11.727097: Pseudo dice [0.7168]
2024-12-17 12:00:11.728088: Epoch time: 88.21 s
2024-12-17 12:00:12.948908: 
2024-12-17 12:00:12.950451: Epoch 128
2024-12-17 12:00:12.951637: Current learning rate: 0.00178
2024-12-17 12:01:41.283816: Validation loss did not improve from -0.54789. Patience: 65/50
2024-12-17 12:01:41.296261: train_loss -0.7592
2024-12-17 12:01:41.298366: val_loss -0.5062
2024-12-17 12:01:41.299788: Pseudo dice [0.7268]
2024-12-17 12:01:41.301004: Epoch time: 88.35 s
2024-12-17 12:01:42.557754: 
2024-12-17 12:01:42.560101: Epoch 129
2024-12-17 12:01:42.561313: Current learning rate: 0.0017
2024-12-17 12:03:10.829568: Validation loss did not improve from -0.54789. Patience: 66/50
2024-12-17 12:03:10.830860: train_loss -0.7548
2024-12-17 12:03:10.832309: val_loss -0.5056
2024-12-17 12:03:10.833383: Pseudo dice [0.7245]
2024-12-17 12:03:10.834446: Epoch time: 88.27 s
2024-12-17 12:03:12.373877: 
2024-12-17 12:03:12.376144: Epoch 130
2024-12-17 12:03:12.377221: Current learning rate: 0.00163
2024-12-17 12:04:40.655324: Validation loss did not improve from -0.54789. Patience: 67/50
2024-12-17 12:04:40.657081: train_loss -0.7593
2024-12-17 12:04:40.659123: val_loss -0.521
2024-12-17 12:04:40.660226: Pseudo dice [0.7341]
2024-12-17 12:04:40.661341: Epoch time: 88.28 s
2024-12-17 12:04:41.864578: 
2024-12-17 12:04:41.866302: Epoch 131
2024-12-17 12:04:41.867233: Current learning rate: 0.00156
2024-12-17 12:06:10.137174: Validation loss did not improve from -0.54789. Patience: 68/50
2024-12-17 12:06:10.138292: train_loss -0.7548
2024-12-17 12:06:10.139697: val_loss -0.527
2024-12-17 12:06:10.140813: Pseudo dice [0.7402]
2024-12-17 12:06:10.141887: Epoch time: 88.27 s
2024-12-17 12:06:11.343681: 
2024-12-17 12:06:11.345659: Epoch 132
2024-12-17 12:06:11.346910: Current learning rate: 0.00148
2024-12-17 12:07:39.689485: Validation loss did not improve from -0.54789. Patience: 69/50
2024-12-17 12:07:39.690670: train_loss -0.7592
2024-12-17 12:07:39.691721: val_loss -0.4995
2024-12-17 12:07:39.692606: Pseudo dice [0.7217]
2024-12-17 12:07:39.693617: Epoch time: 88.35 s
2024-12-17 12:07:40.894202: 
2024-12-17 12:07:40.896388: Epoch 133
2024-12-17 12:07:40.897625: Current learning rate: 0.00141
2024-12-17 12:09:09.192465: Validation loss did not improve from -0.54789. Patience: 70/50
2024-12-17 12:09:09.193888: train_loss -0.7588
2024-12-17 12:09:09.194955: val_loss -0.5201
2024-12-17 12:09:09.196300: Pseudo dice [0.7253]
2024-12-17 12:09:09.197449: Epoch time: 88.3 s
2024-12-17 12:09:11.165409: 
2024-12-17 12:09:11.167746: Epoch 134
2024-12-17 12:09:11.168928: Current learning rate: 0.00133
2024-12-17 12:10:39.458207: Validation loss did not improve from -0.54789. Patience: 71/50
2024-12-17 12:10:39.459355: train_loss -0.7593
2024-12-17 12:10:39.460450: val_loss -0.5371
2024-12-17 12:10:39.461411: Pseudo dice [0.7405]
2024-12-17 12:10:39.462516: Epoch time: 88.3 s
2024-12-17 12:10:41.014111: 
2024-12-17 12:10:41.016169: Epoch 135
2024-12-17 12:10:41.017433: Current learning rate: 0.00126
2024-12-17 12:12:09.552567: Validation loss did not improve from -0.54789. Patience: 72/50
2024-12-17 12:12:09.553830: train_loss -0.7625
2024-12-17 12:12:09.555340: val_loss -0.5046
2024-12-17 12:12:09.556286: Pseudo dice [0.7264]
2024-12-17 12:12:09.557265: Epoch time: 88.54 s
2024-12-17 12:12:10.782092: 
2024-12-17 12:12:10.784248: Epoch 136
2024-12-17 12:12:10.785633: Current learning rate: 0.00118
2024-12-17 12:13:39.138572: Validation loss did not improve from -0.54789. Patience: 73/50
2024-12-17 12:13:39.139551: train_loss -0.7597
2024-12-17 12:13:39.140528: val_loss -0.5123
2024-12-17 12:13:39.141426: Pseudo dice [0.7241]
2024-12-17 12:13:39.142066: Epoch time: 88.36 s
2024-12-17 12:13:40.367527: 
2024-12-17 12:13:40.369747: Epoch 137
2024-12-17 12:13:40.370726: Current learning rate: 0.00111
2024-12-17 12:15:08.439358: Validation loss did not improve from -0.54789. Patience: 74/50
2024-12-17 12:15:08.440399: train_loss -0.7635
2024-12-17 12:15:08.441401: val_loss -0.5041
2024-12-17 12:15:08.442368: Pseudo dice [0.7381]
2024-12-17 12:15:08.443502: Epoch time: 88.07 s
2024-12-17 12:15:09.661072: 
2024-12-17 12:15:09.662602: Epoch 138
2024-12-17 12:15:09.663727: Current learning rate: 0.00103
2024-12-17 12:16:37.700746: Validation loss did not improve from -0.54789. Patience: 75/50
2024-12-17 12:16:37.702015: train_loss -0.7597
2024-12-17 12:16:37.703228: val_loss -0.5256
2024-12-17 12:16:37.704116: Pseudo dice [0.7372]
2024-12-17 12:16:37.704852: Epoch time: 88.04 s
2024-12-17 12:16:38.913086: 
2024-12-17 12:16:38.915202: Epoch 139
2024-12-17 12:16:38.916462: Current learning rate: 0.00095
2024-12-17 12:18:06.926197: Validation loss did not improve from -0.54789. Patience: 76/50
2024-12-17 12:18:06.927402: train_loss -0.7607
2024-12-17 12:18:06.928420: val_loss -0.5198
2024-12-17 12:18:06.929070: Pseudo dice [0.7304]
2024-12-17 12:18:06.929815: Epoch time: 88.02 s
2024-12-17 12:18:08.472487: 
2024-12-17 12:18:08.474394: Epoch 140
2024-12-17 12:18:08.475432: Current learning rate: 0.00087
2024-12-17 12:19:36.509589: Validation loss did not improve from -0.54789. Patience: 77/50
2024-12-17 12:19:36.510713: train_loss -0.7668
2024-12-17 12:19:36.511882: val_loss -0.5301
2024-12-17 12:19:36.512804: Pseudo dice [0.7454]
2024-12-17 12:19:36.513840: Epoch time: 88.04 s
2024-12-17 12:19:36.514750: Yayy! New best EMA pseudo Dice: 0.732
2024-12-17 12:19:38.147811: 
2024-12-17 12:19:38.149727: Epoch 141
2024-12-17 12:19:38.151023: Current learning rate: 0.00079
2024-12-17 12:21:06.116448: Validation loss did not improve from -0.54789. Patience: 78/50
2024-12-17 12:21:06.117821: train_loss -0.764
2024-12-17 12:21:06.118850: val_loss -0.5131
2024-12-17 12:21:06.119585: Pseudo dice [0.7302]
2024-12-17 12:21:06.120693: Epoch time: 87.97 s
2024-12-17 12:21:07.322177: 
2024-12-17 12:21:07.324201: Epoch 142
2024-12-17 12:21:07.325418: Current learning rate: 0.00071
2024-12-17 12:22:35.340913: Validation loss did not improve from -0.54789. Patience: 79/50
2024-12-17 12:22:35.342113: train_loss -0.7666
2024-12-17 12:22:35.343159: val_loss -0.5299
2024-12-17 12:22:35.343904: Pseudo dice [0.7362]
2024-12-17 12:22:35.344883: Epoch time: 88.02 s
2024-12-17 12:22:35.345581: Yayy! New best EMA pseudo Dice: 0.7322
2024-12-17 12:22:36.860877: 
2024-12-17 12:22:36.862700: Epoch 143
2024-12-17 12:22:36.863806: Current learning rate: 0.00063
2024-12-17 12:24:04.740496: Validation loss did not improve from -0.54789. Patience: 80/50
2024-12-17 12:24:04.741725: train_loss -0.7673
2024-12-17 12:24:04.742822: val_loss -0.5246
2024-12-17 12:24:04.743788: Pseudo dice [0.7295]
2024-12-17 12:24:04.744582: Epoch time: 87.88 s
2024-12-17 12:24:06.348155: 
2024-12-17 12:24:06.350391: Epoch 144
2024-12-17 12:24:06.351688: Current learning rate: 0.00055
2024-12-17 12:25:34.348360: Validation loss did not improve from -0.54789. Patience: 81/50
2024-12-17 12:25:34.349741: train_loss -0.7675
2024-12-17 12:25:34.351277: val_loss -0.5026
2024-12-17 12:25:34.352678: Pseudo dice [0.7289]
2024-12-17 12:25:34.354106: Epoch time: 88.0 s
2024-12-17 12:25:35.903005: 
2024-12-17 12:25:35.905329: Epoch 145
2024-12-17 12:25:35.906763: Current learning rate: 0.00047
2024-12-17 12:27:04.127223: Validation loss did not improve from -0.54789. Patience: 82/50
2024-12-17 12:27:04.128353: train_loss -0.7678
2024-12-17 12:27:04.129543: val_loss -0.5143
2024-12-17 12:27:04.130256: Pseudo dice [0.7312]
2024-12-17 12:27:04.131171: Epoch time: 88.23 s
2024-12-17 12:27:05.341251: 
2024-12-17 12:27:05.343096: Epoch 146
2024-12-17 12:27:05.344014: Current learning rate: 0.00038
2024-12-17 12:28:33.687964: Validation loss did not improve from -0.54789. Patience: 83/50
2024-12-17 12:28:33.689115: train_loss -0.7638
2024-12-17 12:28:33.690102: val_loss -0.5237
2024-12-17 12:28:33.690824: Pseudo dice [0.7349]
2024-12-17 12:28:33.691520: Epoch time: 88.35 s
2024-12-17 12:28:34.901772: 
2024-12-17 12:28:34.904049: Epoch 147
2024-12-17 12:28:34.905071: Current learning rate: 0.0003
2024-12-17 12:30:03.233596: Validation loss did not improve from -0.54789. Patience: 84/50
2024-12-17 12:30:03.234503: train_loss -0.7711
2024-12-17 12:30:03.235566: val_loss -0.5144
2024-12-17 12:30:03.236754: Pseudo dice [0.7289]
2024-12-17 12:30:03.237825: Epoch time: 88.33 s
2024-12-17 12:30:04.464093: 
2024-12-17 12:30:04.465898: Epoch 148
2024-12-17 12:30:04.466963: Current learning rate: 0.00021
2024-12-17 12:31:32.766099: Validation loss did not improve from -0.54789. Patience: 85/50
2024-12-17 12:31:32.767336: train_loss -0.7673
2024-12-17 12:31:32.768271: val_loss -0.5207
2024-12-17 12:31:32.769103: Pseudo dice [0.7328]
2024-12-17 12:31:32.769815: Epoch time: 88.3 s
2024-12-17 12:31:33.983350: 
2024-12-17 12:31:33.985475: Epoch 149
2024-12-17 12:31:33.986699: Current learning rate: 0.00011
2024-12-17 12:33:02.340815: Validation loss did not improve from -0.54789. Patience: 86/50
2024-12-17 12:33:02.341827: train_loss -0.7702
2024-12-17 12:33:02.342763: val_loss -0.4954
2024-12-17 12:33:02.343448: Pseudo dice [0.7262]
2024-12-17 12:33:02.344211: Epoch time: 88.36 s
2024-12-17 12:33:03.977748: Training done.
2024-12-17 12:33:04.581784: Using splits from existing split file: /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset307_Sohee_Calcium_OCT_CrossValidation/splits_final.json
2024-12-17 12:33:04.587090: The split file contains 5 splits.
2024-12-17 12:33:04.588877: Desired fold for training: 4
2024-12-17 12:33:04.589958: This split has 7 training and 1 validation cases.
2024-12-17 12:33:04.591234: predicting 101-045
2024-12-17 12:33:04.616892: 101-045, shape torch.Size([1, 375, 498, 498]), rank 0
2024-12-17 12:38:19.411022: Validation complete
2024-12-17 12:38:19.412326: Mean Validation Dice:  0.7303313696352178
