/home/gridsan/nchutisilp/.conda/envs/nnunet/bin/python
Starting training with CONFIG=3d_32x160x128_b10, DATASET_ID=309, TRAINER=nnUNetTrainerScaleAnalysis40

############################
INFO: You are using the old nnU-Net default plans. We have updated our recommendations. Please consider using those instead! Read more here: https://github.com/MIC-DKFZ/nnUNet/blob/master/documentation/resenc_presets.md
############################

Using device: cuda:0

#######################################################################
Please cite the following paper when using nnU-Net:
Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.
#######################################################################

2024-12-17 21:20:25.080770: Using torch.compile...

############################
INFO: You are using the old nnU-Net default plans. We have updated our recommendations. Please consider using those instead! Read more here: https://github.com/MIC-DKFZ/nnUNet/blob/master/documentation/resenc_presets.md
############################

Using device: cuda:0

#######################################################################
Please cite the following paper when using nnU-Net:
Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.
#######################################################################

2024-12-17 21:20:25.076438: Using torch.compile...
/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
################### Loading pretrained weights from file  /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_results/Dataset300_Lumen_and_Wall_OCT/nnUNetTrainer__nnUNetPreprocessPlans__3d_fullres/fold_all/checkpoint_best.pth ###################
Below is the list of overlapping blocks in pretrained model and nnUNet architecture:
encoder.stages.0.0.convs.0.conv.weight shape torch.Size([32, 1, 3, 3, 3])
encoder.stages.0.0.convs.0.conv.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.norm.weight shape torch.Size([32])
encoder.stages.0.0.convs.0.norm.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.0.weight shape torch.Size([32, 1, 3, 3, 3])
encoder.stages.0.0.convs.0.all_modules.0.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.1.weight shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.1.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
encoder.stages.0.0.convs.1.conv.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.norm.weight shape torch.Size([32])
encoder.stages.0.0.convs.1.norm.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
encoder.stages.0.0.convs.1.all_modules.0.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.1.weight shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.1.bias shape torch.Size([32])
encoder.stages.1.0.convs.0.conv.weight shape torch.Size([64, 32, 3, 3, 3])
encoder.stages.1.0.convs.0.conv.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.norm.weight shape torch.Size([64])
encoder.stages.1.0.convs.0.norm.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.0.weight shape torch.Size([64, 32, 3, 3, 3])
encoder.stages.1.0.convs.0.all_modules.0.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.1.weight shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.1.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
encoder.stages.1.0.convs.1.conv.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.norm.weight shape torch.Size([64])
encoder.stages.1.0.convs.1.norm.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
encoder.stages.1.0.convs.1.all_modules.0.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.1.weight shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.1.bias shape torch.Size([64])
encoder.stages.2.0.convs.0.conv.weight shape torch.Size([128, 64, 3, 3, 3])
encoder.stages.2.0.convs.0.conv.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.norm.weight shape torch.Size([128])
encoder.stages.2.0.convs.0.norm.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.0.weight shape torch.Size([128, 64, 3, 3, 3])
encoder.stages.2.0.convs.0.all_modules.0.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.1.weight shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.1.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
encoder.stages.2.0.convs.1.conv.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.norm.weight shape torch.Size([128])
encoder.stages.2.0.convs.1.norm.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
encoder.stages.2.0.convs.1.all_modules.0.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.1.weight shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.1.bias shape torch.Size([128])
encoder.stages.3.0.convs.0.conv.weight shape torch.Size([256, 128, 3, 3, 3])
encoder.stages.3.0.convs.0.conv.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.norm.weight shape torch.Size([256])
encoder.stages.3.0.convs.0.norm.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.0.weight shape torch.Size([256, 128, 3, 3, 3])
encoder.stages.3.0.convs.0.all_modules.0.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.1.weight shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.1.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
encoder.stages.3.0.convs.1.conv.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.norm.weight shape torch.Size([256])
encoder.stages.3.0.convs.1.norm.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
encoder.stages.3.0.convs.1.all_modules.0.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.1.weight shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.1.bias shape torch.Size([256])
encoder.stages.4.0.convs.0.conv.weight shape torch.Size([320, 256, 3, 3, 3])
encoder.stages.4.0.convs.0.conv.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.norm.weight shape torch.Size([320])
encoder.stages.4.0.convs.0.norm.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.0.weight shape torch.Size([320, 256, 3, 3, 3])
encoder.stages.4.0.convs.0.all_modules.0.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.1.weight shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.1.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.4.0.convs.1.conv.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.norm.weight shape torch.Size([320])
encoder.stages.4.0.convs.1.norm.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.4.0.convs.1.all_modules.0.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.1.weight shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.1.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.0.conv.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.norm.weight shape torch.Size([320])
encoder.stages.5.0.convs.0.norm.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.0.all_modules.0.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.1.weight shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.1.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.1.conv.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.norm.weight shape torch.Size([320])
encoder.stages.5.0.convs.1.norm.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.1.all_modules.0.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.1.weight shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.0.0.convs.0.conv.weight shape torch.Size([32, 1, 3, 3, 3])
decoder.encoder.stages.0.0.convs.0.conv.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.norm.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.norm.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.0.weight shape torch.Size([32, 1, 3, 3, 3])
decoder.encoder.stages.0.0.convs.0.all_modules.0.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.1.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.1.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.encoder.stages.0.0.convs.1.conv.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.norm.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.norm.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.encoder.stages.0.0.convs.1.all_modules.0.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.1.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.1.bias shape torch.Size([32])
decoder.encoder.stages.1.0.convs.0.conv.weight shape torch.Size([64, 32, 3, 3, 3])
decoder.encoder.stages.1.0.convs.0.conv.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.norm.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.norm.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.0.weight shape torch.Size([64, 32, 3, 3, 3])
decoder.encoder.stages.1.0.convs.0.all_modules.0.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.1.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.1.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.encoder.stages.1.0.convs.1.conv.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.norm.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.norm.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.encoder.stages.1.0.convs.1.all_modules.0.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.1.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.1.bias shape torch.Size([64])
decoder.encoder.stages.2.0.convs.0.conv.weight shape torch.Size([128, 64, 3, 3, 3])
decoder.encoder.stages.2.0.convs.0.conv.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.norm.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.norm.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.0.weight shape torch.Size([128, 64, 3, 3, 3])
decoder.encoder.stages.2.0.convs.0.all_modules.0.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.1.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.1.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.encoder.stages.2.0.convs.1.conv.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.norm.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.norm.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.encoder.stages.2.0.convs.1.all_modules.0.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.1.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.1.bias shape torch.Size([128])
decoder.encoder.stages.3.0.convs.0.conv.weight shape torch.Size([256, 128, 3, 3, 3])
decoder.encoder.stages.3.0.convs.0.conv.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.norm.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.norm.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.0.weight shape torch.Size([256, 128, 3, 3, 3])
decoder.encoder.stages.3.0.convs.0.all_modules.0.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.1.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.1.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.encoder.stages.3.0.convs.1.conv.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.norm.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.norm.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.encoder.stages.3.0.convs.1.all_modules.0.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.1.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.1.bias shape torch.Size([256])
decoder.encoder.stages.4.0.convs.0.conv.weight shape torch.Size([320, 256, 3, 3, 3])
decoder.encoder.stages.4.0.convs.0.conv.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.norm.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.norm.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.0.weight shape torch.Size([320, 256, 3, 3, 3])
decoder.encoder.stages.4.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.4.0.convs.1.conv.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.norm.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.norm.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.4.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.0.conv.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.norm.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.norm.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.1.conv.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.norm.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.norm.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.stages.0.convs.0.conv.weight shape torch.Size([320, 640, 3, 3, 3])
decoder.stages.0.convs.0.conv.bias shape torch.Size([320])
decoder.stages.0.convs.0.norm.weight shape torch.Size([320])
decoder.stages.0.convs.0.norm.bias shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.0.weight shape torch.Size([320, 640, 3, 3, 3])
decoder.stages.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.stages.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.stages.0.convs.1.conv.bias shape torch.Size([320])
decoder.stages.0.convs.1.norm.weight shape torch.Size([320])
decoder.stages.0.convs.1.norm.bias shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.stages.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.stages.1.convs.0.conv.weight shape torch.Size([256, 512, 3, 3, 3])
decoder.stages.1.convs.0.conv.bias shape torch.Size([256])
decoder.stages.1.convs.0.norm.weight shape torch.Size([256])
decoder.stages.1.convs.0.norm.bias shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.0.weight shape torch.Size([256, 512, 3, 3, 3])
decoder.stages.1.convs.0.all_modules.0.bias shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.1.weight shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.1.bias shape torch.Size([256])
decoder.stages.1.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.stages.1.convs.1.conv.bias shape torch.Size([256])
decoder.stages.1.convs.1.norm.weight shape torch.Size([256])
decoder.stages.1.convs.1.norm.bias shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.stages.1.convs.1.all_modules.0.bias shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.1.weight shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.1.bias shape torch.Size([256])
decoder.stages.2.convs.0.conv.weight shape torch.Size([128, 256, 3, 3, 3])
decoder.stages.2.convs.0.conv.bias shape torch.Size([128])
decoder.stages.2.convs.0.norm.weight shape torch.Size([128])
decoder.stages.2.convs.0.norm.bias shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.0.weight shape torch.Size([128, 256, 3, 3, 3])
decoder.stages.2.convs.0.all_modules.0.bias shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.1.weight shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.1.bias shape torch.Size([128])
decoder.stages.2.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.stages.2.convs.1.conv.bias shape torch.Size([128])
decoder.stages.2.convs.1.norm.weight shape torch.Size([128])
decoder.stages.2.convs.1.norm.bias shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.stages.2.convs.1.all_modules.0.bias shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.1.weight shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.1.bias shape torch.Size([128])
decoder.stages.3.convs.0.conv.weight shape torch.Size([64, 128, 3, 3, 3])
decoder.stages.3.convs.0.conv.bias shape torch.Size([64])
decoder.stages.3.convs.0.norm.weight shape torch.Size([64])
decoder.stages.3.convs.0.norm.bias shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.0.weight shape torch.Size([64, 128, 3, 3, 3])
decoder.stages.3.convs.0.all_modules.0.bias shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.1.weight shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.1.bias shape torch.Size([64])
decoder.stages.3.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.stages.3.convs.1.conv.bias shape torch.Size([64])
decoder.stages.3.convs.1.norm.weight shape torch.Size([64])
decoder.stages.3.convs.1.norm.bias shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.stages.3.convs.1.all_modules.0.bias shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.1.weight shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.1.bias shape torch.Size([64])
decoder.stages.4.convs.0.conv.weight shape torch.Size([32, 64, 3, 3, 3])
decoder.stages.4.convs.0.conv.bias shape torch.Size([32])
decoder.stages.4.convs.0.norm.weight shape torch.Size([32])
decoder.stages.4.convs.0.norm.bias shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.0.weight shape torch.Size([32, 64, 3, 3, 3])
decoder.stages.4.convs.0.all_modules.0.bias shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.1.weight shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.1.bias shape torch.Size([32])
decoder.stages.4.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.stages.4.convs.1.conv.bias shape torch.Size([32])
decoder.stages.4.convs.1.norm.weight shape torch.Size([32])
decoder.stages.4.convs.1.norm.bias shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.stages.4.convs.1.all_modules.0.bias shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.1.weight shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.1.bias shape torch.Size([32])
decoder.transpconvs.0.weight shape torch.Size([320, 320, 1, 2, 2])
decoder.transpconvs.0.bias shape torch.Size([320])
decoder.transpconvs.1.weight shape torch.Size([320, 256, 2, 2, 2])
decoder.transpconvs.1.bias shape torch.Size([256])
decoder.transpconvs.2.weight shape torch.Size([256, 128, 2, 2, 2])
decoder.transpconvs.2.bias shape torch.Size([128])
decoder.transpconvs.3.weight shape torch.Size([128, 64, 2, 2, 2])
decoder.transpconvs.3.bias shape torch.Size([64])
decoder.transpconvs.4.weight shape torch.Size([64, 32, 2, 2, 2])
decoder.transpconvs.4.bias shape torch.Size([32])
################### Done ###################
2024-12-17 21:20:50.365522: do_dummy_2d_data_aug: True
2024-12-17 21:20:50.400772: Using splits from existing split file: /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset309_Sohee_Calcium_OCT_CrossValidation/splits_final_40.json
2024-12-17 21:20:50.421413: The split file contains 5 splits.
2024-12-17 21:20:50.423525: Desired fold for training: 1
2024-12-17 21:20:50.424862: This split has 3 training and 6 validation cases.
################### Loading pretrained weights from file  /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_results/Dataset300_Lumen_and_Wall_OCT/nnUNetTrainer__nnUNetPreprocessPlans__3d_fullres/fold_all/checkpoint_best.pth ###################
Below is the list of overlapping blocks in pretrained model and nnUNet architecture:
encoder.stages.0.0.convs.0.conv.weight shape torch.Size([32, 1, 3, 3, 3])
encoder.stages.0.0.convs.0.conv.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.norm.weight shape torch.Size([32])
encoder.stages.0.0.convs.0.norm.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.0.weight shape torch.Size([32, 1, 3, 3, 3])
encoder.stages.0.0.convs.0.all_modules.0.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.1.weight shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.1.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
encoder.stages.0.0.convs.1.conv.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.norm.weight shape torch.Size([32])
encoder.stages.0.0.convs.1.norm.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
encoder.stages.0.0.convs.1.all_modules.0.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.1.weight shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.1.bias shape torch.Size([32])
encoder.stages.1.0.convs.0.conv.weight shape torch.Size([64, 32, 3, 3, 3])
encoder.stages.1.0.convs.0.conv.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.norm.weight shape torch.Size([64])
encoder.stages.1.0.convs.0.norm.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.0.weight shape torch.Size([64, 32, 3, 3, 3])
encoder.stages.1.0.convs.0.all_modules.0.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.1.weight shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.1.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
encoder.stages.1.0.convs.1.conv.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.norm.weight shape torch.Size([64])
encoder.stages.1.0.convs.1.norm.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
encoder.stages.1.0.convs.1.all_modules.0.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.1.weight shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.1.bias shape torch.Size([64])
encoder.stages.2.0.convs.0.conv.weight shape torch.Size([128, 64, 3, 3, 3])
encoder.stages.2.0.convs.0.conv.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.norm.weight shape torch.Size([128])
encoder.stages.2.0.convs.0.norm.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.0.weight shape torch.Size([128, 64, 3, 3, 3])
encoder.stages.2.0.convs.0.all_modules.0.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.1.weight shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.1.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
encoder.stages.2.0.convs.1.conv.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.norm.weight shape torch.Size([128])
encoder.stages.2.0.convs.1.norm.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
encoder.stages.2.0.convs.1.all_modules.0.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.1.weight shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.1.bias shape torch.Size([128])
encoder.stages.3.0.convs.0.conv.weight shape torch.Size([256, 128, 3, 3, 3])
encoder.stages.3.0.convs.0.conv.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.norm.weight shape torch.Size([256])
encoder.stages.3.0.convs.0.norm.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.0.weight shape torch.Size([256, 128, 3, 3, 3])
encoder.stages.3.0.convs.0.all_modules.0.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.1.weight shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.1.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
encoder.stages.3.0.convs.1.conv.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.norm.weight shape torch.Size([256])
encoder.stages.3.0.convs.1.norm.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
encoder.stages.3.0.convs.1.all_modules.0.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.1.weight shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.1.bias shape torch.Size([256])
encoder.stages.4.0.convs.0.conv.weight shape torch.Size([320, 256, 3, 3, 3])
encoder.stages.4.0.convs.0.conv.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.norm.weight shape torch.Size([320])
encoder.stages.4.0.convs.0.norm.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.0.weight shape torch.Size([320, 256, 3, 3, 3])
encoder.stages.4.0.convs.0.all_modules.0.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.1.weight shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.1.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.4.0.convs.1.conv.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.norm.weight shape torch.Size([320])
encoder.stages.4.0.convs.1.norm.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.4.0.convs.1.all_modules.0.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.1.weight shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.1.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.0.conv.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.norm.weight shape torch.Size([320])
encoder.stages.5.0.convs.0.norm.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.0.all_modules.0.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.1.weight shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.1.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.1.conv.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.norm.weight shape torch.Size([320])
encoder.stages.5.0.convs.1.norm.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.1.all_modules.0.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.1.weight shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.0.0.convs.0.conv.weight shape torch.Size([32, 1, 3, 3, 3])
decoder.encoder.stages.0.0.convs.0.conv.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.norm.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.norm.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.0.weight shape torch.Size([32, 1, 3, 3, 3])
decoder.encoder.stages.0.0.convs.0.all_modules.0.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.1.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.1.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.encoder.stages.0.0.convs.1.conv.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.norm.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.norm.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.encoder.stages.0.0.convs.1.all_modules.0.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.1.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.1.bias shape torch.Size([32])
decoder.encoder.stages.1.0.convs.0.conv.weight shape torch.Size([64, 32, 3, 3, 3])
decoder.encoder.stages.1.0.convs.0.conv.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.norm.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.norm.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.0.weight shape torch.Size([64, 32, 3, 3, 3])
decoder.encoder.stages.1.0.convs.0.all_modules.0.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.1.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.1.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.encoder.stages.1.0.convs.1.conv.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.norm.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.norm.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.encoder.stages.1.0.convs.1.all_modules.0.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.1.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.1.bias shape torch.Size([64])
decoder.encoder.stages.2.0.convs.0.conv.weight shape torch.Size([128, 64, 3, 3, 3])
decoder.encoder.stages.2.0.convs.0.conv.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.norm.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.norm.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.0.weight shape torch.Size([128, 64, 3, 3, 3])
decoder.encoder.stages.2.0.convs.0.all_modules.0.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.1.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.1.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.encoder.stages.2.0.convs.1.conv.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.norm.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.norm.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.encoder.stages.2.0.convs.1.all_modules.0.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.1.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.1.bias shape torch.Size([128])
decoder.encoder.stages.3.0.convs.0.conv.weight shape torch.Size([256, 128, 3, 3, 3])
decoder.encoder.stages.3.0.convs.0.conv.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.norm.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.norm.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.0.weight shape torch.Size([256, 128, 3, 3, 3])
decoder.encoder.stages.3.0.convs.0.all_modules.0.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.1.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.1.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.encoder.stages.3.0.convs.1.conv.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.norm.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.norm.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.encoder.stages.3.0.convs.1.all_modules.0.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.1.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.1.bias shape torch.Size([256])
decoder.encoder.stages.4.0.convs.0.conv.weight shape torch.Size([320, 256, 3, 3, 3])
decoder.encoder.stages.4.0.convs.0.conv.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.norm.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.norm.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.0.weight shape torch.Size([320, 256, 3, 3, 3])
decoder.encoder.stages.4.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.4.0.convs.1.conv.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.norm.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.norm.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.4.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.0.conv.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.norm.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.norm.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.1.conv.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.norm.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.norm.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.stages.0.convs.0.conv.weight shape torch.Size([320, 640, 3, 3, 3])
decoder.stages.0.convs.0.conv.bias shape torch.Size([320])
decoder.stages.0.convs.0.norm.weight shape torch.Size([320])
decoder.stages.0.convs.0.norm.bias shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.0.weight shape torch.Size([320, 640, 3, 3, 3])
decoder.stages.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.stages.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.stages.0.convs.1.conv.bias shape torch.Size([320])
decoder.stages.0.convs.1.norm.weight shape torch.Size([320])
decoder.stages.0.convs.1.norm.bias shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.stages.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.stages.1.convs.0.conv.weight shape torch.Size([256, 512, 3, 3, 3])
decoder.stages.1.convs.0.conv.bias shape torch.Size([256])
decoder.stages.1.convs.0.norm.weight shape torch.Size([256])
decoder.stages.1.convs.0.norm.bias shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.0.weight shape torch.Size([256, 512, 3, 3, 3])
decoder.stages.1.convs.0.all_modules.0.bias shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.1.weight shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.1.bias shape torch.Size([256])
decoder.stages.1.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.stages.1.convs.1.conv.bias shape torch.Size([256])
decoder.stages.1.convs.1.norm.weight shape torch.Size([256])
decoder.stages.1.convs.1.norm.bias shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.stages.1.convs.1.all_modules.0.bias shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.1.weight shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.1.bias shape torch.Size([256])
decoder.stages.2.convs.0.conv.weight shape torch.Size([128, 256, 3, 3, 3])
decoder.stages.2.convs.0.conv.bias shape torch.Size([128])
decoder.stages.2.convs.0.norm.weight shape torch.Size([128])
decoder.stages.2.convs.0.norm.bias shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.0.weight shape torch.Size([128, 256, 3, 3, 3])
decoder.stages.2.convs.0.all_modules.0.bias shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.1.weight shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.1.bias shape torch.Size([128])
decoder.stages.2.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.stages.2.convs.1.conv.bias shape torch.Size([128])
decoder.stages.2.convs.1.norm.weight shape torch.Size([128])
decoder.stages.2.convs.1.norm.bias shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.stages.2.convs.1.all_modules.0.bias shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.1.weight shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.1.bias shape torch.Size([128])
decoder.stages.3.convs.0.conv.weight shape torch.Size([64, 128, 3, 3, 3])
decoder.stages.3.convs.0.conv.bias shape torch.Size([64])
decoder.stages.3.convs.0.norm.weight shape torch.Size([64])
decoder.stages.3.convs.0.norm.bias shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.0.weight shape torch.Size([64, 128, 3, 3, 3])
decoder.stages.3.convs.0.all_modules.0.bias shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.1.weight shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.1.bias shape torch.Size([64])
decoder.stages.3.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.stages.3.convs.1.conv.bias shape torch.Size([64])
decoder.stages.3.convs.1.norm.weight shape torch.Size([64])
decoder.stages.3.convs.1.norm.bias shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.stages.3.convs.1.all_modules.0.bias shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.1.weight shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.1.bias shape torch.Size([64])
decoder.stages.4.convs.0.conv.weight shape torch.Size([32, 64, 3, 3, 3])
decoder.stages.4.convs.0.conv.bias shape torch.Size([32])
decoder.stages.4.convs.0.norm.weight shape torch.Size([32])
decoder.stages.4.convs.0.norm.bias shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.0.weight shape torch.Size([32, 64, 3, 3, 3])
decoder.stages.4.convs.0.all_modules.0.bias shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.1.weight shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.1.bias shape torch.Size([32])
decoder.stages.4.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.stages.4.convs.1.conv.bias shape torch.Size([32])
decoder.stages.4.convs.1.norm.weight shape torch.Size([32])
decoder.stages.4.convs.1.norm.bias shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.stages.4.convs.1.all_modules.0.bias shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.1.weight shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.1.bias shape torch.Size([32])
decoder.transpconvs.0.weight shape torch.Size([320, 320, 1, 2, 2])
decoder.transpconvs.0.bias shape torch.Size([320])
decoder.transpconvs.1.weight shape torch.Size([320, 256, 2, 2, 2])
decoder.transpconvs.1.bias shape torch.Size([256])
decoder.transpconvs.2.weight shape torch.Size([256, 128, 2, 2, 2])
decoder.transpconvs.2.bias shape torch.Size([128])
decoder.transpconvs.3.weight shape torch.Size([128, 64, 2, 2, 2])
decoder.transpconvs.3.bias shape torch.Size([64])
decoder.transpconvs.4.weight shape torch.Size([64, 32, 2, 2, 2])
decoder.transpconvs.4.bias shape torch.Size([32])
################### Done ###################
2024-12-17 21:20:50.365511: do_dummy_2d_data_aug: True
2024-12-17 21:20:50.400718: Using splits from existing split file: /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset309_Sohee_Calcium_OCT_CrossValidation/splits_final_40.json
2024-12-17 21:20:50.422021: The split file contains 5 splits.
2024-12-17 21:20:50.423663: Desired fold for training: 0
2024-12-17 21:20:50.424876: This split has 3 training and 5 validation cases.
using pin_memory on device 0
using pin_memory on device 0
using pin_memory on device 0

This is the configuration used by this training:
Configuration name: 3d_32x160x128_b10
 {'data_identifier': 'nnUNetPlans_3d_fullres', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 10, 'patch_size': [32, 160, 128], 'median_image_size_in_voxels': [375.0, 498.0, 498.0], 'spacing': [1.0, 1.0, 1.0], 'normalization_schemes': ['ZScoreNormalization'], 'use_mask_for_norm': [False], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'architecture': {'network_class_name': 'dynamic_network_architectures.architectures.unet.PlainConvUNet', 'arch_kwargs': {'n_stages': 6, 'features_per_stage': [32, 64, 128, 256, 320, 320], 'conv_op': 'torch.nn.modules.conv.Conv3d', 'kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]], 'strides': [[1, 1, 1], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [1, 2, 2]], 'n_conv_per_stage': [2, 2, 2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2, 2, 2], 'conv_bias': True, 'norm_op': 'torch.nn.modules.instancenorm.InstanceNorm3d', 'norm_op_kwargs': {'eps': 1e-05, 'affine': True}, 'dropout_op': None, 'dropout_op_kwargs': None, 'nonlin': 'torch.nn.LeakyReLU', 'nonlin_kwargs': {'inplace': True}, 'deep_supervision': True}, '_kw_requires_import': ['conv_op', 'norm_op', 'dropout_op', 'nonlin']}, 'batch_dice': True, 'inherits_from': '3d_fullres'} 

These are the global plan.json settings:
 {'dataset_name': 'Dataset309_Sohee_Calcium_OCT_CrossValidation', 'plans_name': 'nnUNetPlans', 'original_median_spacing_after_transp': [1.0, 1.0, 1.0], 'original_median_shape_after_transp': [375, 498, 498], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 0.4977976381778717, 'mean': 0.13507525622844696, 'median': 0.09599608182907104, 'min': 0.0, 'percentile_00_5': 0.014754901640117168, 'percentile_99_5': 0.4977976381778717, 'std': 0.12025152146816254}}} 

2024-12-17 21:21:17.496961: unpacking dataset...
using pin_memory on device 0

This is the configuration used by this training:
Configuration name: 3d_32x160x128_b10
 {'data_identifier': 'nnUNetPlans_3d_fullres', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 10, 'patch_size': [32, 160, 128], 'median_image_size_in_voxels': [375.0, 498.0, 498.0], 'spacing': [1.0, 1.0, 1.0], 'normalization_schemes': ['ZScoreNormalization'], 'use_mask_for_norm': [False], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'architecture': {'network_class_name': 'dynamic_network_architectures.architectures.unet.PlainConvUNet', 'arch_kwargs': {'n_stages': 6, 'features_per_stage': [32, 64, 128, 256, 320, 320], 'conv_op': 'torch.nn.modules.conv.Conv3d', 'kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]], 'strides': [[1, 1, 1], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [1, 2, 2]], 'n_conv_per_stage': [2, 2, 2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2, 2, 2], 'conv_bias': True, 'norm_op': 'torch.nn.modules.instancenorm.InstanceNorm3d', 'norm_op_kwargs': {'eps': 1e-05, 'affine': True}, 'dropout_op': None, 'dropout_op_kwargs': None, 'nonlin': 'torch.nn.LeakyReLU', 'nonlin_kwargs': {'inplace': True}, 'deep_supervision': True}, '_kw_requires_import': ['conv_op', 'norm_op', 'dropout_op', 'nonlin']}, 'batch_dice': True, 'inherits_from': '3d_fullres'} 

These are the global plan.json settings:
 {'dataset_name': 'Dataset309_Sohee_Calcium_OCT_CrossValidation', 'plans_name': 'nnUNetPlans', 'original_median_spacing_after_transp': [1.0, 1.0, 1.0], 'original_median_shape_after_transp': [375, 498, 498], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 0.4977976381778717, 'mean': 0.13507525622844696, 'median': 0.09599608182907104, 'min': 0.0, 'percentile_00_5': 0.014754901640117168, 'percentile_99_5': 0.4977976381778717, 'std': 0.12025152146816254}}} 

2024-12-17 21:21:17.584651: unpacking dataset...
2024-12-17 21:21:22.577627: unpacking done...
2024-12-17 21:21:22.823316: Unable to plot network architecture: nnUNet_compile is enabled!
2024-12-17 21:21:23.021122: 
2024-12-17 21:21:23.022591: Epoch 0
2024-12-17 21:21:23.023853: Current learning rate: 0.01
2024-12-17 21:28:54.035745: Validation loss improved from 1000.00000 to -0.39178! Patience: 0/50
2024-12-17 21:28:54.036702: train_loss -0.3028
2024-12-17 21:28:54.037781: val_loss -0.3918
2024-12-17 21:28:54.038463: Pseudo dice [0.6714]
2024-12-17 21:28:54.039227: Epoch time: 451.02 s
2024-12-17 21:28:54.039955: Yayy! New best EMA pseudo Dice: 0.6714
2024-12-17 21:28:56.132692: 
2024-12-17 21:28:56.134045: Epoch 1
2024-12-17 21:28:56.135061: Current learning rate: 0.00994
2024-12-17 21:34:13.320116: Validation loss improved from -0.39178 to -0.45283! Patience: 0/50
2024-12-17 21:34:13.321309: train_loss -0.4882
2024-12-17 21:34:13.322474: val_loss -0.4528
2024-12-17 21:34:13.323369: Pseudo dice [0.7036]
2024-12-17 21:34:13.324235: Epoch time: 317.19 s
2024-12-17 21:34:13.325021: Yayy! New best EMA pseudo Dice: 0.6746
2024-12-17 21:34:15.130250: 
2024-12-17 21:34:15.131618: Epoch 2
2024-12-17 21:34:15.132714: Current learning rate: 0.00988
2024-12-17 21:40:34.704856: Validation loss did not improve from -0.45283. Patience: 1/50
2024-12-17 21:40:34.706156: train_loss -0.5546
2024-12-17 21:40:34.707342: val_loss -0.4361
2024-12-17 21:40:34.708239: Pseudo dice [0.6997]
2024-12-17 21:40:34.708984: Epoch time: 379.58 s
2024-12-17 21:40:34.709637: Yayy! New best EMA pseudo Dice: 0.6771
2024-12-17 21:40:36.755592: 
2024-12-17 21:40:36.757631: Epoch 3
2024-12-17 21:40:36.758843: Current learning rate: 0.00982
2024-12-17 21:47:21.076290: Validation loss did not improve from -0.45283. Patience: 2/50
2024-12-17 21:47:21.077535: train_loss -0.5764
2024-12-17 21:47:21.078314: val_loss -0.4396
2024-12-17 21:47:21.078995: Pseudo dice [0.6979]
2024-12-17 21:47:21.079797: Epoch time: 404.32 s
2024-12-17 21:47:21.080494: Yayy! New best EMA pseudo Dice: 0.6792
2024-12-17 21:47:23.024930: 
2024-12-17 21:47:23.025891: Epoch 4
2024-12-17 21:47:23.026636: Current learning rate: 0.00976
2024-12-17 21:53:57.723433: Validation loss did not improve from -0.45283. Patience: 3/50
2024-12-17 21:53:57.724385: train_loss -0.6093
2024-12-17 21:53:57.725317: val_loss -0.4297
2024-12-17 21:53:57.726103: Pseudo dice [0.7049]
2024-12-17 21:53:57.726850: Epoch time: 394.7 s
2024-12-17 21:53:58.105432: Yayy! New best EMA pseudo Dice: 0.6818
2024-12-17 21:53:59.975404: 
2024-12-17 21:53:59.976396: Epoch 5
2024-12-17 21:53:59.977112: Current learning rate: 0.0097
2024-12-17 22:00:56.588339: Validation loss improved from -0.45283 to -0.45559! Patience: 3/50
2024-12-17 22:00:56.589350: train_loss -0.6207
2024-12-17 22:00:56.590097: val_loss -0.4556
2024-12-17 22:00:56.590835: Pseudo dice [0.703]
2024-12-17 22:00:56.591561: Epoch time: 416.62 s
2024-12-17 22:00:56.592315: Yayy! New best EMA pseudo Dice: 0.6839
2024-12-17 22:00:58.429498: 
2024-12-17 22:00:58.431045: Epoch 6
2024-12-17 22:00:58.431964: Current learning rate: 0.00964
2024-12-17 22:08:53.218863: Validation loss improved from -0.45559 to -0.47745! Patience: 0/50
2024-12-17 22:08:53.219995: train_loss -0.6419
2024-12-17 22:08:53.220977: val_loss -0.4774
2024-12-17 22:08:53.221895: Pseudo dice [0.7232]
2024-12-17 22:08:53.222833: Epoch time: 474.79 s
2024-12-17 22:08:53.223623: Yayy! New best EMA pseudo Dice: 0.6878
2024-12-17 22:08:55.052501: 
2024-12-17 22:08:55.053525: Epoch 7
2024-12-17 22:08:55.054355: Current learning rate: 0.00958
2024-12-17 22:17:08.315768: Validation loss improved from -0.47745 to -0.49209! Patience: 0/50
2024-12-17 22:17:08.316775: train_loss -0.6529
2024-12-17 22:17:08.317632: val_loss -0.4921
2024-12-17 22:17:08.318406: Pseudo dice [0.7282]
2024-12-17 22:17:08.319246: Epoch time: 493.27 s
2024-12-17 22:17:08.320053: Yayy! New best EMA pseudo Dice: 0.6919
2024-12-17 22:17:10.173138: 
2024-12-17 22:17:10.174613: Epoch 8
2024-12-17 22:17:10.175672: Current learning rate: 0.00952
2024-12-17 22:25:13.013948: Validation loss did not improve from -0.49209. Patience: 1/50
2024-12-17 22:25:13.014724: train_loss -0.6545
2024-12-17 22:25:13.015762: val_loss -0.4771
2024-12-17 22:25:13.016650: Pseudo dice [0.7186]
2024-12-17 22:25:13.017616: Epoch time: 482.84 s
2024-12-17 22:25:13.018687: Yayy! New best EMA pseudo Dice: 0.6945
2024-12-17 22:25:15.764935: 
2024-12-17 22:25:15.766642: Epoch 9
2024-12-17 22:25:15.767936: Current learning rate: 0.00946
2024-12-17 22:33:15.852123: Validation loss did not improve from -0.49209. Patience: 2/50
2024-12-17 22:33:15.874383: train_loss -0.6693
2024-12-17 22:33:15.875538: val_loss -0.4523
2024-12-17 22:33:15.876422: Pseudo dice [0.7185]
2024-12-17 22:33:15.877258: Epoch time: 480.11 s
2024-12-17 22:33:16.276350: Yayy! New best EMA pseudo Dice: 0.6969
2024-12-17 22:33:18.103722: 
2024-12-17 22:33:18.104850: Epoch 10
2024-12-17 22:33:18.105737: Current learning rate: 0.0094
2024-12-17 22:41:21.767303: Validation loss did not improve from -0.49209. Patience: 3/50
2024-12-17 22:41:21.768228: train_loss -0.6894
2024-12-17 22:41:21.770134: val_loss -0.4726
2024-12-17 22:41:21.771168: Pseudo dice [0.71]
2024-12-17 22:41:21.772183: Epoch time: 483.67 s
2024-12-17 22:41:21.772959: Yayy! New best EMA pseudo Dice: 0.6982
2024-12-17 22:41:23.772105: 
2024-12-17 22:41:23.773629: Epoch 11
2024-12-17 22:41:23.774609: Current learning rate: 0.00934
2024-12-17 22:49:27.782306: Validation loss did not improve from -0.49209. Patience: 4/50
2024-12-17 22:49:27.783313: train_loss -0.6936
2024-12-17 22:49:27.784162: val_loss -0.4638
2024-12-17 22:49:27.785031: Pseudo dice [0.7026]
2024-12-17 22:49:27.785806: Epoch time: 484.01 s
2024-12-17 22:49:27.786452: Yayy! New best EMA pseudo Dice: 0.6987
2024-12-17 22:49:29.695626: 
2024-12-17 22:49:29.696950: Epoch 12
2024-12-17 22:49:29.697718: Current learning rate: 0.00928
2024-12-17 22:57:47.978377: Validation loss improved from -0.49209 to -0.49733! Patience: 4/50
2024-12-17 22:57:47.979549: train_loss -0.6944
2024-12-17 22:57:47.980568: val_loss -0.4973
2024-12-17 22:57:47.981369: Pseudo dice [0.7222]
2024-12-17 22:57:47.982267: Epoch time: 498.29 s
2024-12-17 22:57:47.983035: Yayy! New best EMA pseudo Dice: 0.701
2024-12-17 22:57:49.986102: 
2024-12-17 22:57:49.987521: Epoch 13
2024-12-17 22:57:49.988351: Current learning rate: 0.00922
2024-12-17 23:06:08.616955: Validation loss did not improve from -0.49733. Patience: 1/50
2024-12-17 23:06:08.617892: train_loss -0.7091
2024-12-17 23:06:08.618697: val_loss -0.4398
2024-12-17 23:06:08.619506: Pseudo dice [0.6995]
2024-12-17 23:06:08.620345: Epoch time: 498.63 s
2024-12-17 23:06:10.139802: 
2024-12-17 23:06:10.141402: Epoch 14
2024-12-17 23:06:10.142383: Current learning rate: 0.00916
2024-12-17 23:14:34.119318: Validation loss did not improve from -0.49733. Patience: 2/50
2024-12-17 23:14:34.120316: train_loss -0.7154
2024-12-17 23:14:34.121285: val_loss -0.4858
2024-12-17 23:14:34.122131: Pseudo dice [0.7237]
2024-12-17 23:14:34.123100: Epoch time: 503.98 s
2024-12-17 23:14:34.561474: Yayy! New best EMA pseudo Dice: 0.7032
2024-12-17 23:14:36.451771: 
2024-12-17 23:14:36.453194: Epoch 15
2024-12-17 23:14:36.454082: Current learning rate: 0.0091
2024-12-17 23:22:40.164403: Validation loss did not improve from -0.49733. Patience: 3/50
2024-12-17 23:22:40.167302: train_loss -0.717
2024-12-17 23:22:40.169191: val_loss -0.4521
2024-12-17 23:22:40.170198: Pseudo dice [0.7159]
2024-12-17 23:22:40.171379: Epoch time: 483.72 s
2024-12-17 23:22:40.172504: Yayy! New best EMA pseudo Dice: 0.7044
2024-12-17 23:22:42.147340: 
2024-12-17 23:22:42.148837: Epoch 16
2024-12-17 23:22:42.149843: Current learning rate: 0.00903
2024-12-17 23:30:29.213649: Validation loss did not improve from -0.49733. Patience: 4/50
2024-12-17 23:30:29.214680: train_loss -0.7189
2024-12-17 23:30:29.215641: val_loss -0.4937
2024-12-17 23:30:29.216366: Pseudo dice [0.7319]
2024-12-17 23:30:29.217105: Epoch time: 467.07 s
2024-12-17 23:30:29.217813: Yayy! New best EMA pseudo Dice: 0.7072
2024-12-17 23:30:31.115324: 
2024-12-17 23:30:31.116736: Epoch 17
2024-12-17 23:30:31.117459: Current learning rate: 0.00897
2024-12-17 23:37:57.265574: Validation loss did not improve from -0.49733. Patience: 5/50
2024-12-17 23:37:57.266533: train_loss -0.7291
2024-12-17 23:37:57.267485: val_loss -0.4908
2024-12-17 23:37:57.268254: Pseudo dice [0.72]
2024-12-17 23:37:57.268973: Epoch time: 446.15 s
2024-12-17 23:37:57.269830: Yayy! New best EMA pseudo Dice: 0.7085
2024-12-17 23:37:59.078288: 
2024-12-17 23:37:59.079656: Epoch 18
2024-12-17 23:37:59.080512: Current learning rate: 0.00891
2024-12-17 23:46:03.874135: Validation loss improved from -0.49733 to -0.50019! Patience: 5/50
2024-12-17 23:46:03.875342: train_loss -0.7321
2024-12-17 23:46:03.876429: val_loss -0.5002
2024-12-17 23:46:03.877353: Pseudo dice [0.734]
2024-12-17 23:46:03.878305: Epoch time: 484.8 s
2024-12-17 23:46:03.879294: Yayy! New best EMA pseudo Dice: 0.711
2024-12-17 23:46:06.510571: 
2024-12-17 23:46:06.512160: Epoch 19
2024-12-17 23:46:06.513132: Current learning rate: 0.00885
2024-12-17 23:54:40.008446: Validation loss did not improve from -0.50019. Patience: 1/50
2024-12-17 23:54:40.009422: train_loss -0.736
2024-12-17 23:54:40.010267: val_loss -0.4528
2024-12-17 23:54:40.011155: Pseudo dice [0.7125]
2024-12-17 23:54:40.011871: Epoch time: 513.5 s
2024-12-17 23:54:40.363387: Yayy! New best EMA pseudo Dice: 0.7112
2024-12-17 23:54:42.222238: 
2024-12-17 23:54:42.223524: Epoch 20
2024-12-17 23:54:42.224279: Current learning rate: 0.00879
2024-12-18 00:03:03.530398: Validation loss did not improve from -0.50019. Patience: 2/50
2024-12-18 00:03:03.531523: train_loss -0.7298
2024-12-18 00:03:03.532451: val_loss -0.4493
2024-12-18 00:03:03.533269: Pseudo dice [0.7033]
2024-12-18 00:03:03.534024: Epoch time: 501.31 s
2024-12-18 00:03:05.054678: 
2024-12-18 00:03:05.056365: Epoch 21
2024-12-18 00:03:05.057306: Current learning rate: 0.00873
2024-12-18 00:10:53.395034: Validation loss did not improve from -0.50019. Patience: 3/50
2024-12-18 00:10:53.395984: train_loss -0.7386
2024-12-18 00:10:53.396881: val_loss -0.4915
2024-12-18 00:10:53.397532: Pseudo dice [0.7358]
2024-12-18 00:10:53.398406: Epoch time: 468.34 s
2024-12-18 00:10:53.399082: Yayy! New best EMA pseudo Dice: 0.7129
2024-12-18 00:10:55.205195: 
2024-12-18 00:10:55.206636: Epoch 22
2024-12-18 00:10:55.207558: Current learning rate: 0.00867
2024-12-18 00:19:12.182194: Validation loss did not improve from -0.50019. Patience: 4/50
2024-12-18 00:19:12.183345: train_loss -0.7452
2024-12-18 00:19:12.184111: val_loss -0.4435
2024-12-18 00:19:12.184837: Pseudo dice [0.7019]
2024-12-18 00:19:12.185579: Epoch time: 496.98 s
2024-12-18 00:19:13.613708: 
2024-12-18 00:19:13.615198: Epoch 23
2024-12-18 00:19:13.616178: Current learning rate: 0.00861
2024-12-18 00:27:34.606099: Validation loss did not improve from -0.50019. Patience: 5/50
2024-12-18 00:27:34.609385: train_loss -0.7403
2024-12-18 00:27:34.611330: val_loss -0.4787
2024-12-18 00:27:34.612127: Pseudo dice [0.7179]
2024-12-18 00:27:34.613081: Epoch time: 501.0 s
2024-12-18 00:27:36.083685: 
2024-12-18 00:27:36.085064: Epoch 24
2024-12-18 00:27:36.085896: Current learning rate: 0.00855
2024-12-18 00:35:27.831497: Validation loss did not improve from -0.50019. Patience: 6/50
2024-12-18 00:35:27.832444: train_loss -0.749
2024-12-18 00:35:27.833243: val_loss -0.4336
2024-12-18 00:35:27.833932: Pseudo dice [0.6954]
2024-12-18 00:35:27.834642: Epoch time: 471.75 s
2024-12-18 00:35:29.839065: 
2024-12-18 00:35:29.840513: Epoch 25
2024-12-18 00:35:29.841506: Current learning rate: 0.00849
2024-12-18 00:43:17.456917: Validation loss did not improve from -0.50019. Patience: 7/50
2024-12-18 00:43:17.458017: train_loss -0.7542
2024-12-18 00:43:17.459159: val_loss -0.4828
2024-12-18 00:43:17.460042: Pseudo dice [0.7299]
2024-12-18 00:43:17.460879: Epoch time: 467.62 s
2024-12-18 00:43:18.940912: 
2024-12-18 00:43:18.942171: Epoch 26
2024-12-18 00:43:18.942962: Current learning rate: 0.00843
2024-12-18 00:51:14.269155: Validation loss did not improve from -0.50019. Patience: 8/50
2024-12-18 00:51:14.270667: train_loss -0.7589
2024-12-18 00:51:14.271743: val_loss -0.4432
2024-12-18 00:51:14.272679: Pseudo dice [0.7107]
2024-12-18 00:51:14.273603: Epoch time: 475.33 s
2024-12-18 00:51:15.766775: 
2024-12-18 00:51:15.768320: Epoch 27
2024-12-18 00:51:15.769428: Current learning rate: 0.00836
2024-12-18 00:59:27.690247: Validation loss did not improve from -0.50019. Patience: 9/50
2024-12-18 00:59:27.691236: train_loss -0.7622
2024-12-18 00:59:27.692086: val_loss -0.4902
2024-12-18 00:59:27.692800: Pseudo dice [0.7388]
2024-12-18 00:59:27.693519: Epoch time: 491.93 s
2024-12-18 00:59:27.694345: Yayy! New best EMA pseudo Dice: 0.7151
2024-12-18 00:59:29.521775: 
2024-12-18 00:59:29.522781: Epoch 28
2024-12-18 00:59:29.523557: Current learning rate: 0.0083
2024-12-18 01:07:37.139284: Validation loss did not improve from -0.50019. Patience: 10/50
2024-12-18 01:07:37.140268: train_loss -0.7637
2024-12-18 01:07:37.141087: val_loss -0.4851
2024-12-18 01:07:37.141856: Pseudo dice [0.721]
2024-12-18 01:07:37.142627: Epoch time: 487.62 s
2024-12-18 01:07:37.143400: Yayy! New best EMA pseudo Dice: 0.7157
2024-12-18 01:07:39.104992: 
2024-12-18 01:07:39.106560: Epoch 29
2024-12-18 01:07:39.107578: Current learning rate: 0.00824
2024-12-18 01:15:54.752691: Validation loss did not improve from -0.50019. Patience: 11/50
2024-12-18 01:15:54.753855: train_loss -0.7642
2024-12-18 01:15:54.754850: val_loss -0.4946
2024-12-18 01:15:54.755775: Pseudo dice [0.7409]
2024-12-18 01:15:54.756523: Epoch time: 495.65 s
2024-12-18 01:15:55.222923: Yayy! New best EMA pseudo Dice: 0.7182
2024-12-18 01:15:57.527673: 
2024-12-18 01:15:57.529312: Epoch 30
2024-12-18 01:15:57.530252: Current learning rate: 0.00818
2024-12-18 01:24:04.663230: Validation loss did not improve from -0.50019. Patience: 12/50
2024-12-18 01:24:04.664656: train_loss -0.7682
2024-12-18 01:24:04.665712: val_loss -0.4723
2024-12-18 01:24:04.666814: Pseudo dice [0.7321]
2024-12-18 01:24:04.667827: Epoch time: 487.14 s
2024-12-18 01:24:04.668872: Yayy! New best EMA pseudo Dice: 0.7196
2024-12-18 01:24:06.582985: 
2024-12-18 01:24:06.583970: Epoch 31
2024-12-18 01:24:06.584883: Current learning rate: 0.00812
2024-12-18 01:32:20.105743: Validation loss did not improve from -0.50019. Patience: 13/50
2024-12-18 01:32:20.109339: train_loss -0.77
2024-12-18 01:32:20.110661: val_loss -0.4669
2024-12-18 01:32:20.111567: Pseudo dice [0.7289]
2024-12-18 01:32:20.112520: Epoch time: 493.53 s
2024-12-18 01:32:20.113506: Yayy! New best EMA pseudo Dice: 0.7205
2024-12-18 01:32:22.038929: 
2024-12-18 01:32:22.040337: Epoch 32
2024-12-18 01:32:22.041336: Current learning rate: 0.00806
2024-12-18 01:40:42.430948: Validation loss did not improve from -0.50019. Patience: 14/50
2024-12-18 01:40:42.431874: train_loss -0.7671
2024-12-18 01:40:42.433092: val_loss -0.4448
2024-12-18 01:40:42.434072: Pseudo dice [0.6992]
2024-12-18 01:40:42.435097: Epoch time: 500.39 s
2024-12-18 01:40:43.931240: 
2024-12-18 01:40:43.932698: Epoch 33
2024-12-18 01:40:43.933863: Current learning rate: 0.008
2024-12-18 01:49:01.848165: Validation loss did not improve from -0.50019. Patience: 15/50
2024-12-18 01:49:01.850490: train_loss -0.7723
2024-12-18 01:49:01.851823: val_loss -0.4464
2024-12-18 01:49:01.852939: Pseudo dice [0.7118]
2024-12-18 01:49:01.853964: Epoch time: 497.92 s
2024-12-18 01:49:03.334832: 
2024-12-18 01:49:03.336332: Epoch 34
2024-12-18 01:49:03.337475: Current learning rate: 0.00793
2024-12-18 01:56:02.284116: Validation loss did not improve from -0.50019. Patience: 16/50
2024-12-18 01:56:02.285148: train_loss -0.7747
2024-12-18 01:56:02.285936: val_loss -0.4845
2024-12-18 01:56:02.286575: Pseudo dice [0.7259]
2024-12-18 01:56:02.287244: Epoch time: 418.95 s
2024-12-18 01:56:04.136029: 
2024-12-18 01:56:04.137447: Epoch 35
2024-12-18 01:56:04.138359: Current learning rate: 0.00787
2024-12-18 02:03:21.237557: Validation loss improved from -0.50019 to -0.50221! Patience: 16/50
2024-12-18 02:03:21.238601: train_loss -0.7777
2024-12-18 02:03:21.239525: val_loss -0.5022
2024-12-18 02:03:21.240409: Pseudo dice [0.7383]
2024-12-18 02:03:21.241170: Epoch time: 437.1 s
2024-12-18 02:03:22.775465: 
2024-12-18 02:03:22.776797: Epoch 36
2024-12-18 02:03:22.777570: Current learning rate: 0.00781
2024-12-18 02:10:18.363949: Validation loss did not improve from -0.50221. Patience: 1/50
2024-12-18 02:10:18.365131: train_loss -0.7856
2024-12-18 02:10:18.366086: val_loss -0.4745
2024-12-18 02:10:18.366707: Pseudo dice [0.7266]
2024-12-18 02:10:18.367365: Epoch time: 415.59 s
2024-12-18 02:10:18.368004: Yayy! New best EMA pseudo Dice: 0.7211
2024-12-18 02:10:20.237862: 
2024-12-18 02:10:20.239967: Epoch 37
2024-12-18 02:10:20.241460: Current learning rate: 0.00775
2024-12-18 02:16:29.396617: Validation loss did not improve from -0.50221. Patience: 2/50
2024-12-18 02:16:29.397664: train_loss -0.7785
2024-12-18 02:16:29.398713: val_loss -0.466
2024-12-18 02:16:29.399695: Pseudo dice [0.7198]
2024-12-18 02:16:29.400661: Epoch time: 369.17 s
2024-12-18 02:16:30.936783: 
2024-12-18 02:16:30.938245: Epoch 38
2024-12-18 02:16:30.939548: Current learning rate: 0.00769
2024-12-18 02:22:25.341150: Validation loss did not improve from -0.50221. Patience: 3/50
2024-12-18 02:22:25.344130: train_loss -0.7793
2024-12-18 02:22:25.345137: val_loss -0.4614
2024-12-18 02:22:25.345982: Pseudo dice [0.7073]
2024-12-18 02:22:25.346840: Epoch time: 354.41 s
2024-12-18 02:22:26.851377: 
2024-12-18 02:22:26.852851: Epoch 39
2024-12-18 02:22:26.854002: Current learning rate: 0.00763
2024-12-18 02:31:06.530047: Validation loss did not improve from -0.50221. Patience: 4/50
2024-12-18 02:31:06.531693: train_loss -0.7784
2024-12-18 02:31:06.532812: val_loss -0.4936
2024-12-18 02:31:06.533870: Pseudo dice [0.7335]
2024-12-18 02:31:06.534917: Epoch time: 519.68 s
2024-12-18 02:31:08.406395: 
2024-12-18 02:31:08.408001: Epoch 40
2024-12-18 02:31:08.409218: Current learning rate: 0.00756
2024-12-18 02:39:02.803068: Validation loss did not improve from -0.50221. Patience: 5/50
2024-12-18 02:39:02.803890: train_loss -0.7893
2024-12-18 02:39:02.804710: val_loss -0.461
2024-12-18 02:39:02.805454: Pseudo dice [0.7225]
2024-12-18 02:39:02.806306: Epoch time: 474.4 s
2024-12-18 02:39:02.807134: Yayy! New best EMA pseudo Dice: 0.7212
2024-12-18 02:39:05.227432: 
2024-12-18 02:39:05.228743: Epoch 41
2024-12-18 02:39:05.229708: Current learning rate: 0.0075
2024-12-18 02:47:38.490050: Validation loss did not improve from -0.50221. Patience: 6/50
2024-12-18 02:47:38.491253: train_loss -0.7873
2024-12-18 02:47:38.492228: val_loss -0.4759
2024-12-18 02:47:38.493054: Pseudo dice [0.728]
2024-12-18 02:47:38.493786: Epoch time: 513.27 s
2024-12-18 02:47:38.494531: Yayy! New best EMA pseudo Dice: 0.7218
2024-12-18 02:47:40.304095: 
2024-12-18 02:47:40.305407: Epoch 42
2024-12-18 02:47:40.306267: Current learning rate: 0.00744
2024-12-18 02:56:19.521656: Validation loss did not improve from -0.50221. Patience: 7/50
2024-12-18 02:56:19.523727: train_loss -0.7888
2024-12-18 02:56:19.524687: val_loss -0.4491
2024-12-18 02:56:19.525450: Pseudo dice [0.7149]
2024-12-18 02:56:19.526332: Epoch time: 519.22 s
2024-12-18 02:56:20.948299: 
2024-12-18 02:56:20.949625: Epoch 43
2024-12-18 02:56:20.950637: Current learning rate: 0.00738
2024-12-18 03:05:07.262974: Validation loss did not improve from -0.50221. Patience: 8/50
2024-12-18 03:05:07.264244: train_loss -0.7885
2024-12-18 03:05:07.265482: val_loss -0.4868
2024-12-18 03:05:07.266306: Pseudo dice [0.7419]
2024-12-18 03:05:07.267102: Epoch time: 526.32 s
2024-12-18 03:05:07.267857: Yayy! New best EMA pseudo Dice: 0.7232
2024-12-18 03:05:09.124619: 
2024-12-18 03:05:09.126216: Epoch 44
2024-12-18 03:05:09.127476: Current learning rate: 0.00732
2024-12-18 03:13:31.962625: Validation loss improved from -0.50221 to -0.50391! Patience: 8/50
2024-12-18 03:13:31.963696: train_loss -0.7896
2024-12-18 03:13:31.964488: val_loss -0.5039
2024-12-18 03:13:31.965147: Pseudo dice [0.7386]
2024-12-18 03:13:31.965868: Epoch time: 502.84 s
2024-12-18 03:13:32.354888: Yayy! New best EMA pseudo Dice: 0.7248
2024-12-18 03:13:34.121372: 
2024-12-18 03:13:34.122220: Epoch 45
2024-12-18 03:13:34.123068: Current learning rate: 0.00725
2024-12-18 03:21:31.635072: Validation loss did not improve from -0.50391. Patience: 1/50
2024-12-18 03:21:31.636155: train_loss -0.7918
2024-12-18 03:21:31.636995: val_loss -0.5001
2024-12-18 03:21:31.637787: Pseudo dice [0.7337]
2024-12-18 03:21:31.638534: Epoch time: 477.52 s
2024-12-18 03:21:31.639222: Yayy! New best EMA pseudo Dice: 0.7257
2024-12-18 03:21:33.429249: 
2024-12-18 03:21:33.430744: Epoch 46
2024-12-18 03:21:33.432901: Current learning rate: 0.00719
2024-12-18 03:29:57.883485: Validation loss did not improve from -0.50391. Patience: 2/50
2024-12-18 03:29:57.884584: train_loss -0.7953
2024-12-18 03:29:57.885681: val_loss -0.4726
2024-12-18 03:29:57.886765: Pseudo dice [0.7313]
2024-12-18 03:29:57.887773: Epoch time: 504.46 s
2024-12-18 03:29:57.888771: Yayy! New best EMA pseudo Dice: 0.7262
2024-12-18 03:29:59.754481: 
2024-12-18 03:29:59.756025: Epoch 47
2024-12-18 03:29:59.757075: Current learning rate: 0.00713
2024-12-18 03:38:34.105412: Validation loss did not improve from -0.50391. Patience: 3/50
2024-12-18 03:38:34.106446: train_loss -0.7926
2024-12-18 03:38:34.107234: val_loss -0.4918
2024-12-18 03:38:34.108001: Pseudo dice [0.7371]
2024-12-18 03:38:34.108853: Epoch time: 514.35 s
2024-12-18 03:38:34.109545: Yayy! New best EMA pseudo Dice: 0.7273
2024-12-18 03:38:35.980188: 
2024-12-18 03:38:35.981705: Epoch 48
2024-12-18 03:38:35.982949: Current learning rate: 0.00707
2024-12-18 03:47:09.661709: Validation loss did not improve from -0.50391. Patience: 4/50
2024-12-18 03:47:09.682564: train_loss -0.793
2024-12-18 03:47:09.685091: val_loss -0.4865
2024-12-18 03:47:09.686002: Pseudo dice [0.7304]
2024-12-18 03:47:09.686998: Epoch time: 513.69 s
2024-12-18 03:47:09.687837: Yayy! New best EMA pseudo Dice: 0.7276
2024-12-18 03:47:11.550212: 
2024-12-18 03:47:11.551522: Epoch 49
2024-12-18 03:47:11.552278: Current learning rate: 0.007
2024-12-18 03:55:32.790730: Validation loss did not improve from -0.50391. Patience: 5/50
2024-12-18 03:55:32.792213: train_loss -0.7967
2024-12-18 03:55:32.793092: val_loss -0.4498
2024-12-18 03:55:32.793970: Pseudo dice [0.7153]
2024-12-18 03:55:32.794823: Epoch time: 501.24 s
2024-12-18 03:55:34.589618: 
2024-12-18 03:55:34.590949: Epoch 50
2024-12-18 03:55:34.591859: Current learning rate: 0.00694
2024-12-18 04:03:22.572014: Validation loss did not improve from -0.50391. Patience: 6/50
2024-12-18 04:03:22.573606: train_loss -0.796
2024-12-18 04:03:22.574583: val_loss -0.4707
2024-12-18 04:03:22.575414: Pseudo dice [0.7361]
2024-12-18 04:03:22.576218: Epoch time: 467.99 s
2024-12-18 04:03:24.074617: 
2024-12-18 04:03:24.076030: Epoch 51
2024-12-18 04:03:24.076813: Current learning rate: 0.00688
2024-12-18 04:11:43.133556: Validation loss did not improve from -0.50391. Patience: 7/50
2024-12-18 04:11:43.134984: train_loss -0.7971
2024-12-18 04:11:43.136042: val_loss -0.4321
2024-12-18 04:11:43.136922: Pseudo dice [0.7124]
2024-12-18 04:11:43.137831: Epoch time: 499.06 s
2024-12-18 04:11:45.779007: 
2024-12-18 04:11:45.780586: Epoch 52
2024-12-18 04:11:45.781600: Current learning rate: 0.00682
2024-12-18 04:20:13.757891: Validation loss did not improve from -0.50391. Patience: 8/50
2024-12-18 04:20:13.759204: train_loss -0.7919
2024-12-18 04:20:13.760326: val_loss -0.4608
2024-12-18 04:20:13.761375: Pseudo dice [0.7209]
2024-12-18 04:20:13.762367: Epoch time: 507.98 s
2024-12-18 04:20:15.223257: 
2024-12-18 04:20:15.224389: Epoch 53
2024-12-18 04:20:15.225336: Current learning rate: 0.00675
2024-12-18 04:28:38.172308: Validation loss did not improve from -0.50391. Patience: 9/50
2024-12-18 04:28:38.173376: train_loss -0.7932
2024-12-18 04:28:38.174226: val_loss -0.4712
2024-12-18 04:28:38.175096: Pseudo dice [0.7313]
2024-12-18 04:28:38.176083: Epoch time: 502.95 s
2024-12-18 04:28:39.598176: 
2024-12-18 04:28:39.599761: Epoch 54
2024-12-18 04:28:39.600827: Current learning rate: 0.00669
2024-12-18 04:36:33.703077: Validation loss did not improve from -0.50391. Patience: 10/50
2024-12-18 04:36:33.704246: train_loss -0.7982
2024-12-18 04:36:33.705407: val_loss -0.4416
2024-12-18 04:36:33.706272: Pseudo dice [0.7113]
2024-12-18 04:36:33.707141: Epoch time: 474.11 s
2024-12-18 04:36:35.592755: 
2024-12-18 04:36:35.594059: Epoch 55
2024-12-18 04:36:35.594836: Current learning rate: 0.00663
2024-12-18 04:44:15.156722: Validation loss did not improve from -0.50391. Patience: 11/50
2024-12-18 04:44:15.157846: train_loss -0.8006
2024-12-18 04:44:15.158647: val_loss -0.421
2024-12-18 04:44:15.159372: Pseudo dice [0.6996]
2024-12-18 04:44:15.160203: Epoch time: 459.57 s
2024-12-18 04:44:16.580709: 
2024-12-18 04:44:16.582127: Epoch 56
2024-12-18 04:44:16.583031: Current learning rate: 0.00657
2024-12-18 04:52:12.072595: Validation loss did not improve from -0.50391. Patience: 12/50
2024-12-18 04:52:12.073775: train_loss -0.7993
2024-12-18 04:52:12.075942: val_loss -0.4768
2024-12-18 04:52:12.077021: Pseudo dice [0.7233]
2024-12-18 04:52:12.078073: Epoch time: 475.49 s
2024-12-18 04:52:13.524736: 
2024-12-18 04:52:13.526127: Epoch 57
2024-12-18 04:52:13.527172: Current learning rate: 0.0065
2024-12-18 05:00:22.214893: Validation loss did not improve from -0.50391. Patience: 13/50
2024-12-18 05:00:22.216192: train_loss -0.8057
2024-12-18 05:00:22.217091: val_loss -0.4459
2024-12-18 05:00:22.217944: Pseudo dice [0.7259]
2024-12-18 05:00:22.218737: Epoch time: 488.69 s
2024-12-18 05:00:23.763303: 
2024-12-18 05:00:23.764899: Epoch 58
2024-12-18 05:00:23.765956: Current learning rate: 0.00644
2024-12-18 05:08:59.483599: Validation loss did not improve from -0.50391. Patience: 14/50
2024-12-18 05:08:59.484743: train_loss -0.8073
2024-12-18 05:08:59.485801: val_loss -0.4832
2024-12-18 05:08:59.486739: Pseudo dice [0.7348]
2024-12-18 05:08:59.487839: Epoch time: 515.72 s
2024-12-18 05:09:01.005675: 
2024-12-18 05:09:01.007341: Epoch 59
2024-12-18 05:09:01.008428: Current learning rate: 0.00638
2024-12-18 05:17:32.144916: Validation loss did not improve from -0.50391. Patience: 15/50
2024-12-18 05:17:32.145910: train_loss -0.8081
2024-12-18 05:17:32.146694: val_loss -0.4601
2024-12-18 05:17:32.147373: Pseudo dice [0.7246]
2024-12-18 05:17:32.148206: Epoch time: 511.14 s
2024-12-18 05:17:34.040605: 
2024-12-18 05:17:34.041976: Epoch 60
2024-12-18 05:17:34.042795: Current learning rate: 0.00631
2024-12-18 05:25:32.769471: Validation loss did not improve from -0.50391. Patience: 16/50
2024-12-18 05:25:32.770480: train_loss -0.8076
2024-12-18 05:25:32.771290: val_loss -0.4556
2024-12-18 05:25:32.772131: Pseudo dice [0.7153]
2024-12-18 05:25:32.772923: Epoch time: 478.73 s
2024-12-18 05:25:34.269936: 
2024-12-18 05:25:34.271400: Epoch 61
2024-12-18 05:25:34.272202: Current learning rate: 0.00625
2024-12-18 05:34:02.315919: Validation loss did not improve from -0.50391. Patience: 17/50
2024-12-18 05:34:02.316872: train_loss -0.8073
2024-12-18 05:34:02.317657: val_loss -0.4328
2024-12-18 05:34:02.318503: Pseudo dice [0.7189]
2024-12-18 05:34:02.319310: Epoch time: 508.05 s
2024-12-18 05:34:03.766427: 
2024-12-18 05:34:03.767806: Epoch 62
2024-12-18 05:34:03.768837: Current learning rate: 0.00619
2024-12-18 05:42:29.026986: Validation loss did not improve from -0.50391. Patience: 18/50
2024-12-18 05:42:29.028117: train_loss -0.808
2024-12-18 05:42:29.029070: val_loss -0.4706
2024-12-18 05:42:29.029814: Pseudo dice [0.7296]
2024-12-18 05:42:29.030679: Epoch time: 505.26 s
2024-12-18 05:42:31.083754: 
2024-12-18 05:42:31.085112: Epoch 63
2024-12-18 05:42:31.085839: Current learning rate: 0.00612
2024-12-18 05:51:10.412339: Validation loss did not improve from -0.50391. Patience: 19/50
2024-12-18 05:51:10.413452: train_loss -0.8106
2024-12-18 05:51:10.414320: val_loss -0.4802
2024-12-18 05:51:10.415035: Pseudo dice [0.7438]
2024-12-18 05:51:10.415841: Epoch time: 519.33 s
2024-12-18 05:51:11.890050: 
2024-12-18 05:51:11.891445: Epoch 64
2024-12-18 05:51:11.892229: Current learning rate: 0.00606
2024-12-18 05:59:18.848927: Validation loss did not improve from -0.50391. Patience: 20/50
2024-12-18 05:59:18.850034: train_loss -0.8128
2024-12-18 05:59:18.850822: val_loss -0.4621
2024-12-18 05:59:18.851629: Pseudo dice [0.7172]
2024-12-18 05:59:18.852418: Epoch time: 486.96 s
2024-12-18 05:59:20.775722: 
2024-12-18 05:59:20.776909: Epoch 65
2024-12-18 05:59:20.777766: Current learning rate: 0.006
2024-12-18 06:07:35.844418: Validation loss did not improve from -0.50391. Patience: 21/50
2024-12-18 06:07:35.846662: train_loss -0.8109
2024-12-18 06:07:35.847802: val_loss -0.4417
2024-12-18 06:07:35.848768: Pseudo dice [0.7136]
2024-12-18 06:07:35.849756: Epoch time: 495.07 s
2024-12-18 06:07:37.328765: 
2024-12-18 06:07:37.330178: Epoch 66
2024-12-18 06:07:37.331008: Current learning rate: 0.00593
2024-12-18 06:15:48.021271: Validation loss did not improve from -0.50391. Patience: 22/50
2024-12-18 06:15:48.022458: train_loss -0.8114
2024-12-18 06:15:48.023343: val_loss -0.4287
2024-12-18 06:15:48.024136: Pseudo dice [0.7071]
2024-12-18 06:15:48.024979: Epoch time: 490.7 s
2024-12-18 06:15:49.450595: 
2024-12-18 06:15:49.451777: Epoch 67
2024-12-18 06:15:49.452666: Current learning rate: 0.00587
2024-12-18 06:23:19.020489: Validation loss did not improve from -0.50391. Patience: 23/50
2024-12-18 06:23:19.021357: train_loss -0.8162
2024-12-18 06:23:19.022212: val_loss -0.4353
2024-12-18 06:23:19.022920: Pseudo dice [0.7123]
2024-12-18 06:23:19.023652: Epoch time: 449.57 s
2024-12-18 06:23:20.564109: 
2024-12-18 06:23:20.565780: Epoch 68
2024-12-18 06:23:20.566807: Current learning rate: 0.00581
2024-12-18 06:30:56.434873: Validation loss did not improve from -0.50391. Patience: 24/50
2024-12-18 06:30:56.435868: train_loss -0.8175
2024-12-18 06:30:56.436702: val_loss -0.4486
2024-12-18 06:30:56.437633: Pseudo dice [0.7315]
2024-12-18 06:30:56.438402: Epoch time: 455.87 s
2024-12-18 06:30:57.903561: 
2024-12-18 06:30:57.905155: Epoch 69
2024-12-18 06:30:57.906045: Current learning rate: 0.00574
2024-12-18 06:38:21.627666: Validation loss did not improve from -0.50391. Patience: 25/50
2024-12-18 06:38:21.628754: train_loss -0.8134
2024-12-18 06:38:21.629630: val_loss -0.4405
2024-12-18 06:38:21.630576: Pseudo dice [0.7112]
2024-12-18 06:38:21.631439: Epoch time: 443.73 s
2024-12-18 06:38:23.571968: 
2024-12-18 06:38:23.573265: Epoch 70
2024-12-18 06:38:23.573978: Current learning rate: 0.00568
2024-12-18 06:46:18.331647: Validation loss did not improve from -0.50391. Patience: 26/50
2024-12-18 06:46:18.332786: train_loss -0.8175
2024-12-18 06:46:18.333616: val_loss -0.4873
2024-12-18 06:46:18.334343: Pseudo dice [0.7309]
2024-12-18 06:46:18.335069: Epoch time: 474.76 s
2024-12-18 06:46:19.810373: 
2024-12-18 06:46:19.811860: Epoch 71
2024-12-18 06:46:19.812796: Current learning rate: 0.00562
2024-12-18 06:54:36.654757: Validation loss did not improve from -0.50391. Patience: 27/50
2024-12-18 06:54:36.655617: train_loss -0.8158
2024-12-18 06:54:36.656499: val_loss -0.4271
2024-12-18 06:54:36.657476: Pseudo dice [0.7129]
2024-12-18 06:54:36.658423: Epoch time: 496.85 s
2024-12-18 06:54:38.188805: 
2024-12-18 06:54:38.189758: Epoch 72
2024-12-18 06:54:38.190631: Current learning rate: 0.00555
2024-12-18 07:03:05.440264: Validation loss did not improve from -0.50391. Patience: 28/50
2024-12-18 07:03:05.441391: train_loss -0.8171
2024-12-18 07:03:05.443392: val_loss -0.4679
2024-12-18 07:03:05.444330: Pseudo dice [0.7333]
2024-12-18 07:03:05.445446: Epoch time: 507.25 s
2024-12-18 07:03:06.905845: 
2024-12-18 07:03:06.907999: Epoch 73
2024-12-18 07:03:06.909808: Current learning rate: 0.00549
2024-12-18 07:11:22.266472: Validation loss did not improve from -0.50391. Patience: 29/50
2024-12-18 07:11:22.267503: train_loss -0.816
2024-12-18 07:11:22.268363: val_loss -0.4671
2024-12-18 07:11:22.269118: Pseudo dice [0.7366]
2024-12-18 07:11:22.269993: Epoch time: 495.36 s
2024-12-18 07:11:24.195622: 
2024-12-18 07:11:24.197026: Epoch 74
2024-12-18 07:11:24.197848: Current learning rate: 0.00542
2024-12-18 07:19:39.054092: Validation loss did not improve from -0.50391. Patience: 30/50
2024-12-18 07:19:39.055065: train_loss -0.8127
2024-12-18 07:19:39.055709: val_loss -0.4554
2024-12-18 07:19:39.056356: Pseudo dice [0.7194]
2024-12-18 07:19:39.057321: Epoch time: 494.86 s
2024-12-18 07:19:40.810309: 
2024-12-18 07:19:40.811302: Epoch 75
2024-12-18 07:19:40.811979: Current learning rate: 0.00536
2024-12-18 07:28:01.648166: Validation loss did not improve from -0.50391. Patience: 31/50
2024-12-18 07:28:01.649117: train_loss -0.8183
2024-12-18 07:28:01.650069: val_loss -0.4608
2024-12-18 07:28:01.651000: Pseudo dice [0.7238]
2024-12-18 07:28:01.651927: Epoch time: 500.84 s
2024-12-18 07:28:03.061048: 
2024-12-18 07:28:03.063049: Epoch 76
2024-12-18 07:28:03.064035: Current learning rate: 0.00529
2024-12-18 07:36:27.032129: Validation loss did not improve from -0.50391. Patience: 32/50
2024-12-18 07:36:27.033046: train_loss -0.8197
2024-12-18 07:36:27.033802: val_loss -0.4517
2024-12-18 07:36:27.034575: Pseudo dice [0.7191]
2024-12-18 07:36:27.035510: Epoch time: 503.97 s
2024-12-18 07:36:28.438470: 
2024-12-18 07:36:28.439660: Epoch 77
2024-12-18 07:36:28.440543: Current learning rate: 0.00523
2024-12-18 07:45:00.256382: Validation loss did not improve from -0.50391. Patience: 33/50
2024-12-18 07:45:00.257421: train_loss -0.8225
2024-12-18 07:45:00.258505: val_loss -0.4543
2024-12-18 07:45:00.259276: Pseudo dice [0.7175]
2024-12-18 07:45:00.260022: Epoch time: 511.82 s
2024-12-18 07:45:01.760530: 
2024-12-18 07:45:01.761952: Epoch 78
2024-12-18 07:45:01.763057: Current learning rate: 0.00517
2024-12-18 07:53:48.607995: Validation loss did not improve from -0.50391. Patience: 34/50
2024-12-18 07:53:48.608809: train_loss -0.8195
2024-12-18 07:53:48.609685: val_loss -0.45
2024-12-18 07:53:48.610325: Pseudo dice [0.7202]
2024-12-18 07:53:48.611005: Epoch time: 526.85 s
2024-12-18 07:53:50.049425: 
2024-12-18 07:53:50.050418: Epoch 79
2024-12-18 07:53:50.051169: Current learning rate: 0.0051
2024-12-18 08:01:58.650843: Validation loss did not improve from -0.50391. Patience: 35/50
2024-12-18 08:01:58.652690: train_loss -0.8218
2024-12-18 08:01:58.653680: val_loss -0.4565
2024-12-18 08:01:58.654442: Pseudo dice [0.7347]
2024-12-18 08:01:58.655110: Epoch time: 488.6 s
2024-12-18 08:02:00.642566: 
2024-12-18 08:02:00.644737: Epoch 80
2024-12-18 08:02:00.646348: Current learning rate: 0.00504
2024-12-18 08:09:57.527014: Validation loss did not improve from -0.50391. Patience: 36/50
2024-12-18 08:09:57.527991: train_loss -0.8241
2024-12-18 08:09:57.528980: val_loss -0.4342
2024-12-18 08:09:57.529699: Pseudo dice [0.726]
2024-12-18 08:09:57.530527: Epoch time: 476.89 s
2024-12-18 08:09:59.110315: 
2024-12-18 08:09:59.111578: Epoch 81
2024-12-18 08:09:59.112386: Current learning rate: 0.00497
2024-12-18 08:18:31.337882: Validation loss did not improve from -0.50391. Patience: 37/50
2024-12-18 08:18:31.340718: train_loss -0.8267
2024-12-18 08:18:31.341892: val_loss -0.4202
2024-12-18 08:18:31.342584: Pseudo dice [0.7136]
2024-12-18 08:18:31.343339: Epoch time: 512.23 s
2024-12-18 08:18:32.886475: 
2024-12-18 08:18:32.888017: Epoch 82
2024-12-18 08:18:32.888894: Current learning rate: 0.00491
2024-12-18 08:27:31.065488: Validation loss did not improve from -0.50391. Patience: 38/50
2024-12-18 08:27:31.066682: train_loss -0.8249
2024-12-18 08:27:31.067680: val_loss -0.4053
2024-12-18 08:27:31.068512: Pseudo dice [0.7144]
2024-12-18 08:27:31.069268: Epoch time: 538.18 s
2024-12-18 08:27:32.464345: 
2024-12-18 08:27:32.465756: Epoch 83
2024-12-18 08:27:32.466533: Current learning rate: 0.00484
2024-12-18 08:35:48.273115: Validation loss did not improve from -0.50391. Patience: 39/50
2024-12-18 08:35:48.274083: train_loss -0.8252
2024-12-18 08:35:48.274933: val_loss -0.4139
2024-12-18 08:35:48.275704: Pseudo dice [0.7145]
2024-12-18 08:35:48.276497: Epoch time: 495.81 s
2024-12-18 08:35:50.567502: 
2024-12-18 08:35:50.569084: Epoch 84
2024-12-18 08:35:50.569958: Current learning rate: 0.00478
2024-12-18 08:44:22.894782: Validation loss did not improve from -0.50391. Patience: 40/50
2024-12-18 08:44:22.895755: train_loss -0.8263
2024-12-18 08:44:22.897006: val_loss -0.4328
2024-12-18 08:44:22.898150: Pseudo dice [0.7118]
2024-12-18 08:44:22.899264: Epoch time: 512.33 s
2024-12-18 08:44:24.641920: 
2024-12-18 08:44:24.643640: Epoch 85
2024-12-18 08:44:24.644694: Current learning rate: 0.00471
2024-12-18 08:52:55.767421: Validation loss did not improve from -0.50391. Patience: 41/50
2024-12-18 08:52:55.768551: train_loss -0.8258
2024-12-18 08:52:55.769396: val_loss -0.4251
2024-12-18 08:52:55.770267: Pseudo dice [0.718]
2024-12-18 08:52:55.771115: Epoch time: 511.13 s
2024-12-18 08:52:57.183281: 
2024-12-18 08:52:57.184671: Epoch 86
2024-12-18 08:52:57.185602: Current learning rate: 0.00465
2024-12-18 09:01:38.384392: Validation loss did not improve from -0.50391. Patience: 42/50
2024-12-18 09:01:38.385497: train_loss -0.8256
2024-12-18 09:01:38.386434: val_loss -0.448
2024-12-18 09:01:38.387158: Pseudo dice [0.7296]
2024-12-18 09:01:38.387980: Epoch time: 521.2 s
2024-12-18 09:01:39.773599: 
2024-12-18 09:01:39.774962: Epoch 87
2024-12-18 09:01:39.776057: Current learning rate: 0.00458
2024-12-18 09:10:24.837486: Validation loss did not improve from -0.50391. Patience: 43/50
2024-12-18 09:10:24.838451: train_loss -0.8274
2024-12-18 09:10:24.839291: val_loss -0.4436
2024-12-18 09:10:24.840013: Pseudo dice [0.7195]
2024-12-18 09:10:24.840806: Epoch time: 525.07 s
2024-12-18 09:10:26.319533: 
2024-12-18 09:10:26.320926: Epoch 88
2024-12-18 09:10:26.321764: Current learning rate: 0.00452
2024-12-18 09:18:55.288097: Validation loss did not improve from -0.50391. Patience: 44/50
2024-12-18 09:18:55.289029: train_loss -0.8288
2024-12-18 09:18:55.289894: val_loss -0.4186
2024-12-18 09:18:55.290637: Pseudo dice [0.7127]
2024-12-18 09:18:55.291316: Epoch time: 508.97 s
2024-12-18 09:18:56.802803: 
2024-12-18 09:18:56.804138: Epoch 89
2024-12-18 09:18:56.804855: Current learning rate: 0.00445
2024-12-18 09:27:27.037938: Validation loss did not improve from -0.50391. Patience: 45/50
2024-12-18 09:27:27.039069: train_loss -0.8278
2024-12-18 09:27:27.039851: val_loss -0.429
2024-12-18 09:27:27.040565: Pseudo dice [0.7163]
2024-12-18 09:27:27.041446: Epoch time: 510.24 s
2024-12-18 09:27:28.895400: 
2024-12-18 09:27:28.896665: Epoch 90
2024-12-18 09:27:28.897575: Current learning rate: 0.00438
2024-12-18 09:34:51.861703: Validation loss did not improve from -0.50391. Patience: 46/50
2024-12-18 09:34:51.862893: train_loss -0.83
2024-12-18 09:34:51.863831: val_loss -0.4005
2024-12-18 09:34:51.864774: Pseudo dice [0.7011]
2024-12-18 09:34:51.865558: Epoch time: 442.97 s
2024-12-18 09:34:53.305388: 
2024-12-18 09:34:53.306872: Epoch 91
2024-12-18 09:34:53.308018: Current learning rate: 0.00432
2024-12-18 09:42:34.101386: Validation loss did not improve from -0.50391. Patience: 47/50
2024-12-18 09:42:34.102599: train_loss -0.8302
2024-12-18 09:42:34.103456: val_loss -0.4161
2024-12-18 09:42:34.104306: Pseudo dice [0.7221]
2024-12-18 09:42:34.105068: Epoch time: 460.8 s
2024-12-18 09:42:35.492659: 
2024-12-18 09:42:35.494161: Epoch 92
2024-12-18 09:42:35.495141: Current learning rate: 0.00425
2024-12-18 09:50:25.325149: Validation loss did not improve from -0.50391. Patience: 48/50
2024-12-18 09:50:25.326157: train_loss -0.8309
2024-12-18 09:50:25.327055: val_loss -0.3886
2024-12-18 09:50:25.327833: Pseudo dice [0.7026]
2024-12-18 09:50:25.328593: Epoch time: 469.84 s
2024-12-18 09:50:26.767744: 
2024-12-18 09:50:26.769104: Epoch 93
2024-12-18 09:50:26.770286: Current learning rate: 0.00419
2024-12-18 09:59:03.008595: Validation loss did not improve from -0.50391. Patience: 49/50
2024-12-18 09:59:03.009592: train_loss -0.8325
2024-12-18 09:59:03.010422: val_loss -0.4753
2024-12-18 09:59:03.011397: Pseudo dice [0.7435]
2024-12-18 09:59:03.012373: Epoch time: 516.24 s
2024-12-18 09:59:04.567784: 
2024-12-18 09:59:04.569237: Epoch 94
2024-12-18 09:59:04.570286: Current learning rate: 0.00412
2024-12-18 10:07:25.425027: Validation loss did not improve from -0.50391. Patience: 50/50
2024-12-18 10:07:25.425975: train_loss -0.8315
2024-12-18 10:07:25.426793: val_loss -0.4499
2024-12-18 10:07:25.427479: Pseudo dice [0.7255]
2024-12-18 10:07:25.428378: Epoch time: 500.86 s
2024-12-18 10:07:27.392380: 
2024-12-18 10:07:27.393580: Epoch 95
2024-12-18 10:07:27.394307: Current learning rate: 0.00405
2024-12-18 10:15:23.270129: Validation loss did not improve from -0.50391. Patience: 51/50
2024-12-18 10:15:23.273448: train_loss -0.8305
2024-12-18 10:15:23.274786: val_loss -0.4769
2024-12-18 10:15:23.275578: Pseudo dice [0.7387]
2024-12-18 10:15:23.276503: Epoch time: 475.88 s
2024-12-18 10:15:25.749371: 
2024-12-18 10:15:25.750980: Epoch 96
2024-12-18 10:15:25.751963: Current learning rate: 0.00399
2024-12-18 10:23:22.553036: Validation loss did not improve from -0.50391. Patience: 52/50
2024-12-18 10:23:22.553885: train_loss -0.8355
2024-12-18 10:23:22.554744: val_loss -0.4597
2024-12-18 10:23:22.555542: Pseudo dice [0.7262]
2024-12-18 10:23:22.556270: Epoch time: 476.81 s
2024-12-18 10:23:24.024078: 
2024-12-18 10:23:24.025236: Epoch 97
2024-12-18 10:23:24.026056: Current learning rate: 0.00392
2024-12-18 10:31:16.326267: Validation loss did not improve from -0.50391. Patience: 53/50
2024-12-18 10:31:16.327425: train_loss -0.8348
2024-12-18 10:31:16.328763: val_loss -0.4519
2024-12-18 10:31:16.329751: Pseudo dice [0.7277]
2024-12-18 10:31:16.330607: Epoch time: 472.3 s
2024-12-18 10:31:17.731152: 
2024-12-18 10:31:17.732507: Epoch 98
2024-12-18 10:31:17.733382: Current learning rate: 0.00385
2024-12-18 10:39:19.626048: Validation loss did not improve from -0.50391. Patience: 54/50
2024-12-18 10:39:19.627263: train_loss -0.8369
2024-12-18 10:39:19.628196: val_loss -0.4364
2024-12-18 10:39:19.628891: Pseudo dice [0.7149]
2024-12-18 10:39:19.629709: Epoch time: 481.9 s
2024-12-18 10:39:21.066533: 
2024-12-18 10:39:21.068112: Epoch 99
2024-12-18 10:39:21.068905: Current learning rate: 0.00379
2024-12-18 10:47:28.242871: Validation loss did not improve from -0.50391. Patience: 55/50
2024-12-18 10:47:28.243946: train_loss -0.8363
2024-12-18 10:47:28.244922: val_loss -0.4265
2024-12-18 10:47:28.245939: Pseudo dice [0.724]
2024-12-18 10:47:28.246996: Epoch time: 487.18 s
2024-12-18 10:47:30.188864: 
2024-12-18 10:47:30.190138: Epoch 100
2024-12-18 10:47:30.191213: Current learning rate: 0.00372
2024-12-18 10:55:22.129114: Validation loss did not improve from -0.50391. Patience: 56/50
2024-12-18 10:55:22.130346: train_loss -0.8352
2024-12-18 10:55:22.131202: val_loss -0.445
2024-12-18 10:55:22.132091: Pseudo dice [0.7169]
2024-12-18 10:55:22.133001: Epoch time: 471.94 s
2024-12-18 10:55:23.561279: 
2024-12-18 10:55:23.562455: Epoch 101
2024-12-18 10:55:23.563332: Current learning rate: 0.00365
2024-12-18 11:03:59.073930: Validation loss did not improve from -0.50391. Patience: 57/50
2024-12-18 11:03:59.074919: train_loss -0.8359
2024-12-18 11:03:59.075719: val_loss -0.4154
2024-12-18 11:03:59.076525: Pseudo dice [0.7136]
2024-12-18 11:03:59.077340: Epoch time: 515.52 s
2024-12-18 11:04:00.582862: 
2024-12-18 11:04:00.584194: Epoch 102
2024-12-18 11:04:00.585080: Current learning rate: 0.00359
2024-12-18 11:12:13.879394: Validation loss did not improve from -0.50391. Patience: 58/50
2024-12-18 11:12:13.880069: train_loss -0.8351
2024-12-18 11:12:13.880944: val_loss -0.4662
2024-12-18 11:12:13.881623: Pseudo dice [0.7329]
2024-12-18 11:12:13.882360: Epoch time: 493.3 s
2024-12-18 11:12:15.351154: 
2024-12-18 11:12:15.352412: Epoch 103
2024-12-18 11:12:15.353253: Current learning rate: 0.00352
2024-12-18 11:19:45.607295: Validation loss did not improve from -0.50391. Patience: 59/50
2024-12-18 11:19:45.608317: train_loss -0.8372
2024-12-18 11:19:45.609306: val_loss -0.439
2024-12-18 11:19:45.610043: Pseudo dice [0.7239]
2024-12-18 11:19:45.610956: Epoch time: 450.26 s
2024-12-18 11:19:47.067459: 
2024-12-18 11:19:47.068516: Epoch 104
2024-12-18 11:19:47.069332: Current learning rate: 0.00345
2024-12-18 11:27:08.737347: Validation loss did not improve from -0.50391. Patience: 60/50
2024-12-18 11:27:08.738855: train_loss -0.8377
2024-12-18 11:27:08.740064: val_loss -0.4659
2024-12-18 11:27:08.740830: Pseudo dice [0.7497]
2024-12-18 11:27:08.741608: Epoch time: 441.67 s
2024-12-18 11:27:10.630792: 
2024-12-18 11:27:10.632089: Epoch 105
2024-12-18 11:27:10.633014: Current learning rate: 0.00338
2024-12-18 11:35:22.127470: Validation loss did not improve from -0.50391. Patience: 61/50
2024-12-18 11:35:22.128615: train_loss -0.8371
2024-12-18 11:35:22.129523: val_loss -0.4393
2024-12-18 11:35:22.130320: Pseudo dice [0.7223]
2024-12-18 11:35:22.131027: Epoch time: 491.5 s
2024-12-18 11:35:23.590930: 
2024-12-18 11:35:23.592334: Epoch 106
2024-12-18 11:35:23.593299: Current learning rate: 0.00332
2024-12-18 11:43:34.409742: Validation loss did not improve from -0.50391. Patience: 62/50
2024-12-18 11:43:34.410709: train_loss -0.839
2024-12-18 11:43:34.411617: val_loss -0.4815
2024-12-18 11:43:34.412294: Pseudo dice [0.7453]
2024-12-18 11:43:34.412984: Epoch time: 490.82 s
2024-12-18 11:43:36.429666: 
2024-12-18 11:43:36.431035: Epoch 107
2024-12-18 11:43:36.431993: Current learning rate: 0.00325
2024-12-18 11:51:36.725482: Validation loss did not improve from -0.50391. Patience: 63/50
2024-12-18 11:51:36.726545: train_loss -0.8387
2024-12-18 11:51:36.727390: val_loss -0.4318
2024-12-18 11:51:36.728231: Pseudo dice [0.7242]
2024-12-18 11:51:36.729072: Epoch time: 480.3 s
2024-12-18 11:51:38.148407: 
2024-12-18 11:51:38.149673: Epoch 108
2024-12-18 11:51:38.150462: Current learning rate: 0.00318
2024-12-18 11:59:43.414227: Validation loss did not improve from -0.50391. Patience: 64/50
2024-12-18 11:59:43.415286: train_loss -0.838
2024-12-18 11:59:43.416168: val_loss -0.4634
2024-12-18 11:59:43.416874: Pseudo dice [0.7286]
2024-12-18 11:59:43.417612: Epoch time: 485.27 s
2024-12-18 11:59:44.835693: 
2024-12-18 11:59:44.836849: Epoch 109
2024-12-18 11:59:44.837620: Current learning rate: 0.00311
2024-12-18 12:07:33.354470: Validation loss did not improve from -0.50391. Patience: 65/50
2024-12-18 12:07:33.355582: train_loss -0.8393
2024-12-18 12:07:33.356497: val_loss -0.4807
2024-12-18 12:07:33.357239: Pseudo dice [0.7393]
2024-12-18 12:07:33.358174: Epoch time: 468.52 s
2024-12-18 12:07:33.815224: Yayy! New best EMA pseudo Dice: 0.728
2024-12-18 12:07:35.733321: 
2024-12-18 12:07:35.734745: Epoch 110
2024-12-18 12:07:35.735599: Current learning rate: 0.00304
2024-12-18 12:14:54.454852: Validation loss did not improve from -0.50391. Patience: 66/50
2024-12-18 12:14:54.455774: train_loss -0.8403
2024-12-18 12:14:54.456762: val_loss -0.4318
2024-12-18 12:14:54.457513: Pseudo dice [0.7123]
2024-12-18 12:14:54.458193: Epoch time: 438.72 s
2024-12-18 12:14:55.990716: 
2024-12-18 12:14:55.991995: Epoch 111
2024-12-18 12:14:55.992733: Current learning rate: 0.00297
2024-12-18 12:22:42.894740: Validation loss did not improve from -0.50391. Patience: 67/50
2024-12-18 12:22:42.896461: train_loss -0.8388
2024-12-18 12:22:42.897659: val_loss -0.4819
2024-12-18 12:22:42.898405: Pseudo dice [0.7451]
2024-12-18 12:22:42.899378: Epoch time: 466.91 s
2024-12-18 12:22:42.900511: Yayy! New best EMA pseudo Dice: 0.7283
2024-12-18 12:22:44.776813: 
2024-12-18 12:22:44.778412: Epoch 112
2024-12-18 12:22:44.779258: Current learning rate: 0.00291
2024-12-18 12:30:25.204125: Validation loss did not improve from -0.50391. Patience: 68/50
2024-12-18 12:30:25.205194: train_loss -0.8397
2024-12-18 12:30:25.206077: val_loss -0.4088
2024-12-18 12:30:25.207002: Pseudo dice [0.7109]
2024-12-18 12:30:25.207761: Epoch time: 460.43 s
2024-12-18 12:30:26.684625: 
2024-12-18 12:30:26.685857: Epoch 113
2024-12-18 12:30:26.686507: Current learning rate: 0.00284
2024-12-18 12:38:11.989108: Validation loss did not improve from -0.50391. Patience: 69/50
2024-12-18 12:38:11.991745: train_loss -0.8414
2024-12-18 12:38:11.992729: val_loss -0.4249
2024-12-18 12:38:11.993477: Pseudo dice [0.7241]
2024-12-18 12:38:11.994199: Epoch time: 465.31 s
2024-12-18 12:38:13.464697: 
2024-12-18 12:38:13.465840: Epoch 114
2024-12-18 12:38:13.466695: Current learning rate: 0.00277
2024-12-18 12:45:31.384307: Validation loss did not improve from -0.50391. Patience: 70/50
2024-12-18 12:45:31.385290: train_loss -0.8408
2024-12-18 12:45:31.386111: val_loss -0.458
2024-12-18 12:45:31.387076: Pseudo dice [0.7424]
2024-12-18 12:45:31.387950: Epoch time: 437.92 s
2024-12-18 12:45:33.218181: 
2024-12-18 12:45:33.219446: Epoch 115
2024-12-18 12:45:33.220189: Current learning rate: 0.0027
2024-12-18 12:53:10.403212: Validation loss did not improve from -0.50391. Patience: 71/50
2024-12-18 12:53:10.410184: train_loss -0.8419
2024-12-18 12:53:10.411378: val_loss -0.4024
2024-12-18 12:53:10.412069: Pseudo dice [0.7087]
2024-12-18 12:53:10.412874: Epoch time: 457.19 s
2024-12-18 12:53:11.923144: 
2024-12-18 12:53:11.924662: Epoch 116
2024-12-18 12:53:11.925465: Current learning rate: 0.00263
2024-12-18 13:00:15.921144: Validation loss did not improve from -0.50391. Patience: 72/50
2024-12-18 13:00:15.922197: train_loss -0.8428
2024-12-18 13:00:15.923032: val_loss -0.4539
2024-12-18 13:00:15.923827: Pseudo dice [0.7296]
2024-12-18 13:00:15.925302: Epoch time: 424.0 s
2024-12-18 13:00:17.423311: 
2024-12-18 13:00:17.424658: Epoch 117
2024-12-18 13:00:17.425455: Current learning rate: 0.00256
2024-12-18 13:07:27.983510: Validation loss did not improve from -0.50391. Patience: 73/50
2024-12-18 13:07:27.984877: train_loss -0.843
2024-12-18 13:07:27.985878: val_loss -0.4351
2024-12-18 13:07:27.986765: Pseudo dice [0.7155]
2024-12-18 13:07:27.987741: Epoch time: 430.56 s
2024-12-18 13:07:29.957567: 
2024-12-18 13:07:29.959147: Epoch 118
2024-12-18 13:07:29.960157: Current learning rate: 0.00249
2024-12-18 13:14:47.050305: Validation loss did not improve from -0.50391. Patience: 74/50
2024-12-18 13:14:47.051332: train_loss -0.8444
2024-12-18 13:14:47.052192: val_loss -0.4313
2024-12-18 13:14:47.052928: Pseudo dice [0.7268]
2024-12-18 13:14:47.053888: Epoch time: 437.1 s
2024-12-18 13:14:48.620143: 
2024-12-18 13:14:48.621704: Epoch 119
2024-12-18 13:14:48.622573: Current learning rate: 0.00242
2024-12-18 13:22:10.239720: Validation loss did not improve from -0.50391. Patience: 75/50
2024-12-18 13:22:10.240819: train_loss -0.845
2024-12-18 13:22:10.241605: val_loss -0.4178
2024-12-18 13:22:10.242358: Pseudo dice [0.7133]
2024-12-18 13:22:10.243051: Epoch time: 441.62 s
2024-12-18 13:22:12.154089: 
2024-12-18 13:22:12.155531: Epoch 120
2024-12-18 13:22:12.156269: Current learning rate: 0.00235
2024-12-18 13:28:54.033527: Validation loss did not improve from -0.50391. Patience: 76/50
2024-12-18 13:28:54.035479: train_loss -0.8425
2024-12-18 13:28:54.036545: val_loss -0.3999
2024-12-18 13:28:54.037326: Pseudo dice [0.7025]
2024-12-18 13:28:54.038043: Epoch time: 401.88 s
2024-12-18 13:28:55.540891: 
2024-12-18 13:28:55.542330: Epoch 121
2024-12-18 13:28:55.543216: Current learning rate: 0.00228
2024-12-18 13:36:10.682982: Validation loss did not improve from -0.50391. Patience: 77/50
2024-12-18 13:36:10.684062: train_loss -0.8426
2024-12-18 13:36:10.685092: val_loss -0.4388
2024-12-18 13:36:10.686114: Pseudo dice [0.7218]
2024-12-18 13:36:10.687080: Epoch time: 435.14 s
2024-12-18 13:36:12.261950: 
2024-12-18 13:36:12.263530: Epoch 122
2024-12-18 13:36:12.264530: Current learning rate: 0.00221
2024-12-18 13:43:37.845967: Validation loss did not improve from -0.50391. Patience: 78/50
2024-12-18 13:43:37.847215: train_loss -0.8448
2024-12-18 13:43:37.848260: val_loss -0.4313
2024-12-18 13:43:37.849296: Pseudo dice [0.7264]
2024-12-18 13:43:37.850260: Epoch time: 445.59 s
2024-12-18 13:43:39.323519: 
2024-12-18 13:43:39.325300: Epoch 123
2024-12-18 13:43:39.326445: Current learning rate: 0.00214
2024-12-18 13:51:16.061093: Validation loss did not improve from -0.50391. Patience: 79/50
2024-12-18 13:51:16.062187: train_loss -0.8449
2024-12-18 13:51:16.062919: val_loss -0.4115
2024-12-18 13:51:16.063561: Pseudo dice [0.7197]
2024-12-18 13:51:16.064231: Epoch time: 456.74 s
2024-12-18 13:51:17.540251: 
2024-12-18 13:51:17.541517: Epoch 124
2024-12-18 13:51:17.542314: Current learning rate: 0.00207
2024-12-18 13:58:39.567408: Validation loss did not improve from -0.50391. Patience: 80/50
2024-12-18 13:58:39.568259: train_loss -0.8454
2024-12-18 13:58:39.569068: val_loss -0.4067
2024-12-18 13:58:39.569891: Pseudo dice [0.7125]
2024-12-18 13:58:39.570757: Epoch time: 442.03 s
2024-12-18 13:58:41.470618: 
2024-12-18 13:58:41.471627: Epoch 125
2024-12-18 13:58:41.472412: Current learning rate: 0.00199
2024-12-18 14:05:55.517984: Validation loss did not improve from -0.50391. Patience: 81/50
2024-12-18 14:05:55.519118: train_loss -0.8442
2024-12-18 14:05:55.519944: val_loss -0.3956
2024-12-18 14:05:55.520677: Pseudo dice [0.7143]
2024-12-18 14:05:55.521518: Epoch time: 434.05 s
2024-12-18 14:05:57.003141: 
2024-12-18 14:05:57.004314: Epoch 126
2024-12-18 14:05:57.005117: Current learning rate: 0.00192
2024-12-18 14:13:45.431941: Validation loss did not improve from -0.50391. Patience: 82/50
2024-12-18 14:13:45.432879: train_loss -0.8467
2024-12-18 14:13:45.433704: val_loss -0.4586
2024-12-18 14:13:45.434568: Pseudo dice [0.7327]
2024-12-18 14:13:45.435382: Epoch time: 468.43 s
2024-12-18 14:13:46.880519: 
2024-12-18 14:13:46.881929: Epoch 127
2024-12-18 14:13:46.882737: Current learning rate: 0.00185
2024-12-18 14:20:28.227855: Validation loss did not improve from -0.50391. Patience: 83/50
2024-12-18 14:20:28.228997: train_loss -0.8463
2024-12-18 14:20:28.229898: val_loss -0.4388
2024-12-18 14:20:28.230732: Pseudo dice [0.7293]
2024-12-18 14:20:28.231505: Epoch time: 401.35 s
2024-12-18 14:20:30.160789: 
2024-12-18 14:20:30.162344: Epoch 128
2024-12-18 14:20:30.163316: Current learning rate: 0.00178
2024-12-18 14:28:03.724296: Validation loss did not improve from -0.50391. Patience: 84/50
2024-12-18 14:28:03.725484: train_loss -0.8496
2024-12-18 14:28:03.726391: val_loss -0.4378
2024-12-18 14:28:03.727143: Pseudo dice [0.7268]
2024-12-18 14:28:03.727911: Epoch time: 453.57 s
2024-12-18 14:28:05.190457: 
2024-12-18 14:28:05.191970: Epoch 129
2024-12-18 14:28:05.192848: Current learning rate: 0.0017
2024-12-18 14:35:00.259626: Validation loss did not improve from -0.50391. Patience: 85/50
2024-12-18 14:35:00.260646: train_loss -0.8481
2024-12-18 14:35:00.262089: val_loss -0.4065
2024-12-18 14:35:00.262927: Pseudo dice [0.7146]
2024-12-18 14:35:00.263781: Epoch time: 415.07 s
2024-12-18 14:35:02.180630: 
2024-12-18 14:35:02.182033: Epoch 130
2024-12-18 14:35:02.182920: Current learning rate: 0.00163
2024-12-18 14:42:17.604014: Validation loss did not improve from -0.50391. Patience: 86/50
2024-12-18 14:42:17.605248: train_loss -0.848
2024-12-18 14:42:17.606208: val_loss -0.3715
2024-12-18 14:42:17.607040: Pseudo dice [0.7012]
2024-12-18 14:42:17.607921: Epoch time: 435.43 s
2024-12-18 14:42:19.116745: 
2024-12-18 14:42:19.118128: Epoch 131
2024-12-18 14:42:19.119003: Current learning rate: 0.00156
2024-12-18 14:49:20.426489: Validation loss did not improve from -0.50391. Patience: 87/50
2024-12-18 14:49:20.427547: train_loss -0.8495
2024-12-18 14:49:20.428260: val_loss -0.3874
2024-12-18 14:49:20.428960: Pseudo dice [0.7032]
2024-12-18 14:49:20.429731: Epoch time: 421.31 s
2024-12-18 14:49:21.887750: 
2024-12-18 14:49:21.889225: Epoch 132
2024-12-18 14:49:21.890013: Current learning rate: 0.00148
2024-12-18 14:56:22.870432: Validation loss did not improve from -0.50391. Patience: 88/50
2024-12-18 14:56:22.871518: train_loss -0.8478
2024-12-18 14:56:22.872378: val_loss -0.4171
2024-12-18 14:56:22.873159: Pseudo dice [0.7251]
2024-12-18 14:56:22.873929: Epoch time: 420.99 s
2024-12-18 14:56:24.377821: 
2024-12-18 14:56:24.379175: Epoch 133
2024-12-18 14:56:24.379961: Current learning rate: 0.00141
2024-12-18 15:03:40.528202: Validation loss did not improve from -0.50391. Patience: 89/50
2024-12-18 15:03:40.528973: train_loss -0.8488
2024-12-18 15:03:40.529683: val_loss -0.4224
2024-12-18 15:03:40.530393: Pseudo dice [0.7126]
2024-12-18 15:03:40.531272: Epoch time: 436.15 s
2024-12-18 15:03:42.025930: 
2024-12-18 15:03:42.027155: Epoch 134
2024-12-18 15:03:42.027969: Current learning rate: 0.00133
2024-12-18 15:10:50.201044: Validation loss did not improve from -0.50391. Patience: 90/50
2024-12-18 15:10:50.202153: train_loss -0.8507
2024-12-18 15:10:50.202977: val_loss -0.4466
2024-12-18 15:10:50.203773: Pseudo dice [0.7341]
2024-12-18 15:10:50.204583: Epoch time: 428.18 s
2024-12-18 15:10:52.203096: 
2024-12-18 15:10:52.204591: Epoch 135
2024-12-18 15:10:52.205544: Current learning rate: 0.00126
2024-12-18 15:18:04.206721: Validation loss did not improve from -0.50391. Patience: 91/50
2024-12-18 15:18:04.207924: train_loss -0.849
2024-12-18 15:18:04.208932: val_loss -0.4085
2024-12-18 15:18:04.209709: Pseudo dice [0.7133]
2024-12-18 15:18:04.210469: Epoch time: 432.01 s
2024-12-18 15:18:05.704492: 
2024-12-18 15:18:05.705767: Epoch 136
2024-12-18 15:18:05.706531: Current learning rate: 0.00118
2024-12-18 15:25:11.139137: Validation loss did not improve from -0.50391. Patience: 92/50
2024-12-18 15:25:11.140521: train_loss -0.8484
2024-12-18 15:25:11.141779: val_loss -0.4136
2024-12-18 15:25:11.142611: Pseudo dice [0.7159]
2024-12-18 15:25:11.143509: Epoch time: 425.44 s
2024-12-18 15:25:12.640008: 
2024-12-18 15:25:12.640990: Epoch 137
2024-12-18 15:25:12.641792: Current learning rate: 0.00111
2024-12-18 15:32:15.677878: Validation loss did not improve from -0.50391. Patience: 93/50
2024-12-18 15:32:15.678603: train_loss -0.8522
2024-12-18 15:32:15.679389: val_loss -0.4264
2024-12-18 15:32:15.680120: Pseudo dice [0.7326]
2024-12-18 15:32:15.680879: Epoch time: 423.04 s
2024-12-18 15:32:17.202836: 
2024-12-18 15:32:17.204371: Epoch 138
2024-12-18 15:32:17.205215: Current learning rate: 0.00103
2024-12-18 15:40:01.388833: Validation loss did not improve from -0.50391. Patience: 94/50
2024-12-18 15:40:01.390934: train_loss -0.8526
2024-12-18 15:40:01.391906: val_loss -0.4804
2024-12-18 15:40:01.392750: Pseudo dice [0.7412]
2024-12-18 15:40:01.393561: Epoch time: 464.19 s
2024-12-18 15:40:03.496675: 
2024-12-18 15:40:03.498454: Epoch 139
2024-12-18 15:40:03.499823: Current learning rate: 0.00095
2024-12-18 15:46:44.063537: Validation loss did not improve from -0.50391. Patience: 95/50
2024-12-18 15:46:44.064575: train_loss -0.8507
2024-12-18 15:46:44.065794: val_loss -0.4123
2024-12-18 15:46:44.066771: Pseudo dice [0.7202]
2024-12-18 15:46:44.067702: Epoch time: 400.57 s
2024-12-18 15:46:45.963389: 
2024-12-18 15:46:45.964958: Epoch 140
2024-12-18 15:46:45.965934: Current learning rate: 0.00087
2024-12-18 15:54:15.506641: Validation loss did not improve from -0.50391. Patience: 96/50
2024-12-18 15:54:15.507549: train_loss -0.8507
2024-12-18 15:54:15.508366: val_loss -0.4253
2024-12-18 15:54:15.509125: Pseudo dice [0.7295]
2024-12-18 15:54:15.509986: Epoch time: 449.55 s
2024-12-18 15:54:16.952002: 
2024-12-18 15:54:16.953098: Epoch 141
2024-12-18 15:54:16.953835: Current learning rate: 0.00079
2024-12-18 16:01:16.156697: Validation loss did not improve from -0.50391. Patience: 97/50
2024-12-18 16:01:16.157729: train_loss -0.8503
2024-12-18 16:01:16.158555: val_loss -0.4432
2024-12-18 16:01:16.159261: Pseudo dice [0.7338]
2024-12-18 16:01:16.159955: Epoch time: 419.21 s
2024-12-18 16:01:17.606089: 
2024-12-18 16:01:17.607146: Epoch 142
2024-12-18 16:01:17.607927: Current learning rate: 0.00071
2024-12-18 16:09:05.231212: Validation loss did not improve from -0.50391. Patience: 98/50
2024-12-18 16:09:05.232144: train_loss -0.8507
2024-12-18 16:09:05.232950: val_loss -0.4423
2024-12-18 16:09:05.233789: Pseudo dice [0.7309]
2024-12-18 16:09:05.234580: Epoch time: 467.63 s
2024-12-18 16:09:06.692632: 
2024-12-18 16:09:06.694246: Epoch 143
2024-12-18 16:09:06.695197: Current learning rate: 0.00063
2024-12-18 16:13:47.179882: Validation loss did not improve from -0.50391. Patience: 99/50
2024-12-18 16:13:47.180871: train_loss -0.8535
2024-12-18 16:13:47.181819: val_loss -0.445
2024-12-18 16:13:47.182455: Pseudo dice [0.7322]
2024-12-18 16:13:47.183124: Epoch time: 280.49 s
2024-12-18 16:13:48.637914: 
2024-12-18 16:13:48.639165: Epoch 144
2024-12-18 16:13:48.639855: Current learning rate: 0.00055
2024-12-18 16:19:06.160527: Validation loss did not improve from -0.50391. Patience: 100/50
2024-12-18 16:19:06.161191: train_loss -0.8506
2024-12-18 16:19:06.161923: val_loss -0.4177
2024-12-18 16:19:06.162590: Pseudo dice [0.7218]
2024-12-18 16:19:06.163241: Epoch time: 317.52 s
2024-12-18 16:19:07.974757: 
2024-12-18 16:19:07.975789: Epoch 145
2024-12-18 16:19:07.976472: Current learning rate: 0.00047
2024-12-18 16:24:11.643352: Validation loss did not improve from -0.50391. Patience: 101/50
2024-12-18 16:24:11.644528: train_loss -0.8512
2024-12-18 16:24:11.645300: val_loss -0.4343
2024-12-18 16:24:11.645975: Pseudo dice [0.7304]
2024-12-18 16:24:11.646714: Epoch time: 303.67 s
2024-12-18 16:24:13.186846: 
2024-12-18 16:24:13.188139: Epoch 146
2024-12-18 16:24:13.188941: Current learning rate: 0.00038
2024-12-18 16:29:27.263311: Validation loss did not improve from -0.50391. Patience: 102/50
2024-12-18 16:29:27.264203: train_loss -0.8525
2024-12-18 16:29:27.265182: val_loss -0.388
2024-12-18 16:29:27.266211: Pseudo dice [0.7061]
2024-12-18 16:29:27.267270: Epoch time: 314.08 s
2024-12-18 16:29:28.871688: 
2024-12-18 16:29:28.873061: Epoch 147
2024-12-18 16:29:28.874050: Current learning rate: 0.0003
2024-12-18 16:36:38.337483: Validation loss did not improve from -0.50391. Patience: 103/50
2024-12-18 16:36:38.338464: train_loss -0.8527
2024-12-18 16:36:38.339225: val_loss -0.4233
2024-12-18 16:36:38.339953: Pseudo dice [0.7184]
2024-12-18 16:36:38.340615: Epoch time: 429.47 s
2024-12-18 16:36:39.802066: 
2024-12-18 16:36:39.803179: Epoch 148
2024-12-18 16:36:39.803870: Current learning rate: 0.00021
2024-12-18 16:44:13.703482: Validation loss did not improve from -0.50391. Patience: 104/50
2024-12-18 16:44:13.704553: train_loss -0.8536
2024-12-18 16:44:13.705499: val_loss -0.4042
2024-12-18 16:44:13.706238: Pseudo dice [0.718]
2024-12-18 16:44:13.706914: Epoch time: 453.9 s
2024-12-18 16:44:15.171331: 
2024-12-18 16:44:15.172756: Epoch 149
2024-12-18 16:44:15.173715: Current learning rate: 0.00011
2024-12-18 16:51:27.570086: Validation loss did not improve from -0.50391. Patience: 105/50
2024-12-18 16:51:27.571100: train_loss -0.8518
2024-12-18 16:51:27.571923: val_loss -0.4218
2024-12-18 16:51:27.572617: Pseudo dice [0.7239]
2024-12-18 16:51:27.573325: Epoch time: 432.4 s
2024-12-18 16:51:29.864873: Training done.
2024-12-18 16:51:30.092247: Using splits from existing split file: /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset309_Sohee_Calcium_OCT_CrossValidation/splits_final_40.json
2024-12-18 16:51:30.107619: The split file contains 5 splits.
2024-12-18 16:51:30.108674: Desired fold for training: 1
2024-12-18 16:51:30.110028: This split has 3 training and 6 validation cases.
2024-12-18 16:51:30.111433: predicting 101-019
2024-12-18 16:51:30.164289: 101-019, shape torch.Size([1, 375, 498, 498]), rank 0
2024-12-18 16:54:40.688851: predicting 101-044
2024-12-18 16:54:40.757154: 101-044, shape torch.Size([1, 404, 498, 498]), rank 0
2024-12-18 16:56:32.378113: predicting 101-045
2024-12-18 16:56:32.407639: 101-045, shape torch.Size([1, 375, 498, 498]), rank 0
2024-12-18 16:58:31.908585: predicting 106-002
2024-12-18 16:58:31.925671: 106-002, shape torch.Size([1, 539, 498, 498]), rank 0
2024-12-18 17:01:40.038965: predicting 704-003
2024-12-18 17:01:40.053244: 704-003, shape torch.Size([1, 375, 498, 498]), rank 0
2024-12-18 17:03:40.668270: predicting 706-005
2024-12-18 17:03:40.696244: 706-005, shape torch.Size([1, 375, 498, 498]), rank 0
2024-12-18 17:06:03.937743: Validation complete
2024-12-18 17:06:03.938263: Mean Validation Dice:  0.7173867572447151
2024-12-17 21:21:22.601858: unpacking done...
2024-12-17 21:21:22.822969: Unable to plot network architecture: nnUNet_compile is enabled!
2024-12-17 21:21:23.004807: 
2024-12-17 21:21:23.006354: Epoch 0
2024-12-17 21:21:23.007542: Current learning rate: 0.01
2024-12-17 21:29:12.138604: Validation loss improved from 1000.00000 to -0.41342! Patience: 0/50
2024-12-17 21:29:12.139825: train_loss -0.3013
2024-12-17 21:29:12.140824: val_loss -0.4134
2024-12-17 21:29:12.141757: Pseudo dice [0.6764]
2024-12-17 21:29:12.142864: Epoch time: 469.14 s
2024-12-17 21:29:12.143845: Yayy! New best EMA pseudo Dice: 0.6764
2024-12-17 21:29:13.848860: 
2024-12-17 21:29:13.850504: Epoch 1
2024-12-17 21:29:13.851906: Current learning rate: 0.00994
2024-12-17 21:35:22.085230: Validation loss did not improve from -0.41342. Patience: 1/50
2024-12-17 21:35:22.086274: train_loss -0.5228
2024-12-17 21:35:22.087190: val_loss -0.3602
2024-12-17 21:35:22.088056: Pseudo dice [0.6431]
2024-12-17 21:35:22.088892: Epoch time: 368.24 s
2024-12-17 21:35:23.503556: 
2024-12-17 21:35:23.504963: Epoch 2
2024-12-17 21:35:23.505828: Current learning rate: 0.00988
2024-12-17 21:43:35.300249: Validation loss did not improve from -0.41342. Patience: 2/50
2024-12-17 21:43:35.301264: train_loss -0.5698
2024-12-17 21:43:35.302144: val_loss -0.3959
2024-12-17 21:43:35.302830: Pseudo dice [0.6702]
2024-12-17 21:43:35.303528: Epoch time: 491.8 s
2024-12-17 21:43:36.820785: 
2024-12-17 21:43:36.822222: Epoch 3
2024-12-17 21:43:36.823115: Current learning rate: 0.00982
2024-12-17 21:51:46.820642: Validation loss did not improve from -0.41342. Patience: 3/50
2024-12-17 21:51:46.821395: train_loss -0.5792
2024-12-17 21:51:46.822172: val_loss -0.4097
2024-12-17 21:51:46.823018: Pseudo dice [0.6779]
2024-12-17 21:51:46.823738: Epoch time: 490.0 s
2024-12-17 21:51:48.241050: 
2024-12-17 21:51:48.242403: Epoch 4
2024-12-17 21:51:48.243230: Current learning rate: 0.00976
2024-12-17 21:59:13.393059: Validation loss did not improve from -0.41342. Patience: 4/50
2024-12-17 21:59:13.394060: train_loss -0.6049
2024-12-17 21:59:13.394949: val_loss -0.3924
2024-12-17 21:59:13.395899: Pseudo dice [0.6694]
2024-12-17 21:59:13.396898: Epoch time: 445.15 s
2024-12-17 21:59:15.302453: 
2024-12-17 21:59:15.303957: Epoch 5
2024-12-17 21:59:15.305117: Current learning rate: 0.0097
2024-12-17 22:07:47.226001: Validation loss improved from -0.41342 to -0.46040! Patience: 4/50
2024-12-17 22:07:47.226971: train_loss -0.6325
2024-12-17 22:07:47.227818: val_loss -0.4604
2024-12-17 22:07:47.228603: Pseudo dice [0.7115]
2024-12-17 22:07:47.229450: Epoch time: 511.93 s
2024-12-17 22:07:47.230324: Yayy! New best EMA pseudo Dice: 0.6768
2024-12-17 22:07:49.126770: 
2024-12-17 22:07:49.128083: Epoch 6
2024-12-17 22:07:49.129017: Current learning rate: 0.00964
2024-12-17 22:16:27.416587: Validation loss improved from -0.46040 to -0.46319! Patience: 0/50
2024-12-17 22:16:27.417581: train_loss -0.6531
2024-12-17 22:16:27.418442: val_loss -0.4632
2024-12-17 22:16:27.419254: Pseudo dice [0.7151]
2024-12-17 22:16:27.420014: Epoch time: 518.29 s
2024-12-17 22:16:27.420852: Yayy! New best EMA pseudo Dice: 0.6806
2024-12-17 22:16:29.287070: 
2024-12-17 22:16:29.288275: Epoch 7
2024-12-17 22:16:29.289050: Current learning rate: 0.00958
2024-12-17 22:25:10.766859: Validation loss improved from -0.46319 to -0.47429! Patience: 0/50
2024-12-17 22:25:10.767874: train_loss -0.6579
2024-12-17 22:25:10.768932: val_loss -0.4743
2024-12-17 22:25:10.769959: Pseudo dice [0.7253]
2024-12-17 22:25:10.770842: Epoch time: 521.48 s
2024-12-17 22:25:10.771827: Yayy! New best EMA pseudo Dice: 0.6851
2024-12-17 22:25:12.711888: 
2024-12-17 22:25:12.713093: Epoch 8
2024-12-17 22:25:12.714065: Current learning rate: 0.00952
2024-12-17 22:33:38.999341: Validation loss did not improve from -0.47429. Patience: 1/50
2024-12-17 22:33:39.000587: train_loss -0.6659
2024-12-17 22:33:39.001389: val_loss -0.4519
2024-12-17 22:33:39.002125: Pseudo dice [0.7076]
2024-12-17 22:33:39.002830: Epoch time: 506.29 s
2024-12-17 22:33:39.003538: Yayy! New best EMA pseudo Dice: 0.6873
2024-12-17 22:33:41.635790: 
2024-12-17 22:33:41.637276: Epoch 9
2024-12-17 22:33:41.638016: Current learning rate: 0.00946
2024-12-17 22:41:52.734559: Validation loss did not improve from -0.47429. Patience: 2/50
2024-12-17 22:41:52.735432: train_loss -0.6692
2024-12-17 22:41:52.736263: val_loss -0.4718
2024-12-17 22:41:52.736942: Pseudo dice [0.7302]
2024-12-17 22:41:52.737640: Epoch time: 491.1 s
2024-12-17 22:41:53.108936: Yayy! New best EMA pseudo Dice: 0.6916
2024-12-17 22:41:54.947411: 
2024-12-17 22:41:54.948871: Epoch 10
2024-12-17 22:41:54.949631: Current learning rate: 0.0094
2024-12-17 22:49:56.093444: Validation loss improved from -0.47429 to -0.49681! Patience: 2/50
2024-12-17 22:49:56.094461: train_loss -0.6792
2024-12-17 22:49:56.095464: val_loss -0.4968
2024-12-17 22:49:56.096401: Pseudo dice [0.7321]
2024-12-17 22:49:56.097383: Epoch time: 481.15 s
2024-12-17 22:49:56.098335: Yayy! New best EMA pseudo Dice: 0.6957
2024-12-17 22:49:57.963275: 
2024-12-17 22:49:57.964836: Epoch 11
2024-12-17 22:49:57.965803: Current learning rate: 0.00934
2024-12-17 22:58:20.182444: Validation loss did not improve from -0.49681. Patience: 1/50
2024-12-17 22:58:20.183204: train_loss -0.6882
2024-12-17 22:58:20.187442: val_loss -0.4076
2024-12-17 22:58:20.188282: Pseudo dice [0.6764]
2024-12-17 22:58:20.189181: Epoch time: 502.22 s
2024-12-17 22:58:21.609795: 
2024-12-17 22:58:21.611435: Epoch 12
2024-12-17 22:58:21.612608: Current learning rate: 0.00928
2024-12-17 23:06:27.521315: Validation loss did not improve from -0.49681. Patience: 2/50
2024-12-17 23:06:27.522570: train_loss -0.6945
2024-12-17 23:06:27.523792: val_loss -0.4797
2024-12-17 23:06:27.524828: Pseudo dice [0.7253]
2024-12-17 23:06:27.525771: Epoch time: 485.91 s
2024-12-17 23:06:27.526609: Yayy! New best EMA pseudo Dice: 0.6969
2024-12-17 23:06:29.443648: 
2024-12-17 23:06:29.445174: Epoch 13
2024-12-17 23:06:29.446141: Current learning rate: 0.00922
2024-12-17 23:15:06.205299: Validation loss did not improve from -0.49681. Patience: 3/50
2024-12-17 23:15:06.206316: train_loss -0.7016
2024-12-17 23:15:06.207290: val_loss -0.4564
2024-12-17 23:15:06.208138: Pseudo dice [0.7149]
2024-12-17 23:15:06.209215: Epoch time: 516.76 s
2024-12-17 23:15:06.210184: Yayy! New best EMA pseudo Dice: 0.6987
2024-12-17 23:15:08.094738: 
2024-12-17 23:15:08.096147: Epoch 14
2024-12-17 23:15:08.097061: Current learning rate: 0.00916
2024-12-17 23:23:40.412884: Validation loss did not improve from -0.49681. Patience: 4/50
2024-12-17 23:23:40.415117: train_loss -0.7024
2024-12-17 23:23:40.416194: val_loss -0.4684
2024-12-17 23:23:40.417056: Pseudo dice [0.7171]
2024-12-17 23:23:40.417913: Epoch time: 512.32 s
2024-12-17 23:23:40.831611: Yayy! New best EMA pseudo Dice: 0.7005
2024-12-17 23:23:42.680321: 
2024-12-17 23:23:42.681472: Epoch 15
2024-12-17 23:23:42.682190: Current learning rate: 0.0091
2024-12-17 23:32:01.661496: Validation loss did not improve from -0.49681. Patience: 5/50
2024-12-17 23:32:01.662831: train_loss -0.7066
2024-12-17 23:32:01.663804: val_loss -0.4392
2024-12-17 23:32:01.664573: Pseudo dice [0.7092]
2024-12-17 23:32:01.665293: Epoch time: 498.98 s
2024-12-17 23:32:01.665901: Yayy! New best EMA pseudo Dice: 0.7014
2024-12-17 23:32:03.505922: 
2024-12-17 23:32:03.507315: Epoch 16
2024-12-17 23:32:03.508132: Current learning rate: 0.00903
2024-12-17 23:40:55.564142: Validation loss did not improve from -0.49681. Patience: 6/50
2024-12-17 23:40:55.565863: train_loss -0.7116
2024-12-17 23:40:55.566837: val_loss -0.4882
2024-12-17 23:40:55.567645: Pseudo dice [0.7392]
2024-12-17 23:40:55.568361: Epoch time: 532.06 s
2024-12-17 23:40:55.569108: Yayy! New best EMA pseudo Dice: 0.7052
2024-12-17 23:40:57.504250: 
2024-12-17 23:40:57.505701: Epoch 17
2024-12-17 23:40:57.506530: Current learning rate: 0.00897
2024-12-17 23:49:37.206733: Validation loss did not improve from -0.49681. Patience: 7/50
2024-12-17 23:49:37.207788: train_loss -0.7139
2024-12-17 23:49:37.208558: val_loss -0.4406
2024-12-17 23:49:37.209224: Pseudo dice [0.709]
2024-12-17 23:49:37.210076: Epoch time: 519.71 s
2024-12-17 23:49:37.210973: Yayy! New best EMA pseudo Dice: 0.7056
2024-12-17 23:49:39.029185: 
2024-12-17 23:49:39.030683: Epoch 18
2024-12-17 23:49:39.031440: Current learning rate: 0.00891
2024-12-17 23:58:46.137587: Validation loss did not improve from -0.49681. Patience: 8/50
2024-12-17 23:58:46.138581: train_loss -0.7209
2024-12-17 23:58:46.139508: val_loss -0.4601
2024-12-17 23:58:46.140282: Pseudo dice [0.7063]
2024-12-17 23:58:46.141139: Epoch time: 547.11 s
2024-12-17 23:58:46.141862: Yayy! New best EMA pseudo Dice: 0.7056
2024-12-17 23:58:48.086824: 
2024-12-17 23:58:48.088231: Epoch 19
2024-12-17 23:58:48.089115: Current learning rate: 0.00885
2024-12-18 00:07:34.285307: Validation loss did not improve from -0.49681. Patience: 9/50
2024-12-18 00:07:34.286391: train_loss -0.7232
2024-12-18 00:07:34.287293: val_loss -0.4145
2024-12-18 00:07:34.288226: Pseudo dice [0.6959]
2024-12-18 00:07:34.289050: Epoch time: 526.2 s
2024-12-18 00:07:36.164406: 
2024-12-18 00:07:36.165934: Epoch 20
2024-12-18 00:07:36.166960: Current learning rate: 0.00879
2024-12-18 00:16:30.897025: Validation loss did not improve from -0.49681. Patience: 10/50
2024-12-18 00:16:30.898101: train_loss -0.7249
2024-12-18 00:16:30.898853: val_loss -0.4373
2024-12-18 00:16:30.899487: Pseudo dice [0.7083]
2024-12-18 00:16:30.900197: Epoch time: 534.74 s
2024-12-18 00:16:32.403781: 
2024-12-18 00:16:32.405175: Epoch 21
2024-12-18 00:16:32.405938: Current learning rate: 0.00873
2024-12-18 00:25:21.083173: Validation loss did not improve from -0.49681. Patience: 11/50
2024-12-18 00:25:21.084051: train_loss -0.733
2024-12-18 00:25:21.084874: val_loss -0.4499
2024-12-18 00:25:21.085674: Pseudo dice [0.7129]
2024-12-18 00:25:21.086597: Epoch time: 528.68 s
2024-12-18 00:25:21.087468: Yayy! New best EMA pseudo Dice: 0.7058
2024-12-18 00:25:22.868939: 
2024-12-18 00:25:22.870520: Epoch 22
2024-12-18 00:25:22.871565: Current learning rate: 0.00867
2024-12-18 00:34:27.065782: Validation loss did not improve from -0.49681. Patience: 12/50
2024-12-18 00:34:27.066735: train_loss -0.732
2024-12-18 00:34:27.067593: val_loss -0.4579
2024-12-18 00:34:27.068335: Pseudo dice [0.7224]
2024-12-18 00:34:27.069165: Epoch time: 544.2 s
2024-12-18 00:34:27.069918: Yayy! New best EMA pseudo Dice: 0.7075
2024-12-18 00:34:28.924996: 
2024-12-18 00:34:28.926388: Epoch 23
2024-12-18 00:34:28.927452: Current learning rate: 0.00861
2024-12-18 00:43:05.753596: Validation loss did not improve from -0.49681. Patience: 13/50
2024-12-18 00:43:05.754762: train_loss -0.7368
2024-12-18 00:43:05.755742: val_loss -0.4518
2024-12-18 00:43:05.756705: Pseudo dice [0.7217]
2024-12-18 00:43:05.757643: Epoch time: 516.83 s
2024-12-18 00:43:05.758524: Yayy! New best EMA pseudo Dice: 0.7089
2024-12-18 00:43:07.582593: 
2024-12-18 00:43:07.584067: Epoch 24
2024-12-18 00:43:07.585013: Current learning rate: 0.00855
2024-12-18 00:51:24.644164: Validation loss did not improve from -0.49681. Patience: 14/50
2024-12-18 00:51:24.645262: train_loss -0.7453
2024-12-18 00:51:24.646076: val_loss -0.484
2024-12-18 00:51:24.646785: Pseudo dice [0.723]
2024-12-18 00:51:24.647631: Epoch time: 497.06 s
2024-12-18 00:51:25.102190: Yayy! New best EMA pseudo Dice: 0.7103
2024-12-18 00:51:26.932136: 
2024-12-18 00:51:26.933540: Epoch 25
2024-12-18 00:51:26.934276: Current learning rate: 0.00849
2024-12-18 00:59:59.786810: Validation loss did not improve from -0.49681. Patience: 15/50
2024-12-18 00:59:59.787667: train_loss -0.7443
2024-12-18 00:59:59.788664: val_loss -0.4512
2024-12-18 00:59:59.789549: Pseudo dice [0.7084]
2024-12-18 00:59:59.790597: Epoch time: 512.86 s
2024-12-18 01:00:01.196254: 
2024-12-18 01:00:01.197405: Epoch 26
2024-12-18 01:00:01.198340: Current learning rate: 0.00843
2024-12-18 01:08:15.175969: Validation loss did not improve from -0.49681. Patience: 16/50
2024-12-18 01:08:15.176973: train_loss -0.7459
2024-12-18 01:08:15.177961: val_loss -0.4529
2024-12-18 01:08:15.178753: Pseudo dice [0.7197]
2024-12-18 01:08:15.179789: Epoch time: 493.98 s
2024-12-18 01:08:15.180793: Yayy! New best EMA pseudo Dice: 0.7111
2024-12-18 01:08:16.992960: 
2024-12-18 01:08:16.994417: Epoch 27
2024-12-18 01:08:16.995526: Current learning rate: 0.00836
2024-12-18 01:16:43.853889: Validation loss did not improve from -0.49681. Patience: 17/50
2024-12-18 01:16:43.854686: train_loss -0.7515
2024-12-18 01:16:43.855410: val_loss -0.4589
2024-12-18 01:16:43.856260: Pseudo dice [0.7244]
2024-12-18 01:16:43.856950: Epoch time: 506.86 s
2024-12-18 01:16:43.857560: Yayy! New best EMA pseudo Dice: 0.7124
2024-12-18 01:16:45.742473: 
2024-12-18 01:16:45.743432: Epoch 28
2024-12-18 01:16:45.744103: Current learning rate: 0.0083
2024-12-18 01:24:37.287583: Validation loss did not improve from -0.49681. Patience: 18/50
2024-12-18 01:24:37.288390: train_loss -0.7478
2024-12-18 01:24:37.289092: val_loss -0.4421
2024-12-18 01:24:37.289791: Pseudo dice [0.7116]
2024-12-18 01:24:37.290499: Epoch time: 471.55 s
2024-12-18 01:24:38.716287: 
2024-12-18 01:24:38.717530: Epoch 29
2024-12-18 01:24:38.718295: Current learning rate: 0.00824
2024-12-18 01:32:39.414601: Validation loss did not improve from -0.49681. Patience: 19/50
2024-12-18 01:32:39.416610: train_loss -0.7542
2024-12-18 01:32:39.417486: val_loss -0.4562
2024-12-18 01:32:39.418385: Pseudo dice [0.7132]
2024-12-18 01:32:39.419123: Epoch time: 480.7 s
2024-12-18 01:32:39.842464: Yayy! New best EMA pseudo Dice: 0.7124
2024-12-18 01:32:42.101397: 
2024-12-18 01:32:42.102822: Epoch 30
2024-12-18 01:32:42.103587: Current learning rate: 0.00818
2024-12-18 01:40:58.816774: Validation loss did not improve from -0.49681. Patience: 20/50
2024-12-18 01:40:58.817848: train_loss -0.7582
2024-12-18 01:40:58.818774: val_loss -0.4534
2024-12-18 01:40:58.819527: Pseudo dice [0.7269]
2024-12-18 01:40:58.820253: Epoch time: 496.72 s
2024-12-18 01:40:58.821014: Yayy! New best EMA pseudo Dice: 0.7139
2024-12-18 01:41:00.649769: 
2024-12-18 01:41:00.651198: Epoch 31
2024-12-18 01:41:00.652041: Current learning rate: 0.00812
2024-12-18 01:49:12.800985: Validation loss did not improve from -0.49681. Patience: 21/50
2024-12-18 01:49:12.802119: train_loss -0.7574
2024-12-18 01:49:12.803206: val_loss -0.4309
2024-12-18 01:49:12.804135: Pseudo dice [0.7003]
2024-12-18 01:49:12.805088: Epoch time: 492.15 s
2024-12-18 01:49:14.339217: 
2024-12-18 01:49:14.340581: Epoch 32
2024-12-18 01:49:14.341572: Current learning rate: 0.00806
2024-12-18 01:57:35.070823: Validation loss did not improve from -0.49681. Patience: 22/50
2024-12-18 01:57:35.072160: train_loss -0.7619
2024-12-18 01:57:35.073132: val_loss -0.4679
2024-12-18 01:57:35.074015: Pseudo dice [0.7266]
2024-12-18 01:57:35.074863: Epoch time: 500.73 s
2024-12-18 01:57:35.075691: Yayy! New best EMA pseudo Dice: 0.7139
2024-12-18 01:57:36.943694: 
2024-12-18 01:57:36.945028: Epoch 33
2024-12-18 01:57:36.945968: Current learning rate: 0.008
2024-12-18 02:05:34.895874: Validation loss did not improve from -0.49681. Patience: 23/50
2024-12-18 02:05:34.896782: train_loss -0.757
2024-12-18 02:05:34.897580: val_loss -0.4211
2024-12-18 02:05:34.898302: Pseudo dice [0.6975]
2024-12-18 02:05:34.899127: Epoch time: 477.95 s
2024-12-18 02:05:36.389033: 
2024-12-18 02:05:36.390440: Epoch 34
2024-12-18 02:05:36.391305: Current learning rate: 0.00793
2024-12-18 02:13:29.470073: Validation loss did not improve from -0.49681. Patience: 24/50
2024-12-18 02:13:29.471316: train_loss -0.7597
2024-12-18 02:13:29.472517: val_loss -0.4016
2024-12-18 02:13:29.473609: Pseudo dice [0.6945]
2024-12-18 02:13:29.474643: Epoch time: 473.08 s
2024-12-18 02:13:31.322685: 
2024-12-18 02:13:31.324383: Epoch 35
2024-12-18 02:13:31.325548: Current learning rate: 0.00787
2024-12-18 02:20:25.112390: Validation loss did not improve from -0.49681. Patience: 25/50
2024-12-18 02:20:25.113429: train_loss -0.7611
2024-12-18 02:20:25.114573: val_loss -0.437
2024-12-18 02:20:25.115546: Pseudo dice [0.7199]
2024-12-18 02:20:25.116581: Epoch time: 413.79 s
2024-12-18 02:20:26.692877: 
2024-12-18 02:20:26.694417: Epoch 36
2024-12-18 02:20:26.695504: Current learning rate: 0.00781
2024-12-18 02:28:59.702639: Validation loss did not improve from -0.49681. Patience: 26/50
2024-12-18 02:28:59.704571: train_loss -0.7623
2024-12-18 02:28:59.705925: val_loss -0.474
2024-12-18 02:28:59.706875: Pseudo dice [0.7297]
2024-12-18 02:28:59.707835: Epoch time: 513.01 s
2024-12-18 02:29:01.149750: 
2024-12-18 02:29:01.151109: Epoch 37
2024-12-18 02:29:01.152159: Current learning rate: 0.00775
2024-12-18 02:37:29.483995: Validation loss did not improve from -0.49681. Patience: 27/50
2024-12-18 02:37:29.489633: train_loss -0.7612
2024-12-18 02:37:29.492800: val_loss -0.461
2024-12-18 02:37:29.493774: Pseudo dice [0.7299]
2024-12-18 02:37:29.495095: Epoch time: 508.34 s
2024-12-18 02:37:29.496287: Yayy! New best EMA pseudo Dice: 0.7149
2024-12-18 02:37:31.409260: 
2024-12-18 02:37:31.410728: Epoch 38
2024-12-18 02:37:31.411499: Current learning rate: 0.00769
2024-12-18 02:46:11.079143: Validation loss improved from -0.49681 to -0.51508! Patience: 27/50
2024-12-18 02:46:11.080323: train_loss -0.7703
2024-12-18 02:46:11.081467: val_loss -0.5151
2024-12-18 02:46:11.082555: Pseudo dice [0.7458]
2024-12-18 02:46:11.083530: Epoch time: 519.67 s
2024-12-18 02:46:11.084424: Yayy! New best EMA pseudo Dice: 0.718
2024-12-18 02:46:12.961910: 
2024-12-18 02:46:12.963408: Epoch 39
2024-12-18 02:46:12.964442: Current learning rate: 0.00763
2024-12-18 02:55:16.415922: Validation loss did not improve from -0.51508. Patience: 1/50
2024-12-18 02:55:16.417056: train_loss -0.7708
2024-12-18 02:55:16.417846: val_loss -0.4801
2024-12-18 02:55:16.418769: Pseudo dice [0.735]
2024-12-18 02:55:16.419544: Epoch time: 543.46 s
2024-12-18 02:55:16.811187: Yayy! New best EMA pseudo Dice: 0.7197
2024-12-18 02:55:18.716002: 
2024-12-18 02:55:18.717378: Epoch 40
2024-12-18 02:55:18.718151: Current learning rate: 0.00756
2024-12-18 03:04:01.397949: Validation loss did not improve from -0.51508. Patience: 2/50
2024-12-18 03:04:01.399056: train_loss -0.7717
2024-12-18 03:04:01.399967: val_loss -0.4668
2024-12-18 03:04:01.401068: Pseudo dice [0.728]
2024-12-18 03:04:01.402086: Epoch time: 522.68 s
2024-12-18 03:04:01.403099: Yayy! New best EMA pseudo Dice: 0.7205
2024-12-18 03:04:03.769798: 
2024-12-18 03:04:03.771214: Epoch 41
2024-12-18 03:04:03.772074: Current learning rate: 0.0075
2024-12-18 03:12:34.415386: Validation loss did not improve from -0.51508. Patience: 3/50
2024-12-18 03:12:34.416670: train_loss -0.7658
2024-12-18 03:12:34.417827: val_loss -0.4641
2024-12-18 03:12:34.418561: Pseudo dice [0.7229]
2024-12-18 03:12:34.419456: Epoch time: 510.65 s
2024-12-18 03:12:34.420305: Yayy! New best EMA pseudo Dice: 0.7208
2024-12-18 03:12:36.405137: 
2024-12-18 03:12:36.406680: Epoch 42
2024-12-18 03:12:36.407514: Current learning rate: 0.00744
2024-12-18 03:20:50.333077: Validation loss did not improve from -0.51508. Patience: 4/50
2024-12-18 03:20:50.334142: train_loss -0.7739
2024-12-18 03:20:50.335314: val_loss -0.41
2024-12-18 03:20:50.336373: Pseudo dice [0.702]
2024-12-18 03:20:50.337353: Epoch time: 493.93 s
2024-12-18 03:20:51.717488: 
2024-12-18 03:20:51.718914: Epoch 43
2024-12-18 03:20:51.720018: Current learning rate: 0.00738
2024-12-18 03:29:28.920830: Validation loss did not improve from -0.51508. Patience: 5/50
2024-12-18 03:29:28.921969: train_loss -0.7756
2024-12-18 03:29:28.922711: val_loss -0.4232
2024-12-18 03:29:28.923536: Pseudo dice [0.7021]
2024-12-18 03:29:28.924243: Epoch time: 517.21 s
2024-12-18 03:29:30.260075: 
2024-12-18 03:29:30.261266: Epoch 44
2024-12-18 03:29:30.262117: Current learning rate: 0.00732
2024-12-18 03:38:20.871757: Validation loss did not improve from -0.51508. Patience: 6/50
2024-12-18 03:38:20.872859: train_loss -0.7809
2024-12-18 03:38:20.873796: val_loss -0.4569
2024-12-18 03:38:20.874579: Pseudo dice [0.7174]
2024-12-18 03:38:20.875476: Epoch time: 530.61 s
2024-12-18 03:38:22.652676: 
2024-12-18 03:38:22.654109: Epoch 45
2024-12-18 03:38:22.654902: Current learning rate: 0.00725
2024-12-18 03:47:15.387440: Validation loss did not improve from -0.51508. Patience: 7/50
2024-12-18 03:47:15.388551: train_loss -0.7756
2024-12-18 03:47:15.389518: val_loss -0.4134
2024-12-18 03:47:15.390237: Pseudo dice [0.7087]
2024-12-18 03:47:15.391057: Epoch time: 532.74 s
2024-12-18 03:47:16.785399: 
2024-12-18 03:47:16.786796: Epoch 46
2024-12-18 03:47:16.787652: Current learning rate: 0.00719
2024-12-18 03:56:05.663424: Validation loss did not improve from -0.51508. Patience: 8/50
2024-12-18 03:56:05.664180: train_loss -0.7781
2024-12-18 03:56:05.665075: val_loss -0.4496
2024-12-18 03:56:05.665960: Pseudo dice [0.7217]
2024-12-18 03:56:05.666837: Epoch time: 528.88 s
2024-12-18 03:56:07.077849: 
2024-12-18 03:56:07.079206: Epoch 47
2024-12-18 03:56:07.080084: Current learning rate: 0.00713
2024-12-18 04:04:12.775891: Validation loss did not improve from -0.51508. Patience: 9/50
2024-12-18 04:04:12.777265: train_loss -0.7803
2024-12-18 04:04:12.778143: val_loss -0.4589
2024-12-18 04:04:12.778869: Pseudo dice [0.7199]
2024-12-18 04:04:12.779668: Epoch time: 485.7 s
2024-12-18 04:04:14.210565: 
2024-12-18 04:04:14.211587: Epoch 48
2024-12-18 04:04:14.212291: Current learning rate: 0.00707
2024-12-18 04:12:44.662719: Validation loss did not improve from -0.51508. Patience: 10/50
2024-12-18 04:12:44.663788: train_loss -0.7844
2024-12-18 04:12:44.664876: val_loss -0.4578
2024-12-18 04:12:44.666025: Pseudo dice [0.7184]
2024-12-18 04:12:44.667077: Epoch time: 510.45 s
2024-12-18 04:12:46.136570: 
2024-12-18 04:12:46.138392: Epoch 49
2024-12-18 04:12:46.139817: Current learning rate: 0.007
2024-12-18 04:21:29.084780: Validation loss did not improve from -0.51508. Patience: 11/50
2024-12-18 04:21:29.085890: train_loss -0.7839
2024-12-18 04:21:29.086734: val_loss -0.3974
2024-12-18 04:21:29.087533: Pseudo dice [0.6947]
2024-12-18 04:21:29.088326: Epoch time: 522.95 s
2024-12-18 04:21:30.913753: 
2024-12-18 04:21:30.915224: Epoch 50
2024-12-18 04:21:30.916343: Current learning rate: 0.00694
2024-12-18 04:30:35.124974: Validation loss did not improve from -0.51508. Patience: 12/50
2024-12-18 04:30:35.126077: train_loss -0.788
2024-12-18 04:30:35.127091: val_loss -0.4205
2024-12-18 04:30:35.128010: Pseudo dice [0.7067]
2024-12-18 04:30:35.129029: Epoch time: 544.21 s
2024-12-18 04:30:36.550019: 
2024-12-18 04:30:36.551273: Epoch 51
2024-12-18 04:30:36.552273: Current learning rate: 0.00688
2024-12-18 04:39:32.844026: Validation loss did not improve from -0.51508. Patience: 13/50
2024-12-18 04:39:32.845174: train_loss -0.7849
2024-12-18 04:39:32.846147: val_loss -0.4281
2024-12-18 04:39:32.846941: Pseudo dice [0.7186]
2024-12-18 04:39:32.847763: Epoch time: 536.3 s
2024-12-18 04:39:34.732717: 
2024-12-18 04:39:34.734231: Epoch 52
2024-12-18 04:39:34.735039: Current learning rate: 0.00682
2024-12-18 04:47:57.393073: Validation loss did not improve from -0.51508. Patience: 14/50
2024-12-18 04:47:57.397089: train_loss -0.7849
2024-12-18 04:47:57.398553: val_loss -0.4144
2024-12-18 04:47:57.399790: Pseudo dice [0.7135]
2024-12-18 04:47:57.400931: Epoch time: 502.66 s
2024-12-18 04:47:58.890625: 
2024-12-18 04:47:58.892712: Epoch 53
2024-12-18 04:47:58.893866: Current learning rate: 0.00675
2024-12-18 04:56:29.722098: Validation loss did not improve from -0.51508. Patience: 15/50
2024-12-18 04:56:29.722986: train_loss -0.7886
2024-12-18 04:56:29.724024: val_loss -0.4733
2024-12-18 04:56:29.725037: Pseudo dice [0.7332]
2024-12-18 04:56:29.726125: Epoch time: 510.83 s
2024-12-18 04:56:31.163158: 
2024-12-18 04:56:31.164845: Epoch 54
2024-12-18 04:56:31.166013: Current learning rate: 0.00669
2024-12-18 05:05:10.020516: Validation loss did not improve from -0.51508. Patience: 16/50
2024-12-18 05:05:10.021732: train_loss -0.7878
2024-12-18 05:05:10.022628: val_loss -0.457
2024-12-18 05:05:10.023634: Pseudo dice [0.7191]
2024-12-18 05:05:10.024609: Epoch time: 518.86 s
2024-12-18 05:05:11.845965: 
2024-12-18 05:05:11.847516: Epoch 55
2024-12-18 05:05:11.848434: Current learning rate: 0.00663
2024-12-18 05:14:04.773185: Validation loss did not improve from -0.51508. Patience: 17/50
2024-12-18 05:14:04.774351: train_loss -0.7911
2024-12-18 05:14:04.775464: val_loss -0.4776
2024-12-18 05:14:04.776310: Pseudo dice [0.7309]
2024-12-18 05:14:04.777108: Epoch time: 532.93 s
2024-12-18 05:14:06.197337: 
2024-12-18 05:14:06.198885: Epoch 56
2024-12-18 05:14:06.199810: Current learning rate: 0.00657
2024-12-18 05:23:12.594915: Validation loss did not improve from -0.51508. Patience: 18/50
2024-12-18 05:23:12.596049: train_loss -0.7891
2024-12-18 05:23:12.596841: val_loss -0.4731
2024-12-18 05:23:12.597493: Pseudo dice [0.7405]
2024-12-18 05:23:12.598311: Epoch time: 546.4 s
2024-12-18 05:23:13.976380: 
2024-12-18 05:23:13.977801: Epoch 57
2024-12-18 05:23:13.978706: Current learning rate: 0.0065
2024-12-18 05:32:12.198176: Validation loss did not improve from -0.51508. Patience: 19/50
2024-12-18 05:32:12.199273: train_loss -0.7937
2024-12-18 05:32:12.200027: val_loss -0.4597
2024-12-18 05:32:12.200740: Pseudo dice [0.728]
2024-12-18 05:32:12.201464: Epoch time: 538.22 s
2024-12-18 05:32:12.202259: Yayy! New best EMA pseudo Dice: 0.7211
2024-12-18 05:32:13.996872: 
2024-12-18 05:32:13.998099: Epoch 58
2024-12-18 05:32:13.998916: Current learning rate: 0.00644
2024-12-18 05:41:11.563339: Validation loss did not improve from -0.51508. Patience: 20/50
2024-12-18 05:41:11.564621: train_loss -0.7947
2024-12-18 05:41:11.565410: val_loss -0.3965
2024-12-18 05:41:11.566162: Pseudo dice [0.6901]
2024-12-18 05:41:11.567002: Epoch time: 537.57 s
2024-12-18 05:41:13.077715: 
2024-12-18 05:41:13.079626: Epoch 59
2024-12-18 05:41:13.080488: Current learning rate: 0.00638
2024-12-18 05:48:45.441519: Validation loss did not improve from -0.51508. Patience: 21/50
2024-12-18 05:48:45.442692: train_loss -0.7938
2024-12-18 05:48:45.443716: val_loss -0.4631
2024-12-18 05:48:45.444513: Pseudo dice [0.7245]
2024-12-18 05:48:45.445273: Epoch time: 452.37 s
2024-12-18 05:48:47.307318: 
2024-12-18 05:48:47.308542: Epoch 60
2024-12-18 05:48:47.309305: Current learning rate: 0.00631
2024-12-18 05:57:25.811820: Validation loss did not improve from -0.51508. Patience: 22/50
2024-12-18 05:57:25.815224: train_loss -0.7913
2024-12-18 05:57:25.816725: val_loss -0.4638
2024-12-18 05:57:25.817576: Pseudo dice [0.7229]
2024-12-18 05:57:25.818483: Epoch time: 518.51 s
2024-12-18 05:57:27.245373: 
2024-12-18 05:57:27.246771: Epoch 61
2024-12-18 05:57:27.247595: Current learning rate: 0.00625
2024-12-18 06:06:26.799334: Validation loss did not improve from -0.51508. Patience: 23/50
2024-12-18 06:06:26.800492: train_loss -0.7895
2024-12-18 06:06:26.801519: val_loss -0.4345
2024-12-18 06:06:26.802285: Pseudo dice [0.7065]
2024-12-18 06:06:26.802984: Epoch time: 539.56 s
2024-12-18 06:06:28.275886: 
2024-12-18 06:06:28.277344: Epoch 62
2024-12-18 06:06:28.278251: Current learning rate: 0.00619
2024-12-18 06:14:47.754535: Validation loss did not improve from -0.51508. Patience: 24/50
2024-12-18 06:14:47.755542: train_loss -0.7933
2024-12-18 06:14:47.756327: val_loss -0.4316
2024-12-18 06:14:47.757016: Pseudo dice [0.7193]
2024-12-18 06:14:47.757812: Epoch time: 499.48 s
2024-12-18 06:14:49.655270: 
2024-12-18 06:14:49.656753: Epoch 63
2024-12-18 06:14:49.657628: Current learning rate: 0.00612
2024-12-18 06:23:06.991204: Validation loss did not improve from -0.51508. Patience: 25/50
2024-12-18 06:23:06.992255: train_loss -0.7938
2024-12-18 06:23:06.993012: val_loss -0.4512
2024-12-18 06:23:06.993750: Pseudo dice [0.7205]
2024-12-18 06:23:06.994471: Epoch time: 497.34 s
2024-12-18 06:23:08.481567: 
2024-12-18 06:23:08.482937: Epoch 64
2024-12-18 06:23:08.483761: Current learning rate: 0.00606
2024-12-18 06:31:36.083605: Validation loss did not improve from -0.51508. Patience: 26/50
2024-12-18 06:31:36.084605: train_loss -0.7958
2024-12-18 06:31:36.085583: val_loss -0.4211
2024-12-18 06:31:36.086364: Pseudo dice [0.71]
2024-12-18 06:31:36.087203: Epoch time: 507.6 s
2024-12-18 06:31:37.932936: 
2024-12-18 06:31:37.934224: Epoch 65
2024-12-18 06:31:37.934957: Current learning rate: 0.006
2024-12-18 06:40:21.515782: Validation loss did not improve from -0.51508. Patience: 27/50
2024-12-18 06:40:21.516889: train_loss -0.7971
2024-12-18 06:40:21.517686: val_loss -0.4251
2024-12-18 06:40:21.518313: Pseudo dice [0.7153]
2024-12-18 06:40:21.518958: Epoch time: 523.59 s
2024-12-18 06:40:22.929368: 
2024-12-18 06:40:22.930909: Epoch 66
2024-12-18 06:40:22.931703: Current learning rate: 0.00593
2024-12-18 06:49:26.986396: Validation loss did not improve from -0.51508. Patience: 28/50
2024-12-18 06:49:26.988050: train_loss -0.7956
2024-12-18 06:49:26.988894: val_loss -0.3961
2024-12-18 06:49:26.989620: Pseudo dice [0.7021]
2024-12-18 06:49:26.990302: Epoch time: 544.06 s
2024-12-18 06:49:28.439124: 
2024-12-18 06:49:28.440470: Epoch 67
2024-12-18 06:49:28.441310: Current learning rate: 0.00587
2024-12-18 06:58:12.069801: Validation loss did not improve from -0.51508. Patience: 29/50
2024-12-18 06:58:12.073606: train_loss -0.7969
2024-12-18 06:58:12.074886: val_loss -0.4833
2024-12-18 06:58:12.075631: Pseudo dice [0.7453]
2024-12-18 06:58:12.076619: Epoch time: 523.63 s
2024-12-18 06:58:13.526698: 
2024-12-18 06:58:13.528110: Epoch 68
2024-12-18 06:58:13.528926: Current learning rate: 0.00581
2024-12-18 07:06:54.709682: Validation loss did not improve from -0.51508. Patience: 30/50
2024-12-18 07:06:54.710925: train_loss -0.7999
2024-12-18 07:06:54.711967: val_loss -0.425
2024-12-18 07:06:54.712840: Pseudo dice [0.7151]
2024-12-18 07:06:54.713638: Epoch time: 521.19 s
2024-12-18 07:06:56.178131: 
2024-12-18 07:06:56.179483: Epoch 69
2024-12-18 07:06:56.180287: Current learning rate: 0.00574
2024-12-18 07:15:35.855730: Validation loss did not improve from -0.51508. Patience: 31/50
2024-12-18 07:15:35.856736: train_loss -0.8011
2024-12-18 07:15:35.857527: val_loss -0.4404
2024-12-18 07:15:35.858279: Pseudo dice [0.7208]
2024-12-18 07:15:35.859055: Epoch time: 519.68 s
2024-12-18 07:15:37.650226: 
2024-12-18 07:15:37.651492: Epoch 70
2024-12-18 07:15:37.652211: Current learning rate: 0.00568
2024-12-18 07:24:31.228449: Validation loss did not improve from -0.51508. Patience: 32/50
2024-12-18 07:24:31.229561: train_loss -0.8015
2024-12-18 07:24:31.230346: val_loss -0.4546
2024-12-18 07:24:31.231084: Pseudo dice [0.7201]
2024-12-18 07:24:31.231781: Epoch time: 533.58 s
2024-12-18 07:24:32.723699: 
2024-12-18 07:24:32.724981: Epoch 71
2024-12-18 07:24:32.725705: Current learning rate: 0.00562
2024-12-18 07:33:39.414098: Validation loss did not improve from -0.51508. Patience: 33/50
2024-12-18 07:33:39.415370: train_loss -0.8039
2024-12-18 07:33:39.416156: val_loss -0.4361
2024-12-18 07:33:39.416796: Pseudo dice [0.7227]
2024-12-18 07:33:39.417475: Epoch time: 546.69 s
2024-12-18 07:33:40.820523: 
2024-12-18 07:33:40.821940: Epoch 72
2024-12-18 07:33:40.822662: Current learning rate: 0.00555
2024-12-18 07:42:21.242236: Validation loss did not improve from -0.51508. Patience: 34/50
2024-12-18 07:42:21.243268: train_loss -0.8048
2024-12-18 07:42:21.244098: val_loss -0.4171
2024-12-18 07:42:21.244815: Pseudo dice [0.7087]
2024-12-18 07:42:21.245512: Epoch time: 520.42 s
2024-12-18 07:42:22.819216: 
2024-12-18 07:42:22.820466: Epoch 73
2024-12-18 07:42:22.821308: Current learning rate: 0.00549
2024-12-18 07:51:19.693383: Validation loss did not improve from -0.51508. Patience: 35/50
2024-12-18 07:51:19.694486: train_loss -0.801
2024-12-18 07:51:19.695299: val_loss -0.4686
2024-12-18 07:51:19.696011: Pseudo dice [0.7364]
2024-12-18 07:51:19.696908: Epoch time: 536.88 s
2024-12-18 07:51:21.503901: 
2024-12-18 07:51:21.505403: Epoch 74
2024-12-18 07:51:21.506227: Current learning rate: 0.00542
2024-12-18 07:59:53.915977: Validation loss did not improve from -0.51508. Patience: 36/50
2024-12-18 07:59:53.916999: train_loss -0.8019
2024-12-18 07:59:53.917809: val_loss -0.4539
2024-12-18 07:59:53.918460: Pseudo dice [0.7222]
2024-12-18 07:59:53.919123: Epoch time: 512.41 s
2024-12-18 07:59:55.783954: 
2024-12-18 07:59:55.785433: Epoch 75
2024-12-18 07:59:55.786258: Current learning rate: 0.00536
2024-12-18 08:08:33.910535: Validation loss did not improve from -0.51508. Patience: 37/50
2024-12-18 08:08:33.914835: train_loss -0.8032
2024-12-18 08:08:33.918118: val_loss -0.4079
2024-12-18 08:08:33.919297: Pseudo dice [0.7012]
2024-12-18 08:08:33.920499: Epoch time: 518.13 s
2024-12-18 08:08:35.463836: 
2024-12-18 08:08:35.465105: Epoch 76
2024-12-18 08:08:35.465922: Current learning rate: 0.00529
2024-12-18 08:17:23.743217: Validation loss did not improve from -0.51508. Patience: 38/50
2024-12-18 08:17:23.744387: train_loss -0.8076
2024-12-18 08:17:23.745151: val_loss -0.4552
2024-12-18 08:17:23.745825: Pseudo dice [0.7222]
2024-12-18 08:17:23.746590: Epoch time: 528.28 s
2024-12-18 08:17:25.303173: 
2024-12-18 08:17:25.304455: Epoch 77
2024-12-18 08:17:25.305133: Current learning rate: 0.00523
2024-12-18 08:26:21.447178: Validation loss did not improve from -0.51508. Patience: 39/50
2024-12-18 08:26:21.448175: train_loss -0.8045
2024-12-18 08:26:21.448941: val_loss -0.4155
2024-12-18 08:26:21.449745: Pseudo dice [0.7066]
2024-12-18 08:26:21.450551: Epoch time: 536.15 s
2024-12-18 08:26:22.904040: 
2024-12-18 08:26:22.905304: Epoch 78
2024-12-18 08:26:22.906011: Current learning rate: 0.00517
2024-12-18 08:34:35.451301: Validation loss did not improve from -0.51508. Patience: 40/50
2024-12-18 08:34:35.452383: train_loss -0.8111
2024-12-18 08:34:35.453231: val_loss -0.4959
2024-12-18 08:34:35.454051: Pseudo dice [0.7462]
2024-12-18 08:34:35.454890: Epoch time: 492.55 s
2024-12-18 08:34:36.997043: 
2024-12-18 08:34:36.998346: Epoch 79
2024-12-18 08:34:36.999176: Current learning rate: 0.0051
2024-12-18 08:42:53.940705: Validation loss did not improve from -0.51508. Patience: 41/50
2024-12-18 08:42:53.941983: train_loss -0.8073
2024-12-18 08:42:53.942881: val_loss -0.4376
2024-12-18 08:42:53.943887: Pseudo dice [0.7191]
2024-12-18 08:42:53.944841: Epoch time: 496.95 s
2024-12-18 08:42:55.816275: 
2024-12-18 08:42:55.817940: Epoch 80
2024-12-18 08:42:55.818989: Current learning rate: 0.00504
2024-12-18 08:51:41.667906: Validation loss did not improve from -0.51508. Patience: 42/50
2024-12-18 08:51:41.668948: train_loss -0.8096
2024-12-18 08:51:41.669888: val_loss -0.4493
2024-12-18 08:51:41.670702: Pseudo dice [0.7265]
2024-12-18 08:51:41.671731: Epoch time: 525.85 s
2024-12-18 08:51:43.198584: 
2024-12-18 08:51:43.199857: Epoch 81
2024-12-18 08:51:43.200601: Current learning rate: 0.00497
2024-12-18 09:00:49.456435: Validation loss did not improve from -0.51508. Patience: 43/50
2024-12-18 09:00:49.457547: train_loss -0.8114
2024-12-18 09:00:49.458541: val_loss -0.414
2024-12-18 09:00:49.459523: Pseudo dice [0.7156]
2024-12-18 09:00:49.460577: Epoch time: 546.26 s
2024-12-18 09:00:50.936817: 
2024-12-18 09:00:50.938255: Epoch 82
2024-12-18 09:00:50.939373: Current learning rate: 0.00491
2024-12-18 09:09:47.793028: Validation loss did not improve from -0.51508. Patience: 44/50
2024-12-18 09:09:47.796711: train_loss -0.8106
2024-12-18 09:09:47.797845: val_loss -0.4242
2024-12-18 09:09:47.798575: Pseudo dice [0.7157]
2024-12-18 09:09:47.799426: Epoch time: 536.86 s
2024-12-18 09:09:49.257063: 
2024-12-18 09:09:49.258396: Epoch 83
2024-12-18 09:09:49.259114: Current learning rate: 0.00484
2024-12-18 09:18:22.613717: Validation loss did not improve from -0.51508. Patience: 45/50
2024-12-18 09:18:22.615304: train_loss -0.811
2024-12-18 09:18:22.617151: val_loss -0.3965
2024-12-18 09:18:22.618152: Pseudo dice [0.7024]
2024-12-18 09:18:22.619193: Epoch time: 513.36 s
2024-12-18 09:18:24.059343: 
2024-12-18 09:18:24.060766: Epoch 84
2024-12-18 09:18:24.061510: Current learning rate: 0.00478
2024-12-18 09:27:07.462098: Validation loss did not improve from -0.51508. Patience: 46/50
2024-12-18 09:27:07.463317: train_loss -0.8133
2024-12-18 09:27:07.464262: val_loss -0.3779
2024-12-18 09:27:07.465165: Pseudo dice [0.6897]
2024-12-18 09:27:07.466174: Epoch time: 523.41 s
2024-12-18 09:27:09.761793: 
2024-12-18 09:27:09.763270: Epoch 85
2024-12-18 09:27:09.764551: Current learning rate: 0.00471
2024-12-18 09:35:37.672302: Validation loss did not improve from -0.51508. Patience: 47/50
2024-12-18 09:35:37.673396: train_loss -0.8112
2024-12-18 09:35:37.674238: val_loss -0.4508
2024-12-18 09:35:37.674992: Pseudo dice [0.7307]
2024-12-18 09:35:37.675772: Epoch time: 507.91 s
2024-12-18 09:35:39.039836: 
2024-12-18 09:35:39.041342: Epoch 86
2024-12-18 09:35:39.042184: Current learning rate: 0.00465
2024-12-18 09:44:09.859099: Validation loss did not improve from -0.51508. Patience: 48/50
2024-12-18 09:44:09.860170: train_loss -0.81
2024-12-18 09:44:09.861284: val_loss -0.4139
2024-12-18 09:44:09.862215: Pseudo dice [0.7103]
2024-12-18 09:44:09.863002: Epoch time: 510.82 s
2024-12-18 09:44:11.236740: 
2024-12-18 09:44:11.237935: Epoch 87
2024-12-18 09:44:11.238647: Current learning rate: 0.00458
2024-12-18 09:52:58.505840: Validation loss did not improve from -0.51508. Patience: 49/50
2024-12-18 09:52:58.506670: train_loss -0.8131
2024-12-18 09:52:58.507609: val_loss -0.4124
2024-12-18 09:52:58.508389: Pseudo dice [0.7086]
2024-12-18 09:52:58.509105: Epoch time: 527.27 s
2024-12-18 09:52:59.937962: 
2024-12-18 09:52:59.939373: Epoch 88
2024-12-18 09:52:59.940201: Current learning rate: 0.00452
2024-12-18 10:01:26.888681: Validation loss did not improve from -0.51508. Patience: 50/50
2024-12-18 10:01:26.889778: train_loss -0.8138
2024-12-18 10:01:26.890831: val_loss -0.4224
2024-12-18 10:01:26.891631: Pseudo dice [0.7113]
2024-12-18 10:01:26.892374: Epoch time: 506.95 s
2024-12-18 10:01:28.394177: 
2024-12-18 10:01:28.395292: Epoch 89
2024-12-18 10:01:28.396152: Current learning rate: 0.00445
2024-12-18 10:09:50.957441: Validation loss did not improve from -0.51508. Patience: 51/50
2024-12-18 10:09:50.958439: train_loss -0.8151
2024-12-18 10:09:50.959440: val_loss -0.4257
2024-12-18 10:09:50.960265: Pseudo dice [0.7239]
2024-12-18 10:09:50.961031: Epoch time: 502.57 s
2024-12-18 10:09:52.773504: 
2024-12-18 10:09:52.774629: Epoch 90
2024-12-18 10:09:52.775354: Current learning rate: 0.00438
2024-12-18 10:18:24.555995: Validation loss did not improve from -0.51508. Patience: 52/50
2024-12-18 10:18:24.558120: train_loss -0.8143
2024-12-18 10:18:24.559242: val_loss -0.4829
2024-12-18 10:18:24.560053: Pseudo dice [0.7426]
2024-12-18 10:18:24.560891: Epoch time: 511.79 s
2024-12-18 10:18:26.035839: 
2024-12-18 10:18:26.037117: Epoch 91
2024-12-18 10:18:26.037961: Current learning rate: 0.00432
2024-12-18 10:26:59.337697: Validation loss did not improve from -0.51508. Patience: 53/50
2024-12-18 10:26:59.339041: train_loss -0.8154
2024-12-18 10:26:59.340245: val_loss -0.4125
2024-12-18 10:26:59.341331: Pseudo dice [0.7142]
2024-12-18 10:26:59.342409: Epoch time: 513.3 s
2024-12-18 10:27:00.751164: 
2024-12-18 10:27:00.752529: Epoch 92
2024-12-18 10:27:00.753693: Current learning rate: 0.00425
2024-12-18 10:35:46.272620: Validation loss did not improve from -0.51508. Patience: 54/50
2024-12-18 10:35:46.273382: train_loss -0.8183
2024-12-18 10:35:46.274279: val_loss -0.4479
2024-12-18 10:35:46.274986: Pseudo dice [0.7274]
2024-12-18 10:35:46.275661: Epoch time: 525.52 s
2024-12-18 10:35:47.684876: 
2024-12-18 10:35:47.686158: Epoch 93
2024-12-18 10:35:47.686920: Current learning rate: 0.00419
2024-12-18 10:44:06.209284: Validation loss did not improve from -0.51508. Patience: 55/50
2024-12-18 10:44:06.210202: train_loss -0.8167
2024-12-18 10:44:06.211002: val_loss -0.4398
2024-12-18 10:44:06.211668: Pseudo dice [0.7255]
2024-12-18 10:44:06.212440: Epoch time: 498.53 s
2024-12-18 10:44:07.666291: 
2024-12-18 10:44:07.667420: Epoch 94
2024-12-18 10:44:07.668174: Current learning rate: 0.00412
2024-12-18 10:52:35.458205: Validation loss did not improve from -0.51508. Patience: 56/50
2024-12-18 10:52:35.459187: train_loss -0.8161
2024-12-18 10:52:35.459931: val_loss -0.4208
2024-12-18 10:52:35.460674: Pseudo dice [0.7141]
2024-12-18 10:52:35.461411: Epoch time: 507.79 s
2024-12-18 10:52:37.324395: 
2024-12-18 10:52:37.325833: Epoch 95
2024-12-18 10:52:37.326969: Current learning rate: 0.00405
2024-12-18 11:01:06.554455: Validation loss did not improve from -0.51508. Patience: 57/50
2024-12-18 11:01:06.555395: train_loss -0.8148
2024-12-18 11:01:06.556309: val_loss -0.4405
2024-12-18 11:01:06.557106: Pseudo dice [0.7208]
2024-12-18 11:01:06.558047: Epoch time: 509.23 s
2024-12-18 11:01:08.048397: 
2024-12-18 11:01:08.049530: Epoch 96
2024-12-18 11:01:08.050379: Current learning rate: 0.00399
2024-12-18 11:09:49.509612: Validation loss did not improve from -0.51508. Patience: 58/50
2024-12-18 11:09:49.510570: train_loss -0.8186
2024-12-18 11:09:49.511428: val_loss -0.4761
2024-12-18 11:09:49.512177: Pseudo dice [0.7325]
2024-12-18 11:09:49.512956: Epoch time: 521.46 s
2024-12-18 11:09:51.595620: 
2024-12-18 11:09:51.597157: Epoch 97
2024-12-18 11:09:51.598139: Current learning rate: 0.00392
2024-12-18 11:18:46.333451: Validation loss did not improve from -0.51508. Patience: 59/50
2024-12-18 11:18:46.336604: train_loss -0.8181
2024-12-18 11:18:46.338831: val_loss -0.4076
2024-12-18 11:18:46.339617: Pseudo dice [0.7104]
2024-12-18 11:18:46.340693: Epoch time: 534.74 s
2024-12-18 11:18:47.765006: 
2024-12-18 11:18:47.766338: Epoch 98
2024-12-18 11:18:47.767124: Current learning rate: 0.00385
2024-12-18 11:27:25.780374: Validation loss did not improve from -0.51508. Patience: 60/50
2024-12-18 11:27:25.782948: train_loss -0.8169
2024-12-18 11:27:25.784116: val_loss -0.412
2024-12-18 11:27:25.784974: Pseudo dice [0.7114]
2024-12-18 11:27:25.785755: Epoch time: 518.02 s
2024-12-18 11:27:27.221986: 
2024-12-18 11:27:27.223579: Epoch 99
2024-12-18 11:27:27.224697: Current learning rate: 0.00379
2024-12-18 11:36:09.436337: Validation loss did not improve from -0.51508. Patience: 61/50
2024-12-18 11:36:09.437334: train_loss -0.8216
2024-12-18 11:36:09.438328: val_loss -0.3938
2024-12-18 11:36:09.439172: Pseudo dice [0.6996]
2024-12-18 11:36:09.440034: Epoch time: 522.22 s
2024-12-18 11:36:11.266347: 
2024-12-18 11:36:11.267546: Epoch 100
2024-12-18 11:36:11.268314: Current learning rate: 0.00372
2024-12-18 11:44:42.815182: Validation loss did not improve from -0.51508. Patience: 62/50
2024-12-18 11:44:42.816116: train_loss -0.8192
2024-12-18 11:44:42.817064: val_loss -0.4493
2024-12-18 11:44:42.817802: Pseudo dice [0.7261]
2024-12-18 11:44:42.818596: Epoch time: 511.55 s
2024-12-18 11:44:44.223152: 
2024-12-18 11:44:44.224409: Epoch 101
2024-12-18 11:44:44.225171: Current learning rate: 0.00365
2024-12-18 11:53:26.888183: Validation loss did not improve from -0.51508. Patience: 63/50
2024-12-18 11:53:26.889021: train_loss -0.8217
2024-12-18 11:53:26.889971: val_loss -0.4699
2024-12-18 11:53:26.890991: Pseudo dice [0.7449]
2024-12-18 11:53:26.891900: Epoch time: 522.67 s
2024-12-18 11:53:28.315462: 
2024-12-18 11:53:28.316628: Epoch 102
2024-12-18 11:53:28.317361: Current learning rate: 0.00359
2024-12-18 12:02:26.746615: Validation loss did not improve from -0.51508. Patience: 64/50
2024-12-18 12:02:26.747829: train_loss -0.8209
2024-12-18 12:02:26.748732: val_loss -0.3993
2024-12-18 12:02:26.749499: Pseudo dice [0.7104]
2024-12-18 12:02:26.750413: Epoch time: 538.43 s
2024-12-18 12:02:28.181983: 
2024-12-18 12:02:28.183476: Epoch 103
2024-12-18 12:02:28.184422: Current learning rate: 0.00352
2024-12-18 12:10:52.631789: Validation loss did not improve from -0.51508. Patience: 65/50
2024-12-18 12:10:52.633000: train_loss -0.8233
2024-12-18 12:10:52.633792: val_loss -0.423
2024-12-18 12:10:52.634479: Pseudo dice [0.7153]
2024-12-18 12:10:52.635129: Epoch time: 504.45 s
2024-12-18 12:10:54.062194: 
2024-12-18 12:10:54.063454: Epoch 104
2024-12-18 12:10:54.064290: Current learning rate: 0.00345
2024-12-18 12:19:47.384100: Validation loss did not improve from -0.51508. Patience: 66/50
2024-12-18 12:19:47.385019: train_loss -0.8218
2024-12-18 12:19:47.385880: val_loss -0.422
2024-12-18 12:19:47.386741: Pseudo dice [0.7132]
2024-12-18 12:19:47.387547: Epoch time: 533.33 s
2024-12-18 12:19:49.343733: 
2024-12-18 12:19:49.345135: Epoch 105
2024-12-18 12:19:49.346067: Current learning rate: 0.00338
2024-12-18 12:28:27.534785: Validation loss did not improve from -0.51508. Patience: 67/50
2024-12-18 12:28:27.576207: train_loss -0.8249
2024-12-18 12:28:27.578473: val_loss -0.4509
2024-12-18 12:28:27.579700: Pseudo dice [0.7319]
2024-12-18 12:28:27.580851: Epoch time: 518.23 s
2024-12-18 12:28:29.008229: 
2024-12-18 12:28:29.009664: Epoch 106
2024-12-18 12:28:29.010692: Current learning rate: 0.00332
2024-12-18 12:36:58.684913: Validation loss did not improve from -0.51508. Patience: 68/50
2024-12-18 12:36:58.686447: train_loss -0.8237
2024-12-18 12:36:58.687408: val_loss -0.4095
2024-12-18 12:36:58.688171: Pseudo dice [0.7107]
2024-12-18 12:36:58.688922: Epoch time: 509.68 s
2024-12-18 12:37:00.159931: 
2024-12-18 12:37:00.161347: Epoch 107
2024-12-18 12:37:00.162159: Current learning rate: 0.00325
2024-12-18 12:45:19.331341: Validation loss did not improve from -0.51508. Patience: 69/50
2024-12-18 12:45:19.332339: train_loss -0.8214
2024-12-18 12:45:19.333296: val_loss -0.4302
2024-12-18 12:45:19.334200: Pseudo dice [0.7218]
2024-12-18 12:45:19.334984: Epoch time: 499.17 s
2024-12-18 12:45:21.254993: 
2024-12-18 12:45:21.256258: Epoch 108
2024-12-18 12:45:21.256986: Current learning rate: 0.00318
2024-12-18 12:54:19.558357: Validation loss did not improve from -0.51508. Patience: 70/50
2024-12-18 12:54:19.559853: train_loss -0.8232
2024-12-18 12:54:19.560893: val_loss -0.4128
2024-12-18 12:54:19.561759: Pseudo dice [0.7011]
2024-12-18 12:54:19.562703: Epoch time: 538.31 s
2024-12-18 12:54:21.005205: 
2024-12-18 12:54:21.006223: Epoch 109
2024-12-18 12:54:21.007104: Current learning rate: 0.00311
2024-12-18 13:02:42.530462: Validation loss did not improve from -0.51508. Patience: 71/50
2024-12-18 13:02:42.531328: train_loss -0.8241
2024-12-18 13:02:42.532161: val_loss -0.4609
2024-12-18 13:02:42.532832: Pseudo dice [0.7209]
2024-12-18 13:02:42.533654: Epoch time: 501.53 s
2024-12-18 13:02:44.411448: 
2024-12-18 13:02:44.412679: Epoch 110
2024-12-18 13:02:44.413442: Current learning rate: 0.00304
2024-12-18 13:11:07.990122: Validation loss did not improve from -0.51508. Patience: 72/50
2024-12-18 13:11:07.990786: train_loss -0.8241
2024-12-18 13:11:07.991507: val_loss -0.4429
2024-12-18 13:11:07.992180: Pseudo dice [0.717]
2024-12-18 13:11:07.992932: Epoch time: 503.58 s
2024-12-18 13:11:09.454498: 
2024-12-18 13:11:09.455427: Epoch 111
2024-12-18 13:11:09.456196: Current learning rate: 0.00297
2024-12-18 13:20:06.479233: Validation loss did not improve from -0.51508. Patience: 73/50
2024-12-18 13:20:06.480134: train_loss -0.8279
2024-12-18 13:20:06.480992: val_loss -0.4774
2024-12-18 13:20:06.481786: Pseudo dice [0.7407]
2024-12-18 13:20:06.482596: Epoch time: 537.03 s
2024-12-18 13:20:07.934639: 
2024-12-18 13:20:07.936113: Epoch 112
2024-12-18 13:20:07.936930: Current learning rate: 0.00291
2024-12-18 13:28:38.583176: Validation loss did not improve from -0.51508. Patience: 74/50
2024-12-18 13:28:38.586372: train_loss -0.8258
2024-12-18 13:28:38.587837: val_loss -0.3827
2024-12-18 13:28:38.588643: Pseudo dice [0.6919]
2024-12-18 13:28:38.589491: Epoch time: 510.65 s
2024-12-18 13:28:39.991691: 
2024-12-18 13:28:39.993258: Epoch 113
2024-12-18 13:28:39.994157: Current learning rate: 0.00284
2024-12-18 13:37:09.514293: Validation loss did not improve from -0.51508. Patience: 75/50
2024-12-18 13:37:09.515473: train_loss -0.8251
2024-12-18 13:37:09.516354: val_loss -0.4455
2024-12-18 13:37:09.517123: Pseudo dice [0.7229]
2024-12-18 13:37:09.517864: Epoch time: 509.53 s
2024-12-18 13:37:10.996030: 
2024-12-18 13:37:10.997651: Epoch 114
2024-12-18 13:37:10.998555: Current learning rate: 0.00277
2024-12-18 13:45:45.560997: Validation loss did not improve from -0.51508. Patience: 76/50
2024-12-18 13:45:45.562081: train_loss -0.8259
2024-12-18 13:45:45.562883: val_loss -0.394
2024-12-18 13:45:45.563588: Pseudo dice [0.6948]
2024-12-18 13:45:45.564389: Epoch time: 514.57 s
2024-12-18 13:45:47.424738: 
2024-12-18 13:45:47.426087: Epoch 115
2024-12-18 13:45:47.426923: Current learning rate: 0.0027
2024-12-18 13:54:16.868623: Validation loss did not improve from -0.51508. Patience: 77/50
2024-12-18 13:54:16.870299: train_loss -0.8241
2024-12-18 13:54:16.871312: val_loss -0.46
2024-12-18 13:54:16.872024: Pseudo dice [0.7342]
2024-12-18 13:54:16.872731: Epoch time: 509.45 s
2024-12-18 13:54:18.290075: 
2024-12-18 13:54:18.291626: Epoch 116
2024-12-18 13:54:18.292320: Current learning rate: 0.00263
2024-12-18 14:03:09.508308: Validation loss did not improve from -0.51508. Patience: 78/50
2024-12-18 14:03:09.509413: train_loss -0.8268
2024-12-18 14:03:09.510411: val_loss -0.3931
2024-12-18 14:03:09.511238: Pseudo dice [0.7006]
2024-12-18 14:03:09.511956: Epoch time: 531.22 s
2024-12-18 14:03:10.976570: 
2024-12-18 14:03:10.978774: Epoch 117
2024-12-18 14:03:10.979882: Current learning rate: 0.00256
2024-12-18 14:11:51.562927: Validation loss did not improve from -0.51508. Patience: 79/50
2024-12-18 14:11:51.564532: train_loss -0.828
2024-12-18 14:11:51.565695: val_loss -0.4333
2024-12-18 14:11:51.566640: Pseudo dice [0.7227]
2024-12-18 14:11:51.567439: Epoch time: 520.59 s
2024-12-18 14:11:53.020127: 
2024-12-18 14:11:53.021696: Epoch 118
2024-12-18 14:11:53.022659: Current learning rate: 0.00249
2024-12-18 14:19:16.741637: Validation loss did not improve from -0.51508. Patience: 80/50
2024-12-18 14:19:16.742604: train_loss -0.8268
2024-12-18 14:19:16.743398: val_loss -0.4394
2024-12-18 14:19:16.744025: Pseudo dice [0.7233]
2024-12-18 14:19:16.744684: Epoch time: 443.72 s
2024-12-18 14:19:18.852860: 
2024-12-18 14:19:18.854206: Epoch 119
2024-12-18 14:19:18.854963: Current learning rate: 0.00242
2024-12-18 14:26:18.707032: Validation loss did not improve from -0.51508. Patience: 81/50
2024-12-18 14:26:18.708024: train_loss -0.8276
2024-12-18 14:26:18.709079: val_loss -0.4298
2024-12-18 14:26:18.709800: Pseudo dice [0.7202]
2024-12-18 14:26:18.710563: Epoch time: 419.86 s
2024-12-18 14:26:20.595913: 
2024-12-18 14:26:20.597204: Epoch 120
2024-12-18 14:26:20.597967: Current learning rate: 0.00235
2024-12-18 14:33:33.950921: Validation loss did not improve from -0.51508. Patience: 82/50
2024-12-18 14:33:33.954025: train_loss -0.8275
2024-12-18 14:33:33.955313: val_loss -0.4397
2024-12-18 14:33:33.956087: Pseudo dice [0.7259]
2024-12-18 14:33:33.956995: Epoch time: 433.36 s
2024-12-18 14:33:35.400927: 
2024-12-18 14:33:35.402144: Epoch 121
2024-12-18 14:33:35.402894: Current learning rate: 0.00228
2024-12-18 14:40:26.954452: Validation loss did not improve from -0.51508. Patience: 83/50
2024-12-18 14:40:26.955506: train_loss -0.8286
2024-12-18 14:40:26.956901: val_loss -0.4783
2024-12-18 14:40:26.957826: Pseudo dice [0.738]
2024-12-18 14:40:26.958817: Epoch time: 411.56 s
2024-12-18 14:40:28.401054: 
2024-12-18 14:40:28.402481: Epoch 122
2024-12-18 14:40:28.403415: Current learning rate: 0.00221
2024-12-18 14:47:23.811796: Validation loss did not improve from -0.51508. Patience: 84/50
2024-12-18 14:47:23.812715: train_loss -0.8303
2024-12-18 14:47:23.813488: val_loss -0.4599
2024-12-18 14:47:23.814266: Pseudo dice [0.7454]
2024-12-18 14:47:23.815077: Epoch time: 415.41 s
2024-12-18 14:47:23.815731: Yayy! New best EMA pseudo Dice: 0.7227
2024-12-18 14:47:25.640071: 
2024-12-18 14:47:25.641411: Epoch 123
2024-12-18 14:47:25.642191: Current learning rate: 0.00214
2024-12-18 14:54:20.182071: Validation loss did not improve from -0.51508. Patience: 85/50
2024-12-18 14:54:20.183106: train_loss -0.8279
2024-12-18 14:54:20.184220: val_loss -0.4144
2024-12-18 14:54:20.185379: Pseudo dice [0.7132]
2024-12-18 14:54:20.186499: Epoch time: 414.54 s
2024-12-18 14:54:21.657662: 
2024-12-18 14:54:21.659205: Epoch 124
2024-12-18 14:54:21.660278: Current learning rate: 0.00207
2024-12-18 15:01:44.126523: Validation loss did not improve from -0.51508. Patience: 86/50
2024-12-18 15:01:44.127632: train_loss -0.8308
2024-12-18 15:01:44.128586: val_loss -0.4258
2024-12-18 15:01:44.129385: Pseudo dice [0.7161]
2024-12-18 15:01:44.130079: Epoch time: 442.47 s
2024-12-18 15:01:46.115181: 
2024-12-18 15:01:46.116449: Epoch 125
2024-12-18 15:01:46.117338: Current learning rate: 0.00199
2024-12-18 15:08:48.718199: Validation loss did not improve from -0.51508. Patience: 87/50
2024-12-18 15:08:48.719259: train_loss -0.831
2024-12-18 15:08:48.720258: val_loss -0.4172
2024-12-18 15:08:48.720982: Pseudo dice [0.7127]
2024-12-18 15:08:48.721797: Epoch time: 422.61 s
2024-12-18 15:08:50.169120: 
2024-12-18 15:08:50.170524: Epoch 126
2024-12-18 15:08:50.171473: Current learning rate: 0.00192
2024-12-18 15:16:05.918505: Validation loss did not improve from -0.51508. Patience: 88/50
2024-12-18 15:16:05.919415: train_loss -0.8325
2024-12-18 15:16:05.920157: val_loss -0.4073
2024-12-18 15:16:05.920840: Pseudo dice [0.7118]
2024-12-18 15:16:05.921531: Epoch time: 435.75 s
2024-12-18 15:16:07.421105: 
2024-12-18 15:16:07.422553: Epoch 127
2024-12-18 15:16:07.423376: Current learning rate: 0.00185
2024-12-18 15:23:13.761572: Validation loss did not improve from -0.51508. Patience: 89/50
2024-12-18 15:23:13.763814: train_loss -0.8302
2024-12-18 15:23:13.764948: val_loss -0.3982
2024-12-18 15:23:13.766005: Pseudo dice [0.7002]
2024-12-18 15:23:13.767122: Epoch time: 426.34 s
2024-12-18 15:23:15.252546: 
2024-12-18 15:23:15.254026: Epoch 128
2024-12-18 15:23:15.255090: Current learning rate: 0.00178
2024-12-18 15:30:24.952732: Validation loss did not improve from -0.51508. Patience: 90/50
2024-12-18 15:30:24.953612: train_loss -0.8329
2024-12-18 15:30:24.954381: val_loss -0.3913
2024-12-18 15:30:24.955129: Pseudo dice [0.7059]
2024-12-18 15:30:24.955834: Epoch time: 429.7 s
2024-12-18 15:30:26.390364: 
2024-12-18 15:30:26.391732: Epoch 129
2024-12-18 15:30:26.392633: Current learning rate: 0.0017
2024-12-18 15:37:50.139263: Validation loss did not improve from -0.51508. Patience: 91/50
2024-12-18 15:37:50.141223: train_loss -0.8332
2024-12-18 15:37:50.142237: val_loss -0.4614
2024-12-18 15:37:50.143092: Pseudo dice [0.7335]
2024-12-18 15:37:50.144413: Epoch time: 443.75 s
2024-12-18 15:37:52.979741: 
2024-12-18 15:37:52.981161: Epoch 130
2024-12-18 15:37:52.982022: Current learning rate: 0.00163
2024-12-18 15:45:33.938870: Validation loss did not improve from -0.51508. Patience: 92/50
2024-12-18 15:45:33.940541: train_loss -0.8324
2024-12-18 15:45:33.941330: val_loss -0.4301
2024-12-18 15:45:33.942014: Pseudo dice [0.722]
2024-12-18 15:45:33.942815: Epoch time: 460.96 s
2024-12-18 15:45:35.409189: 
2024-12-18 15:45:35.410670: Epoch 131
2024-12-18 15:45:35.411530: Current learning rate: 0.00156
2024-12-18 15:53:02.438071: Validation loss did not improve from -0.51508. Patience: 93/50
2024-12-18 15:53:02.439260: train_loss -0.8335
2024-12-18 15:53:02.440163: val_loss -0.448
2024-12-18 15:53:02.440968: Pseudo dice [0.7299]
2024-12-18 15:53:02.441637: Epoch time: 447.03 s
2024-12-18 15:53:04.019733: 
2024-12-18 15:53:04.021051: Epoch 132
2024-12-18 15:53:04.021868: Current learning rate: 0.00148
2024-12-18 16:00:46.760317: Validation loss did not improve from -0.51508. Patience: 94/50
2024-12-18 16:00:46.761525: train_loss -0.8326
2024-12-18 16:00:46.762372: val_loss -0.4723
2024-12-18 16:00:46.762991: Pseudo dice [0.7292]
2024-12-18 16:00:46.763769: Epoch time: 462.74 s
2024-12-18 16:00:48.231967: 
2024-12-18 16:00:48.233376: Epoch 133
2024-12-18 16:00:48.234266: Current learning rate: 0.00141
2024-12-18 16:08:29.421770: Validation loss did not improve from -0.51508. Patience: 95/50
2024-12-18 16:08:29.422709: train_loss -0.8346
2024-12-18 16:08:29.423633: val_loss -0.4393
2024-12-18 16:08:29.424379: Pseudo dice [0.7267]
2024-12-18 16:08:29.425123: Epoch time: 461.19 s
2024-12-18 16:08:30.852793: 
2024-12-18 16:08:30.854073: Epoch 134
2024-12-18 16:08:30.854819: Current learning rate: 0.00133
2024-12-18 16:13:47.854734: Validation loss did not improve from -0.51508. Patience: 96/50
2024-12-18 16:13:47.855827: train_loss -0.833
2024-12-18 16:13:47.856791: val_loss -0.3892
2024-12-18 16:13:47.857625: Pseudo dice [0.7031]
2024-12-18 16:13:47.858468: Epoch time: 317.0 s
2024-12-18 16:13:49.765107: 
2024-12-18 16:13:49.766560: Epoch 135
2024-12-18 16:13:49.767359: Current learning rate: 0.00126
2024-12-18 16:19:18.910727: Validation loss did not improve from -0.51508. Patience: 97/50
2024-12-18 16:19:18.911803: train_loss -0.8331
2024-12-18 16:19:18.912765: val_loss -0.4122
2024-12-18 16:19:18.913538: Pseudo dice [0.7068]
2024-12-18 16:19:18.914301: Epoch time: 329.15 s
2024-12-18 16:19:20.408718: 
2024-12-18 16:19:20.410023: Epoch 136
2024-12-18 16:19:20.410763: Current learning rate: 0.00118
2024-12-18 16:24:35.679333: Validation loss did not improve from -0.51508. Patience: 98/50
2024-12-18 16:24:35.681161: train_loss -0.8348
2024-12-18 16:24:35.682017: val_loss -0.4181
2024-12-18 16:24:35.682799: Pseudo dice [0.7198]
2024-12-18 16:24:35.683578: Epoch time: 315.27 s
2024-12-18 16:24:37.211159: 
2024-12-18 16:24:37.212662: Epoch 137
2024-12-18 16:24:37.213771: Current learning rate: 0.00111
2024-12-18 16:30:16.328092: Validation loss did not improve from -0.51508. Patience: 99/50
2024-12-18 16:30:16.329479: train_loss -0.8351
2024-12-18 16:30:16.330910: val_loss -0.4281
2024-12-18 16:30:16.331896: Pseudo dice [0.7254]
2024-12-18 16:30:16.332925: Epoch time: 339.12 s
2024-12-18 16:30:17.817223: 
2024-12-18 16:30:17.818552: Epoch 138
2024-12-18 16:30:17.819504: Current learning rate: 0.00103
2024-12-18 16:37:20.819528: Validation loss did not improve from -0.51508. Patience: 100/50
2024-12-18 16:37:20.820564: train_loss -0.8359
2024-12-18 16:37:20.821444: val_loss -0.4176
2024-12-18 16:37:20.822236: Pseudo dice [0.7206]
2024-12-18 16:37:20.822978: Epoch time: 423.0 s
2024-12-18 16:37:22.269675: 
2024-12-18 16:37:22.271222: Epoch 139
2024-12-18 16:37:22.272092: Current learning rate: 0.00095
2024-12-18 16:43:47.127712: Validation loss did not improve from -0.51508. Patience: 101/50
2024-12-18 16:43:47.131544: train_loss -0.8351
2024-12-18 16:43:47.133240: val_loss -0.4427
2024-12-18 16:43:47.134315: Pseudo dice [0.7236]
2024-12-18 16:43:47.135623: Epoch time: 384.86 s
2024-12-18 16:43:49.177768: 
2024-12-18 16:43:49.179230: Epoch 140
2024-12-18 16:43:49.180063: Current learning rate: 0.00087
2024-12-18 16:50:38.965593: Validation loss did not improve from -0.51508. Patience: 102/50
2024-12-18 16:50:38.966711: train_loss -0.8378
2024-12-18 16:50:38.967613: val_loss -0.396
2024-12-18 16:50:38.968553: Pseudo dice [0.7123]
2024-12-18 16:50:38.969375: Epoch time: 409.79 s
2024-12-18 16:50:41.876031: 
2024-12-18 16:50:41.877612: Epoch 141
2024-12-18 16:50:41.878636: Current learning rate: 0.00079
2024-12-18 16:57:28.664247: Validation loss did not improve from -0.51508. Patience: 103/50
2024-12-18 16:57:28.665177: train_loss -0.8367
2024-12-18 16:57:28.666075: val_loss -0.4487
2024-12-18 16:57:28.666833: Pseudo dice [0.7287]
2024-12-18 16:57:28.667535: Epoch time: 406.79 s
2024-12-18 16:57:30.148097: 
2024-12-18 16:57:30.149519: Epoch 142
2024-12-18 16:57:30.150320: Current learning rate: 0.00071
2024-12-18 17:04:23.483395: Validation loss did not improve from -0.51508. Patience: 104/50
2024-12-18 17:04:23.484322: train_loss -0.8355
2024-12-18 17:04:23.485241: val_loss -0.4208
2024-12-18 17:04:23.486259: Pseudo dice [0.707]
2024-12-18 17:04:23.487289: Epoch time: 413.34 s
2024-12-18 17:04:25.012634: 
2024-12-18 17:04:25.014284: Epoch 143
2024-12-18 17:04:25.015450: Current learning rate: 0.00063
2024-12-18 17:11:22.668701: Validation loss did not improve from -0.51508. Patience: 105/50
2024-12-18 17:11:22.669752: train_loss -0.8374
2024-12-18 17:11:22.670830: val_loss -0.4427
2024-12-18 17:11:22.671863: Pseudo dice [0.7267]
2024-12-18 17:11:22.672724: Epoch time: 417.66 s
2024-12-18 17:11:24.111209: 
2024-12-18 17:11:24.112708: Epoch 144
2024-12-18 17:11:24.113534: Current learning rate: 0.00055
2024-12-18 17:18:20.687424: Validation loss did not improve from -0.51508. Patience: 106/50
2024-12-18 17:18:20.688478: train_loss -0.8347
2024-12-18 17:18:20.689376: val_loss -0.4232
2024-12-18 17:18:20.690148: Pseudo dice [0.7165]
2024-12-18 17:18:20.690804: Epoch time: 416.58 s
2024-12-18 17:18:22.718132: 
2024-12-18 17:18:22.720191: Epoch 145
2024-12-18 17:18:22.720967: Current learning rate: 0.00047
2024-12-18 17:25:35.160072: Validation loss did not improve from -0.51508. Patience: 107/50
2024-12-18 17:25:35.161129: train_loss -0.8356
2024-12-18 17:25:35.161972: val_loss -0.4353
2024-12-18 17:25:35.162663: Pseudo dice [0.7156]
2024-12-18 17:25:35.163496: Epoch time: 432.45 s
2024-12-18 17:25:36.620125: 
2024-12-18 17:25:36.622305: Epoch 146
2024-12-18 17:25:36.623483: Current learning rate: 0.00038
2024-12-18 17:33:25.868513: Validation loss did not improve from -0.51508. Patience: 108/50
2024-12-18 17:33:25.869677: train_loss -0.8361
2024-12-18 17:33:25.870913: val_loss -0.4333
2024-12-18 17:33:25.871982: Pseudo dice [0.7261]
2024-12-18 17:33:25.872900: Epoch time: 469.25 s
2024-12-18 17:33:27.304770: 
2024-12-18 17:33:27.305771: Epoch 147
2024-12-18 17:33:27.306637: Current learning rate: 0.0003
2024-12-18 17:39:48.368410: Validation loss did not improve from -0.51508. Patience: 109/50
2024-12-18 17:39:48.369571: train_loss -0.8359
2024-12-18 17:39:48.370469: val_loss -0.4571
2024-12-18 17:39:48.371339: Pseudo dice [0.7307]
2024-12-18 17:39:48.372077: Epoch time: 381.07 s
2024-12-18 17:39:49.894837: 
2024-12-18 17:39:49.895997: Epoch 148
2024-12-18 17:39:49.896788: Current learning rate: 0.00021
2024-12-18 17:46:33.304068: Validation loss did not improve from -0.51508. Patience: 110/50
2024-12-18 17:46:33.305011: train_loss -0.8345
2024-12-18 17:46:33.306092: val_loss -0.4599
2024-12-18 17:46:33.306829: Pseudo dice [0.7335]
2024-12-18 17:46:33.307588: Epoch time: 403.41 s
2024-12-18 17:46:34.718644: 
2024-12-18 17:46:34.719565: Epoch 149
2024-12-18 17:46:34.720251: Current learning rate: 0.00011
2024-12-18 17:53:17.326275: Validation loss did not improve from -0.51508. Patience: 111/50
2024-12-18 17:53:17.327289: train_loss -0.8372
2024-12-18 17:53:17.328470: val_loss -0.435
2024-12-18 17:53:17.329489: Pseudo dice [0.7277]
2024-12-18 17:53:17.330455: Epoch time: 402.61 s
2024-12-18 17:53:19.227438: Training done.
2024-12-18 17:53:19.369844: Using splits from existing split file: /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset309_Sohee_Calcium_OCT_CrossValidation/splits_final_40.json
2024-12-18 17:53:19.371919: The split file contains 5 splits.
2024-12-18 17:53:19.372681: Desired fold for training: 0
2024-12-18 17:53:19.373453: This split has 3 training and 5 validation cases.
2024-12-18 17:53:19.374513: predicting 101-045
2024-12-18 17:53:19.409616: 101-045, shape torch.Size([1, 375, 498, 498]), rank 0
2024-12-18 17:55:30.913371: predicting 106-002
2024-12-18 17:55:30.933454: 106-002, shape torch.Size([1, 539, 498, 498]), rank 0
2024-12-18 17:58:39.131778: predicting 701-013
2024-12-18 17:58:39.159263: 701-013, shape torch.Size([1, 375, 498, 498]), rank 0
2024-12-18 18:00:59.824651: predicting 704-003
2024-12-18 18:00:59.839925: 704-003, shape torch.Size([1, 375, 498, 498]), rank 0
2024-12-18 18:02:58.346216: predicting 706-005
2024-12-18 18:02:58.362596: 706-005, shape torch.Size([1, 375, 498, 498]), rank 0
2024-12-18 18:05:22.026327: Validation complete
2024-12-18 18:05:22.027435: Mean Validation Dice:  0.7203600992874837

############################
INFO: You are using the old nnU-Net default plans. We have updated our recommendations. Please consider using those instead! Read more here: https://github.com/MIC-DKFZ/nnUNet/blob/master/documentation/resenc_presets.md
############################

Using device: cuda:0

#######################################################################
Please cite the following paper when using nnU-Net:
Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.
#######################################################################

2024-12-18 18:05:28.666358: Using torch.compile...

############################
INFO: You are using the old nnU-Net default plans. We have updated our recommendations. Please consider using those instead! Read more here: https://github.com/MIC-DKFZ/nnUNet/blob/master/documentation/resenc_presets.md
############################

Using device: cuda:0

#######################################################################
Please cite the following paper when using nnU-Net:
Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.
#######################################################################

2024-12-18 18:05:28.670890: Using torch.compile...
/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
################### Loading pretrained weights from file  /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_results/Dataset300_Lumen_and_Wall_OCT/nnUNetTrainer__nnUNetPreprocessPlans__3d_fullres/fold_all/checkpoint_best.pth ###################
Below is the list of overlapping blocks in pretrained model and nnUNet architecture:
encoder.stages.0.0.convs.0.conv.weight shape torch.Size([32, 1, 3, 3, 3])
encoder.stages.0.0.convs.0.conv.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.norm.weight shape torch.Size([32])
encoder.stages.0.0.convs.0.norm.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.0.weight shape torch.Size([32, 1, 3, 3, 3])
encoder.stages.0.0.convs.0.all_modules.0.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.1.weight shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.1.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
encoder.stages.0.0.convs.1.conv.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.norm.weight shape torch.Size([32])
encoder.stages.0.0.convs.1.norm.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
encoder.stages.0.0.convs.1.all_modules.0.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.1.weight shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.1.bias shape torch.Size([32])
encoder.stages.1.0.convs.0.conv.weight shape torch.Size([64, 32, 3, 3, 3])
encoder.stages.1.0.convs.0.conv.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.norm.weight shape torch.Size([64])
encoder.stages.1.0.convs.0.norm.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.0.weight shape torch.Size([64, 32, 3, 3, 3])
encoder.stages.1.0.convs.0.all_modules.0.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.1.weight shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.1.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
encoder.stages.1.0.convs.1.conv.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.norm.weight shape torch.Size([64])
encoder.stages.1.0.convs.1.norm.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
encoder.stages.1.0.convs.1.all_modules.0.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.1.weight shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.1.bias shape torch.Size([64])
encoder.stages.2.0.convs.0.conv.weight shape torch.Size([128, 64, 3, 3, 3])
encoder.stages.2.0.convs.0.conv.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.norm.weight shape torch.Size([128])
encoder.stages.2.0.convs.0.norm.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.0.weight shape torch.Size([128, 64, 3, 3, 3])
encoder.stages.2.0.convs.0.all_modules.0.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.1.weight shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.1.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
encoder.stages.2.0.convs.1.conv.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.norm.weight shape torch.Size([128])
encoder.stages.2.0.convs.1.norm.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
encoder.stages.2.0.convs.1.all_modules.0.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.1.weight shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.1.bias shape torch.Size([128])
encoder.stages.3.0.convs.0.conv.weight shape torch.Size([256, 128, 3, 3, 3])
encoder.stages.3.0.convs.0.conv.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.norm.weight shape torch.Size([256])
encoder.stages.3.0.convs.0.norm.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.0.weight shape torch.Size([256, 128, 3, 3, 3])
encoder.stages.3.0.convs.0.all_modules.0.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.1.weight shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.1.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
encoder.stages.3.0.convs.1.conv.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.norm.weight shape torch.Size([256])
encoder.stages.3.0.convs.1.norm.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
encoder.stages.3.0.convs.1.all_modules.0.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.1.weight shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.1.bias shape torch.Size([256])
encoder.stages.4.0.convs.0.conv.weight shape torch.Size([320, 256, 3, 3, 3])
encoder.stages.4.0.convs.0.conv.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.norm.weight shape torch.Size([320])
encoder.stages.4.0.convs.0.norm.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.0.weight shape torch.Size([320, 256, 3, 3, 3])
encoder.stages.4.0.convs.0.all_modules.0.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.1.weight shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.1.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.4.0.convs.1.conv.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.norm.weight shape torch.Size([320])
encoder.stages.4.0.convs.1.norm.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.4.0.convs.1.all_modules.0.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.1.weight shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.1.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.0.conv.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.norm.weight shape torch.Size([320])
encoder.stages.5.0.convs.0.norm.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.0.all_modules.0.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.1.weight shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.1.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.1.conv.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.norm.weight shape torch.Size([320])
encoder.stages.5.0.convs.1.norm.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.1.all_modules.0.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.1.weight shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.0.0.convs.0.conv.weight shape torch.Size([32, 1, 3, 3, 3])
decoder.encoder.stages.0.0.convs.0.conv.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.norm.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.norm.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.0.weight shape torch.Size([32, 1, 3, 3, 3])
decoder.encoder.stages.0.0.convs.0.all_modules.0.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.1.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.1.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.encoder.stages.0.0.convs.1.conv.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.norm.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.norm.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.encoder.stages.0.0.convs.1.all_modules.0.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.1.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.1.bias shape torch.Size([32])
decoder.encoder.stages.1.0.convs.0.conv.weight shape torch.Size([64, 32, 3, 3, 3])
decoder.encoder.stages.1.0.convs.0.conv.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.norm.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.norm.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.0.weight shape torch.Size([64, 32, 3, 3, 3])
decoder.encoder.stages.1.0.convs.0.all_modules.0.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.1.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.1.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.encoder.stages.1.0.convs.1.conv.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.norm.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.norm.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.encoder.stages.1.0.convs.1.all_modules.0.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.1.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.1.bias shape torch.Size([64])
decoder.encoder.stages.2.0.convs.0.conv.weight shape torch.Size([128, 64, 3, 3, 3])
decoder.encoder.stages.2.0.convs.0.conv.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.norm.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.norm.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.0.weight shape torch.Size([128, 64, 3, 3, 3])
decoder.encoder.stages.2.0.convs.0.all_modules.0.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.1.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.1.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.encoder.stages.2.0.convs.1.conv.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.norm.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.norm.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.encoder.stages.2.0.convs.1.all_modules.0.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.1.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.1.bias shape torch.Size([128])
decoder.encoder.stages.3.0.convs.0.conv.weight shape torch.Size([256, 128, 3, 3, 3])
decoder.encoder.stages.3.0.convs.0.conv.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.norm.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.norm.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.0.weight shape torch.Size([256, 128, 3, 3, 3])
decoder.encoder.stages.3.0.convs.0.all_modules.0.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.1.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.1.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.encoder.stages.3.0.convs.1.conv.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.norm.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.norm.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.encoder.stages.3.0.convs.1.all_modules.0.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.1.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.1.bias shape torch.Size([256])
decoder.encoder.stages.4.0.convs.0.conv.weight shape torch.Size([320, 256, 3, 3, 3])
decoder.encoder.stages.4.0.convs.0.conv.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.norm.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.norm.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.0.weight shape torch.Size([320, 256, 3, 3, 3])
decoder.encoder.stages.4.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.4.0.convs.1.conv.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.norm.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.norm.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.4.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.0.conv.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.norm.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.norm.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.1.conv.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.norm.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.norm.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.stages.0.convs.0.conv.weight shape torch.Size([320, 640, 3, 3, 3])
decoder.stages.0.convs.0.conv.bias shape torch.Size([320])
decoder.stages.0.convs.0.norm.weight shape torch.Size([320])
decoder.stages.0.convs.0.norm.bias shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.0.weight shape torch.Size([320, 640, 3, 3, 3])
decoder.stages.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.stages.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.stages.0.convs.1.conv.bias shape torch.Size([320])
decoder.stages.0.convs.1.norm.weight shape torch.Size([320])
decoder.stages.0.convs.1.norm.bias shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.stages.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.stages.1.convs.0.conv.weight shape torch.Size([256, 512, 3, 3, 3])
decoder.stages.1.convs.0.conv.bias shape torch.Size([256])
decoder.stages.1.convs.0.norm.weight shape torch.Size([256])
decoder.stages.1.convs.0.norm.bias shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.0.weight shape torch.Size([256, 512, 3, 3, 3])
decoder.stages.1.convs.0.all_modules.0.bias shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.1.weight shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.1.bias shape torch.Size([256])
decoder.stages.1.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.stages.1.convs.1.conv.bias shape torch.Size([256])
decoder.stages.1.convs.1.norm.weight shape torch.Size([256])
decoder.stages.1.convs.1.norm.bias shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.stages.1.convs.1.all_modules.0.bias shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.1.weight shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.1.bias shape torch.Size([256])
decoder.stages.2.convs.0.conv.weight shape torch.Size([128, 256, 3, 3, 3])
decoder.stages.2.convs.0.conv.bias shape torch.Size([128])
decoder.stages.2.convs.0.norm.weight shape torch.Size([128])
decoder.stages.2.convs.0.norm.bias shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.0.weight shape torch.Size([128, 256, 3, 3, 3])
decoder.stages.2.convs.0.all_modules.0.bias shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.1.weight shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.1.bias shape torch.Size([128])
decoder.stages.2.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.stages.2.convs.1.conv.bias shape torch.Size([128])
decoder.stages.2.convs.1.norm.weight shape torch.Size([128])
decoder.stages.2.convs.1.norm.bias shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.stages.2.convs.1.all_modules.0.bias shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.1.weight shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.1.bias shape torch.Size([128])
decoder.stages.3.convs.0.conv.weight shape torch.Size([64, 128, 3, 3, 3])
decoder.stages.3.convs.0.conv.bias shape torch.Size([64])
decoder.stages.3.convs.0.norm.weight shape torch.Size([64])
decoder.stages.3.convs.0.norm.bias shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.0.weight shape torch.Size([64, 128, 3, 3, 3])
decoder.stages.3.convs.0.all_modules.0.bias shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.1.weight shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.1.bias shape torch.Size([64])
decoder.stages.3.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.stages.3.convs.1.conv.bias shape torch.Size([64])
decoder.stages.3.convs.1.norm.weight shape torch.Size([64])
decoder.stages.3.convs.1.norm.bias shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.stages.3.convs.1.all_modules.0.bias shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.1.weight shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.1.bias shape torch.Size([64])
decoder.stages.4.convs.0.conv.weight shape torch.Size([32, 64, 3, 3, 3])
decoder.stages.4.convs.0.conv.bias shape torch.Size([32])
decoder.stages.4.convs.0.norm.weight shape torch.Size([32])
decoder.stages.4.convs.0.norm.bias shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.0.weight shape torch.Size([32, 64, 3, 3, 3])
decoder.stages.4.convs.0.all_modules.0.bias shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.1.weight shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.1.bias shape torch.Size([32])
decoder.stages.4.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.stages.4.convs.1.conv.bias shape torch.Size([32])
decoder.stages.4.convs.1.norm.weight shape torch.Size([32])
decoder.stages.4.convs.1.norm.bias shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.stages.4.convs.1.all_modules.0.bias shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.1.weight shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.1.bias shape torch.Size([32])
decoder.transpconvs.0.weight shape torch.Size([320, 320, 1, 2, 2])
decoder.transpconvs.0.bias shape torch.Size([320])
decoder.transpconvs.1.weight shape torch.Size([320, 256, 2, 2, 2])
decoder.transpconvs.1.bias shape torch.Size([256])
decoder.transpconvs.2.weight shape torch.Size([256, 128, 2, 2, 2])
decoder.transpconvs.2.bias shape torch.Size([128])
decoder.transpconvs.3.weight shape torch.Size([128, 64, 2, 2, 2])
decoder.transpconvs.3.bias shape torch.Size([64])
decoder.transpconvs.4.weight shape torch.Size([64, 32, 2, 2, 2])
decoder.transpconvs.4.bias shape torch.Size([32])
################### Done ###################
2024-12-18 18:05:38.567058: do_dummy_2d_data_aug: True
2024-12-18 18:05:38.568326: Using splits from existing split file: /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset309_Sohee_Calcium_OCT_CrossValidation/splits_final_40.json
2024-12-18 18:05:38.569690: The split file contains 5 splits.
2024-12-18 18:05:38.571033: Desired fold for training: 2
2024-12-18 18:05:38.572209: This split has 3 training and 5 validation cases.
################### Loading pretrained weights from file  /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_results/Dataset300_Lumen_and_Wall_OCT/nnUNetTrainer__nnUNetPreprocessPlans__3d_fullres/fold_all/checkpoint_best.pth ###################
Below is the list of overlapping blocks in pretrained model and nnUNet architecture:
encoder.stages.0.0.convs.0.conv.weight shape torch.Size([32, 1, 3, 3, 3])
encoder.stages.0.0.convs.0.conv.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.norm.weight shape torch.Size([32])
encoder.stages.0.0.convs.0.norm.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.0.weight shape torch.Size([32, 1, 3, 3, 3])
encoder.stages.0.0.convs.0.all_modules.0.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.1.weight shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.1.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
encoder.stages.0.0.convs.1.conv.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.norm.weight shape torch.Size([32])
encoder.stages.0.0.convs.1.norm.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
encoder.stages.0.0.convs.1.all_modules.0.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.1.weight shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.1.bias shape torch.Size([32])
encoder.stages.1.0.convs.0.conv.weight shape torch.Size([64, 32, 3, 3, 3])
encoder.stages.1.0.convs.0.conv.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.norm.weight shape torch.Size([64])
encoder.stages.1.0.convs.0.norm.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.0.weight shape torch.Size([64, 32, 3, 3, 3])
encoder.stages.1.0.convs.0.all_modules.0.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.1.weight shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.1.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
encoder.stages.1.0.convs.1.conv.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.norm.weight shape torch.Size([64])
encoder.stages.1.0.convs.1.norm.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
encoder.stages.1.0.convs.1.all_modules.0.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.1.weight shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.1.bias shape torch.Size([64])
encoder.stages.2.0.convs.0.conv.weight shape torch.Size([128, 64, 3, 3, 3])
encoder.stages.2.0.convs.0.conv.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.norm.weight shape torch.Size([128])
encoder.stages.2.0.convs.0.norm.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.0.weight shape torch.Size([128, 64, 3, 3, 3])
encoder.stages.2.0.convs.0.all_modules.0.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.1.weight shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.1.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
encoder.stages.2.0.convs.1.conv.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.norm.weight shape torch.Size([128])
encoder.stages.2.0.convs.1.norm.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
encoder.stages.2.0.convs.1.all_modules.0.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.1.weight shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.1.bias shape torch.Size([128])
encoder.stages.3.0.convs.0.conv.weight shape torch.Size([256, 128, 3, 3, 3])
encoder.stages.3.0.convs.0.conv.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.norm.weight shape torch.Size([256])
encoder.stages.3.0.convs.0.norm.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.0.weight shape torch.Size([256, 128, 3, 3, 3])
encoder.stages.3.0.convs.0.all_modules.0.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.1.weight shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.1.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
encoder.stages.3.0.convs.1.conv.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.norm.weight shape torch.Size([256])
encoder.stages.3.0.convs.1.norm.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
encoder.stages.3.0.convs.1.all_modules.0.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.1.weight shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.1.bias shape torch.Size([256])
encoder.stages.4.0.convs.0.conv.weight shape torch.Size([320, 256, 3, 3, 3])
encoder.stages.4.0.convs.0.conv.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.norm.weight shape torch.Size([320])
encoder.stages.4.0.convs.0.norm.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.0.weight shape torch.Size([320, 256, 3, 3, 3])
encoder.stages.4.0.convs.0.all_modules.0.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.1.weight shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.1.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.4.0.convs.1.conv.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.norm.weight shape torch.Size([320])
encoder.stages.4.0.convs.1.norm.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.4.0.convs.1.all_modules.0.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.1.weight shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.1.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.0.conv.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.norm.weight shape torch.Size([320])
encoder.stages.5.0.convs.0.norm.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.0.all_modules.0.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.1.weight shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.1.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.1.conv.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.norm.weight shape torch.Size([320])
encoder.stages.5.0.convs.1.norm.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.1.all_modules.0.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.1.weight shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.0.0.convs.0.conv.weight shape torch.Size([32, 1, 3, 3, 3])
decoder.encoder.stages.0.0.convs.0.conv.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.norm.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.norm.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.0.weight shape torch.Size([32, 1, 3, 3, 3])
decoder.encoder.stages.0.0.convs.0.all_modules.0.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.1.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.1.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.encoder.stages.0.0.convs.1.conv.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.norm.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.norm.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.encoder.stages.0.0.convs.1.all_modules.0.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.1.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.1.bias shape torch.Size([32])
decoder.encoder.stages.1.0.convs.0.conv.weight shape torch.Size([64, 32, 3, 3, 3])
decoder.encoder.stages.1.0.convs.0.conv.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.norm.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.norm.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.0.weight shape torch.Size([64, 32, 3, 3, 3])
decoder.encoder.stages.1.0.convs.0.all_modules.0.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.1.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.1.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.encoder.stages.1.0.convs.1.conv.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.norm.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.norm.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.encoder.stages.1.0.convs.1.all_modules.0.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.1.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.1.bias shape torch.Size([64])
decoder.encoder.stages.2.0.convs.0.conv.weight shape torch.Size([128, 64, 3, 3, 3])
decoder.encoder.stages.2.0.convs.0.conv.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.norm.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.norm.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.0.weight shape torch.Size([128, 64, 3, 3, 3])
decoder.encoder.stages.2.0.convs.0.all_modules.0.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.1.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.1.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.encoder.stages.2.0.convs.1.conv.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.norm.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.norm.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.encoder.stages.2.0.convs.1.all_modules.0.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.1.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.1.bias shape torch.Size([128])
decoder.encoder.stages.3.0.convs.0.conv.weight shape torch.Size([256, 128, 3, 3, 3])
decoder.encoder.stages.3.0.convs.0.conv.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.norm.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.norm.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.0.weight shape torch.Size([256, 128, 3, 3, 3])
decoder.encoder.stages.3.0.convs.0.all_modules.0.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.1.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.1.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.encoder.stages.3.0.convs.1.conv.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.norm.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.norm.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.encoder.stages.3.0.convs.1.all_modules.0.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.1.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.1.bias shape torch.Size([256])
decoder.encoder.stages.4.0.convs.0.conv.weight shape torch.Size([320, 256, 3, 3, 3])
decoder.encoder.stages.4.0.convs.0.conv.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.norm.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.norm.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.0.weight shape torch.Size([320, 256, 3, 3, 3])
decoder.encoder.stages.4.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.4.0.convs.1.conv.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.norm.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.norm.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.4.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.0.conv.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.norm.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.norm.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.1.conv.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.norm.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.norm.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.stages.0.convs.0.conv.weight shape torch.Size([320, 640, 3, 3, 3])
decoder.stages.0.convs.0.conv.bias shape torch.Size([320])
decoder.stages.0.convs.0.norm.weight shape torch.Size([320])
decoder.stages.0.convs.0.norm.bias shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.0.weight shape torch.Size([320, 640, 3, 3, 3])
decoder.stages.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.stages.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.stages.0.convs.1.conv.bias shape torch.Size([320])
decoder.stages.0.convs.1.norm.weight shape torch.Size([320])
decoder.stages.0.convs.1.norm.bias shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.stages.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.stages.1.convs.0.conv.weight shape torch.Size([256, 512, 3, 3, 3])
decoder.stages.1.convs.0.conv.bias shape torch.Size([256])
decoder.stages.1.convs.0.norm.weight shape torch.Size([256])
decoder.stages.1.convs.0.norm.bias shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.0.weight shape torch.Size([256, 512, 3, 3, 3])
decoder.stages.1.convs.0.all_modules.0.bias shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.1.weight shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.1.bias shape torch.Size([256])
decoder.stages.1.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.stages.1.convs.1.conv.bias shape torch.Size([256])
decoder.stages.1.convs.1.norm.weight shape torch.Size([256])
decoder.stages.1.convs.1.norm.bias shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.stages.1.convs.1.all_modules.0.bias shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.1.weight shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.1.bias shape torch.Size([256])
decoder.stages.2.convs.0.conv.weight shape torch.Size([128, 256, 3, 3, 3])
decoder.stages.2.convs.0.conv.bias shape torch.Size([128])
decoder.stages.2.convs.0.norm.weight shape torch.Size([128])
decoder.stages.2.convs.0.norm.bias shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.0.weight shape torch.Size([128, 256, 3, 3, 3])
decoder.stages.2.convs.0.all_modules.0.bias shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.1.weight shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.1.bias shape torch.Size([128])
decoder.stages.2.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.stages.2.convs.1.conv.bias shape torch.Size([128])
decoder.stages.2.convs.1.norm.weight shape torch.Size([128])
decoder.stages.2.convs.1.norm.bias shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.stages.2.convs.1.all_modules.0.bias shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.1.weight shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.1.bias shape torch.Size([128])
decoder.stages.3.convs.0.conv.weight shape torch.Size([64, 128, 3, 3, 3])
decoder.stages.3.convs.0.conv.bias shape torch.Size([64])
decoder.stages.3.convs.0.norm.weight shape torch.Size([64])
decoder.stages.3.convs.0.norm.bias shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.0.weight shape torch.Size([64, 128, 3, 3, 3])
decoder.stages.3.convs.0.all_modules.0.bias shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.1.weight shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.1.bias shape torch.Size([64])
decoder.stages.3.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.stages.3.convs.1.conv.bias shape torch.Size([64])
decoder.stages.3.convs.1.norm.weight shape torch.Size([64])
decoder.stages.3.convs.1.norm.bias shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.stages.3.convs.1.all_modules.0.bias shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.1.weight shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.1.bias shape torch.Size([64])
decoder.stages.4.convs.0.conv.weight shape torch.Size([32, 64, 3, 3, 3])
decoder.stages.4.convs.0.conv.bias shape torch.Size([32])
decoder.stages.4.convs.0.norm.weight shape torch.Size([32])
decoder.stages.4.convs.0.norm.bias shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.0.weight shape torch.Size([32, 64, 3, 3, 3])
decoder.stages.4.convs.0.all_modules.0.bias shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.1.weight shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.1.bias shape torch.Size([32])
decoder.stages.4.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.stages.4.convs.1.conv.bias shape torch.Size([32])
decoder.stages.4.convs.1.norm.weight shape torch.Size([32])
decoder.stages.4.convs.1.norm.bias shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.stages.4.convs.1.all_modules.0.bias shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.1.weight shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.1.bias shape torch.Size([32])
decoder.transpconvs.0.weight shape torch.Size([320, 320, 1, 2, 2])
decoder.transpconvs.0.bias shape torch.Size([320])
decoder.transpconvs.1.weight shape torch.Size([320, 256, 2, 2, 2])
decoder.transpconvs.1.bias shape torch.Size([256])
decoder.transpconvs.2.weight shape torch.Size([256, 128, 2, 2, 2])
decoder.transpconvs.2.bias shape torch.Size([128])
decoder.transpconvs.3.weight shape torch.Size([128, 64, 2, 2, 2])
decoder.transpconvs.3.bias shape torch.Size([64])
decoder.transpconvs.4.weight shape torch.Size([64, 32, 2, 2, 2])
decoder.transpconvs.4.bias shape torch.Size([32])
################### Done ###################
2024-12-18 18:05:38.565629: do_dummy_2d_data_aug: True
2024-12-18 18:05:38.567207: Using splits from existing split file: /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset309_Sohee_Calcium_OCT_CrossValidation/splits_final_40.json
2024-12-18 18:05:38.568892: The split file contains 5 splits.
2024-12-18 18:05:38.569822: Desired fold for training: 3
2024-12-18 18:05:38.570780: This split has 3 training and 6 validation cases.
using pin_memory on device 0
using pin_memory on device 0

This is the configuration used by this training:
Configuration name: 3d_32x160x128_b10
 {'data_identifier': 'nnUNetPlans_3d_fullres', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 10, 'patch_size': [32, 160, 128], 'median_image_size_in_voxels': [375.0, 498.0, 498.0], 'spacing': [1.0, 1.0, 1.0], 'normalization_schemes': ['ZScoreNormalization'], 'use_mask_for_norm': [False], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'architecture': {'network_class_name': 'dynamic_network_architectures.architectures.unet.PlainConvUNet', 'arch_kwargs': {'n_stages': 6, 'features_per_stage': [32, 64, 128, 256, 320, 320], 'conv_op': 'torch.nn.modules.conv.Conv3d', 'kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]], 'strides': [[1, 1, 1], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [1, 2, 2]], 'n_conv_per_stage': [2, 2, 2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2, 2, 2], 'conv_bias': True, 'norm_op': 'torch.nn.modules.instancenorm.InstanceNorm3d', 'norm_op_kwargs': {'eps': 1e-05, 'affine': True}, 'dropout_op': None, 'dropout_op_kwargs': None, 'nonlin': 'torch.nn.LeakyReLU', 'nonlin_kwargs': {'inplace': True}, 'deep_supervision': True}, '_kw_requires_import': ['conv_op', 'norm_op', 'dropout_op', 'nonlin']}, 'batch_dice': True, 'inherits_from': '3d_fullres'} 

These are the global plan.json settings:
 {'dataset_name': 'Dataset309_Sohee_Calcium_OCT_CrossValidation', 'plans_name': 'nnUNetPlans', 'original_median_spacing_after_transp': [1.0, 1.0, 1.0], 'original_median_shape_after_transp': [375, 498, 498], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 0.4977976381778717, 'mean': 0.13507525622844696, 'median': 0.09599608182907104, 'min': 0.0, 'percentile_00_5': 0.014754901640117168, 'percentile_99_5': 0.4977976381778717, 'std': 0.12025152146816254}}} 

2024-12-18 18:06:03.641518: unpacking dataset...
using pin_memory on device 0
using pin_memory on device 0

This is the configuration used by this training:
Configuration name: 3d_32x160x128_b10
 {'data_identifier': 'nnUNetPlans_3d_fullres', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 10, 'patch_size': [32, 160, 128], 'median_image_size_in_voxels': [375.0, 498.0, 498.0], 'spacing': [1.0, 1.0, 1.0], 'normalization_schemes': ['ZScoreNormalization'], 'use_mask_for_norm': [False], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'architecture': {'network_class_name': 'dynamic_network_architectures.architectures.unet.PlainConvUNet', 'arch_kwargs': {'n_stages': 6, 'features_per_stage': [32, 64, 128, 256, 320, 320], 'conv_op': 'torch.nn.modules.conv.Conv3d', 'kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]], 'strides': [[1, 1, 1], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [1, 2, 2]], 'n_conv_per_stage': [2, 2, 2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2, 2, 2], 'conv_bias': True, 'norm_op': 'torch.nn.modules.instancenorm.InstanceNorm3d', 'norm_op_kwargs': {'eps': 1e-05, 'affine': True}, 'dropout_op': None, 'dropout_op_kwargs': None, 'nonlin': 'torch.nn.LeakyReLU', 'nonlin_kwargs': {'inplace': True}, 'deep_supervision': True}, '_kw_requires_import': ['conv_op', 'norm_op', 'dropout_op', 'nonlin']}, 'batch_dice': True, 'inherits_from': '3d_fullres'} 

These are the global plan.json settings:
 {'dataset_name': 'Dataset309_Sohee_Calcium_OCT_CrossValidation', 'plans_name': 'nnUNetPlans', 'original_median_spacing_after_transp': [1.0, 1.0, 1.0], 'original_median_shape_after_transp': [375, 498, 498], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 0.4977976381778717, 'mean': 0.13507525622844696, 'median': 0.09599608182907104, 'min': 0.0, 'percentile_00_5': 0.014754901640117168, 'percentile_99_5': 0.4977976381778717, 'std': 0.12025152146816254}}} 

2024-12-18 18:06:18.280063: unpacking dataset...
2024-12-18 18:06:07.988441: unpacking done...
2024-12-18 18:06:08.241848: Unable to plot network architecture: nnUNet_compile is enabled!
2024-12-18 18:06:08.324643: 
2024-12-18 18:06:08.325565: Epoch 0
2024-12-18 18:06:08.327051: Current learning rate: 0.01
2024-12-18 18:14:06.205412: Validation loss improved from 1000.00000 to -0.36359! Patience: 0/50
2024-12-18 18:14:06.206736: train_loss -0.3138
2024-12-18 18:14:06.207738: val_loss -0.3636
2024-12-18 18:14:06.208656: Pseudo dice [0.6342]
2024-12-18 18:14:06.209394: Epoch time: 477.88 s
2024-12-18 18:14:06.210163: Yayy! New best EMA pseudo Dice: 0.6342
2024-12-18 18:14:08.324022: 
2024-12-18 18:14:08.325223: Epoch 1
2024-12-18 18:14:08.325987: Current learning rate: 0.00994
2024-12-18 18:20:28.020981: Validation loss improved from -0.36359 to -0.41350! Patience: 0/50
2024-12-18 18:20:28.021948: train_loss -0.4877
2024-12-18 18:20:28.022789: val_loss -0.4135
2024-12-18 18:20:28.023587: Pseudo dice [0.663]
2024-12-18 18:20:28.024473: Epoch time: 379.7 s
2024-12-18 18:20:28.025260: Yayy! New best EMA pseudo Dice: 0.637
2024-12-18 18:20:29.763335: 
2024-12-18 18:20:29.764812: Epoch 2
2024-12-18 18:20:29.765763: Current learning rate: 0.00988
2024-12-18 18:26:28.607646: Validation loss improved from -0.41350 to -0.46499! Patience: 0/50
2024-12-18 18:26:28.608640: train_loss -0.5292
2024-12-18 18:26:28.609930: val_loss -0.465
2024-12-18 18:26:28.611276: Pseudo dice [0.7035]
2024-12-18 18:26:28.612615: Epoch time: 358.85 s
2024-12-18 18:26:28.613791: Yayy! New best EMA pseudo Dice: 0.6437
2024-12-18 18:26:30.544570: 
2024-12-18 18:26:30.545886: Epoch 3
2024-12-18 18:26:30.547151: Current learning rate: 0.00982
2024-12-18 18:31:50.199124: Validation loss improved from -0.46499 to -0.49288! Patience: 0/50
2024-12-18 18:31:50.200117: train_loss -0.5611
2024-12-18 18:31:50.201222: val_loss -0.4929
2024-12-18 18:31:50.202292: Pseudo dice [0.7099]
2024-12-18 18:31:50.203201: Epoch time: 319.66 s
2024-12-18 18:31:50.204148: Yayy! New best EMA pseudo Dice: 0.6503
2024-12-18 18:31:52.007388: 
2024-12-18 18:31:52.008730: Epoch 4
2024-12-18 18:31:52.009481: Current learning rate: 0.00976
2024-12-18 18:37:24.293747: Validation loss did not improve from -0.49288. Patience: 1/50
2024-12-18 18:37:24.294542: train_loss -0.5835
2024-12-18 18:37:24.295379: val_loss -0.3538
2024-12-18 18:37:24.296042: Pseudo dice [0.6282]
2024-12-18 18:37:24.296691: Epoch time: 332.29 s
2024-12-18 18:37:26.257691: 
2024-12-18 18:37:26.259119: Epoch 5
2024-12-18 18:37:26.259951: Current learning rate: 0.0097
2024-12-18 18:43:05.458241: Validation loss did not improve from -0.49288. Patience: 2/50
2024-12-18 18:43:05.459216: train_loss -0.586
2024-12-18 18:43:05.460106: val_loss -0.4351
2024-12-18 18:43:05.460805: Pseudo dice [0.6846]
2024-12-18 18:43:05.461512: Epoch time: 339.2 s
2024-12-18 18:43:05.462212: Yayy! New best EMA pseudo Dice: 0.6518
2024-12-18 18:43:07.267087: 
2024-12-18 18:43:07.268578: Epoch 6
2024-12-18 18:43:07.269622: Current learning rate: 0.00964
2024-12-18 18:48:15.753834: Validation loss did not improve from -0.49288. Patience: 3/50
2024-12-18 18:48:15.754965: train_loss -0.6063
2024-12-18 18:48:15.755822: val_loss -0.4168
2024-12-18 18:48:15.756649: Pseudo dice [0.6853]
2024-12-18 18:48:15.757285: Epoch time: 308.49 s
2024-12-18 18:48:15.757941: Yayy! New best EMA pseudo Dice: 0.6551
2024-12-18 18:48:17.690969: 
2024-12-18 18:48:17.692308: Epoch 7
2024-12-18 18:48:17.693141: Current learning rate: 0.00958
2024-12-18 18:53:22.412717: Validation loss did not improve from -0.49288. Patience: 4/50
2024-12-18 18:53:22.413625: train_loss -0.6105
2024-12-18 18:53:22.414380: val_loss -0.4049
2024-12-18 18:53:22.415077: Pseudo dice [0.6628]
2024-12-18 18:53:22.415874: Epoch time: 304.72 s
2024-12-18 18:53:22.416648: Yayy! New best EMA pseudo Dice: 0.6559
2024-12-18 18:53:24.859647: 
2024-12-18 18:53:24.860854: Epoch 8
2024-12-18 18:53:24.861786: Current learning rate: 0.00952
2024-12-18 18:58:31.675338: Validation loss did not improve from -0.49288. Patience: 5/50
2024-12-18 18:58:31.676207: train_loss -0.6265
2024-12-18 18:58:31.677074: val_loss -0.4631
2024-12-18 18:58:31.677990: Pseudo dice [0.7063]
2024-12-18 18:58:31.678765: Epoch time: 306.82 s
2024-12-18 18:58:31.679650: Yayy! New best EMA pseudo Dice: 0.6609
2024-12-18 18:58:33.628716: 
2024-12-18 18:58:33.629984: Epoch 9
2024-12-18 18:58:33.630919: Current learning rate: 0.00946
2024-12-18 19:03:46.458726: Validation loss did not improve from -0.49288. Patience: 6/50
2024-12-18 19:03:46.459776: train_loss -0.6353
2024-12-18 19:03:46.460688: val_loss -0.4618
2024-12-18 19:03:46.461355: Pseudo dice [0.6972]
2024-12-18 19:03:46.461989: Epoch time: 312.83 s
2024-12-18 19:03:46.864816: Yayy! New best EMA pseudo Dice: 0.6645
2024-12-18 19:03:48.736915: 
2024-12-18 19:03:48.738161: Epoch 10
2024-12-18 19:03:48.739057: Current learning rate: 0.0094
2024-12-18 19:08:07.953452: Validation loss did not improve from -0.49288. Patience: 7/50
2024-12-18 19:08:07.954401: train_loss -0.6383
2024-12-18 19:08:07.955394: val_loss -0.4789
2024-12-18 19:08:07.956269: Pseudo dice [0.7238]
2024-12-18 19:08:07.957182: Epoch time: 259.22 s
2024-12-18 19:08:07.957988: Yayy! New best EMA pseudo Dice: 0.6705
2024-12-18 19:08:09.766806: 
2024-12-18 19:08:09.767997: Epoch 11
2024-12-18 19:08:09.768957: Current learning rate: 0.00934
2024-12-18 19:13:13.724267: Validation loss did not improve from -0.49288. Patience: 8/50
2024-12-18 19:13:13.728335: train_loss -0.6463
2024-12-18 19:13:13.730673: val_loss -0.449
2024-12-18 19:13:13.731577: Pseudo dice [0.6951]
2024-12-18 19:13:13.732693: Epoch time: 303.96 s
2024-12-18 19:13:13.733641: Yayy! New best EMA pseudo Dice: 0.6729
2024-12-18 19:13:15.556302: 
2024-12-18 19:13:15.557763: Epoch 12
2024-12-18 19:13:15.558770: Current learning rate: 0.00928
2024-12-18 19:18:12.293504: Validation loss did not improve from -0.49288. Patience: 9/50
2024-12-18 19:18:12.294379: train_loss -0.6596
2024-12-18 19:18:12.295208: val_loss -0.4574
2024-12-18 19:18:12.296010: Pseudo dice [0.7044]
2024-12-18 19:18:12.296724: Epoch time: 296.74 s
2024-12-18 19:18:12.297650: Yayy! New best EMA pseudo Dice: 0.6761
2024-12-18 19:18:14.102384: 
2024-12-18 19:18:14.103894: Epoch 13
2024-12-18 19:18:14.104862: Current learning rate: 0.00922
2024-12-18 19:23:41.110672: Validation loss did not improve from -0.49288. Patience: 10/50
2024-12-18 19:23:41.113349: train_loss -0.6632
2024-12-18 19:23:41.114459: val_loss -0.4886
2024-12-18 19:23:41.115355: Pseudo dice [0.705]
2024-12-18 19:23:41.116134: Epoch time: 327.01 s
2024-12-18 19:23:41.116905: Yayy! New best EMA pseudo Dice: 0.679
2024-12-18 19:23:43.001776: 
2024-12-18 19:23:43.003166: Epoch 14
2024-12-18 19:23:43.004010: Current learning rate: 0.00916
2024-12-18 19:28:34.558283: Validation loss did not improve from -0.49288. Patience: 11/50
2024-12-18 19:28:34.559180: train_loss -0.6642
2024-12-18 19:28:34.560169: val_loss -0.4636
2024-12-18 19:28:34.561017: Pseudo dice [0.708]
2024-12-18 19:28:34.562000: Epoch time: 291.56 s
2024-12-18 19:28:34.907002: Yayy! New best EMA pseudo Dice: 0.6819
2024-12-18 19:28:36.791442: 
2024-12-18 19:28:36.793164: Epoch 15
2024-12-18 19:28:36.794114: Current learning rate: 0.0091
2024-12-18 19:33:49.898608: Validation loss improved from -0.49288 to -0.49675! Patience: 11/50
2024-12-18 19:33:49.899521: train_loss -0.6664
2024-12-18 19:33:49.900764: val_loss -0.4967
2024-12-18 19:33:49.901875: Pseudo dice [0.7226]
2024-12-18 19:33:49.902899: Epoch time: 313.11 s
2024-12-18 19:33:49.903946: Yayy! New best EMA pseudo Dice: 0.6859
2024-12-18 19:33:51.722252: 
2024-12-18 19:33:51.723492: Epoch 16
2024-12-18 19:33:51.724666: Current learning rate: 0.00903
2024-12-18 19:38:51.499336: Validation loss did not improve from -0.49675. Patience: 1/50
2024-12-18 19:38:51.500328: train_loss -0.6717
2024-12-18 19:38:51.501093: val_loss -0.4134
2024-12-18 19:38:51.501740: Pseudo dice [0.6672]
2024-12-18 19:38:51.502407: Epoch time: 299.78 s
2024-12-18 19:38:52.959096: 
2024-12-18 19:38:52.959941: Epoch 17
2024-12-18 19:38:52.960729: Current learning rate: 0.00897
2024-12-18 19:43:04.047018: Validation loss improved from -0.49675 to -0.51603! Patience: 1/50
2024-12-18 19:43:04.048223: train_loss -0.6822
2024-12-18 19:43:04.049157: val_loss -0.516
2024-12-18 19:43:04.049806: Pseudo dice [0.7477]
2024-12-18 19:43:04.050578: Epoch time: 251.09 s
2024-12-18 19:43:04.051472: Yayy! New best EMA pseudo Dice: 0.6904
2024-12-18 19:43:05.871011: 
2024-12-18 19:43:05.872528: Epoch 18
2024-12-18 19:43:05.873409: Current learning rate: 0.00891
2024-12-18 19:48:23.463176: Validation loss did not improve from -0.51603. Patience: 1/50
2024-12-18 19:48:23.464216: train_loss -0.6823
2024-12-18 19:48:23.464995: val_loss -0.4994
2024-12-18 19:48:23.465621: Pseudo dice [0.718]
2024-12-18 19:48:23.466371: Epoch time: 317.59 s
2024-12-18 19:48:23.466995: Yayy! New best EMA pseudo Dice: 0.6932
2024-12-18 19:48:25.718397: 
2024-12-18 19:48:25.719792: Epoch 19
2024-12-18 19:48:25.720809: Current learning rate: 0.00885
2024-12-18 19:54:01.616679: Validation loss did not improve from -0.51603. Patience: 2/50
2024-12-18 19:54:01.617725: train_loss -0.6902
2024-12-18 19:54:01.618561: val_loss -0.4495
2024-12-18 19:54:01.619393: Pseudo dice [0.709]
2024-12-18 19:54:01.620376: Epoch time: 335.9 s
2024-12-18 19:54:02.029679: Yayy! New best EMA pseudo Dice: 0.6948
2024-12-18 19:54:03.975476: 
2024-12-18 19:54:03.976865: Epoch 20
2024-12-18 19:54:03.977724: Current learning rate: 0.00879
2024-12-18 19:59:29.460510: Validation loss did not improve from -0.51603. Patience: 3/50
2024-12-18 19:59:29.462203: train_loss -0.6864
2024-12-18 19:59:29.462985: val_loss -0.5037
2024-12-18 19:59:29.463784: Pseudo dice [0.7223]
2024-12-18 19:59:29.464456: Epoch time: 325.49 s
2024-12-18 19:59:29.465029: Yayy! New best EMA pseudo Dice: 0.6975
2024-12-18 19:59:31.451745: 
2024-12-18 19:59:31.453123: Epoch 21
2024-12-18 19:59:31.453858: Current learning rate: 0.00873
2024-12-18 20:04:43.960181: Validation loss did not improve from -0.51603. Patience: 4/50
2024-12-18 20:04:43.961247: train_loss -0.6997
2024-12-18 20:04:43.962126: val_loss -0.4494
2024-12-18 20:04:43.962802: Pseudo dice [0.6996]
2024-12-18 20:04:43.963524: Epoch time: 312.51 s
2024-12-18 20:04:43.964295: Yayy! New best EMA pseudo Dice: 0.6977
2024-12-18 20:04:45.749895: 
2024-12-18 20:04:45.751395: Epoch 22
2024-12-18 20:04:45.752275: Current learning rate: 0.00867
2024-12-18 20:09:36.467110: Validation loss did not improve from -0.51603. Patience: 5/50
2024-12-18 20:09:36.469455: train_loss -0.7027
2024-12-18 20:09:36.470449: val_loss -0.5059
2024-12-18 20:09:36.471218: Pseudo dice [0.7319]
2024-12-18 20:09:36.471997: Epoch time: 290.72 s
2024-12-18 20:09:36.472765: Yayy! New best EMA pseudo Dice: 0.7011
2024-12-18 20:09:38.327044: 
2024-12-18 20:09:38.328238: Epoch 23
2024-12-18 20:09:38.329048: Current learning rate: 0.00861
2024-12-18 20:15:14.883888: Validation loss did not improve from -0.51603. Patience: 6/50
2024-12-18 20:15:14.885046: train_loss -0.7097
2024-12-18 20:15:14.886129: val_loss -0.5
2024-12-18 20:15:14.887014: Pseudo dice [0.7324]
2024-12-18 20:15:14.887860: Epoch time: 336.56 s
2024-12-18 20:15:14.888606: Yayy! New best EMA pseudo Dice: 0.7043
2024-12-18 20:15:16.689056: 
2024-12-18 20:15:16.690052: Epoch 24
2024-12-18 20:15:16.690853: Current learning rate: 0.00855
2024-12-18 20:20:38.303300: Validation loss did not improve from -0.51603. Patience: 7/50
2024-12-18 20:20:38.305998: train_loss -0.71
2024-12-18 20:20:38.306884: val_loss -0.4725
2024-12-18 20:20:38.307605: Pseudo dice [0.7142]
2024-12-18 20:20:38.308626: Epoch time: 321.62 s
2024-12-18 20:20:38.741840: Yayy! New best EMA pseudo Dice: 0.7053
2024-12-18 20:20:40.627185: 
2024-12-18 20:20:40.628583: Epoch 25
2024-12-18 20:20:40.629266: Current learning rate: 0.00849
2024-12-18 20:26:14.552168: Validation loss did not improve from -0.51603. Patience: 8/50
2024-12-18 20:26:14.553197: train_loss -0.7088
2024-12-18 20:26:14.553911: val_loss -0.4514
2024-12-18 20:26:14.554691: Pseudo dice [0.6983]
2024-12-18 20:26:14.555407: Epoch time: 333.93 s
2024-12-18 20:26:15.984635: 
2024-12-18 20:26:15.985843: Epoch 26
2024-12-18 20:26:15.986781: Current learning rate: 0.00843
2024-12-18 20:32:00.530062: Validation loss did not improve from -0.51603. Patience: 9/50
2024-12-18 20:32:00.533122: train_loss -0.715
2024-12-18 20:32:00.534694: val_loss -0.4694
2024-12-18 20:32:00.535880: Pseudo dice [0.7144]
2024-12-18 20:32:00.536961: Epoch time: 344.55 s
2024-12-18 20:32:00.538183: Yayy! New best EMA pseudo Dice: 0.7056
2024-12-18 20:32:02.515255: 
2024-12-18 20:32:02.516825: Epoch 27
2024-12-18 20:32:02.518026: Current learning rate: 0.00836
2024-12-18 20:36:52.576586: Validation loss did not improve from -0.51603. Patience: 10/50
2024-12-18 20:36:52.577380: train_loss -0.7155
2024-12-18 20:36:52.578146: val_loss -0.4978
2024-12-18 20:36:52.578825: Pseudo dice [0.7292]
2024-12-18 20:36:52.579505: Epoch time: 290.06 s
2024-12-18 20:36:52.580158: Yayy! New best EMA pseudo Dice: 0.7079
2024-12-18 20:36:54.407658: 
2024-12-18 20:36:54.408864: Epoch 28
2024-12-18 20:36:54.409718: Current learning rate: 0.0083
2024-12-18 20:42:32.586333: Validation loss did not improve from -0.51603. Patience: 11/50
2024-12-18 20:42:32.587451: train_loss -0.7226
2024-12-18 20:42:32.588273: val_loss -0.4716
2024-12-18 20:42:32.589074: Pseudo dice [0.7198]
2024-12-18 20:42:32.589884: Epoch time: 338.18 s
2024-12-18 20:42:32.590676: Yayy! New best EMA pseudo Dice: 0.7091
2024-12-18 20:42:34.420946: 
2024-12-18 20:42:34.422221: Epoch 29
2024-12-18 20:42:34.423052: Current learning rate: 0.00824
2024-12-18 20:47:52.831248: Validation loss did not improve from -0.51603. Patience: 12/50
2024-12-18 20:47:52.832328: train_loss -0.7247
2024-12-18 20:47:52.833213: val_loss -0.4518
2024-12-18 20:47:52.833952: Pseudo dice [0.7096]
2024-12-18 20:47:52.834643: Epoch time: 318.41 s
2024-12-18 20:47:53.206016: Yayy! New best EMA pseudo Dice: 0.7092
2024-12-18 20:47:55.400489: 
2024-12-18 20:47:55.401954: Epoch 30
2024-12-18 20:47:55.402869: Current learning rate: 0.00818
2024-12-18 20:53:07.626599: Validation loss did not improve from -0.51603. Patience: 13/50
2024-12-18 20:53:07.627788: train_loss -0.7301
2024-12-18 20:53:07.628569: val_loss -0.4942
2024-12-18 20:53:07.629218: Pseudo dice [0.7318]
2024-12-18 20:53:07.629876: Epoch time: 312.23 s
2024-12-18 20:53:07.630590: Yayy! New best EMA pseudo Dice: 0.7114
2024-12-18 20:53:09.516248: 
2024-12-18 20:53:09.517658: Epoch 31
2024-12-18 20:53:09.518544: Current learning rate: 0.00812
2024-12-18 20:58:47.961534: Validation loss did not improve from -0.51603. Patience: 14/50
2024-12-18 20:58:47.962544: train_loss -0.7273
2024-12-18 20:58:47.963468: val_loss -0.4574
2024-12-18 20:58:47.964321: Pseudo dice [0.7085]
2024-12-18 20:58:47.965216: Epoch time: 338.45 s
2024-12-18 20:58:49.398169: 
2024-12-18 20:58:49.399297: Epoch 32
2024-12-18 20:58:49.400168: Current learning rate: 0.00806
2024-12-18 21:04:20.628606: Validation loss improved from -0.51603 to -0.52638! Patience: 14/50
2024-12-18 21:04:20.630485: train_loss -0.7351
2024-12-18 21:04:20.631605: val_loss -0.5264
2024-12-18 21:04:20.632418: Pseudo dice [0.7435]
2024-12-18 21:04:20.633302: Epoch time: 331.23 s
2024-12-18 21:04:20.634053: Yayy! New best EMA pseudo Dice: 0.7144
2024-12-18 21:04:22.583640: 
2024-12-18 21:04:22.585299: Epoch 33
2024-12-18 21:04:22.586304: Current learning rate: 0.008
2024-12-18 21:09:41.408003: Validation loss did not improve from -0.52638. Patience: 1/50
2024-12-18 21:09:41.409062: train_loss -0.7363
2024-12-18 21:09:41.409990: val_loss -0.4089
2024-12-18 21:09:41.410688: Pseudo dice [0.6797]
2024-12-18 21:09:41.411388: Epoch time: 318.83 s
2024-12-18 21:09:42.904783: 
2024-12-18 21:09:42.906264: Epoch 34
2024-12-18 21:09:42.906977: Current learning rate: 0.00793
2024-12-18 21:14:49.749777: Validation loss did not improve from -0.52638. Patience: 2/50
2024-12-18 21:14:49.750671: train_loss -0.7365
2024-12-18 21:14:49.751478: val_loss -0.4983
2024-12-18 21:14:49.752341: Pseudo dice [0.7285]
2024-12-18 21:14:49.753323: Epoch time: 306.85 s
2024-12-18 21:14:51.793979: 
2024-12-18 21:14:51.795299: Epoch 35
2024-12-18 21:14:51.796173: Current learning rate: 0.00787
2024-12-18 21:20:04.624182: Validation loss did not improve from -0.52638. Patience: 3/50
2024-12-18 21:20:04.625150: train_loss -0.7427
2024-12-18 21:20:04.626139: val_loss -0.4813
2024-12-18 21:20:04.627140: Pseudo dice [0.7329]
2024-12-18 21:20:04.628154: Epoch time: 312.83 s
2024-12-18 21:20:04.629097: Yayy! New best EMA pseudo Dice: 0.7147
2024-12-18 21:20:06.559733: 
2024-12-18 21:20:06.561366: Epoch 36
2024-12-18 21:20:06.562345: Current learning rate: 0.00781
2024-12-18 21:25:53.459581: Validation loss did not improve from -0.52638. Patience: 4/50
2024-12-18 21:25:53.479765: train_loss -0.7359
2024-12-18 21:25:53.481823: val_loss -0.494
2024-12-18 21:25:53.482778: Pseudo dice [0.7331]
2024-12-18 21:25:53.484046: Epoch time: 346.9 s
2024-12-18 21:25:53.485091: Yayy! New best EMA pseudo Dice: 0.7165
2024-12-18 21:25:55.394359: 
2024-12-18 21:25:55.395568: Epoch 37
2024-12-18 21:25:55.396420: Current learning rate: 0.00775
2024-12-18 21:31:27.447380: Validation loss did not improve from -0.52638. Patience: 5/50
2024-12-18 21:31:27.448388: train_loss -0.7392
2024-12-18 21:31:27.449388: val_loss -0.4925
2024-12-18 21:31:27.450392: Pseudo dice [0.7312]
2024-12-18 21:31:27.451322: Epoch time: 332.06 s
2024-12-18 21:31:27.452227: Yayy! New best EMA pseudo Dice: 0.718
2024-12-18 21:31:29.278003: 
2024-12-18 21:31:29.279002: Epoch 38
2024-12-18 21:31:29.279746: Current learning rate: 0.00769
2024-12-18 21:37:03.306527: Validation loss did not improve from -0.52638. Patience: 6/50
2024-12-18 21:37:03.307519: train_loss -0.747
2024-12-18 21:37:03.308227: val_loss -0.4815
2024-12-18 21:37:03.309019: Pseudo dice [0.7178]
2024-12-18 21:37:03.309811: Epoch time: 334.03 s
2024-12-18 21:37:04.845470: 
2024-12-18 21:37:04.846673: Epoch 39
2024-12-18 21:37:04.847414: Current learning rate: 0.00763
2024-12-18 21:42:23.496692: Validation loss did not improve from -0.52638. Patience: 7/50
2024-12-18 21:42:23.497634: train_loss -0.749
2024-12-18 21:42:23.498470: val_loss -0.4998
2024-12-18 21:42:23.499307: Pseudo dice [0.7286]
2024-12-18 21:42:23.500178: Epoch time: 318.65 s
2024-12-18 21:42:23.851867: Yayy! New best EMA pseudo Dice: 0.719
2024-12-18 21:42:26.856002: 
2024-12-18 21:42:26.856960: Epoch 40
2024-12-18 21:42:26.857886: Current learning rate: 0.00756
2024-12-18 21:47:32.911031: Validation loss did not improve from -0.52638. Patience: 8/50
2024-12-18 21:47:32.912044: train_loss -0.7482
2024-12-18 21:47:32.912924: val_loss -0.4788
2024-12-18 21:47:32.913763: Pseudo dice [0.7249]
2024-12-18 21:47:32.914554: Epoch time: 306.06 s
2024-12-18 21:47:32.915331: Yayy! New best EMA pseudo Dice: 0.7196
2024-12-18 21:47:34.806764: 
2024-12-18 21:47:34.808348: Epoch 41
2024-12-18 21:47:34.809417: Current learning rate: 0.0075
2024-12-18 21:53:22.048234: Validation loss did not improve from -0.52638. Patience: 9/50
2024-12-18 21:53:22.049193: train_loss -0.7484
2024-12-18 21:53:22.050105: val_loss -0.4772
2024-12-18 21:53:22.051050: Pseudo dice [0.7263]
2024-12-18 21:53:22.051822: Epoch time: 347.24 s
2024-12-18 21:53:22.052587: Yayy! New best EMA pseudo Dice: 0.7203
2024-12-18 21:53:23.949127: 
2024-12-18 21:53:23.950437: Epoch 42
2024-12-18 21:53:23.951416: Current learning rate: 0.00744
2024-12-18 21:58:54.893223: Validation loss did not improve from -0.52638. Patience: 10/50
2024-12-18 21:58:54.894310: train_loss -0.7477
2024-12-18 21:58:54.895225: val_loss -0.5176
2024-12-18 21:58:54.896037: Pseudo dice [0.7377]
2024-12-18 21:58:54.896754: Epoch time: 330.95 s
2024-12-18 21:58:54.897596: Yayy! New best EMA pseudo Dice: 0.722
2024-12-18 21:58:56.673625: 
2024-12-18 21:58:56.674919: Epoch 43
2024-12-18 21:58:56.675782: Current learning rate: 0.00738
2024-12-18 22:04:48.302901: Validation loss did not improve from -0.52638. Patience: 11/50
2024-12-18 22:04:48.304017: train_loss -0.7536
2024-12-18 22:04:48.304863: val_loss -0.4791
2024-12-18 22:04:48.305615: Pseudo dice [0.7276]
2024-12-18 22:04:48.306450: Epoch time: 351.63 s
2024-12-18 22:04:48.307250: Yayy! New best EMA pseudo Dice: 0.7226
2024-12-18 22:04:50.096551: 
2024-12-18 22:04:50.097913: Epoch 44
2024-12-18 22:04:50.098607: Current learning rate: 0.00732
2024-12-18 22:10:12.845120: Validation loss did not improve from -0.52638. Patience: 12/50
2024-12-18 22:10:12.846028: train_loss -0.7474
2024-12-18 22:10:12.846999: val_loss -0.4694
2024-12-18 22:10:12.847708: Pseudo dice [0.7186]
2024-12-18 22:10:12.848533: Epoch time: 322.75 s
2024-12-18 22:10:14.760400: 
2024-12-18 22:10:14.761763: Epoch 45
2024-12-18 22:10:14.762585: Current learning rate: 0.00725
2024-12-18 22:15:04.867496: Validation loss did not improve from -0.52638. Patience: 13/50
2024-12-18 22:15:04.868567: train_loss -0.7544
2024-12-18 22:15:04.869537: val_loss -0.4614
2024-12-18 22:15:04.870415: Pseudo dice [0.7091]
2024-12-18 22:15:04.871230: Epoch time: 290.11 s
2024-12-18 22:15:06.291303: 
2024-12-18 22:15:06.293252: Epoch 46
2024-12-18 22:15:06.294057: Current learning rate: 0.00719
2024-12-18 22:20:16.633091: Validation loss did not improve from -0.52638. Patience: 14/50
2024-12-18 22:20:16.633856: train_loss -0.7572
2024-12-18 22:20:16.634895: val_loss -0.4758
2024-12-18 22:20:16.635843: Pseudo dice [0.7262]
2024-12-18 22:20:16.636823: Epoch time: 310.34 s
2024-12-18 22:20:18.011113: 
2024-12-18 22:20:18.012449: Epoch 47
2024-12-18 22:20:18.013474: Current learning rate: 0.00713
2024-12-18 22:25:39.107397: Validation loss did not improve from -0.52638. Patience: 15/50
2024-12-18 22:25:39.108621: train_loss -0.7537
2024-12-18 22:25:39.110706: val_loss -0.4635
2024-12-18 22:25:39.111596: Pseudo dice [0.7284]
2024-12-18 22:25:39.112711: Epoch time: 321.1 s
2024-12-18 22:25:40.499436: 
2024-12-18 22:25:40.500807: Epoch 48
2024-12-18 22:25:40.501584: Current learning rate: 0.00707
2024-12-18 22:31:16.402659: Validation loss did not improve from -0.52638. Patience: 16/50
2024-12-18 22:31:16.405010: train_loss -0.7584
2024-12-18 22:31:16.406317: val_loss -0.4856
2024-12-18 22:31:16.407395: Pseudo dice [0.7277]
2024-12-18 22:31:16.408486: Epoch time: 335.91 s
2024-12-18 22:31:16.409420: Yayy! New best EMA pseudo Dice: 0.7227
2024-12-18 22:31:18.287181: 
2024-12-18 22:31:18.289295: Epoch 49
2024-12-18 22:31:18.290286: Current learning rate: 0.007
2024-12-18 22:36:44.133081: Validation loss did not improve from -0.52638. Patience: 17/50
2024-12-18 22:36:44.134968: train_loss -0.7605
2024-12-18 22:36:44.136460: val_loss -0.4943
2024-12-18 22:36:44.137316: Pseudo dice [0.732]
2024-12-18 22:36:44.138103: Epoch time: 325.85 s
2024-12-18 22:36:44.545787: Yayy! New best EMA pseudo Dice: 0.7236
2024-12-18 22:36:46.336411: 
2024-12-18 22:36:46.337964: Epoch 50
2024-12-18 22:36:46.338883: Current learning rate: 0.00694
2024-12-18 22:41:41.800091: Validation loss did not improve from -0.52638. Patience: 18/50
2024-12-18 22:41:41.800914: train_loss -0.7639
2024-12-18 22:41:41.801773: val_loss -0.4907
2024-12-18 22:41:41.802521: Pseudo dice [0.73]
2024-12-18 22:41:41.803261: Epoch time: 295.47 s
2024-12-18 22:41:41.803919: Yayy! New best EMA pseudo Dice: 0.7242
2024-12-18 22:41:43.613598: 
2024-12-18 22:41:43.615045: Epoch 51
2024-12-18 22:41:43.615875: Current learning rate: 0.00688
2024-12-18 22:47:18.658058: Validation loss did not improve from -0.52638. Patience: 19/50
2024-12-18 22:47:18.659159: train_loss -0.7629
2024-12-18 22:47:18.660137: val_loss -0.4917
2024-12-18 22:47:18.660915: Pseudo dice [0.7356]
2024-12-18 22:47:18.661701: Epoch time: 335.05 s
2024-12-18 22:47:18.662354: Yayy! New best EMA pseudo Dice: 0.7254
2024-12-18 22:47:21.012186: 
2024-12-18 22:47:21.013494: Epoch 52
2024-12-18 22:47:21.014238: Current learning rate: 0.00682
2024-12-18 22:53:06.310733: Validation loss did not improve from -0.52638. Patience: 20/50
2024-12-18 22:53:06.311844: train_loss -0.7673
2024-12-18 22:53:06.312643: val_loss -0.4373
2024-12-18 22:53:06.313529: Pseudo dice [0.6894]
2024-12-18 22:53:06.314376: Epoch time: 345.3 s
2024-12-18 22:53:07.799777: 
2024-12-18 22:53:07.801201: Epoch 53
2024-12-18 22:53:07.801968: Current learning rate: 0.00675
2024-12-18 22:59:07.174441: Validation loss did not improve from -0.52638. Patience: 21/50
2024-12-18 22:59:07.175307: train_loss -0.7678
2024-12-18 22:59:07.176029: val_loss -0.454
2024-12-18 22:59:07.176763: Pseudo dice [0.7024]
2024-12-18 22:59:07.177487: Epoch time: 359.38 s
2024-12-18 22:59:08.685756: 
2024-12-18 22:59:08.686660: Epoch 54
2024-12-18 22:59:08.687448: Current learning rate: 0.00669
2024-12-18 23:04:38.577002: Validation loss did not improve from -0.52638. Patience: 22/50
2024-12-18 23:04:38.578020: train_loss -0.7679
2024-12-18 23:04:38.578896: val_loss -0.4694
2024-12-18 23:04:38.579694: Pseudo dice [0.7231]
2024-12-18 23:04:38.580347: Epoch time: 329.89 s
2024-12-18 23:04:40.395137: 
2024-12-18 23:04:40.396457: Epoch 55
2024-12-18 23:04:40.397239: Current learning rate: 0.00663
2024-12-18 23:09:54.348883: Validation loss did not improve from -0.52638. Patience: 23/50
2024-12-18 23:09:54.349809: train_loss -0.765
2024-12-18 23:09:54.350568: val_loss -0.5048
2024-12-18 23:09:54.351288: Pseudo dice [0.7367]
2024-12-18 23:09:54.352115: Epoch time: 313.96 s
2024-12-18 23:09:55.801569: 
2024-12-18 23:09:55.802852: Epoch 56
2024-12-18 23:09:55.803573: Current learning rate: 0.00657
2024-12-18 23:15:26.233588: Validation loss did not improve from -0.52638. Patience: 24/50
2024-12-18 23:15:26.234547: train_loss -0.7717
2024-12-18 23:15:26.235386: val_loss -0.4732
2024-12-18 23:15:26.236162: Pseudo dice [0.7256]
2024-12-18 23:15:26.236898: Epoch time: 330.43 s
2024-12-18 23:15:27.652714: 
2024-12-18 23:15:27.654024: Epoch 57
2024-12-18 23:15:27.654859: Current learning rate: 0.0065
2024-12-18 23:21:02.422685: Validation loss did not improve from -0.52638. Patience: 25/50
2024-12-18 23:21:02.423728: train_loss -0.7697
2024-12-18 23:21:02.424879: val_loss -0.4633
2024-12-18 23:21:02.425853: Pseudo dice [0.7141]
2024-12-18 23:21:02.426858: Epoch time: 334.77 s
2024-12-18 23:21:03.816833: 
2024-12-18 23:21:03.818269: Epoch 58
2024-12-18 23:21:03.819383: Current learning rate: 0.00644
2024-12-18 23:26:32.971128: Validation loss did not improve from -0.52638. Patience: 26/50
2024-12-18 23:26:32.972265: train_loss -0.7758
2024-12-18 23:26:32.973125: val_loss -0.4756
2024-12-18 23:26:32.973945: Pseudo dice [0.7203]
2024-12-18 23:26:32.974859: Epoch time: 329.16 s
2024-12-18 23:26:34.413918: 
2024-12-18 23:26:34.415329: Epoch 59
2024-12-18 23:26:34.416244: Current learning rate: 0.00638
2024-12-18 23:31:47.133983: Validation loss did not improve from -0.52638. Patience: 27/50
2024-12-18 23:31:47.135375: train_loss -0.777
2024-12-18 23:31:47.137191: val_loss -0.4534
2024-12-18 23:31:47.138154: Pseudo dice [0.7111]
2024-12-18 23:31:47.139301: Epoch time: 312.72 s
2024-12-18 23:31:49.216342: 
2024-12-18 23:31:49.218147: Epoch 60
2024-12-18 23:31:49.219299: Current learning rate: 0.00631
2024-12-18 23:36:48.431612: Validation loss did not improve from -0.52638. Patience: 28/50
2024-12-18 23:36:48.433788: train_loss -0.7771
2024-12-18 23:36:48.434627: val_loss -0.4491
2024-12-18 23:36:48.435316: Pseudo dice [0.7177]
2024-12-18 23:36:48.436089: Epoch time: 299.22 s
2024-12-18 23:36:49.927711: 
2024-12-18 23:36:49.929170: Epoch 61
2024-12-18 23:36:49.929865: Current learning rate: 0.00625
2024-12-18 23:42:04.903891: Validation loss did not improve from -0.52638. Patience: 29/50
2024-12-18 23:42:04.906147: train_loss -0.7744
2024-12-18 23:42:04.907173: val_loss -0.4377
2024-12-18 23:42:04.908069: Pseudo dice [0.713]
2024-12-18 23:42:04.908843: Epoch time: 314.98 s
2024-12-18 23:42:07.438496: 
2024-12-18 23:42:07.439817: Epoch 62
2024-12-18 23:42:07.440670: Current learning rate: 0.00619
2024-12-18 23:47:07.518526: Validation loss did not improve from -0.52638. Patience: 30/50
2024-12-18 23:47:07.519604: train_loss -0.7807
2024-12-18 23:47:07.520652: val_loss -0.4598
2024-12-18 23:47:07.521692: Pseudo dice [0.7196]
2024-12-18 23:47:07.522730: Epoch time: 300.08 s
2024-12-18 23:47:08.993267: 
2024-12-18 23:47:08.994628: Epoch 63
2024-12-18 23:47:08.995366: Current learning rate: 0.00612
2024-12-18 23:52:49.768637: Validation loss did not improve from -0.52638. Patience: 31/50
2024-12-18 23:52:49.769607: train_loss -0.7785
2024-12-18 23:52:49.770388: val_loss -0.4966
2024-12-18 23:52:49.771094: Pseudo dice [0.7262]
2024-12-18 23:52:49.771885: Epoch time: 340.78 s
2024-12-18 23:52:51.448947: 
2024-12-18 23:52:51.450505: Epoch 64
2024-12-18 23:52:51.451330: Current learning rate: 0.00606
2024-12-18 23:58:29.745967: Validation loss did not improve from -0.52638. Patience: 32/50
2024-12-18 23:58:29.747008: train_loss -0.7807
2024-12-18 23:58:29.748329: val_loss -0.4751
2024-12-18 23:58:29.749411: Pseudo dice [0.7287]
2024-12-18 23:58:29.750266: Epoch time: 338.3 s
2024-12-18 23:58:31.609542: 
2024-12-18 23:58:31.610840: Epoch 65
2024-12-18 23:58:31.611749: Current learning rate: 0.006
2024-12-19 00:03:45.746340: Validation loss did not improve from -0.52638. Patience: 33/50
2024-12-19 00:03:45.747446: train_loss -0.778
2024-12-19 00:03:45.748365: val_loss -0.4384
2024-12-19 00:03:45.749075: Pseudo dice [0.7098]
2024-12-19 00:03:45.749870: Epoch time: 314.14 s
2024-12-19 00:03:47.193990: 
2024-12-19 00:03:47.195403: Epoch 66
2024-12-19 00:03:47.196236: Current learning rate: 0.00593
2024-12-19 00:09:51.171746: Validation loss did not improve from -0.52638. Patience: 34/50
2024-12-19 00:09:51.172755: train_loss -0.7755
2024-12-19 00:09:51.173605: val_loss -0.4701
2024-12-19 00:09:51.174462: Pseudo dice [0.7218]
2024-12-19 00:09:51.175267: Epoch time: 363.98 s
2024-12-19 00:09:52.649706: 
2024-12-19 00:09:52.651312: Epoch 67
2024-12-19 00:09:52.652217: Current learning rate: 0.00587
2024-12-19 00:15:26.154569: Validation loss did not improve from -0.52638. Patience: 35/50
2024-12-19 00:15:26.155613: train_loss -0.7824
2024-12-19 00:15:26.156571: val_loss -0.455
2024-12-19 00:15:26.157304: Pseudo dice [0.7219]
2024-12-19 00:15:26.157978: Epoch time: 333.51 s
2024-12-19 00:15:27.706404: 
2024-12-19 00:15:27.707887: Epoch 68
2024-12-19 00:15:27.708781: Current learning rate: 0.00581
2024-12-19 00:20:47.712461: Validation loss did not improve from -0.52638. Patience: 36/50
2024-12-19 00:20:47.713334: train_loss -0.7827
2024-12-19 00:20:47.714317: val_loss -0.489
2024-12-19 00:20:47.715155: Pseudo dice [0.7215]
2024-12-19 00:20:47.716047: Epoch time: 320.01 s
2024-12-19 00:20:49.160955: 
2024-12-19 00:20:49.162543: Epoch 69
2024-12-19 00:20:49.163715: Current learning rate: 0.00574
2024-12-19 00:26:13.785748: Validation loss did not improve from -0.52638. Patience: 37/50
2024-12-19 00:26:13.786661: train_loss -0.7838
2024-12-19 00:26:13.787825: val_loss -0.4619
2024-12-19 00:26:13.788716: Pseudo dice [0.7196]
2024-12-19 00:26:13.789716: Epoch time: 324.63 s
2024-12-19 00:26:15.714198: 
2024-12-19 00:26:15.715699: Epoch 70
2024-12-19 00:26:15.716826: Current learning rate: 0.00568
2024-12-19 00:31:53.804951: Validation loss did not improve from -0.52638. Patience: 38/50
2024-12-19 00:31:53.805970: train_loss -0.7868
2024-12-19 00:31:53.806977: val_loss -0.4562
2024-12-19 00:31:53.807880: Pseudo dice [0.7154]
2024-12-19 00:31:53.808789: Epoch time: 338.09 s
2024-12-19 00:31:55.332794: 
2024-12-19 00:31:55.334132: Epoch 71
2024-12-19 00:31:55.335077: Current learning rate: 0.00562
2024-12-19 00:37:47.256684: Validation loss did not improve from -0.52638. Patience: 39/50
2024-12-19 00:37:47.259387: train_loss -0.7872
2024-12-19 00:37:47.261281: val_loss -0.4642
2024-12-19 00:37:47.262195: Pseudo dice [0.7161]
2024-12-19 00:37:47.263403: Epoch time: 351.93 s
2024-12-19 00:37:48.865577: 
2024-12-19 00:37:48.866885: Epoch 72
2024-12-19 00:37:48.867920: Current learning rate: 0.00555
2024-12-19 00:43:19.934168: Validation loss did not improve from -0.52638. Patience: 40/50
2024-12-19 00:43:19.935286: train_loss -0.7824
2024-12-19 00:43:19.936054: val_loss -0.4673
2024-12-19 00:43:19.936763: Pseudo dice [0.7225]
2024-12-19 00:43:19.937408: Epoch time: 331.07 s
2024-12-19 00:43:21.915013: 
2024-12-19 00:43:21.916316: Epoch 73
2024-12-19 00:43:21.917064: Current learning rate: 0.00549
2024-12-19 00:48:45.658079: Validation loss did not improve from -0.52638. Patience: 41/50
2024-12-19 00:48:45.660359: train_loss -0.7892
2024-12-19 00:48:45.661768: val_loss -0.4142
2024-12-19 00:48:45.662533: Pseudo dice [0.6957]
2024-12-19 00:48:45.663351: Epoch time: 323.75 s
2024-12-19 00:48:47.175664: 
2024-12-19 00:48:47.176958: Epoch 74
2024-12-19 00:48:47.177809: Current learning rate: 0.00542
2024-12-19 00:54:13.012296: Validation loss did not improve from -0.52638. Patience: 42/50
2024-12-19 00:54:13.013414: train_loss -0.7872
2024-12-19 00:54:13.014182: val_loss -0.4995
2024-12-19 00:54:13.014806: Pseudo dice [0.7327]
2024-12-19 00:54:13.015595: Epoch time: 325.84 s
2024-12-19 00:54:14.987019: 
2024-12-19 00:54:14.988421: Epoch 75
2024-12-19 00:54:14.989223: Current learning rate: 0.00536
2024-12-19 00:59:43.172267: Validation loss did not improve from -0.52638. Patience: 43/50
2024-12-19 00:59:43.173294: train_loss -0.7889
2024-12-19 00:59:43.174169: val_loss -0.457
2024-12-19 00:59:43.174821: Pseudo dice [0.7106]
2024-12-19 00:59:43.175508: Epoch time: 328.19 s
2024-12-19 00:59:44.667766: 
2024-12-19 00:59:44.668928: Epoch 76
2024-12-19 00:59:44.669749: Current learning rate: 0.00529
2024-12-19 01:05:32.725116: Validation loss did not improve from -0.52638. Patience: 44/50
2024-12-19 01:05:32.726339: train_loss -0.7864
2024-12-19 01:05:32.727526: val_loss -0.5025
2024-12-19 01:05:32.728488: Pseudo dice [0.7389]
2024-12-19 01:05:32.729403: Epoch time: 348.06 s
2024-12-19 01:05:34.190974: 
2024-12-19 01:05:34.192443: Epoch 77
2024-12-19 01:05:34.193481: Current learning rate: 0.00523
2024-12-19 01:11:13.996601: Validation loss did not improve from -0.52638. Patience: 45/50
2024-12-19 01:11:13.997549: train_loss -0.7871
2024-12-19 01:11:13.998353: val_loss -0.4599
2024-12-19 01:11:13.999119: Pseudo dice [0.7263]
2024-12-19 01:11:14.000022: Epoch time: 339.81 s
2024-12-19 01:11:15.506497: 
2024-12-19 01:11:15.508115: Epoch 78
2024-12-19 01:11:15.509268: Current learning rate: 0.00517
2024-12-19 01:16:27.350466: Validation loss did not improve from -0.52638. Patience: 46/50
2024-12-19 01:16:27.351497: train_loss -0.7908
2024-12-19 01:16:27.352397: val_loss -0.4717
2024-12-19 01:16:27.353221: Pseudo dice [0.7297]
2024-12-19 01:16:27.354055: Epoch time: 311.85 s
2024-12-19 01:16:28.983819: 
2024-12-19 01:16:28.985408: Epoch 79
2024-12-19 01:16:28.986382: Current learning rate: 0.0051
2024-12-19 01:21:34.638853: Validation loss did not improve from -0.52638. Patience: 47/50
2024-12-19 01:21:34.640007: train_loss -0.7898
2024-12-19 01:21:34.640914: val_loss -0.492
2024-12-19 01:21:34.641687: Pseudo dice [0.7333]
2024-12-19 01:21:34.642382: Epoch time: 305.66 s
2024-12-19 01:21:36.576933: 
2024-12-19 01:21:36.578313: Epoch 80
2024-12-19 01:21:36.579123: Current learning rate: 0.00504
2024-12-19 01:26:50.419998: Validation loss did not improve from -0.52638. Patience: 48/50
2024-12-19 01:26:50.421097: train_loss -0.7931
2024-12-19 01:26:50.421863: val_loss -0.452
2024-12-19 01:26:50.422584: Pseudo dice [0.7099]
2024-12-19 01:26:50.423255: Epoch time: 313.85 s
2024-12-19 01:26:51.916543: 
2024-12-19 01:26:51.917605: Epoch 81
2024-12-19 01:26:51.918338: Current learning rate: 0.00497
2024-12-19 01:32:57.270066: Validation loss improved from -0.52638 to -0.53168! Patience: 48/50
2024-12-19 01:32:57.271240: train_loss -0.7943
2024-12-19 01:32:57.272800: val_loss -0.5317
2024-12-19 01:32:57.273595: Pseudo dice [0.7564]
2024-12-19 01:32:57.274436: Epoch time: 365.36 s
2024-12-19 01:32:58.834344: 
2024-12-19 01:32:58.835610: Epoch 82
2024-12-19 01:32:58.836287: Current learning rate: 0.00491
2024-12-19 01:38:36.390039: Validation loss did not improve from -0.53168. Patience: 1/50
2024-12-19 01:38:36.390885: train_loss -0.7938
2024-12-19 01:38:36.391622: val_loss -0.4492
2024-12-19 01:38:36.392300: Pseudo dice [0.7236]
2024-12-19 01:38:36.393013: Epoch time: 337.56 s
2024-12-19 01:38:37.794473: 
2024-12-19 01:38:37.795766: Epoch 83
2024-12-19 01:38:37.796466: Current learning rate: 0.00484
2024-12-19 01:42:28.174479: Validation loss did not improve from -0.53168. Patience: 2/50
2024-12-19 01:42:28.175616: train_loss -0.7996
2024-12-19 01:42:28.176372: val_loss -0.4391
2024-12-19 01:42:28.177105: Pseudo dice [0.7093]
2024-12-19 01:42:28.177881: Epoch time: 230.38 s
2024-12-19 01:42:30.156757: 
2024-12-19 01:42:30.158076: Epoch 84
2024-12-19 01:42:30.158987: Current learning rate: 0.00478
2024-12-19 01:48:13.624002: Validation loss did not improve from -0.53168. Patience: 3/50
2024-12-19 01:48:13.626043: train_loss -0.7964
2024-12-19 01:48:13.627118: val_loss -0.4869
2024-12-19 01:48:13.627974: Pseudo dice [0.7326]
2024-12-19 01:48:13.628932: Epoch time: 343.47 s
2024-12-19 01:48:15.779415: 
2024-12-19 01:48:15.780820: Epoch 85
2024-12-19 01:48:15.781693: Current learning rate: 0.00471
2024-12-19 01:53:35.594683: Validation loss did not improve from -0.53168. Patience: 4/50
2024-12-19 01:53:35.597016: train_loss -0.7985
2024-12-19 01:53:35.598361: val_loss -0.4798
2024-12-19 01:53:35.599119: Pseudo dice [0.726]
2024-12-19 01:53:35.599909: Epoch time: 319.82 s
2024-12-19 01:53:37.056343: 
2024-12-19 01:53:37.057711: Epoch 86
2024-12-19 01:53:37.058408: Current learning rate: 0.00465
2024-12-19 01:59:21.378865: Validation loss did not improve from -0.53168. Patience: 5/50
2024-12-19 01:59:21.380014: train_loss -0.8006
2024-12-19 01:59:21.380883: val_loss -0.4709
2024-12-19 01:59:21.381714: Pseudo dice [0.7182]
2024-12-19 01:59:21.382671: Epoch time: 344.32 s
2024-12-19 01:59:22.841535: 
2024-12-19 01:59:22.842998: Epoch 87
2024-12-19 01:59:22.843848: Current learning rate: 0.00458
2024-12-19 02:04:53.197887: Validation loss did not improve from -0.53168. Patience: 6/50
2024-12-19 02:04:53.199035: train_loss -0.7986
2024-12-19 02:04:53.199800: val_loss -0.4636
2024-12-19 02:04:53.200448: Pseudo dice [0.7276]
2024-12-19 02:04:53.201140: Epoch time: 330.36 s
2024-12-19 02:04:54.618275: 
2024-12-19 02:04:54.619578: Epoch 88
2024-12-19 02:04:54.620426: Current learning rate: 0.00452
2024-12-19 02:09:43.643532: Validation loss did not improve from -0.53168. Patience: 7/50
2024-12-19 02:09:43.644684: train_loss -0.7982
2024-12-19 02:09:43.645647: val_loss -0.4564
2024-12-19 02:09:43.646555: Pseudo dice [0.718]
2024-12-19 02:09:43.647344: Epoch time: 289.03 s
2024-12-19 02:09:45.077658: 
2024-12-19 02:09:45.079007: Epoch 89
2024-12-19 02:09:45.079777: Current learning rate: 0.00445
2024-12-19 02:15:31.121403: Validation loss did not improve from -0.53168. Patience: 8/50
2024-12-19 02:15:31.122383: train_loss -0.7977
2024-12-19 02:15:31.123436: val_loss -0.4855
2024-12-19 02:15:31.124469: Pseudo dice [0.7347]
2024-12-19 02:15:31.125318: Epoch time: 346.05 s
2024-12-19 02:15:33.104834: 
2024-12-19 02:15:33.106256: Epoch 90
2024-12-19 02:15:33.107288: Current learning rate: 0.00438
2024-12-19 02:20:55.721622: Validation loss did not improve from -0.53168. Patience: 9/50
2024-12-19 02:20:55.722475: train_loss -0.7977
2024-12-19 02:20:55.723281: val_loss -0.4551
2024-12-19 02:20:55.724029: Pseudo dice [0.7175]
2024-12-19 02:20:55.724926: Epoch time: 322.62 s
2024-12-19 02:20:57.112567: 
2024-12-19 02:20:57.114083: Epoch 91
2024-12-19 02:20:57.115006: Current learning rate: 0.00432
2024-12-19 02:26:34.184779: Validation loss did not improve from -0.53168. Patience: 10/50
2024-12-19 02:26:34.185771: train_loss -0.8
2024-12-19 02:26:34.186774: val_loss -0.4697
2024-12-19 02:26:34.187777: Pseudo dice [0.726]
2024-12-19 02:26:34.188722: Epoch time: 337.07 s
2024-12-19 02:26:35.641736: 
2024-12-19 02:26:35.643256: Epoch 92
2024-12-19 02:26:35.644354: Current learning rate: 0.00425
2024-12-19 02:32:29.133117: Validation loss did not improve from -0.53168. Patience: 11/50
2024-12-19 02:32:29.133891: train_loss -0.8001
2024-12-19 02:32:29.134645: val_loss -0.4476
2024-12-19 02:32:29.135402: Pseudo dice [0.7142]
2024-12-19 02:32:29.136121: Epoch time: 353.49 s
2024-12-19 02:32:30.545766: 
2024-12-19 02:32:30.546720: Epoch 93
2024-12-19 02:32:30.547487: Current learning rate: 0.00419
2024-12-19 02:38:03.270068: Validation loss did not improve from -0.53168. Patience: 12/50
2024-12-19 02:38:03.271144: train_loss -0.8013
2024-12-19 02:38:03.273137: val_loss -0.4357
2024-12-19 02:38:03.273989: Pseudo dice [0.7074]
2024-12-19 02:38:03.274847: Epoch time: 332.73 s
2024-12-19 02:38:04.631617: 
2024-12-19 02:38:04.633034: Epoch 94
2024-12-19 02:38:04.633731: Current learning rate: 0.00412
2024-12-19 02:43:11.371340: Validation loss did not improve from -0.53168. Patience: 13/50
2024-12-19 02:43:11.373115: train_loss -0.8014
2024-12-19 02:43:11.374253: val_loss -0.4162
2024-12-19 02:43:11.375002: Pseudo dice [0.7109]
2024-12-19 02:43:11.375953: Epoch time: 306.74 s
2024-12-19 02:43:13.250672: 
2024-12-19 02:43:13.252031: Epoch 95
2024-12-19 02:43:13.253077: Current learning rate: 0.00405
2024-12-19 02:48:28.047311: Validation loss did not improve from -0.53168. Patience: 14/50
2024-12-19 02:48:28.050902: train_loss -0.8037
2024-12-19 02:48:28.052127: val_loss -0.481
2024-12-19 02:48:28.053098: Pseudo dice [0.7298]
2024-12-19 02:48:28.054334: Epoch time: 314.8 s
2024-12-19 02:48:30.397996: 
2024-12-19 02:48:30.399924: Epoch 96
2024-12-19 02:48:30.400990: Current learning rate: 0.00399
2024-12-19 02:53:36.525808: Validation loss did not improve from -0.53168. Patience: 15/50
2024-12-19 02:53:36.527285: train_loss -0.8015
2024-12-19 02:53:36.528707: val_loss -0.4366
2024-12-19 02:53:36.529516: Pseudo dice [0.7051]
2024-12-19 02:53:36.530253: Epoch time: 306.13 s
2024-12-19 02:53:37.962588: 
2024-12-19 02:53:37.963771: Epoch 97
2024-12-19 02:53:37.964598: Current learning rate: 0.00392
2024-12-19 02:58:55.946518: Validation loss did not improve from -0.53168. Patience: 16/50
2024-12-19 02:58:55.949537: train_loss -0.8054
2024-12-19 02:58:55.950736: val_loss -0.4407
2024-12-19 02:58:55.951715: Pseudo dice [0.7242]
2024-12-19 02:58:55.952745: Epoch time: 317.99 s
2024-12-19 02:58:57.400822: 
2024-12-19 02:58:57.402096: Epoch 98
2024-12-19 02:58:57.402963: Current learning rate: 0.00385
2024-12-19 03:03:54.535894: Validation loss did not improve from -0.53168. Patience: 17/50
2024-12-19 03:03:54.536937: train_loss -0.8052
2024-12-19 03:03:54.537792: val_loss -0.4577
2024-12-19 03:03:54.538592: Pseudo dice [0.7277]
2024-12-19 03:03:54.539441: Epoch time: 297.14 s
2024-12-19 03:03:56.000649: 
2024-12-19 03:03:56.002010: Epoch 99
2024-12-19 03:03:56.002950: Current learning rate: 0.00379
2024-12-19 03:09:18.697276: Validation loss did not improve from -0.53168. Patience: 18/50
2024-12-19 03:09:18.698414: train_loss -0.801
2024-12-19 03:09:18.699453: val_loss -0.4761
2024-12-19 03:09:18.700491: Pseudo dice [0.723]
2024-12-19 03:09:18.701479: Epoch time: 322.7 s
2024-12-19 03:09:20.605179: 
2024-12-19 03:09:20.606994: Epoch 100
2024-12-19 03:09:20.608122: Current learning rate: 0.00372
2024-12-19 03:14:33.194895: Validation loss did not improve from -0.53168. Patience: 19/50
2024-12-19 03:14:33.195901: train_loss -0.8058
2024-12-19 03:14:33.196686: val_loss -0.4629
2024-12-19 03:14:33.197534: Pseudo dice [0.7368]
2024-12-19 03:14:33.198331: Epoch time: 312.59 s
2024-12-19 03:14:34.687837: 
2024-12-19 03:14:34.689124: Epoch 101
2024-12-19 03:14:34.689862: Current learning rate: 0.00365
2024-12-19 03:20:03.842215: Validation loss did not improve from -0.53168. Patience: 20/50
2024-12-19 03:20:03.843245: train_loss -0.8057
2024-12-19 03:20:03.844207: val_loss -0.4549
2024-12-19 03:20:03.845169: Pseudo dice [0.7178]
2024-12-19 03:20:03.846110: Epoch time: 329.16 s
2024-12-19 03:20:05.319485: 
2024-12-19 03:20:05.320867: Epoch 102
2024-12-19 03:20:05.321921: Current learning rate: 0.00359
2024-12-19 03:25:17.331047: Validation loss did not improve from -0.53168. Patience: 21/50
2024-12-19 03:25:17.332113: train_loss -0.8039
2024-12-19 03:25:17.333058: val_loss -0.4691
2024-12-19 03:25:17.334028: Pseudo dice [0.7254]
2024-12-19 03:25:17.335078: Epoch time: 312.01 s
2024-12-19 03:25:18.772626: 
2024-12-19 03:25:18.774097: Epoch 103
2024-12-19 03:25:18.775339: Current learning rate: 0.00352
2024-12-19 03:30:29.996203: Validation loss did not improve from -0.53168. Patience: 22/50
2024-12-19 03:30:29.997188: train_loss -0.8076
2024-12-19 03:30:29.998136: val_loss -0.4663
2024-12-19 03:30:29.998992: Pseudo dice [0.7245]
2024-12-19 03:30:29.999953: Epoch time: 311.23 s
2024-12-19 03:30:31.463406: 
2024-12-19 03:30:31.465217: Epoch 104
2024-12-19 03:30:31.466072: Current learning rate: 0.00345
2024-12-19 03:36:04.749555: Validation loss did not improve from -0.53168. Patience: 23/50
2024-12-19 03:36:04.750456: train_loss -0.8099
2024-12-19 03:36:04.751493: val_loss -0.4526
2024-12-19 03:36:04.752471: Pseudo dice [0.7151]
2024-12-19 03:36:04.753526: Epoch time: 333.29 s
2024-12-19 03:36:06.658478: 
2024-12-19 03:36:06.660134: Epoch 105
2024-12-19 03:36:06.661315: Current learning rate: 0.00338
2024-12-19 03:41:26.031029: Validation loss did not improve from -0.53168. Patience: 24/50
2024-12-19 03:41:26.031951: train_loss -0.8109
2024-12-19 03:41:26.032722: val_loss -0.467
2024-12-19 03:41:26.033507: Pseudo dice [0.7247]
2024-12-19 03:41:26.034294: Epoch time: 319.37 s
2024-12-19 03:41:27.483194: 
2024-12-19 03:41:27.484566: Epoch 106
2024-12-19 03:41:27.485384: Current learning rate: 0.00332
2024-12-19 03:46:45.288038: Validation loss did not improve from -0.53168. Patience: 25/50
2024-12-19 03:46:45.289026: train_loss -0.8088
2024-12-19 03:46:45.289967: val_loss -0.4166
2024-12-19 03:46:45.290949: Pseudo dice [0.7037]
2024-12-19 03:46:45.291849: Epoch time: 317.81 s
2024-12-19 03:46:47.093433: 
2024-12-19 03:46:47.094975: Epoch 107
2024-12-19 03:46:47.095889: Current learning rate: 0.00325
2024-12-19 03:52:08.715087: Validation loss did not improve from -0.53168. Patience: 26/50
2024-12-19 03:52:08.716795: train_loss -0.8108
2024-12-19 03:52:08.717758: val_loss -0.4976
2024-12-19 03:52:08.718725: Pseudo dice [0.7387]
2024-12-19 03:52:08.719851: Epoch time: 321.62 s
2024-12-19 03:52:10.195995: 
2024-12-19 03:52:10.197427: Epoch 108
2024-12-19 03:52:10.198400: Current learning rate: 0.00318
2024-12-19 03:57:31.286165: Validation loss did not improve from -0.53168. Patience: 27/50
2024-12-19 03:57:31.288390: train_loss -0.8075
2024-12-19 03:57:31.289279: val_loss -0.491
2024-12-19 03:57:31.290026: Pseudo dice [0.7321]
2024-12-19 03:57:31.290805: Epoch time: 321.09 s
2024-12-19 03:57:32.765341: 
2024-12-19 03:57:32.766805: Epoch 109
2024-12-19 03:57:32.767632: Current learning rate: 0.00311
2024-12-19 04:03:08.423189: Validation loss did not improve from -0.53168. Patience: 28/50
2024-12-19 04:03:08.426052: train_loss -0.8133
2024-12-19 04:03:08.427393: val_loss -0.4499
2024-12-19 04:03:08.428358: Pseudo dice [0.7231]
2024-12-19 04:03:08.429207: Epoch time: 335.66 s
2024-12-19 04:03:10.562392: 
2024-12-19 04:03:10.563819: Epoch 110
2024-12-19 04:03:10.564687: Current learning rate: 0.00304
2024-12-19 04:08:24.820120: Validation loss did not improve from -0.53168. Patience: 29/50
2024-12-19 04:08:24.821133: train_loss -0.8146
2024-12-19 04:08:24.822061: val_loss -0.4437
2024-12-19 04:08:24.822982: Pseudo dice [0.7188]
2024-12-19 04:08:24.823759: Epoch time: 314.26 s
2024-12-19 04:08:26.306306: 
2024-12-19 04:08:26.307693: Epoch 111
2024-12-19 04:08:26.308542: Current learning rate: 0.00297
2024-12-19 04:13:33.956712: Validation loss did not improve from -0.53168. Patience: 30/50
2024-12-19 04:13:33.957854: train_loss -0.8114
2024-12-19 04:13:33.958626: val_loss -0.3967
2024-12-19 04:13:33.959318: Pseudo dice [0.6939]
2024-12-19 04:13:33.960098: Epoch time: 307.65 s
2024-12-19 04:13:35.404053: 
2024-12-19 04:13:35.405388: Epoch 112
2024-12-19 04:13:35.406178: Current learning rate: 0.00291
2024-12-19 04:19:01.909270: Validation loss did not improve from -0.53168. Patience: 31/50
2024-12-19 04:19:01.910165: train_loss -0.8096
2024-12-19 04:19:01.910993: val_loss -0.4398
2024-12-19 04:19:01.911802: Pseudo dice [0.7156]
2024-12-19 04:19:01.912578: Epoch time: 326.51 s
2024-12-19 04:19:03.348748: 
2024-12-19 04:19:03.350019: Epoch 113
2024-12-19 04:19:03.350759: Current learning rate: 0.00284
2024-12-19 04:24:17.320704: Validation loss did not improve from -0.53168. Patience: 32/50
2024-12-19 04:24:17.321656: train_loss -0.814
2024-12-19 04:24:17.322530: val_loss -0.4683
2024-12-19 04:24:17.323287: Pseudo dice [0.7286]
2024-12-19 04:24:17.324070: Epoch time: 313.97 s
2024-12-19 04:24:18.746017: 
2024-12-19 04:24:18.748332: Epoch 114
2024-12-19 04:24:18.749518: Current learning rate: 0.00277
2024-12-19 04:29:39.060571: Validation loss did not improve from -0.53168. Patience: 33/50
2024-12-19 04:29:39.061671: train_loss -0.8137
2024-12-19 04:29:39.062596: val_loss -0.4406
2024-12-19 04:29:39.063485: Pseudo dice [0.7119]
2024-12-19 04:29:39.064258: Epoch time: 320.32 s
2024-12-19 04:29:40.916831: 
2024-12-19 04:29:40.918076: Epoch 115
2024-12-19 04:29:40.918813: Current learning rate: 0.0027
2024-12-19 04:34:40.051292: Validation loss did not improve from -0.53168. Patience: 34/50
2024-12-19 04:34:40.052322: train_loss -0.8138
2024-12-19 04:34:40.053243: val_loss -0.4888
2024-12-19 04:34:40.054013: Pseudo dice [0.7395]
2024-12-19 04:34:40.054788: Epoch time: 299.14 s
2024-12-19 04:34:41.641939: 
2024-12-19 04:34:41.643603: Epoch 116
2024-12-19 04:34:41.644531: Current learning rate: 0.00263
2024-12-19 04:38:34.534426: Validation loss did not improve from -0.53168. Patience: 35/50
2024-12-19 04:38:34.535452: train_loss -0.8146
2024-12-19 04:38:34.536356: val_loss -0.4671
2024-12-19 04:38:34.537004: Pseudo dice [0.7358]
2024-12-19 04:38:34.537749: Epoch time: 232.89 s
2024-12-19 04:38:36.020765: 
2024-12-19 04:38:36.022001: Epoch 117
2024-12-19 04:38:36.022746: Current learning rate: 0.00256
2024-12-19 04:43:07.459234: Validation loss did not improve from -0.53168. Patience: 36/50
2024-12-19 04:43:07.460410: train_loss -0.8136
2024-12-19 04:43:07.461187: val_loss -0.4686
2024-12-19 04:43:07.461821: Pseudo dice [0.7206]
2024-12-19 04:43:07.462478: Epoch time: 271.44 s
2024-12-19 04:43:09.379079: 
2024-12-19 04:43:09.380325: Epoch 118
2024-12-19 04:43:09.381081: Current learning rate: 0.00249
2024-12-19 04:47:11.690647: Validation loss did not improve from -0.53168. Patience: 37/50
2024-12-19 04:47:11.691567: train_loss -0.815
2024-12-19 04:47:11.692486: val_loss -0.4219
2024-12-19 04:47:11.693300: Pseudo dice [0.7098]
2024-12-19 04:47:11.694069: Epoch time: 242.31 s
2024-12-19 04:47:13.155228: 
2024-12-19 04:47:13.156630: Epoch 119
2024-12-19 04:47:13.157357: Current learning rate: 0.00242
2024-12-19 04:51:06.727793: Validation loss did not improve from -0.53168. Patience: 38/50
2024-12-19 04:51:06.728618: train_loss -0.8191
2024-12-19 04:51:06.729727: val_loss -0.4419
2024-12-19 04:51:06.730666: Pseudo dice [0.7308]
2024-12-19 04:51:06.731520: Epoch time: 233.57 s
2024-12-19 04:51:08.684047: 
2024-12-19 04:51:08.686447: Epoch 120
2024-12-19 04:51:08.687855: Current learning rate: 0.00235
2024-12-19 04:54:59.186281: Validation loss did not improve from -0.53168. Patience: 39/50
2024-12-19 04:54:59.187315: train_loss -0.8177
2024-12-19 04:54:59.188336: val_loss -0.4663
2024-12-19 04:54:59.189191: Pseudo dice [0.7251]
2024-12-19 04:54:59.190084: Epoch time: 230.51 s
2024-12-19 04:55:00.640980: 
2024-12-19 04:55:00.642474: Epoch 121
2024-12-19 04:55:00.643765: Current learning rate: 0.00228
2024-12-19 04:59:12.666286: Validation loss did not improve from -0.53168. Patience: 40/50
2024-12-19 04:59:12.669697: train_loss -0.8179
2024-12-19 04:59:12.670880: val_loss -0.4412
2024-12-19 04:59:12.671665: Pseudo dice [0.7184]
2024-12-19 04:59:12.672755: Epoch time: 252.03 s
2024-12-19 04:59:14.214982: 
2024-12-19 04:59:14.216456: Epoch 122
2024-12-19 04:59:14.217243: Current learning rate: 0.00221
2024-12-19 05:03:17.251250: Validation loss did not improve from -0.53168. Patience: 41/50
2024-12-19 05:03:17.252964: train_loss -0.8179
2024-12-19 05:03:17.254238: val_loss -0.4217
2024-12-19 05:03:17.255028: Pseudo dice [0.706]
2024-12-19 05:03:17.255784: Epoch time: 243.04 s
2024-12-19 05:03:18.844356: 
2024-12-19 05:03:18.845962: Epoch 123
2024-12-19 05:03:18.846861: Current learning rate: 0.00214
2024-12-19 05:06:55.518163: Validation loss did not improve from -0.53168. Patience: 42/50
2024-12-19 05:06:55.519731: train_loss -0.8189
2024-12-19 05:06:55.520921: val_loss -0.4577
2024-12-19 05:06:55.521693: Pseudo dice [0.7224]
2024-12-19 05:06:55.522551: Epoch time: 216.68 s
2024-12-19 05:06:57.108230: 
2024-12-19 05:06:57.109617: Epoch 124
2024-12-19 05:06:57.110373: Current learning rate: 0.00207
2024-12-19 05:10:46.092411: Validation loss did not improve from -0.53168. Patience: 43/50
2024-12-19 05:10:46.093457: train_loss -0.8188
2024-12-19 05:10:46.094298: val_loss -0.4684
2024-12-19 05:10:46.095084: Pseudo dice [0.7298]
2024-12-19 05:10:46.095766: Epoch time: 228.99 s
2024-12-19 05:10:48.055428: 
2024-12-19 05:10:48.056869: Epoch 125
2024-12-19 05:10:48.057790: Current learning rate: 0.00199
2024-12-19 05:14:40.533824: Validation loss did not improve from -0.53168. Patience: 44/50
2024-12-19 05:14:40.534777: train_loss -0.8202
2024-12-19 05:14:40.535683: val_loss -0.4147
2024-12-19 05:14:40.536534: Pseudo dice [0.7099]
2024-12-19 05:14:40.537358: Epoch time: 232.48 s
2024-12-19 05:14:42.039657: 
2024-12-19 05:14:42.041284: Epoch 126
2024-12-19 05:14:42.042209: Current learning rate: 0.00192
2024-12-19 05:18:40.598950: Validation loss did not improve from -0.53168. Patience: 45/50
2024-12-19 05:18:40.599929: train_loss -0.8193
2024-12-19 05:18:40.600815: val_loss -0.4992
2024-12-19 05:18:40.601641: Pseudo dice [0.7445]
2024-12-19 05:18:40.602449: Epoch time: 238.56 s
2024-12-19 05:18:42.107720: 
2024-12-19 05:18:42.109243: Epoch 127
2024-12-19 05:18:42.110107: Current learning rate: 0.00185
2024-12-19 05:22:06.413576: Validation loss did not improve from -0.53168. Patience: 46/50
2024-12-19 05:22:06.414487: train_loss -0.8199
2024-12-19 05:22:06.415404: val_loss -0.4589
2024-12-19 05:22:06.416170: Pseudo dice [0.7297]
2024-12-19 05:22:06.416942: Epoch time: 204.31 s
2024-12-19 05:22:07.962466: 
2024-12-19 05:22:07.964023: Epoch 128
2024-12-19 05:22:07.965088: Current learning rate: 0.00178
2024-12-19 05:26:13.124250: Validation loss did not improve from -0.53168. Patience: 47/50
2024-12-19 05:26:13.124921: train_loss -0.8185
2024-12-19 05:26:13.125768: val_loss -0.4441
2024-12-19 05:26:13.126661: Pseudo dice [0.7154]
2024-12-19 05:26:13.127501: Epoch time: 245.16 s
2024-12-19 05:26:15.064579: 
2024-12-19 05:26:15.065737: Epoch 129
2024-12-19 05:26:15.066743: Current learning rate: 0.0017
2024-12-19 05:29:30.716110: Validation loss did not improve from -0.53168. Patience: 48/50
2024-12-19 05:29:30.717175: train_loss -0.8195
2024-12-19 05:29:30.717966: val_loss -0.4785
2024-12-19 05:29:30.718653: Pseudo dice [0.7284]
2024-12-19 05:29:30.719313: Epoch time: 195.65 s
2024-12-19 05:29:32.570864: 
2024-12-19 05:29:32.572171: Epoch 130
2024-12-19 05:29:32.572940: Current learning rate: 0.00163
2024-12-19 05:33:01.206026: Validation loss did not improve from -0.53168. Patience: 49/50
2024-12-19 05:33:01.207020: train_loss -0.8198
2024-12-19 05:33:01.207992: val_loss -0.4605
2024-12-19 05:33:01.208950: Pseudo dice [0.7226]
2024-12-19 05:33:01.209713: Epoch time: 208.64 s
2024-12-19 05:33:02.668342: 
2024-12-19 05:33:02.669818: Epoch 131
2024-12-19 05:33:02.670861: Current learning rate: 0.00156
2024-12-19 05:36:43.882835: Validation loss did not improve from -0.53168. Patience: 50/50
2024-12-19 05:36:43.883894: train_loss -0.8207
2024-12-19 05:36:43.884786: val_loss -0.4258
2024-12-19 05:36:43.885741: Pseudo dice [0.7085]
2024-12-19 05:36:43.886677: Epoch time: 221.22 s
2024-12-19 05:36:45.338887: 
2024-12-19 05:36:45.340426: Epoch 132
2024-12-19 05:36:45.341474: Current learning rate: 0.00148
2024-12-19 05:40:10.331935: Validation loss did not improve from -0.53168. Patience: 51/50
2024-12-19 05:40:10.333082: train_loss -0.8222
2024-12-19 05:40:10.333902: val_loss -0.4454
2024-12-19 05:40:10.334630: Pseudo dice [0.7153]
2024-12-19 05:40:10.335290: Epoch time: 205.0 s
2024-12-19 05:40:11.776241: 
2024-12-19 05:40:11.777455: Epoch 133
2024-12-19 05:40:11.778230: Current learning rate: 0.00141
2024-12-19 05:43:46.576895: Validation loss did not improve from -0.53168. Patience: 52/50
2024-12-19 05:43:46.577866: train_loss -0.8207
2024-12-19 05:43:46.578737: val_loss -0.4332
2024-12-19 05:43:46.579608: Pseudo dice [0.7098]
2024-12-19 05:43:46.580563: Epoch time: 214.8 s
2024-12-19 05:43:48.121616: 
2024-12-19 05:43:48.123346: Epoch 134
2024-12-19 05:43:48.124222: Current learning rate: 0.00133
2024-12-19 05:47:20.385913: Validation loss did not improve from -0.53168. Patience: 53/50
2024-12-19 05:47:20.386935: train_loss -0.8227
2024-12-19 05:47:20.387916: val_loss -0.4464
2024-12-19 05:47:20.388799: Pseudo dice [0.7268]
2024-12-19 05:47:20.389640: Epoch time: 212.27 s
2024-12-19 05:47:22.306574: 
2024-12-19 05:47:22.307895: Epoch 135
2024-12-19 05:47:22.308775: Current learning rate: 0.00126
2024-12-19 05:50:40.700269: Validation loss did not improve from -0.53168. Patience: 54/50
2024-12-19 05:50:40.701395: train_loss -0.8226
2024-12-19 05:50:40.702865: val_loss -0.4669
2024-12-19 05:50:40.703782: Pseudo dice [0.7326]
2024-12-19 05:50:40.704639: Epoch time: 198.4 s
2024-12-19 05:50:42.199205: 
2024-12-19 05:50:42.200571: Epoch 136
2024-12-19 05:50:42.201435: Current learning rate: 0.00118
2024-12-19 05:53:57.296647: Validation loss did not improve from -0.53168. Patience: 55/50
2024-12-19 05:53:57.297539: train_loss -0.8237
2024-12-19 05:53:57.298431: val_loss -0.466
2024-12-19 05:53:57.299219: Pseudo dice [0.7278]
2024-12-19 05:53:57.299905: Epoch time: 195.1 s
2024-12-19 05:53:58.796895: 
2024-12-19 05:53:58.798044: Epoch 137
2024-12-19 05:53:58.798867: Current learning rate: 0.00111
2024-12-19 05:57:37.138974: Validation loss did not improve from -0.53168. Patience: 56/50
2024-12-19 05:57:37.139972: train_loss -0.825
2024-12-19 05:57:37.140796: val_loss -0.4657
2024-12-19 05:57:37.141579: Pseudo dice [0.7258]
2024-12-19 05:57:37.142393: Epoch time: 218.34 s
2024-12-19 05:57:38.644981: 
2024-12-19 05:57:38.646499: Epoch 138
2024-12-19 05:57:38.647455: Current learning rate: 0.00103
2024-12-19 06:00:56.084456: Validation loss did not improve from -0.53168. Patience: 57/50
2024-12-19 06:00:56.085773: train_loss -0.8209
2024-12-19 06:00:56.086509: val_loss -0.4614
2024-12-19 06:00:56.087168: Pseudo dice [0.7289]
2024-12-19 06:00:56.087952: Epoch time: 197.44 s
2024-12-19 06:00:57.587653: 
2024-12-19 06:00:57.589292: Epoch 139
2024-12-19 06:00:57.590069: Current learning rate: 0.00095
2024-12-19 06:04:27.985640: Validation loss did not improve from -0.53168. Patience: 58/50
2024-12-19 06:04:27.989102: train_loss -0.8226
2024-12-19 06:04:27.990363: val_loss -0.4329
2024-12-19 06:04:27.991237: Pseudo dice [0.7114]
2024-12-19 06:04:27.992103: Epoch time: 210.4 s
2024-12-19 06:04:30.634440: 
2024-12-19 06:04:30.635791: Epoch 140
2024-12-19 06:04:30.636571: Current learning rate: 0.00087
2024-12-19 06:07:40.176923: Validation loss did not improve from -0.53168. Patience: 59/50
2024-12-19 06:07:40.178188: train_loss -0.8244
2024-12-19 06:07:40.179044: val_loss -0.4217
2024-12-19 06:07:40.179797: Pseudo dice [0.7108]
2024-12-19 06:07:40.180515: Epoch time: 189.54 s
2024-12-19 06:07:41.694033: 
2024-12-19 06:07:41.695363: Epoch 141
2024-12-19 06:07:41.696242: Current learning rate: 0.00079
2024-12-19 06:10:54.622239: Validation loss did not improve from -0.53168. Patience: 60/50
2024-12-19 06:10:54.623397: train_loss -0.8232
2024-12-19 06:10:54.624308: val_loss -0.4826
2024-12-19 06:10:54.625071: Pseudo dice [0.7313]
2024-12-19 06:10:54.625807: Epoch time: 192.93 s
2024-12-19 06:10:56.090048: 
2024-12-19 06:10:56.091482: Epoch 142
2024-12-19 06:10:56.092173: Current learning rate: 0.00071
2024-12-19 06:14:12.245624: Validation loss did not improve from -0.53168. Patience: 61/50
2024-12-19 06:14:12.247725: train_loss -0.8253
2024-12-19 06:14:12.248546: val_loss -0.4431
2024-12-19 06:14:12.249280: Pseudo dice [0.7217]
2024-12-19 06:14:12.250230: Epoch time: 196.16 s
2024-12-19 06:14:13.726928: 
2024-12-19 06:14:13.728440: Epoch 143
2024-12-19 06:14:13.729505: Current learning rate: 0.00063
2024-12-19 06:17:40.345448: Validation loss did not improve from -0.53168. Patience: 62/50
2024-12-19 06:17:40.346227: train_loss -0.8237
2024-12-19 06:17:40.347268: val_loss -0.46
2024-12-19 06:17:40.348318: Pseudo dice [0.7354]
2024-12-19 06:17:40.349320: Epoch time: 206.62 s
2024-12-19 06:17:41.867687: 
2024-12-19 06:17:41.869181: Epoch 144
2024-12-19 06:17:41.870261: Current learning rate: 0.00055
2024-12-19 06:20:50.076543: Validation loss did not improve from -0.53168. Patience: 63/50
2024-12-19 06:20:50.077490: train_loss -0.8256
2024-12-19 06:20:50.078307: val_loss -0.4455
2024-12-19 06:20:50.079053: Pseudo dice [0.7165]
2024-12-19 06:20:50.079737: Epoch time: 188.21 s
2024-12-19 06:20:51.972723: 
2024-12-19 06:20:51.974255: Epoch 145
2024-12-19 06:20:51.975243: Current learning rate: 0.00047
2024-12-19 06:24:14.718322: Validation loss did not improve from -0.53168. Patience: 64/50
2024-12-19 06:24:14.719332: train_loss -0.8278
2024-12-19 06:24:14.720088: val_loss -0.4395
2024-12-19 06:24:14.720745: Pseudo dice [0.7213]
2024-12-19 06:24:14.721536: Epoch time: 202.75 s
2024-12-19 06:24:16.189191: 
2024-12-19 06:24:16.190441: Epoch 146
2024-12-19 06:24:16.191221: Current learning rate: 0.00038
2024-12-19 06:27:30.899931: Validation loss did not improve from -0.53168. Patience: 65/50
2024-12-19 06:27:30.900908: train_loss -0.8258
2024-12-19 06:27:30.901720: val_loss -0.4651
2024-12-19 06:27:30.902612: Pseudo dice [0.743]
2024-12-19 06:27:30.903535: Epoch time: 194.71 s
2024-12-19 06:27:32.413729: 
2024-12-19 06:27:32.415201: Epoch 147
2024-12-19 06:27:32.416125: Current learning rate: 0.0003
2024-12-19 06:30:42.661163: Validation loss did not improve from -0.53168. Patience: 66/50
2024-12-19 06:30:42.662156: train_loss -0.8276
2024-12-19 06:30:42.662895: val_loss -0.4246
2024-12-19 06:30:42.663724: Pseudo dice [0.7161]
2024-12-19 06:30:42.664582: Epoch time: 190.25 s
2024-12-19 06:30:44.127573: 
2024-12-19 06:30:44.128981: Epoch 148
2024-12-19 06:30:44.129820: Current learning rate: 0.00021
2024-12-19 06:34:33.415534: Validation loss did not improve from -0.53168. Patience: 67/50
2024-12-19 06:34:33.416463: train_loss -0.8278
2024-12-19 06:34:33.417173: val_loss -0.4669
2024-12-19 06:34:33.417914: Pseudo dice [0.7328]
2024-12-19 06:34:33.418761: Epoch time: 229.29 s
2024-12-19 06:34:34.936722: 
2024-12-19 06:34:34.938001: Epoch 149
2024-12-19 06:34:34.938791: Current learning rate: 0.00011
2024-12-19 06:38:03.651733: Validation loss did not improve from -0.53168. Patience: 68/50
2024-12-19 06:38:03.652624: train_loss -0.8273
2024-12-19 06:38:03.653362: val_loss -0.4587
2024-12-19 06:38:03.654079: Pseudo dice [0.7189]
2024-12-19 06:38:03.654784: Epoch time: 208.72 s
2024-12-19 06:38:05.665043: Training done.
2024-12-19 06:38:06.190596: Using splits from existing split file: /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset309_Sohee_Calcium_OCT_CrossValidation/splits_final_40.json
2024-12-19 06:38:06.198606: The split file contains 5 splits.
2024-12-19 06:38:06.199384: Desired fold for training: 2
2024-12-19 06:38:06.200305: This split has 3 training and 5 validation cases.
2024-12-19 06:38:06.201195: predicting 101-044
2024-12-19 06:38:06.282013: 101-044, shape torch.Size([1, 404, 498, 498]), rank 0
2024-12-19 06:40:09.734306: predicting 106-002
2024-12-19 06:40:09.776551: 106-002, shape torch.Size([1, 539, 498, 498]), rank 0
2024-12-19 06:43:18.861727: predicting 401-004
2024-12-19 06:43:18.917711: 401-004, shape torch.Size([1, 375, 498, 498]), rank 0
2024-12-19 06:45:17.793351: predicting 701-013
2024-12-19 06:45:17.851760: 701-013, shape torch.Size([1, 375, 498, 498]), rank 0
2024-12-19 06:47:07.498449: predicting 704-003
2024-12-19 06:47:07.519628: 704-003, shape torch.Size([1, 375, 498, 498]), rank 0
2024-12-19 06:49:31.778604: Validation complete
2024-12-19 06:49:31.779253: Mean Validation Dice:  0.7103315012100196
2024-12-18 18:06:22.715874: unpacking done...
2024-12-18 18:06:22.723360: Unable to plot network architecture: nnUNet_compile is enabled!
2024-12-18 18:06:22.760011: 
2024-12-18 18:06:22.760882: Epoch 0
2024-12-18 18:06:22.761729: Current learning rate: 0.01
2024-12-18 18:17:31.674033: Validation loss improved from 1000.00000 to -0.31693! Patience: 0/50
2024-12-18 18:17:31.674987: train_loss -0.343
2024-12-18 18:17:31.675837: val_loss -0.3169
2024-12-18 18:17:31.676615: Pseudo dice [0.6414]
2024-12-18 18:17:31.677381: Epoch time: 668.92 s
2024-12-18 18:17:31.678179: Yayy! New best EMA pseudo Dice: 0.6414
2024-12-18 18:17:33.266465: 
2024-12-18 18:17:33.267788: Epoch 1
2024-12-18 18:17:33.268921: Current learning rate: 0.00994
2024-12-18 18:26:44.650895: Validation loss improved from -0.31693 to -0.39188! Patience: 0/50
2024-12-18 18:26:44.651869: train_loss -0.5183
2024-12-18 18:26:44.652668: val_loss -0.3919
2024-12-18 18:26:44.653408: Pseudo dice [0.6821]
2024-12-18 18:26:44.654081: Epoch time: 551.39 s
2024-12-18 18:26:44.654802: Yayy! New best EMA pseudo Dice: 0.6455
2024-12-18 18:26:46.505348: 
2024-12-18 18:26:46.506615: Epoch 2
2024-12-18 18:26:46.507361: Current learning rate: 0.00988
2024-12-18 18:35:24.403198: Validation loss improved from -0.39188 to -0.41015! Patience: 0/50
2024-12-18 18:35:24.404176: train_loss -0.5485
2024-12-18 18:35:24.404991: val_loss -0.4102
2024-12-18 18:35:24.405706: Pseudo dice [0.6764]
2024-12-18 18:35:24.406408: Epoch time: 517.9 s
2024-12-18 18:35:24.407159: Yayy! New best EMA pseudo Dice: 0.6486
2024-12-18 18:35:26.210535: 
2024-12-18 18:35:26.211610: Epoch 3
2024-12-18 18:35:26.212358: Current learning rate: 0.00982
2024-12-18 18:44:03.138983: Validation loss did not improve from -0.41015. Patience: 1/50
2024-12-18 18:44:03.139965: train_loss -0.5785
2024-12-18 18:44:03.140808: val_loss -0.3849
2024-12-18 18:44:03.141533: Pseudo dice [0.6742]
2024-12-18 18:44:03.142329: Epoch time: 516.93 s
2024-12-18 18:44:03.143380: Yayy! New best EMA pseudo Dice: 0.6511
2024-12-18 18:44:04.922040: 
2024-12-18 18:44:04.923383: Epoch 4
2024-12-18 18:44:04.924525: Current learning rate: 0.00976
2024-12-18 18:51:21.399439: Validation loss did not improve from -0.41015. Patience: 2/50
2024-12-18 18:51:21.400508: train_loss -0.6077
2024-12-18 18:51:21.401303: val_loss -0.398
2024-12-18 18:51:21.402134: Pseudo dice [0.688]
2024-12-18 18:51:21.403097: Epoch time: 436.48 s
2024-12-18 18:51:21.802517: Yayy! New best EMA pseudo Dice: 0.6548
2024-12-18 18:51:23.650944: 
2024-12-18 18:51:23.652157: Epoch 5
2024-12-18 18:51:23.653103: Current learning rate: 0.0097
2024-12-18 18:59:06.627824: Validation loss improved from -0.41015 to -0.42089! Patience: 2/50
2024-12-18 18:59:06.628569: train_loss -0.6228
2024-12-18 18:59:06.629610: val_loss -0.4209
2024-12-18 18:59:06.630567: Pseudo dice [0.6992]
2024-12-18 18:59:06.631583: Epoch time: 462.98 s
2024-12-18 18:59:06.632515: Yayy! New best EMA pseudo Dice: 0.6593
2024-12-18 18:59:08.383970: 
2024-12-18 18:59:08.385460: Epoch 6
2024-12-18 18:59:08.386654: Current learning rate: 0.00964
2024-12-18 19:07:47.502888: Validation loss did not improve from -0.42089. Patience: 1/50
2024-12-18 19:07:47.503507: train_loss -0.6334
2024-12-18 19:07:47.504344: val_loss -0.4048
2024-12-18 19:07:47.505390: Pseudo dice [0.6641]
2024-12-18 19:07:47.506231: Epoch time: 519.12 s
2024-12-18 19:07:47.506919: Yayy! New best EMA pseudo Dice: 0.6597
2024-12-18 19:07:49.299430: 
2024-12-18 19:07:49.300787: Epoch 7
2024-12-18 19:07:49.301534: Current learning rate: 0.00958
2024-12-18 19:16:05.190402: Validation loss did not improve from -0.42089. Patience: 2/50
2024-12-18 19:16:05.192539: train_loss -0.6473
2024-12-18 19:16:05.193694: val_loss -0.4037
2024-12-18 19:16:05.194662: Pseudo dice [0.6875]
2024-12-18 19:16:05.195693: Epoch time: 495.89 s
2024-12-18 19:16:05.196626: Yayy! New best EMA pseudo Dice: 0.6625
2024-12-18 19:16:06.950920: 
2024-12-18 19:16:06.952360: Epoch 8
2024-12-18 19:16:06.953496: Current learning rate: 0.00952
2024-12-18 19:24:21.009828: Validation loss did not improve from -0.42089. Patience: 3/50
2024-12-18 19:24:21.010915: train_loss -0.6604
2024-12-18 19:24:21.012198: val_loss -0.4205
2024-12-18 19:24:21.013382: Pseudo dice [0.6953]
2024-12-18 19:24:21.014604: Epoch time: 494.06 s
2024-12-18 19:24:21.015907: Yayy! New best EMA pseudo Dice: 0.6658
2024-12-18 19:24:24.049344: 
2024-12-18 19:24:24.050958: Epoch 9
2024-12-18 19:24:24.052018: Current learning rate: 0.00946
2024-12-18 19:33:20.953388: Validation loss improved from -0.42089 to -0.43614! Patience: 3/50
2024-12-18 19:33:20.955220: train_loss -0.6613
2024-12-18 19:33:20.956189: val_loss -0.4361
2024-12-18 19:33:20.956973: Pseudo dice [0.6985]
2024-12-18 19:33:20.957687: Epoch time: 536.91 s
2024-12-18 19:33:21.317026: Yayy! New best EMA pseudo Dice: 0.6691
2024-12-18 19:33:23.035869: 
2024-12-18 19:33:23.037283: Epoch 10
2024-12-18 19:33:23.038174: Current learning rate: 0.0094
2024-12-18 19:41:43.396949: Validation loss did not improve from -0.43614. Patience: 1/50
2024-12-18 19:41:43.397890: train_loss -0.6711
2024-12-18 19:41:43.398616: val_loss -0.3932
2024-12-18 19:41:43.399358: Pseudo dice [0.6912]
2024-12-18 19:41:43.400226: Epoch time: 500.36 s
2024-12-18 19:41:43.401022: Yayy! New best EMA pseudo Dice: 0.6713
2024-12-18 19:41:45.225616: 
2024-12-18 19:41:45.226775: Epoch 11
2024-12-18 19:41:45.227624: Current learning rate: 0.00934
2024-12-18 19:50:00.379090: Validation loss did not improve from -0.43614. Patience: 2/50
2024-12-18 19:50:00.380044: train_loss -0.679
2024-12-18 19:50:00.380898: val_loss -0.4266
2024-12-18 19:50:00.381603: Pseudo dice [0.7033]
2024-12-18 19:50:00.382300: Epoch time: 495.16 s
2024-12-18 19:50:00.383043: Yayy! New best EMA pseudo Dice: 0.6745
2024-12-18 19:50:02.165128: 
2024-12-18 19:50:02.166276: Epoch 12
2024-12-18 19:50:02.167169: Current learning rate: 0.00928
2024-12-18 19:59:00.035561: Validation loss improved from -0.43614 to -0.43808! Patience: 2/50
2024-12-18 19:59:00.036416: train_loss -0.6859
2024-12-18 19:59:00.037200: val_loss -0.4381
2024-12-18 19:59:00.037976: Pseudo dice [0.71]
2024-12-18 19:59:00.038642: Epoch time: 537.87 s
2024-12-18 19:59:00.039301: Yayy! New best EMA pseudo Dice: 0.678
2024-12-18 19:59:01.823911: 
2024-12-18 19:59:01.825141: Epoch 13
2024-12-18 19:59:01.825805: Current learning rate: 0.00922
2024-12-18 20:07:58.268877: Validation loss improved from -0.43808 to -0.45492! Patience: 0/50
2024-12-18 20:07:58.269909: train_loss -0.6886
2024-12-18 20:07:58.270758: val_loss -0.4549
2024-12-18 20:07:58.271511: Pseudo dice [0.7142]
2024-12-18 20:07:58.272362: Epoch time: 536.45 s
2024-12-18 20:07:58.273147: Yayy! New best EMA pseudo Dice: 0.6817
2024-12-18 20:08:00.060614: 
2024-12-18 20:08:00.062150: Epoch 14
2024-12-18 20:08:00.063293: Current learning rate: 0.00916
2024-12-18 20:16:42.042663: Validation loss did not improve from -0.45492. Patience: 1/50
2024-12-18 20:16:42.044283: train_loss -0.6972
2024-12-18 20:16:42.045946: val_loss -0.4452
2024-12-18 20:16:42.046609: Pseudo dice [0.7091]
2024-12-18 20:16:42.047541: Epoch time: 521.98 s
2024-12-18 20:16:42.412635: Yayy! New best EMA pseudo Dice: 0.6844
2024-12-18 20:16:44.274189: 
2024-12-18 20:16:44.275328: Epoch 15
2024-12-18 20:16:44.276041: Current learning rate: 0.0091
2024-12-18 20:25:20.149753: Validation loss improved from -0.45492 to -0.47056! Patience: 1/50
2024-12-18 20:25:20.150883: train_loss -0.706
2024-12-18 20:25:20.151847: val_loss -0.4706
2024-12-18 20:25:20.152651: Pseudo dice [0.7267]
2024-12-18 20:25:20.153655: Epoch time: 515.88 s
2024-12-18 20:25:20.154659: Yayy! New best EMA pseudo Dice: 0.6886
2024-12-18 20:25:21.902026: 
2024-12-18 20:25:21.903220: Epoch 16
2024-12-18 20:25:21.904168: Current learning rate: 0.00903
2024-12-18 20:34:20.551974: Validation loss did not improve from -0.47056. Patience: 1/50
2024-12-18 20:34:20.553021: train_loss -0.7112
2024-12-18 20:34:20.553930: val_loss -0.4639
2024-12-18 20:34:20.554734: Pseudo dice [0.7096]
2024-12-18 20:34:20.555428: Epoch time: 538.65 s
2024-12-18 20:34:20.556103: Yayy! New best EMA pseudo Dice: 0.6907
2024-12-18 20:34:22.372534: 
2024-12-18 20:34:22.374196: Epoch 17
2024-12-18 20:34:22.375090: Current learning rate: 0.00897
2024-12-18 20:43:30.260633: Validation loss did not improve from -0.47056. Patience: 2/50
2024-12-18 20:43:30.261680: train_loss -0.7118
2024-12-18 20:43:30.262460: val_loss -0.4528
2024-12-18 20:43:30.263246: Pseudo dice [0.71]
2024-12-18 20:43:30.264003: Epoch time: 547.89 s
2024-12-18 20:43:30.264655: Yayy! New best EMA pseudo Dice: 0.6926
2024-12-18 20:43:32.051411: 
2024-12-18 20:43:32.052566: Epoch 18
2024-12-18 20:43:32.053290: Current learning rate: 0.00891
2024-12-18 20:51:50.181517: Validation loss did not improve from -0.47056. Patience: 3/50
2024-12-18 20:51:50.182358: train_loss -0.7142
2024-12-18 20:51:50.183569: val_loss -0.4597
2024-12-18 20:51:50.184616: Pseudo dice [0.7185]
2024-12-18 20:51:50.185643: Epoch time: 498.13 s
2024-12-18 20:51:50.186547: Yayy! New best EMA pseudo Dice: 0.6952
2024-12-18 20:51:52.446020: 
2024-12-18 20:51:52.447391: Epoch 19
2024-12-18 20:51:52.448261: Current learning rate: 0.00885
2024-12-18 21:00:39.837976: Validation loss did not improve from -0.47056. Patience: 4/50
2024-12-18 21:00:39.838950: train_loss -0.7221
2024-12-18 21:00:39.839759: val_loss -0.4674
2024-12-18 21:00:39.840423: Pseudo dice [0.7219]
2024-12-18 21:00:39.841106: Epoch time: 527.39 s
2024-12-18 21:00:40.296872: Yayy! New best EMA pseudo Dice: 0.6979
2024-12-18 21:00:42.195855: 
2024-12-18 21:00:42.197122: Epoch 20
2024-12-18 21:00:42.197838: Current learning rate: 0.00879
2024-12-18 21:10:02.726012: Validation loss did not improve from -0.47056. Patience: 5/50
2024-12-18 21:10:02.726976: train_loss -0.7256
2024-12-18 21:10:02.727717: val_loss -0.4616
2024-12-18 21:10:02.728423: Pseudo dice [0.7177]
2024-12-18 21:10:02.729075: Epoch time: 560.53 s
2024-12-18 21:10:02.729820: Yayy! New best EMA pseudo Dice: 0.6999
2024-12-18 21:10:04.548902: 
2024-12-18 21:10:04.550188: Epoch 21
2024-12-18 21:10:04.550876: Current learning rate: 0.00873
2024-12-18 21:19:08.282985: Validation loss did not improve from -0.47056. Patience: 6/50
2024-12-18 21:19:08.284006: train_loss -0.7274
2024-12-18 21:19:08.285128: val_loss -0.4341
2024-12-18 21:19:08.286186: Pseudo dice [0.7086]
2024-12-18 21:19:08.287181: Epoch time: 543.74 s
2024-12-18 21:19:08.288158: Yayy! New best EMA pseudo Dice: 0.7008
2024-12-18 21:19:10.076934: 
2024-12-18 21:19:10.078418: Epoch 22
2024-12-18 21:19:10.079419: Current learning rate: 0.00867
2024-12-18 21:27:43.932927: Validation loss did not improve from -0.47056. Patience: 7/50
2024-12-18 21:27:43.934868: train_loss -0.7336
2024-12-18 21:27:43.936029: val_loss -0.4506
2024-12-18 21:27:43.936812: Pseudo dice [0.7218]
2024-12-18 21:27:43.937649: Epoch time: 513.86 s
2024-12-18 21:27:43.938431: Yayy! New best EMA pseudo Dice: 0.7029
2024-12-18 21:27:45.704281: 
2024-12-18 21:27:45.705781: Epoch 23
2024-12-18 21:27:45.706642: Current learning rate: 0.00861
2024-12-18 21:36:39.641996: Validation loss did not improve from -0.47056. Patience: 8/50
2024-12-18 21:36:39.707840: train_loss -0.7322
2024-12-18 21:36:39.709024: val_loss -0.4429
2024-12-18 21:36:39.709720: Pseudo dice [0.7082]
2024-12-18 21:36:39.710490: Epoch time: 534.0 s
2024-12-18 21:36:39.711226: Yayy! New best EMA pseudo Dice: 0.7034
2024-12-18 21:36:41.413259: 
2024-12-18 21:36:41.414437: Epoch 24
2024-12-18 21:36:41.415222: Current learning rate: 0.00855
2024-12-18 21:45:43.361127: Validation loss did not improve from -0.47056. Patience: 9/50
2024-12-18 21:45:43.362078: train_loss -0.7431
2024-12-18 21:45:43.362921: val_loss -0.4619
2024-12-18 21:45:43.363679: Pseudo dice [0.7158]
2024-12-18 21:45:43.364383: Epoch time: 541.95 s
2024-12-18 21:45:43.717501: Yayy! New best EMA pseudo Dice: 0.7046
2024-12-18 21:45:45.454228: 
2024-12-18 21:45:45.455473: Epoch 25
2024-12-18 21:45:45.456269: Current learning rate: 0.00849
2024-12-18 21:54:01.982081: Validation loss did not improve from -0.47056. Patience: 10/50
2024-12-18 21:54:01.982864: train_loss -0.7404
2024-12-18 21:54:01.983680: val_loss -0.4622
2024-12-18 21:54:01.984371: Pseudo dice [0.7176]
2024-12-18 21:54:01.985088: Epoch time: 496.53 s
2024-12-18 21:54:01.985724: Yayy! New best EMA pseudo Dice: 0.7059
2024-12-18 21:54:03.884042: 
2024-12-18 21:54:03.885049: Epoch 26
2024-12-18 21:54:03.885723: Current learning rate: 0.00843
2024-12-18 22:02:49.646135: Validation loss did not improve from -0.47056. Patience: 11/50
2024-12-18 22:02:49.647091: train_loss -0.7424
2024-12-18 22:02:49.647806: val_loss -0.4345
2024-12-18 22:02:49.648469: Pseudo dice [0.7071]
2024-12-18 22:02:49.649268: Epoch time: 525.76 s
2024-12-18 22:02:49.649928: Yayy! New best EMA pseudo Dice: 0.706
2024-12-18 22:02:51.409750: 
2024-12-18 22:02:51.411152: Epoch 27
2024-12-18 22:02:51.411974: Current learning rate: 0.00836
2024-12-18 22:11:42.291440: Validation loss did not improve from -0.47056. Patience: 12/50
2024-12-18 22:11:42.292466: train_loss -0.747
2024-12-18 22:11:42.293408: val_loss -0.433
2024-12-18 22:11:42.294157: Pseudo dice [0.7035]
2024-12-18 22:11:42.294941: Epoch time: 530.88 s
2024-12-18 22:11:43.643924: 
2024-12-18 22:11:43.645309: Epoch 28
2024-12-18 22:11:43.646116: Current learning rate: 0.0083
2024-12-18 22:20:42.497852: Validation loss did not improve from -0.47056. Patience: 13/50
2024-12-18 22:20:42.498839: train_loss -0.7518
2024-12-18 22:20:42.499604: val_loss -0.4289
2024-12-18 22:20:42.500315: Pseudo dice [0.7164]
2024-12-18 22:20:42.500998: Epoch time: 538.86 s
2024-12-18 22:20:42.501662: Yayy! New best EMA pseudo Dice: 0.7068
2024-12-18 22:20:44.716872: 
2024-12-18 22:20:44.717998: Epoch 29
2024-12-18 22:20:44.718802: Current learning rate: 0.00824
2024-12-18 22:30:21.159544: Validation loss did not improve from -0.47056. Patience: 14/50
2024-12-18 22:30:21.161664: train_loss -0.7503
2024-12-18 22:30:21.162578: val_loss -0.4424
2024-12-18 22:30:21.163355: Pseudo dice [0.7092]
2024-12-18 22:30:21.164321: Epoch time: 576.45 s
2024-12-18 22:30:21.605429: Yayy! New best EMA pseudo Dice: 0.7071
2024-12-18 22:30:23.318310: 
2024-12-18 22:30:23.319486: Epoch 30
2024-12-18 22:30:23.320221: Current learning rate: 0.00818
2024-12-18 22:39:09.920683: Validation loss did not improve from -0.47056. Patience: 15/50
2024-12-18 22:39:09.921881: train_loss -0.749
2024-12-18 22:39:09.922750: val_loss -0.433
2024-12-18 22:39:09.923552: Pseudo dice [0.7032]
2024-12-18 22:39:09.924532: Epoch time: 526.6 s
2024-12-18 22:39:11.307013: 
2024-12-18 22:39:11.308191: Epoch 31
2024-12-18 22:39:11.309018: Current learning rate: 0.00812
2024-12-18 22:48:01.483444: Validation loss did not improve from -0.47056. Patience: 16/50
2024-12-18 22:48:01.484429: train_loss -0.7567
2024-12-18 22:48:01.485408: val_loss -0.4194
2024-12-18 22:48:01.486189: Pseudo dice [0.7015]
2024-12-18 22:48:01.486934: Epoch time: 530.18 s
2024-12-18 22:48:02.899802: 
2024-12-18 22:48:02.900960: Epoch 32
2024-12-18 22:48:02.901787: Current learning rate: 0.00806
2024-12-18 22:56:45.216020: Validation loss did not improve from -0.47056. Patience: 17/50
2024-12-18 22:56:45.217073: train_loss -0.7526
2024-12-18 22:56:45.217933: val_loss -0.4303
2024-12-18 22:56:45.218587: Pseudo dice [0.7043]
2024-12-18 22:56:45.219253: Epoch time: 522.32 s
2024-12-18 22:56:46.619173: 
2024-12-18 22:56:46.620381: Epoch 33
2024-12-18 22:56:46.621119: Current learning rate: 0.008
2024-12-18 23:06:11.823130: Validation loss did not improve from -0.47056. Patience: 18/50
2024-12-18 23:06:11.823979: train_loss -0.7583
2024-12-18 23:06:11.824893: val_loss -0.4204
2024-12-18 23:06:11.825725: Pseudo dice [0.7023]
2024-12-18 23:06:11.826598: Epoch time: 565.21 s
2024-12-18 23:06:13.299160: 
2024-12-18 23:06:13.300519: Epoch 34
2024-12-18 23:06:13.301451: Current learning rate: 0.00793
2024-12-18 23:14:52.094412: Validation loss did not improve from -0.47056. Patience: 19/50
2024-12-18 23:14:52.095594: train_loss -0.7629
2024-12-18 23:14:52.096434: val_loss -0.4414
2024-12-18 23:14:52.097215: Pseudo dice [0.7145]
2024-12-18 23:14:52.097888: Epoch time: 518.8 s
2024-12-18 23:14:54.004826: 
2024-12-18 23:14:54.006115: Epoch 35
2024-12-18 23:14:54.007080: Current learning rate: 0.00787
2024-12-18 23:23:14.426959: Validation loss did not improve from -0.47056. Patience: 20/50
2024-12-18 23:23:14.427902: train_loss -0.7664
2024-12-18 23:23:14.428661: val_loss -0.4249
2024-12-18 23:23:14.429269: Pseudo dice [0.712]
2024-12-18 23:23:14.429915: Epoch time: 500.42 s
2024-12-18 23:23:15.861254: 
2024-12-18 23:23:15.862554: Epoch 36
2024-12-18 23:23:15.863359: Current learning rate: 0.00781
2024-12-18 23:32:12.806113: Validation loss did not improve from -0.47056. Patience: 21/50
2024-12-18 23:32:12.808457: train_loss -0.7655
2024-12-18 23:32:12.809383: val_loss -0.4537
2024-12-18 23:32:12.810424: Pseudo dice [0.7254]
2024-12-18 23:32:12.811743: Epoch time: 536.95 s
2024-12-18 23:32:12.813000: Yayy! New best EMA pseudo Dice: 0.7089
2024-12-18 23:32:14.772569: 
2024-12-18 23:32:14.774171: Epoch 37
2024-12-18 23:32:14.775063: Current learning rate: 0.00775
2024-12-18 23:41:10.928354: Validation loss did not improve from -0.47056. Patience: 22/50
2024-12-18 23:41:10.929431: train_loss -0.7643
2024-12-18 23:41:10.930336: val_loss -0.4219
2024-12-18 23:41:10.931203: Pseudo dice [0.7064]
2024-12-18 23:41:10.932173: Epoch time: 536.16 s
2024-12-18 23:41:12.340562: 
2024-12-18 23:41:12.342018: Epoch 38
2024-12-18 23:41:12.342808: Current learning rate: 0.00769
2024-12-18 23:49:48.695556: Validation loss did not improve from -0.47056. Patience: 23/50
2024-12-18 23:49:48.696553: train_loss -0.7675
2024-12-18 23:49:48.697323: val_loss -0.4044
2024-12-18 23:49:48.698098: Pseudo dice [0.7014]
2024-12-18 23:49:48.698801: Epoch time: 516.36 s
2024-12-18 23:49:50.098361: 
2024-12-18 23:49:50.099667: Epoch 39
2024-12-18 23:49:50.100423: Current learning rate: 0.00763
2024-12-18 23:58:17.128947: Validation loss did not improve from -0.47056. Patience: 24/50
2024-12-18 23:58:17.130007: train_loss -0.7731
2024-12-18 23:58:17.130852: val_loss -0.4428
2024-12-18 23:58:17.131631: Pseudo dice [0.718]
2024-12-18 23:58:17.132492: Epoch time: 507.03 s
2024-12-18 23:58:17.639673: Yayy! New best EMA pseudo Dice: 0.7089
2024-12-18 23:58:19.793463: 
2024-12-18 23:58:19.794564: Epoch 40
2024-12-18 23:58:19.795510: Current learning rate: 0.00756
2024-12-19 00:06:58.511044: Validation loss did not improve from -0.47056. Patience: 25/50
2024-12-19 00:06:58.511941: train_loss -0.7713
2024-12-19 00:06:58.512878: val_loss -0.4449
2024-12-19 00:06:58.513532: Pseudo dice [0.7212]
2024-12-19 00:06:58.514405: Epoch time: 518.72 s
2024-12-19 00:06:58.515221: Yayy! New best EMA pseudo Dice: 0.7101
2024-12-19 00:07:00.554988: 
2024-12-19 00:07:00.556509: Epoch 41
2024-12-19 00:07:00.557306: Current learning rate: 0.0075
2024-12-19 00:16:08.275400: Validation loss did not improve from -0.47056. Patience: 26/50
2024-12-19 00:16:08.276587: train_loss -0.776
2024-12-19 00:16:08.277434: val_loss -0.447
2024-12-19 00:16:08.278126: Pseudo dice [0.7161]
2024-12-19 00:16:08.278827: Epoch time: 547.72 s
2024-12-19 00:16:08.279626: Yayy! New best EMA pseudo Dice: 0.7107
2024-12-19 00:16:10.082615: 
2024-12-19 00:16:10.083854: Epoch 42
2024-12-19 00:16:10.084784: Current learning rate: 0.00744
2024-12-19 00:24:36.463648: Validation loss did not improve from -0.47056. Patience: 27/50
2024-12-19 00:24:36.464627: train_loss -0.773
2024-12-19 00:24:36.465491: val_loss -0.4314
2024-12-19 00:24:36.466228: Pseudo dice [0.7188]
2024-12-19 00:24:36.467107: Epoch time: 506.38 s
2024-12-19 00:24:36.467950: Yayy! New best EMA pseudo Dice: 0.7115
2024-12-19 00:24:38.180431: 
2024-12-19 00:24:38.181832: Epoch 43
2024-12-19 00:24:38.182824: Current learning rate: 0.00738
2024-12-19 00:33:24.392673: Validation loss did not improve from -0.47056. Patience: 28/50
2024-12-19 00:33:24.393526: train_loss -0.7736
2024-12-19 00:33:24.394421: val_loss -0.4159
2024-12-19 00:33:24.395108: Pseudo dice [0.6946]
2024-12-19 00:33:24.395869: Epoch time: 526.21 s
2024-12-19 00:33:25.757391: 
2024-12-19 00:33:25.758769: Epoch 44
2024-12-19 00:33:25.759613: Current learning rate: 0.00732
2024-12-19 00:41:36.458858: Validation loss did not improve from -0.47056. Patience: 29/50
2024-12-19 00:41:36.460762: train_loss -0.7766
2024-12-19 00:41:36.461749: val_loss -0.4386
2024-12-19 00:41:36.462771: Pseudo dice [0.7107]
2024-12-19 00:41:36.463753: Epoch time: 490.7 s
2024-12-19 00:41:38.248300: 
2024-12-19 00:41:38.249648: Epoch 45
2024-12-19 00:41:38.250434: Current learning rate: 0.00725
2024-12-19 00:50:47.198755: Validation loss did not improve from -0.47056. Patience: 30/50
2024-12-19 00:50:47.199937: train_loss -0.7838
2024-12-19 00:50:47.200698: val_loss -0.4427
2024-12-19 00:50:47.201372: Pseudo dice [0.715]
2024-12-19 00:50:47.202018: Epoch time: 548.95 s
2024-12-19 00:50:48.588542: 
2024-12-19 00:50:48.589859: Epoch 46
2024-12-19 00:50:48.590633: Current learning rate: 0.00719
2024-12-19 00:59:37.596906: Validation loss did not improve from -0.47056. Patience: 31/50
2024-12-19 00:59:37.598021: train_loss -0.7844
2024-12-19 00:59:37.598859: val_loss -0.4091
2024-12-19 00:59:37.599654: Pseudo dice [0.7052]
2024-12-19 00:59:37.600408: Epoch time: 529.01 s
2024-12-19 00:59:39.005902: 
2024-12-19 00:59:39.007132: Epoch 47
2024-12-19 00:59:39.007818: Current learning rate: 0.00713
2024-12-19 01:07:40.296067: Validation loss did not improve from -0.47056. Patience: 32/50
2024-12-19 01:07:40.297121: train_loss -0.781
2024-12-19 01:07:40.297954: val_loss -0.4126
2024-12-19 01:07:40.298846: Pseudo dice [0.7153]
2024-12-19 01:07:40.299663: Epoch time: 481.29 s
2024-12-19 01:07:41.640628: 
2024-12-19 01:07:41.642025: Epoch 48
2024-12-19 01:07:41.642977: Current learning rate: 0.00707
2024-12-19 01:16:18.755961: Validation loss did not improve from -0.47056. Patience: 33/50
2024-12-19 01:16:18.757070: train_loss -0.787
2024-12-19 01:16:18.758032: val_loss -0.4245
2024-12-19 01:16:18.758766: Pseudo dice [0.7079]
2024-12-19 01:16:18.759622: Epoch time: 517.12 s
2024-12-19 01:16:20.141736: 
2024-12-19 01:16:20.143194: Epoch 49
2024-12-19 01:16:20.144202: Current learning rate: 0.007
2024-12-19 01:25:33.997334: Validation loss did not improve from -0.47056. Patience: 34/50
2024-12-19 01:25:33.998466: train_loss -0.787
2024-12-19 01:25:33.999452: val_loss -0.4174
2024-12-19 01:25:34.000314: Pseudo dice [0.7035]
2024-12-19 01:25:34.001177: Epoch time: 553.86 s
2024-12-19 01:25:35.813988: 
2024-12-19 01:25:35.815367: Epoch 50
2024-12-19 01:25:35.816336: Current learning rate: 0.00694
2024-12-19 01:34:17.574663: Validation loss did not improve from -0.47056. Patience: 35/50
2024-12-19 01:34:17.575694: train_loss -0.788
2024-12-19 01:34:17.576435: val_loss -0.4138
2024-12-19 01:34:17.577096: Pseudo dice [0.7156]
2024-12-19 01:34:17.577885: Epoch time: 521.76 s
2024-12-19 01:34:19.042524: 
2024-12-19 01:34:19.043911: Epoch 51
2024-12-19 01:34:19.044841: Current learning rate: 0.00688
2024-12-19 01:42:23.783416: Validation loss did not improve from -0.47056. Patience: 36/50
2024-12-19 01:42:23.785859: train_loss -0.7867
2024-12-19 01:42:23.786596: val_loss -0.4197
2024-12-19 01:42:23.787255: Pseudo dice [0.712]
2024-12-19 01:42:23.788274: Epoch time: 484.75 s
2024-12-19 01:42:25.646324: 
2024-12-19 01:42:25.647552: Epoch 52
2024-12-19 01:42:25.648256: Current learning rate: 0.00682
2024-12-19 01:51:26.747013: Validation loss did not improve from -0.47056. Patience: 37/50
2024-12-19 01:51:26.747891: train_loss -0.7897
2024-12-19 01:51:26.748807: val_loss -0.4327
2024-12-19 01:51:26.749554: Pseudo dice [0.7179]
2024-12-19 01:51:26.750257: Epoch time: 541.1 s
2024-12-19 01:51:28.114191: 
2024-12-19 01:51:28.115449: Epoch 53
2024-12-19 01:51:28.116360: Current learning rate: 0.00675
2024-12-19 01:59:40.280078: Validation loss did not improve from -0.47056. Patience: 38/50
2024-12-19 01:59:40.281187: train_loss -0.7905
2024-12-19 01:59:40.282266: val_loss -0.4121
2024-12-19 01:59:40.283099: Pseudo dice [0.7103]
2024-12-19 01:59:40.284017: Epoch time: 492.17 s
2024-12-19 01:59:41.751338: 
2024-12-19 01:59:41.752830: Epoch 54
2024-12-19 01:59:41.753884: Current learning rate: 0.00669
2024-12-19 02:08:00.764416: Validation loss did not improve from -0.47056. Patience: 39/50
2024-12-19 02:08:00.765486: train_loss -0.7913
2024-12-19 02:08:00.766545: val_loss -0.4186
2024-12-19 02:08:00.767459: Pseudo dice [0.711]
2024-12-19 02:08:00.768375: Epoch time: 499.02 s
2024-12-19 02:08:02.553318: 
2024-12-19 02:08:02.554705: Epoch 55
2024-12-19 02:08:02.555768: Current learning rate: 0.00663
2024-12-19 02:16:31.654868: Validation loss did not improve from -0.47056. Patience: 40/50
2024-12-19 02:16:31.655972: train_loss -0.7914
2024-12-19 02:16:31.656843: val_loss -0.4258
2024-12-19 02:16:31.657497: Pseudo dice [0.7076]
2024-12-19 02:16:31.658221: Epoch time: 509.1 s
2024-12-19 02:16:33.131004: 
2024-12-19 02:16:33.132404: Epoch 56
2024-12-19 02:16:33.133158: Current learning rate: 0.00657
2024-12-19 02:24:55.985332: Validation loss did not improve from -0.47056. Patience: 41/50
2024-12-19 02:24:55.986270: train_loss -0.7895
2024-12-19 02:24:55.987076: val_loss -0.3917
2024-12-19 02:24:55.987968: Pseudo dice [0.698]
2024-12-19 02:24:55.988956: Epoch time: 502.86 s
2024-12-19 02:24:57.369325: 
2024-12-19 02:24:57.370677: Epoch 57
2024-12-19 02:24:57.371579: Current learning rate: 0.0065
2024-12-19 02:33:40.150384: Validation loss did not improve from -0.47056. Patience: 42/50
2024-12-19 02:33:40.151179: train_loss -0.7931
2024-12-19 02:33:40.152161: val_loss -0.4227
2024-12-19 02:33:40.152877: Pseudo dice [0.7051]
2024-12-19 02:33:40.153638: Epoch time: 522.78 s
2024-12-19 02:33:41.555001: 
2024-12-19 02:33:41.556067: Epoch 58
2024-12-19 02:33:41.556736: Current learning rate: 0.00644
2024-12-19 02:42:26.656956: Validation loss did not improve from -0.47056. Patience: 43/50
2024-12-19 02:42:26.657950: train_loss -0.7921
2024-12-19 02:42:26.658811: val_loss -0.4284
2024-12-19 02:42:26.659553: Pseudo dice [0.715]
2024-12-19 02:42:26.660470: Epoch time: 525.1 s
2024-12-19 02:42:28.062420: 
2024-12-19 02:42:28.063803: Epoch 59
2024-12-19 02:42:28.064705: Current learning rate: 0.00638
2024-12-19 02:50:49.919292: Validation loss did not improve from -0.47056. Patience: 44/50
2024-12-19 02:50:49.942201: train_loss -0.7951
2024-12-19 02:50:49.943348: val_loss -0.4355
2024-12-19 02:50:49.944249: Pseudo dice [0.7083]
2024-12-19 02:50:49.945172: Epoch time: 501.88 s
2024-12-19 02:50:51.730994: 
2024-12-19 02:50:51.732523: Epoch 60
2024-12-19 02:50:51.733410: Current learning rate: 0.00631
2024-12-19 02:59:17.738020: Validation loss did not improve from -0.47056. Patience: 45/50
2024-12-19 02:59:17.739288: train_loss -0.7948
2024-12-19 02:59:17.740159: val_loss -0.4195
2024-12-19 02:59:17.740974: Pseudo dice [0.7031]
2024-12-19 02:59:17.741853: Epoch time: 506.01 s
2024-12-19 02:59:19.136332: 
2024-12-19 02:59:19.137553: Epoch 61
2024-12-19 02:59:19.138404: Current learning rate: 0.00625
2024-12-19 03:08:33.114940: Validation loss did not improve from -0.47056. Patience: 46/50
2024-12-19 03:08:33.115871: train_loss -0.7958
2024-12-19 03:08:33.116555: val_loss -0.4575
2024-12-19 03:08:33.117144: Pseudo dice [0.725]
2024-12-19 03:08:33.117939: Epoch time: 553.98 s
2024-12-19 03:08:34.493684: 
2024-12-19 03:08:34.495075: Epoch 62
2024-12-19 03:08:34.495893: Current learning rate: 0.00619
2024-12-19 03:16:49.035299: Validation loss did not improve from -0.47056. Patience: 47/50
2024-12-19 03:16:49.036454: train_loss -0.7994
2024-12-19 03:16:49.037278: val_loss -0.4089
2024-12-19 03:16:49.037973: Pseudo dice [0.7063]
2024-12-19 03:16:49.038790: Epoch time: 494.54 s
2024-12-19 03:16:50.958175: 
2024-12-19 03:16:50.959666: Epoch 63
2024-12-19 03:16:50.960355: Current learning rate: 0.00612
2024-12-19 03:24:58.782464: Validation loss did not improve from -0.47056. Patience: 48/50
2024-12-19 03:24:58.783718: train_loss -0.7987
2024-12-19 03:24:58.785026: val_loss -0.4033
2024-12-19 03:24:58.786136: Pseudo dice [0.704]
2024-12-19 03:24:58.787208: Epoch time: 487.83 s
2024-12-19 03:25:00.360119: 
2024-12-19 03:25:00.361782: Epoch 64
2024-12-19 03:25:00.362933: Current learning rate: 0.00606
2024-12-19 03:33:12.569785: Validation loss did not improve from -0.47056. Patience: 49/50
2024-12-19 03:33:12.570902: train_loss -0.8
2024-12-19 03:33:12.572137: val_loss -0.4444
2024-12-19 03:33:12.573309: Pseudo dice [0.7104]
2024-12-19 03:33:12.574577: Epoch time: 492.21 s
2024-12-19 03:33:14.492560: 
2024-12-19 03:33:14.494137: Epoch 65
2024-12-19 03:33:14.495340: Current learning rate: 0.006
2024-12-19 03:41:19.493718: Validation loss did not improve from -0.47056. Patience: 50/50
2024-12-19 03:41:19.494706: train_loss -0.8023
2024-12-19 03:41:19.496196: val_loss -0.4378
2024-12-19 03:41:19.496892: Pseudo dice [0.7085]
2024-12-19 03:41:19.497796: Epoch time: 485.0 s
2024-12-19 03:41:20.926654: 
2024-12-19 03:41:20.927911: Epoch 66
2024-12-19 03:41:20.928662: Current learning rate: 0.00593
2024-12-19 03:49:53.989399: Validation loss did not improve from -0.47056. Patience: 51/50
2024-12-19 03:49:53.990453: train_loss -0.8024
2024-12-19 03:49:53.991250: val_loss -0.4148
2024-12-19 03:49:53.991977: Pseudo dice [0.7139]
2024-12-19 03:49:53.992775: Epoch time: 513.07 s
2024-12-19 03:49:55.468596: 
2024-12-19 03:49:55.469834: Epoch 67
2024-12-19 03:49:55.470536: Current learning rate: 0.00587
2024-12-19 03:58:11.082181: Validation loss did not improve from -0.47056. Patience: 52/50
2024-12-19 03:58:11.101401: train_loss -0.802
2024-12-19 03:58:11.102580: val_loss -0.4368
2024-12-19 03:58:11.103380: Pseudo dice [0.7218]
2024-12-19 03:58:11.104159: Epoch time: 495.63 s
2024-12-19 03:58:12.504416: 
2024-12-19 03:58:12.505548: Epoch 68
2024-12-19 03:58:12.506340: Current learning rate: 0.00581
2024-12-19 04:06:16.008304: Validation loss did not improve from -0.47056. Patience: 53/50
2024-12-19 04:06:16.009871: train_loss -0.8018
2024-12-19 04:06:16.010846: val_loss -0.4049
2024-12-19 04:06:16.011489: Pseudo dice [0.7033]
2024-12-19 04:06:16.012120: Epoch time: 483.51 s
2024-12-19 04:06:17.500313: 
2024-12-19 04:06:17.501400: Epoch 69
2024-12-19 04:06:17.502087: Current learning rate: 0.00574
2024-12-19 04:14:55.551375: Validation loss did not improve from -0.47056. Patience: 54/50
2024-12-19 04:14:55.552484: train_loss -0.8056
2024-12-19 04:14:55.553433: val_loss -0.4354
2024-12-19 04:14:55.554133: Pseudo dice [0.7091]
2024-12-19 04:14:55.554815: Epoch time: 518.05 s
2024-12-19 04:14:57.403307: 
2024-12-19 04:14:57.404587: Epoch 70
2024-12-19 04:14:57.405468: Current learning rate: 0.00568
2024-12-19 04:23:15.741038: Validation loss did not improve from -0.47056. Patience: 55/50
2024-12-19 04:23:15.742123: train_loss -0.8049
2024-12-19 04:23:15.744170: val_loss -0.4154
2024-12-19 04:23:15.745044: Pseudo dice [0.7141]
2024-12-19 04:23:15.746075: Epoch time: 498.34 s
2024-12-19 04:23:17.206693: 
2024-12-19 04:23:17.208104: Epoch 71
2024-12-19 04:23:17.209207: Current learning rate: 0.00562
2024-12-19 04:31:26.970628: Validation loss did not improve from -0.47056. Patience: 56/50
2024-12-19 04:31:26.971678: train_loss -0.8048
2024-12-19 04:31:26.972511: val_loss -0.4173
2024-12-19 04:31:26.973159: Pseudo dice [0.7151]
2024-12-19 04:31:26.973920: Epoch time: 489.77 s
2024-12-19 04:31:28.383900: 
2024-12-19 04:31:28.385013: Epoch 72
2024-12-19 04:31:28.385837: Current learning rate: 0.00555
2024-12-19 04:39:08.721997: Validation loss did not improve from -0.47056. Patience: 57/50
2024-12-19 04:39:08.722929: train_loss -0.8082
2024-12-19 04:39:08.723977: val_loss -0.4101
2024-12-19 04:39:08.724756: Pseudo dice [0.7094]
2024-12-19 04:39:08.725488: Epoch time: 460.34 s
2024-12-19 04:39:10.193165: 
2024-12-19 04:39:10.194722: Epoch 73
2024-12-19 04:39:10.195505: Current learning rate: 0.00549
2024-12-19 04:47:01.199388: Validation loss did not improve from -0.47056. Patience: 58/50
2024-12-19 04:47:01.200336: train_loss -0.8074
2024-12-19 04:47:01.202309: val_loss -0.4136
2024-12-19 04:47:01.203164: Pseudo dice [0.7113]
2024-12-19 04:47:01.204064: Epoch time: 471.01 s
2024-12-19 04:47:02.921718: 
2024-12-19 04:47:02.922808: Epoch 74
2024-12-19 04:47:02.923573: Current learning rate: 0.00542
2024-12-19 04:54:51.904120: Validation loss did not improve from -0.47056. Patience: 59/50
2024-12-19 04:54:51.905019: train_loss -0.8072
2024-12-19 04:54:51.905873: val_loss -0.4338
2024-12-19 04:54:51.906620: Pseudo dice [0.7137]
2024-12-19 04:54:51.907641: Epoch time: 468.98 s
2024-12-19 04:54:53.733400: 
2024-12-19 04:54:53.734989: Epoch 75
2024-12-19 04:54:53.735998: Current learning rate: 0.00536
2024-12-19 05:02:15.805777: Validation loss did not improve from -0.47056. Patience: 60/50
2024-12-19 05:02:15.807820: train_loss -0.8091
2024-12-19 05:02:15.808902: val_loss -0.3595
2024-12-19 05:02:15.809690: Pseudo dice [0.6926]
2024-12-19 05:02:15.810331: Epoch time: 442.08 s
2024-12-19 05:02:17.282699: 
2024-12-19 05:02:17.283920: Epoch 76
2024-12-19 05:02:17.284719: Current learning rate: 0.00529
2024-12-19 05:09:28.501764: Validation loss did not improve from -0.47056. Patience: 61/50
2024-12-19 05:09:28.504923: train_loss -0.8103
2024-12-19 05:09:28.506208: val_loss -0.4058
2024-12-19 05:09:28.506970: Pseudo dice [0.7168]
2024-12-19 05:09:28.507722: Epoch time: 431.22 s
2024-12-19 05:09:30.034845: 
2024-12-19 05:09:30.036252: Epoch 77
2024-12-19 05:09:30.037064: Current learning rate: 0.00523
2024-12-19 05:17:22.395391: Validation loss did not improve from -0.47056. Patience: 62/50
2024-12-19 05:17:22.396344: train_loss -0.8104
2024-12-19 05:17:22.397539: val_loss -0.4232
2024-12-19 05:17:22.398530: Pseudo dice [0.7162]
2024-12-19 05:17:22.399453: Epoch time: 472.36 s
2024-12-19 05:17:23.872962: 
2024-12-19 05:17:23.874259: Epoch 78
2024-12-19 05:17:23.875193: Current learning rate: 0.00517
2024-12-19 05:24:32.503778: Validation loss did not improve from -0.47056. Patience: 63/50
2024-12-19 05:24:32.504856: train_loss -0.81
2024-12-19 05:24:32.505624: val_loss -0.4032
2024-12-19 05:24:32.506264: Pseudo dice [0.7137]
2024-12-19 05:24:32.507066: Epoch time: 428.63 s
2024-12-19 05:24:33.946441: 
2024-12-19 05:24:33.947703: Epoch 79
2024-12-19 05:24:33.948451: Current learning rate: 0.0051
2024-12-19 05:29:30.350097: Validation loss did not improve from -0.47056. Patience: 64/50
2024-12-19 05:29:30.351137: train_loss -0.8097
2024-12-19 05:29:30.351900: val_loss -0.4484
2024-12-19 05:29:30.352569: Pseudo dice [0.7276]
2024-12-19 05:29:30.353323: Epoch time: 296.41 s
2024-12-19 05:29:30.804116: Yayy! New best EMA pseudo Dice: 0.7126
2024-12-19 05:29:32.757193: 
2024-12-19 05:29:32.758447: Epoch 80
2024-12-19 05:29:32.759305: Current learning rate: 0.00504
2024-12-19 05:34:32.814413: Validation loss did not improve from -0.47056. Patience: 65/50
2024-12-19 05:34:32.815268: train_loss -0.8131
2024-12-19 05:34:32.816063: val_loss -0.4014
2024-12-19 05:34:32.816837: Pseudo dice [0.7171]
2024-12-19 05:34:32.817639: Epoch time: 300.06 s
2024-12-19 05:34:32.818395: Yayy! New best EMA pseudo Dice: 0.7131
2024-12-19 05:34:34.647584: 
2024-12-19 05:34:34.648877: Epoch 81
2024-12-19 05:34:34.649692: Current learning rate: 0.00497
2024-12-19 05:39:04.135942: Validation loss did not improve from -0.47056. Patience: 66/50
2024-12-19 05:39:04.136790: train_loss -0.8134
2024-12-19 05:39:04.137552: val_loss -0.4021
2024-12-19 05:39:04.138217: Pseudo dice [0.6975]
2024-12-19 05:39:04.138985: Epoch time: 269.49 s
2024-12-19 05:39:05.586601: 
2024-12-19 05:39:05.587734: Epoch 82
2024-12-19 05:39:05.588503: Current learning rate: 0.00491
2024-12-19 05:43:55.800290: Validation loss did not improve from -0.47056. Patience: 67/50
2024-12-19 05:43:55.801327: train_loss -0.8136
2024-12-19 05:43:55.802329: val_loss -0.4084
2024-12-19 05:43:55.803095: Pseudo dice [0.7122]
2024-12-19 05:43:55.803830: Epoch time: 290.22 s
2024-12-19 05:43:57.173111: 
2024-12-19 05:43:57.174481: Epoch 83
2024-12-19 05:43:57.175253: Current learning rate: 0.00484
2024-12-19 05:48:12.776706: Validation loss did not improve from -0.47056. Patience: 68/50
2024-12-19 05:48:12.777680: train_loss -0.8148
2024-12-19 05:48:12.778512: val_loss -0.4206
2024-12-19 05:48:12.779320: Pseudo dice [0.7095]
2024-12-19 05:48:12.780228: Epoch time: 255.61 s
2024-12-19 05:48:14.321572: 
2024-12-19 05:48:14.322954: Epoch 84
2024-12-19 05:48:14.323780: Current learning rate: 0.00478
2024-12-19 05:53:06.267742: Validation loss did not improve from -0.47056. Patience: 69/50
2024-12-19 05:53:06.268740: train_loss -0.8115
2024-12-19 05:53:06.270082: val_loss -0.4212
2024-12-19 05:53:06.271147: Pseudo dice [0.7153]
2024-12-19 05:53:06.272149: Epoch time: 291.95 s
2024-12-19 05:53:08.637394: 
2024-12-19 05:53:08.638977: Epoch 85
2024-12-19 05:53:08.640109: Current learning rate: 0.00471
2024-12-19 05:57:34.386775: Validation loss did not improve from -0.47056. Patience: 70/50
2024-12-19 05:57:34.387721: train_loss -0.8133
2024-12-19 05:57:34.388404: val_loss -0.4194
2024-12-19 05:57:34.389179: Pseudo dice [0.7174]
2024-12-19 05:57:34.389819: Epoch time: 265.75 s
2024-12-19 05:57:35.808931: 
2024-12-19 05:57:35.810167: Epoch 86
2024-12-19 05:57:35.810889: Current learning rate: 0.00465
2024-12-19 06:02:26.667382: Validation loss did not improve from -0.47056. Patience: 71/50
2024-12-19 06:02:26.669339: train_loss -0.8135
2024-12-19 06:02:26.670197: val_loss -0.4231
2024-12-19 06:02:26.670974: Pseudo dice [0.7159]
2024-12-19 06:02:26.671822: Epoch time: 290.86 s
2024-12-19 06:02:28.028068: 
2024-12-19 06:02:28.029289: Epoch 87
2024-12-19 06:02:28.030097: Current learning rate: 0.00458
2024-12-19 06:06:55.555505: Validation loss did not improve from -0.47056. Patience: 72/50
2024-12-19 06:06:55.556473: train_loss -0.8162
2024-12-19 06:06:55.557738: val_loss -0.403
2024-12-19 06:06:55.558881: Pseudo dice [0.7115]
2024-12-19 06:06:55.559896: Epoch time: 267.53 s
2024-12-19 06:06:56.889948: 
2024-12-19 06:06:56.891602: Epoch 88
2024-12-19 06:06:56.892724: Current learning rate: 0.00452
2024-12-19 06:11:48.035245: Validation loss did not improve from -0.47056. Patience: 73/50
2024-12-19 06:11:48.036243: train_loss -0.8172
2024-12-19 06:11:48.037270: val_loss -0.3816
2024-12-19 06:11:48.038222: Pseudo dice [0.7042]
2024-12-19 06:11:48.039238: Epoch time: 291.15 s
2024-12-19 06:11:49.393295: 
2024-12-19 06:11:49.394751: Epoch 89
2024-12-19 06:11:49.395910: Current learning rate: 0.00445
2024-12-19 06:16:48.943414: Validation loss did not improve from -0.47056. Patience: 74/50
2024-12-19 06:16:48.945019: train_loss -0.8169
2024-12-19 06:16:48.946200: val_loss -0.3965
2024-12-19 06:16:48.946819: Pseudo dice [0.7135]
2024-12-19 06:16:48.947449: Epoch time: 299.55 s
2024-12-19 06:16:50.723073: 
2024-12-19 06:16:50.724215: Epoch 90
2024-12-19 06:16:50.724910: Current learning rate: 0.00438
2024-12-19 06:21:53.197012: Validation loss did not improve from -0.47056. Patience: 75/50
2024-12-19 06:21:53.197786: train_loss -0.8168
2024-12-19 06:21:53.198481: val_loss -0.4181
2024-12-19 06:21:53.199092: Pseudo dice [0.7153]
2024-12-19 06:21:53.199740: Epoch time: 302.48 s
2024-12-19 06:21:54.554883: 
2024-12-19 06:21:54.555810: Epoch 91
2024-12-19 06:21:54.556499: Current learning rate: 0.00432
2024-12-19 06:26:43.781519: Validation loss did not improve from -0.47056. Patience: 76/50
2024-12-19 06:26:43.782782: train_loss -0.8187
2024-12-19 06:26:43.783726: val_loss -0.3744
2024-12-19 06:26:43.784418: Pseudo dice [0.7041]
2024-12-19 06:26:43.785423: Epoch time: 289.23 s
2024-12-19 06:26:45.176235: 
2024-12-19 06:26:45.177644: Epoch 92
2024-12-19 06:26:45.178462: Current learning rate: 0.00425
2024-12-19 06:31:54.905872: Validation loss did not improve from -0.47056. Patience: 77/50
2024-12-19 06:31:54.906888: train_loss -0.8197
2024-12-19 06:31:54.907847: val_loss -0.3994
2024-12-19 06:31:54.908697: Pseudo dice [0.701]
2024-12-19 06:31:54.909480: Epoch time: 309.73 s
2024-12-19 06:31:56.266879: 
2024-12-19 06:31:56.268194: Epoch 93
2024-12-19 06:31:56.269023: Current learning rate: 0.00419
2024-12-19 06:36:30.021542: Validation loss did not improve from -0.47056. Patience: 78/50
2024-12-19 06:36:30.022525: train_loss -0.8175
2024-12-19 06:36:30.023364: val_loss -0.4292
2024-12-19 06:36:30.024135: Pseudo dice [0.7174]
2024-12-19 06:36:30.024894: Epoch time: 273.76 s
2024-12-19 06:36:31.364876: 
2024-12-19 06:36:31.366208: Epoch 94
2024-12-19 06:36:31.366983: Current learning rate: 0.00412
2024-12-19 06:41:56.412586: Validation loss did not improve from -0.47056. Patience: 79/50
2024-12-19 06:41:56.413306: train_loss -0.8189
2024-12-19 06:41:56.414332: val_loss -0.3866
2024-12-19 06:41:56.415107: Pseudo dice [0.7038]
2024-12-19 06:41:56.415798: Epoch time: 325.05 s
2024-12-19 06:41:58.146824: 
2024-12-19 06:41:58.149150: Epoch 95
2024-12-19 06:41:58.150218: Current learning rate: 0.00405
2024-12-19 06:47:54.401611: Validation loss did not improve from -0.47056. Patience: 80/50
2024-12-19 06:47:54.403366: train_loss -0.8195
2024-12-19 06:47:54.404471: val_loss -0.3896
2024-12-19 06:47:54.405297: Pseudo dice [0.7166]
2024-12-19 06:47:54.405996: Epoch time: 356.26 s
2024-12-19 06:47:55.876669: 
2024-12-19 06:47:55.878023: Epoch 96
2024-12-19 06:47:55.879129: Current learning rate: 0.00399
2024-12-19 06:53:49.940436: Validation loss did not improve from -0.47056. Patience: 81/50
2024-12-19 06:53:49.941577: train_loss -0.8198
2024-12-19 06:53:49.942553: val_loss -0.3967
2024-12-19 06:53:49.943328: Pseudo dice [0.7063]
2024-12-19 06:53:49.944238: Epoch time: 354.07 s
2024-12-19 06:53:51.829845: 
2024-12-19 06:53:51.831447: Epoch 97
2024-12-19 06:53:51.832425: Current learning rate: 0.00392
2024-12-19 06:59:04.639811: Validation loss did not improve from -0.47056. Patience: 82/50
2024-12-19 06:59:04.641716: train_loss -0.8209
2024-12-19 06:59:04.642663: val_loss -0.4199
2024-12-19 06:59:04.643447: Pseudo dice [0.7177]
2024-12-19 06:59:04.644446: Epoch time: 312.81 s
2024-12-19 06:59:06.001222: 
2024-12-19 06:59:06.002904: Epoch 98
2024-12-19 06:59:06.004094: Current learning rate: 0.00385
2024-12-19 07:04:52.892489: Validation loss did not improve from -0.47056. Patience: 83/50
2024-12-19 07:04:52.894926: train_loss -0.8201
2024-12-19 07:04:52.895943: val_loss -0.3928
2024-12-19 07:04:52.896760: Pseudo dice [0.7096]
2024-12-19 07:04:52.897527: Epoch time: 346.9 s
2024-12-19 07:04:54.327871: 
2024-12-19 07:04:54.329043: Epoch 99
2024-12-19 07:04:54.329741: Current learning rate: 0.00379
2024-12-19 07:10:17.602705: Validation loss did not improve from -0.47056. Patience: 84/50
2024-12-19 07:10:17.603739: train_loss -0.8226
2024-12-19 07:10:17.604736: val_loss -0.4063
2024-12-19 07:10:17.605538: Pseudo dice [0.7129]
2024-12-19 07:10:17.606372: Epoch time: 323.28 s
2024-12-19 07:10:19.409665: 
2024-12-19 07:10:19.411157: Epoch 100
2024-12-19 07:10:19.412121: Current learning rate: 0.00372
2024-12-19 07:15:08.157210: Validation loss did not improve from -0.47056. Patience: 85/50
2024-12-19 07:15:08.158167: train_loss -0.8234
2024-12-19 07:15:08.158949: val_loss -0.4109
2024-12-19 07:15:08.159778: Pseudo dice [0.7075]
2024-12-19 07:15:08.160494: Epoch time: 288.75 s
2024-12-19 07:15:09.554593: 
2024-12-19 07:15:09.556051: Epoch 101
2024-12-19 07:15:09.556878: Current learning rate: 0.00365
2024-12-19 07:20:37.562498: Validation loss did not improve from -0.47056. Patience: 86/50
2024-12-19 07:20:37.563516: train_loss -0.8222
2024-12-19 07:20:37.564628: val_loss -0.3825
2024-12-19 07:20:37.565673: Pseudo dice [0.7058]
2024-12-19 07:20:37.566838: Epoch time: 328.01 s
2024-12-19 07:20:38.938953: 
2024-12-19 07:20:38.940171: Epoch 102
2024-12-19 07:20:38.941078: Current learning rate: 0.00359
2024-12-19 07:25:14.306317: Validation loss did not improve from -0.47056. Patience: 87/50
2024-12-19 07:25:14.307584: train_loss -0.8224
2024-12-19 07:25:14.308375: val_loss -0.3997
2024-12-19 07:25:14.309049: Pseudo dice [0.7193]
2024-12-19 07:25:14.309807: Epoch time: 275.37 s
2024-12-19 07:25:15.676898: 
2024-12-19 07:25:15.677846: Epoch 103
2024-12-19 07:25:15.678510: Current learning rate: 0.00352
2024-12-19 07:30:36.995338: Validation loss did not improve from -0.47056. Patience: 88/50
2024-12-19 07:30:36.996316: train_loss -0.8231
2024-12-19 07:30:36.997531: val_loss -0.3827
2024-12-19 07:30:36.998518: Pseudo dice [0.7101]
2024-12-19 07:30:36.999215: Epoch time: 321.32 s
2024-12-19 07:30:38.350198: 
2024-12-19 07:30:38.351933: Epoch 104
2024-12-19 07:30:38.352876: Current learning rate: 0.00345
2024-12-19 07:36:09.718428: Validation loss did not improve from -0.47056. Patience: 89/50
2024-12-19 07:36:09.719547: train_loss -0.8243
2024-12-19 07:36:09.720570: val_loss -0.4028
2024-12-19 07:36:09.721489: Pseudo dice [0.7162]
2024-12-19 07:36:09.722475: Epoch time: 331.37 s
2024-12-19 07:36:11.667032: 
2024-12-19 07:36:11.668978: Epoch 105
2024-12-19 07:36:11.670811: Current learning rate: 0.00338
2024-12-19 07:41:02.846126: Validation loss did not improve from -0.47056. Patience: 90/50
2024-12-19 07:41:02.847402: train_loss -0.8246
2024-12-19 07:41:02.848537: val_loss -0.3944
2024-12-19 07:41:02.849430: Pseudo dice [0.713]
2024-12-19 07:41:02.850168: Epoch time: 291.18 s
2024-12-19 07:41:04.237203: 
2024-12-19 07:41:04.238541: Epoch 106
2024-12-19 07:41:04.239376: Current learning rate: 0.00332
2024-12-19 07:46:32.062806: Validation loss did not improve from -0.47056. Patience: 91/50
2024-12-19 07:46:32.063806: train_loss -0.8261
2024-12-19 07:46:32.064743: val_loss -0.4133
2024-12-19 07:46:32.065434: Pseudo dice [0.7232]
2024-12-19 07:46:32.066430: Epoch time: 327.83 s
2024-12-19 07:46:34.029093: 
2024-12-19 07:46:34.030646: Epoch 107
2024-12-19 07:46:34.031521: Current learning rate: 0.00325
2024-12-19 07:51:59.730815: Validation loss did not improve from -0.47056. Patience: 92/50
2024-12-19 07:51:59.731797: train_loss -0.8261
2024-12-19 07:51:59.732638: val_loss -0.3957
2024-12-19 07:51:59.733376: Pseudo dice [0.708]
2024-12-19 07:51:59.734197: Epoch time: 325.7 s
2024-12-19 07:52:01.201367: 
2024-12-19 07:52:01.203547: Epoch 108
2024-12-19 07:52:01.204640: Current learning rate: 0.00318
2024-12-19 07:57:53.772379: Validation loss did not improve from -0.47056. Patience: 93/50
2024-12-19 07:57:53.794551: train_loss -0.8281
2024-12-19 07:57:53.797215: val_loss -0.401
2024-12-19 07:57:53.798186: Pseudo dice [0.7078]
2024-12-19 07:57:53.799418: Epoch time: 352.58 s
2024-12-19 07:57:55.235472: 
2024-12-19 07:57:55.237511: Epoch 109
2024-12-19 07:57:55.238446: Current learning rate: 0.00311
2024-12-19 08:03:30.787397: Validation loss did not improve from -0.47056. Patience: 94/50
2024-12-19 08:03:30.788391: train_loss -0.8296
2024-12-19 08:03:30.789389: val_loss -0.3968
2024-12-19 08:03:30.790249: Pseudo dice [0.7129]
2024-12-19 08:03:30.791056: Epoch time: 335.56 s
2024-12-19 08:03:32.639276: 
2024-12-19 08:03:32.641167: Epoch 110
2024-12-19 08:03:32.642271: Current learning rate: 0.00304
2024-12-19 08:08:53.761337: Validation loss did not improve from -0.47056. Patience: 95/50
2024-12-19 08:08:53.762337: train_loss -0.8294
2024-12-19 08:08:53.763633: val_loss -0.434
2024-12-19 08:08:53.764557: Pseudo dice [0.7217]
2024-12-19 08:08:53.765417: Epoch time: 321.12 s
2024-12-19 08:08:55.224362: 
2024-12-19 08:08:55.225691: Epoch 111
2024-12-19 08:08:55.226826: Current learning rate: 0.00297
2024-12-19 08:14:29.924439: Validation loss did not improve from -0.47056. Patience: 96/50
2024-12-19 08:14:29.925550: train_loss -0.8271
2024-12-19 08:14:29.926438: val_loss -0.3813
2024-12-19 08:14:29.927073: Pseudo dice [0.6979]
2024-12-19 08:14:29.927744: Epoch time: 334.7 s
2024-12-19 08:14:31.362233: 
2024-12-19 08:14:31.363526: Epoch 112
2024-12-19 08:14:31.364239: Current learning rate: 0.00291
2024-12-19 08:19:47.358166: Validation loss did not improve from -0.47056. Patience: 97/50
2024-12-19 08:19:47.359208: train_loss -0.829
2024-12-19 08:19:47.360060: val_loss -0.4006
2024-12-19 08:19:47.360827: Pseudo dice [0.7166]
2024-12-19 08:19:47.361525: Epoch time: 316.0 s
2024-12-19 08:19:48.847949: 
2024-12-19 08:19:48.849256: Epoch 113
2024-12-19 08:19:48.850066: Current learning rate: 0.00284
2024-12-19 08:25:32.022896: Validation loss did not improve from -0.47056. Patience: 98/50
2024-12-19 08:25:32.024240: train_loss -0.8292
2024-12-19 08:25:32.025055: val_loss -0.3882
2024-12-19 08:25:32.025720: Pseudo dice [0.715]
2024-12-19 08:25:32.026780: Epoch time: 343.18 s
2024-12-19 08:25:33.396814: 
2024-12-19 08:25:33.398249: Epoch 114
2024-12-19 08:25:33.399582: Current learning rate: 0.00277
2024-12-19 08:30:59.194185: Validation loss did not improve from -0.47056. Patience: 99/50
2024-12-19 08:30:59.196846: train_loss -0.8289
2024-12-19 08:30:59.197936: val_loss -0.4097
2024-12-19 08:30:59.198831: Pseudo dice [0.7082]
2024-12-19 08:30:59.199831: Epoch time: 325.8 s
2024-12-19 08:31:01.034164: 
2024-12-19 08:31:01.035447: Epoch 115
2024-12-19 08:31:01.036094: Current learning rate: 0.0027
2024-12-19 08:36:45.140068: Validation loss did not improve from -0.47056. Patience: 100/50
2024-12-19 08:36:45.141123: train_loss -0.8286
2024-12-19 08:36:45.141942: val_loss -0.4044
2024-12-19 08:36:45.143174: Pseudo dice [0.7138]
2024-12-19 08:36:45.143857: Epoch time: 344.11 s
2024-12-19 08:36:46.528293: 
2024-12-19 08:36:46.529668: Epoch 116
2024-12-19 08:36:46.530682: Current learning rate: 0.00263
2024-12-19 08:42:19.534687: Validation loss did not improve from -0.47056. Patience: 101/50
2024-12-19 08:42:19.535696: train_loss -0.8286
2024-12-19 08:42:19.536433: val_loss -0.3799
2024-12-19 08:42:19.537022: Pseudo dice [0.71]
2024-12-19 08:42:19.537660: Epoch time: 333.01 s
2024-12-19 08:42:20.999253: 
2024-12-19 08:42:21.000566: Epoch 117
2024-12-19 08:42:21.001359: Current learning rate: 0.00256
2024-12-19 08:47:42.556280: Validation loss did not improve from -0.47056. Patience: 102/50
2024-12-19 08:47:42.559123: train_loss -0.8302
2024-12-19 08:47:42.560374: val_loss -0.3901
2024-12-19 08:47:42.561321: Pseudo dice [0.7075]
2024-12-19 08:47:42.562343: Epoch time: 321.56 s
2024-12-19 08:47:44.198990: 
2024-12-19 08:47:44.200371: Epoch 118
2024-12-19 08:47:44.201551: Current learning rate: 0.00249
2024-12-19 08:53:14.974675: Validation loss did not improve from -0.47056. Patience: 103/50
2024-12-19 08:53:14.989231: train_loss -0.8306
2024-12-19 08:53:14.990856: val_loss -0.3762
2024-12-19 08:53:14.991922: Pseudo dice [0.7064]
2024-12-19 08:53:14.993271: Epoch time: 330.79 s
2024-12-19 08:53:17.125606: 
2024-12-19 08:53:17.126885: Epoch 119
2024-12-19 08:53:17.127766: Current learning rate: 0.00242
2024-12-19 08:57:24.367778: Validation loss did not improve from -0.47056. Patience: 104/50
2024-12-19 08:57:24.368879: train_loss -0.8317
2024-12-19 08:57:24.369674: val_loss -0.3738
2024-12-19 08:57:24.370392: Pseudo dice [0.7045]
2024-12-19 08:57:24.371216: Epoch time: 247.24 s
2024-12-19 08:57:26.374019: 
2024-12-19 08:57:26.375639: Epoch 120
2024-12-19 08:57:26.376676: Current learning rate: 0.00235
2024-12-19 09:00:38.023956: Validation loss did not improve from -0.47056. Patience: 105/50
2024-12-19 09:00:38.025214: train_loss -0.832
2024-12-19 09:00:38.026237: val_loss -0.4078
2024-12-19 09:00:38.026927: Pseudo dice [0.7172]
2024-12-19 09:00:38.027713: Epoch time: 191.65 s
2024-12-19 09:00:39.462323: 
2024-12-19 09:00:39.463912: Epoch 121
2024-12-19 09:00:39.464868: Current learning rate: 0.00228
2024-12-19 09:03:47.608619: Validation loss did not improve from -0.47056. Patience: 106/50
2024-12-19 09:03:47.609629: train_loss -0.8329
2024-12-19 09:03:47.611979: val_loss -0.4042
2024-12-19 09:03:47.612979: Pseudo dice [0.7223]
2024-12-19 09:03:47.614260: Epoch time: 188.15 s
2024-12-19 09:03:49.028908: 
2024-12-19 09:03:49.030301: Epoch 122
2024-12-19 09:03:49.031162: Current learning rate: 0.00221
2024-12-19 09:07:09.488699: Validation loss did not improve from -0.47056. Patience: 107/50
2024-12-19 09:07:09.489566: train_loss -0.8334
2024-12-19 09:07:09.490572: val_loss -0.3881
2024-12-19 09:07:09.491429: Pseudo dice [0.7068]
2024-12-19 09:07:09.492436: Epoch time: 200.46 s
2024-12-19 09:07:10.925593: 
2024-12-19 09:07:10.926706: Epoch 123
2024-12-19 09:07:10.927578: Current learning rate: 0.00214
2024-12-19 09:10:10.377672: Validation loss did not improve from -0.47056. Patience: 108/50
2024-12-19 09:10:10.378729: train_loss -0.8352
2024-12-19 09:10:10.379622: val_loss -0.3865
2024-12-19 09:10:10.380472: Pseudo dice [0.7084]
2024-12-19 09:10:10.381357: Epoch time: 179.45 s
2024-12-19 09:10:11.824343: 
2024-12-19 09:10:11.825650: Epoch 124
2024-12-19 09:10:11.826475: Current learning rate: 0.00207
2024-12-19 09:13:48.984226: Validation loss did not improve from -0.47056. Patience: 109/50
2024-12-19 09:13:48.985360: train_loss -0.8357
2024-12-19 09:13:48.986335: val_loss -0.3816
2024-12-19 09:13:48.987208: Pseudo dice [0.715]
2024-12-19 09:13:48.988041: Epoch time: 217.16 s
2024-12-19 09:13:50.786038: 
2024-12-19 09:13:50.787537: Epoch 125
2024-12-19 09:13:50.788411: Current learning rate: 0.00199
2024-12-19 09:17:06.635569: Validation loss did not improve from -0.47056. Patience: 110/50
2024-12-19 09:17:06.636619: train_loss -0.8341
2024-12-19 09:17:06.637824: val_loss -0.3834
2024-12-19 09:17:06.638812: Pseudo dice [0.7109]
2024-12-19 09:17:06.639700: Epoch time: 195.85 s
2024-12-19 09:17:08.079468: 
2024-12-19 09:17:08.081012: Epoch 126
2024-12-19 09:17:08.082214: Current learning rate: 0.00192
2024-12-19 09:20:23.340921: Validation loss did not improve from -0.47056. Patience: 111/50
2024-12-19 09:20:23.342280: train_loss -0.8352
2024-12-19 09:20:23.343238: val_loss -0.3796
2024-12-19 09:20:23.344017: Pseudo dice [0.7162]
2024-12-19 09:20:23.344896: Epoch time: 195.26 s
2024-12-19 09:20:24.782422: 
2024-12-19 09:20:24.783595: Epoch 127
2024-12-19 09:20:24.784608: Current learning rate: 0.00185
2024-12-19 09:23:43.465054: Validation loss did not improve from -0.47056. Patience: 112/50
2024-12-19 09:23:43.465946: train_loss -0.8346
2024-12-19 09:23:43.467152: val_loss -0.3859
2024-12-19 09:23:43.468118: Pseudo dice [0.7103]
2024-12-19 09:23:43.468973: Epoch time: 198.68 s
2024-12-19 09:23:44.928885: 
2024-12-19 09:23:44.930532: Epoch 128
2024-12-19 09:23:44.931651: Current learning rate: 0.00178
2024-12-19 09:27:07.766159: Validation loss did not improve from -0.47056. Patience: 113/50
2024-12-19 09:27:07.767512: train_loss -0.836
2024-12-19 09:27:07.768596: val_loss -0.3431
2024-12-19 09:27:07.769424: Pseudo dice [0.7027]
2024-12-19 09:27:07.770137: Epoch time: 202.84 s
2024-12-19 09:27:09.831999: 
2024-12-19 09:27:09.834447: Epoch 129
2024-12-19 09:27:09.835481: Current learning rate: 0.0017
2024-12-19 09:30:21.468349: Validation loss did not improve from -0.47056. Patience: 114/50
2024-12-19 09:30:21.469257: train_loss -0.8371
2024-12-19 09:30:21.470159: val_loss -0.3698
2024-12-19 09:30:21.470921: Pseudo dice [0.7067]
2024-12-19 09:30:21.471746: Epoch time: 191.64 s
2024-12-19 09:30:23.371269: 
2024-12-19 09:30:23.372674: Epoch 130
2024-12-19 09:30:23.373447: Current learning rate: 0.00163
2024-12-19 09:33:48.660107: Validation loss did not improve from -0.47056. Patience: 115/50
2024-12-19 09:33:48.661573: train_loss -0.838
2024-12-19 09:33:48.662367: val_loss -0.3767
2024-12-19 09:33:48.663362: Pseudo dice [0.6985]
2024-12-19 09:33:48.664245: Epoch time: 205.29 s
2024-12-19 09:33:50.042798: 
2024-12-19 09:33:50.044399: Epoch 131
2024-12-19 09:33:50.045261: Current learning rate: 0.00156
2024-12-19 09:37:08.423012: Validation loss did not improve from -0.47056. Patience: 116/50
2024-12-19 09:37:08.423949: train_loss -0.8398
2024-12-19 09:37:08.424727: val_loss -0.4148
2024-12-19 09:37:08.425384: Pseudo dice [0.7204]
2024-12-19 09:37:08.426061: Epoch time: 198.38 s
2024-12-19 09:37:09.880412: 
2024-12-19 09:37:09.881802: Epoch 132
2024-12-19 09:37:09.882572: Current learning rate: 0.00148
2024-12-19 09:40:08.560929: Validation loss did not improve from -0.47056. Patience: 117/50
2024-12-19 09:40:08.562183: train_loss -0.836
2024-12-19 09:40:08.563064: val_loss -0.3916
2024-12-19 09:40:08.564192: Pseudo dice [0.7189]
2024-12-19 09:40:08.565172: Epoch time: 178.68 s
2024-12-19 09:40:09.921590: 
2024-12-19 09:40:09.923070: Epoch 133
2024-12-19 09:40:09.923916: Current learning rate: 0.00141
2024-12-19 09:43:47.914198: Validation loss did not improve from -0.47056. Patience: 118/50
2024-12-19 09:43:47.915107: train_loss -0.8375
2024-12-19 09:43:47.915943: val_loss -0.3989
2024-12-19 09:43:47.916601: Pseudo dice [0.7213]
2024-12-19 09:43:47.917252: Epoch time: 217.99 s
2024-12-19 09:43:49.363750: 
2024-12-19 09:43:49.364884: Epoch 134
2024-12-19 09:43:49.365685: Current learning rate: 0.00133
2024-12-19 09:47:12.809409: Validation loss did not improve from -0.47056. Patience: 119/50
2024-12-19 09:47:12.810421: train_loss -0.837
2024-12-19 09:47:12.811354: val_loss -0.3863
2024-12-19 09:47:12.812116: Pseudo dice [0.7083]
2024-12-19 09:47:12.813087: Epoch time: 203.45 s
2024-12-19 09:47:14.725006: 
2024-12-19 09:47:14.726350: Epoch 135
2024-12-19 09:47:14.727044: Current learning rate: 0.00126
2024-12-19 09:50:27.720579: Validation loss did not improve from -0.47056. Patience: 120/50
2024-12-19 09:50:27.721610: train_loss -0.8381
2024-12-19 09:50:27.722539: val_loss -0.4113
2024-12-19 09:50:27.723205: Pseudo dice [0.719]
2024-12-19 09:50:27.724090: Epoch time: 193.0 s
2024-12-19 09:50:29.125368: 
2024-12-19 09:50:29.127191: Epoch 136
2024-12-19 09:50:29.128017: Current learning rate: 0.00118
2024-12-19 09:53:54.310135: Validation loss did not improve from -0.47056. Patience: 121/50
2024-12-19 09:53:54.311724: train_loss -0.8378
2024-12-19 09:53:54.313028: val_loss -0.391
2024-12-19 09:53:54.313701: Pseudo dice [0.7069]
2024-12-19 09:53:54.314405: Epoch time: 205.19 s
2024-12-19 09:53:55.714293: 
2024-12-19 09:53:55.715764: Epoch 137
2024-12-19 09:53:55.716669: Current learning rate: 0.00111
2024-12-19 09:57:15.819871: Validation loss did not improve from -0.47056. Patience: 122/50
2024-12-19 09:57:15.824301: train_loss -0.8381
2024-12-19 09:57:15.825471: val_loss -0.386
2024-12-19 09:57:15.826216: Pseudo dice [0.7096]
2024-12-19 09:57:15.826986: Epoch time: 200.11 s
2024-12-19 09:57:17.273502: 
2024-12-19 09:57:17.274755: Epoch 138
2024-12-19 09:57:17.275473: Current learning rate: 0.00103
2024-12-19 10:00:23.785923: Validation loss did not improve from -0.47056. Patience: 123/50
2024-12-19 10:00:23.786665: train_loss -0.8379
2024-12-19 10:00:23.787588: val_loss -0.392
2024-12-19 10:00:23.788478: Pseudo dice [0.7121]
2024-12-19 10:00:23.789239: Epoch time: 186.51 s
2024-12-19 10:00:25.214366: 
2024-12-19 10:00:25.215824: Epoch 139
2024-12-19 10:00:25.216844: Current learning rate: 0.00095
2024-12-19 10:03:50.981811: Validation loss did not improve from -0.47056. Patience: 124/50
2024-12-19 10:03:50.982666: train_loss -0.8397
2024-12-19 10:03:50.983509: val_loss -0.3953
2024-12-19 10:03:50.984333: Pseudo dice [0.708]
2024-12-19 10:03:50.985023: Epoch time: 205.77 s
2024-12-19 10:03:54.279563: 
2024-12-19 10:03:54.280843: Epoch 140
2024-12-19 10:03:54.281864: Current learning rate: 0.00087
2024-12-19 10:07:20.521583: Validation loss did not improve from -0.47056. Patience: 125/50
2024-12-19 10:07:20.523197: train_loss -0.8401
2024-12-19 10:07:20.524427: val_loss -0.3837
2024-12-19 10:07:20.525207: Pseudo dice [0.7061]
2024-12-19 10:07:20.526026: Epoch time: 206.24 s
2024-12-19 10:07:22.147997: 
2024-12-19 10:07:22.149987: Epoch 141
2024-12-19 10:07:22.151509: Current learning rate: 0.00079
2024-12-19 10:10:33.536719: Validation loss did not improve from -0.47056. Patience: 126/50
2024-12-19 10:10:33.537782: train_loss -0.8387
2024-12-19 10:10:33.539354: val_loss -0.3732
2024-12-19 10:10:33.540254: Pseudo dice [0.7104]
2024-12-19 10:10:33.541423: Epoch time: 191.39 s
2024-12-19 10:10:34.832490: 
2024-12-19 10:10:34.834737: Epoch 142
2024-12-19 10:10:34.836030: Current learning rate: 0.00071
2024-12-19 10:13:52.350793: Validation loss did not improve from -0.47056. Patience: 127/50
2024-12-19 10:13:52.352041: train_loss -0.8392
2024-12-19 10:13:52.353053: val_loss -0.3829
2024-12-19 10:13:52.353987: Pseudo dice [0.7072]
2024-12-19 10:13:52.354899: Epoch time: 197.52 s
2024-12-19 10:13:53.779711: 
2024-12-19 10:13:53.781253: Epoch 143
2024-12-19 10:13:53.782262: Current learning rate: 0.00063
2024-12-19 10:17:09.568947: Validation loss did not improve from -0.47056. Patience: 128/50
2024-12-19 10:17:09.569947: train_loss -0.84
2024-12-19 10:17:09.570672: val_loss -0.408
2024-12-19 10:17:09.571378: Pseudo dice [0.7161]
2024-12-19 10:17:09.572063: Epoch time: 195.79 s
2024-12-19 10:17:11.038466: 
2024-12-19 10:17:11.039771: Epoch 144
2024-12-19 10:17:11.040662: Current learning rate: 0.00055
2024-12-19 10:20:20.041705: Validation loss did not improve from -0.47056. Patience: 129/50
2024-12-19 10:20:20.042755: train_loss -0.8398
2024-12-19 10:20:20.043686: val_loss -0.3868
2024-12-19 10:20:20.044473: Pseudo dice [0.7192]
2024-12-19 10:20:20.045304: Epoch time: 189.01 s
2024-12-19 10:20:21.967529: 
2024-12-19 10:20:21.968949: Epoch 145
2024-12-19 10:20:21.970013: Current learning rate: 0.00047
2024-12-19 10:23:53.608100: Validation loss did not improve from -0.47056. Patience: 130/50
2024-12-19 10:23:53.609203: train_loss -0.8402
2024-12-19 10:23:53.610305: val_loss -0.3925
2024-12-19 10:23:53.611383: Pseudo dice [0.7132]
2024-12-19 10:23:53.612461: Epoch time: 211.64 s
2024-12-19 10:23:55.124834: 
2024-12-19 10:23:55.126302: Epoch 146
2024-12-19 10:23:55.127268: Current learning rate: 0.00038
2024-12-19 10:27:13.307933: Validation loss did not improve from -0.47056. Patience: 131/50
2024-12-19 10:27:13.308855: train_loss -0.8427
2024-12-19 10:27:13.309609: val_loss -0.3759
2024-12-19 10:27:13.310418: Pseudo dice [0.7141]
2024-12-19 10:27:13.311345: Epoch time: 198.19 s
2024-12-19 10:27:14.955119: 
2024-12-19 10:27:14.957170: Epoch 147
2024-12-19 10:27:14.958429: Current learning rate: 0.0003
2024-12-19 10:30:21.425938: Validation loss did not improve from -0.47056. Patience: 132/50
2024-12-19 10:30:21.426841: train_loss -0.8385
2024-12-19 10:30:21.427597: val_loss -0.3731
2024-12-19 10:30:21.428203: Pseudo dice [0.7048]
2024-12-19 10:30:21.428849: Epoch time: 186.47 s
2024-12-19 10:30:22.844501: 
2024-12-19 10:30:22.845697: Epoch 148
2024-12-19 10:30:22.846384: Current learning rate: 0.00021
2024-12-19 10:33:45.111130: Validation loss did not improve from -0.47056. Patience: 133/50
2024-12-19 10:33:45.114141: train_loss -0.8405
2024-12-19 10:33:45.115461: val_loss -0.3835
2024-12-19 10:33:45.116332: Pseudo dice [0.7122]
2024-12-19 10:33:45.117193: Epoch time: 202.27 s
2024-12-19 10:33:46.570843: 
2024-12-19 10:33:46.571891: Epoch 149
2024-12-19 10:33:46.572896: Current learning rate: 0.00011
2024-12-19 10:36:43.274133: Validation loss did not improve from -0.47056. Patience: 134/50
2024-12-19 10:36:43.275264: train_loss -0.8413
2024-12-19 10:36:43.276044: val_loss -0.3976
2024-12-19 10:36:43.276712: Pseudo dice [0.7107]
2024-12-19 10:36:43.277334: Epoch time: 176.71 s
2024-12-19 10:36:46.524826: Training done.
2024-12-19 10:36:47.293287: Using splits from existing split file: /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset309_Sohee_Calcium_OCT_CrossValidation/splits_final_40.json
2024-12-19 10:36:47.350109: The split file contains 5 splits.
2024-12-19 10:36:47.351800: Desired fold for training: 3
2024-12-19 10:36:47.353126: This split has 3 training and 6 validation cases.
2024-12-19 10:36:47.354646: predicting 101-044
2024-12-19 10:36:47.447959: 101-044, shape torch.Size([1, 404, 498, 498]), rank 0
2024-12-19 10:38:54.295027: predicting 101-045
2024-12-19 10:38:54.321953: 101-045, shape torch.Size([1, 375, 498, 498]), rank 0
2024-12-19 10:40:56.667192: predicting 106-002
2024-12-19 10:40:56.693052: 106-002, shape torch.Size([1, 539, 498, 498]), rank 0
2024-12-19 10:45:14.761285: predicting 401-004
2024-12-19 10:45:14.783124: 401-004, shape torch.Size([1, 375, 498, 498]), rank 0
2024-12-19 10:48:11.314409: predicting 704-003
2024-12-19 10:48:11.333033: 704-003, shape torch.Size([1, 375, 498, 498]), rank 0
2024-12-19 10:51:25.131018: predicting 706-005
2024-12-19 10:51:25.150951: 706-005, shape torch.Size([1, 375, 498, 498]), rank 0
2024-12-19 10:55:39.641699: Validation complete
2024-12-19 10:55:39.642813: Mean Validation Dice:  0.6874094485776734

############################
INFO: You are using the old nnU-Net default plans. We have updated our recommendations. Please consider using those instead! Read more here: https://github.com/MIC-DKFZ/nnUNet/blob/master/documentation/resenc_presets.md
############################

Using device: cuda:0

#######################################################################
Please cite the following paper when using nnU-Net:
Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.
#######################################################################

2024-12-19 10:55:57.267413: Using torch.compile...
/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
################### Loading pretrained weights from file  /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_results/Dataset300_Lumen_and_Wall_OCT/nnUNetTrainer__nnUNetPreprocessPlans__3d_fullres/fold_all/checkpoint_best.pth ###################
Below is the list of overlapping blocks in pretrained model and nnUNet architecture:
encoder.stages.0.0.convs.0.conv.weight shape torch.Size([32, 1, 3, 3, 3])
encoder.stages.0.0.convs.0.conv.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.norm.weight shape torch.Size([32])
encoder.stages.0.0.convs.0.norm.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.0.weight shape torch.Size([32, 1, 3, 3, 3])
encoder.stages.0.0.convs.0.all_modules.0.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.1.weight shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.1.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
encoder.stages.0.0.convs.1.conv.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.norm.weight shape torch.Size([32])
encoder.stages.0.0.convs.1.norm.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
encoder.stages.0.0.convs.1.all_modules.0.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.1.weight shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.1.bias shape torch.Size([32])
encoder.stages.1.0.convs.0.conv.weight shape torch.Size([64, 32, 3, 3, 3])
encoder.stages.1.0.convs.0.conv.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.norm.weight shape torch.Size([64])
encoder.stages.1.0.convs.0.norm.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.0.weight shape torch.Size([64, 32, 3, 3, 3])
encoder.stages.1.0.convs.0.all_modules.0.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.1.weight shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.1.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
encoder.stages.1.0.convs.1.conv.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.norm.weight shape torch.Size([64])
encoder.stages.1.0.convs.1.norm.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
encoder.stages.1.0.convs.1.all_modules.0.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.1.weight shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.1.bias shape torch.Size([64])
encoder.stages.2.0.convs.0.conv.weight shape torch.Size([128, 64, 3, 3, 3])
encoder.stages.2.0.convs.0.conv.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.norm.weight shape torch.Size([128])
encoder.stages.2.0.convs.0.norm.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.0.weight shape torch.Size([128, 64, 3, 3, 3])
encoder.stages.2.0.convs.0.all_modules.0.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.1.weight shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.1.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
encoder.stages.2.0.convs.1.conv.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.norm.weight shape torch.Size([128])
encoder.stages.2.0.convs.1.norm.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
encoder.stages.2.0.convs.1.all_modules.0.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.1.weight shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.1.bias shape torch.Size([128])
encoder.stages.3.0.convs.0.conv.weight shape torch.Size([256, 128, 3, 3, 3])
encoder.stages.3.0.convs.0.conv.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.norm.weight shape torch.Size([256])
encoder.stages.3.0.convs.0.norm.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.0.weight shape torch.Size([256, 128, 3, 3, 3])
encoder.stages.3.0.convs.0.all_modules.0.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.1.weight shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.1.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
encoder.stages.3.0.convs.1.conv.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.norm.weight shape torch.Size([256])
encoder.stages.3.0.convs.1.norm.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
encoder.stages.3.0.convs.1.all_modules.0.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.1.weight shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.1.bias shape torch.Size([256])
encoder.stages.4.0.convs.0.conv.weight shape torch.Size([320, 256, 3, 3, 3])
encoder.stages.4.0.convs.0.conv.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.norm.weight shape torch.Size([320])
encoder.stages.4.0.convs.0.norm.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.0.weight shape torch.Size([320, 256, 3, 3, 3])
encoder.stages.4.0.convs.0.all_modules.0.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.1.weight shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.1.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.4.0.convs.1.conv.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.norm.weight shape torch.Size([320])
encoder.stages.4.0.convs.1.norm.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.4.0.convs.1.all_modules.0.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.1.weight shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.1.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.0.conv.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.norm.weight shape torch.Size([320])
encoder.stages.5.0.convs.0.norm.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.0.all_modules.0.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.1.weight shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.1.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.1.conv.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.norm.weight shape torch.Size([320])
encoder.stages.5.0.convs.1.norm.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.1.all_modules.0.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.1.weight shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.0.0.convs.0.conv.weight shape torch.Size([32, 1, 3, 3, 3])
decoder.encoder.stages.0.0.convs.0.conv.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.norm.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.norm.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.0.weight shape torch.Size([32, 1, 3, 3, 3])
decoder.encoder.stages.0.0.convs.0.all_modules.0.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.1.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.1.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.encoder.stages.0.0.convs.1.conv.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.norm.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.norm.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.encoder.stages.0.0.convs.1.all_modules.0.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.1.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.1.bias shape torch.Size([32])
decoder.encoder.stages.1.0.convs.0.conv.weight shape torch.Size([64, 32, 3, 3, 3])
decoder.encoder.stages.1.0.convs.0.conv.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.norm.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.norm.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.0.weight shape torch.Size([64, 32, 3, 3, 3])
decoder.encoder.stages.1.0.convs.0.all_modules.0.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.1.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.1.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.encoder.stages.1.0.convs.1.conv.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.norm.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.norm.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.encoder.stages.1.0.convs.1.all_modules.0.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.1.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.1.bias shape torch.Size([64])
decoder.encoder.stages.2.0.convs.0.conv.weight shape torch.Size([128, 64, 3, 3, 3])
decoder.encoder.stages.2.0.convs.0.conv.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.norm.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.norm.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.0.weight shape torch.Size([128, 64, 3, 3, 3])
decoder.encoder.stages.2.0.convs.0.all_modules.0.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.1.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.1.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.encoder.stages.2.0.convs.1.conv.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.norm.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.norm.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.encoder.stages.2.0.convs.1.all_modules.0.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.1.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.1.bias shape torch.Size([128])
decoder.encoder.stages.3.0.convs.0.conv.weight shape torch.Size([256, 128, 3, 3, 3])
decoder.encoder.stages.3.0.convs.0.conv.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.norm.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.norm.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.0.weight shape torch.Size([256, 128, 3, 3, 3])
decoder.encoder.stages.3.0.convs.0.all_modules.0.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.1.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.1.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.encoder.stages.3.0.convs.1.conv.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.norm.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.norm.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.encoder.stages.3.0.convs.1.all_modules.0.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.1.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.1.bias shape torch.Size([256])
decoder.encoder.stages.4.0.convs.0.conv.weight shape torch.Size([320, 256, 3, 3, 3])
decoder.encoder.stages.4.0.convs.0.conv.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.norm.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.norm.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.0.weight shape torch.Size([320, 256, 3, 3, 3])
decoder.encoder.stages.4.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.4.0.convs.1.conv.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.norm.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.norm.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.4.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.0.conv.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.norm.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.norm.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.1.conv.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.norm.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.norm.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.stages.0.convs.0.conv.weight shape torch.Size([320, 640, 3, 3, 3])
decoder.stages.0.convs.0.conv.bias shape torch.Size([320])
decoder.stages.0.convs.0.norm.weight shape torch.Size([320])
decoder.stages.0.convs.0.norm.bias shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.0.weight shape torch.Size([320, 640, 3, 3, 3])
decoder.stages.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.stages.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.stages.0.convs.1.conv.bias shape torch.Size([320])
decoder.stages.0.convs.1.norm.weight shape torch.Size([320])
decoder.stages.0.convs.1.norm.bias shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.stages.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.stages.1.convs.0.conv.weight shape torch.Size([256, 512, 3, 3, 3])
decoder.stages.1.convs.0.conv.bias shape torch.Size([256])
decoder.stages.1.convs.0.norm.weight shape torch.Size([256])
decoder.stages.1.convs.0.norm.bias shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.0.weight shape torch.Size([256, 512, 3, 3, 3])
decoder.stages.1.convs.0.all_modules.0.bias shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.1.weight shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.1.bias shape torch.Size([256])
decoder.stages.1.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.stages.1.convs.1.conv.bias shape torch.Size([256])
decoder.stages.1.convs.1.norm.weight shape torch.Size([256])
decoder.stages.1.convs.1.norm.bias shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.stages.1.convs.1.all_modules.0.bias shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.1.weight shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.1.bias shape torch.Size([256])
decoder.stages.2.convs.0.conv.weight shape torch.Size([128, 256, 3, 3, 3])
decoder.stages.2.convs.0.conv.bias shape torch.Size([128])
decoder.stages.2.convs.0.norm.weight shape torch.Size([128])
decoder.stages.2.convs.0.norm.bias shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.0.weight shape torch.Size([128, 256, 3, 3, 3])
decoder.stages.2.convs.0.all_modules.0.bias shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.1.weight shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.1.bias shape torch.Size([128])
decoder.stages.2.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.stages.2.convs.1.conv.bias shape torch.Size([128])
decoder.stages.2.convs.1.norm.weight shape torch.Size([128])
decoder.stages.2.convs.1.norm.bias shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.stages.2.convs.1.all_modules.0.bias shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.1.weight shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.1.bias shape torch.Size([128])
decoder.stages.3.convs.0.conv.weight shape torch.Size([64, 128, 3, 3, 3])
decoder.stages.3.convs.0.conv.bias shape torch.Size([64])
decoder.stages.3.convs.0.norm.weight shape torch.Size([64])
decoder.stages.3.convs.0.norm.bias shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.0.weight shape torch.Size([64, 128, 3, 3, 3])
decoder.stages.3.convs.0.all_modules.0.bias shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.1.weight shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.1.bias shape torch.Size([64])
decoder.stages.3.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.stages.3.convs.1.conv.bias shape torch.Size([64])
decoder.stages.3.convs.1.norm.weight shape torch.Size([64])
decoder.stages.3.convs.1.norm.bias shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.stages.3.convs.1.all_modules.0.bias shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.1.weight shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.1.bias shape torch.Size([64])
decoder.stages.4.convs.0.conv.weight shape torch.Size([32, 64, 3, 3, 3])
decoder.stages.4.convs.0.conv.bias shape torch.Size([32])
decoder.stages.4.convs.0.norm.weight shape torch.Size([32])
decoder.stages.4.convs.0.norm.bias shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.0.weight shape torch.Size([32, 64, 3, 3, 3])
decoder.stages.4.convs.0.all_modules.0.bias shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.1.weight shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.1.bias shape torch.Size([32])
decoder.stages.4.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.stages.4.convs.1.conv.bias shape torch.Size([32])
decoder.stages.4.convs.1.norm.weight shape torch.Size([32])
decoder.stages.4.convs.1.norm.bias shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.stages.4.convs.1.all_modules.0.bias shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.1.weight shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.1.bias shape torch.Size([32])
decoder.transpconvs.0.weight shape torch.Size([320, 320, 1, 2, 2])
decoder.transpconvs.0.bias shape torch.Size([320])
decoder.transpconvs.1.weight shape torch.Size([320, 256, 2, 2, 2])
decoder.transpconvs.1.bias shape torch.Size([256])
decoder.transpconvs.2.weight shape torch.Size([256, 128, 2, 2, 2])
decoder.transpconvs.2.bias shape torch.Size([128])
decoder.transpconvs.3.weight shape torch.Size([128, 64, 2, 2, 2])
decoder.transpconvs.3.bias shape torch.Size([64])
decoder.transpconvs.4.weight shape torch.Size([64, 32, 2, 2, 2])
decoder.transpconvs.4.bias shape torch.Size([32])
################### Done ###################
2024-12-19 10:56:19.863896: do_dummy_2d_data_aug: True
2024-12-19 10:56:19.865830: Using splits from existing split file: /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset309_Sohee_Calcium_OCT_CrossValidation/splits_final_40.json
2024-12-19 10:56:19.868415: The split file contains 5 splits.
2024-12-19 10:56:19.869566: Desired fold for training: 4
2024-12-19 10:56:19.870576: This split has 3 training and 5 validation cases.
using pin_memory on device 0
using pin_memory on device 0

This is the configuration used by this training:
Configuration name: 3d_32x160x128_b10
 {'data_identifier': 'nnUNetPlans_3d_fullres', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 10, 'patch_size': [32, 160, 128], 'median_image_size_in_voxels': [375.0, 498.0, 498.0], 'spacing': [1.0, 1.0, 1.0], 'normalization_schemes': ['ZScoreNormalization'], 'use_mask_for_norm': [False], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'architecture': {'network_class_name': 'dynamic_network_architectures.architectures.unet.PlainConvUNet', 'arch_kwargs': {'n_stages': 6, 'features_per_stage': [32, 64, 128, 256, 320, 320], 'conv_op': 'torch.nn.modules.conv.Conv3d', 'kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]], 'strides': [[1, 1, 1], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [1, 2, 2]], 'n_conv_per_stage': [2, 2, 2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2, 2, 2], 'conv_bias': True, 'norm_op': 'torch.nn.modules.instancenorm.InstanceNorm3d', 'norm_op_kwargs': {'eps': 1e-05, 'affine': True}, 'dropout_op': None, 'dropout_op_kwargs': None, 'nonlin': 'torch.nn.LeakyReLU', 'nonlin_kwargs': {'inplace': True}, 'deep_supervision': True}, '_kw_requires_import': ['conv_op', 'norm_op', 'dropout_op', 'nonlin']}, 'batch_dice': True, 'inherits_from': '3d_fullres'} 

These are the global plan.json settings:
 {'dataset_name': 'Dataset309_Sohee_Calcium_OCT_CrossValidation', 'plans_name': 'nnUNetPlans', 'original_median_spacing_after_transp': [1.0, 1.0, 1.0], 'original_median_shape_after_transp': [375, 498, 498], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 0.4977976381778717, 'mean': 0.13507525622844696, 'median': 0.09599608182907104, 'min': 0.0, 'percentile_00_5': 0.014754901640117168, 'percentile_99_5': 0.4977976381778717, 'std': 0.12025152146816254}}} 

2024-12-19 10:56:54.382854: unpacking dataset...
2024-12-19 10:56:58.796148: unpacking done...
2024-12-19 10:56:59.287766: Unable to plot network architecture: nnUNet_compile is enabled!
2024-12-19 10:56:59.399636: 
2024-12-19 10:56:59.400872: Epoch 0
2024-12-19 10:56:59.401789: Current learning rate: 0.01
2024-12-19 11:00:18.827424: Validation loss improved from 1000.00000 to -0.41610! Patience: 0/50
2024-12-19 11:00:18.828424: train_loss -0.302
2024-12-19 11:00:18.829690: val_loss -0.4161
2024-12-19 11:00:18.830474: Pseudo dice [0.6775]
2024-12-19 11:00:18.831247: Epoch time: 199.43 s
2024-12-19 11:00:18.832033: Yayy! New best EMA pseudo Dice: 0.6775
2024-12-19 11:00:23.220409: 
2024-12-19 11:00:23.221874: Epoch 1
2024-12-19 11:00:23.222921: Current learning rate: 0.00994
2024-12-19 11:01:58.761552: Validation loss improved from -0.41610 to -0.43832! Patience: 0/50
2024-12-19 11:01:58.762544: train_loss -0.4793
2024-12-19 11:01:58.763510: val_loss -0.4383
2024-12-19 11:01:58.764202: Pseudo dice [0.6926]
2024-12-19 11:01:58.764832: Epoch time: 95.54 s
2024-12-19 11:01:58.765490: Yayy! New best EMA pseudo Dice: 0.679
2024-12-19 11:02:00.447277: 
2024-12-19 11:02:00.448944: Epoch 2
2024-12-19 11:02:00.449866: Current learning rate: 0.00988
2024-12-19 11:03:36.203442: Validation loss improved from -0.43832 to -0.49025! Patience: 0/50
2024-12-19 11:03:36.204399: train_loss -0.5268
2024-12-19 11:03:36.205280: val_loss -0.4902
2024-12-19 11:03:36.206103: Pseudo dice [0.7185]
2024-12-19 11:03:36.206905: Epoch time: 95.76 s
2024-12-19 11:03:36.207658: Yayy! New best EMA pseudo Dice: 0.6829
2024-12-19 11:03:38.134220: 
2024-12-19 11:03:38.136164: Epoch 3
2024-12-19 11:03:38.136955: Current learning rate: 0.00982
2024-12-19 11:05:11.251733: Validation loss improved from -0.49025 to -0.49980! Patience: 0/50
2024-12-19 11:05:11.252957: train_loss -0.5507
2024-12-19 11:05:11.253913: val_loss -0.4998
2024-12-19 11:05:11.254732: Pseudo dice [0.7236]
2024-12-19 11:05:11.255596: Epoch time: 93.12 s
2024-12-19 11:05:11.256357: Yayy! New best EMA pseudo Dice: 0.687
2024-12-19 11:05:13.470041: 
2024-12-19 11:05:13.471218: Epoch 4
2024-12-19 11:05:13.472230: Current learning rate: 0.00976
2024-12-19 11:06:44.203876: Validation loss did not improve from -0.49980. Patience: 1/50
2024-12-19 11:06:44.205160: train_loss -0.5716
2024-12-19 11:06:44.206209: val_loss -0.4744
2024-12-19 11:06:44.207055: Pseudo dice [0.7108]
2024-12-19 11:06:44.207777: Epoch time: 90.74 s
2024-12-19 11:06:44.721069: Yayy! New best EMA pseudo Dice: 0.6894
2024-12-19 11:06:46.458516: 
2024-12-19 11:06:46.460336: Epoch 5
2024-12-19 11:06:46.461439: Current learning rate: 0.0097
2024-12-19 11:08:18.102673: Validation loss did not improve from -0.49980. Patience: 2/50
2024-12-19 11:08:18.103893: train_loss -0.5903
2024-12-19 11:08:18.105015: val_loss -0.4899
2024-12-19 11:08:18.105779: Pseudo dice [0.7113]
2024-12-19 11:08:18.106539: Epoch time: 91.65 s
2024-12-19 11:08:18.107369: Yayy! New best EMA pseudo Dice: 0.6916
2024-12-19 11:08:19.815126: 
2024-12-19 11:08:19.816036: Epoch 6
2024-12-19 11:08:19.817025: Current learning rate: 0.00964
2024-12-19 11:09:53.316723: Validation loss did not improve from -0.49980. Patience: 3/50
2024-12-19 11:09:53.317949: train_loss -0.5944
2024-12-19 11:09:53.318955: val_loss -0.4831
2024-12-19 11:09:53.319671: Pseudo dice [0.7192]
2024-12-19 11:09:53.320448: Epoch time: 93.5 s
2024-12-19 11:09:53.321266: Yayy! New best EMA pseudo Dice: 0.6943
2024-12-19 11:09:55.056311: 
2024-12-19 11:09:55.058002: Epoch 7
2024-12-19 11:09:55.058807: Current learning rate: 0.00958
2024-12-19 11:11:30.294374: Validation loss improved from -0.49980 to -0.50381! Patience: 3/50
2024-12-19 11:11:30.295489: train_loss -0.6086
2024-12-19 11:11:30.296468: val_loss -0.5038
2024-12-19 11:11:30.297333: Pseudo dice [0.7295]
2024-12-19 11:11:30.298166: Epoch time: 95.24 s
2024-12-19 11:11:30.299006: Yayy! New best EMA pseudo Dice: 0.6979
2024-12-19 11:11:32.079728: 
2024-12-19 11:11:32.081143: Epoch 8
2024-12-19 11:11:32.082002: Current learning rate: 0.00952
2024-12-19 11:13:03.189622: Validation loss did not improve from -0.50381. Patience: 1/50
2024-12-19 11:13:03.190803: train_loss -0.6101
2024-12-19 11:13:03.191843: val_loss -0.4916
2024-12-19 11:13:03.192608: Pseudo dice [0.7191]
2024-12-19 11:13:03.193426: Epoch time: 91.11 s
2024-12-19 11:13:03.194149: Yayy! New best EMA pseudo Dice: 0.7
2024-12-19 11:13:05.364563: 
2024-12-19 11:13:05.366200: Epoch 9
2024-12-19 11:13:05.367231: Current learning rate: 0.00946
2024-12-19 11:14:59.277619: Validation loss did not improve from -0.50381. Patience: 2/50
2024-12-19 11:14:59.278514: train_loss -0.6258
2024-12-19 11:14:59.279407: val_loss -0.4716
2024-12-19 11:14:59.280146: Pseudo dice [0.7116]
2024-12-19 11:14:59.281343: Epoch time: 113.91 s
2024-12-19 11:14:59.669531: Yayy! New best EMA pseudo Dice: 0.7011
2024-12-19 11:15:01.365043: 
2024-12-19 11:15:01.366724: Epoch 10
2024-12-19 11:15:01.367976: Current learning rate: 0.0094
2024-12-19 11:17:45.853402: Validation loss did not improve from -0.50381. Patience: 3/50
2024-12-19 11:17:45.854339: train_loss -0.6363
2024-12-19 11:17:45.855150: val_loss -0.4852
2024-12-19 11:17:45.855856: Pseudo dice [0.7097]
2024-12-19 11:17:45.856550: Epoch time: 164.49 s
2024-12-19 11:17:45.857395: Yayy! New best EMA pseudo Dice: 0.702
2024-12-19 11:17:47.680835: 
2024-12-19 11:17:47.682142: Epoch 11
2024-12-19 11:17:47.682853: Current learning rate: 0.00934
2024-12-19 11:21:04.472522: Validation loss did not improve from -0.50381. Patience: 4/50
2024-12-19 11:21:04.473452: train_loss -0.6338
2024-12-19 11:21:04.474719: val_loss -0.4982
2024-12-19 11:21:04.475759: Pseudo dice [0.7196]
2024-12-19 11:21:04.476876: Epoch time: 196.79 s
2024-12-19 11:21:04.478088: Yayy! New best EMA pseudo Dice: 0.7038
2024-12-19 11:21:06.279498: 
2024-12-19 11:21:06.281084: Epoch 12
2024-12-19 11:21:06.282120: Current learning rate: 0.00928
2024-12-19 11:24:13.890048: Validation loss improved from -0.50381 to -0.55624! Patience: 4/50
2024-12-19 11:24:13.891026: train_loss -0.6424
2024-12-19 11:24:13.891842: val_loss -0.5562
2024-12-19 11:24:13.892533: Pseudo dice [0.7465]
2024-12-19 11:24:13.893307: Epoch time: 187.61 s
2024-12-19 11:24:13.894087: Yayy! New best EMA pseudo Dice: 0.708
2024-12-19 11:24:15.773830: 
2024-12-19 11:24:15.775112: Epoch 13
2024-12-19 11:24:15.775843: Current learning rate: 0.00922
2024-12-19 11:27:02.829585: Validation loss did not improve from -0.55624. Patience: 1/50
2024-12-19 11:27:02.830891: train_loss -0.6554
2024-12-19 11:27:02.832063: val_loss -0.5047
2024-12-19 11:27:02.833141: Pseudo dice [0.7254]
2024-12-19 11:27:02.834286: Epoch time: 167.06 s
2024-12-19 11:27:02.835480: Yayy! New best EMA pseudo Dice: 0.7098
2024-12-19 11:27:04.650071: 
2024-12-19 11:27:04.651579: Epoch 14
2024-12-19 11:27:04.652811: Current learning rate: 0.00916
2024-12-19 11:30:06.846678: Validation loss did not improve from -0.55624. Patience: 2/50
2024-12-19 11:30:06.847759: train_loss -0.6563
2024-12-19 11:30:06.848660: val_loss -0.5061
2024-12-19 11:30:06.849331: Pseudo dice [0.723]
2024-12-19 11:30:06.850005: Epoch time: 182.2 s
2024-12-19 11:30:08.178777: Yayy! New best EMA pseudo Dice: 0.7111
2024-12-19 11:30:10.566731: 
2024-12-19 11:30:10.568053: Epoch 15
2024-12-19 11:30:10.568764: Current learning rate: 0.0091
2024-12-19 11:32:59.972456: Validation loss did not improve from -0.55624. Patience: 3/50
2024-12-19 11:32:59.973429: train_loss -0.6649
2024-12-19 11:32:59.974215: val_loss -0.5082
2024-12-19 11:32:59.974960: Pseudo dice [0.7281]
2024-12-19 11:32:59.975646: Epoch time: 169.41 s
2024-12-19 11:32:59.976413: Yayy! New best EMA pseudo Dice: 0.7128
2024-12-19 11:33:01.715705: 
2024-12-19 11:33:01.717116: Epoch 16
2024-12-19 11:33:01.717881: Current learning rate: 0.00903
2024-12-19 11:36:34.599134: Validation loss did not improve from -0.55624. Patience: 4/50
2024-12-19 11:36:34.600538: train_loss -0.6653
2024-12-19 11:36:34.601286: val_loss -0.5363
2024-12-19 11:36:34.601947: Pseudo dice [0.743]
2024-12-19 11:36:34.602606: Epoch time: 212.89 s
2024-12-19 11:36:34.603281: Yayy! New best EMA pseudo Dice: 0.7158
2024-12-19 11:36:36.482743: 
2024-12-19 11:36:36.484183: Epoch 17
2024-12-19 11:36:36.485218: Current learning rate: 0.00897
2024-12-19 11:39:50.575637: Validation loss did not improve from -0.55624. Patience: 5/50
2024-12-19 11:39:50.576752: train_loss -0.6785
2024-12-19 11:39:50.577531: val_loss -0.5085
2024-12-19 11:39:50.578447: Pseudo dice [0.7328]
2024-12-19 11:39:50.579175: Epoch time: 194.1 s
2024-12-19 11:39:50.579803: Yayy! New best EMA pseudo Dice: 0.7175
2024-12-19 11:39:52.797889: 
2024-12-19 11:39:52.799273: Epoch 18
2024-12-19 11:39:52.800095: Current learning rate: 0.00891
2024-12-19 11:42:44.523928: Validation loss did not improve from -0.55624. Patience: 6/50
2024-12-19 11:42:44.524952: train_loss -0.6826
2024-12-19 11:42:44.526015: val_loss -0.5434
2024-12-19 11:42:44.527117: Pseudo dice [0.743]
2024-12-19 11:42:44.528011: Epoch time: 171.73 s
2024-12-19 11:42:44.528964: Yayy! New best EMA pseudo Dice: 0.7201
2024-12-19 11:42:46.347742: 
2024-12-19 11:42:46.348831: Epoch 19
2024-12-19 11:42:46.349845: Current learning rate: 0.00885
2024-12-19 11:46:29.317374: Validation loss did not improve from -0.55624. Patience: 7/50
2024-12-19 11:46:29.318439: train_loss -0.6854
2024-12-19 11:46:29.319364: val_loss -0.5131
2024-12-19 11:46:29.320041: Pseudo dice [0.7266]
2024-12-19 11:46:29.320724: Epoch time: 222.97 s
2024-12-19 11:46:29.729255: Yayy! New best EMA pseudo Dice: 0.7207
2024-12-19 11:46:31.652337: 
2024-12-19 11:46:31.653669: Epoch 20
2024-12-19 11:46:31.654401: Current learning rate: 0.00879
2024-12-19 11:49:53.725731: Validation loss did not improve from -0.55624. Patience: 8/50
2024-12-19 11:49:53.726799: train_loss -0.6868
2024-12-19 11:49:53.727804: val_loss -0.536
2024-12-19 11:49:53.728570: Pseudo dice [0.7435]
2024-12-19 11:49:53.729328: Epoch time: 202.08 s
2024-12-19 11:49:53.729996: Yayy! New best EMA pseudo Dice: 0.723
2024-12-19 11:49:55.619664: 
2024-12-19 11:49:55.620993: Epoch 21
2024-12-19 11:49:55.621754: Current learning rate: 0.00873
2024-12-19 11:52:48.156991: Validation loss did not improve from -0.55624. Patience: 9/50
2024-12-19 11:52:48.158037: train_loss -0.6799
2024-12-19 11:52:48.158835: val_loss -0.4784
2024-12-19 11:52:48.159463: Pseudo dice [0.7169]
2024-12-19 11:52:48.160218: Epoch time: 172.54 s
2024-12-19 11:52:49.508147: 
2024-12-19 11:52:49.509750: Epoch 22
2024-12-19 11:52:49.510714: Current learning rate: 0.00867
2024-12-19 11:56:29.372792: Validation loss did not improve from -0.55624. Patience: 10/50
2024-12-19 11:56:29.373901: train_loss -0.6893
2024-12-19 11:56:29.374993: val_loss -0.5189
2024-12-19 11:56:29.375860: Pseudo dice [0.7232]
2024-12-19 11:56:29.376679: Epoch time: 219.87 s
2024-12-19 11:56:30.752276: 
2024-12-19 11:56:30.753477: Epoch 23
2024-12-19 11:56:30.754373: Current learning rate: 0.00861
2024-12-19 12:00:00.190716: Validation loss did not improve from -0.55624. Patience: 11/50
2024-12-19 12:00:00.191720: train_loss -0.6936
2024-12-19 12:00:00.192664: val_loss -0.5361
2024-12-19 12:00:00.193585: Pseudo dice [0.7405]
2024-12-19 12:00:00.194452: Epoch time: 209.44 s
2024-12-19 12:00:00.195227: Yayy! New best EMA pseudo Dice: 0.7243
2024-12-19 12:00:02.030466: 
2024-12-19 12:00:02.031903: Epoch 24
2024-12-19 12:00:02.032923: Current learning rate: 0.00855
2024-12-19 12:02:57.712173: Validation loss improved from -0.55624 to -0.56003! Patience: 11/50
2024-12-19 12:02:57.725134: train_loss -0.6988
2024-12-19 12:02:57.727437: val_loss -0.56
2024-12-19 12:02:57.728451: Pseudo dice [0.7507]
2024-12-19 12:02:57.729745: Epoch time: 175.69 s
2024-12-19 12:02:58.238393: Yayy! New best EMA pseudo Dice: 0.7269
2024-12-19 12:03:00.162448: 
2024-12-19 12:03:00.164328: Epoch 25
2024-12-19 12:03:00.165499: Current learning rate: 0.00849
2024-12-19 12:06:21.794254: Validation loss did not improve from -0.56003. Patience: 1/50
2024-12-19 12:06:21.795655: train_loss -0.7052
2024-12-19 12:06:21.797022: val_loss -0.5187
2024-12-19 12:06:21.797720: Pseudo dice [0.7322]
2024-12-19 12:06:21.798529: Epoch time: 201.63 s
2024-12-19 12:06:21.799187: Yayy! New best EMA pseudo Dice: 0.7274
2024-12-19 12:06:23.945985: 
2024-12-19 12:06:23.946954: Epoch 26
2024-12-19 12:06:23.947740: Current learning rate: 0.00843
2024-12-19 12:08:54.692494: Validation loss did not improve from -0.56003. Patience: 2/50
2024-12-19 12:08:54.693780: train_loss -0.703
2024-12-19 12:08:54.694960: val_loss -0.4993
2024-12-19 12:08:54.695873: Pseudo dice [0.736]
2024-12-19 12:08:54.696862: Epoch time: 150.75 s
2024-12-19 12:08:54.697736: Yayy! New best EMA pseudo Dice: 0.7283
2024-12-19 12:08:56.453628: 
2024-12-19 12:08:56.454715: Epoch 27
2024-12-19 12:08:56.455615: Current learning rate: 0.00836
2024-12-19 12:12:39.458018: Validation loss did not improve from -0.56003. Patience: 3/50
2024-12-19 12:12:39.459009: train_loss -0.7057
2024-12-19 12:12:39.459833: val_loss -0.541
2024-12-19 12:12:39.460673: Pseudo dice [0.7432]
2024-12-19 12:12:39.461449: Epoch time: 223.01 s
2024-12-19 12:12:39.462098: Yayy! New best EMA pseudo Dice: 0.7298
2024-12-19 12:12:41.246988: 
2024-12-19 12:12:41.248301: Epoch 28
2024-12-19 12:12:41.249008: Current learning rate: 0.0083
2024-12-19 12:16:11.066661: Validation loss did not improve from -0.56003. Patience: 4/50
2024-12-19 12:16:11.067712: train_loss -0.7082
2024-12-19 12:16:11.068476: val_loss -0.5254
2024-12-19 12:16:11.069250: Pseudo dice [0.7323]
2024-12-19 12:16:11.069968: Epoch time: 209.82 s
2024-12-19 12:16:11.070768: Yayy! New best EMA pseudo Dice: 0.73
2024-12-19 12:16:13.261191: 
2024-12-19 12:16:13.262557: Epoch 29
2024-12-19 12:16:13.263324: Current learning rate: 0.00824
2024-12-19 12:19:10.676154: Validation loss did not improve from -0.56003. Patience: 5/50
2024-12-19 12:19:10.677143: train_loss -0.7122
2024-12-19 12:19:10.678025: val_loss -0.5219
2024-12-19 12:19:10.678786: Pseudo dice [0.7369]
2024-12-19 12:19:10.679475: Epoch time: 177.42 s
2024-12-19 12:19:11.195098: Yayy! New best EMA pseudo Dice: 0.7307
2024-12-19 12:19:12.942212: 
2024-12-19 12:19:12.943239: Epoch 30
2024-12-19 12:19:12.944083: Current learning rate: 0.00818
2024-12-19 12:22:39.906835: Validation loss did not improve from -0.56003. Patience: 6/50
2024-12-19 12:22:39.908075: train_loss -0.7143
2024-12-19 12:22:39.908889: val_loss -0.4959
2024-12-19 12:22:39.909634: Pseudo dice [0.7256]
2024-12-19 12:22:39.910346: Epoch time: 206.97 s
2024-12-19 12:22:41.368070: 
2024-12-19 12:22:41.369418: Epoch 31
2024-12-19 12:22:41.370285: Current learning rate: 0.00812
2024-12-19 12:25:20.382788: Validation loss did not improve from -0.56003. Patience: 7/50
2024-12-19 12:25:20.383623: train_loss -0.7161
2024-12-19 12:25:20.384386: val_loss -0.5402
2024-12-19 12:25:20.385048: Pseudo dice [0.7466]
2024-12-19 12:25:20.385845: Epoch time: 159.02 s
2024-12-19 12:25:20.386580: Yayy! New best EMA pseudo Dice: 0.7319
2024-12-19 12:25:22.157762: 
2024-12-19 12:25:22.159162: Epoch 32
2024-12-19 12:25:22.160077: Current learning rate: 0.00806
2024-12-19 12:28:55.991385: Validation loss did not improve from -0.56003. Patience: 8/50
2024-12-19 12:28:55.992513: train_loss -0.7124
2024-12-19 12:28:55.993576: val_loss -0.5215
2024-12-19 12:28:55.994359: Pseudo dice [0.724]
2024-12-19 12:28:55.995052: Epoch time: 213.84 s
2024-12-19 12:28:57.463008: 
2024-12-19 12:28:57.464636: Epoch 33
2024-12-19 12:28:57.465434: Current learning rate: 0.008
2024-12-19 12:32:24.805117: Validation loss did not improve from -0.56003. Patience: 9/50
2024-12-19 12:32:24.806203: train_loss -0.7151
2024-12-19 12:32:24.807480: val_loss -0.5442
2024-12-19 12:32:24.808463: Pseudo dice [0.7449]
2024-12-19 12:32:24.809356: Epoch time: 207.35 s
2024-12-19 12:32:24.810149: Yayy! New best EMA pseudo Dice: 0.7325
2024-12-19 12:32:26.663853: 
2024-12-19 12:32:26.665420: Epoch 34
2024-12-19 12:32:26.666508: Current learning rate: 0.00793
2024-12-19 12:35:32.956811: Validation loss did not improve from -0.56003. Patience: 10/50
2024-12-19 12:35:32.957578: train_loss -0.719
2024-12-19 12:35:32.958666: val_loss -0.5343
2024-12-19 12:35:32.959502: Pseudo dice [0.745]
2024-12-19 12:35:32.960245: Epoch time: 186.29 s
2024-12-19 12:35:33.351873: Yayy! New best EMA pseudo Dice: 0.7337
2024-12-19 12:35:35.100038: 
2024-12-19 12:35:35.101423: Epoch 35
2024-12-19 12:35:35.102204: Current learning rate: 0.00787
2024-12-19 12:38:56.793140: Validation loss did not improve from -0.56003. Patience: 11/50
2024-12-19 12:38:56.794155: train_loss -0.724
2024-12-19 12:38:56.795075: val_loss -0.5361
2024-12-19 12:38:56.795838: Pseudo dice [0.7452]
2024-12-19 12:38:56.796556: Epoch time: 201.7 s
2024-12-19 12:38:56.797308: Yayy! New best EMA pseudo Dice: 0.7349
2024-12-19 12:38:58.705780: 
2024-12-19 12:38:58.707619: Epoch 36
2024-12-19 12:38:58.708830: Current learning rate: 0.00781
2024-12-19 12:42:08.126070: Validation loss did not improve from -0.56003. Patience: 12/50
2024-12-19 12:42:08.127647: train_loss -0.7226
2024-12-19 12:42:08.128779: val_loss -0.5127
2024-12-19 12:42:08.129535: Pseudo dice [0.7316]
2024-12-19 12:42:08.130386: Epoch time: 189.42 s
2024-12-19 12:42:09.536335: 
2024-12-19 12:42:09.537537: Epoch 37
2024-12-19 12:42:09.538356: Current learning rate: 0.00775
2024-12-19 12:45:35.411753: Validation loss did not improve from -0.56003. Patience: 13/50
2024-12-19 12:45:35.412816: train_loss -0.7261
2024-12-19 12:45:35.413711: val_loss -0.5501
2024-12-19 12:45:35.414516: Pseudo dice [0.7513]
2024-12-19 12:45:35.415087: Epoch time: 205.88 s
2024-12-19 12:45:35.415662: Yayy! New best EMA pseudo Dice: 0.7362
2024-12-19 12:45:37.223902: 
2024-12-19 12:45:37.225161: Epoch 38
2024-12-19 12:45:37.225883: Current learning rate: 0.00769
2024-12-19 12:48:59.214635: Validation loss did not improve from -0.56003. Patience: 14/50
2024-12-19 12:48:59.215565: train_loss -0.7259
2024-12-19 12:48:59.216270: val_loss -0.5073
2024-12-19 12:48:59.216949: Pseudo dice [0.7321]
2024-12-19 12:48:59.217724: Epoch time: 201.99 s
2024-12-19 12:49:01.162473: 
2024-12-19 12:49:01.163279: Epoch 39
2024-12-19 12:49:01.163965: Current learning rate: 0.00763
2024-12-19 12:52:04.695949: Validation loss did not improve from -0.56003. Patience: 15/50
2024-12-19 12:52:04.697027: train_loss -0.7327
2024-12-19 12:52:04.697741: val_loss -0.524
2024-12-19 12:52:04.698401: Pseudo dice [0.7413]
2024-12-19 12:52:04.699030: Epoch time: 183.54 s
2024-12-19 12:52:05.151335: Yayy! New best EMA pseudo Dice: 0.7364
2024-12-19 12:52:06.936642: 
2024-12-19 12:52:06.938241: Epoch 40
2024-12-19 12:52:06.939016: Current learning rate: 0.00756
2024-12-19 12:55:52.324451: Validation loss did not improve from -0.56003. Patience: 16/50
2024-12-19 12:55:52.327130: train_loss -0.7347
2024-12-19 12:55:52.330003: val_loss -0.5229
2024-12-19 12:55:52.330995: Pseudo dice [0.7301]
2024-12-19 12:55:52.332323: Epoch time: 225.39 s
2024-12-19 12:55:53.827080: 
2024-12-19 12:55:53.828572: Epoch 41
2024-12-19 12:55:53.829829: Current learning rate: 0.0075
2024-12-19 12:59:42.194578: Validation loss did not improve from -0.56003. Patience: 17/50
2024-12-19 12:59:42.195606: train_loss -0.7369
2024-12-19 12:59:42.196520: val_loss -0.5374
2024-12-19 12:59:42.197249: Pseudo dice [0.7493]
2024-12-19 12:59:42.197927: Epoch time: 228.37 s
2024-12-19 12:59:42.198573: Yayy! New best EMA pseudo Dice: 0.7371
2024-12-19 12:59:44.584443: 
2024-12-19 12:59:44.585768: Epoch 42
2024-12-19 12:59:44.586508: Current learning rate: 0.00744
2024-12-19 13:02:51.092757: Validation loss did not improve from -0.56003. Patience: 18/50
2024-12-19 13:02:51.093744: train_loss -0.7369
2024-12-19 13:02:51.094815: val_loss -0.5472
2024-12-19 13:02:51.095723: Pseudo dice [0.7517]
2024-12-19 13:02:51.097132: Epoch time: 186.51 s
2024-12-19 13:02:51.098113: Yayy! New best EMA pseudo Dice: 0.7385
2024-12-19 13:02:52.774124: 
2024-12-19 13:02:52.775267: Epoch 43
2024-12-19 13:02:52.776042: Current learning rate: 0.00738
2024-12-19 13:06:41.981300: Validation loss did not improve from -0.56003. Patience: 19/50
2024-12-19 13:06:41.982013: train_loss -0.7407
2024-12-19 13:06:41.982914: val_loss -0.4912
2024-12-19 13:06:41.983856: Pseudo dice [0.7225]
2024-12-19 13:06:41.984766: Epoch time: 229.21 s
2024-12-19 13:06:43.352368: 
2024-12-19 13:06:43.353553: Epoch 44
2024-12-19 13:06:43.354403: Current learning rate: 0.00732
2024-12-19 13:09:25.934824: Validation loss did not improve from -0.56003. Patience: 20/50
2024-12-19 13:09:26.002758: train_loss -0.7433
2024-12-19 13:09:26.004062: val_loss -0.5405
2024-12-19 13:09:26.004811: Pseudo dice [0.7486]
2024-12-19 13:09:26.005650: Epoch time: 162.65 s
2024-12-19 13:09:28.023606: 
2024-12-19 13:09:28.025337: Epoch 45
2024-12-19 13:09:28.026076: Current learning rate: 0.00725
2024-12-19 13:13:36.685636: Validation loss did not improve from -0.56003. Patience: 21/50
2024-12-19 13:13:36.686600: train_loss -0.7458
2024-12-19 13:13:36.687730: val_loss -0.5021
2024-12-19 13:13:36.688614: Pseudo dice [0.7311]
2024-12-19 13:13:36.689347: Epoch time: 248.66 s
2024-12-19 13:13:38.186196: 
2024-12-19 13:13:38.187401: Epoch 46
2024-12-19 13:13:38.188082: Current learning rate: 0.00719
2024-12-19 13:17:29.633651: Validation loss did not improve from -0.56003. Patience: 22/50
2024-12-19 13:17:29.634698: train_loss -0.7485
2024-12-19 13:17:29.635519: val_loss -0.5282
2024-12-19 13:17:29.636190: Pseudo dice [0.7447]
2024-12-19 13:17:29.636916: Epoch time: 231.45 s
2024-12-19 13:17:31.041310: 
2024-12-19 13:17:31.042717: Epoch 47
2024-12-19 13:17:31.043606: Current learning rate: 0.00713
2024-12-19 13:20:26.928155: Validation loss did not improve from -0.56003. Patience: 23/50
2024-12-19 13:20:26.929090: train_loss -0.7481
2024-12-19 13:20:26.930048: val_loss -0.5395
2024-12-19 13:20:26.930885: Pseudo dice [0.7481]
2024-12-19 13:20:26.931687: Epoch time: 175.89 s
2024-12-19 13:20:26.932389: Yayy! New best EMA pseudo Dice: 0.7391
2024-12-19 13:20:28.676913: 
2024-12-19 13:20:28.678755: Epoch 48
2024-12-19 13:20:28.679927: Current learning rate: 0.00707
2024-12-19 13:23:55.912528: Validation loss did not improve from -0.56003. Patience: 24/50
2024-12-19 13:23:55.913846: train_loss -0.7488
2024-12-19 13:23:55.914763: val_loss -0.5237
2024-12-19 13:23:55.915593: Pseudo dice [0.739]
2024-12-19 13:23:55.916437: Epoch time: 207.24 s
2024-12-19 13:23:57.368032: 
2024-12-19 13:23:57.369082: Epoch 49
2024-12-19 13:23:57.369760: Current learning rate: 0.007
2024-12-19 13:26:58.419265: Validation loss did not improve from -0.56003. Patience: 25/50
2024-12-19 13:26:58.420017: train_loss -0.748
2024-12-19 13:26:58.421341: val_loss -0.522
2024-12-19 13:26:58.422084: Pseudo dice [0.7398]
2024-12-19 13:26:58.422957: Epoch time: 181.05 s
2024-12-19 13:26:59.179362: Yayy! New best EMA pseudo Dice: 0.7392
2024-12-19 13:27:00.947426: 
2024-12-19 13:27:00.948737: Epoch 50
2024-12-19 13:27:00.949723: Current learning rate: 0.00694
2024-12-19 13:30:50.386666: Validation loss did not improve from -0.56003. Patience: 26/50
2024-12-19 13:30:50.387676: train_loss -0.7512
2024-12-19 13:30:50.388450: val_loss -0.5396
2024-12-19 13:30:50.389421: Pseudo dice [0.7469]
2024-12-19 13:30:50.390167: Epoch time: 229.44 s
2024-12-19 13:30:50.390846: Yayy! New best EMA pseudo Dice: 0.74
2024-12-19 13:30:53.109960: 
2024-12-19 13:30:53.111619: Epoch 51
2024-12-19 13:30:53.112594: Current learning rate: 0.00688
2024-12-19 13:34:29.700883: Validation loss did not improve from -0.56003. Patience: 27/50
2024-12-19 13:34:29.701953: train_loss -0.7482
2024-12-19 13:34:29.702732: val_loss -0.5457
2024-12-19 13:34:29.703486: Pseudo dice [0.7459]
2024-12-19 13:34:29.704300: Epoch time: 216.59 s
2024-12-19 13:34:29.705129: Yayy! New best EMA pseudo Dice: 0.7406
2024-12-19 13:34:31.446859: 
2024-12-19 13:34:31.448149: Epoch 52
2024-12-19 13:34:31.448869: Current learning rate: 0.00682
2024-12-19 13:37:41.488116: Validation loss did not improve from -0.56003. Patience: 28/50
2024-12-19 13:37:41.489238: train_loss -0.7545
2024-12-19 13:37:41.490250: val_loss -0.5222
2024-12-19 13:37:41.491097: Pseudo dice [0.7354]
2024-12-19 13:37:41.491868: Epoch time: 190.04 s
2024-12-19 13:37:42.852486: 
2024-12-19 13:37:42.853734: Epoch 53
2024-12-19 13:37:42.854560: Current learning rate: 0.00675
2024-12-19 13:41:00.636039: Validation loss did not improve from -0.56003. Patience: 29/50
2024-12-19 13:41:00.637041: train_loss -0.7549
2024-12-19 13:41:00.637953: val_loss -0.5381
2024-12-19 13:41:00.638849: Pseudo dice [0.7543]
2024-12-19 13:41:00.639659: Epoch time: 197.79 s
2024-12-19 13:41:00.640318: Yayy! New best EMA pseudo Dice: 0.7415
2024-12-19 13:41:02.573178: 
2024-12-19 13:41:02.574311: Epoch 54
2024-12-19 13:41:02.575203: Current learning rate: 0.00669
2024-12-19 13:43:58.513942: Validation loss did not improve from -0.56003. Patience: 30/50
2024-12-19 13:43:58.514842: train_loss -0.7548
2024-12-19 13:43:58.515727: val_loss -0.545
2024-12-19 13:43:58.516632: Pseudo dice [0.746]
2024-12-19 13:43:58.517346: Epoch time: 175.94 s
2024-12-19 13:43:58.900401: Yayy! New best EMA pseudo Dice: 0.7419
2024-12-19 13:44:00.666012: 
2024-12-19 13:44:00.667610: Epoch 55
2024-12-19 13:44:00.668678: Current learning rate: 0.00663
2024-12-19 13:47:54.423092: Validation loss did not improve from -0.56003. Patience: 31/50
2024-12-19 13:47:54.424012: train_loss -0.7542
2024-12-19 13:47:54.424744: val_loss -0.5286
2024-12-19 13:47:54.425541: Pseudo dice [0.7284]
2024-12-19 13:47:54.426342: Epoch time: 233.76 s
2024-12-19 13:47:55.853148: 
2024-12-19 13:47:55.854262: Epoch 56
2024-12-19 13:47:55.855262: Current learning rate: 0.00657
2024-12-19 13:51:26.510497: Validation loss did not improve from -0.56003. Patience: 32/50
2024-12-19 13:51:26.511491: train_loss -0.7542
2024-12-19 13:51:26.512456: val_loss -0.525
2024-12-19 13:51:26.513289: Pseudo dice [0.7453]
2024-12-19 13:51:26.514081: Epoch time: 210.66 s
2024-12-19 13:51:27.967247: 
2024-12-19 13:51:27.968674: Epoch 57
2024-12-19 13:51:27.969437: Current learning rate: 0.0065
2024-12-19 13:54:30.630548: Validation loss did not improve from -0.56003. Patience: 33/50
2024-12-19 13:54:30.631263: train_loss -0.7584
2024-12-19 13:54:30.632174: val_loss -0.526
2024-12-19 13:54:30.632989: Pseudo dice [0.7473]
2024-12-19 13:54:30.633732: Epoch time: 182.67 s
2024-12-19 13:54:31.980415: 
2024-12-19 13:54:31.981678: Epoch 58
2024-12-19 13:54:31.982509: Current learning rate: 0.00644
2024-12-19 13:58:07.804926: Validation loss did not improve from -0.56003. Patience: 34/50
2024-12-19 13:58:07.806200: train_loss -0.7626
2024-12-19 13:58:07.807081: val_loss -0.5586
2024-12-19 13:58:07.808029: Pseudo dice [0.7598]
2024-12-19 13:58:07.808949: Epoch time: 215.83 s
2024-12-19 13:58:07.809772: Yayy! New best EMA pseudo Dice: 0.7435
2024-12-19 13:58:09.670606: 
2024-12-19 13:58:09.671680: Epoch 59
2024-12-19 13:58:09.672555: Current learning rate: 0.00638
2024-12-19 14:00:50.123059: Validation loss did not improve from -0.56003. Patience: 35/50
2024-12-19 14:00:50.124568: train_loss -0.7559
2024-12-19 14:00:50.126552: val_loss -0.5119
2024-12-19 14:00:50.127314: Pseudo dice [0.725]
2024-12-19 14:00:50.128240: Epoch time: 160.45 s
2024-12-19 14:00:51.997180: 
2024-12-19 14:00:51.998706: Epoch 60
2024-12-19 14:00:51.999953: Current learning rate: 0.00631
2024-12-19 14:04:40.624688: Validation loss did not improve from -0.56003. Patience: 36/50
2024-12-19 14:04:40.625678: train_loss -0.7586
2024-12-19 14:04:40.626600: val_loss -0.5447
2024-12-19 14:04:40.627414: Pseudo dice [0.7462]
2024-12-19 14:04:40.628196: Epoch time: 228.63 s
2024-12-19 14:04:42.592482: 
2024-12-19 14:04:42.593932: Epoch 61
2024-12-19 14:04:42.594848: Current learning rate: 0.00625
2024-12-19 14:08:12.829637: Validation loss improved from -0.56003 to -0.56792! Patience: 36/50
2024-12-19 14:08:12.830448: train_loss -0.7638
2024-12-19 14:08:12.831202: val_loss -0.5679
2024-12-19 14:08:12.831847: Pseudo dice [0.7689]
2024-12-19 14:08:12.832635: Epoch time: 210.24 s
2024-12-19 14:08:12.833347: Yayy! New best EMA pseudo Dice: 0.7448
2024-12-19 14:08:14.679955: 
2024-12-19 14:08:14.681224: Epoch 62
2024-12-19 14:08:14.681926: Current learning rate: 0.00619
2024-12-19 14:11:06.166156: Validation loss did not improve from -0.56792. Patience: 1/50
2024-12-19 14:11:06.167134: train_loss -0.7657
2024-12-19 14:11:06.167973: val_loss -0.5169
2024-12-19 14:11:06.168636: Pseudo dice [0.738]
2024-12-19 14:11:06.169310: Epoch time: 171.49 s
2024-12-19 14:11:07.534826: 
2024-12-19 14:11:07.536350: Epoch 63
2024-12-19 14:11:07.537161: Current learning rate: 0.00612
2024-12-19 14:15:07.123885: Validation loss did not improve from -0.56792. Patience: 2/50
2024-12-19 14:15:07.156389: train_loss -0.7657
2024-12-19 14:15:07.157720: val_loss -0.5445
2024-12-19 14:15:07.158520: Pseudo dice [0.7489]
2024-12-19 14:15:07.159224: Epoch time: 239.62 s
2024-12-19 14:15:08.642339: 
2024-12-19 14:15:08.643630: Epoch 64
2024-12-19 14:15:08.644360: Current learning rate: 0.00606
2024-12-19 14:18:43.830231: Validation loss did not improve from -0.56792. Patience: 3/50
2024-12-19 14:18:43.831284: train_loss -0.7661
2024-12-19 14:18:43.832255: val_loss -0.5415
2024-12-19 14:18:43.833216: Pseudo dice [0.7517]
2024-12-19 14:18:43.834137: Epoch time: 215.19 s
2024-12-19 14:18:44.307908: Yayy! New best EMA pseudo Dice: 0.7453
2024-12-19 14:18:46.215029: 
2024-12-19 14:18:46.216723: Epoch 65
2024-12-19 14:18:46.217649: Current learning rate: 0.006
2024-12-19 14:21:46.673744: Validation loss did not improve from -0.56792. Patience: 4/50
2024-12-19 14:21:46.674921: train_loss -0.7691
2024-12-19 14:21:46.675933: val_loss -0.5283
2024-12-19 14:21:46.676833: Pseudo dice [0.7485]
2024-12-19 14:21:46.677598: Epoch time: 180.46 s
2024-12-19 14:21:46.678311: Yayy! New best EMA pseudo Dice: 0.7456
2024-12-19 14:21:48.484108: 
2024-12-19 14:21:48.485418: Epoch 66
2024-12-19 14:21:48.486310: Current learning rate: 0.00593
2024-12-19 14:25:11.248055: Validation loss did not improve from -0.56792. Patience: 5/50
2024-12-19 14:25:11.249150: train_loss -0.7663
2024-12-19 14:25:11.250512: val_loss -0.5169
2024-12-19 14:25:11.251577: Pseudo dice [0.7348]
2024-12-19 14:25:11.252581: Epoch time: 202.77 s
2024-12-19 14:25:12.727931: 
2024-12-19 14:25:12.729266: Epoch 67
2024-12-19 14:25:12.730238: Current learning rate: 0.00587
2024-12-19 14:28:11.660544: Validation loss did not improve from -0.56792. Patience: 6/50
2024-12-19 14:28:11.661533: train_loss -0.7691
2024-12-19 14:28:11.662360: val_loss -0.5603
2024-12-19 14:28:11.663216: Pseudo dice [0.7622]
2024-12-19 14:28:11.664029: Epoch time: 178.93 s
2024-12-19 14:28:11.664768: Yayy! New best EMA pseudo Dice: 0.7463
2024-12-19 14:28:13.430146: 
2024-12-19 14:28:13.432007: Epoch 68
2024-12-19 14:28:13.433197: Current learning rate: 0.00581
2024-12-19 14:31:57.476239: Validation loss did not improve from -0.56792. Patience: 7/50
2024-12-19 14:31:57.477140: train_loss -0.7691
2024-12-19 14:31:57.478195: val_loss -0.5199
2024-12-19 14:31:57.479216: Pseudo dice [0.7398]
2024-12-19 14:31:57.480116: Epoch time: 224.05 s
2024-12-19 14:31:58.876016: 
2024-12-19 14:31:58.877422: Epoch 69
2024-12-19 14:31:58.878484: Current learning rate: 0.00574
2024-12-19 14:35:30.696948: Validation loss did not improve from -0.56792. Patience: 8/50
2024-12-19 14:35:30.697917: train_loss -0.7656
2024-12-19 14:35:30.698890: val_loss -0.5588
2024-12-19 14:35:30.699642: Pseudo dice [0.7538]
2024-12-19 14:35:30.700425: Epoch time: 211.82 s
2024-12-19 14:35:31.126493: Yayy! New best EMA pseudo Dice: 0.7465
2024-12-19 14:35:32.952844: 
2024-12-19 14:35:32.954250: Epoch 70
2024-12-19 14:35:32.955006: Current learning rate: 0.00568
2024-12-19 14:38:28.068072: Validation loss did not improve from -0.56792. Patience: 9/50
2024-12-19 14:38:28.069857: train_loss -0.7703
2024-12-19 14:38:28.071133: val_loss -0.5174
2024-12-19 14:38:28.072044: Pseudo dice [0.7347]
2024-12-19 14:38:28.073001: Epoch time: 175.12 s
2024-12-19 14:38:29.471401: 
2024-12-19 14:38:29.473007: Epoch 71
2024-12-19 14:38:29.473945: Current learning rate: 0.00562
2024-12-19 14:42:24.899471: Validation loss did not improve from -0.56792. Patience: 10/50
2024-12-19 14:42:24.900355: train_loss -0.773
2024-12-19 14:42:24.901384: val_loss -0.527
2024-12-19 14:42:24.902272: Pseudo dice [0.7433]
2024-12-19 14:42:24.903070: Epoch time: 235.43 s
2024-12-19 14:42:26.924819: 
2024-12-19 14:42:26.926282: Epoch 72
2024-12-19 14:42:26.927353: Current learning rate: 0.00555
2024-12-19 14:45:56.573340: Validation loss did not improve from -0.56792. Patience: 11/50
2024-12-19 14:45:56.574207: train_loss -0.7729
2024-12-19 14:45:56.575189: val_loss -0.5268
2024-12-19 14:45:56.575895: Pseudo dice [0.7404]
2024-12-19 14:45:56.576618: Epoch time: 209.65 s
2024-12-19 14:45:58.026817: 
2024-12-19 14:45:58.027928: Epoch 73
2024-12-19 14:45:58.028610: Current learning rate: 0.00549
2024-12-19 14:48:57.482355: Validation loss did not improve from -0.56792. Patience: 12/50
2024-12-19 14:48:57.483392: train_loss -0.766
2024-12-19 14:48:57.484458: val_loss -0.5042
2024-12-19 14:48:57.485450: Pseudo dice [0.7371]
2024-12-19 14:48:57.486326: Epoch time: 179.46 s
2024-12-19 14:48:58.911878: 
2024-12-19 14:48:58.913365: Epoch 74
2024-12-19 14:48:58.914129: Current learning rate: 0.00542
2024-12-19 14:52:54.578827: Validation loss did not improve from -0.56792. Patience: 13/50
2024-12-19 14:52:54.579845: train_loss -0.7724
2024-12-19 14:52:54.580826: val_loss -0.5393
2024-12-19 14:52:54.581597: Pseudo dice [0.7469]
2024-12-19 14:52:54.582421: Epoch time: 235.67 s
2024-12-19 14:52:56.502611: 
2024-12-19 14:52:56.504251: Epoch 75
2024-12-19 14:52:56.505324: Current learning rate: 0.00536
2024-12-19 14:56:31.272408: Validation loss did not improve from -0.56792. Patience: 14/50
2024-12-19 14:56:31.274350: train_loss -0.7708
2024-12-19 14:56:31.275279: val_loss -0.5554
2024-12-19 14:56:31.275967: Pseudo dice [0.7604]
2024-12-19 14:56:31.276655: Epoch time: 214.77 s
2024-12-19 14:56:32.745203: 
2024-12-19 14:56:32.746458: Epoch 76
2024-12-19 14:56:32.747262: Current learning rate: 0.00529
2024-12-19 14:59:46.343665: Validation loss did not improve from -0.56792. Patience: 15/50
2024-12-19 14:59:46.344574: train_loss -0.7758
2024-12-19 14:59:46.345688: val_loss -0.5095
2024-12-19 14:59:46.346653: Pseudo dice [0.7387]
2024-12-19 14:59:46.347512: Epoch time: 193.6 s
2024-12-19 14:59:47.754487: 
2024-12-19 14:59:47.755905: Epoch 77
2024-12-19 14:59:47.756694: Current learning rate: 0.00523
2024-12-19 15:03:57.418272: Validation loss did not improve from -0.56792. Patience: 16/50
2024-12-19 15:03:57.420096: train_loss -0.7773
2024-12-19 15:03:57.422280: val_loss -0.5436
2024-12-19 15:03:57.422897: Pseudo dice [0.7583]
2024-12-19 15:03:57.423881: Epoch time: 249.67 s
2024-12-19 15:03:58.896077: 
2024-12-19 15:03:58.896935: Epoch 78
2024-12-19 15:03:58.897576: Current learning rate: 0.00517
2024-12-19 15:07:33.332391: Validation loss did not improve from -0.56792. Patience: 17/50
2024-12-19 15:07:33.333541: train_loss -0.7783
2024-12-19 15:07:33.334487: val_loss -0.494
2024-12-19 15:07:33.335173: Pseudo dice [0.7307]
2024-12-19 15:07:33.335913: Epoch time: 214.44 s
2024-12-19 15:07:34.800684: 
2024-12-19 15:07:34.802031: Epoch 79
2024-12-19 15:07:34.802860: Current learning rate: 0.0051
2024-12-19 15:10:48.327221: Validation loss did not improve from -0.56792. Patience: 18/50
2024-12-19 15:10:48.328146: train_loss -0.7786
2024-12-19 15:10:48.329099: val_loss -0.5412
2024-12-19 15:10:48.329771: Pseudo dice [0.7491]
2024-12-19 15:10:48.330414: Epoch time: 193.53 s
2024-12-19 15:10:50.244854: 
2024-12-19 15:10:50.246281: Epoch 80
2024-12-19 15:10:50.247042: Current learning rate: 0.00504
2024-12-19 15:14:50.826800: Validation loss did not improve from -0.56792. Patience: 19/50
2024-12-19 15:14:50.827807: train_loss -0.7805
2024-12-19 15:14:50.828763: val_loss -0.5285
2024-12-19 15:14:50.829634: Pseudo dice [0.7408]
2024-12-19 15:14:50.830545: Epoch time: 240.59 s
2024-12-19 15:14:52.326672: 
2024-12-19 15:14:52.329170: Epoch 81
2024-12-19 15:14:52.330487: Current learning rate: 0.00497
2024-12-19 15:18:35.957041: Validation loss did not improve from -0.56792. Patience: 20/50
2024-12-19 15:18:35.973749: train_loss -0.7826
2024-12-19 15:18:35.974922: val_loss -0.5285
2024-12-19 15:18:35.975757: Pseudo dice [0.7403]
2024-12-19 15:18:35.976771: Epoch time: 223.65 s
2024-12-19 15:18:37.466892: 
2024-12-19 15:18:37.468040: Epoch 82
2024-12-19 15:18:37.468986: Current learning rate: 0.00491
2024-12-19 15:21:33.372329: Validation loss did not improve from -0.56792. Patience: 21/50
2024-12-19 15:21:33.373362: train_loss -0.7822
2024-12-19 15:21:33.374234: val_loss -0.5222
2024-12-19 15:21:33.375029: Pseudo dice [0.7414]
2024-12-19 15:21:33.375922: Epoch time: 175.91 s
2024-12-19 15:21:35.225825: 
2024-12-19 15:21:35.227017: Epoch 83
2024-12-19 15:21:35.227816: Current learning rate: 0.00484
2024-12-19 15:25:27.294266: Validation loss did not improve from -0.56792. Patience: 22/50
2024-12-19 15:25:27.295116: train_loss -0.779
2024-12-19 15:25:27.295889: val_loss -0.5438
2024-12-19 15:25:27.296674: Pseudo dice [0.756]
2024-12-19 15:25:27.297518: Epoch time: 232.07 s
2024-12-19 15:25:28.641576: 
2024-12-19 15:25:28.642787: Epoch 84
2024-12-19 15:25:28.643709: Current learning rate: 0.00478
2024-12-19 15:28:46.448060: Validation loss did not improve from -0.56792. Patience: 23/50
2024-12-19 15:28:46.449967: train_loss -0.7845
2024-12-19 15:28:46.450892: val_loss -0.5044
2024-12-19 15:28:46.451647: Pseudo dice [0.7372]
2024-12-19 15:28:46.452408: Epoch time: 197.81 s
2024-12-19 15:28:48.174388: 
2024-12-19 15:28:48.175589: Epoch 85
2024-12-19 15:28:48.176342: Current learning rate: 0.00471
2024-12-19 15:32:07.034997: Validation loss did not improve from -0.56792. Patience: 24/50
2024-12-19 15:32:07.036169: train_loss -0.7819
2024-12-19 15:32:07.037316: val_loss -0.5069
2024-12-19 15:32:07.038230: Pseudo dice [0.7413]
2024-12-19 15:32:07.039060: Epoch time: 198.86 s
2024-12-19 15:32:08.382109: 
2024-12-19 15:32:08.383546: Epoch 86
2024-12-19 15:32:08.384426: Current learning rate: 0.00465
2024-12-19 15:35:35.840594: Validation loss did not improve from -0.56792. Patience: 25/50
2024-12-19 15:35:35.841548: train_loss -0.7849
2024-12-19 15:35:35.842582: val_loss -0.5279
2024-12-19 15:35:35.843516: Pseudo dice [0.7353]
2024-12-19 15:35:35.844551: Epoch time: 207.46 s
2024-12-19 15:35:37.316983: 
2024-12-19 15:35:37.318528: Epoch 87
2024-12-19 15:35:37.319583: Current learning rate: 0.00458
2024-12-19 15:38:55.125376: Validation loss did not improve from -0.56792. Patience: 26/50
2024-12-19 15:38:55.126432: train_loss -0.7866
2024-12-19 15:38:55.127398: val_loss -0.5503
2024-12-19 15:38:55.128226: Pseudo dice [0.7547]
2024-12-19 15:38:55.128886: Epoch time: 197.81 s
2024-12-19 15:38:56.472217: 
2024-12-19 15:38:56.473424: Epoch 88
2024-12-19 15:38:56.474212: Current learning rate: 0.00452
2024-12-19 15:42:48.786824: Validation loss did not improve from -0.56792. Patience: 27/50
2024-12-19 15:42:48.787914: train_loss -0.7853
2024-12-19 15:42:48.788904: val_loss -0.5197
2024-12-19 15:42:48.789696: Pseudo dice [0.7428]
2024-12-19 15:42:48.790401: Epoch time: 232.32 s
2024-12-19 15:42:50.061158: 
2024-12-19 15:42:50.062614: Epoch 89
2024-12-19 15:42:50.063704: Current learning rate: 0.00445
2024-12-19 15:46:29.898551: Validation loss did not improve from -0.56792. Patience: 28/50
2024-12-19 15:46:29.899667: train_loss -0.7872
2024-12-19 15:46:29.900722: val_loss -0.5045
2024-12-19 15:46:29.901622: Pseudo dice [0.7305]
2024-12-19 15:46:29.902491: Epoch time: 219.84 s
2024-12-19 15:46:31.723453: 
2024-12-19 15:46:31.724982: Epoch 90
2024-12-19 15:46:31.725940: Current learning rate: 0.00438
2024-12-19 15:50:00.183442: Validation loss did not improve from -0.56792. Patience: 29/50
2024-12-19 15:50:00.184655: train_loss -0.7861
2024-12-19 15:50:00.185712: val_loss -0.5488
2024-12-19 15:50:00.186410: Pseudo dice [0.7527]
2024-12-19 15:50:00.187117: Epoch time: 208.46 s
2024-12-19 15:50:01.516765: 
2024-12-19 15:50:01.517900: Epoch 91
2024-12-19 15:50:01.518646: Current learning rate: 0.00432
2024-12-19 15:53:54.877936: Validation loss did not improve from -0.56792. Patience: 30/50
2024-12-19 15:53:54.878910: train_loss -0.7894
2024-12-19 15:53:54.879762: val_loss -0.5114
2024-12-19 15:53:54.880500: Pseudo dice [0.7335]
2024-12-19 15:53:54.881281: Epoch time: 233.36 s
2024-12-19 15:53:56.235124: 
2024-12-19 15:53:56.236552: Epoch 92
2024-12-19 15:53:56.237817: Current learning rate: 0.00425
2024-12-19 15:57:26.207551: Validation loss did not improve from -0.56792. Patience: 31/50
2024-12-19 15:57:26.208542: train_loss -0.79
2024-12-19 15:57:26.209666: val_loss -0.5432
2024-12-19 15:57:26.210492: Pseudo dice [0.7546]
2024-12-19 15:57:26.211416: Epoch time: 209.97 s
2024-12-19 15:57:27.572593: 
2024-12-19 15:57:27.573843: Epoch 93
2024-12-19 15:57:27.574603: Current learning rate: 0.00419
2024-12-19 16:00:31.205899: Validation loss improved from -0.56792 to -0.57179! Patience: 31/50
2024-12-19 16:00:31.206820: train_loss -0.7901
2024-12-19 16:00:31.207782: val_loss -0.5718
2024-12-19 16:00:31.208513: Pseudo dice [0.77]
2024-12-19 16:00:31.209205: Epoch time: 183.64 s
2024-12-19 16:00:31.209940: Yayy! New best EMA pseudo Dice: 0.7466
2024-12-19 16:00:33.053149: 
2024-12-19 16:00:33.054912: Epoch 94
2024-12-19 16:00:33.055700: Current learning rate: 0.00412
2024-12-19 16:03:57.478997: Validation loss did not improve from -0.57179. Patience: 1/50
2024-12-19 16:03:57.480052: train_loss -0.791
2024-12-19 16:03:57.481000: val_loss -0.5366
2024-12-19 16:03:57.481869: Pseudo dice [0.7554]
2024-12-19 16:03:57.482857: Epoch time: 204.43 s
2024-12-19 16:03:57.917983: Yayy! New best EMA pseudo Dice: 0.7475
2024-12-19 16:04:00.428423: 
2024-12-19 16:04:00.429445: Epoch 95
2024-12-19 16:04:00.430134: Current learning rate: 0.00405
2024-12-19 16:07:11.409637: Validation loss did not improve from -0.57179. Patience: 2/50
2024-12-19 16:07:11.410657: train_loss -0.7931
2024-12-19 16:07:11.411570: val_loss -0.5303
2024-12-19 16:07:11.412275: Pseudo dice [0.7401]
2024-12-19 16:07:11.413173: Epoch time: 190.98 s
2024-12-19 16:07:12.732176: 
2024-12-19 16:07:12.733748: Epoch 96
2024-12-19 16:07:12.734906: Current learning rate: 0.00399
2024-12-19 16:11:07.300833: Validation loss did not improve from -0.57179. Patience: 3/50
2024-12-19 16:11:07.303006: train_loss -0.7927
2024-12-19 16:11:07.305282: val_loss -0.5258
2024-12-19 16:11:07.306279: Pseudo dice [0.7417]
2024-12-19 16:11:07.307662: Epoch time: 234.57 s
2024-12-19 16:11:08.749355: 
2024-12-19 16:11:08.750638: Epoch 97
2024-12-19 16:11:08.751482: Current learning rate: 0.00392
2024-12-19 16:14:43.496691: Validation loss did not improve from -0.57179. Patience: 4/50
2024-12-19 16:14:43.497798: train_loss -0.7936
2024-12-19 16:14:43.498594: val_loss -0.5189
2024-12-19 16:14:43.499297: Pseudo dice [0.746]
2024-12-19 16:14:43.500028: Epoch time: 214.75 s
2024-12-19 16:14:44.905028: 
2024-12-19 16:14:44.906231: Epoch 98
2024-12-19 16:14:44.907018: Current learning rate: 0.00385
2024-12-19 16:17:44.587109: Validation loss did not improve from -0.57179. Patience: 5/50
2024-12-19 16:17:44.588222: train_loss -0.7942
2024-12-19 16:17:44.589236: val_loss -0.5293
2024-12-19 16:17:44.589972: Pseudo dice [0.7453]
2024-12-19 16:17:44.590701: Epoch time: 179.68 s
2024-12-19 16:17:45.991268: 
2024-12-19 16:17:45.992452: Epoch 99
2024-12-19 16:17:45.993305: Current learning rate: 0.00379
2024-12-19 16:21:14.461505: Validation loss did not improve from -0.57179. Patience: 6/50
2024-12-19 16:21:14.462506: train_loss -0.7947
2024-12-19 16:21:14.463305: val_loss -0.5201
2024-12-19 16:21:14.464017: Pseudo dice [0.7445]
2024-12-19 16:21:14.464694: Epoch time: 208.47 s
2024-12-19 16:21:16.289931: 
2024-12-19 16:21:16.291214: Epoch 100
2024-12-19 16:21:16.291910: Current learning rate: 0.00372
2024-12-19 16:24:17.734300: Validation loss did not improve from -0.57179. Patience: 7/50
2024-12-19 16:24:17.778629: train_loss -0.7912
2024-12-19 16:24:17.780119: val_loss -0.5476
2024-12-19 16:24:17.781164: Pseudo dice [0.7484]
2024-12-19 16:24:17.781979: Epoch time: 181.48 s
2024-12-19 16:24:19.216328: 
2024-12-19 16:24:19.217721: Epoch 101
2024-12-19 16:24:19.218746: Current learning rate: 0.00365
2024-12-19 16:28:07.045449: Validation loss did not improve from -0.57179. Patience: 8/50
2024-12-19 16:28:07.046594: train_loss -0.795
2024-12-19 16:28:07.047504: val_loss -0.536
2024-12-19 16:28:07.048249: Pseudo dice [0.75]
2024-12-19 16:28:07.049021: Epoch time: 227.83 s
2024-12-19 16:28:08.462042: 
2024-12-19 16:28:08.463454: Epoch 102
2024-12-19 16:28:08.464408: Current learning rate: 0.00359
2024-12-19 16:31:35.801786: Validation loss did not improve from -0.57179. Patience: 9/50
2024-12-19 16:31:35.802826: train_loss -0.7947
2024-12-19 16:31:35.803687: val_loss -0.5369
2024-12-19 16:31:35.804404: Pseudo dice [0.7538]
2024-12-19 16:31:35.805130: Epoch time: 207.34 s
2024-12-19 16:31:37.225971: 
2024-12-19 16:31:37.227442: Epoch 103
2024-12-19 16:31:37.228447: Current learning rate: 0.00352
2024-12-19 16:34:39.541350: Validation loss did not improve from -0.57179. Patience: 10/50
2024-12-19 16:34:39.542416: train_loss -0.7986
2024-12-19 16:34:39.543260: val_loss -0.524
2024-12-19 16:34:39.544091: Pseudo dice [0.7526]
2024-12-19 16:34:39.544903: Epoch time: 182.32 s
2024-12-19 16:34:39.545570: Yayy! New best EMA pseudo Dice: 0.7478
2024-12-19 16:34:41.368582: 
2024-12-19 16:34:41.370042: Epoch 104
2024-12-19 16:34:41.371179: Current learning rate: 0.00345
2024-12-19 16:38:34.783062: Validation loss did not improve from -0.57179. Patience: 11/50
2024-12-19 16:38:34.784128: train_loss -0.7928
2024-12-19 16:38:34.784973: val_loss -0.5438
2024-12-19 16:38:34.785772: Pseudo dice [0.7596]
2024-12-19 16:38:34.786592: Epoch time: 233.42 s
2024-12-19 16:38:35.184102: Yayy! New best EMA pseudo Dice: 0.749
2024-12-19 16:38:37.001571: 
2024-12-19 16:38:37.003003: Epoch 105
2024-12-19 16:38:37.004001: Current learning rate: 0.00338
2024-12-19 16:41:35.106442: Validation loss did not improve from -0.57179. Patience: 12/50
2024-12-19 16:41:35.107426: train_loss -0.7952
2024-12-19 16:41:35.108459: val_loss -0.5468
2024-12-19 16:41:35.109257: Pseudo dice [0.7546]
2024-12-19 16:41:35.110150: Epoch time: 178.11 s
2024-12-19 16:41:35.111006: Yayy! New best EMA pseudo Dice: 0.7496
2024-12-19 16:41:36.891053: 
2024-12-19 16:41:36.892289: Epoch 106
2024-12-19 16:41:36.893220: Current learning rate: 0.00332
2024-12-19 16:45:09.776130: Validation loss did not improve from -0.57179. Patience: 13/50
2024-12-19 16:45:09.777076: train_loss -0.7938
2024-12-19 16:45:09.778069: val_loss -0.528
2024-12-19 16:45:09.778804: Pseudo dice [0.7487]
2024-12-19 16:45:09.779564: Epoch time: 212.89 s
2024-12-19 16:45:11.726341: 
2024-12-19 16:45:11.727408: Epoch 107
2024-12-19 16:45:11.728120: Current learning rate: 0.00325
2024-12-19 16:48:56.776547: Validation loss did not improve from -0.57179. Patience: 14/50
2024-12-19 16:48:56.777683: train_loss -0.7942
2024-12-19 16:48:56.778826: val_loss -0.5477
2024-12-19 16:48:56.779831: Pseudo dice [0.7527]
2024-12-19 16:48:56.780837: Epoch time: 225.05 s
2024-12-19 16:48:56.781860: Yayy! New best EMA pseudo Dice: 0.7498
2024-12-19 16:48:58.626099: 
2024-12-19 16:48:58.627675: Epoch 108
2024-12-19 16:48:58.628685: Current learning rate: 0.00318
2024-12-19 16:52:11.922826: Validation loss did not improve from -0.57179. Patience: 15/50
2024-12-19 16:52:11.923862: train_loss -0.7984
2024-12-19 16:52:11.924669: val_loss -0.4925
2024-12-19 16:52:11.925567: Pseudo dice [0.7302]
2024-12-19 16:52:11.926515: Epoch time: 193.3 s
2024-12-19 16:52:13.292747: 
2024-12-19 16:52:13.294008: Epoch 109
2024-12-19 16:52:13.294746: Current learning rate: 0.00311
2024-12-19 16:56:00.135656: Validation loss did not improve from -0.57179. Patience: 16/50
2024-12-19 16:56:00.136673: train_loss -0.799
2024-12-19 16:56:00.137681: val_loss -0.5096
2024-12-19 16:56:00.138573: Pseudo dice [0.7443]
2024-12-19 16:56:00.139332: Epoch time: 226.85 s
2024-12-19 16:56:02.054435: 
2024-12-19 16:56:02.055896: Epoch 110
2024-12-19 16:56:02.056695: Current learning rate: 0.00304
2024-12-19 16:59:42.921144: Validation loss did not improve from -0.57179. Patience: 17/50
2024-12-19 16:59:42.922172: train_loss -0.8005
2024-12-19 16:59:42.923755: val_loss -0.4982
2024-12-19 16:59:42.924720: Pseudo dice [0.7312]
2024-12-19 16:59:42.925623: Epoch time: 220.87 s
2024-12-19 16:59:44.413212: 
2024-12-19 16:59:44.414760: Epoch 111
2024-12-19 16:59:44.416002: Current learning rate: 0.00297
2024-12-19 17:02:50.982728: Validation loss did not improve from -0.57179. Patience: 18/50
2024-12-19 17:02:50.984816: train_loss -0.7989
2024-12-19 17:02:50.985749: val_loss -0.5473
2024-12-19 17:02:50.986632: Pseudo dice [0.7546]
2024-12-19 17:02:50.987615: Epoch time: 186.57 s
2024-12-19 17:02:52.429997: 
2024-12-19 17:02:52.431211: Epoch 112
2024-12-19 17:02:52.432013: Current learning rate: 0.00291
2024-12-19 17:06:48.139304: Validation loss did not improve from -0.57179. Patience: 19/50
2024-12-19 17:06:48.140540: train_loss -0.8022
2024-12-19 17:06:48.141472: val_loss -0.5224
2024-12-19 17:06:48.142346: Pseudo dice [0.7482]
2024-12-19 17:06:48.143282: Epoch time: 235.71 s
2024-12-19 17:06:49.566881: 
2024-12-19 17:06:49.568614: Epoch 113
2024-12-19 17:06:49.570040: Current learning rate: 0.00284
2024-12-19 17:10:07.271031: Validation loss did not improve from -0.57179. Patience: 20/50
2024-12-19 17:10:07.273166: train_loss -0.799
2024-12-19 17:10:07.274352: val_loss -0.533
2024-12-19 17:10:07.275145: Pseudo dice [0.7529]
2024-12-19 17:10:07.276032: Epoch time: 197.71 s
2024-12-19 17:10:08.655156: 
2024-12-19 17:10:08.656546: Epoch 114
2024-12-19 17:10:08.657709: Current learning rate: 0.00277
2024-12-19 17:13:25.060364: Validation loss did not improve from -0.57179. Patience: 21/50
2024-12-19 17:13:25.062115: train_loss -0.8033
2024-12-19 17:13:25.063200: val_loss -0.5038
2024-12-19 17:13:25.064048: Pseudo dice [0.7362]
2024-12-19 17:13:25.065243: Epoch time: 196.41 s
2024-12-19 17:13:26.884475: 
2024-12-19 17:13:26.886054: Epoch 115
2024-12-19 17:13:26.887040: Current learning rate: 0.0027
2024-12-19 17:17:13.929801: Validation loss did not improve from -0.57179. Patience: 22/50
2024-12-19 17:17:13.931438: train_loss -0.8007
2024-12-19 17:17:13.932633: val_loss -0.5304
2024-12-19 17:17:13.933359: Pseudo dice [0.7527]
2024-12-19 17:17:13.934034: Epoch time: 227.05 s
2024-12-19 17:17:15.394273: 
2024-12-19 17:17:15.395218: Epoch 116
2024-12-19 17:17:15.396011: Current learning rate: 0.00263
2024-12-19 17:20:10.524525: Validation loss did not improve from -0.57179. Patience: 23/50
2024-12-19 17:20:10.525770: train_loss -0.8037
2024-12-19 17:20:10.526630: val_loss -0.5131
2024-12-19 17:20:10.527460: Pseudo dice [0.7415]
2024-12-19 17:20:10.528207: Epoch time: 175.13 s
2024-12-19 17:20:12.572524: 
2024-12-19 17:20:12.573626: Epoch 117
2024-12-19 17:20:12.574393: Current learning rate: 0.00256
2024-12-19 17:24:01.953837: Validation loss did not improve from -0.57179. Patience: 24/50
2024-12-19 17:24:01.955945: train_loss -0.8052
2024-12-19 17:24:01.957132: val_loss -0.5257
2024-12-19 17:24:01.957931: Pseudo dice [0.7457]
2024-12-19 17:24:01.958737: Epoch time: 229.38 s
2024-12-19 17:24:03.497694: 
2024-12-19 17:24:03.499098: Epoch 118
2024-12-19 17:24:03.500143: Current learning rate: 0.00249
2024-12-19 17:27:02.166280: Validation loss did not improve from -0.57179. Patience: 25/50
2024-12-19 17:27:02.167397: train_loss -0.8053
2024-12-19 17:27:02.168348: val_loss -0.5253
2024-12-19 17:27:02.169061: Pseudo dice [0.7476]
2024-12-19 17:27:02.169866: Epoch time: 178.67 s
2024-12-19 17:27:03.610871: 
2024-12-19 17:27:03.612181: Epoch 119
2024-12-19 17:27:03.613192: Current learning rate: 0.00242
2024-12-19 17:30:49.804049: Validation loss did not improve from -0.57179. Patience: 26/50
2024-12-19 17:30:49.821073: train_loss -0.8044
2024-12-19 17:30:49.822491: val_loss -0.5342
2024-12-19 17:30:49.823298: Pseudo dice [0.7461]
2024-12-19 17:30:49.824108: Epoch time: 226.21 s
2024-12-19 17:30:51.683224: 
2024-12-19 17:30:51.685013: Epoch 120
2024-12-19 17:30:51.686078: Current learning rate: 0.00235
2024-12-19 17:34:28.455083: Validation loss did not improve from -0.57179. Patience: 27/50
2024-12-19 17:34:28.455928: train_loss -0.8031
2024-12-19 17:34:28.456671: val_loss -0.5283
2024-12-19 17:34:28.457316: Pseudo dice [0.7423]
2024-12-19 17:34:28.458169: Epoch time: 216.77 s
2024-12-19 17:34:29.914894: 
2024-12-19 17:34:29.916334: Epoch 121
2024-12-19 17:34:29.917082: Current learning rate: 0.00228
2024-12-19 17:37:50.177176: Validation loss did not improve from -0.57179. Patience: 28/50
2024-12-19 17:37:50.177828: train_loss -0.8054
2024-12-19 17:37:50.178708: val_loss -0.5032
2024-12-19 17:37:50.179462: Pseudo dice [0.7429]
2024-12-19 17:37:50.180330: Epoch time: 200.26 s
2024-12-19 17:37:51.619303: 
2024-12-19 17:37:51.620712: Epoch 122
2024-12-19 17:37:51.621497: Current learning rate: 0.00221
2024-12-19 17:41:52.203181: Validation loss did not improve from -0.57179. Patience: 29/50
2024-12-19 17:41:52.204562: train_loss -0.8057
2024-12-19 17:41:52.205533: val_loss -0.5334
2024-12-19 17:41:52.206238: Pseudo dice [0.7478]
2024-12-19 17:41:52.206940: Epoch time: 240.59 s
2024-12-19 17:41:53.640933: 
2024-12-19 17:41:53.642519: Epoch 123
2024-12-19 17:41:53.643239: Current learning rate: 0.00214
2024-12-19 17:45:46.391481: Validation loss did not improve from -0.57179. Patience: 30/50
2024-12-19 17:45:46.392370: train_loss -0.8067
2024-12-19 17:45:46.393153: val_loss -0.5328
2024-12-19 17:45:46.393889: Pseudo dice [0.7461]
2024-12-19 17:45:46.394810: Epoch time: 232.75 s
2024-12-19 17:45:47.840095: 
2024-12-19 17:45:47.841280: Epoch 124
2024-12-19 17:45:47.842134: Current learning rate: 0.00207
2024-12-19 17:48:56.563599: Validation loss did not improve from -0.57179. Patience: 31/50
2024-12-19 17:48:56.564596: train_loss -0.8057
2024-12-19 17:48:56.565540: val_loss -0.504
2024-12-19 17:48:56.566518: Pseudo dice [0.7358]
2024-12-19 17:48:56.567452: Epoch time: 188.73 s
2024-12-19 17:48:58.374124: 
2024-12-19 17:48:58.375362: Epoch 125
2024-12-19 17:48:58.376269: Current learning rate: 0.00199
2024-12-19 17:52:43.263599: Validation loss did not improve from -0.57179. Patience: 32/50
2024-12-19 17:52:43.264364: train_loss -0.8083
2024-12-19 17:52:43.265123: val_loss -0.5185
2024-12-19 17:52:43.265874: Pseudo dice [0.7465]
2024-12-19 17:52:43.266618: Epoch time: 224.89 s
2024-12-19 17:52:44.691944: 
2024-12-19 17:52:44.693397: Epoch 126
2024-12-19 17:52:44.694454: Current learning rate: 0.00192
2024-12-19 17:55:45.420521: Validation loss did not improve from -0.57179. Patience: 33/50
2024-12-19 17:55:45.421387: train_loss -0.8076
2024-12-19 17:55:45.422300: val_loss -0.5165
2024-12-19 17:55:45.423270: Pseudo dice [0.7476]
2024-12-19 17:55:45.424145: Epoch time: 180.73 s
2024-12-19 17:55:46.924793: 
2024-12-19 17:55:46.926127: Epoch 127
2024-12-19 17:55:46.926989: Current learning rate: 0.00185
2024-12-19 17:59:39.377919: Validation loss did not improve from -0.57179. Patience: 34/50
2024-12-19 17:59:39.378923: train_loss -0.8086
2024-12-19 17:59:39.379915: val_loss -0.5135
2024-12-19 17:59:39.380668: Pseudo dice [0.7333]
2024-12-19 17:59:39.381576: Epoch time: 232.46 s
2024-12-19 17:59:40.813466: 
2024-12-19 17:59:40.815084: Epoch 128
2024-12-19 17:59:40.816021: Current learning rate: 0.00178
2024-12-19 18:03:26.368158: Validation loss did not improve from -0.57179. Patience: 35/50
2024-12-19 18:03:26.369086: train_loss -0.8065
2024-12-19 18:03:26.369906: val_loss -0.5217
2024-12-19 18:03:26.370561: Pseudo dice [0.7404]
2024-12-19 18:03:26.371264: Epoch time: 225.56 s
2024-12-19 18:03:27.793762: 
2024-12-19 18:03:27.795089: Epoch 129
2024-12-19 18:03:27.795897: Current learning rate: 0.0017
2024-12-19 18:06:44.614259: Validation loss did not improve from -0.57179. Patience: 36/50
2024-12-19 18:06:44.615239: train_loss -0.8084
2024-12-19 18:06:44.617383: val_loss -0.5443
2024-12-19 18:06:44.618580: Pseudo dice [0.7564]
2024-12-19 18:06:44.619786: Epoch time: 196.82 s
2024-12-19 18:06:46.460322: 
2024-12-19 18:06:46.462362: Epoch 130
2024-12-19 18:06:46.463283: Current learning rate: 0.00163
2024-12-19 18:10:40.136006: Validation loss did not improve from -0.57179. Patience: 37/50
2024-12-19 18:10:40.137184: train_loss -0.809
2024-12-19 18:10:40.137971: val_loss -0.5222
2024-12-19 18:10:40.138731: Pseudo dice [0.7355]
2024-12-19 18:10:40.139398: Epoch time: 233.68 s
2024-12-19 18:10:41.590471: 
2024-12-19 18:10:41.592211: Epoch 131
2024-12-19 18:10:41.593337: Current learning rate: 0.00156
2024-12-19 18:14:20.794016: Validation loss did not improve from -0.57179. Patience: 38/50
2024-12-19 18:14:20.796160: train_loss -0.8085
2024-12-19 18:14:20.797185: val_loss -0.5143
2024-12-19 18:14:20.797948: Pseudo dice [0.7503]
2024-12-19 18:14:20.798814: Epoch time: 219.21 s
2024-12-19 18:14:22.271837: 
2024-12-19 18:14:22.273245: Epoch 132
2024-12-19 18:14:22.274230: Current learning rate: 0.00148
2024-12-19 18:17:36.639605: Validation loss did not improve from -0.57179. Patience: 39/50
2024-12-19 18:17:36.641423: train_loss -0.8086
2024-12-19 18:17:36.642384: val_loss -0.5345
2024-12-19 18:17:36.643097: Pseudo dice [0.7411]
2024-12-19 18:17:36.643920: Epoch time: 194.37 s
2024-12-19 18:17:37.997142: 
2024-12-19 18:17:37.998685: Epoch 133
2024-12-19 18:17:37.999466: Current learning rate: 0.00141
2024-12-19 18:21:13.772507: Validation loss did not improve from -0.57179. Patience: 40/50
2024-12-19 18:21:13.773625: train_loss -0.8115
2024-12-19 18:21:13.774585: val_loss -0.5438
2024-12-19 18:21:13.775327: Pseudo dice [0.7544]
2024-12-19 18:21:13.776038: Epoch time: 215.78 s
2024-12-19 18:21:15.229793: 
2024-12-19 18:21:15.231158: Epoch 134
2024-12-19 18:21:15.231876: Current learning rate: 0.00133
2024-12-19 18:24:17.942940: Validation loss did not improve from -0.57179. Patience: 41/50
2024-12-19 18:24:17.943994: train_loss -0.8103
2024-12-19 18:24:17.945009: val_loss -0.5429
2024-12-19 18:24:17.945797: Pseudo dice [0.7588]
2024-12-19 18:24:17.946590: Epoch time: 182.72 s
2024-12-19 18:24:19.781076: 
2024-12-19 18:24:19.782471: Epoch 135
2024-12-19 18:24:19.783328: Current learning rate: 0.00126
2024-12-19 18:28:14.093584: Validation loss did not improve from -0.57179. Patience: 42/50
2024-12-19 18:28:14.094297: train_loss -0.8126
2024-12-19 18:28:14.095202: val_loss -0.5136
2024-12-19 18:28:14.095898: Pseudo dice [0.7415]
2024-12-19 18:28:14.096594: Epoch time: 234.31 s
2024-12-19 18:28:15.565805: 
2024-12-19 18:28:15.567671: Epoch 136
2024-12-19 18:28:15.568448: Current learning rate: 0.00118
2024-12-19 18:31:58.883080: Validation loss did not improve from -0.57179. Patience: 43/50
2024-12-19 18:31:58.884650: train_loss -0.8099
2024-12-19 18:31:58.885675: val_loss -0.5234
2024-12-19 18:31:58.886522: Pseudo dice [0.7516]
2024-12-19 18:31:58.887443: Epoch time: 223.32 s
2024-12-19 18:32:00.433563: 
2024-12-19 18:32:00.434942: Epoch 137
2024-12-19 18:32:00.435648: Current learning rate: 0.00111
2024-12-19 18:35:18.024169: Validation loss did not improve from -0.57179. Patience: 44/50
2024-12-19 18:35:18.030569: train_loss -0.8117
2024-12-19 18:35:18.031841: val_loss -0.5458
2024-12-19 18:35:18.032800: Pseudo dice [0.7582]
2024-12-19 18:35:18.033827: Epoch time: 197.59 s
2024-12-19 18:35:19.500737: 
2024-12-19 18:35:19.501847: Epoch 138
2024-12-19 18:35:19.502688: Current learning rate: 0.00103
2024-12-19 18:39:18.950172: Validation loss did not improve from -0.57179. Patience: 45/50
2024-12-19 18:39:18.951760: train_loss -0.8114
2024-12-19 18:39:18.953298: val_loss -0.5341
2024-12-19 18:39:18.954539: Pseudo dice [0.7528]
2024-12-19 18:39:18.955370: Epoch time: 239.45 s
2024-12-19 18:39:20.376538: 
2024-12-19 18:39:20.377892: Epoch 139
2024-12-19 18:39:20.378766: Current learning rate: 0.00095
2024-12-19 18:42:56.273438: Validation loss did not improve from -0.57179. Patience: 46/50
2024-12-19 18:42:56.274754: train_loss -0.8128
2024-12-19 18:42:56.276227: val_loss -0.5165
2024-12-19 18:42:56.277488: Pseudo dice [0.7409]
2024-12-19 18:42:56.278694: Epoch time: 215.9 s
2024-12-19 18:42:58.830330: 
2024-12-19 18:42:58.831517: Epoch 140
2024-12-19 18:42:58.832594: Current learning rate: 0.00087
2024-12-19 18:46:06.085641: Validation loss did not improve from -0.57179. Patience: 47/50
2024-12-19 18:46:06.086602: train_loss -0.8129
2024-12-19 18:46:06.087488: val_loss -0.5121
2024-12-19 18:46:06.088226: Pseudo dice [0.7448]
2024-12-19 18:46:06.088900: Epoch time: 187.26 s
2024-12-19 18:46:07.474346: 
2024-12-19 18:46:07.475744: Epoch 141
2024-12-19 18:46:07.476653: Current learning rate: 0.00079
2024-12-19 18:49:39.182161: Validation loss did not improve from -0.57179. Patience: 48/50
2024-12-19 18:49:39.183600: train_loss -0.8128
2024-12-19 18:49:39.184781: val_loss -0.4975
2024-12-19 18:49:39.185563: Pseudo dice [0.7361]
2024-12-19 18:49:39.186386: Epoch time: 211.71 s
2024-12-19 18:49:40.657779: 
2024-12-19 18:49:40.659055: Epoch 142
2024-12-19 18:49:40.659760: Current learning rate: 0.00071
2024-12-19 18:52:42.792250: Validation loss did not improve from -0.57179. Patience: 49/50
2024-12-19 18:52:42.793211: train_loss -0.8127
2024-12-19 18:52:42.794099: val_loss -0.5408
2024-12-19 18:52:42.795195: Pseudo dice [0.7515]
2024-12-19 18:52:42.796085: Epoch time: 182.14 s
2024-12-19 18:52:44.241500: 
2024-12-19 18:52:44.243308: Epoch 143
2024-12-19 18:52:44.244328: Current learning rate: 0.00063
2024-12-19 18:56:20.602472: Validation loss did not improve from -0.57179. Patience: 50/50
2024-12-19 18:56:20.603678: train_loss -0.8165
2024-12-19 18:56:20.604870: val_loss -0.5351
2024-12-19 18:56:20.605828: Pseudo dice [0.7529]
2024-12-19 18:56:20.606645: Epoch time: 216.36 s
2024-12-19 18:56:22.029931: 
2024-12-19 18:56:22.031611: Epoch 144
2024-12-19 18:56:22.032479: Current learning rate: 0.00055
2024-12-19 19:00:06.606605: Validation loss did not improve from -0.57179. Patience: 51/50
2024-12-19 19:00:06.607682: train_loss -0.8143
2024-12-19 19:00:06.608939: val_loss -0.5332
2024-12-19 19:00:06.609936: Pseudo dice [0.7537]
2024-12-19 19:00:06.610981: Epoch time: 224.58 s
2024-12-19 19:00:08.494273: 
2024-12-19 19:00:08.495708: Epoch 145
2024-12-19 19:00:08.496680: Current learning rate: 0.00047
2024-12-19 19:03:18.727825: Validation loss did not improve from -0.57179. Patience: 52/50
2024-12-19 19:03:18.729055: train_loss -0.8151
2024-12-19 19:03:18.731407: val_loss -0.5433
2024-12-19 19:03:18.732330: Pseudo dice [0.7602]
2024-12-19 19:03:18.733585: Epoch time: 190.24 s
2024-12-19 19:03:20.178666: 
2024-12-19 19:03:20.180118: Epoch 146
2024-12-19 19:03:20.180972: Current learning rate: 0.00038
2024-12-19 19:07:10.450432: Validation loss did not improve from -0.57179. Patience: 53/50
2024-12-19 19:07:10.451369: train_loss -0.816
2024-12-19 19:07:10.452281: val_loss -0.5405
2024-12-19 19:07:10.453060: Pseudo dice [0.7547]
2024-12-19 19:07:10.453912: Epoch time: 230.27 s
2024-12-19 19:07:11.956954: 
2024-12-19 19:07:11.958991: Epoch 147
2024-12-19 19:07:11.960278: Current learning rate: 0.0003
2024-12-19 19:10:53.808300: Validation loss did not improve from -0.57179. Patience: 54/50
2024-12-19 19:10:53.808967: train_loss -0.8144
2024-12-19 19:10:53.809852: val_loss -0.5416
2024-12-19 19:10:53.810640: Pseudo dice [0.7572]
2024-12-19 19:10:53.811340: Epoch time: 221.85 s
2024-12-19 19:10:53.812039: Yayy! New best EMA pseudo Dice: 0.7505
2024-12-19 19:10:55.654005: 
2024-12-19 19:10:55.655940: Epoch 148
2024-12-19 19:10:55.656786: Current learning rate: 0.00021
2024-12-19 19:14:11.389777: Validation loss did not improve from -0.57179. Patience: 55/50
2024-12-19 19:14:11.390881: train_loss -0.8117
2024-12-19 19:14:11.391902: val_loss -0.527
2024-12-19 19:14:11.392678: Pseudo dice [0.7494]
2024-12-19 19:14:11.393595: Epoch time: 195.74 s
2024-12-19 19:14:12.846965: 
2024-12-19 19:14:12.848493: Epoch 149
2024-12-19 19:14:12.849293: Current learning rate: 0.00011
2024-12-19 19:17:34.406580: Validation loss did not improve from -0.57179. Patience: 56/50
2024-12-19 19:17:34.407606: train_loss -0.8155
2024-12-19 19:17:34.408640: val_loss -0.5293
2024-12-19 19:17:34.409528: Pseudo dice [0.7514]
2024-12-19 19:17:34.410398: Epoch time: 201.56 s
2024-12-19 19:17:36.820855: Training done.
2024-12-19 19:17:37.298947: Using splits from existing split file: /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset309_Sohee_Calcium_OCT_CrossValidation/splits_final_40.json
2024-12-19 19:17:37.307166: The split file contains 5 splits.
2024-12-19 19:17:37.308391: Desired fold for training: 4
2024-12-19 19:17:37.309700: This split has 3 training and 5 validation cases.
2024-12-19 19:17:37.311281: predicting 101-044
2024-12-19 19:17:37.440125: 101-044, shape torch.Size([1, 404, 498, 498]), rank 0
2024-12-19 19:19:52.378877: predicting 101-045
2024-12-19 19:19:52.413133: 101-045, shape torch.Size([1, 375, 498, 498]), rank 0
2024-12-19 19:21:48.032706: predicting 401-004
2024-12-19 19:21:48.055857: 401-004, shape torch.Size([1, 375, 498, 498]), rank 0
2024-12-19 19:23:53.803499: predicting 704-003
2024-12-19 19:23:53.827864: 704-003, shape torch.Size([1, 375, 498, 498]), rank 0
2024-12-19 19:25:48.085775: predicting 706-005
2024-12-19 19:25:48.116647: 706-005, shape torch.Size([1, 375, 498, 498]), rank 0
2024-12-19 19:28:10.564283: Validation complete
2024-12-19 19:28:10.565743: Mean Validation Dice:  0.7208060947235005
