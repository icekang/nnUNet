/home/gridsan/nchutisilp/.conda/envs/nnunet/bin/python
Starting training with CONFIG=3d_32x160x128_b10, DATASET_ID=309, TRAINER=nnUNetTrainerScaleAnalysis20

############################
INFO: You are using the old nnU-Net default plans. We have updated our recommendations. Please consider using those instead! Read more here: https://github.com/MIC-DKFZ/nnUNet/blob/master/documentation/resenc_presets.md
############################

Using device: cuda:0

#######################################################################
Please cite the following paper when using nnU-Net:
Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.
#######################################################################

2024-12-17 21:33:36.187523: Using torch.compile...

############################
INFO: You are using the old nnU-Net default plans. We have updated our recommendations. Please consider using those instead! Read more here: https://github.com/MIC-DKFZ/nnUNet/blob/master/documentation/resenc_presets.md
############################

Using device: cuda:0

#######################################################################
Please cite the following paper when using nnU-Net:
Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.
#######################################################################

2024-12-17 21:33:36.189429: Using torch.compile...
/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
################### Loading pretrained weights from file  /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_results/Dataset300_Lumen_and_Wall_OCT/nnUNetTrainer__nnUNetPreprocessPlans__3d_fullres/fold_all/checkpoint_best.pth ###################
Below is the list of overlapping blocks in pretrained model and nnUNet architecture:
encoder.stages.0.0.convs.0.conv.weight shape torch.Size([32, 1, 3, 3, 3])
encoder.stages.0.0.convs.0.conv.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.norm.weight shape torch.Size([32])
encoder.stages.0.0.convs.0.norm.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.0.weight shape torch.Size([32, 1, 3, 3, 3])
encoder.stages.0.0.convs.0.all_modules.0.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.1.weight shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.1.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
encoder.stages.0.0.convs.1.conv.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.norm.weight shape torch.Size([32])
encoder.stages.0.0.convs.1.norm.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
encoder.stages.0.0.convs.1.all_modules.0.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.1.weight shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.1.bias shape torch.Size([32])
encoder.stages.1.0.convs.0.conv.weight shape torch.Size([64, 32, 3, 3, 3])
encoder.stages.1.0.convs.0.conv.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.norm.weight shape torch.Size([64])
encoder.stages.1.0.convs.0.norm.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.0.weight shape torch.Size([64, 32, 3, 3, 3])
encoder.stages.1.0.convs.0.all_modules.0.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.1.weight shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.1.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
encoder.stages.1.0.convs.1.conv.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.norm.weight shape torch.Size([64])
encoder.stages.1.0.convs.1.norm.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
encoder.stages.1.0.convs.1.all_modules.0.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.1.weight shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.1.bias shape torch.Size([64])
encoder.stages.2.0.convs.0.conv.weight shape torch.Size([128, 64, 3, 3, 3])
encoder.stages.2.0.convs.0.conv.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.norm.weight shape torch.Size([128])
encoder.stages.2.0.convs.0.norm.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.0.weight shape torch.Size([128, 64, 3, 3, 3])
encoder.stages.2.0.convs.0.all_modules.0.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.1.weight shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.1.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
encoder.stages.2.0.convs.1.conv.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.norm.weight shape torch.Size([128])
encoder.stages.2.0.convs.1.norm.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
encoder.stages.2.0.convs.1.all_modules.0.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.1.weight shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.1.bias shape torch.Size([128])
encoder.stages.3.0.convs.0.conv.weight shape torch.Size([256, 128, 3, 3, 3])
encoder.stages.3.0.convs.0.conv.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.norm.weight shape torch.Size([256])
encoder.stages.3.0.convs.0.norm.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.0.weight shape torch.Size([256, 128, 3, 3, 3])
encoder.stages.3.0.convs.0.all_modules.0.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.1.weight shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.1.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
encoder.stages.3.0.convs.1.conv.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.norm.weight shape torch.Size([256])
encoder.stages.3.0.convs.1.norm.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
encoder.stages.3.0.convs.1.all_modules.0.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.1.weight shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.1.bias shape torch.Size([256])
encoder.stages.4.0.convs.0.conv.weight shape torch.Size([320, 256, 3, 3, 3])
encoder.stages.4.0.convs.0.conv.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.norm.weight shape torch.Size([320])
encoder.stages.4.0.convs.0.norm.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.0.weight shape torch.Size([320, 256, 3, 3, 3])
encoder.stages.4.0.convs.0.all_modules.0.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.1.weight shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.1.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.4.0.convs.1.conv.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.norm.weight shape torch.Size([320])
encoder.stages.4.0.convs.1.norm.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.4.0.convs.1.all_modules.0.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.1.weight shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.1.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.0.conv.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.norm.weight shape torch.Size([320])
encoder.stages.5.0.convs.0.norm.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.0.all_modules.0.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.1.weight shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.1.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.1.conv.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.norm.weight shape torch.Size([320])
encoder.stages.5.0.convs.1.norm.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.1.all_modules.0.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.1.weight shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.0.0.convs.0.conv.weight shape torch.Size([32, 1, 3, 3, 3])
decoder.encoder.stages.0.0.convs.0.conv.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.norm.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.norm.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.0.weight shape torch.Size([32, 1, 3, 3, 3])
decoder.encoder.stages.0.0.convs.0.all_modules.0.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.1.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.1.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.encoder.stages.0.0.convs.1.conv.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.norm.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.norm.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.encoder.stages.0.0.convs.1.all_modules.0.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.1.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.1.bias shape torch.Size([32])
decoder.encoder.stages.1.0.convs.0.conv.weight shape torch.Size([64, 32, 3, 3, 3])
decoder.encoder.stages.1.0.convs.0.conv.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.norm.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.norm.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.0.weight shape torch.Size([64, 32, 3, 3, 3])
decoder.encoder.stages.1.0.convs.0.all_modules.0.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.1.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.1.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.encoder.stages.1.0.convs.1.conv.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.norm.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.norm.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.encoder.stages.1.0.convs.1.all_modules.0.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.1.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.1.bias shape torch.Size([64])
decoder.encoder.stages.2.0.convs.0.conv.weight shape torch.Size([128, 64, 3, 3, 3])
decoder.encoder.stages.2.0.convs.0.conv.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.norm.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.norm.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.0.weight shape torch.Size([128, 64, 3, 3, 3])
decoder.encoder.stages.2.0.convs.0.all_modules.0.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.1.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.1.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.encoder.stages.2.0.convs.1.conv.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.norm.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.norm.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.encoder.stages.2.0.convs.1.all_modules.0.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.1.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.1.bias shape torch.Size([128])
decoder.encoder.stages.3.0.convs.0.conv.weight shape torch.Size([256, 128, 3, 3, 3])
decoder.encoder.stages.3.0.convs.0.conv.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.norm.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.norm.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.0.weight shape torch.Size([256, 128, 3, 3, 3])
decoder.encoder.stages.3.0.convs.0.all_modules.0.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.1.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.1.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.encoder.stages.3.0.convs.1.conv.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.norm.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.norm.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.encoder.stages.3.0.convs.1.all_modules.0.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.1.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.1.bias shape torch.Size([256])
decoder.encoder.stages.4.0.convs.0.conv.weight shape torch.Size([320, 256, 3, 3, 3])
decoder.encoder.stages.4.0.convs.0.conv.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.norm.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.norm.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.0.weight shape torch.Size([320, 256, 3, 3, 3])
decoder.encoder.stages.4.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.4.0.convs.1.conv.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.norm.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.norm.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.4.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.0.conv.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.norm.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.norm.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.1.conv.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.norm.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.norm.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.stages.0.convs.0.conv.weight shape torch.Size([320, 640, 3, 3, 3])
decoder.stages.0.convs.0.conv.bias shape torch.Size([320])
decoder.stages.0.convs.0.norm.weight shape torch.Size([320])
decoder.stages.0.convs.0.norm.bias shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.0.weight shape torch.Size([320, 640, 3, 3, 3])
decoder.stages.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.stages.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.stages.0.convs.1.conv.bias shape torch.Size([320])
decoder.stages.0.convs.1.norm.weight shape torch.Size([320])
decoder.stages.0.convs.1.norm.bias shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.stages.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.stages.1.convs.0.conv.weight shape torch.Size([256, 512, 3, 3, 3])
decoder.stages.1.convs.0.conv.bias shape torch.Size([256])
decoder.stages.1.convs.0.norm.weight shape torch.Size([256])
decoder.stages.1.convs.0.norm.bias shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.0.weight shape torch.Size([256, 512, 3, 3, 3])
decoder.stages.1.convs.0.all_modules.0.bias shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.1.weight shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.1.bias shape torch.Size([256])
decoder.stages.1.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.stages.1.convs.1.conv.bias shape torch.Size([256])
decoder.stages.1.convs.1.norm.weight shape torch.Size([256])
decoder.stages.1.convs.1.norm.bias shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.stages.1.convs.1.all_modules.0.bias shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.1.weight shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.1.bias shape torch.Size([256])
decoder.stages.2.convs.0.conv.weight shape torch.Size([128, 256, 3, 3, 3])
decoder.stages.2.convs.0.conv.bias shape torch.Size([128])
decoder.stages.2.convs.0.norm.weight shape torch.Size([128])
decoder.stages.2.convs.0.norm.bias shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.0.weight shape torch.Size([128, 256, 3, 3, 3])
decoder.stages.2.convs.0.all_modules.0.bias shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.1.weight shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.1.bias shape torch.Size([128])
decoder.stages.2.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.stages.2.convs.1.conv.bias shape torch.Size([128])
decoder.stages.2.convs.1.norm.weight shape torch.Size([128])
decoder.stages.2.convs.1.norm.bias shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.stages.2.convs.1.all_modules.0.bias shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.1.weight shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.1.bias shape torch.Size([128])
decoder.stages.3.convs.0.conv.weight shape torch.Size([64, 128, 3, 3, 3])
decoder.stages.3.convs.0.conv.bias shape torch.Size([64])
decoder.stages.3.convs.0.norm.weight shape torch.Size([64])
decoder.stages.3.convs.0.norm.bias shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.0.weight shape torch.Size([64, 128, 3, 3, 3])
decoder.stages.3.convs.0.all_modules.0.bias shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.1.weight shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.1.bias shape torch.Size([64])
decoder.stages.3.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.stages.3.convs.1.conv.bias shape torch.Size([64])
decoder.stages.3.convs.1.norm.weight shape torch.Size([64])
decoder.stages.3.convs.1.norm.bias shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.stages.3.convs.1.all_modules.0.bias shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.1.weight shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.1.bias shape torch.Size([64])
decoder.stages.4.convs.0.conv.weight shape torch.Size([32, 64, 3, 3, 3])
decoder.stages.4.convs.0.conv.bias shape torch.Size([32])
decoder.stages.4.convs.0.norm.weight shape torch.Size([32])
decoder.stages.4.convs.0.norm.bias shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.0.weight shape torch.Size([32, 64, 3, 3, 3])
decoder.stages.4.convs.0.all_modules.0.bias shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.1.weight shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.1.bias shape torch.Size([32])
decoder.stages.4.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.stages.4.convs.1.conv.bias shape torch.Size([32])
decoder.stages.4.convs.1.norm.weight shape torch.Size([32])
decoder.stages.4.convs.1.norm.bias shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.stages.4.convs.1.all_modules.0.bias shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.1.weight shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.1.bias shape torch.Size([32])
decoder.transpconvs.0.weight shape torch.Size([320, 320, 1, 2, 2])
decoder.transpconvs.0.bias shape torch.Size([320])
decoder.transpconvs.1.weight shape torch.Size([320, 256, 2, 2, 2])
decoder.transpconvs.1.bias shape torch.Size([256])
decoder.transpconvs.2.weight shape torch.Size([256, 128, 2, 2, 2])
decoder.transpconvs.2.bias shape torch.Size([128])
decoder.transpconvs.3.weight shape torch.Size([128, 64, 2, 2, 2])
decoder.transpconvs.3.bias shape torch.Size([64])
decoder.transpconvs.4.weight shape torch.Size([64, 32, 2, 2, 2])
decoder.transpconvs.4.bias shape torch.Size([32])
################### Done ###################
2024-12-17 21:33:38.549041: do_dummy_2d_data_aug: True
2024-12-17 21:33:38.597576: Using splits from existing split file: /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset309_Sohee_Calcium_OCT_CrossValidation/splits_final_20.json
2024-12-17 21:33:38.617233: The split file contains 5 splits.
2024-12-17 21:33:38.618813: Desired fold for training: 0
2024-12-17 21:33:38.619717: This split has 1 training and 7 validation cases.
################### Loading pretrained weights from file  /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_results/Dataset300_Lumen_and_Wall_OCT/nnUNetTrainer__nnUNetPreprocessPlans__3d_fullres/fold_all/checkpoint_best.pth ###################
Below is the list of overlapping blocks in pretrained model and nnUNet architecture:
encoder.stages.0.0.convs.0.conv.weight shape torch.Size([32, 1, 3, 3, 3])
encoder.stages.0.0.convs.0.conv.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.norm.weight shape torch.Size([32])
encoder.stages.0.0.convs.0.norm.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.0.weight shape torch.Size([32, 1, 3, 3, 3])
encoder.stages.0.0.convs.0.all_modules.0.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.1.weight shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.1.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
encoder.stages.0.0.convs.1.conv.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.norm.weight shape torch.Size([32])
encoder.stages.0.0.convs.1.norm.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
encoder.stages.0.0.convs.1.all_modules.0.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.1.weight shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.1.bias shape torch.Size([32])
encoder.stages.1.0.convs.0.conv.weight shape torch.Size([64, 32, 3, 3, 3])
encoder.stages.1.0.convs.0.conv.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.norm.weight shape torch.Size([64])
encoder.stages.1.0.convs.0.norm.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.0.weight shape torch.Size([64, 32, 3, 3, 3])
encoder.stages.1.0.convs.0.all_modules.0.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.1.weight shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.1.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
encoder.stages.1.0.convs.1.conv.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.norm.weight shape torch.Size([64])
encoder.stages.1.0.convs.1.norm.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
encoder.stages.1.0.convs.1.all_modules.0.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.1.weight shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.1.bias shape torch.Size([64])
encoder.stages.2.0.convs.0.conv.weight shape torch.Size([128, 64, 3, 3, 3])
encoder.stages.2.0.convs.0.conv.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.norm.weight shape torch.Size([128])
encoder.stages.2.0.convs.0.norm.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.0.weight shape torch.Size([128, 64, 3, 3, 3])
encoder.stages.2.0.convs.0.all_modules.0.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.1.weight shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.1.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
encoder.stages.2.0.convs.1.conv.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.norm.weight shape torch.Size([128])
encoder.stages.2.0.convs.1.norm.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
encoder.stages.2.0.convs.1.all_modules.0.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.1.weight shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.1.bias shape torch.Size([128])
encoder.stages.3.0.convs.0.conv.weight shape torch.Size([256, 128, 3, 3, 3])
encoder.stages.3.0.convs.0.conv.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.norm.weight shape torch.Size([256])
encoder.stages.3.0.convs.0.norm.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.0.weight shape torch.Size([256, 128, 3, 3, 3])
encoder.stages.3.0.convs.0.all_modules.0.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.1.weight shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.1.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
encoder.stages.3.0.convs.1.conv.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.norm.weight shape torch.Size([256])
encoder.stages.3.0.convs.1.norm.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
encoder.stages.3.0.convs.1.all_modules.0.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.1.weight shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.1.bias shape torch.Size([256])
encoder.stages.4.0.convs.0.conv.weight shape torch.Size([320, 256, 3, 3, 3])
encoder.stages.4.0.convs.0.conv.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.norm.weight shape torch.Size([320])
encoder.stages.4.0.convs.0.norm.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.0.weight shape torch.Size([320, 256, 3, 3, 3])
encoder.stages.4.0.convs.0.all_modules.0.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.1.weight shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.1.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.4.0.convs.1.conv.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.norm.weight shape torch.Size([320])
encoder.stages.4.0.convs.1.norm.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.4.0.convs.1.all_modules.0.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.1.weight shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.1.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.0.conv.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.norm.weight shape torch.Size([320])
encoder.stages.5.0.convs.0.norm.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.0.all_modules.0.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.1.weight shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.1.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.1.conv.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.norm.weight shape torch.Size([320])
encoder.stages.5.0.convs.1.norm.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.1.all_modules.0.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.1.weight shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.0.0.convs.0.conv.weight shape torch.Size([32, 1, 3, 3, 3])
decoder.encoder.stages.0.0.convs.0.conv.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.norm.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.norm.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.0.weight shape torch.Size([32, 1, 3, 3, 3])
decoder.encoder.stages.0.0.convs.0.all_modules.0.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.1.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.1.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.encoder.stages.0.0.convs.1.conv.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.norm.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.norm.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.encoder.stages.0.0.convs.1.all_modules.0.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.1.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.1.bias shape torch.Size([32])
decoder.encoder.stages.1.0.convs.0.conv.weight shape torch.Size([64, 32, 3, 3, 3])
decoder.encoder.stages.1.0.convs.0.conv.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.norm.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.norm.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.0.weight shape torch.Size([64, 32, 3, 3, 3])
decoder.encoder.stages.1.0.convs.0.all_modules.0.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.1.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.1.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.encoder.stages.1.0.convs.1.conv.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.norm.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.norm.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.encoder.stages.1.0.convs.1.all_modules.0.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.1.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.1.bias shape torch.Size([64])
decoder.encoder.stages.2.0.convs.0.conv.weight shape torch.Size([128, 64, 3, 3, 3])
decoder.encoder.stages.2.0.convs.0.conv.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.norm.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.norm.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.0.weight shape torch.Size([128, 64, 3, 3, 3])
decoder.encoder.stages.2.0.convs.0.all_modules.0.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.1.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.1.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.encoder.stages.2.0.convs.1.conv.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.norm.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.norm.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.encoder.stages.2.0.convs.1.all_modules.0.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.1.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.1.bias shape torch.Size([128])
decoder.encoder.stages.3.0.convs.0.conv.weight shape torch.Size([256, 128, 3, 3, 3])
decoder.encoder.stages.3.0.convs.0.conv.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.norm.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.norm.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.0.weight shape torch.Size([256, 128, 3, 3, 3])
decoder.encoder.stages.3.0.convs.0.all_modules.0.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.1.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.1.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.encoder.stages.3.0.convs.1.conv.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.norm.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.norm.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.encoder.stages.3.0.convs.1.all_modules.0.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.1.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.1.bias shape torch.Size([256])
decoder.encoder.stages.4.0.convs.0.conv.weight shape torch.Size([320, 256, 3, 3, 3])
decoder.encoder.stages.4.0.convs.0.conv.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.norm.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.norm.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.0.weight shape torch.Size([320, 256, 3, 3, 3])
decoder.encoder.stages.4.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.4.0.convs.1.conv.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.norm.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.norm.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.4.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.0.conv.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.norm.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.norm.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.1.conv.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.norm.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.norm.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.stages.0.convs.0.conv.weight shape torch.Size([320, 640, 3, 3, 3])
decoder.stages.0.convs.0.conv.bias shape torch.Size([320])
decoder.stages.0.convs.0.norm.weight shape torch.Size([320])
decoder.stages.0.convs.0.norm.bias shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.0.weight shape torch.Size([320, 640, 3, 3, 3])
decoder.stages.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.stages.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.stages.0.convs.1.conv.bias shape torch.Size([320])
decoder.stages.0.convs.1.norm.weight shape torch.Size([320])
decoder.stages.0.convs.1.norm.bias shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.stages.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.stages.1.convs.0.conv.weight shape torch.Size([256, 512, 3, 3, 3])
decoder.stages.1.convs.0.conv.bias shape torch.Size([256])
decoder.stages.1.convs.0.norm.weight shape torch.Size([256])
decoder.stages.1.convs.0.norm.bias shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.0.weight shape torch.Size([256, 512, 3, 3, 3])
decoder.stages.1.convs.0.all_modules.0.bias shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.1.weight shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.1.bias shape torch.Size([256])
decoder.stages.1.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.stages.1.convs.1.conv.bias shape torch.Size([256])
decoder.stages.1.convs.1.norm.weight shape torch.Size([256])
decoder.stages.1.convs.1.norm.bias shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.stages.1.convs.1.all_modules.0.bias shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.1.weight shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.1.bias shape torch.Size([256])
decoder.stages.2.convs.0.conv.weight shape torch.Size([128, 256, 3, 3, 3])
decoder.stages.2.convs.0.conv.bias shape torch.Size([128])
decoder.stages.2.convs.0.norm.weight shape torch.Size([128])
decoder.stages.2.convs.0.norm.bias shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.0.weight shape torch.Size([128, 256, 3, 3, 3])
decoder.stages.2.convs.0.all_modules.0.bias shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.1.weight shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.1.bias shape torch.Size([128])
decoder.stages.2.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.stages.2.convs.1.conv.bias shape torch.Size([128])
decoder.stages.2.convs.1.norm.weight shape torch.Size([128])
decoder.stages.2.convs.1.norm.bias shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.stages.2.convs.1.all_modules.0.bias shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.1.weight shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.1.bias shape torch.Size([128])
decoder.stages.3.convs.0.conv.weight shape torch.Size([64, 128, 3, 3, 3])
decoder.stages.3.convs.0.conv.bias shape torch.Size([64])
decoder.stages.3.convs.0.norm.weight shape torch.Size([64])
decoder.stages.3.convs.0.norm.bias shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.0.weight shape torch.Size([64, 128, 3, 3, 3])
decoder.stages.3.convs.0.all_modules.0.bias shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.1.weight shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.1.bias shape torch.Size([64])
decoder.stages.3.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.stages.3.convs.1.conv.bias shape torch.Size([64])
decoder.stages.3.convs.1.norm.weight shape torch.Size([64])
decoder.stages.3.convs.1.norm.bias shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.stages.3.convs.1.all_modules.0.bias shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.1.weight shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.1.bias shape torch.Size([64])
decoder.stages.4.convs.0.conv.weight shape torch.Size([32, 64, 3, 3, 3])
decoder.stages.4.convs.0.conv.bias shape torch.Size([32])
decoder.stages.4.convs.0.norm.weight shape torch.Size([32])
decoder.stages.4.convs.0.norm.bias shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.0.weight shape torch.Size([32, 64, 3, 3, 3])
decoder.stages.4.convs.0.all_modules.0.bias shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.1.weight shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.1.bias shape torch.Size([32])
decoder.stages.4.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.stages.4.convs.1.conv.bias shape torch.Size([32])
decoder.stages.4.convs.1.norm.weight shape torch.Size([32])
decoder.stages.4.convs.1.norm.bias shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.stages.4.convs.1.all_modules.0.bias shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.1.weight shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.1.bias shape torch.Size([32])
decoder.transpconvs.0.weight shape torch.Size([320, 320, 1, 2, 2])
decoder.transpconvs.0.bias shape torch.Size([320])
decoder.transpconvs.1.weight shape torch.Size([320, 256, 2, 2, 2])
decoder.transpconvs.1.bias shape torch.Size([256])
decoder.transpconvs.2.weight shape torch.Size([256, 128, 2, 2, 2])
decoder.transpconvs.2.bias shape torch.Size([128])
decoder.transpconvs.3.weight shape torch.Size([128, 64, 2, 2, 2])
decoder.transpconvs.3.bias shape torch.Size([64])
decoder.transpconvs.4.weight shape torch.Size([64, 32, 2, 2, 2])
decoder.transpconvs.4.bias shape torch.Size([32])
################### Done ###################
2024-12-17 21:33:38.549047: do_dummy_2d_data_aug: True
2024-12-17 21:33:38.597549: Using splits from existing split file: /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset309_Sohee_Calcium_OCT_CrossValidation/splits_final_20.json
2024-12-17 21:33:38.617731: The split file contains 5 splits.
2024-12-17 21:33:38.618892: Desired fold for training: 1
2024-12-17 21:33:38.619721: This split has 1 training and 7 validation cases.
using pin_memory on device 0
using pin_memory on device 0
using pin_memory on device 0

This is the configuration used by this training:
Configuration name: 3d_32x160x128_b10
 {'data_identifier': 'nnUNetPlans_3d_fullres', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 10, 'patch_size': [32, 160, 128], 'median_image_size_in_voxels': [375.0, 498.0, 498.0], 'spacing': [1.0, 1.0, 1.0], 'normalization_schemes': ['ZScoreNormalization'], 'use_mask_for_norm': [False], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'architecture': {'network_class_name': 'dynamic_network_architectures.architectures.unet.PlainConvUNet', 'arch_kwargs': {'n_stages': 6, 'features_per_stage': [32, 64, 128, 256, 320, 320], 'conv_op': 'torch.nn.modules.conv.Conv3d', 'kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]], 'strides': [[1, 1, 1], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [1, 2, 2]], 'n_conv_per_stage': [2, 2, 2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2, 2, 2], 'conv_bias': True, 'norm_op': 'torch.nn.modules.instancenorm.InstanceNorm3d', 'norm_op_kwargs': {'eps': 1e-05, 'affine': True}, 'dropout_op': None, 'dropout_op_kwargs': None, 'nonlin': 'torch.nn.LeakyReLU', 'nonlin_kwargs': {'inplace': True}, 'deep_supervision': True}, '_kw_requires_import': ['conv_op', 'norm_op', 'dropout_op', 'nonlin']}, 'batch_dice': True, 'inherits_from': '3d_fullres'} 

These are the global plan.json settings:
 {'dataset_name': 'Dataset309_Sohee_Calcium_OCT_CrossValidation', 'plans_name': 'nnUNetPlans', 'original_median_spacing_after_transp': [1.0, 1.0, 1.0], 'original_median_shape_after_transp': [375, 498, 498], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 0.4977976381778717, 'mean': 0.13507525622844696, 'median': 0.09599608182907104, 'min': 0.0, 'percentile_00_5': 0.014754901640117168, 'percentile_99_5': 0.4977976381778717, 'std': 0.12025152146816254}}} 

2024-12-17 21:34:05.694668: unpacking dataset...
using pin_memory on device 0

This is the configuration used by this training:
Configuration name: 3d_32x160x128_b10
 {'data_identifier': 'nnUNetPlans_3d_fullres', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 10, 'patch_size': [32, 160, 128], 'median_image_size_in_voxels': [375.0, 498.0, 498.0], 'spacing': [1.0, 1.0, 1.0], 'normalization_schemes': ['ZScoreNormalization'], 'use_mask_for_norm': [False], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'architecture': {'network_class_name': 'dynamic_network_architectures.architectures.unet.PlainConvUNet', 'arch_kwargs': {'n_stages': 6, 'features_per_stage': [32, 64, 128, 256, 320, 320], 'conv_op': 'torch.nn.modules.conv.Conv3d', 'kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]], 'strides': [[1, 1, 1], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [1, 2, 2]], 'n_conv_per_stage': [2, 2, 2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2, 2, 2], 'conv_bias': True, 'norm_op': 'torch.nn.modules.instancenorm.InstanceNorm3d', 'norm_op_kwargs': {'eps': 1e-05, 'affine': True}, 'dropout_op': None, 'dropout_op_kwargs': None, 'nonlin': 'torch.nn.LeakyReLU', 'nonlin_kwargs': {'inplace': True}, 'deep_supervision': True}, '_kw_requires_import': ['conv_op', 'norm_op', 'dropout_op', 'nonlin']}, 'batch_dice': True, 'inherits_from': '3d_fullres'} 

These are the global plan.json settings:
 {'dataset_name': 'Dataset309_Sohee_Calcium_OCT_CrossValidation', 'plans_name': 'nnUNetPlans', 'original_median_spacing_after_transp': [1.0, 1.0, 1.0], 'original_median_shape_after_transp': [375, 498, 498], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 0.4977976381778717, 'mean': 0.13507525622844696, 'median': 0.09599608182907104, 'min': 0.0, 'percentile_00_5': 0.014754901640117168, 'percentile_99_5': 0.4977976381778717, 'std': 0.12025152146816254}}} 

2024-12-17 21:34:10.067689: unpacking dataset...
2024-12-17 21:34:14.391083: unpacking done...
2024-12-17 21:34:14.402616: Unable to plot network architecture: nnUNet_compile is enabled!
2024-12-17 21:34:14.486238: 
2024-12-17 21:34:14.487335: Epoch 0
2024-12-17 21:34:14.488170: Current learning rate: 0.01
2024-12-17 21:41:36.720827: Validation loss improved from 1000.00000 to -0.32872! Patience: 0/50
2024-12-17 21:41:36.721860: train_loss -0.4096
2024-12-17 21:41:36.722650: val_loss -0.3287
2024-12-17 21:41:36.723317: Pseudo dice [0.6295]
2024-12-17 21:41:36.723985: Epoch time: 442.24 s
2024-12-17 21:41:36.724554: Yayy! New best EMA pseudo Dice: 0.6295
2024-12-17 21:41:38.426740: 
2024-12-17 21:41:38.427915: Epoch 1
2024-12-17 21:41:38.428613: Current learning rate: 0.00994
2024-12-17 21:47:57.557386: Validation loss improved from -0.32872 to -0.36777! Patience: 0/50
2024-12-17 21:47:57.558244: train_loss -0.6185
2024-12-17 21:47:57.558945: val_loss -0.3678
2024-12-17 21:47:57.559622: Pseudo dice [0.6863]
2024-12-17 21:47:57.560391: Epoch time: 379.13 s
2024-12-17 21:47:57.560960: Yayy! New best EMA pseudo Dice: 0.6352
2024-12-17 21:47:59.416981: 
2024-12-17 21:47:59.418154: Epoch 2
2024-12-17 21:47:59.419093: Current learning rate: 0.00988
2024-12-17 21:53:32.937756: Validation loss improved from -0.36777 to -0.38036! Patience: 0/50
2024-12-17 21:53:32.938634: train_loss -0.6669
2024-12-17 21:53:32.939516: val_loss -0.3804
2024-12-17 21:53:32.940269: Pseudo dice [0.6783]
2024-12-17 21:53:32.941016: Epoch time: 333.52 s
2024-12-17 21:53:32.941698: Yayy! New best EMA pseudo Dice: 0.6395
2024-12-17 21:53:34.819447: 
2024-12-17 21:53:34.820579: Epoch 3
2024-12-17 21:53:34.821370: Current learning rate: 0.00982
2024-12-17 21:59:35.095119: Validation loss did not improve from -0.38036. Patience: 1/50
2024-12-17 21:59:35.096139: train_loss -0.6983
2024-12-17 21:59:35.097138: val_loss -0.3559
2024-12-17 21:59:35.098027: Pseudo dice [0.669]
2024-12-17 21:59:35.098880: Epoch time: 360.28 s
2024-12-17 21:59:35.099607: Yayy! New best EMA pseudo Dice: 0.6425
2024-12-17 21:59:36.967355: 
2024-12-17 21:59:36.968617: Epoch 4
2024-12-17 21:59:36.969364: Current learning rate: 0.00976
2024-12-17 22:07:18.013680: Validation loss improved from -0.38036 to -0.40875! Patience: 1/50
2024-12-17 22:07:18.014581: train_loss -0.7177
2024-12-17 22:07:18.015338: val_loss -0.4088
2024-12-17 22:07:18.016041: Pseudo dice [0.6961]
2024-12-17 22:07:18.016712: Epoch time: 461.05 s
2024-12-17 22:07:18.402491: Yayy! New best EMA pseudo Dice: 0.6478
2024-12-17 22:07:20.364257: 
2024-12-17 22:07:20.365446: Epoch 5
2024-12-17 22:07:20.366197: Current learning rate: 0.0097
2024-12-17 22:14:35.216917: Validation loss did not improve from -0.40875. Patience: 1/50
2024-12-17 22:14:35.217914: train_loss -0.7373
2024-12-17 22:14:35.219176: val_loss -0.4032
2024-12-17 22:14:35.220052: Pseudo dice [0.6989]
2024-12-17 22:14:35.220975: Epoch time: 434.86 s
2024-12-17 22:14:35.221820: Yayy! New best EMA pseudo Dice: 0.653
2024-12-17 22:14:36.995993: 
2024-12-17 22:14:36.997342: Epoch 6
2024-12-17 22:14:36.998251: Current learning rate: 0.00964
2024-12-17 22:22:21.843020: Validation loss did not improve from -0.40875. Patience: 2/50
2024-12-17 22:22:21.844034: train_loss -0.7444
2024-12-17 22:22:21.844823: val_loss -0.3769
2024-12-17 22:22:21.845510: Pseudo dice [0.6713]
2024-12-17 22:22:21.846291: Epoch time: 464.85 s
2024-12-17 22:22:21.847045: Yayy! New best EMA pseudo Dice: 0.6548
2024-12-17 22:22:23.679225: 
2024-12-17 22:22:23.680552: Epoch 7
2024-12-17 22:22:23.681398: Current learning rate: 0.00958
2024-12-17 22:30:13.928405: Validation loss did not improve from -0.40875. Patience: 3/50
2024-12-17 22:30:13.929365: train_loss -0.7488
2024-12-17 22:30:13.930102: val_loss -0.3783
2024-12-17 22:30:13.930802: Pseudo dice [0.6841]
2024-12-17 22:30:13.931535: Epoch time: 470.25 s
2024-12-17 22:30:13.932180: Yayy! New best EMA pseudo Dice: 0.6577
2024-12-17 22:30:15.758339: 
2024-12-17 22:30:15.759638: Epoch 8
2024-12-17 22:30:15.760566: Current learning rate: 0.00952
2024-12-17 22:38:02.659851: Validation loss did not improve from -0.40875. Patience: 4/50
2024-12-17 22:38:02.661220: train_loss -0.7586
2024-12-17 22:38:02.662042: val_loss -0.3301
2024-12-17 22:38:02.662708: Pseudo dice [0.6609]
2024-12-17 22:38:02.663424: Epoch time: 466.9 s
2024-12-17 22:38:02.664170: Yayy! New best EMA pseudo Dice: 0.658
2024-12-17 22:38:04.941216: 
2024-12-17 22:38:04.942352: Epoch 9
2024-12-17 22:38:04.943098: Current learning rate: 0.00946
2024-12-17 22:45:50.227028: Validation loss did not improve from -0.40875. Patience: 5/50
2024-12-17 22:45:50.228227: train_loss -0.7683
2024-12-17 22:45:50.229225: val_loss -0.3884
2024-12-17 22:45:50.230033: Pseudo dice [0.7]
2024-12-17 22:45:50.230788: Epoch time: 465.29 s
2024-12-17 22:45:50.616656: Yayy! New best EMA pseudo Dice: 0.6622
2024-12-17 22:45:52.350859: 
2024-12-17 22:45:52.352116: Epoch 10
2024-12-17 22:45:52.352962: Current learning rate: 0.0094
2024-12-17 22:53:35.470395: Validation loss did not improve from -0.40875. Patience: 6/50
2024-12-17 22:53:35.471181: train_loss -0.7722
2024-12-17 22:53:35.471975: val_loss -0.343
2024-12-17 22:53:35.472741: Pseudo dice [0.6735]
2024-12-17 22:53:35.473546: Epoch time: 463.12 s
2024-12-17 22:53:35.474285: Yayy! New best EMA pseudo Dice: 0.6634
2024-12-17 22:53:37.228795: 
2024-12-17 22:53:37.230203: Epoch 11
2024-12-17 22:53:37.231292: Current learning rate: 0.00934
2024-12-17 23:01:10.661344: Validation loss did not improve from -0.40875. Patience: 7/50
2024-12-17 23:01:10.662308: train_loss -0.7771
2024-12-17 23:01:10.663276: val_loss -0.37
2024-12-17 23:01:10.664086: Pseudo dice [0.6978]
2024-12-17 23:01:10.664881: Epoch time: 453.44 s
2024-12-17 23:01:10.665610: Yayy! New best EMA pseudo Dice: 0.6668
2024-12-17 23:01:12.488194: 
2024-12-17 23:01:12.489542: Epoch 12
2024-12-17 23:01:12.490350: Current learning rate: 0.00928
2024-12-17 23:08:10.325500: Validation loss did not improve from -0.40875. Patience: 8/50
2024-12-17 23:08:10.326531: train_loss -0.7843
2024-12-17 23:08:10.328110: val_loss -0.3422
2024-12-17 23:08:10.328806: Pseudo dice [0.6724]
2024-12-17 23:08:10.329654: Epoch time: 417.84 s
2024-12-17 23:08:10.330348: Yayy! New best EMA pseudo Dice: 0.6674
2024-12-17 23:08:12.123569: 
2024-12-17 23:08:12.124828: Epoch 13
2024-12-17 23:08:12.125643: Current learning rate: 0.00922
2024-12-17 23:15:35.481825: Validation loss did not improve from -0.40875. Patience: 9/50
2024-12-17 23:15:35.482549: train_loss -0.7808
2024-12-17 23:15:35.483461: val_loss -0.3255
2024-12-17 23:15:35.484465: Pseudo dice [0.6612]
2024-12-17 23:15:35.485302: Epoch time: 443.36 s
2024-12-17 23:15:36.987533: 
2024-12-17 23:15:36.988758: Epoch 14
2024-12-17 23:15:36.989586: Current learning rate: 0.00916
2024-12-17 23:23:10.108226: Validation loss did not improve from -0.40875. Patience: 10/50
2024-12-17 23:23:10.109313: train_loss -0.7852
2024-12-17 23:23:10.110136: val_loss -0.3858
2024-12-17 23:23:10.111003: Pseudo dice [0.6972]
2024-12-17 23:23:10.111748: Epoch time: 453.12 s
2024-12-17 23:23:10.532851: Yayy! New best EMA pseudo Dice: 0.6698
2024-12-17 23:23:12.387981: 
2024-12-17 23:23:12.389008: Epoch 15
2024-12-17 23:23:12.389713: Current learning rate: 0.0091
2024-12-17 23:30:44.404876: Validation loss did not improve from -0.40875. Patience: 11/50
2024-12-17 23:30:44.405900: train_loss -0.7914
2024-12-17 23:30:44.406884: val_loss -0.3681
2024-12-17 23:30:44.407826: Pseudo dice [0.6927]
2024-12-17 23:30:44.408738: Epoch time: 452.02 s
2024-12-17 23:30:44.409560: Yayy! New best EMA pseudo Dice: 0.6721
2024-12-17 23:30:46.282082: 
2024-12-17 23:30:46.283116: Epoch 16
2024-12-17 23:30:46.283919: Current learning rate: 0.00903
2024-12-17 23:37:50.525721: Validation loss did not improve from -0.40875. Patience: 12/50
2024-12-17 23:37:50.527793: train_loss -0.7975
2024-12-17 23:37:50.528557: val_loss -0.3983
2024-12-17 23:37:50.529201: Pseudo dice [0.6935]
2024-12-17 23:37:50.529949: Epoch time: 424.25 s
2024-12-17 23:37:50.530607: Yayy! New best EMA pseudo Dice: 0.6742
2024-12-17 23:37:52.401473: 
2024-12-17 23:37:52.402512: Epoch 17
2024-12-17 23:37:52.403230: Current learning rate: 0.00897
2024-12-17 23:45:33.233479: Validation loss did not improve from -0.40875. Patience: 13/50
2024-12-17 23:45:33.234705: train_loss -0.8006
2024-12-17 23:45:33.236714: val_loss -0.3451
2024-12-17 23:45:33.237693: Pseudo dice [0.6907]
2024-12-17 23:45:33.238749: Epoch time: 460.83 s
2024-12-17 23:45:33.239638: Yayy! New best EMA pseudo Dice: 0.6759
2024-12-17 23:45:35.408851: 
2024-12-17 23:45:35.410060: Epoch 18
2024-12-17 23:45:35.411132: Current learning rate: 0.00891
2024-12-17 23:51:50.769267: Validation loss did not improve from -0.40875. Patience: 14/50
2024-12-17 23:51:50.770347: train_loss -0.7993
2024-12-17 23:51:50.771030: val_loss -0.3423
2024-12-17 23:51:50.771682: Pseudo dice [0.6725]
2024-12-17 23:51:50.772434: Epoch time: 375.36 s
2024-12-17 23:51:52.322390: 
2024-12-17 23:51:52.323605: Epoch 19
2024-12-17 23:51:52.324386: Current learning rate: 0.00885
2024-12-17 23:59:31.547843: Validation loss did not improve from -0.40875. Patience: 15/50
2024-12-17 23:59:31.550000: train_loss -0.7997
2024-12-17 23:59:31.550885: val_loss -0.373
2024-12-17 23:59:31.551541: Pseudo dice [0.6966]
2024-12-17 23:59:31.552554: Epoch time: 459.23 s
2024-12-17 23:59:31.951891: Yayy! New best EMA pseudo Dice: 0.6776
2024-12-17 23:59:34.975503: 
2024-12-17 23:59:34.976676: Epoch 20
2024-12-17 23:59:34.977400: Current learning rate: 0.00879
2024-12-18 00:07:16.964170: Validation loss did not improve from -0.40875. Patience: 16/50
2024-12-18 00:07:16.965152: train_loss -0.8026
2024-12-18 00:07:16.966014: val_loss -0.3535
2024-12-18 00:07:16.966856: Pseudo dice [0.6867]
2024-12-18 00:07:16.967677: Epoch time: 461.99 s
2024-12-18 00:07:16.968573: Yayy! New best EMA pseudo Dice: 0.6785
2024-12-18 00:07:18.855525: 
2024-12-18 00:07:18.856843: Epoch 21
2024-12-18 00:07:18.857608: Current learning rate: 0.00873
2024-12-18 00:14:20.739315: Validation loss did not improve from -0.40875. Patience: 17/50
2024-12-18 00:14:20.740248: train_loss -0.812
2024-12-18 00:14:20.741002: val_loss -0.3461
2024-12-18 00:14:20.741611: Pseudo dice [0.6915]
2024-12-18 00:14:20.742241: Epoch time: 421.89 s
2024-12-18 00:14:20.742828: Yayy! New best EMA pseudo Dice: 0.6798
2024-12-18 00:14:22.574536: 
2024-12-18 00:14:22.575577: Epoch 22
2024-12-18 00:14:22.576435: Current learning rate: 0.00867
2024-12-18 00:21:38.984210: Validation loss did not improve from -0.40875. Patience: 18/50
2024-12-18 00:21:38.985086: train_loss -0.812
2024-12-18 00:21:38.985960: val_loss -0.3264
2024-12-18 00:21:38.986585: Pseudo dice [0.6736]
2024-12-18 00:21:38.987301: Epoch time: 436.41 s
2024-12-18 00:21:40.384918: 
2024-12-18 00:21:40.386196: Epoch 23
2024-12-18 00:21:40.386948: Current learning rate: 0.00861
2024-12-18 00:28:18.825272: Validation loss did not improve from -0.40875. Patience: 19/50
2024-12-18 00:28:18.826147: train_loss -0.8128
2024-12-18 00:28:18.827061: val_loss -0.3499
2024-12-18 00:28:18.827837: Pseudo dice [0.6781]
2024-12-18 00:28:18.828500: Epoch time: 398.44 s
2024-12-18 00:28:20.201028: 
2024-12-18 00:28:20.202763: Epoch 24
2024-12-18 00:28:20.203470: Current learning rate: 0.00855
2024-12-18 00:35:49.598163: Validation loss did not improve from -0.40875. Patience: 20/50
2024-12-18 00:35:49.599272: train_loss -0.8157
2024-12-18 00:35:49.600008: val_loss -0.373
2024-12-18 00:35:49.600643: Pseudo dice [0.7041]
2024-12-18 00:35:49.601468: Epoch time: 449.4 s
2024-12-18 00:35:50.046054: Yayy! New best EMA pseudo Dice: 0.6816
2024-12-18 00:35:51.925881: 
2024-12-18 00:35:51.927319: Epoch 25
2024-12-18 00:35:51.928083: Current learning rate: 0.00849
2024-12-18 00:43:36.886440: Validation loss did not improve from -0.40875. Patience: 21/50
2024-12-18 00:43:36.888306: train_loss -0.8149
2024-12-18 00:43:36.889318: val_loss -0.3347
2024-12-18 00:43:36.890107: Pseudo dice [0.6737]
2024-12-18 00:43:36.891019: Epoch time: 464.96 s
2024-12-18 00:43:38.391504: 
2024-12-18 00:43:38.392746: Epoch 26
2024-12-18 00:43:38.393635: Current learning rate: 0.00843
2024-12-18 00:51:18.147443: Validation loss did not improve from -0.40875. Patience: 22/50
2024-12-18 00:51:18.161702: train_loss -0.8173
2024-12-18 00:51:18.164061: val_loss -0.3202
2024-12-18 00:51:18.164970: Pseudo dice [0.6714]
2024-12-18 00:51:18.165809: Epoch time: 459.77 s
2024-12-18 00:51:19.610269: 
2024-12-18 00:51:19.611310: Epoch 27
2024-12-18 00:51:19.612087: Current learning rate: 0.00836
2024-12-18 00:58:41.767117: Validation loss did not improve from -0.40875. Patience: 23/50
2024-12-18 00:58:41.767978: train_loss -0.8163
2024-12-18 00:58:41.768661: val_loss -0.3194
2024-12-18 00:58:41.769316: Pseudo dice [0.6671]
2024-12-18 00:58:41.769954: Epoch time: 442.16 s
2024-12-18 00:58:43.183895: 
2024-12-18 00:58:43.184921: Epoch 28
2024-12-18 00:58:43.185578: Current learning rate: 0.0083
2024-12-18 01:06:16.398880: Validation loss did not improve from -0.40875. Patience: 24/50
2024-12-18 01:06:16.400008: train_loss -0.819
2024-12-18 01:06:16.400824: val_loss -0.368
2024-12-18 01:06:16.401620: Pseudo dice [0.681]
2024-12-18 01:06:16.402428: Epoch time: 453.22 s
2024-12-18 01:06:17.842807: 
2024-12-18 01:06:17.843895: Epoch 29
2024-12-18 01:06:17.844675: Current learning rate: 0.00824
2024-12-18 01:14:01.632636: Validation loss did not improve from -0.40875. Patience: 25/50
2024-12-18 01:14:01.633632: train_loss -0.8162
2024-12-18 01:14:01.634576: val_loss -0.3283
2024-12-18 01:14:01.635541: Pseudo dice [0.6813]
2024-12-18 01:14:01.636458: Epoch time: 463.79 s
2024-12-18 01:14:03.796850: 
2024-12-18 01:14:03.798350: Epoch 30
2024-12-18 01:14:03.799395: Current learning rate: 0.00818
2024-12-18 01:21:36.379563: Validation loss did not improve from -0.40875. Patience: 26/50
2024-12-18 01:21:36.380202: train_loss -0.823
2024-12-18 01:21:36.380947: val_loss -0.3027
2024-12-18 01:21:36.381634: Pseudo dice [0.6736]
2024-12-18 01:21:36.382348: Epoch time: 452.58 s
2024-12-18 01:21:37.811323: 
2024-12-18 01:21:37.812710: Epoch 31
2024-12-18 01:21:37.813785: Current learning rate: 0.00812
2024-12-18 01:28:43.075197: Validation loss did not improve from -0.40875. Patience: 27/50
2024-12-18 01:28:43.076265: train_loss -0.8214
2024-12-18 01:28:43.077192: val_loss -0.3071
2024-12-18 01:28:43.077906: Pseudo dice [0.6784]
2024-12-18 01:28:43.078671: Epoch time: 425.27 s
2024-12-18 01:28:44.541700: 
2024-12-18 01:28:44.542867: Epoch 32
2024-12-18 01:28:44.543743: Current learning rate: 0.00806
2024-12-18 01:36:05.825243: Validation loss did not improve from -0.40875. Patience: 28/50
2024-12-18 01:36:05.826103: train_loss -0.8253
2024-12-18 01:36:05.826809: val_loss -0.3076
2024-12-18 01:36:05.827499: Pseudo dice [0.686]
2024-12-18 01:36:05.828147: Epoch time: 441.29 s
2024-12-18 01:36:07.246577: 
2024-12-18 01:36:07.247585: Epoch 33
2024-12-18 01:36:07.248210: Current learning rate: 0.008
2024-12-18 01:43:40.864644: Validation loss did not improve from -0.40875. Patience: 29/50
2024-12-18 01:43:40.865757: train_loss -0.824
2024-12-18 01:43:40.866757: val_loss -0.3093
2024-12-18 01:43:40.867675: Pseudo dice [0.6714]
2024-12-18 01:43:40.868606: Epoch time: 453.62 s
2024-12-18 01:43:42.273151: 
2024-12-18 01:43:42.274430: Epoch 34
2024-12-18 01:43:42.275268: Current learning rate: 0.00793
2024-12-18 01:51:06.990382: Validation loss did not improve from -0.40875. Patience: 30/50
2024-12-18 01:51:06.991355: train_loss -0.8266
2024-12-18 01:51:06.992325: val_loss -0.3415
2024-12-18 01:51:06.993132: Pseudo dice [0.6805]
2024-12-18 01:51:06.993759: Epoch time: 444.72 s
2024-12-18 01:51:08.934163: 
2024-12-18 01:51:08.935765: Epoch 35
2024-12-18 01:51:08.936793: Current learning rate: 0.00787
2024-12-18 01:57:42.190194: Validation loss did not improve from -0.40875. Patience: 31/50
2024-12-18 01:57:42.192862: train_loss -0.8279
2024-12-18 01:57:42.194155: val_loss -0.366
2024-12-18 01:57:42.195052: Pseudo dice [0.6984]
2024-12-18 01:57:42.195793: Epoch time: 393.26 s
2024-12-18 01:57:43.719578: 
2024-12-18 01:57:43.720972: Epoch 36
2024-12-18 01:57:43.721793: Current learning rate: 0.00781
2024-12-18 02:04:32.262859: Validation loss did not improve from -0.40875. Patience: 32/50
2024-12-18 02:04:32.264026: train_loss -0.8341
2024-12-18 02:04:32.265036: val_loss -0.2947
2024-12-18 02:04:32.265794: Pseudo dice [0.6731]
2024-12-18 02:04:32.266609: Epoch time: 408.55 s
2024-12-18 02:04:33.728948: 
2024-12-18 02:04:33.730072: Epoch 37
2024-12-18 02:04:33.730865: Current learning rate: 0.00775
2024-12-18 02:11:07.601357: Validation loss did not improve from -0.40875. Patience: 33/50
2024-12-18 02:11:07.603839: train_loss -0.8308
2024-12-18 02:11:07.604914: val_loss -0.3302
2024-12-18 02:11:07.605740: Pseudo dice [0.6797]
2024-12-18 02:11:07.606945: Epoch time: 393.88 s
2024-12-18 02:11:09.131480: 
2024-12-18 02:11:09.132761: Epoch 38
2024-12-18 02:11:09.133634: Current learning rate: 0.00769
2024-12-18 02:16:26.742314: Validation loss did not improve from -0.40875. Patience: 34/50
2024-12-18 02:16:26.742992: train_loss -0.8329
2024-12-18 02:16:26.743797: val_loss -0.3362
2024-12-18 02:16:26.744530: Pseudo dice [0.6821]
2024-12-18 02:16:26.745434: Epoch time: 317.61 s
2024-12-18 02:16:28.245245: 
2024-12-18 02:16:28.246395: Epoch 39
2024-12-18 02:16:28.247322: Current learning rate: 0.00763
2024-12-18 02:22:03.845274: Validation loss did not improve from -0.40875. Patience: 35/50
2024-12-18 02:22:03.846290: train_loss -0.8323
2024-12-18 02:22:03.847159: val_loss -0.3077
2024-12-18 02:22:03.847816: Pseudo dice [0.6835]
2024-12-18 02:22:03.848462: Epoch time: 335.6 s
2024-12-18 02:22:05.743264: 
2024-12-18 02:22:05.744620: Epoch 40
2024-12-18 02:22:05.745463: Current learning rate: 0.00756
2024-12-18 02:29:00.149108: Validation loss did not improve from -0.40875. Patience: 36/50
2024-12-18 02:29:00.150782: train_loss -0.8334
2024-12-18 02:29:00.151754: val_loss -0.3468
2024-12-18 02:29:00.152433: Pseudo dice [0.6966]
2024-12-18 02:29:00.153090: Epoch time: 414.41 s
2024-12-18 02:29:00.153732: Yayy! New best EMA pseudo Dice: 0.6821
2024-12-18 02:29:02.823788: 
2024-12-18 02:29:02.825113: Epoch 41
2024-12-18 02:29:02.825893: Current learning rate: 0.0075
2024-12-18 02:36:05.935826: Validation loss did not improve from -0.40875. Patience: 37/50
2024-12-18 02:36:05.936799: train_loss -0.8351
2024-12-18 02:36:05.937670: val_loss -0.2866
2024-12-18 02:36:05.938490: Pseudo dice [0.6698]
2024-12-18 02:36:05.939364: Epoch time: 423.11 s
2024-12-18 02:36:07.355537: 
2024-12-18 02:36:07.356919: Epoch 42
2024-12-18 02:36:07.357784: Current learning rate: 0.00744
2024-12-18 02:42:31.521791: Validation loss did not improve from -0.40875. Patience: 38/50
2024-12-18 02:42:31.522657: train_loss -0.8371
2024-12-18 02:42:31.523423: val_loss -0.3136
2024-12-18 02:42:31.524060: Pseudo dice [0.6819]
2024-12-18 02:42:31.524779: Epoch time: 384.17 s
2024-12-18 02:42:32.894747: 
2024-12-18 02:42:32.895867: Epoch 43
2024-12-18 02:42:32.896649: Current learning rate: 0.00738
2024-12-18 02:49:15.997548: Validation loss did not improve from -0.40875. Patience: 39/50
2024-12-18 02:49:15.998202: train_loss -0.8389
2024-12-18 02:49:15.999357: val_loss -0.2809
2024-12-18 02:49:16.000652: Pseudo dice [0.6701]
2024-12-18 02:49:16.001642: Epoch time: 403.1 s
2024-12-18 02:49:17.416847: 
2024-12-18 02:49:17.418298: Epoch 44
2024-12-18 02:49:17.419426: Current learning rate: 0.00732
2024-12-18 02:56:05.911672: Validation loss did not improve from -0.40875. Patience: 40/50
2024-12-18 02:56:05.912469: train_loss -0.8383
2024-12-18 02:56:05.913353: val_loss -0.2978
2024-12-18 02:56:05.914088: Pseudo dice [0.6741]
2024-12-18 02:56:05.914998: Epoch time: 408.5 s
2024-12-18 02:56:07.814524: 
2024-12-18 02:56:07.815672: Epoch 45
2024-12-18 02:56:07.816532: Current learning rate: 0.00725
2024-12-18 03:02:41.770836: Validation loss did not improve from -0.40875. Patience: 41/50
2024-12-18 03:02:41.771902: train_loss -0.8402
2024-12-18 03:02:41.772778: val_loss -0.3141
2024-12-18 03:02:41.773586: Pseudo dice [0.673]
2024-12-18 03:02:41.774374: Epoch time: 393.96 s
2024-12-18 03:02:43.152246: 
2024-12-18 03:02:43.153479: Epoch 46
2024-12-18 03:02:43.154265: Current learning rate: 0.00719
2024-12-18 03:09:04.322608: Validation loss did not improve from -0.40875. Patience: 42/50
2024-12-18 03:09:04.323689: train_loss -0.8387
2024-12-18 03:09:04.324442: val_loss -0.3455
2024-12-18 03:09:04.325227: Pseudo dice [0.6887]
2024-12-18 03:09:04.325932: Epoch time: 381.17 s
2024-12-18 03:09:05.743863: 
2024-12-18 03:09:05.745011: Epoch 47
2024-12-18 03:09:05.745817: Current learning rate: 0.00713
2024-12-18 03:15:37.826328: Validation loss did not improve from -0.40875. Patience: 43/50
2024-12-18 03:15:37.829016: train_loss -0.8435
2024-12-18 03:15:37.830116: val_loss -0.332
2024-12-18 03:15:37.831162: Pseudo dice [0.6925]
2024-12-18 03:15:37.832259: Epoch time: 392.09 s
2024-12-18 03:15:39.304611: 
2024-12-18 03:15:39.305868: Epoch 48
2024-12-18 03:15:39.306610: Current learning rate: 0.00707
2024-12-18 03:21:43.362992: Validation loss did not improve from -0.40875. Patience: 44/50
2024-12-18 03:21:43.363853: train_loss -0.8447
2024-12-18 03:21:43.365000: val_loss -0.3015
2024-12-18 03:21:43.365794: Pseudo dice [0.6722]
2024-12-18 03:21:43.366635: Epoch time: 364.06 s
2024-12-18 03:21:44.793719: 
2024-12-18 03:21:44.794924: Epoch 49
2024-12-18 03:21:44.795732: Current learning rate: 0.007
2024-12-18 03:28:37.622794: Validation loss did not improve from -0.40875. Patience: 45/50
2024-12-18 03:28:37.623649: train_loss -0.8452
2024-12-18 03:28:37.624420: val_loss -0.2705
2024-12-18 03:28:37.625079: Pseudo dice [0.6623]
2024-12-18 03:28:37.625747: Epoch time: 412.83 s
2024-12-18 03:28:39.454189: 
2024-12-18 03:28:39.455530: Epoch 50
2024-12-18 03:28:39.456352: Current learning rate: 0.00694
2024-12-18 03:34:37.045165: Validation loss did not improve from -0.40875. Patience: 46/50
2024-12-18 03:34:37.046171: train_loss -0.8469
2024-12-18 03:34:37.047115: val_loss -0.2805
2024-12-18 03:34:37.047852: Pseudo dice [0.6728]
2024-12-18 03:34:37.048590: Epoch time: 357.59 s
2024-12-18 03:34:38.505158: 
2024-12-18 03:34:38.506353: Epoch 51
2024-12-18 03:34:38.507317: Current learning rate: 0.00688
2024-12-18 03:41:36.093277: Validation loss did not improve from -0.40875. Patience: 47/50
2024-12-18 03:41:36.094160: train_loss -0.8446
2024-12-18 03:41:36.094998: val_loss -0.3114
2024-12-18 03:41:36.095749: Pseudo dice [0.6734]
2024-12-18 03:41:36.097725: Epoch time: 417.59 s
2024-12-18 03:41:38.083669: 
2024-12-18 03:41:38.085047: Epoch 52
2024-12-18 03:41:38.085928: Current learning rate: 0.00682
2024-12-18 03:48:18.237300: Validation loss did not improve from -0.40875. Patience: 48/50
2024-12-18 03:48:18.238225: train_loss -0.8467
2024-12-18 03:48:18.238999: val_loss -0.3048
2024-12-18 03:48:18.239686: Pseudo dice [0.6754]
2024-12-18 03:48:18.240374: Epoch time: 400.16 s
2024-12-18 03:48:19.774833: 
2024-12-18 03:48:19.776176: Epoch 53
2024-12-18 03:48:19.777046: Current learning rate: 0.00675
2024-12-18 03:55:03.306770: Validation loss did not improve from -0.40875. Patience: 49/50
2024-12-18 03:55:03.307721: train_loss -0.8484
2024-12-18 03:55:03.308599: val_loss -0.2785
2024-12-18 03:55:03.309390: Pseudo dice [0.6743]
2024-12-18 03:55:03.310210: Epoch time: 403.53 s
2024-12-18 03:55:04.766853: 
2024-12-18 03:55:04.768349: Epoch 54
2024-12-18 03:55:04.769393: Current learning rate: 0.00669
2024-12-18 04:02:03.406433: Validation loss did not improve from -0.40875. Patience: 50/50
2024-12-18 04:02:03.407555: train_loss -0.8485
2024-12-18 04:02:03.408406: val_loss -0.3029
2024-12-18 04:02:03.409179: Pseudo dice [0.6768]
2024-12-18 04:02:03.409973: Epoch time: 418.64 s
2024-12-18 04:02:05.313757: 
2024-12-18 04:02:05.314772: Epoch 55
2024-12-18 04:02:05.315579: Current learning rate: 0.00663
2024-12-18 04:08:53.381866: Validation loss did not improve from -0.40875. Patience: 51/50
2024-12-18 04:08:53.383099: train_loss -0.8504
2024-12-18 04:08:53.384137: val_loss -0.3071
2024-12-18 04:08:53.384936: Pseudo dice [0.6856]
2024-12-18 04:08:53.385764: Epoch time: 408.07 s
2024-12-18 04:08:54.815396: 
2024-12-18 04:08:54.816631: Epoch 56
2024-12-18 04:08:54.817318: Current learning rate: 0.00657
2024-12-18 04:15:34.720595: Validation loss did not improve from -0.40875. Patience: 52/50
2024-12-18 04:15:34.721539: train_loss -0.8512
2024-12-18 04:15:34.722492: val_loss -0.3029
2024-12-18 04:15:34.723144: Pseudo dice [0.6856]
2024-12-18 04:15:34.723788: Epoch time: 399.91 s
2024-12-18 04:15:36.151431: 
2024-12-18 04:15:36.153000: Epoch 57
2024-12-18 04:15:36.153814: Current learning rate: 0.0065
2024-12-18 04:22:25.139568: Validation loss did not improve from -0.40875. Patience: 53/50
2024-12-18 04:22:25.140562: train_loss -0.8533
2024-12-18 04:22:25.141452: val_loss -0.2826
2024-12-18 04:22:25.142141: Pseudo dice [0.6716]
2024-12-18 04:22:25.142918: Epoch time: 408.99 s
2024-12-18 04:22:26.568429: 
2024-12-18 04:22:26.569651: Epoch 58
2024-12-18 04:22:26.570465: Current learning rate: 0.00644
2024-12-18 04:29:59.661675: Validation loss did not improve from -0.40875. Patience: 54/50
2024-12-18 04:29:59.662533: train_loss -0.8537
2024-12-18 04:29:59.663497: val_loss -0.2521
2024-12-18 04:29:59.664515: Pseudo dice [0.6672]
2024-12-18 04:29:59.665407: Epoch time: 453.1 s
2024-12-18 04:30:01.183217: 
2024-12-18 04:30:01.184581: Epoch 59
2024-12-18 04:30:01.185547: Current learning rate: 0.00638
2024-12-18 04:37:07.268786: Validation loss did not improve from -0.40875. Patience: 55/50
2024-12-18 04:37:07.269789: train_loss -0.8537
2024-12-18 04:37:07.270627: val_loss -0.2763
2024-12-18 04:37:07.271409: Pseudo dice [0.6645]
2024-12-18 04:37:07.272183: Epoch time: 426.09 s
2024-12-18 04:37:09.110023: 
2024-12-18 04:37:09.111294: Epoch 60
2024-12-18 04:37:09.112267: Current learning rate: 0.00631
2024-12-18 04:43:35.624780: Validation loss did not improve from -0.40875. Patience: 56/50
2024-12-18 04:43:35.625510: train_loss -0.8539
2024-12-18 04:43:35.626184: val_loss -0.2862
2024-12-18 04:43:35.626816: Pseudo dice [0.6707]
2024-12-18 04:43:35.627487: Epoch time: 386.52 s
2024-12-18 04:43:37.108390: 
2024-12-18 04:43:37.109690: Epoch 61
2024-12-18 04:43:37.110362: Current learning rate: 0.00625
2024-12-18 04:50:19.505378: Validation loss did not improve from -0.40875. Patience: 57/50
2024-12-18 04:50:19.506374: train_loss -0.8546
2024-12-18 04:50:19.507107: val_loss -0.2692
2024-12-18 04:50:19.507739: Pseudo dice [0.6753]
2024-12-18 04:50:19.508403: Epoch time: 402.4 s
2024-12-18 04:50:20.916565: 
2024-12-18 04:50:20.917833: Epoch 62
2024-12-18 04:50:20.918764: Current learning rate: 0.00619
2024-12-18 04:57:28.022319: Validation loss did not improve from -0.40875. Patience: 58/50
2024-12-18 04:57:28.023322: train_loss -0.8568
2024-12-18 04:57:28.024154: val_loss -0.2815
2024-12-18 04:57:28.024956: Pseudo dice [0.6835]
2024-12-18 04:57:28.025791: Epoch time: 427.11 s
2024-12-18 04:57:29.884542: 
2024-12-18 04:57:29.885819: Epoch 63
2024-12-18 04:57:29.886571: Current learning rate: 0.00612
2024-12-18 05:04:38.764428: Validation loss did not improve from -0.40875. Patience: 59/50
2024-12-18 05:04:38.765247: train_loss -0.8581
2024-12-18 05:04:38.766224: val_loss -0.2788
2024-12-18 05:04:38.767025: Pseudo dice [0.6844]
2024-12-18 05:04:38.767829: Epoch time: 428.88 s
2024-12-18 05:04:40.231777: 
2024-12-18 05:04:40.233391: Epoch 64
2024-12-18 05:04:40.234370: Current learning rate: 0.00606
2024-12-18 05:10:33.011935: Validation loss did not improve from -0.40875. Patience: 60/50
2024-12-18 05:10:33.014220: train_loss -0.8605
2024-12-18 05:10:33.016098: val_loss -0.2796
2024-12-18 05:10:33.016996: Pseudo dice [0.6732]
2024-12-18 05:10:33.017954: Epoch time: 352.78 s
2024-12-18 05:10:34.892828: 
2024-12-18 05:10:34.893991: Epoch 65
2024-12-18 05:10:34.894696: Current learning rate: 0.006
2024-12-18 05:16:38.722954: Validation loss did not improve from -0.40875. Patience: 61/50
2024-12-18 05:16:38.724013: train_loss -0.8609
2024-12-18 05:16:38.724864: val_loss -0.2857
2024-12-18 05:16:38.725629: Pseudo dice [0.6764]
2024-12-18 05:16:38.726338: Epoch time: 363.83 s
2024-12-18 05:16:40.267830: 
2024-12-18 05:16:40.269156: Epoch 66
2024-12-18 05:16:40.269904: Current learning rate: 0.00593
2024-12-18 05:23:38.279707: Validation loss did not improve from -0.40875. Patience: 62/50
2024-12-18 05:23:38.280549: train_loss -0.8604
2024-12-18 05:23:38.281427: val_loss -0.2834
2024-12-18 05:23:38.282165: Pseudo dice [0.6877]
2024-12-18 05:23:38.282825: Epoch time: 418.01 s
2024-12-18 05:23:39.744129: 
2024-12-18 05:23:39.745370: Epoch 67
2024-12-18 05:23:39.746231: Current learning rate: 0.00587
2024-12-18 05:30:21.609109: Validation loss did not improve from -0.40875. Patience: 63/50
2024-12-18 05:30:21.610255: train_loss -0.862
2024-12-18 05:30:21.611240: val_loss -0.276
2024-12-18 05:30:21.612002: Pseudo dice [0.6796]
2024-12-18 05:30:21.612709: Epoch time: 401.87 s
2024-12-18 05:30:23.135530: 
2024-12-18 05:30:23.136869: Epoch 68
2024-12-18 05:30:23.137788: Current learning rate: 0.00581
2024-12-18 05:36:30.546597: Validation loss did not improve from -0.40875. Patience: 64/50
2024-12-18 05:36:30.547412: train_loss -0.8628
2024-12-18 05:36:30.548182: val_loss -0.2522
2024-12-18 05:36:30.548936: Pseudo dice [0.6843]
2024-12-18 05:36:30.549711: Epoch time: 367.41 s
2024-12-18 05:36:32.007119: 
2024-12-18 05:36:32.008335: Epoch 69
2024-12-18 05:36:32.009108: Current learning rate: 0.00574
2024-12-18 05:42:56.549582: Validation loss did not improve from -0.40875. Patience: 65/50
2024-12-18 05:42:56.550669: train_loss -0.8613
2024-12-18 05:42:56.551483: val_loss -0.2943
2024-12-18 05:42:56.552266: Pseudo dice [0.6976]
2024-12-18 05:42:56.553104: Epoch time: 384.54 s
2024-12-18 05:42:58.467215: 
2024-12-18 05:42:58.468610: Epoch 70
2024-12-18 05:42:58.469486: Current learning rate: 0.00568
2024-12-18 05:49:58.840888: Validation loss did not improve from -0.40875. Patience: 66/50
2024-12-18 05:49:58.841765: train_loss -0.8636
2024-12-18 05:49:58.842666: val_loss -0.2615
2024-12-18 05:49:58.843493: Pseudo dice [0.6768]
2024-12-18 05:49:58.844307: Epoch time: 420.38 s
2024-12-18 05:50:00.349010: 
2024-12-18 05:50:00.350305: Epoch 71
2024-12-18 05:50:00.351168: Current learning rate: 0.00562
2024-12-18 05:57:17.767404: Validation loss did not improve from -0.40875. Patience: 67/50
2024-12-18 05:57:17.768405: train_loss -0.8665
2024-12-18 05:57:17.769428: val_loss -0.3126
2024-12-18 05:57:17.770510: Pseudo dice [0.6861]
2024-12-18 05:57:17.771399: Epoch time: 437.42 s
2024-12-18 05:57:19.289418: 
2024-12-18 05:57:19.290580: Epoch 72
2024-12-18 05:57:19.291369: Current learning rate: 0.00555
2024-12-18 06:04:08.664283: Validation loss did not improve from -0.40875. Patience: 68/50
2024-12-18 06:04:08.665168: train_loss -0.8676
2024-12-18 06:04:08.665899: val_loss -0.2363
2024-12-18 06:04:08.666638: Pseudo dice [0.6746]
2024-12-18 06:04:08.667267: Epoch time: 409.38 s
2024-12-18 06:04:10.105524: 
2024-12-18 06:04:10.106679: Epoch 73
2024-12-18 06:04:10.107388: Current learning rate: 0.00549
2024-12-18 06:10:37.195400: Validation loss did not improve from -0.40875. Patience: 69/50
2024-12-18 06:10:37.196341: train_loss -0.8671
2024-12-18 06:10:37.197064: val_loss -0.3024
2024-12-18 06:10:37.197682: Pseudo dice [0.6835]
2024-12-18 06:10:37.198407: Epoch time: 387.09 s
2024-12-18 06:10:39.186063: 
2024-12-18 06:10:39.187356: Epoch 74
2024-12-18 06:10:39.188185: Current learning rate: 0.00542
2024-12-18 06:16:41.653212: Validation loss did not improve from -0.40875. Patience: 70/50
2024-12-18 06:16:41.674834: train_loss -0.8701
2024-12-18 06:16:41.677252: val_loss -0.2639
2024-12-18 06:16:41.678101: Pseudo dice [0.6751]
2024-12-18 06:16:41.678951: Epoch time: 362.49 s
2024-12-18 06:16:43.512675: 
2024-12-18 06:16:43.514012: Epoch 75
2024-12-18 06:16:43.514661: Current learning rate: 0.00536
2024-12-18 06:23:26.713423: Validation loss did not improve from -0.40875. Patience: 71/50
2024-12-18 06:23:26.714408: train_loss -0.8721
2024-12-18 06:23:26.715228: val_loss -0.2477
2024-12-18 06:23:26.715963: Pseudo dice [0.666]
2024-12-18 06:23:26.716745: Epoch time: 403.2 s
2024-12-18 06:23:28.234902: 
2024-12-18 06:23:28.236200: Epoch 76
2024-12-18 06:23:28.237140: Current learning rate: 0.00529
2024-12-18 06:30:13.819226: Validation loss did not improve from -0.40875. Patience: 72/50
2024-12-18 06:30:13.821592: train_loss -0.8708
2024-12-18 06:30:13.822546: val_loss -0.2624
2024-12-18 06:30:13.823293: Pseudo dice [0.6735]
2024-12-18 06:30:13.824385: Epoch time: 405.59 s
2024-12-18 06:30:15.319736: 
2024-12-18 06:30:15.320887: Epoch 77
2024-12-18 06:30:15.321760: Current learning rate: 0.00523
2024-12-18 06:36:46.856612: Validation loss did not improve from -0.40875. Patience: 73/50
2024-12-18 06:36:46.857756: train_loss -0.8724
2024-12-18 06:36:46.858416: val_loss -0.263
2024-12-18 06:36:46.859018: Pseudo dice [0.6577]
2024-12-18 06:36:46.859682: Epoch time: 391.54 s
2024-12-18 06:36:48.374035: 
2024-12-18 06:36:48.375341: Epoch 78
2024-12-18 06:36:48.376720: Current learning rate: 0.00517
2024-12-18 06:42:55.367732: Validation loss did not improve from -0.40875. Patience: 74/50
2024-12-18 06:42:55.368685: train_loss -0.8742
2024-12-18 06:42:55.369414: val_loss -0.2532
2024-12-18 06:42:55.370250: Pseudo dice [0.6606]
2024-12-18 06:42:55.371049: Epoch time: 367.0 s
2024-12-18 06:42:56.861929: 
2024-12-18 06:42:56.863216: Epoch 79
2024-12-18 06:42:56.864394: Current learning rate: 0.0051
2024-12-18 06:50:21.600790: Validation loss did not improve from -0.40875. Patience: 75/50
2024-12-18 06:50:21.601719: train_loss -0.8741
2024-12-18 06:50:21.602508: val_loss -0.247
2024-12-18 06:50:21.603319: Pseudo dice [0.6757]
2024-12-18 06:50:21.604105: Epoch time: 444.74 s
2024-12-18 06:50:23.524822: 
2024-12-18 06:50:23.525927: Epoch 80
2024-12-18 06:50:23.526674: Current learning rate: 0.00504
2024-12-18 06:57:37.652842: Validation loss did not improve from -0.40875. Patience: 76/50
2024-12-18 06:57:37.654066: train_loss -0.8765
2024-12-18 06:57:37.654784: val_loss -0.2834
2024-12-18 06:57:37.655514: Pseudo dice [0.6806]
2024-12-18 06:57:37.656233: Epoch time: 434.13 s
2024-12-18 06:57:39.142182: 
2024-12-18 06:57:39.143313: Epoch 81
2024-12-18 06:57:39.144016: Current learning rate: 0.00497
2024-12-18 07:04:12.289264: Validation loss did not improve from -0.40875. Patience: 77/50
2024-12-18 07:04:12.290236: train_loss -0.8779
2024-12-18 07:04:12.290965: val_loss -0.2559
2024-12-18 07:04:12.291663: Pseudo dice [0.663]
2024-12-18 07:04:12.292378: Epoch time: 393.15 s
2024-12-18 07:04:13.755435: 
2024-12-18 07:04:13.756906: Epoch 82
2024-12-18 07:04:13.758082: Current learning rate: 0.00491
2024-12-18 07:11:22.561647: Validation loss did not improve from -0.40875. Patience: 78/50
2024-12-18 07:11:22.562403: train_loss -0.8778
2024-12-18 07:11:22.563229: val_loss -0.2499
2024-12-18 07:11:22.564184: Pseudo dice [0.6771]
2024-12-18 07:11:22.564984: Epoch time: 428.81 s
2024-12-18 07:11:23.925851: 
2024-12-18 07:11:23.927241: Epoch 83
2024-12-18 07:11:23.928208: Current learning rate: 0.00484
2024-12-18 07:17:36.319159: Validation loss did not improve from -0.40875. Patience: 79/50
2024-12-18 07:17:36.320243: train_loss -0.8786
2024-12-18 07:17:36.321057: val_loss -0.2244
2024-12-18 07:17:36.321724: Pseudo dice [0.6661]
2024-12-18 07:17:36.322334: Epoch time: 372.4 s
2024-12-18 07:17:37.731570: 
2024-12-18 07:17:37.732810: Epoch 84
2024-12-18 07:17:37.733662: Current learning rate: 0.00478
2024-12-18 07:24:34.859129: Validation loss did not improve from -0.40875. Patience: 80/50
2024-12-18 07:24:34.860145: train_loss -0.881
2024-12-18 07:24:34.860934: val_loss -0.2314
2024-12-18 07:24:34.861547: Pseudo dice [0.6689]
2024-12-18 07:24:34.862348: Epoch time: 417.13 s
2024-12-18 07:24:37.063387: 
2024-12-18 07:24:37.064607: Epoch 85
2024-12-18 07:24:37.065273: Current learning rate: 0.00471
2024-12-18 07:31:32.649773: Validation loss did not improve from -0.40875. Patience: 81/50
2024-12-18 07:31:32.651056: train_loss -0.8798
2024-12-18 07:31:32.652084: val_loss -0.2743
2024-12-18 07:31:32.652965: Pseudo dice [0.6876]
2024-12-18 07:31:32.653850: Epoch time: 415.59 s
2024-12-18 07:31:34.117177: 
2024-12-18 07:31:34.118674: Epoch 86
2024-12-18 07:31:34.119726: Current learning rate: 0.00465
2024-12-18 07:38:25.498266: Validation loss did not improve from -0.40875. Patience: 82/50
2024-12-18 07:38:25.499454: train_loss -0.8825
2024-12-18 07:38:25.500347: val_loss -0.1974
2024-12-18 07:38:25.501111: Pseudo dice [0.6481]
2024-12-18 07:38:25.502140: Epoch time: 411.38 s
2024-12-18 07:38:26.881315: 
2024-12-18 07:38:26.882677: Epoch 87
2024-12-18 07:38:26.883443: Current learning rate: 0.00458
2024-12-18 07:45:02.962655: Validation loss did not improve from -0.40875. Patience: 83/50
2024-12-18 07:45:02.965269: train_loss -0.8801
2024-12-18 07:45:02.966267: val_loss -0.2083
2024-12-18 07:45:02.967671: Pseudo dice [0.661]
2024-12-18 07:45:02.968405: Epoch time: 396.09 s
2024-12-18 07:45:04.397903: 
2024-12-18 07:45:04.398731: Epoch 88
2024-12-18 07:45:04.399577: Current learning rate: 0.00452
2024-12-18 07:52:06.867756: Validation loss did not improve from -0.40875. Patience: 84/50
2024-12-18 07:52:06.868750: train_loss -0.883
2024-12-18 07:52:06.869442: val_loss -0.2462
2024-12-18 07:52:06.870066: Pseudo dice [0.6794]
2024-12-18 07:52:06.870668: Epoch time: 422.47 s
2024-12-18 07:52:08.297297: 
2024-12-18 07:52:08.298088: Epoch 89
2024-12-18 07:52:08.298908: Current learning rate: 0.00445
2024-12-18 07:59:23.408037: Validation loss did not improve from -0.40875. Patience: 85/50
2024-12-18 07:59:23.409011: train_loss -0.8841
2024-12-18 07:59:23.409674: val_loss -0.2528
2024-12-18 07:59:23.410411: Pseudo dice [0.6726]
2024-12-18 07:59:23.411081: Epoch time: 435.11 s
2024-12-18 07:59:25.307836: 
2024-12-18 07:59:25.308973: Epoch 90
2024-12-18 07:59:25.309652: Current learning rate: 0.00438
2024-12-18 08:06:11.884106: Validation loss did not improve from -0.40875. Patience: 86/50
2024-12-18 08:06:11.885083: train_loss -0.8841
2024-12-18 08:06:11.885834: val_loss -0.2124
2024-12-18 08:06:11.886491: Pseudo dice [0.6571]
2024-12-18 08:06:11.887172: Epoch time: 406.58 s
2024-12-18 08:06:13.347904: 
2024-12-18 08:06:13.348957: Epoch 91
2024-12-18 08:06:13.349741: Current learning rate: 0.00432
2024-12-18 08:12:47.747036: Validation loss did not improve from -0.40875. Patience: 87/50
2024-12-18 08:12:47.748086: train_loss -0.8868
2024-12-18 08:12:47.748788: val_loss -0.242
2024-12-18 08:12:47.749450: Pseudo dice [0.6681]
2024-12-18 08:12:47.750265: Epoch time: 394.4 s
2024-12-18 08:12:49.217582: 
2024-12-18 08:12:49.218766: Epoch 92
2024-12-18 08:12:49.219438: Current learning rate: 0.00425
2024-12-18 08:19:02.538112: Validation loss did not improve from -0.40875. Patience: 88/50
2024-12-18 08:19:02.539067: train_loss -0.8864
2024-12-18 08:19:02.540133: val_loss -0.2356
2024-12-18 08:19:02.541067: Pseudo dice [0.6771]
2024-12-18 08:19:02.542008: Epoch time: 373.32 s
2024-12-18 08:19:04.004659: 
2024-12-18 08:19:04.006009: Epoch 93
2024-12-18 08:19:04.007229: Current learning rate: 0.00419
2024-12-18 08:26:05.144371: Validation loss did not improve from -0.40875. Patience: 89/50
2024-12-18 08:26:05.147120: train_loss -0.8887
2024-12-18 08:26:05.148134: val_loss -0.272
2024-12-18 08:26:05.148818: Pseudo dice [0.6757]
2024-12-18 08:26:05.149464: Epoch time: 421.14 s
2024-12-18 08:26:06.574021: 
2024-12-18 08:26:06.575326: Epoch 94
2024-12-18 08:26:06.576103: Current learning rate: 0.00412
2024-12-18 08:33:09.919976: Validation loss did not improve from -0.40875. Patience: 90/50
2024-12-18 08:33:09.921019: train_loss -0.8905
2024-12-18 08:33:09.921814: val_loss -0.209
2024-12-18 08:33:09.922565: Pseudo dice [0.6696]
2024-12-18 08:33:09.923304: Epoch time: 423.35 s
2024-12-18 08:33:11.772577: 
2024-12-18 08:33:11.774003: Epoch 95
2024-12-18 08:33:11.774820: Current learning rate: 0.00405
2024-12-18 08:39:41.791088: Validation loss did not improve from -0.40875. Patience: 91/50
2024-12-18 08:39:41.793519: train_loss -0.8898
2024-12-18 08:39:41.794619: val_loss -0.234
2024-12-18 08:39:41.795423: Pseudo dice [0.6736]
2024-12-18 08:39:41.796433: Epoch time: 390.02 s
2024-12-18 08:39:43.306264: 
2024-12-18 08:39:43.307684: Epoch 96
2024-12-18 08:39:43.308600: Current learning rate: 0.00399
2024-12-18 08:46:36.414589: Validation loss did not improve from -0.40875. Patience: 92/50
2024-12-18 08:46:36.415528: train_loss -0.8899
2024-12-18 08:46:36.416293: val_loss -0.2464
2024-12-18 08:46:36.416938: Pseudo dice [0.6788]
2024-12-18 08:46:36.417587: Epoch time: 413.11 s
2024-12-18 08:46:38.748086: 
2024-12-18 08:46:38.749413: Epoch 97
2024-12-18 08:46:38.750104: Current learning rate: 0.00392
2024-12-18 08:53:10.593341: Validation loss did not improve from -0.40875. Patience: 93/50
2024-12-18 08:53:10.594383: train_loss -0.8915
2024-12-18 08:53:10.595286: val_loss -0.2026
2024-12-18 08:53:10.595952: Pseudo dice [0.667]
2024-12-18 08:53:10.596676: Epoch time: 391.85 s
2024-12-18 08:53:12.065865: 
2024-12-18 08:53:12.067156: Epoch 98
2024-12-18 08:53:12.067904: Current learning rate: 0.00385
2024-12-18 09:00:06.930025: Validation loss did not improve from -0.40875. Patience: 94/50
2024-12-18 09:00:06.931002: train_loss -0.8912
2024-12-18 09:00:06.931889: val_loss -0.1961
2024-12-18 09:00:06.932647: Pseudo dice [0.6646]
2024-12-18 09:00:06.933453: Epoch time: 414.87 s
2024-12-18 09:00:08.389335: 
2024-12-18 09:00:08.390513: Epoch 99
2024-12-18 09:00:08.391481: Current learning rate: 0.00379
2024-12-18 09:06:42.538792: Validation loss did not improve from -0.40875. Patience: 95/50
2024-12-18 09:06:42.540323: train_loss -0.8926
2024-12-18 09:06:42.541079: val_loss -0.2273
2024-12-18 09:06:42.541805: Pseudo dice [0.6853]
2024-12-18 09:06:42.542485: Epoch time: 394.15 s
2024-12-18 09:06:44.449561: 
2024-12-18 09:06:44.450865: Epoch 100
2024-12-18 09:06:44.451967: Current learning rate: 0.00372
2024-12-18 09:13:28.431467: Validation loss did not improve from -0.40875. Patience: 96/50
2024-12-18 09:13:28.432446: train_loss -0.8934
2024-12-18 09:13:28.433455: val_loss -0.1931
2024-12-18 09:13:28.434399: Pseudo dice [0.6606]
2024-12-18 09:13:28.435241: Epoch time: 403.98 s
2024-12-18 09:13:29.887740: 
2024-12-18 09:13:29.889161: Epoch 101
2024-12-18 09:13:29.890017: Current learning rate: 0.00365
2024-12-18 09:19:08.123741: Validation loss did not improve from -0.40875. Patience: 97/50
2024-12-18 09:19:08.124893: train_loss -0.8949
2024-12-18 09:19:08.125789: val_loss -0.2301
2024-12-18 09:19:08.126480: Pseudo dice [0.6701]
2024-12-18 09:19:08.127367: Epoch time: 338.24 s
2024-12-18 09:19:09.588070: 
2024-12-18 09:19:09.589220: Epoch 102
2024-12-18 09:19:09.589929: Current learning rate: 0.00359
2024-12-18 09:26:13.819878: Validation loss did not improve from -0.40875. Patience: 98/50
2024-12-18 09:26:13.820930: train_loss -0.8957
2024-12-18 09:26:13.821834: val_loss -0.2414
2024-12-18 09:26:13.822452: Pseudo dice [0.6752]
2024-12-18 09:26:13.823332: Epoch time: 424.23 s
2024-12-18 09:26:15.291484: 
2024-12-18 09:26:15.292817: Epoch 103
2024-12-18 09:26:15.293944: Current learning rate: 0.00352
2024-12-18 09:32:48.667882: Validation loss did not improve from -0.40875. Patience: 99/50
2024-12-18 09:32:48.670366: train_loss -0.8972
2024-12-18 09:32:48.672391: val_loss -0.1568
2024-12-18 09:32:48.673178: Pseudo dice [0.6403]
2024-12-18 09:32:48.673998: Epoch time: 393.38 s
2024-12-18 09:32:50.129850: 
2024-12-18 09:32:50.130848: Epoch 104
2024-12-18 09:32:50.131579: Current learning rate: 0.00345
2024-12-18 09:38:40.089602: Validation loss did not improve from -0.40875. Patience: 100/50
2024-12-18 09:38:40.090333: train_loss -0.8986
2024-12-18 09:38:40.091194: val_loss -0.1542
2024-12-18 09:38:40.091937: Pseudo dice [0.6505]
2024-12-18 09:38:40.092622: Epoch time: 349.96 s
2024-12-18 09:38:41.953183: 
2024-12-18 09:38:41.954347: Epoch 105
2024-12-18 09:38:41.955169: Current learning rate: 0.00338
2024-12-18 09:45:08.235872: Validation loss did not improve from -0.40875. Patience: 101/50
2024-12-18 09:45:08.236929: train_loss -0.8985
2024-12-18 09:45:08.237833: val_loss -0.2336
2024-12-18 09:45:08.238644: Pseudo dice [0.6593]
2024-12-18 09:45:08.239464: Epoch time: 386.29 s
2024-12-18 09:45:09.719708: 
2024-12-18 09:45:09.721173: Epoch 106
2024-12-18 09:45:09.722228: Current learning rate: 0.00332
2024-12-18 09:51:12.601874: Validation loss did not improve from -0.40875. Patience: 102/50
2024-12-18 09:51:12.602682: train_loss -0.8998
2024-12-18 09:51:12.603519: val_loss -0.1811
2024-12-18 09:51:12.604292: Pseudo dice [0.6616]
2024-12-18 09:51:12.605171: Epoch time: 362.88 s
2024-12-18 09:51:14.489198: 
2024-12-18 09:51:14.490625: Epoch 107
2024-12-18 09:51:14.491537: Current learning rate: 0.00325
2024-12-18 09:56:38.291257: Validation loss did not improve from -0.40875. Patience: 103/50
2024-12-18 09:56:38.292360: train_loss -0.8979
2024-12-18 09:56:38.293039: val_loss -0.1706
2024-12-18 09:56:38.293636: Pseudo dice [0.6595]
2024-12-18 09:56:38.294410: Epoch time: 323.8 s
2024-12-18 09:56:39.789321: 
2024-12-18 09:56:39.790785: Epoch 108
2024-12-18 09:56:39.791707: Current learning rate: 0.00318
2024-12-18 10:02:36.981121: Validation loss did not improve from -0.40875. Patience: 104/50
2024-12-18 10:02:36.982079: train_loss -0.8989
2024-12-18 10:02:36.982909: val_loss -0.2242
2024-12-18 10:02:36.983620: Pseudo dice [0.6726]
2024-12-18 10:02:36.984320: Epoch time: 357.19 s
2024-12-18 10:02:38.421465: 
2024-12-18 10:02:38.423165: Epoch 109
2024-12-18 10:02:38.424041: Current learning rate: 0.00311
2024-12-18 10:08:59.333858: Validation loss did not improve from -0.40875. Patience: 105/50
2024-12-18 10:08:59.334796: train_loss -0.8996
2024-12-18 10:08:59.335798: val_loss -0.2147
2024-12-18 10:08:59.336562: Pseudo dice [0.654]
2024-12-18 10:08:59.337244: Epoch time: 380.92 s
2024-12-18 10:09:01.197865: 
2024-12-18 10:09:01.198904: Epoch 110
2024-12-18 10:09:01.199750: Current learning rate: 0.00304
2024-12-18 10:15:08.255366: Validation loss did not improve from -0.40875. Patience: 106/50
2024-12-18 10:15:08.256512: train_loss -0.8996
2024-12-18 10:15:08.257221: val_loss -0.2245
2024-12-18 10:15:08.257870: Pseudo dice [0.6794]
2024-12-18 10:15:08.258610: Epoch time: 367.06 s
2024-12-18 10:15:09.756262: 
2024-12-18 10:15:09.757536: Epoch 111
2024-12-18 10:15:09.758286: Current learning rate: 0.00297
2024-12-18 10:21:24.793442: Validation loss did not improve from -0.40875. Patience: 107/50
2024-12-18 10:21:24.794898: train_loss -0.8994
2024-12-18 10:21:24.795793: val_loss -0.1929
2024-12-18 10:21:24.796657: Pseudo dice [0.6629]
2024-12-18 10:21:24.797470: Epoch time: 375.04 s
2024-12-18 10:21:26.273652: 
2024-12-18 10:21:26.275024: Epoch 112
2024-12-18 10:21:26.276095: Current learning rate: 0.00291
2024-12-18 10:27:56.049930: Validation loss did not improve from -0.40875. Patience: 108/50
2024-12-18 10:27:56.050824: train_loss -0.9029
2024-12-18 10:27:56.051666: val_loss -0.218
2024-12-18 10:27:56.052491: Pseudo dice [0.6746]
2024-12-18 10:27:56.053279: Epoch time: 389.78 s
2024-12-18 10:27:57.503513: 
2024-12-18 10:27:57.505021: Epoch 113
2024-12-18 10:27:57.505990: Current learning rate: 0.00284
2024-12-18 10:34:36.376374: Validation loss did not improve from -0.40875. Patience: 109/50
2024-12-18 10:34:36.378110: train_loss -0.9021
2024-12-18 10:34:36.378906: val_loss -0.2316
2024-12-18 10:34:36.379552: Pseudo dice [0.6783]
2024-12-18 10:34:36.380280: Epoch time: 398.88 s
2024-12-18 10:34:37.838050: 
2024-12-18 10:34:37.839673: Epoch 114
2024-12-18 10:34:37.840577: Current learning rate: 0.00277
2024-12-18 10:40:47.526756: Validation loss did not improve from -0.40875. Patience: 110/50
2024-12-18 10:40:47.527770: train_loss -0.9027
2024-12-18 10:40:47.529010: val_loss -0.1956
2024-12-18 10:40:47.530087: Pseudo dice [0.6626]
2024-12-18 10:40:47.531056: Epoch time: 369.69 s
2024-12-18 10:40:49.489357: 
2024-12-18 10:40:49.490922: Epoch 115
2024-12-18 10:40:49.492196: Current learning rate: 0.0027
2024-12-18 10:46:54.935873: Validation loss did not improve from -0.40875. Patience: 111/50
2024-12-18 10:46:54.937157: train_loss -0.9031
2024-12-18 10:46:54.938123: val_loss -0.2389
2024-12-18 10:46:54.939074: Pseudo dice [0.6733]
2024-12-18 10:46:54.939911: Epoch time: 365.45 s
2024-12-18 10:46:56.492360: 
2024-12-18 10:46:56.493623: Epoch 116
2024-12-18 10:46:56.494438: Current learning rate: 0.00263
2024-12-18 10:53:23.066101: Validation loss did not improve from -0.40875. Patience: 112/50
2024-12-18 10:53:23.067291: train_loss -0.9055
2024-12-18 10:53:23.068074: val_loss -0.1952
2024-12-18 10:53:23.068852: Pseudo dice [0.6706]
2024-12-18 10:53:23.069797: Epoch time: 386.58 s
2024-12-18 10:53:24.585411: 
2024-12-18 10:53:24.586619: Epoch 117
2024-12-18 10:53:24.587793: Current learning rate: 0.00256
2024-12-18 11:00:10.122725: Validation loss did not improve from -0.40875. Patience: 113/50
2024-12-18 11:00:10.125206: train_loss -0.9067
2024-12-18 11:00:10.126047: val_loss -0.1966
2024-12-18 11:00:10.126861: Pseudo dice [0.6663]
2024-12-18 11:00:10.127561: Epoch time: 405.54 s
2024-12-18 11:00:12.328778: 
2024-12-18 11:00:12.330200: Epoch 118
2024-12-18 11:00:12.331081: Current learning rate: 0.00249
2024-12-18 11:06:38.276648: Validation loss did not improve from -0.40875. Patience: 114/50
2024-12-18 11:06:38.277547: train_loss -0.9073
2024-12-18 11:06:38.278858: val_loss -0.1481
2024-12-18 11:06:38.279742: Pseudo dice [0.6633]
2024-12-18 11:06:38.280605: Epoch time: 385.95 s
2024-12-18 11:06:39.812547: 
2024-12-18 11:06:39.813965: Epoch 119
2024-12-18 11:06:39.814782: Current learning rate: 0.00242
2024-12-18 11:13:00.731214: Validation loss did not improve from -0.40875. Patience: 115/50
2024-12-18 11:13:00.733162: train_loss -0.9069
2024-12-18 11:13:00.734318: val_loss -0.17
2024-12-18 11:13:00.735079: Pseudo dice [0.6594]
2024-12-18 11:13:00.735900: Epoch time: 380.92 s
2024-12-18 11:13:02.754907: 
2024-12-18 11:13:02.755882: Epoch 120
2024-12-18 11:13:02.756550: Current learning rate: 0.00235
2024-12-18 11:19:36.387848: Validation loss did not improve from -0.40875. Patience: 116/50
2024-12-18 11:19:36.388933: train_loss -0.9061
2024-12-18 11:19:36.389838: val_loss -0.1774
2024-12-18 11:19:36.390530: Pseudo dice [0.6644]
2024-12-18 11:19:36.391207: Epoch time: 393.64 s
2024-12-18 11:19:37.899785: 
2024-12-18 11:19:37.900888: Epoch 121
2024-12-18 11:19:37.901587: Current learning rate: 0.00228
2024-12-18 11:25:52.796065: Validation loss did not improve from -0.40875. Patience: 117/50
2024-12-18 11:25:52.797079: train_loss -0.908
2024-12-18 11:25:52.798265: val_loss -0.1523
2024-12-18 11:25:52.799221: Pseudo dice [0.6571]
2024-12-18 11:25:52.800103: Epoch time: 374.9 s
2024-12-18 11:25:54.334243: 
2024-12-18 11:25:54.335728: Epoch 122
2024-12-18 11:25:54.336728: Current learning rate: 0.00221
2024-12-18 11:32:49.465798: Validation loss did not improve from -0.40875. Patience: 118/50
2024-12-18 11:32:49.466900: train_loss -0.9078
2024-12-18 11:32:49.467691: val_loss -0.1335
2024-12-18 11:32:49.468418: Pseudo dice [0.6478]
2024-12-18 11:32:49.469048: Epoch time: 415.13 s
2024-12-18 11:32:51.011565: 
2024-12-18 11:32:51.013071: Epoch 123
2024-12-18 11:32:51.014326: Current learning rate: 0.00214
2024-12-18 11:39:25.196999: Validation loss did not improve from -0.40875. Patience: 119/50
2024-12-18 11:39:25.198126: train_loss -0.9082
2024-12-18 11:39:25.199697: val_loss -0.1589
2024-12-18 11:39:25.200455: Pseudo dice [0.6572]
2024-12-18 11:39:25.201308: Epoch time: 394.19 s
2024-12-18 11:39:26.739263: 
2024-12-18 11:39:26.740408: Epoch 124
2024-12-18 11:39:26.741122: Current learning rate: 0.00207
2024-12-18 11:45:56.156313: Validation loss did not improve from -0.40875. Patience: 120/50
2024-12-18 11:45:56.157264: train_loss -0.9084
2024-12-18 11:45:56.158399: val_loss -0.1737
2024-12-18 11:45:56.159230: Pseudo dice [0.6646]
2024-12-18 11:45:56.160079: Epoch time: 389.42 s
2024-12-18 11:45:58.045347: 
2024-12-18 11:45:58.046716: Epoch 125
2024-12-18 11:45:58.047828: Current learning rate: 0.00199
2024-12-18 11:52:37.686167: Validation loss did not improve from -0.40875. Patience: 121/50
2024-12-18 11:52:37.688754: train_loss -0.9106
2024-12-18 11:52:37.689619: val_loss -0.2152
2024-12-18 11:52:37.690341: Pseudo dice [0.6592]
2024-12-18 11:52:37.691382: Epoch time: 399.64 s
2024-12-18 11:52:39.369617: 
2024-12-18 11:52:39.370890: Epoch 126
2024-12-18 11:52:39.371745: Current learning rate: 0.00192
2024-12-18 11:59:04.217167: Validation loss did not improve from -0.40875. Patience: 122/50
2024-12-18 11:59:04.218202: train_loss -0.9105
2024-12-18 11:59:04.219048: val_loss -0.2086
2024-12-18 11:59:04.219737: Pseudo dice [0.6736]
2024-12-18 11:59:04.220500: Epoch time: 384.85 s
2024-12-18 11:59:05.749751: 
2024-12-18 11:59:05.750848: Epoch 127
2024-12-18 11:59:05.751628: Current learning rate: 0.00185
2024-12-18 12:05:42.026040: Validation loss did not improve from -0.40875. Patience: 123/50
2024-12-18 12:05:42.026969: train_loss -0.9109
2024-12-18 12:05:42.027962: val_loss -0.2094
2024-12-18 12:05:42.028837: Pseudo dice [0.6511]
2024-12-18 12:05:42.029689: Epoch time: 396.28 s
2024-12-18 12:05:43.502218: 
2024-12-18 12:05:43.503544: Epoch 128
2024-12-18 12:05:43.504520: Current learning rate: 0.00178
2024-12-18 12:12:33.826939: Validation loss did not improve from -0.40875. Patience: 124/50
2024-12-18 12:12:33.827923: train_loss -0.9124
2024-12-18 12:12:33.828998: val_loss -0.1846
2024-12-18 12:12:33.829851: Pseudo dice [0.6868]
2024-12-18 12:12:33.830651: Epoch time: 410.33 s
2024-12-18 12:12:37.098815: 
2024-12-18 12:12:37.099989: Epoch 129
2024-12-18 12:12:37.100866: Current learning rate: 0.0017
2024-12-18 12:19:28.884974: Validation loss did not improve from -0.40875. Patience: 125/50
2024-12-18 12:19:28.885931: train_loss -0.912
2024-12-18 12:19:28.886712: val_loss -0.1558
2024-12-18 12:19:28.887558: Pseudo dice [0.6676]
2024-12-18 12:19:28.888229: Epoch time: 411.79 s
2024-12-18 12:19:30.958911: 
2024-12-18 12:19:30.960382: Epoch 130
2024-12-18 12:19:30.961109: Current learning rate: 0.00163
2024-12-18 12:26:07.300820: Validation loss did not improve from -0.40875. Patience: 126/50
2024-12-18 12:26:07.302223: train_loss -0.9118
2024-12-18 12:26:07.303224: val_loss -0.1551
2024-12-18 12:26:07.304057: Pseudo dice [0.648]
2024-12-18 12:26:07.304897: Epoch time: 396.34 s
2024-12-18 12:26:08.812307: 
2024-12-18 12:26:08.814257: Epoch 131
2024-12-18 12:26:08.815076: Current learning rate: 0.00156
2024-12-18 12:32:26.243328: Validation loss did not improve from -0.40875. Patience: 127/50
2024-12-18 12:32:26.244612: train_loss -0.9127
2024-12-18 12:32:26.245538: val_loss -0.1342
2024-12-18 12:32:26.246322: Pseudo dice [0.6535]
2024-12-18 12:32:26.247063: Epoch time: 377.43 s
2024-12-18 12:32:27.688287: 
2024-12-18 12:32:27.689669: Epoch 132
2024-12-18 12:32:27.690593: Current learning rate: 0.00148
2024-12-18 12:38:43.104194: Validation loss did not improve from -0.40875. Patience: 128/50
2024-12-18 12:38:43.105064: train_loss -0.9115
2024-12-18 12:38:43.105711: val_loss -0.1614
2024-12-18 12:38:43.106393: Pseudo dice [0.6565]
2024-12-18 12:38:43.107050: Epoch time: 375.42 s
2024-12-18 12:38:44.532302: 
2024-12-18 12:38:44.533470: Epoch 133
2024-12-18 12:38:44.534034: Current learning rate: 0.00141
2024-12-18 12:45:20.138628: Validation loss did not improve from -0.40875. Patience: 129/50
2024-12-18 12:45:20.140734: train_loss -0.911
2024-12-18 12:45:20.142501: val_loss -0.1874
2024-12-18 12:45:20.143362: Pseudo dice [0.6697]
2024-12-18 12:45:20.145485: Epoch time: 395.61 s
2024-12-18 12:45:21.629183: 
2024-12-18 12:45:21.630450: Epoch 134
2024-12-18 12:45:21.631336: Current learning rate: 0.00133
2024-12-18 12:52:07.591009: Validation loss did not improve from -0.40875. Patience: 130/50
2024-12-18 12:52:07.592121: train_loss -0.913
2024-12-18 12:52:07.592983: val_loss -0.2174
2024-12-18 12:52:07.593692: Pseudo dice [0.6857]
2024-12-18 12:52:07.594382: Epoch time: 405.96 s
2024-12-18 12:52:09.429267: 
2024-12-18 12:52:09.430524: Epoch 135
2024-12-18 12:52:09.431281: Current learning rate: 0.00126
2024-12-18 12:58:12.660786: Validation loss did not improve from -0.40875. Patience: 131/50
2024-12-18 12:58:12.663427: train_loss -0.914
2024-12-18 12:58:12.664346: val_loss -0.1528
2024-12-18 12:58:12.665117: Pseudo dice [0.6513]
2024-12-18 12:58:12.666374: Epoch time: 363.23 s
2024-12-18 12:58:14.110973: 
2024-12-18 12:58:14.112094: Epoch 136
2024-12-18 12:58:14.112870: Current learning rate: 0.00118
2024-12-18 13:04:06.080816: Validation loss did not improve from -0.40875. Patience: 132/50
2024-12-18 13:04:06.081743: train_loss -0.9144
2024-12-18 13:04:06.082632: val_loss -0.1595
2024-12-18 13:04:06.083493: Pseudo dice [0.6695]
2024-12-18 13:04:06.084195: Epoch time: 351.97 s
2024-12-18 13:04:07.510927: 
2024-12-18 13:04:07.512141: Epoch 137
2024-12-18 13:04:07.512825: Current learning rate: 0.00111
2024-12-18 13:10:00.457384: Validation loss did not improve from -0.40875. Patience: 133/50
2024-12-18 13:10:00.458286: train_loss -0.9128
2024-12-18 13:10:00.459007: val_loss -0.113
2024-12-18 13:10:00.459721: Pseudo dice [0.6459]
2024-12-18 13:10:00.460450: Epoch time: 352.95 s
2024-12-18 13:10:01.978227: 
2024-12-18 13:10:01.979431: Epoch 138
2024-12-18 13:10:01.980114: Current learning rate: 0.00103
2024-12-18 13:14:42.614437: Validation loss did not improve from -0.40875. Patience: 134/50
2024-12-18 13:14:42.615339: train_loss -0.9152
2024-12-18 13:14:42.616121: val_loss -0.1717
2024-12-18 13:14:42.616774: Pseudo dice [0.6615]
2024-12-18 13:14:42.617456: Epoch time: 280.64 s
2024-12-18 13:14:44.129714: 
2024-12-18 13:14:44.130847: Epoch 139
2024-12-18 13:14:44.131583: Current learning rate: 0.00095
2024-12-18 13:20:04.299570: Validation loss did not improve from -0.40875. Patience: 135/50
2024-12-18 13:20:04.300695: train_loss -0.9159
2024-12-18 13:20:04.301600: val_loss -0.196
2024-12-18 13:20:04.302340: Pseudo dice [0.6613]
2024-12-18 13:20:04.303042: Epoch time: 320.17 s
2024-12-18 13:20:06.903708: 
2024-12-18 13:20:06.904668: Epoch 140
2024-12-18 13:20:06.905357: Current learning rate: 0.00087
2024-12-18 13:25:27.944078: Validation loss did not improve from -0.40875. Patience: 136/50
2024-12-18 13:25:27.944894: train_loss -0.9148
2024-12-18 13:25:27.945660: val_loss -0.1766
2024-12-18 13:25:27.946520: Pseudo dice [0.6644]
2024-12-18 13:25:27.947086: Epoch time: 321.04 s
2024-12-18 13:25:29.428497: 
2024-12-18 13:25:29.429665: Epoch 141
2024-12-18 13:25:29.430434: Current learning rate: 0.00079
2024-12-18 13:30:52.692748: Validation loss did not improve from -0.40875. Patience: 137/50
2024-12-18 13:30:52.694274: train_loss -0.9147
2024-12-18 13:30:52.695472: val_loss -0.143
2024-12-18 13:30:52.696166: Pseudo dice [0.6588]
2024-12-18 13:30:52.696780: Epoch time: 323.27 s
2024-12-18 13:30:54.122853: 
2024-12-18 13:30:54.124010: Epoch 142
2024-12-18 13:30:54.124657: Current learning rate: 0.00071
2024-12-18 13:36:30.863543: Validation loss did not improve from -0.40875. Patience: 138/50
2024-12-18 13:36:30.864525: train_loss -0.9163
2024-12-18 13:36:30.865538: val_loss -0.1787
2024-12-18 13:36:30.866336: Pseudo dice [0.6809]
2024-12-18 13:36:30.867204: Epoch time: 336.74 s
2024-12-18 13:36:32.309705: 
2024-12-18 13:36:32.311100: Epoch 143
2024-12-18 13:36:32.311974: Current learning rate: 0.00063
2024-12-18 13:42:08.906621: Validation loss did not improve from -0.40875. Patience: 139/50
2024-12-18 13:42:08.907577: train_loss -0.9165
2024-12-18 13:42:08.908369: val_loss -0.1992
2024-12-18 13:42:08.909015: Pseudo dice [0.6698]
2024-12-18 13:42:08.909674: Epoch time: 336.6 s
2024-12-18 13:42:10.465200: 
2024-12-18 13:42:10.467302: Epoch 144
2024-12-18 13:42:10.468052: Current learning rate: 0.00055
2024-12-18 13:47:35.532732: Validation loss did not improve from -0.40875. Patience: 140/50
2024-12-18 13:47:35.533782: train_loss -0.9166
2024-12-18 13:47:35.534806: val_loss -0.1626
2024-12-18 13:47:35.535636: Pseudo dice [0.6653]
2024-12-18 13:47:35.536446: Epoch time: 325.07 s
2024-12-18 13:47:37.462571: 
2024-12-18 13:47:37.463899: Epoch 145
2024-12-18 13:47:37.464685: Current learning rate: 0.00047
2024-12-18 13:53:01.402570: Validation loss did not improve from -0.40875. Patience: 141/50
2024-12-18 13:53:01.403388: train_loss -0.9167
2024-12-18 13:53:01.404230: val_loss -0.1831
2024-12-18 13:53:01.404930: Pseudo dice [0.6739]
2024-12-18 13:53:01.405660: Epoch time: 323.94 s
2024-12-18 13:53:02.937860: 
2024-12-18 13:53:02.939090: Epoch 146
2024-12-18 13:53:02.939877: Current learning rate: 0.00038
2024-12-18 13:57:40.627675: Validation loss did not improve from -0.40875. Patience: 142/50
2024-12-18 13:57:40.628642: train_loss -0.9161
2024-12-18 13:57:40.629512: val_loss -0.1706
2024-12-18 13:57:40.630426: Pseudo dice [0.6668]
2024-12-18 13:57:40.631151: Epoch time: 277.69 s
2024-12-18 13:57:42.161760: 
2024-12-18 13:57:42.162833: Epoch 147
2024-12-18 13:57:42.163795: Current learning rate: 0.0003
2024-12-18 14:03:22.585010: Validation loss did not improve from -0.40875. Patience: 143/50
2024-12-18 14:03:22.587204: train_loss -0.9177
2024-12-18 14:03:22.588128: val_loss -0.1646
2024-12-18 14:03:22.588920: Pseudo dice [0.6594]
2024-12-18 14:03:22.589989: Epoch time: 340.43 s
2024-12-18 14:03:24.095757: 
2024-12-18 14:03:24.096909: Epoch 148
2024-12-18 14:03:24.097698: Current learning rate: 0.00021
2024-12-18 14:09:04.979383: Validation loss did not improve from -0.40875. Patience: 144/50
2024-12-18 14:09:04.980458: train_loss -0.9175
2024-12-18 14:09:04.981244: val_loss -0.1542
2024-12-18 14:09:04.981874: Pseudo dice [0.6462]
2024-12-18 14:09:04.982687: Epoch time: 340.89 s
2024-12-18 14:09:06.466017: 
2024-12-18 14:09:06.467242: Epoch 149
2024-12-18 14:09:06.468059: Current learning rate: 0.00011
2024-12-18 14:14:36.689138: Validation loss did not improve from -0.40875. Patience: 145/50
2024-12-18 14:14:36.692092: train_loss -0.9184
2024-12-18 14:14:36.693033: val_loss -0.1424
2024-12-18 14:14:36.693913: Pseudo dice [0.6553]
2024-12-18 14:14:36.694622: Epoch time: 330.23 s
2024-12-18 14:14:39.466797: Training done.
2024-12-18 14:14:39.645495: Using splits from existing split file: /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset309_Sohee_Calcium_OCT_CrossValidation/splits_final_20.json
2024-12-18 14:14:39.666555: The split file contains 5 splits.
2024-12-18 14:14:39.667339: Desired fold for training: 1
2024-12-18 14:14:39.668142: This split has 1 training and 7 validation cases.
2024-12-18 14:14:39.669116: predicting 101-019
2024-12-18 14:14:39.692729: 101-019, shape torch.Size([1, 375, 498, 498]), rank 0
2024-12-18 14:17:22.164333: predicting 101-045
2024-12-18 14:17:22.177343: 101-045, shape torch.Size([1, 375, 498, 498]), rank 0
2024-12-18 14:19:19.675373: predicting 106-002
2024-12-18 14:19:19.691290: 106-002, shape torch.Size([1, 539, 498, 498]), rank 0
2024-12-18 14:22:25.342371: predicting 401-004
2024-12-18 14:22:25.385070: 401-004, shape torch.Size([1, 375, 498, 498]), rank 0
2024-12-18 14:24:42.957404: predicting 701-013
2024-12-18 14:24:42.970616: 701-013, shape torch.Size([1, 375, 498, 498]), rank 0
2024-12-18 14:26:59.282354: predicting 704-003
2024-12-18 14:26:59.292743: 704-003, shape torch.Size([1, 375, 498, 498]), rank 0
2024-12-18 14:29:01.311991: predicting 706-005
2024-12-18 14:29:01.337620: 706-005, shape torch.Size([1, 375, 498, 498]), rank 0
2024-12-18 14:31:15.688899: Validation complete
2024-12-18 14:31:15.689459: Mean Validation Dice:  0.6402460839181234
2024-12-17 21:34:10.214105: unpacking done...
2024-12-17 21:34:10.288606: Unable to plot network architecture: nnUNet_compile is enabled!
2024-12-17 21:34:10.369721: 
2024-12-17 21:34:10.370876: Epoch 0
2024-12-17 21:34:10.371781: Current learning rate: 0.01
2024-12-17 21:44:03.124970: Validation loss improved from 1000.00000 to -0.25549! Patience: 0/50
2024-12-17 21:44:03.126186: train_loss -0.3986
2024-12-17 21:44:03.126988: val_loss -0.2555
2024-12-17 21:44:03.127690: Pseudo dice [0.6027]
2024-12-17 21:44:03.128339: Epoch time: 592.76 s
2024-12-17 21:44:03.129158: Yayy! New best EMA pseudo Dice: 0.6027
2024-12-17 21:44:04.752053: 
2024-12-17 21:44:04.753176: Epoch 1
2024-12-17 21:44:04.753946: Current learning rate: 0.00994
2024-12-17 21:52:51.796201: Validation loss improved from -0.25549 to -0.36030! Patience: 0/50
2024-12-17 21:52:51.797164: train_loss -0.6423
2024-12-17 21:52:51.798093: val_loss -0.3603
2024-12-17 21:52:51.798820: Pseudo dice [0.6536]
2024-12-17 21:52:51.799657: Epoch time: 527.05 s
2024-12-17 21:52:51.800404: Yayy! New best EMA pseudo Dice: 0.6078
2024-12-17 21:52:53.766595: 
2024-12-17 21:52:53.767748: Epoch 2
2024-12-17 21:52:53.768480: Current learning rate: 0.00988
2024-12-17 22:01:47.226985: Validation loss did not improve from -0.36030. Patience: 1/50
2024-12-17 22:01:47.227723: train_loss -0.693
2024-12-17 22:01:47.228446: val_loss -0.3275
2024-12-17 22:01:47.229099: Pseudo dice [0.6383]
2024-12-17 22:01:47.229774: Epoch time: 533.46 s
2024-12-17 22:01:47.230404: Yayy! New best EMA pseudo Dice: 0.6108
2024-12-17 22:01:49.069623: 
2024-12-17 22:01:49.070974: Epoch 3
2024-12-17 22:01:49.071736: Current learning rate: 0.00982
2024-12-17 22:09:41.536671: Validation loss did not improve from -0.36030. Patience: 2/50
2024-12-17 22:09:41.537589: train_loss -0.7147
2024-12-17 22:09:41.538518: val_loss -0.3017
2024-12-17 22:09:41.539283: Pseudo dice [0.624]
2024-12-17 22:09:41.540021: Epoch time: 472.47 s
2024-12-17 22:09:41.540727: Yayy! New best EMA pseudo Dice: 0.6121
2024-12-17 22:09:43.346366: 
2024-12-17 22:09:43.347345: Epoch 4
2024-12-17 22:09:43.348088: Current learning rate: 0.00976
2024-12-17 22:17:53.042460: Validation loss did not improve from -0.36030. Patience: 3/50
2024-12-17 22:17:53.043228: train_loss -0.7281
2024-12-17 22:17:53.044017: val_loss -0.2105
2024-12-17 22:17:53.044806: Pseudo dice [0.592]
2024-12-17 22:17:53.045534: Epoch time: 489.7 s
2024-12-17 22:17:54.878171: 
2024-12-17 22:17:54.879407: Epoch 5
2024-12-17 22:17:54.880396: Current learning rate: 0.0097
2024-12-17 22:26:15.445490: Validation loss did not improve from -0.36030. Patience: 4/50
2024-12-17 22:26:15.446360: train_loss -0.744
2024-12-17 22:26:15.447122: val_loss -0.2843
2024-12-17 22:26:15.447771: Pseudo dice [0.6395]
2024-12-17 22:26:15.448618: Epoch time: 500.57 s
2024-12-17 22:26:15.449303: Yayy! New best EMA pseudo Dice: 0.6131
2024-12-17 22:26:17.244397: 
2024-12-17 22:26:17.245519: Epoch 6
2024-12-17 22:26:17.246262: Current learning rate: 0.00964
2024-12-17 22:34:49.172657: Validation loss did not improve from -0.36030. Patience: 5/50
2024-12-17 22:34:49.173666: train_loss -0.7499
2024-12-17 22:34:49.174386: val_loss -0.1794
2024-12-17 22:34:49.175018: Pseudo dice [0.5603]
2024-12-17 22:34:49.175659: Epoch time: 511.93 s
2024-12-17 22:34:50.616631: 
2024-12-17 22:34:50.617810: Epoch 7
2024-12-17 22:34:50.618742: Current learning rate: 0.00958
2024-12-17 22:43:18.580379: Validation loss did not improve from -0.36030. Patience: 6/50
2024-12-17 22:43:18.584540: train_loss -0.7572
2024-12-17 22:43:18.585704: val_loss -0.292
2024-12-17 22:43:18.586422: Pseudo dice [0.6354]
2024-12-17 22:43:18.587219: Epoch time: 507.97 s
2024-12-17 22:43:20.026564: 
2024-12-17 22:43:20.027804: Epoch 8
2024-12-17 22:43:20.028667: Current learning rate: 0.00952
2024-12-17 22:51:33.380164: Validation loss did not improve from -0.36030. Patience: 7/50
2024-12-17 22:51:33.381565: train_loss -0.7663
2024-12-17 22:51:33.382391: val_loss -0.2555
2024-12-17 22:51:33.383160: Pseudo dice [0.6262]
2024-12-17 22:51:33.383917: Epoch time: 493.36 s
2024-12-17 22:51:35.419073: 
2024-12-17 22:51:35.420035: Epoch 9
2024-12-17 22:51:35.420738: Current learning rate: 0.00946
2024-12-17 23:00:38.887616: Validation loss did not improve from -0.36030. Patience: 8/50
2024-12-17 23:00:38.888484: train_loss -0.773
2024-12-17 23:00:38.889431: val_loss -0.2476
2024-12-17 23:00:38.890290: Pseudo dice [0.6187]
2024-12-17 23:00:38.891104: Epoch time: 543.47 s
2024-12-17 23:00:40.680198: 
2024-12-17 23:00:40.681580: Epoch 10
2024-12-17 23:00:40.682472: Current learning rate: 0.0094
2024-12-17 23:09:39.606298: Validation loss did not improve from -0.36030. Patience: 9/50
2024-12-17 23:09:39.607186: train_loss -0.7831
2024-12-17 23:09:39.608071: val_loss -0.2888
2024-12-17 23:09:39.608887: Pseudo dice [0.6412]
2024-12-17 23:09:39.609738: Epoch time: 538.93 s
2024-12-17 23:09:39.610502: Yayy! New best EMA pseudo Dice: 0.6156
2024-12-17 23:09:41.461839: 
2024-12-17 23:09:41.462884: Epoch 11
2024-12-17 23:09:41.463714: Current learning rate: 0.00934
2024-12-17 23:18:47.464780: Validation loss did not improve from -0.36030. Patience: 10/50
2024-12-17 23:18:47.465583: train_loss -0.7878
2024-12-17 23:18:47.466315: val_loss -0.2642
2024-12-17 23:18:47.467142: Pseudo dice [0.6237]
2024-12-17 23:18:47.467968: Epoch time: 546.0 s
2024-12-17 23:18:47.468821: Yayy! New best EMA pseudo Dice: 0.6164
2024-12-17 23:18:49.224391: 
2024-12-17 23:18:49.225572: Epoch 12
2024-12-17 23:18:49.226398: Current learning rate: 0.00928
2024-12-17 23:27:04.400344: Validation loss did not improve from -0.36030. Patience: 11/50
2024-12-17 23:27:04.401275: train_loss -0.7867
2024-12-17 23:27:04.402174: val_loss -0.2804
2024-12-17 23:27:04.402868: Pseudo dice [0.6514]
2024-12-17 23:27:04.403555: Epoch time: 495.18 s
2024-12-17 23:27:04.404240: Yayy! New best EMA pseudo Dice: 0.6199
2024-12-17 23:27:06.326082: 
2024-12-17 23:27:06.327484: Epoch 13
2024-12-17 23:27:06.328361: Current learning rate: 0.00922
2024-12-17 23:35:40.539277: Validation loss did not improve from -0.36030. Patience: 12/50
2024-12-17 23:35:40.540210: train_loss -0.7965
2024-12-17 23:35:40.540970: val_loss -0.2213
2024-12-17 23:35:40.541647: Pseudo dice [0.606]
2024-12-17 23:35:40.542305: Epoch time: 514.22 s
2024-12-17 23:35:41.946480: 
2024-12-17 23:35:41.947667: Epoch 14
2024-12-17 23:35:41.948436: Current learning rate: 0.00916
2024-12-17 23:43:55.429693: Validation loss did not improve from -0.36030. Patience: 13/50
2024-12-17 23:43:55.433439: train_loss -0.8015
2024-12-17 23:43:55.434457: val_loss -0.2766
2024-12-17 23:43:55.435154: Pseudo dice [0.6453]
2024-12-17 23:43:55.435837: Epoch time: 493.49 s
2024-12-17 23:43:55.819586: Yayy! New best EMA pseudo Dice: 0.6212
2024-12-17 23:43:57.661179: 
2024-12-17 23:43:57.662571: Epoch 15
2024-12-17 23:43:57.663444: Current learning rate: 0.0091
2024-12-17 23:51:43.750729: Validation loss did not improve from -0.36030. Patience: 14/50
2024-12-17 23:51:43.752539: train_loss -0.8005
2024-12-17 23:51:43.753532: val_loss -0.2706
2024-12-17 23:51:43.754291: Pseudo dice [0.6451]
2024-12-17 23:51:43.754919: Epoch time: 466.09 s
2024-12-17 23:51:43.755606: Yayy! New best EMA pseudo Dice: 0.6236
2024-12-17 23:51:47.545000: 
2024-12-17 23:51:47.546247: Epoch 16
2024-12-17 23:51:47.546982: Current learning rate: 0.00903
2024-12-17 23:59:41.361398: Validation loss did not improve from -0.36030. Patience: 15/50
2024-12-17 23:59:41.362242: train_loss -0.802
2024-12-17 23:59:41.362878: val_loss -0.2096
2024-12-17 23:59:41.363575: Pseudo dice [0.6117]
2024-12-17 23:59:41.364344: Epoch time: 473.82 s
2024-12-17 23:59:42.889608: 
2024-12-17 23:59:42.890607: Epoch 17
2024-12-17 23:59:42.891335: Current learning rate: 0.00897
2024-12-18 00:07:31.979610: Validation loss did not improve from -0.36030. Patience: 16/50
2024-12-18 00:07:31.980581: train_loss -0.8068
2024-12-18 00:07:31.981396: val_loss -0.2115
2024-12-18 00:07:31.982167: Pseudo dice [0.6214]
2024-12-18 00:07:31.983094: Epoch time: 469.09 s
2024-12-18 00:07:33.473387: 
2024-12-18 00:07:33.474942: Epoch 18
2024-12-18 00:07:33.476092: Current learning rate: 0.00891
2024-12-18 00:15:30.053146: Validation loss did not improve from -0.36030. Patience: 17/50
2024-12-18 00:15:30.054236: train_loss -0.8057
2024-12-18 00:15:30.055043: val_loss -0.283
2024-12-18 00:15:30.055617: Pseudo dice [0.6439]
2024-12-18 00:15:30.056239: Epoch time: 476.58 s
2024-12-18 00:15:30.056861: Yayy! New best EMA pseudo Dice: 0.6245
2024-12-18 00:15:31.895072: 
2024-12-18 00:15:31.896204: Epoch 19
2024-12-18 00:15:31.896923: Current learning rate: 0.00885
2024-12-18 00:23:15.750207: Validation loss did not improve from -0.36030. Patience: 18/50
2024-12-18 00:23:15.751169: train_loss -0.81
2024-12-18 00:23:15.752113: val_loss -0.2952
2024-12-18 00:23:15.752932: Pseudo dice [0.6406]
2024-12-18 00:23:15.753654: Epoch time: 463.86 s
2024-12-18 00:23:16.165267: Yayy! New best EMA pseudo Dice: 0.6261
2024-12-18 00:23:18.392179: 
2024-12-18 00:23:18.393356: Epoch 20
2024-12-18 00:23:18.394032: Current learning rate: 0.00879
2024-12-18 00:31:26.771006: Validation loss did not improve from -0.36030. Patience: 19/50
2024-12-18 00:31:26.771950: train_loss -0.8102
2024-12-18 00:31:26.772728: val_loss -0.2871
2024-12-18 00:31:26.773507: Pseudo dice [0.6468]
2024-12-18 00:31:26.774289: Epoch time: 488.38 s
2024-12-18 00:31:26.775046: Yayy! New best EMA pseudo Dice: 0.6282
2024-12-18 00:31:28.686505: 
2024-12-18 00:31:28.687699: Epoch 21
2024-12-18 00:31:28.688431: Current learning rate: 0.00873
2024-12-18 00:39:42.605373: Validation loss did not improve from -0.36030. Patience: 20/50
2024-12-18 00:39:42.606445: train_loss -0.8168
2024-12-18 00:39:42.607159: val_loss -0.2526
2024-12-18 00:39:42.607863: Pseudo dice [0.6412]
2024-12-18 00:39:42.608569: Epoch time: 493.92 s
2024-12-18 00:39:42.609196: Yayy! New best EMA pseudo Dice: 0.6295
2024-12-18 00:39:44.324902: 
2024-12-18 00:39:44.326154: Epoch 22
2024-12-18 00:39:44.327085: Current learning rate: 0.00867
2024-12-18 00:47:29.622803: Validation loss did not improve from -0.36030. Patience: 21/50
2024-12-18 00:47:29.623857: train_loss -0.8208
2024-12-18 00:47:29.624806: val_loss -0.267
2024-12-18 00:47:29.625724: Pseudo dice [0.6263]
2024-12-18 00:47:29.626762: Epoch time: 465.3 s
2024-12-18 00:47:31.093417: 
2024-12-18 00:47:31.094802: Epoch 23
2024-12-18 00:47:31.095892: Current learning rate: 0.00861
2024-12-18 00:55:20.578459: Validation loss did not improve from -0.36030. Patience: 22/50
2024-12-18 00:55:20.580304: train_loss -0.8212
2024-12-18 00:55:20.581345: val_loss -0.216
2024-12-18 00:55:20.582145: Pseudo dice [0.6183]
2024-12-18 00:55:20.582903: Epoch time: 469.49 s
2024-12-18 00:55:21.981918: 
2024-12-18 00:55:21.983320: Epoch 24
2024-12-18 00:55:21.984409: Current learning rate: 0.00855
2024-12-18 01:02:59.089880: Validation loss did not improve from -0.36030. Patience: 23/50
2024-12-18 01:02:59.091691: train_loss -0.8237
2024-12-18 01:02:59.092562: val_loss -0.1741
2024-12-18 01:02:59.093300: Pseudo dice [0.6032]
2024-12-18 01:02:59.094298: Epoch time: 457.11 s
2024-12-18 01:03:01.001140: 
2024-12-18 01:03:01.002280: Epoch 25
2024-12-18 01:03:01.003411: Current learning rate: 0.00849
2024-12-18 01:11:11.957952: Validation loss did not improve from -0.36030. Patience: 24/50
2024-12-18 01:11:11.958868: train_loss -0.8224
2024-12-18 01:11:11.959730: val_loss -0.1739
2024-12-18 01:11:11.960357: Pseudo dice [0.5989]
2024-12-18 01:11:11.960948: Epoch time: 490.96 s
2024-12-18 01:11:13.387475: 
2024-12-18 01:11:13.388699: Epoch 26
2024-12-18 01:11:13.389419: Current learning rate: 0.00843
2024-12-18 01:19:25.094690: Validation loss did not improve from -0.36030. Patience: 25/50
2024-12-18 01:19:25.095607: train_loss -0.8267
2024-12-18 01:19:25.096544: val_loss -0.2746
2024-12-18 01:19:25.097790: Pseudo dice [0.6377]
2024-12-18 01:19:25.098783: Epoch time: 491.71 s
2024-12-18 01:19:26.535075: 
2024-12-18 01:19:26.536420: Epoch 27
2024-12-18 01:19:26.537456: Current learning rate: 0.00836
2024-12-18 01:27:36.753801: Validation loss did not improve from -0.36030. Patience: 26/50
2024-12-18 01:27:36.754662: train_loss -0.8302
2024-12-18 01:27:36.755615: val_loss -0.2004
2024-12-18 01:27:36.756675: Pseudo dice [0.6149]
2024-12-18 01:27:36.757621: Epoch time: 490.22 s
2024-12-18 01:27:38.228094: 
2024-12-18 01:27:38.229325: Epoch 28
2024-12-18 01:27:38.230154: Current learning rate: 0.0083
2024-12-18 01:35:40.240027: Validation loss did not improve from -0.36030. Patience: 27/50
2024-12-18 01:35:40.240923: train_loss -0.8326
2024-12-18 01:35:40.241918: val_loss -0.276
2024-12-18 01:35:40.242810: Pseudo dice [0.6425]
2024-12-18 01:35:40.243701: Epoch time: 482.01 s
2024-12-18 01:35:41.617426: 
2024-12-18 01:35:41.618739: Epoch 29
2024-12-18 01:35:41.619519: Current learning rate: 0.00824
2024-12-18 01:43:59.765537: Validation loss did not improve from -0.36030. Patience: 28/50
2024-12-18 01:43:59.766520: train_loss -0.8298
2024-12-18 01:43:59.767432: val_loss -0.2027
2024-12-18 01:43:59.768066: Pseudo dice [0.6311]
2024-12-18 01:43:59.768750: Epoch time: 498.15 s
2024-12-18 01:44:01.748130: 
2024-12-18 01:44:01.749345: Epoch 30
2024-12-18 01:44:01.750022: Current learning rate: 0.00818
2024-12-18 01:52:14.564802: Validation loss did not improve from -0.36030. Patience: 29/50
2024-12-18 01:52:14.565624: train_loss -0.8348
2024-12-18 01:52:14.566487: val_loss -0.2144
2024-12-18 01:52:14.567134: Pseudo dice [0.6218]
2024-12-18 01:52:14.567781: Epoch time: 492.82 s
2024-12-18 01:52:16.362846: 
2024-12-18 01:52:16.363871: Epoch 31
2024-12-18 01:52:16.364869: Current learning rate: 0.00812
2024-12-18 01:59:11.305416: Validation loss did not improve from -0.36030. Patience: 30/50
2024-12-18 01:59:11.306869: train_loss -0.835
2024-12-18 01:59:11.307775: val_loss -0.1676
2024-12-18 01:59:11.308453: Pseudo dice [0.5931]
2024-12-18 01:59:11.309150: Epoch time: 414.95 s
2024-12-18 01:59:12.727425: 
2024-12-18 01:59:12.728616: Epoch 32
2024-12-18 01:59:12.729451: Current learning rate: 0.00806
2024-12-18 02:05:56.089657: Validation loss did not improve from -0.36030. Patience: 31/50
2024-12-18 02:05:56.090524: train_loss -0.8344
2024-12-18 02:05:56.091555: val_loss -0.1654
2024-12-18 02:05:56.092426: Pseudo dice [0.5878]
2024-12-18 02:05:56.093216: Epoch time: 403.36 s
2024-12-18 02:05:57.603898: 
2024-12-18 02:05:57.605323: Epoch 33
2024-12-18 02:05:57.606609: Current learning rate: 0.008
2024-12-18 02:13:17.031453: Validation loss did not improve from -0.36030. Patience: 32/50
2024-12-18 02:13:17.032586: train_loss -0.8328
2024-12-18 02:13:17.033334: val_loss -0.1414
2024-12-18 02:13:17.033962: Pseudo dice [0.5719]
2024-12-18 02:13:17.034651: Epoch time: 439.43 s
2024-12-18 02:13:18.495177: 
2024-12-18 02:13:18.496449: Epoch 34
2024-12-18 02:13:18.497149: Current learning rate: 0.00793
2024-12-18 02:19:09.957249: Validation loss did not improve from -0.36030. Patience: 33/50
2024-12-18 02:19:09.958200: train_loss -0.8367
2024-12-18 02:19:09.959148: val_loss -0.1985
2024-12-18 02:19:09.960037: Pseudo dice [0.611]
2024-12-18 02:19:09.960830: Epoch time: 351.46 s
2024-12-18 02:19:11.852613: 
2024-12-18 02:19:11.853859: Epoch 35
2024-12-18 02:19:11.854713: Current learning rate: 0.00787
2024-12-18 02:25:40.347041: Validation loss did not improve from -0.36030. Patience: 34/50
2024-12-18 02:25:40.347989: train_loss -0.8392
2024-12-18 02:25:40.348712: val_loss -0.1923
2024-12-18 02:25:40.349396: Pseudo dice [0.6134]
2024-12-18 02:25:40.350091: Epoch time: 388.5 s
2024-12-18 02:25:41.825955: 
2024-12-18 02:25:41.827087: Epoch 36
2024-12-18 02:25:41.827943: Current learning rate: 0.00781
2024-12-18 02:32:41.773623: Validation loss did not improve from -0.36030. Patience: 35/50
2024-12-18 02:32:41.775233: train_loss -0.8423
2024-12-18 02:32:41.776386: val_loss -0.1953
2024-12-18 02:32:41.777318: Pseudo dice [0.6372]
2024-12-18 02:32:41.778285: Epoch time: 419.95 s
2024-12-18 02:32:43.274315: 
2024-12-18 02:32:43.275614: Epoch 37
2024-12-18 02:32:43.276567: Current learning rate: 0.00775
2024-12-18 02:40:21.994523: Validation loss did not improve from -0.36030. Patience: 36/50
2024-12-18 02:40:21.995548: train_loss -0.8418
2024-12-18 02:40:21.996500: val_loss -0.1348
2024-12-18 02:40:21.997144: Pseudo dice [0.6011]
2024-12-18 02:40:21.997918: Epoch time: 458.72 s
2024-12-18 02:40:23.451518: 
2024-12-18 02:40:23.452698: Epoch 38
2024-12-18 02:40:23.453344: Current learning rate: 0.00769
2024-12-18 02:47:44.861567: Validation loss did not improve from -0.36030. Patience: 37/50
2024-12-18 02:47:44.862555: train_loss -0.8479
2024-12-18 02:47:44.863439: val_loss -0.2138
2024-12-18 02:47:44.864119: Pseudo dice [0.6233]
2024-12-18 02:47:44.864863: Epoch time: 441.41 s
2024-12-18 02:47:46.356544: 
2024-12-18 02:47:46.357788: Epoch 39
2024-12-18 02:47:46.358556: Current learning rate: 0.00763
2024-12-18 02:55:47.315302: Validation loss did not improve from -0.36030. Patience: 38/50
2024-12-18 02:55:47.316183: train_loss -0.8441
2024-12-18 02:55:47.317087: val_loss -0.1855
2024-12-18 02:55:47.317849: Pseudo dice [0.6152]
2024-12-18 02:55:47.318497: Epoch time: 480.96 s
2024-12-18 02:55:49.213399: 
2024-12-18 02:55:49.214762: Epoch 40
2024-12-18 02:55:49.215583: Current learning rate: 0.00756
2024-12-18 03:02:41.291852: Validation loss did not improve from -0.36030. Patience: 39/50
2024-12-18 03:02:41.294575: train_loss -0.8476
2024-12-18 03:02:41.296359: val_loss -0.1466
2024-12-18 03:02:41.297153: Pseudo dice [0.6134]
2024-12-18 03:02:41.298021: Epoch time: 412.08 s
2024-12-18 03:02:42.816566: 
2024-12-18 03:02:42.817813: Epoch 41
2024-12-18 03:02:42.818637: Current learning rate: 0.0075
2024-12-18 03:08:34.542335: Validation loss did not improve from -0.36030. Patience: 40/50
2024-12-18 03:08:34.543734: train_loss -0.8461
2024-12-18 03:08:34.544484: val_loss -0.1936
2024-12-18 03:08:34.545179: Pseudo dice [0.6138]
2024-12-18 03:08:34.545928: Epoch time: 351.73 s
2024-12-18 03:08:36.520925: 
2024-12-18 03:08:36.521875: Epoch 42
2024-12-18 03:08:36.522548: Current learning rate: 0.00744
2024-12-18 03:15:41.002093: Validation loss did not improve from -0.36030. Patience: 41/50
2024-12-18 03:15:41.003284: train_loss -0.8457
2024-12-18 03:15:41.004068: val_loss -0.2429
2024-12-18 03:15:41.004751: Pseudo dice [0.6312]
2024-12-18 03:15:41.005527: Epoch time: 424.48 s
2024-12-18 03:15:42.402170: 
2024-12-18 03:15:42.403145: Epoch 43
2024-12-18 03:15:42.403886: Current learning rate: 0.00738
2024-12-18 03:22:54.901379: Validation loss did not improve from -0.36030. Patience: 42/50
2024-12-18 03:22:54.902481: train_loss -0.8496
2024-12-18 03:22:54.903403: val_loss -0.1984
2024-12-18 03:22:54.904323: Pseudo dice [0.6223]
2024-12-18 03:22:54.905195: Epoch time: 432.5 s
2024-12-18 03:22:56.293843: 
2024-12-18 03:22:56.295331: Epoch 44
2024-12-18 03:22:56.296334: Current learning rate: 0.00732
2024-12-18 03:30:12.956640: Validation loss did not improve from -0.36030. Patience: 43/50
2024-12-18 03:30:12.957536: train_loss -0.8508
2024-12-18 03:30:12.958262: val_loss -0.1661
2024-12-18 03:30:12.958853: Pseudo dice [0.6077]
2024-12-18 03:30:12.959531: Epoch time: 436.67 s
2024-12-18 03:30:14.696985: 
2024-12-18 03:30:14.698117: Epoch 45
2024-12-18 03:30:14.698807: Current learning rate: 0.00725
2024-12-18 03:38:02.621266: Validation loss did not improve from -0.36030. Patience: 44/50
2024-12-18 03:38:02.622290: train_loss -0.8518
2024-12-18 03:38:02.622940: val_loss -0.1412
2024-12-18 03:38:02.623551: Pseudo dice [0.5904]
2024-12-18 03:38:02.624180: Epoch time: 467.93 s
2024-12-18 03:38:03.997513: 
2024-12-18 03:38:03.998922: Epoch 46
2024-12-18 03:38:03.999631: Current learning rate: 0.00719
2024-12-18 03:44:31.651569: Validation loss did not improve from -0.36030. Patience: 45/50
2024-12-18 03:44:31.652587: train_loss -0.8528
2024-12-18 03:44:31.653340: val_loss -0.157
2024-12-18 03:44:31.654035: Pseudo dice [0.6022]
2024-12-18 03:44:31.654777: Epoch time: 387.66 s
2024-12-18 03:44:33.025569: 
2024-12-18 03:44:33.026603: Epoch 47
2024-12-18 03:44:33.027442: Current learning rate: 0.00713
2024-12-18 03:50:01.084041: Validation loss did not improve from -0.36030. Patience: 46/50
2024-12-18 03:50:01.085060: train_loss -0.8548
2024-12-18 03:50:01.085773: val_loss -0.1574
2024-12-18 03:50:01.086446: Pseudo dice [0.6126]
2024-12-18 03:50:01.087227: Epoch time: 328.06 s
2024-12-18 03:50:02.513433: 
2024-12-18 03:50:02.514600: Epoch 48
2024-12-18 03:50:02.515270: Current learning rate: 0.00707
2024-12-18 03:57:04.975186: Validation loss did not improve from -0.36030. Patience: 47/50
2024-12-18 03:57:04.976180: train_loss -0.8542
2024-12-18 03:57:04.977000: val_loss -0.2109
2024-12-18 03:57:04.977663: Pseudo dice [0.6365]
2024-12-18 03:57:04.978470: Epoch time: 422.46 s
2024-12-18 03:57:06.406832: 
2024-12-18 03:57:06.407992: Epoch 49
2024-12-18 03:57:06.408714: Current learning rate: 0.007
2024-12-18 04:04:48.332709: Validation loss did not improve from -0.36030. Patience: 48/50
2024-12-18 04:04:48.335042: train_loss -0.8527
2024-12-18 04:04:48.335978: val_loss -0.1778
2024-12-18 04:04:48.336735: Pseudo dice [0.6203]
2024-12-18 04:04:48.337414: Epoch time: 461.93 s
2024-12-18 04:04:50.229458: 
2024-12-18 04:04:50.230778: Epoch 50
2024-12-18 04:04:50.231611: Current learning rate: 0.00694
2024-12-18 04:12:19.243933: Validation loss did not improve from -0.36030. Patience: 49/50
2024-12-18 04:12:19.244959: train_loss -0.8547
2024-12-18 04:12:19.245982: val_loss -0.0733
2024-12-18 04:12:19.246845: Pseudo dice [0.5642]
2024-12-18 04:12:19.247671: Epoch time: 449.02 s
2024-12-18 04:12:20.652295: 
2024-12-18 04:12:20.653368: Epoch 51
2024-12-18 04:12:20.654228: Current learning rate: 0.00688
2024-12-18 04:20:11.216105: Validation loss did not improve from -0.36030. Patience: 50/50
2024-12-18 04:20:11.218075: train_loss -0.8581
2024-12-18 04:20:11.218914: val_loss -0.1841
2024-12-18 04:20:11.219636: Pseudo dice [0.6134]
2024-12-18 04:20:11.220482: Epoch time: 470.57 s
2024-12-18 04:20:12.629306: 
2024-12-18 04:20:12.630433: Epoch 52
2024-12-18 04:20:12.631203: Current learning rate: 0.00682
2024-12-18 04:27:26.097807: Validation loss did not improve from -0.36030. Patience: 51/50
2024-12-18 04:27:26.098649: train_loss -0.859
2024-12-18 04:27:26.099532: val_loss -0.0891
2024-12-18 04:27:26.100340: Pseudo dice [0.5833]
2024-12-18 04:27:26.100936: Epoch time: 433.47 s
2024-12-18 04:27:27.923983: 
2024-12-18 04:27:27.925115: Epoch 53
2024-12-18 04:27:27.925795: Current learning rate: 0.00675
2024-12-18 04:34:00.749909: Validation loss did not improve from -0.36030. Patience: 52/50
2024-12-18 04:34:00.750960: train_loss -0.8588
2024-12-18 04:34:00.751794: val_loss -0.1755
2024-12-18 04:34:00.752434: Pseudo dice [0.6208]
2024-12-18 04:34:00.753206: Epoch time: 392.83 s
2024-12-18 04:34:02.175453: 
2024-12-18 04:34:02.176308: Epoch 54
2024-12-18 04:34:02.177001: Current learning rate: 0.00669
2024-12-18 04:40:55.944948: Validation loss did not improve from -0.36030. Patience: 53/50
2024-12-18 04:40:55.945787: train_loss -0.8612
2024-12-18 04:40:55.946509: val_loss -0.1508
2024-12-18 04:40:55.947110: Pseudo dice [0.6035]
2024-12-18 04:40:55.947713: Epoch time: 413.77 s
2024-12-18 04:40:57.730382: 
2024-12-18 04:40:57.731604: Epoch 55
2024-12-18 04:40:57.732273: Current learning rate: 0.00663
2024-12-18 04:47:55.208705: Validation loss did not improve from -0.36030. Patience: 54/50
2024-12-18 04:47:55.210048: train_loss -0.8605
2024-12-18 04:47:55.210858: val_loss -0.2039
2024-12-18 04:47:55.211478: Pseudo dice [0.6306]
2024-12-18 04:47:55.212223: Epoch time: 417.48 s
2024-12-18 04:47:56.631230: 
2024-12-18 04:47:56.632712: Epoch 56
2024-12-18 04:47:56.633916: Current learning rate: 0.00657
2024-12-18 04:55:02.189631: Validation loss did not improve from -0.36030. Patience: 55/50
2024-12-18 04:55:02.190460: train_loss -0.8619
2024-12-18 04:55:02.191281: val_loss -0.1183
2024-12-18 04:55:02.191911: Pseudo dice [0.6067]
2024-12-18 04:55:02.192649: Epoch time: 425.56 s
2024-12-18 04:55:03.662037: 
2024-12-18 04:55:03.663133: Epoch 57
2024-12-18 04:55:03.663880: Current learning rate: 0.0065
2024-12-18 05:01:31.785546: Validation loss did not improve from -0.36030. Patience: 56/50
2024-12-18 05:01:31.786431: train_loss -0.8625
2024-12-18 05:01:31.787437: val_loss -0.2382
2024-12-18 05:01:31.788351: Pseudo dice [0.6547]
2024-12-18 05:01:31.789145: Epoch time: 388.13 s
2024-12-18 05:01:33.275711: 
2024-12-18 05:01:33.277058: Epoch 58
2024-12-18 05:01:33.278168: Current learning rate: 0.00644
2024-12-18 05:08:52.073289: Validation loss did not improve from -0.36030. Patience: 57/50
2024-12-18 05:08:52.075086: train_loss -0.8657
2024-12-18 05:08:52.075942: val_loss -0.1405
2024-12-18 05:08:52.076600: Pseudo dice [0.6131]
2024-12-18 05:08:52.077302: Epoch time: 438.8 s
2024-12-18 05:08:53.546803: 
2024-12-18 05:08:53.547932: Epoch 59
2024-12-18 05:08:53.548609: Current learning rate: 0.00638
2024-12-18 05:15:42.853826: Validation loss did not improve from -0.36030. Patience: 58/50
2024-12-18 05:15:42.854675: train_loss -0.8643
2024-12-18 05:15:42.855420: val_loss -0.092
2024-12-18 05:15:42.856073: Pseudo dice [0.5745]
2024-12-18 05:15:42.856787: Epoch time: 409.31 s
2024-12-18 05:15:44.780287: 
2024-12-18 05:15:44.781508: Epoch 60
2024-12-18 05:15:44.782233: Current learning rate: 0.00631
2024-12-18 05:22:14.891708: Validation loss did not improve from -0.36030. Patience: 59/50
2024-12-18 05:22:14.908738: train_loss -0.8619
2024-12-18 05:22:14.910062: val_loss -0.1572
2024-12-18 05:22:14.910887: Pseudo dice [0.613]
2024-12-18 05:22:14.911757: Epoch time: 390.11 s
2024-12-18 05:22:16.486492: 
2024-12-18 05:22:16.487743: Epoch 61
2024-12-18 05:22:16.488633: Current learning rate: 0.00625
2024-12-18 05:28:50.922323: Validation loss did not improve from -0.36030. Patience: 60/50
2024-12-18 05:28:50.923273: train_loss -0.8666
2024-12-18 05:28:50.924093: val_loss -0.1464
2024-12-18 05:28:50.924722: Pseudo dice [0.6083]
2024-12-18 05:28:50.925371: Epoch time: 394.44 s
2024-12-18 05:28:52.450005: 
2024-12-18 05:28:52.451210: Epoch 62
2024-12-18 05:28:52.451918: Current learning rate: 0.00619
2024-12-18 05:35:26.207503: Validation loss did not improve from -0.36030. Patience: 61/50
2024-12-18 05:35:26.208419: train_loss -0.8693
2024-12-18 05:35:26.209175: val_loss -0.1061
2024-12-18 05:35:26.209890: Pseudo dice [0.5872]
2024-12-18 05:35:26.210716: Epoch time: 393.76 s
2024-12-18 05:35:27.712106: 
2024-12-18 05:35:27.713375: Epoch 63
2024-12-18 05:35:27.714176: Current learning rate: 0.00612
2024-12-18 05:42:43.982363: Validation loss did not improve from -0.36030. Patience: 62/50
2024-12-18 05:42:43.983262: train_loss -0.8695
2024-12-18 05:42:43.984107: val_loss -0.0858
2024-12-18 05:42:43.985136: Pseudo dice [0.5957]
2024-12-18 05:42:43.986035: Epoch time: 436.27 s
2024-12-18 05:42:46.024448: 
2024-12-18 05:42:46.025839: Epoch 64
2024-12-18 05:42:46.026782: Current learning rate: 0.00606
2024-12-18 05:50:25.321739: Validation loss did not improve from -0.36030. Patience: 63/50
2024-12-18 05:50:25.322618: train_loss -0.8687
2024-12-18 05:50:25.323617: val_loss -0.0707
2024-12-18 05:50:25.324402: Pseudo dice [0.5756]
2024-12-18 05:50:25.325281: Epoch time: 459.3 s
2024-12-18 05:50:27.252859: 
2024-12-18 05:50:27.254269: Epoch 65
2024-12-18 05:50:27.255145: Current learning rate: 0.006
2024-12-18 05:57:02.479698: Validation loss did not improve from -0.36030. Patience: 64/50
2024-12-18 05:57:02.480741: train_loss -0.8679
2024-12-18 05:57:02.481579: val_loss -0.0994
2024-12-18 05:57:02.482469: Pseudo dice [0.6034]
2024-12-18 05:57:02.483201: Epoch time: 395.23 s
2024-12-18 05:57:03.893026: 
2024-12-18 05:57:03.894304: Epoch 66
2024-12-18 05:57:03.895115: Current learning rate: 0.00593
2024-12-18 06:03:43.349293: Validation loss did not improve from -0.36030. Patience: 65/50
2024-12-18 06:03:43.350230: train_loss -0.8675
2024-12-18 06:03:43.351013: val_loss -0.1073
2024-12-18 06:03:43.351683: Pseudo dice [0.5812]
2024-12-18 06:03:43.352497: Epoch time: 399.46 s
2024-12-18 06:03:44.874279: 
2024-12-18 06:03:44.876378: Epoch 67
2024-12-18 06:03:44.877113: Current learning rate: 0.00587
2024-12-18 06:10:21.136788: Validation loss did not improve from -0.36030. Patience: 66/50
2024-12-18 06:10:21.137767: train_loss -0.8705
2024-12-18 06:10:21.138443: val_loss -0.0522
2024-12-18 06:10:21.139240: Pseudo dice [0.5778]
2024-12-18 06:10:21.140022: Epoch time: 396.27 s
2024-12-18 06:10:22.540790: 
2024-12-18 06:10:22.541944: Epoch 68
2024-12-18 06:10:22.542681: Current learning rate: 0.00581
2024-12-18 06:17:22.004363: Validation loss did not improve from -0.36030. Patience: 67/50
2024-12-18 06:17:22.007047: train_loss -0.8702
2024-12-18 06:17:22.008101: val_loss -0.1114
2024-12-18 06:17:22.009058: Pseudo dice [0.5984]
2024-12-18 06:17:22.009827: Epoch time: 419.47 s
2024-12-18 06:17:23.420134: 
2024-12-18 06:17:23.421463: Epoch 69
2024-12-18 06:17:23.422435: Current learning rate: 0.00574
2024-12-18 06:24:47.585271: Validation loss did not improve from -0.36030. Patience: 68/50
2024-12-18 06:24:47.586141: train_loss -0.8711
2024-12-18 06:24:47.586872: val_loss -0.0763
2024-12-18 06:24:47.587502: Pseudo dice [0.6044]
2024-12-18 06:24:47.588156: Epoch time: 444.17 s
2024-12-18 06:24:49.417939: 
2024-12-18 06:24:49.418972: Epoch 70
2024-12-18 06:24:49.419701: Current learning rate: 0.00568
2024-12-18 06:32:14.078406: Validation loss did not improve from -0.36030. Patience: 69/50
2024-12-18 06:32:14.079461: train_loss -0.8746
2024-12-18 06:32:14.080344: val_loss -0.1308
2024-12-18 06:32:14.081106: Pseudo dice [0.6161]
2024-12-18 06:32:14.081864: Epoch time: 444.66 s
2024-12-18 06:32:15.540174: 
2024-12-18 06:32:15.542443: Epoch 71
2024-12-18 06:32:15.543633: Current learning rate: 0.00562
2024-12-18 06:39:50.857306: Validation loss did not improve from -0.36030. Patience: 70/50
2024-12-18 06:39:50.858374: train_loss -0.8737
2024-12-18 06:39:50.859372: val_loss -0.0724
2024-12-18 06:39:50.860134: Pseudo dice [0.5828]
2024-12-18 06:39:50.860909: Epoch time: 455.32 s
2024-12-18 06:39:52.342355: 
2024-12-18 06:39:52.343608: Epoch 72
2024-12-18 06:39:52.344361: Current learning rate: 0.00555
2024-12-18 06:46:51.937022: Validation loss did not improve from -0.36030. Patience: 71/50
2024-12-18 06:46:51.938073: train_loss -0.8749
2024-12-18 06:46:51.938996: val_loss -0.1063
2024-12-18 06:46:51.939831: Pseudo dice [0.6045]
2024-12-18 06:46:51.940788: Epoch time: 419.6 s
2024-12-18 06:46:53.409312: 
2024-12-18 06:46:53.410690: Epoch 73
2024-12-18 06:46:53.411569: Current learning rate: 0.00549
2024-12-18 06:54:33.571788: Validation loss did not improve from -0.36030. Patience: 72/50
2024-12-18 06:54:33.572794: train_loss -0.8743
2024-12-18 06:54:33.573588: val_loss -0.1133
2024-12-18 06:54:33.574305: Pseudo dice [0.6104]
2024-12-18 06:54:33.575067: Epoch time: 460.16 s
2024-12-18 06:54:35.508416: 
2024-12-18 06:54:35.509480: Epoch 74
2024-12-18 06:54:35.510187: Current learning rate: 0.00542
2024-12-18 07:01:50.280152: Validation loss did not improve from -0.36030. Patience: 73/50
2024-12-18 07:01:50.281121: train_loss -0.8768
2024-12-18 07:01:50.281856: val_loss -0.0666
2024-12-18 07:01:50.282589: Pseudo dice [0.5858]
2024-12-18 07:01:50.283252: Epoch time: 434.77 s
2024-12-18 07:01:52.104285: 
2024-12-18 07:01:52.105390: Epoch 75
2024-12-18 07:01:52.106088: Current learning rate: 0.00536
2024-12-18 07:09:01.014336: Validation loss did not improve from -0.36030. Patience: 74/50
2024-12-18 07:09:01.015317: train_loss -0.8745
2024-12-18 07:09:01.016102: val_loss -0.1336
2024-12-18 07:09:01.016780: Pseudo dice [0.602]
2024-12-18 07:09:01.017521: Epoch time: 428.91 s
2024-12-18 07:09:02.540282: 
2024-12-18 07:09:02.541377: Epoch 76
2024-12-18 07:09:02.542221: Current learning rate: 0.00529
2024-12-18 07:16:01.565597: Validation loss did not improve from -0.36030. Patience: 75/50
2024-12-18 07:16:01.566602: train_loss -0.8759
2024-12-18 07:16:01.567461: val_loss -0.1225
2024-12-18 07:16:01.568324: Pseudo dice [0.6075]
2024-12-18 07:16:01.569327: Epoch time: 419.03 s
2024-12-18 07:16:03.028744: 
2024-12-18 07:16:03.030221: Epoch 77
2024-12-18 07:16:03.031279: Current learning rate: 0.00523
2024-12-18 07:23:23.128964: Validation loss did not improve from -0.36030. Patience: 76/50
2024-12-18 07:23:23.132382: train_loss -0.8782
2024-12-18 07:23:23.134635: val_loss -0.0988
2024-12-18 07:23:23.135550: Pseudo dice [0.5887]
2024-12-18 07:23:23.136676: Epoch time: 440.1 s
2024-12-18 07:23:24.731954: 
2024-12-18 07:23:24.733284: Epoch 78
2024-12-18 07:23:24.734231: Current learning rate: 0.00517
2024-12-18 07:29:40.979026: Validation loss did not improve from -0.36030. Patience: 77/50
2024-12-18 07:29:40.979949: train_loss -0.8773
2024-12-18 07:29:40.980759: val_loss -0.1508
2024-12-18 07:29:40.981658: Pseudo dice [0.6183]
2024-12-18 07:29:40.982445: Epoch time: 376.25 s
2024-12-18 07:29:42.455858: 
2024-12-18 07:29:42.457112: Epoch 79
2024-12-18 07:29:42.457972: Current learning rate: 0.0051
2024-12-18 07:36:59.427859: Validation loss did not improve from -0.36030. Patience: 78/50
2024-12-18 07:36:59.429795: train_loss -0.8797
2024-12-18 07:36:59.430773: val_loss -0.1128
2024-12-18 07:36:59.431531: Pseudo dice [0.6246]
2024-12-18 07:36:59.432422: Epoch time: 436.97 s
2024-12-18 07:37:01.564851: 
2024-12-18 07:37:01.566064: Epoch 80
2024-12-18 07:37:01.566831: Current learning rate: 0.00504
2024-12-18 07:44:05.668555: Validation loss did not improve from -0.36030. Patience: 79/50
2024-12-18 07:44:05.669392: train_loss -0.8797
2024-12-18 07:44:05.670268: val_loss -0.1128
2024-12-18 07:44:05.671112: Pseudo dice [0.5973]
2024-12-18 07:44:05.671951: Epoch time: 424.11 s
2024-12-18 07:44:07.367465: 
2024-12-18 07:44:07.368667: Epoch 81
2024-12-18 07:44:07.369458: Current learning rate: 0.00497
2024-12-18 07:50:57.293436: Validation loss did not improve from -0.36030. Patience: 80/50
2024-12-18 07:50:57.294390: train_loss -0.8807
2024-12-18 07:50:57.295177: val_loss -0.1424
2024-12-18 07:50:57.295817: Pseudo dice [0.6172]
2024-12-18 07:50:57.296584: Epoch time: 409.93 s
2024-12-18 07:50:58.799499: 
2024-12-18 07:50:58.800676: Epoch 82
2024-12-18 07:50:58.801402: Current learning rate: 0.00491
2024-12-18 07:57:01.423625: Validation loss did not improve from -0.36030. Patience: 81/50
2024-12-18 07:57:01.424331: train_loss -0.8806
2024-12-18 07:57:01.425044: val_loss -0.0966
2024-12-18 07:57:01.425646: Pseudo dice [0.6024]
2024-12-18 07:57:01.426285: Epoch time: 362.63 s
2024-12-18 07:57:02.862544: 
2024-12-18 07:57:02.863628: Epoch 83
2024-12-18 07:57:02.864401: Current learning rate: 0.00484
2024-12-18 08:03:36.001055: Validation loss did not improve from -0.36030. Patience: 82/50
2024-12-18 08:03:36.002114: train_loss -0.8802
2024-12-18 08:03:36.002834: val_loss -0.0906
2024-12-18 08:03:36.003474: Pseudo dice [0.5996]
2024-12-18 08:03:36.004111: Epoch time: 393.14 s
2024-12-18 08:03:37.495623: 
2024-12-18 08:03:37.496825: Epoch 84
2024-12-18 08:03:37.497510: Current learning rate: 0.00478
2024-12-18 08:10:53.707928: Validation loss did not improve from -0.36030. Patience: 83/50
2024-12-18 08:10:53.708779: train_loss -0.8817
2024-12-18 08:10:53.709629: val_loss -0.1318
2024-12-18 08:10:53.710411: Pseudo dice [0.6056]
2024-12-18 08:10:53.711236: Epoch time: 436.21 s
2024-12-18 08:10:56.003892: 
2024-12-18 08:10:56.004813: Epoch 85
2024-12-18 08:10:56.005460: Current learning rate: 0.00471
2024-12-18 08:18:33.865724: Validation loss did not improve from -0.36030. Patience: 84/50
2024-12-18 08:18:33.866648: train_loss -0.8832
2024-12-18 08:18:33.867510: val_loss -0.0698
2024-12-18 08:18:33.868189: Pseudo dice [0.5976]
2024-12-18 08:18:33.868864: Epoch time: 457.86 s
2024-12-18 08:18:35.267542: 
2024-12-18 08:18:35.268700: Epoch 86
2024-12-18 08:18:35.269484: Current learning rate: 0.00465
2024-12-18 08:26:16.667646: Validation loss did not improve from -0.36030. Patience: 85/50
2024-12-18 08:26:16.669306: train_loss -0.8824
2024-12-18 08:26:16.670142: val_loss -0.1062
2024-12-18 08:26:16.670882: Pseudo dice [0.615]
2024-12-18 08:26:16.671582: Epoch time: 461.4 s
2024-12-18 08:26:18.155594: 
2024-12-18 08:26:18.156877: Epoch 87
2024-12-18 08:26:18.157916: Current learning rate: 0.00458
2024-12-18 08:32:47.416051: Validation loss did not improve from -0.36030. Patience: 86/50
2024-12-18 08:32:47.417040: train_loss -0.882
2024-12-18 08:32:47.417803: val_loss -0.118
2024-12-18 08:32:47.418546: Pseudo dice [0.6118]
2024-12-18 08:32:47.419345: Epoch time: 389.26 s
2024-12-18 08:32:48.861508: 
2024-12-18 08:32:48.862879: Epoch 88
2024-12-18 08:32:48.863871: Current learning rate: 0.00452
2024-12-18 08:39:42.906812: Validation loss did not improve from -0.36030. Patience: 87/50
2024-12-18 08:39:42.907964: train_loss -0.8825
2024-12-18 08:39:42.908962: val_loss -0.1292
2024-12-18 08:39:42.909732: Pseudo dice [0.6198]
2024-12-18 08:39:42.910563: Epoch time: 414.05 s
2024-12-18 08:39:44.394642: 
2024-12-18 08:39:44.396056: Epoch 89
2024-12-18 08:39:44.397010: Current learning rate: 0.00445
2024-12-18 08:46:52.793141: Validation loss did not improve from -0.36030. Patience: 88/50
2024-12-18 08:46:52.794210: train_loss -0.8853
2024-12-18 08:46:52.795035: val_loss -0.0566
2024-12-18 08:46:52.795853: Pseudo dice [0.5905]
2024-12-18 08:46:52.796774: Epoch time: 428.4 s
2024-12-18 08:46:54.653340: 
2024-12-18 08:46:54.654799: Epoch 90
2024-12-18 08:46:54.655765: Current learning rate: 0.00438
2024-12-18 08:53:57.273534: Validation loss did not improve from -0.36030. Patience: 89/50
2024-12-18 08:53:57.274393: train_loss -0.8848
2024-12-18 08:53:57.275036: val_loss -0.0593
2024-12-18 08:53:57.275701: Pseudo dice [0.602]
2024-12-18 08:53:57.276357: Epoch time: 422.62 s
2024-12-18 08:53:58.716256: 
2024-12-18 08:53:58.717518: Epoch 91
2024-12-18 08:53:58.718322: Current learning rate: 0.00432
2024-12-18 09:01:29.527180: Validation loss did not improve from -0.36030. Patience: 90/50
2024-12-18 09:01:29.528184: train_loss -0.8833
2024-12-18 09:01:29.528968: val_loss -0.0954
2024-12-18 09:01:29.529668: Pseudo dice [0.6052]
2024-12-18 09:01:29.530501: Epoch time: 450.81 s
2024-12-18 09:01:30.944496: 
2024-12-18 09:01:30.945679: Epoch 92
2024-12-18 09:01:30.946414: Current learning rate: 0.00425
2024-12-18 09:08:56.989267: Validation loss did not improve from -0.36030. Patience: 91/50
2024-12-18 09:08:56.990144: train_loss -0.8859
2024-12-18 09:08:56.990898: val_loss -0.063
2024-12-18 09:08:56.991679: Pseudo dice [0.6005]
2024-12-18 09:08:56.992509: Epoch time: 446.05 s
2024-12-18 09:08:58.414441: 
2024-12-18 09:08:58.415844: Epoch 93
2024-12-18 09:08:58.416730: Current learning rate: 0.00419
2024-12-18 09:14:55.435045: Validation loss did not improve from -0.36030. Patience: 92/50
2024-12-18 09:14:55.435886: train_loss -0.8872
2024-12-18 09:14:55.436864: val_loss -0.088
2024-12-18 09:14:55.437857: Pseudo dice [0.6242]
2024-12-18 09:14:55.438768: Epoch time: 357.02 s
2024-12-18 09:14:56.858379: 
2024-12-18 09:14:56.859310: Epoch 94
2024-12-18 09:14:56.860246: Current learning rate: 0.00412
2024-12-18 09:21:54.632484: Validation loss did not improve from -0.36030. Patience: 93/50
2024-12-18 09:21:54.633549: train_loss -0.8883
2024-12-18 09:21:54.634382: val_loss -0.0832
2024-12-18 09:21:54.635225: Pseudo dice [0.5948]
2024-12-18 09:21:54.635973: Epoch time: 417.78 s
2024-12-18 09:21:56.444018: 
2024-12-18 09:21:56.445420: Epoch 95
2024-12-18 09:21:56.446511: Current learning rate: 0.00405
2024-12-18 09:28:43.980814: Validation loss did not improve from -0.36030. Patience: 94/50
2024-12-18 09:28:43.981940: train_loss -0.8874
2024-12-18 09:28:43.982782: val_loss -0.0846
2024-12-18 09:28:43.983496: Pseudo dice [0.6087]
2024-12-18 09:28:43.984173: Epoch time: 407.54 s
2024-12-18 09:28:45.430352: 
2024-12-18 09:28:45.431459: Epoch 96
2024-12-18 09:28:45.432289: Current learning rate: 0.00399
2024-12-18 09:35:54.897977: Validation loss did not improve from -0.36030. Patience: 95/50
2024-12-18 09:35:54.899711: train_loss -0.8869
2024-12-18 09:35:54.900549: val_loss 0.0005
2024-12-18 09:35:54.901359: Pseudo dice [0.5782]
2024-12-18 09:35:54.902120: Epoch time: 429.47 s
2024-12-18 09:35:56.831338: 
2024-12-18 09:35:56.832880: Epoch 97
2024-12-18 09:35:56.833612: Current learning rate: 0.00392
2024-12-18 09:43:27.496228: Validation loss did not improve from -0.36030. Patience: 96/50
2024-12-18 09:43:27.497804: train_loss -0.8885
2024-12-18 09:43:27.498582: val_loss -0.0997
2024-12-18 09:43:27.499208: Pseudo dice [0.611]
2024-12-18 09:43:27.499974: Epoch time: 450.67 s
2024-12-18 09:43:28.910336: 
2024-12-18 09:43:28.911670: Epoch 98
2024-12-18 09:43:28.912544: Current learning rate: 0.00385
2024-12-18 09:51:08.002718: Validation loss did not improve from -0.36030. Patience: 97/50
2024-12-18 09:51:08.003697: train_loss -0.8908
2024-12-18 09:51:08.004565: val_loss -0.0399
2024-12-18 09:51:08.005311: Pseudo dice [0.5833]
2024-12-18 09:51:08.005974: Epoch time: 459.09 s
2024-12-18 09:51:09.466782: 
2024-12-18 09:51:09.468434: Epoch 99
2024-12-18 09:51:09.469361: Current learning rate: 0.00379
2024-12-18 09:59:01.543040: Validation loss did not improve from -0.36030. Patience: 98/50
2024-12-18 09:59:01.543796: train_loss -0.8884
2024-12-18 09:59:01.544553: val_loss -0.1009
2024-12-18 09:59:01.545296: Pseudo dice [0.6118]
2024-12-18 09:59:01.545923: Epoch time: 472.08 s
2024-12-18 09:59:03.477656: 
2024-12-18 09:59:03.479219: Epoch 100
2024-12-18 09:59:03.479938: Current learning rate: 0.00372
2024-12-18 10:06:42.938175: Validation loss did not improve from -0.36030. Patience: 99/50
2024-12-18 10:06:42.939134: train_loss -0.8889
2024-12-18 10:06:42.939919: val_loss -0.1245
2024-12-18 10:06:42.940613: Pseudo dice [0.6185]
2024-12-18 10:06:42.941260: Epoch time: 459.46 s
2024-12-18 10:06:44.378284: 
2024-12-18 10:06:44.379794: Epoch 101
2024-12-18 10:06:44.380507: Current learning rate: 0.00365
2024-12-18 10:13:58.308907: Validation loss did not improve from -0.36030. Patience: 100/50
2024-12-18 10:13:58.310190: train_loss -0.8895
2024-12-18 10:13:58.311084: val_loss 0.0186
2024-12-18 10:13:58.311903: Pseudo dice [0.5679]
2024-12-18 10:13:58.312646: Epoch time: 433.93 s
2024-12-18 10:14:00.024002: 
2024-12-18 10:14:00.025915: Epoch 102
2024-12-18 10:14:00.027909: Current learning rate: 0.00359
2024-12-18 10:20:25.018765: Validation loss did not improve from -0.36030. Patience: 101/50
2024-12-18 10:20:25.019628: train_loss -0.8918
2024-12-18 10:20:25.020393: val_loss -0.105
2024-12-18 10:20:25.021105: Pseudo dice [0.6153]
2024-12-18 10:20:25.021777: Epoch time: 385.0 s
2024-12-18 10:20:26.498758: 
2024-12-18 10:20:26.500397: Epoch 103
2024-12-18 10:20:26.501301: Current learning rate: 0.00352
2024-12-18 10:26:48.538962: Validation loss did not improve from -0.36030. Patience: 102/50
2024-12-18 10:26:48.539978: train_loss -0.8913
2024-12-18 10:26:48.540849: val_loss -0.0574
2024-12-18 10:26:48.541658: Pseudo dice [0.6109]
2024-12-18 10:26:48.542446: Epoch time: 382.04 s
2024-12-18 10:26:50.013439: 
2024-12-18 10:26:50.015016: Epoch 104
2024-12-18 10:26:50.015910: Current learning rate: 0.00345
2024-12-18 10:33:13.266600: Validation loss did not improve from -0.36030. Patience: 103/50
2024-12-18 10:33:13.267438: train_loss -0.8924
2024-12-18 10:33:13.268379: val_loss -0.0338
2024-12-18 10:33:13.269112: Pseudo dice [0.5954]
2024-12-18 10:33:13.269875: Epoch time: 383.26 s
2024-12-18 10:33:15.219073: 
2024-12-18 10:33:15.220712: Epoch 105
2024-12-18 10:33:15.221639: Current learning rate: 0.00338
2024-12-18 10:40:08.558916: Validation loss did not improve from -0.36030. Patience: 104/50
2024-12-18 10:40:08.560450: train_loss -0.8931
2024-12-18 10:40:08.561158: val_loss -0.0986
2024-12-18 10:40:08.561803: Pseudo dice [0.5957]
2024-12-18 10:40:08.562468: Epoch time: 413.34 s
2024-12-18 10:40:10.009748: 
2024-12-18 10:40:10.011459: Epoch 106
2024-12-18 10:40:10.012337: Current learning rate: 0.00332
2024-12-18 10:47:46.296324: Validation loss did not improve from -0.36030. Patience: 105/50
2024-12-18 10:47:46.297806: train_loss -0.8956
2024-12-18 10:47:46.298601: val_loss -0.044
2024-12-18 10:47:46.299323: Pseudo dice [0.5876]
2024-12-18 10:47:46.300231: Epoch time: 456.29 s
2024-12-18 10:47:47.855365: 
2024-12-18 10:47:47.856610: Epoch 107
2024-12-18 10:47:47.857412: Current learning rate: 0.00325
2024-12-18 10:55:38.825366: Validation loss did not improve from -0.36030. Patience: 106/50
2024-12-18 10:55:38.826228: train_loss -0.8945
2024-12-18 10:55:38.826953: val_loss -0.0226
2024-12-18 10:55:38.827897: Pseudo dice [0.5841]
2024-12-18 10:55:38.828721: Epoch time: 470.97 s
2024-12-18 10:55:40.770694: 
2024-12-18 10:55:40.771604: Epoch 108
2024-12-18 10:55:40.772346: Current learning rate: 0.00318
2024-12-18 11:03:22.139498: Validation loss did not improve from -0.36030. Patience: 107/50
2024-12-18 11:03:22.140594: train_loss -0.8945
2024-12-18 11:03:22.141878: val_loss -0.0661
2024-12-18 11:03:22.142909: Pseudo dice [0.5883]
2024-12-18 11:03:22.143986: Epoch time: 461.37 s
2024-12-18 11:03:23.702907: 
2024-12-18 11:03:23.704372: Epoch 109
2024-12-18 11:03:23.705373: Current learning rate: 0.00311
2024-12-18 11:11:12.340226: Validation loss did not improve from -0.36030. Patience: 108/50
2024-12-18 11:11:12.341115: train_loss -0.8946
2024-12-18 11:11:12.342225: val_loss -0.0723
2024-12-18 11:11:12.343152: Pseudo dice [0.6085]
2024-12-18 11:11:12.344155: Epoch time: 468.64 s
2024-12-18 11:11:14.231306: 
2024-12-18 11:11:14.232616: Epoch 110
2024-12-18 11:11:14.233707: Current learning rate: 0.00304
2024-12-18 11:18:09.785017: Validation loss did not improve from -0.36030. Patience: 109/50
2024-12-18 11:18:09.785900: train_loss -0.8952
2024-12-18 11:18:09.786631: val_loss -0.0214
2024-12-18 11:18:09.787277: Pseudo dice [0.5843]
2024-12-18 11:18:09.787952: Epoch time: 415.56 s
2024-12-18 11:18:11.231193: 
2024-12-18 11:18:11.232417: Epoch 111
2024-12-18 11:18:11.233105: Current learning rate: 0.00297
2024-12-18 11:25:21.054296: Validation loss did not improve from -0.36030. Patience: 110/50
2024-12-18 11:25:21.055145: train_loss -0.897
2024-12-18 11:25:21.055998: val_loss -0.0249
2024-12-18 11:25:21.056615: Pseudo dice [0.5881]
2024-12-18 11:25:21.057265: Epoch time: 429.83 s
2024-12-18 11:25:22.516244: 
2024-12-18 11:25:22.517606: Epoch 112
2024-12-18 11:25:22.518447: Current learning rate: 0.00291
2024-12-18 11:32:32.486186: Validation loss did not improve from -0.36030. Patience: 111/50
2024-12-18 11:32:32.486908: train_loss -0.8972
2024-12-18 11:32:32.487680: val_loss -0.0421
2024-12-18 11:32:32.488413: Pseudo dice [0.6019]
2024-12-18 11:32:32.489043: Epoch time: 429.97 s
2024-12-18 11:32:33.937889: 
2024-12-18 11:32:33.939127: Epoch 113
2024-12-18 11:32:33.939733: Current learning rate: 0.00284
2024-12-18 11:39:43.916345: Validation loss did not improve from -0.36030. Patience: 112/50
2024-12-18 11:39:43.918843: train_loss -0.8976
2024-12-18 11:39:43.919707: val_loss -0.0479
2024-12-18 11:39:43.920366: Pseudo dice [0.5983]
2024-12-18 11:39:43.921006: Epoch time: 429.98 s
2024-12-18 11:39:45.387449: 
2024-12-18 11:39:45.388501: Epoch 114
2024-12-18 11:39:45.389264: Current learning rate: 0.00277
2024-12-18 11:46:19.518169: Validation loss did not improve from -0.36030. Patience: 113/50
2024-12-18 11:46:19.519085: train_loss -0.898
2024-12-18 11:46:19.519843: val_loss -0.0436
2024-12-18 11:46:19.520525: Pseudo dice [0.583]
2024-12-18 11:46:19.521247: Epoch time: 394.13 s
2024-12-18 11:46:21.385933: 
2024-12-18 11:46:21.387141: Epoch 115
2024-12-18 11:46:21.387951: Current learning rate: 0.0027
2024-12-18 11:53:51.600735: Validation loss did not improve from -0.36030. Patience: 114/50
2024-12-18 11:53:51.601709: train_loss -0.8987
2024-12-18 11:53:51.602571: val_loss -0.0645
2024-12-18 11:53:51.603217: Pseudo dice [0.5893]
2024-12-18 11:53:51.603966: Epoch time: 450.22 s
2024-12-18 11:53:53.112461: 
2024-12-18 11:53:53.113650: Epoch 116
2024-12-18 11:53:53.114398: Current learning rate: 0.00263
2024-12-18 12:01:21.510942: Validation loss did not improve from -0.36030. Patience: 115/50
2024-12-18 12:01:21.512038: train_loss -0.8989
2024-12-18 12:01:21.512719: val_loss -0.0096
2024-12-18 12:01:21.513319: Pseudo dice [0.5954]
2024-12-18 12:01:21.513883: Epoch time: 448.4 s
2024-12-18 12:01:22.945107: 
2024-12-18 12:01:22.946270: Epoch 117
2024-12-18 12:01:22.946906: Current learning rate: 0.00256
2024-12-18 12:09:09.318342: Validation loss did not improve from -0.36030. Patience: 116/50
2024-12-18 12:09:09.319105: train_loss -0.8984
2024-12-18 12:09:09.319875: val_loss -0.0238
2024-12-18 12:09:09.320545: Pseudo dice [0.5879]
2024-12-18 12:09:09.321320: Epoch time: 466.38 s
2024-12-18 12:09:10.835620: 
2024-12-18 12:09:10.836894: Epoch 118
2024-12-18 12:09:10.837886: Current learning rate: 0.00249
2024-12-18 12:16:34.802239: Validation loss did not improve from -0.36030. Patience: 117/50
2024-12-18 12:16:34.803296: train_loss -0.8979
2024-12-18 12:16:34.803999: val_loss -0.0391
2024-12-18 12:16:34.804791: Pseudo dice [0.616]
2024-12-18 12:16:34.805390: Epoch time: 443.97 s
2024-12-18 12:16:36.303837: 
2024-12-18 12:16:36.305147: Epoch 119
2024-12-18 12:16:36.305792: Current learning rate: 0.00242
2024-12-18 12:23:48.106001: Validation loss did not improve from -0.36030. Patience: 118/50
2024-12-18 12:23:48.107080: train_loss -0.8983
2024-12-18 12:23:48.107808: val_loss -0.022
2024-12-18 12:23:48.108480: Pseudo dice [0.5735]
2024-12-18 12:23:48.109107: Epoch time: 431.8 s
2024-12-18 12:23:50.440970: 
2024-12-18 12:23:50.442353: Epoch 120
2024-12-18 12:23:50.443130: Current learning rate: 0.00235
2024-12-18 12:31:27.039448: Validation loss did not improve from -0.36030. Patience: 119/50
2024-12-18 12:31:27.040384: train_loss -0.8996
2024-12-18 12:31:27.041253: val_loss 0.0059
2024-12-18 12:31:27.041986: Pseudo dice [0.5765]
2024-12-18 12:31:27.042789: Epoch time: 456.6 s
2024-12-18 12:31:28.604029: 
2024-12-18 12:31:28.605391: Epoch 121
2024-12-18 12:31:28.606129: Current learning rate: 0.00228
2024-12-18 12:39:00.637641: Validation loss did not improve from -0.36030. Patience: 120/50
2024-12-18 12:39:00.638551: train_loss -0.8987
2024-12-18 12:39:00.639276: val_loss -0.0198
2024-12-18 12:39:00.640064: Pseudo dice [0.5717]
2024-12-18 12:39:00.640748: Epoch time: 452.04 s
2024-12-18 12:39:02.111040: 
2024-12-18 12:39:02.112073: Epoch 122
2024-12-18 12:39:02.112861: Current learning rate: 0.00221
2024-12-18 12:46:44.475817: Validation loss did not improve from -0.36030. Patience: 121/50
2024-12-18 12:46:44.493446: train_loss -0.9014
2024-12-18 12:46:44.494650: val_loss 0.0602
2024-12-18 12:46:44.495302: Pseudo dice [0.5796]
2024-12-18 12:46:44.495985: Epoch time: 462.38 s
2024-12-18 12:46:45.962057: 
2024-12-18 12:46:45.963268: Epoch 123
2024-12-18 12:46:45.964121: Current learning rate: 0.00214
2024-12-18 12:54:14.592976: Validation loss did not improve from -0.36030. Patience: 122/50
2024-12-18 12:54:14.593896: train_loss -0.9027
2024-12-18 12:54:14.594819: val_loss 0.0139
2024-12-18 12:54:14.595653: Pseudo dice [0.5797]
2024-12-18 12:54:14.596587: Epoch time: 448.63 s
2024-12-18 12:54:16.093040: 
2024-12-18 12:54:16.094324: Epoch 124
2024-12-18 12:54:16.095134: Current learning rate: 0.00207
2024-12-18 13:01:24.463971: Validation loss did not improve from -0.36030. Patience: 123/50
2024-12-18 13:01:24.464956: train_loss -0.9008
2024-12-18 13:01:24.465870: val_loss -0.0078
2024-12-18 13:01:24.466776: Pseudo dice [0.5701]
2024-12-18 13:01:24.467667: Epoch time: 428.37 s
2024-12-18 13:01:26.401419: 
2024-12-18 13:01:26.402540: Epoch 125
2024-12-18 13:01:26.403441: Current learning rate: 0.00199
2024-12-18 13:08:41.225197: Validation loss did not improve from -0.36030. Patience: 124/50
2024-12-18 13:08:41.226412: train_loss -0.9018
2024-12-18 13:08:41.227214: val_loss -0.0119
2024-12-18 13:08:41.227900: Pseudo dice [0.583]
2024-12-18 13:08:41.228607: Epoch time: 434.83 s
2024-12-18 13:08:42.678266: 
2024-12-18 13:08:42.679566: Epoch 126
2024-12-18 13:08:42.680243: Current learning rate: 0.00192
2024-12-18 13:16:32.131766: Validation loss did not improve from -0.36030. Patience: 125/50
2024-12-18 13:16:32.132743: train_loss -0.9016
2024-12-18 13:16:32.133500: val_loss -0.0181
2024-12-18 13:16:32.134140: Pseudo dice [0.6005]
2024-12-18 13:16:32.134737: Epoch time: 469.46 s
2024-12-18 13:16:33.602978: 
2024-12-18 13:16:33.604179: Epoch 127
2024-12-18 13:16:33.604980: Current learning rate: 0.00185
2024-12-18 13:23:57.648598: Validation loss did not improve from -0.36030. Patience: 126/50
2024-12-18 13:23:57.649525: train_loss -0.9023
2024-12-18 13:23:57.650351: val_loss -0.0248
2024-12-18 13:23:57.651165: Pseudo dice [0.5864]
2024-12-18 13:23:57.651974: Epoch time: 444.05 s
2024-12-18 13:23:59.163821: 
2024-12-18 13:23:59.164969: Epoch 128
2024-12-18 13:23:59.165775: Current learning rate: 0.00178
2024-12-18 13:31:27.665910: Validation loss did not improve from -0.36030. Patience: 127/50
2024-12-18 13:31:27.667396: train_loss -0.9039
2024-12-18 13:31:27.668355: val_loss -0.0086
2024-12-18 13:31:27.668965: Pseudo dice [0.5954]
2024-12-18 13:31:27.669642: Epoch time: 448.5 s
2024-12-18 13:31:29.139755: 
2024-12-18 13:31:29.140853: Epoch 129
2024-12-18 13:31:29.141610: Current learning rate: 0.0017
2024-12-18 13:38:35.112663: Validation loss did not improve from -0.36030. Patience: 128/50
2024-12-18 13:38:35.113667: train_loss -0.9023
2024-12-18 13:38:35.114472: val_loss 0.0495
2024-12-18 13:38:35.115198: Pseudo dice [0.5885]
2024-12-18 13:38:35.115911: Epoch time: 425.98 s
2024-12-18 13:38:37.273403: 
2024-12-18 13:38:37.274618: Epoch 130
2024-12-18 13:38:37.275473: Current learning rate: 0.00163
2024-12-18 13:45:44.517179: Validation loss did not improve from -0.36030. Patience: 129/50
2024-12-18 13:45:44.518237: train_loss -0.9026
2024-12-18 13:45:44.519121: val_loss 0.0131
2024-12-18 13:45:44.519816: Pseudo dice [0.5811]
2024-12-18 13:45:44.520442: Epoch time: 427.25 s
2024-12-18 13:45:45.995820: 
2024-12-18 13:45:45.997062: Epoch 131
2024-12-18 13:45:45.997802: Current learning rate: 0.00156
2024-12-18 13:52:35.468936: Validation loss did not improve from -0.36030. Patience: 130/50
2024-12-18 13:52:35.472988: train_loss -0.904
2024-12-18 13:52:35.474251: val_loss -0.0439
2024-12-18 13:52:35.474941: Pseudo dice [0.5945]
2024-12-18 13:52:35.475651: Epoch time: 409.48 s
2024-12-18 13:52:36.943087: 
2024-12-18 13:52:36.944156: Epoch 132
2024-12-18 13:52:36.945162: Current learning rate: 0.00148
2024-12-18 13:59:57.119567: Validation loss did not improve from -0.36030. Patience: 131/50
2024-12-18 13:59:57.120511: train_loss -0.9043
2024-12-18 13:59:57.121245: val_loss -0.0599
2024-12-18 13:59:57.121907: Pseudo dice [0.6042]
2024-12-18 13:59:57.122571: Epoch time: 440.18 s
2024-12-18 13:59:58.606037: 
2024-12-18 13:59:58.607023: Epoch 133
2024-12-18 13:59:58.607708: Current learning rate: 0.00141
2024-12-18 14:06:54.737173: Validation loss did not improve from -0.36030. Patience: 132/50
2024-12-18 14:06:54.738266: train_loss -0.9038
2024-12-18 14:06:54.739044: val_loss -0.0244
2024-12-18 14:06:54.739742: Pseudo dice [0.5942]
2024-12-18 14:06:54.740341: Epoch time: 416.13 s
2024-12-18 14:06:56.188851: 
2024-12-18 14:06:56.189989: Epoch 134
2024-12-18 14:06:56.190718: Current learning rate: 0.00133
2024-12-18 14:14:33.182246: Validation loss did not improve from -0.36030. Patience: 133/50
2024-12-18 14:14:33.183568: train_loss -0.9064
2024-12-18 14:14:33.184360: val_loss 0.026
2024-12-18 14:14:33.185137: Pseudo dice [0.579]
2024-12-18 14:14:33.186080: Epoch time: 457.0 s
2024-12-18 14:14:35.153473: 
2024-12-18 14:14:35.154672: Epoch 135
2024-12-18 14:14:35.155551: Current learning rate: 0.00126
2024-12-18 14:21:48.788776: Validation loss did not improve from -0.36030. Patience: 134/50
2024-12-18 14:21:48.789496: train_loss -0.9044
2024-12-18 14:21:48.790145: val_loss -0.0274
2024-12-18 14:21:48.790787: Pseudo dice [0.5925]
2024-12-18 14:21:48.791344: Epoch time: 433.64 s
2024-12-18 14:21:50.230538: 
2024-12-18 14:21:50.231700: Epoch 136
2024-12-18 14:21:50.232321: Current learning rate: 0.00118
2024-12-18 14:29:11.048301: Validation loss did not improve from -0.36030. Patience: 135/50
2024-12-18 14:29:11.049587: train_loss -0.905
2024-12-18 14:29:11.050648: val_loss 0.0084
2024-12-18 14:29:11.051467: Pseudo dice [0.5924]
2024-12-18 14:29:11.052337: Epoch time: 440.82 s
2024-12-18 14:29:12.506912: 
2024-12-18 14:29:12.508145: Epoch 137
2024-12-18 14:29:12.508733: Current learning rate: 0.00111
2024-12-18 14:37:03.832265: Validation loss did not improve from -0.36030. Patience: 136/50
2024-12-18 14:37:03.833249: train_loss -0.9067
2024-12-18 14:37:03.833938: val_loss 0.0819
2024-12-18 14:37:03.834580: Pseudo dice [0.5792]
2024-12-18 14:37:03.835258: Epoch time: 471.33 s
2024-12-18 14:37:05.245784: 
2024-12-18 14:37:05.246988: Epoch 138
2024-12-18 14:37:05.247742: Current learning rate: 0.00103
2024-12-18 14:45:10.543492: Validation loss did not improve from -0.36030. Patience: 137/50
2024-12-18 14:45:10.545087: train_loss -0.906
2024-12-18 14:45:10.546105: val_loss -0.0179
2024-12-18 14:45:10.547037: Pseudo dice [0.5916]
2024-12-18 14:45:10.547941: Epoch time: 485.3 s
2024-12-18 14:45:12.021096: 
2024-12-18 14:45:12.022360: Epoch 139
2024-12-18 14:45:12.023031: Current learning rate: 0.00095
2024-12-18 14:53:07.917214: Validation loss did not improve from -0.36030. Patience: 138/50
2024-12-18 14:53:07.918212: train_loss -0.9057
2024-12-18 14:53:07.919143: val_loss 0.0198
2024-12-18 14:53:07.920048: Pseudo dice [0.5712]
2024-12-18 14:53:07.920876: Epoch time: 475.9 s
2024-12-18 14:53:09.770625: 
2024-12-18 14:53:09.771870: Epoch 140
2024-12-18 14:53:09.772649: Current learning rate: 0.00087
2024-12-18 15:00:29.753339: Validation loss did not improve from -0.36030. Patience: 139/50
2024-12-18 15:00:29.754240: train_loss -0.9076
2024-12-18 15:00:29.755103: val_loss 0.1129
2024-12-18 15:00:29.755762: Pseudo dice [0.5555]
2024-12-18 15:00:29.756404: Epoch time: 439.98 s
2024-12-18 15:00:31.233025: 
2024-12-18 15:00:31.234368: Epoch 141
2024-12-18 15:00:31.235136: Current learning rate: 0.00079
2024-12-18 15:07:47.814485: Validation loss did not improve from -0.36030. Patience: 140/50
2024-12-18 15:07:47.815516: train_loss -0.9078
2024-12-18 15:07:47.816167: val_loss -0.0023
2024-12-18 15:07:47.816810: Pseudo dice [0.5826]
2024-12-18 15:07:47.817436: Epoch time: 436.58 s
2024-12-18 15:07:49.647919: 
2024-12-18 15:07:49.649351: Epoch 142
2024-12-18 15:07:49.650105: Current learning rate: 0.00071
2024-12-18 15:14:34.873652: Validation loss did not improve from -0.36030. Patience: 141/50
2024-12-18 15:14:34.874510: train_loss -0.9094
2024-12-18 15:14:34.875741: val_loss -0.0323
2024-12-18 15:14:34.876554: Pseudo dice [0.6126]
2024-12-18 15:14:34.877367: Epoch time: 405.23 s
2024-12-18 15:14:36.276898: 
2024-12-18 15:14:36.277849: Epoch 143
2024-12-18 15:14:36.278573: Current learning rate: 0.00063
2024-12-18 15:21:49.344031: Validation loss did not improve from -0.36030. Patience: 142/50
2024-12-18 15:21:49.344973: train_loss -0.9074
2024-12-18 15:21:49.345613: val_loss 0.0013
2024-12-18 15:21:49.346194: Pseudo dice [0.5908]
2024-12-18 15:21:49.346793: Epoch time: 433.07 s
2024-12-18 15:21:50.753501: 
2024-12-18 15:21:50.754505: Epoch 144
2024-12-18 15:21:50.755068: Current learning rate: 0.00055
2024-12-18 15:29:33.143353: Validation loss did not improve from -0.36030. Patience: 143/50
2024-12-18 15:29:33.144258: train_loss -0.9072
2024-12-18 15:29:33.145158: val_loss -0.0037
2024-12-18 15:29:33.145814: Pseudo dice [0.5803]
2024-12-18 15:29:33.146511: Epoch time: 462.39 s
2024-12-18 15:29:34.996761: 
2024-12-18 15:29:34.997861: Epoch 145
2024-12-18 15:29:34.998534: Current learning rate: 0.00047
2024-12-18 15:37:28.116290: Validation loss did not improve from -0.36030. Patience: 144/50
2024-12-18 15:37:28.119549: train_loss -0.9084
2024-12-18 15:37:28.121626: val_loss -0.0149
2024-12-18 15:37:28.122543: Pseudo dice [0.586]
2024-12-18 15:37:28.123355: Epoch time: 473.12 s
2024-12-18 15:37:29.603828: 
2024-12-18 15:37:29.604844: Epoch 146
2024-12-18 15:37:29.605462: Current learning rate: 0.00038
2024-12-18 15:45:04.468380: Validation loss did not improve from -0.36030. Patience: 145/50
2024-12-18 15:45:04.469201: train_loss -0.9057
2024-12-18 15:45:04.470044: val_loss -0.0037
2024-12-18 15:45:04.470834: Pseudo dice [0.5951]
2024-12-18 15:45:04.471598: Epoch time: 454.87 s
2024-12-18 15:45:05.868431: 
2024-12-18 15:45:05.869522: Epoch 147
2024-12-18 15:45:05.870304: Current learning rate: 0.0003
2024-12-18 15:52:50.146494: Validation loss did not improve from -0.36030. Patience: 146/50
2024-12-18 15:52:50.147610: train_loss -0.9075
2024-12-18 15:52:50.148427: val_loss 0.0114
2024-12-18 15:52:50.149016: Pseudo dice [0.585]
2024-12-18 15:52:50.149687: Epoch time: 464.28 s
2024-12-18 15:52:51.552389: 
2024-12-18 15:52:51.553556: Epoch 148
2024-12-18 15:52:51.554168: Current learning rate: 0.00021
2024-12-18 16:00:24.107367: Validation loss did not improve from -0.36030. Patience: 147/50
2024-12-18 16:00:24.108321: train_loss -0.9091
2024-12-18 16:00:24.109126: val_loss 0.0075
2024-12-18 16:00:24.110333: Pseudo dice [0.6018]
2024-12-18 16:00:24.111152: Epoch time: 452.56 s
2024-12-18 16:00:25.527734: 
2024-12-18 16:00:25.528945: Epoch 149
2024-12-18 16:00:25.529696: Current learning rate: 0.00011
2024-12-18 16:08:44.898023: Validation loss did not improve from -0.36030. Patience: 148/50
2024-12-18 16:08:44.899323: train_loss -0.9065
2024-12-18 16:08:44.900084: val_loss 0.0385
2024-12-18 16:08:44.900660: Pseudo dice [0.5751]
2024-12-18 16:08:44.901700: Epoch time: 499.37 s
2024-12-18 16:08:46.900703: Training done.
2024-12-18 16:08:47.199303: Using splits from existing split file: /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset309_Sohee_Calcium_OCT_CrossValidation/splits_final_20.json
2024-12-18 16:08:47.209483: The split file contains 5 splits.
2024-12-18 16:08:47.210235: Desired fold for training: 0
2024-12-18 16:08:47.210811: This split has 1 training and 7 validation cases.
2024-12-18 16:08:47.212004: predicting 101-019
2024-12-18 16:08:47.226495: 101-019, shape torch.Size([1, 375, 498, 498]), rank 0
2024-12-18 16:11:28.132853: predicting 101-044
2024-12-18 16:11:28.146178: 101-044, shape torch.Size([1, 404, 498, 498]), rank 0
2024-12-18 16:13:56.197986: predicting 101-045
2024-12-18 16:13:56.211890: 101-045, shape torch.Size([1, 375, 498, 498]), rank 0
2024-12-18 16:15:58.668325: predicting 106-002
2024-12-18 16:15:58.682665: 106-002, shape torch.Size([1, 539, 498, 498]), rank 0
2024-12-18 16:19:22.760266: predicting 701-013
2024-12-18 16:19:22.776594: 701-013, shape torch.Size([1, 375, 498, 498]), rank 0
2024-12-18 16:21:47.577792: predicting 704-003
2024-12-18 16:21:47.591780: 704-003, shape torch.Size([1, 375, 498, 498]), rank 0
2024-12-18 16:23:49.400658: predicting 706-005
2024-12-18 16:23:49.424485: 706-005, shape torch.Size([1, 375, 498, 498]), rank 0
2024-12-18 16:26:15.729686: Validation complete
2024-12-18 16:26:15.730581: Mean Validation Dice:  0.578586242790526

############################
INFO: You are using the old nnU-Net default plans. We have updated our recommendations. Please consider using those instead! Read more here: https://github.com/MIC-DKFZ/nnUNet/blob/master/documentation/resenc_presets.md
############################

Using device: cuda:0

#######################################################################
Please cite the following paper when using nnU-Net:
Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.
#######################################################################

2024-12-18 16:26:28.610988: Using torch.compile...

############################
INFO: You are using the old nnU-Net default plans. We have updated our recommendations. Please consider using those instead! Read more here: https://github.com/MIC-DKFZ/nnUNet/blob/master/documentation/resenc_presets.md
############################

Using device: cuda:0

#######################################################################
Please cite the following paper when using nnU-Net:
Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.
#######################################################################

2024-12-18 16:26:28.614170: Using torch.compile...
/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
################### Loading pretrained weights from file  /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_results/Dataset300_Lumen_and_Wall_OCT/nnUNetTrainer__nnUNetPreprocessPlans__3d_fullres/fold_all/checkpoint_best.pth ###################
Below is the list of overlapping blocks in pretrained model and nnUNet architecture:
encoder.stages.0.0.convs.0.conv.weight shape torch.Size([32, 1, 3, 3, 3])
encoder.stages.0.0.convs.0.conv.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.norm.weight shape torch.Size([32])
encoder.stages.0.0.convs.0.norm.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.0.weight shape torch.Size([32, 1, 3, 3, 3])
encoder.stages.0.0.convs.0.all_modules.0.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.1.weight shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.1.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
encoder.stages.0.0.convs.1.conv.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.norm.weight shape torch.Size([32])
encoder.stages.0.0.convs.1.norm.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
encoder.stages.0.0.convs.1.all_modules.0.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.1.weight shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.1.bias shape torch.Size([32])
encoder.stages.1.0.convs.0.conv.weight shape torch.Size([64, 32, 3, 3, 3])
encoder.stages.1.0.convs.0.conv.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.norm.weight shape torch.Size([64])
encoder.stages.1.0.convs.0.norm.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.0.weight shape torch.Size([64, 32, 3, 3, 3])
encoder.stages.1.0.convs.0.all_modules.0.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.1.weight shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.1.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
encoder.stages.1.0.convs.1.conv.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.norm.weight shape torch.Size([64])
encoder.stages.1.0.convs.1.norm.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
encoder.stages.1.0.convs.1.all_modules.0.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.1.weight shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.1.bias shape torch.Size([64])
encoder.stages.2.0.convs.0.conv.weight shape torch.Size([128, 64, 3, 3, 3])
encoder.stages.2.0.convs.0.conv.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.norm.weight shape torch.Size([128])
encoder.stages.2.0.convs.0.norm.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.0.weight shape torch.Size([128, 64, 3, 3, 3])
encoder.stages.2.0.convs.0.all_modules.0.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.1.weight shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.1.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
encoder.stages.2.0.convs.1.conv.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.norm.weight shape torch.Size([128])
encoder.stages.2.0.convs.1.norm.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
encoder.stages.2.0.convs.1.all_modules.0.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.1.weight shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.1.bias shape torch.Size([128])
encoder.stages.3.0.convs.0.conv.weight shape torch.Size([256, 128, 3, 3, 3])
encoder.stages.3.0.convs.0.conv.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.norm.weight shape torch.Size([256])
encoder.stages.3.0.convs.0.norm.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.0.weight shape torch.Size([256, 128, 3, 3, 3])
encoder.stages.3.0.convs.0.all_modules.0.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.1.weight shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.1.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
encoder.stages.3.0.convs.1.conv.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.norm.weight shape torch.Size([256])
encoder.stages.3.0.convs.1.norm.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
encoder.stages.3.0.convs.1.all_modules.0.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.1.weight shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.1.bias shape torch.Size([256])
encoder.stages.4.0.convs.0.conv.weight shape torch.Size([320, 256, 3, 3, 3])
encoder.stages.4.0.convs.0.conv.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.norm.weight shape torch.Size([320])
encoder.stages.4.0.convs.0.norm.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.0.weight shape torch.Size([320, 256, 3, 3, 3])
encoder.stages.4.0.convs.0.all_modules.0.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.1.weight shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.1.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.4.0.convs.1.conv.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.norm.weight shape torch.Size([320])
encoder.stages.4.0.convs.1.norm.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.4.0.convs.1.all_modules.0.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.1.weight shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.1.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.0.conv.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.norm.weight shape torch.Size([320])
encoder.stages.5.0.convs.0.norm.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.0.all_modules.0.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.1.weight shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.1.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.1.conv.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.norm.weight shape torch.Size([320])
encoder.stages.5.0.convs.1.norm.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.1.all_modules.0.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.1.weight shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.0.0.convs.0.conv.weight shape torch.Size([32, 1, 3, 3, 3])
decoder.encoder.stages.0.0.convs.0.conv.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.norm.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.norm.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.0.weight shape torch.Size([32, 1, 3, 3, 3])
decoder.encoder.stages.0.0.convs.0.all_modules.0.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.1.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.1.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.encoder.stages.0.0.convs.1.conv.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.norm.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.norm.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.encoder.stages.0.0.convs.1.all_modules.0.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.1.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.1.bias shape torch.Size([32])
decoder.encoder.stages.1.0.convs.0.conv.weight shape torch.Size([64, 32, 3, 3, 3])
decoder.encoder.stages.1.0.convs.0.conv.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.norm.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.norm.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.0.weight shape torch.Size([64, 32, 3, 3, 3])
decoder.encoder.stages.1.0.convs.0.all_modules.0.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.1.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.1.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.encoder.stages.1.0.convs.1.conv.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.norm.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.norm.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.encoder.stages.1.0.convs.1.all_modules.0.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.1.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.1.bias shape torch.Size([64])
decoder.encoder.stages.2.0.convs.0.conv.weight shape torch.Size([128, 64, 3, 3, 3])
decoder.encoder.stages.2.0.convs.0.conv.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.norm.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.norm.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.0.weight shape torch.Size([128, 64, 3, 3, 3])
decoder.encoder.stages.2.0.convs.0.all_modules.0.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.1.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.1.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.encoder.stages.2.0.convs.1.conv.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.norm.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.norm.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.encoder.stages.2.0.convs.1.all_modules.0.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.1.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.1.bias shape torch.Size([128])
decoder.encoder.stages.3.0.convs.0.conv.weight shape torch.Size([256, 128, 3, 3, 3])
decoder.encoder.stages.3.0.convs.0.conv.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.norm.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.norm.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.0.weight shape torch.Size([256, 128, 3, 3, 3])
decoder.encoder.stages.3.0.convs.0.all_modules.0.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.1.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.1.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.encoder.stages.3.0.convs.1.conv.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.norm.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.norm.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.encoder.stages.3.0.convs.1.all_modules.0.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.1.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.1.bias shape torch.Size([256])
decoder.encoder.stages.4.0.convs.0.conv.weight shape torch.Size([320, 256, 3, 3, 3])
decoder.encoder.stages.4.0.convs.0.conv.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.norm.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.norm.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.0.weight shape torch.Size([320, 256, 3, 3, 3])
decoder.encoder.stages.4.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.4.0.convs.1.conv.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.norm.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.norm.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.4.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.0.conv.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.norm.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.norm.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.1.conv.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.norm.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.norm.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.stages.0.convs.0.conv.weight shape torch.Size([320, 640, 3, 3, 3])
decoder.stages.0.convs.0.conv.bias shape torch.Size([320])
decoder.stages.0.convs.0.norm.weight shape torch.Size([320])
decoder.stages.0.convs.0.norm.bias shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.0.weight shape torch.Size([320, 640, 3, 3, 3])
decoder.stages.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.stages.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.stages.0.convs.1.conv.bias shape torch.Size([320])
decoder.stages.0.convs.1.norm.weight shape torch.Size([320])
decoder.stages.0.convs.1.norm.bias shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.stages.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.stages.1.convs.0.conv.weight shape torch.Size([256, 512, 3, 3, 3])
decoder.stages.1.convs.0.conv.bias shape torch.Size([256])
decoder.stages.1.convs.0.norm.weight shape torch.Size([256])
decoder.stages.1.convs.0.norm.bias shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.0.weight shape torch.Size([256, 512, 3, 3, 3])
decoder.stages.1.convs.0.all_modules.0.bias shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.1.weight shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.1.bias shape torch.Size([256])
decoder.stages.1.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.stages.1.convs.1.conv.bias shape torch.Size([256])
decoder.stages.1.convs.1.norm.weight shape torch.Size([256])
decoder.stages.1.convs.1.norm.bias shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.stages.1.convs.1.all_modules.0.bias shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.1.weight shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.1.bias shape torch.Size([256])
decoder.stages.2.convs.0.conv.weight shape torch.Size([128, 256, 3, 3, 3])
decoder.stages.2.convs.0.conv.bias shape torch.Size([128])
decoder.stages.2.convs.0.norm.weight shape torch.Size([128])
decoder.stages.2.convs.0.norm.bias shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.0.weight shape torch.Size([128, 256, 3, 3, 3])
decoder.stages.2.convs.0.all_modules.0.bias shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.1.weight shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.1.bias shape torch.Size([128])
decoder.stages.2.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.stages.2.convs.1.conv.bias shape torch.Size([128])
decoder.stages.2.convs.1.norm.weight shape torch.Size([128])
decoder.stages.2.convs.1.norm.bias shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.stages.2.convs.1.all_modules.0.bias shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.1.weight shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.1.bias shape torch.Size([128])
decoder.stages.3.convs.0.conv.weight shape torch.Size([64, 128, 3, 3, 3])
decoder.stages.3.convs.0.conv.bias shape torch.Size([64])
decoder.stages.3.convs.0.norm.weight shape torch.Size([64])
decoder.stages.3.convs.0.norm.bias shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.0.weight shape torch.Size([64, 128, 3, 3, 3])
decoder.stages.3.convs.0.all_modules.0.bias shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.1.weight shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.1.bias shape torch.Size([64])
decoder.stages.3.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.stages.3.convs.1.conv.bias shape torch.Size([64])
decoder.stages.3.convs.1.norm.weight shape torch.Size([64])
decoder.stages.3.convs.1.norm.bias shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.stages.3.convs.1.all_modules.0.bias shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.1.weight shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.1.bias shape torch.Size([64])
decoder.stages.4.convs.0.conv.weight shape torch.Size([32, 64, 3, 3, 3])
decoder.stages.4.convs.0.conv.bias shape torch.Size([32])
decoder.stages.4.convs.0.norm.weight shape torch.Size([32])
decoder.stages.4.convs.0.norm.bias shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.0.weight shape torch.Size([32, 64, 3, 3, 3])
decoder.stages.4.convs.0.all_modules.0.bias shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.1.weight shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.1.bias shape torch.Size([32])
decoder.stages.4.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.stages.4.convs.1.conv.bias shape torch.Size([32])
decoder.stages.4.convs.1.norm.weight shape torch.Size([32])
decoder.stages.4.convs.1.norm.bias shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.stages.4.convs.1.all_modules.0.bias shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.1.weight shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.1.bias shape torch.Size([32])
decoder.transpconvs.0.weight shape torch.Size([320, 320, 1, 2, 2])
decoder.transpconvs.0.bias shape torch.Size([320])
decoder.transpconvs.1.weight shape torch.Size([320, 256, 2, 2, 2])
decoder.transpconvs.1.bias shape torch.Size([256])
decoder.transpconvs.2.weight shape torch.Size([256, 128, 2, 2, 2])
decoder.transpconvs.2.bias shape torch.Size([128])
decoder.transpconvs.3.weight shape torch.Size([128, 64, 2, 2, 2])
decoder.transpconvs.3.bias shape torch.Size([64])
decoder.transpconvs.4.weight shape torch.Size([64, 32, 2, 2, 2])
decoder.transpconvs.4.bias shape torch.Size([32])
################### Done ###################
2024-12-18 16:26:46.427622: do_dummy_2d_data_aug: True
2024-12-18 16:26:46.429710: Using splits from existing split file: /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset309_Sohee_Calcium_OCT_CrossValidation/splits_final_20.json
2024-12-18 16:26:46.431838: The split file contains 5 splits.
2024-12-18 16:26:46.432945: Desired fold for training: 2
2024-12-18 16:26:46.433659: This split has 1 training and 7 validation cases.
################### Loading pretrained weights from file  /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_results/Dataset300_Lumen_and_Wall_OCT/nnUNetTrainer__nnUNetPreprocessPlans__3d_fullres/fold_all/checkpoint_best.pth ###################
Below is the list of overlapping blocks in pretrained model and nnUNet architecture:
encoder.stages.0.0.convs.0.conv.weight shape torch.Size([32, 1, 3, 3, 3])
encoder.stages.0.0.convs.0.conv.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.norm.weight shape torch.Size([32])
encoder.stages.0.0.convs.0.norm.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.0.weight shape torch.Size([32, 1, 3, 3, 3])
encoder.stages.0.0.convs.0.all_modules.0.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.1.weight shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.1.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
encoder.stages.0.0.convs.1.conv.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.norm.weight shape torch.Size([32])
encoder.stages.0.0.convs.1.norm.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
encoder.stages.0.0.convs.1.all_modules.0.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.1.weight shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.1.bias shape torch.Size([32])
encoder.stages.1.0.convs.0.conv.weight shape torch.Size([64, 32, 3, 3, 3])
encoder.stages.1.0.convs.0.conv.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.norm.weight shape torch.Size([64])
encoder.stages.1.0.convs.0.norm.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.0.weight shape torch.Size([64, 32, 3, 3, 3])
encoder.stages.1.0.convs.0.all_modules.0.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.1.weight shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.1.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
encoder.stages.1.0.convs.1.conv.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.norm.weight shape torch.Size([64])
encoder.stages.1.0.convs.1.norm.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
encoder.stages.1.0.convs.1.all_modules.0.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.1.weight shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.1.bias shape torch.Size([64])
encoder.stages.2.0.convs.0.conv.weight shape torch.Size([128, 64, 3, 3, 3])
encoder.stages.2.0.convs.0.conv.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.norm.weight shape torch.Size([128])
encoder.stages.2.0.convs.0.norm.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.0.weight shape torch.Size([128, 64, 3, 3, 3])
encoder.stages.2.0.convs.0.all_modules.0.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.1.weight shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.1.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
encoder.stages.2.0.convs.1.conv.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.norm.weight shape torch.Size([128])
encoder.stages.2.0.convs.1.norm.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
encoder.stages.2.0.convs.1.all_modules.0.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.1.weight shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.1.bias shape torch.Size([128])
encoder.stages.3.0.convs.0.conv.weight shape torch.Size([256, 128, 3, 3, 3])
encoder.stages.3.0.convs.0.conv.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.norm.weight shape torch.Size([256])
encoder.stages.3.0.convs.0.norm.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.0.weight shape torch.Size([256, 128, 3, 3, 3])
encoder.stages.3.0.convs.0.all_modules.0.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.1.weight shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.1.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
encoder.stages.3.0.convs.1.conv.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.norm.weight shape torch.Size([256])
encoder.stages.3.0.convs.1.norm.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
encoder.stages.3.0.convs.1.all_modules.0.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.1.weight shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.1.bias shape torch.Size([256])
encoder.stages.4.0.convs.0.conv.weight shape torch.Size([320, 256, 3, 3, 3])
encoder.stages.4.0.convs.0.conv.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.norm.weight shape torch.Size([320])
encoder.stages.4.0.convs.0.norm.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.0.weight shape torch.Size([320, 256, 3, 3, 3])
encoder.stages.4.0.convs.0.all_modules.0.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.1.weight shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.1.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.4.0.convs.1.conv.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.norm.weight shape torch.Size([320])
encoder.stages.4.0.convs.1.norm.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.4.0.convs.1.all_modules.0.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.1.weight shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.1.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.0.conv.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.norm.weight shape torch.Size([320])
encoder.stages.5.0.convs.0.norm.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.0.all_modules.0.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.1.weight shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.1.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.1.conv.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.norm.weight shape torch.Size([320])
encoder.stages.5.0.convs.1.norm.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.1.all_modules.0.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.1.weight shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.0.0.convs.0.conv.weight shape torch.Size([32, 1, 3, 3, 3])
decoder.encoder.stages.0.0.convs.0.conv.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.norm.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.norm.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.0.weight shape torch.Size([32, 1, 3, 3, 3])
decoder.encoder.stages.0.0.convs.0.all_modules.0.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.1.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.1.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.encoder.stages.0.0.convs.1.conv.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.norm.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.norm.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.encoder.stages.0.0.convs.1.all_modules.0.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.1.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.1.bias shape torch.Size([32])
decoder.encoder.stages.1.0.convs.0.conv.weight shape torch.Size([64, 32, 3, 3, 3])
decoder.encoder.stages.1.0.convs.0.conv.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.norm.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.norm.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.0.weight shape torch.Size([64, 32, 3, 3, 3])
decoder.encoder.stages.1.0.convs.0.all_modules.0.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.1.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.1.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.encoder.stages.1.0.convs.1.conv.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.norm.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.norm.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.encoder.stages.1.0.convs.1.all_modules.0.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.1.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.1.bias shape torch.Size([64])
decoder.encoder.stages.2.0.convs.0.conv.weight shape torch.Size([128, 64, 3, 3, 3])
decoder.encoder.stages.2.0.convs.0.conv.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.norm.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.norm.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.0.weight shape torch.Size([128, 64, 3, 3, 3])
decoder.encoder.stages.2.0.convs.0.all_modules.0.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.1.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.1.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.encoder.stages.2.0.convs.1.conv.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.norm.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.norm.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.encoder.stages.2.0.convs.1.all_modules.0.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.1.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.1.bias shape torch.Size([128])
decoder.encoder.stages.3.0.convs.0.conv.weight shape torch.Size([256, 128, 3, 3, 3])
decoder.encoder.stages.3.0.convs.0.conv.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.norm.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.norm.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.0.weight shape torch.Size([256, 128, 3, 3, 3])
decoder.encoder.stages.3.0.convs.0.all_modules.0.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.1.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.1.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.encoder.stages.3.0.convs.1.conv.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.norm.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.norm.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.encoder.stages.3.0.convs.1.all_modules.0.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.1.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.1.bias shape torch.Size([256])
decoder.encoder.stages.4.0.convs.0.conv.weight shape torch.Size([320, 256, 3, 3, 3])
decoder.encoder.stages.4.0.convs.0.conv.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.norm.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.norm.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.0.weight shape torch.Size([320, 256, 3, 3, 3])
decoder.encoder.stages.4.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.4.0.convs.1.conv.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.norm.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.norm.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.4.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.0.conv.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.norm.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.norm.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.1.conv.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.norm.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.norm.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.stages.0.convs.0.conv.weight shape torch.Size([320, 640, 3, 3, 3])
decoder.stages.0.convs.0.conv.bias shape torch.Size([320])
decoder.stages.0.convs.0.norm.weight shape torch.Size([320])
decoder.stages.0.convs.0.norm.bias shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.0.weight shape torch.Size([320, 640, 3, 3, 3])
decoder.stages.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.stages.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.stages.0.convs.1.conv.bias shape torch.Size([320])
decoder.stages.0.convs.1.norm.weight shape torch.Size([320])
decoder.stages.0.convs.1.norm.bias shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.stages.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.stages.1.convs.0.conv.weight shape torch.Size([256, 512, 3, 3, 3])
decoder.stages.1.convs.0.conv.bias shape torch.Size([256])
decoder.stages.1.convs.0.norm.weight shape torch.Size([256])
decoder.stages.1.convs.0.norm.bias shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.0.weight shape torch.Size([256, 512, 3, 3, 3])
decoder.stages.1.convs.0.all_modules.0.bias shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.1.weight shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.1.bias shape torch.Size([256])
decoder.stages.1.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.stages.1.convs.1.conv.bias shape torch.Size([256])
decoder.stages.1.convs.1.norm.weight shape torch.Size([256])
decoder.stages.1.convs.1.norm.bias shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.stages.1.convs.1.all_modules.0.bias shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.1.weight shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.1.bias shape torch.Size([256])
decoder.stages.2.convs.0.conv.weight shape torch.Size([128, 256, 3, 3, 3])
decoder.stages.2.convs.0.conv.bias shape torch.Size([128])
decoder.stages.2.convs.0.norm.weight shape torch.Size([128])
decoder.stages.2.convs.0.norm.bias shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.0.weight shape torch.Size([128, 256, 3, 3, 3])
decoder.stages.2.convs.0.all_modules.0.bias shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.1.weight shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.1.bias shape torch.Size([128])
decoder.stages.2.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.stages.2.convs.1.conv.bias shape torch.Size([128])
decoder.stages.2.convs.1.norm.weight shape torch.Size([128])
decoder.stages.2.convs.1.norm.bias shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.stages.2.convs.1.all_modules.0.bias shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.1.weight shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.1.bias shape torch.Size([128])
decoder.stages.3.convs.0.conv.weight shape torch.Size([64, 128, 3, 3, 3])
decoder.stages.3.convs.0.conv.bias shape torch.Size([64])
decoder.stages.3.convs.0.norm.weight shape torch.Size([64])
decoder.stages.3.convs.0.norm.bias shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.0.weight shape torch.Size([64, 128, 3, 3, 3])
decoder.stages.3.convs.0.all_modules.0.bias shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.1.weight shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.1.bias shape torch.Size([64])
decoder.stages.3.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.stages.3.convs.1.conv.bias shape torch.Size([64])
decoder.stages.3.convs.1.norm.weight shape torch.Size([64])
decoder.stages.3.convs.1.norm.bias shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.stages.3.convs.1.all_modules.0.bias shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.1.weight shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.1.bias shape torch.Size([64])
decoder.stages.4.convs.0.conv.weight shape torch.Size([32, 64, 3, 3, 3])
decoder.stages.4.convs.0.conv.bias shape torch.Size([32])
decoder.stages.4.convs.0.norm.weight shape torch.Size([32])
decoder.stages.4.convs.0.norm.bias shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.0.weight shape torch.Size([32, 64, 3, 3, 3])
decoder.stages.4.convs.0.all_modules.0.bias shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.1.weight shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.1.bias shape torch.Size([32])
decoder.stages.4.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.stages.4.convs.1.conv.bias shape torch.Size([32])
decoder.stages.4.convs.1.norm.weight shape torch.Size([32])
decoder.stages.4.convs.1.norm.bias shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.stages.4.convs.1.all_modules.0.bias shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.1.weight shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.1.bias shape torch.Size([32])
decoder.transpconvs.0.weight shape torch.Size([320, 320, 1, 2, 2])
decoder.transpconvs.0.bias shape torch.Size([320])
decoder.transpconvs.1.weight shape torch.Size([320, 256, 2, 2, 2])
decoder.transpconvs.1.bias shape torch.Size([256])
decoder.transpconvs.2.weight shape torch.Size([256, 128, 2, 2, 2])
decoder.transpconvs.2.bias shape torch.Size([128])
decoder.transpconvs.3.weight shape torch.Size([128, 64, 2, 2, 2])
decoder.transpconvs.3.bias shape torch.Size([64])
decoder.transpconvs.4.weight shape torch.Size([64, 32, 2, 2, 2])
decoder.transpconvs.4.bias shape torch.Size([32])
################### Done ###################
2024-12-18 16:26:46.427579: do_dummy_2d_data_aug: True
2024-12-18 16:26:46.429665: Using splits from existing split file: /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset309_Sohee_Calcium_OCT_CrossValidation/splits_final_20.json
2024-12-18 16:26:46.431623: The split file contains 5 splits.
2024-12-18 16:26:46.432902: Desired fold for training: 3
2024-12-18 16:26:46.433619: This split has 1 training and 7 validation cases.
using pin_memory on device 0
using pin_memory on device 0
using pin_memory on device 0

This is the configuration used by this training:
Configuration name: 3d_32x160x128_b10
 {'data_identifier': 'nnUNetPlans_3d_fullres', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 10, 'patch_size': [32, 160, 128], 'median_image_size_in_voxels': [375.0, 498.0, 498.0], 'spacing': [1.0, 1.0, 1.0], 'normalization_schemes': ['ZScoreNormalization'], 'use_mask_for_norm': [False], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'architecture': {'network_class_name': 'dynamic_network_architectures.architectures.unet.PlainConvUNet', 'arch_kwargs': {'n_stages': 6, 'features_per_stage': [32, 64, 128, 256, 320, 320], 'conv_op': 'torch.nn.modules.conv.Conv3d', 'kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]], 'strides': [[1, 1, 1], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [1, 2, 2]], 'n_conv_per_stage': [2, 2, 2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2, 2, 2], 'conv_bias': True, 'norm_op': 'torch.nn.modules.instancenorm.InstanceNorm3d', 'norm_op_kwargs': {'eps': 1e-05, 'affine': True}, 'dropout_op': None, 'dropout_op_kwargs': None, 'nonlin': 'torch.nn.LeakyReLU', 'nonlin_kwargs': {'inplace': True}, 'deep_supervision': True}, '_kw_requires_import': ['conv_op', 'norm_op', 'dropout_op', 'nonlin']}, 'batch_dice': True, 'inherits_from': '3d_fullres'} 

These are the global plan.json settings:
 {'dataset_name': 'Dataset309_Sohee_Calcium_OCT_CrossValidation', 'plans_name': 'nnUNetPlans', 'original_median_spacing_after_transp': [1.0, 1.0, 1.0], 'original_median_shape_after_transp': [375, 498, 498], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 0.4977976381778717, 'mean': 0.13507525622844696, 'median': 0.09599608182907104, 'min': 0.0, 'percentile_00_5': 0.014754901640117168, 'percentile_99_5': 0.4977976381778717, 'std': 0.12025152146816254}}} 

2024-12-18 16:27:17.282140: unpacking dataset...
using pin_memory on device 0

This is the configuration used by this training:
Configuration name: 3d_32x160x128_b10
 {'data_identifier': 'nnUNetPlans_3d_fullres', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 10, 'patch_size': [32, 160, 128], 'median_image_size_in_voxels': [375.0, 498.0, 498.0], 'spacing': [1.0, 1.0, 1.0], 'normalization_schemes': ['ZScoreNormalization'], 'use_mask_for_norm': [False], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'architecture': {'network_class_name': 'dynamic_network_architectures.architectures.unet.PlainConvUNet', 'arch_kwargs': {'n_stages': 6, 'features_per_stage': [32, 64, 128, 256, 320, 320], 'conv_op': 'torch.nn.modules.conv.Conv3d', 'kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]], 'strides': [[1, 1, 1], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [1, 2, 2]], 'n_conv_per_stage': [2, 2, 2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2, 2, 2], 'conv_bias': True, 'norm_op': 'torch.nn.modules.instancenorm.InstanceNorm3d', 'norm_op_kwargs': {'eps': 1e-05, 'affine': True}, 'dropout_op': None, 'dropout_op_kwargs': None, 'nonlin': 'torch.nn.LeakyReLU', 'nonlin_kwargs': {'inplace': True}, 'deep_supervision': True}, '_kw_requires_import': ['conv_op', 'norm_op', 'dropout_op', 'nonlin']}, 'batch_dice': True, 'inherits_from': '3d_fullres'} 

These are the global plan.json settings:
 {'dataset_name': 'Dataset309_Sohee_Calcium_OCT_CrossValidation', 'plans_name': 'nnUNetPlans', 'original_median_spacing_after_transp': [1.0, 1.0, 1.0], 'original_median_shape_after_transp': [375, 498, 498], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 0.4977976381778717, 'mean': 0.13507525622844696, 'median': 0.09599608182907104, 'min': 0.0, 'percentile_00_5': 0.014754901640117168, 'percentile_99_5': 0.4977976381778717, 'std': 0.12025152146816254}}} 

2024-12-18 16:27:17.379622: unpacking dataset...
2024-12-18 16:27:22.002514: unpacking done...
2024-12-18 16:27:22.210340: Unable to plot network architecture: nnUNet_compile is enabled!
2024-12-18 16:27:22.294489: 
2024-12-18 16:27:22.295938: Epoch 0
2024-12-18 16:27:22.297164: Current learning rate: 0.01
2024-12-18 16:36:07.556555: Validation loss improved from 1000.00000 to -0.31668! Patience: 0/50
2024-12-18 16:36:07.557563: train_loss -0.3802
2024-12-18 16:36:07.558394: val_loss -0.3167
2024-12-18 16:36:07.559090: Pseudo dice [0.6365]
2024-12-18 16:36:07.559864: Epoch time: 525.26 s
2024-12-18 16:36:07.560517: Yayy! New best EMA pseudo Dice: 0.6365
2024-12-18 16:36:10.157207: 
2024-12-18 16:36:10.158322: Epoch 1
2024-12-18 16:36:10.159062: Current learning rate: 0.00994
2024-12-18 16:44:06.451385: Validation loss improved from -0.31668 to -0.35154! Patience: 0/50
2024-12-18 16:44:06.452178: train_loss -0.5566
2024-12-18 16:44:06.452962: val_loss -0.3515
2024-12-18 16:44:06.453653: Pseudo dice [0.6553]
2024-12-18 16:44:06.454355: Epoch time: 476.3 s
2024-12-18 16:44:06.454986: Yayy! New best EMA pseudo Dice: 0.6383
2024-12-18 16:44:08.272552: 
2024-12-18 16:44:08.273898: Epoch 2
2024-12-18 16:44:08.274749: Current learning rate: 0.00988
2024-12-18 16:52:20.488964: Validation loss improved from -0.35154 to -0.37828! Patience: 0/50
2024-12-18 16:52:20.489823: train_loss -0.6035
2024-12-18 16:52:20.490620: val_loss -0.3783
2024-12-18 16:52:20.491442: Pseudo dice [0.6738]
2024-12-18 16:52:20.492308: Epoch time: 492.22 s
2024-12-18 16:52:20.493174: Yayy! New best EMA pseudo Dice: 0.6419
2024-12-18 16:52:22.369306: 
2024-12-18 16:52:22.370553: Epoch 3
2024-12-18 16:52:22.371370: Current learning rate: 0.00982
2024-12-18 16:57:02.468522: Validation loss did not improve from -0.37828. Patience: 1/50
2024-12-18 16:57:02.469319: train_loss -0.6384
2024-12-18 16:57:02.470199: val_loss -0.3287
2024-12-18 16:57:02.470969: Pseudo dice [0.6413]
2024-12-18 16:57:02.471707: Epoch time: 280.1 s
2024-12-18 16:57:03.878012: 
2024-12-18 16:57:03.879318: Epoch 4
2024-12-18 16:57:03.880173: Current learning rate: 0.00976
2024-12-18 17:01:52.896249: Validation loss did not improve from -0.37828. Patience: 2/50
2024-12-18 17:01:52.897152: train_loss -0.663
2024-12-18 17:01:52.898170: val_loss -0.3244
2024-12-18 17:01:52.898985: Pseudo dice [0.6461]
2024-12-18 17:01:52.899874: Epoch time: 289.02 s
2024-12-18 17:01:53.281999: Yayy! New best EMA pseudo Dice: 0.6423
2024-12-18 17:01:55.187763: 
2024-12-18 17:01:55.188985: Epoch 5
2024-12-18 17:01:55.189842: Current learning rate: 0.0097
2024-12-18 17:06:59.113297: Validation loss did not improve from -0.37828. Patience: 3/50
2024-12-18 17:06:59.114236: train_loss -0.6816
2024-12-18 17:06:59.115287: val_loss -0.3129
2024-12-18 17:06:59.116146: Pseudo dice [0.6543]
2024-12-18 17:06:59.117028: Epoch time: 303.93 s
2024-12-18 17:06:59.117866: Yayy! New best EMA pseudo Dice: 0.6435
2024-12-18 17:07:01.012655: 
2024-12-18 17:07:01.013904: Epoch 6
2024-12-18 17:07:01.014754: Current learning rate: 0.00964
2024-12-18 17:12:20.615332: Validation loss did not improve from -0.37828. Patience: 4/50
2024-12-18 17:12:20.616270: train_loss -0.6957
2024-12-18 17:12:20.617098: val_loss -0.3627
2024-12-18 17:12:20.617822: Pseudo dice [0.657]
2024-12-18 17:12:20.618491: Epoch time: 319.6 s
2024-12-18 17:12:20.619188: Yayy! New best EMA pseudo Dice: 0.6448
2024-12-18 17:12:22.591091: 
2024-12-18 17:12:22.592301: Epoch 7
2024-12-18 17:12:22.593018: Current learning rate: 0.00958
2024-12-18 17:17:00.036594: Validation loss did not improve from -0.37828. Patience: 5/50
2024-12-18 17:17:00.037630: train_loss -0.7006
2024-12-18 17:17:00.038499: val_loss -0.3183
2024-12-18 17:17:00.039297: Pseudo dice [0.652]
2024-12-18 17:17:00.040019: Epoch time: 277.45 s
2024-12-18 17:17:00.040719: Yayy! New best EMA pseudo Dice: 0.6455
2024-12-18 17:17:02.336864: 
2024-12-18 17:17:02.338072: Epoch 8
2024-12-18 17:17:02.338832: Current learning rate: 0.00952
2024-12-18 17:21:42.061439: Validation loss did not improve from -0.37828. Patience: 6/50
2024-12-18 17:21:42.062440: train_loss -0.7111
2024-12-18 17:21:42.063232: val_loss -0.355
2024-12-18 17:21:42.064003: Pseudo dice [0.6671]
2024-12-18 17:21:42.064857: Epoch time: 279.73 s
2024-12-18 17:21:42.065610: Yayy! New best EMA pseudo Dice: 0.6477
2024-12-18 17:21:43.986521: 
2024-12-18 17:21:43.987743: Epoch 9
2024-12-18 17:21:43.988395: Current learning rate: 0.00946
2024-12-18 17:26:26.067629: Validation loss did not improve from -0.37828. Patience: 7/50
2024-12-18 17:26:26.068681: train_loss -0.7186
2024-12-18 17:26:26.069565: val_loss -0.3446
2024-12-18 17:26:26.070210: Pseudo dice [0.6561]
2024-12-18 17:26:26.070816: Epoch time: 282.08 s
2024-12-18 17:26:26.525502: Yayy! New best EMA pseudo Dice: 0.6485
2024-12-18 17:26:28.396961: 
2024-12-18 17:26:28.397953: Epoch 10
2024-12-18 17:26:28.398643: Current learning rate: 0.0094
2024-12-18 17:30:59.729164: Validation loss did not improve from -0.37828. Patience: 8/50
2024-12-18 17:30:59.730400: train_loss -0.7288
2024-12-18 17:30:59.731211: val_loss -0.3537
2024-12-18 17:30:59.731990: Pseudo dice [0.6685]
2024-12-18 17:30:59.732682: Epoch time: 271.33 s
2024-12-18 17:30:59.733349: Yayy! New best EMA pseudo Dice: 0.6505
2024-12-18 17:31:01.548494: 
2024-12-18 17:31:01.549495: Epoch 11
2024-12-18 17:31:01.550234: Current learning rate: 0.00934
2024-12-18 17:36:13.496583: Validation loss did not improve from -0.37828. Patience: 9/50
2024-12-18 17:36:13.497805: train_loss -0.7354
2024-12-18 17:36:13.498642: val_loss -0.344
2024-12-18 17:36:13.499400: Pseudo dice [0.6563]
2024-12-18 17:36:13.500214: Epoch time: 311.95 s
2024-12-18 17:36:13.500823: Yayy! New best EMA pseudo Dice: 0.6511
2024-12-18 17:36:15.448187: 
2024-12-18 17:36:15.449674: Epoch 12
2024-12-18 17:36:15.450384: Current learning rate: 0.00928
2024-12-18 17:41:15.534024: Validation loss did not improve from -0.37828. Patience: 10/50
2024-12-18 17:41:15.535343: train_loss -0.7408
2024-12-18 17:41:15.536524: val_loss -0.3321
2024-12-18 17:41:15.537419: Pseudo dice [0.6634]
2024-12-18 17:41:15.538188: Epoch time: 300.09 s
2024-12-18 17:41:15.538883: Yayy! New best EMA pseudo Dice: 0.6523
2024-12-18 17:41:17.470948: 
2024-12-18 17:41:17.472033: Epoch 13
2024-12-18 17:41:17.472735: Current learning rate: 0.00922
2024-12-18 17:45:50.730569: Validation loss did not improve from -0.37828. Patience: 11/50
2024-12-18 17:45:50.731291: train_loss -0.7497
2024-12-18 17:45:50.732011: val_loss -0.3175
2024-12-18 17:45:50.732751: Pseudo dice [0.6631]
2024-12-18 17:45:50.733455: Epoch time: 273.26 s
2024-12-18 17:45:50.734128: Yayy! New best EMA pseudo Dice: 0.6534
2024-12-18 17:45:52.620965: 
2024-12-18 17:45:52.622192: Epoch 14
2024-12-18 17:45:52.622987: Current learning rate: 0.00916
2024-12-18 17:50:32.390460: Validation loss did not improve from -0.37828. Patience: 12/50
2024-12-18 17:50:32.391542: train_loss -0.7548
2024-12-18 17:50:32.392289: val_loss -0.2736
2024-12-18 17:50:32.392990: Pseudo dice [0.6525]
2024-12-18 17:50:32.393699: Epoch time: 279.77 s
2024-12-18 17:50:34.292653: 
2024-12-18 17:50:34.293789: Epoch 15
2024-12-18 17:50:34.294665: Current learning rate: 0.0091
2024-12-18 17:55:14.842402: Validation loss did not improve from -0.37828. Patience: 13/50
2024-12-18 17:55:14.843365: train_loss -0.7598
2024-12-18 17:55:14.844139: val_loss -0.3201
2024-12-18 17:55:14.844805: Pseudo dice [0.6729]
2024-12-18 17:55:14.845490: Epoch time: 280.55 s
2024-12-18 17:55:14.846090: Yayy! New best EMA pseudo Dice: 0.6553
2024-12-18 17:55:16.689067: 
2024-12-18 17:55:16.689954: Epoch 16
2024-12-18 17:55:16.690608: Current learning rate: 0.00903
2024-12-18 18:00:02.173592: Validation loss did not improve from -0.37828. Patience: 14/50
2024-12-18 18:00:02.174496: train_loss -0.7626
2024-12-18 18:00:02.175332: val_loss -0.3379
2024-12-18 18:00:02.175968: Pseudo dice [0.6746]
2024-12-18 18:00:02.176647: Epoch time: 285.49 s
2024-12-18 18:00:02.177289: Yayy! New best EMA pseudo Dice: 0.6572
2024-12-18 18:00:04.217850: 
2024-12-18 18:00:04.218675: Epoch 17
2024-12-18 18:00:04.219373: Current learning rate: 0.00897
2024-12-18 18:05:09.917266: Validation loss did not improve from -0.37828. Patience: 15/50
2024-12-18 18:05:09.918208: train_loss -0.7644
2024-12-18 18:05:09.919167: val_loss -0.3096
2024-12-18 18:05:09.919963: Pseudo dice [0.6568]
2024-12-18 18:05:09.920751: Epoch time: 305.7 s
2024-12-18 18:05:11.911239: 
2024-12-18 18:05:11.912251: Epoch 18
2024-12-18 18:05:11.913096: Current learning rate: 0.00891
2024-12-18 18:11:25.736054: Validation loss did not improve from -0.37828. Patience: 16/50
2024-12-18 18:11:25.736933: train_loss -0.7676
2024-12-18 18:11:25.737749: val_loss -0.3711
2024-12-18 18:11:25.738463: Pseudo dice [0.6843]
2024-12-18 18:11:25.739149: Epoch time: 373.83 s
2024-12-18 18:11:25.739914: Yayy! New best EMA pseudo Dice: 0.6599
2024-12-18 18:11:27.619069: 
2024-12-18 18:11:27.620180: Epoch 19
2024-12-18 18:11:27.621008: Current learning rate: 0.00885
2024-12-18 18:18:14.740932: Validation loss did not improve from -0.37828. Patience: 17/50
2024-12-18 18:18:14.741623: train_loss -0.7744
2024-12-18 18:18:14.742568: val_loss -0.3419
2024-12-18 18:18:14.743330: Pseudo dice [0.6739]
2024-12-18 18:18:14.743998: Epoch time: 407.12 s
2024-12-18 18:18:15.183276: Yayy! New best EMA pseudo Dice: 0.6613
2024-12-18 18:18:16.961019: 
2024-12-18 18:18:16.961927: Epoch 20
2024-12-18 18:18:16.962658: Current learning rate: 0.00879
2024-12-18 18:24:41.414849: Validation loss did not improve from -0.37828. Patience: 18/50
2024-12-18 18:24:41.442376: train_loss -0.776
2024-12-18 18:24:41.443459: val_loss -0.3364
2024-12-18 18:24:41.444531: Pseudo dice [0.6725]
2024-12-18 18:24:41.445770: Epoch time: 384.48 s
2024-12-18 18:24:41.446964: Yayy! New best EMA pseudo Dice: 0.6624
2024-12-18 18:24:43.449413: 
2024-12-18 18:24:43.450534: Epoch 21
2024-12-18 18:24:43.451659: Current learning rate: 0.00873
2024-12-18 18:29:59.972226: Validation loss did not improve from -0.37828. Patience: 19/50
2024-12-18 18:29:59.973116: train_loss -0.7749
2024-12-18 18:29:59.973835: val_loss -0.298
2024-12-18 18:29:59.974493: Pseudo dice [0.6658]
2024-12-18 18:29:59.975096: Epoch time: 316.53 s
2024-12-18 18:29:59.975637: Yayy! New best EMA pseudo Dice: 0.6627
2024-12-18 18:30:01.866178: 
2024-12-18 18:30:01.866976: Epoch 22
2024-12-18 18:30:01.867632: Current learning rate: 0.00867
2024-12-18 18:34:54.103197: Validation loss did not improve from -0.37828. Patience: 20/50
2024-12-18 18:34:54.104228: train_loss -0.7818
2024-12-18 18:34:54.105320: val_loss -0.3495
2024-12-18 18:34:54.106310: Pseudo dice [0.6812]
2024-12-18 18:34:54.107203: Epoch time: 292.24 s
2024-12-18 18:34:54.108184: Yayy! New best EMA pseudo Dice: 0.6646
2024-12-18 18:34:55.964783: 
2024-12-18 18:34:55.966004: Epoch 23
2024-12-18 18:34:55.966882: Current learning rate: 0.00861
2024-12-18 18:40:01.551855: Validation loss did not improve from -0.37828. Patience: 21/50
2024-12-18 18:40:01.554408: train_loss -0.7839
2024-12-18 18:40:01.556111: val_loss -0.3619
2024-12-18 18:40:01.557034: Pseudo dice [0.688]
2024-12-18 18:40:01.558104: Epoch time: 305.59 s
2024-12-18 18:40:01.559035: Yayy! New best EMA pseudo Dice: 0.6669
2024-12-18 18:40:03.528901: 
2024-12-18 18:40:03.530242: Epoch 24
2024-12-18 18:40:03.530977: Current learning rate: 0.00855
2024-12-18 18:45:15.024362: Validation loss did not improve from -0.37828. Patience: 22/50
2024-12-18 18:45:15.025227: train_loss -0.7861
2024-12-18 18:45:15.025947: val_loss -0.3108
2024-12-18 18:45:15.026571: Pseudo dice [0.6637]
2024-12-18 18:45:15.027212: Epoch time: 311.5 s
2024-12-18 18:45:16.927674: 
2024-12-18 18:45:16.928750: Epoch 25
2024-12-18 18:45:16.929503: Current learning rate: 0.00849
2024-12-18 18:49:28.095940: Validation loss did not improve from -0.37828. Patience: 23/50
2024-12-18 18:49:28.096913: train_loss -0.7867
2024-12-18 18:49:28.097978: val_loss -0.327
2024-12-18 18:49:28.098659: Pseudo dice [0.6686]
2024-12-18 18:49:28.099345: Epoch time: 251.17 s
2024-12-18 18:49:29.549198: 
2024-12-18 18:49:29.550467: Epoch 26
2024-12-18 18:49:29.551177: Current learning rate: 0.00843
2024-12-18 18:53:24.310371: Validation loss did not improve from -0.37828. Patience: 24/50
2024-12-18 18:53:24.311390: train_loss -0.7897
2024-12-18 18:53:24.312128: val_loss -0.2986
2024-12-18 18:53:24.312827: Pseudo dice [0.6671]
2024-12-18 18:53:24.313515: Epoch time: 234.76 s
2024-12-18 18:53:25.772236: 
2024-12-18 18:53:25.773458: Epoch 27
2024-12-18 18:53:25.774194: Current learning rate: 0.00836
2024-12-18 18:58:05.568539: Validation loss did not improve from -0.37828. Patience: 25/50
2024-12-18 18:58:05.569369: train_loss -0.7906
2024-12-18 18:58:05.570124: val_loss -0.3226
2024-12-18 18:58:05.570820: Pseudo dice [0.6775]
2024-12-18 18:58:05.571508: Epoch time: 279.8 s
2024-12-18 18:58:05.572150: Yayy! New best EMA pseudo Dice: 0.6679
2024-12-18 18:58:07.449244: 
2024-12-18 18:58:07.450242: Epoch 28
2024-12-18 18:58:07.451073: Current learning rate: 0.0083
2024-12-18 19:03:07.984205: Validation loss did not improve from -0.37828. Patience: 26/50
2024-12-18 19:03:07.985131: train_loss -0.793
2024-12-18 19:03:07.985953: val_loss -0.2587
2024-12-18 19:03:07.986691: Pseudo dice [0.6499]
2024-12-18 19:03:07.987409: Epoch time: 300.54 s
2024-12-18 19:03:09.777036: 
2024-12-18 19:03:09.778291: Epoch 29
2024-12-18 19:03:09.779085: Current learning rate: 0.00824
2024-12-18 19:08:31.971431: Validation loss did not improve from -0.37828. Patience: 27/50
2024-12-18 19:08:31.972492: train_loss -0.7969
2024-12-18 19:08:31.973321: val_loss -0.2883
2024-12-18 19:08:31.974000: Pseudo dice [0.6593]
2024-12-18 19:08:31.974767: Epoch time: 322.2 s
2024-12-18 19:08:33.872250: 
2024-12-18 19:08:33.873443: Epoch 30
2024-12-18 19:08:33.874153: Current learning rate: 0.00818
2024-12-18 19:13:37.195531: Validation loss did not improve from -0.37828. Patience: 28/50
2024-12-18 19:13:37.196478: train_loss -0.7974
2024-12-18 19:13:37.197460: val_loss -0.2932
2024-12-18 19:13:37.198143: Pseudo dice [0.6737]
2024-12-18 19:13:37.198833: Epoch time: 303.33 s
2024-12-18 19:13:38.714335: 
2024-12-18 19:13:38.715511: Epoch 31
2024-12-18 19:13:38.716339: Current learning rate: 0.00812
2024-12-18 19:18:48.297683: Validation loss did not improve from -0.37828. Patience: 29/50
2024-12-18 19:18:48.298687: train_loss -0.8019
2024-12-18 19:18:48.299490: val_loss -0.2816
2024-12-18 19:18:48.300223: Pseudo dice [0.6513]
2024-12-18 19:18:48.300922: Epoch time: 309.59 s
2024-12-18 19:18:49.731620: 
2024-12-18 19:18:49.732816: Epoch 32
2024-12-18 19:18:49.733674: Current learning rate: 0.00806
2024-12-18 19:23:25.219802: Validation loss did not improve from -0.37828. Patience: 30/50
2024-12-18 19:23:25.220722: train_loss -0.8016
2024-12-18 19:23:25.221610: val_loss -0.3274
2024-12-18 19:23:25.222251: Pseudo dice [0.6852]
2024-12-18 19:23:25.223072: Epoch time: 275.49 s
2024-12-18 19:23:26.698644: 
2024-12-18 19:23:26.699843: Epoch 33
2024-12-18 19:23:26.700596: Current learning rate: 0.008
2024-12-18 19:28:31.411185: Validation loss did not improve from -0.37828. Patience: 31/50
2024-12-18 19:28:31.429404: train_loss -0.8025
2024-12-18 19:28:31.431120: val_loss -0.3076
2024-12-18 19:28:31.431953: Pseudo dice [0.6605]
2024-12-18 19:28:31.432884: Epoch time: 304.73 s
2024-12-18 19:28:32.951006: 
2024-12-18 19:28:32.952397: Epoch 34
2024-12-18 19:28:32.953194: Current learning rate: 0.00793
2024-12-18 19:33:24.396524: Validation loss did not improve from -0.37828. Patience: 32/50
2024-12-18 19:33:24.397636: train_loss -0.8046
2024-12-18 19:33:24.398547: val_loss -0.3218
2024-12-18 19:33:24.399386: Pseudo dice [0.6694]
2024-12-18 19:33:24.400265: Epoch time: 291.45 s
2024-12-18 19:33:26.244706: 
2024-12-18 19:33:26.246164: Epoch 35
2024-12-18 19:33:26.247127: Current learning rate: 0.00787
2024-12-18 19:38:21.025586: Validation loss did not improve from -0.37828. Patience: 33/50
2024-12-18 19:38:21.026650: train_loss -0.8083
2024-12-18 19:38:21.027426: val_loss -0.2773
2024-12-18 19:38:21.028120: Pseudo dice [0.6729]
2024-12-18 19:38:21.028858: Epoch time: 294.78 s
2024-12-18 19:38:22.546196: 
2024-12-18 19:38:22.547547: Epoch 36
2024-12-18 19:38:22.548526: Current learning rate: 0.00781
2024-12-18 19:43:33.654280: Validation loss did not improve from -0.37828. Patience: 34/50
2024-12-18 19:43:33.656171: train_loss -0.8055
2024-12-18 19:43:33.657226: val_loss -0.2703
2024-12-18 19:43:33.657912: Pseudo dice [0.6663]
2024-12-18 19:43:33.658622: Epoch time: 311.11 s
2024-12-18 19:43:35.139236: 
2024-12-18 19:43:35.140515: Epoch 37
2024-12-18 19:43:35.141277: Current learning rate: 0.00775
2024-12-18 19:48:34.989210: Validation loss did not improve from -0.37828. Patience: 35/50
2024-12-18 19:48:34.991126: train_loss -0.8073
2024-12-18 19:48:34.991878: val_loss -0.307
2024-12-18 19:48:34.992498: Pseudo dice [0.6654]
2024-12-18 19:48:34.993330: Epoch time: 299.85 s
2024-12-18 19:48:36.557013: 
2024-12-18 19:48:36.558411: Epoch 38
2024-12-18 19:48:36.559365: Current learning rate: 0.00769
2024-12-18 19:53:18.827400: Validation loss did not improve from -0.37828. Patience: 36/50
2024-12-18 19:53:18.828708: train_loss -0.811
2024-12-18 19:53:18.829946: val_loss -0.3145
2024-12-18 19:53:18.830667: Pseudo dice [0.6701]
2024-12-18 19:53:18.831432: Epoch time: 282.27 s
2024-12-18 19:53:20.266050: 
2024-12-18 19:53:20.267265: Epoch 39
2024-12-18 19:53:20.268227: Current learning rate: 0.00763
2024-12-18 19:58:06.767919: Validation loss did not improve from -0.37828. Patience: 37/50
2024-12-18 19:58:06.768571: train_loss -0.812
2024-12-18 19:58:06.769220: val_loss -0.3152
2024-12-18 19:58:06.769819: Pseudo dice [0.6777]
2024-12-18 19:58:06.770456: Epoch time: 286.5 s
2024-12-18 19:58:07.169721: Yayy! New best EMA pseudo Dice: 0.6683
2024-12-18 19:58:09.874954: 
2024-12-18 19:58:09.876072: Epoch 40
2024-12-18 19:58:09.876837: Current learning rate: 0.00756
2024-12-18 20:02:39.084012: Validation loss did not improve from -0.37828. Patience: 38/50
2024-12-18 20:02:39.084854: train_loss -0.8137
2024-12-18 20:02:39.085600: val_loss -0.2879
2024-12-18 20:02:39.087223: Pseudo dice [0.6668]
2024-12-18 20:02:39.088232: Epoch time: 269.21 s
2024-12-18 20:02:40.544366: 
2024-12-18 20:02:40.545856: Epoch 41
2024-12-18 20:02:40.546984: Current learning rate: 0.0075
2024-12-18 20:07:59.219087: Validation loss did not improve from -0.37828. Patience: 39/50
2024-12-18 20:07:59.219967: train_loss -0.8136
2024-12-18 20:07:59.220731: val_loss -0.2836
2024-12-18 20:07:59.221425: Pseudo dice [0.6746]
2024-12-18 20:07:59.222103: Epoch time: 318.68 s
2024-12-18 20:07:59.222813: Yayy! New best EMA pseudo Dice: 0.6688
2024-12-18 20:08:01.089481: 
2024-12-18 20:08:01.090654: Epoch 42
2024-12-18 20:08:01.091388: Current learning rate: 0.00744
2024-12-18 20:13:13.065088: Validation loss did not improve from -0.37828. Patience: 40/50
2024-12-18 20:13:13.065982: train_loss -0.8149
2024-12-18 20:13:13.066862: val_loss -0.2923
2024-12-18 20:13:13.067895: Pseudo dice [0.67]
2024-12-18 20:13:13.068707: Epoch time: 311.98 s
2024-12-18 20:13:13.069483: Yayy! New best EMA pseudo Dice: 0.6689
2024-12-18 20:13:14.829625: 
2024-12-18 20:13:14.831151: Epoch 43
2024-12-18 20:13:14.832247: Current learning rate: 0.00738
2024-12-18 20:18:34.672519: Validation loss did not improve from -0.37828. Patience: 41/50
2024-12-18 20:18:34.673553: train_loss -0.816
2024-12-18 20:18:34.674374: val_loss -0.2531
2024-12-18 20:18:34.675189: Pseudo dice [0.6568]
2024-12-18 20:18:34.675883: Epoch time: 319.85 s
2024-12-18 20:18:36.048110: 
2024-12-18 20:18:36.049224: Epoch 44
2024-12-18 20:18:36.049914: Current learning rate: 0.00732
2024-12-18 20:23:50.883524: Validation loss did not improve from -0.37828. Patience: 42/50
2024-12-18 20:23:50.884555: train_loss -0.8168
2024-12-18 20:23:50.885641: val_loss -0.2883
2024-12-18 20:23:50.886718: Pseudo dice [0.6647]
2024-12-18 20:23:50.887715: Epoch time: 314.84 s
2024-12-18 20:23:52.731105: 
2024-12-18 20:23:52.732352: Epoch 45
2024-12-18 20:23:52.733086: Current learning rate: 0.00725
2024-12-18 20:28:54.320361: Validation loss did not improve from -0.37828. Patience: 43/50
2024-12-18 20:28:54.321298: train_loss -0.82
2024-12-18 20:28:54.322186: val_loss -0.3333
2024-12-18 20:28:54.323100: Pseudo dice [0.6812]
2024-12-18 20:28:54.323987: Epoch time: 301.59 s
2024-12-18 20:28:55.682594: 
2024-12-18 20:28:55.683944: Epoch 46
2024-12-18 20:28:55.684926: Current learning rate: 0.00719
2024-12-18 20:33:56.832587: Validation loss did not improve from -0.37828. Patience: 44/50
2024-12-18 20:33:56.833544: train_loss -0.8202
2024-12-18 20:33:56.834837: val_loss -0.2886
2024-12-18 20:33:56.835940: Pseudo dice [0.6771]
2024-12-18 20:33:56.837001: Epoch time: 301.15 s
2024-12-18 20:33:56.838089: Yayy! New best EMA pseudo Dice: 0.6696
2024-12-18 20:33:58.658322: 
2024-12-18 20:33:58.659590: Epoch 47
2024-12-18 20:33:58.660593: Current learning rate: 0.00713
2024-12-18 20:38:48.661433: Validation loss did not improve from -0.37828. Patience: 45/50
2024-12-18 20:38:48.662563: train_loss -0.8231
2024-12-18 20:38:48.663400: val_loss -0.2281
2024-12-18 20:38:48.664052: Pseudo dice [0.6426]
2024-12-18 20:38:48.664772: Epoch time: 290.01 s
2024-12-18 20:38:50.122123: 
2024-12-18 20:38:50.123229: Epoch 48
2024-12-18 20:38:50.123941: Current learning rate: 0.00707
2024-12-18 20:44:14.061494: Validation loss did not improve from -0.37828. Patience: 46/50
2024-12-18 20:44:14.062429: train_loss -0.8239
2024-12-18 20:44:14.063509: val_loss -0.2945
2024-12-18 20:44:14.064440: Pseudo dice [0.6746]
2024-12-18 20:44:14.065451: Epoch time: 323.94 s
2024-12-18 20:44:15.487537: 
2024-12-18 20:44:15.488842: Epoch 49
2024-12-18 20:44:15.489796: Current learning rate: 0.007
2024-12-18 20:49:44.203508: Validation loss did not improve from -0.37828. Patience: 47/50
2024-12-18 20:49:44.205729: train_loss -0.8249
2024-12-18 20:49:44.206578: val_loss -0.2921
2024-12-18 20:49:44.207255: Pseudo dice [0.6653]
2024-12-18 20:49:44.208093: Epoch time: 328.72 s
2024-12-18 20:49:45.999491: 
2024-12-18 20:49:46.001753: Epoch 50
2024-12-18 20:49:46.003093: Current learning rate: 0.00694
2024-12-18 20:54:57.795141: Validation loss did not improve from -0.37828. Patience: 48/50
2024-12-18 20:54:57.796086: train_loss -0.8221
2024-12-18 20:54:57.797205: val_loss -0.2582
2024-12-18 20:54:57.798199: Pseudo dice [0.6553]
2024-12-18 20:54:57.799154: Epoch time: 311.8 s
2024-12-18 20:54:59.650049: 
2024-12-18 20:54:59.651551: Epoch 51
2024-12-18 20:54:59.652265: Current learning rate: 0.00688
2024-12-18 21:00:01.746281: Validation loss did not improve from -0.37828. Patience: 49/50
2024-12-18 21:00:01.747288: train_loss -0.8268
2024-12-18 21:00:01.748134: val_loss -0.248
2024-12-18 21:00:01.748778: Pseudo dice [0.6574]
2024-12-18 21:00:01.749432: Epoch time: 302.1 s
2024-12-18 21:00:03.200011: 
2024-12-18 21:00:03.201233: Epoch 52
2024-12-18 21:00:03.201940: Current learning rate: 0.00682
2024-12-18 21:04:41.315984: Validation loss did not improve from -0.37828. Patience: 50/50
2024-12-18 21:04:41.316944: train_loss -0.825
2024-12-18 21:04:41.317806: val_loss -0.2596
2024-12-18 21:04:41.318341: Pseudo dice [0.6674]
2024-12-18 21:04:41.318878: Epoch time: 278.12 s
2024-12-18 21:04:42.709326: 
2024-12-18 21:04:42.710556: Epoch 53
2024-12-18 21:04:42.711303: Current learning rate: 0.00675
2024-12-18 21:09:35.969649: Validation loss did not improve from -0.37828. Patience: 51/50
2024-12-18 21:09:35.970434: train_loss -0.8277
2024-12-18 21:09:35.971261: val_loss -0.264
2024-12-18 21:09:35.971928: Pseudo dice [0.667]
2024-12-18 21:09:35.972739: Epoch time: 293.26 s
2024-12-18 21:09:37.427319: 
2024-12-18 21:09:37.428556: Epoch 54
2024-12-18 21:09:37.429564: Current learning rate: 0.00669
2024-12-18 21:14:13.742927: Validation loss did not improve from -0.37828. Patience: 52/50
2024-12-18 21:14:13.743934: train_loss -0.8263
2024-12-18 21:14:13.744711: val_loss -0.2931
2024-12-18 21:14:13.745506: Pseudo dice [0.6741]
2024-12-18 21:14:13.747726: Epoch time: 276.32 s
2024-12-18 21:14:15.538980: 
2024-12-18 21:14:15.540090: Epoch 55
2024-12-18 21:14:15.540811: Current learning rate: 0.00663
2024-12-18 21:19:12.730125: Validation loss did not improve from -0.37828. Patience: 53/50
2024-12-18 21:19:12.731093: train_loss -0.8279
2024-12-18 21:19:12.732333: val_loss -0.3189
2024-12-18 21:19:12.733323: Pseudo dice [0.6806]
2024-12-18 21:19:12.734289: Epoch time: 297.19 s
2024-12-18 21:19:14.264717: 
2024-12-18 21:19:14.266194: Epoch 56
2024-12-18 21:19:14.267449: Current learning rate: 0.00657
2024-12-18 21:24:10.446768: Validation loss did not improve from -0.37828. Patience: 54/50
2024-12-18 21:24:10.447734: train_loss -0.8319
2024-12-18 21:24:10.448812: val_loss -0.2411
2024-12-18 21:24:10.449778: Pseudo dice [0.6683]
2024-12-18 21:24:10.450669: Epoch time: 296.18 s
2024-12-18 21:24:11.820300: 
2024-12-18 21:24:11.821385: Epoch 57
2024-12-18 21:24:11.822140: Current learning rate: 0.0065
2024-12-18 21:29:49.343133: Validation loss did not improve from -0.37828. Patience: 55/50
2024-12-18 21:29:49.344149: train_loss -0.8285
2024-12-18 21:29:49.344818: val_loss -0.2671
2024-12-18 21:29:49.345368: Pseudo dice [0.6581]
2024-12-18 21:29:49.345958: Epoch time: 337.53 s
2024-12-18 21:29:50.704995: 
2024-12-18 21:29:50.706189: Epoch 58
2024-12-18 21:29:50.706840: Current learning rate: 0.00644
2024-12-18 21:35:00.009740: Validation loss did not improve from -0.37828. Patience: 56/50
2024-12-18 21:35:00.010377: train_loss -0.8344
2024-12-18 21:35:00.011112: val_loss -0.2812
2024-12-18 21:35:00.011888: Pseudo dice [0.6747]
2024-12-18 21:35:00.012527: Epoch time: 309.31 s
2024-12-18 21:35:01.520899: 
2024-12-18 21:35:01.521931: Epoch 59
2024-12-18 21:35:01.522655: Current learning rate: 0.00638
2024-12-18 21:40:05.387862: Validation loss did not improve from -0.37828. Patience: 57/50
2024-12-18 21:40:05.389160: train_loss -0.8342
2024-12-18 21:40:05.390476: val_loss -0.2341
2024-12-18 21:40:05.391246: Pseudo dice [0.6589]
2024-12-18 21:40:05.392255: Epoch time: 303.87 s
2024-12-18 21:40:07.177456: 
2024-12-18 21:40:07.178450: Epoch 60
2024-12-18 21:40:07.179140: Current learning rate: 0.00631
2024-12-18 21:45:09.309969: Validation loss did not improve from -0.37828. Patience: 58/50
2024-12-18 21:45:09.310997: train_loss -0.8348
2024-12-18 21:45:09.311848: val_loss -0.2739
2024-12-18 21:45:09.312535: Pseudo dice [0.6723]
2024-12-18 21:45:09.313188: Epoch time: 302.13 s
2024-12-18 21:45:10.700980: 
2024-12-18 21:45:10.702180: Epoch 61
2024-12-18 21:45:10.702914: Current learning rate: 0.00625
2024-12-18 21:49:55.553356: Validation loss did not improve from -0.37828. Patience: 59/50
2024-12-18 21:49:55.554029: train_loss -0.8333
2024-12-18 21:49:55.554728: val_loss -0.2862
2024-12-18 21:49:55.555427: Pseudo dice [0.6786]
2024-12-18 21:49:55.556152: Epoch time: 284.85 s
2024-12-18 21:49:57.707669: 
2024-12-18 21:49:57.708871: Epoch 62
2024-12-18 21:49:57.709686: Current learning rate: 0.00619
2024-12-18 21:55:23.021252: Validation loss did not improve from -0.37828. Patience: 60/50
2024-12-18 21:55:23.023179: train_loss -0.8358
2024-12-18 21:55:23.024197: val_loss -0.2409
2024-12-18 21:55:23.024943: Pseudo dice [0.6572]
2024-12-18 21:55:23.025669: Epoch time: 325.32 s
2024-12-18 21:55:24.437720: 
2024-12-18 21:55:24.438869: Epoch 63
2024-12-18 21:55:24.439625: Current learning rate: 0.00612
2024-12-18 22:00:46.492060: Validation loss did not improve from -0.37828. Patience: 61/50
2024-12-18 22:00:46.493099: train_loss -0.8364
2024-12-18 22:00:46.494077: val_loss -0.2652
2024-12-18 22:00:46.495239: Pseudo dice [0.6712]
2024-12-18 22:00:46.496070: Epoch time: 322.06 s
2024-12-18 22:00:47.899292: 
2024-12-18 22:00:47.900478: Epoch 64
2024-12-18 22:00:47.901328: Current learning rate: 0.00606
2024-12-18 22:05:29.826740: Validation loss did not improve from -0.37828. Patience: 62/50
2024-12-18 22:05:29.827640: train_loss -0.8393
2024-12-18 22:05:29.828401: val_loss -0.2356
2024-12-18 22:05:29.829021: Pseudo dice [0.6583]
2024-12-18 22:05:29.829676: Epoch time: 281.93 s
2024-12-18 22:05:31.666927: 
2024-12-18 22:05:31.668118: Epoch 65
2024-12-18 22:05:31.668789: Current learning rate: 0.006
2024-12-18 22:10:07.975054: Validation loss did not improve from -0.37828. Patience: 63/50
2024-12-18 22:10:07.975883: train_loss -0.8386
2024-12-18 22:10:07.976619: val_loss -0.2868
2024-12-18 22:10:07.977274: Pseudo dice [0.6725]
2024-12-18 22:10:07.977955: Epoch time: 276.31 s
2024-12-18 22:10:09.410007: 
2024-12-18 22:10:09.411116: Epoch 66
2024-12-18 22:10:09.411878: Current learning rate: 0.00593
2024-12-18 22:15:14.305259: Validation loss did not improve from -0.37828. Patience: 64/50
2024-12-18 22:15:14.306220: train_loss -0.8386
2024-12-18 22:15:14.307025: val_loss -0.2517
2024-12-18 22:15:14.307805: Pseudo dice [0.6554]
2024-12-18 22:15:14.308481: Epoch time: 304.9 s
2024-12-18 22:15:15.739555: 
2024-12-18 22:15:15.740552: Epoch 67
2024-12-18 22:15:15.741378: Current learning rate: 0.00587
2024-12-18 22:20:25.738900: Validation loss did not improve from -0.37828. Patience: 65/50
2024-12-18 22:20:25.739729: train_loss -0.8412
2024-12-18 22:20:25.740563: val_loss -0.2563
2024-12-18 22:20:25.741281: Pseudo dice [0.6628]
2024-12-18 22:20:25.741885: Epoch time: 310.0 s
2024-12-18 22:20:27.135931: 
2024-12-18 22:20:27.137212: Epoch 68
2024-12-18 22:20:27.137891: Current learning rate: 0.00581
2024-12-18 22:25:03.034610: Validation loss did not improve from -0.37828. Patience: 66/50
2024-12-18 22:25:03.035434: train_loss -0.8413
2024-12-18 22:25:03.036088: val_loss -0.2382
2024-12-18 22:25:03.036711: Pseudo dice [0.6641]
2024-12-18 22:25:03.037277: Epoch time: 275.9 s
2024-12-18 22:25:04.458873: 
2024-12-18 22:25:04.459780: Epoch 69
2024-12-18 22:25:04.460473: Current learning rate: 0.00574
2024-12-18 22:29:57.888942: Validation loss did not improve from -0.37828. Patience: 67/50
2024-12-18 22:29:57.889700: train_loss -0.8417
2024-12-18 22:29:57.890543: val_loss -0.235
2024-12-18 22:29:57.891231: Pseudo dice [0.6502]
2024-12-18 22:29:57.892045: Epoch time: 293.43 s
2024-12-18 22:29:59.766453: 
2024-12-18 22:29:59.767581: Epoch 70
2024-12-18 22:29:59.768330: Current learning rate: 0.00568
2024-12-18 22:35:05.653916: Validation loss did not improve from -0.37828. Patience: 68/50
2024-12-18 22:35:05.654680: train_loss -0.8405
2024-12-18 22:35:05.655512: val_loss -0.2499
2024-12-18 22:35:05.656113: Pseudo dice [0.6638]
2024-12-18 22:35:05.656825: Epoch time: 305.89 s
2024-12-18 22:35:07.101377: 
2024-12-18 22:35:07.102180: Epoch 71
2024-12-18 22:35:07.102782: Current learning rate: 0.00562
2024-12-18 22:40:32.497354: Validation loss did not improve from -0.37828. Patience: 69/50
2024-12-18 22:40:32.500296: train_loss -0.8427
2024-12-18 22:40:32.501109: val_loss -0.2025
2024-12-18 22:40:32.501808: Pseudo dice [0.648]
2024-12-18 22:40:32.502599: Epoch time: 325.4 s
2024-12-18 22:40:33.949043: 
2024-12-18 22:40:33.950530: Epoch 72
2024-12-18 22:40:33.951176: Current learning rate: 0.00555
2024-12-18 22:45:39.768942: Validation loss did not improve from -0.37828. Patience: 70/50
2024-12-18 22:45:39.769698: train_loss -0.844
2024-12-18 22:45:39.771045: val_loss -0.2834
2024-12-18 22:45:39.771674: Pseudo dice [0.682]
2024-12-18 22:45:39.772552: Epoch time: 305.82 s
2024-12-18 22:45:41.662308: 
2024-12-18 22:45:41.663608: Epoch 73
2024-12-18 22:45:41.664342: Current learning rate: 0.00549
2024-12-18 22:50:25.717534: Validation loss did not improve from -0.37828. Patience: 71/50
2024-12-18 22:50:25.718463: train_loss -0.8469
2024-12-18 22:50:25.719300: val_loss -0.219
2024-12-18 22:50:25.720076: Pseudo dice [0.6689]
2024-12-18 22:50:25.720813: Epoch time: 284.06 s
2024-12-18 22:50:27.164296: 
2024-12-18 22:50:27.165550: Epoch 74
2024-12-18 22:50:27.166421: Current learning rate: 0.00542
2024-12-18 22:55:51.704264: Validation loss did not improve from -0.37828. Patience: 72/50
2024-12-18 22:55:51.705167: train_loss -0.8444
2024-12-18 22:55:51.706125: val_loss -0.2713
2024-12-18 22:55:51.706877: Pseudo dice [0.6731]
2024-12-18 22:55:51.707624: Epoch time: 324.54 s
2024-12-18 22:55:53.552913: 
2024-12-18 22:55:53.554152: Epoch 75
2024-12-18 22:55:53.554840: Current learning rate: 0.00536
2024-12-18 23:01:21.569304: Validation loss did not improve from -0.37828. Patience: 73/50
2024-12-18 23:01:21.570965: train_loss -0.8455
2024-12-18 23:01:21.571670: val_loss -0.2418
2024-12-18 23:01:21.572212: Pseudo dice [0.6619]
2024-12-18 23:01:21.572814: Epoch time: 328.02 s
2024-12-18 23:01:22.987032: 
2024-12-18 23:01:22.988134: Epoch 76
2024-12-18 23:01:22.989052: Current learning rate: 0.00529
2024-12-18 23:06:32.874990: Validation loss did not improve from -0.37828. Patience: 74/50
2024-12-18 23:06:32.876291: train_loss -0.8463
2024-12-18 23:06:32.877103: val_loss -0.2672
2024-12-18 23:06:32.877744: Pseudo dice [0.6784]
2024-12-18 23:06:32.878392: Epoch time: 309.89 s
2024-12-18 23:06:34.334858: 
2024-12-18 23:06:34.335986: Epoch 77
2024-12-18 23:06:34.336763: Current learning rate: 0.00523
2024-12-18 23:11:53.288763: Validation loss did not improve from -0.37828. Patience: 75/50
2024-12-18 23:11:53.289668: train_loss -0.8465
2024-12-18 23:11:53.290656: val_loss -0.1936
2024-12-18 23:11:53.291459: Pseudo dice [0.6617]
2024-12-18 23:11:53.292226: Epoch time: 318.96 s
2024-12-18 23:11:54.738000: 
2024-12-18 23:11:54.739519: Epoch 78
2024-12-18 23:11:54.740567: Current learning rate: 0.00517
2024-12-18 23:17:01.189429: Validation loss did not improve from -0.37828. Patience: 76/50
2024-12-18 23:17:01.190182: train_loss -0.8493
2024-12-18 23:17:01.190852: val_loss -0.2218
2024-12-18 23:17:01.191450: Pseudo dice [0.6578]
2024-12-18 23:17:01.192107: Epoch time: 306.45 s
2024-12-18 23:17:02.644432: 
2024-12-18 23:17:02.645279: Epoch 79
2024-12-18 23:17:02.645964: Current learning rate: 0.0051
2024-12-18 23:22:18.710694: Validation loss did not improve from -0.37828. Patience: 77/50
2024-12-18 23:22:18.711619: train_loss -0.8488
2024-12-18 23:22:18.712265: val_loss -0.2331
2024-12-18 23:22:18.712908: Pseudo dice [0.6572]
2024-12-18 23:22:18.713492: Epoch time: 316.07 s
2024-12-18 23:22:20.572260: 
2024-12-18 23:22:20.573441: Epoch 80
2024-12-18 23:22:20.574103: Current learning rate: 0.00504
2024-12-18 23:27:12.767527: Validation loss did not improve from -0.37828. Patience: 78/50
2024-12-18 23:27:12.768506: train_loss -0.8488
2024-12-18 23:27:12.769364: val_loss -0.2245
2024-12-18 23:27:12.770046: Pseudo dice [0.6682]
2024-12-18 23:27:12.770733: Epoch time: 292.2 s
2024-12-18 23:27:14.231326: 
2024-12-18 23:27:14.232596: Epoch 81
2024-12-18 23:27:14.233252: Current learning rate: 0.00497
2024-12-18 23:32:12.461770: Validation loss did not improve from -0.37828. Patience: 79/50
2024-12-18 23:32:12.462557: train_loss -0.8512
2024-12-18 23:32:12.463233: val_loss -0.2396
2024-12-18 23:32:12.463850: Pseudo dice [0.6707]
2024-12-18 23:32:12.464469: Epoch time: 298.23 s
2024-12-18 23:32:13.942528: 
2024-12-18 23:32:13.943482: Epoch 82
2024-12-18 23:32:13.944245: Current learning rate: 0.00491
2024-12-18 23:37:12.778885: Validation loss did not improve from -0.37828. Patience: 80/50
2024-12-18 23:37:12.779688: train_loss -0.8523
2024-12-18 23:37:12.780442: val_loss -0.2291
2024-12-18 23:37:12.781184: Pseudo dice [0.6586]
2024-12-18 23:37:12.781925: Epoch time: 298.84 s
2024-12-18 23:37:14.140269: 
2024-12-18 23:37:14.141476: Epoch 83
2024-12-18 23:37:14.142158: Current learning rate: 0.00484
2024-12-18 23:42:07.348610: Validation loss did not improve from -0.37828. Patience: 81/50
2024-12-18 23:42:07.349426: train_loss -0.8542
2024-12-18 23:42:07.350049: val_loss -0.2424
2024-12-18 23:42:07.350739: Pseudo dice [0.6729]
2024-12-18 23:42:07.351403: Epoch time: 293.21 s
2024-12-18 23:42:09.173542: 
2024-12-18 23:42:09.174581: Epoch 84
2024-12-18 23:42:09.175300: Current learning rate: 0.00478
2024-12-18 23:47:14.444691: Validation loss did not improve from -0.37828. Patience: 82/50
2024-12-18 23:47:14.454684: train_loss -0.8556
2024-12-18 23:47:14.456631: val_loss -0.2228
2024-12-18 23:47:14.457240: Pseudo dice [0.6697]
2024-12-18 23:47:14.458388: Epoch time: 305.28 s
2024-12-18 23:47:16.198539: 
2024-12-18 23:47:16.199769: Epoch 85
2024-12-18 23:47:16.200474: Current learning rate: 0.00471
2024-12-18 23:52:14.156225: Validation loss did not improve from -0.37828. Patience: 83/50
2024-12-18 23:52:14.157061: train_loss -0.8555
2024-12-18 23:52:14.157836: val_loss -0.2312
2024-12-18 23:52:14.158514: Pseudo dice [0.6662]
2024-12-18 23:52:14.159177: Epoch time: 297.96 s
2024-12-18 23:52:15.524886: 
2024-12-18 23:52:15.525914: Epoch 86
2024-12-18 23:52:15.526616: Current learning rate: 0.00465
2024-12-18 23:57:09.193388: Validation loss did not improve from -0.37828. Patience: 84/50
2024-12-18 23:57:09.194300: train_loss -0.8532
2024-12-18 23:57:09.195029: val_loss -0.1997
2024-12-18 23:57:09.195863: Pseudo dice [0.6513]
2024-12-18 23:57:09.196484: Epoch time: 293.67 s
2024-12-18 23:57:10.567273: 
2024-12-18 23:57:10.568376: Epoch 87
2024-12-18 23:57:10.569046: Current learning rate: 0.00458
2024-12-19 00:02:22.608194: Validation loss did not improve from -0.37828. Patience: 85/50
2024-12-19 00:02:22.608900: train_loss -0.8547
2024-12-19 00:02:22.609640: val_loss -0.1983
2024-12-19 00:02:22.610316: Pseudo dice [0.6594]
2024-12-19 00:02:22.610980: Epoch time: 312.04 s
2024-12-19 00:02:23.996883: 
2024-12-19 00:02:23.998067: Epoch 88
2024-12-19 00:02:23.998803: Current learning rate: 0.00452
2024-12-19 00:07:25.328909: Validation loss did not improve from -0.37828. Patience: 86/50
2024-12-19 00:07:25.330652: train_loss -0.8569
2024-12-19 00:07:25.331527: val_loss -0.2413
2024-12-19 00:07:25.332227: Pseudo dice [0.6686]
2024-12-19 00:07:25.332839: Epoch time: 301.33 s
2024-12-19 00:07:26.686975: 
2024-12-19 00:07:26.687917: Epoch 89
2024-12-19 00:07:26.688510: Current learning rate: 0.00445
2024-12-19 00:12:58.069508: Validation loss did not improve from -0.37828. Patience: 87/50
2024-12-19 00:12:58.070816: train_loss -0.8576
2024-12-19 00:12:58.071819: val_loss -0.2511
2024-12-19 00:12:58.072440: Pseudo dice [0.6688]
2024-12-19 00:12:58.073083: Epoch time: 331.38 s
2024-12-19 00:12:59.888332: 
2024-12-19 00:12:59.889468: Epoch 90
2024-12-19 00:12:59.890414: Current learning rate: 0.00438
2024-12-19 00:18:11.037202: Validation loss did not improve from -0.37828. Patience: 88/50
2024-12-19 00:18:11.038151: train_loss -0.8575
2024-12-19 00:18:11.039046: val_loss -0.1946
2024-12-19 00:18:11.039825: Pseudo dice [0.6533]
2024-12-19 00:18:11.040628: Epoch time: 311.15 s
2024-12-19 00:18:12.384092: 
2024-12-19 00:18:12.385661: Epoch 91
2024-12-19 00:18:12.386858: Current learning rate: 0.00432
2024-12-19 00:23:31.510532: Validation loss did not improve from -0.37828. Patience: 89/50
2024-12-19 00:23:31.511306: train_loss -0.8583
2024-12-19 00:23:31.512043: val_loss -0.2242
2024-12-19 00:23:31.512682: Pseudo dice [0.6619]
2024-12-19 00:23:31.513340: Epoch time: 319.13 s
2024-12-19 00:23:32.896829: 
2024-12-19 00:23:32.898078: Epoch 92
2024-12-19 00:23:32.898722: Current learning rate: 0.00425
2024-12-19 00:28:40.155033: Validation loss did not improve from -0.37828. Patience: 90/50
2024-12-19 00:28:40.155850: train_loss -0.8578
2024-12-19 00:28:40.156621: val_loss -0.2135
2024-12-19 00:28:40.157318: Pseudo dice [0.6588]
2024-12-19 00:28:40.158029: Epoch time: 307.26 s
2024-12-19 00:28:41.533638: 
2024-12-19 00:28:41.534802: Epoch 93
2024-12-19 00:28:41.535541: Current learning rate: 0.00419
2024-12-19 00:34:21.636805: Validation loss did not improve from -0.37828. Patience: 91/50
2024-12-19 00:34:21.637736: train_loss -0.8612
2024-12-19 00:34:21.638550: val_loss -0.2191
2024-12-19 00:34:21.639254: Pseudo dice [0.6608]
2024-12-19 00:34:21.639987: Epoch time: 340.11 s
2024-12-19 00:34:23.027966: 
2024-12-19 00:34:23.029083: Epoch 94
2024-12-19 00:34:23.030009: Current learning rate: 0.00412
2024-12-19 00:39:47.569271: Validation loss did not improve from -0.37828. Patience: 92/50
2024-12-19 00:39:47.570111: train_loss -0.8608
2024-12-19 00:39:47.570891: val_loss -0.2196
2024-12-19 00:39:47.571620: Pseudo dice [0.66]
2024-12-19 00:39:47.572313: Epoch time: 324.54 s
2024-12-19 00:39:50.017227: 
2024-12-19 00:39:50.018121: Epoch 95
2024-12-19 00:39:50.018800: Current learning rate: 0.00405
2024-12-19 00:44:58.807783: Validation loss did not improve from -0.37828. Patience: 93/50
2024-12-19 00:44:58.808504: train_loss -0.8631
2024-12-19 00:44:58.809216: val_loss -0.194
2024-12-19 00:44:58.809968: Pseudo dice [0.6691]
2024-12-19 00:44:58.810591: Epoch time: 308.79 s
2024-12-19 00:45:00.143927: 
2024-12-19 00:45:00.145088: Epoch 96
2024-12-19 00:45:00.145795: Current learning rate: 0.00399
2024-12-19 00:50:32.558285: Validation loss did not improve from -0.37828. Patience: 94/50
2024-12-19 00:50:32.573534: train_loss -0.8629
2024-12-19 00:50:32.574667: val_loss -0.2209
2024-12-19 00:50:32.575451: Pseudo dice [0.6724]
2024-12-19 00:50:32.576296: Epoch time: 332.43 s
2024-12-19 00:50:34.080310: 
2024-12-19 00:50:34.081501: Epoch 97
2024-12-19 00:50:34.082148: Current learning rate: 0.00392
2024-12-19 00:55:35.971399: Validation loss did not improve from -0.37828. Patience: 95/50
2024-12-19 00:55:35.972249: train_loss -0.8625
2024-12-19 00:55:35.973091: val_loss -0.1966
2024-12-19 00:55:35.973852: Pseudo dice [0.6508]
2024-12-19 00:55:35.974627: Epoch time: 301.89 s
2024-12-19 00:55:37.348146: 
2024-12-19 00:55:37.349625: Epoch 98
2024-12-19 00:55:37.350806: Current learning rate: 0.00385
2024-12-19 01:00:45.175667: Validation loss did not improve from -0.37828. Patience: 96/50
2024-12-19 01:00:45.176715: train_loss -0.8635
2024-12-19 01:00:45.177433: val_loss -0.2566
2024-12-19 01:00:45.178089: Pseudo dice [0.6734]
2024-12-19 01:00:45.179026: Epoch time: 307.83 s
2024-12-19 01:00:46.758651: 
2024-12-19 01:00:46.761603: Epoch 99
2024-12-19 01:00:46.762954: Current learning rate: 0.00379
2024-12-19 01:05:15.699667: Validation loss did not improve from -0.37828. Patience: 97/50
2024-12-19 01:05:15.700521: train_loss -0.8626
2024-12-19 01:05:15.701652: val_loss -0.2327
2024-12-19 01:05:15.702678: Pseudo dice [0.6738]
2024-12-19 01:05:15.703528: Epoch time: 268.94 s
2024-12-19 01:05:17.688087: 
2024-12-19 01:05:17.689333: Epoch 100
2024-12-19 01:05:17.690166: Current learning rate: 0.00372
2024-12-19 01:10:18.786131: Validation loss did not improve from -0.37828. Patience: 98/50
2024-12-19 01:10:18.788029: train_loss -0.8618
2024-12-19 01:10:18.788740: val_loss -0.219
2024-12-19 01:10:18.789505: Pseudo dice [0.6453]
2024-12-19 01:10:18.790283: Epoch time: 301.1 s
2024-12-19 01:10:20.228175: 
2024-12-19 01:10:20.229572: Epoch 101
2024-12-19 01:10:20.230556: Current learning rate: 0.00365
2024-12-19 01:15:55.895566: Validation loss did not improve from -0.37828. Patience: 99/50
2024-12-19 01:15:55.896583: train_loss -0.8646
2024-12-19 01:15:55.897434: val_loss -0.2327
2024-12-19 01:15:55.898223: Pseudo dice [0.6722]
2024-12-19 01:15:55.898915: Epoch time: 335.67 s
2024-12-19 01:15:57.364458: 
2024-12-19 01:15:57.365751: Epoch 102
2024-12-19 01:15:57.366521: Current learning rate: 0.00359
2024-12-19 01:21:04.355951: Validation loss did not improve from -0.37828. Patience: 100/50
2024-12-19 01:21:04.357344: train_loss -0.864
2024-12-19 01:21:04.358631: val_loss -0.2335
2024-12-19 01:21:04.359420: Pseudo dice [0.6724]
2024-12-19 01:21:04.360175: Epoch time: 306.99 s
2024-12-19 01:21:05.893189: 
2024-12-19 01:21:05.894615: Epoch 103
2024-12-19 01:21:05.896065: Current learning rate: 0.00352
2024-12-19 01:26:17.744332: Validation loss did not improve from -0.37828. Patience: 101/50
2024-12-19 01:26:17.745150: train_loss -0.8654
2024-12-19 01:26:17.746096: val_loss -0.2495
2024-12-19 01:26:17.746905: Pseudo dice [0.6656]
2024-12-19 01:26:17.747655: Epoch time: 311.85 s
2024-12-19 01:26:19.223994: 
2024-12-19 01:26:19.225356: Epoch 104
2024-12-19 01:26:19.226202: Current learning rate: 0.00345
2024-12-19 01:30:59.483995: Validation loss did not improve from -0.37828. Patience: 102/50
2024-12-19 01:30:59.484928: train_loss -0.866
2024-12-19 01:30:59.486047: val_loss -0.218
2024-12-19 01:30:59.487067: Pseudo dice [0.6724]
2024-12-19 01:30:59.488043: Epoch time: 280.26 s
2024-12-19 01:31:01.479614: 
2024-12-19 01:31:01.480710: Epoch 105
2024-12-19 01:31:01.481805: Current learning rate: 0.00338
2024-12-19 01:36:24.598831: Validation loss did not improve from -0.37828. Patience: 103/50
2024-12-19 01:36:24.599781: train_loss -0.8673
2024-12-19 01:36:24.600672: val_loss -0.2029
2024-12-19 01:36:24.601532: Pseudo dice [0.6564]
2024-12-19 01:36:24.602440: Epoch time: 323.12 s
2024-12-19 01:36:26.422955: 
2024-12-19 01:36:26.424143: Epoch 106
2024-12-19 01:36:26.424983: Current learning rate: 0.00332
2024-12-19 01:42:09.449032: Validation loss did not improve from -0.37828. Patience: 104/50
2024-12-19 01:42:09.449877: train_loss -0.8679
2024-12-19 01:42:09.450958: val_loss -0.2156
2024-12-19 01:42:09.451956: Pseudo dice [0.6674]
2024-12-19 01:42:09.453014: Epoch time: 343.03 s
2024-12-19 01:42:10.879003: 
2024-12-19 01:42:10.880338: Epoch 107
2024-12-19 01:42:10.881473: Current learning rate: 0.00325
2024-12-19 01:46:54.965275: Validation loss did not improve from -0.37828. Patience: 105/50
2024-12-19 01:46:54.966152: train_loss -0.8696
2024-12-19 01:46:54.967167: val_loss -0.2457
2024-12-19 01:46:54.968167: Pseudo dice [0.678]
2024-12-19 01:46:54.969109: Epoch time: 284.09 s
2024-12-19 01:46:56.432980: 
2024-12-19 01:46:56.434150: Epoch 108
2024-12-19 01:46:56.434891: Current learning rate: 0.00318
2024-12-19 01:52:15.996407: Validation loss did not improve from -0.37828. Patience: 106/50
2024-12-19 01:52:15.997766: train_loss -0.8688
2024-12-19 01:52:15.999249: val_loss -0.2189
2024-12-19 01:52:15.999991: Pseudo dice [0.6686]
2024-12-19 01:52:16.000815: Epoch time: 319.57 s
2024-12-19 01:52:17.521051: 
2024-12-19 01:52:17.522347: Epoch 109
2024-12-19 01:52:17.523091: Current learning rate: 0.00311
2024-12-19 01:57:24.176966: Validation loss did not improve from -0.37828. Patience: 107/50
2024-12-19 01:57:24.177901: train_loss -0.8685
2024-12-19 01:57:24.178630: val_loss -0.2348
2024-12-19 01:57:24.179412: Pseudo dice [0.6764]
2024-12-19 01:57:24.180094: Epoch time: 306.66 s
2024-12-19 01:57:26.243548: 
2024-12-19 01:57:26.244610: Epoch 110
2024-12-19 01:57:26.245498: Current learning rate: 0.00304
2024-12-19 02:02:44.512442: Validation loss did not improve from -0.37828. Patience: 108/50
2024-12-19 02:02:44.513316: train_loss -0.8704
2024-12-19 02:02:44.514232: val_loss -0.1999
2024-12-19 02:02:44.515111: Pseudo dice [0.6619]
2024-12-19 02:02:44.515858: Epoch time: 318.27 s
2024-12-19 02:02:46.023094: 
2024-12-19 02:02:46.024426: Epoch 111
2024-12-19 02:02:46.025369: Current learning rate: 0.00297
2024-12-19 02:08:15.289905: Validation loss did not improve from -0.37828. Patience: 109/50
2024-12-19 02:08:15.290595: train_loss -0.8704
2024-12-19 02:08:15.291289: val_loss -0.1994
2024-12-19 02:08:15.292077: Pseudo dice [0.6535]
2024-12-19 02:08:15.292799: Epoch time: 329.27 s
2024-12-19 02:08:16.782407: 
2024-12-19 02:08:16.783726: Epoch 112
2024-12-19 02:08:16.784702: Current learning rate: 0.00291
2024-12-19 02:12:54.213041: Validation loss did not improve from -0.37828. Patience: 110/50
2024-12-19 02:12:54.214036: train_loss -0.8714
2024-12-19 02:12:54.214848: val_loss -0.2245
2024-12-19 02:12:54.215663: Pseudo dice [0.6926]
2024-12-19 02:12:54.216431: Epoch time: 277.43 s
2024-12-19 02:12:55.791501: 
2024-12-19 02:12:55.792701: Epoch 113
2024-12-19 02:12:55.793452: Current learning rate: 0.00284
2024-12-19 02:18:00.399086: Validation loss did not improve from -0.37828. Patience: 111/50
2024-12-19 02:18:00.400820: train_loss -0.8715
2024-12-19 02:18:00.401690: val_loss -0.2069
2024-12-19 02:18:00.402496: Pseudo dice [0.6633]
2024-12-19 02:18:00.403202: Epoch time: 304.61 s
2024-12-19 02:18:01.899828: 
2024-12-19 02:18:01.900836: Epoch 114
2024-12-19 02:18:01.901598: Current learning rate: 0.00277
2024-12-19 02:23:08.385927: Validation loss did not improve from -0.37828. Patience: 112/50
2024-12-19 02:23:08.386973: train_loss -0.8719
2024-12-19 02:23:08.388129: val_loss -0.1984
2024-12-19 02:23:08.388995: Pseudo dice [0.6669]
2024-12-19 02:23:08.389986: Epoch time: 306.49 s
2024-12-19 02:23:10.432876: 
2024-12-19 02:23:10.433994: Epoch 115
2024-12-19 02:23:10.434901: Current learning rate: 0.0027
2024-12-19 02:28:23.090166: Validation loss did not improve from -0.37828. Patience: 113/50
2024-12-19 02:28:23.091149: train_loss -0.8716
2024-12-19 02:28:23.091984: val_loss -0.199
2024-12-19 02:28:23.092782: Pseudo dice [0.6597]
2024-12-19 02:28:23.093637: Epoch time: 312.66 s
2024-12-19 02:28:24.655536: 
2024-12-19 02:28:24.656979: Epoch 116
2024-12-19 02:28:24.657977: Current learning rate: 0.00263
2024-12-19 02:34:00.116498: Validation loss did not improve from -0.37828. Patience: 114/50
2024-12-19 02:34:00.117473: train_loss -0.873
2024-12-19 02:34:00.118321: val_loss -0.2344
2024-12-19 02:34:00.119079: Pseudo dice [0.6711]
2024-12-19 02:34:00.119756: Epoch time: 335.46 s
2024-12-19 02:34:02.642785: 
2024-12-19 02:34:02.644092: Epoch 117
2024-12-19 02:34:02.644867: Current learning rate: 0.00256
2024-12-19 02:38:35.073250: Validation loss did not improve from -0.37828. Patience: 115/50
2024-12-19 02:38:35.074229: train_loss -0.8743
2024-12-19 02:38:35.075061: val_loss -0.2112
2024-12-19 02:38:35.075822: Pseudo dice [0.6674]
2024-12-19 02:38:35.076553: Epoch time: 272.43 s
2024-12-19 02:38:36.560290: 
2024-12-19 02:38:36.561269: Epoch 118
2024-12-19 02:38:36.561929: Current learning rate: 0.00249
2024-12-19 02:43:46.084957: Validation loss did not improve from -0.37828. Patience: 116/50
2024-12-19 02:43:46.085885: train_loss -0.8733
2024-12-19 02:43:46.086866: val_loss -0.2047
2024-12-19 02:43:46.087832: Pseudo dice [0.6683]
2024-12-19 02:43:46.088713: Epoch time: 309.53 s
2024-12-19 02:43:47.605702: 
2024-12-19 02:43:47.606933: Epoch 119
2024-12-19 02:43:47.607736: Current learning rate: 0.00242
2024-12-19 02:48:46.277610: Validation loss did not improve from -0.37828. Patience: 117/50
2024-12-19 02:48:46.278683: train_loss -0.8749
2024-12-19 02:48:46.279385: val_loss -0.1884
2024-12-19 02:48:46.280020: Pseudo dice [0.6557]
2024-12-19 02:48:46.280692: Epoch time: 298.67 s
2024-12-19 02:48:48.199436: 
2024-12-19 02:48:48.200649: Epoch 120
2024-12-19 02:48:48.201335: Current learning rate: 0.00235
2024-12-19 02:53:32.063478: Validation loss did not improve from -0.37828. Patience: 118/50
2024-12-19 02:53:32.064471: train_loss -0.8754
2024-12-19 02:53:32.065235: val_loss -0.2136
2024-12-19 02:53:32.065905: Pseudo dice [0.6529]
2024-12-19 02:53:32.066627: Epoch time: 283.87 s
2024-12-19 02:53:33.683857: 
2024-12-19 02:53:33.684796: Epoch 121
2024-12-19 02:53:33.685608: Current learning rate: 0.00228
2024-12-19 02:58:16.120991: Validation loss did not improve from -0.37828. Patience: 119/50
2024-12-19 02:58:16.122515: train_loss -0.8761
2024-12-19 02:58:16.123807: val_loss -0.1507
2024-12-19 02:58:16.124409: Pseudo dice [0.6579]
2024-12-19 02:58:16.125231: Epoch time: 282.44 s
2024-12-19 02:58:17.600731: 
2024-12-19 02:58:17.602630: Epoch 122
2024-12-19 02:58:17.603526: Current learning rate: 0.00221
2024-12-19 03:03:31.661442: Validation loss did not improve from -0.37828. Patience: 120/50
2024-12-19 03:03:31.663571: train_loss -0.8754
2024-12-19 03:03:31.664567: val_loss -0.195
2024-12-19 03:03:31.665600: Pseudo dice [0.6638]
2024-12-19 03:03:31.666973: Epoch time: 314.06 s
2024-12-19 03:03:33.125727: 
2024-12-19 03:03:33.127314: Epoch 123
2024-12-19 03:03:33.128354: Current learning rate: 0.00214
2024-12-19 03:08:39.083688: Validation loss did not improve from -0.37828. Patience: 121/50
2024-12-19 03:08:39.084387: train_loss -0.8769
2024-12-19 03:08:39.085358: val_loss -0.192
2024-12-19 03:08:39.086214: Pseudo dice [0.6542]
2024-12-19 03:08:39.087188: Epoch time: 305.96 s
2024-12-19 03:08:40.603317: 
2024-12-19 03:08:40.604302: Epoch 124
2024-12-19 03:08:40.605081: Current learning rate: 0.00207
2024-12-19 03:13:41.395333: Validation loss did not improve from -0.37828. Patience: 122/50
2024-12-19 03:13:41.397139: train_loss -0.8766
2024-12-19 03:13:41.398097: val_loss -0.2142
2024-12-19 03:13:41.398825: Pseudo dice [0.6689]
2024-12-19 03:13:41.399806: Epoch time: 300.79 s
2024-12-19 03:13:43.274313: 
2024-12-19 03:13:43.275529: Epoch 125
2024-12-19 03:13:43.276354: Current learning rate: 0.00199
2024-12-19 03:18:40.337512: Validation loss did not improve from -0.37828. Patience: 123/50
2024-12-19 03:18:40.338443: train_loss -0.8768
2024-12-19 03:18:40.339309: val_loss -0.1924
2024-12-19 03:18:40.340109: Pseudo dice [0.6639]
2024-12-19 03:18:40.340872: Epoch time: 297.07 s
2024-12-19 03:18:41.807027: 
2024-12-19 03:18:41.808159: Epoch 126
2024-12-19 03:18:41.808957: Current learning rate: 0.00192
2024-12-19 03:23:48.439875: Validation loss did not improve from -0.37828. Patience: 124/50
2024-12-19 03:23:48.441758: train_loss -0.8766
2024-12-19 03:23:48.442660: val_loss -0.1731
2024-12-19 03:23:48.443551: Pseudo dice [0.6645]
2024-12-19 03:23:48.444402: Epoch time: 306.64 s
2024-12-19 03:23:49.984694: 
2024-12-19 03:23:49.985863: Epoch 127
2024-12-19 03:23:49.986875: Current learning rate: 0.00185
2024-12-19 03:29:03.316259: Validation loss did not improve from -0.37828. Patience: 125/50
2024-12-19 03:29:03.317471: train_loss -0.8783
2024-12-19 03:29:03.318229: val_loss -0.2005
2024-12-19 03:29:03.318839: Pseudo dice [0.6623]
2024-12-19 03:29:03.319526: Epoch time: 313.33 s
2024-12-19 03:29:05.377417: 
2024-12-19 03:29:05.378587: Epoch 128
2024-12-19 03:29:05.379323: Current learning rate: 0.00178
2024-12-19 03:34:08.853117: Validation loss did not improve from -0.37828. Patience: 126/50
2024-12-19 03:34:08.853931: train_loss -0.8788
2024-12-19 03:34:08.854653: val_loss -0.1976
2024-12-19 03:34:08.855357: Pseudo dice [0.6669]
2024-12-19 03:34:08.856223: Epoch time: 303.48 s
2024-12-19 03:34:10.363289: 
2024-12-19 03:34:10.364686: Epoch 129
2024-12-19 03:34:10.365510: Current learning rate: 0.0017
2024-12-19 03:39:18.661068: Validation loss did not improve from -0.37828. Patience: 127/50
2024-12-19 03:39:18.662951: train_loss -0.8786
2024-12-19 03:39:18.663942: val_loss -0.1892
2024-12-19 03:39:18.664625: Pseudo dice [0.6685]
2024-12-19 03:39:18.665349: Epoch time: 308.3 s
2024-12-19 03:39:20.518346: 
2024-12-19 03:39:20.519584: Epoch 130
2024-12-19 03:39:20.520229: Current learning rate: 0.00163
2024-12-19 03:44:05.041074: Validation loss did not improve from -0.37828. Patience: 128/50
2024-12-19 03:44:05.042086: train_loss -0.88
2024-12-19 03:44:05.042897: val_loss -0.1687
2024-12-19 03:44:05.043656: Pseudo dice [0.6689]
2024-12-19 03:44:05.044435: Epoch time: 284.52 s
2024-12-19 03:44:06.641644: 
2024-12-19 03:44:06.642789: Epoch 131
2024-12-19 03:44:06.643500: Current learning rate: 0.00156
2024-12-19 03:49:04.535994: Validation loss did not improve from -0.37828. Patience: 129/50
2024-12-19 03:49:04.536979: train_loss -0.8782
2024-12-19 03:49:04.537655: val_loss -0.2258
2024-12-19 03:49:04.538349: Pseudo dice [0.6667]
2024-12-19 03:49:04.539027: Epoch time: 297.9 s
2024-12-19 03:49:06.106168: 
2024-12-19 03:49:06.107309: Epoch 132
2024-12-19 03:49:06.108018: Current learning rate: 0.00148
2024-12-19 03:54:26.426473: Validation loss did not improve from -0.37828. Patience: 130/50
2024-12-19 03:54:26.428297: train_loss -0.8786
2024-12-19 03:54:26.429407: val_loss -0.1884
2024-12-19 03:54:26.430144: Pseudo dice [0.6662]
2024-12-19 03:54:26.430805: Epoch time: 320.32 s
2024-12-19 03:54:27.893300: 
2024-12-19 03:54:27.894531: Epoch 133
2024-12-19 03:54:27.895259: Current learning rate: 0.00141
2024-12-19 03:59:33.007856: Validation loss did not improve from -0.37828. Patience: 131/50
2024-12-19 03:59:33.008772: train_loss -0.8793
2024-12-19 03:59:33.009479: val_loss -0.2007
2024-12-19 03:59:33.010089: Pseudo dice [0.6711]
2024-12-19 03:59:33.010772: Epoch time: 305.12 s
2024-12-19 03:59:34.412228: 
2024-12-19 03:59:34.413078: Epoch 134
2024-12-19 03:59:34.413718: Current learning rate: 0.00133
2024-12-19 04:04:54.619050: Validation loss did not improve from -0.37828. Patience: 132/50
2024-12-19 04:04:54.620341: train_loss -0.881
2024-12-19 04:04:54.622330: val_loss -0.1622
2024-12-19 04:04:54.623242: Pseudo dice [0.6496]
2024-12-19 04:04:54.624310: Epoch time: 320.21 s
2024-12-19 04:04:56.532410: 
2024-12-19 04:04:56.533638: Epoch 135
2024-12-19 04:04:56.534606: Current learning rate: 0.00126
2024-12-19 04:09:46.721554: Validation loss did not improve from -0.37828. Patience: 133/50
2024-12-19 04:09:46.724312: train_loss -0.8818
2024-12-19 04:09:46.725371: val_loss -0.203
2024-12-19 04:09:46.726113: Pseudo dice [0.6589]
2024-12-19 04:09:46.727306: Epoch time: 290.19 s
2024-12-19 04:09:48.156565: 
2024-12-19 04:09:48.157665: Epoch 136
2024-12-19 04:09:48.158552: Current learning rate: 0.00118
2024-12-19 04:15:03.032100: Validation loss did not improve from -0.37828. Patience: 134/50
2024-12-19 04:15:03.033308: train_loss -0.8818
2024-12-19 04:15:03.034182: val_loss -0.2226
2024-12-19 04:15:03.034790: Pseudo dice [0.6732]
2024-12-19 04:15:03.035388: Epoch time: 314.88 s
2024-12-19 04:15:04.447030: 
2024-12-19 04:15:04.448146: Epoch 137
2024-12-19 04:15:04.448842: Current learning rate: 0.00111
2024-12-19 04:20:30.371060: Validation loss did not improve from -0.37828. Patience: 135/50
2024-12-19 04:20:30.371951: train_loss -0.8808
2024-12-19 04:20:30.372943: val_loss -0.2003
2024-12-19 04:20:30.373855: Pseudo dice [0.6698]
2024-12-19 04:20:30.374756: Epoch time: 325.93 s
2024-12-19 04:20:31.818718: 
2024-12-19 04:20:31.819945: Epoch 138
2024-12-19 04:20:31.820708: Current learning rate: 0.00103
2024-12-19 04:25:52.975463: Validation loss did not improve from -0.37828. Patience: 136/50
2024-12-19 04:25:52.977299: train_loss -0.8821
2024-12-19 04:25:52.978059: val_loss -0.1667
2024-12-19 04:25:52.978644: Pseudo dice [0.6647]
2024-12-19 04:25:52.979303: Epoch time: 321.16 s
2024-12-19 04:25:58.349486: 
2024-12-19 04:25:58.350581: Epoch 139
2024-12-19 04:25:58.351348: Current learning rate: 0.00095
2024-12-19 04:31:12.055412: Validation loss did not improve from -0.37828. Patience: 137/50
2024-12-19 04:31:12.056343: train_loss -0.8838
2024-12-19 04:31:12.057087: val_loss -0.1878
2024-12-19 04:31:12.057743: Pseudo dice [0.656]
2024-12-19 04:31:12.058393: Epoch time: 313.71 s
2024-12-19 04:31:13.979410: 
2024-12-19 04:31:13.980459: Epoch 140
2024-12-19 04:31:13.981225: Current learning rate: 0.00087
2024-12-19 04:36:03.671355: Validation loss did not improve from -0.37828. Patience: 138/50
2024-12-19 04:36:03.672348: train_loss -0.8835
2024-12-19 04:36:03.673151: val_loss -0.1701
2024-12-19 04:36:03.673849: Pseudo dice [0.6528]
2024-12-19 04:36:03.674513: Epoch time: 289.69 s
2024-12-19 04:36:05.133290: 
2024-12-19 04:36:05.134490: Epoch 141
2024-12-19 04:36:05.135197: Current learning rate: 0.00079
2024-12-19 04:41:16.475847: Validation loss did not improve from -0.37828. Patience: 139/50
2024-12-19 04:41:16.476622: train_loss -0.8832
2024-12-19 04:41:16.477335: val_loss -0.1872
2024-12-19 04:41:16.477985: Pseudo dice [0.6699]
2024-12-19 04:41:16.478615: Epoch time: 311.34 s
2024-12-19 04:41:17.907293: 
2024-12-19 04:41:17.908484: Epoch 142
2024-12-19 04:41:17.909146: Current learning rate: 0.00071
2024-12-19 04:46:35.752256: Validation loss did not improve from -0.37828. Patience: 140/50
2024-12-19 04:46:35.753182: train_loss -0.8831
2024-12-19 04:46:35.753948: val_loss -0.1918
2024-12-19 04:46:35.754723: Pseudo dice [0.6678]
2024-12-19 04:46:35.755382: Epoch time: 317.85 s
2024-12-19 04:46:37.267370: 
2024-12-19 04:46:37.269231: Epoch 143
2024-12-19 04:46:37.270065: Current learning rate: 0.00063
2024-12-19 04:51:13.836605: Validation loss did not improve from -0.37828. Patience: 141/50
2024-12-19 04:51:13.838293: train_loss -0.8829
2024-12-19 04:51:13.839293: val_loss -0.1558
2024-12-19 04:51:13.839920: Pseudo dice [0.6576]
2024-12-19 04:51:13.840604: Epoch time: 276.57 s
2024-12-19 04:51:15.369305: 
2024-12-19 04:51:15.370397: Epoch 144
2024-12-19 04:51:15.371173: Current learning rate: 0.00055
2024-12-19 04:56:21.187897: Validation loss did not improve from -0.37828. Patience: 142/50
2024-12-19 04:56:21.188896: train_loss -0.8832
2024-12-19 04:56:21.189905: val_loss -0.154
2024-12-19 04:56:21.190965: Pseudo dice [0.6613]
2024-12-19 04:56:21.191989: Epoch time: 305.82 s
2024-12-19 04:56:23.209735: 
2024-12-19 04:56:23.211194: Epoch 145
2024-12-19 04:56:23.211980: Current learning rate: 0.00047
2024-12-19 05:01:52.098956: Validation loss did not improve from -0.37828. Patience: 143/50
2024-12-19 05:01:52.099955: train_loss -0.883
2024-12-19 05:01:52.100832: val_loss -0.1951
2024-12-19 05:01:52.101675: Pseudo dice [0.6657]
2024-12-19 05:01:52.102520: Epoch time: 328.89 s
2024-12-19 05:01:53.708140: 
2024-12-19 05:01:53.709371: Epoch 146
2024-12-19 05:01:53.710322: Current learning rate: 0.00038
2024-12-19 05:07:12.419561: Validation loss did not improve from -0.37828. Patience: 144/50
2024-12-19 05:07:12.420814: train_loss -0.8854
2024-12-19 05:07:12.422938: val_loss -0.1799
2024-12-19 05:07:12.423722: Pseudo dice [0.6659]
2024-12-19 05:07:12.424636: Epoch time: 318.71 s
2024-12-19 05:07:13.967892: 
2024-12-19 05:07:13.968943: Epoch 147
2024-12-19 05:07:13.969630: Current learning rate: 0.0003
2024-12-19 05:12:24.105119: Validation loss did not improve from -0.37828. Patience: 145/50
2024-12-19 05:12:24.105912: train_loss -0.8845
2024-12-19 05:12:24.106650: val_loss -0.1711
2024-12-19 05:12:24.107299: Pseudo dice [0.6552]
2024-12-19 05:12:24.107938: Epoch time: 310.14 s
2024-12-19 05:12:25.559680: 
2024-12-19 05:12:25.560913: Epoch 148
2024-12-19 05:12:25.561579: Current learning rate: 0.00021
2024-12-19 05:17:48.380939: Validation loss did not improve from -0.37828. Patience: 146/50
2024-12-19 05:17:48.381905: train_loss -0.8855
2024-12-19 05:17:48.382659: val_loss -0.1746
2024-12-19 05:17:48.383296: Pseudo dice [0.661]
2024-12-19 05:17:48.383997: Epoch time: 322.82 s
2024-12-19 05:17:49.988540: 
2024-12-19 05:17:49.989818: Epoch 149
2024-12-19 05:17:49.990515: Current learning rate: 0.00011
2024-12-19 05:23:25.484839: Validation loss did not improve from -0.37828. Patience: 147/50
2024-12-19 05:23:25.485788: train_loss -0.8853
2024-12-19 05:23:25.486821: val_loss -0.2048
2024-12-19 05:23:25.487794: Pseudo dice [0.6653]
2024-12-19 05:23:25.488687: Epoch time: 335.5 s
2024-12-19 05:23:28.627082: Training done.
2024-12-19 05:23:28.970198: Using splits from existing split file: /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset309_Sohee_Calcium_OCT_CrossValidation/splits_final_20.json
2024-12-19 05:23:28.989463: The split file contains 5 splits.
2024-12-19 05:23:28.990494: Desired fold for training: 2
2024-12-19 05:23:28.991347: This split has 1 training and 7 validation cases.
2024-12-19 05:23:28.992519: predicting 101-019
2024-12-19 05:23:29.030125: 101-019, shape torch.Size([1, 375, 498, 498]), rank 0
2024-12-19 05:25:56.810787: predicting 101-044
2024-12-19 05:25:56.825709: 101-044, shape torch.Size([1, 404, 498, 498]), rank 0
2024-12-19 05:28:09.822974: predicting 101-045
2024-12-19 05:28:09.853077: 101-045, shape torch.Size([1, 375, 498, 498]), rank 0
2024-12-19 05:30:11.287106: predicting 106-002
2024-12-19 05:30:11.302229: 106-002, shape torch.Size([1, 539, 498, 498]), rank 0
2024-12-19 05:33:08.188891: predicting 401-004
2024-12-19 05:33:08.202711: 401-004, shape torch.Size([1, 375, 498, 498]), rank 0
2024-12-19 05:35:14.813727: predicting 704-003
2024-12-19 05:35:14.871560: 704-003, shape torch.Size([1, 375, 498, 498]), rank 0
2024-12-19 05:37:03.120273: predicting 706-005
2024-12-19 05:37:03.132794: 706-005, shape torch.Size([1, 375, 498, 498]), rank 0
2024-12-19 05:39:32.539615: Validation complete
2024-12-19 05:39:32.540229: Mean Validation Dice:  0.6085425814844108
2024-12-18 16:27:22.025047: unpacking done...
2024-12-18 16:27:22.210212: Unable to plot network architecture: nnUNet_compile is enabled!
2024-12-18 16:27:22.276843: 
2024-12-18 16:27:22.278131: Epoch 0
2024-12-18 16:27:22.279379: Current learning rate: 0.01
2024-12-18 16:36:57.463883: Validation loss improved from 1000.00000 to -0.34442! Patience: 0/50
2024-12-18 16:36:57.465287: train_loss -0.386
2024-12-18 16:36:57.466572: val_loss -0.3444
2024-12-18 16:36:57.467227: Pseudo dice [0.6448]
2024-12-18 16:36:57.467861: Epoch time: 575.19 s
2024-12-18 16:36:57.468443: Yayy! New best EMA pseudo Dice: 0.6448
2024-12-18 16:36:59.077802: 
2024-12-18 16:36:59.079220: Epoch 1
2024-12-18 16:36:59.080117: Current learning rate: 0.00994
2024-12-18 16:45:12.569622: Validation loss improved from -0.34442 to -0.35043! Patience: 0/50
2024-12-18 16:45:12.570680: train_loss -0.5904
2024-12-18 16:45:12.571522: val_loss -0.3504
2024-12-18 16:45:12.572229: Pseudo dice [0.6555]
2024-12-18 16:45:12.573022: Epoch time: 493.49 s
2024-12-18 16:45:12.573678: Yayy! New best EMA pseudo Dice: 0.6459
2024-12-18 16:45:14.418662: 
2024-12-18 16:45:14.419809: Epoch 2
2024-12-18 16:45:14.420490: Current learning rate: 0.00988
2024-12-18 16:53:01.466445: Validation loss did not improve from -0.35043. Patience: 1/50
2024-12-18 16:53:01.468437: train_loss -0.6384
2024-12-18 16:53:01.469198: val_loss -0.3171
2024-12-18 16:53:01.470016: Pseudo dice [0.6347]
2024-12-18 16:53:01.470866: Epoch time: 467.05 s
2024-12-18 16:53:02.921140: 
2024-12-18 16:53:02.922352: Epoch 3
2024-12-18 16:53:02.923032: Current learning rate: 0.00982
2024-12-18 17:00:07.971790: Validation loss improved from -0.35043 to -0.35579! Patience: 1/50
2024-12-18 17:00:07.972633: train_loss -0.6689
2024-12-18 17:00:07.973459: val_loss -0.3558
2024-12-18 17:00:07.974360: Pseudo dice [0.6517]
2024-12-18 17:00:07.975080: Epoch time: 425.05 s
2024-12-18 17:00:09.395951: 
2024-12-18 17:00:09.397572: Epoch 4
2024-12-18 17:00:09.398925: Current learning rate: 0.00976
2024-12-18 17:08:36.241057: Validation loss did not improve from -0.35579. Patience: 1/50
2024-12-18 17:08:36.242736: train_loss -0.6833
2024-12-18 17:08:36.243613: val_loss -0.3436
2024-12-18 17:08:36.244340: Pseudo dice [0.6593]
2024-12-18 17:08:36.245020: Epoch time: 506.85 s
2024-12-18 17:08:36.661199: Yayy! New best EMA pseudo Dice: 0.6468
2024-12-18 17:08:38.647524: 
2024-12-18 17:08:38.649438: Epoch 5
2024-12-18 17:08:38.650392: Current learning rate: 0.0097
2024-12-18 17:16:34.473315: Validation loss did not improve from -0.35579. Patience: 2/50
2024-12-18 17:16:34.474251: train_loss -0.6991
2024-12-18 17:16:34.475115: val_loss -0.3262
2024-12-18 17:16:34.475775: Pseudo dice [0.6381]
2024-12-18 17:16:34.476566: Epoch time: 475.83 s
2024-12-18 17:16:35.840284: 
2024-12-18 17:16:35.841632: Epoch 6
2024-12-18 17:16:35.842424: Current learning rate: 0.00964
2024-12-18 17:24:46.296769: Validation loss did not improve from -0.35579. Patience: 3/50
2024-12-18 17:24:46.298695: train_loss -0.7153
2024-12-18 17:24:46.299636: val_loss -0.3275
2024-12-18 17:24:46.300400: Pseudo dice [0.6335]
2024-12-18 17:24:46.301071: Epoch time: 490.46 s
2024-12-18 17:24:47.748651: 
2024-12-18 17:24:47.750674: Epoch 7
2024-12-18 17:24:47.751610: Current learning rate: 0.00958
2024-12-18 17:33:50.495032: Validation loss did not improve from -0.35579. Patience: 4/50
2024-12-18 17:33:50.506765: train_loss -0.7239
2024-12-18 17:33:50.508235: val_loss -0.2694
2024-12-18 17:33:50.509101: Pseudo dice [0.6341]
2024-12-18 17:33:50.509980: Epoch time: 542.76 s
2024-12-18 17:33:52.702687: 
2024-12-18 17:33:52.703899: Epoch 8
2024-12-18 17:33:52.704752: Current learning rate: 0.00952
2024-12-18 17:41:28.679897: Validation loss improved from -0.35579 to -0.37099! Patience: 4/50
2024-12-18 17:41:28.681859: train_loss -0.7344
2024-12-18 17:41:28.682759: val_loss -0.371
2024-12-18 17:41:28.683593: Pseudo dice [0.6798]
2024-12-18 17:41:28.684424: Epoch time: 455.98 s
2024-12-18 17:41:28.685186: Yayy! New best EMA pseudo Dice: 0.6473
2024-12-18 17:41:30.585683: 
2024-12-18 17:41:30.587103: Epoch 9
2024-12-18 17:41:30.588124: Current learning rate: 0.00946
2024-12-18 17:49:22.958690: Validation loss did not improve from -0.37099. Patience: 1/50
2024-12-18 17:49:22.959598: train_loss -0.7368
2024-12-18 17:49:22.960472: val_loss -0.3665
2024-12-18 17:49:22.961295: Pseudo dice [0.6662]
2024-12-18 17:49:22.962105: Epoch time: 472.38 s
2024-12-18 17:49:23.407499: Yayy! New best EMA pseudo Dice: 0.6492
2024-12-18 17:49:25.215896: 
2024-12-18 17:49:25.216882: Epoch 10
2024-12-18 17:49:25.217558: Current learning rate: 0.0094
2024-12-18 17:57:16.773335: Validation loss did not improve from -0.37099. Patience: 2/50
2024-12-18 17:57:16.774123: train_loss -0.7472
2024-12-18 17:57:16.775147: val_loss -0.3427
2024-12-18 17:57:16.775957: Pseudo dice [0.6753]
2024-12-18 17:57:16.776706: Epoch time: 471.56 s
2024-12-18 17:57:16.777449: Yayy! New best EMA pseudo Dice: 0.6518
2024-12-18 17:57:18.577631: 
2024-12-18 17:57:18.578926: Epoch 11
2024-12-18 17:57:18.579841: Current learning rate: 0.00934
2024-12-18 18:04:02.492253: Validation loss did not improve from -0.37099. Patience: 3/50
2024-12-18 18:04:02.493132: train_loss -0.751
2024-12-18 18:04:02.493909: val_loss -0.3247
2024-12-18 18:04:02.494580: Pseudo dice [0.659]
2024-12-18 18:04:02.495251: Epoch time: 403.92 s
2024-12-18 18:04:02.495917: Yayy! New best EMA pseudo Dice: 0.6525
2024-12-18 18:04:04.335743: 
2024-12-18 18:04:04.337169: Epoch 12
2024-12-18 18:04:04.338097: Current learning rate: 0.00928
2024-12-18 18:14:18.400639: Validation loss did not improve from -0.37099. Patience: 4/50
2024-12-18 18:14:18.401544: train_loss -0.7568
2024-12-18 18:14:18.402232: val_loss -0.3235
2024-12-18 18:14:18.402886: Pseudo dice [0.6615]
2024-12-18 18:14:18.403564: Epoch time: 614.07 s
2024-12-18 18:14:18.404203: Yayy! New best EMA pseudo Dice: 0.6534
2024-12-18 18:14:20.271487: 
2024-12-18 18:14:20.272413: Epoch 13
2024-12-18 18:14:20.273181: Current learning rate: 0.00922
2024-12-18 18:25:27.550349: Validation loss did not improve from -0.37099. Patience: 5/50
2024-12-18 18:25:27.551167: train_loss -0.7607
2024-12-18 18:25:27.552051: val_loss -0.3395
2024-12-18 18:25:27.552857: Pseudo dice [0.6517]
2024-12-18 18:25:27.553719: Epoch time: 667.28 s
2024-12-18 18:25:29.008152: 
2024-12-18 18:25:29.009101: Epoch 14
2024-12-18 18:25:29.010089: Current learning rate: 0.00916
2024-12-18 18:34:08.520350: Validation loss did not improve from -0.37099. Patience: 6/50
2024-12-18 18:34:08.521193: train_loss -0.7648
2024-12-18 18:34:08.522039: val_loss -0.3084
2024-12-18 18:34:08.522812: Pseudo dice [0.6468]
2024-12-18 18:34:08.523458: Epoch time: 519.51 s
2024-12-18 18:34:10.470854: 
2024-12-18 18:34:10.471848: Epoch 15
2024-12-18 18:34:10.472554: Current learning rate: 0.0091
2024-12-18 18:42:53.371969: Validation loss did not improve from -0.37099. Patience: 7/50
2024-12-18 18:42:53.373031: train_loss -0.7683
2024-12-18 18:42:53.373974: val_loss -0.3294
2024-12-18 18:42:53.374784: Pseudo dice [0.6635]
2024-12-18 18:42:53.375574: Epoch time: 522.9 s
2024-12-18 18:42:53.376337: Yayy! New best EMA pseudo Dice: 0.6537
2024-12-18 18:42:55.270071: 
2024-12-18 18:42:55.271245: Epoch 16
2024-12-18 18:42:55.272045: Current learning rate: 0.00903
2024-12-18 18:49:38.444844: Validation loss did not improve from -0.37099. Patience: 8/50
2024-12-18 18:49:38.445673: train_loss -0.775
2024-12-18 18:49:38.446404: val_loss -0.2805
2024-12-18 18:49:38.447090: Pseudo dice [0.6223]
2024-12-18 18:49:38.447780: Epoch time: 403.18 s
2024-12-18 18:49:39.987576: 
2024-12-18 18:49:39.988881: Epoch 17
2024-12-18 18:49:39.989635: Current learning rate: 0.00897
2024-12-18 18:58:10.007217: Validation loss did not improve from -0.37099. Patience: 9/50
2024-12-18 18:58:10.008053: train_loss -0.7779
2024-12-18 18:58:10.008853: val_loss -0.2803
2024-12-18 18:58:10.009479: Pseudo dice [0.6356]
2024-12-18 18:58:10.010154: Epoch time: 510.02 s
2024-12-18 18:58:11.499282: 
2024-12-18 18:58:11.500622: Epoch 18
2024-12-18 18:58:11.501579: Current learning rate: 0.00891
2024-12-18 19:07:08.461281: Validation loss did not improve from -0.37099. Patience: 10/50
2024-12-18 19:07:08.462601: train_loss -0.7777
2024-12-18 19:07:08.463633: val_loss -0.337
2024-12-18 19:07:08.464401: Pseudo dice [0.6777]
2024-12-18 19:07:08.465157: Epoch time: 536.96 s
2024-12-18 19:07:10.497615: 
2024-12-18 19:07:10.498861: Epoch 19
2024-12-18 19:07:10.499784: Current learning rate: 0.00885
2024-12-18 19:14:56.691312: Validation loss did not improve from -0.37099. Patience: 11/50
2024-12-18 19:14:56.692234: train_loss -0.7784
2024-12-18 19:14:56.692946: val_loss -0.3382
2024-12-18 19:14:56.693614: Pseudo dice [0.6705]
2024-12-18 19:14:56.694290: Epoch time: 466.2 s
2024-12-18 19:14:57.122590: Yayy! New best EMA pseudo Dice: 0.6538
2024-12-18 19:14:59.018408: 
2024-12-18 19:14:59.019670: Epoch 20
2024-12-18 19:14:59.020431: Current learning rate: 0.00879
2024-12-18 19:23:26.172915: Validation loss did not improve from -0.37099. Patience: 12/50
2024-12-18 19:23:26.173751: train_loss -0.7845
2024-12-18 19:23:26.174458: val_loss -0.271
2024-12-18 19:23:26.175152: Pseudo dice [0.6276]
2024-12-18 19:23:26.175924: Epoch time: 507.16 s
2024-12-18 19:23:27.705811: 
2024-12-18 19:23:27.706905: Epoch 21
2024-12-18 19:23:27.707748: Current learning rate: 0.00873
2024-12-18 19:32:12.239301: Validation loss did not improve from -0.37099. Patience: 13/50
2024-12-18 19:32:12.240240: train_loss -0.7879
2024-12-18 19:32:12.240979: val_loss -0.3636
2024-12-18 19:32:12.241667: Pseudo dice [0.6774]
2024-12-18 19:32:12.242332: Epoch time: 524.54 s
2024-12-18 19:32:12.242990: Yayy! New best EMA pseudo Dice: 0.6538
2024-12-18 19:32:14.076780: 
2024-12-18 19:32:14.077821: Epoch 22
2024-12-18 19:32:14.078724: Current learning rate: 0.00867
2024-12-18 19:39:59.976890: Validation loss did not improve from -0.37099. Patience: 14/50
2024-12-18 19:39:59.977817: train_loss -0.7918
2024-12-18 19:39:59.978909: val_loss -0.3407
2024-12-18 19:39:59.979902: Pseudo dice [0.6613]
2024-12-18 19:39:59.980828: Epoch time: 465.9 s
2024-12-18 19:39:59.981603: Yayy! New best EMA pseudo Dice: 0.6545
2024-12-18 19:40:01.750488: 
2024-12-18 19:40:01.751863: Epoch 23
2024-12-18 19:40:01.752840: Current learning rate: 0.00861
2024-12-18 19:49:07.950018: Validation loss did not improve from -0.37099. Patience: 15/50
2024-12-18 19:49:07.950918: train_loss -0.7896
2024-12-18 19:49:07.951646: val_loss -0.327
2024-12-18 19:49:07.952306: Pseudo dice [0.6568]
2024-12-18 19:49:07.953093: Epoch time: 546.2 s
2024-12-18 19:49:07.953832: Yayy! New best EMA pseudo Dice: 0.6547
2024-12-18 19:49:09.794895: 
2024-12-18 19:49:09.795976: Epoch 24
2024-12-18 19:49:09.796901: Current learning rate: 0.00855
2024-12-18 19:58:20.603381: Validation loss did not improve from -0.37099. Patience: 16/50
2024-12-18 19:58:20.604293: train_loss -0.7923
2024-12-18 19:58:20.605074: val_loss -0.2587
2024-12-18 19:58:20.605809: Pseudo dice [0.6336]
2024-12-18 19:58:20.606504: Epoch time: 550.81 s
2024-12-18 19:58:22.463462: 
2024-12-18 19:58:22.464649: Epoch 25
2024-12-18 19:58:22.465308: Current learning rate: 0.00849
2024-12-18 20:06:51.999338: Validation loss did not improve from -0.37099. Patience: 17/50
2024-12-18 20:06:52.000174: train_loss -0.792
2024-12-18 20:06:52.000964: val_loss -0.2349
2024-12-18 20:06:52.001613: Pseudo dice [0.6232]
2024-12-18 20:06:52.002407: Epoch time: 509.54 s
2024-12-18 20:06:53.517847: 
2024-12-18 20:06:53.518902: Epoch 26
2024-12-18 20:06:53.519528: Current learning rate: 0.00843
2024-12-18 20:15:45.968114: Validation loss did not improve from -0.37099. Patience: 18/50
2024-12-18 20:15:45.968901: train_loss -0.7988
2024-12-18 20:15:45.969740: val_loss -0.2737
2024-12-18 20:15:45.970423: Pseudo dice [0.6574]
2024-12-18 20:15:45.971094: Epoch time: 532.45 s
2024-12-18 20:15:47.406094: 
2024-12-18 20:15:47.407228: Epoch 27
2024-12-18 20:15:47.407999: Current learning rate: 0.00836
2024-12-18 20:25:07.363656: Validation loss did not improve from -0.37099. Patience: 19/50
2024-12-18 20:25:07.364549: train_loss -0.7993
2024-12-18 20:25:07.365370: val_loss -0.283
2024-12-18 20:25:07.366163: Pseudo dice [0.6489]
2024-12-18 20:25:07.366985: Epoch time: 559.96 s
2024-12-18 20:25:08.793642: 
2024-12-18 20:25:08.795054: Epoch 28
2024-12-18 20:25:08.795809: Current learning rate: 0.0083
2024-12-18 20:33:23.270309: Validation loss did not improve from -0.37099. Patience: 20/50
2024-12-18 20:33:23.273449: train_loss -0.8009
2024-12-18 20:33:23.275053: val_loss -0.2458
2024-12-18 20:33:23.275808: Pseudo dice [0.6316]
2024-12-18 20:33:23.276882: Epoch time: 494.48 s
2024-12-18 20:33:24.738759: 
2024-12-18 20:33:24.740009: Epoch 29
2024-12-18 20:33:24.740955: Current learning rate: 0.00824
2024-12-18 20:42:50.983677: Validation loss did not improve from -0.37099. Patience: 21/50
2024-12-18 20:42:50.984535: train_loss -0.7994
2024-12-18 20:42:50.985251: val_loss -0.2424
2024-12-18 20:42:50.985880: Pseudo dice [0.6203]
2024-12-18 20:42:50.986556: Epoch time: 566.25 s
2024-12-18 20:42:53.374587: 
2024-12-18 20:42:53.375855: Epoch 30
2024-12-18 20:42:53.376579: Current learning rate: 0.00818
2024-12-18 20:52:10.121938: Validation loss did not improve from -0.37099. Patience: 22/50
2024-12-18 20:52:10.123040: train_loss -0.8009
2024-12-18 20:52:10.124073: val_loss -0.2448
2024-12-18 20:52:10.124979: Pseudo dice [0.6353]
2024-12-18 20:52:10.125935: Epoch time: 556.75 s
2024-12-18 20:52:11.558364: 
2024-12-18 20:52:11.559818: Epoch 31
2024-12-18 20:52:11.560755: Current learning rate: 0.00812
2024-12-18 21:00:08.892863: Validation loss did not improve from -0.37099. Patience: 23/50
2024-12-18 21:00:08.894040: train_loss -0.8069
2024-12-18 21:00:08.894831: val_loss -0.2882
2024-12-18 21:00:08.895597: Pseudo dice [0.6557]
2024-12-18 21:00:08.896322: Epoch time: 477.34 s
2024-12-18 21:00:10.403427: 
2024-12-18 21:00:10.405363: Epoch 32
2024-12-18 21:00:10.406292: Current learning rate: 0.00806
2024-12-18 21:09:27.173544: Validation loss did not improve from -0.37099. Patience: 24/50
2024-12-18 21:09:27.174388: train_loss -0.8027
2024-12-18 21:09:27.175117: val_loss -0.2519
2024-12-18 21:09:27.175790: Pseudo dice [0.6389]
2024-12-18 21:09:27.176517: Epoch time: 556.77 s
2024-12-18 21:09:28.646626: 
2024-12-18 21:09:28.647661: Epoch 33
2024-12-18 21:09:28.648378: Current learning rate: 0.008
2024-12-18 21:18:09.901204: Validation loss did not improve from -0.37099. Patience: 25/50
2024-12-18 21:18:09.902058: train_loss -0.8074
2024-12-18 21:18:09.903094: val_loss -0.2704
2024-12-18 21:18:09.904145: Pseudo dice [0.6345]
2024-12-18 21:18:09.905109: Epoch time: 521.26 s
2024-12-18 21:18:11.461123: 
2024-12-18 21:18:11.462348: Epoch 34
2024-12-18 21:18:11.463471: Current learning rate: 0.00793
2024-12-18 21:26:27.349005: Validation loss did not improve from -0.37099. Patience: 26/50
2024-12-18 21:26:27.349946: train_loss -0.8063
2024-12-18 21:26:27.350965: val_loss -0.2731
2024-12-18 21:26:27.351959: Pseudo dice [0.6512]
2024-12-18 21:26:27.352937: Epoch time: 495.89 s
2024-12-18 21:26:29.272486: 
2024-12-18 21:26:29.274031: Epoch 35
2024-12-18 21:26:29.275017: Current learning rate: 0.00787
2024-12-18 21:34:57.158225: Validation loss did not improve from -0.37099. Patience: 27/50
2024-12-18 21:34:57.160097: train_loss -0.8108
2024-12-18 21:34:57.161046: val_loss -0.3159
2024-12-18 21:34:57.161850: Pseudo dice [0.6689]
2024-12-18 21:34:57.162569: Epoch time: 507.89 s
2024-12-18 21:34:58.732621: 
2024-12-18 21:34:58.733967: Epoch 36
2024-12-18 21:34:58.734934: Current learning rate: 0.00781
2024-12-18 21:43:41.202861: Validation loss did not improve from -0.37099. Patience: 28/50
2024-12-18 21:43:41.203887: train_loss -0.8121
2024-12-18 21:43:41.204687: val_loss -0.2358
2024-12-18 21:43:41.205334: Pseudo dice [0.6365]
2024-12-18 21:43:41.206151: Epoch time: 522.47 s
2024-12-18 21:43:42.701385: 
2024-12-18 21:43:42.702599: Epoch 37
2024-12-18 21:43:42.703344: Current learning rate: 0.00775
2024-12-18 21:53:02.028882: Validation loss did not improve from -0.37099. Patience: 29/50
2024-12-18 21:53:02.030307: train_loss -0.8125
2024-12-18 21:53:02.031464: val_loss -0.3358
2024-12-18 21:53:02.032431: Pseudo dice [0.6727]
2024-12-18 21:53:02.033441: Epoch time: 559.33 s
2024-12-18 21:53:03.489904: 
2024-12-18 21:53:03.491466: Epoch 38
2024-12-18 21:53:03.492572: Current learning rate: 0.00769
2024-12-18 22:02:22.373096: Validation loss did not improve from -0.37099. Patience: 30/50
2024-12-18 22:02:22.374137: train_loss -0.8129
2024-12-18 22:02:22.375037: val_loss -0.2337
2024-12-18 22:02:22.375740: Pseudo dice [0.6397]
2024-12-18 22:02:22.376493: Epoch time: 558.89 s
2024-12-18 22:02:23.818876: 
2024-12-18 22:02:23.820227: Epoch 39
2024-12-18 22:02:23.821029: Current learning rate: 0.00763
2024-12-18 22:11:16.841275: Validation loss did not improve from -0.37099. Patience: 31/50
2024-12-18 22:11:16.842029: train_loss -0.8171
2024-12-18 22:11:16.842992: val_loss -0.2998
2024-12-18 22:11:16.843755: Pseudo dice [0.6445]
2024-12-18 22:11:16.844384: Epoch time: 533.02 s
2024-12-18 22:11:18.872403: 
2024-12-18 22:11:18.873616: Epoch 40
2024-12-18 22:11:18.874414: Current learning rate: 0.00756
2024-12-18 22:20:51.793274: Validation loss did not improve from -0.37099. Patience: 32/50
2024-12-18 22:20:51.793890: train_loss -0.8161
2024-12-18 22:20:51.794580: val_loss -0.3169
2024-12-18 22:20:51.795230: Pseudo dice [0.6689]
2024-12-18 22:20:51.795946: Epoch time: 572.92 s
2024-12-18 22:20:54.020340: 
2024-12-18 22:20:54.021520: Epoch 41
2024-12-18 22:20:54.022298: Current learning rate: 0.0075
2024-12-18 22:30:05.361962: Validation loss did not improve from -0.37099. Patience: 33/50
2024-12-18 22:30:05.363051: train_loss -0.8149
2024-12-18 22:30:05.363778: val_loss -0.2416
2024-12-18 22:30:05.364419: Pseudo dice [0.64]
2024-12-18 22:30:05.365152: Epoch time: 551.34 s
2024-12-18 22:30:06.813003: 
2024-12-18 22:30:06.814020: Epoch 42
2024-12-18 22:30:06.814693: Current learning rate: 0.00744
2024-12-18 22:39:18.984782: Validation loss did not improve from -0.37099. Patience: 34/50
2024-12-18 22:39:18.985868: train_loss -0.8187
2024-12-18 22:39:18.986735: val_loss -0.3283
2024-12-18 22:39:18.987561: Pseudo dice [0.6809]
2024-12-18 22:39:18.988406: Epoch time: 552.17 s
2024-12-18 22:39:20.402310: 
2024-12-18 22:39:20.403559: Epoch 43
2024-12-18 22:39:20.404272: Current learning rate: 0.00738
2024-12-18 22:48:37.876436: Validation loss did not improve from -0.37099. Patience: 35/50
2024-12-18 22:48:37.877448: train_loss -0.8192
2024-12-18 22:48:37.878206: val_loss -0.2795
2024-12-18 22:48:37.878904: Pseudo dice [0.6482]
2024-12-18 22:48:37.879609: Epoch time: 557.48 s
2024-12-18 22:48:39.268116: 
2024-12-18 22:48:39.269165: Epoch 44
2024-12-18 22:48:39.269961: Current learning rate: 0.00732
2024-12-18 22:57:24.395588: Validation loss did not improve from -0.37099. Patience: 36/50
2024-12-18 22:57:24.397595: train_loss -0.8187
2024-12-18 22:57:24.399047: val_loss -0.2934
2024-12-18 22:57:24.400012: Pseudo dice [0.6584]
2024-12-18 22:57:24.401094: Epoch time: 525.13 s
2024-12-18 22:57:26.350910: 
2024-12-18 22:57:26.352161: Epoch 45
2024-12-18 22:57:26.352961: Current learning rate: 0.00725
2024-12-18 23:06:40.896187: Validation loss did not improve from -0.37099. Patience: 37/50
2024-12-18 23:06:40.897289: train_loss -0.8226
2024-12-18 23:06:40.898164: val_loss -0.2625
2024-12-18 23:06:40.898992: Pseudo dice [0.6416]
2024-12-18 23:06:40.899781: Epoch time: 554.55 s
2024-12-18 23:06:42.366246: 
2024-12-18 23:06:42.367664: Epoch 46
2024-12-18 23:06:42.368500: Current learning rate: 0.00719
2024-12-18 23:15:59.927339: Validation loss did not improve from -0.37099. Patience: 38/50
2024-12-18 23:15:59.928230: train_loss -0.8228
2024-12-18 23:15:59.929249: val_loss -0.2021
2024-12-18 23:15:59.930202: Pseudo dice [0.6264]
2024-12-18 23:15:59.931205: Epoch time: 557.56 s
2024-12-18 23:16:01.376270: 
2024-12-18 23:16:01.377621: Epoch 47
2024-12-18 23:16:01.379009: Current learning rate: 0.00713
2024-12-18 23:25:12.512020: Validation loss did not improve from -0.37099. Patience: 39/50
2024-12-18 23:25:12.512830: train_loss -0.8225
2024-12-18 23:25:12.513755: val_loss -0.2584
2024-12-18 23:25:12.514410: Pseudo dice [0.6516]
2024-12-18 23:25:12.515065: Epoch time: 551.14 s
2024-12-18 23:25:14.123150: 
2024-12-18 23:25:14.124117: Epoch 48
2024-12-18 23:25:14.124789: Current learning rate: 0.00707
2024-12-18 23:34:22.959269: Validation loss did not improve from -0.37099. Patience: 40/50
2024-12-18 23:34:22.959941: train_loss -0.8215
2024-12-18 23:34:22.960705: val_loss -0.2529
2024-12-18 23:34:22.961465: Pseudo dice [0.6488]
2024-12-18 23:34:22.962187: Epoch time: 548.84 s
2024-12-18 23:34:24.501267: 
2024-12-18 23:34:24.502523: Epoch 49
2024-12-18 23:34:24.503421: Current learning rate: 0.007
2024-12-18 23:43:09.951192: Validation loss did not improve from -0.37099. Patience: 41/50
2024-12-18 23:43:09.952105: train_loss -0.8244
2024-12-18 23:43:09.952892: val_loss -0.2554
2024-12-18 23:43:09.953567: Pseudo dice [0.6462]
2024-12-18 23:43:09.954279: Epoch time: 525.45 s
2024-12-18 23:43:11.842808: 
2024-12-18 23:43:11.843821: Epoch 50
2024-12-18 23:43:11.844533: Current learning rate: 0.00694
2024-12-18 23:52:30.090523: Validation loss did not improve from -0.37099. Patience: 42/50
2024-12-18 23:52:30.091692: train_loss -0.8277
2024-12-18 23:52:30.092560: val_loss -0.2312
2024-12-18 23:52:30.093236: Pseudo dice [0.6347]
2024-12-18 23:52:30.093994: Epoch time: 558.25 s
2024-12-18 23:52:31.766306: 
2024-12-18 23:52:31.767528: Epoch 51
2024-12-18 23:52:31.768389: Current learning rate: 0.00688
2024-12-19 00:02:02.976165: Validation loss did not improve from -0.37099. Patience: 43/50
2024-12-19 00:02:02.976995: train_loss -0.8244
2024-12-19 00:02:02.977784: val_loss -0.2544
2024-12-19 00:02:02.978541: Pseudo dice [0.6308]
2024-12-19 00:02:02.979341: Epoch time: 571.21 s
2024-12-19 00:02:05.203691: 
2024-12-19 00:02:05.204997: Epoch 52
2024-12-19 00:02:05.205903: Current learning rate: 0.00682
2024-12-19 00:10:36.493235: Validation loss did not improve from -0.37099. Patience: 44/50
2024-12-19 00:10:36.494345: train_loss -0.8306
2024-12-19 00:10:36.495255: val_loss -0.2697
2024-12-19 00:10:36.495942: Pseudo dice [0.6621]
2024-12-19 00:10:36.496703: Epoch time: 511.29 s
2024-12-19 00:10:37.949288: 
2024-12-19 00:10:37.950100: Epoch 53
2024-12-19 00:10:37.950812: Current learning rate: 0.00675
2024-12-19 00:19:24.602407: Validation loss did not improve from -0.37099. Patience: 45/50
2024-12-19 00:19:24.603374: train_loss -0.8297
2024-12-19 00:19:24.604323: val_loss -0.209
2024-12-19 00:19:24.605080: Pseudo dice [0.6276]
2024-12-19 00:19:24.605903: Epoch time: 526.66 s
2024-12-19 00:19:26.033059: 
2024-12-19 00:19:26.034274: Epoch 54
2024-12-19 00:19:26.035031: Current learning rate: 0.00669
2024-12-19 00:28:50.385125: Validation loss did not improve from -0.37099. Patience: 46/50
2024-12-19 00:28:50.386104: train_loss -0.8297
2024-12-19 00:28:50.386917: val_loss -0.22
2024-12-19 00:28:50.387696: Pseudo dice [0.6303]
2024-12-19 00:28:50.388433: Epoch time: 564.35 s
2024-12-19 00:28:52.386311: 
2024-12-19 00:28:52.387644: Epoch 55
2024-12-19 00:28:52.388713: Current learning rate: 0.00663
2024-12-19 00:37:55.643657: Validation loss did not improve from -0.37099. Patience: 47/50
2024-12-19 00:37:55.644597: train_loss -0.8303
2024-12-19 00:37:55.645563: val_loss -0.1635
2024-12-19 00:37:55.646245: Pseudo dice [0.627]
2024-12-19 00:37:55.647032: Epoch time: 543.26 s
2024-12-19 00:37:57.136462: 
2024-12-19 00:37:57.137578: Epoch 56
2024-12-19 00:37:57.138437: Current learning rate: 0.00657
2024-12-19 00:47:19.652497: Validation loss did not improve from -0.37099. Patience: 48/50
2024-12-19 00:47:19.654147: train_loss -0.8315
2024-12-19 00:47:19.655446: val_loss -0.2377
2024-12-19 00:47:19.656590: Pseudo dice [0.6467]
2024-12-19 00:47:19.657709: Epoch time: 562.52 s
2024-12-19 00:47:21.124889: 
2024-12-19 00:47:21.126421: Epoch 57
2024-12-19 00:47:21.127566: Current learning rate: 0.0065
2024-12-19 00:55:53.263602: Validation loss did not improve from -0.37099. Patience: 49/50
2024-12-19 00:55:53.264609: train_loss -0.8326
2024-12-19 00:55:53.265581: val_loss -0.2068
2024-12-19 00:55:53.266337: Pseudo dice [0.6292]
2024-12-19 00:55:53.267067: Epoch time: 512.14 s
2024-12-19 00:55:54.761558: 
2024-12-19 00:55:54.762395: Epoch 58
2024-12-19 00:55:54.763318: Current learning rate: 0.00644
2024-12-19 01:05:02.136379: Validation loss did not improve from -0.37099. Patience: 50/50
2024-12-19 01:05:02.137316: train_loss -0.8326
2024-12-19 01:05:02.138316: val_loss -0.2503
2024-12-19 01:05:02.139004: Pseudo dice [0.6505]
2024-12-19 01:05:02.139702: Epoch time: 547.38 s
2024-12-19 01:05:03.624046: 
2024-12-19 01:05:03.625084: Epoch 59
2024-12-19 01:05:03.625776: Current learning rate: 0.00638
2024-12-19 01:13:47.036943: Validation loss did not improve from -0.37099. Patience: 51/50
2024-12-19 01:13:47.038117: train_loss -0.8346
2024-12-19 01:13:47.038932: val_loss -0.2056
2024-12-19 01:13:47.039694: Pseudo dice [0.6279]
2024-12-19 01:13:47.040509: Epoch time: 523.42 s
2024-12-19 01:13:48.929675: 
2024-12-19 01:13:48.931168: Epoch 60
2024-12-19 01:13:48.932012: Current learning rate: 0.00631
2024-12-19 01:22:16.257916: Validation loss did not improve from -0.37099. Patience: 52/50
2024-12-19 01:22:16.258847: train_loss -0.8321
2024-12-19 01:22:16.259990: val_loss -0.1911
2024-12-19 01:22:16.261023: Pseudo dice [0.628]
2024-12-19 01:22:16.262016: Epoch time: 507.33 s
2024-12-19 01:22:17.764410: 
2024-12-19 01:22:17.765979: Epoch 61
2024-12-19 01:22:17.767080: Current learning rate: 0.00625
2024-12-19 01:31:17.668326: Validation loss did not improve from -0.37099. Patience: 53/50
2024-12-19 01:31:17.669363: train_loss -0.8347
2024-12-19 01:31:17.670189: val_loss -0.2577
2024-12-19 01:31:17.670981: Pseudo dice [0.6537]
2024-12-19 01:31:17.671792: Epoch time: 539.91 s
2024-12-19 01:31:19.280099: 
2024-12-19 01:31:19.281357: Epoch 62
2024-12-19 01:31:19.282263: Current learning rate: 0.00619
2024-12-19 01:39:38.898462: Validation loss did not improve from -0.37099. Patience: 54/50
2024-12-19 01:39:38.899344: train_loss -0.8357
2024-12-19 01:39:38.900138: val_loss -0.2436
2024-12-19 01:39:38.900913: Pseudo dice [0.6471]
2024-12-19 01:39:38.901556: Epoch time: 499.62 s
2024-12-19 01:39:40.786903: 
2024-12-19 01:39:40.788158: Epoch 63
2024-12-19 01:39:40.788855: Current learning rate: 0.00612
2024-12-19 01:49:01.334369: Validation loss did not improve from -0.37099. Patience: 55/50
2024-12-19 01:49:01.335135: train_loss -0.8394
2024-12-19 01:49:01.335891: val_loss -0.1973
2024-12-19 01:49:01.336586: Pseudo dice [0.6293]
2024-12-19 01:49:01.337364: Epoch time: 560.55 s
2024-12-19 01:49:02.820697: 
2024-12-19 01:49:02.821726: Epoch 64
2024-12-19 01:49:02.822562: Current learning rate: 0.00606
2024-12-19 01:57:22.629302: Validation loss did not improve from -0.37099. Patience: 56/50
2024-12-19 01:57:22.632386: train_loss -0.839
2024-12-19 01:57:22.633287: val_loss -0.1652
2024-12-19 01:57:22.633939: Pseudo dice [0.6203]
2024-12-19 01:57:22.634956: Epoch time: 499.81 s
2024-12-19 01:57:24.745335: 
2024-12-19 01:57:24.746383: Epoch 65
2024-12-19 01:57:24.747097: Current learning rate: 0.006
2024-12-19 02:05:49.934282: Validation loss did not improve from -0.37099. Patience: 57/50
2024-12-19 02:05:49.935078: train_loss -0.8386
2024-12-19 02:05:49.935762: val_loss -0.2276
2024-12-19 02:05:49.936544: Pseudo dice [0.6452]
2024-12-19 02:05:49.937402: Epoch time: 505.19 s
2024-12-19 02:05:51.488622: 
2024-12-19 02:05:51.489980: Epoch 66
2024-12-19 02:05:51.490828: Current learning rate: 0.00593
2024-12-19 02:14:56.734586: Validation loss did not improve from -0.37099. Patience: 58/50
2024-12-19 02:14:56.735354: train_loss -0.8399
2024-12-19 02:14:56.750007: val_loss -0.2712
2024-12-19 02:14:56.750951: Pseudo dice [0.6524]
2024-12-19 02:14:56.751640: Epoch time: 545.25 s
2024-12-19 02:14:58.201024: 
2024-12-19 02:14:58.202254: Epoch 67
2024-12-19 02:14:58.203320: Current learning rate: 0.00587
2024-12-19 02:23:12.515830: Validation loss did not improve from -0.37099. Patience: 59/50
2024-12-19 02:23:12.516817: train_loss -0.8413
2024-12-19 02:23:12.517859: val_loss -0.2561
2024-12-19 02:23:12.518756: Pseudo dice [0.6541]
2024-12-19 02:23:12.519672: Epoch time: 494.32 s
2024-12-19 02:23:14.088759: 
2024-12-19 02:23:14.090395: Epoch 68
2024-12-19 02:23:14.091352: Current learning rate: 0.00581
2024-12-19 02:32:04.733142: Validation loss did not improve from -0.37099. Patience: 60/50
2024-12-19 02:32:04.733923: train_loss -0.8444
2024-12-19 02:32:04.734939: val_loss -0.2742
2024-12-19 02:32:04.735842: Pseudo dice [0.6595]
2024-12-19 02:32:04.736734: Epoch time: 530.65 s
2024-12-19 02:32:06.219350: 
2024-12-19 02:32:06.220514: Epoch 69
2024-12-19 02:32:06.221298: Current learning rate: 0.00574
2024-12-19 02:41:14.390378: Validation loss did not improve from -0.37099. Patience: 61/50
2024-12-19 02:41:14.391332: train_loss -0.8443
2024-12-19 02:41:14.392295: val_loss -0.2138
2024-12-19 02:41:14.393123: Pseudo dice [0.6368]
2024-12-19 02:41:14.393897: Epoch time: 548.17 s
2024-12-19 02:41:16.424258: 
2024-12-19 02:41:16.425737: Epoch 70
2024-12-19 02:41:16.426794: Current learning rate: 0.00568
2024-12-19 02:50:58.910399: Validation loss did not improve from -0.37099. Patience: 62/50
2024-12-19 02:50:58.911231: train_loss -0.8441
2024-12-19 02:50:58.912181: val_loss -0.27
2024-12-19 02:50:58.913086: Pseudo dice [0.6569]
2024-12-19 02:50:58.914017: Epoch time: 582.49 s
2024-12-19 02:51:00.428058: 
2024-12-19 02:51:00.429393: Epoch 71
2024-12-19 02:51:00.430189: Current learning rate: 0.00562
2024-12-19 03:00:13.911603: Validation loss did not improve from -0.37099. Patience: 63/50
2024-12-19 03:00:13.912507: train_loss -0.8467
2024-12-19 03:00:13.913393: val_loss -0.2238
2024-12-19 03:00:13.914273: Pseudo dice [0.6352]
2024-12-19 03:00:13.915178: Epoch time: 553.49 s
2024-12-19 03:00:15.413979: 
2024-12-19 03:00:15.415209: Epoch 72
2024-12-19 03:00:15.416203: Current learning rate: 0.00555
2024-12-19 03:09:27.738436: Validation loss did not improve from -0.37099. Patience: 64/50
2024-12-19 03:09:27.739446: train_loss -0.8488
2024-12-19 03:09:27.740315: val_loss -0.2391
2024-12-19 03:09:27.740974: Pseudo dice [0.6413]
2024-12-19 03:09:27.741769: Epoch time: 552.33 s
2024-12-19 03:09:29.661382: 
2024-12-19 03:09:29.662658: Epoch 73
2024-12-19 03:09:29.663317: Current learning rate: 0.00549
2024-12-19 03:18:22.624563: Validation loss did not improve from -0.37099. Patience: 65/50
2024-12-19 03:18:22.625437: train_loss -0.845
2024-12-19 03:18:22.626371: val_loss -0.2199
2024-12-19 03:18:22.627310: Pseudo dice [0.6373]
2024-12-19 03:18:22.628286: Epoch time: 532.97 s
2024-12-19 03:18:24.139545: 
2024-12-19 03:18:24.140935: Epoch 74
2024-12-19 03:18:24.141791: Current learning rate: 0.00542
2024-12-19 03:27:00.003838: Validation loss did not improve from -0.37099. Patience: 66/50
2024-12-19 03:27:00.005068: train_loss -0.8467
2024-12-19 03:27:00.006360: val_loss -0.1574
2024-12-19 03:27:00.007158: Pseudo dice [0.6066]
2024-12-19 03:27:00.008025: Epoch time: 515.87 s
2024-12-19 03:27:01.892373: 
2024-12-19 03:27:01.893756: Epoch 75
2024-12-19 03:27:01.894647: Current learning rate: 0.00536
2024-12-19 03:36:03.885158: Validation loss did not improve from -0.37099. Patience: 67/50
2024-12-19 03:36:03.886042: train_loss -0.8485
2024-12-19 03:36:03.886807: val_loss -0.2467
2024-12-19 03:36:03.887479: Pseudo dice [0.6398]
2024-12-19 03:36:03.888144: Epoch time: 542.0 s
2024-12-19 03:36:05.363543: 
2024-12-19 03:36:05.364773: Epoch 76
2024-12-19 03:36:05.365530: Current learning rate: 0.00529
2024-12-19 03:44:10.437199: Validation loss did not improve from -0.37099. Patience: 68/50
2024-12-19 03:44:10.438089: train_loss -0.851
2024-12-19 03:44:10.438941: val_loss -0.1656
2024-12-19 03:44:10.439737: Pseudo dice [0.6208]
2024-12-19 03:44:10.440472: Epoch time: 485.08 s
2024-12-19 03:44:12.022312: 
2024-12-19 03:44:12.023470: Epoch 77
2024-12-19 03:44:12.024221: Current learning rate: 0.00523
2024-12-19 03:53:07.898396: Validation loss did not improve from -0.37099. Patience: 69/50
2024-12-19 03:53:07.899409: train_loss -0.8483
2024-12-19 03:53:07.900461: val_loss -0.1655
2024-12-19 03:53:07.901157: Pseudo dice [0.6199]
2024-12-19 03:53:07.901931: Epoch time: 535.88 s
2024-12-19 03:53:09.414533: 
2024-12-19 03:53:09.415745: Epoch 78
2024-12-19 03:53:09.416630: Current learning rate: 0.00517
2024-12-19 04:01:50.714250: Validation loss did not improve from -0.37099. Patience: 70/50
2024-12-19 04:01:50.715072: train_loss -0.8511
2024-12-19 04:01:50.715900: val_loss -0.1802
2024-12-19 04:01:50.716835: Pseudo dice [0.6278]
2024-12-19 04:01:50.717553: Epoch time: 521.3 s
2024-12-19 04:01:52.293538: 
2024-12-19 04:01:52.295028: Epoch 79
2024-12-19 04:01:52.295913: Current learning rate: 0.0051
2024-12-19 04:10:09.416634: Validation loss did not improve from -0.37099. Patience: 71/50
2024-12-19 04:10:09.417536: train_loss -0.8516
2024-12-19 04:10:09.418302: val_loss -0.139
2024-12-19 04:10:09.419043: Pseudo dice [0.6111]
2024-12-19 04:10:09.419772: Epoch time: 497.13 s
2024-12-19 04:10:11.620213: 
2024-12-19 04:10:11.621394: Epoch 80
2024-12-19 04:10:11.622340: Current learning rate: 0.00504
2024-12-19 04:18:59.373339: Validation loss did not improve from -0.37099. Patience: 72/50
2024-12-19 04:18:59.374351: train_loss -0.8523
2024-12-19 04:18:59.375196: val_loss -0.2258
2024-12-19 04:18:59.375890: Pseudo dice [0.6566]
2024-12-19 04:18:59.376642: Epoch time: 527.76 s
2024-12-19 04:19:00.957671: 
2024-12-19 04:19:00.958961: Epoch 81
2024-12-19 04:19:00.959685: Current learning rate: 0.00497
2024-12-19 04:27:35.310789: Validation loss did not improve from -0.37099. Patience: 73/50
2024-12-19 04:27:35.311893: train_loss -0.8525
2024-12-19 04:27:35.312886: val_loss -0.2106
2024-12-19 04:27:35.313592: Pseudo dice [0.6415]
2024-12-19 04:27:35.314336: Epoch time: 514.36 s
2024-12-19 04:27:36.860639: 
2024-12-19 04:27:36.861984: Epoch 82
2024-12-19 04:27:36.862944: Current learning rate: 0.00491
2024-12-19 04:35:20.576426: Validation loss did not improve from -0.37099. Patience: 74/50
2024-12-19 04:35:20.577683: train_loss -0.8528
2024-12-19 04:35:20.578826: val_loss -0.1943
2024-12-19 04:35:20.579623: Pseudo dice [0.6321]
2024-12-19 04:35:20.580382: Epoch time: 463.72 s
2024-12-19 04:35:22.020003: 
2024-12-19 04:35:22.021391: Epoch 83
2024-12-19 04:35:22.022424: Current learning rate: 0.00484
2024-12-19 04:42:40.915017: Validation loss did not improve from -0.37099. Patience: 75/50
2024-12-19 04:42:40.915794: train_loss -0.8533
2024-12-19 04:42:40.916496: val_loss -0.2338
2024-12-19 04:42:40.917126: Pseudo dice [0.6581]
2024-12-19 04:42:40.917765: Epoch time: 438.9 s
2024-12-19 04:42:42.721278: 
2024-12-19 04:42:42.722236: Epoch 84
2024-12-19 04:42:42.723079: Current learning rate: 0.00478
2024-12-19 04:50:01.128331: Validation loss did not improve from -0.37099. Patience: 76/50
2024-12-19 04:50:01.129245: train_loss -0.8555
2024-12-19 04:50:01.130162: val_loss -0.224
2024-12-19 04:50:01.130930: Pseudo dice [0.6448]
2024-12-19 04:50:01.131557: Epoch time: 438.41 s
2024-12-19 04:50:03.514027: 
2024-12-19 04:50:03.515132: Epoch 85
2024-12-19 04:50:03.515848: Current learning rate: 0.00471
2024-12-19 04:56:49.826185: Validation loss did not improve from -0.37099. Patience: 77/50
2024-12-19 04:56:49.827066: train_loss -0.8576
2024-12-19 04:56:49.827810: val_loss -0.1876
2024-12-19 04:56:49.828469: Pseudo dice [0.6434]
2024-12-19 04:56:49.829097: Epoch time: 406.31 s
2024-12-19 04:56:51.289845: 
2024-12-19 04:56:51.291098: Epoch 86
2024-12-19 04:56:51.291754: Current learning rate: 0.00465
2024-12-19 05:03:50.840721: Validation loss did not improve from -0.37099. Patience: 78/50
2024-12-19 05:03:50.841839: train_loss -0.8558
2024-12-19 05:03:50.842767: val_loss -0.1692
2024-12-19 05:03:50.843411: Pseudo dice [0.6234]
2024-12-19 05:03:50.844148: Epoch time: 419.55 s
2024-12-19 05:03:52.328609: 
2024-12-19 05:03:52.329668: Epoch 87
2024-12-19 05:03:52.330406: Current learning rate: 0.00458
2024-12-19 05:11:43.374544: Validation loss did not improve from -0.37099. Patience: 79/50
2024-12-19 05:11:43.378261: train_loss -0.8568
2024-12-19 05:11:43.379191: val_loss -0.1518
2024-12-19 05:11:43.379883: Pseudo dice [0.6173]
2024-12-19 05:11:43.380700: Epoch time: 471.05 s
2024-12-19 05:11:44.834907: 
2024-12-19 05:11:44.836132: Epoch 88
2024-12-19 05:11:44.836836: Current learning rate: 0.00452
2024-12-19 05:19:12.386884: Validation loss did not improve from -0.37099. Patience: 80/50
2024-12-19 05:19:12.387823: train_loss -0.8556
2024-12-19 05:19:12.388969: val_loss -0.2149
2024-12-19 05:19:12.389812: Pseudo dice [0.643]
2024-12-19 05:19:12.390677: Epoch time: 447.55 s
2024-12-19 05:19:13.808185: 
2024-12-19 05:19:13.809614: Epoch 89
2024-12-19 05:19:13.810390: Current learning rate: 0.00445
2024-12-19 05:27:11.462827: Validation loss did not improve from -0.37099. Patience: 81/50
2024-12-19 05:27:11.463608: train_loss -0.858
2024-12-19 05:27:11.464455: val_loss -0.1819
2024-12-19 05:27:11.465297: Pseudo dice [0.6381]
2024-12-19 05:27:11.466105: Epoch time: 477.66 s
2024-12-19 05:27:13.269954: 
2024-12-19 05:27:13.271044: Epoch 90
2024-12-19 05:27:13.271971: Current learning rate: 0.00438
2024-12-19 05:35:51.121647: Validation loss did not improve from -0.37099. Patience: 82/50
2024-12-19 05:35:51.124449: train_loss -0.8572
2024-12-19 05:35:51.125407: val_loss -0.2262
2024-12-19 05:35:51.126239: Pseudo dice [0.6499]
2024-12-19 05:35:51.126961: Epoch time: 517.86 s
2024-12-19 05:35:52.518703: 
2024-12-19 05:35:52.519998: Epoch 91
2024-12-19 05:35:52.520746: Current learning rate: 0.00432
2024-12-19 05:45:13.398947: Validation loss did not improve from -0.37099. Patience: 83/50
2024-12-19 05:45:13.400016: train_loss -0.8586
2024-12-19 05:45:13.400870: val_loss -0.1491
2024-12-19 05:45:13.401903: Pseudo dice [0.6196]
2024-12-19 05:45:13.402780: Epoch time: 560.88 s
2024-12-19 05:45:14.763834: 
2024-12-19 05:45:14.764872: Epoch 92
2024-12-19 05:45:14.765717: Current learning rate: 0.00425
2024-12-19 05:55:07.406297: Validation loss did not improve from -0.37099. Patience: 84/50
2024-12-19 05:55:07.407094: train_loss -0.8596
2024-12-19 05:55:07.408067: val_loss -0.2088
2024-12-19 05:55:07.408864: Pseudo dice [0.6406]
2024-12-19 05:55:07.409583: Epoch time: 592.64 s
2024-12-19 05:55:08.770727: 
2024-12-19 05:55:08.771905: Epoch 93
2024-12-19 05:55:08.772553: Current learning rate: 0.00419
2024-12-19 06:05:07.436593: Validation loss did not improve from -0.37099. Patience: 85/50
2024-12-19 06:05:07.437251: train_loss -0.8619
2024-12-19 06:05:07.437866: val_loss -0.2058
2024-12-19 06:05:07.438486: Pseudo dice [0.6339]
2024-12-19 06:05:07.439045: Epoch time: 598.67 s
2024-12-19 06:05:08.773547: 
2024-12-19 06:05:08.774767: Epoch 94
2024-12-19 06:05:08.775464: Current learning rate: 0.00412
2024-12-19 06:15:18.908518: Validation loss did not improve from -0.37099. Patience: 86/50
2024-12-19 06:15:18.909544: train_loss -0.8618
2024-12-19 06:15:18.910264: val_loss -0.2034
2024-12-19 06:15:18.910872: Pseudo dice [0.6304]
2024-12-19 06:15:18.911941: Epoch time: 610.14 s
2024-12-19 06:15:21.164149: 
2024-12-19 06:15:21.165662: Epoch 95
2024-12-19 06:15:21.166504: Current learning rate: 0.00405
2024-12-19 06:24:59.478482: Validation loss did not improve from -0.37099. Patience: 87/50
2024-12-19 06:24:59.479277: train_loss -0.8623
2024-12-19 06:24:59.480445: val_loss -0.2085
2024-12-19 06:24:59.481371: Pseudo dice [0.6414]
2024-12-19 06:24:59.482176: Epoch time: 578.32 s
2024-12-19 06:25:00.820283: 
2024-12-19 06:25:00.821793: Epoch 96
2024-12-19 06:25:00.822821: Current learning rate: 0.00399
2024-12-19 06:34:23.000072: Validation loss did not improve from -0.37099. Patience: 88/50
2024-12-19 06:34:23.001080: train_loss -0.8632
2024-12-19 06:34:23.001993: val_loss -0.1092
2024-12-19 06:34:23.002578: Pseudo dice [0.6116]
2024-12-19 06:34:23.003190: Epoch time: 562.18 s
2024-12-19 06:34:24.383467: 
2024-12-19 06:34:24.384501: Epoch 97
2024-12-19 06:34:24.385085: Current learning rate: 0.00392
2024-12-19 06:42:09.717741: Validation loss did not improve from -0.37099. Patience: 89/50
2024-12-19 06:42:09.718955: train_loss -0.8633
2024-12-19 06:42:09.719646: val_loss -0.2104
2024-12-19 06:42:09.720285: Pseudo dice [0.6366]
2024-12-19 06:42:09.720812: Epoch time: 465.34 s
2024-12-19 06:42:11.098562: 
2024-12-19 06:42:11.099920: Epoch 98
2024-12-19 06:42:11.100688: Current learning rate: 0.00385
2024-12-19 06:48:23.288252: Validation loss did not improve from -0.37099. Patience: 90/50
2024-12-19 06:48:23.291018: train_loss -0.8645
2024-12-19 06:48:23.292782: val_loss -0.1732
2024-12-19 06:48:23.293453: Pseudo dice [0.6192]
2024-12-19 06:48:23.294262: Epoch time: 372.19 s
2024-12-19 06:48:24.699247: 
2024-12-19 06:48:24.700462: Epoch 99
2024-12-19 06:48:24.701175: Current learning rate: 0.00379
2024-12-19 06:54:31.874660: Validation loss did not improve from -0.37099. Patience: 91/50
2024-12-19 06:54:31.876164: train_loss -0.8655
2024-12-19 06:54:31.877091: val_loss -0.2197
2024-12-19 06:54:31.877885: Pseudo dice [0.6502]
2024-12-19 06:54:31.878619: Epoch time: 367.18 s
2024-12-19 06:54:33.696739: 
2024-12-19 06:54:33.697818: Epoch 100
2024-12-19 06:54:33.698648: Current learning rate: 0.00372
2024-12-19 07:00:12.840492: Validation loss did not improve from -0.37099. Patience: 92/50
2024-12-19 07:00:12.841848: train_loss -0.8656
2024-12-19 07:00:12.843137: val_loss -0.2266
2024-12-19 07:00:12.844195: Pseudo dice [0.6525]
2024-12-19 07:00:12.845144: Epoch time: 339.15 s
2024-12-19 07:00:14.240482: 
2024-12-19 07:00:14.241615: Epoch 101
2024-12-19 07:00:14.242408: Current learning rate: 0.00365
2024-12-19 07:05:50.277442: Validation loss did not improve from -0.37099. Patience: 93/50
2024-12-19 07:05:50.278356: train_loss -0.8662
2024-12-19 07:05:50.279241: val_loss -0.2204
2024-12-19 07:05:50.279904: Pseudo dice [0.6422]
2024-12-19 07:05:50.280643: Epoch time: 336.04 s
2024-12-19 07:05:51.671030: 
2024-12-19 07:05:51.672487: Epoch 102
2024-12-19 07:05:51.673805: Current learning rate: 0.00359
2024-12-19 07:11:48.265619: Validation loss did not improve from -0.37099. Patience: 94/50
2024-12-19 07:11:48.266543: train_loss -0.8659
2024-12-19 07:11:48.267691: val_loss -0.2262
2024-12-19 07:11:48.268409: Pseudo dice [0.6521]
2024-12-19 07:11:48.269205: Epoch time: 356.6 s
2024-12-19 07:11:49.674013: 
2024-12-19 07:11:49.675050: Epoch 103
2024-12-19 07:11:49.675689: Current learning rate: 0.00352
2024-12-19 07:16:57.412900: Validation loss did not improve from -0.37099. Patience: 95/50
2024-12-19 07:16:57.413887: train_loss -0.8684
2024-12-19 07:16:57.414663: val_loss -0.1595
2024-12-19 07:16:57.415575: Pseudo dice [0.6249]
2024-12-19 07:16:57.416410: Epoch time: 307.74 s
2024-12-19 07:16:58.826299: 
2024-12-19 07:16:58.827867: Epoch 104
2024-12-19 07:16:58.828789: Current learning rate: 0.00345
2024-12-19 07:22:51.776669: Validation loss did not improve from -0.37099. Patience: 96/50
2024-12-19 07:22:51.777495: train_loss -0.8685
2024-12-19 07:22:51.778325: val_loss -0.1915
2024-12-19 07:22:51.779153: Pseudo dice [0.6356]
2024-12-19 07:22:51.779989: Epoch time: 352.95 s
2024-12-19 07:22:53.705581: 
2024-12-19 07:22:53.706576: Epoch 105
2024-12-19 07:22:53.707321: Current learning rate: 0.00338
2024-12-19 07:28:30.234595: Validation loss did not improve from -0.37099. Patience: 97/50
2024-12-19 07:28:30.235473: train_loss -0.869
2024-12-19 07:28:30.236331: val_loss -0.0821
2024-12-19 07:28:30.237360: Pseudo dice [0.6079]
2024-12-19 07:28:30.238402: Epoch time: 336.53 s
2024-12-19 07:28:32.333470: 
2024-12-19 07:28:32.334944: Epoch 106
2024-12-19 07:28:32.335867: Current learning rate: 0.00332
2024-12-19 07:34:04.783819: Validation loss did not improve from -0.37099. Patience: 98/50
2024-12-19 07:34:04.784610: train_loss -0.8686
2024-12-19 07:34:04.785361: val_loss -0.1901
2024-12-19 07:34:04.785992: Pseudo dice [0.6402]
2024-12-19 07:34:04.786752: Epoch time: 332.45 s
2024-12-19 07:34:06.179344: 
2024-12-19 07:34:06.180376: Epoch 107
2024-12-19 07:34:06.181112: Current learning rate: 0.00325
2024-12-19 07:40:12.061269: Validation loss did not improve from -0.37099. Patience: 99/50
2024-12-19 07:40:12.076307: train_loss -0.8707
2024-12-19 07:40:12.077948: val_loss -0.2237
2024-12-19 07:40:12.078854: Pseudo dice [0.6484]
2024-12-19 07:40:12.079656: Epoch time: 365.9 s
2024-12-19 07:40:13.583908: 
2024-12-19 07:40:13.585048: Epoch 108
2024-12-19 07:40:13.585922: Current learning rate: 0.00318
2024-12-19 07:45:45.734462: Validation loss did not improve from -0.37099. Patience: 100/50
2024-12-19 07:45:45.735300: train_loss -0.8702
2024-12-19 07:45:45.736089: val_loss -0.1248
2024-12-19 07:45:45.736811: Pseudo dice [0.6092]
2024-12-19 07:45:45.737498: Epoch time: 332.15 s
2024-12-19 07:45:47.131802: 
2024-12-19 07:45:47.133132: Epoch 109
2024-12-19 07:45:47.134090: Current learning rate: 0.00311
2024-12-19 07:51:11.256261: Validation loss did not improve from -0.37099. Patience: 101/50
2024-12-19 07:51:11.257119: train_loss -0.872
2024-12-19 07:51:11.257800: val_loss -0.1874
2024-12-19 07:51:11.258410: Pseudo dice [0.6306]
2024-12-19 07:51:11.259057: Epoch time: 324.13 s
2024-12-19 07:51:13.119259: 
2024-12-19 07:51:13.120769: Epoch 110
2024-12-19 07:51:13.121418: Current learning rate: 0.00304
2024-12-19 07:56:56.222002: Validation loss did not improve from -0.37099. Patience: 102/50
2024-12-19 07:56:56.222676: train_loss -0.873
2024-12-19 07:56:56.223466: val_loss -0.1822
2024-12-19 07:56:56.224135: Pseudo dice [0.6418]
2024-12-19 07:56:56.224827: Epoch time: 343.1 s
2024-12-19 07:56:57.635671: 
2024-12-19 07:56:57.636775: Epoch 111
2024-12-19 07:56:57.637457: Current learning rate: 0.00297
2024-12-19 08:01:49.171269: Validation loss did not improve from -0.37099. Patience: 103/50
2024-12-19 08:01:49.172172: train_loss -0.8717
2024-12-19 08:01:49.172884: val_loss -0.2184
2024-12-19 08:01:49.173513: Pseudo dice [0.6558]
2024-12-19 08:01:49.174089: Epoch time: 291.54 s
2024-12-19 08:01:50.555433: 
2024-12-19 08:01:50.556583: Epoch 112
2024-12-19 08:01:50.557182: Current learning rate: 0.00291
2024-12-19 08:07:28.598684: Validation loss did not improve from -0.37099. Patience: 104/50
2024-12-19 08:07:28.599642: train_loss -0.8714
2024-12-19 08:07:28.600481: val_loss -0.1639
2024-12-19 08:07:28.601050: Pseudo dice [0.6235]
2024-12-19 08:07:28.601679: Epoch time: 338.05 s
2024-12-19 08:07:29.988680: 
2024-12-19 08:07:29.990006: Epoch 113
2024-12-19 08:07:29.990741: Current learning rate: 0.00284
2024-12-19 08:13:14.107066: Validation loss did not improve from -0.37099. Patience: 105/50
2024-12-19 08:13:14.107929: train_loss -0.8732
2024-12-19 08:13:14.108657: val_loss -0.1778
2024-12-19 08:13:14.109316: Pseudo dice [0.6213]
2024-12-19 08:13:14.110053: Epoch time: 344.12 s
2024-12-19 08:13:15.506278: 
2024-12-19 08:13:15.507716: Epoch 114
2024-12-19 08:13:15.508679: Current learning rate: 0.00277
2024-12-19 08:19:01.920691: Validation loss did not improve from -0.37099. Patience: 106/50
2024-12-19 08:19:01.921701: train_loss -0.8725
2024-12-19 08:19:01.922379: val_loss -0.1691
2024-12-19 08:19:01.922985: Pseudo dice [0.6258]
2024-12-19 08:19:01.923960: Epoch time: 346.42 s
2024-12-19 08:19:03.870165: 
2024-12-19 08:19:03.871523: Epoch 115
2024-12-19 08:19:03.872240: Current learning rate: 0.0027
2024-12-19 08:24:31.753082: Validation loss did not improve from -0.37099. Patience: 107/50
2024-12-19 08:24:31.754109: train_loss -0.8737
2024-12-19 08:24:31.754991: val_loss -0.2177
2024-12-19 08:24:31.755655: Pseudo dice [0.6472]
2024-12-19 08:24:31.756465: Epoch time: 327.89 s
2024-12-19 08:24:33.208013: 
2024-12-19 08:24:33.209288: Epoch 116
2024-12-19 08:24:33.210059: Current learning rate: 0.00263
2024-12-19 08:29:43.159686: Validation loss did not improve from -0.37099. Patience: 108/50
2024-12-19 08:29:43.160688: train_loss -0.8741
2024-12-19 08:29:43.161380: val_loss -0.1729
2024-12-19 08:29:43.162104: Pseudo dice [0.6363]
2024-12-19 08:29:43.162696: Epoch time: 309.95 s
2024-12-19 08:29:45.411648: 
2024-12-19 08:29:45.412777: Epoch 117
2024-12-19 08:29:45.413553: Current learning rate: 0.00256
2024-12-19 08:35:30.598405: Validation loss did not improve from -0.37099. Patience: 109/50
2024-12-19 08:35:30.600335: train_loss -0.8755
2024-12-19 08:35:30.601404: val_loss -0.2091
2024-12-19 08:35:30.602173: Pseudo dice [0.635]
2024-12-19 08:35:30.602781: Epoch time: 345.19 s
2024-12-19 08:35:32.028558: 
2024-12-19 08:35:32.030122: Epoch 118
2024-12-19 08:35:32.030987: Current learning rate: 0.00249
2024-12-19 08:40:36.104804: Validation loss did not improve from -0.37099. Patience: 110/50
2024-12-19 08:40:36.106848: train_loss -0.8748
2024-12-19 08:40:36.108493: val_loss -0.1833
2024-12-19 08:40:36.109169: Pseudo dice [0.6292]
2024-12-19 08:40:36.110131: Epoch time: 304.08 s
2024-12-19 08:40:37.554209: 
2024-12-19 08:40:37.555656: Epoch 119
2024-12-19 08:40:37.556726: Current learning rate: 0.00242
2024-12-19 08:46:16.422524: Validation loss did not improve from -0.37099. Patience: 111/50
2024-12-19 08:46:16.423631: train_loss -0.8765
2024-12-19 08:46:16.424365: val_loss -0.1428
2024-12-19 08:46:16.425043: Pseudo dice [0.6264]
2024-12-19 08:46:16.425696: Epoch time: 338.87 s
2024-12-19 08:46:18.334958: 
2024-12-19 08:46:18.336644: Epoch 120
2024-12-19 08:46:18.337469: Current learning rate: 0.00235
2024-12-19 08:52:22.395142: Validation loss did not improve from -0.37099. Patience: 112/50
2024-12-19 08:52:22.396348: train_loss -0.8763
2024-12-19 08:52:22.397448: val_loss -0.1622
2024-12-19 08:52:22.398345: Pseudo dice [0.6424]
2024-12-19 08:52:22.398954: Epoch time: 364.06 s
2024-12-19 08:52:23.900553: 
2024-12-19 08:52:23.901778: Epoch 121
2024-12-19 08:52:23.902718: Current learning rate: 0.00228
2024-12-19 08:57:47.028672: Validation loss did not improve from -0.37099. Patience: 113/50
2024-12-19 08:57:47.029436: train_loss -0.8764
2024-12-19 08:57:47.030109: val_loss -0.1324
2024-12-19 08:57:47.030703: Pseudo dice [0.6273]
2024-12-19 08:57:47.031343: Epoch time: 323.13 s
2024-12-19 08:57:48.475931: 
2024-12-19 08:57:48.477019: Epoch 122
2024-12-19 08:57:48.477664: Current learning rate: 0.00221
2024-12-19 09:02:57.114762: Validation loss did not improve from -0.37099. Patience: 114/50
2024-12-19 09:02:57.115633: train_loss -0.8785
2024-12-19 09:02:57.116296: val_loss -0.1734
2024-12-19 09:02:57.117001: Pseudo dice [0.6423]
2024-12-19 09:02:57.117653: Epoch time: 308.64 s
2024-12-19 09:02:58.527857: 
2024-12-19 09:02:58.528623: Epoch 123
2024-12-19 09:02:58.529258: Current learning rate: 0.00214
2024-12-19 09:08:09.386645: Validation loss did not improve from -0.37099. Patience: 115/50
2024-12-19 09:08:09.387541: train_loss -0.8778
2024-12-19 09:08:09.388347: val_loss -0.1852
2024-12-19 09:08:09.389016: Pseudo dice [0.6366]
2024-12-19 09:08:09.389707: Epoch time: 310.86 s
2024-12-19 09:08:10.815996: 
2024-12-19 09:08:10.817134: Epoch 124
2024-12-19 09:08:10.817951: Current learning rate: 0.00207
2024-12-19 09:13:17.406265: Validation loss did not improve from -0.37099. Patience: 116/50
2024-12-19 09:13:17.407109: train_loss -0.8771
2024-12-19 09:13:17.407911: val_loss -0.1851
2024-12-19 09:13:17.408597: Pseudo dice [0.6373]
2024-12-19 09:13:17.409398: Epoch time: 306.59 s
2024-12-19 09:13:19.236800: 
2024-12-19 09:13:19.238108: Epoch 125
2024-12-19 09:13:19.238902: Current learning rate: 0.00199
2024-12-19 09:18:05.940996: Validation loss did not improve from -0.37099. Patience: 117/50
2024-12-19 09:18:05.941803: train_loss -0.8799
2024-12-19 09:18:05.942685: val_loss -0.125
2024-12-19 09:18:05.944055: Pseudo dice [0.618]
2024-12-19 09:18:05.944703: Epoch time: 286.71 s
2024-12-19 09:18:07.354670: 
2024-12-19 09:18:07.355797: Epoch 126
2024-12-19 09:18:07.356497: Current learning rate: 0.00192
2024-12-19 09:23:06.949051: Validation loss did not improve from -0.37099. Patience: 118/50
2024-12-19 09:23:06.950136: train_loss -0.8794
2024-12-19 09:23:06.951589: val_loss -0.1484
2024-12-19 09:23:06.952446: Pseudo dice [0.6335]
2024-12-19 09:23:06.953404: Epoch time: 299.6 s
2024-12-19 09:23:08.976207: 
2024-12-19 09:23:08.977324: Epoch 127
2024-12-19 09:23:08.978088: Current learning rate: 0.00185
2024-12-19 09:28:05.190767: Validation loss did not improve from -0.37099. Patience: 119/50
2024-12-19 09:28:05.191767: train_loss -0.8798
2024-12-19 09:28:05.192524: val_loss -0.1399
2024-12-19 09:28:05.193228: Pseudo dice [0.6368]
2024-12-19 09:28:05.193922: Epoch time: 296.22 s
2024-12-19 09:28:06.627808: 
2024-12-19 09:28:06.628869: Epoch 128
2024-12-19 09:28:06.629573: Current learning rate: 0.00178
2024-12-19 09:33:15.411214: Validation loss did not improve from -0.37099. Patience: 120/50
2024-12-19 09:33:15.412120: train_loss -0.8807
2024-12-19 09:33:15.412867: val_loss -0.1492
2024-12-19 09:33:15.413465: Pseudo dice [0.6308]
2024-12-19 09:33:15.414515: Epoch time: 308.79 s
2024-12-19 09:33:16.877571: 
2024-12-19 09:33:16.878853: Epoch 129
2024-12-19 09:33:16.879467: Current learning rate: 0.0017
2024-12-19 09:38:04.666170: Validation loss did not improve from -0.37099. Patience: 121/50
2024-12-19 09:38:04.666816: train_loss -0.8814
2024-12-19 09:38:04.667457: val_loss -0.1434
2024-12-19 09:38:04.668211: Pseudo dice [0.6366]
2024-12-19 09:38:04.668902: Epoch time: 287.79 s
2024-12-19 09:38:06.476675: 
2024-12-19 09:38:06.478368: Epoch 130
2024-12-19 09:38:06.479150: Current learning rate: 0.00163
2024-12-19 09:43:21.876662: Validation loss did not improve from -0.37099. Patience: 122/50
2024-12-19 09:43:21.914499: train_loss -0.8817
2024-12-19 09:43:21.915812: val_loss -0.1408
2024-12-19 09:43:21.916696: Pseudo dice [0.638]
2024-12-19 09:43:21.917503: Epoch time: 315.43 s
2024-12-19 09:43:23.338604: 
2024-12-19 09:43:23.339934: Epoch 131
2024-12-19 09:43:23.340727: Current learning rate: 0.00156
2024-12-19 09:48:17.289521: Validation loss did not improve from -0.37099. Patience: 123/50
2024-12-19 09:48:17.291522: train_loss -0.8824
2024-12-19 09:48:17.293502: val_loss -0.1915
2024-12-19 09:48:17.294739: Pseudo dice [0.6412]
2024-12-19 09:48:17.296149: Epoch time: 293.95 s
2024-12-19 09:48:18.882921: 
2024-12-19 09:48:18.884800: Epoch 132
2024-12-19 09:48:18.885866: Current learning rate: 0.00148
2024-12-19 09:53:20.429438: Validation loss did not improve from -0.37099. Patience: 124/50
2024-12-19 09:53:20.430555: train_loss -0.8799
2024-12-19 09:53:20.431359: val_loss -0.1473
2024-12-19 09:53:20.432107: Pseudo dice [0.6244]
2024-12-19 09:53:20.432840: Epoch time: 301.55 s
2024-12-19 09:53:21.916978: 
2024-12-19 09:53:21.918359: Epoch 133
2024-12-19 09:53:21.919197: Current learning rate: 0.00141
2024-12-19 09:58:14.031038: Validation loss did not improve from -0.37099. Patience: 125/50
2024-12-19 09:58:14.031940: train_loss -0.8824
2024-12-19 09:58:14.032692: val_loss -0.1763
2024-12-19 09:58:14.033725: Pseudo dice [0.6466]
2024-12-19 09:58:14.034548: Epoch time: 292.12 s
2024-12-19 09:58:15.490557: 
2024-12-19 09:58:15.491577: Epoch 134
2024-12-19 09:58:15.492625: Current learning rate: 0.00133
2024-12-19 10:03:24.035364: Validation loss did not improve from -0.37099. Patience: 126/50
2024-12-19 10:03:24.036161: train_loss -0.8828
2024-12-19 10:03:24.036955: val_loss -0.1191
2024-12-19 10:03:24.037609: Pseudo dice [0.6122]
2024-12-19 10:03:24.038218: Epoch time: 308.55 s
2024-12-19 10:03:26.006384: 
2024-12-19 10:03:26.007522: Epoch 135
2024-12-19 10:03:26.008214: Current learning rate: 0.00126
2024-12-19 10:08:09.374430: Validation loss did not improve from -0.37099. Patience: 127/50
2024-12-19 10:08:09.375333: train_loss -0.8851
2024-12-19 10:08:09.376110: val_loss -0.1752
2024-12-19 10:08:09.376764: Pseudo dice [0.6377]
2024-12-19 10:08:09.377568: Epoch time: 283.37 s
2024-12-19 10:08:10.819906: 
2024-12-19 10:08:10.820992: Epoch 136
2024-12-19 10:08:10.821850: Current learning rate: 0.00118
2024-12-19 10:13:12.714485: Validation loss did not improve from -0.37099. Patience: 128/50
2024-12-19 10:13:12.715521: train_loss -0.8844
2024-12-19 10:13:12.716408: val_loss -0.1572
2024-12-19 10:13:12.717438: Pseudo dice [0.6275]
2024-12-19 10:13:12.718195: Epoch time: 301.9 s
2024-12-19 10:13:14.166732: 
2024-12-19 10:13:14.168211: Epoch 137
2024-12-19 10:13:14.170114: Current learning rate: 0.00111
2024-12-19 10:18:20.023662: Validation loss did not improve from -0.37099. Patience: 129/50
2024-12-19 10:18:20.025858: train_loss -0.885
2024-12-19 10:18:20.027304: val_loss -0.1644
2024-12-19 10:18:20.028600: Pseudo dice [0.6483]
2024-12-19 10:18:20.029843: Epoch time: 305.86 s
2024-12-19 10:18:22.365198: 
2024-12-19 10:18:22.367640: Epoch 138
2024-12-19 10:18:22.368868: Current learning rate: 0.00103
2024-12-19 10:23:19.549147: Validation loss did not improve from -0.37099. Patience: 130/50
2024-12-19 10:23:19.550765: train_loss -0.8859
2024-12-19 10:23:19.551440: val_loss -0.146
2024-12-19 10:23:19.552016: Pseudo dice [0.6256]
2024-12-19 10:23:19.552714: Epoch time: 297.19 s
2024-12-19 10:23:21.060735: 
2024-12-19 10:23:21.061936: Epoch 139
2024-12-19 10:23:21.062617: Current learning rate: 0.00095
2024-12-19 10:28:01.735642: Validation loss did not improve from -0.37099. Patience: 131/50
2024-12-19 10:28:01.737267: train_loss -0.8855
2024-12-19 10:28:01.738049: val_loss -0.1681
2024-12-19 10:28:01.738893: Pseudo dice [0.6214]
2024-12-19 10:28:01.739686: Epoch time: 280.68 s
2024-12-19 10:28:03.750444: 
2024-12-19 10:28:03.751620: Epoch 140
2024-12-19 10:28:03.752414: Current learning rate: 0.00087
2024-12-19 10:34:30.786752: Validation loss did not improve from -0.37099. Patience: 132/50
2024-12-19 10:34:30.787563: train_loss -0.8876
2024-12-19 10:34:30.788348: val_loss -0.1295
2024-12-19 10:34:30.789008: Pseudo dice [0.6174]
2024-12-19 10:34:30.789735: Epoch time: 387.04 s
2024-12-19 10:34:32.268975: 
2024-12-19 10:34:32.270637: Epoch 141
2024-12-19 10:34:32.272012: Current learning rate: 0.00079
2024-12-19 10:38:15.497511: Validation loss did not improve from -0.37099. Patience: 133/50
2024-12-19 10:38:15.498480: train_loss -0.8857
2024-12-19 10:38:15.499200: val_loss -0.1558
2024-12-19 10:38:15.499964: Pseudo dice [0.6277]
2024-12-19 10:38:15.500687: Epoch time: 223.23 s
2024-12-19 10:38:16.955622: 
2024-12-19 10:38:16.957359: Epoch 142
2024-12-19 10:38:16.958129: Current learning rate: 0.00071
2024-12-19 10:40:14.695577: Validation loss did not improve from -0.37099. Patience: 134/50
2024-12-19 10:40:14.696847: train_loss -0.8846
2024-12-19 10:40:14.697818: val_loss -0.1785
2024-12-19 10:40:14.698880: Pseudo dice [0.633]
2024-12-19 10:40:14.699718: Epoch time: 117.74 s
2024-12-19 10:40:16.107349: 
2024-12-19 10:40:16.108843: Epoch 143
2024-12-19 10:40:16.109837: Current learning rate: 0.00063
2024-12-19 10:42:12.914060: Validation loss did not improve from -0.37099. Patience: 135/50
2024-12-19 10:42:12.914985: train_loss -0.8858
2024-12-19 10:42:12.915857: val_loss -0.1352
2024-12-19 10:42:12.916789: Pseudo dice [0.6277]
2024-12-19 10:42:12.917454: Epoch time: 116.81 s
2024-12-19 10:42:14.379714: 
2024-12-19 10:42:14.393737: Epoch 144
2024-12-19 10:42:14.395523: Current learning rate: 0.00055
2024-12-19 10:44:06.444225: Validation loss did not improve from -0.37099. Patience: 136/50
2024-12-19 10:44:06.448659: train_loss -0.8867
2024-12-19 10:44:06.450023: val_loss -0.137
2024-12-19 10:44:06.450880: Pseudo dice [0.6201]
2024-12-19 10:44:06.451788: Epoch time: 112.07 s
2024-12-19 10:44:08.524090: 
2024-12-19 10:44:08.525338: Epoch 145
2024-12-19 10:44:08.526189: Current learning rate: 0.00047
2024-12-19 10:46:04.303403: Validation loss did not improve from -0.37099. Patience: 137/50
2024-12-19 10:46:04.304497: train_loss -0.8863
2024-12-19 10:46:04.305358: val_loss -0.1371
2024-12-19 10:46:04.306041: Pseudo dice [0.6191]
2024-12-19 10:46:04.306675: Epoch time: 115.78 s
2024-12-19 10:46:05.726514: 
2024-12-19 10:46:05.728396: Epoch 146
2024-12-19 10:46:05.729398: Current learning rate: 0.00038
2024-12-19 10:47:58.826043: Validation loss did not improve from -0.37099. Patience: 138/50
2024-12-19 10:47:58.826752: train_loss -0.8865
2024-12-19 10:47:58.827450: val_loss -0.1245
2024-12-19 10:47:58.828208: Pseudo dice [0.6317]
2024-12-19 10:47:58.828908: Epoch time: 113.1 s
2024-12-19 10:48:00.207485: 
2024-12-19 10:48:00.208870: Epoch 147
2024-12-19 10:48:00.209538: Current learning rate: 0.0003
2024-12-19 10:49:53.516891: Validation loss did not improve from -0.37099. Patience: 139/50
2024-12-19 10:49:53.517812: train_loss -0.8885
2024-12-19 10:49:53.519604: val_loss -0.1231
2024-12-19 10:49:53.520386: Pseudo dice [0.6268]
2024-12-19 10:49:53.521328: Epoch time: 113.31 s
2024-12-19 10:49:56.043563: 
2024-12-19 10:49:56.044979: Epoch 148
2024-12-19 10:49:56.045635: Current learning rate: 0.00021
2024-12-19 10:51:46.328205: Validation loss did not improve from -0.37099. Patience: 140/50
2024-12-19 10:51:46.329268: train_loss -0.8881
2024-12-19 10:51:46.330231: val_loss -0.1333
2024-12-19 10:51:46.330911: Pseudo dice [0.6316]
2024-12-19 10:51:46.331573: Epoch time: 110.29 s
2024-12-19 10:51:47.794152: 
2024-12-19 10:51:47.795063: Epoch 149
2024-12-19 10:51:47.795875: Current learning rate: 0.00011
2024-12-19 10:53:41.309014: Validation loss did not improve from -0.37099. Patience: 141/50
2024-12-19 10:53:41.310400: train_loss -0.8882
2024-12-19 10:53:41.311560: val_loss -0.1438
2024-12-19 10:53:41.312470: Pseudo dice [0.6318]
2024-12-19 10:53:41.313428: Epoch time: 113.52 s
2024-12-19 10:53:43.264741: Training done.
2024-12-19 10:53:43.652130: Using splits from existing split file: /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset309_Sohee_Calcium_OCT_CrossValidation/splits_final_20.json
2024-12-19 10:53:43.664876: The split file contains 5 splits.
2024-12-19 10:53:43.665650: Desired fold for training: 3
2024-12-19 10:53:43.666388: This split has 1 training and 7 validation cases.
2024-12-19 10:53:43.668011: predicting 101-044
2024-12-19 10:53:43.753830: 101-044, shape torch.Size([1, 404, 498, 498]), rank 0
2024-12-19 10:55:38.716447: predicting 101-045
2024-12-19 10:55:38.807515: 101-045, shape torch.Size([1, 375, 498, 498]), rank 0
2024-12-19 10:57:13.189712: predicting 106-002
2024-12-19 10:57:13.237572: 106-002, shape torch.Size([1, 539, 498, 498]), rank 0
2024-12-19 11:02:16.407891: predicting 401-004
2024-12-19 11:02:16.425210: 401-004, shape torch.Size([1, 375, 498, 498]), rank 0
2024-12-19 11:04:36.615350: predicting 701-013
2024-12-19 11:04:36.640708: 701-013, shape torch.Size([1, 375, 498, 498]), rank 0
2024-12-19 11:08:05.454494: predicting 704-003
2024-12-19 11:08:05.940039: 704-003, shape torch.Size([1, 375, 498, 498]), rank 0
2024-12-19 11:10:25.183522: predicting 706-005
2024-12-19 11:10:25.199590: 706-005, shape torch.Size([1, 375, 498, 498]), rank 0
2024-12-19 11:13:28.370733: Validation complete
2024-12-19 11:13:28.371608: Mean Validation Dice:  0.6178931949254995

############################
INFO: You are using the old nnU-Net default plans. We have updated our recommendations. Please consider using those instead! Read more here: https://github.com/MIC-DKFZ/nnUNet/blob/master/documentation/resenc_presets.md
############################

Using device: cuda:0

#######################################################################
Please cite the following paper when using nnU-Net:
Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.
#######################################################################

2024-12-19 11:13:37.493698: Using torch.compile...
/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
################### Loading pretrained weights from file  /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_results/Dataset300_Lumen_and_Wall_OCT/nnUNetTrainer__nnUNetPreprocessPlans__3d_fullres/fold_all/checkpoint_best.pth ###################
Below is the list of overlapping blocks in pretrained model and nnUNet architecture:
encoder.stages.0.0.convs.0.conv.weight shape torch.Size([32, 1, 3, 3, 3])
encoder.stages.0.0.convs.0.conv.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.norm.weight shape torch.Size([32])
encoder.stages.0.0.convs.0.norm.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.0.weight shape torch.Size([32, 1, 3, 3, 3])
encoder.stages.0.0.convs.0.all_modules.0.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.1.weight shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.1.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
encoder.stages.0.0.convs.1.conv.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.norm.weight shape torch.Size([32])
encoder.stages.0.0.convs.1.norm.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
encoder.stages.0.0.convs.1.all_modules.0.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.1.weight shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.1.bias shape torch.Size([32])
encoder.stages.1.0.convs.0.conv.weight shape torch.Size([64, 32, 3, 3, 3])
encoder.stages.1.0.convs.0.conv.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.norm.weight shape torch.Size([64])
encoder.stages.1.0.convs.0.norm.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.0.weight shape torch.Size([64, 32, 3, 3, 3])
encoder.stages.1.0.convs.0.all_modules.0.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.1.weight shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.1.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
encoder.stages.1.0.convs.1.conv.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.norm.weight shape torch.Size([64])
encoder.stages.1.0.convs.1.norm.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
encoder.stages.1.0.convs.1.all_modules.0.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.1.weight shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.1.bias shape torch.Size([64])
encoder.stages.2.0.convs.0.conv.weight shape torch.Size([128, 64, 3, 3, 3])
encoder.stages.2.0.convs.0.conv.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.norm.weight shape torch.Size([128])
encoder.stages.2.0.convs.0.norm.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.0.weight shape torch.Size([128, 64, 3, 3, 3])
encoder.stages.2.0.convs.0.all_modules.0.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.1.weight shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.1.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
encoder.stages.2.0.convs.1.conv.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.norm.weight shape torch.Size([128])
encoder.stages.2.0.convs.1.norm.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
encoder.stages.2.0.convs.1.all_modules.0.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.1.weight shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.1.bias shape torch.Size([128])
encoder.stages.3.0.convs.0.conv.weight shape torch.Size([256, 128, 3, 3, 3])
encoder.stages.3.0.convs.0.conv.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.norm.weight shape torch.Size([256])
encoder.stages.3.0.convs.0.norm.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.0.weight shape torch.Size([256, 128, 3, 3, 3])
encoder.stages.3.0.convs.0.all_modules.0.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.1.weight shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.1.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
encoder.stages.3.0.convs.1.conv.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.norm.weight shape torch.Size([256])
encoder.stages.3.0.convs.1.norm.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
encoder.stages.3.0.convs.1.all_modules.0.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.1.weight shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.1.bias shape torch.Size([256])
encoder.stages.4.0.convs.0.conv.weight shape torch.Size([320, 256, 3, 3, 3])
encoder.stages.4.0.convs.0.conv.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.norm.weight shape torch.Size([320])
encoder.stages.4.0.convs.0.norm.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.0.weight shape torch.Size([320, 256, 3, 3, 3])
encoder.stages.4.0.convs.0.all_modules.0.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.1.weight shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.1.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.4.0.convs.1.conv.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.norm.weight shape torch.Size([320])
encoder.stages.4.0.convs.1.norm.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.4.0.convs.1.all_modules.0.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.1.weight shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.1.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.0.conv.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.norm.weight shape torch.Size([320])
encoder.stages.5.0.convs.0.norm.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.0.all_modules.0.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.1.weight shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.1.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.1.conv.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.norm.weight shape torch.Size([320])
encoder.stages.5.0.convs.1.norm.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.1.all_modules.0.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.1.weight shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.0.0.convs.0.conv.weight shape torch.Size([32, 1, 3, 3, 3])
decoder.encoder.stages.0.0.convs.0.conv.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.norm.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.norm.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.0.weight shape torch.Size([32, 1, 3, 3, 3])
decoder.encoder.stages.0.0.convs.0.all_modules.0.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.1.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.1.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.encoder.stages.0.0.convs.1.conv.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.norm.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.norm.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.encoder.stages.0.0.convs.1.all_modules.0.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.1.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.1.bias shape torch.Size([32])
decoder.encoder.stages.1.0.convs.0.conv.weight shape torch.Size([64, 32, 3, 3, 3])
decoder.encoder.stages.1.0.convs.0.conv.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.norm.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.norm.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.0.weight shape torch.Size([64, 32, 3, 3, 3])
decoder.encoder.stages.1.0.convs.0.all_modules.0.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.1.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.1.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.encoder.stages.1.0.convs.1.conv.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.norm.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.norm.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.encoder.stages.1.0.convs.1.all_modules.0.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.1.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.1.bias shape torch.Size([64])
decoder.encoder.stages.2.0.convs.0.conv.weight shape torch.Size([128, 64, 3, 3, 3])
decoder.encoder.stages.2.0.convs.0.conv.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.norm.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.norm.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.0.weight shape torch.Size([128, 64, 3, 3, 3])
decoder.encoder.stages.2.0.convs.0.all_modules.0.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.1.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.1.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.encoder.stages.2.0.convs.1.conv.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.norm.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.norm.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.encoder.stages.2.0.convs.1.all_modules.0.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.1.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.1.bias shape torch.Size([128])
decoder.encoder.stages.3.0.convs.0.conv.weight shape torch.Size([256, 128, 3, 3, 3])
decoder.encoder.stages.3.0.convs.0.conv.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.norm.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.norm.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.0.weight shape torch.Size([256, 128, 3, 3, 3])
decoder.encoder.stages.3.0.convs.0.all_modules.0.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.1.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.1.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.encoder.stages.3.0.convs.1.conv.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.norm.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.norm.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.encoder.stages.3.0.convs.1.all_modules.0.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.1.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.1.bias shape torch.Size([256])
decoder.encoder.stages.4.0.convs.0.conv.weight shape torch.Size([320, 256, 3, 3, 3])
decoder.encoder.stages.4.0.convs.0.conv.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.norm.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.norm.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.0.weight shape torch.Size([320, 256, 3, 3, 3])
decoder.encoder.stages.4.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.4.0.convs.1.conv.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.norm.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.norm.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.4.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.0.conv.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.norm.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.norm.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.1.conv.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.norm.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.norm.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.stages.0.convs.0.conv.weight shape torch.Size([320, 640, 3, 3, 3])
decoder.stages.0.convs.0.conv.bias shape torch.Size([320])
decoder.stages.0.convs.0.norm.weight shape torch.Size([320])
decoder.stages.0.convs.0.norm.bias shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.0.weight shape torch.Size([320, 640, 3, 3, 3])
decoder.stages.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.stages.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.stages.0.convs.1.conv.bias shape torch.Size([320])
decoder.stages.0.convs.1.norm.weight shape torch.Size([320])
decoder.stages.0.convs.1.norm.bias shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.stages.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.stages.1.convs.0.conv.weight shape torch.Size([256, 512, 3, 3, 3])
decoder.stages.1.convs.0.conv.bias shape torch.Size([256])
decoder.stages.1.convs.0.norm.weight shape torch.Size([256])
decoder.stages.1.convs.0.norm.bias shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.0.weight shape torch.Size([256, 512, 3, 3, 3])
decoder.stages.1.convs.0.all_modules.0.bias shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.1.weight shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.1.bias shape torch.Size([256])
decoder.stages.1.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.stages.1.convs.1.conv.bias shape torch.Size([256])
decoder.stages.1.convs.1.norm.weight shape torch.Size([256])
decoder.stages.1.convs.1.norm.bias shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.stages.1.convs.1.all_modules.0.bias shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.1.weight shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.1.bias shape torch.Size([256])
decoder.stages.2.convs.0.conv.weight shape torch.Size([128, 256, 3, 3, 3])
decoder.stages.2.convs.0.conv.bias shape torch.Size([128])
decoder.stages.2.convs.0.norm.weight shape torch.Size([128])
decoder.stages.2.convs.0.norm.bias shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.0.weight shape torch.Size([128, 256, 3, 3, 3])
decoder.stages.2.convs.0.all_modules.0.bias shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.1.weight shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.1.bias shape torch.Size([128])
decoder.stages.2.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.stages.2.convs.1.conv.bias shape torch.Size([128])
decoder.stages.2.convs.1.norm.weight shape torch.Size([128])
decoder.stages.2.convs.1.norm.bias shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.stages.2.convs.1.all_modules.0.bias shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.1.weight shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.1.bias shape torch.Size([128])
decoder.stages.3.convs.0.conv.weight shape torch.Size([64, 128, 3, 3, 3])
decoder.stages.3.convs.0.conv.bias shape torch.Size([64])
decoder.stages.3.convs.0.norm.weight shape torch.Size([64])
decoder.stages.3.convs.0.norm.bias shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.0.weight shape torch.Size([64, 128, 3, 3, 3])
decoder.stages.3.convs.0.all_modules.0.bias shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.1.weight shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.1.bias shape torch.Size([64])
decoder.stages.3.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.stages.3.convs.1.conv.bias shape torch.Size([64])
decoder.stages.3.convs.1.norm.weight shape torch.Size([64])
decoder.stages.3.convs.1.norm.bias shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.stages.3.convs.1.all_modules.0.bias shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.1.weight shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.1.bias shape torch.Size([64])
decoder.stages.4.convs.0.conv.weight shape torch.Size([32, 64, 3, 3, 3])
decoder.stages.4.convs.0.conv.bias shape torch.Size([32])
decoder.stages.4.convs.0.norm.weight shape torch.Size([32])
decoder.stages.4.convs.0.norm.bias shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.0.weight shape torch.Size([32, 64, 3, 3, 3])
decoder.stages.4.convs.0.all_modules.0.bias shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.1.weight shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.1.bias shape torch.Size([32])
decoder.stages.4.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.stages.4.convs.1.conv.bias shape torch.Size([32])
decoder.stages.4.convs.1.norm.weight shape torch.Size([32])
decoder.stages.4.convs.1.norm.bias shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.stages.4.convs.1.all_modules.0.bias shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.1.weight shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.1.bias shape torch.Size([32])
decoder.transpconvs.0.weight shape torch.Size([320, 320, 1, 2, 2])
decoder.transpconvs.0.bias shape torch.Size([320])
decoder.transpconvs.1.weight shape torch.Size([320, 256, 2, 2, 2])
decoder.transpconvs.1.bias shape torch.Size([256])
decoder.transpconvs.2.weight shape torch.Size([256, 128, 2, 2, 2])
decoder.transpconvs.2.bias shape torch.Size([128])
decoder.transpconvs.3.weight shape torch.Size([128, 64, 2, 2, 2])
decoder.transpconvs.3.bias shape torch.Size([64])
decoder.transpconvs.4.weight shape torch.Size([64, 32, 2, 2, 2])
decoder.transpconvs.4.bias shape torch.Size([32])
################### Done ###################
2024-12-19 11:13:42.677155: do_dummy_2d_data_aug: True
2024-12-19 11:13:42.679326: Using splits from existing split file: /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset309_Sohee_Calcium_OCT_CrossValidation/splits_final_20.json
2024-12-19 11:13:42.682167: The split file contains 5 splits.
2024-12-19 11:13:42.683425: Desired fold for training: 4
2024-12-19 11:13:42.684476: This split has 1 training and 7 validation cases.
using pin_memory on device 0
using pin_memory on device 0

This is the configuration used by this training:
Configuration name: 3d_32x160x128_b10
 {'data_identifier': 'nnUNetPlans_3d_fullres', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 10, 'patch_size': [32, 160, 128], 'median_image_size_in_voxels': [375.0, 498.0, 498.0], 'spacing': [1.0, 1.0, 1.0], 'normalization_schemes': ['ZScoreNormalization'], 'use_mask_for_norm': [False], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'architecture': {'network_class_name': 'dynamic_network_architectures.architectures.unet.PlainConvUNet', 'arch_kwargs': {'n_stages': 6, 'features_per_stage': [32, 64, 128, 256, 320, 320], 'conv_op': 'torch.nn.modules.conv.Conv3d', 'kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]], 'strides': [[1, 1, 1], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [1, 2, 2]], 'n_conv_per_stage': [2, 2, 2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2, 2, 2], 'conv_bias': True, 'norm_op': 'torch.nn.modules.instancenorm.InstanceNorm3d', 'norm_op_kwargs': {'eps': 1e-05, 'affine': True}, 'dropout_op': None, 'dropout_op_kwargs': None, 'nonlin': 'torch.nn.LeakyReLU', 'nonlin_kwargs': {'inplace': True}, 'deep_supervision': True}, '_kw_requires_import': ['conv_op', 'norm_op', 'dropout_op', 'nonlin']}, 'batch_dice': True, 'inherits_from': '3d_fullres'} 

These are the global plan.json settings:
 {'dataset_name': 'Dataset309_Sohee_Calcium_OCT_CrossValidation', 'plans_name': 'nnUNetPlans', 'original_median_spacing_after_transp': [1.0, 1.0, 1.0], 'original_median_shape_after_transp': [375, 498, 498], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 0.4977976381778717, 'mean': 0.13507525622844696, 'median': 0.09599608182907104, 'min': 0.0, 'percentile_00_5': 0.014754901640117168, 'percentile_99_5': 0.4977976381778717, 'std': 0.12025152146816254}}} 

2024-12-19 11:14:17.802368: unpacking dataset...
2024-12-19 11:14:22.330182: unpacking done...
2024-12-19 11:14:22.642916: Unable to plot network architecture: nnUNet_compile is enabled!
2024-12-19 11:14:22.702320: 
2024-12-19 11:14:22.703569: Epoch 0
2024-12-19 11:14:22.704507: Current learning rate: 0.01
2024-12-19 11:20:48.887340: Validation loss improved from 1000.00000 to -0.35910! Patience: 0/50
2024-12-19 11:20:48.888249: train_loss -0.3918
2024-12-19 11:20:48.888981: val_loss -0.3591
2024-12-19 11:20:48.889575: Pseudo dice [0.6384]
2024-12-19 11:20:48.890219: Epoch time: 386.19 s
2024-12-19 11:20:48.890978: Yayy! New best EMA pseudo Dice: 0.6384
2024-12-19 11:20:50.711406: 
2024-12-19 11:20:50.712707: Epoch 1
2024-12-19 11:20:50.713611: Current learning rate: 0.00994
2024-12-19 11:25:48.133429: Validation loss improved from -0.35910 to -0.38522! Patience: 0/50
2024-12-19 11:25:48.134278: train_loss -0.5564
2024-12-19 11:25:48.135038: val_loss -0.3852
2024-12-19 11:25:48.135820: Pseudo dice [0.6481]
2024-12-19 11:25:48.136714: Epoch time: 297.42 s
2024-12-19 11:25:48.137621: Yayy! New best EMA pseudo Dice: 0.6393
2024-12-19 11:25:50.079998: 
2024-12-19 11:25:50.081200: Epoch 2
2024-12-19 11:25:50.081930: Current learning rate: 0.00988
2024-12-19 11:31:06.894098: Validation loss did not improve from -0.38522. Patience: 1/50
2024-12-19 11:31:06.895142: train_loss -0.6064
2024-12-19 11:31:06.896010: val_loss -0.3756
2024-12-19 11:31:06.896803: Pseudo dice [0.6463]
2024-12-19 11:31:06.897619: Epoch time: 316.82 s
2024-12-19 11:31:06.898340: Yayy! New best EMA pseudo Dice: 0.64
2024-12-19 11:31:08.854601: 
2024-12-19 11:31:08.855607: Epoch 3
2024-12-19 11:31:08.856327: Current learning rate: 0.00982
2024-12-19 11:36:16.069467: Validation loss did not improve from -0.38522. Patience: 2/50
2024-12-19 11:36:16.070490: train_loss -0.6319
2024-12-19 11:36:16.071625: val_loss -0.3654
2024-12-19 11:36:16.072563: Pseudo dice [0.6304]
2024-12-19 11:36:16.073503: Epoch time: 307.22 s
2024-12-19 11:36:17.485888: 
2024-12-19 11:36:17.487533: Epoch 4
2024-12-19 11:36:17.488709: Current learning rate: 0.00976
2024-12-19 11:41:14.403932: Validation loss improved from -0.38522 to -0.40198! Patience: 2/50
2024-12-19 11:41:14.405744: train_loss -0.6511
2024-12-19 11:41:14.406699: val_loss -0.402
2024-12-19 11:41:14.407326: Pseudo dice [0.6809]
2024-12-19 11:41:14.408097: Epoch time: 296.92 s
2024-12-19 11:41:14.782769: Yayy! New best EMA pseudo Dice: 0.6433
2024-12-19 11:41:16.613187: 
2024-12-19 11:41:16.614311: Epoch 5
2024-12-19 11:41:16.615004: Current learning rate: 0.0097
2024-12-19 11:46:20.510614: Validation loss improved from -0.40198 to -0.41996! Patience: 0/50
2024-12-19 11:46:20.511522: train_loss -0.6701
2024-12-19 11:46:20.512348: val_loss -0.42
2024-12-19 11:46:20.513013: Pseudo dice [0.6841]
2024-12-19 11:46:20.513738: Epoch time: 303.9 s
2024-12-19 11:46:20.514325: Yayy! New best EMA pseudo Dice: 0.6473
2024-12-19 11:46:22.319693: 
2024-12-19 11:46:22.320451: Epoch 6
2024-12-19 11:46:22.321087: Current learning rate: 0.00964
2024-12-19 11:51:26.222838: Validation loss improved from -0.41996 to -0.43030! Patience: 0/50
2024-12-19 11:51:26.223601: train_loss -0.6836
2024-12-19 11:51:26.224647: val_loss -0.4303
2024-12-19 11:51:26.225603: Pseudo dice [0.681]
2024-12-19 11:51:26.226509: Epoch time: 303.91 s
2024-12-19 11:51:26.227427: Yayy! New best EMA pseudo Dice: 0.6507
2024-12-19 11:51:27.937667: 
2024-12-19 11:51:27.938642: Epoch 7
2024-12-19 11:51:27.939609: Current learning rate: 0.00958
2024-12-19 11:56:31.153861: Validation loss improved from -0.43030 to -0.43456! Patience: 0/50
2024-12-19 11:56:31.154575: train_loss -0.688
2024-12-19 11:56:31.155350: val_loss -0.4346
2024-12-19 11:56:31.156003: Pseudo dice [0.6843]
2024-12-19 11:56:31.156689: Epoch time: 303.22 s
2024-12-19 11:56:31.157314: Yayy! New best EMA pseudo Dice: 0.6541
2024-12-19 11:56:33.348951: 
2024-12-19 11:56:33.349994: Epoch 8
2024-12-19 11:56:33.350678: Current learning rate: 0.00952
2024-12-19 12:01:43.610153: Validation loss improved from -0.43456 to -0.44107! Patience: 0/50
2024-12-19 12:01:43.610805: train_loss -0.6895
2024-12-19 12:01:43.611909: val_loss -0.4411
2024-12-19 12:01:43.612751: Pseudo dice [0.7035]
2024-12-19 12:01:43.613400: Epoch time: 310.26 s
2024-12-19 12:01:43.614056: Yayy! New best EMA pseudo Dice: 0.659
2024-12-19 12:01:45.706434: 
2024-12-19 12:01:45.707249: Epoch 9
2024-12-19 12:01:45.707975: Current learning rate: 0.00946
2024-12-19 12:07:05.596172: Validation loss improved from -0.44107 to -0.45529! Patience: 0/50
2024-12-19 12:07:05.596950: train_loss -0.6998
2024-12-19 12:07:05.597957: val_loss -0.4553
2024-12-19 12:07:05.598731: Pseudo dice [0.6991]
2024-12-19 12:07:05.599820: Epoch time: 319.89 s
2024-12-19 12:07:06.065630: Yayy! New best EMA pseudo Dice: 0.663
2024-12-19 12:07:07.802329: 
2024-12-19 12:07:07.803390: Epoch 10
2024-12-19 12:07:07.803980: Current learning rate: 0.0094
2024-12-19 12:12:38.103969: Validation loss did not improve from -0.45529. Patience: 1/50
2024-12-19 12:12:38.104850: train_loss -0.7139
2024-12-19 12:12:38.106015: val_loss -0.4381
2024-12-19 12:12:38.107041: Pseudo dice [0.7031]
2024-12-19 12:12:38.107979: Epoch time: 330.3 s
2024-12-19 12:12:38.108932: Yayy! New best EMA pseudo Dice: 0.667
2024-12-19 12:12:39.820210: 
2024-12-19 12:12:39.821556: Epoch 11
2024-12-19 12:12:39.822570: Current learning rate: 0.00934
2024-12-19 12:18:03.166953: Validation loss did not improve from -0.45529. Patience: 2/50
2024-12-19 12:18:03.167909: train_loss -0.7195
2024-12-19 12:18:03.168649: val_loss -0.4507
2024-12-19 12:18:03.169353: Pseudo dice [0.7063]
2024-12-19 12:18:03.170221: Epoch time: 323.35 s
2024-12-19 12:18:03.171018: Yayy! New best EMA pseudo Dice: 0.671
2024-12-19 12:18:04.860487: 
2024-12-19 12:18:04.861688: Epoch 12
2024-12-19 12:18:04.862410: Current learning rate: 0.00928
2024-12-19 12:23:28.733231: Validation loss did not improve from -0.45529. Patience: 3/50
2024-12-19 12:23:28.875826: train_loss -0.72
2024-12-19 12:23:28.877651: val_loss -0.4279
2024-12-19 12:23:28.878442: Pseudo dice [0.6912]
2024-12-19 12:23:28.879219: Epoch time: 324.02 s
2024-12-19 12:23:28.879853: Yayy! New best EMA pseudo Dice: 0.673
2024-12-19 12:23:30.849761: 
2024-12-19 12:23:30.851020: Epoch 13
2024-12-19 12:23:30.851691: Current learning rate: 0.00922
2024-12-19 12:28:56.901431: Validation loss did not improve from -0.45529. Patience: 4/50
2024-12-19 12:28:56.902293: train_loss -0.726
2024-12-19 12:28:56.902959: val_loss -0.4163
2024-12-19 12:28:56.903529: Pseudo dice [0.692]
2024-12-19 12:28:56.904183: Epoch time: 326.05 s
2024-12-19 12:28:56.904767: Yayy! New best EMA pseudo Dice: 0.6749
2024-12-19 12:28:58.736218: 
2024-12-19 12:28:58.736989: Epoch 14
2024-12-19 12:28:58.737612: Current learning rate: 0.00916
2024-12-19 12:34:28.373996: Validation loss did not improve from -0.45529. Patience: 5/50
2024-12-19 12:34:28.374951: train_loss -0.7298
2024-12-19 12:34:28.375647: val_loss -0.4219
2024-12-19 12:34:28.376368: Pseudo dice [0.6879]
2024-12-19 12:34:28.377358: Epoch time: 329.64 s
2024-12-19 12:34:28.869163: Yayy! New best EMA pseudo Dice: 0.6762
2024-12-19 12:34:30.629075: 
2024-12-19 12:34:30.630435: Epoch 15
2024-12-19 12:34:30.631000: Current learning rate: 0.0091
2024-12-19 12:39:52.621154: Validation loss did not improve from -0.45529. Patience: 6/50
2024-12-19 12:39:52.622090: train_loss -0.739
2024-12-19 12:39:52.622878: val_loss -0.4158
2024-12-19 12:39:52.623514: Pseudo dice [0.686]
2024-12-19 12:39:52.624127: Epoch time: 321.99 s
2024-12-19 12:39:52.624817: Yayy! New best EMA pseudo Dice: 0.6772
2024-12-19 12:39:54.472450: 
2024-12-19 12:39:54.473831: Epoch 16
2024-12-19 12:39:54.474745: Current learning rate: 0.00903
2024-12-19 12:45:04.627253: Validation loss did not improve from -0.45529. Patience: 7/50
2024-12-19 12:45:04.627970: train_loss -0.743
2024-12-19 12:45:04.629053: val_loss -0.4283
2024-12-19 12:45:04.629861: Pseudo dice [0.6925]
2024-12-19 12:45:04.631168: Epoch time: 310.16 s
2024-12-19 12:45:04.632095: Yayy! New best EMA pseudo Dice: 0.6787
2024-12-19 12:45:06.671549: 
2024-12-19 12:45:06.672695: Epoch 17
2024-12-19 12:45:06.673487: Current learning rate: 0.00897
2024-12-19 12:50:08.582988: Validation loss did not improve from -0.45529. Patience: 8/50
2024-12-19 12:50:08.583886: train_loss -0.7462
2024-12-19 12:50:08.584869: val_loss -0.3884
2024-12-19 12:50:08.585528: Pseudo dice [0.674]
2024-12-19 12:50:08.586283: Epoch time: 301.91 s
2024-12-19 12:50:11.376433: 
2024-12-19 12:50:11.377715: Epoch 18
2024-12-19 12:50:11.378511: Current learning rate: 0.00891
2024-12-19 12:55:29.782621: Validation loss did not improve from -0.45529. Patience: 9/50
2024-12-19 12:55:29.783562: train_loss -0.7522
2024-12-19 12:55:29.784324: val_loss -0.4442
2024-12-19 12:55:29.784982: Pseudo dice [0.6994]
2024-12-19 12:55:29.785765: Epoch time: 318.41 s
2024-12-19 12:55:29.786542: Yayy! New best EMA pseudo Dice: 0.6803
2024-12-19 12:55:31.631132: 
2024-12-19 12:55:31.632441: Epoch 19
2024-12-19 12:55:31.633290: Current learning rate: 0.00885
2024-12-19 13:01:18.819686: Validation loss did not improve from -0.45529. Patience: 10/50
2024-12-19 13:01:18.820434: train_loss -0.7534
2024-12-19 13:01:18.821397: val_loss -0.3529
2024-12-19 13:01:18.822393: Pseudo dice [0.6546]
2024-12-19 13:01:18.823370: Epoch time: 347.19 s
2024-12-19 13:01:20.627448: 
2024-12-19 13:01:20.628908: Epoch 20
2024-12-19 13:01:20.629776: Current learning rate: 0.00879
2024-12-19 13:07:24.747931: Validation loss did not improve from -0.45529. Patience: 11/50
2024-12-19 13:07:24.749623: train_loss -0.7568
2024-12-19 13:07:24.751130: val_loss -0.432
2024-12-19 13:07:24.751786: Pseudo dice [0.697]
2024-12-19 13:07:24.753075: Epoch time: 364.12 s
2024-12-19 13:07:26.320551: 
2024-12-19 13:07:26.321660: Epoch 21
2024-12-19 13:07:26.322517: Current learning rate: 0.00873
2024-12-19 13:13:30.057193: Validation loss did not improve from -0.45529. Patience: 12/50
2024-12-19 13:13:30.058156: train_loss -0.7588
2024-12-19 13:13:30.058966: val_loss -0.4549
2024-12-19 13:13:30.059558: Pseudo dice [0.7107]
2024-12-19 13:13:30.060198: Epoch time: 363.74 s
2024-12-19 13:13:30.060766: Yayy! New best EMA pseudo Dice: 0.6828
2024-12-19 13:13:31.786372: 
2024-12-19 13:13:31.787465: Epoch 22
2024-12-19 13:13:31.788144: Current learning rate: 0.00867
2024-12-19 13:19:35.615009: Validation loss did not improve from -0.45529. Patience: 13/50
2024-12-19 13:19:35.615890: train_loss -0.759
2024-12-19 13:19:35.616742: val_loss -0.3999
2024-12-19 13:19:35.617416: Pseudo dice [0.6831]
2024-12-19 13:19:35.618349: Epoch time: 363.83 s
2024-12-19 13:19:35.619307: Yayy! New best EMA pseudo Dice: 0.6828
2024-12-19 13:19:37.402293: 
2024-12-19 13:19:37.403259: Epoch 23
2024-12-19 13:19:37.404078: Current learning rate: 0.00861
2024-12-19 13:25:16.162786: Validation loss did not improve from -0.45529. Patience: 14/50
2024-12-19 13:25:16.184526: train_loss -0.763
2024-12-19 13:25:16.185352: val_loss -0.4341
2024-12-19 13:25:16.186187: Pseudo dice [0.6985]
2024-12-19 13:25:16.186808: Epoch time: 338.78 s
2024-12-19 13:25:16.187377: Yayy! New best EMA pseudo Dice: 0.6844
2024-12-19 13:25:17.942990: 
2024-12-19 13:25:17.944383: Epoch 24
2024-12-19 13:25:17.945148: Current learning rate: 0.00855
2024-12-19 13:30:50.575158: Validation loss did not improve from -0.45529. Patience: 15/50
2024-12-19 13:30:50.576305: train_loss -0.7668
2024-12-19 13:30:50.577109: val_loss -0.3729
2024-12-19 13:30:50.577870: Pseudo dice [0.6675]
2024-12-19 13:30:50.578471: Epoch time: 332.63 s
2024-12-19 13:30:52.408032: 
2024-12-19 13:30:52.409336: Epoch 25
2024-12-19 13:30:52.410006: Current learning rate: 0.00849
2024-12-19 13:36:22.083663: Validation loss did not improve from -0.45529. Patience: 16/50
2024-12-19 13:36:22.084595: train_loss -0.7704
2024-12-19 13:36:22.085373: val_loss -0.4278
2024-12-19 13:36:22.086099: Pseudo dice [0.7035]
2024-12-19 13:36:22.087017: Epoch time: 329.68 s
2024-12-19 13:36:22.087982: Yayy! New best EMA pseudo Dice: 0.6848
2024-12-19 13:36:23.855689: 
2024-12-19 13:36:23.856763: Epoch 26
2024-12-19 13:36:23.857671: Current learning rate: 0.00843
2024-12-19 13:41:54.219084: Validation loss did not improve from -0.45529. Patience: 17/50
2024-12-19 13:41:54.220041: train_loss -0.7763
2024-12-19 13:41:54.220860: val_loss -0.4387
2024-12-19 13:41:54.221499: Pseudo dice [0.7135]
2024-12-19 13:41:54.222181: Epoch time: 330.37 s
2024-12-19 13:41:54.222846: Yayy! New best EMA pseudo Dice: 0.6877
2024-12-19 13:41:56.007841: 
2024-12-19 13:41:56.009053: Epoch 27
2024-12-19 13:41:56.009954: Current learning rate: 0.00836
2024-12-19 13:47:33.777168: Validation loss did not improve from -0.45529. Patience: 18/50
2024-12-19 13:47:33.777985: train_loss -0.7717
2024-12-19 13:47:33.778850: val_loss -0.4422
2024-12-19 13:47:33.779415: Pseudo dice [0.7041]
2024-12-19 13:47:33.780037: Epoch time: 337.77 s
2024-12-19 13:47:33.780747: Yayy! New best EMA pseudo Dice: 0.6893
2024-12-19 13:47:36.071043: 
2024-12-19 13:47:36.072635: Epoch 28
2024-12-19 13:47:36.073402: Current learning rate: 0.0083
2024-12-19 13:52:59.181446: Validation loss did not improve from -0.45529. Patience: 19/50
2024-12-19 13:52:59.182683: train_loss -0.7741
2024-12-19 13:52:59.184024: val_loss -0.402
2024-12-19 13:52:59.185029: Pseudo dice [0.7006]
2024-12-19 13:52:59.185920: Epoch time: 323.12 s
2024-12-19 13:52:59.186787: Yayy! New best EMA pseudo Dice: 0.6904
2024-12-19 13:53:00.878253: 
2024-12-19 13:53:00.879558: Epoch 29
2024-12-19 13:53:00.880464: Current learning rate: 0.00824
2024-12-19 13:58:46.435154: Validation loss did not improve from -0.45529. Patience: 20/50
2024-12-19 13:58:46.436069: train_loss -0.7769
2024-12-19 13:58:46.437601: val_loss -0.4261
2024-12-19 13:58:46.438355: Pseudo dice [0.7092]
2024-12-19 13:58:46.439180: Epoch time: 345.56 s
2024-12-19 13:58:46.849652: Yayy! New best EMA pseudo Dice: 0.6923
2024-12-19 13:58:48.640105: 
2024-12-19 13:58:48.641525: Epoch 30
2024-12-19 13:58:48.642234: Current learning rate: 0.00818
2024-12-19 14:04:13.150014: Validation loss did not improve from -0.45529. Patience: 21/50
2024-12-19 14:04:13.150781: train_loss -0.7773
2024-12-19 14:04:13.151749: val_loss -0.4345
2024-12-19 14:04:13.152532: Pseudo dice [0.7031]
2024-12-19 14:04:13.153316: Epoch time: 324.51 s
2024-12-19 14:04:13.154000: Yayy! New best EMA pseudo Dice: 0.6934
2024-12-19 14:04:14.869986: 
2024-12-19 14:04:14.871344: Epoch 31
2024-12-19 14:04:14.872236: Current learning rate: 0.00812
2024-12-19 14:09:33.465020: Validation loss did not improve from -0.45529. Patience: 22/50
2024-12-19 14:09:33.466346: train_loss -0.7759
2024-12-19 14:09:33.467143: val_loss -0.4039
2024-12-19 14:09:33.467737: Pseudo dice [0.6934]
2024-12-19 14:09:33.468599: Epoch time: 318.6 s
2024-12-19 14:09:33.469632: Yayy! New best EMA pseudo Dice: 0.6934
2024-12-19 14:09:35.719886: 
2024-12-19 14:09:35.721123: Epoch 32
2024-12-19 14:09:35.721887: Current learning rate: 0.00806
2024-12-19 14:15:10.465087: Validation loss did not improve from -0.45529. Patience: 23/50
2024-12-19 14:15:10.466237: train_loss -0.7808
2024-12-19 14:15:10.467008: val_loss -0.4095
2024-12-19 14:15:10.467650: Pseudo dice [0.6967]
2024-12-19 14:15:10.468262: Epoch time: 334.75 s
2024-12-19 14:15:10.468972: Yayy! New best EMA pseudo Dice: 0.6937
2024-12-19 14:15:12.268126: 
2024-12-19 14:15:12.269162: Epoch 33
2024-12-19 14:15:12.269922: Current learning rate: 0.008
2024-12-19 14:20:45.183230: Validation loss did not improve from -0.45529. Patience: 24/50
2024-12-19 14:20:45.184056: train_loss -0.7823
2024-12-19 14:20:45.184869: val_loss -0.4384
2024-12-19 14:20:45.185818: Pseudo dice [0.7097]
2024-12-19 14:20:45.186690: Epoch time: 332.92 s
2024-12-19 14:20:45.187643: Yayy! New best EMA pseudo Dice: 0.6953
2024-12-19 14:20:46.956840: 
2024-12-19 14:20:46.957696: Epoch 34
2024-12-19 14:20:46.958361: Current learning rate: 0.00793
2024-12-19 14:26:06.367147: Validation loss did not improve from -0.45529. Patience: 25/50
2024-12-19 14:26:06.369388: train_loss -0.7829
2024-12-19 14:26:06.370485: val_loss -0.4413
2024-12-19 14:26:06.371326: Pseudo dice [0.7063]
2024-12-19 14:26:06.372307: Epoch time: 319.41 s
2024-12-19 14:26:06.807446: Yayy! New best EMA pseudo Dice: 0.6964
2024-12-19 14:26:08.602802: 
2024-12-19 14:26:08.604403: Epoch 35
2024-12-19 14:26:08.605833: Current learning rate: 0.00787
2024-12-19 14:31:33.453305: Validation loss did not improve from -0.45529. Patience: 26/50
2024-12-19 14:31:33.455564: train_loss -0.7858
2024-12-19 14:31:33.456658: val_loss -0.3953
2024-12-19 14:31:33.457337: Pseudo dice [0.6814]
2024-12-19 14:31:33.458089: Epoch time: 324.85 s
2024-12-19 14:31:34.872643: 
2024-12-19 14:31:34.873855: Epoch 36
2024-12-19 14:31:34.874739: Current learning rate: 0.00781
2024-12-19 14:36:40.325396: Validation loss did not improve from -0.45529. Patience: 27/50
2024-12-19 14:36:40.326448: train_loss -0.7867
2024-12-19 14:36:40.327574: val_loss -0.4171
2024-12-19 14:36:40.328521: Pseudo dice [0.6908]
2024-12-19 14:36:40.329345: Epoch time: 305.45 s
2024-12-19 14:36:41.768884: 
2024-12-19 14:36:41.770497: Epoch 37
2024-12-19 14:36:41.771494: Current learning rate: 0.00775
2024-12-19 14:42:02.072734: Validation loss did not improve from -0.45529. Patience: 28/50
2024-12-19 14:42:02.073558: train_loss -0.7918
2024-12-19 14:42:02.074722: val_loss -0.4191
2024-12-19 14:42:02.075386: Pseudo dice [0.6895]
2024-12-19 14:42:02.076288: Epoch time: 320.31 s
2024-12-19 14:42:03.469939: 
2024-12-19 14:42:03.471233: Epoch 38
2024-12-19 14:42:03.471938: Current learning rate: 0.00769
2024-12-19 14:47:14.984982: Validation loss improved from -0.45529 to -0.46269! Patience: 28/50
2024-12-19 14:47:14.985692: train_loss -0.791
2024-12-19 14:47:14.986678: val_loss -0.4627
2024-12-19 14:47:14.987576: Pseudo dice [0.7261]
2024-12-19 14:47:14.988536: Epoch time: 311.52 s
2024-12-19 14:47:14.989551: Yayy! New best EMA pseudo Dice: 0.6972
2024-12-19 14:47:17.266278: 
2024-12-19 14:47:17.267599: Epoch 39
2024-12-19 14:47:17.268511: Current learning rate: 0.00763
2024-12-19 14:52:36.542776: Validation loss did not improve from -0.46269. Patience: 1/50
2024-12-19 14:52:36.543770: train_loss -0.7933
2024-12-19 14:52:36.544591: val_loss -0.4285
2024-12-19 14:52:36.545353: Pseudo dice [0.6961]
2024-12-19 14:52:36.546246: Epoch time: 319.28 s
2024-12-19 14:52:38.420760: 
2024-12-19 14:52:38.421873: Epoch 40
2024-12-19 14:52:38.422587: Current learning rate: 0.00756
2024-12-19 14:57:56.342897: Validation loss did not improve from -0.46269. Patience: 2/50
2024-12-19 14:57:56.343877: train_loss -0.7935
2024-12-19 14:57:56.345047: val_loss -0.4084
2024-12-19 14:57:56.345944: Pseudo dice [0.6882]
2024-12-19 14:57:56.346721: Epoch time: 317.92 s
2024-12-19 14:57:57.845751: 
2024-12-19 14:57:57.847099: Epoch 41
2024-12-19 14:57:57.848002: Current learning rate: 0.0075
2024-12-19 15:03:36.926896: Validation loss did not improve from -0.46269. Patience: 3/50
2024-12-19 15:03:36.927829: train_loss -0.795
2024-12-19 15:03:36.929516: val_loss -0.4181
2024-12-19 15:03:36.930439: Pseudo dice [0.7017]
2024-12-19 15:03:36.931604: Epoch time: 339.08 s
2024-12-19 15:03:38.269608: 
2024-12-19 15:03:38.270675: Epoch 42
2024-12-19 15:03:38.271405: Current learning rate: 0.00744
2024-12-19 15:08:52.251605: Validation loss did not improve from -0.46269. Patience: 4/50
2024-12-19 15:08:52.252372: train_loss -0.794
2024-12-19 15:08:52.253435: val_loss -0.4316
2024-12-19 15:08:52.254188: Pseudo dice [0.7056]
2024-12-19 15:08:52.254958: Epoch time: 313.98 s
2024-12-19 15:08:52.255610: Yayy! New best EMA pseudo Dice: 0.6977
2024-12-19 15:08:54.032232: 
2024-12-19 15:08:54.033401: Epoch 43
2024-12-19 15:08:54.034448: Current learning rate: 0.00738
2024-12-19 15:14:33.063069: Validation loss did not improve from -0.46269. Patience: 5/50
2024-12-19 15:14:33.064570: train_loss -0.7984
2024-12-19 15:14:33.065488: val_loss -0.3998
2024-12-19 15:14:33.066254: Pseudo dice [0.6876]
2024-12-19 15:14:33.066998: Epoch time: 339.03 s
2024-12-19 15:14:34.503123: 
2024-12-19 15:14:34.504336: Epoch 44
2024-12-19 15:14:34.505366: Current learning rate: 0.00732
2024-12-19 15:20:00.675697: Validation loss did not improve from -0.46269. Patience: 6/50
2024-12-19 15:20:00.677043: train_loss -0.7981
2024-12-19 15:20:00.677808: val_loss -0.3753
2024-12-19 15:20:00.678567: Pseudo dice [0.6751]
2024-12-19 15:20:00.679332: Epoch time: 326.18 s
2024-12-19 15:20:02.485718: 
2024-12-19 15:20:02.487110: Epoch 45
2024-12-19 15:20:02.487973: Current learning rate: 0.00725
2024-12-19 15:25:51.990429: Validation loss did not improve from -0.46269. Patience: 7/50
2024-12-19 15:25:51.991304: train_loss -0.7996
2024-12-19 15:25:51.992192: val_loss -0.4456
2024-12-19 15:25:51.993344: Pseudo dice [0.7068]
2024-12-19 15:25:51.994404: Epoch time: 349.51 s
2024-12-19 15:25:53.365279: 
2024-12-19 15:25:53.366356: Epoch 46
2024-12-19 15:25:53.367044: Current learning rate: 0.00719
2024-12-19 15:31:29.182702: Validation loss did not improve from -0.46269. Patience: 8/50
2024-12-19 15:31:29.183378: train_loss -0.8023
2024-12-19 15:31:29.184122: val_loss -0.4238
2024-12-19 15:31:29.184760: Pseudo dice [0.705]
2024-12-19 15:31:29.185531: Epoch time: 335.82 s
2024-12-19 15:31:30.503690: 
2024-12-19 15:31:30.504971: Epoch 47
2024-12-19 15:31:30.505713: Current learning rate: 0.00713
2024-12-19 15:36:49.851999: Validation loss did not improve from -0.46269. Patience: 9/50
2024-12-19 15:36:49.854181: train_loss -0.8011
2024-12-19 15:36:49.862992: val_loss -0.4052
2024-12-19 15:36:49.863758: Pseudo dice [0.6834]
2024-12-19 15:36:49.864427: Epoch time: 319.35 s
2024-12-19 15:36:51.291679: 
2024-12-19 15:36:51.292559: Epoch 48
2024-12-19 15:36:51.293386: Current learning rate: 0.00707
2024-12-19 15:42:21.750120: Validation loss did not improve from -0.46269. Patience: 10/50
2024-12-19 15:42:21.751490: train_loss -0.804
2024-12-19 15:42:21.752601: val_loss -0.4027
2024-12-19 15:42:21.753532: Pseudo dice [0.6855]
2024-12-19 15:42:21.754375: Epoch time: 330.46 s
2024-12-19 15:42:23.206091: 
2024-12-19 15:42:23.207872: Epoch 49
2024-12-19 15:42:23.208974: Current learning rate: 0.007
2024-12-19 15:48:17.877400: Validation loss did not improve from -0.46269. Patience: 11/50
2024-12-19 15:48:17.878232: train_loss -0.8039
2024-12-19 15:48:17.879019: val_loss -0.4251
2024-12-19 15:48:17.879612: Pseudo dice [0.7035]
2024-12-19 15:48:17.880237: Epoch time: 354.67 s
2024-12-19 15:48:22.482354: 
2024-12-19 15:48:22.483734: Epoch 50
2024-12-19 15:48:22.484539: Current learning rate: 0.00694
2024-12-19 15:53:55.796764: Validation loss did not improve from -0.46269. Patience: 12/50
2024-12-19 15:53:55.797762: train_loss -0.8037
2024-12-19 15:53:55.798580: val_loss -0.3778
2024-12-19 15:53:55.799287: Pseudo dice [0.6862]
2024-12-19 15:53:55.799931: Epoch time: 333.32 s
2024-12-19 15:53:57.167143: 
2024-12-19 15:53:57.168610: Epoch 51
2024-12-19 15:53:57.169490: Current learning rate: 0.00688
2024-12-19 15:59:40.057899: Validation loss did not improve from -0.46269. Patience: 13/50
2024-12-19 15:59:40.059156: train_loss -0.8052
2024-12-19 15:59:40.059856: val_loss -0.428
2024-12-19 15:59:40.060644: Pseudo dice [0.6993]
2024-12-19 15:59:40.061288: Epoch time: 342.89 s
2024-12-19 15:59:41.467910: 
2024-12-19 15:59:41.468792: Epoch 52
2024-12-19 15:59:41.469518: Current learning rate: 0.00682
2024-12-19 16:05:05.663027: Validation loss did not improve from -0.46269. Patience: 14/50
2024-12-19 16:05:05.663861: train_loss -0.808
2024-12-19 16:05:05.664724: val_loss -0.3592
2024-12-19 16:05:05.665643: Pseudo dice [0.6712]
2024-12-19 16:05:05.666546: Epoch time: 324.2 s
2024-12-19 16:05:07.016789: 
2024-12-19 16:05:07.018145: Epoch 53
2024-12-19 16:05:07.019340: Current learning rate: 0.00675
2024-12-19 16:10:45.594175: Validation loss did not improve from -0.46269. Patience: 15/50
2024-12-19 16:10:45.595005: train_loss -0.8085
2024-12-19 16:10:45.595799: val_loss -0.4028
2024-12-19 16:10:45.596602: Pseudo dice [0.704]
2024-12-19 16:10:45.597394: Epoch time: 338.58 s
2024-12-19 16:10:46.949879: 
2024-12-19 16:10:46.951165: Epoch 54
2024-12-19 16:10:46.952076: Current learning rate: 0.00669
2024-12-19 16:16:19.609292: Validation loss did not improve from -0.46269. Patience: 16/50
2024-12-19 16:16:19.610246: train_loss -0.8071
2024-12-19 16:16:19.611238: val_loss -0.3715
2024-12-19 16:16:19.612082: Pseudo dice [0.6713]
2024-12-19 16:16:19.613288: Epoch time: 332.66 s
2024-12-19 16:16:21.488182: 
2024-12-19 16:16:21.489586: Epoch 55
2024-12-19 16:16:21.490284: Current learning rate: 0.00663
2024-12-19 16:22:04.598662: Validation loss did not improve from -0.46269. Patience: 17/50
2024-12-19 16:22:04.600190: train_loss -0.8067
2024-12-19 16:22:04.601174: val_loss -0.4577
2024-12-19 16:22:04.601979: Pseudo dice [0.7141]
2024-12-19 16:22:04.602862: Epoch time: 343.11 s
2024-12-19 16:22:05.999397: 
2024-12-19 16:22:06.000902: Epoch 56
2024-12-19 16:22:06.001691: Current learning rate: 0.00657
2024-12-19 16:27:40.594955: Validation loss did not improve from -0.46269. Patience: 18/50
2024-12-19 16:27:40.595881: train_loss -0.8084
2024-12-19 16:27:40.596807: val_loss -0.3957
2024-12-19 16:27:40.597466: Pseudo dice [0.6889]
2024-12-19 16:27:40.598227: Epoch time: 334.6 s
2024-12-19 16:27:41.986602: 
2024-12-19 16:27:41.987866: Epoch 57
2024-12-19 16:27:41.988670: Current learning rate: 0.0065
2024-12-19 16:33:06.026175: Validation loss did not improve from -0.46269. Patience: 19/50
2024-12-19 16:33:06.027109: train_loss -0.811
2024-12-19 16:33:06.027895: val_loss -0.4283
2024-12-19 16:33:06.028680: Pseudo dice [0.7017]
2024-12-19 16:33:06.029530: Epoch time: 324.04 s
2024-12-19 16:33:07.447391: 
2024-12-19 16:33:07.448741: Epoch 58
2024-12-19 16:33:07.449620: Current learning rate: 0.00644
2024-12-19 16:39:05.370676: Validation loss did not improve from -0.46269. Patience: 20/50
2024-12-19 16:39:05.371317: train_loss -0.8131
2024-12-19 16:39:05.372024: val_loss -0.4282
2024-12-19 16:39:05.372628: Pseudo dice [0.7021]
2024-12-19 16:39:05.373418: Epoch time: 357.93 s
2024-12-19 16:39:06.763582: 
2024-12-19 16:39:06.764974: Epoch 59
2024-12-19 16:39:06.766109: Current learning rate: 0.00638
2024-12-19 16:44:39.239187: Validation loss did not improve from -0.46269. Patience: 21/50
2024-12-19 16:44:39.249143: train_loss -0.8129
2024-12-19 16:44:39.257709: val_loss -0.4084
2024-12-19 16:44:39.258589: Pseudo dice [0.7044]
2024-12-19 16:44:39.259345: Epoch time: 332.48 s
2024-12-19 16:44:41.477266: 
2024-12-19 16:44:41.478502: Epoch 60
2024-12-19 16:44:41.479292: Current learning rate: 0.00631
2024-12-19 16:49:57.133487: Validation loss did not improve from -0.46269. Patience: 22/50
2024-12-19 16:49:57.134724: train_loss -0.8103
2024-12-19 16:49:57.135603: val_loss -0.4371
2024-12-19 16:49:57.136435: Pseudo dice [0.7078]
2024-12-19 16:49:57.137151: Epoch time: 315.66 s
2024-12-19 16:49:58.535959: 
2024-12-19 16:49:58.537496: Epoch 61
2024-12-19 16:49:58.538511: Current learning rate: 0.00625
2024-12-19 16:55:35.290963: Validation loss did not improve from -0.46269. Patience: 23/50
2024-12-19 16:55:35.291940: train_loss -0.8123
2024-12-19 16:55:35.292783: val_loss -0.3941
2024-12-19 16:55:35.293492: Pseudo dice [0.6895]
2024-12-19 16:55:35.294295: Epoch time: 336.76 s
2024-12-19 16:55:37.745929: 
2024-12-19 16:55:37.747221: Epoch 62
2024-12-19 16:55:37.748034: Current learning rate: 0.00619
2024-12-19 17:01:10.634158: Validation loss did not improve from -0.46269. Patience: 24/50
2024-12-19 17:01:10.635543: train_loss -0.8136
2024-12-19 17:01:10.636726: val_loss -0.3988
2024-12-19 17:01:10.637676: Pseudo dice [0.6904]
2024-12-19 17:01:10.638555: Epoch time: 332.89 s
2024-12-19 17:01:12.147407: 
2024-12-19 17:01:12.148672: Epoch 63
2024-12-19 17:01:12.149582: Current learning rate: 0.00612
2024-12-19 17:07:07.371266: Validation loss did not improve from -0.46269. Patience: 25/50
2024-12-19 17:07:07.372238: train_loss -0.8169
2024-12-19 17:07:07.373181: val_loss -0.4388
2024-12-19 17:07:07.373883: Pseudo dice [0.7124]
2024-12-19 17:07:07.374540: Epoch time: 355.23 s
2024-12-19 17:07:08.807654: 
2024-12-19 17:07:08.809318: Epoch 64
2024-12-19 17:07:08.810225: Current learning rate: 0.00606
2024-12-19 17:12:51.624915: Validation loss did not improve from -0.46269. Patience: 26/50
2024-12-19 17:12:51.627247: train_loss -0.8211
2024-12-19 17:12:51.628905: val_loss -0.4216
2024-12-19 17:12:51.629683: Pseudo dice [0.7017]
2024-12-19 17:12:51.630576: Epoch time: 342.82 s
2024-12-19 17:12:52.106297: Yayy! New best EMA pseudo Dice: 0.6978
2024-12-19 17:12:53.993155: 
2024-12-19 17:12:53.994462: Epoch 65
2024-12-19 17:12:53.995578: Current learning rate: 0.006
2024-12-19 17:18:37.358246: Validation loss did not improve from -0.46269. Patience: 27/50
2024-12-19 17:18:37.360654: train_loss -0.8205
2024-12-19 17:18:37.361666: val_loss -0.4114
2024-12-19 17:18:37.362435: Pseudo dice [0.7014]
2024-12-19 17:18:37.363156: Epoch time: 343.37 s
2024-12-19 17:18:37.363899: Yayy! New best EMA pseudo Dice: 0.6981
2024-12-19 17:18:39.138698: 
2024-12-19 17:18:39.139920: Epoch 66
2024-12-19 17:18:39.140700: Current learning rate: 0.00593
2024-12-19 17:24:34.032919: Validation loss did not improve from -0.46269. Patience: 28/50
2024-12-19 17:24:34.033928: train_loss -0.8174
2024-12-19 17:24:34.034845: val_loss -0.4074
2024-12-19 17:24:34.035505: Pseudo dice [0.6974]
2024-12-19 17:24:34.036484: Epoch time: 354.9 s
2024-12-19 17:24:35.444039: 
2024-12-19 17:24:35.445197: Epoch 67
2024-12-19 17:24:35.446194: Current learning rate: 0.00587
2024-12-19 17:30:21.377558: Validation loss did not improve from -0.46269. Patience: 29/50
2024-12-19 17:30:21.378963: train_loss -0.8168
2024-12-19 17:30:21.379935: val_loss -0.3955
2024-12-19 17:30:21.380591: Pseudo dice [0.6816]
2024-12-19 17:30:21.381394: Epoch time: 345.94 s
2024-12-19 17:30:22.818530: 
2024-12-19 17:30:22.819747: Epoch 68
2024-12-19 17:30:22.820696: Current learning rate: 0.00581
2024-12-19 17:35:44.479743: Validation loss did not improve from -0.46269. Patience: 30/50
2024-12-19 17:35:44.480662: train_loss -0.8171
2024-12-19 17:35:44.481461: val_loss -0.3978
2024-12-19 17:35:44.482177: Pseudo dice [0.6865]
2024-12-19 17:35:44.482794: Epoch time: 321.66 s
2024-12-19 17:35:45.882968: 
2024-12-19 17:35:45.884171: Epoch 69
2024-12-19 17:35:45.884831: Current learning rate: 0.00574
2024-12-19 17:41:33.740185: Validation loss did not improve from -0.46269. Patience: 31/50
2024-12-19 17:41:33.741307: train_loss -0.8119
2024-12-19 17:41:33.742021: val_loss -0.3854
2024-12-19 17:41:33.742636: Pseudo dice [0.6836]
2024-12-19 17:41:33.743234: Epoch time: 347.86 s
2024-12-19 17:41:35.567770: 
2024-12-19 17:41:35.568949: Epoch 70
2024-12-19 17:41:35.569700: Current learning rate: 0.00568
2024-12-19 17:47:17.388044: Validation loss did not improve from -0.46269. Patience: 32/50
2024-12-19 17:47:17.390238: train_loss -0.8178
2024-12-19 17:47:17.391291: val_loss -0.416
2024-12-19 17:47:17.391901: Pseudo dice [0.6969]
2024-12-19 17:47:17.392526: Epoch time: 341.82 s
2024-12-19 17:47:18.801169: 
2024-12-19 17:47:18.802567: Epoch 71
2024-12-19 17:47:18.803470: Current learning rate: 0.00562
2024-12-19 17:53:13.966680: Validation loss did not improve from -0.46269. Patience: 33/50
2024-12-19 17:53:13.968207: train_loss -0.8161
2024-12-19 17:53:13.969194: val_loss -0.3981
2024-12-19 17:53:13.969983: Pseudo dice [0.7]
2024-12-19 17:53:13.970786: Epoch time: 355.17 s
2024-12-19 17:53:15.812254: 
2024-12-19 17:53:15.813558: Epoch 72
2024-12-19 17:53:15.814383: Current learning rate: 0.00555
2024-12-19 17:59:15.148472: Validation loss did not improve from -0.46269. Patience: 34/50
2024-12-19 17:59:15.149424: train_loss -0.8192
2024-12-19 17:59:15.150345: val_loss -0.4199
2024-12-19 17:59:15.151128: Pseudo dice [0.7048]
2024-12-19 17:59:15.151814: Epoch time: 359.34 s
2024-12-19 17:59:16.555158: 
2024-12-19 17:59:16.556446: Epoch 73
2024-12-19 17:59:16.557107: Current learning rate: 0.00549
2024-12-19 18:04:45.691027: Validation loss did not improve from -0.46269. Patience: 35/50
2024-12-19 18:04:45.692275: train_loss -0.8227
2024-12-19 18:04:45.693130: val_loss -0.4031
2024-12-19 18:04:45.693893: Pseudo dice [0.7063]
2024-12-19 18:04:45.694637: Epoch time: 329.14 s
2024-12-19 18:04:47.101075: 
2024-12-19 18:04:47.102496: Epoch 74
2024-12-19 18:04:47.103215: Current learning rate: 0.00542
2024-12-19 18:10:29.634875: Validation loss did not improve from -0.46269. Patience: 36/50
2024-12-19 18:10:29.635723: train_loss -0.8234
2024-12-19 18:10:29.636510: val_loss -0.4294
2024-12-19 18:10:29.637421: Pseudo dice [0.7091]
2024-12-19 18:10:29.638189: Epoch time: 342.54 s
2024-12-19 18:10:30.084745: Yayy! New best EMA pseudo Dice: 0.6983
2024-12-19 18:10:31.928380: 
2024-12-19 18:10:31.929875: Epoch 75
2024-12-19 18:10:31.930967: Current learning rate: 0.00536
2024-12-19 18:16:22.194851: Validation loss did not improve from -0.46269. Patience: 37/50
2024-12-19 18:16:22.196699: train_loss -0.8237
2024-12-19 18:16:22.197895: val_loss -0.4509
2024-12-19 18:16:22.198664: Pseudo dice [0.7189]
2024-12-19 18:16:22.199476: Epoch time: 350.27 s
2024-12-19 18:16:22.200341: Yayy! New best EMA pseudo Dice: 0.7003
2024-12-19 18:16:24.050342: 
2024-12-19 18:16:24.051529: Epoch 76
2024-12-19 18:16:24.052370: Current learning rate: 0.00529
2024-12-19 18:22:05.796000: Validation loss did not improve from -0.46269. Patience: 38/50
2024-12-19 18:22:05.796846: train_loss -0.8213
2024-12-19 18:22:05.798039: val_loss -0.4156
2024-12-19 18:22:05.799027: Pseudo dice [0.6939]
2024-12-19 18:22:05.800178: Epoch time: 341.75 s
2024-12-19 18:22:07.142549: 
2024-12-19 18:22:07.143938: Epoch 77
2024-12-19 18:22:07.145056: Current learning rate: 0.00523
2024-12-19 18:27:49.849707: Validation loss did not improve from -0.46269. Patience: 39/50
2024-12-19 18:27:49.850734: train_loss -0.8243
2024-12-19 18:27:49.851569: val_loss -0.3682
2024-12-19 18:27:49.852232: Pseudo dice [0.68]
2024-12-19 18:27:49.852878: Epoch time: 342.71 s
2024-12-19 18:27:51.360643: 
2024-12-19 18:27:51.361445: Epoch 78
2024-12-19 18:27:51.362088: Current learning rate: 0.00517
2024-12-19 18:33:28.969843: Validation loss did not improve from -0.46269. Patience: 40/50
2024-12-19 18:33:28.971043: train_loss -0.8248
2024-12-19 18:33:28.972622: val_loss -0.3772
2024-12-19 18:33:28.973249: Pseudo dice [0.6924]
2024-12-19 18:33:28.974158: Epoch time: 337.61 s
2024-12-19 18:33:30.385235: 
2024-12-19 18:33:30.386271: Epoch 79
2024-12-19 18:33:30.386921: Current learning rate: 0.0051
2024-12-19 18:39:05.239316: Validation loss did not improve from -0.46269. Patience: 41/50
2024-12-19 18:39:05.240097: train_loss -0.8229
2024-12-19 18:39:05.240896: val_loss -0.3829
2024-12-19 18:39:05.241599: Pseudo dice [0.6893]
2024-12-19 18:39:05.242403: Epoch time: 334.86 s
2024-12-19 18:39:07.166233: 
2024-12-19 18:39:07.167420: Epoch 80
2024-12-19 18:39:07.168220: Current learning rate: 0.00504
2024-12-19 18:44:40.100663: Validation loss did not improve from -0.46269. Patience: 42/50
2024-12-19 18:44:40.101583: train_loss -0.8258
2024-12-19 18:44:40.102630: val_loss -0.4226
2024-12-19 18:44:40.103506: Pseudo dice [0.702]
2024-12-19 18:44:40.104598: Epoch time: 332.94 s
2024-12-19 18:44:41.561402: 
2024-12-19 18:44:41.562506: Epoch 81
2024-12-19 18:44:41.563334: Current learning rate: 0.00497
2024-12-19 18:50:25.744023: Validation loss did not improve from -0.46269. Patience: 43/50
2024-12-19 18:50:25.746008: train_loss -0.8283
2024-12-19 18:50:25.747267: val_loss -0.4327
2024-12-19 18:50:25.747969: Pseudo dice [0.7182]
2024-12-19 18:50:25.748666: Epoch time: 344.19 s
2024-12-19 18:50:27.160835: 
2024-12-19 18:50:27.161864: Epoch 82
2024-12-19 18:50:27.162681: Current learning rate: 0.00491
2024-12-19 18:55:50.327707: Validation loss did not improve from -0.46269. Patience: 44/50
2024-12-19 18:55:50.328670: train_loss -0.8279
2024-12-19 18:55:50.329738: val_loss -0.3836
2024-12-19 18:55:50.330840: Pseudo dice [0.6961]
2024-12-19 18:55:50.331780: Epoch time: 323.17 s
2024-12-19 18:55:52.344850: 
2024-12-19 18:55:52.346066: Epoch 83
2024-12-19 18:55:52.347198: Current learning rate: 0.00484
2024-12-19 19:01:21.079540: Validation loss did not improve from -0.46269. Patience: 45/50
2024-12-19 19:01:21.081152: train_loss -0.8269
2024-12-19 19:01:21.082074: val_loss -0.415
2024-12-19 19:01:21.082939: Pseudo dice [0.7016]
2024-12-19 19:01:21.083834: Epoch time: 328.74 s
2024-12-19 19:01:22.416545: 
2024-12-19 19:01:22.417721: Epoch 84
2024-12-19 19:01:22.418634: Current learning rate: 0.00478
2024-12-19 19:07:10.594402: Validation loss did not improve from -0.46269. Patience: 46/50
2024-12-19 19:07:10.595401: train_loss -0.8297
2024-12-19 19:07:10.596812: val_loss -0.3486
2024-12-19 19:07:10.597977: Pseudo dice [0.6799]
2024-12-19 19:07:10.599101: Epoch time: 348.18 s
2024-12-19 19:07:12.310671: 
2024-12-19 19:07:12.311926: Epoch 85
2024-12-19 19:07:12.313029: Current learning rate: 0.00471
2024-12-19 19:13:00.125805: Validation loss did not improve from -0.46269. Patience: 47/50
2024-12-19 19:13:00.127035: train_loss -0.8308
2024-12-19 19:13:00.127925: val_loss -0.4598
2024-12-19 19:13:00.128727: Pseudo dice [0.7141]
2024-12-19 19:13:00.129385: Epoch time: 347.82 s
2024-12-19 19:13:01.441514: 
2024-12-19 19:13:01.443213: Epoch 86
2024-12-19 19:13:01.444346: Current learning rate: 0.00465
2024-12-19 19:18:17.825873: Validation loss did not improve from -0.46269. Patience: 48/50
2024-12-19 19:18:17.826781: train_loss -0.8294
2024-12-19 19:18:17.827555: val_loss -0.3979
2024-12-19 19:18:17.828138: Pseudo dice [0.6987]
2024-12-19 19:18:17.828820: Epoch time: 316.39 s
2024-12-19 19:18:19.193432: 
2024-12-19 19:18:19.194764: Epoch 87
2024-12-19 19:18:19.195405: Current learning rate: 0.00458
2024-12-19 19:20:59.542725: Validation loss did not improve from -0.46269. Patience: 49/50
2024-12-19 19:20:59.543726: train_loss -0.8288
2024-12-19 19:20:59.544692: val_loss -0.4
2024-12-19 19:20:59.545590: Pseudo dice [0.6916]
2024-12-19 19:20:59.546269: Epoch time: 160.35 s
2024-12-19 19:21:00.955204: 
2024-12-19 19:21:00.957446: Epoch 88
2024-12-19 19:21:00.958511: Current learning rate: 0.00452
2024-12-19 19:23:41.213652: Validation loss did not improve from -0.46269. Patience: 50/50
2024-12-19 19:23:41.214517: train_loss -0.8301
2024-12-19 19:23:41.215625: val_loss -0.4063
2024-12-19 19:23:41.216740: Pseudo dice [0.6998]
2024-12-19 19:23:41.217828: Epoch time: 160.26 s
2024-12-19 19:23:42.599859: 
2024-12-19 19:23:42.601141: Epoch 89
2024-12-19 19:23:42.602121: Current learning rate: 0.00445
2024-12-19 19:26:26.978587: Validation loss did not improve from -0.46269. Patience: 51/50
2024-12-19 19:26:26.979521: train_loss -0.8305
2024-12-19 19:26:26.980500: val_loss -0.3715
2024-12-19 19:26:26.981549: Pseudo dice [0.6715]
2024-12-19 19:26:26.982525: Epoch time: 164.38 s
2024-12-19 19:26:28.749211: 
2024-12-19 19:26:28.751275: Epoch 90
2024-12-19 19:26:28.752411: Current learning rate: 0.00438
2024-12-19 19:29:04.726882: Validation loss did not improve from -0.46269. Patience: 52/50
2024-12-19 19:29:04.728405: train_loss -0.8291
2024-12-19 19:29:04.729549: val_loss -0.3939
2024-12-19 19:29:04.730406: Pseudo dice [0.6921]
2024-12-19 19:29:04.731239: Epoch time: 155.98 s
2024-12-19 19:29:06.112329: 
2024-12-19 19:29:06.113605: Epoch 91
2024-12-19 19:29:06.114502: Current learning rate: 0.00432
2024-12-19 19:31:43.908272: Validation loss did not improve from -0.46269. Patience: 53/50
2024-12-19 19:31:43.909306: train_loss -0.8311
2024-12-19 19:31:43.910290: val_loss -0.3995
2024-12-19 19:31:43.911439: Pseudo dice [0.6961]
2024-12-19 19:31:43.912179: Epoch time: 157.8 s
2024-12-19 19:31:45.224859: 
2024-12-19 19:31:45.226199: Epoch 92
2024-12-19 19:31:45.227006: Current learning rate: 0.00425
2024-12-19 19:34:23.563130: Validation loss did not improve from -0.46269. Patience: 54/50
2024-12-19 19:34:23.564279: train_loss -0.8319
2024-12-19 19:34:23.565386: val_loss -0.4375
2024-12-19 19:34:23.566099: Pseudo dice [0.7141]
2024-12-19 19:34:23.566830: Epoch time: 158.34 s
2024-12-19 19:34:24.889489: 
2024-12-19 19:34:24.890563: Epoch 93
2024-12-19 19:34:24.891268: Current learning rate: 0.00419
2024-12-19 19:37:03.365625: Validation loss did not improve from -0.46269. Patience: 55/50
2024-12-19 19:37:03.367071: train_loss -0.8325
2024-12-19 19:37:03.369030: val_loss -0.3843
2024-12-19 19:37:03.369856: Pseudo dice [0.6916]
2024-12-19 19:37:03.370987: Epoch time: 158.48 s
2024-12-19 19:37:04.696797: 
2024-12-19 19:37:04.697838: Epoch 94
2024-12-19 19:37:04.698696: Current learning rate: 0.00412
2024-12-19 19:39:41.359974: Validation loss did not improve from -0.46269. Patience: 56/50
2024-12-19 19:39:41.361203: train_loss -0.8332
2024-12-19 19:39:41.362212: val_loss -0.4231
2024-12-19 19:39:41.362980: Pseudo dice [0.7025]
2024-12-19 19:39:41.363901: Epoch time: 156.67 s
2024-12-19 19:39:43.665064: 
2024-12-19 19:39:43.666854: Epoch 95
2024-12-19 19:39:43.667667: Current learning rate: 0.00405
2024-12-19 19:42:22.109656: Validation loss did not improve from -0.46269. Patience: 57/50
2024-12-19 19:42:22.110562: train_loss -0.8339
2024-12-19 19:42:22.111691: val_loss -0.3977
2024-12-19 19:42:22.112715: Pseudo dice [0.6893]
2024-12-19 19:42:22.113612: Epoch time: 158.45 s
2024-12-19 19:42:23.434165: 
2024-12-19 19:42:23.435416: Epoch 96
2024-12-19 19:42:23.436148: Current learning rate: 0.00399
2024-12-19 19:45:02.846536: Validation loss did not improve from -0.46269. Patience: 58/50
2024-12-19 19:45:02.847427: train_loss -0.8317
2024-12-19 19:45:02.848360: val_loss -0.4275
2024-12-19 19:45:02.849209: Pseudo dice [0.7084]
2024-12-19 19:45:02.850045: Epoch time: 159.41 s
2024-12-19 19:45:04.181818: 
2024-12-19 19:45:04.183407: Epoch 97
2024-12-19 19:45:04.184178: Current learning rate: 0.00392
2024-12-19 19:47:40.502054: Validation loss did not improve from -0.46269. Patience: 59/50
2024-12-19 19:47:40.503206: train_loss -0.835
2024-12-19 19:47:40.504169: val_loss -0.376
2024-12-19 19:47:40.505086: Pseudo dice [0.6972]
2024-12-19 19:47:40.505809: Epoch time: 156.32 s
2024-12-19 19:47:41.830082: 
2024-12-19 19:47:41.831024: Epoch 98
2024-12-19 19:47:41.831676: Current learning rate: 0.00385
2024-12-19 19:50:21.819011: Validation loss did not improve from -0.46269. Patience: 60/50
2024-12-19 19:50:21.820233: train_loss -0.8338
2024-12-19 19:50:21.821209: val_loss -0.3885
2024-12-19 19:50:21.821960: Pseudo dice [0.6898]
2024-12-19 19:50:21.822682: Epoch time: 159.99 s
2024-12-19 19:50:23.122872: 
2024-12-19 19:50:23.124218: Epoch 99
2024-12-19 19:50:23.125063: Current learning rate: 0.00379
2024-12-19 19:53:01.385742: Validation loss did not improve from -0.46269. Patience: 61/50
2024-12-19 19:53:01.386704: train_loss -0.8341
2024-12-19 19:53:01.387510: val_loss -0.41
2024-12-19 19:53:01.388159: Pseudo dice [0.7066]
2024-12-19 19:53:01.388996: Epoch time: 158.27 s
2024-12-19 19:53:03.117621: 
2024-12-19 19:53:03.119270: Epoch 100
2024-12-19 19:53:03.120167: Current learning rate: 0.00372
2024-12-19 19:55:40.857120: Validation loss did not improve from -0.46269. Patience: 62/50
2024-12-19 19:55:40.859777: train_loss -0.8358
2024-12-19 19:55:40.860651: val_loss -0.3589
2024-12-19 19:55:40.861278: Pseudo dice [0.6849]
2024-12-19 19:55:40.862038: Epoch time: 157.74 s
2024-12-19 19:55:42.250476: 
2024-12-19 19:55:42.252373: Epoch 101
2024-12-19 19:55:42.253068: Current learning rate: 0.00365
2024-12-19 19:58:22.635434: Validation loss did not improve from -0.46269. Patience: 63/50
2024-12-19 19:58:22.636554: train_loss -0.8354
2024-12-19 19:58:22.637368: val_loss -0.372
2024-12-19 19:58:22.638115: Pseudo dice [0.6947]
2024-12-19 19:58:22.638895: Epoch time: 160.39 s
2024-12-19 19:58:23.977799: 
2024-12-19 19:58:23.979558: Epoch 102
2024-12-19 19:58:23.980716: Current learning rate: 0.00359
2024-12-19 20:01:02.637408: Validation loss did not improve from -0.46269. Patience: 64/50
2024-12-19 20:01:02.638747: train_loss -0.8353
2024-12-19 20:01:02.639743: val_loss -0.367
2024-12-19 20:01:02.640706: Pseudo dice [0.697]
2024-12-19 20:01:02.641541: Epoch time: 158.66 s
2024-12-19 20:01:03.966844: 
2024-12-19 20:01:03.968168: Epoch 103
2024-12-19 20:01:03.968999: Current learning rate: 0.00352
2024-12-19 20:03:40.704459: Validation loss did not improve from -0.46269. Patience: 65/50
2024-12-19 20:03:40.705562: train_loss -0.8349
2024-12-19 20:03:40.706437: val_loss -0.3635
2024-12-19 20:03:40.707215: Pseudo dice [0.6832]
2024-12-19 20:03:40.708088: Epoch time: 156.74 s
2024-12-19 20:03:42.052608: 
2024-12-19 20:03:42.054027: Epoch 104
2024-12-19 20:03:42.054982: Current learning rate: 0.00345
2024-12-19 20:06:21.452113: Validation loss did not improve from -0.46269. Patience: 66/50
2024-12-19 20:06:21.453266: train_loss -0.8375
2024-12-19 20:06:21.454281: val_loss -0.3751
2024-12-19 20:06:21.455229: Pseudo dice [0.6915]
2024-12-19 20:06:21.456071: Epoch time: 159.4 s
2024-12-19 20:06:23.229614: 
2024-12-19 20:06:23.230909: Epoch 105
2024-12-19 20:06:23.231796: Current learning rate: 0.00338
2024-12-19 20:08:59.959035: Validation loss did not improve from -0.46269. Patience: 67/50
2024-12-19 20:08:59.959952: train_loss -0.8368
2024-12-19 20:08:59.960856: val_loss -0.4092
2024-12-19 20:08:59.961712: Pseudo dice [0.695]
2024-12-19 20:08:59.962367: Epoch time: 156.73 s
2024-12-19 20:09:02.250598: 
2024-12-19 20:09:02.251896: Epoch 106
2024-12-19 20:09:02.252632: Current learning rate: 0.00332
2024-12-19 20:11:39.715500: Validation loss did not improve from -0.46269. Patience: 68/50
2024-12-19 20:11:39.716197: train_loss -0.8379
2024-12-19 20:11:39.716884: val_loss -0.3762
2024-12-19 20:11:39.717464: Pseudo dice [0.6918]
2024-12-19 20:11:39.718109: Epoch time: 157.47 s
2024-12-19 20:11:41.144706: 
2024-12-19 20:11:41.146750: Epoch 107
2024-12-19 20:11:41.148298: Current learning rate: 0.00325
2024-12-19 20:14:18.657829: Validation loss did not improve from -0.46269. Patience: 69/50
2024-12-19 20:14:18.658904: train_loss -0.8387
2024-12-19 20:14:18.659637: val_loss -0.3446
2024-12-19 20:14:18.660450: Pseudo dice [0.6867]
2024-12-19 20:14:18.661231: Epoch time: 157.52 s
2024-12-19 20:14:20.012475: 
2024-12-19 20:14:20.014164: Epoch 108
2024-12-19 20:14:20.015416: Current learning rate: 0.00318
2024-12-19 20:16:57.486837: Validation loss did not improve from -0.46269. Patience: 70/50
2024-12-19 20:16:57.488326: train_loss -0.8396
2024-12-19 20:16:57.489304: val_loss -0.4015
2024-12-19 20:16:57.490048: Pseudo dice [0.6978]
2024-12-19 20:16:57.490710: Epoch time: 157.48 s
2024-12-19 20:16:58.857434: 
2024-12-19 20:16:58.858797: Epoch 109
2024-12-19 20:16:58.859665: Current learning rate: 0.00311
2024-12-19 20:19:37.357409: Validation loss did not improve from -0.46269. Patience: 71/50
2024-12-19 20:19:37.358336: train_loss -0.8403
2024-12-19 20:19:37.359054: val_loss -0.4032
2024-12-19 20:19:37.359689: Pseudo dice [0.6985]
2024-12-19 20:19:37.360422: Epoch time: 158.5 s
2024-12-19 20:19:39.078623: 
2024-12-19 20:19:39.079474: Epoch 110
2024-12-19 20:19:39.080300: Current learning rate: 0.00304
2024-12-19 20:22:15.956668: Validation loss did not improve from -0.46269. Patience: 72/50
2024-12-19 20:22:15.957598: train_loss -0.8402
2024-12-19 20:22:15.958565: val_loss -0.3706
2024-12-19 20:22:15.959356: Pseudo dice [0.6846]
2024-12-19 20:22:15.960036: Epoch time: 156.88 s
2024-12-19 20:22:17.320374: 
2024-12-19 20:22:17.322094: Epoch 111
2024-12-19 20:22:17.323006: Current learning rate: 0.00297
2024-12-19 20:23:57.546988: Validation loss did not improve from -0.46269. Patience: 73/50
2024-12-19 20:23:57.547867: train_loss -0.8405
2024-12-19 20:23:57.548828: val_loss -0.4062
2024-12-19 20:23:57.549644: Pseudo dice [0.7035]
2024-12-19 20:23:57.550492: Epoch time: 100.23 s
2024-12-19 20:23:58.873702: 
2024-12-19 20:23:58.874749: Epoch 112
2024-12-19 20:23:58.875566: Current learning rate: 0.00291
2024-12-19 20:25:27.963076: Validation loss did not improve from -0.46269. Patience: 74/50
2024-12-19 20:25:27.964049: train_loss -0.841
2024-12-19 20:25:27.964849: val_loss -0.4161
2024-12-19 20:25:27.965532: Pseudo dice [0.7097]
2024-12-19 20:25:27.966231: Epoch time: 89.09 s
2024-12-19 20:25:29.298266: 
2024-12-19 20:25:29.299945: Epoch 113
2024-12-19 20:25:29.300684: Current learning rate: 0.00284
2024-12-19 20:27:01.899909: Validation loss did not improve from -0.46269. Patience: 75/50
2024-12-19 20:27:01.900852: train_loss -0.8383
2024-12-19 20:27:01.901750: val_loss -0.3838
2024-12-19 20:27:01.902579: Pseudo dice [0.6908]
2024-12-19 20:27:01.903362: Epoch time: 92.6 s
2024-12-19 20:27:03.284111: 
2024-12-19 20:27:03.285873: Epoch 114
2024-12-19 20:27:03.286595: Current learning rate: 0.00277
2024-12-19 20:28:32.393406: Validation loss did not improve from -0.46269. Patience: 76/50
2024-12-19 20:28:32.394531: train_loss -0.8406
2024-12-19 20:28:32.395441: val_loss -0.3705
2024-12-19 20:28:32.396370: Pseudo dice [0.696]
2024-12-19 20:28:32.397359: Epoch time: 89.11 s
2024-12-19 20:28:34.175806: 
2024-12-19 20:28:34.177579: Epoch 115
2024-12-19 20:28:34.178676: Current learning rate: 0.0027
2024-12-19 20:30:05.639875: Validation loss did not improve from -0.46269. Patience: 77/50
2024-12-19 20:30:05.640882: train_loss -0.8427
2024-12-19 20:30:05.641828: val_loss -0.3793
2024-12-19 20:30:05.642598: Pseudo dice [0.6959]
2024-12-19 20:30:05.643345: Epoch time: 91.47 s
2024-12-19 20:30:06.975108: 
2024-12-19 20:30:06.976886: Epoch 116
2024-12-19 20:30:06.978019: Current learning rate: 0.00263
2024-12-19 20:31:40.717230: Validation loss did not improve from -0.46269. Patience: 78/50
2024-12-19 20:31:40.718174: train_loss -0.8406
2024-12-19 20:31:40.719010: val_loss -0.3725
2024-12-19 20:31:40.719822: Pseudo dice [0.6867]
2024-12-19 20:31:40.720755: Epoch time: 93.74 s
2024-12-19 20:31:42.627514: 
2024-12-19 20:31:42.629238: Epoch 117
2024-12-19 20:31:42.630392: Current learning rate: 0.00256
2024-12-19 20:33:09.229336: Validation loss did not improve from -0.46269. Patience: 79/50
2024-12-19 20:33:09.230353: train_loss -0.8416
2024-12-19 20:33:09.231221: val_loss -0.3688
2024-12-19 20:33:09.231968: Pseudo dice [0.6896]
2024-12-19 20:33:09.232716: Epoch time: 86.6 s
2024-12-19 20:33:10.540598: 
2024-12-19 20:33:10.543464: Epoch 118
2024-12-19 20:33:10.545355: Current learning rate: 0.00249
2024-12-19 20:34:39.459943: Validation loss did not improve from -0.46269. Patience: 80/50
2024-12-19 20:34:39.460711: train_loss -0.8419
2024-12-19 20:34:39.461636: val_loss -0.3776
2024-12-19 20:34:39.462301: Pseudo dice [0.6946]
2024-12-19 20:34:39.462908: Epoch time: 88.92 s
2024-12-19 20:34:40.762437: 
2024-12-19 20:34:40.764791: Epoch 119
2024-12-19 20:34:40.765740: Current learning rate: 0.00242
2024-12-19 20:36:07.499645: Validation loss did not improve from -0.46269. Patience: 81/50
2024-12-19 20:36:07.500684: train_loss -0.8428
2024-12-19 20:36:07.501814: val_loss -0.3749
2024-12-19 20:36:07.503083: Pseudo dice [0.6954]
2024-12-19 20:36:07.504327: Epoch time: 86.74 s
2024-12-19 20:36:09.177580: 
2024-12-19 20:36:09.179155: Epoch 120
2024-12-19 20:36:09.180078: Current learning rate: 0.00235
2024-12-19 20:37:36.198413: Validation loss did not improve from -0.46269. Patience: 82/50
2024-12-19 20:37:36.199279: train_loss -0.8419
2024-12-19 20:37:36.200113: val_loss -0.3897
2024-12-19 20:37:36.200861: Pseudo dice [0.6963]
2024-12-19 20:37:36.201716: Epoch time: 87.02 s
2024-12-19 20:37:37.477869: 
2024-12-19 20:37:37.480151: Epoch 121
2024-12-19 20:37:37.481049: Current learning rate: 0.00228
2024-12-19 20:39:04.348616: Validation loss did not improve from -0.46269. Patience: 83/50
2024-12-19 20:39:04.349936: train_loss -0.843
2024-12-19 20:39:04.351132: val_loss -0.3992
2024-12-19 20:39:04.352167: Pseudo dice [0.6997]
2024-12-19 20:39:04.352882: Epoch time: 86.87 s
2024-12-19 20:39:05.568721: 
2024-12-19 20:39:05.570785: Epoch 122
2024-12-19 20:39:05.571659: Current learning rate: 0.00221
2024-12-19 20:40:32.539786: Validation loss did not improve from -0.46269. Patience: 84/50
2024-12-19 20:40:32.541027: train_loss -0.8422
2024-12-19 20:40:32.542186: val_loss -0.379
2024-12-19 20:40:32.542991: Pseudo dice [0.6914]
2024-12-19 20:40:32.543857: Epoch time: 86.97 s
2024-12-19 20:40:33.790504: 
2024-12-19 20:40:33.792938: Epoch 123
2024-12-19 20:40:33.794227: Current learning rate: 0.00214
2024-12-19 20:42:00.881041: Validation loss did not improve from -0.46269. Patience: 85/50
2024-12-19 20:42:00.882105: train_loss -0.8444
2024-12-19 20:42:00.883137: val_loss -0.3503
2024-12-19 20:42:00.883952: Pseudo dice [0.6892]
2024-12-19 20:42:00.884774: Epoch time: 87.09 s
2024-12-19 20:42:02.127876: 
2024-12-19 20:42:02.129879: Epoch 124
2024-12-19 20:42:02.131161: Current learning rate: 0.00207
2024-12-19 20:43:29.307172: Validation loss did not improve from -0.46269. Patience: 86/50
2024-12-19 20:43:29.308336: train_loss -0.8418
2024-12-19 20:43:29.309459: val_loss -0.3874
2024-12-19 20:43:29.310173: Pseudo dice [0.695]
2024-12-19 20:43:29.310892: Epoch time: 87.18 s
2024-12-19 20:43:30.952881: 
2024-12-19 20:43:30.955601: Epoch 125
2024-12-19 20:43:30.957006: Current learning rate: 0.00199
2024-12-19 20:44:58.077346: Validation loss did not improve from -0.46269. Patience: 87/50
2024-12-19 20:44:58.078384: train_loss -0.8431
2024-12-19 20:44:58.079316: val_loss -0.348
2024-12-19 20:44:58.080177: Pseudo dice [0.682]
2024-12-19 20:44:58.080900: Epoch time: 87.13 s
2024-12-19 20:44:59.323303: 
2024-12-19 20:44:59.325284: Epoch 126
2024-12-19 20:44:59.326282: Current learning rate: 0.00192
2024-12-19 20:46:26.407358: Validation loss did not improve from -0.46269. Patience: 88/50
2024-12-19 20:46:26.408370: train_loss -0.8422
2024-12-19 20:46:26.409326: val_loss -0.4028
2024-12-19 20:46:26.410238: Pseudo dice [0.698]
2024-12-19 20:46:26.411121: Epoch time: 87.09 s
2024-12-19 20:46:27.616683: 
2024-12-19 20:46:27.618513: Epoch 127
2024-12-19 20:46:27.619721: Current learning rate: 0.00185
2024-12-19 20:47:54.725235: Validation loss did not improve from -0.46269. Patience: 89/50
2024-12-19 20:47:54.726412: train_loss -0.8452
2024-12-19 20:47:54.727106: val_loss -0.3594
2024-12-19 20:47:54.727728: Pseudo dice [0.6953]
2024-12-19 20:47:54.728357: Epoch time: 87.11 s
2024-12-19 20:47:56.462945: 
2024-12-19 20:47:56.464290: Epoch 128
2024-12-19 20:47:56.465167: Current learning rate: 0.00178
2024-12-19 20:49:23.550198: Validation loss did not improve from -0.46269. Patience: 90/50
2024-12-19 20:49:23.551413: train_loss -0.8442
2024-12-19 20:49:23.552886: val_loss -0.4127
2024-12-19 20:49:23.554020: Pseudo dice [0.7021]
2024-12-19 20:49:23.555030: Epoch time: 87.09 s
2024-12-19 20:49:24.738552: 
2024-12-19 20:49:24.741029: Epoch 129
2024-12-19 20:49:24.742499: Current learning rate: 0.0017
2024-12-19 20:50:51.809207: Validation loss did not improve from -0.46269. Patience: 91/50
2024-12-19 20:50:51.810556: train_loss -0.8447
2024-12-19 20:50:51.812109: val_loss -0.4324
2024-12-19 20:50:51.813440: Pseudo dice [0.7118]
2024-12-19 20:50:51.814826: Epoch time: 87.07 s
2024-12-19 20:50:53.420730: 
2024-12-19 20:50:53.422596: Epoch 130
2024-12-19 20:50:53.423881: Current learning rate: 0.00163
2024-12-19 20:52:20.621094: Validation loss did not improve from -0.46269. Patience: 92/50
2024-12-19 20:52:20.622745: train_loss -0.8447
2024-12-19 20:52:20.624567: val_loss -0.3641
2024-12-19 20:52:20.625619: Pseudo dice [0.6859]
2024-12-19 20:52:20.626544: Epoch time: 87.2 s
2024-12-19 20:52:21.866276: 
2024-12-19 20:52:21.868119: Epoch 131
2024-12-19 20:52:21.868986: Current learning rate: 0.00156
2024-12-19 20:53:49.188812: Validation loss did not improve from -0.46269. Patience: 93/50
2024-12-19 20:53:49.189743: train_loss -0.8452
2024-12-19 20:53:49.190713: val_loss -0.3412
2024-12-19 20:53:49.191489: Pseudo dice [0.6795]
2024-12-19 20:53:49.192250: Epoch time: 87.32 s
2024-12-19 20:53:50.419894: 
2024-12-19 20:53:50.422068: Epoch 132
2024-12-19 20:53:50.423551: Current learning rate: 0.00148
2024-12-19 20:55:17.822524: Validation loss did not improve from -0.46269. Patience: 94/50
2024-12-19 20:55:17.823400: train_loss -0.8455
2024-12-19 20:55:17.824408: val_loss -0.3718
2024-12-19 20:55:17.825490: Pseudo dice [0.6835]
2024-12-19 20:55:17.826590: Epoch time: 87.4 s
2024-12-19 20:55:19.055602: 
2024-12-19 20:55:19.057375: Epoch 133
2024-12-19 20:55:19.058073: Current learning rate: 0.00141
2024-12-19 20:56:46.486342: Validation loss did not improve from -0.46269. Patience: 95/50
2024-12-19 20:56:46.487669: train_loss -0.8428
2024-12-19 20:56:46.489107: val_loss -0.3894
2024-12-19 20:56:46.489960: Pseudo dice [0.6982]
2024-12-19 20:56:46.490990: Epoch time: 87.43 s
2024-12-19 20:56:47.705805: 
2024-12-19 20:56:47.707352: Epoch 134
2024-12-19 20:56:47.708353: Current learning rate: 0.00133
2024-12-19 20:58:14.900736: Validation loss did not improve from -0.46269. Patience: 96/50
2024-12-19 20:58:14.901972: train_loss -0.8428
2024-12-19 20:58:14.902940: val_loss -0.3877
2024-12-19 20:58:14.903837: Pseudo dice [0.6948]
2024-12-19 20:58:14.904640: Epoch time: 87.2 s
2024-12-19 20:58:16.490834: 
2024-12-19 20:58:16.492352: Epoch 135
2024-12-19 20:58:16.493810: Current learning rate: 0.00126
2024-12-19 20:59:43.515152: Validation loss did not improve from -0.46269. Patience: 97/50
2024-12-19 20:59:43.515782: train_loss -0.8487
2024-12-19 20:59:43.516654: val_loss -0.3964
2024-12-19 20:59:43.517452: Pseudo dice [0.6945]
2024-12-19 20:59:43.518159: Epoch time: 87.03 s
2024-12-19 20:59:44.807617: 
2024-12-19 20:59:44.809711: Epoch 136
2024-12-19 20:59:44.811193: Current learning rate: 0.00118
2024-12-19 21:01:11.931732: Validation loss did not improve from -0.46269. Patience: 98/50
2024-12-19 21:01:11.934714: train_loss -0.8446
2024-12-19 21:01:11.936405: val_loss -0.3573
2024-12-19 21:01:11.937109: Pseudo dice [0.6842]
2024-12-19 21:01:11.937827: Epoch time: 87.13 s
2024-12-19 21:01:13.235621: 
2024-12-19 21:01:13.237657: Epoch 137
2024-12-19 21:01:13.239083: Current learning rate: 0.00111
2024-12-19 21:02:40.465308: Validation loss did not improve from -0.46269. Patience: 99/50
2024-12-19 21:02:40.466482: train_loss -0.8461
2024-12-19 21:02:40.467145: val_loss -0.3962
2024-12-19 21:02:40.467780: Pseudo dice [0.7024]
2024-12-19 21:02:40.468405: Epoch time: 87.23 s
2024-12-19 21:02:41.729254: 
2024-12-19 21:02:41.730785: Epoch 138
2024-12-19 21:02:41.731611: Current learning rate: 0.00103
2024-12-19 21:04:08.775899: Validation loss did not improve from -0.46269. Patience: 100/50
2024-12-19 21:04:08.776727: train_loss -0.8445
2024-12-19 21:04:08.777847: val_loss -0.3977
2024-12-19 21:04:08.778796: Pseudo dice [0.6994]
2024-12-19 21:04:08.779865: Epoch time: 87.05 s
2024-12-19 21:04:10.393661: 
2024-12-19 21:04:10.395346: Epoch 139
2024-12-19 21:04:10.396407: Current learning rate: 0.00095
2024-12-19 21:05:37.409821: Validation loss did not improve from -0.46269. Patience: 101/50
2024-12-19 21:05:37.410657: train_loss -0.8447
2024-12-19 21:05:37.411666: val_loss -0.3829
2024-12-19 21:05:37.412465: Pseudo dice [0.6927]
2024-12-19 21:05:37.413285: Epoch time: 87.02 s
2024-12-19 21:05:39.009953: 
2024-12-19 21:05:39.011554: Epoch 140
2024-12-19 21:05:39.012459: Current learning rate: 0.00087
2024-12-19 21:07:06.080512: Validation loss did not improve from -0.46269. Patience: 102/50
2024-12-19 21:07:06.082082: train_loss -0.8458
2024-12-19 21:07:06.083293: val_loss -0.3735
2024-12-19 21:07:06.084194: Pseudo dice [0.6978]
2024-12-19 21:07:06.084918: Epoch time: 87.07 s
2024-12-19 21:07:07.342764: 
2024-12-19 21:07:07.344455: Epoch 141
2024-12-19 21:07:07.345537: Current learning rate: 0.00079
2024-12-19 21:08:34.403965: Validation loss did not improve from -0.46269. Patience: 103/50
2024-12-19 21:08:34.404776: train_loss -0.8472
2024-12-19 21:08:34.405660: val_loss -0.3841
2024-12-19 21:08:34.406472: Pseudo dice [0.6961]
2024-12-19 21:08:34.407122: Epoch time: 87.06 s
2024-12-19 21:08:35.661602: 
2024-12-19 21:08:35.663241: Epoch 142
2024-12-19 21:08:35.664349: Current learning rate: 0.00071
2024-12-19 21:10:02.398475: Validation loss did not improve from -0.46269. Patience: 104/50
2024-12-19 21:10:02.399406: train_loss -0.8462
2024-12-19 21:10:02.400133: val_loss -0.3736
2024-12-19 21:10:02.400857: Pseudo dice [0.6922]
2024-12-19 21:10:02.401614: Epoch time: 86.74 s
2024-12-19 21:10:03.663486: 
2024-12-19 21:10:03.665007: Epoch 143
2024-12-19 21:10:03.665843: Current learning rate: 0.00063
2024-12-19 21:11:30.307316: Validation loss did not improve from -0.46269. Patience: 105/50
2024-12-19 21:11:30.308191: train_loss -0.8478
2024-12-19 21:11:30.309190: val_loss -0.334
2024-12-19 21:11:30.310113: Pseudo dice [0.6784]
2024-12-19 21:11:30.310919: Epoch time: 86.65 s
2024-12-19 21:11:31.560573: 
2024-12-19 21:11:31.562761: Epoch 144
2024-12-19 21:11:31.563960: Current learning rate: 0.00055
2024-12-19 21:12:58.183281: Validation loss did not improve from -0.46269. Patience: 106/50
2024-12-19 21:12:58.184674: train_loss -0.8496
2024-12-19 21:12:58.185479: val_loss -0.3782
2024-12-19 21:12:58.186200: Pseudo dice [0.6988]
2024-12-19 21:12:58.186791: Epoch time: 86.63 s
2024-12-19 21:12:59.792319: 
2024-12-19 21:12:59.793935: Epoch 145
2024-12-19 21:12:59.795263: Current learning rate: 0.00047
2024-12-19 21:14:26.550073: Validation loss did not improve from -0.46269. Patience: 107/50
2024-12-19 21:14:26.551588: train_loss -0.8484
2024-12-19 21:14:26.552566: val_loss -0.3677
2024-12-19 21:14:26.553380: Pseudo dice [0.6977]
2024-12-19 21:14:26.554015: Epoch time: 86.76 s
2024-12-19 21:14:27.782972: 
2024-12-19 21:14:27.784817: Epoch 146
2024-12-19 21:14:27.785875: Current learning rate: 0.00038
2024-12-19 21:15:54.474463: Validation loss did not improve from -0.46269. Patience: 108/50
2024-12-19 21:15:54.475178: train_loss -0.8477
2024-12-19 21:15:54.476014: val_loss -0.3786
2024-12-19 21:15:54.476622: Pseudo dice [0.6909]
2024-12-19 21:15:54.477229: Epoch time: 86.69 s
2024-12-19 21:15:55.719349: 
2024-12-19 21:15:55.721412: Epoch 147
2024-12-19 21:15:55.722341: Current learning rate: 0.0003
2024-12-19 21:17:22.365618: Validation loss did not improve from -0.46269. Patience: 109/50
2024-12-19 21:17:22.366670: train_loss -0.8483
2024-12-19 21:17:22.367647: val_loss -0.4064
2024-12-19 21:17:22.368498: Pseudo dice [0.7109]
2024-12-19 21:17:22.369398: Epoch time: 86.65 s
2024-12-19 21:17:23.605718: 
2024-12-19 21:17:23.607346: Epoch 148
2024-12-19 21:17:23.608559: Current learning rate: 0.00021
2024-12-19 21:18:50.383715: Validation loss did not improve from -0.46269. Patience: 110/50
2024-12-19 21:18:50.384992: train_loss -0.8485
2024-12-19 21:18:50.385930: val_loss -0.4065
2024-12-19 21:18:50.386647: Pseudo dice [0.6949]
2024-12-19 21:18:50.387274: Epoch time: 86.78 s
2024-12-19 21:18:51.627151: 
2024-12-19 21:18:51.629360: Epoch 149
2024-12-19 21:18:51.630184: Current learning rate: 0.00011
2024-12-19 21:20:18.489122: Validation loss did not improve from -0.46269. Patience: 111/50
2024-12-19 21:20:18.490139: train_loss -0.8487
2024-12-19 21:20:18.491421: val_loss -0.3904
2024-12-19 21:20:18.492496: Pseudo dice [0.6981]
2024-12-19 21:20:18.493570: Epoch time: 86.86 s
2024-12-19 21:20:20.679196: Training done.
2024-12-19 21:20:20.893693: Using splits from existing split file: /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset309_Sohee_Calcium_OCT_CrossValidation/splits_final_20.json
2024-12-19 21:20:20.929467: The split file contains 5 splits.
2024-12-19 21:20:20.930567: Desired fold for training: 4
2024-12-19 21:20:20.931513: This split has 1 training and 7 validation cases.
2024-12-19 21:20:20.932638: predicting 101-019
2024-12-19 21:20:20.954319: 101-019, shape torch.Size([1, 375, 498, 498]), rank 0
2024-12-19 21:22:06.723638: predicting 101-044
2024-12-19 21:22:06.756417: 101-044, shape torch.Size([1, 404, 498, 498]), rank 0
2024-12-19 21:23:43.586358: predicting 101-045
2024-12-19 21:23:43.620103: 101-045, shape torch.Size([1, 375, 498, 498]), rank 0
2024-12-19 21:25:12.322182: predicting 401-004
2024-12-19 21:25:12.365663: 401-004, shape torch.Size([1, 375, 498, 498]), rank 0
2024-12-19 21:26:41.327159: predicting 701-013
2024-12-19 21:26:41.358241: 701-013, shape torch.Size([1, 375, 498, 498]), rank 0
2024-12-19 21:28:11.761791: predicting 704-003
2024-12-19 21:28:11.789434: 704-003, shape torch.Size([1, 375, 498, 498]), rank 0
2024-12-19 21:29:40.695739: predicting 706-005
2024-12-19 21:29:40.730080: 706-005, shape torch.Size([1, 375, 498, 498]), rank 0
2024-12-19 21:31:34.220687: Validation complete
2024-12-19 21:31:34.222190: Mean Validation Dice:  0.6791793240376609
