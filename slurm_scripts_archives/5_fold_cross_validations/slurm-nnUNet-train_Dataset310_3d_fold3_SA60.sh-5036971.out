/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/bin/python
Starting training with CONFIG=3d_32x160x128_b10, DATASET_ID=310, TRAINER=nnUNetTrainerScaleAnalysis60
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/nnunetv2/training/nnUNetTrainer/nnUNetTrainer.py:169: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  self.grad_scaler = GradScaler() if self.device.type == 'cuda' else None

############################
INFO: You are using the old nnU-Net default plans. We have updated our recommendations. Please consider using those instead! Read more here: https://github.com/MIC-DKFZ/nnUNet/blob/master/documentation/resenc_presets.md
############################

Using device: cuda:0

#######################################################################
Please cite the following paper when using nnU-Net:
Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.
#######################################################################

2025-10-05 23:06:37.255441: do_dummy_2d_data_aug: True
2025-10-05 23:06:37.255876: Using splits from existing split file: /nfs/erelab001/shared/Computational_Group/Naravich/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset310_nnInteractive_Calcium_OCT_CrossValidation/splits_final_60.json
2025-10-05 23:06:37.256209: The split file contains 5 splits.
2025-10-05 23:06:37.256393: Desired fold for training: 3
2025-10-05 23:06:37.256572: This split has 4 training and 5 validation cases.
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/fft_conv_pytorch/fft_conv.py:139: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:306.)
  output = output[crop_slices].contiguous()
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/fft_conv_pytorch/fft_conv.py:139: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:306.)
  output = output[crop_slices].contiguous()
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/fft_conv_pytorch/fft_conv.py:139: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:306.)
  output = output[crop_slices].contiguous()
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/fft_conv_pytorch/fft_conv.py:139: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:306.)
  output = output[crop_slices].contiguous()
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/fft_conv_pytorch/fft_conv.py:139: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:306.)
  output = output[crop_slices].contiguous()
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/fft_conv_pytorch/fft_conv.py:139: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:306.)
  output = output[crop_slices].contiguous()
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/fft_conv_pytorch/fft_conv.py:139: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:306.)
  output = output[crop_slices].contiguous()
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/fft_conv_pytorch/fft_conv.py:139: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:306.)
  output = output[crop_slices].contiguous()
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/fft_conv_pytorch/fft_conv.py:139: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:306.)
  output = output[crop_slices].contiguous()
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/fft_conv_pytorch/fft_conv.py:139: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:306.)
  output = output[crop_slices].contiguous()
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/fft_conv_pytorch/fft_conv.py:139: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:306.)
  output = output[crop_slices].contiguous()
using pin_memory on device 0
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/fft_conv_pytorch/fft_conv.py:139: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:306.)
  output = output[crop_slices].contiguous()
using pin_memory on device 0
2025-10-05 23:06:41.610168: Using torch.compile...

This is the configuration used by this training:
Configuration name: 3d_32x160x128_b10
 {'data_identifier': 'nnUNetPlans_3d_fullres', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 10, 'patch_size': [32, 160, 128], 'median_image_size_in_voxels': [375.0, 498.0, 498.0], 'spacing': [1.0, 1.0, 1.0], 'normalization_schemes': ['ZScoreNormalization'], 'use_mask_for_norm': [False], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'architecture': {'network_class_name': 'dynamic_network_architectures.architectures.unet.PlainConvUNet', 'arch_kwargs': {'n_stages': 6, 'features_per_stage': [32, 64, 128, 256, 320, 320], 'conv_op': 'torch.nn.modules.conv.Conv3d', 'kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]], 'strides': [[1, 1, 1], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [1, 2, 2]], 'n_conv_per_stage': [2, 2, 2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2, 2, 2], 'conv_bias': True, 'norm_op': 'torch.nn.modules.instancenorm.InstanceNorm3d', 'norm_op_kwargs': {'eps': 1e-05, 'affine': True}, 'dropout_op': None, 'dropout_op_kwargs': None, 'nonlin': 'torch.nn.LeakyReLU', 'nonlin_kwargs': {'inplace': True}, 'deep_supervision': True}, '_kw_requires_import': ['conv_op', 'norm_op', 'dropout_op', 'nonlin']}, 'batch_dice': True, 'inherits_from': '3d_fullres'} 

These are the global plan.json settings:
 {'dataset_name': 'Dataset310_nnInteractive_Calcium_OCT_CrossValidation', 'plans_name': 'nnUNetPlans', 'original_median_spacing_after_transp': [1.0, 1.0, 1.0], 'original_median_shape_after_transp': [375, 498, 498], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 0.4977976381778717, 'mean': 0.1394134759902954, 'median': 0.09849607944488525, 'min': 0.0, 'percentile_00_5': 0.015305490233004093, 'percentile_99_5': 0.4977976381778717, 'std': 0.121165432035923}}} 

2025-10-05 23:06:42.863252: unpacking dataset...
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/torch/_inductor/lowering.py:7095: UserWarning: 
Online softmax is disabled on the fly since Inductor decides to
split the reduction. Cut an issue to PyTorch if this is an
important use case and you want to speed it up with online
softmax.

  warnings.warn(
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/torch/_inductor/lowering.py:7095: UserWarning: 
Online softmax is disabled on the fly since Inductor decides to
split the reduction. Cut an issue to PyTorch if this is an
important use case and you want to speed it up with online
softmax.

  warnings.warn(
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/torch/_inductor/lowering.py:7095: UserWarning: 
Online softmax is disabled on the fly since Inductor decides to
split the reduction. Cut an issue to PyTorch if this is an
important use case and you want to speed it up with online
softmax.

  warnings.warn(
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/torch/_inductor/lowering.py:7095: UserWarning: 
Online softmax is disabled on the fly since Inductor decides to
split the reduction. Cut an issue to PyTorch if this is an
important use case and you want to speed it up with online
softmax.

  warnings.warn(
2025-10-05 23:06:46.857701: unpacking done...
2025-10-05 23:06:46.859765: Unable to plot network architecture: nnUNet_compile is enabled!
2025-10-05 23:06:46.864278: 
2025-10-05 23:06:46.864515: Epoch 0
2025-10-05 23:06:46.864731: Current learning rate: 0.01
2025-10-05 23:08:05.595173: Validation loss improved from 1000.00000 to -0.06460! Patience: 0/50
2025-10-05 23:08:05.595819: train_loss -0.1674
2025-10-05 23:08:05.596023: val_loss -0.0646
2025-10-05 23:08:05.596180: Pseudo dice [np.float32(0.4793)]
2025-10-05 23:08:05.596424: Epoch time: 78.73 s
2025-10-05 23:08:05.596916: Yayy! New best EMA pseudo Dice: 0.47929999232292175
2025-10-05 23:08:06.511661: 
2025-10-05 23:08:06.512020: Epoch 1
2025-10-05 23:08:06.512223: Current learning rate: 0.00994
2025-10-05 23:08:52.442548: Validation loss improved from -0.06460 to -0.15800! Patience: 0/50
2025-10-05 23:08:52.443026: train_loss -0.2799
2025-10-05 23:08:52.443193: val_loss -0.158
2025-10-05 23:08:52.443383: Pseudo dice [np.float32(0.5322)]
2025-10-05 23:08:52.443522: Epoch time: 45.93 s
2025-10-05 23:08:52.443664: Yayy! New best EMA pseudo Dice: 0.484499990940094
2025-10-05 23:08:53.493913: 
2025-10-05 23:08:53.494231: Epoch 2
2025-10-05 23:08:53.494402: Current learning rate: 0.00988
2025-10-05 23:09:39.540220: Validation loss did not improve from -0.15800. Patience: 1/50
2025-10-05 23:09:39.540933: train_loss -0.313
2025-10-05 23:09:39.541235: val_loss -0.0925
2025-10-05 23:09:39.541477: Pseudo dice [np.float32(0.5026)]
2025-10-05 23:09:39.541736: Epoch time: 46.05 s
2025-10-05 23:09:39.541965: Yayy! New best EMA pseudo Dice: 0.4864000082015991
2025-10-05 23:09:40.618010: 
2025-10-05 23:09:40.618250: Epoch 3
2025-10-05 23:09:40.618425: Current learning rate: 0.00982
2025-10-05 23:10:26.706169: Validation loss did not improve from -0.15800. Patience: 2/50
2025-10-05 23:10:26.706537: train_loss -0.3508
2025-10-05 23:10:26.706707: val_loss -0.1457
2025-10-05 23:10:26.706879: Pseudo dice [np.float32(0.5477)]
2025-10-05 23:10:26.707093: Epoch time: 46.09 s
2025-10-05 23:10:26.707238: Yayy! New best EMA pseudo Dice: 0.4925000071525574
2025-10-05 23:10:27.771586: 
2025-10-05 23:10:27.771842: Epoch 4
2025-10-05 23:10:27.772058: Current learning rate: 0.00976
2025-10-05 23:11:13.829854: Validation loss improved from -0.15800 to -0.24913! Patience: 2/50
2025-10-05 23:11:13.830431: train_loss -0.4385
2025-10-05 23:11:13.830583: val_loss -0.2491
2025-10-05 23:11:13.830703: Pseudo dice [np.float32(0.6)]
2025-10-05 23:11:13.830860: Epoch time: 46.06 s
2025-10-05 23:11:14.209767: Yayy! New best EMA pseudo Dice: 0.5031999945640564
2025-10-05 23:11:15.268266: 
2025-10-05 23:11:15.268590: Epoch 5
2025-10-05 23:11:15.268760: Current learning rate: 0.0097
2025-10-05 23:12:01.322771: Validation loss did not improve from -0.24913. Patience: 1/50
2025-10-05 23:12:01.323225: train_loss -0.45
2025-10-05 23:12:01.323393: val_loss -0.2462
2025-10-05 23:12:01.323519: Pseudo dice [np.float32(0.5739)]
2025-10-05 23:12:01.323677: Epoch time: 46.06 s
2025-10-05 23:12:01.323839: Yayy! New best EMA pseudo Dice: 0.5102999806404114
2025-10-05 23:12:02.415169: 
2025-10-05 23:12:02.415507: Epoch 6
2025-10-05 23:12:02.415760: Current learning rate: 0.00964
2025-10-05 23:12:48.487650: Validation loss improved from -0.24913 to -0.24914! Patience: 1/50
2025-10-05 23:12:48.488363: train_loss -0.4777
2025-10-05 23:12:48.488555: val_loss -0.2491
2025-10-05 23:12:48.488708: Pseudo dice [np.float32(0.6115)]
2025-10-05 23:12:48.488868: Epoch time: 46.07 s
2025-10-05 23:12:48.489018: Yayy! New best EMA pseudo Dice: 0.5203999876976013
2025-10-05 23:12:49.552949: 
2025-10-05 23:12:49.553238: Epoch 7
2025-10-05 23:12:49.553497: Current learning rate: 0.00958
2025-10-05 23:13:35.628414: Validation loss did not improve from -0.24914. Patience: 1/50
2025-10-05 23:13:35.628786: train_loss -0.49
2025-10-05 23:13:35.628986: val_loss -0.2316
2025-10-05 23:13:35.629122: Pseudo dice [np.float32(0.6075)]
2025-10-05 23:13:35.629268: Epoch time: 46.08 s
2025-10-05 23:13:35.629394: Yayy! New best EMA pseudo Dice: 0.5291000008583069
2025-10-05 23:13:36.700168: 
2025-10-05 23:13:36.700561: Epoch 8
2025-10-05 23:13:36.700837: Current learning rate: 0.00952
2025-10-05 23:14:22.826667: Validation loss improved from -0.24914 to -0.25705! Patience: 1/50
2025-10-05 23:14:22.827279: train_loss -0.523
2025-10-05 23:14:22.827450: val_loss -0.257
2025-10-05 23:14:22.827602: Pseudo dice [np.float32(0.622)]
2025-10-05 23:14:22.827779: Epoch time: 46.13 s
2025-10-05 23:14:22.827953: Yayy! New best EMA pseudo Dice: 0.5383999943733215
2025-10-05 23:14:23.900956: 
2025-10-05 23:14:23.901376: Epoch 9
2025-10-05 23:14:23.901563: Current learning rate: 0.00946
2025-10-05 23:15:10.020633: Validation loss did not improve from -0.25705. Patience: 1/50
2025-10-05 23:15:10.021125: train_loss -0.5194
2025-10-05 23:15:10.021333: val_loss -0.2143
2025-10-05 23:15:10.021478: Pseudo dice [np.float32(0.6152)]
2025-10-05 23:15:10.021666: Epoch time: 46.12 s
2025-10-05 23:15:10.500582: Yayy! New best EMA pseudo Dice: 0.5461000204086304
2025-10-05 23:15:11.552742: 
2025-10-05 23:15:11.552985: Epoch 10
2025-10-05 23:15:11.553228: Current learning rate: 0.0094
2025-10-05 23:15:57.710716: Validation loss improved from -0.25705 to -0.26896! Patience: 1/50
2025-10-05 23:15:57.711601: train_loss -0.5429
2025-10-05 23:15:57.711750: val_loss -0.269
2025-10-05 23:15:57.711894: Pseudo dice [np.float32(0.6287)]
2025-10-05 23:15:57.712054: Epoch time: 46.16 s
2025-10-05 23:15:57.712183: Yayy! New best EMA pseudo Dice: 0.5544000267982483
2025-10-05 23:15:58.764918: 
2025-10-05 23:15:58.765254: Epoch 11
2025-10-05 23:15:58.765468: Current learning rate: 0.00934
2025-10-05 23:16:44.906805: Validation loss improved from -0.26896 to -0.26988! Patience: 0/50
2025-10-05 23:16:44.907223: train_loss -0.5553
2025-10-05 23:16:44.907431: val_loss -0.2699
2025-10-05 23:16:44.907573: Pseudo dice [np.float32(0.6085)]
2025-10-05 23:16:44.907714: Epoch time: 46.14 s
2025-10-05 23:16:44.907836: Yayy! New best EMA pseudo Dice: 0.5598000288009644
2025-10-05 23:16:46.368219: 
2025-10-05 23:16:46.368550: Epoch 12
2025-10-05 23:16:46.368703: Current learning rate: 0.00928
2025-10-05 23:17:32.488476: Validation loss did not improve from -0.26988. Patience: 1/50
2025-10-05 23:17:32.489147: train_loss -0.548
2025-10-05 23:17:32.489352: val_loss -0.2686
2025-10-05 23:17:32.489542: Pseudo dice [np.float32(0.632)]
2025-10-05 23:17:32.489698: Epoch time: 46.12 s
2025-10-05 23:17:32.489859: Yayy! New best EMA pseudo Dice: 0.5669999718666077
2025-10-05 23:17:33.565806: 
2025-10-05 23:17:33.566137: Epoch 13
2025-10-05 23:17:33.566382: Current learning rate: 0.00922
2025-10-05 23:18:19.648095: Validation loss did not improve from -0.26988. Patience: 2/50
2025-10-05 23:18:19.648547: train_loss -0.5559
2025-10-05 23:18:19.648731: val_loss -0.2665
2025-10-05 23:18:19.648942: Pseudo dice [np.float32(0.626)]
2025-10-05 23:18:19.649124: Epoch time: 46.08 s
2025-10-05 23:18:19.649289: Yayy! New best EMA pseudo Dice: 0.5728999972343445
2025-10-05 23:18:20.746973: 
2025-10-05 23:18:20.747269: Epoch 14
2025-10-05 23:18:20.747530: Current learning rate: 0.00916
2025-10-05 23:19:06.816745: Validation loss did not improve from -0.26988. Patience: 3/50
2025-10-05 23:19:06.817444: train_loss -0.5741
2025-10-05 23:19:06.817647: val_loss -0.2504
2025-10-05 23:19:06.817835: Pseudo dice [np.float32(0.6127)]
2025-10-05 23:19:06.818111: Epoch time: 46.07 s
2025-10-05 23:19:07.270449: Yayy! New best EMA pseudo Dice: 0.5769000053405762
2025-10-05 23:19:08.334030: 
2025-10-05 23:19:08.334486: Epoch 15
2025-10-05 23:19:08.334762: Current learning rate: 0.0091
2025-10-05 23:19:54.378175: Validation loss improved from -0.26988 to -0.29571! Patience: 3/50
2025-10-05 23:19:54.378615: train_loss -0.5869
2025-10-05 23:19:54.378815: val_loss -0.2957
2025-10-05 23:19:54.379006: Pseudo dice [np.float32(0.6402)]
2025-10-05 23:19:54.379223: Epoch time: 46.05 s
2025-10-05 23:19:54.379398: Yayy! New best EMA pseudo Dice: 0.5831999778747559
2025-10-05 23:19:55.480115: 
2025-10-05 23:19:55.480436: Epoch 16
2025-10-05 23:19:55.480691: Current learning rate: 0.00903
2025-10-05 23:20:41.616252: Validation loss improved from -0.29571 to -0.31927! Patience: 0/50
2025-10-05 23:20:41.617006: train_loss -0.5956
2025-10-05 23:20:41.617250: val_loss -0.3193
2025-10-05 23:20:41.617453: Pseudo dice [np.float32(0.6513)]
2025-10-05 23:20:41.617682: Epoch time: 46.14 s
2025-10-05 23:20:41.617899: Yayy! New best EMA pseudo Dice: 0.5899999737739563
2025-10-05 23:20:42.729995: 
2025-10-05 23:20:42.730391: Epoch 17
2025-10-05 23:20:42.730664: Current learning rate: 0.00897
2025-10-05 23:21:28.863534: Validation loss did not improve from -0.31927. Patience: 1/50
2025-10-05 23:21:28.863991: train_loss -0.5963
2025-10-05 23:21:28.864168: val_loss -0.2417
2025-10-05 23:21:28.864317: Pseudo dice [np.float32(0.6428)]
2025-10-05 23:21:28.864528: Epoch time: 46.13 s
2025-10-05 23:21:28.864679: Yayy! New best EMA pseudo Dice: 0.595300018787384
2025-10-05 23:21:29.947323: 
2025-10-05 23:21:29.947671: Epoch 18
2025-10-05 23:21:29.947900: Current learning rate: 0.00891
2025-10-05 23:22:16.029508: Validation loss did not improve from -0.31927. Patience: 2/50
2025-10-05 23:22:16.030209: train_loss -0.5978
2025-10-05 23:22:16.030457: val_loss -0.2914
2025-10-05 23:22:16.030605: Pseudo dice [np.float32(0.6361)]
2025-10-05 23:22:16.030776: Epoch time: 46.08 s
2025-10-05 23:22:16.030985: Yayy! New best EMA pseudo Dice: 0.599399983882904
2025-10-05 23:22:17.127820: 
2025-10-05 23:22:17.128186: Epoch 19
2025-10-05 23:22:17.128405: Current learning rate: 0.00885
2025-10-05 23:23:03.227821: Validation loss did not improve from -0.31927. Patience: 3/50
2025-10-05 23:23:03.228217: train_loss -0.6151
2025-10-05 23:23:03.228372: val_loss -0.2777
2025-10-05 23:23:03.228496: Pseudo dice [np.float32(0.6463)]
2025-10-05 23:23:03.228634: Epoch time: 46.1 s
2025-10-05 23:23:03.673737: Yayy! New best EMA pseudo Dice: 0.6040999889373779
2025-10-05 23:23:04.770592: 
2025-10-05 23:23:04.770852: Epoch 20
2025-10-05 23:23:04.771060: Current learning rate: 0.00879
2025-10-05 23:23:50.883008: Validation loss did not improve from -0.31927. Patience: 4/50
2025-10-05 23:23:50.883587: train_loss -0.6179
2025-10-05 23:23:50.883786: val_loss -0.2883
2025-10-05 23:23:50.883951: Pseudo dice [np.float32(0.6518)]
2025-10-05 23:23:50.884094: Epoch time: 46.11 s
2025-10-05 23:23:50.884221: Yayy! New best EMA pseudo Dice: 0.6087999939918518
2025-10-05 23:23:51.976004: 
2025-10-05 23:23:51.976446: Epoch 21
2025-10-05 23:23:51.976745: Current learning rate: 0.00873
2025-10-05 23:24:38.095963: Validation loss did not improve from -0.31927. Patience: 5/50
2025-10-05 23:24:38.096416: train_loss -0.6288
2025-10-05 23:24:38.096638: val_loss -0.2868
2025-10-05 23:24:38.096812: Pseudo dice [np.float32(0.6243)]
2025-10-05 23:24:38.096997: Epoch time: 46.12 s
2025-10-05 23:24:38.097148: Yayy! New best EMA pseudo Dice: 0.6104000210762024
2025-10-05 23:24:39.161874: 
2025-10-05 23:24:39.162223: Epoch 22
2025-10-05 23:24:39.162419: Current learning rate: 0.00867
2025-10-05 23:25:25.301847: Validation loss did not improve from -0.31927. Patience: 6/50
2025-10-05 23:25:25.302465: train_loss -0.6126
2025-10-05 23:25:25.302655: val_loss -0.2779
2025-10-05 23:25:25.302793: Pseudo dice [np.float32(0.6488)]
2025-10-05 23:25:25.302940: Epoch time: 46.14 s
2025-10-05 23:25:25.303076: Yayy! New best EMA pseudo Dice: 0.6141999959945679
2025-10-05 23:25:26.382767: 
2025-10-05 23:25:26.383055: Epoch 23
2025-10-05 23:25:26.383308: Current learning rate: 0.00861
2025-10-05 23:26:12.514622: Validation loss improved from -0.31927 to -0.32427! Patience: 6/50
2025-10-05 23:26:12.515046: train_loss -0.6415
2025-10-05 23:26:12.515204: val_loss -0.3243
2025-10-05 23:26:12.515401: Pseudo dice [np.float32(0.6635)]
2025-10-05 23:26:12.515574: Epoch time: 46.13 s
2025-10-05 23:26:12.515711: Yayy! New best EMA pseudo Dice: 0.6191999912261963
2025-10-05 23:26:13.592669: 
2025-10-05 23:26:13.593082: Epoch 24
2025-10-05 23:26:13.593271: Current learning rate: 0.00855
2025-10-05 23:26:59.691212: Validation loss did not improve from -0.32427. Patience: 1/50
2025-10-05 23:26:59.691952: train_loss -0.6279
2025-10-05 23:26:59.692176: val_loss -0.2394
2025-10-05 23:26:59.692349: Pseudo dice [np.float32(0.6148)]
2025-10-05 23:26:59.692499: Epoch time: 46.1 s
2025-10-05 23:27:00.773077: 
2025-10-05 23:27:00.773428: Epoch 25
2025-10-05 23:27:00.773662: Current learning rate: 0.00849
2025-10-05 23:27:46.866365: Validation loss did not improve from -0.32427. Patience: 2/50
2025-10-05 23:27:46.866821: train_loss -0.651
2025-10-05 23:27:46.867014: val_loss -0.2671
2025-10-05 23:27:46.867144: Pseudo dice [np.float32(0.6261)]
2025-10-05 23:27:46.867289: Epoch time: 46.09 s
2025-10-05 23:27:46.867470: Yayy! New best EMA pseudo Dice: 0.6194999814033508
2025-10-05 23:27:47.950078: 
2025-10-05 23:27:47.950356: Epoch 26
2025-10-05 23:27:47.950575: Current learning rate: 0.00843
2025-10-05 23:28:34.012004: Validation loss did not improve from -0.32427. Patience: 3/50
2025-10-05 23:28:34.012634: train_loss -0.657
2025-10-05 23:28:34.012808: val_loss -0.3069
2025-10-05 23:28:34.012952: Pseudo dice [np.float32(0.6539)]
2025-10-05 23:28:34.013110: Epoch time: 46.06 s
2025-10-05 23:28:34.013291: Yayy! New best EMA pseudo Dice: 0.6229000091552734
2025-10-05 23:28:35.435871: 
2025-10-05 23:28:35.436103: Epoch 27
2025-10-05 23:28:35.436256: Current learning rate: 0.00836
2025-10-05 23:29:21.573798: Validation loss did not improve from -0.32427. Patience: 4/50
2025-10-05 23:29:21.574168: train_loss -0.6601
2025-10-05 23:29:21.574344: val_loss -0.2982
2025-10-05 23:29:21.574468: Pseudo dice [np.float32(0.6651)]
2025-10-05 23:29:21.574623: Epoch time: 46.14 s
2025-10-05 23:29:21.574748: Yayy! New best EMA pseudo Dice: 0.6270999908447266
2025-10-05 23:29:22.657798: 
2025-10-05 23:29:22.658084: Epoch 28
2025-10-05 23:29:22.658266: Current learning rate: 0.0083
2025-10-05 23:30:08.740545: Validation loss improved from -0.32427 to -0.33655! Patience: 4/50
2025-10-05 23:30:08.741072: train_loss -0.6655
2025-10-05 23:30:08.741253: val_loss -0.3366
2025-10-05 23:30:08.741395: Pseudo dice [np.float32(0.6626)]
2025-10-05 23:30:08.741566: Epoch time: 46.08 s
2025-10-05 23:30:08.741687: Yayy! New best EMA pseudo Dice: 0.6306999921798706
2025-10-05 23:30:09.806499: 
2025-10-05 23:30:09.806829: Epoch 29
2025-10-05 23:30:09.807012: Current learning rate: 0.00824
2025-10-05 23:30:55.956089: Validation loss did not improve from -0.33655. Patience: 1/50
2025-10-05 23:30:55.956502: train_loss -0.6609
2025-10-05 23:30:55.956681: val_loss -0.2892
2025-10-05 23:30:55.956832: Pseudo dice [np.float32(0.6415)]
2025-10-05 23:30:55.956994: Epoch time: 46.15 s
2025-10-05 23:30:56.385391: Yayy! New best EMA pseudo Dice: 0.6316999793052673
2025-10-05 23:30:57.481036: 
2025-10-05 23:30:57.481375: Epoch 30
2025-10-05 23:30:57.481572: Current learning rate: 0.00818
2025-10-05 23:31:43.597771: Validation loss did not improve from -0.33655. Patience: 2/50
2025-10-05 23:31:43.598494: train_loss -0.6773
2025-10-05 23:31:43.598788: val_loss -0.2386
2025-10-05 23:31:43.599039: Pseudo dice [np.float32(0.6446)]
2025-10-05 23:31:43.599304: Epoch time: 46.12 s
2025-10-05 23:31:43.599502: Yayy! New best EMA pseudo Dice: 0.6330000162124634
2025-10-05 23:31:44.683747: 
2025-10-05 23:31:44.684068: Epoch 31
2025-10-05 23:31:44.684286: Current learning rate: 0.00812
2025-10-05 23:32:30.795552: Validation loss did not improve from -0.33655. Patience: 3/50
2025-10-05 23:32:30.795951: train_loss -0.6823
2025-10-05 23:32:30.796165: val_loss -0.2444
2025-10-05 23:32:30.796340: Pseudo dice [np.float32(0.6462)]
2025-10-05 23:32:30.796533: Epoch time: 46.11 s
2025-10-05 23:32:30.796720: Yayy! New best EMA pseudo Dice: 0.6342999935150146
2025-10-05 23:32:31.982910: 
2025-10-05 23:32:31.983240: Epoch 32
2025-10-05 23:32:31.983442: Current learning rate: 0.00806
2025-10-05 23:33:18.119450: Validation loss did not improve from -0.33655. Patience: 4/50
2025-10-05 23:33:18.120080: train_loss -0.6898
2025-10-05 23:33:18.120259: val_loss -0.273
2025-10-05 23:33:18.120410: Pseudo dice [np.float32(0.6387)]
2025-10-05 23:33:18.120567: Epoch time: 46.14 s
2025-10-05 23:33:18.120698: Yayy! New best EMA pseudo Dice: 0.6348000168800354
2025-10-05 23:33:19.213613: 
2025-10-05 23:33:19.213937: Epoch 33
2025-10-05 23:33:19.214118: Current learning rate: 0.008
2025-10-05 23:34:05.328736: Validation loss did not improve from -0.33655. Patience: 5/50
2025-10-05 23:34:05.329184: train_loss -0.6999
2025-10-05 23:34:05.329357: val_loss -0.2852
2025-10-05 23:34:05.329576: Pseudo dice [np.float32(0.6459)]
2025-10-05 23:34:05.329758: Epoch time: 46.12 s
2025-10-05 23:34:05.329879: Yayy! New best EMA pseudo Dice: 0.6359000205993652
2025-10-05 23:34:06.413466: 
2025-10-05 23:34:06.414484: Epoch 34
2025-10-05 23:34:06.415099: Current learning rate: 0.00793
2025-10-05 23:34:52.563284: Validation loss did not improve from -0.33655. Patience: 6/50
2025-10-05 23:34:52.563898: train_loss -0.6958
2025-10-05 23:34:52.564048: val_loss -0.2763
2025-10-05 23:34:52.564172: Pseudo dice [np.float32(0.6568)]
2025-10-05 23:34:52.564319: Epoch time: 46.15 s
2025-10-05 23:34:53.030510: Yayy! New best EMA pseudo Dice: 0.6380000114440918
2025-10-05 23:34:54.108946: 
2025-10-05 23:34:54.109261: Epoch 35
2025-10-05 23:34:54.109431: Current learning rate: 0.00787
2025-10-05 23:35:40.217212: Validation loss did not improve from -0.33655. Patience: 7/50
2025-10-05 23:35:40.217594: train_loss -0.713
2025-10-05 23:35:40.217773: val_loss -0.2423
2025-10-05 23:35:40.217917: Pseudo dice [np.float32(0.6322)]
2025-10-05 23:35:40.218078: Epoch time: 46.11 s
2025-10-05 23:35:40.844507: 
2025-10-05 23:35:40.844754: Epoch 36
2025-10-05 23:35:40.844908: Current learning rate: 0.00781
2025-10-05 23:36:26.970262: Validation loss did not improve from -0.33655. Patience: 8/50
2025-10-05 23:36:26.970805: train_loss -0.7223
2025-10-05 23:36:26.970953: val_loss -0.2664
2025-10-05 23:36:26.971098: Pseudo dice [np.float32(0.6445)]
2025-10-05 23:36:26.971255: Epoch time: 46.13 s
2025-10-05 23:36:26.971447: Yayy! New best EMA pseudo Dice: 0.6381000280380249
2025-10-05 23:36:28.078115: 
2025-10-05 23:36:28.078476: Epoch 37
2025-10-05 23:36:28.078770: Current learning rate: 0.00775
2025-10-05 23:37:14.255532: Validation loss did not improve from -0.33655. Patience: 9/50
2025-10-05 23:37:14.255998: train_loss -0.712
2025-10-05 23:37:14.256222: val_loss -0.2713
2025-10-05 23:37:14.256457: Pseudo dice [np.float32(0.6473)]
2025-10-05 23:37:14.256665: Epoch time: 46.18 s
2025-10-05 23:37:14.256841: Yayy! New best EMA pseudo Dice: 0.6389999985694885
2025-10-05 23:37:15.435264: 
2025-10-05 23:37:15.435682: Epoch 38
2025-10-05 23:37:15.435929: Current learning rate: 0.00769
2025-10-05 23:38:01.571503: Validation loss did not improve from -0.33655. Patience: 10/50
2025-10-05 23:38:01.572062: train_loss -0.7204
2025-10-05 23:38:01.572221: val_loss -0.2455
2025-10-05 23:38:01.572395: Pseudo dice [np.float32(0.6548)]
2025-10-05 23:38:01.572594: Epoch time: 46.14 s
2025-10-05 23:38:01.572729: Yayy! New best EMA pseudo Dice: 0.6406000256538391
2025-10-05 23:38:02.680869: 
2025-10-05 23:38:02.681519: Epoch 39
2025-10-05 23:38:02.681958: Current learning rate: 0.00763
2025-10-05 23:38:48.855124: Validation loss did not improve from -0.33655. Patience: 11/50
2025-10-05 23:38:48.855508: train_loss -0.72
2025-10-05 23:38:48.855706: val_loss -0.2673
2025-10-05 23:38:48.855877: Pseudo dice [np.float32(0.6452)]
2025-10-05 23:38:48.856055: Epoch time: 46.18 s
2025-10-05 23:38:49.339699: Yayy! New best EMA pseudo Dice: 0.6410999894142151
2025-10-05 23:38:50.438851: 
2025-10-05 23:38:50.439151: Epoch 40
2025-10-05 23:38:50.439415: Current learning rate: 0.00756
2025-10-05 23:39:36.578350: Validation loss did not improve from -0.33655. Patience: 12/50
2025-10-05 23:39:36.578967: train_loss -0.7357
2025-10-05 23:39:36.579177: val_loss -0.242
2025-10-05 23:39:36.579339: Pseudo dice [np.float32(0.6541)]
2025-10-05 23:39:36.579512: Epoch time: 46.14 s
2025-10-05 23:39:36.579695: Yayy! New best EMA pseudo Dice: 0.6424000263214111
2025-10-05 23:39:37.685448: 
2025-10-05 23:39:37.685825: Epoch 41
2025-10-05 23:39:37.686083: Current learning rate: 0.0075
2025-10-05 23:40:23.807373: Validation loss did not improve from -0.33655. Patience: 13/50
2025-10-05 23:40:23.807826: train_loss -0.7418
2025-10-05 23:40:23.808071: val_loss -0.26
2025-10-05 23:40:23.808300: Pseudo dice [np.float32(0.6504)]
2025-10-05 23:40:23.808512: Epoch time: 46.12 s
2025-10-05 23:40:23.808677: Yayy! New best EMA pseudo Dice: 0.6431999802589417
2025-10-05 23:40:25.281987: 
2025-10-05 23:40:25.282349: Epoch 42
2025-10-05 23:40:25.282501: Current learning rate: 0.00744
2025-10-05 23:41:11.475845: Validation loss did not improve from -0.33655. Patience: 14/50
2025-10-05 23:41:11.476377: train_loss -0.7343
2025-10-05 23:41:11.476522: val_loss -0.2529
2025-10-05 23:41:11.476712: Pseudo dice [np.float32(0.6449)]
2025-10-05 23:41:11.476858: Epoch time: 46.2 s
2025-10-05 23:41:11.476994: Yayy! New best EMA pseudo Dice: 0.6432999968528748
2025-10-05 23:41:12.586876: 
2025-10-05 23:41:12.587155: Epoch 43
2025-10-05 23:41:12.587318: Current learning rate: 0.00738
2025-10-05 23:41:58.765800: Validation loss did not improve from -0.33655. Patience: 15/50
2025-10-05 23:41:58.766130: train_loss -0.7424
2025-10-05 23:41:58.766312: val_loss -0.2264
2025-10-05 23:41:58.766447: Pseudo dice [np.float32(0.6396)]
2025-10-05 23:41:58.766630: Epoch time: 46.18 s
2025-10-05 23:41:59.385236: 
2025-10-05 23:41:59.385491: Epoch 44
2025-10-05 23:41:59.385667: Current learning rate: 0.00732
2025-10-05 23:42:45.560888: Validation loss did not improve from -0.33655. Patience: 16/50
2025-10-05 23:42:45.561557: train_loss -0.7497
2025-10-05 23:42:45.561741: val_loss -0.2824
2025-10-05 23:42:45.561861: Pseudo dice [np.float32(0.6453)]
2025-10-05 23:42:45.562005: Epoch time: 46.18 s
2025-10-05 23:42:46.626355: 
2025-10-05 23:42:46.626602: Epoch 45
2025-10-05 23:42:46.626775: Current learning rate: 0.00725
2025-10-05 23:43:32.859931: Validation loss did not improve from -0.33655. Patience: 17/50
2025-10-05 23:43:32.860317: train_loss -0.7462
2025-10-05 23:43:32.860505: val_loss -0.2565
2025-10-05 23:43:32.860677: Pseudo dice [np.float32(0.6571)]
2025-10-05 23:43:32.860869: Epoch time: 46.23 s
2025-10-05 23:43:32.861040: Yayy! New best EMA pseudo Dice: 0.644599974155426
2025-10-05 23:43:33.958618: 
2025-10-05 23:43:33.958973: Epoch 46
2025-10-05 23:43:33.959237: Current learning rate: 0.00719
2025-10-05 23:44:20.170580: Validation loss did not improve from -0.33655. Patience: 18/50
2025-10-05 23:44:20.171146: train_loss -0.7459
2025-10-05 23:44:20.171367: val_loss -0.2323
2025-10-05 23:44:20.171498: Pseudo dice [np.float32(0.6505)]
2025-10-05 23:44:20.171638: Epoch time: 46.21 s
2025-10-05 23:44:20.171768: Yayy! New best EMA pseudo Dice: 0.6452000141143799
2025-10-05 23:44:21.259259: 
2025-10-05 23:44:21.259497: Epoch 47
2025-10-05 23:44:21.259677: Current learning rate: 0.00713
2025-10-05 23:45:07.487029: Validation loss did not improve from -0.33655. Patience: 19/50
2025-10-05 23:45:07.487543: train_loss -0.7515
2025-10-05 23:45:07.487770: val_loss -0.2584
2025-10-05 23:45:07.487950: Pseudo dice [np.float32(0.6368)]
2025-10-05 23:45:07.488133: Epoch time: 46.23 s
2025-10-05 23:45:08.113853: 
2025-10-05 23:45:08.114206: Epoch 48
2025-10-05 23:45:08.114376: Current learning rate: 0.00707
2025-10-05 23:45:54.267637: Validation loss did not improve from -0.33655. Patience: 20/50
2025-10-05 23:45:54.268245: train_loss -0.7604
2025-10-05 23:45:54.268445: val_loss -0.201
2025-10-05 23:45:54.268598: Pseudo dice [np.float32(0.6406)]
2025-10-05 23:45:54.268771: Epoch time: 46.16 s
2025-10-05 23:45:54.892987: 
2025-10-05 23:45:54.893266: Epoch 49
2025-10-05 23:45:54.893488: Current learning rate: 0.007
2025-10-05 23:46:41.055873: Validation loss did not improve from -0.33655. Patience: 21/50
2025-10-05 23:46:41.056283: train_loss -0.7681
2025-10-05 23:46:41.056468: val_loss -0.2326
2025-10-05 23:46:41.056619: Pseudo dice [np.float32(0.6554)]
2025-10-05 23:46:41.056775: Epoch time: 46.16 s
2025-10-05 23:46:42.140373: 
2025-10-05 23:46:42.140636: Epoch 50
2025-10-05 23:46:42.140828: Current learning rate: 0.00694
2025-10-05 23:47:28.281572: Validation loss did not improve from -0.33655. Patience: 22/50
2025-10-05 23:47:28.282229: train_loss -0.7702
2025-10-05 23:47:28.282616: val_loss -0.2312
2025-10-05 23:47:28.282784: Pseudo dice [np.float32(0.6552)]
2025-10-05 23:47:28.282962: Epoch time: 46.14 s
2025-10-05 23:47:28.283123: Yayy! New best EMA pseudo Dice: 0.6460999846458435
2025-10-05 23:47:29.373971: 
2025-10-05 23:47:29.374265: Epoch 51
2025-10-05 23:47:29.374432: Current learning rate: 0.00688
2025-10-05 23:48:15.446702: Validation loss did not improve from -0.33655. Patience: 23/50
2025-10-05 23:48:15.447130: train_loss -0.7757
2025-10-05 23:48:15.447332: val_loss -0.2715
2025-10-05 23:48:15.447457: Pseudo dice [np.float32(0.6564)]
2025-10-05 23:48:15.447604: Epoch time: 46.07 s
2025-10-05 23:48:15.447746: Yayy! New best EMA pseudo Dice: 0.6471999883651733
2025-10-05 23:48:16.539835: 
2025-10-05 23:48:16.540065: Epoch 52
2025-10-05 23:48:16.540237: Current learning rate: 0.00682
2025-10-05 23:49:02.704311: Validation loss did not improve from -0.33655. Patience: 24/50
2025-10-05 23:49:02.704829: train_loss -0.7776
2025-10-05 23:49:02.705023: val_loss -0.2332
2025-10-05 23:49:02.705186: Pseudo dice [np.float32(0.6664)]
2025-10-05 23:49:02.705350: Epoch time: 46.17 s
2025-10-05 23:49:02.705493: Yayy! New best EMA pseudo Dice: 0.6491000056266785
2025-10-05 23:49:03.807748: 
2025-10-05 23:49:03.807995: Epoch 53
2025-10-05 23:49:03.808233: Current learning rate: 0.00675
2025-10-05 23:49:49.915486: Validation loss did not improve from -0.33655. Patience: 25/50
2025-10-05 23:49:49.915843: train_loss -0.7772
2025-10-05 23:49:49.916026: val_loss -0.2842
2025-10-05 23:49:49.916189: Pseudo dice [np.float32(0.6503)]
2025-10-05 23:49:49.916334: Epoch time: 46.11 s
2025-10-05 23:49:49.916460: Yayy! New best EMA pseudo Dice: 0.6492000222206116
2025-10-05 23:49:51.007363: 
2025-10-05 23:49:51.007679: Epoch 54
2025-10-05 23:49:51.007866: Current learning rate: 0.00669
2025-10-05 23:50:37.155114: Validation loss did not improve from -0.33655. Patience: 26/50
2025-10-05 23:50:37.155790: train_loss -0.7811
2025-10-05 23:50:37.156131: val_loss -0.2426
2025-10-05 23:50:37.156371: Pseudo dice [np.float32(0.6577)]
2025-10-05 23:50:37.156689: Epoch time: 46.15 s
2025-10-05 23:50:37.690469: Yayy! New best EMA pseudo Dice: 0.6500999927520752
2025-10-05 23:50:38.839972: 
2025-10-05 23:50:38.840390: Epoch 55
2025-10-05 23:50:38.840691: Current learning rate: 0.00663
2025-10-05 23:51:24.969801: Validation loss did not improve from -0.33655. Patience: 27/50
2025-10-05 23:51:24.970197: train_loss -0.7818
2025-10-05 23:51:24.970455: val_loss -0.2669
2025-10-05 23:51:24.970619: Pseudo dice [np.float32(0.668)]
2025-10-05 23:51:24.970807: Epoch time: 46.13 s
2025-10-05 23:51:24.970998: Yayy! New best EMA pseudo Dice: 0.6517999768257141
2025-10-05 23:51:26.065211: 
2025-10-05 23:51:26.065505: Epoch 56
2025-10-05 23:51:26.065772: Current learning rate: 0.00657
2025-10-05 23:52:12.241395: Validation loss did not improve from -0.33655. Patience: 28/50
2025-10-05 23:52:12.242053: train_loss -0.7912
2025-10-05 23:52:12.242336: val_loss -0.2374
2025-10-05 23:52:12.242524: Pseudo dice [np.float32(0.6528)]
2025-10-05 23:52:12.242693: Epoch time: 46.18 s
2025-10-05 23:52:12.242938: Yayy! New best EMA pseudo Dice: 0.6518999934196472
2025-10-05 23:52:13.355844: 
2025-10-05 23:52:13.356122: Epoch 57
2025-10-05 23:52:13.356361: Current learning rate: 0.0065
2025-10-05 23:52:59.502571: Validation loss did not improve from -0.33655. Patience: 29/50
2025-10-05 23:52:59.503051: train_loss -0.8011
2025-10-05 23:52:59.503236: val_loss -0.2091
2025-10-05 23:52:59.503412: Pseudo dice [np.float32(0.6362)]
2025-10-05 23:52:59.503594: Epoch time: 46.15 s
2025-10-05 23:53:00.501361: 
2025-10-05 23:53:00.501708: Epoch 58
2025-10-05 23:53:00.501896: Current learning rate: 0.00644
2025-10-05 23:53:46.642013: Validation loss did not improve from -0.33655. Patience: 30/50
2025-10-05 23:53:46.642552: train_loss -0.7919
2025-10-05 23:53:46.642734: val_loss -0.1885
2025-10-05 23:53:46.642906: Pseudo dice [np.float32(0.6587)]
2025-10-05 23:53:46.643189: Epoch time: 46.14 s
2025-10-05 23:53:47.275958: 
2025-10-05 23:53:47.276275: Epoch 59
2025-10-05 23:53:47.276451: Current learning rate: 0.00638
2025-10-05 23:54:33.442964: Validation loss did not improve from -0.33655. Patience: 31/50
2025-10-05 23:54:33.443429: train_loss -0.7987
2025-10-05 23:54:33.443606: val_loss -0.1875
2025-10-05 23:54:33.443810: Pseudo dice [np.float32(0.6446)]
2025-10-05 23:54:33.443990: Epoch time: 46.17 s
2025-10-05 23:54:34.549889: 
2025-10-05 23:54:34.550140: Epoch 60
2025-10-05 23:54:34.550302: Current learning rate: 0.00631
2025-10-05 23:55:20.680399: Validation loss did not improve from -0.33655. Patience: 32/50
2025-10-05 23:55:20.680993: train_loss -0.8032
2025-10-05 23:55:20.681157: val_loss -0.2452
2025-10-05 23:55:20.681299: Pseudo dice [np.float32(0.6604)]
2025-10-05 23:55:20.681432: Epoch time: 46.13 s
2025-10-05 23:55:21.314029: 
2025-10-05 23:55:21.314299: Epoch 61
2025-10-05 23:55:21.314469: Current learning rate: 0.00625
2025-10-05 23:56:07.484426: Validation loss did not improve from -0.33655. Patience: 33/50
2025-10-05 23:56:07.484835: train_loss -0.8078
2025-10-05 23:56:07.485046: val_loss -0.2175
2025-10-05 23:56:07.485246: Pseudo dice [np.float32(0.6552)]
2025-10-05 23:56:07.485450: Epoch time: 46.17 s
2025-10-05 23:56:08.117483: 
2025-10-05 23:56:08.117781: Epoch 62
2025-10-05 23:56:08.117986: Current learning rate: 0.00619
2025-10-05 23:56:54.240058: Validation loss did not improve from -0.33655. Patience: 34/50
2025-10-05 23:56:54.240670: train_loss -0.8124
2025-10-05 23:56:54.240881: val_loss -0.1636
2025-10-05 23:56:54.241013: Pseudo dice [np.float32(0.652)]
2025-10-05 23:56:54.241188: Epoch time: 46.12 s
2025-10-05 23:56:54.877416: 
2025-10-05 23:56:54.877705: Epoch 63
2025-10-05 23:56:54.877857: Current learning rate: 0.00612
2025-10-05 23:57:41.024744: Validation loss did not improve from -0.33655. Patience: 35/50
2025-10-05 23:57:41.025280: train_loss -0.8053
2025-10-05 23:57:41.025569: val_loss -0.2032
2025-10-05 23:57:41.025832: Pseudo dice [np.float32(0.6493)]
2025-10-05 23:57:41.026237: Epoch time: 46.15 s
2025-10-05 23:57:41.662947: 
2025-10-05 23:57:41.663384: Epoch 64
2025-10-05 23:57:41.663709: Current learning rate: 0.00606
2025-10-05 23:58:27.834898: Validation loss did not improve from -0.33655. Patience: 36/50
2025-10-05 23:58:27.835584: train_loss -0.8071
2025-10-05 23:58:27.835741: val_loss -0.1832
2025-10-05 23:58:27.835872: Pseudo dice [np.float32(0.6536)]
2025-10-05 23:58:27.836056: Epoch time: 46.17 s
2025-10-05 23:58:28.903371: 
2025-10-05 23:58:28.903686: Epoch 65
2025-10-05 23:58:28.904155: Current learning rate: 0.006
2025-10-05 23:59:15.046413: Validation loss did not improve from -0.33655. Patience: 37/50
2025-10-05 23:59:15.046769: train_loss -0.8159
2025-10-05 23:59:15.046971: val_loss -0.1936
2025-10-05 23:59:15.047124: Pseudo dice [np.float32(0.6677)]
2025-10-05 23:59:15.047269: Epoch time: 46.14 s
2025-10-05 23:59:15.047424: Yayy! New best EMA pseudo Dice: 0.6534000039100647
2025-10-05 23:59:16.138584: 
2025-10-05 23:59:16.138939: Epoch 66
2025-10-05 23:59:16.139157: Current learning rate: 0.00593
2025-10-06 00:00:02.335030: Validation loss did not improve from -0.33655. Patience: 38/50
2025-10-06 00:00:02.335622: train_loss -0.8232
2025-10-06 00:00:02.335799: val_loss -0.2108
2025-10-06 00:00:02.335943: Pseudo dice [np.float32(0.6325)]
2025-10-06 00:00:02.336123: Epoch time: 46.2 s
2025-10-06 00:00:02.975233: 
2025-10-06 00:00:02.975560: Epoch 67
2025-10-06 00:00:02.975743: Current learning rate: 0.00587
2025-10-06 00:00:49.069445: Validation loss did not improve from -0.33655. Patience: 39/50
2025-10-06 00:00:49.069880: train_loss -0.824
2025-10-06 00:00:49.070104: val_loss -0.2425
2025-10-06 00:00:49.070323: Pseudo dice [np.float32(0.6694)]
2025-10-06 00:00:49.070549: Epoch time: 46.1 s
2025-10-06 00:00:49.700760: 
2025-10-06 00:00:49.701024: Epoch 68
2025-10-06 00:00:49.701249: Current learning rate: 0.00581
2025-10-06 00:01:35.853061: Validation loss did not improve from -0.33655. Patience: 40/50
2025-10-06 00:01:35.853822: train_loss -0.8254
2025-10-06 00:01:35.854069: val_loss -0.1854
2025-10-06 00:01:35.854298: Pseudo dice [np.float32(0.6509)]
2025-10-06 00:01:35.854580: Epoch time: 46.15 s
2025-10-06 00:01:36.494414: 
2025-10-06 00:01:36.494691: Epoch 69
2025-10-06 00:01:36.494946: Current learning rate: 0.00574
2025-10-06 00:02:22.597286: Validation loss did not improve from -0.33655. Patience: 41/50
2025-10-06 00:02:22.597713: train_loss -0.823
2025-10-06 00:02:22.597864: val_loss -0.2072
2025-10-06 00:02:22.598029: Pseudo dice [np.float32(0.6493)]
2025-10-06 00:02:22.598204: Epoch time: 46.1 s
2025-10-06 00:02:23.677459: 
2025-10-06 00:02:23.677708: Epoch 70
2025-10-06 00:02:23.677882: Current learning rate: 0.00568
2025-10-06 00:03:09.778364: Validation loss did not improve from -0.33655. Patience: 42/50
2025-10-06 00:03:09.779053: train_loss -0.8297
2025-10-06 00:03:09.779301: val_loss -0.2102
2025-10-06 00:03:09.779527: Pseudo dice [np.float32(0.6651)]
2025-10-06 00:03:09.779793: Epoch time: 46.1 s
2025-10-06 00:03:09.780015: Yayy! New best EMA pseudo Dice: 0.6538000106811523
2025-10-06 00:03:10.881748: 
2025-10-06 00:03:10.882040: Epoch 71
2025-10-06 00:03:10.882262: Current learning rate: 0.00562
2025-10-06 00:03:57.053120: Validation loss did not improve from -0.33655. Patience: 43/50
2025-10-06 00:03:57.053454: train_loss -0.8305
2025-10-06 00:03:57.053628: val_loss -0.2007
2025-10-06 00:03:57.053771: Pseudo dice [np.float32(0.6447)]
2025-10-06 00:03:57.053941: Epoch time: 46.17 s
2025-10-06 00:03:57.685022: 
2025-10-06 00:03:57.685283: Epoch 72
2025-10-06 00:03:57.685456: Current learning rate: 0.00555
2025-10-06 00:04:43.782053: Validation loss did not improve from -0.33655. Patience: 44/50
2025-10-06 00:04:43.782750: train_loss -0.8345
2025-10-06 00:04:43.782923: val_loss -0.184
2025-10-06 00:04:43.783051: Pseudo dice [np.float32(0.6638)]
2025-10-06 00:04:43.783325: Epoch time: 46.1 s
2025-10-06 00:04:43.783556: Yayy! New best EMA pseudo Dice: 0.6539999842643738
2025-10-06 00:04:45.241197: 
2025-10-06 00:04:45.241538: Epoch 73
2025-10-06 00:04:45.241722: Current learning rate: 0.00549
2025-10-06 00:05:31.372555: Validation loss did not improve from -0.33655. Patience: 45/50
2025-10-06 00:05:31.373062: train_loss -0.8342
2025-10-06 00:05:31.373250: val_loss -0.2029
2025-10-06 00:05:31.373389: Pseudo dice [np.float32(0.6608)]
2025-10-06 00:05:31.373535: Epoch time: 46.13 s
2025-10-06 00:05:31.373653: Yayy! New best EMA pseudo Dice: 0.654699981212616
2025-10-06 00:05:32.466859: 
2025-10-06 00:05:32.467143: Epoch 74
2025-10-06 00:05:32.467363: Current learning rate: 0.00542
2025-10-06 00:06:18.576125: Validation loss did not improve from -0.33655. Patience: 46/50
2025-10-06 00:06:18.576756: train_loss -0.8335
2025-10-06 00:06:18.576944: val_loss -0.2416
2025-10-06 00:06:18.577167: Pseudo dice [np.float32(0.6544)]
2025-10-06 00:06:18.577356: Epoch time: 46.11 s
2025-10-06 00:06:19.663181: 
2025-10-06 00:06:19.663532: Epoch 75
2025-10-06 00:06:19.663730: Current learning rate: 0.00536
2025-10-06 00:07:05.796807: Validation loss did not improve from -0.33655. Patience: 47/50
2025-10-06 00:07:05.797214: train_loss -0.8369
2025-10-06 00:07:05.797419: val_loss -0.2009
2025-10-06 00:07:05.797577: Pseudo dice [np.float32(0.6719)]
2025-10-06 00:07:05.797752: Epoch time: 46.13 s
2025-10-06 00:07:05.797925: Yayy! New best EMA pseudo Dice: 0.6564000248908997
2025-10-06 00:07:06.908378: 
2025-10-06 00:07:06.908705: Epoch 76
2025-10-06 00:07:06.908906: Current learning rate: 0.00529
2025-10-06 00:07:53.026004: Validation loss did not improve from -0.33655. Patience: 48/50
2025-10-06 00:07:53.026647: train_loss -0.8391
2025-10-06 00:07:53.026837: val_loss -0.1786
2025-10-06 00:07:53.027017: Pseudo dice [np.float32(0.6539)]
2025-10-06 00:07:53.027199: Epoch time: 46.12 s
2025-10-06 00:07:53.657976: 
2025-10-06 00:07:53.658244: Epoch 77
2025-10-06 00:07:53.658401: Current learning rate: 0.00523
2025-10-06 00:08:39.784752: Validation loss did not improve from -0.33655. Patience: 49/50
2025-10-06 00:08:39.785213: train_loss -0.8426
2025-10-06 00:08:39.785408: val_loss -0.2259
2025-10-06 00:08:39.785584: Pseudo dice [np.float32(0.6743)]
2025-10-06 00:08:39.785777: Epoch time: 46.13 s
2025-10-06 00:08:39.785929: Yayy! New best EMA pseudo Dice: 0.6578999757766724
2025-10-06 00:08:40.883173: 
2025-10-06 00:08:40.883455: Epoch 78
2025-10-06 00:08:40.883606: Current learning rate: 0.00517
2025-10-06 00:09:27.038080: Validation loss did not improve from -0.33655. Patience: 50/50
2025-10-06 00:09:27.038710: train_loss -0.8429
2025-10-06 00:09:27.038992: val_loss -0.1889
2025-10-06 00:09:27.039235: Pseudo dice [np.float32(0.6477)]
2025-10-06 00:09:27.039565: Epoch time: 46.16 s
2025-10-06 00:09:27.691138: 
2025-10-06 00:09:27.691451: Epoch 79
2025-10-06 00:09:27.691639: Current learning rate: 0.0051
2025-10-06 00:10:13.832726: Validation loss did not improve from -0.33655. Patience: 51/50
2025-10-06 00:10:13.833095: train_loss -0.8437
2025-10-06 00:10:13.833278: val_loss -0.2032
2025-10-06 00:10:13.833404: Pseudo dice [np.float32(0.6579)]
2025-10-06 00:10:13.833545: Epoch time: 46.14 s
2025-10-06 00:10:14.920897: 
2025-10-06 00:10:14.921186: Epoch 80
2025-10-06 00:10:14.921365: Current learning rate: 0.00504
2025-10-06 00:11:01.148294: Validation loss did not improve from -0.33655. Patience: 52/50
2025-10-06 00:11:01.149020: train_loss -0.8423
2025-10-06 00:11:01.149188: val_loss -0.1942
2025-10-06 00:11:01.149316: Pseudo dice [np.float32(0.656)]
2025-10-06 00:11:01.149452: Epoch time: 46.23 s
2025-10-06 00:11:01.789512: 
2025-10-06 00:11:01.789809: Epoch 81
2025-10-06 00:11:01.789963: Current learning rate: 0.00497
2025-10-06 00:11:47.979897: Validation loss did not improve from -0.33655. Patience: 53/50
2025-10-06 00:11:47.980303: train_loss -0.846
2025-10-06 00:11:47.980487: val_loss -0.2187
2025-10-06 00:11:47.980632: Pseudo dice [np.float32(0.6598)]
2025-10-06 00:11:47.980798: Epoch time: 46.19 s
2025-10-06 00:11:48.623988: 
2025-10-06 00:11:48.624226: Epoch 82
2025-10-06 00:11:48.624381: Current learning rate: 0.00491
2025-10-06 00:12:34.780541: Validation loss did not improve from -0.33655. Patience: 54/50
2025-10-06 00:12:34.781191: train_loss -0.8442
2025-10-06 00:12:34.781387: val_loss -0.1929
2025-10-06 00:12:34.781666: Pseudo dice [np.float32(0.67)]
2025-10-06 00:12:34.781851: Epoch time: 46.16 s
2025-10-06 00:12:34.782048: Yayy! New best EMA pseudo Dice: 0.6585000157356262
2025-10-06 00:12:35.888883: 
2025-10-06 00:12:35.889240: Epoch 83
2025-10-06 00:12:35.889521: Current learning rate: 0.00484
2025-10-06 00:13:22.041240: Validation loss did not improve from -0.33655. Patience: 55/50
2025-10-06 00:13:22.041676: train_loss -0.8502
2025-10-06 00:13:22.041889: val_loss -0.2177
2025-10-06 00:13:22.042058: Pseudo dice [np.float32(0.6633)]
2025-10-06 00:13:22.042200: Epoch time: 46.15 s
2025-10-06 00:13:22.042330: Yayy! New best EMA pseudo Dice: 0.6589999794960022
2025-10-06 00:13:23.126672: 
2025-10-06 00:13:23.126951: Epoch 84
2025-10-06 00:13:23.127140: Current learning rate: 0.00478
2025-10-06 00:14:09.276989: Validation loss did not improve from -0.33655. Patience: 56/50
2025-10-06 00:14:09.277597: train_loss -0.8524
2025-10-06 00:14:09.277774: val_loss -0.1964
2025-10-06 00:14:09.277934: Pseudo dice [np.float32(0.6642)]
2025-10-06 00:14:09.278073: Epoch time: 46.15 s
2025-10-06 00:14:09.715062: Yayy! New best EMA pseudo Dice: 0.659500002861023
2025-10-06 00:14:10.777234: 
2025-10-06 00:14:10.777544: Epoch 85
2025-10-06 00:14:10.777695: Current learning rate: 0.00471
2025-10-06 00:14:56.935390: Validation loss did not improve from -0.33655. Patience: 57/50
2025-10-06 00:14:56.935791: train_loss -0.8542
2025-10-06 00:14:56.935961: val_loss -0.2109
2025-10-06 00:14:56.936137: Pseudo dice [np.float32(0.6723)]
2025-10-06 00:14:56.936286: Epoch time: 46.16 s
2025-10-06 00:14:56.936445: Yayy! New best EMA pseudo Dice: 0.6607999801635742
2025-10-06 00:14:57.999276: 
2025-10-06 00:14:57.999611: Epoch 86
2025-10-06 00:14:57.999920: Current learning rate: 0.00465
2025-10-06 00:15:44.183898: Validation loss did not improve from -0.33655. Patience: 58/50
2025-10-06 00:15:44.184563: train_loss -0.8562
2025-10-06 00:15:44.184711: val_loss -0.1856
2025-10-06 00:15:44.184862: Pseudo dice [np.float32(0.6395)]
2025-10-06 00:15:44.185062: Epoch time: 46.19 s
2025-10-06 00:15:44.806149: 
2025-10-06 00:15:44.806403: Epoch 87
2025-10-06 00:15:44.806581: Current learning rate: 0.00458
2025-10-06 00:16:30.949755: Validation loss did not improve from -0.33655. Patience: 59/50
2025-10-06 00:16:30.950213: train_loss -0.8584
2025-10-06 00:16:30.950372: val_loss -0.1257
2025-10-06 00:16:30.950513: Pseudo dice [np.float32(0.6532)]
2025-10-06 00:16:30.950647: Epoch time: 46.14 s
2025-10-06 00:16:31.931202: 
2025-10-06 00:16:31.931553: Epoch 88
2025-10-06 00:16:31.931772: Current learning rate: 0.00452
2025-10-06 00:17:18.056492: Validation loss did not improve from -0.33655. Patience: 60/50
2025-10-06 00:17:18.057101: train_loss -0.8611
2025-10-06 00:17:18.057249: val_loss -0.2182
2025-10-06 00:17:18.057374: Pseudo dice [np.float32(0.6665)]
2025-10-06 00:17:18.057508: Epoch time: 46.13 s
2025-10-06 00:17:18.684029: 
2025-10-06 00:17:18.684373: Epoch 89
2025-10-06 00:17:18.684567: Current learning rate: 0.00445
2025-10-06 00:18:04.768142: Validation loss did not improve from -0.33655. Patience: 61/50
2025-10-06 00:18:04.768529: train_loss -0.8581
2025-10-06 00:18:04.768678: val_loss -0.1365
2025-10-06 00:18:04.768839: Pseudo dice [np.float32(0.644)]
2025-10-06 00:18:04.768976: Epoch time: 46.09 s
2025-10-06 00:18:05.832882: 
2025-10-06 00:18:05.833162: Epoch 90
2025-10-06 00:18:05.833423: Current learning rate: 0.00438
2025-10-06 00:18:51.940095: Validation loss did not improve from -0.33655. Patience: 62/50
2025-10-06 00:18:51.940637: train_loss -0.8625
2025-10-06 00:18:51.940791: val_loss -0.2267
2025-10-06 00:18:51.940931: Pseudo dice [np.float32(0.664)]
2025-10-06 00:18:51.941094: Epoch time: 46.11 s
2025-10-06 00:18:52.567690: 
2025-10-06 00:18:52.567985: Epoch 91
2025-10-06 00:18:52.568237: Current learning rate: 0.00432
2025-10-06 00:19:38.689343: Validation loss did not improve from -0.33655. Patience: 63/50
2025-10-06 00:19:38.689686: train_loss -0.8608
2025-10-06 00:19:38.689871: val_loss -0.2102
2025-10-06 00:19:38.690057: Pseudo dice [np.float32(0.6636)]
2025-10-06 00:19:38.690259: Epoch time: 46.12 s
2025-10-06 00:19:39.316622: 
2025-10-06 00:19:39.316910: Epoch 92
2025-10-06 00:19:39.317105: Current learning rate: 0.00425
2025-10-06 00:20:25.438696: Validation loss did not improve from -0.33655. Patience: 64/50
2025-10-06 00:20:25.439396: train_loss -0.8616
2025-10-06 00:20:25.439614: val_loss -0.1779
2025-10-06 00:20:25.439795: Pseudo dice [np.float32(0.6621)]
2025-10-06 00:20:25.439967: Epoch time: 46.12 s
2025-10-06 00:20:26.074342: 
2025-10-06 00:20:26.074651: Epoch 93
2025-10-06 00:20:26.074836: Current learning rate: 0.00419
2025-10-06 00:21:12.169337: Validation loss did not improve from -0.33655. Patience: 65/50
2025-10-06 00:21:12.169711: train_loss -0.8657
2025-10-06 00:21:12.169935: val_loss -0.1714
2025-10-06 00:21:12.170122: Pseudo dice [np.float32(0.6629)]
2025-10-06 00:21:12.170341: Epoch time: 46.1 s
2025-10-06 00:21:12.798191: 
2025-10-06 00:21:12.798568: Epoch 94
2025-10-06 00:21:12.798827: Current learning rate: 0.00412
2025-10-06 00:21:58.945765: Validation loss did not improve from -0.33655. Patience: 66/50
2025-10-06 00:21:58.946345: train_loss -0.8637
2025-10-06 00:21:58.946552: val_loss -0.1649
2025-10-06 00:21:58.946780: Pseudo dice [np.float32(0.6501)]
2025-10-06 00:21:58.946962: Epoch time: 46.15 s
2025-10-06 00:22:00.043216: 
2025-10-06 00:22:00.043566: Epoch 95
2025-10-06 00:22:00.043771: Current learning rate: 0.00405
2025-10-06 00:22:46.216025: Validation loss did not improve from -0.33655. Patience: 67/50
2025-10-06 00:22:46.216589: train_loss -0.8622
2025-10-06 00:22:46.216869: val_loss -0.165
2025-10-06 00:22:46.217074: Pseudo dice [np.float32(0.6709)]
2025-10-06 00:22:46.217304: Epoch time: 46.17 s
2025-10-06 00:22:46.850177: 
2025-10-06 00:22:46.850459: Epoch 96
2025-10-06 00:22:46.850610: Current learning rate: 0.00399
2025-10-06 00:23:33.003215: Validation loss did not improve from -0.33655. Patience: 68/50
2025-10-06 00:23:33.003757: train_loss -0.8657
2025-10-06 00:23:33.003916: val_loss -0.2103
2025-10-06 00:23:33.004059: Pseudo dice [np.float32(0.6649)]
2025-10-06 00:23:33.004225: Epoch time: 46.15 s
2025-10-06 00:23:33.637138: 
2025-10-06 00:23:33.637388: Epoch 97
2025-10-06 00:23:33.637559: Current learning rate: 0.00392
2025-10-06 00:24:19.745981: Validation loss did not improve from -0.33655. Patience: 69/50
2025-10-06 00:24:19.746341: train_loss -0.8687
2025-10-06 00:24:19.746492: val_loss -0.1271
2025-10-06 00:24:19.746628: Pseudo dice [np.float32(0.6383)]
2025-10-06 00:24:19.746758: Epoch time: 46.11 s
2025-10-06 00:24:20.375993: 
2025-10-06 00:24:20.376298: Epoch 98
2025-10-06 00:24:20.376472: Current learning rate: 0.00385
2025-10-06 00:25:06.451320: Validation loss did not improve from -0.33655. Patience: 70/50
2025-10-06 00:25:06.452003: train_loss -0.8698
2025-10-06 00:25:06.452152: val_loss -0.1392
2025-10-06 00:25:06.452294: Pseudo dice [np.float32(0.6563)]
2025-10-06 00:25:06.452436: Epoch time: 46.08 s
2025-10-06 00:25:07.081077: 
2025-10-06 00:25:07.081385: Epoch 99
2025-10-06 00:25:07.081536: Current learning rate: 0.00379
2025-10-06 00:25:53.221068: Validation loss did not improve from -0.33655. Patience: 71/50
2025-10-06 00:25:53.221460: train_loss -0.869
2025-10-06 00:25:53.221621: val_loss -0.1253
2025-10-06 00:25:53.221772: Pseudo dice [np.float32(0.6519)]
2025-10-06 00:25:53.221906: Epoch time: 46.14 s
2025-10-06 00:25:54.331657: 
2025-10-06 00:25:54.331986: Epoch 100
2025-10-06 00:25:54.332166: Current learning rate: 0.00372
2025-10-06 00:26:40.459510: Validation loss did not improve from -0.33655. Patience: 72/50
2025-10-06 00:26:40.460178: train_loss -0.8729
2025-10-06 00:26:40.460386: val_loss -0.1867
2025-10-06 00:26:40.460577: Pseudo dice [np.float32(0.6705)]
2025-10-06 00:26:40.460763: Epoch time: 46.13 s
2025-10-06 00:26:41.088318: 
2025-10-06 00:26:41.088664: Epoch 101
2025-10-06 00:26:41.088851: Current learning rate: 0.00365
2025-10-06 00:27:27.207622: Validation loss did not improve from -0.33655. Patience: 73/50
2025-10-06 00:27:27.207989: train_loss -0.873
2025-10-06 00:27:27.208143: val_loss -0.1758
2025-10-06 00:27:27.208297: Pseudo dice [np.float32(0.6754)]
2025-10-06 00:27:27.208451: Epoch time: 46.12 s
2025-10-06 00:27:27.840959: 
2025-10-06 00:27:27.841175: Epoch 102
2025-10-06 00:27:27.841377: Current learning rate: 0.00359
2025-10-06 00:28:13.670267: Validation loss did not improve from -0.33655. Patience: 74/50
2025-10-06 00:28:13.670889: train_loss -0.8741
2025-10-06 00:28:13.671049: val_loss -0.1248
2025-10-06 00:28:13.671170: Pseudo dice [np.float32(0.6637)]
2025-10-06 00:28:13.671321: Epoch time: 45.83 s
2025-10-06 00:28:14.300777: 
2025-10-06 00:28:14.301097: Epoch 103
2025-10-06 00:28:14.301251: Current learning rate: 0.00352
2025-10-06 00:29:00.416937: Validation loss did not improve from -0.33655. Patience: 75/50
2025-10-06 00:29:00.417385: train_loss -0.8757
2025-10-06 00:29:00.417586: val_loss -0.172
2025-10-06 00:29:00.417777: Pseudo dice [np.float32(0.6635)]
2025-10-06 00:29:00.417943: Epoch time: 46.12 s
2025-10-06 00:29:00.418101: Yayy! New best EMA pseudo Dice: 0.6608999967575073
2025-10-06 00:29:01.874366: 
2025-10-06 00:29:01.874625: Epoch 104
2025-10-06 00:29:01.874823: Current learning rate: 0.00345
2025-10-06 00:29:47.957381: Validation loss did not improve from -0.33655. Patience: 76/50
2025-10-06 00:29:47.958006: train_loss -0.8767
2025-10-06 00:29:47.958193: val_loss -0.1342
2025-10-06 00:29:47.958338: Pseudo dice [np.float32(0.6521)]
2025-10-06 00:29:47.958506: Epoch time: 46.08 s
2025-10-06 00:29:49.037915: 
2025-10-06 00:29:49.038255: Epoch 105
2025-10-06 00:29:49.038446: Current learning rate: 0.00338
2025-10-06 00:30:35.143167: Validation loss did not improve from -0.33655. Patience: 77/50
2025-10-06 00:30:35.143685: train_loss -0.8779
2025-10-06 00:30:35.143936: val_loss -0.204
2025-10-06 00:30:35.144067: Pseudo dice [np.float32(0.6709)]
2025-10-06 00:30:35.144228: Epoch time: 46.11 s
2025-10-06 00:30:35.144361: Yayy! New best EMA pseudo Dice: 0.6610999703407288
2025-10-06 00:30:36.239452: 
2025-10-06 00:30:36.239717: Epoch 106
2025-10-06 00:30:36.239962: Current learning rate: 0.00332
2025-10-06 00:31:22.378246: Validation loss did not improve from -0.33655. Patience: 78/50
2025-10-06 00:31:22.378771: train_loss -0.8779
2025-10-06 00:31:22.378937: val_loss -0.1489
2025-10-06 00:31:22.379057: Pseudo dice [np.float32(0.656)]
2025-10-06 00:31:22.379209: Epoch time: 46.14 s
2025-10-06 00:31:23.012363: 
2025-10-06 00:31:23.012655: Epoch 107
2025-10-06 00:31:23.012820: Current learning rate: 0.00325
2025-10-06 00:32:09.143227: Validation loss did not improve from -0.33655. Patience: 79/50
2025-10-06 00:32:09.143580: train_loss -0.8832
2025-10-06 00:32:09.143752: val_loss -0.189
2025-10-06 00:32:09.143898: Pseudo dice [np.float32(0.6765)]
2025-10-06 00:32:09.144057: Epoch time: 46.13 s
2025-10-06 00:32:09.144230: Yayy! New best EMA pseudo Dice: 0.6621999740600586
2025-10-06 00:32:10.240577: 
2025-10-06 00:32:10.240862: Epoch 108
2025-10-06 00:32:10.241010: Current learning rate: 0.00318
2025-10-06 00:32:56.389444: Validation loss did not improve from -0.33655. Patience: 80/50
2025-10-06 00:32:56.390039: train_loss -0.8826
2025-10-06 00:32:56.390190: val_loss -0.1654
2025-10-06 00:32:56.390322: Pseudo dice [np.float32(0.6806)]
2025-10-06 00:32:56.390471: Epoch time: 46.15 s
2025-10-06 00:32:56.390592: Yayy! New best EMA pseudo Dice: 0.6639999747276306
2025-10-06 00:32:57.470310: 
2025-10-06 00:32:57.470561: Epoch 109
2025-10-06 00:32:57.470742: Current learning rate: 0.00311
2025-10-06 00:33:43.571422: Validation loss did not improve from -0.33655. Patience: 81/50
2025-10-06 00:33:43.571835: train_loss -0.8803
2025-10-06 00:33:43.572012: val_loss -0.1185
2025-10-06 00:33:43.572163: Pseudo dice [np.float32(0.6718)]
2025-10-06 00:33:43.572346: Epoch time: 46.1 s
2025-10-06 00:33:44.019610: Yayy! New best EMA pseudo Dice: 0.6647999882698059
2025-10-06 00:33:45.071896: 
2025-10-06 00:33:45.072235: Epoch 110
2025-10-06 00:33:45.072432: Current learning rate: 0.00304
2025-10-06 00:34:31.117264: Validation loss did not improve from -0.33655. Patience: 82/50
2025-10-06 00:34:31.118016: train_loss -0.8832
2025-10-06 00:34:31.118255: val_loss -0.1601
2025-10-06 00:34:31.118513: Pseudo dice [np.float32(0.666)]
2025-10-06 00:34:31.118670: Epoch time: 46.05 s
2025-10-06 00:34:31.118915: Yayy! New best EMA pseudo Dice: 0.664900004863739
2025-10-06 00:34:32.195832: 
2025-10-06 00:34:32.196218: Epoch 111
2025-10-06 00:34:32.196551: Current learning rate: 0.00297
2025-10-06 00:35:18.301810: Validation loss did not improve from -0.33655. Patience: 83/50
2025-10-06 00:35:18.302250: train_loss -0.8814
2025-10-06 00:35:18.302486: val_loss -0.1304
2025-10-06 00:35:18.302639: Pseudo dice [np.float32(0.655)]
2025-10-06 00:35:18.302800: Epoch time: 46.11 s
2025-10-06 00:35:18.927945: 
2025-10-06 00:35:18.928209: Epoch 112
2025-10-06 00:35:18.928433: Current learning rate: 0.00291
2025-10-06 00:36:04.962220: Validation loss did not improve from -0.33655. Patience: 84/50
2025-10-06 00:36:04.962956: train_loss -0.8843
2025-10-06 00:36:04.963188: val_loss -0.1374
2025-10-06 00:36:04.963331: Pseudo dice [np.float32(0.6598)]
2025-10-06 00:36:04.963588: Epoch time: 46.04 s
2025-10-06 00:36:05.584898: 
2025-10-06 00:36:05.585151: Epoch 113
2025-10-06 00:36:05.585329: Current learning rate: 0.00284
2025-10-06 00:36:51.695599: Validation loss did not improve from -0.33655. Patience: 85/50
2025-10-06 00:36:51.695988: train_loss -0.8889
2025-10-06 00:36:51.696165: val_loss -0.1468
2025-10-06 00:36:51.696329: Pseudo dice [np.float32(0.6539)]
2025-10-06 00:36:51.696470: Epoch time: 46.11 s
2025-10-06 00:36:52.318244: 
2025-10-06 00:36:52.318537: Epoch 114
2025-10-06 00:36:52.318693: Current learning rate: 0.00277
2025-10-06 00:37:38.413834: Validation loss did not improve from -0.33655. Patience: 86/50
2025-10-06 00:37:38.414445: train_loss -0.8877
2025-10-06 00:37:38.414676: val_loss -0.1223
2025-10-06 00:37:38.414906: Pseudo dice [np.float32(0.6626)]
2025-10-06 00:37:38.415055: Epoch time: 46.1 s
2025-10-06 00:37:39.494559: 
2025-10-06 00:37:39.494881: Epoch 115
2025-10-06 00:37:39.495052: Current learning rate: 0.0027
2025-10-06 00:38:25.583975: Validation loss did not improve from -0.33655. Patience: 87/50
2025-10-06 00:38:25.584383: train_loss -0.8883
2025-10-06 00:38:25.584556: val_loss -0.1464
2025-10-06 00:38:25.584681: Pseudo dice [np.float32(0.6599)]
2025-10-06 00:38:25.584885: Epoch time: 46.09 s
2025-10-06 00:38:26.210193: 
2025-10-06 00:38:26.210510: Epoch 116
2025-10-06 00:38:26.210693: Current learning rate: 0.00263
2025-10-06 00:39:12.279750: Validation loss did not improve from -0.33655. Patience: 88/50
2025-10-06 00:39:12.280414: train_loss -0.8882
2025-10-06 00:39:12.280636: val_loss -0.1431
2025-10-06 00:39:12.280757: Pseudo dice [np.float32(0.6678)]
2025-10-06 00:39:12.280923: Epoch time: 46.07 s
2025-10-06 00:39:12.910054: 
2025-10-06 00:39:12.910313: Epoch 117
2025-10-06 00:39:12.910597: Current learning rate: 0.00256
2025-10-06 00:39:59.003797: Validation loss did not improve from -0.33655. Patience: 89/50
2025-10-06 00:39:59.004209: train_loss -0.8913
2025-10-06 00:39:59.004378: val_loss -0.1331
2025-10-06 00:39:59.004539: Pseudo dice [np.float32(0.6665)]
2025-10-06 00:39:59.004706: Epoch time: 46.09 s
2025-10-06 00:39:59.631927: 
2025-10-06 00:39:59.632220: Epoch 118
2025-10-06 00:39:59.632386: Current learning rate: 0.00249
2025-10-06 00:40:45.741186: Validation loss did not improve from -0.33655. Patience: 90/50
2025-10-06 00:40:45.741792: train_loss -0.8905
2025-10-06 00:40:45.741941: val_loss -0.1279
2025-10-06 00:40:45.742085: Pseudo dice [np.float32(0.6615)]
2025-10-06 00:40:45.742220: Epoch time: 46.11 s
2025-10-06 00:40:46.719189: 
2025-10-06 00:40:46.719484: Epoch 119
2025-10-06 00:40:46.719661: Current learning rate: 0.00242
2025-10-06 00:41:32.831439: Validation loss did not improve from -0.33655. Patience: 91/50
2025-10-06 00:41:32.831963: train_loss -0.8902
2025-10-06 00:41:32.832215: val_loss -0.1846
2025-10-06 00:41:32.832411: Pseudo dice [np.float32(0.6599)]
2025-10-06 00:41:32.832570: Epoch time: 46.11 s
2025-10-06 00:41:33.913765: 
2025-10-06 00:41:33.914154: Epoch 120
2025-10-06 00:41:33.914397: Current learning rate: 0.00235
2025-10-06 00:42:19.983654: Validation loss did not improve from -0.33655. Patience: 92/50
2025-10-06 00:42:19.984233: train_loss -0.8906
2025-10-06 00:42:19.984451: val_loss -0.075
2025-10-06 00:42:19.984624: Pseudo dice [np.float32(0.6471)]
2025-10-06 00:42:19.984829: Epoch time: 46.07 s
2025-10-06 00:42:20.620418: 
2025-10-06 00:42:20.620725: Epoch 121
2025-10-06 00:42:20.620902: Current learning rate: 0.00228
2025-10-06 00:43:06.743033: Validation loss did not improve from -0.33655. Patience: 93/50
2025-10-06 00:43:06.743470: train_loss -0.8918
2025-10-06 00:43:06.743693: val_loss -0.104
2025-10-06 00:43:06.743849: Pseudo dice [np.float32(0.6546)]
2025-10-06 00:43:06.744006: Epoch time: 46.12 s
2025-10-06 00:43:07.378558: 
2025-10-06 00:43:07.378849: Epoch 122
2025-10-06 00:43:07.379157: Current learning rate: 0.00221
2025-10-06 00:43:53.498949: Validation loss did not improve from -0.33655. Patience: 94/50
2025-10-06 00:43:53.499573: train_loss -0.8929
2025-10-06 00:43:53.499794: val_loss -0.1844
2025-10-06 00:43:53.499957: Pseudo dice [np.float32(0.6645)]
2025-10-06 00:43:53.500142: Epoch time: 46.12 s
2025-10-06 00:43:54.130645: 
2025-10-06 00:43:54.130957: Epoch 123
2025-10-06 00:43:54.131152: Current learning rate: 0.00214
2025-10-06 00:44:40.308429: Validation loss did not improve from -0.33655. Patience: 95/50
2025-10-06 00:44:40.308853: train_loss -0.8946
2025-10-06 00:44:40.309061: val_loss -0.0527
2025-10-06 00:44:40.309226: Pseudo dice [np.float32(0.6544)]
2025-10-06 00:44:40.309446: Epoch time: 46.18 s
2025-10-06 00:44:40.941128: 
2025-10-06 00:44:40.941514: Epoch 124
2025-10-06 00:44:40.941764: Current learning rate: 0.00207
2025-10-06 00:45:27.068911: Validation loss did not improve from -0.33655. Patience: 96/50
2025-10-06 00:45:27.069561: train_loss -0.8913
2025-10-06 00:45:27.069734: val_loss -0.137
2025-10-06 00:45:27.069877: Pseudo dice [np.float32(0.6632)]
2025-10-06 00:45:27.070036: Epoch time: 46.13 s
2025-10-06 00:45:28.156718: 
2025-10-06 00:45:28.157120: Epoch 125
2025-10-06 00:45:28.157313: Current learning rate: 0.00199
2025-10-06 00:46:14.298663: Validation loss did not improve from -0.33655. Patience: 97/50
2025-10-06 00:46:14.299229: train_loss -0.8965
2025-10-06 00:46:14.299559: val_loss -0.1106
2025-10-06 00:46:14.299767: Pseudo dice [np.float32(0.6547)]
2025-10-06 00:46:14.299965: Epoch time: 46.14 s
2025-10-06 00:46:14.939889: 
2025-10-06 00:46:14.940103: Epoch 126
2025-10-06 00:46:14.940251: Current learning rate: 0.00192
2025-10-06 00:47:01.083657: Validation loss did not improve from -0.33655. Patience: 98/50
2025-10-06 00:47:01.084183: train_loss -0.8977
2025-10-06 00:47:01.084329: val_loss -0.1395
2025-10-06 00:47:01.084471: Pseudo dice [np.float32(0.671)]
2025-10-06 00:47:01.084622: Epoch time: 46.14 s
2025-10-06 00:47:01.712074: 
2025-10-06 00:47:01.712358: Epoch 127
2025-10-06 00:47:01.712527: Current learning rate: 0.00185
2025-10-06 00:47:47.848632: Validation loss did not improve from -0.33655. Patience: 99/50
2025-10-06 00:47:47.849127: train_loss -0.8991
2025-10-06 00:47:47.849415: val_loss -0.088
2025-10-06 00:47:47.849625: Pseudo dice [np.float32(0.6557)]
2025-10-06 00:47:47.849836: Epoch time: 46.14 s
2025-10-06 00:47:48.485521: 
2025-10-06 00:47:48.485819: Epoch 128
2025-10-06 00:47:48.485976: Current learning rate: 0.00178
2025-10-06 00:48:34.686790: Validation loss did not improve from -0.33655. Patience: 100/50
2025-10-06 00:48:34.687402: train_loss -0.8979
2025-10-06 00:48:34.687573: val_loss -0.1374
2025-10-06 00:48:34.687714: Pseudo dice [np.float32(0.6654)]
2025-10-06 00:48:34.687849: Epoch time: 46.2 s
2025-10-06 00:48:35.308335: 
2025-10-06 00:48:35.308678: Epoch 129
2025-10-06 00:48:35.308860: Current learning rate: 0.0017
2025-10-06 00:49:21.458195: Validation loss did not improve from -0.33655. Patience: 101/50
2025-10-06 00:49:21.458627: train_loss -0.8966
2025-10-06 00:49:21.458858: val_loss -0.106
2025-10-06 00:49:21.459118: Pseudo dice [np.float32(0.6529)]
2025-10-06 00:49:21.459310: Epoch time: 46.15 s
2025-10-06 00:49:22.529549: 
2025-10-06 00:49:22.529880: Epoch 130
2025-10-06 00:49:22.530211: Current learning rate: 0.00163
2025-10-06 00:50:08.636436: Validation loss did not improve from -0.33655. Patience: 102/50
2025-10-06 00:50:08.637044: train_loss -0.9013
2025-10-06 00:50:08.637264: val_loss -0.1361
2025-10-06 00:50:08.637424: Pseudo dice [np.float32(0.6727)]
2025-10-06 00:50:08.637624: Epoch time: 46.11 s
2025-10-06 00:50:09.262243: 
2025-10-06 00:50:09.262538: Epoch 131
2025-10-06 00:50:09.262749: Current learning rate: 0.00156
2025-10-06 00:50:55.451407: Validation loss did not improve from -0.33655. Patience: 103/50
2025-10-06 00:50:55.451778: train_loss -0.8997
2025-10-06 00:50:55.452022: val_loss -0.1228
2025-10-06 00:50:55.452144: Pseudo dice [np.float32(0.6657)]
2025-10-06 00:50:55.452412: Epoch time: 46.19 s
2025-10-06 00:50:56.075714: 
2025-10-06 00:50:56.075985: Epoch 132
2025-10-06 00:50:56.076183: Current learning rate: 0.00148
2025-10-06 00:51:42.206392: Validation loss did not improve from -0.33655. Patience: 104/50
2025-10-06 00:51:42.206966: train_loss -0.8999
2025-10-06 00:51:42.207140: val_loss -0.1425
2025-10-06 00:51:42.207330: Pseudo dice [np.float32(0.6595)]
2025-10-06 00:51:42.207546: Epoch time: 46.13 s
2025-10-06 00:51:42.830796: 
2025-10-06 00:51:42.831133: Epoch 133
2025-10-06 00:51:42.831327: Current learning rate: 0.00141
2025-10-06 00:52:28.951499: Validation loss did not improve from -0.33655. Patience: 105/50
2025-10-06 00:52:28.951939: train_loss -0.9003
2025-10-06 00:52:28.952114: val_loss -0.1058
2025-10-06 00:52:28.952266: Pseudo dice [np.float32(0.6582)]
2025-10-06 00:52:28.952483: Epoch time: 46.12 s
2025-10-06 00:52:29.574874: 
2025-10-06 00:52:29.575219: Epoch 134
2025-10-06 00:52:29.575440: Current learning rate: 0.00133
2025-10-06 00:53:15.736501: Validation loss did not improve from -0.33655. Patience: 106/50
2025-10-06 00:53:15.737112: train_loss -0.9006
2025-10-06 00:53:15.737266: val_loss -0.1247
2025-10-06 00:53:15.737402: Pseudo dice [np.float32(0.664)]
2025-10-06 00:53:15.737531: Epoch time: 46.16 s
2025-10-06 00:53:17.180315: 
2025-10-06 00:53:17.180595: Epoch 135
2025-10-06 00:53:17.180765: Current learning rate: 0.00126
2025-10-06 00:54:03.410337: Validation loss did not improve from -0.33655. Patience: 107/50
2025-10-06 00:54:03.410784: train_loss -0.9004
2025-10-06 00:54:03.410957: val_loss -0.1328
2025-10-06 00:54:03.411083: Pseudo dice [np.float32(0.6629)]
2025-10-06 00:54:03.411224: Epoch time: 46.23 s
2025-10-06 00:54:04.038173: 
2025-10-06 00:54:04.038502: Epoch 136
2025-10-06 00:54:04.038664: Current learning rate: 0.00118
2025-10-06 00:54:50.187136: Validation loss did not improve from -0.33655. Patience: 108/50
2025-10-06 00:54:50.187681: train_loss -0.9025
2025-10-06 00:54:50.187820: val_loss -0.116
2025-10-06 00:54:50.187962: Pseudo dice [np.float32(0.6612)]
2025-10-06 00:54:50.188104: Epoch time: 46.15 s
2025-10-06 00:54:50.813427: 
2025-10-06 00:54:50.813757: Epoch 137
2025-10-06 00:54:50.813935: Current learning rate: 0.00111
2025-10-06 00:55:36.964961: Validation loss did not improve from -0.33655. Patience: 109/50
2025-10-06 00:55:36.965525: train_loss -0.9019
2025-10-06 00:55:36.965745: val_loss -0.1231
2025-10-06 00:55:36.965904: Pseudo dice [np.float32(0.6658)]
2025-10-06 00:55:36.966182: Epoch time: 46.15 s
2025-10-06 00:55:37.619998: 
2025-10-06 00:55:37.620265: Epoch 138
2025-10-06 00:55:37.620463: Current learning rate: 0.00103
2025-10-06 00:56:23.818173: Validation loss did not improve from -0.33655. Patience: 110/50
2025-10-06 00:56:23.818801: train_loss -0.9033
2025-10-06 00:56:23.818979: val_loss -0.0882
2025-10-06 00:56:23.819140: Pseudo dice [np.float32(0.6673)]
2025-10-06 00:56:23.819302: Epoch time: 46.2 s
2025-10-06 00:56:24.446975: 
2025-10-06 00:56:24.447270: Epoch 139
2025-10-06 00:56:24.447427: Current learning rate: 0.00095
2025-10-06 00:57:10.628079: Validation loss did not improve from -0.33655. Patience: 111/50
2025-10-06 00:57:10.628530: train_loss -0.905
2025-10-06 00:57:10.628713: val_loss -0.106
2025-10-06 00:57:10.628866: Pseudo dice [np.float32(0.6658)]
2025-10-06 00:57:10.629032: Epoch time: 46.18 s
2025-10-06 00:57:11.725929: 
2025-10-06 00:57:11.726229: Epoch 140
2025-10-06 00:57:11.726475: Current learning rate: 0.00087
2025-10-06 00:57:57.902918: Validation loss did not improve from -0.33655. Patience: 112/50
2025-10-06 00:57:57.903501: train_loss -0.9055
2025-10-06 00:57:57.903655: val_loss -0.0666
2025-10-06 00:57:57.903853: Pseudo dice [np.float32(0.6564)]
2025-10-06 00:57:57.904039: Epoch time: 46.18 s
2025-10-06 00:57:58.533192: 
2025-10-06 00:57:58.533544: Epoch 141
2025-10-06 00:57:58.533779: Current learning rate: 0.00079
2025-10-06 00:58:44.643279: Validation loss did not improve from -0.33655. Patience: 113/50
2025-10-06 00:58:44.643703: train_loss -0.9035
2025-10-06 00:58:44.643924: val_loss -0.0795
2025-10-06 00:58:44.644089: Pseudo dice [np.float32(0.6492)]
2025-10-06 00:58:44.644343: Epoch time: 46.11 s
2025-10-06 00:58:45.272620: 
2025-10-06 00:58:45.272877: Epoch 142
2025-10-06 00:58:45.273174: Current learning rate: 0.00071
2025-10-06 00:59:31.426167: Validation loss did not improve from -0.33655. Patience: 114/50
2025-10-06 00:59:31.426719: train_loss -0.9088
2025-10-06 00:59:31.426892: val_loss -0.1297
2025-10-06 00:59:31.427080: Pseudo dice [np.float32(0.6624)]
2025-10-06 00:59:31.427234: Epoch time: 46.15 s
2025-10-06 00:59:32.055411: 
2025-10-06 00:59:32.055736: Epoch 143
2025-10-06 00:59:32.055969: Current learning rate: 0.00063
2025-10-06 01:00:18.167392: Validation loss did not improve from -0.33655. Patience: 115/50
2025-10-06 01:00:18.167817: train_loss -0.9078
2025-10-06 01:00:18.168020: val_loss -0.0697
2025-10-06 01:00:18.168223: Pseudo dice [np.float32(0.6598)]
2025-10-06 01:00:18.168484: Epoch time: 46.11 s
2025-10-06 01:00:18.806838: 
2025-10-06 01:00:18.807183: Epoch 144
2025-10-06 01:00:18.807406: Current learning rate: 0.00055
2025-10-06 01:01:05.001916: Validation loss did not improve from -0.33655. Patience: 116/50
2025-10-06 01:01:05.002539: train_loss -0.9081
2025-10-06 01:01:05.002712: val_loss -0.1211
2025-10-06 01:01:05.002944: Pseudo dice [np.float32(0.667)]
2025-10-06 01:01:05.003080: Epoch time: 46.2 s
2025-10-06 01:01:06.085389: 
2025-10-06 01:01:06.085667: Epoch 145
2025-10-06 01:01:06.085832: Current learning rate: 0.00047
2025-10-06 01:01:52.160092: Validation loss did not improve from -0.33655. Patience: 117/50
2025-10-06 01:01:52.160598: train_loss -0.9074
2025-10-06 01:01:52.160868: val_loss -0.0926
2025-10-06 01:01:52.161017: Pseudo dice [np.float32(0.6603)]
2025-10-06 01:01:52.161215: Epoch time: 46.08 s
2025-10-06 01:01:52.797018: 
2025-10-06 01:01:52.797283: Epoch 146
2025-10-06 01:01:52.797448: Current learning rate: 0.00038
2025-10-06 01:02:38.846947: Validation loss did not improve from -0.33655. Patience: 118/50
2025-10-06 01:02:38.847574: train_loss -0.9086
2025-10-06 01:02:38.847728: val_loss -0.1233
2025-10-06 01:02:38.847852: Pseudo dice [np.float32(0.6703)]
2025-10-06 01:02:38.847986: Epoch time: 46.05 s
2025-10-06 01:02:39.482972: 
2025-10-06 01:02:39.483490: Epoch 147
2025-10-06 01:02:39.483738: Current learning rate: 0.0003
2025-10-06 01:03:25.630833: Validation loss did not improve from -0.33655. Patience: 119/50
2025-10-06 01:03:25.631293: train_loss -0.9047
2025-10-06 01:03:25.631513: val_loss -0.1129
2025-10-06 01:03:25.631680: Pseudo dice [np.float32(0.6732)]
2025-10-06 01:03:25.631886: Epoch time: 46.15 s
2025-10-06 01:03:26.264833: 
2025-10-06 01:03:26.265162: Epoch 148
2025-10-06 01:03:26.265409: Current learning rate: 0.00021
2025-10-06 01:04:12.420970: Validation loss did not improve from -0.33655. Patience: 120/50
2025-10-06 01:04:12.421578: train_loss -0.9088
2025-10-06 01:04:12.421768: val_loss -0.0564
2025-10-06 01:04:12.421988: Pseudo dice [np.float32(0.6624)]
2025-10-06 01:04:12.422191: Epoch time: 46.16 s
2025-10-06 01:04:13.052935: 
2025-10-06 01:04:13.053171: Epoch 149
2025-10-06 01:04:13.053345: Current learning rate: 0.00011
2025-10-06 01:04:59.277781: Validation loss did not improve from -0.33655. Patience: 121/50
2025-10-06 01:04:59.278228: train_loss -0.9083
2025-10-06 01:04:59.278434: val_loss -0.0909
2025-10-06 01:04:59.278591: Pseudo dice [np.float32(0.6674)]
2025-10-06 01:04:59.278743: Epoch time: 46.23 s
2025-10-06 01:05:00.776910: Training done.
2025-10-06 01:05:00.787654: Using splits from existing split file: /nfs/erelab001/shared/Computational_Group/Naravich/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset310_nnInteractive_Calcium_OCT_CrossValidation/splits_final_60.json
2025-10-06 01:05:00.787941: The split file contains 5 splits.
2025-10-06 01:05:00.788108: Desired fold for training: 3
2025-10-06 01:05:00.788260: This split has 4 training and 5 validation cases.
2025-10-06 01:05:00.788475: predicting 101-045
2025-10-06 01:05:00.790670: 101-045, shape torch.Size([1, 375, 498, 498]), rank 0
2025-10-06 01:05:48.161974: predicting 106-002
2025-10-06 01:05:48.177274: 106-002, shape torch.Size([1, 539, 498, 498]), rank 0
2025-10-06 01:06:36.935042: predicting 401-004
2025-10-06 01:06:36.954406: 401-004, shape torch.Size([1, 375, 498, 498]), rank 0
2025-10-06 01:07:11.504292: predicting 704-003
2025-10-06 01:07:11.514944: 704-003, shape torch.Size([1, 375, 498, 498]), rank 0
2025-10-06 01:07:45.986533: predicting 706-005
2025-10-06 01:07:46.004882: 706-005, shape torch.Size([1, 375, 498, 498]), rank 0
2025-10-06 01:08:33.814149: Validation complete
2025-10-06 01:08:33.814378: Mean Validation Dice:  0.6318335649727554
Finished training fold 3 saving to /nfs/erelab001/shared/Computational_Group/Naravich/datasets/nnUNet_Datasets/nnUNet_results/Dataset310_nnInteractive_Calcium_OCT_CrossValidation/nnUNetTrainerScaleAnalysis60__nnUNetPlans__3d_32x160x128_b10/fold_3_No_Pretrained
