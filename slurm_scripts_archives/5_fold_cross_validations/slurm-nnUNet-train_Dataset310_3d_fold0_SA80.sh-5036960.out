/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/bin/python
Starting training with CONFIG=3d_32x160x128_b10, DATASET_ID=310, TRAINER=nnUNetTrainerScaleAnalysis80
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/nnunetv2/training/nnUNetTrainer/nnUNetTrainer.py:169: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  self.grad_scaler = GradScaler() if self.device.type == 'cuda' else None

############################
INFO: You are using the old nnU-Net default plans. We have updated our recommendations. Please consider using those instead! Read more here: https://github.com/MIC-DKFZ/nnUNet/blob/master/documentation/resenc_presets.md
############################

Using device: cuda:0

#######################################################################
Please cite the following paper when using nnU-Net:
Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.
#######################################################################

2025-10-05 10:49:27.216650: do_dummy_2d_data_aug: True
2025-10-05 10:49:27.217035: Using splits from existing split file: /nfs/erelab001/shared/Computational_Group/Naravich/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset310_nnInteractive_Calcium_OCT_CrossValidation/splits_final_80.json
2025-10-05 10:49:27.217406: The split file contains 5 splits.
2025-10-05 10:49:27.217550: Desired fold for training: 0
2025-10-05 10:49:27.217681: This split has 6 training and 4 validation cases.
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/fft_conv_pytorch/fft_conv.py:139: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:306.)
  output = output[crop_slices].contiguous()
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/fft_conv_pytorch/fft_conv.py:139: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:306.)
  output = output[crop_slices].contiguous()
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/fft_conv_pytorch/fft_conv.py:139: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:306.)
  output = output[crop_slices].contiguous()
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/fft_conv_pytorch/fft_conv.py:139: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:306.)
  output = output[crop_slices].contiguous()
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/fft_conv_pytorch/fft_conv.py:139: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:306.)
  output = output[crop_slices].contiguous()
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/fft_conv_pytorch/fft_conv.py:139: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:306.)
  output = output[crop_slices].contiguous()
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/fft_conv_pytorch/fft_conv.py:139: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:306.)
  output = output[crop_slices].contiguous()
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/fft_conv_pytorch/fft_conv.py:139: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:306.)
  output = output[crop_slices].contiguous()
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/fft_conv_pytorch/fft_conv.py:139: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:306.)
  output = output[crop_slices].contiguous()
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/fft_conv_pytorch/fft_conv.py:139: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:306.)
  output = output[crop_slices].contiguous()
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/fft_conv_pytorch/fft_conv.py:139: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:306.)
  output = output[crop_slices].contiguous()
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/fft_conv_pytorch/fft_conv.py:139: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:306.)
  output = output[crop_slices].contiguous()
using pin_memory on device 0
using pin_memory on device 0
2025-10-05 10:49:31.418222: Using torch.compile...

This is the configuration used by this training:
Configuration name: 3d_32x160x128_b10
 {'data_identifier': 'nnUNetPlans_3d_fullres', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 10, 'patch_size': [32, 160, 128], 'median_image_size_in_voxels': [375.0, 498.0, 498.0], 'spacing': [1.0, 1.0, 1.0], 'normalization_schemes': ['ZScoreNormalization'], 'use_mask_for_norm': [False], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'architecture': {'network_class_name': 'dynamic_network_architectures.architectures.unet.PlainConvUNet', 'arch_kwargs': {'n_stages': 6, 'features_per_stage': [32, 64, 128, 256, 320, 320], 'conv_op': 'torch.nn.modules.conv.Conv3d', 'kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]], 'strides': [[1, 1, 1], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [1, 2, 2]], 'n_conv_per_stage': [2, 2, 2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2, 2, 2], 'conv_bias': True, 'norm_op': 'torch.nn.modules.instancenorm.InstanceNorm3d', 'norm_op_kwargs': {'eps': 1e-05, 'affine': True}, 'dropout_op': None, 'dropout_op_kwargs': None, 'nonlin': 'torch.nn.LeakyReLU', 'nonlin_kwargs': {'inplace': True}, 'deep_supervision': True}, '_kw_requires_import': ['conv_op', 'norm_op', 'dropout_op', 'nonlin']}, 'batch_dice': True, 'inherits_from': '3d_fullres'} 

These are the global plan.json settings:
 {'dataset_name': 'Dataset310_nnInteractive_Calcium_OCT_CrossValidation', 'plans_name': 'nnUNetPlans', 'original_median_spacing_after_transp': [1.0, 1.0, 1.0], 'original_median_shape_after_transp': [375, 498, 498], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 0.4977976381778717, 'mean': 0.1394134759902954, 'median': 0.09849607944488525, 'min': 0.0, 'percentile_00_5': 0.015305490233004093, 'percentile_99_5': 0.4977976381778717, 'std': 0.121165432035923}}} 

2025-10-05 10:49:33.000454: unpacking dataset...
2025-10-05 10:49:37.198365: unpacking done...
2025-10-05 10:49:37.200807: Unable to plot network architecture: nnUNet_compile is enabled!
2025-10-05 10:49:37.205623: 
2025-10-05 10:49:37.205861: Epoch 0
2025-10-05 10:49:37.206080: Current learning rate: 0.01
2025-10-05 10:50:29.058693: Validation loss improved from 1000.00000 to -0.15734! Patience: 0/50
2025-10-05 10:50:29.059466: train_loss -0.0958
2025-10-05 10:50:29.059718: val_loss -0.1573
2025-10-05 10:50:29.059969: Pseudo dice [np.float32(0.5455)]
2025-10-05 10:50:29.060209: Epoch time: 51.85 s
2025-10-05 10:50:29.060390: Yayy! New best EMA pseudo Dice: 0.5454999804496765
2025-10-05 10:50:30.251339: 
2025-10-05 10:50:30.251676: Epoch 1
2025-10-05 10:50:30.251931: Current learning rate: 0.00994
2025-10-05 10:51:16.204282: Validation loss improved from -0.15734 to -0.26208! Patience: 0/50
2025-10-05 10:51:16.204804: train_loss -0.2483
2025-10-05 10:51:16.205005: val_loss -0.2621
2025-10-05 10:51:16.205171: Pseudo dice [np.float32(0.5765)]
2025-10-05 10:51:16.205309: Epoch time: 45.95 s
2025-10-05 10:51:16.205446: Yayy! New best EMA pseudo Dice: 0.5486000180244446
2025-10-05 10:51:17.333981: 
2025-10-05 10:51:17.334275: Epoch 2
2025-10-05 10:51:17.334494: Current learning rate: 0.00988
2025-10-05 10:52:03.349564: Validation loss did not improve from -0.26208. Patience: 1/50
2025-10-05 10:52:03.350233: train_loss -0.3107
2025-10-05 10:52:03.350554: val_loss -0.2534
2025-10-05 10:52:03.350897: Pseudo dice [np.float32(0.5783)]
2025-10-05 10:52:03.351246: Epoch time: 46.02 s
2025-10-05 10:52:03.351593: Yayy! New best EMA pseudo Dice: 0.5515999794006348
2025-10-05 10:52:04.416611: 
2025-10-05 10:52:04.416926: Epoch 3
2025-10-05 10:52:04.417289: Current learning rate: 0.00982
2025-10-05 10:52:50.445507: Validation loss did not improve from -0.26208. Patience: 2/50
2025-10-05 10:52:50.446030: train_loss -0.3606
2025-10-05 10:52:50.446497: val_loss -0.244
2025-10-05 10:52:50.446725: Pseudo dice [np.float32(0.5858)]
2025-10-05 10:52:50.446950: Epoch time: 46.03 s
2025-10-05 10:52:50.447169: Yayy! New best EMA pseudo Dice: 0.5550000071525574
2025-10-05 10:52:51.532092: 
2025-10-05 10:52:51.532437: Epoch 4
2025-10-05 10:52:51.532730: Current learning rate: 0.00976
2025-10-05 10:53:37.573596: Validation loss improved from -0.26208 to -0.33105! Patience: 2/50
2025-10-05 10:53:37.574119: train_loss -0.3769
2025-10-05 10:53:37.574276: val_loss -0.331
2025-10-05 10:53:37.574419: Pseudo dice [np.float32(0.6203)]
2025-10-05 10:53:37.574615: Epoch time: 46.04 s
2025-10-05 10:53:37.932680: Yayy! New best EMA pseudo Dice: 0.5615000128746033
2025-10-05 10:53:38.980862: 
2025-10-05 10:53:38.981246: Epoch 5
2025-10-05 10:53:38.981533: Current learning rate: 0.0097
2025-10-05 10:54:24.944208: Validation loss improved from -0.33105 to -0.36351! Patience: 0/50
2025-10-05 10:54:24.944808: train_loss -0.4147
2025-10-05 10:54:24.945190: val_loss -0.3635
2025-10-05 10:54:24.945533: Pseudo dice [np.float32(0.6464)]
2025-10-05 10:54:24.945817: Epoch time: 45.96 s
2025-10-05 10:54:24.946093: Yayy! New best EMA pseudo Dice: 0.5699999928474426
2025-10-05 10:54:26.021165: 
2025-10-05 10:54:26.021582: Epoch 6
2025-10-05 10:54:26.021855: Current learning rate: 0.00964
2025-10-05 10:55:11.912952: Validation loss improved from -0.36351 to -0.36439! Patience: 0/50
2025-10-05 10:55:11.913465: train_loss -0.4326
2025-10-05 10:55:11.913645: val_loss -0.3644
2025-10-05 10:55:11.913854: Pseudo dice [np.float32(0.6413)]
2025-10-05 10:55:11.914026: Epoch time: 45.89 s
2025-10-05 10:55:11.914174: Yayy! New best EMA pseudo Dice: 0.5770999789237976
2025-10-05 10:55:12.959994: 
2025-10-05 10:55:12.960490: Epoch 7
2025-10-05 10:55:12.960831: Current learning rate: 0.00958
2025-10-05 10:55:58.847959: Validation loss did not improve from -0.36439. Patience: 1/50
2025-10-05 10:55:58.848866: train_loss -0.4618
2025-10-05 10:55:58.849449: val_loss -0.3444
2025-10-05 10:55:58.849936: Pseudo dice [np.float32(0.6196)]
2025-10-05 10:55:58.850387: Epoch time: 45.89 s
2025-10-05 10:55:58.850816: Yayy! New best EMA pseudo Dice: 0.5813999772071838
2025-10-05 10:56:00.012153: 
2025-10-05 10:56:00.012505: Epoch 8
2025-10-05 10:56:00.012818: Current learning rate: 0.00952
2025-10-05 10:56:45.936078: Validation loss did not improve from -0.36439. Patience: 2/50
2025-10-05 10:56:45.936740: train_loss -0.4924
2025-10-05 10:56:45.937014: val_loss -0.3286
2025-10-05 10:56:45.937241: Pseudo dice [np.float32(0.6165)]
2025-10-05 10:56:45.937515: Epoch time: 45.93 s
2025-10-05 10:56:45.937792: Yayy! New best EMA pseudo Dice: 0.5849000215530396
2025-10-05 10:56:46.970034: 
2025-10-05 10:56:46.970392: Epoch 9
2025-10-05 10:56:46.970635: Current learning rate: 0.00946
2025-10-05 10:57:32.929513: Validation loss did not improve from -0.36439. Patience: 3/50
2025-10-05 10:57:32.930026: train_loss -0.5004
2025-10-05 10:57:32.930218: val_loss -0.3498
2025-10-05 10:57:32.930344: Pseudo dice [np.float32(0.6449)]
2025-10-05 10:57:32.930500: Epoch time: 45.96 s
2025-10-05 10:57:33.378849: Yayy! New best EMA pseudo Dice: 0.5909000039100647
2025-10-05 10:57:34.419178: 
2025-10-05 10:57:34.419524: Epoch 10
2025-10-05 10:57:34.419782: Current learning rate: 0.0094
2025-10-05 10:58:20.357333: Validation loss improved from -0.36439 to -0.36910! Patience: 3/50
2025-10-05 10:58:20.357946: train_loss -0.5128
2025-10-05 10:58:20.358128: val_loss -0.3691
2025-10-05 10:58:20.358279: Pseudo dice [np.float32(0.6396)]
2025-10-05 10:58:20.358443: Epoch time: 45.94 s
2025-10-05 10:58:20.358570: Yayy! New best EMA pseudo Dice: 0.59579998254776
2025-10-05 10:58:21.404322: 
2025-10-05 10:58:21.404753: Epoch 11
2025-10-05 10:58:21.405055: Current learning rate: 0.00934
2025-10-05 10:59:07.331851: Validation loss improved from -0.36910 to -0.39125! Patience: 0/50
2025-10-05 10:59:07.332331: train_loss -0.5306
2025-10-05 10:59:07.332571: val_loss -0.3913
2025-10-05 10:59:07.332738: Pseudo dice [np.float32(0.6672)]
2025-10-05 10:59:07.332954: Epoch time: 45.93 s
2025-10-05 10:59:07.333136: Yayy! New best EMA pseudo Dice: 0.6029000282287598
2025-10-05 10:59:08.745242: 
2025-10-05 10:59:08.745602: Epoch 12
2025-10-05 10:59:08.745831: Current learning rate: 0.00928
2025-10-05 10:59:54.609632: Validation loss did not improve from -0.39125. Patience: 1/50
2025-10-05 10:59:54.610299: train_loss -0.5339
2025-10-05 10:59:54.610633: val_loss -0.321
2025-10-05 10:59:54.610924: Pseudo dice [np.float32(0.6041)]
2025-10-05 10:59:54.611193: Epoch time: 45.87 s
2025-10-05 10:59:54.611469: Yayy! New best EMA pseudo Dice: 0.6029999852180481
2025-10-05 10:59:55.696355: 
2025-10-05 10:59:55.696844: Epoch 13
2025-10-05 10:59:55.697201: Current learning rate: 0.00922
2025-10-05 11:00:41.608899: Validation loss did not improve from -0.39125. Patience: 2/50
2025-10-05 11:00:41.609643: train_loss -0.5495
2025-10-05 11:00:41.610132: val_loss -0.3363
2025-10-05 11:00:41.610491: Pseudo dice [np.float32(0.614)]
2025-10-05 11:00:41.610951: Epoch time: 45.91 s
2025-10-05 11:00:41.611284: Yayy! New best EMA pseudo Dice: 0.6040999889373779
2025-10-05 11:00:42.672226: 
2025-10-05 11:00:42.672717: Epoch 14
2025-10-05 11:00:42.673081: Current learning rate: 0.00916
2025-10-05 11:01:28.529525: Validation loss improved from -0.39125 to -0.39176! Patience: 2/50
2025-10-05 11:01:28.530174: train_loss -0.5609
2025-10-05 11:01:28.530409: val_loss -0.3918
2025-10-05 11:01:28.530644: Pseudo dice [np.float32(0.6558)]
2025-10-05 11:01:28.530834: Epoch time: 45.86 s
2025-10-05 11:01:29.040080: Yayy! New best EMA pseudo Dice: 0.6093000173568726
2025-10-05 11:01:30.144460: 
2025-10-05 11:01:30.144868: Epoch 15
2025-10-05 11:01:30.145140: Current learning rate: 0.0091
2025-10-05 11:02:16.060314: Validation loss did not improve from -0.39176. Patience: 1/50
2025-10-05 11:02:16.060769: train_loss -0.563
2025-10-05 11:02:16.061015: val_loss -0.2903
2025-10-05 11:02:16.061183: Pseudo dice [np.float32(0.5999)]
2025-10-05 11:02:16.061370: Epoch time: 45.92 s
2025-10-05 11:02:16.682614: 
2025-10-05 11:02:16.682962: Epoch 16
2025-10-05 11:02:16.683458: Current learning rate: 0.00903
2025-10-05 11:03:02.706194: Validation loss improved from -0.39176 to -0.39947! Patience: 1/50
2025-10-05 11:03:02.707049: train_loss -0.5677
2025-10-05 11:03:02.707251: val_loss -0.3995
2025-10-05 11:03:02.707387: Pseudo dice [np.float32(0.675)]
2025-10-05 11:03:02.707546: Epoch time: 46.03 s
2025-10-05 11:03:02.707687: Yayy! New best EMA pseudo Dice: 0.6150000095367432
2025-10-05 11:03:03.769064: 
2025-10-05 11:03:03.769468: Epoch 17
2025-10-05 11:03:03.769763: Current learning rate: 0.00897
2025-10-05 11:03:49.658463: Validation loss did not improve from -0.39947. Patience: 1/50
2025-10-05 11:03:49.658993: train_loss -0.5847
2025-10-05 11:03:49.659225: val_loss -0.3293
2025-10-05 11:03:49.659444: Pseudo dice [np.float32(0.6163)]
2025-10-05 11:03:49.659665: Epoch time: 45.89 s
2025-10-05 11:03:49.659879: Yayy! New best EMA pseudo Dice: 0.6151999831199646
2025-10-05 11:03:50.728493: 
2025-10-05 11:03:50.728881: Epoch 18
2025-10-05 11:03:50.729198: Current learning rate: 0.00891
2025-10-05 11:04:36.639952: Validation loss did not improve from -0.39947. Patience: 2/50
2025-10-05 11:04:36.640654: train_loss -0.5809
2025-10-05 11:04:36.640982: val_loss -0.3758
2025-10-05 11:04:36.641235: Pseudo dice [np.float32(0.6527)]
2025-10-05 11:04:36.641508: Epoch time: 45.91 s
2025-10-05 11:04:36.641758: Yayy! New best EMA pseudo Dice: 0.6189000010490417
2025-10-05 11:04:37.743650: 
2025-10-05 11:04:37.744016: Epoch 19
2025-10-05 11:04:37.744252: Current learning rate: 0.00885
2025-10-05 11:05:23.628247: Validation loss improved from -0.39947 to -0.41522! Patience: 2/50
2025-10-05 11:05:23.628668: train_loss -0.5858
2025-10-05 11:05:23.628857: val_loss -0.4152
2025-10-05 11:05:23.629036: Pseudo dice [np.float32(0.6801)]
2025-10-05 11:05:23.629209: Epoch time: 45.89 s
2025-10-05 11:05:24.056273: Yayy! New best EMA pseudo Dice: 0.625
2025-10-05 11:05:25.118967: 
2025-10-05 11:05:25.119279: Epoch 20
2025-10-05 11:05:25.119482: Current learning rate: 0.00879
2025-10-05 11:06:11.087990: Validation loss did not improve from -0.41522. Patience: 1/50
2025-10-05 11:06:11.088580: train_loss -0.6048
2025-10-05 11:06:11.088763: val_loss -0.3632
2025-10-05 11:06:11.088919: Pseudo dice [np.float32(0.6495)]
2025-10-05 11:06:11.089079: Epoch time: 45.97 s
2025-10-05 11:06:11.089226: Yayy! New best EMA pseudo Dice: 0.6274999976158142
2025-10-05 11:06:12.153697: 
2025-10-05 11:06:12.154078: Epoch 21
2025-10-05 11:06:12.154378: Current learning rate: 0.00873
2025-10-05 11:06:58.097022: Validation loss did not improve from -0.41522. Patience: 2/50
2025-10-05 11:06:58.097547: train_loss -0.6059
2025-10-05 11:06:58.097761: val_loss -0.3667
2025-10-05 11:06:58.097907: Pseudo dice [np.float32(0.6548)]
2025-10-05 11:06:58.098059: Epoch time: 45.94 s
2025-10-05 11:06:58.098197: Yayy! New best EMA pseudo Dice: 0.6302000284194946
2025-10-05 11:06:59.141944: 
2025-10-05 11:06:59.142303: Epoch 22
2025-10-05 11:06:59.142543: Current learning rate: 0.00867
2025-10-05 11:07:45.073218: Validation loss did not improve from -0.41522. Patience: 3/50
2025-10-05 11:07:45.073887: train_loss -0.6002
2025-10-05 11:07:45.074064: val_loss -0.3479
2025-10-05 11:07:45.074210: Pseudo dice [np.float32(0.6388)]
2025-10-05 11:07:45.074372: Epoch time: 45.93 s
2025-10-05 11:07:45.074568: Yayy! New best EMA pseudo Dice: 0.6310999989509583
2025-10-05 11:07:46.114216: 
2025-10-05 11:07:46.114565: Epoch 23
2025-10-05 11:07:46.114819: Current learning rate: 0.00861
2025-10-05 11:08:32.041396: Validation loss did not improve from -0.41522. Patience: 4/50
2025-10-05 11:08:32.041975: train_loss -0.6136
2025-10-05 11:08:32.042222: val_loss -0.3749
2025-10-05 11:08:32.042461: Pseudo dice [np.float32(0.6707)]
2025-10-05 11:08:32.042706: Epoch time: 45.93 s
2025-10-05 11:08:32.042942: Yayy! New best EMA pseudo Dice: 0.6349999904632568
2025-10-05 11:08:33.420934: 
2025-10-05 11:08:33.421319: Epoch 24
2025-10-05 11:08:33.421557: Current learning rate: 0.00855
2025-10-05 11:09:19.453126: Validation loss did not improve from -0.41522. Patience: 5/50
2025-10-05 11:09:19.453780: train_loss -0.6311
2025-10-05 11:09:19.454064: val_loss -0.3051
2025-10-05 11:09:19.454312: Pseudo dice [np.float32(0.6143)]
2025-10-05 11:09:19.454539: Epoch time: 46.03 s
2025-10-05 11:09:20.518587: 
2025-10-05 11:09:20.518962: Epoch 25
2025-10-05 11:09:20.519260: Current learning rate: 0.00849
2025-10-05 11:10:06.542281: Validation loss did not improve from -0.41522. Patience: 6/50
2025-10-05 11:10:06.542878: train_loss -0.6354
2025-10-05 11:10:06.543232: val_loss -0.3499
2025-10-05 11:10:06.543535: Pseudo dice [np.float32(0.6485)]
2025-10-05 11:10:06.543890: Epoch time: 46.03 s
2025-10-05 11:10:07.169270: 
2025-10-05 11:10:07.169595: Epoch 26
2025-10-05 11:10:07.169892: Current learning rate: 0.00843
2025-10-05 11:10:53.160721: Validation loss did not improve from -0.41522. Patience: 7/50
2025-10-05 11:10:53.161277: train_loss -0.6426
2025-10-05 11:10:53.161435: val_loss -0.3796
2025-10-05 11:10:53.161575: Pseudo dice [np.float32(0.6676)]
2025-10-05 11:10:53.161723: Epoch time: 45.99 s
2025-10-05 11:10:53.161855: Yayy! New best EMA pseudo Dice: 0.6377999782562256
2025-10-05 11:10:54.223377: 
2025-10-05 11:10:54.223656: Epoch 27
2025-10-05 11:10:54.223887: Current learning rate: 0.00836
2025-10-05 11:11:40.181818: Validation loss did not improve from -0.41522. Patience: 8/50
2025-10-05 11:11:40.182465: train_loss -0.653
2025-10-05 11:11:40.182739: val_loss -0.3938
2025-10-05 11:11:40.182946: Pseudo dice [np.float32(0.6733)]
2025-10-05 11:11:40.183181: Epoch time: 45.96 s
2025-10-05 11:11:40.183399: Yayy! New best EMA pseudo Dice: 0.6413999795913696
2025-10-05 11:11:41.335297: 
2025-10-05 11:11:41.335627: Epoch 28
2025-10-05 11:11:41.335910: Current learning rate: 0.0083
2025-10-05 11:12:27.354017: Validation loss did not improve from -0.41522. Patience: 9/50
2025-10-05 11:12:27.354610: train_loss -0.642
2025-10-05 11:12:27.354775: val_loss -0.3142
2025-10-05 11:12:27.354901: Pseudo dice [np.float32(0.6197)]
2025-10-05 11:12:27.355038: Epoch time: 46.02 s
2025-10-05 11:12:27.969997: 
2025-10-05 11:12:27.970357: Epoch 29
2025-10-05 11:12:27.970594: Current learning rate: 0.00824
2025-10-05 11:13:13.957292: Validation loss did not improve from -0.41522. Patience: 10/50
2025-10-05 11:13:13.957717: train_loss -0.6497
2025-10-05 11:13:13.957910: val_loss -0.2905
2025-10-05 11:13:13.958040: Pseudo dice [np.float32(0.6261)]
2025-10-05 11:13:13.958217: Epoch time: 45.99 s
2025-10-05 11:13:15.027195: 
2025-10-05 11:13:15.027516: Epoch 30
2025-10-05 11:13:15.027737: Current learning rate: 0.00818
2025-10-05 11:14:01.029788: Validation loss did not improve from -0.41522. Patience: 11/50
2025-10-05 11:14:01.030293: train_loss -0.6555
2025-10-05 11:14:01.030492: val_loss -0.3148
2025-10-05 11:14:01.030711: Pseudo dice [np.float32(0.6199)]
2025-10-05 11:14:01.030866: Epoch time: 46.0 s
2025-10-05 11:14:01.652769: 
2025-10-05 11:14:01.653110: Epoch 31
2025-10-05 11:14:01.653399: Current learning rate: 0.00812
2025-10-05 11:14:47.633943: Validation loss did not improve from -0.41522. Patience: 12/50
2025-10-05 11:14:47.634407: train_loss -0.6647
2025-10-05 11:14:47.634610: val_loss -0.3968
2025-10-05 11:14:47.634813: Pseudo dice [np.float32(0.6743)]
2025-10-05 11:14:47.634994: Epoch time: 45.98 s
2025-10-05 11:14:48.258218: 
2025-10-05 11:14:48.258556: Epoch 32
2025-10-05 11:14:48.258854: Current learning rate: 0.00806
2025-10-05 11:15:34.243247: Validation loss did not improve from -0.41522. Patience: 13/50
2025-10-05 11:15:34.243954: train_loss -0.6652
2025-10-05 11:15:34.244163: val_loss -0.3446
2025-10-05 11:15:34.244374: Pseudo dice [np.float32(0.6343)]
2025-10-05 11:15:34.244587: Epoch time: 45.99 s
2025-10-05 11:15:34.868785: 
2025-10-05 11:15:34.869204: Epoch 33
2025-10-05 11:15:34.869545: Current learning rate: 0.008
2025-10-05 11:16:20.873883: Validation loss did not improve from -0.41522. Patience: 14/50
2025-10-05 11:16:20.874375: train_loss -0.6611
2025-10-05 11:16:20.874532: val_loss -0.3314
2025-10-05 11:16:20.874705: Pseudo dice [np.float32(0.6336)]
2025-10-05 11:16:20.874890: Epoch time: 46.01 s
2025-10-05 11:16:21.500612: 
2025-10-05 11:16:21.501019: Epoch 34
2025-10-05 11:16:21.501336: Current learning rate: 0.00793
2025-10-05 11:17:07.490489: Validation loss did not improve from -0.41522. Patience: 15/50
2025-10-05 11:17:07.491246: train_loss -0.6647
2025-10-05 11:17:07.491510: val_loss -0.365
2025-10-05 11:17:07.491670: Pseudo dice [np.float32(0.6512)]
2025-10-05 11:17:07.491851: Epoch time: 45.99 s
2025-10-05 11:17:08.565810: 
2025-10-05 11:17:08.566175: Epoch 35
2025-10-05 11:17:08.566486: Current learning rate: 0.00787
2025-10-05 11:17:54.480341: Validation loss did not improve from -0.41522. Patience: 16/50
2025-10-05 11:17:54.480893: train_loss -0.679
2025-10-05 11:17:54.481151: val_loss -0.3667
2025-10-05 11:17:54.481419: Pseudo dice [np.float32(0.6766)]
2025-10-05 11:17:54.481727: Epoch time: 45.92 s
2025-10-05 11:17:54.482040: Yayy! New best EMA pseudo Dice: 0.6437000036239624
2025-10-05 11:17:55.943842: 
2025-10-05 11:17:55.944187: Epoch 36
2025-10-05 11:17:55.944362: Current learning rate: 0.00781
2025-10-05 11:18:41.936346: Validation loss did not improve from -0.41522. Patience: 17/50
2025-10-05 11:18:41.937037: train_loss -0.6765
2025-10-05 11:18:41.937261: val_loss -0.3406
2025-10-05 11:18:41.937462: Pseudo dice [np.float32(0.615)]
2025-10-05 11:18:41.937650: Epoch time: 45.99 s
2025-10-05 11:18:42.559607: 
2025-10-05 11:18:42.559950: Epoch 37
2025-10-05 11:18:42.560220: Current learning rate: 0.00775
2025-10-05 11:19:28.546970: Validation loss did not improve from -0.41522. Patience: 18/50
2025-10-05 11:19:28.547421: train_loss -0.6887
2025-10-05 11:19:28.547606: val_loss -0.3656
2025-10-05 11:19:28.547799: Pseudo dice [np.float32(0.6652)]
2025-10-05 11:19:28.547939: Epoch time: 45.99 s
2025-10-05 11:19:29.174042: 
2025-10-05 11:19:29.174308: Epoch 38
2025-10-05 11:19:29.174500: Current learning rate: 0.00769
2025-10-05 11:20:15.109486: Validation loss did not improve from -0.41522. Patience: 19/50
2025-10-05 11:20:15.110112: train_loss -0.6822
2025-10-05 11:20:15.110286: val_loss -0.3522
2025-10-05 11:20:15.110432: Pseudo dice [np.float32(0.641)]
2025-10-05 11:20:15.110579: Epoch time: 45.94 s
2025-10-05 11:20:15.742045: 
2025-10-05 11:20:15.742347: Epoch 39
2025-10-05 11:20:15.742558: Current learning rate: 0.00763
2025-10-05 11:21:01.711815: Validation loss did not improve from -0.41522. Patience: 20/50
2025-10-05 11:21:01.712290: train_loss -0.6888
2025-10-05 11:21:01.712593: val_loss -0.3791
2025-10-05 11:21:01.712856: Pseudo dice [np.float32(0.6619)]
2025-10-05 11:21:01.713115: Epoch time: 45.97 s
2025-10-05 11:21:02.210247: Yayy! New best EMA pseudo Dice: 0.6449000239372253
2025-10-05 11:21:03.338460: 
2025-10-05 11:21:03.338856: Epoch 40
2025-10-05 11:21:03.339128: Current learning rate: 0.00756
2025-10-05 11:21:49.289548: Validation loss did not improve from -0.41522. Patience: 21/50
2025-10-05 11:21:49.290147: train_loss -0.7076
2025-10-05 11:21:49.290396: val_loss -0.3249
2025-10-05 11:21:49.290632: Pseudo dice [np.float32(0.6425)]
2025-10-05 11:21:49.290898: Epoch time: 45.95 s
2025-10-05 11:21:49.923090: 
2025-10-05 11:21:49.923413: Epoch 41
2025-10-05 11:21:49.923634: Current learning rate: 0.0075
2025-10-05 11:22:35.849026: Validation loss did not improve from -0.41522. Patience: 22/50
2025-10-05 11:22:35.849525: train_loss -0.7013
2025-10-05 11:22:35.849701: val_loss -0.4016
2025-10-05 11:22:35.849861: Pseudo dice [np.float32(0.6633)]
2025-10-05 11:22:35.850050: Epoch time: 45.93 s
2025-10-05 11:22:35.850208: Yayy! New best EMA pseudo Dice: 0.6464999914169312
2025-10-05 11:22:36.896915: 
2025-10-05 11:22:36.897281: Epoch 42
2025-10-05 11:22:36.897540: Current learning rate: 0.00744
2025-10-05 11:23:22.895962: Validation loss did not improve from -0.41522. Patience: 23/50
2025-10-05 11:23:22.896607: train_loss -0.6982
2025-10-05 11:23:22.896808: val_loss -0.3282
2025-10-05 11:23:22.896997: Pseudo dice [np.float32(0.6511)]
2025-10-05 11:23:22.897195: Epoch time: 46.0 s
2025-10-05 11:23:22.897368: Yayy! New best EMA pseudo Dice: 0.6470000147819519
2025-10-05 11:23:23.988529: 
2025-10-05 11:23:23.988861: Epoch 43
2025-10-05 11:23:23.989094: Current learning rate: 0.00738
2025-10-05 11:24:09.942709: Validation loss did not improve from -0.41522. Patience: 24/50
2025-10-05 11:24:09.943260: train_loss -0.6976
2025-10-05 11:24:09.943558: val_loss -0.3945
2025-10-05 11:24:09.943793: Pseudo dice [np.float32(0.6785)]
2025-10-05 11:24:09.944012: Epoch time: 45.96 s
2025-10-05 11:24:09.944267: Yayy! New best EMA pseudo Dice: 0.6500999927520752
2025-10-05 11:24:11.002362: 
2025-10-05 11:24:11.002667: Epoch 44
2025-10-05 11:24:11.002888: Current learning rate: 0.00732
2025-10-05 11:24:56.920298: Validation loss did not improve from -0.41522. Patience: 25/50
2025-10-05 11:24:56.920916: train_loss -0.7108
2025-10-05 11:24:56.921141: val_loss -0.3589
2025-10-05 11:24:56.921270: Pseudo dice [np.float32(0.6662)]
2025-10-05 11:24:56.921408: Epoch time: 45.92 s
2025-10-05 11:24:57.460810: Yayy! New best EMA pseudo Dice: 0.6517000198364258
2025-10-05 11:24:58.645078: 
2025-10-05 11:24:58.645341: Epoch 45
2025-10-05 11:24:58.645599: Current learning rate: 0.00725
2025-10-05 11:25:44.626958: Validation loss did not improve from -0.41522. Patience: 26/50
2025-10-05 11:25:44.627555: train_loss -0.7034
2025-10-05 11:25:44.627910: val_loss -0.3406
2025-10-05 11:25:44.628101: Pseudo dice [np.float32(0.6321)]
2025-10-05 11:25:44.628351: Epoch time: 45.98 s
2025-10-05 11:25:45.253661: 
2025-10-05 11:25:45.254019: Epoch 46
2025-10-05 11:25:45.254220: Current learning rate: 0.00719
2025-10-05 11:26:31.185884: Validation loss did not improve from -0.41522. Patience: 27/50
2025-10-05 11:26:31.186480: train_loss -0.7179
2025-10-05 11:26:31.186700: val_loss -0.2883
2025-10-05 11:26:31.186895: Pseudo dice [np.float32(0.6176)]
2025-10-05 11:26:31.187061: Epoch time: 45.93 s
2025-10-05 11:26:32.040238: 
2025-10-05 11:26:32.040694: Epoch 47
2025-10-05 11:26:32.040932: Current learning rate: 0.00713
2025-10-05 11:27:18.086293: Validation loss did not improve from -0.41522. Patience: 28/50
2025-10-05 11:27:18.086712: train_loss -0.7212
2025-10-05 11:27:18.086867: val_loss -0.3974
2025-10-05 11:27:18.087026: Pseudo dice [np.float32(0.6859)]
2025-10-05 11:27:18.087198: Epoch time: 46.05 s
2025-10-05 11:27:18.697578: 
2025-10-05 11:27:18.697869: Epoch 48
2025-10-05 11:27:18.698056: Current learning rate: 0.00707
2025-10-05 11:28:04.799062: Validation loss did not improve from -0.41522. Patience: 29/50
2025-10-05 11:28:04.799710: train_loss -0.7223
2025-10-05 11:28:04.799921: val_loss -0.339
2025-10-05 11:28:04.800093: Pseudo dice [np.float32(0.6524)]
2025-10-05 11:28:04.800295: Epoch time: 46.1 s
2025-10-05 11:28:05.410952: 
2025-10-05 11:28:05.411291: Epoch 49
2025-10-05 11:28:05.411531: Current learning rate: 0.007
2025-10-05 11:28:51.452509: Validation loss did not improve from -0.41522. Patience: 30/50
2025-10-05 11:28:51.453085: train_loss -0.7336
2025-10-05 11:28:51.453386: val_loss -0.3307
2025-10-05 11:28:51.453736: Pseudo dice [np.float32(0.6439)]
2025-10-05 11:28:51.454034: Epoch time: 46.04 s
2025-10-05 11:28:52.490178: 
2025-10-05 11:28:52.490492: Epoch 50
2025-10-05 11:28:52.490780: Current learning rate: 0.00694
2025-10-05 11:29:38.503236: Validation loss did not improve from -0.41522. Patience: 31/50
2025-10-05 11:29:38.503960: train_loss -0.7305
2025-10-05 11:29:38.504239: val_loss -0.3592
2025-10-05 11:29:38.504419: Pseudo dice [np.float32(0.6539)]
2025-10-05 11:29:38.504629: Epoch time: 46.01 s
2025-10-05 11:29:39.123592: 
2025-10-05 11:29:39.123892: Epoch 51
2025-10-05 11:29:39.124084: Current learning rate: 0.00688
2025-10-05 11:30:25.175898: Validation loss did not improve from -0.41522. Patience: 32/50
2025-10-05 11:30:25.176438: train_loss -0.7393
2025-10-05 11:30:25.176744: val_loss -0.3537
2025-10-05 11:30:25.177035: Pseudo dice [np.float32(0.6658)]
2025-10-05 11:30:25.177307: Epoch time: 46.05 s
2025-10-05 11:30:25.177541: Yayy! New best EMA pseudo Dice: 0.6518999934196472
2025-10-05 11:30:26.222803: 
2025-10-05 11:30:26.223170: Epoch 52
2025-10-05 11:30:26.223366: Current learning rate: 0.00682
2025-10-05 11:31:12.241168: Validation loss did not improve from -0.41522. Patience: 33/50
2025-10-05 11:31:12.241826: train_loss -0.7458
2025-10-05 11:31:12.242079: val_loss -0.3767
2025-10-05 11:31:12.242440: Pseudo dice [np.float32(0.686)]
2025-10-05 11:31:12.242605: Epoch time: 46.02 s
2025-10-05 11:31:12.242798: Yayy! New best EMA pseudo Dice: 0.6553000211715698
2025-10-05 11:31:13.291973: 
2025-10-05 11:31:13.292248: Epoch 53
2025-10-05 11:31:13.292438: Current learning rate: 0.00675
2025-10-05 11:31:59.205782: Validation loss did not improve from -0.41522. Patience: 34/50
2025-10-05 11:31:59.206215: train_loss -0.7482
2025-10-05 11:31:59.206381: val_loss -0.3746
2025-10-05 11:31:59.206522: Pseudo dice [np.float32(0.6675)]
2025-10-05 11:31:59.206680: Epoch time: 45.91 s
2025-10-05 11:31:59.206813: Yayy! New best EMA pseudo Dice: 0.6565999984741211
2025-10-05 11:32:00.260576: 
2025-10-05 11:32:00.260812: Epoch 54
2025-10-05 11:32:00.260989: Current learning rate: 0.00669
2025-10-05 11:32:46.242750: Validation loss did not improve from -0.41522. Patience: 35/50
2025-10-05 11:32:46.243382: train_loss -0.7434
2025-10-05 11:32:46.243613: val_loss -0.3868
2025-10-05 11:32:46.243753: Pseudo dice [np.float32(0.684)]
2025-10-05 11:32:46.243896: Epoch time: 45.98 s
2025-10-05 11:32:46.692455: Yayy! New best EMA pseudo Dice: 0.6593000292778015
2025-10-05 11:32:47.734612: 
2025-10-05 11:32:47.735022: Epoch 55
2025-10-05 11:32:47.735245: Current learning rate: 0.00663
2025-10-05 11:33:33.865498: Validation loss did not improve from -0.41522. Patience: 36/50
2025-10-05 11:33:33.865972: train_loss -0.7575
2025-10-05 11:33:33.866156: val_loss -0.3627
2025-10-05 11:33:33.866338: Pseudo dice [np.float32(0.658)]
2025-10-05 11:33:33.866490: Epoch time: 46.13 s
2025-10-05 11:33:34.477476: 
2025-10-05 11:33:34.477873: Epoch 56
2025-10-05 11:33:34.478113: Current learning rate: 0.00657
2025-10-05 11:34:20.481833: Validation loss did not improve from -0.41522. Patience: 37/50
2025-10-05 11:34:20.482454: train_loss -0.7514
2025-10-05 11:34:20.482679: val_loss -0.3656
2025-10-05 11:34:20.482896: Pseudo dice [np.float32(0.6592)]
2025-10-05 11:34:20.483104: Epoch time: 46.01 s
2025-10-05 11:34:21.105094: 
2025-10-05 11:34:21.105397: Epoch 57
2025-10-05 11:34:21.105611: Current learning rate: 0.0065
2025-10-05 11:35:07.113972: Validation loss did not improve from -0.41522. Patience: 38/50
2025-10-05 11:35:07.114509: train_loss -0.7572
2025-10-05 11:35:07.114799: val_loss -0.318
2025-10-05 11:35:07.115065: Pseudo dice [np.float32(0.6285)]
2025-10-05 11:35:07.115397: Epoch time: 46.01 s
2025-10-05 11:35:07.727369: 
2025-10-05 11:35:07.727712: Epoch 58
2025-10-05 11:35:07.727986: Current learning rate: 0.00644
2025-10-05 11:35:53.705140: Validation loss did not improve from -0.41522. Patience: 39/50
2025-10-05 11:35:53.705644: train_loss -0.7526
2025-10-05 11:35:53.705825: val_loss -0.3699
2025-10-05 11:35:53.705954: Pseudo dice [np.float32(0.6722)]
2025-10-05 11:35:53.706111: Epoch time: 45.98 s
2025-10-05 11:35:54.577139: 
2025-10-05 11:35:54.577395: Epoch 59
2025-10-05 11:35:54.577563: Current learning rate: 0.00638
2025-10-05 11:36:40.562739: Validation loss did not improve from -0.41522. Patience: 40/50
2025-10-05 11:36:40.563243: train_loss -0.7599
2025-10-05 11:36:40.563408: val_loss -0.3211
2025-10-05 11:36:40.563550: Pseudo dice [np.float32(0.6545)]
2025-10-05 11:36:40.563725: Epoch time: 45.99 s
2025-10-05 11:36:41.626165: 
2025-10-05 11:36:41.626507: Epoch 60
2025-10-05 11:36:41.626686: Current learning rate: 0.00631
2025-10-05 11:37:27.698512: Validation loss did not improve from -0.41522. Patience: 41/50
2025-10-05 11:37:27.699160: train_loss -0.7644
2025-10-05 11:37:27.699398: val_loss -0.349
2025-10-05 11:37:27.699629: Pseudo dice [np.float32(0.655)]
2025-10-05 11:37:27.699904: Epoch time: 46.07 s
2025-10-05 11:37:28.317993: 
2025-10-05 11:37:28.318418: Epoch 61
2025-10-05 11:37:28.318701: Current learning rate: 0.00625
2025-10-05 11:38:14.304494: Validation loss did not improve from -0.41522. Patience: 42/50
2025-10-05 11:38:14.305082: train_loss -0.7644
2025-10-05 11:38:14.305439: val_loss -0.3798
2025-10-05 11:38:14.305714: Pseudo dice [np.float32(0.6867)]
2025-10-05 11:38:14.306064: Epoch time: 45.99 s
2025-10-05 11:38:14.306284: Yayy! New best EMA pseudo Dice: 0.660099983215332
2025-10-05 11:38:15.369600: 
2025-10-05 11:38:15.369934: Epoch 62
2025-10-05 11:38:15.370142: Current learning rate: 0.00619
2025-10-05 11:39:01.381527: Validation loss did not improve from -0.41522. Patience: 43/50
2025-10-05 11:39:01.382318: train_loss -0.7687
2025-10-05 11:39:01.382649: val_loss -0.2882
2025-10-05 11:39:01.382912: Pseudo dice [np.float32(0.6145)]
2025-10-05 11:39:01.383256: Epoch time: 46.01 s
2025-10-05 11:39:02.021480: 
2025-10-05 11:39:02.021785: Epoch 63
2025-10-05 11:39:02.022011: Current learning rate: 0.00612
2025-10-05 11:39:48.026318: Validation loss did not improve from -0.41522. Patience: 44/50
2025-10-05 11:39:48.026809: train_loss -0.7633
2025-10-05 11:39:48.026981: val_loss -0.3345
2025-10-05 11:39:48.027119: Pseudo dice [np.float32(0.6579)]
2025-10-05 11:39:48.027261: Epoch time: 46.01 s
2025-10-05 11:39:48.652422: 
2025-10-05 11:39:48.652821: Epoch 64
2025-10-05 11:39:48.653056: Current learning rate: 0.00606
2025-10-05 11:40:34.656672: Validation loss did not improve from -0.41522. Patience: 45/50
2025-10-05 11:40:34.657494: train_loss -0.7699
2025-10-05 11:40:34.657840: val_loss -0.263
2025-10-05 11:40:34.658193: Pseudo dice [np.float32(0.6084)]
2025-10-05 11:40:34.658516: Epoch time: 46.01 s
2025-10-05 11:40:35.805936: 
2025-10-05 11:40:35.806272: Epoch 65
2025-10-05 11:40:35.806530: Current learning rate: 0.006
2025-10-05 11:41:21.912649: Validation loss did not improve from -0.41522. Patience: 46/50
2025-10-05 11:41:21.913163: train_loss -0.7769
2025-10-05 11:41:21.913327: val_loss -0.3784
2025-10-05 11:41:21.913478: Pseudo dice [np.float32(0.6693)]
2025-10-05 11:41:21.913659: Epoch time: 46.11 s
2025-10-05 11:41:22.543939: 
2025-10-05 11:41:22.544232: Epoch 66
2025-10-05 11:41:22.544394: Current learning rate: 0.00593
2025-10-05 11:42:08.653469: Validation loss did not improve from -0.41522. Patience: 47/50
2025-10-05 11:42:08.654095: train_loss -0.7731
2025-10-05 11:42:08.654302: val_loss -0.3555
2025-10-05 11:42:08.654489: Pseudo dice [np.float32(0.6639)]
2025-10-05 11:42:08.654633: Epoch time: 46.11 s
2025-10-05 11:42:09.278982: 
2025-10-05 11:42:09.279228: Epoch 67
2025-10-05 11:42:09.279392: Current learning rate: 0.00587
2025-10-05 11:42:55.272639: Validation loss did not improve from -0.41522. Patience: 48/50
2025-10-05 11:42:55.273089: train_loss -0.7794
2025-10-05 11:42:55.273292: val_loss -0.3279
2025-10-05 11:42:55.273486: Pseudo dice [np.float32(0.6607)]
2025-10-05 11:42:55.273643: Epoch time: 45.99 s
2025-10-05 11:42:55.896429: 
2025-10-05 11:42:55.896721: Epoch 68
2025-10-05 11:42:55.896997: Current learning rate: 0.00581
2025-10-05 11:43:41.923458: Validation loss did not improve from -0.41522. Patience: 49/50
2025-10-05 11:43:41.924032: train_loss -0.7884
2025-10-05 11:43:41.924269: val_loss -0.3336
2025-10-05 11:43:41.924444: Pseudo dice [np.float32(0.6551)]
2025-10-05 11:43:41.924602: Epoch time: 46.03 s
2025-10-05 11:43:42.549169: 
2025-10-05 11:43:42.549469: Epoch 69
2025-10-05 11:43:42.549647: Current learning rate: 0.00574
2025-10-05 11:44:28.573460: Validation loss did not improve from -0.41522. Patience: 50/50
2025-10-05 11:44:28.573899: train_loss -0.7814
2025-10-05 11:44:28.574110: val_loss -0.3282
2025-10-05 11:44:28.574323: Pseudo dice [np.float32(0.6611)]
2025-10-05 11:44:28.574534: Epoch time: 46.03 s
2025-10-05 11:44:29.618620: 
2025-10-05 11:44:29.619127: Epoch 70
2025-10-05 11:44:29.619448: Current learning rate: 0.00568
2025-10-05 11:45:15.717711: Validation loss did not improve from -0.41522. Patience: 51/50
2025-10-05 11:45:15.718402: train_loss -0.783
2025-10-05 11:45:15.718653: val_loss -0.3196
2025-10-05 11:45:15.718834: Pseudo dice [np.float32(0.6494)]
2025-10-05 11:45:15.719059: Epoch time: 46.1 s
2025-10-05 11:45:16.608028: 
2025-10-05 11:45:16.608369: Epoch 71
2025-10-05 11:45:16.608674: Current learning rate: 0.00562
2025-10-05 11:46:02.558882: Validation loss did not improve from -0.41522. Patience: 52/50
2025-10-05 11:46:02.559391: train_loss -0.7843
2025-10-05 11:46:02.559650: val_loss -0.2962
2025-10-05 11:46:02.559996: Pseudo dice [np.float32(0.6345)]
2025-10-05 11:46:02.560157: Epoch time: 45.95 s
2025-10-05 11:46:03.183521: 
2025-10-05 11:46:03.183873: Epoch 72
2025-10-05 11:46:03.184172: Current learning rate: 0.00555
2025-10-05 11:46:49.115662: Validation loss did not improve from -0.41522. Patience: 53/50
2025-10-05 11:46:49.116210: train_loss -0.7922
2025-10-05 11:46:49.116395: val_loss -0.3648
2025-10-05 11:46:49.116537: Pseudo dice [np.float32(0.677)]
2025-10-05 11:46:49.116695: Epoch time: 45.93 s
2025-10-05 11:46:49.742166: 
2025-10-05 11:46:49.742432: Epoch 73
2025-10-05 11:46:49.742649: Current learning rate: 0.00549
2025-10-05 11:47:35.701888: Validation loss did not improve from -0.41522. Patience: 54/50
2025-10-05 11:47:35.702618: train_loss -0.7894
2025-10-05 11:47:35.702888: val_loss -0.3371
2025-10-05 11:47:35.703118: Pseudo dice [np.float32(0.6525)]
2025-10-05 11:47:35.703289: Epoch time: 45.96 s
2025-10-05 11:47:36.338660: 
2025-10-05 11:47:36.339002: Epoch 74
2025-10-05 11:47:36.339208: Current learning rate: 0.00542
2025-10-05 11:48:22.209379: Validation loss did not improve from -0.41522. Patience: 55/50
2025-10-05 11:48:22.209974: train_loss -0.7941
2025-10-05 11:48:22.210240: val_loss -0.3457
2025-10-05 11:48:22.210390: Pseudo dice [np.float32(0.6605)]
2025-10-05 11:48:22.210570: Epoch time: 45.87 s
2025-10-05 11:48:23.344921: 
2025-10-05 11:48:23.345245: Epoch 75
2025-10-05 11:48:23.345462: Current learning rate: 0.00536
2025-10-05 11:49:09.375862: Validation loss did not improve from -0.41522. Patience: 56/50
2025-10-05 11:49:09.376282: train_loss -0.8023
2025-10-05 11:49:09.376454: val_loss -0.3474
2025-10-05 11:49:09.376614: Pseudo dice [np.float32(0.6673)]
2025-10-05 11:49:09.376756: Epoch time: 46.03 s
2025-10-05 11:49:10.002717: 
2025-10-05 11:49:10.003065: Epoch 76
2025-10-05 11:49:10.003280: Current learning rate: 0.00529
2025-10-05 11:49:56.076261: Validation loss did not improve from -0.41522. Patience: 57/50
2025-10-05 11:49:56.076966: train_loss -0.8005
2025-10-05 11:49:56.077266: val_loss -0.2888
2025-10-05 11:49:56.077519: Pseudo dice [np.float32(0.6295)]
2025-10-05 11:49:56.077726: Epoch time: 46.07 s
2025-10-05 11:49:56.704157: 
2025-10-05 11:49:56.704553: Epoch 77
2025-10-05 11:49:56.704815: Current learning rate: 0.00523
2025-10-05 11:50:42.800128: Validation loss did not improve from -0.41522. Patience: 58/50
2025-10-05 11:50:42.800539: train_loss -0.8053
2025-10-05 11:50:42.800743: val_loss -0.3445
2025-10-05 11:50:42.800919: Pseudo dice [np.float32(0.6667)]
2025-10-05 11:50:42.801194: Epoch time: 46.1 s
2025-10-05 11:50:43.437074: 
2025-10-05 11:50:43.437440: Epoch 78
2025-10-05 11:50:43.437646: Current learning rate: 0.00517
2025-10-05 11:51:29.560524: Validation loss did not improve from -0.41522. Patience: 59/50
2025-10-05 11:51:29.561235: train_loss -0.7999
2025-10-05 11:51:29.561562: val_loss -0.3398
2025-10-05 11:51:29.561850: Pseudo dice [np.float32(0.6488)]
2025-10-05 11:51:29.562046: Epoch time: 46.12 s
2025-10-05 11:51:30.203479: 
2025-10-05 11:51:30.203813: Epoch 79
2025-10-05 11:51:30.204031: Current learning rate: 0.0051
2025-10-05 11:52:16.318484: Validation loss did not improve from -0.41522. Patience: 60/50
2025-10-05 11:52:16.318940: train_loss -0.8026
2025-10-05 11:52:16.319145: val_loss -0.2816
2025-10-05 11:52:16.319317: Pseudo dice [np.float32(0.6258)]
2025-10-05 11:52:16.319496: Epoch time: 46.12 s
2025-10-05 11:52:17.385556: 
2025-10-05 11:52:17.385891: Epoch 80
2025-10-05 11:52:17.386169: Current learning rate: 0.00504
2025-10-05 11:53:03.533105: Validation loss did not improve from -0.41522. Patience: 61/50
2025-10-05 11:53:03.533818: train_loss -0.8079
2025-10-05 11:53:03.534152: val_loss -0.3265
2025-10-05 11:53:03.534473: Pseudo dice [np.float32(0.6666)]
2025-10-05 11:53:03.534765: Epoch time: 46.15 s
2025-10-05 11:53:04.167026: 
2025-10-05 11:53:04.167478: Epoch 81
2025-10-05 11:53:04.167770: Current learning rate: 0.00497
2025-10-05 11:53:50.316022: Validation loss did not improve from -0.41522. Patience: 62/50
2025-10-05 11:53:50.316447: train_loss -0.8092
2025-10-05 11:53:50.316627: val_loss -0.3523
2025-10-05 11:53:50.316791: Pseudo dice [np.float32(0.668)]
2025-10-05 11:53:50.316950: Epoch time: 46.15 s
2025-10-05 11:53:51.213516: 
2025-10-05 11:53:51.213794: Epoch 82
2025-10-05 11:53:51.213985: Current learning rate: 0.00491
2025-10-05 11:54:37.354100: Validation loss did not improve from -0.41522. Patience: 63/50
2025-10-05 11:54:37.354662: train_loss -0.8114
2025-10-05 11:54:37.354810: val_loss -0.2769
2025-10-05 11:54:37.354944: Pseudo dice [np.float32(0.6351)]
2025-10-05 11:54:37.355087: Epoch time: 46.14 s
2025-10-05 11:54:37.966737: 
2025-10-05 11:54:37.967149: Epoch 83
2025-10-05 11:54:37.967367: Current learning rate: 0.00484
2025-10-05 11:55:24.079484: Validation loss did not improve from -0.41522. Patience: 64/50
2025-10-05 11:55:24.080094: train_loss -0.8111
2025-10-05 11:55:24.080547: val_loss -0.349
2025-10-05 11:55:24.080894: Pseudo dice [np.float32(0.6667)]
2025-10-05 11:55:24.081194: Epoch time: 46.11 s
2025-10-05 11:55:24.702173: 
2025-10-05 11:55:24.702486: Epoch 84
2025-10-05 11:55:24.702698: Current learning rate: 0.00478
2025-10-05 11:56:10.722903: Validation loss did not improve from -0.41522. Patience: 65/50
2025-10-05 11:56:10.723418: train_loss -0.8177
2025-10-05 11:56:10.723622: val_loss -0.3295
2025-10-05 11:56:10.723766: Pseudo dice [np.float32(0.6572)]
2025-10-05 11:56:10.723962: Epoch time: 46.02 s
2025-10-05 11:56:11.771524: 
2025-10-05 11:56:11.771934: Epoch 85
2025-10-05 11:56:11.772175: Current learning rate: 0.00471
2025-10-05 11:56:57.703801: Validation loss did not improve from -0.41522. Patience: 66/50
2025-10-05 11:56:57.704270: train_loss -0.8176
2025-10-05 11:56:57.704453: val_loss -0.3362
2025-10-05 11:56:57.704611: Pseudo dice [np.float32(0.6719)]
2025-10-05 11:56:57.704759: Epoch time: 45.93 s
2025-10-05 11:56:58.320577: 
2025-10-05 11:56:58.321028: Epoch 86
2025-10-05 11:56:58.321252: Current learning rate: 0.00465
2025-10-05 11:57:44.306791: Validation loss did not improve from -0.41522. Patience: 67/50
2025-10-05 11:57:44.307395: train_loss -0.8182
2025-10-05 11:57:44.307586: val_loss -0.3667
2025-10-05 11:57:44.307747: Pseudo dice [np.float32(0.675)]
2025-10-05 11:57:44.307928: Epoch time: 45.99 s
2025-10-05 11:57:44.922076: 
2025-10-05 11:57:44.922404: Epoch 87
2025-10-05 11:57:44.922631: Current learning rate: 0.00458
2025-10-05 11:58:31.044357: Validation loss did not improve from -0.41522. Patience: 68/50
2025-10-05 11:58:31.044944: train_loss -0.8186
2025-10-05 11:58:31.045287: val_loss -0.2989
2025-10-05 11:58:31.045630: Pseudo dice [np.float32(0.6333)]
2025-10-05 11:58:31.046033: Epoch time: 46.12 s
2025-10-05 11:58:31.655486: 
2025-10-05 11:58:31.655839: Epoch 88
2025-10-05 11:58:31.656183: Current learning rate: 0.00452
2025-10-05 11:59:17.646640: Validation loss did not improve from -0.41522. Patience: 69/50
2025-10-05 11:59:17.647282: train_loss -0.8231
2025-10-05 11:59:17.647463: val_loss -0.3326
2025-10-05 11:59:17.647619: Pseudo dice [np.float32(0.6649)]
2025-10-05 11:59:17.647794: Epoch time: 45.99 s
2025-10-05 11:59:18.258090: 
2025-10-05 11:59:18.258461: Epoch 89
2025-10-05 11:59:18.258652: Current learning rate: 0.00445
2025-10-05 12:00:04.366220: Validation loss did not improve from -0.41522. Patience: 70/50
2025-10-05 12:00:04.366902: train_loss -0.8192
2025-10-05 12:00:04.367283: val_loss -0.3039
2025-10-05 12:00:04.367673: Pseudo dice [np.float32(0.6576)]
2025-10-05 12:00:04.368054: Epoch time: 46.11 s
2025-10-05 12:00:05.392272: 
2025-10-05 12:00:05.392540: Epoch 90
2025-10-05 12:00:05.392820: Current learning rate: 0.00438
2025-10-05 12:00:51.443068: Validation loss did not improve from -0.41522. Patience: 71/50
2025-10-05 12:00:51.443593: train_loss -0.8216
2025-10-05 12:00:51.443750: val_loss -0.3473
2025-10-05 12:00:51.443932: Pseudo dice [np.float32(0.6728)]
2025-10-05 12:00:51.444080: Epoch time: 46.05 s
2025-10-05 12:00:52.060420: 
2025-10-05 12:00:52.060740: Epoch 91
2025-10-05 12:00:52.060924: Current learning rate: 0.00432
2025-10-05 12:01:38.074240: Validation loss did not improve from -0.41522. Patience: 72/50
2025-10-05 12:01:38.074862: train_loss -0.8275
2025-10-05 12:01:38.075043: val_loss -0.3006
2025-10-05 12:01:38.075176: Pseudo dice [np.float32(0.6453)]
2025-10-05 12:01:38.075326: Epoch time: 46.02 s
2025-10-05 12:01:38.694205: 
2025-10-05 12:01:38.694494: Epoch 92
2025-10-05 12:01:38.694672: Current learning rate: 0.00425
2025-10-05 12:02:24.690016: Validation loss did not improve from -0.41522. Patience: 73/50
2025-10-05 12:02:24.690619: train_loss -0.8242
2025-10-05 12:02:24.690804: val_loss -0.3125
2025-10-05 12:02:24.691004: Pseudo dice [np.float32(0.6537)]
2025-10-05 12:02:24.691196: Epoch time: 46.0 s
2025-10-05 12:02:25.303244: 
2025-10-05 12:02:25.303503: Epoch 93
2025-10-05 12:02:25.303719: Current learning rate: 0.00419
2025-10-05 12:03:11.326648: Validation loss did not improve from -0.41522. Patience: 74/50
2025-10-05 12:03:11.327059: train_loss -0.8171
2025-10-05 12:03:11.327227: val_loss -0.3258
2025-10-05 12:03:11.327437: Pseudo dice [np.float32(0.6646)]
2025-10-05 12:03:11.327590: Epoch time: 46.02 s
2025-10-05 12:03:11.940207: 
2025-10-05 12:03:11.940448: Epoch 94
2025-10-05 12:03:11.940623: Current learning rate: 0.00412
2025-10-05 12:03:57.937511: Validation loss did not improve from -0.41522. Patience: 75/50
2025-10-05 12:03:57.938119: train_loss -0.8221
2025-10-05 12:03:57.938278: val_loss -0.294
2025-10-05 12:03:57.938428: Pseudo dice [np.float32(0.6364)]
2025-10-05 12:03:57.938641: Epoch time: 46.0 s
2025-10-05 12:03:59.368880: 
2025-10-05 12:03:59.369208: Epoch 95
2025-10-05 12:03:59.369389: Current learning rate: 0.00405
2025-10-05 12:04:45.349217: Validation loss did not improve from -0.41522. Patience: 76/50
2025-10-05 12:04:45.349657: train_loss -0.8312
2025-10-05 12:04:45.349886: val_loss -0.2592
2025-10-05 12:04:45.350033: Pseudo dice [np.float32(0.6385)]
2025-10-05 12:04:45.350297: Epoch time: 45.98 s
2025-10-05 12:04:45.965833: 
2025-10-05 12:04:45.966200: Epoch 96
2025-10-05 12:04:45.966471: Current learning rate: 0.00399
2025-10-05 12:05:31.944901: Validation loss did not improve from -0.41522. Patience: 77/50
2025-10-05 12:05:31.945514: train_loss -0.8345
2025-10-05 12:05:31.945778: val_loss -0.3102
2025-10-05 12:05:31.946031: Pseudo dice [np.float32(0.6509)]
2025-10-05 12:05:31.946321: Epoch time: 45.98 s
2025-10-05 12:05:32.567300: 
2025-10-05 12:05:32.567643: Epoch 97
2025-10-05 12:05:32.567875: Current learning rate: 0.00392
2025-10-05 12:06:18.666836: Validation loss did not improve from -0.41522. Patience: 78/50
2025-10-05 12:06:18.667336: train_loss -0.8351
2025-10-05 12:06:18.667524: val_loss -0.3337
2025-10-05 12:06:18.667732: Pseudo dice [np.float32(0.6762)]
2025-10-05 12:06:18.667904: Epoch time: 46.1 s
2025-10-05 12:06:19.295856: 
2025-10-05 12:06:19.296175: Epoch 98
2025-10-05 12:06:19.296414: Current learning rate: 0.00385
2025-10-05 12:07:05.360549: Validation loss did not improve from -0.41522. Patience: 79/50
2025-10-05 12:07:05.361353: train_loss -0.8316
2025-10-05 12:07:05.361636: val_loss -0.3615
2025-10-05 12:07:05.361911: Pseudo dice [np.float32(0.6782)]
2025-10-05 12:07:05.362189: Epoch time: 46.07 s
2025-10-05 12:07:05.994601: 
2025-10-05 12:07:05.995037: Epoch 99
2025-10-05 12:07:05.995268: Current learning rate: 0.00379
2025-10-05 12:07:52.108502: Validation loss did not improve from -0.41522. Patience: 80/50
2025-10-05 12:07:52.109046: train_loss -0.8352
2025-10-05 12:07:52.109300: val_loss -0.3271
2025-10-05 12:07:52.109456: Pseudo dice [np.float32(0.6731)]
2025-10-05 12:07:52.109608: Epoch time: 46.12 s
2025-10-05 12:07:53.162771: 
2025-10-05 12:07:53.163108: Epoch 100
2025-10-05 12:07:53.163397: Current learning rate: 0.00372
2025-10-05 12:08:39.204319: Validation loss did not improve from -0.41522. Patience: 81/50
2025-10-05 12:08:39.204873: train_loss -0.841
2025-10-05 12:08:39.205055: val_loss -0.3618
2025-10-05 12:08:39.205185: Pseudo dice [np.float32(0.6954)]
2025-10-05 12:08:39.205341: Epoch time: 46.04 s
2025-10-05 12:08:39.205465: Yayy! New best EMA pseudo Dice: 0.6629999876022339
2025-10-05 12:08:40.262885: 
2025-10-05 12:08:40.263222: Epoch 101
2025-10-05 12:08:40.263433: Current learning rate: 0.00365
2025-10-05 12:09:26.292376: Validation loss did not improve from -0.41522. Patience: 82/50
2025-10-05 12:09:26.292943: train_loss -0.843
2025-10-05 12:09:26.293293: val_loss -0.2424
2025-10-05 12:09:26.293559: Pseudo dice [np.float32(0.6311)]
2025-10-05 12:09:26.293870: Epoch time: 46.03 s
2025-10-05 12:09:26.932509: 
2025-10-05 12:09:26.932888: Epoch 102
2025-10-05 12:09:26.933095: Current learning rate: 0.00359
2025-10-05 12:10:12.987112: Validation loss did not improve from -0.41522. Patience: 83/50
2025-10-05 12:10:12.987838: train_loss -0.8407
2025-10-05 12:10:12.988170: val_loss -0.2832
2025-10-05 12:10:12.988415: Pseudo dice [np.float32(0.6481)]
2025-10-05 12:10:12.988590: Epoch time: 46.06 s
2025-10-05 12:10:13.622701: 
2025-10-05 12:10:13.623001: Epoch 103
2025-10-05 12:10:13.623202: Current learning rate: 0.00352
2025-10-05 12:10:59.591602: Validation loss did not improve from -0.41522. Patience: 84/50
2025-10-05 12:10:59.592064: train_loss -0.8397
2025-10-05 12:10:59.592580: val_loss -0.2794
2025-10-05 12:10:59.592755: Pseudo dice [np.float32(0.6468)]
2025-10-05 12:10:59.592901: Epoch time: 45.97 s
2025-10-05 12:11:00.213042: 
2025-10-05 12:11:00.213297: Epoch 104
2025-10-05 12:11:00.213501: Current learning rate: 0.00345
2025-10-05 12:11:46.365163: Validation loss did not improve from -0.41522. Patience: 85/50
2025-10-05 12:11:46.365859: train_loss -0.8452
2025-10-05 12:11:46.366099: val_loss -0.2563
2025-10-05 12:11:46.366392: Pseudo dice [np.float32(0.6324)]
2025-10-05 12:11:46.366638: Epoch time: 46.15 s
2025-10-05 12:11:47.583249: 
2025-10-05 12:11:47.583632: Epoch 105
2025-10-05 12:11:47.583903: Current learning rate: 0.00338
2025-10-05 12:12:33.789311: Validation loss did not improve from -0.41522. Patience: 86/50
2025-10-05 12:12:33.789809: train_loss -0.8425
2025-10-05 12:12:33.790014: val_loss -0.2604
2025-10-05 12:12:33.790155: Pseudo dice [np.float32(0.6299)]
2025-10-05 12:12:33.790304: Epoch time: 46.21 s
2025-10-05 12:12:34.420351: 
2025-10-05 12:12:34.420640: Epoch 106
2025-10-05 12:12:34.420851: Current learning rate: 0.00332
2025-10-05 12:13:20.555418: Validation loss did not improve from -0.41522. Patience: 87/50
2025-10-05 12:13:20.556172: train_loss -0.8475
2025-10-05 12:13:20.556417: val_loss -0.2629
2025-10-05 12:13:20.556636: Pseudo dice [np.float32(0.6223)]
2025-10-05 12:13:20.556856: Epoch time: 46.14 s
2025-10-05 12:13:21.452646: 
2025-10-05 12:13:21.453003: Epoch 107
2025-10-05 12:13:21.453248: Current learning rate: 0.00325
2025-10-05 12:14:07.401670: Validation loss did not improve from -0.41522. Patience: 88/50
2025-10-05 12:14:07.402235: train_loss -0.8429
2025-10-05 12:14:07.402578: val_loss -0.3276
2025-10-05 12:14:07.402828: Pseudo dice [np.float32(0.6692)]
2025-10-05 12:14:07.403117: Epoch time: 45.95 s
2025-10-05 12:14:08.047526: 
2025-10-05 12:14:08.047914: Epoch 108
2025-10-05 12:14:08.048216: Current learning rate: 0.00318
2025-10-05 12:14:54.016430: Validation loss did not improve from -0.41522. Patience: 89/50
2025-10-05 12:14:54.017156: train_loss -0.8491
2025-10-05 12:14:54.017571: val_loss -0.3267
2025-10-05 12:14:54.017967: Pseudo dice [np.float32(0.6737)]
2025-10-05 12:14:54.018298: Epoch time: 45.97 s
2025-10-05 12:14:54.650225: 
2025-10-05 12:14:54.650598: Epoch 109
2025-10-05 12:14:54.650789: Current learning rate: 0.00311
2025-10-05 12:15:40.629535: Validation loss did not improve from -0.41522. Patience: 90/50
2025-10-05 12:15:40.629983: train_loss -0.8473
2025-10-05 12:15:40.630145: val_loss -0.2575
2025-10-05 12:15:40.630279: Pseudo dice [np.float32(0.633)]
2025-10-05 12:15:40.630423: Epoch time: 45.98 s
2025-10-05 12:15:41.680828: 
2025-10-05 12:15:41.681104: Epoch 110
2025-10-05 12:15:41.681293: Current learning rate: 0.00304
2025-10-05 12:16:27.718527: Validation loss did not improve from -0.41522. Patience: 91/50
2025-10-05 12:16:27.718971: train_loss -0.8491
2025-10-05 12:16:27.719124: val_loss -0.2981
2025-10-05 12:16:27.719284: Pseudo dice [np.float32(0.6589)]
2025-10-05 12:16:27.719630: Epoch time: 46.04 s
2025-10-05 12:16:28.344358: 
2025-10-05 12:16:28.344653: Epoch 111
2025-10-05 12:16:28.344834: Current learning rate: 0.00297
2025-10-05 12:17:14.422525: Validation loss did not improve from -0.41522. Patience: 92/50
2025-10-05 12:17:14.422937: train_loss -0.8495
2025-10-05 12:17:14.423131: val_loss -0.2133
2025-10-05 12:17:14.423308: Pseudo dice [np.float32(0.6236)]
2025-10-05 12:17:14.423543: Epoch time: 46.08 s
2025-10-05 12:17:15.049193: 
2025-10-05 12:17:15.049576: Epoch 112
2025-10-05 12:17:15.049769: Current learning rate: 0.00291
2025-10-05 12:18:01.092218: Validation loss did not improve from -0.41522. Patience: 93/50
2025-10-05 12:18:01.092848: train_loss -0.8525
2025-10-05 12:18:01.093048: val_loss -0.3019
2025-10-05 12:18:01.093244: Pseudo dice [np.float32(0.6714)]
2025-10-05 12:18:01.093419: Epoch time: 46.04 s
2025-10-05 12:18:01.724720: 
2025-10-05 12:18:01.724962: Epoch 113
2025-10-05 12:18:01.725224: Current learning rate: 0.00284
2025-10-05 12:18:47.777056: Validation loss did not improve from -0.41522. Patience: 94/50
2025-10-05 12:18:47.777439: train_loss -0.8522
2025-10-05 12:18:47.777596: val_loss -0.3191
2025-10-05 12:18:47.777807: Pseudo dice [np.float32(0.6654)]
2025-10-05 12:18:47.777981: Epoch time: 46.05 s
2025-10-05 12:18:48.407665: 
2025-10-05 12:18:48.408029: Epoch 114
2025-10-05 12:18:48.408221: Current learning rate: 0.00277
2025-10-05 12:19:34.477081: Validation loss did not improve from -0.41522. Patience: 95/50
2025-10-05 12:19:34.477613: train_loss -0.853
2025-10-05 12:19:34.477868: val_loss -0.2232
2025-10-05 12:19:34.478036: Pseudo dice [np.float32(0.612)]
2025-10-05 12:19:34.478191: Epoch time: 46.07 s
2025-10-05 12:19:35.530351: 
2025-10-05 12:19:35.530684: Epoch 115
2025-10-05 12:19:35.530890: Current learning rate: 0.0027
2025-10-05 12:20:21.570855: Validation loss did not improve from -0.41522. Patience: 96/50
2025-10-05 12:20:21.571326: train_loss -0.8518
2025-10-05 12:20:21.571566: val_loss -0.2816
2025-10-05 12:20:21.571842: Pseudo dice [np.float32(0.6481)]
2025-10-05 12:20:21.572113: Epoch time: 46.04 s
2025-10-05 12:20:22.207296: 
2025-10-05 12:20:22.207625: Epoch 116
2025-10-05 12:20:22.207901: Current learning rate: 0.00263
2025-10-05 12:21:08.237047: Validation loss did not improve from -0.41522. Patience: 97/50
2025-10-05 12:21:08.237768: train_loss -0.8534
2025-10-05 12:21:08.238047: val_loss -0.2472
2025-10-05 12:21:08.238264: Pseudo dice [np.float32(0.6548)]
2025-10-05 12:21:08.238493: Epoch time: 46.03 s
2025-10-05 12:21:08.869168: 
2025-10-05 12:21:08.869424: Epoch 117
2025-10-05 12:21:08.869644: Current learning rate: 0.00256
2025-10-05 12:21:54.886584: Validation loss did not improve from -0.41522. Patience: 98/50
2025-10-05 12:21:54.887246: train_loss -0.8578
2025-10-05 12:21:54.887647: val_loss -0.2456
2025-10-05 12:21:54.887957: Pseudo dice [np.float32(0.6343)]
2025-10-05 12:21:54.888255: Epoch time: 46.02 s
2025-10-05 12:21:55.776296: 
2025-10-05 12:21:55.776739: Epoch 118
2025-10-05 12:21:55.777056: Current learning rate: 0.00249
2025-10-05 12:22:41.728895: Validation loss did not improve from -0.41522. Patience: 99/50
2025-10-05 12:22:41.729378: train_loss -0.8581
2025-10-05 12:22:41.729602: val_loss -0.3363
2025-10-05 12:22:41.729897: Pseudo dice [np.float32(0.6761)]
2025-10-05 12:22:41.730144: Epoch time: 45.95 s
2025-10-05 12:22:42.370578: 
2025-10-05 12:22:42.370975: Epoch 119
2025-10-05 12:22:42.371203: Current learning rate: 0.00242
2025-10-05 12:23:28.379324: Validation loss did not improve from -0.41522. Patience: 100/50
2025-10-05 12:23:28.379771: train_loss -0.8606
2025-10-05 12:23:28.379972: val_loss -0.2899
2025-10-05 12:23:28.380121: Pseudo dice [np.float32(0.6662)]
2025-10-05 12:23:28.380260: Epoch time: 46.01 s
2025-10-05 12:23:29.439887: 
2025-10-05 12:23:29.440151: Epoch 120
2025-10-05 12:23:29.440326: Current learning rate: 0.00235
2025-10-05 12:24:15.409647: Validation loss did not improve from -0.41522. Patience: 101/50
2025-10-05 12:24:15.410213: train_loss -0.8554
2025-10-05 12:24:15.410378: val_loss -0.285
2025-10-05 12:24:15.410502: Pseudo dice [np.float32(0.6626)]
2025-10-05 12:24:15.410676: Epoch time: 45.97 s
2025-10-05 12:24:16.053710: 
2025-10-05 12:24:16.054065: Epoch 121
2025-10-05 12:24:16.054239: Current learning rate: 0.00228
2025-10-05 12:25:02.052648: Validation loss did not improve from -0.41522. Patience: 102/50
2025-10-05 12:25:02.053145: train_loss -0.8591
2025-10-05 12:25:02.053334: val_loss -0.278
2025-10-05 12:25:02.053472: Pseudo dice [np.float32(0.6506)]
2025-10-05 12:25:02.053615: Epoch time: 46.0 s
2025-10-05 12:25:02.692186: 
2025-10-05 12:25:02.692522: Epoch 122
2025-10-05 12:25:02.692709: Current learning rate: 0.00221
2025-10-05 12:25:48.762345: Validation loss did not improve from -0.41522. Patience: 103/50
2025-10-05 12:25:48.762858: train_loss -0.8574
2025-10-05 12:25:48.763027: val_loss -0.2838
2025-10-05 12:25:48.763174: Pseudo dice [np.float32(0.6487)]
2025-10-05 12:25:48.763333: Epoch time: 46.07 s
2025-10-05 12:25:49.400345: 
2025-10-05 12:25:49.400644: Epoch 123
2025-10-05 12:25:49.400813: Current learning rate: 0.00214
2025-10-05 12:26:35.469801: Validation loss did not improve from -0.41522. Patience: 104/50
2025-10-05 12:26:35.470172: train_loss -0.8634
2025-10-05 12:26:35.470348: val_loss -0.2775
2025-10-05 12:26:35.470483: Pseudo dice [np.float32(0.6471)]
2025-10-05 12:26:35.470623: Epoch time: 46.07 s
2025-10-05 12:26:36.108589: 
2025-10-05 12:26:36.108911: Epoch 124
2025-10-05 12:26:36.109107: Current learning rate: 0.00207
2025-10-05 12:27:22.177168: Validation loss did not improve from -0.41522. Patience: 105/50
2025-10-05 12:27:22.177733: train_loss -0.8631
2025-10-05 12:27:22.177932: val_loss -0.28
2025-10-05 12:27:22.178122: Pseudo dice [np.float32(0.6487)]
2025-10-05 12:27:22.178306: Epoch time: 46.07 s
2025-10-05 12:27:23.250619: 
2025-10-05 12:27:23.250949: Epoch 125
2025-10-05 12:27:23.251183: Current learning rate: 0.00199
2025-10-05 12:28:09.325717: Validation loss did not improve from -0.41522. Patience: 106/50
2025-10-05 12:28:09.326214: train_loss -0.8635
2025-10-05 12:28:09.326412: val_loss -0.319
2025-10-05 12:28:09.326575: Pseudo dice [np.float32(0.6649)]
2025-10-05 12:28:09.326772: Epoch time: 46.08 s
2025-10-05 12:28:09.962026: 
2025-10-05 12:28:09.962348: Epoch 126
2025-10-05 12:28:09.962545: Current learning rate: 0.00192
2025-10-05 12:28:56.040604: Validation loss did not improve from -0.41522. Patience: 107/50
2025-10-05 12:28:56.041375: train_loss -0.8594
2025-10-05 12:28:56.041669: val_loss -0.2936
2025-10-05 12:28:56.041964: Pseudo dice [np.float32(0.6531)]
2025-10-05 12:28:56.042271: Epoch time: 46.08 s
2025-10-05 12:28:56.687271: 
2025-10-05 12:28:56.687598: Epoch 127
2025-10-05 12:28:56.687815: Current learning rate: 0.00185
2025-10-05 12:29:42.727872: Validation loss did not improve from -0.41522. Patience: 108/50
2025-10-05 12:29:42.728312: train_loss -0.8639
2025-10-05 12:29:42.728557: val_loss -0.2615
2025-10-05 12:29:42.728878: Pseudo dice [np.float32(0.653)]
2025-10-05 12:29:42.729032: Epoch time: 46.04 s
2025-10-05 12:29:43.364620: 
2025-10-05 12:29:43.364914: Epoch 128
2025-10-05 12:29:43.365108: Current learning rate: 0.00178
2025-10-05 12:30:29.440481: Validation loss did not improve from -0.41522. Patience: 109/50
2025-10-05 12:30:29.441394: train_loss -0.8666
2025-10-05 12:30:29.441555: val_loss -0.2577
2025-10-05 12:30:29.441713: Pseudo dice [np.float32(0.6436)]
2025-10-05 12:30:29.441864: Epoch time: 46.08 s
2025-10-05 12:30:30.068891: 
2025-10-05 12:30:30.069191: Epoch 129
2025-10-05 12:30:30.069451: Current learning rate: 0.0017
2025-10-05 12:31:16.183557: Validation loss did not improve from -0.41522. Patience: 110/50
2025-10-05 12:31:16.184023: train_loss -0.8661
2025-10-05 12:31:16.184207: val_loss -0.2908
2025-10-05 12:31:16.184360: Pseudo dice [np.float32(0.6571)]
2025-10-05 12:31:16.184531: Epoch time: 46.12 s
2025-10-05 12:31:17.524402: 
2025-10-05 12:31:17.524770: Epoch 130
2025-10-05 12:31:17.524959: Current learning rate: 0.00163
2025-10-05 12:32:03.508839: Validation loss did not improve from -0.41522. Patience: 111/50
2025-10-05 12:32:03.509594: train_loss -0.8687
2025-10-05 12:32:03.509814: val_loss -0.2938
2025-10-05 12:32:03.509945: Pseudo dice [np.float32(0.6779)]
2025-10-05 12:32:03.510084: Epoch time: 45.99 s
2025-10-05 12:32:04.141408: 
2025-10-05 12:32:04.141720: Epoch 131
2025-10-05 12:32:04.141906: Current learning rate: 0.00156
2025-10-05 12:32:50.185950: Validation loss did not improve from -0.41522. Patience: 112/50
2025-10-05 12:32:50.186389: train_loss -0.8695
2025-10-05 12:32:50.186543: val_loss -0.2955
2025-10-05 12:32:50.186693: Pseudo dice [np.float32(0.6669)]
2025-10-05 12:32:50.186924: Epoch time: 46.05 s
2025-10-05 12:32:50.817300: 
2025-10-05 12:32:50.817656: Epoch 132
2025-10-05 12:32:50.817850: Current learning rate: 0.00148
2025-10-05 12:33:36.865736: Validation loss did not improve from -0.41522. Patience: 113/50
2025-10-05 12:33:36.866286: train_loss -0.8686
2025-10-05 12:33:36.866477: val_loss -0.2524
2025-10-05 12:33:36.866628: Pseudo dice [np.float32(0.64)]
2025-10-05 12:33:36.866848: Epoch time: 46.05 s
2025-10-05 12:33:37.495130: 
2025-10-05 12:33:37.495486: Epoch 133
2025-10-05 12:33:37.495787: Current learning rate: 0.00141
2025-10-05 12:34:23.491402: Validation loss did not improve from -0.41522. Patience: 114/50
2025-10-05 12:34:23.491847: train_loss -0.8685
2025-10-05 12:34:23.492034: val_loss -0.2856
2025-10-05 12:34:23.492213: Pseudo dice [np.float32(0.6409)]
2025-10-05 12:34:23.492377: Epoch time: 46.0 s
2025-10-05 12:34:24.125765: 
2025-10-05 12:34:24.126056: Epoch 134
2025-10-05 12:34:24.126260: Current learning rate: 0.00133
2025-10-05 12:35:10.150381: Validation loss did not improve from -0.41522. Patience: 115/50
2025-10-05 12:35:10.151041: train_loss -0.8713
2025-10-05 12:35:10.151289: val_loss -0.2455
2025-10-05 12:35:10.151488: Pseudo dice [np.float32(0.6376)]
2025-10-05 12:35:10.151714: Epoch time: 46.03 s
2025-10-05 12:35:11.369527: 
2025-10-05 12:35:11.369799: Epoch 135
2025-10-05 12:35:11.370049: Current learning rate: 0.00126
2025-10-05 12:35:57.345457: Validation loss did not improve from -0.41522. Patience: 116/50
2025-10-05 12:35:57.345886: train_loss -0.8685
2025-10-05 12:35:57.346052: val_loss -0.2501
2025-10-05 12:35:57.346217: Pseudo dice [np.float32(0.6396)]
2025-10-05 12:35:57.346370: Epoch time: 45.98 s
2025-10-05 12:35:57.985576: 
2025-10-05 12:35:57.985854: Epoch 136
2025-10-05 12:35:57.986021: Current learning rate: 0.00118
2025-10-05 12:36:43.963863: Validation loss did not improve from -0.41522. Patience: 117/50
2025-10-05 12:36:43.964418: train_loss -0.8692
2025-10-05 12:36:43.964568: val_loss -0.2426
2025-10-05 12:36:43.964756: Pseudo dice [np.float32(0.6306)]
2025-10-05 12:36:43.964903: Epoch time: 45.98 s
2025-10-05 12:36:44.600136: 
2025-10-05 12:36:44.600445: Epoch 137
2025-10-05 12:36:44.600609: Current learning rate: 0.00111
2025-10-05 12:37:30.606477: Validation loss did not improve from -0.41522. Patience: 118/50
2025-10-05 12:37:30.606963: train_loss -0.8696
2025-10-05 12:37:30.607151: val_loss -0.3155
2025-10-05 12:37:30.607331: Pseudo dice [np.float32(0.6638)]
2025-10-05 12:37:30.607543: Epoch time: 46.01 s
2025-10-05 12:37:31.238611: 
2025-10-05 12:37:31.238954: Epoch 138
2025-10-05 12:37:31.239167: Current learning rate: 0.00103
2025-10-05 12:38:17.233358: Validation loss did not improve from -0.41522. Patience: 119/50
2025-10-05 12:38:17.234053: train_loss -0.8727
2025-10-05 12:38:17.234234: val_loss -0.2795
2025-10-05 12:38:17.234416: Pseudo dice [np.float32(0.6447)]
2025-10-05 12:38:17.234662: Epoch time: 46.0 s
2025-10-05 12:38:17.864411: 
2025-10-05 12:38:17.864749: Epoch 139
2025-10-05 12:38:17.864971: Current learning rate: 0.00095
2025-10-05 12:39:03.840451: Validation loss did not improve from -0.41522. Patience: 120/50
2025-10-05 12:39:03.840967: train_loss -0.8743
2025-10-05 12:39:03.841193: val_loss -0.2
2025-10-05 12:39:03.841408: Pseudo dice [np.float32(0.627)]
2025-10-05 12:39:03.841779: Epoch time: 45.98 s
2025-10-05 12:39:04.949904: 
2025-10-05 12:39:04.950175: Epoch 140
2025-10-05 12:39:04.950366: Current learning rate: 0.00087
2025-10-05 12:39:50.898725: Validation loss did not improve from -0.41522. Patience: 121/50
2025-10-05 12:39:50.899442: train_loss -0.8737
2025-10-05 12:39:50.899765: val_loss -0.3084
2025-10-05 12:39:50.900114: Pseudo dice [np.float32(0.673)]
2025-10-05 12:39:50.900427: Epoch time: 45.95 s
2025-10-05 12:39:51.543166: 
2025-10-05 12:39:51.543565: Epoch 141
2025-10-05 12:39:51.543825: Current learning rate: 0.00079
2025-10-05 12:40:37.739057: Validation loss did not improve from -0.41522. Patience: 122/50
2025-10-05 12:40:37.739432: train_loss -0.8722
2025-10-05 12:40:37.739590: val_loss -0.2732
2025-10-05 12:40:37.739778: Pseudo dice [np.float32(0.6618)]
2025-10-05 12:40:37.739937: Epoch time: 46.2 s
2025-10-05 12:40:38.378102: 
2025-10-05 12:40:38.378461: Epoch 142
2025-10-05 12:40:38.378646: Current learning rate: 0.00071
2025-10-05 12:41:24.378793: Validation loss did not improve from -0.41522. Patience: 123/50
2025-10-05 12:41:24.379371: train_loss -0.8737
2025-10-05 12:41:24.379564: val_loss -0.2662
2025-10-05 12:41:24.379694: Pseudo dice [np.float32(0.647)]
2025-10-05 12:41:24.379839: Epoch time: 46.0 s
2025-10-05 12:41:25.017072: 
2025-10-05 12:41:25.017419: Epoch 143
2025-10-05 12:41:25.017602: Current learning rate: 0.00063
2025-10-05 12:42:11.004060: Validation loss did not improve from -0.41522. Patience: 124/50
2025-10-05 12:42:11.004537: train_loss -0.8767
2025-10-05 12:42:11.004717: val_loss -0.2335
2025-10-05 12:42:11.004910: Pseudo dice [np.float32(0.6498)]
2025-10-05 12:42:11.005110: Epoch time: 45.99 s
2025-10-05 12:42:11.647604: 
2025-10-05 12:42:11.647995: Epoch 144
2025-10-05 12:42:11.648203: Current learning rate: 0.00055
2025-10-05 12:42:57.671286: Validation loss did not improve from -0.41522. Patience: 125/50
2025-10-05 12:42:57.671849: train_loss -0.878
2025-10-05 12:42:57.672049: val_loss -0.2493
2025-10-05 12:42:57.672212: Pseudo dice [np.float32(0.6446)]
2025-10-05 12:42:57.672418: Epoch time: 46.02 s
2025-10-05 12:42:58.754311: 
2025-10-05 12:42:58.754612: Epoch 145
2025-10-05 12:42:58.754869: Current learning rate: 0.00047
2025-10-05 12:43:44.816006: Validation loss did not improve from -0.41522. Patience: 126/50
2025-10-05 12:43:44.816618: train_loss -0.8767
2025-10-05 12:43:44.816880: val_loss -0.2419
2025-10-05 12:43:44.817159: Pseudo dice [np.float32(0.6357)]
2025-10-05 12:43:44.817418: Epoch time: 46.06 s
2025-10-05 12:43:45.455117: 
2025-10-05 12:43:45.455467: Epoch 146
2025-10-05 12:43:45.455763: Current learning rate: 0.00038
2025-10-05 12:44:31.536054: Validation loss did not improve from -0.41522. Patience: 127/50
2025-10-05 12:44:31.536416: train_loss -0.8768
2025-10-05 12:44:31.536654: val_loss -0.2764
2025-10-05 12:44:31.536851: Pseudo dice [np.float32(0.6422)]
2025-10-05 12:44:31.537016: Epoch time: 46.08 s
2025-10-05 12:44:32.171219: 
2025-10-05 12:44:32.171603: Epoch 147
2025-10-05 12:44:32.172031: Current learning rate: 0.0003
2025-10-05 12:45:18.213717: Validation loss did not improve from -0.41522. Patience: 128/50
2025-10-05 12:45:18.214206: train_loss -0.8767
2025-10-05 12:45:18.214403: val_loss -0.2681
2025-10-05 12:45:18.214578: Pseudo dice [np.float32(0.6536)]
2025-10-05 12:45:18.214795: Epoch time: 46.04 s
2025-10-05 12:45:18.851450: 
2025-10-05 12:45:18.851806: Epoch 148
2025-10-05 12:45:18.851980: Current learning rate: 0.00021
2025-10-05 12:46:04.901746: Validation loss did not improve from -0.41522. Patience: 129/50
2025-10-05 12:46:04.902324: train_loss -0.8771
2025-10-05 12:46:04.902480: val_loss -0.281
2025-10-05 12:46:04.902601: Pseudo dice [np.float32(0.6483)]
2025-10-05 12:46:04.902735: Epoch time: 46.05 s
2025-10-05 12:46:05.541433: 
2025-10-05 12:46:05.541798: Epoch 149
2025-10-05 12:46:05.541996: Current learning rate: 0.00011
2025-10-05 12:46:51.568864: Validation loss did not improve from -0.41522. Patience: 130/50
2025-10-05 12:46:51.569532: train_loss -0.8785
2025-10-05 12:46:51.569788: val_loss -0.2844
2025-10-05 12:46:51.570025: Pseudo dice [np.float32(0.6644)]
2025-10-05 12:46:51.570279: Epoch time: 46.03 s
2025-10-05 12:46:52.661394: Training done.
2025-10-05 12:46:52.705472: Using splits from existing split file: /nfs/erelab001/shared/Computational_Group/Naravich/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset310_nnInteractive_Calcium_OCT_CrossValidation/splits_final_80.json
2025-10-05 12:46:52.706282: The split file contains 5 splits.
2025-10-05 12:46:52.706684: Desired fold for training: 0
2025-10-05 12:46:52.707412: This split has 6 training and 4 validation cases.
2025-10-05 12:46:52.707846: predicting 101-045
2025-10-05 12:46:52.711612: 101-045, shape torch.Size([1, 375, 498, 498]), rank 0
2025-10-05 12:47:30.259233: predicting 701-013
2025-10-05 12:47:30.274672: 701-013, shape torch.Size([1, 375, 498, 498]), rank 0
2025-10-05 12:48:04.360748: predicting 704-003
2025-10-05 12:48:04.375035: 704-003, shape torch.Size([1, 375, 498, 498]), rank 0
2025-10-05 12:48:38.460887: predicting 706-005
2025-10-05 12:48:38.476985: 706-005, shape torch.Size([1, 375, 498, 498]), rank 0
2025-10-05 12:49:25.607574: Validation complete
2025-10-05 12:49:25.607816: Mean Validation Dice:  0.6537544901286249
Finished training fold 0 saving to /nfs/erelab001/shared/Computational_Group/Naravich/datasets/nnUNet_Datasets/nnUNet_results/Dataset310_nnInteractive_Calcium_OCT_CrossValidation/nnUNetTrainerScaleAnalysis80__nnUNetPlans__3d_32x160x128_b10/fold_0_No_Pretrained
