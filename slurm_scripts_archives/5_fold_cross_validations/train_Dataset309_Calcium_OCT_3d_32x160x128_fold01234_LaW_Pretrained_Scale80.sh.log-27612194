/home/gridsan/nchutisilp/.conda/envs/nnunet/bin/python
Starting training with CONFIG=3d_32x160x128_b10, DATASET_ID=309, TRAINER=nnUNetTrainerScaleAnalysis80

############################
INFO: You are using the old nnU-Net default plans. We have updated our recommendations. Please consider using those instead! Read more here: https://github.com/MIC-DKFZ/nnUNet/blob/master/documentation/resenc_presets.md
############################

Using device: cuda:0

#######################################################################
Please cite the following paper when using nnU-Net:
Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.
#######################################################################

2024-12-17 12:39:07.969509: Using torch.compile...

############################
INFO: You are using the old nnU-Net default plans. We have updated our recommendations. Please consider using those instead! Read more here: https://github.com/MIC-DKFZ/nnUNet/blob/master/documentation/resenc_presets.md
############################

Using device: cuda:0

#######################################################################
Please cite the following paper when using nnU-Net:
Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.
#######################################################################

2024-12-17 12:39:07.968872: Using torch.compile...
/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
################### Loading pretrained weights from file  /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_results/Dataset300_Lumen_and_Wall_OCT/nnUNetTrainer__nnUNetPreprocessPlans__3d_fullres/fold_all/checkpoint_best.pth ###################
Below is the list of overlapping blocks in pretrained model and nnUNet architecture:
encoder.stages.0.0.convs.0.conv.weight shape torch.Size([32, 1, 3, 3, 3])
encoder.stages.0.0.convs.0.conv.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.norm.weight shape torch.Size([32])
encoder.stages.0.0.convs.0.norm.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.0.weight shape torch.Size([32, 1, 3, 3, 3])
encoder.stages.0.0.convs.0.all_modules.0.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.1.weight shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.1.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
encoder.stages.0.0.convs.1.conv.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.norm.weight shape torch.Size([32])
encoder.stages.0.0.convs.1.norm.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
encoder.stages.0.0.convs.1.all_modules.0.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.1.weight shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.1.bias shape torch.Size([32])
encoder.stages.1.0.convs.0.conv.weight shape torch.Size([64, 32, 3, 3, 3])
encoder.stages.1.0.convs.0.conv.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.norm.weight shape torch.Size([64])
encoder.stages.1.0.convs.0.norm.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.0.weight shape torch.Size([64, 32, 3, 3, 3])
encoder.stages.1.0.convs.0.all_modules.0.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.1.weight shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.1.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
encoder.stages.1.0.convs.1.conv.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.norm.weight shape torch.Size([64])
encoder.stages.1.0.convs.1.norm.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
encoder.stages.1.0.convs.1.all_modules.0.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.1.weight shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.1.bias shape torch.Size([64])
encoder.stages.2.0.convs.0.conv.weight shape torch.Size([128, 64, 3, 3, 3])
encoder.stages.2.0.convs.0.conv.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.norm.weight shape torch.Size([128])
encoder.stages.2.0.convs.0.norm.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.0.weight shape torch.Size([128, 64, 3, 3, 3])
encoder.stages.2.0.convs.0.all_modules.0.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.1.weight shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.1.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
encoder.stages.2.0.convs.1.conv.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.norm.weight shape torch.Size([128])
encoder.stages.2.0.convs.1.norm.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
encoder.stages.2.0.convs.1.all_modules.0.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.1.weight shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.1.bias shape torch.Size([128])
encoder.stages.3.0.convs.0.conv.weight shape torch.Size([256, 128, 3, 3, 3])
encoder.stages.3.0.convs.0.conv.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.norm.weight shape torch.Size([256])
encoder.stages.3.0.convs.0.norm.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.0.weight shape torch.Size([256, 128, 3, 3, 3])
encoder.stages.3.0.convs.0.all_modules.0.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.1.weight shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.1.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
encoder.stages.3.0.convs.1.conv.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.norm.weight shape torch.Size([256])
encoder.stages.3.0.convs.1.norm.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
encoder.stages.3.0.convs.1.all_modules.0.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.1.weight shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.1.bias shape torch.Size([256])
encoder.stages.4.0.convs.0.conv.weight shape torch.Size([320, 256, 3, 3, 3])
encoder.stages.4.0.convs.0.conv.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.norm.weight shape torch.Size([320])
encoder.stages.4.0.convs.0.norm.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.0.weight shape torch.Size([320, 256, 3, 3, 3])
encoder.stages.4.0.convs.0.all_modules.0.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.1.weight shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.1.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.4.0.convs.1.conv.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.norm.weight shape torch.Size([320])
encoder.stages.4.0.convs.1.norm.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.4.0.convs.1.all_modules.0.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.1.weight shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.1.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.0.conv.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.norm.weight shape torch.Size([320])
encoder.stages.5.0.convs.0.norm.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.0.all_modules.0.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.1.weight shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.1.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.1.conv.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.norm.weight shape torch.Size([320])
encoder.stages.5.0.convs.1.norm.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.1.all_modules.0.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.1.weight shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.0.0.convs.0.conv.weight shape torch.Size([32, 1, 3, 3, 3])
decoder.encoder.stages.0.0.convs.0.conv.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.norm.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.norm.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.0.weight shape torch.Size([32, 1, 3, 3, 3])
decoder.encoder.stages.0.0.convs.0.all_modules.0.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.1.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.1.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.encoder.stages.0.0.convs.1.conv.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.norm.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.norm.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.encoder.stages.0.0.convs.1.all_modules.0.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.1.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.1.bias shape torch.Size([32])
decoder.encoder.stages.1.0.convs.0.conv.weight shape torch.Size([64, 32, 3, 3, 3])
decoder.encoder.stages.1.0.convs.0.conv.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.norm.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.norm.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.0.weight shape torch.Size([64, 32, 3, 3, 3])
decoder.encoder.stages.1.0.convs.0.all_modules.0.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.1.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.1.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.encoder.stages.1.0.convs.1.conv.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.norm.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.norm.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.encoder.stages.1.0.convs.1.all_modules.0.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.1.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.1.bias shape torch.Size([64])
decoder.encoder.stages.2.0.convs.0.conv.weight shape torch.Size([128, 64, 3, 3, 3])
decoder.encoder.stages.2.0.convs.0.conv.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.norm.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.norm.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.0.weight shape torch.Size([128, 64, 3, 3, 3])
decoder.encoder.stages.2.0.convs.0.all_modules.0.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.1.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.1.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.encoder.stages.2.0.convs.1.conv.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.norm.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.norm.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.encoder.stages.2.0.convs.1.all_modules.0.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.1.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.1.bias shape torch.Size([128])
decoder.encoder.stages.3.0.convs.0.conv.weight shape torch.Size([256, 128, 3, 3, 3])
decoder.encoder.stages.3.0.convs.0.conv.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.norm.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.norm.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.0.weight shape torch.Size([256, 128, 3, 3, 3])
decoder.encoder.stages.3.0.convs.0.all_modules.0.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.1.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.1.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.encoder.stages.3.0.convs.1.conv.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.norm.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.norm.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.encoder.stages.3.0.convs.1.all_modules.0.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.1.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.1.bias shape torch.Size([256])
decoder.encoder.stages.4.0.convs.0.conv.weight shape torch.Size([320, 256, 3, 3, 3])
decoder.encoder.stages.4.0.convs.0.conv.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.norm.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.norm.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.0.weight shape torch.Size([320, 256, 3, 3, 3])
decoder.encoder.stages.4.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.4.0.convs.1.conv.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.norm.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.norm.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.4.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.0.conv.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.norm.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.norm.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.1.conv.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.norm.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.norm.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.stages.0.convs.0.conv.weight shape torch.Size([320, 640, 3, 3, 3])
decoder.stages.0.convs.0.conv.bias shape torch.Size([320])
decoder.stages.0.convs.0.norm.weight shape torch.Size([320])
decoder.stages.0.convs.0.norm.bias shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.0.weight shape torch.Size([320, 640, 3, 3, 3])
decoder.stages.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.stages.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.stages.0.convs.1.conv.bias shape torch.Size([320])
decoder.stages.0.convs.1.norm.weight shape torch.Size([320])
decoder.stages.0.convs.1.norm.bias shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.stages.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.stages.1.convs.0.conv.weight shape torch.Size([256, 512, 3, 3, 3])
decoder.stages.1.convs.0.conv.bias shape torch.Size([256])
decoder.stages.1.convs.0.norm.weight shape torch.Size([256])
decoder.stages.1.convs.0.norm.bias shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.0.weight shape torch.Size([256, 512, 3, 3, 3])
decoder.stages.1.convs.0.all_modules.0.bias shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.1.weight shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.1.bias shape torch.Size([256])
decoder.stages.1.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.stages.1.convs.1.conv.bias shape torch.Size([256])
decoder.stages.1.convs.1.norm.weight shape torch.Size([256])
decoder.stages.1.convs.1.norm.bias shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.stages.1.convs.1.all_modules.0.bias shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.1.weight shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.1.bias shape torch.Size([256])
decoder.stages.2.convs.0.conv.weight shape torch.Size([128, 256, 3, 3, 3])
decoder.stages.2.convs.0.conv.bias shape torch.Size([128])
decoder.stages.2.convs.0.norm.weight shape torch.Size([128])
decoder.stages.2.convs.0.norm.bias shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.0.weight shape torch.Size([128, 256, 3, 3, 3])
decoder.stages.2.convs.0.all_modules.0.bias shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.1.weight shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.1.bias shape torch.Size([128])
decoder.stages.2.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.stages.2.convs.1.conv.bias shape torch.Size([128])
decoder.stages.2.convs.1.norm.weight shape torch.Size([128])
decoder.stages.2.convs.1.norm.bias shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.stages.2.convs.1.all_modules.0.bias shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.1.weight shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.1.bias shape torch.Size([128])
decoder.stages.3.convs.0.conv.weight shape torch.Size([64, 128, 3, 3, 3])
decoder.stages.3.convs.0.conv.bias shape torch.Size([64])
decoder.stages.3.convs.0.norm.weight shape torch.Size([64])
decoder.stages.3.convs.0.norm.bias shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.0.weight shape torch.Size([64, 128, 3, 3, 3])
decoder.stages.3.convs.0.all_modules.0.bias shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.1.weight shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.1.bias shape torch.Size([64])
decoder.stages.3.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.stages.3.convs.1.conv.bias shape torch.Size([64])
decoder.stages.3.convs.1.norm.weight shape torch.Size([64])
decoder.stages.3.convs.1.norm.bias shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.stages.3.convs.1.all_modules.0.bias shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.1.weight shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.1.bias shape torch.Size([64])
decoder.stages.4.convs.0.conv.weight shape torch.Size([32, 64, 3, 3, 3])
decoder.stages.4.convs.0.conv.bias shape torch.Size([32])
decoder.stages.4.convs.0.norm.weight shape torch.Size([32])
decoder.stages.4.convs.0.norm.bias shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.0.weight shape torch.Size([32, 64, 3, 3, 3])
decoder.stages.4.convs.0.all_modules.0.bias shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.1.weight shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.1.bias shape torch.Size([32])
decoder.stages.4.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.stages.4.convs.1.conv.bias shape torch.Size([32])
decoder.stages.4.convs.1.norm.weight shape torch.Size([32])
decoder.stages.4.convs.1.norm.bias shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.stages.4.convs.1.all_modules.0.bias shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.1.weight shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.1.bias shape torch.Size([32])
decoder.transpconvs.0.weight shape torch.Size([320, 320, 1, 2, 2])
decoder.transpconvs.0.bias shape torch.Size([320])
decoder.transpconvs.1.weight shape torch.Size([320, 256, 2, 2, 2])
decoder.transpconvs.1.bias shape torch.Size([256])
decoder.transpconvs.2.weight shape torch.Size([256, 128, 2, 2, 2])
decoder.transpconvs.2.bias shape torch.Size([128])
decoder.transpconvs.3.weight shape torch.Size([128, 64, 2, 2, 2])
decoder.transpconvs.3.bias shape torch.Size([64])
decoder.transpconvs.4.weight shape torch.Size([64, 32, 2, 2, 2])
decoder.transpconvs.4.bias shape torch.Size([32])
################### Done ###################
2024-12-17 12:39:24.389391: do_dummy_2d_data_aug: True
2024-12-17 12:39:24.419055: Using splits from existing split file: /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset309_Sohee_Calcium_OCT_CrossValidation/splits_final_80.json
2024-12-17 12:39:24.439542: The split file contains 5 splits.
2024-12-17 12:39:24.441421: Desired fold for training: 1
2024-12-17 12:39:24.442720: This split has 6 training and 4 validation cases.
################### Loading pretrained weights from file  /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_results/Dataset300_Lumen_and_Wall_OCT/nnUNetTrainer__nnUNetPreprocessPlans__3d_fullres/fold_all/checkpoint_best.pth ###################
Below is the list of overlapping blocks in pretrained model and nnUNet architecture:
encoder.stages.0.0.convs.0.conv.weight shape torch.Size([32, 1, 3, 3, 3])
encoder.stages.0.0.convs.0.conv.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.norm.weight shape torch.Size([32])
encoder.stages.0.0.convs.0.norm.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.0.weight shape torch.Size([32, 1, 3, 3, 3])
encoder.stages.0.0.convs.0.all_modules.0.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.1.weight shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.1.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
encoder.stages.0.0.convs.1.conv.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.norm.weight shape torch.Size([32])
encoder.stages.0.0.convs.1.norm.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
encoder.stages.0.0.convs.1.all_modules.0.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.1.weight shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.1.bias shape torch.Size([32])
encoder.stages.1.0.convs.0.conv.weight shape torch.Size([64, 32, 3, 3, 3])
encoder.stages.1.0.convs.0.conv.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.norm.weight shape torch.Size([64])
encoder.stages.1.0.convs.0.norm.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.0.weight shape torch.Size([64, 32, 3, 3, 3])
encoder.stages.1.0.convs.0.all_modules.0.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.1.weight shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.1.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
encoder.stages.1.0.convs.1.conv.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.norm.weight shape torch.Size([64])
encoder.stages.1.0.convs.1.norm.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
encoder.stages.1.0.convs.1.all_modules.0.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.1.weight shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.1.bias shape torch.Size([64])
encoder.stages.2.0.convs.0.conv.weight shape torch.Size([128, 64, 3, 3, 3])
encoder.stages.2.0.convs.0.conv.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.norm.weight shape torch.Size([128])
encoder.stages.2.0.convs.0.norm.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.0.weight shape torch.Size([128, 64, 3, 3, 3])
encoder.stages.2.0.convs.0.all_modules.0.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.1.weight shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.1.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
encoder.stages.2.0.convs.1.conv.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.norm.weight shape torch.Size([128])
encoder.stages.2.0.convs.1.norm.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
encoder.stages.2.0.convs.1.all_modules.0.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.1.weight shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.1.bias shape torch.Size([128])
encoder.stages.3.0.convs.0.conv.weight shape torch.Size([256, 128, 3, 3, 3])
encoder.stages.3.0.convs.0.conv.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.norm.weight shape torch.Size([256])
encoder.stages.3.0.convs.0.norm.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.0.weight shape torch.Size([256, 128, 3, 3, 3])
encoder.stages.3.0.convs.0.all_modules.0.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.1.weight shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.1.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
encoder.stages.3.0.convs.1.conv.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.norm.weight shape torch.Size([256])
encoder.stages.3.0.convs.1.norm.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
encoder.stages.3.0.convs.1.all_modules.0.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.1.weight shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.1.bias shape torch.Size([256])
encoder.stages.4.0.convs.0.conv.weight shape torch.Size([320, 256, 3, 3, 3])
encoder.stages.4.0.convs.0.conv.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.norm.weight shape torch.Size([320])
encoder.stages.4.0.convs.0.norm.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.0.weight shape torch.Size([320, 256, 3, 3, 3])
encoder.stages.4.0.convs.0.all_modules.0.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.1.weight shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.1.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.4.0.convs.1.conv.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.norm.weight shape torch.Size([320])
encoder.stages.4.0.convs.1.norm.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.4.0.convs.1.all_modules.0.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.1.weight shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.1.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.0.conv.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.norm.weight shape torch.Size([320])
encoder.stages.5.0.convs.0.norm.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.0.all_modules.0.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.1.weight shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.1.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.1.conv.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.norm.weight shape torch.Size([320])
encoder.stages.5.0.convs.1.norm.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.1.all_modules.0.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.1.weight shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.0.0.convs.0.conv.weight shape torch.Size([32, 1, 3, 3, 3])
decoder.encoder.stages.0.0.convs.0.conv.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.norm.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.norm.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.0.weight shape torch.Size([32, 1, 3, 3, 3])
decoder.encoder.stages.0.0.convs.0.all_modules.0.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.1.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.1.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.encoder.stages.0.0.convs.1.conv.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.norm.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.norm.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.encoder.stages.0.0.convs.1.all_modules.0.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.1.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.1.bias shape torch.Size([32])
decoder.encoder.stages.1.0.convs.0.conv.weight shape torch.Size([64, 32, 3, 3, 3])
decoder.encoder.stages.1.0.convs.0.conv.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.norm.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.norm.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.0.weight shape torch.Size([64, 32, 3, 3, 3])
decoder.encoder.stages.1.0.convs.0.all_modules.0.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.1.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.1.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.encoder.stages.1.0.convs.1.conv.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.norm.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.norm.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.encoder.stages.1.0.convs.1.all_modules.0.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.1.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.1.bias shape torch.Size([64])
decoder.encoder.stages.2.0.convs.0.conv.weight shape torch.Size([128, 64, 3, 3, 3])
decoder.encoder.stages.2.0.convs.0.conv.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.norm.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.norm.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.0.weight shape torch.Size([128, 64, 3, 3, 3])
decoder.encoder.stages.2.0.convs.0.all_modules.0.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.1.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.1.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.encoder.stages.2.0.convs.1.conv.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.norm.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.norm.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.encoder.stages.2.0.convs.1.all_modules.0.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.1.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.1.bias shape torch.Size([128])
decoder.encoder.stages.3.0.convs.0.conv.weight shape torch.Size([256, 128, 3, 3, 3])
decoder.encoder.stages.3.0.convs.0.conv.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.norm.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.norm.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.0.weight shape torch.Size([256, 128, 3, 3, 3])
decoder.encoder.stages.3.0.convs.0.all_modules.0.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.1.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.1.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.encoder.stages.3.0.convs.1.conv.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.norm.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.norm.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.encoder.stages.3.0.convs.1.all_modules.0.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.1.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.1.bias shape torch.Size([256])
decoder.encoder.stages.4.0.convs.0.conv.weight shape torch.Size([320, 256, 3, 3, 3])
decoder.encoder.stages.4.0.convs.0.conv.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.norm.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.norm.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.0.weight shape torch.Size([320, 256, 3, 3, 3])
decoder.encoder.stages.4.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.4.0.convs.1.conv.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.norm.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.norm.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.4.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.0.conv.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.norm.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.norm.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.1.conv.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.norm.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.norm.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.stages.0.convs.0.conv.weight shape torch.Size([320, 640, 3, 3, 3])
decoder.stages.0.convs.0.conv.bias shape torch.Size([320])
decoder.stages.0.convs.0.norm.weight shape torch.Size([320])
decoder.stages.0.convs.0.norm.bias shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.0.weight shape torch.Size([320, 640, 3, 3, 3])
decoder.stages.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.stages.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.stages.0.convs.1.conv.bias shape torch.Size([320])
decoder.stages.0.convs.1.norm.weight shape torch.Size([320])
decoder.stages.0.convs.1.norm.bias shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.stages.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.stages.1.convs.0.conv.weight shape torch.Size([256, 512, 3, 3, 3])
decoder.stages.1.convs.0.conv.bias shape torch.Size([256])
decoder.stages.1.convs.0.norm.weight shape torch.Size([256])
decoder.stages.1.convs.0.norm.bias shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.0.weight shape torch.Size([256, 512, 3, 3, 3])
decoder.stages.1.convs.0.all_modules.0.bias shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.1.weight shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.1.bias shape torch.Size([256])
decoder.stages.1.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.stages.1.convs.1.conv.bias shape torch.Size([256])
decoder.stages.1.convs.1.norm.weight shape torch.Size([256])
decoder.stages.1.convs.1.norm.bias shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.stages.1.convs.1.all_modules.0.bias shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.1.weight shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.1.bias shape torch.Size([256])
decoder.stages.2.convs.0.conv.weight shape torch.Size([128, 256, 3, 3, 3])
decoder.stages.2.convs.0.conv.bias shape torch.Size([128])
decoder.stages.2.convs.0.norm.weight shape torch.Size([128])
decoder.stages.2.convs.0.norm.bias shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.0.weight shape torch.Size([128, 256, 3, 3, 3])
decoder.stages.2.convs.0.all_modules.0.bias shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.1.weight shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.1.bias shape torch.Size([128])
decoder.stages.2.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.stages.2.convs.1.conv.bias shape torch.Size([128])
decoder.stages.2.convs.1.norm.weight shape torch.Size([128])
decoder.stages.2.convs.1.norm.bias shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.stages.2.convs.1.all_modules.0.bias shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.1.weight shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.1.bias shape torch.Size([128])
decoder.stages.3.convs.0.conv.weight shape torch.Size([64, 128, 3, 3, 3])
decoder.stages.3.convs.0.conv.bias shape torch.Size([64])
decoder.stages.3.convs.0.norm.weight shape torch.Size([64])
decoder.stages.3.convs.0.norm.bias shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.0.weight shape torch.Size([64, 128, 3, 3, 3])
decoder.stages.3.convs.0.all_modules.0.bias shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.1.weight shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.1.bias shape torch.Size([64])
decoder.stages.3.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.stages.3.convs.1.conv.bias shape torch.Size([64])
decoder.stages.3.convs.1.norm.weight shape torch.Size([64])
decoder.stages.3.convs.1.norm.bias shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.stages.3.convs.1.all_modules.0.bias shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.1.weight shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.1.bias shape torch.Size([64])
decoder.stages.4.convs.0.conv.weight shape torch.Size([32, 64, 3, 3, 3])
decoder.stages.4.convs.0.conv.bias shape torch.Size([32])
decoder.stages.4.convs.0.norm.weight shape torch.Size([32])
decoder.stages.4.convs.0.norm.bias shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.0.weight shape torch.Size([32, 64, 3, 3, 3])
decoder.stages.4.convs.0.all_modules.0.bias shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.1.weight shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.1.bias shape torch.Size([32])
decoder.stages.4.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.stages.4.convs.1.conv.bias shape torch.Size([32])
decoder.stages.4.convs.1.norm.weight shape torch.Size([32])
decoder.stages.4.convs.1.norm.bias shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.stages.4.convs.1.all_modules.0.bias shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.1.weight shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.1.bias shape torch.Size([32])
decoder.transpconvs.0.weight shape torch.Size([320, 320, 1, 2, 2])
decoder.transpconvs.0.bias shape torch.Size([320])
decoder.transpconvs.1.weight shape torch.Size([320, 256, 2, 2, 2])
decoder.transpconvs.1.bias shape torch.Size([256])
decoder.transpconvs.2.weight shape torch.Size([256, 128, 2, 2, 2])
decoder.transpconvs.2.bias shape torch.Size([128])
decoder.transpconvs.3.weight shape torch.Size([128, 64, 2, 2, 2])
decoder.transpconvs.3.bias shape torch.Size([64])
decoder.transpconvs.4.weight shape torch.Size([64, 32, 2, 2, 2])
decoder.transpconvs.4.bias shape torch.Size([32])
################### Done ###################
2024-12-17 12:39:24.389395: do_dummy_2d_data_aug: True
2024-12-17 12:39:24.419095: Using splits from existing split file: /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset309_Sohee_Calcium_OCT_CrossValidation/splits_final_80.json
2024-12-17 12:39:24.438840: The split file contains 5 splits.
2024-12-17 12:39:24.440887: Desired fold for training: 0
2024-12-17 12:39:24.442316: This split has 6 training and 4 validation cases.
using pin_memory on device 0
using pin_memory on device 0
using pin_memory on device 0

This is the configuration used by this training:
Configuration name: 3d_32x160x128_b10
 {'data_identifier': 'nnUNetPlans_3d_fullres', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 10, 'patch_size': [32, 160, 128], 'median_image_size_in_voxels': [375.0, 498.0, 498.0], 'spacing': [1.0, 1.0, 1.0], 'normalization_schemes': ['ZScoreNormalization'], 'use_mask_for_norm': [False], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'architecture': {'network_class_name': 'dynamic_network_architectures.architectures.unet.PlainConvUNet', 'arch_kwargs': {'n_stages': 6, 'features_per_stage': [32, 64, 128, 256, 320, 320], 'conv_op': 'torch.nn.modules.conv.Conv3d', 'kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]], 'strides': [[1, 1, 1], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [1, 2, 2]], 'n_conv_per_stage': [2, 2, 2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2, 2, 2], 'conv_bias': True, 'norm_op': 'torch.nn.modules.instancenorm.InstanceNorm3d', 'norm_op_kwargs': {'eps': 1e-05, 'affine': True}, 'dropout_op': None, 'dropout_op_kwargs': None, 'nonlin': 'torch.nn.LeakyReLU', 'nonlin_kwargs': {'inplace': True}, 'deep_supervision': True}, '_kw_requires_import': ['conv_op', 'norm_op', 'dropout_op', 'nonlin']}, 'batch_dice': True, 'inherits_from': '3d_fullres'} 

These are the global plan.json settings:
 {'dataset_name': 'Dataset309_Sohee_Calcium_OCT_CrossValidation', 'plans_name': 'nnUNetPlans', 'original_median_spacing_after_transp': [1.0, 1.0, 1.0], 'original_median_shape_after_transp': [375, 498, 498], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 0.4977976381778717, 'mean': 0.13507525622844696, 'median': 0.09599608182907104, 'min': 0.0, 'percentile_00_5': 0.014754901640117168, 'percentile_99_5': 0.4977976381778717, 'std': 0.12025152146816254}}} 

2024-12-17 12:39:35.751288: unpacking dataset...
using pin_memory on device 0

This is the configuration used by this training:
Configuration name: 3d_32x160x128_b10
 {'data_identifier': 'nnUNetPlans_3d_fullres', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 10, 'patch_size': [32, 160, 128], 'median_image_size_in_voxels': [375.0, 498.0, 498.0], 'spacing': [1.0, 1.0, 1.0], 'normalization_schemes': ['ZScoreNormalization'], 'use_mask_for_norm': [False], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'architecture': {'network_class_name': 'dynamic_network_architectures.architectures.unet.PlainConvUNet', 'arch_kwargs': {'n_stages': 6, 'features_per_stage': [32, 64, 128, 256, 320, 320], 'conv_op': 'torch.nn.modules.conv.Conv3d', 'kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]], 'strides': [[1, 1, 1], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [1, 2, 2]], 'n_conv_per_stage': [2, 2, 2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2, 2, 2], 'conv_bias': True, 'norm_op': 'torch.nn.modules.instancenorm.InstanceNorm3d', 'norm_op_kwargs': {'eps': 1e-05, 'affine': True}, 'dropout_op': None, 'dropout_op_kwargs': None, 'nonlin': 'torch.nn.LeakyReLU', 'nonlin_kwargs': {'inplace': True}, 'deep_supervision': True}, '_kw_requires_import': ['conv_op', 'norm_op', 'dropout_op', 'nonlin']}, 'batch_dice': True, 'inherits_from': '3d_fullres'} 

These are the global plan.json settings:
 {'dataset_name': 'Dataset309_Sohee_Calcium_OCT_CrossValidation', 'plans_name': 'nnUNetPlans', 'original_median_spacing_after_transp': [1.0, 1.0, 1.0], 'original_median_shape_after_transp': [375, 498, 498], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 0.4977976381778717, 'mean': 0.13507525622844696, 'median': 0.09599608182907104, 'min': 0.0, 'percentile_00_5': 0.014754901640117168, 'percentile_99_5': 0.4977976381778717, 'std': 0.12025152146816254}}} 

2024-12-17 12:39:36.371956: unpacking dataset...
2024-12-17 12:39:40.492098: unpacking done...
2024-12-17 12:39:40.503992: Unable to plot network architecture: nnUNet_compile is enabled!
2024-12-17 12:39:40.543095: 
2024-12-17 12:39:40.544809: Epoch 0
2024-12-17 12:39:40.546494: Current learning rate: 0.01
2024-12-17 12:42:48.589578: Validation loss improved from 1000.00000 to -0.42703! Patience: 0/50
2024-12-17 12:42:48.590631: train_loss -0.3175
2024-12-17 12:42:48.591796: val_loss -0.427
2024-12-17 12:42:48.592815: Pseudo dice [0.6812]
2024-12-17 12:42:48.593523: Epoch time: 188.05 s
2024-12-17 12:42:48.594279: Yayy! New best EMA pseudo Dice: 0.6812
2024-12-17 12:42:50.206416: 
2024-12-17 12:42:50.207765: Epoch 1
2024-12-17 12:42:50.208675: Current learning rate: 0.00994
2024-12-17 12:44:25.951555: Validation loss improved from -0.42703 to -0.43567! Patience: 0/50
2024-12-17 12:44:25.952508: train_loss -0.4754
2024-12-17 12:44:25.953681: val_loss -0.4357
2024-12-17 12:44:25.954740: Pseudo dice [0.6987]
2024-12-17 12:44:25.955673: Epoch time: 95.75 s
2024-12-17 12:44:25.956754: Yayy! New best EMA pseudo Dice: 0.683
2024-12-17 12:44:27.858040: 
2024-12-17 12:44:27.859382: Epoch 2
2024-12-17 12:44:27.860420: Current learning rate: 0.00988
2024-12-17 12:46:06.079970: Validation loss improved from -0.43567 to -0.43864! Patience: 0/50
2024-12-17 12:46:06.080895: train_loss -0.5083
2024-12-17 12:46:06.081955: val_loss -0.4386
2024-12-17 12:46:06.082902: Pseudo dice [0.7059]
2024-12-17 12:46:06.083912: Epoch time: 98.22 s
2024-12-17 12:46:06.084920: Yayy! New best EMA pseudo Dice: 0.6853
2024-12-17 12:46:08.037982: 
2024-12-17 12:46:08.039525: Epoch 3
2024-12-17 12:46:08.040531: Current learning rate: 0.00982
2024-12-17 12:47:46.137365: Validation loss improved from -0.43864 to -0.49206! Patience: 0/50
2024-12-17 12:47:46.138455: train_loss -0.5337
2024-12-17 12:47:46.139336: val_loss -0.4921
2024-12-17 12:47:46.140237: Pseudo dice [0.7358]
2024-12-17 12:47:46.140971: Epoch time: 98.1 s
2024-12-17 12:47:46.141626: Yayy! New best EMA pseudo Dice: 0.6903
2024-12-17 12:47:48.009428: 
2024-12-17 12:47:48.010861: Epoch 4
2024-12-17 12:47:48.011673: Current learning rate: 0.00976
2024-12-17 12:49:26.802540: Validation loss improved from -0.49206 to -0.49537! Patience: 0/50
2024-12-17 12:49:26.803583: train_loss -0.5505
2024-12-17 12:49:26.804449: val_loss -0.4954
2024-12-17 12:49:26.805214: Pseudo dice [0.7331]
2024-12-17 12:49:26.805993: Epoch time: 98.8 s
2024-12-17 12:49:27.187595: Yayy! New best EMA pseudo Dice: 0.6946
2024-12-17 12:49:29.070495: 
2024-12-17 12:49:29.071640: Epoch 5
2024-12-17 12:49:29.072584: Current learning rate: 0.0097
2024-12-17 12:51:08.928342: Validation loss did not improve from -0.49537. Patience: 1/50
2024-12-17 12:51:08.929310: train_loss -0.5618
2024-12-17 12:51:08.930376: val_loss -0.4649
2024-12-17 12:51:08.931303: Pseudo dice [0.7089]
2024-12-17 12:51:08.932091: Epoch time: 99.86 s
2024-12-17 12:51:08.932985: Yayy! New best EMA pseudo Dice: 0.696
2024-12-17 12:51:10.807791: 
2024-12-17 12:51:10.809258: Epoch 6
2024-12-17 12:51:10.810161: Current learning rate: 0.00964
2024-12-17 12:52:52.868444: Validation loss improved from -0.49537 to -0.49962! Patience: 1/50
2024-12-17 12:52:52.869597: train_loss -0.5731
2024-12-17 12:52:52.870428: val_loss -0.4996
2024-12-17 12:52:52.871207: Pseudo dice [0.7297]
2024-12-17 12:52:52.872050: Epoch time: 102.06 s
2024-12-17 12:52:52.872761: Yayy! New best EMA pseudo Dice: 0.6994
2024-12-17 12:52:54.723034: 
2024-12-17 12:52:54.724288: Epoch 7
2024-12-17 12:52:54.725058: Current learning rate: 0.00958
2024-12-17 12:54:32.114463: Validation loss did not improve from -0.49962. Patience: 1/50
2024-12-17 12:54:32.115607: train_loss -0.5905
2024-12-17 12:54:32.116356: val_loss -0.4826
2024-12-17 12:54:32.117071: Pseudo dice [0.7247]
2024-12-17 12:54:32.117826: Epoch time: 97.39 s
2024-12-17 12:54:32.118585: Yayy! New best EMA pseudo Dice: 0.7019
2024-12-17 12:54:34.481103: 
2024-12-17 12:54:34.482348: Epoch 8
2024-12-17 12:54:34.483215: Current learning rate: 0.00952
2024-12-17 12:56:17.622837: Validation loss did not improve from -0.49962. Patience: 2/50
2024-12-17 12:56:17.623893: train_loss -0.5894
2024-12-17 12:56:17.624820: val_loss -0.4867
2024-12-17 12:56:17.625605: Pseudo dice [0.7248]
2024-12-17 12:56:17.626417: Epoch time: 103.14 s
2024-12-17 12:56:17.627185: Yayy! New best EMA pseudo Dice: 0.7042
2024-12-17 12:56:19.509977: 
2024-12-17 12:56:19.511335: Epoch 9
2024-12-17 12:56:19.512202: Current learning rate: 0.00946
2024-12-17 12:58:03.177590: Validation loss did not improve from -0.49962. Patience: 3/50
2024-12-17 12:58:03.178597: train_loss -0.5869
2024-12-17 12:58:03.180034: val_loss -0.4669
2024-12-17 12:58:03.181157: Pseudo dice [0.7154]
2024-12-17 12:58:03.182199: Epoch time: 103.67 s
2024-12-17 12:58:03.581701: Yayy! New best EMA pseudo Dice: 0.7053
2024-12-17 12:58:05.331902: 
2024-12-17 12:58:05.333585: Epoch 10
2024-12-17 12:58:05.334963: Current learning rate: 0.0094
2024-12-17 12:59:51.402753: Validation loss did not improve from -0.49962. Patience: 4/50
2024-12-17 12:59:51.403651: train_loss -0.6021
2024-12-17 12:59:51.404397: val_loss -0.4964
2024-12-17 12:59:51.405035: Pseudo dice [0.7284]
2024-12-17 12:59:51.405837: Epoch time: 106.07 s
2024-12-17 12:59:51.406600: Yayy! New best EMA pseudo Dice: 0.7076
2024-12-17 12:59:53.222090: 
2024-12-17 12:59:53.223511: Epoch 11
2024-12-17 12:59:53.224361: Current learning rate: 0.00934
2024-12-17 13:01:35.122734: Validation loss improved from -0.49962 to -0.50823! Patience: 4/50
2024-12-17 13:01:35.123954: train_loss -0.6158
2024-12-17 13:01:35.124736: val_loss -0.5082
2024-12-17 13:01:35.125431: Pseudo dice [0.7332]
2024-12-17 13:01:35.126087: Epoch time: 101.9 s
2024-12-17 13:01:35.126830: Yayy! New best EMA pseudo Dice: 0.7102
2024-12-17 13:01:36.900625: 
2024-12-17 13:01:36.902028: Epoch 12
2024-12-17 13:01:36.902766: Current learning rate: 0.00928
2024-12-17 13:03:24.176356: Validation loss improved from -0.50823 to -0.51318! Patience: 0/50
2024-12-17 13:03:24.177378: train_loss -0.6295
2024-12-17 13:03:24.178264: val_loss -0.5132
2024-12-17 13:03:24.178967: Pseudo dice [0.7434]
2024-12-17 13:03:24.179764: Epoch time: 107.28 s
2024-12-17 13:03:24.180493: Yayy! New best EMA pseudo Dice: 0.7135
2024-12-17 13:03:26.083291: 
2024-12-17 13:03:26.084768: Epoch 13
2024-12-17 13:03:26.085940: Current learning rate: 0.00922
2024-12-17 13:05:00.975534: Validation loss did not improve from -0.51318. Patience: 1/50
2024-12-17 13:05:00.976561: train_loss -0.6255
2024-12-17 13:05:00.977474: val_loss -0.4993
2024-12-17 13:05:00.978275: Pseudo dice [0.7345]
2024-12-17 13:05:00.979233: Epoch time: 94.89 s
2024-12-17 13:05:00.980176: Yayy! New best EMA pseudo Dice: 0.7156
2024-12-17 13:05:02.949700: 
2024-12-17 13:05:02.951142: Epoch 14
2024-12-17 13:05:02.952057: Current learning rate: 0.00916
2024-12-17 13:06:53.781578: Validation loss did not improve from -0.51318. Patience: 2/50
2024-12-17 13:06:53.782710: train_loss -0.6361
2024-12-17 13:06:53.783805: val_loss -0.5016
2024-12-17 13:06:53.784737: Pseudo dice [0.7265]
2024-12-17 13:06:53.785581: Epoch time: 110.83 s
2024-12-17 13:06:54.244044: Yayy! New best EMA pseudo Dice: 0.7167
2024-12-17 13:06:56.145796: 
2024-12-17 13:06:56.147199: Epoch 15
2024-12-17 13:06:56.148434: Current learning rate: 0.0091
2024-12-17 13:08:34.081551: Validation loss did not improve from -0.51318. Patience: 3/50
2024-12-17 13:08:34.083270: train_loss -0.6352
2024-12-17 13:08:34.084280: val_loss -0.513
2024-12-17 13:08:34.085109: Pseudo dice [0.7353]
2024-12-17 13:08:34.085927: Epoch time: 97.94 s
2024-12-17 13:08:34.086601: Yayy! New best EMA pseudo Dice: 0.7186
2024-12-17 13:08:35.961361: 
2024-12-17 13:08:35.962621: Epoch 16
2024-12-17 13:08:35.963413: Current learning rate: 0.00903
2024-12-17 13:10:23.013040: Validation loss did not improve from -0.51318. Patience: 4/50
2024-12-17 13:10:23.014025: train_loss -0.6457
2024-12-17 13:10:23.015020: val_loss -0.5037
2024-12-17 13:10:23.015949: Pseudo dice [0.7332]
2024-12-17 13:10:23.016731: Epoch time: 107.05 s
2024-12-17 13:10:23.017578: Yayy! New best EMA pseudo Dice: 0.72
2024-12-17 13:10:24.880352: 
2024-12-17 13:10:24.881912: Epoch 17
2024-12-17 13:10:24.883039: Current learning rate: 0.00897
2024-12-17 13:12:12.357820: Validation loss did not improve from -0.51318. Patience: 5/50
2024-12-17 13:12:12.358742: train_loss -0.6511
2024-12-17 13:12:12.359609: val_loss -0.5102
2024-12-17 13:12:12.360350: Pseudo dice [0.744]
2024-12-17 13:12:12.361122: Epoch time: 107.48 s
2024-12-17 13:12:12.361902: Yayy! New best EMA pseudo Dice: 0.7224
2024-12-17 13:12:14.312455: 
2024-12-17 13:12:14.313837: Epoch 18
2024-12-17 13:12:14.314779: Current learning rate: 0.00891
2024-12-17 13:14:05.721227: Validation loss did not improve from -0.51318. Patience: 6/50
2024-12-17 13:14:05.722220: train_loss -0.6499
2024-12-17 13:14:05.723068: val_loss -0.5073
2024-12-17 13:14:05.723737: Pseudo dice [0.7314]
2024-12-17 13:14:05.724505: Epoch time: 111.41 s
2024-12-17 13:14:05.725235: Yayy! New best EMA pseudo Dice: 0.7233
2024-12-17 13:14:08.029159: 
2024-12-17 13:14:08.030670: Epoch 19
2024-12-17 13:14:08.031558: Current learning rate: 0.00885
2024-12-17 13:15:52.613651: Validation loss did not improve from -0.51318. Patience: 7/50
2024-12-17 13:15:52.614500: train_loss -0.6554
2024-12-17 13:15:52.615482: val_loss -0.4981
2024-12-17 13:15:52.616575: Pseudo dice [0.7392]
2024-12-17 13:15:52.617539: Epoch time: 104.59 s
2024-12-17 13:15:53.077192: Yayy! New best EMA pseudo Dice: 0.7249
2024-12-17 13:15:55.004522: 
2024-12-17 13:15:55.005934: Epoch 20
2024-12-17 13:15:55.007133: Current learning rate: 0.00879
2024-12-17 13:17:46.449439: Validation loss did not improve from -0.51318. Patience: 8/50
2024-12-17 13:17:46.450277: train_loss -0.6505
2024-12-17 13:17:46.451212: val_loss -0.505
2024-12-17 13:17:46.452054: Pseudo dice [0.7347]
2024-12-17 13:17:46.452894: Epoch time: 111.45 s
2024-12-17 13:17:46.453704: Yayy! New best EMA pseudo Dice: 0.7259
2024-12-17 13:17:48.439446: 
2024-12-17 13:17:48.440962: Epoch 21
2024-12-17 13:17:48.441925: Current learning rate: 0.00873
2024-12-17 13:19:40.614806: Validation loss improved from -0.51318 to -0.52802! Patience: 8/50
2024-12-17 13:19:40.616361: train_loss -0.6634
2024-12-17 13:19:40.617162: val_loss -0.528
2024-12-17 13:19:40.618016: Pseudo dice [0.743]
2024-12-17 13:19:40.618804: Epoch time: 112.18 s
2024-12-17 13:19:40.619463: Yayy! New best EMA pseudo Dice: 0.7276
2024-12-17 13:19:42.405014: 
2024-12-17 13:19:42.406249: Epoch 22
2024-12-17 13:19:42.407115: Current learning rate: 0.00867
2024-12-17 13:21:33.629396: Validation loss did not improve from -0.52802. Patience: 1/50
2024-12-17 13:21:33.630428: train_loss -0.6672
2024-12-17 13:21:33.636922: val_loss -0.476
2024-12-17 13:21:33.637980: Pseudo dice [0.7179]
2024-12-17 13:21:33.638909: Epoch time: 111.23 s
2024-12-17 13:21:35.130489: 
2024-12-17 13:21:35.131843: Epoch 23
2024-12-17 13:21:35.132726: Current learning rate: 0.00861
2024-12-17 13:23:26.435321: Validation loss did not improve from -0.52802. Patience: 2/50
2024-12-17 13:23:26.436345: train_loss -0.6739
2024-12-17 13:23:26.437161: val_loss -0.5199
2024-12-17 13:23:26.437939: Pseudo dice [0.7378]
2024-12-17 13:23:26.438676: Epoch time: 111.31 s
2024-12-17 13:23:26.439411: Yayy! New best EMA pseudo Dice: 0.7277
2024-12-17 13:23:28.242741: 
2024-12-17 13:23:28.243951: Epoch 24
2024-12-17 13:23:28.244866: Current learning rate: 0.00855
2024-12-17 13:25:19.914169: Validation loss did not improve from -0.52802. Patience: 3/50
2024-12-17 13:25:19.915640: train_loss -0.6777
2024-12-17 13:25:19.916753: val_loss -0.5016
2024-12-17 13:25:19.917525: Pseudo dice [0.7445]
2024-12-17 13:25:19.918208: Epoch time: 111.67 s
2024-12-17 13:25:20.373554: Yayy! New best EMA pseudo Dice: 0.7294
2024-12-17 13:25:22.220273: 
2024-12-17 13:25:22.221469: Epoch 25
2024-12-17 13:25:22.222400: Current learning rate: 0.00849
2024-12-17 13:27:14.134121: Validation loss did not improve from -0.52802. Patience: 4/50
2024-12-17 13:27:14.135232: train_loss -0.6712
2024-12-17 13:27:14.136087: val_loss -0.5235
2024-12-17 13:27:14.136753: Pseudo dice [0.7495]
2024-12-17 13:27:14.137519: Epoch time: 111.92 s
2024-12-17 13:27:14.138180: Yayy! New best EMA pseudo Dice: 0.7314
2024-12-17 13:27:15.964680: 
2024-12-17 13:27:15.966052: Epoch 26
2024-12-17 13:27:15.966859: Current learning rate: 0.00843
2024-12-17 13:29:09.328239: Validation loss improved from -0.52802 to -0.53252! Patience: 4/50
2024-12-17 13:29:09.329117: train_loss -0.6764
2024-12-17 13:29:09.329886: val_loss -0.5325
2024-12-17 13:29:09.330594: Pseudo dice [0.7528]
2024-12-17 13:29:09.331396: Epoch time: 113.37 s
2024-12-17 13:29:09.332177: Yayy! New best EMA pseudo Dice: 0.7336
2024-12-17 13:29:11.209229: 
2024-12-17 13:29:11.210750: Epoch 27
2024-12-17 13:29:11.211866: Current learning rate: 0.00836
2024-12-17 13:31:03.225045: Validation loss did not improve from -0.53252. Patience: 1/50
2024-12-17 13:31:03.226038: train_loss -0.684
2024-12-17 13:31:03.226865: val_loss -0.508
2024-12-17 13:31:03.227556: Pseudo dice [0.7483]
2024-12-17 13:31:03.228328: Epoch time: 112.02 s
2024-12-17 13:31:03.228959: Yayy! New best EMA pseudo Dice: 0.735
2024-12-17 13:31:05.178915: 
2024-12-17 13:31:05.180250: Epoch 28
2024-12-17 13:31:05.181164: Current learning rate: 0.0083
2024-12-17 13:32:57.293074: Validation loss did not improve from -0.53252. Patience: 2/50
2024-12-17 13:32:57.294052: train_loss -0.6862
2024-12-17 13:32:57.294794: val_loss -0.52
2024-12-17 13:32:57.295569: Pseudo dice [0.7446]
2024-12-17 13:32:57.296285: Epoch time: 112.12 s
2024-12-17 13:32:57.297062: Yayy! New best EMA pseudo Dice: 0.736
2024-12-17 13:32:59.538996: 
2024-12-17 13:32:59.540597: Epoch 29
2024-12-17 13:32:59.541359: Current learning rate: 0.00824
2024-12-17 13:34:53.278347: Validation loss improved from -0.53252 to -0.54341! Patience: 2/50
2024-12-17 13:34:53.279360: train_loss -0.6798
2024-12-17 13:34:53.280216: val_loss -0.5434
2024-12-17 13:34:53.280964: Pseudo dice [0.7584]
2024-12-17 13:34:53.281787: Epoch time: 113.74 s
2024-12-17 13:34:53.721084: Yayy! New best EMA pseudo Dice: 0.7382
2024-12-17 13:34:55.654954: 
2024-12-17 13:34:55.656303: Epoch 30
2024-12-17 13:34:55.657116: Current learning rate: 0.00818
2024-12-17 13:36:47.224354: Validation loss did not improve from -0.54341. Patience: 1/50
2024-12-17 13:36:47.225332: train_loss -0.686
2024-12-17 13:36:47.226283: val_loss -0.5331
2024-12-17 13:36:47.227121: Pseudo dice [0.7576]
2024-12-17 13:36:47.227925: Epoch time: 111.57 s
2024-12-17 13:36:47.228794: Yayy! New best EMA pseudo Dice: 0.7402
2024-12-17 13:36:49.151662: 
2024-12-17 13:36:49.153156: Epoch 31
2024-12-17 13:36:49.154043: Current learning rate: 0.00812
2024-12-17 13:38:41.335469: Validation loss did not improve from -0.54341. Patience: 2/50
2024-12-17 13:38:41.336600: train_loss -0.6865
2024-12-17 13:38:41.337719: val_loss -0.5049
2024-12-17 13:38:41.338526: Pseudo dice [0.7455]
2024-12-17 13:38:41.339345: Epoch time: 112.19 s
2024-12-17 13:38:41.340098: Yayy! New best EMA pseudo Dice: 0.7407
2024-12-17 13:38:43.241583: 
2024-12-17 13:38:43.242850: Epoch 32
2024-12-17 13:38:43.243705: Current learning rate: 0.00806
2024-12-17 13:40:41.571048: Validation loss improved from -0.54341 to -0.55312! Patience: 2/50
2024-12-17 13:40:41.572118: train_loss -0.7017
2024-12-17 13:40:41.573125: val_loss -0.5531
2024-12-17 13:40:41.573940: Pseudo dice [0.7602]
2024-12-17 13:40:41.574718: Epoch time: 118.33 s
2024-12-17 13:40:41.575370: Yayy! New best EMA pseudo Dice: 0.7427
2024-12-17 13:40:43.481055: 
2024-12-17 13:40:43.482326: Epoch 33
2024-12-17 13:40:43.483057: Current learning rate: 0.008
2024-12-17 13:42:36.255381: Validation loss did not improve from -0.55312. Patience: 1/50
2024-12-17 13:42:36.256357: train_loss -0.7013
2024-12-17 13:42:36.257324: val_loss -0.5362
2024-12-17 13:42:36.258168: Pseudo dice [0.7538]
2024-12-17 13:42:36.259138: Epoch time: 112.78 s
2024-12-17 13:42:36.260068: Yayy! New best EMA pseudo Dice: 0.7438
2024-12-17 13:42:38.194866: 
2024-12-17 13:42:38.196468: Epoch 34
2024-12-17 13:42:38.197766: Current learning rate: 0.00793
2024-12-17 13:44:18.652155: Validation loss did not improve from -0.55312. Patience: 2/50
2024-12-17 13:44:18.653408: train_loss -0.6973
2024-12-17 13:44:18.654701: val_loss -0.5244
2024-12-17 13:44:18.655508: Pseudo dice [0.7482]
2024-12-17 13:44:18.656334: Epoch time: 100.46 s
2024-12-17 13:44:19.097440: Yayy! New best EMA pseudo Dice: 0.7442
2024-12-17 13:44:20.905342: 
2024-12-17 13:44:20.907200: Epoch 35
2024-12-17 13:44:20.908093: Current learning rate: 0.00787
2024-12-17 13:45:57.675413: Validation loss improved from -0.55312 to -0.55719! Patience: 2/50
2024-12-17 13:45:57.676938: train_loss -0.7026
2024-12-17 13:45:57.678373: val_loss -0.5572
2024-12-17 13:45:57.679029: Pseudo dice [0.7618]
2024-12-17 13:45:57.679712: Epoch time: 96.77 s
2024-12-17 13:45:57.680339: Yayy! New best EMA pseudo Dice: 0.746
2024-12-17 13:45:59.820545: 
2024-12-17 13:45:59.822276: Epoch 36
2024-12-17 13:45:59.823110: Current learning rate: 0.00781
2024-12-17 13:47:37.117293: Validation loss did not improve from -0.55719. Patience: 1/50
2024-12-17 13:47:37.118471: train_loss -0.7053
2024-12-17 13:47:37.119765: val_loss -0.5409
2024-12-17 13:47:37.120652: Pseudo dice [0.7516]
2024-12-17 13:47:37.121544: Epoch time: 97.3 s
2024-12-17 13:47:37.122382: Yayy! New best EMA pseudo Dice: 0.7465
2024-12-17 13:47:38.946670: 
2024-12-17 13:47:38.948250: Epoch 37
2024-12-17 13:47:38.948983: Current learning rate: 0.00775
2024-12-17 13:49:16.151826: Validation loss did not improve from -0.55719. Patience: 2/50
2024-12-17 13:49:16.153183: train_loss -0.7079
2024-12-17 13:49:16.154404: val_loss -0.5184
2024-12-17 13:49:16.155406: Pseudo dice [0.7487]
2024-12-17 13:49:16.156336: Epoch time: 97.21 s
2024-12-17 13:49:16.157285: Yayy! New best EMA pseudo Dice: 0.7468
2024-12-17 13:49:17.981633: 
2024-12-17 13:49:17.983146: Epoch 38
2024-12-17 13:49:17.984090: Current learning rate: 0.00769
2024-12-17 13:51:10.270736: Validation loss did not improve from -0.55719. Patience: 3/50
2024-12-17 13:51:10.272076: train_loss -0.714
2024-12-17 13:51:10.273152: val_loss -0.5348
2024-12-17 13:51:10.274029: Pseudo dice [0.7599]
2024-12-17 13:51:10.275055: Epoch time: 112.29 s
2024-12-17 13:51:10.275913: Yayy! New best EMA pseudo Dice: 0.7481
2024-12-17 13:51:12.507592: 
2024-12-17 13:51:12.509183: Epoch 39
2024-12-17 13:51:12.510151: Current learning rate: 0.00763
2024-12-17 13:53:52.698309: Validation loss did not improve from -0.55719. Patience: 4/50
2024-12-17 13:53:52.699290: train_loss -0.7042
2024-12-17 13:53:52.700209: val_loss -0.5092
2024-12-17 13:53:52.701046: Pseudo dice [0.7475]
2024-12-17 13:53:52.701795: Epoch time: 160.19 s
2024-12-17 13:53:54.577153: 
2024-12-17 13:53:54.578291: Epoch 40
2024-12-17 13:53:54.579046: Current learning rate: 0.00756
2024-12-17 13:57:32.354147: Validation loss did not improve from -0.55719. Patience: 5/50
2024-12-17 13:57:32.355157: train_loss -0.7096
2024-12-17 13:57:32.355887: val_loss -0.5129
2024-12-17 13:57:32.356668: Pseudo dice [0.7458]
2024-12-17 13:57:32.357480: Epoch time: 217.78 s
2024-12-17 13:57:33.861091: 
2024-12-17 13:57:33.862377: Epoch 41
2024-12-17 13:57:33.863099: Current learning rate: 0.0075
2024-12-17 14:01:06.720677: Validation loss did not improve from -0.55719. Patience: 6/50
2024-12-17 14:01:06.721582: train_loss -0.7138
2024-12-17 14:01:06.722435: val_loss -0.4978
2024-12-17 14:01:06.723048: Pseudo dice [0.7385]
2024-12-17 14:01:06.723659: Epoch time: 212.86 s
2024-12-17 14:01:08.164637: 
2024-12-17 14:01:08.165959: Epoch 42
2024-12-17 14:01:08.166742: Current learning rate: 0.00744
2024-12-17 14:04:39.741769: Validation loss did not improve from -0.55719. Patience: 7/50
2024-12-17 14:04:39.742704: train_loss -0.7138
2024-12-17 14:04:39.743569: val_loss -0.5382
2024-12-17 14:04:39.744380: Pseudo dice [0.7548]
2024-12-17 14:04:39.745379: Epoch time: 211.58 s
2024-12-17 14:04:41.120192: 
2024-12-17 14:04:41.121547: Epoch 43
2024-12-17 14:04:41.122396: Current learning rate: 0.00738
2024-12-17 14:08:11.197473: Validation loss did not improve from -0.55719. Patience: 8/50
2024-12-17 14:08:11.198452: train_loss -0.7257
2024-12-17 14:08:11.199236: val_loss -0.5291
2024-12-17 14:08:11.200000: Pseudo dice [0.7535]
2024-12-17 14:08:11.200785: Epoch time: 210.08 s
2024-12-17 14:08:11.201455: Yayy! New best EMA pseudo Dice: 0.7482
2024-12-17 14:08:13.011900: 
2024-12-17 14:08:13.013266: Epoch 44
2024-12-17 14:08:13.013948: Current learning rate: 0.00732
2024-12-17 14:11:43.352876: Validation loss did not improve from -0.55719. Patience: 9/50
2024-12-17 14:11:43.353882: train_loss -0.7199
2024-12-17 14:11:43.354645: val_loss -0.5069
2024-12-17 14:11:43.355291: Pseudo dice [0.7406]
2024-12-17 14:11:43.355954: Epoch time: 210.34 s
2024-12-17 14:11:45.163517: 
2024-12-17 14:11:45.164797: Epoch 45
2024-12-17 14:11:45.165511: Current learning rate: 0.00725
2024-12-17 14:15:12.969861: Validation loss did not improve from -0.55719. Patience: 10/50
2024-12-17 14:15:12.970888: train_loss -0.7195
2024-12-17 14:15:12.971683: val_loss -0.5108
2024-12-17 14:15:12.972352: Pseudo dice [0.7486]
2024-12-17 14:15:12.973047: Epoch time: 207.81 s
2024-12-17 14:15:14.361008: 
2024-12-17 14:15:14.362158: Epoch 46
2024-12-17 14:15:14.362988: Current learning rate: 0.00719
2024-12-17 14:18:45.010896: Validation loss did not improve from -0.55719. Patience: 11/50
2024-12-17 14:18:45.011992: train_loss -0.7236
2024-12-17 14:18:45.013107: val_loss -0.536
2024-12-17 14:18:45.014124: Pseudo dice [0.751]
2024-12-17 14:18:45.014991: Epoch time: 210.65 s
2024-12-17 14:18:46.429392: 
2024-12-17 14:18:46.430758: Epoch 47
2024-12-17 14:18:46.431499: Current learning rate: 0.00713
2024-12-17 14:22:38.154696: Validation loss did not improve from -0.55719. Patience: 12/50
2024-12-17 14:22:38.155708: train_loss -0.725
2024-12-17 14:22:38.156718: val_loss -0.5161
2024-12-17 14:22:38.157682: Pseudo dice [0.7535]
2024-12-17 14:22:38.158604: Epoch time: 231.73 s
2024-12-17 14:22:38.159584: Yayy! New best EMA pseudo Dice: 0.7485
2024-12-17 14:22:39.914750: 
2024-12-17 14:22:39.916283: Epoch 48
2024-12-17 14:22:39.917312: Current learning rate: 0.00707
2024-12-17 14:26:15.951953: Validation loss did not improve from -0.55719. Patience: 13/50
2024-12-17 14:26:15.953013: train_loss -0.7315
2024-12-17 14:26:15.953782: val_loss -0.5365
2024-12-17 14:26:15.954493: Pseudo dice [0.7542]
2024-12-17 14:26:15.955195: Epoch time: 216.04 s
2024-12-17 14:26:15.955929: Yayy! New best EMA pseudo Dice: 0.7491
2024-12-17 14:26:17.806546: 
2024-12-17 14:26:17.807957: Epoch 49
2024-12-17 14:26:17.808892: Current learning rate: 0.007
2024-12-17 14:29:54.239855: Validation loss did not improve from -0.55719. Patience: 14/50
2024-12-17 14:29:54.240851: train_loss -0.7262
2024-12-17 14:29:54.241865: val_loss -0.5539
2024-12-17 14:29:54.242676: Pseudo dice [0.7617]
2024-12-17 14:29:54.243497: Epoch time: 216.44 s
2024-12-17 14:29:54.673107: Yayy! New best EMA pseudo Dice: 0.7503
2024-12-17 14:29:56.831659: 
2024-12-17 14:29:56.833168: Epoch 50
2024-12-17 14:29:56.834065: Current learning rate: 0.00694
2024-12-17 14:33:33.213301: Validation loss did not improve from -0.55719. Patience: 15/50
2024-12-17 14:33:33.214378: train_loss -0.7308
2024-12-17 14:33:33.215130: val_loss -0.5386
2024-12-17 14:33:33.215775: Pseudo dice [0.7614]
2024-12-17 14:33:33.216600: Epoch time: 216.38 s
2024-12-17 14:33:33.217347: Yayy! New best EMA pseudo Dice: 0.7514
2024-12-17 14:33:34.988033: 
2024-12-17 14:33:34.989348: Epoch 51
2024-12-17 14:33:34.990218: Current learning rate: 0.00688
2024-12-17 14:36:55.783217: Validation loss did not improve from -0.55719. Patience: 16/50
2024-12-17 14:36:55.784277: train_loss -0.7376
2024-12-17 14:36:55.785203: val_loss -0.5377
2024-12-17 14:36:55.785978: Pseudo dice [0.7676]
2024-12-17 14:36:55.786754: Epoch time: 200.8 s
2024-12-17 14:36:55.787422: Yayy! New best EMA pseudo Dice: 0.753
2024-12-17 14:36:57.546095: 
2024-12-17 14:36:57.547630: Epoch 52
2024-12-17 14:36:57.548535: Current learning rate: 0.00682
2024-12-17 14:40:35.872623: Validation loss did not improve from -0.55719. Patience: 17/50
2024-12-17 14:40:35.873645: train_loss -0.7376
2024-12-17 14:40:35.874450: val_loss -0.5327
2024-12-17 14:40:35.875067: Pseudo dice [0.7571]
2024-12-17 14:40:35.875685: Epoch time: 218.33 s
2024-12-17 14:40:35.876465: Yayy! New best EMA pseudo Dice: 0.7535
2024-12-17 14:40:37.638163: 
2024-12-17 14:40:37.639525: Epoch 53
2024-12-17 14:40:37.640295: Current learning rate: 0.00675
2024-12-17 14:44:17.457741: Validation loss did not improve from -0.55719. Patience: 18/50
2024-12-17 14:44:17.458683: train_loss -0.7348
2024-12-17 14:44:17.459542: val_loss -0.5311
2024-12-17 14:44:17.460294: Pseudo dice [0.7564]
2024-12-17 14:44:17.461032: Epoch time: 219.82 s
2024-12-17 14:44:17.461645: Yayy! New best EMA pseudo Dice: 0.7538
2024-12-17 14:44:19.279926: 
2024-12-17 14:44:19.281050: Epoch 54
2024-12-17 14:44:19.281797: Current learning rate: 0.00669
2024-12-17 14:48:31.161546: Validation loss improved from -0.55719 to -0.56561! Patience: 18/50
2024-12-17 14:48:31.162427: train_loss -0.7349
2024-12-17 14:48:31.164043: val_loss -0.5656
2024-12-17 14:48:31.164799: Pseudo dice [0.7736]
2024-12-17 14:48:31.165630: Epoch time: 251.88 s
2024-12-17 14:48:31.578428: Yayy! New best EMA pseudo Dice: 0.7557
2024-12-17 14:48:33.354501: 
2024-12-17 14:48:33.355782: Epoch 55
2024-12-17 14:48:33.356561: Current learning rate: 0.00663
2024-12-17 14:52:05.169893: Validation loss did not improve from -0.56561. Patience: 1/50
2024-12-17 14:52:05.171841: train_loss -0.7359
2024-12-17 14:52:05.172875: val_loss -0.5389
2024-12-17 14:52:05.173505: Pseudo dice [0.7566]
2024-12-17 14:52:05.174253: Epoch time: 211.82 s
2024-12-17 14:52:05.174962: Yayy! New best EMA pseudo Dice: 0.7558
2024-12-17 14:52:06.947358: 
2024-12-17 14:52:06.948705: Epoch 56
2024-12-17 14:52:06.949519: Current learning rate: 0.00657
2024-12-17 14:55:48.204133: Validation loss did not improve from -0.56561. Patience: 2/50
2024-12-17 14:55:48.205123: train_loss -0.7415
2024-12-17 14:55:48.206065: val_loss -0.5352
2024-12-17 14:55:48.207033: Pseudo dice [0.7587]
2024-12-17 14:55:48.208013: Epoch time: 221.26 s
2024-12-17 14:55:48.208931: Yayy! New best EMA pseudo Dice: 0.7561
2024-12-17 14:55:49.973114: 
2024-12-17 14:55:49.974564: Epoch 57
2024-12-17 14:55:49.975589: Current learning rate: 0.0065
2024-12-17 14:59:38.180430: Validation loss did not improve from -0.56561. Patience: 3/50
2024-12-17 14:59:38.181453: train_loss -0.7408
2024-12-17 14:59:38.182375: val_loss -0.5435
2024-12-17 14:59:38.183140: Pseudo dice [0.7711]
2024-12-17 14:59:38.183848: Epoch time: 228.21 s
2024-12-17 14:59:38.184500: Yayy! New best EMA pseudo Dice: 0.7576
2024-12-17 14:59:39.981695: 
2024-12-17 14:59:39.982862: Epoch 58
2024-12-17 14:59:39.983665: Current learning rate: 0.00644
2024-12-17 15:03:26.171970: Validation loss improved from -0.56561 to -0.56957! Patience: 3/50
2024-12-17 15:03:26.172990: train_loss -0.7414
2024-12-17 15:03:26.174065: val_loss -0.5696
2024-12-17 15:03:26.174890: Pseudo dice [0.7751]
2024-12-17 15:03:26.175758: Epoch time: 226.19 s
2024-12-17 15:03:26.176553: Yayy! New best EMA pseudo Dice: 0.7594
2024-12-17 15:03:28.014706: 
2024-12-17 15:03:28.015949: Epoch 59
2024-12-17 15:03:28.016802: Current learning rate: 0.00638
2024-12-17 15:07:05.084988: Validation loss did not improve from -0.56957. Patience: 1/50
2024-12-17 15:07:05.086129: train_loss -0.7473
2024-12-17 15:07:05.087103: val_loss -0.5401
2024-12-17 15:07:05.087939: Pseudo dice [0.7598]
2024-12-17 15:07:05.088801: Epoch time: 217.07 s
2024-12-17 15:07:05.500286: Yayy! New best EMA pseudo Dice: 0.7594
2024-12-17 15:07:07.316037: 
2024-12-17 15:07:07.317631: Epoch 60
2024-12-17 15:07:07.318765: Current learning rate: 0.00631
2024-12-17 15:10:47.107533: Validation loss did not improve from -0.56957. Patience: 2/50
2024-12-17 15:10:47.108410: train_loss -0.7477
2024-12-17 15:10:47.109149: val_loss -0.524
2024-12-17 15:10:47.109804: Pseudo dice [0.7462]
2024-12-17 15:10:47.110463: Epoch time: 219.79 s
2024-12-17 15:10:49.934455: 
2024-12-17 15:10:49.935789: Epoch 61
2024-12-17 15:10:49.936547: Current learning rate: 0.00625
2024-12-17 15:14:30.512962: Validation loss did not improve from -0.56957. Patience: 3/50
2024-12-17 15:14:30.513941: train_loss -0.7434
2024-12-17 15:14:30.514631: val_loss -0.5239
2024-12-17 15:14:30.515377: Pseudo dice [0.7457]
2024-12-17 15:14:30.516192: Epoch time: 220.58 s
2024-12-17 15:14:31.910438: 
2024-12-17 15:14:31.911839: Epoch 62
2024-12-17 15:14:31.912594: Current learning rate: 0.00619
2024-12-17 15:18:17.528030: Validation loss did not improve from -0.56957. Patience: 4/50
2024-12-17 15:18:17.529129: train_loss -0.7507
2024-12-17 15:18:17.530145: val_loss -0.5553
2024-12-17 15:18:17.530830: Pseudo dice [0.7727]
2024-12-17 15:18:17.531510: Epoch time: 225.62 s
2024-12-17 15:18:18.938714: 
2024-12-17 15:18:18.940276: Epoch 63
2024-12-17 15:18:18.941036: Current learning rate: 0.00612
2024-12-17 15:22:15.553878: Validation loss did not improve from -0.56957. Patience: 5/50
2024-12-17 15:22:15.555004: train_loss -0.7536
2024-12-17 15:22:15.555965: val_loss -0.5437
2024-12-17 15:22:15.556712: Pseudo dice [0.7664]
2024-12-17 15:22:15.557584: Epoch time: 236.62 s
2024-12-17 15:22:17.037972: 
2024-12-17 15:22:17.039514: Epoch 64
2024-12-17 15:22:17.040449: Current learning rate: 0.00606
2024-12-17 15:26:11.147609: Validation loss did not improve from -0.56957. Patience: 6/50
2024-12-17 15:26:11.148680: train_loss -0.7497
2024-12-17 15:26:11.149919: val_loss -0.5089
2024-12-17 15:26:11.150923: Pseudo dice [0.7442]
2024-12-17 15:26:11.151933: Epoch time: 234.11 s
2024-12-17 15:26:12.924612: 
2024-12-17 15:26:12.926107: Epoch 65
2024-12-17 15:26:12.927184: Current learning rate: 0.006
2024-12-17 15:30:09.009892: Validation loss did not improve from -0.56957. Patience: 7/50
2024-12-17 15:30:09.010879: train_loss -0.7543
2024-12-17 15:30:09.012001: val_loss -0.5282
2024-12-17 15:30:09.012864: Pseudo dice [0.7533]
2024-12-17 15:30:09.013821: Epoch time: 236.09 s
2024-12-17 15:30:10.448530: 
2024-12-17 15:30:10.449963: Epoch 66
2024-12-17 15:30:10.450974: Current learning rate: 0.00593
2024-12-17 15:34:12.206684: Validation loss did not improve from -0.56957. Patience: 8/50
2024-12-17 15:34:12.207656: train_loss -0.7503
2024-12-17 15:34:12.208382: val_loss -0.5485
2024-12-17 15:34:12.208960: Pseudo dice [0.7635]
2024-12-17 15:34:12.209682: Epoch time: 241.76 s
2024-12-17 15:34:13.642961: 
2024-12-17 15:34:13.644168: Epoch 67
2024-12-17 15:34:13.644933: Current learning rate: 0.00587
2024-12-17 15:38:19.288588: Validation loss did not improve from -0.56957. Patience: 9/50
2024-12-17 15:38:19.289610: train_loss -0.7517
2024-12-17 15:38:19.290497: val_loss -0.538
2024-12-17 15:38:19.291374: Pseudo dice [0.7621]
2024-12-17 15:38:19.292143: Epoch time: 245.65 s
2024-12-17 15:38:20.717457: 
2024-12-17 15:38:20.719034: Epoch 68
2024-12-17 15:38:20.720065: Current learning rate: 0.00581
2024-12-17 15:42:16.831133: Validation loss did not improve from -0.56957. Patience: 10/50
2024-12-17 15:42:16.832112: train_loss -0.7511
2024-12-17 15:42:16.833073: val_loss -0.5273
2024-12-17 15:42:16.833717: Pseudo dice [0.7637]
2024-12-17 15:42:16.834452: Epoch time: 236.12 s
2024-12-17 15:42:18.218414: 
2024-12-17 15:42:18.219801: Epoch 69
2024-12-17 15:42:18.220591: Current learning rate: 0.00574
2024-12-17 15:46:29.831586: Validation loss did not improve from -0.56957. Patience: 11/50
2024-12-17 15:46:29.832589: train_loss -0.7528
2024-12-17 15:46:29.833358: val_loss -0.5391
2024-12-17 15:46:29.834116: Pseudo dice [0.7621]
2024-12-17 15:46:29.834946: Epoch time: 251.62 s
2024-12-17 15:46:31.678352: 
2024-12-17 15:46:31.680901: Epoch 70
2024-12-17 15:46:31.681704: Current learning rate: 0.00568
2024-12-17 15:50:28.216424: Validation loss did not improve from -0.56957. Patience: 12/50
2024-12-17 15:50:28.217118: train_loss -0.7573
2024-12-17 15:50:28.218000: val_loss -0.5319
2024-12-17 15:50:28.218705: Pseudo dice [0.7584]
2024-12-17 15:50:28.219536: Epoch time: 236.54 s
2024-12-17 15:50:29.628131: 
2024-12-17 15:50:29.629464: Epoch 71
2024-12-17 15:50:29.630226: Current learning rate: 0.00562
2024-12-17 15:54:27.249293: Validation loss did not improve from -0.56957. Patience: 13/50
2024-12-17 15:54:27.251087: train_loss -0.7591
2024-12-17 15:54:27.252743: val_loss -0.5485
2024-12-17 15:54:27.253691: Pseudo dice [0.762]
2024-12-17 15:54:27.255050: Epoch time: 237.62 s
2024-12-17 15:54:29.053457: 
2024-12-17 15:54:29.054937: Epoch 72
2024-12-17 15:54:29.056009: Current learning rate: 0.00555
2024-12-17 15:58:15.353355: Validation loss did not improve from -0.56957. Patience: 14/50
2024-12-17 15:58:15.354905: train_loss -0.7558
2024-12-17 15:58:15.355776: val_loss -0.526
2024-12-17 15:58:15.356536: Pseudo dice [0.7494]
2024-12-17 15:58:15.357319: Epoch time: 226.3 s
2024-12-17 15:58:16.805231: 
2024-12-17 15:58:16.806689: Epoch 73
2024-12-17 15:58:16.807458: Current learning rate: 0.00549
2024-12-17 16:02:21.853436: Validation loss did not improve from -0.56957. Patience: 15/50
2024-12-17 16:02:21.854371: train_loss -0.7604
2024-12-17 16:02:21.855290: val_loss -0.5297
2024-12-17 16:02:21.856071: Pseudo dice [0.7546]
2024-12-17 16:02:21.856813: Epoch time: 245.05 s
2024-12-17 16:02:23.279975: 
2024-12-17 16:02:23.281282: Epoch 74
2024-12-17 16:02:23.282096: Current learning rate: 0.00542
2024-12-17 16:06:20.000383: Validation loss did not improve from -0.56957. Patience: 16/50
2024-12-17 16:06:20.001435: train_loss -0.7616
2024-12-17 16:06:20.002136: val_loss -0.5442
2024-12-17 16:06:20.002882: Pseudo dice [0.7554]
2024-12-17 16:06:20.003593: Epoch time: 236.72 s
2024-12-17 16:06:21.859818: 
2024-12-17 16:06:21.861324: Epoch 75
2024-12-17 16:06:21.862439: Current learning rate: 0.00536
2024-12-17 16:10:13.186236: Validation loss did not improve from -0.56957. Patience: 17/50
2024-12-17 16:10:13.187177: train_loss -0.7604
2024-12-17 16:10:13.187980: val_loss -0.5358
2024-12-17 16:10:13.188749: Pseudo dice [0.7618]
2024-12-17 16:10:13.189484: Epoch time: 231.33 s
2024-12-17 16:10:14.590787: 
2024-12-17 16:10:14.592108: Epoch 76
2024-12-17 16:10:14.592893: Current learning rate: 0.00529
2024-12-17 16:13:46.279351: Validation loss did not improve from -0.56957. Patience: 18/50
2024-12-17 16:13:46.280107: train_loss -0.759
2024-12-17 16:13:46.280844: val_loss -0.5267
2024-12-17 16:13:46.281451: Pseudo dice [0.7489]
2024-12-17 16:13:46.282068: Epoch time: 211.69 s
2024-12-17 16:13:47.684302: 
2024-12-17 16:13:47.685593: Epoch 77
2024-12-17 16:13:47.686274: Current learning rate: 0.00523
2024-12-17 16:17:29.977963: Validation loss did not improve from -0.56957. Patience: 19/50
2024-12-17 16:17:29.979009: train_loss -0.7661
2024-12-17 16:17:29.979704: val_loss -0.5355
2024-12-17 16:17:29.980319: Pseudo dice [0.757]
2024-12-17 16:17:29.980974: Epoch time: 222.3 s
2024-12-17 16:17:31.409456: 
2024-12-17 16:17:31.410627: Epoch 78
2024-12-17 16:17:31.411455: Current learning rate: 0.00517
2024-12-17 16:21:10.967917: Validation loss did not improve from -0.56957. Patience: 20/50
2024-12-17 16:21:10.968868: train_loss -0.7641
2024-12-17 16:21:10.969954: val_loss -0.54
2024-12-17 16:21:10.970776: Pseudo dice [0.7684]
2024-12-17 16:21:10.971621: Epoch time: 219.56 s
2024-12-17 16:21:12.503564: 
2024-12-17 16:21:12.504898: Epoch 79
2024-12-17 16:21:12.505761: Current learning rate: 0.0051
2024-12-17 16:24:55.202312: Validation loss did not improve from -0.56957. Patience: 21/50
2024-12-17 16:24:55.203002: train_loss -0.7685
2024-12-17 16:24:55.203836: val_loss -0.5341
2024-12-17 16:24:55.204624: Pseudo dice [0.7599]
2024-12-17 16:24:55.205433: Epoch time: 222.7 s
2024-12-17 16:24:57.061118: 
2024-12-17 16:24:57.062482: Epoch 80
2024-12-17 16:24:57.063289: Current learning rate: 0.00504
2024-12-17 16:28:45.172180: Validation loss did not improve from -0.56957. Patience: 22/50
2024-12-17 16:28:45.173072: train_loss -0.7702
2024-12-17 16:28:45.173903: val_loss -0.5529
2024-12-17 16:28:45.174658: Pseudo dice [0.7676]
2024-12-17 16:28:45.175347: Epoch time: 228.11 s
2024-12-17 16:28:46.621802: 
2024-12-17 16:28:46.623061: Epoch 81
2024-12-17 16:28:46.623849: Current learning rate: 0.00497
2024-12-17 16:33:03.973162: Validation loss did not improve from -0.56957. Patience: 23/50
2024-12-17 16:33:03.974293: train_loss -0.7691
2024-12-17 16:33:03.975137: val_loss -0.5139
2024-12-17 16:33:03.975783: Pseudo dice [0.7489]
2024-12-17 16:33:03.976499: Epoch time: 257.35 s
2024-12-17 16:33:05.394935: 
2024-12-17 16:33:05.396179: Epoch 82
2024-12-17 16:33:05.396910: Current learning rate: 0.00491
2024-12-17 16:36:35.530721: Validation loss did not improve from -0.56957. Patience: 24/50
2024-12-17 16:36:35.531730: train_loss -0.7686
2024-12-17 16:36:35.532540: val_loss -0.5483
2024-12-17 16:36:35.533350: Pseudo dice [0.7596]
2024-12-17 16:36:35.534092: Epoch time: 210.14 s
2024-12-17 16:36:37.279430: 
2024-12-17 16:36:37.280752: Epoch 83
2024-12-17 16:36:37.281575: Current learning rate: 0.00484
2024-12-17 16:40:22.690449: Validation loss did not improve from -0.56957. Patience: 25/50
2024-12-17 16:40:22.691464: train_loss -0.7697
2024-12-17 16:40:22.692229: val_loss -0.523
2024-12-17 16:40:22.692964: Pseudo dice [0.7539]
2024-12-17 16:40:22.693712: Epoch time: 225.41 s
2024-12-17 16:40:24.038420: 
2024-12-17 16:40:24.039464: Epoch 84
2024-12-17 16:40:24.040205: Current learning rate: 0.00478
2024-12-17 16:44:13.517475: Validation loss did not improve from -0.56957. Patience: 26/50
2024-12-17 16:44:13.518538: train_loss -0.7695
2024-12-17 16:44:13.519586: val_loss -0.4807
2024-12-17 16:44:13.520443: Pseudo dice [0.7343]
2024-12-17 16:44:13.521280: Epoch time: 229.48 s
2024-12-17 16:44:15.434148: 
2024-12-17 16:44:15.436757: Epoch 85
2024-12-17 16:44:15.437929: Current learning rate: 0.00471
2024-12-17 16:48:10.632972: Validation loss did not improve from -0.56957. Patience: 27/50
2024-12-17 16:48:10.633943: train_loss -0.769
2024-12-17 16:48:10.634869: val_loss -0.5208
2024-12-17 16:48:10.635659: Pseudo dice [0.7465]
2024-12-17 16:48:10.636387: Epoch time: 235.2 s
2024-12-17 16:48:12.047905: 
2024-12-17 16:48:12.049252: Epoch 86
2024-12-17 16:48:12.049964: Current learning rate: 0.00465
2024-12-17 16:52:02.413515: Validation loss did not improve from -0.56957. Patience: 28/50
2024-12-17 16:52:02.414611: train_loss -0.7724
2024-12-17 16:52:02.415298: val_loss -0.5224
2024-12-17 16:52:02.415982: Pseudo dice [0.7427]
2024-12-17 16:52:02.416689: Epoch time: 230.37 s
2024-12-17 16:52:03.798005: 
2024-12-17 16:52:03.799352: Epoch 87
2024-12-17 16:52:03.800166: Current learning rate: 0.00458
2024-12-17 16:55:59.151253: Validation loss did not improve from -0.56957. Patience: 29/50
2024-12-17 16:55:59.152144: train_loss -0.7722
2024-12-17 16:55:59.153082: val_loss -0.5485
2024-12-17 16:55:59.153944: Pseudo dice [0.7647]
2024-12-17 16:55:59.154843: Epoch time: 235.36 s
2024-12-17 16:56:00.595417: 
2024-12-17 16:56:00.596989: Epoch 88
2024-12-17 16:56:00.597914: Current learning rate: 0.00452
2024-12-17 16:59:49.336691: Validation loss did not improve from -0.56957. Patience: 30/50
2024-12-17 16:59:49.338802: train_loss -0.7736
2024-12-17 16:59:49.339741: val_loss -0.5253
2024-12-17 16:59:49.340463: Pseudo dice [0.7572]
2024-12-17 16:59:49.341167: Epoch time: 228.74 s
2024-12-17 16:59:50.730952: 
2024-12-17 16:59:50.732313: Epoch 89
2024-12-17 16:59:50.733011: Current learning rate: 0.00445
2024-12-17 17:03:36.302735: Validation loss did not improve from -0.56957. Patience: 31/50
2024-12-17 17:03:36.303939: train_loss -0.7783
2024-12-17 17:03:36.305134: val_loss -0.501
2024-12-17 17:03:36.306216: Pseudo dice [0.7485]
2024-12-17 17:03:36.307280: Epoch time: 225.57 s
2024-12-17 17:03:38.093606: 
2024-12-17 17:03:38.094995: Epoch 90
2024-12-17 17:03:38.095975: Current learning rate: 0.00438
2024-12-17 17:07:25.677392: Validation loss did not improve from -0.56957. Patience: 32/50
2024-12-17 17:07:25.678341: train_loss -0.7723
2024-12-17 17:07:25.679234: val_loss -0.5273
2024-12-17 17:07:25.680051: Pseudo dice [0.7584]
2024-12-17 17:07:25.680840: Epoch time: 227.59 s
2024-12-17 17:07:27.112944: 
2024-12-17 17:07:27.114207: Epoch 91
2024-12-17 17:07:27.114941: Current learning rate: 0.00432
2024-12-17 17:11:26.548278: Validation loss did not improve from -0.56957. Patience: 33/50
2024-12-17 17:11:26.549196: train_loss -0.776
2024-12-17 17:11:26.550105: val_loss -0.5171
2024-12-17 17:11:26.550837: Pseudo dice [0.7505]
2024-12-17 17:11:26.551674: Epoch time: 239.44 s
2024-12-17 17:11:27.976493: 
2024-12-17 17:11:27.977640: Epoch 92
2024-12-17 17:11:27.978474: Current learning rate: 0.00425
2024-12-17 17:15:33.958944: Validation loss did not improve from -0.56957. Patience: 34/50
2024-12-17 17:15:33.959982: train_loss -0.7732
2024-12-17 17:15:33.960934: val_loss -0.5386
2024-12-17 17:15:33.961627: Pseudo dice [0.7622]
2024-12-17 17:15:33.962447: Epoch time: 245.98 s
2024-12-17 17:15:35.378654: 
2024-12-17 17:15:35.380034: Epoch 93
2024-12-17 17:15:35.380851: Current learning rate: 0.00419
2024-12-17 17:19:48.973603: Validation loss did not improve from -0.56957. Patience: 35/50
2024-12-17 17:19:48.974532: train_loss -0.7772
2024-12-17 17:19:48.975309: val_loss -0.5536
2024-12-17 17:19:48.976050: Pseudo dice [0.7627]
2024-12-17 17:19:48.976754: Epoch time: 253.6 s
2024-12-17 17:19:50.367788: 
2024-12-17 17:19:50.369114: Epoch 94
2024-12-17 17:19:50.370139: Current learning rate: 0.00412
2024-12-17 17:23:37.008250: Validation loss did not improve from -0.56957. Patience: 36/50
2024-12-17 17:23:37.009288: train_loss -0.7771
2024-12-17 17:23:37.010269: val_loss -0.5508
2024-12-17 17:23:37.011098: Pseudo dice [0.7727]
2024-12-17 17:23:37.011810: Epoch time: 226.64 s
2024-12-17 17:23:39.558787: 
2024-12-17 17:23:39.560327: Epoch 95
2024-12-17 17:23:39.561468: Current learning rate: 0.00405
2024-12-17 17:27:26.059269: Validation loss did not improve from -0.56957. Patience: 37/50
2024-12-17 17:27:26.060498: train_loss -0.7764
2024-12-17 17:27:26.061456: val_loss -0.5289
2024-12-17 17:27:26.062215: Pseudo dice [0.7575]
2024-12-17 17:27:26.063004: Epoch time: 226.5 s
2024-12-17 17:27:27.434295: 
2024-12-17 17:27:27.435608: Epoch 96
2024-12-17 17:27:27.436446: Current learning rate: 0.00399
2024-12-17 17:31:19.194582: Validation loss did not improve from -0.56957. Patience: 38/50
2024-12-17 17:31:19.195586: train_loss -0.7796
2024-12-17 17:31:19.196790: val_loss -0.5349
2024-12-17 17:31:19.197705: Pseudo dice [0.7589]
2024-12-17 17:31:19.198675: Epoch time: 231.76 s
2024-12-17 17:31:20.681406: 
2024-12-17 17:31:20.682853: Epoch 97
2024-12-17 17:31:20.683803: Current learning rate: 0.00392
2024-12-17 17:35:26.127187: Validation loss did not improve from -0.56957. Patience: 39/50
2024-12-17 17:35:26.128186: train_loss -0.7761
2024-12-17 17:35:26.129000: val_loss -0.5525
2024-12-17 17:35:26.129792: Pseudo dice [0.767]
2024-12-17 17:35:26.130491: Epoch time: 245.45 s
2024-12-17 17:35:27.581209: 
2024-12-17 17:35:27.582587: Epoch 98
2024-12-17 17:35:27.583412: Current learning rate: 0.00385
2024-12-17 17:39:32.707459: Validation loss did not improve from -0.56957. Patience: 40/50
2024-12-17 17:39:32.708533: train_loss -0.78
2024-12-17 17:39:32.709511: val_loss -0.5161
2024-12-17 17:39:32.710374: Pseudo dice [0.7541]
2024-12-17 17:39:32.711172: Epoch time: 245.13 s
2024-12-17 17:39:34.315868: 
2024-12-17 17:39:34.317320: Epoch 99
2024-12-17 17:39:34.318337: Current learning rate: 0.00379
2024-12-17 17:43:47.595025: Validation loss did not improve from -0.56957. Patience: 41/50
2024-12-17 17:43:47.596143: train_loss -0.7824
2024-12-17 17:43:47.597315: val_loss -0.5458
2024-12-17 17:43:47.598370: Pseudo dice [0.7686]
2024-12-17 17:43:47.599395: Epoch time: 253.28 s
2024-12-17 17:43:49.471374: 
2024-12-17 17:43:49.472709: Epoch 100
2024-12-17 17:43:49.473592: Current learning rate: 0.00372
2024-12-17 17:48:06.337348: Validation loss did not improve from -0.56957. Patience: 42/50
2024-12-17 17:48:06.338405: train_loss -0.7846
2024-12-17 17:48:06.339169: val_loss -0.5378
2024-12-17 17:48:06.339897: Pseudo dice [0.7591]
2024-12-17 17:48:06.340594: Epoch time: 256.87 s
2024-12-17 17:48:07.759795: 
2024-12-17 17:48:07.761128: Epoch 101
2024-12-17 17:48:07.761945: Current learning rate: 0.00365
2024-12-17 17:52:14.187186: Validation loss did not improve from -0.56957. Patience: 43/50
2024-12-17 17:52:14.188172: train_loss -0.7824
2024-12-17 17:52:14.188974: val_loss -0.5319
2024-12-17 17:52:14.189707: Pseudo dice [0.7533]
2024-12-17 17:52:14.190518: Epoch time: 246.43 s
2024-12-17 17:52:15.628418: 
2024-12-17 17:52:15.629775: Epoch 102
2024-12-17 17:52:15.630535: Current learning rate: 0.00359
2024-12-17 17:56:17.700522: Validation loss did not improve from -0.56957. Patience: 44/50
2024-12-17 17:56:17.701552: train_loss -0.7833
2024-12-17 17:56:17.702454: val_loss -0.558
2024-12-17 17:56:17.703135: Pseudo dice [0.7648]
2024-12-17 17:56:17.703916: Epoch time: 242.07 s
2024-12-17 17:56:19.194245: 
2024-12-17 17:56:19.195512: Epoch 103
2024-12-17 17:56:19.196315: Current learning rate: 0.00352
2024-12-17 18:00:23.500719: Validation loss did not improve from -0.56957. Patience: 45/50
2024-12-17 18:00:23.501774: train_loss -0.7817
2024-12-17 18:00:23.502528: val_loss -0.526
2024-12-17 18:00:23.503215: Pseudo dice [0.7634]
2024-12-17 18:00:23.503945: Epoch time: 244.31 s
2024-12-17 18:00:23.504594: Yayy! New best EMA pseudo Dice: 0.7596
2024-12-17 18:00:25.326327: 
2024-12-17 18:00:25.327401: Epoch 104
2024-12-17 18:00:25.328119: Current learning rate: 0.00345
2024-12-17 18:04:16.284801: Validation loss did not improve from -0.56957. Patience: 46/50
2024-12-17 18:04:16.285829: train_loss -0.7844
2024-12-17 18:04:16.286824: val_loss -0.5331
2024-12-17 18:04:16.287618: Pseudo dice [0.7616]
2024-12-17 18:04:16.288475: Epoch time: 230.96 s
2024-12-17 18:04:16.686174: Yayy! New best EMA pseudo Dice: 0.7598
2024-12-17 18:04:18.510271: 
2024-12-17 18:04:18.511526: Epoch 105
2024-12-17 18:04:18.512302: Current learning rate: 0.00338
2024-12-17 18:08:09.801681: Validation loss did not improve from -0.56957. Patience: 47/50
2024-12-17 18:08:09.803230: train_loss -0.7871
2024-12-17 18:08:09.804075: val_loss -0.5312
2024-12-17 18:08:09.804866: Pseudo dice [0.7582]
2024-12-17 18:08:09.805622: Epoch time: 231.29 s
2024-12-17 18:08:12.028913: 
2024-12-17 18:08:12.030211: Epoch 106
2024-12-17 18:08:12.031053: Current learning rate: 0.00332
2024-12-17 18:12:10.354514: Validation loss did not improve from -0.56957. Patience: 48/50
2024-12-17 18:12:10.355375: train_loss -0.7857
2024-12-17 18:12:10.356353: val_loss -0.5331
2024-12-17 18:12:10.357347: Pseudo dice [0.7589]
2024-12-17 18:12:10.358262: Epoch time: 238.33 s
2024-12-17 18:12:11.771969: 
2024-12-17 18:12:11.773247: Epoch 107
2024-12-17 18:12:11.774302: Current learning rate: 0.00325
2024-12-17 18:15:59.418693: Validation loss did not improve from -0.56957. Patience: 49/50
2024-12-17 18:15:59.419500: train_loss -0.7866
2024-12-17 18:15:59.420539: val_loss -0.5462
2024-12-17 18:15:59.421434: Pseudo dice [0.7651]
2024-12-17 18:15:59.422280: Epoch time: 227.65 s
2024-12-17 18:15:59.423024: Yayy! New best EMA pseudo Dice: 0.7601
2024-12-17 18:16:01.318851: 
2024-12-17 18:16:01.320311: Epoch 108
2024-12-17 18:16:01.321142: Current learning rate: 0.00318
2024-12-17 18:19:56.955176: Validation loss did not improve from -0.56957. Patience: 50/50
2024-12-17 18:19:56.956339: train_loss -0.7874
2024-12-17 18:19:56.957139: val_loss -0.5306
2024-12-17 18:19:56.957837: Pseudo dice [0.7568]
2024-12-17 18:19:56.958562: Epoch time: 235.64 s
2024-12-17 18:19:58.443634: 
2024-12-17 18:19:58.444872: Epoch 109
2024-12-17 18:19:58.445647: Current learning rate: 0.00311
2024-12-17 18:23:59.954182: Validation loss did not improve from -0.56957. Patience: 51/50
2024-12-17 18:23:59.955252: train_loss -0.7894
2024-12-17 18:23:59.956276: val_loss -0.5595
2024-12-17 18:23:59.957265: Pseudo dice [0.7681]
2024-12-17 18:23:59.958266: Epoch time: 241.51 s
2024-12-17 18:24:00.398104: Yayy! New best EMA pseudo Dice: 0.7606
2024-12-17 18:24:02.320130: 
2024-12-17 18:24:02.321593: Epoch 110
2024-12-17 18:24:02.322655: Current learning rate: 0.00304
2024-12-17 18:27:57.676231: Validation loss did not improve from -0.56957. Patience: 52/50
2024-12-17 18:27:57.677094: train_loss -0.7869
2024-12-17 18:27:57.678025: val_loss -0.5293
2024-12-17 18:27:57.678829: Pseudo dice [0.7615]
2024-12-17 18:27:57.679910: Epoch time: 235.36 s
2024-12-17 18:27:57.680902: Yayy! New best EMA pseudo Dice: 0.7607
2024-12-17 18:27:59.585729: 
2024-12-17 18:27:59.587317: Epoch 111
2024-12-17 18:27:59.588342: Current learning rate: 0.00297
2024-12-17 18:32:07.369625: Validation loss did not improve from -0.56957. Patience: 53/50
2024-12-17 18:32:07.371003: train_loss -0.7903
2024-12-17 18:32:07.372117: val_loss -0.498
2024-12-17 18:32:07.372836: Pseudo dice [0.7385]
2024-12-17 18:32:07.373656: Epoch time: 247.79 s
2024-12-17 18:32:08.959253: 
2024-12-17 18:32:08.960612: Epoch 112
2024-12-17 18:32:08.961479: Current learning rate: 0.00291
2024-12-17 18:36:07.998157: Validation loss did not improve from -0.56957. Patience: 54/50
2024-12-17 18:36:07.999207: train_loss -0.7886
2024-12-17 18:36:08.000162: val_loss -0.5373
2024-12-17 18:36:08.001004: Pseudo dice [0.7596]
2024-12-17 18:36:08.001837: Epoch time: 239.04 s
2024-12-17 18:36:09.440914: 
2024-12-17 18:36:09.442636: Epoch 113
2024-12-17 18:36:09.443828: Current learning rate: 0.00284
2024-12-17 18:40:06.426590: Validation loss did not improve from -0.56957. Patience: 55/50
2024-12-17 18:40:06.427759: train_loss -0.7869
2024-12-17 18:40:06.428689: val_loss -0.5184
2024-12-17 18:40:06.429460: Pseudo dice [0.7571]
2024-12-17 18:40:06.430327: Epoch time: 236.99 s
2024-12-17 18:40:07.887574: 
2024-12-17 18:40:07.888770: Epoch 114
2024-12-17 18:40:07.889568: Current learning rate: 0.00277
2024-12-17 18:44:00.128763: Validation loss did not improve from -0.56957. Patience: 56/50
2024-12-17 18:44:00.129754: train_loss -0.7925
2024-12-17 18:44:00.130518: val_loss -0.5236
2024-12-17 18:44:00.131197: Pseudo dice [0.7517]
2024-12-17 18:44:00.131864: Epoch time: 232.24 s
2024-12-17 18:44:01.992649: 
2024-12-17 18:44:01.993945: Epoch 115
2024-12-17 18:44:01.994770: Current learning rate: 0.0027
2024-12-17 18:47:54.661369: Validation loss did not improve from -0.56957. Patience: 57/50
2024-12-17 18:47:54.662428: train_loss -0.7929
2024-12-17 18:47:54.663542: val_loss -0.5198
2024-12-17 18:47:54.664603: Pseudo dice [0.7557]
2024-12-17 18:47:54.665658: Epoch time: 232.67 s
2024-12-17 18:47:56.139510: 
2024-12-17 18:47:56.141089: Epoch 116
2024-12-17 18:47:56.142247: Current learning rate: 0.00263
2024-12-17 18:51:57.685359: Validation loss did not improve from -0.56957. Patience: 58/50
2024-12-17 18:51:57.686483: train_loss -0.789
2024-12-17 18:51:57.687674: val_loss -0.5208
2024-12-17 18:51:57.688704: Pseudo dice [0.7607]
2024-12-17 18:51:57.689779: Epoch time: 241.55 s
2024-12-17 18:51:59.625065: 
2024-12-17 18:51:59.626568: Epoch 117
2024-12-17 18:51:59.627703: Current learning rate: 0.00256
2024-12-17 18:55:52.275059: Validation loss did not improve from -0.56957. Patience: 59/50
2024-12-17 18:55:52.276052: train_loss -0.793
2024-12-17 18:55:52.276881: val_loss -0.5313
2024-12-17 18:55:52.277743: Pseudo dice [0.7561]
2024-12-17 18:55:52.278750: Epoch time: 232.65 s
2024-12-17 18:55:53.720267: 
2024-12-17 18:55:53.721480: Epoch 118
2024-12-17 18:55:53.722241: Current learning rate: 0.00249
2024-12-17 18:59:51.084641: Validation loss did not improve from -0.56957. Patience: 60/50
2024-12-17 18:59:51.085629: train_loss -0.7935
2024-12-17 18:59:51.086641: val_loss -0.5123
2024-12-17 18:59:51.087446: Pseudo dice [0.7548]
2024-12-17 18:59:51.088245: Epoch time: 237.37 s
2024-12-17 18:59:52.585763: 
2024-12-17 18:59:52.587192: Epoch 119
2024-12-17 18:59:52.587997: Current learning rate: 0.00242
2024-12-17 19:03:55.817613: Validation loss did not improve from -0.56957. Patience: 61/50
2024-12-17 19:03:55.818610: train_loss -0.7939
2024-12-17 19:03:55.819529: val_loss -0.5217
2024-12-17 19:03:55.820473: Pseudo dice [0.7586]
2024-12-17 19:03:55.821336: Epoch time: 243.23 s
2024-12-17 19:03:57.782305: 
2024-12-17 19:03:57.783714: Epoch 120
2024-12-17 19:03:57.784822: Current learning rate: 0.00235
2024-12-17 19:07:47.626717: Validation loss did not improve from -0.56957. Patience: 62/50
2024-12-17 19:07:47.627845: train_loss -0.7952
2024-12-17 19:07:47.628932: val_loss -0.5654
2024-12-17 19:07:47.629935: Pseudo dice [0.7754]
2024-12-17 19:07:47.630794: Epoch time: 229.85 s
2024-12-17 19:07:49.201831: 
2024-12-17 19:07:49.203189: Epoch 121
2024-12-17 19:07:49.203943: Current learning rate: 0.00228
2024-12-17 19:11:51.371515: Validation loss did not improve from -0.56957. Patience: 63/50
2024-12-17 19:11:51.401428: train_loss -0.7931
2024-12-17 19:11:51.402472: val_loss -0.5192
2024-12-17 19:11:51.403359: Pseudo dice [0.7553]
2024-12-17 19:11:51.404313: Epoch time: 242.2 s
2024-12-17 19:11:52.969089: 
2024-12-17 19:11:52.970494: Epoch 122
2024-12-17 19:11:52.971365: Current learning rate: 0.00221
2024-12-17 19:16:00.174492: Validation loss did not improve from -0.56957. Patience: 64/50
2024-12-17 19:16:00.175788: train_loss -0.7923
2024-12-17 19:16:00.176766: val_loss -0.5329
2024-12-17 19:16:00.177490: Pseudo dice [0.7603]
2024-12-17 19:16:00.178212: Epoch time: 247.21 s
2024-12-17 19:16:01.652173: 
2024-12-17 19:16:01.653457: Epoch 123
2024-12-17 19:16:01.654236: Current learning rate: 0.00214
2024-12-17 19:20:07.245150: Validation loss did not improve from -0.56957. Patience: 65/50
2024-12-17 19:20:07.246387: train_loss -0.7934
2024-12-17 19:20:07.247257: val_loss -0.5322
2024-12-17 19:20:07.248020: Pseudo dice [0.7562]
2024-12-17 19:20:07.248870: Epoch time: 245.6 s
2024-12-17 19:20:08.689357: 
2024-12-17 19:20:08.690749: Epoch 124
2024-12-17 19:20:08.691561: Current learning rate: 0.00207
2024-12-17 19:24:09.168159: Validation loss did not improve from -0.56957. Patience: 66/50
2024-12-17 19:24:09.169101: train_loss -0.7962
2024-12-17 19:24:09.169969: val_loss -0.538
2024-12-17 19:24:09.170822: Pseudo dice [0.7651]
2024-12-17 19:24:09.171554: Epoch time: 240.48 s
2024-12-17 19:24:11.046430: 
2024-12-17 19:24:11.047858: Epoch 125
2024-12-17 19:24:11.048677: Current learning rate: 0.00199
2024-12-17 19:28:12.573827: Validation loss did not improve from -0.56957. Patience: 67/50
2024-12-17 19:28:12.574860: train_loss -0.7955
2024-12-17 19:28:12.575699: val_loss -0.531
2024-12-17 19:28:12.576481: Pseudo dice [0.7642]
2024-12-17 19:28:12.577204: Epoch time: 241.53 s
2024-12-17 19:28:14.056231: 
2024-12-17 19:28:14.057559: Epoch 126
2024-12-17 19:28:14.058348: Current learning rate: 0.00192
2024-12-17 19:32:08.755712: Validation loss did not improve from -0.56957. Patience: 68/50
2024-12-17 19:32:08.756800: train_loss -0.798
2024-12-17 19:32:08.757829: val_loss -0.5496
2024-12-17 19:32:08.758581: Pseudo dice [0.7624]
2024-12-17 19:32:08.759345: Epoch time: 234.7 s
2024-12-17 19:32:10.245548: 
2024-12-17 19:32:10.247078: Epoch 127
2024-12-17 19:32:10.248295: Current learning rate: 0.00185
2024-12-17 19:35:56.068120: Validation loss did not improve from -0.56957. Patience: 69/50
2024-12-17 19:35:56.069253: train_loss -0.7977
2024-12-17 19:35:56.070205: val_loss -0.5267
2024-12-17 19:35:56.070986: Pseudo dice [0.7593]
2024-12-17 19:35:56.071698: Epoch time: 225.82 s
2024-12-17 19:35:58.018687: 
2024-12-17 19:35:58.020020: Epoch 128
2024-12-17 19:35:58.020771: Current learning rate: 0.00178
2024-12-17 19:39:56.818091: Validation loss did not improve from -0.56957. Patience: 70/50
2024-12-17 19:39:56.819052: train_loss -0.7966
2024-12-17 19:39:56.819959: val_loss -0.5246
2024-12-17 19:39:56.820843: Pseudo dice [0.759]
2024-12-17 19:39:56.821798: Epoch time: 238.8 s
2024-12-17 19:39:58.256156: 
2024-12-17 19:39:58.257495: Epoch 129
2024-12-17 19:39:58.258328: Current learning rate: 0.0017
2024-12-17 19:44:06.313640: Validation loss did not improve from -0.56957. Patience: 71/50
2024-12-17 19:44:06.314660: train_loss -0.7973
2024-12-17 19:44:06.315506: val_loss -0.5324
2024-12-17 19:44:06.316294: Pseudo dice [0.753]
2024-12-17 19:44:06.317132: Epoch time: 248.06 s
2024-12-17 19:44:08.257865: 
2024-12-17 19:44:08.259420: Epoch 130
2024-12-17 19:44:08.260561: Current learning rate: 0.00163
2024-12-17 19:48:03.560192: Validation loss did not improve from -0.56957. Patience: 72/50
2024-12-17 19:48:03.561326: train_loss -0.7994
2024-12-17 19:48:03.562395: val_loss -0.523
2024-12-17 19:48:03.563320: Pseudo dice [0.7527]
2024-12-17 19:48:03.564243: Epoch time: 235.3 s
2024-12-17 19:48:05.047766: 
2024-12-17 19:48:05.049194: Epoch 131
2024-12-17 19:48:05.050227: Current learning rate: 0.00156
2024-12-17 19:52:20.486390: Validation loss did not improve from -0.56957. Patience: 73/50
2024-12-17 19:52:20.487528: train_loss -0.8
2024-12-17 19:52:20.488860: val_loss -0.5326
2024-12-17 19:52:20.489899: Pseudo dice [0.7631]
2024-12-17 19:52:20.491066: Epoch time: 255.44 s
2024-12-17 19:52:21.951441: 
2024-12-17 19:52:21.952981: Epoch 132
2024-12-17 19:52:21.954029: Current learning rate: 0.00148
2024-12-17 19:56:33.393892: Validation loss did not improve from -0.56957. Patience: 74/50
2024-12-17 19:56:33.394969: train_loss -0.797
2024-12-17 19:56:33.395864: val_loss -0.5307
2024-12-17 19:56:33.396555: Pseudo dice [0.7605]
2024-12-17 19:56:33.397246: Epoch time: 251.44 s
2024-12-17 19:56:34.897151: 
2024-12-17 19:56:34.898724: Epoch 133
2024-12-17 19:56:34.899613: Current learning rate: 0.00141
2024-12-17 20:00:51.689385: Validation loss did not improve from -0.56957. Patience: 75/50
2024-12-17 20:00:51.690591: train_loss -0.7984
2024-12-17 20:00:51.691645: val_loss -0.4945
2024-12-17 20:00:51.692458: Pseudo dice [0.7506]
2024-12-17 20:00:51.693228: Epoch time: 256.79 s
2024-12-17 20:00:53.235502: 
2024-12-17 20:00:53.236960: Epoch 134
2024-12-17 20:00:53.237892: Current learning rate: 0.00133
2024-12-17 20:05:01.648278: Validation loss did not improve from -0.56957. Patience: 76/50
2024-12-17 20:05:01.649393: train_loss -0.7986
2024-12-17 20:05:01.650393: val_loss -0.5258
2024-12-17 20:05:01.651394: Pseudo dice [0.7633]
2024-12-17 20:05:01.652400: Epoch time: 248.42 s
2024-12-17 20:05:03.696562: 
2024-12-17 20:05:03.698049: Epoch 135
2024-12-17 20:05:03.699095: Current learning rate: 0.00126
2024-12-17 20:08:56.956961: Validation loss did not improve from -0.56957. Patience: 77/50
2024-12-17 20:08:56.957956: train_loss -0.7995
2024-12-17 20:08:56.959092: val_loss -0.5219
2024-12-17 20:08:56.960150: Pseudo dice [0.7576]
2024-12-17 20:08:56.961060: Epoch time: 233.26 s
2024-12-17 20:08:58.433743: 
2024-12-17 20:08:58.435364: Epoch 136
2024-12-17 20:08:58.436481: Current learning rate: 0.00118
2024-12-17 20:12:51.148921: Validation loss did not improve from -0.56957. Patience: 78/50
2024-12-17 20:12:51.149892: train_loss -0.8006
2024-12-17 20:12:51.150742: val_loss -0.5346
2024-12-17 20:12:51.151520: Pseudo dice [0.7551]
2024-12-17 20:12:51.152301: Epoch time: 232.72 s
2024-12-17 20:12:52.647909: 
2024-12-17 20:12:52.649374: Epoch 137
2024-12-17 20:12:52.650189: Current learning rate: 0.00111
2024-12-17 20:16:50.273559: Validation loss did not improve from -0.56957. Patience: 79/50
2024-12-17 20:16:50.277900: train_loss -0.8018
2024-12-17 20:16:50.279922: val_loss -0.5252
2024-12-17 20:16:50.280983: Pseudo dice [0.7609]
2024-12-17 20:16:50.282202: Epoch time: 237.63 s
2024-12-17 20:16:51.868148: 
2024-12-17 20:16:51.869652: Epoch 138
2024-12-17 20:16:51.870594: Current learning rate: 0.00103
2024-12-17 20:20:49.555598: Validation loss did not improve from -0.56957. Patience: 80/50
2024-12-17 20:20:49.557601: train_loss -0.805
2024-12-17 20:20:49.558794: val_loss -0.5181
2024-12-17 20:20:49.559607: Pseudo dice [0.7564]
2024-12-17 20:20:49.560372: Epoch time: 237.69 s
2024-12-17 20:20:51.835396: 
2024-12-17 20:20:51.836989: Epoch 139
2024-12-17 20:20:51.837995: Current learning rate: 0.00095
2024-12-17 20:25:01.411552: Validation loss did not improve from -0.56957. Patience: 81/50
2024-12-17 20:25:01.412499: train_loss -0.8004
2024-12-17 20:25:01.413569: val_loss -0.5311
2024-12-17 20:25:01.414587: Pseudo dice [0.7583]
2024-12-17 20:25:01.415510: Epoch time: 249.58 s
2024-12-17 20:25:03.352335: 
2024-12-17 20:25:03.353537: Epoch 140
2024-12-17 20:25:03.354388: Current learning rate: 0.00087
2024-12-17 20:29:13.712634: Validation loss did not improve from -0.56957. Patience: 82/50
2024-12-17 20:29:13.713523: train_loss -0.8027
2024-12-17 20:29:13.714529: val_loss -0.5338
2024-12-17 20:29:13.715359: Pseudo dice [0.767]
2024-12-17 20:29:13.716259: Epoch time: 250.36 s
2024-12-17 20:29:15.244988: 
2024-12-17 20:29:15.246367: Epoch 141
2024-12-17 20:29:15.247240: Current learning rate: 0.00079
2024-12-17 20:33:10.168800: Validation loss did not improve from -0.56957. Patience: 83/50
2024-12-17 20:33:10.169828: train_loss -0.7991
2024-12-17 20:33:10.170651: val_loss -0.5121
2024-12-17 20:33:10.171365: Pseudo dice [0.7513]
2024-12-17 20:33:10.172177: Epoch time: 234.93 s
2024-12-17 20:33:11.700513: 
2024-12-17 20:33:11.701856: Epoch 142
2024-12-17 20:33:11.702600: Current learning rate: 0.00071
2024-12-17 20:37:16.434611: Validation loss did not improve from -0.56957. Patience: 84/50
2024-12-17 20:37:16.435612: train_loss -0.8022
2024-12-17 20:37:16.436692: val_loss -0.5329
2024-12-17 20:37:16.437799: Pseudo dice [0.7605]
2024-12-17 20:37:16.438825: Epoch time: 244.74 s
2024-12-17 20:37:17.909442: 
2024-12-17 20:37:17.910764: Epoch 143
2024-12-17 20:37:17.911780: Current learning rate: 0.00063
2024-12-17 20:41:04.827255: Validation loss did not improve from -0.56957. Patience: 85/50
2024-12-17 20:41:04.828547: train_loss -0.8017
2024-12-17 20:41:04.829979: val_loss -0.5251
2024-12-17 20:41:04.830913: Pseudo dice [0.7603]
2024-12-17 20:41:04.831769: Epoch time: 226.92 s
2024-12-17 20:41:06.303751: 
2024-12-17 20:41:06.305291: Epoch 144
2024-12-17 20:41:06.306327: Current learning rate: 0.00055
2024-12-17 20:44:55.401395: Validation loss did not improve from -0.56957. Patience: 86/50
2024-12-17 20:44:55.402370: train_loss -0.8032
2024-12-17 20:44:55.403251: val_loss -0.5426
2024-12-17 20:44:55.404004: Pseudo dice [0.7758]
2024-12-17 20:44:55.404802: Epoch time: 229.1 s
2024-12-17 20:44:57.348720: 
2024-12-17 20:44:57.350056: Epoch 145
2024-12-17 20:44:57.350837: Current learning rate: 0.00047
2024-12-17 20:48:43.961653: Validation loss did not improve from -0.56957. Patience: 87/50
2024-12-17 20:48:43.962661: train_loss -0.8048
2024-12-17 20:48:43.963598: val_loss -0.5221
2024-12-17 20:48:43.964591: Pseudo dice [0.7607]
2024-12-17 20:48:43.965262: Epoch time: 226.62 s
2024-12-17 20:48:45.423998: 
2024-12-17 20:48:45.425252: Epoch 146
2024-12-17 20:48:45.426049: Current learning rate: 0.00038
2024-12-17 20:52:23.965643: Validation loss did not improve from -0.56957. Patience: 88/50
2024-12-17 20:52:23.966631: train_loss -0.7998
2024-12-17 20:52:23.967441: val_loss -0.5265
2024-12-17 20:52:23.968086: Pseudo dice [0.7621]
2024-12-17 20:52:23.968798: Epoch time: 218.54 s
2024-12-17 20:52:25.476241: 
2024-12-17 20:52:25.477652: Epoch 147
2024-12-17 20:52:25.478486: Current learning rate: 0.0003
2024-12-17 20:56:21.559108: Validation loss did not improve from -0.56957. Patience: 89/50
2024-12-17 20:56:21.560136: train_loss -0.8039
2024-12-17 20:56:21.561141: val_loss -0.5287
2024-12-17 20:56:21.562184: Pseudo dice [0.7516]
2024-12-17 20:56:21.563187: Epoch time: 236.09 s
2024-12-17 20:56:23.078194: 
2024-12-17 20:56:23.079766: Epoch 148
2024-12-17 20:56:23.080823: Current learning rate: 0.00021
2024-12-17 21:00:12.544461: Validation loss did not improve from -0.56957. Patience: 90/50
2024-12-17 21:00:12.545794: train_loss -0.8044
2024-12-17 21:00:12.546755: val_loss -0.5365
2024-12-17 21:00:12.547643: Pseudo dice [0.7596]
2024-12-17 21:00:12.548421: Epoch time: 229.47 s
2024-12-17 21:00:14.140375: 
2024-12-17 21:00:14.141745: Epoch 149
2024-12-17 21:00:14.142746: Current learning rate: 0.00011
2024-12-17 21:04:12.495801: Validation loss did not improve from -0.56957. Patience: 91/50
2024-12-17 21:04:12.496771: train_loss -0.8048
2024-12-17 21:04:12.497772: val_loss -0.5439
2024-12-17 21:04:12.498758: Pseudo dice [0.7633]
2024-12-17 21:04:12.499649: Epoch time: 238.36 s
2024-12-17 21:04:14.970610: Training done.
2024-12-17 21:04:15.499146: Using splits from existing split file: /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset309_Sohee_Calcium_OCT_CrossValidation/splits_final_80.json
2024-12-17 21:04:15.501499: The split file contains 5 splits.
2024-12-17 21:04:15.502247: Desired fold for training: 1
2024-12-17 21:04:15.502880: This split has 6 training and 4 validation cases.
2024-12-17 21:04:15.503824: predicting 106-002
2024-12-17 21:04:15.634198: 106-002, shape torch.Size([1, 539, 498, 498]), rank 0
2024-12-17 21:07:59.881884: predicting 401-004
2024-12-17 21:07:59.909966: 401-004, shape torch.Size([1, 375, 498, 498]), rank 0
2024-12-17 21:10:12.346153: predicting 704-003
2024-12-17 21:10:12.365650: 704-003, shape torch.Size([1, 375, 498, 498]), rank 0
2024-12-17 21:12:02.581055: predicting 706-005
2024-12-17 21:12:02.598943: 706-005, shape torch.Size([1, 375, 498, 498]), rank 0
2024-12-17 21:14:29.026068: Validation complete
2024-12-17 21:14:29.026694: Mean Validation Dice:  0.7521540854724273
2024-12-17 12:39:40.111612: unpacking done...
2024-12-17 12:39:40.190917: Unable to plot network architecture: nnUNet_compile is enabled!
2024-12-17 12:39:40.292614: 
2024-12-17 12:39:40.294395: Epoch 0
2024-12-17 12:39:40.295759: Current learning rate: 0.01
2024-12-17 12:42:45.049136: Validation loss improved from 1000.00000 to -0.40328! Patience: 0/50
2024-12-17 12:42:45.050205: train_loss -0.3172
2024-12-17 12:42:45.051088: val_loss -0.4033
2024-12-17 12:42:45.051728: Pseudo dice [0.6725]
2024-12-17 12:42:45.052423: Epoch time: 184.76 s
2024-12-17 12:42:45.053072: Yayy! New best EMA pseudo Dice: 0.6725
2024-12-17 12:42:47.220880: 
2024-12-17 12:42:47.222601: Epoch 1
2024-12-17 12:42:47.223578: Current learning rate: 0.00994
2024-12-17 12:44:17.711806: Validation loss improved from -0.40328 to -0.44792! Patience: 0/50
2024-12-17 12:44:17.712909: train_loss -0.511
2024-12-17 12:44:17.713916: val_loss -0.4479
2024-12-17 12:44:17.714757: Pseudo dice [0.6904]
2024-12-17 12:44:17.715890: Epoch time: 90.5 s
2024-12-17 12:44:17.716810: Yayy! New best EMA pseudo Dice: 0.6743
2024-12-17 12:44:19.568261: 
2024-12-17 12:44:19.569946: Epoch 2
2024-12-17 12:44:19.571362: Current learning rate: 0.00988
2024-12-17 12:45:52.003477: Validation loss did not improve from -0.44792. Patience: 1/50
2024-12-17 12:45:52.004646: train_loss -0.5478
2024-12-17 12:45:52.005676: val_loss -0.4478
2024-12-17 12:45:52.006539: Pseudo dice [0.6916]
2024-12-17 12:45:52.007342: Epoch time: 92.44 s
2024-12-17 12:45:52.008152: Yayy! New best EMA pseudo Dice: 0.6761
2024-12-17 12:45:53.934988: 
2024-12-17 12:45:53.936298: Epoch 3
2024-12-17 12:45:53.937747: Current learning rate: 0.00982
2024-12-17 12:47:26.409490: Validation loss improved from -0.44792 to -0.45628! Patience: 1/50
2024-12-17 12:47:26.410392: train_loss -0.5697
2024-12-17 12:47:26.411443: val_loss -0.4563
2024-12-17 12:47:26.412224: Pseudo dice [0.6893]
2024-12-17 12:47:26.412975: Epoch time: 92.48 s
2024-12-17 12:47:26.413625: Yayy! New best EMA pseudo Dice: 0.6774
2024-12-17 12:47:28.381321: 
2024-12-17 12:47:28.382668: Epoch 4
2024-12-17 12:47:28.383670: Current learning rate: 0.00976
2024-12-17 12:49:02.772310: Validation loss improved from -0.45628 to -0.50343! Patience: 0/50
2024-12-17 12:49:02.773377: train_loss -0.5818
2024-12-17 12:49:02.774682: val_loss -0.5034
2024-12-17 12:49:02.775630: Pseudo dice [0.7259]
2024-12-17 12:49:02.776715: Epoch time: 94.39 s
2024-12-17 12:49:03.166102: Yayy! New best EMA pseudo Dice: 0.6822
2024-12-17 12:49:05.057658: 
2024-12-17 12:49:05.059278: Epoch 5
2024-12-17 12:49:05.060573: Current learning rate: 0.0097
2024-12-17 12:50:41.795568: Validation loss did not improve from -0.50343. Patience: 1/50
2024-12-17 12:50:41.796660: train_loss -0.5804
2024-12-17 12:50:41.797555: val_loss -0.4936
2024-12-17 12:50:41.798433: Pseudo dice [0.7242]
2024-12-17 12:50:41.799398: Epoch time: 96.74 s
2024-12-17 12:50:41.800149: Yayy! New best EMA pseudo Dice: 0.6864
2024-12-17 12:50:43.600033: 
2024-12-17 12:50:43.601450: Epoch 6
2024-12-17 12:50:43.602545: Current learning rate: 0.00964
2024-12-17 12:52:18.706834: Validation loss improved from -0.50343 to -0.51224! Patience: 1/50
2024-12-17 12:52:18.707831: train_loss -0.609
2024-12-17 12:52:18.708683: val_loss -0.5122
2024-12-17 12:52:18.709520: Pseudo dice [0.7284]
2024-12-17 12:52:18.710320: Epoch time: 95.11 s
2024-12-17 12:52:18.711192: Yayy! New best EMA pseudo Dice: 0.6906
2024-12-17 12:52:20.565353: 
2024-12-17 12:52:20.566679: Epoch 7
2024-12-17 12:52:20.567478: Current learning rate: 0.00958
2024-12-17 12:54:01.001829: Validation loss improved from -0.51224 to -0.51376! Patience: 0/50
2024-12-17 12:54:01.002792: train_loss -0.6318
2024-12-17 12:54:01.003778: val_loss -0.5138
2024-12-17 12:54:01.004538: Pseudo dice [0.7418]
2024-12-17 12:54:01.005344: Epoch time: 100.44 s
2024-12-17 12:54:01.006000: Yayy! New best EMA pseudo Dice: 0.6957
2024-12-17 12:54:03.675800: 
2024-12-17 12:54:03.677394: Epoch 8
2024-12-17 12:54:03.678255: Current learning rate: 0.00952
2024-12-17 12:55:43.030534: Validation loss did not improve from -0.51376. Patience: 1/50
2024-12-17 12:55:43.031483: train_loss -0.635
2024-12-17 12:55:43.032326: val_loss -0.5055
2024-12-17 12:55:43.033027: Pseudo dice [0.7225]
2024-12-17 12:55:43.033746: Epoch time: 99.36 s
2024-12-17 12:55:43.034482: Yayy! New best EMA pseudo Dice: 0.6984
2024-12-17 12:55:44.956685: 
2024-12-17 12:55:44.958191: Epoch 9
2024-12-17 12:55:44.959113: Current learning rate: 0.00946
2024-12-17 12:57:24.932537: Validation loss did not improve from -0.51376. Patience: 2/50
2024-12-17 12:57:24.933623: train_loss -0.6452
2024-12-17 12:57:24.934592: val_loss -0.4477
2024-12-17 12:57:24.935425: Pseudo dice [0.699]
2024-12-17 12:57:24.936208: Epoch time: 99.98 s
2024-12-17 12:57:25.371023: Yayy! New best EMA pseudo Dice: 0.6985
2024-12-17 12:57:27.210269: 
2024-12-17 12:57:27.211724: Epoch 10
2024-12-17 12:57:27.212534: Current learning rate: 0.0094
2024-12-17 12:59:06.811154: Validation loss did not improve from -0.51376. Patience: 3/50
2024-12-17 12:59:06.812104: train_loss -0.6402
2024-12-17 12:59:06.813196: val_loss -0.4791
2024-12-17 12:59:06.813943: Pseudo dice [0.7092]
2024-12-17 12:59:06.814690: Epoch time: 99.6 s
2024-12-17 12:59:06.815439: Yayy! New best EMA pseudo Dice: 0.6995
2024-12-17 12:59:08.662671: 
2024-12-17 12:59:08.663991: Epoch 11
2024-12-17 12:59:08.664668: Current learning rate: 0.00934
2024-12-17 13:00:53.766054: Validation loss did not improve from -0.51376. Patience: 4/50
2024-12-17 13:00:53.767104: train_loss -0.6542
2024-12-17 13:00:53.768263: val_loss -0.4648
2024-12-17 13:00:53.769131: Pseudo dice [0.6986]
2024-12-17 13:00:53.769950: Epoch time: 105.11 s
2024-12-17 13:00:55.203994: 
2024-12-17 13:00:55.205104: Epoch 12
2024-12-17 13:00:55.205844: Current learning rate: 0.00928
2024-12-17 13:02:35.896657: Validation loss did not improve from -0.51376. Patience: 5/50
2024-12-17 13:02:35.897714: train_loss -0.6623
2024-12-17 13:02:35.898614: val_loss -0.5062
2024-12-17 13:02:35.899348: Pseudo dice [0.7265]
2024-12-17 13:02:35.900074: Epoch time: 100.7 s
2024-12-17 13:02:35.900806: Yayy! New best EMA pseudo Dice: 0.7021
2024-12-17 13:02:37.836912: 
2024-12-17 13:02:37.838531: Epoch 13
2024-12-17 13:02:37.839628: Current learning rate: 0.00922
2024-12-17 13:04:20.838413: Validation loss did not improve from -0.51376. Patience: 6/50
2024-12-17 13:04:20.839395: train_loss -0.6683
2024-12-17 13:04:20.840268: val_loss -0.4587
2024-12-17 13:04:20.841114: Pseudo dice [0.7059]
2024-12-17 13:04:20.842102: Epoch time: 103.0 s
2024-12-17 13:04:20.842978: Yayy! New best EMA pseudo Dice: 0.7025
2024-12-17 13:04:22.728996: 
2024-12-17 13:04:22.730365: Epoch 14
2024-12-17 13:04:22.731397: Current learning rate: 0.00916
2024-12-17 13:06:04.579980: Validation loss did not improve from -0.51376. Patience: 7/50
2024-12-17 13:06:04.581005: train_loss -0.6662
2024-12-17 13:06:04.581924: val_loss -0.4789
2024-12-17 13:06:04.582891: Pseudo dice [0.7113]
2024-12-17 13:06:04.583705: Epoch time: 101.85 s
2024-12-17 13:06:05.023452: Yayy! New best EMA pseudo Dice: 0.7034
2024-12-17 13:06:06.917489: 
2024-12-17 13:06:06.918783: Epoch 15
2024-12-17 13:06:06.919689: Current learning rate: 0.0091
2024-12-17 13:07:53.729276: Validation loss improved from -0.51376 to -0.51555! Patience: 7/50
2024-12-17 13:07:53.730300: train_loss -0.6742
2024-12-17 13:07:53.731215: val_loss -0.5156
2024-12-17 13:07:53.732215: Pseudo dice [0.7419]
2024-12-17 13:07:53.733049: Epoch time: 106.81 s
2024-12-17 13:07:53.733987: Yayy! New best EMA pseudo Dice: 0.7072
2024-12-17 13:07:55.632123: 
2024-12-17 13:07:55.633557: Epoch 16
2024-12-17 13:07:55.634509: Current learning rate: 0.00903
2024-12-17 13:09:37.472312: Validation loss did not improve from -0.51555. Patience: 1/50
2024-12-17 13:09:37.473312: train_loss -0.686
2024-12-17 13:09:37.474371: val_loss -0.4903
2024-12-17 13:09:37.475203: Pseudo dice [0.7251]
2024-12-17 13:09:37.476015: Epoch time: 101.84 s
2024-12-17 13:09:37.476939: Yayy! New best EMA pseudo Dice: 0.709
2024-12-17 13:09:39.409886: 
2024-12-17 13:09:39.411399: Epoch 17
2024-12-17 13:09:39.412662: Current learning rate: 0.00897
2024-12-17 13:11:24.774879: Validation loss did not improve from -0.51555. Patience: 2/50
2024-12-17 13:11:24.775980: train_loss -0.6884
2024-12-17 13:11:24.776884: val_loss -0.4652
2024-12-17 13:11:24.777646: Pseudo dice [0.7047]
2024-12-17 13:11:24.778425: Epoch time: 105.37 s
2024-12-17 13:11:26.866616: 
2024-12-17 13:11:26.867996: Epoch 18
2024-12-17 13:11:26.870254: Current learning rate: 0.00891
2024-12-17 13:13:11.328158: Validation loss did not improve from -0.51555. Patience: 3/50
2024-12-17 13:13:11.329341: train_loss -0.6868
2024-12-17 13:13:11.330308: val_loss -0.4998
2024-12-17 13:13:11.330972: Pseudo dice [0.7243]
2024-12-17 13:13:11.331807: Epoch time: 104.46 s
2024-12-17 13:13:11.332605: Yayy! New best EMA pseudo Dice: 0.7102
2024-12-17 13:13:13.248316: 
2024-12-17 13:13:13.249850: Epoch 19
2024-12-17 13:13:13.250695: Current learning rate: 0.00885
2024-12-17 13:15:01.461680: Validation loss improved from -0.51555 to -0.53718! Patience: 3/50
2024-12-17 13:15:01.462866: train_loss -0.6868
2024-12-17 13:15:01.463924: val_loss -0.5372
2024-12-17 13:15:01.465030: Pseudo dice [0.7495]
2024-12-17 13:15:01.466055: Epoch time: 108.22 s
2024-12-17 13:15:01.893328: Yayy! New best EMA pseudo Dice: 0.7141
2024-12-17 13:15:03.819380: 
2024-12-17 13:15:03.821668: Epoch 20
2024-12-17 13:15:03.822901: Current learning rate: 0.00879
2024-12-17 13:16:48.747048: Validation loss did not improve from -0.53718. Patience: 1/50
2024-12-17 13:16:48.748046: train_loss -0.698
2024-12-17 13:16:48.748976: val_loss -0.5123
2024-12-17 13:16:48.749876: Pseudo dice [0.7287]
2024-12-17 13:16:48.750762: Epoch time: 104.93 s
2024-12-17 13:16:48.751666: Yayy! New best EMA pseudo Dice: 0.7156
2024-12-17 13:16:50.686200: 
2024-12-17 13:16:50.687593: Epoch 21
2024-12-17 13:16:50.688669: Current learning rate: 0.00873
2024-12-17 13:18:43.266140: Validation loss did not improve from -0.53718. Patience: 2/50
2024-12-17 13:18:43.267175: train_loss -0.6925
2024-12-17 13:18:43.268037: val_loss -0.5079
2024-12-17 13:18:43.268800: Pseudo dice [0.7272]
2024-12-17 13:18:43.269573: Epoch time: 112.58 s
2024-12-17 13:18:43.270384: Yayy! New best EMA pseudo Dice: 0.7167
2024-12-17 13:18:45.157138: 
2024-12-17 13:18:45.158508: Epoch 22
2024-12-17 13:18:45.159640: Current learning rate: 0.00867
2024-12-17 13:20:33.207134: Validation loss did not improve from -0.53718. Patience: 3/50
2024-12-17 13:20:33.208175: train_loss -0.7031
2024-12-17 13:20:33.209098: val_loss -0.5079
2024-12-17 13:20:33.209821: Pseudo dice [0.739]
2024-12-17 13:20:33.210701: Epoch time: 108.05 s
2024-12-17 13:20:33.211591: Yayy! New best EMA pseudo Dice: 0.719
2024-12-17 13:20:35.030181: 
2024-12-17 13:20:35.031411: Epoch 23
2024-12-17 13:20:35.032231: Current learning rate: 0.00861
2024-12-17 13:22:28.635710: Validation loss did not improve from -0.53718. Patience: 4/50
2024-12-17 13:22:28.636891: train_loss -0.711
2024-12-17 13:22:28.637721: val_loss -0.4639
2024-12-17 13:22:28.638417: Pseudo dice [0.7084]
2024-12-17 13:22:28.639179: Epoch time: 113.61 s
2024-12-17 13:22:30.042846: 
2024-12-17 13:22:30.044141: Epoch 24
2024-12-17 13:22:30.045012: Current learning rate: 0.00855
2024-12-17 13:24:21.871877: Validation loss did not improve from -0.53718. Patience: 5/50
2024-12-17 13:24:21.872951: train_loss -0.7005
2024-12-17 13:24:21.873955: val_loss -0.4835
2024-12-17 13:24:21.874854: Pseudo dice [0.719]
2024-12-17 13:24:21.875545: Epoch time: 111.83 s
2024-12-17 13:24:23.703834: 
2024-12-17 13:24:23.705247: Epoch 25
2024-12-17 13:24:23.706221: Current learning rate: 0.00849
2024-12-17 13:26:17.774332: Validation loss did not improve from -0.53718. Patience: 6/50
2024-12-17 13:26:17.775410: train_loss -0.7046
2024-12-17 13:26:17.776270: val_loss -0.4772
2024-12-17 13:26:17.776970: Pseudo dice [0.7095]
2024-12-17 13:26:17.777708: Epoch time: 114.07 s
2024-12-17 13:26:19.194184: 
2024-12-17 13:26:19.195571: Epoch 26
2024-12-17 13:26:19.196409: Current learning rate: 0.00843
2024-12-17 13:28:13.839968: Validation loss did not improve from -0.53718. Patience: 7/50
2024-12-17 13:28:13.840926: train_loss -0.7007
2024-12-17 13:28:13.841799: val_loss -0.5041
2024-12-17 13:28:13.842512: Pseudo dice [0.7267]
2024-12-17 13:28:13.843262: Epoch time: 114.65 s
2024-12-17 13:28:15.313102: 
2024-12-17 13:28:15.314298: Epoch 27
2024-12-17 13:28:15.315179: Current learning rate: 0.00836
2024-12-17 13:30:11.778143: Validation loss did not improve from -0.53718. Patience: 8/50
2024-12-17 13:30:11.779018: train_loss -0.7094
2024-12-17 13:30:11.779828: val_loss -0.5265
2024-12-17 13:30:11.780633: Pseudo dice [0.741]
2024-12-17 13:30:11.781493: Epoch time: 116.47 s
2024-12-17 13:30:11.782190: Yayy! New best EMA pseudo Dice: 0.7204
2024-12-17 13:30:14.137405: 
2024-12-17 13:30:14.138743: Epoch 28
2024-12-17 13:30:14.139524: Current learning rate: 0.0083
2024-12-17 13:32:10.637113: Validation loss did not improve from -0.53718. Patience: 9/50
2024-12-17 13:32:10.638119: train_loss -0.7089
2024-12-17 13:32:10.638970: val_loss -0.4941
2024-12-17 13:32:10.639637: Pseudo dice [0.7166]
2024-12-17 13:32:10.640366: Epoch time: 116.5 s
2024-12-17 13:32:12.168521: 
2024-12-17 13:32:12.170033: Epoch 29
2024-12-17 13:32:12.171165: Current learning rate: 0.00824
2024-12-17 13:34:10.752913: Validation loss did not improve from -0.53718. Patience: 10/50
2024-12-17 13:34:10.755469: train_loss -0.7223
2024-12-17 13:34:10.756842: val_loss -0.5299
2024-12-17 13:34:10.757906: Pseudo dice [0.7475]
2024-12-17 13:34:10.758955: Epoch time: 118.59 s
2024-12-17 13:34:11.217579: Yayy! New best EMA pseudo Dice: 0.7228
2024-12-17 13:34:13.330961: 
2024-12-17 13:34:13.332523: Epoch 30
2024-12-17 13:34:13.333561: Current learning rate: 0.00818
2024-12-17 13:36:13.921871: Validation loss did not improve from -0.53718. Patience: 11/50
2024-12-17 13:36:13.922990: train_loss -0.7239
2024-12-17 13:36:13.923947: val_loss -0.5043
2024-12-17 13:36:13.924725: Pseudo dice [0.7308]
2024-12-17 13:36:13.925586: Epoch time: 120.59 s
2024-12-17 13:36:13.926341: Yayy! New best EMA pseudo Dice: 0.7236
2024-12-17 13:36:15.934690: 
2024-12-17 13:36:15.936282: Epoch 31
2024-12-17 13:36:15.937162: Current learning rate: 0.00812
2024-12-17 13:38:11.596266: Validation loss did not improve from -0.53718. Patience: 12/50
2024-12-17 13:38:11.597181: train_loss -0.7256
2024-12-17 13:38:11.598001: val_loss -0.5054
2024-12-17 13:38:11.598791: Pseudo dice [0.7351]
2024-12-17 13:38:11.599593: Epoch time: 115.66 s
2024-12-17 13:38:11.600369: Yayy! New best EMA pseudo Dice: 0.7247
2024-12-17 13:38:13.680450: 
2024-12-17 13:38:13.681842: Epoch 32
2024-12-17 13:38:13.682671: Current learning rate: 0.00806
2024-12-17 13:40:08.657614: Validation loss did not improve from -0.53718. Patience: 13/50
2024-12-17 13:40:08.658718: train_loss -0.7221
2024-12-17 13:40:08.659709: val_loss -0.4313
2024-12-17 13:40:08.660626: Pseudo dice [0.6875]
2024-12-17 13:40:08.661579: Epoch time: 114.98 s
2024-12-17 13:40:10.202951: 
2024-12-17 13:40:10.204312: Epoch 33
2024-12-17 13:40:10.205131: Current learning rate: 0.008
2024-12-17 13:42:09.600760: Validation loss did not improve from -0.53718. Patience: 14/50
2024-12-17 13:42:09.601859: train_loss -0.7252
2024-12-17 13:42:09.602739: val_loss -0.5209
2024-12-17 13:42:09.603522: Pseudo dice [0.7393]
2024-12-17 13:42:09.604297: Epoch time: 119.4 s
2024-12-17 13:42:11.137861: 
2024-12-17 13:42:11.139154: Epoch 34
2024-12-17 13:42:11.140190: Current learning rate: 0.00793
2024-12-17 13:43:49.429573: Validation loss did not improve from -0.53718. Patience: 15/50
2024-12-17 13:43:49.430712: train_loss -0.7256
2024-12-17 13:43:49.431681: val_loss -0.5201
2024-12-17 13:43:49.432603: Pseudo dice [0.7432]
2024-12-17 13:43:49.433336: Epoch time: 98.29 s
2024-12-17 13:43:49.844169: Yayy! New best EMA pseudo Dice: 0.7249
2024-12-17 13:43:51.653651: 
2024-12-17 13:43:51.655399: Epoch 35
2024-12-17 13:43:51.656446: Current learning rate: 0.00787
2024-12-17 13:45:23.880847: Validation loss did not improve from -0.53718. Patience: 16/50
2024-12-17 13:45:23.907399: train_loss -0.7315
2024-12-17 13:45:23.910055: val_loss -0.5321
2024-12-17 13:45:23.911097: Pseudo dice [0.7381]
2024-12-17 13:45:23.912939: Epoch time: 92.26 s
2024-12-17 13:45:23.914218: Yayy! New best EMA pseudo Dice: 0.7262
2024-12-17 13:45:25.745038: 
2024-12-17 13:45:25.746703: Epoch 36
2024-12-17 13:45:25.747616: Current learning rate: 0.00781
2024-12-17 13:46:58.136493: Validation loss did not improve from -0.53718. Patience: 17/50
2024-12-17 13:46:58.137613: train_loss -0.7274
2024-12-17 13:46:58.138922: val_loss -0.46
2024-12-17 13:46:58.139938: Pseudo dice [0.7074]
2024-12-17 13:46:58.140911: Epoch time: 92.39 s
2024-12-17 13:46:59.557976: 
2024-12-17 13:46:59.559429: Epoch 37
2024-12-17 13:46:59.560314: Current learning rate: 0.00775
2024-12-17 13:48:31.828764: Validation loss did not improve from -0.53718. Patience: 18/50
2024-12-17 13:48:31.829823: train_loss -0.7334
2024-12-17 13:48:31.830922: val_loss -0.5236
2024-12-17 13:48:31.831752: Pseudo dice [0.7392]
2024-12-17 13:48:31.832544: Epoch time: 92.27 s
2024-12-17 13:48:33.267032: 
2024-12-17 13:48:33.268366: Epoch 38
2024-12-17 13:48:33.269219: Current learning rate: 0.00769
2024-12-17 13:50:05.237970: Validation loss did not improve from -0.53718. Patience: 19/50
2024-12-17 13:50:05.239053: train_loss -0.7407
2024-12-17 13:50:05.239928: val_loss -0.5226
2024-12-17 13:50:05.240667: Pseudo dice [0.7419]
2024-12-17 13:50:05.241475: Epoch time: 91.97 s
2024-12-17 13:50:05.242173: Yayy! New best EMA pseudo Dice: 0.7274
2024-12-17 13:50:07.585996: 
2024-12-17 13:50:07.587470: Epoch 39
2024-12-17 13:50:07.588536: Current learning rate: 0.00763
2024-12-17 13:51:59.710684: Validation loss did not improve from -0.53718. Patience: 20/50
2024-12-17 13:51:59.711789: train_loss -0.7397
2024-12-17 13:51:59.712994: val_loss -0.5204
2024-12-17 13:51:59.713965: Pseudo dice [0.7344]
2024-12-17 13:51:59.714909: Epoch time: 112.13 s
2024-12-17 13:52:00.152713: Yayy! New best EMA pseudo Dice: 0.7281
2024-12-17 13:52:02.202297: 
2024-12-17 13:52:02.203858: Epoch 40
2024-12-17 13:52:02.205051: Current learning rate: 0.00756
2024-12-17 13:56:09.901235: Validation loss did not improve from -0.53718. Patience: 21/50
2024-12-17 13:56:09.902234: train_loss -0.7454
2024-12-17 13:56:09.903141: val_loss -0.4951
2024-12-17 13:56:09.903780: Pseudo dice [0.7336]
2024-12-17 13:56:09.904449: Epoch time: 247.7 s
2024-12-17 13:56:09.905115: Yayy! New best EMA pseudo Dice: 0.7287
2024-12-17 13:56:11.869750: 
2024-12-17 13:56:11.871114: Epoch 41
2024-12-17 13:56:11.871806: Current learning rate: 0.0075
2024-12-17 14:00:44.790893: Validation loss did not improve from -0.53718. Patience: 22/50
2024-12-17 14:00:44.791899: train_loss -0.7377
2024-12-17 14:00:44.792769: val_loss -0.5011
2024-12-17 14:00:44.793663: Pseudo dice [0.7313]
2024-12-17 14:00:44.794594: Epoch time: 272.92 s
2024-12-17 14:00:44.795590: Yayy! New best EMA pseudo Dice: 0.7289
2024-12-17 14:00:46.630840: 
2024-12-17 14:00:46.632153: Epoch 42
2024-12-17 14:00:46.633104: Current learning rate: 0.00744
2024-12-17 14:04:33.273049: Validation loss did not improve from -0.53718. Patience: 23/50
2024-12-17 14:04:33.273996: train_loss -0.7371
2024-12-17 14:04:33.274829: val_loss -0.4814
2024-12-17 14:04:33.275629: Pseudo dice [0.7169]
2024-12-17 14:04:33.276438: Epoch time: 226.64 s
2024-12-17 14:04:34.672095: 
2024-12-17 14:04:34.673471: Epoch 43
2024-12-17 14:04:34.674376: Current learning rate: 0.00738
2024-12-17 14:08:00.941173: Validation loss did not improve from -0.53718. Patience: 24/50
2024-12-17 14:08:00.942235: train_loss -0.7395
2024-12-17 14:08:00.943146: val_loss -0.4869
2024-12-17 14:08:00.943837: Pseudo dice [0.7182]
2024-12-17 14:08:00.944655: Epoch time: 206.27 s
2024-12-17 14:08:02.451934: 
2024-12-17 14:08:02.453408: Epoch 44
2024-12-17 14:08:02.454229: Current learning rate: 0.00732
2024-12-17 14:11:35.564950: Validation loss did not improve from -0.53718. Patience: 25/50
2024-12-17 14:11:35.566050: train_loss -0.7429
2024-12-17 14:11:35.566900: val_loss -0.5197
2024-12-17 14:11:35.567628: Pseudo dice [0.7433]
2024-12-17 14:11:35.568360: Epoch time: 213.12 s
2024-12-17 14:11:37.418006: 
2024-12-17 14:11:37.419495: Epoch 45
2024-12-17 14:11:37.420379: Current learning rate: 0.00725
2024-12-17 14:15:27.402410: Validation loss did not improve from -0.53718. Patience: 26/50
2024-12-17 14:15:27.403602: train_loss -0.7483
2024-12-17 14:15:27.404656: val_loss -0.5011
2024-12-17 14:15:27.405557: Pseudo dice [0.7323]
2024-12-17 14:15:27.406461: Epoch time: 229.99 s
2024-12-17 14:15:28.879898: 
2024-12-17 14:15:28.881392: Epoch 46
2024-12-17 14:15:28.882279: Current learning rate: 0.00719
2024-12-17 14:19:30.902767: Validation loss did not improve from -0.53718. Patience: 27/50
2024-12-17 14:19:30.903567: train_loss -0.7431
2024-12-17 14:19:30.904404: val_loss -0.5101
2024-12-17 14:19:30.905161: Pseudo dice [0.7339]
2024-12-17 14:19:30.905936: Epoch time: 242.03 s
2024-12-17 14:19:30.906726: Yayy! New best EMA pseudo Dice: 0.7293
2024-12-17 14:19:32.845317: 
2024-12-17 14:19:32.846680: Epoch 47
2024-12-17 14:19:32.847587: Current learning rate: 0.00713
2024-12-17 14:23:50.013275: Validation loss did not improve from -0.53718. Patience: 28/50
2024-12-17 14:23:50.014281: train_loss -0.7507
2024-12-17 14:23:50.015196: val_loss -0.5049
2024-12-17 14:23:50.015909: Pseudo dice [0.7403]
2024-12-17 14:23:50.016621: Epoch time: 257.17 s
2024-12-17 14:23:50.017395: Yayy! New best EMA pseudo Dice: 0.7304
2024-12-17 14:23:51.996802: 
2024-12-17 14:23:51.997936: Epoch 48
2024-12-17 14:23:51.998729: Current learning rate: 0.00707
2024-12-17 14:27:36.338548: Validation loss did not improve from -0.53718. Patience: 29/50
2024-12-17 14:27:36.339653: train_loss -0.7507
2024-12-17 14:27:36.340507: val_loss -0.5265
2024-12-17 14:27:36.341365: Pseudo dice [0.7422]
2024-12-17 14:27:36.342193: Epoch time: 224.34 s
2024-12-17 14:27:36.343124: Yayy! New best EMA pseudo Dice: 0.7316
2024-12-17 14:27:38.343050: 
2024-12-17 14:27:38.344155: Epoch 49
2024-12-17 14:27:38.344938: Current learning rate: 0.007
2024-12-17 14:31:34.342157: Validation loss did not improve from -0.53718. Patience: 30/50
2024-12-17 14:31:34.343335: train_loss -0.7505
2024-12-17 14:31:34.344194: val_loss -0.5015
2024-12-17 14:31:34.344935: Pseudo dice [0.7325]
2024-12-17 14:31:34.345760: Epoch time: 236.0 s
2024-12-17 14:31:34.801723: Yayy! New best EMA pseudo Dice: 0.7317
2024-12-17 14:31:37.421573: 
2024-12-17 14:31:37.423022: Epoch 50
2024-12-17 14:31:37.423934: Current learning rate: 0.00694
2024-12-17 14:35:35.736991: Validation loss did not improve from -0.53718. Patience: 31/50
2024-12-17 14:35:35.738062: train_loss -0.75
2024-12-17 14:35:35.738935: val_loss -0.5278
2024-12-17 14:35:35.739778: Pseudo dice [0.7374]
2024-12-17 14:35:35.740771: Epoch time: 238.32 s
2024-12-17 14:35:35.741787: Yayy! New best EMA pseudo Dice: 0.7323
2024-12-17 14:35:37.704396: 
2024-12-17 14:35:37.706047: Epoch 51
2024-12-17 14:35:37.707150: Current learning rate: 0.00688
2024-12-17 14:39:31.443261: Validation loss did not improve from -0.53718. Patience: 32/50
2024-12-17 14:39:31.444197: train_loss -0.7535
2024-12-17 14:39:31.444993: val_loss -0.4838
2024-12-17 14:39:31.445619: Pseudo dice [0.7253]
2024-12-17 14:39:31.446407: Epoch time: 233.74 s
2024-12-17 14:39:32.923203: 
2024-12-17 14:39:32.924123: Epoch 52
2024-12-17 14:39:32.924867: Current learning rate: 0.00682
2024-12-17 14:43:29.003524: Validation loss did not improve from -0.53718. Patience: 33/50
2024-12-17 14:43:29.004591: train_loss -0.7578
2024-12-17 14:43:29.005697: val_loss -0.4908
2024-12-17 14:43:29.006680: Pseudo dice [0.7335]
2024-12-17 14:43:29.007751: Epoch time: 236.08 s
2024-12-17 14:43:30.505615: 
2024-12-17 14:43:30.507318: Epoch 53
2024-12-17 14:43:30.508325: Current learning rate: 0.00675
2024-12-17 14:47:51.762833: Validation loss did not improve from -0.53718. Patience: 34/50
2024-12-17 14:47:51.776166: train_loss -0.7592
2024-12-17 14:47:51.777200: val_loss -0.5143
2024-12-17 14:47:51.777998: Pseudo dice [0.7382]
2024-12-17 14:47:51.779217: Epoch time: 261.27 s
2024-12-17 14:47:51.780345: Yayy! New best EMA pseudo Dice: 0.7324
2024-12-17 14:47:54.185189: 
2024-12-17 14:47:54.186624: Epoch 54
2024-12-17 14:47:54.187541: Current learning rate: 0.00669
2024-12-17 14:51:42.261849: Validation loss did not improve from -0.53718. Patience: 35/50
2024-12-17 14:51:42.296598: train_loss -0.7593
2024-12-17 14:51:42.298290: val_loss -0.4977
2024-12-17 14:51:42.299266: Pseudo dice [0.738]
2024-12-17 14:51:42.300077: Epoch time: 228.09 s
2024-12-17 14:51:42.696205: Yayy! New best EMA pseudo Dice: 0.733
2024-12-17 14:51:44.929152: 
2024-12-17 14:51:44.930409: Epoch 55
2024-12-17 14:51:44.931232: Current learning rate: 0.00663
2024-12-17 14:55:47.881353: Validation loss did not improve from -0.53718. Patience: 36/50
2024-12-17 14:55:47.882496: train_loss -0.7554
2024-12-17 14:55:47.883345: val_loss -0.4969
2024-12-17 14:55:47.884335: Pseudo dice [0.7345]
2024-12-17 14:55:47.885385: Epoch time: 242.95 s
2024-12-17 14:55:47.886283: Yayy! New best EMA pseudo Dice: 0.7331
2024-12-17 14:55:49.795827: 
2024-12-17 14:55:49.797232: Epoch 56
2024-12-17 14:55:49.798314: Current learning rate: 0.00657
2024-12-17 14:59:48.586915: Validation loss did not improve from -0.53718. Patience: 37/50
2024-12-17 14:59:48.588028: train_loss -0.7599
2024-12-17 14:59:48.588997: val_loss -0.5055
2024-12-17 14:59:48.589983: Pseudo dice [0.7339]
2024-12-17 14:59:48.591005: Epoch time: 238.79 s
2024-12-17 14:59:48.591976: Yayy! New best EMA pseudo Dice: 0.7332
2024-12-17 14:59:50.614031: 
2024-12-17 14:59:50.615200: Epoch 57
2024-12-17 14:59:50.616120: Current learning rate: 0.0065
2024-12-17 15:03:45.756902: Validation loss did not improve from -0.53718. Patience: 38/50
2024-12-17 15:03:45.758085: train_loss -0.7629
2024-12-17 15:03:45.758918: val_loss -0.4853
2024-12-17 15:03:45.759670: Pseudo dice [0.727]
2024-12-17 15:03:45.760488: Epoch time: 235.15 s
2024-12-17 15:03:47.301320: 
2024-12-17 15:03:47.302588: Epoch 58
2024-12-17 15:03:47.303361: Current learning rate: 0.00644
2024-12-17 15:07:36.003115: Validation loss did not improve from -0.53718. Patience: 39/50
2024-12-17 15:07:36.004172: train_loss -0.7623
2024-12-17 15:07:36.005202: val_loss -0.5287
2024-12-17 15:07:36.005949: Pseudo dice [0.7413]
2024-12-17 15:07:36.006824: Epoch time: 228.7 s
2024-12-17 15:07:36.007668: Yayy! New best EMA pseudo Dice: 0.7334
2024-12-17 15:07:37.985657: 
2024-12-17 15:07:37.986994: Epoch 59
2024-12-17 15:07:37.987817: Current learning rate: 0.00638
2024-12-17 15:11:15.095814: Validation loss did not improve from -0.53718. Patience: 40/50
2024-12-17 15:11:15.096768: train_loss -0.7621
2024-12-17 15:11:15.097655: val_loss -0.5043
2024-12-17 15:11:15.098452: Pseudo dice [0.7346]
2024-12-17 15:11:15.099315: Epoch time: 217.11 s
2024-12-17 15:11:15.549542: Yayy! New best EMA pseudo Dice: 0.7336
2024-12-17 15:11:17.924175: 
2024-12-17 15:11:17.925846: Epoch 60
2024-12-17 15:11:17.927030: Current learning rate: 0.00631
2024-12-17 15:15:03.869006: Validation loss did not improve from -0.53718. Patience: 41/50
2024-12-17 15:15:03.870329: train_loss -0.7631
2024-12-17 15:15:03.871491: val_loss -0.5301
2024-12-17 15:15:03.872605: Pseudo dice [0.7457]
2024-12-17 15:15:03.873607: Epoch time: 225.95 s
2024-12-17 15:15:03.874651: Yayy! New best EMA pseudo Dice: 0.7348
2024-12-17 15:15:05.834007: 
2024-12-17 15:15:05.835006: Epoch 61
2024-12-17 15:15:05.835945: Current learning rate: 0.00625
2024-12-17 15:18:52.015966: Validation loss did not improve from -0.53718. Patience: 42/50
2024-12-17 15:18:52.017187: train_loss -0.7692
2024-12-17 15:18:52.018107: val_loss -0.5024
2024-12-17 15:18:52.019072: Pseudo dice [0.7333]
2024-12-17 15:18:52.020033: Epoch time: 226.18 s
2024-12-17 15:18:53.579590: 
2024-12-17 15:18:53.581100: Epoch 62
2024-12-17 15:18:53.581908: Current learning rate: 0.00619
2024-12-17 15:22:56.876539: Validation loss did not improve from -0.53718. Patience: 43/50
2024-12-17 15:22:56.877669: train_loss -0.768
2024-12-17 15:22:56.878735: val_loss -0.5123
2024-12-17 15:22:56.879710: Pseudo dice [0.7387]
2024-12-17 15:22:56.880644: Epoch time: 243.3 s
2024-12-17 15:22:56.881511: Yayy! New best EMA pseudo Dice: 0.735
2024-12-17 15:22:58.816630: 
2024-12-17 15:22:58.818148: Epoch 63
2024-12-17 15:22:58.819340: Current learning rate: 0.00612
2024-12-17 15:27:01.726408: Validation loss did not improve from -0.53718. Patience: 44/50
2024-12-17 15:27:01.727519: train_loss -0.768
2024-12-17 15:27:01.728548: val_loss -0.5305
2024-12-17 15:27:01.729537: Pseudo dice [0.7534]
2024-12-17 15:27:01.730517: Epoch time: 242.91 s
2024-12-17 15:27:01.731380: Yayy! New best EMA pseudo Dice: 0.7369
2024-12-17 15:27:03.693574: 
2024-12-17 15:27:03.695150: Epoch 64
2024-12-17 15:27:03.696190: Current learning rate: 0.00606
2024-12-17 15:31:04.362214: Validation loss did not improve from -0.53718. Patience: 45/50
2024-12-17 15:31:04.363272: train_loss -0.7745
2024-12-17 15:31:04.364318: val_loss -0.4906
2024-12-17 15:31:04.365213: Pseudo dice [0.7304]
2024-12-17 15:31:04.366188: Epoch time: 240.67 s
2024-12-17 15:31:06.314500: 
2024-12-17 15:31:06.315863: Epoch 65
2024-12-17 15:31:06.316735: Current learning rate: 0.006
2024-12-17 15:34:58.148187: Validation loss did not improve from -0.53718. Patience: 46/50
2024-12-17 15:34:58.149200: train_loss -0.7722
2024-12-17 15:34:58.150138: val_loss -0.5251
2024-12-17 15:34:58.150988: Pseudo dice [0.7412]
2024-12-17 15:34:58.151864: Epoch time: 231.84 s
2024-12-17 15:34:59.735934: 
2024-12-17 15:34:59.737181: Epoch 66
2024-12-17 15:34:59.738038: Current learning rate: 0.00593
2024-12-17 15:38:57.536118: Validation loss did not improve from -0.53718. Patience: 47/50
2024-12-17 15:38:57.537255: train_loss -0.7713
2024-12-17 15:38:57.538212: val_loss -0.4871
2024-12-17 15:38:57.539078: Pseudo dice [0.7179]
2024-12-17 15:38:57.539946: Epoch time: 237.8 s
2024-12-17 15:38:59.083252: 
2024-12-17 15:38:59.084456: Epoch 67
2024-12-17 15:38:59.085209: Current learning rate: 0.00587
2024-12-17 15:43:02.801387: Validation loss did not improve from -0.53718. Patience: 48/50
2024-12-17 15:43:02.802502: train_loss -0.771
2024-12-17 15:43:02.803668: val_loss -0.4995
2024-12-17 15:43:02.804607: Pseudo dice [0.7308]
2024-12-17 15:43:02.805404: Epoch time: 243.72 s
2024-12-17 15:43:04.406229: 
2024-12-17 15:43:04.407707: Epoch 68
2024-12-17 15:43:04.408908: Current learning rate: 0.00581
2024-12-17 15:47:26.711472: Validation loss did not improve from -0.53718. Patience: 49/50
2024-12-17 15:47:26.712693: train_loss -0.7724
2024-12-17 15:47:26.713714: val_loss -0.4837
2024-12-17 15:47:26.714556: Pseudo dice [0.7261]
2024-12-17 15:47:26.715418: Epoch time: 262.31 s
2024-12-17 15:47:28.259970: 
2024-12-17 15:47:28.261325: Epoch 69
2024-12-17 15:47:28.262299: Current learning rate: 0.00574
2024-12-17 15:51:34.435519: Validation loss did not improve from -0.53718. Patience: 50/50
2024-12-17 15:51:34.436568: train_loss -0.7741
2024-12-17 15:51:34.437604: val_loss -0.5107
2024-12-17 15:51:34.438435: Pseudo dice [0.7416]
2024-12-17 15:51:34.439397: Epoch time: 246.18 s
2024-12-17 15:51:36.501730: 
2024-12-17 15:51:36.503121: Epoch 70
2024-12-17 15:51:36.504062: Current learning rate: 0.00568
2024-12-17 15:55:47.312917: Validation loss did not improve from -0.53718. Patience: 51/50
2024-12-17 15:55:47.316278: train_loss -0.7722
2024-12-17 15:55:47.317395: val_loss -0.4386
2024-12-17 15:55:47.318235: Pseudo dice [0.7063]
2024-12-17 15:55:47.319001: Epoch time: 250.82 s
2024-12-17 15:55:49.415412: 
2024-12-17 15:55:49.416953: Epoch 71
2024-12-17 15:55:49.418048: Current learning rate: 0.00562
2024-12-17 16:00:14.831404: Validation loss did not improve from -0.53718. Patience: 52/50
2024-12-17 16:00:14.832417: train_loss -0.776
2024-12-17 16:00:14.833277: val_loss -0.4642
2024-12-17 16:00:14.833992: Pseudo dice [0.707]
2024-12-17 16:00:14.834848: Epoch time: 265.42 s
2024-12-17 16:00:16.363106: 
2024-12-17 16:00:16.364824: Epoch 72
2024-12-17 16:00:16.365874: Current learning rate: 0.00555
2024-12-17 16:04:17.663284: Validation loss did not improve from -0.53718. Patience: 53/50
2024-12-17 16:04:17.664289: train_loss -0.7708
2024-12-17 16:04:17.665210: val_loss -0.4842
2024-12-17 16:04:17.666055: Pseudo dice [0.7233]
2024-12-17 16:04:17.667031: Epoch time: 241.3 s
2024-12-17 16:04:19.243884: 
2024-12-17 16:04:19.245571: Epoch 73
2024-12-17 16:04:19.246558: Current learning rate: 0.00549
2024-12-17 16:08:31.526408: Validation loss did not improve from -0.53718. Patience: 54/50
2024-12-17 16:08:31.527498: train_loss -0.7757
2024-12-17 16:08:31.528421: val_loss -0.5059
2024-12-17 16:08:31.529154: Pseudo dice [0.7409]
2024-12-17 16:08:31.529947: Epoch time: 252.29 s
2024-12-17 16:08:33.053457: 
2024-12-17 16:08:33.054599: Epoch 74
2024-12-17 16:08:33.055421: Current learning rate: 0.00542
2024-12-17 16:12:33.467124: Validation loss did not improve from -0.53718. Patience: 55/50
2024-12-17 16:12:33.468155: train_loss -0.7783
2024-12-17 16:12:33.469300: val_loss -0.5037
2024-12-17 16:12:33.470296: Pseudo dice [0.7384]
2024-12-17 16:12:33.471212: Epoch time: 240.42 s
2024-12-17 16:12:35.429971: 
2024-12-17 16:12:35.431456: Epoch 75
2024-12-17 16:12:35.432369: Current learning rate: 0.00536
2024-12-17 16:16:59.278662: Validation loss improved from -0.53718 to -0.54072! Patience: 55/50
2024-12-17 16:16:59.279694: train_loss -0.7802
2024-12-17 16:16:59.280677: val_loss -0.5407
2024-12-17 16:16:59.281568: Pseudo dice [0.7525]
2024-12-17 16:16:59.282410: Epoch time: 263.85 s
2024-12-17 16:17:00.847403: 
2024-12-17 16:17:00.848537: Epoch 76
2024-12-17 16:17:00.849351: Current learning rate: 0.00529
2024-12-17 16:21:12.178834: Validation loss did not improve from -0.54072. Patience: 1/50
2024-12-17 16:21:12.179851: train_loss -0.7792
2024-12-17 16:21:12.180785: val_loss -0.4949
2024-12-17 16:21:12.181628: Pseudo dice [0.7285]
2024-12-17 16:21:12.182541: Epoch time: 251.33 s
2024-12-17 16:21:13.688921: 
2024-12-17 16:21:13.690288: Epoch 77
2024-12-17 16:21:13.691121: Current learning rate: 0.00523
2024-12-17 16:25:28.318125: Validation loss did not improve from -0.54072. Patience: 2/50
2024-12-17 16:25:28.319039: train_loss -0.7796
2024-12-17 16:25:28.320276: val_loss -0.5298
2024-12-17 16:25:28.321532: Pseudo dice [0.7441]
2024-12-17 16:25:28.322649: Epoch time: 254.63 s
2024-12-17 16:25:29.919813: 
2024-12-17 16:25:29.921444: Epoch 78
2024-12-17 16:25:29.922491: Current learning rate: 0.00517
2024-12-17 16:29:42.584880: Validation loss did not improve from -0.54072. Patience: 3/50
2024-12-17 16:29:42.585869: train_loss -0.7774
2024-12-17 16:29:42.586797: val_loss -0.474
2024-12-17 16:29:42.587606: Pseudo dice [0.7298]
2024-12-17 16:29:42.588546: Epoch time: 252.67 s
2024-12-17 16:29:44.177934: 
2024-12-17 16:29:44.179429: Epoch 79
2024-12-17 16:29:44.180342: Current learning rate: 0.0051
2024-12-17 16:34:30.381966: Validation loss did not improve from -0.54072. Patience: 4/50
2024-12-17 16:34:30.383128: train_loss -0.7811
2024-12-17 16:34:30.384199: val_loss -0.5051
2024-12-17 16:34:30.385122: Pseudo dice [0.7348]
2024-12-17 16:34:30.386062: Epoch time: 286.21 s
2024-12-17 16:34:32.344010: 
2024-12-17 16:34:32.345334: Epoch 80
2024-12-17 16:34:32.346161: Current learning rate: 0.00504
2024-12-17 16:38:30.990203: Validation loss did not improve from -0.54072. Patience: 5/50
2024-12-17 16:38:30.991081: train_loss -0.782
2024-12-17 16:38:30.991881: val_loss -0.4976
2024-12-17 16:38:30.992677: Pseudo dice [0.7256]
2024-12-17 16:38:30.993491: Epoch time: 238.65 s
2024-12-17 16:38:32.932908: 
2024-12-17 16:38:32.934304: Epoch 81
2024-12-17 16:38:32.935194: Current learning rate: 0.00497
2024-12-17 16:42:47.090196: Validation loss did not improve from -0.54072. Patience: 6/50
2024-12-17 16:42:47.091462: train_loss -0.7823
2024-12-17 16:42:47.092337: val_loss -0.533
2024-12-17 16:42:47.093148: Pseudo dice [0.7576]
2024-12-17 16:42:47.093993: Epoch time: 254.16 s
2024-12-17 16:42:48.620842: 
2024-12-17 16:42:48.622136: Epoch 82
2024-12-17 16:42:48.623012: Current learning rate: 0.00491
2024-12-17 16:46:58.782421: Validation loss did not improve from -0.54072. Patience: 7/50
2024-12-17 16:46:58.783363: train_loss -0.7814
2024-12-17 16:46:58.784268: val_loss -0.503
2024-12-17 16:46:58.785112: Pseudo dice [0.7299]
2024-12-17 16:46:58.785966: Epoch time: 250.16 s
2024-12-17 16:47:00.259103: 
2024-12-17 16:47:00.260465: Epoch 83
2024-12-17 16:47:00.261309: Current learning rate: 0.00484
2024-12-17 16:51:19.770539: Validation loss improved from -0.54072 to -0.54325! Patience: 7/50
2024-12-17 16:51:19.771634: train_loss -0.7828
2024-12-17 16:51:19.772538: val_loss -0.5432
2024-12-17 16:51:19.773251: Pseudo dice [0.7555]
2024-12-17 16:51:19.774047: Epoch time: 259.51 s
2024-12-17 16:51:21.255757: 
2024-12-17 16:51:21.257343: Epoch 84
2024-12-17 16:51:21.258371: Current learning rate: 0.00478
2024-12-17 16:55:30.350324: Validation loss did not improve from -0.54325. Patience: 1/50
2024-12-17 16:55:30.351308: train_loss -0.7849
2024-12-17 16:55:30.352225: val_loss -0.5008
2024-12-17 16:55:30.353060: Pseudo dice [0.7295]
2024-12-17 16:55:30.353963: Epoch time: 249.1 s
2024-12-17 16:55:32.174003: 
2024-12-17 16:55:32.180295: Epoch 85
2024-12-17 16:55:32.181304: Current learning rate: 0.00471
2024-12-17 16:59:32.659743: Validation loss did not improve from -0.54325. Patience: 2/50
2024-12-17 16:59:32.662235: train_loss -0.7874
2024-12-17 16:59:32.664109: val_loss -0.4862
2024-12-17 16:59:32.665202: Pseudo dice [0.7286]
2024-12-17 16:59:32.666366: Epoch time: 240.49 s
2024-12-17 16:59:34.161081: 
2024-12-17 16:59:34.162493: Epoch 86
2024-12-17 16:59:34.163641: Current learning rate: 0.00465
2024-12-17 17:03:26.731825: Validation loss did not improve from -0.54325. Patience: 3/50
2024-12-17 17:03:26.735553: train_loss -0.7858
2024-12-17 17:03:26.737214: val_loss -0.4835
2024-12-17 17:03:26.738110: Pseudo dice [0.7316]
2024-12-17 17:03:26.738991: Epoch time: 232.58 s
2024-12-17 17:03:28.267870: 
2024-12-17 17:03:28.269393: Epoch 87
2024-12-17 17:03:28.270408: Current learning rate: 0.00458
2024-12-17 17:07:32.837551: Validation loss did not improve from -0.54325. Patience: 4/50
2024-12-17 17:07:32.838567: train_loss -0.7872
2024-12-17 17:07:32.839463: val_loss -0.4658
2024-12-17 17:07:32.840295: Pseudo dice [0.7098]
2024-12-17 17:07:32.841183: Epoch time: 244.57 s
2024-12-17 17:07:34.286859: 
2024-12-17 17:07:34.288252: Epoch 88
2024-12-17 17:07:34.289049: Current learning rate: 0.00452
2024-12-17 17:11:44.910665: Validation loss did not improve from -0.54325. Patience: 5/50
2024-12-17 17:11:44.911666: train_loss -0.7878
2024-12-17 17:11:44.912431: val_loss -0.5081
2024-12-17 17:11:44.913212: Pseudo dice [0.7339]
2024-12-17 17:11:44.913996: Epoch time: 250.63 s
2024-12-17 17:11:46.373808: 
2024-12-17 17:11:46.375378: Epoch 89
2024-12-17 17:11:46.376280: Current learning rate: 0.00445
2024-12-17 17:16:08.085587: Validation loss did not improve from -0.54325. Patience: 6/50
2024-12-17 17:16:08.086498: train_loss -0.7888
2024-12-17 17:16:08.087358: val_loss -0.5205
2024-12-17 17:16:08.088187: Pseudo dice [0.7373]
2024-12-17 17:16:08.088969: Epoch time: 261.71 s
2024-12-17 17:16:10.002555: 
2024-12-17 17:16:10.004074: Epoch 90
2024-12-17 17:16:10.005000: Current learning rate: 0.00438
2024-12-17 17:20:33.913660: Validation loss did not improve from -0.54325. Patience: 7/50
2024-12-17 17:20:33.914743: train_loss -0.7869
2024-12-17 17:20:33.915605: val_loss -0.4868
2024-12-17 17:20:33.916349: Pseudo dice [0.7286]
2024-12-17 17:20:33.917161: Epoch time: 263.91 s
2024-12-17 17:20:35.377640: 
2024-12-17 17:20:35.379042: Epoch 91
2024-12-17 17:20:35.379973: Current learning rate: 0.00432
2024-12-17 17:24:49.469452: Validation loss did not improve from -0.54325. Patience: 8/50
2024-12-17 17:24:49.470582: train_loss -0.7872
2024-12-17 17:24:49.471483: val_loss -0.534
2024-12-17 17:24:49.472323: Pseudo dice [0.7464]
2024-12-17 17:24:49.473233: Epoch time: 254.09 s
2024-12-17 17:24:50.971320: 
2024-12-17 17:24:50.972790: Epoch 92
2024-12-17 17:24:50.973943: Current learning rate: 0.00425
2024-12-17 17:28:50.832183: Validation loss did not improve from -0.54325. Patience: 9/50
2024-12-17 17:28:50.833188: train_loss -0.7884
2024-12-17 17:28:50.834133: val_loss -0.4721
2024-12-17 17:28:50.834875: Pseudo dice [0.7264]
2024-12-17 17:28:50.835652: Epoch time: 239.86 s
2024-12-17 17:28:52.822351: 
2024-12-17 17:28:52.823651: Epoch 93
2024-12-17 17:28:52.824572: Current learning rate: 0.00419
2024-12-17 17:33:08.359049: Validation loss did not improve from -0.54325. Patience: 10/50
2024-12-17 17:33:08.359880: train_loss -0.7918
2024-12-17 17:33:08.360824: val_loss -0.4554
2024-12-17 17:33:08.361542: Pseudo dice [0.7039]
2024-12-17 17:33:08.362357: Epoch time: 255.54 s
2024-12-17 17:33:09.765395: 
2024-12-17 17:33:09.766329: Epoch 94
2024-12-17 17:33:09.767210: Current learning rate: 0.00412
2024-12-17 17:37:10.742537: Validation loss did not improve from -0.54325. Patience: 11/50
2024-12-17 17:37:10.743683: train_loss -0.7898
2024-12-17 17:37:10.744618: val_loss -0.4855
2024-12-17 17:37:10.745554: Pseudo dice [0.722]
2024-12-17 17:37:10.746389: Epoch time: 240.98 s
2024-12-17 17:37:12.637761: 
2024-12-17 17:37:12.639085: Epoch 95
2024-12-17 17:37:12.639808: Current learning rate: 0.00405
2024-12-17 17:41:22.994416: Validation loss did not improve from -0.54325. Patience: 12/50
2024-12-17 17:41:22.995606: train_loss -0.7921
2024-12-17 17:41:22.996626: val_loss -0.5028
2024-12-17 17:41:22.997510: Pseudo dice [0.7314]
2024-12-17 17:41:22.998493: Epoch time: 250.36 s
2024-12-17 17:41:24.481184: 
2024-12-17 17:41:24.482636: Epoch 96
2024-12-17 17:41:24.483692: Current learning rate: 0.00399
2024-12-17 17:45:36.915828: Validation loss did not improve from -0.54325. Patience: 13/50
2024-12-17 17:45:36.916881: train_loss -0.7951
2024-12-17 17:45:36.917670: val_loss -0.4879
2024-12-17 17:45:36.918517: Pseudo dice [0.7413]
2024-12-17 17:45:36.919363: Epoch time: 252.44 s
2024-12-17 17:45:38.373830: 
2024-12-17 17:45:38.375233: Epoch 97
2024-12-17 17:45:38.376059: Current learning rate: 0.00392
2024-12-17 17:49:59.607670: Validation loss did not improve from -0.54325. Patience: 14/50
2024-12-17 17:49:59.608572: train_loss -0.794
2024-12-17 17:49:59.609544: val_loss -0.496
2024-12-17 17:49:59.610402: Pseudo dice [0.7298]
2024-12-17 17:49:59.611147: Epoch time: 261.24 s
2024-12-17 17:50:01.077702: 
2024-12-17 17:50:01.079051: Epoch 98
2024-12-17 17:50:01.080033: Current learning rate: 0.00385
2024-12-17 17:54:14.836713: Validation loss did not improve from -0.54325. Patience: 15/50
2024-12-17 17:54:14.837727: train_loss -0.7924
2024-12-17 17:54:14.838569: val_loss -0.47
2024-12-17 17:54:14.839566: Pseudo dice [0.7198]
2024-12-17 17:54:14.840568: Epoch time: 253.76 s
2024-12-17 17:54:16.347819: 
2024-12-17 17:54:16.349346: Epoch 99
2024-12-17 17:54:16.350335: Current learning rate: 0.00379
2024-12-17 17:58:36.551373: Validation loss did not improve from -0.54325. Patience: 16/50
2024-12-17 17:58:36.552514: train_loss -0.7944
2024-12-17 17:58:36.553583: val_loss -0.5052
2024-12-17 17:58:36.554510: Pseudo dice [0.7368]
2024-12-17 17:58:36.555406: Epoch time: 260.21 s
2024-12-17 17:58:38.470532: 
2024-12-17 17:58:38.472098: Epoch 100
2024-12-17 17:58:38.473083: Current learning rate: 0.00372
2024-12-17 18:03:02.048185: Validation loss did not improve from -0.54325. Patience: 17/50
2024-12-17 18:03:02.049706: train_loss -0.7956
2024-12-17 18:03:02.050845: val_loss -0.4898
2024-12-17 18:03:02.051714: Pseudo dice [0.7359]
2024-12-17 18:03:02.052906: Epoch time: 263.58 s
2024-12-17 18:03:03.560355: 
2024-12-17 18:03:03.561720: Epoch 101
2024-12-17 18:03:03.562698: Current learning rate: 0.00365
2024-12-17 18:07:19.816304: Validation loss did not improve from -0.54325. Patience: 18/50
2024-12-17 18:07:19.861658: train_loss -0.7927
2024-12-17 18:07:19.863758: val_loss -0.5197
2024-12-17 18:07:19.864980: Pseudo dice [0.7455]
2024-12-17 18:07:19.866129: Epoch time: 256.3 s
2024-12-17 18:07:21.507823: 
2024-12-17 18:07:21.509328: Epoch 102
2024-12-17 18:07:21.510350: Current learning rate: 0.00359
2024-12-17 18:11:38.123814: Validation loss did not improve from -0.54325. Patience: 19/50
2024-12-17 18:11:38.125496: train_loss -0.7958
2024-12-17 18:11:38.126512: val_loss -0.4698
2024-12-17 18:11:38.127191: Pseudo dice [0.7204]
2024-12-17 18:11:38.127993: Epoch time: 256.62 s
2024-12-17 18:11:39.593684: 
2024-12-17 18:11:39.595110: Epoch 103
2024-12-17 18:11:39.596103: Current learning rate: 0.00352
2024-12-17 18:15:55.029863: Validation loss did not improve from -0.54325. Patience: 20/50
2024-12-17 18:15:55.031525: train_loss -0.7966
2024-12-17 18:15:55.032519: val_loss -0.4841
2024-12-17 18:15:55.033478: Pseudo dice [0.7325]
2024-12-17 18:15:55.034557: Epoch time: 255.44 s
2024-12-17 18:15:56.527819: 
2024-12-17 18:15:56.529412: Epoch 104
2024-12-17 18:15:56.530372: Current learning rate: 0.00345
2024-12-17 18:20:09.610847: Validation loss did not improve from -0.54325. Patience: 21/50
2024-12-17 18:20:09.612647: train_loss -0.7963
2024-12-17 18:20:09.613900: val_loss -0.4906
2024-12-17 18:20:09.614798: Pseudo dice [0.7352]
2024-12-17 18:20:09.615658: Epoch time: 253.09 s
2024-12-17 18:20:12.004339: 
2024-12-17 18:20:12.005795: Epoch 105
2024-12-17 18:20:12.006635: Current learning rate: 0.00338
2024-12-17 18:24:19.682999: Validation loss did not improve from -0.54325. Patience: 22/50
2024-12-17 18:24:19.684184: train_loss -0.7961
2024-12-17 18:24:19.685225: val_loss -0.5003
2024-12-17 18:24:19.686215: Pseudo dice [0.7286]
2024-12-17 18:24:19.687083: Epoch time: 247.68 s
2024-12-17 18:24:21.228317: 
2024-12-17 18:24:21.229726: Epoch 106
2024-12-17 18:24:21.230727: Current learning rate: 0.00332
2024-12-17 18:28:32.615218: Validation loss did not improve from -0.54325. Patience: 23/50
2024-12-17 18:28:32.616223: train_loss -0.7959
2024-12-17 18:28:32.617149: val_loss -0.4852
2024-12-17 18:28:32.617890: Pseudo dice [0.7292]
2024-12-17 18:28:32.618754: Epoch time: 251.39 s
2024-12-17 18:28:34.132019: 
2024-12-17 18:28:34.133501: Epoch 107
2024-12-17 18:28:34.134349: Current learning rate: 0.00325
2024-12-17 18:32:56.571996: Validation loss did not improve from -0.54325. Patience: 24/50
2024-12-17 18:32:56.573042: train_loss -0.8002
2024-12-17 18:32:56.573895: val_loss -0.4686
2024-12-17 18:32:56.574774: Pseudo dice [0.7113]
2024-12-17 18:32:56.575516: Epoch time: 262.44 s
2024-12-17 18:32:58.029461: 
2024-12-17 18:32:58.030755: Epoch 108
2024-12-17 18:32:58.031687: Current learning rate: 0.00318
2024-12-17 18:37:28.750770: Validation loss did not improve from -0.54325. Patience: 25/50
2024-12-17 18:37:28.751887: train_loss -0.7991
2024-12-17 18:37:28.753117: val_loss -0.5164
2024-12-17 18:37:28.754133: Pseudo dice [0.746]
2024-12-17 18:37:28.755110: Epoch time: 270.72 s
2024-12-17 18:37:30.277516: 
2024-12-17 18:37:30.278850: Epoch 109
2024-12-17 18:37:30.279914: Current learning rate: 0.00311
2024-12-17 18:41:51.615242: Validation loss did not improve from -0.54325. Patience: 26/50
2024-12-17 18:41:51.616318: train_loss -0.7962
2024-12-17 18:41:51.617092: val_loss -0.5104
2024-12-17 18:41:51.617788: Pseudo dice [0.7407]
2024-12-17 18:41:51.618590: Epoch time: 261.34 s
2024-12-17 18:41:53.550752: 
2024-12-17 18:41:53.552167: Epoch 110
2024-12-17 18:41:53.553011: Current learning rate: 0.00304
2024-12-17 18:46:05.777831: Validation loss did not improve from -0.54325. Patience: 27/50
2024-12-17 18:46:05.778856: train_loss -0.8038
2024-12-17 18:46:05.779723: val_loss -0.5187
2024-12-17 18:46:05.780574: Pseudo dice [0.7475]
2024-12-17 18:46:05.781441: Epoch time: 252.23 s
2024-12-17 18:46:07.361007: 
2024-12-17 18:46:07.362605: Epoch 111
2024-12-17 18:46:07.363553: Current learning rate: 0.00297
2024-12-17 18:50:33.069684: Validation loss did not improve from -0.54325. Patience: 28/50
2024-12-17 18:50:33.070700: train_loss -0.8041
2024-12-17 18:50:33.071735: val_loss -0.5234
2024-12-17 18:50:33.072695: Pseudo dice [0.7552]
2024-12-17 18:50:33.073433: Epoch time: 265.71 s
2024-12-17 18:50:34.609267: 
2024-12-17 18:50:34.610722: Epoch 112
2024-12-17 18:50:34.611704: Current learning rate: 0.00291
2024-12-17 18:54:53.126043: Validation loss did not improve from -0.54325. Patience: 29/50
2024-12-17 18:54:53.126994: train_loss -0.8033
2024-12-17 18:54:53.127864: val_loss -0.4749
2024-12-17 18:54:53.128689: Pseudo dice [0.7225]
2024-12-17 18:54:53.129496: Epoch time: 258.52 s
2024-12-17 18:54:54.593744: 
2024-12-17 18:54:54.595181: Epoch 113
2024-12-17 18:54:54.596096: Current learning rate: 0.00284
2024-12-17 18:59:08.797060: Validation loss did not improve from -0.54325. Patience: 30/50
2024-12-17 18:59:08.798336: train_loss -0.8021
2024-12-17 18:59:08.799463: val_loss -0.4653
2024-12-17 18:59:08.800268: Pseudo dice [0.7233]
2024-12-17 18:59:08.801134: Epoch time: 254.21 s
2024-12-17 18:59:10.285818: 
2024-12-17 18:59:10.287112: Epoch 114
2024-12-17 18:59:10.288110: Current learning rate: 0.00277
2024-12-17 19:03:35.871557: Validation loss did not improve from -0.54325. Patience: 31/50
2024-12-17 19:03:35.872511: train_loss -0.7995
2024-12-17 19:03:35.873304: val_loss -0.514
2024-12-17 19:03:35.874206: Pseudo dice [0.7404]
2024-12-17 19:03:35.875226: Epoch time: 265.59 s
2024-12-17 19:03:37.718503: 
2024-12-17 19:03:37.719826: Epoch 115
2024-12-17 19:03:37.720873: Current learning rate: 0.0027
2024-12-17 19:07:58.504745: Validation loss did not improve from -0.54325. Patience: 32/50
2024-12-17 19:07:58.506346: train_loss -0.806
2024-12-17 19:07:58.507332: val_loss -0.5137
2024-12-17 19:07:58.508066: Pseudo dice [0.7407]
2024-12-17 19:07:58.509113: Epoch time: 260.79 s
2024-12-17 19:08:00.684043: 
2024-12-17 19:08:00.685455: Epoch 116
2024-12-17 19:08:00.686275: Current learning rate: 0.00263
2024-12-17 19:12:10.784647: Validation loss did not improve from -0.54325. Patience: 33/50
2024-12-17 19:12:10.785790: train_loss -0.8056
2024-12-17 19:12:10.786790: val_loss -0.4791
2024-12-17 19:12:10.787543: Pseudo dice [0.7227]
2024-12-17 19:12:10.788404: Epoch time: 250.1 s
2024-12-17 19:12:12.319803: 
2024-12-17 19:12:12.321369: Epoch 117
2024-12-17 19:12:12.322380: Current learning rate: 0.00256
2024-12-17 19:16:27.200017: Validation loss did not improve from -0.54325. Patience: 34/50
2024-12-17 19:16:27.201222: train_loss -0.8036
2024-12-17 19:16:27.202218: val_loss -0.4934
2024-12-17 19:16:27.203251: Pseudo dice [0.737]
2024-12-17 19:16:27.204233: Epoch time: 254.88 s
2024-12-17 19:16:28.748475: 
2024-12-17 19:16:28.750040: Epoch 118
2024-12-17 19:16:28.751040: Current learning rate: 0.00249
2024-12-17 19:20:44.840205: Validation loss did not improve from -0.54325. Patience: 35/50
2024-12-17 19:20:44.841104: train_loss -0.8055
2024-12-17 19:20:44.842005: val_loss -0.5114
2024-12-17 19:20:44.842779: Pseudo dice [0.7376]
2024-12-17 19:20:44.843547: Epoch time: 256.09 s
2024-12-17 19:20:46.371331: 
2024-12-17 19:20:46.372705: Epoch 119
2024-12-17 19:20:46.373473: Current learning rate: 0.00242
2024-12-17 19:25:02.843263: Validation loss did not improve from -0.54325. Patience: 36/50
2024-12-17 19:25:02.844306: train_loss -0.805
2024-12-17 19:25:02.845249: val_loss -0.4963
2024-12-17 19:25:02.846094: Pseudo dice [0.7421]
2024-12-17 19:25:02.846921: Epoch time: 256.47 s
2024-12-17 19:25:04.836812: 
2024-12-17 19:25:04.837877: Epoch 120
2024-12-17 19:25:04.838729: Current learning rate: 0.00235
2024-12-17 19:29:23.755076: Validation loss did not improve from -0.54325. Patience: 37/50
2024-12-17 19:29:23.756191: train_loss -0.8037
2024-12-17 19:29:23.757440: val_loss -0.5008
2024-12-17 19:29:23.758538: Pseudo dice [0.7372]
2024-12-17 19:29:23.759593: Epoch time: 258.92 s
2024-12-17 19:29:25.298219: 
2024-12-17 19:29:25.299644: Epoch 121
2024-12-17 19:29:25.300511: Current learning rate: 0.00228
2024-12-17 19:33:42.580901: Validation loss did not improve from -0.54325. Patience: 38/50
2024-12-17 19:33:42.582008: train_loss -0.8066
2024-12-17 19:33:42.582905: val_loss -0.4602
2024-12-17 19:33:42.583704: Pseudo dice [0.7173]
2024-12-17 19:33:42.584475: Epoch time: 257.29 s
2024-12-17 19:33:44.097728: 
2024-12-17 19:33:44.098835: Epoch 122
2024-12-17 19:33:44.099590: Current learning rate: 0.00221
2024-12-17 19:38:02.656697: Validation loss did not improve from -0.54325. Patience: 39/50
2024-12-17 19:38:02.657636: train_loss -0.8093
2024-12-17 19:38:02.658606: val_loss -0.4852
2024-12-17 19:38:02.659361: Pseudo dice [0.7302]
2024-12-17 19:38:02.660120: Epoch time: 258.56 s
2024-12-17 19:38:04.224695: 
2024-12-17 19:38:04.226212: Epoch 123
2024-12-17 19:38:04.227052: Current learning rate: 0.00214
2024-12-17 19:42:13.601519: Validation loss did not improve from -0.54325. Patience: 40/50
2024-12-17 19:42:13.602716: train_loss -0.8099
2024-12-17 19:42:13.603664: val_loss -0.4784
2024-12-17 19:42:13.604439: Pseudo dice [0.7284]
2024-12-17 19:42:13.605256: Epoch time: 249.38 s
2024-12-17 19:42:15.148051: 
2024-12-17 19:42:15.149458: Epoch 124
2024-12-17 19:42:15.150228: Current learning rate: 0.00207
2024-12-17 19:46:29.737205: Validation loss did not improve from -0.54325. Patience: 41/50
2024-12-17 19:46:29.738188: train_loss -0.8042
2024-12-17 19:46:29.739124: val_loss -0.5016
2024-12-17 19:46:29.739923: Pseudo dice [0.7317]
2024-12-17 19:46:29.740805: Epoch time: 254.59 s
2024-12-17 19:46:31.705793: 
2024-12-17 19:46:31.707202: Epoch 125
2024-12-17 19:46:31.707944: Current learning rate: 0.00199
2024-12-17 19:50:46.160211: Validation loss did not improve from -0.54325. Patience: 42/50
2024-12-17 19:50:46.161261: train_loss -0.808
2024-12-17 19:50:46.162097: val_loss -0.4924
2024-12-17 19:50:46.162901: Pseudo dice [0.727]
2024-12-17 19:50:46.163615: Epoch time: 254.46 s
2024-12-17 19:50:47.658217: 
2024-12-17 19:50:47.659542: Epoch 126
2024-12-17 19:50:47.660377: Current learning rate: 0.00192
2024-12-17 19:54:58.854018: Validation loss did not improve from -0.54325. Patience: 43/50
2024-12-17 19:54:58.855077: train_loss -0.8079
2024-12-17 19:54:58.855950: val_loss -0.4951
2024-12-17 19:54:58.856874: Pseudo dice [0.7375]
2024-12-17 19:54:58.857723: Epoch time: 251.2 s
2024-12-17 19:55:00.816066: 
2024-12-17 19:55:00.817531: Epoch 127
2024-12-17 19:55:00.818430: Current learning rate: 0.00185
2024-12-17 19:59:17.221736: Validation loss did not improve from -0.54325. Patience: 44/50
2024-12-17 19:59:17.222839: train_loss -0.8071
2024-12-17 19:59:17.223756: val_loss -0.5039
2024-12-17 19:59:17.224587: Pseudo dice [0.7334]
2024-12-17 19:59:17.225457: Epoch time: 256.41 s
2024-12-17 19:59:18.786094: 
2024-12-17 19:59:18.787450: Epoch 128
2024-12-17 19:59:18.788302: Current learning rate: 0.00178
2024-12-17 20:03:48.010863: Validation loss did not improve from -0.54325. Patience: 45/50
2024-12-17 20:03:48.011812: train_loss -0.8095
2024-12-17 20:03:48.012610: val_loss -0.4885
2024-12-17 20:03:48.013309: Pseudo dice [0.737]
2024-12-17 20:03:48.014100: Epoch time: 269.23 s
2024-12-17 20:03:49.602195: 
2024-12-17 20:03:49.603649: Epoch 129
2024-12-17 20:03:49.604365: Current learning rate: 0.0017
2024-12-17 20:08:17.132443: Validation loss did not improve from -0.54325. Patience: 46/50
2024-12-17 20:08:17.133676: train_loss -0.8097
2024-12-17 20:08:17.134710: val_loss -0.5123
2024-12-17 20:08:17.135588: Pseudo dice [0.7418]
2024-12-17 20:08:17.136411: Epoch time: 267.53 s
2024-12-17 20:08:19.271267: 
2024-12-17 20:08:19.272671: Epoch 130
2024-12-17 20:08:19.273441: Current learning rate: 0.00163
2024-12-17 20:12:38.289498: Validation loss did not improve from -0.54325. Patience: 47/50
2024-12-17 20:12:38.290631: train_loss -0.8082
2024-12-17 20:12:38.291609: val_loss -0.4734
2024-12-17 20:12:38.292394: Pseudo dice [0.7221]
2024-12-17 20:12:38.293253: Epoch time: 259.02 s
2024-12-17 20:12:39.776164: 
2024-12-17 20:12:39.778375: Epoch 131
2024-12-17 20:12:39.780135: Current learning rate: 0.00156
2024-12-17 20:16:57.961211: Validation loss did not improve from -0.54325. Patience: 48/50
2024-12-17 20:16:57.962555: train_loss -0.8078
2024-12-17 20:16:57.963380: val_loss -0.5011
2024-12-17 20:16:57.964234: Pseudo dice [0.7412]
2024-12-17 20:16:57.965183: Epoch time: 258.19 s
2024-12-17 20:16:59.505622: 
2024-12-17 20:16:59.507745: Epoch 132
2024-12-17 20:16:59.508633: Current learning rate: 0.00148
2024-12-17 20:21:02.153258: Validation loss did not improve from -0.54325. Patience: 49/50
2024-12-17 20:21:02.154327: train_loss -0.8095
2024-12-17 20:21:02.155204: val_loss -0.5208
2024-12-17 20:21:02.155863: Pseudo dice [0.7373]
2024-12-17 20:21:02.156593: Epoch time: 242.65 s
2024-12-17 20:21:03.662668: 
2024-12-17 20:21:03.664122: Epoch 133
2024-12-17 20:21:03.665019: Current learning rate: 0.00141
2024-12-17 20:25:16.018328: Validation loss did not improve from -0.54325. Patience: 50/50
2024-12-17 20:25:16.020047: train_loss -0.811
2024-12-17 20:25:16.021185: val_loss -0.4969
2024-12-17 20:25:16.021954: Pseudo dice [0.7349]
2024-12-17 20:25:16.022826: Epoch time: 252.36 s
2024-12-17 20:25:17.635422: 
2024-12-17 20:25:17.636881: Epoch 134
2024-12-17 20:25:17.637806: Current learning rate: 0.00133
2024-12-17 20:29:22.423169: Validation loss did not improve from -0.54325. Patience: 51/50
2024-12-17 20:29:22.425694: train_loss -0.8101
2024-12-17 20:29:22.426785: val_loss -0.4656
2024-12-17 20:29:22.427690: Pseudo dice [0.7231]
2024-12-17 20:29:22.428605: Epoch time: 244.79 s
2024-12-17 20:29:24.445849: 
2024-12-17 20:29:24.447296: Epoch 135
2024-12-17 20:29:24.448249: Current learning rate: 0.00126
2024-12-17 20:33:38.931729: Validation loss did not improve from -0.54325. Patience: 52/50
2024-12-17 20:33:38.932766: train_loss -0.8112
2024-12-17 20:33:38.933678: val_loss -0.4915
2024-12-17 20:33:38.934408: Pseudo dice [0.7311]
2024-12-17 20:33:38.935218: Epoch time: 254.49 s
2024-12-17 20:33:40.461808: 
2024-12-17 20:33:40.463325: Epoch 136
2024-12-17 20:33:40.464184: Current learning rate: 0.00118
2024-12-17 20:38:00.269361: Validation loss did not improve from -0.54325. Patience: 53/50
2024-12-17 20:38:00.270469: train_loss -0.8086
2024-12-17 20:38:00.271563: val_loss -0.5079
2024-12-17 20:38:00.272364: Pseudo dice [0.7405]
2024-12-17 20:38:00.273175: Epoch time: 259.81 s
2024-12-17 20:38:01.866320: 
2024-12-17 20:38:01.867841: Epoch 137
2024-12-17 20:38:01.868666: Current learning rate: 0.00111
2024-12-17 20:42:25.290181: Validation loss did not improve from -0.54325. Patience: 54/50
2024-12-17 20:42:25.290997: train_loss -0.8082
2024-12-17 20:42:25.291886: val_loss -0.4896
2024-12-17 20:42:25.292722: Pseudo dice [0.7323]
2024-12-17 20:42:25.293722: Epoch time: 263.43 s
2024-12-17 20:42:27.315526: 
2024-12-17 20:42:27.316949: Epoch 138
2024-12-17 20:42:27.318059: Current learning rate: 0.00103
2024-12-17 20:46:48.609848: Validation loss did not improve from -0.54325. Patience: 55/50
2024-12-17 20:46:48.611078: train_loss -0.8092
2024-12-17 20:46:48.612175: val_loss -0.5006
2024-12-17 20:46:48.613009: Pseudo dice [0.74]
2024-12-17 20:46:48.613767: Epoch time: 261.3 s
2024-12-17 20:46:50.141800: 
2024-12-17 20:46:50.143134: Epoch 139
2024-12-17 20:46:50.143865: Current learning rate: 0.00095
2024-12-17 20:51:09.569143: Validation loss did not improve from -0.54325. Patience: 56/50
2024-12-17 20:51:09.570180: train_loss -0.8111
2024-12-17 20:51:09.571098: val_loss -0.5073
2024-12-17 20:51:09.571977: Pseudo dice [0.7397]
2024-12-17 20:51:09.572927: Epoch time: 259.43 s
2024-12-17 20:51:11.535029: 
2024-12-17 20:51:11.536361: Epoch 140
2024-12-17 20:51:11.537180: Current learning rate: 0.00087
2024-12-17 20:55:36.184024: Validation loss did not improve from -0.54325. Patience: 57/50
2024-12-17 20:55:36.184940: train_loss -0.8133
2024-12-17 20:55:36.185917: val_loss -0.4829
2024-12-17 20:55:36.186701: Pseudo dice [0.7332]
2024-12-17 20:55:36.187487: Epoch time: 264.65 s
2024-12-17 20:55:37.749665: 
2024-12-17 20:55:37.750911: Epoch 141
2024-12-17 20:55:37.751768: Current learning rate: 0.00079
2024-12-17 20:59:56.310133: Validation loss did not improve from -0.54325. Patience: 58/50
2024-12-17 20:59:56.311151: train_loss -0.8116
2024-12-17 20:59:56.312054: val_loss -0.4901
2024-12-17 20:59:56.312883: Pseudo dice [0.7303]
2024-12-17 20:59:56.313583: Epoch time: 258.56 s
2024-12-17 20:59:57.831004: 
2024-12-17 20:59:57.832294: Epoch 142
2024-12-17 20:59:57.833088: Current learning rate: 0.00071
2024-12-17 21:04:23.247686: Validation loss did not improve from -0.54325. Patience: 59/50
2024-12-17 21:04:23.248747: train_loss -0.8118
2024-12-17 21:04:23.249682: val_loss -0.5104
2024-12-17 21:04:23.250455: Pseudo dice [0.7371]
2024-12-17 21:04:23.251267: Epoch time: 265.42 s
2024-12-17 21:04:24.678951: 
2024-12-17 21:04:24.680466: Epoch 143
2024-12-17 21:04:24.681304: Current learning rate: 0.00063
2024-12-17 21:10:48.170079: Validation loss did not improve from -0.54325. Patience: 60/50
2024-12-17 21:10:48.171067: train_loss -0.8163
2024-12-17 21:10:48.171747: val_loss -0.4987
2024-12-17 21:10:48.172536: Pseudo dice [0.731]
2024-12-17 21:10:48.173519: Epoch time: 383.49 s
2024-12-17 21:10:49.609333: 
2024-12-17 21:10:49.611570: Epoch 144
2024-12-17 21:10:49.612939: Current learning rate: 0.00055
2024-12-17 21:17:34.849392: Validation loss did not improve from -0.54325. Patience: 61/50
2024-12-17 21:17:34.850604: train_loss -0.8133
2024-12-17 21:17:34.851559: val_loss -0.4803
2024-12-17 21:17:34.852335: Pseudo dice [0.7296]
2024-12-17 21:17:34.853066: Epoch time: 405.24 s
2024-12-17 21:17:36.718267: 
2024-12-17 21:17:36.719627: Epoch 145
2024-12-17 21:17:36.720462: Current learning rate: 0.00047
2024-12-17 21:23:54.496494: Validation loss did not improve from -0.54325. Patience: 62/50
2024-12-17 21:23:54.497691: train_loss -0.8129
2024-12-17 21:23:54.498492: val_loss -0.5013
2024-12-17 21:23:54.499160: Pseudo dice [0.7408]
2024-12-17 21:23:54.500070: Epoch time: 377.78 s
2024-12-17 21:23:55.924278: 
2024-12-17 21:23:55.939938: Epoch 146
2024-12-17 21:23:55.941048: Current learning rate: 0.00038
2024-12-17 21:29:59.169425: Validation loss did not improve from -0.54325. Patience: 63/50
2024-12-17 21:29:59.170445: train_loss -0.8141
2024-12-17 21:29:59.171138: val_loss -0.5248
2024-12-17 21:29:59.171785: Pseudo dice [0.7479]
2024-12-17 21:29:59.172427: Epoch time: 363.25 s
2024-12-17 21:30:00.612785: 
2024-12-17 21:30:00.614211: Epoch 147
2024-12-17 21:30:00.615729: Current learning rate: 0.0003
2024-12-17 21:36:07.547448: Validation loss did not improve from -0.54325. Patience: 64/50
2024-12-17 21:36:07.548672: train_loss -0.8138
2024-12-17 21:36:07.549469: val_loss -0.5175
2024-12-17 21:36:07.550217: Pseudo dice [0.7491]
2024-12-17 21:36:07.551042: Epoch time: 366.94 s
2024-12-17 21:36:07.551862: Yayy! New best EMA pseudo Dice: 0.737
2024-12-17 21:36:09.556800: 
2024-12-17 21:36:09.558027: Epoch 148
2024-12-17 21:36:09.558943: Current learning rate: 0.00021
2024-12-17 21:43:37.197984: Validation loss did not improve from -0.54325. Patience: 65/50
2024-12-17 21:43:37.198701: train_loss -0.8138
2024-12-17 21:43:37.199488: val_loss -0.4864
2024-12-17 21:43:37.200137: Pseudo dice [0.7255]
2024-12-17 21:43:37.200790: Epoch time: 447.64 s
2024-12-17 21:43:39.088468: 
2024-12-17 21:43:39.089878: Epoch 149
2024-12-17 21:43:39.090842: Current learning rate: 0.00011
2024-12-17 21:51:08.051228: Validation loss did not improve from -0.54325. Patience: 66/50
2024-12-17 21:51:08.052244: train_loss -0.8138
2024-12-17 21:51:08.053099: val_loss -0.491
2024-12-17 21:51:08.053861: Pseudo dice [0.7285]
2024-12-17 21:51:08.054664: Epoch time: 448.97 s
2024-12-17 21:51:09.941075: Training done.
2024-12-17 21:51:10.100387: Using splits from existing split file: /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset309_Sohee_Calcium_OCT_CrossValidation/splits_final_80.json
2024-12-17 21:51:10.102627: The split file contains 5 splits.
2024-12-17 21:51:10.103609: Desired fold for training: 0
2024-12-17 21:51:10.104551: This split has 6 training and 4 validation cases.
2024-12-17 21:51:10.105642: predicting 101-045
2024-12-17 21:51:10.134108: 101-045, shape torch.Size([1, 375, 498, 498]), rank 0
2024-12-17 21:53:22.196841: predicting 701-013
2024-12-17 21:53:22.214324: 701-013, shape torch.Size([1, 375, 498, 498]), rank 0
2024-12-17 21:55:31.309469: predicting 704-003
2024-12-17 21:55:31.323996: 704-003, shape torch.Size([1, 375, 498, 498]), rank 0
2024-12-17 21:57:27.963402: predicting 706-005
2024-12-17 21:57:27.979438: 706-005, shape torch.Size([1, 375, 498, 498]), rank 0
2024-12-17 21:59:44.485558: Validation complete
2024-12-17 21:59:44.486476: Mean Validation Dice:  0.738269137175319

############################
INFO: You are using the old nnU-Net default plans. We have updated our recommendations. Please consider using those instead! Read more here: https://github.com/MIC-DKFZ/nnUNet/blob/master/documentation/resenc_presets.md
############################

Using device: cuda:0

#######################################################################
Please cite the following paper when using nnU-Net:
Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.
#######################################################################

2024-12-17 21:59:50.415751: Using torch.compile...

############################
INFO: You are using the old nnU-Net default plans. We have updated our recommendations. Please consider using those instead! Read more here: https://github.com/MIC-DKFZ/nnUNet/blob/master/documentation/resenc_presets.md
############################

Using device: cuda:0

#######################################################################
Please cite the following paper when using nnU-Net:
Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.
#######################################################################

2024-12-17 21:59:50.428814: Using torch.compile...
/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
################### Loading pretrained weights from file  /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_results/Dataset300_Lumen_and_Wall_OCT/nnUNetTrainer__nnUNetPreprocessPlans__3d_fullres/fold_all/checkpoint_best.pth ###################
Below is the list of overlapping blocks in pretrained model and nnUNet architecture:
encoder.stages.0.0.convs.0.conv.weight shape torch.Size([32, 1, 3, 3, 3])
encoder.stages.0.0.convs.0.conv.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.norm.weight shape torch.Size([32])
encoder.stages.0.0.convs.0.norm.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.0.weight shape torch.Size([32, 1, 3, 3, 3])
encoder.stages.0.0.convs.0.all_modules.0.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.1.weight shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.1.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
encoder.stages.0.0.convs.1.conv.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.norm.weight shape torch.Size([32])
encoder.stages.0.0.convs.1.norm.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
encoder.stages.0.0.convs.1.all_modules.0.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.1.weight shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.1.bias shape torch.Size([32])
encoder.stages.1.0.convs.0.conv.weight shape torch.Size([64, 32, 3, 3, 3])
encoder.stages.1.0.convs.0.conv.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.norm.weight shape torch.Size([64])
encoder.stages.1.0.convs.0.norm.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.0.weight shape torch.Size([64, 32, 3, 3, 3])
encoder.stages.1.0.convs.0.all_modules.0.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.1.weight shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.1.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
encoder.stages.1.0.convs.1.conv.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.norm.weight shape torch.Size([64])
encoder.stages.1.0.convs.1.norm.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
encoder.stages.1.0.convs.1.all_modules.0.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.1.weight shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.1.bias shape torch.Size([64])
encoder.stages.2.0.convs.0.conv.weight shape torch.Size([128, 64, 3, 3, 3])
encoder.stages.2.0.convs.0.conv.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.norm.weight shape torch.Size([128])
encoder.stages.2.0.convs.0.norm.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.0.weight shape torch.Size([128, 64, 3, 3, 3])
encoder.stages.2.0.convs.0.all_modules.0.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.1.weight shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.1.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
encoder.stages.2.0.convs.1.conv.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.norm.weight shape torch.Size([128])
encoder.stages.2.0.convs.1.norm.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
encoder.stages.2.0.convs.1.all_modules.0.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.1.weight shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.1.bias shape torch.Size([128])
encoder.stages.3.0.convs.0.conv.weight shape torch.Size([256, 128, 3, 3, 3])
encoder.stages.3.0.convs.0.conv.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.norm.weight shape torch.Size([256])
encoder.stages.3.0.convs.0.norm.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.0.weight shape torch.Size([256, 128, 3, 3, 3])
encoder.stages.3.0.convs.0.all_modules.0.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.1.weight shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.1.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
encoder.stages.3.0.convs.1.conv.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.norm.weight shape torch.Size([256])
encoder.stages.3.0.convs.1.norm.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
encoder.stages.3.0.convs.1.all_modules.0.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.1.weight shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.1.bias shape torch.Size([256])
encoder.stages.4.0.convs.0.conv.weight shape torch.Size([320, 256, 3, 3, 3])
encoder.stages.4.0.convs.0.conv.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.norm.weight shape torch.Size([320])
encoder.stages.4.0.convs.0.norm.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.0.weight shape torch.Size([320, 256, 3, 3, 3])
encoder.stages.4.0.convs.0.all_modules.0.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.1.weight shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.1.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.4.0.convs.1.conv.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.norm.weight shape torch.Size([320])
encoder.stages.4.0.convs.1.norm.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.4.0.convs.1.all_modules.0.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.1.weight shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.1.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.0.conv.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.norm.weight shape torch.Size([320])
encoder.stages.5.0.convs.0.norm.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.0.all_modules.0.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.1.weight shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.1.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.1.conv.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.norm.weight shape torch.Size([320])
encoder.stages.5.0.convs.1.norm.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.1.all_modules.0.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.1.weight shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.0.0.convs.0.conv.weight shape torch.Size([32, 1, 3, 3, 3])
decoder.encoder.stages.0.0.convs.0.conv.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.norm.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.norm.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.0.weight shape torch.Size([32, 1, 3, 3, 3])
decoder.encoder.stages.0.0.convs.0.all_modules.0.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.1.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.1.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.encoder.stages.0.0.convs.1.conv.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.norm.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.norm.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.encoder.stages.0.0.convs.1.all_modules.0.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.1.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.1.bias shape torch.Size([32])
decoder.encoder.stages.1.0.convs.0.conv.weight shape torch.Size([64, 32, 3, 3, 3])
decoder.encoder.stages.1.0.convs.0.conv.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.norm.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.norm.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.0.weight shape torch.Size([64, 32, 3, 3, 3])
decoder.encoder.stages.1.0.convs.0.all_modules.0.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.1.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.1.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.encoder.stages.1.0.convs.1.conv.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.norm.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.norm.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.encoder.stages.1.0.convs.1.all_modules.0.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.1.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.1.bias shape torch.Size([64])
decoder.encoder.stages.2.0.convs.0.conv.weight shape torch.Size([128, 64, 3, 3, 3])
decoder.encoder.stages.2.0.convs.0.conv.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.norm.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.norm.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.0.weight shape torch.Size([128, 64, 3, 3, 3])
decoder.encoder.stages.2.0.convs.0.all_modules.0.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.1.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.1.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.encoder.stages.2.0.convs.1.conv.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.norm.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.norm.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.encoder.stages.2.0.convs.1.all_modules.0.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.1.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.1.bias shape torch.Size([128])
decoder.encoder.stages.3.0.convs.0.conv.weight shape torch.Size([256, 128, 3, 3, 3])
decoder.encoder.stages.3.0.convs.0.conv.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.norm.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.norm.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.0.weight shape torch.Size([256, 128, 3, 3, 3])
decoder.encoder.stages.3.0.convs.0.all_modules.0.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.1.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.1.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.encoder.stages.3.0.convs.1.conv.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.norm.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.norm.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.encoder.stages.3.0.convs.1.all_modules.0.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.1.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.1.bias shape torch.Size([256])
decoder.encoder.stages.4.0.convs.0.conv.weight shape torch.Size([320, 256, 3, 3, 3])
decoder.encoder.stages.4.0.convs.0.conv.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.norm.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.norm.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.0.weight shape torch.Size([320, 256, 3, 3, 3])
decoder.encoder.stages.4.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.4.0.convs.1.conv.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.norm.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.norm.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.4.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.0.conv.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.norm.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.norm.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.1.conv.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.norm.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.norm.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.stages.0.convs.0.conv.weight shape torch.Size([320, 640, 3, 3, 3])
decoder.stages.0.convs.0.conv.bias shape torch.Size([320])
decoder.stages.0.convs.0.norm.weight shape torch.Size([320])
decoder.stages.0.convs.0.norm.bias shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.0.weight shape torch.Size([320, 640, 3, 3, 3])
decoder.stages.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.stages.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.stages.0.convs.1.conv.bias shape torch.Size([320])
decoder.stages.0.convs.1.norm.weight shape torch.Size([320])
decoder.stages.0.convs.1.norm.bias shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.stages.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.stages.1.convs.0.conv.weight shape torch.Size([256, 512, 3, 3, 3])
decoder.stages.1.convs.0.conv.bias shape torch.Size([256])
decoder.stages.1.convs.0.norm.weight shape torch.Size([256])
decoder.stages.1.convs.0.norm.bias shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.0.weight shape torch.Size([256, 512, 3, 3, 3])
decoder.stages.1.convs.0.all_modules.0.bias shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.1.weight shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.1.bias shape torch.Size([256])
decoder.stages.1.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.stages.1.convs.1.conv.bias shape torch.Size([256])
decoder.stages.1.convs.1.norm.weight shape torch.Size([256])
decoder.stages.1.convs.1.norm.bias shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.stages.1.convs.1.all_modules.0.bias shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.1.weight shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.1.bias shape torch.Size([256])
decoder.stages.2.convs.0.conv.weight shape torch.Size([128, 256, 3, 3, 3])
decoder.stages.2.convs.0.conv.bias shape torch.Size([128])
decoder.stages.2.convs.0.norm.weight shape torch.Size([128])
decoder.stages.2.convs.0.norm.bias shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.0.weight shape torch.Size([128, 256, 3, 3, 3])
decoder.stages.2.convs.0.all_modules.0.bias shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.1.weight shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.1.bias shape torch.Size([128])
decoder.stages.2.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.stages.2.convs.1.conv.bias shape torch.Size([128])
decoder.stages.2.convs.1.norm.weight shape torch.Size([128])
decoder.stages.2.convs.1.norm.bias shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.stages.2.convs.1.all_modules.0.bias shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.1.weight shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.1.bias shape torch.Size([128])
decoder.stages.3.convs.0.conv.weight shape torch.Size([64, 128, 3, 3, 3])
decoder.stages.3.convs.0.conv.bias shape torch.Size([64])
decoder.stages.3.convs.0.norm.weight shape torch.Size([64])
decoder.stages.3.convs.0.norm.bias shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.0.weight shape torch.Size([64, 128, 3, 3, 3])
decoder.stages.3.convs.0.all_modules.0.bias shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.1.weight shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.1.bias shape torch.Size([64])
decoder.stages.3.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.stages.3.convs.1.conv.bias shape torch.Size([64])
decoder.stages.3.convs.1.norm.weight shape torch.Size([64])
decoder.stages.3.convs.1.norm.bias shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.stages.3.convs.1.all_modules.0.bias shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.1.weight shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.1.bias shape torch.Size([64])
decoder.stages.4.convs.0.conv.weight shape torch.Size([32, 64, 3, 3, 3])
decoder.stages.4.convs.0.conv.bias shape torch.Size([32])
decoder.stages.4.convs.0.norm.weight shape torch.Size([32])
decoder.stages.4.convs.0.norm.bias shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.0.weight shape torch.Size([32, 64, 3, 3, 3])
decoder.stages.4.convs.0.all_modules.0.bias shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.1.weight shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.1.bias shape torch.Size([32])
decoder.stages.4.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.stages.4.convs.1.conv.bias shape torch.Size([32])
decoder.stages.4.convs.1.norm.weight shape torch.Size([32])
decoder.stages.4.convs.1.norm.bias shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.stages.4.convs.1.all_modules.0.bias shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.1.weight shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.1.bias shape torch.Size([32])
decoder.transpconvs.0.weight shape torch.Size([320, 320, 1, 2, 2])
decoder.transpconvs.0.bias shape torch.Size([320])
decoder.transpconvs.1.weight shape torch.Size([320, 256, 2, 2, 2])
decoder.transpconvs.1.bias shape torch.Size([256])
decoder.transpconvs.2.weight shape torch.Size([256, 128, 2, 2, 2])
decoder.transpconvs.2.bias shape torch.Size([128])
decoder.transpconvs.3.weight shape torch.Size([128, 64, 2, 2, 2])
decoder.transpconvs.3.bias shape torch.Size([64])
decoder.transpconvs.4.weight shape torch.Size([64, 32, 2, 2, 2])
decoder.transpconvs.4.bias shape torch.Size([32])
################### Done ###################
2024-12-17 21:59:52.920709: do_dummy_2d_data_aug: True
2024-12-17 21:59:52.922856: Using splits from existing split file: /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset309_Sohee_Calcium_OCT_CrossValidation/splits_final_80.json
2024-12-17 21:59:52.924618: The split file contains 5 splits.
2024-12-17 21:59:52.925581: Desired fold for training: 2
2024-12-17 21:59:52.926451: This split has 6 training and 3 validation cases.
################### Loading pretrained weights from file  /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_results/Dataset300_Lumen_and_Wall_OCT/nnUNetTrainer__nnUNetPreprocessPlans__3d_fullres/fold_all/checkpoint_best.pth ###################
Below is the list of overlapping blocks in pretrained model and nnUNet architecture:
encoder.stages.0.0.convs.0.conv.weight shape torch.Size([32, 1, 3, 3, 3])
encoder.stages.0.0.convs.0.conv.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.norm.weight shape torch.Size([32])
encoder.stages.0.0.convs.0.norm.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.0.weight shape torch.Size([32, 1, 3, 3, 3])
encoder.stages.0.0.convs.0.all_modules.0.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.1.weight shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.1.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
encoder.stages.0.0.convs.1.conv.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.norm.weight shape torch.Size([32])
encoder.stages.0.0.convs.1.norm.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
encoder.stages.0.0.convs.1.all_modules.0.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.1.weight shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.1.bias shape torch.Size([32])
encoder.stages.1.0.convs.0.conv.weight shape torch.Size([64, 32, 3, 3, 3])
encoder.stages.1.0.convs.0.conv.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.norm.weight shape torch.Size([64])
encoder.stages.1.0.convs.0.norm.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.0.weight shape torch.Size([64, 32, 3, 3, 3])
encoder.stages.1.0.convs.0.all_modules.0.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.1.weight shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.1.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
encoder.stages.1.0.convs.1.conv.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.norm.weight shape torch.Size([64])
encoder.stages.1.0.convs.1.norm.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
encoder.stages.1.0.convs.1.all_modules.0.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.1.weight shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.1.bias shape torch.Size([64])
encoder.stages.2.0.convs.0.conv.weight shape torch.Size([128, 64, 3, 3, 3])
encoder.stages.2.0.convs.0.conv.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.norm.weight shape torch.Size([128])
encoder.stages.2.0.convs.0.norm.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.0.weight shape torch.Size([128, 64, 3, 3, 3])
encoder.stages.2.0.convs.0.all_modules.0.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.1.weight shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.1.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
encoder.stages.2.0.convs.1.conv.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.norm.weight shape torch.Size([128])
encoder.stages.2.0.convs.1.norm.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
encoder.stages.2.0.convs.1.all_modules.0.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.1.weight shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.1.bias shape torch.Size([128])
encoder.stages.3.0.convs.0.conv.weight shape torch.Size([256, 128, 3, 3, 3])
encoder.stages.3.0.convs.0.conv.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.norm.weight shape torch.Size([256])
encoder.stages.3.0.convs.0.norm.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.0.weight shape torch.Size([256, 128, 3, 3, 3])
encoder.stages.3.0.convs.0.all_modules.0.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.1.weight shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.1.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
encoder.stages.3.0.convs.1.conv.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.norm.weight shape torch.Size([256])
encoder.stages.3.0.convs.1.norm.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
encoder.stages.3.0.convs.1.all_modules.0.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.1.weight shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.1.bias shape torch.Size([256])
encoder.stages.4.0.convs.0.conv.weight shape torch.Size([320, 256, 3, 3, 3])
encoder.stages.4.0.convs.0.conv.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.norm.weight shape torch.Size([320])
encoder.stages.4.0.convs.0.norm.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.0.weight shape torch.Size([320, 256, 3, 3, 3])
encoder.stages.4.0.convs.0.all_modules.0.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.1.weight shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.1.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.4.0.convs.1.conv.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.norm.weight shape torch.Size([320])
encoder.stages.4.0.convs.1.norm.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.4.0.convs.1.all_modules.0.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.1.weight shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.1.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.0.conv.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.norm.weight shape torch.Size([320])
encoder.stages.5.0.convs.0.norm.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.0.all_modules.0.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.1.weight shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.1.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.1.conv.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.norm.weight shape torch.Size([320])
encoder.stages.5.0.convs.1.norm.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.1.all_modules.0.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.1.weight shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.0.0.convs.0.conv.weight shape torch.Size([32, 1, 3, 3, 3])
decoder.encoder.stages.0.0.convs.0.conv.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.norm.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.norm.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.0.weight shape torch.Size([32, 1, 3, 3, 3])
decoder.encoder.stages.0.0.convs.0.all_modules.0.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.1.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.1.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.encoder.stages.0.0.convs.1.conv.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.norm.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.norm.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.encoder.stages.0.0.convs.1.all_modules.0.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.1.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.1.bias shape torch.Size([32])
decoder.encoder.stages.1.0.convs.0.conv.weight shape torch.Size([64, 32, 3, 3, 3])
decoder.encoder.stages.1.0.convs.0.conv.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.norm.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.norm.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.0.weight shape torch.Size([64, 32, 3, 3, 3])
decoder.encoder.stages.1.0.convs.0.all_modules.0.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.1.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.1.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.encoder.stages.1.0.convs.1.conv.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.norm.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.norm.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.encoder.stages.1.0.convs.1.all_modules.0.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.1.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.1.bias shape torch.Size([64])
decoder.encoder.stages.2.0.convs.0.conv.weight shape torch.Size([128, 64, 3, 3, 3])
decoder.encoder.stages.2.0.convs.0.conv.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.norm.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.norm.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.0.weight shape torch.Size([128, 64, 3, 3, 3])
decoder.encoder.stages.2.0.convs.0.all_modules.0.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.1.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.1.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.encoder.stages.2.0.convs.1.conv.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.norm.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.norm.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.encoder.stages.2.0.convs.1.all_modules.0.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.1.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.1.bias shape torch.Size([128])
decoder.encoder.stages.3.0.convs.0.conv.weight shape torch.Size([256, 128, 3, 3, 3])
decoder.encoder.stages.3.0.convs.0.conv.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.norm.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.norm.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.0.weight shape torch.Size([256, 128, 3, 3, 3])
decoder.encoder.stages.3.0.convs.0.all_modules.0.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.1.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.1.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.encoder.stages.3.0.convs.1.conv.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.norm.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.norm.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.encoder.stages.3.0.convs.1.all_modules.0.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.1.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.1.bias shape torch.Size([256])
decoder.encoder.stages.4.0.convs.0.conv.weight shape torch.Size([320, 256, 3, 3, 3])
decoder.encoder.stages.4.0.convs.0.conv.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.norm.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.norm.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.0.weight shape torch.Size([320, 256, 3, 3, 3])
decoder.encoder.stages.4.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.4.0.convs.1.conv.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.norm.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.norm.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.4.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.0.conv.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.norm.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.norm.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.1.conv.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.norm.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.norm.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.stages.0.convs.0.conv.weight shape torch.Size([320, 640, 3, 3, 3])
decoder.stages.0.convs.0.conv.bias shape torch.Size([320])
decoder.stages.0.convs.0.norm.weight shape torch.Size([320])
decoder.stages.0.convs.0.norm.bias shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.0.weight shape torch.Size([320, 640, 3, 3, 3])
decoder.stages.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.stages.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.stages.0.convs.1.conv.bias shape torch.Size([320])
decoder.stages.0.convs.1.norm.weight shape torch.Size([320])
decoder.stages.0.convs.1.norm.bias shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.stages.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.stages.1.convs.0.conv.weight shape torch.Size([256, 512, 3, 3, 3])
decoder.stages.1.convs.0.conv.bias shape torch.Size([256])
decoder.stages.1.convs.0.norm.weight shape torch.Size([256])
decoder.stages.1.convs.0.norm.bias shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.0.weight shape torch.Size([256, 512, 3, 3, 3])
decoder.stages.1.convs.0.all_modules.0.bias shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.1.weight shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.1.bias shape torch.Size([256])
decoder.stages.1.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.stages.1.convs.1.conv.bias shape torch.Size([256])
decoder.stages.1.convs.1.norm.weight shape torch.Size([256])
decoder.stages.1.convs.1.norm.bias shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.stages.1.convs.1.all_modules.0.bias shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.1.weight shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.1.bias shape torch.Size([256])
decoder.stages.2.convs.0.conv.weight shape torch.Size([128, 256, 3, 3, 3])
decoder.stages.2.convs.0.conv.bias shape torch.Size([128])
decoder.stages.2.convs.0.norm.weight shape torch.Size([128])
decoder.stages.2.convs.0.norm.bias shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.0.weight shape torch.Size([128, 256, 3, 3, 3])
decoder.stages.2.convs.0.all_modules.0.bias shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.1.weight shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.1.bias shape torch.Size([128])
decoder.stages.2.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.stages.2.convs.1.conv.bias shape torch.Size([128])
decoder.stages.2.convs.1.norm.weight shape torch.Size([128])
decoder.stages.2.convs.1.norm.bias shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.stages.2.convs.1.all_modules.0.bias shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.1.weight shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.1.bias shape torch.Size([128])
decoder.stages.3.convs.0.conv.weight shape torch.Size([64, 128, 3, 3, 3])
decoder.stages.3.convs.0.conv.bias shape torch.Size([64])
decoder.stages.3.convs.0.norm.weight shape torch.Size([64])
decoder.stages.3.convs.0.norm.bias shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.0.weight shape torch.Size([64, 128, 3, 3, 3])
decoder.stages.3.convs.0.all_modules.0.bias shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.1.weight shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.1.bias shape torch.Size([64])
decoder.stages.3.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.stages.3.convs.1.conv.bias shape torch.Size([64])
decoder.stages.3.convs.1.norm.weight shape torch.Size([64])
decoder.stages.3.convs.1.norm.bias shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.stages.3.convs.1.all_modules.0.bias shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.1.weight shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.1.bias shape torch.Size([64])
decoder.stages.4.convs.0.conv.weight shape torch.Size([32, 64, 3, 3, 3])
decoder.stages.4.convs.0.conv.bias shape torch.Size([32])
decoder.stages.4.convs.0.norm.weight shape torch.Size([32])
decoder.stages.4.convs.0.norm.bias shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.0.weight shape torch.Size([32, 64, 3, 3, 3])
decoder.stages.4.convs.0.all_modules.0.bias shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.1.weight shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.1.bias shape torch.Size([32])
decoder.stages.4.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.stages.4.convs.1.conv.bias shape torch.Size([32])
decoder.stages.4.convs.1.norm.weight shape torch.Size([32])
decoder.stages.4.convs.1.norm.bias shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.stages.4.convs.1.all_modules.0.bias shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.1.weight shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.1.bias shape torch.Size([32])
decoder.transpconvs.0.weight shape torch.Size([320, 320, 1, 2, 2])
decoder.transpconvs.0.bias shape torch.Size([320])
decoder.transpconvs.1.weight shape torch.Size([320, 256, 2, 2, 2])
decoder.transpconvs.1.bias shape torch.Size([256])
decoder.transpconvs.2.weight shape torch.Size([256, 128, 2, 2, 2])
decoder.transpconvs.2.bias shape torch.Size([128])
decoder.transpconvs.3.weight shape torch.Size([128, 64, 2, 2, 2])
decoder.transpconvs.3.bias shape torch.Size([64])
decoder.transpconvs.4.weight shape torch.Size([64, 32, 2, 2, 2])
decoder.transpconvs.4.bias shape torch.Size([32])
################### Done ###################
2024-12-17 21:59:52.920696: do_dummy_2d_data_aug: True
2024-12-17 21:59:52.922260: Using splits from existing split file: /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset309_Sohee_Calcium_OCT_CrossValidation/splits_final_80.json
2024-12-17 21:59:52.924350: The split file contains 5 splits.
2024-12-17 21:59:52.925188: Desired fold for training: 3
2024-12-17 21:59:52.925916: This split has 6 training and 3 validation cases.
using pin_memory on device 0
using pin_memory on device 0
using pin_memory on device 0

This is the configuration used by this training:
Configuration name: 3d_32x160x128_b10
 {'data_identifier': 'nnUNetPlans_3d_fullres', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 10, 'patch_size': [32, 160, 128], 'median_image_size_in_voxels': [375.0, 498.0, 498.0], 'spacing': [1.0, 1.0, 1.0], 'normalization_schemes': ['ZScoreNormalization'], 'use_mask_for_norm': [False], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'architecture': {'network_class_name': 'dynamic_network_architectures.architectures.unet.PlainConvUNet', 'arch_kwargs': {'n_stages': 6, 'features_per_stage': [32, 64, 128, 256, 320, 320], 'conv_op': 'torch.nn.modules.conv.Conv3d', 'kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]], 'strides': [[1, 1, 1], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [1, 2, 2]], 'n_conv_per_stage': [2, 2, 2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2, 2, 2], 'conv_bias': True, 'norm_op': 'torch.nn.modules.instancenorm.InstanceNorm3d', 'norm_op_kwargs': {'eps': 1e-05, 'affine': True}, 'dropout_op': None, 'dropout_op_kwargs': None, 'nonlin': 'torch.nn.LeakyReLU', 'nonlin_kwargs': {'inplace': True}, 'deep_supervision': True}, '_kw_requires_import': ['conv_op', 'norm_op', 'dropout_op', 'nonlin']}, 'batch_dice': True, 'inherits_from': '3d_fullres'} 

These are the global plan.json settings:
 {'dataset_name': 'Dataset309_Sohee_Calcium_OCT_CrossValidation', 'plans_name': 'nnUNetPlans', 'original_median_spacing_after_transp': [1.0, 1.0, 1.0], 'original_median_shape_after_transp': [375, 498, 498], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 0.4977976381778717, 'mean': 0.13507525622844696, 'median': 0.09599608182907104, 'min': 0.0, 'percentile_00_5': 0.014754901640117168, 'percentile_99_5': 0.4977976381778717, 'std': 0.12025152146816254}}} 

2024-12-17 22:00:15.368037: unpacking dataset...
using pin_memory on device 0

This is the configuration used by this training:
Configuration name: 3d_32x160x128_b10
 {'data_identifier': 'nnUNetPlans_3d_fullres', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 10, 'patch_size': [32, 160, 128], 'median_image_size_in_voxels': [375.0, 498.0, 498.0], 'spacing': [1.0, 1.0, 1.0], 'normalization_schemes': ['ZScoreNormalization'], 'use_mask_for_norm': [False], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'architecture': {'network_class_name': 'dynamic_network_architectures.architectures.unet.PlainConvUNet', 'arch_kwargs': {'n_stages': 6, 'features_per_stage': [32, 64, 128, 256, 320, 320], 'conv_op': 'torch.nn.modules.conv.Conv3d', 'kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]], 'strides': [[1, 1, 1], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [1, 2, 2]], 'n_conv_per_stage': [2, 2, 2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2, 2, 2], 'conv_bias': True, 'norm_op': 'torch.nn.modules.instancenorm.InstanceNorm3d', 'norm_op_kwargs': {'eps': 1e-05, 'affine': True}, 'dropout_op': None, 'dropout_op_kwargs': None, 'nonlin': 'torch.nn.LeakyReLU', 'nonlin_kwargs': {'inplace': True}, 'deep_supervision': True}, '_kw_requires_import': ['conv_op', 'norm_op', 'dropout_op', 'nonlin']}, 'batch_dice': True, 'inherits_from': '3d_fullres'} 

These are the global plan.json settings:
 {'dataset_name': 'Dataset309_Sohee_Calcium_OCT_CrossValidation', 'plans_name': 'nnUNetPlans', 'original_median_spacing_after_transp': [1.0, 1.0, 1.0], 'original_median_shape_after_transp': [375, 498, 498], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 0.4977976381778717, 'mean': 0.13507525622844696, 'median': 0.09599608182907104, 'min': 0.0, 'percentile_00_5': 0.014754901640117168, 'percentile_99_5': 0.4977976381778717, 'std': 0.12025152146816254}}} 

2024-12-17 22:00:18.871089: unpacking dataset...
2024-12-17 22:00:23.167515: unpacking done...
2024-12-17 22:00:23.174741: Unable to plot network architecture: nnUNet_compile is enabled!
2024-12-17 22:00:23.231897: 
2024-12-17 22:00:23.233135: Epoch 0
2024-12-17 22:00:23.234072: Current learning rate: 0.01
2024-12-17 22:05:31.056555: Validation loss improved from 1000.00000 to -0.42108! Patience: 0/50
2024-12-17 22:05:31.057974: train_loss -0.3172
2024-12-17 22:05:31.059201: val_loss -0.4211
2024-12-17 22:05:31.060640: Pseudo dice [0.6755]
2024-12-17 22:05:31.061720: Epoch time: 307.83 s
2024-12-17 22:05:31.062818: Yayy! New best EMA pseudo Dice: 0.6755
2024-12-17 22:05:32.976994: 
2024-12-17 22:05:32.978821: Epoch 1
2024-12-17 22:05:32.979779: Current learning rate: 0.00994
2024-12-17 22:10:09.931037: Validation loss improved from -0.42108 to -0.45519! Patience: 0/50
2024-12-17 22:10:09.932157: train_loss -0.4589
2024-12-17 22:10:09.933112: val_loss -0.4552
2024-12-17 22:10:09.933945: Pseudo dice [0.6917]
2024-12-17 22:10:09.934741: Epoch time: 276.96 s
2024-12-17 22:10:09.935483: Yayy! New best EMA pseudo Dice: 0.6771
2024-12-17 22:10:11.805588: 
2024-12-17 22:10:11.806903: Epoch 2
2024-12-17 22:10:11.807731: Current learning rate: 0.00988
2024-12-17 22:14:14.391647: Validation loss improved from -0.45519 to -0.47652! Patience: 0/50
2024-12-17 22:14:14.392693: train_loss -0.4952
2024-12-17 22:14:14.393785: val_loss -0.4765
2024-12-17 22:14:14.394738: Pseudo dice [0.7022]
2024-12-17 22:14:14.395800: Epoch time: 242.59 s
2024-12-17 22:14:14.396718: Yayy! New best EMA pseudo Dice: 0.6796
2024-12-17 22:14:16.369859: 
2024-12-17 22:14:16.371337: Epoch 3
2024-12-17 22:14:16.372245: Current learning rate: 0.00982
2024-12-17 22:18:48.351060: Validation loss improved from -0.47652 to -0.48812! Patience: 0/50
2024-12-17 22:18:48.352008: train_loss -0.521
2024-12-17 22:18:48.353104: val_loss -0.4881
2024-12-17 22:18:48.353884: Pseudo dice [0.7056]
2024-12-17 22:18:48.354672: Epoch time: 271.98 s
2024-12-17 22:18:48.355573: Yayy! New best EMA pseudo Dice: 0.6822
2024-12-17 22:18:50.227477: 
2024-12-17 22:18:50.229042: Epoch 4
2024-12-17 22:18:50.230169: Current learning rate: 0.00976
2024-12-17 22:22:41.501436: Validation loss improved from -0.48812 to -0.51510! Patience: 0/50
2024-12-17 22:22:41.502470: train_loss -0.5394
2024-12-17 22:22:41.503292: val_loss -0.5151
2024-12-17 22:22:41.504077: Pseudo dice [0.7163]
2024-12-17 22:22:41.504947: Epoch time: 231.28 s
2024-12-17 22:22:41.880913: Yayy! New best EMA pseudo Dice: 0.6856
2024-12-17 22:22:43.715981: 
2024-12-17 22:22:43.717504: Epoch 5
2024-12-17 22:22:43.718412: Current learning rate: 0.0097
2024-12-17 22:26:43.947039: Validation loss did not improve from -0.51510. Patience: 1/50
2024-12-17 22:26:43.948108: train_loss -0.5578
2024-12-17 22:26:43.949085: val_loss -0.4745
2024-12-17 22:26:43.949943: Pseudo dice [0.6966]
2024-12-17 22:26:43.950813: Epoch time: 240.23 s
2024-12-17 22:26:43.951656: Yayy! New best EMA pseudo Dice: 0.6867
2024-12-17 22:26:45.779894: 
2024-12-17 22:26:45.781287: Epoch 6
2024-12-17 22:26:45.782278: Current learning rate: 0.00964
2024-12-17 22:30:29.223950: Validation loss improved from -0.51510 to -0.52615! Patience: 1/50
2024-12-17 22:30:29.224959: train_loss -0.5545
2024-12-17 22:30:29.226060: val_loss -0.5262
2024-12-17 22:30:29.227015: Pseudo dice [0.7263]
2024-12-17 22:30:29.227937: Epoch time: 223.45 s
2024-12-17 22:30:29.228912: Yayy! New best EMA pseudo Dice: 0.6907
2024-12-17 22:30:31.180393: 
2024-12-17 22:30:31.182049: Epoch 7
2024-12-17 22:30:31.183129: Current learning rate: 0.00958
2024-12-17 22:34:07.760162: Validation loss did not improve from -0.52615. Patience: 1/50
2024-12-17 22:34:07.761300: train_loss -0.5781
2024-12-17 22:34:07.762138: val_loss -0.5099
2024-12-17 22:34:07.762878: Pseudo dice [0.717]
2024-12-17 22:34:07.763699: Epoch time: 216.58 s
2024-12-17 22:34:07.764358: Yayy! New best EMA pseudo Dice: 0.6933
2024-12-17 22:34:10.254908: 
2024-12-17 22:34:10.256118: Epoch 8
2024-12-17 22:34:10.256882: Current learning rate: 0.00952
2024-12-17 22:38:10.242752: Validation loss did not improve from -0.52615. Patience: 2/50
2024-12-17 22:38:10.243786: train_loss -0.5859
2024-12-17 22:38:10.244585: val_loss -0.5039
2024-12-17 22:38:10.245238: Pseudo dice [0.7052]
2024-12-17 22:38:10.245911: Epoch time: 239.99 s
2024-12-17 22:38:10.246536: Yayy! New best EMA pseudo Dice: 0.6945
2024-12-17 22:38:12.075199: 
2024-12-17 22:38:12.077255: Epoch 9
2024-12-17 22:38:12.078918: Current learning rate: 0.00946
2024-12-17 22:42:08.022974: Validation loss did not improve from -0.52615. Patience: 3/50
2024-12-17 22:42:08.024071: train_loss -0.5842
2024-12-17 22:42:08.024917: val_loss -0.5253
2024-12-17 22:42:08.025854: Pseudo dice [0.7185]
2024-12-17 22:42:08.026700: Epoch time: 235.95 s
2024-12-17 22:42:08.453332: Yayy! New best EMA pseudo Dice: 0.6969
2024-12-17 22:42:10.256011: 
2024-12-17 22:42:10.258178: Epoch 10
2024-12-17 22:42:10.259264: Current learning rate: 0.0094
2024-12-17 22:46:25.859095: Validation loss did not improve from -0.52615. Patience: 4/50
2024-12-17 22:46:25.860603: train_loss -0.5805
2024-12-17 22:46:25.862057: val_loss -0.4995
2024-12-17 22:46:25.862887: Pseudo dice [0.7072]
2024-12-17 22:46:25.863861: Epoch time: 255.61 s
2024-12-17 22:46:25.864655: Yayy! New best EMA pseudo Dice: 0.6979
2024-12-17 22:46:27.591053: 
2024-12-17 22:46:27.592672: Epoch 11
2024-12-17 22:46:27.593636: Current learning rate: 0.00934
2024-12-17 22:50:30.233331: Validation loss improved from -0.52615 to -0.52999! Patience: 4/50
2024-12-17 22:50:30.234450: train_loss -0.6017
2024-12-17 22:50:30.235303: val_loss -0.53
2024-12-17 22:50:30.236078: Pseudo dice [0.7298]
2024-12-17 22:50:30.236791: Epoch time: 242.64 s
2024-12-17 22:50:30.237448: Yayy! New best EMA pseudo Dice: 0.7011
2024-12-17 22:50:32.052837: 
2024-12-17 22:50:32.054540: Epoch 12
2024-12-17 22:50:32.055965: Current learning rate: 0.00928
2024-12-17 22:54:36.365804: Validation loss improved from -0.52999 to -0.53203! Patience: 0/50
2024-12-17 22:54:36.366781: train_loss -0.6089
2024-12-17 22:54:36.367580: val_loss -0.532
2024-12-17 22:54:36.368292: Pseudo dice [0.7266]
2024-12-17 22:54:36.369013: Epoch time: 244.32 s
2024-12-17 22:54:36.369795: Yayy! New best EMA pseudo Dice: 0.7037
2024-12-17 22:54:38.249267: 
2024-12-17 22:54:38.250786: Epoch 13
2024-12-17 22:54:38.251889: Current learning rate: 0.00922
2024-12-17 22:58:59.881155: Validation loss improved from -0.53203 to -0.55050! Patience: 0/50
2024-12-17 22:58:59.882057: train_loss -0.6123
2024-12-17 22:58:59.883190: val_loss -0.5505
2024-12-17 22:58:59.884116: Pseudo dice [0.7389]
2024-12-17 22:58:59.885000: Epoch time: 261.63 s
2024-12-17 22:58:59.885857: Yayy! New best EMA pseudo Dice: 0.7072
2024-12-17 22:59:01.747616: 
2024-12-17 22:59:01.749128: Epoch 14
2024-12-17 22:59:01.750108: Current learning rate: 0.00916
2024-12-17 23:03:43.984973: Validation loss did not improve from -0.55050. Patience: 1/50
2024-12-17 23:03:43.985986: train_loss -0.619
2024-12-17 23:03:43.986955: val_loss -0.5223
2024-12-17 23:03:43.987706: Pseudo dice [0.7191]
2024-12-17 23:03:43.988568: Epoch time: 282.24 s
2024-12-17 23:03:44.409858: Yayy! New best EMA pseudo Dice: 0.7084
2024-12-17 23:03:46.243786: 
2024-12-17 23:03:46.245099: Epoch 15
2024-12-17 23:03:46.245914: Current learning rate: 0.0091
2024-12-17 23:07:45.466877: Validation loss did not improve from -0.55050. Patience: 2/50
2024-12-17 23:07:45.470663: train_loss -0.6225
2024-12-17 23:07:45.472734: val_loss -0.5292
2024-12-17 23:07:45.473755: Pseudo dice [0.7315]
2024-12-17 23:07:45.474858: Epoch time: 239.23 s
2024-12-17 23:07:45.475799: Yayy! New best EMA pseudo Dice: 0.7107
2024-12-17 23:07:47.430010: 
2024-12-17 23:07:47.431419: Epoch 16
2024-12-17 23:07:47.432499: Current learning rate: 0.00903
2024-12-17 23:12:12.054070: Validation loss did not improve from -0.55050. Patience: 3/50
2024-12-17 23:12:12.054958: train_loss -0.6284
2024-12-17 23:12:12.055913: val_loss -0.4882
2024-12-17 23:12:12.056692: Pseudo dice [0.7014]
2024-12-17 23:12:12.057447: Epoch time: 264.63 s
2024-12-17 23:12:13.561505: 
2024-12-17 23:12:13.562878: Epoch 17
2024-12-17 23:12:13.563693: Current learning rate: 0.00897
2024-12-17 23:16:03.558501: Validation loss did not improve from -0.55050. Patience: 4/50
2024-12-17 23:16:03.559650: train_loss -0.6249
2024-12-17 23:16:03.561131: val_loss -0.519
2024-12-17 23:16:03.562175: Pseudo dice [0.7167]
2024-12-17 23:16:03.563171: Epoch time: 230.0 s
2024-12-17 23:16:05.952087: 
2024-12-17 23:16:05.953730: Epoch 18
2024-12-17 23:16:05.954822: Current learning rate: 0.00891
2024-12-17 23:20:17.424640: Validation loss improved from -0.55050 to -0.55521! Patience: 4/50
2024-12-17 23:20:17.426775: train_loss -0.6347
2024-12-17 23:20:17.427749: val_loss -0.5552
2024-12-17 23:20:17.428471: Pseudo dice [0.7421]
2024-12-17 23:20:17.429188: Epoch time: 251.48 s
2024-12-17 23:20:17.430082: Yayy! New best EMA pseudo Dice: 0.7136
2024-12-17 23:20:19.274115: 
2024-12-17 23:20:19.275684: Epoch 19
2024-12-17 23:20:19.276661: Current learning rate: 0.00885
2024-12-17 23:24:21.965572: Validation loss did not improve from -0.55521. Patience: 1/50
2024-12-17 23:24:21.966635: train_loss -0.636
2024-12-17 23:24:21.967527: val_loss -0.5366
2024-12-17 23:24:21.968328: Pseudo dice [0.7393]
2024-12-17 23:24:21.969128: Epoch time: 242.69 s
2024-12-17 23:24:22.349900: Yayy! New best EMA pseudo Dice: 0.7162
2024-12-17 23:24:24.212560: 
2024-12-17 23:24:24.214023: Epoch 20
2024-12-17 23:24:24.214874: Current learning rate: 0.00879
2024-12-17 23:28:56.073600: Validation loss did not improve from -0.55521. Patience: 2/50
2024-12-17 23:28:56.074772: train_loss -0.6499
2024-12-17 23:28:56.075706: val_loss -0.5175
2024-12-17 23:28:56.076518: Pseudo dice [0.7296]
2024-12-17 23:28:56.077302: Epoch time: 271.86 s
2024-12-17 23:28:56.078252: Yayy! New best EMA pseudo Dice: 0.7175
2024-12-17 23:28:58.072836: 
2024-12-17 23:28:58.074338: Epoch 21
2024-12-17 23:28:58.075184: Current learning rate: 0.00873
2024-12-17 23:33:31.763909: Validation loss did not improve from -0.55521. Patience: 3/50
2024-12-17 23:33:31.765125: train_loss -0.646
2024-12-17 23:33:31.766143: val_loss -0.5385
2024-12-17 23:33:31.767071: Pseudo dice [0.7305]
2024-12-17 23:33:31.767887: Epoch time: 273.69 s
2024-12-17 23:33:31.768560: Yayy! New best EMA pseudo Dice: 0.7188
2024-12-17 23:33:33.591801: 
2024-12-17 23:33:33.593152: Epoch 22
2024-12-17 23:33:33.593968: Current learning rate: 0.00867
2024-12-17 23:38:29.505672: Validation loss improved from -0.55521 to -0.55595! Patience: 3/50
2024-12-17 23:38:29.506657: train_loss -0.6472
2024-12-17 23:38:29.507554: val_loss -0.556
2024-12-17 23:38:29.508279: Pseudo dice [0.7429]
2024-12-17 23:38:29.509126: Epoch time: 295.92 s
2024-12-17 23:38:29.509851: Yayy! New best EMA pseudo Dice: 0.7212
2024-12-17 23:38:31.343087: 
2024-12-17 23:38:31.344878: Epoch 23
2024-12-17 23:38:31.346276: Current learning rate: 0.00861
2024-12-17 23:42:40.831955: Validation loss did not improve from -0.55595. Patience: 1/50
2024-12-17 23:42:40.832968: train_loss -0.6598
2024-12-17 23:42:40.833806: val_loss -0.5135
2024-12-17 23:42:40.834521: Pseudo dice [0.7212]
2024-12-17 23:42:40.835322: Epoch time: 249.49 s
2024-12-17 23:42:42.342136: 
2024-12-17 23:42:42.343553: Epoch 24
2024-12-17 23:42:42.344434: Current learning rate: 0.00855
2024-12-17 23:47:37.262394: Validation loss did not improve from -0.55595. Patience: 2/50
2024-12-17 23:47:37.263552: train_loss -0.6648
2024-12-17 23:47:37.264303: val_loss -0.4988
2024-12-17 23:47:37.265041: Pseudo dice [0.7122]
2024-12-17 23:47:37.265797: Epoch time: 294.92 s
2024-12-17 23:47:39.127238: 
2024-12-17 23:47:39.128173: Epoch 25
2024-12-17 23:47:39.129146: Current learning rate: 0.00849
2024-12-17 23:52:05.336164: Validation loss did not improve from -0.55595. Patience: 3/50
2024-12-17 23:52:05.337115: train_loss -0.6657
2024-12-17 23:52:05.338047: val_loss -0.5456
2024-12-17 23:52:05.338935: Pseudo dice [0.7492]
2024-12-17 23:52:05.339806: Epoch time: 266.21 s
2024-12-17 23:52:05.340756: Yayy! New best EMA pseudo Dice: 0.7232
2024-12-17 23:52:07.238213: 
2024-12-17 23:52:07.239618: Epoch 26
2024-12-17 23:52:07.240468: Current learning rate: 0.00843
2024-12-17 23:56:46.753879: Validation loss did not improve from -0.55595. Patience: 4/50
2024-12-17 23:56:46.754910: train_loss -0.6591
2024-12-17 23:56:46.755939: val_loss -0.5067
2024-12-17 23:56:46.756728: Pseudo dice [0.7202]
2024-12-17 23:56:46.757676: Epoch time: 279.52 s
2024-12-17 23:56:48.175225: 
2024-12-17 23:56:48.177171: Epoch 27
2024-12-17 23:56:48.178403: Current learning rate: 0.00836
2024-12-18 00:01:52.002801: Validation loss did not improve from -0.55595. Patience: 5/50
2024-12-18 00:01:52.003662: train_loss -0.6636
2024-12-18 00:01:52.004493: val_loss -0.5499
2024-12-18 00:01:52.005680: Pseudo dice [0.7429]
2024-12-18 00:01:52.006520: Epoch time: 303.83 s
2024-12-18 00:01:52.007323: Yayy! New best EMA pseudo Dice: 0.7249
2024-12-18 00:01:53.840447: 
2024-12-18 00:01:53.841696: Epoch 28
2024-12-18 00:01:53.842792: Current learning rate: 0.0083
2024-12-18 00:05:57.777058: Validation loss did not improve from -0.55595. Patience: 6/50
2024-12-18 00:05:57.778656: train_loss -0.6581
2024-12-18 00:05:57.779556: val_loss -0.5451
2024-12-18 00:05:57.780197: Pseudo dice [0.734]
2024-12-18 00:05:57.780953: Epoch time: 243.94 s
2024-12-18 00:05:57.781682: Yayy! New best EMA pseudo Dice: 0.7258
2024-12-18 00:06:00.085567: 
2024-12-18 00:06:00.086964: Epoch 29
2024-12-18 00:06:00.087815: Current learning rate: 0.00824
2024-12-18 00:10:41.308219: Validation loss did not improve from -0.55595. Patience: 7/50
2024-12-18 00:10:41.310230: train_loss -0.6687
2024-12-18 00:10:41.311419: val_loss -0.5481
2024-12-18 00:10:41.312236: Pseudo dice [0.741]
2024-12-18 00:10:41.313212: Epoch time: 281.23 s
2024-12-18 00:10:41.728535: Yayy! New best EMA pseudo Dice: 0.7273
2024-12-18 00:10:43.671790: 
2024-12-18 00:10:43.673905: Epoch 30
2024-12-18 00:10:43.674772: Current learning rate: 0.00818
2024-12-18 00:14:51.833587: Validation loss did not improve from -0.55595. Patience: 8/50
2024-12-18 00:14:51.834919: train_loss -0.6683
2024-12-18 00:14:51.835882: val_loss -0.5367
2024-12-18 00:14:51.836655: Pseudo dice [0.7322]
2024-12-18 00:14:51.837389: Epoch time: 248.17 s
2024-12-18 00:14:51.838122: Yayy! New best EMA pseudo Dice: 0.7278
2024-12-18 00:14:53.724820: 
2024-12-18 00:14:53.726196: Epoch 31
2024-12-18 00:14:53.727087: Current learning rate: 0.00812
2024-12-18 00:19:44.636144: Validation loss did not improve from -0.55595. Patience: 9/50
2024-12-18 00:19:44.637744: train_loss -0.6754
2024-12-18 00:19:44.638804: val_loss -0.5433
2024-12-18 00:19:44.639571: Pseudo dice [0.7354]
2024-12-18 00:19:44.640362: Epoch time: 290.91 s
2024-12-18 00:19:44.641180: Yayy! New best EMA pseudo Dice: 0.7286
2024-12-18 00:19:46.480346: 
2024-12-18 00:19:46.481925: Epoch 32
2024-12-18 00:19:46.482718: Current learning rate: 0.00806
2024-12-18 00:23:55.186719: Validation loss did not improve from -0.55595. Patience: 10/50
2024-12-18 00:23:55.187823: train_loss -0.6758
2024-12-18 00:23:55.189094: val_loss -0.5119
2024-12-18 00:23:55.190184: Pseudo dice [0.7141]
2024-12-18 00:23:55.191195: Epoch time: 248.71 s
2024-12-18 00:23:56.670253: 
2024-12-18 00:23:56.671779: Epoch 33
2024-12-18 00:23:56.673135: Current learning rate: 0.008
2024-12-18 00:29:10.969918: Validation loss did not improve from -0.55595. Patience: 11/50
2024-12-18 00:29:10.972479: train_loss -0.6756
2024-12-18 00:29:10.973511: val_loss -0.5441
2024-12-18 00:29:10.974241: Pseudo dice [0.7356]
2024-12-18 00:29:10.975024: Epoch time: 314.3 s
2024-12-18 00:29:12.424183: 
2024-12-18 00:29:12.425459: Epoch 34
2024-12-18 00:29:12.426172: Current learning rate: 0.00793
2024-12-18 00:33:09.490242: Validation loss improved from -0.55595 to -0.58610! Patience: 11/50
2024-12-18 00:33:09.491218: train_loss -0.6792
2024-12-18 00:33:09.492137: val_loss -0.5861
2024-12-18 00:33:09.493094: Pseudo dice [0.7628]
2024-12-18 00:33:09.494037: Epoch time: 237.07 s
2024-12-18 00:33:09.909350: Yayy! New best EMA pseudo Dice: 0.7314
2024-12-18 00:33:11.874036: 
2024-12-18 00:33:11.875674: Epoch 35
2024-12-18 00:33:11.876663: Current learning rate: 0.00787
2024-12-18 00:38:15.722062: Validation loss did not improve from -0.58610. Patience: 1/50
2024-12-18 00:38:15.723051: train_loss -0.6784
2024-12-18 00:38:15.723890: val_loss -0.5499
2024-12-18 00:38:15.724676: Pseudo dice [0.7397]
2024-12-18 00:38:15.725508: Epoch time: 303.85 s
2024-12-18 00:38:15.726216: Yayy! New best EMA pseudo Dice: 0.7323
2024-12-18 00:38:17.592980: 
2024-12-18 00:38:17.594345: Epoch 36
2024-12-18 00:38:17.595184: Current learning rate: 0.00781
2024-12-18 00:42:32.729487: Validation loss did not improve from -0.58610. Patience: 2/50
2024-12-18 00:42:32.730396: train_loss -0.6668
2024-12-18 00:42:32.731211: val_loss -0.5585
2024-12-18 00:42:32.731950: Pseudo dice [0.7445]
2024-12-18 00:42:32.732710: Epoch time: 255.14 s
2024-12-18 00:42:32.733539: Yayy! New best EMA pseudo Dice: 0.7335
2024-12-18 00:42:34.658779: 
2024-12-18 00:42:34.660422: Epoch 37
2024-12-18 00:42:34.661778: Current learning rate: 0.00775
2024-12-18 00:47:01.064513: Validation loss did not improve from -0.58610. Patience: 3/50
2024-12-18 00:47:01.066088: train_loss -0.6814
2024-12-18 00:47:01.067050: val_loss -0.5439
2024-12-18 00:47:01.067895: Pseudo dice [0.7367]
2024-12-18 00:47:01.068627: Epoch time: 266.41 s
2024-12-18 00:47:01.069432: Yayy! New best EMA pseudo Dice: 0.7338
2024-12-18 00:47:02.972845: 
2024-12-18 00:47:02.974160: Epoch 38
2024-12-18 00:47:02.975175: Current learning rate: 0.00769
2024-12-18 00:51:17.663884: Validation loss did not improve from -0.58610. Patience: 4/50
2024-12-18 00:51:17.665665: train_loss -0.6846
2024-12-18 00:51:17.666767: val_loss -0.5271
2024-12-18 00:51:17.667630: Pseudo dice [0.7265]
2024-12-18 00:51:17.668483: Epoch time: 254.69 s
2024-12-18 00:51:19.741837: 
2024-12-18 00:51:19.743186: Epoch 39
2024-12-18 00:51:19.743984: Current learning rate: 0.00763
2024-12-18 00:55:07.851788: Validation loss did not improve from -0.58610. Patience: 5/50
2024-12-18 00:55:07.852856: train_loss -0.6826
2024-12-18 00:55:07.853668: val_loss -0.5442
2024-12-18 00:55:07.854367: Pseudo dice [0.7334]
2024-12-18 00:55:07.855168: Epoch time: 228.11 s
2024-12-18 00:55:09.817215: 
2024-12-18 00:55:09.818673: Epoch 40
2024-12-18 00:55:09.819575: Current learning rate: 0.00756
2024-12-18 00:59:30.797179: Validation loss did not improve from -0.58610. Patience: 6/50
2024-12-18 00:59:30.798246: train_loss -0.6928
2024-12-18 00:59:30.799516: val_loss -0.5579
2024-12-18 00:59:30.800542: Pseudo dice [0.7411]
2024-12-18 00:59:30.801653: Epoch time: 260.98 s
2024-12-18 00:59:30.802492: Yayy! New best EMA pseudo Dice: 0.7339
2024-12-18 00:59:32.739115: 
2024-12-18 00:59:32.740819: Epoch 41
2024-12-18 00:59:32.741973: Current learning rate: 0.0075
2024-12-18 01:03:38.018902: Validation loss did not improve from -0.58610. Patience: 7/50
2024-12-18 01:03:38.019951: train_loss -0.691
2024-12-18 01:03:38.020910: val_loss -0.5582
2024-12-18 01:03:38.021734: Pseudo dice [0.744]
2024-12-18 01:03:38.022600: Epoch time: 245.28 s
2024-12-18 01:03:38.023331: Yayy! New best EMA pseudo Dice: 0.7349
2024-12-18 01:03:39.874767: 
2024-12-18 01:03:39.876184: Epoch 42
2024-12-18 01:03:39.877011: Current learning rate: 0.00744
2024-12-18 01:07:49.861059: Validation loss did not improve from -0.58610. Patience: 8/50
2024-12-18 01:07:49.862029: train_loss -0.6929
2024-12-18 01:07:49.862861: val_loss -0.569
2024-12-18 01:07:49.863654: Pseudo dice [0.7457]
2024-12-18 01:07:49.864439: Epoch time: 249.99 s
2024-12-18 01:07:49.865092: Yayy! New best EMA pseudo Dice: 0.736
2024-12-18 01:07:51.706443: 
2024-12-18 01:07:51.707997: Epoch 43
2024-12-18 01:07:51.708800: Current learning rate: 0.00738
2024-12-18 01:11:49.033566: Validation loss did not improve from -0.58610. Patience: 9/50
2024-12-18 01:11:49.035632: train_loss -0.6942
2024-12-18 01:11:49.036705: val_loss -0.5482
2024-12-18 01:11:49.037328: Pseudo dice [0.7427]
2024-12-18 01:11:49.038099: Epoch time: 237.33 s
2024-12-18 01:11:49.038795: Yayy! New best EMA pseudo Dice: 0.7367
2024-12-18 01:11:50.862580: 
2024-12-18 01:11:50.863914: Epoch 44
2024-12-18 01:11:50.864990: Current learning rate: 0.00732
2024-12-18 01:15:57.560865: Validation loss did not improve from -0.58610. Patience: 10/50
2024-12-18 01:15:57.562530: train_loss -0.7003
2024-12-18 01:15:57.563804: val_loss -0.5445
2024-12-18 01:15:57.564794: Pseudo dice [0.7363]
2024-12-18 01:15:57.565634: Epoch time: 246.7 s
2024-12-18 01:15:59.362029: 
2024-12-18 01:15:59.363131: Epoch 45
2024-12-18 01:15:59.363901: Current learning rate: 0.00725
2024-12-18 01:20:11.384828: Validation loss did not improve from -0.58610. Patience: 11/50
2024-12-18 01:20:11.386054: train_loss -0.7015
2024-12-18 01:20:11.387364: val_loss -0.5407
2024-12-18 01:20:11.388482: Pseudo dice [0.7333]
2024-12-18 01:20:11.389522: Epoch time: 252.03 s
2024-12-18 01:20:12.855938: 
2024-12-18 01:20:12.857548: Epoch 46
2024-12-18 01:20:12.858743: Current learning rate: 0.00719
2024-12-18 01:24:25.414576: Validation loss did not improve from -0.58610. Patience: 12/50
2024-12-18 01:24:25.415628: train_loss -0.7047
2024-12-18 01:24:25.416423: val_loss -0.5533
2024-12-18 01:24:25.417182: Pseudo dice [0.7407]
2024-12-18 01:24:25.417850: Epoch time: 252.56 s
2024-12-18 01:24:25.418555: Yayy! New best EMA pseudo Dice: 0.7367
2024-12-18 01:24:27.228530: 
2024-12-18 01:24:27.229727: Epoch 47
2024-12-18 01:24:27.230658: Current learning rate: 0.00713
2024-12-18 01:28:48.336275: Validation loss did not improve from -0.58610. Patience: 13/50
2024-12-18 01:28:48.337311: train_loss -0.6985
2024-12-18 01:28:48.338170: val_loss -0.5431
2024-12-18 01:28:48.338934: Pseudo dice [0.7344]
2024-12-18 01:28:48.339731: Epoch time: 261.11 s
2024-12-18 01:28:49.790044: 
2024-12-18 01:28:49.791413: Epoch 48
2024-12-18 01:28:49.792244: Current learning rate: 0.00707
2024-12-18 01:32:49.346277: Validation loss did not improve from -0.58610. Patience: 14/50
2024-12-18 01:32:49.349010: train_loss -0.7011
2024-12-18 01:32:49.350032: val_loss -0.5411
2024-12-18 01:32:49.350897: Pseudo dice [0.7332]
2024-12-18 01:32:49.351811: Epoch time: 239.56 s
2024-12-18 01:32:50.766189: 
2024-12-18 01:32:50.767772: Epoch 49
2024-12-18 01:32:50.768770: Current learning rate: 0.007
2024-12-18 01:37:25.964714: Validation loss did not improve from -0.58610. Patience: 15/50
2024-12-18 01:37:25.965997: train_loss -0.711
2024-12-18 01:37:25.966778: val_loss -0.5574
2024-12-18 01:37:25.967462: Pseudo dice [0.7512]
2024-12-18 01:37:25.968272: Epoch time: 275.2 s
2024-12-18 01:37:27.011364: Yayy! New best EMA pseudo Dice: 0.7377
2024-12-18 01:37:28.867523: 
2024-12-18 01:37:28.869425: Epoch 50
2024-12-18 01:37:28.870309: Current learning rate: 0.00694
2024-12-18 01:41:18.101449: Validation loss did not improve from -0.58610. Patience: 16/50
2024-12-18 01:41:18.102454: train_loss -0.7181
2024-12-18 01:41:18.103553: val_loss -0.5833
2024-12-18 01:41:18.104554: Pseudo dice [0.7556]
2024-12-18 01:41:18.105504: Epoch time: 229.24 s
2024-12-18 01:41:18.106272: Yayy! New best EMA pseudo Dice: 0.7395
2024-12-18 01:41:19.929951: 
2024-12-18 01:41:19.931542: Epoch 51
2024-12-18 01:41:19.932449: Current learning rate: 0.00688
2024-12-18 01:46:21.139420: Validation loss did not improve from -0.58610. Patience: 17/50
2024-12-18 01:46:21.141647: train_loss -0.7134
2024-12-18 01:46:21.142768: val_loss -0.556
2024-12-18 01:46:21.143699: Pseudo dice [0.7491]
2024-12-18 01:46:21.144541: Epoch time: 301.21 s
2024-12-18 01:46:21.145356: Yayy! New best EMA pseudo Dice: 0.7404
2024-12-18 01:46:22.999963: 
2024-12-18 01:46:23.001549: Epoch 52
2024-12-18 01:46:23.002736: Current learning rate: 0.00682
2024-12-18 01:50:24.188396: Validation loss did not improve from -0.58610. Patience: 18/50
2024-12-18 01:50:24.190232: train_loss -0.7132
2024-12-18 01:50:24.191365: val_loss -0.5702
2024-12-18 01:50:24.192075: Pseudo dice [0.7496]
2024-12-18 01:50:24.192904: Epoch time: 241.19 s
2024-12-18 01:50:24.193621: Yayy! New best EMA pseudo Dice: 0.7413
2024-12-18 01:50:26.033306: 
2024-12-18 01:50:26.034680: Epoch 53
2024-12-18 01:50:26.035694: Current learning rate: 0.00675
2024-12-18 01:54:48.703549: Validation loss did not improve from -0.58610. Patience: 19/50
2024-12-18 01:54:48.704349: train_loss -0.7104
2024-12-18 01:54:48.705481: val_loss -0.5834
2024-12-18 01:54:48.706247: Pseudo dice [0.763]
2024-12-18 01:54:48.707048: Epoch time: 262.68 s
2024-12-18 01:54:48.707825: Yayy! New best EMA pseudo Dice: 0.7435
2024-12-18 01:54:50.456525: 
2024-12-18 01:54:50.457687: Epoch 54
2024-12-18 01:54:50.458448: Current learning rate: 0.00669
2024-12-18 01:57:41.355309: Validation loss did not improve from -0.58610. Patience: 20/50
2024-12-18 01:57:41.356426: train_loss -0.7101
2024-12-18 01:57:41.357276: val_loss -0.5432
2024-12-18 01:57:41.358008: Pseudo dice [0.7355]
2024-12-18 01:57:41.358811: Epoch time: 170.9 s
2024-12-18 01:57:43.183120: 
2024-12-18 01:57:43.184505: Epoch 55
2024-12-18 01:57:43.185384: Current learning rate: 0.00663
2024-12-18 02:01:58.690346: Validation loss did not improve from -0.58610. Patience: 21/50
2024-12-18 02:01:58.691318: train_loss -0.7118
2024-12-18 02:01:58.692101: val_loss -0.5462
2024-12-18 02:01:58.692892: Pseudo dice [0.7328]
2024-12-18 02:01:58.693698: Epoch time: 255.51 s
2024-12-18 02:02:00.108429: 
2024-12-18 02:02:00.110000: Epoch 56
2024-12-18 02:02:00.111099: Current learning rate: 0.00657
2024-12-18 02:05:32.744010: Validation loss did not improve from -0.58610. Patience: 22/50
2024-12-18 02:05:32.745026: train_loss -0.7167
2024-12-18 02:05:32.745908: val_loss -0.5431
2024-12-18 02:05:32.746912: Pseudo dice [0.7369]
2024-12-18 02:05:32.748187: Epoch time: 212.64 s
2024-12-18 02:05:34.129321: 
2024-12-18 02:05:34.130811: Epoch 57
2024-12-18 02:05:34.131844: Current learning rate: 0.0065
2024-12-18 02:09:25.118727: Validation loss did not improve from -0.58610. Patience: 23/50
2024-12-18 02:09:25.119707: train_loss -0.7211
2024-12-18 02:09:25.120648: val_loss -0.5544
2024-12-18 02:09:25.121419: Pseudo dice [0.7423]
2024-12-18 02:09:25.122122: Epoch time: 230.99 s
2024-12-18 02:09:26.606834: 
2024-12-18 02:09:26.608129: Epoch 58
2024-12-18 02:09:26.608863: Current learning rate: 0.00644
2024-12-18 02:13:22.745833: Validation loss did not improve from -0.58610. Patience: 24/50
2024-12-18 02:13:22.746910: train_loss -0.7149
2024-12-18 02:13:22.747817: val_loss -0.5327
2024-12-18 02:13:22.748512: Pseudo dice [0.736]
2024-12-18 02:13:22.749290: Epoch time: 236.14 s
2024-12-18 02:13:24.204031: 
2024-12-18 02:13:24.205267: Epoch 59
2024-12-18 02:13:24.206011: Current learning rate: 0.00638
2024-12-18 02:16:38.162878: Validation loss did not improve from -0.58610. Patience: 25/50
2024-12-18 02:16:38.163767: train_loss -0.7164
2024-12-18 02:16:38.164574: val_loss -0.5487
2024-12-18 02:16:38.165398: Pseudo dice [0.7383]
2024-12-18 02:16:38.166303: Epoch time: 193.96 s
2024-12-18 02:16:40.448026: 
2024-12-18 02:16:40.449298: Epoch 60
2024-12-18 02:16:40.450265: Current learning rate: 0.00631
2024-12-18 02:20:20.433681: Validation loss did not improve from -0.58610. Patience: 26/50
2024-12-18 02:20:20.434779: train_loss -0.7251
2024-12-18 02:20:20.435827: val_loss -0.5586
2024-12-18 02:20:20.436808: Pseudo dice [0.7489]
2024-12-18 02:20:20.437795: Epoch time: 219.99 s
2024-12-18 02:20:21.834253: 
2024-12-18 02:20:21.835737: Epoch 61
2024-12-18 02:20:21.836746: Current learning rate: 0.00625
2024-12-18 02:24:50.197177: Validation loss did not improve from -0.58610. Patience: 27/50
2024-12-18 02:24:50.199629: train_loss -0.7239
2024-12-18 02:24:50.201025: val_loss -0.5656
2024-12-18 02:24:50.202017: Pseudo dice [0.7497]
2024-12-18 02:24:50.203258: Epoch time: 268.37 s
2024-12-18 02:24:51.750396: 
2024-12-18 02:24:51.751644: Epoch 62
2024-12-18 02:24:51.752688: Current learning rate: 0.00619
2024-12-18 02:28:59.990720: Validation loss did not improve from -0.58610. Patience: 28/50
2024-12-18 02:28:59.992156: train_loss -0.7255
2024-12-18 02:28:59.993134: val_loss -0.5342
2024-12-18 02:28:59.993987: Pseudo dice [0.7343]
2024-12-18 02:28:59.994922: Epoch time: 248.24 s
2024-12-18 02:29:01.477069: 
2024-12-18 02:29:01.478683: Epoch 63
2024-12-18 02:29:01.479637: Current learning rate: 0.00612
2024-12-18 02:34:03.676310: Validation loss did not improve from -0.58610. Patience: 29/50
2024-12-18 02:34:03.677362: train_loss -0.7254
2024-12-18 02:34:03.678616: val_loss -0.5674
2024-12-18 02:34:03.679624: Pseudo dice [0.7562]
2024-12-18 02:34:03.680645: Epoch time: 302.2 s
2024-12-18 02:34:05.119498: 
2024-12-18 02:34:05.121017: Epoch 64
2024-12-18 02:34:05.122030: Current learning rate: 0.00606
2024-12-18 02:38:51.455655: Validation loss did not improve from -0.58610. Patience: 30/50
2024-12-18 02:38:51.456843: train_loss -0.7303
2024-12-18 02:38:51.457738: val_loss -0.5354
2024-12-18 02:38:51.458403: Pseudo dice [0.7432]
2024-12-18 02:38:51.459120: Epoch time: 286.34 s
2024-12-18 02:38:53.190582: 
2024-12-18 02:38:53.191945: Epoch 65
2024-12-18 02:38:53.192760: Current learning rate: 0.006
2024-12-18 02:43:40.532884: Validation loss did not improve from -0.58610. Patience: 31/50
2024-12-18 02:43:40.533936: train_loss -0.7292
2024-12-18 02:43:40.534769: val_loss -0.5103
2024-12-18 02:43:40.535566: Pseudo dice [0.718]
2024-12-18 02:43:40.536269: Epoch time: 287.34 s
2024-12-18 02:43:41.924083: 
2024-12-18 02:43:41.925442: Epoch 66
2024-12-18 02:43:41.926154: Current learning rate: 0.00593
2024-12-18 02:48:31.282063: Validation loss did not improve from -0.58610. Patience: 32/50
2024-12-18 02:48:31.283082: train_loss -0.7298
2024-12-18 02:48:31.283887: val_loss -0.5554
2024-12-18 02:48:31.284590: Pseudo dice [0.7477]
2024-12-18 02:48:31.285370: Epoch time: 289.36 s
2024-12-18 02:48:32.731088: 
2024-12-18 02:48:32.732407: Epoch 67
2024-12-18 02:48:32.733179: Current learning rate: 0.00587
2024-12-18 02:53:25.520000: Validation loss did not improve from -0.58610. Patience: 33/50
2024-12-18 02:53:25.520777: train_loss -0.7308
2024-12-18 02:53:25.521591: val_loss -0.5619
2024-12-18 02:53:25.522273: Pseudo dice [0.7515]
2024-12-18 02:53:25.522939: Epoch time: 292.79 s
2024-12-18 02:53:26.950702: 
2024-12-18 02:53:26.952123: Epoch 68
2024-12-18 02:53:26.952868: Current learning rate: 0.00581
2024-12-18 02:58:56.872425: Validation loss did not improve from -0.58610. Patience: 34/50
2024-12-18 02:58:56.873501: train_loss -0.7375
2024-12-18 02:58:56.874509: val_loss -0.5601
2024-12-18 02:58:56.875435: Pseudo dice [0.7444]
2024-12-18 02:58:56.876224: Epoch time: 329.92 s
2024-12-18 02:58:58.294346: 
2024-12-18 02:58:58.295810: Epoch 69
2024-12-18 02:58:58.296884: Current learning rate: 0.00574
2024-12-18 03:02:58.796470: Validation loss did not improve from -0.58610. Patience: 35/50
2024-12-18 03:02:58.797470: train_loss -0.7338
2024-12-18 03:02:58.798376: val_loss -0.5577
2024-12-18 03:02:58.799189: Pseudo dice [0.7451]
2024-12-18 03:02:58.799899: Epoch time: 240.5 s
2024-12-18 03:03:00.625137: 
2024-12-18 03:03:00.626532: Epoch 70
2024-12-18 03:03:00.627429: Current learning rate: 0.00568
2024-12-18 03:08:05.367592: Validation loss did not improve from -0.58610. Patience: 36/50
2024-12-18 03:08:05.369262: train_loss -0.7334
2024-12-18 03:08:05.370305: val_loss -0.5848
2024-12-18 03:08:05.371164: Pseudo dice [0.7631]
2024-12-18 03:08:05.371882: Epoch time: 304.75 s
2024-12-18 03:08:05.372694: Yayy! New best EMA pseudo Dice: 0.7447
2024-12-18 03:08:07.526571: 
2024-12-18 03:08:07.527793: Epoch 71
2024-12-18 03:08:07.528596: Current learning rate: 0.00562
2024-12-18 03:12:40.010239: Validation loss did not improve from -0.58610. Patience: 37/50
2024-12-18 03:12:40.011415: train_loss -0.733
2024-12-18 03:12:40.012373: val_loss -0.5621
2024-12-18 03:12:40.013373: Pseudo dice [0.744]
2024-12-18 03:12:40.014148: Epoch time: 272.49 s
2024-12-18 03:12:41.490894: 
2024-12-18 03:12:41.492917: Epoch 72
2024-12-18 03:12:41.494081: Current learning rate: 0.00555
2024-12-18 03:17:23.701561: Validation loss did not improve from -0.58610. Patience: 38/50
2024-12-18 03:17:23.702733: train_loss -0.7365
2024-12-18 03:17:23.703572: val_loss -0.5564
2024-12-18 03:17:23.704259: Pseudo dice [0.7437]
2024-12-18 03:17:23.705108: Epoch time: 282.21 s
2024-12-18 03:17:25.188782: 
2024-12-18 03:17:25.190038: Epoch 73
2024-12-18 03:17:25.190804: Current learning rate: 0.00549
2024-12-18 03:21:46.580806: Validation loss did not improve from -0.58610. Patience: 39/50
2024-12-18 03:21:46.582299: train_loss -0.7413
2024-12-18 03:21:46.583442: val_loss -0.5586
2024-12-18 03:21:46.584359: Pseudo dice [0.7449]
2024-12-18 03:21:46.585185: Epoch time: 261.39 s
2024-12-18 03:21:48.139558: 
2024-12-18 03:21:48.141019: Epoch 74
2024-12-18 03:21:48.142378: Current learning rate: 0.00542
2024-12-18 03:26:21.145352: Validation loss did not improve from -0.58610. Patience: 40/50
2024-12-18 03:26:21.147113: train_loss -0.7382
2024-12-18 03:26:21.148281: val_loss -0.529
2024-12-18 03:26:21.150575: Pseudo dice [0.7389]
2024-12-18 03:26:21.151742: Epoch time: 273.01 s
2024-12-18 03:26:23.036898: 
2024-12-18 03:26:23.038520: Epoch 75
2024-12-18 03:26:23.039496: Current learning rate: 0.00536
2024-12-18 03:30:42.887736: Validation loss did not improve from -0.58610. Patience: 41/50
2024-12-18 03:30:42.889045: train_loss -0.7304
2024-12-18 03:30:42.889963: val_loss -0.5679
2024-12-18 03:30:42.890615: Pseudo dice [0.7587]
2024-12-18 03:30:42.891417: Epoch time: 259.85 s
2024-12-18 03:30:42.892118: Yayy! New best EMA pseudo Dice: 0.7455
2024-12-18 03:30:44.815831: 
2024-12-18 03:30:44.817256: Epoch 76
2024-12-18 03:30:44.818166: Current learning rate: 0.00529
2024-12-18 03:35:19.045737: Validation loss did not improve from -0.58610. Patience: 42/50
2024-12-18 03:35:19.046660: train_loss -0.7367
2024-12-18 03:35:19.047597: val_loss -0.5297
2024-12-18 03:35:19.048468: Pseudo dice [0.7335]
2024-12-18 03:35:19.049221: Epoch time: 274.23 s
2024-12-18 03:35:20.552122: 
2024-12-18 03:35:20.553465: Epoch 77
2024-12-18 03:35:20.554414: Current learning rate: 0.00523
2024-12-18 03:39:49.157453: Validation loss did not improve from -0.58610. Patience: 43/50
2024-12-18 03:39:49.158480: train_loss -0.7403
2024-12-18 03:39:49.159323: val_loss -0.5527
2024-12-18 03:39:49.160025: Pseudo dice [0.7492]
2024-12-18 03:39:49.160798: Epoch time: 268.61 s
2024-12-18 03:39:50.697083: 
2024-12-18 03:39:50.698405: Epoch 78
2024-12-18 03:39:50.699329: Current learning rate: 0.00517
2024-12-18 03:44:27.815959: Validation loss did not improve from -0.58610. Patience: 44/50
2024-12-18 03:44:27.817412: train_loss -0.7463
2024-12-18 03:44:27.818492: val_loss -0.5712
2024-12-18 03:44:27.819350: Pseudo dice [0.7587]
2024-12-18 03:44:27.820118: Epoch time: 277.12 s
2024-12-18 03:44:27.820924: Yayy! New best EMA pseudo Dice: 0.7462
2024-12-18 03:44:29.754736: 
2024-12-18 03:44:29.755968: Epoch 79
2024-12-18 03:44:29.756846: Current learning rate: 0.0051
2024-12-18 03:49:51.486013: Validation loss did not improve from -0.58610. Patience: 45/50
2024-12-18 03:49:51.486772: train_loss -0.7474
2024-12-18 03:49:51.487620: val_loss -0.5549
2024-12-18 03:49:51.488492: Pseudo dice [0.7512]
2024-12-18 03:49:51.489361: Epoch time: 321.73 s
2024-12-18 03:49:51.890507: Yayy! New best EMA pseudo Dice: 0.7467
2024-12-18 03:49:53.823250: 
2024-12-18 03:49:53.824646: Epoch 80
2024-12-18 03:49:53.825520: Current learning rate: 0.00504
2024-12-18 03:54:15.866184: Validation loss did not improve from -0.58610. Patience: 46/50
2024-12-18 03:54:15.866978: train_loss -0.7442
2024-12-18 03:54:15.867907: val_loss -0.5764
2024-12-18 03:54:15.868585: Pseudo dice [0.7598]
2024-12-18 03:54:15.869349: Epoch time: 262.04 s
2024-12-18 03:54:15.870080: Yayy! New best EMA pseudo Dice: 0.748
2024-12-18 03:54:18.237011: 
2024-12-18 03:54:18.238499: Epoch 81
2024-12-18 03:54:18.239570: Current learning rate: 0.00497
2024-12-18 03:59:43.757149: Validation loss did not improve from -0.58610. Patience: 47/50
2024-12-18 03:59:43.758121: train_loss -0.7429
2024-12-18 03:59:43.758986: val_loss -0.5481
2024-12-18 03:59:43.760228: Pseudo dice [0.7537]
2024-12-18 03:59:43.761136: Epoch time: 325.52 s
2024-12-18 03:59:43.761795: Yayy! New best EMA pseudo Dice: 0.7486
2024-12-18 03:59:45.703303: 
2024-12-18 03:59:45.704651: Epoch 82
2024-12-18 03:59:45.705427: Current learning rate: 0.00491
2024-12-18 04:04:10.138783: Validation loss did not improve from -0.58610. Patience: 48/50
2024-12-18 04:04:10.139934: train_loss -0.7457
2024-12-18 04:04:10.140829: val_loss -0.4972
2024-12-18 04:04:10.141688: Pseudo dice [0.7141]
2024-12-18 04:04:10.142411: Epoch time: 264.44 s
2024-12-18 04:04:11.591336: 
2024-12-18 04:04:11.592674: Epoch 83
2024-12-18 04:04:11.593571: Current learning rate: 0.00484
2024-12-18 04:09:01.614225: Validation loss did not improve from -0.58610. Patience: 49/50
2024-12-18 04:09:01.615262: train_loss -0.7505
2024-12-18 04:09:01.616032: val_loss -0.5643
2024-12-18 04:09:01.616698: Pseudo dice [0.7547]
2024-12-18 04:09:01.617406: Epoch time: 290.03 s
2024-12-18 04:09:03.134232: 
2024-12-18 04:09:03.135732: Epoch 84
2024-12-18 04:09:03.136548: Current learning rate: 0.00478
2024-12-18 04:13:31.355526: Validation loss improved from -0.58610 to -0.60026! Patience: 49/50
2024-12-18 04:13:31.356638: train_loss -0.7516
2024-12-18 04:13:31.357382: val_loss -0.6003
2024-12-18 04:13:31.358100: Pseudo dice [0.7684]
2024-12-18 04:13:31.358923: Epoch time: 268.22 s
2024-12-18 04:13:33.170710: 
2024-12-18 04:13:33.172113: Epoch 85
2024-12-18 04:13:33.172888: Current learning rate: 0.00471
2024-12-18 04:18:21.262042: Validation loss did not improve from -0.60026. Patience: 1/50
2024-12-18 04:18:21.263124: train_loss -0.7474
2024-12-18 04:18:21.263978: val_loss -0.5564
2024-12-18 04:18:21.264772: Pseudo dice [0.7466]
2024-12-18 04:18:21.265436: Epoch time: 288.09 s
2024-12-18 04:18:22.712966: 
2024-12-18 04:18:22.714242: Epoch 86
2024-12-18 04:18:22.715124: Current learning rate: 0.00465
2024-12-18 04:23:37.670178: Validation loss did not improve from -0.60026. Patience: 2/50
2024-12-18 04:23:37.671235: train_loss -0.7517
2024-12-18 04:23:37.672029: val_loss -0.5305
2024-12-18 04:23:37.672740: Pseudo dice [0.7366]
2024-12-18 04:23:37.673437: Epoch time: 314.96 s
2024-12-18 04:23:39.129233: 
2024-12-18 04:23:39.130584: Epoch 87
2024-12-18 04:23:39.131378: Current learning rate: 0.00458
2024-12-18 04:27:54.619961: Validation loss did not improve from -0.60026. Patience: 3/50
2024-12-18 04:27:54.621076: train_loss -0.753
2024-12-18 04:27:54.621851: val_loss -0.5413
2024-12-18 04:27:54.622513: Pseudo dice [0.731]
2024-12-18 04:27:54.623350: Epoch time: 255.49 s
2024-12-18 04:27:56.036553: 
2024-12-18 04:27:56.037822: Epoch 88
2024-12-18 04:27:56.038671: Current learning rate: 0.00452
2024-12-18 04:32:57.224936: Validation loss did not improve from -0.60026. Patience: 4/50
2024-12-18 04:32:57.227171: train_loss -0.7544
2024-12-18 04:32:57.228042: val_loss -0.5679
2024-12-18 04:32:57.228867: Pseudo dice [0.7539]
2024-12-18 04:32:57.229687: Epoch time: 301.19 s
2024-12-18 04:32:58.610809: 
2024-12-18 04:32:58.612190: Epoch 89
2024-12-18 04:32:58.612910: Current learning rate: 0.00445
2024-12-18 04:38:06.277247: Validation loss did not improve from -0.60026. Patience: 5/50
2024-12-18 04:38:06.278277: train_loss -0.758
2024-12-18 04:38:06.279219: val_loss -0.5681
2024-12-18 04:38:06.279983: Pseudo dice [0.7619]
2024-12-18 04:38:06.280725: Epoch time: 307.67 s
2024-12-18 04:38:08.126360: 
2024-12-18 04:38:08.127545: Epoch 90
2024-12-18 04:38:08.128336: Current learning rate: 0.00438
2024-12-18 04:43:35.365939: Validation loss did not improve from -0.60026. Patience: 6/50
2024-12-18 04:43:35.367035: train_loss -0.7577
2024-12-18 04:43:35.367939: val_loss -0.5617
2024-12-18 04:43:35.368704: Pseudo dice [0.753]
2024-12-18 04:43:35.369459: Epoch time: 327.24 s
2024-12-18 04:43:36.724964: 
2024-12-18 04:43:36.726454: Epoch 91
2024-12-18 04:43:36.727430: Current learning rate: 0.00432
2024-12-18 04:47:57.325333: Validation loss did not improve from -0.60026. Patience: 7/50
2024-12-18 04:47:57.327813: train_loss -0.7584
2024-12-18 04:47:57.328882: val_loss -0.5536
2024-12-18 04:47:57.329611: Pseudo dice [0.7426]
2024-12-18 04:47:57.330425: Epoch time: 260.6 s
2024-12-18 04:47:58.777339: 
2024-12-18 04:47:58.779269: Epoch 92
2024-12-18 04:47:58.780132: Current learning rate: 0.00425
2024-12-18 04:52:37.087675: Validation loss did not improve from -0.60026. Patience: 8/50
2024-12-18 04:52:37.088765: train_loss -0.7597
2024-12-18 04:52:37.089614: val_loss -0.5396
2024-12-18 04:52:37.090675: Pseudo dice [0.7441]
2024-12-18 04:52:37.091483: Epoch time: 278.31 s
2024-12-18 04:52:38.894826: 
2024-12-18 04:52:38.896251: Epoch 93
2024-12-18 04:52:38.897093: Current learning rate: 0.00419
2024-12-18 04:57:38.402580: Validation loss did not improve from -0.60026. Patience: 9/50
2024-12-18 04:57:38.403682: train_loss -0.76
2024-12-18 04:57:38.404422: val_loss -0.5483
2024-12-18 04:57:38.405047: Pseudo dice [0.743]
2024-12-18 04:57:38.405710: Epoch time: 299.51 s
2024-12-18 04:57:39.798390: 
2024-12-18 04:57:39.799706: Epoch 94
2024-12-18 04:57:39.800717: Current learning rate: 0.00412
2024-12-18 05:01:52.012539: Validation loss did not improve from -0.60026. Patience: 10/50
2024-12-18 05:01:52.013443: train_loss -0.7578
2024-12-18 05:01:52.014253: val_loss -0.5342
2024-12-18 05:01:52.014915: Pseudo dice [0.7374]
2024-12-18 05:01:52.015690: Epoch time: 252.22 s
2024-12-18 05:01:53.918600: 
2024-12-18 05:01:53.919611: Epoch 95
2024-12-18 05:01:53.920533: Current learning rate: 0.00405
2024-12-18 05:06:09.201473: Validation loss did not improve from -0.60026. Patience: 11/50
2024-12-18 05:06:09.202350: train_loss -0.7602
2024-12-18 05:06:09.203152: val_loss -0.5441
2024-12-18 05:06:09.203871: Pseudo dice [0.7426]
2024-12-18 05:06:09.204546: Epoch time: 255.29 s
2024-12-18 05:06:10.619812: 
2024-12-18 05:06:10.621140: Epoch 96
2024-12-18 05:06:10.621951: Current learning rate: 0.00399
2024-12-18 05:11:00.122710: Validation loss did not improve from -0.60026. Patience: 12/50
2024-12-18 05:11:00.123894: train_loss -0.7613
2024-12-18 05:11:00.124894: val_loss -0.571
2024-12-18 05:11:00.125598: Pseudo dice [0.7549]
2024-12-18 05:11:00.126448: Epoch time: 289.51 s
2024-12-18 05:11:01.672043: 
2024-12-18 05:11:01.673822: Epoch 97
2024-12-18 05:11:01.674834: Current learning rate: 0.00392
2024-12-18 05:15:21.276424: Validation loss did not improve from -0.60026. Patience: 13/50
2024-12-18 05:15:21.277770: train_loss -0.7597
2024-12-18 05:15:21.279047: val_loss -0.516
2024-12-18 05:15:21.279793: Pseudo dice [0.7311]
2024-12-18 05:15:21.280598: Epoch time: 259.61 s
2024-12-18 05:15:22.793224: 
2024-12-18 05:15:22.794453: Epoch 98
2024-12-18 05:15:22.795309: Current learning rate: 0.00385
2024-12-18 05:20:31.425390: Validation loss did not improve from -0.60026. Patience: 14/50
2024-12-18 05:20:31.426262: train_loss -0.7657
2024-12-18 05:20:31.427272: val_loss -0.5549
2024-12-18 05:20:31.428048: Pseudo dice [0.748]
2024-12-18 05:20:31.428819: Epoch time: 308.63 s
2024-12-18 05:20:32.901299: 
2024-12-18 05:20:32.902789: Epoch 99
2024-12-18 05:20:32.903706: Current learning rate: 0.00379
2024-12-18 05:26:11.446368: Validation loss did not improve from -0.60026. Patience: 15/50
2024-12-18 05:26:11.447393: train_loss -0.7662
2024-12-18 05:26:11.448286: val_loss -0.5565
2024-12-18 05:26:11.449021: Pseudo dice [0.7569]
2024-12-18 05:26:11.449819: Epoch time: 338.55 s
2024-12-18 05:26:13.361629: 
2024-12-18 05:26:13.362680: Epoch 100
2024-12-18 05:26:13.363491: Current learning rate: 0.00372
2024-12-18 05:30:31.123124: Validation loss did not improve from -0.60026. Patience: 16/50
2024-12-18 05:30:31.124194: train_loss -0.7685
2024-12-18 05:30:31.125060: val_loss -0.5478
2024-12-18 05:30:31.125941: Pseudo dice [0.7442]
2024-12-18 05:30:31.126937: Epoch time: 257.76 s
2024-12-18 05:30:32.621079: 
2024-12-18 05:30:32.622453: Epoch 101
2024-12-18 05:30:32.623491: Current learning rate: 0.00365
2024-12-18 05:35:26.738946: Validation loss did not improve from -0.60026. Patience: 17/50
2024-12-18 05:35:26.739926: train_loss -0.7655
2024-12-18 05:35:26.741187: val_loss -0.5223
2024-12-18 05:35:26.742409: Pseudo dice [0.7332]
2024-12-18 05:35:26.743634: Epoch time: 294.12 s
2024-12-18 05:35:28.287076: 
2024-12-18 05:35:28.288687: Epoch 102
2024-12-18 05:35:28.289838: Current learning rate: 0.00359
2024-12-18 05:40:08.276343: Validation loss did not improve from -0.60026. Patience: 18/50
2024-12-18 05:40:08.278278: train_loss -0.7667
2024-12-18 05:40:08.279605: val_loss -0.5557
2024-12-18 05:40:08.280632: Pseudo dice [0.7482]
2024-12-18 05:40:08.281634: Epoch time: 279.99 s
2024-12-18 05:40:09.755688: 
2024-12-18 05:40:09.757231: Epoch 103
2024-12-18 05:40:09.758204: Current learning rate: 0.00352
2024-12-18 05:45:38.273508: Validation loss did not improve from -0.60026. Patience: 19/50
2024-12-18 05:45:38.274521: train_loss -0.768
2024-12-18 05:45:38.275430: val_loss -0.571
2024-12-18 05:45:38.276125: Pseudo dice [0.7602]
2024-12-18 05:45:38.276948: Epoch time: 328.52 s
2024-12-18 05:45:40.258305: 
2024-12-18 05:45:40.259556: Epoch 104
2024-12-18 05:45:40.260296: Current learning rate: 0.00345
2024-12-18 05:49:27.855859: Validation loss did not improve from -0.60026. Patience: 20/50
2024-12-18 05:49:27.856793: train_loss -0.7715
2024-12-18 05:49:27.857603: val_loss -0.5446
2024-12-18 05:49:27.858382: Pseudo dice [0.741]
2024-12-18 05:49:27.859142: Epoch time: 227.6 s
2024-12-18 05:49:29.763597: 
2024-12-18 05:49:29.765075: Epoch 105
2024-12-18 05:49:29.765964: Current learning rate: 0.00338
2024-12-18 05:54:22.209582: Validation loss did not improve from -0.60026. Patience: 21/50
2024-12-18 05:54:22.210676: train_loss -0.7677
2024-12-18 05:54:22.211680: val_loss -0.5444
2024-12-18 05:54:22.212551: Pseudo dice [0.7405]
2024-12-18 05:54:22.213354: Epoch time: 292.45 s
2024-12-18 05:54:23.722946: 
2024-12-18 05:54:23.724340: Epoch 106
2024-12-18 05:54:23.725311: Current learning rate: 0.00332
2024-12-18 05:58:28.902563: Validation loss did not improve from -0.60026. Patience: 22/50
2024-12-18 05:58:28.903597: train_loss -0.7675
2024-12-18 05:58:28.904456: val_loss -0.5486
2024-12-18 05:58:28.905256: Pseudo dice [0.7465]
2024-12-18 05:58:28.906125: Epoch time: 245.18 s
2024-12-18 05:58:30.388069: 
2024-12-18 05:58:30.389118: Epoch 107
2024-12-18 05:58:30.389836: Current learning rate: 0.00325
2024-12-18 06:02:57.348989: Validation loss did not improve from -0.60026. Patience: 23/50
2024-12-18 06:02:57.350136: train_loss -0.7695
2024-12-18 06:02:57.351190: val_loss -0.5186
2024-12-18 06:02:57.352022: Pseudo dice [0.7253]
2024-12-18 06:02:57.352865: Epoch time: 266.96 s
2024-12-18 06:02:58.795058: 
2024-12-18 06:02:58.796620: Epoch 108
2024-12-18 06:02:58.797460: Current learning rate: 0.00318
2024-12-18 06:08:29.249575: Validation loss did not improve from -0.60026. Patience: 24/50
2024-12-18 06:08:29.250525: train_loss -0.7619
2024-12-18 06:08:29.251432: val_loss -0.5144
2024-12-18 06:08:29.252223: Pseudo dice [0.7281]
2024-12-18 06:08:29.253000: Epoch time: 330.46 s
2024-12-18 06:08:30.727534: 
2024-12-18 06:08:30.729179: Epoch 109
2024-12-18 06:08:30.730201: Current learning rate: 0.00311
2024-12-18 06:13:52.063738: Validation loss did not improve from -0.60026. Patience: 25/50
2024-12-18 06:13:52.064893: train_loss -0.7699
2024-12-18 06:13:52.065910: val_loss -0.5582
2024-12-18 06:13:52.066762: Pseudo dice [0.7522]
2024-12-18 06:13:52.067563: Epoch time: 321.34 s
2024-12-18 06:13:53.923674: 
2024-12-18 06:13:53.924915: Epoch 110
2024-12-18 06:13:53.925687: Current learning rate: 0.00304
2024-12-18 06:19:24.991200: Validation loss did not improve from -0.60026. Patience: 26/50
2024-12-18 06:19:24.992229: train_loss -0.7704
2024-12-18 06:19:24.993155: val_loss -0.5642
2024-12-18 06:19:24.993977: Pseudo dice [0.7614]
2024-12-18 06:19:24.994668: Epoch time: 331.07 s
2024-12-18 06:19:26.443196: 
2024-12-18 06:19:26.444546: Epoch 111
2024-12-18 06:19:26.445596: Current learning rate: 0.00297
2024-12-18 06:23:33.274715: Validation loss did not improve from -0.60026. Patience: 27/50
2024-12-18 06:23:33.275769: train_loss -0.7692
2024-12-18 06:23:33.276490: val_loss -0.5465
2024-12-18 06:23:33.277350: Pseudo dice [0.7436]
2024-12-18 06:23:33.278184: Epoch time: 246.83 s
2024-12-18 06:23:34.723561: 
2024-12-18 06:23:34.725112: Epoch 112
2024-12-18 06:23:34.726083: Current learning rate: 0.00291
2024-12-18 06:28:30.884195: Validation loss did not improve from -0.60026. Patience: 28/50
2024-12-18 06:28:30.885265: train_loss -0.7723
2024-12-18 06:28:30.886297: val_loss -0.5752
2024-12-18 06:28:30.887319: Pseudo dice [0.7608]
2024-12-18 06:28:30.888193: Epoch time: 296.16 s
2024-12-18 06:28:32.441053: 
2024-12-18 06:28:32.442546: Epoch 113
2024-12-18 06:28:32.443670: Current learning rate: 0.00284
2024-12-18 06:33:45.706562: Validation loss did not improve from -0.60026. Patience: 29/50
2024-12-18 06:33:45.707585: train_loss -0.7748
2024-12-18 06:33:45.708517: val_loss -0.5479
2024-12-18 06:33:45.709603: Pseudo dice [0.7483]
2024-12-18 06:33:45.710424: Epoch time: 313.27 s
2024-12-18 06:33:47.186045: 
2024-12-18 06:33:47.187403: Epoch 114
2024-12-18 06:33:47.188252: Current learning rate: 0.00277
2024-12-18 06:37:46.825657: Validation loss did not improve from -0.60026. Patience: 30/50
2024-12-18 06:37:46.826763: train_loss -0.7737
2024-12-18 06:37:46.827800: val_loss -0.556
2024-12-18 06:37:46.828635: Pseudo dice [0.7518]
2024-12-18 06:37:46.829507: Epoch time: 239.64 s
2024-12-18 06:37:49.205016: 
2024-12-18 06:37:49.206566: Epoch 115
2024-12-18 06:37:49.207365: Current learning rate: 0.0027
2024-12-18 06:43:13.586159: Validation loss did not improve from -0.60026. Patience: 31/50
2024-12-18 06:43:13.589241: train_loss -0.7748
2024-12-18 06:43:13.591446: val_loss -0.5485
2024-12-18 06:43:13.592348: Pseudo dice [0.7449]
2024-12-18 06:43:13.593584: Epoch time: 324.39 s
2024-12-18 06:43:15.104744: 
2024-12-18 06:43:15.106349: Epoch 116
2024-12-18 06:43:15.107371: Current learning rate: 0.00263
2024-12-18 06:47:37.481972: Validation loss did not improve from -0.60026. Patience: 32/50
2024-12-18 06:47:37.482884: train_loss -0.7742
2024-12-18 06:47:37.483859: val_loss -0.5638
2024-12-18 06:47:37.484738: Pseudo dice [0.7629]
2024-12-18 06:47:37.485569: Epoch time: 262.38 s
2024-12-18 06:47:38.972872: 
2024-12-18 06:47:38.974241: Epoch 117
2024-12-18 06:47:38.975137: Current learning rate: 0.00256
2024-12-18 06:52:32.895288: Validation loss did not improve from -0.60026. Patience: 33/50
2024-12-18 06:52:32.896482: train_loss -0.7756
2024-12-18 06:52:32.897481: val_loss -0.5391
2024-12-18 06:52:32.898404: Pseudo dice [0.7497]
2024-12-18 06:52:32.899214: Epoch time: 293.93 s
2024-12-18 06:52:32.900032: Yayy! New best EMA pseudo Dice: 0.7486
2024-12-18 06:52:34.844225: 
2024-12-18 06:52:34.845836: Epoch 118
2024-12-18 06:52:34.846952: Current learning rate: 0.00249
2024-12-18 06:57:37.562126: Validation loss did not improve from -0.60026. Patience: 34/50
2024-12-18 06:57:37.563231: train_loss -0.7757
2024-12-18 06:57:37.564045: val_loss -0.571
2024-12-18 06:57:37.564747: Pseudo dice [0.7621]
2024-12-18 06:57:37.565464: Epoch time: 302.72 s
2024-12-18 06:57:37.566107: Yayy! New best EMA pseudo Dice: 0.75
2024-12-18 06:57:39.520179: 
2024-12-18 06:57:39.521535: Epoch 119
2024-12-18 06:57:39.522376: Current learning rate: 0.00242
2024-12-18 07:02:32.783598: Validation loss did not improve from -0.60026. Patience: 35/50
2024-12-18 07:02:32.784812: train_loss -0.78
2024-12-18 07:02:32.785599: val_loss -0.5532
2024-12-18 07:02:32.786298: Pseudo dice [0.7541]
2024-12-18 07:02:32.787016: Epoch time: 293.27 s
2024-12-18 07:02:33.187261: Yayy! New best EMA pseudo Dice: 0.7504
2024-12-18 07:02:35.062600: 
2024-12-18 07:02:35.063975: Epoch 120
2024-12-18 07:02:35.064682: Current learning rate: 0.00235
2024-12-18 07:07:05.441274: Validation loss did not improve from -0.60026. Patience: 36/50
2024-12-18 07:07:05.442339: train_loss -0.7792
2024-12-18 07:07:05.443234: val_loss -0.5741
2024-12-18 07:07:05.444062: Pseudo dice [0.7577]
2024-12-18 07:07:05.445010: Epoch time: 270.38 s
2024-12-18 07:07:05.445766: Yayy! New best EMA pseudo Dice: 0.7511
2024-12-18 07:07:07.344994: 
2024-12-18 07:07:07.346423: Epoch 121
2024-12-18 07:07:07.347310: Current learning rate: 0.00228
2024-12-18 07:11:53.406301: Validation loss did not improve from -0.60026. Patience: 37/50
2024-12-18 07:11:53.407376: train_loss -0.7811
2024-12-18 07:11:53.408198: val_loss -0.5412
2024-12-18 07:11:53.408938: Pseudo dice [0.7426]
2024-12-18 07:11:53.409658: Epoch time: 286.06 s
2024-12-18 07:11:54.875139: 
2024-12-18 07:11:54.876440: Epoch 122
2024-12-18 07:11:54.877125: Current learning rate: 0.00221
2024-12-18 07:16:30.897005: Validation loss did not improve from -0.60026. Patience: 38/50
2024-12-18 07:16:30.898034: train_loss -0.7801
2024-12-18 07:16:30.898893: val_loss -0.5566
2024-12-18 07:16:30.899554: Pseudo dice [0.7504]
2024-12-18 07:16:30.900330: Epoch time: 276.02 s
2024-12-18 07:16:32.427116: 
2024-12-18 07:16:32.428506: Epoch 123
2024-12-18 07:16:32.429342: Current learning rate: 0.00214
2024-12-18 07:21:19.447860: Validation loss did not improve from -0.60026. Patience: 39/50
2024-12-18 07:21:19.448568: train_loss -0.782
2024-12-18 07:21:19.449387: val_loss -0.5598
2024-12-18 07:21:19.450037: Pseudo dice [0.7494]
2024-12-18 07:21:19.450754: Epoch time: 287.02 s
2024-12-18 07:21:20.953832: 
2024-12-18 07:21:20.955146: Epoch 124
2024-12-18 07:21:20.955945: Current learning rate: 0.00207
2024-12-18 07:26:35.617892: Validation loss did not improve from -0.60026. Patience: 40/50
2024-12-18 07:26:35.618864: train_loss -0.7836
2024-12-18 07:26:35.619656: val_loss -0.5437
2024-12-18 07:26:35.620386: Pseudo dice [0.7522]
2024-12-18 07:26:35.621224: Epoch time: 314.67 s
2024-12-18 07:26:37.486076: 
2024-12-18 07:26:37.487980: Epoch 125
2024-12-18 07:26:37.489194: Current learning rate: 0.00199
2024-12-18 07:31:26.218710: Validation loss did not improve from -0.60026. Patience: 41/50
2024-12-18 07:31:26.219682: train_loss -0.7818
2024-12-18 07:31:26.220636: val_loss -0.5535
2024-12-18 07:31:26.221406: Pseudo dice [0.7449]
2024-12-18 07:31:26.222213: Epoch time: 288.74 s
2024-12-18 07:31:28.368625: 
2024-12-18 07:31:28.369883: Epoch 126
2024-12-18 07:31:28.370739: Current learning rate: 0.00192
2024-12-18 07:36:04.756427: Validation loss did not improve from -0.60026. Patience: 42/50
2024-12-18 07:36:04.757396: train_loss -0.7853
2024-12-18 07:36:04.758274: val_loss -0.536
2024-12-18 07:36:04.759212: Pseudo dice [0.7467]
2024-12-18 07:36:04.759937: Epoch time: 276.39 s
2024-12-18 07:36:06.195783: 
2024-12-18 07:36:06.197241: Epoch 127
2024-12-18 07:36:06.198159: Current learning rate: 0.00185
2024-12-18 07:41:14.018553: Validation loss did not improve from -0.60026. Patience: 43/50
2024-12-18 07:41:14.019404: train_loss -0.786
2024-12-18 07:41:14.020202: val_loss -0.542
2024-12-18 07:41:14.021117: Pseudo dice [0.7412]
2024-12-18 07:41:14.022103: Epoch time: 307.82 s
2024-12-18 07:41:15.475386: 
2024-12-18 07:41:15.476638: Epoch 128
2024-12-18 07:41:15.477566: Current learning rate: 0.00178
2024-12-18 07:46:40.890933: Validation loss did not improve from -0.60026. Patience: 44/50
2024-12-18 07:46:40.893012: train_loss -0.7874
2024-12-18 07:46:40.894017: val_loss -0.5515
2024-12-18 07:46:40.894877: Pseudo dice [0.7465]
2024-12-18 07:46:40.895759: Epoch time: 325.42 s
2024-12-18 07:46:42.347732: 
2024-12-18 07:46:42.349261: Epoch 129
2024-12-18 07:46:42.350142: Current learning rate: 0.0017
2024-12-18 07:51:11.808541: Validation loss did not improve from -0.60026. Patience: 45/50
2024-12-18 07:51:11.810582: train_loss -0.7842
2024-12-18 07:51:11.811354: val_loss -0.5478
2024-12-18 07:51:11.812007: Pseudo dice [0.7512]
2024-12-18 07:51:11.812642: Epoch time: 269.46 s
2024-12-18 07:51:13.624927: 
2024-12-18 07:51:13.626126: Epoch 130
2024-12-18 07:51:13.627007: Current learning rate: 0.00163
2024-12-18 07:56:01.387375: Validation loss did not improve from -0.60026. Patience: 46/50
2024-12-18 07:56:01.388448: train_loss -0.7863
2024-12-18 07:56:01.389521: val_loss -0.5576
2024-12-18 07:56:01.390166: Pseudo dice [0.7456]
2024-12-18 07:56:01.390839: Epoch time: 287.76 s
2024-12-18 07:56:02.831703: 
2024-12-18 07:56:02.832795: Epoch 131
2024-12-18 07:56:02.833642: Current learning rate: 0.00156
2024-12-18 08:00:40.610739: Validation loss did not improve from -0.60026. Patience: 47/50
2024-12-18 08:00:40.612993: train_loss -0.785
2024-12-18 08:00:40.614034: val_loss -0.5451
2024-12-18 08:00:40.614648: Pseudo dice [0.7539]
2024-12-18 08:00:40.615437: Epoch time: 277.78 s
2024-12-18 08:00:42.070318: 
2024-12-18 08:00:42.071653: Epoch 132
2024-12-18 08:00:42.072519: Current learning rate: 0.00148
2024-12-18 08:05:35.646314: Validation loss did not improve from -0.60026. Patience: 48/50
2024-12-18 08:05:35.647484: train_loss -0.7851
2024-12-18 08:05:35.648273: val_loss -0.5253
2024-12-18 08:05:35.649088: Pseudo dice [0.7391]
2024-12-18 08:05:35.649879: Epoch time: 293.58 s
2024-12-18 08:05:37.054044: 
2024-12-18 08:05:37.055184: Epoch 133
2024-12-18 08:05:37.056007: Current learning rate: 0.00141
2024-12-18 08:10:25.085850: Validation loss did not improve from -0.60026. Patience: 49/50
2024-12-18 08:10:25.086858: train_loss -0.7842
2024-12-18 08:10:25.087549: val_loss -0.5846
2024-12-18 08:10:25.088208: Pseudo dice [0.768]
2024-12-18 08:10:25.088973: Epoch time: 288.03 s
2024-12-18 08:10:26.562536: 
2024-12-18 08:10:26.563944: Epoch 134
2024-12-18 08:10:26.564837: Current learning rate: 0.00133
2024-12-18 08:15:24.844025: Validation loss did not improve from -0.60026. Patience: 50/50
2024-12-18 08:15:24.844928: train_loss -0.7862
2024-12-18 08:15:24.845806: val_loss -0.5435
2024-12-18 08:15:24.846456: Pseudo dice [0.7471]
2024-12-18 08:15:24.847169: Epoch time: 298.28 s
2024-12-18 08:15:26.943651: 
2024-12-18 08:15:26.945593: Epoch 135
2024-12-18 08:15:26.947079: Current learning rate: 0.00126
2024-12-18 08:20:58.436843: Validation loss did not improve from -0.60026. Patience: 51/50
2024-12-18 08:20:58.437782: train_loss -0.7841
2024-12-18 08:20:58.438650: val_loss -0.5494
2024-12-18 08:20:58.439271: Pseudo dice [0.7529]
2024-12-18 08:20:58.440007: Epoch time: 331.5 s
2024-12-18 08:20:59.890660: 
2024-12-18 08:20:59.891939: Epoch 136
2024-12-18 08:20:59.892751: Current learning rate: 0.00118
2024-12-18 08:25:03.064590: Validation loss did not improve from -0.60026. Patience: 52/50
2024-12-18 08:25:03.065754: train_loss -0.7872
2024-12-18 08:25:03.066558: val_loss -0.5634
2024-12-18 08:25:03.067371: Pseudo dice [0.7589]
2024-12-18 08:25:03.068192: Epoch time: 243.18 s
2024-12-18 08:25:04.975602: 
2024-12-18 08:25:04.976944: Epoch 137
2024-12-18 08:25:04.977760: Current learning rate: 0.00111
2024-12-18 08:30:08.415689: Validation loss did not improve from -0.60026. Patience: 53/50
2024-12-18 08:30:08.416787: train_loss -0.7899
2024-12-18 08:30:08.417886: val_loss -0.5361
2024-12-18 08:30:08.418712: Pseudo dice [0.7477]
2024-12-18 08:30:08.419568: Epoch time: 303.44 s
2024-12-18 08:30:10.183874: 
2024-12-18 08:30:10.185048: Epoch 138
2024-12-18 08:30:10.185898: Current learning rate: 0.00103
2024-12-18 08:35:18.444221: Validation loss did not improve from -0.60026. Patience: 54/50
2024-12-18 08:35:18.445288: train_loss -0.7909
2024-12-18 08:35:18.446225: val_loss -0.5624
2024-12-18 08:35:18.446966: Pseudo dice [0.755]
2024-12-18 08:35:18.447726: Epoch time: 308.26 s
2024-12-18 08:35:20.058962: 
2024-12-18 08:35:20.060451: Epoch 139
2024-12-18 08:35:20.061645: Current learning rate: 0.00095
2024-12-18 08:39:39.710871: Validation loss did not improve from -0.60026. Patience: 55/50
2024-12-18 08:39:39.711803: train_loss -0.7866
2024-12-18 08:39:39.712743: val_loss -0.5356
2024-12-18 08:39:39.713621: Pseudo dice [0.7436]
2024-12-18 08:39:39.714431: Epoch time: 259.65 s
2024-12-18 08:39:41.705268: 
2024-12-18 08:39:41.706272: Epoch 140
2024-12-18 08:39:41.707005: Current learning rate: 0.00087
2024-12-18 08:44:05.017503: Validation loss did not improve from -0.60026. Patience: 56/50
2024-12-18 08:44:05.018576: train_loss -0.7881
2024-12-18 08:44:05.019451: val_loss -0.5642
2024-12-18 08:44:05.020316: Pseudo dice [0.7562]
2024-12-18 08:44:05.021129: Epoch time: 263.31 s
2024-12-18 08:44:06.581486: 
2024-12-18 08:44:06.582605: Epoch 141
2024-12-18 08:44:06.583420: Current learning rate: 0.00079
2024-12-18 08:49:34.917158: Validation loss did not improve from -0.60026. Patience: 57/50
2024-12-18 08:49:34.918237: train_loss -0.7886
2024-12-18 08:49:34.919200: val_loss -0.5789
2024-12-18 08:49:34.920029: Pseudo dice [0.7634]
2024-12-18 08:49:34.920851: Epoch time: 328.34 s
2024-12-18 08:49:34.921671: Yayy! New best EMA pseudo Dice: 0.7521
2024-12-18 08:49:36.885398: 
2024-12-18 08:49:36.886724: Epoch 142
2024-12-18 08:49:36.887594: Current learning rate: 0.00071
2024-12-18 08:54:24.512691: Validation loss did not improve from -0.60026. Patience: 58/50
2024-12-18 08:54:24.514302: train_loss -0.7913
2024-12-18 08:54:24.515980: val_loss -0.5598
2024-12-18 08:54:24.516814: Pseudo dice [0.7623]
2024-12-18 08:54:24.517730: Epoch time: 287.63 s
2024-12-18 08:54:24.518605: Yayy! New best EMA pseudo Dice: 0.7532
2024-12-18 08:54:26.545326: 
2024-12-18 08:54:26.546713: Epoch 143
2024-12-18 08:54:26.547728: Current learning rate: 0.00063
2024-12-18 08:58:37.972761: Validation loss did not improve from -0.60026. Patience: 59/50
2024-12-18 08:58:37.974391: train_loss -0.7871
2024-12-18 08:58:37.975565: val_loss -0.5501
2024-12-18 08:58:37.976586: Pseudo dice [0.7458]
2024-12-18 08:58:37.977578: Epoch time: 251.43 s
2024-12-18 08:58:39.494635: 
2024-12-18 08:58:39.496247: Epoch 144
2024-12-18 08:58:39.497383: Current learning rate: 0.00055
2024-12-18 09:04:24.560987: Validation loss did not improve from -0.60026. Patience: 60/50
2024-12-18 09:04:24.562089: train_loss -0.7924
2024-12-18 09:04:24.562956: val_loss -0.5508
2024-12-18 09:04:24.563746: Pseudo dice [0.7516]
2024-12-18 09:04:24.564576: Epoch time: 345.07 s
2024-12-18 09:04:26.502624: 
2024-12-18 09:04:26.503865: Epoch 145
2024-12-18 09:04:26.504611: Current learning rate: 0.00047
2024-12-18 09:09:10.677077: Validation loss did not improve from -0.60026. Patience: 61/50
2024-12-18 09:09:10.678101: train_loss -0.7899
2024-12-18 09:09:10.678915: val_loss -0.5495
2024-12-18 09:09:10.679710: Pseudo dice [0.7503]
2024-12-18 09:09:10.680517: Epoch time: 284.18 s
2024-12-18 09:09:12.249974: 
2024-12-18 09:09:12.251293: Epoch 146
2024-12-18 09:09:12.252045: Current learning rate: 0.00038
2024-12-18 09:13:48.952836: Validation loss did not improve from -0.60026. Patience: 62/50
2024-12-18 09:13:48.954304: train_loss -0.7929
2024-12-18 09:13:48.955316: val_loss -0.5759
2024-12-18 09:13:48.956050: Pseudo dice [0.7612]
2024-12-18 09:13:48.956913: Epoch time: 276.71 s
2024-12-18 09:13:50.509405: 
2024-12-18 09:13:50.510868: Epoch 147
2024-12-18 09:13:50.511669: Current learning rate: 0.0003
2024-12-18 09:18:33.824695: Validation loss did not improve from -0.60026. Patience: 63/50
2024-12-18 09:18:33.825656: train_loss -0.7905
2024-12-18 09:18:33.826436: val_loss -0.5525
2024-12-18 09:18:33.827178: Pseudo dice [0.7525]
2024-12-18 09:18:33.827922: Epoch time: 283.32 s
2024-12-18 09:18:35.813732: 
2024-12-18 09:18:35.815056: Epoch 148
2024-12-18 09:18:35.815761: Current learning rate: 0.00021
2024-12-18 09:23:28.299140: Validation loss did not improve from -0.60026. Patience: 64/50
2024-12-18 09:23:28.300205: train_loss -0.7931
2024-12-18 09:23:28.301098: val_loss -0.5366
2024-12-18 09:23:28.301950: Pseudo dice [0.7339]
2024-12-18 09:23:28.302673: Epoch time: 292.49 s
2024-12-18 09:23:29.864640: 
2024-12-18 09:23:29.865790: Epoch 149
2024-12-18 09:23:29.866625: Current learning rate: 0.00011
2024-12-18 09:28:04.075422: Validation loss did not improve from -0.60026. Patience: 65/50
2024-12-18 09:28:04.076525: train_loss -0.7917
2024-12-18 09:28:04.077387: val_loss -0.532
2024-12-18 09:28:04.078240: Pseudo dice [0.7442]
2024-12-18 09:28:04.079098: Epoch time: 274.21 s
2024-12-18 09:28:06.132180: Training done.
2024-12-18 09:28:06.594435: Using splits from existing split file: /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset309_Sohee_Calcium_OCT_CrossValidation/splits_final_80.json
2024-12-18 09:28:06.610255: The split file contains 5 splits.
2024-12-18 09:28:06.611423: Desired fold for training: 3
2024-12-18 09:28:06.612208: This split has 6 training and 3 validation cases.
2024-12-18 09:28:06.613156: predicting 101-019
2024-12-18 09:28:06.667445: 101-019, shape torch.Size([1, 375, 498, 498]), rank 0
2024-12-18 09:30:49.663321: predicting 101-044
2024-12-18 09:30:49.684655: 101-044, shape torch.Size([1, 404, 498, 498]), rank 0
2024-12-18 09:33:33.171910: predicting 401-004
2024-12-18 09:33:33.192423: 401-004, shape torch.Size([1, 375, 498, 498]), rank 0
2024-12-18 09:36:28.362752: Validation complete
2024-12-18 09:36:28.363559: Mean Validation Dice:  0.7409397272638008
2024-12-17 22:00:19.832042: unpacking done...
2024-12-17 22:00:20.138860: Unable to plot network architecture: nnUNet_compile is enabled!
2024-12-17 22:00:20.329002: 
2024-12-17 22:00:20.330425: Epoch 0
2024-12-17 22:00:20.331367: Current learning rate: 0.01
2024-12-17 22:06:47.510499: Validation loss improved from 1000.00000 to -0.42061! Patience: 0/50
2024-12-17 22:06:47.511705: train_loss -0.3405
2024-12-17 22:06:47.512713: val_loss -0.4206
2024-12-17 22:06:47.513401: Pseudo dice [0.6768]
2024-12-17 22:06:47.514111: Epoch time: 387.18 s
2024-12-17 22:06:47.514822: Yayy! New best EMA pseudo Dice: 0.6768
2024-12-17 22:06:49.241479: 
2024-12-17 22:06:49.242871: Epoch 1
2024-12-17 22:06:49.243863: Current learning rate: 0.00994
2024-12-17 22:12:36.418031: Validation loss did not improve from -0.42061. Patience: 1/50
2024-12-17 22:12:36.419076: train_loss -0.4889
2024-12-17 22:12:36.420099: val_loss -0.3466
2024-12-17 22:12:36.421112: Pseudo dice [0.6401]
2024-12-17 22:12:36.422054: Epoch time: 347.18 s
2024-12-17 22:12:37.898242: 
2024-12-17 22:12:37.899733: Epoch 2
2024-12-17 22:12:37.900680: Current learning rate: 0.00988
2024-12-17 22:18:04.012625: Validation loss improved from -0.42061 to -0.48482! Patience: 1/50
2024-12-17 22:18:04.013525: train_loss -0.5309
2024-12-17 22:18:04.014476: val_loss -0.4848
2024-12-17 22:18:04.015409: Pseudo dice [0.7012]
2024-12-17 22:18:04.016300: Epoch time: 326.12 s
2024-12-17 22:18:05.581551: 
2024-12-17 22:18:05.583129: Epoch 3
2024-12-17 22:18:05.584509: Current learning rate: 0.00982
2024-12-17 22:23:32.567138: Validation loss did not improve from -0.48482. Patience: 1/50
2024-12-17 22:23:32.568255: train_loss -0.5547
2024-12-17 22:23:32.569184: val_loss -0.4155
2024-12-17 22:23:32.570081: Pseudo dice [0.6711]
2024-12-17 22:23:32.570921: Epoch time: 326.99 s
2024-12-17 22:23:34.033571: 
2024-12-17 22:23:34.034965: Epoch 4
2024-12-17 22:23:34.035879: Current learning rate: 0.00976
2024-12-17 22:29:46.356881: Validation loss improved from -0.48482 to -0.48688! Patience: 1/50
2024-12-17 22:29:46.357905: train_loss -0.5536
2024-12-17 22:29:46.358837: val_loss -0.4869
2024-12-17 22:29:46.359691: Pseudo dice [0.7088]
2024-12-17 22:29:46.360494: Epoch time: 372.33 s
2024-12-17 22:29:46.908935: Yayy! New best EMA pseudo Dice: 0.6788
2024-12-17 22:29:48.690580: 
2024-12-17 22:29:48.691951: Epoch 5
2024-12-17 22:29:48.692779: Current learning rate: 0.0097
2024-12-17 22:35:10.251901: Validation loss did not improve from -0.48688. Patience: 1/50
2024-12-17 22:35:10.252962: train_loss -0.5716
2024-12-17 22:35:10.253823: val_loss -0.4637
2024-12-17 22:35:10.254579: Pseudo dice [0.6965]
2024-12-17 22:35:10.255266: Epoch time: 321.56 s
2024-12-17 22:35:10.255938: Yayy! New best EMA pseudo Dice: 0.6805
2024-12-17 22:35:12.105111: 
2024-12-17 22:35:12.106252: Epoch 6
2024-12-17 22:35:12.106977: Current learning rate: 0.00964
2024-12-17 22:40:31.135524: Validation loss improved from -0.48688 to -0.50706! Patience: 1/50
2024-12-17 22:40:31.136457: train_loss -0.5776
2024-12-17 22:40:31.137227: val_loss -0.5071
2024-12-17 22:40:31.138155: Pseudo dice [0.7192]
2024-12-17 22:40:31.138819: Epoch time: 319.03 s
2024-12-17 22:40:31.139609: Yayy! New best EMA pseudo Dice: 0.6844
2024-12-17 22:40:32.954985: 
2024-12-17 22:40:32.956424: Epoch 7
2024-12-17 22:40:32.957341: Current learning rate: 0.00958
2024-12-17 22:46:38.649634: Validation loss did not improve from -0.50706. Patience: 1/50
2024-12-17 22:46:38.650711: train_loss -0.5998
2024-12-17 22:46:38.651663: val_loss -0.4562
2024-12-17 22:46:38.652571: Pseudo dice [0.7064]
2024-12-17 22:46:38.653350: Epoch time: 365.7 s
2024-12-17 22:46:38.654149: Yayy! New best EMA pseudo Dice: 0.6866
2024-12-17 22:46:41.079492: 
2024-12-17 22:46:41.080866: Epoch 8
2024-12-17 22:46:41.081814: Current learning rate: 0.00952
2024-12-17 22:52:17.548042: Validation loss did not improve from -0.50706. Patience: 2/50
2024-12-17 22:52:17.548938: train_loss -0.6037
2024-12-17 22:52:17.550056: val_loss -0.4979
2024-12-17 22:52:17.551132: Pseudo dice [0.7146]
2024-12-17 22:52:17.552271: Epoch time: 336.47 s
2024-12-17 22:52:17.553343: Yayy! New best EMA pseudo Dice: 0.6894
2024-12-17 22:52:19.468392: 
2024-12-17 22:52:19.469970: Epoch 9
2024-12-17 22:52:19.470987: Current learning rate: 0.00946
2024-12-17 22:57:37.777838: Validation loss did not improve from -0.50706. Patience: 3/50
2024-12-17 22:57:37.779057: train_loss -0.6137
2024-12-17 22:57:37.780219: val_loss -0.505
2024-12-17 22:57:37.781212: Pseudo dice [0.7219]
2024-12-17 22:57:37.782199: Epoch time: 318.31 s
2024-12-17 22:57:38.222865: Yayy! New best EMA pseudo Dice: 0.6927
2024-12-17 22:57:40.041424: 
2024-12-17 22:57:40.042837: Epoch 10
2024-12-17 22:57:40.043927: Current learning rate: 0.0094
2024-12-17 23:03:51.397214: Validation loss did not improve from -0.50706. Patience: 4/50
2024-12-17 23:03:51.398250: train_loss -0.6164
2024-12-17 23:03:51.399030: val_loss -0.4723
2024-12-17 23:03:51.399844: Pseudo dice [0.7013]
2024-12-17 23:03:51.400611: Epoch time: 371.36 s
2024-12-17 23:03:51.401402: Yayy! New best EMA pseudo Dice: 0.6935
2024-12-17 23:03:53.246934: 
2024-12-17 23:03:53.248246: Epoch 11
2024-12-17 23:03:53.248968: Current learning rate: 0.00934
2024-12-17 23:09:20.353697: Validation loss improved from -0.50706 to -0.50775! Patience: 4/50
2024-12-17 23:09:20.354982: train_loss -0.624
2024-12-17 23:09:20.355866: val_loss -0.5077
2024-12-17 23:09:20.356578: Pseudo dice [0.7212]
2024-12-17 23:09:20.357444: Epoch time: 327.11 s
2024-12-17 23:09:20.358136: Yayy! New best EMA pseudo Dice: 0.6963
2024-12-17 23:09:22.258985: 
2024-12-17 23:09:22.260635: Epoch 12
2024-12-17 23:09:22.261488: Current learning rate: 0.00928
2024-12-17 23:15:07.223882: Validation loss improved from -0.50775 to -0.52862! Patience: 0/50
2024-12-17 23:15:07.224954: train_loss -0.6195
2024-12-17 23:15:07.225888: val_loss -0.5286
2024-12-17 23:15:07.226723: Pseudo dice [0.7321]
2024-12-17 23:15:07.227595: Epoch time: 344.97 s
2024-12-17 23:15:07.228377: Yayy! New best EMA pseudo Dice: 0.6999
2024-12-17 23:15:09.083578: 
2024-12-17 23:15:09.084558: Epoch 13
2024-12-17 23:15:09.085342: Current learning rate: 0.00922
2024-12-17 23:20:56.512792: Validation loss did not improve from -0.52862. Patience: 1/50
2024-12-17 23:20:56.517066: train_loss -0.6321
2024-12-17 23:20:56.517960: val_loss -0.5097
2024-12-17 23:20:56.518687: Pseudo dice [0.7234]
2024-12-17 23:20:56.519576: Epoch time: 347.43 s
2024-12-17 23:20:56.520371: Yayy! New best EMA pseudo Dice: 0.7022
2024-12-17 23:20:58.344755: 
2024-12-17 23:20:58.346098: Epoch 14
2024-12-17 23:20:58.346968: Current learning rate: 0.00916
2024-12-17 23:26:37.518041: Validation loss did not improve from -0.52862. Patience: 2/50
2024-12-17 23:26:37.519148: train_loss -0.6388
2024-12-17 23:26:37.520336: val_loss -0.5025
2024-12-17 23:26:37.521409: Pseudo dice [0.7244]
2024-12-17 23:26:37.522374: Epoch time: 339.18 s
2024-12-17 23:26:37.972224: Yayy! New best EMA pseudo Dice: 0.7044
2024-12-17 23:26:39.912460: 
2024-12-17 23:26:39.913975: Epoch 15
2024-12-17 23:26:39.914925: Current learning rate: 0.0091
2024-12-17 23:32:18.342885: Validation loss did not improve from -0.52862. Patience: 3/50
2024-12-17 23:32:18.343803: train_loss -0.6481
2024-12-17 23:32:18.344671: val_loss -0.4815
2024-12-17 23:32:18.345516: Pseudo dice [0.7091]
2024-12-17 23:32:18.346374: Epoch time: 338.43 s
2024-12-17 23:32:18.347100: Yayy! New best EMA pseudo Dice: 0.7049
2024-12-17 23:32:20.123258: 
2024-12-17 23:32:20.124753: Epoch 16
2024-12-17 23:32:20.125945: Current learning rate: 0.00903
2024-12-17 23:38:30.658122: Validation loss did not improve from -0.52862. Patience: 4/50
2024-12-17 23:38:30.659085: train_loss -0.6497
2024-12-17 23:38:30.659913: val_loss -0.5056
2024-12-17 23:38:30.660748: Pseudo dice [0.7277]
2024-12-17 23:38:30.661593: Epoch time: 370.54 s
2024-12-17 23:38:30.662567: Yayy! New best EMA pseudo Dice: 0.7072
2024-12-17 23:38:32.487338: 
2024-12-17 23:38:32.488849: Epoch 17
2024-12-17 23:38:32.489772: Current learning rate: 0.00897
2024-12-17 23:43:51.612104: Validation loss did not improve from -0.52862. Patience: 5/50
2024-12-17 23:43:51.613017: train_loss -0.6527
2024-12-17 23:43:51.613997: val_loss -0.5196
2024-12-17 23:43:51.614776: Pseudo dice [0.7385]
2024-12-17 23:43:51.615537: Epoch time: 319.13 s
2024-12-17 23:43:51.616270: Yayy! New best EMA pseudo Dice: 0.7103
2024-12-17 23:43:53.508053: 
2024-12-17 23:43:53.509050: Epoch 18
2024-12-17 23:43:53.509928: Current learning rate: 0.00891
2024-12-17 23:49:34.009916: Validation loss improved from -0.52862 to -0.54235! Patience: 5/50
2024-12-17 23:49:34.010823: train_loss -0.6502
2024-12-17 23:49:34.011943: val_loss -0.5424
2024-12-17 23:49:34.012980: Pseudo dice [0.7418]
2024-12-17 23:49:34.014076: Epoch time: 340.5 s
2024-12-17 23:49:34.015123: Yayy! New best EMA pseudo Dice: 0.7135
2024-12-17 23:49:36.416595: 
2024-12-17 23:49:36.418134: Epoch 19
2024-12-17 23:49:36.419464: Current learning rate: 0.00885
2024-12-17 23:55:23.376139: Validation loss did not improve from -0.54235. Patience: 1/50
2024-12-17 23:55:23.377013: train_loss -0.6621
2024-12-17 23:55:23.377807: val_loss -0.4707
2024-12-17 23:55:23.378670: Pseudo dice [0.7081]
2024-12-17 23:55:23.379524: Epoch time: 346.96 s
2024-12-17 23:55:25.183631: 
2024-12-17 23:55:25.184976: Epoch 20
2024-12-17 23:55:25.185954: Current learning rate: 0.00879
2024-12-18 00:01:17.230904: Validation loss did not improve from -0.54235. Patience: 2/50
2024-12-18 00:01:17.232022: train_loss -0.6593
2024-12-18 00:01:17.232974: val_loss -0.5362
2024-12-18 00:01:17.233761: Pseudo dice [0.7389]
2024-12-18 00:01:17.234535: Epoch time: 352.05 s
2024-12-18 00:01:17.235371: Yayy! New best EMA pseudo Dice: 0.7155
2024-12-18 00:01:19.264176: 
2024-12-18 00:01:19.265573: Epoch 21
2024-12-18 00:01:19.266569: Current learning rate: 0.00873
2024-12-18 00:06:58.728475: Validation loss did not improve from -0.54235. Patience: 3/50
2024-12-18 00:06:58.729592: train_loss -0.668
2024-12-18 00:06:58.730512: val_loss -0.5107
2024-12-18 00:06:58.731317: Pseudo dice [0.7239]
2024-12-18 00:06:58.732144: Epoch time: 339.47 s
2024-12-18 00:06:58.732979: Yayy! New best EMA pseudo Dice: 0.7164
2024-12-18 00:07:00.543310: 
2024-12-18 00:07:00.544540: Epoch 22
2024-12-18 00:07:00.545353: Current learning rate: 0.00867
2024-12-18 00:12:49.858688: Validation loss did not improve from -0.54235. Patience: 4/50
2024-12-18 00:12:49.860244: train_loss -0.6721
2024-12-18 00:12:49.861886: val_loss -0.4863
2024-12-18 00:12:49.862960: Pseudo dice [0.7215]
2024-12-18 00:12:49.863938: Epoch time: 349.32 s
2024-12-18 00:12:49.864881: Yayy! New best EMA pseudo Dice: 0.7169
2024-12-18 00:12:51.729714: 
2024-12-18 00:12:51.731085: Epoch 23
2024-12-18 00:12:51.732097: Current learning rate: 0.00861
2024-12-18 00:18:45.943185: Validation loss improved from -0.54235 to -0.54354! Patience: 4/50
2024-12-18 00:18:45.944086: train_loss -0.672
2024-12-18 00:18:45.944824: val_loss -0.5435
2024-12-18 00:18:45.945588: Pseudo dice [0.7405]
2024-12-18 00:18:45.946280: Epoch time: 354.22 s
2024-12-18 00:18:45.946962: Yayy! New best EMA pseudo Dice: 0.7192
2024-12-18 00:18:47.715885: 
2024-12-18 00:18:47.717362: Epoch 24
2024-12-18 00:18:47.718280: Current learning rate: 0.00855
2024-12-18 00:24:08.166867: Validation loss did not improve from -0.54354. Patience: 1/50
2024-12-18 00:24:08.168496: train_loss -0.6689
2024-12-18 00:24:08.169925: val_loss -0.4956
2024-12-18 00:24:08.171005: Pseudo dice [0.7225]
2024-12-18 00:24:08.172069: Epoch time: 320.45 s
2024-12-18 00:24:08.557546: Yayy! New best EMA pseudo Dice: 0.7196
2024-12-18 00:24:10.367987: 
2024-12-18 00:24:10.369759: Epoch 25
2024-12-18 00:24:10.370661: Current learning rate: 0.00849
2024-12-18 00:29:43.918967: Validation loss did not improve from -0.54354. Patience: 2/50
2024-12-18 00:29:43.920127: train_loss -0.6731
2024-12-18 00:29:43.921115: val_loss -0.5342
2024-12-18 00:29:43.922038: Pseudo dice [0.7396]
2024-12-18 00:29:43.922987: Epoch time: 333.55 s
2024-12-18 00:29:43.924004: Yayy! New best EMA pseudo Dice: 0.7216
2024-12-18 00:29:45.792737: 
2024-12-18 00:29:45.794249: Epoch 26
2024-12-18 00:29:45.795409: Current learning rate: 0.00843
2024-12-18 00:36:07.538949: Validation loss improved from -0.54354 to -0.55750! Patience: 2/50
2024-12-18 00:36:07.540140: train_loss -0.6796
2024-12-18 00:36:07.540969: val_loss -0.5575
2024-12-18 00:36:07.541745: Pseudo dice [0.7474]
2024-12-18 00:36:07.542505: Epoch time: 381.75 s
2024-12-18 00:36:07.543183: Yayy! New best EMA pseudo Dice: 0.7242
2024-12-18 00:36:09.409272: 
2024-12-18 00:36:09.413006: Epoch 27
2024-12-18 00:36:09.413914: Current learning rate: 0.00836
2024-12-18 00:41:55.992212: Validation loss did not improve from -0.55750. Patience: 1/50
2024-12-18 00:41:55.993702: train_loss -0.6773
2024-12-18 00:41:55.994673: val_loss -0.5147
2024-12-18 00:41:55.995395: Pseudo dice [0.7329]
2024-12-18 00:41:55.996032: Epoch time: 346.59 s
2024-12-18 00:41:55.996748: Yayy! New best EMA pseudo Dice: 0.725
2024-12-18 00:41:57.797378: 
2024-12-18 00:41:57.798680: Epoch 28
2024-12-18 00:41:57.799498: Current learning rate: 0.0083
2024-12-18 00:47:23.003045: Validation loss did not improve from -0.55750. Patience: 2/50
2024-12-18 00:47:23.004843: train_loss -0.685
2024-12-18 00:47:23.005742: val_loss -0.5551
2024-12-18 00:47:23.006562: Pseudo dice [0.7505]
2024-12-18 00:47:23.007265: Epoch time: 325.21 s
2024-12-18 00:47:23.007998: Yayy! New best EMA pseudo Dice: 0.7276
2024-12-18 00:47:25.283698: 
2024-12-18 00:47:25.285047: Epoch 29
2024-12-18 00:47:25.285930: Current learning rate: 0.00824
2024-12-18 00:52:56.127159: Validation loss did not improve from -0.55750. Patience: 3/50
2024-12-18 00:52:56.128255: train_loss -0.6753
2024-12-18 00:52:56.129143: val_loss -0.544
2024-12-18 00:52:56.129826: Pseudo dice [0.7374]
2024-12-18 00:52:56.130767: Epoch time: 330.85 s
2024-12-18 00:52:56.571847: Yayy! New best EMA pseudo Dice: 0.7286
2024-12-18 00:52:58.404840: 
2024-12-18 00:52:58.406198: Epoch 30
2024-12-18 00:52:58.407048: Current learning rate: 0.00818
2024-12-18 00:58:44.641789: Validation loss improved from -0.55750 to -0.56517! Patience: 3/50
2024-12-18 00:58:44.642811: train_loss -0.6885
2024-12-18 00:58:44.643787: val_loss -0.5652
2024-12-18 00:58:44.644786: Pseudo dice [0.7511]
2024-12-18 00:58:44.645679: Epoch time: 346.24 s
2024-12-18 00:58:44.646601: Yayy! New best EMA pseudo Dice: 0.7308
2024-12-18 00:58:46.427370: 
2024-12-18 00:58:46.428759: Epoch 31
2024-12-18 00:58:46.429825: Current learning rate: 0.00812
2024-12-18 01:04:48.132684: Validation loss did not improve from -0.56517. Patience: 1/50
2024-12-18 01:04:48.133903: train_loss -0.6932
2024-12-18 01:04:48.134799: val_loss -0.5404
2024-12-18 01:04:48.135557: Pseudo dice [0.7427]
2024-12-18 01:04:48.136309: Epoch time: 361.71 s
2024-12-18 01:04:48.137048: Yayy! New best EMA pseudo Dice: 0.732
2024-12-18 01:04:50.036832: 
2024-12-18 01:04:50.038310: Epoch 32
2024-12-18 01:04:50.039399: Current learning rate: 0.00806
2024-12-18 01:10:07.561036: Validation loss did not improve from -0.56517. Patience: 2/50
2024-12-18 01:10:07.561694: train_loss -0.6889
2024-12-18 01:10:07.562555: val_loss -0.5529
2024-12-18 01:10:07.563387: Pseudo dice [0.7506]
2024-12-18 01:10:07.564359: Epoch time: 317.53 s
2024-12-18 01:10:07.565167: Yayy! New best EMA pseudo Dice: 0.7339
2024-12-18 01:10:09.379430: 
2024-12-18 01:10:09.380683: Epoch 33
2024-12-18 01:10:09.381582: Current learning rate: 0.008
2024-12-18 01:15:44.791703: Validation loss did not improve from -0.56517. Patience: 3/50
2024-12-18 01:15:44.793797: train_loss -0.6985
2024-12-18 01:15:44.795552: val_loss -0.5024
2024-12-18 01:15:44.796366: Pseudo dice [0.7223]
2024-12-18 01:15:44.797495: Epoch time: 335.42 s
2024-12-18 01:15:46.265748: 
2024-12-18 01:15:46.266821: Epoch 34
2024-12-18 01:15:46.267682: Current learning rate: 0.00793
2024-12-18 01:21:49.676708: Validation loss did not improve from -0.56517. Patience: 4/50
2024-12-18 01:21:49.677831: train_loss -0.6935
2024-12-18 01:21:49.678693: val_loss -0.5612
2024-12-18 01:21:49.679486: Pseudo dice [0.7501]
2024-12-18 01:21:49.680189: Epoch time: 363.41 s
2024-12-18 01:21:50.065063: Yayy! New best EMA pseudo Dice: 0.7345
2024-12-18 01:21:51.966812: 
2024-12-18 01:21:51.968133: Epoch 35
2024-12-18 01:21:51.968995: Current learning rate: 0.00787
2024-12-18 01:27:32.080444: Validation loss improved from -0.56517 to -0.56526! Patience: 4/50
2024-12-18 01:27:32.081889: train_loss -0.7003
2024-12-18 01:27:32.082623: val_loss -0.5653
2024-12-18 01:27:32.083405: Pseudo dice [0.7546]
2024-12-18 01:27:32.084145: Epoch time: 340.12 s
2024-12-18 01:27:32.084816: Yayy! New best EMA pseudo Dice: 0.7365
2024-12-18 01:27:33.878098: 
2024-12-18 01:27:33.879070: Epoch 36
2024-12-18 01:27:33.879750: Current learning rate: 0.00781
2024-12-18 01:33:10.978682: Validation loss did not improve from -0.56526. Patience: 1/50
2024-12-18 01:33:10.979817: train_loss -0.6945
2024-12-18 01:33:10.980743: val_loss -0.5184
2024-12-18 01:33:10.981619: Pseudo dice [0.7296]
2024-12-18 01:33:10.982538: Epoch time: 337.1 s
2024-12-18 01:33:12.469618: 
2024-12-18 01:33:12.471086: Epoch 37
2024-12-18 01:33:12.471928: Current learning rate: 0.00775
2024-12-18 01:38:37.702199: Validation loss did not improve from -0.56526. Patience: 2/50
2024-12-18 01:38:37.703354: train_loss -0.7063
2024-12-18 01:38:37.704219: val_loss -0.5513
2024-12-18 01:38:37.704910: Pseudo dice [0.7477]
2024-12-18 01:38:37.705602: Epoch time: 325.24 s
2024-12-18 01:38:37.706378: Yayy! New best EMA pseudo Dice: 0.737
2024-12-18 01:38:39.545879: 
2024-12-18 01:38:39.547145: Epoch 38
2024-12-18 01:38:39.548004: Current learning rate: 0.00769
2024-12-18 01:44:15.064416: Validation loss did not improve from -0.56526. Patience: 3/50
2024-12-18 01:44:15.065427: train_loss -0.7011
2024-12-18 01:44:15.066317: val_loss -0.5609
2024-12-18 01:44:15.067145: Pseudo dice [0.7514]
2024-12-18 01:44:15.067878: Epoch time: 335.52 s
2024-12-18 01:44:15.068648: Yayy! New best EMA pseudo Dice: 0.7384
2024-12-18 01:44:16.947831: 
2024-12-18 01:44:16.949177: Epoch 39
2024-12-18 01:44:16.949961: Current learning rate: 0.00763
2024-12-18 01:49:52.770667: Validation loss did not improve from -0.56526. Patience: 4/50
2024-12-18 01:49:52.771634: train_loss -0.705
2024-12-18 01:49:52.772528: val_loss -0.5354
2024-12-18 01:49:52.773293: Pseudo dice [0.7378]
2024-12-18 01:49:52.774040: Epoch time: 335.82 s
2024-12-18 01:49:55.022866: 
2024-12-18 01:49:55.024336: Epoch 40
2024-12-18 01:49:55.025216: Current learning rate: 0.00756
2024-12-18 01:54:58.424846: Validation loss did not improve from -0.56526. Patience: 5/50
2024-12-18 01:54:58.425865: train_loss -0.7132
2024-12-18 01:54:58.426680: val_loss -0.5554
2024-12-18 01:54:58.427485: Pseudo dice [0.7441]
2024-12-18 01:54:58.428235: Epoch time: 303.4 s
2024-12-18 01:54:58.428950: Yayy! New best EMA pseudo Dice: 0.7389
2024-12-18 01:55:00.459698: 
2024-12-18 01:55:00.461110: Epoch 41
2024-12-18 01:55:00.461990: Current learning rate: 0.0075
2024-12-18 01:59:40.448320: Validation loss did not improve from -0.56526. Patience: 6/50
2024-12-18 01:59:40.449275: train_loss -0.7098
2024-12-18 01:59:40.450118: val_loss -0.5263
2024-12-18 01:59:40.450881: Pseudo dice [0.7359]
2024-12-18 01:59:40.451748: Epoch time: 279.99 s
2024-12-18 01:59:41.852462: 
2024-12-18 01:59:41.853644: Epoch 42
2024-12-18 01:59:41.854522: Current learning rate: 0.00744
2024-12-18 02:05:03.011612: Validation loss did not improve from -0.56526. Patience: 7/50
2024-12-18 02:05:03.012640: train_loss -0.7134
2024-12-18 02:05:03.013515: val_loss -0.5605
2024-12-18 02:05:03.014259: Pseudo dice [0.749]
2024-12-18 02:05:03.014982: Epoch time: 321.16 s
2024-12-18 02:05:03.015751: Yayy! New best EMA pseudo Dice: 0.7397
2024-12-18 02:05:04.800237: 
2024-12-18 02:05:04.801548: Epoch 43
2024-12-18 02:05:04.802300: Current learning rate: 0.00738
2024-12-18 02:10:14.555813: Validation loss did not improve from -0.56526. Patience: 8/50
2024-12-18 02:10:14.557082: train_loss -0.7199
2024-12-18 02:10:14.557895: val_loss -0.5295
2024-12-18 02:10:14.558634: Pseudo dice [0.7404]
2024-12-18 02:10:14.559431: Epoch time: 309.76 s
2024-12-18 02:10:14.560221: Yayy! New best EMA pseudo Dice: 0.7397
2024-12-18 02:10:16.391261: 
2024-12-18 02:10:16.392521: Epoch 44
2024-12-18 02:10:16.393299: Current learning rate: 0.00732
2024-12-18 02:15:20.379664: Validation loss did not improve from -0.56526. Patience: 9/50
2024-12-18 02:15:20.380400: train_loss -0.7217
2024-12-18 02:15:20.381114: val_loss -0.4999
2024-12-18 02:15:20.381919: Pseudo dice [0.7293]
2024-12-18 02:15:20.382682: Epoch time: 303.99 s
2024-12-18 02:15:22.148754: 
2024-12-18 02:15:22.149634: Epoch 45
2024-12-18 02:15:22.150371: Current learning rate: 0.00725
2024-12-18 02:20:22.325365: Validation loss did not improve from -0.56526. Patience: 10/50
2024-12-18 02:20:22.326477: train_loss -0.72
2024-12-18 02:20:22.327281: val_loss -0.5181
2024-12-18 02:20:22.327991: Pseudo dice [0.7306]
2024-12-18 02:20:22.328866: Epoch time: 300.18 s
2024-12-18 02:20:23.734007: 
2024-12-18 02:20:23.735357: Epoch 46
2024-12-18 02:20:23.736732: Current learning rate: 0.00719
2024-12-18 02:25:13.920098: Validation loss did not improve from -0.56526. Patience: 11/50
2024-12-18 02:25:13.921267: train_loss -0.72
2024-12-18 02:25:13.922097: val_loss -0.5355
2024-12-18 02:25:13.922877: Pseudo dice [0.7433]
2024-12-18 02:25:13.923711: Epoch time: 290.19 s
2024-12-18 02:25:15.325973: 
2024-12-18 02:25:15.327485: Epoch 47
2024-12-18 02:25:15.328539: Current learning rate: 0.00713
2024-12-18 02:30:58.747034: Validation loss did not improve from -0.56526. Patience: 12/50
2024-12-18 02:30:58.749183: train_loss -0.7211
2024-12-18 02:30:58.750302: val_loss -0.5474
2024-12-18 02:30:58.751011: Pseudo dice [0.7412]
2024-12-18 02:30:58.751828: Epoch time: 343.42 s
2024-12-18 02:31:00.188625: 
2024-12-18 02:31:00.189993: Epoch 48
2024-12-18 02:31:00.190762: Current learning rate: 0.00707
2024-12-18 02:37:11.158856: Validation loss did not improve from -0.56526. Patience: 13/50
2024-12-18 02:37:11.176500: train_loss -0.7238
2024-12-18 02:37:11.177691: val_loss -0.538
2024-12-18 02:37:11.178401: Pseudo dice [0.7484]
2024-12-18 02:37:11.179190: Epoch time: 370.99 s
2024-12-18 02:37:12.683946: 
2024-12-18 02:37:12.685248: Epoch 49
2024-12-18 02:37:12.686025: Current learning rate: 0.007
2024-12-18 02:43:42.378186: Validation loss did not improve from -0.56526. Patience: 14/50
2024-12-18 02:43:42.379201: train_loss -0.7251
2024-12-18 02:43:42.380014: val_loss -0.5194
2024-12-18 02:43:42.380877: Pseudo dice [0.7411]
2024-12-18 02:43:42.381681: Epoch time: 389.7 s
2024-12-18 02:43:42.731112: Yayy! New best EMA pseudo Dice: 0.7398
2024-12-18 02:43:45.141217: 
2024-12-18 02:43:45.142784: Epoch 50
2024-12-18 02:43:45.143698: Current learning rate: 0.00694
2024-12-18 02:50:03.267862: Validation loss did not improve from -0.56526. Patience: 15/50
2024-12-18 02:50:03.268758: train_loss -0.7244
2024-12-18 02:50:03.269738: val_loss -0.5231
2024-12-18 02:50:03.270554: Pseudo dice [0.7368]
2024-12-18 02:50:03.271244: Epoch time: 378.13 s
2024-12-18 02:50:04.746191: 
2024-12-18 02:50:04.747733: Epoch 51
2024-12-18 02:50:04.748512: Current learning rate: 0.00688
2024-12-18 02:55:52.513675: Validation loss did not improve from -0.56526. Patience: 16/50
2024-12-18 02:55:52.514657: train_loss -0.7198
2024-12-18 02:55:52.515837: val_loss -0.5272
2024-12-18 02:55:52.517030: Pseudo dice [0.7443]
2024-12-18 02:55:52.518003: Epoch time: 347.77 s
2024-12-18 02:55:52.518882: Yayy! New best EMA pseudo Dice: 0.74
2024-12-18 02:55:54.366338: 
2024-12-18 02:55:54.367914: Epoch 52
2024-12-18 02:55:54.368834: Current learning rate: 0.00682
2024-12-18 03:01:30.752536: Validation loss did not improve from -0.56526. Patience: 17/50
2024-12-18 03:01:30.753439: train_loss -0.7254
2024-12-18 03:01:30.754231: val_loss -0.5363
2024-12-18 03:01:30.754950: Pseudo dice [0.7447]
2024-12-18 03:01:30.755638: Epoch time: 336.39 s
2024-12-18 03:01:30.756589: Yayy! New best EMA pseudo Dice: 0.7405
2024-12-18 03:01:32.580729: 
2024-12-18 03:01:32.582288: Epoch 53
2024-12-18 03:01:32.583247: Current learning rate: 0.00675
2024-12-18 03:07:19.453528: Validation loss improved from -0.56526 to -0.56608! Patience: 17/50
2024-12-18 03:07:19.454499: train_loss -0.7279
2024-12-18 03:07:19.455474: val_loss -0.5661
2024-12-18 03:07:19.456214: Pseudo dice [0.7574]
2024-12-18 03:07:19.457001: Epoch time: 346.88 s
2024-12-18 03:07:19.457797: Yayy! New best EMA pseudo Dice: 0.7421
2024-12-18 03:07:21.263431: 
2024-12-18 03:07:21.265083: Epoch 54
2024-12-18 03:07:21.266310: Current learning rate: 0.00669
2024-12-18 03:13:28.779399: Validation loss did not improve from -0.56608. Patience: 1/50
2024-12-18 03:13:28.780381: train_loss -0.7315
2024-12-18 03:13:28.781297: val_loss -0.527
2024-12-18 03:13:28.782078: Pseudo dice [0.741]
2024-12-18 03:13:28.782886: Epoch time: 367.52 s
2024-12-18 03:13:30.659558: 
2024-12-18 03:13:30.661121: Epoch 55
2024-12-18 03:13:30.662155: Current learning rate: 0.00663
2024-12-18 03:19:45.532762: Validation loss did not improve from -0.56608. Patience: 2/50
2024-12-18 03:19:45.533818: train_loss -0.7333
2024-12-18 03:19:45.534752: val_loss -0.5658
2024-12-18 03:19:45.535431: Pseudo dice [0.7574]
2024-12-18 03:19:45.536112: Epoch time: 374.88 s
2024-12-18 03:19:45.536820: Yayy! New best EMA pseudo Dice: 0.7436
2024-12-18 03:19:47.397392: 
2024-12-18 03:19:47.398703: Epoch 56
2024-12-18 03:19:47.399428: Current learning rate: 0.00657
2024-12-18 03:26:08.744887: Validation loss did not improve from -0.56608. Patience: 3/50
2024-12-18 03:26:08.747804: train_loss -0.7325
2024-12-18 03:26:08.749858: val_loss -0.5313
2024-12-18 03:26:08.750787: Pseudo dice [0.7377]
2024-12-18 03:26:08.752186: Epoch time: 381.35 s
2024-12-18 03:26:10.211846: 
2024-12-18 03:26:10.213234: Epoch 57
2024-12-18 03:26:10.213953: Current learning rate: 0.0065
2024-12-18 03:32:13.675218: Validation loss did not improve from -0.56608. Patience: 4/50
2024-12-18 03:32:13.676400: train_loss -0.7361
2024-12-18 03:32:13.677212: val_loss -0.5314
2024-12-18 03:32:13.678007: Pseudo dice [0.7448]
2024-12-18 03:32:13.678806: Epoch time: 363.47 s
2024-12-18 03:32:15.167445: 
2024-12-18 03:32:15.168932: Epoch 58
2024-12-18 03:32:15.169869: Current learning rate: 0.00644
2024-12-18 03:37:51.051859: Validation loss did not improve from -0.56608. Patience: 5/50
2024-12-18 03:37:51.053579: train_loss -0.7351
2024-12-18 03:37:51.054727: val_loss -0.5117
2024-12-18 03:37:51.055558: Pseudo dice [0.7378]
2024-12-18 03:37:51.056403: Epoch time: 335.89 s
2024-12-18 03:37:52.562104: 
2024-12-18 03:37:52.563631: Epoch 59
2024-12-18 03:37:52.564587: Current learning rate: 0.00638
2024-12-18 03:43:31.095924: Validation loss did not improve from -0.56608. Patience: 6/50
2024-12-18 03:43:31.098097: train_loss -0.7337
2024-12-18 03:43:31.098991: val_loss -0.542
2024-12-18 03:43:31.099753: Pseudo dice [0.7453]
2024-12-18 03:43:31.100500: Epoch time: 338.54 s
2024-12-18 03:43:33.015237: 
2024-12-18 03:43:33.016609: Epoch 60
2024-12-18 03:43:33.017481: Current learning rate: 0.00631
2024-12-18 03:49:29.052226: Validation loss improved from -0.56608 to -0.57006! Patience: 6/50
2024-12-18 03:49:29.054083: train_loss -0.7381
2024-12-18 03:49:29.055048: val_loss -0.5701
2024-12-18 03:49:29.055778: Pseudo dice [0.76]
2024-12-18 03:49:29.056565: Epoch time: 356.04 s
2024-12-18 03:49:29.057521: Yayy! New best EMA pseudo Dice: 0.7446
2024-12-18 03:49:31.395023: 
2024-12-18 03:49:31.396502: Epoch 61
2024-12-18 03:49:31.397452: Current learning rate: 0.00625
2024-12-18 03:55:36.156904: Validation loss did not improve from -0.57006. Patience: 1/50
2024-12-18 03:55:36.158029: train_loss -0.7394
2024-12-18 03:55:36.158863: val_loss -0.5383
2024-12-18 03:55:36.159573: Pseudo dice [0.7487]
2024-12-18 03:55:36.160425: Epoch time: 364.76 s
2024-12-18 03:55:36.161084: Yayy! New best EMA pseudo Dice: 0.745
2024-12-18 03:55:38.039451: 
2024-12-18 03:55:38.040922: Epoch 62
2024-12-18 03:55:38.041785: Current learning rate: 0.00619
2024-12-18 04:01:34.816350: Validation loss improved from -0.57006 to -0.57494! Patience: 1/50
2024-12-18 04:01:34.817481: train_loss -0.7366
2024-12-18 04:01:34.818377: val_loss -0.5749
2024-12-18 04:01:34.819184: Pseudo dice [0.7622]
2024-12-18 04:01:34.820064: Epoch time: 356.78 s
2024-12-18 04:01:34.820920: Yayy! New best EMA pseudo Dice: 0.7467
2024-12-18 04:01:36.708375: 
2024-12-18 04:01:36.710045: Epoch 63
2024-12-18 04:01:36.710961: Current learning rate: 0.00612
2024-12-18 04:07:43.054174: Validation loss did not improve from -0.57494. Patience: 1/50
2024-12-18 04:07:43.055271: train_loss -0.746
2024-12-18 04:07:43.056059: val_loss -0.5365
2024-12-18 04:07:43.056700: Pseudo dice [0.7436]
2024-12-18 04:07:43.057377: Epoch time: 366.35 s
2024-12-18 04:07:44.496864: 
2024-12-18 04:07:44.498234: Epoch 64
2024-12-18 04:07:44.499101: Current learning rate: 0.00606
2024-12-18 04:14:01.512718: Validation loss did not improve from -0.57494. Patience: 2/50
2024-12-18 04:14:01.513631: train_loss -0.7439
2024-12-18 04:14:01.514522: val_loss -0.5436
2024-12-18 04:14:01.515479: Pseudo dice [0.7503]
2024-12-18 04:14:01.516325: Epoch time: 377.02 s
2024-12-18 04:14:01.962531: Yayy! New best EMA pseudo Dice: 0.7468
2024-12-18 04:14:03.876588: 
2024-12-18 04:14:03.877853: Epoch 65
2024-12-18 04:14:03.878797: Current learning rate: 0.006
2024-12-18 04:19:34.950453: Validation loss did not improve from -0.57494. Patience: 3/50
2024-12-18 04:19:34.951442: train_loss -0.7454
2024-12-18 04:19:34.952708: val_loss -0.4815
2024-12-18 04:19:34.953641: Pseudo dice [0.7212]
2024-12-18 04:19:34.954615: Epoch time: 331.08 s
2024-12-18 04:19:36.475206: 
2024-12-18 04:19:36.476540: Epoch 66
2024-12-18 04:19:36.477415: Current learning rate: 0.00593
2024-12-18 04:25:16.252577: Validation loss did not improve from -0.57494. Patience: 4/50
2024-12-18 04:25:16.253434: train_loss -0.7461
2024-12-18 04:25:16.254491: val_loss -0.5231
2024-12-18 04:25:16.255422: Pseudo dice [0.7244]
2024-12-18 04:25:16.256306: Epoch time: 339.78 s
2024-12-18 04:25:17.753084: 
2024-12-18 04:25:17.754144: Epoch 67
2024-12-18 04:25:17.754963: Current learning rate: 0.00587
2024-12-18 04:31:33.110440: Validation loss did not improve from -0.57494. Patience: 5/50
2024-12-18 04:31:33.112263: train_loss -0.7465
2024-12-18 04:31:33.113503: val_loss -0.5622
2024-12-18 04:31:33.114230: Pseudo dice [0.7549]
2024-12-18 04:31:33.115273: Epoch time: 375.36 s
2024-12-18 04:31:34.635273: 
2024-12-18 04:31:34.636847: Epoch 68
2024-12-18 04:31:34.637780: Current learning rate: 0.00581
2024-12-18 04:37:48.746975: Validation loss did not improve from -0.57494. Patience: 6/50
2024-12-18 04:37:48.748131: train_loss -0.7456
2024-12-18 04:37:48.748930: val_loss -0.5255
2024-12-18 04:37:48.749722: Pseudo dice [0.7392]
2024-12-18 04:37:48.750408: Epoch time: 374.11 s
2024-12-18 04:37:50.232550: 
2024-12-18 04:37:50.233769: Epoch 69
2024-12-18 04:37:50.234469: Current learning rate: 0.00574
2024-12-18 04:44:08.924532: Validation loss did not improve from -0.57494. Patience: 7/50
2024-12-18 04:44:08.926255: train_loss -0.7482
2024-12-18 04:44:08.927337: val_loss -0.5539
2024-12-18 04:44:08.928175: Pseudo dice [0.7509]
2024-12-18 04:44:08.929044: Epoch time: 378.69 s
2024-12-18 04:44:10.875197: 
2024-12-18 04:44:10.876479: Epoch 70
2024-12-18 04:44:10.877390: Current learning rate: 0.00568
2024-12-18 04:50:11.536289: Validation loss did not improve from -0.57494. Patience: 8/50
2024-12-18 04:50:11.537491: train_loss -0.7491
2024-12-18 04:50:11.538215: val_loss -0.5505
2024-12-18 04:50:11.538969: Pseudo dice [0.7469]
2024-12-18 04:50:11.539886: Epoch time: 360.66 s
2024-12-18 04:50:13.649748: 
2024-12-18 04:50:13.651063: Epoch 71
2024-12-18 04:50:13.651833: Current learning rate: 0.00562
2024-12-18 04:56:39.947666: Validation loss did not improve from -0.57494. Patience: 9/50
2024-12-18 04:56:39.948624: train_loss -0.7546
2024-12-18 04:56:39.949486: val_loss -0.5434
2024-12-18 04:56:39.950154: Pseudo dice [0.7469]
2024-12-18 04:56:39.950867: Epoch time: 386.3 s
2024-12-18 04:56:41.430403: 
2024-12-18 04:56:41.431737: Epoch 72
2024-12-18 04:56:41.432405: Current learning rate: 0.00555
2024-12-18 05:02:40.129552: Validation loss did not improve from -0.57494. Patience: 10/50
2024-12-18 05:02:40.130544: train_loss -0.7546
2024-12-18 05:02:40.131608: val_loss -0.5567
2024-12-18 05:02:40.132629: Pseudo dice [0.7508]
2024-12-18 05:02:40.133623: Epoch time: 358.7 s
2024-12-18 05:02:41.581704: 
2024-12-18 05:02:41.583010: Epoch 73
2024-12-18 05:02:41.583894: Current learning rate: 0.00549
2024-12-18 05:08:22.093997: Validation loss did not improve from -0.57494. Patience: 11/50
2024-12-18 05:08:22.094871: train_loss -0.7572
2024-12-18 05:08:22.095801: val_loss -0.5355
2024-12-18 05:08:22.096668: Pseudo dice [0.7388]
2024-12-18 05:08:22.097644: Epoch time: 340.51 s
2024-12-18 05:08:23.605451: 
2024-12-18 05:08:23.606796: Epoch 74
2024-12-18 05:08:23.607783: Current learning rate: 0.00542
2024-12-18 05:14:12.470854: Validation loss did not improve from -0.57494. Patience: 12/50
2024-12-18 05:14:12.471918: train_loss -0.7517
2024-12-18 05:14:12.472828: val_loss -0.5368
2024-12-18 05:14:12.473648: Pseudo dice [0.743]
2024-12-18 05:14:12.474560: Epoch time: 348.87 s
2024-12-18 05:14:14.275111: 
2024-12-18 05:14:14.276361: Epoch 75
2024-12-18 05:14:14.277218: Current learning rate: 0.00536
2024-12-18 05:20:20.611977: Validation loss did not improve from -0.57494. Patience: 13/50
2024-12-18 05:20:20.613106: train_loss -0.7553
2024-12-18 05:20:20.614063: val_loss -0.559
2024-12-18 05:20:20.614938: Pseudo dice [0.7543]
2024-12-18 05:20:20.615768: Epoch time: 366.34 s
2024-12-18 05:20:21.996890: 
2024-12-18 05:20:21.998402: Epoch 76
2024-12-18 05:20:21.999510: Current learning rate: 0.00529
2024-12-18 05:26:35.205692: Validation loss did not improve from -0.57494. Patience: 14/50
2024-12-18 05:26:35.206710: train_loss -0.7547
2024-12-18 05:26:35.207699: val_loss -0.5609
2024-12-18 05:26:35.208595: Pseudo dice [0.76]
2024-12-18 05:26:35.209469: Epoch time: 373.21 s
2024-12-18 05:26:36.673866: 
2024-12-18 05:26:36.675106: Epoch 77
2024-12-18 05:26:36.675992: Current learning rate: 0.00523
2024-12-18 05:32:29.758562: Validation loss did not improve from -0.57494. Patience: 15/50
2024-12-18 05:32:29.759482: train_loss -0.7597
2024-12-18 05:32:29.760310: val_loss -0.5325
2024-12-18 05:32:29.761014: Pseudo dice [0.7399]
2024-12-18 05:32:29.761668: Epoch time: 353.09 s
2024-12-18 05:32:31.247226: 
2024-12-18 05:32:31.248403: Epoch 78
2024-12-18 05:32:31.249213: Current learning rate: 0.00517
2024-12-18 05:38:13.700036: Validation loss did not improve from -0.57494. Patience: 16/50
2024-12-18 05:38:13.702114: train_loss -0.7561
2024-12-18 05:38:13.703750: val_loss -0.5626
2024-12-18 05:38:13.704427: Pseudo dice [0.7552]
2024-12-18 05:38:13.705438: Epoch time: 342.46 s
2024-12-18 05:38:13.706367: Yayy! New best EMA pseudo Dice: 0.747
2024-12-18 05:38:15.569664: 
2024-12-18 05:38:15.571464: Epoch 79
2024-12-18 05:38:15.572348: Current learning rate: 0.0051
2024-12-18 05:44:16.852497: Validation loss did not improve from -0.57494. Patience: 17/50
2024-12-18 05:44:16.854831: train_loss -0.756
2024-12-18 05:44:16.856304: val_loss -0.5088
2024-12-18 05:44:16.857246: Pseudo dice [0.7357]
2024-12-18 05:44:16.857921: Epoch time: 361.29 s
2024-12-18 05:44:18.706952: 
2024-12-18 05:44:18.708452: Epoch 80
2024-12-18 05:44:18.709323: Current learning rate: 0.00504
2024-12-18 05:50:26.842221: Validation loss did not improve from -0.57494. Patience: 18/50
2024-12-18 05:50:26.845809: train_loss -0.7646
2024-12-18 05:50:26.846762: val_loss -0.5649
2024-12-18 05:50:26.847396: Pseudo dice [0.7627]
2024-12-18 05:50:26.848073: Epoch time: 368.14 s
2024-12-18 05:50:26.848701: Yayy! New best EMA pseudo Dice: 0.7476
2024-12-18 05:50:28.628835: 
2024-12-18 05:50:28.630020: Epoch 81
2024-12-18 05:50:28.630739: Current learning rate: 0.00497
2024-12-18 05:56:16.488705: Validation loss did not improve from -0.57494. Patience: 19/50
2024-12-18 05:56:16.489838: train_loss -0.7563
2024-12-18 05:56:16.490724: val_loss -0.5574
2024-12-18 05:56:16.491429: Pseudo dice [0.75]
2024-12-18 05:56:16.492066: Epoch time: 347.86 s
2024-12-18 05:56:16.492889: Yayy! New best EMA pseudo Dice: 0.7478
2024-12-18 05:56:18.803133: 
2024-12-18 05:56:18.804650: Epoch 82
2024-12-18 05:56:18.805451: Current learning rate: 0.00491
2024-12-18 06:01:55.156819: Validation loss did not improve from -0.57494. Patience: 20/50
2024-12-18 06:01:55.157829: train_loss -0.7612
2024-12-18 06:01:55.158620: val_loss -0.5626
2024-12-18 06:01:55.159454: Pseudo dice [0.7532]
2024-12-18 06:01:55.160290: Epoch time: 336.36 s
2024-12-18 06:01:55.161186: Yayy! New best EMA pseudo Dice: 0.7483
2024-12-18 06:01:56.854958: 
2024-12-18 06:01:56.856445: Epoch 83
2024-12-18 06:01:56.857527: Current learning rate: 0.00484
2024-12-18 06:08:24.518430: Validation loss improved from -0.57494 to -0.58169! Patience: 20/50
2024-12-18 06:08:24.519392: train_loss -0.7634
2024-12-18 06:08:24.520305: val_loss -0.5817
2024-12-18 06:08:24.521093: Pseudo dice [0.7682]
2024-12-18 06:08:24.521900: Epoch time: 387.67 s
2024-12-18 06:08:24.522951: Yayy! New best EMA pseudo Dice: 0.7503
2024-12-18 06:08:26.361855: 
2024-12-18 06:08:26.363276: Epoch 84
2024-12-18 06:08:26.364181: Current learning rate: 0.00478
2024-12-18 06:14:32.351388: Validation loss did not improve from -0.58169. Patience: 1/50
2024-12-18 06:14:32.352292: train_loss -0.7644
2024-12-18 06:14:32.353051: val_loss -0.5518
2024-12-18 06:14:32.353837: Pseudo dice [0.7521]
2024-12-18 06:14:32.354701: Epoch time: 365.99 s
2024-12-18 06:14:32.738622: Yayy! New best EMA pseudo Dice: 0.7505
2024-12-18 06:14:34.486867: 
2024-12-18 06:14:34.488108: Epoch 85
2024-12-18 06:14:34.488835: Current learning rate: 0.00471
2024-12-18 06:20:40.173375: Validation loss did not improve from -0.58169. Patience: 2/50
2024-12-18 06:20:40.174513: train_loss -0.762
2024-12-18 06:20:40.175319: val_loss -0.5643
2024-12-18 06:20:40.176086: Pseudo dice [0.7558]
2024-12-18 06:20:40.176709: Epoch time: 365.69 s
2024-12-18 06:20:40.177359: Yayy! New best EMA pseudo Dice: 0.751
2024-12-18 06:20:41.954733: 
2024-12-18 06:20:41.956123: Epoch 86
2024-12-18 06:20:41.956858: Current learning rate: 0.00465
2024-12-18 06:26:34.658550: Validation loss did not improve from -0.58169. Patience: 3/50
2024-12-18 06:26:34.659624: train_loss -0.7666
2024-12-18 06:26:34.660466: val_loss -0.5395
2024-12-18 06:26:34.661159: Pseudo dice [0.7543]
2024-12-18 06:26:34.661897: Epoch time: 352.71 s
2024-12-18 06:26:34.663142: Yayy! New best EMA pseudo Dice: 0.7514
2024-12-18 06:26:36.468263: 
2024-12-18 06:26:36.469818: Epoch 87
2024-12-18 06:26:36.470787: Current learning rate: 0.00458
2024-12-18 06:32:45.682409: Validation loss did not improve from -0.58169. Patience: 4/50
2024-12-18 06:32:45.683342: train_loss -0.7617
2024-12-18 06:32:45.684314: val_loss -0.5364
2024-12-18 06:32:45.685185: Pseudo dice [0.7454]
2024-12-18 06:32:45.686468: Epoch time: 369.22 s
2024-12-18 06:32:47.028126: 
2024-12-18 06:32:47.029573: Epoch 88
2024-12-18 06:32:47.030429: Current learning rate: 0.00452
2024-12-18 06:38:40.453921: Validation loss did not improve from -0.58169. Patience: 5/50
2024-12-18 06:38:40.455026: train_loss -0.7624
2024-12-18 06:38:40.455930: val_loss -0.538
2024-12-18 06:38:40.456709: Pseudo dice [0.7484]
2024-12-18 06:38:40.457763: Epoch time: 353.43 s
2024-12-18 06:38:41.834253: 
2024-12-18 06:38:41.835771: Epoch 89
2024-12-18 06:38:41.837214: Current learning rate: 0.00445
2024-12-18 06:44:26.686669: Validation loss did not improve from -0.58169. Patience: 6/50
2024-12-18 06:44:26.687851: train_loss -0.7677
2024-12-18 06:44:26.688748: val_loss -0.5727
2024-12-18 06:44:26.689623: Pseudo dice [0.7656]
2024-12-18 06:44:26.690403: Epoch time: 344.86 s
2024-12-18 06:44:27.130141: Yayy! New best EMA pseudo Dice: 0.752
2024-12-18 06:44:28.846230: 
2024-12-18 06:44:28.847552: Epoch 90
2024-12-18 06:44:28.848407: Current learning rate: 0.00438
2024-12-18 06:50:47.512459: Validation loss did not improve from -0.58169. Patience: 7/50
2024-12-18 06:50:47.513534: train_loss -0.7644
2024-12-18 06:50:47.514681: val_loss -0.5465
2024-12-18 06:50:47.515349: Pseudo dice [0.7491]
2024-12-18 06:50:47.516123: Epoch time: 378.67 s
2024-12-18 06:50:48.857048: 
2024-12-18 06:50:48.858394: Epoch 91
2024-12-18 06:50:48.859161: Current learning rate: 0.00432
2024-12-18 06:56:45.240180: Validation loss did not improve from -0.58169. Patience: 8/50
2024-12-18 06:56:45.243785: train_loss -0.7706
2024-12-18 06:56:45.244684: val_loss -0.556
2024-12-18 06:56:45.245641: Pseudo dice [0.7499]
2024-12-18 06:56:45.246417: Epoch time: 356.39 s
2024-12-18 06:56:47.266296: 
2024-12-18 06:56:47.267218: Epoch 92
2024-12-18 06:56:47.267947: Current learning rate: 0.00425
2024-12-18 07:02:39.703579: Validation loss did not improve from -0.58169. Patience: 9/50
2024-12-18 07:02:39.704573: train_loss -0.7673
2024-12-18 07:02:39.705368: val_loss -0.5332
2024-12-18 07:02:39.706114: Pseudo dice [0.7494]
2024-12-18 07:02:39.706788: Epoch time: 352.44 s
2024-12-18 07:02:41.048445: 
2024-12-18 07:02:41.049943: Epoch 93
2024-12-18 07:02:41.050698: Current learning rate: 0.00419
2024-12-18 07:08:17.402749: Validation loss did not improve from -0.58169. Patience: 10/50
2024-12-18 07:08:17.403780: train_loss -0.7716
2024-12-18 07:08:17.404675: val_loss -0.5657
2024-12-18 07:08:17.405441: Pseudo dice [0.7606]
2024-12-18 07:08:17.406216: Epoch time: 336.36 s
2024-12-18 07:08:17.407066: Yayy! New best EMA pseudo Dice: 0.7523
2024-12-18 07:08:19.087636: 
2024-12-18 07:08:19.088614: Epoch 94
2024-12-18 07:08:19.089581: Current learning rate: 0.00412
2024-12-18 07:14:39.431274: Validation loss improved from -0.58169 to -0.58382! Patience: 10/50
2024-12-18 07:14:39.432370: train_loss -0.7729
2024-12-18 07:14:39.433322: val_loss -0.5838
2024-12-18 07:14:39.434051: Pseudo dice [0.7629]
2024-12-18 07:14:39.434765: Epoch time: 380.35 s
2024-12-18 07:14:39.813174: Yayy! New best EMA pseudo Dice: 0.7533
2024-12-18 07:14:41.543549: 
2024-12-18 07:14:41.544719: Epoch 95
2024-12-18 07:14:41.545434: Current learning rate: 0.00405
2024-12-18 07:20:54.359777: Validation loss did not improve from -0.58382. Patience: 1/50
2024-12-18 07:20:54.360810: train_loss -0.7688
2024-12-18 07:20:54.361588: val_loss -0.5561
2024-12-18 07:20:54.362321: Pseudo dice [0.7482]
2024-12-18 07:20:54.362973: Epoch time: 372.82 s
2024-12-18 07:20:55.674204: 
2024-12-18 07:20:55.675327: Epoch 96
2024-12-18 07:20:55.676116: Current learning rate: 0.00399
2024-12-18 07:27:03.077315: Validation loss did not improve from -0.58382. Patience: 2/50
2024-12-18 07:27:03.078283: train_loss -0.7716
2024-12-18 07:27:03.078988: val_loss -0.5226
2024-12-18 07:27:03.079706: Pseudo dice [0.7369]
2024-12-18 07:27:03.080609: Epoch time: 367.41 s
2024-12-18 07:27:04.450193: 
2024-12-18 07:27:04.451385: Epoch 97
2024-12-18 07:27:04.452224: Current learning rate: 0.00392
2024-12-18 07:32:41.605761: Validation loss did not improve from -0.58382. Patience: 3/50
2024-12-18 07:32:41.606719: train_loss -0.773
2024-12-18 07:32:41.607637: val_loss -0.5481
2024-12-18 07:32:41.608299: Pseudo dice [0.7498]
2024-12-18 07:32:41.609029: Epoch time: 337.16 s
2024-12-18 07:32:42.998941: 
2024-12-18 07:32:43.000410: Epoch 98
2024-12-18 07:32:43.001559: Current learning rate: 0.00385
2024-12-18 07:38:55.006915: Validation loss did not improve from -0.58382. Patience: 4/50
2024-12-18 07:38:55.007849: train_loss -0.7716
2024-12-18 07:38:55.008561: val_loss -0.5696
2024-12-18 07:38:55.009249: Pseudo dice [0.7613]
2024-12-18 07:38:55.010062: Epoch time: 372.01 s
2024-12-18 07:38:56.392531: 
2024-12-18 07:38:56.393798: Epoch 99
2024-12-18 07:38:56.394617: Current learning rate: 0.00379
2024-12-18 07:44:38.126171: Validation loss did not improve from -0.58382. Patience: 5/50
2024-12-18 07:44:38.127108: train_loss -0.7705
2024-12-18 07:44:38.127937: val_loss -0.5714
2024-12-18 07:44:38.128733: Pseudo dice [0.7603]
2024-12-18 07:44:38.129543: Epoch time: 341.74 s
2024-12-18 07:44:39.919864: 
2024-12-18 07:44:39.921257: Epoch 100
2024-12-18 07:44:39.922386: Current learning rate: 0.00372
2024-12-18 07:50:42.571561: Validation loss did not improve from -0.58382. Patience: 6/50
2024-12-18 07:50:42.572635: train_loss -0.7759
2024-12-18 07:50:42.574531: val_loss -0.5398
2024-12-18 07:50:42.575174: Pseudo dice [0.745]
2024-12-18 07:50:42.576022: Epoch time: 362.65 s
2024-12-18 07:50:43.926389: 
2024-12-18 07:50:43.927797: Epoch 101
2024-12-18 07:50:43.928593: Current learning rate: 0.00365
2024-12-18 07:56:52.206282: Validation loss did not improve from -0.58382. Patience: 7/50
2024-12-18 07:56:52.207716: train_loss -0.7746
2024-12-18 07:56:52.208593: val_loss -0.5595
2024-12-18 07:56:52.209468: Pseudo dice [0.7527]
2024-12-18 07:56:52.210278: Epoch time: 368.28 s
2024-12-18 07:56:53.601331: 
2024-12-18 07:56:53.602592: Epoch 102
2024-12-18 07:56:53.603589: Current learning rate: 0.00359
2024-12-18 08:02:55.488199: Validation loss did not improve from -0.58382. Patience: 8/50
2024-12-18 08:02:55.489326: train_loss -0.774
2024-12-18 08:02:55.490278: val_loss -0.5519
2024-12-18 08:02:55.491093: Pseudo dice [0.7601]
2024-12-18 08:02:55.492008: Epoch time: 361.89 s
2024-12-18 08:02:56.917908: 
2024-12-18 08:02:56.919487: Epoch 103
2024-12-18 08:02:56.920191: Current learning rate: 0.00352
2024-12-18 08:08:32.103559: Validation loss did not improve from -0.58382. Patience: 9/50
2024-12-18 08:08:32.104653: train_loss -0.7753
2024-12-18 08:08:32.105869: val_loss -0.5342
2024-12-18 08:08:32.106856: Pseudo dice [0.7431]
2024-12-18 08:08:32.107872: Epoch time: 335.19 s
2024-12-18 08:08:34.472681: 
2024-12-18 08:08:34.474403: Epoch 104
2024-12-18 08:08:34.475588: Current learning rate: 0.00345
2024-12-18 08:14:34.703257: Validation loss did not improve from -0.58382. Patience: 10/50
2024-12-18 08:14:34.704365: train_loss -0.7781
2024-12-18 08:14:34.705100: val_loss -0.5545
2024-12-18 08:14:34.705763: Pseudo dice [0.7506]
2024-12-18 08:14:34.706481: Epoch time: 360.23 s
2024-12-18 08:14:36.557750: 
2024-12-18 08:14:36.559254: Epoch 105
2024-12-18 08:14:36.560271: Current learning rate: 0.00338
2024-12-18 08:20:28.503679: Validation loss improved from -0.58382 to -0.59353! Patience: 10/50
2024-12-18 08:20:28.504705: train_loss -0.7778
2024-12-18 08:20:28.505495: val_loss -0.5935
2024-12-18 08:20:28.506398: Pseudo dice [0.7712]
2024-12-18 08:20:28.507116: Epoch time: 351.95 s
2024-12-18 08:20:28.507773: Yayy! New best EMA pseudo Dice: 0.7538
2024-12-18 08:20:30.187418: 
2024-12-18 08:20:30.188542: Epoch 106
2024-12-18 08:20:30.189272: Current learning rate: 0.00332
2024-12-18 08:26:20.739392: Validation loss did not improve from -0.59353. Patience: 1/50
2024-12-18 08:26:20.740567: train_loss -0.7769
2024-12-18 08:26:20.741618: val_loss -0.5742
2024-12-18 08:26:20.742705: Pseudo dice [0.7668]
2024-12-18 08:26:20.743749: Epoch time: 350.55 s
2024-12-18 08:26:20.744797: Yayy! New best EMA pseudo Dice: 0.7551
2024-12-18 08:26:22.615084: 
2024-12-18 08:26:22.616550: Epoch 107
2024-12-18 08:26:22.617487: Current learning rate: 0.00325
2024-12-18 08:32:17.644300: Validation loss did not improve from -0.59353. Patience: 2/50
2024-12-18 08:32:17.645726: train_loss -0.7819
2024-12-18 08:32:17.646477: val_loss -0.5447
2024-12-18 08:32:17.647284: Pseudo dice [0.7544]
2024-12-18 08:32:17.648091: Epoch time: 355.03 s
2024-12-18 08:32:19.032092: 
2024-12-18 08:32:19.033158: Epoch 108
2024-12-18 08:32:19.033979: Current learning rate: 0.00318
2024-12-18 08:38:46.165132: Validation loss did not improve from -0.59353. Patience: 3/50
2024-12-18 08:38:46.165879: train_loss -0.7822
2024-12-18 08:38:46.166694: val_loss -0.5878
2024-12-18 08:38:46.167545: Pseudo dice [0.7697]
2024-12-18 08:38:46.168399: Epoch time: 387.14 s
2024-12-18 08:38:46.169180: Yayy! New best EMA pseudo Dice: 0.7565
2024-12-18 08:38:47.932975: 
2024-12-18 08:38:47.934235: Epoch 109
2024-12-18 08:38:47.935108: Current learning rate: 0.00311
2024-12-18 08:45:14.195517: Validation loss did not improve from -0.59353. Patience: 4/50
2024-12-18 08:45:14.196516: train_loss -0.7836
2024-12-18 08:45:14.197299: val_loss -0.5539
2024-12-18 08:45:14.197991: Pseudo dice [0.7533]
2024-12-18 08:45:14.198710: Epoch time: 386.26 s
2024-12-18 08:45:16.047545: 
2024-12-18 08:45:16.048903: Epoch 110
2024-12-18 08:45:16.049747: Current learning rate: 0.00304
2024-12-18 08:51:18.609803: Validation loss did not improve from -0.59353. Patience: 5/50
2024-12-18 08:51:18.611214: train_loss -0.7794
2024-12-18 08:51:18.612372: val_loss -0.5451
2024-12-18 08:51:18.613156: Pseudo dice [0.7526]
2024-12-18 08:51:18.614102: Epoch time: 362.57 s
2024-12-18 08:51:19.987876: 
2024-12-18 08:51:19.989366: Epoch 111
2024-12-18 08:51:19.990489: Current learning rate: 0.00297
2024-12-18 08:57:10.381056: Validation loss did not improve from -0.59353. Patience: 6/50
2024-12-18 08:57:10.382218: train_loss -0.7839
2024-12-18 08:57:10.382979: val_loss -0.5477
2024-12-18 08:57:10.383852: Pseudo dice [0.7497]
2024-12-18 08:57:10.384542: Epoch time: 350.4 s
2024-12-18 08:57:11.732825: 
2024-12-18 08:57:11.734096: Epoch 112
2024-12-18 08:57:11.735037: Current learning rate: 0.00291
2024-12-18 09:03:09.561875: Validation loss did not improve from -0.59353. Patience: 7/50
2024-12-18 09:03:09.587245: train_loss -0.7851
2024-12-18 09:03:09.588315: val_loss -0.5422
2024-12-18 09:03:09.588915: Pseudo dice [0.7477]
2024-12-18 09:03:09.589942: Epoch time: 357.86 s
2024-12-18 09:03:11.014771: 
2024-12-18 09:03:11.015970: Epoch 113
2024-12-18 09:03:11.016892: Current learning rate: 0.00284
2024-12-18 09:08:57.958301: Validation loss did not improve from -0.59353. Patience: 8/50
2024-12-18 09:08:57.961428: train_loss -0.7849
2024-12-18 09:08:57.962489: val_loss -0.5606
2024-12-18 09:08:57.963506: Pseudo dice [0.7647]
2024-12-18 09:08:57.964656: Epoch time: 346.95 s
2024-12-18 09:08:59.434724: 
2024-12-18 09:08:59.436757: Epoch 114
2024-12-18 09:08:59.437769: Current learning rate: 0.00277
2024-12-18 09:15:10.902911: Validation loss did not improve from -0.59353. Patience: 9/50
2024-12-18 09:15:10.904524: train_loss -0.7854
2024-12-18 09:15:10.905639: val_loss -0.5468
2024-12-18 09:15:10.906389: Pseudo dice [0.75]
2024-12-18 09:15:10.907265: Epoch time: 371.47 s
2024-12-18 09:15:13.752571: 
2024-12-18 09:15:13.754046: Epoch 115
2024-12-18 09:15:13.754857: Current learning rate: 0.0027
2024-12-18 09:21:17.156305: Validation loss did not improve from -0.59353. Patience: 10/50
2024-12-18 09:21:17.157392: train_loss -0.7872
2024-12-18 09:21:17.158316: val_loss -0.5369
2024-12-18 09:21:17.159128: Pseudo dice [0.7508]
2024-12-18 09:21:17.159856: Epoch time: 363.41 s
2024-12-18 09:21:18.732090: 
2024-12-18 09:21:18.733418: Epoch 116
2024-12-18 09:21:18.734224: Current learning rate: 0.00263
2024-12-18 09:27:35.307570: Validation loss did not improve from -0.59353. Patience: 11/50
2024-12-18 09:27:35.308719: train_loss -0.785
2024-12-18 09:27:35.309482: val_loss -0.5773
2024-12-18 09:27:35.310141: Pseudo dice [0.7652]
2024-12-18 09:27:35.311005: Epoch time: 376.58 s
2024-12-18 09:27:36.719355: 
2024-12-18 09:27:36.720885: Epoch 117
2024-12-18 09:27:36.721891: Current learning rate: 0.00256
2024-12-18 09:33:54.752115: Validation loss did not improve from -0.59353. Patience: 12/50
2024-12-18 09:33:54.753189: train_loss -0.7875
2024-12-18 09:33:54.754027: val_loss -0.5885
2024-12-18 09:33:54.754706: Pseudo dice [0.7676]
2024-12-18 09:33:54.755460: Epoch time: 378.04 s
2024-12-18 09:33:54.756227: Yayy! New best EMA pseudo Dice: 0.7568
2024-12-18 09:33:56.569125: 
2024-12-18 09:33:56.570530: Epoch 118
2024-12-18 09:33:56.571455: Current learning rate: 0.00249
2024-12-18 09:40:14.942390: Validation loss did not improve from -0.59353. Patience: 13/50
2024-12-18 09:40:14.943286: train_loss -0.7861
2024-12-18 09:40:14.944027: val_loss -0.556
2024-12-18 09:40:14.944789: Pseudo dice [0.757]
2024-12-18 09:40:14.945615: Epoch time: 378.38 s
2024-12-18 09:40:14.946350: Yayy! New best EMA pseudo Dice: 0.7568
2024-12-18 09:40:16.698779: 
2024-12-18 09:40:16.700153: Epoch 119
2024-12-18 09:40:16.700904: Current learning rate: 0.00242
2024-12-18 09:47:19.810938: Validation loss did not improve from -0.59353. Patience: 14/50
2024-12-18 09:47:19.812013: train_loss -0.7841
2024-12-18 09:47:19.813027: val_loss -0.5311
2024-12-18 09:47:19.813826: Pseudo dice [0.7413]
2024-12-18 09:47:19.814626: Epoch time: 423.11 s
2024-12-18 09:47:21.575763: 
2024-12-18 09:47:21.577036: Epoch 120
2024-12-18 09:47:21.577820: Current learning rate: 0.00235
2024-12-18 09:54:31.648686: Validation loss did not improve from -0.59353. Patience: 15/50
2024-12-18 09:54:31.649599: train_loss -0.7859
2024-12-18 09:54:31.650415: val_loss -0.5815
2024-12-18 09:54:31.651105: Pseudo dice [0.7664]
2024-12-18 09:54:31.651806: Epoch time: 430.08 s
2024-12-18 09:54:33.044100: 
2024-12-18 09:54:33.045678: Epoch 121
2024-12-18 09:54:33.046595: Current learning rate: 0.00228
2024-12-18 10:01:10.148412: Validation loss did not improve from -0.59353. Patience: 16/50
2024-12-18 10:01:10.149617: train_loss -0.7893
2024-12-18 10:01:10.150629: val_loss -0.557
2024-12-18 10:01:10.151438: Pseudo dice [0.7595]
2024-12-18 10:01:10.152271: Epoch time: 397.11 s
2024-12-18 10:01:11.604235: 
2024-12-18 10:01:11.605618: Epoch 122
2024-12-18 10:01:11.606682: Current learning rate: 0.00221
2024-12-18 10:07:30.640431: Validation loss did not improve from -0.59353. Patience: 17/50
2024-12-18 10:07:30.641436: train_loss -0.7899
2024-12-18 10:07:30.642170: val_loss -0.5686
2024-12-18 10:07:30.642762: Pseudo dice [0.7657]
2024-12-18 10:07:30.643391: Epoch time: 379.04 s
2024-12-18 10:07:30.644232: Yayy! New best EMA pseudo Dice: 0.7576
2024-12-18 10:07:32.530581: 
2024-12-18 10:07:32.532511: Epoch 123
2024-12-18 10:07:32.533399: Current learning rate: 0.00214
2024-12-18 10:13:35.458106: Validation loss did not improve from -0.59353. Patience: 18/50
2024-12-18 10:13:35.459091: train_loss -0.7876
2024-12-18 10:13:35.459914: val_loss -0.5617
2024-12-18 10:13:35.460739: Pseudo dice [0.7585]
2024-12-18 10:13:35.461609: Epoch time: 362.93 s
2024-12-18 10:13:35.462295: Yayy! New best EMA pseudo Dice: 0.7577
2024-12-18 10:13:37.287609: 
2024-12-18 10:13:37.289169: Epoch 124
2024-12-18 10:13:37.290025: Current learning rate: 0.00207
2024-12-18 10:19:57.691913: Validation loss did not improve from -0.59353. Patience: 19/50
2024-12-18 10:19:57.692884: train_loss -0.7898
2024-12-18 10:19:57.693850: val_loss -0.5315
2024-12-18 10:19:57.694589: Pseudo dice [0.7501]
2024-12-18 10:19:57.695240: Epoch time: 380.41 s
2024-12-18 10:19:59.684855: 
2024-12-18 10:19:59.687418: Epoch 125
2024-12-18 10:19:59.688986: Current learning rate: 0.00199
2024-12-18 10:26:03.917099: Validation loss did not improve from -0.59353. Patience: 20/50
2024-12-18 10:26:03.918024: train_loss -0.7917
2024-12-18 10:26:03.919003: val_loss -0.5468
2024-12-18 10:26:03.919952: Pseudo dice [0.7559]
2024-12-18 10:26:03.921001: Epoch time: 364.24 s
2024-12-18 10:26:05.825506: 
2024-12-18 10:26:05.827371: Epoch 126
2024-12-18 10:26:05.828817: Current learning rate: 0.00192
2024-12-18 10:32:15.398814: Validation loss did not improve from -0.59353. Patience: 21/50
2024-12-18 10:32:15.399642: train_loss -0.7902
2024-12-18 10:32:15.400556: val_loss -0.5517
2024-12-18 10:32:15.401348: Pseudo dice [0.7556]
2024-12-18 10:32:15.402243: Epoch time: 369.58 s
2024-12-18 10:32:16.836892: 
2024-12-18 10:32:16.838120: Epoch 127
2024-12-18 10:32:16.838854: Current learning rate: 0.00185
2024-12-18 10:38:29.266712: Validation loss did not improve from -0.59353. Patience: 22/50
2024-12-18 10:38:29.267800: train_loss -0.7879
2024-12-18 10:38:29.269158: val_loss -0.5806
2024-12-18 10:38:29.270334: Pseudo dice [0.7631]
2024-12-18 10:38:29.271320: Epoch time: 372.43 s
2024-12-18 10:38:30.695412: 
2024-12-18 10:38:30.696999: Epoch 128
2024-12-18 10:38:30.698170: Current learning rate: 0.00178
2024-12-18 10:44:47.789747: Validation loss did not improve from -0.59353. Patience: 23/50
2024-12-18 10:44:47.793182: train_loss -0.7918
2024-12-18 10:44:47.795940: val_loss -0.5435
2024-12-18 10:44:47.797133: Pseudo dice [0.7538]
2024-12-18 10:44:47.798451: Epoch time: 377.1 s
2024-12-18 10:44:49.387511: 
2024-12-18 10:44:49.389191: Epoch 129
2024-12-18 10:44:49.390265: Current learning rate: 0.0017
2024-12-18 10:51:01.591300: Validation loss did not improve from -0.59353. Patience: 24/50
2024-12-18 10:51:01.592392: train_loss -0.7931
2024-12-18 10:51:01.593760: val_loss -0.5339
2024-12-18 10:51:01.594808: Pseudo dice [0.7436]
2024-12-18 10:51:01.595902: Epoch time: 372.21 s
2024-12-18 10:51:03.318023: 
2024-12-18 10:51:03.319365: Epoch 130
2024-12-18 10:51:03.320216: Current learning rate: 0.00163
2024-12-18 10:57:14.277564: Validation loss did not improve from -0.59353. Patience: 25/50
2024-12-18 10:57:14.278820: train_loss -0.7942
2024-12-18 10:57:14.280036: val_loss -0.5654
2024-12-18 10:57:14.281247: Pseudo dice [0.7564]
2024-12-18 10:57:14.282347: Epoch time: 370.96 s
2024-12-18 10:57:15.666860: 
2024-12-18 10:57:15.667969: Epoch 131
2024-12-18 10:57:15.668951: Current learning rate: 0.00156
2024-12-18 11:03:18.001026: Validation loss did not improve from -0.59353. Patience: 26/50
2024-12-18 11:03:18.001840: train_loss -0.795
2024-12-18 11:03:18.002716: val_loss -0.5715
2024-12-18 11:03:18.003501: Pseudo dice [0.7608]
2024-12-18 11:03:18.004744: Epoch time: 362.34 s
2024-12-18 11:03:19.517880: 
2024-12-18 11:03:19.519344: Epoch 132
2024-12-18 11:03:19.520182: Current learning rate: 0.00148
2024-12-18 11:09:32.508702: Validation loss did not improve from -0.59353. Patience: 27/50
2024-12-18 11:09:32.510051: train_loss -0.7938
2024-12-18 11:09:32.511254: val_loss -0.5405
2024-12-18 11:09:32.512564: Pseudo dice [0.7526]
2024-12-18 11:09:32.513773: Epoch time: 372.99 s
2024-12-18 11:09:33.932065: 
2024-12-18 11:09:33.933584: Epoch 133
2024-12-18 11:09:33.934667: Current learning rate: 0.00141
2024-12-18 11:16:04.061094: Validation loss did not improve from -0.59353. Patience: 28/50
2024-12-18 11:16:04.062275: train_loss -0.795
2024-12-18 11:16:04.063292: val_loss -0.5586
2024-12-18 11:16:04.064289: Pseudo dice [0.7542]
2024-12-18 11:16:04.065294: Epoch time: 390.13 s
2024-12-18 11:16:05.523847: 
2024-12-18 11:16:05.525302: Epoch 134
2024-12-18 11:16:05.527197: Current learning rate: 0.00133
2024-12-18 11:22:30.757980: Validation loss did not improve from -0.59353. Patience: 29/50
2024-12-18 11:22:30.758883: train_loss -0.795
2024-12-18 11:22:30.759778: val_loss -0.5774
2024-12-18 11:22:30.760632: Pseudo dice [0.7668]
2024-12-18 11:22:30.761375: Epoch time: 385.24 s
2024-12-18 11:22:32.586039: 
2024-12-18 11:22:32.587697: Epoch 135
2024-12-18 11:22:32.588604: Current learning rate: 0.00126
2024-12-18 11:29:10.889468: Validation loss did not improve from -0.59353. Patience: 30/50
2024-12-18 11:29:10.890608: train_loss -0.7962
2024-12-18 11:29:10.891753: val_loss -0.5711
2024-12-18 11:29:10.892705: Pseudo dice [0.7648]
2024-12-18 11:29:10.893546: Epoch time: 398.31 s
2024-12-18 11:29:12.287256: 
2024-12-18 11:29:12.288686: Epoch 136
2024-12-18 11:29:12.289738: Current learning rate: 0.00118
2024-12-18 11:36:09.757774: Validation loss did not improve from -0.59353. Patience: 31/50
2024-12-18 11:36:09.760023: train_loss -0.7942
2024-12-18 11:36:09.761303: val_loss -0.5525
2024-12-18 11:36:09.762237: Pseudo dice [0.7583]
2024-12-18 11:36:09.763208: Epoch time: 417.47 s
2024-12-18 11:36:12.025387: 
2024-12-18 11:36:12.026839: Epoch 137
2024-12-18 11:36:12.027761: Current learning rate: 0.00111
2024-12-18 11:42:16.083063: Validation loss did not improve from -0.59353. Patience: 32/50
2024-12-18 11:42:16.084005: train_loss -0.795
2024-12-18 11:42:16.084742: val_loss -0.5575
2024-12-18 11:42:16.085462: Pseudo dice [0.7619]
2024-12-18 11:42:16.086132: Epoch time: 364.06 s
2024-12-18 11:42:16.086918: Yayy! New best EMA pseudo Dice: 0.7581
2024-12-18 11:42:17.910605: 
2024-12-18 11:42:17.912184: Epoch 138
2024-12-18 11:42:17.913248: Current learning rate: 0.00103
2024-12-18 11:48:35.856766: Validation loss did not improve from -0.59353. Patience: 33/50
2024-12-18 11:48:35.861597: train_loss -0.7975
2024-12-18 11:48:35.863589: val_loss -0.5794
2024-12-18 11:48:35.864806: Pseudo dice [0.7719]
2024-12-18 11:48:35.865690: Epoch time: 377.95 s
2024-12-18 11:48:35.867119: Yayy! New best EMA pseudo Dice: 0.7595
2024-12-18 11:48:37.700575: 
2024-12-18 11:48:37.702393: Epoch 139
2024-12-18 11:48:37.703296: Current learning rate: 0.00095
2024-12-18 11:55:00.684079: Validation loss did not improve from -0.59353. Patience: 34/50
2024-12-18 11:55:00.685447: train_loss -0.7982
2024-12-18 11:55:00.686186: val_loss -0.5535
2024-12-18 11:55:00.687025: Pseudo dice [0.7553]
2024-12-18 11:55:00.688150: Epoch time: 382.99 s
2024-12-18 11:55:02.400217: 
2024-12-18 11:55:02.401850: Epoch 140
2024-12-18 11:55:02.403060: Current learning rate: 0.00087
2024-12-18 12:01:24.309384: Validation loss did not improve from -0.59353. Patience: 35/50
2024-12-18 12:01:24.310759: train_loss -0.7965
2024-12-18 12:01:24.311689: val_loss -0.544
2024-12-18 12:01:24.312724: Pseudo dice [0.7525]
2024-12-18 12:01:24.313792: Epoch time: 381.91 s
2024-12-18 12:01:25.750814: 
2024-12-18 12:01:25.752009: Epoch 141
2024-12-18 12:01:25.752752: Current learning rate: 0.00079
2024-12-18 12:07:44.547261: Validation loss did not improve from -0.59353. Patience: 36/50
2024-12-18 12:07:44.548245: train_loss -0.8008
2024-12-18 12:07:44.549569: val_loss -0.5327
2024-12-18 12:07:44.550671: Pseudo dice [0.7468]
2024-12-18 12:07:44.551747: Epoch time: 378.8 s
2024-12-18 12:07:45.993416: 
2024-12-18 12:07:45.995216: Epoch 142
2024-12-18 12:07:45.996472: Current learning rate: 0.00071
2024-12-18 12:14:23.970433: Validation loss did not improve from -0.59353. Patience: 37/50
2024-12-18 12:14:23.971301: train_loss -0.7991
2024-12-18 12:14:23.972192: val_loss -0.5362
2024-12-18 12:14:23.972875: Pseudo dice [0.7522]
2024-12-18 12:14:23.973673: Epoch time: 397.98 s
2024-12-18 12:14:25.397258: 
2024-12-18 12:14:25.398240: Epoch 143
2024-12-18 12:14:25.399032: Current learning rate: 0.00063
2024-12-18 12:21:00.278990: Validation loss did not improve from -0.59353. Patience: 38/50
2024-12-18 12:21:00.279966: train_loss -0.7995
2024-12-18 12:21:00.281385: val_loss -0.5432
2024-12-18 12:21:00.282279: Pseudo dice [0.7494]
2024-12-18 12:21:00.283069: Epoch time: 394.88 s
2024-12-18 12:21:01.705344: 
2024-12-18 12:21:01.706824: Epoch 144
2024-12-18 12:21:01.707775: Current learning rate: 0.00055
2024-12-18 12:27:36.480745: Validation loss did not improve from -0.59353. Patience: 39/50
2024-12-18 12:27:36.481956: train_loss -0.8014
2024-12-18 12:27:36.483074: val_loss -0.5364
2024-12-18 12:27:36.483882: Pseudo dice [0.7521]
2024-12-18 12:27:36.484644: Epoch time: 394.78 s
2024-12-18 12:27:38.390875: 
2024-12-18 12:27:38.392993: Epoch 145
2024-12-18 12:27:38.394276: Current learning rate: 0.00047
2024-12-18 12:34:11.038779: Validation loss did not improve from -0.59353. Patience: 40/50
2024-12-18 12:34:11.039912: train_loss -0.8027
2024-12-18 12:34:11.041004: val_loss -0.5867
2024-12-18 12:34:11.041875: Pseudo dice [0.7751]
2024-12-18 12:34:11.042733: Epoch time: 392.65 s
2024-12-18 12:34:12.489075: 
2024-12-18 12:34:12.490024: Epoch 146
2024-12-18 12:34:12.490747: Current learning rate: 0.00038
2024-12-18 12:40:57.748950: Validation loss did not improve from -0.59353. Patience: 41/50
2024-12-18 12:40:57.749708: train_loss -0.797
2024-12-18 12:40:57.750737: val_loss -0.5472
2024-12-18 12:40:57.751719: Pseudo dice [0.7515]
2024-12-18 12:40:57.752843: Epoch time: 405.26 s
2024-12-18 12:41:00.064264: 
2024-12-18 12:41:00.065438: Epoch 147
2024-12-18 12:41:00.066232: Current learning rate: 0.0003
2024-12-18 12:47:26.938056: Validation loss did not improve from -0.59353. Patience: 42/50
2024-12-18 12:47:26.939942: train_loss -0.7986
2024-12-18 12:47:26.941254: val_loss -0.5595
2024-12-18 12:47:26.942209: Pseudo dice [0.7586]
2024-12-18 12:47:26.943287: Epoch time: 386.88 s
2024-12-18 12:47:28.391538: 
2024-12-18 12:47:28.393142: Epoch 148
2024-12-18 12:47:28.394093: Current learning rate: 0.00021
2024-12-18 12:54:08.955548: Validation loss did not improve from -0.59353. Patience: 43/50
2024-12-18 12:54:08.959467: train_loss -0.8
2024-12-18 12:54:08.961740: val_loss -0.565
2024-12-18 12:54:08.962537: Pseudo dice [0.7643]
2024-12-18 12:54:08.963529: Epoch time: 400.57 s
2024-12-18 12:54:10.408984: 
2024-12-18 12:54:10.411147: Epoch 149
2024-12-18 12:54:10.412071: Current learning rate: 0.00011
2024-12-18 13:00:35.111101: Validation loss did not improve from -0.59353. Patience: 44/50
2024-12-18 13:00:35.112296: train_loss -0.8014
2024-12-18 13:00:35.113701: val_loss -0.5547
2024-12-18 13:00:35.114923: Pseudo dice [0.758]
2024-12-18 13:00:35.116399: Epoch time: 384.71 s
2024-12-18 13:00:36.961723: Training done.
2024-12-18 13:00:37.119910: Using splits from existing split file: /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset309_Sohee_Calcium_OCT_CrossValidation/splits_final_80.json
2024-12-18 13:00:37.123285: The split file contains 5 splits.
2024-12-18 13:00:37.124310: Desired fold for training: 2
2024-12-18 13:00:37.125267: This split has 6 training and 3 validation cases.
2024-12-18 13:00:37.126729: predicting 101-045
2024-12-18 13:00:37.157315: 101-045, shape torch.Size([1, 375, 498, 498]), rank 0
2024-12-18 13:03:11.102890: predicting 401-004
2024-12-18 13:03:11.121287: 401-004, shape torch.Size([1, 375, 498, 498]), rank 0
2024-12-18 13:06:03.438599: predicting 704-003
2024-12-18 13:06:03.453238: 704-003, shape torch.Size([1, 375, 498, 498]), rank 0
2024-12-18 13:08:26.705498: Validation complete
2024-12-18 13:08:26.706023: Mean Validation Dice:  0.7421886333328572

############################
INFO: You are using the old nnU-Net default plans. We have updated our recommendations. Please consider using those instead! Read more here: https://github.com/MIC-DKFZ/nnUNet/blob/master/documentation/resenc_presets.md
############################

Using device: cuda:0

#######################################################################
Please cite the following paper when using nnU-Net:
Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.
#######################################################################

2024-12-18 13:08:39.098694: Using torch.compile...
/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
################### Loading pretrained weights from file  /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_results/Dataset300_Lumen_and_Wall_OCT/nnUNetTrainer__nnUNetPreprocessPlans__3d_fullres/fold_all/checkpoint_best.pth ###################
Below is the list of overlapping blocks in pretrained model and nnUNet architecture:
encoder.stages.0.0.convs.0.conv.weight shape torch.Size([32, 1, 3, 3, 3])
encoder.stages.0.0.convs.0.conv.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.norm.weight shape torch.Size([32])
encoder.stages.0.0.convs.0.norm.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.0.weight shape torch.Size([32, 1, 3, 3, 3])
encoder.stages.0.0.convs.0.all_modules.0.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.1.weight shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.1.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
encoder.stages.0.0.convs.1.conv.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.norm.weight shape torch.Size([32])
encoder.stages.0.0.convs.1.norm.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
encoder.stages.0.0.convs.1.all_modules.0.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.1.weight shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.1.bias shape torch.Size([32])
encoder.stages.1.0.convs.0.conv.weight shape torch.Size([64, 32, 3, 3, 3])
encoder.stages.1.0.convs.0.conv.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.norm.weight shape torch.Size([64])
encoder.stages.1.0.convs.0.norm.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.0.weight shape torch.Size([64, 32, 3, 3, 3])
encoder.stages.1.0.convs.0.all_modules.0.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.1.weight shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.1.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
encoder.stages.1.0.convs.1.conv.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.norm.weight shape torch.Size([64])
encoder.stages.1.0.convs.1.norm.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
encoder.stages.1.0.convs.1.all_modules.0.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.1.weight shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.1.bias shape torch.Size([64])
encoder.stages.2.0.convs.0.conv.weight shape torch.Size([128, 64, 3, 3, 3])
encoder.stages.2.0.convs.0.conv.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.norm.weight shape torch.Size([128])
encoder.stages.2.0.convs.0.norm.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.0.weight shape torch.Size([128, 64, 3, 3, 3])
encoder.stages.2.0.convs.0.all_modules.0.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.1.weight shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.1.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
encoder.stages.2.0.convs.1.conv.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.norm.weight shape torch.Size([128])
encoder.stages.2.0.convs.1.norm.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
encoder.stages.2.0.convs.1.all_modules.0.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.1.weight shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.1.bias shape torch.Size([128])
encoder.stages.3.0.convs.0.conv.weight shape torch.Size([256, 128, 3, 3, 3])
encoder.stages.3.0.convs.0.conv.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.norm.weight shape torch.Size([256])
encoder.stages.3.0.convs.0.norm.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.0.weight shape torch.Size([256, 128, 3, 3, 3])
encoder.stages.3.0.convs.0.all_modules.0.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.1.weight shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.1.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
encoder.stages.3.0.convs.1.conv.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.norm.weight shape torch.Size([256])
encoder.stages.3.0.convs.1.norm.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
encoder.stages.3.0.convs.1.all_modules.0.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.1.weight shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.1.bias shape torch.Size([256])
encoder.stages.4.0.convs.0.conv.weight shape torch.Size([320, 256, 3, 3, 3])
encoder.stages.4.0.convs.0.conv.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.norm.weight shape torch.Size([320])
encoder.stages.4.0.convs.0.norm.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.0.weight shape torch.Size([320, 256, 3, 3, 3])
encoder.stages.4.0.convs.0.all_modules.0.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.1.weight shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.1.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.4.0.convs.1.conv.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.norm.weight shape torch.Size([320])
encoder.stages.4.0.convs.1.norm.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.4.0.convs.1.all_modules.0.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.1.weight shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.1.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.0.conv.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.norm.weight shape torch.Size([320])
encoder.stages.5.0.convs.0.norm.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.0.all_modules.0.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.1.weight shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.1.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.1.conv.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.norm.weight shape torch.Size([320])
encoder.stages.5.0.convs.1.norm.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.1.all_modules.0.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.1.weight shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.0.0.convs.0.conv.weight shape torch.Size([32, 1, 3, 3, 3])
decoder.encoder.stages.0.0.convs.0.conv.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.norm.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.norm.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.0.weight shape torch.Size([32, 1, 3, 3, 3])
decoder.encoder.stages.0.0.convs.0.all_modules.0.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.1.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.1.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.encoder.stages.0.0.convs.1.conv.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.norm.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.norm.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.encoder.stages.0.0.convs.1.all_modules.0.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.1.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.1.bias shape torch.Size([32])
decoder.encoder.stages.1.0.convs.0.conv.weight shape torch.Size([64, 32, 3, 3, 3])
decoder.encoder.stages.1.0.convs.0.conv.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.norm.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.norm.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.0.weight shape torch.Size([64, 32, 3, 3, 3])
decoder.encoder.stages.1.0.convs.0.all_modules.0.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.1.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.1.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.encoder.stages.1.0.convs.1.conv.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.norm.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.norm.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.encoder.stages.1.0.convs.1.all_modules.0.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.1.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.1.bias shape torch.Size([64])
decoder.encoder.stages.2.0.convs.0.conv.weight shape torch.Size([128, 64, 3, 3, 3])
decoder.encoder.stages.2.0.convs.0.conv.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.norm.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.norm.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.0.weight shape torch.Size([128, 64, 3, 3, 3])
decoder.encoder.stages.2.0.convs.0.all_modules.0.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.1.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.1.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.encoder.stages.2.0.convs.1.conv.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.norm.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.norm.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.encoder.stages.2.0.convs.1.all_modules.0.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.1.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.1.bias shape torch.Size([128])
decoder.encoder.stages.3.0.convs.0.conv.weight shape torch.Size([256, 128, 3, 3, 3])
decoder.encoder.stages.3.0.convs.0.conv.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.norm.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.norm.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.0.weight shape torch.Size([256, 128, 3, 3, 3])
decoder.encoder.stages.3.0.convs.0.all_modules.0.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.1.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.1.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.encoder.stages.3.0.convs.1.conv.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.norm.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.norm.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.encoder.stages.3.0.convs.1.all_modules.0.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.1.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.1.bias shape torch.Size([256])
decoder.encoder.stages.4.0.convs.0.conv.weight shape torch.Size([320, 256, 3, 3, 3])
decoder.encoder.stages.4.0.convs.0.conv.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.norm.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.norm.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.0.weight shape torch.Size([320, 256, 3, 3, 3])
decoder.encoder.stages.4.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.4.0.convs.1.conv.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.norm.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.norm.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.4.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.0.conv.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.norm.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.norm.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.1.conv.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.norm.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.norm.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.stages.0.convs.0.conv.weight shape torch.Size([320, 640, 3, 3, 3])
decoder.stages.0.convs.0.conv.bias shape torch.Size([320])
decoder.stages.0.convs.0.norm.weight shape torch.Size([320])
decoder.stages.0.convs.0.norm.bias shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.0.weight shape torch.Size([320, 640, 3, 3, 3])
decoder.stages.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.stages.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.stages.0.convs.1.conv.bias shape torch.Size([320])
decoder.stages.0.convs.1.norm.weight shape torch.Size([320])
decoder.stages.0.convs.1.norm.bias shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.stages.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.stages.1.convs.0.conv.weight shape torch.Size([256, 512, 3, 3, 3])
decoder.stages.1.convs.0.conv.bias shape torch.Size([256])
decoder.stages.1.convs.0.norm.weight shape torch.Size([256])
decoder.stages.1.convs.0.norm.bias shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.0.weight shape torch.Size([256, 512, 3, 3, 3])
decoder.stages.1.convs.0.all_modules.0.bias shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.1.weight shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.1.bias shape torch.Size([256])
decoder.stages.1.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.stages.1.convs.1.conv.bias shape torch.Size([256])
decoder.stages.1.convs.1.norm.weight shape torch.Size([256])
decoder.stages.1.convs.1.norm.bias shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.stages.1.convs.1.all_modules.0.bias shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.1.weight shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.1.bias shape torch.Size([256])
decoder.stages.2.convs.0.conv.weight shape torch.Size([128, 256, 3, 3, 3])
decoder.stages.2.convs.0.conv.bias shape torch.Size([128])
decoder.stages.2.convs.0.norm.weight shape torch.Size([128])
decoder.stages.2.convs.0.norm.bias shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.0.weight shape torch.Size([128, 256, 3, 3, 3])
decoder.stages.2.convs.0.all_modules.0.bias shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.1.weight shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.1.bias shape torch.Size([128])
decoder.stages.2.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.stages.2.convs.1.conv.bias shape torch.Size([128])
decoder.stages.2.convs.1.norm.weight shape torch.Size([128])
decoder.stages.2.convs.1.norm.bias shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.stages.2.convs.1.all_modules.0.bias shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.1.weight shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.1.bias shape torch.Size([128])
decoder.stages.3.convs.0.conv.weight shape torch.Size([64, 128, 3, 3, 3])
decoder.stages.3.convs.0.conv.bias shape torch.Size([64])
decoder.stages.3.convs.0.norm.weight shape torch.Size([64])
decoder.stages.3.convs.0.norm.bias shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.0.weight shape torch.Size([64, 128, 3, 3, 3])
decoder.stages.3.convs.0.all_modules.0.bias shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.1.weight shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.1.bias shape torch.Size([64])
decoder.stages.3.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.stages.3.convs.1.conv.bias shape torch.Size([64])
decoder.stages.3.convs.1.norm.weight shape torch.Size([64])
decoder.stages.3.convs.1.norm.bias shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.stages.3.convs.1.all_modules.0.bias shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.1.weight shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.1.bias shape torch.Size([64])
decoder.stages.4.convs.0.conv.weight shape torch.Size([32, 64, 3, 3, 3])
decoder.stages.4.convs.0.conv.bias shape torch.Size([32])
decoder.stages.4.convs.0.norm.weight shape torch.Size([32])
decoder.stages.4.convs.0.norm.bias shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.0.weight shape torch.Size([32, 64, 3, 3, 3])
decoder.stages.4.convs.0.all_modules.0.bias shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.1.weight shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.1.bias shape torch.Size([32])
decoder.stages.4.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.stages.4.convs.1.conv.bias shape torch.Size([32])
decoder.stages.4.convs.1.norm.weight shape torch.Size([32])
decoder.stages.4.convs.1.norm.bias shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.stages.4.convs.1.all_modules.0.bias shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.1.weight shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.1.bias shape torch.Size([32])
decoder.transpconvs.0.weight shape torch.Size([320, 320, 1, 2, 2])
decoder.transpconvs.0.bias shape torch.Size([320])
decoder.transpconvs.1.weight shape torch.Size([320, 256, 2, 2, 2])
decoder.transpconvs.1.bias shape torch.Size([256])
decoder.transpconvs.2.weight shape torch.Size([256, 128, 2, 2, 2])
decoder.transpconvs.2.bias shape torch.Size([128])
decoder.transpconvs.3.weight shape torch.Size([128, 64, 2, 2, 2])
decoder.transpconvs.3.bias shape torch.Size([64])
decoder.transpconvs.4.weight shape torch.Size([64, 32, 2, 2, 2])
decoder.transpconvs.4.bias shape torch.Size([32])
################### Done ###################
2024-12-18 13:08:59.862682: do_dummy_2d_data_aug: True
2024-12-18 13:08:59.864702: Using splits from existing split file: /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset309_Sohee_Calcium_OCT_CrossValidation/splits_final_80.json
2024-12-18 13:08:59.867351: The split file contains 5 splits.
2024-12-18 13:08:59.868653: Desired fold for training: 4
2024-12-18 13:08:59.869761: This split has 6 training and 5 validation cases.
using pin_memory on device 0
using pin_memory on device 0

This is the configuration used by this training:
Configuration name: 3d_32x160x128_b10
 {'data_identifier': 'nnUNetPlans_3d_fullres', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 10, 'patch_size': [32, 160, 128], 'median_image_size_in_voxels': [375.0, 498.0, 498.0], 'spacing': [1.0, 1.0, 1.0], 'normalization_schemes': ['ZScoreNormalization'], 'use_mask_for_norm': [False], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'architecture': {'network_class_name': 'dynamic_network_architectures.architectures.unet.PlainConvUNet', 'arch_kwargs': {'n_stages': 6, 'features_per_stage': [32, 64, 128, 256, 320, 320], 'conv_op': 'torch.nn.modules.conv.Conv3d', 'kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]], 'strides': [[1, 1, 1], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [1, 2, 2]], 'n_conv_per_stage': [2, 2, 2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2, 2, 2], 'conv_bias': True, 'norm_op': 'torch.nn.modules.instancenorm.InstanceNorm3d', 'norm_op_kwargs': {'eps': 1e-05, 'affine': True}, 'dropout_op': None, 'dropout_op_kwargs': None, 'nonlin': 'torch.nn.LeakyReLU', 'nonlin_kwargs': {'inplace': True}, 'deep_supervision': True}, '_kw_requires_import': ['conv_op', 'norm_op', 'dropout_op', 'nonlin']}, 'batch_dice': True, 'inherits_from': '3d_fullres'} 

These are the global plan.json settings:
 {'dataset_name': 'Dataset309_Sohee_Calcium_OCT_CrossValidation', 'plans_name': 'nnUNetPlans', 'original_median_spacing_after_transp': [1.0, 1.0, 1.0], 'original_median_shape_after_transp': [375, 498, 498], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 0.4977976381778717, 'mean': 0.13507525622844696, 'median': 0.09599608182907104, 'min': 0.0, 'percentile_00_5': 0.014754901640117168, 'percentile_99_5': 0.4977976381778717, 'std': 0.12025152146816254}}} 

2024-12-18 13:09:27.765743: unpacking dataset...
2024-12-18 13:09:32.036760: unpacking done...
2024-12-18 13:09:32.242774: Unable to plot network architecture: nnUNet_compile is enabled!
2024-12-18 13:09:32.429904: 
2024-12-18 13:09:32.431026: Epoch 0
2024-12-18 13:09:32.431862: Current learning rate: 0.01
2024-12-18 13:15:13.058490: Validation loss improved from 1000.00000 to -0.43019! Patience: 0/50
2024-12-18 13:15:13.059645: train_loss -0.3167
2024-12-18 13:15:13.060853: val_loss -0.4302
2024-12-18 13:15:13.061966: Pseudo dice [0.679]
2024-12-18 13:15:13.063051: Epoch time: 340.63 s
2024-12-18 13:15:13.063875: Yayy! New best EMA pseudo Dice: 0.679
2024-12-18 13:15:15.055946: 
2024-12-18 13:15:15.056993: Epoch 1
2024-12-18 13:15:15.057844: Current learning rate: 0.00994
2024-12-18 13:20:07.742587: Validation loss improved from -0.43019 to -0.46873! Patience: 0/50
2024-12-18 13:20:07.743469: train_loss -0.4808
2024-12-18 13:20:07.744406: val_loss -0.4687
2024-12-18 13:20:07.745352: Pseudo dice [0.6989]
2024-12-18 13:20:07.746475: Epoch time: 292.69 s
2024-12-18 13:20:07.747442: Yayy! New best EMA pseudo Dice: 0.681
2024-12-18 13:20:09.448762: 
2024-12-18 13:20:09.450016: Epoch 2
2024-12-18 13:20:09.451064: Current learning rate: 0.00988
2024-12-18 13:25:08.230227: Validation loss improved from -0.46873 to -0.48066! Patience: 0/50
2024-12-18 13:25:08.231251: train_loss -0.52
2024-12-18 13:25:08.231985: val_loss -0.4807
2024-12-18 13:25:08.232632: Pseudo dice [0.7084]
2024-12-18 13:25:08.233367: Epoch time: 298.78 s
2024-12-18 13:25:08.233946: Yayy! New best EMA pseudo Dice: 0.6837
2024-12-18 13:25:10.011649: 
2024-12-18 13:25:10.012890: Epoch 3
2024-12-18 13:25:10.013571: Current learning rate: 0.00982
2024-12-18 13:29:30.347283: Validation loss did not improve from -0.48066. Patience: 1/50
2024-12-18 13:29:30.349732: train_loss -0.5476
2024-12-18 13:29:30.351070: val_loss -0.4651
2024-12-18 13:29:30.351841: Pseudo dice [0.6951]
2024-12-18 13:29:30.352613: Epoch time: 260.34 s
2024-12-18 13:29:30.353381: Yayy! New best EMA pseudo Dice: 0.6849
2024-12-18 13:29:31.991258: 
2024-12-18 13:29:31.992903: Epoch 4
2024-12-18 13:29:31.993802: Current learning rate: 0.00976
2024-12-18 13:34:21.160042: Validation loss did not improve from -0.48066. Patience: 2/50
2024-12-18 13:34:21.161006: train_loss -0.5759
2024-12-18 13:34:21.161740: val_loss -0.48
2024-12-18 13:34:21.162642: Pseudo dice [0.7109]
2024-12-18 13:34:21.163351: Epoch time: 289.17 s
2024-12-18 13:34:21.492172: Yayy! New best EMA pseudo Dice: 0.6875
2024-12-18 13:34:23.270895: 
2024-12-18 13:34:23.271968: Epoch 5
2024-12-18 13:34:23.272766: Current learning rate: 0.0097
2024-12-18 13:39:11.313879: Validation loss improved from -0.48066 to -0.49832! Patience: 2/50
2024-12-18 13:39:11.315111: train_loss -0.584
2024-12-18 13:39:11.315907: val_loss -0.4983
2024-12-18 13:39:11.316705: Pseudo dice [0.7049]
2024-12-18 13:39:11.317479: Epoch time: 288.05 s
2024-12-18 13:39:11.318101: Yayy! New best EMA pseudo Dice: 0.6892
2024-12-18 13:39:12.993228: 
2024-12-18 13:39:12.994717: Epoch 6
2024-12-18 13:39:12.995596: Current learning rate: 0.00964
2024-12-18 13:43:49.660184: Validation loss improved from -0.49832 to -0.49882! Patience: 0/50
2024-12-18 13:43:49.661160: train_loss -0.6007
2024-12-18 13:43:49.662148: val_loss -0.4988
2024-12-18 13:43:49.663249: Pseudo dice [0.7192]
2024-12-18 13:43:49.663972: Epoch time: 276.67 s
2024-12-18 13:43:49.664604: Yayy! New best EMA pseudo Dice: 0.6922
2024-12-18 13:43:51.450294: 
2024-12-18 13:43:51.451876: Epoch 7
2024-12-18 13:43:51.452970: Current learning rate: 0.00958
2024-12-18 13:48:37.452904: Validation loss did not improve from -0.49882. Patience: 1/50
2024-12-18 13:48:37.453876: train_loss -0.6086
2024-12-18 13:48:37.454799: val_loss -0.4966
2024-12-18 13:48:37.455511: Pseudo dice [0.7169]
2024-12-18 13:48:37.456146: Epoch time: 286.0 s
2024-12-18 13:48:37.456844: Yayy! New best EMA pseudo Dice: 0.6947
2024-12-18 13:48:39.531869: 
2024-12-18 13:48:39.533571: Epoch 8
2024-12-18 13:48:39.534698: Current learning rate: 0.00952
2024-12-18 13:53:15.501412: Validation loss improved from -0.49882 to -0.49900! Patience: 1/50
2024-12-18 13:53:15.502422: train_loss -0.6134
2024-12-18 13:53:15.503736: val_loss -0.499
2024-12-18 13:53:15.504675: Pseudo dice [0.7137]
2024-12-18 13:53:15.505400: Epoch time: 275.97 s
2024-12-18 13:53:15.506152: Yayy! New best EMA pseudo Dice: 0.6966
2024-12-18 13:53:17.266483: 
2024-12-18 13:53:17.267798: Epoch 9
2024-12-18 13:53:17.269344: Current learning rate: 0.00946
2024-12-18 13:57:50.631147: Validation loss improved from -0.49900 to -0.50907! Patience: 0/50
2024-12-18 13:57:50.632196: train_loss -0.6109
2024-12-18 13:57:50.633604: val_loss -0.5091
2024-12-18 13:57:50.634371: Pseudo dice [0.7211]
2024-12-18 13:57:50.635138: Epoch time: 273.37 s
2024-12-18 13:57:51.027009: Yayy! New best EMA pseudo Dice: 0.699
2024-12-18 13:57:52.672710: 
2024-12-18 13:57:52.673976: Epoch 10
2024-12-18 13:57:52.674732: Current learning rate: 0.0094
2024-12-18 14:02:45.387726: Validation loss did not improve from -0.50907. Patience: 1/50
2024-12-18 14:02:45.388706: train_loss -0.6262
2024-12-18 14:02:45.389412: val_loss -0.4876
2024-12-18 14:02:45.390061: Pseudo dice [0.7126]
2024-12-18 14:02:45.390686: Epoch time: 292.72 s
2024-12-18 14:02:45.391582: Yayy! New best EMA pseudo Dice: 0.7004
2024-12-18 14:02:47.069710: 
2024-12-18 14:02:47.071005: Epoch 11
2024-12-18 14:02:47.071913: Current learning rate: 0.00934
2024-12-18 14:07:36.259199: Validation loss improved from -0.50907 to -0.52196! Patience: 1/50
2024-12-18 14:07:36.260226: train_loss -0.6342
2024-12-18 14:07:36.261055: val_loss -0.522
2024-12-18 14:07:36.261867: Pseudo dice [0.7344]
2024-12-18 14:07:36.262852: Epoch time: 289.19 s
2024-12-18 14:07:36.263604: Yayy! New best EMA pseudo Dice: 0.7038
2024-12-18 14:07:37.971102: 
2024-12-18 14:07:37.972608: Epoch 12
2024-12-18 14:07:37.973459: Current learning rate: 0.00928
2024-12-18 14:12:14.507319: Validation loss did not improve from -0.52196. Patience: 1/50
2024-12-18 14:12:14.508044: train_loss -0.6388
2024-12-18 14:12:14.509301: val_loss -0.4591
2024-12-18 14:12:14.510285: Pseudo dice [0.6891]
2024-12-18 14:12:14.511117: Epoch time: 276.54 s
2024-12-18 14:12:15.840954: 
2024-12-18 14:12:15.842732: Epoch 13
2024-12-18 14:12:15.844054: Current learning rate: 0.00922
2024-12-18 14:17:08.796018: Validation loss did not improve from -0.52196. Patience: 2/50
2024-12-18 14:17:08.799037: train_loss -0.6424
2024-12-18 14:17:08.800738: val_loss -0.492
2024-12-18 14:17:08.801883: Pseudo dice [0.7159]
2024-12-18 14:17:08.803166: Epoch time: 292.96 s
2024-12-18 14:17:10.184315: 
2024-12-18 14:17:10.186022: Epoch 14
2024-12-18 14:17:10.186938: Current learning rate: 0.00916
2024-12-18 14:21:59.108055: Validation loss did not improve from -0.52196. Patience: 3/50
2024-12-18 14:21:59.109371: train_loss -0.6566
2024-12-18 14:21:59.110779: val_loss -0.4993
2024-12-18 14:21:59.111640: Pseudo dice [0.725]
2024-12-18 14:21:59.112352: Epoch time: 288.93 s
2024-12-18 14:21:59.475914: Yayy! New best EMA pseudo Dice: 0.7058
2024-12-18 14:22:01.216243: 
2024-12-18 14:22:01.217643: Epoch 15
2024-12-18 14:22:01.218456: Current learning rate: 0.0091
2024-12-18 14:26:30.581929: Validation loss did not improve from -0.52196. Patience: 4/50
2024-12-18 14:26:30.583594: train_loss -0.6527
2024-12-18 14:26:30.585072: val_loss -0.5033
2024-12-18 14:26:30.586141: Pseudo dice [0.7259]
2024-12-18 14:26:30.586974: Epoch time: 269.37 s
2024-12-18 14:26:30.587879: Yayy! New best EMA pseudo Dice: 0.7078
2024-12-18 14:26:32.350262: 
2024-12-18 14:26:32.351803: Epoch 16
2024-12-18 14:26:32.352655: Current learning rate: 0.00903
2024-12-18 14:31:15.339580: Validation loss did not improve from -0.52196. Patience: 5/50
2024-12-18 14:31:15.340561: train_loss -0.6547
2024-12-18 14:31:15.341445: val_loss -0.5081
2024-12-18 14:31:15.342148: Pseudo dice [0.7258]
2024-12-18 14:31:15.342987: Epoch time: 282.99 s
2024-12-18 14:31:15.343903: Yayy! New best EMA pseudo Dice: 0.7096
2024-12-18 14:31:17.089152: 
2024-12-18 14:31:17.090549: Epoch 17
2024-12-18 14:31:17.091302: Current learning rate: 0.00897
2024-12-18 14:35:40.377970: Validation loss did not improve from -0.52196. Patience: 6/50
2024-12-18 14:35:40.378901: train_loss -0.6675
2024-12-18 14:35:40.379660: val_loss -0.4978
2024-12-18 14:35:40.380291: Pseudo dice [0.721]
2024-12-18 14:35:40.381010: Epoch time: 263.29 s
2024-12-18 14:35:40.381612: Yayy! New best EMA pseudo Dice: 0.7108
2024-12-18 14:35:42.139624: 
2024-12-18 14:35:42.140962: Epoch 18
2024-12-18 14:35:42.141696: Current learning rate: 0.00891
2024-12-18 14:40:34.321177: Validation loss did not improve from -0.52196. Patience: 7/50
2024-12-18 14:40:34.322417: train_loss -0.6652
2024-12-18 14:40:34.323334: val_loss -0.4967
2024-12-18 14:40:34.324077: Pseudo dice [0.7258]
2024-12-18 14:40:34.324786: Epoch time: 292.18 s
2024-12-18 14:40:34.325711: Yayy! New best EMA pseudo Dice: 0.7123
2024-12-18 14:40:36.527382: 
2024-12-18 14:40:36.528685: Epoch 19
2024-12-18 14:40:36.529371: Current learning rate: 0.00885
2024-12-18 14:45:26.974818: Validation loss did not improve from -0.52196. Patience: 8/50
2024-12-18 14:45:26.975744: train_loss -0.6725
2024-12-18 14:45:26.976487: val_loss -0.4462
2024-12-18 14:45:26.977275: Pseudo dice [0.6794]
2024-12-18 14:45:26.978085: Epoch time: 290.45 s
2024-12-18 14:45:28.858533: 
2024-12-18 14:45:28.859911: Epoch 20
2024-12-18 14:45:28.860686: Current learning rate: 0.00879
2024-12-18 14:50:09.921004: Validation loss did not improve from -0.52196. Patience: 9/50
2024-12-18 14:50:09.922062: train_loss -0.6782
2024-12-18 14:50:09.922790: val_loss -0.5073
2024-12-18 14:50:09.923471: Pseudo dice [0.7224]
2024-12-18 14:50:09.924511: Epoch time: 281.06 s
2024-12-18 14:50:11.338325: 
2024-12-18 14:50:11.339662: Epoch 21
2024-12-18 14:50:11.340457: Current learning rate: 0.00873
2024-12-18 14:55:09.169864: Validation loss did not improve from -0.52196. Patience: 10/50
2024-12-18 14:55:09.170831: train_loss -0.6771
2024-12-18 14:55:09.171847: val_loss -0.4298
2024-12-18 14:55:09.172669: Pseudo dice [0.6762]
2024-12-18 14:55:09.173620: Epoch time: 297.83 s
2024-12-18 14:55:10.465751: 
2024-12-18 14:55:10.467131: Epoch 22
2024-12-18 14:55:10.467792: Current learning rate: 0.00867
2024-12-18 15:00:14.956370: Validation loss did not improve from -0.52196. Patience: 11/50
2024-12-18 15:00:14.957034: train_loss -0.6903
2024-12-18 15:00:14.958282: val_loss -0.4862
2024-12-18 15:00:14.959226: Pseudo dice [0.6972]
2024-12-18 15:00:14.960010: Epoch time: 304.49 s
2024-12-18 15:00:16.281607: 
2024-12-18 15:00:16.282753: Epoch 23
2024-12-18 15:00:16.283484: Current learning rate: 0.00861
2024-12-18 15:05:05.578818: Validation loss did not improve from -0.52196. Patience: 12/50
2024-12-18 15:05:05.579998: train_loss -0.6852
2024-12-18 15:05:05.580888: val_loss -0.5015
2024-12-18 15:05:05.581565: Pseudo dice [0.7134]
2024-12-18 15:05:05.582234: Epoch time: 289.3 s
2024-12-18 15:05:06.894500: 
2024-12-18 15:05:06.895831: Epoch 24
2024-12-18 15:05:06.896675: Current learning rate: 0.00855
2024-12-18 15:09:56.952474: Validation loss did not improve from -0.52196. Patience: 13/50
2024-12-18 15:09:56.953502: train_loss -0.6863
2024-12-18 15:09:56.954437: val_loss -0.4753
2024-12-18 15:09:56.955273: Pseudo dice [0.6978]
2024-12-18 15:09:56.956343: Epoch time: 290.06 s
2024-12-18 15:09:58.698243: 
2024-12-18 15:09:58.699564: Epoch 25
2024-12-18 15:09:58.700366: Current learning rate: 0.00849
2024-12-18 15:14:37.589660: Validation loss did not improve from -0.52196. Patience: 14/50
2024-12-18 15:14:37.594083: train_loss -0.7007
2024-12-18 15:14:37.595330: val_loss -0.5077
2024-12-18 15:14:37.596094: Pseudo dice [0.7272]
2024-12-18 15:14:37.597008: Epoch time: 278.9 s
2024-12-18 15:14:38.980562: 
2024-12-18 15:14:38.981959: Epoch 26
2024-12-18 15:14:38.982638: Current learning rate: 0.00843
2024-12-18 15:19:32.065432: Validation loss did not improve from -0.52196. Patience: 15/50
2024-12-18 15:19:32.067094: train_loss -0.7024
2024-12-18 15:19:32.068295: val_loss -0.4798
2024-12-18 15:19:32.069015: Pseudo dice [0.7068]
2024-12-18 15:19:32.069820: Epoch time: 293.09 s
2024-12-18 15:19:33.448028: 
2024-12-18 15:19:33.449934: Epoch 27
2024-12-18 15:19:33.451061: Current learning rate: 0.00836
2024-12-18 15:24:24.449830: Validation loss did not improve from -0.52196. Patience: 16/50
2024-12-18 15:24:24.450961: train_loss -0.7002
2024-12-18 15:24:24.452964: val_loss -0.5113
2024-12-18 15:24:24.453915: Pseudo dice [0.7212]
2024-12-18 15:24:24.455036: Epoch time: 291.0 s
2024-12-18 15:24:25.800964: 
2024-12-18 15:24:25.802283: Epoch 28
2024-12-18 15:24:25.803183: Current learning rate: 0.0083
2024-12-18 15:29:03.258735: Validation loss did not improve from -0.52196. Patience: 17/50
2024-12-18 15:29:03.259728: train_loss -0.7037
2024-12-18 15:29:03.260650: val_loss -0.4641
2024-12-18 15:29:03.261881: Pseudo dice [0.6965]
2024-12-18 15:29:03.262852: Epoch time: 277.46 s
2024-12-18 15:29:04.956061: 
2024-12-18 15:29:04.957425: Epoch 29
2024-12-18 15:29:04.958146: Current learning rate: 0.00824
2024-12-18 15:33:49.591551: Validation loss did not improve from -0.52196. Patience: 18/50
2024-12-18 15:33:49.592523: train_loss -0.7032
2024-12-18 15:33:49.593677: val_loss -0.5012
2024-12-18 15:33:49.594450: Pseudo dice [0.7264]
2024-12-18 15:33:49.595052: Epoch time: 284.64 s
2024-12-18 15:33:51.586176: 
2024-12-18 15:33:51.587458: Epoch 30
2024-12-18 15:33:51.588128: Current learning rate: 0.00818
2024-12-18 15:38:38.752804: Validation loss improved from -0.52196 to -0.52766! Patience: 18/50
2024-12-18 15:38:38.753752: train_loss -0.7027
2024-12-18 15:38:38.754516: val_loss -0.5277
2024-12-18 15:38:38.755331: Pseudo dice [0.7307]
2024-12-18 15:38:38.756342: Epoch time: 287.17 s
2024-12-18 15:38:40.107296: 
2024-12-18 15:38:40.108719: Epoch 31
2024-12-18 15:38:40.109562: Current learning rate: 0.00812
2024-12-18 15:43:18.542161: Validation loss did not improve from -0.52766. Patience: 1/50
2024-12-18 15:43:18.543307: train_loss -0.7051
2024-12-18 15:43:18.544302: val_loss -0.4834
2024-12-18 15:43:18.545510: Pseudo dice [0.7164]
2024-12-18 15:43:18.546779: Epoch time: 278.44 s
2024-12-18 15:43:18.547936: Yayy! New best EMA pseudo Dice: 0.7123
2024-12-18 15:43:20.327725: 
2024-12-18 15:43:20.329091: Epoch 32
2024-12-18 15:43:20.329844: Current learning rate: 0.00806
2024-12-18 15:48:15.117860: Validation loss did not improve from -0.52766. Patience: 2/50
2024-12-18 15:48:15.118840: train_loss -0.7114
2024-12-18 15:48:15.119889: val_loss -0.4146
2024-12-18 15:48:15.120704: Pseudo dice [0.6708]
2024-12-18 15:48:15.121506: Epoch time: 294.79 s
2024-12-18 15:48:16.496132: 
2024-12-18 15:48:16.497232: Epoch 33
2024-12-18 15:48:16.497990: Current learning rate: 0.008
2024-12-18 15:52:45.058911: Validation loss did not improve from -0.52766. Patience: 3/50
2024-12-18 15:52:45.059811: train_loss -0.7135
2024-12-18 15:52:45.060787: val_loss -0.5173
2024-12-18 15:52:45.061764: Pseudo dice [0.7257]
2024-12-18 15:52:45.062659: Epoch time: 268.56 s
2024-12-18 15:52:46.426554: 
2024-12-18 15:52:46.428079: Epoch 34
2024-12-18 15:52:46.429099: Current learning rate: 0.00793
2024-12-18 15:57:29.364551: Validation loss did not improve from -0.52766. Patience: 4/50
2024-12-18 15:57:29.365838: train_loss -0.7168
2024-12-18 15:57:29.366867: val_loss -0.4918
2024-12-18 15:57:29.367966: Pseudo dice [0.7169]
2024-12-18 15:57:29.369037: Epoch time: 282.94 s
2024-12-18 15:57:31.190558: 
2024-12-18 15:57:31.192366: Epoch 35
2024-12-18 15:57:31.193286: Current learning rate: 0.00787
2024-12-18 16:02:03.664991: Validation loss did not improve from -0.52766. Patience: 5/50
2024-12-18 16:02:03.665971: train_loss -0.7157
2024-12-18 16:02:03.666817: val_loss -0.5196
2024-12-18 16:02:03.667608: Pseudo dice [0.7378]
2024-12-18 16:02:03.668276: Epoch time: 272.48 s
2024-12-18 16:02:03.668939: Yayy! New best EMA pseudo Dice: 0.7133
2024-12-18 16:02:05.419238: 
2024-12-18 16:02:05.420569: Epoch 36
2024-12-18 16:02:05.421355: Current learning rate: 0.00781
2024-12-18 16:06:29.978314: Validation loss did not improve from -0.52766. Patience: 6/50
2024-12-18 16:06:29.979399: train_loss -0.7237
2024-12-18 16:06:29.980129: val_loss -0.5091
2024-12-18 16:06:29.980861: Pseudo dice [0.7219]
2024-12-18 16:06:29.981499: Epoch time: 264.56 s
2024-12-18 16:06:29.982200: Yayy! New best EMA pseudo Dice: 0.7142
2024-12-18 16:06:31.788387: 
2024-12-18 16:06:31.789834: Epoch 37
2024-12-18 16:06:31.790516: Current learning rate: 0.00775
2024-12-18 16:11:41.338741: Validation loss did not improve from -0.52766. Patience: 7/50
2024-12-18 16:11:41.339704: train_loss -0.7231
2024-12-18 16:11:41.340759: val_loss -0.4839
2024-12-18 16:11:41.341725: Pseudo dice [0.714]
2024-12-18 16:11:41.342536: Epoch time: 309.55 s
2024-12-18 16:11:42.744953: 
2024-12-18 16:11:42.746585: Epoch 38
2024-12-18 16:11:42.747569: Current learning rate: 0.00769
2024-12-18 16:17:07.134458: Validation loss did not improve from -0.52766. Patience: 8/50
2024-12-18 16:17:07.135467: train_loss -0.7325
2024-12-18 16:17:07.136396: val_loss -0.4978
2024-12-18 16:17:07.137110: Pseudo dice [0.7202]
2024-12-18 16:17:07.138200: Epoch time: 324.39 s
2024-12-18 16:17:07.138928: Yayy! New best EMA pseudo Dice: 0.7148
2024-12-18 16:17:08.976043: 
2024-12-18 16:17:08.977552: Epoch 39
2024-12-18 16:17:08.978331: Current learning rate: 0.00763
2024-12-18 16:22:18.994518: Validation loss did not improve from -0.52766. Patience: 9/50
2024-12-18 16:22:19.008323: train_loss -0.7215
2024-12-18 16:22:19.009882: val_loss -0.5094
2024-12-18 16:22:19.010859: Pseudo dice [0.7285]
2024-12-18 16:22:19.011919: Epoch time: 310.03 s
2024-12-18 16:22:20.069779: Yayy! New best EMA pseudo Dice: 0.7161
2024-12-18 16:22:21.897175: 
2024-12-18 16:22:21.898778: Epoch 40
2024-12-18 16:22:21.899725: Current learning rate: 0.00756
2024-12-18 16:27:34.532853: Validation loss did not improve from -0.52766. Patience: 10/50
2024-12-18 16:27:34.534310: train_loss -0.7267
2024-12-18 16:27:34.536353: val_loss -0.479
2024-12-18 16:27:34.537154: Pseudo dice [0.7246]
2024-12-18 16:27:34.537989: Epoch time: 312.64 s
2024-12-18 16:27:34.538684: Yayy! New best EMA pseudo Dice: 0.717
2024-12-18 16:27:36.355364: 
2024-12-18 16:27:36.356918: Epoch 41
2024-12-18 16:27:36.358391: Current learning rate: 0.0075
2024-12-18 16:33:20.009166: Validation loss did not improve from -0.52766. Patience: 11/50
2024-12-18 16:33:20.010558: train_loss -0.7295
2024-12-18 16:33:20.011462: val_loss -0.5164
2024-12-18 16:33:20.012346: Pseudo dice [0.7284]
2024-12-18 16:33:20.013239: Epoch time: 343.66 s
2024-12-18 16:33:20.014141: Yayy! New best EMA pseudo Dice: 0.7181
2024-12-18 16:33:21.694069: 
2024-12-18 16:33:21.695590: Epoch 42
2024-12-18 16:33:21.696487: Current learning rate: 0.00744
2024-12-18 16:39:32.769162: Validation loss did not improve from -0.52766. Patience: 12/50
2024-12-18 16:39:32.770134: train_loss -0.7308
2024-12-18 16:39:32.770993: val_loss -0.512
2024-12-18 16:39:32.771827: Pseudo dice [0.7347]
2024-12-18 16:39:32.772730: Epoch time: 371.08 s
2024-12-18 16:39:32.773643: Yayy! New best EMA pseudo Dice: 0.7198
2024-12-18 16:39:34.491592: 
2024-12-18 16:39:34.493047: Epoch 43
2024-12-18 16:39:34.493946: Current learning rate: 0.00738
2024-12-18 16:45:47.619023: Validation loss did not improve from -0.52766. Patience: 13/50
2024-12-18 16:45:47.620027: train_loss -0.7276
2024-12-18 16:45:47.621040: val_loss -0.4876
2024-12-18 16:45:47.622274: Pseudo dice [0.7169]
2024-12-18 16:45:47.623008: Epoch time: 373.13 s
2024-12-18 16:45:48.942566: 
2024-12-18 16:45:48.943897: Epoch 44
2024-12-18 16:45:48.944885: Current learning rate: 0.00732
2024-12-18 16:51:38.198998: Validation loss did not improve from -0.52766. Patience: 14/50
2024-12-18 16:51:38.200026: train_loss -0.7322
2024-12-18 16:51:38.200829: val_loss -0.5211
2024-12-18 16:51:38.201716: Pseudo dice [0.7412]
2024-12-18 16:51:38.202379: Epoch time: 349.26 s
2024-12-18 16:51:38.627047: Yayy! New best EMA pseudo Dice: 0.7217
2024-12-18 16:51:40.360107: 
2024-12-18 16:51:40.361535: Epoch 45
2024-12-18 16:51:40.362649: Current learning rate: 0.00725
2024-12-18 16:57:19.034932: Validation loss did not improve from -0.52766. Patience: 15/50
2024-12-18 16:57:19.035869: train_loss -0.7362
2024-12-18 16:57:19.036715: val_loss -0.4853
2024-12-18 16:57:19.037624: Pseudo dice [0.7108]
2024-12-18 16:57:19.038384: Epoch time: 338.68 s
2024-12-18 16:57:20.394230: 
2024-12-18 16:57:20.396178: Epoch 46
2024-12-18 16:57:20.396940: Current learning rate: 0.00719
2024-12-18 17:02:57.269192: Validation loss did not improve from -0.52766. Patience: 16/50
2024-12-18 17:02:57.270191: train_loss -0.7346
2024-12-18 17:02:57.271066: val_loss -0.4687
2024-12-18 17:02:57.271965: Pseudo dice [0.7077]
2024-12-18 17:02:57.272661: Epoch time: 336.88 s
2024-12-18 17:02:58.716525: 
2024-12-18 17:02:58.717941: Epoch 47
2024-12-18 17:02:58.718899: Current learning rate: 0.00713
2024-12-18 17:09:20.279837: Validation loss did not improve from -0.52766. Patience: 17/50
2024-12-18 17:09:20.280794: train_loss -0.7397
2024-12-18 17:09:20.281809: val_loss -0.5147
2024-12-18 17:09:20.282582: Pseudo dice [0.7318]
2024-12-18 17:09:20.283514: Epoch time: 381.57 s
2024-12-18 17:09:21.657796: 
2024-12-18 17:09:21.659494: Epoch 48
2024-12-18 17:09:21.660399: Current learning rate: 0.00707
2024-12-18 17:15:32.484870: Validation loss did not improve from -0.52766. Patience: 18/50
2024-12-18 17:15:32.485933: train_loss -0.737
2024-12-18 17:15:32.486776: val_loss -0.4629
2024-12-18 17:15:32.487605: Pseudo dice [0.6981]
2024-12-18 17:15:32.488499: Epoch time: 370.83 s
2024-12-18 17:15:33.913761: 
2024-12-18 17:15:33.915383: Epoch 49
2024-12-18 17:15:33.916444: Current learning rate: 0.007
2024-12-18 17:21:20.644643: Validation loss did not improve from -0.52766. Patience: 19/50
2024-12-18 17:21:20.645771: train_loss -0.7475
2024-12-18 17:21:20.647169: val_loss -0.5035
2024-12-18 17:21:20.648288: Pseudo dice [0.7226]
2024-12-18 17:21:20.649416: Epoch time: 346.73 s
2024-12-18 17:21:23.179965: 
2024-12-18 17:21:23.181786: Epoch 50
2024-12-18 17:21:23.182908: Current learning rate: 0.00694
2024-12-18 17:26:59.452203: Validation loss did not improve from -0.52766. Patience: 20/50
2024-12-18 17:26:59.455324: train_loss -0.7485
2024-12-18 17:26:59.456575: val_loss -0.5193
2024-12-18 17:26:59.457282: Pseudo dice [0.7357]
2024-12-18 17:26:59.458180: Epoch time: 336.28 s
2024-12-18 17:27:00.913652: 
2024-12-18 17:27:00.914838: Epoch 51
2024-12-18 17:27:00.915673: Current learning rate: 0.00688
2024-12-18 17:33:28.116564: Validation loss did not improve from -0.52766. Patience: 21/50
2024-12-18 17:33:28.118557: train_loss -0.7468
2024-12-18 17:33:28.120272: val_loss -0.475
2024-12-18 17:33:28.121084: Pseudo dice [0.7127]
2024-12-18 17:33:28.122036: Epoch time: 387.21 s
2024-12-18 17:33:29.541328: 
2024-12-18 17:33:29.542556: Epoch 52
2024-12-18 17:33:29.543201: Current learning rate: 0.00682
2024-12-18 17:39:15.950639: Validation loss did not improve from -0.52766. Patience: 22/50
2024-12-18 17:39:15.951767: train_loss -0.7468
2024-12-18 17:39:15.953025: val_loss -0.4942
2024-12-18 17:39:15.954214: Pseudo dice [0.7225]
2024-12-18 17:39:15.955113: Epoch time: 346.41 s
2024-12-18 17:39:17.364016: 
2024-12-18 17:39:17.365376: Epoch 53
2024-12-18 17:39:17.366436: Current learning rate: 0.00675
2024-12-18 17:44:49.636105: Validation loss did not improve from -0.52766. Patience: 23/50
2024-12-18 17:44:49.637096: train_loss -0.7499
2024-12-18 17:44:49.638180: val_loss -0.5154
2024-12-18 17:44:49.639042: Pseudo dice [0.737]
2024-12-18 17:44:49.639743: Epoch time: 332.27 s
2024-12-18 17:44:51.034791: 
2024-12-18 17:44:51.036371: Epoch 54
2024-12-18 17:44:51.037335: Current learning rate: 0.00669
2024-12-18 17:50:34.438659: Validation loss did not improve from -0.52766. Patience: 24/50
2024-12-18 17:50:34.439388: train_loss -0.7539
2024-12-18 17:50:34.440263: val_loss -0.4867
2024-12-18 17:50:34.441086: Pseudo dice [0.7173]
2024-12-18 17:50:34.441805: Epoch time: 343.41 s
2024-12-18 17:50:36.316247: 
2024-12-18 17:50:36.317891: Epoch 55
2024-12-18 17:50:36.319153: Current learning rate: 0.00663
2024-12-18 17:56:29.156257: Validation loss did not improve from -0.52766. Patience: 25/50
2024-12-18 17:56:29.157342: train_loss -0.7482
2024-12-18 17:56:29.158265: val_loss -0.4916
2024-12-18 17:56:29.159006: Pseudo dice [0.7231]
2024-12-18 17:56:29.159724: Epoch time: 352.84 s
2024-12-18 17:56:30.512842: 
2024-12-18 17:56:30.514171: Epoch 56
2024-12-18 17:56:30.515560: Current learning rate: 0.00657
2024-12-18 18:02:05.139859: Validation loss did not improve from -0.52766. Patience: 26/50
2024-12-18 18:02:05.140855: train_loss -0.747
2024-12-18 18:02:05.141593: val_loss -0.4832
2024-12-18 18:02:05.142250: Pseudo dice [0.7198]
2024-12-18 18:02:05.142948: Epoch time: 334.63 s
2024-12-18 18:02:06.573091: 
2024-12-18 18:02:06.574332: Epoch 57
2024-12-18 18:02:06.575076: Current learning rate: 0.0065
2024-12-18 18:07:34.497392: Validation loss did not improve from -0.52766. Patience: 27/50
2024-12-18 18:07:34.498308: train_loss -0.7541
2024-12-18 18:07:34.499186: val_loss -0.3872
2024-12-18 18:07:34.499926: Pseudo dice [0.6553]
2024-12-18 18:07:34.500709: Epoch time: 327.93 s
2024-12-18 18:07:35.855701: 
2024-12-18 18:07:35.857448: Epoch 58
2024-12-18 18:07:35.858457: Current learning rate: 0.00644
2024-12-18 18:15:04.246759: Validation loss improved from -0.52766 to -0.52816! Patience: 27/50
2024-12-18 18:15:04.247861: train_loss -0.7532
2024-12-18 18:15:04.248976: val_loss -0.5282
2024-12-18 18:15:04.249990: Pseudo dice [0.7449]
2024-12-18 18:15:04.250897: Epoch time: 448.39 s
2024-12-18 18:15:05.604812: 
2024-12-18 18:15:05.605929: Epoch 59
2024-12-18 18:15:05.606943: Current learning rate: 0.00638
2024-12-18 18:22:24.097386: Validation loss did not improve from -0.52816. Patience: 1/50
2024-12-18 18:22:24.098551: train_loss -0.7548
2024-12-18 18:22:24.099838: val_loss -0.5062
2024-12-18 18:22:24.100657: Pseudo dice [0.7243]
2024-12-18 18:22:24.101576: Epoch time: 438.49 s
2024-12-18 18:22:25.899222: 
2024-12-18 18:22:25.900601: Epoch 60
2024-12-18 18:22:25.901700: Current learning rate: 0.00631
2024-12-18 18:29:08.099413: Validation loss did not improve from -0.52816. Patience: 2/50
2024-12-18 18:29:08.102304: train_loss -0.7576
2024-12-18 18:29:08.103428: val_loss -0.5116
2024-12-18 18:29:08.104234: Pseudo dice [0.7303]
2024-12-18 18:29:08.105379: Epoch time: 402.2 s
2024-12-18 18:29:10.359607: 
2024-12-18 18:29:10.361041: Epoch 61
2024-12-18 18:29:10.361918: Current learning rate: 0.00625
2024-12-18 18:35:36.189638: Validation loss did not improve from -0.52816. Patience: 3/50
2024-12-18 18:35:36.190591: train_loss -0.7557
2024-12-18 18:35:36.192256: val_loss -0.4958
2024-12-18 18:35:36.193029: Pseudo dice [0.7219]
2024-12-18 18:35:36.193738: Epoch time: 385.83 s
2024-12-18 18:35:37.623193: 
2024-12-18 18:35:37.624298: Epoch 62
2024-12-18 18:35:37.624995: Current learning rate: 0.00619
2024-12-18 18:42:02.266791: Validation loss did not improve from -0.52816. Patience: 4/50
2024-12-18 18:42:02.268845: train_loss -0.7593
2024-12-18 18:42:02.270042: val_loss -0.4832
2024-12-18 18:42:02.271133: Pseudo dice [0.7073]
2024-12-18 18:42:02.272296: Epoch time: 384.65 s
2024-12-18 18:42:03.709515: 
2024-12-18 18:42:03.710863: Epoch 63
2024-12-18 18:42:03.711792: Current learning rate: 0.00612
2024-12-18 18:47:48.960874: Validation loss did not improve from -0.52816. Patience: 5/50
2024-12-18 18:47:48.961961: train_loss -0.7599
2024-12-18 18:47:48.962842: val_loss -0.5008
2024-12-18 18:47:48.963638: Pseudo dice [0.7263]
2024-12-18 18:47:48.964338: Epoch time: 345.25 s
2024-12-18 18:47:50.336004: 
2024-12-18 18:47:50.337279: Epoch 64
2024-12-18 18:47:50.338152: Current learning rate: 0.00606
2024-12-18 18:53:54.552872: Validation loss did not improve from -0.52816. Patience: 6/50
2024-12-18 18:53:54.553929: train_loss -0.7572
2024-12-18 18:53:54.555195: val_loss -0.5031
2024-12-18 18:53:54.556153: Pseudo dice [0.723]
2024-12-18 18:53:54.556851: Epoch time: 364.22 s
2024-12-18 18:53:56.380057: 
2024-12-18 18:53:56.381499: Epoch 65
2024-12-18 18:53:56.382575: Current learning rate: 0.006
2024-12-18 19:00:28.920675: Validation loss did not improve from -0.52816. Patience: 7/50
2024-12-18 19:00:28.921727: train_loss -0.7628
2024-12-18 19:00:28.922594: val_loss -0.4957
2024-12-18 19:00:28.923449: Pseudo dice [0.7098]
2024-12-18 19:00:28.924434: Epoch time: 392.54 s
2024-12-18 19:00:30.294005: 
2024-12-18 19:00:30.295205: Epoch 66
2024-12-18 19:00:30.296033: Current learning rate: 0.00593
2024-12-18 19:07:05.748762: Validation loss did not improve from -0.52816. Patience: 8/50
2024-12-18 19:07:05.750119: train_loss -0.7633
2024-12-18 19:07:05.750968: val_loss -0.5038
2024-12-18 19:07:05.751915: Pseudo dice [0.7308]
2024-12-18 19:07:05.752741: Epoch time: 395.46 s
2024-12-18 19:07:07.098985: 
2024-12-18 19:07:07.100323: Epoch 67
2024-12-18 19:07:07.101068: Current learning rate: 0.00587
2024-12-18 19:13:44.982732: Validation loss did not improve from -0.52816. Patience: 9/50
2024-12-18 19:13:44.983685: train_loss -0.7649
2024-12-18 19:13:44.984946: val_loss -0.4935
2024-12-18 19:13:44.986039: Pseudo dice [0.7288]
2024-12-18 19:13:44.986989: Epoch time: 397.89 s
2024-12-18 19:13:46.375218: 
2024-12-18 19:13:46.376860: Epoch 68
2024-12-18 19:13:46.377689: Current learning rate: 0.00581
2024-12-18 19:20:13.617208: Validation loss did not improve from -0.52816. Patience: 10/50
2024-12-18 19:20:13.618123: train_loss -0.7648
2024-12-18 19:20:13.619121: val_loss -0.5216
2024-12-18 19:20:13.619963: Pseudo dice [0.7414]
2024-12-18 19:20:13.620752: Epoch time: 387.24 s
2024-12-18 19:20:13.621447: Yayy! New best EMA pseudo Dice: 0.7228
2024-12-18 19:20:15.469757: 
2024-12-18 19:20:15.470992: Epoch 69
2024-12-18 19:20:15.471658: Current learning rate: 0.00574
2024-12-18 19:26:43.832458: Validation loss did not improve from -0.52816. Patience: 11/50
2024-12-18 19:26:43.833456: train_loss -0.7646
2024-12-18 19:26:43.834242: val_loss -0.4534
2024-12-18 19:26:43.834920: Pseudo dice [0.7072]
2024-12-18 19:26:43.835983: Epoch time: 388.37 s
2024-12-18 19:26:45.616711: 
2024-12-18 19:26:45.618020: Epoch 70
2024-12-18 19:26:45.618978: Current learning rate: 0.00568
2024-12-18 19:33:30.634190: Validation loss did not improve from -0.52816. Patience: 12/50
2024-12-18 19:33:30.637466: train_loss -0.7646
2024-12-18 19:33:30.638592: val_loss -0.5114
2024-12-18 19:33:30.639310: Pseudo dice [0.731]
2024-12-18 19:33:30.640133: Epoch time: 405.02 s
2024-12-18 19:33:32.043660: 
2024-12-18 19:33:32.045018: Epoch 71
2024-12-18 19:33:32.045855: Current learning rate: 0.00562
2024-12-18 19:40:13.391176: Validation loss did not improve from -0.52816. Patience: 13/50
2024-12-18 19:40:13.393012: train_loss -0.7676
2024-12-18 19:40:13.394599: val_loss -0.4904
2024-12-18 19:40:13.395567: Pseudo dice [0.7249]
2024-12-18 19:40:13.396705: Epoch time: 401.35 s
2024-12-18 19:40:15.449038: 
2024-12-18 19:40:15.450625: Epoch 72
2024-12-18 19:40:15.451713: Current learning rate: 0.00555
2024-12-18 19:46:49.589564: Validation loss did not improve from -0.52816. Patience: 14/50
2024-12-18 19:46:49.591496: train_loss -0.7639
2024-12-18 19:46:49.593631: val_loss -0.4896
2024-12-18 19:46:49.594509: Pseudo dice [0.7221]
2024-12-18 19:46:49.595316: Epoch time: 394.14 s
2024-12-18 19:46:50.980293: 
2024-12-18 19:46:50.981786: Epoch 73
2024-12-18 19:46:50.982551: Current learning rate: 0.00549
2024-12-18 19:53:31.752964: Validation loss did not improve from -0.52816. Patience: 15/50
2024-12-18 19:53:31.753949: train_loss -0.7698
2024-12-18 19:53:31.754752: val_loss -0.4228
2024-12-18 19:53:31.755568: Pseudo dice [0.6774]
2024-12-18 19:53:31.756346: Epoch time: 400.77 s
2024-12-18 19:53:33.126841: 
2024-12-18 19:53:33.128212: Epoch 74
2024-12-18 19:53:33.128847: Current learning rate: 0.00542
2024-12-18 20:00:12.289475: Validation loss did not improve from -0.52816. Patience: 16/50
2024-12-18 20:00:12.290511: train_loss -0.7685
2024-12-18 20:00:12.291326: val_loss -0.4504
2024-12-18 20:00:12.291970: Pseudo dice [0.6935]
2024-12-18 20:00:12.292636: Epoch time: 399.16 s
2024-12-18 20:00:14.039042: 
2024-12-18 20:00:14.041579: Epoch 75
2024-12-18 20:00:14.042559: Current learning rate: 0.00536
2024-12-18 20:06:56.352138: Validation loss did not improve from -0.52816. Patience: 17/50
2024-12-18 20:06:56.353173: train_loss -0.7717
2024-12-18 20:06:56.354088: val_loss -0.4747
2024-12-18 20:06:56.354778: Pseudo dice [0.7155]
2024-12-18 20:06:56.355434: Epoch time: 402.32 s
2024-12-18 20:06:57.719647: 
2024-12-18 20:06:57.721372: Epoch 76
2024-12-18 20:06:57.722268: Current learning rate: 0.00529
2024-12-18 20:13:03.526604: Validation loss did not improve from -0.52816. Patience: 18/50
2024-12-18 20:13:03.527776: train_loss -0.7751
2024-12-18 20:13:03.528772: val_loss -0.4988
2024-12-18 20:13:03.529768: Pseudo dice [0.7146]
2024-12-18 20:13:03.530838: Epoch time: 365.81 s
2024-12-18 20:13:05.012909: 
2024-12-18 20:13:05.015049: Epoch 77
2024-12-18 20:13:05.016300: Current learning rate: 0.00523
2024-12-18 20:19:30.720846: Validation loss did not improve from -0.52816. Patience: 19/50
2024-12-18 20:19:30.721877: train_loss -0.7724
2024-12-18 20:19:30.722880: val_loss -0.4846
2024-12-18 20:19:30.723819: Pseudo dice [0.7185]
2024-12-18 20:19:30.724576: Epoch time: 385.71 s
2024-12-18 20:19:32.266827: 
2024-12-18 20:19:32.268165: Epoch 78
2024-12-18 20:19:32.268881: Current learning rate: 0.00517
2024-12-18 20:26:05.326414: Validation loss did not improve from -0.52816. Patience: 20/50
2024-12-18 20:26:05.327339: train_loss -0.7699
2024-12-18 20:26:05.328393: val_loss -0.4596
2024-12-18 20:26:05.329473: Pseudo dice [0.7114]
2024-12-18 20:26:05.330570: Epoch time: 393.06 s
2024-12-18 20:26:06.811300: 
2024-12-18 20:26:06.812839: Epoch 79
2024-12-18 20:26:06.813845: Current learning rate: 0.0051
2024-12-18 20:33:01.844987: Validation loss did not improve from -0.52816. Patience: 21/50
2024-12-18 20:33:01.846053: train_loss -0.775
2024-12-18 20:33:01.846844: val_loss -0.4953
2024-12-18 20:33:01.847504: Pseudo dice [0.7279]
2024-12-18 20:33:01.848263: Epoch time: 415.04 s
2024-12-18 20:33:03.816098: 
2024-12-18 20:33:03.817971: Epoch 80
2024-12-18 20:33:03.818882: Current learning rate: 0.00504
2024-12-18 20:39:42.448418: Validation loss did not improve from -0.52816. Patience: 22/50
2024-12-18 20:39:42.451214: train_loss -0.7758
2024-12-18 20:39:42.452514: val_loss -0.5045
2024-12-18 20:39:42.453331: Pseudo dice [0.7373]
2024-12-18 20:39:42.454321: Epoch time: 398.64 s
2024-12-18 20:39:43.988995: 
2024-12-18 20:39:43.990849: Epoch 81
2024-12-18 20:39:43.991670: Current learning rate: 0.00497
2024-12-18 20:45:50.247621: Validation loss did not improve from -0.52816. Patience: 23/50
2024-12-18 20:45:50.249485: train_loss -0.7751
2024-12-18 20:45:50.251231: val_loss -0.4993
2024-12-18 20:45:50.252251: Pseudo dice [0.7249]
2024-12-18 20:45:50.253344: Epoch time: 366.26 s
2024-12-18 20:45:51.790521: 
2024-12-18 20:45:51.792893: Epoch 82
2024-12-18 20:45:51.794115: Current learning rate: 0.00491
2024-12-18 20:52:16.913748: Validation loss did not improve from -0.52816. Patience: 24/50
2024-12-18 20:52:16.915228: train_loss -0.7768
2024-12-18 20:52:16.916000: val_loss -0.5053
2024-12-18 20:52:16.916728: Pseudo dice [0.7315]
2024-12-18 20:52:16.917700: Epoch time: 385.13 s
2024-12-18 20:52:18.937652: 
2024-12-18 20:52:18.938956: Epoch 83
2024-12-18 20:52:18.939882: Current learning rate: 0.00484
2024-12-18 20:58:59.521311: Validation loss did not improve from -0.52816. Patience: 25/50
2024-12-18 20:58:59.522316: train_loss -0.7738
2024-12-18 20:58:59.523081: val_loss -0.4939
2024-12-18 20:58:59.523794: Pseudo dice [0.7246]
2024-12-18 20:58:59.524603: Epoch time: 400.59 s
2024-12-18 20:59:00.840681: 
2024-12-18 20:59:00.841975: Epoch 84
2024-12-18 20:59:00.842875: Current learning rate: 0.00478
2024-12-18 21:05:35.393765: Validation loss did not improve from -0.52816. Patience: 26/50
2024-12-18 21:05:35.394823: train_loss -0.7768
2024-12-18 21:05:35.395749: val_loss -0.4871
2024-12-18 21:05:35.396736: Pseudo dice [0.7231]
2024-12-18 21:05:35.397756: Epoch time: 394.56 s
2024-12-18 21:05:37.587016: 
2024-12-18 21:05:37.588310: Epoch 85
2024-12-18 21:05:37.589395: Current learning rate: 0.00471
2024-12-18 21:11:58.946566: Validation loss did not improve from -0.52816. Patience: 27/50
2024-12-18 21:11:58.947570: train_loss -0.7817
2024-12-18 21:11:58.948571: val_loss -0.474
2024-12-18 21:11:58.949457: Pseudo dice [0.7187]
2024-12-18 21:11:58.950675: Epoch time: 381.36 s
2024-12-18 21:12:00.331848: 
2024-12-18 21:12:00.334497: Epoch 86
2024-12-18 21:12:00.335532: Current learning rate: 0.00465
2024-12-18 21:18:48.490168: Validation loss did not improve from -0.52816. Patience: 28/50
2024-12-18 21:18:48.491123: train_loss -0.7817
2024-12-18 21:18:48.491985: val_loss -0.4753
2024-12-18 21:18:48.492793: Pseudo dice [0.7186]
2024-12-18 21:18:48.493574: Epoch time: 408.16 s
2024-12-18 21:18:49.873611: 
2024-12-18 21:18:49.875578: Epoch 87
2024-12-18 21:18:49.876838: Current learning rate: 0.00458
2024-12-18 21:25:20.747451: Validation loss did not improve from -0.52816. Patience: 29/50
2024-12-18 21:25:20.748526: train_loss -0.7786
2024-12-18 21:25:20.749390: val_loss -0.4997
2024-12-18 21:25:20.750115: Pseudo dice [0.7297]
2024-12-18 21:25:20.751175: Epoch time: 390.88 s
2024-12-18 21:25:22.139577: 
2024-12-18 21:25:22.140938: Epoch 88
2024-12-18 21:25:22.141687: Current learning rate: 0.00452
2024-12-18 21:31:48.700281: Validation loss did not improve from -0.52816. Patience: 30/50
2024-12-18 21:31:48.701321: train_loss -0.783
2024-12-18 21:31:48.702189: val_loss -0.5127
2024-12-18 21:31:48.702890: Pseudo dice [0.7331]
2024-12-18 21:31:48.703630: Epoch time: 386.56 s
2024-12-18 21:31:50.138098: 
2024-12-18 21:31:50.139778: Epoch 89
2024-12-18 21:31:50.141237: Current learning rate: 0.00445
2024-12-18 21:38:30.361041: Validation loss did not improve from -0.52816. Patience: 31/50
2024-12-18 21:38:30.362033: train_loss -0.7818
2024-12-18 21:38:30.362739: val_loss -0.4902
2024-12-18 21:38:30.363503: Pseudo dice [0.7244]
2024-12-18 21:38:30.364163: Epoch time: 400.23 s
2024-12-18 21:38:30.787727: Yayy! New best EMA pseudo Dice: 0.7229
2024-12-18 21:38:32.697584: 
2024-12-18 21:38:32.698571: Epoch 90
2024-12-18 21:38:32.699432: Current learning rate: 0.00438
2024-12-18 21:45:02.682401: Validation loss did not improve from -0.52816. Patience: 32/50
2024-12-18 21:45:02.715946: train_loss -0.7825
2024-12-18 21:45:02.717196: val_loss -0.5105
2024-12-18 21:45:02.718105: Pseudo dice [0.7313]
2024-12-18 21:45:02.719660: Epoch time: 390.02 s
2024-12-18 21:45:02.720858: Yayy! New best EMA pseudo Dice: 0.7237
2024-12-18 21:45:04.487443: 
2024-12-18 21:45:04.488839: Epoch 91
2024-12-18 21:45:04.489753: Current learning rate: 0.00432
2024-12-18 21:51:27.730548: Validation loss did not improve from -0.52816. Patience: 33/50
2024-12-18 21:51:27.732601: train_loss -0.7834
2024-12-18 21:51:27.733683: val_loss -0.4892
2024-12-18 21:51:27.734338: Pseudo dice [0.7174]
2024-12-18 21:51:27.735102: Epoch time: 383.25 s
2024-12-18 21:51:29.220227: 
2024-12-18 21:51:29.222876: Epoch 92
2024-12-18 21:51:29.224025: Current learning rate: 0.00425
2024-12-18 21:58:00.620055: Validation loss did not improve from -0.52816. Patience: 34/50
2024-12-18 21:58:00.621597: train_loss -0.785
2024-12-18 21:58:00.622671: val_loss -0.4891
2024-12-18 21:58:00.623702: Pseudo dice [0.7274]
2024-12-18 21:58:00.624709: Epoch time: 391.4 s
2024-12-18 21:58:01.968450: 
2024-12-18 21:58:01.969888: Epoch 93
2024-12-18 21:58:01.971228: Current learning rate: 0.00419
2024-12-18 22:04:52.946283: Validation loss did not improve from -0.52816. Patience: 35/50
2024-12-18 22:04:52.947420: train_loss -0.7841
2024-12-18 22:04:52.948235: val_loss -0.4493
2024-12-18 22:04:52.949179: Pseudo dice [0.6979]
2024-12-18 22:04:52.950323: Epoch time: 410.98 s
2024-12-18 22:04:54.339804: 
2024-12-18 22:04:54.341129: Epoch 94
2024-12-18 22:04:54.341868: Current learning rate: 0.00412
2024-12-18 22:12:13.084637: Validation loss did not improve from -0.52816. Patience: 36/50
2024-12-18 22:12:13.085646: train_loss -0.7824
2024-12-18 22:12:13.086995: val_loss -0.4768
2024-12-18 22:12:13.087868: Pseudo dice [0.7185]
2024-12-18 22:12:13.088692: Epoch time: 438.75 s
2024-12-18 22:12:15.830314: 
2024-12-18 22:12:15.831947: Epoch 95
2024-12-18 22:12:15.833140: Current learning rate: 0.00405
2024-12-18 22:18:53.823662: Validation loss did not improve from -0.52816. Patience: 37/50
2024-12-18 22:18:53.824545: train_loss -0.786
2024-12-18 22:18:53.825809: val_loss -0.5096
2024-12-18 22:18:53.827440: Pseudo dice [0.7266]
2024-12-18 22:18:53.828896: Epoch time: 398.0 s
2024-12-18 22:18:55.154710: 
2024-12-18 22:18:55.156058: Epoch 96
2024-12-18 22:18:55.157079: Current learning rate: 0.00399
2024-12-18 22:25:37.217184: Validation loss did not improve from -0.52816. Patience: 38/50
2024-12-18 22:25:37.218193: train_loss -0.7857
2024-12-18 22:25:37.218996: val_loss -0.5038
2024-12-18 22:25:37.219713: Pseudo dice [0.7314]
2024-12-18 22:25:37.220385: Epoch time: 402.06 s
2024-12-18 22:25:38.579500: 
2024-12-18 22:25:38.580770: Epoch 97
2024-12-18 22:25:38.581413: Current learning rate: 0.00392
2024-12-18 22:32:27.085840: Validation loss did not improve from -0.52816. Patience: 39/50
2024-12-18 22:32:27.086911: train_loss -0.7863
2024-12-18 22:32:27.087924: val_loss -0.4814
2024-12-18 22:32:27.088982: Pseudo dice [0.7151]
2024-12-18 22:32:27.089976: Epoch time: 408.51 s
2024-12-18 22:32:28.465776: 
2024-12-18 22:32:28.467106: Epoch 98
2024-12-18 22:32:28.467961: Current learning rate: 0.00385
2024-12-18 22:39:32.485727: Validation loss did not improve from -0.52816. Patience: 40/50
2024-12-18 22:39:32.486592: train_loss -0.7891
2024-12-18 22:39:32.487517: val_loss -0.4953
2024-12-18 22:39:32.488361: Pseudo dice [0.7253]
2024-12-18 22:39:32.489500: Epoch time: 424.02 s
2024-12-18 22:39:33.863515: 
2024-12-18 22:39:33.864646: Epoch 99
2024-12-18 22:39:33.865684: Current learning rate: 0.00379
2024-12-18 22:46:17.865300: Validation loss did not improve from -0.52816. Patience: 41/50
2024-12-18 22:46:17.866476: train_loss -0.7876
2024-12-18 22:46:17.867427: val_loss -0.4618
2024-12-18 22:46:17.868124: Pseudo dice [0.7099]
2024-12-18 22:46:17.869000: Epoch time: 404.0 s
2024-12-18 22:46:19.622207: 
2024-12-18 22:46:19.623259: Epoch 100
2024-12-18 22:46:19.624009: Current learning rate: 0.00372
2024-12-18 22:53:07.118599: Validation loss did not improve from -0.52816. Patience: 42/50
2024-12-18 22:53:07.121899: train_loss -0.7885
2024-12-18 22:53:07.124303: val_loss -0.5108
2024-12-18 22:53:07.125746: Pseudo dice [0.7383]
2024-12-18 22:53:07.127595: Epoch time: 407.5 s
2024-12-18 22:53:08.491991: 
2024-12-18 22:53:08.493459: Epoch 101
2024-12-18 22:53:08.494282: Current learning rate: 0.00365
2024-12-18 23:00:28.865979: Validation loss did not improve from -0.52816. Patience: 43/50
2024-12-18 23:00:28.867267: train_loss -0.7884
2024-12-18 23:00:28.868102: val_loss -0.5027
2024-12-18 23:00:28.868747: Pseudo dice [0.7282]
2024-12-18 23:00:28.869542: Epoch time: 440.38 s
2024-12-18 23:00:30.317328: 
2024-12-18 23:00:30.318806: Epoch 102
2024-12-18 23:00:30.320040: Current learning rate: 0.00359
2024-12-18 23:07:10.204006: Validation loss did not improve from -0.52816. Patience: 44/50
2024-12-18 23:07:10.227764: train_loss -0.7878
2024-12-18 23:07:10.229249: val_loss -0.5037
2024-12-18 23:07:10.230001: Pseudo dice [0.7284]
2024-12-18 23:07:10.230818: Epoch time: 399.91 s
2024-12-18 23:07:11.710668: 
2024-12-18 23:07:11.712673: Epoch 103
2024-12-18 23:07:11.713702: Current learning rate: 0.00352
2024-12-18 23:13:34.734472: Validation loss did not improve from -0.52816. Patience: 45/50
2024-12-18 23:13:34.735434: train_loss -0.7901
2024-12-18 23:13:34.736164: val_loss -0.5161
2024-12-18 23:13:34.736835: Pseudo dice [0.7352]
2024-12-18 23:13:34.737628: Epoch time: 383.03 s
2024-12-18 23:13:34.738349: Yayy! New best EMA pseudo Dice: 0.7248
2024-12-18 23:13:36.728086: 
2024-12-18 23:13:36.729453: Epoch 104
2024-12-18 23:13:36.730411: Current learning rate: 0.00345
2024-12-18 23:20:21.722275: Validation loss did not improve from -0.52816. Patience: 46/50
2024-12-18 23:20:21.723452: train_loss -0.7893
2024-12-18 23:20:21.724283: val_loss -0.4592
2024-12-18 23:20:21.724955: Pseudo dice [0.7058]
2024-12-18 23:20:21.725741: Epoch time: 405.0 s
2024-12-18 23:20:23.632429: 
2024-12-18 23:20:23.634195: Epoch 105
2024-12-18 23:20:23.634970: Current learning rate: 0.00338
2024-12-18 23:27:28.737370: Validation loss did not improve from -0.52816. Patience: 47/50
2024-12-18 23:27:28.738397: train_loss -0.7902
2024-12-18 23:27:28.739702: val_loss -0.4974
2024-12-18 23:27:28.740330: Pseudo dice [0.7257]
2024-12-18 23:27:28.741301: Epoch time: 425.11 s
2024-12-18 23:27:31.405360: 
2024-12-18 23:27:31.406514: Epoch 106
2024-12-18 23:27:31.407606: Current learning rate: 0.00332
2024-12-18 23:34:34.672240: Validation loss did not improve from -0.52816. Patience: 48/50
2024-12-18 23:34:34.673230: train_loss -0.7942
2024-12-18 23:34:34.674064: val_loss -0.4876
2024-12-18 23:34:34.675054: Pseudo dice [0.7284]
2024-12-18 23:34:34.675835: Epoch time: 423.27 s
2024-12-18 23:34:36.030000: 
2024-12-18 23:34:36.031480: Epoch 107
2024-12-18 23:34:36.032580: Current learning rate: 0.00325
2024-12-18 23:41:44.305673: Validation loss did not improve from -0.52816. Patience: 49/50
2024-12-18 23:41:44.306724: train_loss -0.7937
2024-12-18 23:41:44.307674: val_loss -0.4856
2024-12-18 23:41:44.308412: Pseudo dice [0.7192]
2024-12-18 23:41:44.309166: Epoch time: 428.28 s
2024-12-18 23:41:45.658916: 
2024-12-18 23:41:45.660295: Epoch 108
2024-12-18 23:41:45.661057: Current learning rate: 0.00318
2024-12-18 23:48:32.386675: Validation loss did not improve from -0.52816. Patience: 50/50
2024-12-18 23:48:32.387738: train_loss -0.7942
2024-12-18 23:48:32.388554: val_loss -0.5258
2024-12-18 23:48:32.389340: Pseudo dice [0.7376]
2024-12-18 23:48:32.389977: Epoch time: 406.73 s
2024-12-18 23:48:33.765787: 
2024-12-18 23:48:33.767048: Epoch 109
2024-12-18 23:48:33.767878: Current learning rate: 0.00311
2024-12-18 23:55:07.392097: Validation loss did not improve from -0.52816. Patience: 51/50
2024-12-18 23:55:07.395480: train_loss -0.7945
2024-12-18 23:55:07.396768: val_loss -0.4781
2024-12-18 23:55:07.397482: Pseudo dice [0.7216]
2024-12-18 23:55:07.398374: Epoch time: 393.63 s
2024-12-18 23:55:09.190422: 
2024-12-18 23:55:09.191620: Epoch 110
2024-12-18 23:55:09.192394: Current learning rate: 0.00304
2024-12-19 00:02:22.986827: Validation loss did not improve from -0.52816. Patience: 52/50
2024-12-19 00:02:22.987713: train_loss -0.791
2024-12-19 00:02:22.989178: val_loss -0.4627
2024-12-19 00:02:22.989980: Pseudo dice [0.7108]
2024-12-19 00:02:22.991027: Epoch time: 433.8 s
2024-12-19 00:02:24.337837: 
2024-12-19 00:02:24.339207: Epoch 111
2024-12-19 00:02:24.339878: Current learning rate: 0.00297
2024-12-19 00:09:34.817736: Validation loss did not improve from -0.52816. Patience: 53/50
2024-12-19 00:09:34.819362: train_loss -0.7948
2024-12-19 00:09:34.820258: val_loss -0.4776
2024-12-19 00:09:34.821039: Pseudo dice [0.7101]
2024-12-19 00:09:34.821742: Epoch time: 430.48 s
2024-12-19 00:09:36.276366: 
2024-12-19 00:09:36.277786: Epoch 112
2024-12-19 00:09:36.279230: Current learning rate: 0.00291
2024-12-19 00:16:42.828857: Validation loss did not improve from -0.52816. Patience: 54/50
2024-12-19 00:16:42.830416: train_loss -0.7964
2024-12-19 00:16:42.831944: val_loss -0.5047
2024-12-19 00:16:42.833071: Pseudo dice [0.7322]
2024-12-19 00:16:42.834562: Epoch time: 426.56 s
2024-12-19 00:16:44.294191: 
2024-12-19 00:16:44.295733: Epoch 113
2024-12-19 00:16:44.296735: Current learning rate: 0.00284
2024-12-19 00:23:41.975485: Validation loss did not improve from -0.52816. Patience: 55/50
2024-12-19 00:23:41.976504: train_loss -0.7946
2024-12-19 00:23:41.977394: val_loss -0.5012
2024-12-19 00:23:41.978082: Pseudo dice [0.7278]
2024-12-19 00:23:41.978835: Epoch time: 417.68 s
2024-12-19 00:23:43.330258: 
2024-12-19 00:23:43.332083: Epoch 114
2024-12-19 00:23:43.332920: Current learning rate: 0.00277
2024-12-19 00:30:41.890390: Validation loss did not improve from -0.52816. Patience: 56/50
2024-12-19 00:30:41.891306: train_loss -0.7957
2024-12-19 00:30:41.892468: val_loss -0.4679
2024-12-19 00:30:41.893180: Pseudo dice [0.7171]
2024-12-19 00:30:41.893843: Epoch time: 418.56 s
2024-12-19 00:30:43.704059: 
2024-12-19 00:30:43.705584: Epoch 115
2024-12-19 00:30:43.706397: Current learning rate: 0.0027
2024-12-19 00:37:47.129863: Validation loss did not improve from -0.52816. Patience: 57/50
2024-12-19 00:37:47.131298: train_loss -0.7995
2024-12-19 00:37:47.132582: val_loss -0.459
2024-12-19 00:37:47.133578: Pseudo dice [0.7169]
2024-12-19 00:37:47.134648: Epoch time: 423.43 s
2024-12-19 00:37:48.511639: 
2024-12-19 00:37:48.512911: Epoch 116
2024-12-19 00:37:48.513670: Current learning rate: 0.00263
2024-12-19 00:44:40.848542: Validation loss did not improve from -0.52816. Patience: 58/50
2024-12-19 00:44:40.849729: train_loss -0.7977
2024-12-19 00:44:40.850559: val_loss -0.4452
2024-12-19 00:44:40.851340: Pseudo dice [0.7057]
2024-12-19 00:44:40.852049: Epoch time: 412.34 s
2024-12-19 00:44:42.252387: 
2024-12-19 00:44:42.253744: Epoch 117
2024-12-19 00:44:42.254566: Current learning rate: 0.00256
2024-12-19 00:51:21.676440: Validation loss did not improve from -0.52816. Patience: 59/50
2024-12-19 00:51:21.677393: train_loss -0.7973
2024-12-19 00:51:21.678307: val_loss -0.4679
2024-12-19 00:51:21.679362: Pseudo dice [0.72]
2024-12-19 00:51:21.679949: Epoch time: 399.43 s
2024-12-19 00:51:24.028433: 
2024-12-19 00:51:24.029874: Epoch 118
2024-12-19 00:51:24.030574: Current learning rate: 0.00249
2024-12-19 00:57:56.975033: Validation loss did not improve from -0.52816. Patience: 60/50
2024-12-19 00:57:56.975944: train_loss -0.7976
2024-12-19 00:57:56.976629: val_loss -0.4743
2024-12-19 00:57:56.977346: Pseudo dice [0.7192]
2024-12-19 00:57:56.978231: Epoch time: 392.95 s
2024-12-19 00:57:58.357588: 
2024-12-19 00:57:58.359093: Epoch 119
2024-12-19 00:57:58.359959: Current learning rate: 0.00242
2024-12-19 01:05:01.547719: Validation loss did not improve from -0.52816. Patience: 61/50
2024-12-19 01:05:01.579821: train_loss -0.8036
2024-12-19 01:05:01.583373: val_loss -0.4504
2024-12-19 01:05:01.584270: Pseudo dice [0.7054]
2024-12-19 01:05:01.585672: Epoch time: 423.22 s
2024-12-19 01:05:03.467710: 
2024-12-19 01:05:03.469117: Epoch 120
2024-12-19 01:05:03.470015: Current learning rate: 0.00235
2024-12-19 01:11:52.477218: Validation loss did not improve from -0.52816. Patience: 62/50
2024-12-19 01:11:52.478815: train_loss -0.8017
2024-12-19 01:11:52.479565: val_loss -0.4866
2024-12-19 01:11:52.480328: Pseudo dice [0.7272]
2024-12-19 01:11:52.481153: Epoch time: 409.01 s
2024-12-19 01:11:53.856761: 
2024-12-19 01:11:53.857989: Epoch 121
2024-12-19 01:11:53.858714: Current learning rate: 0.00228
2024-12-19 01:18:37.247489: Validation loss did not improve from -0.52816. Patience: 63/50
2024-12-19 01:18:37.248727: train_loss -0.8009
2024-12-19 01:18:37.249756: val_loss -0.4708
2024-12-19 01:18:37.250724: Pseudo dice [0.7174]
2024-12-19 01:18:37.251513: Epoch time: 403.39 s
2024-12-19 01:18:38.642629: 
2024-12-19 01:18:38.644212: Epoch 122
2024-12-19 01:18:38.645418: Current learning rate: 0.00221
2024-12-19 01:25:24.028598: Validation loss did not improve from -0.52816. Patience: 64/50
2024-12-19 01:25:24.029574: train_loss -0.7992
2024-12-19 01:25:24.030787: val_loss -0.4746
2024-12-19 01:25:24.031784: Pseudo dice [0.7203]
2024-12-19 01:25:24.032726: Epoch time: 405.39 s
2024-12-19 01:25:25.413124: 
2024-12-19 01:25:25.414788: Epoch 123
2024-12-19 01:25:25.416054: Current learning rate: 0.00214
2024-12-19 01:32:24.485559: Validation loss did not improve from -0.52816. Patience: 65/50
2024-12-19 01:32:24.486592: train_loss -0.7998
2024-12-19 01:32:24.487643: val_loss -0.4759
2024-12-19 01:32:24.488433: Pseudo dice [0.7288]
2024-12-19 01:32:24.489338: Epoch time: 419.07 s
2024-12-19 01:32:25.991873: 
2024-12-19 01:32:25.993227: Epoch 124
2024-12-19 01:32:25.994030: Current learning rate: 0.00207
2024-12-19 01:39:28.544239: Validation loss did not improve from -0.52816. Patience: 66/50
2024-12-19 01:39:28.545243: train_loss -0.8006
2024-12-19 01:39:28.546223: val_loss -0.4655
2024-12-19 01:39:28.547070: Pseudo dice [0.7148]
2024-12-19 01:39:28.548096: Epoch time: 422.55 s
2024-12-19 01:39:30.574583: 
2024-12-19 01:39:30.575958: Epoch 125
2024-12-19 01:39:30.576775: Current learning rate: 0.00199
2024-12-19 01:46:33.102023: Validation loss did not improve from -0.52816. Patience: 67/50
2024-12-19 01:46:33.103102: train_loss -0.8026
2024-12-19 01:46:33.103916: val_loss -0.5088
2024-12-19 01:46:33.104675: Pseudo dice [0.7325]
2024-12-19 01:46:33.105525: Epoch time: 422.53 s
2024-12-19 01:46:34.505579: 
2024-12-19 01:46:34.506980: Epoch 126
2024-12-19 01:46:34.507699: Current learning rate: 0.00192
2024-12-19 01:52:44.287341: Validation loss did not improve from -0.52816. Patience: 68/50
2024-12-19 01:52:44.288309: train_loss -0.802
2024-12-19 01:52:44.289091: val_loss -0.4811
2024-12-19 01:52:44.289816: Pseudo dice [0.7167]
2024-12-19 01:52:44.290675: Epoch time: 369.78 s
2024-12-19 01:52:45.665267: 
2024-12-19 01:52:45.666229: Epoch 127
2024-12-19 01:52:45.667274: Current learning rate: 0.00185
2024-12-19 01:59:30.054028: Validation loss did not improve from -0.52816. Patience: 69/50
2024-12-19 01:59:30.055158: train_loss -0.8011
2024-12-19 01:59:30.056267: val_loss -0.4672
2024-12-19 01:59:30.057252: Pseudo dice [0.7202]
2024-12-19 01:59:30.058228: Epoch time: 404.39 s
2024-12-19 01:59:32.768177: 
2024-12-19 01:59:32.770058: Epoch 128
2024-12-19 01:59:32.771041: Current learning rate: 0.00178
2024-12-19 02:06:32.691241: Validation loss did not improve from -0.52816. Patience: 70/50
2024-12-19 02:06:32.695029: train_loss -0.8026
2024-12-19 02:06:32.696278: val_loss -0.4849
2024-12-19 02:06:32.696958: Pseudo dice [0.7235]
2024-12-19 02:06:32.697750: Epoch time: 419.93 s
2024-12-19 02:06:34.107858: 
2024-12-19 02:06:34.109358: Epoch 129
2024-12-19 02:06:34.110083: Current learning rate: 0.0017
2024-12-19 02:13:14.705812: Validation loss did not improve from -0.52816. Patience: 71/50
2024-12-19 02:13:14.707237: train_loss -0.8071
2024-12-19 02:13:14.708795: val_loss -0.4582
2024-12-19 02:13:14.709876: Pseudo dice [0.7074]
2024-12-19 02:13:14.710705: Epoch time: 400.6 s
2024-12-19 02:13:16.584176: 
2024-12-19 02:13:16.585515: Epoch 130
2024-12-19 02:13:16.586744: Current learning rate: 0.00163
2024-12-19 02:19:44.403223: Validation loss did not improve from -0.52816. Patience: 72/50
2024-12-19 02:19:44.404701: train_loss -0.8039
2024-12-19 02:19:44.405395: val_loss -0.4666
2024-12-19 02:19:44.406003: Pseudo dice [0.725]
2024-12-19 02:19:44.406622: Epoch time: 387.82 s
2024-12-19 02:19:45.816617: 
2024-12-19 02:19:45.817928: Epoch 131
2024-12-19 02:19:45.818823: Current learning rate: 0.00156
2024-12-19 02:26:33.171314: Validation loss did not improve from -0.52816. Patience: 73/50
2024-12-19 02:26:33.172204: train_loss -0.804
2024-12-19 02:26:33.173115: val_loss -0.4588
2024-12-19 02:26:33.173989: Pseudo dice [0.7145]
2024-12-19 02:26:33.174738: Epoch time: 407.36 s
2024-12-19 02:26:34.533628: 
2024-12-19 02:26:34.535249: Epoch 132
2024-12-19 02:26:34.536476: Current learning rate: 0.00148
2024-12-19 02:33:23.337092: Validation loss did not improve from -0.52816. Patience: 74/50
2024-12-19 02:33:23.337861: train_loss -0.8023
2024-12-19 02:33:23.338963: val_loss -0.4955
2024-12-19 02:33:23.339828: Pseudo dice [0.7291]
2024-12-19 02:33:23.340864: Epoch time: 408.81 s
2024-12-19 02:33:24.694352: 
2024-12-19 02:33:24.695725: Epoch 133
2024-12-19 02:33:24.696620: Current learning rate: 0.00141
2024-12-19 02:40:23.417048: Validation loss did not improve from -0.52816. Patience: 75/50
2024-12-19 02:40:23.418012: train_loss -0.8076
2024-12-19 02:40:23.418991: val_loss -0.5064
2024-12-19 02:40:23.419759: Pseudo dice [0.7323]
2024-12-19 02:40:23.420566: Epoch time: 418.72 s
2024-12-19 02:40:24.826617: 
2024-12-19 02:40:24.828042: Epoch 134
2024-12-19 02:40:24.829629: Current learning rate: 0.00133
2024-12-19 02:47:21.839967: Validation loss did not improve from -0.52816. Patience: 76/50
2024-12-19 02:47:21.840855: train_loss -0.8067
2024-12-19 02:47:21.841587: val_loss -0.4802
2024-12-19 02:47:21.842238: Pseudo dice [0.7207]
2024-12-19 02:47:21.842875: Epoch time: 417.02 s
2024-12-19 02:47:23.652804: 
2024-12-19 02:47:23.654279: Epoch 135
2024-12-19 02:47:23.655584: Current learning rate: 0.00126
2024-12-19 02:54:39.127069: Validation loss did not improve from -0.52816. Patience: 77/50
2024-12-19 02:54:39.128042: train_loss -0.8052
2024-12-19 02:54:39.131378: val_loss -0.4451
2024-12-19 02:54:39.132141: Pseudo dice [0.6932]
2024-12-19 02:54:39.133457: Epoch time: 435.48 s
2024-12-19 02:54:40.527420: 
2024-12-19 02:54:40.528647: Epoch 136
2024-12-19 02:54:40.529655: Current learning rate: 0.00118
2024-12-19 03:01:41.548379: Validation loss did not improve from -0.52816. Patience: 78/50
2024-12-19 03:01:41.549214: train_loss -0.8067
2024-12-19 03:01:41.550144: val_loss -0.4768
2024-12-19 03:01:41.550790: Pseudo dice [0.7265]
2024-12-19 03:01:41.551397: Epoch time: 421.02 s
2024-12-19 03:01:42.977126: 
2024-12-19 03:01:42.978104: Epoch 137
2024-12-19 03:01:42.978750: Current learning rate: 0.00111
2024-12-19 03:08:41.582223: Validation loss did not improve from -0.52816. Patience: 79/50
2024-12-19 03:08:41.584454: train_loss -0.8056
2024-12-19 03:08:41.585943: val_loss -0.4526
2024-12-19 03:08:41.586644: Pseudo dice [0.7083]
2024-12-19 03:08:41.587587: Epoch time: 418.61 s
2024-12-19 03:08:42.986182: 
2024-12-19 03:08:42.987430: Epoch 138
2024-12-19 03:08:42.988044: Current learning rate: 0.00103
2024-12-19 03:15:37.448661: Validation loss did not improve from -0.52816. Patience: 80/50
2024-12-19 03:15:37.463107: train_loss -0.8062
2024-12-19 03:15:37.464710: val_loss -0.5041
2024-12-19 03:15:37.465646: Pseudo dice [0.7316]
2024-12-19 03:15:37.466417: Epoch time: 414.48 s
2024-12-19 03:15:38.846543: 
2024-12-19 03:15:38.847713: Epoch 139
2024-12-19 03:15:38.848399: Current learning rate: 0.00095
2024-12-19 03:22:38.369296: Validation loss did not improve from -0.52816. Patience: 81/50
2024-12-19 03:22:38.370661: train_loss -0.8089
2024-12-19 03:22:38.371607: val_loss -0.4731
2024-12-19 03:22:38.372356: Pseudo dice [0.7152]
2024-12-19 03:22:38.373036: Epoch time: 419.53 s
2024-12-19 03:22:41.530806: 
2024-12-19 03:22:41.532140: Epoch 140
2024-12-19 03:22:41.532848: Current learning rate: 0.00087
2024-12-19 03:29:14.504517: Validation loss did not improve from -0.52816. Patience: 82/50
2024-12-19 03:29:14.505417: train_loss -0.8065
2024-12-19 03:29:14.506228: val_loss -0.4583
2024-12-19 03:29:14.506933: Pseudo dice [0.717]
2024-12-19 03:29:14.507627: Epoch time: 392.98 s
2024-12-19 03:29:15.955978: 
2024-12-19 03:29:15.957435: Epoch 141
2024-12-19 03:29:15.958202: Current learning rate: 0.00079
2024-12-19 03:36:25.347904: Validation loss did not improve from -0.52816. Patience: 83/50
2024-12-19 03:36:25.349382: train_loss -0.8083
2024-12-19 03:36:25.350643: val_loss -0.4623
2024-12-19 03:36:25.351831: Pseudo dice [0.7123]
2024-12-19 03:36:25.352851: Epoch time: 429.39 s
2024-12-19 03:36:26.794978: 
2024-12-19 03:36:26.796728: Epoch 142
2024-12-19 03:36:26.798124: Current learning rate: 0.00071
2024-12-19 03:43:14.289134: Validation loss did not improve from -0.52816. Patience: 84/50
2024-12-19 03:43:14.315389: train_loss -0.8075
2024-12-19 03:43:14.316427: val_loss -0.4849
2024-12-19 03:43:14.317052: Pseudo dice [0.7216]
2024-12-19 03:43:14.317803: Epoch time: 407.52 s
2024-12-19 03:43:15.730880: 
2024-12-19 03:43:15.731992: Epoch 143
2024-12-19 03:43:15.732619: Current learning rate: 0.00063
2024-12-19 03:50:22.227249: Validation loss did not improve from -0.52816. Patience: 85/50
2024-12-19 03:50:22.228268: train_loss -0.8096
2024-12-19 03:50:22.229088: val_loss -0.474
2024-12-19 03:50:22.229784: Pseudo dice [0.7163]
2024-12-19 03:50:22.230542: Epoch time: 426.5 s
2024-12-19 03:50:23.622930: 
2024-12-19 03:50:23.624479: Epoch 144
2024-12-19 03:50:23.625187: Current learning rate: 0.00055
2024-12-19 03:57:03.208417: Validation loss did not improve from -0.52816. Patience: 86/50
2024-12-19 03:57:03.209412: train_loss -0.8119
2024-12-19 03:57:03.210210: val_loss -0.4694
2024-12-19 03:57:03.210829: Pseudo dice [0.7104]
2024-12-19 03:57:03.211416: Epoch time: 399.59 s
2024-12-19 03:57:05.065699: 
2024-12-19 03:57:05.067063: Epoch 145
2024-12-19 03:57:05.068122: Current learning rate: 0.00047
2024-12-19 04:03:43.450149: Validation loss did not improve from -0.52816. Patience: 87/50
2024-12-19 04:03:43.451284: train_loss -0.8086
2024-12-19 04:03:43.452248: val_loss -0.4607
2024-12-19 04:03:43.452975: Pseudo dice [0.7144]
2024-12-19 04:03:43.453636: Epoch time: 398.39 s
2024-12-19 04:03:44.849549: 
2024-12-19 04:03:44.850859: Epoch 146
2024-12-19 04:03:44.851742: Current learning rate: 0.00038
2024-12-19 04:10:42.599419: Validation loss did not improve from -0.52816. Patience: 88/50
2024-12-19 04:10:42.600643: train_loss -0.8135
2024-12-19 04:10:42.601648: val_loss -0.4809
2024-12-19 04:10:42.602681: Pseudo dice [0.7214]
2024-12-19 04:10:42.603505: Epoch time: 417.75 s
2024-12-19 04:10:44.068632: 
2024-12-19 04:10:44.070164: Epoch 147
2024-12-19 04:10:44.071096: Current learning rate: 0.0003
2024-12-19 04:17:26.200514: Validation loss did not improve from -0.52816. Patience: 89/50
2024-12-19 04:17:26.203919: train_loss -0.811
2024-12-19 04:17:26.204982: val_loss -0.4681
2024-12-19 04:17:26.205904: Pseudo dice [0.716]
2024-12-19 04:17:26.206695: Epoch time: 402.14 s
2024-12-19 04:17:27.629179: 
2024-12-19 04:17:27.630302: Epoch 148
2024-12-19 04:17:27.631206: Current learning rate: 0.00021
2024-12-19 04:23:40.534764: Validation loss did not improve from -0.52816. Patience: 90/50
2024-12-19 04:23:40.535934: train_loss -0.8092
2024-12-19 04:23:40.537713: val_loss -0.4753
2024-12-19 04:23:40.538761: Pseudo dice [0.7231]
2024-12-19 04:23:40.540036: Epoch time: 372.91 s
2024-12-19 04:23:41.928076: 
2024-12-19 04:23:41.929752: Epoch 149
2024-12-19 04:23:41.930920: Current learning rate: 0.00011
2024-12-19 04:30:26.811595: Validation loss did not improve from -0.52816. Patience: 91/50
2024-12-19 04:30:26.812605: train_loss -0.8089
2024-12-19 04:30:26.813476: val_loss -0.4719
2024-12-19 04:30:26.814122: Pseudo dice [0.7264]
2024-12-19 04:30:26.814739: Epoch time: 404.89 s
2024-12-19 04:30:28.711505: Training done.
2024-12-19 04:30:29.408094: Using splits from existing split file: /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset309_Sohee_Calcium_OCT_CrossValidation/splits_final_80.json
2024-12-19 04:30:29.418093: The split file contains 5 splits.
2024-12-19 04:30:29.419030: Desired fold for training: 4
2024-12-19 04:30:29.419758: This split has 6 training and 5 validation cases.
2024-12-19 04:30:29.420836: predicting 101-044
2024-12-19 04:30:29.482816: 101-044, shape torch.Size([1, 404, 498, 498]), rank 0
2024-12-19 04:33:15.282385: predicting 401-004
2024-12-19 04:33:15.302444: 401-004, shape torch.Size([1, 375, 498, 498]), rank 0
2024-12-19 04:35:40.184388: predicting 701-013
2024-12-19 04:35:40.203523: 701-013, shape torch.Size([1, 375, 498, 498]), rank 0
2024-12-19 04:38:11.143439: predicting 704-003
2024-12-19 04:38:11.159329: 704-003, shape torch.Size([1, 375, 498, 498]), rank 0
2024-12-19 04:40:17.659780: predicting 706-005
2024-12-19 04:40:17.674551: 706-005, shape torch.Size([1, 375, 498, 498]), rank 0
2024-12-19 04:42:57.663548: Validation complete
2024-12-19 04:42:57.664490: Mean Validation Dice:  0.7042298538230215
