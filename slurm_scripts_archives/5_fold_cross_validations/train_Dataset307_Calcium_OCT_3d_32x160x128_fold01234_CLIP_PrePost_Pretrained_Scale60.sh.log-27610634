/home/gridsan/nchutisilp/.conda/envs/nnunet/bin/python
Starting training with CONFIG=3d_32x160x128_b10, DATASET_ID=307, TRAINER=nnUNetTrainerScaleAnalysis60

############################
INFO: You are using the old nnU-Net default plans. We have updated our recommendations. Please consider using those instead! Read more here: https://github.com/MIC-DKFZ/nnUNet/blob/master/documentation/resenc_presets.md
############################

Using device: cuda:0

#######################################################################
Please cite the following paper when using nnU-Net:
Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.
#######################################################################

2024-12-13 18:44:08.183862: Using torch.compile...

############################
INFO: You are using the old nnU-Net default plans. We have updated our recommendations. Please consider using those instead! Read more here: https://github.com/MIC-DKFZ/nnUNet/blob/master/documentation/resenc_presets.md
############################

Using device: cuda:0

#######################################################################
Please cite the following paper when using nnU-Net:
Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.
#######################################################################

2024-12-13 18:44:08.184423: Using torch.compile...
/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
################### Loading pretrained weights from file  /home/gridsan/nchutisilp/projects/OpenAI-CLIP/clip_preivl_poststent_pretrained_nnUNet.pt ###################
Below is the list of overlapping blocks in pretrained model and nnUNet architecture:
encoder.stages.0.0.convs.0.conv.weight shape torch.Size([32, 1, 3, 3, 3])
encoder.stages.0.0.convs.0.conv.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.norm.weight shape torch.Size([32])
encoder.stages.0.0.convs.0.norm.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.0.weight shape torch.Size([32, 1, 3, 3, 3])
encoder.stages.0.0.convs.0.all_modules.0.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.1.weight shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.1.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
encoder.stages.0.0.convs.1.conv.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.norm.weight shape torch.Size([32])
encoder.stages.0.0.convs.1.norm.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
encoder.stages.0.0.convs.1.all_modules.0.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.1.weight shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.1.bias shape torch.Size([32])
encoder.stages.1.0.convs.0.conv.weight shape torch.Size([64, 32, 3, 3, 3])
encoder.stages.1.0.convs.0.conv.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.norm.weight shape torch.Size([64])
encoder.stages.1.0.convs.0.norm.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.0.weight shape torch.Size([64, 32, 3, 3, 3])
encoder.stages.1.0.convs.0.all_modules.0.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.1.weight shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.1.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
encoder.stages.1.0.convs.1.conv.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.norm.weight shape torch.Size([64])
encoder.stages.1.0.convs.1.norm.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
encoder.stages.1.0.convs.1.all_modules.0.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.1.weight shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.1.bias shape torch.Size([64])
encoder.stages.2.0.convs.0.conv.weight shape torch.Size([128, 64, 3, 3, 3])
encoder.stages.2.0.convs.0.conv.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.norm.weight shape torch.Size([128])
encoder.stages.2.0.convs.0.norm.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.0.weight shape torch.Size([128, 64, 3, 3, 3])
encoder.stages.2.0.convs.0.all_modules.0.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.1.weight shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.1.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
encoder.stages.2.0.convs.1.conv.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.norm.weight shape torch.Size([128])
encoder.stages.2.0.convs.1.norm.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
encoder.stages.2.0.convs.1.all_modules.0.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.1.weight shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.1.bias shape torch.Size([128])
encoder.stages.3.0.convs.0.conv.weight shape torch.Size([256, 128, 3, 3, 3])
encoder.stages.3.0.convs.0.conv.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.norm.weight shape torch.Size([256])
encoder.stages.3.0.convs.0.norm.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.0.weight shape torch.Size([256, 128, 3, 3, 3])
encoder.stages.3.0.convs.0.all_modules.0.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.1.weight shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.1.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
encoder.stages.3.0.convs.1.conv.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.norm.weight shape torch.Size([256])
encoder.stages.3.0.convs.1.norm.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
encoder.stages.3.0.convs.1.all_modules.0.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.1.weight shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.1.bias shape torch.Size([256])
encoder.stages.4.0.convs.0.conv.weight shape torch.Size([320, 256, 3, 3, 3])
encoder.stages.4.0.convs.0.conv.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.norm.weight shape torch.Size([320])
encoder.stages.4.0.convs.0.norm.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.0.weight shape torch.Size([320, 256, 3, 3, 3])
encoder.stages.4.0.convs.0.all_modules.0.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.1.weight shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.1.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.4.0.convs.1.conv.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.norm.weight shape torch.Size([320])
encoder.stages.4.0.convs.1.norm.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.4.0.convs.1.all_modules.0.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.1.weight shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.1.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.0.conv.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.norm.weight shape torch.Size([320])
encoder.stages.5.0.convs.0.norm.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.0.all_modules.0.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.1.weight shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.1.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.1.conv.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.norm.weight shape torch.Size([320])
encoder.stages.5.0.convs.1.norm.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.1.all_modules.0.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.1.weight shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.0.0.convs.0.conv.weight shape torch.Size([32, 1, 3, 3, 3])
decoder.encoder.stages.0.0.convs.0.conv.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.norm.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.norm.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.0.weight shape torch.Size([32, 1, 3, 3, 3])
decoder.encoder.stages.0.0.convs.0.all_modules.0.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.1.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.1.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.encoder.stages.0.0.convs.1.conv.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.norm.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.norm.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.encoder.stages.0.0.convs.1.all_modules.0.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.1.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.1.bias shape torch.Size([32])
decoder.encoder.stages.1.0.convs.0.conv.weight shape torch.Size([64, 32, 3, 3, 3])
decoder.encoder.stages.1.0.convs.0.conv.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.norm.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.norm.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.0.weight shape torch.Size([64, 32, 3, 3, 3])
decoder.encoder.stages.1.0.convs.0.all_modules.0.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.1.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.1.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.encoder.stages.1.0.convs.1.conv.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.norm.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.norm.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.encoder.stages.1.0.convs.1.all_modules.0.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.1.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.1.bias shape torch.Size([64])
decoder.encoder.stages.2.0.convs.0.conv.weight shape torch.Size([128, 64, 3, 3, 3])
decoder.encoder.stages.2.0.convs.0.conv.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.norm.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.norm.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.0.weight shape torch.Size([128, 64, 3, 3, 3])
decoder.encoder.stages.2.0.convs.0.all_modules.0.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.1.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.1.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.encoder.stages.2.0.convs.1.conv.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.norm.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.norm.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.encoder.stages.2.0.convs.1.all_modules.0.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.1.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.1.bias shape torch.Size([128])
decoder.encoder.stages.3.0.convs.0.conv.weight shape torch.Size([256, 128, 3, 3, 3])
decoder.encoder.stages.3.0.convs.0.conv.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.norm.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.norm.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.0.weight shape torch.Size([256, 128, 3, 3, 3])
decoder.encoder.stages.3.0.convs.0.all_modules.0.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.1.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.1.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.encoder.stages.3.0.convs.1.conv.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.norm.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.norm.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.encoder.stages.3.0.convs.1.all_modules.0.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.1.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.1.bias shape torch.Size([256])
decoder.encoder.stages.4.0.convs.0.conv.weight shape torch.Size([320, 256, 3, 3, 3])
decoder.encoder.stages.4.0.convs.0.conv.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.norm.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.norm.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.0.weight shape torch.Size([320, 256, 3, 3, 3])
decoder.encoder.stages.4.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.4.0.convs.1.conv.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.norm.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.norm.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.4.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.0.conv.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.norm.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.norm.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.1.conv.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.norm.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.norm.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.stages.0.convs.0.conv.weight shape torch.Size([320, 640, 3, 3, 3])
decoder.stages.0.convs.0.conv.bias shape torch.Size([320])
decoder.stages.0.convs.0.norm.weight shape torch.Size([320])
decoder.stages.0.convs.0.norm.bias shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.0.weight shape torch.Size([320, 640, 3, 3, 3])
decoder.stages.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.stages.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.stages.0.convs.1.conv.bias shape torch.Size([320])
decoder.stages.0.convs.1.norm.weight shape torch.Size([320])
decoder.stages.0.convs.1.norm.bias shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.stages.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.stages.1.convs.0.conv.weight shape torch.Size([256, 512, 3, 3, 3])
decoder.stages.1.convs.0.conv.bias shape torch.Size([256])
decoder.stages.1.convs.0.norm.weight shape torch.Size([256])
decoder.stages.1.convs.0.norm.bias shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.0.weight shape torch.Size([256, 512, 3, 3, 3])
decoder.stages.1.convs.0.all_modules.0.bias shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.1.weight shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.1.bias shape torch.Size([256])
decoder.stages.1.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.stages.1.convs.1.conv.bias shape torch.Size([256])
decoder.stages.1.convs.1.norm.weight shape torch.Size([256])
decoder.stages.1.convs.1.norm.bias shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.stages.1.convs.1.all_modules.0.bias shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.1.weight shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.1.bias shape torch.Size([256])
decoder.stages.2.convs.0.conv.weight shape torch.Size([128, 256, 3, 3, 3])
decoder.stages.2.convs.0.conv.bias shape torch.Size([128])
decoder.stages.2.convs.0.norm.weight shape torch.Size([128])
decoder.stages.2.convs.0.norm.bias shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.0.weight shape torch.Size([128, 256, 3, 3, 3])
decoder.stages.2.convs.0.all_modules.0.bias shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.1.weight shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.1.bias shape torch.Size([128])
decoder.stages.2.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.stages.2.convs.1.conv.bias shape torch.Size([128])
decoder.stages.2.convs.1.norm.weight shape torch.Size([128])
decoder.stages.2.convs.1.norm.bias shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.stages.2.convs.1.all_modules.0.bias shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.1.weight shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.1.bias shape torch.Size([128])
decoder.stages.3.convs.0.conv.weight shape torch.Size([64, 128, 3, 3, 3])
decoder.stages.3.convs.0.conv.bias shape torch.Size([64])
decoder.stages.3.convs.0.norm.weight shape torch.Size([64])
decoder.stages.3.convs.0.norm.bias shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.0.weight shape torch.Size([64, 128, 3, 3, 3])
decoder.stages.3.convs.0.all_modules.0.bias shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.1.weight shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.1.bias shape torch.Size([64])
decoder.stages.3.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.stages.3.convs.1.conv.bias shape torch.Size([64])
decoder.stages.3.convs.1.norm.weight shape torch.Size([64])
decoder.stages.3.convs.1.norm.bias shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.stages.3.convs.1.all_modules.0.bias shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.1.weight shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.1.bias shape torch.Size([64])
decoder.stages.4.convs.0.conv.weight shape torch.Size([32, 64, 3, 3, 3])
decoder.stages.4.convs.0.conv.bias shape torch.Size([32])
decoder.stages.4.convs.0.norm.weight shape torch.Size([32])
decoder.stages.4.convs.0.norm.bias shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.0.weight shape torch.Size([32, 64, 3, 3, 3])
decoder.stages.4.convs.0.all_modules.0.bias shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.1.weight shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.1.bias shape torch.Size([32])
decoder.stages.4.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.stages.4.convs.1.conv.bias shape torch.Size([32])
decoder.stages.4.convs.1.norm.weight shape torch.Size([32])
decoder.stages.4.convs.1.norm.bias shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.stages.4.convs.1.all_modules.0.bias shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.1.weight shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.1.bias shape torch.Size([32])
decoder.transpconvs.0.weight shape torch.Size([320, 320, 1, 2, 2])
decoder.transpconvs.0.bias shape torch.Size([320])
decoder.transpconvs.1.weight shape torch.Size([320, 256, 2, 2, 2])
decoder.transpconvs.1.bias shape torch.Size([256])
decoder.transpconvs.2.weight shape torch.Size([256, 128, 2, 2, 2])
decoder.transpconvs.2.bias shape torch.Size([128])
decoder.transpconvs.3.weight shape torch.Size([128, 64, 2, 2, 2])
decoder.transpconvs.3.bias shape torch.Size([64])
decoder.transpconvs.4.weight shape torch.Size([64, 32, 2, 2, 2])
decoder.transpconvs.4.bias shape torch.Size([32])
################### Done ###################
2024-12-13 18:44:11.076361: do_dummy_2d_data_aug: True
2024-12-13 18:44:11.078295: /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset307_Sohee_Calcium_OCT_CrossValidation/nnUNetPlans_3d_fullres
2024-12-13 18:44:11.081312: Creating new 5-fold cross-validation split...
2024-12-13 18:44:11.089362: Desired fold for training: 1
2024-12-13 18:44:11.090466: This split has 4 training and 5 validation cases.
################### Loading pretrained weights from file  /home/gridsan/nchutisilp/projects/OpenAI-CLIP/clip_preivl_poststent_pretrained_nnUNet.pt ###################
Below is the list of overlapping blocks in pretrained model and nnUNet architecture:
encoder.stages.0.0.convs.0.conv.weight shape torch.Size([32, 1, 3, 3, 3])
encoder.stages.0.0.convs.0.conv.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.norm.weight shape torch.Size([32])
encoder.stages.0.0.convs.0.norm.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.0.weight shape torch.Size([32, 1, 3, 3, 3])
encoder.stages.0.0.convs.0.all_modules.0.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.1.weight shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.1.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
encoder.stages.0.0.convs.1.conv.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.norm.weight shape torch.Size([32])
encoder.stages.0.0.convs.1.norm.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
encoder.stages.0.0.convs.1.all_modules.0.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.1.weight shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.1.bias shape torch.Size([32])
encoder.stages.1.0.convs.0.conv.weight shape torch.Size([64, 32, 3, 3, 3])
encoder.stages.1.0.convs.0.conv.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.norm.weight shape torch.Size([64])
encoder.stages.1.0.convs.0.norm.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.0.weight shape torch.Size([64, 32, 3, 3, 3])
encoder.stages.1.0.convs.0.all_modules.0.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.1.weight shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.1.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
encoder.stages.1.0.convs.1.conv.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.norm.weight shape torch.Size([64])
encoder.stages.1.0.convs.1.norm.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
encoder.stages.1.0.convs.1.all_modules.0.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.1.weight shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.1.bias shape torch.Size([64])
encoder.stages.2.0.convs.0.conv.weight shape torch.Size([128, 64, 3, 3, 3])
encoder.stages.2.0.convs.0.conv.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.norm.weight shape torch.Size([128])
encoder.stages.2.0.convs.0.norm.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.0.weight shape torch.Size([128, 64, 3, 3, 3])
encoder.stages.2.0.convs.0.all_modules.0.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.1.weight shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.1.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
encoder.stages.2.0.convs.1.conv.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.norm.weight shape torch.Size([128])
encoder.stages.2.0.convs.1.norm.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
encoder.stages.2.0.convs.1.all_modules.0.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.1.weight shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.1.bias shape torch.Size([128])
encoder.stages.3.0.convs.0.conv.weight shape torch.Size([256, 128, 3, 3, 3])
encoder.stages.3.0.convs.0.conv.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.norm.weight shape torch.Size([256])
encoder.stages.3.0.convs.0.norm.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.0.weight shape torch.Size([256, 128, 3, 3, 3])
encoder.stages.3.0.convs.0.all_modules.0.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.1.weight shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.1.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
encoder.stages.3.0.convs.1.conv.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.norm.weight shape torch.Size([256])
encoder.stages.3.0.convs.1.norm.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
encoder.stages.3.0.convs.1.all_modules.0.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.1.weight shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.1.bias shape torch.Size([256])
encoder.stages.4.0.convs.0.conv.weight shape torch.Size([320, 256, 3, 3, 3])
encoder.stages.4.0.convs.0.conv.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.norm.weight shape torch.Size([320])
encoder.stages.4.0.convs.0.norm.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.0.weight shape torch.Size([320, 256, 3, 3, 3])
encoder.stages.4.0.convs.0.all_modules.0.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.1.weight shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.1.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.4.0.convs.1.conv.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.norm.weight shape torch.Size([320])
encoder.stages.4.0.convs.1.norm.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.4.0.convs.1.all_modules.0.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.1.weight shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.1.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.0.conv.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.norm.weight shape torch.Size([320])
encoder.stages.5.0.convs.0.norm.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.0.all_modules.0.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.1.weight shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.1.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.1.conv.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.norm.weight shape torch.Size([320])
encoder.stages.5.0.convs.1.norm.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.1.all_modules.0.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.1.weight shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.0.0.convs.0.conv.weight shape torch.Size([32, 1, 3, 3, 3])
decoder.encoder.stages.0.0.convs.0.conv.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.norm.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.norm.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.0.weight shape torch.Size([32, 1, 3, 3, 3])
decoder.encoder.stages.0.0.convs.0.all_modules.0.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.1.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.1.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.encoder.stages.0.0.convs.1.conv.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.norm.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.norm.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.encoder.stages.0.0.convs.1.all_modules.0.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.1.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.1.bias shape torch.Size([32])
decoder.encoder.stages.1.0.convs.0.conv.weight shape torch.Size([64, 32, 3, 3, 3])
decoder.encoder.stages.1.0.convs.0.conv.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.norm.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.norm.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.0.weight shape torch.Size([64, 32, 3, 3, 3])
decoder.encoder.stages.1.0.convs.0.all_modules.0.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.1.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.1.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.encoder.stages.1.0.convs.1.conv.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.norm.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.norm.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.encoder.stages.1.0.convs.1.all_modules.0.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.1.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.1.bias shape torch.Size([64])
decoder.encoder.stages.2.0.convs.0.conv.weight shape torch.Size([128, 64, 3, 3, 3])
decoder.encoder.stages.2.0.convs.0.conv.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.norm.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.norm.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.0.weight shape torch.Size([128, 64, 3, 3, 3])
decoder.encoder.stages.2.0.convs.0.all_modules.0.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.1.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.1.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.encoder.stages.2.0.convs.1.conv.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.norm.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.norm.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.encoder.stages.2.0.convs.1.all_modules.0.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.1.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.1.bias shape torch.Size([128])
decoder.encoder.stages.3.0.convs.0.conv.weight shape torch.Size([256, 128, 3, 3, 3])
decoder.encoder.stages.3.0.convs.0.conv.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.norm.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.norm.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.0.weight shape torch.Size([256, 128, 3, 3, 3])
decoder.encoder.stages.3.0.convs.0.all_modules.0.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.1.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.1.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.encoder.stages.3.0.convs.1.conv.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.norm.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.norm.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.encoder.stages.3.0.convs.1.all_modules.0.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.1.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.1.bias shape torch.Size([256])
decoder.encoder.stages.4.0.convs.0.conv.weight shape torch.Size([320, 256, 3, 3, 3])
decoder.encoder.stages.4.0.convs.0.conv.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.norm.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.norm.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.0.weight shape torch.Size([320, 256, 3, 3, 3])
decoder.encoder.stages.4.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.4.0.convs.1.conv.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.norm.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.norm.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.4.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.0.conv.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.norm.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.norm.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.1.conv.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.norm.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.norm.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.stages.0.convs.0.conv.weight shape torch.Size([320, 640, 3, 3, 3])
decoder.stages.0.convs.0.conv.bias shape torch.Size([320])
decoder.stages.0.convs.0.norm.weight shape torch.Size([320])
decoder.stages.0.convs.0.norm.bias shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.0.weight shape torch.Size([320, 640, 3, 3, 3])
decoder.stages.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.stages.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.stages.0.convs.1.conv.bias shape torch.Size([320])
decoder.stages.0.convs.1.norm.weight shape torch.Size([320])
decoder.stages.0.convs.1.norm.bias shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.stages.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.stages.1.convs.0.conv.weight shape torch.Size([256, 512, 3, 3, 3])
decoder.stages.1.convs.0.conv.bias shape torch.Size([256])
decoder.stages.1.convs.0.norm.weight shape torch.Size([256])
decoder.stages.1.convs.0.norm.bias shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.0.weight shape torch.Size([256, 512, 3, 3, 3])
decoder.stages.1.convs.0.all_modules.0.bias shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.1.weight shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.1.bias shape torch.Size([256])
decoder.stages.1.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.stages.1.convs.1.conv.bias shape torch.Size([256])
decoder.stages.1.convs.1.norm.weight shape torch.Size([256])
decoder.stages.1.convs.1.norm.bias shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.stages.1.convs.1.all_modules.0.bias shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.1.weight shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.1.bias shape torch.Size([256])
decoder.stages.2.convs.0.conv.weight shape torch.Size([128, 256, 3, 3, 3])
decoder.stages.2.convs.0.conv.bias shape torch.Size([128])
decoder.stages.2.convs.0.norm.weight shape torch.Size([128])
decoder.stages.2.convs.0.norm.bias shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.0.weight shape torch.Size([128, 256, 3, 3, 3])
decoder.stages.2.convs.0.all_modules.0.bias shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.1.weight shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.1.bias shape torch.Size([128])
decoder.stages.2.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.stages.2.convs.1.conv.bias shape torch.Size([128])
decoder.stages.2.convs.1.norm.weight shape torch.Size([128])
decoder.stages.2.convs.1.norm.bias shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.stages.2.convs.1.all_modules.0.bias shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.1.weight shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.1.bias shape torch.Size([128])
decoder.stages.3.convs.0.conv.weight shape torch.Size([64, 128, 3, 3, 3])
decoder.stages.3.convs.0.conv.bias shape torch.Size([64])
decoder.stages.3.convs.0.norm.weight shape torch.Size([64])
decoder.stages.3.convs.0.norm.bias shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.0.weight shape torch.Size([64, 128, 3, 3, 3])
decoder.stages.3.convs.0.all_modules.0.bias shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.1.weight shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.1.bias shape torch.Size([64])
decoder.stages.3.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.stages.3.convs.1.conv.bias shape torch.Size([64])
decoder.stages.3.convs.1.norm.weight shape torch.Size([64])
decoder.stages.3.convs.1.norm.bias shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.stages.3.convs.1.all_modules.0.bias shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.1.weight shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.1.bias shape torch.Size([64])
decoder.stages.4.convs.0.conv.weight shape torch.Size([32, 64, 3, 3, 3])
decoder.stages.4.convs.0.conv.bias shape torch.Size([32])
decoder.stages.4.convs.0.norm.weight shape torch.Size([32])
decoder.stages.4.convs.0.norm.bias shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.0.weight shape torch.Size([32, 64, 3, 3, 3])
decoder.stages.4.convs.0.all_modules.0.bias shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.1.weight shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.1.bias shape torch.Size([32])
decoder.stages.4.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.stages.4.convs.1.conv.bias shape torch.Size([32])
decoder.stages.4.convs.1.norm.weight shape torch.Size([32])
decoder.stages.4.convs.1.norm.bias shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.stages.4.convs.1.all_modules.0.bias shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.1.weight shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.1.bias shape torch.Size([32])
decoder.transpconvs.0.weight shape torch.Size([320, 320, 1, 2, 2])
decoder.transpconvs.0.bias shape torch.Size([320])
decoder.transpconvs.1.weight shape torch.Size([320, 256, 2, 2, 2])
decoder.transpconvs.1.bias shape torch.Size([256])
decoder.transpconvs.2.weight shape torch.Size([256, 128, 2, 2, 2])
decoder.transpconvs.2.bias shape torch.Size([128])
decoder.transpconvs.3.weight shape torch.Size([128, 64, 2, 2, 2])
decoder.transpconvs.3.bias shape torch.Size([64])
decoder.transpconvs.4.weight shape torch.Size([64, 32, 2, 2, 2])
decoder.transpconvs.4.bias shape torch.Size([32])
################### Done ###################
2024-12-13 18:44:11.076355: do_dummy_2d_data_aug: True
2024-12-13 18:44:11.078555: /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset307_Sohee_Calcium_OCT_CrossValidation/nnUNetPlans_3d_fullres
2024-12-13 18:44:11.081108: Creating new 5-fold cross-validation split...
2024-12-13 18:44:11.085854: Desired fold for training: 0
2024-12-13 18:44:11.087168: This split has 4 training and 4 validation cases.
using pin_memory on device 0
using pin_memory on device 0
using pin_memory on device 0

This is the configuration used by this training:
Configuration name: 3d_32x160x128_b10
 {'data_identifier': 'nnUNetPlans_3d_fullres', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 10, 'patch_size': [32, 160, 128], 'median_image_size_in_voxels': [375.0, 498.0, 498.0], 'spacing': [1.0, 1.0, 1.0], 'normalization_schemes': ['ZScoreNormalization'], 'use_mask_for_norm': [False], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'architecture': {'network_class_name': 'dynamic_network_architectures.architectures.unet.PlainConvUNet', 'arch_kwargs': {'n_stages': 6, 'features_per_stage': [32, 64, 128, 256, 320, 320], 'conv_op': 'torch.nn.modules.conv.Conv3d', 'kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]], 'strides': [[1, 1, 1], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [1, 2, 2]], 'n_conv_per_stage': [2, 2, 2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2, 2, 2], 'conv_bias': True, 'norm_op': 'torch.nn.modules.instancenorm.InstanceNorm3d', 'norm_op_kwargs': {'eps': 1e-05, 'affine': True}, 'dropout_op': None, 'dropout_op_kwargs': None, 'nonlin': 'torch.nn.LeakyReLU', 'nonlin_kwargs': {'inplace': True}, 'deep_supervision': True}, '_kw_requires_import': ['conv_op', 'norm_op', 'dropout_op', 'nonlin']}, 'batch_dice': True, 'inherits_from': '3d_fullres'} 

These are the global plan.json settings:
 {'dataset_name': 'Dataset307_Sohee_Calcium_OCT_CrossValidation', 'plans_name': 'nnUNetPlans', 'original_median_spacing_after_transp': [1.0, 1.0, 1.0], 'original_median_shape_after_transp': [375, 498, 498], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 0.4977976381778717, 'mean': 0.13507525622844696, 'median': 0.09599608182907104, 'min': 0.0, 'percentile_00_5': 0.014754901640117168, 'percentile_99_5': 0.4977976381778717, 'std': 0.12025152146816254}}} 

2024-12-13 18:44:26.606450: unpacking dataset...
using pin_memory on device 0

This is the configuration used by this training:
Configuration name: 3d_32x160x128_b10
 {'data_identifier': 'nnUNetPlans_3d_fullres', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 10, 'patch_size': [32, 160, 128], 'median_image_size_in_voxels': [375.0, 498.0, 498.0], 'spacing': [1.0, 1.0, 1.0], 'normalization_schemes': ['ZScoreNormalization'], 'use_mask_for_norm': [False], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'architecture': {'network_class_name': 'dynamic_network_architectures.architectures.unet.PlainConvUNet', 'arch_kwargs': {'n_stages': 6, 'features_per_stage': [32, 64, 128, 256, 320, 320], 'conv_op': 'torch.nn.modules.conv.Conv3d', 'kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]], 'strides': [[1, 1, 1], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [1, 2, 2]], 'n_conv_per_stage': [2, 2, 2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2, 2, 2], 'conv_bias': True, 'norm_op': 'torch.nn.modules.instancenorm.InstanceNorm3d', 'norm_op_kwargs': {'eps': 1e-05, 'affine': True}, 'dropout_op': None, 'dropout_op_kwargs': None, 'nonlin': 'torch.nn.LeakyReLU', 'nonlin_kwargs': {'inplace': True}, 'deep_supervision': True}, '_kw_requires_import': ['conv_op', 'norm_op', 'dropout_op', 'nonlin']}, 'batch_dice': True, 'inherits_from': '3d_fullres'} 

These are the global plan.json settings:
 {'dataset_name': 'Dataset307_Sohee_Calcium_OCT_CrossValidation', 'plans_name': 'nnUNetPlans', 'original_median_spacing_after_transp': [1.0, 1.0, 1.0], 'original_median_shape_after_transp': [375, 498, 498], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 0.4977976381778717, 'mean': 0.13507525622844696, 'median': 0.09599608182907104, 'min': 0.0, 'percentile_00_5': 0.014754901640117168, 'percentile_99_5': 0.4977976381778717, 'std': 0.12025152146816254}}} 

2024-12-13 18:44:28.352563: unpacking dataset...
2024-12-13 18:44:32.108121: unpacking done...
2024-12-13 18:44:32.118471: Unable to plot network architecture: nnUNet_compile is enabled!
2024-12-13 18:44:32.251748: 
2024-12-13 18:44:32.253028: Epoch 0
2024-12-13 18:44:32.253971: Current learning rate: 0.01
2024-12-13 18:50:36.692579: Validation loss improved from 1000.00000 to -0.21252! Patience: 0/50
2024-12-13 18:50:36.694052: train_loss -0.0827
2024-12-13 18:50:36.695303: val_loss -0.2125
2024-12-13 18:50:36.696038: Pseudo dice [0.5459]
2024-12-13 18:50:36.696737: Epoch time: 364.44 s
2024-12-13 18:50:36.697409: Yayy! New best EMA pseudo Dice: 0.5459
2024-12-13 18:50:38.355887: 
2024-12-13 18:50:38.356946: Epoch 1
2024-12-13 18:50:38.357776: Current learning rate: 0.00994
2024-12-13 18:55:40.857256: Validation loss improved from -0.21252 to -0.24250! Patience: 0/50
2024-12-13 18:55:40.858238: train_loss -0.2396
2024-12-13 18:55:40.859009: val_loss -0.2425
2024-12-13 18:55:40.859672: Pseudo dice [0.5791]
2024-12-13 18:55:40.860369: Epoch time: 302.5 s
2024-12-13 18:55:40.861014: Yayy! New best EMA pseudo Dice: 0.5492
2024-12-13 18:55:42.702634: 
2024-12-13 18:55:42.703844: Epoch 2
2024-12-13 18:55:42.704560: Current learning rate: 0.00988
2024-12-13 19:00:49.084370: Validation loss improved from -0.24250 to -0.32398! Patience: 0/50
2024-12-13 19:00:49.085407: train_loss -0.3158
2024-12-13 19:00:49.086344: val_loss -0.324
2024-12-13 19:00:49.087080: Pseudo dice [0.6247]
2024-12-13 19:00:49.087870: Epoch time: 306.38 s
2024-12-13 19:00:49.088563: Yayy! New best EMA pseudo Dice: 0.5567
2024-12-13 19:00:50.951951: 
2024-12-13 19:00:50.953253: Epoch 3
2024-12-13 19:00:50.954042: Current learning rate: 0.00982
2024-12-13 19:06:07.103365: Validation loss improved from -0.32398 to -0.36215! Patience: 0/50
2024-12-13 19:06:07.104361: train_loss -0.3523
2024-12-13 19:06:07.105231: val_loss -0.3621
2024-12-13 19:06:07.105991: Pseudo dice [0.6365]
2024-12-13 19:06:07.106796: Epoch time: 316.15 s
2024-12-13 19:06:07.107479: Yayy! New best EMA pseudo Dice: 0.5647
2024-12-13 19:06:08.937153: 
2024-12-13 19:06:08.938528: Epoch 4
2024-12-13 19:06:08.939420: Current learning rate: 0.00976
2024-12-13 19:11:21.359941: Validation loss did not improve from -0.36215. Patience: 1/50
2024-12-13 19:11:21.360982: train_loss -0.3703
2024-12-13 19:11:21.361842: val_loss -0.3358
2024-12-13 19:11:21.362682: Pseudo dice [0.6233]
2024-12-13 19:11:21.363376: Epoch time: 312.43 s
2024-12-13 19:11:21.709009: Yayy! New best EMA pseudo Dice: 0.5706
2024-12-13 19:11:23.543003: 
2024-12-13 19:11:23.544497: Epoch 5
2024-12-13 19:11:23.545553: Current learning rate: 0.0097
2024-12-13 19:16:46.935031: Validation loss improved from -0.36215 to -0.38087! Patience: 1/50
2024-12-13 19:16:46.936095: train_loss -0.4024
2024-12-13 19:16:46.936959: val_loss -0.3809
2024-12-13 19:16:46.937856: Pseudo dice [0.662]
2024-12-13 19:16:46.938689: Epoch time: 323.39 s
2024-12-13 19:16:46.939457: Yayy! New best EMA pseudo Dice: 0.5797
2024-12-13 19:16:48.718468: 
2024-12-13 19:16:48.719857: Epoch 6
2024-12-13 19:16:48.720659: Current learning rate: 0.00964
2024-12-13 19:22:05.458737: Validation loss improved from -0.38087 to -0.40396! Patience: 0/50
2024-12-13 19:22:05.459801: train_loss -0.4305
2024-12-13 19:22:05.460751: val_loss -0.404
2024-12-13 19:22:05.461422: Pseudo dice [0.6698]
2024-12-13 19:22:05.462144: Epoch time: 316.74 s
2024-12-13 19:22:05.462795: Yayy! New best EMA pseudo Dice: 0.5887
2024-12-13 19:22:07.267349: 
2024-12-13 19:22:07.268535: Epoch 7
2024-12-13 19:22:07.269246: Current learning rate: 0.00958
2024-12-13 19:27:47.305127: Validation loss did not improve from -0.40396. Patience: 1/50
2024-12-13 19:27:47.306066: train_loss -0.4613
2024-12-13 19:27:47.307123: val_loss -0.3616
2024-12-13 19:27:47.307983: Pseudo dice [0.6237]
2024-12-13 19:27:47.308939: Epoch time: 340.04 s
2024-12-13 19:27:47.310097: Yayy! New best EMA pseudo Dice: 0.5922
2024-12-13 19:27:49.508407: 
2024-12-13 19:27:49.509967: Epoch 8
2024-12-13 19:27:49.510810: Current learning rate: 0.00952
2024-12-13 19:33:36.135073: Validation loss improved from -0.40396 to -0.43210! Patience: 1/50
2024-12-13 19:33:36.136095: train_loss -0.5014
2024-12-13 19:33:36.137121: val_loss -0.4321
2024-12-13 19:33:36.137918: Pseudo dice [0.6898]
2024-12-13 19:33:36.138701: Epoch time: 346.63 s
2024-12-13 19:33:36.139533: Yayy! New best EMA pseudo Dice: 0.602
2024-12-13 19:33:37.975691: 
2024-12-13 19:33:37.977170: Epoch 9
2024-12-13 19:33:37.978190: Current learning rate: 0.00946
2024-12-13 19:39:25.786262: Validation loss did not improve from -0.43210. Patience: 1/50
2024-12-13 19:39:25.787192: train_loss -0.5031
2024-12-13 19:39:25.788124: val_loss -0.4215
2024-12-13 19:39:25.788978: Pseudo dice [0.6766]
2024-12-13 19:39:25.789951: Epoch time: 347.81 s
2024-12-13 19:39:26.193089: Yayy! New best EMA pseudo Dice: 0.6094
2024-12-13 19:39:27.913927: 
2024-12-13 19:39:27.915279: Epoch 10
2024-12-13 19:39:27.916202: Current learning rate: 0.0094
2024-12-13 19:44:57.022367: Validation loss did not improve from -0.43210. Patience: 2/50
2024-12-13 19:44:57.023348: train_loss -0.5002
2024-12-13 19:44:57.024018: val_loss -0.4273
2024-12-13 19:44:57.024655: Pseudo dice [0.6924]
2024-12-13 19:44:57.025325: Epoch time: 329.11 s
2024-12-13 19:44:57.025961: Yayy! New best EMA pseudo Dice: 0.6177
2024-12-13 19:44:58.757515: 
2024-12-13 19:44:58.759006: Epoch 11
2024-12-13 19:44:58.759896: Current learning rate: 0.00934
2024-12-13 19:50:49.042630: Validation loss improved from -0.43210 to -0.44088! Patience: 2/50
2024-12-13 19:50:49.044887: train_loss -0.5119
2024-12-13 19:50:49.045801: val_loss -0.4409
2024-12-13 19:50:49.046474: Pseudo dice [0.6792]
2024-12-13 19:50:49.047469: Epoch time: 350.29 s
2024-12-13 19:50:49.048486: Yayy! New best EMA pseudo Dice: 0.6239
2024-12-13 19:50:50.813844: 
2024-12-13 19:50:50.815032: Epoch 12
2024-12-13 19:50:50.815764: Current learning rate: 0.00928
2024-12-13 19:56:58.219755: Validation loss did not improve from -0.44088. Patience: 1/50
2024-12-13 19:56:58.220802: train_loss -0.523
2024-12-13 19:56:58.221576: val_loss -0.4386
2024-12-13 19:56:58.222221: Pseudo dice [0.6904]
2024-12-13 19:56:58.222865: Epoch time: 367.41 s
2024-12-13 19:56:58.223485: Yayy! New best EMA pseudo Dice: 0.6305
2024-12-13 19:57:00.025516: 
2024-12-13 19:57:00.026801: Epoch 13
2024-12-13 19:57:00.027642: Current learning rate: 0.00922
2024-12-13 20:02:37.162832: Validation loss improved from -0.44088 to -0.45548! Patience: 1/50
2024-12-13 20:02:37.163732: train_loss -0.5491
2024-12-13 20:02:37.164649: val_loss -0.4555
2024-12-13 20:02:37.165547: Pseudo dice [0.7062]
2024-12-13 20:02:37.166391: Epoch time: 337.14 s
2024-12-13 20:02:37.167295: Yayy! New best EMA pseudo Dice: 0.6381
2024-12-13 20:02:38.969713: 
2024-12-13 20:02:38.970950: Epoch 14
2024-12-13 20:02:38.971876: Current learning rate: 0.00916
2024-12-13 20:08:35.123124: Validation loss improved from -0.45548 to -0.45885! Patience: 0/50
2024-12-13 20:08:35.124087: train_loss -0.5538
2024-12-13 20:08:35.124843: val_loss -0.4589
2024-12-13 20:08:35.125487: Pseudo dice [0.7069]
2024-12-13 20:08:35.126153: Epoch time: 356.16 s
2024-12-13 20:08:35.472057: Yayy! New best EMA pseudo Dice: 0.645
2024-12-13 20:08:37.239659: 
2024-12-13 20:08:37.240815: Epoch 15
2024-12-13 20:08:37.241576: Current learning rate: 0.0091
2024-12-13 20:13:58.736248: Validation loss did not improve from -0.45885. Patience: 1/50
2024-12-13 20:13:58.737255: train_loss -0.5563
2024-12-13 20:13:58.738449: val_loss -0.4506
2024-12-13 20:13:58.739437: Pseudo dice [0.6947]
2024-12-13 20:13:58.740520: Epoch time: 321.5 s
2024-12-13 20:13:58.741582: Yayy! New best EMA pseudo Dice: 0.65
2024-12-13 20:14:00.547335: 
2024-12-13 20:14:00.548746: Epoch 16
2024-12-13 20:14:00.549795: Current learning rate: 0.00903
2024-12-13 20:20:02.109701: Validation loss did not improve from -0.45885. Patience: 2/50
2024-12-13 20:20:02.110668: train_loss -0.5608
2024-12-13 20:20:02.111568: val_loss -0.3922
2024-12-13 20:20:02.112276: Pseudo dice [0.6572]
2024-12-13 20:20:02.113025: Epoch time: 361.56 s
2024-12-13 20:20:02.113698: Yayy! New best EMA pseudo Dice: 0.6507
2024-12-13 20:20:03.938173: 
2024-12-13 20:20:03.939421: Epoch 17
2024-12-13 20:20:03.940145: Current learning rate: 0.00897
2024-12-13 20:25:44.988331: Validation loss did not improve from -0.45885. Patience: 3/50
2024-12-13 20:25:44.989184: train_loss -0.5822
2024-12-13 20:25:44.990010: val_loss -0.4534
2024-12-13 20:25:44.990771: Pseudo dice [0.697]
2024-12-13 20:25:44.991596: Epoch time: 341.05 s
2024-12-13 20:25:44.992282: Yayy! New best EMA pseudo Dice: 0.6553
2024-12-13 20:25:46.840638: 
2024-12-13 20:25:46.841736: Epoch 18
2024-12-13 20:25:46.842443: Current learning rate: 0.00891
2024-12-13 20:31:38.220978: Validation loss improved from -0.45885 to -0.47046! Patience: 3/50
2024-12-13 20:31:38.222111: train_loss -0.5745
2024-12-13 20:31:38.222979: val_loss -0.4705
2024-12-13 20:31:38.223653: Pseudo dice [0.7087]
2024-12-13 20:31:38.224409: Epoch time: 351.38 s
2024-12-13 20:31:38.225162: Yayy! New best EMA pseudo Dice: 0.6606
2024-12-13 20:31:40.476684: 
2024-12-13 20:31:40.477879: Epoch 19
2024-12-13 20:31:40.478543: Current learning rate: 0.00885
2024-12-13 20:37:58.028958: Validation loss did not improve from -0.47046. Patience: 1/50
2024-12-13 20:37:58.029943: train_loss -0.5782
2024-12-13 20:37:58.030712: val_loss -0.4605
2024-12-13 20:37:58.031520: Pseudo dice [0.7013]
2024-12-13 20:37:58.032231: Epoch time: 377.55 s
2024-12-13 20:37:58.453609: Yayy! New best EMA pseudo Dice: 0.6647
2024-12-13 20:38:00.311093: 
2024-12-13 20:38:00.312317: Epoch 20
2024-12-13 20:38:00.313252: Current learning rate: 0.00879
2024-12-13 20:44:19.693495: Validation loss did not improve from -0.47046. Patience: 2/50
2024-12-13 20:44:19.694582: train_loss -0.5794
2024-12-13 20:44:19.695726: val_loss -0.43
2024-12-13 20:44:19.697023: Pseudo dice [0.6744]
2024-12-13 20:44:19.697795: Epoch time: 379.38 s
2024-12-13 20:44:19.698446: Yayy! New best EMA pseudo Dice: 0.6657
2024-12-13 20:44:21.541003: 
2024-12-13 20:44:21.542518: Epoch 21
2024-12-13 20:44:21.543486: Current learning rate: 0.00873
2024-12-13 20:51:21.392305: Validation loss did not improve from -0.47046. Patience: 3/50
2024-12-13 20:51:21.393289: train_loss -0.6047
2024-12-13 20:51:21.394038: val_loss -0.4272
2024-12-13 20:51:21.394732: Pseudo dice [0.6818]
2024-12-13 20:51:21.395485: Epoch time: 419.85 s
2024-12-13 20:51:21.396132: Yayy! New best EMA pseudo Dice: 0.6673
2024-12-13 20:51:23.131815: 
2024-12-13 20:51:23.133123: Epoch 22
2024-12-13 20:51:23.133925: Current learning rate: 0.00867
2024-12-13 20:59:06.413609: Validation loss did not improve from -0.47046. Patience: 4/50
2024-12-13 20:59:06.414679: train_loss -0.5945
2024-12-13 20:59:06.415524: val_loss -0.4284
2024-12-13 20:59:06.416208: Pseudo dice [0.6728]
2024-12-13 20:59:06.417027: Epoch time: 463.28 s
2024-12-13 20:59:06.417793: Yayy! New best EMA pseudo Dice: 0.6678
2024-12-13 20:59:08.141520: 
2024-12-13 20:59:08.142599: Epoch 23
2024-12-13 20:59:08.143385: Current learning rate: 0.00861
2024-12-13 21:06:38.138678: Validation loss did not improve from -0.47046. Patience: 5/50
2024-12-13 21:06:38.139735: train_loss -0.6141
2024-12-13 21:06:38.140633: val_loss -0.4553
2024-12-13 21:06:38.141283: Pseudo dice [0.6914]
2024-12-13 21:06:38.141946: Epoch time: 450.0 s
2024-12-13 21:06:38.142725: Yayy! New best EMA pseudo Dice: 0.6702
2024-12-13 21:06:39.852706: 
2024-12-13 21:06:39.853712: Epoch 24
2024-12-13 21:06:39.854465: Current learning rate: 0.00855
2024-12-13 21:14:36.168351: Validation loss did not improve from -0.47046. Patience: 6/50
2024-12-13 21:14:36.169331: train_loss -0.6093
2024-12-13 21:14:36.170144: val_loss -0.4351
2024-12-13 21:14:36.170901: Pseudo dice [0.681]
2024-12-13 21:14:36.171659: Epoch time: 476.32 s
2024-12-13 21:14:36.570715: Yayy! New best EMA pseudo Dice: 0.6713
2024-12-13 21:14:38.373084: 
2024-12-13 21:14:38.374256: Epoch 25
2024-12-13 21:14:38.375011: Current learning rate: 0.00849
2024-12-13 21:22:24.243621: Validation loss improved from -0.47046 to -0.47167! Patience: 6/50
2024-12-13 21:22:24.244544: train_loss -0.6117
2024-12-13 21:22:24.245487: val_loss -0.4717
2024-12-13 21:22:24.246316: Pseudo dice [0.6998]
2024-12-13 21:22:24.247255: Epoch time: 465.87 s
2024-12-13 21:22:24.248117: Yayy! New best EMA pseudo Dice: 0.6741
2024-12-13 21:22:26.002630: 
2024-12-13 21:22:26.004048: Epoch 26
2024-12-13 21:22:26.004887: Current learning rate: 0.00843
2024-12-13 21:30:21.229856: Validation loss did not improve from -0.47167. Patience: 1/50
2024-12-13 21:30:21.230950: train_loss -0.6226
2024-12-13 21:30:21.231688: val_loss -0.463
2024-12-13 21:30:21.232550: Pseudo dice [0.6959]
2024-12-13 21:30:21.233392: Epoch time: 475.23 s
2024-12-13 21:30:21.234018: Yayy! New best EMA pseudo Dice: 0.6763
2024-12-13 21:30:23.025146: 
2024-12-13 21:30:23.026229: Epoch 27
2024-12-13 21:30:23.026920: Current learning rate: 0.00836
2024-12-13 21:38:26.025284: Validation loss improved from -0.47167 to -0.51180! Patience: 1/50
2024-12-13 21:38:26.026132: train_loss -0.639
2024-12-13 21:38:26.027062: val_loss -0.5118
2024-12-13 21:38:26.027820: Pseudo dice [0.7308]
2024-12-13 21:38:26.028529: Epoch time: 483.0 s
2024-12-13 21:38:26.029309: Yayy! New best EMA pseudo Dice: 0.6818
2024-12-13 21:38:27.795936: 
2024-12-13 21:38:27.797275: Epoch 28
2024-12-13 21:38:27.797993: Current learning rate: 0.0083
2024-12-13 21:46:20.775959: Validation loss did not improve from -0.51180. Patience: 1/50
2024-12-13 21:46:20.776601: train_loss -0.6411
2024-12-13 21:46:20.777625: val_loss -0.4513
2024-12-13 21:46:20.778566: Pseudo dice [0.6924]
2024-12-13 21:46:20.779575: Epoch time: 472.98 s
2024-12-13 21:46:20.780545: Yayy! New best EMA pseudo Dice: 0.6828
2024-12-13 21:46:23.276727: 
2024-12-13 21:46:23.277797: Epoch 29
2024-12-13 21:46:23.278802: Current learning rate: 0.00824
2024-12-13 21:54:21.869323: Validation loss did not improve from -0.51180. Patience: 2/50
2024-12-13 21:54:21.870292: train_loss -0.6368
2024-12-13 21:54:21.871173: val_loss -0.437
2024-12-13 21:54:21.871952: Pseudo dice [0.6834]
2024-12-13 21:54:21.872824: Epoch time: 478.59 s
2024-12-13 21:54:22.298138: Yayy! New best EMA pseudo Dice: 0.6829
2024-12-13 21:54:24.067849: 
2024-12-13 21:54:24.069479: Epoch 30
2024-12-13 21:54:24.070538: Current learning rate: 0.00818
2024-12-13 22:02:32.632385: Validation loss did not improve from -0.51180. Patience: 3/50
2024-12-13 22:02:32.637591: train_loss -0.6391
2024-12-13 22:02:32.639123: val_loss -0.4716
2024-12-13 22:02:32.640061: Pseudo dice [0.7098]
2024-12-13 22:02:32.641101: Epoch time: 488.57 s
2024-12-13 22:02:32.642150: Yayy! New best EMA pseudo Dice: 0.6856
2024-12-13 22:02:34.500879: 
2024-12-13 22:02:34.502254: Epoch 31
2024-12-13 22:02:34.502948: Current learning rate: 0.00812
2024-12-13 22:10:58.600765: Validation loss did not improve from -0.51180. Patience: 4/50
2024-12-13 22:10:58.601727: train_loss -0.6408
2024-12-13 22:10:58.603087: val_loss -0.4262
2024-12-13 22:10:58.603849: Pseudo dice [0.6866]
2024-12-13 22:10:58.604882: Epoch time: 504.1 s
2024-12-13 22:10:58.605636: Yayy! New best EMA pseudo Dice: 0.6857
2024-12-13 22:11:00.454892: 
2024-12-13 22:11:00.456134: Epoch 32
2024-12-13 22:11:00.456862: Current learning rate: 0.00806
2024-12-13 22:19:27.362234: Validation loss did not improve from -0.51180. Patience: 5/50
2024-12-13 22:19:27.362836: train_loss -0.6462
2024-12-13 22:19:27.363512: val_loss -0.4597
2024-12-13 22:19:27.364128: Pseudo dice [0.7001]
2024-12-13 22:19:27.364858: Epoch time: 506.91 s
2024-12-13 22:19:27.365528: Yayy! New best EMA pseudo Dice: 0.6871
2024-12-13 22:19:29.135210: 
2024-12-13 22:19:29.136052: Epoch 33
2024-12-13 22:19:29.136803: Current learning rate: 0.008
2024-12-13 22:28:01.241634: Validation loss did not improve from -0.51180. Patience: 6/50
2024-12-13 22:28:01.242307: train_loss -0.6507
2024-12-13 22:28:01.243152: val_loss -0.4548
2024-12-13 22:28:01.244068: Pseudo dice [0.6992]
2024-12-13 22:28:01.244871: Epoch time: 512.11 s
2024-12-13 22:28:01.245580: Yayy! New best EMA pseudo Dice: 0.6883
2024-12-13 22:28:03.117826: 
2024-12-13 22:28:03.118946: Epoch 34
2024-12-13 22:28:03.119698: Current learning rate: 0.00793
2024-12-13 22:36:21.392510: Validation loss did not improve from -0.51180. Patience: 7/50
2024-12-13 22:36:21.393302: train_loss -0.6502
2024-12-13 22:36:21.394138: val_loss -0.4644
2024-12-13 22:36:21.394856: Pseudo dice [0.7045]
2024-12-13 22:36:21.395525: Epoch time: 498.28 s
2024-12-13 22:36:21.764331: Yayy! New best EMA pseudo Dice: 0.6899
2024-12-13 22:36:23.546164: 
2024-12-13 22:36:23.547316: Epoch 35
2024-12-13 22:36:23.548038: Current learning rate: 0.00787
2024-12-13 22:44:45.363950: Validation loss did not improve from -0.51180. Patience: 8/50
2024-12-13 22:44:45.364686: train_loss -0.6644
2024-12-13 22:44:45.365456: val_loss -0.3914
2024-12-13 22:44:45.366080: Pseudo dice [0.6576]
2024-12-13 22:44:45.366782: Epoch time: 501.82 s
2024-12-13 22:44:46.761490: 
2024-12-13 22:44:46.762499: Epoch 36
2024-12-13 22:44:46.763156: Current learning rate: 0.00781
2024-12-13 22:52:40.679857: Validation loss did not improve from -0.51180. Patience: 9/50
2024-12-13 22:52:40.680541: train_loss -0.6617
2024-12-13 22:52:40.681309: val_loss -0.4529
2024-12-13 22:52:40.682066: Pseudo dice [0.7016]
2024-12-13 22:52:40.682809: Epoch time: 473.92 s
2024-12-13 22:52:42.081266: 
2024-12-13 22:52:42.081952: Epoch 37
2024-12-13 22:52:42.082693: Current learning rate: 0.00775
2024-12-13 23:00:49.794075: Validation loss did not improve from -0.51180. Patience: 10/50
2024-12-13 23:00:49.794778: train_loss -0.6626
2024-12-13 23:00:49.795598: val_loss -0.4631
2024-12-13 23:00:49.796265: Pseudo dice [0.6912]
2024-12-13 23:00:49.796849: Epoch time: 487.71 s
2024-12-13 23:00:51.232532: 
2024-12-13 23:00:51.233371: Epoch 38
2024-12-13 23:00:51.234030: Current learning rate: 0.00769
2024-12-13 23:09:04.424129: Validation loss did not improve from -0.51180. Patience: 11/50
2024-12-13 23:09:04.427872: train_loss -0.6654
2024-12-13 23:09:04.428929: val_loss -0.4511
2024-12-13 23:09:04.429746: Pseudo dice [0.7016]
2024-12-13 23:09:04.430735: Epoch time: 493.2 s
2024-12-13 23:09:05.864561: 
2024-12-13 23:09:05.865755: Epoch 39
2024-12-13 23:09:05.866457: Current learning rate: 0.00763
2024-12-13 23:17:36.333522: Validation loss did not improve from -0.51180. Patience: 12/50
2024-12-13 23:17:36.334389: train_loss -0.6697
2024-12-13 23:17:36.336118: val_loss -0.4407
2024-12-13 23:17:36.337123: Pseudo dice [0.6974]
2024-12-13 23:17:36.338284: Epoch time: 510.47 s
2024-12-13 23:17:37.905410: Yayy! New best EMA pseudo Dice: 0.6906
2024-12-13 23:17:39.860856: 
2024-12-13 23:17:39.862019: Epoch 40
2024-12-13 23:17:39.863065: Current learning rate: 0.00756
2024-12-13 23:25:54.053369: Validation loss did not improve from -0.51180. Patience: 13/50
2024-12-13 23:25:54.054083: train_loss -0.6778
2024-12-13 23:25:54.055109: val_loss -0.4535
2024-12-13 23:25:54.056105: Pseudo dice [0.687]
2024-12-13 23:25:54.057022: Epoch time: 494.19 s
2024-12-13 23:25:55.457992: 
2024-12-13 23:25:55.459008: Epoch 41
2024-12-13 23:25:55.459944: Current learning rate: 0.0075
2024-12-13 23:34:21.728004: Validation loss did not improve from -0.51180. Patience: 14/50
2024-12-13 23:34:21.728733: train_loss -0.6785
2024-12-13 23:34:21.729454: val_loss -0.4471
2024-12-13 23:34:21.730079: Pseudo dice [0.6903]
2024-12-13 23:34:21.730741: Epoch time: 506.27 s
2024-12-13 23:34:23.076369: 
2024-12-13 23:34:23.077288: Epoch 42
2024-12-13 23:34:23.078068: Current learning rate: 0.00744
2024-12-13 23:42:34.717421: Validation loss did not improve from -0.51180. Patience: 15/50
2024-12-13 23:42:34.718138: train_loss -0.6776
2024-12-13 23:42:34.719200: val_loss -0.4938
2024-12-13 23:42:34.720057: Pseudo dice [0.7189]
2024-12-13 23:42:34.720922: Epoch time: 491.64 s
2024-12-13 23:42:34.721874: Yayy! New best EMA pseudo Dice: 0.6931
2024-12-13 23:42:36.519744: 
2024-12-13 23:42:36.520764: Epoch 43
2024-12-13 23:42:36.521657: Current learning rate: 0.00738
2024-12-13 23:50:58.648324: Validation loss did not improve from -0.51180. Patience: 16/50
2024-12-13 23:50:58.649057: train_loss -0.6698
2024-12-13 23:50:58.649858: val_loss -0.4746
2024-12-13 23:50:58.650486: Pseudo dice [0.7154]
2024-12-13 23:50:58.651226: Epoch time: 502.13 s
2024-12-13 23:50:58.651979: Yayy! New best EMA pseudo Dice: 0.6953
2024-12-13 23:51:00.440193: 
2024-12-13 23:51:00.441175: Epoch 44
2024-12-13 23:51:00.442067: Current learning rate: 0.00732
2024-12-13 23:59:27.101166: Validation loss did not improve from -0.51180. Patience: 17/50
2024-12-13 23:59:27.101909: train_loss -0.6832
2024-12-13 23:59:27.102610: val_loss -0.4714
2024-12-13 23:59:27.103252: Pseudo dice [0.7149]
2024-12-13 23:59:27.104046: Epoch time: 506.66 s
2024-12-13 23:59:27.519895: Yayy! New best EMA pseudo Dice: 0.6973
2024-12-13 23:59:29.278741: 
2024-12-13 23:59:29.279710: Epoch 45
2024-12-13 23:59:29.280367: Current learning rate: 0.00725
2024-12-14 00:07:51.182102: Validation loss did not improve from -0.51180. Patience: 18/50
2024-12-14 00:07:51.182935: train_loss -0.6879
2024-12-14 00:07:51.183772: val_loss -0.4681
2024-12-14 00:07:51.184454: Pseudo dice [0.7132]
2024-12-14 00:07:51.185149: Epoch time: 501.91 s
2024-12-14 00:07:51.185925: Yayy! New best EMA pseudo Dice: 0.6989
2024-12-14 00:07:52.947410: 
2024-12-14 00:07:52.948173: Epoch 46
2024-12-14 00:07:52.948895: Current learning rate: 0.00719
2024-12-14 00:16:21.004231: Validation loss did not improve from -0.51180. Patience: 19/50
2024-12-14 00:16:21.005599: train_loss -0.6871
2024-12-14 00:16:21.006371: val_loss -0.4508
2024-12-14 00:16:21.007031: Pseudo dice [0.6896]
2024-12-14 00:16:21.007940: Epoch time: 508.06 s
2024-12-14 00:16:22.380025: 
2024-12-14 00:16:22.380887: Epoch 47
2024-12-14 00:16:22.381542: Current learning rate: 0.00713
2024-12-14 00:24:58.383004: Validation loss did not improve from -0.51180. Patience: 20/50
2024-12-14 00:24:58.383747: train_loss -0.6802
2024-12-14 00:24:58.384502: val_loss -0.4917
2024-12-14 00:24:58.385131: Pseudo dice [0.7185]
2024-12-14 00:24:58.385775: Epoch time: 516.0 s
2024-12-14 00:24:58.386416: Yayy! New best EMA pseudo Dice: 0.7
2024-12-14 00:25:00.106508: 
2024-12-14 00:25:00.107380: Epoch 48
2024-12-14 00:25:00.108062: Current learning rate: 0.00707
2024-12-14 00:33:40.160830: Validation loss did not improve from -0.51180. Patience: 21/50
2024-12-14 00:33:40.161537: train_loss -0.6915
2024-12-14 00:33:40.162300: val_loss -0.4528
2024-12-14 00:33:40.163012: Pseudo dice [0.7107]
2024-12-14 00:33:40.163695: Epoch time: 520.06 s
2024-12-14 00:33:40.164341: Yayy! New best EMA pseudo Dice: 0.7011
2024-12-14 00:33:41.997085: 
2024-12-14 00:33:41.998116: Epoch 49
2024-12-14 00:33:41.998892: Current learning rate: 0.007
2024-12-14 00:41:38.984872: Validation loss did not improve from -0.51180. Patience: 22/50
2024-12-14 00:41:38.985507: train_loss -0.6951
2024-12-14 00:41:38.986290: val_loss -0.489
2024-12-14 00:41:38.986973: Pseudo dice [0.723]
2024-12-14 00:41:38.987638: Epoch time: 476.99 s
2024-12-14 00:41:39.411061: Yayy! New best EMA pseudo Dice: 0.7033
2024-12-14 00:41:41.558114: 
2024-12-14 00:41:41.558998: Epoch 50
2024-12-14 00:41:41.559881: Current learning rate: 0.00694
2024-12-14 00:49:37.793256: Validation loss did not improve from -0.51180. Patience: 23/50
2024-12-14 00:49:37.794253: train_loss -0.6905
2024-12-14 00:49:37.795085: val_loss -0.4657
2024-12-14 00:49:37.795728: Pseudo dice [0.7113]
2024-12-14 00:49:37.796399: Epoch time: 476.24 s
2024-12-14 00:49:37.797065: Yayy! New best EMA pseudo Dice: 0.7041
2024-12-14 00:49:39.672526: 
2024-12-14 00:49:39.673442: Epoch 51
2024-12-14 00:49:39.674144: Current learning rate: 0.00688
2024-12-14 00:57:58.425548: Validation loss did not improve from -0.51180. Patience: 24/50
2024-12-14 00:57:58.426312: train_loss -0.6993
2024-12-14 00:57:58.427067: val_loss -0.465
2024-12-14 00:57:58.427788: Pseudo dice [0.7127]
2024-12-14 00:57:58.428367: Epoch time: 498.75 s
2024-12-14 00:57:58.429145: Yayy! New best EMA pseudo Dice: 0.7049
2024-12-14 00:58:00.296634: 
2024-12-14 00:58:00.297473: Epoch 52
2024-12-14 00:58:00.298115: Current learning rate: 0.00682
2024-12-14 01:06:14.452873: Validation loss did not improve from -0.51180. Patience: 25/50
2024-12-14 01:06:14.453482: train_loss -0.6947
2024-12-14 01:06:14.454131: val_loss -0.4476
2024-12-14 01:06:14.454800: Pseudo dice [0.6872]
2024-12-14 01:06:14.455596: Epoch time: 494.16 s
2024-12-14 01:06:15.799915: 
2024-12-14 01:06:15.800844: Epoch 53
2024-12-14 01:06:15.801543: Current learning rate: 0.00675
2024-12-14 01:14:40.298311: Validation loss did not improve from -0.51180. Patience: 26/50
2024-12-14 01:14:40.299679: train_loss -0.7039
2024-12-14 01:14:40.300648: val_loss -0.4565
2024-12-14 01:14:40.301417: Pseudo dice [0.7225]
2024-12-14 01:14:40.302202: Epoch time: 504.5 s
2024-12-14 01:14:40.302979: Yayy! New best EMA pseudo Dice: 0.7051
2024-12-14 01:14:42.056462: 
2024-12-14 01:14:42.057466: Epoch 54
2024-12-14 01:14:42.058302: Current learning rate: 0.00669
2024-12-14 01:23:14.061290: Validation loss did not improve from -0.51180. Patience: 27/50
2024-12-14 01:23:14.061953: train_loss -0.7094
2024-12-14 01:23:14.063482: val_loss -0.4049
2024-12-14 01:23:14.064325: Pseudo dice [0.6742]
2024-12-14 01:23:14.065300: Epoch time: 512.01 s
2024-12-14 01:23:15.793762: 
2024-12-14 01:23:15.794919: Epoch 55
2024-12-14 01:23:15.795780: Current learning rate: 0.00663
2024-12-14 01:31:39.872163: Validation loss did not improve from -0.51180. Patience: 28/50
2024-12-14 01:31:39.872858: train_loss -0.711
2024-12-14 01:31:39.873572: val_loss -0.4559
2024-12-14 01:31:39.874206: Pseudo dice [0.7096]
2024-12-14 01:31:39.874847: Epoch time: 504.08 s
2024-12-14 01:31:41.239228: 
2024-12-14 01:31:41.240154: Epoch 56
2024-12-14 01:31:41.240862: Current learning rate: 0.00657
2024-12-14 01:40:01.376259: Validation loss did not improve from -0.51180. Patience: 29/50
2024-12-14 01:40:01.377056: train_loss -0.7137
2024-12-14 01:40:01.377794: val_loss -0.4423
2024-12-14 01:40:01.378478: Pseudo dice [0.6898]
2024-12-14 01:40:01.379147: Epoch time: 500.14 s
2024-12-14 01:40:02.753033: 
2024-12-14 01:40:02.754005: Epoch 57
2024-12-14 01:40:02.754706: Current learning rate: 0.0065
2024-12-14 01:48:52.260026: Validation loss did not improve from -0.51180. Patience: 30/50
2024-12-14 01:48:52.261034: train_loss -0.7124
2024-12-14 01:48:52.261868: val_loss -0.4907
2024-12-14 01:48:52.262527: Pseudo dice [0.7226]
2024-12-14 01:48:52.263278: Epoch time: 529.51 s
2024-12-14 01:48:53.655810: 
2024-12-14 01:48:53.656834: Epoch 58
2024-12-14 01:48:53.657660: Current learning rate: 0.00644
2024-12-14 01:57:30.759932: Validation loss did not improve from -0.51180. Patience: 31/50
2024-12-14 01:57:30.760657: train_loss -0.7146
2024-12-14 01:57:30.761377: val_loss -0.4834
2024-12-14 01:57:30.762055: Pseudo dice [0.7308]
2024-12-14 01:57:30.762731: Epoch time: 517.11 s
2024-12-14 01:57:30.763392: Yayy! New best EMA pseudo Dice: 0.7063
2024-12-14 01:57:32.599100: 
2024-12-14 01:57:32.599995: Epoch 59
2024-12-14 01:57:32.600649: Current learning rate: 0.00638
2024-12-14 02:06:05.444487: Validation loss did not improve from -0.51180. Patience: 32/50
2024-12-14 02:06:05.446405: train_loss -0.713
2024-12-14 02:06:05.447777: val_loss -0.4477
2024-12-14 02:06:05.448390: Pseudo dice [0.6953]
2024-12-14 02:06:05.448990: Epoch time: 512.85 s
2024-12-14 02:06:07.167598: 
2024-12-14 02:06:07.168543: Epoch 60
2024-12-14 02:06:07.169319: Current learning rate: 0.00631
2024-12-14 02:14:22.864809: Validation loss did not improve from -0.51180. Patience: 33/50
2024-12-14 02:14:22.865514: train_loss -0.7204
2024-12-14 02:14:22.866182: val_loss -0.4344
2024-12-14 02:14:22.866826: Pseudo dice [0.6969]
2024-12-14 02:14:22.867470: Epoch time: 495.7 s
2024-12-14 02:14:25.548407: 
2024-12-14 02:14:25.549455: Epoch 61
2024-12-14 02:14:25.550150: Current learning rate: 0.00625
2024-12-14 02:22:28.362381: Validation loss did not improve from -0.51180. Patience: 34/50
2024-12-14 02:22:28.366347: train_loss -0.7112
2024-12-14 02:22:28.367821: val_loss -0.4689
2024-12-14 02:22:28.368684: Pseudo dice [0.7145]
2024-12-14 02:22:28.369808: Epoch time: 482.82 s
2024-12-14 02:22:29.807940: 
2024-12-14 02:22:29.808950: Epoch 62
2024-12-14 02:22:29.809736: Current learning rate: 0.00619
2024-12-14 02:30:49.540429: Validation loss did not improve from -0.51180. Patience: 35/50
2024-12-14 02:30:49.541027: train_loss -0.7236
2024-12-14 02:30:49.541978: val_loss -0.4442
2024-12-14 02:30:49.542899: Pseudo dice [0.6996]
2024-12-14 02:30:49.543760: Epoch time: 499.73 s
2024-12-14 02:30:50.949422: 
2024-12-14 02:30:50.950534: Epoch 63
2024-12-14 02:30:50.951468: Current learning rate: 0.00612
2024-12-14 02:39:43.359917: Validation loss did not improve from -0.51180. Patience: 36/50
2024-12-14 02:39:43.360563: train_loss -0.7273
2024-12-14 02:39:43.361445: val_loss -0.4396
2024-12-14 02:39:43.362175: Pseudo dice [0.7007]
2024-12-14 02:39:43.362966: Epoch time: 532.41 s
2024-12-14 02:39:44.808567: 
2024-12-14 02:39:44.809603: Epoch 64
2024-12-14 02:39:44.810387: Current learning rate: 0.00606
2024-12-14 02:48:17.779024: Validation loss did not improve from -0.51180. Patience: 37/50
2024-12-14 02:48:17.779701: train_loss -0.7228
2024-12-14 02:48:17.780445: val_loss -0.4841
2024-12-14 02:48:17.781103: Pseudo dice [0.7252]
2024-12-14 02:48:17.781786: Epoch time: 512.97 s
2024-12-14 02:48:18.143605: Yayy! New best EMA pseudo Dice: 0.7065
2024-12-14 02:48:19.954374: 
2024-12-14 02:48:19.955264: Epoch 65
2024-12-14 02:48:19.955923: Current learning rate: 0.006
2024-12-14 02:56:47.771384: Validation loss did not improve from -0.51180. Patience: 38/50
2024-12-14 02:56:47.772050: train_loss -0.7243
2024-12-14 02:56:47.772766: val_loss -0.4463
2024-12-14 02:56:47.773387: Pseudo dice [0.7078]
2024-12-14 02:56:47.774024: Epoch time: 507.82 s
2024-12-14 02:56:47.774686: Yayy! New best EMA pseudo Dice: 0.7066
2024-12-14 02:56:49.633033: 
2024-12-14 02:56:49.633918: Epoch 66
2024-12-14 02:56:49.634595: Current learning rate: 0.00593
2024-12-14 03:05:07.198892: Validation loss did not improve from -0.51180. Patience: 39/50
2024-12-14 03:05:07.199580: train_loss -0.7212
2024-12-14 03:05:07.200343: val_loss -0.4137
2024-12-14 03:05:07.200980: Pseudo dice [0.6745]
2024-12-14 03:05:07.201737: Epoch time: 497.57 s
2024-12-14 03:05:08.619800: 
2024-12-14 03:05:08.620817: Epoch 67
2024-12-14 03:05:08.621457: Current learning rate: 0.00587
2024-12-14 03:13:29.776319: Validation loss did not improve from -0.51180. Patience: 40/50
2024-12-14 03:13:29.776990: train_loss -0.723
2024-12-14 03:13:29.777698: val_loss -0.422
2024-12-14 03:13:29.778386: Pseudo dice [0.6948]
2024-12-14 03:13:29.778955: Epoch time: 501.16 s
2024-12-14 03:13:31.198911: 
2024-12-14 03:13:31.199749: Epoch 68
2024-12-14 03:13:31.200448: Current learning rate: 0.00581
2024-12-14 03:21:47.042833: Validation loss did not improve from -0.51180. Patience: 41/50
2024-12-14 03:21:47.043530: train_loss -0.7284
2024-12-14 03:21:47.044258: val_loss -0.4963
2024-12-14 03:21:47.045064: Pseudo dice [0.7318]
2024-12-14 03:21:47.045751: Epoch time: 495.85 s
2024-12-14 03:21:48.455406: 
2024-12-14 03:21:48.456427: Epoch 69
2024-12-14 03:21:48.457167: Current learning rate: 0.00574
2024-12-14 03:30:06.142374: Validation loss did not improve from -0.51180. Patience: 42/50
2024-12-14 03:30:06.143175: train_loss -0.7293
2024-12-14 03:30:06.143932: val_loss -0.4506
2024-12-14 03:30:06.144927: Pseudo dice [0.6988]
2024-12-14 03:30:06.145708: Epoch time: 497.69 s
2024-12-14 03:30:07.899809: 
2024-12-14 03:30:07.900760: Epoch 70
2024-12-14 03:30:07.901480: Current learning rate: 0.00568
2024-12-14 03:38:24.361802: Validation loss did not improve from -0.51180. Patience: 43/50
2024-12-14 03:38:24.362592: train_loss -0.7207
2024-12-14 03:38:24.363333: val_loss -0.4488
2024-12-14 03:38:24.364006: Pseudo dice [0.6984]
2024-12-14 03:38:24.364683: Epoch time: 496.46 s
2024-12-14 03:38:26.308164: 
2024-12-14 03:38:26.309018: Epoch 71
2024-12-14 03:38:26.309649: Current learning rate: 0.00562
2024-12-14 03:46:44.950687: Validation loss did not improve from -0.51180. Patience: 44/50
2024-12-14 03:46:44.951290: train_loss -0.7266
2024-12-14 03:46:44.952190: val_loss -0.4625
2024-12-14 03:46:44.953155: Pseudo dice [0.7102]
2024-12-14 03:46:44.953892: Epoch time: 498.64 s
2024-12-14 03:46:46.382323: 
2024-12-14 03:46:46.383352: Epoch 72
2024-12-14 03:46:46.384192: Current learning rate: 0.00555
2024-12-14 03:55:15.132323: Validation loss did not improve from -0.51180. Patience: 45/50
2024-12-14 03:55:15.133119: train_loss -0.7379
2024-12-14 03:55:15.133836: val_loss -0.452
2024-12-14 03:55:15.134491: Pseudo dice [0.7095]
2024-12-14 03:55:15.135207: Epoch time: 508.75 s
2024-12-14 03:55:16.553342: 
2024-12-14 03:55:16.554354: Epoch 73
2024-12-14 03:55:16.555205: Current learning rate: 0.00549
2024-12-14 04:03:39.446919: Validation loss did not improve from -0.51180. Patience: 46/50
2024-12-14 04:03:39.447681: train_loss -0.7322
2024-12-14 04:03:39.448430: val_loss -0.506
2024-12-14 04:03:39.449070: Pseudo dice [0.7403]
2024-12-14 04:03:39.449711: Epoch time: 502.9 s
2024-12-14 04:03:39.450316: Yayy! New best EMA pseudo Dice: 0.7087
2024-12-14 04:03:41.227470: 
2024-12-14 04:03:41.228273: Epoch 74
2024-12-14 04:03:41.228920: Current learning rate: 0.00542
2024-12-14 04:12:10.980454: Validation loss did not improve from -0.51180. Patience: 47/50
2024-12-14 04:12:10.981202: train_loss -0.7418
2024-12-14 04:12:10.981872: val_loss -0.5043
2024-12-14 04:12:10.982506: Pseudo dice [0.7277]
2024-12-14 04:12:10.983140: Epoch time: 509.75 s
2024-12-14 04:12:11.398298: Yayy! New best EMA pseudo Dice: 0.7106
2024-12-14 04:12:13.193990: 
2024-12-14 04:12:13.194764: Epoch 75
2024-12-14 04:12:13.195459: Current learning rate: 0.00536
2024-12-14 04:20:40.856837: Validation loss did not improve from -0.51180. Patience: 48/50
2024-12-14 04:20:40.857543: train_loss -0.7467
2024-12-14 04:20:40.858239: val_loss -0.4456
2024-12-14 04:20:40.858883: Pseudo dice [0.7102]
2024-12-14 04:20:40.859589: Epoch time: 507.66 s
2024-12-14 04:20:42.276924: 
2024-12-14 04:20:42.277884: Epoch 76
2024-12-14 04:20:42.278522: Current learning rate: 0.00529
2024-12-14 04:29:08.472876: Validation loss did not improve from -0.51180. Patience: 49/50
2024-12-14 04:29:08.474159: train_loss -0.7373
2024-12-14 04:29:08.475092: val_loss -0.475
2024-12-14 04:29:08.476032: Pseudo dice [0.7141]
2024-12-14 04:29:08.476889: Epoch time: 506.2 s
2024-12-14 04:29:08.477665: Yayy! New best EMA pseudo Dice: 0.7109
2024-12-14 04:29:10.399295: 
2024-12-14 04:29:10.400225: Epoch 77
2024-12-14 04:29:10.401074: Current learning rate: 0.00523
2024-12-14 04:37:23.729980: Validation loss did not improve from -0.51180. Patience: 50/50
2024-12-14 04:37:23.733080: train_loss -0.7368
2024-12-14 04:37:23.734694: val_loss -0.4728
2024-12-14 04:37:23.735481: Pseudo dice [0.7189]
2024-12-14 04:37:23.736552: Epoch time: 493.33 s
2024-12-14 04:37:23.737332: Yayy! New best EMA pseudo Dice: 0.7117
2024-12-14 04:37:25.684781: 
2024-12-14 04:37:25.685707: Epoch 78
2024-12-14 04:37:25.686486: Current learning rate: 0.00517
2024-12-14 04:45:53.882605: Validation loss did not improve from -0.51180. Patience: 51/50
2024-12-14 04:45:53.883600: train_loss -0.7398
2024-12-14 04:45:53.884391: val_loss -0.4476
2024-12-14 04:45:53.885030: Pseudo dice [0.6935]
2024-12-14 04:45:53.885687: Epoch time: 508.2 s
2024-12-14 04:45:55.353518: 
2024-12-14 04:45:55.354445: Epoch 79
2024-12-14 04:45:55.355201: Current learning rate: 0.0051
2024-12-14 04:54:19.953593: Validation loss did not improve from -0.51180. Patience: 52/50
2024-12-14 04:54:19.954315: train_loss -0.7473
2024-12-14 04:54:19.954990: val_loss -0.4042
2024-12-14 04:54:19.955717: Pseudo dice [0.6948]
2024-12-14 04:54:19.956484: Epoch time: 504.6 s
2024-12-14 04:54:21.730519: 
2024-12-14 04:54:21.731354: Epoch 80
2024-12-14 04:54:21.732081: Current learning rate: 0.00504
2024-12-14 05:02:38.400639: Validation loss did not improve from -0.51180. Patience: 53/50
2024-12-14 05:02:38.401620: train_loss -0.7459
2024-12-14 05:02:38.402407: val_loss -0.456
2024-12-14 05:02:38.403137: Pseudo dice [0.7028]
2024-12-14 05:02:38.403935: Epoch time: 496.67 s
2024-12-14 05:02:40.174904: 
2024-12-14 05:02:40.175883: Epoch 81
2024-12-14 05:02:40.176808: Current learning rate: 0.00497
2024-12-14 05:11:14.631222: Validation loss did not improve from -0.51180. Patience: 54/50
2024-12-14 05:11:14.631905: train_loss -0.7559
2024-12-14 05:11:14.632828: val_loss -0.4863
2024-12-14 05:11:14.633631: Pseudo dice [0.7179]
2024-12-14 05:11:14.634465: Epoch time: 514.46 s
2024-12-14 05:11:16.056048: 
2024-12-14 05:11:16.057246: Epoch 82
2024-12-14 05:11:16.058033: Current learning rate: 0.00491
2024-12-14 05:20:13.760826: Validation loss did not improve from -0.51180. Patience: 55/50
2024-12-14 05:20:13.761554: train_loss -0.7521
2024-12-14 05:20:13.762349: val_loss -0.4748
2024-12-14 05:20:13.763066: Pseudo dice [0.7168]
2024-12-14 05:20:13.764011: Epoch time: 537.71 s
2024-12-14 05:20:15.129040: 
2024-12-14 05:20:15.130241: Epoch 83
2024-12-14 05:20:15.131158: Current learning rate: 0.00484
2024-12-14 05:29:12.834761: Validation loss did not improve from -0.51180. Patience: 56/50
2024-12-14 05:29:12.835492: train_loss -0.7537
2024-12-14 05:29:12.836218: val_loss -0.4809
2024-12-14 05:29:12.836886: Pseudo dice [0.7241]
2024-12-14 05:29:12.837586: Epoch time: 537.71 s
2024-12-14 05:29:14.326055: 
2024-12-14 05:29:14.327484: Epoch 84
2024-12-14 05:29:14.328363: Current learning rate: 0.00478
2024-12-14 05:38:08.498322: Validation loss did not improve from -0.51180. Patience: 57/50
2024-12-14 05:38:08.501669: train_loss -0.7558
2024-12-14 05:38:08.502516: val_loss -0.4794
2024-12-14 05:38:08.503240: Pseudo dice [0.7195]
2024-12-14 05:38:08.504344: Epoch time: 534.18 s
2024-12-14 05:38:08.922247: Yayy! New best EMA pseudo Dice: 0.7119
2024-12-14 05:38:10.706288: 
2024-12-14 05:38:10.707496: Epoch 85
2024-12-14 05:38:10.708297: Current learning rate: 0.00471
2024-12-14 05:46:42.984807: Validation loss did not improve from -0.51180. Patience: 58/50
2024-12-14 05:46:42.986172: train_loss -0.7493
2024-12-14 05:46:42.986916: val_loss -0.4111
2024-12-14 05:46:42.987617: Pseudo dice [0.685]
2024-12-14 05:46:42.988245: Epoch time: 512.28 s
2024-12-14 05:46:44.365168: 
2024-12-14 05:46:44.366012: Epoch 86
2024-12-14 05:46:44.366699: Current learning rate: 0.00465
2024-12-14 05:55:29.383778: Validation loss did not improve from -0.51180. Patience: 59/50
2024-12-14 05:55:29.384411: train_loss -0.7559
2024-12-14 05:55:29.385139: val_loss -0.4932
2024-12-14 05:55:29.385797: Pseudo dice [0.7219]
2024-12-14 05:55:29.386418: Epoch time: 525.02 s
2024-12-14 05:55:30.719372: 
2024-12-14 05:55:30.720186: Epoch 87
2024-12-14 05:55:30.721005: Current learning rate: 0.00458
2024-12-14 06:04:14.727383: Validation loss did not improve from -0.51180. Patience: 60/50
2024-12-14 06:04:14.728141: train_loss -0.7564
2024-12-14 06:04:14.728880: val_loss -0.4086
2024-12-14 06:04:14.729562: Pseudo dice [0.6893]
2024-12-14 06:04:14.730191: Epoch time: 524.01 s
2024-12-14 06:04:16.062453: 
2024-12-14 06:04:16.063694: Epoch 88
2024-12-14 06:04:16.064536: Current learning rate: 0.00452
2024-12-14 06:12:43.755061: Validation loss did not improve from -0.51180. Patience: 61/50
2024-12-14 06:12:43.755743: train_loss -0.7578
2024-12-14 06:12:43.756613: val_loss -0.3981
2024-12-14 06:12:43.757400: Pseudo dice [0.6709]
2024-12-14 06:12:43.758207: Epoch time: 507.69 s
2024-12-14 06:12:45.117594: 
2024-12-14 06:12:45.118600: Epoch 89
2024-12-14 06:12:45.119344: Current learning rate: 0.00445
2024-12-14 06:21:01.565997: Validation loss did not improve from -0.51180. Patience: 62/50
2024-12-14 06:21:01.566682: train_loss -0.7595
2024-12-14 06:21:01.567475: val_loss -0.463
2024-12-14 06:21:01.568221: Pseudo dice [0.7046]
2024-12-14 06:21:01.568971: Epoch time: 496.45 s
2024-12-14 06:21:03.352201: 
2024-12-14 06:21:03.353323: Epoch 90
2024-12-14 06:21:03.354109: Current learning rate: 0.00438
2024-12-14 06:29:25.559680: Validation loss did not improve from -0.51180. Patience: 63/50
2024-12-14 06:29:25.560532: train_loss -0.7624
2024-12-14 06:29:25.561307: val_loss -0.425
2024-12-14 06:29:25.562033: Pseudo dice [0.6962]
2024-12-14 06:29:25.562798: Epoch time: 502.21 s
2024-12-14 06:29:27.006390: 
2024-12-14 06:29:27.007278: Epoch 91
2024-12-14 06:29:27.007928: Current learning rate: 0.00432
2024-12-14 06:38:19.062283: Validation loss did not improve from -0.51180. Patience: 64/50
2024-12-14 06:38:19.063218: train_loss -0.7636
2024-12-14 06:38:19.064048: val_loss -0.4754
2024-12-14 06:38:19.064755: Pseudo dice [0.7224]
2024-12-14 06:38:19.065593: Epoch time: 532.06 s
2024-12-14 06:38:20.406402: 
2024-12-14 06:38:20.407308: Epoch 92
2024-12-14 06:38:20.408075: Current learning rate: 0.00425
2024-12-14 06:46:51.442645: Validation loss did not improve from -0.51180. Patience: 65/50
2024-12-14 06:46:51.446377: train_loss -0.7631
2024-12-14 06:46:51.448198: val_loss -0.4816
2024-12-14 06:46:51.448922: Pseudo dice [0.7199]
2024-12-14 06:46:51.450133: Epoch time: 511.04 s
2024-12-14 06:46:53.233036: 
2024-12-14 06:46:53.233927: Epoch 93
2024-12-14 06:46:53.234707: Current learning rate: 0.00419
2024-12-14 06:55:20.373391: Validation loss did not improve from -0.51180. Patience: 66/50
2024-12-14 06:55:20.374920: train_loss -0.7631
2024-12-14 06:55:20.375879: val_loss -0.4531
2024-12-14 06:55:20.376827: Pseudo dice [0.7083]
2024-12-14 06:55:20.377816: Epoch time: 507.14 s
2024-12-14 06:55:21.779345: 
2024-12-14 06:55:21.781142: Epoch 94
2024-12-14 06:55:21.782461: Current learning rate: 0.00412
2024-12-14 07:04:02.005173: Validation loss did not improve from -0.51180. Patience: 67/50
2024-12-14 07:04:02.006252: train_loss -0.7628
2024-12-14 07:04:02.006938: val_loss -0.4817
2024-12-14 07:04:02.007535: Pseudo dice [0.7165]
2024-12-14 07:04:02.008532: Epoch time: 520.23 s
2024-12-14 07:04:04.000437: 
2024-12-14 07:04:04.001798: Epoch 95
2024-12-14 07:04:04.002563: Current learning rate: 0.00405
2024-12-14 07:12:39.805486: Validation loss did not improve from -0.51180. Patience: 68/50
2024-12-14 07:12:39.806522: train_loss -0.7651
2024-12-14 07:12:39.807443: val_loss -0.4059
2024-12-14 07:12:39.808256: Pseudo dice [0.676]
2024-12-14 07:12:39.809111: Epoch time: 515.81 s
2024-12-14 07:12:41.144661: 
2024-12-14 07:12:41.145799: Epoch 96
2024-12-14 07:12:41.146559: Current learning rate: 0.00399
2024-12-14 07:21:08.817099: Validation loss did not improve from -0.51180. Patience: 69/50
2024-12-14 07:21:08.818062: train_loss -0.7655
2024-12-14 07:21:08.818790: val_loss -0.4451
2024-12-14 07:21:08.819418: Pseudo dice [0.71]
2024-12-14 07:21:08.820142: Epoch time: 507.67 s
2024-12-14 07:21:10.187767: 
2024-12-14 07:21:10.189049: Epoch 97
2024-12-14 07:21:10.189742: Current learning rate: 0.00392
2024-12-14 07:29:42.971411: Validation loss did not improve from -0.51180. Patience: 70/50
2024-12-14 07:29:42.972322: train_loss -0.7702
2024-12-14 07:29:42.973039: val_loss -0.4235
2024-12-14 07:29:42.973661: Pseudo dice [0.6842]
2024-12-14 07:29:42.974280: Epoch time: 512.79 s
2024-12-14 07:29:44.359771: 
2024-12-14 07:29:44.361001: Epoch 98
2024-12-14 07:29:44.361770: Current learning rate: 0.00385
2024-12-14 07:38:30.618964: Validation loss did not improve from -0.51180. Patience: 71/50
2024-12-14 07:38:30.619981: train_loss -0.7698
2024-12-14 07:38:30.620897: val_loss -0.4209
2024-12-14 07:38:30.621732: Pseudo dice [0.691]
2024-12-14 07:38:30.622630: Epoch time: 526.26 s
2024-12-14 07:38:31.980184: 
2024-12-14 07:38:31.981471: Epoch 99
2024-12-14 07:38:31.982194: Current learning rate: 0.00379
2024-12-14 07:47:17.954267: Validation loss did not improve from -0.51180. Patience: 72/50
2024-12-14 07:47:17.958127: train_loss -0.7703
2024-12-14 07:47:17.959716: val_loss -0.4831
2024-12-14 07:47:17.960672: Pseudo dice [0.7206]
2024-12-14 07:47:17.961862: Epoch time: 525.98 s
2024-12-14 07:47:19.763931: 
2024-12-14 07:47:19.765225: Epoch 100
2024-12-14 07:47:19.765974: Current learning rate: 0.00372
2024-12-14 07:56:08.339468: Validation loss did not improve from -0.51180. Patience: 73/50
2024-12-14 07:56:08.340538: train_loss -0.7717
2024-12-14 07:56:08.341812: val_loss -0.4326
2024-12-14 07:56:08.342592: Pseudo dice [0.6894]
2024-12-14 07:56:08.343195: Epoch time: 528.58 s
2024-12-14 07:56:09.746353: 
2024-12-14 07:56:09.747576: Epoch 101
2024-12-14 07:56:09.748322: Current learning rate: 0.00365
2024-12-14 08:04:46.339847: Validation loss did not improve from -0.51180. Patience: 74/50
2024-12-14 08:04:46.340894: train_loss -0.7735
2024-12-14 08:04:46.341678: val_loss -0.4117
2024-12-14 08:04:46.342467: Pseudo dice [0.6895]
2024-12-14 08:04:46.343314: Epoch time: 516.6 s
2024-12-14 08:04:47.737758: 
2024-12-14 08:04:47.739084: Epoch 102
2024-12-14 08:04:47.739930: Current learning rate: 0.00359
2024-12-14 08:13:20.986065: Validation loss did not improve from -0.51180. Patience: 75/50
2024-12-14 08:13:20.987006: train_loss -0.772
2024-12-14 08:13:20.987813: val_loss -0.4455
2024-12-14 08:13:20.988648: Pseudo dice [0.7014]
2024-12-14 08:13:20.989452: Epoch time: 513.25 s
2024-12-14 08:13:22.408429: 
2024-12-14 08:13:22.409608: Epoch 103
2024-12-14 08:13:22.410390: Current learning rate: 0.00352
2024-12-14 08:22:17.647393: Validation loss did not improve from -0.51180. Patience: 76/50
2024-12-14 08:22:17.648220: train_loss -0.7659
2024-12-14 08:22:17.649161: val_loss -0.4284
2024-12-14 08:22:17.649749: Pseudo dice [0.7001]
2024-12-14 08:22:17.650543: Epoch time: 535.24 s
2024-12-14 08:22:19.532145: 
2024-12-14 08:22:19.533462: Epoch 104
2024-12-14 08:22:19.534116: Current learning rate: 0.00345
2024-12-14 08:30:58.364496: Validation loss did not improve from -0.51180. Patience: 77/50
2024-12-14 08:30:58.365363: train_loss -0.7738
2024-12-14 08:30:58.366287: val_loss -0.4363
2024-12-14 08:30:58.367244: Pseudo dice [0.7033]
2024-12-14 08:30:58.368119: Epoch time: 518.83 s
2024-12-14 08:31:00.150011: 
2024-12-14 08:31:00.151501: Epoch 105
2024-12-14 08:31:00.152587: Current learning rate: 0.00338
2024-12-14 08:40:04.641431: Validation loss did not improve from -0.51180. Patience: 78/50
2024-12-14 08:40:04.642515: train_loss -0.7758
2024-12-14 08:40:04.643770: val_loss -0.4197
2024-12-14 08:40:04.644766: Pseudo dice [0.689]
2024-12-14 08:40:04.645700: Epoch time: 544.49 s
2024-12-14 08:40:06.025753: 
2024-12-14 08:40:06.027216: Epoch 106
2024-12-14 08:40:06.028250: Current learning rate: 0.00332
2024-12-14 08:48:47.527903: Validation loss did not improve from -0.51180. Patience: 79/50
2024-12-14 08:48:47.529310: train_loss -0.7734
2024-12-14 08:48:47.530088: val_loss -0.4652
2024-12-14 08:48:47.530801: Pseudo dice [0.7106]
2024-12-14 08:48:47.531507: Epoch time: 521.5 s
2024-12-14 08:48:48.910757: 
2024-12-14 08:48:48.912071: Epoch 107
2024-12-14 08:48:48.913124: Current learning rate: 0.00325
2024-12-14 08:57:31.160830: Validation loss did not improve from -0.51180. Patience: 80/50
2024-12-14 08:57:31.162289: train_loss -0.771
2024-12-14 08:57:31.163754: val_loss -0.4121
2024-12-14 08:57:31.164550: Pseudo dice [0.6866]
2024-12-14 08:57:31.165607: Epoch time: 522.25 s
2024-12-14 08:57:32.555362: 
2024-12-14 08:57:32.556694: Epoch 108
2024-12-14 08:57:32.557427: Current learning rate: 0.00318
2024-12-14 09:06:09.219135: Validation loss did not improve from -0.51180. Patience: 81/50
2024-12-14 09:06:09.220318: train_loss -0.7745
2024-12-14 09:06:09.221333: val_loss -0.4674
2024-12-14 09:06:09.221997: Pseudo dice [0.7191]
2024-12-14 09:06:09.222668: Epoch time: 516.67 s
2024-12-14 09:06:10.742044: 
2024-12-14 09:06:10.744707: Epoch 109
2024-12-14 09:06:10.745687: Current learning rate: 0.00311
2024-12-14 09:14:54.853237: Validation loss did not improve from -0.51180. Patience: 82/50
2024-12-14 09:14:54.854208: train_loss -0.7749
2024-12-14 09:14:54.855144: val_loss -0.4259
2024-12-14 09:14:54.855815: Pseudo dice [0.6953]
2024-12-14 09:14:54.856420: Epoch time: 524.11 s
2024-12-14 09:14:56.584244: 
2024-12-14 09:14:56.585499: Epoch 110
2024-12-14 09:14:56.586208: Current learning rate: 0.00304
2024-12-14 09:23:20.763906: Validation loss did not improve from -0.51180. Patience: 83/50
2024-12-14 09:23:20.766079: train_loss -0.7793
2024-12-14 09:23:20.767159: val_loss -0.4488
2024-12-14 09:23:20.768036: Pseudo dice [0.712]
2024-12-14 09:23:20.769022: Epoch time: 504.18 s
2024-12-14 09:23:22.204494: 
2024-12-14 09:23:22.206031: Epoch 111
2024-12-14 09:23:22.207211: Current learning rate: 0.00297
2024-12-14 09:31:39.850171: Validation loss did not improve from -0.51180. Patience: 84/50
2024-12-14 09:31:39.851130: train_loss -0.7816
2024-12-14 09:31:39.852018: val_loss -0.4245
2024-12-14 09:31:39.852797: Pseudo dice [0.6901]
2024-12-14 09:31:39.853756: Epoch time: 497.65 s
2024-12-14 09:31:41.251766: 
2024-12-14 09:31:41.253158: Epoch 112
2024-12-14 09:31:41.253954: Current learning rate: 0.00291
2024-12-14 09:40:20.874361: Validation loss did not improve from -0.51180. Patience: 85/50
2024-12-14 09:40:20.875324: train_loss -0.778
2024-12-14 09:40:20.876239: val_loss -0.4597
2024-12-14 09:40:20.876957: Pseudo dice [0.7142]
2024-12-14 09:40:20.877639: Epoch time: 519.62 s
2024-12-14 09:40:22.262323: 
2024-12-14 09:40:22.263712: Epoch 113
2024-12-14 09:40:22.264497: Current learning rate: 0.00284
2024-12-14 09:49:00.227812: Validation loss did not improve from -0.51180. Patience: 86/50
2024-12-14 09:49:00.228759: train_loss -0.7797
2024-12-14 09:49:00.229540: val_loss -0.4613
2024-12-14 09:49:00.230279: Pseudo dice [0.7189]
2024-12-14 09:49:00.231077: Epoch time: 517.97 s
2024-12-14 09:49:01.645538: 
2024-12-14 09:49:01.646852: Epoch 114
2024-12-14 09:49:01.647613: Current learning rate: 0.00277
2024-12-14 09:57:54.586293: Validation loss did not improve from -0.51180. Patience: 87/50
2024-12-14 09:57:54.590129: train_loss -0.781
2024-12-14 09:57:54.591748: val_loss -0.414
2024-12-14 09:57:54.592542: Pseudo dice [0.6872]
2024-12-14 09:57:54.593423: Epoch time: 532.94 s
2024-12-14 09:57:56.894423: 
2024-12-14 09:57:56.895875: Epoch 115
2024-12-14 09:57:56.896750: Current learning rate: 0.0027
2024-12-14 10:06:05.002146: Validation loss did not improve from -0.51180. Patience: 88/50
2024-12-14 10:06:05.003068: train_loss -0.7794
2024-12-14 10:06:05.003799: val_loss -0.397
2024-12-14 10:06:05.004492: Pseudo dice [0.6724]
2024-12-14 10:06:05.005184: Epoch time: 488.11 s
2024-12-14 10:06:06.419124: 
2024-12-14 10:06:06.420354: Epoch 116
2024-12-14 10:06:06.421082: Current learning rate: 0.00263
2024-12-14 10:14:35.302095: Validation loss did not improve from -0.51180. Patience: 89/50
2024-12-14 10:14:35.303013: train_loss -0.7806
2024-12-14 10:14:35.303855: val_loss -0.4359
2024-12-14 10:14:35.304738: Pseudo dice [0.6969]
2024-12-14 10:14:35.305633: Epoch time: 508.89 s
2024-12-14 10:14:36.725106: 
2024-12-14 10:14:36.726380: Epoch 117
2024-12-14 10:14:36.727105: Current learning rate: 0.00256
2024-12-14 10:23:29.141183: Validation loss did not improve from -0.51180. Patience: 90/50
2024-12-14 10:23:29.142186: train_loss -0.7837
2024-12-14 10:23:29.142927: val_loss -0.4482
2024-12-14 10:23:29.143572: Pseudo dice [0.7085]
2024-12-14 10:23:29.144333: Epoch time: 532.42 s
2024-12-14 10:23:30.570068: 
2024-12-14 10:23:30.571294: Epoch 118
2024-12-14 10:23:30.572124: Current learning rate: 0.00249
2024-12-14 10:32:28.656860: Validation loss did not improve from -0.51180. Patience: 91/50
2024-12-14 10:32:28.657824: train_loss -0.7841
2024-12-14 10:32:28.658693: val_loss -0.4712
2024-12-14 10:32:28.659350: Pseudo dice [0.7171]
2024-12-14 10:32:28.660142: Epoch time: 538.09 s
2024-12-14 10:32:30.095670: 
2024-12-14 10:32:30.097010: Epoch 119
2024-12-14 10:32:30.098118: Current learning rate: 0.00242
2024-12-14 10:41:22.544798: Validation loss did not improve from -0.51180. Patience: 92/50
2024-12-14 10:41:22.545706: train_loss -0.7855
2024-12-14 10:41:22.546549: val_loss -0.4537
2024-12-14 10:41:22.547270: Pseudo dice [0.7148]
2024-12-14 10:41:22.548126: Epoch time: 532.45 s
2024-12-14 10:41:24.357741: 
2024-12-14 10:41:24.359050: Epoch 120
2024-12-14 10:41:24.359760: Current learning rate: 0.00235
2024-12-14 10:49:52.351226: Validation loss did not improve from -0.51180. Patience: 93/50
2024-12-14 10:49:52.352195: train_loss -0.7847
2024-12-14 10:49:52.352967: val_loss -0.4937
2024-12-14 10:49:52.353738: Pseudo dice [0.7323]
2024-12-14 10:49:52.354407: Epoch time: 508.0 s
2024-12-14 10:49:53.806884: 
2024-12-14 10:49:53.808025: Epoch 121
2024-12-14 10:49:53.808682: Current learning rate: 0.00228
2024-12-14 10:58:41.882545: Validation loss did not improve from -0.51180. Patience: 94/50
2024-12-14 10:58:41.884307: train_loss -0.7835
2024-12-14 10:58:41.885188: val_loss -0.4652
2024-12-14 10:58:41.885951: Pseudo dice [0.7118]
2024-12-14 10:58:41.886649: Epoch time: 528.08 s
2024-12-14 10:58:43.315497: 
2024-12-14 10:58:43.316523: Epoch 122
2024-12-14 10:58:43.317214: Current learning rate: 0.00221
2024-12-14 11:07:19.050785: Validation loss did not improve from -0.51180. Patience: 95/50
2024-12-14 11:07:19.051632: train_loss -0.7871
2024-12-14 11:07:19.052653: val_loss -0.4693
2024-12-14 11:07:19.053339: Pseudo dice [0.7162]
2024-12-14 11:07:19.054099: Epoch time: 515.74 s
2024-12-14 11:07:20.478416: 
2024-12-14 11:07:20.479403: Epoch 123
2024-12-14 11:07:20.480135: Current learning rate: 0.00214
2024-12-14 11:15:29.323555: Validation loss did not improve from -0.51180. Patience: 96/50
2024-12-14 11:15:29.324342: train_loss -0.787
2024-12-14 11:15:29.325247: val_loss -0.4344
2024-12-14 11:15:29.326005: Pseudo dice [0.7011]
2024-12-14 11:15:29.326810: Epoch time: 488.85 s
2024-12-14 11:15:30.719915: 
2024-12-14 11:15:30.720788: Epoch 124
2024-12-14 11:15:30.721463: Current learning rate: 0.00207
2024-12-14 11:24:02.363988: Validation loss did not improve from -0.51180. Patience: 97/50
2024-12-14 11:24:02.364976: train_loss -0.7867
2024-12-14 11:24:02.365760: val_loss -0.4373
2024-12-14 11:24:02.366451: Pseudo dice [0.7013]
2024-12-14 11:24:02.367200: Epoch time: 511.65 s
2024-12-14 11:24:04.624068: 
2024-12-14 11:24:04.625206: Epoch 125
2024-12-14 11:24:04.626092: Current learning rate: 0.00199
2024-12-14 11:32:58.286425: Validation loss did not improve from -0.51180. Patience: 98/50
2024-12-14 11:32:58.287382: train_loss -0.7894
2024-12-14 11:32:58.288082: val_loss -0.4765
2024-12-14 11:32:58.288717: Pseudo dice [0.7324]
2024-12-14 11:32:58.289502: Epoch time: 533.66 s
2024-12-14 11:32:59.743585: 
2024-12-14 11:32:59.745016: Epoch 126
2024-12-14 11:32:59.745760: Current learning rate: 0.00192
2024-12-14 11:41:53.473670: Validation loss did not improve from -0.51180. Patience: 99/50
2024-12-14 11:41:53.475422: train_loss -0.7894
2024-12-14 11:41:53.476222: val_loss -0.445
2024-12-14 11:41:53.476863: Pseudo dice [0.7042]
2024-12-14 11:41:53.477539: Epoch time: 533.73 s
2024-12-14 11:41:54.887763: 
2024-12-14 11:41:54.888887: Epoch 127
2024-12-14 11:41:54.889563: Current learning rate: 0.00185
2024-12-14 11:50:47.621283: Validation loss did not improve from -0.51180. Patience: 100/50
2024-12-14 11:50:47.622302: train_loss -0.7878
2024-12-14 11:50:47.623110: val_loss -0.4337
2024-12-14 11:50:47.623759: Pseudo dice [0.7025]
2024-12-14 11:50:47.624465: Epoch time: 532.74 s
2024-12-14 11:50:49.101610: 
2024-12-14 11:50:49.102915: Epoch 128
2024-12-14 11:50:49.103759: Current learning rate: 0.00178
2024-12-14 12:00:00.500230: Validation loss did not improve from -0.51180. Patience: 101/50
2024-12-14 12:00:00.501147: train_loss -0.7893
2024-12-14 12:00:00.501985: val_loss -0.4522
2024-12-14 12:00:00.502885: Pseudo dice [0.7056]
2024-12-14 12:00:00.504013: Epoch time: 551.4 s
2024-12-14 12:00:01.890733: 
2024-12-14 12:00:01.892111: Epoch 129
2024-12-14 12:00:01.892928: Current learning rate: 0.0017
2024-12-14 12:08:58.989883: Validation loss did not improve from -0.51180. Patience: 102/50
2024-12-14 12:08:59.010535: train_loss -0.7911
2024-12-14 12:08:59.013017: val_loss -0.4309
2024-12-14 12:08:59.013731: Pseudo dice [0.7097]
2024-12-14 12:08:59.014817: Epoch time: 537.12 s
2024-12-14 12:09:00.922505: 
2024-12-14 12:09:00.923738: Epoch 130
2024-12-14 12:09:00.924384: Current learning rate: 0.00163
2024-12-14 12:17:41.118059: Validation loss did not improve from -0.51180. Patience: 103/50
2024-12-14 12:17:41.119221: train_loss -0.7882
2024-12-14 12:17:41.119942: val_loss -0.4729
2024-12-14 12:17:41.120607: Pseudo dice [0.7148]
2024-12-14 12:17:41.121317: Epoch time: 520.2 s
2024-12-14 12:17:42.545344: 
2024-12-14 12:17:42.546632: Epoch 131
2024-12-14 12:17:42.547437: Current learning rate: 0.00156
2024-12-14 12:26:33.194184: Validation loss did not improve from -0.51180. Patience: 104/50
2024-12-14 12:26:33.195217: train_loss -0.7912
2024-12-14 12:26:33.196417: val_loss -0.4565
2024-12-14 12:26:33.197204: Pseudo dice [0.706]
2024-12-14 12:26:33.197856: Epoch time: 530.65 s
2024-12-14 12:26:34.622882: 
2024-12-14 12:26:34.624244: Epoch 132
2024-12-14 12:26:34.625026: Current learning rate: 0.00148
2024-12-14 12:35:42.703000: Validation loss did not improve from -0.51180. Patience: 105/50
2024-12-14 12:35:42.703969: train_loss -0.7928
2024-12-14 12:35:42.704752: val_loss -0.4488
2024-12-14 12:35:42.705500: Pseudo dice [0.7027]
2024-12-14 12:35:42.706214: Epoch time: 548.08 s
2024-12-14 12:35:44.102668: 
2024-12-14 12:35:44.103910: Epoch 133
2024-12-14 12:35:44.104693: Current learning rate: 0.00141
2024-12-14 12:44:47.866246: Validation loss did not improve from -0.51180. Patience: 106/50
2024-12-14 12:44:47.867185: train_loss -0.7934
2024-12-14 12:44:47.868000: val_loss -0.4856
2024-12-14 12:44:47.868632: Pseudo dice [0.7297]
2024-12-14 12:44:47.869319: Epoch time: 543.77 s
2024-12-14 12:44:49.266468: 
2024-12-14 12:44:49.267839: Epoch 134
2024-12-14 12:44:49.268717: Current learning rate: 0.00133
2024-12-14 12:53:25.096546: Validation loss did not improve from -0.51180. Patience: 107/50
2024-12-14 12:53:25.097477: train_loss -0.7918
2024-12-14 12:53:25.098214: val_loss -0.4552
2024-12-14 12:53:25.098927: Pseudo dice [0.7134]
2024-12-14 12:53:25.099537: Epoch time: 515.83 s
2024-12-14 12:53:26.899421: 
2024-12-14 12:53:26.900589: Epoch 135
2024-12-14 12:53:26.901334: Current learning rate: 0.00126
2024-12-14 13:01:43.341409: Validation loss did not improve from -0.51180. Patience: 108/50
2024-12-14 13:01:43.342346: train_loss -0.7938
2024-12-14 13:01:43.343088: val_loss -0.448
2024-12-14 13:01:43.343719: Pseudo dice [0.711]
2024-12-14 13:01:43.344579: Epoch time: 496.44 s
2024-12-14 13:01:45.568718: 
2024-12-14 13:01:45.570005: Epoch 136
2024-12-14 13:01:45.570665: Current learning rate: 0.00118
2024-12-14 13:10:01.926924: Validation loss did not improve from -0.51180. Patience: 109/50
2024-12-14 13:10:01.928354: train_loss -0.7956
2024-12-14 13:10:01.929428: val_loss -0.4408
2024-12-14 13:10:01.930408: Pseudo dice [0.7169]
2024-12-14 13:10:01.931227: Epoch time: 496.36 s
2024-12-14 13:10:03.361123: 
2024-12-14 13:10:03.366413: Epoch 137
2024-12-14 13:10:03.367448: Current learning rate: 0.00111
2024-12-14 13:18:16.917743: Validation loss did not improve from -0.51180. Patience: 110/50
2024-12-14 13:18:16.918861: train_loss -0.7936
2024-12-14 13:18:16.919644: val_loss -0.4693
2024-12-14 13:18:16.920241: Pseudo dice [0.7153]
2024-12-14 13:18:16.920852: Epoch time: 493.56 s
2024-12-14 13:18:18.380920: 
2024-12-14 13:18:18.382226: Epoch 138
2024-12-14 13:18:18.383012: Current learning rate: 0.00103
2024-12-14 13:26:28.815637: Validation loss did not improve from -0.51180. Patience: 111/50
2024-12-14 13:26:28.816638: train_loss -0.7944
2024-12-14 13:26:28.817428: val_loss -0.4754
2024-12-14 13:26:28.818170: Pseudo dice [0.7206]
2024-12-14 13:26:28.818822: Epoch time: 490.44 s
2024-12-14 13:26:28.819592: Yayy! New best EMA pseudo Dice: 0.7123
2024-12-14 13:26:30.602714: 
2024-12-14 13:26:30.603874: Epoch 139
2024-12-14 13:26:30.604762: Current learning rate: 0.00095
2024-12-14 13:33:57.237348: Validation loss did not improve from -0.51180. Patience: 112/50
2024-12-14 13:33:57.238162: train_loss -0.7965
2024-12-14 13:33:57.239080: val_loss -0.47
2024-12-14 13:33:57.239923: Pseudo dice [0.7195]
2024-12-14 13:33:57.240650: Epoch time: 446.64 s
2024-12-14 13:33:57.627112: Yayy! New best EMA pseudo Dice: 0.7131
2024-12-14 13:33:59.475665: 
2024-12-14 13:33:59.476993: Epoch 140
2024-12-14 13:33:59.477738: Current learning rate: 0.00087
2024-12-14 13:41:52.994630: Validation loss did not improve from -0.51180. Patience: 113/50
2024-12-14 13:41:52.995694: train_loss -0.7938
2024-12-14 13:41:52.996437: val_loss -0.4434
2024-12-14 13:41:52.997129: Pseudo dice [0.7071]
2024-12-14 13:41:52.997967: Epoch time: 473.52 s
2024-12-14 13:41:54.447696: 
2024-12-14 13:41:54.449002: Epoch 141
2024-12-14 13:41:54.449724: Current learning rate: 0.00079
2024-12-14 13:49:55.814710: Validation loss did not improve from -0.51180. Patience: 114/50
2024-12-14 13:49:55.815704: train_loss -0.7957
2024-12-14 13:49:55.816639: val_loss -0.4229
2024-12-14 13:49:55.817490: Pseudo dice [0.6996]
2024-12-14 13:49:55.818451: Epoch time: 481.37 s
2024-12-14 13:49:57.234113: 
2024-12-14 13:49:57.235634: Epoch 142
2024-12-14 13:49:57.236555: Current learning rate: 0.00071
2024-12-14 13:57:41.299116: Validation loss did not improve from -0.51180. Patience: 115/50
2024-12-14 13:57:41.300114: train_loss -0.7961
2024-12-14 13:57:41.301083: val_loss -0.4572
2024-12-14 13:57:41.301772: Pseudo dice [0.7236]
2024-12-14 13:57:41.302577: Epoch time: 464.07 s
2024-12-14 13:57:42.743818: 
2024-12-14 13:57:42.745051: Epoch 143
2024-12-14 13:57:42.745752: Current learning rate: 0.00063
2024-12-14 14:05:43.103430: Validation loss did not improve from -0.51180. Patience: 116/50
2024-12-14 14:05:43.104303: train_loss -0.7989
2024-12-14 14:05:43.105291: val_loss -0.4764
2024-12-14 14:05:43.106040: Pseudo dice [0.7242]
2024-12-14 14:05:43.106848: Epoch time: 480.36 s
2024-12-14 14:05:43.107560: Yayy! New best EMA pseudo Dice: 0.7136
2024-12-14 14:05:44.954446: 
2024-12-14 14:05:44.955833: Epoch 144
2024-12-14 14:05:44.956679: Current learning rate: 0.00055
2024-12-14 14:13:39.612421: Validation loss did not improve from -0.51180. Patience: 117/50
2024-12-14 14:13:39.613410: train_loss -0.7966
2024-12-14 14:13:39.614370: val_loss -0.4379
2024-12-14 14:13:39.615211: Pseudo dice [0.7197]
2024-12-14 14:13:39.616036: Epoch time: 474.66 s
2024-12-14 14:13:40.027316: Yayy! New best EMA pseudo Dice: 0.7142
2024-12-14 14:13:41.872167: 
2024-12-14 14:13:41.873627: Epoch 145
2024-12-14 14:13:41.874675: Current learning rate: 0.00047
2024-12-14 14:21:17.639643: Validation loss did not improve from -0.51180. Patience: 118/50
2024-12-14 14:21:17.640633: train_loss -0.7963
2024-12-14 14:21:17.641486: val_loss -0.4512
2024-12-14 14:21:17.642396: Pseudo dice [0.7121]
2024-12-14 14:21:17.643370: Epoch time: 455.77 s
2024-12-14 14:21:19.692013: 
2024-12-14 14:21:19.693363: Epoch 146
2024-12-14 14:21:19.694178: Current learning rate: 0.00038
2024-12-14 14:27:57.989704: Validation loss did not improve from -0.51180. Patience: 119/50
2024-12-14 14:27:57.991100: train_loss -0.7976
2024-12-14 14:27:57.992262: val_loss -0.4774
2024-12-14 14:27:57.993248: Pseudo dice [0.7267]
2024-12-14 14:27:57.994196: Epoch time: 398.3 s
2024-12-14 14:27:57.995137: Yayy! New best EMA pseudo Dice: 0.7153
2024-12-14 14:27:59.961056: 
2024-12-14 14:27:59.962200: Epoch 147
2024-12-14 14:27:59.963341: Current learning rate: 0.0003
2024-12-14 14:34:57.408411: Validation loss did not improve from -0.51180. Patience: 120/50
2024-12-14 14:34:57.409416: train_loss -0.8
2024-12-14 14:34:57.410220: val_loss -0.4228
2024-12-14 14:34:57.410885: Pseudo dice [0.6967]
2024-12-14 14:34:57.411651: Epoch time: 417.45 s
2024-12-14 14:34:58.829480: 
2024-12-14 14:34:58.830729: Epoch 148
2024-12-14 14:34:58.831589: Current learning rate: 0.00021
2024-12-14 14:42:00.548369: Validation loss did not improve from -0.51180. Patience: 121/50
2024-12-14 14:42:00.549418: train_loss -0.8
2024-12-14 14:42:00.550738: val_loss -0.4424
2024-12-14 14:42:00.551907: Pseudo dice [0.6997]
2024-12-14 14:42:00.553027: Epoch time: 421.72 s
2024-12-14 14:42:02.003042: 
2024-12-14 14:42:02.004695: Epoch 149
2024-12-14 14:42:02.005815: Current learning rate: 0.00011
2024-12-14 14:48:49.334846: Validation loss did not improve from -0.51180. Patience: 122/50
2024-12-14 14:48:49.335847: train_loss -0.7983
2024-12-14 14:48:49.336778: val_loss -0.4607
2024-12-14 14:48:49.337517: Pseudo dice [0.717]
2024-12-14 14:48:49.338152: Epoch time: 407.33 s
2024-12-14 14:48:51.217800: Training done.
2024-12-14 14:48:51.598969: Using splits from existing split file: /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset307_Sohee_Calcium_OCT_CrossValidation/splits_final_60.json
2024-12-14 14:48:51.612990: The split file contains 5 splits.
2024-12-14 14:48:51.613807: Desired fold for training: 0
2024-12-14 14:48:51.614597: This split has 4 training and 4 validation cases.
2024-12-14 14:48:51.615526: predicting 101-045
2024-12-14 14:48:51.678025: 101-045, shape torch.Size([1, 375, 498, 498]), rank 0
2024-12-14 14:50:43.279055: predicting 701-013
2024-12-14 14:50:43.346560: 701-013, shape torch.Size([1, 375, 498, 498]), rank 0
2024-12-14 14:52:34.251626: predicting 704-003
2024-12-14 14:52:34.287534: 704-003, shape torch.Size([1, 375, 498, 498]), rank 0
2024-12-14 14:54:21.376020: predicting 706-005
2024-12-14 14:54:21.395336: 706-005, shape torch.Size([1, 375, 498, 498]), rank 0
2024-12-14 14:56:44.808713: Validation complete
2024-12-14 14:56:44.809408: Mean Validation Dice:  0.7087045275034254
2024-12-13 18:44:32.725703: unpacking done...
2024-12-13 18:44:32.737181: Unable to plot network architecture: nnUNet_compile is enabled!
2024-12-13 18:44:32.769682: 
2024-12-13 18:44:32.771007: Epoch 0
2024-12-13 18:44:32.771810: Current learning rate: 0.01
2024-12-13 18:52:07.372988: Validation loss improved from 1000.00000 to -0.17785! Patience: 0/50
2024-12-13 18:52:07.374062: train_loss -0.0843
2024-12-13 18:52:07.374909: val_loss -0.1779
2024-12-13 18:52:07.375638: Pseudo dice [0.5164]
2024-12-13 18:52:07.376603: Epoch time: 454.61 s
2024-12-13 18:52:07.377301: Yayy! New best EMA pseudo Dice: 0.5164
2024-12-13 18:52:09.017236: 
2024-12-13 18:52:09.018752: Epoch 1
2024-12-13 18:52:09.019718: Current learning rate: 0.00994
2024-12-13 18:59:01.666952: Validation loss improved from -0.17785 to -0.20123! Patience: 0/50
2024-12-13 18:59:01.668874: train_loss -0.2673
2024-12-13 18:59:01.670209: val_loss -0.2012
2024-12-13 18:59:01.671342: Pseudo dice [0.5668]
2024-12-13 18:59:01.672520: Epoch time: 412.65 s
2024-12-13 18:59:01.673615: Yayy! New best EMA pseudo Dice: 0.5214
2024-12-13 18:59:03.517796: 
2024-12-13 18:59:03.519282: Epoch 2
2024-12-13 18:59:03.520216: Current learning rate: 0.00988
2024-12-13 19:06:13.951812: Validation loss improved from -0.20123 to -0.25966! Patience: 0/50
2024-12-13 19:06:13.952592: train_loss -0.3201
2024-12-13 19:06:13.953401: val_loss -0.2597
2024-12-13 19:06:13.954201: Pseudo dice [0.5891]
2024-12-13 19:06:13.955049: Epoch time: 430.44 s
2024-12-13 19:06:13.955865: Yayy! New best EMA pseudo Dice: 0.5282
2024-12-13 19:06:15.821378: 
2024-12-13 19:06:15.822747: Epoch 3
2024-12-13 19:06:15.823811: Current learning rate: 0.00982
2024-12-13 19:13:29.587748: Validation loss improved from -0.25966 to -0.30647! Patience: 0/50
2024-12-13 19:13:29.588719: train_loss -0.3688
2024-12-13 19:13:29.589509: val_loss -0.3065
2024-12-13 19:13:29.590185: Pseudo dice [0.6125]
2024-12-13 19:13:29.591009: Epoch time: 433.77 s
2024-12-13 19:13:29.591704: Yayy! New best EMA pseudo Dice: 0.5366
2024-12-13 19:13:31.375228: 
2024-12-13 19:13:31.376509: Epoch 4
2024-12-13 19:13:31.377326: Current learning rate: 0.00976
2024-12-13 19:20:51.006879: Validation loss did not improve from -0.30647. Patience: 1/50
2024-12-13 19:20:51.007932: train_loss -0.3942
2024-12-13 19:20:51.008616: val_loss -0.28
2024-12-13 19:20:51.009307: Pseudo dice [0.6082]
2024-12-13 19:20:51.009997: Epoch time: 439.63 s
2024-12-13 19:20:51.352449: Yayy! New best EMA pseudo Dice: 0.5438
2024-12-13 19:20:53.319658: 
2024-12-13 19:20:53.321086: Epoch 5
2024-12-13 19:20:53.321930: Current learning rate: 0.0097
2024-12-13 19:28:08.790800: Validation loss improved from -0.30647 to -0.35430! Patience: 1/50
2024-12-13 19:28:08.791692: train_loss -0.4234
2024-12-13 19:28:08.792420: val_loss -0.3543
2024-12-13 19:28:08.793184: Pseudo dice [0.6395]
2024-12-13 19:28:08.793938: Epoch time: 435.47 s
2024-12-13 19:28:08.794621: Yayy! New best EMA pseudo Dice: 0.5533
2024-12-13 19:28:10.660319: 
2024-12-13 19:28:10.661463: Epoch 6
2024-12-13 19:28:10.662245: Current learning rate: 0.00964
2024-12-13 19:35:48.116470: Validation loss did not improve from -0.35430. Patience: 1/50
2024-12-13 19:35:48.117480: train_loss -0.4379
2024-12-13 19:35:48.118512: val_loss -0.326
2024-12-13 19:35:48.119450: Pseudo dice [0.6382]
2024-12-13 19:35:48.120440: Epoch time: 457.46 s
2024-12-13 19:35:48.121336: Yayy! New best EMA pseudo Dice: 0.5618
2024-12-13 19:35:49.930104: 
2024-12-13 19:35:49.931315: Epoch 7
2024-12-13 19:35:49.932176: Current learning rate: 0.00958
2024-12-13 19:43:14.735152: Validation loss improved from -0.35430 to -0.35841! Patience: 1/50
2024-12-13 19:43:14.736094: train_loss -0.4507
2024-12-13 19:43:14.737093: val_loss -0.3584
2024-12-13 19:43:14.738016: Pseudo dice [0.6415]
2024-12-13 19:43:14.738983: Epoch time: 444.81 s
2024-12-13 19:43:14.739970: Yayy! New best EMA pseudo Dice: 0.5698
2024-12-13 19:43:17.069099: 
2024-12-13 19:43:17.070512: Epoch 8
2024-12-13 19:43:17.071343: Current learning rate: 0.00952
2024-12-13 19:50:58.964357: Validation loss improved from -0.35841 to -0.36243! Patience: 0/50
2024-12-13 19:50:58.965469: train_loss -0.4735
2024-12-13 19:50:58.966198: val_loss -0.3624
2024-12-13 19:50:58.966973: Pseudo dice [0.6503]
2024-12-13 19:50:58.967710: Epoch time: 461.9 s
2024-12-13 19:50:58.968349: Yayy! New best EMA pseudo Dice: 0.5778
2024-12-13 19:51:00.784974: 
2024-12-13 19:51:00.786317: Epoch 9
2024-12-13 19:51:00.787019: Current learning rate: 0.00946
2024-12-13 19:59:14.573397: Validation loss improved from -0.36243 to -0.39216! Patience: 0/50
2024-12-13 19:59:14.574431: train_loss -0.4795
2024-12-13 19:59:14.575922: val_loss -0.3922
2024-12-13 19:59:14.576892: Pseudo dice [0.6613]
2024-12-13 19:59:14.577878: Epoch time: 493.79 s
2024-12-13 19:59:14.947180: Yayy! New best EMA pseudo Dice: 0.5862
2024-12-13 19:59:16.738088: 
2024-12-13 19:59:16.739399: Epoch 10
2024-12-13 19:59:16.740213: Current learning rate: 0.0094
2024-12-13 20:07:11.966631: Validation loss did not improve from -0.39216. Patience: 1/50
2024-12-13 20:07:11.967766: train_loss -0.5093
2024-12-13 20:07:11.968672: val_loss -0.27
2024-12-13 20:07:11.969346: Pseudo dice [0.5949]
2024-12-13 20:07:11.970083: Epoch time: 475.23 s
2024-12-13 20:07:11.970787: Yayy! New best EMA pseudo Dice: 0.5871
2024-12-13 20:07:13.683950: 
2024-12-13 20:07:13.685308: Epoch 11
2024-12-13 20:07:13.686160: Current learning rate: 0.00934
2024-12-13 20:14:39.674838: Validation loss did not improve from -0.39216. Patience: 2/50
2024-12-13 20:14:39.675680: train_loss -0.5271
2024-12-13 20:14:39.676709: val_loss -0.339
2024-12-13 20:14:39.677616: Pseudo dice [0.6351]
2024-12-13 20:14:39.678566: Epoch time: 445.99 s
2024-12-13 20:14:39.679479: Yayy! New best EMA pseudo Dice: 0.5919
2024-12-13 20:14:41.461017: 
2024-12-13 20:14:41.462133: Epoch 12
2024-12-13 20:14:41.462927: Current learning rate: 0.00928
2024-12-13 20:23:00.445925: Validation loss improved from -0.39216 to -0.39997! Patience: 2/50
2024-12-13 20:23:00.446891: train_loss -0.5219
2024-12-13 20:23:00.447708: val_loss -0.4
2024-12-13 20:23:00.448355: Pseudo dice [0.6665]
2024-12-13 20:23:00.449174: Epoch time: 498.99 s
2024-12-13 20:23:00.449936: Yayy! New best EMA pseudo Dice: 0.5993
2024-12-13 20:23:02.259372: 
2024-12-13 20:23:02.260801: Epoch 13
2024-12-13 20:23:02.261515: Current learning rate: 0.00922
2024-12-13 20:30:39.161420: Validation loss improved from -0.39997 to -0.41030! Patience: 0/50
2024-12-13 20:30:39.162441: train_loss -0.5261
2024-12-13 20:30:39.163189: val_loss -0.4103
2024-12-13 20:30:39.163855: Pseudo dice [0.6912]
2024-12-13 20:30:39.164556: Epoch time: 456.9 s
2024-12-13 20:30:39.165264: Yayy! New best EMA pseudo Dice: 0.6085
2024-12-13 20:30:40.939376: 
2024-12-13 20:30:40.940650: Epoch 14
2024-12-13 20:30:40.941375: Current learning rate: 0.00916
2024-12-13 20:38:50.728133: Validation loss did not improve from -0.41030. Patience: 1/50
2024-12-13 20:38:50.729012: train_loss -0.5484
2024-12-13 20:38:50.730080: val_loss -0.3985
2024-12-13 20:38:50.731173: Pseudo dice [0.6744]
2024-12-13 20:38:50.732242: Epoch time: 489.79 s
2024-12-13 20:38:51.141875: Yayy! New best EMA pseudo Dice: 0.6151
2024-12-13 20:38:52.975641: 
2024-12-13 20:38:52.977257: Epoch 15
2024-12-13 20:38:52.978381: Current learning rate: 0.0091
2024-12-13 20:46:40.252446: Validation loss did not improve from -0.41030. Patience: 2/50
2024-12-13 20:46:40.253556: train_loss -0.5369
2024-12-13 20:46:40.254441: val_loss -0.3953
2024-12-13 20:46:40.255346: Pseudo dice [0.6667]
2024-12-13 20:46:40.256325: Epoch time: 467.28 s
2024-12-13 20:46:40.257328: Yayy! New best EMA pseudo Dice: 0.6203
2024-12-13 20:46:42.160881: 
2024-12-13 20:46:42.162439: Epoch 16
2024-12-13 20:46:42.163465: Current learning rate: 0.00903
2024-12-13 20:56:00.592770: Validation loss did not improve from -0.41030. Patience: 3/50
2024-12-13 20:56:00.597673: train_loss -0.5421
2024-12-13 20:56:00.599019: val_loss -0.377
2024-12-13 20:56:00.599738: Pseudo dice [0.6447]
2024-12-13 20:56:00.600498: Epoch time: 558.44 s
2024-12-13 20:56:00.601272: Yayy! New best EMA pseudo Dice: 0.6227
2024-12-13 20:56:02.475564: 
2024-12-13 20:56:02.476991: Epoch 17
2024-12-13 20:56:02.477822: Current learning rate: 0.00897
2024-12-13 21:05:12.115353: Validation loss did not improve from -0.41030. Patience: 4/50
2024-12-13 21:05:12.116437: train_loss -0.5603
2024-12-13 21:05:12.118720: val_loss -0.3768
2024-12-13 21:05:12.119649: Pseudo dice [0.6651]
2024-12-13 21:05:12.121174: Epoch time: 549.64 s
2024-12-13 21:05:12.122181: Yayy! New best EMA pseudo Dice: 0.627
2024-12-13 21:05:13.962724: 
2024-12-13 21:05:13.963900: Epoch 18
2024-12-13 21:05:13.964753: Current learning rate: 0.00891
2024-12-13 21:14:40.717636: Validation loss did not improve from -0.41030. Patience: 5/50
2024-12-13 21:14:40.718571: train_loss -0.5675
2024-12-13 21:14:40.719436: val_loss -0.3908
2024-12-13 21:14:40.720083: Pseudo dice [0.6836]
2024-12-13 21:14:40.720772: Epoch time: 566.76 s
2024-12-13 21:14:40.721541: Yayy! New best EMA pseudo Dice: 0.6326
2024-12-13 21:14:44.096517: 
2024-12-13 21:14:44.097754: Epoch 19
2024-12-13 21:14:44.098463: Current learning rate: 0.00885
2024-12-13 21:24:23.543481: Validation loss did not improve from -0.41030. Patience: 6/50
2024-12-13 21:24:23.544221: train_loss -0.5863
2024-12-13 21:24:23.545014: val_loss -0.3957
2024-12-13 21:24:23.545714: Pseudo dice [0.6759]
2024-12-13 21:24:23.546422: Epoch time: 579.45 s
2024-12-13 21:24:23.947333: Yayy! New best EMA pseudo Dice: 0.6369
2024-12-13 21:24:25.784330: 
2024-12-13 21:24:25.785833: Epoch 20
2024-12-13 21:24:25.786600: Current learning rate: 0.00879
2024-12-13 21:34:13.520356: Validation loss did not improve from -0.41030. Patience: 7/50
2024-12-13 21:34:13.521441: train_loss -0.6042
2024-12-13 21:34:13.522319: val_loss -0.3846
2024-12-13 21:34:13.523157: Pseudo dice [0.664]
2024-12-13 21:34:13.523984: Epoch time: 587.74 s
2024-12-13 21:34:13.524847: Yayy! New best EMA pseudo Dice: 0.6396
2024-12-13 21:34:15.377515: 
2024-12-13 21:34:15.378928: Epoch 21
2024-12-13 21:34:15.379836: Current learning rate: 0.00873
2024-12-13 21:43:42.769078: Validation loss improved from -0.41030 to -0.45854! Patience: 7/50
2024-12-13 21:43:42.769846: train_loss -0.6038
2024-12-13 21:43:42.770511: val_loss -0.4585
2024-12-13 21:43:42.771234: Pseudo dice [0.7176]
2024-12-13 21:43:42.771916: Epoch time: 567.39 s
2024-12-13 21:43:42.772618: Yayy! New best EMA pseudo Dice: 0.6474
2024-12-13 21:43:44.574906: 
2024-12-13 21:43:44.576133: Epoch 22
2024-12-13 21:43:44.576839: Current learning rate: 0.00867
2024-12-13 21:53:19.335583: Validation loss did not improve from -0.45854. Patience: 1/50
2024-12-13 21:53:19.336754: train_loss -0.605
2024-12-13 21:53:19.337745: val_loss -0.4471
2024-12-13 21:53:19.338634: Pseudo dice [0.6961]
2024-12-13 21:53:19.339594: Epoch time: 574.76 s
2024-12-13 21:53:19.340443: Yayy! New best EMA pseudo Dice: 0.6523
2024-12-13 21:53:21.081968: 
2024-12-13 21:53:21.083419: Epoch 23
2024-12-13 21:53:21.084235: Current learning rate: 0.00861
2024-12-13 22:02:40.182429: Validation loss did not improve from -0.45854. Patience: 2/50
2024-12-13 22:02:40.184967: train_loss -0.6188
2024-12-13 22:02:40.185788: val_loss -0.4301
2024-12-13 22:02:40.186521: Pseudo dice [0.6913]
2024-12-13 22:02:40.187242: Epoch time: 559.1 s
2024-12-13 22:02:40.188019: Yayy! New best EMA pseudo Dice: 0.6562
2024-12-13 22:02:41.921498: 
2024-12-13 22:02:41.922748: Epoch 24
2024-12-13 22:02:41.923493: Current learning rate: 0.00855
2024-12-13 22:12:54.198673: Validation loss did not improve from -0.45854. Patience: 3/50
2024-12-13 22:12:54.199677: train_loss -0.6268
2024-12-13 22:12:54.200643: val_loss -0.431
2024-12-13 22:12:54.201485: Pseudo dice [0.6846]
2024-12-13 22:12:54.202364: Epoch time: 612.28 s
2024-12-13 22:12:54.612357: Yayy! New best EMA pseudo Dice: 0.6591
2024-12-13 22:12:56.386391: 
2024-12-13 22:12:56.387921: Epoch 25
2024-12-13 22:12:56.388831: Current learning rate: 0.00849
2024-12-13 22:22:46.903940: Validation loss did not improve from -0.45854. Patience: 4/50
2024-12-13 22:22:46.904706: train_loss -0.6186
2024-12-13 22:22:46.905739: val_loss -0.4155
2024-12-13 22:22:46.906692: Pseudo dice [0.6755]
2024-12-13 22:22:46.907616: Epoch time: 590.52 s
2024-12-13 22:22:46.908474: Yayy! New best EMA pseudo Dice: 0.6607
2024-12-13 22:22:48.777891: 
2024-12-13 22:22:48.778833: Epoch 26
2024-12-13 22:22:48.779604: Current learning rate: 0.00843
2024-12-13 22:32:16.707055: Validation loss did not improve from -0.45854. Patience: 5/50
2024-12-13 22:32:16.707842: train_loss -0.624
2024-12-13 22:32:16.708688: val_loss -0.4449
2024-12-13 22:32:16.709522: Pseudo dice [0.6991]
2024-12-13 22:32:16.710339: Epoch time: 567.93 s
2024-12-13 22:32:16.711156: Yayy! New best EMA pseudo Dice: 0.6645
2024-12-13 22:32:18.513312: 
2024-12-13 22:32:18.514335: Epoch 27
2024-12-13 22:32:18.515142: Current learning rate: 0.00836
2024-12-13 22:42:01.717193: Validation loss did not improve from -0.45854. Patience: 6/50
2024-12-13 22:42:01.717885: train_loss -0.6404
2024-12-13 22:42:01.718768: val_loss -0.441
2024-12-13 22:42:01.719583: Pseudo dice [0.7058]
2024-12-13 22:42:01.720390: Epoch time: 583.21 s
2024-12-13 22:42:01.721207: Yayy! New best EMA pseudo Dice: 0.6687
2024-12-13 22:42:03.525492: 
2024-12-13 22:42:03.526278: Epoch 28
2024-12-13 22:42:03.526945: Current learning rate: 0.0083
2024-12-13 22:51:27.427882: Validation loss did not improve from -0.45854. Patience: 7/50
2024-12-13 22:51:27.428710: train_loss -0.6488
2024-12-13 22:51:27.429641: val_loss -0.3969
2024-12-13 22:51:27.430411: Pseudo dice [0.6796]
2024-12-13 22:51:27.431097: Epoch time: 563.9 s
2024-12-13 22:51:27.431753: Yayy! New best EMA pseudo Dice: 0.6698
2024-12-13 22:51:29.630054: 
2024-12-13 22:51:29.631000: Epoch 29
2024-12-13 22:51:29.631783: Current learning rate: 0.00824
2024-12-13 23:01:00.790741: Validation loss did not improve from -0.45854. Patience: 8/50
2024-12-13 23:01:00.791502: train_loss -0.6453
2024-12-13 23:01:00.792246: val_loss -0.4128
2024-12-13 23:01:00.792917: Pseudo dice [0.6744]
2024-12-13 23:01:00.793662: Epoch time: 571.16 s
2024-12-13 23:01:01.238015: Yayy! New best EMA pseudo Dice: 0.6702
2024-12-13 23:01:03.063232: 
2024-12-13 23:01:03.064327: Epoch 30
2024-12-13 23:01:03.065289: Current learning rate: 0.00818
2024-12-13 23:10:47.225144: Validation loss did not improve from -0.45854. Patience: 9/50
2024-12-13 23:10:47.259537: train_loss -0.6378
2024-12-13 23:10:47.260575: val_loss -0.3877
2024-12-13 23:10:47.261364: Pseudo dice [0.6722]
2024-12-13 23:10:47.262245: Epoch time: 584.2 s
2024-12-13 23:10:47.263041: Yayy! New best EMA pseudo Dice: 0.6704
2024-12-13 23:10:49.154081: 
2024-12-13 23:10:49.155048: Epoch 31
2024-12-13 23:10:49.155715: Current learning rate: 0.00812
2024-12-13 23:20:59.449732: Validation loss did not improve from -0.45854. Patience: 10/50
2024-12-13 23:20:59.450445: train_loss -0.6555
2024-12-13 23:20:59.451277: val_loss -0.407
2024-12-13 23:20:59.452117: Pseudo dice [0.6876]
2024-12-13 23:20:59.452847: Epoch time: 610.3 s
2024-12-13 23:20:59.453645: Yayy! New best EMA pseudo Dice: 0.6721
2024-12-13 23:21:01.260288: 
2024-12-13 23:21:01.261282: Epoch 32
2024-12-13 23:21:01.262182: Current learning rate: 0.00806
2024-12-13 23:30:55.065063: Validation loss did not improve from -0.45854. Patience: 11/50
2024-12-13 23:30:55.065853: train_loss -0.6558
2024-12-13 23:30:55.066603: val_loss -0.4482
2024-12-13 23:30:55.067236: Pseudo dice [0.7076]
2024-12-13 23:30:55.067861: Epoch time: 593.81 s
2024-12-13 23:30:55.068473: Yayy! New best EMA pseudo Dice: 0.6757
2024-12-13 23:30:56.832513: 
2024-12-13 23:30:56.833626: Epoch 33
2024-12-13 23:30:56.834311: Current learning rate: 0.008
2024-12-13 23:40:46.800544: Validation loss did not improve from -0.45854. Patience: 12/50
2024-12-13 23:40:46.801244: train_loss -0.6525
2024-12-13 23:40:46.802002: val_loss -0.4342
2024-12-13 23:40:46.802607: Pseudo dice [0.7049]
2024-12-13 23:40:46.803234: Epoch time: 589.97 s
2024-12-13 23:40:46.803855: Yayy! New best EMA pseudo Dice: 0.6786
2024-12-13 23:40:48.557508: 
2024-12-13 23:40:48.558351: Epoch 34
2024-12-13 23:40:48.559001: Current learning rate: 0.00793
2024-12-13 23:50:45.576357: Validation loss did not improve from -0.45854. Patience: 13/50
2024-12-13 23:50:45.577396: train_loss -0.6718
2024-12-13 23:50:45.578336: val_loss -0.3875
2024-12-13 23:50:45.579342: Pseudo dice [0.6835]
2024-12-13 23:50:45.580213: Epoch time: 597.02 s
2024-12-13 23:50:45.997899: Yayy! New best EMA pseudo Dice: 0.6791
2024-12-13 23:50:47.879246: 
2024-12-13 23:50:47.880230: Epoch 35
2024-12-13 23:50:47.881029: Current learning rate: 0.00787
2024-12-14 00:00:33.945689: Validation loss did not improve from -0.45854. Patience: 14/50
2024-12-14 00:00:33.946361: train_loss -0.6787
2024-12-14 00:00:33.946992: val_loss -0.3919
2024-12-14 00:00:33.947590: Pseudo dice [0.6742]
2024-12-14 00:00:33.948259: Epoch time: 586.07 s
2024-12-14 00:00:35.355615: 
2024-12-14 00:00:35.356492: Epoch 36
2024-12-14 00:00:35.357141: Current learning rate: 0.00781
2024-12-14 00:10:22.995039: Validation loss did not improve from -0.45854. Patience: 15/50
2024-12-14 00:10:22.999290: train_loss -0.6775
2024-12-14 00:10:23.000650: val_loss -0.4025
2024-12-14 00:10:23.001259: Pseudo dice [0.6832]
2024-12-14 00:10:23.001911: Epoch time: 587.64 s
2024-12-14 00:10:24.430513: 
2024-12-14 00:10:24.431499: Epoch 37
2024-12-14 00:10:24.432328: Current learning rate: 0.00775
2024-12-14 00:20:34.130361: Validation loss did not improve from -0.45854. Patience: 16/50
2024-12-14 00:20:34.131191: train_loss -0.6878
2024-12-14 00:20:34.132437: val_loss -0.4152
2024-12-14 00:20:34.133048: Pseudo dice [0.6934]
2024-12-14 00:20:34.133770: Epoch time: 609.7 s
2024-12-14 00:20:34.134335: Yayy! New best EMA pseudo Dice: 0.6805
2024-12-14 00:20:35.994037: 
2024-12-14 00:20:35.994972: Epoch 38
2024-12-14 00:20:35.995699: Current learning rate: 0.00769
2024-12-14 00:30:31.496527: Validation loss did not improve from -0.45854. Patience: 17/50
2024-12-14 00:30:31.497184: train_loss -0.6843
2024-12-14 00:30:31.498029: val_loss -0.4197
2024-12-14 00:30:31.498722: Pseudo dice [0.6911]
2024-12-14 00:30:31.499508: Epoch time: 595.5 s
2024-12-14 00:30:31.500218: Yayy! New best EMA pseudo Dice: 0.6816
2024-12-14 00:30:33.336967: 
2024-12-14 00:30:33.337978: Epoch 39
2024-12-14 00:30:33.338728: Current learning rate: 0.00763
2024-12-14 00:40:10.394635: Validation loss did not improve from -0.45854. Patience: 18/50
2024-12-14 00:40:10.395297: train_loss -0.6888
2024-12-14 00:40:10.396305: val_loss -0.4476
2024-12-14 00:40:10.397168: Pseudo dice [0.7145]
2024-12-14 00:40:10.397994: Epoch time: 577.06 s
2024-12-14 00:40:10.841196: Yayy! New best EMA pseudo Dice: 0.6849
2024-12-14 00:40:14.384330: 
2024-12-14 00:40:14.385264: Epoch 40
2024-12-14 00:40:14.385976: Current learning rate: 0.00756
2024-12-14 00:49:41.934100: Validation loss did not improve from -0.45854. Patience: 19/50
2024-12-14 00:49:41.934770: train_loss -0.6968
2024-12-14 00:49:41.935549: val_loss -0.4017
2024-12-14 00:49:41.936259: Pseudo dice [0.6872]
2024-12-14 00:49:41.937006: Epoch time: 567.55 s
2024-12-14 00:49:41.937670: Yayy! New best EMA pseudo Dice: 0.6851
2024-12-14 00:49:43.838574: 
2024-12-14 00:49:43.839622: Epoch 41
2024-12-14 00:49:43.840348: Current learning rate: 0.0075
2024-12-14 00:59:47.937699: Validation loss did not improve from -0.45854. Patience: 20/50
2024-12-14 00:59:47.938524: train_loss -0.6925
2024-12-14 00:59:47.939522: val_loss -0.4516
2024-12-14 00:59:47.940366: Pseudo dice [0.7132]
2024-12-14 00:59:47.941215: Epoch time: 604.1 s
2024-12-14 00:59:47.942011: Yayy! New best EMA pseudo Dice: 0.6879
2024-12-14 00:59:49.763544: 
2024-12-14 00:59:49.764434: Epoch 42
2024-12-14 00:59:49.765195: Current learning rate: 0.00744
2024-12-14 01:09:41.919772: Validation loss did not improve from -0.45854. Patience: 21/50
2024-12-14 01:09:41.920439: train_loss -0.6946
2024-12-14 01:09:41.921247: val_loss -0.3875
2024-12-14 01:09:41.922009: Pseudo dice [0.6782]
2024-12-14 01:09:41.922757: Epoch time: 592.16 s
2024-12-14 01:09:43.323288: 
2024-12-14 01:09:43.324385: Epoch 43
2024-12-14 01:09:43.325165: Current learning rate: 0.00738
2024-12-14 01:19:42.079072: Validation loss did not improve from -0.45854. Patience: 22/50
2024-12-14 01:19:42.082915: train_loss -0.7043
2024-12-14 01:19:42.083673: val_loss -0.3623
2024-12-14 01:19:42.084284: Pseudo dice [0.662]
2024-12-14 01:19:42.085141: Epoch time: 598.76 s
2024-12-14 01:19:43.443521: 
2024-12-14 01:19:43.444525: Epoch 44
2024-12-14 01:19:43.445229: Current learning rate: 0.00732
2024-12-14 01:29:33.006507: Validation loss did not improve from -0.45854. Patience: 23/50
2024-12-14 01:29:33.007641: train_loss -0.7038
2024-12-14 01:29:33.008671: val_loss -0.4361
2024-12-14 01:29:33.009580: Pseudo dice [0.6932]
2024-12-14 01:29:33.010411: Epoch time: 589.57 s
2024-12-14 01:29:34.800686: 
2024-12-14 01:29:34.801702: Epoch 45
2024-12-14 01:29:34.802523: Current learning rate: 0.00725
2024-12-14 01:39:31.607919: Validation loss did not improve from -0.45854. Patience: 24/50
2024-12-14 01:39:31.608583: train_loss -0.7056
2024-12-14 01:39:31.609578: val_loss -0.4135
2024-12-14 01:39:31.610421: Pseudo dice [0.7032]
2024-12-14 01:39:31.611365: Epoch time: 596.81 s
2024-12-14 01:39:32.994134: 
2024-12-14 01:39:32.995185: Epoch 46
2024-12-14 01:39:32.996109: Current learning rate: 0.00719
2024-12-14 01:49:09.318871: Validation loss did not improve from -0.45854. Patience: 25/50
2024-12-14 01:49:09.319559: train_loss -0.7069
2024-12-14 01:49:09.320194: val_loss -0.4035
2024-12-14 01:49:09.320807: Pseudo dice [0.6842]
2024-12-14 01:49:09.321445: Epoch time: 576.33 s
2024-12-14 01:49:10.679721: 
2024-12-14 01:49:10.680781: Epoch 47
2024-12-14 01:49:10.681508: Current learning rate: 0.00713
2024-12-14 01:58:57.739702: Validation loss did not improve from -0.45854. Patience: 26/50
2024-12-14 01:58:57.740494: train_loss -0.7114
2024-12-14 01:58:57.741407: val_loss -0.4086
2024-12-14 01:58:57.742112: Pseudo dice [0.7034]
2024-12-14 01:58:57.742844: Epoch time: 587.06 s
2024-12-14 01:58:57.743588: Yayy! New best EMA pseudo Dice: 0.6885
2024-12-14 01:58:59.466763: 
2024-12-14 01:58:59.467746: Epoch 48
2024-12-14 01:58:59.468538: Current learning rate: 0.00707
2024-12-14 02:08:42.679775: Validation loss did not improve from -0.45854. Patience: 27/50
2024-12-14 02:08:42.680599: train_loss -0.7092
2024-12-14 02:08:42.682073: val_loss -0.4352
2024-12-14 02:08:42.682812: Pseudo dice [0.7106]
2024-12-14 02:08:42.683481: Epoch time: 583.21 s
2024-12-14 02:08:42.684083: Yayy! New best EMA pseudo Dice: 0.6907
2024-12-14 02:08:44.494315: 
2024-12-14 02:08:44.495044: Epoch 49
2024-12-14 02:08:44.495824: Current learning rate: 0.007
2024-12-14 02:18:38.376206: Validation loss did not improve from -0.45854. Patience: 28/50
2024-12-14 02:18:38.376948: train_loss -0.7064
2024-12-14 02:18:38.377667: val_loss -0.4166
2024-12-14 02:18:38.378470: Pseudo dice [0.6825]
2024-12-14 02:18:38.379310: Epoch time: 593.88 s
2024-12-14 02:18:40.536796: 
2024-12-14 02:18:40.537931: Epoch 50
2024-12-14 02:18:40.538673: Current learning rate: 0.00694
2024-12-14 02:28:36.453618: Validation loss did not improve from -0.45854. Patience: 29/50
2024-12-14 02:28:36.455469: train_loss -0.7212
2024-12-14 02:28:36.456946: val_loss -0.4249
2024-12-14 02:28:36.457674: Pseudo dice [0.7076]
2024-12-14 02:28:36.458534: Epoch time: 595.92 s
2024-12-14 02:28:36.459152: Yayy! New best EMA pseudo Dice: 0.6917
2024-12-14 02:28:38.205263: 
2024-12-14 02:28:38.206159: Epoch 51
2024-12-14 02:28:38.206791: Current learning rate: 0.00688
2024-12-14 02:38:45.007962: Validation loss did not improve from -0.45854. Patience: 30/50
2024-12-14 02:38:45.008771: train_loss -0.7224
2024-12-14 02:38:45.009527: val_loss -0.4326
2024-12-14 02:38:45.010237: Pseudo dice [0.7136]
2024-12-14 02:38:45.011032: Epoch time: 606.8 s
2024-12-14 02:38:45.011802: Yayy! New best EMA pseudo Dice: 0.6938
2024-12-14 02:38:46.792616: 
2024-12-14 02:38:46.793637: Epoch 52
2024-12-14 02:38:46.794409: Current learning rate: 0.00682
2024-12-14 02:48:21.369556: Validation loss did not improve from -0.45854. Patience: 31/50
2024-12-14 02:48:21.370301: train_loss -0.7242
2024-12-14 02:48:21.371177: val_loss -0.3909
2024-12-14 02:48:21.371809: Pseudo dice [0.6927]
2024-12-14 02:48:21.372491: Epoch time: 574.58 s
2024-12-14 02:48:22.795838: 
2024-12-14 02:48:22.796801: Epoch 53
2024-12-14 02:48:22.797541: Current learning rate: 0.00675
2024-12-14 02:58:12.815588: Validation loss did not improve from -0.45854. Patience: 32/50
2024-12-14 02:58:12.816176: train_loss -0.7202
2024-12-14 02:58:12.816796: val_loss -0.4289
2024-12-14 02:58:12.817363: Pseudo dice [0.7]
2024-12-14 02:58:12.818061: Epoch time: 590.02 s
2024-12-14 02:58:12.818698: Yayy! New best EMA pseudo Dice: 0.6944
2024-12-14 02:58:14.611367: 
2024-12-14 02:58:14.612242: Epoch 54
2024-12-14 02:58:14.612961: Current learning rate: 0.00669
2024-12-14 03:08:17.996668: Validation loss did not improve from -0.45854. Patience: 33/50
2024-12-14 03:08:17.997426: train_loss -0.7247
2024-12-14 03:08:17.998161: val_loss -0.435
2024-12-14 03:08:17.998825: Pseudo dice [0.7046]
2024-12-14 03:08:17.999560: Epoch time: 603.39 s
2024-12-14 03:08:18.433824: Yayy! New best EMA pseudo Dice: 0.6954
2024-12-14 03:08:20.236073: 
2024-12-14 03:08:20.236970: Epoch 55
2024-12-14 03:08:20.237803: Current learning rate: 0.00663
2024-12-14 03:18:09.261103: Validation loss did not improve from -0.45854. Patience: 34/50
2024-12-14 03:18:09.261942: train_loss -0.7279
2024-12-14 03:18:09.262688: val_loss -0.4287
2024-12-14 03:18:09.263362: Pseudo dice [0.7065]
2024-12-14 03:18:09.264078: Epoch time: 589.03 s
2024-12-14 03:18:09.264749: Yayy! New best EMA pseudo Dice: 0.6965
2024-12-14 03:18:11.067191: 
2024-12-14 03:18:11.068117: Epoch 56
2024-12-14 03:18:11.068879: Current learning rate: 0.00657
2024-12-14 03:27:55.204331: Validation loss did not improve from -0.45854. Patience: 35/50
2024-12-14 03:27:55.208905: train_loss -0.7346
2024-12-14 03:27:55.209944: val_loss -0.4188
2024-12-14 03:27:55.210688: Pseudo dice [0.6922]
2024-12-14 03:27:55.211745: Epoch time: 584.14 s
2024-12-14 03:27:56.653729: 
2024-12-14 03:27:56.654529: Epoch 57
2024-12-14 03:27:56.655201: Current learning rate: 0.0065
2024-12-14 03:37:54.138274: Validation loss did not improve from -0.45854. Patience: 36/50
2024-12-14 03:37:54.139291: train_loss -0.7345
2024-12-14 03:37:54.141009: val_loss -0.4345
2024-12-14 03:37:54.141771: Pseudo dice [0.6985]
2024-12-14 03:37:54.142632: Epoch time: 597.49 s
2024-12-14 03:37:55.534920: 
2024-12-14 03:37:55.536018: Epoch 58
2024-12-14 03:37:55.536825: Current learning rate: 0.00644
2024-12-14 03:47:50.094623: Validation loss did not improve from -0.45854. Patience: 37/50
2024-12-14 03:47:50.095583: train_loss -0.7354
2024-12-14 03:47:50.096442: val_loss -0.4525
2024-12-14 03:47:50.097213: Pseudo dice [0.7068]
2024-12-14 03:47:50.097973: Epoch time: 594.56 s
2024-12-14 03:47:50.098788: Yayy! New best EMA pseudo Dice: 0.6973
2024-12-14 03:47:51.923191: 
2024-12-14 03:47:51.924027: Epoch 59
2024-12-14 03:47:51.924923: Current learning rate: 0.00638
2024-12-14 03:57:47.102107: Validation loss did not improve from -0.45854. Patience: 38/50
2024-12-14 03:57:47.102789: train_loss -0.7404
2024-12-14 03:57:47.103868: val_loss -0.4243
2024-12-14 03:57:47.104849: Pseudo dice [0.6988]
2024-12-14 03:57:47.105895: Epoch time: 595.18 s
2024-12-14 03:57:47.457446: Yayy! New best EMA pseudo Dice: 0.6975
2024-12-14 03:57:49.652419: 
2024-12-14 03:57:49.653718: Epoch 60
2024-12-14 03:57:49.654835: Current learning rate: 0.00631
2024-12-14 04:07:58.456477: Validation loss did not improve from -0.45854. Patience: 39/50
2024-12-14 04:07:58.457154: train_loss -0.7368
2024-12-14 04:07:58.457976: val_loss -0.4156
2024-12-14 04:07:58.458727: Pseudo dice [0.6871]
2024-12-14 04:07:58.459544: Epoch time: 608.81 s
2024-12-14 04:07:59.856180: 
2024-12-14 04:07:59.857234: Epoch 61
2024-12-14 04:07:59.858187: Current learning rate: 0.00625
2024-12-14 04:18:00.425878: Validation loss did not improve from -0.45854. Patience: 40/50
2024-12-14 04:18:00.426667: train_loss -0.7422
2024-12-14 04:18:00.427472: val_loss -0.4118
2024-12-14 04:18:00.428242: Pseudo dice [0.6984]
2024-12-14 04:18:00.429138: Epoch time: 600.57 s
2024-12-14 04:18:01.864095: 
2024-12-14 04:18:01.864988: Epoch 62
2024-12-14 04:18:01.865763: Current learning rate: 0.00619
2024-12-14 04:27:48.458100: Validation loss did not improve from -0.45854. Patience: 41/50
2024-12-14 04:27:48.459166: train_loss -0.7513
2024-12-14 04:27:48.460136: val_loss -0.4154
2024-12-14 04:27:48.461003: Pseudo dice [0.6901]
2024-12-14 04:27:48.461765: Epoch time: 586.6 s
2024-12-14 04:27:49.886672: 
2024-12-14 04:27:49.887872: Epoch 63
2024-12-14 04:27:49.888713: Current learning rate: 0.00612
2024-12-14 04:37:56.424098: Validation loss did not improve from -0.45854. Patience: 42/50
2024-12-14 04:37:56.425820: train_loss -0.7496
2024-12-14 04:37:56.426699: val_loss -0.4231
2024-12-14 04:37:56.427696: Pseudo dice [0.7031]
2024-12-14 04:37:56.428634: Epoch time: 606.54 s
2024-12-14 04:37:58.015030: 
2024-12-14 04:37:58.016638: Epoch 64
2024-12-14 04:37:58.017675: Current learning rate: 0.00606
2024-12-14 04:47:56.880940: Validation loss did not improve from -0.45854. Patience: 43/50
2024-12-14 04:47:56.881896: train_loss -0.7464
2024-12-14 04:47:56.882947: val_loss -0.4163
2024-12-14 04:47:56.884065: Pseudo dice [0.7016]
2024-12-14 04:47:56.885058: Epoch time: 598.87 s
2024-12-14 04:47:58.754969: 
2024-12-14 04:47:58.756745: Epoch 65
2024-12-14 04:47:58.757837: Current learning rate: 0.006
2024-12-14 04:58:28.095296: Validation loss improved from -0.45854 to -0.46032! Patience: 43/50
2024-12-14 04:58:28.096112: train_loss -0.7487
2024-12-14 04:58:28.096839: val_loss -0.4603
2024-12-14 04:58:28.097592: Pseudo dice [0.7159]
2024-12-14 04:58:28.098389: Epoch time: 629.34 s
2024-12-14 04:58:28.099193: Yayy! New best EMA pseudo Dice: 0.6991
2024-12-14 04:58:29.887552: 
2024-12-14 04:58:29.888725: Epoch 66
2024-12-14 04:58:29.889581: Current learning rate: 0.00593
2024-12-14 05:09:00.210552: Validation loss did not improve from -0.46032. Patience: 1/50
2024-12-14 05:09:00.211237: train_loss -0.7518
2024-12-14 05:09:00.212085: val_loss -0.433
2024-12-14 05:09:00.212817: Pseudo dice [0.7127]
2024-12-14 05:09:00.213578: Epoch time: 630.33 s
2024-12-14 05:09:00.214254: Yayy! New best EMA pseudo Dice: 0.7004
2024-12-14 05:09:02.002112: 
2024-12-14 05:09:02.003041: Epoch 67
2024-12-14 05:09:02.003783: Current learning rate: 0.00587
2024-12-14 05:19:00.592726: Validation loss did not improve from -0.46032. Patience: 2/50
2024-12-14 05:19:00.593542: train_loss -0.7528
2024-12-14 05:19:00.594304: val_loss -0.4481
2024-12-14 05:19:00.595078: Pseudo dice [0.7156]
2024-12-14 05:19:00.595698: Epoch time: 598.59 s
2024-12-14 05:19:00.596408: Yayy! New best EMA pseudo Dice: 0.702
2024-12-14 05:19:02.403743: 
2024-12-14 05:19:02.404656: Epoch 68
2024-12-14 05:19:02.405482: Current learning rate: 0.00581
2024-12-14 05:29:12.003729: Validation loss did not improve from -0.46032. Patience: 3/50
2024-12-14 05:29:12.004435: train_loss -0.7529
2024-12-14 05:29:12.005142: val_loss -0.4209
2024-12-14 05:29:12.005864: Pseudo dice [0.7045]
2024-12-14 05:29:12.006524: Epoch time: 609.6 s
2024-12-14 05:29:12.007285: Yayy! New best EMA pseudo Dice: 0.7022
2024-12-14 05:29:13.826607: 
2024-12-14 05:29:13.827350: Epoch 69
2024-12-14 05:29:13.828149: Current learning rate: 0.00574
2024-12-14 05:39:25.531880: Validation loss did not improve from -0.46032. Patience: 4/50
2024-12-14 05:39:25.533900: train_loss -0.7451
2024-12-14 05:39:25.535726: val_loss -0.4268
2024-12-14 05:39:25.536654: Pseudo dice [0.7099]
2024-12-14 05:39:25.537727: Epoch time: 611.71 s
2024-12-14 05:39:25.937391: Yayy! New best EMA pseudo Dice: 0.703
2024-12-14 05:39:27.757331: 
2024-12-14 05:39:27.758863: Epoch 70
2024-12-14 05:39:27.759582: Current learning rate: 0.00568
2024-12-14 05:49:23.497040: Validation loss did not improve from -0.46032. Patience: 5/50
2024-12-14 05:49:23.498042: train_loss -0.7448
2024-12-14 05:49:23.498760: val_loss -0.4385
2024-12-14 05:49:23.499352: Pseudo dice [0.7161]
2024-12-14 05:49:23.500007: Epoch time: 595.74 s
2024-12-14 05:49:23.500653: Yayy! New best EMA pseudo Dice: 0.7043
2024-12-14 05:49:25.713687: 
2024-12-14 05:49:25.714664: Epoch 71
2024-12-14 05:49:25.715457: Current learning rate: 0.00562
2024-12-14 05:59:18.056472: Validation loss did not improve from -0.46032. Patience: 6/50
2024-12-14 05:59:18.057128: train_loss -0.7546
2024-12-14 05:59:18.057764: val_loss -0.4194
2024-12-14 05:59:18.058379: Pseudo dice [0.6989]
2024-12-14 05:59:18.058983: Epoch time: 592.34 s
2024-12-14 05:59:19.469478: 
2024-12-14 05:59:19.470491: Epoch 72
2024-12-14 05:59:19.471159: Current learning rate: 0.00555
2024-12-14 06:09:32.616816: Validation loss improved from -0.46032 to -0.46907! Patience: 6/50
2024-12-14 06:09:32.617589: train_loss -0.7578
2024-12-14 06:09:32.618665: val_loss -0.4691
2024-12-14 06:09:32.619514: Pseudo dice [0.7233]
2024-12-14 06:09:32.620414: Epoch time: 613.15 s
2024-12-14 06:09:32.621405: Yayy! New best EMA pseudo Dice: 0.7057
2024-12-14 06:09:34.456521: 
2024-12-14 06:09:34.457699: Epoch 73
2024-12-14 06:09:34.458668: Current learning rate: 0.00549
2024-12-14 06:19:39.141537: Validation loss did not improve from -0.46907. Patience: 1/50
2024-12-14 06:19:39.142271: train_loss -0.7622
2024-12-14 06:19:39.143077: val_loss -0.42
2024-12-14 06:19:39.143773: Pseudo dice [0.7062]
2024-12-14 06:19:39.144504: Epoch time: 604.69 s
2024-12-14 06:19:39.145232: Yayy! New best EMA pseudo Dice: 0.7058
2024-12-14 06:19:40.951566: 
2024-12-14 06:19:40.952596: Epoch 74
2024-12-14 06:19:40.953287: Current learning rate: 0.00542
2024-12-14 06:29:32.504275: Validation loss did not improve from -0.46907. Patience: 2/50
2024-12-14 06:29:32.505164: train_loss -0.762
2024-12-14 06:29:32.505994: val_loss -0.3686
2024-12-14 06:29:32.506634: Pseudo dice [0.6732]
2024-12-14 06:29:32.507240: Epoch time: 591.55 s
2024-12-14 06:29:34.355759: 
2024-12-14 06:29:34.356614: Epoch 75
2024-12-14 06:29:34.357234: Current learning rate: 0.00536
2024-12-14 06:39:27.651646: Validation loss did not improve from -0.46907. Patience: 3/50
2024-12-14 06:39:27.653386: train_loss -0.7565
2024-12-14 06:39:27.654293: val_loss -0.391
2024-12-14 06:39:27.655060: Pseudo dice [0.6827]
2024-12-14 06:39:27.655890: Epoch time: 593.3 s
2024-12-14 06:39:29.086389: 
2024-12-14 06:39:29.087230: Epoch 76
2024-12-14 06:39:29.088012: Current learning rate: 0.00529
2024-12-14 06:49:21.075972: Validation loss did not improve from -0.46907. Patience: 4/50
2024-12-14 06:49:21.076632: train_loss -0.7623
2024-12-14 06:49:21.077480: val_loss -0.4402
2024-12-14 06:49:21.078314: Pseudo dice [0.7064]
2024-12-14 06:49:21.079159: Epoch time: 591.99 s
2024-12-14 06:49:22.483405: 
2024-12-14 06:49:22.484418: Epoch 77
2024-12-14 06:49:22.485290: Current learning rate: 0.00523
2024-12-14 06:59:21.193167: Validation loss did not improve from -0.46907. Patience: 5/50
2024-12-14 06:59:21.194219: train_loss -0.7579
2024-12-14 06:59:21.194951: val_loss -0.4146
2024-12-14 06:59:21.195636: Pseudo dice [0.6933]
2024-12-14 06:59:21.196339: Epoch time: 598.71 s
2024-12-14 06:59:22.617965: 
2024-12-14 06:59:22.619185: Epoch 78
2024-12-14 06:59:22.619960: Current learning rate: 0.00517
2024-12-14 07:09:37.141415: Validation loss did not improve from -0.46907. Patience: 6/50
2024-12-14 07:09:37.142486: train_loss -0.7614
2024-12-14 07:09:37.143734: val_loss -0.449
2024-12-14 07:09:37.144998: Pseudo dice [0.7274]
2024-12-14 07:09:37.146070: Epoch time: 614.53 s
2024-12-14 07:09:38.564915: 
2024-12-14 07:09:38.566407: Epoch 79
2024-12-14 07:09:38.567507: Current learning rate: 0.0051
2024-12-14 07:19:41.827950: Validation loss did not improve from -0.46907. Patience: 7/50
2024-12-14 07:19:41.828962: train_loss -0.7659
2024-12-14 07:19:41.829907: val_loss -0.3959
2024-12-14 07:19:41.830597: Pseudo dice [0.6831]
2024-12-14 07:19:41.831466: Epoch time: 603.27 s
2024-12-14 07:19:43.672351: 
2024-12-14 07:19:43.673627: Epoch 80
2024-12-14 07:19:43.674344: Current learning rate: 0.00504
2024-12-14 07:29:37.498556: Validation loss did not improve from -0.46907. Patience: 8/50
2024-12-14 07:29:37.499561: train_loss -0.7728
2024-12-14 07:29:37.500560: val_loss -0.4104
2024-12-14 07:29:37.501395: Pseudo dice [0.6949]
2024-12-14 07:29:37.502154: Epoch time: 593.83 s
2024-12-14 07:29:39.398529: 
2024-12-14 07:29:39.399821: Epoch 81
2024-12-14 07:29:39.400564: Current learning rate: 0.00497
2024-12-14 07:39:51.685609: Validation loss did not improve from -0.46907. Patience: 9/50
2024-12-14 07:39:51.687044: train_loss -0.7701
2024-12-14 07:39:51.687819: val_loss -0.4196
2024-12-14 07:39:51.688462: Pseudo dice [0.694]
2024-12-14 07:39:51.689093: Epoch time: 612.29 s
2024-12-14 07:39:53.121534: 
2024-12-14 07:39:53.122875: Epoch 82
2024-12-14 07:39:53.123599: Current learning rate: 0.00491
2024-12-14 07:50:07.211646: Validation loss did not improve from -0.46907. Patience: 10/50
2024-12-14 07:50:07.213490: train_loss -0.7714
2024-12-14 07:50:07.215557: val_loss -0.4141
2024-12-14 07:50:07.216553: Pseudo dice [0.6973]
2024-12-14 07:50:07.217631: Epoch time: 614.09 s
2024-12-14 07:50:08.586043: 
2024-12-14 07:50:08.587163: Epoch 83
2024-12-14 07:50:08.587939: Current learning rate: 0.00484
2024-12-14 08:00:31.478361: Validation loss did not improve from -0.46907. Patience: 11/50
2024-12-14 08:00:31.479609: train_loss -0.7731
2024-12-14 08:00:31.480451: val_loss -0.453
2024-12-14 08:00:31.481307: Pseudo dice [0.7231]
2024-12-14 08:00:31.482104: Epoch time: 622.89 s
2024-12-14 08:00:32.804801: 
2024-12-14 08:00:32.805838: Epoch 84
2024-12-14 08:00:32.806751: Current learning rate: 0.00478
2024-12-14 08:10:36.257778: Validation loss did not improve from -0.46907. Patience: 12/50
2024-12-14 08:10:36.258785: train_loss -0.7764
2024-12-14 08:10:36.259680: val_loss -0.3899
2024-12-14 08:10:36.260377: Pseudo dice [0.6901]
2024-12-14 08:10:36.261081: Epoch time: 603.46 s
2024-12-14 08:10:37.916932: 
2024-12-14 08:10:37.918190: Epoch 85
2024-12-14 08:10:37.918867: Current learning rate: 0.00471
2024-12-14 08:20:34.816630: Validation loss did not improve from -0.46907. Patience: 13/50
2024-12-14 08:20:34.817588: train_loss -0.7746
2024-12-14 08:20:34.818490: val_loss -0.4438
2024-12-14 08:20:34.819338: Pseudo dice [0.715]
2024-12-14 08:20:34.820040: Epoch time: 596.9 s
2024-12-14 08:20:36.166007: 
2024-12-14 08:20:36.167173: Epoch 86
2024-12-14 08:20:36.167972: Current learning rate: 0.00465
2024-12-14 08:30:32.184712: Validation loss did not improve from -0.46907. Patience: 14/50
2024-12-14 08:30:32.185623: train_loss -0.7743
2024-12-14 08:30:32.186323: val_loss -0.4575
2024-12-14 08:30:32.187030: Pseudo dice [0.7226]
2024-12-14 08:30:32.187737: Epoch time: 596.02 s
2024-12-14 08:30:33.555543: 
2024-12-14 08:30:33.556642: Epoch 87
2024-12-14 08:30:33.557436: Current learning rate: 0.00458
2024-12-14 08:40:47.847673: Validation loss did not improve from -0.46907. Patience: 15/50
2024-12-14 08:40:47.848606: train_loss -0.7776
2024-12-14 08:40:47.849710: val_loss -0.4336
2024-12-14 08:40:47.850720: Pseudo dice [0.7128]
2024-12-14 08:40:47.851506: Epoch time: 614.29 s
2024-12-14 08:40:49.187863: 
2024-12-14 08:40:49.189138: Epoch 88
2024-12-14 08:40:49.189871: Current learning rate: 0.00452
2024-12-14 08:50:51.415774: Validation loss did not improve from -0.46907. Patience: 16/50
2024-12-14 08:50:51.418886: train_loss -0.7803
2024-12-14 08:50:51.420082: val_loss -0.4436
2024-12-14 08:50:51.420884: Pseudo dice [0.7097]
2024-12-14 08:50:51.421568: Epoch time: 602.23 s
2024-12-14 08:50:52.784604: 
2024-12-14 08:50:52.785768: Epoch 89
2024-12-14 08:50:52.786427: Current learning rate: 0.00445
2024-12-14 09:01:07.632175: Validation loss did not improve from -0.46907. Patience: 17/50
2024-12-14 09:01:07.633111: train_loss -0.7788
2024-12-14 09:01:07.634171: val_loss -0.4564
2024-12-14 09:01:07.635254: Pseudo dice [0.7224]
2024-12-14 09:01:07.636253: Epoch time: 614.85 s
2024-12-14 09:01:07.996707: Yayy! New best EMA pseudo Dice: 0.7072
2024-12-14 09:01:09.709994: 
2024-12-14 09:01:09.711430: Epoch 90
2024-12-14 09:01:09.712449: Current learning rate: 0.00438
2024-12-14 09:11:19.325638: Validation loss did not improve from -0.46907. Patience: 18/50
2024-12-14 09:11:19.326638: train_loss -0.7832
2024-12-14 09:11:19.327453: val_loss -0.4635
2024-12-14 09:11:19.328154: Pseudo dice [0.7117]
2024-12-14 09:11:19.328932: Epoch time: 609.62 s
2024-12-14 09:11:19.329868: Yayy! New best EMA pseudo Dice: 0.7077
2024-12-14 09:11:21.042928: 
2024-12-14 09:11:21.044235: Epoch 91
2024-12-14 09:11:21.045030: Current learning rate: 0.00432
2024-12-14 09:21:42.278438: Validation loss did not improve from -0.46907. Patience: 19/50
2024-12-14 09:21:42.279385: train_loss -0.7835
2024-12-14 09:21:42.280429: val_loss -0.4492
2024-12-14 09:21:42.281177: Pseudo dice [0.7123]
2024-12-14 09:21:42.281861: Epoch time: 621.24 s
2024-12-14 09:21:42.282598: Yayy! New best EMA pseudo Dice: 0.7081
2024-12-14 09:21:44.073305: 
2024-12-14 09:21:44.074434: Epoch 92
2024-12-14 09:21:44.075140: Current learning rate: 0.00425
2024-12-14 09:31:43.076593: Validation loss did not improve from -0.46907. Patience: 20/50
2024-12-14 09:31:43.077407: train_loss -0.7839
2024-12-14 09:31:43.078101: val_loss -0.4227
2024-12-14 09:31:43.078737: Pseudo dice [0.7077]
2024-12-14 09:31:43.079454: Epoch time: 599.01 s
2024-12-14 09:31:45.162359: 
2024-12-14 09:31:45.163726: Epoch 93
2024-12-14 09:31:45.164551: Current learning rate: 0.00419
2024-12-14 09:41:41.667907: Validation loss did not improve from -0.46907. Patience: 21/50
2024-12-14 09:41:41.668909: train_loss -0.7817
2024-12-14 09:41:41.669766: val_loss -0.3854
2024-12-14 09:41:41.670489: Pseudo dice [0.6857]
2024-12-14 09:41:41.671266: Epoch time: 596.51 s
2024-12-14 09:41:43.057075: 
2024-12-14 09:41:43.058349: Epoch 94
2024-12-14 09:41:43.059035: Current learning rate: 0.00412
2024-12-14 09:51:28.006221: Validation loss did not improve from -0.46907. Patience: 22/50
2024-12-14 09:51:28.007125: train_loss -0.786
2024-12-14 09:51:28.007964: val_loss -0.4145
2024-12-14 09:51:28.008716: Pseudo dice [0.7039]
2024-12-14 09:51:28.009457: Epoch time: 584.95 s
2024-12-14 09:51:29.759511: 
2024-12-14 09:51:29.760783: Epoch 95
2024-12-14 09:51:29.761530: Current learning rate: 0.00405
2024-12-14 10:01:46.239716: Validation loss did not improve from -0.46907. Patience: 23/50
2024-12-14 10:01:46.241656: train_loss -0.7865
2024-12-14 10:01:46.243172: val_loss -0.41
2024-12-14 10:01:46.243901: Pseudo dice [0.7055]
2024-12-14 10:01:46.244797: Epoch time: 616.48 s
2024-12-14 10:01:47.585865: 
2024-12-14 10:01:47.587124: Epoch 96
2024-12-14 10:01:47.587828: Current learning rate: 0.00399
2024-12-14 10:12:07.266368: Validation loss did not improve from -0.46907. Patience: 24/50
2024-12-14 10:12:07.267671: train_loss -0.7858
2024-12-14 10:12:07.268408: val_loss -0.442
2024-12-14 10:12:07.269132: Pseudo dice [0.7172]
2024-12-14 10:12:07.269783: Epoch time: 619.68 s
2024-12-14 10:12:08.620327: 
2024-12-14 10:12:08.621536: Epoch 97
2024-12-14 10:12:08.622322: Current learning rate: 0.00392
2024-12-14 10:22:43.596629: Validation loss did not improve from -0.46907. Patience: 25/50
2024-12-14 10:22:43.597724: train_loss -0.7899
2024-12-14 10:22:43.598569: val_loss -0.4028
2024-12-14 10:22:43.599376: Pseudo dice [0.7]
2024-12-14 10:22:43.600205: Epoch time: 634.98 s
2024-12-14 10:22:44.955734: 
2024-12-14 10:22:44.957212: Epoch 98
2024-12-14 10:22:44.958150: Current learning rate: 0.00385
2024-12-14 10:32:39.711608: Validation loss did not improve from -0.46907. Patience: 26/50
2024-12-14 10:32:39.712664: train_loss -0.7878
2024-12-14 10:32:39.713458: val_loss -0.431
2024-12-14 10:32:39.714197: Pseudo dice [0.7098]
2024-12-14 10:32:39.714863: Epoch time: 594.76 s
2024-12-14 10:32:41.074023: 
2024-12-14 10:32:41.075445: Epoch 99
2024-12-14 10:32:41.076358: Current learning rate: 0.00379
2024-12-14 10:43:00.036846: Validation loss did not improve from -0.46907. Patience: 27/50
2024-12-14 10:43:00.037846: train_loss -0.7909
2024-12-14 10:43:00.038846: val_loss -0.4453
2024-12-14 10:43:00.039770: Pseudo dice [0.7225]
2024-12-14 10:43:00.040620: Epoch time: 618.97 s
2024-12-14 10:43:01.755950: 
2024-12-14 10:43:01.757349: Epoch 100
2024-12-14 10:43:01.758358: Current learning rate: 0.00372
2024-12-14 10:53:27.994570: Validation loss did not improve from -0.46907. Patience: 28/50
2024-12-14 10:53:27.995534: train_loss -0.7933
2024-12-14 10:53:27.996350: val_loss -0.4325
2024-12-14 10:53:27.997111: Pseudo dice [0.7172]
2024-12-14 10:53:27.997889: Epoch time: 626.24 s
2024-12-14 10:53:27.998532: Yayy! New best EMA pseudo Dice: 0.709
2024-12-14 10:53:29.748117: 
2024-12-14 10:53:29.749435: Epoch 101
2024-12-14 10:53:29.750157: Current learning rate: 0.00365
2024-12-14 11:03:35.452647: Validation loss did not improve from -0.46907. Patience: 29/50
2024-12-14 11:03:35.472234: train_loss -0.7916
2024-12-14 11:03:35.474566: val_loss -0.4144
2024-12-14 11:03:35.475466: Pseudo dice [0.7033]
2024-12-14 11:03:35.476893: Epoch time: 605.72 s
2024-12-14 11:03:36.850470: 
2024-12-14 11:03:36.851756: Epoch 102
2024-12-14 11:03:36.852546: Current learning rate: 0.00359
2024-12-14 11:13:46.336538: Validation loss did not improve from -0.46907. Patience: 30/50
2024-12-14 11:13:46.337365: train_loss -0.7937
2024-12-14 11:13:46.338100: val_loss -0.4674
2024-12-14 11:13:46.338748: Pseudo dice [0.7256]
2024-12-14 11:13:46.339435: Epoch time: 609.49 s
2024-12-14 11:13:46.340071: Yayy! New best EMA pseudo Dice: 0.7101
2024-12-14 11:13:48.124534: 
2024-12-14 11:13:48.125437: Epoch 103
2024-12-14 11:13:48.126258: Current learning rate: 0.00352
2024-12-14 11:23:52.363750: Validation loss did not improve from -0.46907. Patience: 31/50
2024-12-14 11:23:52.364511: train_loss -0.7918
2024-12-14 11:23:52.365350: val_loss -0.3963
2024-12-14 11:23:52.366123: Pseudo dice [0.6985]
2024-12-14 11:23:52.366784: Epoch time: 604.24 s
2024-12-14 11:23:54.677135: 
2024-12-14 11:23:54.678353: Epoch 104
2024-12-14 11:23:54.679074: Current learning rate: 0.00345
2024-12-14 11:33:58.264773: Validation loss did not improve from -0.46907. Patience: 32/50
2024-12-14 11:33:58.265777: train_loss -0.7957
2024-12-14 11:33:58.266581: val_loss -0.4202
2024-12-14 11:33:58.267242: Pseudo dice [0.7167]
2024-12-14 11:33:58.268127: Epoch time: 603.59 s
2024-12-14 11:34:00.066699: 
2024-12-14 11:34:00.068022: Epoch 105
2024-12-14 11:34:00.068940: Current learning rate: 0.00338
2024-12-14 11:44:13.981003: Validation loss did not improve from -0.46907. Patience: 33/50
2024-12-14 11:44:13.981929: train_loss -0.7933
2024-12-14 11:44:13.982702: val_loss -0.4272
2024-12-14 11:44:13.983426: Pseudo dice [0.7138]
2024-12-14 11:44:13.984014: Epoch time: 613.92 s
2024-12-14 11:44:13.984617: Yayy! New best EMA pseudo Dice: 0.7102
2024-12-14 11:44:15.776132: 
2024-12-14 11:44:15.777291: Epoch 106
2024-12-14 11:44:15.777992: Current learning rate: 0.00332
2024-12-14 11:54:36.082551: Validation loss did not improve from -0.46907. Patience: 34/50
2024-12-14 11:54:36.083523: train_loss -0.7955
2024-12-14 11:54:36.084546: val_loss -0.4202
2024-12-14 11:54:36.085493: Pseudo dice [0.7195]
2024-12-14 11:54:36.086459: Epoch time: 620.31 s
2024-12-14 11:54:36.087425: Yayy! New best EMA pseudo Dice: 0.7111
2024-12-14 11:54:37.905949: 
2024-12-14 11:54:37.907359: Epoch 107
2024-12-14 11:54:37.908256: Current learning rate: 0.00325
2024-12-14 12:05:10.441329: Validation loss did not improve from -0.46907. Patience: 35/50
2024-12-14 12:05:10.467527: train_loss -0.8
2024-12-14 12:05:10.468753: val_loss -0.4517
2024-12-14 12:05:10.469845: Pseudo dice [0.7081]
2024-12-14 12:05:10.470564: Epoch time: 632.54 s
2024-12-14 12:05:11.924688: 
2024-12-14 12:05:11.925879: Epoch 108
2024-12-14 12:05:11.926611: Current learning rate: 0.00318
2024-12-14 12:15:26.295221: Validation loss did not improve from -0.46907. Patience: 36/50
2024-12-14 12:15:26.296398: train_loss -0.798
2024-12-14 12:15:26.297465: val_loss -0.4611
2024-12-14 12:15:26.298385: Pseudo dice [0.7282]
2024-12-14 12:15:26.299311: Epoch time: 614.37 s
2024-12-14 12:15:26.300120: Yayy! New best EMA pseudo Dice: 0.7125
2024-12-14 12:15:28.165555: 
2024-12-14 12:15:28.166965: Epoch 109
2024-12-14 12:15:28.167850: Current learning rate: 0.00311
2024-12-14 12:25:42.013877: Validation loss did not improve from -0.46907. Patience: 37/50
2024-12-14 12:25:42.015010: train_loss -0.799
2024-12-14 12:25:42.015892: val_loss -0.4357
2024-12-14 12:25:42.016689: Pseudo dice [0.7151]
2024-12-14 12:25:42.017529: Epoch time: 613.85 s
2024-12-14 12:25:42.410394: Yayy! New best EMA pseudo Dice: 0.7128
2024-12-14 12:25:44.191685: 
2024-12-14 12:25:44.193067: Epoch 110
2024-12-14 12:25:44.194010: Current learning rate: 0.00304
2024-12-14 12:36:20.280047: Validation loss did not improve from -0.46907. Patience: 38/50
2024-12-14 12:36:20.280983: train_loss -0.8019
2024-12-14 12:36:20.281900: val_loss -0.3986
2024-12-14 12:36:20.282671: Pseudo dice [0.6953]
2024-12-14 12:36:20.283427: Epoch time: 636.09 s
2024-12-14 12:36:21.707910: 
2024-12-14 12:36:21.709356: Epoch 111
2024-12-14 12:36:21.710302: Current learning rate: 0.00297
2024-12-14 12:46:20.625475: Validation loss did not improve from -0.46907. Patience: 39/50
2024-12-14 12:46:20.626521: train_loss -0.7945
2024-12-14 12:46:20.627522: val_loss -0.4133
2024-12-14 12:46:20.628464: Pseudo dice [0.6939]
2024-12-14 12:46:20.629395: Epoch time: 598.92 s
2024-12-14 12:46:21.988483: 
2024-12-14 12:46:21.989763: Epoch 112
2024-12-14 12:46:21.990488: Current learning rate: 0.00291
2024-12-14 12:56:18.245859: Validation loss did not improve from -0.46907. Patience: 40/50
2024-12-14 12:56:18.246894: train_loss -0.8009
2024-12-14 12:56:18.247727: val_loss -0.4281
2024-12-14 12:56:18.248503: Pseudo dice [0.7083]
2024-12-14 12:56:18.249279: Epoch time: 596.26 s
2024-12-14 12:56:19.638638: 
2024-12-14 12:56:19.640209: Epoch 113
2024-12-14 12:56:19.641115: Current learning rate: 0.00284
2024-12-14 13:05:40.598684: Validation loss did not improve from -0.46907. Patience: 41/50
2024-12-14 13:05:40.599760: train_loss -0.8002
2024-12-14 13:05:40.600505: val_loss -0.4079
2024-12-14 13:05:40.601187: Pseudo dice [0.707]
2024-12-14 13:05:40.601938: Epoch time: 560.96 s
2024-12-14 13:05:41.987385: 
2024-12-14 13:05:41.988762: Epoch 114
2024-12-14 13:05:41.989482: Current learning rate: 0.00277
2024-12-14 13:14:59.114559: Validation loss did not improve from -0.46907. Patience: 42/50
2024-12-14 13:14:59.119624: train_loss -0.802
2024-12-14 13:14:59.121926: val_loss -0.4133
2024-12-14 13:14:59.122734: Pseudo dice [0.6956]
2024-12-14 13:14:59.124003: Epoch time: 557.13 s
2024-12-14 13:15:01.324939: 
2024-12-14 13:15:01.326126: Epoch 115
2024-12-14 13:15:01.326844: Current learning rate: 0.0027
2024-12-14 13:24:04.634794: Validation loss did not improve from -0.46907. Patience: 43/50
2024-12-14 13:24:04.635697: train_loss -0.8039
2024-12-14 13:24:04.636477: val_loss -0.4472
2024-12-14 13:24:04.637333: Pseudo dice [0.7222]
2024-12-14 13:24:04.638174: Epoch time: 543.31 s
2024-12-14 13:24:06.044919: 
2024-12-14 13:24:06.046100: Epoch 116
2024-12-14 13:24:06.046871: Current learning rate: 0.00263
2024-12-14 13:33:22.360943: Validation loss did not improve from -0.46907. Patience: 44/50
2024-12-14 13:33:22.362310: train_loss -0.8018
2024-12-14 13:33:22.363290: val_loss -0.4373
2024-12-14 13:33:22.364169: Pseudo dice [0.706]
2024-12-14 13:33:22.365011: Epoch time: 556.32 s
2024-12-14 13:33:23.748651: 
2024-12-14 13:33:23.749951: Epoch 117
2024-12-14 13:33:23.750880: Current learning rate: 0.00256
2024-12-14 13:42:18.409638: Validation loss did not improve from -0.46907. Patience: 45/50
2024-12-14 13:42:18.410549: train_loss -0.8014
2024-12-14 13:42:18.411344: val_loss -0.4365
2024-12-14 13:42:18.412226: Pseudo dice [0.7165]
2024-12-14 13:42:18.413211: Epoch time: 534.66 s
2024-12-14 13:42:19.824657: 
2024-12-14 13:42:19.826071: Epoch 118
2024-12-14 13:42:19.827095: Current learning rate: 0.00249
2024-12-14 13:51:57.574205: Validation loss did not improve from -0.46907. Patience: 46/50
2024-12-14 13:51:57.575277: train_loss -0.8033
2024-12-14 13:51:57.576164: val_loss -0.4296
2024-12-14 13:51:57.576910: Pseudo dice [0.7041]
2024-12-14 13:51:57.577682: Epoch time: 577.75 s
2024-12-14 13:51:58.959406: 
2024-12-14 13:51:58.960757: Epoch 119
2024-12-14 13:51:58.961511: Current learning rate: 0.00242
2024-12-14 14:01:32.882346: Validation loss did not improve from -0.46907. Patience: 47/50
2024-12-14 14:01:32.883183: train_loss -0.8032
2024-12-14 14:01:32.884175: val_loss -0.4685
2024-12-14 14:01:32.885141: Pseudo dice [0.7274]
2024-12-14 14:01:32.886000: Epoch time: 573.93 s
2024-12-14 14:01:34.708854: 
2024-12-14 14:01:34.710201: Epoch 120
2024-12-14 14:01:34.711248: Current learning rate: 0.00235
2024-12-14 14:10:52.690006: Validation loss did not improve from -0.46907. Patience: 48/50
2024-12-14 14:10:52.690956: train_loss -0.8053
2024-12-14 14:10:52.691839: val_loss -0.4078
2024-12-14 14:10:52.692777: Pseudo dice [0.7036]
2024-12-14 14:10:52.693589: Epoch time: 557.98 s
2024-12-14 14:10:54.107156: 
2024-12-14 14:10:54.108401: Epoch 121
2024-12-14 14:10:54.109348: Current learning rate: 0.00228
2024-12-14 14:20:12.855542: Validation loss did not improve from -0.46907. Patience: 49/50
2024-12-14 14:20:12.861481: train_loss -0.8065
2024-12-14 14:20:12.864123: val_loss -0.4437
2024-12-14 14:20:12.865059: Pseudo dice [0.7159]
2024-12-14 14:20:12.866491: Epoch time: 558.75 s
2024-12-14 14:20:14.279402: 
2024-12-14 14:20:14.280899: Epoch 122
2024-12-14 14:20:14.282073: Current learning rate: 0.00221
2024-12-14 14:28:05.626951: Validation loss did not improve from -0.46907. Patience: 50/50
2024-12-14 14:28:05.627800: train_loss -0.8082
2024-12-14 14:28:05.628714: val_loss -0.4099
2024-12-14 14:28:05.629440: Pseudo dice [0.7196]
2024-12-14 14:28:05.630075: Epoch time: 471.35 s
2024-12-14 14:28:07.027717: 
2024-12-14 14:28:07.029109: Epoch 123
2024-12-14 14:28:07.029924: Current learning rate: 0.00214
2024-12-14 14:36:04.717324: Validation loss did not improve from -0.46907. Patience: 51/50
2024-12-14 14:36:04.718487: train_loss -0.8048
2024-12-14 14:36:04.719402: val_loss -0.4059
2024-12-14 14:36:04.720292: Pseudo dice [0.7059]
2024-12-14 14:36:04.721011: Epoch time: 477.69 s
2024-12-14 14:36:06.135854: 
2024-12-14 14:36:06.137280: Epoch 124
2024-12-14 14:36:06.138059: Current learning rate: 0.00207
2024-12-14 14:44:17.355267: Validation loss did not improve from -0.46907. Patience: 52/50
2024-12-14 14:44:17.356223: train_loss -0.8067
2024-12-14 14:44:17.357054: val_loss -0.4192
2024-12-14 14:44:17.357728: Pseudo dice [0.6959]
2024-12-14 14:44:17.358489: Epoch time: 491.22 s
2024-12-14 14:44:19.252156: 
2024-12-14 14:44:19.253465: Epoch 125
2024-12-14 14:44:19.254563: Current learning rate: 0.00199
2024-12-14 14:52:45.298275: Validation loss did not improve from -0.46907. Patience: 53/50
2024-12-14 14:52:45.299494: train_loss -0.8099
2024-12-14 14:52:45.300352: val_loss -0.3958
2024-12-14 14:52:45.301035: Pseudo dice [0.7061]
2024-12-14 14:52:45.301810: Epoch time: 506.05 s
2024-12-14 14:52:47.202640: 
2024-12-14 14:52:47.204516: Epoch 126
2024-12-14 14:52:47.205381: Current learning rate: 0.00192
2024-12-14 15:01:20.805119: Validation loss did not improve from -0.46907. Patience: 54/50
2024-12-14 15:01:20.806933: train_loss -0.8118
2024-12-14 15:01:20.808055: val_loss -0.4258
2024-12-14 15:01:20.808842: Pseudo dice [0.718]
2024-12-14 15:01:20.809904: Epoch time: 513.61 s
2024-12-14 15:01:22.253418: 
2024-12-14 15:01:22.254865: Epoch 127
2024-12-14 15:01:22.255699: Current learning rate: 0.00185
2024-12-14 15:09:26.298058: Validation loss did not improve from -0.46907. Patience: 55/50
2024-12-14 15:09:26.298990: train_loss -0.8074
2024-12-14 15:09:26.299842: val_loss -0.4235
2024-12-14 15:09:26.300632: Pseudo dice [0.7184]
2024-12-14 15:09:26.301474: Epoch time: 484.05 s
2024-12-14 15:09:27.702567: 
2024-12-14 15:09:27.704112: Epoch 128
2024-12-14 15:09:27.704948: Current learning rate: 0.00178
2024-12-14 15:18:00.019176: Validation loss did not improve from -0.46907. Patience: 56/50
2024-12-14 15:18:00.021156: train_loss -0.8077
2024-12-14 15:18:00.022327: val_loss -0.4181
2024-12-14 15:18:00.023015: Pseudo dice [0.7044]
2024-12-14 15:18:00.023843: Epoch time: 512.32 s
2024-12-14 15:18:01.469457: 
2024-12-14 15:18:01.471037: Epoch 129
2024-12-14 15:18:01.471913: Current learning rate: 0.0017
2024-12-14 15:25:15.532236: Validation loss did not improve from -0.46907. Patience: 57/50
2024-12-14 15:25:15.533718: train_loss -0.8112
2024-12-14 15:25:15.535417: val_loss -0.4332
2024-12-14 15:25:15.536241: Pseudo dice [0.7112]
2024-12-14 15:25:15.537014: Epoch time: 434.07 s
2024-12-14 15:25:17.292021: 
2024-12-14 15:25:17.293446: Epoch 130
2024-12-14 15:25:17.294447: Current learning rate: 0.00163
2024-12-14 15:31:06.581897: Validation loss did not improve from -0.46907. Patience: 58/50
2024-12-14 15:31:06.583250: train_loss -0.8149
2024-12-14 15:31:06.584114: val_loss -0.4089
2024-12-14 15:31:06.585064: Pseudo dice [0.6968]
2024-12-14 15:31:06.585761: Epoch time: 349.29 s
2024-12-14 15:31:07.950089: 
2024-12-14 15:31:07.951311: Epoch 131
2024-12-14 15:31:07.952046: Current learning rate: 0.00156
2024-12-14 15:36:46.637390: Validation loss did not improve from -0.46907. Patience: 59/50
2024-12-14 15:36:46.638397: train_loss -0.8139
2024-12-14 15:36:46.639240: val_loss -0.4494
2024-12-14 15:36:46.639980: Pseudo dice [0.724]
2024-12-14 15:36:46.640712: Epoch time: 338.69 s
2024-12-14 15:36:48.009195: 
2024-12-14 15:36:48.010536: Epoch 132
2024-12-14 15:36:48.011235: Current learning rate: 0.00148
2024-12-14 15:42:38.613656: Validation loss did not improve from -0.46907. Patience: 60/50
2024-12-14 15:42:38.615061: train_loss -0.8149
2024-12-14 15:42:38.616102: val_loss -0.3777
2024-12-14 15:42:38.616799: Pseudo dice [0.6825]
2024-12-14 15:42:38.617605: Epoch time: 350.61 s
2024-12-14 15:42:39.977523: 
2024-12-14 15:42:39.979104: Epoch 133
2024-12-14 15:42:39.980006: Current learning rate: 0.00141
2024-12-14 15:48:09.314841: Validation loss did not improve from -0.46907. Patience: 61/50
2024-12-14 15:48:09.315905: train_loss -0.8126
2024-12-14 15:48:09.316952: val_loss -0.4479
2024-12-14 15:48:09.317948: Pseudo dice [0.7212]
2024-12-14 15:48:09.318899: Epoch time: 329.34 s
2024-12-14 15:48:10.731936: 
2024-12-14 15:48:10.733388: Epoch 134
2024-12-14 15:48:10.734401: Current learning rate: 0.00133
2024-12-14 15:53:30.255685: Validation loss did not improve from -0.46907. Patience: 62/50
2024-12-14 15:53:30.256627: train_loss -0.8127
2024-12-14 15:53:30.257819: val_loss -0.4362
2024-12-14 15:53:30.259173: Pseudo dice [0.7167]
2024-12-14 15:53:30.260104: Epoch time: 319.53 s
2024-12-14 15:53:32.038572: 
2024-12-14 15:53:32.039941: Epoch 135
2024-12-14 15:53:32.041045: Current learning rate: 0.00126
2024-12-14 15:58:52.062231: Validation loss did not improve from -0.46907. Patience: 63/50
2024-12-14 15:58:52.063340: train_loss -0.8135
2024-12-14 15:58:52.064304: val_loss -0.4038
2024-12-14 15:58:52.065110: Pseudo dice [0.7037]
2024-12-14 15:58:52.066032: Epoch time: 320.03 s
2024-12-14 15:58:53.456401: 
2024-12-14 15:58:53.457696: Epoch 136
2024-12-14 15:58:53.458443: Current learning rate: 0.00118
2024-12-14 16:04:51.797749: Validation loss did not improve from -0.46907. Patience: 64/50
2024-12-14 16:04:51.801294: train_loss -0.8158
2024-12-14 16:04:51.803182: val_loss -0.4164
2024-12-14 16:04:51.804247: Pseudo dice [0.7157]
2024-12-14 16:04:51.805282: Epoch time: 358.35 s
2024-12-14 16:04:54.246264: 
2024-12-14 16:04:54.247733: Epoch 137
2024-12-14 16:04:54.248583: Current learning rate: 0.00111
2024-12-14 16:10:29.645377: Validation loss did not improve from -0.46907. Patience: 65/50
2024-12-14 16:10:29.646306: train_loss -0.8146
2024-12-14 16:10:29.647106: val_loss -0.432
2024-12-14 16:10:29.647804: Pseudo dice [0.7145]
2024-12-14 16:10:29.648485: Epoch time: 335.4 s
2024-12-14 16:10:31.050071: 
2024-12-14 16:10:31.051467: Epoch 138
2024-12-14 16:10:31.052606: Current learning rate: 0.00103
2024-12-14 16:15:49.027120: Validation loss did not improve from -0.46907. Patience: 66/50
2024-12-14 16:15:49.028520: train_loss -0.8117
2024-12-14 16:15:49.029384: val_loss -0.4555
2024-12-14 16:15:49.030212: Pseudo dice [0.7317]
2024-12-14 16:15:49.030985: Epoch time: 317.98 s
2024-12-14 16:15:50.411431: 
2024-12-14 16:15:50.412883: Epoch 139
2024-12-14 16:15:50.414284: Current learning rate: 0.00095
2024-12-14 16:20:32.239105: Validation loss did not improve from -0.46907. Patience: 67/50
2024-12-14 16:20:32.240116: train_loss -0.8167
2024-12-14 16:20:32.240882: val_loss -0.4135
2024-12-14 16:20:32.241547: Pseudo dice [0.7124]
2024-12-14 16:20:32.242251: Epoch time: 281.83 s
2024-12-14 16:20:34.062703: 
2024-12-14 16:20:34.063756: Epoch 140
2024-12-14 16:20:34.064510: Current learning rate: 0.00087
2024-12-14 16:25:52.404725: Validation loss did not improve from -0.46907. Patience: 68/50
2024-12-14 16:25:52.405896: train_loss -0.8143
2024-12-14 16:25:52.406931: val_loss -0.4307
2024-12-14 16:25:52.407665: Pseudo dice [0.7192]
2024-12-14 16:25:52.408396: Epoch time: 318.34 s
2024-12-14 16:25:52.409076: Yayy! New best EMA pseudo Dice: 0.7131
2024-12-14 16:25:54.342124: 
2024-12-14 16:25:54.343355: Epoch 141
2024-12-14 16:25:54.344134: Current learning rate: 0.00079
2024-12-14 16:31:15.472923: Validation loss did not improve from -0.46907. Patience: 69/50
2024-12-14 16:31:15.474432: train_loss -0.8173
2024-12-14 16:31:15.475380: val_loss -0.4102
2024-12-14 16:31:15.476488: Pseudo dice [0.7052]
2024-12-14 16:31:15.478088: Epoch time: 321.13 s
2024-12-14 16:31:16.837526: 
2024-12-14 16:31:16.838954: Epoch 142
2024-12-14 16:31:16.839966: Current learning rate: 0.00071
2024-12-14 16:36:44.253353: Validation loss did not improve from -0.46907. Patience: 70/50
2024-12-14 16:36:44.254761: train_loss -0.8184
2024-12-14 16:36:44.255534: val_loss -0.429
2024-12-14 16:36:44.256272: Pseudo dice [0.708]
2024-12-14 16:36:44.256933: Epoch time: 327.42 s
2024-12-14 16:36:45.683105: 
2024-12-14 16:36:45.684535: Epoch 143
2024-12-14 16:36:45.685267: Current learning rate: 0.00063
2024-12-14 16:43:42.712746: Validation loss did not improve from -0.46907. Patience: 71/50
2024-12-14 16:43:42.713634: train_loss -0.8154
2024-12-14 16:43:42.714554: val_loss -0.4312
2024-12-14 16:43:42.715399: Pseudo dice [0.7079]
2024-12-14 16:43:42.716286: Epoch time: 417.03 s
2024-12-14 16:43:44.098456: 
2024-12-14 16:43:44.100026: Epoch 144
2024-12-14 16:43:44.100934: Current learning rate: 0.00055
2024-12-14 16:50:37.448069: Validation loss did not improve from -0.46907. Patience: 72/50
2024-12-14 16:50:37.449159: train_loss -0.8149
2024-12-14 16:50:37.450175: val_loss -0.4194
2024-12-14 16:50:37.450999: Pseudo dice [0.7081]
2024-12-14 16:50:37.451689: Epoch time: 413.35 s
2024-12-14 16:50:39.293839: 
2024-12-14 16:50:39.295291: Epoch 145
2024-12-14 16:50:39.296100: Current learning rate: 0.00047
2024-12-14 16:57:33.467966: Validation loss did not improve from -0.46907. Patience: 73/50
2024-12-14 16:57:33.468965: train_loss -0.8194
2024-12-14 16:57:33.469795: val_loss -0.4122
2024-12-14 16:57:33.470513: Pseudo dice [0.7145]
2024-12-14 16:57:33.471279: Epoch time: 414.18 s
2024-12-14 16:57:35.163160: 
2024-12-14 16:57:35.164073: Epoch 146
2024-12-14 16:57:35.164810: Current learning rate: 0.00038
2024-12-14 17:04:41.366907: Validation loss did not improve from -0.46907. Patience: 74/50
2024-12-14 17:04:41.370584: train_loss -0.8183
2024-12-14 17:04:41.372005: val_loss -0.3804
2024-12-14 17:04:41.372895: Pseudo dice [0.6923]
2024-12-14 17:04:41.373666: Epoch time: 426.21 s
2024-12-14 17:04:42.787567: 
2024-12-14 17:04:42.788712: Epoch 147
2024-12-14 17:04:42.789419: Current learning rate: 0.0003
2024-12-14 17:12:00.392575: Validation loss did not improve from -0.46907. Patience: 75/50
2024-12-14 17:12:00.393671: train_loss -0.8182
2024-12-14 17:12:00.395240: val_loss -0.423
2024-12-14 17:12:00.396488: Pseudo dice [0.7078]
2024-12-14 17:12:00.397260: Epoch time: 437.61 s
2024-12-14 17:12:02.636398: 
2024-12-14 17:12:02.637693: Epoch 148
2024-12-14 17:12:02.638468: Current learning rate: 0.00021
2024-12-14 17:18:59.549942: Validation loss did not improve from -0.46907. Patience: 76/50
2024-12-14 17:18:59.550812: train_loss -0.8192
2024-12-14 17:18:59.551545: val_loss -0.4204
2024-12-14 17:18:59.552287: Pseudo dice [0.7132]
2024-12-14 17:18:59.553286: Epoch time: 416.92 s
2024-12-14 17:19:00.964768: 
2024-12-14 17:19:00.966269: Epoch 149
2024-12-14 17:19:00.967323: Current learning rate: 0.00011
2024-12-14 17:26:06.343531: Validation loss did not improve from -0.46907. Patience: 77/50
2024-12-14 17:26:06.344554: train_loss -0.816
2024-12-14 17:26:06.345289: val_loss -0.4233
2024-12-14 17:26:06.345993: Pseudo dice [0.7098]
2024-12-14 17:26:06.346866: Epoch time: 425.38 s
2024-12-14 17:26:08.398141: Training done.
2024-12-14 17:26:08.550883: Using splits from existing split file: /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset307_Sohee_Calcium_OCT_CrossValidation/splits_final_60.json
2024-12-14 17:26:08.553233: The split file contains 5 splits.
2024-12-14 17:26:08.553939: Desired fold for training: 1
2024-12-14 17:26:08.554660: This split has 4 training and 5 validation cases.
2024-12-14 17:26:08.555913: predicting 101-019
2024-12-14 17:26:08.586108: 101-019, shape torch.Size([1, 375, 498, 498]), rank 0
2024-12-14 17:28:40.457147: predicting 101-045
2024-12-14 17:28:40.474594: 101-045, shape torch.Size([1, 375, 498, 498]), rank 0
2024-12-14 17:30:45.150695: predicting 106-002
2024-12-14 17:30:45.168010: 106-002, shape torch.Size([1, 539, 498, 498]), rank 0
2024-12-14 17:33:56.665788: predicting 704-003
2024-12-14 17:33:56.683130: 704-003, shape torch.Size([1, 375, 498, 498]), rank 0
2024-12-14 17:36:01.319535: predicting 706-005
2024-12-14 17:36:01.334667: 706-005, shape torch.Size([1, 375, 498, 498]), rank 0
2024-12-14 17:38:37.954006: Validation complete
2024-12-14 17:38:37.954711: Mean Validation Dice:  0.7066545309153048

############################
INFO: You are using the old nnU-Net default plans. We have updated our recommendations. Please consider using those instead! Read more here: https://github.com/MIC-DKFZ/nnUNet/blob/master/documentation/resenc_presets.md
############################

Using device: cuda:0

#######################################################################
Please cite the following paper when using nnU-Net:
Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.
#######################################################################

2024-12-14 17:38:50.581888: Using torch.compile...

############################
INFO: You are using the old nnU-Net default plans. We have updated our recommendations. Please consider using those instead! Read more here: https://github.com/MIC-DKFZ/nnUNet/blob/master/documentation/resenc_presets.md
############################

Using device: cuda:0

#######################################################################
Please cite the following paper when using nnU-Net:
Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.
#######################################################################

2024-12-14 17:38:50.578358: Using torch.compile...
/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
################### Loading pretrained weights from file  /home/gridsan/nchutisilp/projects/OpenAI-CLIP/clip_preivl_poststent_pretrained_nnUNet.pt ###################
Below is the list of overlapping blocks in pretrained model and nnUNet architecture:
encoder.stages.0.0.convs.0.conv.weight shape torch.Size([32, 1, 3, 3, 3])
encoder.stages.0.0.convs.0.conv.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.norm.weight shape torch.Size([32])
encoder.stages.0.0.convs.0.norm.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.0.weight shape torch.Size([32, 1, 3, 3, 3])
encoder.stages.0.0.convs.0.all_modules.0.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.1.weight shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.1.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
encoder.stages.0.0.convs.1.conv.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.norm.weight shape torch.Size([32])
encoder.stages.0.0.convs.1.norm.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
encoder.stages.0.0.convs.1.all_modules.0.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.1.weight shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.1.bias shape torch.Size([32])
encoder.stages.1.0.convs.0.conv.weight shape torch.Size([64, 32, 3, 3, 3])
encoder.stages.1.0.convs.0.conv.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.norm.weight shape torch.Size([64])
encoder.stages.1.0.convs.0.norm.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.0.weight shape torch.Size([64, 32, 3, 3, 3])
encoder.stages.1.0.convs.0.all_modules.0.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.1.weight shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.1.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
encoder.stages.1.0.convs.1.conv.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.norm.weight shape torch.Size([64])
encoder.stages.1.0.convs.1.norm.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
encoder.stages.1.0.convs.1.all_modules.0.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.1.weight shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.1.bias shape torch.Size([64])
encoder.stages.2.0.convs.0.conv.weight shape torch.Size([128, 64, 3, 3, 3])
encoder.stages.2.0.convs.0.conv.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.norm.weight shape torch.Size([128])
encoder.stages.2.0.convs.0.norm.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.0.weight shape torch.Size([128, 64, 3, 3, 3])
encoder.stages.2.0.convs.0.all_modules.0.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.1.weight shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.1.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
encoder.stages.2.0.convs.1.conv.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.norm.weight shape torch.Size([128])
encoder.stages.2.0.convs.1.norm.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
encoder.stages.2.0.convs.1.all_modules.0.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.1.weight shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.1.bias shape torch.Size([128])
encoder.stages.3.0.convs.0.conv.weight shape torch.Size([256, 128, 3, 3, 3])
encoder.stages.3.0.convs.0.conv.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.norm.weight shape torch.Size([256])
encoder.stages.3.0.convs.0.norm.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.0.weight shape torch.Size([256, 128, 3, 3, 3])
encoder.stages.3.0.convs.0.all_modules.0.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.1.weight shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.1.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
encoder.stages.3.0.convs.1.conv.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.norm.weight shape torch.Size([256])
encoder.stages.3.0.convs.1.norm.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
encoder.stages.3.0.convs.1.all_modules.0.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.1.weight shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.1.bias shape torch.Size([256])
encoder.stages.4.0.convs.0.conv.weight shape torch.Size([320, 256, 3, 3, 3])
encoder.stages.4.0.convs.0.conv.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.norm.weight shape torch.Size([320])
encoder.stages.4.0.convs.0.norm.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.0.weight shape torch.Size([320, 256, 3, 3, 3])
encoder.stages.4.0.convs.0.all_modules.0.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.1.weight shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.1.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.4.0.convs.1.conv.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.norm.weight shape torch.Size([320])
encoder.stages.4.0.convs.1.norm.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.4.0.convs.1.all_modules.0.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.1.weight shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.1.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.0.conv.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.norm.weight shape torch.Size([320])
encoder.stages.5.0.convs.0.norm.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.0.all_modules.0.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.1.weight shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.1.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.1.conv.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.norm.weight shape torch.Size([320])
encoder.stages.5.0.convs.1.norm.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.1.all_modules.0.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.1.weight shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.0.0.convs.0.conv.weight shape torch.Size([32, 1, 3, 3, 3])
decoder.encoder.stages.0.0.convs.0.conv.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.norm.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.norm.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.0.weight shape torch.Size([32, 1, 3, 3, 3])
decoder.encoder.stages.0.0.convs.0.all_modules.0.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.1.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.1.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.encoder.stages.0.0.convs.1.conv.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.norm.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.norm.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.encoder.stages.0.0.convs.1.all_modules.0.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.1.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.1.bias shape torch.Size([32])
decoder.encoder.stages.1.0.convs.0.conv.weight shape torch.Size([64, 32, 3, 3, 3])
decoder.encoder.stages.1.0.convs.0.conv.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.norm.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.norm.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.0.weight shape torch.Size([64, 32, 3, 3, 3])
decoder.encoder.stages.1.0.convs.0.all_modules.0.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.1.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.1.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.encoder.stages.1.0.convs.1.conv.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.norm.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.norm.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.encoder.stages.1.0.convs.1.all_modules.0.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.1.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.1.bias shape torch.Size([64])
decoder.encoder.stages.2.0.convs.0.conv.weight shape torch.Size([128, 64, 3, 3, 3])
decoder.encoder.stages.2.0.convs.0.conv.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.norm.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.norm.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.0.weight shape torch.Size([128, 64, 3, 3, 3])
decoder.encoder.stages.2.0.convs.0.all_modules.0.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.1.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.1.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.encoder.stages.2.0.convs.1.conv.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.norm.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.norm.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.encoder.stages.2.0.convs.1.all_modules.0.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.1.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.1.bias shape torch.Size([128])
decoder.encoder.stages.3.0.convs.0.conv.weight shape torch.Size([256, 128, 3, 3, 3])
decoder.encoder.stages.3.0.convs.0.conv.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.norm.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.norm.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.0.weight shape torch.Size([256, 128, 3, 3, 3])
decoder.encoder.stages.3.0.convs.0.all_modules.0.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.1.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.1.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.encoder.stages.3.0.convs.1.conv.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.norm.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.norm.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.encoder.stages.3.0.convs.1.all_modules.0.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.1.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.1.bias shape torch.Size([256])
decoder.encoder.stages.4.0.convs.0.conv.weight shape torch.Size([320, 256, 3, 3, 3])
decoder.encoder.stages.4.0.convs.0.conv.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.norm.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.norm.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.0.weight shape torch.Size([320, 256, 3, 3, 3])
decoder.encoder.stages.4.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.4.0.convs.1.conv.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.norm.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.norm.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.4.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.0.conv.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.norm.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.norm.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.1.conv.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.norm.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.norm.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.stages.0.convs.0.conv.weight shape torch.Size([320, 640, 3, 3, 3])
decoder.stages.0.convs.0.conv.bias shape torch.Size([320])
decoder.stages.0.convs.0.norm.weight shape torch.Size([320])
decoder.stages.0.convs.0.norm.bias shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.0.weight shape torch.Size([320, 640, 3, 3, 3])
decoder.stages.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.stages.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.stages.0.convs.1.conv.bias shape torch.Size([320])
decoder.stages.0.convs.1.norm.weight shape torch.Size([320])
decoder.stages.0.convs.1.norm.bias shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.stages.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.stages.1.convs.0.conv.weight shape torch.Size([256, 512, 3, 3, 3])
decoder.stages.1.convs.0.conv.bias shape torch.Size([256])
decoder.stages.1.convs.0.norm.weight shape torch.Size([256])
decoder.stages.1.convs.0.norm.bias shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.0.weight shape torch.Size([256, 512, 3, 3, 3])
decoder.stages.1.convs.0.all_modules.0.bias shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.1.weight shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.1.bias shape torch.Size([256])
decoder.stages.1.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.stages.1.convs.1.conv.bias shape torch.Size([256])
decoder.stages.1.convs.1.norm.weight shape torch.Size([256])
decoder.stages.1.convs.1.norm.bias shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.stages.1.convs.1.all_modules.0.bias shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.1.weight shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.1.bias shape torch.Size([256])
decoder.stages.2.convs.0.conv.weight shape torch.Size([128, 256, 3, 3, 3])
decoder.stages.2.convs.0.conv.bias shape torch.Size([128])
decoder.stages.2.convs.0.norm.weight shape torch.Size([128])
decoder.stages.2.convs.0.norm.bias shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.0.weight shape torch.Size([128, 256, 3, 3, 3])
decoder.stages.2.convs.0.all_modules.0.bias shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.1.weight shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.1.bias shape torch.Size([128])
decoder.stages.2.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.stages.2.convs.1.conv.bias shape torch.Size([128])
decoder.stages.2.convs.1.norm.weight shape torch.Size([128])
decoder.stages.2.convs.1.norm.bias shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.stages.2.convs.1.all_modules.0.bias shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.1.weight shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.1.bias shape torch.Size([128])
decoder.stages.3.convs.0.conv.weight shape torch.Size([64, 128, 3, 3, 3])
decoder.stages.3.convs.0.conv.bias shape torch.Size([64])
decoder.stages.3.convs.0.norm.weight shape torch.Size([64])
decoder.stages.3.convs.0.norm.bias shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.0.weight shape torch.Size([64, 128, 3, 3, 3])
decoder.stages.3.convs.0.all_modules.0.bias shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.1.weight shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.1.bias shape torch.Size([64])
decoder.stages.3.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.stages.3.convs.1.conv.bias shape torch.Size([64])
decoder.stages.3.convs.1.norm.weight shape torch.Size([64])
decoder.stages.3.convs.1.norm.bias shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.stages.3.convs.1.all_modules.0.bias shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.1.weight shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.1.bias shape torch.Size([64])
decoder.stages.4.convs.0.conv.weight shape torch.Size([32, 64, 3, 3, 3])
decoder.stages.4.convs.0.conv.bias shape torch.Size([32])
decoder.stages.4.convs.0.norm.weight shape torch.Size([32])
decoder.stages.4.convs.0.norm.bias shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.0.weight shape torch.Size([32, 64, 3, 3, 3])
decoder.stages.4.convs.0.all_modules.0.bias shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.1.weight shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.1.bias shape torch.Size([32])
decoder.stages.4.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.stages.4.convs.1.conv.bias shape torch.Size([32])
decoder.stages.4.convs.1.norm.weight shape torch.Size([32])
decoder.stages.4.convs.1.norm.bias shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.stages.4.convs.1.all_modules.0.bias shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.1.weight shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.1.bias shape torch.Size([32])
decoder.transpconvs.0.weight shape torch.Size([320, 320, 1, 2, 2])
decoder.transpconvs.0.bias shape torch.Size([320])
decoder.transpconvs.1.weight shape torch.Size([320, 256, 2, 2, 2])
decoder.transpconvs.1.bias shape torch.Size([256])
decoder.transpconvs.2.weight shape torch.Size([256, 128, 2, 2, 2])
decoder.transpconvs.2.bias shape torch.Size([128])
decoder.transpconvs.3.weight shape torch.Size([128, 64, 2, 2, 2])
decoder.transpconvs.3.bias shape torch.Size([64])
decoder.transpconvs.4.weight shape torch.Size([64, 32, 2, 2, 2])
decoder.transpconvs.4.bias shape torch.Size([32])
################### Done ###################
2024-12-14 17:38:52.995073: do_dummy_2d_data_aug: True
2024-12-14 17:38:52.996363: Using splits from existing split file: /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset307_Sohee_Calcium_OCT_CrossValidation/splits_final_60.json
2024-12-14 17:38:52.998008: The split file contains 5 splits.
2024-12-14 17:38:52.999331: Desired fold for training: 2
2024-12-14 17:38:53.000372: This split has 4 training and 4 validation cases.
################### Loading pretrained weights from file  /home/gridsan/nchutisilp/projects/OpenAI-CLIP/clip_preivl_poststent_pretrained_nnUNet.pt ###################
Below is the list of overlapping blocks in pretrained model and nnUNet architecture:
encoder.stages.0.0.convs.0.conv.weight shape torch.Size([32, 1, 3, 3, 3])
encoder.stages.0.0.convs.0.conv.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.norm.weight shape torch.Size([32])
encoder.stages.0.0.convs.0.norm.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.0.weight shape torch.Size([32, 1, 3, 3, 3])
encoder.stages.0.0.convs.0.all_modules.0.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.1.weight shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.1.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
encoder.stages.0.0.convs.1.conv.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.norm.weight shape torch.Size([32])
encoder.stages.0.0.convs.1.norm.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
encoder.stages.0.0.convs.1.all_modules.0.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.1.weight shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.1.bias shape torch.Size([32])
encoder.stages.1.0.convs.0.conv.weight shape torch.Size([64, 32, 3, 3, 3])
encoder.stages.1.0.convs.0.conv.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.norm.weight shape torch.Size([64])
encoder.stages.1.0.convs.0.norm.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.0.weight shape torch.Size([64, 32, 3, 3, 3])
encoder.stages.1.0.convs.0.all_modules.0.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.1.weight shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.1.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
encoder.stages.1.0.convs.1.conv.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.norm.weight shape torch.Size([64])
encoder.stages.1.0.convs.1.norm.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
encoder.stages.1.0.convs.1.all_modules.0.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.1.weight shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.1.bias shape torch.Size([64])
encoder.stages.2.0.convs.0.conv.weight shape torch.Size([128, 64, 3, 3, 3])
encoder.stages.2.0.convs.0.conv.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.norm.weight shape torch.Size([128])
encoder.stages.2.0.convs.0.norm.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.0.weight shape torch.Size([128, 64, 3, 3, 3])
encoder.stages.2.0.convs.0.all_modules.0.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.1.weight shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.1.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
encoder.stages.2.0.convs.1.conv.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.norm.weight shape torch.Size([128])
encoder.stages.2.0.convs.1.norm.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
encoder.stages.2.0.convs.1.all_modules.0.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.1.weight shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.1.bias shape torch.Size([128])
encoder.stages.3.0.convs.0.conv.weight shape torch.Size([256, 128, 3, 3, 3])
encoder.stages.3.0.convs.0.conv.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.norm.weight shape torch.Size([256])
encoder.stages.3.0.convs.0.norm.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.0.weight shape torch.Size([256, 128, 3, 3, 3])
encoder.stages.3.0.convs.0.all_modules.0.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.1.weight shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.1.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
encoder.stages.3.0.convs.1.conv.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.norm.weight shape torch.Size([256])
encoder.stages.3.0.convs.1.norm.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
encoder.stages.3.0.convs.1.all_modules.0.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.1.weight shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.1.bias shape torch.Size([256])
encoder.stages.4.0.convs.0.conv.weight shape torch.Size([320, 256, 3, 3, 3])
encoder.stages.4.0.convs.0.conv.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.norm.weight shape torch.Size([320])
encoder.stages.4.0.convs.0.norm.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.0.weight shape torch.Size([320, 256, 3, 3, 3])
encoder.stages.4.0.convs.0.all_modules.0.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.1.weight shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.1.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.4.0.convs.1.conv.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.norm.weight shape torch.Size([320])
encoder.stages.4.0.convs.1.norm.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.4.0.convs.1.all_modules.0.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.1.weight shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.1.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.0.conv.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.norm.weight shape torch.Size([320])
encoder.stages.5.0.convs.0.norm.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.0.all_modules.0.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.1.weight shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.1.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.1.conv.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.norm.weight shape torch.Size([320])
encoder.stages.5.0.convs.1.norm.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.1.all_modules.0.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.1.weight shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.0.0.convs.0.conv.weight shape torch.Size([32, 1, 3, 3, 3])
decoder.encoder.stages.0.0.convs.0.conv.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.norm.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.norm.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.0.weight shape torch.Size([32, 1, 3, 3, 3])
decoder.encoder.stages.0.0.convs.0.all_modules.0.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.1.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.1.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.encoder.stages.0.0.convs.1.conv.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.norm.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.norm.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.encoder.stages.0.0.convs.1.all_modules.0.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.1.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.1.bias shape torch.Size([32])
decoder.encoder.stages.1.0.convs.0.conv.weight shape torch.Size([64, 32, 3, 3, 3])
decoder.encoder.stages.1.0.convs.0.conv.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.norm.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.norm.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.0.weight shape torch.Size([64, 32, 3, 3, 3])
decoder.encoder.stages.1.0.convs.0.all_modules.0.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.1.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.1.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.encoder.stages.1.0.convs.1.conv.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.norm.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.norm.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.encoder.stages.1.0.convs.1.all_modules.0.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.1.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.1.bias shape torch.Size([64])
decoder.encoder.stages.2.0.convs.0.conv.weight shape torch.Size([128, 64, 3, 3, 3])
decoder.encoder.stages.2.0.convs.0.conv.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.norm.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.norm.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.0.weight shape torch.Size([128, 64, 3, 3, 3])
decoder.encoder.stages.2.0.convs.0.all_modules.0.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.1.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.1.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.encoder.stages.2.0.convs.1.conv.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.norm.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.norm.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.encoder.stages.2.0.convs.1.all_modules.0.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.1.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.1.bias shape torch.Size([128])
decoder.encoder.stages.3.0.convs.0.conv.weight shape torch.Size([256, 128, 3, 3, 3])
decoder.encoder.stages.3.0.convs.0.conv.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.norm.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.norm.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.0.weight shape torch.Size([256, 128, 3, 3, 3])
decoder.encoder.stages.3.0.convs.0.all_modules.0.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.1.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.1.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.encoder.stages.3.0.convs.1.conv.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.norm.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.norm.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.encoder.stages.3.0.convs.1.all_modules.0.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.1.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.1.bias shape torch.Size([256])
decoder.encoder.stages.4.0.convs.0.conv.weight shape torch.Size([320, 256, 3, 3, 3])
decoder.encoder.stages.4.0.convs.0.conv.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.norm.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.norm.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.0.weight shape torch.Size([320, 256, 3, 3, 3])
decoder.encoder.stages.4.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.4.0.convs.1.conv.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.norm.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.norm.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.4.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.0.conv.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.norm.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.norm.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.1.conv.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.norm.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.norm.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.stages.0.convs.0.conv.weight shape torch.Size([320, 640, 3, 3, 3])
decoder.stages.0.convs.0.conv.bias shape torch.Size([320])
decoder.stages.0.convs.0.norm.weight shape torch.Size([320])
decoder.stages.0.convs.0.norm.bias shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.0.weight shape torch.Size([320, 640, 3, 3, 3])
decoder.stages.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.stages.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.stages.0.convs.1.conv.bias shape torch.Size([320])
decoder.stages.0.convs.1.norm.weight shape torch.Size([320])
decoder.stages.0.convs.1.norm.bias shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.stages.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.stages.1.convs.0.conv.weight shape torch.Size([256, 512, 3, 3, 3])
decoder.stages.1.convs.0.conv.bias shape torch.Size([256])
decoder.stages.1.convs.0.norm.weight shape torch.Size([256])
decoder.stages.1.convs.0.norm.bias shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.0.weight shape torch.Size([256, 512, 3, 3, 3])
decoder.stages.1.convs.0.all_modules.0.bias shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.1.weight shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.1.bias shape torch.Size([256])
decoder.stages.1.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.stages.1.convs.1.conv.bias shape torch.Size([256])
decoder.stages.1.convs.1.norm.weight shape torch.Size([256])
decoder.stages.1.convs.1.norm.bias shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.stages.1.convs.1.all_modules.0.bias shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.1.weight shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.1.bias shape torch.Size([256])
decoder.stages.2.convs.0.conv.weight shape torch.Size([128, 256, 3, 3, 3])
decoder.stages.2.convs.0.conv.bias shape torch.Size([128])
decoder.stages.2.convs.0.norm.weight shape torch.Size([128])
decoder.stages.2.convs.0.norm.bias shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.0.weight shape torch.Size([128, 256, 3, 3, 3])
decoder.stages.2.convs.0.all_modules.0.bias shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.1.weight shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.1.bias shape torch.Size([128])
decoder.stages.2.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.stages.2.convs.1.conv.bias shape torch.Size([128])
decoder.stages.2.convs.1.norm.weight shape torch.Size([128])
decoder.stages.2.convs.1.norm.bias shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.stages.2.convs.1.all_modules.0.bias shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.1.weight shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.1.bias shape torch.Size([128])
decoder.stages.3.convs.0.conv.weight shape torch.Size([64, 128, 3, 3, 3])
decoder.stages.3.convs.0.conv.bias shape torch.Size([64])
decoder.stages.3.convs.0.norm.weight shape torch.Size([64])
decoder.stages.3.convs.0.norm.bias shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.0.weight shape torch.Size([64, 128, 3, 3, 3])
decoder.stages.3.convs.0.all_modules.0.bias shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.1.weight shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.1.bias shape torch.Size([64])
decoder.stages.3.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.stages.3.convs.1.conv.bias shape torch.Size([64])
decoder.stages.3.convs.1.norm.weight shape torch.Size([64])
decoder.stages.3.convs.1.norm.bias shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.stages.3.convs.1.all_modules.0.bias shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.1.weight shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.1.bias shape torch.Size([64])
decoder.stages.4.convs.0.conv.weight shape torch.Size([32, 64, 3, 3, 3])
decoder.stages.4.convs.0.conv.bias shape torch.Size([32])
decoder.stages.4.convs.0.norm.weight shape torch.Size([32])
decoder.stages.4.convs.0.norm.bias shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.0.weight shape torch.Size([32, 64, 3, 3, 3])
decoder.stages.4.convs.0.all_modules.0.bias shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.1.weight shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.1.bias shape torch.Size([32])
decoder.stages.4.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.stages.4.convs.1.conv.bias shape torch.Size([32])
decoder.stages.4.convs.1.norm.weight shape torch.Size([32])
decoder.stages.4.convs.1.norm.bias shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.stages.4.convs.1.all_modules.0.bias shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.1.weight shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.1.bias shape torch.Size([32])
decoder.transpconvs.0.weight shape torch.Size([320, 320, 1, 2, 2])
decoder.transpconvs.0.bias shape torch.Size([320])
decoder.transpconvs.1.weight shape torch.Size([320, 256, 2, 2, 2])
decoder.transpconvs.1.bias shape torch.Size([256])
decoder.transpconvs.2.weight shape torch.Size([256, 128, 2, 2, 2])
decoder.transpconvs.2.bias shape torch.Size([128])
decoder.transpconvs.3.weight shape torch.Size([128, 64, 2, 2, 2])
decoder.transpconvs.3.bias shape torch.Size([64])
decoder.transpconvs.4.weight shape torch.Size([64, 32, 2, 2, 2])
decoder.transpconvs.4.bias shape torch.Size([32])
################### Done ###################
2024-12-14 17:38:52.992751: do_dummy_2d_data_aug: True
2024-12-14 17:38:52.994766: Using splits from existing split file: /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset307_Sohee_Calcium_OCT_CrossValidation/splits_final_60.json
2024-12-14 17:38:52.996801: The split file contains 5 splits.
2024-12-14 17:38:52.997936: Desired fold for training: 3
2024-12-14 17:38:52.999347: This split has 4 training and 5 validation cases.
using pin_memory on device 0
using pin_memory on device 0
using pin_memory on device 0

This is the configuration used by this training:
Configuration name: 3d_32x160x128_b10
 {'data_identifier': 'nnUNetPlans_3d_fullres', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 10, 'patch_size': [32, 160, 128], 'median_image_size_in_voxels': [375.0, 498.0, 498.0], 'spacing': [1.0, 1.0, 1.0], 'normalization_schemes': ['ZScoreNormalization'], 'use_mask_for_norm': [False], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'architecture': {'network_class_name': 'dynamic_network_architectures.architectures.unet.PlainConvUNet', 'arch_kwargs': {'n_stages': 6, 'features_per_stage': [32, 64, 128, 256, 320, 320], 'conv_op': 'torch.nn.modules.conv.Conv3d', 'kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]], 'strides': [[1, 1, 1], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [1, 2, 2]], 'n_conv_per_stage': [2, 2, 2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2, 2, 2], 'conv_bias': True, 'norm_op': 'torch.nn.modules.instancenorm.InstanceNorm3d', 'norm_op_kwargs': {'eps': 1e-05, 'affine': True}, 'dropout_op': None, 'dropout_op_kwargs': None, 'nonlin': 'torch.nn.LeakyReLU', 'nonlin_kwargs': {'inplace': True}, 'deep_supervision': True}, '_kw_requires_import': ['conv_op', 'norm_op', 'dropout_op', 'nonlin']}, 'batch_dice': True, 'inherits_from': '3d_fullres'} 

These are the global plan.json settings:
 {'dataset_name': 'Dataset307_Sohee_Calcium_OCT_CrossValidation', 'plans_name': 'nnUNetPlans', 'original_median_spacing_after_transp': [1.0, 1.0, 1.0], 'original_median_shape_after_transp': [375, 498, 498], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 0.4977976381778717, 'mean': 0.13507525622844696, 'median': 0.09599608182907104, 'min': 0.0, 'percentile_00_5': 0.014754901640117168, 'percentile_99_5': 0.4977976381778717, 'std': 0.12025152146816254}}} 

2024-12-14 17:39:24.662311: unpacking dataset...
using pin_memory on device 0

This is the configuration used by this training:
Configuration name: 3d_32x160x128_b10
 {'data_identifier': 'nnUNetPlans_3d_fullres', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 10, 'patch_size': [32, 160, 128], 'median_image_size_in_voxels': [375.0, 498.0, 498.0], 'spacing': [1.0, 1.0, 1.0], 'normalization_schemes': ['ZScoreNormalization'], 'use_mask_for_norm': [False], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'architecture': {'network_class_name': 'dynamic_network_architectures.architectures.unet.PlainConvUNet', 'arch_kwargs': {'n_stages': 6, 'features_per_stage': [32, 64, 128, 256, 320, 320], 'conv_op': 'torch.nn.modules.conv.Conv3d', 'kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]], 'strides': [[1, 1, 1], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [1, 2, 2]], 'n_conv_per_stage': [2, 2, 2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2, 2, 2], 'conv_bias': True, 'norm_op': 'torch.nn.modules.instancenorm.InstanceNorm3d', 'norm_op_kwargs': {'eps': 1e-05, 'affine': True}, 'dropout_op': None, 'dropout_op_kwargs': None, 'nonlin': 'torch.nn.LeakyReLU', 'nonlin_kwargs': {'inplace': True}, 'deep_supervision': True}, '_kw_requires_import': ['conv_op', 'norm_op', 'dropout_op', 'nonlin']}, 'batch_dice': True, 'inherits_from': '3d_fullres'} 

These are the global plan.json settings:
 {'dataset_name': 'Dataset307_Sohee_Calcium_OCT_CrossValidation', 'plans_name': 'nnUNetPlans', 'original_median_spacing_after_transp': [1.0, 1.0, 1.0], 'original_median_shape_after_transp': [375, 498, 498], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 0.4977976381778717, 'mean': 0.13507525622844696, 'median': 0.09599608182907104, 'min': 0.0, 'percentile_00_5': 0.014754901640117168, 'percentile_99_5': 0.4977976381778717, 'std': 0.12025152146816254}}} 

2024-12-14 17:39:25.623329: unpacking dataset...
2024-12-14 17:39:29.161148: unpacking done...
2024-12-14 17:39:29.261881: Unable to plot network architecture: nnUNet_compile is enabled!
2024-12-14 17:39:29.311102: 
2024-12-14 17:39:29.312345: Epoch 0
2024-12-14 17:39:29.313283: Current learning rate: 0.01
2024-12-14 17:47:34.896989: Validation loss improved from 1000.00000 to -0.24608! Patience: 0/50
2024-12-14 17:47:34.897996: train_loss -0.0815
2024-12-14 17:47:34.898859: val_loss -0.2461
2024-12-14 17:47:34.899621: Pseudo dice [0.5689]
2024-12-14 17:47:34.900456: Epoch time: 485.59 s
2024-12-14 17:47:34.901148: Yayy! New best EMA pseudo Dice: 0.5689
2024-12-14 17:47:36.477934: 
2024-12-14 17:47:36.479012: Epoch 1
2024-12-14 17:47:36.479789: Current learning rate: 0.00994
2024-12-14 17:54:54.973220: Validation loss did not improve from -0.24608. Patience: 1/50
2024-12-14 17:54:54.974303: train_loss -0.2127
2024-12-14 17:54:54.975106: val_loss -0.2334
2024-12-14 17:54:54.975921: Pseudo dice [0.5759]
2024-12-14 17:54:54.976705: Epoch time: 438.5 s
2024-12-14 17:54:54.977391: Yayy! New best EMA pseudo Dice: 0.5696
2024-12-14 17:54:56.741477: 
2024-12-14 17:54:56.742728: Epoch 2
2024-12-14 17:54:56.743414: Current learning rate: 0.00988
2024-12-14 18:02:26.613363: Validation loss improved from -0.24608 to -0.30370! Patience: 1/50
2024-12-14 18:02:26.614341: train_loss -0.27
2024-12-14 18:02:26.615126: val_loss -0.3037
2024-12-14 18:02:26.615840: Pseudo dice [0.5912]
2024-12-14 18:02:26.616489: Epoch time: 449.87 s
2024-12-14 18:02:26.617239: Yayy! New best EMA pseudo Dice: 0.5718
2024-12-14 18:02:28.400810: 
2024-12-14 18:02:28.402184: Epoch 3
2024-12-14 18:02:28.402991: Current learning rate: 0.00982
2024-12-14 18:09:54.751393: Validation loss improved from -0.30370 to -0.32072! Patience: 0/50
2024-12-14 18:09:54.752437: train_loss -0.3285
2024-12-14 18:09:54.753598: val_loss -0.3207
2024-12-14 18:09:54.754519: Pseudo dice [0.6381]
2024-12-14 18:09:54.755529: Epoch time: 446.35 s
2024-12-14 18:09:54.756543: Yayy! New best EMA pseudo Dice: 0.5784
2024-12-14 18:09:56.575450: 
2024-12-14 18:09:56.576982: Epoch 4
2024-12-14 18:09:56.578011: Current learning rate: 0.00976
2024-12-14 18:17:28.599849: Validation loss improved from -0.32072 to -0.35248! Patience: 0/50
2024-12-14 18:17:28.600953: train_loss -0.3509
2024-12-14 18:17:28.601768: val_loss -0.3525
2024-12-14 18:17:28.602392: Pseudo dice [0.6098]
2024-12-14 18:17:28.603064: Epoch time: 452.03 s
2024-12-14 18:17:28.967177: Yayy! New best EMA pseudo Dice: 0.5815
2024-12-14 18:17:30.755383: 
2024-12-14 18:17:30.756891: Epoch 5
2024-12-14 18:17:30.757718: Current learning rate: 0.0097
2024-12-14 18:25:22.238999: Validation loss improved from -0.35248 to -0.38890! Patience: 0/50
2024-12-14 18:25:22.239994: train_loss -0.395
2024-12-14 18:25:22.240864: val_loss -0.3889
2024-12-14 18:25:22.241547: Pseudo dice [0.6506]
2024-12-14 18:25:22.242156: Epoch time: 471.49 s
2024-12-14 18:25:22.242815: Yayy! New best EMA pseudo Dice: 0.5884
2024-12-14 18:25:23.955516: 
2024-12-14 18:25:23.956867: Epoch 6
2024-12-14 18:25:23.957558: Current learning rate: 0.00964
2024-12-14 18:33:08.951706: Validation loss improved from -0.38890 to -0.43359! Patience: 0/50
2024-12-14 18:33:08.954442: train_loss -0.4073
2024-12-14 18:33:08.955329: val_loss -0.4336
2024-12-14 18:33:08.956064: Pseudo dice [0.6816]
2024-12-14 18:33:08.956743: Epoch time: 465.0 s
2024-12-14 18:33:08.957484: Yayy! New best EMA pseudo Dice: 0.5978
2024-12-14 18:33:10.694729: 
2024-12-14 18:33:10.696176: Epoch 7
2024-12-14 18:33:10.696940: Current learning rate: 0.00958
2024-12-14 18:40:56.156329: Validation loss improved from -0.43359 to -0.44030! Patience: 0/50
2024-12-14 18:40:56.157437: train_loss -0.431
2024-12-14 18:40:56.158325: val_loss -0.4403
2024-12-14 18:40:56.159205: Pseudo dice [0.69]
2024-12-14 18:40:56.160108: Epoch time: 465.46 s
2024-12-14 18:40:56.160892: Yayy! New best EMA pseudo Dice: 0.607
2024-12-14 18:40:58.051742: 
2024-12-14 18:40:58.053232: Epoch 8
2024-12-14 18:40:58.054173: Current learning rate: 0.00952
2024-12-14 18:47:28.770727: Validation loss improved from -0.44030 to -0.45588! Patience: 0/50
2024-12-14 18:47:28.772077: train_loss -0.465
2024-12-14 18:47:28.773862: val_loss -0.4559
2024-12-14 18:47:28.774788: Pseudo dice [0.6967]
2024-12-14 18:47:28.775670: Epoch time: 390.72 s
2024-12-14 18:47:28.776483: Yayy! New best EMA pseudo Dice: 0.616
2024-12-14 18:47:31.066549: 
2024-12-14 18:47:31.067942: Epoch 9
2024-12-14 18:47:31.068777: Current learning rate: 0.00946
2024-12-14 18:53:57.940194: Validation loss did not improve from -0.45588. Patience: 1/50
2024-12-14 18:53:57.941187: train_loss -0.4774
2024-12-14 18:53:57.942084: val_loss -0.4357
2024-12-14 18:53:57.942736: Pseudo dice [0.6772]
2024-12-14 18:53:57.943386: Epoch time: 386.88 s
2024-12-14 18:53:58.292548: Yayy! New best EMA pseudo Dice: 0.6221
2024-12-14 18:53:59.986524: 
2024-12-14 18:53:59.987535: Epoch 10
2024-12-14 18:53:59.988347: Current learning rate: 0.0094
2024-12-14 19:00:37.506701: Validation loss improved from -0.45588 to -0.46768! Patience: 1/50
2024-12-14 19:00:37.507770: train_loss -0.4797
2024-12-14 19:00:37.508525: val_loss -0.4677
2024-12-14 19:00:37.509143: Pseudo dice [0.6922]
2024-12-14 19:00:37.509766: Epoch time: 397.52 s
2024-12-14 19:00:37.510351: Yayy! New best EMA pseudo Dice: 0.6291
2024-12-14 19:00:39.232103: 
2024-12-14 19:00:39.233312: Epoch 11
2024-12-14 19:00:39.234016: Current learning rate: 0.00934
2024-12-14 19:07:57.255342: Validation loss did not improve from -0.46768. Patience: 1/50
2024-12-14 19:07:57.256323: train_loss -0.4864
2024-12-14 19:07:57.257051: val_loss -0.4137
2024-12-14 19:07:57.257663: Pseudo dice [0.6559]
2024-12-14 19:07:57.258279: Epoch time: 438.03 s
2024-12-14 19:07:57.258930: Yayy! New best EMA pseudo Dice: 0.6318
2024-12-14 19:07:59.005933: 
2024-12-14 19:07:59.007254: Epoch 12
2024-12-14 19:07:59.008009: Current learning rate: 0.00928
2024-12-14 19:15:37.323128: Validation loss improved from -0.46768 to -0.48304! Patience: 1/50
2024-12-14 19:15:37.324180: train_loss -0.4949
2024-12-14 19:15:37.324869: val_loss -0.483
2024-12-14 19:15:37.325522: Pseudo dice [0.702]
2024-12-14 19:15:37.326267: Epoch time: 458.32 s
2024-12-14 19:15:37.326903: Yayy! New best EMA pseudo Dice: 0.6388
2024-12-14 19:15:39.105672: 
2024-12-14 19:15:39.106947: Epoch 13
2024-12-14 19:15:39.107671: Current learning rate: 0.00922
2024-12-14 19:23:20.371202: Validation loss did not improve from -0.48304. Patience: 1/50
2024-12-14 19:23:20.372289: train_loss -0.5176
2024-12-14 19:23:20.373112: val_loss -0.4607
2024-12-14 19:23:20.373825: Pseudo dice [0.6929]
2024-12-14 19:23:20.374541: Epoch time: 461.27 s
2024-12-14 19:23:20.375173: Yayy! New best EMA pseudo Dice: 0.6442
2024-12-14 19:23:22.090638: 
2024-12-14 19:23:22.091976: Epoch 14
2024-12-14 19:23:22.092743: Current learning rate: 0.00916
2024-12-14 19:31:32.516043: Validation loss did not improve from -0.48304. Patience: 2/50
2024-12-14 19:31:32.517106: train_loss -0.5252
2024-12-14 19:31:32.518021: val_loss -0.4242
2024-12-14 19:31:32.518862: Pseudo dice [0.6836]
2024-12-14 19:31:32.519789: Epoch time: 490.43 s
2024-12-14 19:31:32.940288: Yayy! New best EMA pseudo Dice: 0.6481
2024-12-14 19:31:34.708278: 
2024-12-14 19:31:34.709673: Epoch 15
2024-12-14 19:31:34.710484: Current learning rate: 0.0091
2024-12-14 19:39:32.090498: Validation loss did not improve from -0.48304. Patience: 3/50
2024-12-14 19:39:32.104732: train_loss -0.5214
2024-12-14 19:39:32.105848: val_loss -0.4593
2024-12-14 19:39:32.106832: Pseudo dice [0.6925]
2024-12-14 19:39:32.107832: Epoch time: 477.4 s
2024-12-14 19:39:32.108882: Yayy! New best EMA pseudo Dice: 0.6526
2024-12-14 19:39:33.930760: 
2024-12-14 19:39:33.932104: Epoch 16
2024-12-14 19:39:33.933061: Current learning rate: 0.00903
2024-12-14 19:47:23.433275: Validation loss did not improve from -0.48304. Patience: 4/50
2024-12-14 19:47:23.434320: train_loss -0.5238
2024-12-14 19:47:23.435116: val_loss -0.4583
2024-12-14 19:47:23.435850: Pseudo dice [0.6868]
2024-12-14 19:47:23.436507: Epoch time: 469.5 s
2024-12-14 19:47:23.437242: Yayy! New best EMA pseudo Dice: 0.656
2024-12-14 19:47:25.237955: 
2024-12-14 19:47:25.239101: Epoch 17
2024-12-14 19:47:25.239889: Current learning rate: 0.00897
2024-12-14 19:55:29.798415: Validation loss did not improve from -0.48304. Patience: 5/50
2024-12-14 19:55:29.799922: train_loss -0.5503
2024-12-14 19:55:29.801117: val_loss -0.4139
2024-12-14 19:55:29.801897: Pseudo dice [0.638]
2024-12-14 19:55:29.802742: Epoch time: 484.56 s
2024-12-14 19:55:31.210458: 
2024-12-14 19:55:31.211716: Epoch 18
2024-12-14 19:55:31.212601: Current learning rate: 0.00891
2024-12-14 20:03:06.164121: Validation loss improved from -0.48304 to -0.50008! Patience: 5/50
2024-12-14 20:03:06.165170: train_loss -0.5501
2024-12-14 20:03:06.166085: val_loss -0.5001
2024-12-14 20:03:06.166915: Pseudo dice [0.7166]
2024-12-14 20:03:06.167603: Epoch time: 454.96 s
2024-12-14 20:03:06.168202: Yayy! New best EMA pseudo Dice: 0.6604
2024-12-14 20:03:07.902407: 
2024-12-14 20:03:07.903580: Epoch 19
2024-12-14 20:03:07.904286: Current learning rate: 0.00885
2024-12-14 20:10:55.480076: Validation loss did not improve from -0.50008. Patience: 1/50
2024-12-14 20:10:55.481058: train_loss -0.5518
2024-12-14 20:10:55.481747: val_loss -0.4761
2024-12-14 20:10:55.482357: Pseudo dice [0.7077]
2024-12-14 20:10:55.483130: Epoch time: 467.58 s
2024-12-14 20:10:55.865263: Yayy! New best EMA pseudo Dice: 0.6652
2024-12-14 20:10:57.987311: 
2024-12-14 20:10:57.988731: Epoch 20
2024-12-14 20:10:57.989616: Current learning rate: 0.00879
2024-12-14 20:18:36.651678: Validation loss did not improve from -0.50008. Patience: 2/50
2024-12-14 20:18:36.652684: train_loss -0.5703
2024-12-14 20:18:36.653454: val_loss -0.4736
2024-12-14 20:18:36.654100: Pseudo dice [0.7052]
2024-12-14 20:18:36.654761: Epoch time: 458.67 s
2024-12-14 20:18:36.655444: Yayy! New best EMA pseudo Dice: 0.6692
2024-12-14 20:18:38.476580: 
2024-12-14 20:18:38.477918: Epoch 21
2024-12-14 20:18:38.478575: Current learning rate: 0.00873
2024-12-14 20:26:28.203466: Validation loss did not improve from -0.50008. Patience: 3/50
2024-12-14 20:26:28.204447: train_loss -0.5692
2024-12-14 20:26:28.205126: val_loss -0.4844
2024-12-14 20:26:28.205840: Pseudo dice [0.6935]
2024-12-14 20:26:28.206529: Epoch time: 469.73 s
2024-12-14 20:26:28.207231: Yayy! New best EMA pseudo Dice: 0.6716
2024-12-14 20:26:29.897678: 
2024-12-14 20:26:29.898905: Epoch 22
2024-12-14 20:26:29.899738: Current learning rate: 0.00867
2024-12-14 20:34:26.317676: Validation loss did not improve from -0.50008. Patience: 4/50
2024-12-14 20:34:26.318770: train_loss -0.5695
2024-12-14 20:34:26.319956: val_loss -0.4855
2024-12-14 20:34:26.320950: Pseudo dice [0.723]
2024-12-14 20:34:26.321973: Epoch time: 476.42 s
2024-12-14 20:34:26.323028: Yayy! New best EMA pseudo Dice: 0.6767
2024-12-14 20:34:28.056748: 
2024-12-14 20:34:28.058214: Epoch 23
2024-12-14 20:34:28.059339: Current learning rate: 0.00861
2024-12-14 20:42:33.967704: Validation loss did not improve from -0.50008. Patience: 5/50
2024-12-14 20:42:33.971800: train_loss -0.5676
2024-12-14 20:42:33.973045: val_loss -0.4745
2024-12-14 20:42:33.974317: Pseudo dice [0.7062]
2024-12-14 20:42:33.975586: Epoch time: 485.92 s
2024-12-14 20:42:33.976868: Yayy! New best EMA pseudo Dice: 0.6797
2024-12-14 20:42:35.646010: 
2024-12-14 20:42:35.647546: Epoch 24
2024-12-14 20:42:35.648582: Current learning rate: 0.00855
2024-12-14 20:50:29.221090: Validation loss did not improve from -0.50008. Patience: 6/50
2024-12-14 20:50:29.222118: train_loss -0.5825
2024-12-14 20:50:29.223307: val_loss -0.4657
2024-12-14 20:50:29.224213: Pseudo dice [0.6893]
2024-12-14 20:50:29.225230: Epoch time: 473.58 s
2024-12-14 20:50:29.623054: Yayy! New best EMA pseudo Dice: 0.6806
2024-12-14 20:50:31.358959: 
2024-12-14 20:50:31.360496: Epoch 25
2024-12-14 20:50:31.361729: Current learning rate: 0.00849
2024-12-14 20:58:05.801440: Validation loss did not improve from -0.50008. Patience: 7/50
2024-12-14 20:58:05.802611: train_loss -0.5921
2024-12-14 20:58:05.803427: val_loss -0.4849
2024-12-14 20:58:05.804107: Pseudo dice [0.7112]
2024-12-14 20:58:05.804791: Epoch time: 454.44 s
2024-12-14 20:58:05.805459: Yayy! New best EMA pseudo Dice: 0.6837
2024-12-14 20:58:07.577028: 
2024-12-14 20:58:07.578218: Epoch 26
2024-12-14 20:58:07.578937: Current learning rate: 0.00843
2024-12-14 21:05:46.004400: Validation loss did not improve from -0.50008. Patience: 8/50
2024-12-14 21:05:46.005425: train_loss -0.5949
2024-12-14 21:05:46.006496: val_loss -0.4364
2024-12-14 21:05:46.007183: Pseudo dice [0.6821]
2024-12-14 21:05:46.008016: Epoch time: 458.43 s
2024-12-14 21:05:47.364849: 
2024-12-14 21:05:47.366008: Epoch 27
2024-12-14 21:05:47.366736: Current learning rate: 0.00836
2024-12-14 21:13:40.866131: Validation loss improved from -0.50008 to -0.50271! Patience: 8/50
2024-12-14 21:13:40.867182: train_loss -0.588
2024-12-14 21:13:40.868003: val_loss -0.5027
2024-12-14 21:13:40.868779: Pseudo dice [0.7163]
2024-12-14 21:13:40.869583: Epoch time: 473.5 s
2024-12-14 21:13:40.870347: Yayy! New best EMA pseudo Dice: 0.6868
2024-12-14 21:13:42.613206: 
2024-12-14 21:13:42.614354: Epoch 28
2024-12-14 21:13:42.615088: Current learning rate: 0.0083
2024-12-14 21:21:39.310786: Validation loss did not improve from -0.50271. Patience: 1/50
2024-12-14 21:21:39.311862: train_loss -0.5886
2024-12-14 21:21:39.312774: val_loss -0.4824
2024-12-14 21:21:39.313421: Pseudo dice [0.7079]
2024-12-14 21:21:39.314083: Epoch time: 476.7 s
2024-12-14 21:21:39.314730: Yayy! New best EMA pseudo Dice: 0.6889
2024-12-14 21:21:41.032955: 
2024-12-14 21:21:41.034408: Epoch 29
2024-12-14 21:21:41.035238: Current learning rate: 0.00824
2024-12-14 21:29:33.639515: Validation loss did not improve from -0.50271. Patience: 2/50
2024-12-14 21:29:33.640352: train_loss -0.604
2024-12-14 21:29:33.641018: val_loss -0.4905
2024-12-14 21:29:33.641707: Pseudo dice [0.7167]
2024-12-14 21:29:33.642476: Epoch time: 472.61 s
2024-12-14 21:29:34.032295: Yayy! New best EMA pseudo Dice: 0.6917
2024-12-14 21:29:36.161913: 
2024-12-14 21:29:36.162863: Epoch 30
2024-12-14 21:29:36.163557: Current learning rate: 0.00818
2024-12-14 21:37:22.294171: Validation loss did not improve from -0.50271. Patience: 3/50
2024-12-14 21:37:22.295144: train_loss -0.6133
2024-12-14 21:37:22.296015: val_loss -0.4921
2024-12-14 21:37:22.296785: Pseudo dice [0.7216]
2024-12-14 21:37:22.297535: Epoch time: 466.13 s
2024-12-14 21:37:22.298314: Yayy! New best EMA pseudo Dice: 0.6947
2024-12-14 21:37:24.049200: 
2024-12-14 21:37:24.050416: Epoch 31
2024-12-14 21:37:24.051166: Current learning rate: 0.00812
2024-12-14 21:45:27.953787: Validation loss did not improve from -0.50271. Patience: 4/50
2024-12-14 21:45:27.954763: train_loss -0.621
2024-12-14 21:45:27.955584: val_loss -0.4994
2024-12-14 21:45:27.956402: Pseudo dice [0.725]
2024-12-14 21:45:27.957168: Epoch time: 483.91 s
2024-12-14 21:45:27.958008: Yayy! New best EMA pseudo Dice: 0.6977
2024-12-14 21:45:29.744683: 
2024-12-14 21:45:29.745594: Epoch 32
2024-12-14 21:45:29.746314: Current learning rate: 0.00806
2024-12-14 21:53:31.048935: Validation loss improved from -0.50271 to -0.51163! Patience: 4/50
2024-12-14 21:53:31.051613: train_loss -0.6155
2024-12-14 21:53:31.052782: val_loss -0.5116
2024-12-14 21:53:31.053729: Pseudo dice [0.7285]
2024-12-14 21:53:31.054929: Epoch time: 481.31 s
2024-12-14 21:53:31.056188: Yayy! New best EMA pseudo Dice: 0.7008
2024-12-14 21:53:32.785790: 
2024-12-14 21:53:32.787287: Epoch 33
2024-12-14 21:53:32.788347: Current learning rate: 0.008
2024-12-14 22:01:35.184587: Validation loss did not improve from -0.51163. Patience: 1/50
2024-12-14 22:01:35.186154: train_loss -0.6143
2024-12-14 22:01:35.187885: val_loss -0.4926
2024-12-14 22:01:35.188802: Pseudo dice [0.716]
2024-12-14 22:01:35.189759: Epoch time: 482.4 s
2024-12-14 22:01:35.190512: Yayy! New best EMA pseudo Dice: 0.7023
2024-12-14 22:01:36.965043: 
2024-12-14 22:01:36.966377: Epoch 34
2024-12-14 22:01:36.967304: Current learning rate: 0.00793
2024-12-14 22:09:25.088217: Validation loss improved from -0.51163 to -0.51271! Patience: 1/50
2024-12-14 22:09:25.089339: train_loss -0.6254
2024-12-14 22:09:25.090347: val_loss -0.5127
2024-12-14 22:09:25.091336: Pseudo dice [0.7215]
2024-12-14 22:09:25.092271: Epoch time: 468.13 s
2024-12-14 22:09:25.534629: Yayy! New best EMA pseudo Dice: 0.7042
2024-12-14 22:09:27.280071: 
2024-12-14 22:09:27.281630: Epoch 35
2024-12-14 22:09:27.282658: Current learning rate: 0.00787
2024-12-14 22:17:13.879050: Validation loss did not improve from -0.51271. Patience: 1/50
2024-12-14 22:17:13.880039: train_loss -0.6238
2024-12-14 22:17:13.880929: val_loss -0.5021
2024-12-14 22:17:13.881792: Pseudo dice [0.7215]
2024-12-14 22:17:13.882809: Epoch time: 466.6 s
2024-12-14 22:17:13.883744: Yayy! New best EMA pseudo Dice: 0.706
2024-12-14 22:17:15.641676: 
2024-12-14 22:17:15.643226: Epoch 36
2024-12-14 22:17:15.644333: Current learning rate: 0.00781
2024-12-14 22:25:01.042789: Validation loss improved from -0.51271 to -0.51997! Patience: 1/50
2024-12-14 22:25:01.043973: train_loss -0.6163
2024-12-14 22:25:01.045132: val_loss -0.52
2024-12-14 22:25:01.046124: Pseudo dice [0.7388]
2024-12-14 22:25:01.047055: Epoch time: 465.4 s
2024-12-14 22:25:01.048034: Yayy! New best EMA pseudo Dice: 0.7093
2024-12-14 22:25:02.785385: 
2024-12-14 22:25:02.786953: Epoch 37
2024-12-14 22:25:02.787834: Current learning rate: 0.00775
2024-12-14 22:32:44.664312: Validation loss did not improve from -0.51997. Patience: 1/50
2024-12-14 22:32:44.665362: train_loss -0.6366
2024-12-14 22:32:44.666170: val_loss -0.5003
2024-12-14 22:32:44.666774: Pseudo dice [0.7185]
2024-12-14 22:32:44.667546: Epoch time: 461.88 s
2024-12-14 22:32:44.668288: Yayy! New best EMA pseudo Dice: 0.7102
2024-12-14 22:32:46.429694: 
2024-12-14 22:32:46.431184: Epoch 38
2024-12-14 22:32:46.431983: Current learning rate: 0.00769
2024-12-14 22:40:46.913667: Validation loss did not improve from -0.51997. Patience: 2/50
2024-12-14 22:40:46.914681: train_loss -0.6446
2024-12-14 22:40:46.915682: val_loss -0.5135
2024-12-14 22:40:46.916560: Pseudo dice [0.7301]
2024-12-14 22:40:46.917492: Epoch time: 480.49 s
2024-12-14 22:40:46.918440: Yayy! New best EMA pseudo Dice: 0.7122
2024-12-14 22:40:48.671453: 
2024-12-14 22:40:48.672886: Epoch 39
2024-12-14 22:40:48.673830: Current learning rate: 0.00763
2024-12-14 22:48:50.863623: Validation loss did not improve from -0.51997. Patience: 3/50
2024-12-14 22:48:50.864438: train_loss -0.6321
2024-12-14 22:48:50.865221: val_loss -0.5145
2024-12-14 22:48:50.865946: Pseudo dice [0.7262]
2024-12-14 22:48:50.866581: Epoch time: 482.19 s
2024-12-14 22:48:51.267991: Yayy! New best EMA pseudo Dice: 0.7136
2024-12-14 22:48:53.055045: 
2024-12-14 22:48:53.056458: Epoch 40
2024-12-14 22:48:53.057344: Current learning rate: 0.00756
2024-12-14 22:56:56.667900: Validation loss did not improve from -0.51997. Patience: 4/50
2024-12-14 22:56:56.670330: train_loss -0.6398
2024-12-14 22:56:56.671288: val_loss -0.4956
2024-12-14 22:56:56.671952: Pseudo dice [0.7201]
2024-12-14 22:56:56.672930: Epoch time: 483.62 s
2024-12-14 22:56:56.674333: Yayy! New best EMA pseudo Dice: 0.7142
2024-12-14 22:56:58.820575: 
2024-12-14 22:56:58.821759: Epoch 41
2024-12-14 22:56:58.822520: Current learning rate: 0.0075
2024-12-14 23:04:57.848698: Validation loss improved from -0.51997 to -0.52100! Patience: 4/50
2024-12-14 23:04:57.850184: train_loss -0.6409
2024-12-14 23:04:57.852098: val_loss -0.521
2024-12-14 23:04:57.853022: Pseudo dice [0.7387]
2024-12-14 23:04:57.853960: Epoch time: 479.03 s
2024-12-14 23:04:57.854749: Yayy! New best EMA pseudo Dice: 0.7167
2024-12-14 23:04:59.613075: 
2024-12-14 23:04:59.614220: Epoch 42
2024-12-14 23:04:59.614957: Current learning rate: 0.00744
2024-12-14 23:12:48.483366: Validation loss improved from -0.52100 to -0.52275! Patience: 0/50
2024-12-14 23:12:48.484700: train_loss -0.6509
2024-12-14 23:12:48.485632: val_loss -0.5228
2024-12-14 23:12:48.486519: Pseudo dice [0.7357]
2024-12-14 23:12:48.487399: Epoch time: 468.87 s
2024-12-14 23:12:48.488125: Yayy! New best EMA pseudo Dice: 0.7186
2024-12-14 23:12:50.285409: 
2024-12-14 23:12:50.286858: Epoch 43
2024-12-14 23:12:50.287636: Current learning rate: 0.00738
2024-12-14 23:20:32.288862: Validation loss improved from -0.52275 to -0.52967! Patience: 0/50
2024-12-14 23:20:32.289868: train_loss -0.65
2024-12-14 23:20:32.290757: val_loss -0.5297
2024-12-14 23:20:32.291568: Pseudo dice [0.734]
2024-12-14 23:20:32.292366: Epoch time: 462.01 s
2024-12-14 23:20:32.293112: Yayy! New best EMA pseudo Dice: 0.7201
2024-12-14 23:20:33.990416: 
2024-12-14 23:20:33.991810: Epoch 44
2024-12-14 23:20:33.992678: Current learning rate: 0.00732
2024-12-14 23:28:40.309535: Validation loss did not improve from -0.52967. Patience: 1/50
2024-12-14 23:28:40.310599: train_loss -0.6602
2024-12-14 23:28:40.311478: val_loss -0.4857
2024-12-14 23:28:40.312164: Pseudo dice [0.7049]
2024-12-14 23:28:40.312824: Epoch time: 486.32 s
2024-12-14 23:28:42.049460: 
2024-12-14 23:28:42.050894: Epoch 45
2024-12-14 23:28:42.051660: Current learning rate: 0.00725
2024-12-14 23:36:48.035112: Validation loss did not improve from -0.52967. Patience: 2/50
2024-12-14 23:36:48.036092: train_loss -0.6576
2024-12-14 23:36:48.037026: val_loss -0.513
2024-12-14 23:36:48.037820: Pseudo dice [0.7294]
2024-12-14 23:36:48.038540: Epoch time: 485.99 s
2024-12-14 23:36:49.355212: 
2024-12-14 23:36:49.356620: Epoch 46
2024-12-14 23:36:49.357437: Current learning rate: 0.00719
2024-12-14 23:45:29.006468: Validation loss did not improve from -0.52967. Patience: 3/50
2024-12-14 23:45:29.007541: train_loss -0.6637
2024-12-14 23:45:29.008492: val_loss -0.5273
2024-12-14 23:45:29.009232: Pseudo dice [0.7333]
2024-12-14 23:45:29.010053: Epoch time: 519.65 s
2024-12-14 23:45:29.010751: Yayy! New best EMA pseudo Dice: 0.721
2024-12-14 23:45:30.792801: 
2024-12-14 23:45:30.794332: Epoch 47
2024-12-14 23:45:30.795334: Current learning rate: 0.00713
2024-12-14 23:53:38.966771: Validation loss did not improve from -0.52967. Patience: 4/50
2024-12-14 23:53:38.967790: train_loss -0.6704
2024-12-14 23:53:38.968655: val_loss -0.5215
2024-12-14 23:53:38.969551: Pseudo dice [0.729]
2024-12-14 23:53:38.970337: Epoch time: 488.18 s
2024-12-14 23:53:38.971181: Yayy! New best EMA pseudo Dice: 0.7218
2024-12-14 23:53:40.747844: 
2024-12-14 23:53:40.749117: Epoch 48
2024-12-14 23:53:40.750036: Current learning rate: 0.00707
2024-12-15 00:01:47.704811: Validation loss did not improve from -0.52967. Patience: 5/50
2024-12-15 00:01:47.706509: train_loss -0.6724
2024-12-15 00:01:47.707706: val_loss -0.5195
2024-12-15 00:01:47.708513: Pseudo dice [0.7191]
2024-12-15 00:01:47.709288: Epoch time: 486.96 s
2024-12-15 00:01:49.048450: 
2024-12-15 00:01:49.049926: Epoch 49
2024-12-15 00:01:49.050739: Current learning rate: 0.007
2024-12-15 00:09:48.849345: Validation loss did not improve from -0.52967. Patience: 6/50
2024-12-15 00:09:48.851227: train_loss -0.6739
2024-12-15 00:09:48.852851: val_loss -0.5247
2024-12-15 00:09:48.853587: Pseudo dice [0.7369]
2024-12-15 00:09:48.854583: Epoch time: 479.8 s
2024-12-15 00:09:49.249927: Yayy! New best EMA pseudo Dice: 0.7231
2024-12-15 00:09:50.995754: 
2024-12-15 00:09:50.996969: Epoch 50
2024-12-15 00:09:50.997692: Current learning rate: 0.00694
2024-12-15 00:17:52.070402: Validation loss improved from -0.52967 to -0.54633! Patience: 6/50
2024-12-15 00:17:52.071387: train_loss -0.6694
2024-12-15 00:17:52.072336: val_loss -0.5463
2024-12-15 00:17:52.073185: Pseudo dice [0.7504]
2024-12-15 00:17:52.074205: Epoch time: 481.08 s
2024-12-15 00:17:52.075060: Yayy! New best EMA pseudo Dice: 0.7258
2024-12-15 00:17:53.822731: 
2024-12-15 00:17:53.824067: Epoch 51
2024-12-15 00:17:53.824898: Current learning rate: 0.00688
2024-12-15 00:25:46.491932: Validation loss improved from -0.54633 to -0.55997! Patience: 0/50
2024-12-15 00:25:46.492995: train_loss -0.6737
2024-12-15 00:25:46.493836: val_loss -0.56
2024-12-15 00:25:46.494575: Pseudo dice [0.7546]
2024-12-15 00:25:46.495210: Epoch time: 472.67 s
2024-12-15 00:25:46.495847: Yayy! New best EMA pseudo Dice: 0.7287
2024-12-15 00:25:48.622604: 
2024-12-15 00:25:48.624017: Epoch 52
2024-12-15 00:25:48.624977: Current learning rate: 0.00682
2024-12-15 00:34:09.172171: Validation loss did not improve from -0.55997. Patience: 1/50
2024-12-15 00:34:09.173136: train_loss -0.6817
2024-12-15 00:34:09.174007: val_loss -0.549
2024-12-15 00:34:09.174632: Pseudo dice [0.7396]
2024-12-15 00:34:09.175277: Epoch time: 500.55 s
2024-12-15 00:34:09.175982: Yayy! New best EMA pseudo Dice: 0.7298
2024-12-15 00:34:10.904292: 
2024-12-15 00:34:10.905739: Epoch 53
2024-12-15 00:34:10.906450: Current learning rate: 0.00675
2024-12-15 00:42:22.068786: Validation loss did not improve from -0.55997. Patience: 2/50
2024-12-15 00:42:22.069944: train_loss -0.6881
2024-12-15 00:42:22.070832: val_loss -0.5403
2024-12-15 00:42:22.071596: Pseudo dice [0.7407]
2024-12-15 00:42:22.072533: Epoch time: 491.17 s
2024-12-15 00:42:22.073289: Yayy! New best EMA pseudo Dice: 0.7309
2024-12-15 00:42:23.819743: 
2024-12-15 00:42:23.821367: Epoch 54
2024-12-15 00:42:23.822189: Current learning rate: 0.00669
2024-12-15 00:50:25.256985: Validation loss did not improve from -0.55997. Patience: 3/50
2024-12-15 00:50:25.257762: train_loss -0.6839
2024-12-15 00:50:25.258672: val_loss -0.5243
2024-12-15 00:50:25.259669: Pseudo dice [0.7424]
2024-12-15 00:50:25.260538: Epoch time: 481.44 s
2024-12-15 00:50:25.624735: Yayy! New best EMA pseudo Dice: 0.732
2024-12-15 00:50:27.359610: 
2024-12-15 00:50:27.361025: Epoch 55
2024-12-15 00:50:27.361845: Current learning rate: 0.00663
2024-12-15 00:58:28.053309: Validation loss did not improve from -0.55997. Patience: 4/50
2024-12-15 00:58:28.054284: train_loss -0.6895
2024-12-15 00:58:28.055331: val_loss -0.5203
2024-12-15 00:58:28.056317: Pseudo dice [0.7334]
2024-12-15 00:58:28.057256: Epoch time: 480.7 s
2024-12-15 00:58:28.058127: Yayy! New best EMA pseudo Dice: 0.7322
2024-12-15 00:58:29.813504: 
2024-12-15 00:58:29.814983: Epoch 56
2024-12-15 00:58:29.815917: Current learning rate: 0.00657
2024-12-15 01:06:25.064068: Validation loss did not improve from -0.55997. Patience: 5/50
2024-12-15 01:06:25.065620: train_loss -0.6886
2024-12-15 01:06:25.066745: val_loss -0.5512
2024-12-15 01:06:25.067373: Pseudo dice [0.749]
2024-12-15 01:06:25.068139: Epoch time: 475.25 s
2024-12-15 01:06:25.068866: Yayy! New best EMA pseudo Dice: 0.7339
2024-12-15 01:06:26.885731: 
2024-12-15 01:06:26.887015: Epoch 57
2024-12-15 01:06:26.887778: Current learning rate: 0.0065
2024-12-15 01:14:21.166420: Validation loss did not improve from -0.55997. Patience: 6/50
2024-12-15 01:14:21.167773: train_loss -0.6861
2024-12-15 01:14:21.168904: val_loss -0.5592
2024-12-15 01:14:21.169759: Pseudo dice [0.7598]
2024-12-15 01:14:21.170578: Epoch time: 474.28 s
2024-12-15 01:14:21.171267: Yayy! New best EMA pseudo Dice: 0.7364
2024-12-15 01:14:22.913576: 
2024-12-15 01:14:22.915017: Epoch 58
2024-12-15 01:14:22.915848: Current learning rate: 0.00644
2024-12-15 01:22:06.632324: Validation loss did not improve from -0.55997. Patience: 7/50
2024-12-15 01:22:06.633519: train_loss -0.6874
2024-12-15 01:22:06.634321: val_loss -0.5328
2024-12-15 01:22:06.634982: Pseudo dice [0.7411]
2024-12-15 01:22:06.635677: Epoch time: 463.72 s
2024-12-15 01:22:06.636285: Yayy! New best EMA pseudo Dice: 0.7369
2024-12-15 01:22:08.387516: 
2024-12-15 01:22:08.388614: Epoch 59
2024-12-15 01:22:08.389355: Current learning rate: 0.00638
2024-12-15 01:30:05.560462: Validation loss did not improve from -0.55997. Patience: 8/50
2024-12-15 01:30:05.562169: train_loss -0.6944
2024-12-15 01:30:05.563447: val_loss -0.5409
2024-12-15 01:30:05.564234: Pseudo dice [0.751]
2024-12-15 01:30:05.565108: Epoch time: 477.18 s
2024-12-15 01:30:05.954777: Yayy! New best EMA pseudo Dice: 0.7383
2024-12-15 01:30:07.752837: 
2024-12-15 01:30:07.754000: Epoch 60
2024-12-15 01:30:07.755019: Current learning rate: 0.00631
2024-12-15 01:38:13.984619: Validation loss did not improve from -0.55997. Patience: 9/50
2024-12-15 01:38:13.985389: train_loss -0.7065
2024-12-15 01:38:13.986455: val_loss -0.5426
2024-12-15 01:38:13.987382: Pseudo dice [0.7547]
2024-12-15 01:38:13.988280: Epoch time: 486.23 s
2024-12-15 01:38:13.989007: Yayy! New best EMA pseudo Dice: 0.74
2024-12-15 01:38:15.765252: 
2024-12-15 01:38:15.766701: Epoch 61
2024-12-15 01:38:15.767538: Current learning rate: 0.00625
2024-12-15 01:46:00.200984: Validation loss did not improve from -0.55997. Patience: 10/50
2024-12-15 01:46:00.201921: train_loss -0.6983
2024-12-15 01:46:00.202636: val_loss -0.5223
2024-12-15 01:46:00.203441: Pseudo dice [0.7297]
2024-12-15 01:46:00.204238: Epoch time: 464.44 s
2024-12-15 01:46:01.609970: 
2024-12-15 01:46:01.611358: Epoch 62
2024-12-15 01:46:01.612127: Current learning rate: 0.00619
2024-12-15 01:53:59.840100: Validation loss did not improve from -0.55997. Patience: 11/50
2024-12-15 01:53:59.841012: train_loss -0.6954
2024-12-15 01:53:59.841833: val_loss -0.543
2024-12-15 01:53:59.842527: Pseudo dice [0.7434]
2024-12-15 01:53:59.843246: Epoch time: 478.23 s
2024-12-15 01:54:01.672009: 
2024-12-15 01:54:01.673252: Epoch 63
2024-12-15 01:54:01.674239: Current learning rate: 0.00612
2024-12-15 02:02:08.804605: Validation loss did not improve from -0.55997. Patience: 12/50
2024-12-15 02:02:08.805536: train_loss -0.7006
2024-12-15 02:02:08.806303: val_loss -0.5303
2024-12-15 02:02:08.807018: Pseudo dice [0.7377]
2024-12-15 02:02:08.807710: Epoch time: 487.13 s
2024-12-15 02:02:10.213115: 
2024-12-15 02:02:10.214461: Epoch 64
2024-12-15 02:02:10.215158: Current learning rate: 0.00606
2024-12-15 02:10:17.763411: Validation loss did not improve from -0.55997. Patience: 13/50
2024-12-15 02:10:17.767843: train_loss -0.7052
2024-12-15 02:10:17.769016: val_loss -0.546
2024-12-15 02:10:17.769792: Pseudo dice [0.7419]
2024-12-15 02:10:17.770922: Epoch time: 487.56 s
2024-12-15 02:10:19.610064: 
2024-12-15 02:10:19.611339: Epoch 65
2024-12-15 02:10:19.612135: Current learning rate: 0.006
2024-12-15 02:18:09.618107: Validation loss did not improve from -0.55997. Patience: 14/50
2024-12-15 02:18:09.619109: train_loss -0.7121
2024-12-15 02:18:09.620009: val_loss -0.5386
2024-12-15 02:18:09.620818: Pseudo dice [0.7457]
2024-12-15 02:18:09.621775: Epoch time: 470.01 s
2024-12-15 02:18:09.622593: Yayy! New best EMA pseudo Dice: 0.7401
2024-12-15 02:18:11.411427: 
2024-12-15 02:18:11.413019: Epoch 66
2024-12-15 02:18:11.413959: Current learning rate: 0.00593
2024-12-15 02:26:02.770645: Validation loss did not improve from -0.55997. Patience: 15/50
2024-12-15 02:26:02.771766: train_loss -0.7081
2024-12-15 02:26:02.772789: val_loss -0.5475
2024-12-15 02:26:02.773569: Pseudo dice [0.7525]
2024-12-15 02:26:02.774395: Epoch time: 471.36 s
2024-12-15 02:26:02.775220: Yayy! New best EMA pseudo Dice: 0.7413
2024-12-15 02:26:04.606210: 
2024-12-15 02:26:04.607437: Epoch 67
2024-12-15 02:26:04.608237: Current learning rate: 0.00587
2024-12-15 02:34:00.820025: Validation loss did not improve from -0.55997. Patience: 16/50
2024-12-15 02:34:00.821609: train_loss -0.7137
2024-12-15 02:34:00.822680: val_loss -0.5252
2024-12-15 02:34:00.823541: Pseudo dice [0.7326]
2024-12-15 02:34:00.824375: Epoch time: 476.22 s
2024-12-15 02:34:02.231147: 
2024-12-15 02:34:02.232549: Epoch 68
2024-12-15 02:34:02.233327: Current learning rate: 0.00581
2024-12-15 02:41:55.134083: Validation loss did not improve from -0.55997. Patience: 17/50
2024-12-15 02:41:55.134990: train_loss -0.7188
2024-12-15 02:41:55.135805: val_loss -0.5446
2024-12-15 02:41:55.136501: Pseudo dice [0.7486]
2024-12-15 02:41:55.137169: Epoch time: 472.91 s
2024-12-15 02:41:56.519465: 
2024-12-15 02:41:56.520675: Epoch 69
2024-12-15 02:41:56.521390: Current learning rate: 0.00574
2024-12-15 02:49:42.129505: Validation loss did not improve from -0.55997. Patience: 18/50
2024-12-15 02:49:42.130340: train_loss -0.7174
2024-12-15 02:49:42.131151: val_loss -0.5341
2024-12-15 02:49:42.131935: Pseudo dice [0.7358]
2024-12-15 02:49:42.132780: Epoch time: 465.61 s
2024-12-15 02:49:43.903798: 
2024-12-15 02:49:43.905070: Epoch 70
2024-12-15 02:49:43.905880: Current learning rate: 0.00568
2024-12-15 02:57:33.171212: Validation loss did not improve from -0.55997. Patience: 19/50
2024-12-15 02:57:33.172025: train_loss -0.7119
2024-12-15 02:57:33.172750: val_loss -0.5264
2024-12-15 02:57:33.173407: Pseudo dice [0.7406]
2024-12-15 02:57:33.174215: Epoch time: 469.27 s
2024-12-15 02:57:34.552371: 
2024-12-15 02:57:34.553672: Epoch 71
2024-12-15 02:57:34.554497: Current learning rate: 0.00562
2024-12-15 03:05:46.070063: Validation loss did not improve from -0.55997. Patience: 20/50
2024-12-15 03:05:46.071052: train_loss -0.7073
2024-12-15 03:05:46.071825: val_loss -0.5376
2024-12-15 03:05:46.072512: Pseudo dice [0.7354]
2024-12-15 03:05:46.073220: Epoch time: 491.52 s
2024-12-15 03:05:47.464907: 
2024-12-15 03:05:47.466259: Epoch 72
2024-12-15 03:05:47.467044: Current learning rate: 0.00555
2024-12-15 03:13:55.709203: Validation loss did not improve from -0.55997. Patience: 21/50
2024-12-15 03:13:55.712609: train_loss -0.7126
2024-12-15 03:13:55.713600: val_loss -0.5446
2024-12-15 03:13:55.714340: Pseudo dice [0.7414]
2024-12-15 03:13:55.715375: Epoch time: 488.25 s
2024-12-15 03:13:57.465934: 
2024-12-15 03:13:57.467312: Epoch 73
2024-12-15 03:13:57.468104: Current learning rate: 0.00549
2024-12-15 03:22:00.438182: Validation loss did not improve from -0.55997. Patience: 22/50
2024-12-15 03:22:00.439250: train_loss -0.7137
2024-12-15 03:22:00.440304: val_loss -0.5586
2024-12-15 03:22:00.441290: Pseudo dice [0.7553]
2024-12-15 03:22:00.442198: Epoch time: 482.97 s
2024-12-15 03:22:00.443118: Yayy! New best EMA pseudo Dice: 0.7418
2024-12-15 03:22:02.168852: 
2024-12-15 03:22:02.170409: Epoch 74
2024-12-15 03:22:02.171441: Current learning rate: 0.00542
2024-12-15 03:30:03.836151: Validation loss did not improve from -0.55997. Patience: 23/50
2024-12-15 03:30:03.837249: train_loss -0.7193
2024-12-15 03:30:03.838032: val_loss -0.5593
2024-12-15 03:30:03.838860: Pseudo dice [0.7538]
2024-12-15 03:30:03.839716: Epoch time: 481.67 s
2024-12-15 03:30:04.210627: Yayy! New best EMA pseudo Dice: 0.743
2024-12-15 03:30:05.966727: 
2024-12-15 03:30:05.967774: Epoch 75
2024-12-15 03:30:05.968527: Current learning rate: 0.00536
2024-12-15 03:38:04.017548: Validation loss did not improve from -0.55997. Patience: 24/50
2024-12-15 03:38:04.019463: train_loss -0.719
2024-12-15 03:38:04.020729: val_loss -0.5431
2024-12-15 03:38:04.021441: Pseudo dice [0.7527]
2024-12-15 03:38:04.022215: Epoch time: 478.05 s
2024-12-15 03:38:04.022896: Yayy! New best EMA pseudo Dice: 0.744
2024-12-15 03:38:05.853034: 
2024-12-15 03:38:05.854457: Epoch 76
2024-12-15 03:38:05.855210: Current learning rate: 0.00529
2024-12-15 03:46:08.266845: Validation loss did not improve from -0.55997. Patience: 25/50
2024-12-15 03:46:08.267982: train_loss -0.7245
2024-12-15 03:46:08.268845: val_loss -0.5375
2024-12-15 03:46:08.269542: Pseudo dice [0.7428]
2024-12-15 03:46:08.270459: Epoch time: 482.42 s
2024-12-15 03:46:09.804435: 
2024-12-15 03:46:09.805763: Epoch 77
2024-12-15 03:46:09.806695: Current learning rate: 0.00523
2024-12-15 03:54:00.869738: Validation loss did not improve from -0.55997. Patience: 26/50
2024-12-15 03:54:00.870806: train_loss -0.7287
2024-12-15 03:54:00.871805: val_loss -0.5183
2024-12-15 03:54:00.872701: Pseudo dice [0.7312]
2024-12-15 03:54:00.873683: Epoch time: 471.07 s
2024-12-15 03:54:02.358659: 
2024-12-15 03:54:02.360229: Epoch 78
2024-12-15 03:54:02.361284: Current learning rate: 0.00517
2024-12-15 04:01:52.313774: Validation loss did not improve from -0.55997. Patience: 27/50
2024-12-15 04:01:52.314782: train_loss -0.7288
2024-12-15 04:01:52.315745: val_loss -0.5478
2024-12-15 04:01:52.316392: Pseudo dice [0.7537]
2024-12-15 04:01:52.317238: Epoch time: 469.96 s
2024-12-15 04:01:53.718242: 
2024-12-15 04:01:53.719581: Epoch 79
2024-12-15 04:01:53.720237: Current learning rate: 0.0051
2024-12-15 04:10:09.412390: Validation loss improved from -0.55997 to -0.56788! Patience: 27/50
2024-12-15 04:10:09.413438: train_loss -0.7277
2024-12-15 04:10:09.414241: val_loss -0.5679
2024-12-15 04:10:09.415022: Pseudo dice [0.7565]
2024-12-15 04:10:09.415804: Epoch time: 495.7 s
2024-12-15 04:10:09.789813: Yayy! New best EMA pseudo Dice: 0.745
2024-12-15 04:10:11.593705: 
2024-12-15 04:10:11.594948: Epoch 80
2024-12-15 04:10:11.595697: Current learning rate: 0.00504
2024-12-15 04:18:34.302129: Validation loss did not improve from -0.56788. Patience: 1/50
2024-12-15 04:18:34.317828: train_loss -0.7278
2024-12-15 04:18:34.319105: val_loss -0.5575
2024-12-15 04:18:34.319911: Pseudo dice [0.7547]
2024-12-15 04:18:34.320790: Epoch time: 502.73 s
2024-12-15 04:18:34.321696: Yayy! New best EMA pseudo Dice: 0.746
2024-12-15 04:18:36.107596: 
2024-12-15 04:18:36.108886: Epoch 81
2024-12-15 04:18:36.109639: Current learning rate: 0.00497
2024-12-15 04:26:40.816707: Validation loss did not improve from -0.56788. Patience: 2/50
2024-12-15 04:26:40.817733: train_loss -0.734
2024-12-15 04:26:40.818553: val_loss -0.5662
2024-12-15 04:26:40.819234: Pseudo dice [0.7552]
2024-12-15 04:26:40.819874: Epoch time: 484.71 s
2024-12-15 04:26:40.820403: Yayy! New best EMA pseudo Dice: 0.7469
2024-12-15 04:26:42.752750: 
2024-12-15 04:26:42.754117: Epoch 82
2024-12-15 04:26:42.755018: Current learning rate: 0.00491
2024-12-15 04:34:30.092261: Validation loss did not improve from -0.56788. Patience: 3/50
2024-12-15 04:34:30.093363: train_loss -0.7324
2024-12-15 04:34:30.094231: val_loss -0.5201
2024-12-15 04:34:30.094895: Pseudo dice [0.7321]
2024-12-15 04:34:30.095731: Epoch time: 467.34 s
2024-12-15 04:34:31.798457: 
2024-12-15 04:34:31.799754: Epoch 83
2024-12-15 04:34:31.800619: Current learning rate: 0.00484
2024-12-15 04:42:27.856351: Validation loss improved from -0.56788 to -0.56892! Patience: 3/50
2024-12-15 04:42:27.857610: train_loss -0.7358
2024-12-15 04:42:27.858552: val_loss -0.5689
2024-12-15 04:42:27.859280: Pseudo dice [0.7611]
2024-12-15 04:42:27.860103: Epoch time: 476.06 s
2024-12-15 04:42:27.860885: Yayy! New best EMA pseudo Dice: 0.747
2024-12-15 04:42:29.575613: 
2024-12-15 04:42:29.576982: Epoch 84
2024-12-15 04:42:29.577752: Current learning rate: 0.00478
2024-12-15 04:50:11.717906: Validation loss did not improve from -0.56892. Patience: 1/50
2024-12-15 04:50:11.718912: train_loss -0.7362
2024-12-15 04:50:11.719640: val_loss -0.5452
2024-12-15 04:50:11.720274: Pseudo dice [0.7484]
2024-12-15 04:50:11.720892: Epoch time: 462.14 s
2024-12-15 04:50:12.135761: Yayy! New best EMA pseudo Dice: 0.7471
2024-12-15 04:50:13.856337: 
2024-12-15 04:50:13.857677: Epoch 85
2024-12-15 04:50:13.858466: Current learning rate: 0.00471
2024-12-15 04:58:02.368653: Validation loss did not improve from -0.56892. Patience: 2/50
2024-12-15 04:58:02.369746: train_loss -0.7394
2024-12-15 04:58:02.370796: val_loss -0.5141
2024-12-15 04:58:02.371701: Pseudo dice [0.7401]
2024-12-15 04:58:02.372693: Epoch time: 468.51 s
2024-12-15 04:58:03.680034: 
2024-12-15 04:58:03.681602: Epoch 86
2024-12-15 04:58:03.682664: Current learning rate: 0.00465
2024-12-15 05:05:59.837631: Validation loss did not improve from -0.56892. Patience: 3/50
2024-12-15 05:05:59.838618: train_loss -0.7389
2024-12-15 05:05:59.839571: val_loss -0.5458
2024-12-15 05:05:59.840401: Pseudo dice [0.7503]
2024-12-15 05:05:59.841263: Epoch time: 476.16 s
2024-12-15 05:06:01.155283: 
2024-12-15 05:06:01.156827: Epoch 87
2024-12-15 05:06:01.157835: Current learning rate: 0.00458
2024-12-15 05:13:54.293231: Validation loss did not improve from -0.56892. Patience: 4/50
2024-12-15 05:13:54.294239: train_loss -0.7499
2024-12-15 05:13:54.295019: val_loss -0.5611
2024-12-15 05:13:54.295683: Pseudo dice [0.7641]
2024-12-15 05:13:54.296416: Epoch time: 473.14 s
2024-12-15 05:13:54.297104: Yayy! New best EMA pseudo Dice: 0.7485
2024-12-15 05:13:55.981501: 
2024-12-15 05:13:55.982726: Epoch 88
2024-12-15 05:13:55.983398: Current learning rate: 0.00452
2024-12-15 05:21:38.599256: Validation loss did not improve from -0.56892. Patience: 5/50
2024-12-15 05:21:38.600140: train_loss -0.7415
2024-12-15 05:21:38.600840: val_loss -0.5567
2024-12-15 05:21:38.601454: Pseudo dice [0.7579]
2024-12-15 05:21:38.602115: Epoch time: 462.62 s
2024-12-15 05:21:38.602720: Yayy! New best EMA pseudo Dice: 0.7495
2024-12-15 05:21:40.355942: 
2024-12-15 05:21:40.357263: Epoch 89
2024-12-15 05:21:40.358092: Current learning rate: 0.00445
2024-12-15 05:29:42.197130: Validation loss did not improve from -0.56892. Patience: 6/50
2024-12-15 05:29:42.198894: train_loss -0.7414
2024-12-15 05:29:42.199758: val_loss -0.5017
2024-12-15 05:29:42.200504: Pseudo dice [0.7255]
2024-12-15 05:29:42.201141: Epoch time: 481.84 s
2024-12-15 05:29:43.977285: 
2024-12-15 05:29:43.978624: Epoch 90
2024-12-15 05:29:43.979348: Current learning rate: 0.00438
2024-12-15 05:37:36.894959: Validation loss did not improve from -0.56892. Patience: 7/50
2024-12-15 05:37:36.896430: train_loss -0.733
2024-12-15 05:37:36.898244: val_loss -0.5428
2024-12-15 05:37:36.899145: Pseudo dice [0.7447]
2024-12-15 05:37:36.900155: Epoch time: 472.92 s
2024-12-15 05:37:38.243076: 
2024-12-15 05:37:38.244344: Epoch 91
2024-12-15 05:37:38.245071: Current learning rate: 0.00432
2024-12-15 05:45:27.355872: Validation loss did not improve from -0.56892. Patience: 8/50
2024-12-15 05:45:27.356886: train_loss -0.7383
2024-12-15 05:45:27.357646: val_loss -0.5384
2024-12-15 05:45:27.358313: Pseudo dice [0.7449]
2024-12-15 05:45:27.358920: Epoch time: 469.12 s
2024-12-15 05:45:28.710029: 
2024-12-15 05:45:28.711858: Epoch 92
2024-12-15 05:45:28.712714: Current learning rate: 0.00425
2024-12-15 05:53:22.880127: Validation loss did not improve from -0.56892. Patience: 9/50
2024-12-15 05:53:22.881032: train_loss -0.7451
2024-12-15 05:53:22.882028: val_loss -0.5331
2024-12-15 05:53:22.883012: Pseudo dice [0.7448]
2024-12-15 05:53:22.883942: Epoch time: 474.17 s
2024-12-15 05:53:24.234535: 
2024-12-15 05:53:24.235825: Epoch 93
2024-12-15 05:53:24.236918: Current learning rate: 0.00419
2024-12-15 06:01:26.112071: Validation loss improved from -0.56892 to -0.57071! Patience: 9/50
2024-12-15 06:01:26.113113: train_loss -0.7483
2024-12-15 06:01:26.114057: val_loss -0.5707
2024-12-15 06:01:26.114860: Pseudo dice [0.7649]
2024-12-15 06:01:26.115601: Epoch time: 481.88 s
2024-12-15 06:01:27.932776: 
2024-12-15 06:01:27.934152: Epoch 94
2024-12-15 06:01:27.934977: Current learning rate: 0.00412
2024-12-15 06:09:24.588002: Validation loss did not improve from -0.57071. Patience: 1/50
2024-12-15 06:09:24.588950: train_loss -0.753
2024-12-15 06:09:24.589768: val_loss -0.5411
2024-12-15 06:09:24.590454: Pseudo dice [0.7436]
2024-12-15 06:09:24.591199: Epoch time: 476.66 s
2024-12-15 06:09:26.296147: 
2024-12-15 06:09:26.297158: Epoch 95
2024-12-15 06:09:26.297891: Current learning rate: 0.00405
2024-12-15 06:17:17.246217: Validation loss did not improve from -0.57071. Patience: 2/50
2024-12-15 06:17:17.247297: train_loss -0.7522
2024-12-15 06:17:17.248354: val_loss -0.5549
2024-12-15 06:17:17.249318: Pseudo dice [0.7571]
2024-12-15 06:17:17.250306: Epoch time: 470.95 s
2024-12-15 06:17:18.599753: 
2024-12-15 06:17:18.601024: Epoch 96
2024-12-15 06:17:18.601850: Current learning rate: 0.00399
2024-12-15 06:25:13.644547: Validation loss did not improve from -0.57071. Patience: 3/50
2024-12-15 06:25:13.645467: train_loss -0.7493
2024-12-15 06:25:13.646423: val_loss -0.5343
2024-12-15 06:25:13.647369: Pseudo dice [0.7436]
2024-12-15 06:25:13.648336: Epoch time: 475.05 s
2024-12-15 06:25:15.022552: 
2024-12-15 06:25:15.024078: Epoch 97
2024-12-15 06:25:15.025127: Current learning rate: 0.00392
2024-12-15 06:33:18.311008: Validation loss did not improve from -0.57071. Patience: 4/50
2024-12-15 06:33:18.314042: train_loss -0.7534
2024-12-15 06:33:18.315288: val_loss -0.5655
2024-12-15 06:33:18.316160: Pseudo dice [0.7547]
2024-12-15 06:33:18.317287: Epoch time: 483.29 s
2024-12-15 06:33:19.689121: 
2024-12-15 06:33:19.690579: Epoch 98
2024-12-15 06:33:19.691661: Current learning rate: 0.00385
2024-12-15 06:41:29.829222: Validation loss did not improve from -0.57071. Patience: 5/50
2024-12-15 06:41:29.831173: train_loss -0.7537
2024-12-15 06:41:29.833247: val_loss -0.5388
2024-12-15 06:41:29.834171: Pseudo dice [0.7509]
2024-12-15 06:41:29.835164: Epoch time: 490.14 s
2024-12-15 06:41:31.292077: 
2024-12-15 06:41:31.293435: Epoch 99
2024-12-15 06:41:31.294217: Current learning rate: 0.00379
2024-12-15 06:49:42.769353: Validation loss did not improve from -0.57071. Patience: 6/50
2024-12-15 06:49:42.770463: train_loss -0.7579
2024-12-15 06:49:42.771266: val_loss -0.548
2024-12-15 06:49:42.771904: Pseudo dice [0.754]
2024-12-15 06:49:42.772691: Epoch time: 491.48 s
2024-12-15 06:49:43.150508: Yayy! New best EMA pseudo Dice: 0.7496
2024-12-15 06:49:44.873129: 
2024-12-15 06:49:44.874453: Epoch 100
2024-12-15 06:49:44.875276: Current learning rate: 0.00372
2024-12-15 06:57:29.438484: Validation loss did not improve from -0.57071. Patience: 7/50
2024-12-15 06:57:29.439636: train_loss -0.7526
2024-12-15 06:57:29.440462: val_loss -0.5641
2024-12-15 06:57:29.441307: Pseudo dice [0.7569]
2024-12-15 06:57:29.441971: Epoch time: 464.57 s
2024-12-15 06:57:29.442614: Yayy! New best EMA pseudo Dice: 0.7503
2024-12-15 06:57:31.193753: 
2024-12-15 06:57:31.194994: Epoch 101
2024-12-15 06:57:31.195801: Current learning rate: 0.00365
2024-12-15 07:05:14.416985: Validation loss did not improve from -0.57071. Patience: 8/50
2024-12-15 07:05:14.417905: train_loss -0.7548
2024-12-15 07:05:14.418669: val_loss -0.5269
2024-12-15 07:05:14.419377: Pseudo dice [0.7436]
2024-12-15 07:05:14.420103: Epoch time: 463.23 s
2024-12-15 07:05:15.779015: 
2024-12-15 07:05:15.780601: Epoch 102
2024-12-15 07:05:15.781597: Current learning rate: 0.00359
2024-12-15 07:13:08.349965: Validation loss did not improve from -0.57071. Patience: 9/50
2024-12-15 07:13:08.350974: train_loss -0.7558
2024-12-15 07:13:08.351819: val_loss -0.5387
2024-12-15 07:13:08.352609: Pseudo dice [0.7472]
2024-12-15 07:13:08.353355: Epoch time: 472.57 s
2024-12-15 07:13:09.776793: 
2024-12-15 07:13:09.777710: Epoch 103
2024-12-15 07:13:09.778576: Current learning rate: 0.00352
2024-12-15 07:20:57.590072: Validation loss did not improve from -0.57071. Patience: 10/50
2024-12-15 07:20:57.591183: train_loss -0.7573
2024-12-15 07:20:57.591966: val_loss -0.5544
2024-12-15 07:20:57.592690: Pseudo dice [0.7539]
2024-12-15 07:20:57.593497: Epoch time: 467.82 s
2024-12-15 07:20:58.947267: 
2024-12-15 07:20:58.948759: Epoch 104
2024-12-15 07:20:58.949870: Current learning rate: 0.00345
2024-12-15 07:28:57.693822: Validation loss did not improve from -0.57071. Patience: 11/50
2024-12-15 07:28:57.694955: train_loss -0.7633
2024-12-15 07:28:57.695812: val_loss -0.5658
2024-12-15 07:28:57.696631: Pseudo dice [0.7649]
2024-12-15 07:28:57.697407: Epoch time: 478.75 s
2024-12-15 07:28:58.092383: Yayy! New best EMA pseudo Dice: 0.7514
2024-12-15 07:29:00.281814: 
2024-12-15 07:29:00.283225: Epoch 105
2024-12-15 07:29:00.284121: Current learning rate: 0.00338
2024-12-15 07:36:57.177794: Validation loss did not improve from -0.57071. Patience: 12/50
2024-12-15 07:36:57.179496: train_loss -0.7657
2024-12-15 07:36:57.180620: val_loss -0.5572
2024-12-15 07:36:57.181384: Pseudo dice [0.7582]
2024-12-15 07:36:57.182098: Epoch time: 476.9 s
2024-12-15 07:36:57.182893: Yayy! New best EMA pseudo Dice: 0.752
2024-12-15 07:36:58.893221: 
2024-12-15 07:36:58.894804: Epoch 106
2024-12-15 07:36:58.895828: Current learning rate: 0.00332
2024-12-15 07:45:04.960456: Validation loss did not improve from -0.57071. Patience: 13/50
2024-12-15 07:45:04.962154: train_loss -0.7628
2024-12-15 07:45:04.963866: val_loss -0.5521
2024-12-15 07:45:04.964792: Pseudo dice [0.7534]
2024-12-15 07:45:04.965705: Epoch time: 486.07 s
2024-12-15 07:45:04.966344: Yayy! New best EMA pseudo Dice: 0.7522
2024-12-15 07:45:06.751214: 
2024-12-15 07:45:06.752484: Epoch 107
2024-12-15 07:45:06.753198: Current learning rate: 0.00325
2024-12-15 07:52:57.170828: Validation loss did not improve from -0.57071. Patience: 14/50
2024-12-15 07:52:57.171895: train_loss -0.7645
2024-12-15 07:52:57.172699: val_loss -0.552
2024-12-15 07:52:57.173406: Pseudo dice [0.7553]
2024-12-15 07:52:57.174045: Epoch time: 470.42 s
2024-12-15 07:52:57.174692: Yayy! New best EMA pseudo Dice: 0.7525
2024-12-15 07:52:58.975890: 
2024-12-15 07:52:58.977294: Epoch 108
2024-12-15 07:52:58.978081: Current learning rate: 0.00318
2024-12-15 08:00:51.269698: Validation loss did not improve from -0.57071. Patience: 15/50
2024-12-15 08:00:51.270761: train_loss -0.7645
2024-12-15 08:00:51.271642: val_loss -0.5501
2024-12-15 08:00:51.272313: Pseudo dice [0.7505]
2024-12-15 08:00:51.273194: Epoch time: 472.3 s
2024-12-15 08:00:52.676439: 
2024-12-15 08:00:52.677773: Epoch 109
2024-12-15 08:00:52.678568: Current learning rate: 0.00311
2024-12-15 08:08:47.619439: Validation loss did not improve from -0.57071. Patience: 16/50
2024-12-15 08:08:47.620505: train_loss -0.766
2024-12-15 08:08:47.621367: val_loss -0.552
2024-12-15 08:08:47.622141: Pseudo dice [0.7518]
2024-12-15 08:08:47.622945: Epoch time: 474.95 s
2024-12-15 08:08:49.365752: 
2024-12-15 08:08:49.367124: Epoch 110
2024-12-15 08:08:49.367963: Current learning rate: 0.00304
2024-12-15 08:16:44.958314: Validation loss did not improve from -0.57071. Patience: 17/50
2024-12-15 08:16:44.959316: train_loss -0.7644
2024-12-15 08:16:44.960258: val_loss -0.5246
2024-12-15 08:16:44.961051: Pseudo dice [0.7425]
2024-12-15 08:16:44.961867: Epoch time: 475.59 s
2024-12-15 08:16:46.308578: 
2024-12-15 08:16:46.309857: Epoch 111
2024-12-15 08:16:46.310566: Current learning rate: 0.00297
2024-12-15 08:24:45.316558: Validation loss did not improve from -0.57071. Patience: 18/50
2024-12-15 08:24:45.317479: train_loss -0.7677
2024-12-15 08:24:45.318203: val_loss -0.5518
2024-12-15 08:24:45.318866: Pseudo dice [0.7509]
2024-12-15 08:24:45.319556: Epoch time: 479.01 s
2024-12-15 08:24:46.686153: 
2024-12-15 08:24:46.687681: Epoch 112
2024-12-15 08:24:46.688397: Current learning rate: 0.00291
2024-12-15 08:32:47.287431: Validation loss did not improve from -0.57071. Patience: 19/50
2024-12-15 08:32:47.288418: train_loss -0.7695
2024-12-15 08:32:47.289252: val_loss -0.534
2024-12-15 08:32:47.290059: Pseudo dice [0.7459]
2024-12-15 08:32:47.291051: Epoch time: 480.6 s
2024-12-15 08:32:48.632940: 
2024-12-15 08:32:48.634276: Epoch 113
2024-12-15 08:32:48.634969: Current learning rate: 0.00284
2024-12-15 08:40:51.496039: Validation loss did not improve from -0.57071. Patience: 20/50
2024-12-15 08:40:51.497557: train_loss -0.7705
2024-12-15 08:40:51.498487: val_loss -0.5537
2024-12-15 08:40:51.499273: Pseudo dice [0.7536]
2024-12-15 08:40:51.500099: Epoch time: 482.87 s
2024-12-15 08:40:52.858153: 
2024-12-15 08:40:52.859723: Epoch 114
2024-12-15 08:40:52.860716: Current learning rate: 0.00277
2024-12-15 08:48:15.000748: Validation loss did not improve from -0.57071. Patience: 21/50
2024-12-15 08:48:15.001781: train_loss -0.7709
2024-12-15 08:48:15.002545: val_loss -0.5306
2024-12-15 08:48:15.003208: Pseudo dice [0.7422]
2024-12-15 08:48:15.003849: Epoch time: 442.14 s
2024-12-15 08:48:16.726513: 
2024-12-15 08:48:16.727847: Epoch 115
2024-12-15 08:48:16.728560: Current learning rate: 0.0027
2024-12-15 08:55:16.481620: Validation loss did not improve from -0.57071. Patience: 22/50
2024-12-15 08:55:16.482939: train_loss -0.7707
2024-12-15 08:55:16.483750: val_loss -0.5363
2024-12-15 08:55:16.484474: Pseudo dice [0.748]
2024-12-15 08:55:16.485215: Epoch time: 419.76 s
2024-12-15 08:55:18.630641: 
2024-12-15 08:55:18.632022: Epoch 116
2024-12-15 08:55:18.632827: Current learning rate: 0.00263
2024-12-15 09:02:41.068473: Validation loss did not improve from -0.57071. Patience: 23/50
2024-12-15 09:02:41.069605: train_loss -0.7716
2024-12-15 09:02:41.070758: val_loss -0.5653
2024-12-15 09:02:41.071507: Pseudo dice [0.7601]
2024-12-15 09:02:41.072310: Epoch time: 442.44 s
2024-12-15 09:02:42.475481: 
2024-12-15 09:02:42.477015: Epoch 117
2024-12-15 09:02:42.478054: Current learning rate: 0.00256
2024-12-15 09:09:31.942174: Validation loss did not improve from -0.57071. Patience: 24/50
2024-12-15 09:09:31.943355: train_loss -0.7729
2024-12-15 09:09:31.944368: val_loss -0.5533
2024-12-15 09:09:31.945111: Pseudo dice [0.7575]
2024-12-15 09:09:31.945946: Epoch time: 409.47 s
2024-12-15 09:09:33.354252: 
2024-12-15 09:09:33.355470: Epoch 118
2024-12-15 09:09:33.356141: Current learning rate: 0.00249
2024-12-15 09:15:59.245602: Validation loss did not improve from -0.57071. Patience: 25/50
2024-12-15 09:15:59.246407: train_loss -0.772
2024-12-15 09:15:59.247135: val_loss -0.5457
2024-12-15 09:15:59.247811: Pseudo dice [0.7525]
2024-12-15 09:15:59.248509: Epoch time: 385.89 s
2024-12-15 09:16:00.687380: 
2024-12-15 09:16:00.688657: Epoch 119
2024-12-15 09:16:00.689358: Current learning rate: 0.00242
2024-12-15 09:22:42.583755: Validation loss did not improve from -0.57071. Patience: 26/50
2024-12-15 09:22:42.584712: train_loss -0.7744
2024-12-15 09:22:42.585514: val_loss -0.5383
2024-12-15 09:22:42.586301: Pseudo dice [0.7438]
2024-12-15 09:22:42.587043: Epoch time: 401.9 s
2024-12-15 09:22:44.360207: 
2024-12-15 09:22:44.361549: Epoch 120
2024-12-15 09:22:44.362415: Current learning rate: 0.00235
2024-12-15 09:29:28.951287: Validation loss did not improve from -0.57071. Patience: 27/50
2024-12-15 09:29:28.952217: train_loss -0.7729
2024-12-15 09:29:28.953007: val_loss -0.5503
2024-12-15 09:29:28.953757: Pseudo dice [0.761]
2024-12-15 09:29:28.954549: Epoch time: 404.59 s
2024-12-15 09:29:30.375528: 
2024-12-15 09:29:30.376947: Epoch 121
2024-12-15 09:29:30.377804: Current learning rate: 0.00228
2024-12-15 09:36:02.655857: Validation loss did not improve from -0.57071. Patience: 28/50
2024-12-15 09:36:02.656838: train_loss -0.7722
2024-12-15 09:36:02.657765: val_loss -0.5479
2024-12-15 09:36:02.658529: Pseudo dice [0.7556]
2024-12-15 09:36:02.659271: Epoch time: 392.28 s
2024-12-15 09:36:04.042958: 
2024-12-15 09:36:04.044463: Epoch 122
2024-12-15 09:36:04.045402: Current learning rate: 0.00221
2024-12-15 09:42:35.932663: Validation loss did not improve from -0.57071. Patience: 29/50
2024-12-15 09:42:35.934203: train_loss -0.7776
2024-12-15 09:42:35.935285: val_loss -0.5542
2024-12-15 09:42:35.936029: Pseudo dice [0.7571]
2024-12-15 09:42:35.936783: Epoch time: 391.89 s
2024-12-15 09:42:35.937553: Yayy! New best EMA pseudo Dice: 0.7527
2024-12-15 09:42:37.733354: 
2024-12-15 09:42:37.734599: Epoch 123
2024-12-15 09:42:37.735304: Current learning rate: 0.00214
2024-12-15 09:49:30.868143: Validation loss did not improve from -0.57071. Patience: 30/50
2024-12-15 09:49:30.869258: train_loss -0.7777
2024-12-15 09:49:30.870289: val_loss -0.52
2024-12-15 09:49:30.871240: Pseudo dice [0.7339]
2024-12-15 09:49:30.872261: Epoch time: 413.14 s
2024-12-15 09:49:32.256146: 
2024-12-15 09:49:32.257164: Epoch 124
2024-12-15 09:49:32.257975: Current learning rate: 0.00207
2024-12-15 09:56:34.061620: Validation loss did not improve from -0.57071. Patience: 31/50
2024-12-15 09:56:34.062538: train_loss -0.7797
2024-12-15 09:56:34.063447: val_loss -0.5404
2024-12-15 09:56:34.064300: Pseudo dice [0.7489]
2024-12-15 09:56:34.065290: Epoch time: 421.81 s
2024-12-15 09:56:35.831750: 
2024-12-15 09:56:35.833081: Epoch 125
2024-12-15 09:56:35.833892: Current learning rate: 0.00199
2024-12-15 10:03:03.922000: Validation loss did not improve from -0.57071. Patience: 32/50
2024-12-15 10:03:03.922860: train_loss -0.7801
2024-12-15 10:03:03.923688: val_loss -0.5521
2024-12-15 10:03:03.924384: Pseudo dice [0.7528]
2024-12-15 10:03:03.925263: Epoch time: 388.09 s
2024-12-15 10:03:05.311291: 
2024-12-15 10:03:05.312721: Epoch 126
2024-12-15 10:03:05.313510: Current learning rate: 0.00192
2024-12-15 10:09:23.625003: Validation loss did not improve from -0.57071. Patience: 33/50
2024-12-15 10:09:23.626374: train_loss -0.7788
2024-12-15 10:09:23.627450: val_loss -0.5311
2024-12-15 10:09:23.628132: Pseudo dice [0.753]
2024-12-15 10:09:23.628742: Epoch time: 378.32 s
2024-12-15 10:09:25.482909: 
2024-12-15 10:09:25.484221: Epoch 127
2024-12-15 10:09:25.484881: Current learning rate: 0.00185
2024-12-15 10:15:54.747812: Validation loss did not improve from -0.57071. Patience: 34/50
2024-12-15 10:15:54.748665: train_loss -0.7808
2024-12-15 10:15:54.749557: val_loss -0.5691
2024-12-15 10:15:54.750364: Pseudo dice [0.7704]
2024-12-15 10:15:54.751077: Epoch time: 389.27 s
2024-12-15 10:15:54.751786: Yayy! New best EMA pseudo Dice: 0.753
2024-12-15 10:15:56.499640: 
2024-12-15 10:15:56.500934: Epoch 128
2024-12-15 10:15:56.501743: Current learning rate: 0.00178
2024-12-15 10:22:44.389909: Validation loss did not improve from -0.57071. Patience: 35/50
2024-12-15 10:22:44.390992: train_loss -0.7828
2024-12-15 10:22:44.391943: val_loss -0.5513
2024-12-15 10:22:44.392948: Pseudo dice [0.7574]
2024-12-15 10:22:44.393818: Epoch time: 407.89 s
2024-12-15 10:22:44.394625: Yayy! New best EMA pseudo Dice: 0.7534
2024-12-15 10:22:46.194780: 
2024-12-15 10:22:46.195965: Epoch 129
2024-12-15 10:22:46.196834: Current learning rate: 0.0017
2024-12-15 10:29:54.081755: Validation loss did not improve from -0.57071. Patience: 36/50
2024-12-15 10:29:54.082608: train_loss -0.7828
2024-12-15 10:29:54.083652: val_loss -0.5428
2024-12-15 10:29:54.084570: Pseudo dice [0.7476]
2024-12-15 10:29:54.085561: Epoch time: 427.89 s
2024-12-15 10:29:55.845047: 
2024-12-15 10:29:55.846526: Epoch 130
2024-12-15 10:29:55.847514: Current learning rate: 0.00163
2024-12-15 10:36:27.368943: Validation loss did not improve from -0.57071. Patience: 37/50
2024-12-15 10:36:27.370000: train_loss -0.7836
2024-12-15 10:36:27.370885: val_loss -0.5588
2024-12-15 10:36:27.371641: Pseudo dice [0.7618]
2024-12-15 10:36:27.372536: Epoch time: 391.53 s
2024-12-15 10:36:27.373414: Yayy! New best EMA pseudo Dice: 0.7538
2024-12-15 10:36:29.200796: 
2024-12-15 10:36:29.202283: Epoch 131
2024-12-15 10:36:29.203280: Current learning rate: 0.00156
2024-12-15 10:42:58.513549: Validation loss did not improve from -0.57071. Patience: 38/50
2024-12-15 10:42:58.514512: train_loss -0.7844
2024-12-15 10:42:58.515270: val_loss -0.5596
2024-12-15 10:42:58.515924: Pseudo dice [0.757]
2024-12-15 10:42:58.516701: Epoch time: 389.31 s
2024-12-15 10:42:58.517366: Yayy! New best EMA pseudo Dice: 0.7541
2024-12-15 10:43:00.356154: 
2024-12-15 10:43:00.357394: Epoch 132
2024-12-15 10:43:00.358159: Current learning rate: 0.00148
2024-12-15 10:49:16.182252: Validation loss did not improve from -0.57071. Patience: 39/50
2024-12-15 10:49:16.186608: train_loss -0.7847
2024-12-15 10:49:16.187734: val_loss -0.5571
2024-12-15 10:49:16.188705: Pseudo dice [0.7552]
2024-12-15 10:49:16.189939: Epoch time: 375.83 s
2024-12-15 10:49:16.191218: Yayy! New best EMA pseudo Dice: 0.7542
2024-12-15 10:49:17.970696: 
2024-12-15 10:49:17.972178: Epoch 133
2024-12-15 10:49:17.973272: Current learning rate: 0.00141
2024-12-15 10:55:40.232805: Validation loss did not improve from -0.57071. Patience: 40/50
2024-12-15 10:55:40.233840: train_loss -0.7835
2024-12-15 10:55:40.234565: val_loss -0.5263
2024-12-15 10:55:40.235240: Pseudo dice [0.7455]
2024-12-15 10:55:40.235896: Epoch time: 382.26 s
2024-12-15 10:55:41.614641: 
2024-12-15 10:55:41.615964: Epoch 134
2024-12-15 10:55:41.616767: Current learning rate: 0.00133
2024-12-15 11:02:15.889566: Validation loss did not improve from -0.57071. Patience: 41/50
2024-12-15 11:02:15.891263: train_loss -0.7886
2024-12-15 11:02:15.893141: val_loss -0.5432
2024-12-15 11:02:15.894280: Pseudo dice [0.7477]
2024-12-15 11:02:15.895602: Epoch time: 394.28 s
2024-12-15 11:02:17.746159: 
2024-12-15 11:02:17.747647: Epoch 135
2024-12-15 11:02:17.748767: Current learning rate: 0.00126
2024-12-15 11:09:16.142547: Validation loss did not improve from -0.57071. Patience: 42/50
2024-12-15 11:09:16.143724: train_loss -0.7864
2024-12-15 11:09:16.144562: val_loss -0.5433
2024-12-15 11:09:16.145267: Pseudo dice [0.7472]
2024-12-15 11:09:16.145943: Epoch time: 418.4 s
2024-12-15 11:09:17.581132: 
2024-12-15 11:09:17.582352: Epoch 136
2024-12-15 11:09:17.583036: Current learning rate: 0.00118
2024-12-15 11:16:10.672084: Validation loss did not improve from -0.57071. Patience: 43/50
2024-12-15 11:16:10.673251: train_loss -0.7837
2024-12-15 11:16:10.674107: val_loss -0.5591
2024-12-15 11:16:10.674812: Pseudo dice [0.7577]
2024-12-15 11:16:10.675490: Epoch time: 413.09 s
2024-12-15 11:16:12.085018: 
2024-12-15 11:16:12.086435: Epoch 137
2024-12-15 11:16:12.087281: Current learning rate: 0.00111
2024-12-15 11:22:42.386316: Validation loss did not improve from -0.57071. Patience: 44/50
2024-12-15 11:22:42.387314: train_loss -0.7879
2024-12-15 11:22:42.388083: val_loss -0.566
2024-12-15 11:22:42.388816: Pseudo dice [0.7637]
2024-12-15 11:22:42.389460: Epoch time: 390.3 s
2024-12-15 11:22:44.338187: 
2024-12-15 11:22:44.339561: Epoch 138
2024-12-15 11:22:44.340345: Current learning rate: 0.00103
2024-12-15 11:29:19.132583: Validation loss did not improve from -0.57071. Patience: 45/50
2024-12-15 11:29:19.133252: train_loss -0.7878
2024-12-15 11:29:19.134068: val_loss -0.5575
2024-12-15 11:29:19.134745: Pseudo dice [0.7546]
2024-12-15 11:29:19.135410: Epoch time: 394.8 s
2024-12-15 11:29:20.551090: 
2024-12-15 11:29:20.552331: Epoch 139
2024-12-15 11:29:20.553143: Current learning rate: 0.00095
2024-12-15 11:35:34.689559: Validation loss did not improve from -0.57071. Patience: 46/50
2024-12-15 11:35:34.690306: train_loss -0.7846
2024-12-15 11:35:34.691032: val_loss -0.5487
2024-12-15 11:35:34.691664: Pseudo dice [0.7522]
2024-12-15 11:35:34.692303: Epoch time: 374.14 s
2024-12-15 11:35:36.581831: 
2024-12-15 11:35:36.583182: Epoch 140
2024-12-15 11:35:36.583882: Current learning rate: 0.00087
2024-12-15 11:41:37.691928: Validation loss did not improve from -0.57071. Patience: 47/50
2024-12-15 11:41:37.692956: train_loss -0.7874
2024-12-15 11:41:37.693774: val_loss -0.5633
2024-12-15 11:41:37.694494: Pseudo dice [0.7646]
2024-12-15 11:41:37.695149: Epoch time: 361.11 s
2024-12-15 11:41:37.695898: Yayy! New best EMA pseudo Dice: 0.7548
2024-12-15 11:41:39.525434: 
2024-12-15 11:41:39.526752: Epoch 141
2024-12-15 11:41:39.527579: Current learning rate: 0.00079
2024-12-15 11:47:07.377127: Validation loss did not improve from -0.57071. Patience: 48/50
2024-12-15 11:47:07.378114: train_loss -0.7865
2024-12-15 11:47:07.378987: val_loss -0.5494
2024-12-15 11:47:07.379721: Pseudo dice [0.7606]
2024-12-15 11:47:07.380449: Epoch time: 327.85 s
2024-12-15 11:47:07.381156: Yayy! New best EMA pseudo Dice: 0.7554
2024-12-15 11:47:09.250762: 
2024-12-15 11:47:09.252135: Epoch 142
2024-12-15 11:47:09.252909: Current learning rate: 0.00071
2024-12-15 11:50:59.835305: Validation loss did not improve from -0.57071. Patience: 49/50
2024-12-15 11:50:59.836331: train_loss -0.7843
2024-12-15 11:50:59.837151: val_loss -0.5274
2024-12-15 11:50:59.837807: Pseudo dice [0.742]
2024-12-15 11:50:59.838516: Epoch time: 230.59 s
2024-12-15 11:51:01.308787: 
2024-12-15 11:51:01.310182: Epoch 143
2024-12-15 11:51:01.310918: Current learning rate: 0.00063
2024-12-15 11:55:26.955982: Validation loss did not improve from -0.57071. Patience: 50/50
2024-12-15 11:55:26.966742: train_loss -0.7886
2024-12-15 11:55:26.967840: val_loss -0.5597
2024-12-15 11:55:26.968589: Pseudo dice [0.7542]
2024-12-15 11:55:26.969659: Epoch time: 265.65 s
2024-12-15 11:55:28.521285: 
2024-12-15 11:55:28.522574: Epoch 144
2024-12-15 11:55:28.523360: Current learning rate: 0.00055
2024-12-15 12:02:17.221726: Validation loss did not improve from -0.57071. Patience: 51/50
2024-12-15 12:02:17.222757: train_loss -0.7919
2024-12-15 12:02:17.223854: val_loss -0.538
2024-12-15 12:02:17.224867: Pseudo dice [0.7497]
2024-12-15 12:02:17.225864: Epoch time: 408.7 s
2024-12-15 12:02:19.103563: 
2024-12-15 12:02:19.104936: Epoch 145
2024-12-15 12:02:19.106157: Current learning rate: 0.00047
2024-12-15 12:09:16.119764: Validation loss did not improve from -0.57071. Patience: 52/50
2024-12-15 12:09:16.120839: train_loss -0.7902
2024-12-15 12:09:16.122758: val_loss -0.5628
2024-12-15 12:09:16.123614: Pseudo dice [0.7624]
2024-12-15 12:09:16.124486: Epoch time: 417.02 s
2024-12-15 12:09:17.579388: 
2024-12-15 12:09:17.580666: Epoch 146
2024-12-15 12:09:17.581575: Current learning rate: 0.00038
2024-12-15 12:16:18.996753: Validation loss did not improve from -0.57071. Patience: 53/50
2024-12-15 12:16:18.997907: train_loss -0.7891
2024-12-15 12:16:18.998701: val_loss -0.5419
2024-12-15 12:16:18.999401: Pseudo dice [0.7493]
2024-12-15 12:16:19.000164: Epoch time: 421.42 s
2024-12-15 12:16:20.432450: 
2024-12-15 12:16:20.433617: Epoch 147
2024-12-15 12:16:20.434276: Current learning rate: 0.0003
2024-12-15 12:23:13.661859: Validation loss did not improve from -0.57071. Patience: 54/50
2024-12-15 12:23:13.662865: train_loss -0.7917
2024-12-15 12:23:13.664165: val_loss -0.5605
2024-12-15 12:23:13.664911: Pseudo dice [0.7608]
2024-12-15 12:23:13.665681: Epoch time: 413.23 s
2024-12-15 12:23:15.827402: 
2024-12-15 12:23:15.828900: Epoch 148
2024-12-15 12:23:15.829710: Current learning rate: 0.00021
2024-12-15 12:30:14.349380: Validation loss did not improve from -0.57071. Patience: 55/50
2024-12-15 12:30:14.350521: train_loss -0.7941
2024-12-15 12:30:14.351465: val_loss -0.5602
2024-12-15 12:30:14.352419: Pseudo dice [0.7675]
2024-12-15 12:30:14.353408: Epoch time: 418.52 s
2024-12-15 12:30:14.354311: Yayy! New best EMA pseudo Dice: 0.756
2024-12-15 12:30:16.240035: 
2024-12-15 12:30:16.241878: Epoch 149
2024-12-15 12:30:16.242843: Current learning rate: 0.00011
2024-12-15 12:36:36.525610: Validation loss did not improve from -0.57071. Patience: 56/50
2024-12-15 12:36:36.526809: train_loss -0.7932
2024-12-15 12:36:36.528335: val_loss -0.5512
2024-12-15 12:36:36.529356: Pseudo dice [0.749]
2024-12-15 12:36:36.530431: Epoch time: 380.29 s
2024-12-15 12:36:38.561149: Training done.
2024-12-15 12:36:38.820694: Using splits from existing split file: /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset307_Sohee_Calcium_OCT_CrossValidation/splits_final_60.json
2024-12-15 12:36:38.836323: The split file contains 5 splits.
2024-12-15 12:36:38.837468: Desired fold for training: 2
2024-12-15 12:36:38.838325: This split has 4 training and 4 validation cases.
2024-12-15 12:36:38.839348: predicting 101-044
2024-12-15 12:36:38.948279: 101-044, shape torch.Size([1, 404, 498, 498]), rank 0
2024-12-15 12:38:35.867564: predicting 101-045
2024-12-15 12:38:35.886122: 101-045, shape torch.Size([1, 375, 498, 498]), rank 0
2024-12-15 12:40:44.589336: predicting 704-003
2024-12-15 12:40:44.607936: 704-003, shape torch.Size([1, 375, 498, 498]), rank 0
2024-12-15 12:42:46.203144: predicting 706-005
2024-12-15 12:42:46.223621: 706-005, shape torch.Size([1, 375, 498, 498]), rank 0
2024-12-15 12:45:21.706822: Validation complete
2024-12-15 12:45:21.707448: Mean Validation Dice:  0.7444390959179578
2024-12-14 17:39:30.026589: unpacking done...
2024-12-14 17:39:30.034529: Unable to plot network architecture: nnUNet_compile is enabled!
2024-12-14 17:39:30.096241: 
2024-12-14 17:39:30.097713: Epoch 0
2024-12-14 17:39:30.098649: Current learning rate: 0.01
2024-12-14 17:46:50.784316: Validation loss improved from 1000.00000 to -0.17408! Patience: 0/50
2024-12-14 17:46:50.785501: train_loss -0.0934
2024-12-14 17:46:50.786437: val_loss -0.1741
2024-12-14 17:46:50.787143: Pseudo dice [0.5258]
2024-12-14 17:46:50.787831: Epoch time: 440.69 s
2024-12-14 17:46:50.788512: Yayy! New best EMA pseudo Dice: 0.5258
2024-12-14 17:46:52.432878: 
2024-12-14 17:46:52.434221: Epoch 1
2024-12-14 17:46:52.435085: Current learning rate: 0.00994
2024-12-14 17:53:52.718426: Validation loss improved from -0.17408 to -0.17810! Patience: 0/50
2024-12-14 17:53:52.719449: train_loss -0.2685
2024-12-14 17:53:52.720413: val_loss -0.1781
2024-12-14 17:53:52.721351: Pseudo dice [0.5643]
2024-12-14 17:53:52.722175: Epoch time: 420.29 s
2024-12-14 17:53:52.723025: Yayy! New best EMA pseudo Dice: 0.5297
2024-12-14 17:53:54.511814: 
2024-12-14 17:53:54.513257: Epoch 2
2024-12-14 17:53:54.514485: Current learning rate: 0.00988
2024-12-14 18:00:21.293699: Validation loss improved from -0.17810 to -0.22945! Patience: 0/50
2024-12-14 18:00:21.294776: train_loss -0.3061
2024-12-14 18:00:21.295651: val_loss -0.2294
2024-12-14 18:00:21.296445: Pseudo dice [0.5864]
2024-12-14 18:00:21.297123: Epoch time: 386.78 s
2024-12-14 18:00:21.298004: Yayy! New best EMA pseudo Dice: 0.5353
2024-12-14 18:00:23.211559: 
2024-12-14 18:00:23.212776: Epoch 3
2024-12-14 18:00:23.213630: Current learning rate: 0.00982
2024-12-14 18:07:13.515433: Validation loss did not improve from -0.22945. Patience: 1/50
2024-12-14 18:07:13.516487: train_loss -0.3619
2024-12-14 18:07:13.517335: val_loss -0.2263
2024-12-14 18:07:13.518216: Pseudo dice [0.5717]
2024-12-14 18:07:13.518913: Epoch time: 410.31 s
2024-12-14 18:07:13.519758: Yayy! New best EMA pseudo Dice: 0.539
2024-12-14 18:07:15.356221: 
2024-12-14 18:07:15.357454: Epoch 4
2024-12-14 18:07:15.358297: Current learning rate: 0.00976
2024-12-14 18:14:26.819858: Validation loss improved from -0.22945 to -0.23749! Patience: 1/50
2024-12-14 18:14:26.820736: train_loss -0.3693
2024-12-14 18:14:26.821549: val_loss -0.2375
2024-12-14 18:14:26.822316: Pseudo dice [0.5865]
2024-12-14 18:14:26.823100: Epoch time: 431.47 s
2024-12-14 18:14:27.150061: Yayy! New best EMA pseudo Dice: 0.5437
2024-12-14 18:14:28.945709: 
2024-12-14 18:14:28.947111: Epoch 5
2024-12-14 18:14:28.947957: Current learning rate: 0.0097
2024-12-14 18:21:50.461294: Validation loss improved from -0.23749 to -0.25223! Patience: 0/50
2024-12-14 18:21:50.462350: train_loss -0.4019
2024-12-14 18:21:50.463164: val_loss -0.2522
2024-12-14 18:21:50.463970: Pseudo dice [0.5973]
2024-12-14 18:21:50.464666: Epoch time: 441.52 s
2024-12-14 18:21:50.465326: Yayy! New best EMA pseudo Dice: 0.5491
2024-12-14 18:21:52.238324: 
2024-12-14 18:21:52.239638: Epoch 6
2024-12-14 18:21:52.240400: Current learning rate: 0.00964
2024-12-14 18:28:40.235351: Validation loss improved from -0.25223 to -0.31243! Patience: 0/50
2024-12-14 18:28:40.236445: train_loss -0.4276
2024-12-14 18:28:40.237132: val_loss -0.3124
2024-12-14 18:28:40.237777: Pseudo dice [0.6227]
2024-12-14 18:28:40.238447: Epoch time: 408.0 s
2024-12-14 18:28:40.239132: Yayy! New best EMA pseudo Dice: 0.5564
2024-12-14 18:28:42.032921: 
2024-12-14 18:28:42.034363: Epoch 7
2024-12-14 18:28:42.035178: Current learning rate: 0.00958
2024-12-14 18:36:03.597539: Validation loss did not improve from -0.31243. Patience: 1/50
2024-12-14 18:36:03.598568: train_loss -0.4611
2024-12-14 18:36:03.599759: val_loss -0.3086
2024-12-14 18:36:03.600637: Pseudo dice [0.619]
2024-12-14 18:36:03.601473: Epoch time: 441.57 s
2024-12-14 18:36:03.602154: Yayy! New best EMA pseudo Dice: 0.5627
2024-12-14 18:36:05.402403: 
2024-12-14 18:36:05.403632: Epoch 8
2024-12-14 18:36:05.404463: Current learning rate: 0.00952
2024-12-14 18:43:02.599235: Validation loss did not improve from -0.31243. Patience: 2/50
2024-12-14 18:43:02.600280: train_loss -0.4744
2024-12-14 18:43:02.601117: val_loss -0.3044
2024-12-14 18:43:02.601918: Pseudo dice [0.6291]
2024-12-14 18:43:02.602899: Epoch time: 417.2 s
2024-12-14 18:43:02.603608: Yayy! New best EMA pseudo Dice: 0.5693
2024-12-14 18:43:04.946541: 
2024-12-14 18:43:04.947566: Epoch 9
2024-12-14 18:43:04.948311: Current learning rate: 0.00946
2024-12-14 18:49:35.883974: Validation loss improved from -0.31243 to -0.34931! Patience: 2/50
2024-12-14 18:49:35.885165: train_loss -0.4864
2024-12-14 18:49:35.886022: val_loss -0.3493
2024-12-14 18:49:35.886982: Pseudo dice [0.6525]
2024-12-14 18:49:35.887828: Epoch time: 390.94 s
2024-12-14 18:49:36.241379: Yayy! New best EMA pseudo Dice: 0.5777
2024-12-14 18:49:37.978148: 
2024-12-14 18:49:37.979654: Epoch 10
2024-12-14 18:49:37.980520: Current learning rate: 0.0094
2024-12-14 18:56:39.995960: Validation loss improved from -0.34931 to -0.37127! Patience: 0/50
2024-12-14 18:56:39.996853: train_loss -0.5081
2024-12-14 18:56:39.997635: val_loss -0.3713
2024-12-14 18:56:39.998527: Pseudo dice [0.6656]
2024-12-14 18:56:39.999393: Epoch time: 422.02 s
2024-12-14 18:56:40.000199: Yayy! New best EMA pseudo Dice: 0.5864
2024-12-14 18:56:41.810863: 
2024-12-14 18:56:41.812098: Epoch 11
2024-12-14 18:56:41.813054: Current learning rate: 0.00934
2024-12-14 19:04:23.695453: Validation loss improved from -0.37127 to -0.37567! Patience: 0/50
2024-12-14 19:04:23.696465: train_loss -0.5107
2024-12-14 19:04:23.697331: val_loss -0.3757
2024-12-14 19:04:23.698066: Pseudo dice [0.6522]
2024-12-14 19:04:23.698711: Epoch time: 461.89 s
2024-12-14 19:04:23.699455: Yayy! New best EMA pseudo Dice: 0.593
2024-12-14 19:04:25.444282: 
2024-12-14 19:04:25.445670: Epoch 12
2024-12-14 19:04:25.446504: Current learning rate: 0.00928
2024-12-14 19:12:17.890044: Validation loss did not improve from -0.37567. Patience: 1/50
2024-12-14 19:12:17.891040: train_loss -0.5274
2024-12-14 19:12:17.891892: val_loss -0.3673
2024-12-14 19:12:17.892611: Pseudo dice [0.6581]
2024-12-14 19:12:17.893309: Epoch time: 472.45 s
2024-12-14 19:12:17.894021: Yayy! New best EMA pseudo Dice: 0.5995
2024-12-14 19:12:19.663549: 
2024-12-14 19:12:19.664814: Epoch 13
2024-12-14 19:12:19.665766: Current learning rate: 0.00922
2024-12-14 19:20:07.312964: Validation loss improved from -0.37567 to -0.38235! Patience: 1/50
2024-12-14 19:20:07.313992: train_loss -0.5329
2024-12-14 19:20:07.314857: val_loss -0.3824
2024-12-14 19:20:07.315514: Pseudo dice [0.6683]
2024-12-14 19:20:07.316181: Epoch time: 467.65 s
2024-12-14 19:20:07.316820: Yayy! New best EMA pseudo Dice: 0.6064
2024-12-14 19:20:09.104588: 
2024-12-14 19:20:09.105761: Epoch 14
2024-12-14 19:20:09.106712: Current learning rate: 0.00916
2024-12-14 19:27:53.166176: Validation loss improved from -0.38235 to -0.39094! Patience: 0/50
2024-12-14 19:27:53.167104: train_loss -0.5449
2024-12-14 19:27:53.167983: val_loss -0.3909
2024-12-14 19:27:53.168780: Pseudo dice [0.671]
2024-12-14 19:27:53.169774: Epoch time: 464.06 s
2024-12-14 19:27:53.561586: Yayy! New best EMA pseudo Dice: 0.6129
2024-12-14 19:27:55.317405: 
2024-12-14 19:27:55.318551: Epoch 15
2024-12-14 19:27:55.319366: Current learning rate: 0.0091
2024-12-14 19:35:50.580799: Validation loss did not improve from -0.39094. Patience: 1/50
2024-12-14 19:35:50.581871: train_loss -0.5603
2024-12-14 19:35:50.582703: val_loss -0.3701
2024-12-14 19:35:50.583377: Pseudo dice [0.6756]
2024-12-14 19:35:50.584058: Epoch time: 475.27 s
2024-12-14 19:35:50.584748: Yayy! New best EMA pseudo Dice: 0.6191
2024-12-14 19:35:52.427721: 
2024-12-14 19:35:52.429101: Epoch 16
2024-12-14 19:35:52.429869: Current learning rate: 0.00903
2024-12-14 19:44:07.609190: Validation loss improved from -0.39094 to -0.39231! Patience: 1/50
2024-12-14 19:44:07.610271: train_loss -0.5595
2024-12-14 19:44:07.611277: val_loss -0.3923
2024-12-14 19:44:07.612289: Pseudo dice [0.6722]
2024-12-14 19:44:07.613212: Epoch time: 495.18 s
2024-12-14 19:44:07.614126: Yayy! New best EMA pseudo Dice: 0.6244
2024-12-14 19:44:09.416941: 
2024-12-14 19:44:09.418549: Epoch 17
2024-12-14 19:44:09.419586: Current learning rate: 0.00897
2024-12-14 19:52:19.719296: Validation loss did not improve from -0.39231. Patience: 1/50
2024-12-14 19:52:19.721143: train_loss -0.573
2024-12-14 19:52:19.723184: val_loss -0.3725
2024-12-14 19:52:19.724111: Pseudo dice [0.6686]
2024-12-14 19:52:19.725592: Epoch time: 490.31 s
2024-12-14 19:52:19.726577: Yayy! New best EMA pseudo Dice: 0.6289
2024-12-14 19:52:21.581187: 
2024-12-14 19:52:21.582399: Epoch 18
2024-12-14 19:52:21.583281: Current learning rate: 0.00891
2024-12-14 20:00:20.064255: Validation loss improved from -0.39231 to -0.40222! Patience: 1/50
2024-12-14 20:00:20.065365: train_loss -0.5817
2024-12-14 20:00:20.066262: val_loss -0.4022
2024-12-14 20:00:20.067094: Pseudo dice [0.6846]
2024-12-14 20:00:20.067860: Epoch time: 478.49 s
2024-12-14 20:00:20.068609: Yayy! New best EMA pseudo Dice: 0.6344
2024-12-14 20:00:22.351107: 
2024-12-14 20:00:22.352554: Epoch 19
2024-12-14 20:00:22.353353: Current learning rate: 0.00885
2024-12-14 20:08:35.830405: Validation loss did not improve from -0.40222. Patience: 1/50
2024-12-14 20:08:35.831376: train_loss -0.5849
2024-12-14 20:08:35.832361: val_loss -0.3678
2024-12-14 20:08:35.833302: Pseudo dice [0.6689]
2024-12-14 20:08:35.834214: Epoch time: 493.48 s
2024-12-14 20:08:36.204139: Yayy! New best EMA pseudo Dice: 0.6379
2024-12-14 20:08:37.999521: 
2024-12-14 20:08:38.000886: Epoch 20
2024-12-14 20:08:38.001965: Current learning rate: 0.00879
2024-12-14 20:16:43.404776: Validation loss did not improve from -0.40222. Patience: 2/50
2024-12-14 20:16:43.406020: train_loss -0.5981
2024-12-14 20:16:43.406885: val_loss -0.3761
2024-12-14 20:16:43.407555: Pseudo dice [0.6771]
2024-12-14 20:16:43.408247: Epoch time: 485.41 s
2024-12-14 20:16:43.408999: Yayy! New best EMA pseudo Dice: 0.6418
2024-12-14 20:16:45.281954: 
2024-12-14 20:16:45.283505: Epoch 21
2024-12-14 20:16:45.284410: Current learning rate: 0.00873
2024-12-14 20:25:16.360923: Validation loss did not improve from -0.40222. Patience: 3/50
2024-12-14 20:25:16.361928: train_loss -0.6072
2024-12-14 20:25:16.362754: val_loss -0.3421
2024-12-14 20:25:16.363534: Pseudo dice [0.6583]
2024-12-14 20:25:16.364310: Epoch time: 511.08 s
2024-12-14 20:25:16.365105: Yayy! New best EMA pseudo Dice: 0.6435
2024-12-14 20:25:18.117466: 
2024-12-14 20:25:18.119261: Epoch 22
2024-12-14 20:25:18.120318: Current learning rate: 0.00867
2024-12-14 20:33:34.121954: Validation loss did not improve from -0.40222. Patience: 4/50
2024-12-14 20:33:34.123082: train_loss -0.6072
2024-12-14 20:33:34.124006: val_loss -0.3768
2024-12-14 20:33:34.124944: Pseudo dice [0.6761]
2024-12-14 20:33:34.125681: Epoch time: 496.01 s
2024-12-14 20:33:34.126550: Yayy! New best EMA pseudo Dice: 0.6467
2024-12-14 20:33:35.888017: 
2024-12-14 20:33:35.889441: Epoch 23
2024-12-14 20:33:35.890249: Current learning rate: 0.00861
2024-12-14 20:41:45.991094: Validation loss improved from -0.40222 to -0.43072! Patience: 4/50
2024-12-14 20:41:45.992245: train_loss -0.6205
2024-12-14 20:41:45.992960: val_loss -0.4307
2024-12-14 20:41:45.993618: Pseudo dice [0.7046]
2024-12-14 20:41:45.994329: Epoch time: 490.11 s
2024-12-14 20:41:45.994947: Yayy! New best EMA pseudo Dice: 0.6525
2024-12-14 20:41:47.733555: 
2024-12-14 20:41:47.734872: Epoch 24
2024-12-14 20:41:47.735669: Current learning rate: 0.00855
2024-12-14 20:49:45.688933: Validation loss did not improve from -0.43072. Patience: 1/50
2024-12-14 20:49:45.689927: train_loss -0.6207
2024-12-14 20:49:45.690977: val_loss -0.3811
2024-12-14 20:49:45.691967: Pseudo dice [0.6869]
2024-12-14 20:49:45.692802: Epoch time: 477.96 s
2024-12-14 20:49:46.064488: Yayy! New best EMA pseudo Dice: 0.6559
2024-12-14 20:49:47.791142: 
2024-12-14 20:49:47.792552: Epoch 25
2024-12-14 20:49:47.793393: Current learning rate: 0.00849
2024-12-14 20:57:51.462597: Validation loss did not improve from -0.43072. Patience: 2/50
2024-12-14 20:57:51.465003: train_loss -0.6214
2024-12-14 20:57:51.466582: val_loss -0.3476
2024-12-14 20:57:51.467407: Pseudo dice [0.6744]
2024-12-14 20:57:51.468240: Epoch time: 483.68 s
2024-12-14 20:57:51.469071: Yayy! New best EMA pseudo Dice: 0.6578
2024-12-14 20:57:53.271407: 
2024-12-14 20:57:53.272869: Epoch 26
2024-12-14 20:57:53.274025: Current learning rate: 0.00843
2024-12-14 21:05:51.549993: Validation loss did not improve from -0.43072. Patience: 3/50
2024-12-14 21:05:51.550921: train_loss -0.6225
2024-12-14 21:05:51.551689: val_loss -0.3643
2024-12-14 21:05:51.552636: Pseudo dice [0.6727]
2024-12-14 21:05:51.553425: Epoch time: 478.28 s
2024-12-14 21:05:51.554096: Yayy! New best EMA pseudo Dice: 0.6593
2024-12-14 21:05:53.507145: 
2024-12-14 21:05:53.508475: Epoch 27
2024-12-14 21:05:53.509291: Current learning rate: 0.00836
2024-12-14 21:14:07.441067: Validation loss did not improve from -0.43072. Patience: 4/50
2024-12-14 21:14:07.442279: train_loss -0.6326
2024-12-14 21:14:07.443266: val_loss -0.3718
2024-12-14 21:14:07.444067: Pseudo dice [0.6722]
2024-12-14 21:14:07.444893: Epoch time: 493.94 s
2024-12-14 21:14:07.445586: Yayy! New best EMA pseudo Dice: 0.6606
2024-12-14 21:14:09.252249: 
2024-12-14 21:14:09.253542: Epoch 28
2024-12-14 21:14:09.254704: Current learning rate: 0.0083
2024-12-14 21:22:30.102675: Validation loss did not improve from -0.43072. Patience: 5/50
2024-12-14 21:22:30.103815: train_loss -0.6456
2024-12-14 21:22:30.104762: val_loss -0.3874
2024-12-14 21:22:30.105594: Pseudo dice [0.6756]
2024-12-14 21:22:30.106501: Epoch time: 500.85 s
2024-12-14 21:22:30.107368: Yayy! New best EMA pseudo Dice: 0.6621
2024-12-14 21:22:32.161384: 
2024-12-14 21:22:32.162701: Epoch 29
2024-12-14 21:22:32.163416: Current learning rate: 0.00824
2024-12-14 21:30:43.793363: Validation loss did not improve from -0.43072. Patience: 6/50
2024-12-14 21:30:43.794173: train_loss -0.6462
2024-12-14 21:30:43.795061: val_loss -0.3977
2024-12-14 21:30:43.795789: Pseudo dice [0.6841]
2024-12-14 21:30:43.796607: Epoch time: 491.63 s
2024-12-14 21:30:44.279135: Yayy! New best EMA pseudo Dice: 0.6643
2024-12-14 21:30:46.074958: 
2024-12-14 21:30:46.076152: Epoch 30
2024-12-14 21:30:46.076914: Current learning rate: 0.00818
2024-12-14 21:38:46.720232: Validation loss did not improve from -0.43072. Patience: 7/50
2024-12-14 21:38:46.721268: train_loss -0.6515
2024-12-14 21:38:46.722350: val_loss -0.3676
2024-12-14 21:38:46.723020: Pseudo dice [0.6816]
2024-12-14 21:38:46.723700: Epoch time: 480.65 s
2024-12-14 21:38:46.724427: Yayy! New best EMA pseudo Dice: 0.666
2024-12-14 21:38:48.532168: 
2024-12-14 21:38:48.533588: Epoch 31
2024-12-14 21:38:48.534470: Current learning rate: 0.00812
2024-12-14 21:46:55.649367: Validation loss did not improve from -0.43072. Patience: 8/50
2024-12-14 21:46:55.678478: train_loss -0.65
2024-12-14 21:46:55.679532: val_loss -0.3982
2024-12-14 21:46:55.680226: Pseudo dice [0.6884]
2024-12-14 21:46:55.680924: Epoch time: 487.15 s
2024-12-14 21:46:55.681561: Yayy! New best EMA pseudo Dice: 0.6683
2024-12-14 21:46:57.541052: 
2024-12-14 21:46:57.542472: Epoch 32
2024-12-14 21:46:57.543209: Current learning rate: 0.00806
2024-12-14 21:55:11.625209: Validation loss did not improve from -0.43072. Patience: 9/50
2024-12-14 21:55:11.626523: train_loss -0.657
2024-12-14 21:55:11.627514: val_loss -0.3924
2024-12-14 21:55:11.628323: Pseudo dice [0.687]
2024-12-14 21:55:11.629197: Epoch time: 494.09 s
2024-12-14 21:55:11.630019: Yayy! New best EMA pseudo Dice: 0.6701
2024-12-14 21:55:13.495254: 
2024-12-14 21:55:13.496543: Epoch 33
2024-12-14 21:55:13.497322: Current learning rate: 0.008
2024-12-14 22:03:40.313252: Validation loss did not improve from -0.43072. Patience: 10/50
2024-12-14 22:03:40.314392: train_loss -0.6556
2024-12-14 22:03:40.315316: val_loss -0.4256
2024-12-14 22:03:40.316105: Pseudo dice [0.7016]
2024-12-14 22:03:40.316952: Epoch time: 506.82 s
2024-12-14 22:03:40.317756: Yayy! New best EMA pseudo Dice: 0.6733
2024-12-14 22:03:42.113135: 
2024-12-14 22:03:42.114525: Epoch 34
2024-12-14 22:03:42.115470: Current learning rate: 0.00793
2024-12-14 22:11:45.923218: Validation loss did not improve from -0.43072. Patience: 11/50
2024-12-14 22:11:45.924562: train_loss -0.664
2024-12-14 22:11:45.925657: val_loss -0.4009
2024-12-14 22:11:45.926299: Pseudo dice [0.6913]
2024-12-14 22:11:45.926955: Epoch time: 483.81 s
2024-12-14 22:11:46.318730: Yayy! New best EMA pseudo Dice: 0.6751
2024-12-14 22:11:48.104315: 
2024-12-14 22:11:48.105584: Epoch 35
2024-12-14 22:11:48.106390: Current learning rate: 0.00787
2024-12-14 22:19:49.302340: Validation loss did not improve from -0.43072. Patience: 12/50
2024-12-14 22:19:49.303351: train_loss -0.6646
2024-12-14 22:19:49.304136: val_loss -0.4249
2024-12-14 22:19:49.304771: Pseudo dice [0.6978]
2024-12-14 22:19:49.305552: Epoch time: 481.2 s
2024-12-14 22:19:49.306221: Yayy! New best EMA pseudo Dice: 0.6774
2024-12-14 22:19:51.132775: 
2024-12-14 22:19:51.134085: Epoch 36
2024-12-14 22:19:51.134781: Current learning rate: 0.00781
2024-12-14 22:27:59.244221: Validation loss did not improve from -0.43072. Patience: 13/50
2024-12-14 22:27:59.245159: train_loss -0.6725
2024-12-14 22:27:59.246286: val_loss -0.34
2024-12-14 22:27:59.247236: Pseudo dice [0.6711]
2024-12-14 22:27:59.248043: Epoch time: 488.11 s
2024-12-14 22:28:00.642567: 
2024-12-14 22:28:00.644091: Epoch 37
2024-12-14 22:28:00.644979: Current learning rate: 0.00775
2024-12-14 22:35:48.721308: Validation loss did not improve from -0.43072. Patience: 14/50
2024-12-14 22:35:48.722399: train_loss -0.6719
2024-12-14 22:35:48.723163: val_loss -0.3861
2024-12-14 22:35:48.723830: Pseudo dice [0.6857]
2024-12-14 22:35:48.724613: Epoch time: 468.08 s
2024-12-14 22:35:48.725402: Yayy! New best EMA pseudo Dice: 0.6776
2024-12-14 22:35:50.513341: 
2024-12-14 22:35:50.514807: Epoch 38
2024-12-14 22:35:50.515669: Current learning rate: 0.00769
2024-12-14 22:43:58.169780: Validation loss did not improve from -0.43072. Patience: 15/50
2024-12-14 22:43:58.170867: train_loss -0.6747
2024-12-14 22:43:58.171839: val_loss -0.3926
2024-12-14 22:43:58.172614: Pseudo dice [0.6867]
2024-12-14 22:43:58.173424: Epoch time: 487.66 s
2024-12-14 22:43:58.174232: Yayy! New best EMA pseudo Dice: 0.6785
2024-12-14 22:43:59.967143: 
2024-12-14 22:43:59.968554: Epoch 39
2024-12-14 22:43:59.969726: Current learning rate: 0.00763
2024-12-14 22:52:10.905089: Validation loss did not improve from -0.43072. Patience: 16/50
2024-12-14 22:52:10.908194: train_loss -0.675
2024-12-14 22:52:10.909083: val_loss -0.4144
2024-12-14 22:52:10.909802: Pseudo dice [0.6924]
2024-12-14 22:52:10.910477: Epoch time: 490.94 s
2024-12-14 22:52:11.280302: Yayy! New best EMA pseudo Dice: 0.6799
2024-12-14 22:52:13.588962: 
2024-12-14 22:52:13.590318: Epoch 40
2024-12-14 22:52:13.591191: Current learning rate: 0.00756
2024-12-14 23:00:31.006021: Validation loss did not improve from -0.43072. Patience: 17/50
2024-12-14 23:00:31.006962: train_loss -0.6855
2024-12-14 23:00:31.007749: val_loss -0.4187
2024-12-14 23:00:31.008502: Pseudo dice [0.6967]
2024-12-14 23:00:31.009121: Epoch time: 497.42 s
2024-12-14 23:00:31.009804: Yayy! New best EMA pseudo Dice: 0.6816
2024-12-14 23:00:32.843165: 
2024-12-14 23:00:32.844352: Epoch 41
2024-12-14 23:00:32.845006: Current learning rate: 0.0075
2024-12-14 23:08:48.694607: Validation loss did not improve from -0.43072. Patience: 18/50
2024-12-14 23:08:48.695690: train_loss -0.6835
2024-12-14 23:08:48.696687: val_loss -0.4197
2024-12-14 23:08:48.697412: Pseudo dice [0.7006]
2024-12-14 23:08:48.698335: Epoch time: 495.85 s
2024-12-14 23:08:48.699050: Yayy! New best EMA pseudo Dice: 0.6835
2024-12-14 23:08:50.452161: 
2024-12-14 23:08:50.453367: Epoch 42
2024-12-14 23:08:50.454314: Current learning rate: 0.00744
2024-12-14 23:17:04.428851: Validation loss did not improve from -0.43072. Patience: 19/50
2024-12-14 23:17:04.430413: train_loss -0.6897
2024-12-14 23:17:04.431517: val_loss -0.4051
2024-12-14 23:17:04.432336: Pseudo dice [0.6891]
2024-12-14 23:17:04.433028: Epoch time: 493.98 s
2024-12-14 23:17:04.433814: Yayy! New best EMA pseudo Dice: 0.684
2024-12-14 23:17:06.210855: 
2024-12-14 23:17:06.212409: Epoch 43
2024-12-14 23:17:06.213172: Current learning rate: 0.00738
2024-12-14 23:25:34.875673: Validation loss did not improve from -0.43072. Patience: 20/50
2024-12-14 23:25:34.876667: train_loss -0.6926
2024-12-14 23:25:34.877432: val_loss -0.395
2024-12-14 23:25:34.878136: Pseudo dice [0.6955]
2024-12-14 23:25:34.878907: Epoch time: 508.67 s
2024-12-14 23:25:34.879692: Yayy! New best EMA pseudo Dice: 0.6852
2024-12-14 23:25:36.668436: 
2024-12-14 23:25:36.669918: Epoch 44
2024-12-14 23:25:36.670831: Current learning rate: 0.00732
2024-12-14 23:34:13.254436: Validation loss improved from -0.43072 to -0.43573! Patience: 20/50
2024-12-14 23:34:13.255406: train_loss -0.6988
2024-12-14 23:34:13.256173: val_loss -0.4357
2024-12-14 23:34:13.256979: Pseudo dice [0.6989]
2024-12-14 23:34:13.257729: Epoch time: 516.59 s
2024-12-14 23:34:13.644302: Yayy! New best EMA pseudo Dice: 0.6866
2024-12-14 23:34:15.468184: 
2024-12-14 23:34:15.469321: Epoch 45
2024-12-14 23:34:15.470450: Current learning rate: 0.00725
2024-12-14 23:42:29.772600: Validation loss did not improve from -0.43573. Patience: 1/50
2024-12-14 23:42:29.773566: train_loss -0.698
2024-12-14 23:42:29.774268: val_loss -0.3864
2024-12-14 23:42:29.774932: Pseudo dice [0.6825]
2024-12-14 23:42:29.775582: Epoch time: 494.31 s
2024-12-14 23:42:31.110288: 
2024-12-14 23:42:31.111662: Epoch 46
2024-12-14 23:42:31.112938: Current learning rate: 0.00719
2024-12-14 23:50:59.037881: Validation loss did not improve from -0.43573. Patience: 2/50
2024-12-14 23:50:59.038922: train_loss -0.6997
2024-12-14 23:50:59.039815: val_loss -0.3777
2024-12-14 23:50:59.040589: Pseudo dice [0.6918]
2024-12-14 23:50:59.041477: Epoch time: 507.93 s
2024-12-14 23:50:59.042368: Yayy! New best EMA pseudo Dice: 0.6867
2024-12-14 23:51:00.766930: 
2024-12-14 23:51:00.768238: Epoch 47
2024-12-14 23:51:00.769099: Current learning rate: 0.00713
2024-12-14 23:59:34.718463: Validation loss improved from -0.43573 to -0.44614! Patience: 2/50
2024-12-14 23:59:34.721687: train_loss -0.7045
2024-12-14 23:59:34.722735: val_loss -0.4461
2024-12-14 23:59:34.723402: Pseudo dice [0.7099]
2024-12-14 23:59:34.724338: Epoch time: 513.96 s
2024-12-14 23:59:34.725224: Yayy! New best EMA pseudo Dice: 0.689
2024-12-14 23:59:36.474040: 
2024-12-14 23:59:36.475372: Epoch 48
2024-12-14 23:59:36.476258: Current learning rate: 0.00707
2024-12-15 00:07:58.838084: Validation loss did not improve from -0.44614. Patience: 1/50
2024-12-15 00:07:58.839121: train_loss -0.711
2024-12-15 00:07:58.839947: val_loss -0.4195
2024-12-15 00:07:58.840654: Pseudo dice [0.7035]
2024-12-15 00:07:58.841396: Epoch time: 502.37 s
2024-12-15 00:07:58.842143: Yayy! New best EMA pseudo Dice: 0.6905
2024-12-15 00:08:00.611148: 
2024-12-15 00:08:00.612412: Epoch 49
2024-12-15 00:08:00.613251: Current learning rate: 0.007
2024-12-15 00:16:10.310584: Validation loss did not improve from -0.44614. Patience: 2/50
2024-12-15 00:16:10.311573: train_loss -0.7086
2024-12-15 00:16:10.312297: val_loss -0.383
2024-12-15 00:16:10.313059: Pseudo dice [0.6774]
2024-12-15 00:16:10.313841: Epoch time: 489.7 s
2024-12-15 00:16:12.636982: 
2024-12-15 00:16:12.638375: Epoch 50
2024-12-15 00:16:12.639199: Current learning rate: 0.00694
2024-12-15 00:24:40.228953: Validation loss did not improve from -0.44614. Patience: 3/50
2024-12-15 00:24:40.230697: train_loss -0.7091
2024-12-15 00:24:40.232244: val_loss -0.357
2024-12-15 00:24:40.233060: Pseudo dice [0.6779]
2024-12-15 00:24:40.233879: Epoch time: 507.59 s
2024-12-15 00:24:41.694387: 
2024-12-15 00:24:41.695947: Epoch 51
2024-12-15 00:24:41.696769: Current learning rate: 0.00688
2024-12-15 00:33:15.321689: Validation loss did not improve from -0.44614. Patience: 4/50
2024-12-15 00:33:15.322967: train_loss -0.7148
2024-12-15 00:33:15.324056: val_loss -0.3832
2024-12-15 00:33:15.324836: Pseudo dice [0.6958]
2024-12-15 00:33:15.325724: Epoch time: 513.63 s
2024-12-15 00:33:16.718778: 
2024-12-15 00:33:16.720278: Epoch 52
2024-12-15 00:33:16.721076: Current learning rate: 0.00682
2024-12-15 00:41:36.536048: Validation loss did not improve from -0.44614. Patience: 5/50
2024-12-15 00:41:36.537375: train_loss -0.7125
2024-12-15 00:41:36.538380: val_loss -0.4168
2024-12-15 00:41:36.539214: Pseudo dice [0.7011]
2024-12-15 00:41:36.539889: Epoch time: 499.82 s
2024-12-15 00:41:37.912555: 
2024-12-15 00:41:37.914159: Epoch 53
2024-12-15 00:41:37.915143: Current learning rate: 0.00675
2024-12-15 00:49:47.871674: Validation loss did not improve from -0.44614. Patience: 6/50
2024-12-15 00:49:47.872702: train_loss -0.7139
2024-12-15 00:49:47.873520: val_loss -0.3688
2024-12-15 00:49:47.874380: Pseudo dice [0.69]
2024-12-15 00:49:47.875171: Epoch time: 489.96 s
2024-12-15 00:49:49.238589: 
2024-12-15 00:49:49.239892: Epoch 54
2024-12-15 00:49:49.240612: Current learning rate: 0.00669
2024-12-15 00:58:02.128213: Validation loss did not improve from -0.44614. Patience: 7/50
2024-12-15 00:58:02.129435: train_loss -0.7184
2024-12-15 00:58:02.130349: val_loss -0.4285
2024-12-15 00:58:02.131154: Pseudo dice [0.7118]
2024-12-15 00:58:02.132021: Epoch time: 492.89 s
2024-12-15 00:58:02.546454: Yayy! New best EMA pseudo Dice: 0.6922
2024-12-15 00:58:04.335736: 
2024-12-15 00:58:04.337194: Epoch 55
2024-12-15 00:58:04.338228: Current learning rate: 0.00663
2024-12-15 01:06:18.382587: Validation loss did not improve from -0.44614. Patience: 8/50
2024-12-15 01:06:18.385841: train_loss -0.7207
2024-12-15 01:06:18.386746: val_loss -0.4126
2024-12-15 01:06:18.387448: Pseudo dice [0.6991]
2024-12-15 01:06:18.388291: Epoch time: 494.05 s
2024-12-15 01:06:18.389183: Yayy! New best EMA pseudo Dice: 0.6929
2024-12-15 01:06:20.182554: 
2024-12-15 01:06:20.183890: Epoch 56
2024-12-15 01:06:20.184598: Current learning rate: 0.00657
2024-12-15 01:14:38.912732: Validation loss did not improve from -0.44614. Patience: 9/50
2024-12-15 01:14:38.913953: train_loss -0.7193
2024-12-15 01:14:38.915534: val_loss -0.4193
2024-12-15 01:14:38.916197: Pseudo dice [0.7047]
2024-12-15 01:14:38.917398: Epoch time: 498.73 s
2024-12-15 01:14:38.918151: Yayy! New best EMA pseudo Dice: 0.6941
2024-12-15 01:14:40.711721: 
2024-12-15 01:14:40.713143: Epoch 57
2024-12-15 01:14:40.714001: Current learning rate: 0.0065
2024-12-15 01:22:47.777528: Validation loss did not improve from -0.44614. Patience: 10/50
2024-12-15 01:22:47.778614: train_loss -0.7233
2024-12-15 01:22:47.779527: val_loss -0.4192
2024-12-15 01:22:47.780309: Pseudo dice [0.6977]
2024-12-15 01:22:47.781109: Epoch time: 487.07 s
2024-12-15 01:22:47.781811: Yayy! New best EMA pseudo Dice: 0.6945
2024-12-15 01:22:49.553357: 
2024-12-15 01:22:49.554747: Epoch 58
2024-12-15 01:22:49.555570: Current learning rate: 0.00644
2024-12-15 01:31:01.217557: Validation loss did not improve from -0.44614. Patience: 11/50
2024-12-15 01:31:01.218594: train_loss -0.7199
2024-12-15 01:31:01.219511: val_loss -0.4162
2024-12-15 01:31:01.220329: Pseudo dice [0.6866]
2024-12-15 01:31:01.221147: Epoch time: 491.67 s
2024-12-15 01:31:02.627317: 
2024-12-15 01:31:02.628515: Epoch 59
2024-12-15 01:31:02.629431: Current learning rate: 0.00638
2024-12-15 01:39:09.226443: Validation loss did not improve from -0.44614. Patience: 12/50
2024-12-15 01:39:09.227393: train_loss -0.7277
2024-12-15 01:39:09.228303: val_loss -0.4269
2024-12-15 01:39:09.228965: Pseudo dice [0.7105]
2024-12-15 01:39:09.229686: Epoch time: 486.6 s
2024-12-15 01:39:09.600410: Yayy! New best EMA pseudo Dice: 0.6954
2024-12-15 01:39:11.373768: 
2024-12-15 01:39:11.375285: Epoch 60
2024-12-15 01:39:11.376318: Current learning rate: 0.00631
2024-12-15 01:47:06.539861: Validation loss did not improve from -0.44614. Patience: 13/50
2024-12-15 01:47:06.540884: train_loss -0.7321
2024-12-15 01:47:06.541638: val_loss -0.435
2024-12-15 01:47:06.542448: Pseudo dice [0.7111]
2024-12-15 01:47:06.543135: Epoch time: 475.17 s
2024-12-15 01:47:06.543801: Yayy! New best EMA pseudo Dice: 0.6969
2024-12-15 01:47:08.904067: 
2024-12-15 01:47:08.905364: Epoch 61
2024-12-15 01:47:08.906213: Current learning rate: 0.00625
2024-12-15 01:55:10.748925: Validation loss did not improve from -0.44614. Patience: 14/50
2024-12-15 01:55:10.749971: train_loss -0.7388
2024-12-15 01:55:10.750751: val_loss -0.4011
2024-12-15 01:55:10.751492: Pseudo dice [0.7065]
2024-12-15 01:55:10.752238: Epoch time: 481.85 s
2024-12-15 01:55:10.752907: Yayy! New best EMA pseudo Dice: 0.6979
2024-12-15 01:55:12.563147: 
2024-12-15 01:55:12.564521: Epoch 62
2024-12-15 01:55:12.565508: Current learning rate: 0.00619
2024-12-15 02:03:22.759571: Validation loss did not improve from -0.44614. Patience: 15/50
2024-12-15 02:03:22.760657: train_loss -0.7392
2024-12-15 02:03:22.761550: val_loss -0.4086
2024-12-15 02:03:22.762334: Pseudo dice [0.6952]
2024-12-15 02:03:22.763037: Epoch time: 490.2 s
2024-12-15 02:03:24.173959: 
2024-12-15 02:03:24.175260: Epoch 63
2024-12-15 02:03:24.176050: Current learning rate: 0.00612
2024-12-15 02:11:39.205059: Validation loss did not improve from -0.44614. Patience: 16/50
2024-12-15 02:11:39.206630: train_loss -0.7371
2024-12-15 02:11:39.207458: val_loss -0.404
2024-12-15 02:11:39.208093: Pseudo dice [0.7048]
2024-12-15 02:11:39.208796: Epoch time: 495.03 s
2024-12-15 02:11:39.209377: Yayy! New best EMA pseudo Dice: 0.6983
2024-12-15 02:11:40.954951: 
2024-12-15 02:11:40.956261: Epoch 64
2024-12-15 02:11:40.957042: Current learning rate: 0.00606
2024-12-15 02:19:57.696162: Validation loss did not improve from -0.44614. Patience: 17/50
2024-12-15 02:19:57.697684: train_loss -0.7391
2024-12-15 02:19:57.699302: val_loss -0.4298
2024-12-15 02:19:57.700222: Pseudo dice [0.717]
2024-12-15 02:19:57.701291: Epoch time: 496.74 s
2024-12-15 02:19:58.082365: Yayy! New best EMA pseudo Dice: 0.7002
2024-12-15 02:19:59.896357: 
2024-12-15 02:19:59.897810: Epoch 65
2024-12-15 02:19:59.898745: Current learning rate: 0.006
2024-12-15 02:28:14.879242: Validation loss did not improve from -0.44614. Patience: 18/50
2024-12-15 02:28:14.880345: train_loss -0.7429
2024-12-15 02:28:14.881083: val_loss -0.4128
2024-12-15 02:28:14.881809: Pseudo dice [0.7085]
2024-12-15 02:28:14.882496: Epoch time: 494.99 s
2024-12-15 02:28:14.883302: Yayy! New best EMA pseudo Dice: 0.701
2024-12-15 02:28:16.734200: 
2024-12-15 02:28:16.735642: Epoch 66
2024-12-15 02:28:16.736491: Current learning rate: 0.00593
2024-12-15 02:36:37.712375: Validation loss did not improve from -0.44614. Patience: 19/50
2024-12-15 02:36:37.713360: train_loss -0.7371
2024-12-15 02:36:37.714315: val_loss -0.3681
2024-12-15 02:36:37.715294: Pseudo dice [0.6903]
2024-12-15 02:36:37.716217: Epoch time: 500.98 s
2024-12-15 02:36:39.109314: 
2024-12-15 02:36:39.110722: Epoch 67
2024-12-15 02:36:39.111659: Current learning rate: 0.00587
2024-12-15 02:44:18.665018: Validation loss did not improve from -0.44614. Patience: 20/50
2024-12-15 02:44:18.666065: train_loss -0.7428
2024-12-15 02:44:18.666910: val_loss -0.3725
2024-12-15 02:44:18.667627: Pseudo dice [0.684]
2024-12-15 02:44:18.668388: Epoch time: 459.56 s
2024-12-15 02:44:20.072547: 
2024-12-15 02:44:20.074071: Epoch 68
2024-12-15 02:44:20.075072: Current learning rate: 0.00581
2024-12-15 02:52:23.198042: Validation loss did not improve from -0.44614. Patience: 21/50
2024-12-15 02:52:23.199156: train_loss -0.7461
2024-12-15 02:52:23.199996: val_loss -0.3896
2024-12-15 02:52:23.200670: Pseudo dice [0.6958]
2024-12-15 02:52:23.201443: Epoch time: 483.13 s
2024-12-15 02:52:24.586867: 
2024-12-15 02:52:24.588367: Epoch 69
2024-12-15 02:52:24.589184: Current learning rate: 0.00574
2024-12-15 03:00:42.423533: Validation loss did not improve from -0.44614. Patience: 22/50
2024-12-15 03:00:42.424588: train_loss -0.7482
2024-12-15 03:00:42.425254: val_loss -0.4203
2024-12-15 03:00:42.426047: Pseudo dice [0.7027]
2024-12-15 03:00:42.426687: Epoch time: 497.84 s
2024-12-15 03:00:44.258818: 
2024-12-15 03:00:44.260082: Epoch 70
2024-12-15 03:00:44.260862: Current learning rate: 0.00568
2024-12-15 03:08:59.354656: Validation loss did not improve from -0.44614. Patience: 23/50
2024-12-15 03:08:59.355586: train_loss -0.7473
2024-12-15 03:08:59.356367: val_loss -0.416
2024-12-15 03:08:59.357077: Pseudo dice [0.7041]
2024-12-15 03:08:59.357897: Epoch time: 495.1 s
2024-12-15 03:09:00.771903: 
2024-12-15 03:09:00.773343: Epoch 71
2024-12-15 03:09:00.774152: Current learning rate: 0.00562
2024-12-15 03:16:58.106092: Validation loss did not improve from -0.44614. Patience: 24/50
2024-12-15 03:16:58.107699: train_loss -0.7502
2024-12-15 03:16:58.108892: val_loss -0.4035
2024-12-15 03:16:58.109678: Pseudo dice [0.7012]
2024-12-15 03:16:58.110355: Epoch time: 477.34 s
2024-12-15 03:16:59.499018: 
2024-12-15 03:16:59.500541: Epoch 72
2024-12-15 03:16:59.501360: Current learning rate: 0.00555
2024-12-15 03:25:12.719680: Validation loss did not improve from -0.44614. Patience: 25/50
2024-12-15 03:25:12.721216: train_loss -0.7524
2024-12-15 03:25:12.723391: val_loss -0.3876
2024-12-15 03:25:12.724218: Pseudo dice [0.6918]
2024-12-15 03:25:12.725205: Epoch time: 493.22 s
2024-12-15 03:25:14.137607: 
2024-12-15 03:25:14.139123: Epoch 73
2024-12-15 03:25:14.139918: Current learning rate: 0.00549
2024-12-15 03:33:44.100829: Validation loss did not improve from -0.44614. Patience: 26/50
2024-12-15 03:33:44.102110: train_loss -0.7534
2024-12-15 03:33:44.102889: val_loss -0.4203
2024-12-15 03:33:44.103628: Pseudo dice [0.7018]
2024-12-15 03:33:44.104342: Epoch time: 509.97 s
2024-12-15 03:33:45.510689: 
2024-12-15 03:33:45.512034: Epoch 74
2024-12-15 03:33:45.512872: Current learning rate: 0.00542
2024-12-15 03:42:14.220461: Validation loss did not improve from -0.44614. Patience: 27/50
2024-12-15 03:42:14.221647: train_loss -0.757
2024-12-15 03:42:14.222602: val_loss -0.4043
2024-12-15 03:42:14.223441: Pseudo dice [0.6968]
2024-12-15 03:42:14.224239: Epoch time: 508.71 s
2024-12-15 03:42:15.990065: 
2024-12-15 03:42:15.991363: Epoch 75
2024-12-15 03:42:15.992287: Current learning rate: 0.00536
2024-12-15 03:50:54.656769: Validation loss did not improve from -0.44614. Patience: 28/50
2024-12-15 03:50:54.657821: train_loss -0.7562
2024-12-15 03:50:54.658687: val_loss -0.3897
2024-12-15 03:50:54.659340: Pseudo dice [0.6965]
2024-12-15 03:50:54.660061: Epoch time: 518.67 s
2024-12-15 03:50:56.079244: 
2024-12-15 03:50:56.080594: Epoch 76
2024-12-15 03:50:56.081368: Current learning rate: 0.00529
2024-12-15 03:59:27.775330: Validation loss did not improve from -0.44614. Patience: 29/50
2024-12-15 03:59:27.776304: train_loss -0.7589
2024-12-15 03:59:27.777086: val_loss -0.3921
2024-12-15 03:59:27.777912: Pseudo dice [0.6995]
2024-12-15 03:59:27.778691: Epoch time: 511.7 s
2024-12-15 03:59:29.176749: 
2024-12-15 03:59:29.178046: Epoch 77
2024-12-15 03:59:29.178840: Current learning rate: 0.00523
2024-12-15 04:07:42.356480: Validation loss did not improve from -0.44614. Patience: 30/50
2024-12-15 04:07:42.357535: train_loss -0.7578
2024-12-15 04:07:42.358287: val_loss -0.3953
2024-12-15 04:07:42.358928: Pseudo dice [0.7005]
2024-12-15 04:07:42.359616: Epoch time: 493.18 s
2024-12-15 04:07:43.777942: 
2024-12-15 04:07:43.779406: Epoch 78
2024-12-15 04:07:43.780056: Current learning rate: 0.00517
2024-12-15 04:15:45.046283: Validation loss did not improve from -0.44614. Patience: 31/50
2024-12-15 04:15:45.047371: train_loss -0.7593
2024-12-15 04:15:45.048173: val_loss -0.4006
2024-12-15 04:15:45.048818: Pseudo dice [0.7058]
2024-12-15 04:15:45.049609: Epoch time: 481.27 s
2024-12-15 04:15:46.484031: 
2024-12-15 04:15:46.485429: Epoch 79
2024-12-15 04:15:46.486238: Current learning rate: 0.0051
2024-12-15 04:23:41.477640: Validation loss did not improve from -0.44614. Patience: 32/50
2024-12-15 04:23:41.478596: train_loss -0.7586
2024-12-15 04:23:41.479392: val_loss -0.4047
2024-12-15 04:23:41.480176: Pseudo dice [0.7121]
2024-12-15 04:23:41.480850: Epoch time: 475.0 s
2024-12-15 04:23:43.224559: 
2024-12-15 04:23:43.225908: Epoch 80
2024-12-15 04:23:43.226767: Current learning rate: 0.00504
2024-12-15 04:32:05.638483: Validation loss did not improve from -0.44614. Patience: 33/50
2024-12-15 04:32:05.640422: train_loss -0.7634
2024-12-15 04:32:05.642245: val_loss -0.4118
2024-12-15 04:32:05.643036: Pseudo dice [0.7092]
2024-12-15 04:32:05.643899: Epoch time: 502.42 s
2024-12-15 04:32:05.644684: Yayy! New best EMA pseudo Dice: 0.7016
2024-12-15 04:32:07.440123: 
2024-12-15 04:32:07.441337: Epoch 81
2024-12-15 04:32:07.442165: Current learning rate: 0.00497
2024-12-15 04:40:05.988379: Validation loss did not improve from -0.44614. Patience: 34/50
2024-12-15 04:40:05.989679: train_loss -0.7635
2024-12-15 04:40:05.990884: val_loss -0.3421
2024-12-15 04:40:05.991846: Pseudo dice [0.6782]
2024-12-15 04:40:05.992652: Epoch time: 478.55 s
2024-12-15 04:40:07.796011: 
2024-12-15 04:40:07.797579: Epoch 82
2024-12-15 04:40:07.798653: Current learning rate: 0.00491
2024-12-15 04:48:18.572480: Validation loss did not improve from -0.44614. Patience: 35/50
2024-12-15 04:48:18.573507: train_loss -0.7602
2024-12-15 04:48:18.574382: val_loss -0.4114
2024-12-15 04:48:18.575055: Pseudo dice [0.7132]
2024-12-15 04:48:18.575751: Epoch time: 490.78 s
2024-12-15 04:48:19.907247: 
2024-12-15 04:48:19.908871: Epoch 83
2024-12-15 04:48:19.909931: Current learning rate: 0.00484
2024-12-15 04:56:20.136185: Validation loss did not improve from -0.44614. Patience: 36/50
2024-12-15 04:56:20.137229: train_loss -0.7644
2024-12-15 04:56:20.138167: val_loss -0.3885
2024-12-15 04:56:20.139053: Pseudo dice [0.7072]
2024-12-15 04:56:20.139858: Epoch time: 480.23 s
2024-12-15 04:56:21.463923: 
2024-12-15 04:56:21.465259: Epoch 84
2024-12-15 04:56:21.466047: Current learning rate: 0.00478
2024-12-15 05:04:37.529177: Validation loss did not improve from -0.44614. Patience: 37/50
2024-12-15 05:04:37.530102: train_loss -0.7659
2024-12-15 05:04:37.531014: val_loss -0.4093
2024-12-15 05:04:37.531640: Pseudo dice [0.7053]
2024-12-15 05:04:37.532404: Epoch time: 496.07 s
2024-12-15 05:04:37.930916: Yayy! New best EMA pseudo Dice: 0.7017
2024-12-15 05:04:39.695218: 
2024-12-15 05:04:39.696436: Epoch 85
2024-12-15 05:04:39.697145: Current learning rate: 0.00471
2024-12-15 05:12:55.141861: Validation loss did not improve from -0.44614. Patience: 38/50
2024-12-15 05:12:55.142880: train_loss -0.773
2024-12-15 05:12:55.143743: val_loss -0.3614
2024-12-15 05:12:55.144420: Pseudo dice [0.6889]
2024-12-15 05:12:55.145212: Epoch time: 495.45 s
2024-12-15 05:12:56.482029: 
2024-12-15 05:12:56.483478: Epoch 86
2024-12-15 05:12:56.484378: Current learning rate: 0.00465
2024-12-15 05:21:14.309654: Validation loss did not improve from -0.44614. Patience: 39/50
2024-12-15 05:21:14.310612: train_loss -0.7674
2024-12-15 05:21:14.311540: val_loss -0.4114
2024-12-15 05:21:14.312259: Pseudo dice [0.7139]
2024-12-15 05:21:14.312942: Epoch time: 497.83 s
2024-12-15 05:21:14.313711: Yayy! New best EMA pseudo Dice: 0.7018
2024-12-15 05:21:16.124055: 
2024-12-15 05:21:16.125398: Epoch 87
2024-12-15 05:21:16.126332: Current learning rate: 0.00458
2024-12-15 05:29:39.050028: Validation loss did not improve from -0.44614. Patience: 40/50
2024-12-15 05:29:39.053660: train_loss -0.7744
2024-12-15 05:29:39.054844: val_loss -0.389
2024-12-15 05:29:39.056018: Pseudo dice [0.7005]
2024-12-15 05:29:39.056977: Epoch time: 502.93 s
2024-12-15 05:29:40.444400: 
2024-12-15 05:29:40.445604: Epoch 88
2024-12-15 05:29:40.446373: Current learning rate: 0.00452
2024-12-15 05:37:59.506153: Validation loss did not improve from -0.44614. Patience: 41/50
2024-12-15 05:37:59.507559: train_loss -0.7717
2024-12-15 05:37:59.508548: val_loss -0.3801
2024-12-15 05:37:59.509451: Pseudo dice [0.689]
2024-12-15 05:37:59.510289: Epoch time: 499.06 s
2024-12-15 05:38:00.867600: 
2024-12-15 05:38:00.868986: Epoch 89
2024-12-15 05:38:00.869970: Current learning rate: 0.00445
2024-12-15 05:46:12.255511: Validation loss did not improve from -0.44614. Patience: 42/50
2024-12-15 05:46:12.256774: train_loss -0.7709
2024-12-15 05:46:12.258071: val_loss -0.3759
2024-12-15 05:46:12.258948: Pseudo dice [0.6957]
2024-12-15 05:46:12.260033: Epoch time: 491.39 s
2024-12-15 05:46:13.964108: 
2024-12-15 05:46:13.965573: Epoch 90
2024-12-15 05:46:13.966474: Current learning rate: 0.00438
2024-12-15 05:54:13.413845: Validation loss did not improve from -0.44614. Patience: 43/50
2024-12-15 05:54:13.414904: train_loss -0.769
2024-12-15 05:54:13.415978: val_loss -0.3788
2024-12-15 05:54:13.416807: Pseudo dice [0.6979]
2024-12-15 05:54:13.417599: Epoch time: 479.45 s
2024-12-15 05:54:14.772976: 
2024-12-15 05:54:14.774334: Epoch 91
2024-12-15 05:54:14.775083: Current learning rate: 0.00432
2024-12-15 06:02:10.060505: Validation loss did not improve from -0.44614. Patience: 44/50
2024-12-15 06:02:10.061530: train_loss -0.7741
2024-12-15 06:02:10.062301: val_loss -0.4099
2024-12-15 06:02:10.062962: Pseudo dice [0.6986]
2024-12-15 06:02:10.063658: Epoch time: 475.29 s
2024-12-15 06:02:11.391877: 
2024-12-15 06:02:11.393277: Epoch 92
2024-12-15 06:02:11.394187: Current learning rate: 0.00425
2024-12-15 06:10:31.144518: Validation loss did not improve from -0.44614. Patience: 45/50
2024-12-15 06:10:31.145426: train_loss -0.7752
2024-12-15 06:10:31.146166: val_loss -0.3838
2024-12-15 06:10:31.146873: Pseudo dice [0.6989]
2024-12-15 06:10:31.147516: Epoch time: 499.75 s
2024-12-15 06:10:32.873355: 
2024-12-15 06:10:32.874501: Epoch 93
2024-12-15 06:10:32.875207: Current learning rate: 0.00419
2024-12-15 06:18:50.639367: Validation loss did not improve from -0.44614. Patience: 46/50
2024-12-15 06:18:50.640362: train_loss -0.776
2024-12-15 06:18:50.641094: val_loss -0.4161
2024-12-15 06:18:50.641783: Pseudo dice [0.7068]
2024-12-15 06:18:50.642704: Epoch time: 497.77 s
2024-12-15 06:18:52.021965: 
2024-12-15 06:18:52.023455: Epoch 94
2024-12-15 06:18:52.024277: Current learning rate: 0.00412
2024-12-15 06:27:11.570814: Validation loss did not improve from -0.44614. Patience: 47/50
2024-12-15 06:27:11.572993: train_loss -0.78
2024-12-15 06:27:11.574188: val_loss -0.3858
2024-12-15 06:27:11.575003: Pseudo dice [0.7049]
2024-12-15 06:27:11.575796: Epoch time: 499.55 s
2024-12-15 06:27:13.284703: 
2024-12-15 06:27:13.286041: Epoch 95
2024-12-15 06:27:13.287011: Current learning rate: 0.00405
2024-12-15 06:35:18.418036: Validation loss did not improve from -0.44614. Patience: 48/50
2024-12-15 06:35:18.419005: train_loss -0.7802
2024-12-15 06:35:18.419833: val_loss -0.4031
2024-12-15 06:35:18.420555: Pseudo dice [0.7018]
2024-12-15 06:35:18.421246: Epoch time: 485.14 s
2024-12-15 06:35:19.776882: 
2024-12-15 06:35:19.778255: Epoch 96
2024-12-15 06:35:19.779042: Current learning rate: 0.00399
2024-12-15 06:43:42.477956: Validation loss did not improve from -0.44614. Patience: 49/50
2024-12-15 06:43:42.479129: train_loss -0.7806
2024-12-15 06:43:42.479982: val_loss -0.4239
2024-12-15 06:43:42.480715: Pseudo dice [0.7044]
2024-12-15 06:43:42.481490: Epoch time: 502.7 s
2024-12-15 06:43:43.843030: 
2024-12-15 06:43:43.844346: Epoch 97
2024-12-15 06:43:43.845185: Current learning rate: 0.00392
2024-12-15 06:52:03.122687: Validation loss did not improve from -0.44614. Patience: 50/50
2024-12-15 06:52:03.124486: train_loss -0.7838
2024-12-15 06:52:03.125847: val_loss -0.4255
2024-12-15 06:52:03.126681: Pseudo dice [0.716]
2024-12-15 06:52:03.127488: Epoch time: 499.28 s
2024-12-15 06:52:03.128280: Yayy! New best EMA pseudo Dice: 0.7027
2024-12-15 06:52:04.913533: 
2024-12-15 06:52:04.914974: Epoch 98
2024-12-15 06:52:04.915980: Current learning rate: 0.00385
2024-12-15 07:00:32.224876: Validation loss did not improve from -0.44614. Patience: 51/50
2024-12-15 07:00:32.225875: train_loss -0.779
2024-12-15 07:00:32.226757: val_loss -0.4351
2024-12-15 07:00:32.227601: Pseudo dice [0.7158]
2024-12-15 07:00:32.228848: Epoch time: 507.31 s
2024-12-15 07:00:32.229864: Yayy! New best EMA pseudo Dice: 0.704
2024-12-15 07:00:34.030473: 
2024-12-15 07:00:34.032254: Epoch 99
2024-12-15 07:00:34.033223: Current learning rate: 0.00379
2024-12-15 07:08:43.261220: Validation loss did not improve from -0.44614. Patience: 52/50
2024-12-15 07:08:43.262182: train_loss -0.7823
2024-12-15 07:08:43.263033: val_loss -0.3967
2024-12-15 07:08:43.263782: Pseudo dice [0.6968]
2024-12-15 07:08:43.264557: Epoch time: 489.23 s
2024-12-15 07:08:45.025264: 
2024-12-15 07:08:45.026813: Epoch 100
2024-12-15 07:08:45.027558: Current learning rate: 0.00372
2024-12-15 07:17:00.297021: Validation loss did not improve from -0.44614. Patience: 53/50
2024-12-15 07:17:00.298065: train_loss -0.7826
2024-12-15 07:17:00.298962: val_loss -0.3414
2024-12-15 07:17:00.299756: Pseudo dice [0.6985]
2024-12-15 07:17:00.300568: Epoch time: 495.27 s
2024-12-15 07:17:01.655476: 
2024-12-15 07:17:01.656669: Epoch 101
2024-12-15 07:17:01.657426: Current learning rate: 0.00365
2024-12-15 07:25:09.961958: Validation loss did not improve from -0.44614. Patience: 54/50
2024-12-15 07:25:09.962923: train_loss -0.7849
2024-12-15 07:25:09.963936: val_loss -0.3757
2024-12-15 07:25:09.964792: Pseudo dice [0.6962]
2024-12-15 07:25:09.965703: Epoch time: 488.31 s
2024-12-15 07:25:11.356347: 
2024-12-15 07:25:11.357915: Epoch 102
2024-12-15 07:25:11.358925: Current learning rate: 0.00359
2024-12-15 07:33:18.239109: Validation loss did not improve from -0.44614. Patience: 55/50
2024-12-15 07:33:18.243441: train_loss -0.7857
2024-12-15 07:33:18.244516: val_loss -0.3967
2024-12-15 07:33:18.245200: Pseudo dice [0.7012]
2024-12-15 07:33:18.246115: Epoch time: 486.89 s
2024-12-15 07:33:20.077065: 
2024-12-15 07:33:20.078489: Epoch 103
2024-12-15 07:33:20.079433: Current learning rate: 0.00352
2024-12-15 07:41:22.957891: Validation loss did not improve from -0.44614. Patience: 56/50
2024-12-15 07:41:22.958927: train_loss -0.7897
2024-12-15 07:41:22.959742: val_loss -0.4085
2024-12-15 07:41:22.960373: Pseudo dice [0.7111]
2024-12-15 07:41:22.961096: Epoch time: 482.88 s
2024-12-15 07:41:24.360910: 
2024-12-15 07:41:24.362230: Epoch 104
2024-12-15 07:41:24.363043: Current learning rate: 0.00345
2024-12-15 07:49:02.825221: Validation loss did not improve from -0.44614. Patience: 57/50
2024-12-15 07:49:02.826514: train_loss -0.7872
2024-12-15 07:49:02.827446: val_loss -0.4045
2024-12-15 07:49:02.828184: Pseudo dice [0.7105]
2024-12-15 07:49:02.828913: Epoch time: 458.47 s
2024-12-15 07:49:04.574200: 
2024-12-15 07:49:04.575618: Epoch 105
2024-12-15 07:49:04.576618: Current learning rate: 0.00338
2024-12-15 07:57:05.718583: Validation loss did not improve from -0.44614. Patience: 58/50
2024-12-15 07:57:05.720118: train_loss -0.7859
2024-12-15 07:57:05.721208: val_loss -0.4083
2024-12-15 07:57:05.721938: Pseudo dice [0.7113]
2024-12-15 07:57:05.722660: Epoch time: 481.15 s
2024-12-15 07:57:05.723432: Yayy! New best EMA pseudo Dice: 0.7045
2024-12-15 07:57:07.526659: 
2024-12-15 07:57:07.528199: Epoch 106
2024-12-15 07:57:07.529266: Current learning rate: 0.00332
2024-12-15 08:05:22.128130: Validation loss did not improve from -0.44614. Patience: 59/50
2024-12-15 08:05:22.129127: train_loss -0.7901
2024-12-15 08:05:22.129855: val_loss -0.4047
2024-12-15 08:05:22.130600: Pseudo dice [0.6983]
2024-12-15 08:05:22.131236: Epoch time: 494.6 s
2024-12-15 08:05:23.518271: 
2024-12-15 08:05:23.519798: Epoch 107
2024-12-15 08:05:23.520814: Current learning rate: 0.00325
2024-12-15 08:13:37.843168: Validation loss did not improve from -0.44614. Patience: 60/50
2024-12-15 08:13:37.844197: train_loss -0.788
2024-12-15 08:13:37.845139: val_loss -0.3839
2024-12-15 08:13:37.846016: Pseudo dice [0.6989]
2024-12-15 08:13:37.846798: Epoch time: 494.33 s
2024-12-15 08:13:39.206202: 
2024-12-15 08:13:39.207549: Epoch 108
2024-12-15 08:13:39.208378: Current learning rate: 0.00318
2024-12-15 08:21:56.875191: Validation loss did not improve from -0.44614. Patience: 61/50
2024-12-15 08:21:56.876261: train_loss -0.7898
2024-12-15 08:21:56.877023: val_loss -0.3988
2024-12-15 08:21:56.877756: Pseudo dice [0.7038]
2024-12-15 08:21:56.878433: Epoch time: 497.67 s
2024-12-15 08:21:58.237212: 
2024-12-15 08:21:58.238438: Epoch 109
2024-12-15 08:21:58.239158: Current learning rate: 0.00311
2024-12-15 08:30:03.891215: Validation loss did not improve from -0.44614. Patience: 62/50
2024-12-15 08:30:03.892233: train_loss -0.7906
2024-12-15 08:30:03.892990: val_loss -0.4113
2024-12-15 08:30:03.893675: Pseudo dice [0.7085]
2024-12-15 08:30:03.894400: Epoch time: 485.66 s
2024-12-15 08:30:05.639626: 
2024-12-15 08:30:05.640992: Epoch 110
2024-12-15 08:30:05.641867: Current learning rate: 0.00304
2024-12-15 08:38:37.040561: Validation loss did not improve from -0.44614. Patience: 63/50
2024-12-15 08:38:37.043147: train_loss -0.7913
2024-12-15 08:38:37.043889: val_loss -0.3775
2024-12-15 08:38:37.044694: Pseudo dice [0.6942]
2024-12-15 08:38:37.045384: Epoch time: 511.4 s
2024-12-15 08:38:38.440068: 
2024-12-15 08:38:38.441139: Epoch 111
2024-12-15 08:38:38.441861: Current learning rate: 0.00297
2024-12-15 08:46:28.973194: Validation loss did not improve from -0.44614. Patience: 64/50
2024-12-15 08:46:28.974164: train_loss -0.7914
2024-12-15 08:46:28.974970: val_loss -0.4196
2024-12-15 08:46:28.975714: Pseudo dice [0.7129]
2024-12-15 08:46:28.976440: Epoch time: 470.54 s
2024-12-15 08:46:30.371990: 
2024-12-15 08:46:30.373174: Epoch 112
2024-12-15 08:46:30.373905: Current learning rate: 0.00291
2024-12-15 08:53:51.399053: Validation loss did not improve from -0.44614. Patience: 65/50
2024-12-15 08:53:51.423391: train_loss -0.7947
2024-12-15 08:53:51.425720: val_loss -0.3925
2024-12-15 08:53:51.426674: Pseudo dice [0.7]
2024-12-15 08:53:51.427644: Epoch time: 441.03 s
2024-12-15 08:53:52.933385: 
2024-12-15 08:53:52.934841: Epoch 113
2024-12-15 08:53:52.935567: Current learning rate: 0.00284
2024-12-15 09:00:38.664580: Validation loss did not improve from -0.44614. Patience: 66/50
2024-12-15 09:00:38.666020: train_loss -0.7959
2024-12-15 09:00:38.666845: val_loss -0.39
2024-12-15 09:00:38.667614: Pseudo dice [0.7092]
2024-12-15 09:00:38.668280: Epoch time: 405.73 s
2024-12-15 09:00:40.398732: 
2024-12-15 09:00:40.400011: Epoch 114
2024-12-15 09:00:40.400756: Current learning rate: 0.00277
2024-12-15 09:07:30.907333: Validation loss did not improve from -0.44614. Patience: 67/50
2024-12-15 09:07:30.908305: train_loss -0.7953
2024-12-15 09:07:30.909046: val_loss -0.4454
2024-12-15 09:07:30.909814: Pseudo dice [0.7268]
2024-12-15 09:07:30.910527: Epoch time: 410.51 s
2024-12-15 09:07:31.293870: Yayy! New best EMA pseudo Dice: 0.7064
2024-12-15 09:07:33.009448: 
2024-12-15 09:07:33.010811: Epoch 115
2024-12-15 09:07:33.011621: Current learning rate: 0.0027
2024-12-15 09:14:12.107706: Validation loss did not improve from -0.44614. Patience: 68/50
2024-12-15 09:14:12.108601: train_loss -0.7976
2024-12-15 09:14:12.109445: val_loss -0.4419
2024-12-15 09:14:12.110054: Pseudo dice [0.7155]
2024-12-15 09:14:12.110723: Epoch time: 399.1 s
2024-12-15 09:14:12.111447: Yayy! New best EMA pseudo Dice: 0.7073
2024-12-15 09:14:13.889437: 
2024-12-15 09:14:13.890821: Epoch 116
2024-12-15 09:14:13.891649: Current learning rate: 0.00263
2024-12-15 09:21:09.079819: Validation loss did not improve from -0.44614. Patience: 69/50
2024-12-15 09:21:09.080942: train_loss -0.795
2024-12-15 09:21:09.082006: val_loss -0.3861
2024-12-15 09:21:09.082905: Pseudo dice [0.6987]
2024-12-15 09:21:09.083752: Epoch time: 415.19 s
2024-12-15 09:21:10.478091: 
2024-12-15 09:21:10.479576: Epoch 117
2024-12-15 09:21:10.480381: Current learning rate: 0.00256
2024-12-15 09:27:51.967754: Validation loss did not improve from -0.44614. Patience: 70/50
2024-12-15 09:27:51.968749: train_loss -0.7975
2024-12-15 09:27:51.969777: val_loss -0.3917
2024-12-15 09:27:51.970777: Pseudo dice [0.7056]
2024-12-15 09:27:51.971871: Epoch time: 401.49 s
2024-12-15 09:27:53.362741: 
2024-12-15 09:27:53.364366: Epoch 118
2024-12-15 09:27:53.365413: Current learning rate: 0.00249
2024-12-15 09:34:44.244492: Validation loss did not improve from -0.44614. Patience: 71/50
2024-12-15 09:34:44.245439: train_loss -0.7948
2024-12-15 09:34:44.246339: val_loss -0.3918
2024-12-15 09:34:44.247359: Pseudo dice [0.6985]
2024-12-15 09:34:44.248286: Epoch time: 410.88 s
2024-12-15 09:34:45.719007: 
2024-12-15 09:34:45.720573: Epoch 119
2024-12-15 09:34:45.721586: Current learning rate: 0.00242
2024-12-15 09:41:57.520182: Validation loss did not improve from -0.44614. Patience: 72/50
2024-12-15 09:41:57.521239: train_loss -0.7973
2024-12-15 09:41:57.522417: val_loss -0.3792
2024-12-15 09:41:57.523457: Pseudo dice [0.7002]
2024-12-15 09:41:57.524439: Epoch time: 431.8 s
2024-12-15 09:41:59.434444: 
2024-12-15 09:41:59.435977: Epoch 120
2024-12-15 09:41:59.436952: Current learning rate: 0.00235
2024-12-15 09:49:00.037341: Validation loss did not improve from -0.44614. Patience: 73/50
2024-12-15 09:49:00.039934: train_loss -0.7968
2024-12-15 09:49:00.040738: val_loss -0.3793
2024-12-15 09:49:00.041413: Pseudo dice [0.6927]
2024-12-15 09:49:00.042182: Epoch time: 420.61 s
2024-12-15 09:49:01.436945: 
2024-12-15 09:49:01.438137: Epoch 121
2024-12-15 09:49:01.438859: Current learning rate: 0.00228
2024-12-15 09:56:06.791665: Validation loss did not improve from -0.44614. Patience: 74/50
2024-12-15 09:56:06.793120: train_loss -0.7994
2024-12-15 09:56:06.795000: val_loss -0.3843
2024-12-15 09:56:06.795778: Pseudo dice [0.7035]
2024-12-15 09:56:06.796746: Epoch time: 425.36 s
2024-12-15 09:56:08.228239: 
2024-12-15 09:56:08.229620: Epoch 122
2024-12-15 09:56:08.230397: Current learning rate: 0.00221
2024-12-15 10:02:58.654328: Validation loss did not improve from -0.44614. Patience: 75/50
2024-12-15 10:02:58.655622: train_loss -0.8012
2024-12-15 10:02:58.656398: val_loss -0.3662
2024-12-15 10:02:58.657096: Pseudo dice [0.6957]
2024-12-15 10:02:58.657872: Epoch time: 410.43 s
2024-12-15 10:03:00.125990: 
2024-12-15 10:03:00.127090: Epoch 123
2024-12-15 10:03:00.127951: Current learning rate: 0.00214
2024-12-15 10:09:24.838825: Validation loss did not improve from -0.44614. Patience: 76/50
2024-12-15 10:09:24.839830: train_loss -0.7997
2024-12-15 10:09:24.840619: val_loss -0.418
2024-12-15 10:09:24.841242: Pseudo dice [0.7138]
2024-12-15 10:09:24.841882: Epoch time: 384.72 s
2024-12-15 10:09:26.650825: 
2024-12-15 10:09:26.651637: Epoch 124
2024-12-15 10:09:26.652337: Current learning rate: 0.00207
2024-12-15 10:16:00.167640: Validation loss did not improve from -0.44614. Patience: 77/50
2024-12-15 10:16:00.168557: train_loss -0.8017
2024-12-15 10:16:00.169458: val_loss -0.3625
2024-12-15 10:16:00.170166: Pseudo dice [0.6948]
2024-12-15 10:16:00.170946: Epoch time: 393.52 s
2024-12-15 10:16:02.000574: 
2024-12-15 10:16:02.001876: Epoch 125
2024-12-15 10:16:02.002853: Current learning rate: 0.00199
2024-12-15 10:22:50.604249: Validation loss did not improve from -0.44614. Patience: 78/50
2024-12-15 10:22:50.605321: train_loss -0.8015
2024-12-15 10:22:50.606381: val_loss -0.4252
2024-12-15 10:22:50.607357: Pseudo dice [0.7217]
2024-12-15 10:22:50.608259: Epoch time: 408.61 s
2024-12-15 10:22:52.087839: 
2024-12-15 10:22:52.089406: Epoch 126
2024-12-15 10:22:52.090407: Current learning rate: 0.00192
2024-12-15 10:30:08.732550: Validation loss did not improve from -0.44614. Patience: 79/50
2024-12-15 10:30:08.733568: train_loss -0.8021
2024-12-15 10:30:08.734258: val_loss -0.4078
2024-12-15 10:30:08.735008: Pseudo dice [0.7176]
2024-12-15 10:30:08.735825: Epoch time: 436.65 s
2024-12-15 10:30:10.192975: 
2024-12-15 10:30:10.194684: Epoch 127
2024-12-15 10:30:10.195513: Current learning rate: 0.00185
2024-12-15 10:37:28.028716: Validation loss did not improve from -0.44614. Patience: 80/50
2024-12-15 10:37:28.029669: train_loss -0.8025
2024-12-15 10:37:28.030471: val_loss -0.3845
2024-12-15 10:37:28.031192: Pseudo dice [0.6992]
2024-12-15 10:37:28.032002: Epoch time: 437.84 s
2024-12-15 10:37:29.436668: 
2024-12-15 10:37:29.437933: Epoch 128
2024-12-15 10:37:29.438660: Current learning rate: 0.00178
2024-12-15 10:44:05.837933: Validation loss did not improve from -0.44614. Patience: 81/50
2024-12-15 10:44:05.839045: train_loss -0.8044
2024-12-15 10:44:05.839847: val_loss -0.3891
2024-12-15 10:44:05.840603: Pseudo dice [0.7082]
2024-12-15 10:44:05.841235: Epoch time: 396.4 s
2024-12-15 10:44:07.257384: 
2024-12-15 10:44:07.258510: Epoch 129
2024-12-15 10:44:07.259233: Current learning rate: 0.0017
2024-12-15 10:50:25.227645: Validation loss did not improve from -0.44614. Patience: 82/50
2024-12-15 10:50:25.228692: train_loss -0.8059
2024-12-15 10:50:25.229541: val_loss -0.3863
2024-12-15 10:50:25.230244: Pseudo dice [0.6948]
2024-12-15 10:50:25.231091: Epoch time: 377.97 s
2024-12-15 10:50:27.022964: 
2024-12-15 10:50:27.024101: Epoch 130
2024-12-15 10:50:27.024892: Current learning rate: 0.00163
2024-12-15 10:56:26.542728: Validation loss did not improve from -0.44614. Patience: 83/50
2024-12-15 10:56:26.543760: train_loss -0.8064
2024-12-15 10:56:26.544574: val_loss -0.3805
2024-12-15 10:56:26.545560: Pseudo dice [0.703]
2024-12-15 10:56:26.546404: Epoch time: 359.52 s
2024-12-15 10:56:27.925119: 
2024-12-15 10:56:27.926487: Epoch 131
2024-12-15 10:56:27.927387: Current learning rate: 0.00156
2024-12-15 11:03:23.505715: Validation loss did not improve from -0.44614. Patience: 84/50
2024-12-15 11:03:23.506902: train_loss -0.8044
2024-12-15 11:03:23.507766: val_loss -0.3871
2024-12-15 11:03:23.508583: Pseudo dice [0.6962]
2024-12-15 11:03:23.509465: Epoch time: 415.58 s
2024-12-15 11:03:24.886611: 
2024-12-15 11:03:24.887975: Epoch 132
2024-12-15 11:03:24.888841: Current learning rate: 0.00148
2024-12-15 11:10:20.658737: Validation loss did not improve from -0.44614. Patience: 85/50
2024-12-15 11:10:20.659806: train_loss -0.8059
2024-12-15 11:10:20.660686: val_loss -0.3881
2024-12-15 11:10:20.661394: Pseudo dice [0.7147]
2024-12-15 11:10:20.662230: Epoch time: 415.77 s
2024-12-15 11:10:22.030612: 
2024-12-15 11:10:22.031769: Epoch 133
2024-12-15 11:10:22.032462: Current learning rate: 0.00141
2024-12-15 11:17:24.563378: Validation loss did not improve from -0.44614. Patience: 86/50
2024-12-15 11:17:24.565414: train_loss -0.8053
2024-12-15 11:17:24.566314: val_loss -0.4146
2024-12-15 11:17:24.566951: Pseudo dice [0.7183]
2024-12-15 11:17:24.567634: Epoch time: 422.54 s
2024-12-15 11:17:25.961749: 
2024-12-15 11:17:25.963147: Epoch 134
2024-12-15 11:17:25.963827: Current learning rate: 0.00133
2024-12-15 11:24:09.958284: Validation loss did not improve from -0.44614. Patience: 87/50
2024-12-15 11:24:09.959267: train_loss -0.8031
2024-12-15 11:24:09.960267: val_loss -0.3661
2024-12-15 11:24:09.961022: Pseudo dice [0.6896]
2024-12-15 11:24:09.961926: Epoch time: 404.0 s
2024-12-15 11:24:12.132118: 
2024-12-15 11:24:12.133511: Epoch 135
2024-12-15 11:24:12.134467: Current learning rate: 0.00126
2024-12-15 11:30:54.657657: Validation loss did not improve from -0.44614. Patience: 88/50
2024-12-15 11:30:54.658362: train_loss -0.8083
2024-12-15 11:30:54.659372: val_loss -0.4234
2024-12-15 11:30:54.660221: Pseudo dice [0.7239]
2024-12-15 11:30:54.661045: Epoch time: 402.53 s
2024-12-15 11:30:56.084897: 
2024-12-15 11:30:56.086476: Epoch 136
2024-12-15 11:30:56.087673: Current learning rate: 0.00118
2024-12-15 11:37:17.954904: Validation loss did not improve from -0.44614. Patience: 89/50
2024-12-15 11:37:17.955968: train_loss -0.8052
2024-12-15 11:37:17.956791: val_loss -0.3966
2024-12-15 11:37:17.957457: Pseudo dice [0.7083]
2024-12-15 11:37:17.958160: Epoch time: 381.87 s
2024-12-15 11:37:19.538868: 
2024-12-15 11:37:19.540292: Epoch 137
2024-12-15 11:37:19.540979: Current learning rate: 0.00111
2024-12-15 11:42:38.718519: Validation loss did not improve from -0.44614. Patience: 90/50
2024-12-15 11:42:38.719710: train_loss -0.8076
2024-12-15 11:42:38.720972: val_loss -0.3995
2024-12-15 11:42:38.722072: Pseudo dice [0.7164]
2024-12-15 11:42:38.723190: Epoch time: 319.18 s
2024-12-15 11:42:38.724220: Yayy! New best EMA pseudo Dice: 0.7076
2024-12-15 11:42:40.570473: 
2024-12-15 11:42:40.571874: Epoch 138
2024-12-15 11:42:40.572799: Current learning rate: 0.00103
2024-12-15 11:48:36.232642: Validation loss did not improve from -0.44614. Patience: 91/50
2024-12-15 11:48:36.233613: train_loss -0.8089
2024-12-15 11:48:36.234566: val_loss -0.4034
2024-12-15 11:48:36.235255: Pseudo dice [0.7183]
2024-12-15 11:48:36.236158: Epoch time: 355.66 s
2024-12-15 11:48:36.237048: Yayy! New best EMA pseudo Dice: 0.7087
2024-12-15 11:48:38.108101: 
2024-12-15 11:48:38.109922: Epoch 139
2024-12-15 11:48:38.110651: Current learning rate: 0.00095
2024-12-15 11:52:15.329295: Validation loss did not improve from -0.44614. Patience: 92/50
2024-12-15 11:52:15.330350: train_loss -0.8093
2024-12-15 11:52:15.331139: val_loss -0.4166
2024-12-15 11:52:15.331829: Pseudo dice [0.7165]
2024-12-15 11:52:15.332528: Epoch time: 217.22 s
2024-12-15 11:52:15.754918: Yayy! New best EMA pseudo Dice: 0.7095
2024-12-15 11:52:17.629932: 
2024-12-15 11:52:17.631025: Epoch 140
2024-12-15 11:52:17.631801: Current learning rate: 0.00087
2024-12-15 11:57:43.179532: Validation loss did not improve from -0.44614. Patience: 93/50
2024-12-15 11:57:43.180639: train_loss -0.808
2024-12-15 11:57:43.181453: val_loss -0.3874
2024-12-15 11:57:43.182130: Pseudo dice [0.7016]
2024-12-15 11:57:43.183005: Epoch time: 325.55 s
2024-12-15 11:57:44.748545: 
2024-12-15 11:57:44.750008: Epoch 141
2024-12-15 11:57:44.750808: Current learning rate: 0.00079
2024-12-15 12:04:37.900749: Validation loss did not improve from -0.44614. Patience: 94/50
2024-12-15 12:04:37.902096: train_loss -0.8085
2024-12-15 12:04:37.903476: val_loss -0.4042
2024-12-15 12:04:37.904485: Pseudo dice [0.7051]
2024-12-15 12:04:37.905452: Epoch time: 413.15 s
2024-12-15 12:04:39.364520: 
2024-12-15 12:04:39.365935: Epoch 142
2024-12-15 12:04:39.366818: Current learning rate: 0.00071
2024-12-15 12:10:55.595510: Validation loss did not improve from -0.44614. Patience: 95/50
2024-12-15 12:10:55.597448: train_loss -0.8111
2024-12-15 12:10:55.598553: val_loss -0.396
2024-12-15 12:10:55.599280: Pseudo dice [0.7118]
2024-12-15 12:10:55.600059: Epoch time: 376.23 s
2024-12-15 12:10:57.062222: 
2024-12-15 12:10:57.063711: Epoch 143
2024-12-15 12:10:57.064507: Current learning rate: 0.00063
2024-12-15 12:17:44.672826: Validation loss did not improve from -0.44614. Patience: 96/50
2024-12-15 12:17:44.673926: train_loss -0.811
2024-12-15 12:17:44.675057: val_loss -0.3994
2024-12-15 12:17:44.675851: Pseudo dice [0.7088]
2024-12-15 12:17:44.676550: Epoch time: 407.61 s
2024-12-15 12:17:46.143989: 
2024-12-15 12:17:46.145264: Epoch 144
2024-12-15 12:17:46.146206: Current learning rate: 0.00055
2024-12-15 12:23:51.071185: Validation loss did not improve from -0.44614. Patience: 97/50
2024-12-15 12:23:51.072100: train_loss -0.8104
2024-12-15 12:23:51.073157: val_loss -0.415
2024-12-15 12:23:51.074056: Pseudo dice [0.7228]
2024-12-15 12:23:51.075007: Epoch time: 364.93 s
2024-12-15 12:23:51.503740: Yayy! New best EMA pseudo Dice: 0.7101
2024-12-15 12:23:54.000601: 
2024-12-15 12:23:54.001842: Epoch 145
2024-12-15 12:23:54.002766: Current learning rate: 0.00047
2024-12-15 12:30:28.131326: Validation loss did not improve from -0.44614. Patience: 98/50
2024-12-15 12:30:28.132313: train_loss -0.8089
2024-12-15 12:30:28.133190: val_loss -0.3617
2024-12-15 12:30:28.133852: Pseudo dice [0.7025]
2024-12-15 12:30:28.134687: Epoch time: 394.13 s
2024-12-15 12:30:29.593814: 
2024-12-15 12:30:29.595088: Epoch 146
2024-12-15 12:30:29.595903: Current learning rate: 0.00038
2024-12-15 12:36:36.081414: Validation loss did not improve from -0.44614. Patience: 99/50
2024-12-15 12:36:36.082423: train_loss -0.8107
2024-12-15 12:36:36.083215: val_loss -0.3615
2024-12-15 12:36:36.083918: Pseudo dice [0.6943]
2024-12-15 12:36:36.084669: Epoch time: 366.49 s
2024-12-15 12:36:37.558066: 
2024-12-15 12:36:37.559720: Epoch 147
2024-12-15 12:36:37.560816: Current learning rate: 0.0003
2024-12-15 12:42:50.447040: Validation loss did not improve from -0.44614. Patience: 100/50
2024-12-15 12:42:50.448008: train_loss -0.809
2024-12-15 12:42:50.448808: val_loss -0.3842
2024-12-15 12:42:50.449475: Pseudo dice [0.7052]
2024-12-15 12:42:50.450234: Epoch time: 372.89 s
2024-12-15 12:42:51.867840: 
2024-12-15 12:42:51.869142: Epoch 148
2024-12-15 12:42:51.869877: Current learning rate: 0.00021
2024-12-15 12:49:23.589251: Validation loss did not improve from -0.44614. Patience: 101/50
2024-12-15 12:49:23.590226: train_loss -0.8139
2024-12-15 12:49:23.591156: val_loss -0.3838
2024-12-15 12:49:23.591840: Pseudo dice [0.6879]
2024-12-15 12:49:23.592627: Epoch time: 391.72 s
2024-12-15 12:49:25.004418: 
2024-12-15 12:49:25.005742: Epoch 149
2024-12-15 12:49:25.006573: Current learning rate: 0.00011
2024-12-15 12:55:43.443496: Validation loss did not improve from -0.44614. Patience: 102/50
2024-12-15 12:55:43.444609: train_loss -0.8117
2024-12-15 12:55:43.445398: val_loss -0.3742
2024-12-15 12:55:43.446183: Pseudo dice [0.6963]
2024-12-15 12:55:43.446841: Epoch time: 378.44 s
2024-12-15 12:55:45.355843: Training done.
2024-12-15 12:55:45.474342: Using splits from existing split file: /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset307_Sohee_Calcium_OCT_CrossValidation/splits_final_60.json
2024-12-15 12:55:45.476032: The split file contains 5 splits.
2024-12-15 12:55:45.476758: Desired fold for training: 3
2024-12-15 12:55:45.477468: This split has 4 training and 5 validation cases.
2024-12-15 12:55:45.478346: predicting 101-045
2024-12-15 12:55:45.493742: 101-045, shape torch.Size([1, 375, 498, 498]), rank 0
2024-12-15 12:58:09.326293: predicting 106-002
2024-12-15 12:58:09.350216: 106-002, shape torch.Size([1, 539, 498, 498]), rank 0
2024-12-15 13:01:33.355596: predicting 401-004
2024-12-15 13:01:33.370594: 401-004, shape torch.Size([1, 375, 498, 498]), rank 0
2024-12-15 13:03:47.838292: predicting 704-003
2024-12-15 13:03:47.858398: 704-003, shape torch.Size([1, 375, 498, 498]), rank 0
2024-12-15 13:05:55.486431: predicting 706-005
2024-12-15 13:05:55.501105: 706-005, shape torch.Size([1, 375, 498, 498]), rank 0
2024-12-15 13:08:43.642601: Validation complete
2024-12-15 13:08:43.643398: Mean Validation Dice:  0.6848677266432646

############################
INFO: You are using the old nnU-Net default plans. We have updated our recommendations. Please consider using those instead! Read more here: https://github.com/MIC-DKFZ/nnUNet/blob/master/documentation/resenc_presets.md
############################

Using device: cuda:0

#######################################################################
Please cite the following paper when using nnU-Net:
Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.
#######################################################################

2024-12-15 13:08:50.418303: Using torch.compile...
/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
################### Loading pretrained weights from file  /home/gridsan/nchutisilp/projects/OpenAI-CLIP/clip_preivl_poststent_pretrained_nnUNet.pt ###################
Below is the list of overlapping blocks in pretrained model and nnUNet architecture:
encoder.stages.0.0.convs.0.conv.weight shape torch.Size([32, 1, 3, 3, 3])
encoder.stages.0.0.convs.0.conv.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.norm.weight shape torch.Size([32])
encoder.stages.0.0.convs.0.norm.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.0.weight shape torch.Size([32, 1, 3, 3, 3])
encoder.stages.0.0.convs.0.all_modules.0.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.1.weight shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.1.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
encoder.stages.0.0.convs.1.conv.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.norm.weight shape torch.Size([32])
encoder.stages.0.0.convs.1.norm.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
encoder.stages.0.0.convs.1.all_modules.0.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.1.weight shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.1.bias shape torch.Size([32])
encoder.stages.1.0.convs.0.conv.weight shape torch.Size([64, 32, 3, 3, 3])
encoder.stages.1.0.convs.0.conv.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.norm.weight shape torch.Size([64])
encoder.stages.1.0.convs.0.norm.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.0.weight shape torch.Size([64, 32, 3, 3, 3])
encoder.stages.1.0.convs.0.all_modules.0.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.1.weight shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.1.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
encoder.stages.1.0.convs.1.conv.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.norm.weight shape torch.Size([64])
encoder.stages.1.0.convs.1.norm.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
encoder.stages.1.0.convs.1.all_modules.0.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.1.weight shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.1.bias shape torch.Size([64])
encoder.stages.2.0.convs.0.conv.weight shape torch.Size([128, 64, 3, 3, 3])
encoder.stages.2.0.convs.0.conv.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.norm.weight shape torch.Size([128])
encoder.stages.2.0.convs.0.norm.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.0.weight shape torch.Size([128, 64, 3, 3, 3])
encoder.stages.2.0.convs.0.all_modules.0.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.1.weight shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.1.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
encoder.stages.2.0.convs.1.conv.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.norm.weight shape torch.Size([128])
encoder.stages.2.0.convs.1.norm.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
encoder.stages.2.0.convs.1.all_modules.0.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.1.weight shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.1.bias shape torch.Size([128])
encoder.stages.3.0.convs.0.conv.weight shape torch.Size([256, 128, 3, 3, 3])
encoder.stages.3.0.convs.0.conv.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.norm.weight shape torch.Size([256])
encoder.stages.3.0.convs.0.norm.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.0.weight shape torch.Size([256, 128, 3, 3, 3])
encoder.stages.3.0.convs.0.all_modules.0.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.1.weight shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.1.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
encoder.stages.3.0.convs.1.conv.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.norm.weight shape torch.Size([256])
encoder.stages.3.0.convs.1.norm.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
encoder.stages.3.0.convs.1.all_modules.0.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.1.weight shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.1.bias shape torch.Size([256])
encoder.stages.4.0.convs.0.conv.weight shape torch.Size([320, 256, 3, 3, 3])
encoder.stages.4.0.convs.0.conv.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.norm.weight shape torch.Size([320])
encoder.stages.4.0.convs.0.norm.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.0.weight shape torch.Size([320, 256, 3, 3, 3])
encoder.stages.4.0.convs.0.all_modules.0.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.1.weight shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.1.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.4.0.convs.1.conv.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.norm.weight shape torch.Size([320])
encoder.stages.4.0.convs.1.norm.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.4.0.convs.1.all_modules.0.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.1.weight shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.1.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.0.conv.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.norm.weight shape torch.Size([320])
encoder.stages.5.0.convs.0.norm.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.0.all_modules.0.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.1.weight shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.1.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.1.conv.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.norm.weight shape torch.Size([320])
encoder.stages.5.0.convs.1.norm.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.1.all_modules.0.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.1.weight shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.0.0.convs.0.conv.weight shape torch.Size([32, 1, 3, 3, 3])
decoder.encoder.stages.0.0.convs.0.conv.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.norm.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.norm.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.0.weight shape torch.Size([32, 1, 3, 3, 3])
decoder.encoder.stages.0.0.convs.0.all_modules.0.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.1.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.1.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.encoder.stages.0.0.convs.1.conv.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.norm.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.norm.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.encoder.stages.0.0.convs.1.all_modules.0.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.1.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.1.bias shape torch.Size([32])
decoder.encoder.stages.1.0.convs.0.conv.weight shape torch.Size([64, 32, 3, 3, 3])
decoder.encoder.stages.1.0.convs.0.conv.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.norm.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.norm.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.0.weight shape torch.Size([64, 32, 3, 3, 3])
decoder.encoder.stages.1.0.convs.0.all_modules.0.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.1.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.1.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.encoder.stages.1.0.convs.1.conv.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.norm.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.norm.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.encoder.stages.1.0.convs.1.all_modules.0.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.1.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.1.bias shape torch.Size([64])
decoder.encoder.stages.2.0.convs.0.conv.weight shape torch.Size([128, 64, 3, 3, 3])
decoder.encoder.stages.2.0.convs.0.conv.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.norm.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.norm.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.0.weight shape torch.Size([128, 64, 3, 3, 3])
decoder.encoder.stages.2.0.convs.0.all_modules.0.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.1.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.1.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.encoder.stages.2.0.convs.1.conv.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.norm.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.norm.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.encoder.stages.2.0.convs.1.all_modules.0.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.1.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.1.bias shape torch.Size([128])
decoder.encoder.stages.3.0.convs.0.conv.weight shape torch.Size([256, 128, 3, 3, 3])
decoder.encoder.stages.3.0.convs.0.conv.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.norm.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.norm.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.0.weight shape torch.Size([256, 128, 3, 3, 3])
decoder.encoder.stages.3.0.convs.0.all_modules.0.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.1.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.1.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.encoder.stages.3.0.convs.1.conv.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.norm.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.norm.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.encoder.stages.3.0.convs.1.all_modules.0.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.1.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.1.bias shape torch.Size([256])
decoder.encoder.stages.4.0.convs.0.conv.weight shape torch.Size([320, 256, 3, 3, 3])
decoder.encoder.stages.4.0.convs.0.conv.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.norm.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.norm.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.0.weight shape torch.Size([320, 256, 3, 3, 3])
decoder.encoder.stages.4.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.4.0.convs.1.conv.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.norm.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.norm.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.4.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.0.conv.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.norm.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.norm.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.1.conv.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.norm.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.norm.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.stages.0.convs.0.conv.weight shape torch.Size([320, 640, 3, 3, 3])
decoder.stages.0.convs.0.conv.bias shape torch.Size([320])
decoder.stages.0.convs.0.norm.weight shape torch.Size([320])
decoder.stages.0.convs.0.norm.bias shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.0.weight shape torch.Size([320, 640, 3, 3, 3])
decoder.stages.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.stages.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.stages.0.convs.1.conv.bias shape torch.Size([320])
decoder.stages.0.convs.1.norm.weight shape torch.Size([320])
decoder.stages.0.convs.1.norm.bias shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.stages.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.stages.1.convs.0.conv.weight shape torch.Size([256, 512, 3, 3, 3])
decoder.stages.1.convs.0.conv.bias shape torch.Size([256])
decoder.stages.1.convs.0.norm.weight shape torch.Size([256])
decoder.stages.1.convs.0.norm.bias shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.0.weight shape torch.Size([256, 512, 3, 3, 3])
decoder.stages.1.convs.0.all_modules.0.bias shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.1.weight shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.1.bias shape torch.Size([256])
decoder.stages.1.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.stages.1.convs.1.conv.bias shape torch.Size([256])
decoder.stages.1.convs.1.norm.weight shape torch.Size([256])
decoder.stages.1.convs.1.norm.bias shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.stages.1.convs.1.all_modules.0.bias shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.1.weight shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.1.bias shape torch.Size([256])
decoder.stages.2.convs.0.conv.weight shape torch.Size([128, 256, 3, 3, 3])
decoder.stages.2.convs.0.conv.bias shape torch.Size([128])
decoder.stages.2.convs.0.norm.weight shape torch.Size([128])
decoder.stages.2.convs.0.norm.bias shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.0.weight shape torch.Size([128, 256, 3, 3, 3])
decoder.stages.2.convs.0.all_modules.0.bias shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.1.weight shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.1.bias shape torch.Size([128])
decoder.stages.2.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.stages.2.convs.1.conv.bias shape torch.Size([128])
decoder.stages.2.convs.1.norm.weight shape torch.Size([128])
decoder.stages.2.convs.1.norm.bias shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.stages.2.convs.1.all_modules.0.bias shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.1.weight shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.1.bias shape torch.Size([128])
decoder.stages.3.convs.0.conv.weight shape torch.Size([64, 128, 3, 3, 3])
decoder.stages.3.convs.0.conv.bias shape torch.Size([64])
decoder.stages.3.convs.0.norm.weight shape torch.Size([64])
decoder.stages.3.convs.0.norm.bias shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.0.weight shape torch.Size([64, 128, 3, 3, 3])
decoder.stages.3.convs.0.all_modules.0.bias shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.1.weight shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.1.bias shape torch.Size([64])
decoder.stages.3.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.stages.3.convs.1.conv.bias shape torch.Size([64])
decoder.stages.3.convs.1.norm.weight shape torch.Size([64])
decoder.stages.3.convs.1.norm.bias shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.stages.3.convs.1.all_modules.0.bias shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.1.weight shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.1.bias shape torch.Size([64])
decoder.stages.4.convs.0.conv.weight shape torch.Size([32, 64, 3, 3, 3])
decoder.stages.4.convs.0.conv.bias shape torch.Size([32])
decoder.stages.4.convs.0.norm.weight shape torch.Size([32])
decoder.stages.4.convs.0.norm.bias shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.0.weight shape torch.Size([32, 64, 3, 3, 3])
decoder.stages.4.convs.0.all_modules.0.bias shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.1.weight shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.1.bias shape torch.Size([32])
decoder.stages.4.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.stages.4.convs.1.conv.bias shape torch.Size([32])
decoder.stages.4.convs.1.norm.weight shape torch.Size([32])
decoder.stages.4.convs.1.norm.bias shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.stages.4.convs.1.all_modules.0.bias shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.1.weight shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.1.bias shape torch.Size([32])
decoder.transpconvs.0.weight shape torch.Size([320, 320, 1, 2, 2])
decoder.transpconvs.0.bias shape torch.Size([320])
decoder.transpconvs.1.weight shape torch.Size([320, 256, 2, 2, 2])
decoder.transpconvs.1.bias shape torch.Size([256])
decoder.transpconvs.2.weight shape torch.Size([256, 128, 2, 2, 2])
decoder.transpconvs.2.bias shape torch.Size([128])
decoder.transpconvs.3.weight shape torch.Size([128, 64, 2, 2, 2])
decoder.transpconvs.3.bias shape torch.Size([64])
decoder.transpconvs.4.weight shape torch.Size([64, 32, 2, 2, 2])
decoder.transpconvs.4.bias shape torch.Size([32])
################### Done ###################
2024-12-15 13:09:03.984157: do_dummy_2d_data_aug: True
2024-12-15 13:09:03.985929: Using splits from existing split file: /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset307_Sohee_Calcium_OCT_CrossValidation/splits_final_60.json
2024-12-15 13:09:03.989252: The split file contains 5 splits.
2024-12-15 13:09:03.990707: Desired fold for training: 4
2024-12-15 13:09:03.992258: This split has 4 training and 4 validation cases.
using pin_memory on device 0
using pin_memory on device 0

This is the configuration used by this training:
Configuration name: 3d_32x160x128_b10
 {'data_identifier': 'nnUNetPlans_3d_fullres', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 10, 'patch_size': [32, 160, 128], 'median_image_size_in_voxels': [375.0, 498.0, 498.0], 'spacing': [1.0, 1.0, 1.0], 'normalization_schemes': ['ZScoreNormalization'], 'use_mask_for_norm': [False], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'architecture': {'network_class_name': 'dynamic_network_architectures.architectures.unet.PlainConvUNet', 'arch_kwargs': {'n_stages': 6, 'features_per_stage': [32, 64, 128, 256, 320, 320], 'conv_op': 'torch.nn.modules.conv.Conv3d', 'kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]], 'strides': [[1, 1, 1], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [1, 2, 2]], 'n_conv_per_stage': [2, 2, 2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2, 2, 2], 'conv_bias': True, 'norm_op': 'torch.nn.modules.instancenorm.InstanceNorm3d', 'norm_op_kwargs': {'eps': 1e-05, 'affine': True}, 'dropout_op': None, 'dropout_op_kwargs': None, 'nonlin': 'torch.nn.LeakyReLU', 'nonlin_kwargs': {'inplace': True}, 'deep_supervision': True}, '_kw_requires_import': ['conv_op', 'norm_op', 'dropout_op', 'nonlin']}, 'batch_dice': True, 'inherits_from': '3d_fullres'} 

These are the global plan.json settings:
 {'dataset_name': 'Dataset307_Sohee_Calcium_OCT_CrossValidation', 'plans_name': 'nnUNetPlans', 'original_median_spacing_after_transp': [1.0, 1.0, 1.0], 'original_median_shape_after_transp': [375, 498, 498], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 0.4977976381778717, 'mean': 0.13507525622844696, 'median': 0.09599608182907104, 'min': 0.0, 'percentile_00_5': 0.014754901640117168, 'percentile_99_5': 0.4977976381778717, 'std': 0.12025152146816254}}} 

2024-12-15 13:09:34.525420: unpacking dataset...
2024-12-15 13:09:38.733509: unpacking done...
2024-12-15 13:09:38.939000: Unable to plot network architecture: nnUNet_compile is enabled!
2024-12-15 13:09:39.065328: 
2024-12-15 13:09:39.066550: Epoch 0
2024-12-15 13:09:39.067562: Current learning rate: 0.01
2024-12-15 13:16:54.183697: Validation loss improved from 1000.00000 to -0.19965! Patience: 0/50
2024-12-15 13:16:54.185097: train_loss -0.1152
2024-12-15 13:16:54.186167: val_loss -0.1996
2024-12-15 13:16:54.187002: Pseudo dice [0.5513]
2024-12-15 13:16:54.187898: Epoch time: 435.12 s
2024-12-15 13:16:54.188654: Yayy! New best EMA pseudo Dice: 0.5513
2024-12-15 13:16:56.184024: 
2024-12-15 13:16:56.185464: Epoch 1
2024-12-15 13:16:56.186557: Current learning rate: 0.00994
2024-12-15 13:23:20.285174: Validation loss improved from -0.19965 to -0.29606! Patience: 0/50
2024-12-15 13:23:20.286086: train_loss -0.2608
2024-12-15 13:23:20.286825: val_loss -0.2961
2024-12-15 13:23:20.287570: Pseudo dice [0.6027]
2024-12-15 13:23:20.288347: Epoch time: 384.1 s
2024-12-15 13:23:20.288919: Yayy! New best EMA pseudo Dice: 0.5564
2024-12-15 13:23:21.988128: 
2024-12-15 13:23:21.989440: Epoch 2
2024-12-15 13:23:21.990190: Current learning rate: 0.00988
2024-12-15 13:30:18.120407: Validation loss did not improve from -0.29606. Patience: 1/50
2024-12-15 13:30:18.121453: train_loss -0.3136
2024-12-15 13:30:18.122325: val_loss -0.2742
2024-12-15 13:30:18.123187: Pseudo dice [0.5838]
2024-12-15 13:30:18.123890: Epoch time: 416.13 s
2024-12-15 13:30:18.124634: Yayy! New best EMA pseudo Dice: 0.5592
2024-12-15 13:30:19.913923: 
2024-12-15 13:30:19.915184: Epoch 3
2024-12-15 13:30:19.915991: Current learning rate: 0.00982
2024-12-15 13:36:39.794118: Validation loss improved from -0.29606 to -0.32203! Patience: 1/50
2024-12-15 13:36:39.795395: train_loss -0.366
2024-12-15 13:36:39.796604: val_loss -0.322
2024-12-15 13:36:39.797734: Pseudo dice [0.6148]
2024-12-15 13:36:39.798813: Epoch time: 379.88 s
2024-12-15 13:36:39.799577: Yayy! New best EMA pseudo Dice: 0.5647
2024-12-15 13:36:41.568792: 
2024-12-15 13:36:41.570117: Epoch 4
2024-12-15 13:36:41.571174: Current learning rate: 0.00976
2024-12-15 13:43:37.836783: Validation loss improved from -0.32203 to -0.37231! Patience: 0/50
2024-12-15 13:43:37.837839: train_loss -0.3958
2024-12-15 13:43:37.838542: val_loss -0.3723
2024-12-15 13:43:37.839308: Pseudo dice [0.6408]
2024-12-15 13:43:37.839955: Epoch time: 416.27 s
2024-12-15 13:43:38.226848: Yayy! New best EMA pseudo Dice: 0.5723
2024-12-15 13:43:39.968204: 
2024-12-15 13:43:39.969537: Epoch 5
2024-12-15 13:43:39.970285: Current learning rate: 0.0097
2024-12-15 13:50:07.787968: Validation loss improved from -0.37231 to -0.40691! Patience: 0/50
2024-12-15 13:50:07.788992: train_loss -0.4167
2024-12-15 13:50:07.789839: val_loss -0.4069
2024-12-15 13:50:07.790554: Pseudo dice [0.6518]
2024-12-15 13:50:07.791286: Epoch time: 387.82 s
2024-12-15 13:50:07.791949: Yayy! New best EMA pseudo Dice: 0.5803
2024-12-15 13:50:09.505242: 
2024-12-15 13:50:09.506665: Epoch 6
2024-12-15 13:50:09.507520: Current learning rate: 0.00964
2024-12-15 13:57:08.677860: Validation loss did not improve from -0.40691. Patience: 1/50
2024-12-15 13:57:08.678817: train_loss -0.4255
2024-12-15 13:57:08.679680: val_loss -0.3829
2024-12-15 13:57:08.680650: Pseudo dice [0.6393]
2024-12-15 13:57:08.681342: Epoch time: 419.17 s
2024-12-15 13:57:08.682200: Yayy! New best EMA pseudo Dice: 0.5862
2024-12-15 13:57:10.381808: 
2024-12-15 13:57:10.382988: Epoch 7
2024-12-15 13:57:10.383942: Current learning rate: 0.00958
2024-12-15 14:04:11.469260: Validation loss did not improve from -0.40691. Patience: 2/50
2024-12-15 14:04:11.470237: train_loss -0.4608
2024-12-15 14:04:11.471109: val_loss -0.4033
2024-12-15 14:04:11.471754: Pseudo dice [0.665]
2024-12-15 14:04:11.472614: Epoch time: 421.09 s
2024-12-15 14:04:11.473460: Yayy! New best EMA pseudo Dice: 0.5941
2024-12-15 14:04:13.231945: 
2024-12-15 14:04:13.233285: Epoch 8
2024-12-15 14:04:13.234262: Current learning rate: 0.00952
2024-12-15 14:11:29.620006: Validation loss improved from -0.40691 to -0.42211! Patience: 2/50
2024-12-15 14:11:29.620996: train_loss -0.4654
2024-12-15 14:11:29.622436: val_loss -0.4221
2024-12-15 14:11:29.623359: Pseudo dice [0.673]
2024-12-15 14:11:29.624469: Epoch time: 436.39 s
2024-12-15 14:11:29.625473: Yayy! New best EMA pseudo Dice: 0.602
2024-12-15 14:11:31.717618: 
2024-12-15 14:11:31.719054: Epoch 9
2024-12-15 14:11:31.719994: Current learning rate: 0.00946
2024-12-15 14:18:51.217351: Validation loss did not improve from -0.42211. Patience: 1/50
2024-12-15 14:18:51.219836: train_loss -0.484
2024-12-15 14:18:51.221620: val_loss -0.4061
2024-12-15 14:18:51.222575: Pseudo dice [0.6589]
2024-12-15 14:18:51.224001: Epoch time: 439.5 s
2024-12-15 14:18:51.624429: Yayy! New best EMA pseudo Dice: 0.6077
2024-12-15 14:18:53.319411: 
2024-12-15 14:18:53.320861: Epoch 10
2024-12-15 14:18:53.322148: Current learning rate: 0.0094
2024-12-15 14:26:05.686931: Validation loss improved from -0.42211 to -0.42750! Patience: 1/50
2024-12-15 14:26:05.687944: train_loss -0.49
2024-12-15 14:26:05.689230: val_loss -0.4275
2024-12-15 14:26:05.690235: Pseudo dice [0.6763]
2024-12-15 14:26:05.691132: Epoch time: 432.37 s
2024-12-15 14:26:05.691934: Yayy! New best EMA pseudo Dice: 0.6145
2024-12-15 14:26:07.419498: 
2024-12-15 14:26:07.420782: Epoch 11
2024-12-15 14:26:07.421705: Current learning rate: 0.00934
2024-12-15 14:33:09.749668: Validation loss did not improve from -0.42750. Patience: 1/50
2024-12-15 14:33:09.750694: train_loss -0.5011
2024-12-15 14:33:09.751411: val_loss -0.3834
2024-12-15 14:33:09.752187: Pseudo dice [0.6614]
2024-12-15 14:33:09.752885: Epoch time: 422.33 s
2024-12-15 14:33:09.753590: Yayy! New best EMA pseudo Dice: 0.6192
2024-12-15 14:33:11.438984: 
2024-12-15 14:33:11.440415: Epoch 12
2024-12-15 14:33:11.441500: Current learning rate: 0.00928
2024-12-15 14:40:30.058763: Validation loss did not improve from -0.42750. Patience: 2/50
2024-12-15 14:40:30.059666: train_loss -0.5034
2024-12-15 14:40:30.060505: val_loss -0.4138
2024-12-15 14:40:30.061186: Pseudo dice [0.6638]
2024-12-15 14:40:30.061777: Epoch time: 438.62 s
2024-12-15 14:40:30.062517: Yayy! New best EMA pseudo Dice: 0.6237
2024-12-15 14:40:31.818159: 
2024-12-15 14:40:31.819674: Epoch 13
2024-12-15 14:40:31.820399: Current learning rate: 0.00922
2024-12-15 14:47:40.435158: Validation loss did not improve from -0.42750. Patience: 3/50
2024-12-15 14:47:40.437433: train_loss -0.5231
2024-12-15 14:47:40.438343: val_loss -0.4123
2024-12-15 14:47:40.439269: Pseudo dice [0.6682]
2024-12-15 14:47:40.440230: Epoch time: 428.62 s
2024-12-15 14:47:40.440977: Yayy! New best EMA pseudo Dice: 0.6281
2024-12-15 14:47:42.197503: 
2024-12-15 14:47:42.198831: Epoch 14
2024-12-15 14:47:42.199533: Current learning rate: 0.00916
2024-12-15 14:54:12.525794: Validation loss improved from -0.42750 to -0.44251! Patience: 3/50
2024-12-15 14:54:12.527515: train_loss -0.5281
2024-12-15 14:54:12.529275: val_loss -0.4425
2024-12-15 14:54:12.529954: Pseudo dice [0.6884]
2024-12-15 14:54:12.530930: Epoch time: 390.33 s
2024-12-15 14:54:12.898165: Yayy! New best EMA pseudo Dice: 0.6341
2024-12-15 14:54:14.680382: 
2024-12-15 14:54:14.681649: Epoch 15
2024-12-15 14:54:14.682789: Current learning rate: 0.0091
2024-12-15 15:01:02.367821: Validation loss did not improve from -0.44251. Patience: 1/50
2024-12-15 15:01:02.368835: train_loss -0.5423
2024-12-15 15:01:02.369838: val_loss -0.4279
2024-12-15 15:01:02.370634: Pseudo dice [0.6843]
2024-12-15 15:01:02.371434: Epoch time: 407.69 s
2024-12-15 15:01:02.372102: Yayy! New best EMA pseudo Dice: 0.6392
2024-12-15 15:01:04.107583: 
2024-12-15 15:01:04.108994: Epoch 16
2024-12-15 15:01:04.109779: Current learning rate: 0.00903
2024-12-15 15:07:36.915226: Validation loss improved from -0.44251 to -0.47153! Patience: 1/50
2024-12-15 15:07:36.916215: train_loss -0.545
2024-12-15 15:07:36.916947: val_loss -0.4715
2024-12-15 15:07:36.917587: Pseudo dice [0.6994]
2024-12-15 15:07:36.918353: Epoch time: 392.81 s
2024-12-15 15:07:36.919049: Yayy! New best EMA pseudo Dice: 0.6452
2024-12-15 15:07:38.672985: 
2024-12-15 15:07:38.674449: Epoch 17
2024-12-15 15:07:38.675273: Current learning rate: 0.00897
2024-12-15 15:14:16.258465: Validation loss did not improve from -0.47153. Patience: 1/50
2024-12-15 15:14:16.260421: train_loss -0.5585
2024-12-15 15:14:16.261489: val_loss -0.4458
2024-12-15 15:14:16.262505: Pseudo dice [0.6874]
2024-12-15 15:14:16.263381: Epoch time: 397.59 s
2024-12-15 15:14:16.264133: Yayy! New best EMA pseudo Dice: 0.6494
2024-12-15 15:14:17.947841: 
2024-12-15 15:14:17.949482: Epoch 18
2024-12-15 15:14:17.950803: Current learning rate: 0.00891
2024-12-15 15:20:58.226483: Validation loss did not improve from -0.47153. Patience: 2/50
2024-12-15 15:20:58.227682: train_loss -0.5612
2024-12-15 15:20:58.228570: val_loss -0.4596
2024-12-15 15:20:58.229244: Pseudo dice [0.6974]
2024-12-15 15:20:58.229946: Epoch time: 400.28 s
2024-12-15 15:20:58.230676: Yayy! New best EMA pseudo Dice: 0.6542
2024-12-15 15:21:00.545226: 
2024-12-15 15:21:00.546566: Epoch 19
2024-12-15 15:21:00.547363: Current learning rate: 0.00885
2024-12-15 15:25:50.482256: Validation loss did not improve from -0.47153. Patience: 3/50
2024-12-15 15:25:50.485368: train_loss -0.5616
2024-12-15 15:25:50.486287: val_loss -0.4605
2024-12-15 15:25:50.486953: Pseudo dice [0.6916]
2024-12-15 15:25:50.487630: Epoch time: 289.94 s
2024-12-15 15:25:50.815420: Yayy! New best EMA pseudo Dice: 0.6579
2024-12-15 15:25:52.625490: 
2024-12-15 15:25:52.626790: Epoch 20
2024-12-15 15:25:52.627438: Current learning rate: 0.00879
2024-12-15 15:29:55.854179: Validation loss improved from -0.47153 to -0.47521! Patience: 3/50
2024-12-15 15:29:55.855259: train_loss -0.5685
2024-12-15 15:29:55.856226: val_loss -0.4752
2024-12-15 15:29:55.856815: Pseudo dice [0.7074]
2024-12-15 15:29:55.857413: Epoch time: 243.23 s
2024-12-15 15:29:55.857997: Yayy! New best EMA pseudo Dice: 0.6629
2024-12-15 15:29:57.650903: 
2024-12-15 15:29:57.652096: Epoch 21
2024-12-15 15:29:57.652833: Current learning rate: 0.00873
2024-12-15 15:34:24.402025: Validation loss improved from -0.47521 to -0.49260! Patience: 0/50
2024-12-15 15:34:24.402884: train_loss -0.5791
2024-12-15 15:34:24.403709: val_loss -0.4926
2024-12-15 15:34:24.404392: Pseudo dice [0.7158]
2024-12-15 15:34:24.405015: Epoch time: 266.75 s
2024-12-15 15:34:24.405659: Yayy! New best EMA pseudo Dice: 0.6682
2024-12-15 15:34:26.136678: 
2024-12-15 15:34:26.138211: Epoch 22
2024-12-15 15:34:26.138944: Current learning rate: 0.00867
2024-12-15 15:39:26.333661: Validation loss did not improve from -0.49260. Patience: 1/50
2024-12-15 15:39:26.334728: train_loss -0.5866
2024-12-15 15:39:26.335785: val_loss -0.49
2024-12-15 15:39:26.336701: Pseudo dice [0.7117]
2024-12-15 15:39:26.337456: Epoch time: 300.2 s
2024-12-15 15:39:26.338163: Yayy! New best EMA pseudo Dice: 0.6725
2024-12-15 15:39:28.036337: 
2024-12-15 15:39:28.037718: Epoch 23
2024-12-15 15:39:28.038510: Current learning rate: 0.00861
2024-12-15 15:44:31.148487: Validation loss did not improve from -0.49260. Patience: 2/50
2024-12-15 15:44:31.149349: train_loss -0.587
2024-12-15 15:44:31.150310: val_loss -0.4906
2024-12-15 15:44:31.151419: Pseudo dice [0.7124]
2024-12-15 15:44:31.152168: Epoch time: 303.11 s
2024-12-15 15:44:31.153176: Yayy! New best EMA pseudo Dice: 0.6765
2024-12-15 15:44:32.877379: 
2024-12-15 15:44:32.878755: Epoch 24
2024-12-15 15:44:32.879393: Current learning rate: 0.00855
2024-12-15 15:49:18.681481: Validation loss did not improve from -0.49260. Patience: 3/50
2024-12-15 15:49:18.682668: train_loss -0.5908
2024-12-15 15:49:18.683467: val_loss -0.4679
2024-12-15 15:49:18.684290: Pseudo dice [0.6956]
2024-12-15 15:49:18.684950: Epoch time: 285.81 s
2024-12-15 15:49:19.077717: Yayy! New best EMA pseudo Dice: 0.6784
2024-12-15 15:49:20.806432: 
2024-12-15 15:49:20.807719: Epoch 25
2024-12-15 15:49:20.808553: Current learning rate: 0.00849
2024-12-15 15:53:06.766416: Validation loss did not improve from -0.49260. Patience: 4/50
2024-12-15 15:53:06.768610: train_loss -0.5854
2024-12-15 15:53:06.771035: val_loss -0.4717
2024-12-15 15:53:06.772026: Pseudo dice [0.7068]
2024-12-15 15:53:06.773563: Epoch time: 225.96 s
2024-12-15 15:53:06.774567: Yayy! New best EMA pseudo Dice: 0.6813
2024-12-15 15:53:08.542226: 
2024-12-15 15:53:08.543576: Epoch 26
2024-12-15 15:53:08.544448: Current learning rate: 0.00843
2024-12-15 15:57:43.268469: Validation loss improved from -0.49260 to -0.50806! Patience: 4/50
2024-12-15 15:57:43.269140: train_loss -0.5944
2024-12-15 15:57:43.270006: val_loss -0.5081
2024-12-15 15:57:43.270748: Pseudo dice [0.7276]
2024-12-15 15:57:43.271429: Epoch time: 274.73 s
2024-12-15 15:57:43.272110: Yayy! New best EMA pseudo Dice: 0.6859
2024-12-15 15:57:45.006688: 
2024-12-15 15:57:45.007908: Epoch 27
2024-12-15 15:57:45.008736: Current learning rate: 0.00836
2024-12-15 16:02:17.545714: Validation loss did not improve from -0.50806. Patience: 1/50
2024-12-15 16:02:17.546668: train_loss -0.6075
2024-12-15 16:02:17.547633: val_loss -0.5
2024-12-15 16:02:17.548281: Pseudo dice [0.7251]
2024-12-15 16:02:17.548936: Epoch time: 272.54 s
2024-12-15 16:02:17.549604: Yayy! New best EMA pseudo Dice: 0.6898
2024-12-15 16:02:19.323474: 
2024-12-15 16:02:19.324810: Epoch 28
2024-12-15 16:02:19.325590: Current learning rate: 0.0083
2024-12-15 16:06:53.862054: Validation loss did not improve from -0.50806. Patience: 2/50
2024-12-15 16:06:53.863100: train_loss -0.599
2024-12-15 16:06:53.864002: val_loss -0.5076
2024-12-15 16:06:53.864795: Pseudo dice [0.7212]
2024-12-15 16:06:53.865659: Epoch time: 274.54 s
2024-12-15 16:06:53.866410: Yayy! New best EMA pseudo Dice: 0.693
2024-12-15 16:06:56.110274: 
2024-12-15 16:06:56.111860: Epoch 29
2024-12-15 16:06:56.113226: Current learning rate: 0.00824
2024-12-15 16:11:31.522418: Validation loss did not improve from -0.50806. Patience: 3/50
2024-12-15 16:11:31.523297: train_loss -0.6116
2024-12-15 16:11:31.524105: val_loss -0.4734
2024-12-15 16:11:31.525356: Pseudo dice [0.6956]
2024-12-15 16:11:31.526263: Epoch time: 275.41 s
2024-12-15 16:11:31.900310: Yayy! New best EMA pseudo Dice: 0.6932
2024-12-15 16:11:33.631187: 
2024-12-15 16:11:33.632537: Epoch 30
2024-12-15 16:11:33.633409: Current learning rate: 0.00818
2024-12-15 16:16:07.612796: Validation loss did not improve from -0.50806. Patience: 4/50
2024-12-15 16:16:07.614016: train_loss -0.6047
2024-12-15 16:16:07.614915: val_loss -0.4956
2024-12-15 16:16:07.615566: Pseudo dice [0.7165]
2024-12-15 16:16:07.616297: Epoch time: 273.98 s
2024-12-15 16:16:07.617028: Yayy! New best EMA pseudo Dice: 0.6955
2024-12-15 16:16:09.361034: 
2024-12-15 16:16:09.362574: Epoch 31
2024-12-15 16:16:09.363551: Current learning rate: 0.00812
2024-12-15 16:20:47.999434: Validation loss did not improve from -0.50806. Patience: 5/50
2024-12-15 16:20:48.000457: train_loss -0.6142
2024-12-15 16:20:48.001281: val_loss -0.4983
2024-12-15 16:20:48.001955: Pseudo dice [0.7196]
2024-12-15 16:20:48.002687: Epoch time: 278.64 s
2024-12-15 16:20:48.003296: Yayy! New best EMA pseudo Dice: 0.698
2024-12-15 16:20:49.755118: 
2024-12-15 16:20:49.756531: Epoch 32
2024-12-15 16:20:49.757214: Current learning rate: 0.00806
2024-12-15 16:25:32.024691: Validation loss did not improve from -0.50806. Patience: 6/50
2024-12-15 16:25:32.025828: train_loss -0.6201
2024-12-15 16:25:32.026551: val_loss -0.5054
2024-12-15 16:25:32.027233: Pseudo dice [0.7218]
2024-12-15 16:25:32.027959: Epoch time: 282.27 s
2024-12-15 16:25:32.028675: Yayy! New best EMA pseudo Dice: 0.7003
2024-12-15 16:25:33.777963: 
2024-12-15 16:25:33.779353: Epoch 33
2024-12-15 16:25:33.780125: Current learning rate: 0.008
2024-12-15 16:30:09.415762: Validation loss improved from -0.50806 to -0.51109! Patience: 6/50
2024-12-15 16:30:09.419154: train_loss -0.6166
2024-12-15 16:30:09.420139: val_loss -0.5111
2024-12-15 16:30:09.420924: Pseudo dice [0.7244]
2024-12-15 16:30:09.421747: Epoch time: 275.64 s
2024-12-15 16:30:09.422526: Yayy! New best EMA pseudo Dice: 0.7027
2024-12-15 16:30:11.166356: 
2024-12-15 16:30:11.168098: Epoch 34
2024-12-15 16:30:11.169372: Current learning rate: 0.00793
2024-12-15 16:34:48.627577: Validation loss did not improve from -0.51109. Patience: 1/50
2024-12-15 16:34:48.629257: train_loss -0.6098
2024-12-15 16:34:48.630288: val_loss -0.5024
2024-12-15 16:34:48.630985: Pseudo dice [0.7209]
2024-12-15 16:34:48.631649: Epoch time: 277.46 s
2024-12-15 16:34:49.005275: Yayy! New best EMA pseudo Dice: 0.7046
2024-12-15 16:34:50.744208: 
2024-12-15 16:34:50.745625: Epoch 35
2024-12-15 16:34:50.746302: Current learning rate: 0.00787
2024-12-15 16:39:27.734161: Validation loss did not improve from -0.51109. Patience: 2/50
2024-12-15 16:39:27.735055: train_loss -0.6265
2024-12-15 16:39:27.735978: val_loss -0.5095
2024-12-15 16:39:27.737155: Pseudo dice [0.7356]
2024-12-15 16:39:27.738173: Epoch time: 276.99 s
2024-12-15 16:39:27.739110: Yayy! New best EMA pseudo Dice: 0.7077
2024-12-15 16:39:29.481244: 
2024-12-15 16:39:29.482708: Epoch 36
2024-12-15 16:39:29.483603: Current learning rate: 0.00781
2024-12-15 16:44:11.186415: Validation loss did not improve from -0.51109. Patience: 3/50
2024-12-15 16:44:11.187088: train_loss -0.6261
2024-12-15 16:44:11.188334: val_loss -0.5
2024-12-15 16:44:11.189471: Pseudo dice [0.7269]
2024-12-15 16:44:11.190500: Epoch time: 281.71 s
2024-12-15 16:44:11.191620: Yayy! New best EMA pseudo Dice: 0.7096
2024-12-15 16:44:12.955652: 
2024-12-15 16:44:12.957296: Epoch 37
2024-12-15 16:44:12.958249: Current learning rate: 0.00775
2024-12-15 16:48:39.069568: Validation loss did not improve from -0.51109. Patience: 4/50
2024-12-15 16:48:39.070575: train_loss -0.6388
2024-12-15 16:48:39.071403: val_loss -0.5108
2024-12-15 16:48:39.072181: Pseudo dice [0.7308]
2024-12-15 16:48:39.072945: Epoch time: 266.12 s
2024-12-15 16:48:39.073660: Yayy! New best EMA pseudo Dice: 0.7117
2024-12-15 16:48:40.801338: 
2024-12-15 16:48:40.802567: Epoch 38
2024-12-15 16:48:40.803254: Current learning rate: 0.00769
2024-12-15 16:53:11.788054: Validation loss improved from -0.51109 to -0.51406! Patience: 4/50
2024-12-15 16:53:11.788904: train_loss -0.6458
2024-12-15 16:53:11.789731: val_loss -0.5141
2024-12-15 16:53:11.790683: Pseudo dice [0.7261]
2024-12-15 16:53:11.791379: Epoch time: 270.99 s
2024-12-15 16:53:11.792053: Yayy! New best EMA pseudo Dice: 0.7132
2024-12-15 16:53:13.625908: 
2024-12-15 16:53:13.627436: Epoch 39
2024-12-15 16:53:13.628266: Current learning rate: 0.00763
2024-12-15 16:57:46.796691: Validation loss did not improve from -0.51406. Patience: 1/50
2024-12-15 16:57:46.798781: train_loss -0.6452
2024-12-15 16:57:46.800857: val_loss -0.4742
2024-12-15 16:57:46.802097: Pseudo dice [0.6891]
2024-12-15 16:57:46.803531: Epoch time: 273.17 s
2024-12-15 16:57:49.102957: 
2024-12-15 16:57:49.104988: Epoch 40
2024-12-15 16:57:49.105830: Current learning rate: 0.00756
2024-12-15 17:02:19.409432: Validation loss improved from -0.51406 to -0.52110! Patience: 1/50
2024-12-15 17:02:19.410397: train_loss -0.6458
2024-12-15 17:02:19.411347: val_loss -0.5211
2024-12-15 17:02:19.412298: Pseudo dice [0.7291]
2024-12-15 17:02:19.413200: Epoch time: 270.31 s
2024-12-15 17:02:20.817318: 
2024-12-15 17:02:20.818758: Epoch 41
2024-12-15 17:02:20.819510: Current learning rate: 0.0075
2024-12-15 17:06:24.990534: Validation loss did not improve from -0.52110. Patience: 1/50
2024-12-15 17:06:24.991874: train_loss -0.6536
2024-12-15 17:06:24.992772: val_loss -0.505
2024-12-15 17:06:24.993471: Pseudo dice [0.7249]
2024-12-15 17:06:24.994411: Epoch time: 244.18 s
2024-12-15 17:06:24.995000: Yayy! New best EMA pseudo Dice: 0.7138
2024-12-15 17:06:26.666161: 
2024-12-15 17:06:26.667744: Epoch 42
2024-12-15 17:06:26.668375: Current learning rate: 0.00744
2024-12-15 17:11:08.782541: Validation loss did not improve from -0.52110. Patience: 2/50
2024-12-15 17:11:08.783525: train_loss -0.6566
2024-12-15 17:11:08.784548: val_loss -0.5086
2024-12-15 17:11:08.785262: Pseudo dice [0.7253]
2024-12-15 17:11:08.786071: Epoch time: 282.12 s
2024-12-15 17:11:08.786804: Yayy! New best EMA pseudo Dice: 0.715
2024-12-15 17:11:10.549792: 
2024-12-15 17:11:10.551092: Epoch 43
2024-12-15 17:11:10.551861: Current learning rate: 0.00738
2024-12-15 17:16:10.727047: Validation loss did not improve from -0.52110. Patience: 3/50
2024-12-15 17:16:10.727986: train_loss -0.6534
2024-12-15 17:16:10.728688: val_loss -0.4888
2024-12-15 17:16:10.729379: Pseudo dice [0.7062]
2024-12-15 17:16:10.730074: Epoch time: 300.18 s
2024-12-15 17:16:12.034092: 
2024-12-15 17:16:12.035346: Epoch 44
2024-12-15 17:16:12.035990: Current learning rate: 0.00732
2024-12-15 17:21:04.043097: Validation loss did not improve from -0.52110. Patience: 4/50
2024-12-15 17:21:04.043934: train_loss -0.6576
2024-12-15 17:21:04.045162: val_loss -0.5142
2024-12-15 17:21:04.045825: Pseudo dice [0.7157]
2024-12-15 17:21:04.046721: Epoch time: 292.01 s
2024-12-15 17:21:05.759901: 
2024-12-15 17:21:05.761114: Epoch 45
2024-12-15 17:21:05.761849: Current learning rate: 0.00725
2024-12-15 17:25:41.823686: Validation loss did not improve from -0.52110. Patience: 5/50
2024-12-15 17:25:41.824702: train_loss -0.6598
2024-12-15 17:25:41.825481: val_loss -0.5069
2024-12-15 17:25:41.826232: Pseudo dice [0.7211]
2024-12-15 17:25:41.827051: Epoch time: 276.07 s
2024-12-15 17:25:43.149617: 
2024-12-15 17:25:43.151131: Epoch 46
2024-12-15 17:25:43.152153: Current learning rate: 0.00719
2024-12-15 17:29:17.439714: Validation loss did not improve from -0.52110. Patience: 6/50
2024-12-15 17:29:17.440699: train_loss -0.6703
2024-12-15 17:29:17.441467: val_loss -0.4883
2024-12-15 17:29:17.442169: Pseudo dice [0.7102]
2024-12-15 17:29:17.442806: Epoch time: 214.29 s
2024-12-15 17:29:18.816443: 
2024-12-15 17:29:18.817770: Epoch 47
2024-12-15 17:29:18.818569: Current learning rate: 0.00713
2024-12-15 17:33:27.183696: Validation loss improved from -0.52110 to -0.53357! Patience: 6/50
2024-12-15 17:33:27.185281: train_loss -0.6607
2024-12-15 17:33:27.186547: val_loss -0.5336
2024-12-15 17:33:27.187427: Pseudo dice [0.7363]
2024-12-15 17:33:27.188425: Epoch time: 248.37 s
2024-12-15 17:33:27.189296: Yayy! New best EMA pseudo Dice: 0.7167
2024-12-15 17:33:28.971595: 
2024-12-15 17:33:28.973187: Epoch 48
2024-12-15 17:33:28.974213: Current learning rate: 0.00707
2024-12-15 17:38:02.591888: Validation loss did not improve from -0.53357. Patience: 1/50
2024-12-15 17:38:02.594815: train_loss -0.6742
2024-12-15 17:38:02.595752: val_loss -0.4807
2024-12-15 17:38:02.596558: Pseudo dice [0.7105]
2024-12-15 17:38:02.597321: Epoch time: 273.62 s
2024-12-15 17:38:03.982701: 
2024-12-15 17:38:03.983989: Epoch 49
2024-12-15 17:38:03.984721: Current learning rate: 0.007
2024-12-15 17:42:32.148088: Validation loss did not improve from -0.53357. Patience: 2/50
2024-12-15 17:42:32.149118: train_loss -0.6673
2024-12-15 17:42:32.149981: val_loss -0.5122
2024-12-15 17:42:32.150720: Pseudo dice [0.7268]
2024-12-15 17:42:32.151480: Epoch time: 268.17 s
2024-12-15 17:42:32.563386: Yayy! New best EMA pseudo Dice: 0.7171
2024-12-15 17:42:35.171531: 
2024-12-15 17:42:35.172762: Epoch 50
2024-12-15 17:42:35.173486: Current learning rate: 0.00694
2024-12-15 17:47:22.388798: Validation loss did not improve from -0.53357. Patience: 3/50
2024-12-15 17:47:22.389820: train_loss -0.6648
2024-12-15 17:47:22.390666: val_loss -0.5324
2024-12-15 17:47:22.391556: Pseudo dice [0.7371]
2024-12-15 17:47:22.392415: Epoch time: 287.22 s
2024-12-15 17:47:22.393188: Yayy! New best EMA pseudo Dice: 0.7191
2024-12-15 17:47:24.191322: 
2024-12-15 17:47:24.192749: Epoch 51
2024-12-15 17:47:24.193754: Current learning rate: 0.00688
2024-12-15 17:52:22.308252: Validation loss did not improve from -0.53357. Patience: 4/50
2024-12-15 17:52:22.309191: train_loss -0.6751
2024-12-15 17:52:22.310045: val_loss -0.4774
2024-12-15 17:52:22.310713: Pseudo dice [0.7151]
2024-12-15 17:52:22.311417: Epoch time: 298.12 s
2024-12-15 17:52:23.659827: 
2024-12-15 17:52:23.661225: Epoch 52
2024-12-15 17:52:23.661961: Current learning rate: 0.00682
2024-12-15 17:57:08.918473: Validation loss did not improve from -0.53357. Patience: 5/50
2024-12-15 17:57:08.919728: train_loss -0.6782
2024-12-15 17:57:08.920523: val_loss -0.5327
2024-12-15 17:57:08.921355: Pseudo dice [0.7402]
2024-12-15 17:57:08.922098: Epoch time: 285.26 s
2024-12-15 17:57:08.923000: Yayy! New best EMA pseudo Dice: 0.7209
2024-12-15 17:57:10.695949: 
2024-12-15 17:57:10.697147: Epoch 53
2024-12-15 17:57:10.697773: Current learning rate: 0.00675
2024-12-15 18:01:51.221778: Validation loss did not improve from -0.53357. Patience: 6/50
2024-12-15 18:01:51.222780: train_loss -0.6862
2024-12-15 18:01:51.223859: val_loss -0.529
2024-12-15 18:01:51.224955: Pseudo dice [0.7293]
2024-12-15 18:01:51.226037: Epoch time: 280.53 s
2024-12-15 18:01:51.227101: Yayy! New best EMA pseudo Dice: 0.7217
2024-12-15 18:01:52.964080: 
2024-12-15 18:01:52.965700: Epoch 54
2024-12-15 18:01:52.966897: Current learning rate: 0.00669
2024-12-15 18:06:32.626354: Validation loss did not improve from -0.53357. Patience: 7/50
2024-12-15 18:06:32.628806: train_loss -0.683
2024-12-15 18:06:32.630826: val_loss -0.5182
2024-12-15 18:06:32.631809: Pseudo dice [0.7317]
2024-12-15 18:06:32.633004: Epoch time: 279.67 s
2024-12-15 18:06:33.034663: Yayy! New best EMA pseudo Dice: 0.7227
2024-12-15 18:06:34.825516: 
2024-12-15 18:06:34.826744: Epoch 55
2024-12-15 18:06:34.827748: Current learning rate: 0.00663
2024-12-15 18:11:10.774944: Validation loss did not improve from -0.53357. Patience: 8/50
2024-12-15 18:11:10.775970: train_loss -0.6895
2024-12-15 18:11:10.776747: val_loss -0.5302
2024-12-15 18:11:10.777532: Pseudo dice [0.7429]
2024-12-15 18:11:10.778220: Epoch time: 275.95 s
2024-12-15 18:11:10.778869: Yayy! New best EMA pseudo Dice: 0.7247
2024-12-15 18:11:12.544030: 
2024-12-15 18:11:12.545408: Epoch 56
2024-12-15 18:11:12.546294: Current learning rate: 0.00657
2024-12-15 18:15:57.656007: Validation loss did not improve from -0.53357. Patience: 9/50
2024-12-15 18:15:57.657104: train_loss -0.685
2024-12-15 18:15:57.658033: val_loss -0.5272
2024-12-15 18:15:57.658921: Pseudo dice [0.7343]
2024-12-15 18:15:57.659759: Epoch time: 285.11 s
2024-12-15 18:15:57.660615: Yayy! New best EMA pseudo Dice: 0.7257
2024-12-15 18:15:59.436904: 
2024-12-15 18:15:59.438421: Epoch 57
2024-12-15 18:15:59.439526: Current learning rate: 0.0065
2024-12-15 18:20:36.851425: Validation loss did not improve from -0.53357. Patience: 10/50
2024-12-15 18:20:36.852482: train_loss -0.6904
2024-12-15 18:20:36.853237: val_loss -0.5121
2024-12-15 18:20:36.853875: Pseudo dice [0.7303]
2024-12-15 18:20:36.854608: Epoch time: 277.42 s
2024-12-15 18:20:36.855176: Yayy! New best EMA pseudo Dice: 0.7261
2024-12-15 18:20:38.644587: 
2024-12-15 18:20:38.645992: Epoch 58
2024-12-15 18:20:38.647444: Current learning rate: 0.00644
2024-12-15 18:25:18.903876: Validation loss improved from -0.53357 to -0.53384! Patience: 10/50
2024-12-15 18:25:18.904865: train_loss -0.6913
2024-12-15 18:25:18.905800: val_loss -0.5338
2024-12-15 18:25:18.906502: Pseudo dice [0.7381]
2024-12-15 18:25:18.907112: Epoch time: 280.26 s
2024-12-15 18:25:18.907699: Yayy! New best EMA pseudo Dice: 0.7273
2024-12-15 18:25:20.757377: 
2024-12-15 18:25:20.758891: Epoch 59
2024-12-15 18:25:20.759588: Current learning rate: 0.00638
2024-12-15 18:29:49.696380: Validation loss improved from -0.53384 to -0.55732! Patience: 0/50
2024-12-15 18:29:49.697250: train_loss -0.6947
2024-12-15 18:29:49.698126: val_loss -0.5573
2024-12-15 18:29:49.698910: Pseudo dice [0.7556]
2024-12-15 18:29:49.699653: Epoch time: 268.94 s
2024-12-15 18:29:50.109442: Yayy! New best EMA pseudo Dice: 0.7302
2024-12-15 18:29:51.863222: 
2024-12-15 18:29:51.864814: Epoch 60
2024-12-15 18:29:51.865995: Current learning rate: 0.00631
2024-12-15 18:34:24.732703: Validation loss did not improve from -0.55732. Patience: 1/50
2024-12-15 18:34:24.733738: train_loss -0.6892
2024-12-15 18:34:24.734621: val_loss -0.5283
2024-12-15 18:34:24.735392: Pseudo dice [0.737]
2024-12-15 18:34:24.736188: Epoch time: 272.87 s
2024-12-15 18:34:24.736908: Yayy! New best EMA pseudo Dice: 0.7308
2024-12-15 18:34:27.356641: 
2024-12-15 18:34:27.357958: Epoch 61
2024-12-15 18:34:27.358728: Current learning rate: 0.00625
2024-12-15 18:39:06.698769: Validation loss did not improve from -0.55732. Patience: 2/50
2024-12-15 18:39:06.700492: train_loss -0.6965
2024-12-15 18:39:06.701334: val_loss -0.5402
2024-12-15 18:39:06.701997: Pseudo dice [0.7457]
2024-12-15 18:39:06.702861: Epoch time: 279.34 s
2024-12-15 18:39:06.703614: Yayy! New best EMA pseudo Dice: 0.7323
2024-12-15 18:39:08.536571: 
2024-12-15 18:39:08.538266: Epoch 62
2024-12-15 18:39:08.539393: Current learning rate: 0.00619
2024-12-15 18:43:47.086676: Validation loss did not improve from -0.55732. Patience: 3/50
2024-12-15 18:43:47.089335: train_loss -0.6942
2024-12-15 18:43:47.090352: val_loss -0.5564
2024-12-15 18:43:47.091037: Pseudo dice [0.7566]
2024-12-15 18:43:47.091670: Epoch time: 278.55 s
2024-12-15 18:43:47.092406: Yayy! New best EMA pseudo Dice: 0.7348
2024-12-15 18:43:48.889762: 
2024-12-15 18:43:48.890808: Epoch 63
2024-12-15 18:43:48.891469: Current learning rate: 0.00612
2024-12-15 18:48:20.994327: Validation loss did not improve from -0.55732. Patience: 4/50
2024-12-15 18:48:20.995585: train_loss -0.6969
2024-12-15 18:48:20.996332: val_loss -0.5292
2024-12-15 18:48:20.996931: Pseudo dice [0.743]
2024-12-15 18:48:20.997630: Epoch time: 272.11 s
2024-12-15 18:48:20.998185: Yayy! New best EMA pseudo Dice: 0.7356
2024-12-15 18:48:22.825129: 
2024-12-15 18:48:22.826477: Epoch 64
2024-12-15 18:48:22.827248: Current learning rate: 0.00606
2024-12-15 18:52:18.399846: Validation loss did not improve from -0.55732. Patience: 5/50
2024-12-15 18:52:18.400920: train_loss -0.7007
2024-12-15 18:52:18.401687: val_loss -0.5297
2024-12-15 18:52:18.402322: Pseudo dice [0.7366]
2024-12-15 18:52:18.402964: Epoch time: 235.58 s
2024-12-15 18:52:18.823114: Yayy! New best EMA pseudo Dice: 0.7357
2024-12-15 18:52:20.611800: 
2024-12-15 18:52:20.612994: Epoch 65
2024-12-15 18:52:20.613724: Current learning rate: 0.006
2024-12-15 18:57:18.465470: Validation loss did not improve from -0.55732. Patience: 6/50
2024-12-15 18:57:18.466407: train_loss -0.7035
2024-12-15 18:57:18.467280: val_loss -0.5338
2024-12-15 18:57:18.467970: Pseudo dice [0.7403]
2024-12-15 18:57:18.468667: Epoch time: 297.86 s
2024-12-15 18:57:18.469391: Yayy! New best EMA pseudo Dice: 0.7361
2024-12-15 18:57:20.238428: 
2024-12-15 18:57:20.239436: Epoch 66
2024-12-15 18:57:20.240163: Current learning rate: 0.00593
2024-12-15 19:02:12.236821: Validation loss did not improve from -0.55732. Patience: 7/50
2024-12-15 19:02:12.237687: train_loss -0.7068
2024-12-15 19:02:12.238476: val_loss -0.5014
2024-12-15 19:02:12.239128: Pseudo dice [0.7257]
2024-12-15 19:02:12.239755: Epoch time: 292.0 s
2024-12-15 19:02:13.601763: 
2024-12-15 19:02:13.603333: Epoch 67
2024-12-15 19:02:13.604716: Current learning rate: 0.00587
2024-12-15 19:06:55.382465: Validation loss did not improve from -0.55732. Patience: 8/50
2024-12-15 19:06:55.383469: train_loss -0.7105
2024-12-15 19:06:55.384276: val_loss -0.5182
2024-12-15 19:06:55.385020: Pseudo dice [0.7311]
2024-12-15 19:06:55.385696: Epoch time: 281.78 s
2024-12-15 19:06:56.774483: 
2024-12-15 19:06:56.775994: Epoch 68
2024-12-15 19:06:56.777026: Current learning rate: 0.00581
2024-12-15 19:10:53.218845: Validation loss did not improve from -0.55732. Patience: 9/50
2024-12-15 19:10:53.224344: train_loss -0.7115
2024-12-15 19:10:53.225990: val_loss -0.5101
2024-12-15 19:10:53.226765: Pseudo dice [0.7282]
2024-12-15 19:10:53.227807: Epoch time: 236.45 s
2024-12-15 19:10:54.780942: 
2024-12-15 19:10:54.782260: Epoch 69
2024-12-15 19:10:54.783278: Current learning rate: 0.00574
2024-12-15 19:15:19.582970: Validation loss did not improve from -0.55732. Patience: 10/50
2024-12-15 19:15:19.584101: train_loss -0.7097
2024-12-15 19:15:19.584889: val_loss -0.5219
2024-12-15 19:15:19.585657: Pseudo dice [0.736]
2024-12-15 19:15:19.586416: Epoch time: 264.8 s
2024-12-15 19:15:21.396706: 
2024-12-15 19:15:21.397735: Epoch 70
2024-12-15 19:15:21.398528: Current learning rate: 0.00568
2024-12-15 19:19:51.091661: Validation loss did not improve from -0.55732. Patience: 11/50
2024-12-15 19:19:51.092610: train_loss -0.7126
2024-12-15 19:19:51.093548: val_loss -0.5287
2024-12-15 19:19:51.094204: Pseudo dice [0.7362]
2024-12-15 19:19:51.095063: Epoch time: 269.7 s
2024-12-15 19:19:52.480149: 
2024-12-15 19:19:52.481593: Epoch 71
2024-12-15 19:19:52.482378: Current learning rate: 0.00562
2024-12-15 19:24:29.730341: Validation loss did not improve from -0.55732. Patience: 12/50
2024-12-15 19:24:29.731321: train_loss -0.7102
2024-12-15 19:24:29.732134: val_loss -0.4994
2024-12-15 19:24:29.732782: Pseudo dice [0.724]
2024-12-15 19:24:29.733650: Epoch time: 277.25 s
2024-12-15 19:24:31.570323: 
2024-12-15 19:24:31.571651: Epoch 72
2024-12-15 19:24:31.572364: Current learning rate: 0.00555
2024-12-15 19:29:04.712490: Validation loss did not improve from -0.55732. Patience: 13/50
2024-12-15 19:29:04.713351: train_loss -0.7162
2024-12-15 19:29:04.714147: val_loss -0.5266
2024-12-15 19:29:04.714819: Pseudo dice [0.7332]
2024-12-15 19:29:04.715907: Epoch time: 273.14 s
2024-12-15 19:29:06.087361: 
2024-12-15 19:29:06.088936: Epoch 73
2024-12-15 19:29:06.089729: Current learning rate: 0.00549
2024-12-15 19:33:59.799386: Validation loss did not improve from -0.55732. Patience: 14/50
2024-12-15 19:33:59.800179: train_loss -0.7065
2024-12-15 19:33:59.800905: val_loss -0.5557
2024-12-15 19:33:59.801697: Pseudo dice [0.7508]
2024-12-15 19:33:59.802469: Epoch time: 293.71 s
2024-12-15 19:34:01.162021: 
2024-12-15 19:34:01.163442: Epoch 74
2024-12-15 19:34:01.164234: Current learning rate: 0.00542
2024-12-15 19:38:47.495984: Validation loss did not improve from -0.55732. Patience: 15/50
2024-12-15 19:38:47.496941: train_loss -0.7114
2024-12-15 19:38:47.497788: val_loss -0.5444
2024-12-15 19:38:47.498390: Pseudo dice [0.7439]
2024-12-15 19:38:47.499796: Epoch time: 286.34 s
2024-12-15 19:38:49.323268: 
2024-12-15 19:38:49.324550: Epoch 75
2024-12-15 19:38:49.325253: Current learning rate: 0.00536
2024-12-15 19:43:30.493897: Validation loss did not improve from -0.55732. Patience: 16/50
2024-12-15 19:43:30.495376: train_loss -0.7193
2024-12-15 19:43:30.496132: val_loss -0.5384
2024-12-15 19:43:30.496755: Pseudo dice [0.7471]
2024-12-15 19:43:30.497408: Epoch time: 281.17 s
2024-12-15 19:43:30.498167: Yayy! New best EMA pseudo Dice: 0.7371
2024-12-15 19:43:32.343452: 
2024-12-15 19:43:32.344838: Epoch 76
2024-12-15 19:43:32.345657: Current learning rate: 0.00529
2024-12-15 19:48:12.494742: Validation loss did not improve from -0.55732. Patience: 17/50
2024-12-15 19:48:12.497426: train_loss -0.7147
2024-12-15 19:48:12.498280: val_loss -0.4932
2024-12-15 19:48:12.498935: Pseudo dice [0.7254]
2024-12-15 19:48:12.499653: Epoch time: 280.15 s
2024-12-15 19:48:13.879339: 
2024-12-15 19:48:13.880768: Epoch 77
2024-12-15 19:48:13.881788: Current learning rate: 0.00523
2024-12-15 19:52:54.627524: Validation loss improved from -0.55732 to -0.56017! Patience: 17/50
2024-12-15 19:52:54.628577: train_loss -0.7225
2024-12-15 19:52:54.629315: val_loss -0.5602
2024-12-15 19:52:54.629977: Pseudo dice [0.757]
2024-12-15 19:52:54.630739: Epoch time: 280.75 s
2024-12-15 19:52:54.631405: Yayy! New best EMA pseudo Dice: 0.738
2024-12-15 19:52:56.456966: 
2024-12-15 19:52:56.458735: Epoch 78
2024-12-15 19:52:56.459469: Current learning rate: 0.00517
2024-12-15 19:57:49.965286: Validation loss did not improve from -0.56017. Patience: 1/50
2024-12-15 19:57:49.966033: train_loss -0.7268
2024-12-15 19:57:49.966777: val_loss -0.5439
2024-12-15 19:57:49.967422: Pseudo dice [0.7482]
2024-12-15 19:57:49.968111: Epoch time: 293.51 s
2024-12-15 19:57:49.968752: Yayy! New best EMA pseudo Dice: 0.7391
2024-12-15 19:57:51.756135: 
2024-12-15 19:57:51.758439: Epoch 79
2024-12-15 19:57:51.759345: Current learning rate: 0.0051
2024-12-15 20:02:22.358895: Validation loss did not improve from -0.56017. Patience: 2/50
2024-12-15 20:02:22.359826: train_loss -0.7221
2024-12-15 20:02:22.360693: val_loss -0.5518
2024-12-15 20:02:22.361584: Pseudo dice [0.748]
2024-12-15 20:02:22.362323: Epoch time: 270.6 s
2024-12-15 20:02:22.824898: Yayy! New best EMA pseudo Dice: 0.7399
2024-12-15 20:02:24.613278: 
2024-12-15 20:02:24.614990: Epoch 80
2024-12-15 20:02:24.615988: Current learning rate: 0.00504
2024-12-15 20:07:00.801772: Validation loss did not improve from -0.56017. Patience: 3/50
2024-12-15 20:07:00.802655: train_loss -0.7273
2024-12-15 20:07:00.803596: val_loss -0.5062
2024-12-15 20:07:00.804364: Pseudo dice [0.7352]
2024-12-15 20:07:00.805053: Epoch time: 276.19 s
2024-12-15 20:07:02.205992: 
2024-12-15 20:07:02.207758: Epoch 81
2024-12-15 20:07:02.208459: Current learning rate: 0.00497
2024-12-15 20:11:35.565790: Validation loss did not improve from -0.56017. Patience: 4/50
2024-12-15 20:11:35.566737: train_loss -0.7262
2024-12-15 20:11:35.567660: val_loss -0.5526
2024-12-15 20:11:35.568572: Pseudo dice [0.7467]
2024-12-15 20:11:35.569565: Epoch time: 273.36 s
2024-12-15 20:11:35.570444: Yayy! New best EMA pseudo Dice: 0.7402
2024-12-15 20:11:37.383244: 
2024-12-15 20:11:37.384756: Epoch 82
2024-12-15 20:11:37.385598: Current learning rate: 0.00491
2024-12-15 20:16:10.878153: Validation loss did not improve from -0.56017. Patience: 5/50
2024-12-15 20:16:10.879956: train_loss -0.7247
2024-12-15 20:16:10.881907: val_loss -0.5079
2024-12-15 20:16:10.882928: Pseudo dice [0.7173]
2024-12-15 20:16:10.884599: Epoch time: 273.5 s
2024-12-15 20:16:12.673151: 
2024-12-15 20:16:12.674909: Epoch 83
2024-12-15 20:16:12.675879: Current learning rate: 0.00484
2024-12-15 20:20:51.013666: Validation loss did not improve from -0.56017. Patience: 6/50
2024-12-15 20:20:51.014698: train_loss -0.7318
2024-12-15 20:20:51.015797: val_loss -0.5172
2024-12-15 20:20:51.016834: Pseudo dice [0.7351]
2024-12-15 20:20:51.017449: Epoch time: 278.34 s
2024-12-15 20:20:52.332698: 
2024-12-15 20:20:52.333978: Epoch 84
2024-12-15 20:20:52.334734: Current learning rate: 0.00478
2024-12-15 20:24:52.942348: Validation loss did not improve from -0.56017. Patience: 7/50
2024-12-15 20:24:52.943317: train_loss -0.7296
2024-12-15 20:24:52.944205: val_loss -0.5502
2024-12-15 20:24:52.945053: Pseudo dice [0.7508]
2024-12-15 20:24:52.945833: Epoch time: 240.61 s
2024-12-15 20:24:54.682537: 
2024-12-15 20:24:54.683987: Epoch 85
2024-12-15 20:24:54.684673: Current learning rate: 0.00471
2024-12-15 20:29:41.339391: Validation loss did not improve from -0.56017. Patience: 8/50
2024-12-15 20:29:41.340440: train_loss -0.7332
2024-12-15 20:29:41.341297: val_loss -0.5537
2024-12-15 20:29:41.342100: Pseudo dice [0.7548]
2024-12-15 20:29:41.342859: Epoch time: 286.66 s
2024-12-15 20:29:41.343616: Yayy! New best EMA pseudo Dice: 0.7405
2024-12-15 20:29:43.050538: 
2024-12-15 20:29:43.051991: Epoch 86
2024-12-15 20:29:43.052911: Current learning rate: 0.00465
2024-12-15 20:34:45.272229: Validation loss did not improve from -0.56017. Patience: 9/50
2024-12-15 20:34:45.273633: train_loss -0.7313
2024-12-15 20:34:45.274691: val_loss -0.5383
2024-12-15 20:34:45.275925: Pseudo dice [0.7517]
2024-12-15 20:34:45.276686: Epoch time: 302.22 s
2024-12-15 20:34:45.277434: Yayy! New best EMA pseudo Dice: 0.7416
2024-12-15 20:34:46.976661: 
2024-12-15 20:34:46.978008: Epoch 87
2024-12-15 20:34:46.978843: Current learning rate: 0.00458
2024-12-15 20:39:54.902596: Validation loss did not improve from -0.56017. Patience: 10/50
2024-12-15 20:39:54.903580: train_loss -0.7341
2024-12-15 20:39:54.904330: val_loss -0.5035
2024-12-15 20:39:54.904990: Pseudo dice [0.7357]
2024-12-15 20:39:54.905757: Epoch time: 307.93 s
2024-12-15 20:39:56.211501: 
2024-12-15 20:39:56.212783: Epoch 88
2024-12-15 20:39:56.213783: Current learning rate: 0.00452
2024-12-15 20:44:53.464138: Validation loss did not improve from -0.56017. Patience: 11/50
2024-12-15 20:44:53.464998: train_loss -0.7386
2024-12-15 20:44:53.465740: val_loss -0.5157
2024-12-15 20:44:53.466466: Pseudo dice [0.7217]
2024-12-15 20:44:53.467198: Epoch time: 297.25 s
2024-12-15 20:44:54.780291: 
2024-12-15 20:44:54.782041: Epoch 89
2024-12-15 20:44:54.783110: Current learning rate: 0.00445
2024-12-15 20:48:34.520841: Validation loss did not improve from -0.56017. Patience: 12/50
2024-12-15 20:48:34.522209: train_loss -0.7349
2024-12-15 20:48:34.523104: val_loss -0.5498
2024-12-15 20:48:34.523885: Pseudo dice [0.7577]
2024-12-15 20:48:34.524873: Epoch time: 219.74 s
2024-12-15 20:48:36.231825: 
2024-12-15 20:48:36.233419: Epoch 90
2024-12-15 20:48:36.234584: Current learning rate: 0.00438
2024-12-15 20:53:06.352370: Validation loss did not improve from -0.56017. Patience: 13/50
2024-12-15 20:53:06.355289: train_loss -0.7419
2024-12-15 20:53:06.356380: val_loss -0.5202
2024-12-15 20:53:06.357127: Pseudo dice [0.7338]
2024-12-15 20:53:06.357929: Epoch time: 270.12 s
2024-12-15 20:53:07.678639: 
2024-12-15 20:53:07.680017: Epoch 91
2024-12-15 20:53:07.680912: Current learning rate: 0.00432
2024-12-15 20:57:43.499326: Validation loss did not improve from -0.56017. Patience: 14/50
2024-12-15 20:57:43.500288: train_loss -0.7421
2024-12-15 20:57:43.501005: val_loss -0.5237
2024-12-15 20:57:43.501722: Pseudo dice [0.7377]
2024-12-15 20:57:43.502420: Epoch time: 275.82 s
2024-12-15 20:57:44.818797: 
2024-12-15 20:57:44.820267: Epoch 92
2024-12-15 20:57:44.821035: Current learning rate: 0.00425
2024-12-15 21:02:26.430350: Validation loss did not improve from -0.56017. Patience: 15/50
2024-12-15 21:02:26.431531: train_loss -0.7425
2024-12-15 21:02:26.432354: val_loss -0.5237
2024-12-15 21:02:26.433001: Pseudo dice [0.7357]
2024-12-15 21:02:26.433844: Epoch time: 281.61 s
2024-12-15 21:02:27.823461: 
2024-12-15 21:02:27.824782: Epoch 93
2024-12-15 21:02:27.825514: Current learning rate: 0.00419
2024-12-15 21:07:11.096011: Validation loss did not improve from -0.56017. Patience: 16/50
2024-12-15 21:07:11.097039: train_loss -0.7464
2024-12-15 21:07:11.097899: val_loss -0.5415
2024-12-15 21:07:11.098558: Pseudo dice [0.7451]
2024-12-15 21:07:11.099333: Epoch time: 283.27 s
2024-12-15 21:07:12.996414: 
2024-12-15 21:07:12.997839: Epoch 94
2024-12-15 21:07:12.998885: Current learning rate: 0.00412
2024-12-15 21:11:56.654603: Validation loss did not improve from -0.56017. Patience: 17/50
2024-12-15 21:11:56.655700: train_loss -0.7481
2024-12-15 21:11:56.656539: val_loss -0.525
2024-12-15 21:11:56.657144: Pseudo dice [0.7323]
2024-12-15 21:11:56.657888: Epoch time: 283.66 s
2024-12-15 21:11:58.414172: 
2024-12-15 21:11:58.415663: Epoch 95
2024-12-15 21:11:58.416357: Current learning rate: 0.00405
2024-12-15 21:16:42.852793: Validation loss did not improve from -0.56017. Patience: 18/50
2024-12-15 21:16:42.854209: train_loss -0.7504
2024-12-15 21:16:42.855441: val_loss -0.5396
2024-12-15 21:16:42.856224: Pseudo dice [0.7379]
2024-12-15 21:16:42.857254: Epoch time: 284.44 s
2024-12-15 21:16:44.188419: 
2024-12-15 21:16:44.189631: Epoch 96
2024-12-15 21:16:44.190318: Current learning rate: 0.00399
2024-12-15 21:21:53.305017: Validation loss did not improve from -0.56017. Patience: 19/50
2024-12-15 21:21:53.307464: train_loss -0.7515
2024-12-15 21:21:53.309739: val_loss -0.5284
2024-12-15 21:21:53.310591: Pseudo dice [0.7369]
2024-12-15 21:21:53.312029: Epoch time: 309.12 s
2024-12-15 21:21:54.674265: 
2024-12-15 21:21:54.675658: Epoch 97
2024-12-15 21:21:54.676419: Current learning rate: 0.00392
2024-12-15 21:27:02.969427: Validation loss did not improve from -0.56017. Patience: 20/50
2024-12-15 21:27:02.970484: train_loss -0.7493
2024-12-15 21:27:02.971418: val_loss -0.5484
2024-12-15 21:27:02.972397: Pseudo dice [0.7466]
2024-12-15 21:27:02.973345: Epoch time: 308.3 s
2024-12-15 21:27:04.315762: 
2024-12-15 21:27:04.317266: Epoch 98
2024-12-15 21:27:04.318305: Current learning rate: 0.00385
2024-12-15 21:32:07.321858: Validation loss did not improve from -0.56017. Patience: 21/50
2024-12-15 21:32:07.323204: train_loss -0.7497
2024-12-15 21:32:07.324365: val_loss -0.5374
2024-12-15 21:32:07.325372: Pseudo dice [0.7445]
2024-12-15 21:32:07.326151: Epoch time: 303.01 s
2024-12-15 21:32:08.673471: 
2024-12-15 21:32:08.674761: Epoch 99
2024-12-15 21:32:08.675561: Current learning rate: 0.00379
2024-12-15 21:36:45.065303: Validation loss did not improve from -0.56017. Patience: 22/50
2024-12-15 21:36:45.066291: train_loss -0.7519
2024-12-15 21:36:45.067390: val_loss -0.5229
2024-12-15 21:36:45.068393: Pseudo dice [0.7362]
2024-12-15 21:36:45.069237: Epoch time: 276.39 s
2024-12-15 21:36:46.797885: 
2024-12-15 21:36:46.799528: Epoch 100
2024-12-15 21:36:46.800662: Current learning rate: 0.00372
2024-12-15 21:41:16.362341: Validation loss did not improve from -0.56017. Patience: 23/50
2024-12-15 21:41:16.363279: train_loss -0.7512
2024-12-15 21:41:16.364043: val_loss -0.511
2024-12-15 21:41:16.364875: Pseudo dice [0.7305]
2024-12-15 21:41:16.365622: Epoch time: 269.57 s
2024-12-15 21:41:17.718887: 
2024-12-15 21:41:17.720076: Epoch 101
2024-12-15 21:41:17.720754: Current learning rate: 0.00365
2024-12-15 21:45:49.322654: Validation loss did not improve from -0.56017. Patience: 24/50
2024-12-15 21:45:49.323599: train_loss -0.7561
2024-12-15 21:45:49.324489: val_loss -0.5122
2024-12-15 21:45:49.325323: Pseudo dice [0.7316]
2024-12-15 21:45:49.326109: Epoch time: 271.61 s
2024-12-15 21:45:50.692548: 
2024-12-15 21:45:50.694021: Epoch 102
2024-12-15 21:45:50.694967: Current learning rate: 0.00359
2024-12-15 21:50:08.713469: Validation loss did not improve from -0.56017. Patience: 25/50
2024-12-15 21:50:08.714439: train_loss -0.7572
2024-12-15 21:50:08.715423: val_loss -0.544
2024-12-15 21:50:08.716259: Pseudo dice [0.7404]
2024-12-15 21:50:08.717105: Epoch time: 258.02 s
2024-12-15 21:50:10.169300: 
2024-12-15 21:50:10.171048: Epoch 103
2024-12-15 21:50:10.172097: Current learning rate: 0.00352
2024-12-15 21:54:03.691701: Validation loss did not improve from -0.56017. Patience: 26/50
2024-12-15 21:54:03.692654: train_loss -0.7575
2024-12-15 21:54:03.693410: val_loss -0.5261
2024-12-15 21:54:03.694093: Pseudo dice [0.7442]
2024-12-15 21:54:03.694729: Epoch time: 233.53 s
2024-12-15 21:54:05.038068: 
2024-12-15 21:54:05.039459: Epoch 104
2024-12-15 21:54:05.040193: Current learning rate: 0.00345
2024-12-15 21:59:01.282119: Validation loss did not improve from -0.56017. Patience: 27/50
2024-12-15 21:59:01.285547: train_loss -0.7568
2024-12-15 21:59:01.287067: val_loss -0.5452
2024-12-15 21:59:01.287898: Pseudo dice [0.7483]
2024-12-15 21:59:01.288650: Epoch time: 296.25 s
2024-12-15 21:59:03.020442: 
2024-12-15 21:59:03.022024: Epoch 105
2024-12-15 21:59:03.022796: Current learning rate: 0.00338
2024-12-15 22:03:48.199676: Validation loss did not improve from -0.56017. Patience: 28/50
2024-12-15 22:03:48.200657: train_loss -0.7524
2024-12-15 22:03:48.201398: val_loss -0.5407
2024-12-15 22:03:48.202539: Pseudo dice [0.7492]
2024-12-15 22:03:48.203944: Epoch time: 285.18 s
2024-12-15 22:03:50.057308: 
2024-12-15 22:03:50.058567: Epoch 106
2024-12-15 22:03:50.059524: Current learning rate: 0.00332
2024-12-15 22:08:30.794508: Validation loss did not improve from -0.56017. Patience: 29/50
2024-12-15 22:08:30.795565: train_loss -0.7615
2024-12-15 22:08:30.796298: val_loss -0.5174
2024-12-15 22:08:30.796958: Pseudo dice [0.7381]
2024-12-15 22:08:30.797739: Epoch time: 280.74 s
2024-12-15 22:08:32.151469: 
2024-12-15 22:08:32.153252: Epoch 107
2024-12-15 22:08:32.154306: Current learning rate: 0.00325
2024-12-15 22:13:12.579798: Validation loss did not improve from -0.56017. Patience: 30/50
2024-12-15 22:13:12.581204: train_loss -0.7558
2024-12-15 22:13:12.582071: val_loss -0.5419
2024-12-15 22:13:12.582753: Pseudo dice [0.7522]
2024-12-15 22:13:12.583387: Epoch time: 280.43 s
2024-12-15 22:13:12.584252: Yayy! New best EMA pseudo Dice: 0.7417
2024-12-15 22:13:14.295695: 
2024-12-15 22:13:14.296904: Epoch 108
2024-12-15 22:13:14.297663: Current learning rate: 0.00318
2024-12-15 22:17:07.398769: Validation loss did not improve from -0.56017. Patience: 31/50
2024-12-15 22:17:07.400508: train_loss -0.7617
2024-12-15 22:17:07.401425: val_loss -0.5191
2024-12-15 22:17:07.402275: Pseudo dice [0.7402]
2024-12-15 22:17:07.402962: Epoch time: 233.11 s
2024-12-15 22:17:08.752057: 
2024-12-15 22:17:08.753358: Epoch 109
2024-12-15 22:17:08.754117: Current learning rate: 0.00311
2024-12-15 22:21:20.550779: Validation loss did not improve from -0.56017. Patience: 32/50
2024-12-15 22:21:20.551962: train_loss -0.7681
2024-12-15 22:21:20.552750: val_loss -0.5275
2024-12-15 22:21:20.553543: Pseudo dice [0.742]
2024-12-15 22:21:20.554399: Epoch time: 251.8 s
2024-12-15 22:21:22.333965: 
2024-12-15 22:21:22.335713: Epoch 110
2024-12-15 22:21:22.336422: Current learning rate: 0.00304
2024-12-15 22:26:01.183705: Validation loss did not improve from -0.56017. Patience: 33/50
2024-12-15 22:26:01.185731: train_loss -0.7647
2024-12-15 22:26:01.187257: val_loss -0.5336
2024-12-15 22:26:01.188075: Pseudo dice [0.7479]
2024-12-15 22:26:01.189073: Epoch time: 278.85 s
2024-12-15 22:26:01.189903: Yayy! New best EMA pseudo Dice: 0.7422
2024-12-15 22:26:02.953384: 
2024-12-15 22:26:02.954761: Epoch 111
2024-12-15 22:26:02.955503: Current learning rate: 0.00297
2024-12-15 22:31:01.026704: Validation loss did not improve from -0.56017. Patience: 34/50
2024-12-15 22:31:01.027800: train_loss -0.7613
2024-12-15 22:31:01.028496: val_loss -0.5253
2024-12-15 22:31:01.029139: Pseudo dice [0.7428]
2024-12-15 22:31:01.029776: Epoch time: 298.08 s
2024-12-15 22:31:01.030484: Yayy! New best EMA pseudo Dice: 0.7423
2024-12-15 22:31:02.756865: 
2024-12-15 22:31:02.758187: Epoch 112
2024-12-15 22:31:02.759351: Current learning rate: 0.00291
2024-12-15 22:36:06.761680: Validation loss did not improve from -0.56017. Patience: 35/50
2024-12-15 22:36:06.762797: train_loss -0.7645
2024-12-15 22:36:06.763630: val_loss -0.5402
2024-12-15 22:36:06.764333: Pseudo dice [0.741]
2024-12-15 22:36:06.764988: Epoch time: 304.01 s
2024-12-15 22:36:08.121879: 
2024-12-15 22:36:08.123360: Epoch 113
2024-12-15 22:36:08.124191: Current learning rate: 0.00284
2024-12-15 22:40:55.048857: Validation loss did not improve from -0.56017. Patience: 36/50
2024-12-15 22:40:55.049866: train_loss -0.7673
2024-12-15 22:40:55.050805: val_loss -0.5096
2024-12-15 22:40:55.051463: Pseudo dice [0.7333]
2024-12-15 22:40:55.052092: Epoch time: 286.93 s
2024-12-15 22:40:56.452739: 
2024-12-15 22:40:56.454353: Epoch 114
2024-12-15 22:40:56.455419: Current learning rate: 0.00277
2024-12-15 22:44:42.429413: Validation loss did not improve from -0.56017. Patience: 37/50
2024-12-15 22:44:42.430486: train_loss -0.769
2024-12-15 22:44:42.431246: val_loss -0.538
2024-12-15 22:44:42.431917: Pseudo dice [0.7577]
2024-12-15 22:44:42.432595: Epoch time: 225.98 s
2024-12-15 22:44:42.838960: Yayy! New best EMA pseudo Dice: 0.7429
2024-12-15 22:44:44.561073: 
2024-12-15 22:44:44.562280: Epoch 115
2024-12-15 22:44:44.563053: Current learning rate: 0.0027
2024-12-15 22:49:17.421148: Validation loss did not improve from -0.56017. Patience: 38/50
2024-12-15 22:49:17.422193: train_loss -0.7652
2024-12-15 22:49:17.422935: val_loss -0.5501
2024-12-15 22:49:17.423662: Pseudo dice [0.7507]
2024-12-15 22:49:17.424307: Epoch time: 272.86 s
2024-12-15 22:49:17.424923: Yayy! New best EMA pseudo Dice: 0.7437
2024-12-15 22:49:19.304355: 
2024-12-15 22:49:19.306200: Epoch 116
2024-12-15 22:49:19.307390: Current learning rate: 0.00263
2024-12-15 22:54:05.444078: Validation loss did not improve from -0.56017. Patience: 39/50
2024-12-15 22:54:05.445163: train_loss -0.771
2024-12-15 22:54:05.446103: val_loss -0.5337
2024-12-15 22:54:05.446885: Pseudo dice [0.7395]
2024-12-15 22:54:05.447626: Epoch time: 286.15 s
2024-12-15 22:54:07.645819: 
2024-12-15 22:54:07.647499: Epoch 117
2024-12-15 22:54:07.648368: Current learning rate: 0.00256
2024-12-15 22:58:51.855567: Validation loss did not improve from -0.56017. Patience: 40/50
2024-12-15 22:58:51.856477: train_loss -0.7698
2024-12-15 22:58:51.857268: val_loss -0.5393
2024-12-15 22:58:51.857987: Pseudo dice [0.7524]
2024-12-15 22:58:51.858673: Epoch time: 284.21 s
2024-12-15 22:58:51.859279: Yayy! New best EMA pseudo Dice: 0.7442
2024-12-15 22:58:53.674172: 
2024-12-15 22:58:53.675538: Epoch 118
2024-12-15 22:58:53.676379: Current learning rate: 0.00249
2024-12-15 23:03:37.434014: Validation loss did not improve from -0.56017. Patience: 41/50
2024-12-15 23:03:37.436851: train_loss -0.7721
2024-12-15 23:03:37.438025: val_loss -0.5461
2024-12-15 23:03:37.438762: Pseudo dice [0.7512]
2024-12-15 23:03:37.439839: Epoch time: 283.76 s
2024-12-15 23:03:37.440960: Yayy! New best EMA pseudo Dice: 0.7449
2024-12-15 23:03:39.226157: 
2024-12-15 23:03:39.227446: Epoch 119
2024-12-15 23:03:39.228371: Current learning rate: 0.00242
2024-12-15 23:08:51.843645: Validation loss did not improve from -0.56017. Patience: 42/50
2024-12-15 23:08:51.844583: train_loss -0.7722
2024-12-15 23:08:51.845489: val_loss -0.545
2024-12-15 23:08:51.846418: Pseudo dice [0.75]
2024-12-15 23:08:51.847270: Epoch time: 312.62 s
2024-12-15 23:08:52.235642: Yayy! New best EMA pseudo Dice: 0.7454
2024-12-15 23:08:53.978185: 
2024-12-15 23:08:53.979483: Epoch 120
2024-12-15 23:08:53.980262: Current learning rate: 0.00235
2024-12-15 23:13:39.975394: Validation loss did not improve from -0.56017. Patience: 43/50
2024-12-15 23:13:39.976191: train_loss -0.7734
2024-12-15 23:13:39.976956: val_loss -0.5437
2024-12-15 23:13:39.977784: Pseudo dice [0.7566]
2024-12-15 23:13:39.978544: Epoch time: 286.0 s
2024-12-15 23:13:39.979307: Yayy! New best EMA pseudo Dice: 0.7465
2024-12-15 23:13:41.783296: 
2024-12-15 23:13:41.784282: Epoch 121
2024-12-15 23:13:41.785042: Current learning rate: 0.00228
2024-12-15 23:18:26.438550: Validation loss did not improve from -0.56017. Patience: 44/50
2024-12-15 23:18:26.439807: train_loss -0.7698
2024-12-15 23:18:26.440538: val_loss -0.5502
2024-12-15 23:18:26.441187: Pseudo dice [0.7522]
2024-12-15 23:18:26.441835: Epoch time: 284.66 s
2024-12-15 23:18:26.442502: Yayy! New best EMA pseudo Dice: 0.7471
2024-12-15 23:18:28.234504: 
2024-12-15 23:18:28.235733: Epoch 122
2024-12-15 23:18:28.236487: Current learning rate: 0.00221
2024-12-15 23:23:15.949414: Validation loss did not improve from -0.56017. Patience: 45/50
2024-12-15 23:23:15.950251: train_loss -0.7742
2024-12-15 23:23:15.951217: val_loss -0.5531
2024-12-15 23:23:15.952043: Pseudo dice [0.7568]
2024-12-15 23:23:15.952905: Epoch time: 287.72 s
2024-12-15 23:23:15.953748: Yayy! New best EMA pseudo Dice: 0.7481
2024-12-15 23:23:17.712043: 
2024-12-15 23:23:17.713431: Epoch 123
2024-12-15 23:23:17.714268: Current learning rate: 0.00214
2024-12-15 23:26:26.545043: Validation loss did not improve from -0.56017. Patience: 46/50
2024-12-15 23:26:26.546299: train_loss -0.7764
2024-12-15 23:26:26.547252: val_loss -0.5381
2024-12-15 23:26:26.548248: Pseudo dice [0.7496]
2024-12-15 23:26:26.549303: Epoch time: 188.84 s
2024-12-15 23:26:26.549953: Yayy! New best EMA pseudo Dice: 0.7482
2024-12-15 23:26:28.337765: 
2024-12-15 23:26:28.339373: Epoch 124
2024-12-15 23:26:28.340224: Current learning rate: 0.00207
2024-12-15 23:29:41.291251: Validation loss did not improve from -0.56017. Patience: 47/50
2024-12-15 23:29:41.293516: train_loss -0.7765
2024-12-15 23:29:41.295712: val_loss -0.5458
2024-12-15 23:29:41.296959: Pseudo dice [0.7488]
2024-12-15 23:29:41.298729: Epoch time: 192.96 s
2024-12-15 23:29:41.684617: Yayy! New best EMA pseudo Dice: 0.7483
2024-12-15 23:29:43.440190: 
2024-12-15 23:29:43.441695: Epoch 125
2024-12-15 23:29:43.442671: Current learning rate: 0.00199
2024-12-15 23:32:56.523619: Validation loss improved from -0.56017 to -0.56238! Patience: 47/50
2024-12-15 23:32:56.524653: train_loss -0.7777
2024-12-15 23:32:56.525612: val_loss -0.5624
2024-12-15 23:32:56.526537: Pseudo dice [0.7637]
2024-12-15 23:32:56.527497: Epoch time: 193.09 s
2024-12-15 23:32:56.528287: Yayy! New best EMA pseudo Dice: 0.7498
2024-12-15 23:32:58.286896: 
2024-12-15 23:32:58.288047: Epoch 126
2024-12-15 23:32:58.288761: Current learning rate: 0.00192
2024-12-15 23:36:05.412615: Validation loss improved from -0.56238 to -0.56551! Patience: 0/50
2024-12-15 23:36:05.413568: train_loss -0.78
2024-12-15 23:36:05.414418: val_loss -0.5655
2024-12-15 23:36:05.415115: Pseudo dice [0.7591]
2024-12-15 23:36:05.415953: Epoch time: 187.13 s
2024-12-15 23:36:05.416631: Yayy! New best EMA pseudo Dice: 0.7507
2024-12-15 23:36:07.728197: 
2024-12-15 23:36:07.729290: Epoch 127
2024-12-15 23:36:07.729984: Current learning rate: 0.00185
2024-12-15 23:39:26.631537: Validation loss did not improve from -0.56551. Patience: 1/50
2024-12-15 23:39:26.632613: train_loss -0.7791
2024-12-15 23:39:26.633558: val_loss -0.5287
2024-12-15 23:39:26.634232: Pseudo dice [0.7491]
2024-12-15 23:39:26.635144: Epoch time: 198.91 s
2024-12-15 23:39:28.003659: 
2024-12-15 23:39:28.005330: Epoch 128
2024-12-15 23:39:28.006361: Current learning rate: 0.00178
2024-12-15 23:42:46.861837: Validation loss did not improve from -0.56551. Patience: 2/50
2024-12-15 23:42:46.863154: train_loss -0.7801
2024-12-15 23:42:46.864063: val_loss -0.5168
2024-12-15 23:42:46.864895: Pseudo dice [0.7297]
2024-12-15 23:42:46.865548: Epoch time: 198.86 s
2024-12-15 23:42:48.227322: 
2024-12-15 23:42:48.228664: Epoch 129
2024-12-15 23:42:48.229477: Current learning rate: 0.0017
2024-12-15 23:46:10.891651: Validation loss did not improve from -0.56551. Patience: 3/50
2024-12-15 23:46:10.892718: train_loss -0.7769
2024-12-15 23:46:10.893697: val_loss -0.5496
2024-12-15 23:46:10.894576: Pseudo dice [0.7508]
2024-12-15 23:46:10.895380: Epoch time: 202.67 s
2024-12-15 23:46:12.638841: 
2024-12-15 23:46:12.640302: Epoch 130
2024-12-15 23:46:12.641332: Current learning rate: 0.00163
2024-12-15 23:49:32.684343: Validation loss did not improve from -0.56551. Patience: 4/50
2024-12-15 23:49:32.685340: train_loss -0.7805
2024-12-15 23:49:32.686065: val_loss -0.532
2024-12-15 23:49:32.686681: Pseudo dice [0.7406]
2024-12-15 23:49:32.687426: Epoch time: 200.05 s
2024-12-15 23:49:34.045235: 
2024-12-15 23:49:34.047024: Epoch 131
2024-12-15 23:49:34.047778: Current learning rate: 0.00156
2024-12-15 23:52:59.176736: Validation loss did not improve from -0.56551. Patience: 5/50
2024-12-15 23:52:59.177700: train_loss -0.7794
2024-12-15 23:52:59.178514: val_loss -0.5431
2024-12-15 23:52:59.179335: Pseudo dice [0.7484]
2024-12-15 23:52:59.180162: Epoch time: 205.13 s
2024-12-15 23:53:00.525155: 
2024-12-15 23:53:00.526499: Epoch 132
2024-12-15 23:53:00.527327: Current learning rate: 0.00148
2024-12-15 23:56:27.302867: Validation loss did not improve from -0.56551. Patience: 6/50
2024-12-15 23:56:27.303926: train_loss -0.78
2024-12-15 23:56:27.304844: val_loss -0.5565
2024-12-15 23:56:27.305458: Pseudo dice [0.7592]
2024-12-15 23:56:27.306199: Epoch time: 206.78 s
2024-12-15 23:56:28.656144: 
2024-12-15 23:56:28.657692: Epoch 133
2024-12-15 23:56:28.658633: Current learning rate: 0.00141
2024-12-15 23:59:51.991710: Validation loss did not improve from -0.56551. Patience: 7/50
2024-12-15 23:59:51.993488: train_loss -0.7832
2024-12-15 23:59:51.994316: val_loss -0.5341
2024-12-15 23:59:51.995101: Pseudo dice [0.7466]
2024-12-15 23:59:51.995995: Epoch time: 203.34 s
2024-12-15 23:59:53.362786: 
2024-12-15 23:59:53.363897: Epoch 134
2024-12-15 23:59:53.364694: Current learning rate: 0.00133
2024-12-16 00:03:14.362520: Validation loss did not improve from -0.56551. Patience: 8/50
2024-12-16 00:03:14.363550: train_loss -0.7791
2024-12-16 00:03:14.364323: val_loss -0.5648
2024-12-16 00:03:14.365087: Pseudo dice [0.7586]
2024-12-16 00:03:14.365829: Epoch time: 201.0 s
2024-12-16 00:03:16.196268: 
2024-12-16 00:03:16.197913: Epoch 135
2024-12-16 00:03:16.198901: Current learning rate: 0.00126
2024-12-16 00:06:43.044677: Validation loss did not improve from -0.56551. Patience: 9/50
2024-12-16 00:06:43.047261: train_loss -0.7829
2024-12-16 00:06:43.048615: val_loss -0.5418
2024-12-16 00:06:43.049491: Pseudo dice [0.7511]
2024-12-16 00:06:43.050324: Epoch time: 206.85 s
2024-12-16 00:06:44.437917: 
2024-12-16 00:06:44.439684: Epoch 136
2024-12-16 00:06:44.440710: Current learning rate: 0.00118
2024-12-16 00:10:07.074630: Validation loss did not improve from -0.56551. Patience: 10/50
2024-12-16 00:10:07.075678: train_loss -0.7838
2024-12-16 00:10:07.076491: val_loss -0.5458
2024-12-16 00:10:07.077242: Pseudo dice [0.7562]
2024-12-16 00:10:07.078048: Epoch time: 202.64 s
2024-12-16 00:10:08.449183: 
2024-12-16 00:10:08.450443: Epoch 137
2024-12-16 00:10:08.451267: Current learning rate: 0.00111
2024-12-16 00:13:32.967111: Validation loss did not improve from -0.56551. Patience: 11/50
2024-12-16 00:13:32.968231: train_loss -0.7813
2024-12-16 00:13:32.969158: val_loss -0.5555
2024-12-16 00:13:32.969925: Pseudo dice [0.7551]
2024-12-16 00:13:32.970870: Epoch time: 204.52 s
2024-12-16 00:13:32.971637: Yayy! New best EMA pseudo Dice: 0.751
2024-12-16 00:13:34.853670: 
2024-12-16 00:13:34.855480: Epoch 138
2024-12-16 00:13:34.856370: Current learning rate: 0.00103
2024-12-16 00:16:54.998668: Validation loss did not improve from -0.56551. Patience: 12/50
2024-12-16 00:16:54.999585: train_loss -0.788
2024-12-16 00:16:55.000340: val_loss -0.5287
2024-12-16 00:16:55.001069: Pseudo dice [0.7476]
2024-12-16 00:16:55.001750: Epoch time: 200.15 s
2024-12-16 00:16:56.371698: 
2024-12-16 00:16:56.373103: Epoch 139
2024-12-16 00:16:56.373851: Current learning rate: 0.00095
2024-12-16 00:20:12.868444: Validation loss did not improve from -0.56551. Patience: 13/50
2024-12-16 00:20:12.869640: train_loss -0.7848
2024-12-16 00:20:12.870871: val_loss -0.548
2024-12-16 00:20:12.871742: Pseudo dice [0.7573]
2024-12-16 00:20:12.872640: Epoch time: 196.5 s
2024-12-16 00:20:13.337281: Yayy! New best EMA pseudo Dice: 0.7513
2024-12-16 00:20:15.184306: 
2024-12-16 00:20:15.186054: Epoch 140
2024-12-16 00:20:15.187164: Current learning rate: 0.00087
2024-12-16 00:23:34.953686: Validation loss did not improve from -0.56551. Patience: 14/50
2024-12-16 00:23:34.954602: train_loss -0.7833
2024-12-16 00:23:34.955443: val_loss -0.5396
2024-12-16 00:23:34.956276: Pseudo dice [0.7523]
2024-12-16 00:23:34.957155: Epoch time: 199.77 s
2024-12-16 00:23:34.958014: Yayy! New best EMA pseudo Dice: 0.7514
2024-12-16 00:23:36.731312: 
2024-12-16 00:23:36.732989: Epoch 141
2024-12-16 00:23:36.734018: Current learning rate: 0.00079
2024-12-16 00:27:01.338680: Validation loss did not improve from -0.56551. Patience: 15/50
2024-12-16 00:27:01.339710: train_loss -0.7842
2024-12-16 00:27:01.340719: val_loss -0.5262
2024-12-16 00:27:01.341522: Pseudo dice [0.7407]
2024-12-16 00:27:01.342512: Epoch time: 204.61 s
2024-12-16 00:27:02.698797: 
2024-12-16 00:27:02.700315: Epoch 142
2024-12-16 00:27:02.701371: Current learning rate: 0.00071
2024-12-16 00:30:26.366635: Validation loss did not improve from -0.56551. Patience: 16/50
2024-12-16 00:30:26.367351: train_loss -0.7865
2024-12-16 00:30:26.368066: val_loss -0.552
2024-12-16 00:30:26.368720: Pseudo dice [0.756]
2024-12-16 00:30:26.369398: Epoch time: 203.67 s
2024-12-16 00:30:27.760496: 
2024-12-16 00:30:27.762024: Epoch 143
2024-12-16 00:30:27.762841: Current learning rate: 0.00063
2024-12-16 00:33:53.270474: Validation loss did not improve from -0.56551. Patience: 17/50
2024-12-16 00:33:53.272649: train_loss -0.7869
2024-12-16 00:33:53.274781: val_loss -0.533
2024-12-16 00:33:53.275542: Pseudo dice [0.7454]
2024-12-16 00:33:53.276898: Epoch time: 205.51 s
2024-12-16 00:33:54.693371: 
2024-12-16 00:33:54.695086: Epoch 144
2024-12-16 00:33:54.695963: Current learning rate: 0.00055
2024-12-16 00:37:15.297288: Validation loss did not improve from -0.56551. Patience: 18/50
2024-12-16 00:37:15.298212: train_loss -0.7847
2024-12-16 00:37:15.299051: val_loss -0.5402
2024-12-16 00:37:15.299835: Pseudo dice [0.7494]
2024-12-16 00:37:15.300582: Epoch time: 200.61 s
2024-12-16 00:37:17.101750: 
2024-12-16 00:37:17.103181: Epoch 145
2024-12-16 00:37:17.103990: Current learning rate: 0.00047
2024-12-16 00:40:38.436634: Validation loss did not improve from -0.56551. Patience: 19/50
2024-12-16 00:40:38.437559: train_loss -0.7886
2024-12-16 00:40:38.438259: val_loss -0.5309
2024-12-16 00:40:38.438944: Pseudo dice [0.748]
2024-12-16 00:40:38.439642: Epoch time: 201.34 s
2024-12-16 00:40:39.822196: 
2024-12-16 00:40:39.823448: Epoch 146
2024-12-16 00:40:39.824087: Current learning rate: 0.00038
2024-12-16 00:44:10.057511: Validation loss did not improve from -0.56551. Patience: 20/50
2024-12-16 00:44:10.058533: train_loss -0.7874
2024-12-16 00:44:10.059416: val_loss -0.5519
2024-12-16 00:44:10.060219: Pseudo dice [0.7506]
2024-12-16 00:44:10.060998: Epoch time: 210.24 s
2024-12-16 00:44:11.455145: 
2024-12-16 00:44:11.456630: Epoch 147
2024-12-16 00:44:11.457455: Current learning rate: 0.0003
2024-12-16 00:47:30.890114: Validation loss did not improve from -0.56551. Patience: 21/50
2024-12-16 00:47:30.891179: train_loss -0.7909
2024-12-16 00:47:30.892020: val_loss -0.5628
2024-12-16 00:47:30.892747: Pseudo dice [0.7629]
2024-12-16 00:47:30.893437: Epoch time: 199.44 s
2024-12-16 00:47:32.307461: 
2024-12-16 00:47:32.308917: Epoch 148
2024-12-16 00:47:32.309616: Current learning rate: 0.00021
2024-12-16 00:50:57.100053: Validation loss did not improve from -0.56551. Patience: 22/50
2024-12-16 00:50:57.100807: train_loss -0.7882
2024-12-16 00:50:57.101612: val_loss -0.5496
2024-12-16 00:50:57.102332: Pseudo dice [0.7542]
2024-12-16 00:50:57.103063: Epoch time: 204.79 s
2024-12-16 00:50:57.103727: Yayy! New best EMA pseudo Dice: 0.7517
2024-12-16 00:50:59.579635: 
2024-12-16 00:50:59.581037: Epoch 149
2024-12-16 00:50:59.581968: Current learning rate: 0.00011
2024-12-16 00:54:14.708507: Validation loss did not improve from -0.56551. Patience: 23/50
2024-12-16 00:54:14.709988: train_loss -0.7909
2024-12-16 00:54:14.711429: val_loss -0.5322
2024-12-16 00:54:14.712679: Pseudo dice [0.7468]
2024-12-16 00:54:14.713758: Epoch time: 195.13 s
2024-12-16 00:54:16.599900: Training done.
2024-12-16 00:54:16.869242: Using splits from existing split file: /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset307_Sohee_Calcium_OCT_CrossValidation/splits_final_60.json
2024-12-16 00:54:16.883181: The split file contains 5 splits.
2024-12-16 00:54:16.884340: Desired fold for training: 4
2024-12-16 00:54:16.885212: This split has 4 training and 4 validation cases.
2024-12-16 00:54:16.886584: predicting 101-044
2024-12-16 00:54:17.003428: 101-044, shape torch.Size([1, 404, 498, 498]), rank 0
2024-12-16 00:56:33.330171: predicting 101-045
2024-12-16 00:56:33.361184: 101-045, shape torch.Size([1, 375, 498, 498]), rank 0
2024-12-16 00:58:32.283464: predicting 401-004
2024-12-16 00:58:32.300502: 401-004, shape torch.Size([1, 375, 498, 498]), rank 0
2024-12-16 01:00:39.730764: predicting 706-005
2024-12-16 01:00:39.748260: 706-005, shape torch.Size([1, 375, 498, 498]), rank 0
2024-12-16 01:03:09.299142: Validation complete
2024-12-16 01:03:09.299818: Mean Validation Dice:  0.741071119605303
