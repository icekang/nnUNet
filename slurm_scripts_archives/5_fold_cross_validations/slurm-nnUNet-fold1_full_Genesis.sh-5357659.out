/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/bin/python
Starting training with CONFIG=3d_32x160x128_b10, DATASET_ID=307, TRAINER=nnUNetTrainer
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/nnunetv2/training/nnUNetTrainer/nnUNetTrainer.py:169: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  self.grad_scaler = GradScaler() if self.device.type == 'cuda' else None

############################
INFO: You are using the old nnU-Net default plans. We have updated our recommendations. Please consider using those instead! Read more here: https://github.com/MIC-DKFZ/nnUNet/blob/master/documentation/resenc_presets.md
############################

Using device: cuda:0

#######################################################################
Please cite the following paper when using nnU-Net:
Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.
#######################################################################

2025-10-14 15:12:00.710046: Using torch.compile...
################### Loading pretrained weights from file  /nfs/erelab001/shared/Computational_Group/Naravich/datasets/nnUNet_Datasets/../ModelGenesisOutputs/ModelGenesisNNUNetPretrainingV2_noNorm_correct_orientation/Converted_nnUNet_Genesis_OCT_Best.pt ###################
Below is the list of overlapping blocks in pretrained model and nnUNet architecture:
encoder.stages.0.0.convs.0.conv.weight shape torch.Size([32, 1, 3, 3, 3])
encoder.stages.0.0.convs.0.conv.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.norm.weight shape torch.Size([32])
encoder.stages.0.0.convs.0.norm.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.0.weight shape torch.Size([32, 1, 3, 3, 3])
encoder.stages.0.0.convs.0.all_modules.0.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.1.weight shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.1.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
encoder.stages.0.0.convs.1.conv.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.norm.weight shape torch.Size([32])
encoder.stages.0.0.convs.1.norm.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
encoder.stages.0.0.convs.1.all_modules.0.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.1.weight shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.1.bias shape torch.Size([32])
encoder.stages.1.0.convs.0.conv.weight shape torch.Size([64, 32, 3, 3, 3])
encoder.stages.1.0.convs.0.conv.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.norm.weight shape torch.Size([64])
encoder.stages.1.0.convs.0.norm.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.0.weight shape torch.Size([64, 32, 3, 3, 3])
encoder.stages.1.0.convs.0.all_modules.0.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.1.weight shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.1.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
encoder.stages.1.0.convs.1.conv.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.norm.weight shape torch.Size([64])
encoder.stages.1.0.convs.1.norm.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
encoder.stages.1.0.convs.1.all_modules.0.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.1.weight shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.1.bias shape torch.Size([64])
encoder.stages.2.0.convs.0.conv.weight shape torch.Size([128, 64, 3, 3, 3])
encoder.stages.2.0.convs.0.conv.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.norm.weight shape torch.Size([128])
encoder.stages.2.0.convs.0.norm.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.0.weight shape torch.Size([128, 64, 3, 3, 3])
encoder.stages.2.0.convs.0.all_modules.0.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.1.weight shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.1.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
encoder.stages.2.0.convs.1.conv.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.norm.weight shape torch.Size([128])
encoder.stages.2.0.convs.1.norm.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
encoder.stages.2.0.convs.1.all_modules.0.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.1.weight shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.1.bias shape torch.Size([128])
encoder.stages.3.0.convs.0.conv.weight shape torch.Size([256, 128, 3, 3, 3])
encoder.stages.3.0.convs.0.conv.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.norm.weight shape torch.Size([256])
encoder.stages.3.0.convs.0.norm.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.0.weight shape torch.Size([256, 128, 3, 3, 3])
encoder.stages.3.0.convs.0.all_modules.0.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.1.weight shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.1.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
encoder.stages.3.0.convs.1.conv.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.norm.weight shape torch.Size([256])
encoder.stages.3.0.convs.1.norm.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
encoder.stages.3.0.convs.1.all_modules.0.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.1.weight shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.1.bias shape torch.Size([256])
encoder.stages.4.0.convs.0.conv.weight shape torch.Size([320, 256, 3, 3, 3])
encoder.stages.4.0.convs.0.conv.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.norm.weight shape torch.Size([320])
encoder.stages.4.0.convs.0.norm.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.0.weight shape torch.Size([320, 256, 3, 3, 3])
encoder.stages.4.0.convs.0.all_modules.0.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.1.weight shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.1.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.4.0.convs.1.conv.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.norm.weight shape torch.Size([320])
encoder.stages.4.0.convs.1.norm.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.4.0.convs.1.all_modules.0.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.1.weight shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.1.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.0.conv.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.norm.weight shape torch.Size([320])
encoder.stages.5.0.convs.0.norm.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.0.all_modules.0.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.1.weight shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.1.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.1.conv.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.norm.weight shape torch.Size([320])
encoder.stages.5.0.convs.1.norm.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.1.all_modules.0.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.1.weight shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.0.0.convs.0.conv.weight shape torch.Size([32, 1, 3, 3, 3])
decoder.encoder.stages.0.0.convs.0.conv.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.norm.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.norm.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.0.weight shape torch.Size([32, 1, 3, 3, 3])
decoder.encoder.stages.0.0.convs.0.all_modules.0.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.1.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.1.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.encoder.stages.0.0.convs.1.conv.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.norm.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.norm.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.encoder.stages.0.0.convs.1.all_modules.0.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.1.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.1.bias shape torch.Size([32])
decoder.encoder.stages.1.0.convs.0.conv.weight shape torch.Size([64, 32, 3, 3, 3])
decoder.encoder.stages.1.0.convs.0.conv.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.norm.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.norm.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.0.weight shape torch.Size([64, 32, 3, 3, 3])
decoder.encoder.stages.1.0.convs.0.all_modules.0.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.1.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.1.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.encoder.stages.1.0.convs.1.conv.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.norm.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.norm.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.encoder.stages.1.0.convs.1.all_modules.0.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.1.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.1.bias shape torch.Size([64])
decoder.encoder.stages.2.0.convs.0.conv.weight shape torch.Size([128, 64, 3, 3, 3])
decoder.encoder.stages.2.0.convs.0.conv.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.norm.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.norm.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.0.weight shape torch.Size([128, 64, 3, 3, 3])
decoder.encoder.stages.2.0.convs.0.all_modules.0.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.1.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.1.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.encoder.stages.2.0.convs.1.conv.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.norm.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.norm.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.encoder.stages.2.0.convs.1.all_modules.0.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.1.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.1.bias shape torch.Size([128])
decoder.encoder.stages.3.0.convs.0.conv.weight shape torch.Size([256, 128, 3, 3, 3])
decoder.encoder.stages.3.0.convs.0.conv.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.norm.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.norm.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.0.weight shape torch.Size([256, 128, 3, 3, 3])
decoder.encoder.stages.3.0.convs.0.all_modules.0.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.1.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.1.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.encoder.stages.3.0.convs.1.conv.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.norm.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.norm.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.encoder.stages.3.0.convs.1.all_modules.0.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.1.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.1.bias shape torch.Size([256])
decoder.encoder.stages.4.0.convs.0.conv.weight shape torch.Size([320, 256, 3, 3, 3])
decoder.encoder.stages.4.0.convs.0.conv.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.norm.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.norm.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.0.weight shape torch.Size([320, 256, 3, 3, 3])
decoder.encoder.stages.4.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.4.0.convs.1.conv.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.norm.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.norm.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.4.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.0.conv.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.norm.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.norm.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.1.conv.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.norm.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.norm.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.stages.0.convs.0.conv.weight shape torch.Size([320, 640, 3, 3, 3])
decoder.stages.0.convs.0.conv.bias shape torch.Size([320])
decoder.stages.0.convs.0.norm.weight shape torch.Size([320])
decoder.stages.0.convs.0.norm.bias shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.0.weight shape torch.Size([320, 640, 3, 3, 3])
decoder.stages.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.stages.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.stages.0.convs.1.conv.bias shape torch.Size([320])
decoder.stages.0.convs.1.norm.weight shape torch.Size([320])
decoder.stages.0.convs.1.norm.bias shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.stages.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.stages.1.convs.0.conv.weight shape torch.Size([256, 512, 3, 3, 3])
decoder.stages.1.convs.0.conv.bias shape torch.Size([256])
decoder.stages.1.convs.0.norm.weight shape torch.Size([256])
decoder.stages.1.convs.0.norm.bias shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.0.weight shape torch.Size([256, 512, 3, 3, 3])
decoder.stages.1.convs.0.all_modules.0.bias shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.1.weight shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.1.bias shape torch.Size([256])
decoder.stages.1.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.stages.1.convs.1.conv.bias shape torch.Size([256])
decoder.stages.1.convs.1.norm.weight shape torch.Size([256])
decoder.stages.1.convs.1.norm.bias shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.stages.1.convs.1.all_modules.0.bias shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.1.weight shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.1.bias shape torch.Size([256])
decoder.stages.2.convs.0.conv.weight shape torch.Size([128, 256, 3, 3, 3])
decoder.stages.2.convs.0.conv.bias shape torch.Size([128])
decoder.stages.2.convs.0.norm.weight shape torch.Size([128])
decoder.stages.2.convs.0.norm.bias shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.0.weight shape torch.Size([128, 256, 3, 3, 3])
decoder.stages.2.convs.0.all_modules.0.bias shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.1.weight shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.1.bias shape torch.Size([128])
decoder.stages.2.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.stages.2.convs.1.conv.bias shape torch.Size([128])
decoder.stages.2.convs.1.norm.weight shape torch.Size([128])
decoder.stages.2.convs.1.norm.bias shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.stages.2.convs.1.all_modules.0.bias shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.1.weight shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.1.bias shape torch.Size([128])
decoder.stages.3.convs.0.conv.weight shape torch.Size([64, 128, 3, 3, 3])
decoder.stages.3.convs.0.conv.bias shape torch.Size([64])
decoder.stages.3.convs.0.norm.weight shape torch.Size([64])
decoder.stages.3.convs.0.norm.bias shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.0.weight shape torch.Size([64, 128, 3, 3, 3])
decoder.stages.3.convs.0.all_modules.0.bias shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.1.weight shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.1.bias shape torch.Size([64])
decoder.stages.3.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.stages.3.convs.1.conv.bias shape torch.Size([64])
decoder.stages.3.convs.1.norm.weight shape torch.Size([64])
decoder.stages.3.convs.1.norm.bias shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.stages.3.convs.1.all_modules.0.bias shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.1.weight shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.1.bias shape torch.Size([64])
decoder.stages.4.convs.0.conv.weight shape torch.Size([32, 64, 3, 3, 3])
decoder.stages.4.convs.0.conv.bias shape torch.Size([32])
decoder.stages.4.convs.0.norm.weight shape torch.Size([32])
decoder.stages.4.convs.0.norm.bias shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.0.weight shape torch.Size([32, 64, 3, 3, 3])
decoder.stages.4.convs.0.all_modules.0.bias shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.1.weight shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.1.bias shape torch.Size([32])
decoder.stages.4.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.stages.4.convs.1.conv.bias shape torch.Size([32])
decoder.stages.4.convs.1.norm.weight shape torch.Size([32])
decoder.stages.4.convs.1.norm.bias shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.stages.4.convs.1.all_modules.0.bias shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.1.weight shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.1.bias shape torch.Size([32])
decoder.transpconvs.0.weight shape torch.Size([320, 320, 1, 2, 2])
decoder.transpconvs.0.bias shape torch.Size([320])
decoder.transpconvs.1.weight shape torch.Size([320, 256, 2, 2, 2])
decoder.transpconvs.1.bias shape torch.Size([256])
decoder.transpconvs.2.weight shape torch.Size([256, 128, 2, 2, 2])
decoder.transpconvs.2.bias shape torch.Size([128])
decoder.transpconvs.3.weight shape torch.Size([128, 64, 2, 2, 2])
decoder.transpconvs.3.bias shape torch.Size([64])
decoder.transpconvs.4.weight shape torch.Size([64, 32, 2, 2, 2])
decoder.transpconvs.4.bias shape torch.Size([32])
################### Done ###################
2025-10-14 15:12:02.340890: do_dummy_2d_data_aug: True
2025-10-14 15:12:02.341726: Using splits from existing split file: /nfs/erelab001/shared/Computational_Group/Naravich/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset307_Sohee_Calcium_OCT_CrossValidation/splits_final.json
2025-10-14 15:12:02.342039: The split file contains 5 splits.
2025-10-14 15:12:02.342143: Desired fold for training: 1
2025-10-14 15:12:02.342231: This split has 6 training and 2 validation cases.
using pin_memory on device 0
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/fft_conv_pytorch/fft_conv.py:139: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:306.)
  output = output[crop_slices].contiguous()
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/fft_conv_pytorch/fft_conv.py:139: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:306.)
  output = output[crop_slices].contiguous()
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/fft_conv_pytorch/fft_conv.py:139: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:306.)
  output = output[crop_slices].contiguous()
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/fft_conv_pytorch/fft_conv.py:139: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:306.)
  output = output[crop_slices].contiguous()
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/fft_conv_pytorch/fft_conv.py:139: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:306.)
  output = output[crop_slices].contiguous()
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/fft_conv_pytorch/fft_conv.py:139: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:306.)
  output = output[crop_slices].contiguous()
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/fft_conv_pytorch/fft_conv.py:139: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:306.)
  output = output[crop_slices].contiguous()
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/fft_conv_pytorch/fft_conv.py:139: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:306.)
  output = output[crop_slices].contiguous()
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/fft_conv_pytorch/fft_conv.py:139: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:306.)
  output = output[crop_slices].contiguous()
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/fft_conv_pytorch/fft_conv.py:139: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:306.)
  output = output[crop_slices].contiguous()
using pin_memory on device 0

This is the configuration used by this training:
Configuration name: 3d_32x160x128_b10
 {'data_identifier': 'nnUNetPlans_3d_fullres', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 10, 'patch_size': [32, 160, 128], 'median_image_size_in_voxels': [375.0, 498.0, 498.0], 'spacing': [1.0, 1.0, 1.0], 'normalization_schemes': ['ZScoreNormalization'], 'use_mask_for_norm': [False], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'architecture': {'network_class_name': 'dynamic_network_architectures.architectures.unet.PlainConvUNet', 'arch_kwargs': {'n_stages': 6, 'features_per_stage': [32, 64, 128, 256, 320, 320], 'conv_op': 'torch.nn.modules.conv.Conv3d', 'kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]], 'strides': [[1, 1, 1], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [1, 2, 2]], 'n_conv_per_stage': [2, 2, 2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2, 2, 2], 'conv_bias': True, 'norm_op': 'torch.nn.modules.instancenorm.InstanceNorm3d', 'norm_op_kwargs': {'eps': 1e-05, 'affine': True}, 'dropout_op': None, 'dropout_op_kwargs': None, 'nonlin': 'torch.nn.LeakyReLU', 'nonlin_kwargs': {'inplace': True}, 'deep_supervision': True}, '_kw_requires_import': ['conv_op', 'norm_op', 'dropout_op', 'nonlin']}, 'batch_dice': True, 'inherits_from': '3d_fullres'} 

These are the global plan.json settings:
 {'dataset_name': 'Dataset307_Sohee_Calcium_OCT_CrossValidation', 'plans_name': 'nnUNetPlans', 'original_median_spacing_after_transp': [1.0, 1.0, 1.0], 'original_median_shape_after_transp': [375, 498, 498], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 0.4977976381778717, 'mean': 0.13507525622844696, 'median': 0.09599608182907104, 'min': 0.0, 'percentile_00_5': 0.014754901640117168, 'percentile_99_5': 0.4977976381778717, 'std': 0.12025152146816254}}} 

2025-10-14 15:12:06.587188: unpacking dataset...
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/fft_conv_pytorch/fft_conv.py:139: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:306.)
  output = output[crop_slices].contiguous()
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/fft_conv_pytorch/fft_conv.py:139: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:306.)
  output = output[crop_slices].contiguous()
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/torch/_inductor/lowering.py:7095: UserWarning: 
Online softmax is disabled on the fly since Inductor decides to
split the reduction. Cut an issue to PyTorch if this is an
important use case and you want to speed it up with online
softmax.

  warnings.warn(
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/torch/_inductor/lowering.py:7095: UserWarning: 
Online softmax is disabled on the fly since Inductor decides to
split the reduction. Cut an issue to PyTorch if this is an
important use case and you want to speed it up with online
softmax.

  warnings.warn(
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/torch/_inductor/lowering.py:7095: UserWarning: 
Online softmax is disabled on the fly since Inductor decides to
split the reduction. Cut an issue to PyTorch if this is an
important use case and you want to speed it up with online
softmax.

  warnings.warn(
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/torch/_inductor/lowering.py:7095: UserWarning: 
Online softmax is disabled on the fly since Inductor decides to
split the reduction. Cut an issue to PyTorch if this is an
important use case and you want to speed it up with online
softmax.

  warnings.warn(
2025-10-14 15:12:12.869521: unpacking done...
2025-10-14 15:12:12.872283: Unable to plot network architecture: nnUNet_compile is enabled!
2025-10-14 15:12:12.880371: 
2025-10-14 15:12:12.880617: Epoch 0
2025-10-14 15:12:12.880961: Current learning rate: 0.01
2025-10-14 15:13:32.551966: Validation loss improved from 1000.00000 to -0.16912! Patience: 0/50
2025-10-14 15:13:32.552754: train_loss -0.1419
2025-10-14 15:13:32.552993: val_loss -0.1691
2025-10-14 15:13:32.553202: Pseudo dice [np.float32(0.4892)]
2025-10-14 15:13:32.553394: Epoch time: 79.67 s
2025-10-14 15:13:32.553570: Yayy! New best EMA pseudo Dice: 0.48919999599456787
2025-10-14 15:13:33.511154: 
2025-10-14 15:13:33.511497: Epoch 1
2025-10-14 15:13:33.511692: Current learning rate: 0.00994
2025-10-14 15:14:19.512275: Validation loss improved from -0.16912 to -0.29753! Patience: 0/50
2025-10-14 15:14:19.512728: train_loss -0.3284
2025-10-14 15:14:19.512882: val_loss -0.2975
2025-10-14 15:14:19.513025: Pseudo dice [np.float32(0.5778)]
2025-10-14 15:14:19.513165: Epoch time: 46.0 s
2025-10-14 15:14:19.513290: Yayy! New best EMA pseudo Dice: 0.49799999594688416
2025-10-14 15:14:20.572705: 
2025-10-14 15:14:20.573036: Epoch 2
2025-10-14 15:14:20.573221: Current learning rate: 0.00988
2025-10-14 15:15:06.696696: Validation loss improved from -0.29753 to -0.31992! Patience: 0/50
2025-10-14 15:15:06.697652: train_loss -0.3757
2025-10-14 15:15:06.697990: val_loss -0.3199
2025-10-14 15:15:06.698305: Pseudo dice [np.float32(0.6118)]
2025-10-14 15:15:06.698619: Epoch time: 46.13 s
2025-10-14 15:15:06.698930: Yayy! New best EMA pseudo Dice: 0.5094000101089478
2025-10-14 15:15:07.764549: 
2025-10-14 15:15:07.764884: Epoch 3
2025-10-14 15:15:07.765072: Current learning rate: 0.00982
2025-10-14 15:15:53.830788: Validation loss improved from -0.31992 to -0.38120! Patience: 0/50
2025-10-14 15:15:53.831199: train_loss -0.394
2025-10-14 15:15:53.831337: val_loss -0.3812
2025-10-14 15:15:53.831444: Pseudo dice [np.float32(0.6577)]
2025-10-14 15:15:53.831604: Epoch time: 46.07 s
2025-10-14 15:15:53.831714: Yayy! New best EMA pseudo Dice: 0.5242999792098999
2025-10-14 15:15:54.889125: 
2025-10-14 15:15:54.889446: Epoch 4
2025-10-14 15:15:54.889606: Current learning rate: 0.00976
2025-10-14 15:16:41.009449: Validation loss did not improve from -0.38120. Patience: 1/50
2025-10-14 15:16:41.009940: train_loss -0.4185
2025-10-14 15:16:41.010085: val_loss -0.343
2025-10-14 15:16:41.010201: Pseudo dice [np.float32(0.629)]
2025-10-14 15:16:41.010382: Epoch time: 46.12 s
2025-10-14 15:16:41.402646: Yayy! New best EMA pseudo Dice: 0.5346999764442444
2025-10-14 15:16:42.459491: 
2025-10-14 15:16:42.459863: Epoch 5
2025-10-14 15:16:42.460119: Current learning rate: 0.0097
2025-10-14 15:17:28.592442: Validation loss did not improve from -0.38120. Patience: 2/50
2025-10-14 15:17:28.592883: train_loss -0.4297
2025-10-14 15:17:28.593029: val_loss -0.3732
2025-10-14 15:17:28.593184: Pseudo dice [np.float32(0.6391)]
2025-10-14 15:17:28.593342: Epoch time: 46.13 s
2025-10-14 15:17:28.593453: Yayy! New best EMA pseudo Dice: 0.545199990272522
2025-10-14 15:17:29.653001: 
2025-10-14 15:17:29.653327: Epoch 6
2025-10-14 15:17:29.653561: Current learning rate: 0.00964
2025-10-14 15:18:15.733380: Validation loss improved from -0.38120 to -0.42340! Patience: 2/50
2025-10-14 15:18:15.734524: train_loss -0.4663
2025-10-14 15:18:15.734832: val_loss -0.4234
2025-10-14 15:18:15.735121: Pseudo dice [np.float32(0.6753)]
2025-10-14 15:18:15.735371: Epoch time: 46.08 s
2025-10-14 15:18:15.735646: Yayy! New best EMA pseudo Dice: 0.5582000017166138
2025-10-14 15:18:16.799484: 
2025-10-14 15:18:16.799762: Epoch 7
2025-10-14 15:18:16.799953: Current learning rate: 0.00958
2025-10-14 15:19:02.890124: Validation loss did not improve from -0.42340. Patience: 1/50
2025-10-14 15:19:02.890630: train_loss -0.4793
2025-10-14 15:19:02.890911: val_loss -0.4086
2025-10-14 15:19:02.891109: Pseudo dice [np.float32(0.6447)]
2025-10-14 15:19:02.891326: Epoch time: 46.09 s
2025-10-14 15:19:02.891510: Yayy! New best EMA pseudo Dice: 0.5667999982833862
2025-10-14 15:19:03.968640: 
2025-10-14 15:19:03.969306: Epoch 8
2025-10-14 15:19:03.969885: Current learning rate: 0.00952
2025-10-14 15:19:50.109105: Validation loss improved from -0.42340 to -0.43338! Patience: 1/50
2025-10-14 15:19:50.109903: train_loss -0.4768
2025-10-14 15:19:50.110240: val_loss -0.4334
2025-10-14 15:19:50.110418: Pseudo dice [np.float32(0.6684)]
2025-10-14 15:19:50.110612: Epoch time: 46.14 s
2025-10-14 15:19:50.110763: Yayy! New best EMA pseudo Dice: 0.5770000219345093
2025-10-14 15:19:51.155981: 
2025-10-14 15:19:51.156611: Epoch 9
2025-10-14 15:19:51.157265: Current learning rate: 0.00946
2025-10-14 15:20:37.342978: Validation loss did not improve from -0.43338. Patience: 1/50
2025-10-14 15:20:37.343652: train_loss -0.4974
2025-10-14 15:20:37.343962: val_loss -0.4171
2025-10-14 15:20:37.344223: Pseudo dice [np.float32(0.6675)]
2025-10-14 15:20:37.344384: Epoch time: 46.19 s
2025-10-14 15:20:37.781976: Yayy! New best EMA pseudo Dice: 0.5860000252723694
2025-10-14 15:20:38.821035: 
2025-10-14 15:20:38.821347: Epoch 10
2025-10-14 15:20:38.821542: Current learning rate: 0.0094
2025-10-14 15:21:25.008500: Validation loss improved from -0.43338 to -0.45422! Patience: 1/50
2025-10-14 15:21:25.009080: train_loss -0.4945
2025-10-14 15:21:25.009217: val_loss -0.4542
2025-10-14 15:21:25.009325: Pseudo dice [np.float32(0.6841)]
2025-10-14 15:21:25.009459: Epoch time: 46.19 s
2025-10-14 15:21:25.009591: Yayy! New best EMA pseudo Dice: 0.59579998254776
2025-10-14 15:21:26.079829: 
2025-10-14 15:21:26.080298: Epoch 11
2025-10-14 15:21:26.080460: Current learning rate: 0.00934
2025-10-14 15:22:12.210158: Validation loss did not improve from -0.45422. Patience: 1/50
2025-10-14 15:22:12.210576: train_loss -0.4975
2025-10-14 15:22:12.210748: val_loss -0.4329
2025-10-14 15:22:12.210862: Pseudo dice [np.float32(0.6763)]
2025-10-14 15:22:12.211037: Epoch time: 46.13 s
2025-10-14 15:22:12.211141: Yayy! New best EMA pseudo Dice: 0.6039000153541565
2025-10-14 15:22:13.273235: 
2025-10-14 15:22:13.273516: Epoch 12
2025-10-14 15:22:13.273762: Current learning rate: 0.00928
2025-10-14 15:22:59.421134: Validation loss did not improve from -0.45422. Patience: 2/50
2025-10-14 15:22:59.421991: train_loss -0.5117
2025-10-14 15:22:59.422180: val_loss -0.4434
2025-10-14 15:22:59.422405: Pseudo dice [np.float32(0.6811)]
2025-10-14 15:22:59.422641: Epoch time: 46.15 s
2025-10-14 15:22:59.422891: Yayy! New best EMA pseudo Dice: 0.6115999817848206
2025-10-14 15:23:00.972928: 
2025-10-14 15:23:00.973264: Epoch 13
2025-10-14 15:23:00.973467: Current learning rate: 0.00922
2025-10-14 15:23:47.126311: Validation loss improved from -0.45422 to -0.45482! Patience: 2/50
2025-10-14 15:23:47.126695: train_loss -0.5276
2025-10-14 15:23:47.126859: val_loss -0.4548
2025-10-14 15:23:47.127036: Pseudo dice [np.float32(0.696)]
2025-10-14 15:23:47.127166: Epoch time: 46.15 s
2025-10-14 15:23:47.127295: Yayy! New best EMA pseudo Dice: 0.6200000047683716
2025-10-14 15:23:48.209183: 
2025-10-14 15:23:48.209692: Epoch 14
2025-10-14 15:23:48.210225: Current learning rate: 0.00916
2025-10-14 15:24:34.334021: Validation loss did not improve from -0.45482. Patience: 1/50
2025-10-14 15:24:34.334658: train_loss -0.5287
2025-10-14 15:24:34.334826: val_loss -0.4278
2025-10-14 15:24:34.334953: Pseudo dice [np.float32(0.6788)]
2025-10-14 15:24:34.335096: Epoch time: 46.13 s
2025-10-14 15:24:34.791506: Yayy! New best EMA pseudo Dice: 0.6258999705314636
2025-10-14 15:24:35.874756: 
2025-10-14 15:24:35.875041: Epoch 15
2025-10-14 15:24:35.875258: Current learning rate: 0.0091
2025-10-14 15:25:22.002351: Validation loss improved from -0.45482 to -0.46547! Patience: 1/50
2025-10-14 15:25:22.002791: train_loss -0.5392
2025-10-14 15:25:22.003010: val_loss -0.4655
2025-10-14 15:25:22.003215: Pseudo dice [np.float32(0.6871)]
2025-10-14 15:25:22.003431: Epoch time: 46.13 s
2025-10-14 15:25:22.003640: Yayy! New best EMA pseudo Dice: 0.6320000290870667
2025-10-14 15:25:23.133262: 
2025-10-14 15:25:23.133551: Epoch 16
2025-10-14 15:25:23.133747: Current learning rate: 0.00903
2025-10-14 15:26:09.263823: Validation loss did not improve from -0.46547. Patience: 1/50
2025-10-14 15:26:09.264311: train_loss -0.5469
2025-10-14 15:26:09.264638: val_loss -0.4566
2025-10-14 15:26:09.264839: Pseudo dice [np.float32(0.6923)]
2025-10-14 15:26:09.264970: Epoch time: 46.13 s
2025-10-14 15:26:09.265091: Yayy! New best EMA pseudo Dice: 0.6381000280380249
2025-10-14 15:26:10.313543: 
2025-10-14 15:26:10.313897: Epoch 17
2025-10-14 15:26:10.314067: Current learning rate: 0.00897
2025-10-14 15:26:56.479374: Validation loss did not improve from -0.46547. Patience: 2/50
2025-10-14 15:26:56.479973: train_loss -0.5569
2025-10-14 15:26:56.480315: val_loss -0.451
2025-10-14 15:26:56.480656: Pseudo dice [np.float32(0.68)]
2025-10-14 15:26:56.481030: Epoch time: 46.17 s
2025-10-14 15:26:56.481166: Yayy! New best EMA pseudo Dice: 0.6421999931335449
2025-10-14 15:26:57.538059: 
2025-10-14 15:26:57.538380: Epoch 18
2025-10-14 15:26:57.538546: Current learning rate: 0.00891
2025-10-14 15:27:43.708459: Validation loss did not improve from -0.46547. Patience: 3/50
2025-10-14 15:27:43.709092: train_loss -0.5612
2025-10-14 15:27:43.709234: val_loss -0.4346
2025-10-14 15:27:43.709345: Pseudo dice [np.float32(0.6743)]
2025-10-14 15:27:43.709467: Epoch time: 46.17 s
2025-10-14 15:27:43.709575: Yayy! New best EMA pseudo Dice: 0.6455000042915344
2025-10-14 15:27:44.781780: 
2025-10-14 15:27:44.782087: Epoch 19
2025-10-14 15:27:44.782261: Current learning rate: 0.00885
2025-10-14 15:28:30.969417: Validation loss improved from -0.46547 to -0.47767! Patience: 3/50
2025-10-14 15:28:30.969816: train_loss -0.5506
2025-10-14 15:28:30.969963: val_loss -0.4777
2025-10-14 15:28:30.970098: Pseudo dice [np.float32(0.697)]
2025-10-14 15:28:30.970337: Epoch time: 46.19 s
2025-10-14 15:28:31.406838: Yayy! New best EMA pseudo Dice: 0.650600016117096
2025-10-14 15:28:32.466322: 
2025-10-14 15:28:32.466801: Epoch 20
2025-10-14 15:28:32.467289: Current learning rate: 0.00879
2025-10-14 15:29:18.652611: Validation loss did not improve from -0.47767. Patience: 1/50
2025-10-14 15:29:18.653120: train_loss -0.569
2025-10-14 15:29:18.653275: val_loss -0.4673
2025-10-14 15:29:18.653403: Pseudo dice [np.float32(0.6938)]
2025-10-14 15:29:18.653544: Epoch time: 46.19 s
2025-10-14 15:29:18.653691: Yayy! New best EMA pseudo Dice: 0.6549000144004822
2025-10-14 15:29:19.719369: 
2025-10-14 15:29:19.719706: Epoch 21
2025-10-14 15:29:19.719940: Current learning rate: 0.00873
2025-10-14 15:30:05.907637: Validation loss improved from -0.47767 to -0.47892! Patience: 1/50
2025-10-14 15:30:05.908123: train_loss -0.5628
2025-10-14 15:30:05.908336: val_loss -0.4789
2025-10-14 15:30:05.908511: Pseudo dice [np.float32(0.6929)]
2025-10-14 15:30:05.908709: Epoch time: 46.19 s
2025-10-14 15:30:05.908885: Yayy! New best EMA pseudo Dice: 0.6586999893188477
2025-10-14 15:30:06.944825: 
2025-10-14 15:30:06.945236: Epoch 22
2025-10-14 15:30:06.945492: Current learning rate: 0.00867
2025-10-14 15:30:53.119910: Validation loss did not improve from -0.47892. Patience: 1/50
2025-10-14 15:30:53.121217: train_loss -0.5632
2025-10-14 15:30:53.121625: val_loss -0.4595
2025-10-14 15:30:53.121956: Pseudo dice [np.float32(0.6966)]
2025-10-14 15:30:53.122207: Epoch time: 46.18 s
2025-10-14 15:30:53.122427: Yayy! New best EMA pseudo Dice: 0.6625000238418579
2025-10-14 15:30:54.181219: 
2025-10-14 15:30:54.181545: Epoch 23
2025-10-14 15:30:54.181755: Current learning rate: 0.00861
2025-10-14 15:31:40.345472: Validation loss did not improve from -0.47892. Patience: 2/50
2025-10-14 15:31:40.345932: train_loss -0.5794
2025-10-14 15:31:40.346089: val_loss -0.4538
2025-10-14 15:31:40.346205: Pseudo dice [np.float32(0.6892)]
2025-10-14 15:31:40.346473: Epoch time: 46.17 s
2025-10-14 15:31:40.346800: Yayy! New best EMA pseudo Dice: 0.6651999950408936
2025-10-14 15:31:41.400564: 
2025-10-14 15:31:41.400808: Epoch 24
2025-10-14 15:31:41.401002: Current learning rate: 0.00855
2025-10-14 15:32:27.578557: Validation loss did not improve from -0.47892. Patience: 3/50
2025-10-14 15:32:27.579198: train_loss -0.58
2025-10-14 15:32:27.579373: val_loss -0.4583
2025-10-14 15:32:27.579529: Pseudo dice [np.float32(0.6841)]
2025-10-14 15:32:27.579716: Epoch time: 46.18 s
2025-10-14 15:32:28.025403: Yayy! New best EMA pseudo Dice: 0.6671000123023987
2025-10-14 15:32:29.097219: 
2025-10-14 15:32:29.097493: Epoch 25
2025-10-14 15:32:29.097665: Current learning rate: 0.00849
2025-10-14 15:33:15.291528: Validation loss did not improve from -0.47892. Patience: 4/50
2025-10-14 15:33:15.292005: train_loss -0.5846
2025-10-14 15:33:15.292145: val_loss -0.4476
2025-10-14 15:33:15.292320: Pseudo dice [np.float32(0.6786)]
2025-10-14 15:33:15.292554: Epoch time: 46.2 s
2025-10-14 15:33:15.292747: Yayy! New best EMA pseudo Dice: 0.6682000160217285
2025-10-14 15:33:16.355853: 
2025-10-14 15:33:16.356099: Epoch 26
2025-10-14 15:33:16.356285: Current learning rate: 0.00843
2025-10-14 15:34:02.560978: Validation loss did not improve from -0.47892. Patience: 5/50
2025-10-14 15:34:02.561614: train_loss -0.5968
2025-10-14 15:34:02.561837: val_loss -0.4701
2025-10-14 15:34:02.561950: Pseudo dice [np.float32(0.6953)]
2025-10-14 15:34:02.562072: Epoch time: 46.21 s
2025-10-14 15:34:02.562174: Yayy! New best EMA pseudo Dice: 0.6708999872207642
2025-10-14 15:34:03.621826: 
2025-10-14 15:34:03.622086: Epoch 27
2025-10-14 15:34:03.622247: Current learning rate: 0.00836
2025-10-14 15:34:49.801914: Validation loss improved from -0.47892 to -0.48443! Patience: 5/50
2025-10-14 15:34:49.802442: train_loss -0.6039
2025-10-14 15:34:49.802749: val_loss -0.4844
2025-10-14 15:34:49.802978: Pseudo dice [np.float32(0.7052)]
2025-10-14 15:34:49.803233: Epoch time: 46.18 s
2025-10-14 15:34:49.803427: Yayy! New best EMA pseudo Dice: 0.6743999719619751
2025-10-14 15:34:51.206757: 
2025-10-14 15:34:51.207134: Epoch 28
2025-10-14 15:34:51.207326: Current learning rate: 0.0083
2025-10-14 15:35:37.413639: Validation loss improved from -0.48443 to -0.48807! Patience: 0/50
2025-10-14 15:35:37.414510: train_loss -0.6095
2025-10-14 15:35:37.414800: val_loss -0.4881
2025-10-14 15:35:37.415068: Pseudo dice [np.float32(0.7077)]
2025-10-14 15:35:37.415360: Epoch time: 46.21 s
2025-10-14 15:35:37.415656: Yayy! New best EMA pseudo Dice: 0.6776999831199646
2025-10-14 15:35:38.476703: 
2025-10-14 15:35:38.477129: Epoch 29
2025-10-14 15:35:38.477423: Current learning rate: 0.00824
2025-10-14 15:36:24.668780: Validation loss did not improve from -0.48807. Patience: 1/50
2025-10-14 15:36:24.669160: train_loss -0.605
2025-10-14 15:36:24.669329: val_loss -0.4848
2025-10-14 15:36:24.669483: Pseudo dice [np.float32(0.7019)]
2025-10-14 15:36:24.669620: Epoch time: 46.19 s
2025-10-14 15:36:25.114602: Yayy! New best EMA pseudo Dice: 0.6801000237464905
2025-10-14 15:36:26.158328: 
2025-10-14 15:36:26.158792: Epoch 30
2025-10-14 15:36:26.159240: Current learning rate: 0.00818
2025-10-14 15:37:12.305136: Validation loss did not improve from -0.48807. Patience: 2/50
2025-10-14 15:37:12.305850: train_loss -0.6166
2025-10-14 15:37:12.306118: val_loss -0.4784
2025-10-14 15:37:12.306345: Pseudo dice [np.float32(0.7005)]
2025-10-14 15:37:12.306872: Epoch time: 46.15 s
2025-10-14 15:37:12.307328: Yayy! New best EMA pseudo Dice: 0.6820999979972839
2025-10-14 15:37:13.378941: 
2025-10-14 15:37:13.379285: Epoch 31
2025-10-14 15:37:13.379501: Current learning rate: 0.00812
2025-10-14 15:37:59.531095: Validation loss improved from -0.48807 to -0.53196! Patience: 2/50
2025-10-14 15:37:59.531560: train_loss -0.6134
2025-10-14 15:37:59.531711: val_loss -0.532
2025-10-14 15:37:59.531839: Pseudo dice [np.float32(0.7287)]
2025-10-14 15:37:59.532014: Epoch time: 46.15 s
2025-10-14 15:37:59.532127: Yayy! New best EMA pseudo Dice: 0.6868000030517578
2025-10-14 15:38:00.584747: 
2025-10-14 15:38:00.585110: Epoch 32
2025-10-14 15:38:00.585348: Current learning rate: 0.00806
2025-10-14 15:38:46.774297: Validation loss did not improve from -0.53196. Patience: 1/50
2025-10-14 15:38:46.774880: train_loss -0.6125
2025-10-14 15:38:46.775028: val_loss -0.4879
2025-10-14 15:38:46.775170: Pseudo dice [np.float32(0.7096)]
2025-10-14 15:38:46.775297: Epoch time: 46.19 s
2025-10-14 15:38:46.775614: Yayy! New best EMA pseudo Dice: 0.6891000270843506
2025-10-14 15:38:47.827927: 
2025-10-14 15:38:47.828192: Epoch 33
2025-10-14 15:38:47.828462: Current learning rate: 0.008
2025-10-14 15:39:33.995061: Validation loss did not improve from -0.53196. Patience: 2/50
2025-10-14 15:39:33.995561: train_loss -0.6179
2025-10-14 15:39:33.995807: val_loss -0.4985
2025-10-14 15:39:33.995946: Pseudo dice [np.float32(0.7133)]
2025-10-14 15:39:33.996153: Epoch time: 46.17 s
2025-10-14 15:39:33.996284: Yayy! New best EMA pseudo Dice: 0.6915000081062317
2025-10-14 15:39:35.041180: 
2025-10-14 15:39:35.041456: Epoch 34
2025-10-14 15:39:35.041620: Current learning rate: 0.00793
2025-10-14 15:40:21.145238: Validation loss did not improve from -0.53196. Patience: 3/50
2025-10-14 15:40:21.145816: train_loss -0.6254
2025-10-14 15:40:21.146003: val_loss -0.5016
2025-10-14 15:40:21.146137: Pseudo dice [np.float32(0.7101)]
2025-10-14 15:40:21.146277: Epoch time: 46.11 s
2025-10-14 15:40:21.573749: Yayy! New best EMA pseudo Dice: 0.6934000253677368
2025-10-14 15:40:22.603416: 
2025-10-14 15:40:22.603737: Epoch 35
2025-10-14 15:40:22.603914: Current learning rate: 0.00787
2025-10-14 15:41:08.747235: Validation loss did not improve from -0.53196. Patience: 4/50
2025-10-14 15:41:08.747738: train_loss -0.6251
2025-10-14 15:41:08.748054: val_loss -0.4898
2025-10-14 15:41:08.748399: Pseudo dice [np.float32(0.7042)]
2025-10-14 15:41:08.748645: Epoch time: 46.14 s
2025-10-14 15:41:08.748951: Yayy! New best EMA pseudo Dice: 0.6944000124931335
2025-10-14 15:41:09.828283: 
2025-10-14 15:41:09.828544: Epoch 36
2025-10-14 15:41:09.828736: Current learning rate: 0.00781
2025-10-14 15:41:55.994439: Validation loss did not improve from -0.53196. Patience: 5/50
2025-10-14 15:41:55.994982: train_loss -0.6238
2025-10-14 15:41:55.995147: val_loss -0.4998
2025-10-14 15:41:55.995284: Pseudo dice [np.float32(0.713)]
2025-10-14 15:41:55.995410: Epoch time: 46.17 s
2025-10-14 15:41:55.995512: Yayy! New best EMA pseudo Dice: 0.6963000297546387
2025-10-14 15:41:57.061569: 
2025-10-14 15:41:57.061846: Epoch 37
2025-10-14 15:41:57.062018: Current learning rate: 0.00775
2025-10-14 15:42:43.255130: Validation loss did not improve from -0.53196. Patience: 6/50
2025-10-14 15:42:43.255517: train_loss -0.6275
2025-10-14 15:42:43.255661: val_loss -0.5071
2025-10-14 15:42:43.255818: Pseudo dice [np.float32(0.7163)]
2025-10-14 15:42:43.255966: Epoch time: 46.19 s
2025-10-14 15:42:43.256151: Yayy! New best EMA pseudo Dice: 0.6983000040054321
2025-10-14 15:42:44.315190: 
2025-10-14 15:42:44.315656: Epoch 38
2025-10-14 15:42:44.315977: Current learning rate: 0.00769
2025-10-14 15:43:30.530911: Validation loss did not improve from -0.53196. Patience: 7/50
2025-10-14 15:43:30.531499: train_loss -0.6264
2025-10-14 15:43:30.531700: val_loss -0.5237
2025-10-14 15:43:30.531854: Pseudo dice [np.float32(0.7355)]
2025-10-14 15:43:30.532019: Epoch time: 46.22 s
2025-10-14 15:43:30.532201: Yayy! New best EMA pseudo Dice: 0.7020000219345093
2025-10-14 15:43:31.596079: 
2025-10-14 15:43:31.596384: Epoch 39
2025-10-14 15:43:31.596572: Current learning rate: 0.00763
2025-10-14 15:44:17.815247: Validation loss did not improve from -0.53196. Patience: 8/50
2025-10-14 15:44:17.815638: train_loss -0.6321
2025-10-14 15:44:17.815777: val_loss -0.5021
2025-10-14 15:44:17.815907: Pseudo dice [np.float32(0.7152)]
2025-10-14 15:44:17.816051: Epoch time: 46.22 s
2025-10-14 15:44:18.253351: Yayy! New best EMA pseudo Dice: 0.7032999992370605
2025-10-14 15:44:19.302253: 
2025-10-14 15:44:19.302528: Epoch 40
2025-10-14 15:44:19.302752: Current learning rate: 0.00756
2025-10-14 15:45:05.509117: Validation loss did not improve from -0.53196. Patience: 9/50
2025-10-14 15:45:05.509720: train_loss -0.6382
2025-10-14 15:45:05.509907: val_loss -0.4931
2025-10-14 15:45:05.510029: Pseudo dice [np.float32(0.7178)]
2025-10-14 15:45:05.510168: Epoch time: 46.21 s
2025-10-14 15:45:05.510296: Yayy! New best EMA pseudo Dice: 0.704800009727478
2025-10-14 15:45:06.582596: 
2025-10-14 15:45:06.582916: Epoch 41
2025-10-14 15:45:06.583124: Current learning rate: 0.0075
2025-10-14 15:45:52.807359: Validation loss improved from -0.53196 to -0.53298! Patience: 9/50
2025-10-14 15:45:52.807806: train_loss -0.644
2025-10-14 15:45:52.807996: val_loss -0.533
2025-10-14 15:45:52.808132: Pseudo dice [np.float32(0.732)]
2025-10-14 15:45:52.808283: Epoch time: 46.23 s
2025-10-14 15:45:52.808395: Yayy! New best EMA pseudo Dice: 0.7074999809265137
2025-10-14 15:45:53.843540: 
2025-10-14 15:45:53.843863: Epoch 42
2025-10-14 15:45:53.844036: Current learning rate: 0.00744
2025-10-14 15:46:40.032643: Validation loss did not improve from -0.53298. Patience: 1/50
2025-10-14 15:46:40.033267: train_loss -0.6352
2025-10-14 15:46:40.033463: val_loss -0.5033
2025-10-14 15:46:40.033607: Pseudo dice [np.float32(0.7154)]
2025-10-14 15:46:40.033734: Epoch time: 46.19 s
2025-10-14 15:46:40.033849: Yayy! New best EMA pseudo Dice: 0.708299994468689
2025-10-14 15:46:41.083034: 
2025-10-14 15:46:41.083370: Epoch 43
2025-10-14 15:46:41.083552: Current learning rate: 0.00738
2025-10-14 15:47:27.324616: Validation loss did not improve from -0.53298. Patience: 2/50
2025-10-14 15:47:27.324990: train_loss -0.6402
2025-10-14 15:47:27.325157: val_loss -0.4794
2025-10-14 15:47:27.325307: Pseudo dice [np.float32(0.7053)]
2025-10-14 15:47:27.325453: Epoch time: 46.24 s
2025-10-14 15:47:28.333846: 
2025-10-14 15:47:28.334111: Epoch 44
2025-10-14 15:47:28.334333: Current learning rate: 0.00732
2025-10-14 15:48:14.510804: Validation loss did not improve from -0.53298. Patience: 3/50
2025-10-14 15:48:14.511814: train_loss -0.6469
2025-10-14 15:48:14.511951: val_loss -0.5107
2025-10-14 15:48:14.512058: Pseudo dice [np.float32(0.7143)]
2025-10-14 15:48:14.512177: Epoch time: 46.18 s
2025-10-14 15:48:14.952267: Yayy! New best EMA pseudo Dice: 0.7085999846458435
2025-10-14 15:48:15.983840: 
2025-10-14 15:48:15.984168: Epoch 45
2025-10-14 15:48:15.984371: Current learning rate: 0.00725
2025-10-14 15:49:02.135891: Validation loss did not improve from -0.53298. Patience: 4/50
2025-10-14 15:49:02.136317: train_loss -0.6481
2025-10-14 15:49:02.136502: val_loss -0.4968
2025-10-14 15:49:02.136645: Pseudo dice [np.float32(0.7185)]
2025-10-14 15:49:02.136793: Epoch time: 46.15 s
2025-10-14 15:49:02.136930: Yayy! New best EMA pseudo Dice: 0.7095999717712402
2025-10-14 15:49:03.185588: 
2025-10-14 15:49:03.186104: Epoch 46
2025-10-14 15:49:03.186476: Current learning rate: 0.00719
2025-10-14 15:49:49.316288: Validation loss did not improve from -0.53298. Patience: 5/50
2025-10-14 15:49:49.316901: train_loss -0.6497
2025-10-14 15:49:49.317142: val_loss -0.4813
2025-10-14 15:49:49.317349: Pseudo dice [np.float32(0.7046)]
2025-10-14 15:49:49.317565: Epoch time: 46.13 s
2025-10-14 15:49:49.936749: 
2025-10-14 15:49:49.937075: Epoch 47
2025-10-14 15:49:49.937280: Current learning rate: 0.00713
2025-10-14 15:50:36.139156: Validation loss did not improve from -0.53298. Patience: 6/50
2025-10-14 15:50:36.139555: train_loss -0.6475
2025-10-14 15:50:36.139717: val_loss -0.5281
2025-10-14 15:50:36.139856: Pseudo dice [np.float32(0.7303)]
2025-10-14 15:50:36.139994: Epoch time: 46.2 s
2025-10-14 15:50:36.140119: Yayy! New best EMA pseudo Dice: 0.7111999988555908
2025-10-14 15:50:37.185888: 
2025-10-14 15:50:37.186177: Epoch 48
2025-10-14 15:50:37.186389: Current learning rate: 0.00707
2025-10-14 15:51:23.357907: Validation loss did not improve from -0.53298. Patience: 7/50
2025-10-14 15:51:23.358525: train_loss -0.6494
2025-10-14 15:51:23.358662: val_loss -0.4978
2025-10-14 15:51:23.358791: Pseudo dice [np.float32(0.7118)]
2025-10-14 15:51:23.358946: Epoch time: 46.17 s
2025-10-14 15:51:23.359057: Yayy! New best EMA pseudo Dice: 0.7113000154495239
2025-10-14 15:51:24.432393: 
2025-10-14 15:51:24.432713: Epoch 49
2025-10-14 15:51:24.432914: Current learning rate: 0.007
2025-10-14 15:52:10.571158: Validation loss did not improve from -0.53298. Patience: 8/50
2025-10-14 15:52:10.571606: train_loss -0.6593
2025-10-14 15:52:10.571777: val_loss -0.4999
2025-10-14 15:52:10.571922: Pseudo dice [np.float32(0.7193)]
2025-10-14 15:52:10.572259: Epoch time: 46.14 s
2025-10-14 15:52:11.002221: Yayy! New best EMA pseudo Dice: 0.7121000289916992
2025-10-14 15:52:12.057461: 
2025-10-14 15:52:12.057732: Epoch 50
2025-10-14 15:52:12.057922: Current learning rate: 0.00694
2025-10-14 15:52:58.191344: Validation loss did not improve from -0.53298. Patience: 9/50
2025-10-14 15:52:58.192079: train_loss -0.6398
2025-10-14 15:52:58.192355: val_loss -0.5078
2025-10-14 15:52:58.192541: Pseudo dice [np.float32(0.7176)]
2025-10-14 15:52:58.192725: Epoch time: 46.14 s
2025-10-14 15:52:58.192850: Yayy! New best EMA pseudo Dice: 0.7125999927520752
2025-10-14 15:52:59.238655: 
2025-10-14 15:52:59.238971: Epoch 51
2025-10-14 15:52:59.239168: Current learning rate: 0.00688
2025-10-14 15:53:45.434042: Validation loss did not improve from -0.53298. Patience: 10/50
2025-10-14 15:53:45.434563: train_loss -0.6463
2025-10-14 15:53:45.434695: val_loss -0.5113
2025-10-14 15:53:45.434801: Pseudo dice [np.float32(0.7254)]
2025-10-14 15:53:45.434926: Epoch time: 46.2 s
2025-10-14 15:53:45.435045: Yayy! New best EMA pseudo Dice: 0.7139000296592712
2025-10-14 15:53:46.483705: 
2025-10-14 15:53:46.484317: Epoch 52
2025-10-14 15:53:46.484778: Current learning rate: 0.00682
2025-10-14 15:54:32.654162: Validation loss did not improve from -0.53298. Patience: 11/50
2025-10-14 15:54:32.655113: train_loss -0.6607
2025-10-14 15:54:32.655480: val_loss -0.4899
2025-10-14 15:54:32.655818: Pseudo dice [np.float32(0.7126)]
2025-10-14 15:54:32.656165: Epoch time: 46.17 s
2025-10-14 15:54:33.285831: 
2025-10-14 15:54:33.286128: Epoch 53
2025-10-14 15:54:33.286316: Current learning rate: 0.00675
2025-10-14 15:55:19.501875: Validation loss improved from -0.53298 to -0.53582! Patience: 11/50
2025-10-14 15:55:19.502306: train_loss -0.6679
2025-10-14 15:55:19.502479: val_loss -0.5358
2025-10-14 15:55:19.502592: Pseudo dice [np.float32(0.7403)]
2025-10-14 15:55:19.502753: Epoch time: 46.22 s
2025-10-14 15:55:19.502871: Yayy! New best EMA pseudo Dice: 0.7164000272750854
2025-10-14 15:55:20.566018: 
2025-10-14 15:55:20.566343: Epoch 54
2025-10-14 15:55:20.566575: Current learning rate: 0.00669
2025-10-14 15:56:06.725380: Validation loss did not improve from -0.53582. Patience: 1/50
2025-10-14 15:56:06.725925: train_loss -0.6728
2025-10-14 15:56:06.726088: val_loss -0.5164
2025-10-14 15:56:06.726226: Pseudo dice [np.float32(0.7221)]
2025-10-14 15:56:06.726373: Epoch time: 46.16 s
2025-10-14 15:56:07.165991: Yayy! New best EMA pseudo Dice: 0.7170000076293945
2025-10-14 15:56:08.203090: 
2025-10-14 15:56:08.203610: Epoch 55
2025-10-14 15:56:08.203986: Current learning rate: 0.00663
2025-10-14 15:56:54.335432: Validation loss did not improve from -0.53582. Patience: 2/50
2025-10-14 15:56:54.335977: train_loss -0.6716
2025-10-14 15:56:54.336299: val_loss -0.5248
2025-10-14 15:56:54.336661: Pseudo dice [np.float32(0.7283)]
2025-10-14 15:56:54.337062: Epoch time: 46.13 s
2025-10-14 15:56:54.337445: Yayy! New best EMA pseudo Dice: 0.7181000113487244
2025-10-14 15:56:55.409553: 
2025-10-14 15:56:55.410065: Epoch 56
2025-10-14 15:56:55.410429: Current learning rate: 0.00657
2025-10-14 15:57:41.560894: Validation loss did not improve from -0.53582. Patience: 3/50
2025-10-14 15:57:41.561483: train_loss -0.6729
2025-10-14 15:57:41.561687: val_loss -0.5304
2025-10-14 15:57:41.561830: Pseudo dice [np.float32(0.7236)]
2025-10-14 15:57:41.561985: Epoch time: 46.15 s
2025-10-14 15:57:41.562123: Yayy! New best EMA pseudo Dice: 0.7186999917030334
2025-10-14 15:57:42.613283: 
2025-10-14 15:57:42.613606: Epoch 57
2025-10-14 15:57:42.613833: Current learning rate: 0.0065
2025-10-14 15:58:28.727717: Validation loss did not improve from -0.53582. Patience: 4/50
2025-10-14 15:58:28.728267: train_loss -0.662
2025-10-14 15:58:28.728422: val_loss -0.4809
2025-10-14 15:58:28.728550: Pseudo dice [np.float32(0.6988)]
2025-10-14 15:58:28.728707: Epoch time: 46.12 s
2025-10-14 15:58:29.357343: 
2025-10-14 15:58:29.357579: Epoch 58
2025-10-14 15:58:29.357744: Current learning rate: 0.00644
2025-10-14 15:59:15.478405: Validation loss did not improve from -0.53582. Patience: 5/50
2025-10-14 15:59:15.479101: train_loss -0.6596
2025-10-14 15:59:15.479323: val_loss -0.5314
2025-10-14 15:59:15.479604: Pseudo dice [np.float32(0.729)]
2025-10-14 15:59:15.479813: Epoch time: 46.12 s
2025-10-14 15:59:16.118988: 
2025-10-14 15:59:16.119298: Epoch 59
2025-10-14 15:59:16.119488: Current learning rate: 0.00638
2025-10-14 16:00:02.268934: Validation loss did not improve from -0.53582. Patience: 6/50
2025-10-14 16:00:02.269342: train_loss -0.6703
2025-10-14 16:00:02.269484: val_loss -0.5173
2025-10-14 16:00:02.269601: Pseudo dice [np.float32(0.7273)]
2025-10-14 16:00:02.269719: Epoch time: 46.15 s
2025-10-14 16:00:02.707796: Yayy! New best EMA pseudo Dice: 0.7189000248908997
2025-10-14 16:00:04.207997: 
2025-10-14 16:00:04.208333: Epoch 60
2025-10-14 16:00:04.208496: Current learning rate: 0.00631
2025-10-14 16:00:50.348994: Validation loss improved from -0.53582 to -0.54129! Patience: 6/50
2025-10-14 16:00:50.349656: train_loss -0.6782
2025-10-14 16:00:50.349900: val_loss -0.5413
2025-10-14 16:00:50.350122: Pseudo dice [np.float32(0.7394)]
2025-10-14 16:00:50.350391: Epoch time: 46.14 s
2025-10-14 16:00:50.350590: Yayy! New best EMA pseudo Dice: 0.7208999991416931
2025-10-14 16:00:51.424735: 
2025-10-14 16:00:51.425007: Epoch 61
2025-10-14 16:00:51.425251: Current learning rate: 0.00625
2025-10-14 16:01:37.602299: Validation loss did not improve from -0.54129. Patience: 1/50
2025-10-14 16:01:37.602877: train_loss -0.6763
2025-10-14 16:01:37.603200: val_loss -0.5229
2025-10-14 16:01:37.603443: Pseudo dice [np.float32(0.7238)]
2025-10-14 16:01:37.603683: Epoch time: 46.18 s
2025-10-14 16:01:37.603936: Yayy! New best EMA pseudo Dice: 0.7211999893188477
2025-10-14 16:01:38.662428: 
2025-10-14 16:01:38.662651: Epoch 62
2025-10-14 16:01:38.662906: Current learning rate: 0.00619
2025-10-14 16:02:24.832358: Validation loss did not improve from -0.54129. Patience: 2/50
2025-10-14 16:02:24.832924: train_loss -0.6796
2025-10-14 16:02:24.833069: val_loss -0.523
2025-10-14 16:02:24.833187: Pseudo dice [np.float32(0.7278)]
2025-10-14 16:02:24.833313: Epoch time: 46.17 s
2025-10-14 16:02:24.833428: Yayy! New best EMA pseudo Dice: 0.7218999862670898
2025-10-14 16:02:25.901609: 
2025-10-14 16:02:25.901920: Epoch 63
2025-10-14 16:02:25.902121: Current learning rate: 0.00612
2025-10-14 16:03:12.114275: Validation loss did not improve from -0.54129. Patience: 3/50
2025-10-14 16:03:12.114646: train_loss -0.6751
2025-10-14 16:03:12.114840: val_loss -0.5257
2025-10-14 16:03:12.114995: Pseudo dice [np.float32(0.7269)]
2025-10-14 16:03:12.115183: Epoch time: 46.21 s
2025-10-14 16:03:12.115358: Yayy! New best EMA pseudo Dice: 0.7224000096321106
2025-10-14 16:03:13.183285: 
2025-10-14 16:03:13.183706: Epoch 64
2025-10-14 16:03:13.183951: Current learning rate: 0.00606
2025-10-14 16:03:59.383590: Validation loss improved from -0.54129 to -0.54977! Patience: 3/50
2025-10-14 16:03:59.384154: train_loss -0.6823
2025-10-14 16:03:59.384320: val_loss -0.5498
2025-10-14 16:03:59.384488: Pseudo dice [np.float32(0.7418)]
2025-10-14 16:03:59.384625: Epoch time: 46.2 s
2025-10-14 16:03:59.811012: Yayy! New best EMA pseudo Dice: 0.7243000268936157
2025-10-14 16:04:00.868735: 
2025-10-14 16:04:00.868969: Epoch 65
2025-10-14 16:04:00.869196: Current learning rate: 0.006
2025-10-14 16:04:47.040583: Validation loss did not improve from -0.54977. Patience: 1/50
2025-10-14 16:04:47.040983: train_loss -0.6797
2025-10-14 16:04:47.041135: val_loss -0.5131
2025-10-14 16:04:47.041396: Pseudo dice [np.float32(0.7222)]
2025-10-14 16:04:47.041554: Epoch time: 46.17 s
2025-10-14 16:04:47.677990: 
2025-10-14 16:04:47.678274: Epoch 66
2025-10-14 16:04:47.678449: Current learning rate: 0.00593
2025-10-14 16:05:33.866268: Validation loss did not improve from -0.54977. Patience: 2/50
2025-10-14 16:05:33.866858: train_loss -0.6808
2025-10-14 16:05:33.867012: val_loss -0.5283
2025-10-14 16:05:33.867125: Pseudo dice [np.float32(0.7347)]
2025-10-14 16:05:33.867283: Epoch time: 46.19 s
2025-10-14 16:05:33.867518: Yayy! New best EMA pseudo Dice: 0.7251999974250793
2025-10-14 16:05:34.937174: 
2025-10-14 16:05:34.937436: Epoch 67
2025-10-14 16:05:34.937685: Current learning rate: 0.00587
2025-10-14 16:06:21.079680: Validation loss did not improve from -0.54977. Patience: 3/50
2025-10-14 16:06:21.080169: train_loss -0.6844
2025-10-14 16:06:21.080405: val_loss -0.5169
2025-10-14 16:06:21.080599: Pseudo dice [np.float32(0.7252)]
2025-10-14 16:06:21.080798: Epoch time: 46.14 s
2025-10-14 16:06:21.081002: Yayy! New best EMA pseudo Dice: 0.7251999974250793
2025-10-14 16:06:22.158710: 
2025-10-14 16:06:22.158973: Epoch 68
2025-10-14 16:06:22.159152: Current learning rate: 0.00581
2025-10-14 16:07:08.248267: Validation loss did not improve from -0.54977. Patience: 4/50
2025-10-14 16:07:08.249289: train_loss -0.6804
2025-10-14 16:07:08.249506: val_loss -0.5239
2025-10-14 16:07:08.249696: Pseudo dice [np.float32(0.7295)]
2025-10-14 16:07:08.249864: Epoch time: 46.09 s
2025-10-14 16:07:08.250074: Yayy! New best EMA pseudo Dice: 0.725600004196167
2025-10-14 16:07:09.312376: 
2025-10-14 16:07:09.312748: Epoch 69
2025-10-14 16:07:09.312991: Current learning rate: 0.00574
2025-10-14 16:07:55.434471: Validation loss did not improve from -0.54977. Patience: 5/50
2025-10-14 16:07:55.434884: train_loss -0.6916
2025-10-14 16:07:55.435055: val_loss -0.5436
2025-10-14 16:07:55.435198: Pseudo dice [np.float32(0.7406)]
2025-10-14 16:07:55.435346: Epoch time: 46.12 s
2025-10-14 16:07:55.863475: Yayy! New best EMA pseudo Dice: 0.7271000146865845
2025-10-14 16:07:56.925296: 
2025-10-14 16:07:56.925571: Epoch 70
2025-10-14 16:07:56.925760: Current learning rate: 0.00568
2025-10-14 16:08:43.072150: Validation loss did not improve from -0.54977. Patience: 6/50
2025-10-14 16:08:43.073223: train_loss -0.6936
2025-10-14 16:08:43.073542: val_loss -0.5128
2025-10-14 16:08:43.073828: Pseudo dice [np.float32(0.7263)]
2025-10-14 16:08:43.074168: Epoch time: 46.15 s
2025-10-14 16:08:43.706247: 
2025-10-14 16:08:43.706594: Epoch 71
2025-10-14 16:08:43.706780: Current learning rate: 0.00562
2025-10-14 16:09:29.865657: Validation loss did not improve from -0.54977. Patience: 7/50
2025-10-14 16:09:29.866037: train_loss -0.6943
2025-10-14 16:09:29.866186: val_loss -0.5226
2025-10-14 16:09:29.866301: Pseudo dice [np.float32(0.7351)]
2025-10-14 16:09:29.866426: Epoch time: 46.16 s
2025-10-14 16:09:29.866549: Yayy! New best EMA pseudo Dice: 0.7278000116348267
2025-10-14 16:09:30.941955: 
2025-10-14 16:09:30.942472: Epoch 72
2025-10-14 16:09:30.942899: Current learning rate: 0.00555
2025-10-14 16:10:17.111434: Validation loss did not improve from -0.54977. Patience: 8/50
2025-10-14 16:10:17.112033: train_loss -0.693
2025-10-14 16:10:17.112273: val_loss -0.5218
2025-10-14 16:10:17.112473: Pseudo dice [np.float32(0.7259)]
2025-10-14 16:10:17.112715: Epoch time: 46.17 s
2025-10-14 16:10:17.752215: 
2025-10-14 16:10:17.752511: Epoch 73
2025-10-14 16:10:17.752730: Current learning rate: 0.00549
2025-10-14 16:11:03.904392: Validation loss did not improve from -0.54977. Patience: 9/50
2025-10-14 16:11:03.904968: train_loss -0.6882
2025-10-14 16:11:03.905308: val_loss -0.5466
2025-10-14 16:11:03.905678: Pseudo dice [np.float32(0.7491)]
2025-10-14 16:11:03.906192: Epoch time: 46.15 s
2025-10-14 16:11:03.906576: Yayy! New best EMA pseudo Dice: 0.7297999858856201
2025-10-14 16:11:04.968771: 
2025-10-14 16:11:04.969032: Epoch 74
2025-10-14 16:11:04.969248: Current learning rate: 0.00542
2025-10-14 16:11:51.121326: Validation loss did not improve from -0.54977. Patience: 10/50
2025-10-14 16:11:51.121900: train_loss -0.702
2025-10-14 16:11:51.122048: val_loss -0.5362
2025-10-14 16:11:51.122246: Pseudo dice [np.float32(0.7371)]
2025-10-14 16:11:51.122468: Epoch time: 46.15 s
2025-10-14 16:11:51.564390: Yayy! New best EMA pseudo Dice: 0.7304999828338623
2025-10-14 16:11:53.045733: 
2025-10-14 16:11:53.045975: Epoch 75
2025-10-14 16:11:53.046159: Current learning rate: 0.00536
2025-10-14 16:12:39.277768: Validation loss improved from -0.54977 to -0.55901! Patience: 10/50
2025-10-14 16:12:39.278233: train_loss -0.6955
2025-10-14 16:12:39.278392: val_loss -0.559
2025-10-14 16:12:39.278497: Pseudo dice [np.float32(0.7559)]
2025-10-14 16:12:39.278622: Epoch time: 46.23 s
2025-10-14 16:12:39.278735: Yayy! New best EMA pseudo Dice: 0.7329999804496765
2025-10-14 16:12:40.358297: 
2025-10-14 16:12:40.358573: Epoch 76
2025-10-14 16:12:40.358750: Current learning rate: 0.00529
2025-10-14 16:13:26.561767: Validation loss did not improve from -0.55901. Patience: 1/50
2025-10-14 16:13:26.563025: train_loss -0.7021
2025-10-14 16:13:26.563387: val_loss -0.5267
2025-10-14 16:13:26.563750: Pseudo dice [np.float32(0.7347)]
2025-10-14 16:13:26.564126: Epoch time: 46.21 s
2025-10-14 16:13:26.564549: Yayy! New best EMA pseudo Dice: 0.7332000136375427
2025-10-14 16:13:27.638356: 
2025-10-14 16:13:27.638714: Epoch 77
2025-10-14 16:13:27.638913: Current learning rate: 0.00523
2025-10-14 16:14:13.893103: Validation loss did not improve from -0.55901. Patience: 2/50
2025-10-14 16:14:13.893503: train_loss -0.6942
2025-10-14 16:14:13.893642: val_loss -0.5248
2025-10-14 16:14:13.893757: Pseudo dice [np.float32(0.7271)]
2025-10-14 16:14:13.893892: Epoch time: 46.26 s
2025-10-14 16:14:14.552190: 
2025-10-14 16:14:14.552770: Epoch 78
2025-10-14 16:14:14.553178: Current learning rate: 0.00517
2025-10-14 16:15:00.825706: Validation loss did not improve from -0.55901. Patience: 3/50
2025-10-14 16:15:00.826421: train_loss -0.7009
2025-10-14 16:15:00.826668: val_loss -0.4989
2025-10-14 16:15:00.826882: Pseudo dice [np.float32(0.7195)]
2025-10-14 16:15:00.827123: Epoch time: 46.27 s
2025-10-14 16:15:01.490846: 
2025-10-14 16:15:01.491228: Epoch 79
2025-10-14 16:15:01.491461: Current learning rate: 0.0051
2025-10-14 16:15:47.764235: Validation loss did not improve from -0.55901. Patience: 4/50
2025-10-14 16:15:47.764724: train_loss -0.6996
2025-10-14 16:15:47.764872: val_loss -0.523
2025-10-14 16:15:47.765034: Pseudo dice [np.float32(0.7318)]
2025-10-14 16:15:47.765169: Epoch time: 46.27 s
2025-10-14 16:15:48.856736: 
2025-10-14 16:15:48.857022: Epoch 80
2025-10-14 16:15:48.857266: Current learning rate: 0.00504
2025-10-14 16:16:35.137759: Validation loss did not improve from -0.55901. Patience: 5/50
2025-10-14 16:16:35.138548: train_loss -0.7081
2025-10-14 16:16:35.138825: val_loss -0.5196
2025-10-14 16:16:35.139091: Pseudo dice [np.float32(0.7375)]
2025-10-14 16:16:35.139306: Epoch time: 46.28 s
2025-10-14 16:16:35.800804: 
2025-10-14 16:16:35.801300: Epoch 81
2025-10-14 16:16:35.801685: Current learning rate: 0.00497
2025-10-14 16:17:22.030352: Validation loss did not improve from -0.55901. Patience: 6/50
2025-10-14 16:17:22.030766: train_loss -0.7138
2025-10-14 16:17:22.030913: val_loss -0.5083
2025-10-14 16:17:22.031063: Pseudo dice [np.float32(0.7243)]
2025-10-14 16:17:22.031197: Epoch time: 46.23 s
2025-10-14 16:17:22.677184: 
2025-10-14 16:17:22.677543: Epoch 82
2025-10-14 16:17:22.677727: Current learning rate: 0.00491
2025-10-14 16:18:08.944011: Validation loss did not improve from -0.55901. Patience: 7/50
2025-10-14 16:18:08.944677: train_loss -0.7113
2025-10-14 16:18:08.944871: val_loss -0.5295
2025-10-14 16:18:08.945007: Pseudo dice [np.float32(0.7369)]
2025-10-14 16:18:08.945141: Epoch time: 46.27 s
2025-10-14 16:18:09.576899: 
2025-10-14 16:18:09.577230: Epoch 83
2025-10-14 16:18:09.577407: Current learning rate: 0.00484
2025-10-14 16:18:55.833566: Validation loss did not improve from -0.55901. Patience: 8/50
2025-10-14 16:18:55.834244: train_loss -0.7105
2025-10-14 16:18:55.834556: val_loss -0.5313
2025-10-14 16:18:55.834916: Pseudo dice [np.float32(0.736)]
2025-10-14 16:18:55.835289: Epoch time: 46.26 s
2025-10-14 16:18:56.465374: 
2025-10-14 16:18:56.465665: Epoch 84
2025-10-14 16:18:56.465869: Current learning rate: 0.00478
2025-10-14 16:19:42.685812: Validation loss did not improve from -0.55901. Patience: 9/50
2025-10-14 16:19:42.686812: train_loss -0.7072
2025-10-14 16:19:42.687213: val_loss -0.512
2025-10-14 16:19:42.687619: Pseudo dice [np.float32(0.7305)]
2025-10-14 16:19:42.687858: Epoch time: 46.22 s
2025-10-14 16:19:43.758087: 
2025-10-14 16:19:43.758598: Epoch 85
2025-10-14 16:19:43.758830: Current learning rate: 0.00471
2025-10-14 16:20:29.982087: Validation loss improved from -0.55901 to -0.57184! Patience: 9/50
2025-10-14 16:20:29.982564: train_loss -0.7051
2025-10-14 16:20:29.982713: val_loss -0.5718
2025-10-14 16:20:29.982838: Pseudo dice [np.float32(0.7553)]
2025-10-14 16:20:29.982978: Epoch time: 46.23 s
2025-10-14 16:20:29.983095: Yayy! New best EMA pseudo Dice: 0.7343000173568726
2025-10-14 16:20:31.062663: 
2025-10-14 16:20:31.063030: Epoch 86
2025-10-14 16:20:31.063224: Current learning rate: 0.00465
2025-10-14 16:21:17.277412: Validation loss did not improve from -0.57184. Patience: 1/50
2025-10-14 16:21:17.278310: train_loss -0.7194
2025-10-14 16:21:17.278563: val_loss -0.5068
2025-10-14 16:21:17.278915: Pseudo dice [np.float32(0.7227)]
2025-10-14 16:21:17.279237: Epoch time: 46.22 s
2025-10-14 16:21:17.911529: 
2025-10-14 16:21:17.911873: Epoch 87
2025-10-14 16:21:17.912077: Current learning rate: 0.00458
2025-10-14 16:22:04.100524: Validation loss did not improve from -0.57184. Patience: 2/50
2025-10-14 16:22:04.101114: train_loss -0.7088
2025-10-14 16:22:04.101535: val_loss -0.4811
2025-10-14 16:22:04.101908: Pseudo dice [np.float32(0.7146)]
2025-10-14 16:22:04.102335: Epoch time: 46.19 s
2025-10-14 16:22:04.742266: 
2025-10-14 16:22:04.742793: Epoch 88
2025-10-14 16:22:04.743097: Current learning rate: 0.00452
2025-10-14 16:22:50.998109: Validation loss did not improve from -0.57184. Patience: 3/50
2025-10-14 16:22:50.998816: train_loss -0.7088
2025-10-14 16:22:50.998987: val_loss -0.5442
2025-10-14 16:22:50.999134: Pseudo dice [np.float32(0.7469)]
2025-10-14 16:22:50.999294: Epoch time: 46.26 s
2025-10-14 16:22:51.627114: 
2025-10-14 16:22:51.627481: Epoch 89
2025-10-14 16:22:51.627669: Current learning rate: 0.00445
2025-10-14 16:23:37.898244: Validation loss did not improve from -0.57184. Patience: 4/50
2025-10-14 16:23:37.898787: train_loss -0.7141
2025-10-14 16:23:37.898949: val_loss -0.5349
2025-10-14 16:23:37.899103: Pseudo dice [np.float32(0.7405)]
2025-10-14 16:23:37.899258: Epoch time: 46.27 s
2025-10-14 16:23:38.970817: 
2025-10-14 16:23:38.971204: Epoch 90
2025-10-14 16:23:38.971434: Current learning rate: 0.00438
2025-10-14 16:24:25.149373: Validation loss did not improve from -0.57184. Patience: 5/50
2025-10-14 16:24:25.150258: train_loss -0.7259
2025-10-14 16:24:25.150531: val_loss -0.563
2025-10-14 16:24:25.150777: Pseudo dice [np.float32(0.7595)]
2025-10-14 16:24:25.151052: Epoch time: 46.18 s
2025-10-14 16:24:25.151271: Yayy! New best EMA pseudo Dice: 0.7361999750137329
2025-10-14 16:24:26.721261: 
2025-10-14 16:24:26.721684: Epoch 91
2025-10-14 16:24:26.721940: Current learning rate: 0.00432
2025-10-14 16:25:12.973283: Validation loss did not improve from -0.57184. Patience: 6/50
2025-10-14 16:25:12.973844: train_loss -0.7252
2025-10-14 16:25:12.974130: val_loss -0.5556
2025-10-14 16:25:12.974484: Pseudo dice [np.float32(0.7555)]
2025-10-14 16:25:12.974690: Epoch time: 46.25 s
2025-10-14 16:25:12.974862: Yayy! New best EMA pseudo Dice: 0.7382000088691711
2025-10-14 16:25:14.046058: 
2025-10-14 16:25:14.046468: Epoch 92
2025-10-14 16:25:14.046786: Current learning rate: 0.00425
2025-10-14 16:26:00.228170: Validation loss did not improve from -0.57184. Patience: 7/50
2025-10-14 16:26:00.228802: train_loss -0.7257
2025-10-14 16:26:00.228985: val_loss -0.507
2025-10-14 16:26:00.229160: Pseudo dice [np.float32(0.729)]
2025-10-14 16:26:00.229291: Epoch time: 46.18 s
2025-10-14 16:26:00.858979: 
2025-10-14 16:26:00.859346: Epoch 93
2025-10-14 16:26:00.859566: Current learning rate: 0.00419
2025-10-14 16:26:47.087486: Validation loss did not improve from -0.57184. Patience: 8/50
2025-10-14 16:26:47.087973: train_loss -0.7271
2025-10-14 16:26:47.088188: val_loss -0.5037
2025-10-14 16:26:47.088339: Pseudo dice [np.float32(0.7148)]
2025-10-14 16:26:47.088526: Epoch time: 46.23 s
2025-10-14 16:26:47.718357: 
2025-10-14 16:26:47.718643: Epoch 94
2025-10-14 16:26:47.718811: Current learning rate: 0.00412
2025-10-14 16:27:33.914166: Validation loss did not improve from -0.57184. Patience: 9/50
2025-10-14 16:27:33.915465: train_loss -0.725
2025-10-14 16:27:33.915818: val_loss -0.5311
2025-10-14 16:27:33.916172: Pseudo dice [np.float32(0.739)]
2025-10-14 16:27:33.916526: Epoch time: 46.2 s
2025-10-14 16:27:35.009919: 
2025-10-14 16:27:35.010268: Epoch 95
2025-10-14 16:27:35.010441: Current learning rate: 0.00405
2025-10-14 16:28:21.189034: Validation loss did not improve from -0.57184. Patience: 10/50
2025-10-14 16:28:21.189492: train_loss -0.7275
2025-10-14 16:28:21.189675: val_loss -0.5499
2025-10-14 16:28:21.189868: Pseudo dice [np.float32(0.7451)]
2025-10-14 16:28:21.190034: Epoch time: 46.18 s
2025-10-14 16:28:21.826226: 
2025-10-14 16:28:21.826564: Epoch 96
2025-10-14 16:28:21.826758: Current learning rate: 0.00399
2025-10-14 16:29:08.093822: Validation loss did not improve from -0.57184. Patience: 11/50
2025-10-14 16:29:08.094387: train_loss -0.7258
2025-10-14 16:29:08.094539: val_loss -0.5498
2025-10-14 16:29:08.094727: Pseudo dice [np.float32(0.7409)]
2025-10-14 16:29:08.094859: Epoch time: 46.27 s
2025-10-14 16:29:08.739338: 
2025-10-14 16:29:08.739663: Epoch 97
2025-10-14 16:29:08.739870: Current learning rate: 0.00392
2025-10-14 16:29:54.993071: Validation loss did not improve from -0.57184. Patience: 12/50
2025-10-14 16:29:54.993537: train_loss -0.7224
2025-10-14 16:29:54.993735: val_loss -0.5473
2025-10-14 16:29:54.993880: Pseudo dice [np.float32(0.7365)]
2025-10-14 16:29:54.994053: Epoch time: 46.25 s
2025-10-14 16:29:55.632275: 
2025-10-14 16:29:55.632685: Epoch 98
2025-10-14 16:29:55.632887: Current learning rate: 0.00385
2025-10-14 16:30:41.846897: Validation loss did not improve from -0.57184. Patience: 13/50
2025-10-14 16:30:41.847524: train_loss -0.7272
2025-10-14 16:30:41.847686: val_loss -0.5452
2025-10-14 16:30:41.847820: Pseudo dice [np.float32(0.7403)]
2025-10-14 16:30:41.847968: Epoch time: 46.22 s
2025-10-14 16:30:42.486046: 
2025-10-14 16:30:42.486338: Epoch 99
2025-10-14 16:30:42.486536: Current learning rate: 0.00379
2025-10-14 16:31:28.587694: Validation loss did not improve from -0.57184. Patience: 14/50
2025-10-14 16:31:28.588290: train_loss -0.7307
2025-10-14 16:31:28.588566: val_loss -0.5316
2025-10-14 16:31:28.588870: Pseudo dice [np.float32(0.7372)]
2025-10-14 16:31:28.589208: Epoch time: 46.1 s
2025-10-14 16:31:29.674747: 
2025-10-14 16:31:29.675402: Epoch 100
2025-10-14 16:31:29.675894: Current learning rate: 0.00372
2025-10-14 16:32:15.808103: Validation loss did not improve from -0.57184. Patience: 15/50
2025-10-14 16:32:15.808764: train_loss -0.7303
2025-10-14 16:32:15.808945: val_loss -0.5523
2025-10-14 16:32:15.809099: Pseudo dice [np.float32(0.7494)]
2025-10-14 16:32:15.809262: Epoch time: 46.13 s
2025-10-14 16:32:15.809407: Yayy! New best EMA pseudo Dice: 0.7383999824523926
2025-10-14 16:32:16.890043: 
2025-10-14 16:32:16.890473: Epoch 101
2025-10-14 16:32:16.890720: Current learning rate: 0.00365
2025-10-14 16:33:03.014570: Validation loss did not improve from -0.57184. Patience: 16/50
2025-10-14 16:33:03.015070: train_loss -0.7292
2025-10-14 16:33:03.015213: val_loss -0.5245
2025-10-14 16:33:03.015342: Pseudo dice [np.float32(0.7341)]
2025-10-14 16:33:03.015502: Epoch time: 46.13 s
2025-10-14 16:33:03.650038: 
2025-10-14 16:33:03.650300: Epoch 102
2025-10-14 16:33:03.650493: Current learning rate: 0.00359
2025-10-14 16:33:49.845741: Validation loss did not improve from -0.57184. Patience: 17/50
2025-10-14 16:33:49.846373: train_loss -0.7319
2025-10-14 16:33:49.846568: val_loss -0.5296
2025-10-14 16:33:49.846743: Pseudo dice [np.float32(0.7383)]
2025-10-14 16:33:49.846923: Epoch time: 46.2 s
2025-10-14 16:33:50.484982: 
2025-10-14 16:33:50.485391: Epoch 103
2025-10-14 16:33:50.485777: Current learning rate: 0.00352
2025-10-14 16:34:36.604289: Validation loss improved from -0.57184 to -0.57401! Patience: 17/50
2025-10-14 16:34:36.604794: train_loss -0.7362
2025-10-14 16:34:36.604948: val_loss -0.574
2025-10-14 16:34:36.605078: Pseudo dice [np.float32(0.7616)]
2025-10-14 16:34:36.605208: Epoch time: 46.12 s
2025-10-14 16:34:36.605499: Yayy! New best EMA pseudo Dice: 0.7402999997138977
2025-10-14 16:34:37.684281: 
2025-10-14 16:34:37.684581: Epoch 104
2025-10-14 16:34:37.684751: Current learning rate: 0.00345
2025-10-14 16:35:23.915669: Validation loss did not improve from -0.57401. Patience: 1/50
2025-10-14 16:35:23.916422: train_loss -0.7381
2025-10-14 16:35:23.916570: val_loss -0.5501
2025-10-14 16:35:23.916758: Pseudo dice [np.float32(0.751)]
2025-10-14 16:35:23.916982: Epoch time: 46.23 s
2025-10-14 16:35:24.358751: Yayy! New best EMA pseudo Dice: 0.7414000034332275
2025-10-14 16:35:25.419955: 
2025-10-14 16:35:25.420510: Epoch 105
2025-10-14 16:35:25.420899: Current learning rate: 0.00338
2025-10-14 16:36:11.568245: Validation loss did not improve from -0.57401. Patience: 2/50
2025-10-14 16:36:11.568803: train_loss -0.7405
2025-10-14 16:36:11.569021: val_loss -0.5096
2025-10-14 16:36:11.569277: Pseudo dice [np.float32(0.7307)]
2025-10-14 16:36:11.569545: Epoch time: 46.15 s
2025-10-14 16:36:12.203618: 
2025-10-14 16:36:12.203890: Epoch 106
2025-10-14 16:36:12.204073: Current learning rate: 0.00332
2025-10-14 16:36:58.328484: Validation loss did not improve from -0.57401. Patience: 3/50
2025-10-14 16:36:58.329486: train_loss -0.7396
2025-10-14 16:36:58.329824: val_loss -0.5628
2025-10-14 16:36:58.330012: Pseudo dice [np.float32(0.7557)]
2025-10-14 16:36:58.330169: Epoch time: 46.13 s
2025-10-14 16:36:58.330327: Yayy! New best EMA pseudo Dice: 0.7419000267982483
2025-10-14 16:36:59.892995: 
2025-10-14 16:36:59.893363: Epoch 107
2025-10-14 16:36:59.893555: Current learning rate: 0.00325
2025-10-14 16:37:46.053282: Validation loss did not improve from -0.57401. Patience: 4/50
2025-10-14 16:37:46.053786: train_loss -0.7435
2025-10-14 16:37:46.053949: val_loss -0.5648
2025-10-14 16:37:46.054074: Pseudo dice [np.float32(0.7549)]
2025-10-14 16:37:46.054220: Epoch time: 46.16 s
2025-10-14 16:37:46.054367: Yayy! New best EMA pseudo Dice: 0.7432000041007996
2025-10-14 16:37:47.134067: 
2025-10-14 16:37:47.134418: Epoch 108
2025-10-14 16:37:47.134625: Current learning rate: 0.00318
2025-10-14 16:38:33.285793: Validation loss improved from -0.57401 to -0.58192! Patience: 4/50
2025-10-14 16:38:33.286493: train_loss -0.7416
2025-10-14 16:38:33.286762: val_loss -0.5819
2025-10-14 16:38:33.286991: Pseudo dice [np.float32(0.7605)]
2025-10-14 16:38:33.287227: Epoch time: 46.15 s
2025-10-14 16:38:33.287422: Yayy! New best EMA pseudo Dice: 0.7448999881744385
2025-10-14 16:38:34.372120: 
2025-10-14 16:38:34.372809: Epoch 109
2025-10-14 16:38:34.373223: Current learning rate: 0.00311
2025-10-14 16:39:20.509782: Validation loss did not improve from -0.58192. Patience: 1/50
2025-10-14 16:39:20.510331: train_loss -0.7419
2025-10-14 16:39:20.510534: val_loss -0.5507
2025-10-14 16:39:20.510702: Pseudo dice [np.float32(0.7606)]
2025-10-14 16:39:20.510851: Epoch time: 46.14 s
2025-10-14 16:39:20.961290: Yayy! New best EMA pseudo Dice: 0.7465000152587891
2025-10-14 16:39:22.023494: 
2025-10-14 16:39:22.023815: Epoch 110
2025-10-14 16:39:22.023994: Current learning rate: 0.00304
2025-10-14 16:40:08.148474: Validation loss did not improve from -0.58192. Patience: 2/50
2025-10-14 16:40:08.150198: train_loss -0.7444
2025-10-14 16:40:08.150725: val_loss -0.5532
2025-10-14 16:40:08.151225: Pseudo dice [np.float32(0.7538)]
2025-10-14 16:40:08.151752: Epoch time: 46.13 s
2025-10-14 16:40:08.152226: Yayy! New best EMA pseudo Dice: 0.7472000122070312
2025-10-14 16:40:09.230229: 
2025-10-14 16:40:09.230709: Epoch 111
2025-10-14 16:40:09.231120: Current learning rate: 0.00297
2025-10-14 16:40:55.418076: Validation loss did not improve from -0.58192. Patience: 3/50
2025-10-14 16:40:55.418578: train_loss -0.7437
2025-10-14 16:40:55.418941: val_loss -0.5228
2025-10-14 16:40:55.419157: Pseudo dice [np.float32(0.7309)]
2025-10-14 16:40:55.419405: Epoch time: 46.19 s
2025-10-14 16:40:56.062716: 
2025-10-14 16:40:56.062980: Epoch 112
2025-10-14 16:40:56.063235: Current learning rate: 0.00291
2025-10-14 16:41:42.189974: Validation loss did not improve from -0.58192. Patience: 4/50
2025-10-14 16:41:42.191164: train_loss -0.7487
2025-10-14 16:41:42.191504: val_loss -0.5595
2025-10-14 16:41:42.191794: Pseudo dice [np.float32(0.7562)]
2025-10-14 16:41:42.192115: Epoch time: 46.13 s
2025-10-14 16:41:42.831276: 
2025-10-14 16:41:42.831619: Epoch 113
2025-10-14 16:41:42.831815: Current learning rate: 0.00284
2025-10-14 16:42:28.969912: Validation loss did not improve from -0.58192. Patience: 5/50
2025-10-14 16:42:28.970387: train_loss -0.7468
2025-10-14 16:42:28.970538: val_loss -0.5442
2025-10-14 16:42:28.970651: Pseudo dice [np.float32(0.7438)]
2025-10-14 16:42:28.970788: Epoch time: 46.14 s
2025-10-14 16:42:29.604934: 
2025-10-14 16:42:29.605313: Epoch 114
2025-10-14 16:42:29.605577: Current learning rate: 0.00277
2025-10-14 16:43:15.780315: Validation loss did not improve from -0.58192. Patience: 6/50
2025-10-14 16:43:15.781004: train_loss -0.7462
2025-10-14 16:43:15.781138: val_loss -0.5192
2025-10-14 16:43:15.781270: Pseudo dice [np.float32(0.7351)]
2025-10-14 16:43:15.781395: Epoch time: 46.18 s
2025-10-14 16:43:16.870401: 
2025-10-14 16:43:16.870721: Epoch 115
2025-10-14 16:43:16.870900: Current learning rate: 0.0027
2025-10-14 16:44:02.952597: Validation loss did not improve from -0.58192. Patience: 7/50
2025-10-14 16:44:02.953103: train_loss -0.7467
2025-10-14 16:44:02.953244: val_loss -0.5397
2025-10-14 16:44:02.953381: Pseudo dice [np.float32(0.7496)]
2025-10-14 16:44:02.953535: Epoch time: 46.08 s
2025-10-14 16:44:03.592799: 
2025-10-14 16:44:03.593050: Epoch 116
2025-10-14 16:44:03.593228: Current learning rate: 0.00263
2025-10-14 16:44:49.686631: Validation loss did not improve from -0.58192. Patience: 8/50
2025-10-14 16:44:49.687332: train_loss -0.7453
2025-10-14 16:44:49.687606: val_loss -0.5316
2025-10-14 16:44:49.687859: Pseudo dice [np.float32(0.7469)]
2025-10-14 16:44:49.688052: Epoch time: 46.1 s
2025-10-14 16:44:50.331195: 
2025-10-14 16:44:50.331595: Epoch 117
2025-10-14 16:44:50.331835: Current learning rate: 0.00256
2025-10-14 16:45:36.432402: Validation loss did not improve from -0.58192. Patience: 9/50
2025-10-14 16:45:36.432990: train_loss -0.7519
2025-10-14 16:45:36.433248: val_loss -0.5337
2025-10-14 16:45:36.433478: Pseudo dice [np.float32(0.7456)]
2025-10-14 16:45:36.433749: Epoch time: 46.1 s
2025-10-14 16:45:37.085056: 
2025-10-14 16:45:37.085410: Epoch 118
2025-10-14 16:45:37.085614: Current learning rate: 0.00249
2025-10-14 16:46:23.145660: Validation loss did not improve from -0.58192. Patience: 10/50
2025-10-14 16:46:23.146293: train_loss -0.7516
2025-10-14 16:46:23.146427: val_loss -0.5675
2025-10-14 16:46:23.146540: Pseudo dice [np.float32(0.759)]
2025-10-14 16:46:23.146674: Epoch time: 46.06 s
2025-10-14 16:46:23.792117: 
2025-10-14 16:46:23.792521: Epoch 119
2025-10-14 16:46:23.792818: Current learning rate: 0.00242
2025-10-14 16:47:09.839967: Validation loss did not improve from -0.58192. Patience: 11/50
2025-10-14 16:47:09.840533: train_loss -0.7545
2025-10-14 16:47:09.840716: val_loss -0.5383
2025-10-14 16:47:09.840905: Pseudo dice [np.float32(0.7457)]
2025-10-14 16:47:09.841113: Epoch time: 46.05 s
2025-10-14 16:47:10.926307: 
2025-10-14 16:47:10.926736: Epoch 120
2025-10-14 16:47:10.926974: Current learning rate: 0.00235
2025-10-14 16:47:57.084422: Validation loss did not improve from -0.58192. Patience: 12/50
2025-10-14 16:47:57.085053: train_loss -0.752
2025-10-14 16:47:57.085198: val_loss -0.5405
2025-10-14 16:47:57.085312: Pseudo dice [np.float32(0.7451)]
2025-10-14 16:47:57.085497: Epoch time: 46.16 s
2025-10-14 16:47:57.728036: 
2025-10-14 16:47:57.728385: Epoch 121
2025-10-14 16:47:57.728585: Current learning rate: 0.00228
2025-10-14 16:48:43.863460: Validation loss did not improve from -0.58192. Patience: 13/50
2025-10-14 16:48:43.863963: train_loss -0.7536
2025-10-14 16:48:43.864139: val_loss -0.537
2025-10-14 16:48:43.864251: Pseudo dice [np.float32(0.7445)]
2025-10-14 16:48:43.864376: Epoch time: 46.14 s
2025-10-14 16:48:44.977320: 
2025-10-14 16:48:44.977637: Epoch 122
2025-10-14 16:48:44.977825: Current learning rate: 0.00221
2025-10-14 16:49:31.085878: Validation loss did not improve from -0.58192. Patience: 14/50
2025-10-14 16:49:31.086620: train_loss -0.7503
2025-10-14 16:49:31.086811: val_loss -0.522
2025-10-14 16:49:31.086933: Pseudo dice [np.float32(0.7354)]
2025-10-14 16:49:31.087080: Epoch time: 46.11 s
2025-10-14 16:49:31.731565: 
2025-10-14 16:49:31.731904: Epoch 123
2025-10-14 16:49:31.732187: Current learning rate: 0.00214
2025-10-14 16:50:17.920310: Validation loss did not improve from -0.58192. Patience: 15/50
2025-10-14 16:50:17.920778: train_loss -0.7539
2025-10-14 16:50:17.920940: val_loss -0.5331
2025-10-14 16:50:17.921052: Pseudo dice [np.float32(0.7472)]
2025-10-14 16:50:17.921181: Epoch time: 46.19 s
2025-10-14 16:50:18.561230: 
2025-10-14 16:50:18.561788: Epoch 124
2025-10-14 16:50:18.562744: Current learning rate: 0.00207
2025-10-14 16:51:04.736453: Validation loss did not improve from -0.58192. Patience: 16/50
2025-10-14 16:51:04.737280: train_loss -0.7556
2025-10-14 16:51:04.737577: val_loss -0.5542
2025-10-14 16:51:04.737768: Pseudo dice [np.float32(0.7506)]
2025-10-14 16:51:04.737976: Epoch time: 46.18 s
2025-10-14 16:51:05.813544: 
2025-10-14 16:51:05.813864: Epoch 125
2025-10-14 16:51:05.814038: Current learning rate: 0.00199
2025-10-14 16:51:52.014340: Validation loss did not improve from -0.58192. Patience: 17/50
2025-10-14 16:51:52.014788: train_loss -0.7552
2025-10-14 16:51:52.015004: val_loss -0.5459
2025-10-14 16:51:52.015159: Pseudo dice [np.float32(0.7358)]
2025-10-14 16:51:52.015353: Epoch time: 46.2 s
2025-10-14 16:51:52.654716: 
2025-10-14 16:51:52.655298: Epoch 126
2025-10-14 16:51:52.655714: Current learning rate: 0.00192
2025-10-14 16:52:38.842905: Validation loss did not improve from -0.58192. Patience: 18/50
2025-10-14 16:52:38.843459: train_loss -0.7578
2025-10-14 16:52:38.843605: val_loss -0.5305
2025-10-14 16:52:38.843722: Pseudo dice [np.float32(0.7351)]
2025-10-14 16:52:38.843863: Epoch time: 46.19 s
2025-10-14 16:52:39.490637: 
2025-10-14 16:52:39.491067: Epoch 127
2025-10-14 16:52:39.491377: Current learning rate: 0.00185
2025-10-14 16:53:25.677077: Validation loss did not improve from -0.58192. Patience: 19/50
2025-10-14 16:53:25.677545: train_loss -0.7626
2025-10-14 16:53:25.677686: val_loss -0.5593
2025-10-14 16:53:25.677868: Pseudo dice [np.float32(0.7499)]
2025-10-14 16:53:25.678043: Epoch time: 46.19 s
2025-10-14 16:53:26.318082: 
2025-10-14 16:53:26.318658: Epoch 128
2025-10-14 16:53:26.319082: Current learning rate: 0.00178
2025-10-14 16:54:12.525376: Validation loss did not improve from -0.58192. Patience: 20/50
2025-10-14 16:54:12.526098: train_loss -0.7608
2025-10-14 16:54:12.526237: val_loss -0.5453
2025-10-14 16:54:12.526384: Pseudo dice [np.float32(0.7492)]
2025-10-14 16:54:12.526531: Epoch time: 46.21 s
2025-10-14 16:54:13.161077: 
2025-10-14 16:54:13.161389: Epoch 129
2025-10-14 16:54:13.161602: Current learning rate: 0.0017
2025-10-14 16:54:59.335717: Validation loss did not improve from -0.58192. Patience: 21/50
2025-10-14 16:54:59.336504: train_loss -0.7579
2025-10-14 16:54:59.336903: val_loss -0.5549
2025-10-14 16:54:59.337092: Pseudo dice [np.float32(0.7478)]
2025-10-14 16:54:59.337272: Epoch time: 46.18 s
2025-10-14 16:55:00.397485: 
2025-10-14 16:55:00.397845: Epoch 130
2025-10-14 16:55:00.398031: Current learning rate: 0.00163
2025-10-14 16:55:46.611051: Validation loss did not improve from -0.58192. Patience: 22/50
2025-10-14 16:55:46.611712: train_loss -0.7635
2025-10-14 16:55:46.611889: val_loss -0.5461
2025-10-14 16:55:46.612044: Pseudo dice [np.float32(0.7459)]
2025-10-14 16:55:46.612234: Epoch time: 46.21 s
2025-10-14 16:55:47.244387: 
2025-10-14 16:55:47.244703: Epoch 131
2025-10-14 16:55:47.244873: Current learning rate: 0.00156
2025-10-14 16:56:33.471200: Validation loss did not improve from -0.58192. Patience: 23/50
2025-10-14 16:56:33.471722: train_loss -0.7617
2025-10-14 16:56:33.471904: val_loss -0.5603
2025-10-14 16:56:33.472060: Pseudo dice [np.float32(0.7461)]
2025-10-14 16:56:33.472185: Epoch time: 46.23 s
2025-10-14 16:56:34.106327: 
2025-10-14 16:56:34.106580: Epoch 132
2025-10-14 16:56:34.106772: Current learning rate: 0.00148
2025-10-14 16:57:20.311963: Validation loss did not improve from -0.58192. Patience: 24/50
2025-10-14 16:57:20.312765: train_loss -0.7613
2025-10-14 16:57:20.312931: val_loss -0.5532
2025-10-14 16:57:20.313066: Pseudo dice [np.float32(0.7524)]
2025-10-14 16:57:20.313219: Epoch time: 46.21 s
2025-10-14 16:57:20.950148: 
2025-10-14 16:57:20.950499: Epoch 133
2025-10-14 16:57:20.950848: Current learning rate: 0.00141
2025-10-14 16:58:07.159050: Validation loss did not improve from -0.58192. Patience: 25/50
2025-10-14 16:58:07.159510: train_loss -0.7633
2025-10-14 16:58:07.159685: val_loss -0.5551
2025-10-14 16:58:07.159825: Pseudo dice [np.float32(0.7544)]
2025-10-14 16:58:07.159991: Epoch time: 46.21 s
2025-10-14 16:58:07.796191: 
2025-10-14 16:58:07.796533: Epoch 134
2025-10-14 16:58:07.796725: Current learning rate: 0.00133
2025-10-14 16:58:53.995162: Validation loss did not improve from -0.58192. Patience: 26/50
2025-10-14 16:58:53.995949: train_loss -0.7645
2025-10-14 16:58:53.996157: val_loss -0.5376
2025-10-14 16:58:53.996341: Pseudo dice [np.float32(0.7472)]
2025-10-14 16:58:53.996556: Epoch time: 46.2 s
2025-10-14 16:58:55.072120: 
2025-10-14 16:58:55.072426: Epoch 135
2025-10-14 16:58:55.072697: Current learning rate: 0.00126
2025-10-14 16:59:41.338696: Validation loss did not improve from -0.58192. Patience: 27/50
2025-10-14 16:59:41.339264: train_loss -0.7644
2025-10-14 16:59:41.339522: val_loss -0.566
2025-10-14 16:59:41.339701: Pseudo dice [np.float32(0.7562)]
2025-10-14 16:59:41.339872: Epoch time: 46.27 s
2025-10-14 16:59:41.340013: Yayy! New best EMA pseudo Dice: 0.7480000257492065
2025-10-14 16:59:42.423817: 
2025-10-14 16:59:42.424124: Epoch 136
2025-10-14 16:59:42.424298: Current learning rate: 0.00118
2025-10-14 17:00:28.664663: Validation loss did not improve from -0.58192. Patience: 28/50
2025-10-14 17:00:28.665216: train_loss -0.7636
2025-10-14 17:00:28.665376: val_loss -0.5712
2025-10-14 17:00:28.665551: Pseudo dice [np.float32(0.7632)]
2025-10-14 17:00:28.665740: Epoch time: 46.24 s
2025-10-14 17:00:28.665892: Yayy! New best EMA pseudo Dice: 0.7494999766349792
2025-10-14 17:00:29.737957: 
2025-10-14 17:00:29.738336: Epoch 137
2025-10-14 17:00:29.738585: Current learning rate: 0.00111
2025-10-14 17:01:15.934826: Validation loss did not improve from -0.58192. Patience: 29/50
2025-10-14 17:01:15.935303: train_loss -0.7625
2025-10-14 17:01:15.935476: val_loss -0.5525
2025-10-14 17:01:15.935678: Pseudo dice [np.float32(0.752)]
2025-10-14 17:01:15.935815: Epoch time: 46.2 s
2025-10-14 17:01:15.935969: Yayy! New best EMA pseudo Dice: 0.7497000098228455
2025-10-14 17:01:17.493972: 
2025-10-14 17:01:17.494359: Epoch 138
2025-10-14 17:01:17.494546: Current learning rate: 0.00103
2025-10-14 17:02:03.697861: Validation loss did not improve from -0.58192. Patience: 30/50
2025-10-14 17:02:03.698589: train_loss -0.7675
2025-10-14 17:02:03.698749: val_loss -0.5376
2025-10-14 17:02:03.698887: Pseudo dice [np.float32(0.7428)]
2025-10-14 17:02:03.699036: Epoch time: 46.21 s
2025-10-14 17:02:04.340116: 
2025-10-14 17:02:04.340475: Epoch 139
2025-10-14 17:02:04.340656: Current learning rate: 0.00095
2025-10-14 17:02:50.636375: Validation loss did not improve from -0.58192. Patience: 31/50
2025-10-14 17:02:50.636874: train_loss -0.7644
2025-10-14 17:02:50.637031: val_loss -0.5276
2025-10-14 17:02:50.637155: Pseudo dice [np.float32(0.7462)]
2025-10-14 17:02:50.637293: Epoch time: 46.3 s
2025-10-14 17:02:51.725038: 
2025-10-14 17:02:51.725392: Epoch 140
2025-10-14 17:02:51.725565: Current learning rate: 0.00087
2025-10-14 17:03:37.980086: Validation loss did not improve from -0.58192. Patience: 32/50
2025-10-14 17:03:37.980717: train_loss -0.7696
2025-10-14 17:03:37.980875: val_loss -0.5535
2025-10-14 17:03:37.981042: Pseudo dice [np.float32(0.7526)]
2025-10-14 17:03:37.981187: Epoch time: 46.26 s
2025-10-14 17:03:38.627674: 
2025-10-14 17:03:38.628010: Epoch 141
2025-10-14 17:03:38.628204: Current learning rate: 0.00079
2025-10-14 17:04:24.844093: Validation loss improved from -0.58192 to -0.58465! Patience: 32/50
2025-10-14 17:04:24.844779: train_loss -0.7675
2025-10-14 17:04:24.845024: val_loss -0.5847
2025-10-14 17:04:24.845238: Pseudo dice [np.float32(0.7652)]
2025-10-14 17:04:24.845469: Epoch time: 46.22 s
2025-10-14 17:04:24.845751: Yayy! New best EMA pseudo Dice: 0.7506999969482422
2025-10-14 17:04:25.927155: 
2025-10-14 17:04:25.927439: Epoch 142
2025-10-14 17:04:25.927667: Current learning rate: 0.00071
2025-10-14 17:05:12.128728: Validation loss did not improve from -0.58465. Patience: 1/50
2025-10-14 17:05:12.129383: train_loss -0.7694
2025-10-14 17:05:12.129549: val_loss -0.5415
2025-10-14 17:05:12.129685: Pseudo dice [np.float32(0.751)]
2025-10-14 17:05:12.129914: Epoch time: 46.2 s
2025-10-14 17:05:12.130032: Yayy! New best EMA pseudo Dice: 0.7508000135421753
2025-10-14 17:05:13.224900: 
2025-10-14 17:05:13.225237: Epoch 143
2025-10-14 17:05:13.225404: Current learning rate: 0.00063
2025-10-14 17:05:59.465134: Validation loss did not improve from -0.58465. Patience: 2/50
2025-10-14 17:05:59.465636: train_loss -0.772
2025-10-14 17:05:59.465801: val_loss -0.5361
2025-10-14 17:05:59.465918: Pseudo dice [np.float32(0.7459)]
2025-10-14 17:05:59.466050: Epoch time: 46.24 s
2025-10-14 17:06:00.106377: 
2025-10-14 17:06:00.107326: Epoch 144
2025-10-14 17:06:00.107831: Current learning rate: 0.00055
2025-10-14 17:06:46.328046: Validation loss did not improve from -0.58465. Patience: 3/50
2025-10-14 17:06:46.328761: train_loss -0.7728
2025-10-14 17:06:46.328928: val_loss -0.5364
2025-10-14 17:06:46.329118: Pseudo dice [np.float32(0.7395)]
2025-10-14 17:06:46.329270: Epoch time: 46.22 s
2025-10-14 17:06:47.434853: 
2025-10-14 17:06:47.435381: Epoch 145
2025-10-14 17:06:47.435779: Current learning rate: 0.00047
2025-10-14 17:07:33.635294: Validation loss did not improve from -0.58465. Patience: 4/50
2025-10-14 17:07:33.635819: train_loss -0.767
2025-10-14 17:07:33.635973: val_loss -0.552
2025-10-14 17:07:33.636114: Pseudo dice [np.float32(0.7573)]
2025-10-14 17:07:33.636247: Epoch time: 46.2 s
2025-10-14 17:07:34.280815: 
2025-10-14 17:07:34.281164: Epoch 146
2025-10-14 17:07:34.281370: Current learning rate: 0.00038
2025-10-14 17:08:20.459696: Validation loss did not improve from -0.58465. Patience: 5/50
2025-10-14 17:08:20.460352: train_loss -0.7719
2025-10-14 17:08:20.460540: val_loss -0.5123
2025-10-14 17:08:20.460687: Pseudo dice [np.float32(0.7335)]
2025-10-14 17:08:20.460826: Epoch time: 46.18 s
2025-10-14 17:08:21.111045: 
2025-10-14 17:08:21.111339: Epoch 147
2025-10-14 17:08:21.111537: Current learning rate: 0.0003
2025-10-14 17:09:07.347973: Validation loss did not improve from -0.58465. Patience: 6/50
2025-10-14 17:09:07.348425: train_loss -0.7748
2025-10-14 17:09:07.348614: val_loss -0.5529
2025-10-14 17:09:07.348771: Pseudo dice [np.float32(0.7554)]
2025-10-14 17:09:07.348920: Epoch time: 46.24 s
2025-10-14 17:09:07.995264: 
2025-10-14 17:09:07.995633: Epoch 148
2025-10-14 17:09:07.995839: Current learning rate: 0.00021
2025-10-14 17:09:54.223675: Validation loss did not improve from -0.58465. Patience: 7/50
2025-10-14 17:09:54.224345: train_loss -0.7729
2025-10-14 17:09:54.224504: val_loss -0.5646
2025-10-14 17:09:54.224658: Pseudo dice [np.float32(0.7523)]
2025-10-14 17:09:54.224804: Epoch time: 46.23 s
2025-10-14 17:09:54.868793: 
2025-10-14 17:09:54.869329: Epoch 149
2025-10-14 17:09:54.869645: Current learning rate: 0.00011
2025-10-14 17:10:41.096089: Validation loss did not improve from -0.58465. Patience: 8/50
2025-10-14 17:10:41.096579: train_loss -0.7725
2025-10-14 17:10:41.096714: val_loss -0.5427
2025-10-14 17:10:41.096822: Pseudo dice [np.float32(0.7503)]
2025-10-14 17:10:41.096964: Epoch time: 46.23 s
2025-10-14 17:10:42.256232: Training done.
2025-10-14 17:10:42.284584: Using splits from existing split file: /nfs/erelab001/shared/Computational_Group/Naravich/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset307_Sohee_Calcium_OCT_CrossValidation/splits_final.json
2025-10-14 17:10:42.284904: The split file contains 5 splits.
2025-10-14 17:10:42.285041: Desired fold for training: 1
2025-10-14 17:10:42.285228: This split has 6 training and 2 validation cases.
2025-10-14 17:10:42.285529: predicting 101-019
2025-10-14 17:10:42.288100: 101-019, shape torch.Size([1, 375, 498, 498]), rank 0
2025-10-14 17:11:29.824914: predicting 401-004
2025-10-14 17:11:29.838286: 401-004, shape torch.Size([1, 375, 498, 498]), rank 0
2025-10-14 17:12:16.840230: Validation complete
2025-10-14 17:12:16.840535: Mean Validation Dice:  0.7371059416401214
Finished training fold 1 saving to /nfs/erelab001/shared/Computational_Group/Naravich/datasets/nnUNet_Datasets/nnUNet_results/Dataset307_Sohee_Calcium_OCT_CrossValidation/nnUNetTrainer__nnUNetPlans__3d_32x160x128_b10/fold_1_Genesis_Pretrained
