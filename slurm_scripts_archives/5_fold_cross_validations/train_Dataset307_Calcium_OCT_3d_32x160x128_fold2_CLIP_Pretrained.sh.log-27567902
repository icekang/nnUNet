
CommandNotFoundError: Your shell has not been properly configured to use 'conda activate'.
To initialize your shell, run

    $ conda init <SHELL_NAME>

Currently supported shells are:
  - bash
  - fish
  - tcsh
  - xonsh
  - zsh
  - powershell

See 'conda init --help' for more information and options.

IMPORTANT: You may need to close and restart your shell after running 'conda init'.


/home/gridsan/nchutisilp/.conda/envs/nnunet/bin/python
Starting training with CONFIG=3d_32x160x128_b10, DATASET_ID=307, TRAINER=nnUNetTrainer

############################
INFO: You are using the old nnU-Net default plans. We have updated our recommendations. Please consider using those instead! Read more here: https://github.com/MIC-DKFZ/nnUNet/blob/master/documentation/resenc_presets.md
############################

Using device: cuda:0

#######################################################################
Please cite the following paper when using nnU-Net:
Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.
#######################################################################

2024-12-07 03:56:16.848030: Using torch.compile...
/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
################### Loading pretrained weights from file  /home/gridsan/nchutisilp/projects/OpenAI-CLIP/clip_pretrained_nnUNet.pt ###################
Below is the list of overlapping blocks in pretrained model and nnUNet architecture:
encoder.stages.0.0.convs.0.conv.weight shape torch.Size([32, 1, 3, 3, 3])
encoder.stages.0.0.convs.0.conv.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.norm.weight shape torch.Size([32])
encoder.stages.0.0.convs.0.norm.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.0.weight shape torch.Size([32, 1, 3, 3, 3])
encoder.stages.0.0.convs.0.all_modules.0.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.1.weight shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.1.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
encoder.stages.0.0.convs.1.conv.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.norm.weight shape torch.Size([32])
encoder.stages.0.0.convs.1.norm.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
encoder.stages.0.0.convs.1.all_modules.0.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.1.weight shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.1.bias shape torch.Size([32])
encoder.stages.1.0.convs.0.conv.weight shape torch.Size([64, 32, 3, 3, 3])
encoder.stages.1.0.convs.0.conv.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.norm.weight shape torch.Size([64])
encoder.stages.1.0.convs.0.norm.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.0.weight shape torch.Size([64, 32, 3, 3, 3])
encoder.stages.1.0.convs.0.all_modules.0.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.1.weight shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.1.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
encoder.stages.1.0.convs.1.conv.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.norm.weight shape torch.Size([64])
encoder.stages.1.0.convs.1.norm.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
encoder.stages.1.0.convs.1.all_modules.0.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.1.weight shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.1.bias shape torch.Size([64])
encoder.stages.2.0.convs.0.conv.weight shape torch.Size([128, 64, 3, 3, 3])
encoder.stages.2.0.convs.0.conv.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.norm.weight shape torch.Size([128])
encoder.stages.2.0.convs.0.norm.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.0.weight shape torch.Size([128, 64, 3, 3, 3])
encoder.stages.2.0.convs.0.all_modules.0.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.1.weight shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.1.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
encoder.stages.2.0.convs.1.conv.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.norm.weight shape torch.Size([128])
encoder.stages.2.0.convs.1.norm.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
encoder.stages.2.0.convs.1.all_modules.0.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.1.weight shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.1.bias shape torch.Size([128])
encoder.stages.3.0.convs.0.conv.weight shape torch.Size([256, 128, 3, 3, 3])
encoder.stages.3.0.convs.0.conv.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.norm.weight shape torch.Size([256])
encoder.stages.3.0.convs.0.norm.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.0.weight shape torch.Size([256, 128, 3, 3, 3])
encoder.stages.3.0.convs.0.all_modules.0.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.1.weight shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.1.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
encoder.stages.3.0.convs.1.conv.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.norm.weight shape torch.Size([256])
encoder.stages.3.0.convs.1.norm.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
encoder.stages.3.0.convs.1.all_modules.0.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.1.weight shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.1.bias shape torch.Size([256])
encoder.stages.4.0.convs.0.conv.weight shape torch.Size([320, 256, 3, 3, 3])
encoder.stages.4.0.convs.0.conv.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.norm.weight shape torch.Size([320])
encoder.stages.4.0.convs.0.norm.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.0.weight shape torch.Size([320, 256, 3, 3, 3])
encoder.stages.4.0.convs.0.all_modules.0.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.1.weight shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.1.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.4.0.convs.1.conv.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.norm.weight shape torch.Size([320])
encoder.stages.4.0.convs.1.norm.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.4.0.convs.1.all_modules.0.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.1.weight shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.1.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.0.conv.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.norm.weight shape torch.Size([320])
encoder.stages.5.0.convs.0.norm.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.0.all_modules.0.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.1.weight shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.1.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.1.conv.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.norm.weight shape torch.Size([320])
encoder.stages.5.0.convs.1.norm.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.1.all_modules.0.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.1.weight shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.0.0.convs.0.conv.weight shape torch.Size([32, 1, 3, 3, 3])
decoder.encoder.stages.0.0.convs.0.conv.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.norm.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.norm.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.0.weight shape torch.Size([32, 1, 3, 3, 3])
decoder.encoder.stages.0.0.convs.0.all_modules.0.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.1.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.1.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.encoder.stages.0.0.convs.1.conv.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.norm.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.norm.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.encoder.stages.0.0.convs.1.all_modules.0.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.1.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.1.bias shape torch.Size([32])
decoder.encoder.stages.1.0.convs.0.conv.weight shape torch.Size([64, 32, 3, 3, 3])
decoder.encoder.stages.1.0.convs.0.conv.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.norm.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.norm.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.0.weight shape torch.Size([64, 32, 3, 3, 3])
decoder.encoder.stages.1.0.convs.0.all_modules.0.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.1.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.1.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.encoder.stages.1.0.convs.1.conv.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.norm.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.norm.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.encoder.stages.1.0.convs.1.all_modules.0.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.1.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.1.bias shape torch.Size([64])
decoder.encoder.stages.2.0.convs.0.conv.weight shape torch.Size([128, 64, 3, 3, 3])
decoder.encoder.stages.2.0.convs.0.conv.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.norm.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.norm.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.0.weight shape torch.Size([128, 64, 3, 3, 3])
decoder.encoder.stages.2.0.convs.0.all_modules.0.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.1.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.1.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.encoder.stages.2.0.convs.1.conv.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.norm.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.norm.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.encoder.stages.2.0.convs.1.all_modules.0.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.1.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.1.bias shape torch.Size([128])
decoder.encoder.stages.3.0.convs.0.conv.weight shape torch.Size([256, 128, 3, 3, 3])
decoder.encoder.stages.3.0.convs.0.conv.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.norm.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.norm.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.0.weight shape torch.Size([256, 128, 3, 3, 3])
decoder.encoder.stages.3.0.convs.0.all_modules.0.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.1.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.1.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.encoder.stages.3.0.convs.1.conv.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.norm.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.norm.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.encoder.stages.3.0.convs.1.all_modules.0.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.1.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.1.bias shape torch.Size([256])
decoder.encoder.stages.4.0.convs.0.conv.weight shape torch.Size([320, 256, 3, 3, 3])
decoder.encoder.stages.4.0.convs.0.conv.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.norm.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.norm.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.0.weight shape torch.Size([320, 256, 3, 3, 3])
decoder.encoder.stages.4.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.4.0.convs.1.conv.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.norm.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.norm.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.4.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.0.conv.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.norm.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.norm.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.1.conv.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.norm.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.norm.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.stages.0.convs.0.conv.weight shape torch.Size([320, 640, 3, 3, 3])
decoder.stages.0.convs.0.conv.bias shape torch.Size([320])
decoder.stages.0.convs.0.norm.weight shape torch.Size([320])
decoder.stages.0.convs.0.norm.bias shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.0.weight shape torch.Size([320, 640, 3, 3, 3])
decoder.stages.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.stages.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.stages.0.convs.1.conv.bias shape torch.Size([320])
decoder.stages.0.convs.1.norm.weight shape torch.Size([320])
decoder.stages.0.convs.1.norm.bias shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.stages.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.stages.1.convs.0.conv.weight shape torch.Size([256, 512, 3, 3, 3])
decoder.stages.1.convs.0.conv.bias shape torch.Size([256])
decoder.stages.1.convs.0.norm.weight shape torch.Size([256])
decoder.stages.1.convs.0.norm.bias shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.0.weight shape torch.Size([256, 512, 3, 3, 3])
decoder.stages.1.convs.0.all_modules.0.bias shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.1.weight shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.1.bias shape torch.Size([256])
decoder.stages.1.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.stages.1.convs.1.conv.bias shape torch.Size([256])
decoder.stages.1.convs.1.norm.weight shape torch.Size([256])
decoder.stages.1.convs.1.norm.bias shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.stages.1.convs.1.all_modules.0.bias shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.1.weight shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.1.bias shape torch.Size([256])
decoder.stages.2.convs.0.conv.weight shape torch.Size([128, 256, 3, 3, 3])
decoder.stages.2.convs.0.conv.bias shape torch.Size([128])
decoder.stages.2.convs.0.norm.weight shape torch.Size([128])
decoder.stages.2.convs.0.norm.bias shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.0.weight shape torch.Size([128, 256, 3, 3, 3])
decoder.stages.2.convs.0.all_modules.0.bias shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.1.weight shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.1.bias shape torch.Size([128])
decoder.stages.2.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.stages.2.convs.1.conv.bias shape torch.Size([128])
decoder.stages.2.convs.1.norm.weight shape torch.Size([128])
decoder.stages.2.convs.1.norm.bias shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.stages.2.convs.1.all_modules.0.bias shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.1.weight shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.1.bias shape torch.Size([128])
decoder.stages.3.convs.0.conv.weight shape torch.Size([64, 128, 3, 3, 3])
decoder.stages.3.convs.0.conv.bias shape torch.Size([64])
decoder.stages.3.convs.0.norm.weight shape torch.Size([64])
decoder.stages.3.convs.0.norm.bias shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.0.weight shape torch.Size([64, 128, 3, 3, 3])
decoder.stages.3.convs.0.all_modules.0.bias shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.1.weight shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.1.bias shape torch.Size([64])
decoder.stages.3.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.stages.3.convs.1.conv.bias shape torch.Size([64])
decoder.stages.3.convs.1.norm.weight shape torch.Size([64])
decoder.stages.3.convs.1.norm.bias shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.stages.3.convs.1.all_modules.0.bias shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.1.weight shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.1.bias shape torch.Size([64])
decoder.stages.4.convs.0.conv.weight shape torch.Size([32, 64, 3, 3, 3])
decoder.stages.4.convs.0.conv.bias shape torch.Size([32])
decoder.stages.4.convs.0.norm.weight shape torch.Size([32])
decoder.stages.4.convs.0.norm.bias shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.0.weight shape torch.Size([32, 64, 3, 3, 3])
decoder.stages.4.convs.0.all_modules.0.bias shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.1.weight shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.1.bias shape torch.Size([32])
decoder.stages.4.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.stages.4.convs.1.conv.bias shape torch.Size([32])
decoder.stages.4.convs.1.norm.weight shape torch.Size([32])
decoder.stages.4.convs.1.norm.bias shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.stages.4.convs.1.all_modules.0.bias shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.1.weight shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.1.bias shape torch.Size([32])
decoder.transpconvs.0.weight shape torch.Size([320, 320, 1, 2, 2])
decoder.transpconvs.0.bias shape torch.Size([320])
decoder.transpconvs.1.weight shape torch.Size([320, 256, 2, 2, 2])
decoder.transpconvs.1.bias shape torch.Size([256])
decoder.transpconvs.2.weight shape torch.Size([256, 128, 2, 2, 2])
decoder.transpconvs.2.bias shape torch.Size([128])
decoder.transpconvs.3.weight shape torch.Size([128, 64, 2, 2, 2])
decoder.transpconvs.3.bias shape torch.Size([64])
decoder.transpconvs.4.weight shape torch.Size([64, 32, 2, 2, 2])
decoder.transpconvs.4.bias shape torch.Size([32])
################### Done ###################
2024-12-07 03:56:19.749043: do_dummy_2d_data_aug: True
2024-12-07 03:56:19.752594: Using splits from existing split file: /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset307_Sohee_Calcium_OCT_CrossValidation/splits_final.json
2024-12-07 03:56:19.755897: The split file contains 5 splits.
2024-12-07 03:56:19.757766: Desired fold for training: 2
2024-12-07 03:56:19.758926: This split has 6 training and 2 validation cases.
using pin_memory on device 0
using pin_memory on device 0

This is the configuration used by this training:
Configuration name: 3d_32x160x128_b10
 {'data_identifier': 'nnUNetPlans_3d_fullres', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 10, 'patch_size': [32, 160, 128], 'median_image_size_in_voxels': [375.0, 498.0, 498.0], 'spacing': [1.0, 1.0, 1.0], 'normalization_schemes': ['ZScoreNormalization'], 'use_mask_for_norm': [False], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'architecture': {'network_class_name': 'dynamic_network_architectures.architectures.unet.PlainConvUNet', 'arch_kwargs': {'n_stages': 6, 'features_per_stage': [32, 64, 128, 256, 320, 320], 'conv_op': 'torch.nn.modules.conv.Conv3d', 'kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]], 'strides': [[1, 1, 1], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [1, 2, 2]], 'n_conv_per_stage': [2, 2, 2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2, 2, 2], 'conv_bias': True, 'norm_op': 'torch.nn.modules.instancenorm.InstanceNorm3d', 'norm_op_kwargs': {'eps': 1e-05, 'affine': True}, 'dropout_op': None, 'dropout_op_kwargs': None, 'nonlin': 'torch.nn.LeakyReLU', 'nonlin_kwargs': {'inplace': True}, 'deep_supervision': True}, '_kw_requires_import': ['conv_op', 'norm_op', 'dropout_op', 'nonlin']}, 'batch_dice': True, 'inherits_from': '3d_fullres'} 

These are the global plan.json settings:
 {'dataset_name': 'Dataset307_Sohee_Calcium_OCT_CrossValidation', 'plans_name': 'nnUNetPlans', 'original_median_spacing_after_transp': [1.0, 1.0, 1.0], 'original_median_shape_after_transp': [375, 498, 498], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 0.4977976381778717, 'mean': 0.13507525622844696, 'median': 0.09599608182907104, 'min': 0.0, 'percentile_00_5': 0.014754901640117168, 'percentile_99_5': 0.4977976381778717, 'std': 0.12025152146816254}}} 

2024-12-07 03:56:43.731435: unpacking dataset...
2024-12-07 03:56:49.166005: unpacking done...
2024-12-07 03:56:49.194824: Unable to plot network architecture: nnUNet_compile is enabled!
2024-12-07 03:56:49.267676: 
2024-12-07 03:56:49.268988: Epoch 0
2024-12-07 03:56:49.269922: Current learning rate: 0.01
2024-12-07 04:03:31.622758: Validation loss improved from 1000.00000 to -0.22183! Patience: 0/50
2024-12-07 04:03:31.624093: train_loss -0.0822
2024-12-07 04:03:31.625120: val_loss -0.2218
2024-12-07 04:03:31.626087: Pseudo dice [0.559]
2024-12-07 04:03:31.627019: Epoch time: 402.36 s
2024-12-07 04:03:31.627731: Yayy! New best EMA pseudo Dice: 0.559
2024-12-07 04:03:33.303224: 
2024-12-07 04:03:33.304885: Epoch 1
2024-12-07 04:03:33.305632: Current learning rate: 0.00999
2024-12-07 04:09:17.166779: Validation loss improved from -0.22183 to -0.22493! Patience: 0/50
2024-12-07 04:09:17.167852: train_loss -0.2207
2024-12-07 04:09:17.168677: val_loss -0.2249
2024-12-07 04:09:17.169430: Pseudo dice [0.545]
2024-12-07 04:09:17.170099: Epoch time: 343.87 s
2024-12-07 04:09:18.514705: 
2024-12-07 04:09:18.516183: Epoch 2
2024-12-07 04:09:18.516984: Current learning rate: 0.00998
2024-12-07 04:15:03.694532: Validation loss improved from -0.22493 to -0.32630! Patience: 0/50
2024-12-07 04:15:03.695521: train_loss -0.2686
2024-12-07 04:15:03.696703: val_loss -0.3263
2024-12-07 04:15:03.697951: Pseudo dice [0.6114]
2024-12-07 04:15:03.699036: Epoch time: 345.18 s
2024-12-07 04:15:03.699897: Yayy! New best EMA pseudo Dice: 0.563
2024-12-07 04:15:05.477035: 
2024-12-07 04:15:05.478485: Epoch 3
2024-12-07 04:15:05.479430: Current learning rate: 0.00997
2024-12-07 04:20:39.558878: Validation loss did not improve from -0.32630. Patience: 1/50
2024-12-07 04:20:39.559956: train_loss -0.3021
2024-12-07 04:20:39.561236: val_loss -0.3181
2024-12-07 04:20:39.561927: Pseudo dice [0.5999]
2024-12-07 04:20:39.562628: Epoch time: 334.08 s
2024-12-07 04:20:39.563259: Yayy! New best EMA pseudo Dice: 0.5667
2024-12-07 04:20:41.268724: 
2024-12-07 04:20:41.269797: Epoch 4
2024-12-07 04:20:41.270560: Current learning rate: 0.00996
2024-12-07 04:26:21.535991: Validation loss improved from -0.32630 to -0.33119! Patience: 1/50
2024-12-07 04:26:21.536971: train_loss -0.3063
2024-12-07 04:26:21.537740: val_loss -0.3312
2024-12-07 04:26:21.538490: Pseudo dice [0.624]
2024-12-07 04:26:21.539090: Epoch time: 340.27 s
2024-12-07 04:26:21.906416: Yayy! New best EMA pseudo Dice: 0.5724
2024-12-07 04:26:23.656477: 
2024-12-07 04:26:23.657753: Epoch 5
2024-12-07 04:26:23.658480: Current learning rate: 0.00995
2024-12-07 04:32:13.143634: Validation loss improved from -0.33119 to -0.40918! Patience: 0/50
2024-12-07 04:32:13.144620: train_loss -0.3508
2024-12-07 04:32:13.145598: val_loss -0.4092
2024-12-07 04:32:13.146291: Pseudo dice [0.6479]
2024-12-07 04:32:13.146918: Epoch time: 349.49 s
2024-12-07 04:32:13.147660: Yayy! New best EMA pseudo Dice: 0.58
2024-12-07 04:32:14.851934: 
2024-12-07 04:32:14.853366: Epoch 6
2024-12-07 04:32:14.854106: Current learning rate: 0.00995
2024-12-07 04:38:08.186181: Validation loss did not improve from -0.40918. Patience: 1/50
2024-12-07 04:38:08.187156: train_loss -0.3747
2024-12-07 04:38:08.187842: val_loss -0.4036
2024-12-07 04:38:08.188446: Pseudo dice [0.656]
2024-12-07 04:38:08.189361: Epoch time: 353.34 s
2024-12-07 04:38:08.190012: Yayy! New best EMA pseudo Dice: 0.5876
2024-12-07 04:38:09.912292: 
2024-12-07 04:38:09.913693: Epoch 7
2024-12-07 04:38:09.914553: Current learning rate: 0.00994
2024-12-07 04:44:11.515473: Validation loss improved from -0.40918 to -0.44542! Patience: 1/50
2024-12-07 04:44:11.516642: train_loss -0.3973
2024-12-07 04:44:11.517346: val_loss -0.4454
2024-12-07 04:44:11.517962: Pseudo dice [0.6795]
2024-12-07 04:44:11.518588: Epoch time: 361.61 s
2024-12-07 04:44:11.519185: Yayy! New best EMA pseudo Dice: 0.5968
2024-12-07 04:44:13.653495: 
2024-12-07 04:44:13.654796: Epoch 8
2024-12-07 04:44:13.655673: Current learning rate: 0.00993
2024-12-07 04:50:21.907666: Validation loss did not improve from -0.44542. Patience: 1/50
2024-12-07 04:50:21.908561: train_loss -0.41
2024-12-07 04:50:21.909369: val_loss -0.4351
2024-12-07 04:50:21.910105: Pseudo dice [0.676]
2024-12-07 04:50:21.911109: Epoch time: 368.26 s
2024-12-07 04:50:21.911989: Yayy! New best EMA pseudo Dice: 0.6047
2024-12-07 04:50:23.685347: 
2024-12-07 04:50:23.686712: Epoch 9
2024-12-07 04:50:23.687888: Current learning rate: 0.00992
2024-12-07 04:56:34.533063: Validation loss did not improve from -0.44542. Patience: 2/50
2024-12-07 04:56:34.534395: train_loss -0.4333
2024-12-07 04:56:34.535296: val_loss -0.444
2024-12-07 04:56:34.536213: Pseudo dice [0.6798]
2024-12-07 04:56:34.537392: Epoch time: 370.85 s
2024-12-07 04:56:34.944643: Yayy! New best EMA pseudo Dice: 0.6122
2024-12-07 04:56:36.630854: 
2024-12-07 04:56:36.632195: Epoch 10
2024-12-07 04:56:36.633093: Current learning rate: 0.00991
2024-12-07 05:02:51.582279: Validation loss improved from -0.44542 to -0.47199! Patience: 2/50
2024-12-07 05:02:51.585095: train_loss -0.4389
2024-12-07 05:02:51.586808: val_loss -0.472
2024-12-07 05:02:51.587531: Pseudo dice [0.682]
2024-12-07 05:02:51.588581: Epoch time: 374.96 s
2024-12-07 05:02:51.589413: Yayy! New best EMA pseudo Dice: 0.6192
2024-12-07 05:02:53.309681: 
2024-12-07 05:02:53.311131: Epoch 11
2024-12-07 05:02:53.311941: Current learning rate: 0.0099
2024-12-07 05:09:15.532986: Validation loss did not improve from -0.47199. Patience: 1/50
2024-12-07 05:09:15.535112: train_loss -0.4392
2024-12-07 05:09:15.536093: val_loss -0.4484
2024-12-07 05:09:15.536732: Pseudo dice [0.6881]
2024-12-07 05:09:15.537381: Epoch time: 382.23 s
2024-12-07 05:09:15.537968: Yayy! New best EMA pseudo Dice: 0.6261
2024-12-07 05:09:17.249870: 
2024-12-07 05:09:17.251128: Epoch 12
2024-12-07 05:09:17.251818: Current learning rate: 0.00989
2024-12-07 05:15:50.433865: Validation loss did not improve from -0.47199. Patience: 2/50
2024-12-07 05:15:50.434877: train_loss -0.4545
2024-12-07 05:15:50.436258: val_loss -0.4569
2024-12-07 05:15:50.437333: Pseudo dice [0.6791]
2024-12-07 05:15:50.438385: Epoch time: 393.19 s
2024-12-07 05:15:50.439149: Yayy! New best EMA pseudo Dice: 0.6314
2024-12-07 05:15:52.152716: 
2024-12-07 05:15:52.154377: Epoch 13
2024-12-07 05:15:52.155651: Current learning rate: 0.00988
2024-12-07 05:22:12.000576: Validation loss did not improve from -0.47199. Patience: 3/50
2024-12-07 05:22:12.001490: train_loss -0.4635
2024-12-07 05:22:12.002216: val_loss -0.4324
2024-12-07 05:22:12.002869: Pseudo dice [0.6715]
2024-12-07 05:22:12.003501: Epoch time: 379.85 s
2024-12-07 05:22:12.004233: Yayy! New best EMA pseudo Dice: 0.6354
2024-12-07 05:22:13.720979: 
2024-12-07 05:22:13.722360: Epoch 14
2024-12-07 05:22:13.723214: Current learning rate: 0.00987
2024-12-07 05:28:30.027199: Validation loss improved from -0.47199 to -0.49274! Patience: 3/50
2024-12-07 05:28:30.028071: train_loss -0.4829
2024-12-07 05:28:30.028768: val_loss -0.4927
2024-12-07 05:28:30.029509: Pseudo dice [0.7115]
2024-12-07 05:28:30.030186: Epoch time: 376.31 s
2024-12-07 05:28:30.373011: Yayy! New best EMA pseudo Dice: 0.643
2024-12-07 05:28:32.097816: 
2024-12-07 05:28:32.099229: Epoch 15
2024-12-07 05:28:32.100242: Current learning rate: 0.00986
2024-12-07 05:34:48.996049: Validation loss improved from -0.49274 to -0.50087! Patience: 0/50
2024-12-07 05:34:48.997057: train_loss -0.5008
2024-12-07 05:34:48.997900: val_loss -0.5009
2024-12-07 05:34:48.998631: Pseudo dice [0.7048]
2024-12-07 05:34:48.999298: Epoch time: 376.9 s
2024-12-07 05:34:49.000010: Yayy! New best EMA pseudo Dice: 0.6492
2024-12-07 05:34:50.732531: 
2024-12-07 05:34:50.734115: Epoch 16
2024-12-07 05:34:50.735090: Current learning rate: 0.00986
2024-12-07 05:41:08.478897: Validation loss did not improve from -0.50087. Patience: 1/50
2024-12-07 05:41:08.479825: train_loss -0.5027
2024-12-07 05:41:08.480840: val_loss -0.4902
2024-12-07 05:41:08.481715: Pseudo dice [0.7043]
2024-12-07 05:41:08.482581: Epoch time: 377.75 s
2024-12-07 05:41:08.483481: Yayy! New best EMA pseudo Dice: 0.6547
2024-12-07 05:41:10.257809: 
2024-12-07 05:41:10.259308: Epoch 17
2024-12-07 05:41:10.260407: Current learning rate: 0.00985
2024-12-07 05:47:30.753796: Validation loss improved from -0.50087 to -0.53061! Patience: 1/50
2024-12-07 05:47:30.754856: train_loss -0.5048
2024-12-07 05:47:30.755586: val_loss -0.5306
2024-12-07 05:47:30.756461: Pseudo dice [0.732]
2024-12-07 05:47:30.757388: Epoch time: 380.5 s
2024-12-07 05:47:30.758091: Yayy! New best EMA pseudo Dice: 0.6624
2024-12-07 05:47:33.408947: 
2024-12-07 05:47:33.410222: Epoch 18
2024-12-07 05:47:33.410876: Current learning rate: 0.00984
2024-12-07 05:54:01.682658: Validation loss did not improve from -0.53061. Patience: 1/50
2024-12-07 05:54:01.683953: train_loss -0.5183
2024-12-07 05:54:01.684674: val_loss -0.4882
2024-12-07 05:54:01.685349: Pseudo dice [0.7007]
2024-12-07 05:54:01.686041: Epoch time: 388.28 s
2024-12-07 05:54:01.686943: Yayy! New best EMA pseudo Dice: 0.6663
2024-12-07 05:54:03.433357: 
2024-12-07 05:54:03.434517: Epoch 19
2024-12-07 05:54:03.435250: Current learning rate: 0.00983
2024-12-07 06:00:39.103475: Validation loss did not improve from -0.53061. Patience: 2/50
2024-12-07 06:00:39.104499: train_loss -0.5142
2024-12-07 06:00:39.105581: val_loss -0.489
2024-12-07 06:00:39.106772: Pseudo dice [0.6969]
2024-12-07 06:00:39.107844: Epoch time: 395.67 s
2024-12-07 06:00:39.557187: Yayy! New best EMA pseudo Dice: 0.6693
2024-12-07 06:00:41.592315: 
2024-12-07 06:00:41.593945: Epoch 20
2024-12-07 06:00:41.594962: Current learning rate: 0.00982
2024-12-07 06:07:15.686158: Validation loss did not improve from -0.53061. Patience: 3/50
2024-12-07 06:07:15.688849: train_loss -0.5279
2024-12-07 06:07:15.690390: val_loss -0.4869
2024-12-07 06:07:15.691247: Pseudo dice [0.6947]
2024-12-07 06:07:15.692457: Epoch time: 394.1 s
2024-12-07 06:07:15.693323: Yayy! New best EMA pseudo Dice: 0.6719
2024-12-07 06:07:17.483530: 
2024-12-07 06:07:17.485384: Epoch 21
2024-12-07 06:07:17.486398: Current learning rate: 0.00981
2024-12-07 06:13:49.732014: Validation loss did not improve from -0.53061. Patience: 4/50
2024-12-07 06:13:49.734608: train_loss -0.5262
2024-12-07 06:13:49.735850: val_loss -0.5297
2024-12-07 06:13:49.736713: Pseudo dice [0.7264]
2024-12-07 06:13:49.738093: Epoch time: 392.25 s
2024-12-07 06:13:49.739486: Yayy! New best EMA pseudo Dice: 0.6773
2024-12-07 06:13:51.494746: 
2024-12-07 06:13:51.496137: Epoch 22
2024-12-07 06:13:51.497014: Current learning rate: 0.0098
2024-12-07 06:20:22.953607: Validation loss did not improve from -0.53061. Patience: 5/50
2024-12-07 06:20:22.954817: train_loss -0.5371
2024-12-07 06:20:22.955724: val_loss -0.5005
2024-12-07 06:20:22.956546: Pseudo dice [0.7088]
2024-12-07 06:20:22.957790: Epoch time: 391.46 s
2024-12-07 06:20:22.958565: Yayy! New best EMA pseudo Dice: 0.6805
2024-12-07 06:20:24.646085: 
2024-12-07 06:20:24.647398: Epoch 23
2024-12-07 06:20:24.648777: Current learning rate: 0.00979
2024-12-07 06:27:00.206926: Validation loss did not improve from -0.53061. Patience: 6/50
2024-12-07 06:27:00.207779: train_loss -0.5343
2024-12-07 06:27:00.208497: val_loss -0.5065
2024-12-07 06:27:00.209156: Pseudo dice [0.7101]
2024-12-07 06:27:00.209923: Epoch time: 395.56 s
2024-12-07 06:27:00.210876: Yayy! New best EMA pseudo Dice: 0.6834
2024-12-07 06:27:01.922141: 
2024-12-07 06:27:01.923503: Epoch 24
2024-12-07 06:27:01.924524: Current learning rate: 0.00978
2024-12-07 06:33:40.115805: Validation loss did not improve from -0.53061. Patience: 7/50
2024-12-07 06:33:40.117187: train_loss -0.5505
2024-12-07 06:33:40.118331: val_loss -0.5303
2024-12-07 06:33:40.119116: Pseudo dice [0.7321]
2024-12-07 06:33:40.120148: Epoch time: 398.2 s
2024-12-07 06:33:40.449708: Yayy! New best EMA pseudo Dice: 0.6883
2024-12-07 06:33:42.136225: 
2024-12-07 06:33:42.137859: Epoch 25
2024-12-07 06:33:42.138677: Current learning rate: 0.00977
2024-12-07 06:40:14.866360: Validation loss improved from -0.53061 to -0.54209! Patience: 7/50
2024-12-07 06:40:14.867490: train_loss -0.5552
2024-12-07 06:40:14.868728: val_loss -0.5421
2024-12-07 06:40:14.869404: Pseudo dice [0.734]
2024-12-07 06:40:14.870363: Epoch time: 392.73 s
2024-12-07 06:40:14.871109: Yayy! New best EMA pseudo Dice: 0.6929
2024-12-07 06:40:16.577382: 
2024-12-07 06:40:16.578868: Epoch 26
2024-12-07 06:40:16.580076: Current learning rate: 0.00977
2024-12-07 06:46:59.302638: Validation loss improved from -0.54209 to -0.56834! Patience: 0/50
2024-12-07 06:46:59.303925: train_loss -0.5464
2024-12-07 06:46:59.304999: val_loss -0.5683
2024-12-07 06:46:59.306011: Pseudo dice [0.7491]
2024-12-07 06:46:59.307081: Epoch time: 402.73 s
2024-12-07 06:46:59.308304: Yayy! New best EMA pseudo Dice: 0.6985
2024-12-07 06:47:00.972891: 
2024-12-07 06:47:00.974225: Epoch 27
2024-12-07 06:47:00.975104: Current learning rate: 0.00976
2024-12-07 06:53:31.305804: Validation loss did not improve from -0.56834. Patience: 1/50
2024-12-07 06:53:31.307240: train_loss -0.5609
2024-12-07 06:53:31.308564: val_loss -0.5569
2024-12-07 06:53:31.309221: Pseudo dice [0.7415]
2024-12-07 06:53:31.311496: Epoch time: 390.34 s
2024-12-07 06:53:31.312372: Yayy! New best EMA pseudo Dice: 0.7028
2024-12-07 06:53:33.463132: 
2024-12-07 06:53:33.464523: Epoch 28
2024-12-07 06:53:33.465437: Current learning rate: 0.00975
2024-12-07 07:00:13.872943: Validation loss did not improve from -0.56834. Patience: 2/50
2024-12-07 07:00:13.874050: train_loss -0.5582
2024-12-07 07:00:13.874836: val_loss -0.5285
2024-12-07 07:00:13.875510: Pseudo dice [0.7285]
2024-12-07 07:00:13.876203: Epoch time: 400.41 s
2024-12-07 07:00:13.876851: Yayy! New best EMA pseudo Dice: 0.7054
2024-12-07 07:00:15.591526: 
2024-12-07 07:00:15.592879: Epoch 29
2024-12-07 07:00:15.593552: Current learning rate: 0.00974
2024-12-07 07:06:49.068558: Validation loss did not improve from -0.56834. Patience: 3/50
2024-12-07 07:06:49.069631: train_loss -0.5671
2024-12-07 07:06:49.070497: val_loss -0.556
2024-12-07 07:06:49.071258: Pseudo dice [0.7435]
2024-12-07 07:06:49.072192: Epoch time: 393.48 s
2024-12-07 07:06:49.453649: Yayy! New best EMA pseudo Dice: 0.7092
2024-12-07 07:06:51.182447: 
2024-12-07 07:06:51.183742: Epoch 30
2024-12-07 07:06:51.184462: Current learning rate: 0.00973
2024-12-07 07:13:39.154638: Validation loss did not improve from -0.56834. Patience: 4/50
2024-12-07 07:13:39.158053: train_loss -0.5831
2024-12-07 07:13:39.159781: val_loss -0.5596
2024-12-07 07:13:39.160902: Pseudo dice [0.7474]
2024-12-07 07:13:39.162187: Epoch time: 407.98 s
2024-12-07 07:13:39.163119: Yayy! New best EMA pseudo Dice: 0.713
2024-12-07 07:13:40.899292: 
2024-12-07 07:13:40.900588: Epoch 31
2024-12-07 07:13:40.901639: Current learning rate: 0.00972
2024-12-07 07:20:22.379531: Validation loss did not improve from -0.56834. Patience: 5/50
2024-12-07 07:20:22.381598: train_loss -0.5716
2024-12-07 07:20:22.382997: val_loss -0.5046
2024-12-07 07:20:22.383949: Pseudo dice [0.7009]
2024-12-07 07:20:22.384730: Epoch time: 401.48 s
2024-12-07 07:20:23.731419: 
2024-12-07 07:20:23.732644: Epoch 32
2024-12-07 07:20:23.733516: Current learning rate: 0.00971
2024-12-07 07:27:00.262862: Validation loss did not improve from -0.56834. Patience: 6/50
2024-12-07 07:27:00.263755: train_loss -0.5682
2024-12-07 07:27:00.264585: val_loss -0.5506
2024-12-07 07:27:00.265298: Pseudo dice [0.7356]
2024-12-07 07:27:00.266362: Epoch time: 396.53 s
2024-12-07 07:27:00.267391: Yayy! New best EMA pseudo Dice: 0.7142
2024-12-07 07:27:01.989424: 
2024-12-07 07:27:01.990948: Epoch 33
2024-12-07 07:27:01.992134: Current learning rate: 0.0097
2024-12-07 07:33:30.152160: Validation loss did not improve from -0.56834. Patience: 7/50
2024-12-07 07:33:30.153081: train_loss -0.5706
2024-12-07 07:33:30.153892: val_loss -0.564
2024-12-07 07:33:30.154830: Pseudo dice [0.7469]
2024-12-07 07:33:30.155971: Epoch time: 388.17 s
2024-12-07 07:33:30.156734: Yayy! New best EMA pseudo Dice: 0.7174
2024-12-07 07:33:31.895432: 
2024-12-07 07:33:31.896998: Epoch 34
2024-12-07 07:33:31.897827: Current learning rate: 0.00969
2024-12-07 07:40:10.658849: Validation loss did not improve from -0.56834. Patience: 8/50
2024-12-07 07:40:10.659849: train_loss -0.5771
2024-12-07 07:40:10.660786: val_loss -0.546
2024-12-07 07:40:10.662378: Pseudo dice [0.7427]
2024-12-07 07:40:10.663330: Epoch time: 398.77 s
2024-12-07 07:40:11.013970: Yayy! New best EMA pseudo Dice: 0.72
2024-12-07 07:40:12.793262: 
2024-12-07 07:40:12.794838: Epoch 35
2024-12-07 07:40:12.796026: Current learning rate: 0.00968
2024-12-07 07:46:47.401497: Validation loss did not improve from -0.56834. Patience: 9/50
2024-12-07 07:46:47.402484: train_loss -0.5764
2024-12-07 07:46:47.403169: val_loss -0.5267
2024-12-07 07:46:47.403761: Pseudo dice [0.7261]
2024-12-07 07:46:47.404389: Epoch time: 394.61 s
2024-12-07 07:46:47.404983: Yayy! New best EMA pseudo Dice: 0.7206
2024-12-07 07:46:49.134828: 
2024-12-07 07:46:49.136134: Epoch 36
2024-12-07 07:46:49.136953: Current learning rate: 0.00968
2024-12-07 07:53:27.427925: Validation loss did not improve from -0.56834. Patience: 10/50
2024-12-07 07:53:27.428877: train_loss -0.5883
2024-12-07 07:53:27.429847: val_loss -0.526
2024-12-07 07:53:27.430899: Pseudo dice [0.7212]
2024-12-07 07:53:27.431587: Epoch time: 398.3 s
2024-12-07 07:53:27.432599: Yayy! New best EMA pseudo Dice: 0.7206
2024-12-07 07:53:29.176445: 
2024-12-07 07:53:29.177668: Epoch 37
2024-12-07 07:53:29.178360: Current learning rate: 0.00967
2024-12-07 08:00:09.452741: Validation loss did not improve from -0.56834. Patience: 11/50
2024-12-07 08:00:09.453562: train_loss -0.5819
2024-12-07 08:00:09.454648: val_loss -0.5437
2024-12-07 08:00:09.455639: Pseudo dice [0.7327]
2024-12-07 08:00:09.456461: Epoch time: 400.28 s
2024-12-07 08:00:09.457257: Yayy! New best EMA pseudo Dice: 0.7218
2024-12-07 08:00:11.588648: 
2024-12-07 08:00:11.590136: Epoch 38
2024-12-07 08:00:11.590864: Current learning rate: 0.00966
2024-12-07 08:06:43.455892: Validation loss did not improve from -0.56834. Patience: 12/50
2024-12-07 08:06:43.457358: train_loss -0.5816
2024-12-07 08:06:43.458656: val_loss -0.5597
2024-12-07 08:06:43.459597: Pseudo dice [0.7413]
2024-12-07 08:06:43.460624: Epoch time: 391.87 s
2024-12-07 08:06:43.461308: Yayy! New best EMA pseudo Dice: 0.7238
2024-12-07 08:06:45.217038: 
2024-12-07 08:06:45.218234: Epoch 39
2024-12-07 08:06:45.219090: Current learning rate: 0.00965
2024-12-07 08:13:23.611682: Validation loss did not improve from -0.56834. Patience: 13/50
2024-12-07 08:13:23.612617: train_loss -0.5882
2024-12-07 08:13:23.613423: val_loss -0.5464
2024-12-07 08:13:23.614107: Pseudo dice [0.7359]
2024-12-07 08:13:23.614859: Epoch time: 398.4 s
2024-12-07 08:13:23.986189: Yayy! New best EMA pseudo Dice: 0.725
2024-12-07 08:13:25.797912: 
2024-12-07 08:13:25.799446: Epoch 40
2024-12-07 08:13:25.800323: Current learning rate: 0.00964
2024-12-07 08:19:57.290826: Validation loss did not improve from -0.56834. Patience: 14/50
2024-12-07 08:19:57.295171: train_loss -0.5936
2024-12-07 08:19:57.297580: val_loss -0.5567
2024-12-07 08:19:57.298625: Pseudo dice [0.7473]
2024-12-07 08:19:57.300102: Epoch time: 391.5 s
2024-12-07 08:19:57.301186: Yayy! New best EMA pseudo Dice: 0.7272
2024-12-07 08:19:59.106550: 
2024-12-07 08:19:59.107986: Epoch 41
2024-12-07 08:19:59.108926: Current learning rate: 0.00963
2024-12-07 08:26:35.622520: Validation loss improved from -0.56834 to -0.57166! Patience: 14/50
2024-12-07 08:26:35.624486: train_loss -0.5987
2024-12-07 08:26:35.625456: val_loss -0.5717
2024-12-07 08:26:35.626250: Pseudo dice [0.753]
2024-12-07 08:26:35.626965: Epoch time: 396.52 s
2024-12-07 08:26:35.627733: Yayy! New best EMA pseudo Dice: 0.7298
2024-12-07 08:26:37.326421: 
2024-12-07 08:26:37.327507: Epoch 42
2024-12-07 08:26:37.328386: Current learning rate: 0.00962
2024-12-07 08:33:20.543307: Validation loss did not improve from -0.57166. Patience: 1/50
2024-12-07 08:33:20.544289: train_loss -0.6108
2024-12-07 08:33:20.545040: val_loss -0.5704
2024-12-07 08:33:20.545675: Pseudo dice [0.7494]
2024-12-07 08:33:20.546622: Epoch time: 403.22 s
2024-12-07 08:33:20.547284: Yayy! New best EMA pseudo Dice: 0.7318
2024-12-07 08:33:22.237043: 
2024-12-07 08:33:22.237934: Epoch 43
2024-12-07 08:33:22.238576: Current learning rate: 0.00961
2024-12-07 08:40:03.811580: Validation loss did not improve from -0.57166. Patience: 2/50
2024-12-07 08:40:03.812657: train_loss -0.607
2024-12-07 08:40:03.813889: val_loss -0.5618
2024-12-07 08:40:03.814942: Pseudo dice [0.7448]
2024-12-07 08:40:03.816070: Epoch time: 401.58 s
2024-12-07 08:40:03.817219: Yayy! New best EMA pseudo Dice: 0.7331
2024-12-07 08:40:05.520420: 
2024-12-07 08:40:05.522027: Epoch 44
2024-12-07 08:40:05.523046: Current learning rate: 0.0096
2024-12-07 08:46:52.936068: Validation loss improved from -0.57166 to -0.60024! Patience: 2/50
2024-12-07 08:46:52.937739: train_loss -0.6122
2024-12-07 08:46:52.938905: val_loss -0.6002
2024-12-07 08:46:52.939752: Pseudo dice [0.7614]
2024-12-07 08:46:52.940460: Epoch time: 407.42 s
2024-12-07 08:46:53.329039: Yayy! New best EMA pseudo Dice: 0.7359
2024-12-07 08:46:55.032135: 
2024-12-07 08:46:55.033674: Epoch 45
2024-12-07 08:46:55.034645: Current learning rate: 0.00959
2024-12-07 08:53:50.275251: Validation loss did not improve from -0.60024. Patience: 1/50
2024-12-07 08:53:50.276176: train_loss -0.6067
2024-12-07 08:53:50.277413: val_loss -0.5621
2024-12-07 08:53:50.278435: Pseudo dice [0.7438]
2024-12-07 08:53:50.279343: Epoch time: 415.25 s
2024-12-07 08:53:50.280221: Yayy! New best EMA pseudo Dice: 0.7367
2024-12-07 08:53:52.035399: 
2024-12-07 08:53:52.036536: Epoch 46
2024-12-07 08:53:52.037212: Current learning rate: 0.00959
2024-12-07 09:00:37.892398: Validation loss did not improve from -0.60024. Patience: 2/50
2024-12-07 09:00:37.893329: train_loss -0.6174
2024-12-07 09:00:37.894261: val_loss -0.5844
2024-12-07 09:00:37.895024: Pseudo dice [0.7589]
2024-12-07 09:00:37.895859: Epoch time: 405.86 s
2024-12-07 09:00:37.896723: Yayy! New best EMA pseudo Dice: 0.7389
2024-12-07 09:00:39.541482: 
2024-12-07 09:00:39.542444: Epoch 47
2024-12-07 09:00:39.543398: Current learning rate: 0.00958
2024-12-07 09:07:19.505607: Validation loss did not improve from -0.60024. Patience: 3/50
2024-12-07 09:07:19.506443: train_loss -0.6176
2024-12-07 09:07:19.507269: val_loss -0.5481
2024-12-07 09:07:19.508110: Pseudo dice [0.7359]
2024-12-07 09:07:19.508884: Epoch time: 399.97 s
2024-12-07 09:07:21.234760: 
2024-12-07 09:07:21.236061: Epoch 48
2024-12-07 09:07:21.236802: Current learning rate: 0.00957
2024-12-07 09:13:56.712398: Validation loss did not improve from -0.60024. Patience: 4/50
2024-12-07 09:13:56.713438: train_loss -0.6191
2024-12-07 09:13:56.714281: val_loss -0.5714
2024-12-07 09:13:56.715035: Pseudo dice [0.7502]
2024-12-07 09:13:56.715792: Epoch time: 395.48 s
2024-12-07 09:13:56.716475: Yayy! New best EMA pseudo Dice: 0.7398
2024-12-07 09:13:58.399788: 
2024-12-07 09:13:58.400756: Epoch 49
2024-12-07 09:13:58.401477: Current learning rate: 0.00956
2024-12-07 09:20:15.950909: Validation loss did not improve from -0.60024. Patience: 5/50
2024-12-07 09:20:15.951729: train_loss -0.6206
2024-12-07 09:20:15.952454: val_loss -0.5448
2024-12-07 09:20:15.953109: Pseudo dice [0.7353]
2024-12-07 09:20:15.953810: Epoch time: 377.55 s
2024-12-07 09:20:17.620709: 
2024-12-07 09:20:17.622001: Epoch 50
2024-12-07 09:20:17.622915: Current learning rate: 0.00955
2024-12-07 09:26:04.589624: Validation loss did not improve from -0.60024. Patience: 6/50
2024-12-07 09:26:04.592782: train_loss -0.6247
2024-12-07 09:26:04.594568: val_loss -0.556
2024-12-07 09:26:04.595399: Pseudo dice [0.7366]
2024-12-07 09:26:04.596553: Epoch time: 346.97 s
2024-12-07 09:26:05.932361: 
2024-12-07 09:26:05.933552: Epoch 51
2024-12-07 09:26:05.934318: Current learning rate: 0.00954
2024-12-07 09:32:08.717926: Validation loss did not improve from -0.60024. Patience: 7/50
2024-12-07 09:32:08.719666: train_loss -0.6241
2024-12-07 09:32:08.720708: val_loss -0.5683
2024-12-07 09:32:08.721497: Pseudo dice [0.744]
2024-12-07 09:32:08.722325: Epoch time: 362.79 s
2024-12-07 09:32:10.046784: 
2024-12-07 09:32:10.048039: Epoch 52
2024-12-07 09:32:10.048949: Current learning rate: 0.00953
2024-12-07 09:38:16.117041: Validation loss did not improve from -0.60024. Patience: 8/50
2024-12-07 09:38:16.118009: train_loss -0.6237
2024-12-07 09:38:16.118890: val_loss -0.5803
2024-12-07 09:38:16.119529: Pseudo dice [0.7561]
2024-12-07 09:38:16.120297: Epoch time: 366.07 s
2024-12-07 09:38:16.120966: Yayy! New best EMA pseudo Dice: 0.7412
2024-12-07 09:38:17.797933: 
2024-12-07 09:38:17.799182: Epoch 53
2024-12-07 09:38:17.799856: Current learning rate: 0.00952
2024-12-07 09:44:31.157230: Validation loss did not improve from -0.60024. Patience: 9/50
2024-12-07 09:44:31.158239: train_loss -0.6372
2024-12-07 09:44:31.159205: val_loss -0.5676
2024-12-07 09:44:31.159912: Pseudo dice [0.7503]
2024-12-07 09:44:31.160848: Epoch time: 373.36 s
2024-12-07 09:44:31.161853: Yayy! New best EMA pseudo Dice: 0.7421
2024-12-07 09:44:32.853721: 
2024-12-07 09:44:32.855207: Epoch 54
2024-12-07 09:44:32.856346: Current learning rate: 0.00951
2024-12-07 09:50:50.078893: Validation loss did not improve from -0.60024. Patience: 10/50
2024-12-07 09:50:50.079925: train_loss -0.624
2024-12-07 09:50:50.080854: val_loss -0.5706
2024-12-07 09:50:50.081697: Pseudo dice [0.7539]
2024-12-07 09:50:50.082458: Epoch time: 377.23 s
2024-12-07 09:50:50.459250: Yayy! New best EMA pseudo Dice: 0.7433
2024-12-07 09:50:52.151149: 
2024-12-07 09:50:52.152652: Epoch 55
2024-12-07 09:50:52.153784: Current learning rate: 0.0095
2024-12-07 09:57:19.612252: Validation loss did not improve from -0.60024. Patience: 11/50
2024-12-07 09:57:19.613392: train_loss -0.627
2024-12-07 09:57:19.614412: val_loss -0.5983
2024-12-07 09:57:19.615423: Pseudo dice [0.7705]
2024-12-07 09:57:19.616185: Epoch time: 387.46 s
2024-12-07 09:57:19.617019: Yayy! New best EMA pseudo Dice: 0.746
2024-12-07 09:57:21.342320: 
2024-12-07 09:57:21.343634: Epoch 56
2024-12-07 09:57:21.344499: Current learning rate: 0.00949
2024-12-07 10:04:03.319814: Validation loss did not improve from -0.60024. Patience: 12/50
2024-12-07 10:04:03.320826: train_loss -0.6318
2024-12-07 10:04:03.321843: val_loss -0.5919
2024-12-07 10:04:03.322741: Pseudo dice [0.7638]
2024-12-07 10:04:03.323715: Epoch time: 401.98 s
2024-12-07 10:04:03.324580: Yayy! New best EMA pseudo Dice: 0.7478
2024-12-07 10:04:05.026658: 
2024-12-07 10:04:05.027776: Epoch 57
2024-12-07 10:04:05.028550: Current learning rate: 0.00949
2024-12-07 10:10:35.613867: Validation loss did not improve from -0.60024. Patience: 13/50
2024-12-07 10:10:35.614826: train_loss -0.6393
2024-12-07 10:10:35.615602: val_loss -0.5613
2024-12-07 10:10:35.616243: Pseudo dice [0.7431]
2024-12-07 10:10:35.617245: Epoch time: 390.59 s
2024-12-07 10:10:36.918218: 
2024-12-07 10:10:36.919681: Epoch 58
2024-12-07 10:10:36.920437: Current learning rate: 0.00948
2024-12-07 10:17:06.837676: Validation loss did not improve from -0.60024. Patience: 14/50
2024-12-07 10:17:06.838771: train_loss -0.6263
2024-12-07 10:17:06.839740: val_loss -0.5791
2024-12-07 10:17:06.840552: Pseudo dice [0.7597]
2024-12-07 10:17:06.841322: Epoch time: 389.92 s
2024-12-07 10:17:06.842116: Yayy! New best EMA pseudo Dice: 0.7486
2024-12-07 10:17:09.242951: 
2024-12-07 10:17:09.244466: Epoch 59
2024-12-07 10:17:09.245291: Current learning rate: 0.00947
2024-12-07 10:23:36.242559: Validation loss did not improve from -0.60024. Patience: 15/50
2024-12-07 10:23:36.243608: train_loss -0.6378
2024-12-07 10:23:36.244525: val_loss -0.5376
2024-12-07 10:23:36.245653: Pseudo dice [0.729]
2024-12-07 10:23:36.246604: Epoch time: 387.0 s
2024-12-07 10:23:38.000910: 
2024-12-07 10:23:38.002496: Epoch 60
2024-12-07 10:23:38.003705: Current learning rate: 0.00946
2024-12-07 10:30:09.091252: Validation loss did not improve from -0.60024. Patience: 16/50
2024-12-07 10:30:09.095202: train_loss -0.6444
2024-12-07 10:30:09.097542: val_loss -0.5655
2024-12-07 10:30:09.098640: Pseudo dice [0.753]
2024-12-07 10:30:09.099905: Epoch time: 391.1 s
2024-12-07 10:30:10.479562: 
2024-12-07 10:30:10.480803: Epoch 61
2024-12-07 10:30:10.482191: Current learning rate: 0.00945
2024-12-07 10:36:39.142023: Validation loss did not improve from -0.60024. Patience: 17/50
2024-12-07 10:36:39.143970: train_loss -0.6469
2024-12-07 10:36:39.144916: val_loss -0.5852
2024-12-07 10:36:39.145819: Pseudo dice [0.7571]
2024-12-07 10:36:39.146592: Epoch time: 388.67 s
2024-12-07 10:36:40.508659: 
2024-12-07 10:36:40.509905: Epoch 62
2024-12-07 10:36:40.510616: Current learning rate: 0.00944
2024-12-07 10:43:08.303621: Validation loss did not improve from -0.60024. Patience: 18/50
2024-12-07 10:43:08.304522: train_loss -0.6512
2024-12-07 10:43:08.305302: val_loss -0.591
2024-12-07 10:43:08.305946: Pseudo dice [0.7646]
2024-12-07 10:43:08.306700: Epoch time: 387.8 s
2024-12-07 10:43:08.307462: Yayy! New best EMA pseudo Dice: 0.7499
2024-12-07 10:43:10.026484: 
2024-12-07 10:43:10.027889: Epoch 63
2024-12-07 10:43:10.028806: Current learning rate: 0.00943
2024-12-07 10:49:46.359008: Validation loss did not improve from -0.60024. Patience: 19/50
2024-12-07 10:49:46.360329: train_loss -0.6534
2024-12-07 10:49:46.361275: val_loss -0.5814
2024-12-07 10:49:46.362100: Pseudo dice [0.7559]
2024-12-07 10:49:46.363054: Epoch time: 396.33 s
2024-12-07 10:49:46.363781: Yayy! New best EMA pseudo Dice: 0.7505
2024-12-07 10:49:48.074138: 
2024-12-07 10:49:48.075594: Epoch 64
2024-12-07 10:49:48.076601: Current learning rate: 0.00942
2024-12-07 10:56:16.739331: Validation loss did not improve from -0.60024. Patience: 20/50
2024-12-07 10:56:16.740504: train_loss -0.6536
2024-12-07 10:56:16.741244: val_loss -0.5933
2024-12-07 10:56:16.742059: Pseudo dice [0.7672]
2024-12-07 10:56:16.742799: Epoch time: 388.67 s
2024-12-07 10:56:17.086623: Yayy! New best EMA pseudo Dice: 0.7521
2024-12-07 10:56:18.808451: 
2024-12-07 10:56:18.810187: Epoch 65
2024-12-07 10:56:18.811277: Current learning rate: 0.00941
2024-12-07 11:02:57.530119: Validation loss did not improve from -0.60024. Patience: 21/50
2024-12-07 11:02:57.531082: train_loss -0.6564
2024-12-07 11:02:57.531893: val_loss -0.5575
2024-12-07 11:02:57.532650: Pseudo dice [0.7413]
2024-12-07 11:02:57.533316: Epoch time: 398.72 s
2024-12-07 11:02:58.933961: 
2024-12-07 11:02:58.935296: Epoch 66
2024-12-07 11:02:58.936001: Current learning rate: 0.0094
2024-12-07 11:09:44.798944: Validation loss improved from -0.60024 to -0.60532! Patience: 21/50
2024-12-07 11:09:44.800023: train_loss -0.6617
2024-12-07 11:09:44.800829: val_loss -0.6053
2024-12-07 11:09:44.801469: Pseudo dice [0.7703]
2024-12-07 11:09:44.802212: Epoch time: 405.87 s
2024-12-07 11:09:44.802923: Yayy! New best EMA pseudo Dice: 0.753
2024-12-07 11:09:46.564376: 
2024-12-07 11:09:46.565579: Epoch 67
2024-12-07 11:09:46.566337: Current learning rate: 0.00939
2024-12-07 11:16:21.696144: Validation loss did not improve from -0.60532. Patience: 1/50
2024-12-07 11:16:21.697143: train_loss -0.6546
2024-12-07 11:16:21.697952: val_loss -0.5947
2024-12-07 11:16:21.698617: Pseudo dice [0.7713]
2024-12-07 11:16:21.699397: Epoch time: 395.13 s
2024-12-07 11:16:21.700387: Yayy! New best EMA pseudo Dice: 0.7548
2024-12-07 11:16:23.450971: 
2024-12-07 11:16:23.452439: Epoch 68
2024-12-07 11:16:23.453833: Current learning rate: 0.00939
2024-12-07 11:22:53.070572: Validation loss did not improve from -0.60532. Patience: 2/50
2024-12-07 11:22:53.071996: train_loss -0.658
2024-12-07 11:22:53.073071: val_loss -0.5385
2024-12-07 11:22:53.073823: Pseudo dice [0.7359]
2024-12-07 11:22:53.074549: Epoch time: 389.62 s
2024-12-07 11:22:54.434165: 
2024-12-07 11:22:54.435689: Epoch 69
2024-12-07 11:22:54.436496: Current learning rate: 0.00938
2024-12-07 11:29:33.770554: Validation loss did not improve from -0.60532. Patience: 3/50
2024-12-07 11:29:33.771598: train_loss -0.666
2024-12-07 11:29:33.772877: val_loss -0.585
2024-12-07 11:29:33.773613: Pseudo dice [0.7618]
2024-12-07 11:29:33.774341: Epoch time: 399.34 s
2024-12-07 11:29:36.064002: 
2024-12-07 11:29:36.065696: Epoch 70
2024-12-07 11:29:36.066686: Current learning rate: 0.00937
2024-12-07 11:36:14.184935: Validation loss did not improve from -0.60532. Patience: 4/50
2024-12-07 11:36:14.189973: train_loss -0.6622
2024-12-07 11:36:14.192174: val_loss -0.5984
2024-12-07 11:36:14.193199: Pseudo dice [0.7701]
2024-12-07 11:36:14.194313: Epoch time: 398.13 s
2024-12-07 11:36:14.195315: Yayy! New best EMA pseudo Dice: 0.7554
2024-12-07 11:36:16.026842: 
2024-12-07 11:36:16.028694: Epoch 71
2024-12-07 11:36:16.029461: Current learning rate: 0.00936
2024-12-07 11:42:39.133878: Validation loss improved from -0.60532 to -0.60541! Patience: 4/50
2024-12-07 11:42:39.135845: train_loss -0.6731
2024-12-07 11:42:39.136950: val_loss -0.6054
2024-12-07 11:42:39.137631: Pseudo dice [0.7779]
2024-12-07 11:42:39.138318: Epoch time: 383.11 s
2024-12-07 11:42:39.138926: Yayy! New best EMA pseudo Dice: 0.7577
2024-12-07 11:42:40.948348: 
2024-12-07 11:42:40.949537: Epoch 72
2024-12-07 11:42:40.950321: Current learning rate: 0.00935
2024-12-07 11:49:07.533891: Validation loss did not improve from -0.60541. Patience: 1/50
2024-12-07 11:49:07.534874: train_loss -0.6659
2024-12-07 11:49:07.535587: val_loss -0.5944
2024-12-07 11:49:07.536425: Pseudo dice [0.7604]
2024-12-07 11:49:07.537579: Epoch time: 386.59 s
2024-12-07 11:49:07.538388: Yayy! New best EMA pseudo Dice: 0.758
2024-12-07 11:49:09.282116: 
2024-12-07 11:49:09.283777: Epoch 73
2024-12-07 11:49:09.284624: Current learning rate: 0.00934
2024-12-07 11:55:33.671397: Validation loss did not improve from -0.60541. Patience: 2/50
2024-12-07 11:55:33.672498: train_loss -0.6615
2024-12-07 11:55:33.673520: val_loss -0.586
2024-12-07 11:55:33.674381: Pseudo dice [0.7648]
2024-12-07 11:55:33.675236: Epoch time: 384.39 s
2024-12-07 11:55:33.676004: Yayy! New best EMA pseudo Dice: 0.7586
2024-12-07 11:55:35.437285: 
2024-12-07 11:55:35.438313: Epoch 74
2024-12-07 11:55:35.439363: Current learning rate: 0.00933
2024-12-07 12:02:07.168910: Validation loss did not improve from -0.60541. Patience: 3/50
2024-12-07 12:02:07.170062: train_loss -0.6661
2024-12-07 12:02:07.171012: val_loss -0.5675
2024-12-07 12:02:07.171694: Pseudo dice [0.752]
2024-12-07 12:02:07.172384: Epoch time: 391.73 s
2024-12-07 12:02:09.077419: 
2024-12-07 12:02:09.078875: Epoch 75
2024-12-07 12:02:09.079713: Current learning rate: 0.00932
2024-12-07 12:08:42.938096: Validation loss did not improve from -0.60541. Patience: 4/50
2024-12-07 12:08:42.939435: train_loss -0.6765
2024-12-07 12:08:42.940456: val_loss -0.5779
2024-12-07 12:08:42.941594: Pseudo dice [0.7559]
2024-12-07 12:08:42.942558: Epoch time: 393.86 s
2024-12-07 12:08:44.316128: 
2024-12-07 12:08:44.317496: Epoch 76
2024-12-07 12:08:44.318521: Current learning rate: 0.00931
2024-12-07 12:15:08.213458: Validation loss did not improve from -0.60541. Patience: 5/50
2024-12-07 12:15:08.214973: train_loss -0.667
2024-12-07 12:15:08.215867: val_loss -0.56
2024-12-07 12:15:08.216727: Pseudo dice [0.7412]
2024-12-07 12:15:08.217772: Epoch time: 383.9 s
2024-12-07 12:15:09.624840: 
2024-12-07 12:15:09.626242: Epoch 77
2024-12-07 12:15:09.627306: Current learning rate: 0.0093
2024-12-07 12:21:28.127445: Validation loss did not improve from -0.60541. Patience: 6/50
2024-12-07 12:21:28.128506: train_loss -0.6713
2024-12-07 12:21:28.129474: val_loss -0.5925
2024-12-07 12:21:28.130296: Pseudo dice [0.76]
2024-12-07 12:21:28.131089: Epoch time: 378.5 s
2024-12-07 12:21:29.546211: 
2024-12-07 12:21:29.547619: Epoch 78
2024-12-07 12:21:29.548765: Current learning rate: 0.0093
2024-12-07 12:28:19.251526: Validation loss did not improve from -0.60541. Patience: 7/50
2024-12-07 12:28:19.252492: train_loss -0.6671
2024-12-07 12:28:19.253368: val_loss -0.5752
2024-12-07 12:28:19.254164: Pseudo dice [0.7578]
2024-12-07 12:28:19.254871: Epoch time: 409.71 s
2024-12-07 12:28:20.677213: 
2024-12-07 12:28:20.678285: Epoch 79
2024-12-07 12:28:20.679028: Current learning rate: 0.00929
2024-12-07 12:35:11.289051: Validation loss did not improve from -0.60541. Patience: 8/50
2024-12-07 12:35:11.290236: train_loss -0.6743
2024-12-07 12:35:11.291307: val_loss -0.6031
2024-12-07 12:35:11.292072: Pseudo dice [0.7668]
2024-12-07 12:35:11.292784: Epoch time: 410.61 s
2024-12-07 12:35:14.324881: 
2024-12-07 12:35:14.326463: Epoch 80
2024-12-07 12:35:14.327235: Current learning rate: 0.00928
2024-12-07 12:42:05.083029: Validation loss did not improve from -0.60541. Patience: 9/50
2024-12-07 12:42:05.086967: train_loss -0.6831
2024-12-07 12:42:05.088523: val_loss -0.5633
2024-12-07 12:42:05.089318: Pseudo dice [0.7479]
2024-12-07 12:42:05.090253: Epoch time: 410.76 s
2024-12-07 12:42:06.541727: 
2024-12-07 12:42:06.543393: Epoch 81
2024-12-07 12:42:06.544267: Current learning rate: 0.00927
2024-12-07 12:49:06.462323: Validation loss did not improve from -0.60541. Patience: 10/50
2024-12-07 12:49:06.464173: train_loss -0.6829
2024-12-07 12:49:06.465037: val_loss -0.5803
2024-12-07 12:49:06.465627: Pseudo dice [0.7549]
2024-12-07 12:49:06.466290: Epoch time: 419.92 s
2024-12-07 12:49:07.870425: 
2024-12-07 12:49:07.871732: Epoch 82
2024-12-07 12:49:07.872529: Current learning rate: 0.00926
2024-12-07 12:56:07.694096: Validation loss did not improve from -0.60541. Patience: 11/50
2024-12-07 12:56:07.695112: train_loss -0.6826
2024-12-07 12:56:07.695954: val_loss -0.573
2024-12-07 12:56:07.696677: Pseudo dice [0.7542]
2024-12-07 12:56:07.697662: Epoch time: 419.83 s
2024-12-07 12:56:09.057179: 
2024-12-07 12:56:09.058435: Epoch 83
2024-12-07 12:56:09.059146: Current learning rate: 0.00925
2024-12-07 13:03:09.413938: Validation loss did not improve from -0.60541. Patience: 12/50
2024-12-07 13:03:09.415392: train_loss -0.6796
2024-12-07 13:03:09.416171: val_loss -0.5914
2024-12-07 13:03:09.416753: Pseudo dice [0.7666]
2024-12-07 13:03:09.417393: Epoch time: 420.36 s
2024-12-07 13:03:10.716933: 
2024-12-07 13:03:10.718183: Epoch 84
2024-12-07 13:03:10.718929: Current learning rate: 0.00924
2024-12-07 13:10:05.713824: Validation loss did not improve from -0.60541. Patience: 13/50
2024-12-07 13:10:05.714491: train_loss -0.6794
2024-12-07 13:10:05.715251: val_loss -0.5988
2024-12-07 13:10:05.715921: Pseudo dice [0.7657]
2024-12-07 13:10:05.716563: Epoch time: 415.0 s
2024-12-07 13:10:07.530934: 
2024-12-07 13:10:07.532340: Epoch 85
2024-12-07 13:10:07.533108: Current learning rate: 0.00923
2024-12-07 13:16:52.741612: Validation loss did not improve from -0.60541. Patience: 14/50
2024-12-07 13:16:52.742624: train_loss -0.6874
2024-12-07 13:16:52.743882: val_loss -0.5626
2024-12-07 13:16:52.744799: Pseudo dice [0.7631]
2024-12-07 13:16:52.745803: Epoch time: 405.21 s
2024-12-07 13:16:54.080459: 
2024-12-07 13:16:54.081757: Epoch 86
2024-12-07 13:16:54.083008: Current learning rate: 0.00922
2024-12-07 13:23:45.600459: Validation loss did not improve from -0.60541. Patience: 15/50
2024-12-07 13:23:45.601308: train_loss -0.6868
2024-12-07 13:23:45.602137: val_loss -0.5959
2024-12-07 13:23:45.602890: Pseudo dice [0.7742]
2024-12-07 13:23:45.603503: Epoch time: 411.52 s
2024-12-07 13:23:45.604100: Yayy! New best EMA pseudo Dice: 0.7602
2024-12-07 13:23:47.307862: 
2024-12-07 13:23:47.309247: Epoch 87
2024-12-07 13:23:47.309971: Current learning rate: 0.00921
2024-12-07 13:30:33.777701: Validation loss did not improve from -0.60541. Patience: 16/50
2024-12-07 13:30:33.778930: train_loss -0.6835
2024-12-07 13:30:33.780054: val_loss -0.5681
2024-12-07 13:30:33.780979: Pseudo dice [0.757]
2024-12-07 13:30:33.781698: Epoch time: 406.47 s
2024-12-07 13:30:35.211409: 
2024-12-07 13:30:35.213770: Epoch 88
2024-12-07 13:30:35.214797: Current learning rate: 0.0092
2024-12-07 13:37:32.799698: Validation loss improved from -0.60541 to -0.61040! Patience: 16/50
2024-12-07 13:37:32.800804: train_loss -0.6908
2024-12-07 13:37:32.801722: val_loss -0.6104
2024-12-07 13:37:32.802915: Pseudo dice [0.774]
2024-12-07 13:37:32.803710: Epoch time: 417.59 s
2024-12-07 13:37:32.804656: Yayy! New best EMA pseudo Dice: 0.7613
2024-12-07 13:37:34.529337: 
2024-12-07 13:37:34.531756: Epoch 89
2024-12-07 13:37:34.532681: Current learning rate: 0.0092
2024-12-07 13:44:29.472686: Validation loss did not improve from -0.61040. Patience: 1/50
2024-12-07 13:44:29.476406: train_loss -0.6836
2024-12-07 13:44:29.478840: val_loss -0.5659
2024-12-07 13:44:29.479798: Pseudo dice [0.7549]
2024-12-07 13:44:29.480877: Epoch time: 414.95 s
2024-12-07 13:44:31.343888: 
2024-12-07 13:44:31.345337: Epoch 90
2024-12-07 13:44:31.346407: Current learning rate: 0.00919
2024-12-07 13:51:51.324821: Validation loss did not improve from -0.61040. Patience: 2/50
2024-12-07 13:51:51.326971: train_loss -0.6892
2024-12-07 13:51:51.327890: val_loss -0.5835
2024-12-07 13:51:51.328788: Pseudo dice [0.762]
2024-12-07 13:51:51.329741: Epoch time: 439.98 s
2024-12-07 13:51:52.680029: 
2024-12-07 13:51:52.681663: Epoch 91
2024-12-07 13:51:52.682455: Current learning rate: 0.00918
2024-12-07 13:58:46.932877: Validation loss did not improve from -0.61040. Patience: 3/50
2024-12-07 13:58:46.933929: train_loss -0.7019
2024-12-07 13:58:46.934689: val_loss -0.6013
2024-12-07 13:58:46.935465: Pseudo dice [0.7702]
2024-12-07 13:58:46.936280: Epoch time: 414.26 s
2024-12-07 13:58:46.937268: Yayy! New best EMA pseudo Dice: 0.7617
2024-12-07 13:58:48.825649: 
2024-12-07 13:58:48.826642: Epoch 92
2024-12-07 13:58:48.827491: Current learning rate: 0.00917
2024-12-07 14:05:41.343668: Validation loss did not improve from -0.61040. Patience: 4/50
2024-12-07 14:05:41.345087: train_loss -0.6796
2024-12-07 14:05:41.346439: val_loss -0.5978
2024-12-07 14:05:41.347225: Pseudo dice [0.7721]
2024-12-07 14:05:41.348123: Epoch time: 412.52 s
2024-12-07 14:05:41.348943: Yayy! New best EMA pseudo Dice: 0.7628
2024-12-07 14:05:43.124911: 
2024-12-07 14:05:43.126302: Epoch 93
2024-12-07 14:05:43.127580: Current learning rate: 0.00916
2024-12-07 14:12:33.863803: Validation loss did not improve from -0.61040. Patience: 5/50
2024-12-07 14:12:33.864697: train_loss -0.6944
2024-12-07 14:12:33.865536: val_loss -0.587
2024-12-07 14:12:33.866217: Pseudo dice [0.7676]
2024-12-07 14:12:33.866980: Epoch time: 410.74 s
2024-12-07 14:12:33.867761: Yayy! New best EMA pseudo Dice: 0.7632
2024-12-07 14:12:35.565924: 
2024-12-07 14:12:35.567460: Epoch 94
2024-12-07 14:12:35.568236: Current learning rate: 0.00915
2024-12-07 14:19:16.244474: Validation loss did not improve from -0.61040. Patience: 6/50
2024-12-07 14:19:16.245807: train_loss -0.6961
2024-12-07 14:19:16.246852: val_loss -0.566
2024-12-07 14:19:16.247639: Pseudo dice [0.7523]
2024-12-07 14:19:16.248384: Epoch time: 400.68 s
2024-12-07 14:19:17.979978: 
2024-12-07 14:19:17.981476: Epoch 95
2024-12-07 14:19:17.982969: Current learning rate: 0.00914
2024-12-07 14:26:02.003195: Validation loss did not improve from -0.61040. Patience: 7/50
2024-12-07 14:26:02.004173: train_loss -0.6961
2024-12-07 14:26:02.008358: val_loss -0.5844
2024-12-07 14:26:02.009031: Pseudo dice [0.7613]
2024-12-07 14:26:02.009625: Epoch time: 404.03 s
2024-12-07 14:26:03.306670: 
2024-12-07 14:26:03.307771: Epoch 96
2024-12-07 14:26:03.308428: Current learning rate: 0.00913
2024-12-07 14:32:52.961215: Validation loss improved from -0.61040 to -0.61049! Patience: 7/50
2024-12-07 14:32:52.962240: train_loss -0.6949
2024-12-07 14:32:52.962970: val_loss -0.6105
2024-12-07 14:32:52.963985: Pseudo dice [0.777]
2024-12-07 14:32:52.964837: Epoch time: 409.66 s
2024-12-07 14:32:52.965517: Yayy! New best EMA pseudo Dice: 0.7635
2024-12-07 14:32:54.755032: 
2024-12-07 14:32:54.756410: Epoch 97
2024-12-07 14:32:54.757168: Current learning rate: 0.00912
2024-12-07 14:39:41.613674: Validation loss did not improve from -0.61049. Patience: 1/50
2024-12-07 14:39:41.614725: train_loss -0.693
2024-12-07 14:39:41.615713: val_loss -0.5713
2024-12-07 14:39:41.616793: Pseudo dice [0.7562]
2024-12-07 14:39:41.617739: Epoch time: 406.86 s
2024-12-07 14:39:42.946889: 
2024-12-07 14:39:42.948730: Epoch 98
2024-12-07 14:39:42.949796: Current learning rate: 0.00911
2024-12-07 14:46:36.942726: Validation loss improved from -0.61049 to -0.62653! Patience: 1/50
2024-12-07 14:46:36.944059: train_loss -0.6923
2024-12-07 14:46:36.945032: val_loss -0.6265
2024-12-07 14:46:36.945790: Pseudo dice [0.7816]
2024-12-07 14:46:36.946417: Epoch time: 414.0 s
2024-12-07 14:46:36.947312: Yayy! New best EMA pseudo Dice: 0.7647
2024-12-07 14:46:38.687598: 
2024-12-07 14:46:38.689000: Epoch 99
2024-12-07 14:46:38.690071: Current learning rate: 0.0091
2024-12-07 14:53:24.307149: Validation loss did not improve from -0.62653. Patience: 1/50
2024-12-07 14:53:24.321246: train_loss -0.7032
2024-12-07 14:53:24.323056: val_loss -0.5972
2024-12-07 14:53:24.323935: Pseudo dice [0.7639]
2024-12-07 14:53:24.325294: Epoch time: 405.63 s
2024-12-07 14:53:26.186645: 
2024-12-07 14:53:26.188579: Epoch 100
2024-12-07 14:53:26.189749: Current learning rate: 0.0091
2024-12-07 15:00:14.137346: Validation loss did not improve from -0.62653. Patience: 2/50
2024-12-07 15:00:14.228503: train_loss -0.7044
2024-12-07 15:00:14.229694: val_loss -0.6171
2024-12-07 15:00:14.230320: Pseudo dice [0.7842]
2024-12-07 15:00:14.231045: Epoch time: 407.95 s
2024-12-07 15:00:14.231765: Yayy! New best EMA pseudo Dice: 0.7666
2024-12-07 15:00:16.048026: 
2024-12-07 15:00:16.049298: Epoch 101
2024-12-07 15:00:16.050337: Current learning rate: 0.00909
2024-12-07 15:07:08.097628: Validation loss did not improve from -0.62653. Patience: 3/50
2024-12-07 15:07:08.098766: train_loss -0.7043
2024-12-07 15:07:08.099875: val_loss -0.5751
2024-12-07 15:07:08.100860: Pseudo dice [0.7494]
2024-12-07 15:07:08.101668: Epoch time: 412.05 s
2024-12-07 15:07:09.423064: 
2024-12-07 15:07:09.424411: Epoch 102
2024-12-07 15:07:09.425303: Current learning rate: 0.00908
2024-12-07 15:13:51.183390: Validation loss did not improve from -0.62653. Patience: 4/50
2024-12-07 15:13:51.184227: train_loss -0.7051
2024-12-07 15:13:51.185283: val_loss -0.581
2024-12-07 15:13:51.186103: Pseudo dice [0.7634]
2024-12-07 15:13:51.187154: Epoch time: 401.76 s
2024-12-07 15:13:52.890457: 
2024-12-07 15:13:52.891629: Epoch 103
2024-12-07 15:13:52.892442: Current learning rate: 0.00907
2024-12-07 15:20:34.600245: Validation loss did not improve from -0.62653. Patience: 5/50
2024-12-07 15:20:34.601196: train_loss -0.7067
2024-12-07 15:20:34.602177: val_loss -0.6036
2024-12-07 15:20:34.602990: Pseudo dice [0.7734]
2024-12-07 15:20:34.603749: Epoch time: 401.71 s
2024-12-07 15:20:35.927749: 
2024-12-07 15:20:35.929092: Epoch 104
2024-12-07 15:20:35.929903: Current learning rate: 0.00906
2024-12-07 15:27:11.638256: Validation loss did not improve from -0.62653. Patience: 6/50
2024-12-07 15:27:11.639259: train_loss -0.7029
2024-12-07 15:27:11.640014: val_loss -0.5952
2024-12-07 15:27:11.640634: Pseudo dice [0.7723]
2024-12-07 15:27:11.641235: Epoch time: 395.71 s
2024-12-07 15:27:13.453009: 
2024-12-07 15:27:13.454325: Epoch 105
2024-12-07 15:27:13.455276: Current learning rate: 0.00905
2024-12-07 15:33:58.606826: Validation loss did not improve from -0.62653. Patience: 7/50
2024-12-07 15:33:58.607717: train_loss -0.7055
2024-12-07 15:33:58.608465: val_loss -0.6106
2024-12-07 15:33:58.609135: Pseudo dice [0.7752]
2024-12-07 15:33:58.609940: Epoch time: 405.16 s
2024-12-07 15:33:58.610647: Yayy! New best EMA pseudo Dice: 0.7671
2024-12-07 15:34:00.366145: 
2024-12-07 15:34:00.367509: Epoch 106
2024-12-07 15:34:00.368316: Current learning rate: 0.00904
2024-12-07 15:40:48.700891: Validation loss did not improve from -0.62653. Patience: 8/50
2024-12-07 15:40:48.702009: train_loss -0.6989
2024-12-07 15:40:48.702828: val_loss -0.61
2024-12-07 15:40:48.703659: Pseudo dice [0.7779]
2024-12-07 15:40:48.704459: Epoch time: 408.34 s
2024-12-07 15:40:48.705215: Yayy! New best EMA pseudo Dice: 0.7682
2024-12-07 15:40:50.456519: 
2024-12-07 15:40:50.457823: Epoch 107
2024-12-07 15:40:50.458724: Current learning rate: 0.00903
2024-12-07 15:48:11.724702: Validation loss did not improve from -0.62653. Patience: 9/50
2024-12-07 15:48:11.725760: train_loss -0.7099
2024-12-07 15:48:11.726681: val_loss -0.5956
2024-12-07 15:48:11.727485: Pseudo dice [0.7719]
2024-12-07 15:48:11.728491: Epoch time: 441.27 s
2024-12-07 15:48:11.729470: Yayy! New best EMA pseudo Dice: 0.7686
2024-12-07 15:48:13.476290: 
2024-12-07 15:48:13.477695: Epoch 108
2024-12-07 15:48:13.478435: Current learning rate: 0.00902
2024-12-07 15:55:28.592577: Validation loss did not improve from -0.62653. Patience: 10/50
2024-12-07 15:55:28.597116: train_loss -0.7057
2024-12-07 15:55:28.599448: val_loss -0.5967
2024-12-07 15:55:28.600347: Pseudo dice [0.7691]
2024-12-07 15:55:28.601244: Epoch time: 435.12 s
2024-12-07 15:55:28.602600: Yayy! New best EMA pseudo Dice: 0.7686
2024-12-07 15:55:30.369258: 
2024-12-07 15:55:30.370502: Epoch 109
2024-12-07 15:55:30.371278: Current learning rate: 0.00901
2024-12-07 16:02:16.584831: Validation loss did not improve from -0.62653. Patience: 11/50
2024-12-07 16:02:16.586362: train_loss -0.6996
2024-12-07 16:02:16.587184: val_loss -0.622
2024-12-07 16:02:16.587899: Pseudo dice [0.7841]
2024-12-07 16:02:16.588578: Epoch time: 406.22 s
2024-12-07 16:02:17.029975: Yayy! New best EMA pseudo Dice: 0.7702
2024-12-07 16:02:18.781013: 
2024-12-07 16:02:18.782339: Epoch 110
2024-12-07 16:02:18.783138: Current learning rate: 0.009
2024-12-07 16:09:15.259237: Validation loss did not improve from -0.62653. Patience: 12/50
2024-12-07 16:09:15.260740: train_loss -0.7074
2024-12-07 16:09:15.261691: val_loss -0.5822
2024-12-07 16:09:15.262433: Pseudo dice [0.7549]
2024-12-07 16:09:15.263160: Epoch time: 416.48 s
2024-12-07 16:09:16.596008: 
2024-12-07 16:09:16.597345: Epoch 111
2024-12-07 16:09:16.598212: Current learning rate: 0.009
2024-12-07 16:16:00.953391: Validation loss did not improve from -0.62653. Patience: 13/50
2024-12-07 16:16:00.954211: train_loss -0.7044
2024-12-07 16:16:00.955385: val_loss -0.5731
2024-12-07 16:16:00.956261: Pseudo dice [0.7544]
2024-12-07 16:16:00.957066: Epoch time: 404.36 s
2024-12-07 16:16:02.295298: 
2024-12-07 16:16:02.296479: Epoch 112
2024-12-07 16:16:02.297317: Current learning rate: 0.00899
2024-12-07 16:22:48.278969: Validation loss did not improve from -0.62653. Patience: 14/50
2024-12-07 16:22:48.280223: train_loss -0.7081
2024-12-07 16:22:48.280967: val_loss -0.565
2024-12-07 16:22:48.281582: Pseudo dice [0.7569]
2024-12-07 16:22:48.282265: Epoch time: 405.99 s
2024-12-07 16:22:51.227196: 
2024-12-07 16:22:51.228536: Epoch 113
2024-12-07 16:22:51.229425: Current learning rate: 0.00898
2024-12-07 16:29:41.636683: Validation loss did not improve from -0.62653. Patience: 15/50
2024-12-07 16:29:41.637642: train_loss -0.7144
2024-12-07 16:29:41.639023: val_loss -0.6052
2024-12-07 16:29:41.640333: Pseudo dice [0.7727]
2024-12-07 16:29:41.641635: Epoch time: 410.41 s
2024-12-07 16:29:42.972798: 
2024-12-07 16:29:42.974241: Epoch 114
2024-12-07 16:29:42.975227: Current learning rate: 0.00897
2024-12-07 16:36:36.377059: Validation loss did not improve from -0.62653. Patience: 16/50
2024-12-07 16:36:36.377989: train_loss -0.7177
2024-12-07 16:36:36.378860: val_loss -0.592
2024-12-07 16:36:36.379711: Pseudo dice [0.765]
2024-12-07 16:36:36.381037: Epoch time: 413.41 s
2024-12-07 16:36:38.504480: 
2024-12-07 16:36:38.505868: Epoch 115
2024-12-07 16:36:38.506670: Current learning rate: 0.00896
2024-12-07 16:43:46.078174: Validation loss did not improve from -0.62653. Patience: 17/50
2024-12-07 16:43:46.079077: train_loss -0.7193
2024-12-07 16:43:46.079897: val_loss -0.5462
2024-12-07 16:43:46.080736: Pseudo dice [0.7279]
2024-12-07 16:43:46.081527: Epoch time: 427.58 s
2024-12-07 16:43:47.424415: 
2024-12-07 16:43:47.425722: Epoch 116
2024-12-07 16:43:47.426469: Current learning rate: 0.00895
2024-12-07 16:50:32.083144: Validation loss did not improve from -0.62653. Patience: 18/50
2024-12-07 16:50:32.084094: train_loss -0.7224
2024-12-07 16:50:32.084831: val_loss -0.5812
2024-12-07 16:50:32.085504: Pseudo dice [0.7582]
2024-12-07 16:50:32.086195: Epoch time: 404.66 s
2024-12-07 16:50:33.465439: 
2024-12-07 16:50:33.466765: Epoch 117
2024-12-07 16:50:33.467694: Current learning rate: 0.00894
2024-12-07 16:57:31.250926: Validation loss did not improve from -0.62653. Patience: 19/50
2024-12-07 16:57:31.252330: train_loss -0.7217
2024-12-07 16:57:31.254636: val_loss -0.5901
2024-12-07 16:57:31.255674: Pseudo dice [0.7631]
2024-12-07 16:57:31.256554: Epoch time: 417.79 s
2024-12-07 16:57:32.617049: 
2024-12-07 16:57:32.618257: Epoch 118
2024-12-07 16:57:32.618955: Current learning rate: 0.00893
2024-12-07 17:04:33.168784: Validation loss did not improve from -0.62653. Patience: 20/50
2024-12-07 17:04:33.172365: train_loss -0.724
2024-12-07 17:04:33.173609: val_loss -0.5715
2024-12-07 17:04:33.174567: Pseudo dice [0.7609]
2024-12-07 17:04:33.175564: Epoch time: 420.56 s
2024-12-07 17:04:34.588156: 
2024-12-07 17:04:34.589507: Epoch 119
2024-12-07 17:04:34.590331: Current learning rate: 0.00892
2024-12-07 17:11:22.314074: Validation loss did not improve from -0.62653. Patience: 21/50
2024-12-07 17:11:22.315599: train_loss -0.7219
2024-12-07 17:11:22.316545: val_loss -0.5825
2024-12-07 17:11:22.317373: Pseudo dice [0.7572]
2024-12-07 17:11:22.318130: Epoch time: 407.73 s
2024-12-07 17:11:24.118553: 
2024-12-07 17:11:24.119720: Epoch 120
2024-12-07 17:11:24.120895: Current learning rate: 0.00891
2024-12-07 17:18:21.615958: Validation loss did not improve from -0.62653. Patience: 22/50
2024-12-07 17:18:21.616818: train_loss -0.7181
2024-12-07 17:18:21.617687: val_loss -0.5862
2024-12-07 17:18:21.618318: Pseudo dice [0.7679]
2024-12-07 17:18:21.619286: Epoch time: 417.5 s
2024-12-07 17:18:22.973494: 
2024-12-07 17:18:22.974842: Epoch 121
2024-12-07 17:18:22.975754: Current learning rate: 0.0089
2024-12-07 17:25:10.199759: Validation loss did not improve from -0.62653. Patience: 23/50
2024-12-07 17:25:10.200754: train_loss -0.727
2024-12-07 17:25:10.201500: val_loss -0.5951
2024-12-07 17:25:10.202213: Pseudo dice [0.7664]
2024-12-07 17:25:10.203017: Epoch time: 407.23 s
2024-12-07 17:25:11.554872: 
2024-12-07 17:25:11.556078: Epoch 122
2024-12-07 17:25:11.556766: Current learning rate: 0.00889
2024-12-07 17:31:41.932510: Validation loss did not improve from -0.62653. Patience: 24/50
2024-12-07 17:31:41.933465: train_loss -0.7272
2024-12-07 17:31:41.934279: val_loss -0.5918
2024-12-07 17:31:41.935060: Pseudo dice [0.7607]
2024-12-07 17:31:41.936203: Epoch time: 390.38 s
2024-12-07 17:31:43.280522: 
2024-12-07 17:31:43.281682: Epoch 123
2024-12-07 17:31:43.282422: Current learning rate: 0.00889
2024-12-07 17:38:25.748856: Validation loss did not improve from -0.62653. Patience: 25/50
2024-12-07 17:38:25.749753: train_loss -0.7272
2024-12-07 17:38:25.750490: val_loss -0.601
2024-12-07 17:38:25.751279: Pseudo dice [0.7663]
2024-12-07 17:38:25.752023: Epoch time: 402.47 s
2024-12-07 17:38:27.671978: 
2024-12-07 17:38:27.673207: Epoch 124
2024-12-07 17:38:27.673934: Current learning rate: 0.00888
2024-12-07 17:45:15.763134: Validation loss did not improve from -0.62653. Patience: 26/50
2024-12-07 17:45:15.763906: train_loss -0.7293
2024-12-07 17:45:15.764661: val_loss -0.5928
2024-12-07 17:45:15.765312: Pseudo dice [0.7676]
2024-12-07 17:45:15.765986: Epoch time: 408.09 s
2024-12-07 17:45:17.473095: 
2024-12-07 17:45:17.474540: Epoch 125
2024-12-07 17:45:17.475377: Current learning rate: 0.00887
2024-12-07 17:52:14.597438: Validation loss did not improve from -0.62653. Patience: 27/50
2024-12-07 17:52:14.598371: train_loss -0.7266
2024-12-07 17:52:14.599068: val_loss -0.5625
2024-12-07 17:52:14.599788: Pseudo dice [0.7513]
2024-12-07 17:52:14.600436: Epoch time: 417.13 s
2024-12-07 17:52:15.972526: 
2024-12-07 17:52:15.973649: Epoch 126
2024-12-07 17:52:15.974383: Current learning rate: 0.00886
2024-12-07 17:59:10.234477: Validation loss did not improve from -0.62653. Patience: 28/50
2024-12-07 17:59:10.235473: train_loss -0.7278
2024-12-07 17:59:10.236457: val_loss -0.5933
2024-12-07 17:59:10.237406: Pseudo dice [0.7715]
2024-12-07 17:59:10.238113: Epoch time: 414.26 s
2024-12-07 17:59:11.610192: 
2024-12-07 17:59:11.611625: Epoch 127
2024-12-07 17:59:11.612398: Current learning rate: 0.00885
2024-12-07 18:06:04.292579: Validation loss did not improve from -0.62653. Patience: 29/50
2024-12-07 18:06:04.296086: train_loss -0.7263
2024-12-07 18:06:04.297603: val_loss -0.5854
2024-12-07 18:06:04.298286: Pseudo dice [0.759]
2024-12-07 18:06:04.299287: Epoch time: 412.69 s
2024-12-07 18:06:05.685777: 
2024-12-07 18:06:05.687361: Epoch 128
2024-12-07 18:06:05.688058: Current learning rate: 0.00884
2024-12-07 18:12:59.180885: Validation loss did not improve from -0.62653. Patience: 30/50
2024-12-07 18:12:59.182640: train_loss -0.7254
2024-12-07 18:12:59.183827: val_loss -0.575
2024-12-07 18:12:59.184455: Pseudo dice [0.7598]
2024-12-07 18:12:59.185330: Epoch time: 413.5 s
2024-12-07 18:13:00.540964: 
2024-12-07 18:13:00.542134: Epoch 129
2024-12-07 18:13:00.542778: Current learning rate: 0.00883
2024-12-07 18:19:51.501873: Validation loss did not improve from -0.62653. Patience: 31/50
2024-12-07 18:19:51.503484: train_loss -0.7274
2024-12-07 18:19:51.504585: val_loss -0.6018
2024-12-07 18:19:51.505426: Pseudo dice [0.7744]
2024-12-07 18:19:51.506189: Epoch time: 410.96 s
2024-12-07 18:19:53.474665: 
2024-12-07 18:19:53.475733: Epoch 130
2024-12-07 18:19:53.476515: Current learning rate: 0.00882
2024-12-07 18:26:59.284161: Validation loss did not improve from -0.62653. Patience: 32/50
2024-12-07 18:26:59.284866: train_loss -0.7257
2024-12-07 18:26:59.285754: val_loss -0.579
2024-12-07 18:26:59.286618: Pseudo dice [0.7612]
2024-12-07 18:26:59.287336: Epoch time: 425.81 s
2024-12-07 18:27:00.660344: 
2024-12-07 18:27:00.661471: Epoch 131
2024-12-07 18:27:00.662219: Current learning rate: 0.00881
2024-12-07 18:33:58.029794: Validation loss did not improve from -0.62653. Patience: 33/50
2024-12-07 18:33:58.031213: train_loss -0.7301
2024-12-07 18:33:58.032397: val_loss -0.5925
2024-12-07 18:33:58.033208: Pseudo dice [0.7629]
2024-12-07 18:33:58.034063: Epoch time: 417.37 s
2024-12-07 18:33:59.384866: 
2024-12-07 18:33:59.386380: Epoch 132
2024-12-07 18:33:59.387283: Current learning rate: 0.0088
2024-12-07 18:40:57.802994: Validation loss did not improve from -0.62653. Patience: 34/50
2024-12-07 18:40:57.803961: train_loss -0.7336
2024-12-07 18:40:57.804747: val_loss -0.5859
2024-12-07 18:40:57.805604: Pseudo dice [0.7657]
2024-12-07 18:40:57.806484: Epoch time: 418.42 s
2024-12-07 18:40:59.166376: 
2024-12-07 18:40:59.167459: Epoch 133
2024-12-07 18:40:59.168451: Current learning rate: 0.00879
2024-12-07 18:47:33.890948: Validation loss did not improve from -0.62653. Patience: 35/50
2024-12-07 18:47:33.891885: train_loss -0.7351
2024-12-07 18:47:33.892681: val_loss -0.6107
2024-12-07 18:47:33.893440: Pseudo dice [0.7824]
2024-12-07 18:47:33.894267: Epoch time: 394.73 s
2024-12-07 18:47:35.259540: 
2024-12-07 18:47:35.260815: Epoch 134
2024-12-07 18:47:35.261862: Current learning rate: 0.00879
2024-12-07 18:54:22.790776: Validation loss did not improve from -0.62653. Patience: 36/50
2024-12-07 18:54:22.791797: train_loss -0.723
2024-12-07 18:54:22.792522: val_loss -0.5896
2024-12-07 18:54:22.793196: Pseudo dice [0.763]
2024-12-07 18:54:22.793937: Epoch time: 407.53 s
2024-12-07 18:54:25.236708: 
2024-12-07 18:54:25.238136: Epoch 135
2024-12-07 18:54:25.238966: Current learning rate: 0.00878
2024-12-07 19:01:31.746598: Validation loss did not improve from -0.62653. Patience: 37/50
2024-12-07 19:01:31.747599: train_loss -0.7196
2024-12-07 19:01:31.748564: val_loss -0.6215
2024-12-07 19:01:31.749219: Pseudo dice [0.7854]
2024-12-07 19:01:31.749969: Epoch time: 426.51 s
2024-12-07 19:01:33.125865: 
2024-12-07 19:01:33.127255: Epoch 136
2024-12-07 19:01:33.128019: Current learning rate: 0.00877
2024-12-07 19:08:21.841561: Validation loss did not improve from -0.62653. Patience: 38/50
2024-12-07 19:08:21.843126: train_loss -0.7303
2024-12-07 19:08:21.844737: val_loss -0.5854
2024-12-07 19:08:21.845659: Pseudo dice [0.7622]
2024-12-07 19:08:21.846639: Epoch time: 408.72 s
2024-12-07 19:08:23.234279: 
2024-12-07 19:08:23.235228: Epoch 137
2024-12-07 19:08:23.236204: Current learning rate: 0.00876
2024-12-07 19:15:15.371590: Validation loss did not improve from -0.62653. Patience: 39/50
2024-12-07 19:15:15.374908: train_loss -0.7327
2024-12-07 19:15:15.376249: val_loss -0.6001
2024-12-07 19:15:15.376927: Pseudo dice [0.7785]
2024-12-07 19:15:15.377748: Epoch time: 412.14 s
2024-12-07 19:15:16.783037: 
2024-12-07 19:15:16.784165: Epoch 138
2024-12-07 19:15:16.784888: Current learning rate: 0.00875
2024-12-07 19:22:19.438794: Validation loss did not improve from -0.62653. Patience: 40/50
2024-12-07 19:22:19.439875: train_loss -0.7253
2024-12-07 19:22:19.440824: val_loss -0.621
2024-12-07 19:22:19.441716: Pseudo dice [0.7813]
2024-12-07 19:22:19.442697: Epoch time: 422.66 s
2024-12-07 19:22:20.822979: 
2024-12-07 19:22:20.824330: Epoch 139
2024-12-07 19:22:20.825728: Current learning rate: 0.00874
2024-12-07 19:29:16.449726: Validation loss did not improve from -0.62653. Patience: 41/50
2024-12-07 19:29:16.450710: train_loss -0.7309
2024-12-07 19:29:16.451385: val_loss -0.5736
2024-12-07 19:29:16.452044: Pseudo dice [0.7602]
2024-12-07 19:29:16.452779: Epoch time: 415.63 s
2024-12-07 19:29:18.390892: 
2024-12-07 19:29:18.391964: Epoch 140
2024-12-07 19:29:18.392577: Current learning rate: 0.00873
2024-12-07 19:36:09.531348: Validation loss did not improve from -0.62653. Patience: 42/50
2024-12-07 19:36:09.532449: train_loss -0.7329
2024-12-07 19:36:09.533180: val_loss -0.5929
2024-12-07 19:36:09.533833: Pseudo dice [0.7737]
2024-12-07 19:36:09.534589: Epoch time: 411.14 s
2024-12-07 19:36:10.904782: 
2024-12-07 19:36:10.906052: Epoch 141
2024-12-07 19:36:10.906896: Current learning rate: 0.00872
2024-12-07 19:43:00.484709: Validation loss did not improve from -0.62653. Patience: 43/50
2024-12-07 19:43:00.486016: train_loss -0.7342
2024-12-07 19:43:00.486953: val_loss -0.6036
2024-12-07 19:43:00.487568: Pseudo dice [0.7715]
2024-12-07 19:43:00.488245: Epoch time: 409.58 s
2024-12-07 19:43:01.887724: 
2024-12-07 19:43:01.889079: Epoch 142
2024-12-07 19:43:01.889881: Current learning rate: 0.00871
2024-12-07 19:49:51.675346: Validation loss did not improve from -0.62653. Patience: 44/50
2024-12-07 19:49:51.676321: train_loss -0.7373
2024-12-07 19:49:51.677026: val_loss -0.5823
2024-12-07 19:49:51.678023: Pseudo dice [0.7542]
2024-12-07 19:49:51.678830: Epoch time: 409.79 s
2024-12-07 19:49:53.081136: 
2024-12-07 19:49:53.082397: Epoch 143
2024-12-07 19:49:53.083180: Current learning rate: 0.0087
2024-12-07 19:56:37.127040: Validation loss did not improve from -0.62653. Patience: 45/50
2024-12-07 19:56:37.128009: train_loss -0.7405
2024-12-07 19:56:37.128818: val_loss -0.5898
2024-12-07 19:56:37.129599: Pseudo dice [0.7695]
2024-12-07 19:56:37.130334: Epoch time: 404.05 s
2024-12-07 19:56:38.525138: 
2024-12-07 19:56:38.526778: Epoch 144
2024-12-07 19:56:38.527971: Current learning rate: 0.00869
2024-12-07 20:03:34.107751: Validation loss did not improve from -0.62653. Patience: 46/50
2024-12-07 20:03:34.108786: train_loss -0.7345
2024-12-07 20:03:34.109675: val_loss -0.6072
2024-12-07 20:03:34.110639: Pseudo dice [0.7825]
2024-12-07 20:03:34.111785: Epoch time: 415.58 s
2024-12-07 20:03:35.855530: 
2024-12-07 20:03:35.857111: Epoch 145
2024-12-07 20:03:35.858062: Current learning rate: 0.00868
2024-12-07 20:10:22.375809: Validation loss did not improve from -0.62653. Patience: 47/50
2024-12-07 20:10:22.376756: train_loss -0.7343
2024-12-07 20:10:22.377482: val_loss -0.5992
2024-12-07 20:10:22.378139: Pseudo dice [0.7712]
2024-12-07 20:10:22.378871: Epoch time: 406.52 s
2024-12-07 20:10:24.876567: 
2024-12-07 20:10:24.877728: Epoch 146
2024-12-07 20:10:24.878489: Current learning rate: 0.00868
2024-12-07 20:17:04.458112: Validation loss improved from -0.62653 to -0.63142! Patience: 47/50
2024-12-07 20:17:04.461226: train_loss -0.7387
2024-12-07 20:17:04.463006: val_loss -0.6314
2024-12-07 20:17:04.463869: Pseudo dice [0.7924]
2024-12-07 20:17:04.464876: Epoch time: 399.59 s
2024-12-07 20:17:04.465708: Yayy! New best EMA pseudo Dice: 0.7718
2024-12-07 20:17:06.316108: 
2024-12-07 20:17:06.317293: Epoch 147
2024-12-07 20:17:06.318025: Current learning rate: 0.00867
2024-12-07 20:23:56.411194: Validation loss did not improve from -0.63142. Patience: 1/50
2024-12-07 20:23:56.412593: train_loss -0.7354
2024-12-07 20:23:56.413514: val_loss -0.5995
2024-12-07 20:23:56.414106: Pseudo dice [0.7799]
2024-12-07 20:23:56.414715: Epoch time: 410.1 s
2024-12-07 20:23:56.415338: Yayy! New best EMA pseudo Dice: 0.7726
2024-12-07 20:23:58.217625: 
2024-12-07 20:23:58.218604: Epoch 148
2024-12-07 20:23:58.219235: Current learning rate: 0.00866
2024-12-07 20:30:46.107259: Validation loss did not improve from -0.63142. Patience: 2/50
2024-12-07 20:30:46.108379: train_loss -0.7263
2024-12-07 20:30:46.109412: val_loss -0.5838
2024-12-07 20:30:46.110252: Pseudo dice [0.7655]
2024-12-07 20:30:46.111135: Epoch time: 407.89 s
2024-12-07 20:30:47.508517: 
2024-12-07 20:30:47.509581: Epoch 149
2024-12-07 20:30:47.510478: Current learning rate: 0.00865
2024-12-07 20:37:59.861102: Validation loss did not improve from -0.63142. Patience: 3/50
2024-12-07 20:37:59.861841: train_loss -0.7319
2024-12-07 20:37:59.862566: val_loss -0.611
2024-12-07 20:37:59.863446: Pseudo dice [0.7836]
2024-12-07 20:37:59.864220: Epoch time: 432.35 s
2024-12-07 20:38:00.268010: Yayy! New best EMA pseudo Dice: 0.7731
2024-12-07 20:38:02.033395: 
2024-12-07 20:38:02.034733: Epoch 150
2024-12-07 20:38:02.035409: Current learning rate: 0.00864
2024-12-07 20:44:52.826494: Validation loss did not improve from -0.63142. Patience: 4/50
2024-12-07 20:44:52.827720: train_loss -0.737
2024-12-07 20:44:52.828545: val_loss -0.5811
2024-12-07 20:44:52.829231: Pseudo dice [0.7597]
2024-12-07 20:44:52.829961: Epoch time: 410.8 s
2024-12-07 20:44:54.254914: 
2024-12-07 20:44:54.256125: Epoch 151
2024-12-07 20:44:54.256924: Current learning rate: 0.00863
2024-12-07 20:51:39.241221: Validation loss did not improve from -0.63142. Patience: 5/50
2024-12-07 20:51:39.242143: train_loss -0.7367
2024-12-07 20:51:39.242994: val_loss -0.5791
2024-12-07 20:51:39.243635: Pseudo dice [0.7583]
2024-12-07 20:51:39.244316: Epoch time: 404.99 s
2024-12-07 20:51:40.621824: 
2024-12-07 20:51:40.623485: Epoch 152
2024-12-07 20:51:40.624434: Current learning rate: 0.00862
2024-12-07 20:58:40.130418: Validation loss did not improve from -0.63142. Patience: 6/50
2024-12-07 20:58:40.131571: train_loss -0.7351
2024-12-07 20:58:40.132676: val_loss -0.5841
2024-12-07 20:58:40.133560: Pseudo dice [0.7649]
2024-12-07 20:58:40.134493: Epoch time: 419.51 s
2024-12-07 20:58:41.525744: 
2024-12-07 20:58:41.527006: Epoch 153
2024-12-07 20:58:41.527820: Current learning rate: 0.00861
2024-12-07 21:05:20.064353: Validation loss did not improve from -0.63142. Patience: 7/50
2024-12-07 21:05:20.065422: train_loss -0.7245
2024-12-07 21:05:20.066179: val_loss -0.5888
2024-12-07 21:05:20.066916: Pseudo dice [0.7679]
2024-12-07 21:05:20.067687: Epoch time: 398.54 s
2024-12-07 21:05:21.495744: 
2024-12-07 21:05:21.497046: Epoch 154
2024-12-07 21:05:21.497882: Current learning rate: 0.0086
2024-12-07 21:12:21.558731: Validation loss did not improve from -0.63142. Patience: 8/50
2024-12-07 21:12:21.559543: train_loss -0.7273
2024-12-07 21:12:21.560728: val_loss -0.6086
2024-12-07 21:12:21.561724: Pseudo dice [0.7811]
2024-12-07 21:12:21.562648: Epoch time: 420.07 s
2024-12-07 21:12:23.418602: 
2024-12-07 21:12:23.420169: Epoch 155
2024-12-07 21:12:23.421242: Current learning rate: 0.00859
2024-12-07 21:19:08.803642: Validation loss did not improve from -0.63142. Patience: 9/50
2024-12-07 21:19:08.806552: train_loss -0.7289
2024-12-07 21:19:08.808873: val_loss -0.5558
2024-12-07 21:19:08.809635: Pseudo dice [0.7394]
2024-12-07 21:19:08.810573: Epoch time: 405.39 s
2024-12-07 21:19:11.618384: 
2024-12-07 21:19:11.620018: Epoch 156
2024-12-07 21:19:11.620879: Current learning rate: 0.00858
2024-12-07 21:25:56.500288: Validation loss did not improve from -0.63142. Patience: 10/50
2024-12-07 21:25:56.501438: train_loss -0.7349
2024-12-07 21:25:56.502190: val_loss -0.5796
2024-12-07 21:25:56.502861: Pseudo dice [0.7663]
2024-12-07 21:25:56.503858: Epoch time: 404.88 s
2024-12-07 21:25:58.022695: 
2024-12-07 21:25:58.024109: Epoch 157
2024-12-07 21:25:58.024931: Current learning rate: 0.00858
2024-12-07 21:32:45.629058: Validation loss did not improve from -0.63142. Patience: 11/50
2024-12-07 21:32:45.630148: train_loss -0.7286
2024-12-07 21:32:45.631093: val_loss -0.6088
2024-12-07 21:32:45.632013: Pseudo dice [0.7721]
2024-12-07 21:32:45.632803: Epoch time: 407.61 s
2024-12-07 21:32:47.071083: 
2024-12-07 21:32:47.072597: Epoch 158
2024-12-07 21:32:47.073552: Current learning rate: 0.00857
2024-12-07 21:39:22.576308: Validation loss did not improve from -0.63142. Patience: 12/50
2024-12-07 21:39:22.577395: train_loss -0.7385
2024-12-07 21:39:22.578273: val_loss -0.5861
2024-12-07 21:39:22.578995: Pseudo dice [0.7564]
2024-12-07 21:39:22.579703: Epoch time: 395.51 s
2024-12-07 21:39:24.029086: 
2024-12-07 21:39:24.030761: Epoch 159
2024-12-07 21:39:24.031743: Current learning rate: 0.00856
2024-12-07 21:46:13.238613: Validation loss did not improve from -0.63142. Patience: 13/50
2024-12-07 21:46:13.239657: train_loss -0.7435
2024-12-07 21:46:13.240511: val_loss -0.5939
2024-12-07 21:46:13.241223: Pseudo dice [0.7717]
2024-12-07 21:46:13.242005: Epoch time: 409.21 s
2024-12-07 21:46:15.209661: 
2024-12-07 21:46:15.211055: Epoch 160
2024-12-07 21:46:15.211720: Current learning rate: 0.00855
2024-12-07 21:53:04.186725: Validation loss did not improve from -0.63142. Patience: 14/50
2024-12-07 21:53:04.188029: train_loss -0.7464
2024-12-07 21:53:04.189101: val_loss -0.5995
2024-12-07 21:53:04.189811: Pseudo dice [0.7664]
2024-12-07 21:53:04.190640: Epoch time: 408.98 s
2024-12-07 21:53:05.612877: 
2024-12-07 21:53:05.614242: Epoch 161
2024-12-07 21:53:05.614950: Current learning rate: 0.00854
2024-12-07 21:59:53.281947: Validation loss did not improve from -0.63142. Patience: 15/50
2024-12-07 21:59:53.282835: train_loss -0.7483
2024-12-07 21:59:53.283613: val_loss -0.5996
2024-12-07 21:59:53.284274: Pseudo dice [0.7619]
2024-12-07 21:59:53.284906: Epoch time: 407.67 s
2024-12-07 21:59:54.705906: 
2024-12-07 21:59:54.707311: Epoch 162
2024-12-07 21:59:54.708077: Current learning rate: 0.00853
2024-12-07 22:06:56.269461: Validation loss did not improve from -0.63142. Patience: 16/50
2024-12-07 22:06:56.270330: train_loss -0.7537
2024-12-07 22:06:56.271572: val_loss -0.6002
2024-12-07 22:06:56.272351: Pseudo dice [0.7755]
2024-12-07 22:06:56.273094: Epoch time: 421.57 s
2024-12-07 22:06:57.707094: 
2024-12-07 22:06:57.734358: Epoch 163
2024-12-07 22:06:57.735586: Current learning rate: 0.00852
2024-12-07 22:13:45.361942: Validation loss did not improve from -0.63142. Patience: 17/50
2024-12-07 22:13:45.363019: train_loss -0.7455
2024-12-07 22:13:45.363818: val_loss -0.5961
2024-12-07 22:13:45.364519: Pseudo dice [0.7702]
2024-12-07 22:13:45.365220: Epoch time: 407.66 s
2024-12-07 22:13:46.834432: 
2024-12-07 22:13:46.835864: Epoch 164
2024-12-07 22:13:46.836523: Current learning rate: 0.00851
2024-12-07 22:20:37.763366: Validation loss did not improve from -0.63142. Patience: 18/50
2024-12-07 22:20:37.764187: train_loss -0.7459
2024-12-07 22:20:37.764992: val_loss -0.5943
2024-12-07 22:20:37.765741: Pseudo dice [0.7763]
2024-12-07 22:20:37.766439: Epoch time: 410.93 s
2024-12-07 22:20:39.539396: 
2024-12-07 22:20:39.540617: Epoch 165
2024-12-07 22:20:39.541414: Current learning rate: 0.0085
2024-12-07 22:27:47.781766: Validation loss did not improve from -0.63142. Patience: 19/50
2024-12-07 22:27:47.785270: train_loss -0.7454
2024-12-07 22:27:47.786839: val_loss -0.6022
2024-12-07 22:27:47.787539: Pseudo dice [0.7733]
2024-12-07 22:27:47.788547: Epoch time: 428.25 s
2024-12-07 22:27:49.915463: 
2024-12-07 22:27:49.916350: Epoch 166
2024-12-07 22:27:49.917026: Current learning rate: 0.00849
2024-12-07 22:34:51.457414: Validation loss did not improve from -0.63142. Patience: 20/50
2024-12-07 22:34:51.458541: train_loss -0.7449
2024-12-07 22:34:51.459193: val_loss -0.6055
2024-12-07 22:34:51.459852: Pseudo dice [0.7754]
2024-12-07 22:34:51.460562: Epoch time: 421.54 s
2024-12-07 22:34:52.837499: 
2024-12-07 22:34:52.839235: Epoch 167
2024-12-07 22:34:52.840381: Current learning rate: 0.00848
2024-12-07 22:41:42.380499: Validation loss did not improve from -0.63142. Patience: 21/50
2024-12-07 22:41:42.381779: train_loss -0.745
2024-12-07 22:41:42.382703: val_loss -0.6081
2024-12-07 22:41:42.383379: Pseudo dice [0.7707]
2024-12-07 22:41:42.384163: Epoch time: 409.55 s
2024-12-07 22:41:43.783252: 
2024-12-07 22:41:43.784494: Epoch 168
2024-12-07 22:41:43.785285: Current learning rate: 0.00847
2024-12-07 22:48:48.017749: Validation loss did not improve from -0.63142. Patience: 22/50
2024-12-07 22:48:48.018975: train_loss -0.7473
2024-12-07 22:48:48.019841: val_loss -0.5913
2024-12-07 22:48:48.020478: Pseudo dice [0.7666]
2024-12-07 22:48:48.021109: Epoch time: 424.24 s
2024-12-07 22:48:49.421353: 
2024-12-07 22:48:49.422630: Epoch 169
2024-12-07 22:48:49.423566: Current learning rate: 0.00847
2024-12-07 22:55:21.339994: Validation loss did not improve from -0.63142. Patience: 23/50
2024-12-07 22:55:21.341288: train_loss -0.7461
2024-12-07 22:55:21.342145: val_loss -0.5933
2024-12-07 22:55:21.343002: Pseudo dice [0.7702]
2024-12-07 22:55:21.343840: Epoch time: 391.92 s
2024-12-07 22:55:23.321802: 
2024-12-07 22:55:23.323040: Epoch 170
2024-12-07 22:55:23.323835: Current learning rate: 0.00846
2024-12-07 23:02:08.122150: Validation loss did not improve from -0.63142. Patience: 24/50
2024-12-07 23:02:08.123031: train_loss -0.751
2024-12-07 23:02:08.123960: val_loss -0.6144
2024-12-07 23:02:08.124779: Pseudo dice [0.7769]
2024-12-07 23:02:08.125584: Epoch time: 404.8 s
2024-12-07 23:02:09.542280: 
2024-12-07 23:02:09.543603: Epoch 171
2024-12-07 23:02:09.544549: Current learning rate: 0.00845
2024-12-07 23:09:02.472450: Validation loss did not improve from -0.63142. Patience: 25/50
2024-12-07 23:09:02.473443: train_loss -0.7561
2024-12-07 23:09:02.474173: val_loss -0.5768
2024-12-07 23:09:02.474963: Pseudo dice [0.7639]
2024-12-07 23:09:02.475605: Epoch time: 412.93 s
2024-12-07 23:09:03.880795: 
2024-12-07 23:09:03.882178: Epoch 172
2024-12-07 23:09:03.883033: Current learning rate: 0.00844
2024-12-07 23:15:50.889380: Validation loss did not improve from -0.63142. Patience: 26/50
2024-12-07 23:15:50.890394: train_loss -0.757
2024-12-07 23:15:50.891233: val_loss -0.6056
2024-12-07 23:15:50.892084: Pseudo dice [0.7819]
2024-12-07 23:15:50.892884: Epoch time: 407.01 s
2024-12-07 23:15:52.350167: 
2024-12-07 23:15:52.351292: Epoch 173
2024-12-07 23:15:52.352020: Current learning rate: 0.00843
2024-12-07 23:22:40.231037: Validation loss did not improve from -0.63142. Patience: 27/50
2024-12-07 23:22:40.232017: train_loss -0.7579
2024-12-07 23:22:40.232911: val_loss -0.592
2024-12-07 23:22:40.233748: Pseudo dice [0.7735]
2024-12-07 23:22:40.234934: Epoch time: 407.88 s
2024-12-07 23:22:41.639316: 
2024-12-07 23:22:41.640827: Epoch 174
2024-12-07 23:22:41.641744: Current learning rate: 0.00842
2024-12-07 23:29:24.786891: Validation loss did not improve from -0.63142. Patience: 28/50
2024-12-07 23:29:24.789469: train_loss -0.7552
2024-12-07 23:29:24.791313: val_loss -0.5711
2024-12-07 23:29:24.792677: Pseudo dice [0.7652]
2024-12-07 23:29:24.793690: Epoch time: 403.15 s
2024-12-07 23:29:26.646320: 
2024-12-07 23:29:26.648495: Epoch 175
2024-12-07 23:29:26.650204: Current learning rate: 0.00841
2024-12-07 23:36:26.061614: Validation loss did not improve from -0.63142. Patience: 29/50
2024-12-07 23:36:26.062857: train_loss -0.7538
2024-12-07 23:36:26.064057: val_loss -0.5929
2024-12-07 23:36:26.064776: Pseudo dice [0.771]
2024-12-07 23:36:26.065543: Epoch time: 419.42 s
2024-12-07 23:36:28.157050: 
2024-12-07 23:36:28.158295: Epoch 176
2024-12-07 23:36:28.159165: Current learning rate: 0.0084
2024-12-07 23:43:23.795372: Validation loss did not improve from -0.63142. Patience: 30/50
2024-12-07 23:43:23.796287: train_loss -0.7532
2024-12-07 23:43:23.797020: val_loss -0.5905
2024-12-07 23:43:23.797741: Pseudo dice [0.7659]
2024-12-07 23:43:23.798410: Epoch time: 415.64 s
2024-12-07 23:43:25.221660: 
2024-12-07 23:43:25.223036: Epoch 177
2024-12-07 23:43:25.223920: Current learning rate: 0.00839
2024-12-07 23:50:12.289245: Validation loss did not improve from -0.63142. Patience: 31/50
2024-12-07 23:50:12.290278: train_loss -0.7549
2024-12-07 23:50:12.291317: val_loss -0.6
2024-12-07 23:50:12.292290: Pseudo dice [0.7748]
2024-12-07 23:50:12.293347: Epoch time: 407.07 s
2024-12-07 23:50:13.705285: 
2024-12-07 23:50:13.707143: Epoch 178
2024-12-07 23:50:13.708184: Current learning rate: 0.00838
2024-12-07 23:57:04.150068: Validation loss did not improve from -0.63142. Patience: 32/50
2024-12-07 23:57:04.151122: train_loss -0.7582
2024-12-07 23:57:04.151945: val_loss -0.5841
2024-12-07 23:57:04.152673: Pseudo dice [0.7597]
2024-12-07 23:57:04.153366: Epoch time: 410.45 s
2024-12-07 23:57:05.569745: 
2024-12-07 23:57:05.571021: Epoch 179
2024-12-07 23:57:05.571845: Current learning rate: 0.00837
2024-12-08 00:03:49.548735: Validation loss did not improve from -0.63142. Patience: 33/50
2024-12-08 00:03:49.550100: train_loss -0.759
2024-12-08 00:03:49.551081: val_loss -0.6022
2024-12-08 00:03:49.551973: Pseudo dice [0.7686]
2024-12-08 00:03:49.553123: Epoch time: 403.98 s
2024-12-08 00:03:51.467023: 
2024-12-08 00:03:51.468509: Epoch 180
2024-12-08 00:03:51.469549: Current learning rate: 0.00836
2024-12-08 00:10:41.874500: Validation loss did not improve from -0.63142. Patience: 34/50
2024-12-08 00:10:41.875526: train_loss -0.7604
2024-12-08 00:10:41.876356: val_loss -0.5633
2024-12-08 00:10:41.877122: Pseudo dice [0.755]
2024-12-08 00:10:41.877832: Epoch time: 410.41 s
2024-12-08 00:10:43.270789: 
2024-12-08 00:10:43.272104: Epoch 181
2024-12-08 00:10:43.272973: Current learning rate: 0.00836
2024-12-08 00:17:58.766229: Validation loss did not improve from -0.63142. Patience: 35/50
2024-12-08 00:17:58.767262: train_loss -0.7563
2024-12-08 00:17:58.768052: val_loss -0.617
2024-12-08 00:17:58.768887: Pseudo dice [0.7867]
2024-12-08 00:17:58.769495: Epoch time: 435.5 s
2024-12-08 00:18:00.168924: 
2024-12-08 00:18:00.170053: Epoch 182
2024-12-08 00:18:00.170811: Current learning rate: 0.00835
2024-12-08 00:25:07.877847: Validation loss did not improve from -0.63142. Patience: 36/50
2024-12-08 00:25:07.878785: train_loss -0.7639
2024-12-08 00:25:07.879591: val_loss -0.6141
2024-12-08 00:25:07.880391: Pseudo dice [0.776]
2024-12-08 00:25:07.881145: Epoch time: 427.71 s
2024-12-08 00:25:09.270398: 
2024-12-08 00:25:09.271836: Epoch 183
2024-12-08 00:25:09.272734: Current learning rate: 0.00834
2024-12-08 00:32:08.563430: Validation loss did not improve from -0.63142. Patience: 37/50
2024-12-08 00:32:08.564965: train_loss -0.7529
2024-12-08 00:32:08.567208: val_loss -0.6095
2024-12-08 00:32:08.568162: Pseudo dice [0.7781]
2024-12-08 00:32:08.569014: Epoch time: 419.3 s
2024-12-08 00:32:10.001933: 
2024-12-08 00:32:10.003367: Epoch 184
2024-12-08 00:32:10.004203: Current learning rate: 0.00833
2024-12-08 00:39:17.782061: Validation loss did not improve from -0.63142. Patience: 38/50
2024-12-08 00:39:17.821067: train_loss -0.757
2024-12-08 00:39:17.822526: val_loss -0.6002
2024-12-08 00:39:17.823386: Pseudo dice [0.7781]
2024-12-08 00:39:17.824452: Epoch time: 427.82 s
2024-12-08 00:39:19.762631: 
2024-12-08 00:39:19.764236: Epoch 185
2024-12-08 00:39:19.765155: Current learning rate: 0.00832
2024-12-08 00:46:44.385801: Validation loss did not improve from -0.63142. Patience: 39/50
2024-12-08 00:46:44.387446: train_loss -0.7572
2024-12-08 00:46:44.388460: val_loss -0.6242
2024-12-08 00:46:44.389261: Pseudo dice [0.7845]
2024-12-08 00:46:44.390004: Epoch time: 444.63 s
2024-12-08 00:46:44.390649: Yayy! New best EMA pseudo Dice: 0.7732
2024-12-08 00:46:46.218606: 
2024-12-08 00:46:46.219965: Epoch 186
2024-12-08 00:46:46.220782: Current learning rate: 0.00831
2024-12-08 00:54:05.455436: Validation loss did not improve from -0.63142. Patience: 40/50
2024-12-08 00:54:05.456505: train_loss -0.7561
2024-12-08 00:54:05.457406: val_loss -0.5945
2024-12-08 00:54:05.458558: Pseudo dice [0.7717]
2024-12-08 00:54:05.459300: Epoch time: 439.24 s
2024-12-08 00:54:08.681507: 
2024-12-08 00:54:08.683189: Epoch 187
2024-12-08 00:54:08.684213: Current learning rate: 0.0083
2024-12-08 01:01:25.369035: Validation loss did not improve from -0.63142. Patience: 41/50
2024-12-08 01:01:25.369984: train_loss -0.7538
2024-12-08 01:01:25.370941: val_loss -0.5963
2024-12-08 01:01:25.371722: Pseudo dice [0.7763]
2024-12-08 01:01:25.372488: Epoch time: 436.69 s
2024-12-08 01:01:25.373325: Yayy! New best EMA pseudo Dice: 0.7734
2024-12-08 01:01:27.215292: 
2024-12-08 01:01:27.216566: Epoch 188
2024-12-08 01:01:27.217608: Current learning rate: 0.00829
2024-12-08 01:08:51.493432: Validation loss did not improve from -0.63142. Patience: 42/50
2024-12-08 01:08:51.494776: train_loss -0.7547
2024-12-08 01:08:51.496079: val_loss -0.6156
2024-12-08 01:08:51.496928: Pseudo dice [0.7779]
2024-12-08 01:08:51.498028: Epoch time: 444.28 s
2024-12-08 01:08:51.498734: Yayy! New best EMA pseudo Dice: 0.7738
2024-12-08 01:08:53.397928: 
2024-12-08 01:08:53.399200: Epoch 189
2024-12-08 01:08:53.400194: Current learning rate: 0.00828
2024-12-08 01:16:19.019980: Validation loss did not improve from -0.63142. Patience: 43/50
2024-12-08 01:16:19.020774: train_loss -0.7521
2024-12-08 01:16:19.021561: val_loss -0.5737
2024-12-08 01:16:19.022197: Pseudo dice [0.7593]
2024-12-08 01:16:19.023221: Epoch time: 445.62 s
2024-12-08 01:16:20.829127: 
2024-12-08 01:16:20.830142: Epoch 190
2024-12-08 01:16:20.830917: Current learning rate: 0.00827
2024-12-08 01:23:44.376386: Validation loss did not improve from -0.63142. Patience: 44/50
2024-12-08 01:23:44.377430: train_loss -0.7521
2024-12-08 01:23:44.378376: val_loss -0.5999
2024-12-08 01:23:44.379051: Pseudo dice [0.7725]
2024-12-08 01:23:44.379886: Epoch time: 443.55 s
2024-12-08 01:23:45.760334: 
2024-12-08 01:23:45.761872: Epoch 191
2024-12-08 01:23:45.763199: Current learning rate: 0.00826
2024-12-08 01:31:17.226544: Validation loss did not improve from -0.63142. Patience: 45/50
2024-12-08 01:31:17.227529: train_loss -0.7401
2024-12-08 01:31:17.228731: val_loss -0.5502
2024-12-08 01:31:17.229532: Pseudo dice [0.7561]
2024-12-08 01:31:17.230545: Epoch time: 451.47 s
2024-12-08 01:31:18.723191: 
2024-12-08 01:31:18.724592: Epoch 192
2024-12-08 01:31:18.725440: Current learning rate: 0.00825
2024-12-08 01:38:48.043756: Validation loss did not improve from -0.63142. Patience: 46/50
2024-12-08 01:38:48.045643: train_loss -0.7444
2024-12-08 01:38:48.047556: val_loss -0.608
2024-12-08 01:38:48.048383: Pseudo dice [0.7784]
2024-12-08 01:38:48.049231: Epoch time: 449.32 s
2024-12-08 01:38:49.457149: 
2024-12-08 01:38:49.458742: Epoch 193
2024-12-08 01:38:49.459884: Current learning rate: 0.00824
2024-12-08 01:46:20.850407: Validation loss did not improve from -0.63142. Patience: 47/50
2024-12-08 01:46:20.882466: train_loss -0.7561
2024-12-08 01:46:20.884019: val_loss -0.5902
2024-12-08 01:46:20.884868: Pseudo dice [0.7694]
2024-12-08 01:46:20.885868: Epoch time: 451.4 s
2024-12-08 01:46:22.507822: 
2024-12-08 01:46:22.509288: Epoch 194
2024-12-08 01:46:22.510126: Current learning rate: 0.00824
2024-12-08 01:53:56.810720: Validation loss did not improve from -0.63142. Patience: 48/50
2024-12-08 01:53:56.811590: train_loss -0.7595
2024-12-08 01:53:56.812802: val_loss -0.5741
2024-12-08 01:53:56.813513: Pseudo dice [0.7679]
2024-12-08 01:53:56.814211: Epoch time: 454.31 s
2024-12-08 01:53:58.736016: 
2024-12-08 01:53:58.737478: Epoch 195
2024-12-08 01:53:58.738291: Current learning rate: 0.00823
2024-12-08 02:01:32.159836: Validation loss did not improve from -0.63142. Patience: 49/50
2024-12-08 02:01:32.161023: train_loss -0.7574
2024-12-08 02:01:32.162102: val_loss -0.5903
2024-12-08 02:01:32.162715: Pseudo dice [0.7657]
2024-12-08 02:01:32.163472: Epoch time: 453.43 s
2024-12-08 02:01:33.561517: 
2024-12-08 02:01:33.562800: Epoch 196
2024-12-08 02:01:33.563583: Current learning rate: 0.00822
2024-12-08 02:09:04.351032: Validation loss did not improve from -0.63142. Patience: 50/50
2024-12-08 02:09:04.352078: train_loss -0.7603
2024-12-08 02:09:04.352906: val_loss -0.6027
2024-12-08 02:09:04.353740: Pseudo dice [0.7788]
2024-12-08 02:09:04.354660: Epoch time: 450.79 s
2024-12-08 02:09:05.757111: Patience reached. Stopping training.
2024-12-08 02:09:06.193913: Training done.
2024-12-08 02:09:06.727266: Using splits from existing split file: /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset307_Sohee_Calcium_OCT_CrossValidation/splits_final.json
2024-12-08 02:09:06.751698: The split file contains 5 splits.
2024-12-08 02:09:06.753007: Desired fold for training: 2
2024-12-08 02:09:06.754424: This split has 6 training and 2 validation cases.
2024-12-08 02:09:06.755702: predicting 101-044
2024-12-08 02:09:06.817064: 101-044, shape torch.Size([1, 404, 498, 498]), rank 0
2024-12-08 02:12:21.472740: predicting 704-003
2024-12-08 02:12:21.487358: 704-003, shape torch.Size([1, 375, 498, 498]), rank 0
2024-12-08 02:15:03.204310: Validation complete
2024-12-08 02:15:03.205648: Mean Validation Dice:  0.7557070816989817
