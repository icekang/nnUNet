/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/bin/python
Starting training with CONFIG=3d_32x160x128_b10, DATASET_ID=310, TRAINER=nnUNetTrainerScaleAnalysis80
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/nnunetv2/training/nnUNetTrainer/nnUNetTrainer.py:169: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  self.grad_scaler = GradScaler() if self.device.type == 'cuda' else None

############################
INFO: You are using the old nnU-Net default plans. We have updated our recommendations. Please consider using those instead! Read more here: https://github.com/MIC-DKFZ/nnUNet/blob/master/documentation/resenc_presets.md
############################

Using device: cuda:0

#######################################################################
Please cite the following paper when using nnU-Net:
Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.
#######################################################################

2025-10-05 23:09:38.490840: do_dummy_2d_data_aug: True
2025-10-05 23:09:38.491185: Using splits from existing split file: /nfs/erelab001/shared/Computational_Group/Naravich/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset310_nnInteractive_Calcium_OCT_CrossValidation/splits_final_80.json
2025-10-05 23:09:38.491478: The split file contains 5 splits.
2025-10-05 23:09:38.491611: Desired fold for training: 3
2025-10-05 23:09:38.491747: This split has 6 training and 3 validation cases.
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/fft_conv_pytorch/fft_conv.py:139: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:306.)
  output = output[crop_slices].contiguous()
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/fft_conv_pytorch/fft_conv.py:139: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:306.)
  output = output[crop_slices].contiguous()
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/fft_conv_pytorch/fft_conv.py:139: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:306.)
  output = output[crop_slices].contiguous()
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/fft_conv_pytorch/fft_conv.py:139: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:306.)
  output = output[crop_slices].contiguous()
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/fft_conv_pytorch/fft_conv.py:139: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:306.)
  output = output[crop_slices].contiguous()
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/fft_conv_pytorch/fft_conv.py:139: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:306.)
  output = output[crop_slices].contiguous()
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/fft_conv_pytorch/fft_conv.py:139: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:306.)
  output = output[crop_slices].contiguous()
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/fft_conv_pytorch/fft_conv.py:139: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:306.)
  output = output[crop_slices].contiguous()
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/fft_conv_pytorch/fft_conv.py:139: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:306.)
  output = output[crop_slices].contiguous()
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/fft_conv_pytorch/fft_conv.py:139: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:306.)
  output = output[crop_slices].contiguous()
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/fft_conv_pytorch/fft_conv.py:139: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:306.)
  output = output[crop_slices].contiguous()
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/fft_conv_pytorch/fft_conv.py:139: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:306.)
  output = output[crop_slices].contiguous()
using pin_memory on device 0
using pin_memory on device 0
2025-10-05 23:09:42.674112: Using torch.compile...

This is the configuration used by this training:
Configuration name: 3d_32x160x128_b10
 {'data_identifier': 'nnUNetPlans_3d_fullres', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 10, 'patch_size': [32, 160, 128], 'median_image_size_in_voxels': [375.0, 498.0, 498.0], 'spacing': [1.0, 1.0, 1.0], 'normalization_schemes': ['ZScoreNormalization'], 'use_mask_for_norm': [False], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'architecture': {'network_class_name': 'dynamic_network_architectures.architectures.unet.PlainConvUNet', 'arch_kwargs': {'n_stages': 6, 'features_per_stage': [32, 64, 128, 256, 320, 320], 'conv_op': 'torch.nn.modules.conv.Conv3d', 'kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]], 'strides': [[1, 1, 1], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [1, 2, 2]], 'n_conv_per_stage': [2, 2, 2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2, 2, 2], 'conv_bias': True, 'norm_op': 'torch.nn.modules.instancenorm.InstanceNorm3d', 'norm_op_kwargs': {'eps': 1e-05, 'affine': True}, 'dropout_op': None, 'dropout_op_kwargs': None, 'nonlin': 'torch.nn.LeakyReLU', 'nonlin_kwargs': {'inplace': True}, 'deep_supervision': True}, '_kw_requires_import': ['conv_op', 'norm_op', 'dropout_op', 'nonlin']}, 'batch_dice': True, 'inherits_from': '3d_fullres'} 

These are the global plan.json settings:
 {'dataset_name': 'Dataset310_nnInteractive_Calcium_OCT_CrossValidation', 'plans_name': 'nnUNetPlans', 'original_median_spacing_after_transp': [1.0, 1.0, 1.0], 'original_median_shape_after_transp': [375, 498, 498], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 0.4977976381778717, 'mean': 0.1394134759902954, 'median': 0.09849607944488525, 'min': 0.0, 'percentile_00_5': 0.015305490233004093, 'percentile_99_5': 0.4977976381778717, 'std': 0.121165432035923}}} 

2025-10-05 23:09:44.883631: unpacking dataset...
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/torch/_inductor/lowering.py:7095: UserWarning: 
Online softmax is disabled on the fly since Inductor decides to
split the reduction. Cut an issue to PyTorch if this is an
important use case and you want to speed it up with online
softmax.

  warnings.warn(
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/torch/_inductor/lowering.py:7095: UserWarning: 
Online softmax is disabled on the fly since Inductor decides to
split the reduction. Cut an issue to PyTorch if this is an
important use case and you want to speed it up with online
softmax.

  warnings.warn(
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/torch/_inductor/lowering.py:7095: UserWarning: 
Online softmax is disabled on the fly since Inductor decides to
split the reduction. Cut an issue to PyTorch if this is an
important use case and you want to speed it up with online
softmax.

  warnings.warn(
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/torch/_inductor/lowering.py:7095: UserWarning: 
Online softmax is disabled on the fly since Inductor decides to
split the reduction. Cut an issue to PyTorch if this is an
important use case and you want to speed it up with online
softmax.

  warnings.warn(
2025-10-05 23:09:49.290483: unpacking done...
2025-10-05 23:09:49.292505: Unable to plot network architecture: nnUNet_compile is enabled!
2025-10-05 23:09:49.297492: 
2025-10-05 23:09:49.297684: Epoch 0
2025-10-05 23:09:49.297904: Current learning rate: 0.01
2025-10-05 23:11:10.722933: Validation loss improved from 1000.00000 to -0.15805! Patience: 0/50
2025-10-05 23:11:10.723752: train_loss -0.1665
2025-10-05 23:11:10.724011: val_loss -0.158
2025-10-05 23:11:10.724225: Pseudo dice [np.float32(0.5179)]
2025-10-05 23:11:10.724440: Epoch time: 81.43 s
2025-10-05 23:11:10.724677: Yayy! New best EMA pseudo Dice: 0.5178999900817871
2025-10-05 23:11:11.693432: 
2025-10-05 23:11:11.693862: Epoch 1
2025-10-05 23:11:11.694075: Current learning rate: 0.00994
2025-10-05 23:11:57.696303: Validation loss improved from -0.15805 to -0.19632! Patience: 0/50
2025-10-05 23:11:57.697130: train_loss -0.2738
2025-10-05 23:11:57.697579: val_loss -0.1963
2025-10-05 23:11:57.697878: Pseudo dice [np.float32(0.5315)]
2025-10-05 23:11:57.698173: Epoch time: 46.0 s
2025-10-05 23:11:57.698442: Yayy! New best EMA pseudo Dice: 0.5192000269889832
2025-10-05 23:11:58.806786: 
2025-10-05 23:11:58.807112: Epoch 2
2025-10-05 23:11:58.807333: Current learning rate: 0.00988
2025-10-05 23:12:44.936396: Validation loss improved from -0.19632 to -0.24105! Patience: 0/50
2025-10-05 23:12:44.937186: train_loss -0.3267
2025-10-05 23:12:44.937328: val_loss -0.2411
2025-10-05 23:12:44.937451: Pseudo dice [np.float32(0.5535)]
2025-10-05 23:12:44.937599: Epoch time: 46.13 s
2025-10-05 23:12:44.937724: Yayy! New best EMA pseudo Dice: 0.5227000117301941
2025-10-05 23:12:46.049512: 
2025-10-05 23:12:46.049758: Epoch 3
2025-10-05 23:12:46.049970: Current learning rate: 0.00982
2025-10-05 23:13:32.194834: Validation loss improved from -0.24105 to -0.31491! Patience: 0/50
2025-10-05 23:13:32.195345: train_loss -0.366
2025-10-05 23:13:32.195480: val_loss -0.3149
2025-10-05 23:13:32.195615: Pseudo dice [np.float32(0.608)]
2025-10-05 23:13:32.195755: Epoch time: 46.15 s
2025-10-05 23:13:32.195899: Yayy! New best EMA pseudo Dice: 0.5311999917030334
2025-10-05 23:13:33.317343: 
2025-10-05 23:13:33.317617: Epoch 4
2025-10-05 23:13:33.317835: Current learning rate: 0.00976
2025-10-05 23:14:19.448050: Validation loss did not improve from -0.31491. Patience: 1/50
2025-10-05 23:14:19.448700: train_loss -0.3583
2025-10-05 23:14:19.448874: val_loss -0.2714
2025-10-05 23:14:19.448997: Pseudo dice [np.float32(0.5946)]
2025-10-05 23:14:19.449187: Epoch time: 46.13 s
2025-10-05 23:14:19.856390: Yayy! New best EMA pseudo Dice: 0.5375000238418579
2025-10-05 23:14:20.973330: 
2025-10-05 23:14:20.973611: Epoch 5
2025-10-05 23:14:20.973896: Current learning rate: 0.0097
2025-10-05 23:15:07.232255: Validation loss improved from -0.31491 to -0.34521! Patience: 1/50
2025-10-05 23:15:07.232805: train_loss -0.4063
2025-10-05 23:15:07.233008: val_loss -0.3452
2025-10-05 23:15:07.233132: Pseudo dice [np.float32(0.6208)]
2025-10-05 23:15:07.233269: Epoch time: 46.26 s
2025-10-05 23:15:07.233388: Yayy! New best EMA pseudo Dice: 0.5458999872207642
2025-10-05 23:15:08.339309: 
2025-10-05 23:15:08.339673: Epoch 6
2025-10-05 23:15:08.339928: Current learning rate: 0.00964
2025-10-05 23:15:54.470519: Validation loss improved from -0.34521 to -0.34862! Patience: 0/50
2025-10-05 23:15:54.471132: train_loss -0.429
2025-10-05 23:15:54.471283: val_loss -0.3486
2025-10-05 23:15:54.471411: Pseudo dice [np.float32(0.6332)]
2025-10-05 23:15:54.471595: Epoch time: 46.13 s
2025-10-05 23:15:54.471727: Yayy! New best EMA pseudo Dice: 0.5546000003814697
2025-10-05 23:15:55.581415: 
2025-10-05 23:15:55.581798: Epoch 7
2025-10-05 23:15:55.582113: Current learning rate: 0.00958
2025-10-05 23:16:41.769773: Validation loss did not improve from -0.34862. Patience: 1/50
2025-10-05 23:16:41.770598: train_loss -0.4279
2025-10-05 23:16:41.770953: val_loss -0.3459
2025-10-05 23:16:41.771215: Pseudo dice [np.float32(0.6158)]
2025-10-05 23:16:41.771545: Epoch time: 46.19 s
2025-10-05 23:16:41.771891: Yayy! New best EMA pseudo Dice: 0.560699999332428
2025-10-05 23:16:42.905657: 
2025-10-05 23:16:42.905973: Epoch 8
2025-10-05 23:16:42.906208: Current learning rate: 0.00952
2025-10-05 23:17:29.152604: Validation loss improved from -0.34862 to -0.36141! Patience: 1/50
2025-10-05 23:17:29.153381: train_loss -0.4315
2025-10-05 23:17:29.153553: val_loss -0.3614
2025-10-05 23:17:29.153704: Pseudo dice [np.float32(0.6268)]
2025-10-05 23:17:29.153849: Epoch time: 46.25 s
2025-10-05 23:17:29.153978: Yayy! New best EMA pseudo Dice: 0.567300021648407
2025-10-05 23:17:30.267356: 
2025-10-05 23:17:30.267673: Epoch 9
2025-10-05 23:17:30.267895: Current learning rate: 0.00946
2025-10-05 23:18:16.440591: Validation loss did not improve from -0.36141. Patience: 1/50
2025-10-05 23:18:16.441108: train_loss -0.4413
2025-10-05 23:18:16.441272: val_loss -0.3395
2025-10-05 23:18:16.441401: Pseudo dice [np.float32(0.6245)]
2025-10-05 23:18:16.441526: Epoch time: 46.17 s
2025-10-05 23:18:16.900426: Yayy! New best EMA pseudo Dice: 0.5730000138282776
2025-10-05 23:18:18.001820: 
2025-10-05 23:18:18.002091: Epoch 10
2025-10-05 23:18:18.002275: Current learning rate: 0.0094
2025-10-05 23:19:04.137975: Validation loss did not improve from -0.36141. Patience: 2/50
2025-10-05 23:19:04.138752: train_loss -0.4717
2025-10-05 23:19:04.138936: val_loss -0.3465
2025-10-05 23:19:04.139067: Pseudo dice [np.float32(0.6227)]
2025-10-05 23:19:04.139202: Epoch time: 46.14 s
2025-10-05 23:19:04.139319: Yayy! New best EMA pseudo Dice: 0.578000009059906
2025-10-05 23:19:05.251225: 
2025-10-05 23:19:05.251578: Epoch 11
2025-10-05 23:19:05.251796: Current learning rate: 0.00934
2025-10-05 23:19:51.407722: Validation loss improved from -0.36141 to -0.38874! Patience: 2/50
2025-10-05 23:19:51.408222: train_loss -0.4682
2025-10-05 23:19:51.408374: val_loss -0.3887
2025-10-05 23:19:51.408487: Pseudo dice [np.float32(0.6459)]
2025-10-05 23:19:51.408630: Epoch time: 46.16 s
2025-10-05 23:19:51.408744: Yayy! New best EMA pseudo Dice: 0.5848000049591064
2025-10-05 23:19:52.526480: 
2025-10-05 23:19:52.526762: Epoch 12
2025-10-05 23:19:52.526958: Current learning rate: 0.00928
2025-10-05 23:20:38.669634: Validation loss did not improve from -0.38874. Patience: 1/50
2025-10-05 23:20:38.670253: train_loss -0.4742
2025-10-05 23:20:38.670391: val_loss -0.2731
2025-10-05 23:20:38.670498: Pseudo dice [np.float32(0.5782)]
2025-10-05 23:20:38.670647: Epoch time: 46.14 s
2025-10-05 23:20:39.838981: 
2025-10-05 23:20:39.839262: Epoch 13
2025-10-05 23:20:39.839453: Current learning rate: 0.00922
2025-10-05 23:21:25.998193: Validation loss did not improve from -0.38874. Patience: 2/50
2025-10-05 23:21:25.998717: train_loss -0.4782
2025-10-05 23:21:25.998959: val_loss -0.3574
2025-10-05 23:21:25.999163: Pseudo dice [np.float32(0.6172)]
2025-10-05 23:21:25.999353: Epoch time: 46.16 s
2025-10-05 23:21:25.999505: Yayy! New best EMA pseudo Dice: 0.5874999761581421
2025-10-05 23:21:27.164797: 
2025-10-05 23:21:27.165198: Epoch 14
2025-10-05 23:21:27.165440: Current learning rate: 0.00916
2025-10-05 23:22:13.335296: Validation loss did not improve from -0.38874. Patience: 3/50
2025-10-05 23:22:13.336172: train_loss -0.4901
2025-10-05 23:22:13.336457: val_loss -0.3833
2025-10-05 23:22:13.336653: Pseudo dice [np.float32(0.6523)]
2025-10-05 23:22:13.336865: Epoch time: 46.17 s
2025-10-05 23:22:13.794306: Yayy! New best EMA pseudo Dice: 0.5939000248908997
2025-10-05 23:22:14.887213: 
2025-10-05 23:22:14.887493: Epoch 15
2025-10-05 23:22:14.887717: Current learning rate: 0.0091
2025-10-05 23:23:01.062189: Validation loss did not improve from -0.38874. Patience: 4/50
2025-10-05 23:23:01.062673: train_loss -0.497
2025-10-05 23:23:01.062867: val_loss -0.3826
2025-10-05 23:23:01.063025: Pseudo dice [np.float32(0.6408)]
2025-10-05 23:23:01.063204: Epoch time: 46.18 s
2025-10-05 23:23:01.063351: Yayy! New best EMA pseudo Dice: 0.5985999703407288
2025-10-05 23:23:02.170286: 
2025-10-05 23:23:02.170660: Epoch 16
2025-10-05 23:23:02.170862: Current learning rate: 0.00903
2025-10-05 23:23:48.437941: Validation loss improved from -0.38874 to -0.41185! Patience: 4/50
2025-10-05 23:23:48.438830: train_loss -0.51
2025-10-05 23:23:48.439046: val_loss -0.4119
2025-10-05 23:23:48.439183: Pseudo dice [np.float32(0.6618)]
2025-10-05 23:23:48.439353: Epoch time: 46.27 s
2025-10-05 23:23:48.439478: Yayy! New best EMA pseudo Dice: 0.6049000024795532
2025-10-05 23:23:49.572687: 
2025-10-05 23:23:49.573082: Epoch 17
2025-10-05 23:23:49.573395: Current learning rate: 0.00897
2025-10-05 23:24:35.814054: Validation loss did not improve from -0.41185. Patience: 1/50
2025-10-05 23:24:35.814523: train_loss -0.5167
2025-10-05 23:24:35.814666: val_loss -0.3999
2025-10-05 23:24:35.814781: Pseudo dice [np.float32(0.6545)]
2025-10-05 23:24:35.814938: Epoch time: 46.24 s
2025-10-05 23:24:35.815069: Yayy! New best EMA pseudo Dice: 0.6098999977111816
2025-10-05 23:24:36.943625: 
2025-10-05 23:24:36.943906: Epoch 18
2025-10-05 23:24:36.944122: Current learning rate: 0.00891
2025-10-05 23:25:23.186429: Validation loss improved from -0.41185 to -0.42432! Patience: 1/50
2025-10-05 23:25:23.187219: train_loss -0.5135
2025-10-05 23:25:23.187379: val_loss -0.4243
2025-10-05 23:25:23.187572: Pseudo dice [np.float32(0.6706)]
2025-10-05 23:25:23.187784: Epoch time: 46.24 s
2025-10-05 23:25:23.187984: Yayy! New best EMA pseudo Dice: 0.6159999966621399
2025-10-05 23:25:24.311164: 
2025-10-05 23:25:24.311459: Epoch 19
2025-10-05 23:25:24.311673: Current learning rate: 0.00885
2025-10-05 23:26:10.521602: Validation loss did not improve from -0.42432. Patience: 1/50
2025-10-05 23:26:10.522049: train_loss -0.5362
2025-10-05 23:26:10.522194: val_loss -0.4175
2025-10-05 23:26:10.522331: Pseudo dice [np.float32(0.6605)]
2025-10-05 23:26:10.522467: Epoch time: 46.21 s
2025-10-05 23:26:10.986925: Yayy! New best EMA pseudo Dice: 0.6204000115394592
2025-10-05 23:26:12.091572: 
2025-10-05 23:26:12.091882: Epoch 20
2025-10-05 23:26:12.092095: Current learning rate: 0.00879
2025-10-05 23:26:58.317349: Validation loss did not improve from -0.42432. Patience: 2/50
2025-10-05 23:26:58.318164: train_loss -0.5295
2025-10-05 23:26:58.318331: val_loss -0.4155
2025-10-05 23:26:58.318442: Pseudo dice [np.float32(0.677)]
2025-10-05 23:26:58.318588: Epoch time: 46.23 s
2025-10-05 23:26:58.318697: Yayy! New best EMA pseudo Dice: 0.6261000037193298
2025-10-05 23:26:59.464765: 
2025-10-05 23:26:59.465099: Epoch 21
2025-10-05 23:26:59.465340: Current learning rate: 0.00873
2025-10-05 23:27:45.655399: Validation loss did not improve from -0.42432. Patience: 3/50
2025-10-05 23:27:45.655890: train_loss -0.5424
2025-10-05 23:27:45.656053: val_loss -0.4128
2025-10-05 23:27:45.656173: Pseudo dice [np.float32(0.6568)]
2025-10-05 23:27:45.656346: Epoch time: 46.19 s
2025-10-05 23:27:45.656471: Yayy! New best EMA pseudo Dice: 0.6291999816894531
2025-10-05 23:27:46.767097: 
2025-10-05 23:27:46.767371: Epoch 22
2025-10-05 23:27:46.767592: Current learning rate: 0.00867
2025-10-05 23:28:32.964298: Validation loss did not improve from -0.42432. Patience: 4/50
2025-10-05 23:28:32.964929: train_loss -0.535
2025-10-05 23:28:32.965101: val_loss -0.4192
2025-10-05 23:28:32.965232: Pseudo dice [np.float32(0.6586)]
2025-10-05 23:28:32.965374: Epoch time: 46.2 s
2025-10-05 23:28:32.965488: Yayy! New best EMA pseudo Dice: 0.632099986076355
2025-10-05 23:28:34.093675: 
2025-10-05 23:28:34.093987: Epoch 23
2025-10-05 23:28:34.094180: Current learning rate: 0.00861
2025-10-05 23:29:20.301490: Validation loss did not improve from -0.42432. Patience: 5/50
2025-10-05 23:29:20.301947: train_loss -0.5547
2025-10-05 23:29:20.302125: val_loss -0.4014
2025-10-05 23:29:20.302249: Pseudo dice [np.float32(0.6625)]
2025-10-05 23:29:20.302421: Epoch time: 46.21 s
2025-10-05 23:29:20.302547: Yayy! New best EMA pseudo Dice: 0.6351000070571899
2025-10-05 23:29:21.434319: 
2025-10-05 23:29:21.434670: Epoch 24
2025-10-05 23:29:21.434896: Current learning rate: 0.00855
2025-10-05 23:30:07.637594: Validation loss did not improve from -0.42432. Patience: 6/50
2025-10-05 23:30:07.638240: train_loss -0.5612
2025-10-05 23:30:07.638402: val_loss -0.4153
2025-10-05 23:30:07.638516: Pseudo dice [np.float32(0.6758)]
2025-10-05 23:30:07.638672: Epoch time: 46.2 s
2025-10-05 23:30:08.110337: Yayy! New best EMA pseudo Dice: 0.63919997215271
2025-10-05 23:30:09.222039: 
2025-10-05 23:30:09.222355: Epoch 25
2025-10-05 23:30:09.222565: Current learning rate: 0.00849
2025-10-05 23:30:55.410142: Validation loss improved from -0.42432 to -0.42788! Patience: 6/50
2025-10-05 23:30:55.410589: train_loss -0.5482
2025-10-05 23:30:55.410769: val_loss -0.4279
2025-10-05 23:30:55.410896: Pseudo dice [np.float32(0.6788)]
2025-10-05 23:30:55.411056: Epoch time: 46.19 s
2025-10-05 23:30:55.411170: Yayy! New best EMA pseudo Dice: 0.6431999802589417
2025-10-05 23:30:56.567728: 
2025-10-05 23:30:56.568058: Epoch 26
2025-10-05 23:30:56.568279: Current learning rate: 0.00843
2025-10-05 23:31:42.790113: Validation loss did not improve from -0.42788. Patience: 1/50
2025-10-05 23:31:42.790740: train_loss -0.5615
2025-10-05 23:31:42.790891: val_loss -0.4012
2025-10-05 23:31:42.791001: Pseudo dice [np.float32(0.6587)]
2025-10-05 23:31:42.791127: Epoch time: 46.22 s
2025-10-05 23:31:42.791265: Yayy! New best EMA pseudo Dice: 0.6446999907493591
2025-10-05 23:31:44.480222: 
2025-10-05 23:31:44.480619: Epoch 27
2025-10-05 23:31:44.480922: Current learning rate: 0.00836
2025-10-05 23:32:30.773184: Validation loss did not improve from -0.42788. Patience: 2/50
2025-10-05 23:32:30.773580: train_loss -0.5689
2025-10-05 23:32:30.773726: val_loss -0.4212
2025-10-05 23:32:30.773872: Pseudo dice [np.float32(0.6631)]
2025-10-05 23:32:30.774003: Epoch time: 46.29 s
2025-10-05 23:32:30.774112: Yayy! New best EMA pseudo Dice: 0.6464999914169312
2025-10-05 23:32:31.944562: 
2025-10-05 23:32:31.944893: Epoch 28
2025-10-05 23:32:31.945084: Current learning rate: 0.0083
2025-10-05 23:33:18.236380: Validation loss did not improve from -0.42788. Patience: 3/50
2025-10-05 23:33:18.237056: train_loss -0.5834
2025-10-05 23:33:18.237194: val_loss -0.4151
2025-10-05 23:33:18.237310: Pseudo dice [np.float32(0.6692)]
2025-10-05 23:33:18.237456: Epoch time: 46.29 s
2025-10-05 23:33:18.237567: Yayy! New best EMA pseudo Dice: 0.6488000154495239
2025-10-05 23:33:19.367630: 
2025-10-05 23:33:19.367966: Epoch 29
2025-10-05 23:33:19.368170: Current learning rate: 0.00824
2025-10-05 23:34:05.619455: Validation loss did not improve from -0.42788. Patience: 4/50
2025-10-05 23:34:05.620967: train_loss -0.5874
2025-10-05 23:34:05.621159: val_loss -0.3949
2025-10-05 23:34:05.621333: Pseudo dice [np.float32(0.655)]
2025-10-05 23:34:05.621521: Epoch time: 46.25 s
2025-10-05 23:34:06.051665: Yayy! New best EMA pseudo Dice: 0.649399995803833
2025-10-05 23:34:07.125988: 
2025-10-05 23:34:07.126362: Epoch 30
2025-10-05 23:34:07.126616: Current learning rate: 0.00818
2025-10-05 23:34:53.318705: Validation loss improved from -0.42788 to -0.44069! Patience: 4/50
2025-10-05 23:34:53.319449: train_loss -0.5887
2025-10-05 23:34:53.319650: val_loss -0.4407
2025-10-05 23:34:53.319797: Pseudo dice [np.float32(0.6899)]
2025-10-05 23:34:53.319978: Epoch time: 46.19 s
2025-10-05 23:34:53.320152: Yayy! New best EMA pseudo Dice: 0.6535000205039978
2025-10-05 23:34:54.437287: 
2025-10-05 23:34:54.437660: Epoch 31
2025-10-05 23:34:54.437967: Current learning rate: 0.00812
2025-10-05 23:35:40.679866: Validation loss improved from -0.44069 to -0.44333! Patience: 0/50
2025-10-05 23:35:40.680316: train_loss -0.5829
2025-10-05 23:35:40.680460: val_loss -0.4433
2025-10-05 23:35:40.680582: Pseudo dice [np.float32(0.6726)]
2025-10-05 23:35:40.680718: Epoch time: 46.24 s
2025-10-05 23:35:40.680869: Yayy! New best EMA pseudo Dice: 0.6553999781608582
2025-10-05 23:35:41.771262: 
2025-10-05 23:35:41.771519: Epoch 32
2025-10-05 23:35:41.771762: Current learning rate: 0.00806
2025-10-05 23:36:28.019843: Validation loss did not improve from -0.44333. Patience: 1/50
2025-10-05 23:36:28.020486: train_loss -0.5973
2025-10-05 23:36:28.020674: val_loss -0.3831
2025-10-05 23:36:28.020843: Pseudo dice [np.float32(0.6484)]
2025-10-05 23:36:28.020978: Epoch time: 46.25 s
2025-10-05 23:36:28.666015: 
2025-10-05 23:36:28.666384: Epoch 33
2025-10-05 23:36:28.666796: Current learning rate: 0.008
2025-10-05 23:37:14.965954: Validation loss did not improve from -0.44333. Patience: 2/50
2025-10-05 23:37:14.966560: train_loss -0.5936
2025-10-05 23:37:14.966855: val_loss -0.431
2025-10-05 23:37:14.967123: Pseudo dice [np.float32(0.6756)]
2025-10-05 23:37:14.967360: Epoch time: 46.3 s
2025-10-05 23:37:14.967600: Yayy! New best EMA pseudo Dice: 0.6567999720573425
2025-10-05 23:37:16.084269: 
2025-10-05 23:37:16.084576: Epoch 34
2025-10-05 23:37:16.084791: Current learning rate: 0.00793
2025-10-05 23:38:02.319285: Validation loss did not improve from -0.44333. Patience: 3/50
2025-10-05 23:38:02.319879: train_loss -0.6008
2025-10-05 23:38:02.320025: val_loss -0.4229
2025-10-05 23:38:02.320134: Pseudo dice [np.float32(0.6713)]
2025-10-05 23:38:02.320264: Epoch time: 46.24 s
2025-10-05 23:38:02.780868: Yayy! New best EMA pseudo Dice: 0.6582000255584717
2025-10-05 23:38:03.872056: 
2025-10-05 23:38:03.872393: Epoch 35
2025-10-05 23:38:03.872598: Current learning rate: 0.00787
2025-10-05 23:38:50.014495: Validation loss did not improve from -0.44333. Patience: 4/50
2025-10-05 23:38:50.015005: train_loss -0.591
2025-10-05 23:38:50.015241: val_loss -0.4198
2025-10-05 23:38:50.015465: Pseudo dice [np.float32(0.6716)]
2025-10-05 23:38:50.015665: Epoch time: 46.14 s
2025-10-05 23:38:50.015775: Yayy! New best EMA pseudo Dice: 0.659600019454956
2025-10-05 23:38:51.125697: 
2025-10-05 23:38:51.125997: Epoch 36
2025-10-05 23:38:51.126180: Current learning rate: 0.00781
2025-10-05 23:39:37.320972: Validation loss did not improve from -0.44333. Patience: 5/50
2025-10-05 23:39:37.321608: train_loss -0.6136
2025-10-05 23:39:37.321783: val_loss -0.4315
2025-10-05 23:39:37.321975: Pseudo dice [np.float32(0.6732)]
2025-10-05 23:39:37.322140: Epoch time: 46.2 s
2025-10-05 23:39:37.322244: Yayy! New best EMA pseudo Dice: 0.6608999967575073
2025-10-05 23:39:38.419366: 
2025-10-05 23:39:38.419685: Epoch 37
2025-10-05 23:39:38.419949: Current learning rate: 0.00775
2025-10-05 23:40:24.617074: Validation loss did not improve from -0.44333. Patience: 6/50
2025-10-05 23:40:24.617479: train_loss -0.6274
2025-10-05 23:40:24.617679: val_loss -0.4112
2025-10-05 23:40:24.617823: Pseudo dice [np.float32(0.6669)]
2025-10-05 23:40:24.617970: Epoch time: 46.2 s
2025-10-05 23:40:24.618096: Yayy! New best EMA pseudo Dice: 0.6614999771118164
2025-10-05 23:40:25.725022: 
2025-10-05 23:40:25.725352: Epoch 38
2025-10-05 23:40:25.725551: Current learning rate: 0.00769
2025-10-05 23:41:11.912014: Validation loss did not improve from -0.44333. Patience: 7/50
2025-10-05 23:41:11.913592: train_loss -0.62
2025-10-05 23:41:11.913875: val_loss -0.4295
2025-10-05 23:41:11.914027: Pseudo dice [np.float32(0.6665)]
2025-10-05 23:41:11.914254: Epoch time: 46.19 s
2025-10-05 23:41:11.914482: Yayy! New best EMA pseudo Dice: 0.6620000004768372
2025-10-05 23:41:13.021030: 
2025-10-05 23:41:13.021464: Epoch 39
2025-10-05 23:41:13.021758: Current learning rate: 0.00763
2025-10-05 23:41:59.218357: Validation loss did not improve from -0.44333. Patience: 8/50
2025-10-05 23:41:59.218824: train_loss -0.6255
2025-10-05 23:41:59.219030: val_loss -0.4188
2025-10-05 23:41:59.219180: Pseudo dice [np.float32(0.6829)]
2025-10-05 23:41:59.219412: Epoch time: 46.2 s
2025-10-05 23:41:59.680177: Yayy! New best EMA pseudo Dice: 0.6640999913215637
2025-10-05 23:42:00.791089: 
2025-10-05 23:42:00.791440: Epoch 40
2025-10-05 23:42:00.791690: Current learning rate: 0.00756
2025-10-05 23:42:46.940771: Validation loss did not improve from -0.44333. Patience: 9/50
2025-10-05 23:42:46.941581: train_loss -0.623
2025-10-05 23:42:46.941939: val_loss -0.3993
2025-10-05 23:42:46.942165: Pseudo dice [np.float32(0.6513)]
2025-10-05 23:42:46.942338: Epoch time: 46.15 s
2025-10-05 23:42:47.607187: 
2025-10-05 23:42:47.607564: Epoch 41
2025-10-05 23:42:47.607823: Current learning rate: 0.0075
2025-10-05 23:43:33.749468: Validation loss did not improve from -0.44333. Patience: 10/50
2025-10-05 23:43:33.750039: train_loss -0.6356
2025-10-05 23:43:33.750186: val_loss -0.4088
2025-10-05 23:43:33.750308: Pseudo dice [np.float32(0.673)]
2025-10-05 23:43:33.750457: Epoch time: 46.14 s
2025-10-05 23:43:34.387466: 
2025-10-05 23:43:34.387742: Epoch 42
2025-10-05 23:43:34.387938: Current learning rate: 0.00744
2025-10-05 23:44:20.622939: Validation loss did not improve from -0.44333. Patience: 11/50
2025-10-05 23:44:20.623417: train_loss -0.6319
2025-10-05 23:44:20.623540: val_loss -0.4409
2025-10-05 23:44:20.623660: Pseudo dice [np.float32(0.6749)]
2025-10-05 23:44:20.623779: Epoch time: 46.24 s
2025-10-05 23:44:20.623880: Yayy! New best EMA pseudo Dice: 0.6650000214576721
2025-10-05 23:44:22.245373: 
2025-10-05 23:44:22.245763: Epoch 43
2025-10-05 23:44:22.245975: Current learning rate: 0.00738
2025-10-05 23:45:08.452370: Validation loss did not improve from -0.44333. Patience: 12/50
2025-10-05 23:45:08.452894: train_loss -0.6412
2025-10-05 23:45:08.453050: val_loss -0.3987
2025-10-05 23:45:08.453159: Pseudo dice [np.float32(0.665)]
2025-10-05 23:45:08.453285: Epoch time: 46.21 s
2025-10-05 23:45:08.453417: Yayy! New best EMA pseudo Dice: 0.6650000214576721
2025-10-05 23:45:09.543615: 
2025-10-05 23:45:09.543873: Epoch 44
2025-10-05 23:45:09.544058: Current learning rate: 0.00732
2025-10-05 23:45:55.773181: Validation loss did not improve from -0.44333. Patience: 13/50
2025-10-05 23:45:55.773845: train_loss -0.6461
2025-10-05 23:45:55.773981: val_loss -0.4416
2025-10-05 23:45:55.774086: Pseudo dice [np.float32(0.694)]
2025-10-05 23:45:55.774205: Epoch time: 46.23 s
2025-10-05 23:45:56.230266: Yayy! New best EMA pseudo Dice: 0.667900025844574
2025-10-05 23:45:57.286639: 
2025-10-05 23:45:57.286985: Epoch 45
2025-10-05 23:45:57.287193: Current learning rate: 0.00725
2025-10-05 23:46:43.475366: Validation loss did not improve from -0.44333. Patience: 14/50
2025-10-05 23:46:43.475865: train_loss -0.6364
2025-10-05 23:46:43.476006: val_loss -0.3934
2025-10-05 23:46:43.476120: Pseudo dice [np.float32(0.6486)]
2025-10-05 23:46:43.476262: Epoch time: 46.19 s
2025-10-05 23:46:44.107124: 
2025-10-05 23:46:44.107534: Epoch 46
2025-10-05 23:46:44.107760: Current learning rate: 0.00719
2025-10-05 23:47:30.288800: Validation loss did not improve from -0.44333. Patience: 15/50
2025-10-05 23:47:30.289879: train_loss -0.633
2025-10-05 23:47:30.290235: val_loss -0.4134
2025-10-05 23:47:30.290572: Pseudo dice [np.float32(0.6644)]
2025-10-05 23:47:30.290893: Epoch time: 46.18 s
2025-10-05 23:47:30.946649: 
2025-10-05 23:47:30.946974: Epoch 47
2025-10-05 23:47:30.947217: Current learning rate: 0.00713
2025-10-05 23:48:17.143139: Validation loss did not improve from -0.44333. Patience: 16/50
2025-10-05 23:48:17.143655: train_loss -0.6445
2025-10-05 23:48:17.143878: val_loss -0.4339
2025-10-05 23:48:17.144024: Pseudo dice [np.float32(0.6809)]
2025-10-05 23:48:17.144164: Epoch time: 46.2 s
2025-10-05 23:48:17.793125: 
2025-10-05 23:48:17.793392: Epoch 48
2025-10-05 23:48:17.793599: Current learning rate: 0.00707
2025-10-05 23:49:04.034948: Validation loss did not improve from -0.44333. Patience: 17/50
2025-10-05 23:49:04.035605: train_loss -0.6502
2025-10-05 23:49:04.035789: val_loss -0.4228
2025-10-05 23:49:04.035959: Pseudo dice [np.float32(0.6702)]
2025-10-05 23:49:04.036167: Epoch time: 46.24 s
2025-10-05 23:49:04.678399: 
2025-10-05 23:49:04.678717: Epoch 49
2025-10-05 23:49:04.678978: Current learning rate: 0.007
2025-10-05 23:49:50.857202: Validation loss did not improve from -0.44333. Patience: 18/50
2025-10-05 23:49:50.857721: train_loss -0.6543
2025-10-05 23:49:50.857878: val_loss -0.4321
2025-10-05 23:49:50.858064: Pseudo dice [np.float32(0.6762)]
2025-10-05 23:49:50.858205: Epoch time: 46.18 s
2025-10-05 23:49:51.316291: Yayy! New best EMA pseudo Dice: 0.6685000061988831
2025-10-05 23:49:52.388261: 
2025-10-05 23:49:52.388696: Epoch 50
2025-10-05 23:49:52.388963: Current learning rate: 0.00694
2025-10-05 23:50:38.526414: Validation loss did not improve from -0.44333. Patience: 19/50
2025-10-05 23:50:38.527139: train_loss -0.6562
2025-10-05 23:50:38.527277: val_loss -0.4291
2025-10-05 23:50:38.527386: Pseudo dice [np.float32(0.6823)]
2025-10-05 23:50:38.527747: Epoch time: 46.14 s
2025-10-05 23:50:38.527917: Yayy! New best EMA pseudo Dice: 0.6697999835014343
2025-10-05 23:50:39.695323: 
2025-10-05 23:50:39.695728: Epoch 51
2025-10-05 23:50:39.695960: Current learning rate: 0.00688
2025-10-05 23:51:25.938634: Validation loss did not improve from -0.44333. Patience: 20/50
2025-10-05 23:51:25.939100: train_loss -0.6691
2025-10-05 23:51:25.939269: val_loss -0.3904
2025-10-05 23:51:25.939400: Pseudo dice [np.float32(0.6605)]
2025-10-05 23:51:25.939539: Epoch time: 46.24 s
2025-10-05 23:51:26.581131: 
2025-10-05 23:51:26.581476: Epoch 52
2025-10-05 23:51:26.581711: Current learning rate: 0.00682
2025-10-05 23:52:13.049252: Validation loss did not improve from -0.44333. Patience: 21/50
2025-10-05 23:52:13.049889: train_loss -0.6724
2025-10-05 23:52:13.050081: val_loss -0.3925
2025-10-05 23:52:13.050188: Pseudo dice [np.float32(0.6613)]
2025-10-05 23:52:13.050329: Epoch time: 46.47 s
2025-10-05 23:52:13.699540: 
2025-10-05 23:52:13.699906: Epoch 53
2025-10-05 23:52:13.700131: Current learning rate: 0.00675
2025-10-05 23:53:00.241514: Validation loss did not improve from -0.44333. Patience: 22/50
2025-10-05 23:53:00.241949: train_loss -0.6774
2025-10-05 23:53:00.242132: val_loss -0.3885
2025-10-05 23:53:00.242298: Pseudo dice [np.float32(0.6579)]
2025-10-05 23:53:00.242467: Epoch time: 46.54 s
2025-10-05 23:53:00.887408: 
2025-10-05 23:53:00.887810: Epoch 54
2025-10-05 23:53:00.888036: Current learning rate: 0.00669
2025-10-05 23:53:47.507718: Validation loss did not improve from -0.44333. Patience: 23/50
2025-10-05 23:53:47.508393: train_loss -0.6808
2025-10-05 23:53:47.508536: val_loss -0.4364
2025-10-05 23:53:47.508644: Pseudo dice [np.float32(0.6869)]
2025-10-05 23:53:47.508769: Epoch time: 46.62 s
2025-10-05 23:53:48.600030: 
2025-10-05 23:53:48.600369: Epoch 55
2025-10-05 23:53:48.600581: Current learning rate: 0.00663
2025-10-05 23:54:35.128727: Validation loss did not improve from -0.44333. Patience: 24/50
2025-10-05 23:54:35.129235: train_loss -0.6778
2025-10-05 23:54:35.129414: val_loss -0.4221
2025-10-05 23:54:35.129579: Pseudo dice [np.float32(0.6692)]
2025-10-05 23:54:35.129716: Epoch time: 46.53 s
2025-10-05 23:54:35.782200: 
2025-10-05 23:54:35.782568: Epoch 56
2025-10-05 23:54:35.782768: Current learning rate: 0.00657
2025-10-05 23:55:22.296497: Validation loss did not improve from -0.44333. Patience: 25/50
2025-10-05 23:55:22.297239: train_loss -0.6729
2025-10-05 23:55:22.297432: val_loss -0.3911
2025-10-05 23:55:22.297598: Pseudo dice [np.float32(0.658)]
2025-10-05 23:55:22.297799: Epoch time: 46.52 s
2025-10-05 23:55:22.940797: 
2025-10-05 23:55:22.941144: Epoch 57
2025-10-05 23:55:22.941368: Current learning rate: 0.0065
2025-10-05 23:56:09.484924: Validation loss did not improve from -0.44333. Patience: 26/50
2025-10-05 23:56:09.485443: train_loss -0.6833
2025-10-05 23:56:09.485625: val_loss -0.4403
2025-10-05 23:56:09.485750: Pseudo dice [np.float32(0.6866)]
2025-10-05 23:56:09.485905: Epoch time: 46.55 s
2025-10-05 23:56:09.486017: Yayy! New best EMA pseudo Dice: 0.6699000000953674
2025-10-05 23:56:10.596633: 
2025-10-05 23:56:10.597067: Epoch 58
2025-10-05 23:56:10.597359: Current learning rate: 0.00644
2025-10-05 23:56:56.927048: Validation loss did not improve from -0.44333. Patience: 27/50
2025-10-05 23:56:56.927707: train_loss -0.6811
2025-10-05 23:56:56.927893: val_loss -0.4318
2025-10-05 23:56:56.928031: Pseudo dice [np.float32(0.6771)]
2025-10-05 23:56:56.928179: Epoch time: 46.33 s
2025-10-05 23:56:56.928310: Yayy! New best EMA pseudo Dice: 0.6705999970436096
2025-10-05 23:56:58.600355: 
2025-10-05 23:56:58.600700: Epoch 59
2025-10-05 23:56:58.600898: Current learning rate: 0.00638
2025-10-05 23:57:44.949396: Validation loss improved from -0.44333 to -0.44801! Patience: 27/50
2025-10-05 23:57:44.949930: train_loss -0.68
2025-10-05 23:57:44.950197: val_loss -0.448
2025-10-05 23:57:44.950469: Pseudo dice [np.float32(0.6852)]
2025-10-05 23:57:44.950739: Epoch time: 46.35 s
2025-10-05 23:57:45.412472: Yayy! New best EMA pseudo Dice: 0.671999990940094
2025-10-05 23:57:46.522986: 
2025-10-05 23:57:46.523418: Epoch 60
2025-10-05 23:57:46.523778: Current learning rate: 0.00631
2025-10-05 23:58:32.785532: Validation loss did not improve from -0.44801. Patience: 1/50
2025-10-05 23:58:32.786386: train_loss -0.6901
2025-10-05 23:58:32.786654: val_loss -0.4086
2025-10-05 23:58:32.786876: Pseudo dice [np.float32(0.6737)]
2025-10-05 23:58:32.787101: Epoch time: 46.26 s
2025-10-05 23:58:32.787325: Yayy! New best EMA pseudo Dice: 0.6722000241279602
2025-10-05 23:58:33.906138: 
2025-10-05 23:58:33.906532: Epoch 61
2025-10-05 23:58:33.906787: Current learning rate: 0.00625
2025-10-05 23:59:20.206782: Validation loss did not improve from -0.44801. Patience: 2/50
2025-10-05 23:59:20.207241: train_loss -0.7019
2025-10-05 23:59:20.207422: val_loss -0.4285
2025-10-05 23:59:20.207564: Pseudo dice [np.float32(0.6748)]
2025-10-05 23:59:20.207716: Epoch time: 46.3 s
2025-10-05 23:59:20.207846: Yayy! New best EMA pseudo Dice: 0.6725000143051147
2025-10-05 23:59:21.333618: 
2025-10-05 23:59:21.333990: Epoch 62
2025-10-05 23:59:21.334205: Current learning rate: 0.00619
2025-10-06 00:00:07.663700: Validation loss did not improve from -0.44801. Patience: 3/50
2025-10-06 00:00:07.664348: train_loss -0.6992
2025-10-06 00:00:07.664501: val_loss -0.4142
2025-10-06 00:00:07.664612: Pseudo dice [np.float32(0.684)]
2025-10-06 00:00:07.664771: Epoch time: 46.33 s
2025-10-06 00:00:07.664881: Yayy! New best EMA pseudo Dice: 0.6736000180244446
2025-10-06 00:00:08.794188: 
2025-10-06 00:00:08.794512: Epoch 63
2025-10-06 00:00:08.794694: Current learning rate: 0.00612
2025-10-06 00:00:55.037200: Validation loss did not improve from -0.44801. Patience: 4/50
2025-10-06 00:00:55.037707: train_loss -0.6958
2025-10-06 00:00:55.037913: val_loss -0.4363
2025-10-06 00:00:55.038081: Pseudo dice [np.float32(0.6834)]
2025-10-06 00:00:55.038246: Epoch time: 46.24 s
2025-10-06 00:00:55.038419: Yayy! New best EMA pseudo Dice: 0.6746000051498413
2025-10-06 00:00:56.168860: 
2025-10-06 00:00:56.169215: Epoch 64
2025-10-06 00:00:56.169443: Current learning rate: 0.00606
2025-10-06 00:01:42.497301: Validation loss did not improve from -0.44801. Patience: 5/50
2025-10-06 00:01:42.497908: train_loss -0.6919
2025-10-06 00:01:42.498088: val_loss -0.4232
2025-10-06 00:01:42.498271: Pseudo dice [np.float32(0.6779)]
2025-10-06 00:01:42.498524: Epoch time: 46.33 s
2025-10-06 00:01:42.965836: Yayy! New best EMA pseudo Dice: 0.6748999953269958
2025-10-06 00:01:44.118988: 
2025-10-06 00:01:44.119371: Epoch 65
2025-10-06 00:01:44.119625: Current learning rate: 0.006
2025-10-06 00:02:30.390626: Validation loss did not improve from -0.44801. Patience: 6/50
2025-10-06 00:02:30.391204: train_loss -0.7027
2025-10-06 00:02:30.391412: val_loss -0.4013
2025-10-06 00:02:30.391594: Pseudo dice [np.float32(0.6662)]
2025-10-06 00:02:30.391861: Epoch time: 46.27 s
2025-10-06 00:02:31.042956: 
2025-10-06 00:02:31.043198: Epoch 66
2025-10-06 00:02:31.043386: Current learning rate: 0.00593
2025-10-06 00:03:17.359423: Validation loss did not improve from -0.44801. Patience: 7/50
2025-10-06 00:03:17.360086: train_loss -0.7027
2025-10-06 00:03:17.360350: val_loss -0.4257
2025-10-06 00:03:17.360498: Pseudo dice [np.float32(0.6786)]
2025-10-06 00:03:17.360652: Epoch time: 46.32 s
2025-10-06 00:03:18.018696: 
2025-10-06 00:03:18.018973: Epoch 67
2025-10-06 00:03:18.019219: Current learning rate: 0.00587
2025-10-06 00:04:04.319182: Validation loss did not improve from -0.44801. Patience: 8/50
2025-10-06 00:04:04.319705: train_loss -0.7039
2025-10-06 00:04:04.319924: val_loss -0.3998
2025-10-06 00:04:04.320174: Pseudo dice [np.float32(0.6708)]
2025-10-06 00:04:04.320416: Epoch time: 46.3 s
2025-10-06 00:04:04.972393: 
2025-10-06 00:04:04.972773: Epoch 68
2025-10-06 00:04:04.973068: Current learning rate: 0.00581
2025-10-06 00:04:51.376781: Validation loss did not improve from -0.44801. Patience: 9/50
2025-10-06 00:04:51.377540: train_loss -0.7167
2025-10-06 00:04:51.377719: val_loss -0.3831
2025-10-06 00:04:51.377883: Pseudo dice [np.float32(0.6734)]
2025-10-06 00:04:51.378263: Epoch time: 46.41 s
2025-10-06 00:04:52.037275: 
2025-10-06 00:04:52.037633: Epoch 69
2025-10-06 00:04:52.037862: Current learning rate: 0.00574
2025-10-06 00:05:38.418427: Validation loss did not improve from -0.44801. Patience: 10/50
2025-10-06 00:05:38.418935: train_loss -0.7152
2025-10-06 00:05:38.419093: val_loss -0.4475
2025-10-06 00:05:38.419226: Pseudo dice [np.float32(0.6852)]
2025-10-06 00:05:38.419358: Epoch time: 46.38 s
2025-10-06 00:05:38.864441: Yayy! New best EMA pseudo Dice: 0.6751999855041504
2025-10-06 00:05:39.962793: 
2025-10-06 00:05:39.963163: Epoch 70
2025-10-06 00:05:39.963398: Current learning rate: 0.00568
2025-10-06 00:06:26.336436: Validation loss did not improve from -0.44801. Patience: 11/50
2025-10-06 00:06:26.337058: train_loss -0.7152
2025-10-06 00:06:26.337226: val_loss -0.4372
2025-10-06 00:06:26.337348: Pseudo dice [np.float32(0.6828)]
2025-10-06 00:06:26.337473: Epoch time: 46.37 s
2025-10-06 00:06:26.337592: Yayy! New best EMA pseudo Dice: 0.6758999824523926
2025-10-06 00:06:27.456956: 
2025-10-06 00:06:27.457180: Epoch 71
2025-10-06 00:06:27.457361: Current learning rate: 0.00562
2025-10-06 00:07:13.744492: Validation loss did not improve from -0.44801. Patience: 12/50
2025-10-06 00:07:13.745027: train_loss -0.72
2025-10-06 00:07:13.745232: val_loss -0.4002
2025-10-06 00:07:13.745342: Pseudo dice [np.float32(0.6665)]
2025-10-06 00:07:13.745488: Epoch time: 46.29 s
2025-10-06 00:07:14.397719: 
2025-10-06 00:07:14.398032: Epoch 72
2025-10-06 00:07:14.398251: Current learning rate: 0.00555
2025-10-06 00:08:00.768976: Validation loss did not improve from -0.44801. Patience: 13/50
2025-10-06 00:08:00.769484: train_loss -0.7288
2025-10-06 00:08:00.769614: val_loss -0.3841
2025-10-06 00:08:00.769765: Pseudo dice [np.float32(0.6727)]
2025-10-06 00:08:00.769925: Epoch time: 46.37 s
2025-10-06 00:08:01.418766: 
2025-10-06 00:08:01.419124: Epoch 73
2025-10-06 00:08:01.419337: Current learning rate: 0.00549
2025-10-06 00:08:47.767701: Validation loss did not improve from -0.44801. Patience: 14/50
2025-10-06 00:08:47.768270: train_loss -0.7287
2025-10-06 00:08:47.768504: val_loss -0.3979
2025-10-06 00:08:47.768728: Pseudo dice [np.float32(0.6559)]
2025-10-06 00:08:47.768954: Epoch time: 46.35 s
2025-10-06 00:08:48.973328: 
2025-10-06 00:08:48.973766: Epoch 74
2025-10-06 00:08:48.974059: Current learning rate: 0.00542
2025-10-06 00:09:35.381409: Validation loss did not improve from -0.44801. Patience: 15/50
2025-10-06 00:09:35.382209: train_loss -0.731
2025-10-06 00:09:35.382403: val_loss -0.3936
2025-10-06 00:09:35.382548: Pseudo dice [np.float32(0.6723)]
2025-10-06 00:09:35.382688: Epoch time: 46.41 s
2025-10-06 00:09:36.504482: 
2025-10-06 00:09:36.504941: Epoch 75
2025-10-06 00:09:36.505279: Current learning rate: 0.00536
2025-10-06 00:10:22.954159: Validation loss did not improve from -0.44801. Patience: 16/50
2025-10-06 00:10:22.954642: train_loss -0.7241
2025-10-06 00:10:22.954831: val_loss -0.4346
2025-10-06 00:10:22.954991: Pseudo dice [np.float32(0.678)]
2025-10-06 00:10:22.955191: Epoch time: 46.45 s
2025-10-06 00:10:23.607991: 
2025-10-06 00:10:23.608297: Epoch 76
2025-10-06 00:10:23.608521: Current learning rate: 0.00529
2025-10-06 00:11:09.997135: Validation loss did not improve from -0.44801. Patience: 17/50
2025-10-06 00:11:09.997876: train_loss -0.7283
2025-10-06 00:11:09.998052: val_loss -0.4156
2025-10-06 00:11:09.998198: Pseudo dice [np.float32(0.6774)]
2025-10-06 00:11:09.998363: Epoch time: 46.39 s
2025-10-06 00:11:10.653822: 
2025-10-06 00:11:10.654135: Epoch 77
2025-10-06 00:11:10.654349: Current learning rate: 0.00523
2025-10-06 00:11:57.158522: Validation loss did not improve from -0.44801. Patience: 18/50
2025-10-06 00:11:57.159035: train_loss -0.7349
2025-10-06 00:11:57.159235: val_loss -0.4273
2025-10-06 00:11:57.159378: Pseudo dice [np.float32(0.6956)]
2025-10-06 00:11:57.159550: Epoch time: 46.51 s
2025-10-06 00:11:57.821468: 
2025-10-06 00:11:57.821769: Epoch 78
2025-10-06 00:11:57.822003: Current learning rate: 0.00517
2025-10-06 00:12:44.264081: Validation loss did not improve from -0.44801. Patience: 19/50
2025-10-06 00:12:44.264756: train_loss -0.7424
2025-10-06 00:12:44.264901: val_loss -0.4468
2025-10-06 00:12:44.265031: Pseudo dice [np.float32(0.7073)]
2025-10-06 00:12:44.265162: Epoch time: 46.44 s
2025-10-06 00:12:44.265297: Yayy! New best EMA pseudo Dice: 0.679099977016449
2025-10-06 00:12:45.428903: 
2025-10-06 00:12:45.429172: Epoch 79
2025-10-06 00:12:45.429438: Current learning rate: 0.0051
2025-10-06 00:13:31.959700: Validation loss did not improve from -0.44801. Patience: 20/50
2025-10-06 00:13:31.960291: train_loss -0.7339
2025-10-06 00:13:31.960582: val_loss -0.442
2025-10-06 00:13:31.960840: Pseudo dice [np.float32(0.6964)]
2025-10-06 00:13:31.961096: Epoch time: 46.53 s
2025-10-06 00:13:32.414089: Yayy! New best EMA pseudo Dice: 0.6808000206947327
2025-10-06 00:13:33.536727: 
2025-10-06 00:13:33.537095: Epoch 80
2025-10-06 00:13:33.537308: Current learning rate: 0.00504
2025-10-06 00:14:20.019695: Validation loss did not improve from -0.44801. Patience: 21/50
2025-10-06 00:14:20.020355: train_loss -0.7393
2025-10-06 00:14:20.020492: val_loss -0.429
2025-10-06 00:14:20.020604: Pseudo dice [np.float32(0.6911)]
2025-10-06 00:14:20.020726: Epoch time: 46.48 s
2025-10-06 00:14:20.020839: Yayy! New best EMA pseudo Dice: 0.6818000078201294
2025-10-06 00:14:21.152928: 
2025-10-06 00:14:21.153196: Epoch 81
2025-10-06 00:14:21.153465: Current learning rate: 0.00497
2025-10-06 00:15:07.540044: Validation loss did not improve from -0.44801. Patience: 22/50
2025-10-06 00:15:07.540542: train_loss -0.7411
2025-10-06 00:15:07.540680: val_loss -0.442
2025-10-06 00:15:07.540891: Pseudo dice [np.float32(0.6916)]
2025-10-06 00:15:07.541098: Epoch time: 46.39 s
2025-10-06 00:15:07.541302: Yayy! New best EMA pseudo Dice: 0.6827999949455261
2025-10-06 00:15:08.659291: 
2025-10-06 00:15:08.659641: Epoch 82
2025-10-06 00:15:08.659917: Current learning rate: 0.00491
2025-10-06 00:15:55.051059: Validation loss did not improve from -0.44801. Patience: 23/50
2025-10-06 00:15:55.051823: train_loss -0.7428
2025-10-06 00:15:55.051959: val_loss -0.4413
2025-10-06 00:15:55.052074: Pseudo dice [np.float32(0.6941)]
2025-10-06 00:15:55.052202: Epoch time: 46.39 s
2025-10-06 00:15:55.052349: Yayy! New best EMA pseudo Dice: 0.683899998664856
2025-10-06 00:15:56.178538: 
2025-10-06 00:15:56.178903: Epoch 83
2025-10-06 00:15:56.179208: Current learning rate: 0.00484
2025-10-06 00:16:42.647511: Validation loss did not improve from -0.44801. Patience: 24/50
2025-10-06 00:16:42.647961: train_loss -0.7473
2025-10-06 00:16:42.648174: val_loss -0.386
2025-10-06 00:16:42.648299: Pseudo dice [np.float32(0.6657)]
2025-10-06 00:16:42.648450: Epoch time: 46.47 s
2025-10-06 00:16:43.306138: 
2025-10-06 00:16:43.306405: Epoch 84
2025-10-06 00:16:43.306596: Current learning rate: 0.00478
2025-10-06 00:17:29.796517: Validation loss did not improve from -0.44801. Patience: 25/50
2025-10-06 00:17:29.797146: train_loss -0.7549
2025-10-06 00:17:29.797288: val_loss -0.3561
2025-10-06 00:17:29.797435: Pseudo dice [np.float32(0.6578)]
2025-10-06 00:17:29.797576: Epoch time: 46.49 s
2025-10-06 00:17:30.911797: 
2025-10-06 00:17:30.912181: Epoch 85
2025-10-06 00:17:30.912426: Current learning rate: 0.00471
2025-10-06 00:18:17.321808: Validation loss did not improve from -0.44801. Patience: 26/50
2025-10-06 00:18:17.322402: train_loss -0.7548
2025-10-06 00:18:17.322643: val_loss -0.4204
2025-10-06 00:18:17.322807: Pseudo dice [np.float32(0.6761)]
2025-10-06 00:18:17.323001: Epoch time: 46.41 s
2025-10-06 00:18:17.990130: 
2025-10-06 00:18:17.990425: Epoch 86
2025-10-06 00:18:17.990664: Current learning rate: 0.00465
2025-10-06 00:19:04.354341: Validation loss did not improve from -0.44801. Patience: 27/50
2025-10-06 00:19:04.355089: train_loss -0.759
2025-10-06 00:19:04.355269: val_loss -0.4121
2025-10-06 00:19:04.355448: Pseudo dice [np.float32(0.6765)]
2025-10-06 00:19:04.355629: Epoch time: 46.37 s
2025-10-06 00:19:05.006638: 
2025-10-06 00:19:05.006937: Epoch 87
2025-10-06 00:19:05.007309: Current learning rate: 0.00458
2025-10-06 00:19:51.291028: Validation loss did not improve from -0.44801. Patience: 28/50
2025-10-06 00:19:51.291585: train_loss -0.7646
2025-10-06 00:19:51.291730: val_loss -0.4137
2025-10-06 00:19:51.291855: Pseudo dice [np.float32(0.6837)]
2025-10-06 00:19:51.292012: Epoch time: 46.29 s
2025-10-06 00:19:51.952705: 
2025-10-06 00:19:51.952973: Epoch 88
2025-10-06 00:19:51.953172: Current learning rate: 0.00452
2025-10-06 00:20:38.324300: Validation loss did not improve from -0.44801. Patience: 29/50
2025-10-06 00:20:38.324989: train_loss -0.7547
2025-10-06 00:20:38.325178: val_loss -0.4265
2025-10-06 00:20:38.325299: Pseudo dice [np.float32(0.6812)]
2025-10-06 00:20:38.325457: Epoch time: 46.37 s
2025-10-06 00:20:38.995492: 
2025-10-06 00:20:38.995862: Epoch 89
2025-10-06 00:20:38.996135: Current learning rate: 0.00445
2025-10-06 00:21:25.501568: Validation loss did not improve from -0.44801. Patience: 30/50
2025-10-06 00:21:25.501993: train_loss -0.7547
2025-10-06 00:21:25.502201: val_loss -0.4035
2025-10-06 00:21:25.502350: Pseudo dice [np.float32(0.667)]
2025-10-06 00:21:25.502499: Epoch time: 46.51 s
2025-10-06 00:21:27.195391: 
2025-10-06 00:21:27.195787: Epoch 90
2025-10-06 00:21:27.195975: Current learning rate: 0.00438
2025-10-06 00:22:13.678684: Validation loss did not improve from -0.44801. Patience: 31/50
2025-10-06 00:22:13.679297: train_loss -0.7562
2025-10-06 00:22:13.679440: val_loss -0.3946
2025-10-06 00:22:13.679567: Pseudo dice [np.float32(0.6681)]
2025-10-06 00:22:13.679775: Epoch time: 46.48 s
2025-10-06 00:22:14.321565: 
2025-10-06 00:22:14.321883: Epoch 91
2025-10-06 00:22:14.322145: Current learning rate: 0.00432
2025-10-06 00:23:00.777281: Validation loss did not improve from -0.44801. Patience: 32/50
2025-10-06 00:23:00.777881: train_loss -0.7661
2025-10-06 00:23:00.778035: val_loss -0.4136
2025-10-06 00:23:00.778198: Pseudo dice [np.float32(0.6854)]
2025-10-06 00:23:00.778340: Epoch time: 46.46 s
2025-10-06 00:23:01.410070: 
2025-10-06 00:23:01.410408: Epoch 92
2025-10-06 00:23:01.410603: Current learning rate: 0.00425
2025-10-06 00:23:47.917065: Validation loss did not improve from -0.44801. Patience: 33/50
2025-10-06 00:23:47.917866: train_loss -0.7688
2025-10-06 00:23:47.918048: val_loss -0.4025
2025-10-06 00:23:47.918186: Pseudo dice [np.float32(0.684)]
2025-10-06 00:23:47.918339: Epoch time: 46.51 s
2025-10-06 00:23:48.555486: 
2025-10-06 00:23:48.555855: Epoch 93
2025-10-06 00:23:48.556108: Current learning rate: 0.00419
2025-10-06 00:24:35.105712: Validation loss did not improve from -0.44801. Patience: 34/50
2025-10-06 00:24:35.106306: train_loss -0.7676
2025-10-06 00:24:35.106547: val_loss -0.3781
2025-10-06 00:24:35.106713: Pseudo dice [np.float32(0.6755)]
2025-10-06 00:24:35.106904: Epoch time: 46.55 s
2025-10-06 00:24:35.795079: 
2025-10-06 00:24:35.795447: Epoch 94
2025-10-06 00:24:35.795640: Current learning rate: 0.00412
2025-10-06 00:25:22.332071: Validation loss did not improve from -0.44801. Patience: 35/50
2025-10-06 00:25:22.332668: train_loss -0.7712
2025-10-06 00:25:22.332848: val_loss -0.3937
2025-10-06 00:25:22.332997: Pseudo dice [np.float32(0.6872)]
2025-10-06 00:25:22.333195: Epoch time: 46.54 s
2025-10-06 00:25:23.444467: 
2025-10-06 00:25:23.444840: Epoch 95
2025-10-06 00:25:23.445063: Current learning rate: 0.00405
2025-10-06 00:26:09.856031: Validation loss did not improve from -0.44801. Patience: 36/50
2025-10-06 00:26:09.856577: train_loss -0.7703
2025-10-06 00:26:09.856762: val_loss -0.4162
2025-10-06 00:26:09.856912: Pseudo dice [np.float32(0.6863)]
2025-10-06 00:26:09.857051: Epoch time: 46.41 s
2025-10-06 00:26:10.512667: 
2025-10-06 00:26:10.512991: Epoch 96
2025-10-06 00:26:10.513207: Current learning rate: 0.00399
2025-10-06 00:26:56.853014: Validation loss did not improve from -0.44801. Patience: 37/50
2025-10-06 00:26:56.853743: train_loss -0.775
2025-10-06 00:26:56.853938: val_loss -0.4035
2025-10-06 00:26:56.854072: Pseudo dice [np.float32(0.6925)]
2025-10-06 00:26:56.854227: Epoch time: 46.34 s
2025-10-06 00:26:57.501981: 
2025-10-06 00:26:57.502385: Epoch 97
2025-10-06 00:26:57.502582: Current learning rate: 0.00392
2025-10-06 00:27:43.938814: Validation loss did not improve from -0.44801. Patience: 38/50
2025-10-06 00:27:43.939450: train_loss -0.7754
2025-10-06 00:27:43.939772: val_loss -0.4266
2025-10-06 00:27:43.939973: Pseudo dice [np.float32(0.6945)]
2025-10-06 00:27:43.940148: Epoch time: 46.44 s
2025-10-06 00:27:44.605433: 
2025-10-06 00:27:44.605684: Epoch 98
2025-10-06 00:27:44.605901: Current learning rate: 0.00385
2025-10-06 00:28:30.997950: Validation loss did not improve from -0.44801. Patience: 39/50
2025-10-06 00:28:30.998611: train_loss -0.7805
2025-10-06 00:28:30.998840: val_loss -0.389
2025-10-06 00:28:30.998958: Pseudo dice [np.float32(0.6677)]
2025-10-06 00:28:30.999100: Epoch time: 46.39 s
2025-10-06 00:28:31.652095: 
2025-10-06 00:28:31.652391: Epoch 99
2025-10-06 00:28:31.652584: Current learning rate: 0.00379
2025-10-06 00:29:18.000135: Validation loss did not improve from -0.44801. Patience: 40/50
2025-10-06 00:29:18.000906: train_loss -0.7811
2025-10-06 00:29:18.001288: val_loss -0.4377
2025-10-06 00:29:18.001532: Pseudo dice [np.float32(0.695)]
2025-10-06 00:29:18.001760: Epoch time: 46.35 s
2025-10-06 00:29:19.082430: 
2025-10-06 00:29:19.082721: Epoch 100
2025-10-06 00:29:19.082916: Current learning rate: 0.00372
2025-10-06 00:30:05.376398: Validation loss did not improve from -0.44801. Patience: 41/50
2025-10-06 00:30:05.377020: train_loss -0.7843
2025-10-06 00:30:05.377200: val_loss -0.3965
2025-10-06 00:30:05.377337: Pseudo dice [np.float32(0.6716)]
2025-10-06 00:30:05.377506: Epoch time: 46.3 s
2025-10-06 00:30:06.021585: 
2025-10-06 00:30:06.021896: Epoch 101
2025-10-06 00:30:06.022184: Current learning rate: 0.00365
2025-10-06 00:30:52.310599: Validation loss did not improve from -0.44801. Patience: 42/50
2025-10-06 00:30:52.311202: train_loss -0.7814
2025-10-06 00:30:52.311399: val_loss -0.3889
2025-10-06 00:30:52.311588: Pseudo dice [np.float32(0.677)]
2025-10-06 00:30:52.311749: Epoch time: 46.29 s
2025-10-06 00:30:52.958598: 
2025-10-06 00:30:52.959002: Epoch 102
2025-10-06 00:30:52.959208: Current learning rate: 0.00359
2025-10-06 00:31:39.273790: Validation loss did not improve from -0.44801. Patience: 43/50
2025-10-06 00:31:39.274466: train_loss -0.7895
2025-10-06 00:31:39.274614: val_loss -0.3884
2025-10-06 00:31:39.274729: Pseudo dice [np.float32(0.6806)]
2025-10-06 00:31:39.274890: Epoch time: 46.32 s
2025-10-06 00:31:39.930346: 
2025-10-06 00:31:39.930693: Epoch 103
2025-10-06 00:31:39.930892: Current learning rate: 0.00352
2025-10-06 00:32:26.305905: Validation loss did not improve from -0.44801. Patience: 44/50
2025-10-06 00:32:26.306418: train_loss -0.788
2025-10-06 00:32:26.306597: val_loss -0.4021
2025-10-06 00:32:26.306774: Pseudo dice [np.float32(0.6779)]
2025-10-06 00:32:26.306958: Epoch time: 46.38 s
2025-10-06 00:32:26.951758: 
2025-10-06 00:32:26.952107: Epoch 104
2025-10-06 00:32:26.952326: Current learning rate: 0.00345
2025-10-06 00:33:13.292418: Validation loss did not improve from -0.44801. Patience: 45/50
2025-10-06 00:33:13.293179: train_loss -0.793
2025-10-06 00:33:13.293325: val_loss -0.4227
2025-10-06 00:33:13.293443: Pseudo dice [np.float32(0.6795)]
2025-10-06 00:33:13.293674: Epoch time: 46.34 s
2025-10-06 00:33:14.419858: 
2025-10-06 00:33:14.420130: Epoch 105
2025-10-06 00:33:14.420353: Current learning rate: 0.00338
2025-10-06 00:34:00.730829: Validation loss did not improve from -0.44801. Patience: 46/50
2025-10-06 00:34:00.731410: train_loss -0.7871
2025-10-06 00:34:00.731715: val_loss -0.4027
2025-10-06 00:34:00.731878: Pseudo dice [np.float32(0.6799)]
2025-10-06 00:34:00.732037: Epoch time: 46.31 s
2025-10-06 00:34:01.944254: 
2025-10-06 00:34:01.944664: Epoch 106
2025-10-06 00:34:01.944899: Current learning rate: 0.00332
2025-10-06 00:34:48.221928: Validation loss did not improve from -0.44801. Patience: 47/50
2025-10-06 00:34:48.222522: train_loss -0.7866
2025-10-06 00:34:48.222696: val_loss -0.3937
2025-10-06 00:34:48.222873: Pseudo dice [np.float32(0.6855)]
2025-10-06 00:34:48.223215: Epoch time: 46.28 s
2025-10-06 00:34:48.884362: 
2025-10-06 00:34:48.884743: Epoch 107
2025-10-06 00:34:48.884963: Current learning rate: 0.00325
2025-10-06 00:35:35.178208: Validation loss did not improve from -0.44801. Patience: 48/50
2025-10-06 00:35:35.178682: train_loss -0.7927
2025-10-06 00:35:35.178886: val_loss -0.4005
2025-10-06 00:35:35.179006: Pseudo dice [np.float32(0.6723)]
2025-10-06 00:35:35.179149: Epoch time: 46.3 s
2025-10-06 00:35:35.833051: 
2025-10-06 00:35:35.833346: Epoch 108
2025-10-06 00:35:35.833592: Current learning rate: 0.00318
2025-10-06 00:36:22.165479: Validation loss did not improve from -0.44801. Patience: 49/50
2025-10-06 00:36:22.166129: train_loss -0.7912
2025-10-06 00:36:22.166276: val_loss -0.3846
2025-10-06 00:36:22.166386: Pseudo dice [np.float32(0.6709)]
2025-10-06 00:36:22.166539: Epoch time: 46.33 s
2025-10-06 00:36:22.818895: 
2025-10-06 00:36:22.819247: Epoch 109
2025-10-06 00:36:22.819454: Current learning rate: 0.00311
2025-10-06 00:37:09.127620: Validation loss did not improve from -0.44801. Patience: 50/50
2025-10-06 00:37:09.128048: train_loss -0.7908
2025-10-06 00:37:09.128227: val_loss -0.3899
2025-10-06 00:37:09.128393: Pseudo dice [np.float32(0.6744)]
2025-10-06 00:37:09.128546: Epoch time: 46.31 s
2025-10-06 00:37:10.253917: 
2025-10-06 00:37:10.254297: Epoch 110
2025-10-06 00:37:10.254513: Current learning rate: 0.00304
2025-10-06 00:37:56.550847: Validation loss did not improve from -0.44801. Patience: 51/50
2025-10-06 00:37:56.551480: train_loss -0.7972
2025-10-06 00:37:56.551668: val_loss -0.4162
2025-10-06 00:37:56.551794: Pseudo dice [np.float32(0.6794)]
2025-10-06 00:37:56.551931: Epoch time: 46.3 s
2025-10-06 00:37:57.204426: 
2025-10-06 00:37:57.204780: Epoch 111
2025-10-06 00:37:57.204986: Current learning rate: 0.00297
2025-10-06 00:38:43.507880: Validation loss did not improve from -0.44801. Patience: 52/50
2025-10-06 00:38:43.508395: train_loss -0.8033
2025-10-06 00:38:43.508646: val_loss -0.3626
2025-10-06 00:38:43.508854: Pseudo dice [np.float32(0.6684)]
2025-10-06 00:38:43.509092: Epoch time: 46.3 s
2025-10-06 00:38:44.160756: 
2025-10-06 00:38:44.161115: Epoch 112
2025-10-06 00:38:44.161350: Current learning rate: 0.00291
2025-10-06 00:39:30.455915: Validation loss did not improve from -0.44801. Patience: 53/50
2025-10-06 00:39:30.456815: train_loss -0.8014
2025-10-06 00:39:30.457067: val_loss -0.4051
2025-10-06 00:39:30.457307: Pseudo dice [np.float32(0.6865)]
2025-10-06 00:39:30.457548: Epoch time: 46.3 s
2025-10-06 00:39:31.118742: 
2025-10-06 00:39:31.119137: Epoch 113
2025-10-06 00:39:31.119366: Current learning rate: 0.00284
2025-10-06 00:40:17.458096: Validation loss did not improve from -0.44801. Patience: 54/50
2025-10-06 00:40:17.458579: train_loss -0.7983
2025-10-06 00:40:17.458834: val_loss -0.3861
2025-10-06 00:40:17.459046: Pseudo dice [np.float32(0.6754)]
2025-10-06 00:40:17.459287: Epoch time: 46.34 s
2025-10-06 00:40:18.108148: 
2025-10-06 00:40:18.108515: Epoch 114
2025-10-06 00:40:18.108782: Current learning rate: 0.00277
2025-10-06 00:41:04.439944: Validation loss did not improve from -0.44801. Patience: 55/50
2025-10-06 00:41:04.440580: train_loss -0.8035
2025-10-06 00:41:04.440739: val_loss -0.3725
2025-10-06 00:41:04.440896: Pseudo dice [np.float32(0.6729)]
2025-10-06 00:41:04.441101: Epoch time: 46.33 s
2025-10-06 00:41:05.556298: 
2025-10-06 00:41:05.556704: Epoch 115
2025-10-06 00:41:05.557007: Current learning rate: 0.0027
2025-10-06 00:41:51.852871: Validation loss did not improve from -0.44801. Patience: 56/50
2025-10-06 00:41:51.853388: train_loss -0.8018
2025-10-06 00:41:51.853564: val_loss -0.3875
2025-10-06 00:41:51.853784: Pseudo dice [np.float32(0.6765)]
2025-10-06 00:41:51.853959: Epoch time: 46.3 s
2025-10-06 00:41:52.501502: 
2025-10-06 00:41:52.501844: Epoch 116
2025-10-06 00:41:52.502085: Current learning rate: 0.00263
2025-10-06 00:42:38.839700: Validation loss did not improve from -0.44801. Patience: 57/50
2025-10-06 00:42:38.840274: train_loss -0.8091
2025-10-06 00:42:38.840430: val_loss -0.3799
2025-10-06 00:42:38.840601: Pseudo dice [np.float32(0.675)]
2025-10-06 00:42:38.840789: Epoch time: 46.34 s
2025-10-06 00:42:39.499208: 
2025-10-06 00:42:39.499483: Epoch 117
2025-10-06 00:42:39.499699: Current learning rate: 0.00256
2025-10-06 00:43:25.832567: Validation loss did not improve from -0.44801. Patience: 58/50
2025-10-06 00:43:25.833225: train_loss -0.8102
2025-10-06 00:43:25.833488: val_loss -0.3681
2025-10-06 00:43:25.833666: Pseudo dice [np.float32(0.6756)]
2025-10-06 00:43:25.833853: Epoch time: 46.33 s
2025-10-06 00:43:26.498798: 
2025-10-06 00:43:26.499044: Epoch 118
2025-10-06 00:43:26.499275: Current learning rate: 0.00249
2025-10-06 00:44:12.942621: Validation loss did not improve from -0.44801. Patience: 59/50
2025-10-06 00:44:12.943374: train_loss -0.8103
2025-10-06 00:44:12.943564: val_loss -0.402
2025-10-06 00:44:12.943716: Pseudo dice [np.float32(0.6955)]
2025-10-06 00:44:12.943876: Epoch time: 46.45 s
2025-10-06 00:44:13.597948: 
2025-10-06 00:44:13.598300: Epoch 119
2025-10-06 00:44:13.598512: Current learning rate: 0.00242
2025-10-06 00:44:59.993821: Validation loss did not improve from -0.44801. Patience: 60/50
2025-10-06 00:44:59.994364: train_loss -0.8077
2025-10-06 00:44:59.994515: val_loss -0.4005
2025-10-06 00:44:59.994670: Pseudo dice [np.float32(0.6868)]
2025-10-06 00:44:59.994844: Epoch time: 46.4 s
2025-10-06 00:45:01.121608: 
2025-10-06 00:45:01.121932: Epoch 120
2025-10-06 00:45:01.122160: Current learning rate: 0.00235
2025-10-06 00:45:47.401278: Validation loss did not improve from -0.44801. Patience: 61/50
2025-10-06 00:45:47.401977: train_loss -0.8105
2025-10-06 00:45:47.402139: val_loss -0.3891
2025-10-06 00:45:47.402270: Pseudo dice [np.float32(0.6707)]
2025-10-06 00:45:47.402435: Epoch time: 46.28 s
2025-10-06 00:45:48.603106: 
2025-10-06 00:45:48.603379: Epoch 121
2025-10-06 00:45:48.603573: Current learning rate: 0.00228
2025-10-06 00:46:34.851045: Validation loss did not improve from -0.44801. Patience: 62/50
2025-10-06 00:46:34.851497: train_loss -0.814
2025-10-06 00:46:34.851683: val_loss -0.3876
2025-10-06 00:46:34.851867: Pseudo dice [np.float32(0.6877)]
2025-10-06 00:46:34.852055: Epoch time: 46.25 s
2025-10-06 00:46:35.513726: 
2025-10-06 00:46:35.514080: Epoch 122
2025-10-06 00:46:35.514299: Current learning rate: 0.00221
2025-10-06 00:47:21.969215: Validation loss did not improve from -0.44801. Patience: 63/50
2025-10-06 00:47:21.969956: train_loss -0.8107
2025-10-06 00:47:21.970190: val_loss -0.3712
2025-10-06 00:47:21.970350: Pseudo dice [np.float32(0.664)]
2025-10-06 00:47:21.970553: Epoch time: 46.46 s
2025-10-06 00:47:22.636279: 
2025-10-06 00:47:22.636655: Epoch 123
2025-10-06 00:47:22.636852: Current learning rate: 0.00214
2025-10-06 00:48:09.014924: Validation loss did not improve from -0.44801. Patience: 64/50
2025-10-06 00:48:09.015468: train_loss -0.8187
2025-10-06 00:48:09.015715: val_loss -0.4019
2025-10-06 00:48:09.015953: Pseudo dice [np.float32(0.6838)]
2025-10-06 00:48:09.016186: Epoch time: 46.38 s
2025-10-06 00:48:09.689248: 
2025-10-06 00:48:09.689627: Epoch 124
2025-10-06 00:48:09.689870: Current learning rate: 0.00207
2025-10-06 00:48:56.262888: Validation loss did not improve from -0.44801. Patience: 65/50
2025-10-06 00:48:56.263608: train_loss -0.8188
2025-10-06 00:48:56.263894: val_loss -0.3932
2025-10-06 00:48:56.264126: Pseudo dice [np.float32(0.6799)]
2025-10-06 00:48:56.264403: Epoch time: 46.58 s
2025-10-06 00:48:57.402946: 
2025-10-06 00:48:57.403294: Epoch 125
2025-10-06 00:48:57.403555: Current learning rate: 0.00199
2025-10-06 00:49:43.833722: Validation loss did not improve from -0.44801. Patience: 66/50
2025-10-06 00:49:43.834260: train_loss -0.8164
2025-10-06 00:49:43.834456: val_loss -0.4238
2025-10-06 00:49:43.834596: Pseudo dice [np.float32(0.6979)]
2025-10-06 00:49:43.834752: Epoch time: 46.43 s
2025-10-06 00:49:44.501314: 
2025-10-06 00:49:44.501599: Epoch 126
2025-10-06 00:49:44.501794: Current learning rate: 0.00192
2025-10-06 00:50:30.897741: Validation loss did not improve from -0.44801. Patience: 67/50
2025-10-06 00:50:30.898519: train_loss -0.8228
2025-10-06 00:50:30.898694: val_loss -0.4204
2025-10-06 00:50:30.898823: Pseudo dice [np.float32(0.6861)]
2025-10-06 00:50:30.898979: Epoch time: 46.4 s
2025-10-06 00:50:31.561953: 
2025-10-06 00:50:31.562253: Epoch 127
2025-10-06 00:50:31.562467: Current learning rate: 0.00185
2025-10-06 00:51:17.933781: Validation loss did not improve from -0.44801. Patience: 68/50
2025-10-06 00:51:17.934249: train_loss -0.8216
2025-10-06 00:51:17.934450: val_loss -0.3837
2025-10-06 00:51:17.934613: Pseudo dice [np.float32(0.674)]
2025-10-06 00:51:17.934787: Epoch time: 46.37 s
2025-10-06 00:51:18.594852: 
2025-10-06 00:51:18.595164: Epoch 128
2025-10-06 00:51:18.595350: Current learning rate: 0.00178
2025-10-06 00:52:04.940760: Validation loss did not improve from -0.44801. Patience: 69/50
2025-10-06 00:52:04.941420: train_loss -0.8229
2025-10-06 00:52:04.941585: val_loss -0.375
2025-10-06 00:52:04.941705: Pseudo dice [np.float32(0.6811)]
2025-10-06 00:52:04.941868: Epoch time: 46.35 s
2025-10-06 00:52:05.590627: 
2025-10-06 00:52:05.590981: Epoch 129
2025-10-06 00:52:05.591190: Current learning rate: 0.0017
2025-10-06 00:52:51.952329: Validation loss did not improve from -0.44801. Patience: 70/50
2025-10-06 00:52:51.952761: train_loss -0.8283
2025-10-06 00:52:51.952966: val_loss -0.3757
2025-10-06 00:52:51.953161: Pseudo dice [np.float32(0.6691)]
2025-10-06 00:52:51.953331: Epoch time: 46.36 s
2025-10-06 00:52:53.078378: 
2025-10-06 00:52:53.078680: Epoch 130
2025-10-06 00:52:53.078876: Current learning rate: 0.00163
2025-10-06 00:53:39.433103: Validation loss did not improve from -0.44801. Patience: 71/50
2025-10-06 00:53:39.433728: train_loss -0.828
2025-10-06 00:53:39.433913: val_loss -0.3863
2025-10-06 00:53:39.434060: Pseudo dice [np.float32(0.6795)]
2025-10-06 00:53:39.434239: Epoch time: 46.36 s
2025-10-06 00:53:40.089509: 
2025-10-06 00:53:40.089823: Epoch 131
2025-10-06 00:53:40.090016: Current learning rate: 0.00156
2025-10-06 00:54:26.421040: Validation loss did not improve from -0.44801. Patience: 72/50
2025-10-06 00:54:26.421556: train_loss -0.8271
2025-10-06 00:54:26.421716: val_loss -0.3484
2025-10-06 00:54:26.421839: Pseudo dice [np.float32(0.6729)]
2025-10-06 00:54:26.421991: Epoch time: 46.33 s
2025-10-06 00:54:27.071400: 
2025-10-06 00:54:27.071768: Epoch 132
2025-10-06 00:54:27.072015: Current learning rate: 0.00148
2025-10-06 00:55:13.465826: Validation loss did not improve from -0.44801. Patience: 73/50
2025-10-06 00:55:13.466528: train_loss -0.8257
2025-10-06 00:55:13.466726: val_loss -0.3981
2025-10-06 00:55:13.466855: Pseudo dice [np.float32(0.6901)]
2025-10-06 00:55:13.467006: Epoch time: 46.4 s
2025-10-06 00:55:14.112753: 
2025-10-06 00:55:14.113092: Epoch 133
2025-10-06 00:55:14.113280: Current learning rate: 0.00141
2025-10-06 00:56:00.548419: Validation loss did not improve from -0.44801. Patience: 74/50
2025-10-06 00:56:00.548893: train_loss -0.8266
2025-10-06 00:56:00.549039: val_loss -0.4194
2025-10-06 00:56:00.549152: Pseudo dice [np.float32(0.6995)]
2025-10-06 00:56:00.549346: Epoch time: 46.44 s
2025-10-06 00:56:01.204958: 
2025-10-06 00:56:01.205209: Epoch 134
2025-10-06 00:56:01.205421: Current learning rate: 0.00133
2025-10-06 00:56:47.732083: Validation loss did not improve from -0.44801. Patience: 75/50
2025-10-06 00:56:47.732826: train_loss -0.8293
2025-10-06 00:56:47.733109: val_loss -0.3519
2025-10-06 00:56:47.733367: Pseudo dice [np.float32(0.6742)]
2025-10-06 00:56:47.733658: Epoch time: 46.53 s
2025-10-06 00:56:48.916057: 
2025-10-06 00:56:48.916387: Epoch 135
2025-10-06 00:56:48.916575: Current learning rate: 0.00126
2025-10-06 00:57:35.297347: Validation loss did not improve from -0.44801. Patience: 76/50
2025-10-06 00:57:35.297894: train_loss -0.8311
2025-10-06 00:57:35.298100: val_loss -0.395
2025-10-06 00:57:35.298222: Pseudo dice [np.float32(0.6781)]
2025-10-06 00:57:35.298417: Epoch time: 46.38 s
2025-10-06 00:57:36.512459: 
2025-10-06 00:57:36.512879: Epoch 136
2025-10-06 00:57:36.513092: Current learning rate: 0.00118
2025-10-06 00:58:22.875381: Validation loss did not improve from -0.44801. Patience: 77/50
2025-10-06 00:58:22.876078: train_loss -0.8301
2025-10-06 00:58:22.876221: val_loss -0.3724
2025-10-06 00:58:22.876334: Pseudo dice [np.float32(0.682)]
2025-10-06 00:58:22.876459: Epoch time: 46.36 s
2025-10-06 00:58:23.538641: 
2025-10-06 00:58:23.538963: Epoch 137
2025-10-06 00:58:23.539153: Current learning rate: 0.00111
2025-10-06 00:59:09.908355: Validation loss did not improve from -0.44801. Patience: 78/50
2025-10-06 00:59:09.908817: train_loss -0.8411
2025-10-06 00:59:09.908957: val_loss -0.3953
2025-10-06 00:59:09.909084: Pseudo dice [np.float32(0.686)]
2025-10-06 00:59:09.909215: Epoch time: 46.37 s
2025-10-06 00:59:10.566087: 
2025-10-06 00:59:10.566434: Epoch 138
2025-10-06 00:59:10.566674: Current learning rate: 0.00103
2025-10-06 00:59:57.049699: Validation loss did not improve from -0.44801. Patience: 79/50
2025-10-06 00:59:57.050322: train_loss -0.8339
2025-10-06 00:59:57.050462: val_loss -0.3711
2025-10-06 00:59:57.050628: Pseudo dice [np.float32(0.6725)]
2025-10-06 00:59:57.050768: Epoch time: 46.48 s
2025-10-06 00:59:57.711945: 
2025-10-06 00:59:57.712238: Epoch 139
2025-10-06 00:59:57.712438: Current learning rate: 0.00095
2025-10-06 01:00:44.099431: Validation loss did not improve from -0.44801. Patience: 80/50
2025-10-06 01:00:44.099887: train_loss -0.8358
2025-10-06 01:00:44.100035: val_loss -0.3783
2025-10-06 01:00:44.100150: Pseudo dice [np.float32(0.6783)]
2025-10-06 01:00:44.100280: Epoch time: 46.39 s
2025-10-06 01:00:45.237521: 
2025-10-06 01:00:45.237805: Epoch 140
2025-10-06 01:00:45.238027: Current learning rate: 0.00087
2025-10-06 01:01:31.540342: Validation loss did not improve from -0.44801. Patience: 81/50
2025-10-06 01:01:31.541044: train_loss -0.836
2025-10-06 01:01:31.541324: val_loss -0.3965
2025-10-06 01:01:31.541544: Pseudo dice [np.float32(0.6926)]
2025-10-06 01:01:31.541797: Epoch time: 46.3 s
2025-10-06 01:01:32.227892: 
2025-10-06 01:01:32.228257: Epoch 141
2025-10-06 01:01:32.228453: Current learning rate: 0.00079
2025-10-06 01:02:18.680130: Validation loss did not improve from -0.44801. Patience: 82/50
2025-10-06 01:02:18.680879: train_loss -0.84
2025-10-06 01:02:18.681261: val_loss -0.3872
2025-10-06 01:02:18.681486: Pseudo dice [np.float32(0.6872)]
2025-10-06 01:02:18.681755: Epoch time: 46.45 s
2025-10-06 01:02:19.354048: 
2025-10-06 01:02:19.354447: Epoch 142
2025-10-06 01:02:19.354674: Current learning rate: 0.00071
2025-10-06 01:03:05.755785: Validation loss did not improve from -0.44801. Patience: 83/50
2025-10-06 01:03:05.756638: train_loss -0.8369
2025-10-06 01:03:05.756899: val_loss -0.3511
2025-10-06 01:03:05.757133: Pseudo dice [np.float32(0.6619)]
2025-10-06 01:03:05.757379: Epoch time: 46.4 s
2025-10-06 01:03:06.423558: 
2025-10-06 01:03:06.423964: Epoch 143
2025-10-06 01:03:06.424269: Current learning rate: 0.00063
2025-10-06 01:03:52.832515: Validation loss did not improve from -0.44801. Patience: 84/50
2025-10-06 01:03:52.833019: train_loss -0.8393
2025-10-06 01:03:52.833207: val_loss -0.3729
2025-10-06 01:03:52.833361: Pseudo dice [np.float32(0.6764)]
2025-10-06 01:03:52.833532: Epoch time: 46.41 s
2025-10-06 01:03:53.493307: 
2025-10-06 01:03:53.493671: Epoch 144
2025-10-06 01:03:53.493922: Current learning rate: 0.00055
2025-10-06 01:04:39.946725: Validation loss did not improve from -0.44801. Patience: 85/50
2025-10-06 01:04:39.947596: train_loss -0.8426
2025-10-06 01:04:39.947850: val_loss -0.3965
2025-10-06 01:04:39.947992: Pseudo dice [np.float32(0.6877)]
2025-10-06 01:04:39.948151: Epoch time: 46.45 s
2025-10-06 01:04:41.145314: 
2025-10-06 01:04:41.145623: Epoch 145
2025-10-06 01:04:41.145845: Current learning rate: 0.00047
2025-10-06 01:05:27.543679: Validation loss did not improve from -0.44801. Patience: 86/50
2025-10-06 01:05:27.544196: train_loss -0.8376
2025-10-06 01:05:27.544338: val_loss -0.3857
2025-10-06 01:05:27.544446: Pseudo dice [np.float32(0.6818)]
2025-10-06 01:05:27.544568: Epoch time: 46.4 s
2025-10-06 01:05:28.199111: 
2025-10-06 01:05:28.199370: Epoch 146
2025-10-06 01:05:28.199724: Current learning rate: 0.00038
2025-10-06 01:06:14.560064: Validation loss did not improve from -0.44801. Patience: 87/50
2025-10-06 01:06:14.560715: train_loss -0.8458
2025-10-06 01:06:14.560869: val_loss -0.3939
2025-10-06 01:06:14.560976: Pseudo dice [np.float32(0.6912)]
2025-10-06 01:06:14.561099: Epoch time: 46.36 s
2025-10-06 01:06:15.217471: 
2025-10-06 01:06:15.217823: Epoch 147
2025-10-06 01:06:15.218039: Current learning rate: 0.0003
2025-10-06 01:07:01.606322: Validation loss did not improve from -0.44801. Patience: 88/50
2025-10-06 01:07:01.606758: train_loss -0.8406
2025-10-06 01:07:01.606937: val_loss -0.3846
2025-10-06 01:07:01.607071: Pseudo dice [np.float32(0.6863)]
2025-10-06 01:07:01.607235: Epoch time: 46.39 s
2025-10-06 01:07:02.263380: 
2025-10-06 01:07:02.263637: Epoch 148
2025-10-06 01:07:02.263837: Current learning rate: 0.00021
2025-10-06 01:07:48.718516: Validation loss did not improve from -0.44801. Patience: 89/50
2025-10-06 01:07:48.719284: train_loss -0.8439
2025-10-06 01:07:48.719461: val_loss -0.3613
2025-10-06 01:07:48.719609: Pseudo dice [np.float32(0.6724)]
2025-10-06 01:07:48.719825: Epoch time: 46.46 s
2025-10-06 01:07:49.391882: 
2025-10-06 01:07:49.392191: Epoch 149
2025-10-06 01:07:49.392386: Current learning rate: 0.00011
2025-10-06 01:08:35.814385: Validation loss did not improve from -0.44801. Patience: 90/50
2025-10-06 01:08:35.815015: train_loss -0.8424
2025-10-06 01:08:35.815286: val_loss -0.3743
2025-10-06 01:08:35.815448: Pseudo dice [np.float32(0.6836)]
2025-10-06 01:08:35.815678: Epoch time: 46.42 s
2025-10-06 01:08:36.985206: Training done.
2025-10-06 01:08:37.005754: Using splits from existing split file: /nfs/erelab001/shared/Computational_Group/Naravich/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset310_nnInteractive_Calcium_OCT_CrossValidation/splits_final_80.json
2025-10-06 01:08:37.006386: The split file contains 5 splits.
2025-10-06 01:08:37.007342: Desired fold for training: 3
2025-10-06 01:08:37.007682: This split has 6 training and 3 validation cases.
2025-10-06 01:08:37.008401: predicting 101-019
2025-10-06 01:08:37.012605: 101-019, shape torch.Size([1, 375, 498, 498]), rank 0
2025-10-06 01:09:25.041931: predicting 101-044
2025-10-06 01:09:25.055062: 101-044, shape torch.Size([1, 404, 498, 498]), rank 0
2025-10-06 01:10:02.103689: predicting 401-004
2025-10-06 01:10:02.116329: 401-004, shape torch.Size([1, 375, 498, 498]), rank 0
2025-10-06 01:10:49.922582: Validation complete
2025-10-06 01:10:49.922853: Mean Validation Dice:  0.6557108850987754
Finished training fold 3 saving to /nfs/erelab001/shared/Computational_Group/Naravich/datasets/nnUNet_Datasets/nnUNet_results/Dataset310_nnInteractive_Calcium_OCT_CrossValidation/nnUNetTrainerScaleAnalysis80__nnUNetPlans__3d_32x160x128_b10/fold_3_No_Pretrained
