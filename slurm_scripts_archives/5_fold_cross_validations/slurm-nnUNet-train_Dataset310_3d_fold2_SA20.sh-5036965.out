/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/bin/python
Starting training with CONFIG=3d_32x160x128_b10, DATASET_ID=310, TRAINER=nnUNetTrainerScaleAnalysis20
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/nnunetv2/training/nnUNetTrainer/nnUNetTrainer.py:169: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  self.grad_scaler = GradScaler() if self.device.type == 'cuda' else None

############################
INFO: You are using the old nnU-Net default plans. We have updated our recommendations. Please consider using those instead! Read more here: https://github.com/MIC-DKFZ/nnUNet/blob/master/documentation/resenc_presets.md
############################

Using device: cuda:0

#######################################################################
Please cite the following paper when using nnU-Net:
Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.
#######################################################################

2025-10-05 16:56:17.998687: do_dummy_2d_data_aug: True
2025-10-05 16:56:17.999092: Using splits from existing split file: /nfs/erelab001/shared/Computational_Group/Naravich/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset310_nnInteractive_Calcium_OCT_CrossValidation/splits_final_20.json
2025-10-05 16:56:17.999421: The split file contains 5 splits.
2025-10-05 16:56:17.999544: Desired fold for training: 2
2025-10-05 16:56:17.999742: This split has 1 training and 7 validation cases.
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/fft_conv_pytorch/fft_conv.py:139: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:306.)
  output = output[crop_slices].contiguous()
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/fft_conv_pytorch/fft_conv.py:139: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:306.)
  output = output[crop_slices].contiguous()
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/fft_conv_pytorch/fft_conv.py:139: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:306.)
  output = output[crop_slices].contiguous()
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/fft_conv_pytorch/fft_conv.py:139: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:306.)
  output = output[crop_slices].contiguous()
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/fft_conv_pytorch/fft_conv.py:139: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:306.)
  output = output[crop_slices].contiguous()
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/fft_conv_pytorch/fft_conv.py:139: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:306.)
  output = output[crop_slices].contiguous()
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/fft_conv_pytorch/fft_conv.py:139: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:306.)
  output = output[crop_slices].contiguous()
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/fft_conv_pytorch/fft_conv.py:139: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:306.)
  output = output[crop_slices].contiguous()
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/fft_conv_pytorch/fft_conv.py:139: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:306.)
  output = output[crop_slices].contiguous()
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/fft_conv_pytorch/fft_conv.py:139: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:306.)
  output = output[crop_slices].contiguous()
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/fft_conv_pytorch/fft_conv.py:139: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:306.)
  output = output[crop_slices].contiguous()
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/fft_conv_pytorch/fft_conv.py:139: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:306.)
  output = output[crop_slices].contiguous()
using pin_memory on device 0
using pin_memory on device 0
2025-10-05 16:56:21.752856: Using torch.compile...

This is the configuration used by this training:
Configuration name: 3d_32x160x128_b10
 {'data_identifier': 'nnUNetPlans_3d_fullres', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 10, 'patch_size': [32, 160, 128], 'median_image_size_in_voxels': [375.0, 498.0, 498.0], 'spacing': [1.0, 1.0, 1.0], 'normalization_schemes': ['ZScoreNormalization'], 'use_mask_for_norm': [False], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'architecture': {'network_class_name': 'dynamic_network_architectures.architectures.unet.PlainConvUNet', 'arch_kwargs': {'n_stages': 6, 'features_per_stage': [32, 64, 128, 256, 320, 320], 'conv_op': 'torch.nn.modules.conv.Conv3d', 'kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]], 'strides': [[1, 1, 1], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [1, 2, 2]], 'n_conv_per_stage': [2, 2, 2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2, 2, 2], 'conv_bias': True, 'norm_op': 'torch.nn.modules.instancenorm.InstanceNorm3d', 'norm_op_kwargs': {'eps': 1e-05, 'affine': True}, 'dropout_op': None, 'dropout_op_kwargs': None, 'nonlin': 'torch.nn.LeakyReLU', 'nonlin_kwargs': {'inplace': True}, 'deep_supervision': True}, '_kw_requires_import': ['conv_op', 'norm_op', 'dropout_op', 'nonlin']}, 'batch_dice': True, 'inherits_from': '3d_fullres'} 

These are the global plan.json settings:
 {'dataset_name': 'Dataset310_nnInteractive_Calcium_OCT_CrossValidation', 'plans_name': 'nnUNetPlans', 'original_median_spacing_after_transp': [1.0, 1.0, 1.0], 'original_median_shape_after_transp': [375, 498, 498], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 0.4977976381778717, 'mean': 0.1394134759902954, 'median': 0.09849607944488525, 'min': 0.0, 'percentile_00_5': 0.015305490233004093, 'percentile_99_5': 0.4977976381778717, 'std': 0.121165432035923}}} 

2025-10-05 16:56:23.401907: unpacking dataset...
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/torch/_inductor/lowering.py:7095: UserWarning: 
Online softmax is disabled on the fly since Inductor decides to
split the reduction. Cut an issue to PyTorch if this is an
important use case and you want to speed it up with online
softmax.

  warnings.warn(
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/torch/_inductor/lowering.py:7095: UserWarning: 
Online softmax is disabled on the fly since Inductor decides to
split the reduction. Cut an issue to PyTorch if this is an
important use case and you want to speed it up with online
softmax.

  warnings.warn(
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/torch/_inductor/lowering.py:7095: UserWarning: 
Online softmax is disabled on the fly since Inductor decides to
split the reduction. Cut an issue to PyTorch if this is an
important use case and you want to speed it up with online
softmax.

  warnings.warn(
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/torch/_inductor/lowering.py:7095: UserWarning: 
Online softmax is disabled on the fly since Inductor decides to
split the reduction. Cut an issue to PyTorch if this is an
important use case and you want to speed it up with online
softmax.

  warnings.warn(
2025-10-05 16:56:27.598124: unpacking done...
2025-10-05 16:56:27.600184: Unable to plot network architecture: nnUNet_compile is enabled!
2025-10-05 16:56:27.604824: 
2025-10-05 16:56:27.604988: Epoch 0
2025-10-05 16:56:27.605187: Current learning rate: 0.01
2025-10-05 16:57:48.375759: Validation loss improved from 1000.00000 to -0.08909! Patience: 0/50
2025-10-05 16:57:48.376570: train_loss -0.2148
2025-10-05 16:57:48.376860: val_loss -0.0891
2025-10-05 16:57:48.377046: Pseudo dice [np.float32(0.4651)]
2025-10-05 16:57:48.377275: Epoch time: 80.77 s
2025-10-05 16:57:48.377479: Yayy! New best EMA pseudo Dice: 0.4650999903678894
2025-10-05 16:57:49.307481: 
2025-10-05 16:57:49.307756: Epoch 1
2025-10-05 16:57:49.307953: Current learning rate: 0.00994
2025-10-05 16:58:35.764038: Validation loss improved from -0.08909 to -0.12798! Patience: 0/50
2025-10-05 16:58:35.764530: train_loss -0.3604
2025-10-05 16:58:35.764668: val_loss -0.128
2025-10-05 16:58:35.764783: Pseudo dice [np.float32(0.5385)]
2025-10-05 16:58:35.764909: Epoch time: 46.46 s
2025-10-05 16:58:35.765028: Yayy! New best EMA pseudo Dice: 0.4724999964237213
2025-10-05 16:58:36.858999: 
2025-10-05 16:58:36.859340: Epoch 2
2025-10-05 16:58:36.859558: Current learning rate: 0.00988
2025-10-05 16:59:23.400990: Validation loss improved from -0.12798 to -0.13915! Patience: 0/50
2025-10-05 16:59:23.401679: train_loss -0.4104
2025-10-05 16:59:23.401893: val_loss -0.1392
2025-10-05 16:59:23.402011: Pseudo dice [np.float32(0.5363)]
2025-10-05 16:59:23.402138: Epoch time: 46.54 s
2025-10-05 16:59:23.402262: Yayy! New best EMA pseudo Dice: 0.4788999855518341
2025-10-05 16:59:24.496148: 
2025-10-05 16:59:24.496453: Epoch 3
2025-10-05 16:59:24.496643: Current learning rate: 0.00982
2025-10-05 17:00:10.921290: Validation loss improved from -0.13915 to -0.17540! Patience: 0/50
2025-10-05 17:00:10.921771: train_loss -0.4571
2025-10-05 17:00:10.921964: val_loss -0.1754
2025-10-05 17:00:10.922124: Pseudo dice [np.float32(0.5677)]
2025-10-05 17:00:10.922274: Epoch time: 46.43 s
2025-10-05 17:00:10.922382: Yayy! New best EMA pseudo Dice: 0.4876999855041504
2025-10-05 17:00:12.017938: 
2025-10-05 17:00:12.018279: Epoch 4
2025-10-05 17:00:12.018482: Current learning rate: 0.00976
2025-10-05 17:00:58.443010: Validation loss did not improve from -0.17540. Patience: 1/50
2025-10-05 17:00:58.443634: train_loss -0.5093
2025-10-05 17:00:58.443826: val_loss -0.1506
2025-10-05 17:00:58.443978: Pseudo dice [np.float32(0.5568)]
2025-10-05 17:00:58.444140: Epoch time: 46.43 s
2025-10-05 17:00:58.850218: Yayy! New best EMA pseudo Dice: 0.49459999799728394
2025-10-05 17:00:59.911889: 
2025-10-05 17:00:59.912225: Epoch 5
2025-10-05 17:00:59.912461: Current learning rate: 0.0097
2025-10-05 17:01:46.299775: Validation loss did not improve from -0.17540. Patience: 2/50
2025-10-05 17:01:46.300190: train_loss -0.5384
2025-10-05 17:01:46.300371: val_loss -0.1415
2025-10-05 17:01:46.300534: Pseudo dice [np.float32(0.5716)]
2025-10-05 17:01:46.300687: Epoch time: 46.39 s
2025-10-05 17:01:46.300827: Yayy! New best EMA pseudo Dice: 0.5023000240325928
2025-10-05 17:01:47.358191: 
2025-10-05 17:01:47.358451: Epoch 6
2025-10-05 17:01:47.358670: Current learning rate: 0.00964
2025-10-05 17:02:33.757767: Validation loss improved from -0.17540 to -0.21229! Patience: 2/50
2025-10-05 17:02:33.758452: train_loss -0.5551
2025-10-05 17:02:33.758628: val_loss -0.2123
2025-10-05 17:02:33.758780: Pseudo dice [np.float32(0.5981)]
2025-10-05 17:02:33.758940: Epoch time: 46.4 s
2025-10-05 17:02:33.759056: Yayy! New best EMA pseudo Dice: 0.511900007724762
2025-10-05 17:02:34.832999: 
2025-10-05 17:02:34.833308: Epoch 7
2025-10-05 17:02:34.833500: Current learning rate: 0.00958
2025-10-05 17:03:21.419509: Validation loss did not improve from -0.21229. Patience: 1/50
2025-10-05 17:03:21.420025: train_loss -0.5752
2025-10-05 17:03:21.420234: val_loss -0.1181
2025-10-05 17:03:21.420382: Pseudo dice [np.float32(0.5567)]
2025-10-05 17:03:21.420529: Epoch time: 46.59 s
2025-10-05 17:03:21.420644: Yayy! New best EMA pseudo Dice: 0.5163999795913696
2025-10-05 17:03:22.507621: 
2025-10-05 17:03:22.508001: Epoch 8
2025-10-05 17:03:22.508291: Current learning rate: 0.00952
2025-10-05 17:04:09.081241: Validation loss did not improve from -0.21229. Patience: 2/50
2025-10-05 17:04:09.081956: train_loss -0.6011
2025-10-05 17:04:09.082108: val_loss -0.1139
2025-10-05 17:04:09.082258: Pseudo dice [np.float32(0.5929)]
2025-10-05 17:04:09.082432: Epoch time: 46.58 s
2025-10-05 17:04:09.082582: Yayy! New best EMA pseudo Dice: 0.5239999890327454
2025-10-05 17:04:10.159712: 
2025-10-05 17:04:10.160051: Epoch 9
2025-10-05 17:04:10.160395: Current learning rate: 0.00946
2025-10-05 17:04:56.662504: Validation loss did not improve from -0.21229. Patience: 3/50
2025-10-05 17:04:56.662971: train_loss -0.6147
2025-10-05 17:04:56.663110: val_loss -0.1901
2025-10-05 17:04:56.663328: Pseudo dice [np.float32(0.5882)]
2025-10-05 17:04:56.663462: Epoch time: 46.5 s
2025-10-05 17:04:57.114922: Yayy! New best EMA pseudo Dice: 0.5304999947547913
2025-10-05 17:04:58.182122: 
2025-10-05 17:04:58.182459: Epoch 10
2025-10-05 17:04:58.182664: Current learning rate: 0.0094
2025-10-05 17:05:44.631334: Validation loss did not improve from -0.21229. Patience: 4/50
2025-10-05 17:05:44.632030: train_loss -0.6386
2025-10-05 17:05:44.632235: val_loss -0.1475
2025-10-05 17:05:44.632405: Pseudo dice [np.float32(0.5808)]
2025-10-05 17:05:44.632607: Epoch time: 46.45 s
2025-10-05 17:05:44.632760: Yayy! New best EMA pseudo Dice: 0.5354999899864197
2025-10-05 17:05:45.722457: 
2025-10-05 17:05:45.722820: Epoch 11
2025-10-05 17:05:45.723022: Current learning rate: 0.00934
2025-10-05 17:06:32.173387: Validation loss did not improve from -0.21229. Patience: 5/50
2025-10-05 17:06:32.173857: train_loss -0.6297
2025-10-05 17:06:32.173992: val_loss -0.1499
2025-10-05 17:06:32.174165: Pseudo dice [np.float32(0.6096)]
2025-10-05 17:06:32.174303: Epoch time: 46.45 s
2025-10-05 17:06:32.174454: Yayy! New best EMA pseudo Dice: 0.542900025844574
2025-10-05 17:06:33.257126: 
2025-10-05 17:06:33.257465: Epoch 12
2025-10-05 17:06:33.257683: Current learning rate: 0.00928
2025-10-05 17:07:19.978205: Validation loss did not improve from -0.21229. Patience: 6/50
2025-10-05 17:07:19.978879: train_loss -0.6519
2025-10-05 17:07:19.979052: val_loss -0.1468
2025-10-05 17:07:19.979171: Pseudo dice [np.float32(0.5903)]
2025-10-05 17:07:19.979302: Epoch time: 46.72 s
2025-10-05 17:07:19.979434: Yayy! New best EMA pseudo Dice: 0.5476999878883362
2025-10-05 17:07:21.606510: 
2025-10-05 17:07:21.606965: Epoch 13
2025-10-05 17:07:21.607261: Current learning rate: 0.00922
2025-10-05 17:08:08.356836: Validation loss did not improve from -0.21229. Patience: 7/50
2025-10-05 17:08:08.357449: train_loss -0.6627
2025-10-05 17:08:08.357657: val_loss -0.1526
2025-10-05 17:08:08.357800: Pseudo dice [np.float32(0.5861)]
2025-10-05 17:08:08.357948: Epoch time: 46.75 s
2025-10-05 17:08:08.358066: Yayy! New best EMA pseudo Dice: 0.5515000224113464
2025-10-05 17:08:09.471554: 
2025-10-05 17:08:09.471863: Epoch 14
2025-10-05 17:08:09.472096: Current learning rate: 0.00916
2025-10-05 17:08:56.153555: Validation loss did not improve from -0.21229. Patience: 8/50
2025-10-05 17:08:56.154183: train_loss -0.6817
2025-10-05 17:08:56.154374: val_loss -0.1169
2025-10-05 17:08:56.154519: Pseudo dice [np.float32(0.5986)]
2025-10-05 17:08:56.154681: Epoch time: 46.68 s
2025-10-05 17:08:56.603456: Yayy! New best EMA pseudo Dice: 0.5562000274658203
2025-10-05 17:08:57.708545: 
2025-10-05 17:08:57.708911: Epoch 15
2025-10-05 17:08:57.709151: Current learning rate: 0.0091
2025-10-05 17:09:44.355088: Validation loss did not improve from -0.21229. Patience: 9/50
2025-10-05 17:09:44.355566: train_loss -0.6891
2025-10-05 17:09:44.355709: val_loss -0.1479
2025-10-05 17:09:44.355844: Pseudo dice [np.float32(0.5948)]
2025-10-05 17:09:44.356009: Epoch time: 46.65 s
2025-10-05 17:09:44.356164: Yayy! New best EMA pseudo Dice: 0.5601000189781189
2025-10-05 17:09:45.445877: 
2025-10-05 17:09:45.446289: Epoch 16
2025-10-05 17:09:45.446544: Current learning rate: 0.00903
2025-10-05 17:10:32.163999: Validation loss did not improve from -0.21229. Patience: 10/50
2025-10-05 17:10:32.165079: train_loss -0.7136
2025-10-05 17:10:32.165237: val_loss -0.0956
2025-10-05 17:10:32.165370: Pseudo dice [np.float32(0.572)]
2025-10-05 17:10:32.165560: Epoch time: 46.72 s
2025-10-05 17:10:32.165716: Yayy! New best EMA pseudo Dice: 0.5612999796867371
2025-10-05 17:10:33.293363: 
2025-10-05 17:10:33.293730: Epoch 17
2025-10-05 17:10:33.293953: Current learning rate: 0.00897
2025-10-05 17:11:19.972786: Validation loss did not improve from -0.21229. Patience: 11/50
2025-10-05 17:11:19.973396: train_loss -0.7158
2025-10-05 17:11:19.973768: val_loss -0.1186
2025-10-05 17:11:19.974008: Pseudo dice [np.float32(0.5906)]
2025-10-05 17:11:19.974304: Epoch time: 46.68 s
2025-10-05 17:11:19.974547: Yayy! New best EMA pseudo Dice: 0.5641999840736389
2025-10-05 17:11:21.109671: 
2025-10-05 17:11:21.110030: Epoch 18
2025-10-05 17:11:21.110282: Current learning rate: 0.00891
2025-10-05 17:12:07.819085: Validation loss did not improve from -0.21229. Patience: 12/50
2025-10-05 17:12:07.819851: train_loss -0.7295
2025-10-05 17:12:07.820005: val_loss -0.0694
2025-10-05 17:12:07.820142: Pseudo dice [np.float32(0.5883)]
2025-10-05 17:12:07.820361: Epoch time: 46.71 s
2025-10-05 17:12:07.820481: Yayy! New best EMA pseudo Dice: 0.5666000247001648
2025-10-05 17:12:08.933186: 
2025-10-05 17:12:08.933541: Epoch 19
2025-10-05 17:12:08.933791: Current learning rate: 0.00885
2025-10-05 17:12:55.572075: Validation loss did not improve from -0.21229. Patience: 13/50
2025-10-05 17:12:55.572497: train_loss -0.7442
2025-10-05 17:12:55.572636: val_loss -0.0706
2025-10-05 17:12:55.572768: Pseudo dice [np.float32(0.5849)]
2025-10-05 17:12:55.572923: Epoch time: 46.64 s
2025-10-05 17:12:56.039253: Yayy! New best EMA pseudo Dice: 0.5684000253677368
2025-10-05 17:12:57.142405: 
2025-10-05 17:12:57.142639: Epoch 20
2025-10-05 17:12:57.142827: Current learning rate: 0.00879
2025-10-05 17:13:43.729466: Validation loss did not improve from -0.21229. Patience: 14/50
2025-10-05 17:13:43.730214: train_loss -0.7548
2025-10-05 17:13:43.730372: val_loss -0.0135
2025-10-05 17:13:43.730554: Pseudo dice [np.float32(0.5779)]
2025-10-05 17:13:43.730722: Epoch time: 46.59 s
2025-10-05 17:13:43.730856: Yayy! New best EMA pseudo Dice: 0.5694000124931335
2025-10-05 17:13:44.870837: 
2025-10-05 17:13:44.871124: Epoch 21
2025-10-05 17:13:44.871351: Current learning rate: 0.00873
2025-10-05 17:14:31.516547: Validation loss did not improve from -0.21229. Patience: 15/50
2025-10-05 17:14:31.517101: train_loss -0.7676
2025-10-05 17:14:31.517255: val_loss -0.0562
2025-10-05 17:14:31.517398: Pseudo dice [np.float32(0.5949)]
2025-10-05 17:14:31.517558: Epoch time: 46.65 s
2025-10-05 17:14:31.517706: Yayy! New best EMA pseudo Dice: 0.5719000101089478
2025-10-05 17:14:32.628944: 
2025-10-05 17:14:32.629234: Epoch 22
2025-10-05 17:14:32.629454: Current learning rate: 0.00867
2025-10-05 17:15:19.405601: Validation loss did not improve from -0.21229. Patience: 16/50
2025-10-05 17:15:19.406478: train_loss -0.768
2025-10-05 17:15:19.406628: val_loss -0.0489
2025-10-05 17:15:19.406749: Pseudo dice [np.float32(0.5881)]
2025-10-05 17:15:19.406900: Epoch time: 46.78 s
2025-10-05 17:15:19.407107: Yayy! New best EMA pseudo Dice: 0.5734999775886536
2025-10-05 17:15:20.503330: 
2025-10-05 17:15:20.503684: Epoch 23
2025-10-05 17:15:20.503897: Current learning rate: 0.00861
2025-10-05 17:16:07.273868: Validation loss did not improve from -0.21229. Patience: 17/50
2025-10-05 17:16:07.274467: train_loss -0.7829
2025-10-05 17:16:07.274714: val_loss -0.0563
2025-10-05 17:16:07.274887: Pseudo dice [np.float32(0.607)]
2025-10-05 17:16:07.275094: Epoch time: 46.77 s
2025-10-05 17:16:07.275248: Yayy! New best EMA pseudo Dice: 0.5769000053405762
2025-10-05 17:16:08.380103: 
2025-10-05 17:16:08.380507: Epoch 24
2025-10-05 17:16:08.380789: Current learning rate: 0.00855
2025-10-05 17:16:55.107060: Validation loss did not improve from -0.21229. Patience: 18/50
2025-10-05 17:16:55.108121: train_loss -0.7949
2025-10-05 17:16:55.108603: val_loss -0.0854
2025-10-05 17:16:55.108935: Pseudo dice [np.float32(0.5722)]
2025-10-05 17:16:55.109243: Epoch time: 46.73 s
2025-10-05 17:16:56.204183: 
2025-10-05 17:16:56.204517: Epoch 25
2025-10-05 17:16:56.204742: Current learning rate: 0.00849
2025-10-05 17:17:42.933042: Validation loss did not improve from -0.21229. Patience: 19/50
2025-10-05 17:17:42.933803: train_loss -0.7878
2025-10-05 17:17:42.934270: val_loss -0.0776
2025-10-05 17:17:42.934517: Pseudo dice [np.float32(0.5891)]
2025-10-05 17:17:42.934753: Epoch time: 46.73 s
2025-10-05 17:17:42.934984: Yayy! New best EMA pseudo Dice: 0.5777000188827515
2025-10-05 17:17:44.048340: 
2025-10-05 17:17:44.048651: Epoch 26
2025-10-05 17:17:44.048964: Current learning rate: 0.00843
2025-10-05 17:18:30.750114: Validation loss did not improve from -0.21229. Patience: 20/50
2025-10-05 17:18:30.751040: train_loss -0.7971
2025-10-05 17:18:30.751244: val_loss -0.0662
2025-10-05 17:18:30.751423: Pseudo dice [np.float32(0.6026)]
2025-10-05 17:18:30.751638: Epoch time: 46.7 s
2025-10-05 17:18:30.751847: Yayy! New best EMA pseudo Dice: 0.5802000164985657
2025-10-05 17:18:31.871225: 
2025-10-05 17:18:31.871595: Epoch 27
2025-10-05 17:18:31.871839: Current learning rate: 0.00836
2025-10-05 17:19:18.639381: Validation loss did not improve from -0.21229. Patience: 21/50
2025-10-05 17:19:18.639918: train_loss -0.7946
2025-10-05 17:19:18.640092: val_loss -0.0426
2025-10-05 17:19:18.640329: Pseudo dice [np.float32(0.5997)]
2025-10-05 17:19:18.640474: Epoch time: 46.77 s
2025-10-05 17:19:18.640652: Yayy! New best EMA pseudo Dice: 0.582099974155426
2025-10-05 17:19:20.299430: 
2025-10-05 17:19:20.299833: Epoch 28
2025-10-05 17:19:20.300083: Current learning rate: 0.0083
2025-10-05 17:20:07.073298: Validation loss did not improve from -0.21229. Patience: 22/50
2025-10-05 17:20:07.074141: train_loss -0.8052
2025-10-05 17:20:07.074317: val_loss -0.0682
2025-10-05 17:20:07.074512: Pseudo dice [np.float32(0.6174)]
2025-10-05 17:20:07.074647: Epoch time: 46.78 s
2025-10-05 17:20:07.074838: Yayy! New best EMA pseudo Dice: 0.5856999754905701
2025-10-05 17:20:08.194324: 
2025-10-05 17:20:08.194713: Epoch 29
2025-10-05 17:20:08.194935: Current learning rate: 0.00824
2025-10-05 17:20:54.889939: Validation loss did not improve from -0.21229. Patience: 23/50
2025-10-05 17:20:54.890395: train_loss -0.8161
2025-10-05 17:20:54.890561: val_loss -0.0851
2025-10-05 17:20:54.890693: Pseudo dice [np.float32(0.6062)]
2025-10-05 17:20:54.890845: Epoch time: 46.7 s
2025-10-05 17:20:55.342122: Yayy! New best EMA pseudo Dice: 0.5877000093460083
2025-10-05 17:20:56.452451: 
2025-10-05 17:20:56.452786: Epoch 30
2025-10-05 17:20:56.453037: Current learning rate: 0.00818
2025-10-05 17:21:43.160691: Validation loss did not improve from -0.21229. Patience: 24/50
2025-10-05 17:21:43.161401: train_loss -0.8173
2025-10-05 17:21:43.161553: val_loss -0.0161
2025-10-05 17:21:43.161676: Pseudo dice [np.float32(0.568)]
2025-10-05 17:21:43.161819: Epoch time: 46.71 s
2025-10-05 17:21:43.820294: 
2025-10-05 17:21:43.820663: Epoch 31
2025-10-05 17:21:43.820909: Current learning rate: 0.00812
2025-10-05 17:22:30.569626: Validation loss did not improve from -0.21229. Patience: 25/50
2025-10-05 17:22:30.570174: train_loss -0.8254
2025-10-05 17:22:30.570391: val_loss -0.0211
2025-10-05 17:22:30.570564: Pseudo dice [np.float32(0.6088)]
2025-10-05 17:22:30.570754: Epoch time: 46.75 s
2025-10-05 17:22:30.570947: Yayy! New best EMA pseudo Dice: 0.5879999995231628
2025-10-05 17:22:31.725693: 
2025-10-05 17:22:31.726079: Epoch 32
2025-10-05 17:22:31.726329: Current learning rate: 0.00806
2025-10-05 17:23:18.566833: Validation loss did not improve from -0.21229. Patience: 26/50
2025-10-05 17:23:18.567544: train_loss -0.8282
2025-10-05 17:23:18.567691: val_loss -0.0118
2025-10-05 17:23:18.567832: Pseudo dice [np.float32(0.6069)]
2025-10-05 17:23:18.567983: Epoch time: 46.84 s
2025-10-05 17:23:18.568118: Yayy! New best EMA pseudo Dice: 0.589900016784668
2025-10-05 17:23:19.707489: 
2025-10-05 17:23:19.707815: Epoch 33
2025-10-05 17:23:19.708039: Current learning rate: 0.008
2025-10-05 17:24:06.526755: Validation loss did not improve from -0.21229. Patience: 27/50
2025-10-05 17:24:06.527337: train_loss -0.8337
2025-10-05 17:24:06.527574: val_loss -0.0361
2025-10-05 17:24:06.527701: Pseudo dice [np.float32(0.5885)]
2025-10-05 17:24:06.527888: Epoch time: 46.82 s
2025-10-05 17:24:07.184101: 
2025-10-05 17:24:07.184392: Epoch 34
2025-10-05 17:24:07.184633: Current learning rate: 0.00793
2025-10-05 17:24:53.925146: Validation loss did not improve from -0.21229. Patience: 28/50
2025-10-05 17:24:53.926168: train_loss -0.8405
2025-10-05 17:24:53.926316: val_loss 0.038
2025-10-05 17:24:53.926457: Pseudo dice [np.float32(0.5839)]
2025-10-05 17:24:53.926639: Epoch time: 46.74 s
2025-10-05 17:24:55.071714: 
2025-10-05 17:24:55.072071: Epoch 35
2025-10-05 17:24:55.072297: Current learning rate: 0.00787
2025-10-05 17:25:41.851350: Validation loss did not improve from -0.21229. Patience: 29/50
2025-10-05 17:25:41.851900: train_loss -0.8426
2025-10-05 17:25:41.852179: val_loss 0.0012
2025-10-05 17:25:41.852364: Pseudo dice [np.float32(0.6033)]
2025-10-05 17:25:41.852535: Epoch time: 46.78 s
2025-10-05 17:25:41.852718: Yayy! New best EMA pseudo Dice: 0.5906000137329102
2025-10-05 17:25:42.980967: 
2025-10-05 17:25:42.981203: Epoch 36
2025-10-05 17:25:42.981412: Current learning rate: 0.00781
2025-10-05 17:26:29.779678: Validation loss did not improve from -0.21229. Patience: 30/50
2025-10-05 17:26:29.780370: train_loss -0.8467
2025-10-05 17:26:29.780510: val_loss 0.0312
2025-10-05 17:26:29.780622: Pseudo dice [np.float32(0.5522)]
2025-10-05 17:26:29.780781: Epoch time: 46.8 s
2025-10-05 17:26:30.438046: 
2025-10-05 17:26:30.438319: Epoch 37
2025-10-05 17:26:30.438521: Current learning rate: 0.00775
2025-10-05 17:27:17.317967: Validation loss did not improve from -0.21229. Patience: 31/50
2025-10-05 17:27:17.318481: train_loss -0.8469
2025-10-05 17:27:17.318658: val_loss -0.0065
2025-10-05 17:27:17.318806: Pseudo dice [np.float32(0.5904)]
2025-10-05 17:27:17.318995: Epoch time: 46.88 s
2025-10-05 17:27:17.971825: 
2025-10-05 17:27:17.972138: Epoch 38
2025-10-05 17:27:17.972368: Current learning rate: 0.00769
2025-10-05 17:28:04.821193: Validation loss did not improve from -0.21229. Patience: 32/50
2025-10-05 17:28:04.821787: train_loss -0.8427
2025-10-05 17:28:04.821956: val_loss 0.0228
2025-10-05 17:28:04.822100: Pseudo dice [np.float32(0.5991)]
2025-10-05 17:28:04.822256: Epoch time: 46.85 s
2025-10-05 17:28:05.468377: 
2025-10-05 17:28:05.468785: Epoch 39
2025-10-05 17:28:05.468994: Current learning rate: 0.00763
2025-10-05 17:28:52.218908: Validation loss did not improve from -0.21229. Patience: 33/50
2025-10-05 17:28:52.219430: train_loss -0.851
2025-10-05 17:28:52.219667: val_loss -0.0634
2025-10-05 17:28:52.219903: Pseudo dice [np.float32(0.6197)]
2025-10-05 17:28:52.220157: Epoch time: 46.75 s
2025-10-05 17:28:52.696080: Yayy! New best EMA pseudo Dice: 0.5914000272750854
2025-10-05 17:28:53.830033: 
2025-10-05 17:28:53.830476: Epoch 40
2025-10-05 17:28:53.830744: Current learning rate: 0.00756
2025-10-05 17:29:40.568338: Validation loss did not improve from -0.21229. Patience: 34/50
2025-10-05 17:29:40.569054: train_loss -0.8552
2025-10-05 17:29:40.569232: val_loss -0.0128
2025-10-05 17:29:40.569399: Pseudo dice [np.float32(0.5966)]
2025-10-05 17:29:40.569587: Epoch time: 46.74 s
2025-10-05 17:29:40.569768: Yayy! New best EMA pseudo Dice: 0.5920000076293945
2025-10-05 17:29:41.693743: 
2025-10-05 17:29:41.694093: Epoch 41
2025-10-05 17:29:41.694333: Current learning rate: 0.0075
2025-10-05 17:30:28.487805: Validation loss did not improve from -0.21229. Patience: 35/50
2025-10-05 17:30:28.488312: train_loss -0.8538
2025-10-05 17:30:28.488466: val_loss 0.0018
2025-10-05 17:30:28.488614: Pseudo dice [np.float32(0.6157)]
2025-10-05 17:30:28.488784: Epoch time: 46.8 s
2025-10-05 17:30:28.488943: Yayy! New best EMA pseudo Dice: 0.5942999720573425
2025-10-05 17:30:30.125846: 
2025-10-05 17:30:30.126136: Epoch 42
2025-10-05 17:30:30.126348: Current learning rate: 0.00744
2025-10-05 17:31:16.974729: Validation loss did not improve from -0.21229. Patience: 36/50
2025-10-05 17:31:16.975587: train_loss -0.8593
2025-10-05 17:31:16.975817: val_loss 0.0465
2025-10-05 17:31:16.976016: Pseudo dice [np.float32(0.5933)]
2025-10-05 17:31:16.976269: Epoch time: 46.85 s
2025-10-05 17:31:17.612298: 
2025-10-05 17:31:17.612599: Epoch 43
2025-10-05 17:31:17.612857: Current learning rate: 0.00738
2025-10-05 17:32:04.443164: Validation loss did not improve from -0.21229. Patience: 37/50
2025-10-05 17:32:04.443658: train_loss -0.8589
2025-10-05 17:32:04.443858: val_loss 0.0443
2025-10-05 17:32:04.443990: Pseudo dice [np.float32(0.5809)]
2025-10-05 17:32:04.444137: Epoch time: 46.83 s
2025-10-05 17:32:05.079427: 
2025-10-05 17:32:05.079685: Epoch 44
2025-10-05 17:32:05.079980: Current learning rate: 0.00732
2025-10-05 17:32:51.753562: Validation loss did not improve from -0.21229. Patience: 38/50
2025-10-05 17:32:51.754245: train_loss -0.8628
2025-10-05 17:32:51.754384: val_loss 0.026
2025-10-05 17:32:51.754556: Pseudo dice [np.float32(0.5882)]
2025-10-05 17:32:51.754724: Epoch time: 46.68 s
2025-10-05 17:32:52.851962: 
2025-10-05 17:32:52.852278: Epoch 45
2025-10-05 17:32:52.852467: Current learning rate: 0.00725
2025-10-05 17:33:39.573643: Validation loss did not improve from -0.21229. Patience: 39/50
2025-10-05 17:33:39.574156: train_loss -0.8684
2025-10-05 17:33:39.574302: val_loss -0.0009
2025-10-05 17:33:39.574507: Pseudo dice [np.float32(0.6021)]
2025-10-05 17:33:39.574645: Epoch time: 46.72 s
2025-10-05 17:33:40.204583: 
2025-10-05 17:33:40.204877: Epoch 46
2025-10-05 17:33:40.205087: Current learning rate: 0.00719
2025-10-05 17:34:26.957372: Validation loss did not improve from -0.21229. Patience: 40/50
2025-10-05 17:34:26.958092: train_loss -0.8625
2025-10-05 17:34:26.958236: val_loss -0.0021
2025-10-05 17:34:26.958347: Pseudo dice [np.float32(0.5987)]
2025-10-05 17:34:26.958474: Epoch time: 46.75 s
2025-10-05 17:34:27.594662: 
2025-10-05 17:34:27.595027: Epoch 47
2025-10-05 17:34:27.595235: Current learning rate: 0.00713
2025-10-05 17:35:14.453969: Validation loss did not improve from -0.21229. Patience: 41/50
2025-10-05 17:35:14.454610: train_loss -0.871
2025-10-05 17:35:14.454977: val_loss 0.0329
2025-10-05 17:35:14.455197: Pseudo dice [np.float32(0.5705)]
2025-10-05 17:35:14.455349: Epoch time: 46.86 s
2025-10-05 17:35:15.100486: 
2025-10-05 17:35:15.100845: Epoch 48
2025-10-05 17:35:15.101174: Current learning rate: 0.00707
2025-10-05 17:36:01.898344: Validation loss did not improve from -0.21229. Patience: 42/50
2025-10-05 17:36:01.898966: train_loss -0.8709
2025-10-05 17:36:01.899126: val_loss 0.0044
2025-10-05 17:36:01.899251: Pseudo dice [np.float32(0.5707)]
2025-10-05 17:36:01.899425: Epoch time: 46.8 s
2025-10-05 17:36:02.539763: 
2025-10-05 17:36:02.540107: Epoch 49
2025-10-05 17:36:02.540363: Current learning rate: 0.007
2025-10-05 17:36:49.252057: Validation loss did not improve from -0.21229. Patience: 43/50
2025-10-05 17:36:49.252594: train_loss -0.8712
2025-10-05 17:36:49.252771: val_loss 0.0261
2025-10-05 17:36:49.252921: Pseudo dice [np.float32(0.5688)]
2025-10-05 17:36:49.253114: Epoch time: 46.71 s
2025-10-05 17:36:50.346670: 
2025-10-05 17:36:50.347081: Epoch 50
2025-10-05 17:36:50.347421: Current learning rate: 0.00694
2025-10-05 17:37:37.067813: Validation loss did not improve from -0.21229. Patience: 44/50
2025-10-05 17:37:37.068619: train_loss -0.8719
2025-10-05 17:37:37.068830: val_loss 0.0127
2025-10-05 17:37:37.069049: Pseudo dice [np.float32(0.5989)]
2025-10-05 17:37:37.069266: Epoch time: 46.72 s
2025-10-05 17:37:37.720461: 
2025-10-05 17:37:37.720886: Epoch 51
2025-10-05 17:37:37.721093: Current learning rate: 0.00688
2025-10-05 17:38:24.433163: Validation loss did not improve from -0.21229. Patience: 45/50
2025-10-05 17:38:24.433661: train_loss -0.8793
2025-10-05 17:38:24.433839: val_loss 0.0141
2025-10-05 17:38:24.434000: Pseudo dice [np.float32(0.596)]
2025-10-05 17:38:24.434248: Epoch time: 46.71 s
2025-10-05 17:38:25.081538: 
2025-10-05 17:38:25.081815: Epoch 52
2025-10-05 17:38:25.082014: Current learning rate: 0.00682
2025-10-05 17:39:11.842488: Validation loss did not improve from -0.21229. Patience: 46/50
2025-10-05 17:39:11.843359: train_loss -0.8767
2025-10-05 17:39:11.843593: val_loss 0.0847
2025-10-05 17:39:11.843727: Pseudo dice [np.float32(0.6037)]
2025-10-05 17:39:11.843993: Epoch time: 46.76 s
2025-10-05 17:39:12.489866: 
2025-10-05 17:39:12.490123: Epoch 53
2025-10-05 17:39:12.490381: Current learning rate: 0.00675
2025-10-05 17:39:59.160151: Validation loss did not improve from -0.21229. Patience: 47/50
2025-10-05 17:39:59.160642: train_loss -0.8838
2025-10-05 17:39:59.160784: val_loss 0.0377
2025-10-05 17:39:59.160962: Pseudo dice [np.float32(0.5967)]
2025-10-05 17:39:59.161116: Epoch time: 46.67 s
2025-10-05 17:39:59.801242: 
2025-10-05 17:39:59.801489: Epoch 54
2025-10-05 17:39:59.801672: Current learning rate: 0.00669
2025-10-05 17:40:46.434468: Validation loss did not improve from -0.21229. Patience: 48/50
2025-10-05 17:40:46.435099: train_loss -0.8853
2025-10-05 17:40:46.435258: val_loss 0.0135
2025-10-05 17:40:46.435370: Pseudo dice [np.float32(0.6203)]
2025-10-05 17:40:46.435499: Epoch time: 46.63 s
2025-10-05 17:40:47.551780: 
2025-10-05 17:40:47.552037: Epoch 55
2025-10-05 17:40:47.552334: Current learning rate: 0.00663
2025-10-05 17:41:34.185510: Validation loss did not improve from -0.21229. Patience: 49/50
2025-10-05 17:41:34.186016: train_loss -0.8887
2025-10-05 17:41:34.186192: val_loss 0.0553
2025-10-05 17:41:34.186337: Pseudo dice [np.float32(0.6037)]
2025-10-05 17:41:34.186467: Epoch time: 46.63 s
2025-10-05 17:41:34.186578: Yayy! New best EMA pseudo Dice: 0.5952000021934509
2025-10-05 17:41:35.315055: 
2025-10-05 17:41:35.315433: Epoch 56
2025-10-05 17:41:35.315646: Current learning rate: 0.00657
2025-10-05 17:42:21.951149: Validation loss did not improve from -0.21229. Patience: 50/50
2025-10-05 17:42:21.952043: train_loss -0.8894
2025-10-05 17:42:21.952216: val_loss 0.033
2025-10-05 17:42:21.952465: Pseudo dice [np.float32(0.5844)]
2025-10-05 17:42:21.952713: Epoch time: 46.64 s
2025-10-05 17:42:22.600282: 
2025-10-05 17:42:22.600697: Epoch 57
2025-10-05 17:42:22.601002: Current learning rate: 0.0065
2025-10-05 17:43:09.312032: Validation loss did not improve from -0.21229. Patience: 51/50
2025-10-05 17:43:09.312946: train_loss -0.8915
2025-10-05 17:43:09.313513: val_loss 0.0773
2025-10-05 17:43:09.314001: Pseudo dice [np.float32(0.6188)]
2025-10-05 17:43:09.314530: Epoch time: 46.71 s
2025-10-05 17:43:09.315098: Yayy! New best EMA pseudo Dice: 0.5965999960899353
2025-10-05 17:43:10.435674: 
2025-10-05 17:43:10.435958: Epoch 58
2025-10-05 17:43:10.436188: Current learning rate: 0.00644
2025-10-05 17:43:57.051640: Validation loss did not improve from -0.21229. Patience: 52/50
2025-10-05 17:43:57.052512: train_loss -0.8926
2025-10-05 17:43:57.052847: val_loss 0.0183
2025-10-05 17:43:57.053100: Pseudo dice [np.float32(0.6007)]
2025-10-05 17:43:57.053302: Epoch time: 46.62 s
2025-10-05 17:43:57.053583: Yayy! New best EMA pseudo Dice: 0.597000002861023
2025-10-05 17:43:58.698979: 
2025-10-05 17:43:58.699321: Epoch 59
2025-10-05 17:43:58.699583: Current learning rate: 0.00638
2025-10-05 17:44:45.330126: Validation loss did not improve from -0.21229. Patience: 53/50
2025-10-05 17:44:45.330634: train_loss -0.8895
2025-10-05 17:44:45.330815: val_loss -0.0008
2025-10-05 17:44:45.331012: Pseudo dice [np.float32(0.611)]
2025-10-05 17:44:45.331156: Epoch time: 46.63 s
2025-10-05 17:44:45.781116: Yayy! New best EMA pseudo Dice: 0.5983999967575073
2025-10-05 17:44:46.873252: 
2025-10-05 17:44:46.873576: Epoch 60
2025-10-05 17:44:46.873833: Current learning rate: 0.00631
2025-10-05 17:45:33.600353: Validation loss did not improve from -0.21229. Patience: 54/50
2025-10-05 17:45:33.601232: train_loss -0.8937
2025-10-05 17:45:33.601457: val_loss 0.0054
2025-10-05 17:45:33.601707: Pseudo dice [np.float32(0.6129)]
2025-10-05 17:45:33.601941: Epoch time: 46.73 s
2025-10-05 17:45:33.602149: Yayy! New best EMA pseudo Dice: 0.5999000072479248
2025-10-05 17:45:34.726568: 
2025-10-05 17:45:34.726897: Epoch 61
2025-10-05 17:45:34.727216: Current learning rate: 0.00625
2025-10-05 17:46:21.548327: Validation loss did not improve from -0.21229. Patience: 55/50
2025-10-05 17:46:21.548863: train_loss -0.897
2025-10-05 17:46:21.549006: val_loss 0.0549
2025-10-05 17:46:21.549118: Pseudo dice [np.float32(0.6005)]
2025-10-05 17:46:21.549244: Epoch time: 46.82 s
2025-10-05 17:46:21.549379: Yayy! New best EMA pseudo Dice: 0.5999000072479248
2025-10-05 17:46:22.676267: 
2025-10-05 17:46:22.676648: Epoch 62
2025-10-05 17:46:22.676954: Current learning rate: 0.00619
2025-10-05 17:47:09.594927: Validation loss did not improve from -0.21229. Patience: 56/50
2025-10-05 17:47:09.595520: train_loss -0.8991
2025-10-05 17:47:09.595654: val_loss 0.0422
2025-10-05 17:47:09.595762: Pseudo dice [np.float32(0.5995)]
2025-10-05 17:47:09.595886: Epoch time: 46.92 s
2025-10-05 17:47:10.257288: 
2025-10-05 17:47:10.257579: Epoch 63
2025-10-05 17:47:10.257787: Current learning rate: 0.00612
2025-10-05 17:47:56.950801: Validation loss did not improve from -0.21229. Patience: 57/50
2025-10-05 17:47:56.951268: train_loss -0.9018
2025-10-05 17:47:56.951416: val_loss 0.0598
2025-10-05 17:47:56.951540: Pseudo dice [np.float32(0.6129)]
2025-10-05 17:47:56.951684: Epoch time: 46.69 s
2025-10-05 17:47:56.951809: Yayy! New best EMA pseudo Dice: 0.6011999845504761
2025-10-05 17:47:58.090409: 
2025-10-05 17:47:58.090702: Epoch 64
2025-10-05 17:47:58.090893: Current learning rate: 0.00606
2025-10-05 17:48:44.806632: Validation loss did not improve from -0.21229. Patience: 58/50
2025-10-05 17:48:44.807376: train_loss -0.902
2025-10-05 17:48:44.807537: val_loss 0.0454
2025-10-05 17:48:44.807676: Pseudo dice [np.float32(0.6207)]
2025-10-05 17:48:44.807907: Epoch time: 46.72 s
2025-10-05 17:48:45.277868: Yayy! New best EMA pseudo Dice: 0.6031000018119812
2025-10-05 17:48:46.398159: 
2025-10-05 17:48:46.398475: Epoch 65
2025-10-05 17:48:46.398766: Current learning rate: 0.006
2025-10-05 17:49:33.170173: Validation loss did not improve from -0.21229. Patience: 59/50
2025-10-05 17:49:33.170654: train_loss -0.9021
2025-10-05 17:49:33.170828: val_loss 0.0346
2025-10-05 17:49:33.170953: Pseudo dice [np.float32(0.6153)]
2025-10-05 17:49:33.171092: Epoch time: 46.77 s
2025-10-05 17:49:33.171237: Yayy! New best EMA pseudo Dice: 0.6043000221252441
2025-10-05 17:49:34.294430: 
2025-10-05 17:49:34.294838: Epoch 66
2025-10-05 17:49:34.295097: Current learning rate: 0.00593
2025-10-05 17:50:21.057827: Validation loss did not improve from -0.21229. Patience: 60/50
2025-10-05 17:50:21.058525: train_loss -0.9023
2025-10-05 17:50:21.058704: val_loss 0.0591
2025-10-05 17:50:21.058871: Pseudo dice [np.float32(0.6069)]
2025-10-05 17:50:21.059123: Epoch time: 46.76 s
2025-10-05 17:50:21.059311: Yayy! New best EMA pseudo Dice: 0.6046000123023987
2025-10-05 17:50:22.193618: 
2025-10-05 17:50:22.193964: Epoch 67
2025-10-05 17:50:22.194231: Current learning rate: 0.00587
2025-10-05 17:51:08.965316: Validation loss did not improve from -0.21229. Patience: 61/50
2025-10-05 17:51:08.965749: train_loss -0.9
2025-10-05 17:51:08.965934: val_loss 0.0488
2025-10-05 17:51:08.966063: Pseudo dice [np.float32(0.5954)]
2025-10-05 17:51:08.966218: Epoch time: 46.77 s
2025-10-05 17:51:09.626529: 
2025-10-05 17:51:09.626907: Epoch 68
2025-10-05 17:51:09.627166: Current learning rate: 0.00581
2025-10-05 17:51:56.434489: Validation loss did not improve from -0.21229. Patience: 62/50
2025-10-05 17:51:56.435124: train_loss -0.9039
2025-10-05 17:51:56.435307: val_loss 0.0864
2025-10-05 17:51:56.435442: Pseudo dice [np.float32(0.5942)]
2025-10-05 17:51:56.435575: Epoch time: 46.81 s
2025-10-05 17:51:57.085023: 
2025-10-05 17:51:57.085272: Epoch 69
2025-10-05 17:51:57.085520: Current learning rate: 0.00574
2025-10-05 17:52:43.808095: Validation loss did not improve from -0.21229. Patience: 63/50
2025-10-05 17:52:43.808579: train_loss -0.9057
2025-10-05 17:52:43.808782: val_loss 0.0737
2025-10-05 17:52:43.808932: Pseudo dice [np.float32(0.5965)]
2025-10-05 17:52:43.809080: Epoch time: 46.72 s
2025-10-05 17:52:44.920579: 
2025-10-05 17:52:44.921058: Epoch 70
2025-10-05 17:52:44.921301: Current learning rate: 0.00568
2025-10-05 17:53:31.629909: Validation loss did not improve from -0.21229. Patience: 64/50
2025-10-05 17:53:31.630505: train_loss -0.907
2025-10-05 17:53:31.630649: val_loss 0.0565
2025-10-05 17:53:31.630771: Pseudo dice [np.float32(0.6202)]
2025-10-05 17:53:31.630908: Epoch time: 46.71 s
2025-10-05 17:53:32.280174: 
2025-10-05 17:53:32.280427: Epoch 71
2025-10-05 17:53:32.280637: Current learning rate: 0.00562
2025-10-05 17:54:19.022236: Validation loss did not improve from -0.21229. Patience: 65/50
2025-10-05 17:54:19.022924: train_loss -0.9088
2025-10-05 17:54:19.023167: val_loss 0.0778
2025-10-05 17:54:19.023322: Pseudo dice [np.float32(0.6106)]
2025-10-05 17:54:19.023483: Epoch time: 46.74 s
2025-10-05 17:54:19.689918: 
2025-10-05 17:54:19.690198: Epoch 72
2025-10-05 17:54:19.690394: Current learning rate: 0.00555
2025-10-05 17:55:06.439875: Validation loss did not improve from -0.21229. Patience: 66/50
2025-10-05 17:55:06.440557: train_loss -0.9073
2025-10-05 17:55:06.440696: val_loss 0.0497
2025-10-05 17:55:06.440813: Pseudo dice [np.float32(0.6038)]
2025-10-05 17:55:06.440975: Epoch time: 46.75 s
2025-10-05 17:55:07.096611: 
2025-10-05 17:55:07.096954: Epoch 73
2025-10-05 17:55:07.097169: Current learning rate: 0.00549
2025-10-05 17:55:54.421658: Validation loss did not improve from -0.21229. Patience: 67/50
2025-10-05 17:55:54.422131: train_loss -0.907
2025-10-05 17:55:54.422310: val_loss 0.1084
2025-10-05 17:55:54.422460: Pseudo dice [np.float32(0.6022)]
2025-10-05 17:55:54.422711: Epoch time: 47.33 s
2025-10-05 17:55:55.069978: 
2025-10-05 17:55:55.070349: Epoch 74
2025-10-05 17:55:55.070611: Current learning rate: 0.00542
2025-10-05 17:56:41.744102: Validation loss did not improve from -0.21229. Patience: 68/50
2025-10-05 17:56:41.744692: train_loss -0.9104
2025-10-05 17:56:41.744894: val_loss 0.0478
2025-10-05 17:56:41.745062: Pseudo dice [np.float32(0.6178)]
2025-10-05 17:56:41.745237: Epoch time: 46.68 s
2025-10-05 17:56:42.220968: Yayy! New best EMA pseudo Dice: 0.6055999994277954
2025-10-05 17:56:43.328905: 
2025-10-05 17:56:43.329265: Epoch 75
2025-10-05 17:56:43.329460: Current learning rate: 0.00536
2025-10-05 17:57:30.183256: Validation loss did not improve from -0.21229. Patience: 69/50
2025-10-05 17:57:30.183852: train_loss -0.9067
2025-10-05 17:57:30.184036: val_loss 0.1254
2025-10-05 17:57:30.184183: Pseudo dice [np.float32(0.5897)]
2025-10-05 17:57:30.184378: Epoch time: 46.86 s
2025-10-05 17:57:30.838604: 
2025-10-05 17:57:30.839055: Epoch 76
2025-10-05 17:57:30.839315: Current learning rate: 0.00529
2025-10-05 17:58:17.542664: Validation loss did not improve from -0.21229. Patience: 70/50
2025-10-05 17:58:17.543371: train_loss -0.9097
2025-10-05 17:58:17.543534: val_loss 0.0479
2025-10-05 17:58:17.543650: Pseudo dice [np.float32(0.606)]
2025-10-05 17:58:17.543788: Epoch time: 46.71 s
2025-10-05 17:58:18.201865: 
2025-10-05 17:58:18.202240: Epoch 77
2025-10-05 17:58:18.202500: Current learning rate: 0.00523
2025-10-05 17:59:05.014207: Validation loss did not improve from -0.21229. Patience: 71/50
2025-10-05 17:59:05.014781: train_loss -0.9123
2025-10-05 17:59:05.014982: val_loss 0.0444
2025-10-05 17:59:05.015139: Pseudo dice [np.float32(0.6032)]
2025-10-05 17:59:05.015300: Epoch time: 46.81 s
2025-10-05 17:59:05.678761: 
2025-10-05 17:59:05.679058: Epoch 78
2025-10-05 17:59:05.679279: Current learning rate: 0.00517
2025-10-05 17:59:52.592591: Validation loss did not improve from -0.21229. Patience: 72/50
2025-10-05 17:59:52.593580: train_loss -0.9138
2025-10-05 17:59:52.593841: val_loss 0.021
2025-10-05 17:59:52.594091: Pseudo dice [np.float32(0.6072)]
2025-10-05 17:59:52.594373: Epoch time: 46.92 s
2025-10-05 17:59:53.260457: 
2025-10-05 17:59:53.260764: Epoch 79
2025-10-05 17:59:53.260981: Current learning rate: 0.0051
2025-10-05 18:00:40.128907: Validation loss did not improve from -0.21229. Patience: 73/50
2025-10-05 18:00:40.129525: train_loss -0.9139
2025-10-05 18:00:40.129785: val_loss 0.0902
2025-10-05 18:00:40.129969: Pseudo dice [np.float32(0.5982)]
2025-10-05 18:00:40.130162: Epoch time: 46.87 s
2025-10-05 18:00:41.266704: 
2025-10-05 18:00:41.266995: Epoch 80
2025-10-05 18:00:41.267258: Current learning rate: 0.00504
2025-10-05 18:01:27.910752: Validation loss did not improve from -0.21229. Patience: 74/50
2025-10-05 18:01:27.911340: train_loss -0.9131
2025-10-05 18:01:27.911476: val_loss 0.1327
2025-10-05 18:01:27.911583: Pseudo dice [np.float32(0.5818)]
2025-10-05 18:01:27.911752: Epoch time: 46.65 s
2025-10-05 18:01:28.570024: 
2025-10-05 18:01:28.570298: Epoch 81
2025-10-05 18:01:28.570482: Current learning rate: 0.00497
2025-10-05 18:02:15.278192: Validation loss did not improve from -0.21229. Patience: 75/50
2025-10-05 18:02:15.278749: train_loss -0.9129
2025-10-05 18:02:15.278908: val_loss 0.0676
2025-10-05 18:02:15.279062: Pseudo dice [np.float32(0.6084)]
2025-10-05 18:02:15.279210: Epoch time: 46.71 s
2025-10-05 18:02:15.939064: 
2025-10-05 18:02:15.939362: Epoch 82
2025-10-05 18:02:15.939598: Current learning rate: 0.00491
2025-10-05 18:03:02.786992: Validation loss did not improve from -0.21229. Patience: 76/50
2025-10-05 18:03:02.787674: train_loss -0.9159
2025-10-05 18:03:02.787827: val_loss 0.0926
2025-10-05 18:03:02.787998: Pseudo dice [np.float32(0.61)]
2025-10-05 18:03:02.788143: Epoch time: 46.85 s
2025-10-05 18:03:03.433036: 
2025-10-05 18:03:03.433406: Epoch 83
2025-10-05 18:03:03.433646: Current learning rate: 0.00484
2025-10-05 18:03:50.154863: Validation loss did not improve from -0.21229. Patience: 77/50
2025-10-05 18:03:50.155242: train_loss -0.9161
2025-10-05 18:03:50.155400: val_loss 0.0241
2025-10-05 18:03:50.155517: Pseudo dice [np.float32(0.6148)]
2025-10-05 18:03:50.155691: Epoch time: 46.72 s
2025-10-05 18:03:50.797854: 
2025-10-05 18:03:50.798104: Epoch 84
2025-10-05 18:03:50.798328: Current learning rate: 0.00478
2025-10-05 18:04:37.587717: Validation loss did not improve from -0.21229. Patience: 78/50
2025-10-05 18:04:37.588378: train_loss -0.9179
2025-10-05 18:04:37.588567: val_loss 0.1522
2025-10-05 18:04:37.588726: Pseudo dice [np.float32(0.5916)]
2025-10-05 18:04:37.588924: Epoch time: 46.79 s
2025-10-05 18:04:38.704668: 
2025-10-05 18:04:38.704983: Epoch 85
2025-10-05 18:04:38.705186: Current learning rate: 0.00471
2025-10-05 18:05:25.508124: Validation loss did not improve from -0.21229. Patience: 79/50
2025-10-05 18:05:25.508646: train_loss -0.9187
2025-10-05 18:05:25.508819: val_loss 0.0894
2025-10-05 18:05:25.508953: Pseudo dice [np.float32(0.6222)]
2025-10-05 18:05:25.509108: Epoch time: 46.8 s
2025-10-05 18:05:26.154218: 
2025-10-05 18:05:26.154458: Epoch 86
2025-10-05 18:05:26.154678: Current learning rate: 0.00465
2025-10-05 18:06:12.899361: Validation loss did not improve from -0.21229. Patience: 80/50
2025-10-05 18:06:12.900029: train_loss -0.9192
2025-10-05 18:06:12.900172: val_loss 0.0678
2025-10-05 18:06:12.900296: Pseudo dice [np.float32(0.6153)]
2025-10-05 18:06:12.900436: Epoch time: 46.75 s
2025-10-05 18:06:12.900552: Yayy! New best EMA pseudo Dice: 0.60589998960495
2025-10-05 18:06:14.004077: 
2025-10-05 18:06:14.004399: Epoch 87
2025-10-05 18:06:14.004649: Current learning rate: 0.00458
2025-10-05 18:07:00.761626: Validation loss did not improve from -0.21229. Patience: 81/50
2025-10-05 18:07:00.762122: train_loss -0.9204
2025-10-05 18:07:00.762287: val_loss 0.0469
2025-10-05 18:07:00.762405: Pseudo dice [np.float32(0.6134)]
2025-10-05 18:07:00.762538: Epoch time: 46.76 s
2025-10-05 18:07:00.762671: Yayy! New best EMA pseudo Dice: 0.6067000031471252
2025-10-05 18:07:01.853464: 
2025-10-05 18:07:01.853853: Epoch 88
2025-10-05 18:07:01.854101: Current learning rate: 0.00452
2025-10-05 18:07:48.608654: Validation loss did not improve from -0.21229. Patience: 82/50
2025-10-05 18:07:48.609352: train_loss -0.9203
2025-10-05 18:07:48.609528: val_loss 0.1098
2025-10-05 18:07:48.609671: Pseudo dice [np.float32(0.5928)]
2025-10-05 18:07:48.609852: Epoch time: 46.76 s
2025-10-05 18:07:49.785545: 
2025-10-05 18:07:49.785805: Epoch 89
2025-10-05 18:07:49.785989: Current learning rate: 0.00445
2025-10-05 18:08:36.484859: Validation loss did not improve from -0.21229. Patience: 83/50
2025-10-05 18:08:36.485319: train_loss -0.9238
2025-10-05 18:08:36.485493: val_loss 0.1199
2025-10-05 18:08:36.485658: Pseudo dice [np.float32(0.6103)]
2025-10-05 18:08:36.485833: Epoch time: 46.7 s
2025-10-05 18:08:37.580783: 
2025-10-05 18:08:37.581103: Epoch 90
2025-10-05 18:08:37.581304: Current learning rate: 0.00438
2025-10-05 18:09:24.440974: Validation loss did not improve from -0.21229. Patience: 84/50
2025-10-05 18:09:24.441668: train_loss -0.9255
2025-10-05 18:09:24.441845: val_loss 0.0691
2025-10-05 18:09:24.441959: Pseudo dice [np.float32(0.6084)]
2025-10-05 18:09:24.442086: Epoch time: 46.86 s
2025-10-05 18:09:25.087966: 
2025-10-05 18:09:25.088333: Epoch 91
2025-10-05 18:09:25.088548: Current learning rate: 0.00432
2025-10-05 18:10:11.863851: Validation loss did not improve from -0.21229. Patience: 85/50
2025-10-05 18:10:11.864381: train_loss -0.9207
2025-10-05 18:10:11.864560: val_loss 0.1245
2025-10-05 18:10:11.864711: Pseudo dice [np.float32(0.6003)]
2025-10-05 18:10:11.864953: Epoch time: 46.78 s
2025-10-05 18:10:12.512405: 
2025-10-05 18:10:12.512675: Epoch 92
2025-10-05 18:10:12.512918: Current learning rate: 0.00425
2025-10-05 18:10:59.308452: Validation loss did not improve from -0.21229. Patience: 86/50
2025-10-05 18:10:59.309306: train_loss -0.9215
2025-10-05 18:10:59.309483: val_loss 0.0132
2025-10-05 18:10:59.309634: Pseudo dice [np.float32(0.6173)]
2025-10-05 18:10:59.309850: Epoch time: 46.8 s
2025-10-05 18:10:59.961943: 
2025-10-05 18:10:59.962299: Epoch 93
2025-10-05 18:10:59.962523: Current learning rate: 0.00419
2025-10-05 18:11:46.738659: Validation loss did not improve from -0.21229. Patience: 87/50
2025-10-05 18:11:46.739414: train_loss -0.9222
2025-10-05 18:11:46.739664: val_loss 0.0712
2025-10-05 18:11:46.739913: Pseudo dice [np.float32(0.6271)]
2025-10-05 18:11:46.740144: Epoch time: 46.78 s
2025-10-05 18:11:46.740336: Yayy! New best EMA pseudo Dice: 0.6086999773979187
2025-10-05 18:11:47.841864: 
2025-10-05 18:11:47.842224: Epoch 94
2025-10-05 18:11:47.842440: Current learning rate: 0.00412
2025-10-05 18:12:34.588974: Validation loss did not improve from -0.21229. Patience: 88/50
2025-10-05 18:12:34.589704: train_loss -0.9248
2025-10-05 18:12:34.589871: val_loss -0.0085
2025-10-05 18:12:34.589992: Pseudo dice [np.float32(0.6299)]
2025-10-05 18:12:34.590147: Epoch time: 46.75 s
2025-10-05 18:12:35.060693: Yayy! New best EMA pseudo Dice: 0.61080002784729
2025-10-05 18:12:36.135288: 
2025-10-05 18:12:36.135621: Epoch 95
2025-10-05 18:12:36.135847: Current learning rate: 0.00405
2025-10-05 18:13:22.961112: Validation loss did not improve from -0.21229. Patience: 89/50
2025-10-05 18:13:22.961644: train_loss -0.9258
2025-10-05 18:13:22.961938: val_loss 0.0691
2025-10-05 18:13:22.962105: Pseudo dice [np.float32(0.6138)]
2025-10-05 18:13:22.962317: Epoch time: 46.83 s
2025-10-05 18:13:22.962444: Yayy! New best EMA pseudo Dice: 0.6111000180244446
2025-10-05 18:13:24.056152: 
2025-10-05 18:13:24.056507: Epoch 96
2025-10-05 18:13:24.056749: Current learning rate: 0.00399
2025-10-05 18:14:10.981364: Validation loss did not improve from -0.21229. Patience: 90/50
2025-10-05 18:14:10.982100: train_loss -0.9234
2025-10-05 18:14:10.982275: val_loss 0.1263
2025-10-05 18:14:10.982413: Pseudo dice [np.float32(0.5782)]
2025-10-05 18:14:10.982572: Epoch time: 46.93 s
2025-10-05 18:14:11.627823: 
2025-10-05 18:14:11.628103: Epoch 97
2025-10-05 18:14:11.628301: Current learning rate: 0.00392
2025-10-05 18:14:58.534427: Validation loss did not improve from -0.21229. Patience: 91/50
2025-10-05 18:14:58.534923: train_loss -0.9246
2025-10-05 18:14:58.535099: val_loss 0.0993
2025-10-05 18:14:58.535219: Pseudo dice [np.float32(0.6174)]
2025-10-05 18:14:58.535378: Epoch time: 46.91 s
2025-10-05 18:14:59.180396: 
2025-10-05 18:14:59.180652: Epoch 98
2025-10-05 18:14:59.180856: Current learning rate: 0.00385
2025-10-05 18:15:45.936874: Validation loss did not improve from -0.21229. Patience: 92/50
2025-10-05 18:15:45.937644: train_loss -0.9261
2025-10-05 18:15:45.937932: val_loss 0.0694
2025-10-05 18:15:45.938163: Pseudo dice [np.float32(0.6241)]
2025-10-05 18:15:45.938434: Epoch time: 46.76 s
2025-10-05 18:15:46.591253: 
2025-10-05 18:15:46.591539: Epoch 99
2025-10-05 18:15:46.591727: Current learning rate: 0.00379
2025-10-05 18:16:33.332654: Validation loss did not improve from -0.21229. Patience: 93/50
2025-10-05 18:16:33.333185: train_loss -0.927
2025-10-05 18:16:33.333355: val_loss 0.1249
2025-10-05 18:16:33.333491: Pseudo dice [np.float32(0.5968)]
2025-10-05 18:16:33.333643: Epoch time: 46.74 s
2025-10-05 18:16:34.443822: 
2025-10-05 18:16:34.444062: Epoch 100
2025-10-05 18:16:34.444278: Current learning rate: 0.00372
2025-10-05 18:17:21.222967: Validation loss did not improve from -0.21229. Patience: 94/50
2025-10-05 18:17:21.223687: train_loss -0.9267
2025-10-05 18:17:21.223965: val_loss 0.0983
2025-10-05 18:17:21.224174: Pseudo dice [np.float32(0.6033)]
2025-10-05 18:17:21.224451: Epoch time: 46.78 s
2025-10-05 18:17:21.883017: 
2025-10-05 18:17:21.883363: Epoch 101
2025-10-05 18:17:21.883553: Current learning rate: 0.00365
2025-10-05 18:18:08.645043: Validation loss did not improve from -0.21229. Patience: 95/50
2025-10-05 18:18:08.645559: train_loss -0.9259
2025-10-05 18:18:08.645705: val_loss 0.0976
2025-10-05 18:18:08.645822: Pseudo dice [np.float32(0.6022)]
2025-10-05 18:18:08.645977: Epoch time: 46.76 s
2025-10-05 18:18:09.297117: 
2025-10-05 18:18:09.297373: Epoch 102
2025-10-05 18:18:09.297587: Current learning rate: 0.00359
2025-10-05 18:18:56.046221: Validation loss did not improve from -0.21229. Patience: 96/50
2025-10-05 18:18:56.046802: train_loss -0.9279
2025-10-05 18:18:56.046969: val_loss 0.1638
2025-10-05 18:18:56.047091: Pseudo dice [np.float32(0.5838)]
2025-10-05 18:18:56.047242: Epoch time: 46.75 s
2025-10-05 18:18:56.695228: 
2025-10-05 18:18:56.695570: Epoch 103
2025-10-05 18:18:56.695780: Current learning rate: 0.00352
2025-10-05 18:19:43.434844: Validation loss did not improve from -0.21229. Patience: 97/50
2025-10-05 18:19:43.435458: train_loss -0.9297
2025-10-05 18:19:43.435695: val_loss 0.0524
2025-10-05 18:19:43.435842: Pseudo dice [np.float32(0.6078)]
2025-10-05 18:19:43.436135: Epoch time: 46.74 s
2025-10-05 18:19:44.097295: 
2025-10-05 18:19:44.097654: Epoch 104
2025-10-05 18:19:44.097871: Current learning rate: 0.00345
2025-10-05 18:20:30.812614: Validation loss did not improve from -0.21229. Patience: 98/50
2025-10-05 18:20:30.813248: train_loss -0.9293
2025-10-05 18:20:30.813426: val_loss 0.0905
2025-10-05 18:20:30.813548: Pseudo dice [np.float32(0.6155)]
2025-10-05 18:20:30.813689: Epoch time: 46.72 s
2025-10-05 18:20:32.463252: 
2025-10-05 18:20:32.463673: Epoch 105
2025-10-05 18:20:32.463914: Current learning rate: 0.00338
2025-10-05 18:21:19.138790: Validation loss did not improve from -0.21229. Patience: 99/50
2025-10-05 18:21:19.139669: train_loss -0.93
2025-10-05 18:21:19.140020: val_loss 0.0638
2025-10-05 18:21:19.140221: Pseudo dice [np.float32(0.6214)]
2025-10-05 18:21:19.140446: Epoch time: 46.68 s
2025-10-05 18:21:19.801451: 
2025-10-05 18:21:19.801787: Epoch 106
2025-10-05 18:21:19.801980: Current learning rate: 0.00332
2025-10-05 18:22:06.520896: Validation loss did not improve from -0.21229. Patience: 100/50
2025-10-05 18:22:06.521553: train_loss -0.9319
2025-10-05 18:22:06.521699: val_loss 0.0941
2025-10-05 18:22:06.521832: Pseudo dice [np.float32(0.6073)]
2025-10-05 18:22:06.521968: Epoch time: 46.72 s
2025-10-05 18:22:07.166517: 
2025-10-05 18:22:07.166964: Epoch 107
2025-10-05 18:22:07.167199: Current learning rate: 0.00325
2025-10-05 18:22:53.916688: Validation loss did not improve from -0.21229. Patience: 101/50
2025-10-05 18:22:53.917169: train_loss -0.9298
2025-10-05 18:22:53.917370: val_loss 0.1004
2025-10-05 18:22:53.917516: Pseudo dice [np.float32(0.6126)]
2025-10-05 18:22:53.917685: Epoch time: 46.75 s
2025-10-05 18:22:54.568124: 
2025-10-05 18:22:54.568364: Epoch 108
2025-10-05 18:22:54.568557: Current learning rate: 0.00318
2025-10-05 18:23:41.328629: Validation loss did not improve from -0.21229. Patience: 102/50
2025-10-05 18:23:41.329541: train_loss -0.9315
2025-10-05 18:23:41.329724: val_loss 0.1177
2025-10-05 18:23:41.329860: Pseudo dice [np.float32(0.6084)]
2025-10-05 18:23:41.330001: Epoch time: 46.76 s
2025-10-05 18:23:41.988487: 
2025-10-05 18:23:41.988880: Epoch 109
2025-10-05 18:23:41.989129: Current learning rate: 0.00311
2025-10-05 18:24:28.743714: Validation loss did not improve from -0.21229. Patience: 103/50
2025-10-05 18:24:28.744124: train_loss -0.9312
2025-10-05 18:24:28.744283: val_loss 0.1062
2025-10-05 18:24:28.744413: Pseudo dice [np.float32(0.5933)]
2025-10-05 18:24:28.744542: Epoch time: 46.76 s
2025-10-05 18:24:29.885150: 
2025-10-05 18:24:29.885504: Epoch 110
2025-10-05 18:24:29.885741: Current learning rate: 0.00304
2025-10-05 18:25:16.564061: Validation loss did not improve from -0.21229. Patience: 104/50
2025-10-05 18:25:16.564731: train_loss -0.9306
2025-10-05 18:25:16.564886: val_loss 0.1054
2025-10-05 18:25:16.565004: Pseudo dice [np.float32(0.5946)]
2025-10-05 18:25:16.565166: Epoch time: 46.68 s
2025-10-05 18:25:17.213985: 
2025-10-05 18:25:17.214332: Epoch 111
2025-10-05 18:25:17.214546: Current learning rate: 0.00297
2025-10-05 18:26:03.891707: Validation loss did not improve from -0.21229. Patience: 105/50
2025-10-05 18:26:03.892129: train_loss -0.9316
2025-10-05 18:26:03.892345: val_loss 0.1153
2025-10-05 18:26:03.892480: Pseudo dice [np.float32(0.6095)]
2025-10-05 18:26:03.892661: Epoch time: 46.68 s
2025-10-05 18:26:04.537441: 
2025-10-05 18:26:04.537810: Epoch 112
2025-10-05 18:26:04.538027: Current learning rate: 0.00291
2025-10-05 18:26:51.377629: Validation loss did not improve from -0.21229. Patience: 106/50
2025-10-05 18:26:51.378336: train_loss -0.933
2025-10-05 18:26:51.378491: val_loss 0.0657
2025-10-05 18:26:51.378686: Pseudo dice [np.float32(0.6119)]
2025-10-05 18:26:51.378828: Epoch time: 46.84 s
2025-10-05 18:26:52.034766: 
2025-10-05 18:26:52.035023: Epoch 113
2025-10-05 18:26:52.035232: Current learning rate: 0.00284
2025-10-05 18:27:38.909713: Validation loss did not improve from -0.21229. Patience: 107/50
2025-10-05 18:27:38.910265: train_loss -0.9332
2025-10-05 18:27:38.910492: val_loss 0.1159
2025-10-05 18:27:38.910689: Pseudo dice [np.float32(0.6089)]
2025-10-05 18:27:38.910910: Epoch time: 46.88 s
2025-10-05 18:27:39.562796: 
2025-10-05 18:27:39.563142: Epoch 114
2025-10-05 18:27:39.563401: Current learning rate: 0.00277
2025-10-05 18:28:26.315623: Validation loss did not improve from -0.21229. Patience: 108/50
2025-10-05 18:28:26.316452: train_loss -0.9351
2025-10-05 18:28:26.316789: val_loss 0.1495
2025-10-05 18:28:26.317089: Pseudo dice [np.float32(0.6092)]
2025-10-05 18:28:26.317308: Epoch time: 46.75 s
2025-10-05 18:28:27.464559: 
2025-10-05 18:28:27.464871: Epoch 115
2025-10-05 18:28:27.465110: Current learning rate: 0.0027
2025-10-05 18:29:14.194206: Validation loss did not improve from -0.21229. Patience: 109/50
2025-10-05 18:29:14.194879: train_loss -0.9335
2025-10-05 18:29:14.195208: val_loss 0.1367
2025-10-05 18:29:14.195453: Pseudo dice [np.float32(0.6051)]
2025-10-05 18:29:14.195651: Epoch time: 46.73 s
2025-10-05 18:29:14.869795: 
2025-10-05 18:29:14.870114: Epoch 116
2025-10-05 18:29:14.870305: Current learning rate: 0.00263
2025-10-05 18:30:01.659865: Validation loss did not improve from -0.21229. Patience: 110/50
2025-10-05 18:30:01.660663: train_loss -0.9352
2025-10-05 18:30:01.660894: val_loss 0.077
2025-10-05 18:30:01.661059: Pseudo dice [np.float32(0.6215)]
2025-10-05 18:30:01.661219: Epoch time: 46.79 s
2025-10-05 18:30:02.324170: 
2025-10-05 18:30:02.324468: Epoch 117
2025-10-05 18:30:02.324740: Current learning rate: 0.00256
2025-10-05 18:30:49.138364: Validation loss did not improve from -0.21229. Patience: 111/50
2025-10-05 18:30:49.138877: train_loss -0.9349
2025-10-05 18:30:49.139044: val_loss 0.0887
2025-10-05 18:30:49.139180: Pseudo dice [np.float32(0.6247)]
2025-10-05 18:30:49.139309: Epoch time: 46.82 s
2025-10-05 18:30:49.797794: 
2025-10-05 18:30:49.798160: Epoch 118
2025-10-05 18:30:49.798368: Current learning rate: 0.00249
2025-10-05 18:31:36.671419: Validation loss did not improve from -0.21229. Patience: 112/50
2025-10-05 18:31:36.672182: train_loss -0.9358
2025-10-05 18:31:36.672383: val_loss 0.0768
2025-10-05 18:31:36.672563: Pseudo dice [np.float32(0.6233)]
2025-10-05 18:31:36.672745: Epoch time: 46.88 s
2025-10-05 18:31:36.672884: Yayy! New best EMA pseudo Dice: 0.611299991607666
2025-10-05 18:31:37.831739: 
2025-10-05 18:31:37.832039: Epoch 119
2025-10-05 18:31:37.832268: Current learning rate: 0.00242
2025-10-05 18:32:24.599038: Validation loss did not improve from -0.21229. Patience: 113/50
2025-10-05 18:32:24.599668: train_loss -0.9361
2025-10-05 18:32:24.599939: val_loss 0.0875
2025-10-05 18:32:24.600121: Pseudo dice [np.float32(0.6216)]
2025-10-05 18:32:24.600307: Epoch time: 46.77 s
2025-10-05 18:32:25.073163: Yayy! New best EMA pseudo Dice: 0.6123999953269958
2025-10-05 18:32:26.168771: 
2025-10-05 18:32:26.169034: Epoch 120
2025-10-05 18:32:26.169255: Current learning rate: 0.00235
2025-10-05 18:33:12.936311: Validation loss did not improve from -0.21229. Patience: 114/50
2025-10-05 18:33:12.937058: train_loss -0.936
2025-10-05 18:33:12.937232: val_loss 0.1151
2025-10-05 18:33:12.937379: Pseudo dice [np.float32(0.6075)]
2025-10-05 18:33:12.937553: Epoch time: 46.77 s
2025-10-05 18:33:14.165546: 
2025-10-05 18:33:14.165854: Epoch 121
2025-10-05 18:33:14.166122: Current learning rate: 0.00228
2025-10-05 18:34:00.836110: Validation loss did not improve from -0.21229. Patience: 115/50
2025-10-05 18:34:00.836699: train_loss -0.9385
2025-10-05 18:34:00.836930: val_loss 0.106
2025-10-05 18:34:00.837120: Pseudo dice [np.float32(0.6063)]
2025-10-05 18:34:00.837343: Epoch time: 46.67 s
2025-10-05 18:34:01.488394: 
2025-10-05 18:34:01.488750: Epoch 122
2025-10-05 18:34:01.489001: Current learning rate: 0.00221
2025-10-05 18:34:48.252234: Validation loss did not improve from -0.21229. Patience: 116/50
2025-10-05 18:34:48.252970: train_loss -0.937
2025-10-05 18:34:48.253131: val_loss 0.1075
2025-10-05 18:34:48.253252: Pseudo dice [np.float32(0.6182)]
2025-10-05 18:34:48.253386: Epoch time: 46.77 s
2025-10-05 18:34:48.909029: 
2025-10-05 18:34:48.909488: Epoch 123
2025-10-05 18:34:48.909742: Current learning rate: 0.00214
2025-10-05 18:35:35.693633: Validation loss did not improve from -0.21229. Patience: 117/50
2025-10-05 18:35:35.694226: train_loss -0.9363
2025-10-05 18:35:35.694597: val_loss 0.1042
2025-10-05 18:35:35.694945: Pseudo dice [np.float32(0.6196)]
2025-10-05 18:35:35.695254: Epoch time: 46.79 s
2025-10-05 18:35:35.695511: Yayy! New best EMA pseudo Dice: 0.6128000020980835
2025-10-05 18:35:36.844316: 
2025-10-05 18:35:36.844688: Epoch 124
2025-10-05 18:35:36.844971: Current learning rate: 0.00207
2025-10-05 18:36:23.531939: Validation loss did not improve from -0.21229. Patience: 118/50
2025-10-05 18:36:23.532667: train_loss -0.937
2025-10-05 18:36:23.532852: val_loss 0.1234
2025-10-05 18:36:23.533029: Pseudo dice [np.float32(0.6043)]
2025-10-05 18:36:23.533222: Epoch time: 46.69 s
2025-10-05 18:36:24.647009: 
2025-10-05 18:36:24.647409: Epoch 125
2025-10-05 18:36:24.647683: Current learning rate: 0.00199
2025-10-05 18:37:11.523313: Validation loss did not improve from -0.21229. Patience: 119/50
2025-10-05 18:37:11.523819: train_loss -0.9377
2025-10-05 18:37:11.524019: val_loss 0.1593
2025-10-05 18:37:11.524267: Pseudo dice [np.float32(0.6107)]
2025-10-05 18:37:11.524493: Epoch time: 46.88 s
2025-10-05 18:37:12.175898: 
2025-10-05 18:37:12.176223: Epoch 126
2025-10-05 18:37:12.176440: Current learning rate: 0.00192
2025-10-05 18:37:58.906276: Validation loss did not improve from -0.21229. Patience: 120/50
2025-10-05 18:37:58.907344: train_loss -0.9385
2025-10-05 18:37:58.907708: val_loss 0.0821
2025-10-05 18:37:58.907937: Pseudo dice [np.float32(0.6217)]
2025-10-05 18:37:58.908197: Epoch time: 46.73 s
2025-10-05 18:37:58.908448: Yayy! New best EMA pseudo Dice: 0.6128000020980835
2025-10-05 18:38:00.048403: 
2025-10-05 18:38:00.048690: Epoch 127
2025-10-05 18:38:00.048885: Current learning rate: 0.00185
2025-10-05 18:38:46.859043: Validation loss did not improve from -0.21229. Patience: 121/50
2025-10-05 18:38:46.859515: train_loss -0.937
2025-10-05 18:38:46.859740: val_loss 0.0352
2025-10-05 18:38:46.859859: Pseudo dice [np.float32(0.6299)]
2025-10-05 18:38:46.859987: Epoch time: 46.81 s
2025-10-05 18:38:46.860139: Yayy! New best EMA pseudo Dice: 0.6144999861717224
2025-10-05 18:38:47.983226: 
2025-10-05 18:38:47.983593: Epoch 128
2025-10-05 18:38:47.983848: Current learning rate: 0.00178
2025-10-05 18:39:34.715247: Validation loss did not improve from -0.21229. Patience: 122/50
2025-10-05 18:39:34.715807: train_loss -0.9378
2025-10-05 18:39:34.715961: val_loss 0.1412
2025-10-05 18:39:34.716069: Pseudo dice [np.float32(0.605)]
2025-10-05 18:39:34.716228: Epoch time: 46.73 s
2025-10-05 18:39:35.367945: 
2025-10-05 18:39:35.368215: Epoch 129
2025-10-05 18:39:35.368403: Current learning rate: 0.0017
2025-10-05 18:40:22.108191: Validation loss did not improve from -0.21229. Patience: 123/50
2025-10-05 18:40:22.108664: train_loss -0.9384
2025-10-05 18:40:22.108825: val_loss 0.0892
2025-10-05 18:40:22.108941: Pseudo dice [np.float32(0.6195)]
2025-10-05 18:40:22.109067: Epoch time: 46.74 s
2025-10-05 18:40:23.241461: 
2025-10-05 18:40:23.241812: Epoch 130
2025-10-05 18:40:23.241994: Current learning rate: 0.00163
2025-10-05 18:41:10.188941: Validation loss did not improve from -0.21229. Patience: 124/50
2025-10-05 18:41:10.189789: train_loss -0.9382
2025-10-05 18:41:10.189987: val_loss 0.1441
2025-10-05 18:41:10.190152: Pseudo dice [np.float32(0.6059)]
2025-10-05 18:41:10.190349: Epoch time: 46.95 s
2025-10-05 18:41:10.838005: 
2025-10-05 18:41:10.838379: Epoch 131
2025-10-05 18:41:10.838605: Current learning rate: 0.00156
2025-10-05 18:41:57.546432: Validation loss did not improve from -0.21229. Patience: 125/50
2025-10-05 18:41:57.546876: train_loss -0.9398
2025-10-05 18:41:57.547104: val_loss 0.1291
2025-10-05 18:41:57.547347: Pseudo dice [np.float32(0.6318)]
2025-10-05 18:41:57.547569: Epoch time: 46.71 s
2025-10-05 18:41:57.547713: Yayy! New best EMA pseudo Dice: 0.6151999831199646
2025-10-05 18:41:58.652501: 
2025-10-05 18:41:58.652944: Epoch 132
2025-10-05 18:41:58.653237: Current learning rate: 0.00148
2025-10-05 18:42:45.405159: Validation loss did not improve from -0.21229. Patience: 126/50
2025-10-05 18:42:45.405800: train_loss -0.9396
2025-10-05 18:42:45.405940: val_loss 0.1384
2025-10-05 18:42:45.406052: Pseudo dice [np.float32(0.6176)]
2025-10-05 18:42:45.406187: Epoch time: 46.75 s
2025-10-05 18:42:45.406303: Yayy! New best EMA pseudo Dice: 0.6154000163078308
2025-10-05 18:42:46.541461: 
2025-10-05 18:42:46.541866: Epoch 133
2025-10-05 18:42:46.542159: Current learning rate: 0.00141
2025-10-05 18:43:33.285457: Validation loss did not improve from -0.21229. Patience: 127/50
2025-10-05 18:43:33.285950: train_loss -0.9422
2025-10-05 18:43:33.286117: val_loss 0.1139
2025-10-05 18:43:33.286267: Pseudo dice [np.float32(0.5991)]
2025-10-05 18:43:33.286422: Epoch time: 46.75 s
2025-10-05 18:43:33.930972: 
2025-10-05 18:43:33.931346: Epoch 134
2025-10-05 18:43:33.931621: Current learning rate: 0.00133
2025-10-05 18:44:20.645785: Validation loss did not improve from -0.21229. Patience: 128/50
2025-10-05 18:44:20.646576: train_loss -0.9415
2025-10-05 18:44:20.646804: val_loss 0.1745
2025-10-05 18:44:20.646977: Pseudo dice [np.float32(0.6019)]
2025-10-05 18:44:20.647232: Epoch time: 46.72 s
2025-10-05 18:44:21.747394: 
2025-10-05 18:44:21.747728: Epoch 135
2025-10-05 18:44:21.747984: Current learning rate: 0.00126
2025-10-05 18:45:08.395168: Validation loss did not improve from -0.21229. Patience: 129/50
2025-10-05 18:45:08.395645: train_loss -0.9403
2025-10-05 18:45:08.395786: val_loss 0.1114
2025-10-05 18:45:08.395896: Pseudo dice [np.float32(0.6024)]
2025-10-05 18:45:08.396025: Epoch time: 46.65 s
2025-10-05 18:45:09.534957: 
2025-10-05 18:45:09.535243: Epoch 136
2025-10-05 18:45:09.535519: Current learning rate: 0.00118
2025-10-05 18:45:56.153099: Validation loss did not improve from -0.21229. Patience: 130/50
2025-10-05 18:45:56.153793: train_loss -0.9402
2025-10-05 18:45:56.153922: val_loss 0.1229
2025-10-05 18:45:56.154124: Pseudo dice [np.float32(0.6156)]
2025-10-05 18:45:56.154253: Epoch time: 46.62 s
2025-10-05 18:45:56.801726: 
2025-10-05 18:45:56.802018: Epoch 137
2025-10-05 18:45:56.802206: Current learning rate: 0.00111
2025-10-05 18:46:43.546411: Validation loss did not improve from -0.21229. Patience: 131/50
2025-10-05 18:46:43.546895: train_loss -0.941
2025-10-05 18:46:43.547069: val_loss 0.1359
2025-10-05 18:46:43.547189: Pseudo dice [np.float32(0.6174)]
2025-10-05 18:46:43.547328: Epoch time: 46.75 s
2025-10-05 18:46:44.207979: 
2025-10-05 18:46:44.208430: Epoch 138
2025-10-05 18:46:44.208681: Current learning rate: 0.00103
2025-10-05 18:47:31.118075: Validation loss did not improve from -0.21229. Patience: 132/50
2025-10-05 18:47:31.118841: train_loss -0.9413
2025-10-05 18:47:31.118975: val_loss 0.1525
2025-10-05 18:47:31.119156: Pseudo dice [np.float32(0.618)]
2025-10-05 18:47:31.119371: Epoch time: 46.91 s
2025-10-05 18:47:31.769000: 
2025-10-05 18:47:31.769349: Epoch 139
2025-10-05 18:47:31.769650: Current learning rate: 0.00095
2025-10-05 18:48:18.561598: Validation loss did not improve from -0.21229. Patience: 133/50
2025-10-05 18:48:18.562062: train_loss -0.9418
2025-10-05 18:48:18.562232: val_loss 0.1677
2025-10-05 18:48:18.562396: Pseudo dice [np.float32(0.614)]
2025-10-05 18:48:18.562538: Epoch time: 46.79 s
2025-10-05 18:48:19.723294: 
2025-10-05 18:48:19.723579: Epoch 140
2025-10-05 18:48:19.723846: Current learning rate: 0.00087
2025-10-05 18:49:06.413840: Validation loss did not improve from -0.21229. Patience: 134/50
2025-10-05 18:49:06.414609: train_loss -0.9433
2025-10-05 18:49:06.414787: val_loss 0.1223
2025-10-05 18:49:06.414928: Pseudo dice [np.float32(0.6112)]
2025-10-05 18:49:06.415064: Epoch time: 46.69 s
2025-10-05 18:49:07.064085: 
2025-10-05 18:49:07.064545: Epoch 141
2025-10-05 18:49:07.064737: Current learning rate: 0.00079
2025-10-05 18:49:53.705858: Validation loss did not improve from -0.21229. Patience: 135/50
2025-10-05 18:49:53.706566: train_loss -0.9425
2025-10-05 18:49:53.706937: val_loss 0.1465
2025-10-05 18:49:53.707197: Pseudo dice [np.float32(0.607)]
2025-10-05 18:49:53.707497: Epoch time: 46.64 s
2025-10-05 18:49:54.362351: 
2025-10-05 18:49:54.362661: Epoch 142
2025-10-05 18:49:54.362937: Current learning rate: 0.00071
2025-10-05 18:50:41.060337: Validation loss did not improve from -0.21229. Patience: 136/50
2025-10-05 18:50:41.061138: train_loss -0.9431
2025-10-05 18:50:41.061316: val_loss 0.1158
2025-10-05 18:50:41.061497: Pseudo dice [np.float32(0.5978)]
2025-10-05 18:50:41.061683: Epoch time: 46.7 s
2025-10-05 18:50:41.718828: 
2025-10-05 18:50:41.719249: Epoch 143
2025-10-05 18:50:41.719487: Current learning rate: 0.00063
2025-10-05 18:51:28.469741: Validation loss did not improve from -0.21229. Patience: 137/50
2025-10-05 18:51:28.470260: train_loss -0.943
2025-10-05 18:51:28.470407: val_loss 0.137
2025-10-05 18:51:28.470581: Pseudo dice [np.float32(0.6168)]
2025-10-05 18:51:28.470747: Epoch time: 46.75 s
2025-10-05 18:51:29.123023: 
2025-10-05 18:51:29.123358: Epoch 144
2025-10-05 18:51:29.123570: Current learning rate: 0.00055
2025-10-05 18:52:15.826909: Validation loss did not improve from -0.21229. Patience: 138/50
2025-10-05 18:52:15.827584: train_loss -0.9433
2025-10-05 18:52:15.827732: val_loss 0.1389
2025-10-05 18:52:15.827884: Pseudo dice [np.float32(0.6158)]
2025-10-05 18:52:15.828040: Epoch time: 46.71 s
2025-10-05 18:52:16.942917: 
2025-10-05 18:52:16.943265: Epoch 145
2025-10-05 18:52:16.943458: Current learning rate: 0.00047
2025-10-05 18:53:03.658865: Validation loss did not improve from -0.21229. Patience: 139/50
2025-10-05 18:53:03.659534: train_loss -0.9445
2025-10-05 18:53:03.659730: val_loss 0.1248
2025-10-05 18:53:03.659964: Pseudo dice [np.float32(0.6125)]
2025-10-05 18:53:03.660208: Epoch time: 46.72 s
2025-10-05 18:53:04.321559: 
2025-10-05 18:53:04.321903: Epoch 146
2025-10-05 18:53:04.322120: Current learning rate: 0.00038
2025-10-05 18:53:50.975082: Validation loss did not improve from -0.21229. Patience: 140/50
2025-10-05 18:53:50.975749: train_loss -0.9439
2025-10-05 18:53:50.975886: val_loss 0.123
2025-10-05 18:53:50.975990: Pseudo dice [np.float32(0.6151)]
2025-10-05 18:53:50.976112: Epoch time: 46.65 s
2025-10-05 18:53:51.624460: 
2025-10-05 18:53:51.624756: Epoch 147
2025-10-05 18:53:51.624948: Current learning rate: 0.0003
2025-10-05 18:54:38.485425: Validation loss did not improve from -0.21229. Patience: 141/50
2025-10-05 18:54:38.485978: train_loss -0.9433
2025-10-05 18:54:38.486158: val_loss 0.1264
2025-10-05 18:54:38.486269: Pseudo dice [np.float32(0.6139)]
2025-10-05 18:54:38.486490: Epoch time: 46.86 s
2025-10-05 18:54:39.144801: 
2025-10-05 18:54:39.145038: Epoch 148
2025-10-05 18:54:39.145223: Current learning rate: 0.00021
2025-10-05 18:55:25.905800: Validation loss did not improve from -0.21229. Patience: 142/50
2025-10-05 18:55:25.906824: train_loss -0.9435
2025-10-05 18:55:25.907149: val_loss 0.2097
2025-10-05 18:55:25.907344: Pseudo dice [np.float32(0.6009)]
2025-10-05 18:55:25.907531: Epoch time: 46.76 s
2025-10-05 18:55:26.578764: 
2025-10-05 18:55:26.579149: Epoch 149
2025-10-05 18:55:26.579405: Current learning rate: 0.00011
2025-10-05 18:56:13.476120: Validation loss did not improve from -0.21229. Patience: 143/50
2025-10-05 18:56:13.476645: train_loss -0.9441
2025-10-05 18:56:13.476817: val_loss 0.1373
2025-10-05 18:56:13.476955: Pseudo dice [np.float32(0.6126)]
2025-10-05 18:56:13.477107: Epoch time: 46.9 s
2025-10-05 18:56:14.593486: Training done.
2025-10-05 18:56:14.655173: Using splits from existing split file: /nfs/erelab001/shared/Computational_Group/Naravich/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset310_nnInteractive_Calcium_OCT_CrossValidation/splits_final_20.json
2025-10-05 18:56:14.656429: The split file contains 5 splits.
2025-10-05 18:56:14.656903: Desired fold for training: 2
2025-10-05 18:56:14.657546: This split has 1 training and 7 validation cases.
2025-10-05 18:56:14.658118: predicting 101-019
2025-10-05 18:56:14.661753: 101-019, shape torch.Size([1, 375, 498, 498]), rank 0
2025-10-05 18:57:02.551391: predicting 101-044
2025-10-05 18:57:02.564751: 101-044, shape torch.Size([1, 404, 498, 498]), rank 0
2025-10-05 18:57:39.740811: predicting 101-045
2025-10-05 18:57:39.753555: 101-045, shape torch.Size([1, 375, 498, 498]), rank 0
2025-10-05 18:58:14.274762: predicting 106-002
2025-10-05 18:58:14.288138: 106-002, shape torch.Size([1, 539, 498, 498]), rank 0
2025-10-05 18:59:03.257623: predicting 401-004
2025-10-05 18:59:03.275113: 401-004, shape torch.Size([1, 375, 498, 498]), rank 0
2025-10-05 18:59:37.904923: predicting 704-003
2025-10-05 18:59:37.917588: 704-003, shape torch.Size([1, 375, 498, 498]), rank 0
2025-10-05 19:00:12.735310: predicting 706-005
2025-10-05 19:00:12.748276: 706-005, shape torch.Size([1, 375, 498, 498]), rank 0
2025-10-05 19:01:01.354871: Validation complete
2025-10-05 19:01:01.355173: Mean Validation Dice:  0.547580920340108
Finished training fold 2 saving to /nfs/erelab001/shared/Computational_Group/Naravich/datasets/nnUNet_Datasets/nnUNet_results/Dataset310_nnInteractive_Calcium_OCT_CrossValidation/nnUNetTrainerScaleAnalysis20__nnUNetPlans__3d_32x160x128_b10/fold_2_No_Pretrained
