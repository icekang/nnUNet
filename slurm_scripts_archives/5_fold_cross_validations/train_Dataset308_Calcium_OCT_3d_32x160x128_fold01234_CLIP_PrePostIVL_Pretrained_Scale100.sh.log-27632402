/home/gridsan/nchutisilp/.conda/envs/nnunet/bin/python
Starting training with CONFIG=3d_32x160x128_b10, DATASET_ID=308, TRAINER=nnUNetTrainer

############################
INFO: You are using the old nnU-Net default plans. We have updated our recommendations. Please consider using those instead! Read more here: https://github.com/MIC-DKFZ/nnUNet/blob/master/documentation/resenc_presets.md
############################

Using device: cuda:0

#######################################################################
Please cite the following paper when using nnU-Net:
Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.
#######################################################################

2024-12-19 08:49:45.885352: Using torch.compile...

############################
INFO: You are using the old nnU-Net default plans. We have updated our recommendations. Please consider using those instead! Read more here: https://github.com/MIC-DKFZ/nnUNet/blob/master/documentation/resenc_presets.md
############################

Using device: cuda:0

#######################################################################
Please cite the following paper when using nnU-Net:
Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.
#######################################################################

2024-12-19 08:49:45.858051: Using torch.compile...
/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
################### Loading pretrained weights from file  /home/gridsan/nchutisilp/projects/OpenAI-CLIP/clip_pretrained_nnUNet.pt ###################
Below is the list of overlapping blocks in pretrained model and nnUNet architecture:
encoder.stages.0.0.convs.0.conv.weight shape torch.Size([32, 1, 3, 3, 3])
encoder.stages.0.0.convs.0.conv.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.norm.weight shape torch.Size([32])
encoder.stages.0.0.convs.0.norm.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.0.weight shape torch.Size([32, 1, 3, 3, 3])
encoder.stages.0.0.convs.0.all_modules.0.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.1.weight shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.1.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
encoder.stages.0.0.convs.1.conv.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.norm.weight shape torch.Size([32])
encoder.stages.0.0.convs.1.norm.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
encoder.stages.0.0.convs.1.all_modules.0.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.1.weight shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.1.bias shape torch.Size([32])
encoder.stages.1.0.convs.0.conv.weight shape torch.Size([64, 32, 3, 3, 3])
encoder.stages.1.0.convs.0.conv.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.norm.weight shape torch.Size([64])
encoder.stages.1.0.convs.0.norm.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.0.weight shape torch.Size([64, 32, 3, 3, 3])
encoder.stages.1.0.convs.0.all_modules.0.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.1.weight shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.1.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
encoder.stages.1.0.convs.1.conv.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.norm.weight shape torch.Size([64])
encoder.stages.1.0.convs.1.norm.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
encoder.stages.1.0.convs.1.all_modules.0.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.1.weight shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.1.bias shape torch.Size([64])
encoder.stages.2.0.convs.0.conv.weight shape torch.Size([128, 64, 3, 3, 3])
encoder.stages.2.0.convs.0.conv.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.norm.weight shape torch.Size([128])
encoder.stages.2.0.convs.0.norm.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.0.weight shape torch.Size([128, 64, 3, 3, 3])
encoder.stages.2.0.convs.0.all_modules.0.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.1.weight shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.1.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
encoder.stages.2.0.convs.1.conv.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.norm.weight shape torch.Size([128])
encoder.stages.2.0.convs.1.norm.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
encoder.stages.2.0.convs.1.all_modules.0.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.1.weight shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.1.bias shape torch.Size([128])
encoder.stages.3.0.convs.0.conv.weight shape torch.Size([256, 128, 3, 3, 3])
encoder.stages.3.0.convs.0.conv.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.norm.weight shape torch.Size([256])
encoder.stages.3.0.convs.0.norm.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.0.weight shape torch.Size([256, 128, 3, 3, 3])
encoder.stages.3.0.convs.0.all_modules.0.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.1.weight shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.1.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
encoder.stages.3.0.convs.1.conv.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.norm.weight shape torch.Size([256])
encoder.stages.3.0.convs.1.norm.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
encoder.stages.3.0.convs.1.all_modules.0.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.1.weight shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.1.bias shape torch.Size([256])
encoder.stages.4.0.convs.0.conv.weight shape torch.Size([320, 256, 3, 3, 3])
encoder.stages.4.0.convs.0.conv.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.norm.weight shape torch.Size([320])
encoder.stages.4.0.convs.0.norm.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.0.weight shape torch.Size([320, 256, 3, 3, 3])
encoder.stages.4.0.convs.0.all_modules.0.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.1.weight shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.1.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.4.0.convs.1.conv.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.norm.weight shape torch.Size([320])
encoder.stages.4.0.convs.1.norm.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.4.0.convs.1.all_modules.0.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.1.weight shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.1.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.0.conv.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.norm.weight shape torch.Size([320])
encoder.stages.5.0.convs.0.norm.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.0.all_modules.0.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.1.weight shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.1.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.1.conv.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.norm.weight shape torch.Size([320])
encoder.stages.5.0.convs.1.norm.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.1.all_modules.0.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.1.weight shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.0.0.convs.0.conv.weight shape torch.Size([32, 1, 3, 3, 3])
decoder.encoder.stages.0.0.convs.0.conv.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.norm.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.norm.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.0.weight shape torch.Size([32, 1, 3, 3, 3])
decoder.encoder.stages.0.0.convs.0.all_modules.0.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.1.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.1.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.encoder.stages.0.0.convs.1.conv.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.norm.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.norm.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.encoder.stages.0.0.convs.1.all_modules.0.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.1.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.1.bias shape torch.Size([32])
decoder.encoder.stages.1.0.convs.0.conv.weight shape torch.Size([64, 32, 3, 3, 3])
decoder.encoder.stages.1.0.convs.0.conv.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.norm.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.norm.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.0.weight shape torch.Size([64, 32, 3, 3, 3])
decoder.encoder.stages.1.0.convs.0.all_modules.0.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.1.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.1.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.encoder.stages.1.0.convs.1.conv.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.norm.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.norm.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.encoder.stages.1.0.convs.1.all_modules.0.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.1.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.1.bias shape torch.Size([64])
decoder.encoder.stages.2.0.convs.0.conv.weight shape torch.Size([128, 64, 3, 3, 3])
decoder.encoder.stages.2.0.convs.0.conv.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.norm.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.norm.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.0.weight shape torch.Size([128, 64, 3, 3, 3])
decoder.encoder.stages.2.0.convs.0.all_modules.0.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.1.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.1.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.encoder.stages.2.0.convs.1.conv.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.norm.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.norm.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.encoder.stages.2.0.convs.1.all_modules.0.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.1.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.1.bias shape torch.Size([128])
decoder.encoder.stages.3.0.convs.0.conv.weight shape torch.Size([256, 128, 3, 3, 3])
decoder.encoder.stages.3.0.convs.0.conv.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.norm.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.norm.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.0.weight shape torch.Size([256, 128, 3, 3, 3])
decoder.encoder.stages.3.0.convs.0.all_modules.0.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.1.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.1.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.encoder.stages.3.0.convs.1.conv.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.norm.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.norm.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.encoder.stages.3.0.convs.1.all_modules.0.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.1.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.1.bias shape torch.Size([256])
decoder.encoder.stages.4.0.convs.0.conv.weight shape torch.Size([320, 256, 3, 3, 3])
decoder.encoder.stages.4.0.convs.0.conv.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.norm.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.norm.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.0.weight shape torch.Size([320, 256, 3, 3, 3])
decoder.encoder.stages.4.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.4.0.convs.1.conv.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.norm.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.norm.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.4.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.0.conv.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.norm.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.norm.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.1.conv.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.norm.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.norm.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.stages.0.convs.0.conv.weight shape torch.Size([320, 640, 3, 3, 3])
decoder.stages.0.convs.0.conv.bias shape torch.Size([320])
decoder.stages.0.convs.0.norm.weight shape torch.Size([320])
decoder.stages.0.convs.0.norm.bias shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.0.weight shape torch.Size([320, 640, 3, 3, 3])
decoder.stages.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.stages.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.stages.0.convs.1.conv.bias shape torch.Size([320])
decoder.stages.0.convs.1.norm.weight shape torch.Size([320])
decoder.stages.0.convs.1.norm.bias shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.stages.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.stages.1.convs.0.conv.weight shape torch.Size([256, 512, 3, 3, 3])
decoder.stages.1.convs.0.conv.bias shape torch.Size([256])
decoder.stages.1.convs.0.norm.weight shape torch.Size([256])
decoder.stages.1.convs.0.norm.bias shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.0.weight shape torch.Size([256, 512, 3, 3, 3])
decoder.stages.1.convs.0.all_modules.0.bias shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.1.weight shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.1.bias shape torch.Size([256])
decoder.stages.1.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.stages.1.convs.1.conv.bias shape torch.Size([256])
decoder.stages.1.convs.1.norm.weight shape torch.Size([256])
decoder.stages.1.convs.1.norm.bias shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.stages.1.convs.1.all_modules.0.bias shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.1.weight shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.1.bias shape torch.Size([256])
decoder.stages.2.convs.0.conv.weight shape torch.Size([128, 256, 3, 3, 3])
decoder.stages.2.convs.0.conv.bias shape torch.Size([128])
decoder.stages.2.convs.0.norm.weight shape torch.Size([128])
decoder.stages.2.convs.0.norm.bias shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.0.weight shape torch.Size([128, 256, 3, 3, 3])
decoder.stages.2.convs.0.all_modules.0.bias shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.1.weight shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.1.bias shape torch.Size([128])
decoder.stages.2.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.stages.2.convs.1.conv.bias shape torch.Size([128])
decoder.stages.2.convs.1.norm.weight shape torch.Size([128])
decoder.stages.2.convs.1.norm.bias shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.stages.2.convs.1.all_modules.0.bias shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.1.weight shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.1.bias shape torch.Size([128])
decoder.stages.3.convs.0.conv.weight shape torch.Size([64, 128, 3, 3, 3])
decoder.stages.3.convs.0.conv.bias shape torch.Size([64])
decoder.stages.3.convs.0.norm.weight shape torch.Size([64])
decoder.stages.3.convs.0.norm.bias shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.0.weight shape torch.Size([64, 128, 3, 3, 3])
decoder.stages.3.convs.0.all_modules.0.bias shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.1.weight shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.1.bias shape torch.Size([64])
decoder.stages.3.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.stages.3.convs.1.conv.bias shape torch.Size([64])
decoder.stages.3.convs.1.norm.weight shape torch.Size([64])
decoder.stages.3.convs.1.norm.bias shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.stages.3.convs.1.all_modules.0.bias shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.1.weight shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.1.bias shape torch.Size([64])
decoder.stages.4.convs.0.conv.weight shape torch.Size([32, 64, 3, 3, 3])
decoder.stages.4.convs.0.conv.bias shape torch.Size([32])
decoder.stages.4.convs.0.norm.weight shape torch.Size([32])
decoder.stages.4.convs.0.norm.bias shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.0.weight shape torch.Size([32, 64, 3, 3, 3])
decoder.stages.4.convs.0.all_modules.0.bias shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.1.weight shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.1.bias shape torch.Size([32])
decoder.stages.4.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.stages.4.convs.1.conv.bias shape torch.Size([32])
decoder.stages.4.convs.1.norm.weight shape torch.Size([32])
decoder.stages.4.convs.1.norm.bias shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.stages.4.convs.1.all_modules.0.bias shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.1.weight shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.1.bias shape torch.Size([32])
decoder.transpconvs.0.weight shape torch.Size([320, 320, 1, 2, 2])
decoder.transpconvs.0.bias shape torch.Size([320])
decoder.transpconvs.1.weight shape torch.Size([320, 256, 2, 2, 2])
decoder.transpconvs.1.bias shape torch.Size([256])
decoder.transpconvs.2.weight shape torch.Size([256, 128, 2, 2, 2])
decoder.transpconvs.2.bias shape torch.Size([128])
decoder.transpconvs.3.weight shape torch.Size([128, 64, 2, 2, 2])
decoder.transpconvs.3.bias shape torch.Size([64])
decoder.transpconvs.4.weight shape torch.Size([64, 32, 2, 2, 2])
decoder.transpconvs.4.bias shape torch.Size([32])
################### Done ###################
2024-12-19 08:49:48.894999: do_dummy_2d_data_aug: True
2024-12-19 08:49:49.066845: Using splits from existing split file: /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset308_Sohee_Calcium_OCT_CrossValidation/splits_final.json
2024-12-19 08:49:49.081158: The split file contains 5 splits.
2024-12-19 08:49:49.083544: Desired fold for training: 1
2024-12-19 08:49:49.085166: This split has 6 training and 2 validation cases.
################### Loading pretrained weights from file  /home/gridsan/nchutisilp/projects/OpenAI-CLIP/clip_pretrained_nnUNet.pt ###################
Below is the list of overlapping blocks in pretrained model and nnUNet architecture:
encoder.stages.0.0.convs.0.conv.weight shape torch.Size([32, 1, 3, 3, 3])
encoder.stages.0.0.convs.0.conv.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.norm.weight shape torch.Size([32])
encoder.stages.0.0.convs.0.norm.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.0.weight shape torch.Size([32, 1, 3, 3, 3])
encoder.stages.0.0.convs.0.all_modules.0.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.1.weight shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.1.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
encoder.stages.0.0.convs.1.conv.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.norm.weight shape torch.Size([32])
encoder.stages.0.0.convs.1.norm.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
encoder.stages.0.0.convs.1.all_modules.0.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.1.weight shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.1.bias shape torch.Size([32])
encoder.stages.1.0.convs.0.conv.weight shape torch.Size([64, 32, 3, 3, 3])
encoder.stages.1.0.convs.0.conv.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.norm.weight shape torch.Size([64])
encoder.stages.1.0.convs.0.norm.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.0.weight shape torch.Size([64, 32, 3, 3, 3])
encoder.stages.1.0.convs.0.all_modules.0.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.1.weight shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.1.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
encoder.stages.1.0.convs.1.conv.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.norm.weight shape torch.Size([64])
encoder.stages.1.0.convs.1.norm.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
encoder.stages.1.0.convs.1.all_modules.0.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.1.weight shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.1.bias shape torch.Size([64])
encoder.stages.2.0.convs.0.conv.weight shape torch.Size([128, 64, 3, 3, 3])
encoder.stages.2.0.convs.0.conv.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.norm.weight shape torch.Size([128])
encoder.stages.2.0.convs.0.norm.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.0.weight shape torch.Size([128, 64, 3, 3, 3])
encoder.stages.2.0.convs.0.all_modules.0.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.1.weight shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.1.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
encoder.stages.2.0.convs.1.conv.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.norm.weight shape torch.Size([128])
encoder.stages.2.0.convs.1.norm.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
encoder.stages.2.0.convs.1.all_modules.0.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.1.weight shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.1.bias shape torch.Size([128])
encoder.stages.3.0.convs.0.conv.weight shape torch.Size([256, 128, 3, 3, 3])
encoder.stages.3.0.convs.0.conv.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.norm.weight shape torch.Size([256])
encoder.stages.3.0.convs.0.norm.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.0.weight shape torch.Size([256, 128, 3, 3, 3])
encoder.stages.3.0.convs.0.all_modules.0.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.1.weight shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.1.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
encoder.stages.3.0.convs.1.conv.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.norm.weight shape torch.Size([256])
encoder.stages.3.0.convs.1.norm.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
encoder.stages.3.0.convs.1.all_modules.0.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.1.weight shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.1.bias shape torch.Size([256])
encoder.stages.4.0.convs.0.conv.weight shape torch.Size([320, 256, 3, 3, 3])
encoder.stages.4.0.convs.0.conv.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.norm.weight shape torch.Size([320])
encoder.stages.4.0.convs.0.norm.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.0.weight shape torch.Size([320, 256, 3, 3, 3])
encoder.stages.4.0.convs.0.all_modules.0.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.1.weight shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.1.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.4.0.convs.1.conv.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.norm.weight shape torch.Size([320])
encoder.stages.4.0.convs.1.norm.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.4.0.convs.1.all_modules.0.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.1.weight shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.1.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.0.conv.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.norm.weight shape torch.Size([320])
encoder.stages.5.0.convs.0.norm.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.0.all_modules.0.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.1.weight shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.1.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.1.conv.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.norm.weight shape torch.Size([320])
encoder.stages.5.0.convs.1.norm.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.1.all_modules.0.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.1.weight shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.0.0.convs.0.conv.weight shape torch.Size([32, 1, 3, 3, 3])
decoder.encoder.stages.0.0.convs.0.conv.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.norm.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.norm.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.0.weight shape torch.Size([32, 1, 3, 3, 3])
decoder.encoder.stages.0.0.convs.0.all_modules.0.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.1.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.1.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.encoder.stages.0.0.convs.1.conv.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.norm.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.norm.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.encoder.stages.0.0.convs.1.all_modules.0.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.1.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.1.bias shape torch.Size([32])
decoder.encoder.stages.1.0.convs.0.conv.weight shape torch.Size([64, 32, 3, 3, 3])
decoder.encoder.stages.1.0.convs.0.conv.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.norm.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.norm.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.0.weight shape torch.Size([64, 32, 3, 3, 3])
decoder.encoder.stages.1.0.convs.0.all_modules.0.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.1.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.1.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.encoder.stages.1.0.convs.1.conv.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.norm.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.norm.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.encoder.stages.1.0.convs.1.all_modules.0.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.1.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.1.bias shape torch.Size([64])
decoder.encoder.stages.2.0.convs.0.conv.weight shape torch.Size([128, 64, 3, 3, 3])
decoder.encoder.stages.2.0.convs.0.conv.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.norm.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.norm.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.0.weight shape torch.Size([128, 64, 3, 3, 3])
decoder.encoder.stages.2.0.convs.0.all_modules.0.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.1.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.1.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.encoder.stages.2.0.convs.1.conv.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.norm.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.norm.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.encoder.stages.2.0.convs.1.all_modules.0.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.1.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.1.bias shape torch.Size([128])
decoder.encoder.stages.3.0.convs.0.conv.weight shape torch.Size([256, 128, 3, 3, 3])
decoder.encoder.stages.3.0.convs.0.conv.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.norm.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.norm.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.0.weight shape torch.Size([256, 128, 3, 3, 3])
decoder.encoder.stages.3.0.convs.0.all_modules.0.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.1.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.1.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.encoder.stages.3.0.convs.1.conv.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.norm.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.norm.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.encoder.stages.3.0.convs.1.all_modules.0.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.1.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.1.bias shape torch.Size([256])
decoder.encoder.stages.4.0.convs.0.conv.weight shape torch.Size([320, 256, 3, 3, 3])
decoder.encoder.stages.4.0.convs.0.conv.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.norm.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.norm.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.0.weight shape torch.Size([320, 256, 3, 3, 3])
decoder.encoder.stages.4.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.4.0.convs.1.conv.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.norm.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.norm.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.4.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.0.conv.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.norm.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.norm.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.1.conv.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.norm.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.norm.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.stages.0.convs.0.conv.weight shape torch.Size([320, 640, 3, 3, 3])
decoder.stages.0.convs.0.conv.bias shape torch.Size([320])
decoder.stages.0.convs.0.norm.weight shape torch.Size([320])
decoder.stages.0.convs.0.norm.bias shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.0.weight shape torch.Size([320, 640, 3, 3, 3])
decoder.stages.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.stages.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.stages.0.convs.1.conv.bias shape torch.Size([320])
decoder.stages.0.convs.1.norm.weight shape torch.Size([320])
decoder.stages.0.convs.1.norm.bias shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.stages.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.stages.1.convs.0.conv.weight shape torch.Size([256, 512, 3, 3, 3])
decoder.stages.1.convs.0.conv.bias shape torch.Size([256])
decoder.stages.1.convs.0.norm.weight shape torch.Size([256])
decoder.stages.1.convs.0.norm.bias shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.0.weight shape torch.Size([256, 512, 3, 3, 3])
decoder.stages.1.convs.0.all_modules.0.bias shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.1.weight shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.1.bias shape torch.Size([256])
decoder.stages.1.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.stages.1.convs.1.conv.bias shape torch.Size([256])
decoder.stages.1.convs.1.norm.weight shape torch.Size([256])
decoder.stages.1.convs.1.norm.bias shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.stages.1.convs.1.all_modules.0.bias shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.1.weight shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.1.bias shape torch.Size([256])
decoder.stages.2.convs.0.conv.weight shape torch.Size([128, 256, 3, 3, 3])
decoder.stages.2.convs.0.conv.bias shape torch.Size([128])
decoder.stages.2.convs.0.norm.weight shape torch.Size([128])
decoder.stages.2.convs.0.norm.bias shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.0.weight shape torch.Size([128, 256, 3, 3, 3])
decoder.stages.2.convs.0.all_modules.0.bias shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.1.weight shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.1.bias shape torch.Size([128])
decoder.stages.2.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.stages.2.convs.1.conv.bias shape torch.Size([128])
decoder.stages.2.convs.1.norm.weight shape torch.Size([128])
decoder.stages.2.convs.1.norm.bias shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.stages.2.convs.1.all_modules.0.bias shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.1.weight shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.1.bias shape torch.Size([128])
decoder.stages.3.convs.0.conv.weight shape torch.Size([64, 128, 3, 3, 3])
decoder.stages.3.convs.0.conv.bias shape torch.Size([64])
decoder.stages.3.convs.0.norm.weight shape torch.Size([64])
decoder.stages.3.convs.0.norm.bias shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.0.weight shape torch.Size([64, 128, 3, 3, 3])
decoder.stages.3.convs.0.all_modules.0.bias shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.1.weight shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.1.bias shape torch.Size([64])
decoder.stages.3.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.stages.3.convs.1.conv.bias shape torch.Size([64])
decoder.stages.3.convs.1.norm.weight shape torch.Size([64])
decoder.stages.3.convs.1.norm.bias shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.stages.3.convs.1.all_modules.0.bias shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.1.weight shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.1.bias shape torch.Size([64])
decoder.stages.4.convs.0.conv.weight shape torch.Size([32, 64, 3, 3, 3])
decoder.stages.4.convs.0.conv.bias shape torch.Size([32])
decoder.stages.4.convs.0.norm.weight shape torch.Size([32])
decoder.stages.4.convs.0.norm.bias shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.0.weight shape torch.Size([32, 64, 3, 3, 3])
decoder.stages.4.convs.0.all_modules.0.bias shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.1.weight shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.1.bias shape torch.Size([32])
decoder.stages.4.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.stages.4.convs.1.conv.bias shape torch.Size([32])
decoder.stages.4.convs.1.norm.weight shape torch.Size([32])
decoder.stages.4.convs.1.norm.bias shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.stages.4.convs.1.all_modules.0.bias shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.1.weight shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.1.bias shape torch.Size([32])
decoder.transpconvs.0.weight shape torch.Size([320, 320, 1, 2, 2])
decoder.transpconvs.0.bias shape torch.Size([320])
decoder.transpconvs.1.weight shape torch.Size([320, 256, 2, 2, 2])
decoder.transpconvs.1.bias shape torch.Size([256])
decoder.transpconvs.2.weight shape torch.Size([256, 128, 2, 2, 2])
decoder.transpconvs.2.bias shape torch.Size([128])
decoder.transpconvs.3.weight shape torch.Size([128, 64, 2, 2, 2])
decoder.transpconvs.3.bias shape torch.Size([64])
decoder.transpconvs.4.weight shape torch.Size([64, 32, 2, 2, 2])
decoder.transpconvs.4.bias shape torch.Size([32])
################### Done ###################
2024-12-19 08:49:48.885668: do_dummy_2d_data_aug: True
2024-12-19 08:49:49.066872: Using splits from existing split file: /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset308_Sohee_Calcium_OCT_CrossValidation/splits_final.json
2024-12-19 08:49:49.081671: The split file contains 5 splits.
2024-12-19 08:49:49.083964: Desired fold for training: 0
2024-12-19 08:49:49.085758: This split has 6 training and 2 validation cases.
using pin_memory on device 0
using pin_memory on device 0
using pin_memory on device 0

This is the configuration used by this training:
Configuration name: 3d_32x160x128_b10
 {'data_identifier': 'nnUNetPlans_3d_fullres', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 10, 'patch_size': [32, 160, 128], 'median_image_size_in_voxels': [375.0, 498.0, 498.0], 'spacing': [1.0, 1.0, 1.0], 'normalization_schemes': ['ZScoreNormalization'], 'use_mask_for_norm': [False], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'architecture': {'network_class_name': 'dynamic_network_architectures.architectures.unet.PlainConvUNet', 'arch_kwargs': {'n_stages': 6, 'features_per_stage': [32, 64, 128, 256, 320, 320], 'conv_op': 'torch.nn.modules.conv.Conv3d', 'kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]], 'strides': [[1, 1, 1], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [1, 2, 2]], 'n_conv_per_stage': [2, 2, 2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2, 2, 2], 'conv_bias': True, 'norm_op': 'torch.nn.modules.instancenorm.InstanceNorm3d', 'norm_op_kwargs': {'eps': 1e-05, 'affine': True}, 'dropout_op': None, 'dropout_op_kwargs': None, 'nonlin': 'torch.nn.LeakyReLU', 'nonlin_kwargs': {'inplace': True}, 'deep_supervision': True}, '_kw_requires_import': ['conv_op', 'norm_op', 'dropout_op', 'nonlin']}, 'batch_dice': True, 'inherits_from': '3d_fullres'} 

These are the global plan.json settings:
 {'dataset_name': 'Dataset308_Sohee_Calcium_OCT_CrossValidation', 'plans_name': 'nnUNetPlans', 'original_median_spacing_after_transp': [1.0, 1.0, 1.0], 'original_median_shape_after_transp': [375, 498, 498], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 0.4977976381778717, 'mean': 0.13507525622844696, 'median': 0.09599608182907104, 'min': 0.0, 'percentile_00_5': 0.014754901640117168, 'percentile_99_5': 0.4977976381778717, 'std': 0.12025152146816254}}} 

2024-12-19 08:49:57.898820: unpacking dataset...
using pin_memory on device 0

This is the configuration used by this training:
Configuration name: 3d_32x160x128_b10
 {'data_identifier': 'nnUNetPlans_3d_fullres', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 10, 'patch_size': [32, 160, 128], 'median_image_size_in_voxels': [375.0, 498.0, 498.0], 'spacing': [1.0, 1.0, 1.0], 'normalization_schemes': ['ZScoreNormalization'], 'use_mask_for_norm': [False], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'architecture': {'network_class_name': 'dynamic_network_architectures.architectures.unet.PlainConvUNet', 'arch_kwargs': {'n_stages': 6, 'features_per_stage': [32, 64, 128, 256, 320, 320], 'conv_op': 'torch.nn.modules.conv.Conv3d', 'kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]], 'strides': [[1, 1, 1], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [1, 2, 2]], 'n_conv_per_stage': [2, 2, 2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2, 2, 2], 'conv_bias': True, 'norm_op': 'torch.nn.modules.instancenorm.InstanceNorm3d', 'norm_op_kwargs': {'eps': 1e-05, 'affine': True}, 'dropout_op': None, 'dropout_op_kwargs': None, 'nonlin': 'torch.nn.LeakyReLU', 'nonlin_kwargs': {'inplace': True}, 'deep_supervision': True}, '_kw_requires_import': ['conv_op', 'norm_op', 'dropout_op', 'nonlin']}, 'batch_dice': True, 'inherits_from': '3d_fullres'} 

These are the global plan.json settings:
 {'dataset_name': 'Dataset308_Sohee_Calcium_OCT_CrossValidation', 'plans_name': 'nnUNetPlans', 'original_median_spacing_after_transp': [1.0, 1.0, 1.0], 'original_median_shape_after_transp': [375, 498, 498], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 0.4977976381778717, 'mean': 0.13507525622844696, 'median': 0.09599608182907104, 'min': 0.0, 'percentile_00_5': 0.014754901640117168, 'percentile_99_5': 0.4977976381778717, 'std': 0.12025152146816254}}} 

2024-12-19 08:49:58.202761: unpacking dataset...
2024-12-19 08:50:02.515487: unpacking done...
2024-12-19 08:50:02.789759: Unable to plot network architecture: nnUNet_compile is enabled!
2024-12-19 08:50:02.847182: 
2024-12-19 08:50:02.849041: Epoch 0
2024-12-19 08:50:02.850974: Current learning rate: 0.01
2024-12-19 08:52:40.053534: Validation loss improved from 1000.00000 to -0.17621! Patience: 0/50
2024-12-19 08:52:40.054774: train_loss -0.0953
2024-12-19 08:52:40.055820: val_loss -0.1762
2024-12-19 08:52:40.056538: Pseudo dice [0.5478]
2024-12-19 08:52:40.057228: Epoch time: 157.21 s
2024-12-19 08:52:40.057974: Yayy! New best EMA pseudo Dice: 0.5478
2024-12-19 08:52:42.560163: 
2024-12-19 08:52:42.561338: Epoch 1
2024-12-19 08:52:42.562340: Current learning rate: 0.00994
2024-12-19 08:54:11.069584: Validation loss improved from -0.17621 to -0.27110! Patience: 0/50
2024-12-19 08:54:11.070585: train_loss -0.2478
2024-12-19 08:54:11.071402: val_loss -0.2711
2024-12-19 08:54:11.072032: Pseudo dice [0.624]
2024-12-19 08:54:11.072795: Epoch time: 88.51 s
2024-12-19 08:54:11.073505: Yayy! New best EMA pseudo Dice: 0.5554
2024-12-19 08:54:12.696481: 
2024-12-19 08:54:12.698546: Epoch 2
2024-12-19 08:54:12.699897: Current learning rate: 0.00988
2024-12-19 08:55:41.832388: Validation loss improved from -0.27110 to -0.30448! Patience: 0/50
2024-12-19 08:55:41.833469: train_loss -0.3016
2024-12-19 08:55:41.834532: val_loss -0.3045
2024-12-19 08:55:41.835207: Pseudo dice [0.6224]
2024-12-19 08:55:41.835854: Epoch time: 89.14 s
2024-12-19 08:55:41.836713: Yayy! New best EMA pseudo Dice: 0.5621
2024-12-19 08:55:43.482879: 
2024-12-19 08:55:43.484558: Epoch 3
2024-12-19 08:55:43.488184: Current learning rate: 0.00982
2024-12-19 08:57:12.697789: Validation loss improved from -0.30448 to -0.34964! Patience: 0/50
2024-12-19 08:57:12.698901: train_loss -0.3472
2024-12-19 08:57:12.699949: val_loss -0.3496
2024-12-19 08:57:12.700832: Pseudo dice [0.6483]
2024-12-19 08:57:12.701952: Epoch time: 89.22 s
2024-12-19 08:57:12.702961: Yayy! New best EMA pseudo Dice: 0.5707
2024-12-19 08:57:14.300771: 
2024-12-19 08:57:14.302280: Epoch 4
2024-12-19 08:57:14.303148: Current learning rate: 0.00976
2024-12-19 08:58:43.639878: Validation loss improved from -0.34964 to -0.36030! Patience: 0/50
2024-12-19 08:58:43.640648: train_loss -0.3786
2024-12-19 08:58:43.641558: val_loss -0.3603
2024-12-19 08:58:43.642413: Pseudo dice [0.6584]
2024-12-19 08:58:43.643168: Epoch time: 89.34 s
2024-12-19 08:58:44.156732: Yayy! New best EMA pseudo Dice: 0.5795
2024-12-19 08:58:45.783694: 
2024-12-19 08:58:45.785188: Epoch 5
2024-12-19 08:58:45.786073: Current learning rate: 0.0097
2024-12-19 09:00:15.064783: Validation loss improved from -0.36030 to -0.38477! Patience: 0/50
2024-12-19 09:00:15.065724: train_loss -0.397
2024-12-19 09:00:15.066538: val_loss -0.3848
2024-12-19 09:00:15.067303: Pseudo dice [0.6719]
2024-12-19 09:00:15.068105: Epoch time: 89.28 s
2024-12-19 09:00:15.068827: Yayy! New best EMA pseudo Dice: 0.5887
2024-12-19 09:00:16.611938: 
2024-12-19 09:00:16.613472: Epoch 6
2024-12-19 09:00:16.614184: Current learning rate: 0.00964
2024-12-19 09:01:45.855128: Validation loss did not improve from -0.38477. Patience: 1/50
2024-12-19 09:01:45.855893: train_loss -0.4083
2024-12-19 09:01:45.856850: val_loss -0.352
2024-12-19 09:01:45.857564: Pseudo dice [0.6611]
2024-12-19 09:01:45.858257: Epoch time: 89.25 s
2024-12-19 09:01:45.858903: Yayy! New best EMA pseudo Dice: 0.596
2024-12-19 09:01:47.420731: 
2024-12-19 09:01:47.422572: Epoch 7
2024-12-19 09:01:47.423463: Current learning rate: 0.00958
2024-12-19 09:03:16.517001: Validation loss improved from -0.38477 to -0.39392! Patience: 1/50
2024-12-19 09:03:16.517891: train_loss -0.4236
2024-12-19 09:03:16.518681: val_loss -0.3939
2024-12-19 09:03:16.519481: Pseudo dice [0.693]
2024-12-19 09:03:16.520231: Epoch time: 89.1 s
2024-12-19 09:03:16.520907: Yayy! New best EMA pseudo Dice: 0.6057
2024-12-19 09:03:18.464205: 
2024-12-19 09:03:18.465524: Epoch 8
2024-12-19 09:03:18.466582: Current learning rate: 0.00952
2024-12-19 09:04:47.867125: Validation loss did not improve from -0.39392. Patience: 1/50
2024-12-19 09:04:47.868231: train_loss -0.4407
2024-12-19 09:04:47.869380: val_loss -0.3876
2024-12-19 09:04:47.870086: Pseudo dice [0.6854]
2024-12-19 09:04:47.870915: Epoch time: 89.4 s
2024-12-19 09:04:47.871783: Yayy! New best EMA pseudo Dice: 0.6136
2024-12-19 09:04:49.494811: 
2024-12-19 09:04:49.496698: Epoch 9
2024-12-19 09:04:49.497828: Current learning rate: 0.00946
2024-12-19 09:06:18.896031: Validation loss did not improve from -0.39392. Patience: 2/50
2024-12-19 09:06:18.897223: train_loss -0.4678
2024-12-19 09:06:18.898390: val_loss -0.3773
2024-12-19 09:06:18.899278: Pseudo dice [0.6815]
2024-12-19 09:06:18.900161: Epoch time: 89.4 s
2024-12-19 09:06:19.236941: Yayy! New best EMA pseudo Dice: 0.6204
2024-12-19 09:06:20.779332: 
2024-12-19 09:06:20.781060: Epoch 10
2024-12-19 09:06:20.782114: Current learning rate: 0.0094
2024-12-19 09:07:50.197331: Validation loss improved from -0.39392 to -0.42840! Patience: 2/50
2024-12-19 09:07:50.198169: train_loss -0.4666
2024-12-19 09:07:50.199115: val_loss -0.4284
2024-12-19 09:07:50.199936: Pseudo dice [0.7101]
2024-12-19 09:07:50.200663: Epoch time: 89.42 s
2024-12-19 09:07:50.201343: Yayy! New best EMA pseudo Dice: 0.6294
2024-12-19 09:07:51.783349: 
2024-12-19 09:07:51.785086: Epoch 11
2024-12-19 09:07:51.786016: Current learning rate: 0.00934
2024-12-19 09:09:21.248447: Validation loss did not improve from -0.42840. Patience: 1/50
2024-12-19 09:09:21.249567: train_loss -0.4619
2024-12-19 09:09:21.250448: val_loss -0.4269
2024-12-19 09:09:21.251251: Pseudo dice [0.699]
2024-12-19 09:09:21.252115: Epoch time: 89.47 s
2024-12-19 09:09:21.253264: Yayy! New best EMA pseudo Dice: 0.6364
2024-12-19 09:09:22.781905: 
2024-12-19 09:09:22.783615: Epoch 12
2024-12-19 09:09:22.785162: Current learning rate: 0.00928
2024-12-19 09:10:52.124386: Validation loss did not improve from -0.42840. Patience: 2/50
2024-12-19 09:10:52.125583: train_loss -0.4865
2024-12-19 09:10:52.126571: val_loss -0.4027
2024-12-19 09:10:52.127659: Pseudo dice [0.6839]
2024-12-19 09:10:52.128555: Epoch time: 89.34 s
2024-12-19 09:10:52.129441: Yayy! New best EMA pseudo Dice: 0.6411
2024-12-19 09:10:53.729772: 
2024-12-19 09:10:53.731371: Epoch 13
2024-12-19 09:10:53.732263: Current learning rate: 0.00922
2024-12-19 09:12:23.113471: Validation loss improved from -0.42840 to -0.45802! Patience: 2/50
2024-12-19 09:12:23.114731: train_loss -0.4978
2024-12-19 09:12:23.115737: val_loss -0.458
2024-12-19 09:12:23.116560: Pseudo dice [0.7268]
2024-12-19 09:12:23.117461: Epoch time: 89.39 s
2024-12-19 09:12:23.118077: Yayy! New best EMA pseudo Dice: 0.6497
2024-12-19 09:12:24.712429: 
2024-12-19 09:12:24.714532: Epoch 14
2024-12-19 09:12:24.715440: Current learning rate: 0.00916
2024-12-19 09:13:54.068296: Validation loss did not improve from -0.45802. Patience: 1/50
2024-12-19 09:13:54.068995: train_loss -0.5144
2024-12-19 09:13:54.070002: val_loss -0.4507
2024-12-19 09:13:54.070888: Pseudo dice [0.7161]
2024-12-19 09:13:54.071681: Epoch time: 89.36 s
2024-12-19 09:13:54.421063: Yayy! New best EMA pseudo Dice: 0.6563
2024-12-19 09:13:55.968426: 
2024-12-19 09:13:55.970117: Epoch 15
2024-12-19 09:13:55.971092: Current learning rate: 0.0091
2024-12-19 09:15:25.295239: Validation loss did not improve from -0.45802. Patience: 2/50
2024-12-19 09:15:25.296041: train_loss -0.5137
2024-12-19 09:15:25.296918: val_loss -0.4316
2024-12-19 09:15:25.297796: Pseudo dice [0.7086]
2024-12-19 09:15:25.298801: Epoch time: 89.33 s
2024-12-19 09:15:25.299669: Yayy! New best EMA pseudo Dice: 0.6615
2024-12-19 09:15:26.851205: 
2024-12-19 09:15:26.852466: Epoch 16
2024-12-19 09:15:26.853310: Current learning rate: 0.00903
2024-12-19 09:16:56.555280: Validation loss did not improve from -0.45802. Patience: 3/50
2024-12-19 09:16:56.556143: train_loss -0.5135
2024-12-19 09:16:56.556937: val_loss -0.4461
2024-12-19 09:16:56.557699: Pseudo dice [0.7079]
2024-12-19 09:16:56.558371: Epoch time: 89.71 s
2024-12-19 09:16:56.558966: Yayy! New best EMA pseudo Dice: 0.6662
2024-12-19 09:16:58.130336: 
2024-12-19 09:16:58.132157: Epoch 17
2024-12-19 09:16:58.132973: Current learning rate: 0.00897
2024-12-19 09:18:27.830112: Validation loss improved from -0.45802 to -0.49722! Patience: 3/50
2024-12-19 09:18:27.831206: train_loss -0.5255
2024-12-19 09:18:27.832300: val_loss -0.4972
2024-12-19 09:18:27.833056: Pseudo dice [0.7425]
2024-12-19 09:18:27.833699: Epoch time: 89.7 s
2024-12-19 09:18:27.834291: Yayy! New best EMA pseudo Dice: 0.6738
2024-12-19 09:18:29.736180: 
2024-12-19 09:18:29.737872: Epoch 18
2024-12-19 09:18:29.739001: Current learning rate: 0.00891
2024-12-19 09:19:59.455238: Validation loss did not improve from -0.49722. Patience: 1/50
2024-12-19 09:19:59.456615: train_loss -0.5358
2024-12-19 09:19:59.457648: val_loss -0.4732
2024-12-19 09:19:59.458339: Pseudo dice [0.7386]
2024-12-19 09:19:59.459406: Epoch time: 89.72 s
2024-12-19 09:19:59.460237: Yayy! New best EMA pseudo Dice: 0.6803
2024-12-19 09:20:01.094548: 
2024-12-19 09:20:01.096496: Epoch 19
2024-12-19 09:20:01.097547: Current learning rate: 0.00885
2024-12-19 09:21:30.686169: Validation loss improved from -0.49722 to -0.51217! Patience: 1/50
2024-12-19 09:21:30.687141: train_loss -0.5437
2024-12-19 09:21:30.688217: val_loss -0.5122
2024-12-19 09:21:30.689032: Pseudo dice [0.7557]
2024-12-19 09:21:30.690802: Epoch time: 89.59 s
2024-12-19 09:21:31.041841: Yayy! New best EMA pseudo Dice: 0.6878
2024-12-19 09:21:32.580822: 
2024-12-19 09:21:32.583346: Epoch 20
2024-12-19 09:21:32.584113: Current learning rate: 0.00879
2024-12-19 09:23:02.200212: Validation loss did not improve from -0.51217. Patience: 1/50
2024-12-19 09:23:02.201360: train_loss -0.5431
2024-12-19 09:23:02.202297: val_loss -0.4953
2024-12-19 09:23:02.203153: Pseudo dice [0.7397]
2024-12-19 09:23:02.203878: Epoch time: 89.62 s
2024-12-19 09:23:02.204476: Yayy! New best EMA pseudo Dice: 0.693
2024-12-19 09:23:03.849278: 
2024-12-19 09:23:03.851258: Epoch 21
2024-12-19 09:23:03.852066: Current learning rate: 0.00873
2024-12-19 09:24:33.428908: Validation loss improved from -0.51217 to -0.51376! Patience: 1/50
2024-12-19 09:24:33.429812: train_loss -0.5571
2024-12-19 09:24:33.430974: val_loss -0.5138
2024-12-19 09:24:33.431912: Pseudo dice [0.7525]
2024-12-19 09:24:33.432867: Epoch time: 89.58 s
2024-12-19 09:24:33.433699: Yayy! New best EMA pseudo Dice: 0.699
2024-12-19 09:24:34.948388: 
2024-12-19 09:24:34.950354: Epoch 22
2024-12-19 09:24:34.951674: Current learning rate: 0.00867
2024-12-19 09:26:04.508893: Validation loss did not improve from -0.51376. Patience: 1/50
2024-12-19 09:26:04.509815: train_loss -0.5549
2024-12-19 09:26:04.510759: val_loss -0.496
2024-12-19 09:26:04.511581: Pseudo dice [0.7478]
2024-12-19 09:26:04.512242: Epoch time: 89.56 s
2024-12-19 09:26:04.512971: Yayy! New best EMA pseudo Dice: 0.7038
2024-12-19 09:26:06.026947: 
2024-12-19 09:26:06.028745: Epoch 23
2024-12-19 09:26:06.030139: Current learning rate: 0.00861
2024-12-19 09:27:35.619543: Validation loss did not improve from -0.51376. Patience: 2/50
2024-12-19 09:27:35.620334: train_loss -0.5627
2024-12-19 09:27:35.621145: val_loss -0.4984
2024-12-19 09:27:35.621900: Pseudo dice [0.7509]
2024-12-19 09:27:35.622522: Epoch time: 89.59 s
2024-12-19 09:27:35.623134: Yayy! New best EMA pseudo Dice: 0.7086
2024-12-19 09:27:37.126568: 
2024-12-19 09:27:37.128503: Epoch 24
2024-12-19 09:27:37.129370: Current learning rate: 0.00855
2024-12-19 09:29:06.910992: Validation loss improved from -0.51376 to -0.51947! Patience: 2/50
2024-12-19 09:29:06.911916: train_loss -0.5648
2024-12-19 09:29:06.912845: val_loss -0.5195
2024-12-19 09:29:06.913700: Pseudo dice [0.7645]
2024-12-19 09:29:06.914397: Epoch time: 89.79 s
2024-12-19 09:29:07.266350: Yayy! New best EMA pseudo Dice: 0.7142
2024-12-19 09:29:08.767464: 
2024-12-19 09:29:08.769146: Epoch 25
2024-12-19 09:29:08.769919: Current learning rate: 0.00849
2024-12-19 09:30:38.642102: Validation loss did not improve from -0.51947. Patience: 1/50
2024-12-19 09:30:38.643402: train_loss -0.5555
2024-12-19 09:30:38.644585: val_loss -0.504
2024-12-19 09:30:38.645511: Pseudo dice [0.7502]
2024-12-19 09:30:38.646435: Epoch time: 89.88 s
2024-12-19 09:30:38.647231: Yayy! New best EMA pseudo Dice: 0.7178
2024-12-19 09:30:40.200921: 
2024-12-19 09:30:40.202636: Epoch 26
2024-12-19 09:30:40.203788: Current learning rate: 0.00843
2024-12-19 09:32:10.035057: Validation loss did not improve from -0.51947. Patience: 2/50
2024-12-19 09:32:10.036239: train_loss -0.5721
2024-12-19 09:32:10.037107: val_loss -0.4717
2024-12-19 09:32:10.037904: Pseudo dice [0.7272]
2024-12-19 09:32:10.038714: Epoch time: 89.84 s
2024-12-19 09:32:10.039541: Yayy! New best EMA pseudo Dice: 0.7187
2024-12-19 09:32:11.653903: 
2024-12-19 09:32:11.655858: Epoch 27
2024-12-19 09:32:11.656953: Current learning rate: 0.00836
2024-12-19 09:33:41.230587: Validation loss did not improve from -0.51947. Patience: 3/50
2024-12-19 09:33:41.231661: train_loss -0.5747
2024-12-19 09:33:41.232552: val_loss -0.5121
2024-12-19 09:33:41.233356: Pseudo dice [0.7603]
2024-12-19 09:33:41.234183: Epoch time: 89.58 s
2024-12-19 09:33:41.234978: Yayy! New best EMA pseudo Dice: 0.7229
2024-12-19 09:33:42.782717: 
2024-12-19 09:33:42.785033: Epoch 28
2024-12-19 09:33:42.785980: Current learning rate: 0.0083
2024-12-19 09:35:12.183868: Validation loss improved from -0.51947 to -0.52141! Patience: 3/50
2024-12-19 09:35:12.184800: train_loss -0.5907
2024-12-19 09:35:12.185730: val_loss -0.5214
2024-12-19 09:35:12.186399: Pseudo dice [0.7568]
2024-12-19 09:35:12.187148: Epoch time: 89.4 s
2024-12-19 09:35:12.187751: Yayy! New best EMA pseudo Dice: 0.7263
2024-12-19 09:35:14.039243: 
2024-12-19 09:35:14.040430: Epoch 29
2024-12-19 09:35:14.041154: Current learning rate: 0.00824
2024-12-19 09:36:43.571793: Validation loss improved from -0.52141 to -0.53295! Patience: 0/50
2024-12-19 09:36:43.572795: train_loss -0.5954
2024-12-19 09:36:43.573732: val_loss -0.5329
2024-12-19 09:36:43.574509: Pseudo dice [0.7699]
2024-12-19 09:36:43.575307: Epoch time: 89.53 s
2024-12-19 09:36:43.948503: Yayy! New best EMA pseudo Dice: 0.7306
2024-12-19 09:36:45.503749: 
2024-12-19 09:36:45.505780: Epoch 30
2024-12-19 09:36:45.506909: Current learning rate: 0.00818
2024-12-19 09:38:14.977611: Validation loss did not improve from -0.53295. Patience: 1/50
2024-12-19 09:38:14.979052: train_loss -0.5951
2024-12-19 09:38:14.980337: val_loss -0.5138
2024-12-19 09:38:14.981052: Pseudo dice [0.7538]
2024-12-19 09:38:14.981958: Epoch time: 89.48 s
2024-12-19 09:38:14.982781: Yayy! New best EMA pseudo Dice: 0.7329
2024-12-19 09:38:16.575687: 
2024-12-19 09:38:16.577319: Epoch 31
2024-12-19 09:38:16.578190: Current learning rate: 0.00812
2024-12-19 09:39:46.058957: Validation loss did not improve from -0.53295. Patience: 2/50
2024-12-19 09:39:46.060011: train_loss -0.5898
2024-12-19 09:39:46.060885: val_loss -0.5173
2024-12-19 09:39:46.061634: Pseudo dice [0.7606]
2024-12-19 09:39:46.062420: Epoch time: 89.49 s
2024-12-19 09:39:46.063015: Yayy! New best EMA pseudo Dice: 0.7357
2024-12-19 09:39:47.638683: 
2024-12-19 09:39:47.640530: Epoch 32
2024-12-19 09:39:47.641526: Current learning rate: 0.00806
2024-12-19 09:41:17.215976: Validation loss did not improve from -0.53295. Patience: 3/50
2024-12-19 09:41:17.216942: train_loss -0.5875
2024-12-19 09:41:17.218064: val_loss -0.5247
2024-12-19 09:41:17.219003: Pseudo dice [0.7632]
2024-12-19 09:41:17.219969: Epoch time: 89.58 s
2024-12-19 09:41:17.221037: Yayy! New best EMA pseudo Dice: 0.7385
2024-12-19 09:41:18.791553: 
2024-12-19 09:41:18.793011: Epoch 33
2024-12-19 09:41:18.794178: Current learning rate: 0.008
2024-12-19 09:42:48.376446: Validation loss did not improve from -0.53295. Patience: 4/50
2024-12-19 09:42:48.377756: train_loss -0.6001
2024-12-19 09:42:48.378871: val_loss -0.5196
2024-12-19 09:42:48.379860: Pseudo dice [0.7576]
2024-12-19 09:42:48.380765: Epoch time: 89.59 s
2024-12-19 09:42:48.381570: Yayy! New best EMA pseudo Dice: 0.7404
2024-12-19 09:42:49.940724: 
2024-12-19 09:42:49.942183: Epoch 34
2024-12-19 09:42:49.943167: Current learning rate: 0.00793
2024-12-19 09:44:19.557441: Validation loss did not improve from -0.53295. Patience: 5/50
2024-12-19 09:44:19.558888: train_loss -0.588
2024-12-19 09:44:19.559950: val_loss -0.4987
2024-12-19 09:44:19.560717: Pseudo dice [0.7378]
2024-12-19 09:44:19.561602: Epoch time: 89.62 s
2024-12-19 09:44:21.173534: 
2024-12-19 09:44:21.175530: Epoch 35
2024-12-19 09:44:21.176352: Current learning rate: 0.00787
2024-12-19 09:45:51.008506: Validation loss did not improve from -0.53295. Patience: 6/50
2024-12-19 09:45:51.009311: train_loss -0.5892
2024-12-19 09:45:51.010046: val_loss -0.4778
2024-12-19 09:45:51.010792: Pseudo dice [0.7346]
2024-12-19 09:45:51.011642: Epoch time: 89.84 s
2024-12-19 09:45:52.278879: 
2024-12-19 09:45:52.280607: Epoch 36
2024-12-19 09:45:52.281761: Current learning rate: 0.00781
2024-12-19 09:47:22.033510: Validation loss did not improve from -0.53295. Patience: 7/50
2024-12-19 09:47:22.034810: train_loss -0.5981
2024-12-19 09:47:22.035996: val_loss -0.4745
2024-12-19 09:47:22.036938: Pseudo dice [0.746]
2024-12-19 09:47:22.037684: Epoch time: 89.76 s
2024-12-19 09:47:23.315124: 
2024-12-19 09:47:23.316681: Epoch 37
2024-12-19 09:47:23.318008: Current learning rate: 0.00775
2024-12-19 09:48:53.128184: Validation loss did not improve from -0.53295. Patience: 8/50
2024-12-19 09:48:53.128886: train_loss -0.6099
2024-12-19 09:48:53.129757: val_loss -0.5135
2024-12-19 09:48:53.130460: Pseudo dice [0.7537]
2024-12-19 09:48:53.131200: Epoch time: 89.81 s
2024-12-19 09:48:53.131885: Yayy! New best EMA pseudo Dice: 0.7416
2024-12-19 09:48:54.823039: 
2024-12-19 09:48:54.823892: Epoch 38
2024-12-19 09:48:54.824620: Current learning rate: 0.00769
2024-12-19 09:50:24.587724: Validation loss did not improve from -0.53295. Patience: 9/50
2024-12-19 09:50:24.588670: train_loss -0.6102
2024-12-19 09:50:24.589481: val_loss -0.4787
2024-12-19 09:50:24.590094: Pseudo dice [0.7221]
2024-12-19 09:50:24.590867: Epoch time: 89.77 s
2024-12-19 09:50:26.191577: 
2024-12-19 09:50:26.193212: Epoch 39
2024-12-19 09:50:26.194034: Current learning rate: 0.00763
2024-12-19 09:51:56.054413: Validation loss did not improve from -0.53295. Patience: 10/50
2024-12-19 09:51:56.055665: train_loss -0.6122
2024-12-19 09:51:56.056947: val_loss -0.4931
2024-12-19 09:51:56.057933: Pseudo dice [0.7439]
2024-12-19 09:51:56.058816: Epoch time: 89.87 s
2024-12-19 09:51:57.661641: 
2024-12-19 09:51:57.663730: Epoch 40
2024-12-19 09:51:57.664459: Current learning rate: 0.00756
2024-12-19 09:53:27.488546: Validation loss improved from -0.53295 to -0.53552! Patience: 10/50
2024-12-19 09:53:27.489610: train_loss -0.6257
2024-12-19 09:53:27.490949: val_loss -0.5355
2024-12-19 09:53:27.491894: Pseudo dice [0.7692]
2024-12-19 09:53:27.493030: Epoch time: 89.83 s
2024-12-19 09:53:27.494075: Yayy! New best EMA pseudo Dice: 0.743
2024-12-19 09:53:29.092385: 
2024-12-19 09:53:29.094329: Epoch 41
2024-12-19 09:53:29.095572: Current learning rate: 0.0075
2024-12-19 09:54:58.886055: Validation loss did not improve from -0.53552. Patience: 1/50
2024-12-19 09:54:58.887404: train_loss -0.6306
2024-12-19 09:54:58.888475: val_loss -0.5227
2024-12-19 09:54:58.889394: Pseudo dice [0.7629]
2024-12-19 09:54:58.890353: Epoch time: 89.8 s
2024-12-19 09:54:58.891518: Yayy! New best EMA pseudo Dice: 0.7449
2024-12-19 09:55:00.473023: 
2024-12-19 09:55:00.477490: Epoch 42
2024-12-19 09:55:00.478228: Current learning rate: 0.00744
2024-12-19 09:56:30.390684: Validation loss did not improve from -0.53552. Patience: 2/50
2024-12-19 09:56:30.391752: train_loss -0.6173
2024-12-19 09:56:30.392798: val_loss -0.5266
2024-12-19 09:56:30.393724: Pseudo dice [0.7652]
2024-12-19 09:56:30.394671: Epoch time: 89.92 s
2024-12-19 09:56:30.395606: Yayy! New best EMA pseudo Dice: 0.747
2024-12-19 09:56:31.959666: 
2024-12-19 09:56:31.961604: Epoch 43
2024-12-19 09:56:31.962580: Current learning rate: 0.00738
2024-12-19 09:58:01.762922: Validation loss did not improve from -0.53552. Patience: 3/50
2024-12-19 09:58:01.763998: train_loss -0.6239
2024-12-19 09:58:01.764809: val_loss -0.4626
2024-12-19 09:58:01.765597: Pseudo dice [0.7317]
2024-12-19 09:58:01.766315: Epoch time: 89.81 s
2024-12-19 09:58:02.941885: 
2024-12-19 09:58:02.943617: Epoch 44
2024-12-19 09:58:02.944625: Current learning rate: 0.00732
2024-12-19 09:59:32.730294: Validation loss did not improve from -0.53552. Patience: 4/50
2024-12-19 09:59:32.731847: train_loss -0.6262
2024-12-19 09:59:32.733227: val_loss -0.5049
2024-12-19 09:59:32.733974: Pseudo dice [0.7614]
2024-12-19 09:59:32.735005: Epoch time: 89.79 s
2024-12-19 09:59:33.113201: Yayy! New best EMA pseudo Dice: 0.747
2024-12-19 09:59:34.625915: 
2024-12-19 09:59:34.627280: Epoch 45
2024-12-19 09:59:34.628154: Current learning rate: 0.00725
2024-12-19 10:01:04.423969: Validation loss did not improve from -0.53552. Patience: 5/50
2024-12-19 10:01:04.425210: train_loss -0.6222
2024-12-19 10:01:04.426278: val_loss -0.5015
2024-12-19 10:01:04.427215: Pseudo dice [0.7512]
2024-12-19 10:01:04.427899: Epoch time: 89.8 s
2024-12-19 10:01:04.428641: Yayy! New best EMA pseudo Dice: 0.7475
2024-12-19 10:01:05.941828: 
2024-12-19 10:01:05.943621: Epoch 46
2024-12-19 10:01:05.944400: Current learning rate: 0.00719
2024-12-19 10:02:35.687636: Validation loss did not improve from -0.53552. Patience: 6/50
2024-12-19 10:02:35.688763: train_loss -0.6354
2024-12-19 10:02:35.689981: val_loss -0.4776
2024-12-19 10:02:35.691056: Pseudo dice [0.73]
2024-12-19 10:02:35.692003: Epoch time: 89.75 s
2024-12-19 10:02:36.860023: 
2024-12-19 10:02:36.861373: Epoch 47
2024-12-19 10:02:36.862356: Current learning rate: 0.00713
2024-12-19 10:04:06.733660: Validation loss did not improve from -0.53552. Patience: 7/50
2024-12-19 10:04:06.734532: train_loss -0.6409
2024-12-19 10:04:06.735448: val_loss -0.504
2024-12-19 10:04:06.736103: Pseudo dice [0.751]
2024-12-19 10:04:06.737078: Epoch time: 89.88 s
2024-12-19 10:04:07.909106: 
2024-12-19 10:04:07.910184: Epoch 48
2024-12-19 10:04:07.910922: Current learning rate: 0.00707
2024-12-19 10:05:37.703768: Validation loss improved from -0.53552 to -0.53759! Patience: 7/50
2024-12-19 10:05:37.705198: train_loss -0.6397
2024-12-19 10:05:37.706084: val_loss -0.5376
2024-12-19 10:05:37.706825: Pseudo dice [0.7739]
2024-12-19 10:05:37.707505: Epoch time: 89.8 s
2024-12-19 10:05:37.708180: Yayy! New best EMA pseudo Dice: 0.749
2024-12-19 10:05:39.281386: 
2024-12-19 10:05:39.283007: Epoch 49
2024-12-19 10:05:39.283983: Current learning rate: 0.007
2024-12-19 10:07:09.079367: Validation loss did not improve from -0.53759. Patience: 1/50
2024-12-19 10:07:09.080309: train_loss -0.6384
2024-12-19 10:07:09.081082: val_loss -0.522
2024-12-19 10:07:09.081969: Pseudo dice [0.7634]
2024-12-19 10:07:09.082626: Epoch time: 89.8 s
2024-12-19 10:07:09.440895: Yayy! New best EMA pseudo Dice: 0.7505
2024-12-19 10:07:11.387677: 
2024-12-19 10:07:11.389467: Epoch 50
2024-12-19 10:07:11.390300: Current learning rate: 0.00694
2024-12-19 10:08:41.085680: Validation loss did not improve from -0.53759. Patience: 2/50
2024-12-19 10:08:41.086785: train_loss -0.6499
2024-12-19 10:08:41.087684: val_loss -0.4914
2024-12-19 10:08:41.088525: Pseudo dice [0.7483]
2024-12-19 10:08:41.089356: Epoch time: 89.7 s
2024-12-19 10:08:42.318818: 
2024-12-19 10:08:42.320209: Epoch 51
2024-12-19 10:08:42.321099: Current learning rate: 0.00688
2024-12-19 10:10:12.303668: Validation loss improved from -0.53759 to -0.54627! Patience: 2/50
2024-12-19 10:10:12.304735: train_loss -0.6472
2024-12-19 10:10:12.305784: val_loss -0.5463
2024-12-19 10:10:12.306634: Pseudo dice [0.7779]
2024-12-19 10:10:12.307358: Epoch time: 89.99 s
2024-12-19 10:10:12.308032: Yayy! New best EMA pseudo Dice: 0.753
2024-12-19 10:10:13.850351: 
2024-12-19 10:10:13.852095: Epoch 52
2024-12-19 10:10:13.852991: Current learning rate: 0.00682
2024-12-19 10:11:43.557083: Validation loss did not improve from -0.54627. Patience: 1/50
2024-12-19 10:11:43.557909: train_loss -0.6468
2024-12-19 10:11:43.559031: val_loss -0.493
2024-12-19 10:11:43.559705: Pseudo dice [0.748]
2024-12-19 10:11:43.560475: Epoch time: 89.71 s
2024-12-19 10:11:44.764446: 
2024-12-19 10:11:44.766485: Epoch 53
2024-12-19 10:11:44.767373: Current learning rate: 0.00675
2024-12-19 10:13:14.540427: Validation loss did not improve from -0.54627. Patience: 2/50
2024-12-19 10:13:14.541193: train_loss -0.6506
2024-12-19 10:13:14.541991: val_loss -0.5034
2024-12-19 10:13:14.542631: Pseudo dice [0.7467]
2024-12-19 10:13:14.543344: Epoch time: 89.78 s
2024-12-19 10:13:15.721901: 
2024-12-19 10:13:15.723826: Epoch 54
2024-12-19 10:13:15.724882: Current learning rate: 0.00669
2024-12-19 10:14:45.459982: Validation loss improved from -0.54627 to -0.54697! Patience: 2/50
2024-12-19 10:14:45.461298: train_loss -0.655
2024-12-19 10:14:45.462895: val_loss -0.547
2024-12-19 10:14:45.463852: Pseudo dice [0.77]
2024-12-19 10:14:45.464756: Epoch time: 89.74 s
2024-12-19 10:14:45.839853: Yayy! New best EMA pseudo Dice: 0.7537
2024-12-19 10:14:47.552132: 
2024-12-19 10:14:47.553955: Epoch 55
2024-12-19 10:14:47.554807: Current learning rate: 0.00663
2024-12-19 10:16:17.012171: Validation loss did not improve from -0.54697. Patience: 1/50
2024-12-19 10:16:17.013535: train_loss -0.6593
2024-12-19 10:16:17.015165: val_loss -0.5324
2024-12-19 10:16:17.016600: Pseudo dice [0.7673]
2024-12-19 10:16:17.017728: Epoch time: 89.46 s
2024-12-19 10:16:17.018926: Yayy! New best EMA pseudo Dice: 0.7551
2024-12-19 10:16:18.583807: 
2024-12-19 10:16:18.585499: Epoch 56
2024-12-19 10:16:18.586723: Current learning rate: 0.00657
2024-12-19 10:17:48.075599: Validation loss did not improve from -0.54697. Patience: 2/50
2024-12-19 10:17:48.076510: train_loss -0.6595
2024-12-19 10:17:48.077476: val_loss -0.5254
2024-12-19 10:17:48.078319: Pseudo dice [0.7632]
2024-12-19 10:17:48.079032: Epoch time: 89.49 s
2024-12-19 10:17:48.079888: Yayy! New best EMA pseudo Dice: 0.7559
2024-12-19 10:17:49.633998: 
2024-12-19 10:17:49.635733: Epoch 57
2024-12-19 10:17:49.636523: Current learning rate: 0.0065
2024-12-19 10:19:19.133567: Validation loss did not improve from -0.54697. Patience: 3/50
2024-12-19 10:19:19.134904: train_loss -0.6648
2024-12-19 10:19:19.136081: val_loss -0.4697
2024-12-19 10:19:19.136864: Pseudo dice [0.7236]
2024-12-19 10:19:19.137638: Epoch time: 89.5 s
2024-12-19 10:19:20.362292: 
2024-12-19 10:19:20.364161: Epoch 58
2024-12-19 10:19:20.365106: Current learning rate: 0.00644
2024-12-19 10:20:49.896658: Validation loss did not improve from -0.54697. Patience: 4/50
2024-12-19 10:20:49.897689: train_loss -0.6636
2024-12-19 10:20:49.898461: val_loss -0.5074
2024-12-19 10:20:49.899116: Pseudo dice [0.7657]
2024-12-19 10:20:49.899808: Epoch time: 89.54 s
2024-12-19 10:20:51.111583: 
2024-12-19 10:20:51.113170: Epoch 59
2024-12-19 10:20:51.113898: Current learning rate: 0.00638
2024-12-19 10:22:20.669611: Validation loss did not improve from -0.54697. Patience: 5/50
2024-12-19 10:22:20.670891: train_loss -0.6651
2024-12-19 10:22:20.671727: val_loss -0.5187
2024-12-19 10:22:20.672453: Pseudo dice [0.7705]
2024-12-19 10:22:20.673176: Epoch time: 89.56 s
2024-12-19 10:22:22.243061: 
2024-12-19 10:22:22.245008: Epoch 60
2024-12-19 10:22:22.245778: Current learning rate: 0.00631
2024-12-19 10:23:51.769785: Validation loss did not improve from -0.54697. Patience: 6/50
2024-12-19 10:23:51.770703: train_loss -0.6665
2024-12-19 10:23:51.771701: val_loss -0.5146
2024-12-19 10:23:51.772710: Pseudo dice [0.7489]
2024-12-19 10:23:51.773660: Epoch time: 89.53 s
2024-12-19 10:23:53.269773: 
2024-12-19 10:23:53.271688: Epoch 61
2024-12-19 10:23:53.272610: Current learning rate: 0.00625
2024-12-19 10:25:22.781366: Validation loss did not improve from -0.54697. Patience: 7/50
2024-12-19 10:25:22.782471: train_loss -0.6638
2024-12-19 10:25:22.783600: val_loss -0.4926
2024-12-19 10:25:22.784320: Pseudo dice [0.7377]
2024-12-19 10:25:22.784978: Epoch time: 89.51 s
2024-12-19 10:25:24.002883: 
2024-12-19 10:25:24.004922: Epoch 62
2024-12-19 10:25:24.005895: Current learning rate: 0.00619
2024-12-19 10:26:53.498611: Validation loss did not improve from -0.54697. Patience: 8/50
2024-12-19 10:26:53.499931: train_loss -0.6752
2024-12-19 10:26:53.501118: val_loss -0.4879
2024-12-19 10:26:53.502357: Pseudo dice [0.7359]
2024-12-19 10:26:53.503570: Epoch time: 89.5 s
2024-12-19 10:26:54.735982: 
2024-12-19 10:26:54.737938: Epoch 63
2024-12-19 10:26:54.738804: Current learning rate: 0.00612
2024-12-19 10:28:24.459216: Validation loss did not improve from -0.54697. Patience: 9/50
2024-12-19 10:28:24.460238: train_loss -0.6723
2024-12-19 10:28:24.461130: val_loss -0.5198
2024-12-19 10:28:24.461942: Pseudo dice [0.7602]
2024-12-19 10:28:24.462692: Epoch time: 89.73 s
2024-12-19 10:28:25.660409: 
2024-12-19 10:28:25.662173: Epoch 64
2024-12-19 10:28:25.663311: Current learning rate: 0.00606
2024-12-19 10:29:55.468319: Validation loss did not improve from -0.54697. Patience: 10/50
2024-12-19 10:29:55.469145: train_loss -0.6715
2024-12-19 10:29:55.469904: val_loss -0.4803
2024-12-19 10:29:55.470635: Pseudo dice [0.7336]
2024-12-19 10:29:55.471608: Epoch time: 89.81 s
2024-12-19 10:29:57.112742: 
2024-12-19 10:29:57.115088: Epoch 65
2024-12-19 10:29:57.116194: Current learning rate: 0.006
2024-12-19 10:31:26.776371: Validation loss improved from -0.54697 to -0.54781! Patience: 10/50
2024-12-19 10:31:26.777452: train_loss -0.6767
2024-12-19 10:31:26.778563: val_loss -0.5478
2024-12-19 10:31:26.779301: Pseudo dice [0.7749]
2024-12-19 10:31:26.780009: Epoch time: 89.67 s
2024-12-19 10:31:28.007908: 
2024-12-19 10:31:28.009583: Epoch 66
2024-12-19 10:31:28.010707: Current learning rate: 0.00593
2024-12-19 10:32:57.713612: Validation loss did not improve from -0.54781. Patience: 1/50
2024-12-19 10:32:57.714719: train_loss -0.6809
2024-12-19 10:32:57.715796: val_loss -0.5147
2024-12-19 10:32:57.716622: Pseudo dice [0.7489]
2024-12-19 10:32:57.717440: Epoch time: 89.71 s
2024-12-19 10:32:58.935223: 
2024-12-19 10:32:58.937085: Epoch 67
2024-12-19 10:32:58.938166: Current learning rate: 0.00587
2024-12-19 10:34:28.619720: Validation loss did not improve from -0.54781. Patience: 2/50
2024-12-19 10:34:28.620623: train_loss -0.6787
2024-12-19 10:34:28.621621: val_loss -0.5185
2024-12-19 10:34:28.622377: Pseudo dice [0.7665]
2024-12-19 10:34:28.623036: Epoch time: 89.69 s
2024-12-19 10:34:30.942090: 
2024-12-19 10:34:30.943838: Epoch 68
2024-12-19 10:34:30.944771: Current learning rate: 0.00581
2024-12-19 10:36:00.622365: Validation loss did not improve from -0.54781. Patience: 3/50
2024-12-19 10:36:00.623275: train_loss -0.6793
2024-12-19 10:36:00.624281: val_loss -0.4665
2024-12-19 10:36:00.625165: Pseudo dice [0.731]
2024-12-19 10:36:00.626039: Epoch time: 89.68 s
2024-12-19 10:36:01.864655: 
2024-12-19 10:36:01.866071: Epoch 69
2024-12-19 10:36:01.866800: Current learning rate: 0.00574
2024-12-19 10:37:31.622169: Validation loss did not improve from -0.54781. Patience: 4/50
2024-12-19 10:37:31.623373: train_loss -0.6794
2024-12-19 10:37:31.624273: val_loss -0.542
2024-12-19 10:37:31.625014: Pseudo dice [0.7782]
2024-12-19 10:37:31.625759: Epoch time: 89.76 s
2024-12-19 10:37:33.203002: 
2024-12-19 10:37:33.205261: Epoch 70
2024-12-19 10:37:33.206160: Current learning rate: 0.00568
2024-12-19 10:39:03.024045: Validation loss did not improve from -0.54781. Patience: 5/50
2024-12-19 10:39:03.025296: train_loss -0.6863
2024-12-19 10:39:03.026440: val_loss -0.5207
2024-12-19 10:39:03.027206: Pseudo dice [0.7553]
2024-12-19 10:39:03.027885: Epoch time: 89.82 s
2024-12-19 10:39:04.276098: 
2024-12-19 10:39:04.277862: Epoch 71
2024-12-19 10:39:04.278929: Current learning rate: 0.00562
2024-12-19 10:40:33.891762: Validation loss did not improve from -0.54781. Patience: 6/50
2024-12-19 10:40:33.893273: train_loss -0.6862
2024-12-19 10:40:33.894239: val_loss -0.5369
2024-12-19 10:40:33.895045: Pseudo dice [0.7722]
2024-12-19 10:40:33.895813: Epoch time: 89.62 s
2024-12-19 10:40:33.896541: Yayy! New best EMA pseudo Dice: 0.7562
2024-12-19 10:40:36.243262: 
2024-12-19 10:40:36.244984: Epoch 72
2024-12-19 10:40:36.245816: Current learning rate: 0.00555
2024-12-19 10:42:05.667819: Validation loss did not improve from -0.54781. Patience: 7/50
2024-12-19 10:42:05.668610: train_loss -0.6891
2024-12-19 10:42:05.669355: val_loss -0.5412
2024-12-19 10:42:05.670100: Pseudo dice [0.7746]
2024-12-19 10:42:05.670972: Epoch time: 89.43 s
2024-12-19 10:42:05.671717: Yayy! New best EMA pseudo Dice: 0.758
2024-12-19 10:42:08.059269: 
2024-12-19 10:42:08.060993: Epoch 73
2024-12-19 10:42:08.061956: Current learning rate: 0.00549
2024-12-19 10:43:37.401193: Validation loss did not improve from -0.54781. Patience: 8/50
2024-12-19 10:43:37.402000: train_loss -0.6972
2024-12-19 10:43:37.403115: val_loss -0.4894
2024-12-19 10:43:37.404006: Pseudo dice [0.7479]
2024-12-19 10:43:37.404958: Epoch time: 89.34 s
2024-12-19 10:43:38.635864: 
2024-12-19 10:43:38.637758: Epoch 74
2024-12-19 10:43:38.638994: Current learning rate: 0.00542
2024-12-19 10:45:07.941816: Validation loss did not improve from -0.54781. Patience: 9/50
2024-12-19 10:45:07.943111: train_loss -0.6875
2024-12-19 10:45:07.944085: val_loss -0.4998
2024-12-19 10:45:07.944803: Pseudo dice [0.7523]
2024-12-19 10:45:07.945435: Epoch time: 89.31 s
2024-12-19 10:45:10.468140: 
2024-12-19 10:45:10.469933: Epoch 75
2024-12-19 10:45:10.470814: Current learning rate: 0.00536
2024-12-19 10:46:40.028640: Validation loss did not improve from -0.54781. Patience: 10/50
2024-12-19 10:46:40.029788: train_loss -0.6875
2024-12-19 10:46:40.030567: val_loss -0.5269
2024-12-19 10:46:40.031340: Pseudo dice [0.7626]
2024-12-19 10:46:40.032020: Epoch time: 89.56 s
2024-12-19 10:46:41.280622: 
2024-12-19 10:46:41.282304: Epoch 76
2024-12-19 10:46:41.283618: Current learning rate: 0.00529
2024-12-19 10:48:10.882785: Validation loss did not improve from -0.54781. Patience: 11/50
2024-12-19 10:48:10.883862: train_loss -0.6879
2024-12-19 10:48:10.884705: val_loss -0.4641
2024-12-19 10:48:10.885372: Pseudo dice [0.732]
2024-12-19 10:48:10.886188: Epoch time: 89.6 s
2024-12-19 10:48:12.234379: 
2024-12-19 10:48:12.236927: Epoch 77
2024-12-19 10:48:12.238037: Current learning rate: 0.00523
2024-12-19 10:49:41.908340: Validation loss did not improve from -0.54781. Patience: 12/50
2024-12-19 10:49:41.909492: train_loss -0.6948
2024-12-19 10:49:41.910414: val_loss -0.5402
2024-12-19 10:49:41.911199: Pseudo dice [0.77]
2024-12-19 10:49:41.912029: Epoch time: 89.68 s
2024-12-19 10:49:43.148260: 
2024-12-19 10:49:43.149671: Epoch 78
2024-12-19 10:49:43.150524: Current learning rate: 0.00517
2024-12-19 10:51:12.614197: Validation loss did not improve from -0.54781. Patience: 13/50
2024-12-19 10:51:12.615546: train_loss -0.6967
2024-12-19 10:51:12.616747: val_loss -0.4674
2024-12-19 10:51:12.617591: Pseudo dice [0.7261]
2024-12-19 10:51:12.618448: Epoch time: 89.47 s
2024-12-19 10:51:13.896334: 
2024-12-19 10:51:13.898232: Epoch 79
2024-12-19 10:51:13.899329: Current learning rate: 0.0051
2024-12-19 10:52:43.570239: Validation loss did not improve from -0.54781. Patience: 14/50
2024-12-19 10:52:43.571126: train_loss -0.7026
2024-12-19 10:52:43.572123: val_loss -0.5417
2024-12-19 10:52:43.573083: Pseudo dice [0.7699]
2024-12-19 10:52:43.574018: Epoch time: 89.68 s
2024-12-19 10:52:45.214100: 
2024-12-19 10:52:45.215915: Epoch 80
2024-12-19 10:52:45.217248: Current learning rate: 0.00504
2024-12-19 10:54:14.951419: Validation loss did not improve from -0.54781. Patience: 15/50
2024-12-19 10:54:14.952509: train_loss -0.6975
2024-12-19 10:54:14.953372: val_loss -0.5416
2024-12-19 10:54:14.954135: Pseudo dice [0.7745]
2024-12-19 10:54:14.954929: Epoch time: 89.74 s
2024-12-19 10:54:16.233203: 
2024-12-19 10:54:16.235139: Epoch 81
2024-12-19 10:54:16.236054: Current learning rate: 0.00497
2024-12-19 10:55:46.002662: Validation loss did not improve from -0.54781. Patience: 16/50
2024-12-19 10:55:46.003823: train_loss -0.7005
2024-12-19 10:55:46.004653: val_loss -0.5047
2024-12-19 10:55:46.005301: Pseudo dice [0.7528]
2024-12-19 10:55:46.005885: Epoch time: 89.77 s
2024-12-19 10:55:47.883167: 
2024-12-19 10:55:47.885031: Epoch 82
2024-12-19 10:55:47.885882: Current learning rate: 0.00491
2024-12-19 10:57:17.623481: Validation loss did not improve from -0.54781. Patience: 17/50
2024-12-19 10:57:17.624261: train_loss -0.7036
2024-12-19 10:57:17.625202: val_loss -0.5133
2024-12-19 10:57:17.626090: Pseudo dice [0.754]
2024-12-19 10:57:17.626953: Epoch time: 89.74 s
2024-12-19 10:57:18.832470: 
2024-12-19 10:57:18.834166: Epoch 83
2024-12-19 10:57:18.835023: Current learning rate: 0.00484
2024-12-19 10:58:48.606655: Validation loss did not improve from -0.54781. Patience: 18/50
2024-12-19 10:58:48.607486: train_loss -0.7122
2024-12-19 10:58:48.608315: val_loss -0.5027
2024-12-19 10:58:48.608998: Pseudo dice [0.766]
2024-12-19 10:58:48.609938: Epoch time: 89.78 s
2024-12-19 10:58:49.905546: 
2024-12-19 10:58:50.632059: Epoch 84
2024-12-19 10:58:50.633969: Current learning rate: 0.00478
2024-12-19 11:00:20.457573: Validation loss did not improve from -0.54781. Patience: 19/50
2024-12-19 11:00:20.458896: train_loss -0.7091
2024-12-19 11:00:20.460166: val_loss -0.5093
2024-12-19 11:00:20.460946: Pseudo dice [0.7569]
2024-12-19 11:00:20.461654: Epoch time: 90.55 s
2024-12-19 11:00:22.038344: 
2024-12-19 11:00:22.039976: Epoch 85
2024-12-19 11:00:22.041130: Current learning rate: 0.00471
2024-12-19 11:01:52.596425: Validation loss improved from -0.54781 to -0.56084! Patience: 19/50
2024-12-19 11:01:52.597572: train_loss -0.7062
2024-12-19 11:01:52.598419: val_loss -0.5608
2024-12-19 11:01:52.599230: Pseudo dice [0.7855]
2024-12-19 11:01:52.599981: Epoch time: 90.56 s
2024-12-19 11:01:52.600668: Yayy! New best EMA pseudo Dice: 0.76
2024-12-19 11:01:54.489341: 
2024-12-19 11:01:54.490790: Epoch 86
2024-12-19 11:01:54.491702: Current learning rate: 0.00465
2024-12-19 11:03:24.319564: Validation loss did not improve from -0.56084. Patience: 1/50
2024-12-19 11:03:24.320812: train_loss -0.7167
2024-12-19 11:03:24.321745: val_loss -0.4892
2024-12-19 11:03:24.322596: Pseudo dice [0.7458]
2024-12-19 11:03:24.323260: Epoch time: 89.83 s
2024-12-19 11:03:25.565437: 
2024-12-19 11:03:25.567223: Epoch 87
2024-12-19 11:03:25.568188: Current learning rate: 0.00458
2024-12-19 11:04:55.454847: Validation loss did not improve from -0.56084. Patience: 2/50
2024-12-19 11:04:55.456268: train_loss -0.7086
2024-12-19 11:04:55.457377: val_loss -0.4604
2024-12-19 11:04:55.458140: Pseudo dice [0.742]
2024-12-19 11:04:55.458966: Epoch time: 89.89 s
2024-12-19 11:04:56.706157: 
2024-12-19 11:04:56.707935: Epoch 88
2024-12-19 11:04:56.708731: Current learning rate: 0.00452
2024-12-19 11:06:26.445705: Validation loss did not improve from -0.56084. Patience: 3/50
2024-12-19 11:06:26.447011: train_loss -0.7149
2024-12-19 11:06:26.447757: val_loss -0.4848
2024-12-19 11:06:26.448533: Pseudo dice [0.7424]
2024-12-19 11:06:26.449326: Epoch time: 89.74 s
2024-12-19 11:06:27.664330: 
2024-12-19 11:06:27.665980: Epoch 89
2024-12-19 11:06:27.666835: Current learning rate: 0.00445
2024-12-19 11:07:57.396764: Validation loss did not improve from -0.56084. Patience: 4/50
2024-12-19 11:07:57.398064: train_loss -0.7176
2024-12-19 11:07:57.398974: val_loss -0.4578
2024-12-19 11:07:57.399820: Pseudo dice [0.7383]
2024-12-19 11:07:57.400522: Epoch time: 89.73 s
2024-12-19 11:07:59.137644: 
2024-12-19 11:07:59.138870: Epoch 90
2024-12-19 11:07:59.139868: Current learning rate: 0.00438
2024-12-19 11:09:28.920735: Validation loss did not improve from -0.56084. Patience: 5/50
2024-12-19 11:09:28.921858: train_loss -0.714
2024-12-19 11:09:28.922733: val_loss -0.4947
2024-12-19 11:09:28.923478: Pseudo dice [0.7463]
2024-12-19 11:09:28.924167: Epoch time: 89.79 s
2024-12-19 11:09:30.111125: 
2024-12-19 11:09:30.112693: Epoch 91
2024-12-19 11:09:30.113459: Current learning rate: 0.00432
2024-12-19 11:10:59.914155: Validation loss did not improve from -0.56084. Patience: 6/50
2024-12-19 11:10:59.915097: train_loss -0.7173
2024-12-19 11:10:59.916056: val_loss -0.5249
2024-12-19 11:10:59.917035: Pseudo dice [0.7663]
2024-12-19 11:10:59.918126: Epoch time: 89.81 s
2024-12-19 11:11:01.121814: 
2024-12-19 11:11:01.123221: Epoch 92
2024-12-19 11:11:01.124432: Current learning rate: 0.00425
2024-12-19 11:12:30.975536: Validation loss did not improve from -0.56084. Patience: 7/50
2024-12-19 11:12:30.978009: train_loss -0.7239
2024-12-19 11:12:30.979208: val_loss -0.4967
2024-12-19 11:12:30.980451: Pseudo dice [0.7566]
2024-12-19 11:12:30.981238: Epoch time: 89.86 s
2024-12-19 11:12:33.040188: 
2024-12-19 11:12:33.041941: Epoch 93
2024-12-19 11:12:33.042700: Current learning rate: 0.00419
2024-12-19 11:14:02.853007: Validation loss did not improve from -0.56084. Patience: 8/50
2024-12-19 11:14:02.854099: train_loss -0.7188
2024-12-19 11:14:02.855019: val_loss -0.5329
2024-12-19 11:14:02.855821: Pseudo dice [0.7754]
2024-12-19 11:14:02.856477: Epoch time: 89.81 s
2024-12-19 11:14:04.065851: 
2024-12-19 11:14:04.068114: Epoch 94
2024-12-19 11:14:04.069277: Current learning rate: 0.00412
2024-12-19 11:15:33.875208: Validation loss did not improve from -0.56084. Patience: 9/50
2024-12-19 11:15:33.875968: train_loss -0.7197
2024-12-19 11:15:33.876724: val_loss -0.5001
2024-12-19 11:15:33.877483: Pseudo dice [0.7547]
2024-12-19 11:15:33.878133: Epoch time: 89.81 s
2024-12-19 11:15:35.458766: 
2024-12-19 11:15:35.460535: Epoch 95
2024-12-19 11:15:35.461382: Current learning rate: 0.00405
2024-12-19 11:17:05.301696: Validation loss did not improve from -0.56084. Patience: 10/50
2024-12-19 11:17:05.303016: train_loss -0.7229
2024-12-19 11:17:05.304241: val_loss -0.5323
2024-12-19 11:17:05.305122: Pseudo dice [0.7666]
2024-12-19 11:17:05.305986: Epoch time: 89.85 s
2024-12-19 11:17:06.562693: 
2024-12-19 11:17:06.564183: Epoch 96
2024-12-19 11:17:06.564968: Current learning rate: 0.00399
2024-12-19 11:18:36.631879: Validation loss did not improve from -0.56084. Patience: 11/50
2024-12-19 11:18:36.632953: train_loss -0.7235
2024-12-19 11:18:36.633953: val_loss -0.5368
2024-12-19 11:18:36.634659: Pseudo dice [0.7781]
2024-12-19 11:18:36.635462: Epoch time: 90.07 s
2024-12-19 11:18:37.892411: 
2024-12-19 11:18:37.894041: Epoch 97
2024-12-19 11:18:37.895092: Current learning rate: 0.00392
2024-12-19 11:20:07.847099: Validation loss did not improve from -0.56084. Patience: 12/50
2024-12-19 11:20:07.848323: train_loss -0.7295
2024-12-19 11:20:07.849348: val_loss -0.5146
2024-12-19 11:20:07.850111: Pseudo dice [0.7563]
2024-12-19 11:20:07.850955: Epoch time: 89.96 s
2024-12-19 11:20:09.037261: 
2024-12-19 11:20:09.039205: Epoch 98
2024-12-19 11:20:09.040537: Current learning rate: 0.00385
2024-12-19 11:21:39.025561: Validation loss did not improve from -0.56084. Patience: 13/50
2024-12-19 11:21:39.026907: train_loss -0.7243
2024-12-19 11:21:39.027977: val_loss -0.5428
2024-12-19 11:21:39.028813: Pseudo dice [0.7788]
2024-12-19 11:21:39.029806: Epoch time: 89.99 s
2024-12-19 11:21:39.030615: Yayy! New best EMA pseudo Dice: 0.7612
2024-12-19 11:21:40.893935: 
2024-12-19 11:21:40.895555: Epoch 99
2024-12-19 11:21:40.896591: Current learning rate: 0.00379
2024-12-19 11:23:10.826248: Validation loss did not improve from -0.56084. Patience: 14/50
2024-12-19 11:23:10.827101: train_loss -0.7301
2024-12-19 11:23:10.827981: val_loss -0.461
2024-12-19 11:23:10.828671: Pseudo dice [0.7355]
2024-12-19 11:23:10.829374: Epoch time: 89.93 s
2024-12-19 11:23:12.389048: 
2024-12-19 11:23:12.390989: Epoch 100
2024-12-19 11:23:12.391846: Current learning rate: 0.00372
2024-12-19 11:24:42.297754: Validation loss did not improve from -0.56084. Patience: 15/50
2024-12-19 11:24:42.298981: train_loss -0.7322
2024-12-19 11:24:42.299922: val_loss -0.536
2024-12-19 11:24:42.300643: Pseudo dice [0.7793]
2024-12-19 11:24:42.301284: Epoch time: 89.91 s
2024-12-19 11:24:43.578250: 
2024-12-19 11:24:43.580006: Epoch 101
2024-12-19 11:24:43.580834: Current learning rate: 0.00365
2024-12-19 11:26:13.515980: Validation loss did not improve from -0.56084. Patience: 16/50
2024-12-19 11:26:13.517265: train_loss -0.7308
2024-12-19 11:26:13.518329: val_loss -0.4702
2024-12-19 11:26:13.519180: Pseudo dice [0.7441]
2024-12-19 11:26:13.519869: Epoch time: 89.94 s
2024-12-19 11:26:14.780807: 
2024-12-19 11:26:14.782130: Epoch 102
2024-12-19 11:26:14.782881: Current learning rate: 0.00359
2024-12-19 11:27:44.703807: Validation loss did not improve from -0.56084. Patience: 17/50
2024-12-19 11:27:44.704900: train_loss -0.7272
2024-12-19 11:27:44.706019: val_loss -0.4499
2024-12-19 11:27:44.706809: Pseudo dice [0.7343]
2024-12-19 11:27:44.707501: Epoch time: 89.93 s
2024-12-19 11:27:45.973596: 
2024-12-19 11:27:45.975047: Epoch 103
2024-12-19 11:27:45.975867: Current learning rate: 0.00352
2024-12-19 11:29:16.123430: Validation loss did not improve from -0.56084. Patience: 18/50
2024-12-19 11:29:16.124893: train_loss -0.7344
2024-12-19 11:29:16.126173: val_loss -0.4993
2024-12-19 11:29:16.127254: Pseudo dice [0.7595]
2024-12-19 11:29:16.128132: Epoch time: 90.15 s
2024-12-19 11:29:17.443470: 
2024-12-19 11:29:17.445212: Epoch 104
2024-12-19 11:29:17.446522: Current learning rate: 0.00345
2024-12-19 11:30:47.360522: Validation loss did not improve from -0.56084. Patience: 19/50
2024-12-19 11:30:47.361809: train_loss -0.7319
2024-12-19 11:30:47.362763: val_loss -0.531
2024-12-19 11:30:47.363775: Pseudo dice [0.7661]
2024-12-19 11:30:47.364554: Epoch time: 89.92 s
2024-12-19 11:30:49.473034: 
2024-12-19 11:30:49.475000: Epoch 105
2024-12-19 11:30:49.475855: Current learning rate: 0.00338
2024-12-19 11:32:19.070290: Validation loss did not improve from -0.56084. Patience: 20/50
2024-12-19 11:32:19.071508: train_loss -0.7377
2024-12-19 11:32:19.072399: val_loss -0.5163
2024-12-19 11:32:19.073266: Pseudo dice [0.7619]
2024-12-19 11:32:19.074068: Epoch time: 89.6 s
2024-12-19 11:32:20.323141: 
2024-12-19 11:32:20.325280: Epoch 106
2024-12-19 11:32:20.326191: Current learning rate: 0.00332
2024-12-19 11:33:50.069209: Validation loss did not improve from -0.56084. Patience: 21/50
2024-12-19 11:33:50.070286: train_loss -0.7362
2024-12-19 11:33:50.071324: val_loss -0.4826
2024-12-19 11:33:50.072034: Pseudo dice [0.7503]
2024-12-19 11:33:50.072845: Epoch time: 89.75 s
2024-12-19 11:33:51.334329: 
2024-12-19 11:33:51.336832: Epoch 107
2024-12-19 11:33:51.337988: Current learning rate: 0.00325
2024-12-19 11:35:21.097688: Validation loss did not improve from -0.56084. Patience: 22/50
2024-12-19 11:35:21.098985: train_loss -0.7407
2024-12-19 11:35:21.100069: val_loss -0.5188
2024-12-19 11:35:21.101084: Pseudo dice [0.765]
2024-12-19 11:35:21.102025: Epoch time: 89.77 s
2024-12-19 11:35:22.366894: 
2024-12-19 11:35:22.369118: Epoch 108
2024-12-19 11:35:22.370039: Current learning rate: 0.00318
2024-12-19 11:36:52.089638: Validation loss did not improve from -0.56084. Patience: 23/50
2024-12-19 11:36:52.090688: train_loss -0.7393
2024-12-19 11:36:52.091753: val_loss -0.5341
2024-12-19 11:36:52.092593: Pseudo dice [0.7794]
2024-12-19 11:36:52.093413: Epoch time: 89.72 s
2024-12-19 11:36:53.369479: 
2024-12-19 11:36:53.371168: Epoch 109
2024-12-19 11:36:53.372192: Current learning rate: 0.00311
2024-12-19 11:38:23.080743: Validation loss did not improve from -0.56084. Patience: 24/50
2024-12-19 11:38:23.081742: train_loss -0.7406
2024-12-19 11:38:23.082637: val_loss -0.5287
2024-12-19 11:38:23.083381: Pseudo dice [0.7783]
2024-12-19 11:38:23.084160: Epoch time: 89.71 s
2024-12-19 11:38:23.553803: Yayy! New best EMA pseudo Dice: 0.7621
2024-12-19 11:38:25.151264: 
2024-12-19 11:38:25.152734: Epoch 110
2024-12-19 11:38:25.153562: Current learning rate: 0.00304
2024-12-19 11:39:54.813245: Validation loss did not improve from -0.56084. Patience: 25/50
2024-12-19 11:39:54.814100: train_loss -0.7404
2024-12-19 11:39:54.815186: val_loss -0.4717
2024-12-19 11:39:54.816200: Pseudo dice [0.7344]
2024-12-19 11:39:54.816960: Epoch time: 89.66 s
2024-12-19 11:39:56.047755: 
2024-12-19 11:39:56.049474: Epoch 111
2024-12-19 11:39:56.050405: Current learning rate: 0.00297
2024-12-19 11:41:25.764678: Validation loss did not improve from -0.56084. Patience: 26/50
2024-12-19 11:41:25.765620: train_loss -0.7408
2024-12-19 11:41:25.766714: val_loss -0.5392
2024-12-19 11:41:25.767477: Pseudo dice [0.7734]
2024-12-19 11:41:25.768206: Epoch time: 89.72 s
2024-12-19 11:41:26.969978: 
2024-12-19 11:41:26.971897: Epoch 112
2024-12-19 11:41:26.972832: Current learning rate: 0.00291
2024-12-19 11:42:56.858980: Validation loss did not improve from -0.56084. Patience: 27/50
2024-12-19 11:42:56.860489: train_loss -0.7427
2024-12-19 11:42:56.861553: val_loss -0.5135
2024-12-19 11:42:56.862233: Pseudo dice [0.7731]
2024-12-19 11:42:56.862919: Epoch time: 89.89 s
2024-12-19 11:42:58.073009: 
2024-12-19 11:42:58.075037: Epoch 113
2024-12-19 11:42:58.076265: Current learning rate: 0.00284
2024-12-19 11:44:28.120068: Validation loss did not improve from -0.56084. Patience: 28/50
2024-12-19 11:44:28.121386: train_loss -0.7475
2024-12-19 11:44:28.122738: val_loss -0.5105
2024-12-19 11:44:28.123734: Pseudo dice [0.7581]
2024-12-19 11:44:28.125000: Epoch time: 90.05 s
2024-12-19 11:44:29.360753: 
2024-12-19 11:44:29.362625: Epoch 114
2024-12-19 11:44:29.363626: Current learning rate: 0.00277
2024-12-19 11:45:59.421258: Validation loss did not improve from -0.56084. Patience: 29/50
2024-12-19 11:45:59.422249: train_loss -0.7502
2024-12-19 11:45:59.423013: val_loss -0.4808
2024-12-19 11:45:59.423834: Pseudo dice [0.7493]
2024-12-19 11:45:59.424875: Epoch time: 90.06 s
2024-12-19 11:46:01.448248: 
2024-12-19 11:46:01.450111: Epoch 115
2024-12-19 11:46:01.451072: Current learning rate: 0.0027
2024-12-19 11:47:31.460027: Validation loss did not improve from -0.56084. Patience: 30/50
2024-12-19 11:47:31.460928: train_loss -0.7496
2024-12-19 11:47:31.461695: val_loss -0.4841
2024-12-19 11:47:31.462497: Pseudo dice [0.7515]
2024-12-19 11:47:31.463462: Epoch time: 90.01 s
2024-12-19 11:47:32.720893: 
2024-12-19 11:47:32.722814: Epoch 116
2024-12-19 11:47:32.723646: Current learning rate: 0.00263
2024-12-19 11:49:02.649765: Validation loss did not improve from -0.56084. Patience: 31/50
2024-12-19 11:49:02.650689: train_loss -0.7462
2024-12-19 11:49:02.651610: val_loss -0.4425
2024-12-19 11:49:02.652567: Pseudo dice [0.7307]
2024-12-19 11:49:02.653320: Epoch time: 89.93 s
2024-12-19 11:49:03.922913: 
2024-12-19 11:49:03.924560: Epoch 117
2024-12-19 11:49:03.925635: Current learning rate: 0.00256
2024-12-19 11:50:33.955248: Validation loss did not improve from -0.56084. Patience: 32/50
2024-12-19 11:50:33.956644: train_loss -0.7465
2024-12-19 11:50:33.958133: val_loss -0.5085
2024-12-19 11:50:33.959057: Pseudo dice [0.7634]
2024-12-19 11:50:33.959880: Epoch time: 90.03 s
2024-12-19 11:50:35.197765: 
2024-12-19 11:50:35.198841: Epoch 118
2024-12-19 11:50:35.199987: Current learning rate: 0.00249
2024-12-19 11:52:05.201844: Validation loss did not improve from -0.56084. Patience: 33/50
2024-12-19 11:52:05.203443: train_loss -0.7509
2024-12-19 11:52:05.204717: val_loss -0.499
2024-12-19 11:52:05.205452: Pseudo dice [0.7564]
2024-12-19 11:52:05.206203: Epoch time: 90.01 s
2024-12-19 11:52:06.425882: 
2024-12-19 11:52:06.427361: Epoch 119
2024-12-19 11:52:06.428152: Current learning rate: 0.00242
2024-12-19 11:53:37.130892: Validation loss did not improve from -0.56084. Patience: 34/50
2024-12-19 11:53:37.132365: train_loss -0.7475
2024-12-19 11:53:37.133614: val_loss -0.4571
2024-12-19 11:53:37.134428: Pseudo dice [0.7325]
2024-12-19 11:53:37.135664: Epoch time: 90.71 s
2024-12-19 11:53:38.729294: 
2024-12-19 11:53:38.731004: Epoch 120
2024-12-19 11:53:38.732011: Current learning rate: 0.00235
2024-12-19 11:55:08.716357: Validation loss did not improve from -0.56084. Patience: 35/50
2024-12-19 11:55:08.717552: train_loss -0.7498
2024-12-19 11:55:08.718795: val_loss -0.4765
2024-12-19 11:55:08.719841: Pseudo dice [0.7413]
2024-12-19 11:55:08.720763: Epoch time: 89.99 s
2024-12-19 11:55:10.005562: 
2024-12-19 11:55:10.007252: Epoch 121
2024-12-19 11:55:10.008105: Current learning rate: 0.00228
2024-12-19 11:56:40.059789: Validation loss did not improve from -0.56084. Patience: 36/50
2024-12-19 11:56:40.060951: train_loss -0.7547
2024-12-19 11:56:40.061873: val_loss -0.4742
2024-12-19 11:56:40.062697: Pseudo dice [0.7486]
2024-12-19 11:56:40.063464: Epoch time: 90.06 s
2024-12-19 11:56:41.334176: 
2024-12-19 11:56:41.336264: Epoch 122
2024-12-19 11:56:41.337396: Current learning rate: 0.00221
2024-12-19 11:58:11.311534: Validation loss did not improve from -0.56084. Patience: 37/50
2024-12-19 11:58:11.312217: train_loss -0.7551
2024-12-19 11:58:11.313109: val_loss -0.5041
2024-12-19 11:58:11.313835: Pseudo dice [0.7656]
2024-12-19 11:58:11.314541: Epoch time: 89.98 s
2024-12-19 11:58:12.585226: 
2024-12-19 11:58:12.586953: Epoch 123
2024-12-19 11:58:12.588060: Current learning rate: 0.00214
2024-12-19 11:59:42.449391: Validation loss did not improve from -0.56084. Patience: 38/50
2024-12-19 11:59:42.450514: train_loss -0.7532
2024-12-19 11:59:42.451479: val_loss -0.5202
2024-12-19 11:59:42.452198: Pseudo dice [0.7644]
2024-12-19 11:59:42.453066: Epoch time: 89.87 s
2024-12-19 11:59:43.751751: 
2024-12-19 11:59:43.754472: Epoch 124
2024-12-19 11:59:43.755307: Current learning rate: 0.00207
2024-12-19 12:01:13.681971: Validation loss did not improve from -0.56084. Patience: 39/50
2024-12-19 12:01:13.683005: train_loss -0.755
2024-12-19 12:01:13.684017: val_loss -0.5309
2024-12-19 12:01:13.685015: Pseudo dice [0.7776]
2024-12-19 12:01:13.686011: Epoch time: 89.93 s
2024-12-19 12:01:15.303231: 
2024-12-19 12:01:15.304849: Epoch 125
2024-12-19 12:01:15.305818: Current learning rate: 0.00199
2024-12-19 12:02:45.163487: Validation loss did not improve from -0.56084. Patience: 40/50
2024-12-19 12:02:45.164574: train_loss -0.7504
2024-12-19 12:02:45.165915: val_loss -0.5033
2024-12-19 12:02:45.166885: Pseudo dice [0.7538]
2024-12-19 12:02:45.167522: Epoch time: 89.86 s
2024-12-19 12:02:47.507358: 
2024-12-19 12:02:47.509048: Epoch 126
2024-12-19 12:02:47.509897: Current learning rate: 0.00192
2024-12-19 12:04:17.351471: Validation loss did not improve from -0.56084. Patience: 41/50
2024-12-19 12:04:17.352531: train_loss -0.756
2024-12-19 12:04:17.353595: val_loss -0.5171
2024-12-19 12:04:17.354620: Pseudo dice [0.7671]
2024-12-19 12:04:17.355586: Epoch time: 89.85 s
2024-12-19 12:04:18.620532: 
2024-12-19 12:04:18.644389: Epoch 127
2024-12-19 12:04:18.645979: Current learning rate: 0.00185
2024-12-19 12:05:53.385264: Validation loss did not improve from -0.56084. Patience: 42/50
2024-12-19 12:05:53.386569: train_loss -0.7577
2024-12-19 12:05:53.387514: val_loss -0.5397
2024-12-19 12:05:53.388315: Pseudo dice [0.7752]
2024-12-19 12:05:53.389020: Epoch time: 94.77 s
2024-12-19 12:05:54.741471: 
2024-12-19 12:05:54.743243: Epoch 128
2024-12-19 12:05:54.744383: Current learning rate: 0.00178
2024-12-19 12:07:24.686764: Validation loss did not improve from -0.56084. Patience: 43/50
2024-12-19 12:07:24.688191: train_loss -0.7559
2024-12-19 12:07:24.689212: val_loss -0.542
2024-12-19 12:07:24.690083: Pseudo dice [0.7712]
2024-12-19 12:07:24.691065: Epoch time: 89.95 s
2024-12-19 12:07:26.161358: 
2024-12-19 12:07:26.201728: Epoch 129
2024-12-19 12:07:26.202896: Current learning rate: 0.0017
2024-12-19 12:08:57.867919: Validation loss did not improve from -0.56084. Patience: 44/50
2024-12-19 12:08:57.869381: train_loss -0.7582
2024-12-19 12:08:57.870530: val_loss -0.5132
2024-12-19 12:08:57.871573: Pseudo dice [0.7712]
2024-12-19 12:08:57.872498: Epoch time: 91.71 s
2024-12-19 12:08:59.815816: 
2024-12-19 12:08:59.817274: Epoch 130
2024-12-19 12:08:59.818067: Current learning rate: 0.00163
2024-12-19 12:11:20.824872: Validation loss did not improve from -0.56084. Patience: 45/50
2024-12-19 12:11:20.826115: train_loss -0.7636
2024-12-19 12:11:20.827362: val_loss -0.5097
2024-12-19 12:11:20.828437: Pseudo dice [0.7656]
2024-12-19 12:11:20.829439: Epoch time: 141.01 s
2024-12-19 12:11:20.830273: Yayy! New best EMA pseudo Dice: 0.7623
2024-12-19 12:11:22.897171: 
2024-12-19 12:11:22.898504: Epoch 131
2024-12-19 12:11:22.899384: Current learning rate: 0.00156
2024-12-19 12:14:09.543859: Validation loss did not improve from -0.56084. Patience: 46/50
2024-12-19 12:14:09.544905: train_loss -0.7613
2024-12-19 12:14:09.545813: val_loss -0.5364
2024-12-19 12:14:09.546705: Pseudo dice [0.7803]
2024-12-19 12:14:09.547464: Epoch time: 166.65 s
2024-12-19 12:14:09.548224: Yayy! New best EMA pseudo Dice: 0.7641
2024-12-19 12:14:11.470020: 
2024-12-19 12:14:11.471445: Epoch 132
2024-12-19 12:14:11.472347: Current learning rate: 0.00148
2024-12-19 12:17:16.881960: Validation loss did not improve from -0.56084. Patience: 47/50
2024-12-19 12:17:16.884653: train_loss -0.7627
2024-12-19 12:17:16.885870: val_loss -0.5163
2024-12-19 12:17:16.886640: Pseudo dice [0.7624]
2024-12-19 12:17:16.887369: Epoch time: 185.42 s
2024-12-19 12:17:18.419152: 
2024-12-19 12:17:18.420635: Epoch 133
2024-12-19 12:17:18.421532: Current learning rate: 0.00141
2024-12-19 12:20:28.827290: Validation loss did not improve from -0.56084. Patience: 48/50
2024-12-19 12:20:28.828485: train_loss -0.7606
2024-12-19 12:20:28.829401: val_loss -0.4961
2024-12-19 12:20:28.830133: Pseudo dice [0.7622]
2024-12-19 12:20:28.831029: Epoch time: 190.41 s
2024-12-19 12:20:30.439477: 
2024-12-19 12:20:30.440952: Epoch 134
2024-12-19 12:20:30.441738: Current learning rate: 0.00133
2024-12-19 12:23:52.843137: Validation loss did not improve from -0.56084. Patience: 49/50
2024-12-19 12:23:52.844405: train_loss -0.762
2024-12-19 12:23:52.845224: val_loss -0.5008
2024-12-19 12:23:52.845896: Pseudo dice [0.7616]
2024-12-19 12:23:52.846646: Epoch time: 202.41 s
2024-12-19 12:23:54.831017: 
2024-12-19 12:23:54.832408: Epoch 135
2024-12-19 12:23:54.833331: Current learning rate: 0.00126
2024-12-19 12:27:20.935484: Validation loss did not improve from -0.56084. Patience: 50/50
2024-12-19 12:27:20.936371: train_loss -0.7619
2024-12-19 12:27:20.937353: val_loss -0.4857
2024-12-19 12:27:20.938192: Pseudo dice [0.7509]
2024-12-19 12:27:20.939104: Epoch time: 206.11 s
2024-12-19 12:27:22.438902: 
2024-12-19 12:27:22.440258: Epoch 136
2024-12-19 12:27:22.441309: Current learning rate: 0.00118
2024-12-19 12:31:00.629566: Validation loss did not improve from -0.56084. Patience: 51/50
2024-12-19 12:31:00.630505: train_loss -0.7656
2024-12-19 12:31:00.631444: val_loss -0.5004
2024-12-19 12:31:00.632185: Pseudo dice [0.7544]
2024-12-19 12:31:00.633026: Epoch time: 218.19 s
2024-12-19 12:31:02.857869: 
2024-12-19 12:31:02.859308: Epoch 137
2024-12-19 12:31:02.860256: Current learning rate: 0.00111
2024-12-19 12:34:33.834513: Validation loss did not improve from -0.56084. Patience: 52/50
2024-12-19 12:34:33.835573: train_loss -0.7593
2024-12-19 12:34:33.836462: val_loss -0.5219
2024-12-19 12:34:33.837170: Pseudo dice [0.7641]
2024-12-19 12:34:33.837884: Epoch time: 210.98 s
2024-12-19 12:34:35.303593: 
2024-12-19 12:34:35.304589: Epoch 138
2024-12-19 12:34:35.305384: Current learning rate: 0.00103
2024-12-19 12:38:19.799900: Validation loss did not improve from -0.56084. Patience: 53/50
2024-12-19 12:38:19.800927: train_loss -0.7652
2024-12-19 12:38:19.801788: val_loss -0.526
2024-12-19 12:38:19.802558: Pseudo dice [0.7757]
2024-12-19 12:38:19.803441: Epoch time: 224.5 s
2024-12-19 12:38:21.287751: 
2024-12-19 12:38:21.289356: Epoch 139
2024-12-19 12:38:21.290385: Current learning rate: 0.00095
2024-12-19 12:42:04.395322: Validation loss did not improve from -0.56084. Patience: 54/50
2024-12-19 12:42:04.396451: train_loss -0.7658
2024-12-19 12:42:04.397449: val_loss -0.5016
2024-12-19 12:42:04.398391: Pseudo dice [0.7722]
2024-12-19 12:42:04.399324: Epoch time: 223.11 s
2024-12-19 12:42:06.366472: 
2024-12-19 12:42:06.367619: Epoch 140
2024-12-19 12:42:06.368487: Current learning rate: 0.00087
2024-12-19 12:45:55.178726: Validation loss did not improve from -0.56084. Patience: 55/50
2024-12-19 12:45:55.179811: train_loss -0.765
2024-12-19 12:45:55.180866: val_loss -0.5378
2024-12-19 12:45:55.181904: Pseudo dice [0.7697]
2024-12-19 12:45:55.182772: Epoch time: 228.81 s
2024-12-19 12:45:55.183666: Yayy! New best EMA pseudo Dice: 0.7646
2024-12-19 12:45:57.315240: 
2024-12-19 12:45:57.316937: Epoch 141
2024-12-19 12:45:57.317810: Current learning rate: 0.00079
2024-12-19 12:49:46.797036: Validation loss did not improve from -0.56084. Patience: 56/50
2024-12-19 12:49:46.798094: train_loss -0.7658
2024-12-19 12:49:46.799161: val_loss -0.4848
2024-12-19 12:49:46.800174: Pseudo dice [0.7511]
2024-12-19 12:49:46.801220: Epoch time: 229.48 s
2024-12-19 12:49:48.480233: 
2024-12-19 12:49:48.481706: Epoch 142
2024-12-19 12:49:48.482787: Current learning rate: 0.00071
2024-12-19 12:53:42.322235: Validation loss did not improve from -0.56084. Patience: 57/50
2024-12-19 12:53:42.323204: train_loss -0.7674
2024-12-19 12:53:42.324423: val_loss -0.478
2024-12-19 12:53:42.325485: Pseudo dice [0.7504]
2024-12-19 12:53:42.326555: Epoch time: 233.84 s
2024-12-19 12:53:43.878714: 
2024-12-19 12:53:43.880314: Epoch 143
2024-12-19 12:53:43.881428: Current learning rate: 0.00063
2024-12-19 12:57:40.405267: Validation loss did not improve from -0.56084. Patience: 58/50
2024-12-19 12:57:40.406316: train_loss -0.7692
2024-12-19 12:57:40.407098: val_loss -0.5048
2024-12-19 12:57:40.407992: Pseudo dice [0.7544]
2024-12-19 12:57:40.408806: Epoch time: 236.53 s
2024-12-19 12:57:41.882153: 
2024-12-19 12:57:41.883601: Epoch 144
2024-12-19 12:57:41.884561: Current learning rate: 0.00055
2024-12-19 13:01:46.041701: Validation loss did not improve from -0.56084. Patience: 59/50
2024-12-19 13:01:46.042799: train_loss -0.7717
2024-12-19 13:01:46.043865: val_loss -0.5037
2024-12-19 13:01:46.044642: Pseudo dice [0.7637]
2024-12-19 13:01:46.045383: Epoch time: 244.16 s
2024-12-19 13:01:48.076061: 
2024-12-19 13:01:48.077407: Epoch 145
2024-12-19 13:01:48.078295: Current learning rate: 0.00047
2024-12-19 13:05:53.349021: Validation loss did not improve from -0.56084. Patience: 60/50
2024-12-19 13:05:53.350130: train_loss -0.7695
2024-12-19 13:05:53.350989: val_loss -0.5029
2024-12-19 13:05:53.351815: Pseudo dice [0.7635]
2024-12-19 13:05:53.352612: Epoch time: 245.28 s
2024-12-19 13:05:54.863508: 
2024-12-19 13:05:54.864970: Epoch 146
2024-12-19 13:05:54.865820: Current learning rate: 0.00038
2024-12-19 13:10:05.965749: Validation loss did not improve from -0.56084. Patience: 61/50
2024-12-19 13:10:05.966771: train_loss -0.7736
2024-12-19 13:10:05.967627: val_loss -0.4954
2024-12-19 13:10:05.968305: Pseudo dice [0.7552]
2024-12-19 13:10:05.969183: Epoch time: 251.1 s
2024-12-19 13:10:08.519469: 
2024-12-19 13:10:08.520937: Epoch 147
2024-12-19 13:10:08.521974: Current learning rate: 0.0003
2024-12-19 13:14:15.987030: Validation loss did not improve from -0.56084. Patience: 62/50
2024-12-19 13:14:15.988216: train_loss -0.7694
2024-12-19 13:14:15.989312: val_loss -0.5026
2024-12-19 13:14:15.995910: Pseudo dice [0.756]
2024-12-19 13:14:15.996994: Epoch time: 247.47 s
2024-12-19 13:14:19.792352: 
2024-12-19 13:14:19.793663: Epoch 148
2024-12-19 13:14:19.794589: Current learning rate: 0.00021
2024-12-19 13:18:13.225412: Validation loss did not improve from -0.56084. Patience: 63/50
2024-12-19 13:18:13.226451: train_loss -0.775
2024-12-19 13:18:13.227544: val_loss -0.5044
2024-12-19 13:18:13.228523: Pseudo dice [0.7656]
2024-12-19 13:18:13.229399: Epoch time: 233.44 s
2024-12-19 13:18:14.775741: 
2024-12-19 13:18:14.777431: Epoch 149
2024-12-19 13:18:14.778592: Current learning rate: 0.00011
2024-12-19 13:22:11.647926: Validation loss did not improve from -0.56084. Patience: 64/50
2024-12-19 13:22:11.648759: train_loss -0.7758
2024-12-19 13:22:11.649649: val_loss -0.5219
2024-12-19 13:22:11.650468: Pseudo dice [0.7665]
2024-12-19 13:22:11.651268: Epoch time: 236.87 s
2024-12-19 13:22:13.787896: Training done.
2024-12-19 08:50:02.336025: unpacking done...
2024-12-19 08:50:02.789146: Unable to plot network architecture: nnUNet_compile is enabled!
2024-12-19 08:50:02.846808: 
2024-12-19 08:50:02.848955: Epoch 0
2024-12-19 08:50:02.850642: Current learning rate: 0.01
2024-12-19 08:52:40.147912: Validation loss improved from 1000.00000 to -0.13973! Patience: 0/50
2024-12-19 08:52:40.150353: train_loss -0.1285
2024-12-19 08:52:40.151788: val_loss -0.1397
2024-12-19 08:52:40.152523: Pseudo dice [0.5002]
2024-12-19 08:52:40.153325: Epoch time: 157.31 s
2024-12-19 08:52:40.154032: Yayy! New best EMA pseudo Dice: 0.5002
2024-12-19 08:52:42.496889: 
2024-12-19 08:52:42.498473: Epoch 1
2024-12-19 08:52:42.499776: Current learning rate: 0.00994
2024-12-19 08:54:11.787642: Validation loss improved from -0.13973 to -0.18174! Patience: 0/50
2024-12-19 08:54:11.788789: train_loss -0.2611
2024-12-19 08:54:11.789829: val_loss -0.1817
2024-12-19 08:54:11.790475: Pseudo dice [0.5392]
2024-12-19 08:54:11.791121: Epoch time: 89.29 s
2024-12-19 08:54:11.791786: Yayy! New best EMA pseudo Dice: 0.5041
2024-12-19 08:54:13.416475: 
2024-12-19 08:54:13.418505: Epoch 2
2024-12-19 08:54:13.419297: Current learning rate: 0.00988
2024-12-19 08:55:43.324785: Validation loss improved from -0.18174 to -0.24163! Patience: 0/50
2024-12-19 08:55:43.325883: train_loss -0.3219
2024-12-19 08:55:43.326832: val_loss -0.2416
2024-12-19 08:55:43.327484: Pseudo dice [0.5586]
2024-12-19 08:55:43.328135: Epoch time: 89.91 s
2024-12-19 08:55:43.328713: Yayy! New best EMA pseudo Dice: 0.5095
2024-12-19 08:55:45.052230: 
2024-12-19 08:55:45.053775: Epoch 3
2024-12-19 08:55:45.054590: Current learning rate: 0.00982
2024-12-19 08:57:15.001528: Validation loss did not improve from -0.24163. Patience: 1/50
2024-12-19 08:57:15.002588: train_loss -0.3278
2024-12-19 08:57:15.003433: val_loss -0.2364
2024-12-19 08:57:15.004153: Pseudo dice [0.5732]
2024-12-19 08:57:15.004897: Epoch time: 89.95 s
2024-12-19 08:57:15.005583: Yayy! New best EMA pseudo Dice: 0.5159
2024-12-19 08:57:16.598571: 
2024-12-19 08:57:16.600347: Epoch 4
2024-12-19 08:57:16.601087: Current learning rate: 0.00976
2024-12-19 08:58:46.502126: Validation loss improved from -0.24163 to -0.34024! Patience: 1/50
2024-12-19 08:58:46.503396: train_loss -0.3776
2024-12-19 08:58:46.504484: val_loss -0.3402
2024-12-19 08:58:46.505185: Pseudo dice [0.6232]
2024-12-19 08:58:46.505917: Epoch time: 89.91 s
2024-12-19 08:58:46.936867: Yayy! New best EMA pseudo Dice: 0.5266
2024-12-19 08:58:48.637055: 
2024-12-19 08:58:48.638947: Epoch 5
2024-12-19 08:58:48.639750: Current learning rate: 0.0097
2024-12-19 09:00:18.583534: Validation loss improved from -0.34024 to -0.34594! Patience: 0/50
2024-12-19 09:00:18.584759: train_loss -0.3922
2024-12-19 09:00:18.585588: val_loss -0.3459
2024-12-19 09:00:18.586310: Pseudo dice [0.6243]
2024-12-19 09:00:18.587415: Epoch time: 89.95 s
2024-12-19 09:00:18.588373: Yayy! New best EMA pseudo Dice: 0.5364
2024-12-19 09:00:20.153633: 
2024-12-19 09:00:20.154927: Epoch 6
2024-12-19 09:00:20.155629: Current learning rate: 0.00964
2024-12-19 09:01:50.041316: Validation loss improved from -0.34594 to -0.35872! Patience: 0/50
2024-12-19 09:01:50.042495: train_loss -0.4085
2024-12-19 09:01:50.043309: val_loss -0.3587
2024-12-19 09:01:50.044116: Pseudo dice [0.6309]
2024-12-19 09:01:50.044823: Epoch time: 89.89 s
2024-12-19 09:01:50.045465: Yayy! New best EMA pseudo Dice: 0.5458
2024-12-19 09:01:51.664438: 
2024-12-19 09:01:51.666508: Epoch 7
2024-12-19 09:01:51.667285: Current learning rate: 0.00958
2024-12-19 09:03:21.456106: Validation loss improved from -0.35872 to -0.38861! Patience: 0/50
2024-12-19 09:03:21.457039: train_loss -0.4211
2024-12-19 09:03:21.457874: val_loss -0.3886
2024-12-19 09:03:21.458547: Pseudo dice [0.6465]
2024-12-19 09:03:21.459309: Epoch time: 89.79 s
2024-12-19 09:03:21.459947: Yayy! New best EMA pseudo Dice: 0.5559
2024-12-19 09:03:23.115628: 
2024-12-19 09:03:23.117073: Epoch 8
2024-12-19 09:03:23.118043: Current learning rate: 0.00952
2024-12-19 09:04:53.240343: Validation loss did not improve from -0.38861. Patience: 1/50
2024-12-19 09:04:53.241558: train_loss -0.4385
2024-12-19 09:04:53.242506: val_loss -0.386
2024-12-19 09:04:53.243394: Pseudo dice [0.6459]
2024-12-19 09:04:53.244184: Epoch time: 90.13 s
2024-12-19 09:04:53.244958: Yayy! New best EMA pseudo Dice: 0.5649
2024-12-19 09:04:55.606376: 
2024-12-19 09:04:55.607900: Epoch 9
2024-12-19 09:04:55.608558: Current learning rate: 0.00946
2024-12-19 09:06:25.727365: Validation loss did not improve from -0.38861. Patience: 2/50
2024-12-19 09:06:25.728664: train_loss -0.4463
2024-12-19 09:06:25.729546: val_loss -0.3342
2024-12-19 09:06:25.730192: Pseudo dice [0.6133]
2024-12-19 09:06:25.730955: Epoch time: 90.12 s
2024-12-19 09:06:26.079036: Yayy! New best EMA pseudo Dice: 0.5697
2024-12-19 09:06:27.689412: 
2024-12-19 09:06:27.691005: Epoch 10
2024-12-19 09:06:27.691836: Current learning rate: 0.0094
2024-12-19 09:07:57.788231: Validation loss improved from -0.38861 to -0.41529! Patience: 2/50
2024-12-19 09:07:57.789539: train_loss -0.4549
2024-12-19 09:07:57.790248: val_loss -0.4153
2024-12-19 09:07:57.791039: Pseudo dice [0.6621]
2024-12-19 09:07:57.792235: Epoch time: 90.1 s
2024-12-19 09:07:57.792988: Yayy! New best EMA pseudo Dice: 0.579
2024-12-19 09:07:59.424944: 
2024-12-19 09:07:59.426818: Epoch 11
2024-12-19 09:07:59.427557: Current learning rate: 0.00934
2024-12-19 09:09:29.566124: Validation loss did not improve from -0.41529. Patience: 1/50
2024-12-19 09:09:29.567140: train_loss -0.4686
2024-12-19 09:09:29.567997: val_loss -0.3801
2024-12-19 09:09:29.568653: Pseudo dice [0.6365]
2024-12-19 09:09:29.569295: Epoch time: 90.14 s
2024-12-19 09:09:29.570029: Yayy! New best EMA pseudo Dice: 0.5847
2024-12-19 09:09:31.222744: 
2024-12-19 09:09:31.224399: Epoch 12
2024-12-19 09:09:31.225265: Current learning rate: 0.00928
2024-12-19 09:11:01.390126: Validation loss did not improve from -0.41529. Patience: 2/50
2024-12-19 09:11:01.391497: train_loss -0.4809
2024-12-19 09:11:01.392439: val_loss -0.3999
2024-12-19 09:11:01.393246: Pseudo dice [0.654]
2024-12-19 09:11:01.394063: Epoch time: 90.17 s
2024-12-19 09:11:01.394831: Yayy! New best EMA pseudo Dice: 0.5916
2024-12-19 09:11:03.085340: 
2024-12-19 09:11:03.087061: Epoch 13
2024-12-19 09:11:03.087810: Current learning rate: 0.00922
2024-12-19 09:12:33.331526: Validation loss did not improve from -0.41529. Patience: 3/50
2024-12-19 09:12:33.332689: train_loss -0.4871
2024-12-19 09:12:33.333656: val_loss -0.3966
2024-12-19 09:12:33.334426: Pseudo dice [0.6591]
2024-12-19 09:12:33.335161: Epoch time: 90.25 s
2024-12-19 09:12:33.335876: Yayy! New best EMA pseudo Dice: 0.5984
2024-12-19 09:12:34.967858: 
2024-12-19 09:12:34.969329: Epoch 14
2024-12-19 09:12:34.970040: Current learning rate: 0.00916
2024-12-19 09:14:05.102182: Validation loss did not improve from -0.41529. Patience: 4/50
2024-12-19 09:14:05.103163: train_loss -0.4907
2024-12-19 09:14:05.104089: val_loss -0.4003
2024-12-19 09:14:05.104877: Pseudo dice [0.6638]
2024-12-19 09:14:05.105626: Epoch time: 90.14 s
2024-12-19 09:14:05.469757: Yayy! New best EMA pseudo Dice: 0.6049
2024-12-19 09:14:07.154426: 
2024-12-19 09:14:07.155897: Epoch 15
2024-12-19 09:14:07.156726: Current learning rate: 0.0091
2024-12-19 09:15:37.154493: Validation loss improved from -0.41529 to -0.42204! Patience: 4/50
2024-12-19 09:15:37.155575: train_loss -0.508
2024-12-19 09:15:37.156570: val_loss -0.422
2024-12-19 09:15:37.157330: Pseudo dice [0.6712]
2024-12-19 09:15:37.158018: Epoch time: 90.0 s
2024-12-19 09:15:37.158647: Yayy! New best EMA pseudo Dice: 0.6116
2024-12-19 09:15:38.802773: 
2024-12-19 09:15:38.804394: Epoch 16
2024-12-19 09:15:38.805134: Current learning rate: 0.00903
2024-12-19 09:17:09.406004: Validation loss improved from -0.42204 to -0.45725! Patience: 0/50
2024-12-19 09:17:09.407267: train_loss -0.5044
2024-12-19 09:17:09.408045: val_loss -0.4572
2024-12-19 09:17:09.408759: Pseudo dice [0.6921]
2024-12-19 09:17:09.409443: Epoch time: 90.61 s
2024-12-19 09:17:09.410152: Yayy! New best EMA pseudo Dice: 0.6196
2024-12-19 09:17:11.082573: 
2024-12-19 09:17:11.084460: Epoch 17
2024-12-19 09:17:11.085234: Current learning rate: 0.00897
2024-12-19 09:18:41.489536: Validation loss did not improve from -0.45725. Patience: 1/50
2024-12-19 09:18:41.490697: train_loss -0.5125
2024-12-19 09:18:41.491505: val_loss -0.4218
2024-12-19 09:18:41.492166: Pseudo dice [0.6785]
2024-12-19 09:18:41.492858: Epoch time: 90.41 s
2024-12-19 09:18:41.493596: Yayy! New best EMA pseudo Dice: 0.6255
2024-12-19 09:18:43.138561: 
2024-12-19 09:18:43.140378: Epoch 18
2024-12-19 09:18:43.141207: Current learning rate: 0.00891
2024-12-19 09:20:13.414720: Validation loss did not improve from -0.45725. Patience: 2/50
2024-12-19 09:20:13.415777: train_loss -0.5325
2024-12-19 09:20:13.416750: val_loss -0.4356
2024-12-19 09:20:13.417673: Pseudo dice [0.676]
2024-12-19 09:20:13.418641: Epoch time: 90.28 s
2024-12-19 09:20:13.419371: Yayy! New best EMA pseudo Dice: 0.6306
2024-12-19 09:20:15.430437: 
2024-12-19 09:20:15.432326: Epoch 19
2024-12-19 09:20:15.433166: Current learning rate: 0.00885
2024-12-19 09:21:45.813842: Validation loss did not improve from -0.45725. Patience: 3/50
2024-12-19 09:21:45.814806: train_loss -0.5265
2024-12-19 09:21:45.815864: val_loss -0.4224
2024-12-19 09:21:45.816594: Pseudo dice [0.6764]
2024-12-19 09:21:45.817483: Epoch time: 90.39 s
2024-12-19 09:21:46.187860: Yayy! New best EMA pseudo Dice: 0.6351
2024-12-19 09:21:47.894729: 
2024-12-19 09:21:47.896527: Epoch 20
2024-12-19 09:21:47.897298: Current learning rate: 0.00879
2024-12-19 09:23:18.134686: Validation loss did not improve from -0.45725. Patience: 4/50
2024-12-19 09:23:18.135901: train_loss -0.5324
2024-12-19 09:23:18.136789: val_loss -0.4484
2024-12-19 09:23:18.137596: Pseudo dice [0.6778]
2024-12-19 09:23:18.138321: Epoch time: 90.24 s
2024-12-19 09:23:18.138966: Yayy! New best EMA pseudo Dice: 0.6394
2024-12-19 09:23:19.882914: 
2024-12-19 09:23:19.884765: Epoch 21
2024-12-19 09:23:19.885458: Current learning rate: 0.00873
2024-12-19 09:24:50.311418: Validation loss did not improve from -0.45725. Patience: 5/50
2024-12-19 09:24:50.312630: train_loss -0.5194
2024-12-19 09:24:50.313451: val_loss -0.4561
2024-12-19 09:24:50.314105: Pseudo dice [0.6913]
2024-12-19 09:24:50.314840: Epoch time: 90.43 s
2024-12-19 09:24:50.315485: Yayy! New best EMA pseudo Dice: 0.6446
2024-12-19 09:24:51.917536: 
2024-12-19 09:24:51.918863: Epoch 22
2024-12-19 09:24:51.919615: Current learning rate: 0.00867
2024-12-19 09:26:22.203263: Validation loss improved from -0.45725 to -0.47602! Patience: 5/50
2024-12-19 09:26:22.203981: train_loss -0.5431
2024-12-19 09:26:22.204882: val_loss -0.476
2024-12-19 09:26:22.205522: Pseudo dice [0.6888]
2024-12-19 09:26:22.206171: Epoch time: 90.29 s
2024-12-19 09:26:22.206867: Yayy! New best EMA pseudo Dice: 0.649
2024-12-19 09:26:23.823086: 
2024-12-19 09:26:23.825166: Epoch 23
2024-12-19 09:26:23.825854: Current learning rate: 0.00861
2024-12-19 09:27:54.103117: Validation loss did not improve from -0.47602. Patience: 1/50
2024-12-19 09:27:54.103876: train_loss -0.5578
2024-12-19 09:27:54.104789: val_loss -0.4628
2024-12-19 09:27:54.105597: Pseudo dice [0.685]
2024-12-19 09:27:54.106439: Epoch time: 90.28 s
2024-12-19 09:27:54.107064: Yayy! New best EMA pseudo Dice: 0.6526
2024-12-19 09:27:55.740203: 
2024-12-19 09:27:55.741681: Epoch 24
2024-12-19 09:27:55.742893: Current learning rate: 0.00855
2024-12-19 09:29:26.243468: Validation loss improved from -0.47602 to -0.47749! Patience: 1/50
2024-12-19 09:29:26.244759: train_loss -0.558
2024-12-19 09:29:26.245780: val_loss -0.4775
2024-12-19 09:29:26.246651: Pseudo dice [0.7053]
2024-12-19 09:29:26.247483: Epoch time: 90.51 s
2024-12-19 09:29:26.615931: Yayy! New best EMA pseudo Dice: 0.6579
2024-12-19 09:29:28.173768: 
2024-12-19 09:29:28.175417: Epoch 25
2024-12-19 09:29:28.176151: Current learning rate: 0.00849
2024-12-19 09:30:58.716351: Validation loss improved from -0.47749 to -0.48068! Patience: 0/50
2024-12-19 09:30:58.717019: train_loss -0.5665
2024-12-19 09:30:58.717889: val_loss -0.4807
2024-12-19 09:30:58.718581: Pseudo dice [0.7085]
2024-12-19 09:30:58.719298: Epoch time: 90.54 s
2024-12-19 09:30:58.720050: Yayy! New best EMA pseudo Dice: 0.663
2024-12-19 09:31:00.296141: 
2024-12-19 09:31:00.298410: Epoch 26
2024-12-19 09:31:00.299448: Current learning rate: 0.00843
2024-12-19 09:32:30.775324: Validation loss improved from -0.48068 to -0.48900! Patience: 0/50
2024-12-19 09:32:30.776465: train_loss -0.5738
2024-12-19 09:32:30.777484: val_loss -0.489
2024-12-19 09:32:30.778298: Pseudo dice [0.7111]
2024-12-19 09:32:30.779273: Epoch time: 90.48 s
2024-12-19 09:32:30.780240: Yayy! New best EMA pseudo Dice: 0.6678
2024-12-19 09:32:32.376654: 
2024-12-19 09:32:32.378290: Epoch 27
2024-12-19 09:32:32.379104: Current learning rate: 0.00836
2024-12-19 09:34:02.865860: Validation loss did not improve from -0.48900. Patience: 1/50
2024-12-19 09:34:02.867014: train_loss -0.5796
2024-12-19 09:34:02.867810: val_loss -0.4779
2024-12-19 09:34:02.868456: Pseudo dice [0.6971]
2024-12-19 09:34:02.869287: Epoch time: 90.49 s
2024-12-19 09:34:02.869952: Yayy! New best EMA pseudo Dice: 0.6707
2024-12-19 09:34:04.488867: 
2024-12-19 09:34:04.490754: Epoch 28
2024-12-19 09:34:04.492077: Current learning rate: 0.0083
2024-12-19 09:35:34.995926: Validation loss did not improve from -0.48900. Patience: 2/50
2024-12-19 09:35:34.997359: train_loss -0.5841
2024-12-19 09:35:34.998387: val_loss -0.4866
2024-12-19 09:35:34.999085: Pseudo dice [0.6986]
2024-12-19 09:35:34.999820: Epoch time: 90.51 s
2024-12-19 09:35:35.000393: Yayy! New best EMA pseudo Dice: 0.6735
2024-12-19 09:35:36.868282: 
2024-12-19 09:35:36.869882: Epoch 29
2024-12-19 09:35:36.870845: Current learning rate: 0.00824
2024-12-19 09:37:07.271412: Validation loss improved from -0.48900 to -0.50297! Patience: 2/50
2024-12-19 09:37:07.272476: train_loss -0.5871
2024-12-19 09:37:07.273421: val_loss -0.503
2024-12-19 09:37:07.274367: Pseudo dice [0.7184]
2024-12-19 09:37:07.275260: Epoch time: 90.41 s
2024-12-19 09:37:07.657613: Yayy! New best EMA pseudo Dice: 0.678
2024-12-19 09:37:09.287266: 
2024-12-19 09:37:09.288779: Epoch 30
2024-12-19 09:37:09.289512: Current learning rate: 0.00818
2024-12-19 09:38:39.413930: Validation loss improved from -0.50297 to -0.51284! Patience: 0/50
2024-12-19 09:38:39.415158: train_loss -0.5796
2024-12-19 09:38:39.416167: val_loss -0.5128
2024-12-19 09:38:39.416949: Pseudo dice [0.7207]
2024-12-19 09:38:39.417867: Epoch time: 90.13 s
2024-12-19 09:38:39.418455: Yayy! New best EMA pseudo Dice: 0.6823
2024-12-19 09:38:41.054829: 
2024-12-19 09:38:41.056505: Epoch 31
2024-12-19 09:38:41.057247: Current learning rate: 0.00812
2024-12-19 09:40:11.145243: Validation loss improved from -0.51284 to -0.51394! Patience: 0/50
2024-12-19 09:40:11.146492: train_loss -0.5909
2024-12-19 09:40:11.147516: val_loss -0.5139
2024-12-19 09:40:11.148499: Pseudo dice [0.7168]
2024-12-19 09:40:11.149328: Epoch time: 90.09 s
2024-12-19 09:40:11.150187: Yayy! New best EMA pseudo Dice: 0.6857
2024-12-19 09:40:12.771780: 
2024-12-19 09:40:12.773588: Epoch 32
2024-12-19 09:40:12.774740: Current learning rate: 0.00806
2024-12-19 09:41:42.996470: Validation loss did not improve from -0.51394. Patience: 1/50
2024-12-19 09:41:42.997391: train_loss -0.5804
2024-12-19 09:41:42.998362: val_loss -0.4962
2024-12-19 09:41:42.999190: Pseudo dice [0.7136]
2024-12-19 09:41:43.000076: Epoch time: 90.23 s
2024-12-19 09:41:43.001038: Yayy! New best EMA pseudo Dice: 0.6885
2024-12-19 09:41:44.753214: 
2024-12-19 09:41:44.754775: Epoch 33
2024-12-19 09:41:44.755862: Current learning rate: 0.008
2024-12-19 09:43:14.921282: Validation loss did not improve from -0.51394. Patience: 2/50
2024-12-19 09:43:14.922359: train_loss -0.592
2024-12-19 09:43:14.923326: val_loss -0.4964
2024-12-19 09:43:14.924082: Pseudo dice [0.7079]
2024-12-19 09:43:14.924947: Epoch time: 90.17 s
2024-12-19 09:43:14.925621: Yayy! New best EMA pseudo Dice: 0.6904
2024-12-19 09:43:16.504337: 
2024-12-19 09:43:16.505913: Epoch 34
2024-12-19 09:43:16.506597: Current learning rate: 0.00793
2024-12-19 09:44:46.672779: Validation loss improved from -0.51394 to -0.53409! Patience: 2/50
2024-12-19 09:44:46.673934: train_loss -0.6035
2024-12-19 09:44:46.674787: val_loss -0.5341
2024-12-19 09:44:46.675509: Pseudo dice [0.7292]
2024-12-19 09:44:46.676225: Epoch time: 90.17 s
2024-12-19 09:44:47.025877: Yayy! New best EMA pseudo Dice: 0.6943
2024-12-19 09:44:48.587446: 
2024-12-19 09:44:48.589230: Epoch 35
2024-12-19 09:44:48.590049: Current learning rate: 0.00787
2024-12-19 09:46:18.776871: Validation loss did not improve from -0.53409. Patience: 1/50
2024-12-19 09:46:18.778167: train_loss -0.6017
2024-12-19 09:46:18.779090: val_loss -0.4776
2024-12-19 09:46:18.779800: Pseudo dice [0.6961]
2024-12-19 09:46:18.780428: Epoch time: 90.19 s
2024-12-19 09:46:18.781044: Yayy! New best EMA pseudo Dice: 0.6945
2024-12-19 09:46:20.372889: 
2024-12-19 09:46:20.374318: Epoch 36
2024-12-19 09:46:20.375044: Current learning rate: 0.00781
2024-12-19 09:47:50.722796: Validation loss did not improve from -0.53409. Patience: 2/50
2024-12-19 09:47:50.723576: train_loss -0.6066
2024-12-19 09:47:50.724704: val_loss -0.4917
2024-12-19 09:47:50.725546: Pseudo dice [0.7111]
2024-12-19 09:47:50.726223: Epoch time: 90.35 s
2024-12-19 09:47:50.726974: Yayy! New best EMA pseudo Dice: 0.6962
2024-12-19 09:47:52.329402: 
2024-12-19 09:47:52.331038: Epoch 37
2024-12-19 09:47:52.331881: Current learning rate: 0.00775
2024-12-19 09:49:22.608754: Validation loss did not improve from -0.53409. Patience: 3/50
2024-12-19 09:49:22.609676: train_loss -0.6016
2024-12-19 09:49:22.610473: val_loss -0.5243
2024-12-19 09:49:22.611163: Pseudo dice [0.7328]
2024-12-19 09:49:22.611934: Epoch time: 90.28 s
2024-12-19 09:49:22.612824: Yayy! New best EMA pseudo Dice: 0.6998
2024-12-19 09:49:24.223688: 
2024-12-19 09:49:24.225619: Epoch 38
2024-12-19 09:49:24.226527: Current learning rate: 0.00769
2024-12-19 09:50:54.762416: Validation loss did not improve from -0.53409. Patience: 4/50
2024-12-19 09:50:54.763441: train_loss -0.6138
2024-12-19 09:50:54.764602: val_loss -0.5075
2024-12-19 09:50:54.765391: Pseudo dice [0.719]
2024-12-19 09:50:54.766012: Epoch time: 90.54 s
2024-12-19 09:50:54.766637: Yayy! New best EMA pseudo Dice: 0.7017
2024-12-19 09:50:56.713773: 
2024-12-19 09:50:56.715440: Epoch 39
2024-12-19 09:50:56.716190: Current learning rate: 0.00763
2024-12-19 09:52:27.178287: Validation loss did not improve from -0.53409. Patience: 5/50
2024-12-19 09:52:27.179398: train_loss -0.6142
2024-12-19 09:52:27.180100: val_loss -0.4895
2024-12-19 09:52:27.180827: Pseudo dice [0.7074]
2024-12-19 09:52:27.181521: Epoch time: 90.47 s
2024-12-19 09:52:27.547632: Yayy! New best EMA pseudo Dice: 0.7023
2024-12-19 09:52:29.175287: 
2024-12-19 09:52:29.176980: Epoch 40
2024-12-19 09:52:29.177943: Current learning rate: 0.00756
2024-12-19 09:53:59.737392: Validation loss did not improve from -0.53409. Patience: 6/50
2024-12-19 09:53:59.738677: train_loss -0.6143
2024-12-19 09:53:59.739810: val_loss -0.5243
2024-12-19 09:53:59.740707: Pseudo dice [0.7292]
2024-12-19 09:53:59.741426: Epoch time: 90.56 s
2024-12-19 09:53:59.742288: Yayy! New best EMA pseudo Dice: 0.705
2024-12-19 09:54:01.377370: 
2024-12-19 09:54:01.379431: Epoch 41
2024-12-19 09:54:01.380611: Current learning rate: 0.0075
2024-12-19 09:55:31.927352: Validation loss did not improve from -0.53409. Patience: 7/50
2024-12-19 09:55:31.939718: train_loss -0.6035
2024-12-19 09:55:31.942228: val_loss -0.5089
2024-12-19 09:55:31.943284: Pseudo dice [0.7275]
2024-12-19 09:55:31.944454: Epoch time: 90.56 s
2024-12-19 09:55:31.945270: Yayy! New best EMA pseudo Dice: 0.7072
2024-12-19 09:55:33.568066: 
2024-12-19 09:55:33.569926: Epoch 42
2024-12-19 09:55:33.570974: Current learning rate: 0.00744
2024-12-19 09:57:04.074287: Validation loss did not improve from -0.53409. Patience: 8/50
2024-12-19 09:57:04.075294: train_loss -0.6263
2024-12-19 09:57:04.076129: val_loss -0.5126
2024-12-19 09:57:04.076922: Pseudo dice [0.7241]
2024-12-19 09:57:04.077617: Epoch time: 90.51 s
2024-12-19 09:57:04.078303: Yayy! New best EMA pseudo Dice: 0.7089
2024-12-19 09:57:05.641921: 
2024-12-19 09:57:05.643904: Epoch 43
2024-12-19 09:57:05.644911: Current learning rate: 0.00738
2024-12-19 09:58:36.182500: Validation loss did not improve from -0.53409. Patience: 9/50
2024-12-19 09:58:36.183690: train_loss -0.6269
2024-12-19 09:58:36.184592: val_loss -0.5026
2024-12-19 09:58:36.185281: Pseudo dice [0.7176]
2024-12-19 09:58:36.186044: Epoch time: 90.54 s
2024-12-19 09:58:36.186715: Yayy! New best EMA pseudo Dice: 0.7098
2024-12-19 09:58:37.736682: 
2024-12-19 09:58:37.737525: Epoch 44
2024-12-19 09:58:37.738586: Current learning rate: 0.00732
2024-12-19 10:00:08.201723: Validation loss did not improve from -0.53409. Patience: 10/50
2024-12-19 10:00:08.203098: train_loss -0.6239
2024-12-19 10:00:08.204010: val_loss -0.5228
2024-12-19 10:00:08.204705: Pseudo dice [0.7308]
2024-12-19 10:00:08.205521: Epoch time: 90.47 s
2024-12-19 10:00:08.593691: Yayy! New best EMA pseudo Dice: 0.7119
2024-12-19 10:00:10.153786: 
2024-12-19 10:00:10.154980: Epoch 45
2024-12-19 10:00:10.155960: Current learning rate: 0.00725
2024-12-19 10:01:40.482826: Validation loss did not improve from -0.53409. Patience: 11/50
2024-12-19 10:01:40.483912: train_loss -0.6191
2024-12-19 10:01:40.484961: val_loss -0.5001
2024-12-19 10:01:40.485662: Pseudo dice [0.7164]
2024-12-19 10:01:40.486308: Epoch time: 90.33 s
2024-12-19 10:01:40.486984: Yayy! New best EMA pseudo Dice: 0.7123
2024-12-19 10:01:42.036206: 
2024-12-19 10:01:42.037862: Epoch 46
2024-12-19 10:01:42.038757: Current learning rate: 0.00719
2024-12-19 10:03:12.488638: Validation loss did not improve from -0.53409. Patience: 12/50
2024-12-19 10:03:12.489964: train_loss -0.6261
2024-12-19 10:03:12.490648: val_loss -0.5235
2024-12-19 10:03:12.491325: Pseudo dice [0.7182]
2024-12-19 10:03:12.492007: Epoch time: 90.45 s
2024-12-19 10:03:12.492709: Yayy! New best EMA pseudo Dice: 0.7129
2024-12-19 10:03:14.058840: 
2024-12-19 10:03:14.060215: Epoch 47
2024-12-19 10:03:14.061006: Current learning rate: 0.00713
2024-12-19 10:04:44.473785: Validation loss did not improve from -0.53409. Patience: 13/50
2024-12-19 10:04:44.474936: train_loss -0.6342
2024-12-19 10:04:44.475882: val_loss -0.4667
2024-12-19 10:04:44.476686: Pseudo dice [0.6917]
2024-12-19 10:04:44.477290: Epoch time: 90.42 s
2024-12-19 10:04:45.697885: 
2024-12-19 10:04:45.699283: Epoch 48
2024-12-19 10:04:45.700061: Current learning rate: 0.00707
2024-12-19 10:06:16.146247: Validation loss did not improve from -0.53409. Patience: 14/50
2024-12-19 10:06:16.147468: train_loss -0.6229
2024-12-19 10:06:16.148350: val_loss -0.49
2024-12-19 10:06:16.149156: Pseudo dice [0.7063]
2024-12-19 10:06:16.149868: Epoch time: 90.45 s
2024-12-19 10:06:17.364310: 
2024-12-19 10:06:17.366079: Epoch 49
2024-12-19 10:06:17.366896: Current learning rate: 0.007
2024-12-19 10:07:47.829074: Validation loss improved from -0.53409 to -0.53575! Patience: 14/50
2024-12-19 10:07:47.829981: train_loss -0.6341
2024-12-19 10:07:47.830693: val_loss -0.5358
2024-12-19 10:07:47.831403: Pseudo dice [0.7385]
2024-12-19 10:07:47.832106: Epoch time: 90.47 s
2024-12-19 10:07:48.197047: Yayy! New best EMA pseudo Dice: 0.7132
2024-12-19 10:07:50.156829: 
2024-12-19 10:07:50.158494: Epoch 50
2024-12-19 10:07:50.159315: Current learning rate: 0.00694
2024-12-19 10:09:20.556483: Validation loss did not improve from -0.53575. Patience: 1/50
2024-12-19 10:09:20.557787: train_loss -0.6396
2024-12-19 10:09:20.558654: val_loss -0.4981
2024-12-19 10:09:20.559416: Pseudo dice [0.7128]
2024-12-19 10:09:20.560151: Epoch time: 90.4 s
2024-12-19 10:09:21.769498: 
2024-12-19 10:09:21.770999: Epoch 51
2024-12-19 10:09:21.771924: Current learning rate: 0.00688
2024-12-19 10:10:52.236132: Validation loss did not improve from -0.53575. Patience: 2/50
2024-12-19 10:10:52.237370: train_loss -0.6426
2024-12-19 10:10:52.238191: val_loss -0.5168
2024-12-19 10:10:52.238943: Pseudo dice [0.7289]
2024-12-19 10:10:52.239688: Epoch time: 90.47 s
2024-12-19 10:10:52.240434: Yayy! New best EMA pseudo Dice: 0.7147
2024-12-19 10:10:53.861105: 
2024-12-19 10:10:53.863107: Epoch 52
2024-12-19 10:10:53.864360: Current learning rate: 0.00682
2024-12-19 10:12:24.237406: Validation loss did not improve from -0.53575. Patience: 3/50
2024-12-19 10:12:24.238520: train_loss -0.6412
2024-12-19 10:12:24.239805: val_loss -0.4969
2024-12-19 10:12:24.240649: Pseudo dice [0.7165]
2024-12-19 10:12:24.241520: Epoch time: 90.38 s
2024-12-19 10:12:24.242318: Yayy! New best EMA pseudo Dice: 0.7149
2024-12-19 10:12:25.884764: 
2024-12-19 10:12:25.886095: Epoch 53
2024-12-19 10:12:25.886869: Current learning rate: 0.00675
2024-12-19 10:13:56.250577: Validation loss did not improve from -0.53575. Patience: 4/50
2024-12-19 10:13:56.251791: train_loss -0.6411
2024-12-19 10:13:56.252786: val_loss -0.489
2024-12-19 10:13:56.253456: Pseudo dice [0.7137]
2024-12-19 10:13:56.254320: Epoch time: 90.37 s
2024-12-19 10:13:57.468726: 
2024-12-19 10:13:57.470523: Epoch 54
2024-12-19 10:13:57.471786: Current learning rate: 0.00669
2024-12-19 10:15:28.177418: Validation loss did not improve from -0.53575. Patience: 5/50
2024-12-19 10:15:28.178263: train_loss -0.6446
2024-12-19 10:15:28.179243: val_loss -0.5033
2024-12-19 10:15:28.179988: Pseudo dice [0.7144]
2024-12-19 10:15:28.180845: Epoch time: 90.71 s
2024-12-19 10:15:29.790810: 
2024-12-19 10:15:29.792523: Epoch 55
2024-12-19 10:15:29.793428: Current learning rate: 0.00663
2024-12-19 10:17:00.271070: Validation loss did not improve from -0.53575. Patience: 6/50
2024-12-19 10:17:00.272110: train_loss -0.6541
2024-12-19 10:17:00.272821: val_loss -0.4859
2024-12-19 10:17:00.273496: Pseudo dice [0.7061]
2024-12-19 10:17:00.274571: Epoch time: 90.48 s
2024-12-19 10:17:01.544205: 
2024-12-19 10:17:01.546235: Epoch 56
2024-12-19 10:17:01.547324: Current learning rate: 0.00657
2024-12-19 10:18:31.972345: Validation loss did not improve from -0.53575. Patience: 7/50
2024-12-19 10:18:31.973462: train_loss -0.6543
2024-12-19 10:18:31.974407: val_loss -0.5046
2024-12-19 10:18:31.975148: Pseudo dice [0.7214]
2024-12-19 10:18:31.975796: Epoch time: 90.43 s
2024-12-19 10:18:33.196902: 
2024-12-19 10:18:33.198621: Epoch 57
2024-12-19 10:18:33.199335: Current learning rate: 0.0065
2024-12-19 10:20:03.699884: Validation loss did not improve from -0.53575. Patience: 8/50
2024-12-19 10:20:03.700635: train_loss -0.6623
2024-12-19 10:20:03.701663: val_loss -0.5017
2024-12-19 10:20:03.702463: Pseudo dice [0.7116]
2024-12-19 10:20:03.703257: Epoch time: 90.5 s
2024-12-19 10:20:04.938672: 
2024-12-19 10:20:04.940232: Epoch 58
2024-12-19 10:20:04.940963: Current learning rate: 0.00644
2024-12-19 10:21:35.466137: Validation loss improved from -0.53575 to -0.54475! Patience: 8/50
2024-12-19 10:21:35.467375: train_loss -0.6578
2024-12-19 10:21:35.468295: val_loss -0.5447
2024-12-19 10:21:35.469216: Pseudo dice [0.7396]
2024-12-19 10:21:35.469982: Epoch time: 90.53 s
2024-12-19 10:21:35.470667: Yayy! New best EMA pseudo Dice: 0.7169
2024-12-19 10:21:37.085897: 
2024-12-19 10:21:37.088295: Epoch 59
2024-12-19 10:21:37.089590: Current learning rate: 0.00638
2024-12-19 10:23:07.507062: Validation loss did not improve from -0.54475. Patience: 1/50
2024-12-19 10:23:07.508285: train_loss -0.6521
2024-12-19 10:23:07.509338: val_loss -0.5269
2024-12-19 10:23:07.510086: Pseudo dice [0.7358]
2024-12-19 10:23:07.511111: Epoch time: 90.42 s
2024-12-19 10:23:07.889878: Yayy! New best EMA pseudo Dice: 0.7187
2024-12-19 10:23:09.479834: 
2024-12-19 10:23:09.482188: Epoch 60
2024-12-19 10:23:09.482900: Current learning rate: 0.00631
2024-12-19 10:24:39.863302: Validation loss did not improve from -0.54475. Patience: 2/50
2024-12-19 10:24:39.864455: train_loss -0.6596
2024-12-19 10:24:39.865484: val_loss -0.4973
2024-12-19 10:24:39.866344: Pseudo dice [0.7195]
2024-12-19 10:24:39.867300: Epoch time: 90.39 s
2024-12-19 10:24:39.868268: Yayy! New best EMA pseudo Dice: 0.7188
2024-12-19 10:24:41.770892: 
2024-12-19 10:24:41.771936: Epoch 61
2024-12-19 10:24:41.772865: Current learning rate: 0.00625
2024-12-19 10:26:11.899495: Validation loss did not improve from -0.54475. Patience: 3/50
2024-12-19 10:26:11.900551: train_loss -0.6598
2024-12-19 10:26:11.901413: val_loss -0.5298
2024-12-19 10:26:11.902175: Pseudo dice [0.7283]
2024-12-19 10:26:11.902866: Epoch time: 90.13 s
2024-12-19 10:26:11.903553: Yayy! New best EMA pseudo Dice: 0.7198
2024-12-19 10:26:13.489079: 
2024-12-19 10:26:13.490425: Epoch 62
2024-12-19 10:26:13.491138: Current learning rate: 0.00619
2024-12-19 10:27:43.581881: Validation loss did not improve from -0.54475. Patience: 4/50
2024-12-19 10:27:43.582994: train_loss -0.6692
2024-12-19 10:27:43.583820: val_loss -0.5162
2024-12-19 10:27:43.584571: Pseudo dice [0.7235]
2024-12-19 10:27:43.585292: Epoch time: 90.09 s
2024-12-19 10:27:43.585980: Yayy! New best EMA pseudo Dice: 0.7201
2024-12-19 10:27:45.148799: 
2024-12-19 10:27:45.150312: Epoch 63
2024-12-19 10:27:45.151201: Current learning rate: 0.00612
2024-12-19 10:29:15.257069: Validation loss did not improve from -0.54475. Patience: 5/50
2024-12-19 10:29:15.258083: train_loss -0.6646
2024-12-19 10:29:15.259151: val_loss -0.5191
2024-12-19 10:29:15.259875: Pseudo dice [0.7322]
2024-12-19 10:29:15.260594: Epoch time: 90.11 s
2024-12-19 10:29:15.261302: Yayy! New best EMA pseudo Dice: 0.7214
2024-12-19 10:29:16.862267: 
2024-12-19 10:29:16.864334: Epoch 64
2024-12-19 10:29:16.865172: Current learning rate: 0.00606
2024-12-19 10:30:46.906786: Validation loss did not improve from -0.54475. Patience: 6/50
2024-12-19 10:30:46.908063: train_loss -0.6653
2024-12-19 10:30:46.908937: val_loss -0.5128
2024-12-19 10:30:46.909724: Pseudo dice [0.7278]
2024-12-19 10:30:46.910458: Epoch time: 90.05 s
2024-12-19 10:30:47.299759: Yayy! New best EMA pseudo Dice: 0.722
2024-12-19 10:30:48.918169: 
2024-12-19 10:30:48.919833: Epoch 65
2024-12-19 10:30:48.920592: Current learning rate: 0.006
2024-12-19 10:32:19.023866: Validation loss did not improve from -0.54475. Patience: 7/50
2024-12-19 10:32:19.024839: train_loss -0.6675
2024-12-19 10:32:19.025562: val_loss -0.5057
2024-12-19 10:32:19.026184: Pseudo dice [0.7213]
2024-12-19 10:32:19.026918: Epoch time: 90.11 s
2024-12-19 10:32:20.308227: 
2024-12-19 10:32:20.310152: Epoch 66
2024-12-19 10:32:20.311080: Current learning rate: 0.00593
2024-12-19 10:33:50.423640: Validation loss did not improve from -0.54475. Patience: 8/50
2024-12-19 10:33:50.424889: train_loss -0.6642
2024-12-19 10:33:50.425806: val_loss -0.5345
2024-12-19 10:33:50.426428: Pseudo dice [0.7396]
2024-12-19 10:33:50.427114: Epoch time: 90.12 s
2024-12-19 10:33:50.427752: Yayy! New best EMA pseudo Dice: 0.7237
2024-12-19 10:33:52.021663: 
2024-12-19 10:33:52.023404: Epoch 67
2024-12-19 10:33:52.024562: Current learning rate: 0.00587
2024-12-19 10:35:22.229169: Validation loss did not improve from -0.54475. Patience: 9/50
2024-12-19 10:35:22.230131: train_loss -0.6677
2024-12-19 10:35:22.231071: val_loss -0.5267
2024-12-19 10:35:22.231782: Pseudo dice [0.7374]
2024-12-19 10:35:22.232405: Epoch time: 90.21 s
2024-12-19 10:35:22.233124: Yayy! New best EMA pseudo Dice: 0.7251
2024-12-19 10:35:23.912388: 
2024-12-19 10:35:23.914203: Epoch 68
2024-12-19 10:35:23.915057: Current learning rate: 0.00581
2024-12-19 10:36:54.107648: Validation loss did not improve from -0.54475. Patience: 10/50
2024-12-19 10:36:54.108889: train_loss -0.6676
2024-12-19 10:36:54.109728: val_loss -0.5316
2024-12-19 10:36:54.110374: Pseudo dice [0.7402]
2024-12-19 10:36:54.111278: Epoch time: 90.2 s
2024-12-19 10:36:54.111917: Yayy! New best EMA pseudo Dice: 0.7266
2024-12-19 10:36:55.732713: 
2024-12-19 10:36:55.734562: Epoch 69
2024-12-19 10:36:55.735446: Current learning rate: 0.00574
2024-12-19 10:38:26.119554: Validation loss did not improve from -0.54475. Patience: 11/50
2024-12-19 10:38:26.120442: train_loss -0.676
2024-12-19 10:38:26.121423: val_loss -0.5318
2024-12-19 10:38:26.122113: Pseudo dice [0.7401]
2024-12-19 10:38:26.122823: Epoch time: 90.39 s
2024-12-19 10:38:26.501468: Yayy! New best EMA pseudo Dice: 0.7279
2024-12-19 10:38:28.168738: 
2024-12-19 10:38:28.170188: Epoch 70
2024-12-19 10:38:28.170926: Current learning rate: 0.00568
2024-12-19 10:39:58.421865: Validation loss did not improve from -0.54475. Patience: 12/50
2024-12-19 10:39:58.423183: train_loss -0.6779
2024-12-19 10:39:58.424163: val_loss -0.5234
2024-12-19 10:39:58.424879: Pseudo dice [0.7346]
2024-12-19 10:39:58.425572: Epoch time: 90.26 s
2024-12-19 10:39:58.426298: Yayy! New best EMA pseudo Dice: 0.7286
2024-12-19 10:40:00.014373: 
2024-12-19 10:40:00.015895: Epoch 71
2024-12-19 10:40:00.016632: Current learning rate: 0.00562
2024-12-19 10:41:30.185787: Validation loss did not improve from -0.54475. Patience: 13/50
2024-12-19 10:41:30.187072: train_loss -0.6746
2024-12-19 10:41:30.188018: val_loss -0.5049
2024-12-19 10:41:30.188629: Pseudo dice [0.7234]
2024-12-19 10:41:30.189200: Epoch time: 90.17 s
2024-12-19 10:41:31.866846: 
2024-12-19 10:41:31.868397: Epoch 72
2024-12-19 10:41:31.869205: Current learning rate: 0.00555
2024-12-19 10:43:01.950201: Validation loss did not improve from -0.54475. Patience: 14/50
2024-12-19 10:43:01.951298: train_loss -0.6785
2024-12-19 10:43:01.952062: val_loss -0.5327
2024-12-19 10:43:01.952731: Pseudo dice [0.7386]
2024-12-19 10:43:01.953439: Epoch time: 90.09 s
2024-12-19 10:43:01.954279: Yayy! New best EMA pseudo Dice: 0.7291
2024-12-19 10:43:03.584828: 
2024-12-19 10:43:03.586982: Epoch 73
2024-12-19 10:43:03.588155: Current learning rate: 0.00549
2024-12-19 10:44:33.622489: Validation loss did not improve from -0.54475. Patience: 15/50
2024-12-19 10:44:33.623637: train_loss -0.6831
2024-12-19 10:44:33.624429: val_loss -0.4796
2024-12-19 10:44:33.625132: Pseudo dice [0.7094]
2024-12-19 10:44:33.625756: Epoch time: 90.04 s
2024-12-19 10:44:34.915666: 
2024-12-19 10:44:34.918015: Epoch 74
2024-12-19 10:44:34.919051: Current learning rate: 0.00542
2024-12-19 10:46:04.985263: Validation loss did not improve from -0.54475. Patience: 16/50
2024-12-19 10:46:04.986451: train_loss -0.685
2024-12-19 10:46:04.987780: val_loss -0.5393
2024-12-19 10:46:04.988726: Pseudo dice [0.7386]
2024-12-19 10:46:04.989594: Epoch time: 90.07 s
2024-12-19 10:46:06.853933: 
2024-12-19 10:46:06.856094: Epoch 75
2024-12-19 10:46:06.857044: Current learning rate: 0.00536
2024-12-19 10:47:36.850561: Validation loss did not improve from -0.54475. Patience: 17/50
2024-12-19 10:47:36.851637: train_loss -0.6825
2024-12-19 10:47:36.852406: val_loss -0.5299
2024-12-19 10:47:36.853135: Pseudo dice [0.7286]
2024-12-19 10:47:36.854024: Epoch time: 90.0 s
2024-12-19 10:47:38.084567: 
2024-12-19 10:47:38.087866: Epoch 76
2024-12-19 10:47:38.088889: Current learning rate: 0.00529
2024-12-19 10:49:08.307237: Validation loss did not improve from -0.54475. Patience: 18/50
2024-12-19 10:49:08.309800: train_loss -0.6847
2024-12-19 10:49:08.311949: val_loss -0.4955
2024-12-19 10:49:08.312897: Pseudo dice [0.713]
2024-12-19 10:49:08.314297: Epoch time: 90.23 s
2024-12-19 10:49:09.893022: 
2024-12-19 10:49:09.894969: Epoch 77
2024-12-19 10:49:09.895742: Current learning rate: 0.00523
2024-12-19 10:50:39.885216: Validation loss did not improve from -0.54475. Patience: 19/50
2024-12-19 10:50:39.886413: train_loss -0.6896
2024-12-19 10:50:39.887700: val_loss -0.5431
2024-12-19 10:50:39.888552: Pseudo dice [0.7472]
2024-12-19 10:50:39.889173: Epoch time: 89.99 s
2024-12-19 10:50:41.195813: 
2024-12-19 10:50:41.197635: Epoch 78
2024-12-19 10:50:41.198375: Current learning rate: 0.00517
2024-12-19 10:52:11.164311: Validation loss did not improve from -0.54475. Patience: 20/50
2024-12-19 10:52:11.165622: train_loss -0.6873
2024-12-19 10:52:11.166597: val_loss -0.5349
2024-12-19 10:52:11.167347: Pseudo dice [0.7378]
2024-12-19 10:52:11.167944: Epoch time: 89.97 s
2024-12-19 10:52:11.168796: Yayy! New best EMA pseudo Dice: 0.7297
2024-12-19 10:52:12.832474: 
2024-12-19 10:52:12.834991: Epoch 79
2024-12-19 10:52:12.836065: Current learning rate: 0.0051
2024-12-19 10:53:43.071156: Validation loss did not improve from -0.54475. Patience: 21/50
2024-12-19 10:53:43.072528: train_loss -0.6947
2024-12-19 10:53:43.073801: val_loss -0.5341
2024-12-19 10:53:43.074490: Pseudo dice [0.7345]
2024-12-19 10:53:43.075164: Epoch time: 90.24 s
2024-12-19 10:53:43.429487: Yayy! New best EMA pseudo Dice: 0.7302
2024-12-19 10:53:45.076585: 
2024-12-19 10:53:45.078502: Epoch 80
2024-12-19 10:53:45.079251: Current learning rate: 0.00504
2024-12-19 10:55:15.412717: Validation loss did not improve from -0.54475. Patience: 22/50
2024-12-19 10:55:15.413742: train_loss -0.6975
2024-12-19 10:55:15.414627: val_loss -0.5268
2024-12-19 10:55:15.415344: Pseudo dice [0.7261]
2024-12-19 10:55:15.415991: Epoch time: 90.34 s
2024-12-19 10:55:16.757749: 
2024-12-19 10:55:16.759504: Epoch 81
2024-12-19 10:55:16.760209: Current learning rate: 0.00497
2024-12-19 10:56:47.091079: Validation loss did not improve from -0.54475. Patience: 23/50
2024-12-19 10:56:47.092178: train_loss -0.6927
2024-12-19 10:56:47.093282: val_loss -0.5351
2024-12-19 10:56:47.094007: Pseudo dice [0.7398]
2024-12-19 10:56:47.094902: Epoch time: 90.34 s
2024-12-19 10:56:47.095675: Yayy! New best EMA pseudo Dice: 0.7308
2024-12-19 10:56:49.090758: 
2024-12-19 10:56:49.092789: Epoch 82
2024-12-19 10:56:49.093932: Current learning rate: 0.00491
2024-12-19 10:58:19.524076: Validation loss did not improve from -0.54475. Patience: 24/50
2024-12-19 10:58:19.525311: train_loss -0.6935
2024-12-19 10:58:19.526158: val_loss -0.5067
2024-12-19 10:58:19.526824: Pseudo dice [0.7282]
2024-12-19 10:58:19.527524: Epoch time: 90.44 s
2024-12-19 10:58:20.753325: 
2024-12-19 10:58:20.754651: Epoch 83
2024-12-19 10:58:20.755602: Current learning rate: 0.00484
2024-12-19 10:59:51.272703: Validation loss improved from -0.54475 to -0.56260! Patience: 24/50
2024-12-19 10:59:51.273792: train_loss -0.6907
2024-12-19 10:59:51.274644: val_loss -0.5626
2024-12-19 10:59:51.275391: Pseudo dice [0.7483]
2024-12-19 10:59:51.275977: Epoch time: 90.52 s
2024-12-19 10:59:51.276670: Yayy! New best EMA pseudo Dice: 0.7323
2024-12-19 10:59:52.898645: 
2024-12-19 10:59:52.900022: Epoch 84
2024-12-19 10:59:52.900777: Current learning rate: 0.00478
2024-12-19 11:01:24.465042: Validation loss did not improve from -0.56260. Patience: 1/50
2024-12-19 11:01:24.512926: train_loss -0.6964
2024-12-19 11:01:24.514241: val_loss -0.543
2024-12-19 11:01:24.515158: Pseudo dice [0.7427]
2024-12-19 11:01:24.516048: Epoch time: 91.61 s
2024-12-19 11:01:26.107404: Yayy! New best EMA pseudo Dice: 0.7334
2024-12-19 11:01:27.787747: 
2024-12-19 11:01:27.789443: Epoch 85
2024-12-19 11:01:27.790435: Current learning rate: 0.00471
2024-12-19 11:02:58.193171: Validation loss did not improve from -0.56260. Patience: 2/50
2024-12-19 11:02:58.194193: train_loss -0.7016
2024-12-19 11:02:58.195302: val_loss -0.542
2024-12-19 11:02:58.196220: Pseudo dice [0.7492]
2024-12-19 11:02:58.197146: Epoch time: 90.41 s
2024-12-19 11:02:58.197903: Yayy! New best EMA pseudo Dice: 0.7349
2024-12-19 11:02:59.790911: 
2024-12-19 11:02:59.792853: Epoch 86
2024-12-19 11:02:59.793977: Current learning rate: 0.00465
2024-12-19 11:04:30.213078: Validation loss did not improve from -0.56260. Patience: 3/50
2024-12-19 11:04:30.213923: train_loss -0.6978
2024-12-19 11:04:30.214628: val_loss -0.5394
2024-12-19 11:04:30.215275: Pseudo dice [0.7462]
2024-12-19 11:04:30.216016: Epoch time: 90.42 s
2024-12-19 11:04:30.216608: Yayy! New best EMA pseudo Dice: 0.7361
2024-12-19 11:04:31.963650: 
2024-12-19 11:04:31.965297: Epoch 87
2024-12-19 11:04:31.966208: Current learning rate: 0.00458
2024-12-19 11:06:02.390087: Validation loss did not improve from -0.56260. Patience: 4/50
2024-12-19 11:06:02.391031: train_loss -0.7017
2024-12-19 11:06:02.391980: val_loss -0.5107
2024-12-19 11:06:02.392631: Pseudo dice [0.7213]
2024-12-19 11:06:02.393353: Epoch time: 90.43 s
2024-12-19 11:06:03.628953: 
2024-12-19 11:06:03.629871: Epoch 88
2024-12-19 11:06:03.630485: Current learning rate: 0.00452
2024-12-19 11:07:34.066144: Validation loss did not improve from -0.56260. Patience: 5/50
2024-12-19 11:07:34.067372: train_loss -0.7004
2024-12-19 11:07:34.068211: val_loss -0.5345
2024-12-19 11:07:34.068889: Pseudo dice [0.7338]
2024-12-19 11:07:34.069588: Epoch time: 90.44 s
2024-12-19 11:07:35.330675: 
2024-12-19 11:07:35.332085: Epoch 89
2024-12-19 11:07:35.333346: Current learning rate: 0.00445
2024-12-19 11:09:05.779426: Validation loss did not improve from -0.56260. Patience: 6/50
2024-12-19 11:09:05.780251: train_loss -0.7095
2024-12-19 11:09:05.781306: val_loss -0.5453
2024-12-19 11:09:05.782235: Pseudo dice [0.7458]
2024-12-19 11:09:05.783222: Epoch time: 90.45 s
2024-12-19 11:09:07.431941: 
2024-12-19 11:09:07.433683: Epoch 90
2024-12-19 11:09:07.434655: Current learning rate: 0.00438
2024-12-19 11:10:37.754653: Validation loss did not improve from -0.56260. Patience: 7/50
2024-12-19 11:10:37.755725: train_loss -0.6985
2024-12-19 11:10:37.756525: val_loss -0.5193
2024-12-19 11:10:37.757184: Pseudo dice [0.7335]
2024-12-19 11:10:37.757966: Epoch time: 90.32 s
2024-12-19 11:10:38.968447: 
2024-12-19 11:10:38.970143: Epoch 91
2024-12-19 11:10:38.970946: Current learning rate: 0.00432
2024-12-19 11:12:09.106215: Validation loss did not improve from -0.56260. Patience: 8/50
2024-12-19 11:12:09.107200: train_loss -0.7109
2024-12-19 11:12:09.108140: val_loss -0.5197
2024-12-19 11:12:09.108763: Pseudo dice [0.73]
2024-12-19 11:12:09.109491: Epoch time: 90.14 s
2024-12-19 11:12:10.346547: 
2024-12-19 11:12:10.348172: Epoch 92
2024-12-19 11:12:10.348975: Current learning rate: 0.00425
2024-12-19 11:13:40.515488: Validation loss did not improve from -0.56260. Patience: 9/50
2024-12-19 11:13:40.516761: train_loss -0.7133
2024-12-19 11:13:40.517819: val_loss -0.5181
2024-12-19 11:13:40.518570: Pseudo dice [0.7243]
2024-12-19 11:13:40.519365: Epoch time: 90.17 s
2024-12-19 11:13:41.749172: 
2024-12-19 11:13:41.750520: Epoch 93
2024-12-19 11:13:41.751481: Current learning rate: 0.00419
2024-12-19 11:15:11.924507: Validation loss did not improve from -0.56260. Patience: 10/50
2024-12-19 11:15:11.925625: train_loss -0.7135
2024-12-19 11:15:11.926588: val_loss -0.5379
2024-12-19 11:15:11.927505: Pseudo dice [0.7391]
2024-12-19 11:15:11.928232: Epoch time: 90.18 s
2024-12-19 11:15:13.514605: 
2024-12-19 11:15:13.516644: Epoch 94
2024-12-19 11:15:13.517440: Current learning rate: 0.00412
2024-12-19 11:16:43.613332: Validation loss did not improve from -0.56260. Patience: 11/50
2024-12-19 11:16:43.614482: train_loss -0.7078
2024-12-19 11:16:43.615548: val_loss -0.5615
2024-12-19 11:16:43.616693: Pseudo dice [0.749]
2024-12-19 11:16:43.617836: Epoch time: 90.1 s
2024-12-19 11:16:45.190958: 
2024-12-19 11:16:45.192548: Epoch 95
2024-12-19 11:16:45.193615: Current learning rate: 0.00405
2024-12-19 11:18:15.277254: Validation loss did not improve from -0.56260. Patience: 12/50
2024-12-19 11:18:15.278494: train_loss -0.713
2024-12-19 11:18:15.279657: val_loss -0.5046
2024-12-19 11:18:15.280876: Pseudo dice [0.7258]
2024-12-19 11:18:15.281659: Epoch time: 90.09 s
2024-12-19 11:18:16.518118: 
2024-12-19 11:18:16.519623: Epoch 96
2024-12-19 11:18:16.520714: Current learning rate: 0.00399
2024-12-19 11:19:46.639543: Validation loss did not improve from -0.56260. Patience: 13/50
2024-12-19 11:19:46.640882: train_loss -0.7164
2024-12-19 11:19:46.642584: val_loss -0.5497
2024-12-19 11:19:46.643688: Pseudo dice [0.7476]
2024-12-19 11:19:46.644746: Epoch time: 90.12 s
2024-12-19 11:19:46.645873: Yayy! New best EMA pseudo Dice: 0.7361
2024-12-19 11:19:48.314745: 
2024-12-19 11:19:48.316709: Epoch 97
2024-12-19 11:19:48.317664: Current learning rate: 0.00392
2024-12-19 11:21:18.327415: Validation loss did not improve from -0.56260. Patience: 14/50
2024-12-19 11:21:18.329032: train_loss -0.717
2024-12-19 11:21:18.329750: val_loss -0.5416
2024-12-19 11:21:18.330387: Pseudo dice [0.749]
2024-12-19 11:21:18.331549: Epoch time: 90.02 s
2024-12-19 11:21:18.332230: Yayy! New best EMA pseudo Dice: 0.7374
2024-12-19 11:21:19.902082: 
2024-12-19 11:21:19.903316: Epoch 98
2024-12-19 11:21:19.903969: Current learning rate: 0.00385
2024-12-19 11:22:50.192386: Validation loss improved from -0.56260 to -0.56491! Patience: 14/50
2024-12-19 11:22:50.193111: train_loss -0.7177
2024-12-19 11:22:50.194448: val_loss -0.5649
2024-12-19 11:22:50.195095: Pseudo dice [0.7532]
2024-12-19 11:22:50.196018: Epoch time: 90.29 s
2024-12-19 11:22:50.197376: Yayy! New best EMA pseudo Dice: 0.739
2024-12-19 11:22:52.041421: 
2024-12-19 11:22:52.043243: Epoch 99
2024-12-19 11:22:52.043936: Current learning rate: 0.00379
2024-12-19 11:24:22.371625: Validation loss did not improve from -0.56491. Patience: 1/50
2024-12-19 11:24:22.372948: train_loss -0.7138
2024-12-19 11:24:22.373990: val_loss -0.5361
2024-12-19 11:24:22.374699: Pseudo dice [0.7397]
2024-12-19 11:24:22.375358: Epoch time: 90.33 s
2024-12-19 11:24:22.744306: Yayy! New best EMA pseudo Dice: 0.739
2024-12-19 11:24:24.418555: 
2024-12-19 11:24:24.420259: Epoch 100
2024-12-19 11:24:24.421444: Current learning rate: 0.00372
2024-12-19 11:25:54.706403: Validation loss did not improve from -0.56491. Patience: 2/50
2024-12-19 11:25:54.707580: train_loss -0.7181
2024-12-19 11:25:54.708475: val_loss -0.5297
2024-12-19 11:25:54.709125: Pseudo dice [0.7418]
2024-12-19 11:25:54.709750: Epoch time: 90.29 s
2024-12-19 11:25:54.710311: Yayy! New best EMA pseudo Dice: 0.7393
2024-12-19 11:25:56.383671: 
2024-12-19 11:25:56.384984: Epoch 101
2024-12-19 11:25:56.385700: Current learning rate: 0.00365
2024-12-19 11:27:26.693564: Validation loss did not improve from -0.56491. Patience: 3/50
2024-12-19 11:27:26.694464: train_loss -0.7243
2024-12-19 11:27:26.695143: val_loss -0.5295
2024-12-19 11:27:26.695761: Pseudo dice [0.7327]
2024-12-19 11:27:26.696406: Epoch time: 90.31 s
2024-12-19 11:27:28.241456: 
2024-12-19 11:27:28.242520: Epoch 102
2024-12-19 11:27:28.243228: Current learning rate: 0.00359
2024-12-19 11:28:58.654957: Validation loss did not improve from -0.56491. Patience: 4/50
2024-12-19 11:28:58.655836: train_loss -0.7222
2024-12-19 11:28:58.656647: val_loss -0.55
2024-12-19 11:28:58.657296: Pseudo dice [0.7489]
2024-12-19 11:28:58.657991: Epoch time: 90.42 s
2024-12-19 11:28:58.658616: Yayy! New best EMA pseudo Dice: 0.7397
2024-12-19 11:29:00.293281: 
2024-12-19 11:29:00.295019: Epoch 103
2024-12-19 11:29:00.295907: Current learning rate: 0.00352
2024-12-19 11:30:30.626819: Validation loss did not improve from -0.56491. Patience: 5/50
2024-12-19 11:30:30.627863: train_loss -0.7227
2024-12-19 11:30:30.628967: val_loss -0.5356
2024-12-19 11:30:30.629641: Pseudo dice [0.7445]
2024-12-19 11:30:30.630272: Epoch time: 90.34 s
2024-12-19 11:30:30.630858: Yayy! New best EMA pseudo Dice: 0.7402
2024-12-19 11:30:32.254490: 
2024-12-19 11:30:32.255964: Epoch 104
2024-12-19 11:30:32.256627: Current learning rate: 0.00345
2024-12-19 11:32:02.913538: Validation loss did not improve from -0.56491. Patience: 6/50
2024-12-19 11:32:02.914697: train_loss -0.7257
2024-12-19 11:32:02.915659: val_loss -0.5408
2024-12-19 11:32:02.916467: Pseudo dice [0.7474]
2024-12-19 11:32:02.917147: Epoch time: 90.66 s
2024-12-19 11:32:03.333562: Yayy! New best EMA pseudo Dice: 0.7409
2024-12-19 11:32:04.925947: 
2024-12-19 11:32:04.927741: Epoch 105
2024-12-19 11:32:04.928585: Current learning rate: 0.00338
2024-12-19 11:33:35.250556: Validation loss did not improve from -0.56491. Patience: 7/50
2024-12-19 11:33:35.252013: train_loss -0.7264
2024-12-19 11:33:35.253690: val_loss -0.4869
2024-12-19 11:33:35.254862: Pseudo dice [0.7198]
2024-12-19 11:33:35.256110: Epoch time: 90.33 s
2024-12-19 11:33:36.479921: 
2024-12-19 11:33:36.481580: Epoch 106
2024-12-19 11:33:36.482580: Current learning rate: 0.00332
2024-12-19 11:35:06.978558: Validation loss did not improve from -0.56491. Patience: 8/50
2024-12-19 11:35:06.979928: train_loss -0.7292
2024-12-19 11:35:06.981086: val_loss -0.5348
2024-12-19 11:35:06.982124: Pseudo dice [0.7483]
2024-12-19 11:35:06.982978: Epoch time: 90.5 s
2024-12-19 11:35:08.212188: 
2024-12-19 11:35:08.214090: Epoch 107
2024-12-19 11:35:08.215062: Current learning rate: 0.00325
2024-12-19 11:36:38.647321: Validation loss did not improve from -0.56491. Patience: 9/50
2024-12-19 11:36:38.648197: train_loss -0.7282
2024-12-19 11:36:38.648917: val_loss -0.5548
2024-12-19 11:36:38.649676: Pseudo dice [0.7469]
2024-12-19 11:36:38.650467: Epoch time: 90.44 s
2024-12-19 11:36:39.925746: 
2024-12-19 11:36:39.928315: Epoch 108
2024-12-19 11:36:39.929922: Current learning rate: 0.00318
2024-12-19 11:38:10.198790: Validation loss did not improve from -0.56491. Patience: 10/50
2024-12-19 11:38:10.199967: train_loss -0.7235
2024-12-19 11:38:10.201041: val_loss -0.5584
2024-12-19 11:38:10.202083: Pseudo dice [0.753]
2024-12-19 11:38:10.202912: Epoch time: 90.28 s
2024-12-19 11:38:10.203491: Yayy! New best EMA pseudo Dice: 0.7417
2024-12-19 11:38:11.897348: 
2024-12-19 11:38:11.898943: Epoch 109
2024-12-19 11:38:11.899807: Current learning rate: 0.00311
2024-12-19 11:39:42.007720: Validation loss did not improve from -0.56491. Patience: 11/50
2024-12-19 11:39:42.008719: train_loss -0.7289
2024-12-19 11:39:42.009600: val_loss -0.5533
2024-12-19 11:39:42.010353: Pseudo dice [0.756]
2024-12-19 11:39:42.011093: Epoch time: 90.11 s
2024-12-19 11:39:42.380550: Yayy! New best EMA pseudo Dice: 0.7431
2024-12-19 11:39:43.973841: 
2024-12-19 11:39:43.975683: Epoch 110
2024-12-19 11:39:43.976748: Current learning rate: 0.00304
2024-12-19 11:41:13.961500: Validation loss did not improve from -0.56491. Patience: 12/50
2024-12-19 11:41:13.962620: train_loss -0.7357
2024-12-19 11:41:13.963358: val_loss -0.5397
2024-12-19 11:41:13.964005: Pseudo dice [0.7388]
2024-12-19 11:41:13.964687: Epoch time: 89.99 s
2024-12-19 11:41:15.254443: 
2024-12-19 11:41:15.256189: Epoch 111
2024-12-19 11:41:15.256958: Current learning rate: 0.00297
2024-12-19 11:42:45.406310: Validation loss did not improve from -0.56491. Patience: 13/50
2024-12-19 11:42:45.407459: train_loss -0.7293
2024-12-19 11:42:45.408476: val_loss -0.5526
2024-12-19 11:42:45.409164: Pseudo dice [0.7452]
2024-12-19 11:42:45.409799: Epoch time: 90.15 s
2024-12-19 11:42:46.672526: 
2024-12-19 11:42:46.674369: Epoch 112
2024-12-19 11:42:46.675176: Current learning rate: 0.00291
2024-12-19 11:44:16.885594: Validation loss did not improve from -0.56491. Patience: 14/50
2024-12-19 11:44:16.887073: train_loss -0.7316
2024-12-19 11:44:16.887856: val_loss -0.5451
2024-12-19 11:44:16.888544: Pseudo dice [0.7441]
2024-12-19 11:44:16.889205: Epoch time: 90.22 s
2024-12-19 11:44:18.112254: 
2024-12-19 11:44:18.113595: Epoch 113
2024-12-19 11:44:18.114450: Current learning rate: 0.00284
2024-12-19 11:45:48.268131: Validation loss improved from -0.56491 to -0.56763! Patience: 14/50
2024-12-19 11:45:48.269271: train_loss -0.7334
2024-12-19 11:45:48.270071: val_loss -0.5676
2024-12-19 11:45:48.270870: Pseudo dice [0.7592]
2024-12-19 11:45:48.271525: Epoch time: 90.16 s
2024-12-19 11:45:48.272187: Yayy! New best EMA pseudo Dice: 0.7447
2024-12-19 11:45:49.916436: 
2024-12-19 11:45:49.918066: Epoch 114
2024-12-19 11:45:49.918856: Current learning rate: 0.00277
2024-12-19 11:47:20.022627: Validation loss did not improve from -0.56763. Patience: 1/50
2024-12-19 11:47:20.023907: train_loss -0.7357
2024-12-19 11:47:20.024803: val_loss -0.5525
2024-12-19 11:47:20.025504: Pseudo dice [0.746]
2024-12-19 11:47:20.026142: Epoch time: 90.11 s
2024-12-19 11:47:20.426991: Yayy! New best EMA pseudo Dice: 0.7448
2024-12-19 11:47:22.396560: 
2024-12-19 11:47:22.398293: Epoch 115
2024-12-19 11:47:22.399092: Current learning rate: 0.0027
2024-12-19 11:48:52.438015: Validation loss did not improve from -0.56763. Patience: 2/50
2024-12-19 11:48:52.439009: train_loss -0.7384
2024-12-19 11:48:52.440017: val_loss -0.5393
2024-12-19 11:48:52.440895: Pseudo dice [0.7406]
2024-12-19 11:48:52.441779: Epoch time: 90.04 s
2024-12-19 11:48:53.726451: 
2024-12-19 11:48:53.728151: Epoch 116
2024-12-19 11:48:53.729120: Current learning rate: 0.00263
2024-12-19 11:50:23.860231: Validation loss improved from -0.56763 to -0.57163! Patience: 2/50
2024-12-19 11:50:23.861155: train_loss -0.7385
2024-12-19 11:50:23.862068: val_loss -0.5716
2024-12-19 11:50:23.862855: Pseudo dice [0.7574]
2024-12-19 11:50:23.863710: Epoch time: 90.14 s
2024-12-19 11:50:23.864526: Yayy! New best EMA pseudo Dice: 0.7457
2024-12-19 11:50:25.491280: 
2024-12-19 11:50:25.493388: Epoch 117
2024-12-19 11:50:25.494390: Current learning rate: 0.00256
2024-12-19 11:51:55.767228: Validation loss did not improve from -0.57163. Patience: 1/50
2024-12-19 11:51:55.768394: train_loss -0.7395
2024-12-19 11:51:55.769382: val_loss -0.5494
2024-12-19 11:51:55.770067: Pseudo dice [0.756]
2024-12-19 11:51:55.770681: Epoch time: 90.28 s
2024-12-19 11:51:55.771268: Yayy! New best EMA pseudo Dice: 0.7467
2024-12-19 11:51:57.525213: 
2024-12-19 11:51:57.526940: Epoch 118
2024-12-19 11:51:57.527812: Current learning rate: 0.00249
2024-12-19 11:53:29.149252: Validation loss did not improve from -0.57163. Patience: 2/50
2024-12-19 11:53:29.180039: train_loss -0.7356
2024-12-19 11:53:29.182988: val_loss -0.5477
2024-12-19 11:53:29.184026: Pseudo dice [0.7568]
2024-12-19 11:53:29.185254: Epoch time: 91.66 s
2024-12-19 11:53:29.186310: Yayy! New best EMA pseudo Dice: 0.7477
2024-12-19 11:53:31.002837: 
2024-12-19 11:53:31.004279: Epoch 119
2024-12-19 11:53:31.005321: Current learning rate: 0.00242
2024-12-19 11:55:01.278548: Validation loss did not improve from -0.57163. Patience: 3/50
2024-12-19 11:55:01.279553: train_loss -0.7456
2024-12-19 11:55:01.280314: val_loss -0.5344
2024-12-19 11:55:01.280930: Pseudo dice [0.7426]
2024-12-19 11:55:01.281557: Epoch time: 90.28 s
2024-12-19 11:55:02.960739: 
2024-12-19 11:55:02.962677: Epoch 120
2024-12-19 11:55:02.963477: Current learning rate: 0.00235
2024-12-19 11:56:33.273292: Validation loss did not improve from -0.57163. Patience: 4/50
2024-12-19 11:56:33.274354: train_loss -0.7433
2024-12-19 11:56:33.275234: val_loss -0.5561
2024-12-19 11:56:33.275977: Pseudo dice [0.7588]
2024-12-19 11:56:33.276573: Epoch time: 90.31 s
2024-12-19 11:56:33.277411: Yayy! New best EMA pseudo Dice: 0.7484
2024-12-19 11:56:34.913675: 
2024-12-19 11:56:34.915546: Epoch 121
2024-12-19 11:56:34.916253: Current learning rate: 0.00228
2024-12-19 11:58:05.252723: Validation loss did not improve from -0.57163. Patience: 5/50
2024-12-19 11:58:05.253970: train_loss -0.7444
2024-12-19 11:58:05.254785: val_loss -0.5609
2024-12-19 11:58:05.255699: Pseudo dice [0.761]
2024-12-19 11:58:05.256491: Epoch time: 90.34 s
2024-12-19 11:58:05.257195: Yayy! New best EMA pseudo Dice: 0.7497
2024-12-19 11:58:06.893418: 
2024-12-19 11:58:06.895421: Epoch 122
2024-12-19 11:58:06.896234: Current learning rate: 0.00221
2024-12-19 11:59:37.157917: Validation loss did not improve from -0.57163. Patience: 6/50
2024-12-19 11:59:37.158880: train_loss -0.7463
2024-12-19 11:59:37.159883: val_loss -0.5405
2024-12-19 11:59:37.160783: Pseudo dice [0.7397]
2024-12-19 11:59:37.161620: Epoch time: 90.27 s
2024-12-19 11:59:38.400050: 
2024-12-19 11:59:38.401532: Epoch 123
2024-12-19 11:59:38.403063: Current learning rate: 0.00214
2024-12-19 12:01:08.852357: Validation loss did not improve from -0.57163. Patience: 7/50
2024-12-19 12:01:08.853369: train_loss -0.743
2024-12-19 12:01:08.854293: val_loss -0.536
2024-12-19 12:01:08.854939: Pseudo dice [0.7464]
2024-12-19 12:01:08.855651: Epoch time: 90.45 s
2024-12-19 12:01:10.125507: 
2024-12-19 12:01:10.126882: Epoch 124
2024-12-19 12:01:10.127765: Current learning rate: 0.00207
2024-12-19 12:02:40.579741: Validation loss did not improve from -0.57163. Patience: 8/50
2024-12-19 12:02:40.580766: train_loss -0.7449
2024-12-19 12:02:40.581686: val_loss -0.5439
2024-12-19 12:02:40.582349: Pseudo dice [0.7507]
2024-12-19 12:02:40.582924: Epoch time: 90.46 s
2024-12-19 12:02:42.281462: 
2024-12-19 12:02:42.283153: Epoch 125
2024-12-19 12:02:42.283903: Current learning rate: 0.00199
2024-12-19 12:04:12.553965: Validation loss did not improve from -0.57163. Patience: 9/50
2024-12-19 12:04:12.555003: train_loss -0.7485
2024-12-19 12:04:12.555776: val_loss -0.5385
2024-12-19 12:04:12.556567: Pseudo dice [0.7422]
2024-12-19 12:04:12.557314: Epoch time: 90.27 s
2024-12-19 12:04:14.241814: 
2024-12-19 12:04:14.243397: Epoch 126
2024-12-19 12:04:14.244458: Current learning rate: 0.00192
2024-12-19 12:05:46.447393: Validation loss did not improve from -0.57163. Patience: 10/50
2024-12-19 12:05:46.462295: train_loss -0.7474
2024-12-19 12:05:46.463871: val_loss -0.5656
2024-12-19 12:05:46.464884: Pseudo dice [0.7589]
2024-12-19 12:05:46.465628: Epoch time: 92.21 s
2024-12-19 12:05:48.223927: 
2024-12-19 12:05:48.225550: Epoch 127
2024-12-19 12:05:48.226300: Current learning rate: 0.00185
2024-12-19 12:07:30.783553: Validation loss did not improve from -0.57163. Patience: 11/50
2024-12-19 12:07:30.784588: train_loss -0.7506
2024-12-19 12:07:30.785822: val_loss -0.5544
2024-12-19 12:07:30.786835: Pseudo dice [0.754]
2024-12-19 12:07:30.787835: Epoch time: 102.56 s
2024-12-19 12:07:32.244804: 
2024-12-19 12:07:32.246309: Epoch 128
2024-12-19 12:07:32.247282: Current learning rate: 0.00178
2024-12-19 12:09:37.605320: Validation loss did not improve from -0.57163. Patience: 12/50
2024-12-19 12:09:37.606478: train_loss -0.752
2024-12-19 12:09:37.607369: val_loss -0.5593
2024-12-19 12:09:37.608021: Pseudo dice [0.7498]
2024-12-19 12:09:37.608774: Epoch time: 125.36 s
2024-12-19 12:09:38.988694: 
2024-12-19 12:09:38.990108: Epoch 129
2024-12-19 12:09:38.990835: Current learning rate: 0.0017
2024-12-19 12:11:59.571737: Validation loss did not improve from -0.57163. Patience: 13/50
2024-12-19 12:11:59.572760: train_loss -0.7488
2024-12-19 12:11:59.573574: val_loss -0.5533
2024-12-19 12:11:59.574199: Pseudo dice [0.7502]
2024-12-19 12:11:59.575014: Epoch time: 140.59 s
2024-12-19 12:12:00.040299: Yayy! New best EMA pseudo Dice: 0.7497
2024-12-19 12:12:02.024113: 
2024-12-19 12:12:02.025650: Epoch 130
2024-12-19 12:12:02.026423: Current learning rate: 0.00163
2024-12-19 12:14:40.634200: Validation loss did not improve from -0.57163. Patience: 14/50
2024-12-19 12:14:40.635030: train_loss -0.7505
2024-12-19 12:14:40.635842: val_loss -0.5384
2024-12-19 12:14:40.636697: Pseudo dice [0.7407]
2024-12-19 12:14:40.637470: Epoch time: 158.61 s
2024-12-19 12:14:42.046578: 
2024-12-19 12:14:42.047960: Epoch 131
2024-12-19 12:14:42.048753: Current learning rate: 0.00156
2024-12-19 12:17:43.728462: Validation loss did not improve from -0.57163. Patience: 15/50
2024-12-19 12:17:43.730678: train_loss -0.7531
2024-12-19 12:17:43.731727: val_loss -0.5218
2024-12-19 12:17:43.732513: Pseudo dice [0.7341]
2024-12-19 12:17:43.733289: Epoch time: 181.69 s
2024-12-19 12:17:45.133149: 
2024-12-19 12:17:45.134476: Epoch 132
2024-12-19 12:17:45.135392: Current learning rate: 0.00148
2024-12-19 12:20:55.707613: Validation loss did not improve from -0.57163. Patience: 16/50
2024-12-19 12:20:55.708411: train_loss -0.754
2024-12-19 12:20:55.709359: val_loss -0.5469
2024-12-19 12:20:55.710044: Pseudo dice [0.7392]
2024-12-19 12:20:55.710877: Epoch time: 190.58 s
2024-12-19 12:20:57.085733: 
2024-12-19 12:20:57.086783: Epoch 133
2024-12-19 12:20:57.087610: Current learning rate: 0.00141
2024-12-19 12:24:09.698798: Validation loss did not improve from -0.57163. Patience: 17/50
2024-12-19 12:24:09.699755: train_loss -0.7539
2024-12-19 12:24:09.700509: val_loss -0.5486
2024-12-19 12:24:09.701208: Pseudo dice [0.747]
2024-12-19 12:24:09.701834: Epoch time: 192.62 s
2024-12-19 12:24:11.092854: 
2024-12-19 12:24:11.094233: Epoch 134
2024-12-19 12:24:11.095026: Current learning rate: 0.00133
2024-12-19 12:27:31.366877: Validation loss did not improve from -0.57163. Patience: 18/50
2024-12-19 12:27:31.367932: train_loss -0.7542
2024-12-19 12:27:31.368780: val_loss -0.5119
2024-12-19 12:27:31.369468: Pseudo dice [0.73]
2024-12-19 12:27:31.370185: Epoch time: 200.28 s
2024-12-19 12:27:33.185916: 
2024-12-19 12:27:33.187262: Epoch 135
2024-12-19 12:27:33.187989: Current learning rate: 0.00126
2024-12-19 12:31:08.263521: Validation loss did not improve from -0.57163. Patience: 19/50
2024-12-19 12:31:08.264522: train_loss -0.7544
2024-12-19 12:31:08.265302: val_loss -0.5516
2024-12-19 12:31:08.266043: Pseudo dice [0.7475]
2024-12-19 12:31:08.266622: Epoch time: 215.08 s
2024-12-19 12:31:10.084439: 
2024-12-19 12:31:10.085694: Epoch 136
2024-12-19 12:31:10.086620: Current learning rate: 0.00118
2024-12-19 12:34:35.551720: Validation loss did not improve from -0.57163. Patience: 20/50
2024-12-19 12:34:35.552571: train_loss -0.7617
2024-12-19 12:34:35.553424: val_loss -0.4922
2024-12-19 12:34:35.554070: Pseudo dice [0.7199]
2024-12-19 12:34:35.554760: Epoch time: 205.47 s
2024-12-19 12:34:37.032783: 
2024-12-19 12:34:37.033975: Epoch 137
2024-12-19 12:34:37.034791: Current learning rate: 0.00111
2024-12-19 12:38:19.510117: Validation loss did not improve from -0.57163. Patience: 21/50
2024-12-19 12:38:19.511241: train_loss -0.759
2024-12-19 12:38:19.512099: val_loss -0.5466
2024-12-19 12:38:19.512798: Pseudo dice [0.7495]
2024-12-19 12:38:19.513519: Epoch time: 222.48 s
2024-12-19 12:38:21.022646: 
2024-12-19 12:38:21.024042: Epoch 138
2024-12-19 12:38:21.024893: Current learning rate: 0.00103
2024-12-19 12:41:55.953893: Validation loss did not improve from -0.57163. Patience: 22/50
2024-12-19 12:41:55.954849: train_loss -0.7609
2024-12-19 12:41:55.955805: val_loss -0.5534
2024-12-19 12:41:55.956551: Pseudo dice [0.748]
2024-12-19 12:41:55.957355: Epoch time: 214.93 s
2024-12-19 12:41:57.376329: 
2024-12-19 12:41:57.377702: Epoch 139
2024-12-19 12:41:57.378563: Current learning rate: 0.00095
2024-12-19 12:45:38.168760: Validation loss did not improve from -0.57163. Patience: 23/50
2024-12-19 12:45:38.169947: train_loss -0.7556
2024-12-19 12:45:38.170777: val_loss -0.5672
2024-12-19 12:45:38.171448: Pseudo dice [0.7541]
2024-12-19 12:45:38.172260: Epoch time: 220.79 s
2024-12-19 12:45:40.121294: 
2024-12-19 12:45:40.122969: Epoch 140
2024-12-19 12:45:40.123813: Current learning rate: 0.00087
2024-12-19 12:49:26.291384: Validation loss did not improve from -0.57163. Patience: 24/50
2024-12-19 12:49:26.292456: train_loss -0.7594
2024-12-19 12:49:26.293445: val_loss -0.548
2024-12-19 12:49:26.294432: Pseudo dice [0.751]
2024-12-19 12:49:26.295202: Epoch time: 226.17 s
2024-12-19 12:49:27.832525: 
2024-12-19 12:49:27.833909: Epoch 141
2024-12-19 12:49:27.834669: Current learning rate: 0.00079
2024-12-19 12:53:19.624111: Validation loss did not improve from -0.57163. Patience: 25/50
2024-12-19 12:53:19.625014: train_loss -0.7616
2024-12-19 12:53:19.625752: val_loss -0.5596
2024-12-19 12:53:19.626503: Pseudo dice [0.7529]
2024-12-19 12:53:19.627278: Epoch time: 231.79 s
2024-12-19 12:53:21.176343: 
2024-12-19 12:53:21.177785: Epoch 142
2024-12-19 12:53:21.178927: Current learning rate: 0.00071
2024-12-19 12:56:57.957160: Validation loss did not improve from -0.57163. Patience: 26/50
2024-12-19 12:56:57.958127: train_loss -0.7559
2024-12-19 12:56:57.959137: val_loss -0.5352
2024-12-19 12:56:57.959864: Pseudo dice [0.7429]
2024-12-19 12:56:57.960835: Epoch time: 216.78 s
2024-12-19 12:56:59.473014: 
2024-12-19 12:56:59.474446: Epoch 143
2024-12-19 12:56:59.475215: Current learning rate: 0.00063
2024-12-19 13:00:45.017482: Validation loss did not improve from -0.57163. Patience: 27/50
2024-12-19 13:00:45.019395: train_loss -0.7637
2024-12-19 13:00:45.021108: val_loss -0.5445
2024-12-19 13:00:45.021998: Pseudo dice [0.749]
2024-12-19 13:00:45.023072: Epoch time: 225.55 s
2024-12-19 13:00:46.584210: 
2024-12-19 13:00:46.585577: Epoch 144
2024-12-19 13:00:46.586481: Current learning rate: 0.00055
2024-12-19 13:04:16.357081: Validation loss did not improve from -0.57163. Patience: 28/50
2024-12-19 13:04:16.358062: train_loss -0.7606
2024-12-19 13:04:16.359165: val_loss -0.5232
2024-12-19 13:04:16.359988: Pseudo dice [0.7314]
2024-12-19 13:04:16.360922: Epoch time: 209.78 s
2024-12-19 13:04:18.220344: 
2024-12-19 13:04:18.221886: Epoch 145
2024-12-19 13:04:18.222901: Current learning rate: 0.00047
2024-12-19 13:08:05.152253: Validation loss did not improve from -0.57163. Patience: 29/50
2024-12-19 13:08:05.153260: train_loss -0.763
2024-12-19 13:08:05.154092: val_loss -0.5332
2024-12-19 13:08:05.154844: Pseudo dice [0.7422]
2024-12-19 13:08:05.155694: Epoch time: 226.93 s
2024-12-19 13:08:06.745328: 
2024-12-19 13:08:06.746852: Epoch 146
2024-12-19 13:08:06.747668: Current learning rate: 0.00038
2024-12-19 13:12:15.919799: Validation loss did not improve from -0.57163. Patience: 30/50
2024-12-19 13:12:15.951396: train_loss -0.7652
2024-12-19 13:12:15.952766: val_loss -0.536
2024-12-19 13:12:15.953886: Pseudo dice [0.7398]
2024-12-19 13:12:15.954713: Epoch time: 249.21 s
2024-12-19 13:12:17.914095: 
2024-12-19 13:12:17.915575: Epoch 147
2024-12-19 13:12:17.916459: Current learning rate: 0.0003
2024-12-19 13:16:19.765881: Validation loss did not improve from -0.57163. Patience: 31/50
2024-12-19 13:16:19.766943: train_loss -0.7667
2024-12-19 13:16:19.767894: val_loss -0.5521
2024-12-19 13:16:19.768727: Pseudo dice [0.7469]
2024-12-19 13:16:19.769575: Epoch time: 241.85 s
2024-12-19 13:16:21.344917: 
2024-12-19 13:16:21.346352: Epoch 148
2024-12-19 13:16:21.347236: Current learning rate: 0.00021
2024-12-19 13:20:25.015687: Validation loss did not improve from -0.57163. Patience: 32/50
2024-12-19 13:20:25.016726: train_loss -0.7684
2024-12-19 13:20:25.017574: val_loss -0.5446
2024-12-19 13:20:25.018543: Pseudo dice [0.7476]
2024-12-19 13:20:25.019528: Epoch time: 243.67 s
2024-12-19 13:20:26.628229: 
2024-12-19 13:20:26.629762: Epoch 149
2024-12-19 13:20:26.630786: Current learning rate: 0.00011
2024-12-19 13:24:42.479329: Validation loss did not improve from -0.57163. Patience: 33/50
2024-12-19 13:24:42.480335: train_loss -0.763
2024-12-19 13:24:42.481246: val_loss -0.5561
2024-12-19 13:24:42.482126: Pseudo dice [0.7463]
2024-12-19 13:24:42.482985: Epoch time: 255.85 s
2024-12-19 13:24:44.498117: Training done.
2024-12-19 13:22:14.110170: Using splits from existing split file: /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset308_Sohee_Calcium_OCT_CrossValidation/splits_final.json
2024-12-19 13:22:14.144030: The split file contains 5 splits.
2024-12-19 13:22:14.144904: Desired fold for training: 0
2024-12-19 13:22:14.145640: This split has 6 training and 2 validation cases.
2024-12-19 13:22:14.146588: predicting 106-002
2024-12-19 13:22:14.333362: 106-002, shape torch.Size([1, 539, 498, 498]), rank 0
2024-12-19 13:25:09.906587: predicting 706-005
2024-12-19 13:25:09.921634: 706-005, shape torch.Size([1, 375, 498, 498]), rank 0
2024-12-19 13:27:29.188788: Validation complete
2024-12-19 13:27:29.189647: Mean Validation Dice:  0.7522326191728987
2024-12-19 13:24:44.648448: Using splits from existing split file: /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset308_Sohee_Calcium_OCT_CrossValidation/splits_final.json
2024-12-19 13:24:44.650336: The split file contains 5 splits.
2024-12-19 13:24:44.650996: Desired fold for training: 1
2024-12-19 13:24:44.651641: This split has 6 training and 2 validation cases.
2024-12-19 13:24:44.652700: predicting 101-019
2024-12-19 13:24:44.671152: 101-019, shape torch.Size([1, 375, 498, 498]), rank 0
2024-12-19 13:27:06.737910: predicting 401-004
2024-12-19 13:27:06.752032: 401-004, shape torch.Size([1, 375, 498, 498]), rank 0
2024-12-19 13:29:27.944828: Validation complete
2024-12-19 13:29:27.946135: Mean Validation Dice:  0.7412386325249443

############################
INFO: You are using the old nnU-Net default plans. We have updated our recommendations. Please consider using those instead! Read more here: https://github.com/MIC-DKFZ/nnUNet/blob/master/documentation/resenc_presets.md
############################

Using device: cuda:0

#######################################################################
Please cite the following paper when using nnU-Net:
Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.
#######################################################################

2024-12-19 13:29:34.443596: Using torch.compile...

############################
INFO: You are using the old nnU-Net default plans. We have updated our recommendations. Please consider using those instead! Read more here: https://github.com/MIC-DKFZ/nnUNet/blob/master/documentation/resenc_presets.md
############################

Using device: cuda:0

#######################################################################
Please cite the following paper when using nnU-Net:
Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.
#######################################################################

2024-12-19 13:29:34.445147: Using torch.compile...
/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
################### Loading pretrained weights from file  /home/gridsan/nchutisilp/projects/OpenAI-CLIP/clip_pretrained_nnUNet.pt ###################
Below is the list of overlapping blocks in pretrained model and nnUNet architecture:
encoder.stages.0.0.convs.0.conv.weight shape torch.Size([32, 1, 3, 3, 3])
encoder.stages.0.0.convs.0.conv.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.norm.weight shape torch.Size([32])
encoder.stages.0.0.convs.0.norm.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.0.weight shape torch.Size([32, 1, 3, 3, 3])
encoder.stages.0.0.convs.0.all_modules.0.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.1.weight shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.1.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
encoder.stages.0.0.convs.1.conv.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.norm.weight shape torch.Size([32])
encoder.stages.0.0.convs.1.norm.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
encoder.stages.0.0.convs.1.all_modules.0.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.1.weight shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.1.bias shape torch.Size([32])
encoder.stages.1.0.convs.0.conv.weight shape torch.Size([64, 32, 3, 3, 3])
encoder.stages.1.0.convs.0.conv.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.norm.weight shape torch.Size([64])
encoder.stages.1.0.convs.0.norm.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.0.weight shape torch.Size([64, 32, 3, 3, 3])
encoder.stages.1.0.convs.0.all_modules.0.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.1.weight shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.1.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
encoder.stages.1.0.convs.1.conv.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.norm.weight shape torch.Size([64])
encoder.stages.1.0.convs.1.norm.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
encoder.stages.1.0.convs.1.all_modules.0.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.1.weight shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.1.bias shape torch.Size([64])
encoder.stages.2.0.convs.0.conv.weight shape torch.Size([128, 64, 3, 3, 3])
encoder.stages.2.0.convs.0.conv.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.norm.weight shape torch.Size([128])
encoder.stages.2.0.convs.0.norm.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.0.weight shape torch.Size([128, 64, 3, 3, 3])
encoder.stages.2.0.convs.0.all_modules.0.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.1.weight shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.1.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
encoder.stages.2.0.convs.1.conv.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.norm.weight shape torch.Size([128])
encoder.stages.2.0.convs.1.norm.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
encoder.stages.2.0.convs.1.all_modules.0.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.1.weight shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.1.bias shape torch.Size([128])
encoder.stages.3.0.convs.0.conv.weight shape torch.Size([256, 128, 3, 3, 3])
encoder.stages.3.0.convs.0.conv.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.norm.weight shape torch.Size([256])
encoder.stages.3.0.convs.0.norm.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.0.weight shape torch.Size([256, 128, 3, 3, 3])
encoder.stages.3.0.convs.0.all_modules.0.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.1.weight shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.1.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
encoder.stages.3.0.convs.1.conv.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.norm.weight shape torch.Size([256])
encoder.stages.3.0.convs.1.norm.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
encoder.stages.3.0.convs.1.all_modules.0.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.1.weight shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.1.bias shape torch.Size([256])
encoder.stages.4.0.convs.0.conv.weight shape torch.Size([320, 256, 3, 3, 3])
encoder.stages.4.0.convs.0.conv.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.norm.weight shape torch.Size([320])
encoder.stages.4.0.convs.0.norm.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.0.weight shape torch.Size([320, 256, 3, 3, 3])
encoder.stages.4.0.convs.0.all_modules.0.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.1.weight shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.1.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.4.0.convs.1.conv.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.norm.weight shape torch.Size([320])
encoder.stages.4.0.convs.1.norm.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.4.0.convs.1.all_modules.0.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.1.weight shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.1.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.0.conv.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.norm.weight shape torch.Size([320])
encoder.stages.5.0.convs.0.norm.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.0.all_modules.0.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.1.weight shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.1.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.1.conv.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.norm.weight shape torch.Size([320])
encoder.stages.5.0.convs.1.norm.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.1.all_modules.0.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.1.weight shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.0.0.convs.0.conv.weight shape torch.Size([32, 1, 3, 3, 3])
decoder.encoder.stages.0.0.convs.0.conv.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.norm.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.norm.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.0.weight shape torch.Size([32, 1, 3, 3, 3])
decoder.encoder.stages.0.0.convs.0.all_modules.0.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.1.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.1.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.encoder.stages.0.0.convs.1.conv.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.norm.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.norm.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.encoder.stages.0.0.convs.1.all_modules.0.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.1.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.1.bias shape torch.Size([32])
decoder.encoder.stages.1.0.convs.0.conv.weight shape torch.Size([64, 32, 3, 3, 3])
decoder.encoder.stages.1.0.convs.0.conv.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.norm.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.norm.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.0.weight shape torch.Size([64, 32, 3, 3, 3])
decoder.encoder.stages.1.0.convs.0.all_modules.0.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.1.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.1.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.encoder.stages.1.0.convs.1.conv.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.norm.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.norm.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.encoder.stages.1.0.convs.1.all_modules.0.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.1.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.1.bias shape torch.Size([64])
decoder.encoder.stages.2.0.convs.0.conv.weight shape torch.Size([128, 64, 3, 3, 3])
decoder.encoder.stages.2.0.convs.0.conv.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.norm.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.norm.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.0.weight shape torch.Size([128, 64, 3, 3, 3])
decoder.encoder.stages.2.0.convs.0.all_modules.0.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.1.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.1.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.encoder.stages.2.0.convs.1.conv.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.norm.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.norm.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.encoder.stages.2.0.convs.1.all_modules.0.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.1.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.1.bias shape torch.Size([128])
decoder.encoder.stages.3.0.convs.0.conv.weight shape torch.Size([256, 128, 3, 3, 3])
decoder.encoder.stages.3.0.convs.0.conv.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.norm.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.norm.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.0.weight shape torch.Size([256, 128, 3, 3, 3])
decoder.encoder.stages.3.0.convs.0.all_modules.0.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.1.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.1.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.encoder.stages.3.0.convs.1.conv.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.norm.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.norm.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.encoder.stages.3.0.convs.1.all_modules.0.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.1.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.1.bias shape torch.Size([256])
decoder.encoder.stages.4.0.convs.0.conv.weight shape torch.Size([320, 256, 3, 3, 3])
decoder.encoder.stages.4.0.convs.0.conv.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.norm.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.norm.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.0.weight shape torch.Size([320, 256, 3, 3, 3])
decoder.encoder.stages.4.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.4.0.convs.1.conv.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.norm.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.norm.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.4.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.0.conv.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.norm.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.norm.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.1.conv.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.norm.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.norm.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.stages.0.convs.0.conv.weight shape torch.Size([320, 640, 3, 3, 3])
decoder.stages.0.convs.0.conv.bias shape torch.Size([320])
decoder.stages.0.convs.0.norm.weight shape torch.Size([320])
decoder.stages.0.convs.0.norm.bias shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.0.weight shape torch.Size([320, 640, 3, 3, 3])
decoder.stages.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.stages.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.stages.0.convs.1.conv.bias shape torch.Size([320])
decoder.stages.0.convs.1.norm.weight shape torch.Size([320])
decoder.stages.0.convs.1.norm.bias shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.stages.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.stages.1.convs.0.conv.weight shape torch.Size([256, 512, 3, 3, 3])
decoder.stages.1.convs.0.conv.bias shape torch.Size([256])
decoder.stages.1.convs.0.norm.weight shape torch.Size([256])
decoder.stages.1.convs.0.norm.bias shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.0.weight shape torch.Size([256, 512, 3, 3, 3])
decoder.stages.1.convs.0.all_modules.0.bias shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.1.weight shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.1.bias shape torch.Size([256])
decoder.stages.1.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.stages.1.convs.1.conv.bias shape torch.Size([256])
decoder.stages.1.convs.1.norm.weight shape torch.Size([256])
decoder.stages.1.convs.1.norm.bias shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.stages.1.convs.1.all_modules.0.bias shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.1.weight shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.1.bias shape torch.Size([256])
decoder.stages.2.convs.0.conv.weight shape torch.Size([128, 256, 3, 3, 3])
decoder.stages.2.convs.0.conv.bias shape torch.Size([128])
decoder.stages.2.convs.0.norm.weight shape torch.Size([128])
decoder.stages.2.convs.0.norm.bias shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.0.weight shape torch.Size([128, 256, 3, 3, 3])
decoder.stages.2.convs.0.all_modules.0.bias shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.1.weight shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.1.bias shape torch.Size([128])
decoder.stages.2.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.stages.2.convs.1.conv.bias shape torch.Size([128])
decoder.stages.2.convs.1.norm.weight shape torch.Size([128])
decoder.stages.2.convs.1.norm.bias shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.stages.2.convs.1.all_modules.0.bias shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.1.weight shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.1.bias shape torch.Size([128])
decoder.stages.3.convs.0.conv.weight shape torch.Size([64, 128, 3, 3, 3])
decoder.stages.3.convs.0.conv.bias shape torch.Size([64])
decoder.stages.3.convs.0.norm.weight shape torch.Size([64])
decoder.stages.3.convs.0.norm.bias shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.0.weight shape torch.Size([64, 128, 3, 3, 3])
decoder.stages.3.convs.0.all_modules.0.bias shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.1.weight shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.1.bias shape torch.Size([64])
decoder.stages.3.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.stages.3.convs.1.conv.bias shape torch.Size([64])
decoder.stages.3.convs.1.norm.weight shape torch.Size([64])
decoder.stages.3.convs.1.norm.bias shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.stages.3.convs.1.all_modules.0.bias shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.1.weight shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.1.bias shape torch.Size([64])
decoder.stages.4.convs.0.conv.weight shape torch.Size([32, 64, 3, 3, 3])
decoder.stages.4.convs.0.conv.bias shape torch.Size([32])
decoder.stages.4.convs.0.norm.weight shape torch.Size([32])
decoder.stages.4.convs.0.norm.bias shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.0.weight shape torch.Size([32, 64, 3, 3, 3])
decoder.stages.4.convs.0.all_modules.0.bias shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.1.weight shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.1.bias shape torch.Size([32])
decoder.stages.4.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.stages.4.convs.1.conv.bias shape torch.Size([32])
decoder.stages.4.convs.1.norm.weight shape torch.Size([32])
decoder.stages.4.convs.1.norm.bias shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.stages.4.convs.1.all_modules.0.bias shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.1.weight shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.1.bias shape torch.Size([32])
decoder.transpconvs.0.weight shape torch.Size([320, 320, 1, 2, 2])
decoder.transpconvs.0.bias shape torch.Size([320])
decoder.transpconvs.1.weight shape torch.Size([320, 256, 2, 2, 2])
decoder.transpconvs.1.bias shape torch.Size([256])
decoder.transpconvs.2.weight shape torch.Size([256, 128, 2, 2, 2])
decoder.transpconvs.2.bias shape torch.Size([128])
decoder.transpconvs.3.weight shape torch.Size([128, 64, 2, 2, 2])
decoder.transpconvs.3.bias shape torch.Size([64])
decoder.transpconvs.4.weight shape torch.Size([64, 32, 2, 2, 2])
decoder.transpconvs.4.bias shape torch.Size([32])
################### Done ###################
2024-12-19 13:29:42.231430: do_dummy_2d_data_aug: True
2024-12-19 13:29:42.233981: Using splits from existing split file: /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset308_Sohee_Calcium_OCT_CrossValidation/splits_final.json
2024-12-19 13:29:42.236001: The split file contains 5 splits.
2024-12-19 13:29:42.236827: Desired fold for training: 3
2024-12-19 13:29:42.237727: This split has 7 training and 1 validation cases.
################### Loading pretrained weights from file  /home/gridsan/nchutisilp/projects/OpenAI-CLIP/clip_pretrained_nnUNet.pt ###################
Below is the list of overlapping blocks in pretrained model and nnUNet architecture:
encoder.stages.0.0.convs.0.conv.weight shape torch.Size([32, 1, 3, 3, 3])
encoder.stages.0.0.convs.0.conv.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.norm.weight shape torch.Size([32])
encoder.stages.0.0.convs.0.norm.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.0.weight shape torch.Size([32, 1, 3, 3, 3])
encoder.stages.0.0.convs.0.all_modules.0.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.1.weight shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.1.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
encoder.stages.0.0.convs.1.conv.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.norm.weight shape torch.Size([32])
encoder.stages.0.0.convs.1.norm.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
encoder.stages.0.0.convs.1.all_modules.0.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.1.weight shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.1.bias shape torch.Size([32])
encoder.stages.1.0.convs.0.conv.weight shape torch.Size([64, 32, 3, 3, 3])
encoder.stages.1.0.convs.0.conv.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.norm.weight shape torch.Size([64])
encoder.stages.1.0.convs.0.norm.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.0.weight shape torch.Size([64, 32, 3, 3, 3])
encoder.stages.1.0.convs.0.all_modules.0.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.1.weight shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.1.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
encoder.stages.1.0.convs.1.conv.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.norm.weight shape torch.Size([64])
encoder.stages.1.0.convs.1.norm.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
encoder.stages.1.0.convs.1.all_modules.0.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.1.weight shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.1.bias shape torch.Size([64])
encoder.stages.2.0.convs.0.conv.weight shape torch.Size([128, 64, 3, 3, 3])
encoder.stages.2.0.convs.0.conv.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.norm.weight shape torch.Size([128])
encoder.stages.2.0.convs.0.norm.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.0.weight shape torch.Size([128, 64, 3, 3, 3])
encoder.stages.2.0.convs.0.all_modules.0.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.1.weight shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.1.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
encoder.stages.2.0.convs.1.conv.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.norm.weight shape torch.Size([128])
encoder.stages.2.0.convs.1.norm.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
encoder.stages.2.0.convs.1.all_modules.0.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.1.weight shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.1.bias shape torch.Size([128])
encoder.stages.3.0.convs.0.conv.weight shape torch.Size([256, 128, 3, 3, 3])
encoder.stages.3.0.convs.0.conv.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.norm.weight shape torch.Size([256])
encoder.stages.3.0.convs.0.norm.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.0.weight shape torch.Size([256, 128, 3, 3, 3])
encoder.stages.3.0.convs.0.all_modules.0.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.1.weight shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.1.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
encoder.stages.3.0.convs.1.conv.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.norm.weight shape torch.Size([256])
encoder.stages.3.0.convs.1.norm.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
encoder.stages.3.0.convs.1.all_modules.0.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.1.weight shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.1.bias shape torch.Size([256])
encoder.stages.4.0.convs.0.conv.weight shape torch.Size([320, 256, 3, 3, 3])
encoder.stages.4.0.convs.0.conv.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.norm.weight shape torch.Size([320])
encoder.stages.4.0.convs.0.norm.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.0.weight shape torch.Size([320, 256, 3, 3, 3])
encoder.stages.4.0.convs.0.all_modules.0.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.1.weight shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.1.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.4.0.convs.1.conv.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.norm.weight shape torch.Size([320])
encoder.stages.4.0.convs.1.norm.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.4.0.convs.1.all_modules.0.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.1.weight shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.1.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.0.conv.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.norm.weight shape torch.Size([320])
encoder.stages.5.0.convs.0.norm.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.0.all_modules.0.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.1.weight shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.1.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.1.conv.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.norm.weight shape torch.Size([320])
encoder.stages.5.0.convs.1.norm.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.1.all_modules.0.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.1.weight shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.0.0.convs.0.conv.weight shape torch.Size([32, 1, 3, 3, 3])
decoder.encoder.stages.0.0.convs.0.conv.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.norm.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.norm.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.0.weight shape torch.Size([32, 1, 3, 3, 3])
decoder.encoder.stages.0.0.convs.0.all_modules.0.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.1.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.1.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.encoder.stages.0.0.convs.1.conv.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.norm.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.norm.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.encoder.stages.0.0.convs.1.all_modules.0.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.1.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.1.bias shape torch.Size([32])
decoder.encoder.stages.1.0.convs.0.conv.weight shape torch.Size([64, 32, 3, 3, 3])
decoder.encoder.stages.1.0.convs.0.conv.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.norm.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.norm.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.0.weight shape torch.Size([64, 32, 3, 3, 3])
decoder.encoder.stages.1.0.convs.0.all_modules.0.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.1.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.1.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.encoder.stages.1.0.convs.1.conv.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.norm.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.norm.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.encoder.stages.1.0.convs.1.all_modules.0.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.1.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.1.bias shape torch.Size([64])
decoder.encoder.stages.2.0.convs.0.conv.weight shape torch.Size([128, 64, 3, 3, 3])
decoder.encoder.stages.2.0.convs.0.conv.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.norm.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.norm.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.0.weight shape torch.Size([128, 64, 3, 3, 3])
decoder.encoder.stages.2.0.convs.0.all_modules.0.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.1.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.1.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.encoder.stages.2.0.convs.1.conv.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.norm.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.norm.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.encoder.stages.2.0.convs.1.all_modules.0.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.1.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.1.bias shape torch.Size([128])
decoder.encoder.stages.3.0.convs.0.conv.weight shape torch.Size([256, 128, 3, 3, 3])
decoder.encoder.stages.3.0.convs.0.conv.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.norm.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.norm.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.0.weight shape torch.Size([256, 128, 3, 3, 3])
decoder.encoder.stages.3.0.convs.0.all_modules.0.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.1.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.1.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.encoder.stages.3.0.convs.1.conv.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.norm.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.norm.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.encoder.stages.3.0.convs.1.all_modules.0.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.1.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.1.bias shape torch.Size([256])
decoder.encoder.stages.4.0.convs.0.conv.weight shape torch.Size([320, 256, 3, 3, 3])
decoder.encoder.stages.4.0.convs.0.conv.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.norm.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.norm.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.0.weight shape torch.Size([320, 256, 3, 3, 3])
decoder.encoder.stages.4.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.4.0.convs.1.conv.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.norm.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.norm.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.4.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.0.conv.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.norm.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.norm.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.1.conv.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.norm.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.norm.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.stages.0.convs.0.conv.weight shape torch.Size([320, 640, 3, 3, 3])
decoder.stages.0.convs.0.conv.bias shape torch.Size([320])
decoder.stages.0.convs.0.norm.weight shape torch.Size([320])
decoder.stages.0.convs.0.norm.bias shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.0.weight shape torch.Size([320, 640, 3, 3, 3])
decoder.stages.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.stages.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.stages.0.convs.1.conv.bias shape torch.Size([320])
decoder.stages.0.convs.1.norm.weight shape torch.Size([320])
decoder.stages.0.convs.1.norm.bias shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.stages.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.stages.1.convs.0.conv.weight shape torch.Size([256, 512, 3, 3, 3])
decoder.stages.1.convs.0.conv.bias shape torch.Size([256])
decoder.stages.1.convs.0.norm.weight shape torch.Size([256])
decoder.stages.1.convs.0.norm.bias shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.0.weight shape torch.Size([256, 512, 3, 3, 3])
decoder.stages.1.convs.0.all_modules.0.bias shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.1.weight shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.1.bias shape torch.Size([256])
decoder.stages.1.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.stages.1.convs.1.conv.bias shape torch.Size([256])
decoder.stages.1.convs.1.norm.weight shape torch.Size([256])
decoder.stages.1.convs.1.norm.bias shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.stages.1.convs.1.all_modules.0.bias shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.1.weight shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.1.bias shape torch.Size([256])
decoder.stages.2.convs.0.conv.weight shape torch.Size([128, 256, 3, 3, 3])
decoder.stages.2.convs.0.conv.bias shape torch.Size([128])
decoder.stages.2.convs.0.norm.weight shape torch.Size([128])
decoder.stages.2.convs.0.norm.bias shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.0.weight shape torch.Size([128, 256, 3, 3, 3])
decoder.stages.2.convs.0.all_modules.0.bias shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.1.weight shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.1.bias shape torch.Size([128])
decoder.stages.2.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.stages.2.convs.1.conv.bias shape torch.Size([128])
decoder.stages.2.convs.1.norm.weight shape torch.Size([128])
decoder.stages.2.convs.1.norm.bias shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.stages.2.convs.1.all_modules.0.bias shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.1.weight shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.1.bias shape torch.Size([128])
decoder.stages.3.convs.0.conv.weight shape torch.Size([64, 128, 3, 3, 3])
decoder.stages.3.convs.0.conv.bias shape torch.Size([64])
decoder.stages.3.convs.0.norm.weight shape torch.Size([64])
decoder.stages.3.convs.0.norm.bias shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.0.weight shape torch.Size([64, 128, 3, 3, 3])
decoder.stages.3.convs.0.all_modules.0.bias shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.1.weight shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.1.bias shape torch.Size([64])
decoder.stages.3.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.stages.3.convs.1.conv.bias shape torch.Size([64])
decoder.stages.3.convs.1.norm.weight shape torch.Size([64])
decoder.stages.3.convs.1.norm.bias shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.stages.3.convs.1.all_modules.0.bias shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.1.weight shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.1.bias shape torch.Size([64])
decoder.stages.4.convs.0.conv.weight shape torch.Size([32, 64, 3, 3, 3])
decoder.stages.4.convs.0.conv.bias shape torch.Size([32])
decoder.stages.4.convs.0.norm.weight shape torch.Size([32])
decoder.stages.4.convs.0.norm.bias shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.0.weight shape torch.Size([32, 64, 3, 3, 3])
decoder.stages.4.convs.0.all_modules.0.bias shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.1.weight shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.1.bias shape torch.Size([32])
decoder.stages.4.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.stages.4.convs.1.conv.bias shape torch.Size([32])
decoder.stages.4.convs.1.norm.weight shape torch.Size([32])
decoder.stages.4.convs.1.norm.bias shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.stages.4.convs.1.all_modules.0.bias shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.1.weight shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.1.bias shape torch.Size([32])
decoder.transpconvs.0.weight shape torch.Size([320, 320, 1, 2, 2])
decoder.transpconvs.0.bias shape torch.Size([320])
decoder.transpconvs.1.weight shape torch.Size([320, 256, 2, 2, 2])
decoder.transpconvs.1.bias shape torch.Size([256])
decoder.transpconvs.2.weight shape torch.Size([256, 128, 2, 2, 2])
decoder.transpconvs.2.bias shape torch.Size([128])
decoder.transpconvs.3.weight shape torch.Size([128, 64, 2, 2, 2])
decoder.transpconvs.3.bias shape torch.Size([64])
decoder.transpconvs.4.weight shape torch.Size([64, 32, 2, 2, 2])
decoder.transpconvs.4.bias shape torch.Size([32])
################### Done ###################
2024-12-19 13:29:42.284625: do_dummy_2d_data_aug: True
2024-12-19 13:29:42.286548: Using splits from existing split file: /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset308_Sohee_Calcium_OCT_CrossValidation/splits_final.json
2024-12-19 13:29:42.288329: The split file contains 5 splits.
2024-12-19 13:29:42.289387: Desired fold for training: 2
2024-12-19 13:29:42.290255: This split has 6 training and 2 validation cases.
using pin_memory on device 0
using pin_memory on device 0
using pin_memory on device 0

This is the configuration used by this training:
Configuration name: 3d_32x160x128_b10
 {'data_identifier': 'nnUNetPlans_3d_fullres', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 10, 'patch_size': [32, 160, 128], 'median_image_size_in_voxels': [375.0, 498.0, 498.0], 'spacing': [1.0, 1.0, 1.0], 'normalization_schemes': ['ZScoreNormalization'], 'use_mask_for_norm': [False], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'architecture': {'network_class_name': 'dynamic_network_architectures.architectures.unet.PlainConvUNet', 'arch_kwargs': {'n_stages': 6, 'features_per_stage': [32, 64, 128, 256, 320, 320], 'conv_op': 'torch.nn.modules.conv.Conv3d', 'kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]], 'strides': [[1, 1, 1], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [1, 2, 2]], 'n_conv_per_stage': [2, 2, 2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2, 2, 2], 'conv_bias': True, 'norm_op': 'torch.nn.modules.instancenorm.InstanceNorm3d', 'norm_op_kwargs': {'eps': 1e-05, 'affine': True}, 'dropout_op': None, 'dropout_op_kwargs': None, 'nonlin': 'torch.nn.LeakyReLU', 'nonlin_kwargs': {'inplace': True}, 'deep_supervision': True}, '_kw_requires_import': ['conv_op', 'norm_op', 'dropout_op', 'nonlin']}, 'batch_dice': True, 'inherits_from': '3d_fullres'} 

These are the global plan.json settings:
 {'dataset_name': 'Dataset308_Sohee_Calcium_OCT_CrossValidation', 'plans_name': 'nnUNetPlans', 'original_median_spacing_after_transp': [1.0, 1.0, 1.0], 'original_median_shape_after_transp': [375, 498, 498], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 0.4977976381778717, 'mean': 0.13507525622844696, 'median': 0.09599608182907104, 'min': 0.0, 'percentile_00_5': 0.014754901640117168, 'percentile_99_5': 0.4977976381778717, 'std': 0.12025152146816254}}} 

2024-12-19 13:30:04.841114: unpacking dataset...
using pin_memory on device 0

This is the configuration used by this training:
Configuration name: 3d_32x160x128_b10
 {'data_identifier': 'nnUNetPlans_3d_fullres', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 10, 'patch_size': [32, 160, 128], 'median_image_size_in_voxels': [375.0, 498.0, 498.0], 'spacing': [1.0, 1.0, 1.0], 'normalization_schemes': ['ZScoreNormalization'], 'use_mask_for_norm': [False], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'architecture': {'network_class_name': 'dynamic_network_architectures.architectures.unet.PlainConvUNet', 'arch_kwargs': {'n_stages': 6, 'features_per_stage': [32, 64, 128, 256, 320, 320], 'conv_op': 'torch.nn.modules.conv.Conv3d', 'kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]], 'strides': [[1, 1, 1], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [1, 2, 2]], 'n_conv_per_stage': [2, 2, 2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2, 2, 2], 'conv_bias': True, 'norm_op': 'torch.nn.modules.instancenorm.InstanceNorm3d', 'norm_op_kwargs': {'eps': 1e-05, 'affine': True}, 'dropout_op': None, 'dropout_op_kwargs': None, 'nonlin': 'torch.nn.LeakyReLU', 'nonlin_kwargs': {'inplace': True}, 'deep_supervision': True}, '_kw_requires_import': ['conv_op', 'norm_op', 'dropout_op', 'nonlin']}, 'batch_dice': True, 'inherits_from': '3d_fullres'} 

These are the global plan.json settings:
 {'dataset_name': 'Dataset308_Sohee_Calcium_OCT_CrossValidation', 'plans_name': 'nnUNetPlans', 'original_median_spacing_after_transp': [1.0, 1.0, 1.0], 'original_median_shape_after_transp': [375, 498, 498], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 0.4977976381778717, 'mean': 0.13507525622844696, 'median': 0.09599608182907104, 'min': 0.0, 'percentile_00_5': 0.014754901640117168, 'percentile_99_5': 0.4977976381778717, 'std': 0.12025152146816254}}} 

2024-12-19 13:30:05.400673: unpacking dataset...
2024-12-19 13:30:10.158217: unpacking done...
2024-12-19 13:30:10.530095: Unable to plot network architecture: nnUNet_compile is enabled!
2024-12-19 13:30:10.673345: 
2024-12-19 13:30:10.674488: Epoch 0
2024-12-19 13:30:10.675420: Current learning rate: 0.01
2024-12-19 13:35:10.536595: Validation loss improved from 1000.00000 to -0.16893! Patience: 0/50
2024-12-19 13:35:10.537794: train_loss -0.0916
2024-12-19 13:35:10.538759: val_loss -0.1689
2024-12-19 13:35:10.539663: Pseudo dice [0.5205]
2024-12-19 13:35:10.540339: Epoch time: 299.87 s
2024-12-19 13:35:10.541442: Yayy! New best EMA pseudo Dice: 0.5205
2024-12-19 13:35:12.630468: 
2024-12-19 13:35:12.631696: Epoch 1
2024-12-19 13:35:12.632450: Current learning rate: 0.00994
2024-12-19 13:38:41.296419: Validation loss improved from -0.16893 to -0.23941! Patience: 0/50
2024-12-19 13:38:41.297455: train_loss -0.2457
2024-12-19 13:38:41.298214: val_loss -0.2394
2024-12-19 13:38:41.298896: Pseudo dice [0.5503]
2024-12-19 13:38:41.299562: Epoch time: 208.67 s
2024-12-19 13:38:41.300314: Yayy! New best EMA pseudo Dice: 0.5235
2024-12-19 13:38:43.150417: 
2024-12-19 13:38:43.152074: Epoch 2
2024-12-19 13:38:43.153164: Current learning rate: 0.00988
2024-12-19 13:42:04.224902: Validation loss improved from -0.23941 to -0.27453! Patience: 0/50
2024-12-19 13:42:04.225906: train_loss -0.2817
2024-12-19 13:42:04.227104: val_loss -0.2745
2024-12-19 13:42:04.228069: Pseudo dice [0.5885]
2024-12-19 13:42:04.229007: Epoch time: 201.08 s
2024-12-19 13:42:04.229947: Yayy! New best EMA pseudo Dice: 0.53
2024-12-19 13:42:06.394981: 
2024-12-19 13:42:06.397224: Epoch 3
2024-12-19 13:42:06.398283: Current learning rate: 0.00982
2024-12-19 13:45:33.972794: Validation loss improved from -0.27453 to -0.37041! Patience: 0/50
2024-12-19 13:45:33.973730: train_loss -0.326
2024-12-19 13:45:33.974705: val_loss -0.3704
2024-12-19 13:45:33.975500: Pseudo dice [0.6514]
2024-12-19 13:45:33.976371: Epoch time: 207.58 s
2024-12-19 13:45:33.977304: Yayy! New best EMA pseudo Dice: 0.5421
2024-12-19 13:45:35.851807: 
2024-12-19 13:45:35.853270: Epoch 4
2024-12-19 13:45:35.854308: Current learning rate: 0.00976
2024-12-19 13:49:14.508053: Validation loss did not improve from -0.37041. Patience: 1/50
2024-12-19 13:49:14.509250: train_loss -0.3822
2024-12-19 13:49:14.510200: val_loss -0.2472
2024-12-19 13:49:14.511130: Pseudo dice [0.5756]
2024-12-19 13:49:14.511911: Epoch time: 218.66 s
2024-12-19 13:49:14.888974: Yayy! New best EMA pseudo Dice: 0.5455
2024-12-19 13:49:16.831797: 
2024-12-19 13:49:16.833019: Epoch 5
2024-12-19 13:49:16.833721: Current learning rate: 0.0097
2024-12-19 13:52:59.017495: Validation loss did not improve from -0.37041. Patience: 2/50
2024-12-19 13:52:59.018481: train_loss -0.3831
2024-12-19 13:52:59.019365: val_loss -0.2534
2024-12-19 13:52:59.020049: Pseudo dice [0.596]
2024-12-19 13:52:59.020690: Epoch time: 222.19 s
2024-12-19 13:52:59.021506: Yayy! New best EMA pseudo Dice: 0.5505
2024-12-19 13:53:00.919738: 
2024-12-19 13:53:00.921132: Epoch 6
2024-12-19 13:53:00.922003: Current learning rate: 0.00964
2024-12-19 13:56:48.896598: Validation loss did not improve from -0.37041. Patience: 3/50
2024-12-19 13:56:48.897563: train_loss -0.421
2024-12-19 13:56:48.898402: val_loss -0.24
2024-12-19 13:56:48.899162: Pseudo dice [0.5607]
2024-12-19 13:56:48.899988: Epoch time: 227.98 s
2024-12-19 13:56:48.900709: Yayy! New best EMA pseudo Dice: 0.5516
2024-12-19 13:56:50.767153: 
2024-12-19 13:56:50.768605: Epoch 7
2024-12-19 13:56:50.769730: Current learning rate: 0.00958
2024-12-19 14:00:31.340160: Validation loss did not improve from -0.37041. Patience: 4/50
2024-12-19 14:00:31.341037: train_loss -0.4407
2024-12-19 14:00:31.341940: val_loss -0.2693
2024-12-19 14:00:31.342744: Pseudo dice [0.58]
2024-12-19 14:00:31.343551: Epoch time: 220.58 s
2024-12-19 14:00:31.344378: Yayy! New best EMA pseudo Dice: 0.5544
2024-12-19 14:00:33.185130: 
2024-12-19 14:00:33.186223: Epoch 8
2024-12-19 14:00:33.187026: Current learning rate: 0.00952
2024-12-19 14:04:16.607948: Validation loss improved from -0.37041 to -0.41120! Patience: 4/50
2024-12-19 14:04:16.608875: train_loss -0.4588
2024-12-19 14:04:16.609961: val_loss -0.4112
2024-12-19 14:04:16.610839: Pseudo dice [0.69]
2024-12-19 14:04:16.611714: Epoch time: 223.42 s
2024-12-19 14:04:16.612472: Yayy! New best EMA pseudo Dice: 0.568
2024-12-19 14:04:18.472352: 
2024-12-19 14:04:18.473752: Epoch 9
2024-12-19 14:04:18.474732: Current learning rate: 0.00946
2024-12-19 14:08:03.446597: Validation loss did not improve from -0.41120. Patience: 1/50
2024-12-19 14:08:03.447635: train_loss -0.4689
2024-12-19 14:08:03.448413: val_loss -0.3849
2024-12-19 14:08:03.449157: Pseudo dice [0.6548]
2024-12-19 14:08:03.449995: Epoch time: 224.98 s
2024-12-19 14:08:04.330671: Yayy! New best EMA pseudo Dice: 0.5766
2024-12-19 14:08:06.116390: 
2024-12-19 14:08:06.117862: Epoch 10
2024-12-19 14:08:06.118773: Current learning rate: 0.0094
2024-12-19 14:11:47.037074: Validation loss did not improve from -0.41120. Patience: 2/50
2024-12-19 14:11:47.038148: train_loss -0.4917
2024-12-19 14:11:47.038975: val_loss -0.3701
2024-12-19 14:11:47.039752: Pseudo dice [0.6605]
2024-12-19 14:11:47.040556: Epoch time: 220.92 s
2024-12-19 14:11:47.041314: Yayy! New best EMA pseudo Dice: 0.585
2024-12-19 14:11:48.979174: 
2024-12-19 14:11:48.980426: Epoch 11
2024-12-19 14:11:48.981236: Current learning rate: 0.00934
2024-12-19 14:15:31.146513: Validation loss did not improve from -0.41120. Patience: 3/50
2024-12-19 14:15:31.147444: train_loss -0.4937
2024-12-19 14:15:31.148256: val_loss -0.3785
2024-12-19 14:15:31.148933: Pseudo dice [0.6561]
2024-12-19 14:15:31.149690: Epoch time: 222.17 s
2024-12-19 14:15:31.150419: Yayy! New best EMA pseudo Dice: 0.5921
2024-12-19 14:15:32.998891: 
2024-12-19 14:15:33.000370: Epoch 12
2024-12-19 14:15:33.001225: Current learning rate: 0.00928
2024-12-19 14:18:58.631489: Validation loss improved from -0.41120 to -0.44660! Patience: 3/50
2024-12-19 14:18:58.632487: train_loss -0.5084
2024-12-19 14:18:58.633355: val_loss -0.4466
2024-12-19 14:18:58.634038: Pseudo dice [0.7019]
2024-12-19 14:18:58.634737: Epoch time: 205.64 s
2024-12-19 14:18:58.635454: Yayy! New best EMA pseudo Dice: 0.6031
2024-12-19 14:19:00.638642: 
2024-12-19 14:19:00.640173: Epoch 13
2024-12-19 14:19:00.641156: Current learning rate: 0.00922
2024-12-19 14:22:21.953691: Validation loss did not improve from -0.44660. Patience: 1/50
2024-12-19 14:22:21.954412: train_loss -0.507
2024-12-19 14:22:21.955356: val_loss -0.3471
2024-12-19 14:22:21.956273: Pseudo dice [0.6425]
2024-12-19 14:22:21.957168: Epoch time: 201.32 s
2024-12-19 14:22:21.958045: Yayy! New best EMA pseudo Dice: 0.607
2024-12-19 14:22:23.907255: 
2024-12-19 14:22:23.908753: Epoch 14
2024-12-19 14:22:23.909628: Current learning rate: 0.00916
2024-12-19 14:26:04.980704: Validation loss did not improve from -0.44660. Patience: 2/50
2024-12-19 14:26:04.981555: train_loss -0.5105
2024-12-19 14:26:04.982304: val_loss -0.3365
2024-12-19 14:26:04.983037: Pseudo dice [0.6044]
2024-12-19 14:26:04.983889: Epoch time: 221.08 s
2024-12-19 14:26:06.901452: 
2024-12-19 14:26:06.902554: Epoch 15
2024-12-19 14:26:06.903352: Current learning rate: 0.0091
2024-12-19 14:29:53.606912: Validation loss did not improve from -0.44660. Patience: 3/50
2024-12-19 14:29:53.607657: train_loss -0.5179
2024-12-19 14:29:53.608794: val_loss -0.3735
2024-12-19 14:29:53.609640: Pseudo dice [0.6527]
2024-12-19 14:29:53.610548: Epoch time: 226.71 s
2024-12-19 14:29:53.611483: Yayy! New best EMA pseudo Dice: 0.6114
2024-12-19 14:29:55.560158: 
2024-12-19 14:29:55.561509: Epoch 16
2024-12-19 14:29:55.562439: Current learning rate: 0.00903
2024-12-19 14:33:46.369959: Validation loss improved from -0.44660 to -0.44826! Patience: 3/50
2024-12-19 14:33:46.370794: train_loss -0.5203
2024-12-19 14:33:46.371643: val_loss -0.4483
2024-12-19 14:33:46.372479: Pseudo dice [0.7066]
2024-12-19 14:33:46.373231: Epoch time: 230.81 s
2024-12-19 14:33:46.373907: Yayy! New best EMA pseudo Dice: 0.6209
2024-12-19 14:33:48.277244: 
2024-12-19 14:33:48.278514: Epoch 17
2024-12-19 14:33:48.279280: Current learning rate: 0.00897
2024-12-19 14:37:34.617781: Validation loss did not improve from -0.44826. Patience: 1/50
2024-12-19 14:37:34.621525: train_loss -0.5379
2024-12-19 14:37:34.623525: val_loss -0.3589
2024-12-19 14:37:34.624283: Pseudo dice [0.6435]
2024-12-19 14:37:34.625298: Epoch time: 226.34 s
2024-12-19 14:37:34.626141: Yayy! New best EMA pseudo Dice: 0.6232
2024-12-19 14:37:36.545747: 
2024-12-19 14:37:36.546842: Epoch 18
2024-12-19 14:37:36.547616: Current learning rate: 0.00891
2024-12-19 14:41:15.716826: Validation loss did not improve from -0.44826. Patience: 2/50
2024-12-19 14:41:15.717961: train_loss -0.557
2024-12-19 14:41:15.718852: val_loss -0.3424
2024-12-19 14:41:15.719715: Pseudo dice [0.6303]
2024-12-19 14:41:15.720442: Epoch time: 219.17 s
2024-12-19 14:41:15.721259: Yayy! New best EMA pseudo Dice: 0.6239
2024-12-19 14:41:18.403106: 
2024-12-19 14:41:18.404603: Epoch 19
2024-12-19 14:41:18.405404: Current learning rate: 0.00885
2024-12-19 14:45:03.426489: Validation loss did not improve from -0.44826. Patience: 3/50
2024-12-19 14:45:03.427426: train_loss -0.5457
2024-12-19 14:45:03.428410: val_loss -0.3137
2024-12-19 14:45:03.429250: Pseudo dice [0.6258]
2024-12-19 14:45:03.430028: Epoch time: 225.03 s
2024-12-19 14:45:03.856809: Yayy! New best EMA pseudo Dice: 0.6241
2024-12-19 14:45:05.745684: 
2024-12-19 14:45:05.746931: Epoch 20
2024-12-19 14:45:05.747701: Current learning rate: 0.00879
2024-12-19 14:48:53.977584: Validation loss did not improve from -0.44826. Patience: 4/50
2024-12-19 14:48:53.978235: train_loss -0.5496
2024-12-19 14:48:53.978920: val_loss -0.2833
2024-12-19 14:48:53.979583: Pseudo dice [0.5562]
2024-12-19 14:48:53.980271: Epoch time: 228.23 s
2024-12-19 14:48:55.455702: 
2024-12-19 14:48:55.457032: Epoch 21
2024-12-19 14:48:55.457899: Current learning rate: 0.00873
2024-12-19 14:52:42.083124: Validation loss did not improve from -0.44826. Patience: 5/50
2024-12-19 14:52:42.084206: train_loss -0.5732
2024-12-19 14:52:42.085401: val_loss -0.3469
2024-12-19 14:52:42.086427: Pseudo dice [0.655]
2024-12-19 14:52:42.087311: Epoch time: 226.63 s
2024-12-19 14:52:43.480376: 
2024-12-19 14:52:43.481761: Epoch 22
2024-12-19 14:52:43.482817: Current learning rate: 0.00867
2024-12-19 14:56:30.564225: Validation loss did not improve from -0.44826. Patience: 6/50
2024-12-19 14:56:30.565375: train_loss -0.5668
2024-12-19 14:56:30.566217: val_loss -0.4476
2024-12-19 14:56:30.567037: Pseudo dice [0.7216]
2024-12-19 14:56:30.567873: Epoch time: 227.09 s
2024-12-19 14:56:30.568643: Yayy! New best EMA pseudo Dice: 0.6311
2024-12-19 14:56:32.429899: 
2024-12-19 14:56:32.431128: Epoch 23
2024-12-19 14:56:32.432064: Current learning rate: 0.00861
2024-12-19 15:00:21.180578: Validation loss did not improve from -0.44826. Patience: 7/50
2024-12-19 15:00:21.181629: train_loss -0.5728
2024-12-19 15:00:21.182451: val_loss -0.4244
2024-12-19 15:00:21.183151: Pseudo dice [0.6802]
2024-12-19 15:00:21.183878: Epoch time: 228.75 s
2024-12-19 15:00:21.184684: Yayy! New best EMA pseudo Dice: 0.636
2024-12-19 15:00:23.093079: 
2024-12-19 15:00:23.094433: Epoch 24
2024-12-19 15:00:23.095201: Current learning rate: 0.00855
2024-12-19 15:04:10.326046: Validation loss did not improve from -0.44826. Patience: 8/50
2024-12-19 15:04:10.326989: train_loss -0.5656
2024-12-19 15:04:10.327911: val_loss -0.3763
2024-12-19 15:04:10.328701: Pseudo dice [0.663]
2024-12-19 15:04:10.329503: Epoch time: 227.24 s
2024-12-19 15:04:10.735107: Yayy! New best EMA pseudo Dice: 0.6387
2024-12-19 15:04:12.619420: 
2024-12-19 15:04:12.620776: Epoch 25
2024-12-19 15:04:12.621847: Current learning rate: 0.00849
2024-12-19 15:07:58.681797: Validation loss did not improve from -0.44826. Patience: 9/50
2024-12-19 15:07:58.682678: train_loss -0.5678
2024-12-19 15:07:58.683445: val_loss -0.4365
2024-12-19 15:07:58.684232: Pseudo dice [0.6817]
2024-12-19 15:07:58.685042: Epoch time: 226.06 s
2024-12-19 15:07:58.685961: Yayy! New best EMA pseudo Dice: 0.643
2024-12-19 15:08:00.524729: 
2024-12-19 15:08:00.526206: Epoch 26
2024-12-19 15:08:00.527117: Current learning rate: 0.00843
2024-12-19 15:11:52.418063: Validation loss did not improve from -0.44826. Patience: 10/50
2024-12-19 15:11:52.418977: train_loss -0.5806
2024-12-19 15:11:52.419873: val_loss -0.4315
2024-12-19 15:11:52.420689: Pseudo dice [0.696]
2024-12-19 15:11:52.421594: Epoch time: 231.9 s
2024-12-19 15:11:52.422325: Yayy! New best EMA pseudo Dice: 0.6483
2024-12-19 15:11:54.292013: 
2024-12-19 15:11:54.293328: Epoch 27
2024-12-19 15:11:54.294400: Current learning rate: 0.00836
2024-12-19 15:15:43.236632: Validation loss did not improve from -0.44826. Patience: 11/50
2024-12-19 15:15:43.237644: train_loss -0.5808
2024-12-19 15:15:43.238690: val_loss -0.2954
2024-12-19 15:15:43.239437: Pseudo dice [0.6177]
2024-12-19 15:15:43.240208: Epoch time: 228.95 s
2024-12-19 15:15:44.672441: 
2024-12-19 15:15:44.673814: Epoch 28
2024-12-19 15:15:44.674651: Current learning rate: 0.0083
2024-12-19 15:19:38.900278: Validation loss did not improve from -0.44826. Patience: 12/50
2024-12-19 15:19:38.901192: train_loss -0.5933
2024-12-19 15:19:38.902037: val_loss -0.4273
2024-12-19 15:19:38.902848: Pseudo dice [0.6828]
2024-12-19 15:19:38.903630: Epoch time: 234.23 s
2024-12-19 15:19:38.904364: Yayy! New best EMA pseudo Dice: 0.649
2024-12-19 15:19:41.153041: 
2024-12-19 15:19:41.154313: Epoch 29
2024-12-19 15:19:41.155156: Current learning rate: 0.00824
2024-12-19 15:23:09.130676: Validation loss did not improve from -0.44826. Patience: 13/50
2024-12-19 15:23:09.131649: train_loss -0.5971
2024-12-19 15:23:09.132691: val_loss -0.3849
2024-12-19 15:23:09.133595: Pseudo dice [0.6556]
2024-12-19 15:23:09.134566: Epoch time: 207.98 s
2024-12-19 15:23:09.580671: Yayy! New best EMA pseudo Dice: 0.6497
2024-12-19 15:23:11.478707: 
2024-12-19 15:23:11.480035: Epoch 30
2024-12-19 15:23:11.480873: Current learning rate: 0.00818
2024-12-19 15:26:48.251910: Validation loss did not improve from -0.44826. Patience: 14/50
2024-12-19 15:26:48.253002: train_loss -0.5972
2024-12-19 15:26:48.253997: val_loss -0.3532
2024-12-19 15:26:48.254874: Pseudo dice [0.6482]
2024-12-19 15:26:48.255697: Epoch time: 216.78 s
2024-12-19 15:26:49.732773: 
2024-12-19 15:26:49.734015: Epoch 31
2024-12-19 15:26:49.735030: Current learning rate: 0.00812
2024-12-19 15:30:54.152020: Validation loss did not improve from -0.44826. Patience: 15/50
2024-12-19 15:30:54.153045: train_loss -0.5954
2024-12-19 15:30:54.153880: val_loss -0.2585
2024-12-19 15:30:54.154619: Pseudo dice [0.5683]
2024-12-19 15:30:54.155472: Epoch time: 244.42 s
2024-12-19 15:30:55.646258: 
2024-12-19 15:30:55.647438: Epoch 32
2024-12-19 15:30:55.648272: Current learning rate: 0.00806
2024-12-19 15:35:03.937556: Validation loss improved from -0.44826 to -0.48196! Patience: 15/50
2024-12-19 15:35:03.938498: train_loss -0.6028
2024-12-19 15:35:03.939317: val_loss -0.482
2024-12-19 15:35:03.940058: Pseudo dice [0.7132]
2024-12-19 15:35:03.940794: Epoch time: 248.29 s
2024-12-19 15:35:05.516171: 
2024-12-19 15:35:05.517820: Epoch 33
2024-12-19 15:35:05.518739: Current learning rate: 0.008
2024-12-19 15:39:04.272581: Validation loss did not improve from -0.48196. Patience: 1/50
2024-12-19 15:39:04.273592: train_loss -0.5937
2024-12-19 15:39:04.274367: val_loss -0.3849
2024-12-19 15:39:04.274999: Pseudo dice [0.6652]
2024-12-19 15:39:04.275671: Epoch time: 238.76 s
2024-12-19 15:39:04.276351: Yayy! New best EMA pseudo Dice: 0.6502
2024-12-19 15:39:06.341136: 
2024-12-19 15:39:06.342505: Epoch 34
2024-12-19 15:39:06.343240: Current learning rate: 0.00793
2024-12-19 15:43:03.524032: Validation loss did not improve from -0.48196. Patience: 2/50
2024-12-19 15:43:03.526028: train_loss -0.5981
2024-12-19 15:43:03.528452: val_loss -0.4092
2024-12-19 15:43:03.529335: Pseudo dice [0.6808]
2024-12-19 15:43:03.530480: Epoch time: 237.19 s
2024-12-19 15:43:03.942933: Yayy! New best EMA pseudo Dice: 0.6533
2024-12-19 15:43:05.885059: 
2024-12-19 15:43:05.886487: Epoch 35
2024-12-19 15:43:05.887383: Current learning rate: 0.00787
2024-12-19 15:47:25.950484: Validation loss did not improve from -0.48196. Patience: 3/50
2024-12-19 15:47:25.952046: train_loss -0.6033
2024-12-19 15:47:25.953294: val_loss -0.3371
2024-12-19 15:47:25.953954: Pseudo dice [0.6312]
2024-12-19 15:47:25.954671: Epoch time: 260.07 s
2024-12-19 15:47:27.992065: 
2024-12-19 15:47:27.993443: Epoch 36
2024-12-19 15:47:27.994354: Current learning rate: 0.00781
2024-12-19 15:51:31.578064: Validation loss did not improve from -0.48196. Patience: 4/50
2024-12-19 15:51:31.578764: train_loss -0.6176
2024-12-19 15:51:31.579579: val_loss -0.422
2024-12-19 15:51:31.580308: Pseudo dice [0.6807]
2024-12-19 15:51:31.581066: Epoch time: 243.59 s
2024-12-19 15:51:31.581759: Yayy! New best EMA pseudo Dice: 0.654
2024-12-19 15:51:33.541819: 
2024-12-19 15:51:33.543236: Epoch 37
2024-12-19 15:51:33.544102: Current learning rate: 0.00775
2024-12-19 15:55:24.932355: Validation loss did not improve from -0.48196. Patience: 5/50
2024-12-19 15:55:24.933243: train_loss -0.6068
2024-12-19 15:55:24.934109: val_loss -0.3855
2024-12-19 15:55:24.934914: Pseudo dice [0.6726]
2024-12-19 15:55:24.935742: Epoch time: 231.39 s
2024-12-19 15:55:24.936497: Yayy! New best EMA pseudo Dice: 0.6559
2024-12-19 15:55:26.970744: 
2024-12-19 15:55:26.971933: Epoch 38
2024-12-19 15:55:26.972928: Current learning rate: 0.00769
2024-12-19 15:59:26.961710: Validation loss did not improve from -0.48196. Patience: 6/50
2024-12-19 15:59:26.962687: train_loss -0.6093
2024-12-19 15:59:26.963740: val_loss -0.4181
2024-12-19 15:59:26.964643: Pseudo dice [0.6876]
2024-12-19 15:59:26.965471: Epoch time: 239.99 s
2024-12-19 15:59:26.966239: Yayy! New best EMA pseudo Dice: 0.6591
2024-12-19 15:59:29.324893: 
2024-12-19 15:59:29.326140: Epoch 39
2024-12-19 15:59:29.326974: Current learning rate: 0.00763
2024-12-19 16:03:23.889968: Validation loss did not improve from -0.48196. Patience: 7/50
2024-12-19 16:03:23.891015: train_loss -0.6137
2024-12-19 16:03:23.921748: val_loss -0.3686
2024-12-19 16:03:23.922691: Pseudo dice [0.6513]
2024-12-19 16:03:23.923443: Epoch time: 234.57 s
2024-12-19 16:03:25.922955: 
2024-12-19 16:03:25.924398: Epoch 40
2024-12-19 16:03:25.925347: Current learning rate: 0.00756
2024-12-19 16:07:13.913364: Validation loss did not improve from -0.48196. Patience: 8/50
2024-12-19 16:07:13.914237: train_loss -0.622
2024-12-19 16:07:13.915105: val_loss -0.3614
2024-12-19 16:07:13.915909: Pseudo dice [0.6633]
2024-12-19 16:07:13.916764: Epoch time: 227.99 s
2024-12-19 16:07:15.520321: 
2024-12-19 16:07:15.521633: Epoch 41
2024-12-19 16:07:15.522542: Current learning rate: 0.0075
2024-12-19 16:11:09.207548: Validation loss did not improve from -0.48196. Patience: 9/50
2024-12-19 16:11:09.208565: train_loss -0.6224
2024-12-19 16:11:09.209368: val_loss -0.3661
2024-12-19 16:11:09.210117: Pseudo dice [0.6647]
2024-12-19 16:11:09.210888: Epoch time: 233.69 s
2024-12-19 16:11:09.211607: Yayy! New best EMA pseudo Dice: 0.6594
2024-12-19 16:11:11.141421: 
2024-12-19 16:11:11.142851: Epoch 42
2024-12-19 16:11:11.143568: Current learning rate: 0.00744
2024-12-19 16:15:02.223457: Validation loss did not improve from -0.48196. Patience: 10/50
2024-12-19 16:15:02.224506: train_loss -0.6291
2024-12-19 16:15:02.225381: val_loss -0.3439
2024-12-19 16:15:02.226155: Pseudo dice [0.653]
2024-12-19 16:15:02.226893: Epoch time: 231.08 s
2024-12-19 16:15:03.615073: 
2024-12-19 16:15:03.616458: Epoch 43
2024-12-19 16:15:03.617188: Current learning rate: 0.00738
2024-12-19 16:19:01.069926: Validation loss did not improve from -0.48196. Patience: 11/50
2024-12-19 16:19:01.070787: train_loss -0.6305
2024-12-19 16:19:01.071649: val_loss -0.3676
2024-12-19 16:19:01.072500: Pseudo dice [0.6589]
2024-12-19 16:19:01.073361: Epoch time: 237.46 s
2024-12-19 16:19:02.475157: 
2024-12-19 16:19:02.476355: Epoch 44
2024-12-19 16:19:02.477224: Current learning rate: 0.00732
2024-12-19 16:23:03.511918: Validation loss did not improve from -0.48196. Patience: 12/50
2024-12-19 16:23:03.512927: train_loss -0.6351
2024-12-19 16:23:03.513734: val_loss -0.3483
2024-12-19 16:23:03.514589: Pseudo dice [0.66]
2024-12-19 16:23:03.515287: Epoch time: 241.04 s
2024-12-19 16:23:05.447313: 
2024-12-19 16:23:05.448743: Epoch 45
2024-12-19 16:23:05.449754: Current learning rate: 0.00725
2024-12-19 16:26:47.484353: Validation loss did not improve from -0.48196. Patience: 13/50
2024-12-19 16:26:47.485401: train_loss -0.6384
2024-12-19 16:26:47.486540: val_loss -0.4153
2024-12-19 16:26:47.487432: Pseudo dice [0.6906]
2024-12-19 16:26:47.488361: Epoch time: 222.04 s
2024-12-19 16:26:47.489213: Yayy! New best EMA pseudo Dice: 0.6621
2024-12-19 16:26:49.408330: 
2024-12-19 16:26:49.410010: Epoch 46
2024-12-19 16:26:49.411051: Current learning rate: 0.00719
2024-12-19 16:30:30.398568: Validation loss did not improve from -0.48196. Patience: 14/50
2024-12-19 16:30:30.399403: train_loss -0.6363
2024-12-19 16:30:30.400341: val_loss -0.3588
2024-12-19 16:30:30.401049: Pseudo dice [0.6524]
2024-12-19 16:30:30.401784: Epoch time: 220.99 s
2024-12-19 16:30:31.836189: 
2024-12-19 16:30:31.837172: Epoch 47
2024-12-19 16:30:31.838184: Current learning rate: 0.00713
2024-12-19 16:34:33.023594: Validation loss did not improve from -0.48196. Patience: 15/50
2024-12-19 16:34:33.024699: train_loss -0.641
2024-12-19 16:34:33.025734: val_loss -0.2213
2024-12-19 16:34:33.026655: Pseudo dice [0.5758]
2024-12-19 16:34:33.027469: Epoch time: 241.19 s
2024-12-19 16:34:34.509574: 
2024-12-19 16:34:34.511084: Epoch 48
2024-12-19 16:34:34.512190: Current learning rate: 0.00707
2024-12-19 16:38:29.294453: Validation loss did not improve from -0.48196. Patience: 16/50
2024-12-19 16:38:29.295354: train_loss -0.6471
2024-12-19 16:38:29.296509: val_loss -0.3563
2024-12-19 16:38:29.297650: Pseudo dice [0.6531]
2024-12-19 16:38:29.298931: Epoch time: 234.79 s
2024-12-19 16:38:30.730093: 
2024-12-19 16:38:30.731469: Epoch 49
2024-12-19 16:38:30.732385: Current learning rate: 0.007
2024-12-19 16:41:43.344174: Validation loss did not improve from -0.48196. Patience: 17/50
2024-12-19 16:41:43.347606: train_loss -0.6449
2024-12-19 16:41:43.348552: val_loss -0.3384
2024-12-19 16:41:43.349304: Pseudo dice [0.6518]
2024-12-19 16:41:43.350504: Epoch time: 192.62 s
2024-12-19 16:41:45.966560: 
2024-12-19 16:41:45.967874: Epoch 50
2024-12-19 16:41:45.968652: Current learning rate: 0.00694
2024-12-19 16:45:40.572511: Validation loss did not improve from -0.48196. Patience: 18/50
2024-12-19 16:45:40.574333: train_loss -0.6446
2024-12-19 16:45:40.575545: val_loss -0.3541
2024-12-19 16:45:40.576586: Pseudo dice [0.6549]
2024-12-19 16:45:40.577597: Epoch time: 234.61 s
2024-12-19 16:45:42.038262: 
2024-12-19 16:45:42.039498: Epoch 51
2024-12-19 16:45:42.040417: Current learning rate: 0.00688
2024-12-19 16:49:35.225278: Validation loss did not improve from -0.48196. Patience: 19/50
2024-12-19 16:49:35.226256: train_loss -0.6497
2024-12-19 16:49:35.227067: val_loss -0.3102
2024-12-19 16:49:35.227794: Pseudo dice [0.6192]
2024-12-19 16:49:35.228562: Epoch time: 233.19 s
2024-12-19 16:49:36.684812: 
2024-12-19 16:49:36.686139: Epoch 52
2024-12-19 16:49:36.686983: Current learning rate: 0.00682
2024-12-19 16:53:37.958624: Validation loss did not improve from -0.48196. Patience: 20/50
2024-12-19 16:53:37.960086: train_loss -0.6526
2024-12-19 16:53:37.961385: val_loss -0.1923
2024-12-19 16:53:37.962383: Pseudo dice [0.5506]
2024-12-19 16:53:37.963188: Epoch time: 241.28 s
2024-12-19 16:53:39.397450: 
2024-12-19 16:53:39.398888: Epoch 53
2024-12-19 16:53:39.399980: Current learning rate: 0.00675
2024-12-19 16:57:31.107057: Validation loss did not improve from -0.48196. Patience: 21/50
2024-12-19 16:57:31.108187: train_loss -0.6585
2024-12-19 16:57:31.109348: val_loss -0.4254
2024-12-19 16:57:31.110419: Pseudo dice [0.7027]
2024-12-19 16:57:31.111418: Epoch time: 231.71 s
2024-12-19 16:57:32.554066: 
2024-12-19 16:57:32.555247: Epoch 54
2024-12-19 16:57:32.556105: Current learning rate: 0.00669
2024-12-19 17:01:30.338408: Validation loss did not improve from -0.48196. Patience: 22/50
2024-12-19 17:01:30.339375: train_loss -0.6516
2024-12-19 17:01:30.340286: val_loss -0.1805
2024-12-19 17:01:30.340982: Pseudo dice [0.5496]
2024-12-19 17:01:30.341794: Epoch time: 237.79 s
2024-12-19 17:01:32.313474: 
2024-12-19 17:01:32.314585: Epoch 55
2024-12-19 17:01:32.315367: Current learning rate: 0.00663
2024-12-19 17:05:27.499407: Validation loss did not improve from -0.48196. Patience: 23/50
2024-12-19 17:05:27.500216: train_loss -0.6543
2024-12-19 17:05:27.501396: val_loss -0.446
2024-12-19 17:05:27.502292: Pseudo dice [0.6995]
2024-12-19 17:05:27.503212: Epoch time: 235.19 s
2024-12-19 17:05:28.973127: 
2024-12-19 17:05:28.974630: Epoch 56
2024-12-19 17:05:28.975676: Current learning rate: 0.00657
2024-12-19 17:09:37.678387: Validation loss did not improve from -0.48196. Patience: 24/50
2024-12-19 17:09:37.679355: train_loss -0.6625
2024-12-19 17:09:37.680121: val_loss -0.2706
2024-12-19 17:09:37.680905: Pseudo dice [0.609]
2024-12-19 17:09:37.681619: Epoch time: 248.71 s
2024-12-19 17:09:39.200295: 
2024-12-19 17:09:39.201510: Epoch 57
2024-12-19 17:09:39.202296: Current learning rate: 0.0065
2024-12-19 17:13:21.564451: Validation loss did not improve from -0.48196. Patience: 25/50
2024-12-19 17:13:21.565271: train_loss -0.6632
2024-12-19 17:13:21.566067: val_loss -0.4158
2024-12-19 17:13:21.566779: Pseudo dice [0.6899]
2024-12-19 17:13:21.567610: Epoch time: 222.37 s
2024-12-19 17:13:22.994075: 
2024-12-19 17:13:22.995352: Epoch 58
2024-12-19 17:13:22.996102: Current learning rate: 0.00644
2024-12-19 17:17:22.610657: Validation loss did not improve from -0.48196. Patience: 26/50
2024-12-19 17:17:22.612667: train_loss -0.6613
2024-12-19 17:17:22.613826: val_loss -0.4153
2024-12-19 17:17:22.614756: Pseudo dice [0.6759]
2024-12-19 17:17:22.615744: Epoch time: 239.62 s
2024-12-19 17:17:24.078844: 
2024-12-19 17:17:24.080272: Epoch 59
2024-12-19 17:17:24.081243: Current learning rate: 0.00638
2024-12-19 17:21:06.727701: Validation loss did not improve from -0.48196. Patience: 27/50
2024-12-19 17:21:06.728778: train_loss -0.6657
2024-12-19 17:21:06.729781: val_loss -0.3581
2024-12-19 17:21:06.730617: Pseudo dice [0.6584]
2024-12-19 17:21:06.731481: Epoch time: 222.65 s
2024-12-19 17:21:08.633269: 
2024-12-19 17:21:08.639238: Epoch 60
2024-12-19 17:21:08.640351: Current learning rate: 0.00631
2024-12-19 17:24:45.569940: Validation loss did not improve from -0.48196. Patience: 28/50
2024-12-19 17:24:45.570839: train_loss -0.6617
2024-12-19 17:24:45.571630: val_loss -0.4762
2024-12-19 17:24:45.572480: Pseudo dice [0.7198]
2024-12-19 17:24:45.573398: Epoch time: 216.94 s
2024-12-19 17:24:47.484721: 
2024-12-19 17:24:47.486167: Epoch 61
2024-12-19 17:24:47.486994: Current learning rate: 0.00625
2024-12-19 17:28:33.605080: Validation loss did not improve from -0.48196. Patience: 29/50
2024-12-19 17:28:33.606295: train_loss -0.6739
2024-12-19 17:28:33.607135: val_loss -0.3251
2024-12-19 17:28:33.607931: Pseudo dice [0.6229]
2024-12-19 17:28:33.608706: Epoch time: 226.12 s
2024-12-19 17:28:35.040682: 
2024-12-19 17:28:35.041972: Epoch 62
2024-12-19 17:28:35.042882: Current learning rate: 0.00619
2024-12-19 17:32:41.090193: Validation loss did not improve from -0.48196. Patience: 30/50
2024-12-19 17:32:41.091164: train_loss -0.6613
2024-12-19 17:32:41.092222: val_loss -0.4231
2024-12-19 17:32:41.093251: Pseudo dice [0.7004]
2024-12-19 17:32:41.094315: Epoch time: 246.05 s
2024-12-19 17:32:42.552564: 
2024-12-19 17:32:42.554260: Epoch 63
2024-12-19 17:32:42.555363: Current learning rate: 0.00612
2024-12-19 17:36:36.279036: Validation loss improved from -0.48196 to -0.48387! Patience: 30/50
2024-12-19 17:36:36.280519: train_loss -0.6684
2024-12-19 17:36:36.281529: val_loss -0.4839
2024-12-19 17:36:36.282321: Pseudo dice [0.7229]
2024-12-19 17:36:36.283133: Epoch time: 233.73 s
2024-12-19 17:36:36.283767: Yayy! New best EMA pseudo Dice: 0.6638
2024-12-19 17:36:38.198210: 
2024-12-19 17:36:38.199571: Epoch 64
2024-12-19 17:36:38.200432: Current learning rate: 0.00606
2024-12-19 17:40:34.763550: Validation loss did not improve from -0.48387. Patience: 1/50
2024-12-19 17:40:34.764530: train_loss -0.6629
2024-12-19 17:40:34.765453: val_loss -0.367
2024-12-19 17:40:34.766214: Pseudo dice [0.6438]
2024-12-19 17:40:34.766879: Epoch time: 236.57 s
2024-12-19 17:40:36.722915: 
2024-12-19 17:40:36.724229: Epoch 65
2024-12-19 17:40:36.724975: Current learning rate: 0.006
2024-12-19 17:44:27.685798: Validation loss did not improve from -0.48387. Patience: 2/50
2024-12-19 17:44:27.686577: train_loss -0.6794
2024-12-19 17:44:27.687336: val_loss -0.3724
2024-12-19 17:44:27.688027: Pseudo dice [0.6609]
2024-12-19 17:44:27.688664: Epoch time: 230.96 s
2024-12-19 17:44:29.146886: 
2024-12-19 17:44:29.148131: Epoch 66
2024-12-19 17:44:29.148848: Current learning rate: 0.00593
2024-12-19 17:48:27.497435: Validation loss did not improve from -0.48387. Patience: 3/50
2024-12-19 17:48:27.500377: train_loss -0.6796
2024-12-19 17:48:27.501061: val_loss -0.3445
2024-12-19 17:48:27.501851: Pseudo dice [0.6464]
2024-12-19 17:48:27.502610: Epoch time: 238.35 s
2024-12-19 17:48:28.984792: 
2024-12-19 17:48:28.985872: Epoch 67
2024-12-19 17:48:28.986723: Current learning rate: 0.00587
2024-12-19 17:52:23.661799: Validation loss did not improve from -0.48387. Patience: 4/50
2024-12-19 17:52:23.662911: train_loss -0.6786
2024-12-19 17:52:23.664438: val_loss -0.3702
2024-12-19 17:52:23.665270: Pseudo dice [0.6626]
2024-12-19 17:52:23.666312: Epoch time: 234.68 s
2024-12-19 17:52:25.160436: 
2024-12-19 17:52:25.161870: Epoch 68
2024-12-19 17:52:25.162693: Current learning rate: 0.00581
2024-12-19 17:56:32.105031: Validation loss did not improve from -0.48387. Patience: 5/50
2024-12-19 17:56:32.106012: train_loss -0.6763
2024-12-19 17:56:32.107060: val_loss -0.3296
2024-12-19 17:56:32.107838: Pseudo dice [0.6437]
2024-12-19 17:56:32.108653: Epoch time: 246.95 s
2024-12-19 17:56:33.583728: 
2024-12-19 17:56:33.584972: Epoch 69
2024-12-19 17:56:33.585805: Current learning rate: 0.00574
2024-12-19 18:00:28.579342: Validation loss did not improve from -0.48387. Patience: 6/50
2024-12-19 18:00:28.580339: train_loss -0.6706
2024-12-19 18:00:28.581078: val_loss -0.3732
2024-12-19 18:00:28.581748: Pseudo dice [0.6596]
2024-12-19 18:00:28.582385: Epoch time: 235.0 s
2024-12-19 18:00:30.582066: 
2024-12-19 18:00:30.583575: Epoch 70
2024-12-19 18:00:30.584431: Current learning rate: 0.00568
2024-12-19 18:04:26.818504: Validation loss did not improve from -0.48387. Patience: 7/50
2024-12-19 18:04:26.819395: train_loss -0.6792
2024-12-19 18:04:26.820261: val_loss -0.4083
2024-12-19 18:04:26.820909: Pseudo dice [0.6785]
2024-12-19 18:04:26.821621: Epoch time: 236.24 s
2024-12-19 18:04:28.813292: 
2024-12-19 18:04:28.814521: Epoch 71
2024-12-19 18:04:28.815247: Current learning rate: 0.00562
2024-12-19 18:08:24.938393: Validation loss did not improve from -0.48387. Patience: 8/50
2024-12-19 18:08:24.939692: train_loss -0.6807
2024-12-19 18:08:24.940677: val_loss -0.3787
2024-12-19 18:08:24.941594: Pseudo dice [0.6692]
2024-12-19 18:08:24.942562: Epoch time: 236.13 s
2024-12-19 18:08:26.389633: 
2024-12-19 18:08:26.391051: Epoch 72
2024-12-19 18:08:26.392123: Current learning rate: 0.00555
2024-12-19 18:12:17.592780: Validation loss did not improve from -0.48387. Patience: 9/50
2024-12-19 18:12:17.593660: train_loss -0.6842
2024-12-19 18:12:17.594519: val_loss -0.4333
2024-12-19 18:12:17.595219: Pseudo dice [0.7001]
2024-12-19 18:12:17.595913: Epoch time: 231.21 s
2024-12-19 18:12:17.596668: Yayy! New best EMA pseudo Dice: 0.6655
2024-12-19 18:12:19.403970: 
2024-12-19 18:12:19.405371: Epoch 73
2024-12-19 18:12:19.406104: Current learning rate: 0.00549
2024-12-19 18:16:11.640400: Validation loss did not improve from -0.48387. Patience: 10/50
2024-12-19 18:16:11.641475: train_loss -0.6826
2024-12-19 18:16:11.642525: val_loss -0.3807
2024-12-19 18:16:11.643394: Pseudo dice [0.6772]
2024-12-19 18:16:11.644281: Epoch time: 232.24 s
2024-12-19 18:16:11.645151: Yayy! New best EMA pseudo Dice: 0.6666
2024-12-19 18:16:13.627692: 
2024-12-19 18:16:13.629669: Epoch 74
2024-12-19 18:16:13.630780: Current learning rate: 0.00542
2024-12-19 18:20:11.218212: Validation loss did not improve from -0.48387. Patience: 11/50
2024-12-19 18:20:11.219397: train_loss -0.6876
2024-12-19 18:20:11.220270: val_loss -0.3238
2024-12-19 18:20:11.220962: Pseudo dice [0.632]
2024-12-19 18:20:11.221736: Epoch time: 237.6 s
2024-12-19 18:20:13.210353: 
2024-12-19 18:20:13.211871: Epoch 75
2024-12-19 18:20:13.212606: Current learning rate: 0.00536
2024-12-19 18:24:17.534174: Validation loss did not improve from -0.48387. Patience: 12/50
2024-12-19 18:24:17.535339: train_loss -0.6895
2024-12-19 18:24:17.536156: val_loss -0.3517
2024-12-19 18:24:17.536956: Pseudo dice [0.6525]
2024-12-19 18:24:17.537685: Epoch time: 244.33 s
2024-12-19 18:24:18.985389: 
2024-12-19 18:24:18.986586: Epoch 76
2024-12-19 18:24:18.987315: Current learning rate: 0.00529
2024-12-19 18:28:14.065875: Validation loss did not improve from -0.48387. Patience: 13/50
2024-12-19 18:28:14.066887: train_loss -0.6845
2024-12-19 18:28:14.067697: val_loss -0.4085
2024-12-19 18:28:14.068353: Pseudo dice [0.6834]
2024-12-19 18:28:14.069027: Epoch time: 235.08 s
2024-12-19 18:28:15.504996: 
2024-12-19 18:28:15.506003: Epoch 77
2024-12-19 18:28:15.506710: Current learning rate: 0.00523
2024-12-19 18:31:45.063818: Validation loss did not improve from -0.48387. Patience: 14/50
2024-12-19 18:31:45.064826: train_loss -0.6951
2024-12-19 18:31:45.066070: val_loss -0.4235
2024-12-19 18:31:45.067045: Pseudo dice [0.6957]
2024-12-19 18:31:45.068011: Epoch time: 209.56 s
2024-12-19 18:31:45.068983: Yayy! New best EMA pseudo Dice: 0.6674
2024-12-19 18:31:46.965926: 
2024-12-19 18:31:46.967149: Epoch 78
2024-12-19 18:31:46.968003: Current learning rate: 0.00517
2024-12-19 18:35:39.701586: Validation loss did not improve from -0.48387. Patience: 15/50
2024-12-19 18:35:39.702506: train_loss -0.6861
2024-12-19 18:35:39.703276: val_loss -0.4794
2024-12-19 18:35:39.704046: Pseudo dice [0.72]
2024-12-19 18:35:39.704722: Epoch time: 232.74 s
2024-12-19 18:35:39.705391: Yayy! New best EMA pseudo Dice: 0.6727
2024-12-19 18:35:41.680476: 
2024-12-19 18:35:41.681791: Epoch 79
2024-12-19 18:35:41.682606: Current learning rate: 0.0051
2024-12-19 18:39:48.094812: Validation loss did not improve from -0.48387. Patience: 16/50
2024-12-19 18:39:48.095741: train_loss -0.6909
2024-12-19 18:39:48.096527: val_loss -0.338
2024-12-19 18:39:48.097168: Pseudo dice [0.6393]
2024-12-19 18:39:48.097911: Epoch time: 246.42 s
2024-12-19 18:39:50.044760: 
2024-12-19 18:39:50.046137: Epoch 80
2024-12-19 18:39:50.046791: Current learning rate: 0.00504
2024-12-19 18:43:59.876723: Validation loss did not improve from -0.48387. Patience: 17/50
2024-12-19 18:43:59.877739: train_loss -0.688
2024-12-19 18:43:59.878545: val_loss -0.398
2024-12-19 18:43:59.879314: Pseudo dice [0.6842]
2024-12-19 18:43:59.880145: Epoch time: 249.83 s
2024-12-19 18:44:01.345931: 
2024-12-19 18:44:01.347309: Epoch 81
2024-12-19 18:44:01.348226: Current learning rate: 0.00497
2024-12-19 18:47:55.923502: Validation loss did not improve from -0.48387. Patience: 18/50
2024-12-19 18:47:55.924465: train_loss -0.6911
2024-12-19 18:47:55.925520: val_loss -0.3615
2024-12-19 18:47:55.926284: Pseudo dice [0.6469]
2024-12-19 18:47:55.926962: Epoch time: 234.58 s
2024-12-19 18:47:57.868410: 
2024-12-19 18:47:57.869785: Epoch 82
2024-12-19 18:47:57.870582: Current learning rate: 0.00491
2024-12-19 18:52:10.015460: Validation loss did not improve from -0.48387. Patience: 19/50
2024-12-19 18:52:10.019321: train_loss -0.7039
2024-12-19 18:52:10.020472: val_loss -0.4438
2024-12-19 18:52:10.021239: Pseudo dice [0.7055]
2024-12-19 18:52:10.022367: Epoch time: 252.15 s
2024-12-19 18:52:11.425338: 
2024-12-19 18:52:11.426410: Epoch 83
2024-12-19 18:52:11.427077: Current learning rate: 0.00484
2024-12-19 18:56:00.468961: Validation loss did not improve from -0.48387. Patience: 20/50
2024-12-19 18:56:00.470248: train_loss -0.7037
2024-12-19 18:56:00.472145: val_loss -0.4034
2024-12-19 18:56:00.473065: Pseudo dice [0.6754]
2024-12-19 18:56:00.474391: Epoch time: 229.05 s
2024-12-19 18:56:01.942386: 
2024-12-19 18:56:01.943781: Epoch 84
2024-12-19 18:56:01.944953: Current learning rate: 0.00478
2024-12-19 18:59:41.121961: Validation loss did not improve from -0.48387. Patience: 21/50
2024-12-19 18:59:41.123304: train_loss -0.7006
2024-12-19 18:59:41.124321: val_loss -0.3856
2024-12-19 18:59:41.125305: Pseudo dice [0.6735]
2024-12-19 18:59:41.126270: Epoch time: 219.18 s
2024-12-19 18:59:42.955665: 
2024-12-19 18:59:42.957152: Epoch 85
2024-12-19 18:59:42.958027: Current learning rate: 0.00471
2024-12-19 19:03:35.759023: Validation loss did not improve from -0.48387. Patience: 22/50
2024-12-19 19:03:35.759920: train_loss -0.7021
2024-12-19 19:03:35.761088: val_loss -0.3669
2024-12-19 19:03:35.761842: Pseudo dice [0.6552]
2024-12-19 19:03:35.762619: Epoch time: 232.81 s
2024-12-19 19:03:37.148761: 
2024-12-19 19:03:37.150119: Epoch 86
2024-12-19 19:03:37.150814: Current learning rate: 0.00465
2024-12-19 19:07:22.935584: Validation loss did not improve from -0.48387. Patience: 23/50
2024-12-19 19:07:22.936561: train_loss -0.7046
2024-12-19 19:07:22.937786: val_loss -0.3974
2024-12-19 19:07:22.938804: Pseudo dice [0.6854]
2024-12-19 19:07:22.939891: Epoch time: 225.79 s
2024-12-19 19:07:24.361474: 
2024-12-19 19:07:24.362877: Epoch 87
2024-12-19 19:07:24.363755: Current learning rate: 0.00458
2024-12-19 19:11:03.921607: Validation loss did not improve from -0.48387. Patience: 24/50
2024-12-19 19:11:03.922616: train_loss -0.7073
2024-12-19 19:11:03.925615: val_loss -0.3969
2024-12-19 19:11:03.926446: Pseudo dice [0.6812]
2024-12-19 19:11:03.927182: Epoch time: 219.56 s
2024-12-19 19:11:03.927833: Yayy! New best EMA pseudo Dice: 0.6732
2024-12-19 19:11:05.727574: 
2024-12-19 19:11:05.728747: Epoch 88
2024-12-19 19:11:05.729563: Current learning rate: 0.00452
2024-12-19 19:15:04.452868: Validation loss did not improve from -0.48387. Patience: 25/50
2024-12-19 19:15:04.453678: train_loss -0.7104
2024-12-19 19:15:04.454813: val_loss -0.3716
2024-12-19 19:15:04.455778: Pseudo dice [0.6721]
2024-12-19 19:15:04.456724: Epoch time: 238.73 s
2024-12-19 19:15:05.864527: 
2024-12-19 19:15:05.865924: Epoch 89
2024-12-19 19:15:05.866835: Current learning rate: 0.00445
2024-12-19 19:19:07.982687: Validation loss did not improve from -0.48387. Patience: 26/50
2024-12-19 19:19:07.983525: train_loss -0.7083
2024-12-19 19:19:07.984364: val_loss -0.3768
2024-12-19 19:19:07.985240: Pseudo dice [0.6647]
2024-12-19 19:19:07.985968: Epoch time: 242.12 s
2024-12-19 19:19:09.834131: 
2024-12-19 19:19:09.835794: Epoch 90
2024-12-19 19:19:09.836916: Current learning rate: 0.00438
2024-12-19 19:23:24.412027: Validation loss did not improve from -0.48387. Patience: 27/50
2024-12-19 19:23:24.412840: train_loss -0.71
2024-12-19 19:23:24.413562: val_loss -0.3951
2024-12-19 19:23:24.414292: Pseudo dice [0.6689]
2024-12-19 19:23:24.415082: Epoch time: 254.58 s
2024-12-19 19:23:25.807862: 
2024-12-19 19:23:25.809294: Epoch 91
2024-12-19 19:23:25.810091: Current learning rate: 0.00432
2024-12-19 19:27:14.534748: Validation loss did not improve from -0.48387. Patience: 28/50
2024-12-19 19:27:14.535823: train_loss -0.7085
2024-12-19 19:27:14.536601: val_loss -0.2418
2024-12-19 19:27:14.537290: Pseudo dice [0.5942]
2024-12-19 19:27:14.538034: Epoch time: 228.73 s
2024-12-19 19:27:15.918577: 
2024-12-19 19:27:15.920061: Epoch 92
2024-12-19 19:27:15.920892: Current learning rate: 0.00425
2024-12-19 19:31:04.044874: Validation loss did not improve from -0.48387. Patience: 29/50
2024-12-19 19:31:04.045934: train_loss -0.7069
2024-12-19 19:31:04.046842: val_loss -0.4296
2024-12-19 19:31:04.047568: Pseudo dice [0.6936]
2024-12-19 19:31:04.048332: Epoch time: 228.13 s
2024-12-19 19:31:05.461236: 
2024-12-19 19:31:05.462909: Epoch 93
2024-12-19 19:31:05.463855: Current learning rate: 0.00419
2024-12-19 19:35:10.455762: Validation loss did not improve from -0.48387. Patience: 30/50
2024-12-19 19:35:10.456717: train_loss -0.7085
2024-12-19 19:35:10.457646: val_loss -0.2418
2024-12-19 19:35:10.458608: Pseudo dice [0.609]
2024-12-19 19:35:10.459535: Epoch time: 245.0 s
2024-12-19 19:35:11.825447: 
2024-12-19 19:35:11.826889: Epoch 94
2024-12-19 19:35:11.828095: Current learning rate: 0.00412
2024-12-19 19:38:58.021126: Validation loss did not improve from -0.48387. Patience: 31/50
2024-12-19 19:38:58.022236: train_loss -0.7128
2024-12-19 19:38:58.023128: val_loss -0.4111
2024-12-19 19:38:58.024003: Pseudo dice [0.6765]
2024-12-19 19:38:58.024932: Epoch time: 226.2 s
2024-12-19 19:38:59.831948: 
2024-12-19 19:38:59.833490: Epoch 95
2024-12-19 19:38:59.834504: Current learning rate: 0.00405
2024-12-19 19:43:28.228489: Validation loss did not improve from -0.48387. Patience: 32/50
2024-12-19 19:43:28.229483: train_loss -0.7217
2024-12-19 19:43:28.230188: val_loss -0.3947
2024-12-19 19:43:28.230847: Pseudo dice [0.6829]
2024-12-19 19:43:28.231533: Epoch time: 268.4 s
2024-12-19 19:43:29.600190: 
2024-12-19 19:43:29.601701: Epoch 96
2024-12-19 19:43:29.602653: Current learning rate: 0.00399
2024-12-19 19:49:11.677687: Validation loss did not improve from -0.48387. Patience: 33/50
2024-12-19 19:49:11.678926: train_loss -0.7208
2024-12-19 19:49:11.679889: val_loss -0.3315
2024-12-19 19:49:11.680710: Pseudo dice [0.6289]
2024-12-19 19:49:11.681511: Epoch time: 342.08 s
2024-12-19 19:49:13.147401: 
2024-12-19 19:49:13.148762: Epoch 97
2024-12-19 19:49:13.149590: Current learning rate: 0.00392
2024-12-19 19:55:00.293598: Validation loss did not improve from -0.48387. Patience: 34/50
2024-12-19 19:55:00.294458: train_loss -0.7159
2024-12-19 19:55:00.295456: val_loss -0.3749
2024-12-19 19:55:00.296305: Pseudo dice [0.6811]
2024-12-19 19:55:00.297204: Epoch time: 347.15 s
2024-12-19 19:55:01.755307: 
2024-12-19 19:55:01.756639: Epoch 98
2024-12-19 19:55:01.757827: Current learning rate: 0.00385
2024-12-19 20:00:59.427552: Validation loss did not improve from -0.48387. Patience: 35/50
2024-12-19 20:00:59.428524: train_loss -0.7144
2024-12-19 20:00:59.430054: val_loss -0.3683
2024-12-19 20:00:59.430753: Pseudo dice [0.6715]
2024-12-19 20:00:59.431646: Epoch time: 357.67 s
2024-12-19 20:01:00.831747: 
2024-12-19 20:01:00.832792: Epoch 99
2024-12-19 20:01:00.833463: Current learning rate: 0.00379
2024-12-19 20:06:26.352384: Validation loss did not improve from -0.48387. Patience: 36/50
2024-12-19 20:06:26.353352: train_loss -0.7144
2024-12-19 20:06:26.354281: val_loss -0.3597
2024-12-19 20:06:26.355226: Pseudo dice [0.6731]
2024-12-19 20:06:26.356329: Epoch time: 325.52 s
2024-12-19 20:06:28.167524: 
2024-12-19 20:06:28.168869: Epoch 100
2024-12-19 20:06:28.169721: Current learning rate: 0.00372
2024-12-19 20:12:09.008016: Validation loss did not improve from -0.48387. Patience: 37/50
2024-12-19 20:12:09.008889: train_loss -0.7192
2024-12-19 20:12:09.009941: val_loss -0.3761
2024-12-19 20:12:09.010664: Pseudo dice [0.6778]
2024-12-19 20:12:09.011414: Epoch time: 340.84 s
2024-12-19 20:12:10.444886: 
2024-12-19 20:12:10.446186: Epoch 101
2024-12-19 20:12:10.446993: Current learning rate: 0.00365
2024-12-19 20:18:00.379740: Validation loss did not improve from -0.48387. Patience: 38/50
2024-12-19 20:18:00.381484: train_loss -0.7235
2024-12-19 20:18:00.382656: val_loss -0.3633
2024-12-19 20:18:00.383584: Pseudo dice [0.6545]
2024-12-19 20:18:00.384377: Epoch time: 349.94 s
2024-12-19 20:18:01.836912: 
2024-12-19 20:18:01.838160: Epoch 102
2024-12-19 20:18:01.839044: Current learning rate: 0.00359
2024-12-19 20:23:44.035038: Validation loss did not improve from -0.48387. Patience: 39/50
2024-12-19 20:23:44.035882: train_loss -0.7292
2024-12-19 20:23:44.036855: val_loss -0.3732
2024-12-19 20:23:44.037597: Pseudo dice [0.6697]
2024-12-19 20:23:44.038563: Epoch time: 342.2 s
2024-12-19 20:23:45.899145: 
2024-12-19 20:23:45.900590: Epoch 103
2024-12-19 20:23:45.901603: Current learning rate: 0.00352
2024-12-19 20:29:33.267136: Validation loss did not improve from -0.48387. Patience: 40/50
2024-12-19 20:29:33.267797: train_loss -0.7301
2024-12-19 20:29:33.268533: val_loss -0.3628
2024-12-19 20:29:33.269149: Pseudo dice [0.6603]
2024-12-19 20:29:33.269833: Epoch time: 347.37 s
2024-12-19 20:29:34.681260: 
2024-12-19 20:29:34.682542: Epoch 104
2024-12-19 20:29:34.683240: Current learning rate: 0.00345
2024-12-19 20:35:20.415498: Validation loss did not improve from -0.48387. Patience: 41/50
2024-12-19 20:35:20.416646: train_loss -0.7222
2024-12-19 20:35:20.417826: val_loss -0.3696
2024-12-19 20:35:20.418751: Pseudo dice [0.665]
2024-12-19 20:35:20.419695: Epoch time: 345.74 s
2024-12-19 20:35:22.290881: 
2024-12-19 20:35:22.292193: Epoch 105
2024-12-19 20:35:22.292990: Current learning rate: 0.00338
2024-12-19 20:41:20.637816: Validation loss did not improve from -0.48387. Patience: 42/50
2024-12-19 20:41:20.638587: train_loss -0.7249
2024-12-19 20:41:20.639640: val_loss -0.2988
2024-12-19 20:41:20.640414: Pseudo dice [0.625]
2024-12-19 20:41:20.641181: Epoch time: 358.35 s
2024-12-19 20:41:22.047985: 
2024-12-19 20:41:22.049135: Epoch 106
2024-12-19 20:41:22.049899: Current learning rate: 0.00332
2024-12-19 20:47:21.419683: Validation loss did not improve from -0.48387. Patience: 43/50
2024-12-19 20:47:21.420677: train_loss -0.7312
2024-12-19 20:47:21.421533: val_loss -0.3981
2024-12-19 20:47:21.422274: Pseudo dice [0.6803]
2024-12-19 20:47:21.423039: Epoch time: 359.37 s
2024-12-19 20:47:22.995646: 
2024-12-19 20:47:22.996996: Epoch 107
2024-12-19 20:47:22.997777: Current learning rate: 0.00325
2024-12-19 20:53:15.518261: Validation loss did not improve from -0.48387. Patience: 44/50
2024-12-19 20:53:15.519214: train_loss -0.7237
2024-12-19 20:53:15.520004: val_loss -0.3972
2024-12-19 20:53:15.520647: Pseudo dice [0.6748]
2024-12-19 20:53:15.521391: Epoch time: 352.52 s
2024-12-19 20:53:16.954295: 
2024-12-19 20:53:16.955367: Epoch 108
2024-12-19 20:53:16.956097: Current learning rate: 0.00318
2024-12-19 20:58:49.002721: Validation loss did not improve from -0.48387. Patience: 45/50
2024-12-19 20:58:49.003566: train_loss -0.7301
2024-12-19 20:58:49.004344: val_loss -0.3001
2024-12-19 20:58:49.005109: Pseudo dice [0.6266]
2024-12-19 20:58:49.005836: Epoch time: 332.05 s
2024-12-19 20:58:50.437500: 
2024-12-19 20:58:50.438698: Epoch 109
2024-12-19 20:58:50.439502: Current learning rate: 0.00311
2024-12-19 21:04:01.118357: Validation loss did not improve from -0.48387. Patience: 46/50
2024-12-19 21:04:01.119410: train_loss -0.7328
2024-12-19 21:04:01.120494: val_loss -0.2919
2024-12-19 21:04:01.121356: Pseudo dice [0.6116]
2024-12-19 21:04:01.122212: Epoch time: 310.68 s
2024-12-19 21:04:02.917819: 
2024-12-19 21:04:02.919244: Epoch 110
2024-12-19 21:04:02.920119: Current learning rate: 0.00304
2024-12-19 21:09:52.089353: Validation loss did not improve from -0.48387. Patience: 47/50
2024-12-19 21:09:52.090281: train_loss -0.7292
2024-12-19 21:09:52.091274: val_loss -0.3582
2024-12-19 21:09:52.092031: Pseudo dice [0.6521]
2024-12-19 21:09:52.092774: Epoch time: 349.17 s
2024-12-19 21:09:53.510271: 
2024-12-19 21:09:53.511598: Epoch 111
2024-12-19 21:09:53.512384: Current learning rate: 0.00297
2024-12-19 21:15:45.781373: Validation loss did not improve from -0.48387. Patience: 48/50
2024-12-19 21:15:45.782487: train_loss -0.7317
2024-12-19 21:15:45.783549: val_loss -0.357
2024-12-19 21:15:45.784531: Pseudo dice [0.6583]
2024-12-19 21:15:45.785460: Epoch time: 352.27 s
2024-12-19 21:15:47.238784: 
2024-12-19 21:15:47.240083: Epoch 112
2024-12-19 21:15:47.240913: Current learning rate: 0.00291
2024-12-19 21:21:59.721861: Validation loss did not improve from -0.48387. Patience: 49/50
2024-12-19 21:21:59.722607: train_loss -0.7359
2024-12-19 21:21:59.723420: val_loss -0.4168
2024-12-19 21:21:59.724115: Pseudo dice [0.68]
2024-12-19 21:21:59.724815: Epoch time: 372.49 s
2024-12-19 21:22:01.132966: 
2024-12-19 21:22:01.134052: Epoch 113
2024-12-19 21:22:01.134700: Current learning rate: 0.00284
2024-12-19 21:27:28.227568: Validation loss did not improve from -0.48387. Patience: 50/50
2024-12-19 21:27:28.228773: train_loss -0.7325
2024-12-19 21:27:28.229759: val_loss -0.4664
2024-12-19 21:27:28.230628: Pseudo dice [0.7201]
2024-12-19 21:27:28.231623: Epoch time: 327.1 s
2024-12-19 21:27:30.001428: 
2024-12-19 21:27:30.002757: Epoch 114
2024-12-19 21:27:30.003703: Current learning rate: 0.00277
2024-12-19 21:33:49.811061: Validation loss did not improve from -0.48387. Patience: 51/50
2024-12-19 21:33:49.812037: train_loss -0.7393
2024-12-19 21:33:49.813023: val_loss -0.3196
2024-12-19 21:33:49.813880: Pseudo dice [0.6369]
2024-12-19 21:33:49.814718: Epoch time: 379.81 s
2024-12-19 21:33:51.605946: 
2024-12-19 21:33:51.607396: Epoch 115
2024-12-19 21:33:51.608516: Current learning rate: 0.0027
2024-12-19 21:41:10.451700: Validation loss did not improve from -0.48387. Patience: 52/50
2024-12-19 21:41:10.452488: train_loss -0.7348
2024-12-19 21:41:10.453170: val_loss -0.415
2024-12-19 21:41:10.453904: Pseudo dice [0.6926]
2024-12-19 21:41:10.454681: Epoch time: 438.85 s
2024-12-19 21:41:11.907149: 
2024-12-19 21:41:11.908330: Epoch 116
2024-12-19 21:41:11.909047: Current learning rate: 0.00263
2024-12-19 21:48:36.361058: Validation loss did not improve from -0.48387. Patience: 53/50
2024-12-19 21:48:36.362036: train_loss -0.7396
2024-12-19 21:48:36.363022: val_loss -0.4458
2024-12-19 21:48:36.363831: Pseudo dice [0.709]
2024-12-19 21:48:36.364617: Epoch time: 444.46 s
2024-12-19 21:48:37.914522: 
2024-12-19 21:48:37.915798: Epoch 117
2024-12-19 21:48:37.916810: Current learning rate: 0.00256
2024-12-19 21:55:35.662214: Validation loss did not improve from -0.48387. Patience: 54/50
2024-12-19 21:55:35.662987: train_loss -0.7436
2024-12-19 21:55:35.664253: val_loss -0.3469
2024-12-19 21:55:35.665265: Pseudo dice [0.659]
2024-12-19 21:55:35.666176: Epoch time: 417.75 s
2024-12-19 21:55:37.140268: 
2024-12-19 21:55:37.141847: Epoch 118
2024-12-19 21:55:37.142915: Current learning rate: 0.00249
2024-12-19 22:02:26.406502: Validation loss did not improve from -0.48387. Patience: 55/50
2024-12-19 22:02:26.407531: train_loss -0.7417
2024-12-19 22:02:26.408487: val_loss -0.412
2024-12-19 22:02:26.409263: Pseudo dice [0.6889]
2024-12-19 22:02:26.410123: Epoch time: 409.27 s
2024-12-19 22:02:27.888412: 
2024-12-19 22:02:27.889493: Epoch 119
2024-12-19 22:02:27.890212: Current learning rate: 0.00242
2024-12-19 22:09:09.955216: Validation loss did not improve from -0.48387. Patience: 56/50
2024-12-19 22:09:09.956807: train_loss -0.7402
2024-12-19 22:09:09.957671: val_loss -0.3877
2024-12-19 22:09:09.958472: Pseudo dice [0.6624]
2024-12-19 22:09:09.959197: Epoch time: 402.07 s
2024-12-19 22:09:11.816893: 
2024-12-19 22:09:11.818256: Epoch 120
2024-12-19 22:09:11.819044: Current learning rate: 0.00235
2024-12-19 22:16:01.217778: Validation loss did not improve from -0.48387. Patience: 57/50
2024-12-19 22:16:01.218711: train_loss -0.7393
2024-12-19 22:16:01.219548: val_loss -0.3872
2024-12-19 22:16:01.220233: Pseudo dice [0.6855]
2024-12-19 22:16:01.221041: Epoch time: 409.4 s
2024-12-19 22:16:02.657602: 
2024-12-19 22:16:02.658800: Epoch 121
2024-12-19 22:16:02.659624: Current learning rate: 0.00228
2024-12-19 22:23:03.477031: Validation loss did not improve from -0.48387. Patience: 58/50
2024-12-19 22:23:03.478088: train_loss -0.741
2024-12-19 22:23:03.478775: val_loss -0.3262
2024-12-19 22:23:03.479541: Pseudo dice [0.6496]
2024-12-19 22:23:03.480311: Epoch time: 420.82 s
2024-12-19 22:23:04.910328: 
2024-12-19 22:23:04.911517: Epoch 122
2024-12-19 22:23:04.912290: Current learning rate: 0.00221
2024-12-19 22:29:30.206448: Validation loss did not improve from -0.48387. Patience: 59/50
2024-12-19 22:29:30.207931: train_loss -0.747
2024-12-19 22:29:30.208906: val_loss -0.3608
2024-12-19 22:29:30.209648: Pseudo dice [0.6624]
2024-12-19 22:29:30.210550: Epoch time: 385.3 s
2024-12-19 22:29:31.674914: 
2024-12-19 22:29:31.676123: Epoch 123
2024-12-19 22:29:31.677001: Current learning rate: 0.00214
2024-12-19 22:37:05.672554: Validation loss did not improve from -0.48387. Patience: 60/50
2024-12-19 22:37:05.673369: train_loss -0.7429
2024-12-19 22:37:05.674054: val_loss -0.3129
2024-12-19 22:37:05.674810: Pseudo dice [0.625]
2024-12-19 22:37:05.675600: Epoch time: 454.0 s
2024-12-19 22:37:07.470030: 
2024-12-19 22:37:07.470898: Epoch 124
2024-12-19 22:37:07.471619: Current learning rate: 0.00207
2024-12-19 22:44:37.103045: Validation loss did not improve from -0.48387. Patience: 61/50
2024-12-19 22:44:37.103851: train_loss -0.7404
2024-12-19 22:44:37.104724: val_loss -0.3816
2024-12-19 22:44:37.105392: Pseudo dice [0.6574]
2024-12-19 22:44:37.106160: Epoch time: 449.63 s
2024-12-19 22:44:39.050208: 
2024-12-19 22:44:39.051278: Epoch 125
2024-12-19 22:44:39.052009: Current learning rate: 0.00199
2024-12-19 22:52:14.140796: Validation loss did not improve from -0.48387. Patience: 62/50
2024-12-19 22:52:14.141880: train_loss -0.7485
2024-12-19 22:52:14.142786: val_loss -0.2653
2024-12-19 22:52:14.143742: Pseudo dice [0.6154]
2024-12-19 22:52:14.144819: Epoch time: 455.09 s
2024-12-19 22:52:15.593829: 
2024-12-19 22:52:15.595373: Epoch 126
2024-12-19 22:52:15.596349: Current learning rate: 0.00192
2024-12-19 22:59:36.713292: Validation loss did not improve from -0.48387. Patience: 63/50
2024-12-19 22:59:36.714076: train_loss -0.7497
2024-12-19 22:59:36.714986: val_loss -0.3675
2024-12-19 22:59:36.715733: Pseudo dice [0.6545]
2024-12-19 22:59:36.716600: Epoch time: 441.12 s
2024-12-19 22:59:38.181942: 
2024-12-19 22:59:38.183288: Epoch 127
2024-12-19 22:59:38.184274: Current learning rate: 0.00185
2024-12-19 23:06:40.847840: Validation loss did not improve from -0.48387. Patience: 64/50
2024-12-19 23:06:40.848723: train_loss -0.7471
2024-12-19 23:06:40.849637: val_loss -0.3386
2024-12-19 23:06:40.850286: Pseudo dice [0.645]
2024-12-19 23:06:40.850934: Epoch time: 422.67 s
2024-12-19 23:06:42.299882: 
2024-12-19 23:06:42.301177: Epoch 128
2024-12-19 23:06:42.301917: Current learning rate: 0.00178
2024-12-19 23:13:41.134679: Validation loss did not improve from -0.48387. Patience: 65/50
2024-12-19 23:13:41.138721: train_loss -0.7499
2024-12-19 23:13:41.140018: val_loss -0.404
2024-12-19 23:13:41.140827: Pseudo dice [0.6791]
2024-12-19 23:13:41.141843: Epoch time: 418.84 s
2024-12-19 23:13:42.598984: 
2024-12-19 23:13:42.600536: Epoch 129
2024-12-19 23:13:42.601423: Current learning rate: 0.0017
2024-12-19 23:20:45.445374: Validation loss did not improve from -0.48387. Patience: 66/50
2024-12-19 23:20:45.446563: train_loss -0.752
2024-12-19 23:20:45.448303: val_loss -0.3647
2024-12-19 23:20:45.449115: Pseudo dice [0.67]
2024-12-19 23:20:45.450037: Epoch time: 422.85 s
2024-12-19 23:20:47.226390: 
2024-12-19 23:20:47.227467: Epoch 130
2024-12-19 23:20:47.228142: Current learning rate: 0.00163
2024-12-19 23:27:29.349366: Validation loss did not improve from -0.48387. Patience: 67/50
2024-12-19 23:27:29.350469: train_loss -0.7485
2024-12-19 23:27:29.351258: val_loss -0.3342
2024-12-19 23:27:29.352320: Pseudo dice [0.6448]
2024-12-19 23:27:29.353096: Epoch time: 402.13 s
2024-12-19 23:27:30.753417: 
2024-12-19 23:27:30.754608: Epoch 131
2024-12-19 23:27:30.755339: Current learning rate: 0.00156
2024-12-19 23:33:46.177880: Validation loss did not improve from -0.48387. Patience: 68/50
2024-12-19 23:33:46.178861: train_loss -0.7504
2024-12-19 23:33:46.179640: val_loss -0.3907
2024-12-19 23:33:46.180332: Pseudo dice [0.674]
2024-12-19 23:33:46.181153: Epoch time: 375.43 s
2024-12-19 23:33:47.624177: 
2024-12-19 23:33:47.625458: Epoch 132
2024-12-19 23:33:47.626234: Current learning rate: 0.00148
2024-12-19 23:40:08.220152: Validation loss did not improve from -0.48387. Patience: 69/50
2024-12-19 23:40:08.221124: train_loss -0.7523
2024-12-19 23:40:08.221834: val_loss -0.3866
2024-12-19 23:40:08.222676: Pseudo dice [0.6616]
2024-12-19 23:40:08.223393: Epoch time: 380.6 s
2024-12-19 23:40:09.739031: 
2024-12-19 23:40:09.740238: Epoch 133
2024-12-19 23:40:09.741084: Current learning rate: 0.00141
2024-12-19 23:46:59.941061: Validation loss did not improve from -0.48387. Patience: 70/50
2024-12-19 23:46:59.941984: train_loss -0.7564
2024-12-19 23:46:59.943062: val_loss -0.4102
2024-12-19 23:46:59.944098: Pseudo dice [0.6888]
2024-12-19 23:46:59.945094: Epoch time: 410.2 s
2024-12-19 23:47:01.365003: 
2024-12-19 23:47:01.366586: Epoch 134
2024-12-19 23:47:01.367605: Current learning rate: 0.00133
2024-12-19 23:53:16.614736: Validation loss did not improve from -0.48387. Patience: 71/50
2024-12-19 23:53:16.615816: train_loss -0.7551
2024-12-19 23:53:16.616629: val_loss -0.3759
2024-12-19 23:53:16.617396: Pseudo dice [0.6687]
2024-12-19 23:53:16.618104: Epoch time: 375.25 s
2024-12-19 23:53:18.535263: 
2024-12-19 23:53:18.536572: Epoch 135
2024-12-19 23:53:18.537644: Current learning rate: 0.00126
2024-12-19 23:58:49.270793: Validation loss did not improve from -0.48387. Patience: 72/50
2024-12-19 23:58:49.271576: train_loss -0.7553
2024-12-19 23:58:49.272287: val_loss -0.3737
2024-12-19 23:58:49.272917: Pseudo dice [0.6719]
2024-12-19 23:58:49.273681: Epoch time: 330.74 s
2024-12-19 23:58:51.250385: 
2024-12-19 23:58:51.251560: Epoch 136
2024-12-19 23:58:51.252266: Current learning rate: 0.00118
2024-12-20 00:06:00.645428: Validation loss did not improve from -0.48387. Patience: 73/50
2024-12-20 00:06:00.646469: train_loss -0.7598
2024-12-20 00:06:00.647500: val_loss -0.4234
2024-12-20 00:06:00.648398: Pseudo dice [0.7051]
2024-12-20 00:06:00.649112: Epoch time: 429.4 s
2024-12-20 00:06:02.166228: 
2024-12-20 00:06:02.167628: Epoch 137
2024-12-20 00:06:02.168556: Current learning rate: 0.00111
2024-12-20 00:13:47.381496: Validation loss did not improve from -0.48387. Patience: 74/50
2024-12-20 00:13:47.382450: train_loss -0.7554
2024-12-20 00:13:47.383185: val_loss -0.4183
2024-12-20 00:13:47.383891: Pseudo dice [0.6889]
2024-12-20 00:13:47.384667: Epoch time: 465.22 s
2024-12-20 00:13:48.842335: 
2024-12-20 00:13:48.843719: Epoch 138
2024-12-20 00:13:48.844598: Current learning rate: 0.00103
2024-12-20 00:21:40.556620: Validation loss did not improve from -0.48387. Patience: 75/50
2024-12-20 00:21:40.558473: train_loss -0.7565
2024-12-20 00:21:40.560383: val_loss -0.3901
2024-12-20 00:21:40.561245: Pseudo dice [0.6811]
2024-12-20 00:21:40.562074: Epoch time: 471.72 s
2024-12-20 00:21:42.068846: 
2024-12-20 00:21:42.070133: Epoch 139
2024-12-20 00:21:42.070939: Current learning rate: 0.00095
2024-12-20 00:29:25.306872: Validation loss did not improve from -0.48387. Patience: 76/50
2024-12-20 00:29:25.308186: train_loss -0.7568
2024-12-20 00:29:25.309325: val_loss -0.3902
2024-12-20 00:29:25.310321: Pseudo dice [0.6831]
2024-12-20 00:29:25.311257: Epoch time: 463.24 s
2024-12-20 00:29:27.182946: 
2024-12-20 00:29:27.184346: Epoch 140
2024-12-20 00:29:27.185219: Current learning rate: 0.00087
2024-12-20 00:36:57.372676: Validation loss did not improve from -0.48387. Patience: 77/50
2024-12-20 00:36:57.373636: train_loss -0.762
2024-12-20 00:36:57.374885: val_loss -0.4691
2024-12-20 00:36:57.375784: Pseudo dice [0.7124]
2024-12-20 00:36:57.376655: Epoch time: 450.19 s
2024-12-20 00:36:57.377392: Yayy! New best EMA pseudo Dice: 0.6767
2024-12-20 00:36:59.385293: 
2024-12-20 00:36:59.386586: Epoch 141
2024-12-20 00:36:59.387323: Current learning rate: 0.00079
2024-12-20 00:43:48.601118: Validation loss did not improve from -0.48387. Patience: 78/50
2024-12-20 00:43:48.602120: train_loss -0.7584
2024-12-20 00:43:48.602907: val_loss -0.3253
2024-12-20 00:43:48.603632: Pseudo dice [0.6334]
2024-12-20 00:43:48.604411: Epoch time: 409.22 s
2024-12-20 00:43:50.130513: 
2024-12-20 00:43:50.131772: Epoch 142
2024-12-20 00:43:50.132580: Current learning rate: 0.00071
2024-12-20 00:51:30.324407: Validation loss did not improve from -0.48387. Patience: 79/50
2024-12-20 00:51:30.325255: train_loss -0.7609
2024-12-20 00:51:30.326039: val_loss -0.3475
2024-12-20 00:51:30.326795: Pseudo dice [0.6509]
2024-12-20 00:51:30.327449: Epoch time: 460.2 s
2024-12-20 00:51:31.785953: 
2024-12-20 00:51:31.787230: Epoch 143
2024-12-20 00:51:31.787860: Current learning rate: 0.00063
2024-12-20 00:59:19.030999: Validation loss did not improve from -0.48387. Patience: 80/50
2024-12-20 00:59:19.032063: train_loss -0.7612
2024-12-20 00:59:19.033067: val_loss -0.3986
2024-12-20 00:59:19.033755: Pseudo dice [0.674]
2024-12-20 00:59:19.034515: Epoch time: 467.25 s
2024-12-20 00:59:20.540273: 
2024-12-20 00:59:20.541922: Epoch 144
2024-12-20 00:59:20.543126: Current learning rate: 0.00055
2024-12-20 01:07:32.540283: Validation loss did not improve from -0.48387. Patience: 81/50
2024-12-20 01:07:32.541325: train_loss -0.7575
2024-12-20 01:07:32.542354: val_loss -0.3841
2024-12-20 01:07:32.543180: Pseudo dice [0.6705]
2024-12-20 01:07:32.543980: Epoch time: 492.0 s
2024-12-20 01:07:34.556782: 
2024-12-20 01:07:34.558064: Epoch 145
2024-12-20 01:07:34.558897: Current learning rate: 0.00047
2024-12-20 01:15:13.979635: Validation loss did not improve from -0.48387. Patience: 82/50
2024-12-20 01:15:13.980701: train_loss -0.7564
2024-12-20 01:15:13.981510: val_loss -0.3815
2024-12-20 01:15:13.982216: Pseudo dice [0.6618]
2024-12-20 01:15:13.982829: Epoch time: 459.43 s
2024-12-20 01:15:15.914225: 
2024-12-20 01:15:15.915564: Epoch 146
2024-12-20 01:15:15.916359: Current learning rate: 0.00038
2024-12-20 01:22:59.830316: Validation loss did not improve from -0.48387. Patience: 83/50
2024-12-20 01:22:59.835060: train_loss -0.758
2024-12-20 01:22:59.836791: val_loss -0.3945
2024-12-20 01:22:59.837667: Pseudo dice [0.6749]
2024-12-20 01:22:59.838818: Epoch time: 463.92 s
2024-12-20 01:23:01.380019: 
2024-12-20 01:23:01.381320: Epoch 147
2024-12-20 01:23:01.382115: Current learning rate: 0.0003
2024-12-20 01:31:14.224021: Validation loss did not improve from -0.48387. Patience: 84/50
2024-12-20 01:31:14.225133: train_loss -0.7646
2024-12-20 01:31:14.226049: val_loss -0.4217
2024-12-20 01:31:14.226853: Pseudo dice [0.6848]
2024-12-20 01:31:14.227684: Epoch time: 492.85 s
2024-12-20 01:31:15.791703: 
2024-12-20 01:31:15.793016: Epoch 148
2024-12-20 01:31:15.793745: Current learning rate: 0.00021
2024-12-20 01:38:52.557314: Validation loss did not improve from -0.48387. Patience: 85/50
2024-12-20 01:38:52.558379: train_loss -0.7684
2024-12-20 01:38:52.559338: val_loss -0.4118
2024-12-20 01:38:52.560112: Pseudo dice [0.6798]
2024-12-20 01:38:52.560925: Epoch time: 456.77 s
2024-12-20 01:38:54.114231: 
2024-12-20 01:38:54.115563: Epoch 149
2024-12-20 01:38:54.116467: Current learning rate: 0.00011
2024-12-20 01:46:49.614402: Validation loss did not improve from -0.48387. Patience: 86/50
2024-12-20 01:46:49.616875: train_loss -0.7623
2024-12-20 01:46:49.617972: val_loss -0.3847
2024-12-20 01:46:49.618657: Pseudo dice [0.6799]
2024-12-20 01:46:49.619503: Epoch time: 475.5 s
2024-12-20 01:46:51.701483: Training done.
2024-12-20 01:46:52.076516: Using splits from existing split file: /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset308_Sohee_Calcium_OCT_CrossValidation/splits_final.json
2024-12-20 01:46:52.089470: The split file contains 5 splits.
2024-12-20 01:46:52.090457: Desired fold for training: 3
2024-12-20 01:46:52.091164: This split has 7 training and 1 validation cases.
2024-12-20 01:46:52.092150: predicting 701-013
2024-12-20 01:46:52.136150: 701-013, shape torch.Size([1, 375, 498, 498]), rank 0
2024-12-20 01:50:28.079031: Validation complete
2024-12-20 01:50:28.079572: Mean Validation Dice:  0.6693466542246648
2024-12-19 13:30:09.472896: unpacking done...
2024-12-19 13:30:10.529529: Unable to plot network architecture: nnUNet_compile is enabled!
2024-12-19 13:30:10.612430: 
2024-12-19 13:30:10.613798: Epoch 0
2024-12-19 13:30:10.614723: Current learning rate: 0.01
2024-12-19 13:35:27.527896: Validation loss improved from 1000.00000 to -0.19051! Patience: 0/50
2024-12-19 13:35:27.529005: train_loss -0.0772
2024-12-19 13:35:27.529929: val_loss -0.1905
2024-12-19 13:35:27.530895: Pseudo dice [0.5275]
2024-12-19 13:35:27.531861: Epoch time: 316.92 s
2024-12-19 13:35:27.532876: Yayy! New best EMA pseudo Dice: 0.5275
2024-12-19 13:35:29.178051: 
2024-12-19 13:35:29.179502: Epoch 1
2024-12-19 13:35:29.180455: Current learning rate: 0.00994
2024-12-19 13:39:15.518021: Validation loss improved from -0.19051 to -0.26902! Patience: 0/50
2024-12-19 13:39:15.518954: train_loss -0.225
2024-12-19 13:39:15.519743: val_loss -0.269
2024-12-19 13:39:15.520491: Pseudo dice [0.5581]
2024-12-19 13:39:15.521237: Epoch time: 226.34 s
2024-12-19 13:39:15.521940: Yayy! New best EMA pseudo Dice: 0.5306
2024-12-19 13:39:17.397574: 
2024-12-19 13:39:17.399009: Epoch 2
2024-12-19 13:39:17.399848: Current learning rate: 0.00988
2024-12-19 13:43:02.980490: Validation loss improved from -0.26902 to -0.31678! Patience: 0/50
2024-12-19 13:43:02.981584: train_loss -0.2702
2024-12-19 13:43:02.982554: val_loss -0.3168
2024-12-19 13:43:02.983302: Pseudo dice [0.5964]
2024-12-19 13:43:02.984050: Epoch time: 225.59 s
2024-12-19 13:43:02.984841: Yayy! New best EMA pseudo Dice: 0.5371
2024-12-19 13:43:04.851412: 
2024-12-19 13:43:04.852746: Epoch 3
2024-12-19 13:43:04.853573: Current learning rate: 0.00982
2024-12-19 13:46:46.147735: Validation loss improved from -0.31678 to -0.36851! Patience: 0/50
2024-12-19 13:46:46.148800: train_loss -0.2954
2024-12-19 13:46:46.149673: val_loss -0.3685
2024-12-19 13:46:46.150343: Pseudo dice [0.6358]
2024-12-19 13:46:46.151025: Epoch time: 221.3 s
2024-12-19 13:46:46.151686: Yayy! New best EMA pseudo Dice: 0.547
2024-12-19 13:46:48.204201: 
2024-12-19 13:46:48.205532: Epoch 4
2024-12-19 13:46:48.206354: Current learning rate: 0.00976
2024-12-19 13:50:41.696624: Validation loss improved from -0.36851 to -0.42279! Patience: 0/50
2024-12-19 13:50:41.697754: train_loss -0.347
2024-12-19 13:50:41.698744: val_loss -0.4228
2024-12-19 13:50:41.699575: Pseudo dice [0.6664]
2024-12-19 13:50:41.700400: Epoch time: 233.49 s
2024-12-19 13:50:42.148815: Yayy! New best EMA pseudo Dice: 0.5589
2024-12-19 13:50:44.086248: 
2024-12-19 13:50:44.087630: Epoch 5
2024-12-19 13:50:44.088436: Current learning rate: 0.0097
2024-12-19 13:54:31.175637: Validation loss did not improve from -0.42279. Patience: 1/50
2024-12-19 13:54:31.176700: train_loss -0.3664
2024-12-19 13:54:31.177658: val_loss -0.4037
2024-12-19 13:54:31.178613: Pseudo dice [0.6598]
2024-12-19 13:54:31.179485: Epoch time: 227.09 s
2024-12-19 13:54:31.180371: Yayy! New best EMA pseudo Dice: 0.569
2024-12-19 13:54:33.060315: 
2024-12-19 13:54:33.061699: Epoch 6
2024-12-19 13:54:33.062572: Current learning rate: 0.00964
2024-12-19 13:58:14.411430: Validation loss improved from -0.42279 to -0.43265! Patience: 1/50
2024-12-19 13:58:14.412429: train_loss -0.3876
2024-12-19 13:58:14.413206: val_loss -0.4326
2024-12-19 13:58:14.413857: Pseudo dice [0.668]
2024-12-19 13:58:14.414712: Epoch time: 221.35 s
2024-12-19 13:58:14.415468: Yayy! New best EMA pseudo Dice: 0.5789
2024-12-19 13:58:16.399018: 
2024-12-19 13:58:16.400437: Epoch 7
2024-12-19 13:58:16.401531: Current learning rate: 0.00958
2024-12-19 14:02:11.476216: Validation loss improved from -0.43265 to -0.43570! Patience: 0/50
2024-12-19 14:02:11.477179: train_loss -0.4025
2024-12-19 14:02:11.477952: val_loss -0.4357
2024-12-19 14:02:11.478679: Pseudo dice [0.6638]
2024-12-19 14:02:11.479414: Epoch time: 235.08 s
2024-12-19 14:02:11.480214: Yayy! New best EMA pseudo Dice: 0.5874
2024-12-19 14:02:13.336230: 
2024-12-19 14:02:13.337462: Epoch 8
2024-12-19 14:02:13.338275: Current learning rate: 0.00952
2024-12-19 14:05:54.868674: Validation loss improved from -0.43570 to -0.45011! Patience: 0/50
2024-12-19 14:05:54.869749: train_loss -0.4248
2024-12-19 14:05:54.870600: val_loss -0.4501
2024-12-19 14:05:54.871245: Pseudo dice [0.6687]
2024-12-19 14:05:54.871959: Epoch time: 221.53 s
2024-12-19 14:05:54.872631: Yayy! New best EMA pseudo Dice: 0.5955
2024-12-19 14:05:57.203032: 
2024-12-19 14:05:57.204329: Epoch 9
2024-12-19 14:05:57.205156: Current learning rate: 0.00946
2024-12-19 14:09:43.559619: Validation loss improved from -0.45011 to -0.46599! Patience: 0/50
2024-12-19 14:09:43.560700: train_loss -0.4372
2024-12-19 14:09:43.561588: val_loss -0.466
2024-12-19 14:09:43.562347: Pseudo dice [0.6926]
2024-12-19 14:09:43.563161: Epoch time: 226.36 s
2024-12-19 14:09:44.048712: Yayy! New best EMA pseudo Dice: 0.6053
2024-12-19 14:09:45.984002: 
2024-12-19 14:09:45.985101: Epoch 10
2024-12-19 14:09:45.985824: Current learning rate: 0.0094
2024-12-19 14:13:42.152588: Validation loss did not improve from -0.46599. Patience: 1/50
2024-12-19 14:13:42.153504: train_loss -0.4655
2024-12-19 14:13:42.154454: val_loss -0.456
2024-12-19 14:13:42.155332: Pseudo dice [0.6739]
2024-12-19 14:13:42.156199: Epoch time: 236.17 s
2024-12-19 14:13:42.156937: Yayy! New best EMA pseudo Dice: 0.6121
2024-12-19 14:13:44.093211: 
2024-12-19 14:13:44.094532: Epoch 11
2024-12-19 14:13:44.095354: Current learning rate: 0.00934
2024-12-19 14:17:37.974955: Validation loss improved from -0.46599 to -0.50763! Patience: 1/50
2024-12-19 14:17:37.975964: train_loss -0.4576
2024-12-19 14:17:37.976879: val_loss -0.5076
2024-12-19 14:17:37.977695: Pseudo dice [0.7078]
2024-12-19 14:17:37.978440: Epoch time: 233.88 s
2024-12-19 14:17:37.979195: Yayy! New best EMA pseudo Dice: 0.6217
2024-12-19 14:17:39.873966: 
2024-12-19 14:17:39.875309: Epoch 12
2024-12-19 14:17:39.876396: Current learning rate: 0.00928
2024-12-19 14:21:29.417730: Validation loss did not improve from -0.50763. Patience: 1/50
2024-12-19 14:21:29.418833: train_loss -0.4705
2024-12-19 14:21:29.419596: val_loss -0.4234
2024-12-19 14:21:29.420347: Pseudo dice [0.6655]
2024-12-19 14:21:29.421116: Epoch time: 229.55 s
2024-12-19 14:21:29.421832: Yayy! New best EMA pseudo Dice: 0.6261
2024-12-19 14:21:31.354239: 
2024-12-19 14:21:31.355618: Epoch 13
2024-12-19 14:21:31.356430: Current learning rate: 0.00922
2024-12-19 14:25:24.929705: Validation loss did not improve from -0.50763. Patience: 2/50
2024-12-19 14:25:24.930787: train_loss -0.4829
2024-12-19 14:25:24.931742: val_loss -0.4844
2024-12-19 14:25:24.932462: Pseudo dice [0.7003]
2024-12-19 14:25:24.933222: Epoch time: 233.58 s
2024-12-19 14:25:24.933985: Yayy! New best EMA pseudo Dice: 0.6335
2024-12-19 14:25:26.864089: 
2024-12-19 14:25:26.865290: Epoch 14
2024-12-19 14:25:26.866098: Current learning rate: 0.00916
2024-12-19 14:29:29.755559: Validation loss did not improve from -0.50763. Patience: 3/50
2024-12-19 14:29:29.756584: train_loss -0.4835
2024-12-19 14:29:29.757532: val_loss -0.4553
2024-12-19 14:29:29.758317: Pseudo dice [0.6736]
2024-12-19 14:29:29.759091: Epoch time: 242.89 s
2024-12-19 14:29:30.197787: Yayy! New best EMA pseudo Dice: 0.6375
2024-12-19 14:29:32.091492: 
2024-12-19 14:29:32.093194: Epoch 15
2024-12-19 14:29:32.094380: Current learning rate: 0.0091
2024-12-19 14:33:57.036728: Validation loss did not improve from -0.50763. Patience: 4/50
2024-12-19 14:33:57.037569: train_loss -0.4974
2024-12-19 14:33:57.038412: val_loss -0.4723
2024-12-19 14:33:57.039190: Pseudo dice [0.6973]
2024-12-19 14:33:57.040072: Epoch time: 264.95 s
2024-12-19 14:33:57.040969: Yayy! New best EMA pseudo Dice: 0.6435
2024-12-19 14:33:58.899533: 
2024-12-19 14:33:58.900935: Epoch 16
2024-12-19 14:33:58.901741: Current learning rate: 0.00903
2024-12-19 14:38:13.584169: Validation loss did not improve from -0.50763. Patience: 5/50
2024-12-19 14:38:13.585387: train_loss -0.4957
2024-12-19 14:38:13.586265: val_loss -0.47
2024-12-19 14:38:13.587095: Pseudo dice [0.6907]
2024-12-19 14:38:13.587955: Epoch time: 254.69 s
2024-12-19 14:38:13.588679: Yayy! New best EMA pseudo Dice: 0.6482
2024-12-19 14:38:15.465715: 
2024-12-19 14:38:15.467220: Epoch 17
2024-12-19 14:38:15.468028: Current learning rate: 0.00897
2024-12-19 14:42:22.312716: Validation loss improved from -0.50763 to -0.51400! Patience: 5/50
2024-12-19 14:42:22.313800: train_loss -0.5074
2024-12-19 14:42:22.314662: val_loss -0.514
2024-12-19 14:42:22.315448: Pseudo dice [0.713]
2024-12-19 14:42:22.316282: Epoch time: 246.85 s
2024-12-19 14:42:22.317135: Yayy! New best EMA pseudo Dice: 0.6547
2024-12-19 14:42:24.318549: 
2024-12-19 14:42:24.319981: Epoch 18
2024-12-19 14:42:24.320781: Current learning rate: 0.00891
2024-12-19 14:46:11.857971: Validation loss improved from -0.51400 to -0.51585! Patience: 0/50
2024-12-19 14:46:11.858956: train_loss -0.518
2024-12-19 14:46:11.859987: val_loss -0.5159
2024-12-19 14:46:11.860795: Pseudo dice [0.7141]
2024-12-19 14:46:11.861655: Epoch time: 227.54 s
2024-12-19 14:46:11.862463: Yayy! New best EMA pseudo Dice: 0.6606
2024-12-19 14:46:14.043696: 
2024-12-19 14:46:14.045189: Epoch 19
2024-12-19 14:46:14.045992: Current learning rate: 0.00885
2024-12-19 14:50:10.646872: Validation loss improved from -0.51585 to -0.55140! Patience: 0/50
2024-12-19 14:50:10.647902: train_loss -0.5288
2024-12-19 14:50:10.648815: val_loss -0.5514
2024-12-19 14:50:10.649558: Pseudo dice [0.7362]
2024-12-19 14:50:10.650286: Epoch time: 236.61 s
2024-12-19 14:50:11.083415: Yayy! New best EMA pseudo Dice: 0.6682
2024-12-19 14:50:12.918922: 
2024-12-19 14:50:12.920240: Epoch 20
2024-12-19 14:50:12.921239: Current learning rate: 0.00879
2024-12-19 14:54:09.802957: Validation loss did not improve from -0.55140. Patience: 1/50
2024-12-19 14:54:09.804003: train_loss -0.5191
2024-12-19 14:54:09.805094: val_loss -0.5329
2024-12-19 14:54:09.806077: Pseudo dice [0.7282]
2024-12-19 14:54:09.807040: Epoch time: 236.89 s
2024-12-19 14:54:09.807987: Yayy! New best EMA pseudo Dice: 0.6742
2024-12-19 14:54:11.682769: 
2024-12-19 14:54:11.683943: Epoch 21
2024-12-19 14:54:11.685104: Current learning rate: 0.00873
2024-12-19 14:57:59.209262: Validation loss did not improve from -0.55140. Patience: 2/50
2024-12-19 14:57:59.210141: train_loss -0.5233
2024-12-19 14:57:59.210966: val_loss -0.5305
2024-12-19 14:57:59.211855: Pseudo dice [0.725]
2024-12-19 14:57:59.212702: Epoch time: 227.53 s
2024-12-19 14:57:59.213413: Yayy! New best EMA pseudo Dice: 0.6793
2024-12-19 14:58:00.939787: 
2024-12-19 14:58:00.940848: Epoch 22
2024-12-19 14:58:00.941599: Current learning rate: 0.00867
2024-12-19 15:01:40.903414: Validation loss did not improve from -0.55140. Patience: 3/50
2024-12-19 15:01:40.904340: train_loss -0.5355
2024-12-19 15:01:40.905310: val_loss -0.506
2024-12-19 15:01:40.906224: Pseudo dice [0.72]
2024-12-19 15:01:40.906991: Epoch time: 219.97 s
2024-12-19 15:01:40.907992: Yayy! New best EMA pseudo Dice: 0.6833
2024-12-19 15:01:42.730582: 
2024-12-19 15:01:42.732012: Epoch 23
2024-12-19 15:01:42.733023: Current learning rate: 0.00861
2024-12-19 15:05:40.309432: Validation loss improved from -0.55140 to -0.55506! Patience: 3/50
2024-12-19 15:05:40.310369: train_loss -0.5415
2024-12-19 15:05:40.311195: val_loss -0.5551
2024-12-19 15:05:40.311977: Pseudo dice [0.747]
2024-12-19 15:05:40.312891: Epoch time: 237.58 s
2024-12-19 15:05:40.313605: Yayy! New best EMA pseudo Dice: 0.6897
2024-12-19 15:05:42.134525: 
2024-12-19 15:05:42.135916: Epoch 24
2024-12-19 15:05:42.136715: Current learning rate: 0.00855
2024-12-19 15:09:49.447351: Validation loss did not improve from -0.55506. Patience: 1/50
2024-12-19 15:09:49.448363: train_loss -0.5466
2024-12-19 15:09:49.449238: val_loss -0.5013
2024-12-19 15:09:49.449994: Pseudo dice [0.7131]
2024-12-19 15:09:49.450735: Epoch time: 247.32 s
2024-12-19 15:09:49.872445: Yayy! New best EMA pseudo Dice: 0.692
2024-12-19 15:09:51.717035: 
2024-12-19 15:09:51.718425: Epoch 25
2024-12-19 15:09:51.719561: Current learning rate: 0.00849
2024-12-19 15:13:54.201063: Validation loss did not improve from -0.55506. Patience: 2/50
2024-12-19 15:13:54.202011: train_loss -0.5518
2024-12-19 15:13:54.202923: val_loss -0.5463
2024-12-19 15:13:54.203765: Pseudo dice [0.7384]
2024-12-19 15:13:54.204587: Epoch time: 242.49 s
2024-12-19 15:13:54.205280: Yayy! New best EMA pseudo Dice: 0.6967
2024-12-19 15:13:56.019919: 
2024-12-19 15:13:56.021191: Epoch 26
2024-12-19 15:13:56.022042: Current learning rate: 0.00843
2024-12-19 15:17:55.978945: Validation loss did not improve from -0.55506. Patience: 3/50
2024-12-19 15:17:55.980005: train_loss -0.5632
2024-12-19 15:17:55.980973: val_loss -0.5453
2024-12-19 15:17:55.981943: Pseudo dice [0.7304]
2024-12-19 15:17:55.982921: Epoch time: 239.96 s
2024-12-19 15:17:55.983925: Yayy! New best EMA pseudo Dice: 0.7
2024-12-19 15:17:57.934084: 
2024-12-19 15:17:57.935623: Epoch 27
2024-12-19 15:17:57.936621: Current learning rate: 0.00836
2024-12-19 15:21:49.869666: Validation loss did not improve from -0.55506. Patience: 4/50
2024-12-19 15:21:49.870418: train_loss -0.5582
2024-12-19 15:21:49.871265: val_loss -0.5369
2024-12-19 15:21:49.871914: Pseudo dice [0.7227]
2024-12-19 15:21:49.872591: Epoch time: 231.94 s
2024-12-19 15:21:49.873242: Yayy! New best EMA pseudo Dice: 0.7023
2024-12-19 15:21:51.666598: 
2024-12-19 15:21:51.667845: Epoch 28
2024-12-19 15:21:51.668630: Current learning rate: 0.0083
2024-12-19 15:25:40.051748: Validation loss did not improve from -0.55506. Patience: 5/50
2024-12-19 15:25:40.052726: train_loss -0.5774
2024-12-19 15:25:40.053596: val_loss -0.5376
2024-12-19 15:25:40.054377: Pseudo dice [0.7295]
2024-12-19 15:25:40.055040: Epoch time: 228.39 s
2024-12-19 15:25:40.055714: Yayy! New best EMA pseudo Dice: 0.705
2024-12-19 15:25:41.857195: 
2024-12-19 15:25:41.858415: Epoch 29
2024-12-19 15:25:41.859175: Current learning rate: 0.00824
2024-12-19 15:30:28.584260: Validation loss did not improve from -0.55506. Patience: 6/50
2024-12-19 15:30:28.585327: train_loss -0.5785
2024-12-19 15:30:28.586223: val_loss -0.5309
2024-12-19 15:30:28.587101: Pseudo dice [0.7315]
2024-12-19 15:30:28.587932: Epoch time: 286.73 s
2024-12-19 15:30:29.413107: Yayy! New best EMA pseudo Dice: 0.7077
2024-12-19 15:30:31.317189: 
2024-12-19 15:30:31.318618: Epoch 30
2024-12-19 15:30:31.319604: Current learning rate: 0.00818
2024-12-19 15:34:33.514723: Validation loss did not improve from -0.55506. Patience: 7/50
2024-12-19 15:34:33.515795: train_loss -0.5843
2024-12-19 15:34:33.516649: val_loss -0.5195
2024-12-19 15:34:33.517452: Pseudo dice [0.7259]
2024-12-19 15:34:33.518178: Epoch time: 242.2 s
2024-12-19 15:34:33.518816: Yayy! New best EMA pseudo Dice: 0.7095
2024-12-19 15:34:35.415746: 
2024-12-19 15:34:35.417003: Epoch 31
2024-12-19 15:34:35.417792: Current learning rate: 0.00812
2024-12-19 15:38:56.933252: Validation loss did not improve from -0.55506. Patience: 8/50
2024-12-19 15:38:56.949918: train_loss -0.5782
2024-12-19 15:38:56.950901: val_loss -0.5184
2024-12-19 15:38:56.951599: Pseudo dice [0.7185]
2024-12-19 15:38:56.952435: Epoch time: 261.54 s
2024-12-19 15:38:56.953470: Yayy! New best EMA pseudo Dice: 0.7104
2024-12-19 15:38:59.023076: 
2024-12-19 15:38:59.024004: Epoch 32
2024-12-19 15:38:59.024969: Current learning rate: 0.00806
2024-12-19 15:43:33.906090: Validation loss did not improve from -0.55506. Patience: 9/50
2024-12-19 15:43:33.907180: train_loss -0.5823
2024-12-19 15:43:33.908140: val_loss -0.5537
2024-12-19 15:43:33.908956: Pseudo dice [0.7408]
2024-12-19 15:43:33.909832: Epoch time: 274.89 s
2024-12-19 15:43:33.910730: Yayy! New best EMA pseudo Dice: 0.7134
2024-12-19 15:43:35.750826: 
2024-12-19 15:43:35.752300: Epoch 33
2024-12-19 15:43:35.753154: Current learning rate: 0.008
2024-12-19 15:48:27.187955: Validation loss improved from -0.55506 to -0.56160! Patience: 9/50
2024-12-19 15:48:27.189689: train_loss -0.5808
2024-12-19 15:48:27.190589: val_loss -0.5616
2024-12-19 15:48:27.191307: Pseudo dice [0.7432]
2024-12-19 15:48:27.191996: Epoch time: 291.44 s
2024-12-19 15:48:27.192677: Yayy! New best EMA pseudo Dice: 0.7164
2024-12-19 15:48:29.108798: 
2024-12-19 15:48:29.110236: Epoch 34
2024-12-19 15:48:29.111020: Current learning rate: 0.00793
2024-12-19 15:52:37.077807: Validation loss improved from -0.56160 to -0.56237! Patience: 0/50
2024-12-19 15:52:37.078942: train_loss -0.5869
2024-12-19 15:52:37.080071: val_loss -0.5624
2024-12-19 15:52:37.080956: Pseudo dice [0.7437]
2024-12-19 15:52:37.081846: Epoch time: 247.97 s
2024-12-19 15:52:37.581116: Yayy! New best EMA pseudo Dice: 0.7192
2024-12-19 15:52:39.579002: 
2024-12-19 15:52:39.580312: Epoch 35
2024-12-19 15:52:39.581192: Current learning rate: 0.00787
2024-12-19 15:56:35.288537: Validation loss did not improve from -0.56237. Patience: 1/50
2024-12-19 15:56:35.289527: train_loss -0.5931
2024-12-19 15:56:35.290388: val_loss -0.5278
2024-12-19 15:56:35.291235: Pseudo dice [0.7268]
2024-12-19 15:56:35.291935: Epoch time: 235.71 s
2024-12-19 15:56:35.292696: Yayy! New best EMA pseudo Dice: 0.7199
2024-12-19 15:56:37.179756: 
2024-12-19 15:56:37.180982: Epoch 36
2024-12-19 15:56:37.181748: Current learning rate: 0.00781
2024-12-19 16:00:54.081434: Validation loss did not improve from -0.56237. Patience: 2/50
2024-12-19 16:00:54.082343: train_loss -0.5926
2024-12-19 16:00:54.083149: val_loss -0.5611
2024-12-19 16:00:54.083846: Pseudo dice [0.7448]
2024-12-19 16:00:54.084542: Epoch time: 256.9 s
2024-12-19 16:00:54.085313: Yayy! New best EMA pseudo Dice: 0.7224
2024-12-19 16:00:55.938868: 
2024-12-19 16:00:55.939892: Epoch 37
2024-12-19 16:00:55.940685: Current learning rate: 0.00775
2024-12-19 16:04:51.122851: Validation loss did not improve from -0.56237. Patience: 3/50
2024-12-19 16:04:51.123822: train_loss -0.5926
2024-12-19 16:04:51.124785: val_loss -0.557
2024-12-19 16:04:51.125698: Pseudo dice [0.7351]
2024-12-19 16:04:51.126693: Epoch time: 235.19 s
2024-12-19 16:04:51.127600: Yayy! New best EMA pseudo Dice: 0.7237
2024-12-19 16:04:53.064355: 
2024-12-19 16:04:53.065707: Epoch 38
2024-12-19 16:04:53.066719: Current learning rate: 0.00769
2024-12-19 16:09:04.748894: Validation loss did not improve from -0.56237. Patience: 4/50
2024-12-19 16:09:04.749836: train_loss -0.5942
2024-12-19 16:09:04.750623: val_loss -0.5239
2024-12-19 16:09:04.751405: Pseudo dice [0.7196]
2024-12-19 16:09:04.752275: Epoch time: 251.69 s
2024-12-19 16:09:06.185520: 
2024-12-19 16:09:06.186639: Epoch 39
2024-12-19 16:09:06.187405: Current learning rate: 0.00763
2024-12-19 16:13:20.797588: Validation loss did not improve from -0.56237. Patience: 5/50
2024-12-19 16:13:20.798372: train_loss -0.602
2024-12-19 16:13:20.799221: val_loss -0.5232
2024-12-19 16:13:20.799971: Pseudo dice [0.7153]
2024-12-19 16:13:20.800842: Epoch time: 254.61 s
2024-12-19 16:13:23.108240: 
2024-12-19 16:13:23.109561: Epoch 40
2024-12-19 16:13:23.110301: Current learning rate: 0.00756
2024-12-19 16:17:42.705374: Validation loss improved from -0.56237 to -0.57033! Patience: 5/50
2024-12-19 16:17:42.706458: train_loss -0.6129
2024-12-19 16:17:42.707347: val_loss -0.5703
2024-12-19 16:17:42.708277: Pseudo dice [0.7511]
2024-12-19 16:17:42.709095: Epoch time: 259.6 s
2024-12-19 16:17:42.709876: Yayy! New best EMA pseudo Dice: 0.7253
2024-12-19 16:17:44.648837: 
2024-12-19 16:17:44.650060: Epoch 41
2024-12-19 16:17:44.650921: Current learning rate: 0.0075
2024-12-19 16:21:43.499286: Validation loss did not improve from -0.57033. Patience: 1/50
2024-12-19 16:21:43.500146: train_loss -0.6101
2024-12-19 16:21:43.501061: val_loss -0.5459
2024-12-19 16:21:43.501904: Pseudo dice [0.7372]
2024-12-19 16:21:43.502680: Epoch time: 238.85 s
2024-12-19 16:21:43.503519: Yayy! New best EMA pseudo Dice: 0.7265
2024-12-19 16:21:45.257472: 
2024-12-19 16:21:45.258706: Epoch 42
2024-12-19 16:21:45.259544: Current learning rate: 0.00744
2024-12-19 16:25:39.242296: Validation loss improved from -0.57033 to -0.57833! Patience: 1/50
2024-12-19 16:25:39.243300: train_loss -0.6094
2024-12-19 16:25:39.244235: val_loss -0.5783
2024-12-19 16:25:39.245093: Pseudo dice [0.764]
2024-12-19 16:25:39.245901: Epoch time: 233.99 s
2024-12-19 16:25:39.246742: Yayy! New best EMA pseudo Dice: 0.7303
2024-12-19 16:25:41.097165: 
2024-12-19 16:25:41.098420: Epoch 43
2024-12-19 16:25:41.099188: Current learning rate: 0.00738
2024-12-19 16:29:36.699362: Validation loss did not improve from -0.57833. Patience: 1/50
2024-12-19 16:29:36.700349: train_loss -0.6169
2024-12-19 16:29:36.701278: val_loss -0.5461
2024-12-19 16:29:36.701936: Pseudo dice [0.7358]
2024-12-19 16:29:36.702645: Epoch time: 235.6 s
2024-12-19 16:29:36.703394: Yayy! New best EMA pseudo Dice: 0.7308
2024-12-19 16:29:38.545501: 
2024-12-19 16:29:38.546900: Epoch 44
2024-12-19 16:29:38.547611: Current learning rate: 0.00732
2024-12-19 16:33:34.491855: Validation loss did not improve from -0.57833. Patience: 2/50
2024-12-19 16:33:34.492874: train_loss -0.617
2024-12-19 16:33:34.493925: val_loss -0.5437
2024-12-19 16:33:34.494818: Pseudo dice [0.7359]
2024-12-19 16:33:34.495622: Epoch time: 235.95 s
2024-12-19 16:33:34.943187: Yayy! New best EMA pseudo Dice: 0.7313
2024-12-19 16:33:36.787572: 
2024-12-19 16:33:36.789009: Epoch 45
2024-12-19 16:33:36.790174: Current learning rate: 0.00725
2024-12-19 16:37:47.162817: Validation loss did not improve from -0.57833. Patience: 3/50
2024-12-19 16:37:47.163835: train_loss -0.6234
2024-12-19 16:37:47.164701: val_loss -0.5427
2024-12-19 16:37:47.165554: Pseudo dice [0.7392]
2024-12-19 16:37:47.166327: Epoch time: 250.38 s
2024-12-19 16:37:47.167156: Yayy! New best EMA pseudo Dice: 0.7321
2024-12-19 16:37:49.068980: 
2024-12-19 16:37:49.070280: Epoch 46
2024-12-19 16:37:49.071011: Current learning rate: 0.00719
2024-12-19 16:41:53.623954: Validation loss did not improve from -0.57833. Patience: 4/50
2024-12-19 16:41:53.624949: train_loss -0.6227
2024-12-19 16:41:53.625859: val_loss -0.5276
2024-12-19 16:41:53.626704: Pseudo dice [0.733]
2024-12-19 16:41:53.627696: Epoch time: 244.56 s
2024-12-19 16:41:53.628631: Yayy! New best EMA pseudo Dice: 0.7322
2024-12-19 16:41:55.509604: 
2024-12-19 16:41:55.511124: Epoch 47
2024-12-19 16:41:55.512271: Current learning rate: 0.00713
2024-12-19 16:46:13.637571: Validation loss did not improve from -0.57833. Patience: 5/50
2024-12-19 16:46:13.638459: train_loss -0.6292
2024-12-19 16:46:13.640833: val_loss -0.5067
2024-12-19 16:46:13.641923: Pseudo dice [0.7095]
2024-12-19 16:46:13.643078: Epoch time: 258.13 s
2024-12-19 16:46:15.087511: 
2024-12-19 16:46:15.088839: Epoch 48
2024-12-19 16:46:15.089617: Current learning rate: 0.00707
2024-12-19 16:50:17.704214: Validation loss did not improve from -0.57833. Patience: 6/50
2024-12-19 16:50:17.705170: train_loss -0.6328
2024-12-19 16:50:17.705979: val_loss -0.5588
2024-12-19 16:50:17.706778: Pseudo dice [0.7434]
2024-12-19 16:50:17.707469: Epoch time: 242.62 s
2024-12-19 16:50:19.149008: 
2024-12-19 16:50:19.150456: Epoch 49
2024-12-19 16:50:19.151246: Current learning rate: 0.007
2024-12-19 16:54:41.433334: Validation loss did not improve from -0.57833. Patience: 7/50
2024-12-19 16:54:41.434318: train_loss -0.6331
2024-12-19 16:54:41.435173: val_loss -0.5442
2024-12-19 16:54:41.435992: Pseudo dice [0.7297]
2024-12-19 16:54:41.436822: Epoch time: 262.29 s
2024-12-19 16:54:43.282507: 
2024-12-19 16:54:43.283758: Epoch 50
2024-12-19 16:54:43.284590: Current learning rate: 0.00694
2024-12-19 16:58:41.492226: Validation loss improved from -0.57833 to -0.58461! Patience: 7/50
2024-12-19 16:58:41.493115: train_loss -0.6429
2024-12-19 16:58:41.494017: val_loss -0.5846
2024-12-19 16:58:41.494783: Pseudo dice [0.7555]
2024-12-19 16:58:41.495497: Epoch time: 238.21 s
2024-12-19 16:58:41.496182: Yayy! New best EMA pseudo Dice: 0.7336
2024-12-19 16:58:43.392295: 
2024-12-19 16:58:43.393630: Epoch 51
2024-12-19 16:58:43.394391: Current learning rate: 0.00688
2024-12-19 17:02:37.635519: Validation loss improved from -0.58461 to -0.59621! Patience: 0/50
2024-12-19 17:02:37.636478: train_loss -0.6406
2024-12-19 17:02:37.637294: val_loss -0.5962
2024-12-19 17:02:37.638038: Pseudo dice [0.771]
2024-12-19 17:02:37.638785: Epoch time: 234.25 s
2024-12-19 17:02:37.639578: Yayy! New best EMA pseudo Dice: 0.7373
2024-12-19 17:02:40.192525: 
2024-12-19 17:02:40.193958: Epoch 52
2024-12-19 17:02:40.194897: Current learning rate: 0.00682
2024-12-19 17:06:58.114905: Validation loss did not improve from -0.59621. Patience: 1/50
2024-12-19 17:06:58.116800: train_loss -0.6412
2024-12-19 17:06:58.117694: val_loss -0.5534
2024-12-19 17:06:58.118369: Pseudo dice [0.7313]
2024-12-19 17:06:58.119142: Epoch time: 257.93 s
2024-12-19 17:06:59.575203: 
2024-12-19 17:06:59.576341: Epoch 53
2024-12-19 17:06:59.577055: Current learning rate: 0.00675
2024-12-19 17:11:09.205939: Validation loss did not improve from -0.59621. Patience: 2/50
2024-12-19 17:11:09.206983: train_loss -0.6473
2024-12-19 17:11:09.207993: val_loss -0.5699
2024-12-19 17:11:09.208787: Pseudo dice [0.7542]
2024-12-19 17:11:09.209551: Epoch time: 249.63 s
2024-12-19 17:11:09.210356: Yayy! New best EMA pseudo Dice: 0.7385
2024-12-19 17:11:11.127627: 
2024-12-19 17:11:11.128994: Epoch 54
2024-12-19 17:11:11.129838: Current learning rate: 0.00669
2024-12-19 17:15:14.502362: Validation loss did not improve from -0.59621. Patience: 3/50
2024-12-19 17:15:14.503485: train_loss -0.6343
2024-12-19 17:15:14.504402: val_loss -0.5824
2024-12-19 17:15:14.505235: Pseudo dice [0.7603]
2024-12-19 17:15:14.506126: Epoch time: 243.38 s
2024-12-19 17:15:14.955016: Yayy! New best EMA pseudo Dice: 0.7406
2024-12-19 17:15:16.776181: 
2024-12-19 17:15:16.777634: Epoch 55
2024-12-19 17:15:16.778667: Current learning rate: 0.00663
2024-12-19 17:19:21.856259: Validation loss did not improve from -0.59621. Patience: 4/50
2024-12-19 17:19:21.857959: train_loss -0.6374
2024-12-19 17:19:21.859262: val_loss -0.5525
2024-12-19 17:19:21.860013: Pseudo dice [0.7367]
2024-12-19 17:19:21.860815: Epoch time: 245.08 s
2024-12-19 17:19:23.330654: 
2024-12-19 17:19:23.332017: Epoch 56
2024-12-19 17:19:23.332728: Current learning rate: 0.00657
2024-12-19 17:23:33.259402: Validation loss did not improve from -0.59621. Patience: 5/50
2024-12-19 17:23:33.260450: train_loss -0.6396
2024-12-19 17:23:33.261427: val_loss -0.5546
2024-12-19 17:23:33.262271: Pseudo dice [0.7411]
2024-12-19 17:23:33.263139: Epoch time: 249.93 s
2024-12-19 17:23:34.754482: 
2024-12-19 17:23:34.755924: Epoch 57
2024-12-19 17:23:34.756802: Current learning rate: 0.0065
2024-12-19 17:27:39.654759: Validation loss did not improve from -0.59621. Patience: 6/50
2024-12-19 17:27:39.655799: train_loss -0.6467
2024-12-19 17:27:39.656948: val_loss -0.5961
2024-12-19 17:27:39.657959: Pseudo dice [0.7706]
2024-12-19 17:27:39.658987: Epoch time: 244.9 s
2024-12-19 17:27:39.660019: Yayy! New best EMA pseudo Dice: 0.7434
2024-12-19 17:27:41.666281: 
2024-12-19 17:27:41.667717: Epoch 58
2024-12-19 17:27:41.668561: Current learning rate: 0.00644
2024-12-19 17:31:54.586017: Validation loss did not improve from -0.59621. Patience: 7/50
2024-12-19 17:31:54.587001: train_loss -0.6552
2024-12-19 17:31:54.588009: val_loss -0.5677
2024-12-19 17:31:54.588843: Pseudo dice [0.7506]
2024-12-19 17:31:54.589630: Epoch time: 252.92 s
2024-12-19 17:31:54.590431: Yayy! New best EMA pseudo Dice: 0.7441
2024-12-19 17:31:56.538105: 
2024-12-19 17:31:56.539551: Epoch 59
2024-12-19 17:31:56.540494: Current learning rate: 0.00638
2024-12-19 17:36:22.768965: Validation loss did not improve from -0.59621. Patience: 8/50
2024-12-19 17:36:22.769917: train_loss -0.6551
2024-12-19 17:36:22.770708: val_loss -0.5583
2024-12-19 17:36:22.771348: Pseudo dice [0.7481]
2024-12-19 17:36:22.772122: Epoch time: 266.23 s
2024-12-19 17:36:23.169590: Yayy! New best EMA pseudo Dice: 0.7445
2024-12-19 17:36:24.930095: 
2024-12-19 17:36:24.931300: Epoch 60
2024-12-19 17:36:24.932152: Current learning rate: 0.00631
2024-12-19 17:41:06.208700: Validation loss did not improve from -0.59621. Patience: 9/50
2024-12-19 17:41:06.209842: train_loss -0.6546
2024-12-19 17:41:06.210729: val_loss -0.5873
2024-12-19 17:41:06.211473: Pseudo dice [0.7687]
2024-12-19 17:41:06.212153: Epoch time: 281.28 s
2024-12-19 17:41:06.212958: Yayy! New best EMA pseudo Dice: 0.7469
2024-12-19 17:41:08.058443: 
2024-12-19 17:41:08.059930: Epoch 61
2024-12-19 17:41:08.060916: Current learning rate: 0.00625
2024-12-19 17:45:29.999281: Validation loss did not improve from -0.59621. Patience: 10/50
2024-12-19 17:45:30.000242: train_loss -0.6463
2024-12-19 17:45:30.001266: val_loss -0.5589
2024-12-19 17:45:30.002033: Pseudo dice [0.7439]
2024-12-19 17:45:30.002792: Epoch time: 261.94 s
2024-12-19 17:45:31.814755: 
2024-12-19 17:45:31.815793: Epoch 62
2024-12-19 17:45:31.816632: Current learning rate: 0.00619
2024-12-19 17:50:01.453074: Validation loss improved from -0.59621 to -0.60906! Patience: 10/50
2024-12-19 17:50:01.454604: train_loss -0.6511
2024-12-19 17:50:01.455817: val_loss -0.6091
2024-12-19 17:50:01.456721: Pseudo dice [0.7809]
2024-12-19 17:50:01.457675: Epoch time: 269.64 s
2024-12-19 17:50:01.458568: Yayy! New best EMA pseudo Dice: 0.75
2024-12-19 17:50:03.335479: 
2024-12-19 17:50:03.336749: Epoch 63
2024-12-19 17:50:03.337629: Current learning rate: 0.00612
2024-12-19 17:54:08.730299: Validation loss did not improve from -0.60906. Patience: 1/50
2024-12-19 17:54:08.731581: train_loss -0.6486
2024-12-19 17:54:08.732481: val_loss -0.5709
2024-12-19 17:54:08.733147: Pseudo dice [0.7501]
2024-12-19 17:54:08.734001: Epoch time: 245.4 s
2024-12-19 17:54:08.734837: Yayy! New best EMA pseudo Dice: 0.7501
2024-12-19 17:54:10.583019: 
2024-12-19 17:54:10.584307: Epoch 64
2024-12-19 17:54:10.584978: Current learning rate: 0.00606
2024-12-19 17:58:16.888746: Validation loss did not improve from -0.60906. Patience: 2/50
2024-12-19 17:58:16.889694: train_loss -0.6601
2024-12-19 17:58:16.890638: val_loss -0.5977
2024-12-19 17:58:16.891677: Pseudo dice [0.7704]
2024-12-19 17:58:16.892621: Epoch time: 246.31 s
2024-12-19 17:58:17.467471: Yayy! New best EMA pseudo Dice: 0.7521
2024-12-19 17:58:19.320443: 
2024-12-19 17:58:19.321537: Epoch 65
2024-12-19 17:58:19.322245: Current learning rate: 0.006
2024-12-19 18:02:27.320905: Validation loss did not improve from -0.60906. Patience: 3/50
2024-12-19 18:02:27.321797: train_loss -0.6665
2024-12-19 18:02:27.322648: val_loss -0.5923
2024-12-19 18:02:27.323373: Pseudo dice [0.7699]
2024-12-19 18:02:27.324122: Epoch time: 248.0 s
2024-12-19 18:02:27.324860: Yayy! New best EMA pseudo Dice: 0.7539
2024-12-19 18:02:29.246247: 
2024-12-19 18:02:29.247391: Epoch 66
2024-12-19 18:02:29.248140: Current learning rate: 0.00593
2024-12-19 18:06:46.161085: Validation loss improved from -0.60906 to -0.61259! Patience: 3/50
2024-12-19 18:06:46.161962: train_loss -0.6707
2024-12-19 18:06:46.162904: val_loss -0.6126
2024-12-19 18:06:46.163726: Pseudo dice [0.781]
2024-12-19 18:06:46.164564: Epoch time: 256.92 s
2024-12-19 18:06:46.165413: Yayy! New best EMA pseudo Dice: 0.7566
2024-12-19 18:06:47.977628: 
2024-12-19 18:06:47.979159: Epoch 67
2024-12-19 18:06:47.980100: Current learning rate: 0.00587
2024-12-19 18:10:42.823451: Validation loss did not improve from -0.61259. Patience: 1/50
2024-12-19 18:10:42.824477: train_loss -0.6725
2024-12-19 18:10:42.825332: val_loss -0.6089
2024-12-19 18:10:42.826113: Pseudo dice [0.7726]
2024-12-19 18:10:42.826959: Epoch time: 234.85 s
2024-12-19 18:10:42.827828: Yayy! New best EMA pseudo Dice: 0.7582
2024-12-19 18:10:44.727492: 
2024-12-19 18:10:44.728949: Epoch 68
2024-12-19 18:10:44.730197: Current learning rate: 0.00581
2024-12-19 18:14:39.925024: Validation loss did not improve from -0.61259. Patience: 2/50
2024-12-19 18:14:39.925947: train_loss -0.6722
2024-12-19 18:14:39.926860: val_loss -0.6
2024-12-19 18:14:39.927691: Pseudo dice [0.7647]
2024-12-19 18:14:39.928536: Epoch time: 235.2 s
2024-12-19 18:14:39.929318: Yayy! New best EMA pseudo Dice: 0.7588
2024-12-19 18:14:41.833353: 
2024-12-19 18:14:41.834906: Epoch 69
2024-12-19 18:14:41.836020: Current learning rate: 0.00574
2024-12-19 18:18:54.904372: Validation loss did not improve from -0.61259. Patience: 3/50
2024-12-19 18:18:54.905500: train_loss -0.6791
2024-12-19 18:18:54.906438: val_loss -0.6042
2024-12-19 18:18:54.907187: Pseudo dice [0.7692]
2024-12-19 18:18:54.908041: Epoch time: 253.07 s
2024-12-19 18:18:55.330798: Yayy! New best EMA pseudo Dice: 0.7599
2024-12-19 18:18:57.177922: 
2024-12-19 18:18:57.179193: Epoch 70
2024-12-19 18:18:57.179948: Current learning rate: 0.00568
2024-12-19 18:23:07.583603: Validation loss did not improve from -0.61259. Patience: 4/50
2024-12-19 18:23:07.584721: train_loss -0.6633
2024-12-19 18:23:07.585638: val_loss -0.6125
2024-12-19 18:23:07.586308: Pseudo dice [0.7741]
2024-12-19 18:23:07.587080: Epoch time: 250.41 s
2024-12-19 18:23:07.587727: Yayy! New best EMA pseudo Dice: 0.7613
2024-12-19 18:23:09.432246: 
2024-12-19 18:23:09.433618: Epoch 71
2024-12-19 18:23:09.434353: Current learning rate: 0.00562
2024-12-19 18:27:03.922359: Validation loss did not improve from -0.61259. Patience: 5/50
2024-12-19 18:27:03.923304: train_loss -0.6722
2024-12-19 18:27:03.924212: val_loss -0.6004
2024-12-19 18:27:03.925109: Pseudo dice [0.7644]
2024-12-19 18:27:03.925855: Epoch time: 234.49 s
2024-12-19 18:27:03.926619: Yayy! New best EMA pseudo Dice: 0.7616
2024-12-19 18:27:05.823963: 
2024-12-19 18:27:05.825104: Epoch 72
2024-12-19 18:27:05.825824: Current learning rate: 0.00555
2024-12-19 18:31:08.238468: Validation loss did not improve from -0.61259. Patience: 6/50
2024-12-19 18:31:08.241162: train_loss -0.6695
2024-12-19 18:31:08.242332: val_loss -0.5687
2024-12-19 18:31:08.243183: Pseudo dice [0.7404]
2024-12-19 18:31:08.244184: Epoch time: 242.42 s
2024-12-19 18:31:10.179932: 
2024-12-19 18:31:10.181628: Epoch 73
2024-12-19 18:31:10.182805: Current learning rate: 0.00549
2024-12-19 18:35:18.069740: Validation loss did not improve from -0.61259. Patience: 7/50
2024-12-19 18:35:18.070675: train_loss -0.6786
2024-12-19 18:35:18.071629: val_loss -0.5767
2024-12-19 18:35:18.072435: Pseudo dice [0.7569]
2024-12-19 18:35:18.073275: Epoch time: 247.89 s
2024-12-19 18:35:19.598034: 
2024-12-19 18:35:19.599339: Epoch 74
2024-12-19 18:35:19.600140: Current learning rate: 0.00542
2024-12-19 18:39:39.513742: Validation loss did not improve from -0.61259. Patience: 8/50
2024-12-19 18:39:39.514636: train_loss -0.686
2024-12-19 18:39:39.515440: val_loss -0.5849
2024-12-19 18:39:39.516123: Pseudo dice [0.7564]
2024-12-19 18:39:39.516959: Epoch time: 259.92 s
2024-12-19 18:39:41.339354: 
2024-12-19 18:39:41.340558: Epoch 75
2024-12-19 18:39:41.341321: Current learning rate: 0.00536
2024-12-19 18:44:06.404325: Validation loss did not improve from -0.61259. Patience: 9/50
2024-12-19 18:44:06.405307: train_loss -0.6861
2024-12-19 18:44:06.406204: val_loss -0.5683
2024-12-19 18:44:06.407112: Pseudo dice [0.7528]
2024-12-19 18:44:06.407857: Epoch time: 265.07 s
2024-12-19 18:44:07.854706: 
2024-12-19 18:44:07.855994: Epoch 76
2024-12-19 18:44:07.856840: Current learning rate: 0.00529
2024-12-19 18:48:28.868060: Validation loss did not improve from -0.61259. Patience: 10/50
2024-12-19 18:48:28.869096: train_loss -0.6822
2024-12-19 18:48:28.870088: val_loss -0.5874
2024-12-19 18:48:28.870950: Pseudo dice [0.7674]
2024-12-19 18:48:28.871804: Epoch time: 261.02 s
2024-12-19 18:48:30.358468: 
2024-12-19 18:48:30.359939: Epoch 77
2024-12-19 18:48:30.360975: Current learning rate: 0.00523
2024-12-19 18:52:41.111352: Validation loss did not improve from -0.61259. Patience: 11/50
2024-12-19 18:52:41.112477: train_loss -0.688
2024-12-19 18:52:41.113617: val_loss -0.5606
2024-12-19 18:52:41.114651: Pseudo dice [0.7501]
2024-12-19 18:52:41.115642: Epoch time: 250.76 s
2024-12-19 18:52:42.592096: 
2024-12-19 18:52:42.593573: Epoch 78
2024-12-19 18:52:42.594709: Current learning rate: 0.00517
2024-12-19 18:56:45.817729: Validation loss did not improve from -0.61259. Patience: 12/50
2024-12-19 18:56:45.818806: train_loss -0.6907
2024-12-19 18:56:45.819676: val_loss -0.5956
2024-12-19 18:56:45.820426: Pseudo dice [0.7717]
2024-12-19 18:56:45.821314: Epoch time: 243.23 s
2024-12-19 18:56:47.299350: 
2024-12-19 18:56:47.300685: Epoch 79
2024-12-19 18:56:47.301440: Current learning rate: 0.0051
2024-12-19 19:00:36.800121: Validation loss did not improve from -0.61259. Patience: 13/50
2024-12-19 19:00:36.801166: train_loss -0.69
2024-12-19 19:00:36.801981: val_loss -0.5864
2024-12-19 19:00:36.802657: Pseudo dice [0.7601]
2024-12-19 19:00:36.803424: Epoch time: 229.5 s
2024-12-19 19:00:38.760719: 
2024-12-19 19:00:38.762110: Epoch 80
2024-12-19 19:00:38.762965: Current learning rate: 0.00504
2024-12-19 19:04:34.483060: Validation loss did not improve from -0.61259. Patience: 14/50
2024-12-19 19:04:34.483888: train_loss -0.6955
2024-12-19 19:04:34.484695: val_loss -0.5969
2024-12-19 19:04:34.485578: Pseudo dice [0.77]
2024-12-19 19:04:34.486318: Epoch time: 235.72 s
2024-12-19 19:04:35.963411: 
2024-12-19 19:04:35.964470: Epoch 81
2024-12-19 19:04:35.965337: Current learning rate: 0.00497
2024-12-19 19:08:33.419755: Validation loss did not improve from -0.61259. Patience: 15/50
2024-12-19 19:08:33.420950: train_loss -0.6921
2024-12-19 19:08:33.421765: val_loss -0.6043
2024-12-19 19:08:33.422468: Pseudo dice [0.773]
2024-12-19 19:08:33.423240: Epoch time: 237.46 s
2024-12-19 19:08:33.423885: Yayy! New best EMA pseudo Dice: 0.762
2024-12-19 19:08:35.312475: 
2024-12-19 19:08:35.313685: Epoch 82
2024-12-19 19:08:35.314426: Current learning rate: 0.00491
2024-12-19 19:12:49.164328: Validation loss did not improve from -0.61259. Patience: 16/50
2024-12-19 19:12:49.165823: train_loss -0.694
2024-12-19 19:12:49.166908: val_loss -0.576
2024-12-19 19:12:49.167929: Pseudo dice [0.7576]
2024-12-19 19:12:49.168932: Epoch time: 253.85 s
2024-12-19 19:12:50.607169: 
2024-12-19 19:12:50.608710: Epoch 83
2024-12-19 19:12:50.609801: Current learning rate: 0.00484
2024-12-19 19:17:07.699814: Validation loss did not improve from -0.61259. Patience: 17/50
2024-12-19 19:17:07.700746: train_loss -0.6943
2024-12-19 19:17:07.701574: val_loss -0.5506
2024-12-19 19:17:07.702342: Pseudo dice [0.7523]
2024-12-19 19:17:07.703017: Epoch time: 257.09 s
2024-12-19 19:17:09.042055: 
2024-12-19 19:17:09.043410: Epoch 84
2024-12-19 19:17:09.044201: Current learning rate: 0.00478
2024-12-19 19:21:19.929696: Validation loss did not improve from -0.61259. Patience: 18/50
2024-12-19 19:21:19.930728: train_loss -0.6924
2024-12-19 19:21:19.931621: val_loss -0.6105
2024-12-19 19:21:19.932363: Pseudo dice [0.7722]
2024-12-19 19:21:19.933222: Epoch time: 250.89 s
2024-12-19 19:21:21.661855: 
2024-12-19 19:21:21.663142: Epoch 85
2024-12-19 19:21:21.663913: Current learning rate: 0.00471
2024-12-19 19:25:45.441664: Validation loss did not improve from -0.61259. Patience: 19/50
2024-12-19 19:25:45.442768: train_loss -0.6892
2024-12-19 19:25:45.448755: val_loss -0.6092
2024-12-19 19:25:45.449545: Pseudo dice [0.7736]
2024-12-19 19:25:45.450395: Epoch time: 263.78 s
2024-12-19 19:25:45.451232: Yayy! New best EMA pseudo Dice: 0.7629
2024-12-19 19:25:47.262623: 
2024-12-19 19:25:47.264054: Epoch 86
2024-12-19 19:25:47.264932: Current learning rate: 0.00465
2024-12-19 19:29:54.995916: Validation loss did not improve from -0.61259. Patience: 20/50
2024-12-19 19:29:54.996920: train_loss -0.6994
2024-12-19 19:29:54.997816: val_loss -0.6019
2024-12-19 19:29:54.998538: Pseudo dice [0.7703]
2024-12-19 19:29:54.999277: Epoch time: 247.74 s
2024-12-19 19:29:54.999931: Yayy! New best EMA pseudo Dice: 0.7637
2024-12-19 19:29:56.832736: 
2024-12-19 19:29:56.833896: Epoch 87
2024-12-19 19:29:56.834711: Current learning rate: 0.00458
2024-12-19 19:33:58.579801: Validation loss did not improve from -0.61259. Patience: 21/50
2024-12-19 19:33:58.580751: train_loss -0.6941
2024-12-19 19:33:58.581605: val_loss -0.6044
2024-12-19 19:33:58.582534: Pseudo dice [0.7704]
2024-12-19 19:33:58.583564: Epoch time: 241.75 s
2024-12-19 19:33:58.584475: Yayy! New best EMA pseudo Dice: 0.7644
2024-12-19 19:34:00.437669: 
2024-12-19 19:34:00.438956: Epoch 88
2024-12-19 19:34:00.439724: Current learning rate: 0.00452
2024-12-19 19:37:56.092382: Validation loss did not improve from -0.61259. Patience: 22/50
2024-12-19 19:37:56.094236: train_loss -0.7034
2024-12-19 19:37:56.095421: val_loss -0.5973
2024-12-19 19:37:56.096217: Pseudo dice [0.7702]
2024-12-19 19:37:56.096988: Epoch time: 235.66 s
2024-12-19 19:37:56.097850: Yayy! New best EMA pseudo Dice: 0.7649
2024-12-19 19:37:57.948246: 
2024-12-19 19:37:57.949633: Epoch 89
2024-12-19 19:37:57.950434: Current learning rate: 0.00445
2024-12-19 19:42:05.757677: Validation loss did not improve from -0.61259. Patience: 23/50
2024-12-19 19:42:05.758517: train_loss -0.701
2024-12-19 19:42:05.759449: val_loss -0.5826
2024-12-19 19:42:05.760233: Pseudo dice [0.764]
2024-12-19 19:42:05.760983: Epoch time: 247.81 s
2024-12-19 19:42:07.571442: 
2024-12-19 19:42:07.572853: Epoch 90
2024-12-19 19:42:07.573633: Current learning rate: 0.00438
2024-12-19 19:47:33.524379: Validation loss did not improve from -0.61259. Patience: 24/50
2024-12-19 19:47:33.525352: train_loss -0.7057
2024-12-19 19:47:33.526225: val_loss -0.5998
2024-12-19 19:47:33.527041: Pseudo dice [0.7685]
2024-12-19 19:47:33.527793: Epoch time: 325.96 s
2024-12-19 19:47:33.528452: Yayy! New best EMA pseudo Dice: 0.7652
2024-12-19 19:47:35.451180: 
2024-12-19 19:47:35.452388: Epoch 91
2024-12-19 19:47:35.453269: Current learning rate: 0.00432
2024-12-19 19:53:13.513463: Validation loss did not improve from -0.61259. Patience: 25/50
2024-12-19 19:53:13.514594: train_loss -0.7085
2024-12-19 19:53:13.515801: val_loss -0.6067
2024-12-19 19:53:13.516782: Pseudo dice [0.7751]
2024-12-19 19:53:13.517778: Epoch time: 338.06 s
2024-12-19 19:53:13.518658: Yayy! New best EMA pseudo Dice: 0.7662
2024-12-19 19:53:15.380599: 
2024-12-19 19:53:15.381922: Epoch 92
2024-12-19 19:53:15.382848: Current learning rate: 0.00425
2024-12-19 19:59:05.618772: Validation loss did not improve from -0.61259. Patience: 26/50
2024-12-19 19:59:05.621900: train_loss -0.7132
2024-12-19 19:59:05.623191: val_loss -0.6063
2024-12-19 19:59:05.624166: Pseudo dice [0.7709]
2024-12-19 19:59:05.625380: Epoch time: 350.24 s
2024-12-19 19:59:05.626740: Yayy! New best EMA pseudo Dice: 0.7667
2024-12-19 19:59:07.470122: 
2024-12-19 19:59:07.471609: Epoch 93
2024-12-19 19:59:07.472583: Current learning rate: 0.00419
2024-12-19 20:05:03.517996: Validation loss did not improve from -0.61259. Patience: 27/50
2024-12-19 20:05:03.519181: train_loss -0.7115
2024-12-19 20:05:03.520298: val_loss -0.524
2024-12-19 20:05:03.521233: Pseudo dice [0.729]
2024-12-19 20:05:03.522192: Epoch time: 356.05 s
2024-12-19 20:05:04.930316: 
2024-12-19 20:05:04.931837: Epoch 94
2024-12-19 20:05:04.932896: Current learning rate: 0.00412
2024-12-19 20:11:09.166859: Validation loss improved from -0.61259 to -0.62147! Patience: 27/50
2024-12-19 20:11:09.167666: train_loss -0.7043
2024-12-19 20:11:09.168476: val_loss -0.6215
2024-12-19 20:11:09.169305: Pseudo dice [0.7819]
2024-12-19 20:11:09.170075: Epoch time: 364.24 s
2024-12-19 20:11:11.532119: 
2024-12-19 20:11:11.533530: Epoch 95
2024-12-19 20:11:11.534455: Current learning rate: 0.00405
2024-12-19 20:16:58.367824: Validation loss did not improve from -0.62147. Patience: 1/50
2024-12-19 20:16:58.368716: train_loss -0.7099
2024-12-19 20:16:58.370809: val_loss -0.591
2024-12-19 20:16:58.371610: Pseudo dice [0.7645]
2024-12-19 20:16:58.372394: Epoch time: 346.84 s
2024-12-19 20:16:59.750874: 
2024-12-19 20:16:59.752247: Epoch 96
2024-12-19 20:16:59.753229: Current learning rate: 0.00399
2024-12-19 20:22:45.422821: Validation loss did not improve from -0.62147. Patience: 2/50
2024-12-19 20:22:45.423932: train_loss -0.7145
2024-12-19 20:22:45.424747: val_loss -0.6096
2024-12-19 20:22:45.425599: Pseudo dice [0.7732]
2024-12-19 20:22:45.426400: Epoch time: 345.67 s
2024-12-19 20:22:46.920902: 
2024-12-19 20:22:46.922036: Epoch 97
2024-12-19 20:22:46.922832: Current learning rate: 0.00392
2024-12-19 20:28:24.902241: Validation loss did not improve from -0.62147. Patience: 3/50
2024-12-19 20:28:24.903116: train_loss -0.7123
2024-12-19 20:28:24.904204: val_loss -0.612
2024-12-19 20:28:24.905001: Pseudo dice [0.7762]
2024-12-19 20:28:24.905768: Epoch time: 337.98 s
2024-12-19 20:28:24.906446: Yayy! New best EMA pseudo Dice: 0.7667
2024-12-19 20:28:26.719400: 
2024-12-19 20:28:26.720782: Epoch 98
2024-12-19 20:28:26.721615: Current learning rate: 0.00385
2024-12-19 20:34:24.829878: Validation loss did not improve from -0.62147. Patience: 4/50
2024-12-19 20:34:24.830992: train_loss -0.7185
2024-12-19 20:34:24.831863: val_loss -0.5935
2024-12-19 20:34:24.832597: Pseudo dice [0.7692]
2024-12-19 20:34:24.833277: Epoch time: 358.11 s
2024-12-19 20:34:24.833938: Yayy! New best EMA pseudo Dice: 0.7669
2024-12-19 20:34:26.624627: 
2024-12-19 20:34:26.625602: Epoch 99
2024-12-19 20:34:26.626312: Current learning rate: 0.00379
2024-12-19 20:40:24.486142: Validation loss did not improve from -0.62147. Patience: 5/50
2024-12-19 20:40:24.487136: train_loss -0.7173
2024-12-19 20:40:24.487944: val_loss -0.5964
2024-12-19 20:40:24.488724: Pseudo dice [0.7543]
2024-12-19 20:40:24.489494: Epoch time: 357.86 s
2024-12-19 20:40:26.343167: 
2024-12-19 20:40:26.344683: Epoch 100
2024-12-19 20:40:26.345654: Current learning rate: 0.00372
2024-12-19 20:46:25.518920: Validation loss did not improve from -0.62147. Patience: 6/50
2024-12-19 20:46:25.519901: train_loss -0.7175
2024-12-19 20:46:25.520799: val_loss -0.6079
2024-12-19 20:46:25.521447: Pseudo dice [0.7743]
2024-12-19 20:46:25.522362: Epoch time: 359.18 s
2024-12-19 20:46:26.948651: 
2024-12-19 20:46:26.950039: Epoch 101
2024-12-19 20:46:26.950783: Current learning rate: 0.00365
2024-12-19 20:52:10.855732: Validation loss did not improve from -0.62147. Patience: 7/50
2024-12-19 20:52:10.856623: train_loss -0.7263
2024-12-19 20:52:10.857731: val_loss -0.5926
2024-12-19 20:52:10.858786: Pseudo dice [0.7643]
2024-12-19 20:52:10.860008: Epoch time: 343.91 s
2024-12-19 20:52:12.246099: 
2024-12-19 20:52:12.247551: Epoch 102
2024-12-19 20:52:12.248592: Current learning rate: 0.00359
2024-12-19 20:57:45.404682: Validation loss did not improve from -0.62147. Patience: 8/50
2024-12-19 20:57:45.406348: train_loss -0.717
2024-12-19 20:57:45.407420: val_loss -0.5999
2024-12-19 20:57:45.408209: Pseudo dice [0.7745]
2024-12-19 20:57:45.409023: Epoch time: 333.16 s
2024-12-19 20:57:45.409817: Yayy! New best EMA pseudo Dice: 0.7671
2024-12-19 20:57:47.298743: 
2024-12-19 20:57:47.300109: Epoch 103
2024-12-19 20:57:47.300987: Current learning rate: 0.00352
2024-12-19 21:03:29.596947: Validation loss did not improve from -0.62147. Patience: 9/50
2024-12-19 21:03:29.600497: train_loss -0.7191
2024-12-19 21:03:29.601425: val_loss -0.6027
2024-12-19 21:03:29.602064: Pseudo dice [0.7745]
2024-12-19 21:03:29.603072: Epoch time: 342.3 s
2024-12-19 21:03:29.603980: Yayy! New best EMA pseudo Dice: 0.7679
2024-12-19 21:03:31.364604: 
2024-12-19 21:03:31.365596: Epoch 104
2024-12-19 21:03:31.366371: Current learning rate: 0.00345
2024-12-19 21:09:28.497711: Validation loss did not improve from -0.62147. Patience: 10/50
2024-12-19 21:09:28.499044: train_loss -0.7231
2024-12-19 21:09:28.500620: val_loss -0.5866
2024-12-19 21:09:28.501476: Pseudo dice [0.7649]
2024-12-19 21:09:28.502476: Epoch time: 357.14 s
2024-12-19 21:09:30.827519: 
2024-12-19 21:09:30.828805: Epoch 105
2024-12-19 21:09:30.829632: Current learning rate: 0.00338
2024-12-19 21:15:25.096538: Validation loss did not improve from -0.62147. Patience: 11/50
2024-12-19 21:15:25.097543: train_loss -0.7248
2024-12-19 21:15:25.098429: val_loss -0.5905
2024-12-19 21:15:25.099216: Pseudo dice [0.7719]
2024-12-19 21:15:25.100018: Epoch time: 354.27 s
2024-12-19 21:15:25.100787: Yayy! New best EMA pseudo Dice: 0.768
2024-12-19 21:15:26.895405: 
2024-12-19 21:15:26.896780: Epoch 106
2024-12-19 21:15:26.897479: Current learning rate: 0.00332
2024-12-19 21:21:37.108809: Validation loss did not improve from -0.62147. Patience: 12/50
2024-12-19 21:21:37.109824: train_loss -0.7233
2024-12-19 21:21:37.110569: val_loss -0.5834
2024-12-19 21:21:37.111309: Pseudo dice [0.7661]
2024-12-19 21:21:37.112093: Epoch time: 370.22 s
2024-12-19 21:21:38.462461: 
2024-12-19 21:21:38.463564: Epoch 107
2024-12-19 21:21:38.464392: Current learning rate: 0.00325
2024-12-19 21:27:30.378086: Validation loss did not improve from -0.62147. Patience: 13/50
2024-12-19 21:27:30.379094: train_loss -0.7283
2024-12-19 21:27:30.380070: val_loss -0.5962
2024-12-19 21:27:30.380890: Pseudo dice [0.7655]
2024-12-19 21:27:30.381662: Epoch time: 351.92 s
2024-12-19 21:27:31.766956: 
2024-12-19 21:27:31.768238: Epoch 108
2024-12-19 21:27:31.769136: Current learning rate: 0.00318
2024-12-19 21:33:22.281923: Validation loss did not improve from -0.62147. Patience: 14/50
2024-12-19 21:33:22.282901: train_loss -0.7308
2024-12-19 21:33:22.284088: val_loss -0.5997
2024-12-19 21:33:22.285098: Pseudo dice [0.7734]
2024-12-19 21:33:22.285994: Epoch time: 350.52 s
2024-12-19 21:33:22.286905: Yayy! New best EMA pseudo Dice: 0.7682
2024-12-19 21:33:24.159478: 
2024-12-19 21:33:24.160949: Epoch 109
2024-12-19 21:33:24.161758: Current learning rate: 0.00311
2024-12-19 21:40:14.000835: Validation loss improved from -0.62147 to -0.63000! Patience: 14/50
2024-12-19 21:40:14.001872: train_loss -0.7306
2024-12-19 21:40:14.002819: val_loss -0.63
2024-12-19 21:40:14.003583: Pseudo dice [0.7841]
2024-12-19 21:40:14.004306: Epoch time: 409.84 s
2024-12-19 21:40:14.400296: Yayy! New best EMA pseudo Dice: 0.7698
2024-12-19 21:40:16.249574: 
2024-12-19 21:40:16.250931: Epoch 110
2024-12-19 21:40:16.251791: Current learning rate: 0.00304
2024-12-19 21:47:04.573802: Validation loss did not improve from -0.63000. Patience: 1/50
2024-12-19 21:47:04.574794: train_loss -0.7332
2024-12-19 21:47:04.575725: val_loss -0.5983
2024-12-19 21:47:04.576394: Pseudo dice [0.7639]
2024-12-19 21:47:04.577155: Epoch time: 408.33 s
2024-12-19 21:47:06.021832: 
2024-12-19 21:47:06.022872: Epoch 111
2024-12-19 21:47:06.023849: Current learning rate: 0.00297
2024-12-19 21:53:52.887154: Validation loss did not improve from -0.63000. Patience: 2/50
2024-12-19 21:53:52.888172: train_loss -0.7337
2024-12-19 21:53:52.889198: val_loss -0.617
2024-12-19 21:53:52.889981: Pseudo dice [0.7816]
2024-12-19 21:53:52.890910: Epoch time: 406.87 s
2024-12-19 21:53:52.891721: Yayy! New best EMA pseudo Dice: 0.7704
2024-12-19 21:53:54.741312: 
2024-12-19 21:53:54.742533: Epoch 112
2024-12-19 21:53:54.743362: Current learning rate: 0.00291
2024-12-19 22:00:27.067275: Validation loss did not improve from -0.63000. Patience: 3/50
2024-12-19 22:00:27.068088: train_loss -0.7297
2024-12-19 22:00:27.069109: val_loss -0.573
2024-12-19 22:00:27.069946: Pseudo dice [0.7536]
2024-12-19 22:00:27.070766: Epoch time: 392.33 s
2024-12-19 22:00:28.551477: 
2024-12-19 22:00:28.553104: Epoch 113
2024-12-19 22:00:28.554059: Current learning rate: 0.00284
2024-12-19 22:07:28.047797: Validation loss did not improve from -0.63000. Patience: 4/50
2024-12-19 22:07:28.051864: train_loss -0.733
2024-12-19 22:07:28.053240: val_loss -0.6254
2024-12-19 22:07:28.053865: Pseudo dice [0.7824]
2024-12-19 22:07:28.054772: Epoch time: 419.5 s
2024-12-19 22:07:29.527111: 
2024-12-19 22:07:29.528485: Epoch 114
2024-12-19 22:07:29.529412: Current learning rate: 0.00277
2024-12-19 22:14:10.232698: Validation loss did not improve from -0.63000. Patience: 5/50
2024-12-19 22:14:10.233732: train_loss -0.7329
2024-12-19 22:14:10.235458: val_loss -0.6167
2024-12-19 22:14:10.236357: Pseudo dice [0.7791]
2024-12-19 22:14:10.237333: Epoch time: 400.71 s
2024-12-19 22:14:10.649656: Yayy! New best EMA pseudo Dice: 0.771
2024-12-19 22:14:12.518866: 
2024-12-19 22:14:12.520097: Epoch 115
2024-12-19 22:14:12.520850: Current learning rate: 0.0027
2024-12-19 22:20:52.517433: Validation loss did not improve from -0.63000. Patience: 6/50
2024-12-19 22:20:52.518390: train_loss -0.7344
2024-12-19 22:20:52.519563: val_loss -0.622
2024-12-19 22:20:52.520447: Pseudo dice [0.784]
2024-12-19 22:20:52.521265: Epoch time: 400.0 s
2024-12-19 22:20:52.521995: Yayy! New best EMA pseudo Dice: 0.7723
2024-12-19 22:20:55.488575: 
2024-12-19 22:20:55.489964: Epoch 116
2024-12-19 22:20:55.490938: Current learning rate: 0.00263
2024-12-19 22:27:18.841112: Validation loss did not improve from -0.63000. Patience: 7/50
2024-12-19 22:27:18.842238: train_loss -0.7399
2024-12-19 22:27:18.843231: val_loss -0.6126
2024-12-19 22:27:18.844221: Pseudo dice [0.7822]
2024-12-19 22:27:18.845069: Epoch time: 383.36 s
2024-12-19 22:27:18.845985: Yayy! New best EMA pseudo Dice: 0.7733
2024-12-19 22:27:20.757167: 
2024-12-19 22:27:20.758586: Epoch 117
2024-12-19 22:27:20.759826: Current learning rate: 0.00256
2024-12-19 22:33:56.782794: Validation loss did not improve from -0.63000. Patience: 8/50
2024-12-19 22:33:56.783844: train_loss -0.7404
2024-12-19 22:33:56.784883: val_loss -0.6022
2024-12-19 22:33:56.785842: Pseudo dice [0.7814]
2024-12-19 22:33:56.786900: Epoch time: 396.03 s
2024-12-19 22:33:56.787952: Yayy! New best EMA pseudo Dice: 0.7741
2024-12-19 22:33:58.757910: 
2024-12-19 22:33:58.759516: Epoch 118
2024-12-19 22:33:58.760467: Current learning rate: 0.00249
2024-12-19 22:40:52.280825: Validation loss did not improve from -0.63000. Patience: 9/50
2024-12-19 22:40:52.281847: train_loss -0.7342
2024-12-19 22:40:52.282622: val_loss -0.6141
2024-12-19 22:40:52.283308: Pseudo dice [0.7781]
2024-12-19 22:40:52.284115: Epoch time: 413.53 s
2024-12-19 22:40:52.284802: Yayy! New best EMA pseudo Dice: 0.7745
2024-12-19 22:40:54.131629: 
2024-12-19 22:40:54.132698: Epoch 119
2024-12-19 22:40:54.133465: Current learning rate: 0.00242
2024-12-19 22:47:48.638593: Validation loss did not improve from -0.63000. Patience: 10/50
2024-12-19 22:47:48.639579: train_loss -0.7379
2024-12-19 22:47:48.640432: val_loss -0.6082
2024-12-19 22:47:48.641232: Pseudo dice [0.7727]
2024-12-19 22:47:48.641980: Epoch time: 414.51 s
2024-12-19 22:47:50.523316: 
2024-12-19 22:47:50.524513: Epoch 120
2024-12-19 22:47:50.525426: Current learning rate: 0.00235
2024-12-19 22:54:38.768118: Validation loss did not improve from -0.63000. Patience: 11/50
2024-12-19 22:54:38.769233: train_loss -0.7413
2024-12-19 22:54:38.770139: val_loss -0.6161
2024-12-19 22:54:38.771031: Pseudo dice [0.7781]
2024-12-19 22:54:38.771794: Epoch time: 408.25 s
2024-12-19 22:54:38.772702: Yayy! New best EMA pseudo Dice: 0.7747
2024-12-19 22:54:40.726107: 
2024-12-19 22:54:40.727582: Epoch 121
2024-12-19 22:54:40.728324: Current learning rate: 0.00228
2024-12-19 23:01:22.874110: Validation loss did not improve from -0.63000. Patience: 12/50
2024-12-19 23:01:22.875031: train_loss -0.7465
2024-12-19 23:01:22.875842: val_loss -0.6009
2024-12-19 23:01:22.876620: Pseudo dice [0.7786]
2024-12-19 23:01:22.877247: Epoch time: 402.15 s
2024-12-19 23:01:22.877908: Yayy! New best EMA pseudo Dice: 0.7751
2024-12-19 23:01:24.745187: 
2024-12-19 23:01:24.746327: Epoch 122
2024-12-19 23:01:24.747041: Current learning rate: 0.00221
2024-12-19 23:08:17.909706: Validation loss did not improve from -0.63000. Patience: 13/50
2024-12-19 23:08:17.911568: train_loss -0.746
2024-12-19 23:08:17.912604: val_loss -0.5979
2024-12-19 23:08:17.913356: Pseudo dice [0.7692]
2024-12-19 23:08:17.914094: Epoch time: 413.17 s
2024-12-19 23:08:19.376724: 
2024-12-19 23:08:19.377996: Epoch 123
2024-12-19 23:08:19.378795: Current learning rate: 0.00214
2024-12-19 23:14:38.522270: Validation loss did not improve from -0.63000. Patience: 14/50
2024-12-19 23:14:38.523468: train_loss -0.7441
2024-12-19 23:14:38.524265: val_loss -0.6292
2024-12-19 23:14:38.524972: Pseudo dice [0.7896]
2024-12-19 23:14:38.525776: Epoch time: 379.15 s
2024-12-19 23:14:38.526453: Yayy! New best EMA pseudo Dice: 0.776
2024-12-19 23:14:40.430334: 
2024-12-19 23:14:40.431565: Epoch 124
2024-12-19 23:14:40.432412: Current learning rate: 0.00207
2024-12-19 23:21:13.759620: Validation loss did not improve from -0.63000. Patience: 15/50
2024-12-19 23:21:13.760590: train_loss -0.7431
2024-12-19 23:21:13.761510: val_loss -0.62
2024-12-19 23:21:13.762239: Pseudo dice [0.7849]
2024-12-19 23:21:13.763022: Epoch time: 393.33 s
2024-12-19 23:21:14.140266: Yayy! New best EMA pseudo Dice: 0.7769
2024-12-19 23:21:16.098239: 
2024-12-19 23:21:16.099725: Epoch 125
2024-12-19 23:21:16.100624: Current learning rate: 0.00199
2024-12-19 23:27:31.942393: Validation loss did not improve from -0.63000. Patience: 16/50
2024-12-19 23:27:31.943432: train_loss -0.7474
2024-12-19 23:27:31.944286: val_loss -0.6201
2024-12-19 23:27:31.945141: Pseudo dice [0.7799]
2024-12-19 23:27:31.945910: Epoch time: 375.85 s
2024-12-19 23:27:31.946698: Yayy! New best EMA pseudo Dice: 0.7772
2024-12-19 23:27:34.894675: 
2024-12-19 23:27:34.896245: Epoch 126
2024-12-19 23:27:34.897319: Current learning rate: 0.00192
2024-12-19 23:33:40.154209: Validation loss did not improve from -0.63000. Patience: 17/50
2024-12-19 23:33:40.155558: train_loss -0.744
2024-12-19 23:33:40.156392: val_loss -0.621
2024-12-19 23:33:40.157179: Pseudo dice [0.7803]
2024-12-19 23:33:40.158022: Epoch time: 365.26 s
2024-12-19 23:33:40.158772: Yayy! New best EMA pseudo Dice: 0.7775
2024-12-19 23:33:42.047850: 
2024-12-19 23:33:42.049137: Epoch 127
2024-12-19 23:33:42.049943: Current learning rate: 0.00185
2024-12-19 23:39:59.378737: Validation loss did not improve from -0.63000. Patience: 18/50
2024-12-19 23:39:59.379703: train_loss -0.7473
2024-12-19 23:39:59.380457: val_loss -0.6063
2024-12-19 23:39:59.381193: Pseudo dice [0.7812]
2024-12-19 23:39:59.381861: Epoch time: 377.33 s
2024-12-19 23:39:59.382577: Yayy! New best EMA pseudo Dice: 0.7779
2024-12-19 23:40:01.257326: 
2024-12-19 23:40:01.259241: Epoch 128
2024-12-19 23:40:01.260018: Current learning rate: 0.00178
2024-12-19 23:46:04.923533: Validation loss did not improve from -0.63000. Patience: 19/50
2024-12-19 23:46:04.925080: train_loss -0.7486
2024-12-19 23:46:04.926382: val_loss -0.5904
2024-12-19 23:46:04.927222: Pseudo dice [0.7653]
2024-12-19 23:46:04.928159: Epoch time: 363.67 s
2024-12-19 23:46:06.346525: 
2024-12-19 23:46:06.348077: Epoch 129
2024-12-19 23:46:06.348948: Current learning rate: 0.0017
2024-12-19 23:51:50.196412: Validation loss did not improve from -0.63000. Patience: 20/50
2024-12-19 23:51:50.197422: train_loss -0.7484
2024-12-19 23:51:50.198402: val_loss -0.6159
2024-12-19 23:51:50.199308: Pseudo dice [0.785]
2024-12-19 23:51:50.200194: Epoch time: 343.85 s
2024-12-19 23:51:52.027851: 
2024-12-19 23:51:52.028963: Epoch 130
2024-12-19 23:51:52.029941: Current learning rate: 0.00163
2024-12-19 23:57:14.587613: Validation loss did not improve from -0.63000. Patience: 21/50
2024-12-19 23:57:14.588558: train_loss -0.7493
2024-12-19 23:57:14.589504: val_loss -0.6111
2024-12-19 23:57:14.590231: Pseudo dice [0.7749]
2024-12-19 23:57:14.590936: Epoch time: 322.56 s
2024-12-19 23:57:16.471526: 
2024-12-19 23:57:16.472899: Epoch 131
2024-12-19 23:57:16.473656: Current learning rate: 0.00156
2024-12-20 00:03:56.104777: Validation loss did not improve from -0.63000. Patience: 22/50
2024-12-20 00:03:56.105831: train_loss -0.7519
2024-12-20 00:03:56.106714: val_loss -0.611
2024-12-20 00:03:56.107419: Pseudo dice [0.7784]
2024-12-20 00:03:56.108033: Epoch time: 399.64 s
2024-12-20 00:03:57.596876: 
2024-12-20 00:03:57.598196: Epoch 132
2024-12-20 00:03:57.598927: Current learning rate: 0.00148
2024-12-20 00:11:06.661648: Validation loss did not improve from -0.63000. Patience: 23/50
2024-12-20 00:11:06.662527: train_loss -0.752
2024-12-20 00:11:06.663439: val_loss -0.61
2024-12-20 00:11:06.664181: Pseudo dice [0.7776]
2024-12-20 00:11:06.664859: Epoch time: 429.07 s
2024-12-20 00:11:08.092028: 
2024-12-20 00:11:08.093354: Epoch 133
2024-12-20 00:11:08.094158: Current learning rate: 0.00141
2024-12-20 00:18:15.827533: Validation loss did not improve from -0.63000. Patience: 24/50
2024-12-20 00:18:15.831870: train_loss -0.7513
2024-12-20 00:18:15.832826: val_loss -0.5769
2024-12-20 00:18:15.833614: Pseudo dice [0.7593]
2024-12-20 00:18:15.834630: Epoch time: 427.74 s
2024-12-20 00:18:17.227023: 
2024-12-20 00:18:17.228603: Epoch 134
2024-12-20 00:18:17.229764: Current learning rate: 0.00133
2024-12-20 00:25:25.042646: Validation loss did not improve from -0.63000. Patience: 25/50
2024-12-20 00:25:25.043708: train_loss -0.752
2024-12-20 00:25:25.044426: val_loss -0.6039
2024-12-20 00:25:25.045110: Pseudo dice [0.7723]
2024-12-20 00:25:25.045889: Epoch time: 427.82 s
2024-12-20 00:25:26.882469: 
2024-12-20 00:25:26.883723: Epoch 135
2024-12-20 00:25:26.884501: Current learning rate: 0.00126
2024-12-20 00:32:47.065319: Validation loss did not improve from -0.63000. Patience: 26/50
2024-12-20 00:32:47.066196: train_loss -0.755
2024-12-20 00:32:47.067160: val_loss -0.6067
2024-12-20 00:32:47.068017: Pseudo dice [0.7845]
2024-12-20 00:32:47.068955: Epoch time: 440.19 s
2024-12-20 00:32:48.497370: 
2024-12-20 00:32:48.498785: Epoch 136
2024-12-20 00:32:48.499736: Current learning rate: 0.00118
2024-12-20 00:39:49.509620: Validation loss did not improve from -0.63000. Patience: 27/50
2024-12-20 00:39:49.510911: train_loss -0.7566
2024-12-20 00:39:49.512084: val_loss -0.5802
2024-12-20 00:39:49.513006: Pseudo dice [0.761]
2024-12-20 00:39:49.513874: Epoch time: 421.01 s
2024-12-20 00:39:52.105699: 
2024-12-20 00:39:52.107299: Epoch 137
2024-12-20 00:39:52.108438: Current learning rate: 0.00111
2024-12-20 00:46:52.583570: Validation loss did not improve from -0.63000. Patience: 28/50
2024-12-20 00:46:52.585337: train_loss -0.7575
2024-12-20 00:46:52.586366: val_loss -0.6092
2024-12-20 00:46:52.587219: Pseudo dice [0.77]
2024-12-20 00:46:52.587999: Epoch time: 420.48 s
2024-12-20 00:46:54.115075: 
2024-12-20 00:46:54.116811: Epoch 138
2024-12-20 00:46:54.117596: Current learning rate: 0.00103
2024-12-20 00:54:09.246127: Validation loss improved from -0.63000 to -0.63387! Patience: 28/50
2024-12-20 00:54:09.247121: train_loss -0.7574
2024-12-20 00:54:09.248086: val_loss -0.6339
2024-12-20 00:54:09.248868: Pseudo dice [0.7903]
2024-12-20 00:54:09.249728: Epoch time: 435.13 s
2024-12-20 00:54:10.748296: 
2024-12-20 00:54:10.749495: Epoch 139
2024-12-20 00:54:10.750384: Current learning rate: 0.00095
2024-12-20 01:01:24.254864: Validation loss did not improve from -0.63387. Patience: 1/50
2024-12-20 01:01:24.255930: train_loss -0.7569
2024-12-20 01:01:24.256975: val_loss -0.6161
2024-12-20 01:01:24.258177: Pseudo dice [0.7888]
2024-12-20 01:01:24.259473: Epoch time: 433.51 s
2024-12-20 01:01:26.174242: 
2024-12-20 01:01:26.175602: Epoch 140
2024-12-20 01:01:26.176484: Current learning rate: 0.00087
2024-12-20 01:08:40.032573: Validation loss did not improve from -0.63387. Patience: 2/50
2024-12-20 01:08:40.033570: train_loss -0.7569
2024-12-20 01:08:40.034357: val_loss -0.6206
2024-12-20 01:08:40.035128: Pseudo dice [0.7841]
2024-12-20 01:08:40.035890: Epoch time: 433.86 s
2024-12-20 01:08:41.502506: 
2024-12-20 01:08:41.503424: Epoch 141
2024-12-20 01:08:41.504233: Current learning rate: 0.00079
2024-12-20 01:15:58.982825: Validation loss did not improve from -0.63387. Patience: 3/50
2024-12-20 01:15:58.983849: train_loss -0.7603
2024-12-20 01:15:58.984717: val_loss -0.6036
2024-12-20 01:15:58.985336: Pseudo dice [0.7747]
2024-12-20 01:15:58.985982: Epoch time: 437.48 s
2024-12-20 01:16:00.471972: 
2024-12-20 01:16:00.473261: Epoch 142
2024-12-20 01:16:00.474001: Current learning rate: 0.00071
2024-12-20 01:23:49.743178: Validation loss did not improve from -0.63387. Patience: 4/50
2024-12-20 01:23:49.745224: train_loss -0.7601
2024-12-20 01:23:49.746303: val_loss -0.6223
2024-12-20 01:23:49.747125: Pseudo dice [0.7852]
2024-12-20 01:23:49.747982: Epoch time: 469.27 s
2024-12-20 01:23:49.748670: Yayy! New best EMA pseudo Dice: 0.7783
2024-12-20 01:23:51.582643: 
2024-12-20 01:23:51.584104: Epoch 143
2024-12-20 01:23:51.584956: Current learning rate: 0.00063
2024-12-20 01:31:09.686879: Validation loss did not improve from -0.63387. Patience: 5/50
2024-12-20 01:31:09.688154: train_loss -0.7618
2024-12-20 01:31:09.690230: val_loss -0.6277
2024-12-20 01:31:09.691092: Pseudo dice [0.7906]
2024-12-20 01:31:09.692211: Epoch time: 438.11 s
2024-12-20 01:31:09.692886: Yayy! New best EMA pseudo Dice: 0.7795
2024-12-20 01:31:11.582989: 
2024-12-20 01:31:11.584468: Epoch 144
2024-12-20 01:31:11.585311: Current learning rate: 0.00055
2024-12-20 01:38:41.952736: Validation loss did not improve from -0.63387. Patience: 6/50
2024-12-20 01:38:41.953550: train_loss -0.7607
2024-12-20 01:38:41.954289: val_loss -0.6004
2024-12-20 01:38:41.954971: Pseudo dice [0.7758]
2024-12-20 01:38:41.955667: Epoch time: 450.37 s
2024-12-20 01:38:43.783523: 
2024-12-20 01:38:43.784898: Epoch 145
2024-12-20 01:38:43.785611: Current learning rate: 0.00047
2024-12-20 01:45:56.659585: Validation loss did not improve from -0.63387. Patience: 7/50
2024-12-20 01:45:56.660797: train_loss -0.7631
2024-12-20 01:45:56.661884: val_loss -0.6054
2024-12-20 01:45:56.662794: Pseudo dice [0.776]
2024-12-20 01:45:56.663731: Epoch time: 432.88 s
2024-12-20 01:45:58.113502: 
2024-12-20 01:45:58.115145: Epoch 146
2024-12-20 01:45:58.116010: Current learning rate: 0.00038
2024-12-20 01:53:01.122839: Validation loss did not improve from -0.63387. Patience: 8/50
2024-12-20 01:53:01.124196: train_loss -0.7611
2024-12-20 01:53:01.125335: val_loss -0.5992
2024-12-20 01:53:01.126184: Pseudo dice [0.7765]
2024-12-20 01:53:01.127123: Epoch time: 423.01 s
2024-12-20 01:53:02.514390: 
2024-12-20 01:53:02.515611: Epoch 147
2024-12-20 01:53:02.516500: Current learning rate: 0.0003
2024-12-20 02:00:16.627275: Validation loss did not improve from -0.63387. Patience: 9/50
2024-12-20 02:00:16.628281: train_loss -0.7636
2024-12-20 02:00:16.629113: val_loss -0.6122
2024-12-20 02:00:16.629820: Pseudo dice [0.7775]
2024-12-20 02:00:16.630546: Epoch time: 434.12 s
2024-12-20 02:00:18.473054: 
2024-12-20 02:00:18.474344: Epoch 148
2024-12-20 02:00:18.475181: Current learning rate: 0.00021
2024-12-20 02:07:07.849590: Validation loss did not improve from -0.63387. Patience: 10/50
2024-12-20 02:07:07.850570: train_loss -0.7625
2024-12-20 02:07:07.851408: val_loss -0.6048
2024-12-20 02:07:07.852436: Pseudo dice [0.7729]
2024-12-20 02:07:07.853317: Epoch time: 409.38 s
2024-12-20 02:07:09.329649: 
2024-12-20 02:07:09.331163: Epoch 149
2024-12-20 02:07:09.332022: Current learning rate: 0.00011
2024-12-20 02:14:22.367884: Validation loss did not improve from -0.63387. Patience: 11/50
2024-12-20 02:14:22.368979: train_loss -0.7621
2024-12-20 02:14:22.369860: val_loss -0.6264
2024-12-20 02:14:22.370664: Pseudo dice [0.7865]
2024-12-20 02:14:22.371402: Epoch time: 433.04 s
2024-12-20 02:14:24.287565: Training done.
2024-12-20 02:14:24.406406: Using splits from existing split file: /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset308_Sohee_Calcium_OCT_CrossValidation/splits_final.json
2024-12-20 02:14:24.408408: The split file contains 5 splits.
2024-12-20 02:14:24.409212: Desired fold for training: 2
2024-12-20 02:14:24.409964: This split has 6 training and 2 validation cases.
2024-12-20 02:14:24.410941: predicting 101-044
2024-12-20 02:14:24.431153: 101-044, shape torch.Size([1, 404, 498, 498]), rank 0
2024-12-20 02:17:19.992914: predicting 704-003
2024-12-20 02:17:20.007524: 704-003, shape torch.Size([1, 375, 498, 498]), rank 0
2024-12-20 02:19:40.285015: Validation complete
2024-12-20 02:19:40.286349: Mean Validation Dice:  0.7545992635623731

############################
INFO: You are using the old nnU-Net default plans. We have updated our recommendations. Please consider using those instead! Read more here: https://github.com/MIC-DKFZ/nnUNet/blob/master/documentation/resenc_presets.md
############################

Using device: cuda:0

#######################################################################
Please cite the following paper when using nnU-Net:
Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.
#######################################################################

2024-12-20 02:19:47.679166: Using torch.compile...
/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
################### Loading pretrained weights from file  /home/gridsan/nchutisilp/projects/OpenAI-CLIP/clip_pretrained_nnUNet.pt ###################
Below is the list of overlapping blocks in pretrained model and nnUNet architecture:
encoder.stages.0.0.convs.0.conv.weight shape torch.Size([32, 1, 3, 3, 3])
encoder.stages.0.0.convs.0.conv.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.norm.weight shape torch.Size([32])
encoder.stages.0.0.convs.0.norm.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.0.weight shape torch.Size([32, 1, 3, 3, 3])
encoder.stages.0.0.convs.0.all_modules.0.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.1.weight shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.1.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
encoder.stages.0.0.convs.1.conv.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.norm.weight shape torch.Size([32])
encoder.stages.0.0.convs.1.norm.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
encoder.stages.0.0.convs.1.all_modules.0.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.1.weight shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.1.bias shape torch.Size([32])
encoder.stages.1.0.convs.0.conv.weight shape torch.Size([64, 32, 3, 3, 3])
encoder.stages.1.0.convs.0.conv.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.norm.weight shape torch.Size([64])
encoder.stages.1.0.convs.0.norm.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.0.weight shape torch.Size([64, 32, 3, 3, 3])
encoder.stages.1.0.convs.0.all_modules.0.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.1.weight shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.1.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
encoder.stages.1.0.convs.1.conv.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.norm.weight shape torch.Size([64])
encoder.stages.1.0.convs.1.norm.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
encoder.stages.1.0.convs.1.all_modules.0.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.1.weight shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.1.bias shape torch.Size([64])
encoder.stages.2.0.convs.0.conv.weight shape torch.Size([128, 64, 3, 3, 3])
encoder.stages.2.0.convs.0.conv.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.norm.weight shape torch.Size([128])
encoder.stages.2.0.convs.0.norm.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.0.weight shape torch.Size([128, 64, 3, 3, 3])
encoder.stages.2.0.convs.0.all_modules.0.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.1.weight shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.1.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
encoder.stages.2.0.convs.1.conv.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.norm.weight shape torch.Size([128])
encoder.stages.2.0.convs.1.norm.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
encoder.stages.2.0.convs.1.all_modules.0.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.1.weight shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.1.bias shape torch.Size([128])
encoder.stages.3.0.convs.0.conv.weight shape torch.Size([256, 128, 3, 3, 3])
encoder.stages.3.0.convs.0.conv.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.norm.weight shape torch.Size([256])
encoder.stages.3.0.convs.0.norm.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.0.weight shape torch.Size([256, 128, 3, 3, 3])
encoder.stages.3.0.convs.0.all_modules.0.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.1.weight shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.1.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
encoder.stages.3.0.convs.1.conv.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.norm.weight shape torch.Size([256])
encoder.stages.3.0.convs.1.norm.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
encoder.stages.3.0.convs.1.all_modules.0.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.1.weight shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.1.bias shape torch.Size([256])
encoder.stages.4.0.convs.0.conv.weight shape torch.Size([320, 256, 3, 3, 3])
encoder.stages.4.0.convs.0.conv.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.norm.weight shape torch.Size([320])
encoder.stages.4.0.convs.0.norm.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.0.weight shape torch.Size([320, 256, 3, 3, 3])
encoder.stages.4.0.convs.0.all_modules.0.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.1.weight shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.1.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.4.0.convs.1.conv.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.norm.weight shape torch.Size([320])
encoder.stages.4.0.convs.1.norm.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.4.0.convs.1.all_modules.0.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.1.weight shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.1.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.0.conv.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.norm.weight shape torch.Size([320])
encoder.stages.5.0.convs.0.norm.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.0.all_modules.0.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.1.weight shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.1.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.1.conv.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.norm.weight shape torch.Size([320])
encoder.stages.5.0.convs.1.norm.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.1.all_modules.0.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.1.weight shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.0.0.convs.0.conv.weight shape torch.Size([32, 1, 3, 3, 3])
decoder.encoder.stages.0.0.convs.0.conv.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.norm.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.norm.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.0.weight shape torch.Size([32, 1, 3, 3, 3])
decoder.encoder.stages.0.0.convs.0.all_modules.0.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.1.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.1.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.encoder.stages.0.0.convs.1.conv.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.norm.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.norm.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.encoder.stages.0.0.convs.1.all_modules.0.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.1.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.1.bias shape torch.Size([32])
decoder.encoder.stages.1.0.convs.0.conv.weight shape torch.Size([64, 32, 3, 3, 3])
decoder.encoder.stages.1.0.convs.0.conv.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.norm.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.norm.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.0.weight shape torch.Size([64, 32, 3, 3, 3])
decoder.encoder.stages.1.0.convs.0.all_modules.0.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.1.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.1.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.encoder.stages.1.0.convs.1.conv.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.norm.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.norm.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.encoder.stages.1.0.convs.1.all_modules.0.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.1.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.1.bias shape torch.Size([64])
decoder.encoder.stages.2.0.convs.0.conv.weight shape torch.Size([128, 64, 3, 3, 3])
decoder.encoder.stages.2.0.convs.0.conv.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.norm.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.norm.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.0.weight shape torch.Size([128, 64, 3, 3, 3])
decoder.encoder.stages.2.0.convs.0.all_modules.0.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.1.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.1.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.encoder.stages.2.0.convs.1.conv.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.norm.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.norm.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.encoder.stages.2.0.convs.1.all_modules.0.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.1.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.1.bias shape torch.Size([128])
decoder.encoder.stages.3.0.convs.0.conv.weight shape torch.Size([256, 128, 3, 3, 3])
decoder.encoder.stages.3.0.convs.0.conv.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.norm.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.norm.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.0.weight shape torch.Size([256, 128, 3, 3, 3])
decoder.encoder.stages.3.0.convs.0.all_modules.0.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.1.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.1.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.encoder.stages.3.0.convs.1.conv.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.norm.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.norm.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.encoder.stages.3.0.convs.1.all_modules.0.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.1.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.1.bias shape torch.Size([256])
decoder.encoder.stages.4.0.convs.0.conv.weight shape torch.Size([320, 256, 3, 3, 3])
decoder.encoder.stages.4.0.convs.0.conv.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.norm.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.norm.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.0.weight shape torch.Size([320, 256, 3, 3, 3])
decoder.encoder.stages.4.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.4.0.convs.1.conv.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.norm.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.norm.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.4.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.0.conv.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.norm.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.norm.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.1.conv.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.norm.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.norm.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.stages.0.convs.0.conv.weight shape torch.Size([320, 640, 3, 3, 3])
decoder.stages.0.convs.0.conv.bias shape torch.Size([320])
decoder.stages.0.convs.0.norm.weight shape torch.Size([320])
decoder.stages.0.convs.0.norm.bias shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.0.weight shape torch.Size([320, 640, 3, 3, 3])
decoder.stages.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.stages.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.stages.0.convs.1.conv.bias shape torch.Size([320])
decoder.stages.0.convs.1.norm.weight shape torch.Size([320])
decoder.stages.0.convs.1.norm.bias shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.stages.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.stages.1.convs.0.conv.weight shape torch.Size([256, 512, 3, 3, 3])
decoder.stages.1.convs.0.conv.bias shape torch.Size([256])
decoder.stages.1.convs.0.norm.weight shape torch.Size([256])
decoder.stages.1.convs.0.norm.bias shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.0.weight shape torch.Size([256, 512, 3, 3, 3])
decoder.stages.1.convs.0.all_modules.0.bias shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.1.weight shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.1.bias shape torch.Size([256])
decoder.stages.1.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.stages.1.convs.1.conv.bias shape torch.Size([256])
decoder.stages.1.convs.1.norm.weight shape torch.Size([256])
decoder.stages.1.convs.1.norm.bias shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.stages.1.convs.1.all_modules.0.bias shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.1.weight shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.1.bias shape torch.Size([256])
decoder.stages.2.convs.0.conv.weight shape torch.Size([128, 256, 3, 3, 3])
decoder.stages.2.convs.0.conv.bias shape torch.Size([128])
decoder.stages.2.convs.0.norm.weight shape torch.Size([128])
decoder.stages.2.convs.0.norm.bias shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.0.weight shape torch.Size([128, 256, 3, 3, 3])
decoder.stages.2.convs.0.all_modules.0.bias shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.1.weight shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.1.bias shape torch.Size([128])
decoder.stages.2.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.stages.2.convs.1.conv.bias shape torch.Size([128])
decoder.stages.2.convs.1.norm.weight shape torch.Size([128])
decoder.stages.2.convs.1.norm.bias shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.stages.2.convs.1.all_modules.0.bias shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.1.weight shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.1.bias shape torch.Size([128])
decoder.stages.3.convs.0.conv.weight shape torch.Size([64, 128, 3, 3, 3])
decoder.stages.3.convs.0.conv.bias shape torch.Size([64])
decoder.stages.3.convs.0.norm.weight shape torch.Size([64])
decoder.stages.3.convs.0.norm.bias shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.0.weight shape torch.Size([64, 128, 3, 3, 3])
decoder.stages.3.convs.0.all_modules.0.bias shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.1.weight shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.1.bias shape torch.Size([64])
decoder.stages.3.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.stages.3.convs.1.conv.bias shape torch.Size([64])
decoder.stages.3.convs.1.norm.weight shape torch.Size([64])
decoder.stages.3.convs.1.norm.bias shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.stages.3.convs.1.all_modules.0.bias shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.1.weight shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.1.bias shape torch.Size([64])
decoder.stages.4.convs.0.conv.weight shape torch.Size([32, 64, 3, 3, 3])
decoder.stages.4.convs.0.conv.bias shape torch.Size([32])
decoder.stages.4.convs.0.norm.weight shape torch.Size([32])
decoder.stages.4.convs.0.norm.bias shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.0.weight shape torch.Size([32, 64, 3, 3, 3])
decoder.stages.4.convs.0.all_modules.0.bias shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.1.weight shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.1.bias shape torch.Size([32])
decoder.stages.4.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.stages.4.convs.1.conv.bias shape torch.Size([32])
decoder.stages.4.convs.1.norm.weight shape torch.Size([32])
decoder.stages.4.convs.1.norm.bias shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.stages.4.convs.1.all_modules.0.bias shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.1.weight shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.1.bias shape torch.Size([32])
decoder.transpconvs.0.weight shape torch.Size([320, 320, 1, 2, 2])
decoder.transpconvs.0.bias shape torch.Size([320])
decoder.transpconvs.1.weight shape torch.Size([320, 256, 2, 2, 2])
decoder.transpconvs.1.bias shape torch.Size([256])
decoder.transpconvs.2.weight shape torch.Size([256, 128, 2, 2, 2])
decoder.transpconvs.2.bias shape torch.Size([128])
decoder.transpconvs.3.weight shape torch.Size([128, 64, 2, 2, 2])
decoder.transpconvs.3.bias shape torch.Size([64])
decoder.transpconvs.4.weight shape torch.Size([64, 32, 2, 2, 2])
decoder.transpconvs.4.bias shape torch.Size([32])
################### Done ###################
2024-12-20 02:20:06.496349: do_dummy_2d_data_aug: True
2024-12-20 02:20:06.498428: Using splits from existing split file: /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset308_Sohee_Calcium_OCT_CrossValidation/splits_final.json
2024-12-20 02:20:06.500265: The split file contains 5 splits.
2024-12-20 02:20:06.501076: Desired fold for training: 4
2024-12-20 02:20:06.501901: This split has 7 training and 1 validation cases.
using pin_memory on device 0
using pin_memory on device 0

This is the configuration used by this training:
Configuration name: 3d_32x160x128_b10
 {'data_identifier': 'nnUNetPlans_3d_fullres', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 10, 'patch_size': [32, 160, 128], 'median_image_size_in_voxels': [375.0, 498.0, 498.0], 'spacing': [1.0, 1.0, 1.0], 'normalization_schemes': ['ZScoreNormalization'], 'use_mask_for_norm': [False], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'architecture': {'network_class_name': 'dynamic_network_architectures.architectures.unet.PlainConvUNet', 'arch_kwargs': {'n_stages': 6, 'features_per_stage': [32, 64, 128, 256, 320, 320], 'conv_op': 'torch.nn.modules.conv.Conv3d', 'kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]], 'strides': [[1, 1, 1], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [1, 2, 2]], 'n_conv_per_stage': [2, 2, 2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2, 2, 2], 'conv_bias': True, 'norm_op': 'torch.nn.modules.instancenorm.InstanceNorm3d', 'norm_op_kwargs': {'eps': 1e-05, 'affine': True}, 'dropout_op': None, 'dropout_op_kwargs': None, 'nonlin': 'torch.nn.LeakyReLU', 'nonlin_kwargs': {'inplace': True}, 'deep_supervision': True}, '_kw_requires_import': ['conv_op', 'norm_op', 'dropout_op', 'nonlin']}, 'batch_dice': True, 'inherits_from': '3d_fullres'} 

These are the global plan.json settings:
 {'dataset_name': 'Dataset308_Sohee_Calcium_OCT_CrossValidation', 'plans_name': 'nnUNetPlans', 'original_median_spacing_after_transp': [1.0, 1.0, 1.0], 'original_median_shape_after_transp': [375, 498, 498], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 0.4977976381778717, 'mean': 0.13507525622844696, 'median': 0.09599608182907104, 'min': 0.0, 'percentile_00_5': 0.014754901640117168, 'percentile_99_5': 0.4977976381778717, 'std': 0.12025152146816254}}} 

2024-12-20 02:20:33.184261: unpacking dataset...
2024-12-20 02:20:37.455523: unpacking done...
2024-12-20 02:20:37.868247: Unable to plot network architecture: nnUNet_compile is enabled!
2024-12-20 02:20:38.029179: 
2024-12-20 02:20:38.030470: Epoch 0
2024-12-20 02:20:38.031665: Current learning rate: 0.01
2024-12-20 02:28:22.900608: Validation loss improved from 1000.00000 to -0.19844! Patience: 0/50
2024-12-20 02:28:22.901774: train_loss -0.0475
2024-12-20 02:28:22.902859: val_loss -0.1984
2024-12-20 02:28:22.903748: Pseudo dice [0.5506]
2024-12-20 02:28:22.904614: Epoch time: 464.87 s
2024-12-20 02:28:22.905366: Yayy! New best EMA pseudo Dice: 0.5506
2024-12-20 02:28:24.968605: 
2024-12-20 02:28:24.970001: Epoch 1
2024-12-20 02:28:24.971120: Current learning rate: 0.00994
2024-12-20 02:34:48.276979: Validation loss improved from -0.19844 to -0.27264! Patience: 0/50
2024-12-20 02:34:48.278133: train_loss -0.2215
2024-12-20 02:34:48.279002: val_loss -0.2726
2024-12-20 02:34:48.279858: Pseudo dice [0.5923]
2024-12-20 02:34:48.280664: Epoch time: 383.31 s
2024-12-20 02:34:48.281564: Yayy! New best EMA pseudo Dice: 0.5547
2024-12-20 02:34:50.155084: 
2024-12-20 02:34:50.156433: Epoch 2
2024-12-20 02:34:50.157267: Current learning rate: 0.00988
2024-12-20 02:41:32.239589: Validation loss did not improve from -0.27264. Patience: 1/50
2024-12-20 02:41:32.240693: train_loss -0.2619
2024-12-20 02:41:32.242218: val_loss -0.2628
2024-12-20 02:41:32.242937: Pseudo dice [0.5906]
2024-12-20 02:41:32.243634: Epoch time: 402.09 s
2024-12-20 02:41:32.244306: Yayy! New best EMA pseudo Dice: 0.5583
2024-12-20 02:41:34.050632: 
2024-12-20 02:41:34.052001: Epoch 3
2024-12-20 02:41:34.052930: Current learning rate: 0.00982
2024-12-20 02:48:16.418829: Validation loss improved from -0.27264 to -0.30774! Patience: 1/50
2024-12-20 02:48:16.419959: train_loss -0.3057
2024-12-20 02:48:16.421135: val_loss -0.3077
2024-12-20 02:48:16.421929: Pseudo dice [0.6245]
2024-12-20 02:48:16.422650: Epoch time: 402.37 s
2024-12-20 02:48:16.423324: Yayy! New best EMA pseudo Dice: 0.5649
2024-12-20 02:48:18.178920: 
2024-12-20 02:48:18.180489: Epoch 4
2024-12-20 02:48:18.181409: Current learning rate: 0.00976
2024-12-20 02:54:55.185024: Validation loss improved from -0.30774 to -0.31288! Patience: 0/50
2024-12-20 02:54:55.185907: train_loss -0.3483
2024-12-20 02:54:55.186728: val_loss -0.3129
2024-12-20 02:54:55.187506: Pseudo dice [0.6033]
2024-12-20 02:54:55.188332: Epoch time: 397.01 s
2024-12-20 02:54:55.585227: Yayy! New best EMA pseudo Dice: 0.5688
2024-12-20 02:54:57.479643: 
2024-12-20 02:54:57.481068: Epoch 5
2024-12-20 02:54:57.481867: Current learning rate: 0.0097
2024-12-20 03:01:38.243931: Validation loss did not improve from -0.31288. Patience: 1/50
2024-12-20 03:01:38.244997: train_loss -0.3689
2024-12-20 03:01:38.245840: val_loss -0.2979
2024-12-20 03:01:38.246578: Pseudo dice [0.5835]
2024-12-20 03:01:38.247293: Epoch time: 400.77 s
2024-12-20 03:01:38.247974: Yayy! New best EMA pseudo Dice: 0.5703
2024-12-20 03:01:40.010247: 
2024-12-20 03:01:40.011577: Epoch 6
2024-12-20 03:01:40.012451: Current learning rate: 0.00964
2024-12-20 03:08:06.534178: Validation loss improved from -0.31288 to -0.38862! Patience: 1/50
2024-12-20 03:08:06.535181: train_loss -0.4027
2024-12-20 03:08:06.535970: val_loss -0.3886
2024-12-20 03:08:06.536916: Pseudo dice [0.6567]
2024-12-20 03:08:06.537910: Epoch time: 386.53 s
2024-12-20 03:08:06.538728: Yayy! New best EMA pseudo Dice: 0.5789
2024-12-20 03:08:08.313882: 
2024-12-20 03:08:08.315297: Epoch 7
2024-12-20 03:08:08.316372: Current learning rate: 0.00958
2024-12-20 03:14:56.422369: Validation loss did not improve from -0.38862. Patience: 1/50
2024-12-20 03:14:56.423429: train_loss -0.4192
2024-12-20 03:14:56.424389: val_loss -0.3753
2024-12-20 03:14:56.425114: Pseudo dice [0.6526]
2024-12-20 03:14:56.425804: Epoch time: 408.11 s
2024-12-20 03:14:56.426472: Yayy! New best EMA pseudo Dice: 0.5863
2024-12-20 03:14:58.172741: 
2024-12-20 03:14:58.174126: Epoch 8
2024-12-20 03:14:58.174908: Current learning rate: 0.00952
2024-12-20 03:21:48.348135: Validation loss did not improve from -0.38862. Patience: 2/50
2024-12-20 03:21:48.349284: train_loss -0.4183
2024-12-20 03:21:48.350102: val_loss -0.3735
2024-12-20 03:21:48.350772: Pseudo dice [0.6431]
2024-12-20 03:21:48.351455: Epoch time: 410.18 s
2024-12-20 03:21:48.352058: Yayy! New best EMA pseudo Dice: 0.592
2024-12-20 03:21:50.511340: 
2024-12-20 03:21:50.512685: Epoch 9
2024-12-20 03:21:50.513324: Current learning rate: 0.00946
2024-12-20 03:28:36.965464: Validation loss improved from -0.38862 to -0.39451! Patience: 2/50
2024-12-20 03:28:36.968584: train_loss -0.4385
2024-12-20 03:28:36.970996: val_loss -0.3945
2024-12-20 03:28:36.971929: Pseudo dice [0.6407]
2024-12-20 03:28:36.972902: Epoch time: 406.46 s
2024-12-20 03:28:37.376908: Yayy! New best EMA pseudo Dice: 0.5968
2024-12-20 03:28:39.178289: 
2024-12-20 03:28:39.180095: Epoch 10
2024-12-20 03:28:39.180954: Current learning rate: 0.0094
2024-12-20 03:35:09.371225: Validation loss improved from -0.39451 to -0.41120! Patience: 0/50
2024-12-20 03:35:09.372415: train_loss -0.4746
2024-12-20 03:35:09.373176: val_loss -0.4112
2024-12-20 03:35:09.373858: Pseudo dice [0.668]
2024-12-20 03:35:09.374621: Epoch time: 390.2 s
2024-12-20 03:35:09.375404: Yayy! New best EMA pseudo Dice: 0.6039
2024-12-20 03:35:11.131173: 
2024-12-20 03:35:11.133006: Epoch 11
2024-12-20 03:35:11.134070: Current learning rate: 0.00934
2024-12-20 03:41:25.550793: Validation loss improved from -0.41120 to -0.42031! Patience: 0/50
2024-12-20 03:41:25.552094: train_loss -0.45
2024-12-20 03:41:25.553010: val_loss -0.4203
2024-12-20 03:41:25.553659: Pseudo dice [0.6727]
2024-12-20 03:41:25.554471: Epoch time: 374.42 s
2024-12-20 03:41:25.555150: Yayy! New best EMA pseudo Dice: 0.6108
2024-12-20 03:41:27.284842: 
2024-12-20 03:41:27.286027: Epoch 12
2024-12-20 03:41:27.286788: Current learning rate: 0.00928
2024-12-20 03:47:46.467739: Validation loss did not improve from -0.42031. Patience: 1/50
2024-12-20 03:47:46.468627: train_loss -0.4555
2024-12-20 03:47:46.469503: val_loss -0.4076
2024-12-20 03:47:46.470349: Pseudo dice [0.6677]
2024-12-20 03:47:46.471184: Epoch time: 379.18 s
2024-12-20 03:47:46.471999: Yayy! New best EMA pseudo Dice: 0.6165
2024-12-20 03:47:48.191920: 
2024-12-20 03:47:48.193316: Epoch 13
2024-12-20 03:47:48.194181: Current learning rate: 0.00922
2024-12-20 03:54:29.966963: Validation loss improved from -0.42031 to -0.44800! Patience: 1/50
2024-12-20 03:54:29.968690: train_loss -0.4835
2024-12-20 03:54:29.969713: val_loss -0.448
2024-12-20 03:54:29.970489: Pseudo dice [0.6911]
2024-12-20 03:54:29.971256: Epoch time: 401.78 s
2024-12-20 03:54:29.971986: Yayy! New best EMA pseudo Dice: 0.624
2024-12-20 03:54:31.685308: 
2024-12-20 03:54:31.686471: Epoch 14
2024-12-20 03:54:31.687232: Current learning rate: 0.00916
2024-12-20 04:00:56.256437: Validation loss did not improve from -0.44800. Patience: 1/50
2024-12-20 04:00:56.261036: train_loss -0.5022
2024-12-20 04:00:56.262805: val_loss -0.4157
2024-12-20 04:00:56.263652: Pseudo dice [0.6751]
2024-12-20 04:00:56.264655: Epoch time: 384.58 s
2024-12-20 04:00:56.680199: Yayy! New best EMA pseudo Dice: 0.6291
2024-12-20 04:00:58.445851: 
2024-12-20 04:00:58.447780: Epoch 15
2024-12-20 04:00:58.448601: Current learning rate: 0.0091
2024-12-20 04:07:35.103968: Validation loss improved from -0.44800 to -0.46075! Patience: 1/50
2024-12-20 04:07:35.105374: train_loss -0.4932
2024-12-20 04:07:35.106443: val_loss -0.4607
2024-12-20 04:07:35.107336: Pseudo dice [0.7009]
2024-12-20 04:07:35.108229: Epoch time: 396.66 s
2024-12-20 04:07:35.108965: Yayy! New best EMA pseudo Dice: 0.6363
2024-12-20 04:07:36.854924: 
2024-12-20 04:07:36.856452: Epoch 16
2024-12-20 04:07:36.857765: Current learning rate: 0.00903
2024-12-20 04:14:04.024796: Validation loss did not improve from -0.46075. Patience: 1/50
2024-12-20 04:14:04.025830: train_loss -0.5068
2024-12-20 04:14:04.026911: val_loss -0.4356
2024-12-20 04:14:04.028027: Pseudo dice [0.6879]
2024-12-20 04:14:04.029130: Epoch time: 387.17 s
2024-12-20 04:14:04.030273: Yayy! New best EMA pseudo Dice: 0.6414
2024-12-20 04:14:05.800902: 
2024-12-20 04:14:05.802595: Epoch 17
2024-12-20 04:14:05.803761: Current learning rate: 0.00897
2024-12-20 04:20:54.619766: Validation loss did not improve from -0.46075. Patience: 2/50
2024-12-20 04:20:54.620790: train_loss -0.513
2024-12-20 04:20:54.621817: val_loss -0.4383
2024-12-20 04:20:54.622713: Pseudo dice [0.6845]
2024-12-20 04:20:54.623631: Epoch time: 408.82 s
2024-12-20 04:20:54.624366: Yayy! New best EMA pseudo Dice: 0.6457
2024-12-20 04:20:56.410849: 
2024-12-20 04:20:56.412047: Epoch 18
2024-12-20 04:20:56.412899: Current learning rate: 0.00891
2024-12-20 04:27:35.196665: Validation loss did not improve from -0.46075. Patience: 3/50
2024-12-20 04:27:35.197681: train_loss -0.5108
2024-12-20 04:27:35.198477: val_loss -0.4342
2024-12-20 04:27:35.199134: Pseudo dice [0.6754]
2024-12-20 04:27:35.199938: Epoch time: 398.79 s
2024-12-20 04:27:35.200707: Yayy! New best EMA pseudo Dice: 0.6487
2024-12-20 04:27:39.451382: 
2024-12-20 04:27:39.452776: Epoch 19
2024-12-20 04:27:39.453526: Current learning rate: 0.00885
2024-12-20 04:34:16.932055: Validation loss did not improve from -0.46075. Patience: 4/50
2024-12-20 04:34:16.933474: train_loss -0.5258
2024-12-20 04:34:16.934813: val_loss -0.4345
2024-12-20 04:34:16.935490: Pseudo dice [0.6645]
2024-12-20 04:34:16.936122: Epoch time: 397.48 s
2024-12-20 04:34:17.838473: Yayy! New best EMA pseudo Dice: 0.6503
2024-12-20 04:34:19.922858: 
2024-12-20 04:34:19.924009: Epoch 20
2024-12-20 04:34:19.925031: Current learning rate: 0.00879
2024-12-20 04:41:37.063495: Validation loss improved from -0.46075 to -0.46650! Patience: 4/50
2024-12-20 04:41:37.064574: train_loss -0.5149
2024-12-20 04:41:37.065387: val_loss -0.4665
2024-12-20 04:41:37.066106: Pseudo dice [0.7119]
2024-12-20 04:41:37.066895: Epoch time: 437.14 s
2024-12-20 04:41:37.067786: Yayy! New best EMA pseudo Dice: 0.6564
2024-12-20 04:41:38.878775: 
2024-12-20 04:41:38.880260: Epoch 21
2024-12-20 04:41:38.881192: Current learning rate: 0.00873
2024-12-20 04:47:47.771801: Validation loss did not improve from -0.46650. Patience: 1/50
2024-12-20 04:47:47.772827: train_loss -0.5312
2024-12-20 04:47:47.773594: val_loss -0.4208
2024-12-20 04:47:47.774300: Pseudo dice [0.6765]
2024-12-20 04:47:47.774973: Epoch time: 368.9 s
2024-12-20 04:47:47.775720: Yayy! New best EMA pseudo Dice: 0.6585
2024-12-20 04:47:49.476175: 
2024-12-20 04:47:49.477571: Epoch 22
2024-12-20 04:47:49.478506: Current learning rate: 0.00867
2024-12-20 04:54:37.103681: Validation loss did not improve from -0.46650. Patience: 2/50
2024-12-20 04:54:37.105361: train_loss -0.5455
2024-12-20 04:54:37.106325: val_loss -0.4626
2024-12-20 04:54:37.107003: Pseudo dice [0.6913]
2024-12-20 04:54:37.107751: Epoch time: 407.63 s
2024-12-20 04:54:37.108756: Yayy! New best EMA pseudo Dice: 0.6617
2024-12-20 04:54:38.820343: 
2024-12-20 04:54:38.821668: Epoch 23
2024-12-20 04:54:38.822357: Current learning rate: 0.00861
2024-12-20 05:01:12.677683: Validation loss did not improve from -0.46650. Patience: 3/50
2024-12-20 05:01:12.679110: train_loss -0.5433
2024-12-20 05:01:12.680082: val_loss -0.4179
2024-12-20 05:01:12.680885: Pseudo dice [0.6733]
2024-12-20 05:01:12.681647: Epoch time: 393.86 s
2024-12-20 05:01:12.682530: Yayy! New best EMA pseudo Dice: 0.6629
2024-12-20 05:01:14.376274: 
2024-12-20 05:01:14.377666: Epoch 24
2024-12-20 05:01:14.378371: Current learning rate: 0.00855
2024-12-20 05:07:42.670263: Validation loss improved from -0.46650 to -0.49683! Patience: 3/50
2024-12-20 05:07:42.705062: train_loss -0.5541
2024-12-20 05:07:42.707408: val_loss -0.4968
2024-12-20 05:07:42.708172: Pseudo dice [0.7229]
2024-12-20 05:07:42.709256: Epoch time: 388.33 s
2024-12-20 05:07:43.235085: Yayy! New best EMA pseudo Dice: 0.6689
2024-12-20 05:07:45.052219: 
2024-12-20 05:07:45.053695: Epoch 25
2024-12-20 05:07:45.054501: Current learning rate: 0.00849
2024-12-20 05:14:20.786444: Validation loss did not improve from -0.49683. Patience: 1/50
2024-12-20 05:14:20.788261: train_loss -0.5592
2024-12-20 05:14:20.789331: val_loss -0.4874
2024-12-20 05:14:20.790134: Pseudo dice [0.7241]
2024-12-20 05:14:20.790900: Epoch time: 395.74 s
2024-12-20 05:14:20.791678: Yayy! New best EMA pseudo Dice: 0.6744
2024-12-20 05:14:22.544618: 
2024-12-20 05:14:22.546033: Epoch 26
2024-12-20 05:14:22.546824: Current learning rate: 0.00843
2024-12-20 05:20:52.428292: Validation loss did not improve from -0.49683. Patience: 2/50
2024-12-20 05:20:52.429366: train_loss -0.5662
2024-12-20 05:20:52.430152: val_loss -0.4745
2024-12-20 05:20:52.430889: Pseudo dice [0.6962]
2024-12-20 05:20:52.431667: Epoch time: 389.89 s
2024-12-20 05:20:52.432290: Yayy! New best EMA pseudo Dice: 0.6766
2024-12-20 05:20:54.178932: 
2024-12-20 05:20:54.180278: Epoch 27
2024-12-20 05:20:54.181131: Current learning rate: 0.00836
2024-12-20 05:27:35.558759: Validation loss did not improve from -0.49683. Patience: 3/50
2024-12-20 05:27:35.559925: train_loss -0.5692
2024-12-20 05:27:35.561063: val_loss -0.3988
2024-12-20 05:27:35.561768: Pseudo dice [0.6707]
2024-12-20 05:27:35.562710: Epoch time: 401.38 s
2024-12-20 05:27:37.039679: 
2024-12-20 05:27:37.041128: Epoch 28
2024-12-20 05:27:37.042056: Current learning rate: 0.0083
2024-12-20 05:34:11.044995: Validation loss did not improve from -0.49683. Patience: 4/50
2024-12-20 05:34:11.046054: train_loss -0.5728
2024-12-20 05:34:11.046894: val_loss -0.4785
2024-12-20 05:34:11.047611: Pseudo dice [0.7092]
2024-12-20 05:34:11.048357: Epoch time: 394.01 s
2024-12-20 05:34:11.048995: Yayy! New best EMA pseudo Dice: 0.6793
2024-12-20 05:34:12.766201: 
2024-12-20 05:34:12.767524: Epoch 29
2024-12-20 05:34:12.768222: Current learning rate: 0.00824
2024-12-20 05:40:38.543867: Validation loss did not improve from -0.49683. Patience: 5/50
2024-12-20 05:40:38.545260: train_loss -0.5685
2024-12-20 05:40:38.546673: val_loss -0.4704
2024-12-20 05:40:38.547696: Pseudo dice [0.6989]
2024-12-20 05:40:38.548633: Epoch time: 385.78 s
2024-12-20 05:40:38.953946: Yayy! New best EMA pseudo Dice: 0.6813
2024-12-20 05:40:41.319243: 
2024-12-20 05:40:41.320845: Epoch 30
2024-12-20 05:40:41.321933: Current learning rate: 0.00818
2024-12-20 05:47:00.273464: Validation loss did not improve from -0.49683. Patience: 6/50
2024-12-20 05:47:00.274590: train_loss -0.5755
2024-12-20 05:47:00.275461: val_loss -0.4688
2024-12-20 05:47:00.276306: Pseudo dice [0.7018]
2024-12-20 05:47:00.277045: Epoch time: 378.96 s
2024-12-20 05:47:00.277719: Yayy! New best EMA pseudo Dice: 0.6833
2024-12-20 05:47:02.109813: 
2024-12-20 05:47:02.111239: Epoch 31
2024-12-20 05:47:02.111977: Current learning rate: 0.00812
2024-12-20 05:54:22.424788: Validation loss did not improve from -0.49683. Patience: 7/50
2024-12-20 05:54:22.425777: train_loss -0.5828
2024-12-20 05:54:22.426628: val_loss -0.4797
2024-12-20 05:54:22.427396: Pseudo dice [0.7096]
2024-12-20 05:54:22.428133: Epoch time: 440.32 s
2024-12-20 05:54:22.428962: Yayy! New best EMA pseudo Dice: 0.686
2024-12-20 05:54:24.203253: 
2024-12-20 05:54:24.204522: Epoch 32
2024-12-20 05:54:24.205398: Current learning rate: 0.00806
2024-12-20 06:01:24.042818: Validation loss did not improve from -0.49683. Patience: 8/50
2024-12-20 06:01:24.043860: train_loss -0.5811
2024-12-20 06:01:24.044595: val_loss -0.465
2024-12-20 06:01:24.045521: Pseudo dice [0.7025]
2024-12-20 06:01:24.046218: Epoch time: 419.84 s
2024-12-20 06:01:24.046876: Yayy! New best EMA pseudo Dice: 0.6876
2024-12-20 06:01:25.782702: 
2024-12-20 06:01:25.784462: Epoch 33
2024-12-20 06:01:25.785524: Current learning rate: 0.008
2024-12-20 06:08:10.806273: Validation loss did not improve from -0.49683. Patience: 9/50
2024-12-20 06:08:10.810117: train_loss -0.5778
2024-12-20 06:08:10.811476: val_loss -0.4436
2024-12-20 06:08:10.812166: Pseudo dice [0.6788]
2024-12-20 06:08:10.812892: Epoch time: 405.03 s
2024-12-20 06:08:12.233501: 
2024-12-20 06:08:12.234874: Epoch 34
2024-12-20 06:08:12.235651: Current learning rate: 0.00793
2024-12-20 06:14:39.741632: Validation loss did not improve from -0.49683. Patience: 10/50
2024-12-20 06:14:39.752658: train_loss -0.5897
2024-12-20 06:14:39.754502: val_loss -0.4657
2024-12-20 06:14:39.755601: Pseudo dice [0.7032]
2024-12-20 06:14:39.756709: Epoch time: 387.52 s
2024-12-20 06:14:40.237255: Yayy! New best EMA pseudo Dice: 0.6884
2024-12-20 06:14:42.071531: 
2024-12-20 06:14:42.072831: Epoch 35
2024-12-20 06:14:42.073572: Current learning rate: 0.00787
2024-12-20 06:21:44.207721: Validation loss did not improve from -0.49683. Patience: 11/50
2024-12-20 06:21:44.208708: train_loss -0.6056
2024-12-20 06:21:44.209714: val_loss -0.4663
2024-12-20 06:21:44.210403: Pseudo dice [0.7027]
2024-12-20 06:21:44.211095: Epoch time: 422.14 s
2024-12-20 06:21:44.211848: Yayy! New best EMA pseudo Dice: 0.6898
2024-12-20 06:21:46.049478: 
2024-12-20 06:21:46.050951: Epoch 36
2024-12-20 06:21:46.051727: Current learning rate: 0.00781
2024-12-20 06:28:48.570493: Validation loss did not improve from -0.49683. Patience: 12/50
2024-12-20 06:28:48.572657: train_loss -0.6039
2024-12-20 06:28:48.573746: val_loss -0.4953
2024-12-20 06:28:48.574624: Pseudo dice [0.7291]
2024-12-20 06:28:48.575507: Epoch time: 422.53 s
2024-12-20 06:28:48.576365: Yayy! New best EMA pseudo Dice: 0.6937
2024-12-20 06:28:50.388263: 
2024-12-20 06:28:50.389961: Epoch 37
2024-12-20 06:28:50.391123: Current learning rate: 0.00775
2024-12-20 06:35:32.295677: Validation loss did not improve from -0.49683. Patience: 13/50
2024-12-20 06:35:32.296714: train_loss -0.5995
2024-12-20 06:35:32.297516: val_loss -0.4682
2024-12-20 06:35:32.298208: Pseudo dice [0.6869]
2024-12-20 06:35:32.299048: Epoch time: 401.91 s
2024-12-20 06:35:33.694917: 
2024-12-20 06:35:33.696291: Epoch 38
2024-12-20 06:35:33.697183: Current learning rate: 0.00769
2024-12-20 06:42:05.794753: Validation loss did not improve from -0.49683. Patience: 14/50
2024-12-20 06:42:05.795835: train_loss -0.608
2024-12-20 06:42:05.796657: val_loss -0.4962
2024-12-20 06:42:05.797548: Pseudo dice [0.7133]
2024-12-20 06:42:05.798369: Epoch time: 392.1 s
2024-12-20 06:42:05.799144: Yayy! New best EMA pseudo Dice: 0.6951
2024-12-20 06:42:07.613167: 
2024-12-20 06:42:07.614584: Epoch 39
2024-12-20 06:42:07.615498: Current learning rate: 0.00763
2024-12-20 06:49:12.533284: Validation loss did not improve from -0.49683. Patience: 15/50
2024-12-20 06:49:12.534473: train_loss -0.6102
2024-12-20 06:49:12.535882: val_loss -0.4778
2024-12-20 06:49:12.536762: Pseudo dice [0.7061]
2024-12-20 06:49:12.537722: Epoch time: 424.92 s
2024-12-20 06:49:12.937096: Yayy! New best EMA pseudo Dice: 0.6962
2024-12-20 06:49:15.270816: 
2024-12-20 06:49:15.272129: Epoch 40
2024-12-20 06:49:15.273073: Current learning rate: 0.00756
2024-12-20 06:56:00.685610: Validation loss did not improve from -0.49683. Patience: 16/50
2024-12-20 06:56:00.686693: train_loss -0.6183
2024-12-20 06:56:00.687683: val_loss -0.4851
2024-12-20 06:56:00.688410: Pseudo dice [0.7187]
2024-12-20 06:56:00.689334: Epoch time: 405.42 s
2024-12-20 06:56:00.690041: Yayy! New best EMA pseudo Dice: 0.6984
2024-12-20 06:56:02.499135: 
2024-12-20 06:56:02.500383: Epoch 41
2024-12-20 06:56:02.501294: Current learning rate: 0.0075
2024-12-20 07:02:35.270880: Validation loss did not improve from -0.49683. Patience: 17/50
2024-12-20 07:02:35.271937: train_loss -0.6212
2024-12-20 07:02:35.272771: val_loss -0.4626
2024-12-20 07:02:35.273586: Pseudo dice [0.7048]
2024-12-20 07:02:35.274309: Epoch time: 392.77 s
2024-12-20 07:02:35.274973: Yayy! New best EMA pseudo Dice: 0.6991
2024-12-20 07:02:36.983610: 
2024-12-20 07:02:36.984957: Epoch 42
2024-12-20 07:02:36.985712: Current learning rate: 0.00744
2024-12-20 07:09:15.620330: Validation loss did not improve from -0.49683. Patience: 18/50
2024-12-20 07:09:15.621351: train_loss -0.6182
2024-12-20 07:09:15.622356: val_loss -0.4866
2024-12-20 07:09:15.623153: Pseudo dice [0.7094]
2024-12-20 07:09:15.624019: Epoch time: 398.64 s
2024-12-20 07:09:15.624834: Yayy! New best EMA pseudo Dice: 0.7001
2024-12-20 07:09:17.376958: 
2024-12-20 07:09:17.378444: Epoch 43
2024-12-20 07:09:17.379433: Current learning rate: 0.00738
2024-12-20 07:15:54.590798: Validation loss did not improve from -0.49683. Patience: 19/50
2024-12-20 07:15:54.596099: train_loss -0.6273
2024-12-20 07:15:54.598521: val_loss -0.4582
2024-12-20 07:15:54.599492: Pseudo dice [0.7085]
2024-12-20 07:15:54.601062: Epoch time: 397.22 s
2024-12-20 07:15:54.602371: Yayy! New best EMA pseudo Dice: 0.701
2024-12-20 07:15:56.285858: 
2024-12-20 07:15:56.287400: Epoch 44
2024-12-20 07:15:56.288367: Current learning rate: 0.00732
2024-12-20 07:22:44.456181: Validation loss did not improve from -0.49683. Patience: 20/50
2024-12-20 07:22:44.457168: train_loss -0.6204
2024-12-20 07:22:44.457904: val_loss -0.4635
2024-12-20 07:22:44.458596: Pseudo dice [0.6919]
2024-12-20 07:22:44.459402: Epoch time: 408.17 s
2024-12-20 07:22:46.152071: 
2024-12-20 07:22:46.153383: Epoch 45
2024-12-20 07:22:46.154293: Current learning rate: 0.00725
2024-12-20 07:29:24.956540: Validation loss did not improve from -0.49683. Patience: 21/50
2024-12-20 07:29:24.957509: train_loss -0.6234
2024-12-20 07:29:24.958455: val_loss -0.4574
2024-12-20 07:29:24.959327: Pseudo dice [0.6992]
2024-12-20 07:29:24.960212: Epoch time: 398.81 s
2024-12-20 07:29:26.307404: 
2024-12-20 07:29:26.308696: Epoch 46
2024-12-20 07:29:26.309605: Current learning rate: 0.00719
2024-12-20 07:36:04.218750: Validation loss did not improve from -0.49683. Patience: 22/50
2024-12-20 07:36:04.219767: train_loss -0.618
2024-12-20 07:36:04.220624: val_loss -0.4863
2024-12-20 07:36:04.221301: Pseudo dice [0.719]
2024-12-20 07:36:04.222105: Epoch time: 397.91 s
2024-12-20 07:36:04.222786: Yayy! New best EMA pseudo Dice: 0.7019
2024-12-20 07:36:06.177272: 
2024-12-20 07:36:06.178586: Epoch 47
2024-12-20 07:36:06.179274: Current learning rate: 0.00713
2024-12-20 07:42:34.489010: Validation loss did not improve from -0.49683. Patience: 23/50
2024-12-20 07:42:34.490548: train_loss -0.6335
2024-12-20 07:42:34.491429: val_loss -0.4877
2024-12-20 07:42:34.492158: Pseudo dice [0.7119]
2024-12-20 07:42:34.492812: Epoch time: 388.31 s
2024-12-20 07:42:34.493594: Yayy! New best EMA pseudo Dice: 0.7029
2024-12-20 07:42:36.296733: 
2024-12-20 07:42:36.299418: Epoch 48
2024-12-20 07:42:36.300463: Current learning rate: 0.00707
2024-12-20 07:49:14.869629: Validation loss improved from -0.49683 to -0.52125! Patience: 23/50
2024-12-20 07:49:14.870731: train_loss -0.6249
2024-12-20 07:49:14.871531: val_loss -0.5213
2024-12-20 07:49:14.872237: Pseudo dice [0.7313]
2024-12-20 07:49:14.873055: Epoch time: 398.58 s
2024-12-20 07:49:14.873810: Yayy! New best EMA pseudo Dice: 0.7057
2024-12-20 07:49:16.719918: 
2024-12-20 07:49:16.721523: Epoch 49
2024-12-20 07:49:16.722299: Current learning rate: 0.007
2024-12-20 07:56:01.437106: Validation loss did not improve from -0.52125. Patience: 1/50
2024-12-20 07:56:01.439628: train_loss -0.6302
2024-12-20 07:56:01.440658: val_loss -0.4931
2024-12-20 07:56:01.441320: Pseudo dice [0.7253]
2024-12-20 07:56:01.442147: Epoch time: 404.72 s
2024-12-20 07:56:01.825459: Yayy! New best EMA pseudo Dice: 0.7077
2024-12-20 07:56:03.559500: 
2024-12-20 07:56:03.560771: Epoch 50
2024-12-20 07:56:03.561505: Current learning rate: 0.00694
2024-12-20 08:02:19.324181: Validation loss did not improve from -0.52125. Patience: 2/50
2024-12-20 08:02:19.325249: train_loss -0.642
2024-12-20 08:02:19.326321: val_loss -0.5057
2024-12-20 08:02:19.327185: Pseudo dice [0.7255]
2024-12-20 08:02:19.328087: Epoch time: 375.77 s
2024-12-20 08:02:19.329039: Yayy! New best EMA pseudo Dice: 0.7095
2024-12-20 08:02:22.161997: 
2024-12-20 08:02:22.163297: Epoch 51
2024-12-20 08:02:22.164090: Current learning rate: 0.00688
2024-12-20 08:09:14.634344: Validation loss did not improve from -0.52125. Patience: 3/50
2024-12-20 08:09:14.635446: train_loss -0.6453
2024-12-20 08:09:14.636482: val_loss -0.4994
2024-12-20 08:09:14.637487: Pseudo dice [0.7228]
2024-12-20 08:09:14.638281: Epoch time: 412.47 s
2024-12-20 08:09:14.639094: Yayy! New best EMA pseudo Dice: 0.7108
2024-12-20 08:09:16.421289: 
2024-12-20 08:09:16.422386: Epoch 52
2024-12-20 08:09:16.423147: Current learning rate: 0.00682
2024-12-20 08:16:06.269189: Validation loss did not improve from -0.52125. Patience: 4/50
2024-12-20 08:16:06.270902: train_loss -0.6411
2024-12-20 08:16:06.271859: val_loss -0.505
2024-12-20 08:16:06.272792: Pseudo dice [0.7144]
2024-12-20 08:16:06.273458: Epoch time: 409.85 s
2024-12-20 08:16:06.274115: Yayy! New best EMA pseudo Dice: 0.7112
2024-12-20 08:16:07.956648: 
2024-12-20 08:16:07.957926: Epoch 53
2024-12-20 08:16:07.958719: Current learning rate: 0.00675
2024-12-20 08:23:02.807765: Validation loss did not improve from -0.52125. Patience: 5/50
2024-12-20 08:23:02.812193: train_loss -0.6445
2024-12-20 08:23:02.814025: val_loss -0.4754
2024-12-20 08:23:02.814870: Pseudo dice [0.7103]
2024-12-20 08:23:02.815916: Epoch time: 414.86 s
2024-12-20 08:23:04.189801: 
2024-12-20 08:23:04.191359: Epoch 54
2024-12-20 08:23:04.192344: Current learning rate: 0.00669
2024-12-20 08:29:35.313492: Validation loss did not improve from -0.52125. Patience: 6/50
2024-12-20 08:29:35.314523: train_loss -0.6412
2024-12-20 08:29:35.315294: val_loss -0.4795
2024-12-20 08:29:35.315979: Pseudo dice [0.7143]
2024-12-20 08:29:35.316752: Epoch time: 391.13 s
2024-12-20 08:29:35.739643: Yayy! New best EMA pseudo Dice: 0.7114
2024-12-20 08:29:37.524802: 
2024-12-20 08:29:37.526237: Epoch 55
2024-12-20 08:29:37.527097: Current learning rate: 0.00663
2024-12-20 08:36:15.133628: Validation loss did not improve from -0.52125. Patience: 7/50
2024-12-20 08:36:15.134551: train_loss -0.6503
2024-12-20 08:36:15.135379: val_loss -0.4628
2024-12-20 08:36:15.136016: Pseudo dice [0.7068]
2024-12-20 08:36:15.136684: Epoch time: 397.61 s
2024-12-20 08:36:16.571501: 
2024-12-20 08:36:16.572783: Epoch 56
2024-12-20 08:36:16.573617: Current learning rate: 0.00657
2024-12-20 08:42:52.522745: Validation loss did not improve from -0.52125. Patience: 8/50
2024-12-20 08:42:52.523692: train_loss -0.6497
2024-12-20 08:42:52.524448: val_loss -0.5114
2024-12-20 08:42:52.525235: Pseudo dice [0.7261]
2024-12-20 08:42:52.525954: Epoch time: 395.95 s
2024-12-20 08:42:52.526677: Yayy! New best EMA pseudo Dice: 0.7125
2024-12-20 08:42:54.260776: 
2024-12-20 08:42:54.262366: Epoch 57
2024-12-20 08:42:54.263172: Current learning rate: 0.0065
2024-12-20 08:49:44.857798: Validation loss did not improve from -0.52125. Patience: 9/50
2024-12-20 08:49:44.858780: train_loss -0.6539
2024-12-20 08:49:44.859624: val_loss -0.4753
2024-12-20 08:49:44.860466: Pseudo dice [0.7105]
2024-12-20 08:49:44.861214: Epoch time: 410.6 s
2024-12-20 08:49:46.276957: 
2024-12-20 08:49:46.278229: Epoch 58
2024-12-20 08:49:46.278999: Current learning rate: 0.00644
2024-12-20 08:56:17.384775: Validation loss did not improve from -0.52125. Patience: 10/50
2024-12-20 08:56:17.385913: train_loss -0.6584
2024-12-20 08:56:17.386729: val_loss -0.4883
2024-12-20 08:56:17.387448: Pseudo dice [0.7186]
2024-12-20 08:56:17.388206: Epoch time: 391.11 s
2024-12-20 08:56:17.388828: Yayy! New best EMA pseudo Dice: 0.7129
2024-12-20 08:56:19.177504: 
2024-12-20 08:56:19.179410: Epoch 59
2024-12-20 08:56:19.180211: Current learning rate: 0.00638
2024-12-20 09:02:34.941190: Validation loss did not improve from -0.52125. Patience: 11/50
2024-12-20 09:02:34.943206: train_loss -0.6476
2024-12-20 09:02:34.944384: val_loss -0.4674
2024-12-20 09:02:34.945133: Pseudo dice [0.6997]
2024-12-20 09:02:34.945941: Epoch time: 375.77 s
2024-12-20 09:02:36.736523: 
2024-12-20 09:02:36.737766: Epoch 60
2024-12-20 09:02:36.738604: Current learning rate: 0.00631
2024-12-20 09:09:24.162035: Validation loss did not improve from -0.52125. Patience: 12/50
2024-12-20 09:09:24.163235: train_loss -0.6515
2024-12-20 09:09:24.164229: val_loss -0.504
2024-12-20 09:09:24.164990: Pseudo dice [0.7195]
2024-12-20 09:09:24.165731: Epoch time: 407.43 s
2024-12-20 09:09:25.532804: 
2024-12-20 09:09:25.534241: Epoch 61
2024-12-20 09:09:25.535053: Current learning rate: 0.00625
2024-12-20 09:16:06.307584: Validation loss did not improve from -0.52125. Patience: 13/50
2024-12-20 09:16:06.308611: train_loss -0.659
2024-12-20 09:16:06.309404: val_loss -0.4431
2024-12-20 09:16:06.310079: Pseudo dice [0.6959]
2024-12-20 09:16:06.310817: Epoch time: 400.78 s
2024-12-20 09:16:08.167494: 
2024-12-20 09:16:08.168963: Epoch 62
2024-12-20 09:16:08.169709: Current learning rate: 0.00619
2024-12-20 09:22:57.348888: Validation loss did not improve from -0.52125. Patience: 14/50
2024-12-20 09:22:57.350631: train_loss -0.6557
2024-12-20 09:22:57.351525: val_loss -0.4891
2024-12-20 09:22:57.352343: Pseudo dice [0.7211]
2024-12-20 09:22:57.353225: Epoch time: 409.18 s
2024-12-20 09:22:58.709161: 
2024-12-20 09:22:58.710557: Epoch 63
2024-12-20 09:22:58.711657: Current learning rate: 0.00612
2024-12-20 09:29:23.855499: Validation loss did not improve from -0.52125. Patience: 15/50
2024-12-20 09:29:23.859824: train_loss -0.6588
2024-12-20 09:29:23.861858: val_loss -0.5004
2024-12-20 09:29:23.862876: Pseudo dice [0.7188]
2024-12-20 09:29:23.875978: Epoch time: 385.15 s
2024-12-20 09:29:25.278847: 
2024-12-20 09:29:25.280218: Epoch 64
2024-12-20 09:29:25.281054: Current learning rate: 0.00606
2024-12-20 09:35:46.155270: Validation loss did not improve from -0.52125. Patience: 16/50
2024-12-20 09:35:46.156173: train_loss -0.6654
2024-12-20 09:35:46.156932: val_loss -0.497
2024-12-20 09:35:46.157637: Pseudo dice [0.7203]
2024-12-20 09:35:46.158333: Epoch time: 380.88 s
2024-12-20 09:35:46.564483: Yayy! New best EMA pseudo Dice: 0.7132
2024-12-20 09:35:48.323159: 
2024-12-20 09:35:48.324484: Epoch 65
2024-12-20 09:35:48.325224: Current learning rate: 0.006
2024-12-20 09:42:21.403687: Validation loss did not improve from -0.52125. Patience: 17/50
2024-12-20 09:42:21.404739: train_loss -0.6618
2024-12-20 09:42:21.405603: val_loss -0.4478
2024-12-20 09:42:21.406406: Pseudo dice [0.7002]
2024-12-20 09:42:21.407206: Epoch time: 393.08 s
2024-12-20 09:42:22.818817: 
2024-12-20 09:42:22.820582: Epoch 66
2024-12-20 09:42:22.821832: Current learning rate: 0.00593
2024-12-20 09:49:17.251729: Validation loss did not improve from -0.52125. Patience: 18/50
2024-12-20 09:49:17.252933: train_loss -0.6666
2024-12-20 09:49:17.253785: val_loss -0.4718
2024-12-20 09:49:17.254539: Pseudo dice [0.7132]
2024-12-20 09:49:17.255365: Epoch time: 414.44 s
2024-12-20 09:49:18.669569: 
2024-12-20 09:49:18.671098: Epoch 67
2024-12-20 09:49:18.672612: Current learning rate: 0.00587
2024-12-20 09:55:56.489624: Validation loss did not improve from -0.52125. Patience: 19/50
2024-12-20 09:55:56.490714: train_loss -0.6725
2024-12-20 09:55:56.491543: val_loss -0.4864
2024-12-20 09:55:56.492259: Pseudo dice [0.6997]
2024-12-20 09:55:56.493061: Epoch time: 397.82 s
2024-12-20 09:55:57.929643: 
2024-12-20 09:55:57.930725: Epoch 68
2024-12-20 09:55:57.931728: Current learning rate: 0.00581
2024-12-20 10:02:26.855906: Validation loss did not improve from -0.52125. Patience: 20/50
2024-12-20 10:02:26.856922: train_loss -0.6691
2024-12-20 10:02:26.857715: val_loss -0.5127
2024-12-20 10:02:26.858375: Pseudo dice [0.7221]
2024-12-20 10:02:26.859051: Epoch time: 388.93 s
2024-12-20 10:02:28.316504: 
2024-12-20 10:02:28.317957: Epoch 69
2024-12-20 10:02:28.318731: Current learning rate: 0.00574
2024-12-20 10:09:12.000095: Validation loss did not improve from -0.52125. Patience: 21/50
2024-12-20 10:09:12.001026: train_loss -0.6727
2024-12-20 10:09:12.002058: val_loss -0.4692
2024-12-20 10:09:12.002824: Pseudo dice [0.7032]
2024-12-20 10:09:12.003586: Epoch time: 403.69 s
2024-12-20 10:09:13.830399: 
2024-12-20 10:09:13.832011: Epoch 70
2024-12-20 10:09:13.833035: Current learning rate: 0.00568
2024-12-20 10:16:10.378945: Validation loss did not improve from -0.52125. Patience: 22/50
2024-12-20 10:16:10.379883: train_loss -0.675
2024-12-20 10:16:10.380746: val_loss -0.504
2024-12-20 10:16:10.381619: Pseudo dice [0.7233]
2024-12-20 10:16:10.382394: Epoch time: 416.55 s
2024-12-20 10:16:11.833744: 
2024-12-20 10:16:11.835417: Epoch 71
2024-12-20 10:16:11.837052: Current learning rate: 0.00562
2024-12-20 10:23:00.205836: Validation loss did not improve from -0.52125. Patience: 23/50
2024-12-20 10:23:00.207719: train_loss -0.6779
2024-12-20 10:23:00.209070: val_loss -0.4505
2024-12-20 10:23:00.210100: Pseudo dice [0.7077]
2024-12-20 10:23:00.211050: Epoch time: 408.38 s
2024-12-20 10:23:01.555074: 
2024-12-20 10:23:01.556295: Epoch 72
2024-12-20 10:23:01.557156: Current learning rate: 0.00555
2024-12-20 10:29:24.360924: Validation loss did not improve from -0.52125. Patience: 24/50
2024-12-20 10:29:24.362637: train_loss -0.6782
2024-12-20 10:29:24.363849: val_loss -0.5013
2024-12-20 10:29:24.364539: Pseudo dice [0.7221]
2024-12-20 10:29:24.365225: Epoch time: 382.81 s
2024-12-20 10:29:27.878220: 
2024-12-20 10:29:27.879586: Epoch 73
2024-12-20 10:29:27.880348: Current learning rate: 0.00549
2024-12-20 10:36:00.796611: Validation loss did not improve from -0.52125. Patience: 25/50
2024-12-20 10:36:00.828671: train_loss -0.679
2024-12-20 10:36:00.830852: val_loss -0.4903
2024-12-20 10:36:00.831718: Pseudo dice [0.7249]
2024-12-20 10:36:00.832730: Epoch time: 392.95 s
2024-12-20 10:36:00.833678: Yayy! New best EMA pseudo Dice: 0.7141
2024-12-20 10:36:02.646994: 
2024-12-20 10:36:02.648230: Epoch 74
2024-12-20 10:36:02.649334: Current learning rate: 0.00542
2024-12-20 10:43:21.905892: Validation loss improved from -0.52125 to -0.52627! Patience: 25/50
2024-12-20 10:43:21.906743: train_loss -0.6808
2024-12-20 10:43:21.907546: val_loss -0.5263
2024-12-20 10:43:21.908437: Pseudo dice [0.7326]
2024-12-20 10:43:21.909188: Epoch time: 439.26 s
2024-12-20 10:43:22.306164: Yayy! New best EMA pseudo Dice: 0.7159
2024-12-20 10:43:24.060907: 
2024-12-20 10:43:24.062219: Epoch 75
2024-12-20 10:43:24.062890: Current learning rate: 0.00536
2024-12-20 10:50:45.210682: Validation loss did not improve from -0.52627. Patience: 1/50
2024-12-20 10:50:45.211685: train_loss -0.6794
2024-12-20 10:50:45.212680: val_loss -0.5011
2024-12-20 10:50:45.213546: Pseudo dice [0.7245]
2024-12-20 10:50:45.214288: Epoch time: 441.15 s
2024-12-20 10:50:45.214927: Yayy! New best EMA pseudo Dice: 0.7168
2024-12-20 10:50:46.987436: 
2024-12-20 10:50:46.988838: Epoch 76
2024-12-20 10:50:46.989651: Current learning rate: 0.00529
2024-12-20 10:57:12.147512: Validation loss did not improve from -0.52627. Patience: 2/50
2024-12-20 10:57:12.148556: train_loss -0.693
2024-12-20 10:57:12.149493: val_loss -0.5229
2024-12-20 10:57:12.150389: Pseudo dice [0.7338]
2024-12-20 10:57:12.151238: Epoch time: 385.16 s
2024-12-20 10:57:12.151956: Yayy! New best EMA pseudo Dice: 0.7185
2024-12-20 10:57:13.989975: 
2024-12-20 10:57:13.991447: Epoch 77
2024-12-20 10:57:13.992214: Current learning rate: 0.00523
2024-12-20 11:03:40.501786: Validation loss did not improve from -0.52627. Patience: 3/50
2024-12-20 11:03:40.502738: train_loss -0.6916
2024-12-20 11:03:40.503514: val_loss -0.5005
2024-12-20 11:03:40.504170: Pseudo dice [0.7191]
2024-12-20 11:03:40.504833: Epoch time: 386.51 s
2024-12-20 11:03:40.505483: Yayy! New best EMA pseudo Dice: 0.7185
2024-12-20 11:03:42.322086: 
2024-12-20 11:03:42.323353: Epoch 78
2024-12-20 11:03:42.324275: Current learning rate: 0.00517
2024-12-20 11:10:24.450190: Validation loss did not improve from -0.52627. Patience: 4/50
2024-12-20 11:10:24.451130: train_loss -0.6942
2024-12-20 11:10:24.452126: val_loss -0.4933
2024-12-20 11:10:24.452972: Pseudo dice [0.7263]
2024-12-20 11:10:24.453850: Epoch time: 402.13 s
2024-12-20 11:10:24.454703: Yayy! New best EMA pseudo Dice: 0.7193
2024-12-20 11:10:26.450471: 
2024-12-20 11:10:26.451880: Epoch 79
2024-12-20 11:10:26.452754: Current learning rate: 0.0051
2024-12-20 11:17:09.906915: Validation loss did not improve from -0.52627. Patience: 5/50
2024-12-20 11:17:09.907731: train_loss -0.6891
2024-12-20 11:17:09.908740: val_loss -0.5242
2024-12-20 11:17:09.909541: Pseudo dice [0.7338]
2024-12-20 11:17:09.910336: Epoch time: 403.46 s
2024-12-20 11:17:10.496269: Yayy! New best EMA pseudo Dice: 0.7208
2024-12-20 11:17:12.379408: 
2024-12-20 11:17:12.380954: Epoch 80
2024-12-20 11:17:12.381898: Current learning rate: 0.00504
2024-12-20 11:24:09.533629: Validation loss improved from -0.52627 to -0.52821! Patience: 5/50
2024-12-20 11:24:09.541794: train_loss -0.6895
2024-12-20 11:24:09.567290: val_loss -0.5282
2024-12-20 11:24:09.568696: Pseudo dice [0.7276]
2024-12-20 11:24:09.569549: Epoch time: 417.16 s
2024-12-20 11:24:09.570388: Yayy! New best EMA pseudo Dice: 0.7215
2024-12-20 11:24:11.691993: 
2024-12-20 11:24:11.693429: Epoch 81
2024-12-20 11:24:11.694189: Current learning rate: 0.00497
2024-12-20 11:30:45.882691: Validation loss improved from -0.52821 to -0.53127! Patience: 0/50
2024-12-20 11:30:45.884030: train_loss -0.6893
2024-12-20 11:30:45.884917: val_loss -0.5313
2024-12-20 11:30:45.885568: Pseudo dice [0.734]
2024-12-20 11:30:45.886220: Epoch time: 394.19 s
2024-12-20 11:30:45.886966: Yayy! New best EMA pseudo Dice: 0.7227
2024-12-20 11:30:47.708710: 
2024-12-20 11:30:47.710189: Epoch 82
2024-12-20 11:30:47.710986: Current learning rate: 0.00491
2024-12-20 11:37:54.571467: Validation loss did not improve from -0.53127. Patience: 1/50
2024-12-20 11:37:54.616848: train_loss -0.6926
2024-12-20 11:37:54.618800: val_loss -0.4551
2024-12-20 11:37:54.619794: Pseudo dice [0.6922]
2024-12-20 11:37:54.620984: Epoch time: 426.91 s
2024-12-20 11:37:56.724967: 
2024-12-20 11:37:56.726265: Epoch 83
2024-12-20 11:37:56.726947: Current learning rate: 0.00484
2024-12-20 11:44:46.713537: Validation loss did not improve from -0.53127. Patience: 2/50
2024-12-20 11:44:46.714603: train_loss -0.6958
2024-12-20 11:44:46.715525: val_loss -0.5005
2024-12-20 11:44:46.716408: Pseudo dice [0.7252]
2024-12-20 11:44:46.717396: Epoch time: 409.99 s
2024-12-20 11:44:48.038773: 
2024-12-20 11:44:48.040132: Epoch 84
2024-12-20 11:44:48.041091: Current learning rate: 0.00478
2024-12-20 11:51:14.782214: Validation loss did not improve from -0.53127. Patience: 3/50
2024-12-20 11:51:14.783347: train_loss -0.6942
2024-12-20 11:51:14.784429: val_loss -0.4916
2024-12-20 11:51:14.785365: Pseudo dice [0.7154]
2024-12-20 11:51:14.786236: Epoch time: 386.75 s
2024-12-20 11:51:16.573344: 
2024-12-20 11:51:16.574739: Epoch 85
2024-12-20 11:51:16.575798: Current learning rate: 0.00471
2024-12-20 11:58:00.552918: Validation loss did not improve from -0.53127. Patience: 4/50
2024-12-20 11:58:00.553937: train_loss -0.7015
2024-12-20 11:58:00.554853: val_loss -0.504
2024-12-20 11:58:00.555707: Pseudo dice [0.7236]
2024-12-20 11:58:00.556479: Epoch time: 403.98 s
2024-12-20 11:58:01.881426: 
2024-12-20 11:58:01.882727: Epoch 86
2024-12-20 11:58:01.883630: Current learning rate: 0.00465
2024-12-20 12:04:47.686379: Validation loss did not improve from -0.53127. Patience: 5/50
2024-12-20 12:04:47.687286: train_loss -0.7033
2024-12-20 12:04:47.688112: val_loss -0.5052
2024-12-20 12:04:47.688870: Pseudo dice [0.7227]
2024-12-20 12:04:47.689530: Epoch time: 405.81 s
2024-12-20 12:04:49.043405: 
2024-12-20 12:04:49.044731: Epoch 87
2024-12-20 12:04:49.045692: Current learning rate: 0.00458
2024-12-20 12:11:59.695261: Validation loss did not improve from -0.53127. Patience: 6/50
2024-12-20 12:11:59.696407: train_loss -0.7043
2024-12-20 12:11:59.697444: val_loss -0.5075
2024-12-20 12:11:59.698127: Pseudo dice [0.7203]
2024-12-20 12:11:59.698786: Epoch time: 430.65 s
2024-12-20 12:12:01.081683: 
2024-12-20 12:12:01.083118: Epoch 88
2024-12-20 12:12:01.083930: Current learning rate: 0.00452
2024-12-20 12:19:31.482566: Validation loss did not improve from -0.53127. Patience: 7/50
2024-12-20 12:19:31.483805: train_loss -0.7154
2024-12-20 12:19:31.484701: val_loss -0.5133
2024-12-20 12:19:31.485649: Pseudo dice [0.7317]
2024-12-20 12:19:31.486399: Epoch time: 450.4 s
2024-12-20 12:19:32.909580: 
2024-12-20 12:19:32.911000: Epoch 89
2024-12-20 12:19:32.911825: Current learning rate: 0.00445
2024-12-20 12:25:57.673871: Validation loss did not improve from -0.53127. Patience: 8/50
2024-12-20 12:25:57.675173: train_loss -0.7077
2024-12-20 12:25:57.676392: val_loss -0.5248
2024-12-20 12:25:57.677179: Pseudo dice [0.7261]
2024-12-20 12:25:57.677985: Epoch time: 384.77 s
2024-12-20 12:25:59.536541: 
2024-12-20 12:25:59.537816: Epoch 90
2024-12-20 12:25:59.538862: Current learning rate: 0.00438
2024-12-20 12:33:13.548172: Validation loss did not improve from -0.53127. Patience: 9/50
2024-12-20 12:33:13.549401: train_loss -0.7098
2024-12-20 12:33:13.550509: val_loss -0.4963
2024-12-20 12:33:13.551297: Pseudo dice [0.7141]
2024-12-20 12:33:13.552042: Epoch time: 434.01 s
2024-12-20 12:33:14.900180: 
2024-12-20 12:33:14.901671: Epoch 91
2024-12-20 12:33:14.902568: Current learning rate: 0.00432
2024-12-20 12:40:29.214494: Validation loss did not improve from -0.53127. Patience: 10/50
2024-12-20 12:40:29.218283: train_loss -0.7126
2024-12-20 12:40:29.220142: val_loss -0.4847
2024-12-20 12:40:29.220949: Pseudo dice [0.7143]
2024-12-20 12:40:29.221845: Epoch time: 434.32 s
2024-12-20 12:40:30.590173: 
2024-12-20 12:40:30.591337: Epoch 92
2024-12-20 12:40:30.592431: Current learning rate: 0.00425
2024-12-20 12:47:03.880383: Validation loss did not improve from -0.53127. Patience: 11/50
2024-12-20 12:47:03.881957: train_loss -0.7082
2024-12-20 12:47:03.882761: val_loss -0.4881
2024-12-20 12:47:03.883469: Pseudo dice [0.7122]
2024-12-20 12:47:03.884399: Epoch time: 393.29 s
2024-12-20 12:47:05.233866: 
2024-12-20 12:47:05.235076: Epoch 93
2024-12-20 12:47:05.235971: Current learning rate: 0.00419
2024-12-20 12:54:03.474848: Validation loss did not improve from -0.53127. Patience: 12/50
2024-12-20 12:54:03.475925: train_loss -0.7132
2024-12-20 12:54:03.476713: val_loss -0.4905
2024-12-20 12:54:03.477713: Pseudo dice [0.7232]
2024-12-20 12:54:03.478554: Epoch time: 418.24 s
2024-12-20 12:54:04.809924: 
2024-12-20 12:54:04.811148: Epoch 94
2024-12-20 12:54:04.812099: Current learning rate: 0.00412
2024-12-20 13:01:30.137814: Validation loss did not improve from -0.53127. Patience: 13/50
2024-12-20 13:01:30.138937: train_loss -0.7115
2024-12-20 13:01:30.140005: val_loss -0.4794
2024-12-20 13:01:30.141044: Pseudo dice [0.707]
2024-12-20 13:01:30.142102: Epoch time: 445.33 s
2024-12-20 13:01:34.442986: 
2024-12-20 13:01:34.444569: Epoch 95
2024-12-20 13:01:34.445591: Current learning rate: 0.00405
2024-12-20 13:09:40.288171: Validation loss did not improve from -0.53127. Patience: 14/50
2024-12-20 13:09:40.289263: train_loss -0.7143
2024-12-20 13:09:40.290133: val_loss -0.5022
2024-12-20 13:09:40.290814: Pseudo dice [0.7257]
2024-12-20 13:09:40.291585: Epoch time: 485.85 s
2024-12-20 13:09:41.642453: 
2024-12-20 13:09:41.643915: Epoch 96
2024-12-20 13:09:41.644911: Current learning rate: 0.00399
2024-12-20 13:16:22.960474: Validation loss did not improve from -0.53127. Patience: 15/50
2024-12-20 13:16:22.961485: train_loss -0.7154
2024-12-20 13:16:22.962434: val_loss -0.5197
2024-12-20 13:16:22.963225: Pseudo dice [0.7331]
2024-12-20 13:16:22.964001: Epoch time: 401.32 s
2024-12-20 13:16:24.379101: 
2024-12-20 13:16:24.380406: Epoch 97
2024-12-20 13:16:24.381173: Current learning rate: 0.00392
2024-12-20 13:22:52.044150: Validation loss did not improve from -0.53127. Patience: 16/50
2024-12-20 13:22:52.045212: train_loss -0.7183
2024-12-20 13:22:52.046082: val_loss -0.4841
2024-12-20 13:22:52.046813: Pseudo dice [0.7246]
2024-12-20 13:22:52.047526: Epoch time: 387.67 s
2024-12-20 13:22:53.429620: 
2024-12-20 13:22:53.430894: Epoch 98
2024-12-20 13:22:53.431575: Current learning rate: 0.00385
2024-12-20 13:29:26.112159: Validation loss did not improve from -0.53127. Patience: 17/50
2024-12-20 13:29:26.113362: train_loss -0.7227
2024-12-20 13:29:26.114693: val_loss -0.4804
2024-12-20 13:29:26.115558: Pseudo dice [0.7153]
2024-12-20 13:29:26.116426: Epoch time: 392.68 s
2024-12-20 13:29:27.535808: 
2024-12-20 13:29:27.537180: Epoch 99
2024-12-20 13:29:27.538172: Current learning rate: 0.00379
2024-12-20 13:36:22.223776: Validation loss did not improve from -0.53127. Patience: 18/50
2024-12-20 13:36:22.224706: train_loss -0.7191
2024-12-20 13:36:22.225820: val_loss -0.5053
2024-12-20 13:36:22.226593: Pseudo dice [0.7284]
2024-12-20 13:36:22.227302: Epoch time: 414.69 s
2024-12-20 13:36:24.123924: 
2024-12-20 13:36:24.125180: Epoch 100
2024-12-20 13:36:24.125878: Current learning rate: 0.00372
2024-12-20 13:44:04.006190: Validation loss did not improve from -0.53127. Patience: 19/50
2024-12-20 13:44:04.007651: train_loss -0.7219
2024-12-20 13:44:04.009079: val_loss -0.4781
2024-12-20 13:44:04.009892: Pseudo dice [0.7157]
2024-12-20 13:44:04.010530: Epoch time: 459.88 s
2024-12-20 13:44:05.655572: 
2024-12-20 13:44:05.656988: Epoch 101
2024-12-20 13:44:05.657706: Current learning rate: 0.00365
2024-12-20 13:50:43.222704: Validation loss did not improve from -0.53127. Patience: 20/50
2024-12-20 13:50:43.256624: train_loss -0.7222
2024-12-20 13:50:43.258527: val_loss -0.5066
2024-12-20 13:50:43.259422: Pseudo dice [0.7208]
2024-12-20 13:50:43.260545: Epoch time: 397.6 s
2024-12-20 13:50:44.687594: 
2024-12-20 13:50:44.689023: Epoch 102
2024-12-20 13:50:44.689908: Current learning rate: 0.00359
2024-12-20 13:57:17.893981: Validation loss did not improve from -0.53127. Patience: 21/50
2024-12-20 13:57:17.895001: train_loss -0.7218
2024-12-20 13:57:17.896066: val_loss -0.5134
2024-12-20 13:57:17.896953: Pseudo dice [0.7356]
2024-12-20 13:57:17.897922: Epoch time: 393.21 s
2024-12-20 13:57:19.265546: 
2024-12-20 13:57:19.267020: Epoch 103
2024-12-20 13:57:19.267829: Current learning rate: 0.00352
2024-12-20 14:03:55.544200: Validation loss did not improve from -0.53127. Patience: 22/50
2024-12-20 14:03:55.545351: train_loss -0.7192
2024-12-20 14:03:55.546394: val_loss -0.4982
2024-12-20 14:03:55.547456: Pseudo dice [0.7205]
2024-12-20 14:03:55.548462: Epoch time: 396.28 s
2024-12-20 14:03:56.888834: 
2024-12-20 14:03:56.890122: Epoch 104
2024-12-20 14:03:56.891108: Current learning rate: 0.00345
2024-12-20 14:09:57.944235: Validation loss did not improve from -0.53127. Patience: 23/50
2024-12-20 14:09:57.945853: train_loss -0.72
2024-12-20 14:09:57.946998: val_loss -0.4715
2024-12-20 14:09:57.948003: Pseudo dice [0.7044]
2024-12-20 14:09:57.948955: Epoch time: 361.06 s
2024-12-20 14:10:00.287076: 
2024-12-20 14:10:00.288853: Epoch 105
2024-12-20 14:10:00.290215: Current learning rate: 0.00338
2024-12-20 14:15:58.671486: Validation loss did not improve from -0.53127. Patience: 24/50
2024-12-20 14:15:58.673208: train_loss -0.7224
2024-12-20 14:15:58.674221: val_loss -0.4944
2024-12-20 14:15:58.674907: Pseudo dice [0.7233]
2024-12-20 14:15:58.675612: Epoch time: 358.39 s
2024-12-20 14:16:00.003194: 
2024-12-20 14:16:00.004745: Epoch 106
2024-12-20 14:16:00.005808: Current learning rate: 0.00332
2024-12-20 14:21:52.734174: Validation loss did not improve from -0.53127. Patience: 25/50
2024-12-20 14:21:52.735265: train_loss -0.7236
2024-12-20 14:21:52.736008: val_loss -0.5197
2024-12-20 14:21:52.736733: Pseudo dice [0.7292]
2024-12-20 14:21:52.737382: Epoch time: 352.73 s
2024-12-20 14:21:54.093680: 
2024-12-20 14:21:54.095637: Epoch 107
2024-12-20 14:21:54.096513: Current learning rate: 0.00325
2024-12-20 14:27:35.508123: Validation loss did not improve from -0.53127. Patience: 26/50
2024-12-20 14:27:35.509309: train_loss -0.7259
2024-12-20 14:27:35.510077: val_loss -0.5261
2024-12-20 14:27:35.510920: Pseudo dice [0.735]
2024-12-20 14:27:35.511837: Epoch time: 341.42 s
2024-12-20 14:27:35.512810: Yayy! New best EMA pseudo Dice: 0.7228
2024-12-20 14:27:37.329974: 
2024-12-20 14:27:37.331636: Epoch 108
2024-12-20 14:27:37.332819: Current learning rate: 0.00318
2024-12-20 14:33:20.648365: Validation loss did not improve from -0.53127. Patience: 27/50
2024-12-20 14:33:20.649282: train_loss -0.7253
2024-12-20 14:33:20.650151: val_loss -0.5218
2024-12-20 14:33:20.650998: Pseudo dice [0.7328]
2024-12-20 14:33:20.651859: Epoch time: 343.32 s
2024-12-20 14:33:20.652701: Yayy! New best EMA pseudo Dice: 0.7238
2024-12-20 14:33:22.439111: 
2024-12-20 14:33:22.439994: Epoch 109
2024-12-20 14:33:22.440671: Current learning rate: 0.00311
2024-12-20 14:38:56.847270: Validation loss did not improve from -0.53127. Patience: 28/50
2024-12-20 14:38:56.848306: train_loss -0.7292
2024-12-20 14:38:56.849224: val_loss -0.5149
2024-12-20 14:38:56.850022: Pseudo dice [0.7263]
2024-12-20 14:38:56.850808: Epoch time: 334.41 s
2024-12-20 14:38:57.205361: Yayy! New best EMA pseudo Dice: 0.7241
2024-12-20 14:38:59.128703: 
2024-12-20 14:38:59.130255: Epoch 110
2024-12-20 14:38:59.131197: Current learning rate: 0.00304
2024-12-20 14:44:44.826525: Validation loss did not improve from -0.53127. Patience: 29/50
2024-12-20 14:44:44.827662: train_loss -0.7303
2024-12-20 14:44:44.828645: val_loss -0.4929
2024-12-20 14:44:44.829516: Pseudo dice [0.7202]
2024-12-20 14:44:44.830271: Epoch time: 345.7 s
2024-12-20 14:44:46.226291: 
2024-12-20 14:44:46.227274: Epoch 111
2024-12-20 14:44:46.228007: Current learning rate: 0.00297
2024-12-20 14:50:06.081587: Validation loss did not improve from -0.53127. Patience: 30/50
2024-12-20 14:50:06.082643: train_loss -0.7329
2024-12-20 14:50:06.083442: val_loss -0.5164
2024-12-20 14:50:06.084299: Pseudo dice [0.7375]
2024-12-20 14:50:06.085130: Epoch time: 319.86 s
2024-12-20 14:50:06.085901: Yayy! New best EMA pseudo Dice: 0.7251
2024-12-20 14:50:07.870124: 
2024-12-20 14:50:07.871321: Epoch 112
2024-12-20 14:50:07.872101: Current learning rate: 0.00291
2024-12-20 14:55:36.806808: Validation loss did not improve from -0.53127. Patience: 31/50
2024-12-20 14:55:36.811624: train_loss -0.7308
2024-12-20 14:55:36.813742: val_loss -0.5253
2024-12-20 14:55:36.814685: Pseudo dice [0.731]
2024-12-20 14:55:36.815776: Epoch time: 328.94 s
2024-12-20 14:55:36.816611: Yayy! New best EMA pseudo Dice: 0.7257
2024-12-20 14:55:38.600238: 
2024-12-20 14:55:38.601619: Epoch 113
2024-12-20 14:55:38.602477: Current learning rate: 0.00284
2024-12-20 15:01:16.216174: Validation loss did not improve from -0.53127. Patience: 32/50
2024-12-20 15:01:16.217112: train_loss -0.7359
2024-12-20 15:01:16.218302: val_loss -0.4763
2024-12-20 15:01:16.219340: Pseudo dice [0.7004]
2024-12-20 15:01:16.220368: Epoch time: 337.62 s
2024-12-20 15:01:17.625501: 
2024-12-20 15:01:17.627056: Epoch 114
2024-12-20 15:01:17.627924: Current learning rate: 0.00277
2024-12-20 15:06:43.694100: Validation loss did not improve from -0.53127. Patience: 33/50
2024-12-20 15:06:43.695184: train_loss -0.7312
2024-12-20 15:06:43.696220: val_loss -0.5044
2024-12-20 15:06:43.697177: Pseudo dice [0.7261]
2024-12-20 15:06:43.698127: Epoch time: 326.07 s
2024-12-20 15:06:45.513425: 
2024-12-20 15:06:45.514743: Epoch 115
2024-12-20 15:06:45.515711: Current learning rate: 0.0027
2024-12-20 15:12:08.620428: Validation loss did not improve from -0.53127. Patience: 34/50
2024-12-20 15:12:08.621358: train_loss -0.7388
2024-12-20 15:12:08.622177: val_loss -0.4836
2024-12-20 15:12:08.622897: Pseudo dice [0.7182]
2024-12-20 15:12:08.623734: Epoch time: 323.11 s
2024-12-20 15:12:10.597977: 
2024-12-20 15:12:10.599175: Epoch 116
2024-12-20 15:12:10.599921: Current learning rate: 0.00263
2024-12-20 15:17:35.660607: Validation loss did not improve from -0.53127. Patience: 35/50
2024-12-20 15:17:35.662012: train_loss -0.7347
2024-12-20 15:17:35.663355: val_loss -0.5215
2024-12-20 15:17:35.664082: Pseudo dice [0.7308]
2024-12-20 15:17:35.664880: Epoch time: 325.06 s
2024-12-20 15:17:37.090886: 
2024-12-20 15:17:37.092416: Epoch 117
2024-12-20 15:17:37.093389: Current learning rate: 0.00256
2024-12-20 15:23:18.399592: Validation loss did not improve from -0.53127. Patience: 36/50
2024-12-20 15:23:18.400616: train_loss -0.7383
2024-12-20 15:23:18.401439: val_loss -0.4807
2024-12-20 15:23:18.402203: Pseudo dice [0.7124]
2024-12-20 15:23:18.402954: Epoch time: 341.31 s
2024-12-20 15:23:19.778324: 
2024-12-20 15:23:19.779804: Epoch 118
2024-12-20 15:23:19.780689: Current learning rate: 0.00249
2024-12-20 15:29:07.209616: Validation loss did not improve from -0.53127. Patience: 37/50
2024-12-20 15:29:07.210698: train_loss -0.7476
2024-12-20 15:29:07.211484: val_loss -0.4851
2024-12-20 15:29:07.212183: Pseudo dice [0.7216]
2024-12-20 15:29:07.212894: Epoch time: 347.43 s
2024-12-20 15:29:08.637526: 
2024-12-20 15:29:08.639631: Epoch 119
2024-12-20 15:29:08.640859: Current learning rate: 0.00242
2024-12-20 15:34:38.117064: Validation loss did not improve from -0.53127. Patience: 38/50
2024-12-20 15:34:38.118293: train_loss -0.7408
2024-12-20 15:34:38.119094: val_loss -0.501
2024-12-20 15:34:38.119754: Pseudo dice [0.7252]
2024-12-20 15:34:38.120443: Epoch time: 329.48 s
2024-12-20 15:34:39.988811: 
2024-12-20 15:34:39.990289: Epoch 120
2024-12-20 15:34:39.991092: Current learning rate: 0.00235
2024-12-20 15:40:02.449479: Validation loss did not improve from -0.53127. Patience: 39/50
2024-12-20 15:40:02.450500: train_loss -0.7434
2024-12-20 15:40:02.451240: val_loss -0.4839
2024-12-20 15:40:02.452129: Pseudo dice [0.7163]
2024-12-20 15:40:02.452804: Epoch time: 322.46 s
2024-12-20 15:40:03.853301: 
2024-12-20 15:40:03.854434: Epoch 121
2024-12-20 15:40:03.855258: Current learning rate: 0.00228
2024-12-20 15:45:28.643647: Validation loss did not improve from -0.53127. Patience: 40/50
2024-12-20 15:45:28.644630: train_loss -0.7418
2024-12-20 15:45:28.645429: val_loss -0.4975
2024-12-20 15:45:28.646145: Pseudo dice [0.7359]
2024-12-20 15:45:28.646763: Epoch time: 324.79 s
2024-12-20 15:45:30.042725: 
2024-12-20 15:45:30.044079: Epoch 122
2024-12-20 15:45:30.044832: Current learning rate: 0.00221
2024-12-20 15:51:08.408507: Validation loss did not improve from -0.53127. Patience: 41/50
2024-12-20 15:51:08.410173: train_loss -0.7402
2024-12-20 15:51:08.411279: val_loss -0.4739
2024-12-20 15:51:08.412221: Pseudo dice [0.7061]
2024-12-20 15:51:08.413197: Epoch time: 338.37 s
2024-12-20 15:51:09.896654: 
2024-12-20 15:51:09.898065: Epoch 123
2024-12-20 15:51:09.898881: Current learning rate: 0.00214
2024-12-20 15:56:42.779358: Validation loss did not improve from -0.53127. Patience: 42/50
2024-12-20 15:56:42.783980: train_loss -0.7486
2024-12-20 15:56:42.785996: val_loss -0.5158
2024-12-20 15:56:42.787039: Pseudo dice [0.7222]
2024-12-20 15:56:42.788353: Epoch time: 332.89 s
2024-12-20 15:56:44.267749: 
2024-12-20 15:56:44.269210: Epoch 124
2024-12-20 15:56:44.270152: Current learning rate: 0.00207
2024-12-20 16:02:14.606522: Validation loss did not improve from -0.53127. Patience: 43/50
2024-12-20 16:02:14.607561: train_loss -0.7462
2024-12-20 16:02:14.608528: val_loss -0.5055
2024-12-20 16:02:14.609282: Pseudo dice [0.7233]
2024-12-20 16:02:14.610043: Epoch time: 330.34 s
2024-12-20 16:02:16.453774: 
2024-12-20 16:02:16.455192: Epoch 125
2024-12-20 16:02:16.456123: Current learning rate: 0.00199
2024-12-20 16:07:03.308324: Validation loss did not improve from -0.53127. Patience: 44/50
2024-12-20 16:07:03.309380: train_loss -0.7489
2024-12-20 16:07:03.310193: val_loss -0.5035
2024-12-20 16:07:03.310839: Pseudo dice [0.727]
2024-12-20 16:07:03.311545: Epoch time: 286.86 s
2024-12-20 16:07:04.740013: 
2024-12-20 16:07:04.741175: Epoch 126
2024-12-20 16:07:04.741914: Current learning rate: 0.00192
2024-12-20 16:11:44.176906: Validation loss did not improve from -0.53127. Patience: 45/50
2024-12-20 16:11:44.177947: train_loss -0.7465
2024-12-20 16:11:44.178719: val_loss -0.5044
2024-12-20 16:11:44.179526: Pseudo dice [0.7252]
2024-12-20 16:11:44.180287: Epoch time: 279.44 s
2024-12-20 16:11:46.248985: 
2024-12-20 16:11:46.250338: Epoch 127
2024-12-20 16:11:46.251008: Current learning rate: 0.00185
2024-12-20 16:16:25.278928: Validation loss did not improve from -0.53127. Patience: 46/50
2024-12-20 16:16:25.279831: train_loss -0.7496
2024-12-20 16:16:25.280731: val_loss -0.4878
2024-12-20 16:16:25.281352: Pseudo dice [0.7132]
2024-12-20 16:16:25.282001: Epoch time: 279.03 s
2024-12-20 16:16:26.673097: 
2024-12-20 16:16:26.674376: Epoch 128
2024-12-20 16:16:26.675206: Current learning rate: 0.00178
2024-12-20 16:21:07.565382: Validation loss did not improve from -0.53127. Patience: 47/50
2024-12-20 16:21:07.566330: train_loss -0.7509
2024-12-20 16:21:07.567209: val_loss -0.5074
2024-12-20 16:21:07.567974: Pseudo dice [0.7288]
2024-12-20 16:21:07.568797: Epoch time: 280.89 s
2024-12-20 16:21:08.963233: 
2024-12-20 16:21:08.964424: Epoch 129
2024-12-20 16:21:08.965219: Current learning rate: 0.0017
2024-12-20 16:25:59.136889: Validation loss did not improve from -0.53127. Patience: 48/50
2024-12-20 16:25:59.137880: train_loss -0.7531
2024-12-20 16:25:59.138633: val_loss -0.5045
2024-12-20 16:25:59.139334: Pseudo dice [0.7288]
2024-12-20 16:25:59.140290: Epoch time: 290.18 s
2024-12-20 16:26:00.937112: 
2024-12-20 16:26:00.938228: Epoch 130
2024-12-20 16:26:00.939122: Current learning rate: 0.00163
2024-12-20 16:29:42.141113: Validation loss did not improve from -0.53127. Patience: 49/50
2024-12-20 16:29:42.142057: train_loss -0.7481
2024-12-20 16:29:42.142838: val_loss -0.4823
2024-12-20 16:29:42.143745: Pseudo dice [0.7164]
2024-12-20 16:29:42.144418: Epoch time: 221.21 s
2024-12-20 16:29:43.498592: 
2024-12-20 16:29:43.501153: Epoch 131
2024-12-20 16:29:43.502464: Current learning rate: 0.00156
2024-12-20 16:33:36.722311: Validation loss did not improve from -0.53127. Patience: 50/50
2024-12-20 16:33:36.723305: train_loss -0.7539
2024-12-20 16:33:36.723946: val_loss -0.5007
2024-12-20 16:33:36.724720: Pseudo dice [0.7264]
2024-12-20 16:33:36.725384: Epoch time: 233.23 s
2024-12-20 16:33:38.106342: 
2024-12-20 16:33:38.107544: Epoch 132
2024-12-20 16:33:38.108203: Current learning rate: 0.00148
2024-12-20 16:38:02.010526: Validation loss did not improve from -0.53127. Patience: 51/50
2024-12-20 16:38:02.012656: train_loss -0.7507
2024-12-20 16:38:02.013729: val_loss -0.5146
2024-12-20 16:38:02.014483: Pseudo dice [0.7301]
2024-12-20 16:38:02.015087: Epoch time: 263.91 s
2024-12-20 16:38:03.426400: 
2024-12-20 16:38:03.427658: Epoch 133
2024-12-20 16:38:03.428513: Current learning rate: 0.00141
2024-12-20 16:43:27.098578: Validation loss improved from -0.53127 to -0.53532! Patience: 51/50
2024-12-20 16:43:27.099648: train_loss -0.7515
2024-12-20 16:43:27.100638: val_loss -0.5353
2024-12-20 16:43:27.101344: Pseudo dice [0.7393]
2024-12-20 16:43:27.102103: Epoch time: 323.67 s
2024-12-20 16:43:28.496755: 
2024-12-20 16:43:28.498783: Epoch 134
2024-12-20 16:43:28.499713: Current learning rate: 0.00133
2024-12-20 16:47:57.039032: Validation loss did not improve from -0.53532. Patience: 1/50
2024-12-20 16:47:57.040333: train_loss -0.7558
2024-12-20 16:47:57.041322: val_loss -0.4995
2024-12-20 16:47:57.042017: Pseudo dice [0.7257]
2024-12-20 16:47:57.042775: Epoch time: 268.55 s
2024-12-20 16:47:58.917278: 
2024-12-20 16:47:58.918621: Epoch 135
2024-12-20 16:47:58.919327: Current learning rate: 0.00126
2024-12-20 16:52:56.382750: Validation loss did not improve from -0.53532. Patience: 2/50
2024-12-20 16:52:56.383755: train_loss -0.7568
2024-12-20 16:52:56.384974: val_loss -0.5169
2024-12-20 16:52:56.386051: Pseudo dice [0.7313]
2024-12-20 16:52:56.387033: Epoch time: 297.47 s
2024-12-20 16:52:56.388353: Yayy! New best EMA pseudo Dice: 0.7258
2024-12-20 16:52:58.227686: 
2024-12-20 16:52:58.229203: Epoch 136
2024-12-20 16:52:58.230389: Current learning rate: 0.00118
2024-12-20 16:57:22.127654: Validation loss did not improve from -0.53532. Patience: 3/50
2024-12-20 16:57:22.131225: train_loss -0.7592
2024-12-20 16:57:22.132566: val_loss -0.5141
2024-12-20 16:57:22.133273: Pseudo dice [0.7306]
2024-12-20 16:57:22.133881: Epoch time: 263.9 s
2024-12-20 16:57:22.134609: Yayy! New best EMA pseudo Dice: 0.7263
2024-12-20 16:57:24.001316: 
2024-12-20 16:57:24.002619: Epoch 137
2024-12-20 16:57:24.003417: Current learning rate: 0.00111
2024-12-20 17:01:42.676211: Validation loss did not improve from -0.53532. Patience: 4/50
2024-12-20 17:01:42.681191: train_loss -0.7571
2024-12-20 17:01:42.683419: val_loss -0.5007
2024-12-20 17:01:42.684438: Pseudo dice [0.731]
2024-12-20 17:01:42.685625: Epoch time: 258.68 s
2024-12-20 17:01:42.686551: Yayy! New best EMA pseudo Dice: 0.7268
2024-12-20 17:01:45.063681: 
2024-12-20 17:01:45.065228: Epoch 138
2024-12-20 17:01:45.066157: Current learning rate: 0.00103
2024-12-20 17:06:36.377872: Validation loss did not improve from -0.53532. Patience: 5/50
2024-12-20 17:06:36.378974: train_loss -0.7584
2024-12-20 17:06:36.380007: val_loss -0.5154
2024-12-20 17:06:36.380783: Pseudo dice [0.7378]
2024-12-20 17:06:36.381586: Epoch time: 291.32 s
2024-12-20 17:06:36.382342: Yayy! New best EMA pseudo Dice: 0.7279
2024-12-20 17:06:38.159714: 
2024-12-20 17:06:38.160897: Epoch 139
2024-12-20 17:06:38.161605: Current learning rate: 0.00095
2024-12-20 17:10:44.172865: Validation loss did not improve from -0.53532. Patience: 6/50
2024-12-20 17:10:44.173924: train_loss -0.7586
2024-12-20 17:10:44.174737: val_loss -0.503
2024-12-20 17:10:44.175479: Pseudo dice [0.7256]
2024-12-20 17:10:44.176133: Epoch time: 246.02 s
2024-12-20 17:10:45.949052: 
2024-12-20 17:10:45.950325: Epoch 140
2024-12-20 17:10:45.951234: Current learning rate: 0.00087
2024-12-20 17:15:38.695070: Validation loss did not improve from -0.53532. Patience: 7/50
2024-12-20 17:15:38.695822: train_loss -0.7609
2024-12-20 17:15:38.696796: val_loss -0.514
2024-12-20 17:15:38.697644: Pseudo dice [0.7358]
2024-12-20 17:15:38.698421: Epoch time: 292.75 s
2024-12-20 17:15:38.699236: Yayy! New best EMA pseudo Dice: 0.7285
2024-12-20 17:15:40.530533: 
2024-12-20 17:15:40.532055: Epoch 141
2024-12-20 17:15:40.532945: Current learning rate: 0.00079
2024-12-20 17:18:28.816422: Validation loss did not improve from -0.53532. Patience: 8/50
2024-12-20 17:18:28.817386: train_loss -0.7563
2024-12-20 17:18:28.818326: val_loss -0.499
2024-12-20 17:18:28.819140: Pseudo dice [0.7208]
2024-12-20 17:18:28.819932: Epoch time: 168.29 s
2024-12-20 17:18:30.308952: 
2024-12-20 17:18:30.311841: Epoch 142
2024-12-20 17:18:30.313178: Current learning rate: 0.00071
2024-12-20 17:20:38.651510: Validation loss did not improve from -0.53532. Patience: 9/50
2024-12-20 17:20:38.652777: train_loss -0.7576
2024-12-20 17:20:38.653653: val_loss -0.5109
2024-12-20 17:20:38.654460: Pseudo dice [0.7346]
2024-12-20 17:20:38.655185: Epoch time: 128.35 s
2024-12-20 17:20:40.097059: 
2024-12-20 17:20:40.098690: Epoch 143
2024-12-20 17:20:40.099602: Current learning rate: 0.00063
2024-12-20 17:23:13.367626: Validation loss did not improve from -0.53532. Patience: 10/50
2024-12-20 17:23:13.369300: train_loss -0.7615
2024-12-20 17:23:13.370244: val_loss -0.5192
2024-12-20 17:23:13.371108: Pseudo dice [0.7416]
2024-12-20 17:23:13.371822: Epoch time: 153.27 s
2024-12-20 17:23:13.372612: Yayy! New best EMA pseudo Dice: 0.7297
2024-12-20 17:23:15.248716: 
2024-12-20 17:23:15.250156: Epoch 144
2024-12-20 17:23:15.251056: Current learning rate: 0.00055
2024-12-20 17:25:57.954504: Validation loss did not improve from -0.53532. Patience: 11/50
2024-12-20 17:25:57.955748: train_loss -0.7608
2024-12-20 17:25:57.957078: val_loss -0.5002
2024-12-20 17:25:57.957878: Pseudo dice [0.7292]
2024-12-20 17:25:57.958712: Epoch time: 162.71 s
2024-12-20 17:25:59.776764: 
2024-12-20 17:25:59.778254: Epoch 145
2024-12-20 17:25:59.779409: Current learning rate: 0.00047
2024-12-20 17:28:37.929554: Validation loss did not improve from -0.53532. Patience: 12/50
2024-12-20 17:28:37.930752: train_loss -0.7581
2024-12-20 17:28:37.931993: val_loss -0.4977
2024-12-20 17:28:37.933078: Pseudo dice [0.7272]
2024-12-20 17:28:37.933959: Epoch time: 158.15 s
2024-12-20 17:28:39.391685: 
2024-12-20 17:28:39.392929: Epoch 146
2024-12-20 17:28:39.393941: Current learning rate: 0.00038
2024-12-20 17:33:10.162974: Validation loss did not improve from -0.53532. Patience: 13/50
2024-12-20 17:33:10.164337: train_loss -0.7646
2024-12-20 17:33:10.165399: val_loss -0.5121
2024-12-20 17:33:10.166522: Pseudo dice [0.7317]
2024-12-20 17:33:10.167469: Epoch time: 270.77 s
2024-12-20 17:33:11.582607: 
2024-12-20 17:33:11.584065: Epoch 147
2024-12-20 17:33:11.585113: Current learning rate: 0.0003
2024-12-20 17:38:01.413621: Validation loss did not improve from -0.53532. Patience: 14/50
2024-12-20 17:38:01.414614: train_loss -0.7621
2024-12-20 17:38:01.415380: val_loss -0.5093
2024-12-20 17:38:01.416172: Pseudo dice [0.73]
2024-12-20 17:38:01.416861: Epoch time: 289.83 s
2024-12-20 17:38:03.438742: 
2024-12-20 17:38:03.440315: Epoch 148
2024-12-20 17:38:03.441407: Current learning rate: 0.00021
2024-12-20 17:43:38.363515: Validation loss did not improve from -0.53532. Patience: 15/50
2024-12-20 17:43:38.365254: train_loss -0.765
2024-12-20 17:43:38.366395: val_loss -0.4906
2024-12-20 17:43:38.367462: Pseudo dice [0.7296]
2024-12-20 17:43:38.368288: Epoch time: 334.93 s
2024-12-20 17:43:39.819458: 
2024-12-20 17:43:39.820469: Epoch 149
2024-12-20 17:43:39.821218: Current learning rate: 0.00011
2024-12-20 17:49:15.059488: Validation loss did not improve from -0.53532. Patience: 16/50
2024-12-20 17:49:15.060184: train_loss -0.7638
2024-12-20 17:49:15.060980: val_loss -0.4942
2024-12-20 17:49:15.061767: Pseudo dice [0.7256]
2024-12-20 17:49:15.062615: Epoch time: 335.24 s
2024-12-20 17:49:16.943865: Training done.
2024-12-20 17:49:17.209362: Using splits from existing split file: /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset308_Sohee_Calcium_OCT_CrossValidation/splits_final.json
2024-12-20 17:49:17.235949: The split file contains 5 splits.
2024-12-20 17:49:17.237546: Desired fold for training: 4
2024-12-20 17:49:17.238847: This split has 7 training and 1 validation cases.
2024-12-20 17:49:17.240493: predicting 101-045
2024-12-20 17:49:17.287119: 101-045, shape torch.Size([1, 375, 498, 498]), rank 0
2024-12-20 17:52:13.033756: Validation complete
2024-12-20 17:52:13.034588: Mean Validation Dice:  0.7222300772681763
