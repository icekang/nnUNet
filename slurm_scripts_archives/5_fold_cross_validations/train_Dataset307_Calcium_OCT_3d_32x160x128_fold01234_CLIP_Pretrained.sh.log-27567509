
CommandNotFoundError: Your shell has not been properly configured to use 'conda activate'.
To initialize your shell, run

    $ conda init <SHELL_NAME>

Currently supported shells are:
  - bash
  - fish
  - tcsh
  - xonsh
  - zsh
  - powershell

See 'conda init --help' for more information and options.

IMPORTANT: You may need to close and restart your shell after running 'conda init'.


/home/gridsan/nchutisilp/.conda/envs/nnunet/bin/python
Starting training with CONFIG=3d_32x160x128_b10, DATASET_ID=307, TRAINER=nnUNetTrainer

############################
INFO: You are using the old nnU-Net default plans. We have updated our recommendations. Please consider using those instead! Read more here: https://github.com/MIC-DKFZ/nnUNet/blob/master/documentation/resenc_presets.md
############################

Using device: cuda:0

#######################################################################
Please cite the following paper when using nnU-Net:
Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.
#######################################################################

2024-12-07 01:40:47.494658: Using torch.compile...

############################
INFO: You are using the old nnU-Net default plans. We have updated our recommendations. Please consider using those instead! Read more here: https://github.com/MIC-DKFZ/nnUNet/blob/master/documentation/resenc_presets.md
############################

Using device: cuda:0

#######################################################################
Please cite the following paper when using nnU-Net:
Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.
#######################################################################

2024-12-07 01:40:47.494633: Using torch.compile...
/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
################### Loading pretrained weights from file  /home/gridsan/nchutisilp/projects/OpenAI-CLIP/clip_pretrained_nnUNet.pt ###################
Below is the list of overlapping blocks in pretrained model and nnUNet architecture:
encoder.stages.0.0.convs.0.conv.weight shape torch.Size([32, 1, 3, 3, 3])
encoder.stages.0.0.convs.0.conv.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.norm.weight shape torch.Size([32])
encoder.stages.0.0.convs.0.norm.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.0.weight shape torch.Size([32, 1, 3, 3, 3])
encoder.stages.0.0.convs.0.all_modules.0.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.1.weight shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.1.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
encoder.stages.0.0.convs.1.conv.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.norm.weight shape torch.Size([32])
encoder.stages.0.0.convs.1.norm.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
encoder.stages.0.0.convs.1.all_modules.0.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.1.weight shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.1.bias shape torch.Size([32])
encoder.stages.1.0.convs.0.conv.weight shape torch.Size([64, 32, 3, 3, 3])
encoder.stages.1.0.convs.0.conv.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.norm.weight shape torch.Size([64])
encoder.stages.1.0.convs.0.norm.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.0.weight shape torch.Size([64, 32, 3, 3, 3])
encoder.stages.1.0.convs.0.all_modules.0.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.1.weight shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.1.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
encoder.stages.1.0.convs.1.conv.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.norm.weight shape torch.Size([64])
encoder.stages.1.0.convs.1.norm.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
encoder.stages.1.0.convs.1.all_modules.0.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.1.weight shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.1.bias shape torch.Size([64])
encoder.stages.2.0.convs.0.conv.weight shape torch.Size([128, 64, 3, 3, 3])
encoder.stages.2.0.convs.0.conv.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.norm.weight shape torch.Size([128])
encoder.stages.2.0.convs.0.norm.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.0.weight shape torch.Size([128, 64, 3, 3, 3])
encoder.stages.2.0.convs.0.all_modules.0.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.1.weight shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.1.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
encoder.stages.2.0.convs.1.conv.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.norm.weight shape torch.Size([128])
encoder.stages.2.0.convs.1.norm.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
encoder.stages.2.0.convs.1.all_modules.0.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.1.weight shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.1.bias shape torch.Size([128])
encoder.stages.3.0.convs.0.conv.weight shape torch.Size([256, 128, 3, 3, 3])
encoder.stages.3.0.convs.0.conv.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.norm.weight shape torch.Size([256])
encoder.stages.3.0.convs.0.norm.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.0.weight shape torch.Size([256, 128, 3, 3, 3])
encoder.stages.3.0.convs.0.all_modules.0.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.1.weight shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.1.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
encoder.stages.3.0.convs.1.conv.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.norm.weight shape torch.Size([256])
encoder.stages.3.0.convs.1.norm.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
encoder.stages.3.0.convs.1.all_modules.0.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.1.weight shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.1.bias shape torch.Size([256])
encoder.stages.4.0.convs.0.conv.weight shape torch.Size([320, 256, 3, 3, 3])
encoder.stages.4.0.convs.0.conv.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.norm.weight shape torch.Size([320])
encoder.stages.4.0.convs.0.norm.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.0.weight shape torch.Size([320, 256, 3, 3, 3])
encoder.stages.4.0.convs.0.all_modules.0.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.1.weight shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.1.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.4.0.convs.1.conv.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.norm.weight shape torch.Size([320])
encoder.stages.4.0.convs.1.norm.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.4.0.convs.1.all_modules.0.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.1.weight shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.1.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.0.conv.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.norm.weight shape torch.Size([320])
encoder.stages.5.0.convs.0.norm.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.0.all_modules.0.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.1.weight shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.1.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.1.conv.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.norm.weight shape torch.Size([320])
encoder.stages.5.0.convs.1.norm.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.1.all_modules.0.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.1.weight shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.0.0.convs.0.conv.weight shape torch.Size([32, 1, 3, 3, 3])
decoder.encoder.stages.0.0.convs.0.conv.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.norm.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.norm.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.0.weight shape torch.Size([32, 1, 3, 3, 3])
decoder.encoder.stages.0.0.convs.0.all_modules.0.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.1.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.1.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.encoder.stages.0.0.convs.1.conv.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.norm.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.norm.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.encoder.stages.0.0.convs.1.all_modules.0.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.1.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.1.bias shape torch.Size([32])
decoder.encoder.stages.1.0.convs.0.conv.weight shape torch.Size([64, 32, 3, 3, 3])
decoder.encoder.stages.1.0.convs.0.conv.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.norm.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.norm.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.0.weight shape torch.Size([64, 32, 3, 3, 3])
decoder.encoder.stages.1.0.convs.0.all_modules.0.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.1.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.1.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.encoder.stages.1.0.convs.1.conv.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.norm.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.norm.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.encoder.stages.1.0.convs.1.all_modules.0.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.1.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.1.bias shape torch.Size([64])
decoder.encoder.stages.2.0.convs.0.conv.weight shape torch.Size([128, 64, 3, 3, 3])
decoder.encoder.stages.2.0.convs.0.conv.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.norm.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.norm.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.0.weight shape torch.Size([128, 64, 3, 3, 3])
decoder.encoder.stages.2.0.convs.0.all_modules.0.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.1.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.1.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.encoder.stages.2.0.convs.1.conv.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.norm.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.norm.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.encoder.stages.2.0.convs.1.all_modules.0.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.1.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.1.bias shape torch.Size([128])
decoder.encoder.stages.3.0.convs.0.conv.weight shape torch.Size([256, 128, 3, 3, 3])
decoder.encoder.stages.3.0.convs.0.conv.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.norm.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.norm.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.0.weight shape torch.Size([256, 128, 3, 3, 3])
decoder.encoder.stages.3.0.convs.0.all_modules.0.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.1.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.1.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.encoder.stages.3.0.convs.1.conv.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.norm.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.norm.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.encoder.stages.3.0.convs.1.all_modules.0.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.1.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.1.bias shape torch.Size([256])
decoder.encoder.stages.4.0.convs.0.conv.weight shape torch.Size([320, 256, 3, 3, 3])
decoder.encoder.stages.4.0.convs.0.conv.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.norm.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.norm.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.0.weight shape torch.Size([320, 256, 3, 3, 3])
decoder.encoder.stages.4.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.4.0.convs.1.conv.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.norm.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.norm.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.4.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.0.conv.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.norm.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.norm.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.1.conv.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.norm.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.norm.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.stages.0.convs.0.conv.weight shape torch.Size([320, 640, 3, 3, 3])
decoder.stages.0.convs.0.conv.bias shape torch.Size([320])
decoder.stages.0.convs.0.norm.weight shape torch.Size([320])
decoder.stages.0.convs.0.norm.bias shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.0.weight shape torch.Size([320, 640, 3, 3, 3])
decoder.stages.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.stages.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.stages.0.convs.1.conv.bias shape torch.Size([320])
decoder.stages.0.convs.1.norm.weight shape torch.Size([320])
decoder.stages.0.convs.1.norm.bias shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.stages.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.stages.1.convs.0.conv.weight shape torch.Size([256, 512, 3, 3, 3])
decoder.stages.1.convs.0.conv.bias shape torch.Size([256])
decoder.stages.1.convs.0.norm.weight shape torch.Size([256])
decoder.stages.1.convs.0.norm.bias shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.0.weight shape torch.Size([256, 512, 3, 3, 3])
decoder.stages.1.convs.0.all_modules.0.bias shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.1.weight shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.1.bias shape torch.Size([256])
decoder.stages.1.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.stages.1.convs.1.conv.bias shape torch.Size([256])
decoder.stages.1.convs.1.norm.weight shape torch.Size([256])
decoder.stages.1.convs.1.norm.bias shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.stages.1.convs.1.all_modules.0.bias shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.1.weight shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.1.bias shape torch.Size([256])
decoder.stages.2.convs.0.conv.weight shape torch.Size([128, 256, 3, 3, 3])
decoder.stages.2.convs.0.conv.bias shape torch.Size([128])
decoder.stages.2.convs.0.norm.weight shape torch.Size([128])
decoder.stages.2.convs.0.norm.bias shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.0.weight shape torch.Size([128, 256, 3, 3, 3])
decoder.stages.2.convs.0.all_modules.0.bias shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.1.weight shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.1.bias shape torch.Size([128])
decoder.stages.2.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.stages.2.convs.1.conv.bias shape torch.Size([128])
decoder.stages.2.convs.1.norm.weight shape torch.Size([128])
decoder.stages.2.convs.1.norm.bias shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.stages.2.convs.1.all_modules.0.bias shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.1.weight shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.1.bias shape torch.Size([128])
decoder.stages.3.convs.0.conv.weight shape torch.Size([64, 128, 3, 3, 3])
decoder.stages.3.convs.0.conv.bias shape torch.Size([64])
decoder.stages.3.convs.0.norm.weight shape torch.Size([64])
decoder.stages.3.convs.0.norm.bias shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.0.weight shape torch.Size([64, 128, 3, 3, 3])
decoder.stages.3.convs.0.all_modules.0.bias shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.1.weight shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.1.bias shape torch.Size([64])
decoder.stages.3.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.stages.3.convs.1.conv.bias shape torch.Size([64])
decoder.stages.3.convs.1.norm.weight shape torch.Size([64])
decoder.stages.3.convs.1.norm.bias shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.stages.3.convs.1.all_modules.0.bias shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.1.weight shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.1.bias shape torch.Size([64])
decoder.stages.4.convs.0.conv.weight shape torch.Size([32, 64, 3, 3, 3])
decoder.stages.4.convs.0.conv.bias shape torch.Size([32])
decoder.stages.4.convs.0.norm.weight shape torch.Size([32])
decoder.stages.4.convs.0.norm.bias shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.0.weight shape torch.Size([32, 64, 3, 3, 3])
decoder.stages.4.convs.0.all_modules.0.bias shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.1.weight shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.1.bias shape torch.Size([32])
decoder.stages.4.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.stages.4.convs.1.conv.bias shape torch.Size([32])
decoder.stages.4.convs.1.norm.weight shape torch.Size([32])
decoder.stages.4.convs.1.norm.bias shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.stages.4.convs.1.all_modules.0.bias shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.1.weight shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.1.bias shape torch.Size([32])
decoder.transpconvs.0.weight shape torch.Size([320, 320, 1, 2, 2])
decoder.transpconvs.0.bias shape torch.Size([320])
decoder.transpconvs.1.weight shape torch.Size([320, 256, 2, 2, 2])
decoder.transpconvs.1.bias shape torch.Size([256])
decoder.transpconvs.2.weight shape torch.Size([256, 128, 2, 2, 2])
decoder.transpconvs.2.bias shape torch.Size([128])
decoder.transpconvs.3.weight shape torch.Size([128, 64, 2, 2, 2])
decoder.transpconvs.3.bias shape torch.Size([64])
decoder.transpconvs.4.weight shape torch.Size([64, 32, 2, 2, 2])
decoder.transpconvs.4.bias shape torch.Size([32])
################### Done ###################
2024-12-07 01:41:09.055184: do_dummy_2d_data_aug: True
2024-12-07 01:41:09.058680: Using splits from existing split file: /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset307_Sohee_Calcium_OCT_CrossValidation/splits_final.json
2024-12-07 01:41:09.061169: The split file contains 5 splits.
2024-12-07 01:41:09.062335: Desired fold for training: 1
2024-12-07 01:41:09.063113: This split has 6 training and 2 validation cases.
################### Loading pretrained weights from file  /home/gridsan/nchutisilp/projects/OpenAI-CLIP/clip_pretrained_nnUNet.pt ###################
Below is the list of overlapping blocks in pretrained model and nnUNet architecture:
encoder.stages.0.0.convs.0.conv.weight shape torch.Size([32, 1, 3, 3, 3])
encoder.stages.0.0.convs.0.conv.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.norm.weight shape torch.Size([32])
encoder.stages.0.0.convs.0.norm.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.0.weight shape torch.Size([32, 1, 3, 3, 3])
encoder.stages.0.0.convs.0.all_modules.0.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.1.weight shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.1.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
encoder.stages.0.0.convs.1.conv.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.norm.weight shape torch.Size([32])
encoder.stages.0.0.convs.1.norm.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
encoder.stages.0.0.convs.1.all_modules.0.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.1.weight shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.1.bias shape torch.Size([32])
encoder.stages.1.0.convs.0.conv.weight shape torch.Size([64, 32, 3, 3, 3])
encoder.stages.1.0.convs.0.conv.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.norm.weight shape torch.Size([64])
encoder.stages.1.0.convs.0.norm.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.0.weight shape torch.Size([64, 32, 3, 3, 3])
encoder.stages.1.0.convs.0.all_modules.0.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.1.weight shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.1.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
encoder.stages.1.0.convs.1.conv.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.norm.weight shape torch.Size([64])
encoder.stages.1.0.convs.1.norm.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
encoder.stages.1.0.convs.1.all_modules.0.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.1.weight shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.1.bias shape torch.Size([64])
encoder.stages.2.0.convs.0.conv.weight shape torch.Size([128, 64, 3, 3, 3])
encoder.stages.2.0.convs.0.conv.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.norm.weight shape torch.Size([128])
encoder.stages.2.0.convs.0.norm.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.0.weight shape torch.Size([128, 64, 3, 3, 3])
encoder.stages.2.0.convs.0.all_modules.0.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.1.weight shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.1.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
encoder.stages.2.0.convs.1.conv.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.norm.weight shape torch.Size([128])
encoder.stages.2.0.convs.1.norm.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
encoder.stages.2.0.convs.1.all_modules.0.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.1.weight shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.1.bias shape torch.Size([128])
encoder.stages.3.0.convs.0.conv.weight shape torch.Size([256, 128, 3, 3, 3])
encoder.stages.3.0.convs.0.conv.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.norm.weight shape torch.Size([256])
encoder.stages.3.0.convs.0.norm.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.0.weight shape torch.Size([256, 128, 3, 3, 3])
encoder.stages.3.0.convs.0.all_modules.0.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.1.weight shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.1.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
encoder.stages.3.0.convs.1.conv.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.norm.weight shape torch.Size([256])
encoder.stages.3.0.convs.1.norm.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
encoder.stages.3.0.convs.1.all_modules.0.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.1.weight shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.1.bias shape torch.Size([256])
encoder.stages.4.0.convs.0.conv.weight shape torch.Size([320, 256, 3, 3, 3])
encoder.stages.4.0.convs.0.conv.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.norm.weight shape torch.Size([320])
encoder.stages.4.0.convs.0.norm.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.0.weight shape torch.Size([320, 256, 3, 3, 3])
encoder.stages.4.0.convs.0.all_modules.0.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.1.weight shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.1.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.4.0.convs.1.conv.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.norm.weight shape torch.Size([320])
encoder.stages.4.0.convs.1.norm.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.4.0.convs.1.all_modules.0.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.1.weight shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.1.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.0.conv.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.norm.weight shape torch.Size([320])
encoder.stages.5.0.convs.0.norm.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.0.all_modules.0.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.1.weight shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.1.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.1.conv.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.norm.weight shape torch.Size([320])
encoder.stages.5.0.convs.1.norm.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.1.all_modules.0.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.1.weight shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.0.0.convs.0.conv.weight shape torch.Size([32, 1, 3, 3, 3])
decoder.encoder.stages.0.0.convs.0.conv.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.norm.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.norm.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.0.weight shape torch.Size([32, 1, 3, 3, 3])
decoder.encoder.stages.0.0.convs.0.all_modules.0.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.1.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.1.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.encoder.stages.0.0.convs.1.conv.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.norm.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.norm.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.encoder.stages.0.0.convs.1.all_modules.0.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.1.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.1.bias shape torch.Size([32])
decoder.encoder.stages.1.0.convs.0.conv.weight shape torch.Size([64, 32, 3, 3, 3])
decoder.encoder.stages.1.0.convs.0.conv.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.norm.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.norm.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.0.weight shape torch.Size([64, 32, 3, 3, 3])
decoder.encoder.stages.1.0.convs.0.all_modules.0.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.1.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.1.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.encoder.stages.1.0.convs.1.conv.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.norm.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.norm.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.encoder.stages.1.0.convs.1.all_modules.0.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.1.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.1.bias shape torch.Size([64])
decoder.encoder.stages.2.0.convs.0.conv.weight shape torch.Size([128, 64, 3, 3, 3])
decoder.encoder.stages.2.0.convs.0.conv.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.norm.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.norm.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.0.weight shape torch.Size([128, 64, 3, 3, 3])
decoder.encoder.stages.2.0.convs.0.all_modules.0.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.1.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.1.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.encoder.stages.2.0.convs.1.conv.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.norm.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.norm.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.encoder.stages.2.0.convs.1.all_modules.0.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.1.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.1.bias shape torch.Size([128])
decoder.encoder.stages.3.0.convs.0.conv.weight shape torch.Size([256, 128, 3, 3, 3])
decoder.encoder.stages.3.0.convs.0.conv.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.norm.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.norm.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.0.weight shape torch.Size([256, 128, 3, 3, 3])
decoder.encoder.stages.3.0.convs.0.all_modules.0.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.1.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.1.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.encoder.stages.3.0.convs.1.conv.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.norm.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.norm.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.encoder.stages.3.0.convs.1.all_modules.0.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.1.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.1.bias shape torch.Size([256])
decoder.encoder.stages.4.0.convs.0.conv.weight shape torch.Size([320, 256, 3, 3, 3])
decoder.encoder.stages.4.0.convs.0.conv.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.norm.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.norm.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.0.weight shape torch.Size([320, 256, 3, 3, 3])
decoder.encoder.stages.4.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.4.0.convs.1.conv.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.norm.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.norm.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.4.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.0.conv.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.norm.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.norm.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.1.conv.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.norm.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.norm.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.stages.0.convs.0.conv.weight shape torch.Size([320, 640, 3, 3, 3])
decoder.stages.0.convs.0.conv.bias shape torch.Size([320])
decoder.stages.0.convs.0.norm.weight shape torch.Size([320])
decoder.stages.0.convs.0.norm.bias shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.0.weight shape torch.Size([320, 640, 3, 3, 3])
decoder.stages.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.stages.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.stages.0.convs.1.conv.bias shape torch.Size([320])
decoder.stages.0.convs.1.norm.weight shape torch.Size([320])
decoder.stages.0.convs.1.norm.bias shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.stages.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.stages.1.convs.0.conv.weight shape torch.Size([256, 512, 3, 3, 3])
decoder.stages.1.convs.0.conv.bias shape torch.Size([256])
decoder.stages.1.convs.0.norm.weight shape torch.Size([256])
decoder.stages.1.convs.0.norm.bias shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.0.weight shape torch.Size([256, 512, 3, 3, 3])
decoder.stages.1.convs.0.all_modules.0.bias shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.1.weight shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.1.bias shape torch.Size([256])
decoder.stages.1.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.stages.1.convs.1.conv.bias shape torch.Size([256])
decoder.stages.1.convs.1.norm.weight shape torch.Size([256])
decoder.stages.1.convs.1.norm.bias shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.stages.1.convs.1.all_modules.0.bias shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.1.weight shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.1.bias shape torch.Size([256])
decoder.stages.2.convs.0.conv.weight shape torch.Size([128, 256, 3, 3, 3])
decoder.stages.2.convs.0.conv.bias shape torch.Size([128])
decoder.stages.2.convs.0.norm.weight shape torch.Size([128])
decoder.stages.2.convs.0.norm.bias shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.0.weight shape torch.Size([128, 256, 3, 3, 3])
decoder.stages.2.convs.0.all_modules.0.bias shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.1.weight shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.1.bias shape torch.Size([128])
decoder.stages.2.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.stages.2.convs.1.conv.bias shape torch.Size([128])
decoder.stages.2.convs.1.norm.weight shape torch.Size([128])
decoder.stages.2.convs.1.norm.bias shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.stages.2.convs.1.all_modules.0.bias shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.1.weight shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.1.bias shape torch.Size([128])
decoder.stages.3.convs.0.conv.weight shape torch.Size([64, 128, 3, 3, 3])
decoder.stages.3.convs.0.conv.bias shape torch.Size([64])
decoder.stages.3.convs.0.norm.weight shape torch.Size([64])
decoder.stages.3.convs.0.norm.bias shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.0.weight shape torch.Size([64, 128, 3, 3, 3])
decoder.stages.3.convs.0.all_modules.0.bias shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.1.weight shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.1.bias shape torch.Size([64])
decoder.stages.3.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.stages.3.convs.1.conv.bias shape torch.Size([64])
decoder.stages.3.convs.1.norm.weight shape torch.Size([64])
decoder.stages.3.convs.1.norm.bias shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.stages.3.convs.1.all_modules.0.bias shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.1.weight shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.1.bias shape torch.Size([64])
decoder.stages.4.convs.0.conv.weight shape torch.Size([32, 64, 3, 3, 3])
decoder.stages.4.convs.0.conv.bias shape torch.Size([32])
decoder.stages.4.convs.0.norm.weight shape torch.Size([32])
decoder.stages.4.convs.0.norm.bias shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.0.weight shape torch.Size([32, 64, 3, 3, 3])
decoder.stages.4.convs.0.all_modules.0.bias shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.1.weight shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.1.bias shape torch.Size([32])
decoder.stages.4.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.stages.4.convs.1.conv.bias shape torch.Size([32])
decoder.stages.4.convs.1.norm.weight shape torch.Size([32])
decoder.stages.4.convs.1.norm.bias shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.stages.4.convs.1.all_modules.0.bias shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.1.weight shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.1.bias shape torch.Size([32])
decoder.transpconvs.0.weight shape torch.Size([320, 320, 1, 2, 2])
decoder.transpconvs.0.bias shape torch.Size([320])
decoder.transpconvs.1.weight shape torch.Size([320, 256, 2, 2, 2])
decoder.transpconvs.1.bias shape torch.Size([256])
decoder.transpconvs.2.weight shape torch.Size([256, 128, 2, 2, 2])
decoder.transpconvs.2.bias shape torch.Size([128])
decoder.transpconvs.3.weight shape torch.Size([128, 64, 2, 2, 2])
decoder.transpconvs.3.bias shape torch.Size([64])
decoder.transpconvs.4.weight shape torch.Size([64, 32, 2, 2, 2])
decoder.transpconvs.4.bias shape torch.Size([32])
################### Done ###################
2024-12-07 01:41:09.055185: do_dummy_2d_data_aug: True
2024-12-07 01:41:09.058689: Using splits from existing split file: /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset307_Sohee_Calcium_OCT_CrossValidation/splits_final.json
2024-12-07 01:41:09.061359: The split file contains 5 splits.
2024-12-07 01:41:09.062460: Desired fold for training: 0
2024-12-07 01:41:09.063142: This split has 6 training and 2 validation cases.
using pin_memory on device 0
using pin_memory on device 0
using pin_memory on device 0

This is the configuration used by this training:
Configuration name: 3d_32x160x128_b10
 {'data_identifier': 'nnUNetPlans_3d_fullres', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 10, 'patch_size': [32, 160, 128], 'median_image_size_in_voxels': [375.0, 498.0, 498.0], 'spacing': [1.0, 1.0, 1.0], 'normalization_schemes': ['ZScoreNormalization'], 'use_mask_for_norm': [False], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'architecture': {'network_class_name': 'dynamic_network_architectures.architectures.unet.PlainConvUNet', 'arch_kwargs': {'n_stages': 6, 'features_per_stage': [32, 64, 128, 256, 320, 320], 'conv_op': 'torch.nn.modules.conv.Conv3d', 'kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]], 'strides': [[1, 1, 1], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [1, 2, 2]], 'n_conv_per_stage': [2, 2, 2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2, 2, 2], 'conv_bias': True, 'norm_op': 'torch.nn.modules.instancenorm.InstanceNorm3d', 'norm_op_kwargs': {'eps': 1e-05, 'affine': True}, 'dropout_op': None, 'dropout_op_kwargs': None, 'nonlin': 'torch.nn.LeakyReLU', 'nonlin_kwargs': {'inplace': True}, 'deep_supervision': True}, '_kw_requires_import': ['conv_op', 'norm_op', 'dropout_op', 'nonlin']}, 'batch_dice': True, 'inherits_from': '3d_fullres'} 

These are the global plan.json settings:
 {'dataset_name': 'Dataset307_Sohee_Calcium_OCT_CrossValidation', 'plans_name': 'nnUNetPlans', 'original_median_spacing_after_transp': [1.0, 1.0, 1.0], 'original_median_shape_after_transp': [375, 498, 498], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 0.4977976381778717, 'mean': 0.13507525622844696, 'median': 0.09599608182907104, 'min': 0.0, 'percentile_00_5': 0.014754901640117168, 'percentile_99_5': 0.4977976381778717, 'std': 0.12025152146816254}}} 

2024-12-07 01:41:19.092694: unpacking dataset...
using pin_memory on device 0

This is the configuration used by this training:
Configuration name: 3d_32x160x128_b10
 {'data_identifier': 'nnUNetPlans_3d_fullres', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 10, 'patch_size': [32, 160, 128], 'median_image_size_in_voxels': [375.0, 498.0, 498.0], 'spacing': [1.0, 1.0, 1.0], 'normalization_schemes': ['ZScoreNormalization'], 'use_mask_for_norm': [False], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'architecture': {'network_class_name': 'dynamic_network_architectures.architectures.unet.PlainConvUNet', 'arch_kwargs': {'n_stages': 6, 'features_per_stage': [32, 64, 128, 256, 320, 320], 'conv_op': 'torch.nn.modules.conv.Conv3d', 'kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]], 'strides': [[1, 1, 1], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [1, 2, 2]], 'n_conv_per_stage': [2, 2, 2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2, 2, 2], 'conv_bias': True, 'norm_op': 'torch.nn.modules.instancenorm.InstanceNorm3d', 'norm_op_kwargs': {'eps': 1e-05, 'affine': True}, 'dropout_op': None, 'dropout_op_kwargs': None, 'nonlin': 'torch.nn.LeakyReLU', 'nonlin_kwargs': {'inplace': True}, 'deep_supervision': True}, '_kw_requires_import': ['conv_op', 'norm_op', 'dropout_op', 'nonlin']}, 'batch_dice': True, 'inherits_from': '3d_fullres'} 

These are the global plan.json settings:
 {'dataset_name': 'Dataset307_Sohee_Calcium_OCT_CrossValidation', 'plans_name': 'nnUNetPlans', 'original_median_spacing_after_transp': [1.0, 1.0, 1.0], 'original_median_shape_after_transp': [375, 498, 498], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 0.4977976381778717, 'mean': 0.13507525622844696, 'median': 0.09599608182907104, 'min': 0.0, 'percentile_00_5': 0.014754901640117168, 'percentile_99_5': 0.4977976381778717, 'std': 0.12025152146816254}}} 

2024-12-07 01:41:20.171683: unpacking dataset...
2024-12-07 01:41:24.997048: unpacking done...
2024-12-07 01:41:25.126820: Unable to plot network architecture: nnUNet_compile is enabled!
2024-12-07 01:41:25.634170: 
2024-12-07 01:41:25.635675: Epoch 0
2024-12-07 01:41:25.636904: Current learning rate: 0.01
2024-12-07 01:44:48.969200: Validation loss improved from 1000.00000 to -0.18677! Patience: 0/50
2024-12-07 01:44:48.983340: train_loss -0.08
2024-12-07 01:44:48.985641: val_loss -0.1868
2024-12-07 01:44:48.986713: Pseudo dice [0.5616]
2024-12-07 01:44:48.987713: Epoch time: 203.34 s
2024-12-07 01:44:48.988519: Yayy! New best EMA pseudo Dice: 0.5616
2024-12-07 01:44:51.354173: 
2024-12-07 01:44:51.355881: Epoch 1
2024-12-07 01:44:51.356790: Current learning rate: 0.00999
2024-12-07 01:46:18.213503: Validation loss improved from -0.18677 to -0.20404! Patience: 0/50
2024-12-07 01:46:18.215114: train_loss -0.2216
2024-12-07 01:46:18.216392: val_loss -0.204
2024-12-07 01:46:18.217524: Pseudo dice [0.5475]
2024-12-07 01:46:18.218424: Epoch time: 86.86 s
2024-12-07 01:46:19.473310: 
2024-12-07 01:46:19.475172: Epoch 2
2024-12-07 01:46:19.475961: Current learning rate: 0.00998
2024-12-07 01:47:47.299356: Validation loss improved from -0.20404 to -0.23009! Patience: 0/50
2024-12-07 01:47:47.300483: train_loss -0.2826
2024-12-07 01:47:47.301443: val_loss -0.2301
2024-12-07 01:47:47.302125: Pseudo dice [0.5826]
2024-12-07 01:47:47.302872: Epoch time: 87.83 s
2024-12-07 01:47:47.303535: Yayy! New best EMA pseudo Dice: 0.5624
2024-12-07 01:47:49.014154: 
2024-12-07 01:47:49.015967: Epoch 3
2024-12-07 01:47:49.016660: Current learning rate: 0.00997
2024-12-07 01:49:17.115474: Validation loss improved from -0.23009 to -0.30387! Patience: 0/50
2024-12-07 01:49:17.116518: train_loss -0.3034
2024-12-07 01:49:17.117418: val_loss -0.3039
2024-12-07 01:49:17.118163: Pseudo dice [0.6285]
2024-12-07 01:49:17.118825: Epoch time: 88.1 s
2024-12-07 01:49:17.119583: Yayy! New best EMA pseudo Dice: 0.569
2024-12-07 01:49:18.793406: 
2024-12-07 01:49:18.795126: Epoch 4
2024-12-07 01:49:18.795900: Current learning rate: 0.00996
2024-12-07 01:50:46.952320: Validation loss improved from -0.30387 to -0.34041! Patience: 0/50
2024-12-07 01:50:46.953407: train_loss -0.3443
2024-12-07 01:50:46.954659: val_loss -0.3404
2024-12-07 01:50:46.955578: Pseudo dice [0.6566]
2024-12-07 01:50:46.956472: Epoch time: 88.16 s
2024-12-07 01:50:47.302463: Yayy! New best EMA pseudo Dice: 0.5778
2024-12-07 01:50:48.998910: 
2024-12-07 01:50:49.000301: Epoch 5
2024-12-07 01:50:49.001185: Current learning rate: 0.00995
2024-12-07 01:52:17.079274: Validation loss did not improve from -0.34041. Patience: 1/50
2024-12-07 01:52:17.080135: train_loss -0.3605
2024-12-07 01:52:17.080909: val_loss -0.337
2024-12-07 01:52:17.081573: Pseudo dice [0.6477]
2024-12-07 01:52:17.082327: Epoch time: 88.08 s
2024-12-07 01:52:17.083071: Yayy! New best EMA pseudo Dice: 0.5848
2024-12-07 01:52:18.730152: 
2024-12-07 01:52:18.732005: Epoch 6
2024-12-07 01:52:18.732812: Current learning rate: 0.00995
2024-12-07 01:53:46.904971: Validation loss improved from -0.34041 to -0.37934! Patience: 1/50
2024-12-07 01:53:46.906135: train_loss -0.3869
2024-12-07 01:53:46.907334: val_loss -0.3793
2024-12-07 01:53:46.908305: Pseudo dice [0.687]
2024-12-07 01:53:46.909293: Epoch time: 88.18 s
2024-12-07 01:53:46.910265: Yayy! New best EMA pseudo Dice: 0.595
2024-12-07 01:53:48.580275: 
2024-12-07 01:53:48.581865: Epoch 7
2024-12-07 01:53:48.582845: Current learning rate: 0.00994
2024-12-07 01:55:16.679848: Validation loss did not improve from -0.37934. Patience: 1/50
2024-12-07 01:55:16.680942: train_loss -0.4222
2024-12-07 01:55:16.681850: val_loss -0.3404
2024-12-07 01:55:16.682805: Pseudo dice [0.667]
2024-12-07 01:55:16.683506: Epoch time: 88.1 s
2024-12-07 01:55:16.684186: Yayy! New best EMA pseudo Dice: 0.6022
2024-12-07 01:55:18.956319: 
2024-12-07 01:55:18.958115: Epoch 8
2024-12-07 01:55:18.958960: Current learning rate: 0.00993
2024-12-07 01:56:47.375839: Validation loss did not improve from -0.37934. Patience: 2/50
2024-12-07 01:56:47.376967: train_loss -0.4331
2024-12-07 01:56:47.377752: val_loss -0.3547
2024-12-07 01:56:47.378448: Pseudo dice [0.6661]
2024-12-07 01:56:47.379322: Epoch time: 88.42 s
2024-12-07 01:56:47.380040: Yayy! New best EMA pseudo Dice: 0.6086
2024-12-07 01:56:49.092195: 
2024-12-07 01:56:49.093800: Epoch 9
2024-12-07 01:56:49.094653: Current learning rate: 0.00992
2024-12-07 01:58:17.520919: Validation loss improved from -0.37934 to -0.41711! Patience: 2/50
2024-12-07 01:58:17.522242: train_loss -0.4465
2024-12-07 01:58:17.523484: val_loss -0.4171
2024-12-07 01:58:17.524412: Pseudo dice [0.7017]
2024-12-07 01:58:17.525424: Epoch time: 88.43 s
2024-12-07 01:58:17.925330: Yayy! New best EMA pseudo Dice: 0.6179
2024-12-07 01:58:19.573294: 
2024-12-07 01:58:19.574594: Epoch 10
2024-12-07 01:58:19.575493: Current learning rate: 0.00991
2024-12-07 01:59:47.962182: Validation loss did not improve from -0.41711. Patience: 1/50
2024-12-07 01:59:47.963374: train_loss -0.4457
2024-12-07 01:59:47.964208: val_loss -0.3884
2024-12-07 01:59:47.964993: Pseudo dice [0.6873]
2024-12-07 01:59:47.965630: Epoch time: 88.39 s
2024-12-07 01:59:47.966273: Yayy! New best EMA pseudo Dice: 0.6248
2024-12-07 01:59:49.583795: 
2024-12-07 01:59:49.586290: Epoch 11
2024-12-07 01:59:49.587220: Current learning rate: 0.0099
2024-12-07 02:01:17.941110: Validation loss improved from -0.41711 to -0.46805! Patience: 1/50
2024-12-07 02:01:17.942187: train_loss -0.4584
2024-12-07 02:01:17.943236: val_loss -0.468
2024-12-07 02:01:17.944215: Pseudo dice [0.7353]
2024-12-07 02:01:17.945108: Epoch time: 88.36 s
2024-12-07 02:01:17.945877: Yayy! New best EMA pseudo Dice: 0.6359
2024-12-07 02:01:19.623981: 
2024-12-07 02:01:19.625086: Epoch 12
2024-12-07 02:01:19.625967: Current learning rate: 0.00989
2024-12-07 02:02:48.043832: Validation loss did not improve from -0.46805. Patience: 1/50
2024-12-07 02:02:48.044993: train_loss -0.4659
2024-12-07 02:02:48.045895: val_loss -0.4606
2024-12-07 02:02:48.046685: Pseudo dice [0.7254]
2024-12-07 02:02:48.047551: Epoch time: 88.42 s
2024-12-07 02:02:48.048213: Yayy! New best EMA pseudo Dice: 0.6448
2024-12-07 02:02:49.739943: 
2024-12-07 02:02:49.741563: Epoch 13
2024-12-07 02:02:49.742407: Current learning rate: 0.00988
2024-12-07 02:04:18.240245: Validation loss did not improve from -0.46805. Patience: 2/50
2024-12-07 02:04:18.241612: train_loss -0.4791
2024-12-07 02:04:18.242454: val_loss -0.4375
2024-12-07 02:04:18.243292: Pseudo dice [0.7202]
2024-12-07 02:04:18.243929: Epoch time: 88.5 s
2024-12-07 02:04:18.244690: Yayy! New best EMA pseudo Dice: 0.6524
2024-12-07 02:04:19.912372: 
2024-12-07 02:04:19.914033: Epoch 14
2024-12-07 02:04:19.914890: Current learning rate: 0.00987
2024-12-07 02:05:48.304297: Validation loss did not improve from -0.46805. Patience: 3/50
2024-12-07 02:05:48.305210: train_loss -0.485
2024-12-07 02:05:48.306012: val_loss -0.4204
2024-12-07 02:05:48.306682: Pseudo dice [0.7169]
2024-12-07 02:05:48.307394: Epoch time: 88.39 s
2024-12-07 02:05:48.711713: Yayy! New best EMA pseudo Dice: 0.6588
2024-12-07 02:05:50.394151: 
2024-12-07 02:05:50.395767: Epoch 15
2024-12-07 02:05:50.396714: Current learning rate: 0.00986
2024-12-07 02:07:18.792095: Validation loss did not improve from -0.46805. Patience: 4/50
2024-12-07 02:07:18.793302: train_loss -0.4975
2024-12-07 02:07:18.794771: val_loss -0.4348
2024-12-07 02:07:18.795566: Pseudo dice [0.7061]
2024-12-07 02:07:18.796235: Epoch time: 88.4 s
2024-12-07 02:07:18.796968: Yayy! New best EMA pseudo Dice: 0.6636
2024-12-07 02:07:20.547861: 
2024-12-07 02:07:20.549761: Epoch 16
2024-12-07 02:07:20.550609: Current learning rate: 0.00986
2024-12-07 02:08:49.347010: Validation loss did not improve from -0.46805. Patience: 5/50
2024-12-07 02:08:49.348133: train_loss -0.5233
2024-12-07 02:08:49.349063: val_loss -0.4251
2024-12-07 02:08:49.349703: Pseudo dice [0.7099]
2024-12-07 02:08:49.350386: Epoch time: 88.8 s
2024-12-07 02:08:49.351121: Yayy! New best EMA pseudo Dice: 0.6682
2024-12-07 02:08:51.066545: 
2024-12-07 02:08:51.068297: Epoch 17
2024-12-07 02:08:51.069058: Current learning rate: 0.00985
2024-12-07 02:10:19.829926: Validation loss improved from -0.46805 to -0.47930! Patience: 5/50
2024-12-07 02:10:19.832774: train_loss -0.5172
2024-12-07 02:10:19.834127: val_loss -0.4793
2024-12-07 02:10:19.834826: Pseudo dice [0.7479]
2024-12-07 02:10:19.835709: Epoch time: 88.77 s
2024-12-07 02:10:19.836378: Yayy! New best EMA pseudo Dice: 0.6762
2024-12-07 02:10:22.163694: 
2024-12-07 02:10:22.165291: Epoch 18
2024-12-07 02:10:22.166054: Current learning rate: 0.00984
2024-12-07 02:11:50.983236: Validation loss did not improve from -0.47930. Patience: 1/50
2024-12-07 02:11:50.984125: train_loss -0.5178
2024-12-07 02:11:50.985048: val_loss -0.4246
2024-12-07 02:11:50.985868: Pseudo dice [0.7174]
2024-12-07 02:11:50.986540: Epoch time: 88.82 s
2024-12-07 02:11:50.987261: Yayy! New best EMA pseudo Dice: 0.6803
2024-12-07 02:11:52.739132: 
2024-12-07 02:11:52.740945: Epoch 19
2024-12-07 02:11:52.741817: Current learning rate: 0.00983
2024-12-07 02:13:21.451336: Validation loss did not improve from -0.47930. Patience: 2/50
2024-12-07 02:13:21.452544: train_loss -0.5331
2024-12-07 02:13:21.453772: val_loss -0.4197
2024-12-07 02:13:21.454635: Pseudo dice [0.7028]
2024-12-07 02:13:21.455386: Epoch time: 88.71 s
2024-12-07 02:13:21.846157: Yayy! New best EMA pseudo Dice: 0.6825
2024-12-07 02:13:23.539183: 
2024-12-07 02:13:23.540817: Epoch 20
2024-12-07 02:13:23.541693: Current learning rate: 0.00982
2024-12-07 02:14:52.195944: Validation loss did not improve from -0.47930. Patience: 3/50
2024-12-07 02:14:52.197252: train_loss -0.531
2024-12-07 02:14:52.198085: val_loss -0.4765
2024-12-07 02:14:52.198848: Pseudo dice [0.7303]
2024-12-07 02:14:52.199532: Epoch time: 88.66 s
2024-12-07 02:14:52.200280: Yayy! New best EMA pseudo Dice: 0.6873
2024-12-07 02:14:53.934132: 
2024-12-07 02:14:53.935691: Epoch 21
2024-12-07 02:14:53.936464: Current learning rate: 0.00981
2024-12-07 02:16:22.612907: Validation loss improved from -0.47930 to -0.49287! Patience: 3/50
2024-12-07 02:16:22.614187: train_loss -0.5488
2024-12-07 02:16:22.615221: val_loss -0.4929
2024-12-07 02:16:22.615920: Pseudo dice [0.7442]
2024-12-07 02:16:22.616602: Epoch time: 88.68 s
2024-12-07 02:16:22.617361: Yayy! New best EMA pseudo Dice: 0.693
2024-12-07 02:16:24.282228: 
2024-12-07 02:16:24.283482: Epoch 22
2024-12-07 02:16:24.284152: Current learning rate: 0.0098
2024-12-07 02:17:52.954266: Validation loss did not improve from -0.49287. Patience: 1/50
2024-12-07 02:17:52.955322: train_loss -0.5464
2024-12-07 02:17:52.956149: val_loss -0.4888
2024-12-07 02:17:52.956988: Pseudo dice [0.7492]
2024-12-07 02:17:52.958033: Epoch time: 88.67 s
2024-12-07 02:17:52.958823: Yayy! New best EMA pseudo Dice: 0.6986
2024-12-07 02:17:54.589988: 
2024-12-07 02:17:54.591958: Epoch 23
2024-12-07 02:17:54.592881: Current learning rate: 0.00979
2024-12-07 02:19:23.042933: Validation loss did not improve from -0.49287. Patience: 2/50
2024-12-07 02:19:23.044236: train_loss -0.5398
2024-12-07 02:19:23.045131: val_loss -0.4618
2024-12-07 02:19:23.045904: Pseudo dice [0.7326]
2024-12-07 02:19:23.046674: Epoch time: 88.46 s
2024-12-07 02:19:23.047527: Yayy! New best EMA pseudo Dice: 0.702
2024-12-07 02:19:24.709016: 
2024-12-07 02:19:24.710302: Epoch 24
2024-12-07 02:19:24.711324: Current learning rate: 0.00978
2024-12-07 02:20:53.107081: Validation loss did not improve from -0.49287. Patience: 3/50
2024-12-07 02:20:53.108248: train_loss -0.5465
2024-12-07 02:20:53.109216: val_loss -0.4924
2024-12-07 02:20:53.109901: Pseudo dice [0.7447]
2024-12-07 02:20:53.110693: Epoch time: 88.4 s
2024-12-07 02:20:53.482812: Yayy! New best EMA pseudo Dice: 0.7063
2024-12-07 02:20:55.092296: 
2024-12-07 02:20:55.093772: Epoch 25
2024-12-07 02:20:55.094459: Current learning rate: 0.00977
2024-12-07 02:22:23.444794: Validation loss improved from -0.49287 to -0.49430! Patience: 3/50
2024-12-07 02:22:23.445756: train_loss -0.5519
2024-12-07 02:22:23.446557: val_loss -0.4943
2024-12-07 02:22:23.447264: Pseudo dice [0.7475]
2024-12-07 02:22:23.447971: Epoch time: 88.35 s
2024-12-07 02:22:23.448764: Yayy! New best EMA pseudo Dice: 0.7104
2024-12-07 02:22:25.097263: 
2024-12-07 02:22:25.098896: Epoch 26
2024-12-07 02:22:25.099849: Current learning rate: 0.00977
2024-12-07 02:23:53.428431: Validation loss improved from -0.49430 to -0.51055! Patience: 0/50
2024-12-07 02:23:53.429656: train_loss -0.5775
2024-12-07 02:23:53.430881: val_loss -0.5105
2024-12-07 02:23:53.431829: Pseudo dice [0.7606]
2024-12-07 02:23:53.432806: Epoch time: 88.33 s
2024-12-07 02:23:53.433673: Yayy! New best EMA pseudo Dice: 0.7154
2024-12-07 02:23:55.124592: 
2024-12-07 02:23:55.126194: Epoch 27
2024-12-07 02:23:55.126935: Current learning rate: 0.00976
2024-12-07 02:25:23.518587: Validation loss did not improve from -0.51055. Patience: 1/50
2024-12-07 02:25:23.519826: train_loss -0.5688
2024-12-07 02:25:23.520865: val_loss -0.509
2024-12-07 02:25:23.521693: Pseudo dice [0.7571]
2024-12-07 02:25:23.522564: Epoch time: 88.4 s
2024-12-07 02:25:23.523376: Yayy! New best EMA pseudo Dice: 0.7196
2024-12-07 02:25:25.201669: 
2024-12-07 02:25:25.203462: Epoch 28
2024-12-07 02:25:25.204307: Current learning rate: 0.00975
2024-12-07 02:26:53.550427: Validation loss did not improve from -0.51055. Patience: 2/50
2024-12-07 02:26:53.551586: train_loss -0.5819
2024-12-07 02:26:53.552587: val_loss -0.4821
2024-12-07 02:26:53.553493: Pseudo dice [0.748]
2024-12-07 02:26:53.554474: Epoch time: 88.35 s
2024-12-07 02:26:53.555367: Yayy! New best EMA pseudo Dice: 0.7224
2024-12-07 02:26:55.610077: 
2024-12-07 02:26:55.611141: Epoch 29
2024-12-07 02:26:55.612105: Current learning rate: 0.00974
2024-12-07 02:28:23.991446: Validation loss did not improve from -0.51055. Patience: 3/50
2024-12-07 02:28:23.992862: train_loss -0.5714
2024-12-07 02:28:23.994113: val_loss -0.5065
2024-12-07 02:28:23.995055: Pseudo dice [0.7602]
2024-12-07 02:28:23.995956: Epoch time: 88.38 s
2024-12-07 02:28:24.387382: Yayy! New best EMA pseudo Dice: 0.7262
2024-12-07 02:28:26.049909: 
2024-12-07 02:28:26.051424: Epoch 30
2024-12-07 02:28:26.052185: Current learning rate: 0.00973
2024-12-07 02:29:54.405199: Validation loss did not improve from -0.51055. Patience: 4/50
2024-12-07 02:29:54.406041: train_loss -0.5752
2024-12-07 02:29:54.406853: val_loss -0.5087
2024-12-07 02:29:54.407638: Pseudo dice [0.76]
2024-12-07 02:29:54.408502: Epoch time: 88.36 s
2024-12-07 02:29:54.409261: Yayy! New best EMA pseudo Dice: 0.7296
2024-12-07 02:29:56.117767: 
2024-12-07 02:29:56.119609: Epoch 31
2024-12-07 02:29:56.120348: Current learning rate: 0.00972
2024-12-07 02:31:24.701673: Validation loss did not improve from -0.51055. Patience: 5/50
2024-12-07 02:31:24.702956: train_loss -0.5755
2024-12-07 02:31:24.703753: val_loss -0.499
2024-12-07 02:31:24.704400: Pseudo dice [0.7449]
2024-12-07 02:31:24.705159: Epoch time: 88.59 s
2024-12-07 02:31:24.705881: Yayy! New best EMA pseudo Dice: 0.7311
2024-12-07 02:31:26.407043: 
2024-12-07 02:31:26.408714: Epoch 32
2024-12-07 02:31:26.409579: Current learning rate: 0.00971
2024-12-07 02:32:55.129034: Validation loss did not improve from -0.51055. Patience: 6/50
2024-12-07 02:32:55.130025: train_loss -0.5813
2024-12-07 02:32:55.131124: val_loss -0.4983
2024-12-07 02:32:55.131969: Pseudo dice [0.7449]
2024-12-07 02:32:55.132961: Epoch time: 88.72 s
2024-12-07 02:32:55.133745: Yayy! New best EMA pseudo Dice: 0.7325
2024-12-07 02:32:56.791277: 
2024-12-07 02:32:56.793071: Epoch 33
2024-12-07 02:32:56.794124: Current learning rate: 0.0097
2024-12-07 02:34:25.453343: Validation loss did not improve from -0.51055. Patience: 7/50
2024-12-07 02:34:25.454887: train_loss -0.5938
2024-12-07 02:34:25.456030: val_loss -0.5005
2024-12-07 02:34:25.456955: Pseudo dice [0.7476]
2024-12-07 02:34:25.457731: Epoch time: 88.66 s
2024-12-07 02:34:25.458565: Yayy! New best EMA pseudo Dice: 0.734
2024-12-07 02:34:27.175009: 
2024-12-07 02:34:27.176565: Epoch 34
2024-12-07 02:34:27.177355: Current learning rate: 0.00969
2024-12-07 02:35:55.882765: Validation loss improved from -0.51055 to -0.54701! Patience: 7/50
2024-12-07 02:35:55.883567: train_loss -0.6018
2024-12-07 02:35:55.884428: val_loss -0.547
2024-12-07 02:35:55.885504: Pseudo dice [0.7753]
2024-12-07 02:35:55.886355: Epoch time: 88.71 s
2024-12-07 02:35:56.293128: Yayy! New best EMA pseudo Dice: 0.7381
2024-12-07 02:35:58.000923: 
2024-12-07 02:35:58.002714: Epoch 35
2024-12-07 02:35:58.003517: Current learning rate: 0.00968
2024-12-07 02:37:26.717167: Validation loss did not improve from -0.54701. Patience: 1/50
2024-12-07 02:37:26.718331: train_loss -0.5929
2024-12-07 02:37:26.719190: val_loss -0.4961
2024-12-07 02:37:26.719928: Pseudo dice [0.7556]
2024-12-07 02:37:26.720587: Epoch time: 88.72 s
2024-12-07 02:37:26.721274: Yayy! New best EMA pseudo Dice: 0.7399
2024-12-07 02:37:28.470968: 
2024-12-07 02:37:28.472748: Epoch 36
2024-12-07 02:37:28.473516: Current learning rate: 0.00968
2024-12-07 02:38:57.177264: Validation loss did not improve from -0.54701. Patience: 2/50
2024-12-07 02:38:57.178445: train_loss -0.5989
2024-12-07 02:38:57.179388: val_loss -0.5001
2024-12-07 02:38:57.180132: Pseudo dice [0.7555]
2024-12-07 02:38:57.180989: Epoch time: 88.71 s
2024-12-07 02:38:57.181715: Yayy! New best EMA pseudo Dice: 0.7415
2024-12-07 02:38:58.873912: 
2024-12-07 02:38:58.875246: Epoch 37
2024-12-07 02:38:58.875945: Current learning rate: 0.00967
2024-12-07 02:40:27.590707: Validation loss did not improve from -0.54701. Patience: 3/50
2024-12-07 02:40:27.591748: train_loss -0.6075
2024-12-07 02:40:27.592797: val_loss -0.4713
2024-12-07 02:40:27.593770: Pseudo dice [0.7322]
2024-12-07 02:40:27.594600: Epoch time: 88.72 s
2024-12-07 02:40:28.899662: 
2024-12-07 02:40:28.901486: Epoch 38
2024-12-07 02:40:28.902707: Current learning rate: 0.00966
2024-12-07 02:41:57.590538: Validation loss did not improve from -0.54701. Patience: 4/50
2024-12-07 02:41:57.591784: train_loss -0.6023
2024-12-07 02:41:57.593008: val_loss -0.4947
2024-12-07 02:41:57.594063: Pseudo dice [0.7576]
2024-12-07 02:41:57.595042: Epoch time: 88.69 s
2024-12-07 02:41:57.596070: Yayy! New best EMA pseudo Dice: 0.7422
2024-12-07 02:41:59.750201: 
2024-12-07 02:41:59.752360: Epoch 39
2024-12-07 02:41:59.753440: Current learning rate: 0.00965
2024-12-07 02:43:28.570976: Validation loss did not improve from -0.54701. Patience: 5/50
2024-12-07 02:43:28.572314: train_loss -0.6177
2024-12-07 02:43:28.573370: val_loss -0.4683
2024-12-07 02:43:28.574083: Pseudo dice [0.7365]
2024-12-07 02:43:28.574879: Epoch time: 88.82 s
2024-12-07 02:43:30.337839: 
2024-12-07 02:43:30.339824: Epoch 40
2024-12-07 02:43:30.340739: Current learning rate: 0.00964
2024-12-07 02:44:58.991700: Validation loss did not improve from -0.54701. Patience: 6/50
2024-12-07 02:44:58.993828: train_loss -0.6159
2024-12-07 02:44:58.995196: val_loss -0.5023
2024-12-07 02:44:58.996002: Pseudo dice [0.752]
2024-12-07 02:44:58.996788: Epoch time: 88.66 s
2024-12-07 02:44:58.997550: Yayy! New best EMA pseudo Dice: 0.7427
2024-12-07 02:45:00.756026: 
2024-12-07 02:45:00.757636: Epoch 41
2024-12-07 02:45:00.758654: Current learning rate: 0.00963
2024-12-07 02:46:29.521129: Validation loss did not improve from -0.54701. Patience: 7/50
2024-12-07 02:46:29.522514: train_loss -0.6009
2024-12-07 02:46:29.523403: val_loss -0.5052
2024-12-07 02:46:29.524260: Pseudo dice [0.7573]
2024-12-07 02:46:29.525089: Epoch time: 88.77 s
2024-12-07 02:46:29.526044: Yayy! New best EMA pseudo Dice: 0.7442
2024-12-07 02:46:31.245232: 
2024-12-07 02:46:31.246777: Epoch 42
2024-12-07 02:46:31.247522: Current learning rate: 0.00962
2024-12-07 02:47:59.943371: Validation loss did not improve from -0.54701. Patience: 8/50
2024-12-07 02:47:59.946037: train_loss -0.6108
2024-12-07 02:47:59.947298: val_loss -0.5228
2024-12-07 02:47:59.948098: Pseudo dice [0.7576]
2024-12-07 02:47:59.949374: Epoch time: 88.7 s
2024-12-07 02:47:59.950640: Yayy! New best EMA pseudo Dice: 0.7455
2024-12-07 02:48:01.670210: 
2024-12-07 02:48:01.671883: Epoch 43
2024-12-07 02:48:01.672709: Current learning rate: 0.00961
2024-12-07 02:49:30.276104: Validation loss did not improve from -0.54701. Patience: 9/50
2024-12-07 02:49:30.276989: train_loss -0.6173
2024-12-07 02:49:30.277985: val_loss -0.5035
2024-12-07 02:49:30.278922: Pseudo dice [0.749]
2024-12-07 02:49:30.279674: Epoch time: 88.61 s
2024-12-07 02:49:30.280449: Yayy! New best EMA pseudo Dice: 0.7459
2024-12-07 02:49:31.971415: 
2024-12-07 02:49:31.973112: Epoch 44
2024-12-07 02:49:31.974065: Current learning rate: 0.0096
2024-12-07 02:51:00.687565: Validation loss did not improve from -0.54701. Patience: 10/50
2024-12-07 02:51:00.689409: train_loss -0.6324
2024-12-07 02:51:00.691749: val_loss -0.4687
2024-12-07 02:51:00.693078: Pseudo dice [0.7452]
2024-12-07 02:51:00.694637: Epoch time: 88.72 s
2024-12-07 02:51:02.291058: 
2024-12-07 02:51:02.293682: Epoch 45
2024-12-07 02:51:02.294854: Current learning rate: 0.00959
2024-12-07 02:52:30.983562: Validation loss did not improve from -0.54701. Patience: 11/50
2024-12-07 02:52:30.984738: train_loss -0.6288
2024-12-07 02:52:30.985825: val_loss -0.4897
2024-12-07 02:52:30.986819: Pseudo dice [0.737]
2024-12-07 02:52:30.987761: Epoch time: 88.69 s
2024-12-07 02:52:32.192478: 
2024-12-07 02:52:32.193877: Epoch 46
2024-12-07 02:52:32.194796: Current learning rate: 0.00959
2024-12-07 02:54:00.816624: Validation loss did not improve from -0.54701. Patience: 12/50
2024-12-07 02:54:00.817742: train_loss -0.6337
2024-12-07 02:54:00.818526: val_loss -0.5257
2024-12-07 02:54:00.819242: Pseudo dice [0.7649]
2024-12-07 02:54:00.819934: Epoch time: 88.63 s
2024-12-07 02:54:00.820688: Yayy! New best EMA pseudo Dice: 0.7469
2024-12-07 02:54:02.476441: 
2024-12-07 02:54:02.477772: Epoch 47
2024-12-07 02:54:02.478502: Current learning rate: 0.00958
2024-12-07 02:55:31.143129: Validation loss did not improve from -0.54701. Patience: 13/50
2024-12-07 02:55:31.156211: train_loss -0.6314
2024-12-07 02:55:31.157763: val_loss -0.4122
2024-12-07 02:55:31.158555: Pseudo dice [0.7099]
2024-12-07 02:55:31.159288: Epoch time: 88.68 s
2024-12-07 02:55:32.421669: 
2024-12-07 02:55:32.423527: Epoch 48
2024-12-07 02:55:32.424624: Current learning rate: 0.00957
2024-12-07 02:57:01.132497: Validation loss did not improve from -0.54701. Patience: 14/50
2024-12-07 02:57:01.133586: train_loss -0.619
2024-12-07 02:57:01.134505: val_loss -0.5328
2024-12-07 02:57:01.135264: Pseudo dice [0.7727]
2024-12-07 02:57:01.136111: Epoch time: 88.71 s
2024-12-07 02:57:02.444042: 
2024-12-07 02:57:02.445496: Epoch 49
2024-12-07 02:57:02.446517: Current learning rate: 0.00956
2024-12-07 02:58:31.079137: Validation loss did not improve from -0.54701. Patience: 15/50
2024-12-07 02:58:31.079974: train_loss -0.6387
2024-12-07 02:58:31.080923: val_loss -0.5361
2024-12-07 02:58:31.081699: Pseudo dice [0.7688]
2024-12-07 02:58:31.082548: Epoch time: 88.64 s
2024-12-07 02:58:31.473673: Yayy! New best EMA pseudo Dice: 0.7484
2024-12-07 02:58:33.592886: 
2024-12-07 02:58:33.594406: Epoch 50
2024-12-07 02:58:33.595370: Current learning rate: 0.00955
2024-12-07 03:00:02.252391: Validation loss did not improve from -0.54701. Patience: 16/50
2024-12-07 03:00:02.253680: train_loss -0.638
2024-12-07 03:00:02.254652: val_loss -0.5096
2024-12-07 03:00:02.255395: Pseudo dice [0.7654]
2024-12-07 03:00:02.256195: Epoch time: 88.66 s
2024-12-07 03:00:02.256994: Yayy! New best EMA pseudo Dice: 0.7501
2024-12-07 03:00:03.915476: 
2024-12-07 03:00:03.917248: Epoch 51
2024-12-07 03:00:03.918192: Current learning rate: 0.00954
2024-12-07 03:01:32.585552: Validation loss did not improve from -0.54701. Patience: 17/50
2024-12-07 03:01:32.586598: train_loss -0.638
2024-12-07 03:01:32.587436: val_loss -0.5166
2024-12-07 03:01:32.588118: Pseudo dice [0.7556]
2024-12-07 03:01:32.588807: Epoch time: 88.67 s
2024-12-07 03:01:32.589556: Yayy! New best EMA pseudo Dice: 0.7507
2024-12-07 03:01:34.274771: 
2024-12-07 03:01:34.276024: Epoch 52
2024-12-07 03:01:34.276764: Current learning rate: 0.00953
2024-12-07 03:03:02.896536: Validation loss did not improve from -0.54701. Patience: 18/50
2024-12-07 03:03:02.897971: train_loss -0.6378
2024-12-07 03:03:02.899106: val_loss -0.5061
2024-12-07 03:03:02.899783: Pseudo dice [0.7465]
2024-12-07 03:03:02.900564: Epoch time: 88.62 s
2024-12-07 03:03:04.177846: 
2024-12-07 03:03:04.179789: Epoch 53
2024-12-07 03:03:04.180617: Current learning rate: 0.00952
2024-12-07 03:04:32.967466: Validation loss improved from -0.54701 to -0.55876! Patience: 18/50
2024-12-07 03:04:32.968823: train_loss -0.6433
2024-12-07 03:04:32.969723: val_loss -0.5588
2024-12-07 03:04:32.970619: Pseudo dice [0.7683]
2024-12-07 03:04:32.971502: Epoch time: 88.79 s
2024-12-07 03:04:32.972314: Yayy! New best EMA pseudo Dice: 0.7521
2024-12-07 03:04:34.683654: 
2024-12-07 03:04:34.686125: Epoch 54
2024-12-07 03:04:34.687519: Current learning rate: 0.00951
2024-12-07 03:06:03.362383: Validation loss did not improve from -0.55876. Patience: 1/50
2024-12-07 03:06:03.363317: train_loss -0.6448
2024-12-07 03:06:03.364245: val_loss -0.4716
2024-12-07 03:06:03.365115: Pseudo dice [0.7285]
2024-12-07 03:06:03.366012: Epoch time: 88.68 s
2024-12-07 03:06:05.039322: 
2024-12-07 03:06:05.041067: Epoch 55
2024-12-07 03:06:05.042006: Current learning rate: 0.0095
2024-12-07 03:07:33.682062: Validation loss did not improve from -0.55876. Patience: 2/50
2024-12-07 03:07:33.683255: train_loss -0.6388
2024-12-07 03:07:33.684356: val_loss -0.5536
2024-12-07 03:07:33.685316: Pseudo dice [0.7683]
2024-12-07 03:07:33.686256: Epoch time: 88.64 s
2024-12-07 03:07:34.970032: 
2024-12-07 03:07:34.971880: Epoch 56
2024-12-07 03:07:34.972798: Current learning rate: 0.00949
2024-12-07 03:09:03.838150: Validation loss did not improve from -0.55876. Patience: 3/50
2024-12-07 03:09:03.839418: train_loss -0.6513
2024-12-07 03:09:03.840507: val_loss -0.5031
2024-12-07 03:09:03.841430: Pseudo dice [0.7591]
2024-12-07 03:09:03.842491: Epoch time: 88.87 s
2024-12-07 03:09:03.843412: Yayy! New best EMA pseudo Dice: 0.7523
2024-12-07 03:09:05.464698: 
2024-12-07 03:09:05.465910: Epoch 57
2024-12-07 03:09:05.466909: Current learning rate: 0.00949
2024-12-07 03:10:34.493966: Validation loss did not improve from -0.55876. Patience: 4/50
2024-12-07 03:10:34.495258: train_loss -0.6449
2024-12-07 03:10:34.496425: val_loss -0.537
2024-12-07 03:10:34.497233: Pseudo dice [0.768]
2024-12-07 03:10:34.498157: Epoch time: 89.03 s
2024-12-07 03:10:34.499098: Yayy! New best EMA pseudo Dice: 0.7539
2024-12-07 03:10:36.157023: 
2024-12-07 03:10:36.158760: Epoch 58
2024-12-07 03:10:36.159584: Current learning rate: 0.00948
2024-12-07 03:12:05.092130: Validation loss did not improve from -0.55876. Patience: 5/50
2024-12-07 03:12:05.093315: train_loss -0.6567
2024-12-07 03:12:05.094471: val_loss -0.5129
2024-12-07 03:12:05.095366: Pseudo dice [0.7518]
2024-12-07 03:12:05.096298: Epoch time: 88.94 s
2024-12-07 03:12:06.380638: 
2024-12-07 03:12:06.382179: Epoch 59
2024-12-07 03:12:06.382957: Current learning rate: 0.00947
2024-12-07 03:13:35.451028: Validation loss did not improve from -0.55876. Patience: 6/50
2024-12-07 03:13:35.452107: train_loss -0.6491
2024-12-07 03:13:35.453110: val_loss -0.4864
2024-12-07 03:13:35.453865: Pseudo dice [0.7376]
2024-12-07 03:13:35.454721: Epoch time: 89.07 s
2024-12-07 03:13:37.142490: 
2024-12-07 03:13:37.143967: Epoch 60
2024-12-07 03:13:37.145031: Current learning rate: 0.00946
2024-12-07 03:15:09.702060: Validation loss did not improve from -0.55876. Patience: 7/50
2024-12-07 03:15:09.703322: train_loss -0.6574
2024-12-07 03:15:09.704374: val_loss -0.5262
2024-12-07 03:15:09.705198: Pseudo dice [0.7626]
2024-12-07 03:15:09.706015: Epoch time: 92.56 s
2024-12-07 03:15:11.678983: 
2024-12-07 03:15:11.680192: Epoch 61
2024-12-07 03:15:11.680947: Current learning rate: 0.00945
2024-12-07 03:16:51.129166: Validation loss did not improve from -0.55876. Patience: 8/50
2024-12-07 03:16:51.130222: train_loss -0.6597
2024-12-07 03:16:51.131706: val_loss -0.5114
2024-12-07 03:16:51.133117: Pseudo dice [0.7595]
2024-12-07 03:16:51.134564: Epoch time: 99.45 s
2024-12-07 03:16:52.699116: 
2024-12-07 03:16:52.700498: Epoch 62
2024-12-07 03:16:52.701340: Current learning rate: 0.00944
2024-12-07 03:19:17.336019: Validation loss did not improve from -0.55876. Patience: 9/50
2024-12-07 03:19:17.337315: train_loss -0.6563
2024-12-07 03:19:17.338503: val_loss -0.4793
2024-12-07 03:19:17.339489: Pseudo dice [0.7481]
2024-12-07 03:19:17.340458: Epoch time: 144.64 s
2024-12-07 03:19:18.882492: 
2024-12-07 03:19:18.883986: Epoch 63
2024-12-07 03:19:18.884875: Current learning rate: 0.00943
2024-12-07 03:22:08.335135: Validation loss did not improve from -0.55876. Patience: 10/50
2024-12-07 03:22:08.336164: train_loss -0.6531
2024-12-07 03:22:08.337205: val_loss -0.5288
2024-12-07 03:22:08.338150: Pseudo dice [0.7654]
2024-12-07 03:22:08.339103: Epoch time: 169.46 s
2024-12-07 03:22:08.340089: Yayy! New best EMA pseudo Dice: 0.7544
2024-12-07 03:22:10.391577: 
2024-12-07 03:22:10.393157: Epoch 64
2024-12-07 03:22:10.394144: Current learning rate: 0.00942
2024-12-07 03:25:20.420509: Validation loss did not improve from -0.55876. Patience: 11/50
2024-12-07 03:25:20.421728: train_loss -0.6516
2024-12-07 03:25:20.422624: val_loss -0.5091
2024-12-07 03:25:20.423325: Pseudo dice [0.7573]
2024-12-07 03:25:20.424183: Epoch time: 190.03 s
2024-12-07 03:25:20.868752: Yayy! New best EMA pseudo Dice: 0.7547
2024-12-07 03:25:22.940822: 
2024-12-07 03:25:22.942332: Epoch 65
2024-12-07 03:25:22.943187: Current learning rate: 0.00941
2024-12-07 03:28:41.975751: Validation loss did not improve from -0.55876. Patience: 12/50
2024-12-07 03:28:41.976825: train_loss -0.6519
2024-12-07 03:28:41.977769: val_loss -0.4728
2024-12-07 03:28:41.978511: Pseudo dice [0.7432]
2024-12-07 03:28:41.979367: Epoch time: 199.04 s
2024-12-07 03:28:43.484952: 
2024-12-07 03:28:43.486511: Epoch 66
2024-12-07 03:28:43.487420: Current learning rate: 0.0094
2024-12-07 03:32:24.480152: Validation loss did not improve from -0.55876. Patience: 13/50
2024-12-07 03:32:24.481173: train_loss -0.6571
2024-12-07 03:32:24.482202: val_loss -0.506
2024-12-07 03:32:24.483045: Pseudo dice [0.7594]
2024-12-07 03:32:24.483971: Epoch time: 221.0 s
2024-12-07 03:32:25.972650: 
2024-12-07 03:32:25.974144: Epoch 67
2024-12-07 03:32:25.975074: Current learning rate: 0.00939
2024-12-07 03:36:00.658715: Validation loss did not improve from -0.55876. Patience: 14/50
2024-12-07 03:36:00.659898: train_loss -0.6626
2024-12-07 03:36:00.660843: val_loss -0.4992
2024-12-07 03:36:00.661593: Pseudo dice [0.7439]
2024-12-07 03:36:00.662447: Epoch time: 214.69 s
2024-12-07 03:36:02.250751: 
2024-12-07 03:36:02.252004: Epoch 68
2024-12-07 03:36:02.252750: Current learning rate: 0.00939
2024-12-07 03:39:40.870577: Validation loss did not improve from -0.55876. Patience: 15/50
2024-12-07 03:39:40.871547: train_loss -0.6648
2024-12-07 03:39:40.872375: val_loss -0.5271
2024-12-07 03:39:40.873051: Pseudo dice [0.7672]
2024-12-07 03:39:40.873829: Epoch time: 218.62 s
2024-12-07 03:39:42.438698: 
2024-12-07 03:39:42.440717: Epoch 69
2024-12-07 03:39:42.441604: Current learning rate: 0.00938
2024-12-07 03:43:15.262818: Validation loss did not improve from -0.55876. Patience: 16/50
2024-12-07 03:43:15.264274: train_loss -0.668
2024-12-07 03:43:15.265587: val_loss -0.5382
2024-12-07 03:43:15.266345: Pseudo dice [0.7769]
2024-12-07 03:43:15.267144: Epoch time: 212.83 s
2024-12-07 03:43:15.717647: Yayy! New best EMA pseudo Dice: 0.7567
2024-12-07 03:43:17.698318: 
2024-12-07 03:43:17.699892: Epoch 70
2024-12-07 03:43:17.700762: Current learning rate: 0.00937
2024-12-07 03:46:59.256952: Validation loss did not improve from -0.55876. Patience: 17/50
2024-12-07 03:46:59.258007: train_loss -0.6781
2024-12-07 03:46:59.259218: val_loss -0.5151
2024-12-07 03:46:59.260230: Pseudo dice [0.7642]
2024-12-07 03:46:59.261245: Epoch time: 221.56 s
2024-12-07 03:46:59.262333: Yayy! New best EMA pseudo Dice: 0.7575
2024-12-07 03:47:01.181626: 
2024-12-07 03:47:01.183064: Epoch 71
2024-12-07 03:47:01.183972: Current learning rate: 0.00936
2024-12-07 03:50:43.369473: Validation loss did not improve from -0.55876. Patience: 18/50
2024-12-07 03:50:43.372320: train_loss -0.6849
2024-12-07 03:50:43.373503: val_loss -0.4894
2024-12-07 03:50:43.374254: Pseudo dice [0.7466]
2024-12-07 03:50:43.375020: Epoch time: 222.19 s
2024-12-07 03:50:45.578871: 
2024-12-07 03:50:45.580805: Epoch 72
2024-12-07 03:50:45.582371: Current learning rate: 0.00935
2024-12-07 03:55:54.415461: Validation loss did not improve from -0.55876. Patience: 19/50
2024-12-07 03:55:54.417057: train_loss -0.6754
2024-12-07 03:55:54.419203: val_loss -0.5108
2024-12-07 03:55:54.420216: Pseudo dice [0.7568]
2024-12-07 03:55:54.421386: Epoch time: 308.84 s
2024-12-07 03:55:55.906323: 
2024-12-07 03:55:55.907530: Epoch 73
2024-12-07 03:55:55.908321: Current learning rate: 0.00934
2024-12-07 04:01:37.588175: Validation loss improved from -0.55876 to -0.56576! Patience: 19/50
2024-12-07 04:01:37.589152: train_loss -0.6797
2024-12-07 04:01:37.589902: val_loss -0.5658
2024-12-07 04:01:37.590614: Pseudo dice [0.7849]
2024-12-07 04:01:37.591243: Epoch time: 341.68 s
2024-12-07 04:01:37.591928: Yayy! New best EMA pseudo Dice: 0.7593
2024-12-07 04:01:39.482989: 
2024-12-07 04:01:39.484370: Epoch 74
2024-12-07 04:01:39.485310: Current learning rate: 0.00933
2024-12-07 04:07:36.642416: Validation loss did not improve from -0.56576. Patience: 1/50
2024-12-07 04:07:36.643636: train_loss -0.6801
2024-12-07 04:07:36.644589: val_loss -0.448
2024-12-07 04:07:36.645443: Pseudo dice [0.7281]
2024-12-07 04:07:36.646127: Epoch time: 357.16 s
2024-12-07 04:07:38.544940: 
2024-12-07 04:07:38.546566: Epoch 75
2024-12-07 04:07:38.547603: Current learning rate: 0.00932
2024-12-07 04:13:44.074545: Validation loss did not improve from -0.56576. Patience: 2/50
2024-12-07 04:13:44.075575: train_loss -0.6809
2024-12-07 04:13:44.076867: val_loss -0.5372
2024-12-07 04:13:44.077536: Pseudo dice [0.7705]
2024-12-07 04:13:44.078227: Epoch time: 365.53 s
2024-12-07 04:13:45.552467: 
2024-12-07 04:13:45.553966: Epoch 76
2024-12-07 04:13:45.554762: Current learning rate: 0.00931
2024-12-07 04:20:04.996261: Validation loss did not improve from -0.56576. Patience: 3/50
2024-12-07 04:20:04.997220: train_loss -0.6857
2024-12-07 04:20:04.997916: val_loss -0.4884
2024-12-07 04:20:04.998555: Pseudo dice [0.748]
2024-12-07 04:20:04.999212: Epoch time: 379.45 s
2024-12-07 04:20:06.493697: 
2024-12-07 04:20:06.495060: Epoch 77
2024-12-07 04:20:06.495886: Current learning rate: 0.0093
2024-12-07 04:26:27.419179: Validation loss did not improve from -0.56576. Patience: 4/50
2024-12-07 04:26:27.420214: train_loss -0.6858
2024-12-07 04:26:27.421005: val_loss -0.5247
2024-12-07 04:26:27.421650: Pseudo dice [0.7749]
2024-12-07 04:26:27.422391: Epoch time: 380.93 s
2024-12-07 04:26:28.930083: 
2024-12-07 04:26:28.931407: Epoch 78
2024-12-07 04:26:28.932232: Current learning rate: 0.0093
2024-12-07 04:32:47.451646: Validation loss did not improve from -0.56576. Patience: 5/50
2024-12-07 04:32:47.452711: train_loss -0.6845
2024-12-07 04:32:47.453539: val_loss -0.5559
2024-12-07 04:32:47.454255: Pseudo dice [0.7745]
2024-12-07 04:32:47.454983: Epoch time: 378.52 s
2024-12-07 04:32:47.455738: Yayy! New best EMA pseudo Dice: 0.7601
2024-12-07 04:32:49.324154: 
2024-12-07 04:32:49.325475: Epoch 79
2024-12-07 04:32:49.326289: Current learning rate: 0.00929
2024-12-07 04:39:06.808026: Validation loss did not improve from -0.56576. Patience: 6/50
2024-12-07 04:39:06.809105: train_loss -0.6908
2024-12-07 04:39:06.810076: val_loss -0.5321
2024-12-07 04:39:06.810797: Pseudo dice [0.7703]
2024-12-07 04:39:06.811444: Epoch time: 377.49 s
2024-12-07 04:39:07.210347: Yayy! New best EMA pseudo Dice: 0.7611
2024-12-07 04:39:09.090467: 
2024-12-07 04:39:09.091773: Epoch 80
2024-12-07 04:39:09.092595: Current learning rate: 0.00928
2024-12-07 04:45:14.283990: Validation loss did not improve from -0.56576. Patience: 7/50
2024-12-07 04:45:14.284997: train_loss -0.6795
2024-12-07 04:45:14.285681: val_loss -0.527
2024-12-07 04:45:14.286283: Pseudo dice [0.7598]
2024-12-07 04:45:14.287101: Epoch time: 365.2 s
2024-12-07 04:45:15.750767: 
2024-12-07 04:45:15.751921: Epoch 81
2024-12-07 04:45:15.752662: Current learning rate: 0.00927
2024-12-07 04:51:12.217771: Validation loss did not improve from -0.56576. Patience: 8/50
2024-12-07 04:51:12.218759: train_loss -0.6811
2024-12-07 04:51:12.219615: val_loss -0.5068
2024-12-07 04:51:12.220393: Pseudo dice [0.766]
2024-12-07 04:51:12.221235: Epoch time: 356.47 s
2024-12-07 04:51:12.222069: Yayy! New best EMA pseudo Dice: 0.7615
2024-12-07 04:51:14.148531: 
2024-12-07 04:51:14.149776: Epoch 82
2024-12-07 04:51:14.150579: Current learning rate: 0.00926
2024-12-07 04:57:05.329149: Validation loss did not improve from -0.56576. Patience: 9/50
2024-12-07 04:57:05.334401: train_loss -0.6843
2024-12-07 04:57:05.336325: val_loss -0.4483
2024-12-07 04:57:05.337509: Pseudo dice [0.7171]
2024-12-07 04:57:05.338677: Epoch time: 351.19 s
2024-12-07 04:57:07.548447: 
2024-12-07 04:57:07.549945: Epoch 83
2024-12-07 04:57:07.550974: Current learning rate: 0.00925
2024-12-07 05:03:12.407458: Validation loss did not improve from -0.56576. Patience: 10/50
2024-12-07 05:03:12.408555: train_loss -0.6681
2024-12-07 05:03:12.409524: val_loss -0.4976
2024-12-07 05:03:12.410298: Pseudo dice [0.7472]
2024-12-07 05:03:12.411331: Epoch time: 364.86 s
2024-12-07 05:03:13.917192: 
2024-12-07 05:03:13.918578: Epoch 84
2024-12-07 05:03:13.919741: Current learning rate: 0.00924
2024-12-07 05:09:08.795508: Validation loss did not improve from -0.56576. Patience: 11/50
2024-12-07 05:09:08.798004: train_loss -0.6809
2024-12-07 05:09:08.799408: val_loss -0.5313
2024-12-07 05:09:08.800424: Pseudo dice [0.7751]
2024-12-07 05:09:08.801319: Epoch time: 354.88 s
2024-12-07 05:09:10.667086: 
2024-12-07 05:09:10.668521: Epoch 85
2024-12-07 05:09:10.669539: Current learning rate: 0.00923
2024-12-07 05:15:10.906340: Validation loss did not improve from -0.56576. Patience: 12/50
2024-12-07 05:15:10.908232: train_loss -0.681
2024-12-07 05:15:10.909391: val_loss -0.5234
2024-12-07 05:15:10.910084: Pseudo dice [0.7621]
2024-12-07 05:15:10.910851: Epoch time: 360.24 s
2024-12-07 05:15:12.315436: 
2024-12-07 05:15:12.317405: Epoch 86
2024-12-07 05:15:12.318299: Current learning rate: 0.00922
2024-12-07 05:21:29.078255: Validation loss did not improve from -0.56576. Patience: 13/50
2024-12-07 05:21:29.079306: train_loss -0.6953
2024-12-07 05:21:29.080214: val_loss -0.4952
2024-12-07 05:21:29.081027: Pseudo dice [0.7591]
2024-12-07 05:21:29.081667: Epoch time: 376.77 s
2024-12-07 05:21:30.471087: 
2024-12-07 05:21:30.472445: Epoch 87
2024-12-07 05:21:30.473237: Current learning rate: 0.00921
2024-12-07 05:28:00.429134: Validation loss did not improve from -0.56576. Patience: 14/50
2024-12-07 05:28:00.429895: train_loss -0.701
2024-12-07 05:28:00.430816: val_loss -0.5271
2024-12-07 05:28:00.431599: Pseudo dice [0.7755]
2024-12-07 05:28:00.432332: Epoch time: 389.96 s
2024-12-07 05:28:01.881037: 
2024-12-07 05:28:01.882665: Epoch 88
2024-12-07 05:28:01.883840: Current learning rate: 0.0092
2024-12-07 05:34:38.040947: Validation loss did not improve from -0.56576. Patience: 15/50
2024-12-07 05:34:38.041960: train_loss -0.6883
2024-12-07 05:34:38.042749: val_loss -0.501
2024-12-07 05:34:38.043446: Pseudo dice [0.7468]
2024-12-07 05:34:38.044222: Epoch time: 396.16 s
2024-12-07 05:34:39.566313: 
2024-12-07 05:34:39.567554: Epoch 89
2024-12-07 05:34:39.568489: Current learning rate: 0.0092
2024-12-07 05:40:53.628841: Validation loss did not improve from -0.56576. Patience: 16/50
2024-12-07 05:40:53.629843: train_loss -0.6908
2024-12-07 05:40:53.630836: val_loss -0.5221
2024-12-07 05:40:53.631632: Pseudo dice [0.7751]
2024-12-07 05:40:53.632481: Epoch time: 374.07 s
2024-12-07 05:40:55.479237: 
2024-12-07 05:40:55.480551: Epoch 90
2024-12-07 05:40:55.481512: Current learning rate: 0.00919
2024-12-07 05:47:01.837957: Validation loss did not improve from -0.56576. Patience: 17/50
2024-12-07 05:47:01.838799: train_loss -0.6859
2024-12-07 05:47:01.839774: val_loss -0.4886
2024-12-07 05:47:01.840744: Pseudo dice [0.7517]
2024-12-07 05:47:01.841699: Epoch time: 366.36 s
2024-12-07 05:47:03.303301: 
2024-12-07 05:47:03.304878: Epoch 91
2024-12-07 05:47:03.306033: Current learning rate: 0.00918
2024-12-07 05:53:08.639038: Validation loss did not improve from -0.56576. Patience: 18/50
2024-12-07 05:53:08.640125: train_loss -0.694
2024-12-07 05:53:08.641266: val_loss -0.5442
2024-12-07 05:53:08.642227: Pseudo dice [0.7796]
2024-12-07 05:53:08.643253: Epoch time: 365.34 s
2024-12-07 05:53:08.644208: Yayy! New best EMA pseudo Dice: 0.7616
2024-12-07 05:53:10.442537: 
2024-12-07 05:53:10.444164: Epoch 92
2024-12-07 05:53:10.445137: Current learning rate: 0.00917
2024-12-07 05:59:23.788393: Validation loss did not improve from -0.56576. Patience: 19/50
2024-12-07 05:59:23.789169: train_loss -0.6946
2024-12-07 05:59:23.790049: val_loss -0.4845
2024-12-07 05:59:23.790788: Pseudo dice [0.7472]
2024-12-07 05:59:23.791942: Epoch time: 373.35 s
2024-12-07 05:59:25.181723: 
2024-12-07 05:59:25.183115: Epoch 93
2024-12-07 05:59:25.184173: Current learning rate: 0.00916
2024-12-07 06:05:37.793358: Validation loss did not improve from -0.56576. Patience: 20/50
2024-12-07 06:05:37.794468: train_loss -0.7024
2024-12-07 06:05:37.795425: val_loss -0.4216
2024-12-07 06:05:37.796256: Pseudo dice [0.7127]
2024-12-07 06:05:37.797044: Epoch time: 372.61 s
2024-12-07 06:05:39.937301: 
2024-12-07 06:05:39.938663: Epoch 94
2024-12-07 06:05:39.939481: Current learning rate: 0.00915
2024-12-07 06:11:49.189342: Validation loss did not improve from -0.56576. Patience: 21/50
2024-12-07 06:11:49.190330: train_loss -0.7042
2024-12-07 06:11:49.191192: val_loss -0.5334
2024-12-07 06:11:49.191991: Pseudo dice [0.7627]
2024-12-07 06:11:49.192698: Epoch time: 369.25 s
2024-12-07 06:11:50.998780: 
2024-12-07 06:11:51.000127: Epoch 95
2024-12-07 06:11:51.001030: Current learning rate: 0.00914
2024-12-07 06:18:13.418209: Validation loss did not improve from -0.56576. Patience: 22/50
2024-12-07 06:18:13.419206: train_loss -0.7046
2024-12-07 06:18:13.419937: val_loss -0.5249
2024-12-07 06:18:13.420632: Pseudo dice [0.7627]
2024-12-07 06:18:13.421428: Epoch time: 382.42 s
2024-12-07 06:18:14.805735: 
2024-12-07 06:18:14.807063: Epoch 96
2024-12-07 06:18:14.807988: Current learning rate: 0.00913
2024-12-07 06:24:44.068948: Validation loss did not improve from -0.56576. Patience: 23/50
2024-12-07 06:24:44.070079: train_loss -0.7125
2024-12-07 06:24:44.070900: val_loss -0.5412
2024-12-07 06:24:44.071781: Pseudo dice [0.7767]
2024-12-07 06:24:44.072850: Epoch time: 389.27 s
2024-12-07 06:24:45.539968: 
2024-12-07 06:24:45.543200: Epoch 97
2024-12-07 06:24:45.544814: Current learning rate: 0.00912
2024-12-07 06:31:21.661838: Validation loss did not improve from -0.56576. Patience: 24/50
2024-12-07 06:31:21.662745: train_loss -0.7041
2024-12-07 06:31:21.663720: val_loss -0.5077
2024-12-07 06:31:21.664504: Pseudo dice [0.7455]
2024-12-07 06:31:21.665169: Epoch time: 396.13 s
2024-12-07 06:31:23.105470: 
2024-12-07 06:31:23.107390: Epoch 98
2024-12-07 06:31:23.108384: Current learning rate: 0.00911
2024-12-07 06:37:51.346105: Validation loss did not improve from -0.56576. Patience: 25/50
2024-12-07 06:37:51.347662: train_loss -0.7046
2024-12-07 06:37:51.349290: val_loss -0.4477
2024-12-07 06:37:51.350228: Pseudo dice [0.7277]
2024-12-07 06:37:51.351619: Epoch time: 388.24 s
2024-12-07 06:37:52.789917: 
2024-12-07 06:37:52.791271: Epoch 99
2024-12-07 06:37:52.791972: Current learning rate: 0.0091
2024-12-07 06:43:54.923550: Validation loss did not improve from -0.56576. Patience: 26/50
2024-12-07 06:43:54.924604: train_loss -0.7163
2024-12-07 06:43:54.925487: val_loss -0.544
2024-12-07 06:43:54.926273: Pseudo dice [0.771]
2024-12-07 06:43:54.926982: Epoch time: 362.14 s
2024-12-07 06:43:56.778404: 
2024-12-07 06:43:56.779675: Epoch 100
2024-12-07 06:43:56.780366: Current learning rate: 0.0091
2024-12-07 06:49:58.995440: Validation loss did not improve from -0.56576. Patience: 27/50
2024-12-07 06:49:58.996472: train_loss -0.7138
2024-12-07 06:49:58.997231: val_loss -0.5178
2024-12-07 06:49:58.997988: Pseudo dice [0.7663]
2024-12-07 06:49:58.998725: Epoch time: 362.22 s
2024-12-07 06:50:00.430533: 
2024-12-07 06:50:00.431662: Epoch 101
2024-12-07 06:50:00.432340: Current learning rate: 0.00909
2024-12-07 06:55:56.972711: Validation loss did not improve from -0.56576. Patience: 28/50
2024-12-07 06:55:56.973837: train_loss -0.7125
2024-12-07 06:55:56.975043: val_loss -0.509
2024-12-07 06:55:56.976087: Pseudo dice [0.7579]
2024-12-07 06:55:56.977011: Epoch time: 356.54 s
2024-12-07 06:55:58.392145: 
2024-12-07 06:55:58.393698: Epoch 102
2024-12-07 06:55:58.394704: Current learning rate: 0.00908
2024-12-07 07:02:07.484805: Validation loss did not improve from -0.56576. Patience: 29/50
2024-12-07 07:02:07.485933: train_loss -0.7145
2024-12-07 07:02:07.486884: val_loss -0.4571
2024-12-07 07:02:07.487856: Pseudo dice [0.7357]
2024-12-07 07:02:07.488811: Epoch time: 369.1 s
2024-12-07 07:02:08.906065: 
2024-12-07 07:02:08.907535: Epoch 103
2024-12-07 07:02:08.908603: Current learning rate: 0.00907
2024-12-07 07:08:22.143814: Validation loss did not improve from -0.56576. Patience: 30/50
2024-12-07 07:08:22.147451: train_loss -0.7157
2024-12-07 07:08:22.149550: val_loss -0.4893
2024-12-07 07:08:22.150512: Pseudo dice [0.7262]
2024-12-07 07:08:22.151575: Epoch time: 373.24 s
2024-12-07 07:08:23.587486: 
2024-12-07 07:08:23.589237: Epoch 104
2024-12-07 07:08:23.590005: Current learning rate: 0.00906
2024-12-07 07:14:37.404729: Validation loss did not improve from -0.56576. Patience: 31/50
2024-12-07 07:14:37.405768: train_loss -0.7107
2024-12-07 07:14:37.406632: val_loss -0.5373
2024-12-07 07:14:37.407482: Pseudo dice [0.7733]
2024-12-07 07:14:37.408340: Epoch time: 373.82 s
2024-12-07 07:14:39.683286: 
2024-12-07 07:14:39.684613: Epoch 105
2024-12-07 07:14:39.685674: Current learning rate: 0.00905
2024-12-07 07:20:46.167209: Validation loss did not improve from -0.56576. Patience: 32/50
2024-12-07 07:20:46.168153: train_loss -0.7118
2024-12-07 07:20:46.169177: val_loss -0.5046
2024-12-07 07:20:46.170129: Pseudo dice [0.7557]
2024-12-07 07:20:46.171124: Epoch time: 366.49 s
2024-12-07 07:20:47.566389: 
2024-12-07 07:20:47.567859: Epoch 106
2024-12-07 07:20:47.568867: Current learning rate: 0.00904
2024-12-07 07:26:42.325637: Validation loss did not improve from -0.56576. Patience: 33/50
2024-12-07 07:26:42.326637: train_loss -0.7092
2024-12-07 07:26:42.327461: val_loss -0.5067
2024-12-07 07:26:42.328234: Pseudo dice [0.7622]
2024-12-07 07:26:42.328881: Epoch time: 354.76 s
2024-12-07 07:26:43.765582: 
2024-12-07 07:26:43.766933: Epoch 107
2024-12-07 07:26:43.767857: Current learning rate: 0.00903
2024-12-07 07:32:54.187961: Validation loss did not improve from -0.56576. Patience: 34/50
2024-12-07 07:32:54.189287: train_loss -0.7211
2024-12-07 07:32:54.190489: val_loss -0.4719
2024-12-07 07:32:54.191315: Pseudo dice [0.7458]
2024-12-07 07:32:54.192097: Epoch time: 370.43 s
2024-12-07 07:32:55.625699: 
2024-12-07 07:32:55.627329: Epoch 108
2024-12-07 07:32:55.628365: Current learning rate: 0.00902
2024-12-07 07:39:09.720613: Validation loss did not improve from -0.56576. Patience: 35/50
2024-12-07 07:39:09.721521: train_loss -0.7168
2024-12-07 07:39:09.722474: val_loss -0.5323
2024-12-07 07:39:09.723345: Pseudo dice [0.773]
2024-12-07 07:39:09.724291: Epoch time: 374.1 s
2024-12-07 07:39:11.148037: 
2024-12-07 07:39:11.149433: Epoch 109
2024-12-07 07:39:11.150406: Current learning rate: 0.00901
2024-12-07 07:45:39.294651: Validation loss did not improve from -0.56576. Patience: 36/50
2024-12-07 07:45:39.295340: train_loss -0.7236
2024-12-07 07:45:39.296142: val_loss -0.5295
2024-12-07 07:45:39.297354: Pseudo dice [0.7705]
2024-12-07 07:45:39.298218: Epoch time: 388.15 s
2024-12-07 07:45:41.176767: 
2024-12-07 07:45:41.178193: Epoch 110
2024-12-07 07:45:41.178929: Current learning rate: 0.009
2024-12-07 07:52:12.762705: Validation loss did not improve from -0.56576. Patience: 37/50
2024-12-07 07:52:12.763644: train_loss -0.7214
2024-12-07 07:52:12.764378: val_loss -0.513
2024-12-07 07:52:12.764979: Pseudo dice [0.7638]
2024-12-07 07:52:12.765671: Epoch time: 391.59 s
2024-12-07 07:52:14.155960: 
2024-12-07 07:52:14.157385: Epoch 111
2024-12-07 07:52:14.158657: Current learning rate: 0.009
2024-12-07 07:58:51.599719: Validation loss did not improve from -0.56576. Patience: 38/50
2024-12-07 07:58:51.600954: train_loss -0.7258
2024-12-07 07:58:51.601942: val_loss -0.5332
2024-12-07 07:58:51.602734: Pseudo dice [0.7621]
2024-12-07 07:58:51.603775: Epoch time: 397.45 s
2024-12-07 07:58:52.998646: 
2024-12-07 07:58:52.999788: Epoch 112
2024-12-07 07:58:53.000499: Current learning rate: 0.00899
2024-12-07 08:05:24.069649: Validation loss did not improve from -0.56576. Patience: 39/50
2024-12-07 08:05:24.070631: train_loss -0.7233
2024-12-07 08:05:24.071682: val_loss -0.4885
2024-12-07 08:05:24.072605: Pseudo dice [0.7494]
2024-12-07 08:05:24.073512: Epoch time: 391.07 s
2024-12-07 08:05:25.463159: 
2024-12-07 08:05:25.464513: Epoch 113
2024-12-07 08:05:25.465961: Current learning rate: 0.00898
2024-12-07 08:12:00.942479: Validation loss did not improve from -0.56576. Patience: 40/50
2024-12-07 08:12:00.947479: train_loss -0.725
2024-12-07 08:12:00.948925: val_loss -0.5162
2024-12-07 08:12:00.949870: Pseudo dice [0.7663]
2024-12-07 08:12:00.950926: Epoch time: 395.49 s
2024-12-07 08:12:02.390224: 
2024-12-07 08:12:02.391677: Epoch 114
2024-12-07 08:12:02.392589: Current learning rate: 0.00897
2024-12-07 08:18:38.479103: Validation loss did not improve from -0.56576. Patience: 41/50
2024-12-07 08:18:38.480832: train_loss -0.7253
2024-12-07 08:18:38.481829: val_loss -0.5091
2024-12-07 08:18:38.482727: Pseudo dice [0.7598]
2024-12-07 08:18:38.483731: Epoch time: 396.09 s
2024-12-07 08:18:41.126844: 
2024-12-07 08:18:41.128186: Epoch 115
2024-12-07 08:18:41.129106: Current learning rate: 0.00896
2024-12-07 08:25:16.508090: Validation loss did not improve from -0.56576. Patience: 42/50
2024-12-07 08:25:16.509192: train_loss -0.7268
2024-12-07 08:25:16.510156: val_loss -0.5465
2024-12-07 08:25:16.510963: Pseudo dice [0.7801]
2024-12-07 08:25:16.511769: Epoch time: 395.38 s
2024-12-07 08:25:17.981796: 
2024-12-07 08:25:17.983129: Epoch 116
2024-12-07 08:25:17.984201: Current learning rate: 0.00895
2024-12-07 08:31:54.588013: Validation loss did not improve from -0.56576. Patience: 43/50
2024-12-07 08:31:54.589118: train_loss -0.724
2024-12-07 08:31:54.589913: val_loss -0.4885
2024-12-07 08:31:54.590518: Pseudo dice [0.76]
2024-12-07 08:31:54.591182: Epoch time: 396.61 s
2024-12-07 08:31:56.029370: 
2024-12-07 08:31:56.030697: Epoch 117
2024-12-07 08:31:56.031384: Current learning rate: 0.00894
2024-12-07 08:38:32.247209: Validation loss did not improve from -0.56576. Patience: 44/50
2024-12-07 08:38:32.248344: train_loss -0.7218
2024-12-07 08:38:32.249326: val_loss -0.5518
2024-12-07 08:38:32.250162: Pseudo dice [0.774]
2024-12-07 08:38:32.251039: Epoch time: 396.22 s
2024-12-07 08:38:32.251832: Yayy! New best EMA pseudo Dice: 0.762
2024-12-07 08:38:34.120315: 
2024-12-07 08:38:34.121859: Epoch 118
2024-12-07 08:38:34.122832: Current learning rate: 0.00893
2024-12-07 08:45:20.385524: Validation loss did not improve from -0.56576. Patience: 45/50
2024-12-07 08:45:20.386564: train_loss -0.7296
2024-12-07 08:45:20.387319: val_loss -0.4758
2024-12-07 08:45:20.388019: Pseudo dice [0.7397]
2024-12-07 08:45:20.388684: Epoch time: 406.27 s
2024-12-07 08:45:21.833966: 
2024-12-07 08:45:21.835573: Epoch 119
2024-12-07 08:45:21.836372: Current learning rate: 0.00892
2024-12-07 08:51:56.396133: Validation loss did not improve from -0.56576. Patience: 46/50
2024-12-07 08:51:56.397178: train_loss -0.7193
2024-12-07 08:51:56.398227: val_loss -0.5165
2024-12-07 08:51:56.398974: Pseudo dice [0.7691]
2024-12-07 08:51:56.399846: Epoch time: 394.56 s
2024-12-07 08:51:58.299110: 
2024-12-07 08:51:58.300505: Epoch 120
2024-12-07 08:51:58.301375: Current learning rate: 0.00891
2024-12-07 08:58:18.040932: Validation loss did not improve from -0.56576. Patience: 47/50
2024-12-07 08:58:18.042160: train_loss -0.7225
2024-12-07 08:58:18.043145: val_loss -0.5219
2024-12-07 08:58:18.043850: Pseudo dice [0.7741]
2024-12-07 08:58:18.044637: Epoch time: 379.74 s
2024-12-07 08:58:18.045370: Yayy! New best EMA pseudo Dice: 0.7621
2024-12-07 08:58:19.906928: 
2024-12-07 08:58:19.908477: Epoch 121
2024-12-07 08:58:19.909669: Current learning rate: 0.0089
2024-12-07 09:04:44.068049: Validation loss did not improve from -0.56576. Patience: 48/50
2024-12-07 09:04:44.069118: train_loss -0.7324
2024-12-07 09:04:44.070201: val_loss -0.5023
2024-12-07 09:04:44.071260: Pseudo dice [0.7653]
2024-12-07 09:04:44.072274: Epoch time: 384.16 s
2024-12-07 09:04:44.073202: Yayy! New best EMA pseudo Dice: 0.7624
2024-12-07 09:04:46.077409: 
2024-12-07 09:04:46.079154: Epoch 122
2024-12-07 09:04:46.080342: Current learning rate: 0.00889
2024-12-07 09:11:07.183708: Validation loss did not improve from -0.56576. Patience: 49/50
2024-12-07 09:11:07.184749: train_loss -0.731
2024-12-07 09:11:07.185697: val_loss -0.461
2024-12-07 09:11:07.186456: Pseudo dice [0.7386]
2024-12-07 09:11:07.187298: Epoch time: 381.11 s
2024-12-07 09:11:08.708570: 
2024-12-07 09:11:08.710738: Epoch 123
2024-12-07 09:11:08.712045: Current learning rate: 0.00889
2024-12-07 09:17:42.890153: Validation loss did not improve from -0.56576. Patience: 50/50
2024-12-07 09:17:42.893973: train_loss -0.7282
2024-12-07 09:17:42.895881: val_loss -0.5215
2024-12-07 09:17:42.896714: Pseudo dice [0.7633]
2024-12-07 09:17:42.897671: Epoch time: 394.19 s
2024-12-07 09:17:44.609368: Patience reached. Stopping training.
2024-12-07 09:17:45.123051: Training done.
2024-12-07 09:17:45.586481: Using splits from existing split file: /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset307_Sohee_Calcium_OCT_CrossValidation/splits_final.json
2024-12-07 09:17:45.590398: The split file contains 5 splits.
2024-12-07 09:17:45.591660: Desired fold for training: 0
2024-12-07 09:17:45.592950: This split has 6 training and 2 validation cases.
2024-12-07 09:17:45.594259: predicting 106-002
2024-12-07 09:17:45.746389: 106-002, shape torch.Size([1, 539, 498, 498]), rank 0
2024-12-07 09:21:36.889381: predicting 706-005
2024-12-07 09:21:36.920170: 706-005, shape torch.Size([1, 375, 498, 498]), rank 0
2024-12-07 09:24:00.184335: Validation complete
2024-12-07 09:24:00.185026: Mean Validation Dice:  0.7746122491085539
2024-12-07 01:41:24.996583: unpacking done...
2024-12-07 01:41:25.127200: Unable to plot network architecture: nnUNet_compile is enabled!
2024-12-07 01:41:25.634983: 
2024-12-07 01:41:25.636049: Epoch 0
2024-12-07 01:41:25.637139: Current learning rate: 0.01
2024-12-07 01:44:49.010709: Validation loss improved from 1000.00000 to -0.10752! Patience: 0/50
2024-12-07 01:44:49.012850: train_loss -0.0483
2024-12-07 01:44:49.013793: val_loss -0.1075
2024-12-07 01:44:49.014654: Pseudo dice [0.4721]
2024-12-07 01:44:49.015608: Epoch time: 203.38 s
2024-12-07 01:44:49.016493: Yayy! New best EMA pseudo Dice: 0.4721
2024-12-07 01:44:51.363068: 
2024-12-07 01:44:51.364403: Epoch 1
2024-12-07 01:44:51.365119: Current learning rate: 0.00999
2024-12-07 01:46:17.525240: Validation loss improved from -0.10752 to -0.18877! Patience: 0/50
2024-12-07 01:46:17.526140: train_loss -0.2292
2024-12-07 01:46:17.526837: val_loss -0.1888
2024-12-07 01:46:17.527442: Pseudo dice [0.5309]
2024-12-07 01:46:17.528052: Epoch time: 86.16 s
2024-12-07 01:46:17.528672: Yayy! New best EMA pseudo Dice: 0.478
2024-12-07 01:46:19.152314: 
2024-12-07 01:46:19.153934: Epoch 2
2024-12-07 01:46:19.155336: Current learning rate: 0.00998
2024-12-07 01:47:45.368520: Validation loss did not improve from -0.18877. Patience: 1/50
2024-12-07 01:47:45.369643: train_loss -0.2645
2024-12-07 01:47:45.370508: val_loss -0.1835
2024-12-07 01:47:45.371215: Pseudo dice [0.529]
2024-12-07 01:47:45.372097: Epoch time: 86.22 s
2024-12-07 01:47:45.373068: Yayy! New best EMA pseudo Dice: 0.4831
2024-12-07 01:47:47.149562: 
2024-12-07 01:47:47.151141: Epoch 3
2024-12-07 01:47:47.151803: Current learning rate: 0.00997
2024-12-07 01:49:13.443178: Validation loss improved from -0.18877 to -0.22627! Patience: 1/50
2024-12-07 01:49:13.443957: train_loss -0.3152
2024-12-07 01:49:13.444787: val_loss -0.2263
2024-12-07 01:49:13.445452: Pseudo dice [0.5591]
2024-12-07 01:49:13.446107: Epoch time: 86.3 s
2024-12-07 01:49:13.446770: Yayy! New best EMA pseudo Dice: 0.4907
2024-12-07 01:49:15.144321: 
2024-12-07 01:49:15.145939: Epoch 4
2024-12-07 01:49:15.146609: Current learning rate: 0.00996
2024-12-07 01:50:41.359749: Validation loss improved from -0.22627 to -0.23818! Patience: 0/50
2024-12-07 01:50:41.361210: train_loss -0.3392
2024-12-07 01:50:41.362571: val_loss -0.2382
2024-12-07 01:50:41.363399: Pseudo dice [0.568]
2024-12-07 01:50:41.364299: Epoch time: 86.22 s
2024-12-07 01:50:41.700173: Yayy! New best EMA pseudo Dice: 0.4984
2024-12-07 01:50:43.474462: 
2024-12-07 01:50:43.475431: Epoch 5
2024-12-07 01:50:43.476232: Current learning rate: 0.00995
2024-12-07 01:52:09.764324: Validation loss did not improve from -0.23818. Patience: 1/50
2024-12-07 01:52:09.765419: train_loss -0.3605
2024-12-07 01:52:09.766338: val_loss -0.2276
2024-12-07 01:52:09.767119: Pseudo dice [0.5533]
2024-12-07 01:52:09.768051: Epoch time: 86.29 s
2024-12-07 01:52:09.768961: Yayy! New best EMA pseudo Dice: 0.5039
2024-12-07 01:52:11.440700: 
2024-12-07 01:52:11.442161: Epoch 6
2024-12-07 01:52:11.442934: Current learning rate: 0.00995
2024-12-07 01:53:37.717640: Validation loss improved from -0.23818 to -0.27312! Patience: 1/50
2024-12-07 01:53:37.718517: train_loss -0.3774
2024-12-07 01:53:37.719297: val_loss -0.2731
2024-12-07 01:53:37.719987: Pseudo dice [0.5618]
2024-12-07 01:53:37.720687: Epoch time: 86.28 s
2024-12-07 01:53:37.721453: Yayy! New best EMA pseudo Dice: 0.5097
2024-12-07 01:53:39.385705: 
2024-12-07 01:53:39.387149: Epoch 7
2024-12-07 01:53:39.387958: Current learning rate: 0.00994
2024-12-07 01:55:05.550923: Validation loss improved from -0.27312 to -0.31398! Patience: 0/50
2024-12-07 01:55:05.552144: train_loss -0.3818
2024-12-07 01:55:05.553049: val_loss -0.314
2024-12-07 01:55:05.553905: Pseudo dice [0.6013]
2024-12-07 01:55:05.554602: Epoch time: 86.17 s
2024-12-07 01:55:05.555322: Yayy! New best EMA pseudo Dice: 0.5189
2024-12-07 01:55:07.771774: 
2024-12-07 01:55:07.773326: Epoch 8
2024-12-07 01:55:07.774386: Current learning rate: 0.00993
2024-12-07 01:56:34.111516: Validation loss improved from -0.31398 to -0.37440! Patience: 0/50
2024-12-07 01:56:34.112604: train_loss -0.4159
2024-12-07 01:56:34.113616: val_loss -0.3744
2024-12-07 01:56:34.114475: Pseudo dice [0.639]
2024-12-07 01:56:34.115427: Epoch time: 86.34 s
2024-12-07 01:56:34.116376: Yayy! New best EMA pseudo Dice: 0.5309
2024-12-07 01:56:35.847877: 
2024-12-07 01:56:35.849379: Epoch 9
2024-12-07 01:56:35.850444: Current learning rate: 0.00992
2024-12-07 01:58:02.150090: Validation loss did not improve from -0.37440. Patience: 1/50
2024-12-07 01:58:02.151199: train_loss -0.4298
2024-12-07 01:58:02.152151: val_loss -0.353
2024-12-07 01:58:02.152842: Pseudo dice [0.6275]
2024-12-07 01:58:02.153506: Epoch time: 86.3 s
2024-12-07 01:58:02.549211: Yayy! New best EMA pseudo Dice: 0.5405
2024-12-07 01:58:04.254611: 
2024-12-07 01:58:04.256274: Epoch 10
2024-12-07 01:58:04.257139: Current learning rate: 0.00991
2024-12-07 01:59:30.651476: Validation loss did not improve from -0.37440. Patience: 2/50
2024-12-07 01:59:30.652842: train_loss -0.4446
2024-12-07 01:59:30.653809: val_loss -0.3744
2024-12-07 01:59:30.654433: Pseudo dice [0.6333]
2024-12-07 01:59:30.655186: Epoch time: 86.4 s
2024-12-07 01:59:30.655853: Yayy! New best EMA pseudo Dice: 0.5498
2024-12-07 01:59:32.372827: 
2024-12-07 01:59:32.374610: Epoch 11
2024-12-07 01:59:32.375627: Current learning rate: 0.0099
2024-12-07 02:00:58.692269: Validation loss did not improve from -0.37440. Patience: 3/50
2024-12-07 02:00:58.693347: train_loss -0.4403
2024-12-07 02:00:58.694145: val_loss -0.3541
2024-12-07 02:00:58.694767: Pseudo dice [0.6256]
2024-12-07 02:00:58.695419: Epoch time: 86.32 s
2024-12-07 02:00:58.696195: Yayy! New best EMA pseudo Dice: 0.5574
2024-12-07 02:01:00.415185: 
2024-12-07 02:01:00.416851: Epoch 12
2024-12-07 02:01:00.417644: Current learning rate: 0.00989
2024-12-07 02:02:26.889596: Validation loss did not improve from -0.37440. Patience: 4/50
2024-12-07 02:02:26.890939: train_loss -0.4697
2024-12-07 02:02:26.892045: val_loss -0.345
2024-12-07 02:02:26.892934: Pseudo dice [0.6332]
2024-12-07 02:02:26.893782: Epoch time: 86.48 s
2024-12-07 02:02:26.894618: Yayy! New best EMA pseudo Dice: 0.565
2024-12-07 02:02:28.555621: 
2024-12-07 02:02:28.557453: Epoch 13
2024-12-07 02:02:28.558680: Current learning rate: 0.00988
2024-12-07 02:03:54.922784: Validation loss improved from -0.37440 to -0.43771! Patience: 4/50
2024-12-07 02:03:54.924039: train_loss -0.4757
2024-12-07 02:03:54.924897: val_loss -0.4377
2024-12-07 02:03:54.925745: Pseudo dice [0.6834]
2024-12-07 02:03:54.926672: Epoch time: 86.37 s
2024-12-07 02:03:54.927439: Yayy! New best EMA pseudo Dice: 0.5768
2024-12-07 02:03:56.590055: 
2024-12-07 02:03:56.592276: Epoch 14
2024-12-07 02:03:56.593178: Current learning rate: 0.00987
2024-12-07 02:05:22.950934: Validation loss did not improve from -0.43771. Patience: 1/50
2024-12-07 02:05:22.952020: train_loss -0.4676
2024-12-07 02:05:22.952894: val_loss -0.4217
2024-12-07 02:05:22.953619: Pseudo dice [0.6736]
2024-12-07 02:05:22.954256: Epoch time: 86.36 s
2024-12-07 02:05:23.351821: Yayy! New best EMA pseudo Dice: 0.5865
2024-12-07 02:05:25.003767: 
2024-12-07 02:05:25.005826: Epoch 15
2024-12-07 02:05:25.006673: Current learning rate: 0.00986
2024-12-07 02:06:51.317676: Validation loss improved from -0.43771 to -0.45293! Patience: 1/50
2024-12-07 02:06:51.318484: train_loss -0.4892
2024-12-07 02:06:51.319427: val_loss -0.4529
2024-12-07 02:06:51.320089: Pseudo dice [0.6946]
2024-12-07 02:06:51.320712: Epoch time: 86.32 s
2024-12-07 02:06:51.321308: Yayy! New best EMA pseudo Dice: 0.5973
2024-12-07 02:06:52.977661: 
2024-12-07 02:06:52.978688: Epoch 16
2024-12-07 02:06:52.979395: Current learning rate: 0.00986
2024-12-07 02:08:19.550453: Validation loss improved from -0.45293 to -0.46557! Patience: 0/50
2024-12-07 02:08:19.551682: train_loss -0.4998
2024-12-07 02:08:19.552395: val_loss -0.4656
2024-12-07 02:08:19.553130: Pseudo dice [0.6915]
2024-12-07 02:08:19.553864: Epoch time: 86.57 s
2024-12-07 02:08:19.554519: Yayy! New best EMA pseudo Dice: 0.6067
2024-12-07 02:08:21.300472: 
2024-12-07 02:08:21.302526: Epoch 17
2024-12-07 02:08:21.303523: Current learning rate: 0.00985
2024-12-07 02:09:47.917505: Validation loss did not improve from -0.46557. Patience: 1/50
2024-12-07 02:09:47.918645: train_loss -0.5005
2024-12-07 02:09:47.919399: val_loss -0.4183
2024-12-07 02:09:47.920075: Pseudo dice [0.6714]
2024-12-07 02:09:47.920807: Epoch time: 86.62 s
2024-12-07 02:09:47.921375: Yayy! New best EMA pseudo Dice: 0.6132
2024-12-07 02:09:49.621693: 
2024-12-07 02:09:49.622951: Epoch 18
2024-12-07 02:09:49.623808: Current learning rate: 0.00984
2024-12-07 02:11:16.164323: Validation loss did not improve from -0.46557. Patience: 2/50
2024-12-07 02:11:16.165350: train_loss -0.5114
2024-12-07 02:11:16.166265: val_loss -0.4392
2024-12-07 02:11:16.167032: Pseudo dice [0.6839]
2024-12-07 02:11:16.167880: Epoch time: 86.54 s
2024-12-07 02:11:16.168535: Yayy! New best EMA pseudo Dice: 0.6203
2024-12-07 02:11:18.274961: 
2024-12-07 02:11:18.276658: Epoch 19
2024-12-07 02:11:18.277539: Current learning rate: 0.00983
2024-12-07 02:12:44.860204: Validation loss did not improve from -0.46557. Patience: 3/50
2024-12-07 02:12:44.861409: train_loss -0.5232
2024-12-07 02:12:44.862284: val_loss -0.4128
2024-12-07 02:12:44.863063: Pseudo dice [0.6721]
2024-12-07 02:12:44.863763: Epoch time: 86.59 s
2024-12-07 02:12:45.244926: Yayy! New best EMA pseudo Dice: 0.6254
2024-12-07 02:12:46.958791: 
2024-12-07 02:12:46.960448: Epoch 20
2024-12-07 02:12:46.961426: Current learning rate: 0.00982
2024-12-07 02:14:13.301461: Validation loss did not improve from -0.46557. Patience: 4/50
2024-12-07 02:14:13.302362: train_loss -0.5288
2024-12-07 02:14:13.303108: val_loss -0.434
2024-12-07 02:14:13.303772: Pseudo dice [0.6827]
2024-12-07 02:14:13.304527: Epoch time: 86.34 s
2024-12-07 02:14:13.305252: Yayy! New best EMA pseudo Dice: 0.6312
2024-12-07 02:14:14.976109: 
2024-12-07 02:14:14.977950: Epoch 21
2024-12-07 02:14:14.978962: Current learning rate: 0.00981
2024-12-07 02:15:41.301030: Validation loss did not improve from -0.46557. Patience: 5/50
2024-12-07 02:15:41.302249: train_loss -0.5223
2024-12-07 02:15:41.303298: val_loss -0.4267
2024-12-07 02:15:41.304061: Pseudo dice [0.6776]
2024-12-07 02:15:41.304958: Epoch time: 86.33 s
2024-12-07 02:15:41.305720: Yayy! New best EMA pseudo Dice: 0.6358
2024-12-07 02:15:42.972437: 
2024-12-07 02:15:42.974124: Epoch 22
2024-12-07 02:15:42.975041: Current learning rate: 0.0098
2024-12-07 02:17:09.317137: Validation loss improved from -0.46557 to -0.47835! Patience: 5/50
2024-12-07 02:17:09.317943: train_loss -0.5384
2024-12-07 02:17:09.318962: val_loss -0.4783
2024-12-07 02:17:09.319655: Pseudo dice [0.7]
2024-12-07 02:17:09.320351: Epoch time: 86.35 s
2024-12-07 02:17:09.320988: Yayy! New best EMA pseudo Dice: 0.6422
2024-12-07 02:17:10.983834: 
2024-12-07 02:17:10.985512: Epoch 23
2024-12-07 02:17:10.986333: Current learning rate: 0.00979
2024-12-07 02:18:37.497750: Validation loss did not improve from -0.47835. Patience: 1/50
2024-12-07 02:18:37.498956: train_loss -0.5446
2024-12-07 02:18:37.499799: val_loss -0.4511
2024-12-07 02:18:37.500625: Pseudo dice [0.6849]
2024-12-07 02:18:37.501462: Epoch time: 86.52 s
2024-12-07 02:18:37.502264: Yayy! New best EMA pseudo Dice: 0.6465
2024-12-07 02:18:39.125217: 
2024-12-07 02:18:39.127411: Epoch 24
2024-12-07 02:18:39.128293: Current learning rate: 0.00978
2024-12-07 02:20:05.539855: Validation loss improved from -0.47835 to -0.47929! Patience: 1/50
2024-12-07 02:20:05.540886: train_loss -0.5542
2024-12-07 02:20:05.542051: val_loss -0.4793
2024-12-07 02:20:05.542872: Pseudo dice [0.7035]
2024-12-07 02:20:05.543828: Epoch time: 86.42 s
2024-12-07 02:20:05.935063: Yayy! New best EMA pseudo Dice: 0.6522
2024-12-07 02:20:07.575964: 
2024-12-07 02:20:07.577721: Epoch 25
2024-12-07 02:20:07.578795: Current learning rate: 0.00977
2024-12-07 02:21:33.893489: Validation loss did not improve from -0.47929. Patience: 1/50
2024-12-07 02:21:33.894742: train_loss -0.5537
2024-12-07 02:21:33.895926: val_loss -0.4545
2024-12-07 02:21:33.896729: Pseudo dice [0.6855]
2024-12-07 02:21:33.897444: Epoch time: 86.32 s
2024-12-07 02:21:33.898183: Yayy! New best EMA pseudo Dice: 0.6555
2024-12-07 02:21:35.540350: 
2024-12-07 02:21:35.542138: Epoch 26
2024-12-07 02:21:35.543311: Current learning rate: 0.00977
2024-12-07 02:23:01.884004: Validation loss did not improve from -0.47929. Patience: 2/50
2024-12-07 02:23:01.885443: train_loss -0.5508
2024-12-07 02:23:01.886527: val_loss -0.426
2024-12-07 02:23:01.887315: Pseudo dice [0.6708]
2024-12-07 02:23:01.888024: Epoch time: 86.35 s
2024-12-07 02:23:01.888717: Yayy! New best EMA pseudo Dice: 0.657
2024-12-07 02:23:03.532445: 
2024-12-07 02:23:03.534106: Epoch 27
2024-12-07 02:23:03.534750: Current learning rate: 0.00976
2024-12-07 02:24:29.937349: Validation loss did not improve from -0.47929. Patience: 3/50
2024-12-07 02:24:29.938664: train_loss -0.5528
2024-12-07 02:24:29.939684: val_loss -0.4375
2024-12-07 02:24:29.940370: Pseudo dice [0.6808]
2024-12-07 02:24:29.941200: Epoch time: 86.41 s
2024-12-07 02:24:29.941945: Yayy! New best EMA pseudo Dice: 0.6594
2024-12-07 02:24:31.552213: 
2024-12-07 02:24:31.553941: Epoch 28
2024-12-07 02:24:31.554804: Current learning rate: 0.00975
2024-12-07 02:25:58.025285: Validation loss did not improve from -0.47929. Patience: 4/50
2024-12-07 02:25:58.026069: train_loss -0.5622
2024-12-07 02:25:58.026885: val_loss -0.4621
2024-12-07 02:25:58.027671: Pseudo dice [0.6883]
2024-12-07 02:25:58.028445: Epoch time: 86.47 s
2024-12-07 02:25:58.029215: Yayy! New best EMA pseudo Dice: 0.6623
2024-12-07 02:26:00.108053: 
2024-12-07 02:26:00.109113: Epoch 29
2024-12-07 02:26:00.109790: Current learning rate: 0.00974
2024-12-07 02:27:26.746998: Validation loss did not improve from -0.47929. Patience: 5/50
2024-12-07 02:27:26.747909: train_loss -0.5603
2024-12-07 02:27:26.748864: val_loss -0.472
2024-12-07 02:27:26.749731: Pseudo dice [0.7024]
2024-12-07 02:27:26.750693: Epoch time: 86.64 s
2024-12-07 02:27:27.129719: Yayy! New best EMA pseudo Dice: 0.6663
2024-12-07 02:27:28.773159: 
2024-12-07 02:27:28.775881: Epoch 30
2024-12-07 02:27:28.776889: Current learning rate: 0.00973
2024-12-07 02:28:55.345640: Validation loss improved from -0.47929 to -0.48180! Patience: 5/50
2024-12-07 02:28:55.346636: train_loss -0.574
2024-12-07 02:28:55.347553: val_loss -0.4818
2024-12-07 02:28:55.348316: Pseudo dice [0.6974]
2024-12-07 02:28:55.349136: Epoch time: 86.57 s
2024-12-07 02:28:55.349810: Yayy! New best EMA pseudo Dice: 0.6694
2024-12-07 02:28:57.035319: 
2024-12-07 02:28:57.037404: Epoch 31
2024-12-07 02:28:57.038479: Current learning rate: 0.00972
2024-12-07 02:30:23.556981: Validation loss improved from -0.48180 to -0.51878! Patience: 0/50
2024-12-07 02:30:23.558264: train_loss -0.5788
2024-12-07 02:30:23.559173: val_loss -0.5188
2024-12-07 02:30:23.559901: Pseudo dice [0.7319]
2024-12-07 02:30:23.560611: Epoch time: 86.52 s
2024-12-07 02:30:23.561223: Yayy! New best EMA pseudo Dice: 0.6757
2024-12-07 02:30:25.255507: 
2024-12-07 02:30:25.257499: Epoch 32
2024-12-07 02:30:25.258402: Current learning rate: 0.00971
2024-12-07 02:31:51.856112: Validation loss did not improve from -0.51878. Patience: 1/50
2024-12-07 02:31:51.857090: train_loss -0.5803
2024-12-07 02:31:51.857986: val_loss -0.4698
2024-12-07 02:31:51.858778: Pseudo dice [0.6937]
2024-12-07 02:31:51.859567: Epoch time: 86.6 s
2024-12-07 02:31:51.860314: Yayy! New best EMA pseudo Dice: 0.6775
2024-12-07 02:31:53.552204: 
2024-12-07 02:31:53.553880: Epoch 33
2024-12-07 02:31:53.554845: Current learning rate: 0.0097
2024-12-07 02:33:20.095091: Validation loss did not improve from -0.51878. Patience: 2/50
2024-12-07 02:33:20.096453: train_loss -0.5802
2024-12-07 02:33:20.097784: val_loss -0.4443
2024-12-07 02:33:20.098799: Pseudo dice [0.6942]
2024-12-07 02:33:20.099570: Epoch time: 86.55 s
2024-12-07 02:33:20.100508: Yayy! New best EMA pseudo Dice: 0.6791
2024-12-07 02:33:21.759509: 
2024-12-07 02:33:21.761421: Epoch 34
2024-12-07 02:33:21.762265: Current learning rate: 0.00969
2024-12-07 02:34:48.324234: Validation loss did not improve from -0.51878. Patience: 3/50
2024-12-07 02:34:48.325343: train_loss -0.5836
2024-12-07 02:34:48.326300: val_loss -0.4863
2024-12-07 02:34:48.327194: Pseudo dice [0.7067]
2024-12-07 02:34:48.328038: Epoch time: 86.57 s
2024-12-07 02:34:48.726805: Yayy! New best EMA pseudo Dice: 0.6819
2024-12-07 02:34:50.437436: 
2024-12-07 02:34:50.439182: Epoch 35
2024-12-07 02:34:50.439862: Current learning rate: 0.00968
2024-12-07 02:36:17.022532: Validation loss did not improve from -0.51878. Patience: 4/50
2024-12-07 02:36:17.023788: train_loss -0.5919
2024-12-07 02:36:17.024827: val_loss -0.4869
2024-12-07 02:36:17.025707: Pseudo dice [0.7073]
2024-12-07 02:36:17.026603: Epoch time: 86.59 s
2024-12-07 02:36:17.027444: Yayy! New best EMA pseudo Dice: 0.6844
2024-12-07 02:36:18.722962: 
2024-12-07 02:36:18.724915: Epoch 36
2024-12-07 02:36:18.725962: Current learning rate: 0.00968
2024-12-07 02:37:45.440684: Validation loss did not improve from -0.51878. Patience: 5/50
2024-12-07 02:37:45.441924: train_loss -0.5928
2024-12-07 02:37:45.443042: val_loss -0.5009
2024-12-07 02:37:45.444022: Pseudo dice [0.7081]
2024-12-07 02:37:45.444882: Epoch time: 86.72 s
2024-12-07 02:37:45.445723: Yayy! New best EMA pseudo Dice: 0.6868
2024-12-07 02:37:47.147359: 
2024-12-07 02:37:47.149202: Epoch 37
2024-12-07 02:37:47.150166: Current learning rate: 0.00967
2024-12-07 02:39:13.888427: Validation loss did not improve from -0.51878. Patience: 6/50
2024-12-07 02:39:13.889440: train_loss -0.598
2024-12-07 02:39:13.890414: val_loss -0.5166
2024-12-07 02:39:13.891147: Pseudo dice [0.7305]
2024-12-07 02:39:13.891773: Epoch time: 86.74 s
2024-12-07 02:39:13.892396: Yayy! New best EMA pseudo Dice: 0.6912
2024-12-07 02:39:15.580440: 
2024-12-07 02:39:15.582058: Epoch 38
2024-12-07 02:39:15.583032: Current learning rate: 0.00966
2024-12-07 02:40:42.297353: Validation loss did not improve from -0.51878. Patience: 7/50
2024-12-07 02:40:42.298387: train_loss -0.6016
2024-12-07 02:40:42.299227: val_loss -0.4951
2024-12-07 02:40:42.299945: Pseudo dice [0.7061]
2024-12-07 02:40:42.300636: Epoch time: 86.72 s
2024-12-07 02:40:42.301288: Yayy! New best EMA pseudo Dice: 0.6927
2024-12-07 02:40:44.008298: 
2024-12-07 02:40:44.009624: Epoch 39
2024-12-07 02:40:44.010468: Current learning rate: 0.00965
2024-12-07 02:42:10.938888: Validation loss did not improve from -0.51878. Patience: 8/50
2024-12-07 02:42:10.939959: train_loss -0.6038
2024-12-07 02:42:10.940953: val_loss -0.4859
2024-12-07 02:42:10.941732: Pseudo dice [0.7027]
2024-12-07 02:42:10.942396: Epoch time: 86.93 s
2024-12-07 02:42:11.328355: Yayy! New best EMA pseudo Dice: 0.6937
2024-12-07 02:42:13.025190: 
2024-12-07 02:42:13.027019: Epoch 40
2024-12-07 02:42:13.027864: Current learning rate: 0.00964
2024-12-07 02:43:39.620279: Validation loss did not improve from -0.51878. Patience: 9/50
2024-12-07 02:43:39.621605: train_loss -0.6101
2024-12-07 02:43:39.622717: val_loss -0.4862
2024-12-07 02:43:39.623903: Pseudo dice [0.7124]
2024-12-07 02:43:39.624857: Epoch time: 86.6 s
2024-12-07 02:43:39.625556: Yayy! New best EMA pseudo Dice: 0.6955
2024-12-07 02:43:41.383013: 
2024-12-07 02:43:41.384511: Epoch 41
2024-12-07 02:43:41.385492: Current learning rate: 0.00963
2024-12-07 02:45:07.923892: Validation loss did not improve from -0.51878. Patience: 10/50
2024-12-07 02:45:07.925100: train_loss -0.6021
2024-12-07 02:45:07.926025: val_loss -0.4739
2024-12-07 02:45:07.926693: Pseudo dice [0.6941]
2024-12-07 02:45:07.927348: Epoch time: 86.54 s
2024-12-07 02:45:09.162963: 
2024-12-07 02:45:09.164587: Epoch 42
2024-12-07 02:45:09.165254: Current learning rate: 0.00962
2024-12-07 02:46:35.766988: Validation loss did not improve from -0.51878. Patience: 11/50
2024-12-07 02:46:35.768312: train_loss -0.6059
2024-12-07 02:46:35.768999: val_loss -0.4906
2024-12-07 02:46:35.769625: Pseudo dice [0.7129]
2024-12-07 02:46:35.770269: Epoch time: 86.61 s
2024-12-07 02:46:35.770959: Yayy! New best EMA pseudo Dice: 0.6971
2024-12-07 02:46:37.436620: 
2024-12-07 02:46:37.438279: Epoch 43
2024-12-07 02:46:37.439081: Current learning rate: 0.00961
2024-12-07 02:48:04.031615: Validation loss did not improve from -0.51878. Patience: 12/50
2024-12-07 02:48:04.033921: train_loss -0.6029
2024-12-07 02:48:04.035102: val_loss -0.4829
2024-12-07 02:48:04.035873: Pseudo dice [0.7061]
2024-12-07 02:48:04.036727: Epoch time: 86.6 s
2024-12-07 02:48:04.037615: Yayy! New best EMA pseudo Dice: 0.698
2024-12-07 02:48:05.666000: 
2024-12-07 02:48:05.667474: Epoch 44
2024-12-07 02:48:05.668450: Current learning rate: 0.0096
2024-12-07 02:49:32.195187: Validation loss did not improve from -0.51878. Patience: 13/50
2024-12-07 02:49:32.196573: train_loss -0.6174
2024-12-07 02:49:32.197634: val_loss -0.4915
2024-12-07 02:49:32.198418: Pseudo dice [0.7109]
2024-12-07 02:49:32.199154: Epoch time: 86.53 s
2024-12-07 02:49:32.575166: Yayy! New best EMA pseudo Dice: 0.6993
2024-12-07 02:49:34.231005: 
2024-12-07 02:49:34.232598: Epoch 45
2024-12-07 02:49:34.233228: Current learning rate: 0.00959
2024-12-07 02:51:00.865746: Validation loss did not improve from -0.51878. Patience: 14/50
2024-12-07 02:51:00.866935: train_loss -0.6185
2024-12-07 02:51:00.868752: val_loss -0.4965
2024-12-07 02:51:00.870187: Pseudo dice [0.7117]
2024-12-07 02:51:00.871582: Epoch time: 86.64 s
2024-12-07 02:51:00.875689: Yayy! New best EMA pseudo Dice: 0.7006
2024-12-07 02:51:02.448051: 
2024-12-07 02:51:02.449617: Epoch 46
2024-12-07 02:51:02.450723: Current learning rate: 0.00959
2024-12-07 02:52:29.049058: Validation loss did not improve from -0.51878. Patience: 15/50
2024-12-07 02:52:29.049898: train_loss -0.6232
2024-12-07 02:52:29.050578: val_loss -0.4686
2024-12-07 02:52:29.051223: Pseudo dice [0.704]
2024-12-07 02:52:29.051867: Epoch time: 86.6 s
2024-12-07 02:52:29.052557: Yayy! New best EMA pseudo Dice: 0.7009
2024-12-07 02:52:30.800378: 
2024-12-07 02:52:30.801818: Epoch 47
2024-12-07 02:52:30.802692: Current learning rate: 0.00958
2024-12-07 02:53:57.464297: Validation loss improved from -0.51878 to -0.52089! Patience: 15/50
2024-12-07 02:53:57.465341: train_loss -0.6238
2024-12-07 02:53:57.466028: val_loss -0.5209
2024-12-07 02:53:57.466711: Pseudo dice [0.726]
2024-12-07 02:53:57.467505: Epoch time: 86.67 s
2024-12-07 02:53:57.468358: Yayy! New best EMA pseudo Dice: 0.7034
2024-12-07 02:53:59.104314: 
2024-12-07 02:53:59.105986: Epoch 48
2024-12-07 02:53:59.106699: Current learning rate: 0.00957
2024-12-07 02:55:25.867191: Validation loss improved from -0.52089 to -0.52400! Patience: 0/50
2024-12-07 02:55:25.868246: train_loss -0.6288
2024-12-07 02:55:25.869270: val_loss -0.524
2024-12-07 02:55:25.870155: Pseudo dice [0.7271]
2024-12-07 02:55:25.871147: Epoch time: 86.76 s
2024-12-07 02:55:25.872003: Yayy! New best EMA pseudo Dice: 0.7058
2024-12-07 02:55:27.571693: 
2024-12-07 02:55:27.573206: Epoch 49
2024-12-07 02:55:27.574367: Current learning rate: 0.00956
2024-12-07 02:56:54.365568: Validation loss did not improve from -0.52400. Patience: 1/50
2024-12-07 02:56:54.366623: train_loss -0.6257
2024-12-07 02:56:54.367585: val_loss -0.4965
2024-12-07 02:56:54.368311: Pseudo dice [0.7227]
2024-12-07 02:56:54.368958: Epoch time: 86.8 s
2024-12-07 02:56:54.747179: Yayy! New best EMA pseudo Dice: 0.7075
2024-12-07 02:56:57.294386: 
2024-12-07 02:56:57.295735: Epoch 50
2024-12-07 02:56:57.296670: Current learning rate: 0.00955
2024-12-07 02:58:23.804175: Validation loss did not improve from -0.52400. Patience: 2/50
2024-12-07 02:58:23.805404: train_loss -0.6163
2024-12-07 02:58:23.806250: val_loss -0.4211
2024-12-07 02:58:23.806976: Pseudo dice [0.6783]
2024-12-07 02:58:23.807737: Epoch time: 86.51 s
2024-12-07 02:58:25.078712: 
2024-12-07 02:58:25.080077: Epoch 51
2024-12-07 02:58:25.080807: Current learning rate: 0.00954
2024-12-07 02:59:51.641985: Validation loss improved from -0.52400 to -0.52543! Patience: 2/50
2024-12-07 02:59:51.642856: train_loss -0.6186
2024-12-07 02:59:51.643556: val_loss -0.5254
2024-12-07 02:59:51.644207: Pseudo dice [0.7331]
2024-12-07 02:59:51.644765: Epoch time: 86.57 s
2024-12-07 02:59:52.929155: 
2024-12-07 02:59:52.930959: Epoch 52
2024-12-07 02:59:52.931985: Current learning rate: 0.00953
2024-12-07 03:01:19.491833: Validation loss did not improve from -0.52543. Patience: 1/50
2024-12-07 03:01:19.492979: train_loss -0.6383
2024-12-07 03:01:19.494006: val_loss -0.4589
2024-12-07 03:01:19.494891: Pseudo dice [0.6923]
2024-12-07 03:01:19.495568: Epoch time: 86.56 s
2024-12-07 03:01:20.768075: 
2024-12-07 03:01:20.769459: Epoch 53
2024-12-07 03:01:20.770331: Current learning rate: 0.00952
2024-12-07 03:02:47.407568: Validation loss did not improve from -0.52543. Patience: 2/50
2024-12-07 03:02:47.408564: train_loss -0.6365
2024-12-07 03:02:47.409512: val_loss -0.4938
2024-12-07 03:02:47.410278: Pseudo dice [0.7184]
2024-12-07 03:02:47.411016: Epoch time: 86.64 s
2024-12-07 03:02:48.661537: 
2024-12-07 03:02:48.662960: Epoch 54
2024-12-07 03:02:48.663670: Current learning rate: 0.00951
2024-12-07 03:04:15.161677: Validation loss improved from -0.52543 to -0.52546! Patience: 2/50
2024-12-07 03:04:15.162879: train_loss -0.6365
2024-12-07 03:04:15.163854: val_loss -0.5255
2024-12-07 03:04:15.164623: Pseudo dice [0.7338]
2024-12-07 03:04:15.165589: Epoch time: 86.5 s
2024-12-07 03:04:15.554279: Yayy! New best EMA pseudo Dice: 0.7098
2024-12-07 03:04:17.241380: 
2024-12-07 03:04:17.243376: Epoch 55
2024-12-07 03:04:17.244264: Current learning rate: 0.0095
2024-12-07 03:05:44.131179: Validation loss did not improve from -0.52546. Patience: 1/50
2024-12-07 03:05:44.132029: train_loss -0.6315
2024-12-07 03:05:44.133214: val_loss -0.5043
2024-12-07 03:05:44.134178: Pseudo dice [0.715]
2024-12-07 03:05:44.135058: Epoch time: 86.89 s
2024-12-07 03:05:44.135850: Yayy! New best EMA pseudo Dice: 0.7103
2024-12-07 03:05:45.806188: 
2024-12-07 03:05:45.807962: Epoch 56
2024-12-07 03:05:45.808940: Current learning rate: 0.00949
2024-12-07 03:07:12.378992: Validation loss did not improve from -0.52546. Patience: 2/50
2024-12-07 03:07:12.379985: train_loss -0.6465
2024-12-07 03:07:12.380830: val_loss -0.5208
2024-12-07 03:07:12.381485: Pseudo dice [0.7279]
2024-12-07 03:07:12.382139: Epoch time: 86.57 s
2024-12-07 03:07:12.382794: Yayy! New best EMA pseudo Dice: 0.7121
2024-12-07 03:07:14.012723: 
2024-12-07 03:07:14.014584: Epoch 57
2024-12-07 03:07:14.015478: Current learning rate: 0.00949
2024-12-07 03:08:40.583888: Validation loss did not improve from -0.52546. Patience: 3/50
2024-12-07 03:08:40.584935: train_loss -0.6429
2024-12-07 03:08:40.585705: val_loss -0.5065
2024-12-07 03:08:40.586365: Pseudo dice [0.7284]
2024-12-07 03:08:40.587007: Epoch time: 86.57 s
2024-12-07 03:08:40.587591: Yayy! New best EMA pseudo Dice: 0.7137
2024-12-07 03:08:42.217736: 
2024-12-07 03:08:42.219415: Epoch 58
2024-12-07 03:08:42.220086: Current learning rate: 0.00948
2024-12-07 03:10:08.911933: Validation loss did not improve from -0.52546. Patience: 4/50
2024-12-07 03:10:08.913241: train_loss -0.647
2024-12-07 03:10:08.914356: val_loss -0.5235
2024-12-07 03:10:08.915295: Pseudo dice [0.7314]
2024-12-07 03:10:08.916071: Epoch time: 86.7 s
2024-12-07 03:10:08.917029: Yayy! New best EMA pseudo Dice: 0.7155
2024-12-07 03:10:10.592896: 
2024-12-07 03:10:10.593995: Epoch 59
2024-12-07 03:10:10.594934: Current learning rate: 0.00947
2024-12-07 03:11:37.241906: Validation loss did not improve from -0.52546. Patience: 5/50
2024-12-07 03:11:37.243047: train_loss -0.6492
2024-12-07 03:11:37.243861: val_loss -0.513
2024-12-07 03:11:37.244484: Pseudo dice [0.7145]
2024-12-07 03:11:37.245191: Epoch time: 86.65 s
2024-12-07 03:11:38.972195: 
2024-12-07 03:11:38.974541: Epoch 60
2024-12-07 03:11:38.975420: Current learning rate: 0.00946
2024-12-07 03:13:07.768602: Validation loss did not improve from -0.52546. Patience: 6/50
2024-12-07 03:13:07.769481: train_loss -0.6461
2024-12-07 03:13:07.770488: val_loss -0.507
2024-12-07 03:13:07.771533: Pseudo dice [0.7258]
2024-12-07 03:13:07.772560: Epoch time: 88.8 s
2024-12-07 03:13:07.773571: Yayy! New best EMA pseudo Dice: 0.7164
2024-12-07 03:13:09.884904: 
2024-12-07 03:13:09.886173: Epoch 61
2024-12-07 03:13:09.886994: Current learning rate: 0.00945
2024-12-07 03:14:40.528251: Validation loss did not improve from -0.52546. Patience: 7/50
2024-12-07 03:14:40.529370: train_loss -0.6461
2024-12-07 03:14:40.530198: val_loss -0.5223
2024-12-07 03:14:40.530999: Pseudo dice [0.731]
2024-12-07 03:14:40.531763: Epoch time: 90.65 s
2024-12-07 03:14:40.532473: Yayy! New best EMA pseudo Dice: 0.7179
2024-12-07 03:14:42.390677: 
2024-12-07 03:14:42.392126: Epoch 62
2024-12-07 03:14:42.392941: Current learning rate: 0.00944
2024-12-07 03:16:21.227308: Validation loss improved from -0.52546 to -0.54212! Patience: 7/50
2024-12-07 03:16:21.228331: train_loss -0.6497
2024-12-07 03:16:21.229186: val_loss -0.5421
2024-12-07 03:16:21.229872: Pseudo dice [0.7455]
2024-12-07 03:16:21.230623: Epoch time: 98.84 s
2024-12-07 03:16:21.231233: Yayy! New best EMA pseudo Dice: 0.7206
2024-12-07 03:16:23.059617: 
2024-12-07 03:16:23.060804: Epoch 63
2024-12-07 03:16:23.061551: Current learning rate: 0.00943
2024-12-07 03:18:44.730953: Validation loss did not improve from -0.54212. Patience: 1/50
2024-12-07 03:18:44.732058: train_loss -0.6587
2024-12-07 03:18:44.733214: val_loss -0.5133
2024-12-07 03:18:44.734035: Pseudo dice [0.7243]
2024-12-07 03:18:44.734905: Epoch time: 141.67 s
2024-12-07 03:18:44.735784: Yayy! New best EMA pseudo Dice: 0.721
2024-12-07 03:18:46.569556: 
2024-12-07 03:18:46.571062: Epoch 64
2024-12-07 03:18:46.572075: Current learning rate: 0.00942
2024-12-07 03:21:23.984621: Validation loss did not improve from -0.54212. Patience: 2/50
2024-12-07 03:21:23.985692: train_loss -0.6556
2024-12-07 03:21:23.986618: val_loss -0.4835
2024-12-07 03:21:23.987304: Pseudo dice [0.7175]
2024-12-07 03:21:23.987931: Epoch time: 157.42 s
2024-12-07 03:21:25.852723: 
2024-12-07 03:21:25.854064: Epoch 65
2024-12-07 03:21:25.854925: Current learning rate: 0.00941
2024-12-07 03:24:09.803467: Validation loss did not improve from -0.54212. Patience: 3/50
2024-12-07 03:24:09.804539: train_loss -0.6509
2024-12-07 03:24:09.805473: val_loss -0.5111
2024-12-07 03:24:09.806165: Pseudo dice [0.7227]
2024-12-07 03:24:09.806859: Epoch time: 163.95 s
2024-12-07 03:24:11.232436: 
2024-12-07 03:24:11.233776: Epoch 66
2024-12-07 03:24:11.234476: Current learning rate: 0.0094
2024-12-07 03:27:07.298952: Validation loss did not improve from -0.54212. Patience: 4/50
2024-12-07 03:27:07.299998: train_loss -0.6497
2024-12-07 03:27:07.300815: val_loss -0.5243
2024-12-07 03:27:07.301600: Pseudo dice [0.7203]
2024-12-07 03:27:07.302400: Epoch time: 176.07 s
2024-12-07 03:27:08.742117: 
2024-12-07 03:27:08.743543: Epoch 67
2024-12-07 03:27:08.744389: Current learning rate: 0.00939
2024-12-07 03:30:28.082910: Validation loss did not improve from -0.54212. Patience: 5/50
2024-12-07 03:30:28.083960: train_loss -0.6621
2024-12-07 03:30:28.084796: val_loss -0.5382
2024-12-07 03:30:28.085538: Pseudo dice [0.7401]
2024-12-07 03:30:28.086315: Epoch time: 199.34 s
2024-12-07 03:30:28.086983: Yayy! New best EMA pseudo Dice: 0.7227
2024-12-07 03:30:29.958716: 
2024-12-07 03:30:29.960098: Epoch 68
2024-12-07 03:30:29.960900: Current learning rate: 0.00939
2024-12-07 03:33:45.736455: Validation loss did not improve from -0.54212. Patience: 6/50
2024-12-07 03:33:45.737545: train_loss -0.6647
2024-12-07 03:33:45.738802: val_loss -0.4966
2024-12-07 03:33:45.739932: Pseudo dice [0.7123]
2024-12-07 03:33:45.741020: Epoch time: 195.78 s
2024-12-07 03:33:47.201200: 
2024-12-07 03:33:47.202908: Epoch 69
2024-12-07 03:33:47.204301: Current learning rate: 0.00938
2024-12-07 03:37:08.866760: Validation loss did not improve from -0.54212. Patience: 7/50
2024-12-07 03:37:08.867697: train_loss -0.6606
2024-12-07 03:37:08.868519: val_loss -0.5095
2024-12-07 03:37:08.869307: Pseudo dice [0.7135]
2024-12-07 03:37:08.869976: Epoch time: 201.67 s
2024-12-07 03:37:10.709637: 
2024-12-07 03:37:10.711025: Epoch 70
2024-12-07 03:37:10.711741: Current learning rate: 0.00937
2024-12-07 03:40:40.417334: Validation loss did not improve from -0.54212. Patience: 8/50
2024-12-07 03:40:40.418281: train_loss -0.6555
2024-12-07 03:40:40.419127: val_loss -0.5196
2024-12-07 03:40:40.420136: Pseudo dice [0.7234]
2024-12-07 03:40:40.421144: Epoch time: 209.71 s
2024-12-07 03:40:41.888052: 
2024-12-07 03:40:41.889293: Epoch 71
2024-12-07 03:40:41.890045: Current learning rate: 0.00936
2024-12-07 03:44:09.076159: Validation loss did not improve from -0.54212. Patience: 9/50
2024-12-07 03:44:09.077105: train_loss -0.6627
2024-12-07 03:44:09.077942: val_loss -0.4898
2024-12-07 03:44:09.078706: Pseudo dice [0.7047]
2024-12-07 03:44:09.079499: Epoch time: 207.19 s
2024-12-07 03:44:11.067915: 
2024-12-07 03:44:11.069270: Epoch 72
2024-12-07 03:44:11.070120: Current learning rate: 0.00935
2024-12-07 03:47:44.694609: Validation loss did not improve from -0.54212. Patience: 10/50
2024-12-07 03:47:44.695510: train_loss -0.6609
2024-12-07 03:47:44.696346: val_loss -0.5245
2024-12-07 03:47:44.697152: Pseudo dice [0.7302]
2024-12-07 03:47:44.697820: Epoch time: 213.63 s
2024-12-07 03:47:46.136427: 
2024-12-07 03:47:46.137793: Epoch 73
2024-12-07 03:47:46.138531: Current learning rate: 0.00934
2024-12-07 03:51:50.588734: Validation loss did not improve from -0.54212. Patience: 11/50
2024-12-07 03:51:50.591732: train_loss -0.6622
2024-12-07 03:51:50.593072: val_loss -0.5007
2024-12-07 03:51:50.593841: Pseudo dice [0.7159]
2024-12-07 03:51:50.594482: Epoch time: 244.46 s
2024-12-07 03:51:52.030141: 
2024-12-07 03:51:52.031416: Epoch 74
2024-12-07 03:51:52.032193: Current learning rate: 0.00933
2024-12-07 03:57:05.251600: Validation loss improved from -0.54212 to -0.54603! Patience: 11/50
2024-12-07 03:57:05.252861: train_loss -0.6714
2024-12-07 03:57:05.253854: val_loss -0.546
2024-12-07 03:57:05.254734: Pseudo dice [0.7425]
2024-12-07 03:57:05.255611: Epoch time: 313.22 s
2024-12-07 03:57:07.140029: 
2024-12-07 03:57:07.141469: Epoch 75
2024-12-07 03:57:07.142284: Current learning rate: 0.00932
2024-12-07 04:02:43.849040: Validation loss did not improve from -0.54603. Patience: 1/50
2024-12-07 04:02:43.850132: train_loss -0.6694
2024-12-07 04:02:43.850947: val_loss -0.52
2024-12-07 04:02:43.851659: Pseudo dice [0.7304]
2024-12-07 04:02:43.852313: Epoch time: 336.71 s
2024-12-07 04:02:43.853025: Yayy! New best EMA pseudo Dice: 0.7231
2024-12-07 04:02:45.703390: 
2024-12-07 04:02:45.704618: Epoch 76
2024-12-07 04:02:45.705521: Current learning rate: 0.00931
2024-12-07 04:08:28.659858: Validation loss did not improve from -0.54603. Patience: 2/50
2024-12-07 04:08:28.660878: train_loss -0.6691
2024-12-07 04:08:28.661768: val_loss -0.5053
2024-12-07 04:08:28.662635: Pseudo dice [0.7276]
2024-12-07 04:08:28.663354: Epoch time: 342.96 s
2024-12-07 04:08:28.664155: Yayy! New best EMA pseudo Dice: 0.7236
2024-12-07 04:08:30.567723: 
2024-12-07 04:08:30.569016: Epoch 77
2024-12-07 04:08:30.569765: Current learning rate: 0.0093
2024-12-07 04:14:13.038566: Validation loss did not improve from -0.54603. Patience: 3/50
2024-12-07 04:14:13.040176: train_loss -0.6769
2024-12-07 04:14:13.041346: val_loss -0.5424
2024-12-07 04:14:13.042152: Pseudo dice [0.7484]
2024-12-07 04:14:13.042891: Epoch time: 342.47 s
2024-12-07 04:14:13.043566: Yayy! New best EMA pseudo Dice: 0.7261
2024-12-07 04:14:14.945955: 
2024-12-07 04:14:14.947360: Epoch 78
2024-12-07 04:14:14.948313: Current learning rate: 0.0093
2024-12-07 04:20:20.160237: Validation loss did not improve from -0.54603. Patience: 4/50
2024-12-07 04:20:20.161201: train_loss -0.6804
2024-12-07 04:20:20.162126: val_loss -0.4938
2024-12-07 04:20:20.162817: Pseudo dice [0.7142]
2024-12-07 04:20:20.163596: Epoch time: 365.22 s
2024-12-07 04:20:21.620030: 
2024-12-07 04:20:21.621513: Epoch 79
2024-12-07 04:20:21.622507: Current learning rate: 0.00929
2024-12-07 04:26:26.288891: Validation loss did not improve from -0.54603. Patience: 5/50
2024-12-07 04:26:26.289801: train_loss -0.6689
2024-12-07 04:26:26.290501: val_loss -0.5338
2024-12-07 04:26:26.291262: Pseudo dice [0.739]
2024-12-07 04:26:26.292242: Epoch time: 364.67 s
2024-12-07 04:26:26.686808: Yayy! New best EMA pseudo Dice: 0.7263
2024-12-07 04:26:28.506551: 
2024-12-07 04:26:28.507902: Epoch 80
2024-12-07 04:26:28.508683: Current learning rate: 0.00928
2024-12-07 04:32:48.544762: Validation loss did not improve from -0.54603. Patience: 6/50
2024-12-07 04:32:48.545734: train_loss -0.6823
2024-12-07 04:32:48.546459: val_loss -0.5089
2024-12-07 04:32:48.547301: Pseudo dice [0.7205]
2024-12-07 04:32:48.548127: Epoch time: 380.04 s
2024-12-07 04:32:49.987817: 
2024-12-07 04:32:49.989307: Epoch 81
2024-12-07 04:32:49.990195: Current learning rate: 0.00927
2024-12-07 04:39:21.959985: Validation loss did not improve from -0.54603. Patience: 7/50
2024-12-07 04:39:21.960927: train_loss -0.6861
2024-12-07 04:39:21.961940: val_loss -0.5458
2024-12-07 04:39:21.962879: Pseudo dice [0.7412]
2024-12-07 04:39:21.963759: Epoch time: 391.97 s
2024-12-07 04:39:21.964625: Yayy! New best EMA pseudo Dice: 0.7273
2024-12-07 04:39:24.237672: 
2024-12-07 04:39:24.239243: Epoch 82
2024-12-07 04:39:24.240343: Current learning rate: 0.00926
2024-12-07 04:45:58.094192: Validation loss did not improve from -0.54603. Patience: 8/50
2024-12-07 04:45:58.095320: train_loss -0.689
2024-12-07 04:45:58.096214: val_loss -0.5189
2024-12-07 04:45:58.096843: Pseudo dice [0.7304]
2024-12-07 04:45:58.097474: Epoch time: 393.86 s
2024-12-07 04:45:58.098036: Yayy! New best EMA pseudo Dice: 0.7276
2024-12-07 04:45:59.865452: 
2024-12-07 04:45:59.866736: Epoch 83
2024-12-07 04:45:59.867471: Current learning rate: 0.00925
2024-12-07 04:52:19.689130: Validation loss did not improve from -0.54603. Patience: 9/50
2024-12-07 04:52:19.690203: train_loss -0.6819
2024-12-07 04:52:19.691113: val_loss -0.5195
2024-12-07 04:52:19.691914: Pseudo dice [0.7286]
2024-12-07 04:52:19.692651: Epoch time: 379.83 s
2024-12-07 04:52:19.693300: Yayy! New best EMA pseudo Dice: 0.7277
2024-12-07 04:52:21.514864: 
2024-12-07 04:52:21.516083: Epoch 84
2024-12-07 04:52:21.516876: Current learning rate: 0.00924
2024-12-07 04:58:32.311710: Validation loss did not improve from -0.54603. Patience: 10/50
2024-12-07 04:58:32.313862: train_loss -0.6757
2024-12-07 04:58:32.315017: val_loss -0.544
2024-12-07 04:58:32.315912: Pseudo dice [0.7395]
2024-12-07 04:58:32.316907: Epoch time: 370.8 s
2024-12-07 04:58:32.727922: Yayy! New best EMA pseudo Dice: 0.7289
2024-12-07 04:58:34.549656: 
2024-12-07 04:58:34.550869: Epoch 85
2024-12-07 04:58:34.551569: Current learning rate: 0.00923
2024-12-07 05:04:28.048502: Validation loss did not improve from -0.54603. Patience: 11/50
2024-12-07 05:04:28.049294: train_loss -0.6806
2024-12-07 05:04:28.050029: val_loss -0.5213
2024-12-07 05:04:28.050697: Pseudo dice [0.725]
2024-12-07 05:04:28.051418: Epoch time: 353.5 s
2024-12-07 05:04:29.425888: 
2024-12-07 05:04:29.426959: Epoch 86
2024-12-07 05:04:29.427672: Current learning rate: 0.00922
2024-12-07 05:10:30.551955: Validation loss did not improve from -0.54603. Patience: 12/50
2024-12-07 05:10:30.553002: train_loss -0.6809
2024-12-07 05:10:30.553897: val_loss -0.526
2024-12-07 05:10:30.554755: Pseudo dice [0.7283]
2024-12-07 05:10:30.555454: Epoch time: 361.13 s
2024-12-07 05:10:31.953507: 
2024-12-07 05:10:31.955015: Epoch 87
2024-12-07 05:10:31.955881: Current learning rate: 0.00921
2024-12-07 05:16:05.282696: Validation loss did not improve from -0.54603. Patience: 13/50
2024-12-07 05:16:05.283874: train_loss -0.6849
2024-12-07 05:16:05.284782: val_loss -0.5365
2024-12-07 05:16:05.285452: Pseudo dice [0.7392]
2024-12-07 05:16:05.286058: Epoch time: 333.33 s
2024-12-07 05:16:05.286816: Yayy! New best EMA pseudo Dice: 0.7295
2024-12-07 05:16:07.078859: 
2024-12-07 05:16:07.080140: Epoch 88
2024-12-07 05:16:07.081016: Current learning rate: 0.0092
2024-12-07 05:21:49.012606: Validation loss did not improve from -0.54603. Patience: 14/50
2024-12-07 05:21:49.013667: train_loss -0.686
2024-12-07 05:21:49.014447: val_loss -0.5102
2024-12-07 05:21:49.015120: Pseudo dice [0.7222]
2024-12-07 05:21:49.015878: Epoch time: 341.94 s
2024-12-07 05:21:50.408178: 
2024-12-07 05:21:50.409669: Epoch 89
2024-12-07 05:21:50.410613: Current learning rate: 0.0092
2024-12-07 05:27:58.263014: Validation loss did not improve from -0.54603. Patience: 15/50
2024-12-07 05:27:58.264065: train_loss -0.6878
2024-12-07 05:27:58.265138: val_loss -0.5286
2024-12-07 05:27:58.265811: Pseudo dice [0.7321]
2024-12-07 05:27:58.266509: Epoch time: 367.86 s
2024-12-07 05:28:00.022578: 
2024-12-07 05:28:00.024066: Epoch 90
2024-12-07 05:28:00.025036: Current learning rate: 0.00919
2024-12-07 05:34:05.029430: Validation loss did not improve from -0.54603. Patience: 16/50
2024-12-07 05:34:05.030421: train_loss -0.6867
2024-12-07 05:34:05.031347: val_loss -0.5378
2024-12-07 05:34:05.032150: Pseudo dice [0.7355]
2024-12-07 05:34:05.032963: Epoch time: 365.01 s
2024-12-07 05:34:05.033779: Yayy! New best EMA pseudo Dice: 0.7298
2024-12-07 05:34:06.788538: 
2024-12-07 05:34:06.789983: Epoch 91
2024-12-07 05:34:06.790860: Current learning rate: 0.00918
2024-12-07 05:40:08.512116: Validation loss did not improve from -0.54603. Patience: 17/50
2024-12-07 05:40:08.512995: train_loss -0.6844
2024-12-07 05:40:08.513860: val_loss -0.4852
2024-12-07 05:40:08.514602: Pseudo dice [0.7061]
2024-12-07 05:40:08.515379: Epoch time: 361.73 s
2024-12-07 05:40:09.875585: 
2024-12-07 05:40:09.876771: Epoch 92
2024-12-07 05:40:09.877447: Current learning rate: 0.00917
2024-12-07 05:46:03.999882: Validation loss did not improve from -0.54603. Patience: 18/50
2024-12-07 05:46:04.000765: train_loss -0.6859
2024-12-07 05:46:04.001567: val_loss -0.5451
2024-12-07 05:46:04.002377: Pseudo dice [0.746]
2024-12-07 05:46:04.003098: Epoch time: 354.13 s
2024-12-07 05:46:05.793345: 
2024-12-07 05:46:05.794759: Epoch 93
2024-12-07 05:46:05.795514: Current learning rate: 0.00916
2024-12-07 05:51:50.979745: Validation loss did not improve from -0.54603. Patience: 19/50
2024-12-07 05:51:50.980753: train_loss -0.6842
2024-12-07 05:51:50.981545: val_loss -0.5331
2024-12-07 05:51:50.982425: Pseudo dice [0.733]
2024-12-07 05:51:50.983211: Epoch time: 345.19 s
2024-12-07 05:51:52.342246: 
2024-12-07 05:51:52.343572: Epoch 94
2024-12-07 05:51:52.344601: Current learning rate: 0.00915
2024-12-07 05:57:43.063318: Validation loss improved from -0.54603 to -0.54698! Patience: 19/50
2024-12-07 05:57:43.064117: train_loss -0.6927
2024-12-07 05:57:43.064822: val_loss -0.547
2024-12-07 05:57:43.065449: Pseudo dice [0.7428]
2024-12-07 05:57:43.066209: Epoch time: 350.72 s
2024-12-07 05:57:43.459671: Yayy! New best EMA pseudo Dice: 0.7309
2024-12-07 05:57:45.245676: 
2024-12-07 05:57:45.247218: Epoch 95
2024-12-07 05:57:45.248061: Current learning rate: 0.00914
2024-12-07 06:03:47.380089: Validation loss did not improve from -0.54698. Patience: 1/50
2024-12-07 06:03:47.383978: train_loss -0.6923
2024-12-07 06:03:47.385726: val_loss -0.5073
2024-12-07 06:03:47.386583: Pseudo dice [0.7213]
2024-12-07 06:03:47.387506: Epoch time: 362.14 s
2024-12-07 06:03:48.854478: 
2024-12-07 06:03:48.856009: Epoch 96
2024-12-07 06:03:48.856793: Current learning rate: 0.00913
2024-12-07 06:10:07.731527: Validation loss did not improve from -0.54698. Patience: 2/50
2024-12-07 06:10:07.732503: train_loss -0.6895
2024-12-07 06:10:07.733260: val_loss -0.4796
2024-12-07 06:10:07.733987: Pseudo dice [0.7127]
2024-12-07 06:10:07.734705: Epoch time: 378.88 s
2024-12-07 06:10:09.108459: 
2024-12-07 06:10:09.109861: Epoch 97
2024-12-07 06:10:09.110651: Current learning rate: 0.00912
2024-12-07 06:16:41.872216: Validation loss did not improve from -0.54698. Patience: 3/50
2024-12-07 06:16:41.873131: train_loss -0.7007
2024-12-07 06:16:41.873947: val_loss -0.524
2024-12-07 06:16:41.874715: Pseudo dice [0.7236]
2024-12-07 06:16:41.875488: Epoch time: 392.77 s
2024-12-07 06:16:43.318026: 
2024-12-07 06:16:43.319279: Epoch 98
2024-12-07 06:16:43.320057: Current learning rate: 0.00911
2024-12-07 06:23:07.613811: Validation loss did not improve from -0.54698. Patience: 4/50
2024-12-07 06:23:07.614601: train_loss -0.703
2024-12-07 06:23:07.615381: val_loss -0.5352
2024-12-07 06:23:07.616109: Pseudo dice [0.7351]
2024-12-07 06:23:07.616893: Epoch time: 384.3 s
2024-12-07 06:23:09.008237: 
2024-12-07 06:23:09.009372: Epoch 99
2024-12-07 06:23:09.010122: Current learning rate: 0.0091
2024-12-07 06:29:45.524491: Validation loss did not improve from -0.54698. Patience: 5/50
2024-12-07 06:29:45.525526: train_loss -0.6952
2024-12-07 06:29:45.526530: val_loss -0.5276
2024-12-07 06:29:45.527440: Pseudo dice [0.7434]
2024-12-07 06:29:45.528358: Epoch time: 396.52 s
2024-12-07 06:29:47.332100: 
2024-12-07 06:29:47.333277: Epoch 100
2024-12-07 06:29:47.333934: Current learning rate: 0.0091
2024-12-07 06:36:06.826249: Validation loss improved from -0.54698 to -0.56515! Patience: 5/50
2024-12-07 06:36:06.827184: train_loss -0.6958
2024-12-07 06:36:06.828153: val_loss -0.5651
2024-12-07 06:36:06.829096: Pseudo dice [0.7503]
2024-12-07 06:36:06.829859: Epoch time: 379.5 s
2024-12-07 06:36:06.830770: Yayy! New best EMA pseudo Dice: 0.732
2024-12-07 06:36:08.611974: 
2024-12-07 06:36:08.613398: Epoch 101
2024-12-07 06:36:08.614110: Current learning rate: 0.00909
2024-12-07 06:42:38.741393: Validation loss did not improve from -0.56515. Patience: 1/50
2024-12-07 06:42:38.742333: train_loss -0.6995
2024-12-07 06:42:38.743334: val_loss -0.5241
2024-12-07 06:42:38.744184: Pseudo dice [0.7362]
2024-12-07 06:42:38.744885: Epoch time: 390.13 s
2024-12-07 06:42:38.745661: Yayy! New best EMA pseudo Dice: 0.7324
2024-12-07 06:42:40.504953: 
2024-12-07 06:42:40.506343: Epoch 102
2024-12-07 06:42:40.507113: Current learning rate: 0.00908
2024-12-07 06:49:00.650611: Validation loss did not improve from -0.56515. Patience: 2/50
2024-12-07 06:49:00.651581: train_loss -0.6977
2024-12-07 06:49:00.652293: val_loss -0.5267
2024-12-07 06:49:00.653027: Pseudo dice [0.7368]
2024-12-07 06:49:00.653755: Epoch time: 380.15 s
2024-12-07 06:49:00.654485: Yayy! New best EMA pseudo Dice: 0.7329
2024-12-07 06:49:02.457521: 
2024-12-07 06:49:02.458936: Epoch 103
2024-12-07 06:49:02.459779: Current learning rate: 0.00907
2024-12-07 06:55:21.108862: Validation loss did not improve from -0.56515. Patience: 3/50
2024-12-07 06:55:21.109868: train_loss -0.6995
2024-12-07 06:55:21.110701: val_loss -0.5231
2024-12-07 06:55:21.111500: Pseudo dice [0.7304]
2024-12-07 06:55:21.112128: Epoch time: 378.65 s
2024-12-07 06:55:22.501355: 
2024-12-07 06:55:22.502680: Epoch 104
2024-12-07 06:55:22.503432: Current learning rate: 0.00906
2024-12-07 07:01:57.901273: Validation loss did not improve from -0.56515. Patience: 4/50
2024-12-07 07:01:57.902279: train_loss -0.6981
2024-12-07 07:01:57.903512: val_loss -0.5562
2024-12-07 07:01:57.904885: Pseudo dice [0.754]
2024-12-07 07:01:57.905856: Epoch time: 395.4 s
2024-12-07 07:01:58.307976: Yayy! New best EMA pseudo Dice: 0.7348
2024-12-07 07:02:00.105078: 
2024-12-07 07:02:00.106883: Epoch 105
2024-12-07 07:02:00.108065: Current learning rate: 0.00905
2024-12-07 07:08:43.543095: Validation loss did not improve from -0.56515. Patience: 5/50
2024-12-07 07:08:43.545176: train_loss -0.7062
2024-12-07 07:08:43.546110: val_loss -0.5435
2024-12-07 07:08:43.546761: Pseudo dice [0.7399]
2024-12-07 07:08:43.547387: Epoch time: 403.44 s
2024-12-07 07:08:43.548162: Yayy! New best EMA pseudo Dice: 0.7353
2024-12-07 07:08:45.324357: 
2024-12-07 07:08:45.325800: Epoch 106
2024-12-07 07:08:45.326499: Current learning rate: 0.00904
2024-12-07 07:15:14.366875: Validation loss did not improve from -0.56515. Patience: 6/50
2024-12-07 07:15:14.368052: train_loss -0.7136
2024-12-07 07:15:14.368837: val_loss -0.5226
2024-12-07 07:15:14.369558: Pseudo dice [0.7349]
2024-12-07 07:15:14.370240: Epoch time: 389.04 s
2024-12-07 07:15:15.765576: 
2024-12-07 07:15:15.766749: Epoch 107
2024-12-07 07:15:15.767403: Current learning rate: 0.00903
2024-12-07 07:21:47.126734: Validation loss did not improve from -0.56515. Patience: 7/50
2024-12-07 07:21:47.127706: train_loss -0.7097
2024-12-07 07:21:47.128891: val_loss -0.5414
2024-12-07 07:21:47.129833: Pseudo dice [0.7427]
2024-12-07 07:21:47.130698: Epoch time: 391.36 s
2024-12-07 07:21:47.131535: Yayy! New best EMA pseudo Dice: 0.736
2024-12-07 07:21:48.909698: 
2024-12-07 07:21:48.911102: Epoch 108
2024-12-07 07:21:48.911926: Current learning rate: 0.00902
2024-12-07 07:28:01.897031: Validation loss did not improve from -0.56515. Patience: 8/50
2024-12-07 07:28:01.898053: train_loss -0.7141
2024-12-07 07:28:01.898815: val_loss -0.516
2024-12-07 07:28:01.899434: Pseudo dice [0.7313]
2024-12-07 07:28:01.900095: Epoch time: 372.99 s
2024-12-07 07:28:03.290455: 
2024-12-07 07:28:03.291878: Epoch 109
2024-12-07 07:28:03.292852: Current learning rate: 0.00901
2024-12-07 07:33:56.141196: Validation loss did not improve from -0.56515. Patience: 9/50
2024-12-07 07:33:56.142192: train_loss -0.7116
2024-12-07 07:33:56.142970: val_loss -0.5384
2024-12-07 07:33:56.143687: Pseudo dice [0.7437]
2024-12-07 07:33:56.144357: Epoch time: 352.85 s
2024-12-07 07:33:56.551824: Yayy! New best EMA pseudo Dice: 0.7363
2024-12-07 07:33:58.344865: 
2024-12-07 07:33:58.346432: Epoch 110
2024-12-07 07:33:58.347449: Current learning rate: 0.009
2024-12-07 07:39:59.816851: Validation loss did not improve from -0.56515. Patience: 10/50
2024-12-07 07:39:59.817870: train_loss -0.7041
2024-12-07 07:39:59.818668: val_loss -0.5282
2024-12-07 07:39:59.819480: Pseudo dice [0.7319]
2024-12-07 07:39:59.820211: Epoch time: 361.47 s
2024-12-07 07:40:01.221416: 
2024-12-07 07:40:01.222472: Epoch 111
2024-12-07 07:40:01.223302: Current learning rate: 0.009
2024-12-07 07:46:15.410501: Validation loss did not improve from -0.56515. Patience: 11/50
2024-12-07 07:46:15.411413: train_loss -0.696
2024-12-07 07:46:15.412097: val_loss -0.5396
2024-12-07 07:46:15.412724: Pseudo dice [0.7434]
2024-12-07 07:46:15.413385: Epoch time: 374.19 s
2024-12-07 07:46:15.414064: Yayy! New best EMA pseudo Dice: 0.7366
2024-12-07 07:46:17.209947: 
2024-12-07 07:46:17.211292: Epoch 112
2024-12-07 07:46:17.211987: Current learning rate: 0.00899
2024-12-07 07:52:40.784790: Validation loss did not improve from -0.56515. Patience: 12/50
2024-12-07 07:52:40.785786: train_loss -0.7039
2024-12-07 07:52:40.786490: val_loss -0.5326
2024-12-07 07:52:40.787110: Pseudo dice [0.7464]
2024-12-07 07:52:40.788200: Epoch time: 383.58 s
2024-12-07 07:52:40.788981: Yayy! New best EMA pseudo Dice: 0.7376
2024-12-07 07:52:42.560738: 
2024-12-07 07:52:42.562123: Epoch 113
2024-12-07 07:52:42.562963: Current learning rate: 0.00898
2024-12-07 07:59:15.727004: Validation loss did not improve from -0.56515. Patience: 13/50
2024-12-07 07:59:15.728096: train_loss -0.7106
2024-12-07 07:59:15.729176: val_loss -0.5254
2024-12-07 07:59:15.730052: Pseudo dice [0.7395]
2024-12-07 07:59:15.730762: Epoch time: 393.17 s
2024-12-07 07:59:15.731438: Yayy! New best EMA pseudo Dice: 0.7378
2024-12-07 07:59:17.896725: 
2024-12-07 07:59:17.898145: Epoch 114
2024-12-07 07:59:17.898949: Current learning rate: 0.00897
2024-12-07 08:05:55.625714: Validation loss did not improve from -0.56515. Patience: 14/50
2024-12-07 08:05:55.626705: train_loss -0.7206
2024-12-07 08:05:55.627500: val_loss -0.5429
2024-12-07 08:05:55.628274: Pseudo dice [0.7427]
2024-12-07 08:05:55.629017: Epoch time: 397.73 s
2024-12-07 08:05:56.046333: Yayy! New best EMA pseudo Dice: 0.7383
2024-12-07 08:05:57.841898: 
2024-12-07 08:05:57.842996: Epoch 115
2024-12-07 08:05:57.843825: Current learning rate: 0.00896
2024-12-07 08:12:51.945205: Validation loss did not improve from -0.56515. Patience: 15/50
2024-12-07 08:12:51.946952: train_loss -0.7164
2024-12-07 08:12:51.948432: val_loss -0.5234
2024-12-07 08:12:51.949231: Pseudo dice [0.7346]
2024-12-07 08:12:51.950041: Epoch time: 414.11 s
2024-12-07 08:12:53.378737: 
2024-12-07 08:12:53.379780: Epoch 116
2024-12-07 08:12:53.380540: Current learning rate: 0.00895
2024-12-07 08:19:48.106452: Validation loss did not improve from -0.56515. Patience: 16/50
2024-12-07 08:19:48.107651: train_loss -0.7085
2024-12-07 08:19:48.108490: val_loss -0.5407
2024-12-07 08:19:48.109393: Pseudo dice [0.7464]
2024-12-07 08:19:48.110154: Epoch time: 414.73 s
2024-12-07 08:19:48.110903: Yayy! New best EMA pseudo Dice: 0.7388
2024-12-07 08:19:49.949773: 
2024-12-07 08:19:49.951187: Epoch 117
2024-12-07 08:19:49.951919: Current learning rate: 0.00894
2024-12-07 08:26:46.540610: Validation loss did not improve from -0.56515. Patience: 17/50
2024-12-07 08:26:46.541693: train_loss -0.6994
2024-12-07 08:26:46.542396: val_loss -0.4955
2024-12-07 08:26:46.543049: Pseudo dice [0.72]
2024-12-07 08:26:46.543697: Epoch time: 416.59 s
2024-12-07 08:26:47.945427: 
2024-12-07 08:26:47.946680: Epoch 118
2024-12-07 08:26:47.947440: Current learning rate: 0.00893
2024-12-07 08:33:50.662785: Validation loss did not improve from -0.56515. Patience: 18/50
2024-12-07 08:33:50.663816: train_loss -0.7038
2024-12-07 08:33:50.664657: val_loss -0.5257
2024-12-07 08:33:50.665370: Pseudo dice [0.7363]
2024-12-07 08:33:50.666052: Epoch time: 422.72 s
2024-12-07 08:33:52.103598: 
2024-12-07 08:33:52.104935: Epoch 119
2024-12-07 08:33:52.105795: Current learning rate: 0.00892
2024-12-07 08:40:57.011635: Validation loss did not improve from -0.56515. Patience: 19/50
2024-12-07 08:40:57.012707: train_loss -0.705
2024-12-07 08:40:57.013600: val_loss -0.5522
2024-12-07 08:40:57.014415: Pseudo dice [0.7538]
2024-12-07 08:40:57.015177: Epoch time: 424.91 s
2024-12-07 08:40:58.832999: 
2024-12-07 08:40:58.834552: Epoch 120
2024-12-07 08:40:58.835441: Current learning rate: 0.00891
2024-12-07 08:47:55.376662: Validation loss did not improve from -0.56515. Patience: 20/50
2024-12-07 08:47:55.377710: train_loss -0.7156
2024-12-07 08:47:55.378492: val_loss -0.533
2024-12-07 08:47:55.379168: Pseudo dice [0.7392]
2024-12-07 08:47:55.379904: Epoch time: 416.55 s
2024-12-07 08:47:56.785816: 
2024-12-07 08:47:56.787237: Epoch 121
2024-12-07 08:47:56.788015: Current learning rate: 0.0089
2024-12-07 08:54:13.626373: Validation loss did not improve from -0.56515. Patience: 21/50
2024-12-07 08:54:13.627052: train_loss -0.7106
2024-12-07 08:54:13.627850: val_loss -0.5349
2024-12-07 08:54:13.628557: Pseudo dice [0.7372]
2024-12-07 08:54:13.629312: Epoch time: 376.84 s
2024-12-07 08:54:15.123386: 
2024-12-07 08:54:15.124560: Epoch 122
2024-12-07 08:54:15.126147: Current learning rate: 0.00889
2024-12-07 09:00:15.088609: Validation loss did not improve from -0.56515. Patience: 22/50
2024-12-07 09:00:15.089727: train_loss -0.7159
2024-12-07 09:00:15.090712: val_loss -0.5319
2024-12-07 09:00:15.091653: Pseudo dice [0.7281]
2024-12-07 09:00:15.092549: Epoch time: 359.97 s
2024-12-07 09:00:16.550085: 
2024-12-07 09:00:16.551691: Epoch 123
2024-12-07 09:00:16.552737: Current learning rate: 0.00889
2024-12-07 09:06:32.666381: Validation loss did not improve from -0.56515. Patience: 23/50
2024-12-07 09:06:32.667315: train_loss -0.715
2024-12-07 09:06:32.668221: val_loss -0.5463
2024-12-07 09:06:32.668900: Pseudo dice [0.7397]
2024-12-07 09:06:32.669708: Epoch time: 376.12 s
2024-12-07 09:06:34.105718: 
2024-12-07 09:06:34.106831: Epoch 124
2024-12-07 09:06:34.107551: Current learning rate: 0.00888
2024-12-07 09:12:58.586662: Validation loss did not improve from -0.56515. Patience: 24/50
2024-12-07 09:12:58.587752: train_loss -0.71
2024-12-07 09:12:58.588785: val_loss -0.5466
2024-12-07 09:12:58.589706: Pseudo dice [0.7371]
2024-12-07 09:12:58.590647: Epoch time: 384.48 s
2024-12-07 09:13:00.876779: 
2024-12-07 09:13:00.878287: Epoch 125
2024-12-07 09:13:00.879419: Current learning rate: 0.00887
2024-12-07 09:19:31.125314: Validation loss did not improve from -0.56515. Patience: 25/50
2024-12-07 09:19:31.126910: train_loss -0.7184
2024-12-07 09:19:31.128086: val_loss -0.5311
2024-12-07 09:19:31.129101: Pseudo dice [0.7295]
2024-12-07 09:19:31.130002: Epoch time: 390.25 s
2024-12-07 09:19:32.621001: 
2024-12-07 09:19:32.622340: Epoch 126
2024-12-07 09:19:32.623247: Current learning rate: 0.00886
2024-12-07 09:26:00.095346: Validation loss did not improve from -0.56515. Patience: 26/50
2024-12-07 09:26:00.096391: train_loss -0.7208
2024-12-07 09:26:00.097304: val_loss -0.5564
2024-12-07 09:26:00.098183: Pseudo dice [0.7543]
2024-12-07 09:26:00.098933: Epoch time: 387.48 s
2024-12-07 09:26:01.501873: 
2024-12-07 09:26:01.503757: Epoch 127
2024-12-07 09:26:01.504655: Current learning rate: 0.00885
2024-12-07 09:32:24.702480: Validation loss did not improve from -0.56515. Patience: 27/50
2024-12-07 09:32:24.703809: train_loss -0.7186
2024-12-07 09:32:24.704936: val_loss -0.5269
2024-12-07 09:32:24.706196: Pseudo dice [0.7413]
2024-12-07 09:32:24.707514: Epoch time: 383.2 s
2024-12-07 09:32:24.708932: Yayy! New best EMA pseudo Dice: 0.7388
2024-12-07 09:32:26.522528: 
2024-12-07 09:32:26.523951: Epoch 128
2024-12-07 09:32:26.524935: Current learning rate: 0.00884
2024-12-07 09:38:57.878114: Validation loss did not improve from -0.56515. Patience: 28/50
2024-12-07 09:38:57.879054: train_loss -0.7291
2024-12-07 09:38:57.879721: val_loss -0.5156
2024-12-07 09:38:57.880362: Pseudo dice [0.7262]
2024-12-07 09:38:57.880988: Epoch time: 391.36 s
2024-12-07 09:38:59.333579: 
2024-12-07 09:38:59.334877: Epoch 129
2024-12-07 09:38:59.335882: Current learning rate: 0.00883
2024-12-07 09:45:42.608006: Validation loss did not improve from -0.56515. Patience: 29/50
2024-12-07 09:45:42.608995: train_loss -0.7248
2024-12-07 09:45:42.610291: val_loss -0.5231
2024-12-07 09:45:42.611303: Pseudo dice [0.735]
2024-12-07 09:45:42.612230: Epoch time: 403.28 s
2024-12-07 09:45:44.359481: 
2024-12-07 09:45:44.360877: Epoch 130
2024-12-07 09:45:44.361655: Current learning rate: 0.00882
2024-12-07 09:52:15.776275: Validation loss did not improve from -0.56515. Patience: 30/50
2024-12-07 09:52:15.777281: train_loss -0.7235
2024-12-07 09:52:15.778207: val_loss -0.5314
2024-12-07 09:52:15.778964: Pseudo dice [0.7455]
2024-12-07 09:52:15.779617: Epoch time: 391.42 s
2024-12-07 09:52:17.173097: 
2024-12-07 09:52:17.174270: Epoch 131
2024-12-07 09:52:17.175032: Current learning rate: 0.00881
2024-12-07 09:59:07.471782: Validation loss did not improve from -0.56515. Patience: 31/50
2024-12-07 09:59:07.472692: train_loss -0.7225
2024-12-07 09:59:07.473386: val_loss -0.4845
2024-12-07 09:59:07.473999: Pseudo dice [0.7035]
2024-12-07 09:59:07.474972: Epoch time: 410.3 s
2024-12-07 09:59:08.857677: 
2024-12-07 09:59:08.858978: Epoch 132
2024-12-07 09:59:08.859686: Current learning rate: 0.0088
2024-12-07 10:05:55.278799: Validation loss did not improve from -0.56515. Patience: 32/50
2024-12-07 10:05:55.279738: train_loss -0.7324
2024-12-07 10:05:55.280635: val_loss -0.4859
2024-12-07 10:05:55.281338: Pseudo dice [0.7226]
2024-12-07 10:05:55.282565: Epoch time: 406.42 s
2024-12-07 10:05:56.673010: 
2024-12-07 10:05:56.674465: Epoch 133
2024-12-07 10:05:56.675662: Current learning rate: 0.00879
2024-12-07 10:12:40.445851: Validation loss did not improve from -0.56515. Patience: 33/50
2024-12-07 10:12:40.446725: train_loss -0.7353
2024-12-07 10:12:40.447569: val_loss -0.5411
2024-12-07 10:12:40.448246: Pseudo dice [0.7385]
2024-12-07 10:12:40.448853: Epoch time: 403.77 s
2024-12-07 10:12:41.844514: 
2024-12-07 10:12:41.845860: Epoch 134
2024-12-07 10:12:41.846569: Current learning rate: 0.00879
2024-12-07 10:19:09.848799: Validation loss did not improve from -0.56515. Patience: 34/50
2024-12-07 10:19:09.849594: train_loss -0.7261
2024-12-07 10:19:09.850466: val_loss -0.5573
2024-12-07 10:19:09.851570: Pseudo dice [0.7446]
2024-12-07 10:19:09.852403: Epoch time: 388.01 s
2024-12-07 10:19:11.697438: 
2024-12-07 10:19:11.698341: Epoch 135
2024-12-07 10:19:11.699070: Current learning rate: 0.00878
2024-12-07 10:25:56.782730: Validation loss did not improve from -0.56515. Patience: 35/50
2024-12-07 10:25:56.783696: train_loss -0.7232
2024-12-07 10:25:56.784497: val_loss -0.5507
2024-12-07 10:25:56.785252: Pseudo dice [0.7474]
2024-12-07 10:25:56.785943: Epoch time: 405.09 s
2024-12-07 10:25:58.546691: 
2024-12-07 10:25:58.548022: Epoch 136
2024-12-07 10:25:58.548834: Current learning rate: 0.00877
2024-12-07 10:32:52.671170: Validation loss did not improve from -0.56515. Patience: 36/50
2024-12-07 10:32:52.675067: train_loss -0.7257
2024-12-07 10:32:52.677511: val_loss -0.5015
2024-12-07 10:32:52.678601: Pseudo dice [0.728]
2024-12-07 10:32:52.680261: Epoch time: 414.13 s
2024-12-07 10:32:54.122866: 
2024-12-07 10:32:54.124630: Epoch 137
2024-12-07 10:32:54.125656: Current learning rate: 0.00876
2024-12-07 10:39:43.535962: Validation loss did not improve from -0.56515. Patience: 37/50
2024-12-07 10:39:43.536909: train_loss -0.716
2024-12-07 10:39:43.537597: val_loss -0.5485
2024-12-07 10:39:43.538306: Pseudo dice [0.7398]
2024-12-07 10:39:43.538978: Epoch time: 409.42 s
2024-12-07 10:39:44.944236: 
2024-12-07 10:39:44.945503: Epoch 138
2024-12-07 10:39:44.946449: Current learning rate: 0.00875
2024-12-07 10:46:22.655271: Validation loss did not improve from -0.56515. Patience: 38/50
2024-12-07 10:46:22.656344: train_loss -0.7308
2024-12-07 10:46:22.657637: val_loss -0.5267
2024-12-07 10:46:22.658565: Pseudo dice [0.7357]
2024-12-07 10:46:22.659208: Epoch time: 397.71 s
2024-12-07 10:46:24.097119: 
2024-12-07 10:46:24.098382: Epoch 139
2024-12-07 10:46:24.099128: Current learning rate: 0.00874
2024-12-07 10:53:15.296571: Validation loss did not improve from -0.56515. Patience: 39/50
2024-12-07 10:53:15.297524: train_loss -0.7297
2024-12-07 10:53:15.298284: val_loss -0.5129
2024-12-07 10:53:15.298966: Pseudo dice [0.7389]
2024-12-07 10:53:15.300021: Epoch time: 411.2 s
2024-12-07 10:53:17.040420: 
2024-12-07 10:53:17.041904: Epoch 140
2024-12-07 10:53:17.042968: Current learning rate: 0.00873
2024-12-07 11:00:16.603630: Validation loss did not improve from -0.56515. Patience: 40/50
2024-12-07 11:00:16.604527: train_loss -0.7246
2024-12-07 11:00:16.605416: val_loss -0.5156
2024-12-07 11:00:16.606071: Pseudo dice [0.727]
2024-12-07 11:00:16.606776: Epoch time: 419.57 s
2024-12-07 11:00:18.020999: 
2024-12-07 11:00:18.022362: Epoch 141
2024-12-07 11:00:18.023160: Current learning rate: 0.00872
2024-12-07 11:07:06.954081: Validation loss did not improve from -0.56515. Patience: 41/50
2024-12-07 11:07:06.955351: train_loss -0.7251
2024-12-07 11:07:06.956432: val_loss -0.5435
2024-12-07 11:07:06.957119: Pseudo dice [0.7459]
2024-12-07 11:07:06.957832: Epoch time: 408.94 s
2024-12-07 11:07:08.381225: 
2024-12-07 11:07:08.382608: Epoch 142
2024-12-07 11:07:08.383339: Current learning rate: 0.00871
2024-12-07 11:13:53.608400: Validation loss did not improve from -0.56515. Patience: 42/50
2024-12-07 11:13:53.609272: train_loss -0.7274
2024-12-07 11:13:53.610337: val_loss -0.5187
2024-12-07 11:13:53.611315: Pseudo dice [0.7343]
2024-12-07 11:13:53.612116: Epoch time: 405.23 s
2024-12-07 11:13:55.020148: 
2024-12-07 11:13:55.021467: Epoch 143
2024-12-07 11:13:55.022364: Current learning rate: 0.0087
2024-12-07 11:20:55.330167: Validation loss did not improve from -0.56515. Patience: 43/50
2024-12-07 11:20:55.331426: train_loss -0.734
2024-12-07 11:20:55.332208: val_loss -0.5516
2024-12-07 11:20:55.333201: Pseudo dice [0.7565]
2024-12-07 11:20:55.333986: Epoch time: 420.31 s
2024-12-07 11:20:56.736698: 
2024-12-07 11:20:56.738007: Epoch 144
2024-12-07 11:20:56.738754: Current learning rate: 0.00869
2024-12-07 11:27:46.286231: Validation loss did not improve from -0.56515. Patience: 44/50
2024-12-07 11:27:46.287180: train_loss -0.7303
2024-12-07 11:27:46.288013: val_loss -0.533
2024-12-07 11:27:46.288631: Pseudo dice [0.7367]
2024-12-07 11:27:46.289386: Epoch time: 409.55 s
2024-12-07 11:27:48.036444: 
2024-12-07 11:27:48.037595: Epoch 145
2024-12-07 11:27:48.038234: Current learning rate: 0.00868
2024-12-07 11:34:36.361921: Validation loss did not improve from -0.56515. Patience: 45/50
2024-12-07 11:34:36.366843: train_loss -0.7305
2024-12-07 11:34:36.367999: val_loss -0.5181
2024-12-07 11:34:36.368692: Pseudo dice [0.733]
2024-12-07 11:34:36.369543: Epoch time: 408.33 s
2024-12-07 11:34:38.494918: 
2024-12-07 11:34:38.496246: Epoch 146
2024-12-07 11:34:38.496984: Current learning rate: 0.00868
2024-12-07 11:41:22.917221: Validation loss did not improve from -0.56515. Patience: 46/50
2024-12-07 11:41:22.919593: train_loss -0.733
2024-12-07 11:41:22.921311: val_loss -0.5089
2024-12-07 11:41:22.922438: Pseudo dice [0.7342]
2024-12-07 11:41:22.923386: Epoch time: 404.43 s
2024-12-07 11:41:24.336275: 
2024-12-07 11:41:24.337316: Epoch 147
2024-12-07 11:41:24.338287: Current learning rate: 0.00867
2024-12-07 11:48:17.856619: Validation loss did not improve from -0.56515. Patience: 47/50
2024-12-07 11:48:17.858198: train_loss -0.7326
2024-12-07 11:48:17.859361: val_loss -0.536
2024-12-07 11:48:17.860046: Pseudo dice [0.7396]
2024-12-07 11:48:17.860738: Epoch time: 413.52 s
2024-12-07 11:48:19.269238: 
2024-12-07 11:48:19.270474: Epoch 148
2024-12-07 11:48:19.271134: Current learning rate: 0.00866
2024-12-07 11:55:06.794392: Validation loss did not improve from -0.56515. Patience: 48/50
2024-12-07 11:55:06.795699: train_loss -0.7415
2024-12-07 11:55:06.796370: val_loss -0.5193
2024-12-07 11:55:06.796968: Pseudo dice [0.7293]
2024-12-07 11:55:06.797689: Epoch time: 407.53 s
2024-12-07 11:55:08.202127: 
2024-12-07 11:55:08.203354: Epoch 149
2024-12-07 11:55:08.204083: Current learning rate: 0.00865
2024-12-07 12:01:50.071918: Validation loss did not improve from -0.56515. Patience: 49/50
2024-12-07 12:01:50.072807: train_loss -0.7398
2024-12-07 12:01:50.073503: val_loss -0.5377
2024-12-07 12:01:50.074113: Pseudo dice [0.7438]
2024-12-07 12:01:50.074702: Epoch time: 401.87 s
2024-12-07 12:01:52.022849: 
2024-12-07 12:01:52.024051: Epoch 150
2024-12-07 12:01:52.025052: Current learning rate: 0.00864
2024-12-07 12:08:22.446938: Validation loss did not improve from -0.56515. Patience: 50/50
2024-12-07 12:08:22.448396: train_loss -0.7377
2024-12-07 12:08:22.449573: val_loss -0.5493
2024-12-07 12:08:22.450651: Pseudo dice [0.7454]
2024-12-07 12:08:22.451427: Epoch time: 390.43 s
2024-12-07 12:08:23.927678: Patience reached. Stopping training.
2024-12-07 12:08:24.353537: Training done.
2024-12-07 12:08:24.687659: Using splits from existing split file: /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset307_Sohee_Calcium_OCT_CrossValidation/splits_final.json
2024-12-07 12:08:24.718695: The split file contains 5 splits.
2024-12-07 12:08:24.719651: Desired fold for training: 1
2024-12-07 12:08:24.720606: This split has 6 training and 2 validation cases.
2024-12-07 12:08:24.721559: predicting 101-019
2024-12-07 12:08:24.792770: 101-019, shape torch.Size([1, 375, 498, 498]), rank 0
2024-12-07 12:10:50.966810: predicting 401-004
2024-12-07 12:10:50.984782: 401-004, shape torch.Size([1, 375, 498, 498]), rank 0
2024-12-07 12:13:25.815725: Validation complete
2024-12-07 12:13:25.816219: Mean Validation Dice:  0.7447499595381684

############################
INFO: You are using the old nnU-Net default plans. We have updated our recommendations. Please consider using those instead! Read more here: https://github.com/MIC-DKFZ/nnUNet/blob/master/documentation/resenc_presets.md
############################

Using device: cuda:0

#######################################################################
Please cite the following paper when using nnU-Net:
Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.
#######################################################################

2024-12-07 12:13:36.910630: Using torch.compile...

############################
INFO: You are using the old nnU-Net default plans. We have updated our recommendations. Please consider using those instead! Read more here: https://github.com/MIC-DKFZ/nnUNet/blob/master/documentation/resenc_presets.md
############################

Using device: cuda:0

#######################################################################
Please cite the following paper when using nnU-Net:
Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.
#######################################################################

2024-12-07 12:13:36.910146: Using torch.compile...
/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
################### Loading pretrained weights from file  /home/gridsan/nchutisilp/projects/OpenAI-CLIP/clip_pretrained_nnUNet.pt ###################
Below is the list of overlapping blocks in pretrained model and nnUNet architecture:
encoder.stages.0.0.convs.0.conv.weight shape torch.Size([32, 1, 3, 3, 3])
encoder.stages.0.0.convs.0.conv.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.norm.weight shape torch.Size([32])
encoder.stages.0.0.convs.0.norm.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.0.weight shape torch.Size([32, 1, 3, 3, 3])
encoder.stages.0.0.convs.0.all_modules.0.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.1.weight shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.1.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
encoder.stages.0.0.convs.1.conv.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.norm.weight shape torch.Size([32])
encoder.stages.0.0.convs.1.norm.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
encoder.stages.0.0.convs.1.all_modules.0.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.1.weight shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.1.bias shape torch.Size([32])
encoder.stages.1.0.convs.0.conv.weight shape torch.Size([64, 32, 3, 3, 3])
encoder.stages.1.0.convs.0.conv.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.norm.weight shape torch.Size([64])
encoder.stages.1.0.convs.0.norm.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.0.weight shape torch.Size([64, 32, 3, 3, 3])
encoder.stages.1.0.convs.0.all_modules.0.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.1.weight shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.1.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
encoder.stages.1.0.convs.1.conv.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.norm.weight shape torch.Size([64])
encoder.stages.1.0.convs.1.norm.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
encoder.stages.1.0.convs.1.all_modules.0.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.1.weight shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.1.bias shape torch.Size([64])
encoder.stages.2.0.convs.0.conv.weight shape torch.Size([128, 64, 3, 3, 3])
encoder.stages.2.0.convs.0.conv.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.norm.weight shape torch.Size([128])
encoder.stages.2.0.convs.0.norm.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.0.weight shape torch.Size([128, 64, 3, 3, 3])
encoder.stages.2.0.convs.0.all_modules.0.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.1.weight shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.1.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
encoder.stages.2.0.convs.1.conv.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.norm.weight shape torch.Size([128])
encoder.stages.2.0.convs.1.norm.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
encoder.stages.2.0.convs.1.all_modules.0.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.1.weight shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.1.bias shape torch.Size([128])
encoder.stages.3.0.convs.0.conv.weight shape torch.Size([256, 128, 3, 3, 3])
encoder.stages.3.0.convs.0.conv.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.norm.weight shape torch.Size([256])
encoder.stages.3.0.convs.0.norm.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.0.weight shape torch.Size([256, 128, 3, 3, 3])
encoder.stages.3.0.convs.0.all_modules.0.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.1.weight shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.1.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
encoder.stages.3.0.convs.1.conv.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.norm.weight shape torch.Size([256])
encoder.stages.3.0.convs.1.norm.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
encoder.stages.3.0.convs.1.all_modules.0.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.1.weight shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.1.bias shape torch.Size([256])
encoder.stages.4.0.convs.0.conv.weight shape torch.Size([320, 256, 3, 3, 3])
encoder.stages.4.0.convs.0.conv.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.norm.weight shape torch.Size([320])
encoder.stages.4.0.convs.0.norm.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.0.weight shape torch.Size([320, 256, 3, 3, 3])
encoder.stages.4.0.convs.0.all_modules.0.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.1.weight shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.1.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.4.0.convs.1.conv.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.norm.weight shape torch.Size([320])
encoder.stages.4.0.convs.1.norm.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.4.0.convs.1.all_modules.0.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.1.weight shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.1.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.0.conv.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.norm.weight shape torch.Size([320])
encoder.stages.5.0.convs.0.norm.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.0.all_modules.0.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.1.weight shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.1.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.1.conv.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.norm.weight shape torch.Size([320])
encoder.stages.5.0.convs.1.norm.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.1.all_modules.0.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.1.weight shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.0.0.convs.0.conv.weight shape torch.Size([32, 1, 3, 3, 3])
decoder.encoder.stages.0.0.convs.0.conv.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.norm.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.norm.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.0.weight shape torch.Size([32, 1, 3, 3, 3])
decoder.encoder.stages.0.0.convs.0.all_modules.0.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.1.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.1.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.encoder.stages.0.0.convs.1.conv.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.norm.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.norm.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.encoder.stages.0.0.convs.1.all_modules.0.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.1.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.1.bias shape torch.Size([32])
decoder.encoder.stages.1.0.convs.0.conv.weight shape torch.Size([64, 32, 3, 3, 3])
decoder.encoder.stages.1.0.convs.0.conv.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.norm.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.norm.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.0.weight shape torch.Size([64, 32, 3, 3, 3])
decoder.encoder.stages.1.0.convs.0.all_modules.0.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.1.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.1.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.encoder.stages.1.0.convs.1.conv.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.norm.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.norm.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.encoder.stages.1.0.convs.1.all_modules.0.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.1.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.1.bias shape torch.Size([64])
decoder.encoder.stages.2.0.convs.0.conv.weight shape torch.Size([128, 64, 3, 3, 3])
decoder.encoder.stages.2.0.convs.0.conv.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.norm.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.norm.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.0.weight shape torch.Size([128, 64, 3, 3, 3])
decoder.encoder.stages.2.0.convs.0.all_modules.0.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.1.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.1.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.encoder.stages.2.0.convs.1.conv.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.norm.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.norm.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.encoder.stages.2.0.convs.1.all_modules.0.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.1.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.1.bias shape torch.Size([128])
decoder.encoder.stages.3.0.convs.0.conv.weight shape torch.Size([256, 128, 3, 3, 3])
decoder.encoder.stages.3.0.convs.0.conv.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.norm.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.norm.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.0.weight shape torch.Size([256, 128, 3, 3, 3])
decoder.encoder.stages.3.0.convs.0.all_modules.0.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.1.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.1.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.encoder.stages.3.0.convs.1.conv.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.norm.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.norm.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.encoder.stages.3.0.convs.1.all_modules.0.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.1.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.1.bias shape torch.Size([256])
decoder.encoder.stages.4.0.convs.0.conv.weight shape torch.Size([320, 256, 3, 3, 3])
decoder.encoder.stages.4.0.convs.0.conv.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.norm.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.norm.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.0.weight shape torch.Size([320, 256, 3, 3, 3])
decoder.encoder.stages.4.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.4.0.convs.1.conv.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.norm.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.norm.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.4.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.0.conv.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.norm.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.norm.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.1.conv.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.norm.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.norm.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.stages.0.convs.0.conv.weight shape torch.Size([320, 640, 3, 3, 3])
decoder.stages.0.convs.0.conv.bias shape torch.Size([320])
decoder.stages.0.convs.0.norm.weight shape torch.Size([320])
decoder.stages.0.convs.0.norm.bias shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.0.weight shape torch.Size([320, 640, 3, 3, 3])
decoder.stages.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.stages.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.stages.0.convs.1.conv.bias shape torch.Size([320])
decoder.stages.0.convs.1.norm.weight shape torch.Size([320])
decoder.stages.0.convs.1.norm.bias shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.stages.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.stages.1.convs.0.conv.weight shape torch.Size([256, 512, 3, 3, 3])
decoder.stages.1.convs.0.conv.bias shape torch.Size([256])
decoder.stages.1.convs.0.norm.weight shape torch.Size([256])
decoder.stages.1.convs.0.norm.bias shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.0.weight shape torch.Size([256, 512, 3, 3, 3])
decoder.stages.1.convs.0.all_modules.0.bias shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.1.weight shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.1.bias shape torch.Size([256])
decoder.stages.1.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.stages.1.convs.1.conv.bias shape torch.Size([256])
decoder.stages.1.convs.1.norm.weight shape torch.Size([256])
decoder.stages.1.convs.1.norm.bias shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.stages.1.convs.1.all_modules.0.bias shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.1.weight shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.1.bias shape torch.Size([256])
decoder.stages.2.convs.0.conv.weight shape torch.Size([128, 256, 3, 3, 3])
decoder.stages.2.convs.0.conv.bias shape torch.Size([128])
decoder.stages.2.convs.0.norm.weight shape torch.Size([128])
decoder.stages.2.convs.0.norm.bias shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.0.weight shape torch.Size([128, 256, 3, 3, 3])
decoder.stages.2.convs.0.all_modules.0.bias shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.1.weight shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.1.bias shape torch.Size([128])
decoder.stages.2.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.stages.2.convs.1.conv.bias shape torch.Size([128])
decoder.stages.2.convs.1.norm.weight shape torch.Size([128])
decoder.stages.2.convs.1.norm.bias shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.stages.2.convs.1.all_modules.0.bias shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.1.weight shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.1.bias shape torch.Size([128])
decoder.stages.3.convs.0.conv.weight shape torch.Size([64, 128, 3, 3, 3])
decoder.stages.3.convs.0.conv.bias shape torch.Size([64])
decoder.stages.3.convs.0.norm.weight shape torch.Size([64])
decoder.stages.3.convs.0.norm.bias shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.0.weight shape torch.Size([64, 128, 3, 3, 3])
decoder.stages.3.convs.0.all_modules.0.bias shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.1.weight shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.1.bias shape torch.Size([64])
decoder.stages.3.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.stages.3.convs.1.conv.bias shape torch.Size([64])
decoder.stages.3.convs.1.norm.weight shape torch.Size([64])
decoder.stages.3.convs.1.norm.bias shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.stages.3.convs.1.all_modules.0.bias shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.1.weight shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.1.bias shape torch.Size([64])
decoder.stages.4.convs.0.conv.weight shape torch.Size([32, 64, 3, 3, 3])
decoder.stages.4.convs.0.conv.bias shape torch.Size([32])
decoder.stages.4.convs.0.norm.weight shape torch.Size([32])
decoder.stages.4.convs.0.norm.bias shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.0.weight shape torch.Size([32, 64, 3, 3, 3])
decoder.stages.4.convs.0.all_modules.0.bias shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.1.weight shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.1.bias shape torch.Size([32])
decoder.stages.4.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.stages.4.convs.1.conv.bias shape torch.Size([32])
decoder.stages.4.convs.1.norm.weight shape torch.Size([32])
decoder.stages.4.convs.1.norm.bias shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.stages.4.convs.1.all_modules.0.bias shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.1.weight shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.1.bias shape torch.Size([32])
decoder.transpconvs.0.weight shape torch.Size([320, 320, 1, 2, 2])
decoder.transpconvs.0.bias shape torch.Size([320])
decoder.transpconvs.1.weight shape torch.Size([320, 256, 2, 2, 2])
decoder.transpconvs.1.bias shape torch.Size([256])
decoder.transpconvs.2.weight shape torch.Size([256, 128, 2, 2, 2])
decoder.transpconvs.2.bias shape torch.Size([128])
decoder.transpconvs.3.weight shape torch.Size([128, 64, 2, 2, 2])
decoder.transpconvs.3.bias shape torch.Size([64])
decoder.transpconvs.4.weight shape torch.Size([64, 32, 2, 2, 2])
decoder.transpconvs.4.bias shape torch.Size([32])
################### Done ###################
2024-12-07 12:13:57.116360: do_dummy_2d_data_aug: True
2024-12-07 12:13:57.119931: Using splits from existing split file: /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset307_Sohee_Calcium_OCT_CrossValidation/splits_final.json
2024-12-07 12:13:57.121941: The split file contains 5 splits.
2024-12-07 12:13:57.123024: Desired fold for training: 3
2024-12-07 12:13:57.124001: This split has 7 training and 1 validation cases.
################### Loading pretrained weights from file  /home/gridsan/nchutisilp/projects/OpenAI-CLIP/clip_pretrained_nnUNet.pt ###################
Below is the list of overlapping blocks in pretrained model and nnUNet architecture:
encoder.stages.0.0.convs.0.conv.weight shape torch.Size([32, 1, 3, 3, 3])
encoder.stages.0.0.convs.0.conv.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.norm.weight shape torch.Size([32])
encoder.stages.0.0.convs.0.norm.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.0.weight shape torch.Size([32, 1, 3, 3, 3])
encoder.stages.0.0.convs.0.all_modules.0.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.1.weight shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.1.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
encoder.stages.0.0.convs.1.conv.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.norm.weight shape torch.Size([32])
encoder.stages.0.0.convs.1.norm.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
encoder.stages.0.0.convs.1.all_modules.0.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.1.weight shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.1.bias shape torch.Size([32])
encoder.stages.1.0.convs.0.conv.weight shape torch.Size([64, 32, 3, 3, 3])
encoder.stages.1.0.convs.0.conv.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.norm.weight shape torch.Size([64])
encoder.stages.1.0.convs.0.norm.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.0.weight shape torch.Size([64, 32, 3, 3, 3])
encoder.stages.1.0.convs.0.all_modules.0.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.1.weight shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.1.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
encoder.stages.1.0.convs.1.conv.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.norm.weight shape torch.Size([64])
encoder.stages.1.0.convs.1.norm.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
encoder.stages.1.0.convs.1.all_modules.0.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.1.weight shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.1.bias shape torch.Size([64])
encoder.stages.2.0.convs.0.conv.weight shape torch.Size([128, 64, 3, 3, 3])
encoder.stages.2.0.convs.0.conv.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.norm.weight shape torch.Size([128])
encoder.stages.2.0.convs.0.norm.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.0.weight shape torch.Size([128, 64, 3, 3, 3])
encoder.stages.2.0.convs.0.all_modules.0.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.1.weight shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.1.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
encoder.stages.2.0.convs.1.conv.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.norm.weight shape torch.Size([128])
encoder.stages.2.0.convs.1.norm.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
encoder.stages.2.0.convs.1.all_modules.0.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.1.weight shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.1.bias shape torch.Size([128])
encoder.stages.3.0.convs.0.conv.weight shape torch.Size([256, 128, 3, 3, 3])
encoder.stages.3.0.convs.0.conv.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.norm.weight shape torch.Size([256])
encoder.stages.3.0.convs.0.norm.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.0.weight shape torch.Size([256, 128, 3, 3, 3])
encoder.stages.3.0.convs.0.all_modules.0.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.1.weight shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.1.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
encoder.stages.3.0.convs.1.conv.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.norm.weight shape torch.Size([256])
encoder.stages.3.0.convs.1.norm.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
encoder.stages.3.0.convs.1.all_modules.0.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.1.weight shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.1.bias shape torch.Size([256])
encoder.stages.4.0.convs.0.conv.weight shape torch.Size([320, 256, 3, 3, 3])
encoder.stages.4.0.convs.0.conv.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.norm.weight shape torch.Size([320])
encoder.stages.4.0.convs.0.norm.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.0.weight shape torch.Size([320, 256, 3, 3, 3])
encoder.stages.4.0.convs.0.all_modules.0.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.1.weight shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.1.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.4.0.convs.1.conv.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.norm.weight shape torch.Size([320])
encoder.stages.4.0.convs.1.norm.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.4.0.convs.1.all_modules.0.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.1.weight shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.1.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.0.conv.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.norm.weight shape torch.Size([320])
encoder.stages.5.0.convs.0.norm.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.0.all_modules.0.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.1.weight shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.1.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.1.conv.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.norm.weight shape torch.Size([320])
encoder.stages.5.0.convs.1.norm.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.1.all_modules.0.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.1.weight shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.0.0.convs.0.conv.weight shape torch.Size([32, 1, 3, 3, 3])
decoder.encoder.stages.0.0.convs.0.conv.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.norm.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.norm.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.0.weight shape torch.Size([32, 1, 3, 3, 3])
decoder.encoder.stages.0.0.convs.0.all_modules.0.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.1.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.1.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.encoder.stages.0.0.convs.1.conv.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.norm.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.norm.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.encoder.stages.0.0.convs.1.all_modules.0.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.1.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.1.bias shape torch.Size([32])
decoder.encoder.stages.1.0.convs.0.conv.weight shape torch.Size([64, 32, 3, 3, 3])
decoder.encoder.stages.1.0.convs.0.conv.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.norm.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.norm.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.0.weight shape torch.Size([64, 32, 3, 3, 3])
decoder.encoder.stages.1.0.convs.0.all_modules.0.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.1.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.1.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.encoder.stages.1.0.convs.1.conv.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.norm.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.norm.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.encoder.stages.1.0.convs.1.all_modules.0.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.1.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.1.bias shape torch.Size([64])
decoder.encoder.stages.2.0.convs.0.conv.weight shape torch.Size([128, 64, 3, 3, 3])
decoder.encoder.stages.2.0.convs.0.conv.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.norm.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.norm.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.0.weight shape torch.Size([128, 64, 3, 3, 3])
decoder.encoder.stages.2.0.convs.0.all_modules.0.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.1.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.1.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.encoder.stages.2.0.convs.1.conv.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.norm.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.norm.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.encoder.stages.2.0.convs.1.all_modules.0.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.1.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.1.bias shape torch.Size([128])
decoder.encoder.stages.3.0.convs.0.conv.weight shape torch.Size([256, 128, 3, 3, 3])
decoder.encoder.stages.3.0.convs.0.conv.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.norm.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.norm.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.0.weight shape torch.Size([256, 128, 3, 3, 3])
decoder.encoder.stages.3.0.convs.0.all_modules.0.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.1.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.1.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.encoder.stages.3.0.convs.1.conv.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.norm.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.norm.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.encoder.stages.3.0.convs.1.all_modules.0.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.1.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.1.bias shape torch.Size([256])
decoder.encoder.stages.4.0.convs.0.conv.weight shape torch.Size([320, 256, 3, 3, 3])
decoder.encoder.stages.4.0.convs.0.conv.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.norm.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.norm.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.0.weight shape torch.Size([320, 256, 3, 3, 3])
decoder.encoder.stages.4.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.4.0.convs.1.conv.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.norm.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.norm.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.4.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.0.conv.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.norm.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.norm.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.1.conv.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.norm.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.norm.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.stages.0.convs.0.conv.weight shape torch.Size([320, 640, 3, 3, 3])
decoder.stages.0.convs.0.conv.bias shape torch.Size([320])
decoder.stages.0.convs.0.norm.weight shape torch.Size([320])
decoder.stages.0.convs.0.norm.bias shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.0.weight shape torch.Size([320, 640, 3, 3, 3])
decoder.stages.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.stages.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.stages.0.convs.1.conv.bias shape torch.Size([320])
decoder.stages.0.convs.1.norm.weight shape torch.Size([320])
decoder.stages.0.convs.1.norm.bias shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.stages.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.stages.1.convs.0.conv.weight shape torch.Size([256, 512, 3, 3, 3])
decoder.stages.1.convs.0.conv.bias shape torch.Size([256])
decoder.stages.1.convs.0.norm.weight shape torch.Size([256])
decoder.stages.1.convs.0.norm.bias shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.0.weight shape torch.Size([256, 512, 3, 3, 3])
decoder.stages.1.convs.0.all_modules.0.bias shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.1.weight shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.1.bias shape torch.Size([256])
decoder.stages.1.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.stages.1.convs.1.conv.bias shape torch.Size([256])
decoder.stages.1.convs.1.norm.weight shape torch.Size([256])
decoder.stages.1.convs.1.norm.bias shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.stages.1.convs.1.all_modules.0.bias shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.1.weight shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.1.bias shape torch.Size([256])
decoder.stages.2.convs.0.conv.weight shape torch.Size([128, 256, 3, 3, 3])
decoder.stages.2.convs.0.conv.bias shape torch.Size([128])
decoder.stages.2.convs.0.norm.weight shape torch.Size([128])
decoder.stages.2.convs.0.norm.bias shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.0.weight shape torch.Size([128, 256, 3, 3, 3])
decoder.stages.2.convs.0.all_modules.0.bias shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.1.weight shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.1.bias shape torch.Size([128])
decoder.stages.2.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.stages.2.convs.1.conv.bias shape torch.Size([128])
decoder.stages.2.convs.1.norm.weight shape torch.Size([128])
decoder.stages.2.convs.1.norm.bias shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.stages.2.convs.1.all_modules.0.bias shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.1.weight shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.1.bias shape torch.Size([128])
decoder.stages.3.convs.0.conv.weight shape torch.Size([64, 128, 3, 3, 3])
decoder.stages.3.convs.0.conv.bias shape torch.Size([64])
decoder.stages.3.convs.0.norm.weight shape torch.Size([64])
decoder.stages.3.convs.0.norm.bias shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.0.weight shape torch.Size([64, 128, 3, 3, 3])
decoder.stages.3.convs.0.all_modules.0.bias shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.1.weight shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.1.bias shape torch.Size([64])
decoder.stages.3.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.stages.3.convs.1.conv.bias shape torch.Size([64])
decoder.stages.3.convs.1.norm.weight shape torch.Size([64])
decoder.stages.3.convs.1.norm.bias shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.stages.3.convs.1.all_modules.0.bias shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.1.weight shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.1.bias shape torch.Size([64])
decoder.stages.4.convs.0.conv.weight shape torch.Size([32, 64, 3, 3, 3])
decoder.stages.4.convs.0.conv.bias shape torch.Size([32])
decoder.stages.4.convs.0.norm.weight shape torch.Size([32])
decoder.stages.4.convs.0.norm.bias shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.0.weight shape torch.Size([32, 64, 3, 3, 3])
decoder.stages.4.convs.0.all_modules.0.bias shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.1.weight shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.1.bias shape torch.Size([32])
decoder.stages.4.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.stages.4.convs.1.conv.bias shape torch.Size([32])
decoder.stages.4.convs.1.norm.weight shape torch.Size([32])
decoder.stages.4.convs.1.norm.bias shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.stages.4.convs.1.all_modules.0.bias shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.1.weight shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.1.bias shape torch.Size([32])
decoder.transpconvs.0.weight shape torch.Size([320, 320, 1, 2, 2])
decoder.transpconvs.0.bias shape torch.Size([320])
decoder.transpconvs.1.weight shape torch.Size([320, 256, 2, 2, 2])
decoder.transpconvs.1.bias shape torch.Size([256])
decoder.transpconvs.2.weight shape torch.Size([256, 128, 2, 2, 2])
decoder.transpconvs.2.bias shape torch.Size([128])
decoder.transpconvs.3.weight shape torch.Size([128, 64, 2, 2, 2])
decoder.transpconvs.3.bias shape torch.Size([64])
decoder.transpconvs.4.weight shape torch.Size([64, 32, 2, 2, 2])
decoder.transpconvs.4.bias shape torch.Size([32])
################### Done ###################
2024-12-07 12:13:57.116342: do_dummy_2d_data_aug: True
2024-12-07 12:13:57.119780: Using splits from existing split file: /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset307_Sohee_Calcium_OCT_CrossValidation/splits_final.json
2024-12-07 12:13:57.121668: The split file contains 5 splits.
2024-12-07 12:13:57.122681: Desired fold for training: 4
2024-12-07 12:13:57.123571: This split has 7 training and 1 validation cases.
using pin_memory on device 0
using pin_memory on device 0
using pin_memory on device 0

This is the configuration used by this training:
Configuration name: 3d_32x160x128_b10
 {'data_identifier': 'nnUNetPlans_3d_fullres', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 10, 'patch_size': [32, 160, 128], 'median_image_size_in_voxels': [375.0, 498.0, 498.0], 'spacing': [1.0, 1.0, 1.0], 'normalization_schemes': ['ZScoreNormalization'], 'use_mask_for_norm': [False], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'architecture': {'network_class_name': 'dynamic_network_architectures.architectures.unet.PlainConvUNet', 'arch_kwargs': {'n_stages': 6, 'features_per_stage': [32, 64, 128, 256, 320, 320], 'conv_op': 'torch.nn.modules.conv.Conv3d', 'kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]], 'strides': [[1, 1, 1], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [1, 2, 2]], 'n_conv_per_stage': [2, 2, 2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2, 2, 2], 'conv_bias': True, 'norm_op': 'torch.nn.modules.instancenorm.InstanceNorm3d', 'norm_op_kwargs': {'eps': 1e-05, 'affine': True}, 'dropout_op': None, 'dropout_op_kwargs': None, 'nonlin': 'torch.nn.LeakyReLU', 'nonlin_kwargs': {'inplace': True}, 'deep_supervision': True}, '_kw_requires_import': ['conv_op', 'norm_op', 'dropout_op', 'nonlin']}, 'batch_dice': True, 'inherits_from': '3d_fullres'} 

These are the global plan.json settings:
 {'dataset_name': 'Dataset307_Sohee_Calcium_OCT_CrossValidation', 'plans_name': 'nnUNetPlans', 'original_median_spacing_after_transp': [1.0, 1.0, 1.0], 'original_median_shape_after_transp': [375, 498, 498], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 0.4977976381778717, 'mean': 0.13507525622844696, 'median': 0.09599608182907104, 'min': 0.0, 'percentile_00_5': 0.014754901640117168, 'percentile_99_5': 0.4977976381778717, 'std': 0.12025152146816254}}} 

2024-12-07 12:14:25.961044: unpacking dataset...
using pin_memory on device 0

This is the configuration used by this training:
Configuration name: 3d_32x160x128_b10
 {'data_identifier': 'nnUNetPlans_3d_fullres', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 10, 'patch_size': [32, 160, 128], 'median_image_size_in_voxels': [375.0, 498.0, 498.0], 'spacing': [1.0, 1.0, 1.0], 'normalization_schemes': ['ZScoreNormalization'], 'use_mask_for_norm': [False], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'architecture': {'network_class_name': 'dynamic_network_architectures.architectures.unet.PlainConvUNet', 'arch_kwargs': {'n_stages': 6, 'features_per_stage': [32, 64, 128, 256, 320, 320], 'conv_op': 'torch.nn.modules.conv.Conv3d', 'kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]], 'strides': [[1, 1, 1], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [1, 2, 2]], 'n_conv_per_stage': [2, 2, 2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2, 2, 2], 'conv_bias': True, 'norm_op': 'torch.nn.modules.instancenorm.InstanceNorm3d', 'norm_op_kwargs': {'eps': 1e-05, 'affine': True}, 'dropout_op': None, 'dropout_op_kwargs': None, 'nonlin': 'torch.nn.LeakyReLU', 'nonlin_kwargs': {'inplace': True}, 'deep_supervision': True}, '_kw_requires_import': ['conv_op', 'norm_op', 'dropout_op', 'nonlin']}, 'batch_dice': True, 'inherits_from': '3d_fullres'} 

These are the global plan.json settings:
 {'dataset_name': 'Dataset307_Sohee_Calcium_OCT_CrossValidation', 'plans_name': 'nnUNetPlans', 'original_median_spacing_after_transp': [1.0, 1.0, 1.0], 'original_median_shape_after_transp': [375, 498, 498], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 0.4977976381778717, 'mean': 0.13507525622844696, 'median': 0.09599608182907104, 'min': 0.0, 'percentile_00_5': 0.014754901640117168, 'percentile_99_5': 0.4977976381778717, 'std': 0.12025152146816254}}} 

2024-12-07 12:14:28.388800: unpacking dataset...
2024-12-07 12:14:33.077169: unpacking done...
2024-12-07 12:14:33.105200: Unable to plot network architecture: nnUNet_compile is enabled!
2024-12-07 12:14:33.191307: 
2024-12-07 12:14:33.192650: Epoch 0
2024-12-07 12:14:33.193669: Current learning rate: 0.01
2024-12-07 12:20:12.490393: Validation loss improved from 1000.00000 to -0.18324! Patience: 0/50
2024-12-07 12:20:12.491452: train_loss -0.0939
2024-12-07 12:20:12.492288: val_loss -0.1832
2024-12-07 12:20:12.492939: Pseudo dice [0.5216]
2024-12-07 12:20:12.493618: Epoch time: 339.3 s
2024-12-07 12:20:12.494352: Yayy! New best EMA pseudo Dice: 0.5216
2024-12-07 12:20:14.688208: 
2024-12-07 12:20:14.690087: Epoch 1
2024-12-07 12:20:14.691339: Current learning rate: 0.00999
2024-12-07 12:26:56.787539: Validation loss did not improve from -0.18324. Patience: 1/50
2024-12-07 12:26:56.788541: train_loss -0.2421
2024-12-07 12:26:56.789466: val_loss -0.0963
2024-12-07 12:26:56.790283: Pseudo dice [0.5091]
2024-12-07 12:26:56.790996: Epoch time: 402.1 s
2024-12-07 12:26:58.165787: 
2024-12-07 12:26:58.167154: Epoch 2
2024-12-07 12:26:58.167940: Current learning rate: 0.00998
2024-12-07 12:33:58.070248: Validation loss improved from -0.18324 to -0.20807! Patience: 1/50
2024-12-07 12:33:58.071340: train_loss -0.2794
2024-12-07 12:33:58.072174: val_loss -0.2081
2024-12-07 12:33:58.072886: Pseudo dice [0.5525]
2024-12-07 12:33:58.073547: Epoch time: 419.91 s
2024-12-07 12:33:58.074215: Yayy! New best EMA pseudo Dice: 0.5236
2024-12-07 12:33:59.880544: 
2024-12-07 12:33:59.881724: Epoch 3
2024-12-07 12:33:59.882502: Current learning rate: 0.00997
2024-12-07 12:40:47.066405: Validation loss did not improve from -0.20807. Patience: 1/50
2024-12-07 12:40:47.067378: train_loss -0.312
2024-12-07 12:40:47.068215: val_loss -0.1451
2024-12-07 12:40:47.069004: Pseudo dice [0.4618]
2024-12-07 12:40:47.069765: Epoch time: 407.19 s
2024-12-07 12:40:48.444468: 
2024-12-07 12:40:48.445625: Epoch 4
2024-12-07 12:40:48.446466: Current learning rate: 0.00996
2024-12-07 12:47:16.805368: Validation loss improved from -0.20807 to -0.25150! Patience: 1/50
2024-12-07 12:47:16.806485: train_loss -0.3474
2024-12-07 12:47:16.807387: val_loss -0.2515
2024-12-07 12:47:16.808028: Pseudo dice [0.5753]
2024-12-07 12:47:16.808717: Epoch time: 388.36 s
2024-12-07 12:47:18.626992: 
2024-12-07 12:47:18.628458: Epoch 5
2024-12-07 12:47:18.629399: Current learning rate: 0.00995
2024-12-07 12:53:49.988672: Validation loss did not improve from -0.25150. Patience: 1/50
2024-12-07 12:53:49.989637: train_loss -0.3662
2024-12-07 12:53:49.990562: val_loss -0.2029
2024-12-07 12:53:49.991265: Pseudo dice [0.5558]
2024-12-07 12:53:49.992010: Epoch time: 391.36 s
2024-12-07 12:53:49.992814: Yayy! New best EMA pseudo Dice: 0.5265
2024-12-07 12:53:51.744988: 
2024-12-07 12:53:51.746336: Epoch 6
2024-12-07 12:53:51.747135: Current learning rate: 0.00995
2024-12-07 13:00:24.224851: Validation loss improved from -0.25150 to -0.26660! Patience: 1/50
2024-12-07 13:00:24.225998: train_loss -0.3886
2024-12-07 13:00:24.226888: val_loss -0.2666
2024-12-07 13:00:24.227646: Pseudo dice [0.565]
2024-12-07 13:00:24.228742: Epoch time: 392.48 s
2024-12-07 13:00:24.229800: Yayy! New best EMA pseudo Dice: 0.5303
2024-12-07 13:00:26.035382: 
2024-12-07 13:00:26.036782: Epoch 7
2024-12-07 13:00:26.037956: Current learning rate: 0.00994
2024-12-07 13:06:50.561128: Validation loss improved from -0.26660 to -0.31615! Patience: 0/50
2024-12-07 13:06:50.561995: train_loss -0.4117
2024-12-07 13:06:50.562850: val_loss -0.3161
2024-12-07 13:06:50.563668: Pseudo dice [0.6168]
2024-12-07 13:06:50.564516: Epoch time: 384.53 s
2024-12-07 13:06:50.565326: Yayy! New best EMA pseudo Dice: 0.539
2024-12-07 13:06:52.885576: 
2024-12-07 13:06:52.886886: Epoch 8
2024-12-07 13:06:52.887580: Current learning rate: 0.00993
2024-12-07 13:13:32.099344: Validation loss did not improve from -0.31615. Patience: 1/50
2024-12-07 13:13:32.100293: train_loss -0.4415
2024-12-07 13:13:32.101022: val_loss -0.2707
2024-12-07 13:13:32.101751: Pseudo dice [0.6052]
2024-12-07 13:13:32.102394: Epoch time: 399.22 s
2024-12-07 13:13:32.103096: Yayy! New best EMA pseudo Dice: 0.5456
2024-12-07 13:13:33.949152: 
2024-12-07 13:13:33.950287: Epoch 9
2024-12-07 13:13:33.951103: Current learning rate: 0.00992
2024-12-07 13:20:07.817424: Validation loss improved from -0.31615 to -0.34293! Patience: 1/50
2024-12-07 13:20:07.833476: train_loss -0.452
2024-12-07 13:20:07.834299: val_loss -0.3429
2024-12-07 13:20:07.835035: Pseudo dice [0.6472]
2024-12-07 13:20:07.835829: Epoch time: 393.89 s
2024-12-07 13:20:08.316782: Yayy! New best EMA pseudo Dice: 0.5557
2024-12-07 13:20:10.122795: 
2024-12-07 13:20:10.124117: Epoch 10
2024-12-07 13:20:10.124963: Current learning rate: 0.00991
2024-12-07 13:26:40.071584: Validation loss did not improve from -0.34293. Patience: 1/50
2024-12-07 13:26:40.073648: train_loss -0.4573
2024-12-07 13:26:40.075008: val_loss -0.2733
2024-12-07 13:26:40.075844: Pseudo dice [0.5938]
2024-12-07 13:26:40.076725: Epoch time: 389.95 s
2024-12-07 13:26:40.077535: Yayy! New best EMA pseudo Dice: 0.5595
2024-12-07 13:26:41.978896: 
2024-12-07 13:26:41.980160: Epoch 11
2024-12-07 13:26:41.980905: Current learning rate: 0.0099
2024-12-07 13:33:14.766407: Validation loss did not improve from -0.34293. Patience: 2/50
2024-12-07 13:33:14.767401: train_loss -0.4734
2024-12-07 13:33:14.768192: val_loss -0.2049
2024-12-07 13:33:14.769006: Pseudo dice [0.524]
2024-12-07 13:33:14.769772: Epoch time: 392.79 s
2024-12-07 13:33:16.142564: 
2024-12-07 13:33:16.143837: Epoch 12
2024-12-07 13:33:16.144716: Current learning rate: 0.00989
2024-12-07 13:39:52.980070: Validation loss did not improve from -0.34293. Patience: 3/50
2024-12-07 13:39:52.980902: train_loss -0.4792
2024-12-07 13:39:52.981793: val_loss -0.3172
2024-12-07 13:39:52.982509: Pseudo dice [0.6262]
2024-12-07 13:39:52.983281: Epoch time: 396.84 s
2024-12-07 13:39:52.984019: Yayy! New best EMA pseudo Dice: 0.563
2024-12-07 13:39:54.787797: 
2024-12-07 13:39:54.789215: Epoch 13
2024-12-07 13:39:54.790004: Current learning rate: 0.00988
2024-12-07 13:46:15.861523: Validation loss did not improve from -0.34293. Patience: 4/50
2024-12-07 13:46:15.862629: train_loss -0.4992
2024-12-07 13:46:15.863473: val_loss -0.3372
2024-12-07 13:46:15.864366: Pseudo dice [0.6378]
2024-12-07 13:46:15.865273: Epoch time: 381.08 s
2024-12-07 13:46:15.866066: Yayy! New best EMA pseudo Dice: 0.5705
2024-12-07 13:46:17.672402: 
2024-12-07 13:46:17.673573: Epoch 14
2024-12-07 13:46:17.674346: Current learning rate: 0.00987
2024-12-07 13:52:36.896609: Validation loss improved from -0.34293 to -0.34748! Patience: 4/50
2024-12-07 13:52:36.897561: train_loss -0.5191
2024-12-07 13:52:36.898378: val_loss -0.3475
2024-12-07 13:52:36.899037: Pseudo dice [0.6507]
2024-12-07 13:52:36.899657: Epoch time: 379.23 s
2024-12-07 13:52:37.342458: Yayy! New best EMA pseudo Dice: 0.5785
2024-12-07 13:52:39.154305: 
2024-12-07 13:52:39.156034: Epoch 15
2024-12-07 13:52:39.156834: Current learning rate: 0.00986
2024-12-07 13:59:00.826765: Validation loss improved from -0.34748 to -0.39250! Patience: 0/50
2024-12-07 13:59:00.827760: train_loss -0.5104
2024-12-07 13:59:00.828584: val_loss -0.3925
2024-12-07 13:59:00.829353: Pseudo dice [0.656]
2024-12-07 13:59:00.830015: Epoch time: 381.67 s
2024-12-07 13:59:00.830699: Yayy! New best EMA pseudo Dice: 0.5863
2024-12-07 13:59:02.651093: 
2024-12-07 13:59:02.652390: Epoch 16
2024-12-07 13:59:02.653236: Current learning rate: 0.00986
2024-12-07 14:05:27.736017: Validation loss did not improve from -0.39250. Patience: 1/50
2024-12-07 14:05:27.737540: train_loss -0.5191
2024-12-07 14:05:27.738693: val_loss -0.3701
2024-12-07 14:05:27.739574: Pseudo dice [0.6488]
2024-12-07 14:05:27.740719: Epoch time: 385.09 s
2024-12-07 14:05:27.741453: Yayy! New best EMA pseudo Dice: 0.5925
2024-12-07 14:05:29.593552: 
2024-12-07 14:05:29.595071: Epoch 17
2024-12-07 14:05:29.595915: Current learning rate: 0.00985
2024-12-07 14:12:08.962253: Validation loss did not improve from -0.39250. Patience: 2/50
2024-12-07 14:12:08.963340: train_loss -0.5308
2024-12-07 14:12:08.964150: val_loss -0.347
2024-12-07 14:12:08.965204: Pseudo dice [0.6477]
2024-12-07 14:12:08.966051: Epoch time: 399.37 s
2024-12-07 14:12:08.966670: Yayy! New best EMA pseudo Dice: 0.598
2024-12-07 14:12:10.761363: 
2024-12-07 14:12:10.762544: Epoch 18
2024-12-07 14:12:10.763269: Current learning rate: 0.00984
2024-12-07 14:18:45.188948: Validation loss did not improve from -0.39250. Patience: 3/50
2024-12-07 14:18:45.190065: train_loss -0.5433
2024-12-07 14:18:45.190799: val_loss -0.2984
2024-12-07 14:18:45.191451: Pseudo dice [0.6255]
2024-12-07 14:18:45.192149: Epoch time: 394.43 s
2024-12-07 14:18:45.192829: Yayy! New best EMA pseudo Dice: 0.6008
2024-12-07 14:18:47.934362: 
2024-12-07 14:18:47.935640: Epoch 19
2024-12-07 14:18:47.936363: Current learning rate: 0.00983
2024-12-07 14:25:40.005916: Validation loss did not improve from -0.39250. Patience: 4/50
2024-12-07 14:25:40.008097: train_loss -0.5405
2024-12-07 14:25:40.009012: val_loss -0.3326
2024-12-07 14:25:40.009733: Pseudo dice [0.6286]
2024-12-07 14:25:40.010669: Epoch time: 412.07 s
2024-12-07 14:25:40.443991: Yayy! New best EMA pseudo Dice: 0.6036
2024-12-07 14:25:42.256844: 
2024-12-07 14:25:42.258099: Epoch 20
2024-12-07 14:25:42.258960: Current learning rate: 0.00982
2024-12-07 14:32:27.193912: Validation loss did not improve from -0.39250. Patience: 5/50
2024-12-07 14:32:27.195039: train_loss -0.5459
2024-12-07 14:32:27.196198: val_loss -0.3133
2024-12-07 14:32:27.197356: Pseudo dice [0.632]
2024-12-07 14:32:27.198293: Epoch time: 404.94 s
2024-12-07 14:32:27.199422: Yayy! New best EMA pseudo Dice: 0.6064
2024-12-07 14:32:29.055343: 
2024-12-07 14:32:29.056820: Epoch 21
2024-12-07 14:32:29.057913: Current learning rate: 0.00981
2024-12-07 14:39:06.698272: Validation loss did not improve from -0.39250. Patience: 6/50
2024-12-07 14:39:06.699056: train_loss -0.54
2024-12-07 14:39:06.700008: val_loss -0.3409
2024-12-07 14:39:06.700777: Pseudo dice [0.6175]
2024-12-07 14:39:06.701445: Epoch time: 397.65 s
2024-12-07 14:39:06.702118: Yayy! New best EMA pseudo Dice: 0.6075
2024-12-07 14:39:08.437254: 
2024-12-07 14:39:08.438514: Epoch 22
2024-12-07 14:39:08.439347: Current learning rate: 0.0098
2024-12-07 14:45:50.215380: Validation loss did not improve from -0.39250. Patience: 7/50
2024-12-07 14:45:50.216366: train_loss -0.5536
2024-12-07 14:45:50.217347: val_loss -0.3743
2024-12-07 14:45:50.218125: Pseudo dice [0.6614]
2024-12-07 14:45:50.218907: Epoch time: 401.78 s
2024-12-07 14:45:50.219709: Yayy! New best EMA pseudo Dice: 0.6129
2024-12-07 14:45:52.015633: 
2024-12-07 14:45:52.017154: Epoch 23
2024-12-07 14:45:52.018204: Current learning rate: 0.00979
2024-12-07 14:52:48.518463: Validation loss improved from -0.39250 to -0.48325! Patience: 7/50
2024-12-07 14:52:48.519488: train_loss -0.5601
2024-12-07 14:52:48.520251: val_loss -0.4832
2024-12-07 14:52:48.520880: Pseudo dice [0.7207]
2024-12-07 14:52:48.521562: Epoch time: 416.51 s
2024-12-07 14:52:48.522238: Yayy! New best EMA pseudo Dice: 0.6237
2024-12-07 14:52:50.264752: 
2024-12-07 14:52:50.266039: Epoch 24
2024-12-07 14:52:50.266999: Current learning rate: 0.00978
2024-12-07 14:59:28.307873: Validation loss did not improve from -0.48325. Patience: 1/50
2024-12-07 14:59:28.308660: train_loss -0.5703
2024-12-07 14:59:28.309395: val_loss -0.3226
2024-12-07 14:59:28.310066: Pseudo dice [0.6263]
2024-12-07 14:59:28.310713: Epoch time: 398.05 s
2024-12-07 14:59:28.782626: Yayy! New best EMA pseudo Dice: 0.6239
2024-12-07 14:59:30.587338: 
2024-12-07 14:59:30.588865: Epoch 25
2024-12-07 14:59:30.589741: Current learning rate: 0.00977
2024-12-07 15:06:27.816902: Validation loss did not improve from -0.48325. Patience: 2/50
2024-12-07 15:06:27.817950: train_loss -0.5599
2024-12-07 15:06:27.818739: val_loss -0.3508
2024-12-07 15:06:27.819465: Pseudo dice [0.6466]
2024-12-07 15:06:27.820102: Epoch time: 417.23 s
2024-12-07 15:06:27.820802: Yayy! New best EMA pseudo Dice: 0.6262
2024-12-07 15:06:29.553359: 
2024-12-07 15:06:29.554725: Epoch 26
2024-12-07 15:06:29.555597: Current learning rate: 0.00977
2024-12-07 15:13:07.549129: Validation loss did not improve from -0.48325. Patience: 3/50
2024-12-07 15:13:07.550059: train_loss -0.5705
2024-12-07 15:13:07.550785: val_loss -0.3869
2024-12-07 15:13:07.551465: Pseudo dice [0.6734]
2024-12-07 15:13:07.552259: Epoch time: 398.0 s
2024-12-07 15:13:07.553052: Yayy! New best EMA pseudo Dice: 0.6309
2024-12-07 15:13:09.297008: 
2024-12-07 15:13:09.298190: Epoch 27
2024-12-07 15:13:09.299017: Current learning rate: 0.00976
2024-12-07 15:19:51.620438: Validation loss did not improve from -0.48325. Patience: 4/50
2024-12-07 15:19:51.621452: train_loss -0.5693
2024-12-07 15:19:51.622239: val_loss -0.3549
2024-12-07 15:19:51.623077: Pseudo dice [0.6555]
2024-12-07 15:19:51.623851: Epoch time: 402.33 s
2024-12-07 15:19:51.624615: Yayy! New best EMA pseudo Dice: 0.6334
2024-12-07 15:19:53.410846: 
2024-12-07 15:19:53.412201: Epoch 28
2024-12-07 15:19:53.412995: Current learning rate: 0.00975
2024-12-07 15:27:01.003273: Validation loss did not improve from -0.48325. Patience: 5/50
2024-12-07 15:27:01.011309: train_loss -0.5664
2024-12-07 15:27:01.013090: val_loss -0.3553
2024-12-07 15:27:01.014132: Pseudo dice [0.6717]
2024-12-07 15:27:01.015198: Epoch time: 427.6 s
2024-12-07 15:27:01.016260: Yayy! New best EMA pseudo Dice: 0.6372
2024-12-07 15:27:03.276881: 
2024-12-07 15:27:03.278188: Epoch 29
2024-12-07 15:27:03.279064: Current learning rate: 0.00974
2024-12-07 15:33:52.143114: Validation loss did not improve from -0.48325. Patience: 6/50
2024-12-07 15:33:52.145657: train_loss -0.5838
2024-12-07 15:33:52.146770: val_loss -0.4194
2024-12-07 15:33:52.147800: Pseudo dice [0.6889]
2024-12-07 15:33:52.148830: Epoch time: 408.87 s
2024-12-07 15:33:52.541635: Yayy! New best EMA pseudo Dice: 0.6424
2024-12-07 15:33:54.360675: 
2024-12-07 15:33:54.362220: Epoch 30
2024-12-07 15:33:54.363102: Current learning rate: 0.00973
2024-12-07 15:40:22.657971: Validation loss did not improve from -0.48325. Patience: 7/50
2024-12-07 15:40:22.659341: train_loss -0.5717
2024-12-07 15:40:22.660582: val_loss -0.3896
2024-12-07 15:40:22.661499: Pseudo dice [0.6688]
2024-12-07 15:40:22.662186: Epoch time: 388.3 s
2024-12-07 15:40:22.663046: Yayy! New best EMA pseudo Dice: 0.645
2024-12-07 15:40:24.536774: 
2024-12-07 15:40:24.538245: Epoch 31
2024-12-07 15:40:24.539272: Current learning rate: 0.00972
2024-12-07 15:47:23.795186: Validation loss did not improve from -0.48325. Patience: 8/50
2024-12-07 15:47:23.796156: train_loss -0.5774
2024-12-07 15:47:23.796997: val_loss -0.414
2024-12-07 15:47:23.797748: Pseudo dice [0.6912]
2024-12-07 15:47:23.798536: Epoch time: 419.26 s
2024-12-07 15:47:23.799252: Yayy! New best EMA pseudo Dice: 0.6496
2024-12-07 15:47:25.574041: 
2024-12-07 15:47:25.575480: Epoch 32
2024-12-07 15:47:25.576421: Current learning rate: 0.00971
2024-12-07 15:54:32.308674: Validation loss did not improve from -0.48325. Patience: 9/50
2024-12-07 15:54:32.309789: train_loss -0.5902
2024-12-07 15:54:32.310905: val_loss -0.3739
2024-12-07 15:54:32.312079: Pseudo dice [0.6448]
2024-12-07 15:54:32.313174: Epoch time: 426.74 s
2024-12-07 15:54:33.721006: 
2024-12-07 15:54:33.722151: Epoch 33
2024-12-07 15:54:33.722888: Current learning rate: 0.0097
2024-12-07 16:01:31.009893: Validation loss did not improve from -0.48325. Patience: 10/50
2024-12-07 16:01:31.011068: train_loss -0.5792
2024-12-07 16:01:31.011972: val_loss -0.4241
2024-12-07 16:01:31.012646: Pseudo dice [0.6855]
2024-12-07 16:01:31.013377: Epoch time: 417.29 s
2024-12-07 16:01:31.014212: Yayy! New best EMA pseudo Dice: 0.6528
2024-12-07 16:01:32.849344: 
2024-12-07 16:01:32.850601: Epoch 34
2024-12-07 16:01:32.851258: Current learning rate: 0.00969
2024-12-07 16:08:16.471123: Validation loss did not improve from -0.48325. Patience: 11/50
2024-12-07 16:08:16.472119: train_loss -0.5943
2024-12-07 16:08:16.472898: val_loss -0.3331
2024-12-07 16:08:16.473638: Pseudo dice [0.6385]
2024-12-07 16:08:16.474279: Epoch time: 403.62 s
2024-12-07 16:08:18.494184: 
2024-12-07 16:08:18.495340: Epoch 35
2024-12-07 16:08:18.496122: Current learning rate: 0.00968
2024-12-07 16:15:12.622570: Validation loss did not improve from -0.48325. Patience: 12/50
2024-12-07 16:15:12.623584: train_loss -0.6036
2024-12-07 16:15:12.624396: val_loss -0.3114
2024-12-07 16:15:12.625055: Pseudo dice [0.6212]
2024-12-07 16:15:12.625885: Epoch time: 414.13 s
2024-12-07 16:15:14.046575: 
2024-12-07 16:15:14.047927: Epoch 36
2024-12-07 16:15:14.048728: Current learning rate: 0.00968
2024-12-07 16:21:42.333646: Validation loss did not improve from -0.48325. Patience: 13/50
2024-12-07 16:21:42.334641: train_loss -0.6059
2024-12-07 16:21:42.335514: val_loss -0.1937
2024-12-07 16:21:42.336399: Pseudo dice [0.5386]
2024-12-07 16:21:42.337095: Epoch time: 388.29 s
2024-12-07 16:21:43.747546: 
2024-12-07 16:21:43.748886: Epoch 37
2024-12-07 16:21:43.749878: Current learning rate: 0.00967
2024-12-07 16:28:56.473084: Validation loss did not improve from -0.48325. Patience: 14/50
2024-12-07 16:28:56.474203: train_loss -0.6062
2024-12-07 16:28:56.474940: val_loss -0.3893
2024-12-07 16:28:56.475677: Pseudo dice [0.6764]
2024-12-07 16:28:56.476356: Epoch time: 432.73 s
2024-12-07 16:28:57.902402: 
2024-12-07 16:28:57.903722: Epoch 38
2024-12-07 16:28:57.904548: Current learning rate: 0.00966
2024-12-07 16:35:44.839153: Validation loss did not improve from -0.48325. Patience: 15/50
2024-12-07 16:35:44.905150: train_loss -0.6022
2024-12-07 16:35:44.907192: val_loss -0.3143
2024-12-07 16:35:44.908176: Pseudo dice [0.5969]
2024-12-07 16:35:44.909175: Epoch time: 407.0 s
2024-12-07 16:35:46.500865: 
2024-12-07 16:35:46.502038: Epoch 39
2024-12-07 16:35:46.502765: Current learning rate: 0.00965
2024-12-07 16:42:19.144401: Validation loss did not improve from -0.48325. Patience: 16/50
2024-12-07 16:42:19.145278: train_loss -0.5974
2024-12-07 16:42:19.146075: val_loss -0.4408
2024-12-07 16:42:19.146745: Pseudo dice [0.6986]
2024-12-07 16:42:19.147541: Epoch time: 392.65 s
2024-12-07 16:42:21.470068: 
2024-12-07 16:42:21.470963: Epoch 40
2024-12-07 16:42:21.471663: Current learning rate: 0.00964
2024-12-07 16:49:25.867351: Validation loss did not improve from -0.48325. Patience: 17/50
2024-12-07 16:49:25.868330: train_loss -0.6179
2024-12-07 16:49:25.869095: val_loss -0.4267
2024-12-07 16:49:25.869727: Pseudo dice [0.6803]
2024-12-07 16:49:25.870361: Epoch time: 424.4 s
2024-12-07 16:49:27.329850: 
2024-12-07 16:49:27.331225: Epoch 41
2024-12-07 16:49:27.331987: Current learning rate: 0.00963
2024-12-07 16:56:16.365121: Validation loss did not improve from -0.48325. Patience: 18/50
2024-12-07 16:56:16.366000: train_loss -0.6258
2024-12-07 16:56:16.366812: val_loss -0.4232
2024-12-07 16:56:16.367585: Pseudo dice [0.668]
2024-12-07 16:56:16.368278: Epoch time: 409.04 s
2024-12-07 16:56:17.710372: 
2024-12-07 16:56:17.711684: Epoch 42
2024-12-07 16:56:17.712532: Current learning rate: 0.00962
2024-12-07 17:03:03.780832: Validation loss did not improve from -0.48325. Patience: 19/50
2024-12-07 17:03:03.781752: train_loss -0.6205
2024-12-07 17:03:03.782611: val_loss -0.3425
2024-12-07 17:03:03.783339: Pseudo dice [0.6514]
2024-12-07 17:03:03.784044: Epoch time: 406.07 s
2024-12-07 17:03:05.145938: 
2024-12-07 17:03:05.147566: Epoch 43
2024-12-07 17:03:05.148504: Current learning rate: 0.00961
2024-12-07 17:09:43.734220: Validation loss did not improve from -0.48325. Patience: 20/50
2024-12-07 17:09:43.735149: train_loss -0.6273
2024-12-07 17:09:43.735816: val_loss -0.2376
2024-12-07 17:09:43.736567: Pseudo dice [0.5712]
2024-12-07 17:09:43.737180: Epoch time: 398.59 s
2024-12-07 17:09:45.094138: 
2024-12-07 17:09:45.095183: Epoch 44
2024-12-07 17:09:45.095968: Current learning rate: 0.0096
2024-12-07 17:16:43.816225: Validation loss did not improve from -0.48325. Patience: 21/50
2024-12-07 17:16:43.817195: train_loss -0.6253
2024-12-07 17:16:43.818021: val_loss -0.3591
2024-12-07 17:16:43.818687: Pseudo dice [0.6592]
2024-12-07 17:16:43.819520: Epoch time: 418.72 s
2024-12-07 17:16:45.655244: 
2024-12-07 17:16:45.656534: Epoch 45
2024-12-07 17:16:45.657267: Current learning rate: 0.00959
2024-12-07 17:23:28.792498: Validation loss did not improve from -0.48325. Patience: 22/50
2024-12-07 17:23:28.793568: train_loss -0.6306
2024-12-07 17:23:28.794428: val_loss -0.4214
2024-12-07 17:23:28.795117: Pseudo dice [0.6894]
2024-12-07 17:23:28.795740: Epoch time: 403.14 s
2024-12-07 17:23:30.145198: 
2024-12-07 17:23:30.146480: Epoch 46
2024-12-07 17:23:30.147410: Current learning rate: 0.00959
2024-12-07 17:30:32.234939: Validation loss did not improve from -0.48325. Patience: 23/50
2024-12-07 17:30:32.235941: train_loss -0.622
2024-12-07 17:30:32.236795: val_loss -0.3757
2024-12-07 17:30:32.237724: Pseudo dice [0.6694]
2024-12-07 17:30:32.238515: Epoch time: 422.09 s
2024-12-07 17:30:33.610525: 
2024-12-07 17:30:33.611843: Epoch 47
2024-12-07 17:30:33.612671: Current learning rate: 0.00958
2024-12-07 17:37:16.683716: Validation loss did not improve from -0.48325. Patience: 24/50
2024-12-07 17:37:16.688112: train_loss -0.623
2024-12-07 17:37:16.690148: val_loss -0.4591
2024-12-07 17:37:16.690958: Pseudo dice [0.7189]
2024-12-07 17:37:16.692226: Epoch time: 403.08 s
2024-12-07 17:37:16.693061: Yayy! New best EMA pseudo Dice: 0.6568
2024-12-07 17:37:18.462163: 
2024-12-07 17:37:18.463442: Epoch 48
2024-12-07 17:37:18.464915: Current learning rate: 0.00957
2024-12-07 17:44:17.586607: Validation loss did not improve from -0.48325. Patience: 25/50
2024-12-07 17:44:17.588500: train_loss -0.6318
2024-12-07 17:44:17.589592: val_loss -0.3748
2024-12-07 17:44:17.590487: Pseudo dice [0.6645]
2024-12-07 17:44:17.591560: Epoch time: 419.13 s
2024-12-07 17:44:17.592599: Yayy! New best EMA pseudo Dice: 0.6576
2024-12-07 17:44:19.302799: 
2024-12-07 17:44:19.304193: Epoch 49
2024-12-07 17:44:19.304917: Current learning rate: 0.00956
2024-12-07 17:51:01.021385: Validation loss did not improve from -0.48325. Patience: 26/50
2024-12-07 17:51:01.022569: train_loss -0.6421
2024-12-07 17:51:01.023942: val_loss -0.3148
2024-12-07 17:51:01.024803: Pseudo dice [0.6267]
2024-12-07 17:51:01.025625: Epoch time: 401.72 s
2024-12-07 17:51:03.466725: 
2024-12-07 17:51:03.468189: Epoch 50
2024-12-07 17:51:03.469060: Current learning rate: 0.00955
2024-12-07 17:57:54.442545: Validation loss improved from -0.48325 to -0.49760! Patience: 26/50
2024-12-07 17:57:54.443594: train_loss -0.6383
2024-12-07 17:57:54.444531: val_loss -0.4976
2024-12-07 17:57:54.445350: Pseudo dice [0.727]
2024-12-07 17:57:54.446019: Epoch time: 410.98 s
2024-12-07 17:57:54.446697: Yayy! New best EMA pseudo Dice: 0.6617
2024-12-07 17:57:56.252326: 
2024-12-07 17:57:56.253955: Epoch 51
2024-12-07 17:57:56.254956: Current learning rate: 0.00954
2024-12-07 18:04:39.585778: Validation loss did not improve from -0.49760. Patience: 1/50
2024-12-07 18:04:39.586859: train_loss -0.6371
2024-12-07 18:04:39.587717: val_loss -0.3727
2024-12-07 18:04:39.588358: Pseudo dice [0.6577]
2024-12-07 18:04:39.589046: Epoch time: 403.34 s
2024-12-07 18:04:40.988386: 
2024-12-07 18:04:40.989719: Epoch 52
2024-12-07 18:04:40.990714: Current learning rate: 0.00953
2024-12-07 18:11:43.785127: Validation loss did not improve from -0.49760. Patience: 2/50
2024-12-07 18:11:43.785874: train_loss -0.6368
2024-12-07 18:11:43.786654: val_loss -0.4363
2024-12-07 18:11:43.787430: Pseudo dice [0.6936]
2024-12-07 18:11:43.788217: Epoch time: 422.8 s
2024-12-07 18:11:43.788972: Yayy! New best EMA pseudo Dice: 0.6646
2024-12-07 18:11:45.589748: 
2024-12-07 18:11:45.591100: Epoch 53
2024-12-07 18:11:45.591817: Current learning rate: 0.00952
2024-12-07 18:18:25.976882: Validation loss did not improve from -0.49760. Patience: 3/50
2024-12-07 18:18:25.977833: train_loss -0.6339
2024-12-07 18:18:25.978840: val_loss -0.4528
2024-12-07 18:18:25.979543: Pseudo dice [0.7048]
2024-12-07 18:18:25.980419: Epoch time: 400.39 s
2024-12-07 18:18:25.981261: Yayy! New best EMA pseudo Dice: 0.6686
2024-12-07 18:18:27.794188: 
2024-12-07 18:18:27.795544: Epoch 54
2024-12-07 18:18:27.796355: Current learning rate: 0.00951
2024-12-07 18:25:25.489803: Validation loss did not improve from -0.49760. Patience: 4/50
2024-12-07 18:25:25.490632: train_loss -0.6527
2024-12-07 18:25:25.491809: val_loss -0.404
2024-12-07 18:25:25.492782: Pseudo dice [0.6553]
2024-12-07 18:25:25.493737: Epoch time: 417.7 s
2024-12-07 18:25:27.355753: 
2024-12-07 18:25:27.357021: Epoch 55
2024-12-07 18:25:27.357861: Current learning rate: 0.0095
2024-12-07 18:32:19.019747: Validation loss did not improve from -0.49760. Patience: 5/50
2024-12-07 18:32:19.020635: train_loss -0.642
2024-12-07 18:32:19.021424: val_loss -0.4673
2024-12-07 18:32:19.022202: Pseudo dice [0.7087]
2024-12-07 18:32:19.022847: Epoch time: 411.67 s
2024-12-07 18:32:19.023509: Yayy! New best EMA pseudo Dice: 0.6714
2024-12-07 18:32:20.815521: 
2024-12-07 18:32:20.816882: Epoch 56
2024-12-07 18:32:20.817682: Current learning rate: 0.00949
2024-12-07 18:39:00.558278: Validation loss did not improve from -0.49760. Patience: 6/50
2024-12-07 18:39:00.559302: train_loss -0.6366
2024-12-07 18:39:00.560154: val_loss -0.4265
2024-12-07 18:39:00.560934: Pseudo dice [0.6879]
2024-12-07 18:39:00.561592: Epoch time: 399.75 s
2024-12-07 18:39:00.562281: Yayy! New best EMA pseudo Dice: 0.673
2024-12-07 18:39:02.347130: 
2024-12-07 18:39:02.348466: Epoch 57
2024-12-07 18:39:02.349277: Current learning rate: 0.00949
2024-12-07 18:45:40.736302: Validation loss did not improve from -0.49760. Patience: 7/50
2024-12-07 18:45:40.741425: train_loss -0.6477
2024-12-07 18:45:40.743071: val_loss -0.4573
2024-12-07 18:45:40.743871: Pseudo dice [0.7001]
2024-12-07 18:45:40.744937: Epoch time: 398.39 s
2024-12-07 18:45:40.745708: Yayy! New best EMA pseudo Dice: 0.6758
2024-12-07 18:45:42.526750: 
2024-12-07 18:45:42.527929: Epoch 58
2024-12-07 18:45:42.528718: Current learning rate: 0.00948
2024-12-07 18:52:29.737526: Validation loss did not improve from -0.49760. Patience: 8/50
2024-12-07 18:52:29.738698: train_loss -0.6473
2024-12-07 18:52:29.739511: val_loss -0.3739
2024-12-07 18:52:29.740186: Pseudo dice [0.6835]
2024-12-07 18:52:29.740906: Epoch time: 407.21 s
2024-12-07 18:52:29.741671: Yayy! New best EMA pseudo Dice: 0.6765
2024-12-07 18:52:31.530213: 
2024-12-07 18:52:31.531555: Epoch 59
2024-12-07 18:52:31.532377: Current learning rate: 0.00947
2024-12-07 18:59:29.099245: Validation loss did not improve from -0.49760. Patience: 9/50
2024-12-07 18:59:29.100523: train_loss -0.6481
2024-12-07 18:59:29.101891: val_loss -0.3389
2024-12-07 18:59:29.102828: Pseudo dice [0.6503]
2024-12-07 18:59:29.103686: Epoch time: 417.57 s
2024-12-07 18:59:30.870912: 
2024-12-07 18:59:30.872261: Epoch 60
2024-12-07 18:59:30.872943: Current learning rate: 0.00946
2024-12-07 19:06:20.536122: Validation loss did not improve from -0.49760. Patience: 10/50
2024-12-07 19:06:20.537008: train_loss -0.6478
2024-12-07 19:06:20.537741: val_loss -0.369
2024-12-07 19:06:20.538453: Pseudo dice [0.6688]
2024-12-07 19:06:20.539119: Epoch time: 409.67 s
2024-12-07 19:06:22.569536: 
2024-12-07 19:06:22.570757: Epoch 61
2024-12-07 19:06:22.571603: Current learning rate: 0.00945
2024-12-07 19:13:26.911513: Validation loss did not improve from -0.49760. Patience: 11/50
2024-12-07 19:13:26.912498: train_loss -0.6478
2024-12-07 19:13:26.913391: val_loss -0.4086
2024-12-07 19:13:26.914045: Pseudo dice [0.6892]
2024-12-07 19:13:26.914808: Epoch time: 424.34 s
2024-12-07 19:13:28.297193: 
2024-12-07 19:13:28.298014: Epoch 62
2024-12-07 19:13:28.300182: Current learning rate: 0.00944
2024-12-07 19:20:23.181500: Validation loss did not improve from -0.49760. Patience: 12/50
2024-12-07 19:20:23.182568: train_loss -0.651
2024-12-07 19:20:23.183529: val_loss -0.3909
2024-12-07 19:20:23.184525: Pseudo dice [0.6618]
2024-12-07 19:20:23.185308: Epoch time: 414.89 s
2024-12-07 19:20:24.581598: 
2024-12-07 19:20:24.582902: Epoch 63
2024-12-07 19:20:24.583609: Current learning rate: 0.00943
2024-12-07 19:27:22.464198: Validation loss did not improve from -0.49760. Patience: 13/50
2024-12-07 19:27:22.465215: train_loss -0.6446
2024-12-07 19:27:22.465975: val_loss -0.4803
2024-12-07 19:27:22.466681: Pseudo dice [0.7163]
2024-12-07 19:27:22.467426: Epoch time: 417.88 s
2024-12-07 19:27:22.468024: Yayy! New best EMA pseudo Dice: 0.6779
2024-12-07 19:27:24.311852: 
2024-12-07 19:27:24.313274: Epoch 64
2024-12-07 19:27:24.314011: Current learning rate: 0.00942
2024-12-07 19:34:12.383013: Validation loss did not improve from -0.49760. Patience: 14/50
2024-12-07 19:34:12.383896: train_loss -0.6538
2024-12-07 19:34:12.385116: val_loss -0.392
2024-12-07 19:34:12.386163: Pseudo dice [0.6646]
2024-12-07 19:34:12.387224: Epoch time: 408.07 s
2024-12-07 19:34:14.213158: 
2024-12-07 19:34:14.214607: Epoch 65
2024-12-07 19:34:14.215503: Current learning rate: 0.00941
2024-12-07 19:41:01.371012: Validation loss did not improve from -0.49760. Patience: 15/50
2024-12-07 19:41:01.371839: train_loss -0.6484
2024-12-07 19:41:01.372556: val_loss -0.4688
2024-12-07 19:41:01.373327: Pseudo dice [0.726]
2024-12-07 19:41:01.373999: Epoch time: 407.16 s
2024-12-07 19:41:01.374662: Yayy! New best EMA pseudo Dice: 0.6815
2024-12-07 19:41:03.163659: 
2024-12-07 19:41:03.164604: Epoch 66
2024-12-07 19:41:03.165450: Current learning rate: 0.0094
2024-12-07 19:47:54.263813: Validation loss did not improve from -0.49760. Patience: 16/50
2024-12-07 19:47:54.266944: train_loss -0.6508
2024-12-07 19:47:54.268605: val_loss -0.3644
2024-12-07 19:47:54.269439: Pseudo dice [0.6488]
2024-12-07 19:47:54.270328: Epoch time: 411.1 s
2024-12-07 19:47:55.726574: 
2024-12-07 19:47:55.727850: Epoch 67
2024-12-07 19:47:55.728579: Current learning rate: 0.00939
2024-12-07 19:54:50.660167: Validation loss did not improve from -0.49760. Patience: 17/50
2024-12-07 19:54:50.662366: train_loss -0.6522
2024-12-07 19:54:50.663217: val_loss -0.3595
2024-12-07 19:54:50.663997: Pseudo dice [0.6608]
2024-12-07 19:54:50.664991: Epoch time: 414.94 s
2024-12-07 19:54:52.116375: 
2024-12-07 19:54:52.117782: Epoch 68
2024-12-07 19:54:52.119256: Current learning rate: 0.00939
2024-12-07 20:01:29.925981: Validation loss did not improve from -0.49760. Patience: 18/50
2024-12-07 20:01:29.927137: train_loss -0.6597
2024-12-07 20:01:29.928248: val_loss -0.4321
2024-12-07 20:01:29.928971: Pseudo dice [0.6932]
2024-12-07 20:01:29.929689: Epoch time: 397.81 s
2024-12-07 20:01:31.336263: 
2024-12-07 20:01:31.337460: Epoch 69
2024-12-07 20:01:31.338134: Current learning rate: 0.00938
2024-12-07 20:08:33.548498: Validation loss did not improve from -0.49760. Patience: 19/50
2024-12-07 20:08:33.549302: train_loss -0.6625
2024-12-07 20:08:33.550014: val_loss -0.3736
2024-12-07 20:08:33.550606: Pseudo dice [0.671]
2024-12-07 20:08:33.551302: Epoch time: 422.21 s
2024-12-07 20:08:35.329179: 
2024-12-07 20:08:35.330168: Epoch 70
2024-12-07 20:08:35.330917: Current learning rate: 0.00937
2024-12-07 20:15:24.567136: Validation loss did not improve from -0.49760. Patience: 20/50
2024-12-07 20:15:24.568061: train_loss -0.6669
2024-12-07 20:15:24.568903: val_loss -0.4015
2024-12-07 20:15:24.569667: Pseudo dice [0.6776]
2024-12-07 20:15:24.570412: Epoch time: 409.24 s
2024-12-07 20:15:26.013191: 
2024-12-07 20:15:26.014678: Epoch 71
2024-12-07 20:15:26.015675: Current learning rate: 0.00936
2024-12-07 20:22:21.805665: Validation loss did not improve from -0.49760. Patience: 21/50
2024-12-07 20:22:21.806674: train_loss -0.6707
2024-12-07 20:22:21.807564: val_loss -0.3501
2024-12-07 20:22:21.808239: Pseudo dice [0.634]
2024-12-07 20:22:21.808921: Epoch time: 415.79 s
2024-12-07 20:22:23.574919: 
2024-12-07 20:22:23.576360: Epoch 72
2024-12-07 20:22:23.577105: Current learning rate: 0.00935
2024-12-07 20:29:15.159895: Validation loss did not improve from -0.49760. Patience: 22/50
2024-12-07 20:29:15.160794: train_loss -0.67
2024-12-07 20:29:15.161593: val_loss -0.3689
2024-12-07 20:29:15.162341: Pseudo dice [0.6494]
2024-12-07 20:29:15.162986: Epoch time: 411.59 s
2024-12-07 20:29:16.577435: 
2024-12-07 20:29:16.578456: Epoch 73
2024-12-07 20:29:16.579237: Current learning rate: 0.00934
2024-12-07 20:36:05.370006: Validation loss did not improve from -0.49760. Patience: 23/50
2024-12-07 20:36:05.370944: train_loss -0.6738
2024-12-07 20:36:05.371829: val_loss -0.4617
2024-12-07 20:36:05.372724: Pseudo dice [0.7122]
2024-12-07 20:36:05.373725: Epoch time: 408.79 s
2024-12-07 20:36:06.874611: 
2024-12-07 20:36:06.875927: Epoch 74
2024-12-07 20:36:06.876662: Current learning rate: 0.00933
2024-12-07 20:43:05.556421: Validation loss improved from -0.49760 to -0.50542! Patience: 23/50
2024-12-07 20:43:05.557289: train_loss -0.6754
2024-12-07 20:43:05.558141: val_loss -0.5054
2024-12-07 20:43:05.558828: Pseudo dice [0.7346]
2024-12-07 20:43:05.559472: Epoch time: 418.68 s
2024-12-07 20:43:07.443726: 
2024-12-07 20:43:07.445081: Epoch 75
2024-12-07 20:43:07.445889: Current learning rate: 0.00932
2024-12-07 20:49:53.363952: Validation loss did not improve from -0.50542. Patience: 1/50
2024-12-07 20:49:53.364826: train_loss -0.6774
2024-12-07 20:49:53.365715: val_loss -0.3909
2024-12-07 20:49:53.366547: Pseudo dice [0.6603]
2024-12-07 20:49:53.367466: Epoch time: 405.92 s
2024-12-07 20:49:54.834969: 
2024-12-07 20:49:54.836240: Epoch 76
2024-12-07 20:49:54.837307: Current learning rate: 0.00931
2024-12-07 20:56:41.665797: Validation loss did not improve from -0.50542. Patience: 2/50
2024-12-07 20:56:41.683432: train_loss -0.686
2024-12-07 20:56:41.684380: val_loss -0.3615
2024-12-07 20:56:41.684991: Pseudo dice [0.6691]
2024-12-07 20:56:41.685794: Epoch time: 406.83 s
2024-12-07 20:56:43.173724: 
2024-12-07 20:56:43.175116: Epoch 77
2024-12-07 20:56:43.175978: Current learning rate: 0.0093
2024-12-07 21:03:37.847571: Validation loss did not improve from -0.50542. Patience: 3/50
2024-12-07 21:03:37.848901: train_loss -0.6805
2024-12-07 21:03:37.849727: val_loss -0.3172
2024-12-07 21:03:37.850500: Pseudo dice [0.6415]
2024-12-07 21:03:37.851205: Epoch time: 414.68 s
2024-12-07 21:03:39.296147: 
2024-12-07 21:03:39.297257: Epoch 78
2024-12-07 21:03:39.298051: Current learning rate: 0.0093
2024-12-07 21:10:25.118490: Validation loss did not improve from -0.50542. Patience: 4/50
2024-12-07 21:10:25.119946: train_loss -0.6783
2024-12-07 21:10:25.121260: val_loss -0.4405
2024-12-07 21:10:25.122065: Pseudo dice [0.6931]
2024-12-07 21:10:25.122748: Epoch time: 405.82 s
2024-12-07 21:10:26.561897: 
2024-12-07 21:10:26.563280: Epoch 79
2024-12-07 21:10:26.564270: Current learning rate: 0.00929
2024-12-07 21:17:24.320759: Validation loss did not improve from -0.50542. Patience: 5/50
2024-12-07 21:17:24.321808: train_loss -0.6895
2024-12-07 21:17:24.322609: val_loss -0.4247
2024-12-07 21:17:24.323329: Pseudo dice [0.6822]
2024-12-07 21:17:24.324021: Epoch time: 417.76 s
2024-12-07 21:17:26.218345: 
2024-12-07 21:17:26.219439: Epoch 80
2024-12-07 21:17:26.220186: Current learning rate: 0.00928
2024-12-07 21:24:04.380920: Validation loss did not improve from -0.50542. Patience: 6/50
2024-12-07 21:24:04.382445: train_loss -0.6856
2024-12-07 21:24:04.383428: val_loss -0.3901
2024-12-07 21:24:04.384147: Pseudo dice [0.6691]
2024-12-07 21:24:04.384888: Epoch time: 398.17 s
2024-12-07 21:24:05.825671: 
2024-12-07 21:24:05.827241: Epoch 81
2024-12-07 21:24:05.828221: Current learning rate: 0.00927
2024-12-07 21:30:57.067411: Validation loss did not improve from -0.50542. Patience: 7/50
2024-12-07 21:30:57.068355: train_loss -0.685
2024-12-07 21:30:57.069094: val_loss -0.353
2024-12-07 21:30:57.069835: Pseudo dice [0.6619]
2024-12-07 21:30:57.070677: Epoch time: 411.24 s
2024-12-07 21:30:58.900794: 
2024-12-07 21:30:58.902082: Epoch 82
2024-12-07 21:30:58.902922: Current learning rate: 0.00926
2024-12-07 21:38:01.975049: Validation loss did not improve from -0.50542. Patience: 8/50
2024-12-07 21:38:01.976092: train_loss -0.6902
2024-12-07 21:38:01.976886: val_loss -0.4567
2024-12-07 21:38:01.977675: Pseudo dice [0.7214]
2024-12-07 21:38:01.978446: Epoch time: 423.08 s
2024-12-07 21:38:03.324812: 
2024-12-07 21:38:03.326003: Epoch 83
2024-12-07 21:38:03.326845: Current learning rate: 0.00925
2024-12-07 21:44:43.082763: Validation loss did not improve from -0.50542. Patience: 9/50
2024-12-07 21:44:43.083806: train_loss -0.6823
2024-12-07 21:44:43.084931: val_loss -0.3467
2024-12-07 21:44:43.085937: Pseudo dice [0.6357]
2024-12-07 21:44:43.086823: Epoch time: 399.76 s
2024-12-07 21:44:44.495054: 
2024-12-07 21:44:44.496495: Epoch 84
2024-12-07 21:44:44.497348: Current learning rate: 0.00924
2024-12-07 21:51:46.281332: Validation loss did not improve from -0.50542. Patience: 10/50
2024-12-07 21:51:46.282296: train_loss -0.6882
2024-12-07 21:51:46.283242: val_loss -0.3292
2024-12-07 21:51:46.284026: Pseudo dice [0.6425]
2024-12-07 21:51:46.284706: Epoch time: 421.79 s
2024-12-07 21:51:48.119283: 
2024-12-07 21:51:48.120815: Epoch 85
2024-12-07 21:51:48.121820: Current learning rate: 0.00923
2024-12-07 21:58:32.470317: Validation loss did not improve from -0.50542. Patience: 11/50
2024-12-07 21:58:32.473433: train_loss -0.6947
2024-12-07 21:58:32.475066: val_loss -0.3618
2024-12-07 21:58:32.475755: Pseudo dice [0.6591]
2024-12-07 21:58:32.476639: Epoch time: 404.36 s
2024-12-07 21:58:33.852792: 
2024-12-07 21:58:33.853956: Epoch 86
2024-12-07 21:58:33.854674: Current learning rate: 0.00922
2024-12-07 22:05:32.584188: Validation loss did not improve from -0.50542. Patience: 12/50
2024-12-07 22:05:32.585981: train_loss -0.6886
2024-12-07 22:05:32.587002: val_loss -0.3432
2024-12-07 22:05:32.587962: Pseudo dice [0.64]
2024-12-07 22:05:32.589175: Epoch time: 418.73 s
2024-12-07 22:05:34.018873: 
2024-12-07 22:05:34.020186: Epoch 87
2024-12-07 22:05:34.021203: Current learning rate: 0.00921
2024-12-07 22:12:11.308286: Validation loss did not improve from -0.50542. Patience: 13/50
2024-12-07 22:12:11.309360: train_loss -0.6935
2024-12-07 22:12:11.310359: val_loss -0.3482
2024-12-07 22:12:11.311103: Pseudo dice [0.6489]
2024-12-07 22:12:11.311889: Epoch time: 397.29 s
2024-12-07 22:12:12.673748: 
2024-12-07 22:12:12.675116: Epoch 88
2024-12-07 22:12:12.675922: Current learning rate: 0.0092
2024-12-07 22:18:59.804110: Validation loss did not improve from -0.50542. Patience: 14/50
2024-12-07 22:18:59.805048: train_loss -0.6955
2024-12-07 22:18:59.806005: val_loss -0.5039
2024-12-07 22:18:59.806971: Pseudo dice [0.7393]
2024-12-07 22:18:59.808002: Epoch time: 407.13 s
2024-12-07 22:19:01.201356: 
2024-12-07 22:19:01.202560: Epoch 89
2024-12-07 22:19:01.203562: Current learning rate: 0.0092
2024-12-07 22:25:53.371143: Validation loss did not improve from -0.50542. Patience: 15/50
2024-12-07 22:25:53.372169: train_loss -0.6909
2024-12-07 22:25:53.373000: val_loss -0.2905
2024-12-07 22:25:53.373692: Pseudo dice [0.6184]
2024-12-07 22:25:53.374414: Epoch time: 412.17 s
2024-12-07 22:25:55.232395: 
2024-12-07 22:25:55.233495: Epoch 90
2024-12-07 22:25:55.234339: Current learning rate: 0.00919
2024-12-07 22:32:38.198747: Validation loss did not improve from -0.50542. Patience: 16/50
2024-12-07 22:32:38.199593: train_loss -0.6914
2024-12-07 22:32:38.200632: val_loss -0.3556
2024-12-07 22:32:38.201430: Pseudo dice [0.6515]
2024-12-07 22:32:38.202144: Epoch time: 402.97 s
2024-12-07 22:32:39.574033: 
2024-12-07 22:32:39.575497: Epoch 91
2024-12-07 22:32:39.576332: Current learning rate: 0.00918
2024-12-07 22:39:28.340915: Validation loss did not improve from -0.50542. Patience: 17/50
2024-12-07 22:39:28.341720: train_loss -0.7037
2024-12-07 22:39:28.342515: val_loss -0.3986
2024-12-07 22:39:28.343334: Pseudo dice [0.6792]
2024-12-07 22:39:28.344127: Epoch time: 408.77 s
2024-12-07 22:39:29.688221: 
2024-12-07 22:39:29.689690: Epoch 92
2024-12-07 22:39:29.690466: Current learning rate: 0.00917
2024-12-07 22:46:17.065385: Validation loss did not improve from -0.50542. Patience: 18/50
2024-12-07 22:46:17.066293: train_loss -0.697
2024-12-07 22:46:17.067099: val_loss -0.3569
2024-12-07 22:46:17.067776: Pseudo dice [0.6371]
2024-12-07 22:46:17.068451: Epoch time: 407.38 s
2024-12-07 22:46:18.792300: 
2024-12-07 22:46:18.793466: Epoch 93
2024-12-07 22:46:18.794164: Current learning rate: 0.00916
2024-12-07 22:53:18.818450: Validation loss did not improve from -0.50542. Patience: 19/50
2024-12-07 22:53:18.819250: train_loss -0.693
2024-12-07 22:53:18.820689: val_loss -0.4509
2024-12-07 22:53:18.821975: Pseudo dice [0.7046]
2024-12-07 22:53:18.823098: Epoch time: 420.03 s
2024-12-07 22:53:20.178195: 
2024-12-07 22:53:20.179577: Epoch 94
2024-12-07 22:53:20.180488: Current learning rate: 0.00915
2024-12-07 23:00:07.048563: Validation loss did not improve from -0.50542. Patience: 20/50
2024-12-07 23:00:07.049726: train_loss -0.6938
2024-12-07 23:00:07.050931: val_loss -0.2298
2024-12-07 23:00:07.052029: Pseudo dice [0.5973]
2024-12-07 23:00:07.053127: Epoch time: 406.87 s
2024-12-07 23:00:08.865912: 
2024-12-07 23:00:08.867154: Epoch 95
2024-12-07 23:00:08.868050: Current learning rate: 0.00914
2024-12-07 23:06:51.878465: Validation loss did not improve from -0.50542. Patience: 21/50
2024-12-07 23:06:51.880281: train_loss -0.7
2024-12-07 23:06:51.881306: val_loss -0.4028
2024-12-07 23:06:51.882089: Pseudo dice [0.6663]
2024-12-07 23:06:51.883028: Epoch time: 403.02 s
2024-12-07 23:06:53.315516: 
2024-12-07 23:06:53.316920: Epoch 96
2024-12-07 23:06:53.317732: Current learning rate: 0.00913
2024-12-07 23:13:52.202962: Validation loss did not improve from -0.50542. Patience: 22/50
2024-12-07 23:13:52.204073: train_loss -0.7027
2024-12-07 23:13:52.204931: val_loss -0.4805
2024-12-07 23:13:52.205697: Pseudo dice [0.7276]
2024-12-07 23:13:52.206475: Epoch time: 418.89 s
2024-12-07 23:13:53.583820: 
2024-12-07 23:13:53.585345: Epoch 97
2024-12-07 23:13:53.586170: Current learning rate: 0.00912
2024-12-07 23:20:36.945609: Validation loss did not improve from -0.50542. Patience: 23/50
2024-12-07 23:20:36.946612: train_loss -0.7039
2024-12-07 23:20:36.947478: val_loss -0.4499
2024-12-07 23:20:36.948138: Pseudo dice [0.7094]
2024-12-07 23:20:36.948909: Epoch time: 403.36 s
2024-12-07 23:20:38.336909: 
2024-12-07 23:20:38.338184: Epoch 98
2024-12-07 23:20:38.338975: Current learning rate: 0.00911
2024-12-07 23:27:25.356881: Validation loss did not improve from -0.50542. Patience: 24/50
2024-12-07 23:27:25.357952: train_loss -0.7022
2024-12-07 23:27:25.359575: val_loss -0.4145
2024-12-07 23:27:25.360440: Pseudo dice [0.6843]
2024-12-07 23:27:25.361398: Epoch time: 407.02 s
2024-12-07 23:27:26.730031: 
2024-12-07 23:27:26.731186: Epoch 99
2024-12-07 23:27:26.732072: Current learning rate: 0.0091
2024-12-07 23:34:13.163569: Validation loss did not improve from -0.50542. Patience: 25/50
2024-12-07 23:34:13.165058: train_loss -0.7008
2024-12-07 23:34:13.166070: val_loss -0.3981
2024-12-07 23:34:13.166864: Pseudo dice [0.6691]
2024-12-07 23:34:13.167548: Epoch time: 406.44 s
2024-12-07 23:34:15.031881: 
2024-12-07 23:34:15.033385: Epoch 100
2024-12-07 23:34:15.034251: Current learning rate: 0.0091
2024-12-07 23:41:11.268876: Validation loss did not improve from -0.50542. Patience: 26/50
2024-12-07 23:41:11.269857: train_loss -0.7005
2024-12-07 23:41:11.270777: val_loss -0.3347
2024-12-07 23:41:11.271566: Pseudo dice [0.6403]
2024-12-07 23:41:11.272342: Epoch time: 416.24 s
2024-12-07 23:41:12.673915: 
2024-12-07 23:41:12.675025: Epoch 101
2024-12-07 23:41:12.675763: Current learning rate: 0.00909
2024-12-07 23:47:59.568579: Validation loss did not improve from -0.50542. Patience: 27/50
2024-12-07 23:47:59.569585: train_loss -0.7042
2024-12-07 23:47:59.570625: val_loss -0.4016
2024-12-07 23:47:59.571574: Pseudo dice [0.6872]
2024-12-07 23:47:59.572492: Epoch time: 406.9 s
2024-12-07 23:48:00.931890: 
2024-12-07 23:48:00.933212: Epoch 102
2024-12-07 23:48:00.934120: Current learning rate: 0.00908
2024-12-07 23:55:00.514890: Validation loss did not improve from -0.50542. Patience: 28/50
2024-12-07 23:55:00.516010: train_loss -0.7035
2024-12-07 23:55:00.516836: val_loss -0.2791
2024-12-07 23:55:00.517688: Pseudo dice [0.6257]
2024-12-07 23:55:00.518431: Epoch time: 419.59 s
2024-12-07 23:55:01.905992: 
2024-12-07 23:55:01.907304: Epoch 103
2024-12-07 23:55:01.908083: Current learning rate: 0.00907
2024-12-08 00:01:42.757783: Validation loss did not improve from -0.50542. Patience: 29/50
2024-12-08 00:01:42.758872: train_loss -0.7048
2024-12-08 00:01:42.759797: val_loss -0.4166
2024-12-08 00:01:42.760608: Pseudo dice [0.679]
2024-12-08 00:01:42.761358: Epoch time: 400.85 s
2024-12-08 00:01:44.133796: 
2024-12-08 00:01:44.135046: Epoch 104
2024-12-08 00:01:44.135970: Current learning rate: 0.00906
2024-12-08 00:08:32.665566: Validation loss did not improve from -0.50542. Patience: 30/50
2024-12-08 00:08:32.702800: train_loss -0.7008
2024-12-08 00:08:32.704472: val_loss -0.4804
2024-12-08 00:08:32.705477: Pseudo dice [0.7176]
2024-12-08 00:08:32.706540: Epoch time: 408.57 s
2024-12-08 00:08:34.877536: 
2024-12-08 00:08:34.878853: Epoch 105
2024-12-08 00:08:34.879585: Current learning rate: 0.00905
2024-12-08 00:15:18.022667: Validation loss did not improve from -0.50542. Patience: 31/50
2024-12-08 00:15:18.023964: train_loss -0.699
2024-12-08 00:15:18.024931: val_loss -0.3476
2024-12-08 00:15:18.025634: Pseudo dice [0.661]
2024-12-08 00:15:18.026676: Epoch time: 403.15 s
2024-12-08 00:15:19.427989: 
2024-12-08 00:15:19.428819: Epoch 106
2024-12-08 00:15:19.429597: Current learning rate: 0.00904
2024-12-08 00:22:10.714751: Validation loss did not improve from -0.50542. Patience: 32/50
2024-12-08 00:22:10.715842: train_loss -0.7033
2024-12-08 00:22:10.716803: val_loss -0.3659
2024-12-08 00:22:10.717587: Pseudo dice [0.6603]
2024-12-08 00:22:10.718406: Epoch time: 411.29 s
2024-12-08 00:22:12.175485: 
2024-12-08 00:22:12.176876: Epoch 107
2024-12-08 00:22:12.177973: Current learning rate: 0.00903
2024-12-08 00:28:59.933667: Validation loss did not improve from -0.50542. Patience: 33/50
2024-12-08 00:28:59.934790: train_loss -0.7156
2024-12-08 00:28:59.935654: val_loss -0.3797
2024-12-08 00:28:59.936386: Pseudo dice [0.6712]
2024-12-08 00:28:59.937209: Epoch time: 407.76 s
2024-12-08 00:29:01.323988: 
2024-12-08 00:29:01.325387: Epoch 108
2024-12-08 00:29:01.326166: Current learning rate: 0.00902
2024-12-08 00:35:32.775861: Validation loss did not improve from -0.50542. Patience: 34/50
2024-12-08 00:35:32.776740: train_loss -0.7189
2024-12-08 00:35:32.777638: val_loss -0.4063
2024-12-08 00:35:32.778350: Pseudo dice [0.6991]
2024-12-08 00:35:32.779012: Epoch time: 391.45 s
2024-12-08 00:35:34.178288: 
2024-12-08 00:35:34.180032: Epoch 109
2024-12-08 00:35:34.180918: Current learning rate: 0.00901
2024-12-08 00:42:15.079073: Validation loss did not improve from -0.50542. Patience: 35/50
2024-12-08 00:42:15.079964: train_loss -0.7104
2024-12-08 00:42:15.080809: val_loss -0.3823
2024-12-08 00:42:15.081715: Pseudo dice [0.6648]
2024-12-08 00:42:15.082545: Epoch time: 400.9 s
2024-12-08 00:42:16.959453: 
2024-12-08 00:42:16.961161: Epoch 110
2024-12-08 00:42:16.962085: Current learning rate: 0.009
2024-12-08 00:48:56.473398: Validation loss did not improve from -0.50542. Patience: 36/50
2024-12-08 00:48:56.474181: train_loss -0.7
2024-12-08 00:48:56.474898: val_loss -0.3997
2024-12-08 00:48:56.475637: Pseudo dice [0.6853]
2024-12-08 00:48:56.476283: Epoch time: 399.52 s
2024-12-08 00:48:57.843811: 
2024-12-08 00:48:57.845001: Epoch 111
2024-12-08 00:48:57.845821: Current learning rate: 0.009
2024-12-08 00:55:14.968385: Validation loss did not improve from -0.50542. Patience: 37/50
2024-12-08 00:55:14.969412: train_loss -0.7017
2024-12-08 00:55:14.970238: val_loss -0.3855
2024-12-08 00:55:14.971007: Pseudo dice [0.6779]
2024-12-08 00:55:14.971873: Epoch time: 377.13 s
2024-12-08 00:55:16.343983: 
2024-12-08 00:55:16.345256: Epoch 112
2024-12-08 00:55:16.346038: Current learning rate: 0.00899
2024-12-08 01:01:33.005434: Validation loss did not improve from -0.50542. Patience: 38/50
2024-12-08 01:01:33.006346: train_loss -0.6953
2024-12-08 01:01:33.007122: val_loss -0.2967
2024-12-08 01:01:33.007779: Pseudo dice [0.6388]
2024-12-08 01:01:33.008508: Epoch time: 376.66 s
2024-12-08 01:01:34.385392: 
2024-12-08 01:01:34.386647: Epoch 113
2024-12-08 01:01:34.387501: Current learning rate: 0.00898
2024-12-08 01:08:03.637685: Validation loss did not improve from -0.50542. Patience: 39/50
2024-12-08 01:08:03.638866: train_loss -0.7031
2024-12-08 01:08:03.639808: val_loss -0.4167
2024-12-08 01:08:03.640802: Pseudo dice [0.6921]
2024-12-08 01:08:03.641597: Epoch time: 389.25 s
2024-12-08 01:08:05.030296: 
2024-12-08 01:08:05.031592: Epoch 114
2024-12-08 01:08:05.032420: Current learning rate: 0.00897
2024-12-08 01:14:42.309990: Validation loss did not improve from -0.50542. Patience: 40/50
2024-12-08 01:14:42.312061: train_loss -0.7141
2024-12-08 01:14:42.313164: val_loss -0.2859
2024-12-08 01:14:42.313971: Pseudo dice [0.6242]
2024-12-08 01:14:42.314791: Epoch time: 397.28 s
2024-12-08 01:14:44.541231: 
2024-12-08 01:14:44.542625: Epoch 115
2024-12-08 01:14:44.543535: Current learning rate: 0.00896
2024-12-08 01:21:14.682262: Validation loss did not improve from -0.50542. Patience: 41/50
2024-12-08 01:21:14.683532: train_loss -0.7104
2024-12-08 01:21:14.684500: val_loss -0.2522
2024-12-08 01:21:14.685405: Pseudo dice [0.6198]
2024-12-08 01:21:14.686294: Epoch time: 390.14 s
2024-12-08 01:21:16.100284: 
2024-12-08 01:21:16.101576: Epoch 116
2024-12-08 01:21:16.102467: Current learning rate: 0.00895
2024-12-08 01:27:55.653139: Validation loss did not improve from -0.50542. Patience: 42/50
2024-12-08 01:27:55.653874: train_loss -0.7141
2024-12-08 01:27:55.654615: val_loss -0.3886
2024-12-08 01:27:55.655357: Pseudo dice [0.672]
2024-12-08 01:27:55.656225: Epoch time: 399.55 s
2024-12-08 01:27:57.068139: 
2024-12-08 01:27:57.069438: Epoch 117
2024-12-08 01:27:57.070379: Current learning rate: 0.00894
2024-12-08 01:34:37.395845: Validation loss did not improve from -0.50542. Patience: 43/50
2024-12-08 01:34:37.396934: train_loss -0.7182
2024-12-08 01:34:37.397786: val_loss -0.3201
2024-12-08 01:34:37.398405: Pseudo dice [0.6302]
2024-12-08 01:34:37.399162: Epoch time: 400.33 s
2024-12-08 01:34:38.807530: 
2024-12-08 01:34:38.809032: Epoch 118
2024-12-08 01:34:38.810030: Current learning rate: 0.00893
2024-12-08 01:41:23.221005: Validation loss did not improve from -0.50542. Patience: 44/50
2024-12-08 01:41:23.221713: train_loss -0.7174
2024-12-08 01:41:23.222435: val_loss -0.4166
2024-12-08 01:41:23.223059: Pseudo dice [0.6733]
2024-12-08 01:41:23.223760: Epoch time: 404.42 s
2024-12-08 01:41:24.636545: 
2024-12-08 01:41:24.637711: Epoch 119
2024-12-08 01:41:24.638489: Current learning rate: 0.00892
2024-12-08 01:48:12.617096: Validation loss did not improve from -0.50542. Patience: 45/50
2024-12-08 01:48:12.618762: train_loss -0.7271
2024-12-08 01:48:12.619798: val_loss -0.4066
2024-12-08 01:48:12.620573: Pseudo dice [0.6795]
2024-12-08 01:48:12.621230: Epoch time: 407.98 s
2024-12-08 01:48:14.478991: 
2024-12-08 01:48:14.480397: Epoch 120
2024-12-08 01:48:14.481351: Current learning rate: 0.00891
2024-12-08 01:54:15.886289: Validation loss did not improve from -0.50542. Patience: 46/50
2024-12-08 01:54:15.887345: train_loss -0.7203
2024-12-08 01:54:15.888236: val_loss -0.3325
2024-12-08 01:54:15.889090: Pseudo dice [0.6369]
2024-12-08 01:54:15.889960: Epoch time: 361.41 s
2024-12-08 01:54:17.292029: 
2024-12-08 01:54:17.293351: Epoch 121
2024-12-08 01:54:17.294132: Current learning rate: 0.0089
2024-12-08 02:00:40.752285: Validation loss did not improve from -0.50542. Patience: 47/50
2024-12-08 02:00:40.753201: train_loss -0.7202
2024-12-08 02:00:40.753998: val_loss -0.3641
2024-12-08 02:00:40.754767: Pseudo dice [0.6629]
2024-12-08 02:00:40.755497: Epoch time: 383.46 s
2024-12-08 02:00:42.161974: 
2024-12-08 02:00:42.163556: Epoch 122
2024-12-08 02:00:42.164550: Current learning rate: 0.00889
2024-12-08 02:07:21.810005: Validation loss did not improve from -0.50542. Patience: 48/50
2024-12-08 02:07:21.811120: train_loss -0.7209
2024-12-08 02:07:21.812238: val_loss -0.3879
2024-12-08 02:07:21.812990: Pseudo dice [0.6753]
2024-12-08 02:07:21.813660: Epoch time: 399.65 s
2024-12-08 02:07:23.263004: 
2024-12-08 02:07:23.264349: Epoch 123
2024-12-08 02:07:23.265341: Current learning rate: 0.00889
2024-12-08 02:13:32.167240: Validation loss did not improve from -0.50542. Patience: 49/50
2024-12-08 02:13:32.168252: train_loss -0.7247
2024-12-08 02:13:32.169004: val_loss -0.3655
2024-12-08 02:13:32.169670: Pseudo dice [0.6571]
2024-12-08 02:13:32.170377: Epoch time: 368.91 s
2024-12-08 02:13:33.590998: 
2024-12-08 02:13:33.592068: Epoch 124
2024-12-08 02:13:33.592801: Current learning rate: 0.00888
2024-12-08 02:19:06.453653: Validation loss did not improve from -0.50542. Patience: 50/50
2024-12-08 02:19:06.457541: train_loss -0.7275
2024-12-08 02:19:06.459506: val_loss -0.331
2024-12-08 02:19:06.460505: Pseudo dice [0.6389]
2024-12-08 02:19:06.461515: Epoch time: 332.87 s
2024-12-08 02:19:08.377191: Patience reached. Stopping training.
2024-12-08 02:19:08.836408: Training done.
2024-12-08 02:19:09.039696: Using splits from existing split file: /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset307_Sohee_Calcium_OCT_CrossValidation/splits_final.json
2024-12-08 02:19:09.042160: The split file contains 5 splits.
2024-12-08 02:19:09.043097: Desired fold for training: 3
2024-12-08 02:19:09.043902: This split has 7 training and 1 validation cases.
2024-12-08 02:19:09.044866: predicting 701-013
2024-12-08 02:19:09.152305: 701-013, shape torch.Size([1, 375, 498, 498]), rank 0
2024-12-08 02:21:10.442305: Validation complete
2024-12-08 02:21:10.443048: Mean Validation Dice:  0.6495714746708007
2024-12-07 12:14:30.604953: unpacking done...
2024-12-07 12:14:30.782561: Unable to plot network architecture: nnUNet_compile is enabled!
2024-12-07 12:14:30.918279: 
2024-12-07 12:14:30.919819: Epoch 0
2024-12-07 12:14:30.921001: Current learning rate: 0.01
2024-12-07 12:20:23.923833: Validation loss improved from 1000.00000 to -0.22587! Patience: 0/50
2024-12-07 12:20:23.924950: train_loss -0.0584
2024-12-07 12:20:23.926116: val_loss -0.2259
2024-12-07 12:20:23.927090: Pseudo dice [0.5641]
2024-12-07 12:20:23.927980: Epoch time: 353.01 s
2024-12-07 12:20:23.928868: Yayy! New best EMA pseudo Dice: 0.5641
2024-12-07 12:20:25.584001: 
2024-12-07 12:20:25.585288: Epoch 1
2024-12-07 12:20:25.586109: Current learning rate: 0.00999
2024-12-07 12:26:46.070727: Validation loss improved from -0.22587 to -0.28054! Patience: 0/50
2024-12-07 12:26:46.071754: train_loss -0.195
2024-12-07 12:26:46.072649: val_loss -0.2805
2024-12-07 12:26:46.073356: Pseudo dice [0.5929]
2024-12-07 12:26:46.074117: Epoch time: 380.49 s
2024-12-07 12:26:46.074787: Yayy! New best EMA pseudo Dice: 0.567
2024-12-07 12:26:47.872681: 
2024-12-07 12:26:47.874128: Epoch 2
2024-12-07 12:26:47.874899: Current learning rate: 0.00998
2024-12-07 12:33:27.084038: Validation loss improved from -0.28054 to -0.29727! Patience: 0/50
2024-12-07 12:33:27.085070: train_loss -0.2457
2024-12-07 12:33:27.085971: val_loss -0.2973
2024-12-07 12:33:27.086628: Pseudo dice [0.5883]
2024-12-07 12:33:27.087405: Epoch time: 399.21 s
2024-12-07 12:33:27.088125: Yayy! New best EMA pseudo Dice: 0.5691
2024-12-07 12:33:28.914183: 
2024-12-07 12:33:28.915325: Epoch 3
2024-12-07 12:33:28.916006: Current learning rate: 0.00997
2024-12-07 12:40:05.003053: Validation loss improved from -0.29727 to -0.32341! Patience: 0/50
2024-12-07 12:40:05.004140: train_loss -0.2855
2024-12-07 12:40:05.004902: val_loss -0.3234
2024-12-07 12:40:05.005564: Pseudo dice [0.6157]
2024-12-07 12:40:05.006287: Epoch time: 396.09 s
2024-12-07 12:40:05.007045: Yayy! New best EMA pseudo Dice: 0.5738
2024-12-07 12:40:06.907149: 
2024-12-07 12:40:06.908289: Epoch 4
2024-12-07 12:40:06.909168: Current learning rate: 0.00996
2024-12-07 12:46:43.323825: Validation loss did not improve from -0.32341. Patience: 1/50
2024-12-07 12:46:43.324863: train_loss -0.3143
2024-12-07 12:46:43.325714: val_loss -0.3058
2024-12-07 12:46:43.326510: Pseudo dice [0.593]
2024-12-07 12:46:43.327262: Epoch time: 396.42 s
2024-12-07 12:46:43.724969: Yayy! New best EMA pseudo Dice: 0.5757
2024-12-07 12:46:45.682071: 
2024-12-07 12:46:45.683278: Epoch 5
2024-12-07 12:46:45.684048: Current learning rate: 0.00995
2024-12-07 12:53:32.930770: Validation loss did not improve from -0.32341. Patience: 2/50
2024-12-07 12:53:32.931794: train_loss -0.3374
2024-12-07 12:53:32.932854: val_loss -0.2535
2024-12-07 12:53:32.933877: Pseudo dice [0.533]
2024-12-07 12:53:32.934819: Epoch time: 407.25 s
2024-12-07 12:53:34.264192: 
2024-12-07 12:53:34.265266: Epoch 6
2024-12-07 12:53:34.266051: Current learning rate: 0.00995
2024-12-07 13:00:39.270502: Validation loss did not improve from -0.32341. Patience: 3/50
2024-12-07 13:00:39.271533: train_loss -0.345
2024-12-07 13:00:39.272514: val_loss -0.2934
2024-12-07 13:00:39.273294: Pseudo dice [0.6013]
2024-12-07 13:00:39.274084: Epoch time: 425.01 s
2024-12-07 13:00:40.707055: 
2024-12-07 13:00:40.708661: Epoch 7
2024-12-07 13:00:40.709677: Current learning rate: 0.00994
2024-12-07 13:07:43.642496: Validation loss improved from -0.32341 to -0.36811! Patience: 3/50
2024-12-07 13:07:43.643633: train_loss -0.3771
2024-12-07 13:07:43.644561: val_loss -0.3681
2024-12-07 13:07:43.645598: Pseudo dice [0.6388]
2024-12-07 13:07:43.646551: Epoch time: 422.94 s
2024-12-07 13:07:43.647506: Yayy! New best EMA pseudo Dice: 0.5809
2024-12-07 13:07:45.908247: 
2024-12-07 13:07:45.909719: Epoch 8
2024-12-07 13:07:45.910739: Current learning rate: 0.00993
2024-12-07 13:14:48.402608: Validation loss did not improve from -0.36811. Patience: 1/50
2024-12-07 13:14:48.403722: train_loss -0.4136
2024-12-07 13:14:48.405334: val_loss -0.345
2024-12-07 13:14:48.406139: Pseudo dice [0.6384]
2024-12-07 13:14:48.407103: Epoch time: 422.5 s
2024-12-07 13:14:48.407862: Yayy! New best EMA pseudo Dice: 0.5866
2024-12-07 13:14:50.255790: 
2024-12-07 13:14:50.257280: Epoch 9
2024-12-07 13:14:50.258351: Current learning rate: 0.00992
2024-12-07 13:21:52.408650: Validation loss improved from -0.36811 to -0.37300! Patience: 1/50
2024-12-07 13:21:52.409933: train_loss -0.43
2024-12-07 13:21:52.410743: val_loss -0.373
2024-12-07 13:21:52.411585: Pseudo dice [0.638]
2024-12-07 13:21:52.412387: Epoch time: 422.16 s
2024-12-07 13:21:52.817717: Yayy! New best EMA pseudo Dice: 0.5918
2024-12-07 13:21:54.613683: 
2024-12-07 13:21:54.614944: Epoch 10
2024-12-07 13:21:54.615656: Current learning rate: 0.00991
2024-12-07 13:28:59.963969: Validation loss improved from -0.37300 to -0.39609! Patience: 0/50
2024-12-07 13:28:59.964923: train_loss -0.4442
2024-12-07 13:28:59.965759: val_loss -0.3961
2024-12-07 13:28:59.966552: Pseudo dice [0.6518]
2024-12-07 13:28:59.967287: Epoch time: 425.35 s
2024-12-07 13:28:59.967972: Yayy! New best EMA pseudo Dice: 0.5978
2024-12-07 13:29:01.797935: 
2024-12-07 13:29:01.799424: Epoch 11
2024-12-07 13:29:01.800224: Current learning rate: 0.0099
2024-12-07 13:35:47.316854: Validation loss improved from -0.39609 to -0.41010! Patience: 0/50
2024-12-07 13:35:47.317976: train_loss -0.4521
2024-12-07 13:35:47.318827: val_loss -0.4101
2024-12-07 13:35:47.319598: Pseudo dice [0.6685]
2024-12-07 13:35:47.320407: Epoch time: 405.52 s
2024-12-07 13:35:47.321160: Yayy! New best EMA pseudo Dice: 0.6048
2024-12-07 13:35:49.141527: 
2024-12-07 13:35:49.142856: Epoch 12
2024-12-07 13:35:49.143648: Current learning rate: 0.00989
2024-12-07 13:42:46.602661: Validation loss improved from -0.41010 to -0.41160! Patience: 0/50
2024-12-07 13:42:46.603771: train_loss -0.4638
2024-12-07 13:42:46.604488: val_loss -0.4116
2024-12-07 13:42:46.605173: Pseudo dice [0.6616]
2024-12-07 13:42:46.605797: Epoch time: 417.46 s
2024-12-07 13:42:46.606507: Yayy! New best EMA pseudo Dice: 0.6105
2024-12-07 13:42:48.444881: 
2024-12-07 13:42:48.446098: Epoch 13
2024-12-07 13:42:48.446765: Current learning rate: 0.00988
2024-12-07 13:49:42.430984: Validation loss did not improve from -0.41160. Patience: 1/50
2024-12-07 13:49:42.432091: train_loss -0.4673
2024-12-07 13:49:42.432811: val_loss -0.3459
2024-12-07 13:49:42.433448: Pseudo dice [0.6406]
2024-12-07 13:49:42.434216: Epoch time: 413.99 s
2024-12-07 13:49:42.434943: Yayy! New best EMA pseudo Dice: 0.6135
2024-12-07 13:49:44.245989: 
2024-12-07 13:49:44.247180: Epoch 14
2024-12-07 13:49:44.247900: Current learning rate: 0.00987
2024-12-07 13:56:23.938894: Validation loss did not improve from -0.41160. Patience: 2/50
2024-12-07 13:56:23.939660: train_loss -0.4766
2024-12-07 13:56:23.940415: val_loss -0.3784
2024-12-07 13:56:23.941088: Pseudo dice [0.6636]
2024-12-07 13:56:23.941813: Epoch time: 399.7 s
2024-12-07 13:56:24.367618: Yayy! New best EMA pseudo Dice: 0.6185
2024-12-07 13:56:26.285746: 
2024-12-07 13:56:26.286980: Epoch 15
2024-12-07 13:56:26.287659: Current learning rate: 0.00986
2024-12-07 14:03:25.147619: Validation loss improved from -0.41160 to -0.43313! Patience: 2/50
2024-12-07 14:03:25.148575: train_loss -0.4806
2024-12-07 14:03:25.149384: val_loss -0.4331
2024-12-07 14:03:25.150215: Pseudo dice [0.683]
2024-12-07 14:03:25.150944: Epoch time: 418.86 s
2024-12-07 14:03:25.151608: Yayy! New best EMA pseudo Dice: 0.625
2024-12-07 14:03:26.992457: 
2024-12-07 14:03:26.993846: Epoch 16
2024-12-07 14:03:26.994793: Current learning rate: 0.00986
2024-12-07 14:10:31.129369: Validation loss did not improve from -0.43313. Patience: 1/50
2024-12-07 14:10:31.130440: train_loss -0.4848
2024-12-07 14:10:31.131294: val_loss -0.4156
2024-12-07 14:10:31.132106: Pseudo dice [0.6756]
2024-12-07 14:10:31.132941: Epoch time: 424.14 s
2024-12-07 14:10:31.133706: Yayy! New best EMA pseudo Dice: 0.63
2024-12-07 14:10:32.999579: 
2024-12-07 14:10:33.000990: Epoch 17
2024-12-07 14:10:33.001731: Current learning rate: 0.00985
2024-12-07 14:17:32.482854: Validation loss improved from -0.43313 to -0.43796! Patience: 1/50
2024-12-07 14:17:32.484065: train_loss -0.5032
2024-12-07 14:17:32.484886: val_loss -0.438
2024-12-07 14:17:32.485628: Pseudo dice [0.6946]
2024-12-07 14:17:32.486601: Epoch time: 419.49 s
2024-12-07 14:17:32.487352: Yayy! New best EMA pseudo Dice: 0.6365
2024-12-07 14:17:34.311309: 
2024-12-07 14:17:34.312549: Epoch 18
2024-12-07 14:17:34.313235: Current learning rate: 0.00984
2024-12-07 14:24:31.769163: Validation loss improved from -0.43796 to -0.44426! Patience: 0/50
2024-12-07 14:24:31.773734: train_loss -0.5169
2024-12-07 14:24:31.775410: val_loss -0.4443
2024-12-07 14:24:31.776358: Pseudo dice [0.6885]
2024-12-07 14:24:31.777308: Epoch time: 417.46 s
2024-12-07 14:24:31.778249: Yayy! New best EMA pseudo Dice: 0.6417
2024-12-07 14:24:34.254090: 
2024-12-07 14:24:34.255234: Epoch 19
2024-12-07 14:24:34.255943: Current learning rate: 0.00983
2024-12-07 14:31:33.444806: Validation loss improved from -0.44426 to -0.46539! Patience: 0/50
2024-12-07 14:31:33.445984: train_loss -0.5136
2024-12-07 14:31:33.447027: val_loss -0.4654
2024-12-07 14:31:33.447832: Pseudo dice [0.6926]
2024-12-07 14:31:33.448628: Epoch time: 419.19 s
2024-12-07 14:31:33.839441: Yayy! New best EMA pseudo Dice: 0.6468
2024-12-07 14:31:35.768029: 
2024-12-07 14:31:35.769649: Epoch 20
2024-12-07 14:31:35.771123: Current learning rate: 0.00982
2024-12-07 14:38:32.142756: Validation loss did not improve from -0.46539. Patience: 1/50
2024-12-07 14:38:32.143552: train_loss -0.5288
2024-12-07 14:38:32.144476: val_loss -0.4602
2024-12-07 14:38:32.145160: Pseudo dice [0.6959]
2024-12-07 14:38:32.145901: Epoch time: 416.38 s
2024-12-07 14:38:32.146571: Yayy! New best EMA pseudo Dice: 0.6517
2024-12-07 14:38:34.068139: 
2024-12-07 14:38:34.069504: Epoch 21
2024-12-07 14:38:34.070396: Current learning rate: 0.00981
2024-12-07 14:45:34.256791: Validation loss did not improve from -0.46539. Patience: 2/50
2024-12-07 14:45:34.257837: train_loss -0.5311
2024-12-07 14:45:34.258702: val_loss -0.4428
2024-12-07 14:45:34.259529: Pseudo dice [0.6933]
2024-12-07 14:45:34.260290: Epoch time: 420.19 s
2024-12-07 14:45:34.260983: Yayy! New best EMA pseudo Dice: 0.6559
2024-12-07 14:45:36.002187: 
2024-12-07 14:45:36.003620: Epoch 22
2024-12-07 14:45:36.004519: Current learning rate: 0.0098
2024-12-07 14:52:36.631396: Validation loss did not improve from -0.46539. Patience: 3/50
2024-12-07 14:52:36.632493: train_loss -0.5263
2024-12-07 14:52:36.633502: val_loss -0.4408
2024-12-07 14:52:36.634369: Pseudo dice [0.6796]
2024-12-07 14:52:36.635182: Epoch time: 420.63 s
2024-12-07 14:52:36.635884: Yayy! New best EMA pseudo Dice: 0.6582
2024-12-07 14:52:38.390399: 
2024-12-07 14:52:38.391690: Epoch 23
2024-12-07 14:52:38.392462: Current learning rate: 0.00979
2024-12-07 14:59:45.482774: Validation loss did not improve from -0.46539. Patience: 4/50
2024-12-07 14:59:45.483791: train_loss -0.5267
2024-12-07 14:59:45.484829: val_loss -0.4228
2024-12-07 14:59:45.485680: Pseudo dice [0.685]
2024-12-07 14:59:45.486398: Epoch time: 427.09 s
2024-12-07 14:59:45.487170: Yayy! New best EMA pseudo Dice: 0.6609
2024-12-07 14:59:47.353186: 
2024-12-07 14:59:47.354558: Epoch 24
2024-12-07 14:59:47.355322: Current learning rate: 0.00978
2024-12-07 15:06:38.699900: Validation loss did not improve from -0.46539. Patience: 5/50
2024-12-07 15:06:38.700879: train_loss -0.5355
2024-12-07 15:06:38.701757: val_loss -0.4091
2024-12-07 15:06:38.702491: Pseudo dice [0.6781]
2024-12-07 15:06:38.703301: Epoch time: 411.35 s
2024-12-07 15:06:39.168761: Yayy! New best EMA pseudo Dice: 0.6626
2024-12-07 15:06:40.938277: 
2024-12-07 15:06:40.939680: Epoch 25
2024-12-07 15:06:40.940514: Current learning rate: 0.00977
2024-12-07 15:13:37.863359: Validation loss did not improve from -0.46539. Patience: 6/50
2024-12-07 15:13:37.864247: train_loss -0.5419
2024-12-07 15:13:37.865057: val_loss -0.4473
2024-12-07 15:13:37.865844: Pseudo dice [0.6843]
2024-12-07 15:13:37.866525: Epoch time: 416.93 s
2024-12-07 15:13:37.867273: Yayy! New best EMA pseudo Dice: 0.6648
2024-12-07 15:13:39.618114: 
2024-12-07 15:13:39.619710: Epoch 26
2024-12-07 15:13:39.620656: Current learning rate: 0.00977
2024-12-07 15:20:38.661903: Validation loss improved from -0.46539 to -0.46707! Patience: 6/50
2024-12-07 15:20:38.662800: train_loss -0.5461
2024-12-07 15:20:38.663545: val_loss -0.4671
2024-12-07 15:20:38.664226: Pseudo dice [0.7011]
2024-12-07 15:20:38.664872: Epoch time: 419.05 s
2024-12-07 15:20:38.665566: Yayy! New best EMA pseudo Dice: 0.6684
2024-12-07 15:20:40.456666: 
2024-12-07 15:20:40.458074: Epoch 27
2024-12-07 15:20:40.458893: Current learning rate: 0.00976
2024-12-07 15:27:35.938601: Validation loss did not improve from -0.46707. Patience: 1/50
2024-12-07 15:27:35.939691: train_loss -0.5404
2024-12-07 15:27:35.940520: val_loss -0.4312
2024-12-07 15:27:35.941263: Pseudo dice [0.6733]
2024-12-07 15:27:35.942041: Epoch time: 415.48 s
2024-12-07 15:27:35.942860: Yayy! New best EMA pseudo Dice: 0.6689
2024-12-07 15:27:37.746687: 
2024-12-07 15:27:37.748207: Epoch 28
2024-12-07 15:27:37.748930: Current learning rate: 0.00975
2024-12-07 15:34:33.560606: Validation loss improved from -0.46707 to -0.48412! Patience: 1/50
2024-12-07 15:34:33.575062: train_loss -0.5713
2024-12-07 15:34:33.575984: val_loss -0.4841
2024-12-07 15:34:33.576712: Pseudo dice [0.7086]
2024-12-07 15:34:33.577587: Epoch time: 415.83 s
2024-12-07 15:34:33.578281: Yayy! New best EMA pseudo Dice: 0.6729
2024-12-07 15:34:35.993104: 
2024-12-07 15:34:35.994494: Epoch 29
2024-12-07 15:34:35.995219: Current learning rate: 0.00974
2024-12-07 15:41:28.227121: Validation loss did not improve from -0.48412. Patience: 1/50
2024-12-07 15:41:28.228228: train_loss -0.5676
2024-12-07 15:41:28.229144: val_loss -0.4567
2024-12-07 15:41:28.229787: Pseudo dice [0.695]
2024-12-07 15:41:28.230563: Epoch time: 412.24 s
2024-12-07 15:41:28.697155: Yayy! New best EMA pseudo Dice: 0.6751
2024-12-07 15:41:30.524610: 
2024-12-07 15:41:30.525935: Epoch 30
2024-12-07 15:41:30.526871: Current learning rate: 0.00973
2024-12-07 15:48:22.348147: Validation loss did not improve from -0.48412. Patience: 2/50
2024-12-07 15:48:22.349227: train_loss -0.5655
2024-12-07 15:48:22.350058: val_loss -0.4282
2024-12-07 15:48:22.350766: Pseudo dice [0.6885]
2024-12-07 15:48:22.351420: Epoch time: 411.83 s
2024-12-07 15:48:22.352061: Yayy! New best EMA pseudo Dice: 0.6764
2024-12-07 15:48:24.245013: 
2024-12-07 15:48:24.246316: Epoch 31
2024-12-07 15:48:24.247101: Current learning rate: 0.00972
2024-12-07 15:55:22.281983: Validation loss did not improve from -0.48412. Patience: 3/50
2024-12-07 15:55:22.283071: train_loss -0.5737
2024-12-07 15:55:22.283978: val_loss -0.4667
2024-12-07 15:55:22.284858: Pseudo dice [0.703]
2024-12-07 15:55:22.285777: Epoch time: 418.04 s
2024-12-07 15:55:22.286729: Yayy! New best EMA pseudo Dice: 0.6791
2024-12-07 15:55:24.141695: 
2024-12-07 15:55:24.143127: Epoch 32
2024-12-07 15:55:24.144245: Current learning rate: 0.00971
2024-12-07 16:02:11.281724: Validation loss improved from -0.48412 to -0.48658! Patience: 3/50
2024-12-07 16:02:11.282978: train_loss -0.5801
2024-12-07 16:02:11.283950: val_loss -0.4866
2024-12-07 16:02:11.284845: Pseudo dice [0.7097]
2024-12-07 16:02:11.285706: Epoch time: 407.14 s
2024-12-07 16:02:11.286544: Yayy! New best EMA pseudo Dice: 0.6822
2024-12-07 16:02:13.123574: 
2024-12-07 16:02:13.125052: Epoch 33
2024-12-07 16:02:13.125794: Current learning rate: 0.0097
2024-12-07 16:08:58.566532: Validation loss did not improve from -0.48658. Patience: 1/50
2024-12-07 16:08:58.567516: train_loss -0.5829
2024-12-07 16:08:58.568531: val_loss -0.4766
2024-12-07 16:08:58.569316: Pseudo dice [0.707]
2024-12-07 16:08:58.570122: Epoch time: 405.45 s
2024-12-07 16:08:58.570879: Yayy! New best EMA pseudo Dice: 0.6846
2024-12-07 16:09:00.427123: 
2024-12-07 16:09:00.428447: Epoch 34
2024-12-07 16:09:00.429348: Current learning rate: 0.00969
2024-12-07 16:15:57.899711: Validation loss improved from -0.48658 to -0.50122! Patience: 1/50
2024-12-07 16:15:57.900425: train_loss -0.5787
2024-12-07 16:15:57.901130: val_loss -0.5012
2024-12-07 16:15:57.902168: Pseudo dice [0.722]
2024-12-07 16:15:57.902979: Epoch time: 417.47 s
2024-12-07 16:15:58.380064: Yayy! New best EMA pseudo Dice: 0.6884
2024-12-07 16:16:00.219994: 
2024-12-07 16:16:00.221722: Epoch 35
2024-12-07 16:16:00.223145: Current learning rate: 0.00968
2024-12-07 16:23:02.634441: Validation loss did not improve from -0.50122. Patience: 1/50
2024-12-07 16:23:02.635440: train_loss -0.5895
2024-12-07 16:23:02.636233: val_loss -0.4725
2024-12-07 16:23:02.636986: Pseudo dice [0.7028]
2024-12-07 16:23:02.637794: Epoch time: 422.42 s
2024-12-07 16:23:02.638522: Yayy! New best EMA pseudo Dice: 0.6898
2024-12-07 16:23:04.510354: 
2024-12-07 16:23:04.511677: Epoch 36
2024-12-07 16:23:04.512442: Current learning rate: 0.00968
2024-12-07 16:29:44.928474: Validation loss did not improve from -0.50122. Patience: 2/50
2024-12-07 16:29:44.929633: train_loss -0.586
2024-12-07 16:29:44.930468: val_loss -0.4239
2024-12-07 16:29:44.931219: Pseudo dice [0.6721]
2024-12-07 16:29:44.931963: Epoch time: 400.42 s
2024-12-07 16:29:46.382368: 
2024-12-07 16:29:46.383748: Epoch 37
2024-12-07 16:29:46.384898: Current learning rate: 0.00967
2024-12-07 16:36:44.364266: Validation loss did not improve from -0.50122. Patience: 3/50
2024-12-07 16:36:44.365522: train_loss -0.585
2024-12-07 16:36:44.366550: val_loss -0.4932
2024-12-07 16:36:44.367347: Pseudo dice [0.7235]
2024-12-07 16:36:44.368216: Epoch time: 417.98 s
2024-12-07 16:36:44.368986: Yayy! New best EMA pseudo Dice: 0.6916
2024-12-07 16:36:46.334699: 
2024-12-07 16:36:46.335989: Epoch 38
2024-12-07 16:36:46.336690: Current learning rate: 0.00966
2024-12-07 16:43:33.056394: Validation loss did not improve from -0.50122. Patience: 4/50
2024-12-07 16:43:33.057858: train_loss -0.5958
2024-12-07 16:43:33.058828: val_loss -0.4281
2024-12-07 16:43:33.059514: Pseudo dice [0.6935]
2024-12-07 16:43:33.060268: Epoch time: 406.72 s
2024-12-07 16:43:33.060903: Yayy! New best EMA pseudo Dice: 0.6918
2024-12-07 16:43:35.378718: 
2024-12-07 16:43:35.379914: Epoch 39
2024-12-07 16:43:35.380634: Current learning rate: 0.00965
2024-12-07 16:50:25.276425: Validation loss did not improve from -0.50122. Patience: 5/50
2024-12-07 16:50:25.277507: train_loss -0.5996
2024-12-07 16:50:25.278422: val_loss -0.4698
2024-12-07 16:50:25.279125: Pseudo dice [0.6978]
2024-12-07 16:50:25.279846: Epoch time: 409.9 s
2024-12-07 16:50:25.630818: Yayy! New best EMA pseudo Dice: 0.6924
2024-12-07 16:50:27.646670: 
2024-12-07 16:50:27.647979: Epoch 40
2024-12-07 16:50:27.648811: Current learning rate: 0.00964
2024-12-07 16:57:10.964555: Validation loss improved from -0.50122 to -0.50323! Patience: 5/50
2024-12-07 16:57:10.965464: train_loss -0.6019
2024-12-07 16:57:10.966365: val_loss -0.5032
2024-12-07 16:57:10.967187: Pseudo dice [0.7148]
2024-12-07 16:57:10.967905: Epoch time: 403.32 s
2024-12-07 16:57:10.968538: Yayy! New best EMA pseudo Dice: 0.6946
2024-12-07 16:57:12.903764: 
2024-12-07 16:57:12.905189: Epoch 41
2024-12-07 16:57:12.905932: Current learning rate: 0.00963
2024-12-07 17:04:07.345393: Validation loss did not improve from -0.50323. Patience: 1/50
2024-12-07 17:04:07.346369: train_loss -0.6019
2024-12-07 17:04:07.347317: val_loss -0.4551
2024-12-07 17:04:07.348269: Pseudo dice [0.702]
2024-12-07 17:04:07.349248: Epoch time: 414.44 s
2024-12-07 17:04:07.350227: Yayy! New best EMA pseudo Dice: 0.6954
2024-12-07 17:04:09.133222: 
2024-12-07 17:04:09.134479: Epoch 42
2024-12-07 17:04:09.135400: Current learning rate: 0.00962
2024-12-07 17:11:09.099632: Validation loss did not improve from -0.50323. Patience: 2/50
2024-12-07 17:11:09.100624: train_loss -0.6091
2024-12-07 17:11:09.101522: val_loss -0.4941
2024-12-07 17:11:09.102152: Pseudo dice [0.7177]
2024-12-07 17:11:09.102839: Epoch time: 419.97 s
2024-12-07 17:11:09.103448: Yayy! New best EMA pseudo Dice: 0.6976
2024-12-07 17:11:10.895410: 
2024-12-07 17:11:10.896516: Epoch 43
2024-12-07 17:11:10.897274: Current learning rate: 0.00961
2024-12-07 17:18:07.421833: Validation loss improved from -0.50323 to -0.51236! Patience: 2/50
2024-12-07 17:18:07.422843: train_loss -0.618
2024-12-07 17:18:07.423689: val_loss -0.5124
2024-12-07 17:18:07.424355: Pseudo dice [0.7225]
2024-12-07 17:18:07.425084: Epoch time: 416.53 s
2024-12-07 17:18:07.425853: Yayy! New best EMA pseudo Dice: 0.7001
2024-12-07 17:18:09.221137: 
2024-12-07 17:18:09.222502: Epoch 44
2024-12-07 17:18:09.223465: Current learning rate: 0.0096
2024-12-07 17:24:59.143102: Validation loss did not improve from -0.51236. Patience: 1/50
2024-12-07 17:24:59.144052: train_loss -0.5972
2024-12-07 17:24:59.144957: val_loss -0.4912
2024-12-07 17:24:59.145715: Pseudo dice [0.7165]
2024-12-07 17:24:59.146556: Epoch time: 409.92 s
2024-12-07 17:24:59.551504: Yayy! New best EMA pseudo Dice: 0.7017
2024-12-07 17:25:01.397051: 
2024-12-07 17:25:01.398032: Epoch 45
2024-12-07 17:25:01.398717: Current learning rate: 0.00959
2024-12-07 17:31:47.370215: Validation loss did not improve from -0.51236. Patience: 2/50
2024-12-07 17:31:47.371014: train_loss -0.608
2024-12-07 17:31:47.372053: val_loss -0.4543
2024-12-07 17:31:47.372828: Pseudo dice [0.7005]
2024-12-07 17:31:47.373807: Epoch time: 405.98 s
2024-12-07 17:31:48.757666: 
2024-12-07 17:31:48.759295: Epoch 46
2024-12-07 17:31:48.760377: Current learning rate: 0.00959
2024-12-07 17:38:34.026835: Validation loss did not improve from -0.51236. Patience: 3/50
2024-12-07 17:38:34.028574: train_loss -0.6103
2024-12-07 17:38:34.029579: val_loss -0.4555
2024-12-07 17:38:34.030242: Pseudo dice [0.6999]
2024-12-07 17:38:34.030995: Epoch time: 405.27 s
2024-12-07 17:38:35.450699: 
2024-12-07 17:38:35.452049: Epoch 47
2024-12-07 17:38:35.453112: Current learning rate: 0.00958
2024-12-07 17:45:27.877184: Validation loss did not improve from -0.51236. Patience: 4/50
2024-12-07 17:45:27.879048: train_loss -0.6103
2024-12-07 17:45:27.879955: val_loss -0.4186
2024-12-07 17:45:27.880607: Pseudo dice [0.678]
2024-12-07 17:45:27.881332: Epoch time: 412.43 s
2024-12-07 17:45:29.255485: 
2024-12-07 17:45:29.256803: Epoch 48
2024-12-07 17:45:29.257489: Current learning rate: 0.00957
2024-12-07 17:52:20.968846: Validation loss did not improve from -0.51236. Patience: 5/50
2024-12-07 17:52:20.969895: train_loss -0.6042
2024-12-07 17:52:20.970877: val_loss -0.4432
2024-12-07 17:52:20.971822: Pseudo dice [0.6946]
2024-12-07 17:52:20.972728: Epoch time: 411.72 s
2024-12-07 17:52:22.772019: 
2024-12-07 17:52:22.773330: Epoch 49
2024-12-07 17:52:22.774010: Current learning rate: 0.00956
2024-12-07 17:59:06.924019: Validation loss did not improve from -0.51236. Patience: 6/50
2024-12-07 17:59:06.925243: train_loss -0.6232
2024-12-07 17:59:06.926401: val_loss -0.4922
2024-12-07 17:59:06.927204: Pseudo dice [0.7135]
2024-12-07 17:59:06.928005: Epoch time: 404.15 s
2024-12-07 17:59:08.715497: 
2024-12-07 17:59:08.716974: Epoch 50
2024-12-07 17:59:08.717755: Current learning rate: 0.00955
2024-12-07 18:05:58.875989: Validation loss did not improve from -0.51236. Patience: 7/50
2024-12-07 18:05:58.877010: train_loss -0.6243
2024-12-07 18:05:58.877944: val_loss -0.4936
2024-12-07 18:05:58.878871: Pseudo dice [0.7164]
2024-12-07 18:05:58.879658: Epoch time: 410.16 s
2024-12-07 18:05:58.880419: Yayy! New best EMA pseudo Dice: 0.7018
2024-12-07 18:06:00.629597: 
2024-12-07 18:06:00.630828: Epoch 51
2024-12-07 18:06:00.631654: Current learning rate: 0.00954
2024-12-07 18:12:58.188426: Validation loss did not improve from -0.51236. Patience: 8/50
2024-12-07 18:12:58.189414: train_loss -0.6294
2024-12-07 18:12:58.190298: val_loss -0.4909
2024-12-07 18:12:58.190966: Pseudo dice [0.7111]
2024-12-07 18:12:58.191702: Epoch time: 417.56 s
2024-12-07 18:12:58.192510: Yayy! New best EMA pseudo Dice: 0.7027
2024-12-07 18:12:59.991696: 
2024-12-07 18:12:59.992970: Epoch 52
2024-12-07 18:12:59.993758: Current learning rate: 0.00953
2024-12-07 18:19:46.246379: Validation loss did not improve from -0.51236. Patience: 9/50
2024-12-07 18:19:46.247170: train_loss -0.633
2024-12-07 18:19:46.248144: val_loss -0.4678
2024-12-07 18:19:46.249078: Pseudo dice [0.7041]
2024-12-07 18:19:46.249959: Epoch time: 406.26 s
2024-12-07 18:19:46.251135: Yayy! New best EMA pseudo Dice: 0.7028
2024-12-07 18:19:48.077260: 
2024-12-07 18:19:48.078559: Epoch 53
2024-12-07 18:19:48.079639: Current learning rate: 0.00952
2024-12-07 18:26:37.160599: Validation loss did not improve from -0.51236. Patience: 10/50
2024-12-07 18:26:37.161404: train_loss -0.6398
2024-12-07 18:26:37.162363: val_loss -0.4955
2024-12-07 18:26:37.163161: Pseudo dice [0.7082]
2024-12-07 18:26:37.163998: Epoch time: 409.09 s
2024-12-07 18:26:37.164770: Yayy! New best EMA pseudo Dice: 0.7034
2024-12-07 18:26:39.027881: 
2024-12-07 18:26:39.029136: Epoch 54
2024-12-07 18:26:39.029902: Current learning rate: 0.00951
2024-12-07 18:33:31.781783: Validation loss did not improve from -0.51236. Patience: 11/50
2024-12-07 18:33:31.782764: train_loss -0.6304
2024-12-07 18:33:31.783562: val_loss -0.4961
2024-12-07 18:33:31.784199: Pseudo dice [0.7153]
2024-12-07 18:33:31.784952: Epoch time: 412.76 s
2024-12-07 18:33:32.256593: Yayy! New best EMA pseudo Dice: 0.7046
2024-12-07 18:33:34.139313: 
2024-12-07 18:33:34.140903: Epoch 55
2024-12-07 18:33:34.142138: Current learning rate: 0.0095
2024-12-07 18:40:29.050373: Validation loss did not improve from -0.51236. Patience: 12/50
2024-12-07 18:40:29.051335: train_loss -0.6289
2024-12-07 18:40:29.052034: val_loss -0.4875
2024-12-07 18:40:29.052827: Pseudo dice [0.7188]
2024-12-07 18:40:29.053493: Epoch time: 414.91 s
2024-12-07 18:40:29.054190: Yayy! New best EMA pseudo Dice: 0.706
2024-12-07 18:40:30.819098: 
2024-12-07 18:40:30.820302: Epoch 56
2024-12-07 18:40:30.821156: Current learning rate: 0.00949
2024-12-07 18:47:27.959507: Validation loss did not improve from -0.51236. Patience: 13/50
2024-12-07 18:47:27.960661: train_loss -0.6387
2024-12-07 18:47:27.961459: val_loss -0.5002
2024-12-07 18:47:27.962216: Pseudo dice [0.7236]
2024-12-07 18:47:27.962859: Epoch time: 417.14 s
2024-12-07 18:47:27.963547: Yayy! New best EMA pseudo Dice: 0.7077
2024-12-07 18:47:29.793591: 
2024-12-07 18:47:29.795285: Epoch 57
2024-12-07 18:47:29.796417: Current learning rate: 0.00949
2024-12-07 18:54:19.643492: Validation loss did not improve from -0.51236. Patience: 14/50
2024-12-07 18:54:19.644500: train_loss -0.6365
2024-12-07 18:54:19.645552: val_loss -0.4383
2024-12-07 18:54:19.646254: Pseudo dice [0.6906]
2024-12-07 18:54:19.647012: Epoch time: 409.85 s
2024-12-07 18:54:21.060526: 
2024-12-07 18:54:21.061994: Epoch 58
2024-12-07 18:54:21.062863: Current learning rate: 0.00948
2024-12-07 19:01:26.754685: Validation loss did not improve from -0.51236. Patience: 15/50
2024-12-07 19:01:26.756972: train_loss -0.6318
2024-12-07 19:01:26.757959: val_loss -0.4523
2024-12-07 19:01:26.758832: Pseudo dice [0.6903]
2024-12-07 19:01:26.759529: Epoch time: 425.7 s
2024-12-07 19:01:28.191150: 
2024-12-07 19:01:28.192360: Epoch 59
2024-12-07 19:01:28.193553: Current learning rate: 0.00947
2024-12-07 19:08:15.030685: Validation loss did not improve from -0.51236. Patience: 16/50
2024-12-07 19:08:15.031677: train_loss -0.6379
2024-12-07 19:08:15.032511: val_loss -0.4866
2024-12-07 19:08:15.033373: Pseudo dice [0.7186]
2024-12-07 19:08:15.034278: Epoch time: 406.84 s
2024-12-07 19:08:17.370562: 
2024-12-07 19:08:17.372150: Epoch 60
2024-12-07 19:08:17.373062: Current learning rate: 0.00946
2024-12-07 19:15:12.289551: Validation loss did not improve from -0.51236. Patience: 17/50
2024-12-07 19:15:12.290617: train_loss -0.641
2024-12-07 19:15:12.291797: val_loss -0.4899
2024-12-07 19:15:12.292781: Pseudo dice [0.7179]
2024-12-07 19:15:12.293756: Epoch time: 414.92 s
2024-12-07 19:15:13.944762: 
2024-12-07 19:15:13.946550: Epoch 61
2024-12-07 19:15:13.947478: Current learning rate: 0.00945
2024-12-07 19:22:23.178555: Validation loss improved from -0.51236 to -0.52075! Patience: 17/50
2024-12-07 19:22:23.179858: train_loss -0.6404
2024-12-07 19:22:23.180804: val_loss -0.5208
2024-12-07 19:22:23.181817: Pseudo dice [0.7265]
2024-12-07 19:22:23.182713: Epoch time: 429.24 s
2024-12-07 19:22:23.183547: Yayy! New best EMA pseudo Dice: 0.709
2024-12-07 19:22:25.140497: 
2024-12-07 19:22:25.141604: Epoch 62
2024-12-07 19:22:25.142341: Current learning rate: 0.00944
2024-12-07 19:29:14.056950: Validation loss did not improve from -0.52075. Patience: 1/50
2024-12-07 19:29:14.058012: train_loss -0.6522
2024-12-07 19:29:14.058821: val_loss -0.4624
2024-12-07 19:29:14.059478: Pseudo dice [0.7131]
2024-12-07 19:29:14.060289: Epoch time: 408.92 s
2024-12-07 19:29:14.061014: Yayy! New best EMA pseudo Dice: 0.7094
2024-12-07 19:29:15.874663: 
2024-12-07 19:29:15.875782: Epoch 63
2024-12-07 19:29:15.876539: Current learning rate: 0.00943
2024-12-07 19:36:05.049782: Validation loss did not improve from -0.52075. Patience: 2/50
2024-12-07 19:36:05.050892: train_loss -0.6489
2024-12-07 19:36:05.051906: val_loss -0.4695
2024-12-07 19:36:05.052808: Pseudo dice [0.7058]
2024-12-07 19:36:05.053792: Epoch time: 409.18 s
2024-12-07 19:36:06.496175: 
2024-12-07 19:36:06.497497: Epoch 64
2024-12-07 19:36:06.498249: Current learning rate: 0.00942
2024-12-07 19:43:02.901489: Validation loss did not improve from -0.52075. Patience: 3/50
2024-12-07 19:43:02.902630: train_loss -0.6557
2024-12-07 19:43:02.903426: val_loss -0.4807
2024-12-07 19:43:02.904161: Pseudo dice [0.7092]
2024-12-07 19:43:02.904960: Epoch time: 416.41 s
2024-12-07 19:43:04.811574: 
2024-12-07 19:43:04.812926: Epoch 65
2024-12-07 19:43:04.813740: Current learning rate: 0.00941
2024-12-07 19:49:45.230817: Validation loss did not improve from -0.52075. Patience: 4/50
2024-12-07 19:49:45.232066: train_loss -0.6544
2024-12-07 19:49:45.232926: val_loss -0.5052
2024-12-07 19:49:45.233659: Pseudo dice [0.7206]
2024-12-07 19:49:45.234365: Epoch time: 400.42 s
2024-12-07 19:49:45.235117: Yayy! New best EMA pseudo Dice: 0.7102
2024-12-07 19:49:47.066245: 
2024-12-07 19:49:47.066993: Epoch 66
2024-12-07 19:49:47.067790: Current learning rate: 0.0094
2024-12-07 19:56:39.523416: Validation loss did not improve from -0.52075. Patience: 5/50
2024-12-07 19:56:39.524603: train_loss -0.6544
2024-12-07 19:56:39.525412: val_loss -0.4981
2024-12-07 19:56:39.526214: Pseudo dice [0.7211]
2024-12-07 19:56:39.526974: Epoch time: 412.46 s
2024-12-07 19:56:39.527744: Yayy! New best EMA pseudo Dice: 0.7113
2024-12-07 19:56:41.429785: 
2024-12-07 19:56:41.431051: Epoch 67
2024-12-07 19:56:41.431854: Current learning rate: 0.00939
2024-12-07 20:03:35.875170: Validation loss did not improve from -0.52075. Patience: 6/50
2024-12-07 20:03:35.876698: train_loss -0.6599
2024-12-07 20:03:35.877728: val_loss -0.5058
2024-12-07 20:03:35.878398: Pseudo dice [0.7219]
2024-12-07 20:03:35.879122: Epoch time: 414.45 s
2024-12-07 20:03:35.879760: Yayy! New best EMA pseudo Dice: 0.7124
2024-12-07 20:03:37.722109: 
2024-12-07 20:03:37.723638: Epoch 68
2024-12-07 20:03:37.724946: Current learning rate: 0.00939
2024-12-07 20:10:15.678702: Validation loss did not improve from -0.52075. Patience: 7/50
2024-12-07 20:10:15.679692: train_loss -0.6542
2024-12-07 20:10:15.680812: val_loss -0.5107
2024-12-07 20:10:15.681756: Pseudo dice [0.7275]
2024-12-07 20:10:15.682646: Epoch time: 397.96 s
2024-12-07 20:10:15.683573: Yayy! New best EMA pseudo Dice: 0.7139
2024-12-07 20:10:17.644943: 
2024-12-07 20:10:17.646636: Epoch 69
2024-12-07 20:10:17.647762: Current learning rate: 0.00938
2024-12-07 20:17:02.856112: Validation loss did not improve from -0.52075. Patience: 8/50
2024-12-07 20:17:02.856860: train_loss -0.6499
2024-12-07 20:17:02.857802: val_loss -0.4891
2024-12-07 20:17:02.858713: Pseudo dice [0.7035]
2024-12-07 20:17:02.859643: Epoch time: 405.21 s
2024-12-07 20:17:05.558512: 
2024-12-07 20:17:05.559598: Epoch 70
2024-12-07 20:17:05.560521: Current learning rate: 0.00937
2024-12-07 20:23:49.317200: Validation loss did not improve from -0.52075. Patience: 9/50
2024-12-07 20:23:49.318091: train_loss -0.6563
2024-12-07 20:23:49.319053: val_loss -0.504
2024-12-07 20:23:49.319868: Pseudo dice [0.7141]
2024-12-07 20:23:49.320630: Epoch time: 403.76 s
2024-12-07 20:23:50.752742: 
2024-12-07 20:23:50.754269: Epoch 71
2024-12-07 20:23:50.755086: Current learning rate: 0.00936
2024-12-07 20:30:49.567374: Validation loss did not improve from -0.52075. Patience: 10/50
2024-12-07 20:30:49.568333: train_loss -0.6688
2024-12-07 20:30:49.574369: val_loss -0.503
2024-12-07 20:30:49.575118: Pseudo dice [0.716]
2024-12-07 20:30:49.575885: Epoch time: 418.82 s
2024-12-07 20:30:51.017942: 
2024-12-07 20:30:51.019139: Epoch 72
2024-12-07 20:30:51.020005: Current learning rate: 0.00935
2024-12-07 20:37:42.200050: Validation loss did not improve from -0.52075. Patience: 11/50
2024-12-07 20:37:42.201012: train_loss -0.6719
2024-12-07 20:37:42.201856: val_loss -0.483
2024-12-07 20:37:42.202601: Pseudo dice [0.7117]
2024-12-07 20:37:42.203320: Epoch time: 411.18 s
2024-12-07 20:37:43.617167: 
2024-12-07 20:37:43.618530: Epoch 73
2024-12-07 20:37:43.619275: Current learning rate: 0.00934
2024-12-07 20:44:27.611032: Validation loss did not improve from -0.52075. Patience: 12/50
2024-12-07 20:44:27.612024: train_loss -0.6742
2024-12-07 20:44:27.612781: val_loss -0.5038
2024-12-07 20:44:27.613552: Pseudo dice [0.7273]
2024-12-07 20:44:27.614185: Epoch time: 404.0 s
2024-12-07 20:44:27.614825: Yayy! New best EMA pseudo Dice: 0.7145
2024-12-07 20:44:29.508427: 
2024-12-07 20:44:29.509782: Epoch 74
2024-12-07 20:44:29.510667: Current learning rate: 0.00933
2024-12-07 20:51:17.245513: Validation loss did not improve from -0.52075. Patience: 13/50
2024-12-07 20:51:17.247782: train_loss -0.6601
2024-12-07 20:51:17.249387: val_loss -0.4418
2024-12-07 20:51:17.250235: Pseudo dice [0.6924]
2024-12-07 20:51:17.251242: Epoch time: 407.74 s
2024-12-07 20:51:19.110450: 
2024-12-07 20:51:19.111856: Epoch 75
2024-12-07 20:51:19.112692: Current learning rate: 0.00932
2024-12-07 20:57:55.578093: Validation loss did not improve from -0.52075. Patience: 14/50
2024-12-07 20:57:55.579097: train_loss -0.6698
2024-12-07 20:57:55.580045: val_loss -0.4919
2024-12-07 20:57:55.580824: Pseudo dice [0.7134]
2024-12-07 20:57:55.581659: Epoch time: 396.47 s
2024-12-07 20:57:57.027380: 
2024-12-07 20:57:57.028842: Epoch 76
2024-12-07 20:57:57.029711: Current learning rate: 0.00931
2024-12-07 21:04:48.620597: Validation loss did not improve from -0.52075. Patience: 15/50
2024-12-07 21:04:48.621714: train_loss -0.6661
2024-12-07 21:04:48.622773: val_loss -0.458
2024-12-07 21:04:48.623804: Pseudo dice [0.7]
2024-12-07 21:04:48.624818: Epoch time: 411.6 s
2024-12-07 21:04:50.119374: 
2024-12-07 21:04:50.120752: Epoch 77
2024-12-07 21:04:50.121630: Current learning rate: 0.0093
2024-12-07 21:11:44.425451: Validation loss improved from -0.52075 to -0.52353! Patience: 15/50
2024-12-07 21:11:44.435634: train_loss -0.6733
2024-12-07 21:11:44.436394: val_loss -0.5235
2024-12-07 21:11:44.437057: Pseudo dice [0.74]
2024-12-07 21:11:44.437745: Epoch time: 414.32 s
2024-12-07 21:11:45.915107: 
2024-12-07 21:11:45.916536: Epoch 78
2024-12-07 21:11:45.917264: Current learning rate: 0.0093
2024-12-07 21:18:30.308558: Validation loss did not improve from -0.52353. Patience: 1/50
2024-12-07 21:18:30.309658: train_loss -0.6709
2024-12-07 21:18:30.310773: val_loss -0.519
2024-12-07 21:18:30.311790: Pseudo dice [0.7274]
2024-12-07 21:18:30.312786: Epoch time: 404.4 s
2024-12-07 21:18:30.313720: Yayy! New best EMA pseudo Dice: 0.7154
2024-12-07 21:18:32.367642: 
2024-12-07 21:18:32.369420: Epoch 79
2024-12-07 21:18:32.370906: Current learning rate: 0.00929
2024-12-07 21:25:28.293084: Validation loss did not improve from -0.52353. Patience: 2/50
2024-12-07 21:25:28.294106: train_loss -0.6681
2024-12-07 21:25:28.295280: val_loss -0.5124
2024-12-07 21:25:28.296178: Pseudo dice [0.7305]
2024-12-07 21:25:28.297029: Epoch time: 415.93 s
2024-12-07 21:25:28.754641: Yayy! New best EMA pseudo Dice: 0.7169
2024-12-07 21:25:30.993116: 
2024-12-07 21:25:30.994540: Epoch 80
2024-12-07 21:25:30.995469: Current learning rate: 0.00928
2024-12-07 21:32:26.787085: Validation loss did not improve from -0.52353. Patience: 3/50
2024-12-07 21:32:26.788146: train_loss -0.6801
2024-12-07 21:32:26.789082: val_loss -0.5002
2024-12-07 21:32:26.789833: Pseudo dice [0.7215]
2024-12-07 21:32:26.790803: Epoch time: 415.8 s
2024-12-07 21:32:26.791779: Yayy! New best EMA pseudo Dice: 0.7174
2024-12-07 21:32:28.625651: 
2024-12-07 21:32:28.626910: Epoch 81
2024-12-07 21:32:28.627782: Current learning rate: 0.00927
2024-12-07 21:39:20.259118: Validation loss did not improve from -0.52353. Patience: 4/50
2024-12-07 21:39:20.260135: train_loss -0.6779
2024-12-07 21:39:20.260886: val_loss -0.4966
2024-12-07 21:39:20.261547: Pseudo dice [0.7151]
2024-12-07 21:39:20.262349: Epoch time: 411.64 s
2024-12-07 21:39:21.800522: 
2024-12-07 21:39:21.802380: Epoch 82
2024-12-07 21:39:21.803530: Current learning rate: 0.00926
2024-12-07 21:46:13.682043: Validation loss did not improve from -0.52353. Patience: 5/50
2024-12-07 21:46:13.683009: train_loss -0.6736
2024-12-07 21:46:13.683753: val_loss -0.5197
2024-12-07 21:46:13.684381: Pseudo dice [0.7257]
2024-12-07 21:46:13.685046: Epoch time: 411.88 s
2024-12-07 21:46:13.685646: Yayy! New best EMA pseudo Dice: 0.718
2024-12-07 21:46:15.454733: 
2024-12-07 21:46:15.456022: Epoch 83
2024-12-07 21:46:15.456766: Current learning rate: 0.00925
2024-12-07 21:53:08.458761: Validation loss did not improve from -0.52353. Patience: 6/50
2024-12-07 21:53:08.459826: train_loss -0.6811
2024-12-07 21:53:08.460838: val_loss -0.5074
2024-12-07 21:53:08.461798: Pseudo dice [0.7179]
2024-12-07 21:53:08.462750: Epoch time: 413.01 s
2024-12-07 21:53:09.796955: 
2024-12-07 21:53:09.797803: Epoch 84
2024-12-07 21:53:09.798633: Current learning rate: 0.00924
2024-12-07 22:00:05.182739: Validation loss improved from -0.52353 to -0.52746! Patience: 6/50
2024-12-07 22:00:05.184125: train_loss -0.6836
2024-12-07 22:00:05.184943: val_loss -0.5275
2024-12-07 22:00:05.185714: Pseudo dice [0.7348]
2024-12-07 22:00:05.186378: Epoch time: 415.39 s
2024-12-07 22:00:05.596900: Yayy! New best EMA pseudo Dice: 0.7197
2024-12-07 22:00:07.297442: 
2024-12-07 22:00:07.298840: Epoch 85
2024-12-07 22:00:07.299655: Current learning rate: 0.00923
2024-12-07 22:06:56.921066: Validation loss did not improve from -0.52746. Patience: 1/50
2024-12-07 22:06:56.922282: train_loss -0.683
2024-12-07 22:06:56.923102: val_loss -0.5202
2024-12-07 22:06:56.923760: Pseudo dice [0.731]
2024-12-07 22:06:56.924489: Epoch time: 409.63 s
2024-12-07 22:06:56.925128: Yayy! New best EMA pseudo Dice: 0.7208
2024-12-07 22:06:58.677448: 
2024-12-07 22:06:58.678665: Epoch 86
2024-12-07 22:06:58.679446: Current learning rate: 0.00922
2024-12-07 22:13:47.002657: Validation loss did not improve from -0.52746. Patience: 2/50
2024-12-07 22:13:47.004106: train_loss -0.6839
2024-12-07 22:13:47.004861: val_loss -0.4767
2024-12-07 22:13:47.005557: Pseudo dice [0.7238]
2024-12-07 22:13:47.006258: Epoch time: 408.33 s
2024-12-07 22:13:47.007002: Yayy! New best EMA pseudo Dice: 0.7211
2024-12-07 22:13:48.780516: 
2024-12-07 22:13:48.781528: Epoch 87
2024-12-07 22:13:48.782363: Current learning rate: 0.00921
2024-12-07 22:20:49.727843: Validation loss did not improve from -0.52746. Patience: 3/50
2024-12-07 22:20:49.728861: train_loss -0.6858
2024-12-07 22:20:49.730197: val_loss -0.4602
2024-12-07 22:20:49.731421: Pseudo dice [0.6937]
2024-12-07 22:20:49.732651: Epoch time: 420.95 s
2024-12-07 22:20:51.093988: 
2024-12-07 22:20:51.095615: Epoch 88
2024-12-07 22:20:51.096758: Current learning rate: 0.0092
2024-12-07 22:27:33.187038: Validation loss did not improve from -0.52746. Patience: 4/50
2024-12-07 22:27:33.188176: train_loss -0.6901
2024-12-07 22:27:33.189153: val_loss -0.4942
2024-12-07 22:27:33.189978: Pseudo dice [0.7114]
2024-12-07 22:27:33.190955: Epoch time: 402.1 s
2024-12-07 22:27:34.549982: 
2024-12-07 22:27:34.551522: Epoch 89
2024-12-07 22:27:34.552573: Current learning rate: 0.0092
2024-12-07 22:34:21.487136: Validation loss improved from -0.52746 to -0.53457! Patience: 4/50
2024-12-07 22:34:21.488120: train_loss -0.6883
2024-12-07 22:34:21.488989: val_loss -0.5346
2024-12-07 22:34:21.489825: Pseudo dice [0.7448]
2024-12-07 22:34:21.490716: Epoch time: 406.94 s
2024-12-07 22:34:23.279938: 
2024-12-07 22:34:23.281246: Epoch 90
2024-12-07 22:34:23.282073: Current learning rate: 0.00919
2024-12-07 22:41:18.637746: Validation loss did not improve from -0.53457. Patience: 1/50
2024-12-07 22:41:18.638777: train_loss -0.6899
2024-12-07 22:41:18.639635: val_loss -0.4952
2024-12-07 22:41:18.640550: Pseudo dice [0.7255]
2024-12-07 22:41:18.641587: Epoch time: 415.36 s
2024-12-07 22:41:20.689339: 
2024-12-07 22:41:20.690844: Epoch 91
2024-12-07 22:41:20.691732: Current learning rate: 0.00918
2024-12-07 22:48:08.826658: Validation loss did not improve from -0.53457. Patience: 2/50
2024-12-07 22:48:08.827712: train_loss -0.6887
2024-12-07 22:48:08.828700: val_loss -0.4886
2024-12-07 22:48:08.829396: Pseudo dice [0.7092]
2024-12-07 22:48:08.830108: Epoch time: 408.14 s
2024-12-07 22:48:10.167413: 
2024-12-07 22:48:10.168322: Epoch 92
2024-12-07 22:48:10.169024: Current learning rate: 0.00917
2024-12-07 22:55:04.138004: Validation loss did not improve from -0.53457. Patience: 3/50
2024-12-07 22:55:04.138964: train_loss -0.6977
2024-12-07 22:55:04.145999: val_loss -0.5009
2024-12-07 22:55:04.146951: Pseudo dice [0.7265]
2024-12-07 22:55:04.147873: Epoch time: 413.97 s
2024-12-07 22:55:05.528009: 
2024-12-07 22:55:05.529432: Epoch 93
2024-12-07 22:55:05.530303: Current learning rate: 0.00916
2024-12-07 23:02:03.328223: Validation loss did not improve from -0.53457. Patience: 4/50
2024-12-07 23:02:03.331325: train_loss -0.6946
2024-12-07 23:02:03.333732: val_loss -0.4817
2024-12-07 23:02:03.334589: Pseudo dice [0.7176]
2024-12-07 23:02:03.335547: Epoch time: 417.8 s
2024-12-07 23:02:04.720777: 
2024-12-07 23:02:04.721715: Epoch 94
2024-12-07 23:02:04.722588: Current learning rate: 0.00915
2024-12-07 23:09:02.352179: Validation loss did not improve from -0.53457. Patience: 5/50
2024-12-07 23:09:02.353319: train_loss -0.6984
2024-12-07 23:09:02.354160: val_loss -0.4559
2024-12-07 23:09:02.354830: Pseudo dice [0.7045]
2024-12-07 23:09:02.355502: Epoch time: 417.63 s
2024-12-07 23:09:04.123870: 
2024-12-07 23:09:04.125161: Epoch 95
2024-12-07 23:09:04.125867: Current learning rate: 0.00914
2024-12-07 23:15:53.918742: Validation loss did not improve from -0.53457. Patience: 6/50
2024-12-07 23:15:53.919859: train_loss -0.6818
2024-12-07 23:15:53.920751: val_loss -0.503
2024-12-07 23:15:53.921455: Pseudo dice [0.7225]
2024-12-07 23:15:53.922222: Epoch time: 409.8 s
2024-12-07 23:15:55.245609: 
2024-12-07 23:15:55.246922: Epoch 96
2024-12-07 23:15:55.247688: Current learning rate: 0.00913
2024-12-07 23:22:33.468680: Validation loss did not improve from -0.53457. Patience: 7/50
2024-12-07 23:22:33.469743: train_loss -0.6897
2024-12-07 23:22:33.470532: val_loss -0.5008
2024-12-07 23:22:33.471151: Pseudo dice [0.7155]
2024-12-07 23:22:33.471969: Epoch time: 398.23 s
2024-12-07 23:22:34.860128: 
2024-12-07 23:22:34.861302: Epoch 97
2024-12-07 23:22:34.862020: Current learning rate: 0.00912
2024-12-07 23:29:27.328696: Validation loss did not improve from -0.53457. Patience: 8/50
2024-12-07 23:29:27.329678: train_loss -0.7012
2024-12-07 23:29:27.330542: val_loss -0.5102
2024-12-07 23:29:27.331392: Pseudo dice [0.7302]
2024-12-07 23:29:27.332232: Epoch time: 412.47 s
2024-12-07 23:29:28.673783: 
2024-12-07 23:29:28.675220: Epoch 98
2024-12-07 23:29:28.676187: Current learning rate: 0.00911
2024-12-07 23:36:21.043597: Validation loss did not improve from -0.53457. Patience: 9/50
2024-12-07 23:36:21.045468: train_loss -0.6925
2024-12-07 23:36:21.046738: val_loss -0.5312
2024-12-07 23:36:21.047583: Pseudo dice [0.7381]
2024-12-07 23:36:21.048423: Epoch time: 412.37 s
2024-12-07 23:36:21.049235: Yayy! New best EMA pseudo Dice: 0.7216
2024-12-07 23:36:22.760911: 
2024-12-07 23:36:22.762200: Epoch 99
2024-12-07 23:36:22.763009: Current learning rate: 0.0091
2024-12-07 23:43:16.379344: Validation loss did not improve from -0.53457. Patience: 10/50
2024-12-07 23:43:16.380302: train_loss -0.6958
2024-12-07 23:43:16.381172: val_loss -0.5054
2024-12-07 23:43:16.381947: Pseudo dice [0.7271]
2024-12-07 23:43:16.382682: Epoch time: 413.62 s
2024-12-07 23:43:16.793598: Yayy! New best EMA pseudo Dice: 0.7221
2024-12-07 23:43:18.545194: 
2024-12-07 23:43:18.546681: Epoch 100
2024-12-07 23:43:18.547875: Current learning rate: 0.0091
2024-12-07 23:50:12.562961: Validation loss did not improve from -0.53457. Patience: 11/50
2024-12-07 23:50:12.564006: train_loss -0.6988
2024-12-07 23:50:12.564729: val_loss -0.4811
2024-12-07 23:50:12.565354: Pseudo dice [0.7143]
2024-12-07 23:50:12.566030: Epoch time: 414.02 s
2024-12-07 23:50:13.908434: 
2024-12-07 23:50:13.909766: Epoch 101
2024-12-07 23:50:13.910730: Current learning rate: 0.00909
2024-12-07 23:56:57.292820: Validation loss did not improve from -0.53457. Patience: 12/50
2024-12-07 23:56:57.293574: train_loss -0.6977
2024-12-07 23:56:57.294374: val_loss -0.4576
2024-12-07 23:56:57.295115: Pseudo dice [0.6864]
2024-12-07 23:56:57.295818: Epoch time: 403.39 s
2024-12-07 23:56:59.385020: 
2024-12-07 23:56:59.386582: Epoch 102
2024-12-07 23:56:59.387844: Current learning rate: 0.00908
2024-12-08 00:03:59.219203: Validation loss did not improve from -0.53457. Patience: 13/50
2024-12-08 00:03:59.220195: train_loss -0.693
2024-12-08 00:03:59.221074: val_loss -0.5026
2024-12-08 00:03:59.221730: Pseudo dice [0.7266]
2024-12-08 00:03:59.222396: Epoch time: 419.84 s
2024-12-08 00:04:00.583915: 
2024-12-08 00:04:00.585210: Epoch 103
2024-12-08 00:04:00.585957: Current learning rate: 0.00907
2024-12-08 00:10:42.405275: Validation loss did not improve from -0.53457. Patience: 14/50
2024-12-08 00:10:42.406621: train_loss -0.7016
2024-12-08 00:10:42.407435: val_loss -0.4747
2024-12-08 00:10:42.408090: Pseudo dice [0.7114]
2024-12-08 00:10:42.408952: Epoch time: 401.82 s
2024-12-08 00:10:43.848529: 
2024-12-08 00:10:43.850014: Epoch 104
2024-12-08 00:10:43.851023: Current learning rate: 0.00906
2024-12-08 00:17:39.548328: Validation loss did not improve from -0.53457. Patience: 15/50
2024-12-08 00:17:39.549396: train_loss -0.688
2024-12-08 00:17:39.550146: val_loss -0.4658
2024-12-08 00:17:39.550904: Pseudo dice [0.7067]
2024-12-08 00:17:39.551726: Epoch time: 415.7 s
2024-12-08 00:17:41.382025: 
2024-12-08 00:17:41.383371: Epoch 105
2024-12-08 00:17:41.384265: Current learning rate: 0.00905
2024-12-08 00:24:38.131354: Validation loss did not improve from -0.53457. Patience: 16/50
2024-12-08 00:24:38.132437: train_loss -0.7024
2024-12-08 00:24:38.133212: val_loss -0.4968
2024-12-08 00:24:38.133952: Pseudo dice [0.725]
2024-12-08 00:24:38.134653: Epoch time: 416.75 s
2024-12-08 00:24:39.686871: 
2024-12-08 00:24:39.688399: Epoch 106
2024-12-08 00:24:39.689213: Current learning rate: 0.00904
2024-12-08 00:31:25.613162: Validation loss did not improve from -0.53457. Patience: 17/50
2024-12-08 00:31:25.614187: train_loss -0.7059
2024-12-08 00:31:25.615086: val_loss -0.5075
2024-12-08 00:31:25.615819: Pseudo dice [0.725]
2024-12-08 00:31:25.616585: Epoch time: 405.93 s
2024-12-08 00:31:26.974064: 
2024-12-08 00:31:26.975657: Epoch 107
2024-12-08 00:31:26.976505: Current learning rate: 0.00903
2024-12-08 00:38:21.295943: Validation loss did not improve from -0.53457. Patience: 18/50
2024-12-08 00:38:21.297281: train_loss -0.7141
2024-12-08 00:38:21.298431: val_loss -0.5242
2024-12-08 00:38:21.299461: Pseudo dice [0.7344]
2024-12-08 00:38:21.300413: Epoch time: 414.32 s
2024-12-08 00:38:22.712002: 
2024-12-08 00:38:22.713345: Epoch 108
2024-12-08 00:38:22.714234: Current learning rate: 0.00902
2024-12-08 00:45:14.406379: Validation loss did not improve from -0.53457. Patience: 19/50
2024-12-08 00:45:14.407827: train_loss -0.7109
2024-12-08 00:45:14.408682: val_loss -0.4982
2024-12-08 00:45:14.409476: Pseudo dice [0.7243]
2024-12-08 00:45:14.410204: Epoch time: 411.7 s
2024-12-08 00:45:15.765110: 
2024-12-08 00:45:15.766203: Epoch 109
2024-12-08 00:45:15.767119: Current learning rate: 0.00901
2024-12-08 00:52:11.918570: Validation loss did not improve from -0.53457. Patience: 20/50
2024-12-08 00:52:11.919903: train_loss -0.7083
2024-12-08 00:52:11.920931: val_loss -0.5133
2024-12-08 00:52:11.921979: Pseudo dice [0.7287]
2024-12-08 00:52:11.922806: Epoch time: 416.16 s
2024-12-08 00:52:13.710836: 
2024-12-08 00:52:13.712081: Epoch 110
2024-12-08 00:52:13.712814: Current learning rate: 0.009
2024-12-08 00:59:26.186579: Validation loss did not improve from -0.53457. Patience: 21/50
2024-12-08 00:59:26.187742: train_loss -0.7113
2024-12-08 00:59:26.188702: val_loss -0.5043
2024-12-08 00:59:26.189383: Pseudo dice [0.7324]
2024-12-08 00:59:26.190188: Epoch time: 432.48 s
2024-12-08 00:59:26.191082: Yayy! New best EMA pseudo Dice: 0.7224
2024-12-08 00:59:28.062462: 
2024-12-08 00:59:28.063807: Epoch 111
2024-12-08 00:59:28.064595: Current learning rate: 0.009
2024-12-08 01:06:29.826423: Validation loss did not improve from -0.53457. Patience: 22/50
2024-12-08 01:06:29.827519: train_loss -0.7142
2024-12-08 01:06:29.828593: val_loss -0.5162
2024-12-08 01:06:29.829603: Pseudo dice [0.7375]
2024-12-08 01:06:29.830494: Epoch time: 421.77 s
2024-12-08 01:06:29.831394: Yayy! New best EMA pseudo Dice: 0.7239
2024-12-08 01:06:31.604685: 
2024-12-08 01:06:31.606064: Epoch 112
2024-12-08 01:06:31.607087: Current learning rate: 0.00899
2024-12-08 01:13:37.572484: Validation loss did not improve from -0.53457. Patience: 23/50
2024-12-08 01:13:37.576156: train_loss -0.7063
2024-12-08 01:13:37.577670: val_loss -0.4983
2024-12-08 01:13:37.578501: Pseudo dice [0.7314]
2024-12-08 01:13:37.579386: Epoch time: 425.97 s
2024-12-08 01:13:37.580191: Yayy! New best EMA pseudo Dice: 0.7246
2024-12-08 01:13:40.261919: 
2024-12-08 01:13:40.263333: Epoch 113
2024-12-08 01:13:40.264287: Current learning rate: 0.00898
2024-12-08 01:20:41.654813: Validation loss did not improve from -0.53457. Patience: 24/50
2024-12-08 01:20:41.656836: train_loss -0.7073
2024-12-08 01:20:41.658010: val_loss -0.4981
2024-12-08 01:20:41.658990: Pseudo dice [0.7223]
2024-12-08 01:20:41.660156: Epoch time: 421.4 s
2024-12-08 01:20:43.049227: 
2024-12-08 01:20:43.050679: Epoch 114
2024-12-08 01:20:43.051754: Current learning rate: 0.00897
2024-12-08 01:27:50.970623: Validation loss did not improve from -0.53457. Patience: 25/50
2024-12-08 01:27:50.971776: train_loss -0.7114
2024-12-08 01:27:50.973066: val_loss -0.5084
2024-12-08 01:27:50.974168: Pseudo dice [0.7408]
2024-12-08 01:27:50.975272: Epoch time: 427.92 s
2024-12-08 01:27:51.532771: Yayy! New best EMA pseudo Dice: 0.726
2024-12-08 01:27:53.297733: 
2024-12-08 01:27:53.299267: Epoch 115
2024-12-08 01:27:53.300210: Current learning rate: 0.00896
2024-12-08 01:34:54.271997: Validation loss did not improve from -0.53457. Patience: 26/50
2024-12-08 01:34:54.273143: train_loss -0.6982
2024-12-08 01:34:54.273974: val_loss -0.5043
2024-12-08 01:34:54.274719: Pseudo dice [0.7295]
2024-12-08 01:34:54.275470: Epoch time: 420.98 s
2024-12-08 01:34:54.276237: Yayy! New best EMA pseudo Dice: 0.7264
2024-12-08 01:34:56.055409: 
2024-12-08 01:34:56.056910: Epoch 116
2024-12-08 01:34:56.057875: Current learning rate: 0.00895
2024-12-08 01:41:53.301842: Validation loss did not improve from -0.53457. Patience: 27/50
2024-12-08 01:41:53.302757: train_loss -0.7123
2024-12-08 01:41:53.303669: val_loss -0.4699
2024-12-08 01:41:53.304490: Pseudo dice [0.7002]
2024-12-08 01:41:53.305244: Epoch time: 417.25 s
2024-12-08 01:41:54.693858: 
2024-12-08 01:41:54.695167: Epoch 117
2024-12-08 01:41:54.695944: Current learning rate: 0.00894
2024-12-08 01:48:46.114225: Validation loss did not improve from -0.53457. Patience: 28/50
2024-12-08 01:48:46.115224: train_loss -0.7166
2024-12-08 01:48:46.115990: val_loss -0.5088
2024-12-08 01:48:46.116721: Pseudo dice [0.7295]
2024-12-08 01:48:46.117386: Epoch time: 411.42 s
2024-12-08 01:48:47.515608: 
2024-12-08 01:48:47.516975: Epoch 118
2024-12-08 01:48:47.517837: Current learning rate: 0.00893
2024-12-08 01:55:31.393755: Validation loss improved from -0.53457 to -0.54665! Patience: 28/50
2024-12-08 01:55:31.394784: train_loss -0.7174
2024-12-08 01:55:31.395584: val_loss -0.5467
2024-12-08 01:55:31.396369: Pseudo dice [0.743]
2024-12-08 01:55:31.397083: Epoch time: 403.88 s
2024-12-08 01:55:32.816241: 
2024-12-08 01:55:32.817612: Epoch 119
2024-12-08 01:55:32.818337: Current learning rate: 0.00892
2024-12-08 02:02:30.792177: Validation loss did not improve from -0.54665. Patience: 1/50
2024-12-08 02:02:30.793170: train_loss -0.7157
2024-12-08 02:02:30.794209: val_loss -0.4606
2024-12-08 02:02:30.795030: Pseudo dice [0.6825]
2024-12-08 02:02:30.795906: Epoch time: 417.98 s
2024-12-08 02:02:32.618770: 
2024-12-08 02:02:32.620086: Epoch 120
2024-12-08 02:02:32.620914: Current learning rate: 0.00891
2024-12-08 02:09:36.351789: Validation loss did not improve from -0.54665. Patience: 2/50
2024-12-08 02:09:36.353285: train_loss -0.6976
2024-12-08 02:09:36.354176: val_loss -0.4598
2024-12-08 02:09:36.355015: Pseudo dice [0.701]
2024-12-08 02:09:36.355706: Epoch time: 423.74 s
2024-12-08 02:09:37.752277: 
2024-12-08 02:09:37.753719: Epoch 121
2024-12-08 02:09:37.754557: Current learning rate: 0.0089
2024-12-08 02:16:05.012333: Validation loss did not improve from -0.54665. Patience: 3/50
2024-12-08 02:16:05.013536: train_loss -0.7074
2024-12-08 02:16:05.014396: val_loss -0.5038
2024-12-08 02:16:05.015172: Pseudo dice [0.7253]
2024-12-08 02:16:05.015923: Epoch time: 387.26 s
2024-12-08 02:16:06.497683: 
2024-12-08 02:16:06.498497: Epoch 122
2024-12-08 02:16:06.499173: Current learning rate: 0.00889
2024-12-08 02:21:19.708199: Validation loss did not improve from -0.54665. Patience: 4/50
2024-12-08 02:21:19.709172: train_loss -0.7185
2024-12-08 02:21:19.709889: val_loss -0.4943
2024-12-08 02:21:19.710878: Pseudo dice [0.7148]
2024-12-08 02:21:19.711544: Epoch time: 313.21 s
2024-12-08 02:21:21.078420: 
2024-12-08 02:21:21.080116: Epoch 123
2024-12-08 02:21:21.081204: Current learning rate: 0.00889
2024-12-08 02:26:02.865771: Validation loss did not improve from -0.54665. Patience: 5/50
2024-12-08 02:26:02.866786: train_loss -0.6909
2024-12-08 02:26:02.867726: val_loss -0.5012
2024-12-08 02:26:02.868654: Pseudo dice [0.7217]
2024-12-08 02:26:02.869558: Epoch time: 281.79 s
2024-12-08 02:26:04.592945: 
2024-12-08 02:26:04.594229: Epoch 124
2024-12-08 02:26:04.595176: Current learning rate: 0.00888
2024-12-08 02:30:41.268765: Validation loss did not improve from -0.54665. Patience: 6/50
2024-12-08 02:30:41.269730: train_loss -0.7005
2024-12-08 02:30:41.270502: val_loss -0.4951
2024-12-08 02:30:41.271116: Pseudo dice [0.7165]
2024-12-08 02:30:41.271860: Epoch time: 276.68 s
2024-12-08 02:30:43.008695: 
2024-12-08 02:30:43.010117: Epoch 125
2024-12-08 02:30:43.010946: Current learning rate: 0.00887
2024-12-08 02:35:24.509251: Validation loss did not improve from -0.54665. Patience: 7/50
2024-12-08 02:35:24.510256: train_loss -0.7145
2024-12-08 02:35:24.511055: val_loss -0.4575
2024-12-08 02:35:24.511810: Pseudo dice [0.7074]
2024-12-08 02:35:24.512480: Epoch time: 281.5 s
2024-12-08 02:35:25.881482: 
2024-12-08 02:35:25.882762: Epoch 126
2024-12-08 02:35:25.883429: Current learning rate: 0.00886
2024-12-08 02:40:10.379811: Validation loss did not improve from -0.54665. Patience: 8/50
2024-12-08 02:40:10.380934: train_loss -0.7169
2024-12-08 02:40:10.381732: val_loss -0.4768
2024-12-08 02:40:10.382415: Pseudo dice [0.7135]
2024-12-08 02:40:10.383124: Epoch time: 284.5 s
2024-12-08 02:40:11.778453: 
2024-12-08 02:40:11.779902: Epoch 127
2024-12-08 02:40:11.780797: Current learning rate: 0.00885
2024-12-08 02:44:49.334870: Validation loss did not improve from -0.54665. Patience: 9/50
2024-12-08 02:44:49.335765: train_loss -0.7175
2024-12-08 02:44:49.336708: val_loss -0.482
2024-12-08 02:44:49.337432: Pseudo dice [0.7099]
2024-12-08 02:44:49.338232: Epoch time: 277.56 s
2024-12-08 02:44:50.723597: 
2024-12-08 02:44:50.725072: Epoch 128
2024-12-08 02:44:50.726041: Current learning rate: 0.00884
2024-12-08 02:49:29.895093: Validation loss did not improve from -0.54665. Patience: 10/50
2024-12-08 02:49:29.896003: train_loss -0.7292
2024-12-08 02:49:29.896824: val_loss -0.5313
2024-12-08 02:49:29.897725: Pseudo dice [0.7433]
2024-12-08 02:49:29.898504: Epoch time: 279.17 s
2024-12-08 02:49:31.265302: 
2024-12-08 02:49:31.266595: Epoch 129
2024-12-08 02:49:31.267344: Current learning rate: 0.00883
2024-12-08 02:54:07.489078: Validation loss did not improve from -0.54665. Patience: 11/50
2024-12-08 02:54:07.489853: train_loss -0.7203
2024-12-08 02:54:07.490565: val_loss -0.4923
2024-12-08 02:54:07.491237: Pseudo dice [0.721]
2024-12-08 02:54:07.491942: Epoch time: 276.23 s
2024-12-08 02:54:09.334985: 
2024-12-08 02:54:09.336338: Epoch 130
2024-12-08 02:54:09.337278: Current learning rate: 0.00882
2024-12-08 02:58:45.585099: Validation loss did not improve from -0.54665. Patience: 12/50
2024-12-08 02:58:45.586138: train_loss -0.7224
2024-12-08 02:58:45.587440: val_loss -0.5237
2024-12-08 02:58:45.588249: Pseudo dice [0.7363]
2024-12-08 02:58:45.589177: Epoch time: 276.25 s
2024-12-08 02:58:46.994193: 
2024-12-08 02:58:46.995701: Epoch 131
2024-12-08 02:58:46.996726: Current learning rate: 0.00881
2024-12-08 03:03:25.361462: Validation loss did not improve from -0.54665. Patience: 13/50
2024-12-08 03:03:25.362418: train_loss -0.7197
2024-12-08 03:03:25.363157: val_loss -0.4803
2024-12-08 03:03:25.363852: Pseudo dice [0.7223]
2024-12-08 03:03:25.364603: Epoch time: 278.37 s
2024-12-08 03:03:26.762245: 
2024-12-08 03:03:26.763589: Epoch 132
2024-12-08 03:03:26.764464: Current learning rate: 0.0088
2024-12-08 03:07:51.765093: Validation loss did not improve from -0.54665. Patience: 14/50
2024-12-08 03:07:51.766250: train_loss -0.7283
2024-12-08 03:07:51.767112: val_loss -0.4957
2024-12-08 03:07:51.767834: Pseudo dice [0.7172]
2024-12-08 03:07:51.768494: Epoch time: 265.01 s
2024-12-08 03:07:53.148842: 
2024-12-08 03:07:53.150146: Epoch 133
2024-12-08 03:07:53.151310: Current learning rate: 0.00879
2024-12-08 03:12:15.500092: Validation loss did not improve from -0.54665. Patience: 15/50
2024-12-08 03:12:15.501359: train_loss -0.7291
2024-12-08 03:12:15.502402: val_loss -0.539
2024-12-08 03:12:15.503356: Pseudo dice [0.7515]
2024-12-08 03:12:15.504318: Epoch time: 262.35 s
2024-12-08 03:12:16.986124: 
2024-12-08 03:12:16.987719: Epoch 134
2024-12-08 03:12:16.988680: Current learning rate: 0.00879
2024-12-08 03:16:41.533968: Validation loss did not improve from -0.54665. Patience: 16/50
2024-12-08 03:16:41.534967: train_loss -0.7314
2024-12-08 03:16:41.535847: val_loss -0.4977
2024-12-08 03:16:41.536527: Pseudo dice [0.727]
2024-12-08 03:16:41.537311: Epoch time: 264.55 s
2024-12-08 03:16:43.661479: 
2024-12-08 03:16:43.662435: Epoch 135
2024-12-08 03:16:43.663154: Current learning rate: 0.00878
2024-12-08 03:21:22.438207: Validation loss did not improve from -0.54665. Patience: 17/50
2024-12-08 03:21:22.439204: train_loss -0.7235
2024-12-08 03:21:22.440242: val_loss -0.487
2024-12-08 03:21:22.441115: Pseudo dice [0.7219]
2024-12-08 03:21:22.442040: Epoch time: 278.78 s
2024-12-08 03:21:23.917685: 
2024-12-08 03:21:23.919151: Epoch 136
2024-12-08 03:21:23.920174: Current learning rate: 0.00877
2024-12-08 03:26:02.361544: Validation loss did not improve from -0.54665. Patience: 18/50
2024-12-08 03:26:02.362672: train_loss -0.7246
2024-12-08 03:26:02.363464: val_loss -0.4708
2024-12-08 03:26:02.364206: Pseudo dice [0.7013]
2024-12-08 03:26:02.365005: Epoch time: 278.45 s
2024-12-08 03:26:03.755285: 
2024-12-08 03:26:03.756458: Epoch 137
2024-12-08 03:26:03.757300: Current learning rate: 0.00876
2024-12-08 03:30:36.318353: Validation loss did not improve from -0.54665. Patience: 19/50
2024-12-08 03:30:36.340352: train_loss -0.7288
2024-12-08 03:30:36.341979: val_loss -0.4578
2024-12-08 03:30:36.342860: Pseudo dice [0.7051]
2024-12-08 03:30:36.343626: Epoch time: 272.57 s
2024-12-08 03:30:37.821025: 
2024-12-08 03:30:37.822199: Epoch 138
2024-12-08 03:30:37.822839: Current learning rate: 0.00875
2024-12-08 03:35:08.084180: Validation loss did not improve from -0.54665. Patience: 20/50
2024-12-08 03:35:08.085291: train_loss -0.7224
2024-12-08 03:35:08.086075: val_loss -0.4652
2024-12-08 03:35:08.086794: Pseudo dice [0.7077]
2024-12-08 03:35:08.087431: Epoch time: 270.27 s
2024-12-08 03:35:09.498816: 
2024-12-08 03:35:09.500343: Epoch 139
2024-12-08 03:35:09.501182: Current learning rate: 0.00874
2024-12-08 03:39:42.131277: Validation loss did not improve from -0.54665. Patience: 21/50
2024-12-08 03:39:42.132352: train_loss -0.7235
2024-12-08 03:39:42.133296: val_loss -0.5278
2024-12-08 03:39:42.134055: Pseudo dice [0.7326]
2024-12-08 03:39:42.134897: Epoch time: 272.63 s
2024-12-08 03:39:44.226444: 
2024-12-08 03:39:44.228427: Epoch 140
2024-12-08 03:39:44.229713: Current learning rate: 0.00873
2024-12-08 03:44:21.945611: Validation loss did not improve from -0.54665. Patience: 22/50
2024-12-08 03:44:21.946711: train_loss -0.7306
2024-12-08 03:44:21.948001: val_loss -0.5278
2024-12-08 03:44:21.949177: Pseudo dice [0.7383]
2024-12-08 03:44:21.950366: Epoch time: 277.73 s
2024-12-08 03:44:23.470067: 
2024-12-08 03:44:23.471831: Epoch 141
2024-12-08 03:44:23.472955: Current learning rate: 0.00872
2024-12-08 03:48:55.871437: Validation loss did not improve from -0.54665. Patience: 23/50
2024-12-08 03:48:55.873437: train_loss -0.7372
2024-12-08 03:48:55.874980: val_loss -0.5072
2024-12-08 03:48:55.876259: Pseudo dice [0.7338]
2024-12-08 03:48:55.877475: Epoch time: 272.4 s
2024-12-08 03:48:57.319320: 
2024-12-08 03:48:57.320723: Epoch 142
2024-12-08 03:48:57.321521: Current learning rate: 0.00871
2024-12-08 03:53:18.194149: Validation loss did not improve from -0.54665. Patience: 24/50
2024-12-08 03:53:18.195080: train_loss -0.7405
2024-12-08 03:53:18.195975: val_loss -0.4817
2024-12-08 03:53:18.196774: Pseudo dice [0.706]
2024-12-08 03:53:18.197423: Epoch time: 260.88 s
2024-12-08 03:53:19.626460: 
2024-12-08 03:53:19.627818: Epoch 143
2024-12-08 03:53:19.628639: Current learning rate: 0.0087
2024-12-08 03:56:49.674515: Validation loss did not improve from -0.54665. Patience: 25/50
2024-12-08 03:56:49.675457: train_loss -0.7363
2024-12-08 03:56:49.676121: val_loss -0.4974
2024-12-08 03:56:49.676880: Pseudo dice [0.7224]
2024-12-08 03:56:49.677616: Epoch time: 210.05 s
2024-12-08 03:56:51.075974: 
2024-12-08 03:56:51.077188: Epoch 144
2024-12-08 03:56:51.078097: Current learning rate: 0.00869
2024-12-08 04:01:32.170852: Validation loss did not improve from -0.54665. Patience: 26/50
2024-12-08 04:01:32.172010: train_loss -0.7348
2024-12-08 04:01:32.173039: val_loss -0.4774
2024-12-08 04:01:32.174599: Pseudo dice [0.7129]
2024-12-08 04:01:32.175947: Epoch time: 281.1 s
2024-12-08 04:01:34.683968: 
2024-12-08 04:01:34.685353: Epoch 145
2024-12-08 04:01:34.686322: Current learning rate: 0.00868
2024-12-08 04:06:04.423770: Validation loss did not improve from -0.54665. Patience: 27/50
2024-12-08 04:06:04.424709: train_loss -0.7389
2024-12-08 04:06:04.425395: val_loss -0.4883
2024-12-08 04:06:04.426025: Pseudo dice [0.7197]
2024-12-08 04:06:04.426678: Epoch time: 269.74 s
2024-12-08 04:06:05.838422: 
2024-12-08 04:06:05.839816: Epoch 146
2024-12-08 04:06:05.840634: Current learning rate: 0.00868
2024-12-08 04:10:39.563787: Validation loss did not improve from -0.54665. Patience: 28/50
2024-12-08 04:10:39.564846: train_loss -0.7379
2024-12-08 04:10:39.565587: val_loss -0.466
2024-12-08 04:10:39.566222: Pseudo dice [0.7066]
2024-12-08 04:10:39.566828: Epoch time: 273.73 s
2024-12-08 04:10:40.992356: 
2024-12-08 04:10:40.993492: Epoch 147
2024-12-08 04:10:40.994192: Current learning rate: 0.00867
2024-12-08 04:14:33.371914: Validation loss did not improve from -0.54665. Patience: 29/50
2024-12-08 04:14:33.373013: train_loss -0.7375
2024-12-08 04:14:33.373880: val_loss -0.4938
2024-12-08 04:14:33.374811: Pseudo dice [0.7227]
2024-12-08 04:14:33.375599: Epoch time: 232.38 s
2024-12-08 04:14:34.801584: 
2024-12-08 04:14:34.802938: Epoch 148
2024-12-08 04:14:34.803766: Current learning rate: 0.00866
2024-12-08 04:18:45.579533: Validation loss did not improve from -0.54665. Patience: 30/50
2024-12-08 04:18:45.580575: train_loss -0.7392
2024-12-08 04:18:45.581382: val_loss -0.4992
2024-12-08 04:18:45.582042: Pseudo dice [0.7176]
2024-12-08 04:18:45.582739: Epoch time: 250.78 s
2024-12-08 04:18:47.064157: 
2024-12-08 04:18:47.066566: Epoch 149
2024-12-08 04:18:47.067693: Current learning rate: 0.00865
2024-12-08 04:23:22.146346: Validation loss did not improve from -0.54665. Patience: 31/50
2024-12-08 04:23:22.147720: train_loss -0.7393
2024-12-08 04:23:22.148997: val_loss -0.5278
2024-12-08 04:23:22.150084: Pseudo dice [0.7398]
2024-12-08 04:23:22.151071: Epoch time: 275.09 s
2024-12-08 04:23:24.050421: 
2024-12-08 04:23:24.052178: Epoch 150
2024-12-08 04:23:24.053225: Current learning rate: 0.00864
2024-12-08 04:27:59.254465: Validation loss did not improve from -0.54665. Patience: 32/50
2024-12-08 04:27:59.255434: train_loss -0.7313
2024-12-08 04:27:59.256344: val_loss -0.4991
2024-12-08 04:27:59.257112: Pseudo dice [0.7178]
2024-12-08 04:27:59.257827: Epoch time: 275.21 s
2024-12-08 04:28:00.740438: 
2024-12-08 04:28:00.742829: Epoch 151
2024-12-08 04:28:00.743898: Current learning rate: 0.00863
2024-12-08 04:32:43.968626: Validation loss did not improve from -0.54665. Patience: 33/50
2024-12-08 04:32:43.973155: train_loss -0.7231
2024-12-08 04:32:43.975434: val_loss -0.4673
2024-12-08 04:32:43.976273: Pseudo dice [0.7065]
2024-12-08 04:32:43.977379: Epoch time: 283.23 s
2024-12-08 04:32:45.520896: 
2024-12-08 04:32:45.522510: Epoch 152
2024-12-08 04:32:45.523696: Current learning rate: 0.00862
2024-12-08 04:37:23.631514: Validation loss did not improve from -0.54665. Patience: 34/50
2024-12-08 04:37:23.632798: train_loss -0.7276
2024-12-08 04:37:23.633651: val_loss -0.4883
2024-12-08 04:37:23.634436: Pseudo dice [0.7199]
2024-12-08 04:37:23.635062: Epoch time: 278.11 s
2024-12-08 04:37:25.195983: 
2024-12-08 04:37:25.198163: Epoch 153
2024-12-08 04:37:25.200453: Current learning rate: 0.00861
2024-12-08 04:41:59.898965: Validation loss did not improve from -0.54665. Patience: 35/50
2024-12-08 04:41:59.900115: train_loss -0.7338
2024-12-08 04:41:59.901423: val_loss -0.5162
2024-12-08 04:41:59.902481: Pseudo dice [0.7303]
2024-12-08 04:41:59.903640: Epoch time: 274.71 s
2024-12-08 04:42:01.456575: 
2024-12-08 04:42:01.458035: Epoch 154
2024-12-08 04:42:01.459211: Current learning rate: 0.0086
2024-12-08 04:46:39.298226: Validation loss did not improve from -0.54665. Patience: 36/50
2024-12-08 04:46:39.300767: train_loss -0.73
2024-12-08 04:46:39.301965: val_loss -0.4807
2024-12-08 04:46:39.302696: Pseudo dice [0.7029]
2024-12-08 04:46:39.303484: Epoch time: 277.85 s
2024-12-08 04:46:42.158572: 
2024-12-08 04:46:42.159873: Epoch 155
2024-12-08 04:46:42.160668: Current learning rate: 0.00859
2024-12-08 04:51:14.927251: Validation loss did not improve from -0.54665. Patience: 37/50
2024-12-08 04:51:14.928195: train_loss -0.7281
2024-12-08 04:51:14.929239: val_loss -0.4707
2024-12-08 04:51:14.930056: Pseudo dice [0.7088]
2024-12-08 04:51:14.930905: Epoch time: 272.77 s
2024-12-08 04:51:16.409938: 
2024-12-08 04:51:16.411313: Epoch 156
2024-12-08 04:51:16.412128: Current learning rate: 0.00858
2024-12-08 04:55:40.881353: Validation loss did not improve from -0.54665. Patience: 38/50
2024-12-08 04:55:40.882324: train_loss -0.7355
2024-12-08 04:55:40.883182: val_loss -0.525
2024-12-08 04:55:40.884038: Pseudo dice [0.7382]
2024-12-08 04:55:40.884960: Epoch time: 264.47 s
2024-12-08 04:55:42.358385: 
2024-12-08 04:55:42.359930: Epoch 157
2024-12-08 04:55:42.360940: Current learning rate: 0.00858
2024-12-08 05:00:20.077325: Validation loss did not improve from -0.54665. Patience: 39/50
2024-12-08 05:00:20.078311: train_loss -0.7362
2024-12-08 05:00:20.079106: val_loss -0.475
2024-12-08 05:00:20.079848: Pseudo dice [0.7119]
2024-12-08 05:00:20.080623: Epoch time: 277.72 s
2024-12-08 05:00:21.584100: 
2024-12-08 05:00:21.585433: Epoch 158
2024-12-08 05:00:21.586476: Current learning rate: 0.00857
2024-12-08 05:04:54.659519: Validation loss did not improve from -0.54665. Patience: 40/50
2024-12-08 05:04:54.660540: train_loss -0.7371
2024-12-08 05:04:54.661319: val_loss -0.4821
2024-12-08 05:04:54.662019: Pseudo dice [0.724]
2024-12-08 05:04:54.662694: Epoch time: 273.08 s
2024-12-08 05:04:56.188492: 
2024-12-08 05:04:56.189927: Epoch 159
2024-12-08 05:04:56.190855: Current learning rate: 0.00856
2024-12-08 05:09:24.795324: Validation loss did not improve from -0.54665. Patience: 41/50
2024-12-08 05:09:24.796781: train_loss -0.7432
2024-12-08 05:09:24.797800: val_loss -0.4935
2024-12-08 05:09:24.798662: Pseudo dice [0.722]
2024-12-08 05:09:24.799421: Epoch time: 268.61 s
2024-12-08 05:09:26.898174: 
2024-12-08 05:09:26.900694: Epoch 160
2024-12-08 05:09:26.902134: Current learning rate: 0.00855
2024-12-08 05:13:54.957385: Validation loss did not improve from -0.54665. Patience: 42/50
2024-12-08 05:13:54.958416: train_loss -0.7344
2024-12-08 05:13:54.959290: val_loss -0.4543
2024-12-08 05:13:54.960017: Pseudo dice [0.7029]
2024-12-08 05:13:54.960734: Epoch time: 268.07 s
2024-12-08 05:13:56.463603: 
2024-12-08 05:13:56.464883: Epoch 161
2024-12-08 05:13:56.465800: Current learning rate: 0.00854
2024-12-08 05:18:42.385865: Validation loss did not improve from -0.54665. Patience: 43/50
2024-12-08 05:18:42.387985: train_loss -0.731
2024-12-08 05:18:42.389161: val_loss -0.4948
2024-12-08 05:18:42.390153: Pseudo dice [0.726]
2024-12-08 05:18:42.391279: Epoch time: 285.93 s
2024-12-08 05:18:43.852312: 
2024-12-08 05:18:43.853527: Epoch 162
2024-12-08 05:18:43.854373: Current learning rate: 0.00853
2024-12-08 05:22:22.132077: Validation loss did not improve from -0.54665. Patience: 44/50
2024-12-08 05:22:22.133122: train_loss -0.7415
2024-12-08 05:22:22.134019: val_loss -0.5184
2024-12-08 05:22:22.134692: Pseudo dice [0.7356]
2024-12-08 05:22:22.135482: Epoch time: 218.28 s
2024-12-08 05:22:23.616560: 
2024-12-08 05:22:23.617976: Epoch 163
2024-12-08 05:22:23.618761: Current learning rate: 0.00852
2024-12-08 05:26:54.855077: Validation loss did not improve from -0.54665. Patience: 45/50
2024-12-08 05:26:54.856143: train_loss -0.737
2024-12-08 05:26:54.856966: val_loss -0.4916
2024-12-08 05:26:54.857620: Pseudo dice [0.7195]
2024-12-08 05:26:54.858391: Epoch time: 271.24 s
2024-12-08 05:26:56.342305: 
2024-12-08 05:26:56.343692: Epoch 164
2024-12-08 05:26:56.344399: Current learning rate: 0.00851
2024-12-08 05:31:34.530917: Validation loss did not improve from -0.54665. Patience: 46/50
2024-12-08 05:31:34.532078: train_loss -0.7417
2024-12-08 05:31:34.533311: val_loss -0.4802
2024-12-08 05:31:34.534234: Pseudo dice [0.7124]
2024-12-08 05:31:34.535124: Epoch time: 278.19 s
2024-12-08 05:31:36.363099: 
2024-12-08 05:31:36.364659: Epoch 165
2024-12-08 05:31:36.365682: Current learning rate: 0.0085
2024-12-08 05:36:16.159019: Validation loss did not improve from -0.54665. Patience: 47/50
2024-12-08 05:36:16.160616: train_loss -0.7478
2024-12-08 05:36:16.162619: val_loss -0.5003
2024-12-08 05:36:16.163730: Pseudo dice [0.7203]
2024-12-08 05:36:16.165227: Epoch time: 279.8 s
2024-12-08 05:36:18.258527: 
2024-12-08 05:36:18.259929: Epoch 166
2024-12-08 05:36:18.260746: Current learning rate: 0.00849
2024-12-08 05:40:52.456738: Validation loss did not improve from -0.54665. Patience: 48/50
2024-12-08 05:40:52.460995: train_loss -0.7506
2024-12-08 05:40:52.462023: val_loss -0.4994
2024-12-08 05:40:52.462788: Pseudo dice [0.7259]
2024-12-08 05:40:52.463529: Epoch time: 274.2 s
2024-12-08 05:40:53.901803: 
2024-12-08 05:40:53.903129: Epoch 167
2024-12-08 05:40:53.903865: Current learning rate: 0.00848
2024-12-08 05:45:33.375672: Validation loss did not improve from -0.54665. Patience: 49/50
2024-12-08 05:45:33.376560: train_loss -0.7493
2024-12-08 05:45:33.377383: val_loss -0.4964
2024-12-08 05:45:33.378119: Pseudo dice [0.7213]
2024-12-08 05:45:33.378887: Epoch time: 279.48 s
2024-12-08 05:45:34.872917: 
2024-12-08 05:45:34.875935: Epoch 168
2024-12-08 05:45:34.877179: Current learning rate: 0.00847
2024-12-08 05:50:14.894341: Validation loss did not improve from -0.54665. Patience: 50/50
2024-12-08 05:50:14.895356: train_loss -0.7487
2024-12-08 05:50:14.896482: val_loss -0.5208
2024-12-08 05:50:14.897331: Pseudo dice [0.7313]
2024-12-08 05:50:14.898166: Epoch time: 280.03 s
2024-12-08 05:50:16.439192: Patience reached. Stopping training.
2024-12-08 05:50:16.917782: Training done.
2024-12-08 05:50:17.135607: Using splits from existing split file: /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset307_Sohee_Calcium_OCT_CrossValidation/splits_final.json
2024-12-08 05:50:17.137912: The split file contains 5 splits.
2024-12-08 05:50:17.138672: Desired fold for training: 4
2024-12-08 05:50:17.139286: This split has 7 training and 1 validation cases.
2024-12-08 05:50:17.140065: predicting 101-045
2024-12-08 05:50:17.219706: 101-045, shape torch.Size([1, 375, 498, 498]), rank 0
2024-12-08 05:52:26.644912: Validation complete
2024-12-08 05:52:26.645896: Mean Validation Dice:  0.7212422435751379
