/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/bin/python
Starting training with CONFIG=3d_32x160x128_b10, DATASET_ID=310, TRAINER=nnUNetTrainerScaleAnalysis40
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/nnunetv2/training/nnUNetTrainer/nnUNetTrainer.py:169: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  self.grad_scaler = GradScaler() if self.device.type == 'cuda' else None

############################
INFO: You are using the old nnU-Net default plans. We have updated our recommendations. Please consider using those instead! Read more here: https://github.com/MIC-DKFZ/nnUNet/blob/master/documentation/resenc_presets.md
############################

Using device: cuda:0

#######################################################################
Please cite the following paper when using nnU-Net:
Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.
#######################################################################

2025-10-05 12:49:42.624857: do_dummy_2d_data_aug: True
2025-10-05 12:49:42.625283: Using splits from existing split file: /nfs/erelab001/shared/Computational_Group/Naravich/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset310_nnInteractive_Calcium_OCT_CrossValidation/splits_final_40.json
2025-10-05 12:49:42.625569: The split file contains 5 splits.
2025-10-05 12:49:42.625674: Desired fold for training: 1
2025-10-05 12:49:42.625777: This split has 3 training and 6 validation cases.
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/fft_conv_pytorch/fft_conv.py:139: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:306.)
  output = output[crop_slices].contiguous()
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/fft_conv_pytorch/fft_conv.py:139: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:306.)
  output = output[crop_slices].contiguous()
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/fft_conv_pytorch/fft_conv.py:139: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:306.)
  output = output[crop_slices].contiguous()
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/fft_conv_pytorch/fft_conv.py:139: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:306.)
  output = output[crop_slices].contiguous()
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/fft_conv_pytorch/fft_conv.py:139: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:306.)
  output = output[crop_slices].contiguous()
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/fft_conv_pytorch/fft_conv.py:139: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:306.)
  output = output[crop_slices].contiguous()
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/fft_conv_pytorch/fft_conv.py:139: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:306.)
  output = output[crop_slices].contiguous()
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/fft_conv_pytorch/fft_conv.py:139: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:306.)
  output = output[crop_slices].contiguous()
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/fft_conv_pytorch/fft_conv.py:139: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:306.)
  output = output[crop_slices].contiguous()
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/fft_conv_pytorch/fft_conv.py:139: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:306.)
  output = output[crop_slices].contiguous()
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/fft_conv_pytorch/fft_conv.py:139: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:306.)
  output = output[crop_slices].contiguous()
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/fft_conv_pytorch/fft_conv.py:139: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:306.)
  output = output[crop_slices].contiguous()
using pin_memory on device 0
using pin_memory on device 0
2025-10-05 12:49:47.575756: Using torch.compile...

This is the configuration used by this training:
Configuration name: 3d_32x160x128_b10
 {'data_identifier': 'nnUNetPlans_3d_fullres', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 10, 'patch_size': [32, 160, 128], 'median_image_size_in_voxels': [375.0, 498.0, 498.0], 'spacing': [1.0, 1.0, 1.0], 'normalization_schemes': ['ZScoreNormalization'], 'use_mask_for_norm': [False], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'architecture': {'network_class_name': 'dynamic_network_architectures.architectures.unet.PlainConvUNet', 'arch_kwargs': {'n_stages': 6, 'features_per_stage': [32, 64, 128, 256, 320, 320], 'conv_op': 'torch.nn.modules.conv.Conv3d', 'kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]], 'strides': [[1, 1, 1], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [1, 2, 2]], 'n_conv_per_stage': [2, 2, 2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2, 2, 2], 'conv_bias': True, 'norm_op': 'torch.nn.modules.instancenorm.InstanceNorm3d', 'norm_op_kwargs': {'eps': 1e-05, 'affine': True}, 'dropout_op': None, 'dropout_op_kwargs': None, 'nonlin': 'torch.nn.LeakyReLU', 'nonlin_kwargs': {'inplace': True}, 'deep_supervision': True}, '_kw_requires_import': ['conv_op', 'norm_op', 'dropout_op', 'nonlin']}, 'batch_dice': True, 'inherits_from': '3d_fullres'} 

These are the global plan.json settings:
 {'dataset_name': 'Dataset310_nnInteractive_Calcium_OCT_CrossValidation', 'plans_name': 'nnUNetPlans', 'original_median_spacing_after_transp': [1.0, 1.0, 1.0], 'original_median_shape_after_transp': [375, 498, 498], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 0.4977976381778717, 'mean': 0.1394134759902954, 'median': 0.09849607944488525, 'min': 0.0, 'percentile_00_5': 0.015305490233004093, 'percentile_99_5': 0.4977976381778717, 'std': 0.121165432035923}}} 

2025-10-05 12:49:48.950362: unpacking dataset...
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/torch/_inductor/lowering.py:7095: UserWarning: 
Online softmax is disabled on the fly since Inductor decides to
split the reduction. Cut an issue to PyTorch if this is an
important use case and you want to speed it up with online
softmax.

  warnings.warn(
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/torch/_inductor/lowering.py:7095: UserWarning: 
Online softmax is disabled on the fly since Inductor decides to
split the reduction. Cut an issue to PyTorch if this is an
important use case and you want to speed it up with online
softmax.

  warnings.warn(
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/torch/_inductor/lowering.py:7095: UserWarning: 
Online softmax is disabled on the fly since Inductor decides to
split the reduction. Cut an issue to PyTorch if this is an
important use case and you want to speed it up with online
softmax.

  warnings.warn(
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/torch/_inductor/lowering.py:7095: UserWarning: 
Online softmax is disabled on the fly since Inductor decides to
split the reduction. Cut an issue to PyTorch if this is an
important use case and you want to speed it up with online
softmax.

  warnings.warn(
2025-10-05 12:49:52.989309: unpacking done...
2025-10-05 12:49:52.991359: Unable to plot network architecture: nnUNet_compile is enabled!
2025-10-05 12:49:52.995795: 
2025-10-05 12:49:52.995984: Epoch 0
2025-10-05 12:49:52.996212: Current learning rate: 0.01
2025-10-05 12:51:12.025844: Validation loss improved from 1000.00000 to -0.09976! Patience: 0/50
2025-10-05 12:51:12.026459: train_loss -0.1412
2025-10-05 12:51:12.026634: val_loss -0.0998
2025-10-05 12:51:12.026886: Pseudo dice [np.float32(0.5451)]
2025-10-05 12:51:12.027144: Epoch time: 79.03 s
2025-10-05 12:51:12.027391: Yayy! New best EMA pseudo Dice: 0.5450999736785889
2025-10-05 12:51:12.959516: 
2025-10-05 12:51:12.959970: Epoch 1
2025-10-05 12:51:12.960114: Current learning rate: 0.00994
2025-10-05 12:51:59.025557: Validation loss improved from -0.09976 to -0.11278! Patience: 0/50
2025-10-05 12:51:59.026108: train_loss -0.2535
2025-10-05 12:51:59.026343: val_loss -0.1128
2025-10-05 12:51:59.026569: Pseudo dice [np.float32(0.5423)]
2025-10-05 12:51:59.026771: Epoch time: 46.07 s
2025-10-05 12:51:59.645587: 
2025-10-05 12:51:59.645908: Epoch 2
2025-10-05 12:51:59.646151: Current learning rate: 0.00988
2025-10-05 12:52:45.728068: Validation loss improved from -0.11278 to -0.15062! Patience: 0/50
2025-10-05 12:52:45.728626: train_loss -0.2812
2025-10-05 12:52:45.728786: val_loss -0.1506
2025-10-05 12:52:45.728934: Pseudo dice [np.float32(0.5561)]
2025-10-05 12:52:45.729112: Epoch time: 46.08 s
2025-10-05 12:52:45.729275: Yayy! New best EMA pseudo Dice: 0.5460000038146973
2025-10-05 12:52:46.836747: 
2025-10-05 12:52:46.837041: Epoch 3
2025-10-05 12:52:46.837239: Current learning rate: 0.00982
2025-10-05 12:53:32.943766: Validation loss improved from -0.15062 to -0.16789! Patience: 0/50
2025-10-05 12:53:32.944325: train_loss -0.3242
2025-10-05 12:53:32.944579: val_loss -0.1679
2025-10-05 12:53:32.944816: Pseudo dice [np.float32(0.5472)]
2025-10-05 12:53:32.945049: Epoch time: 46.11 s
2025-10-05 12:53:32.945256: Yayy! New best EMA pseudo Dice: 0.5461000204086304
2025-10-05 12:53:34.059111: 
2025-10-05 12:53:34.059337: Epoch 4
2025-10-05 12:53:34.059532: Current learning rate: 0.00976
2025-10-05 12:54:20.150441: Validation loss improved from -0.16789 to -0.26462! Patience: 0/50
2025-10-05 12:54:20.150957: train_loss -0.3693
2025-10-05 12:54:20.151100: val_loss -0.2646
2025-10-05 12:54:20.151227: Pseudo dice [np.float32(0.6104)]
2025-10-05 12:54:20.151358: Epoch time: 46.09 s
2025-10-05 12:54:20.560335: Yayy! New best EMA pseudo Dice: 0.5525000095367432
2025-10-05 12:54:21.632064: 
2025-10-05 12:54:21.632327: Epoch 5
2025-10-05 12:54:21.632533: Current learning rate: 0.0097
2025-10-05 12:55:07.685424: Validation loss improved from -0.26462 to -0.26534! Patience: 0/50
2025-10-05 12:55:07.685930: train_loss -0.4182
2025-10-05 12:55:07.686287: val_loss -0.2653
2025-10-05 12:55:07.686486: Pseudo dice [np.float32(0.6009)]
2025-10-05 12:55:07.686750: Epoch time: 46.05 s
2025-10-05 12:55:07.686996: Yayy! New best EMA pseudo Dice: 0.5573999881744385
2025-10-05 12:55:08.860810: 
2025-10-05 12:55:08.861260: Epoch 6
2025-10-05 12:55:08.861566: Current learning rate: 0.00964
2025-10-05 12:55:54.944699: Validation loss did not improve from -0.26534. Patience: 1/50
2025-10-05 12:55:54.945601: train_loss -0.4454
2025-10-05 12:55:54.945872: val_loss -0.2343
2025-10-05 12:55:54.946073: Pseudo dice [np.float32(0.6034)]
2025-10-05 12:55:54.946311: Epoch time: 46.09 s
2025-10-05 12:55:54.946520: Yayy! New best EMA pseudo Dice: 0.5619999766349792
2025-10-05 12:55:56.045672: 
2025-10-05 12:55:56.046021: Epoch 7
2025-10-05 12:55:56.046248: Current learning rate: 0.00958
2025-10-05 12:56:42.069693: Validation loss improved from -0.26534 to -0.29747! Patience: 1/50
2025-10-05 12:56:42.070136: train_loss -0.4742
2025-10-05 12:56:42.070336: val_loss -0.2975
2025-10-05 12:56:42.070528: Pseudo dice [np.float32(0.6453)]
2025-10-05 12:56:42.070823: Epoch time: 46.03 s
2025-10-05 12:56:42.071073: Yayy! New best EMA pseudo Dice: 0.5702999830245972
2025-10-05 12:56:43.143844: 
2025-10-05 12:56:43.144149: Epoch 8
2025-10-05 12:56:43.144410: Current learning rate: 0.00952
2025-10-05 12:57:29.217893: Validation loss did not improve from -0.29747. Patience: 1/50
2025-10-05 12:57:29.218631: train_loss -0.4978
2025-10-05 12:57:29.218868: val_loss -0.2726
2025-10-05 12:57:29.219029: Pseudo dice [np.float32(0.612)]
2025-10-05 12:57:29.219221: Epoch time: 46.08 s
2025-10-05 12:57:29.219397: Yayy! New best EMA pseudo Dice: 0.5745000243186951
2025-10-05 12:57:30.319331: 
2025-10-05 12:57:30.319635: Epoch 9
2025-10-05 12:57:30.319841: Current learning rate: 0.00946
2025-10-05 12:58:16.436807: Validation loss improved from -0.29747 to -0.30528! Patience: 1/50
2025-10-05 12:58:16.437245: train_loss -0.51
2025-10-05 12:58:16.437415: val_loss -0.3053
2025-10-05 12:58:16.437548: Pseudo dice [np.float32(0.631)]
2025-10-05 12:58:16.437713: Epoch time: 46.12 s
2025-10-05 12:58:16.991907: Yayy! New best EMA pseudo Dice: 0.5800999999046326
2025-10-05 12:58:18.132327: 
2025-10-05 12:58:18.132583: Epoch 10
2025-10-05 12:58:18.132759: Current learning rate: 0.0094
2025-10-05 12:59:04.181227: Validation loss did not improve from -0.30528. Patience: 1/50
2025-10-05 12:59:04.182040: train_loss -0.5371
2025-10-05 12:59:04.182324: val_loss -0.3019
2025-10-05 12:59:04.182560: Pseudo dice [np.float32(0.6222)]
2025-10-05 12:59:04.182819: Epoch time: 46.05 s
2025-10-05 12:59:04.183074: Yayy! New best EMA pseudo Dice: 0.5842999815940857
2025-10-05 12:59:05.245328: 
2025-10-05 12:59:05.245616: Epoch 11
2025-10-05 12:59:05.245807: Current learning rate: 0.00934
2025-10-05 12:59:51.285789: Validation loss did not improve from -0.30528. Patience: 2/50
2025-10-05 12:59:51.286197: train_loss -0.5566
2025-10-05 12:59:51.286411: val_loss -0.175
2025-10-05 12:59:51.286558: Pseudo dice [np.float32(0.5659)]
2025-10-05 12:59:51.286734: Epoch time: 46.04 s
2025-10-05 12:59:52.211380: 
2025-10-05 12:59:52.211609: Epoch 12
2025-10-05 12:59:52.211830: Current learning rate: 0.00928
2025-10-05 13:00:38.267913: Validation loss did not improve from -0.30528. Patience: 3/50
2025-10-05 13:00:38.268852: train_loss -0.5632
2025-10-05 13:00:38.269763: val_loss -0.2839
2025-10-05 13:00:38.270598: Pseudo dice [np.float32(0.626)]
2025-10-05 13:00:38.270912: Epoch time: 46.06 s
2025-10-05 13:00:38.271148: Yayy! New best EMA pseudo Dice: 0.5867999792098999
2025-10-05 13:00:39.359210: 
2025-10-05 13:00:39.359506: Epoch 13
2025-10-05 13:00:39.359743: Current learning rate: 0.00922
2025-10-05 13:01:25.447785: Validation loss did not improve from -0.30528. Patience: 4/50
2025-10-05 13:01:25.448184: train_loss -0.5815
2025-10-05 13:01:25.448547: val_loss -0.2408
2025-10-05 13:01:25.448707: Pseudo dice [np.float32(0.611)]
2025-10-05 13:01:25.448882: Epoch time: 46.09 s
2025-10-05 13:01:25.449013: Yayy! New best EMA pseudo Dice: 0.5892999768257141
2025-10-05 13:01:26.503652: 
2025-10-05 13:01:26.503998: Epoch 14
2025-10-05 13:01:26.504180: Current learning rate: 0.00916
2025-10-05 13:02:12.523653: Validation loss did not improve from -0.30528. Patience: 5/50
2025-10-05 13:02:12.524204: train_loss -0.6007
2025-10-05 13:02:12.524634: val_loss -0.2863
2025-10-05 13:02:12.524826: Pseudo dice [np.float32(0.647)]
2025-10-05 13:02:12.525022: Epoch time: 46.02 s
2025-10-05 13:02:12.955848: Yayy! New best EMA pseudo Dice: 0.5950000286102295
2025-10-05 13:02:14.083065: 
2025-10-05 13:02:14.083346: Epoch 15
2025-10-05 13:02:14.083575: Current learning rate: 0.0091
2025-10-05 13:03:00.158020: Validation loss did not improve from -0.30528. Patience: 6/50
2025-10-05 13:03:00.158474: train_loss -0.6039
2025-10-05 13:03:00.158741: val_loss -0.3006
2025-10-05 13:03:00.159019: Pseudo dice [np.float32(0.6387)]
2025-10-05 13:03:00.159230: Epoch time: 46.08 s
2025-10-05 13:03:00.159524: Yayy! New best EMA pseudo Dice: 0.599399983882904
2025-10-05 13:03:01.326977: 
2025-10-05 13:03:01.327373: Epoch 16
2025-10-05 13:03:01.327650: Current learning rate: 0.00903
2025-10-05 13:03:47.467345: Validation loss did not improve from -0.30528. Patience: 7/50
2025-10-05 13:03:47.467919: train_loss -0.6111
2025-10-05 13:03:47.468103: val_loss -0.251
2025-10-05 13:03:47.468295: Pseudo dice [np.float32(0.6292)]
2025-10-05 13:03:47.468477: Epoch time: 46.14 s
2025-10-05 13:03:47.468664: Yayy! New best EMA pseudo Dice: 0.602400004863739
2025-10-05 13:03:48.542207: 
2025-10-05 13:03:48.542500: Epoch 17
2025-10-05 13:03:48.542766: Current learning rate: 0.00897
2025-10-05 13:04:34.613322: Validation loss did not improve from -0.30528. Patience: 8/50
2025-10-05 13:04:34.613747: train_loss -0.641
2025-10-05 13:04:34.613907: val_loss -0.2949
2025-10-05 13:04:34.614074: Pseudo dice [np.float32(0.6469)]
2025-10-05 13:04:34.614240: Epoch time: 46.07 s
2025-10-05 13:04:34.614387: Yayy! New best EMA pseudo Dice: 0.6068000197410583
2025-10-05 13:04:35.712126: 
2025-10-05 13:04:35.712404: Epoch 18
2025-10-05 13:04:35.712574: Current learning rate: 0.00891
2025-10-05 13:05:21.815484: Validation loss improved from -0.30528 to -0.32229! Patience: 8/50
2025-10-05 13:05:21.816431: train_loss -0.6384
2025-10-05 13:05:21.816745: val_loss -0.3223
2025-10-05 13:05:21.816998: Pseudo dice [np.float32(0.6627)]
2025-10-05 13:05:21.817182: Epoch time: 46.1 s
2025-10-05 13:05:21.817350: Yayy! New best EMA pseudo Dice: 0.6123999953269958
2025-10-05 13:05:22.910408: 
2025-10-05 13:05:22.910676: Epoch 19
2025-10-05 13:05:22.910858: Current learning rate: 0.00885
2025-10-05 13:06:09.012860: Validation loss did not improve from -0.32229. Patience: 1/50
2025-10-05 13:06:09.013295: train_loss -0.6501
2025-10-05 13:06:09.013575: val_loss -0.2365
2025-10-05 13:06:09.013783: Pseudo dice [np.float32(0.6131)]
2025-10-05 13:06:09.014012: Epoch time: 46.1 s
2025-10-05 13:06:09.459059: Yayy! New best EMA pseudo Dice: 0.612500011920929
2025-10-05 13:06:10.509816: 
2025-10-05 13:06:10.510095: Epoch 20
2025-10-05 13:06:10.510288: Current learning rate: 0.00879
2025-10-05 13:06:56.588146: Validation loss did not improve from -0.32229. Patience: 2/50
2025-10-05 13:06:56.589071: train_loss -0.6602
2025-10-05 13:06:56.589287: val_loss -0.2653
2025-10-05 13:06:56.589486: Pseudo dice [np.float32(0.6205)]
2025-10-05 13:06:56.589705: Epoch time: 46.08 s
2025-10-05 13:06:56.589935: Yayy! New best EMA pseudo Dice: 0.6133000254631042
2025-10-05 13:06:57.776247: 
2025-10-05 13:06:57.776592: Epoch 21
2025-10-05 13:06:57.776831: Current learning rate: 0.00873
2025-10-05 13:07:43.889739: Validation loss did not improve from -0.32229. Patience: 3/50
2025-10-05 13:07:43.890167: train_loss -0.6716
2025-10-05 13:07:43.890353: val_loss -0.2821
2025-10-05 13:07:43.890525: Pseudo dice [np.float32(0.6484)]
2025-10-05 13:07:43.890753: Epoch time: 46.11 s
2025-10-05 13:07:43.890917: Yayy! New best EMA pseudo Dice: 0.6168000102043152
2025-10-05 13:07:45.034857: 
2025-10-05 13:07:45.035327: Epoch 22
2025-10-05 13:07:45.035658: Current learning rate: 0.00867
2025-10-05 13:08:31.155136: Validation loss did not improve from -0.32229. Patience: 4/50
2025-10-05 13:08:31.155732: train_loss -0.6809
2025-10-05 13:08:31.155936: val_loss -0.2542
2025-10-05 13:08:31.156103: Pseudo dice [np.float32(0.6248)]
2025-10-05 13:08:31.156278: Epoch time: 46.12 s
2025-10-05 13:08:31.156426: Yayy! New best EMA pseudo Dice: 0.6176000237464905
2025-10-05 13:08:32.211893: 
2025-10-05 13:08:32.212204: Epoch 23
2025-10-05 13:08:32.212398: Current learning rate: 0.00861
2025-10-05 13:09:18.320132: Validation loss did not improve from -0.32229. Patience: 5/50
2025-10-05 13:09:18.320610: train_loss -0.6863
2025-10-05 13:09:18.320874: val_loss -0.1824
2025-10-05 13:09:18.321078: Pseudo dice [np.float32(0.5822)]
2025-10-05 13:09:18.321289: Epoch time: 46.11 s
2025-10-05 13:09:18.937713: 
2025-10-05 13:09:18.937999: Epoch 24
2025-10-05 13:09:18.938197: Current learning rate: 0.00855
2025-10-05 13:10:05.105621: Validation loss did not improve from -0.32229. Patience: 6/50
2025-10-05 13:10:05.106184: train_loss -0.6949
2025-10-05 13:10:05.106358: val_loss -0.2285
2025-10-05 13:10:05.106545: Pseudo dice [np.float32(0.6064)]
2025-10-05 13:10:05.106729: Epoch time: 46.17 s
2025-10-05 13:10:06.170395: 
2025-10-05 13:10:06.170731: Epoch 25
2025-10-05 13:10:06.170937: Current learning rate: 0.00849
2025-10-05 13:10:52.292799: Validation loss did not improve from -0.32229. Patience: 7/50
2025-10-05 13:10:52.293313: train_loss -0.7038
2025-10-05 13:10:52.293566: val_loss -0.272
2025-10-05 13:10:52.293732: Pseudo dice [np.float32(0.6392)]
2025-10-05 13:10:52.293903: Epoch time: 46.12 s
2025-10-05 13:10:52.914168: 
2025-10-05 13:10:52.914462: Epoch 26
2025-10-05 13:10:52.914696: Current learning rate: 0.00843
2025-10-05 13:11:39.023538: Validation loss did not improve from -0.32229. Patience: 8/50
2025-10-05 13:11:39.024143: train_loss -0.7172
2025-10-05 13:11:39.024287: val_loss -0.2155
2025-10-05 13:11:39.024411: Pseudo dice [np.float32(0.5968)]
2025-10-05 13:11:39.024577: Epoch time: 46.11 s
2025-10-05 13:11:39.644599: 
2025-10-05 13:11:39.644938: Epoch 27
2025-10-05 13:11:39.645092: Current learning rate: 0.00836
2025-10-05 13:12:25.779677: Validation loss did not improve from -0.32229. Patience: 9/50
2025-10-05 13:12:25.780217: train_loss -0.7212
2025-10-05 13:12:25.780513: val_loss -0.1778
2025-10-05 13:12:25.780732: Pseudo dice [np.float32(0.5906)]
2025-10-05 13:12:25.780934: Epoch time: 46.14 s
2025-10-05 13:12:26.743705: 
2025-10-05 13:12:26.743964: Epoch 28
2025-10-05 13:12:26.744141: Current learning rate: 0.0083
2025-10-05 13:13:12.824454: Validation loss did not improve from -0.32229. Patience: 10/50
2025-10-05 13:13:12.825170: train_loss -0.7399
2025-10-05 13:13:12.825420: val_loss -0.2571
2025-10-05 13:13:12.825571: Pseudo dice [np.float32(0.6355)]
2025-10-05 13:13:12.825714: Epoch time: 46.08 s
2025-10-05 13:13:13.451398: 
2025-10-05 13:13:13.451648: Epoch 29
2025-10-05 13:13:13.451900: Current learning rate: 0.00824
2025-10-05 13:13:59.565580: Validation loss did not improve from -0.32229. Patience: 11/50
2025-10-05 13:13:59.566005: train_loss -0.7232
2025-10-05 13:13:59.566203: val_loss -0.1828
2025-10-05 13:13:59.566403: Pseudo dice [np.float32(0.5823)]
2025-10-05 13:13:59.566674: Epoch time: 46.12 s
2025-10-05 13:14:00.761632: 
2025-10-05 13:14:00.761931: Epoch 30
2025-10-05 13:14:00.762219: Current learning rate: 0.00818
2025-10-05 13:14:46.890815: Validation loss did not improve from -0.32229. Patience: 12/50
2025-10-05 13:14:46.891585: train_loss -0.7464
2025-10-05 13:14:46.891820: val_loss -0.2515
2025-10-05 13:14:46.892016: Pseudo dice [np.float32(0.6463)]
2025-10-05 13:14:46.892260: Epoch time: 46.13 s
2025-10-05 13:14:47.523993: 
2025-10-05 13:14:47.524292: Epoch 31
2025-10-05 13:14:47.524484: Current learning rate: 0.00812
2025-10-05 13:15:33.621499: Validation loss did not improve from -0.32229. Patience: 13/50
2025-10-05 13:15:33.622047: train_loss -0.7454
2025-10-05 13:15:33.622310: val_loss -0.2651
2025-10-05 13:15:33.622622: Pseudo dice [np.float32(0.6544)]
2025-10-05 13:15:33.622980: Epoch time: 46.1 s
2025-10-05 13:15:33.623220: Yayy! New best EMA pseudo Dice: 0.618399977684021
2025-10-05 13:15:34.785710: 
2025-10-05 13:15:34.786028: Epoch 32
2025-10-05 13:15:34.786357: Current learning rate: 0.00806
2025-10-05 13:16:20.900132: Validation loss did not improve from -0.32229. Patience: 14/50
2025-10-05 13:16:20.901176: train_loss -0.7517
2025-10-05 13:16:20.901467: val_loss -0.2514
2025-10-05 13:16:20.901716: Pseudo dice [np.float32(0.6294)]
2025-10-05 13:16:20.902007: Epoch time: 46.12 s
2025-10-05 13:16:20.902204: Yayy! New best EMA pseudo Dice: 0.6194999814033508
2025-10-05 13:16:22.051313: 
2025-10-05 13:16:22.051661: Epoch 33
2025-10-05 13:16:22.051926: Current learning rate: 0.008
2025-10-05 13:17:08.198627: Validation loss did not improve from -0.32229. Patience: 15/50
2025-10-05 13:17:08.199112: train_loss -0.7606
2025-10-05 13:17:08.199423: val_loss -0.2559
2025-10-05 13:17:08.199651: Pseudo dice [np.float32(0.6213)]
2025-10-05 13:17:08.200011: Epoch time: 46.15 s
2025-10-05 13:17:08.200326: Yayy! New best EMA pseudo Dice: 0.619700014591217
2025-10-05 13:17:09.263656: 
2025-10-05 13:17:09.264016: Epoch 34
2025-10-05 13:17:09.264357: Current learning rate: 0.00793
2025-10-05 13:17:55.303697: Validation loss did not improve from -0.32229. Patience: 16/50
2025-10-05 13:17:55.304445: train_loss -0.767
2025-10-05 13:17:55.304626: val_loss -0.2031
2025-10-05 13:17:55.304790: Pseudo dice [np.float32(0.6139)]
2025-10-05 13:17:55.304937: Epoch time: 46.04 s
2025-10-05 13:17:56.359656: 
2025-10-05 13:17:56.360003: Epoch 35
2025-10-05 13:17:56.360216: Current learning rate: 0.00787
2025-10-05 13:18:42.409722: Validation loss did not improve from -0.32229. Patience: 17/50
2025-10-05 13:18:42.410146: train_loss -0.7671
2025-10-05 13:18:42.410348: val_loss -0.2382
2025-10-05 13:18:42.410495: Pseudo dice [np.float32(0.6367)]
2025-10-05 13:18:42.410643: Epoch time: 46.05 s
2025-10-05 13:18:42.410785: Yayy! New best EMA pseudo Dice: 0.6208999752998352
2025-10-05 13:18:43.477459: 
2025-10-05 13:18:43.477715: Epoch 36
2025-10-05 13:18:43.477868: Current learning rate: 0.00781
2025-10-05 13:19:29.571276: Validation loss did not improve from -0.32229. Patience: 18/50
2025-10-05 13:19:29.572056: train_loss -0.7681
2025-10-05 13:19:29.572306: val_loss -0.3116
2025-10-05 13:19:29.572535: Pseudo dice [np.float32(0.6561)]
2025-10-05 13:19:29.572753: Epoch time: 46.1 s
2025-10-05 13:19:29.572893: Yayy! New best EMA pseudo Dice: 0.6244000196456909
2025-10-05 13:19:30.756452: 
2025-10-05 13:19:30.756738: Epoch 37
2025-10-05 13:19:30.756983: Current learning rate: 0.00775
2025-10-05 13:20:16.790813: Validation loss did not improve from -0.32229. Patience: 19/50
2025-10-05 13:20:16.791355: train_loss -0.7808
2025-10-05 13:20:16.791669: val_loss -0.223
2025-10-05 13:20:16.791906: Pseudo dice [np.float32(0.6399)]
2025-10-05 13:20:16.792147: Epoch time: 46.04 s
2025-10-05 13:20:16.792437: Yayy! New best EMA pseudo Dice: 0.6258999705314636
2025-10-05 13:20:17.914317: 
2025-10-05 13:20:17.914767: Epoch 38
2025-10-05 13:20:17.915095: Current learning rate: 0.00769
2025-10-05 13:21:03.963832: Validation loss did not improve from -0.32229. Patience: 20/50
2025-10-05 13:21:03.964404: train_loss -0.7776
2025-10-05 13:21:03.964582: val_loss -0.173
2025-10-05 13:21:03.964763: Pseudo dice [np.float32(0.6105)]
2025-10-05 13:21:03.964922: Epoch time: 46.05 s
2025-10-05 13:21:04.593048: 
2025-10-05 13:21:04.593395: Epoch 39
2025-10-05 13:21:04.593583: Current learning rate: 0.00763
2025-10-05 13:21:50.685672: Validation loss did not improve from -0.32229. Patience: 21/50
2025-10-05 13:21:50.686110: train_loss -0.7937
2025-10-05 13:21:50.686288: val_loss -0.191
2025-10-05 13:21:50.686423: Pseudo dice [np.float32(0.6071)]
2025-10-05 13:21:50.686577: Epoch time: 46.09 s
2025-10-05 13:21:51.762926: 
2025-10-05 13:21:51.763204: Epoch 40
2025-10-05 13:21:51.763471: Current learning rate: 0.00756
2025-10-05 13:22:37.829973: Validation loss did not improve from -0.32229. Patience: 22/50
2025-10-05 13:22:37.830651: train_loss -0.7958
2025-10-05 13:22:37.830827: val_loss -0.2306
2025-10-05 13:22:37.830970: Pseudo dice [np.float32(0.653)]
2025-10-05 13:22:37.831131: Epoch time: 46.07 s
2025-10-05 13:22:38.472492: 
2025-10-05 13:22:38.472804: Epoch 41
2025-10-05 13:22:38.472974: Current learning rate: 0.0075
2025-10-05 13:23:24.508072: Validation loss did not improve from -0.32229. Patience: 23/50
2025-10-05 13:23:24.508562: train_loss -0.7934
2025-10-05 13:23:24.508864: val_loss -0.098
2025-10-05 13:23:24.509146: Pseudo dice [np.float32(0.5756)]
2025-10-05 13:23:24.509411: Epoch time: 46.04 s
2025-10-05 13:23:25.133570: 
2025-10-05 13:23:25.133852: Epoch 42
2025-10-05 13:23:25.134020: Current learning rate: 0.00744
2025-10-05 13:24:11.222134: Validation loss did not improve from -0.32229. Patience: 24/50
2025-10-05 13:24:11.222747: train_loss -0.7949
2025-10-05 13:24:11.222991: val_loss -0.1589
2025-10-05 13:24:11.223139: Pseudo dice [np.float32(0.5996)]
2025-10-05 13:24:11.223294: Epoch time: 46.09 s
2025-10-05 13:24:12.174354: 
2025-10-05 13:24:12.174587: Epoch 43
2025-10-05 13:24:12.174771: Current learning rate: 0.00738
2025-10-05 13:24:58.304038: Validation loss did not improve from -0.32229. Patience: 25/50
2025-10-05 13:24:58.304666: train_loss -0.8047
2025-10-05 13:24:58.304995: val_loss -0.1972
2025-10-05 13:24:58.305262: Pseudo dice [np.float32(0.6212)]
2025-10-05 13:24:58.305554: Epoch time: 46.13 s
2025-10-05 13:24:58.932462: 
2025-10-05 13:24:58.932817: Epoch 44
2025-10-05 13:24:58.933065: Current learning rate: 0.00732
2025-10-05 13:25:45.033427: Validation loss did not improve from -0.32229. Patience: 26/50
2025-10-05 13:25:45.034092: train_loss -0.8059
2025-10-05 13:25:45.034273: val_loss -0.2428
2025-10-05 13:25:45.034402: Pseudo dice [np.float32(0.6248)]
2025-10-05 13:25:45.034536: Epoch time: 46.1 s
2025-10-05 13:25:46.122996: 
2025-10-05 13:25:46.123396: Epoch 45
2025-10-05 13:25:46.123662: Current learning rate: 0.00725
2025-10-05 13:26:32.228778: Validation loss did not improve from -0.32229. Patience: 27/50
2025-10-05 13:26:32.229165: train_loss -0.811
2025-10-05 13:26:32.229355: val_loss -0.0786
2025-10-05 13:26:32.229506: Pseudo dice [np.float32(0.5629)]
2025-10-05 13:26:32.229748: Epoch time: 46.11 s
2025-10-05 13:26:32.850167: 
2025-10-05 13:26:32.850526: Epoch 46
2025-10-05 13:26:32.850707: Current learning rate: 0.00719
2025-10-05 13:27:18.905810: Validation loss did not improve from -0.32229. Patience: 28/50
2025-10-05 13:27:18.906329: train_loss -0.8106
2025-10-05 13:27:18.906507: val_loss -0.1631
2025-10-05 13:27:18.906644: Pseudo dice [np.float32(0.6282)]
2025-10-05 13:27:18.906798: Epoch time: 46.06 s
2025-10-05 13:27:19.524523: 
2025-10-05 13:27:19.524874: Epoch 47
2025-10-05 13:27:19.525084: Current learning rate: 0.00713
2025-10-05 13:28:05.644739: Validation loss did not improve from -0.32229. Patience: 29/50
2025-10-05 13:28:05.645262: train_loss -0.817
2025-10-05 13:28:05.645565: val_loss -0.1924
2025-10-05 13:28:05.645829: Pseudo dice [np.float32(0.6291)]
2025-10-05 13:28:05.646034: Epoch time: 46.12 s
2025-10-05 13:28:06.280358: 
2025-10-05 13:28:06.280679: Epoch 48
2025-10-05 13:28:06.280868: Current learning rate: 0.00707
2025-10-05 13:28:52.385846: Validation loss did not improve from -0.32229. Patience: 30/50
2025-10-05 13:28:52.386530: train_loss -0.8212
2025-10-05 13:28:52.386805: val_loss -0.1153
2025-10-05 13:28:52.387095: Pseudo dice [np.float32(0.5895)]
2025-10-05 13:28:52.387316: Epoch time: 46.11 s
2025-10-05 13:28:53.018862: 
2025-10-05 13:28:53.019115: Epoch 49
2025-10-05 13:28:53.019310: Current learning rate: 0.007
2025-10-05 13:29:39.108836: Validation loss did not improve from -0.32229. Patience: 31/50
2025-10-05 13:29:39.109361: train_loss -0.8187
2025-10-05 13:29:39.109608: val_loss -0.1998
2025-10-05 13:29:39.109813: Pseudo dice [np.float32(0.6261)]
2025-10-05 13:29:39.110105: Epoch time: 46.09 s
2025-10-05 13:29:40.186811: 
2025-10-05 13:29:40.187144: Epoch 50
2025-10-05 13:29:40.187309: Current learning rate: 0.00694
2025-10-05 13:30:26.289348: Validation loss did not improve from -0.32229. Patience: 32/50
2025-10-05 13:30:26.290065: train_loss -0.8215
2025-10-05 13:30:26.290266: val_loss -0.1394
2025-10-05 13:30:26.290457: Pseudo dice [np.float32(0.6073)]
2025-10-05 13:30:26.290625: Epoch time: 46.1 s
2025-10-05 13:30:26.921771: 
2025-10-05 13:30:26.922042: Epoch 51
2025-10-05 13:30:26.922200: Current learning rate: 0.00688
2025-10-05 13:31:13.100007: Validation loss did not improve from -0.32229. Patience: 33/50
2025-10-05 13:31:13.100435: train_loss -0.8294
2025-10-05 13:31:13.100605: val_loss -0.1679
2025-10-05 13:31:13.100763: Pseudo dice [np.float32(0.6172)]
2025-10-05 13:31:13.100938: Epoch time: 46.18 s
2025-10-05 13:31:13.731471: 
2025-10-05 13:31:13.731809: Epoch 52
2025-10-05 13:31:13.732011: Current learning rate: 0.00682
2025-10-05 13:31:59.812336: Validation loss did not improve from -0.32229. Patience: 34/50
2025-10-05 13:31:59.812984: train_loss -0.83
2025-10-05 13:31:59.813180: val_loss -0.1566
2025-10-05 13:31:59.813376: Pseudo dice [np.float32(0.6276)]
2025-10-05 13:31:59.813572: Epoch time: 46.08 s
2025-10-05 13:32:00.447562: 
2025-10-05 13:32:00.447843: Epoch 53
2025-10-05 13:32:00.448102: Current learning rate: 0.00675
2025-10-05 13:32:46.528948: Validation loss did not improve from -0.32229. Patience: 35/50
2025-10-05 13:32:46.529449: train_loss -0.8342
2025-10-05 13:32:46.529759: val_loss -0.1719
2025-10-05 13:32:46.530009: Pseudo dice [np.float32(0.6291)]
2025-10-05 13:32:46.530245: Epoch time: 46.08 s
2025-10-05 13:32:47.161807: 
2025-10-05 13:32:47.162147: Epoch 54
2025-10-05 13:32:47.162383: Current learning rate: 0.00669
2025-10-05 13:33:33.215854: Validation loss did not improve from -0.32229. Patience: 36/50
2025-10-05 13:33:33.216478: train_loss -0.8287
2025-10-05 13:33:33.216642: val_loss -0.1984
2025-10-05 13:33:33.216772: Pseudo dice [np.float32(0.6449)]
2025-10-05 13:33:33.216919: Epoch time: 46.06 s
2025-10-05 13:33:34.303991: 
2025-10-05 13:33:34.304231: Epoch 55
2025-10-05 13:33:34.304405: Current learning rate: 0.00663
2025-10-05 13:34:20.348350: Validation loss did not improve from -0.32229. Patience: 37/50
2025-10-05 13:34:20.348964: train_loss -0.8319
2025-10-05 13:34:20.349261: val_loss -0.081
2025-10-05 13:34:20.349475: Pseudo dice [np.float32(0.567)]
2025-10-05 13:34:20.349696: Epoch time: 46.05 s
2025-10-05 13:34:20.992113: 
2025-10-05 13:34:20.992362: Epoch 56
2025-10-05 13:34:20.992536: Current learning rate: 0.00657
2025-10-05 13:35:07.051037: Validation loss did not improve from -0.32229. Patience: 38/50
2025-10-05 13:35:07.051651: train_loss -0.8345
2025-10-05 13:35:07.051837: val_loss -0.1601
2025-10-05 13:35:07.052008: Pseudo dice [np.float32(0.6206)]
2025-10-05 13:35:07.052154: Epoch time: 46.06 s
2025-10-05 13:35:07.687201: 
2025-10-05 13:35:07.687494: Epoch 57
2025-10-05 13:35:07.687655: Current learning rate: 0.0065
2025-10-05 13:35:53.878154: Validation loss did not improve from -0.32229. Patience: 39/50
2025-10-05 13:35:53.878604: train_loss -0.843
2025-10-05 13:35:53.878791: val_loss -0.1216
2025-10-05 13:35:53.878941: Pseudo dice [np.float32(0.6008)]
2025-10-05 13:35:53.879195: Epoch time: 46.19 s
2025-10-05 13:35:54.509038: 
2025-10-05 13:35:54.509365: Epoch 58
2025-10-05 13:35:54.509539: Current learning rate: 0.00644
2025-10-05 13:36:40.673620: Validation loss did not improve from -0.32229. Patience: 40/50
2025-10-05 13:36:40.674374: train_loss -0.8449
2025-10-05 13:36:40.674601: val_loss -0.1888
2025-10-05 13:36:40.674761: Pseudo dice [np.float32(0.6415)]
2025-10-05 13:36:40.674914: Epoch time: 46.17 s
2025-10-05 13:36:41.660612: 
2025-10-05 13:36:41.660974: Epoch 59
2025-10-05 13:36:41.661160: Current learning rate: 0.00638
2025-10-05 13:37:27.727913: Validation loss did not improve from -0.32229. Patience: 41/50
2025-10-05 13:37:27.728540: train_loss -0.8423
2025-10-05 13:37:27.728905: val_loss -0.1393
2025-10-05 13:37:27.729298: Pseudo dice [np.float32(0.6145)]
2025-10-05 13:37:27.729527: Epoch time: 46.07 s
2025-10-05 13:37:28.971621: 
2025-10-05 13:37:28.971963: Epoch 60
2025-10-05 13:37:28.972239: Current learning rate: 0.00631
2025-10-05 13:38:15.036566: Validation loss did not improve from -0.32229. Patience: 42/50
2025-10-05 13:38:15.037399: train_loss -0.8445
2025-10-05 13:38:15.037660: val_loss -0.1437
2025-10-05 13:38:15.037883: Pseudo dice [np.float32(0.6213)]
2025-10-05 13:38:15.038115: Epoch time: 46.07 s
2025-10-05 13:38:15.677786: 
2025-10-05 13:38:15.678373: Epoch 61
2025-10-05 13:38:15.678634: Current learning rate: 0.00625
2025-10-05 13:39:01.855159: Validation loss did not improve from -0.32229. Patience: 43/50
2025-10-05 13:39:01.855658: train_loss -0.85
2025-10-05 13:39:01.855971: val_loss -0.1902
2025-10-05 13:39:01.856275: Pseudo dice [np.float32(0.6207)]
2025-10-05 13:39:01.856585: Epoch time: 46.18 s
2025-10-05 13:39:02.496314: 
2025-10-05 13:39:02.496675: Epoch 62
2025-10-05 13:39:02.496941: Current learning rate: 0.00619
2025-10-05 13:39:48.625914: Validation loss did not improve from -0.32229. Patience: 44/50
2025-10-05 13:39:48.626535: train_loss -0.8462
2025-10-05 13:39:48.626683: val_loss -0.2175
2025-10-05 13:39:48.626842: Pseudo dice [np.float32(0.6511)]
2025-10-05 13:39:48.627012: Epoch time: 46.13 s
2025-10-05 13:39:49.277528: 
2025-10-05 13:39:49.277759: Epoch 63
2025-10-05 13:39:49.277940: Current learning rate: 0.00612
2025-10-05 13:40:35.397750: Validation loss did not improve from -0.32229. Patience: 45/50
2025-10-05 13:40:35.398176: train_loss -0.8542
2025-10-05 13:40:35.398376: val_loss -0.1285
2025-10-05 13:40:35.398565: Pseudo dice [np.float32(0.6211)]
2025-10-05 13:40:35.398714: Epoch time: 46.12 s
2025-10-05 13:40:36.042422: 
2025-10-05 13:40:36.042732: Epoch 64
2025-10-05 13:40:36.042898: Current learning rate: 0.00606
2025-10-05 13:41:22.185539: Validation loss did not improve from -0.32229. Patience: 46/50
2025-10-05 13:41:22.186160: train_loss -0.8586
2025-10-05 13:41:22.186334: val_loss -0.1111
2025-10-05 13:41:22.186478: Pseudo dice [np.float32(0.5943)]
2025-10-05 13:41:22.186620: Epoch time: 46.14 s
2025-10-05 13:41:23.286304: 
2025-10-05 13:41:23.286614: Epoch 65
2025-10-05 13:41:23.286781: Current learning rate: 0.006
2025-10-05 13:42:09.397359: Validation loss did not improve from -0.32229. Patience: 47/50
2025-10-05 13:42:09.397773: train_loss -0.8534
2025-10-05 13:42:09.397931: val_loss -0.1108
2025-10-05 13:42:09.398136: Pseudo dice [np.float32(0.6156)]
2025-10-05 13:42:09.398282: Epoch time: 46.11 s
2025-10-05 13:42:10.040321: 
2025-10-05 13:42:10.040631: Epoch 66
2025-10-05 13:42:10.040816: Current learning rate: 0.00593
2025-10-05 13:42:56.137594: Validation loss did not improve from -0.32229. Patience: 48/50
2025-10-05 13:42:56.138178: train_loss -0.8598
2025-10-05 13:42:56.138332: val_loss -0.1484
2025-10-05 13:42:56.138512: Pseudo dice [np.float32(0.6257)]
2025-10-05 13:42:56.138679: Epoch time: 46.1 s
2025-10-05 13:42:56.782591: 
2025-10-05 13:42:56.782917: Epoch 67
2025-10-05 13:42:56.783121: Current learning rate: 0.00587
2025-10-05 13:43:42.883162: Validation loss did not improve from -0.32229. Patience: 49/50
2025-10-05 13:43:42.883701: train_loss -0.8612
2025-10-05 13:43:42.883928: val_loss -0.1446
2025-10-05 13:43:42.884121: Pseudo dice [np.float32(0.6352)]
2025-10-05 13:43:42.884280: Epoch time: 46.1 s
2025-10-05 13:43:43.529589: 
2025-10-05 13:43:43.529848: Epoch 68
2025-10-05 13:43:43.530032: Current learning rate: 0.00581
2025-10-05 13:44:29.654409: Validation loss did not improve from -0.32229. Patience: 50/50
2025-10-05 13:44:29.655206: train_loss -0.8645
2025-10-05 13:44:29.655402: val_loss -0.1815
2025-10-05 13:44:29.655610: Pseudo dice [np.float32(0.6467)]
2025-10-05 13:44:29.655838: Epoch time: 46.13 s
2025-10-05 13:44:30.291894: 
2025-10-05 13:44:30.292177: Epoch 69
2025-10-05 13:44:30.292372: Current learning rate: 0.00574
2025-10-05 13:45:16.505788: Validation loss did not improve from -0.32229. Patience: 51/50
2025-10-05 13:45:16.506224: train_loss -0.8676
2025-10-05 13:45:16.506402: val_loss -0.1375
2025-10-05 13:45:16.506548: Pseudo dice [np.float32(0.6195)]
2025-10-05 13:45:16.506786: Epoch time: 46.21 s
2025-10-05 13:45:17.597130: 
2025-10-05 13:45:17.597551: Epoch 70
2025-10-05 13:45:17.597786: Current learning rate: 0.00568
2025-10-05 13:46:03.849737: Validation loss did not improve from -0.32229. Patience: 52/50
2025-10-05 13:46:03.850419: train_loss -0.8692
2025-10-05 13:46:03.850572: val_loss -0.1587
2025-10-05 13:46:03.850719: Pseudo dice [np.float32(0.6244)]
2025-10-05 13:46:03.850870: Epoch time: 46.25 s
2025-10-05 13:46:04.487622: 
2025-10-05 13:46:04.487844: Epoch 71
2025-10-05 13:46:04.488006: Current learning rate: 0.00562
2025-10-05 13:46:50.552293: Validation loss did not improve from -0.32229. Patience: 53/50
2025-10-05 13:46:50.552748: train_loss -0.8682
2025-10-05 13:46:50.552939: val_loss -0.1175
2025-10-05 13:46:50.553118: Pseudo dice [np.float32(0.6076)]
2025-10-05 13:46:50.553289: Epoch time: 46.07 s
2025-10-05 13:46:51.204177: 
2025-10-05 13:46:51.204463: Epoch 72
2025-10-05 13:46:51.204653: Current learning rate: 0.00555
2025-10-05 13:47:37.307253: Validation loss did not improve from -0.32229. Patience: 54/50
2025-10-05 13:47:37.308045: train_loss -0.8654
2025-10-05 13:47:37.308256: val_loss -0.1676
2025-10-05 13:47:37.308440: Pseudo dice [np.float32(0.6187)]
2025-10-05 13:47:37.308673: Epoch time: 46.1 s
2025-10-05 13:47:37.947157: 
2025-10-05 13:47:37.947399: Epoch 73
2025-10-05 13:47:37.947556: Current learning rate: 0.00549
2025-10-05 13:48:24.010947: Validation loss did not improve from -0.32229. Patience: 55/50
2025-10-05 13:48:24.011539: train_loss -0.874
2025-10-05 13:48:24.011812: val_loss -0.1737
2025-10-05 13:48:24.011987: Pseudo dice [np.float32(0.6383)]
2025-10-05 13:48:24.012228: Epoch time: 46.07 s
2025-10-05 13:48:24.994047: 
2025-10-05 13:48:24.994353: Epoch 74
2025-10-05 13:48:24.994610: Current learning rate: 0.00542
2025-10-05 13:49:11.121603: Validation loss did not improve from -0.32229. Patience: 56/50
2025-10-05 13:49:11.122453: train_loss -0.8744
2025-10-05 13:49:11.122758: val_loss -0.1499
2025-10-05 13:49:11.123000: Pseudo dice [np.float32(0.6167)]
2025-10-05 13:49:11.123271: Epoch time: 46.13 s
2025-10-05 13:49:12.471487: 
2025-10-05 13:49:12.471830: Epoch 75
2025-10-05 13:49:12.472133: Current learning rate: 0.00536
2025-10-05 13:49:58.566951: Validation loss did not improve from -0.32229. Patience: 57/50
2025-10-05 13:49:58.567562: train_loss -0.8726
2025-10-05 13:49:58.567911: val_loss -0.087
2025-10-05 13:49:58.568196: Pseudo dice [np.float32(0.6027)]
2025-10-05 13:49:58.568453: Epoch time: 46.1 s
2025-10-05 13:49:59.226995: 
2025-10-05 13:49:59.227422: Epoch 76
2025-10-05 13:49:59.227718: Current learning rate: 0.00529
2025-10-05 13:50:45.308336: Validation loss did not improve from -0.32229. Patience: 58/50
2025-10-05 13:50:45.309340: train_loss -0.8759
2025-10-05 13:50:45.309649: val_loss -0.1608
2025-10-05 13:50:45.309853: Pseudo dice [np.float32(0.6451)]
2025-10-05 13:50:45.310109: Epoch time: 46.08 s
2025-10-05 13:50:45.958192: 
2025-10-05 13:50:45.958498: Epoch 77
2025-10-05 13:50:45.958699: Current learning rate: 0.00523
2025-10-05 13:51:32.081505: Validation loss did not improve from -0.32229. Patience: 59/50
2025-10-05 13:51:32.081928: train_loss -0.8776
2025-10-05 13:51:32.082142: val_loss -0.1303
2025-10-05 13:51:32.082359: Pseudo dice [np.float32(0.6159)]
2025-10-05 13:51:32.082523: Epoch time: 46.12 s
2025-10-05 13:51:32.727408: 
2025-10-05 13:51:32.727777: Epoch 78
2025-10-05 13:51:32.728028: Current learning rate: 0.00517
2025-10-05 13:52:18.820486: Validation loss did not improve from -0.32229. Patience: 60/50
2025-10-05 13:52:18.821110: train_loss -0.8744
2025-10-05 13:52:18.821266: val_loss -0.0762
2025-10-05 13:52:18.821405: Pseudo dice [np.float32(0.5894)]
2025-10-05 13:52:18.821563: Epoch time: 46.09 s
2025-10-05 13:52:19.468703: 
2025-10-05 13:52:19.468989: Epoch 79
2025-10-05 13:52:19.469163: Current learning rate: 0.0051
2025-10-05 13:53:05.582177: Validation loss did not improve from -0.32229. Patience: 61/50
2025-10-05 13:53:05.582843: train_loss -0.88
2025-10-05 13:53:05.583021: val_loss -0.1588
2025-10-05 13:53:05.583165: Pseudo dice [np.float32(0.6401)]
2025-10-05 13:53:05.583334: Epoch time: 46.11 s
2025-10-05 13:53:06.714909: 
2025-10-05 13:53:06.715161: Epoch 80
2025-10-05 13:53:06.715322: Current learning rate: 0.00504
2025-10-05 13:53:52.825940: Validation loss did not improve from -0.32229. Patience: 62/50
2025-10-05 13:53:52.826756: train_loss -0.8844
2025-10-05 13:53:52.826977: val_loss -0.1001
2025-10-05 13:53:52.827182: Pseudo dice [np.float32(0.635)]
2025-10-05 13:53:52.827411: Epoch time: 46.11 s
2025-10-05 13:53:53.472488: 
2025-10-05 13:53:53.472807: Epoch 81
2025-10-05 13:53:53.473033: Current learning rate: 0.00497
2025-10-05 13:54:39.580549: Validation loss did not improve from -0.32229. Patience: 63/50
2025-10-05 13:54:39.580976: train_loss -0.8836
2025-10-05 13:54:39.581167: val_loss -0.1091
2025-10-05 13:54:39.581316: Pseudo dice [np.float32(0.6224)]
2025-10-05 13:54:39.581524: Epoch time: 46.11 s
2025-10-05 13:54:40.232664: 
2025-10-05 13:54:40.233004: Epoch 82
2025-10-05 13:54:40.233202: Current learning rate: 0.00491
2025-10-05 13:55:26.286685: Validation loss did not improve from -0.32229. Patience: 64/50
2025-10-05 13:55:26.287407: train_loss -0.8848
2025-10-05 13:55:26.287651: val_loss -0.1438
2025-10-05 13:55:26.287845: Pseudo dice [np.float32(0.6235)]
2025-10-05 13:55:26.288033: Epoch time: 46.06 s
2025-10-05 13:55:26.914602: 
2025-10-05 13:55:26.914913: Epoch 83
2025-10-05 13:55:26.915130: Current learning rate: 0.00484
2025-10-05 13:56:13.007850: Validation loss did not improve from -0.32229. Patience: 65/50
2025-10-05 13:56:13.008276: train_loss -0.8844
2025-10-05 13:56:13.008456: val_loss -0.1088
2025-10-05 13:56:13.008610: Pseudo dice [np.float32(0.6133)]
2025-10-05 13:56:13.008765: Epoch time: 46.09 s
2025-10-05 13:56:13.635926: 
2025-10-05 13:56:13.636249: Epoch 84
2025-10-05 13:56:13.636407: Current learning rate: 0.00478
2025-10-05 13:56:59.753532: Validation loss did not improve from -0.32229. Patience: 66/50
2025-10-05 13:56:59.754409: train_loss -0.885
2025-10-05 13:56:59.754668: val_loss -0.1751
2025-10-05 13:56:59.754882: Pseudo dice [np.float32(0.6447)]
2025-10-05 13:56:59.755086: Epoch time: 46.12 s
2025-10-05 13:57:00.849718: 
2025-10-05 13:57:00.850025: Epoch 85
2025-10-05 13:57:00.850187: Current learning rate: 0.00471
2025-10-05 13:57:46.933936: Validation loss did not improve from -0.32229. Patience: 67/50
2025-10-05 13:57:46.934364: train_loss -0.8877
2025-10-05 13:57:46.934594: val_loss -0.096
2025-10-05 13:57:46.934722: Pseudo dice [np.float32(0.6177)]
2025-10-05 13:57:46.934871: Epoch time: 46.09 s
2025-10-05 13:57:47.561311: 
2025-10-05 13:57:47.561661: Epoch 86
2025-10-05 13:57:47.561855: Current learning rate: 0.00465
2025-10-05 13:58:33.652121: Validation loss did not improve from -0.32229. Patience: 68/50
2025-10-05 13:58:33.652739: train_loss -0.879
2025-10-05 13:58:33.652898: val_loss -0.1425
2025-10-05 13:58:33.653087: Pseudo dice [np.float32(0.6146)]
2025-10-05 13:58:33.653250: Epoch time: 46.09 s
2025-10-05 13:58:34.279155: 
2025-10-05 13:58:34.279402: Epoch 87
2025-10-05 13:58:34.279564: Current learning rate: 0.00458
2025-10-05 13:59:20.393650: Validation loss did not improve from -0.32229. Patience: 69/50
2025-10-05 13:59:20.394071: train_loss -0.8834
2025-10-05 13:59:20.394348: val_loss -0.1019
2025-10-05 13:59:20.394505: Pseudo dice [np.float32(0.6224)]
2025-10-05 13:59:20.394668: Epoch time: 46.12 s
2025-10-05 13:59:21.020561: 
2025-10-05 13:59:21.020882: Epoch 88
2025-10-05 13:59:21.021089: Current learning rate: 0.00452
2025-10-05 14:00:07.145727: Validation loss did not improve from -0.32229. Patience: 70/50
2025-10-05 14:00:07.146607: train_loss -0.8844
2025-10-05 14:00:07.146853: val_loss -0.0849
2025-10-05 14:00:07.147077: Pseudo dice [np.float32(0.597)]
2025-10-05 14:00:07.147310: Epoch time: 46.13 s
2025-10-05 14:00:08.114448: 
2025-10-05 14:00:08.114762: Epoch 89
2025-10-05 14:00:08.115003: Current learning rate: 0.00445
2025-10-05 14:00:54.247177: Validation loss did not improve from -0.32229. Patience: 71/50
2025-10-05 14:00:54.247749: train_loss -0.8872
2025-10-05 14:00:54.247972: val_loss -0.0812
2025-10-05 14:00:54.248136: Pseudo dice [np.float32(0.6299)]
2025-10-05 14:00:54.248346: Epoch time: 46.13 s
2025-10-05 14:00:55.448533: 
2025-10-05 14:00:55.448989: Epoch 90
2025-10-05 14:00:55.449256: Current learning rate: 0.00438
2025-10-05 14:01:41.556091: Validation loss did not improve from -0.32229. Patience: 72/50
2025-10-05 14:01:41.557022: train_loss -0.8948
2025-10-05 14:01:41.557412: val_loss -0.1127
2025-10-05 14:01:41.557672: Pseudo dice [np.float32(0.6333)]
2025-10-05 14:01:41.557925: Epoch time: 46.11 s
2025-10-05 14:01:42.177331: 
2025-10-05 14:01:42.177571: Epoch 91
2025-10-05 14:01:42.177771: Current learning rate: 0.00432
2025-10-05 14:02:28.321836: Validation loss did not improve from -0.32229. Patience: 73/50
2025-10-05 14:02:28.322409: train_loss -0.8938
2025-10-05 14:02:28.322655: val_loss -0.1105
2025-10-05 14:02:28.322887: Pseudo dice [np.float32(0.6185)]
2025-10-05 14:02:28.323145: Epoch time: 46.15 s
2025-10-05 14:02:28.943426: 
2025-10-05 14:02:28.943725: Epoch 92
2025-10-05 14:02:28.943918: Current learning rate: 0.00425
2025-10-05 14:03:15.083208: Validation loss did not improve from -0.32229. Patience: 74/50
2025-10-05 14:03:15.084132: train_loss -0.8946
2025-10-05 14:03:15.084456: val_loss -0.1179
2025-10-05 14:03:15.084640: Pseudo dice [np.float32(0.6474)]
2025-10-05 14:03:15.084855: Epoch time: 46.14 s
2025-10-05 14:03:15.709484: 
2025-10-05 14:03:15.709877: Epoch 93
2025-10-05 14:03:15.710147: Current learning rate: 0.00419
2025-10-05 14:04:01.848518: Validation loss did not improve from -0.32229. Patience: 75/50
2025-10-05 14:04:01.848956: train_loss -0.8955
2025-10-05 14:04:01.849127: val_loss -0.135
2025-10-05 14:04:01.849289: Pseudo dice [np.float32(0.6279)]
2025-10-05 14:04:01.849478: Epoch time: 46.14 s
2025-10-05 14:04:02.476036: 
2025-10-05 14:04:02.476318: Epoch 94
2025-10-05 14:04:02.476470: Current learning rate: 0.00412
2025-10-05 14:04:48.606018: Validation loss did not improve from -0.32229. Patience: 76/50
2025-10-05 14:04:48.606724: train_loss -0.8987
2025-10-05 14:04:48.606876: val_loss -0.1401
2025-10-05 14:04:48.607002: Pseudo dice [np.float32(0.6341)]
2025-10-05 14:04:48.607168: Epoch time: 46.13 s
2025-10-05 14:04:49.688310: 
2025-10-05 14:04:49.688577: Epoch 95
2025-10-05 14:04:49.688755: Current learning rate: 0.00405
2025-10-05 14:05:35.811577: Validation loss did not improve from -0.32229. Patience: 77/50
2025-10-05 14:05:35.812025: train_loss -0.8942
2025-10-05 14:05:35.812180: val_loss -0.0724
2025-10-05 14:05:35.812336: Pseudo dice [np.float32(0.6055)]
2025-10-05 14:05:35.812486: Epoch time: 46.12 s
2025-10-05 14:05:36.436571: 
2025-10-05 14:05:36.436905: Epoch 96
2025-10-05 14:05:36.437065: Current learning rate: 0.00399
2025-10-05 14:06:22.586756: Validation loss did not improve from -0.32229. Patience: 78/50
2025-10-05 14:06:22.587387: train_loss -0.8965
2025-10-05 14:06:22.587554: val_loss -0.0605
2025-10-05 14:06:22.587709: Pseudo dice [np.float32(0.6114)]
2025-10-05 14:06:22.587853: Epoch time: 46.15 s
2025-10-05 14:06:23.221529: 
2025-10-05 14:06:23.221970: Epoch 97
2025-10-05 14:06:23.222175: Current learning rate: 0.00392
2025-10-05 14:07:09.330654: Validation loss did not improve from -0.32229. Patience: 79/50
2025-10-05 14:07:09.331087: train_loss -0.898
2025-10-05 14:07:09.331254: val_loss -0.0447
2025-10-05 14:07:09.331417: Pseudo dice [np.float32(0.6078)]
2025-10-05 14:07:09.331559: Epoch time: 46.11 s
2025-10-05 14:07:09.961651: 
2025-10-05 14:07:09.961931: Epoch 98
2025-10-05 14:07:09.962116: Current learning rate: 0.00385
2025-10-05 14:07:56.060606: Validation loss did not improve from -0.32229. Patience: 80/50
2025-10-05 14:07:56.061499: train_loss -0.9013
2025-10-05 14:07:56.061753: val_loss -0.112
2025-10-05 14:07:56.062008: Pseudo dice [np.float32(0.6292)]
2025-10-05 14:07:56.062284: Epoch time: 46.1 s
2025-10-05 14:07:56.698272: 
2025-10-05 14:07:56.698645: Epoch 99
2025-10-05 14:07:56.698891: Current learning rate: 0.00379
2025-10-05 14:08:42.740089: Validation loss did not improve from -0.32229. Patience: 81/50
2025-10-05 14:08:42.740568: train_loss -0.9017
2025-10-05 14:08:42.740863: val_loss -0.0698
2025-10-05 14:08:42.741119: Pseudo dice [np.float32(0.6278)]
2025-10-05 14:08:42.741365: Epoch time: 46.04 s
2025-10-05 14:08:43.798720: 
2025-10-05 14:08:43.799029: Epoch 100
2025-10-05 14:08:43.799240: Current learning rate: 0.00372
2025-10-05 14:09:29.883070: Validation loss did not improve from -0.32229. Patience: 82/50
2025-10-05 14:09:29.883614: train_loss -0.8995
2025-10-05 14:09:29.883796: val_loss -0.04
2025-10-05 14:09:29.883954: Pseudo dice [np.float32(0.609)]
2025-10-05 14:09:29.884113: Epoch time: 46.09 s
2025-10-05 14:09:30.519118: 
2025-10-05 14:09:30.519389: Epoch 101
2025-10-05 14:09:30.519569: Current learning rate: 0.00365
2025-10-05 14:10:16.561732: Validation loss did not improve from -0.32229. Patience: 83/50
2025-10-05 14:10:16.562176: train_loss -0.8988
2025-10-05 14:10:16.562379: val_loss -0.089
2025-10-05 14:10:16.562516: Pseudo dice [np.float32(0.6199)]
2025-10-05 14:10:16.562663: Epoch time: 46.04 s
2025-10-05 14:10:17.194445: 
2025-10-05 14:10:17.194867: Epoch 102
2025-10-05 14:10:17.195059: Current learning rate: 0.00359
2025-10-05 14:11:03.268293: Validation loss did not improve from -0.32229. Patience: 84/50
2025-10-05 14:11:03.268917: train_loss -0.901
2025-10-05 14:11:03.269068: val_loss -0.0849
2025-10-05 14:11:03.269186: Pseudo dice [np.float32(0.6276)]
2025-10-05 14:11:03.269324: Epoch time: 46.08 s
2025-10-05 14:11:03.897040: 
2025-10-05 14:11:03.897368: Epoch 103
2025-10-05 14:11:03.897553: Current learning rate: 0.00352
2025-10-05 14:11:49.963993: Validation loss did not improve from -0.32229. Patience: 85/50
2025-10-05 14:11:49.964459: train_loss -0.9007
2025-10-05 14:11:49.964631: val_loss -0.1194
2025-10-05 14:11:49.964786: Pseudo dice [np.float32(0.6289)]
2025-10-05 14:11:49.964923: Epoch time: 46.07 s
2025-10-05 14:11:50.598072: 
2025-10-05 14:11:50.598387: Epoch 104
2025-10-05 14:11:50.598572: Current learning rate: 0.00345
2025-10-05 14:12:36.656831: Validation loss did not improve from -0.32229. Patience: 86/50
2025-10-05 14:12:36.657413: train_loss -0.9002
2025-10-05 14:12:36.657573: val_loss -0.0483
2025-10-05 14:12:36.657704: Pseudo dice [np.float32(0.623)]
2025-10-05 14:12:36.657842: Epoch time: 46.06 s
2025-10-05 14:12:38.075948: 
2025-10-05 14:12:38.076186: Epoch 105
2025-10-05 14:12:38.076344: Current learning rate: 0.00338
2025-10-05 14:13:24.177009: Validation loss did not improve from -0.32229. Patience: 87/50
2025-10-05 14:13:24.177522: train_loss -0.9024
2025-10-05 14:13:24.177763: val_loss -0.09
2025-10-05 14:13:24.177985: Pseudo dice [np.float32(0.6353)]
2025-10-05 14:13:24.178223: Epoch time: 46.1 s
2025-10-05 14:13:24.814802: 
2025-10-05 14:13:24.815056: Epoch 106
2025-10-05 14:13:24.815278: Current learning rate: 0.00332
2025-10-05 14:14:10.898804: Validation loss did not improve from -0.32229. Patience: 88/50
2025-10-05 14:14:10.899840: train_loss -0.9037
2025-10-05 14:14:10.900133: val_loss -0.0733
2025-10-05 14:14:10.900397: Pseudo dice [np.float32(0.6263)]
2025-10-05 14:14:10.900669: Epoch time: 46.09 s
2025-10-05 14:14:11.537531: 
2025-10-05 14:14:11.537845: Epoch 107
2025-10-05 14:14:11.538007: Current learning rate: 0.00325
2025-10-05 14:14:57.643702: Validation loss did not improve from -0.32229. Patience: 89/50
2025-10-05 14:14:57.644238: train_loss -0.9058
2025-10-05 14:14:57.644538: val_loss -0.0623
2025-10-05 14:14:57.644819: Pseudo dice [np.float32(0.6068)]
2025-10-05 14:14:57.645063: Epoch time: 46.11 s
2025-10-05 14:14:58.278551: 
2025-10-05 14:14:58.278841: Epoch 108
2025-10-05 14:14:58.279114: Current learning rate: 0.00318
2025-10-05 14:15:44.378613: Validation loss did not improve from -0.32229. Patience: 90/50
2025-10-05 14:15:44.379491: train_loss -0.9069
2025-10-05 14:15:44.379760: val_loss -0.0899
2025-10-05 14:15:44.379977: Pseudo dice [np.float32(0.6186)]
2025-10-05 14:15:44.380209: Epoch time: 46.1 s
2025-10-05 14:15:45.013964: 
2025-10-05 14:15:45.014361: Epoch 109
2025-10-05 14:15:45.014650: Current learning rate: 0.00311
2025-10-05 14:16:31.105453: Validation loss did not improve from -0.32229. Patience: 91/50
2025-10-05 14:16:31.105915: train_loss -0.9062
2025-10-05 14:16:31.106086: val_loss -0.147
2025-10-05 14:16:31.106217: Pseudo dice [np.float32(0.6406)]
2025-10-05 14:16:31.106369: Epoch time: 46.09 s
2025-10-05 14:16:32.186469: 
2025-10-05 14:16:32.186818: Epoch 110
2025-10-05 14:16:32.187059: Current learning rate: 0.00304
2025-10-05 14:17:18.313529: Validation loss did not improve from -0.32229. Patience: 92/50
2025-10-05 14:17:18.314414: train_loss -0.9078
2025-10-05 14:17:18.314698: val_loss -0.0513
2025-10-05 14:17:18.314931: Pseudo dice [np.float32(0.6035)]
2025-10-05 14:17:18.315185: Epoch time: 46.13 s
2025-10-05 14:17:18.948686: 
2025-10-05 14:17:18.948945: Epoch 111
2025-10-05 14:17:18.949204: Current learning rate: 0.00297
2025-10-05 14:18:05.020416: Validation loss did not improve from -0.32229. Patience: 93/50
2025-10-05 14:18:05.020894: train_loss -0.9093
2025-10-05 14:18:05.021055: val_loss -0.1125
2025-10-05 14:18:05.021183: Pseudo dice [np.float32(0.6375)]
2025-10-05 14:18:05.021330: Epoch time: 46.07 s
2025-10-05 14:18:05.656216: 
2025-10-05 14:18:05.656556: Epoch 112
2025-10-05 14:18:05.656717: Current learning rate: 0.00291
2025-10-05 14:18:51.790015: Validation loss did not improve from -0.32229. Patience: 94/50
2025-10-05 14:18:51.790618: train_loss -0.9105
2025-10-05 14:18:51.790821: val_loss -0.014
2025-10-05 14:18:51.791046: Pseudo dice [np.float32(0.5899)]
2025-10-05 14:18:51.791280: Epoch time: 46.14 s
2025-10-05 14:18:52.431287: 
2025-10-05 14:18:52.431586: Epoch 113
2025-10-05 14:18:52.431742: Current learning rate: 0.00284
2025-10-05 14:19:38.553144: Validation loss did not improve from -0.32229. Patience: 95/50
2025-10-05 14:19:38.553602: train_loss -0.9098
2025-10-05 14:19:38.553773: val_loss -0.0863
2025-10-05 14:19:38.553898: Pseudo dice [np.float32(0.6353)]
2025-10-05 14:19:38.554048: Epoch time: 46.12 s
2025-10-05 14:19:39.188435: 
2025-10-05 14:19:39.188696: Epoch 114
2025-10-05 14:19:39.188859: Current learning rate: 0.00277
2025-10-05 14:20:25.301803: Validation loss did not improve from -0.32229. Patience: 96/50
2025-10-05 14:20:25.302717: train_loss -0.9118
2025-10-05 14:20:25.303030: val_loss -0.0474
2025-10-05 14:20:25.303244: Pseudo dice [np.float32(0.6083)]
2025-10-05 14:20:25.303475: Epoch time: 46.11 s
2025-10-05 14:20:26.470646: 
2025-10-05 14:20:26.470981: Epoch 115
2025-10-05 14:20:26.471255: Current learning rate: 0.0027
2025-10-05 14:21:12.540975: Validation loss did not improve from -0.32229. Patience: 97/50
2025-10-05 14:21:12.541401: train_loss -0.9123
2025-10-05 14:21:12.541611: val_loss -0.0376
2025-10-05 14:21:12.541855: Pseudo dice [np.float32(0.6042)]
2025-10-05 14:21:12.542018: Epoch time: 46.07 s
2025-10-05 14:21:13.182178: 
2025-10-05 14:21:13.182482: Epoch 116
2025-10-05 14:21:13.182642: Current learning rate: 0.00263
2025-10-05 14:21:59.247178: Validation loss did not improve from -0.32229. Patience: 98/50
2025-10-05 14:21:59.247883: train_loss -0.9141
2025-10-05 14:21:59.248060: val_loss -0.0784
2025-10-05 14:21:59.248192: Pseudo dice [np.float32(0.6341)]
2025-10-05 14:21:59.248331: Epoch time: 46.07 s
2025-10-05 14:21:59.887283: 
2025-10-05 14:21:59.887673: Epoch 117
2025-10-05 14:21:59.887883: Current learning rate: 0.00256
2025-10-05 14:22:45.989787: Validation loss did not improve from -0.32229. Patience: 99/50
2025-10-05 14:22:45.990174: train_loss -0.9107
2025-10-05 14:22:45.990371: val_loss -0.119
2025-10-05 14:22:45.990540: Pseudo dice [np.float32(0.6504)]
2025-10-05 14:22:45.990733: Epoch time: 46.1 s
2025-10-05 14:22:46.628599: 
2025-10-05 14:22:46.628819: Epoch 118
2025-10-05 14:22:46.628969: Current learning rate: 0.00249
2025-10-05 14:23:32.698522: Validation loss did not improve from -0.32229. Patience: 100/50
2025-10-05 14:23:32.699334: train_loss -0.9139
2025-10-05 14:23:32.699571: val_loss -0.0637
2025-10-05 14:23:32.699770: Pseudo dice [np.float32(0.6218)]
2025-10-05 14:23:32.700003: Epoch time: 46.07 s
2025-10-05 14:23:33.344358: 
2025-10-05 14:23:33.344658: Epoch 119
2025-10-05 14:23:33.344888: Current learning rate: 0.00242
2025-10-05 14:24:19.393041: Validation loss did not improve from -0.32229. Patience: 101/50
2025-10-05 14:24:19.393480: train_loss -0.9138
2025-10-05 14:24:19.393676: val_loss -0.0318
2025-10-05 14:24:19.393874: Pseudo dice [np.float32(0.5888)]
2025-10-05 14:24:19.394114: Epoch time: 46.05 s
2025-10-05 14:24:20.498761: 
2025-10-05 14:24:20.499140: Epoch 120
2025-10-05 14:24:20.499321: Current learning rate: 0.00235
2025-10-05 14:25:06.569826: Validation loss did not improve from -0.32229. Patience: 102/50
2025-10-05 14:25:06.570516: train_loss -0.9123
2025-10-05 14:25:06.570800: val_loss -0.0886
2025-10-05 14:25:06.571063: Pseudo dice [np.float32(0.621)]
2025-10-05 14:25:06.571334: Epoch time: 46.07 s
2025-10-05 14:25:07.562953: 
2025-10-05 14:25:07.563297: Epoch 121
2025-10-05 14:25:07.563489: Current learning rate: 0.00228
2025-10-05 14:25:53.649386: Validation loss did not improve from -0.32229. Patience: 103/50
2025-10-05 14:25:53.649933: train_loss -0.9142
2025-10-05 14:25:53.650176: val_loss -0.1211
2025-10-05 14:25:53.650355: Pseudo dice [np.float32(0.6385)]
2025-10-05 14:25:53.650506: Epoch time: 46.09 s
2025-10-05 14:25:54.293529: 
2025-10-05 14:25:54.293864: Epoch 122
2025-10-05 14:25:54.294039: Current learning rate: 0.00221
2025-10-05 14:26:40.346129: Validation loss did not improve from -0.32229. Patience: 104/50
2025-10-05 14:26:40.346728: train_loss -0.918
2025-10-05 14:26:40.346880: val_loss 0.0036
2025-10-05 14:26:40.347026: Pseudo dice [np.float32(0.6171)]
2025-10-05 14:26:40.347225: Epoch time: 46.05 s
2025-10-05 14:26:40.989821: 
2025-10-05 14:26:40.990125: Epoch 123
2025-10-05 14:26:40.990291: Current learning rate: 0.00214
2025-10-05 14:27:27.039777: Validation loss did not improve from -0.32229. Patience: 105/50
2025-10-05 14:27:27.040360: train_loss -0.9173
2025-10-05 14:27:27.040623: val_loss -0.0872
2025-10-05 14:27:27.040830: Pseudo dice [np.float32(0.6368)]
2025-10-05 14:27:27.041011: Epoch time: 46.05 s
2025-10-05 14:27:27.684003: 
2025-10-05 14:27:27.684281: Epoch 124
2025-10-05 14:27:27.684506: Current learning rate: 0.00207
2025-10-05 14:28:13.747190: Validation loss did not improve from -0.32229. Patience: 106/50
2025-10-05 14:28:13.747833: train_loss -0.9166
2025-10-05 14:28:13.747989: val_loss -0.0549
2025-10-05 14:28:13.748119: Pseudo dice [np.float32(0.6179)]
2025-10-05 14:28:13.748254: Epoch time: 46.06 s
2025-10-05 14:28:14.867068: 
2025-10-05 14:28:14.867315: Epoch 125
2025-10-05 14:28:14.867506: Current learning rate: 0.00199
2025-10-05 14:29:00.934817: Validation loss did not improve from -0.32229. Patience: 107/50
2025-10-05 14:29:00.935280: train_loss -0.9159
2025-10-05 14:29:00.935527: val_loss -0.0306
2025-10-05 14:29:00.935665: Pseudo dice [np.float32(0.6052)]
2025-10-05 14:29:00.935854: Epoch time: 46.07 s
2025-10-05 14:29:01.604940: 
2025-10-05 14:29:01.605213: Epoch 126
2025-10-05 14:29:01.605382: Current learning rate: 0.00192
2025-10-05 14:29:47.675400: Validation loss did not improve from -0.32229. Patience: 108/50
2025-10-05 14:29:47.676148: train_loss -0.918
2025-10-05 14:29:47.676363: val_loss -0.0742
2025-10-05 14:29:47.676522: Pseudo dice [np.float32(0.6295)]
2025-10-05 14:29:47.676692: Epoch time: 46.07 s
2025-10-05 14:29:48.321608: 
2025-10-05 14:29:48.321922: Epoch 127
2025-10-05 14:29:48.322160: Current learning rate: 0.00185
2025-10-05 14:30:34.411945: Validation loss did not improve from -0.32229. Patience: 109/50
2025-10-05 14:30:34.412349: train_loss -0.9189
2025-10-05 14:30:34.412506: val_loss -0.0798
2025-10-05 14:30:34.412712: Pseudo dice [np.float32(0.641)]
2025-10-05 14:30:34.412989: Epoch time: 46.09 s
2025-10-05 14:30:35.053869: 
2025-10-05 14:30:35.054204: Epoch 128
2025-10-05 14:30:35.054466: Current learning rate: 0.00178
2025-10-05 14:31:21.167053: Validation loss did not improve from -0.32229. Patience: 110/50
2025-10-05 14:31:21.167749: train_loss -0.9185
2025-10-05 14:31:21.167948: val_loss -0.0166
2025-10-05 14:31:21.168094: Pseudo dice [np.float32(0.6105)]
2025-10-05 14:31:21.168259: Epoch time: 46.11 s
2025-10-05 14:31:21.803124: 
2025-10-05 14:31:21.803464: Epoch 129
2025-10-05 14:31:21.803656: Current learning rate: 0.0017
2025-10-05 14:32:07.875154: Validation loss did not improve from -0.32229. Patience: 111/50
2025-10-05 14:32:07.875605: train_loss -0.9188
2025-10-05 14:32:07.875803: val_loss -0.0695
2025-10-05 14:32:07.876053: Pseudo dice [np.float32(0.6229)]
2025-10-05 14:32:07.876200: Epoch time: 46.07 s
2025-10-05 14:32:08.951263: 
2025-10-05 14:32:08.951696: Epoch 130
2025-10-05 14:32:08.952006: Current learning rate: 0.00163
2025-10-05 14:32:55.052666: Validation loss did not improve from -0.32229. Patience: 112/50
2025-10-05 14:32:55.053440: train_loss -0.9196
2025-10-05 14:32:55.053664: val_loss -0.0239
2025-10-05 14:32:55.053903: Pseudo dice [np.float32(0.623)]
2025-10-05 14:32:55.054063: Epoch time: 46.1 s
2025-10-05 14:32:55.685995: 
2025-10-05 14:32:55.686424: Epoch 131
2025-10-05 14:32:55.686716: Current learning rate: 0.00156
2025-10-05 14:33:41.845086: Validation loss did not improve from -0.32229. Patience: 113/50
2025-10-05 14:33:41.845636: train_loss -0.9188
2025-10-05 14:33:41.845957: val_loss -0.086
2025-10-05 14:33:41.846264: Pseudo dice [np.float32(0.6213)]
2025-10-05 14:33:41.846544: Epoch time: 46.16 s
2025-10-05 14:33:42.481825: 
2025-10-05 14:33:42.482215: Epoch 132
2025-10-05 14:33:42.482508: Current learning rate: 0.00148
2025-10-05 14:34:28.657017: Validation loss did not improve from -0.32229. Patience: 114/50
2025-10-05 14:34:28.657730: train_loss -0.9208
2025-10-05 14:34:28.657890: val_loss -0.0668
2025-10-05 14:34:28.658084: Pseudo dice [np.float32(0.6266)]
2025-10-05 14:34:28.658239: Epoch time: 46.18 s
2025-10-05 14:34:29.294184: 
2025-10-05 14:34:29.294451: Epoch 133
2025-10-05 14:34:29.294623: Current learning rate: 0.00141
2025-10-05 14:35:15.464145: Validation loss did not improve from -0.32229. Patience: 115/50
2025-10-05 14:35:15.464558: train_loss -0.9196
2025-10-05 14:35:15.464715: val_loss -0.0299
2025-10-05 14:35:15.464916: Pseudo dice [np.float32(0.6333)]
2025-10-05 14:35:15.465060: Epoch time: 46.17 s
2025-10-05 14:35:16.097553: 
2025-10-05 14:35:16.097887: Epoch 134
2025-10-05 14:35:16.098087: Current learning rate: 0.00133
2025-10-05 14:36:02.243904: Validation loss did not improve from -0.32229. Patience: 116/50
2025-10-05 14:36:02.244856: train_loss -0.9218
2025-10-05 14:36:02.245128: val_loss -0.0674
2025-10-05 14:36:02.245331: Pseudo dice [np.float32(0.616)]
2025-10-05 14:36:02.245538: Epoch time: 46.15 s
2025-10-05 14:36:03.349702: 
2025-10-05 14:36:03.350109: Epoch 135
2025-10-05 14:36:03.350327: Current learning rate: 0.00126
2025-10-05 14:36:49.467644: Validation loss did not improve from -0.32229. Patience: 117/50
2025-10-05 14:36:49.468202: train_loss -0.9222
2025-10-05 14:36:49.468503: val_loss -0.12
2025-10-05 14:36:49.468752: Pseudo dice [np.float32(0.6414)]
2025-10-05 14:36:49.468991: Epoch time: 46.12 s
2025-10-05 14:36:50.109129: 
2025-10-05 14:36:50.109529: Epoch 136
2025-10-05 14:36:50.109798: Current learning rate: 0.00118
2025-10-05 14:37:36.241392: Validation loss did not improve from -0.32229. Patience: 118/50
2025-10-05 14:37:36.242166: train_loss -0.9217
2025-10-05 14:37:36.242397: val_loss -0.1059
2025-10-05 14:37:36.242669: Pseudo dice [np.float32(0.6346)]
2025-10-05 14:37:36.242957: Epoch time: 46.13 s
2025-10-05 14:37:37.220292: 
2025-10-05 14:37:37.220590: Epoch 137
2025-10-05 14:37:37.220856: Current learning rate: 0.00111
2025-10-05 14:38:23.328561: Validation loss did not improve from -0.32229. Patience: 119/50
2025-10-05 14:38:23.329042: train_loss -0.9217
2025-10-05 14:38:23.329244: val_loss -0.1022
2025-10-05 14:38:23.329402: Pseudo dice [np.float32(0.6327)]
2025-10-05 14:38:23.329575: Epoch time: 46.11 s
2025-10-05 14:38:23.329737: Yayy! New best EMA pseudo Dice: 0.6263999938964844
2025-10-05 14:38:24.420679: 
2025-10-05 14:38:24.421193: Epoch 138
2025-10-05 14:38:24.421459: Current learning rate: 0.00103
2025-10-05 14:39:10.516389: Validation loss did not improve from -0.32229. Patience: 120/50
2025-10-05 14:39:10.517163: train_loss -0.9233
2025-10-05 14:39:10.517333: val_loss -0.0977
2025-10-05 14:39:10.517513: Pseudo dice [np.float32(0.6285)]
2025-10-05 14:39:10.517659: Epoch time: 46.1 s
2025-10-05 14:39:10.517821: Yayy! New best EMA pseudo Dice: 0.6266000270843506
2025-10-05 14:39:11.593184: 
2025-10-05 14:39:11.593504: Epoch 139
2025-10-05 14:39:11.593703: Current learning rate: 0.00095
2025-10-05 14:39:57.701324: Validation loss did not improve from -0.32229. Patience: 121/50
2025-10-05 14:39:57.701925: train_loss -0.9225
2025-10-05 14:39:57.702257: val_loss -0.0617
2025-10-05 14:39:57.702545: Pseudo dice [np.float32(0.6249)]
2025-10-05 14:39:57.702837: Epoch time: 46.11 s
2025-10-05 14:39:58.774401: 
2025-10-05 14:39:58.774742: Epoch 140
2025-10-05 14:39:58.774979: Current learning rate: 0.00087
2025-10-05 14:40:44.937417: Validation loss did not improve from -0.32229. Patience: 122/50
2025-10-05 14:40:44.938269: train_loss -0.9236
2025-10-05 14:40:44.938553: val_loss -0.0684
2025-10-05 14:40:44.938725: Pseudo dice [np.float32(0.6351)]
2025-10-05 14:40:44.939018: Epoch time: 46.16 s
2025-10-05 14:40:44.939327: Yayy! New best EMA pseudo Dice: 0.6273000240325928
2025-10-05 14:40:46.027211: 
2025-10-05 14:40:46.027566: Epoch 141
2025-10-05 14:40:46.027850: Current learning rate: 0.00079
2025-10-05 14:41:32.161877: Validation loss did not improve from -0.32229. Patience: 123/50
2025-10-05 14:41:32.162229: train_loss -0.9247
2025-10-05 14:41:32.162399: val_loss -0.0673
2025-10-05 14:41:32.162578: Pseudo dice [np.float32(0.636)]
2025-10-05 14:41:32.162759: Epoch time: 46.14 s
2025-10-05 14:41:32.162885: Yayy! New best EMA pseudo Dice: 0.6281999945640564
2025-10-05 14:41:33.249833: 
2025-10-05 14:41:33.250198: Epoch 142
2025-10-05 14:41:33.250361: Current learning rate: 0.00071
2025-10-05 14:42:19.477928: Validation loss did not improve from -0.32229. Patience: 124/50
2025-10-05 14:42:19.478587: train_loss -0.9239
2025-10-05 14:42:19.478745: val_loss -0.0077
2025-10-05 14:42:19.478872: Pseudo dice [np.float32(0.6011)]
2025-10-05 14:42:19.479028: Epoch time: 46.23 s
2025-10-05 14:42:20.125356: 
2025-10-05 14:42:20.125622: Epoch 143
2025-10-05 14:42:20.125823: Current learning rate: 0.00063
2025-10-05 14:43:06.383871: Validation loss did not improve from -0.32229. Patience: 125/50
2025-10-05 14:43:06.384460: train_loss -0.9247
2025-10-05 14:43:06.384847: val_loss -0.0697
2025-10-05 14:43:06.385169: Pseudo dice [np.float32(0.6194)]
2025-10-05 14:43:06.385441: Epoch time: 46.26 s
2025-10-05 14:43:07.041134: 
2025-10-05 14:43:07.041415: Epoch 144
2025-10-05 14:43:07.041617: Current learning rate: 0.00055
2025-10-05 14:43:53.123963: Validation loss did not improve from -0.32229. Patience: 126/50
2025-10-05 14:43:53.124879: train_loss -0.9248
2025-10-05 14:43:53.125285: val_loss -0.0457
2025-10-05 14:43:53.125582: Pseudo dice [np.float32(0.6255)]
2025-10-05 14:43:53.125828: Epoch time: 46.08 s
2025-10-05 14:43:54.231089: 
2025-10-05 14:43:54.231387: Epoch 145
2025-10-05 14:43:54.231598: Current learning rate: 0.00047
2025-10-05 14:44:40.267418: Validation loss did not improve from -0.32229. Patience: 127/50
2025-10-05 14:44:40.267819: train_loss -0.9249
2025-10-05 14:44:40.268053: val_loss -0.0491
2025-10-05 14:44:40.268243: Pseudo dice [np.float32(0.6267)]
2025-10-05 14:44:40.268421: Epoch time: 46.04 s
2025-10-05 14:44:40.911623: 
2025-10-05 14:44:40.912005: Epoch 146
2025-10-05 14:44:40.912321: Current learning rate: 0.00038
2025-10-05 14:45:26.904322: Validation loss did not improve from -0.32229. Patience: 128/50
2025-10-05 14:45:26.905128: train_loss -0.9244
2025-10-05 14:45:26.905494: val_loss -0.0292
2025-10-05 14:45:26.905767: Pseudo dice [np.float32(0.6007)]
2025-10-05 14:45:26.905997: Epoch time: 45.99 s
2025-10-05 14:45:27.559561: 
2025-10-05 14:45:27.559833: Epoch 147
2025-10-05 14:45:27.560051: Current learning rate: 0.0003
2025-10-05 14:46:13.599300: Validation loss did not improve from -0.32229. Patience: 129/50
2025-10-05 14:46:13.599783: train_loss -0.9254
2025-10-05 14:46:13.599980: val_loss -0.053
2025-10-05 14:46:13.600190: Pseudo dice [np.float32(0.6175)]
2025-10-05 14:46:13.600368: Epoch time: 46.04 s
2025-10-05 14:46:14.249460: 
2025-10-05 14:46:14.249821: Epoch 148
2025-10-05 14:46:14.250032: Current learning rate: 0.00021
2025-10-05 14:47:00.331234: Validation loss did not improve from -0.32229. Patience: 130/50
2025-10-05 14:47:00.331840: train_loss -0.9248
2025-10-05 14:47:00.332061: val_loss -0.069
2025-10-05 14:47:00.332187: Pseudo dice [np.float32(0.6303)]
2025-10-05 14:47:00.332503: Epoch time: 46.08 s
2025-10-05 14:47:00.974804: 
2025-10-05 14:47:00.975118: Epoch 149
2025-10-05 14:47:00.975318: Current learning rate: 0.00011
2025-10-05 14:47:47.056880: Validation loss did not improve from -0.32229. Patience: 131/50
2025-10-05 14:47:47.057339: train_loss -0.9258
2025-10-05 14:47:47.057559: val_loss -0.0294
2025-10-05 14:47:47.057719: Pseudo dice [np.float32(0.621)]
2025-10-05 14:47:47.057886: Epoch time: 46.08 s
2025-10-05 14:47:48.180664: Training done.
2025-10-05 14:47:48.190792: Using splits from existing split file: /nfs/erelab001/shared/Computational_Group/Naravich/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset310_nnInteractive_Calcium_OCT_CrossValidation/splits_final_40.json
2025-10-05 14:47:48.191102: The split file contains 5 splits.
2025-10-05 14:47:48.191227: Desired fold for training: 1
2025-10-05 14:47:48.191351: This split has 3 training and 6 validation cases.
2025-10-05 14:47:48.191544: predicting 101-019
2025-10-05 14:47:48.193712: 101-019, shape torch.Size([1, 375, 498, 498]), rank 0
2025-10-05 14:48:35.384220: predicting 101-044
2025-10-05 14:48:35.398206: 101-044, shape torch.Size([1, 404, 498, 498]), rank 0
2025-10-05 14:49:12.202850: predicting 101-045
2025-10-05 14:49:12.217170: 101-045, shape torch.Size([1, 375, 498, 498]), rank 0
2025-10-05 14:49:46.446256: predicting 106-002
2025-10-05 14:49:46.463280: 106-002, shape torch.Size([1, 539, 498, 498]), rank 0
2025-10-05 14:50:34.929056: predicting 704-003
2025-10-05 14:50:34.946527: 704-003, shape torch.Size([1, 375, 498, 498]), rank 0
2025-10-05 14:51:09.196574: predicting 706-005
2025-10-05 14:51:09.212751: 706-005, shape torch.Size([1, 375, 498, 498]), rank 0
2025-10-05 14:51:56.738604: Validation complete
2025-10-05 14:51:56.738849: Mean Validation Dice:  0.6100059191723872
Finished training fold 1 saving to /nfs/erelab001/shared/Computational_Group/Naravich/datasets/nnUNet_Datasets/nnUNet_results/Dataset310_nnInteractive_Calcium_OCT_CrossValidation/nnUNetTrainerScaleAnalysis40__nnUNetPlans__3d_32x160x128_b10/fold_1_No_Pretrained
