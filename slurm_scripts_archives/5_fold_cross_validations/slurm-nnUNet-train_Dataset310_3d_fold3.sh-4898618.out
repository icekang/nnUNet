/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/bin/python
Starting training with CONFIG=3d_32x160x128_b10, DATASET_ID=310, TRAINER=nnUNetTrainer
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/nnunetv2/training/nnUNetTrainer/nnUNetTrainer.py:169: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  self.grad_scaler = GradScaler() if self.device.type == 'cuda' else None

############################
INFO: You are using the old nnU-Net default plans. We have updated our recommendations. Please consider using those instead! Read more here: https://github.com/MIC-DKFZ/nnUNet/blob/master/documentation/resenc_presets.md
############################

Using device: cuda:0

#######################################################################
Please cite the following paper when using nnU-Net:
Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.
#######################################################################

2025-10-02 18:53:11.390988: do_dummy_2d_data_aug: True
2025-10-02 18:53:11.391945: Using splits from existing split file: /nfs/erelab001/shared/Computational_Group/Naravich/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset310_nnInteractive_Calcium_OCT_CrossValidation/splits_final.json
2025-10-02 18:53:11.392402: The split file contains 5 splits.
2025-10-02 18:53:11.392519: Desired fold for training: 3
2025-10-02 18:53:11.392613: This split has 7 training and 1 validation cases.
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/fft_conv_pytorch/fft_conv.py:139: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:306.)
  output = output[crop_slices].contiguous()
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/fft_conv_pytorch/fft_conv.py:139: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:306.)
  output = output[crop_slices].contiguous()
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/fft_conv_pytorch/fft_conv.py:139: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:306.)
  output = output[crop_slices].contiguous()
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/fft_conv_pytorch/fft_conv.py:139: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:306.)
  output = output[crop_slices].contiguous()
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/fft_conv_pytorch/fft_conv.py:139: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:306.)
  output = output[crop_slices].contiguous()
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/fft_conv_pytorch/fft_conv.py:139: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:306.)
  output = output[crop_slices].contiguous()
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/fft_conv_pytorch/fft_conv.py:139: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:306.)
  output = output[crop_slices].contiguous()
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/fft_conv_pytorch/fft_conv.py:139: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:306.)
  output = output[crop_slices].contiguous()
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/fft_conv_pytorch/fft_conv.py:139: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:306.)
  output = output[crop_slices].contiguous()
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/fft_conv_pytorch/fft_conv.py:139: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:306.)
  output = output[crop_slices].contiguous()
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/fft_conv_pytorch/fft_conv.py:139: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:306.)
  output = output[crop_slices].contiguous()
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/fft_conv_pytorch/fft_conv.py:139: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:306.)
  output = output[crop_slices].contiguous()
using pin_memory on device 0
using pin_memory on device 0
2025-10-02 18:53:17.566912: Using torch.compile...

This is the configuration used by this training:
Configuration name: 3d_32x160x128_b10
 {'data_identifier': 'nnUNetPlans_3d_fullres', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 10, 'patch_size': [32, 160, 128], 'median_image_size_in_voxels': [375.0, 498.0, 498.0], 'spacing': [1.0, 1.0, 1.0], 'normalization_schemes': ['ZScoreNormalization'], 'use_mask_for_norm': [False], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'architecture': {'network_class_name': 'dynamic_network_architectures.architectures.unet.PlainConvUNet', 'arch_kwargs': {'n_stages': 6, 'features_per_stage': [32, 64, 128, 256, 320, 320], 'conv_op': 'torch.nn.modules.conv.Conv3d', 'kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]], 'strides': [[1, 1, 1], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [1, 2, 2]], 'n_conv_per_stage': [2, 2, 2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2, 2, 2], 'conv_bias': True, 'norm_op': 'torch.nn.modules.instancenorm.InstanceNorm3d', 'norm_op_kwargs': {'eps': 1e-05, 'affine': True}, 'dropout_op': None, 'dropout_op_kwargs': None, 'nonlin': 'torch.nn.LeakyReLU', 'nonlin_kwargs': {'inplace': True}, 'deep_supervision': True}, '_kw_requires_import': ['conv_op', 'norm_op', 'dropout_op', 'nonlin']}, 'batch_dice': True, 'inherits_from': '3d_fullres'} 

These are the global plan.json settings:
 {'dataset_name': 'Dataset310_nnInteractive_Calcium_OCT_CrossValidation', 'plans_name': 'nnUNetPlans', 'original_median_spacing_after_transp': [1.0, 1.0, 1.0], 'original_median_shape_after_transp': [375, 498, 498], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 0.4977976381778717, 'mean': 0.1394134759902954, 'median': 0.09849607944488525, 'min': 0.0, 'percentile_00_5': 0.015305490233004093, 'percentile_99_5': 0.4977976381778717, 'std': 0.121165432035923}}} 

2025-10-02 18:53:19.276841: unpacking dataset...
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/torch/_inductor/lowering.py:7095: UserWarning: 
Online softmax is disabled on the fly since Inductor decides to
split the reduction. Cut an issue to PyTorch if this is an
important use case and you want to speed it up with online
softmax.

  warnings.warn(
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/torch/_inductor/lowering.py:7095: UserWarning: 
Online softmax is disabled on the fly since Inductor decides to
split the reduction. Cut an issue to PyTorch if this is an
important use case and you want to speed it up with online
softmax.

  warnings.warn(
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/torch/_inductor/lowering.py:7095: UserWarning: 
Online softmax is disabled on the fly since Inductor decides to
split the reduction. Cut an issue to PyTorch if this is an
important use case and you want to speed it up with online
softmax.

  warnings.warn(
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/torch/_inductor/lowering.py:7095: UserWarning: 
Online softmax is disabled on the fly since Inductor decides to
split the reduction. Cut an issue to PyTorch if this is an
important use case and you want to speed it up with online
softmax.

  warnings.warn(
2025-10-02 18:53:23.553882: unpacking done...
2025-10-02 18:53:23.565653: Unable to plot network architecture: nnUNet_compile is enabled!
2025-10-02 18:53:23.572095: 
2025-10-02 18:53:23.572261: Epoch 0
2025-10-02 18:53:23.572473: Current learning rate: 0.01
2025-10-02 18:54:42.786397: Validation loss improved from 1000.00000 to -0.16536! Patience: 0/50
2025-10-02 18:54:42.786953: train_loss -0.1248
2025-10-02 18:54:42.787161: val_loss -0.1654
2025-10-02 18:54:42.787297: Pseudo dice [np.float32(0.5578)]
2025-10-02 18:54:42.787493: Epoch time: 79.22 s
2025-10-02 18:54:42.787628: Yayy! New best EMA pseudo Dice: 0.5577999949455261
2025-10-02 18:54:43.778435: 
2025-10-02 18:54:43.778862: Epoch 1
2025-10-02 18:54:43.779187: Current learning rate: 0.00994
2025-10-02 18:55:29.565676: Validation loss improved from -0.16536 to -0.19396! Patience: 0/50
2025-10-02 18:55:29.566394: train_loss -0.246
2025-10-02 18:55:29.566724: val_loss -0.194
2025-10-02 18:55:29.567022: Pseudo dice [np.float32(0.554)]
2025-10-02 18:55:29.567357: Epoch time: 45.79 s
2025-10-02 18:55:30.188924: 
2025-10-02 18:55:30.195243: Epoch 2
2025-10-02 18:55:30.201941: Current learning rate: 0.00988
2025-10-02 18:56:16.059330: Validation loss did not improve from -0.19396. Patience: 1/50
2025-10-02 18:56:16.059840: train_loss -0.2801
2025-10-02 18:56:16.060215: val_loss -0.1883
2025-10-02 18:56:16.060462: Pseudo dice [np.float32(0.5803)]
2025-10-02 18:56:16.060638: Epoch time: 45.87 s
2025-10-02 18:56:16.060857: Yayy! New best EMA pseudo Dice: 0.5597000122070312
2025-10-02 18:56:17.127608: 
2025-10-02 18:56:17.127854: Epoch 3
2025-10-02 18:56:17.128009: Current learning rate: 0.00982
2025-10-02 18:57:02.989350: Validation loss improved from -0.19396 to -0.24663! Patience: 1/50
2025-10-02 18:57:02.989982: train_loss -0.3135
2025-10-02 18:57:02.990281: val_loss -0.2466
2025-10-02 18:57:02.990592: Pseudo dice [np.float32(0.6035)]
2025-10-02 18:57:02.990820: Epoch time: 45.86 s
2025-10-02 18:57:02.991037: Yayy! New best EMA pseudo Dice: 0.5641000270843506
2025-10-02 18:57:04.051644: 
2025-10-02 18:57:04.051893: Epoch 4
2025-10-02 18:57:04.052053: Current learning rate: 0.00976
2025-10-02 18:57:49.944898: Validation loss did not improve from -0.24663. Patience: 1/50
2025-10-02 18:57:49.945869: train_loss -0.3616
2025-10-02 18:57:49.946161: val_loss -0.2098
2025-10-02 18:57:49.946468: Pseudo dice [np.float32(0.5701)]
2025-10-02 18:57:49.946804: Epoch time: 45.89 s
2025-10-02 18:57:50.307859: Yayy! New best EMA pseudo Dice: 0.5647000074386597
2025-10-02 18:57:51.329006: 
2025-10-02 18:57:51.329366: Epoch 5
2025-10-02 18:57:51.329556: Current learning rate: 0.0097
2025-10-02 18:58:37.197711: Validation loss did not improve from -0.24663. Patience: 2/50
2025-10-02 18:58:37.198118: train_loss -0.3867
2025-10-02 18:58:37.198304: val_loss -0.2315
2025-10-02 18:58:37.198462: Pseudo dice [np.float32(0.5828)]
2025-10-02 18:58:37.198702: Epoch time: 45.87 s
2025-10-02 18:58:37.198909: Yayy! New best EMA pseudo Dice: 0.5665000081062317
2025-10-02 18:58:38.253474: 
2025-10-02 18:58:38.253715: Epoch 6
2025-10-02 18:58:38.253875: Current learning rate: 0.00964
2025-10-02 18:59:24.082477: Validation loss did not improve from -0.24663. Patience: 3/50
2025-10-02 18:59:24.083435: train_loss -0.403
2025-10-02 18:59:24.083776: val_loss -0.223
2025-10-02 18:59:24.084068: Pseudo dice [np.float32(0.5567)]
2025-10-02 18:59:24.084326: Epoch time: 45.83 s
2025-10-02 18:59:24.701277: 
2025-10-02 18:59:24.701478: Epoch 7
2025-10-02 18:59:24.701614: Current learning rate: 0.00958
2025-10-02 19:00:10.604727: Validation loss improved from -0.24663 to -0.31827! Patience: 3/50
2025-10-02 19:00:10.605250: train_loss -0.4195
2025-10-02 19:00:10.605580: val_loss -0.3183
2025-10-02 19:00:10.605721: Pseudo dice [np.float32(0.6207)]
2025-10-02 19:00:10.605889: Epoch time: 45.9 s
2025-10-02 19:00:10.606035: Yayy! New best EMA pseudo Dice: 0.5709999799728394
2025-10-02 19:00:11.674408: 
2025-10-02 19:00:11.674833: Epoch 8
2025-10-02 19:00:11.675215: Current learning rate: 0.00952
2025-10-02 19:00:57.604019: Validation loss did not improve from -0.31827. Patience: 1/50
2025-10-02 19:00:57.604784: train_loss -0.444
2025-10-02 19:00:57.605026: val_loss -0.2575
2025-10-02 19:00:57.605193: Pseudo dice [np.float32(0.5923)]
2025-10-02 19:00:57.605401: Epoch time: 45.93 s
2025-10-02 19:00:57.605591: Yayy! New best EMA pseudo Dice: 0.573199987411499
2025-10-02 19:00:58.651616: 
2025-10-02 19:00:58.652110: Epoch 9
2025-10-02 19:00:58.652528: Current learning rate: 0.00946
2025-10-02 19:01:44.581844: Validation loss did not improve from -0.31827. Patience: 2/50
2025-10-02 19:01:44.582351: train_loss -0.4534
2025-10-02 19:01:44.582550: val_loss -0.1832
2025-10-02 19:01:44.582695: Pseudo dice [np.float32(0.5263)]
2025-10-02 19:01:44.582814: Epoch time: 45.93 s
2025-10-02 19:01:45.625858: 
2025-10-02 19:01:45.626359: Epoch 10
2025-10-02 19:01:45.626722: Current learning rate: 0.0094
2025-10-02 19:02:31.495185: Validation loss did not improve from -0.31827. Patience: 3/50
2025-10-02 19:02:31.495899: train_loss -0.4558
2025-10-02 19:02:31.496055: val_loss -0.273
2025-10-02 19:02:31.496270: Pseudo dice [np.float32(0.6153)]
2025-10-02 19:02:31.496466: Epoch time: 45.87 s
2025-10-02 19:02:32.116445: 
2025-10-02 19:02:32.116735: Epoch 11
2025-10-02 19:02:32.116871: Current learning rate: 0.00934
2025-10-02 19:03:18.034935: Validation loss did not improve from -0.31827. Patience: 4/50
2025-10-02 19:03:18.035472: train_loss -0.4547
2025-10-02 19:03:18.035779: val_loss -0.2049
2025-10-02 19:03:18.036149: Pseudo dice [np.float32(0.5452)]
2025-10-02 19:03:18.036464: Epoch time: 45.92 s
2025-10-02 19:03:19.008041: 
2025-10-02 19:03:19.008272: Epoch 12
2025-10-02 19:03:19.008414: Current learning rate: 0.00928
2025-10-02 19:04:04.915650: Validation loss improved from -0.31827 to -0.32627! Patience: 4/50
2025-10-02 19:04:04.916290: train_loss -0.4631
2025-10-02 19:04:04.916432: val_loss -0.3263
2025-10-02 19:04:04.916606: Pseudo dice [np.float32(0.6273)]
2025-10-02 19:04:04.916825: Epoch time: 45.91 s
2025-10-02 19:04:04.916961: Yayy! New best EMA pseudo Dice: 0.5760999917984009
2025-10-02 19:04:05.970983: 
2025-10-02 19:04:05.971316: Epoch 13
2025-10-02 19:04:05.971471: Current learning rate: 0.00922
2025-10-02 19:04:51.820445: Validation loss improved from -0.32627 to -0.34401! Patience: 0/50
2025-10-02 19:04:51.820880: train_loss -0.4756
2025-10-02 19:04:51.821021: val_loss -0.344
2025-10-02 19:04:51.821163: Pseudo dice [np.float32(0.6469)]
2025-10-02 19:04:51.821319: Epoch time: 45.85 s
2025-10-02 19:04:51.821456: Yayy! New best EMA pseudo Dice: 0.5831000208854675
2025-10-02 19:04:52.899578: 
2025-10-02 19:04:52.899915: Epoch 14
2025-10-02 19:04:52.900156: Current learning rate: 0.00916
2025-10-02 19:05:38.756028: Validation loss did not improve from -0.34401. Patience: 1/50
2025-10-02 19:05:38.757259: train_loss -0.4883
2025-10-02 19:05:38.757698: val_loss -0.2258
2025-10-02 19:05:38.758142: Pseudo dice [np.float32(0.5743)]
2025-10-02 19:05:38.758590: Epoch time: 45.86 s
2025-10-02 19:05:39.825092: 
2025-10-02 19:05:39.825294: Epoch 15
2025-10-02 19:05:39.825471: Current learning rate: 0.0091
2025-10-02 19:06:25.697471: Validation loss did not improve from -0.34401. Patience: 2/50
2025-10-02 19:06:25.697868: train_loss -0.5006
2025-10-02 19:06:25.698023: val_loss -0.2466
2025-10-02 19:06:25.698155: Pseudo dice [np.float32(0.5909)]
2025-10-02 19:06:25.698312: Epoch time: 45.87 s
2025-10-02 19:06:26.326513: 
2025-10-02 19:06:26.326969: Epoch 16
2025-10-02 19:06:26.327288: Current learning rate: 0.00903
2025-10-02 19:07:12.249674: Validation loss did not improve from -0.34401. Patience: 3/50
2025-10-02 19:07:12.250302: train_loss -0.4973
2025-10-02 19:07:12.250449: val_loss -0.2925
2025-10-02 19:07:12.250571: Pseudo dice [np.float32(0.6004)]
2025-10-02 19:07:12.250754: Epoch time: 45.92 s
2025-10-02 19:07:12.250873: Yayy! New best EMA pseudo Dice: 0.5848000049591064
2025-10-02 19:07:13.331202: 
2025-10-02 19:07:13.331458: Epoch 17
2025-10-02 19:07:13.331635: Current learning rate: 0.00897
2025-10-02 19:07:59.260492: Validation loss did not improve from -0.34401. Patience: 4/50
2025-10-02 19:07:59.261285: train_loss -0.5039
2025-10-02 19:07:59.261637: val_loss -0.1432
2025-10-02 19:07:59.261925: Pseudo dice [np.float32(0.5165)]
2025-10-02 19:07:59.262200: Epoch time: 45.93 s
2025-10-02 19:07:59.902588: 
2025-10-02 19:07:59.902911: Epoch 18
2025-10-02 19:07:59.903255: Current learning rate: 0.00891
2025-10-02 19:08:45.871596: Validation loss did not improve from -0.34401. Patience: 5/50
2025-10-02 19:08:45.872713: train_loss -0.5069
2025-10-02 19:08:45.873088: val_loss -0.3196
2025-10-02 19:08:45.873369: Pseudo dice [np.float32(0.6407)]
2025-10-02 19:08:45.873631: Epoch time: 45.97 s
2025-10-02 19:08:46.511906: 
2025-10-02 19:08:46.512349: Epoch 19
2025-10-02 19:08:46.512742: Current learning rate: 0.00885
2025-10-02 19:09:32.488282: Validation loss did not improve from -0.34401. Patience: 6/50
2025-10-02 19:09:32.488960: train_loss -0.5129
2025-10-02 19:09:32.489199: val_loss -0.2573
2025-10-02 19:09:32.489581: Pseudo dice [np.float32(0.5866)]
2025-10-02 19:09:32.489902: Epoch time: 45.98 s
2025-10-02 19:09:33.583915: 
2025-10-02 19:09:33.584305: Epoch 20
2025-10-02 19:09:33.584576: Current learning rate: 0.00879
2025-10-02 19:10:19.528148: Validation loss did not improve from -0.34401. Patience: 7/50
2025-10-02 19:10:19.528784: train_loss -0.5139
2025-10-02 19:10:19.528930: val_loss -0.2109
2025-10-02 19:10:19.529034: Pseudo dice [np.float32(0.579)]
2025-10-02 19:10:19.529154: Epoch time: 45.95 s
2025-10-02 19:10:20.180133: 
2025-10-02 19:10:20.180546: Epoch 21
2025-10-02 19:10:20.180832: Current learning rate: 0.00873
2025-10-02 19:11:06.060413: Validation loss did not improve from -0.34401. Patience: 8/50
2025-10-02 19:11:06.061023: train_loss -0.5215
2025-10-02 19:11:06.061303: val_loss -0.304
2025-10-02 19:11:06.061552: Pseudo dice [np.float32(0.6282)]
2025-10-02 19:11:06.061827: Epoch time: 45.88 s
2025-10-02 19:11:06.062200: Yayy! New best EMA pseudo Dice: 0.5884000062942505
2025-10-02 19:11:07.137114: 
2025-10-02 19:11:07.137553: Epoch 22
2025-10-02 19:11:07.137893: Current learning rate: 0.00867
2025-10-02 19:11:53.063328: Validation loss did not improve from -0.34401. Patience: 9/50
2025-10-02 19:11:53.064441: train_loss -0.5325
2025-10-02 19:11:53.064765: val_loss -0.3026
2025-10-02 19:11:53.065069: Pseudo dice [np.float32(0.6142)]
2025-10-02 19:11:53.065434: Epoch time: 45.93 s
2025-10-02 19:11:53.065757: Yayy! New best EMA pseudo Dice: 0.5910000205039978
2025-10-02 19:11:54.109357: 
2025-10-02 19:11:54.109692: Epoch 23
2025-10-02 19:11:54.109941: Current learning rate: 0.00861
2025-10-02 19:12:40.009894: Validation loss did not improve from -0.34401. Patience: 10/50
2025-10-02 19:12:40.010641: train_loss -0.5307
2025-10-02 19:12:40.011108: val_loss -0.2964
2025-10-02 19:12:40.011516: Pseudo dice [np.float32(0.6307)]
2025-10-02 19:12:40.011969: Epoch time: 45.9 s
2025-10-02 19:12:40.012426: Yayy! New best EMA pseudo Dice: 0.5949000120162964
2025-10-02 19:12:41.065573: 
2025-10-02 19:12:41.066055: Epoch 24
2025-10-02 19:12:41.066445: Current learning rate: 0.00855
2025-10-02 19:13:27.039302: Validation loss did not improve from -0.34401. Patience: 11/50
2025-10-02 19:13:27.039925: train_loss -0.5404
2025-10-02 19:13:27.040058: val_loss -0.1513
2025-10-02 19:13:27.040230: Pseudo dice [np.float32(0.5184)]
2025-10-02 19:13:27.040364: Epoch time: 45.98 s
2025-10-02 19:13:28.105021: 
2025-10-02 19:13:28.105457: Epoch 25
2025-10-02 19:13:28.105849: Current learning rate: 0.00849
2025-10-02 19:14:13.996669: Validation loss did not improve from -0.34401. Patience: 12/50
2025-10-02 19:14:13.997425: train_loss -0.5363
2025-10-02 19:14:13.997836: val_loss -0.2792
2025-10-02 19:14:13.998044: Pseudo dice [np.float32(0.6027)]
2025-10-02 19:14:13.998237: Epoch time: 45.89 s
2025-10-02 19:14:14.632920: 
2025-10-02 19:14:14.633235: Epoch 26
2025-10-02 19:14:14.633419: Current learning rate: 0.00843
2025-10-02 19:15:00.562584: Validation loss improved from -0.34401 to -0.34476! Patience: 12/50
2025-10-02 19:15:00.563170: train_loss -0.5502
2025-10-02 19:15:00.563483: val_loss -0.3448
2025-10-02 19:15:00.563629: Pseudo dice [np.float32(0.6416)]
2025-10-02 19:15:00.563825: Epoch time: 45.93 s
2025-10-02 19:15:01.203528: 
2025-10-02 19:15:01.203831: Epoch 27
2025-10-02 19:15:01.204034: Current learning rate: 0.00836
2025-10-02 19:15:47.476068: Validation loss did not improve from -0.34476. Patience: 1/50
2025-10-02 19:15:47.476615: train_loss -0.5508
2025-10-02 19:15:47.476788: val_loss -0.257
2025-10-02 19:15:47.476931: Pseudo dice [np.float32(0.598)]
2025-10-02 19:15:47.477055: Epoch time: 46.27 s
2025-10-02 19:15:48.107078: 
2025-10-02 19:15:48.107552: Epoch 28
2025-10-02 19:15:48.107889: Current learning rate: 0.0083
2025-10-02 19:16:34.050927: Validation loss improved from -0.34476 to -0.36681! Patience: 1/50
2025-10-02 19:16:34.051320: train_loss -0.5573
2025-10-02 19:16:34.051496: val_loss -0.3668
2025-10-02 19:16:34.051611: Pseudo dice [np.float32(0.6656)]
2025-10-02 19:16:34.051744: Epoch time: 45.94 s
2025-10-02 19:16:34.051872: Yayy! New best EMA pseudo Dice: 0.6015999913215637
2025-10-02 19:16:35.101549: 
2025-10-02 19:16:35.101933: Epoch 29
2025-10-02 19:16:35.102237: Current learning rate: 0.00824
2025-10-02 19:17:21.047173: Validation loss did not improve from -0.36681. Patience: 1/50
2025-10-02 19:17:21.047638: train_loss -0.5641
2025-10-02 19:17:21.047881: val_loss -0.3367
2025-10-02 19:17:21.048096: Pseudo dice [np.float32(0.6532)]
2025-10-02 19:17:21.048402: Epoch time: 45.95 s
2025-10-02 19:17:21.471820: Yayy! New best EMA pseudo Dice: 0.6068000197410583
2025-10-02 19:17:22.539642: 
2025-10-02 19:17:22.539864: Epoch 30
2025-10-02 19:17:22.540014: Current learning rate: 0.00818
2025-10-02 19:18:08.477536: Validation loss did not improve from -0.36681. Patience: 2/50
2025-10-02 19:18:08.478650: train_loss -0.5683
2025-10-02 19:18:08.478972: val_loss -0.2864
2025-10-02 19:18:08.479280: Pseudo dice [np.float32(0.5897)]
2025-10-02 19:18:08.479594: Epoch time: 45.94 s
2025-10-02 19:18:09.124356: 
2025-10-02 19:18:09.124728: Epoch 31
2025-10-02 19:18:09.125121: Current learning rate: 0.00812
2025-10-02 19:18:54.996629: Validation loss did not improve from -0.36681. Patience: 3/50
2025-10-02 19:18:54.997235: train_loss -0.574
2025-10-02 19:18:54.997570: val_loss -0.3358
2025-10-02 19:18:54.997925: Pseudo dice [np.float32(0.6321)]
2025-10-02 19:18:54.998289: Epoch time: 45.87 s
2025-10-02 19:18:54.998647: Yayy! New best EMA pseudo Dice: 0.6078000068664551
2025-10-02 19:18:56.067008: 
2025-10-02 19:18:56.067342: Epoch 32
2025-10-02 19:18:56.067492: Current learning rate: 0.00806
2025-10-02 19:19:42.002316: Validation loss did not improve from -0.36681. Patience: 4/50
2025-10-02 19:19:42.002980: train_loss -0.5738
2025-10-02 19:19:42.003341: val_loss -0.3182
2025-10-02 19:19:42.003702: Pseudo dice [np.float32(0.6347)]
2025-10-02 19:19:42.004033: Epoch time: 45.94 s
2025-10-02 19:19:42.004361: Yayy! New best EMA pseudo Dice: 0.6104999780654907
2025-10-02 19:19:43.064142: 
2025-10-02 19:19:43.064568: Epoch 33
2025-10-02 19:19:43.064952: Current learning rate: 0.008
2025-10-02 19:20:28.973695: Validation loss did not improve from -0.36681. Patience: 5/50
2025-10-02 19:20:28.974254: train_loss -0.5723
2025-10-02 19:20:28.974459: val_loss -0.2444
2025-10-02 19:20:28.974615: Pseudo dice [np.float32(0.5721)]
2025-10-02 19:20:28.974774: Epoch time: 45.91 s
2025-10-02 19:20:29.613424: 
2025-10-02 19:20:29.613833: Epoch 34
2025-10-02 19:20:29.614162: Current learning rate: 0.00793
2025-10-02 19:21:15.493815: Validation loss improved from -0.36681 to -0.38021! Patience: 5/50
2025-10-02 19:21:15.494587: train_loss -0.575
2025-10-02 19:21:15.494968: val_loss -0.3802
2025-10-02 19:21:15.495296: Pseudo dice [np.float32(0.6796)]
2025-10-02 19:21:15.495633: Epoch time: 45.88 s
2025-10-02 19:21:15.904165: Yayy! New best EMA pseudo Dice: 0.6139000058174133
2025-10-02 19:21:16.969570: 
2025-10-02 19:21:16.969904: Epoch 35
2025-10-02 19:21:16.970145: Current learning rate: 0.00787
2025-10-02 19:22:02.875380: Validation loss did not improve from -0.38021. Patience: 1/50
2025-10-02 19:22:02.875929: train_loss -0.582
2025-10-02 19:22:02.876228: val_loss -0.2673
2025-10-02 19:22:02.876647: Pseudo dice [np.float32(0.5847)]
2025-10-02 19:22:02.876974: Epoch time: 45.91 s
2025-10-02 19:22:03.515926: 
2025-10-02 19:22:03.516444: Epoch 36
2025-10-02 19:22:03.516839: Current learning rate: 0.00781
2025-10-02 19:22:49.428317: Validation loss did not improve from -0.38021. Patience: 2/50
2025-10-02 19:22:49.429089: train_loss -0.5896
2025-10-02 19:22:49.429233: val_loss -0.2672
2025-10-02 19:22:49.429419: Pseudo dice [np.float32(0.6186)]
2025-10-02 19:22:49.429568: Epoch time: 45.91 s
2025-10-02 19:22:50.069911: 
2025-10-02 19:22:50.070158: Epoch 37
2025-10-02 19:22:50.070358: Current learning rate: 0.00775
2025-10-02 19:23:35.947136: Validation loss did not improve from -0.38021. Patience: 3/50
2025-10-02 19:23:35.947755: train_loss -0.5986
2025-10-02 19:23:35.947893: val_loss -0.3664
2025-10-02 19:23:35.948269: Pseudo dice [np.float32(0.6751)]
2025-10-02 19:23:35.948613: Epoch time: 45.88 s
2025-10-02 19:23:35.948950: Yayy! New best EMA pseudo Dice: 0.6180999875068665
2025-10-02 19:23:37.007527: 
2025-10-02 19:23:37.007802: Epoch 38
2025-10-02 19:23:37.007951: Current learning rate: 0.00769
2025-10-02 19:24:22.877112: Validation loss did not improve from -0.38021. Patience: 4/50
2025-10-02 19:24:22.877551: train_loss -0.5867
2025-10-02 19:24:22.877706: val_loss -0.3155
2025-10-02 19:24:22.877840: Pseudo dice [np.float32(0.6332)]
2025-10-02 19:24:22.877993: Epoch time: 45.87 s
2025-10-02 19:24:22.878112: Yayy! New best EMA pseudo Dice: 0.6195999979972839
2025-10-02 19:24:23.952458: 
2025-10-02 19:24:23.952744: Epoch 39
2025-10-02 19:24:23.952919: Current learning rate: 0.00763
2025-10-02 19:25:09.833516: Validation loss did not improve from -0.38021. Patience: 5/50
2025-10-02 19:25:09.834113: train_loss -0.6025
2025-10-02 19:25:09.834343: val_loss -0.3335
2025-10-02 19:25:09.834455: Pseudo dice [np.float32(0.6259)]
2025-10-02 19:25:09.834572: Epoch time: 45.88 s
2025-10-02 19:25:10.260304: Yayy! New best EMA pseudo Dice: 0.620199978351593
2025-10-02 19:25:11.338262: 
2025-10-02 19:25:11.338637: Epoch 40
2025-10-02 19:25:11.338840: Current learning rate: 0.00756
2025-10-02 19:25:57.245708: Validation loss did not improve from -0.38021. Patience: 6/50
2025-10-02 19:25:57.246408: train_loss -0.5972
2025-10-02 19:25:57.246723: val_loss -0.2578
2025-10-02 19:25:57.247041: Pseudo dice [np.float32(0.6163)]
2025-10-02 19:25:57.247387: Epoch time: 45.91 s
2025-10-02 19:25:57.891477: 
2025-10-02 19:25:57.891720: Epoch 41
2025-10-02 19:25:57.891888: Current learning rate: 0.0075
2025-10-02 19:26:43.756563: Validation loss did not improve from -0.38021. Patience: 7/50
2025-10-02 19:26:43.757028: train_loss -0.6016
2025-10-02 19:26:43.757241: val_loss -0.2107
2025-10-02 19:26:43.757431: Pseudo dice [np.float32(0.5613)]
2025-10-02 19:26:43.757670: Epoch time: 45.87 s
2025-10-02 19:26:44.384850: 
2025-10-02 19:26:44.385101: Epoch 42
2025-10-02 19:26:44.385359: Current learning rate: 0.00744
2025-10-02 19:27:30.279541: Validation loss did not improve from -0.38021. Patience: 8/50
2025-10-02 19:27:30.280754: train_loss -0.6106
2025-10-02 19:27:30.281174: val_loss -0.2748
2025-10-02 19:27:30.281541: Pseudo dice [np.float32(0.5943)]
2025-10-02 19:27:30.281931: Epoch time: 45.9 s
2025-10-02 19:27:31.317551: 
2025-10-02 19:27:31.317924: Epoch 43
2025-10-02 19:27:31.318191: Current learning rate: 0.00738
2025-10-02 19:28:17.232188: Validation loss did not improve from -0.38021. Patience: 9/50
2025-10-02 19:28:17.232678: train_loss -0.6086
2025-10-02 19:28:17.232840: val_loss -0.3493
2025-10-02 19:28:17.232976: Pseudo dice [np.float32(0.6714)]
2025-10-02 19:28:17.233168: Epoch time: 45.92 s
2025-10-02 19:28:17.864383: 
2025-10-02 19:28:17.864819: Epoch 44
2025-10-02 19:28:17.865087: Current learning rate: 0.00732
2025-10-02 19:29:03.752986: Validation loss did not improve from -0.38021. Patience: 10/50
2025-10-02 19:29:03.753472: train_loss -0.6028
2025-10-02 19:29:03.753679: val_loss -0.3226
2025-10-02 19:29:03.753840: Pseudo dice [np.float32(0.6359)]
2025-10-02 19:29:03.754029: Epoch time: 45.89 s
2025-10-02 19:29:04.834159: 
2025-10-02 19:29:04.834479: Epoch 45
2025-10-02 19:29:04.834663: Current learning rate: 0.00725
2025-10-02 19:29:50.749432: Validation loss did not improve from -0.38021. Patience: 11/50
2025-10-02 19:29:50.750448: train_loss -0.6091
2025-10-02 19:29:50.750807: val_loss -0.2022
2025-10-02 19:29:50.751095: Pseudo dice [np.float32(0.5809)]
2025-10-02 19:29:50.751378: Epoch time: 45.92 s
2025-10-02 19:29:51.384215: 
2025-10-02 19:29:51.384734: Epoch 46
2025-10-02 19:29:51.385159: Current learning rate: 0.00719
2025-10-02 19:30:37.292678: Validation loss did not improve from -0.38021. Patience: 12/50
2025-10-02 19:30:37.293479: train_loss -0.6066
2025-10-02 19:30:37.293835: val_loss -0.2762
2025-10-02 19:30:37.294175: Pseudo dice [np.float32(0.6212)]
2025-10-02 19:30:37.294524: Epoch time: 45.91 s
2025-10-02 19:30:37.924638: 
2025-10-02 19:30:37.924960: Epoch 47
2025-10-02 19:30:37.925132: Current learning rate: 0.00713
2025-10-02 19:31:23.826102: Validation loss did not improve from -0.38021. Patience: 13/50
2025-10-02 19:31:23.826524: train_loss -0.624
2025-10-02 19:31:23.826690: val_loss -0.2404
2025-10-02 19:31:23.826820: Pseudo dice [np.float32(0.5741)]
2025-10-02 19:31:23.826953: Epoch time: 45.9 s
2025-10-02 19:31:24.463715: 
2025-10-02 19:31:24.464208: Epoch 48
2025-10-02 19:31:24.464523: Current learning rate: 0.00707
2025-10-02 19:32:10.349000: Validation loss did not improve from -0.38021. Patience: 14/50
2025-10-02 19:32:10.350115: train_loss -0.6227
2025-10-02 19:32:10.350441: val_loss -0.2054
2025-10-02 19:32:10.350732: Pseudo dice [np.float32(0.5841)]
2025-10-02 19:32:10.351063: Epoch time: 45.89 s
2025-10-02 19:32:10.994392: 
2025-10-02 19:32:10.994648: Epoch 49
2025-10-02 19:32:10.994817: Current learning rate: 0.007
2025-10-02 19:32:56.927409: Validation loss did not improve from -0.38021. Patience: 15/50
2025-10-02 19:32:56.927830: train_loss -0.6288
2025-10-02 19:32:56.927988: val_loss -0.2622
2025-10-02 19:32:56.928185: Pseudo dice [np.float32(0.6189)]
2025-10-02 19:32:56.928380: Epoch time: 45.93 s
2025-10-02 19:32:58.009243: 
2025-10-02 19:32:58.009526: Epoch 50
2025-10-02 19:32:58.009719: Current learning rate: 0.00694
2025-10-02 19:33:44.037427: Validation loss did not improve from -0.38021. Patience: 16/50
2025-10-02 19:33:44.037846: train_loss -0.6275
2025-10-02 19:33:44.037979: val_loss -0.2181
2025-10-02 19:33:44.038104: Pseudo dice [np.float32(0.5821)]
2025-10-02 19:33:44.038240: Epoch time: 46.03 s
2025-10-02 19:33:44.679172: 
2025-10-02 19:33:44.679690: Epoch 51
2025-10-02 19:33:44.680053: Current learning rate: 0.00688
2025-10-02 19:34:30.648099: Validation loss did not improve from -0.38021. Patience: 17/50
2025-10-02 19:34:30.648912: train_loss -0.6132
2025-10-02 19:34:30.649164: val_loss -0.3687
2025-10-02 19:34:30.649301: Pseudo dice [np.float32(0.6726)]
2025-10-02 19:34:30.649479: Epoch time: 45.97 s
2025-10-02 19:34:31.291157: 
2025-10-02 19:34:31.291433: Epoch 52
2025-10-02 19:34:31.291667: Current learning rate: 0.00682
2025-10-02 19:35:17.202048: Validation loss did not improve from -0.38021. Patience: 18/50
2025-10-02 19:35:17.202872: train_loss -0.63
2025-10-02 19:35:17.203239: val_loss -0.3411
2025-10-02 19:35:17.203563: Pseudo dice [np.float32(0.6481)]
2025-10-02 19:35:17.203909: Epoch time: 45.91 s
2025-10-02 19:35:17.842548: 
2025-10-02 19:35:17.842850: Epoch 53
2025-10-02 19:35:17.843046: Current learning rate: 0.00675
2025-10-02 19:36:03.809455: Validation loss did not improve from -0.38021. Patience: 19/50
2025-10-02 19:36:03.809861: train_loss -0.6313
2025-10-02 19:36:03.810016: val_loss -0.2311
2025-10-02 19:36:03.810128: Pseudo dice [np.float32(0.613)]
2025-10-02 19:36:03.810261: Epoch time: 45.97 s
2025-10-02 19:36:04.446874: 
2025-10-02 19:36:04.447123: Epoch 54
2025-10-02 19:36:04.447345: Current learning rate: 0.00669
2025-10-02 19:36:50.392794: Validation loss did not improve from -0.38021. Patience: 20/50
2025-10-02 19:36:50.393553: train_loss -0.6323
2025-10-02 19:36:50.393808: val_loss -0.2089
2025-10-02 19:36:50.394014: Pseudo dice [np.float32(0.5854)]
2025-10-02 19:36:50.394433: Epoch time: 45.95 s
2025-10-02 19:36:51.498868: 
2025-10-02 19:36:51.499121: Epoch 55
2025-10-02 19:36:51.499288: Current learning rate: 0.00663
2025-10-02 19:37:37.391169: Validation loss did not improve from -0.38021. Patience: 21/50
2025-10-02 19:37:37.391657: train_loss -0.6445
2025-10-02 19:37:37.391838: val_loss -0.3211
2025-10-02 19:37:37.392133: Pseudo dice [np.float32(0.6339)]
2025-10-02 19:37:37.392347: Epoch time: 45.89 s
2025-10-02 19:37:38.031893: 
2025-10-02 19:37:38.032145: Epoch 56
2025-10-02 19:37:38.032301: Current learning rate: 0.00657
2025-10-02 19:38:23.941965: Validation loss did not improve from -0.38021. Patience: 22/50
2025-10-02 19:38:23.942449: train_loss -0.6531
2025-10-02 19:38:23.942593: val_loss -0.1798
2025-10-02 19:38:23.942734: Pseudo dice [np.float32(0.5583)]
2025-10-02 19:38:23.942854: Epoch time: 45.91 s
2025-10-02 19:38:24.582586: 
2025-10-02 19:38:24.582875: Epoch 57
2025-10-02 19:38:24.583034: Current learning rate: 0.0065
2025-10-02 19:39:10.510432: Validation loss did not improve from -0.38021. Patience: 23/50
2025-10-02 19:39:10.510962: train_loss -0.6474
2025-10-02 19:39:10.511115: val_loss -0.1458
2025-10-02 19:39:10.511236: Pseudo dice [np.float32(0.5551)]
2025-10-02 19:39:10.511368: Epoch time: 45.93 s
2025-10-02 19:39:11.534787: 
2025-10-02 19:39:11.535115: Epoch 58
2025-10-02 19:39:11.535302: Current learning rate: 0.00644
2025-10-02 19:39:57.466372: Validation loss did not improve from -0.38021. Patience: 24/50
2025-10-02 19:39:57.466756: train_loss -0.6457
2025-10-02 19:39:57.466886: val_loss -0.3429
2025-10-02 19:39:57.467000: Pseudo dice [np.float32(0.6504)]
2025-10-02 19:39:57.467235: Epoch time: 45.93 s
2025-10-02 19:39:58.116824: 
2025-10-02 19:39:58.117340: Epoch 59
2025-10-02 19:39:58.117725: Current learning rate: 0.00638
2025-10-02 19:40:44.057141: Validation loss did not improve from -0.38021. Patience: 25/50
2025-10-02 19:40:44.057516: train_loss -0.6611
2025-10-02 19:40:44.057706: val_loss -0.2029
2025-10-02 19:40:44.057854: Pseudo dice [np.float32(0.5675)]
2025-10-02 19:40:44.058029: Epoch time: 45.94 s
2025-10-02 19:40:45.144690: 
2025-10-02 19:40:45.145120: Epoch 60
2025-10-02 19:40:45.145456: Current learning rate: 0.00631
2025-10-02 19:41:31.120269: Validation loss did not improve from -0.38021. Patience: 26/50
2025-10-02 19:41:31.120892: train_loss -0.6674
2025-10-02 19:41:31.121136: val_loss -0.3441
2025-10-02 19:41:31.121334: Pseudo dice [np.float32(0.6572)]
2025-10-02 19:41:31.121535: Epoch time: 45.98 s
2025-10-02 19:41:31.767653: 
2025-10-02 19:41:31.767916: Epoch 61
2025-10-02 19:41:31.768172: Current learning rate: 0.00625
2025-10-02 19:42:17.680946: Validation loss did not improve from -0.38021. Patience: 27/50
2025-10-02 19:42:17.681484: train_loss -0.6593
2025-10-02 19:42:17.681869: val_loss -0.2636
2025-10-02 19:42:17.682233: Pseudo dice [np.float32(0.605)]
2025-10-02 19:42:17.682596: Epoch time: 45.91 s
2025-10-02 19:42:18.333869: 
2025-10-02 19:42:18.334095: Epoch 62
2025-10-02 19:42:18.334266: Current learning rate: 0.00619
2025-10-02 19:43:04.272224: Validation loss did not improve from -0.38021. Patience: 28/50
2025-10-02 19:43:04.272678: train_loss -0.6603
2025-10-02 19:43:04.272862: val_loss -0.1351
2025-10-02 19:43:04.273047: Pseudo dice [np.float32(0.5317)]
2025-10-02 19:43:04.273203: Epoch time: 45.94 s
2025-10-02 19:43:04.927483: 
2025-10-02 19:43:04.927773: Epoch 63
2025-10-02 19:43:04.927973: Current learning rate: 0.00612
2025-10-02 19:43:50.905131: Validation loss did not improve from -0.38021. Patience: 29/50
2025-10-02 19:43:50.905689: train_loss -0.668
2025-10-02 19:43:50.905833: val_loss -0.1391
2025-10-02 19:43:50.905952: Pseudo dice [np.float32(0.5534)]
2025-10-02 19:43:50.906084: Epoch time: 45.98 s
2025-10-02 19:43:51.556555: 
2025-10-02 19:43:51.557020: Epoch 64
2025-10-02 19:43:51.557407: Current learning rate: 0.00606
2025-10-02 19:44:37.537632: Validation loss did not improve from -0.38021. Patience: 30/50
2025-10-02 19:44:37.538117: train_loss -0.6722
2025-10-02 19:44:37.538268: val_loss -0.297
2025-10-02 19:44:37.538384: Pseudo dice [np.float32(0.6322)]
2025-10-02 19:44:37.538521: Epoch time: 45.98 s
2025-10-02 19:44:38.618436: 
2025-10-02 19:44:38.618790: Epoch 65
2025-10-02 19:44:38.618964: Current learning rate: 0.006
2025-10-02 19:45:24.596535: Validation loss did not improve from -0.38021. Patience: 31/50
2025-10-02 19:45:24.597007: train_loss -0.6666
2025-10-02 19:45:24.597245: val_loss -0.335
2025-10-02 19:45:24.597459: Pseudo dice [np.float32(0.6378)]
2025-10-02 19:45:24.597660: Epoch time: 45.98 s
2025-10-02 19:45:25.257797: 
2025-10-02 19:45:25.258115: Epoch 66
2025-10-02 19:45:25.258272: Current learning rate: 0.00593
2025-10-02 19:46:11.226631: Validation loss did not improve from -0.38021. Patience: 32/50
2025-10-02 19:46:11.227297: train_loss -0.6788
2025-10-02 19:46:11.227446: val_loss -0.2354
2025-10-02 19:46:11.227589: Pseudo dice [np.float32(0.6046)]
2025-10-02 19:46:11.227829: Epoch time: 45.97 s
2025-10-02 19:46:11.884352: 
2025-10-02 19:46:11.884595: Epoch 67
2025-10-02 19:46:11.884747: Current learning rate: 0.00587
2025-10-02 19:46:57.847081: Validation loss did not improve from -0.38021. Patience: 33/50
2025-10-02 19:46:57.847531: train_loss -0.6768
2025-10-02 19:46:57.847782: val_loss -0.2014
2025-10-02 19:46:57.847920: Pseudo dice [np.float32(0.578)]
2025-10-02 19:46:57.848107: Epoch time: 45.96 s
2025-10-02 19:46:58.502572: 
2025-10-02 19:46:58.502874: Epoch 68
2025-10-02 19:46:58.503039: Current learning rate: 0.00581
2025-10-02 19:47:44.473355: Validation loss did not improve from -0.38021. Patience: 34/50
2025-10-02 19:47:44.473938: train_loss -0.6739
2025-10-02 19:47:44.474103: val_loss -0.2603
2025-10-02 19:47:44.474233: Pseudo dice [np.float32(0.6207)]
2025-10-02 19:47:44.474376: Epoch time: 45.97 s
2025-10-02 19:47:45.120913: 
2025-10-02 19:47:45.121111: Epoch 69
2025-10-02 19:47:45.121247: Current learning rate: 0.00574
2025-10-02 19:48:31.000452: Validation loss did not improve from -0.38021. Patience: 35/50
2025-10-02 19:48:31.000998: train_loss -0.6914
2025-10-02 19:48:31.001180: val_loss -0.2629
2025-10-02 19:48:31.001339: Pseudo dice [np.float32(0.6232)]
2025-10-02 19:48:31.001507: Epoch time: 45.88 s
2025-10-02 19:48:32.083848: 
2025-10-02 19:48:32.084054: Epoch 70
2025-10-02 19:48:32.084256: Current learning rate: 0.00568
2025-10-02 19:49:18.101921: Validation loss did not improve from -0.38021. Patience: 36/50
2025-10-02 19:49:18.102461: train_loss -0.6862
2025-10-02 19:49:18.102693: val_loss -0.166
2025-10-02 19:49:18.102929: Pseudo dice [np.float32(0.5479)]
2025-10-02 19:49:18.103153: Epoch time: 46.02 s
2025-10-02 19:49:18.753425: 
2025-10-02 19:49:18.753799: Epoch 71
2025-10-02 19:49:18.754026: Current learning rate: 0.00562
2025-10-02 19:50:04.726250: Validation loss did not improve from -0.38021. Patience: 37/50
2025-10-02 19:50:04.726660: train_loss -0.6844
2025-10-02 19:50:04.726865: val_loss -0.1827
2025-10-02 19:50:04.727016: Pseudo dice [np.float32(0.581)]
2025-10-02 19:50:04.727188: Epoch time: 45.97 s
2025-10-02 19:50:05.376040: 
2025-10-02 19:50:05.376355: Epoch 72
2025-10-02 19:50:05.376536: Current learning rate: 0.00555
2025-10-02 19:50:51.359856: Validation loss did not improve from -0.38021. Patience: 38/50
2025-10-02 19:50:51.360526: train_loss -0.6963
2025-10-02 19:50:51.360837: val_loss -0.1739
2025-10-02 19:50:51.361155: Pseudo dice [np.float32(0.5614)]
2025-10-02 19:50:51.361511: Epoch time: 45.99 s
2025-10-02 19:50:52.010191: 
2025-10-02 19:50:52.010487: Epoch 73
2025-10-02 19:50:52.010659: Current learning rate: 0.00549
2025-10-02 19:51:37.962544: Validation loss did not improve from -0.38021. Patience: 39/50
2025-10-02 19:51:37.963007: train_loss -0.6961
2025-10-02 19:51:37.963272: val_loss -0.1758
2025-10-02 19:51:37.963441: Pseudo dice [np.float32(0.5511)]
2025-10-02 19:51:37.963640: Epoch time: 45.95 s
2025-10-02 19:51:39.006567: 
2025-10-02 19:51:39.006932: Epoch 74
2025-10-02 19:51:39.007193: Current learning rate: 0.00542
2025-10-02 19:52:24.966841: Validation loss did not improve from -0.38021. Patience: 40/50
2025-10-02 19:52:24.967462: train_loss -0.6927
2025-10-02 19:52:24.967692: val_loss -0.2242
2025-10-02 19:52:24.967875: Pseudo dice [np.float32(0.6116)]
2025-10-02 19:52:24.968020: Epoch time: 45.96 s
2025-10-02 19:52:26.031233: 
2025-10-02 19:52:26.031728: Epoch 75
2025-10-02 19:52:26.032112: Current learning rate: 0.00536
2025-10-02 19:53:11.991399: Validation loss did not improve from -0.38021. Patience: 41/50
2025-10-02 19:53:11.991965: train_loss -0.6928
2025-10-02 19:53:11.992256: val_loss -0.2803
2025-10-02 19:53:11.992442: Pseudo dice [np.float32(0.6358)]
2025-10-02 19:53:11.992610: Epoch time: 45.96 s
2025-10-02 19:53:12.648274: 
2025-10-02 19:53:12.648569: Epoch 76
2025-10-02 19:53:12.648719: Current learning rate: 0.00529
2025-10-02 19:53:58.602874: Validation loss did not improve from -0.38021. Patience: 42/50
2025-10-02 19:53:58.603309: train_loss -0.6961
2025-10-02 19:53:58.603474: val_loss -0.2115
2025-10-02 19:53:58.603607: Pseudo dice [np.float32(0.5746)]
2025-10-02 19:53:58.603784: Epoch time: 45.96 s
2025-10-02 19:53:59.263121: 
2025-10-02 19:53:59.263361: Epoch 77
2025-10-02 19:53:59.263501: Current learning rate: 0.00523
2025-10-02 19:54:45.157175: Validation loss did not improve from -0.38021. Patience: 43/50
2025-10-02 19:54:45.157881: train_loss -0.7043
2025-10-02 19:54:45.158364: val_loss -0.2027
2025-10-02 19:54:45.158762: Pseudo dice [np.float32(0.5896)]
2025-10-02 19:54:45.159150: Epoch time: 45.9 s
2025-10-02 19:54:45.831239: 
2025-10-02 19:54:45.831536: Epoch 78
2025-10-02 19:54:45.831686: Current learning rate: 0.00517
2025-10-02 19:55:31.694787: Validation loss did not improve from -0.38021. Patience: 44/50
2025-10-02 19:55:31.695259: train_loss -0.7053
2025-10-02 19:55:31.695393: val_loss -0.2727
2025-10-02 19:55:31.695519: Pseudo dice [np.float32(0.6349)]
2025-10-02 19:55:31.695659: Epoch time: 45.86 s
2025-10-02 19:55:32.354299: 
2025-10-02 19:55:32.354712: Epoch 79
2025-10-02 19:55:32.355094: Current learning rate: 0.0051
2025-10-02 19:56:18.237252: Validation loss did not improve from -0.38021. Patience: 45/50
2025-10-02 19:56:18.237634: train_loss -0.71
2025-10-02 19:56:18.237832: val_loss -0.1916
2025-10-02 19:56:18.238012: Pseudo dice [np.float32(0.5933)]
2025-10-02 19:56:18.238177: Epoch time: 45.88 s
2025-10-02 19:56:19.309201: 
2025-10-02 19:56:19.309536: Epoch 80
2025-10-02 19:56:19.309747: Current learning rate: 0.00504
2025-10-02 19:57:05.184038: Validation loss did not improve from -0.38021. Patience: 46/50
2025-10-02 19:57:05.184615: train_loss -0.7031
2025-10-02 19:57:05.184783: val_loss -0.1485
2025-10-02 19:57:05.184910: Pseudo dice [np.float32(0.5491)]
2025-10-02 19:57:05.185064: Epoch time: 45.88 s
2025-10-02 19:57:05.846999: 
2025-10-02 19:57:05.847244: Epoch 81
2025-10-02 19:57:05.847398: Current learning rate: 0.00497
2025-10-02 19:57:51.710227: Validation loss did not improve from -0.38021. Patience: 47/50
2025-10-02 19:57:51.710663: train_loss -0.7098
2025-10-02 19:57:51.710814: val_loss -0.3036
2025-10-02 19:57:51.710952: Pseudo dice [np.float32(0.6413)]
2025-10-02 19:57:51.711106: Epoch time: 45.86 s
2025-10-02 19:57:52.379768: 
2025-10-02 19:57:52.380262: Epoch 82
2025-10-02 19:57:52.380623: Current learning rate: 0.00491
2025-10-02 19:58:38.244819: Validation loss did not improve from -0.38021. Patience: 48/50
2025-10-02 19:58:38.245269: train_loss -0.7017
2025-10-02 19:58:38.245439: val_loss -0.2336
2025-10-02 19:58:38.245578: Pseudo dice [np.float32(0.5739)]
2025-10-02 19:58:38.245725: Epoch time: 45.87 s
2025-10-02 19:58:38.889453: 
2025-10-02 19:58:38.889726: Epoch 83
2025-10-02 19:58:38.889930: Current learning rate: 0.00484
2025-10-02 19:59:24.745073: Validation loss did not improve from -0.38021. Patience: 49/50
2025-10-02 19:59:24.745430: train_loss -0.7107
2025-10-02 19:59:24.745571: val_loss -0.1694
2025-10-02 19:59:24.745737: Pseudo dice [np.float32(0.5648)]
2025-10-02 19:59:24.745884: Epoch time: 45.86 s
2025-10-02 19:59:25.391361: 
2025-10-02 19:59:25.391789: Epoch 84
2025-10-02 19:59:25.392190: Current learning rate: 0.00478
2025-10-02 20:00:11.309523: Validation loss did not improve from -0.38021. Patience: 50/50
2025-10-02 20:00:11.309989: train_loss -0.7149
2025-10-02 20:00:11.310150: val_loss -0.1993
2025-10-02 20:00:11.310293: Pseudo dice [np.float32(0.5883)]
2025-10-02 20:00:11.310432: Epoch time: 45.92 s
2025-10-02 20:00:12.363669: 
2025-10-02 20:00:12.363983: Epoch 85
2025-10-02 20:00:12.364137: Current learning rate: 0.00471
2025-10-02 20:00:58.298709: Validation loss did not improve from -0.38021. Patience: 51/50
2025-10-02 20:00:58.299058: train_loss -0.7153
2025-10-02 20:00:58.299235: val_loss -0.2222
2025-10-02 20:00:58.299409: Pseudo dice [np.float32(0.59)]
2025-10-02 20:00:58.299620: Epoch time: 45.94 s
2025-10-02 20:00:58.936028: 
2025-10-02 20:00:58.936522: Epoch 86
2025-10-02 20:00:58.936913: Current learning rate: 0.00465
2025-10-02 20:01:44.880528: Validation loss did not improve from -0.38021. Patience: 52/50
2025-10-02 20:01:44.880976: train_loss -0.7221
2025-10-02 20:01:44.881120: val_loss -0.2262
2025-10-02 20:01:44.881234: Pseudo dice [np.float32(0.6178)]
2025-10-02 20:01:44.881356: Epoch time: 45.95 s
2025-10-02 20:01:45.524426: 
2025-10-02 20:01:45.524725: Epoch 87
2025-10-02 20:01:45.524912: Current learning rate: 0.00458
2025-10-02 20:02:31.462710: Validation loss did not improve from -0.38021. Patience: 53/50
2025-10-02 20:02:31.463119: train_loss -0.7223
2025-10-02 20:02:31.463287: val_loss -0.2065
2025-10-02 20:02:31.463397: Pseudo dice [np.float32(0.5838)]
2025-10-02 20:02:31.463517: Epoch time: 45.94 s
2025-10-02 20:02:32.102268: 
2025-10-02 20:02:32.102702: Epoch 88
2025-10-02 20:02:32.103043: Current learning rate: 0.00452
2025-10-02 20:03:18.039739: Validation loss did not improve from -0.38021. Patience: 54/50
2025-10-02 20:03:18.040260: train_loss -0.7194
2025-10-02 20:03:18.040503: val_loss -0.2463
2025-10-02 20:03:18.040699: Pseudo dice [np.float32(0.6104)]
2025-10-02 20:03:18.040932: Epoch time: 45.94 s
2025-10-02 20:03:19.057373: 
2025-10-02 20:03:19.057730: Epoch 89
2025-10-02 20:03:19.057898: Current learning rate: 0.00445
2025-10-02 20:04:05.014606: Validation loss did not improve from -0.38021. Patience: 55/50
2025-10-02 20:04:05.015046: train_loss -0.7271
2025-10-02 20:04:05.015235: val_loss -0.1542
2025-10-02 20:04:05.015353: Pseudo dice [np.float32(0.5544)]
2025-10-02 20:04:05.015506: Epoch time: 45.96 s
2025-10-02 20:04:06.074441: 
2025-10-02 20:04:06.074862: Epoch 90
2025-10-02 20:04:06.075245: Current learning rate: 0.00438
2025-10-02 20:04:52.032479: Validation loss did not improve from -0.38021. Patience: 56/50
2025-10-02 20:04:52.032948: train_loss -0.719
2025-10-02 20:04:52.033108: val_loss -0.2418
2025-10-02 20:04:52.033226: Pseudo dice [np.float32(0.5975)]
2025-10-02 20:04:52.033394: Epoch time: 45.96 s
2025-10-02 20:04:52.672660: 
2025-10-02 20:04:52.672923: Epoch 91
2025-10-02 20:04:52.673106: Current learning rate: 0.00432
2025-10-02 20:05:38.669833: Validation loss did not improve from -0.38021. Patience: 57/50
2025-10-02 20:05:38.670276: train_loss -0.7294
2025-10-02 20:05:38.670535: val_loss -0.244
2025-10-02 20:05:38.670726: Pseudo dice [np.float32(0.6257)]
2025-10-02 20:05:38.670925: Epoch time: 46.0 s
2025-10-02 20:05:39.319666: 
2025-10-02 20:05:39.319900: Epoch 92
2025-10-02 20:05:39.320065: Current learning rate: 0.00425
2025-10-02 20:06:25.310381: Validation loss did not improve from -0.38021. Patience: 58/50
2025-10-02 20:06:25.310831: train_loss -0.7363
2025-10-02 20:06:25.311080: val_loss -0.2585
2025-10-02 20:06:25.311276: Pseudo dice [np.float32(0.6144)]
2025-10-02 20:06:25.311436: Epoch time: 45.99 s
2025-10-02 20:06:25.956047: 
2025-10-02 20:06:25.956309: Epoch 93
2025-10-02 20:06:25.956514: Current learning rate: 0.00419
2025-10-02 20:07:11.936621: Validation loss did not improve from -0.38021. Patience: 59/50
2025-10-02 20:07:11.937019: train_loss -0.7347
2025-10-02 20:07:11.937191: val_loss -0.2433
2025-10-02 20:07:11.937321: Pseudo dice [np.float32(0.6105)]
2025-10-02 20:07:11.937461: Epoch time: 45.98 s
2025-10-02 20:07:12.583990: 
2025-10-02 20:07:12.584311: Epoch 94
2025-10-02 20:07:12.584532: Current learning rate: 0.00412
2025-10-02 20:07:58.556617: Validation loss did not improve from -0.38021. Patience: 60/50
2025-10-02 20:07:58.557014: train_loss -0.7352
2025-10-02 20:07:58.557170: val_loss -0.0728
2025-10-02 20:07:58.557305: Pseudo dice [np.float32(0.5121)]
2025-10-02 20:07:58.557445: Epoch time: 45.97 s
2025-10-02 20:07:59.625630: 
2025-10-02 20:07:59.625862: Epoch 95
2025-10-02 20:07:59.626025: Current learning rate: 0.00405
2025-10-02 20:08:45.594010: Validation loss did not improve from -0.38021. Patience: 61/50
2025-10-02 20:08:45.594396: train_loss -0.7377
2025-10-02 20:08:45.594538: val_loss -0.2653
2025-10-02 20:08:45.594739: Pseudo dice [np.float32(0.6282)]
2025-10-02 20:08:45.594952: Epoch time: 45.97 s
2025-10-02 20:08:46.233829: 
2025-10-02 20:08:46.234127: Epoch 96
2025-10-02 20:08:46.234294: Current learning rate: 0.00399
2025-10-02 20:09:32.136369: Validation loss did not improve from -0.38021. Patience: 62/50
2025-10-02 20:09:32.136849: train_loss -0.7413
2025-10-02 20:09:32.137008: val_loss -0.1863
2025-10-02 20:09:32.137164: Pseudo dice [np.float32(0.5837)]
2025-10-02 20:09:32.137314: Epoch time: 45.9 s
2025-10-02 20:09:32.781387: 
2025-10-02 20:09:32.781859: Epoch 97
2025-10-02 20:09:32.782239: Current learning rate: 0.00392
2025-10-02 20:10:18.756416: Validation loss did not improve from -0.38021. Patience: 63/50
2025-10-02 20:10:18.756830: train_loss -0.7526
2025-10-02 20:10:18.756964: val_loss -0.098
2025-10-02 20:10:18.757085: Pseudo dice [np.float32(0.524)]
2025-10-02 20:10:18.757204: Epoch time: 45.98 s
2025-10-02 20:10:19.404258: 
2025-10-02 20:10:19.404515: Epoch 98
2025-10-02 20:10:19.404684: Current learning rate: 0.00385
2025-10-02 20:11:05.346101: Validation loss did not improve from -0.38021. Patience: 64/50
2025-10-02 20:11:05.346688: train_loss -0.7433
2025-10-02 20:11:05.346977: val_loss -0.173
2025-10-02 20:11:05.347224: Pseudo dice [np.float32(0.5608)]
2025-10-02 20:11:05.347466: Epoch time: 45.94 s
2025-10-02 20:11:06.000830: 
2025-10-02 20:11:06.001221: Epoch 99
2025-10-02 20:11:06.001479: Current learning rate: 0.00379
2025-10-02 20:11:51.933422: Validation loss did not improve from -0.38021. Patience: 65/50
2025-10-02 20:11:51.933957: train_loss -0.7503
2025-10-02 20:11:51.934218: val_loss -0.1846
2025-10-02 20:11:51.934421: Pseudo dice [np.float32(0.5837)]
2025-10-02 20:11:51.934652: Epoch time: 45.93 s
2025-10-02 20:11:52.995223: 
2025-10-02 20:11:52.995632: Epoch 100
2025-10-02 20:11:52.995892: Current learning rate: 0.00372
2025-10-02 20:12:38.845274: Validation loss did not improve from -0.38021. Patience: 66/50
2025-10-02 20:12:38.845729: train_loss -0.7587
2025-10-02 20:12:38.845958: val_loss -0.2123
2025-10-02 20:12:38.846169: Pseudo dice [np.float32(0.603)]
2025-10-02 20:12:38.846402: Epoch time: 45.85 s
2025-10-02 20:12:39.497261: 
2025-10-02 20:12:39.497507: Epoch 101
2025-10-02 20:12:39.497662: Current learning rate: 0.00365
2025-10-02 20:13:25.447783: Validation loss did not improve from -0.38021. Patience: 67/50
2025-10-02 20:13:25.448331: train_loss -0.7585
2025-10-02 20:13:25.448476: val_loss -0.2552
2025-10-02 20:13:25.448593: Pseudo dice [np.float32(0.6124)]
2025-10-02 20:13:25.448719: Epoch time: 45.95 s
2025-10-02 20:13:26.102644: 
2025-10-02 20:13:26.102912: Epoch 102
2025-10-02 20:13:26.103095: Current learning rate: 0.00359
2025-10-02 20:14:12.007016: Validation loss did not improve from -0.38021. Patience: 68/50
2025-10-02 20:14:12.007495: train_loss -0.755
2025-10-02 20:14:12.007649: val_loss -0.19
2025-10-02 20:14:12.007788: Pseudo dice [np.float32(0.5872)]
2025-10-02 20:14:12.007930: Epoch time: 45.91 s
2025-10-02 20:14:12.651386: 
2025-10-02 20:14:12.651641: Epoch 103
2025-10-02 20:14:12.651798: Current learning rate: 0.00352
2025-10-02 20:14:58.625706: Validation loss did not improve from -0.38021. Patience: 69/50
2025-10-02 20:14:58.626310: train_loss -0.755
2025-10-02 20:14:58.626489: val_loss -0.1236
2025-10-02 20:14:58.626605: Pseudo dice [np.float32(0.5286)]
2025-10-02 20:14:58.626732: Epoch time: 45.98 s
2025-10-02 20:14:59.270724: 
2025-10-02 20:14:59.270954: Epoch 104
2025-10-02 20:14:59.271089: Current learning rate: 0.00345
2025-10-02 20:15:45.251333: Validation loss did not improve from -0.38021. Patience: 70/50
2025-10-02 20:15:45.251864: train_loss -0.7609
2025-10-02 20:15:45.252150: val_loss -0.208
2025-10-02 20:15:45.252385: Pseudo dice [np.float32(0.5971)]
2025-10-02 20:15:45.252631: Epoch time: 45.98 s
2025-10-02 20:15:46.706368: 
2025-10-02 20:15:46.706628: Epoch 105
2025-10-02 20:15:46.706801: Current learning rate: 0.00338
2025-10-02 20:16:32.633782: Validation loss did not improve from -0.38021. Patience: 71/50
2025-10-02 20:16:32.634358: train_loss -0.7573
2025-10-02 20:16:32.634602: val_loss -0.2836
2025-10-02 20:16:32.634839: Pseudo dice [np.float32(0.6362)]
2025-10-02 20:16:32.635059: Epoch time: 45.93 s
2025-10-02 20:16:33.287011: 
2025-10-02 20:16:33.287350: Epoch 106
2025-10-02 20:16:33.287529: Current learning rate: 0.00332
2025-10-02 20:17:19.230766: Validation loss did not improve from -0.38021. Patience: 72/50
2025-10-02 20:17:19.231126: train_loss -0.7677
2025-10-02 20:17:19.231270: val_loss -0.0652
2025-10-02 20:17:19.231461: Pseudo dice [np.float32(0.5038)]
2025-10-02 20:17:19.231593: Epoch time: 45.94 s
2025-10-02 20:17:19.887546: 
2025-10-02 20:17:19.887812: Epoch 107
2025-10-02 20:17:19.887953: Current learning rate: 0.00325
2025-10-02 20:18:05.821288: Validation loss did not improve from -0.38021. Patience: 73/50
2025-10-02 20:18:05.821783: train_loss -0.7622
2025-10-02 20:18:05.821913: val_loss -0.133
2025-10-02 20:18:05.822051: Pseudo dice [np.float32(0.5556)]
2025-10-02 20:18:05.822224: Epoch time: 45.93 s
2025-10-02 20:18:06.479841: 
2025-10-02 20:18:06.480144: Epoch 108
2025-10-02 20:18:06.480368: Current learning rate: 0.00318
2025-10-02 20:18:52.381140: Validation loss did not improve from -0.38021. Patience: 74/50
2025-10-02 20:18:52.381818: train_loss -0.7639
2025-10-02 20:18:52.382166: val_loss -0.1262
2025-10-02 20:18:52.382509: Pseudo dice [np.float32(0.5414)]
2025-10-02 20:18:52.382824: Epoch time: 45.9 s
2025-10-02 20:18:53.038640: 
2025-10-02 20:18:53.039024: Epoch 109
2025-10-02 20:18:53.039388: Current learning rate: 0.00311
2025-10-02 20:19:38.960541: Validation loss did not improve from -0.38021. Patience: 75/50
2025-10-02 20:19:38.961518: train_loss -0.7644
2025-10-02 20:19:38.961831: val_loss -0.2518
2025-10-02 20:19:38.962154: Pseudo dice [np.float32(0.6292)]
2025-10-02 20:19:38.962490: Epoch time: 45.92 s
2025-10-02 20:19:40.035913: 
2025-10-02 20:19:40.036243: Epoch 110
2025-10-02 20:19:40.036501: Current learning rate: 0.00304
2025-10-02 20:20:26.010352: Validation loss did not improve from -0.38021. Patience: 76/50
2025-10-02 20:20:26.011085: train_loss -0.7593
2025-10-02 20:20:26.011277: val_loss -0.2199
2025-10-02 20:20:26.011573: Pseudo dice [np.float32(0.6214)]
2025-10-02 20:20:26.011714: Epoch time: 45.98 s
2025-10-02 20:20:26.666353: 
2025-10-02 20:20:26.666586: Epoch 111
2025-10-02 20:20:26.666758: Current learning rate: 0.00297
2025-10-02 20:21:12.593205: Validation loss did not improve from -0.38021. Patience: 77/50
2025-10-02 20:21:12.593956: train_loss -0.7688
2025-10-02 20:21:12.594374: val_loss -0.1802
2025-10-02 20:21:12.594705: Pseudo dice [np.float32(0.5943)]
2025-10-02 20:21:12.594951: Epoch time: 45.93 s
2025-10-02 20:21:13.253340: 
2025-10-02 20:21:13.253659: Epoch 112
2025-10-02 20:21:13.253824: Current learning rate: 0.00291
2025-10-02 20:21:59.170841: Validation loss did not improve from -0.38021. Patience: 78/50
2025-10-02 20:21:59.171729: train_loss -0.7692
2025-10-02 20:21:59.172119: val_loss -0.16
2025-10-02 20:21:59.172451: Pseudo dice [np.float32(0.5844)]
2025-10-02 20:21:59.172759: Epoch time: 45.92 s
2025-10-02 20:21:59.829021: 
2025-10-02 20:21:59.829339: Epoch 113
2025-10-02 20:21:59.829495: Current learning rate: 0.00284
2025-10-02 20:22:45.719029: Validation loss did not improve from -0.38021. Patience: 79/50
2025-10-02 20:22:45.719600: train_loss -0.7744
2025-10-02 20:22:45.719759: val_loss -0.0077
2025-10-02 20:22:45.719904: Pseudo dice [np.float32(0.4886)]
2025-10-02 20:22:45.720305: Epoch time: 45.89 s
2025-10-02 20:22:46.378445: 
2025-10-02 20:22:46.378935: Epoch 114
2025-10-02 20:22:46.379080: Current learning rate: 0.00277
2025-10-02 20:23:32.270064: Validation loss did not improve from -0.38021. Patience: 80/50
2025-10-02 20:23:32.270535: train_loss -0.7759
2025-10-02 20:23:32.270698: val_loss -0.0652
2025-10-02 20:23:32.270831: Pseudo dice [np.float32(0.5418)]
2025-10-02 20:23:32.270956: Epoch time: 45.89 s
2025-10-02 20:23:33.349461: 
2025-10-02 20:23:33.349712: Epoch 115
2025-10-02 20:23:33.349877: Current learning rate: 0.0027
2025-10-02 20:24:19.286033: Validation loss did not improve from -0.38021. Patience: 81/50
2025-10-02 20:24:19.286754: train_loss -0.7722
2025-10-02 20:24:19.286891: val_loss -0.2088
2025-10-02 20:24:19.287074: Pseudo dice [np.float32(0.6157)]
2025-10-02 20:24:19.287249: Epoch time: 45.94 s
2025-10-02 20:24:19.949942: 
2025-10-02 20:24:19.950267: Epoch 116
2025-10-02 20:24:19.950477: Current learning rate: 0.00263
2025-10-02 20:25:05.858012: Validation loss did not improve from -0.38021. Patience: 82/50
2025-10-02 20:25:05.858442: train_loss -0.7749
2025-10-02 20:25:05.858575: val_loss -0.1433
2025-10-02 20:25:05.858686: Pseudo dice [np.float32(0.5642)]
2025-10-02 20:25:05.858809: Epoch time: 45.91 s
2025-10-02 20:25:06.527073: 
2025-10-02 20:25:06.527391: Epoch 117
2025-10-02 20:25:06.527555: Current learning rate: 0.00256
2025-10-02 20:25:52.426179: Validation loss did not improve from -0.38021. Patience: 83/50
2025-10-02 20:25:52.426965: train_loss -0.7717
2025-10-02 20:25:52.427285: val_loss -0.1206
2025-10-02 20:25:52.427478: Pseudo dice [np.float32(0.5483)]
2025-10-02 20:25:52.427654: Epoch time: 45.9 s
2025-10-02 20:25:53.090481: 
2025-10-02 20:25:53.090787: Epoch 118
2025-10-02 20:25:53.090968: Current learning rate: 0.00249
2025-10-02 20:26:39.047657: Validation loss did not improve from -0.38021. Patience: 84/50
2025-10-02 20:26:39.048574: train_loss -0.7841
2025-10-02 20:26:39.049040: val_loss -0.2477
2025-10-02 20:26:39.049448: Pseudo dice [np.float32(0.6347)]
2025-10-02 20:26:39.049892: Epoch time: 45.96 s
2025-10-02 20:26:39.715171: 
2025-10-02 20:26:39.715766: Epoch 119
2025-10-02 20:26:39.716258: Current learning rate: 0.00242
2025-10-02 20:27:25.696723: Validation loss did not improve from -0.38021. Patience: 85/50
2025-10-02 20:27:25.697254: train_loss -0.7838
2025-10-02 20:27:25.697422: val_loss -0.1507
2025-10-02 20:27:25.697577: Pseudo dice [np.float32(0.5839)]
2025-10-02 20:27:25.697702: Epoch time: 45.98 s
2025-10-02 20:27:27.148135: 
2025-10-02 20:27:27.148617: Epoch 120
2025-10-02 20:27:27.148842: Current learning rate: 0.00235
2025-10-02 20:28:13.097805: Validation loss did not improve from -0.38021. Patience: 86/50
2025-10-02 20:28:13.098388: train_loss -0.7852
2025-10-02 20:28:13.098595: val_loss -0.1866
2025-10-02 20:28:13.098775: Pseudo dice [np.float32(0.596)]
2025-10-02 20:28:13.098998: Epoch time: 45.95 s
2025-10-02 20:28:13.761667: 
2025-10-02 20:28:13.762026: Epoch 121
2025-10-02 20:28:13.762296: Current learning rate: 0.00228
2025-10-02 20:28:59.680219: Validation loss did not improve from -0.38021. Patience: 87/50
2025-10-02 20:28:59.680746: train_loss -0.7889
2025-10-02 20:28:59.680884: val_loss -0.0466
2025-10-02 20:28:59.681038: Pseudo dice [np.float32(0.5135)]
2025-10-02 20:28:59.681273: Epoch time: 45.92 s
2025-10-02 20:29:00.347776: 
2025-10-02 20:29:00.348097: Epoch 122
2025-10-02 20:29:00.348374: Current learning rate: 0.00221
2025-10-02 20:29:46.301831: Validation loss did not improve from -0.38021. Patience: 88/50
2025-10-02 20:29:46.302337: train_loss -0.7822
2025-10-02 20:29:46.302524: val_loss -0.1874
2025-10-02 20:29:46.302857: Pseudo dice [np.float32(0.606)]
2025-10-02 20:29:46.303066: Epoch time: 45.96 s
2025-10-02 20:29:46.968802: 
2025-10-02 20:29:46.969091: Epoch 123
2025-10-02 20:29:46.969274: Current learning rate: 0.00214
2025-10-02 20:30:32.940500: Validation loss did not improve from -0.38021. Patience: 89/50
2025-10-02 20:30:32.941090: train_loss -0.7866
2025-10-02 20:30:32.941226: val_loss -0.1274
2025-10-02 20:30:32.941335: Pseudo dice [np.float32(0.5809)]
2025-10-02 20:30:32.941458: Epoch time: 45.97 s
2025-10-02 20:30:33.611788: 
2025-10-02 20:30:33.611998: Epoch 124
2025-10-02 20:30:33.612131: Current learning rate: 0.00207
2025-10-02 20:31:19.579651: Validation loss did not improve from -0.38021. Patience: 90/50
2025-10-02 20:31:19.580106: train_loss -0.7959
2025-10-02 20:31:19.580286: val_loss -0.0684
2025-10-02 20:31:19.580403: Pseudo dice [np.float32(0.5207)]
2025-10-02 20:31:19.580552: Epoch time: 45.97 s
2025-10-02 20:31:20.670640: 
2025-10-02 20:31:20.670905: Epoch 125
2025-10-02 20:31:20.671044: Current learning rate: 0.00199
2025-10-02 20:32:06.596458: Validation loss did not improve from -0.38021. Patience: 91/50
2025-10-02 20:32:06.596974: train_loss -0.7927
2025-10-02 20:32:06.597140: val_loss -0.1985
2025-10-02 20:32:06.597251: Pseudo dice [np.float32(0.6096)]
2025-10-02 20:32:06.597375: Epoch time: 45.93 s
2025-10-02 20:32:07.260693: 
2025-10-02 20:32:07.261050: Epoch 126
2025-10-02 20:32:07.261259: Current learning rate: 0.00192
2025-10-02 20:32:53.217847: Validation loss did not improve from -0.38021. Patience: 92/50
2025-10-02 20:32:53.218276: train_loss -0.7931
2025-10-02 20:32:53.218429: val_loss -0.1911
2025-10-02 20:32:53.218563: Pseudo dice [np.float32(0.6073)]
2025-10-02 20:32:53.218704: Epoch time: 45.96 s
2025-10-02 20:32:53.884696: 
2025-10-02 20:32:53.885098: Epoch 127
2025-10-02 20:32:53.885345: Current learning rate: 0.00185
2025-10-02 20:33:39.877031: Validation loss did not improve from -0.38021. Patience: 93/50
2025-10-02 20:33:39.877915: train_loss -0.7945
2025-10-02 20:33:39.878195: val_loss -0.1402
2025-10-02 20:33:39.878489: Pseudo dice [np.float32(0.5821)]
2025-10-02 20:33:39.878777: Epoch time: 45.99 s
2025-10-02 20:33:40.551420: 
2025-10-02 20:33:40.551727: Epoch 128
2025-10-02 20:33:40.551902: Current learning rate: 0.00178
2025-10-02 20:34:26.568753: Validation loss did not improve from -0.38021. Patience: 94/50
2025-10-02 20:34:26.569271: train_loss -0.7965
2025-10-02 20:34:26.569411: val_loss -0.1034
2025-10-02 20:34:26.569545: Pseudo dice [np.float32(0.5657)]
2025-10-02 20:34:26.569666: Epoch time: 46.02 s
2025-10-02 20:34:27.225092: 
2025-10-02 20:34:27.225370: Epoch 129
2025-10-02 20:34:27.225554: Current learning rate: 0.0017
2025-10-02 20:35:13.141361: Validation loss did not improve from -0.38021. Patience: 95/50
2025-10-02 20:35:13.141906: train_loss -0.7981
2025-10-02 20:35:13.142059: val_loss -0.1387
2025-10-02 20:35:13.142209: Pseudo dice [np.float32(0.5771)]
2025-10-02 20:35:13.142384: Epoch time: 45.92 s
2025-10-02 20:35:14.241053: 
2025-10-02 20:35:14.241365: Epoch 130
2025-10-02 20:35:14.241558: Current learning rate: 0.00163
2025-10-02 20:36:00.130506: Validation loss did not improve from -0.38021. Patience: 96/50
2025-10-02 20:36:00.131104: train_loss -0.7993
2025-10-02 20:36:00.131366: val_loss -0.1296
2025-10-02 20:36:00.131546: Pseudo dice [np.float32(0.5636)]
2025-10-02 20:36:00.131708: Epoch time: 45.89 s
2025-10-02 20:36:00.796275: 
2025-10-02 20:36:00.796544: Epoch 131
2025-10-02 20:36:00.796685: Current learning rate: 0.00156
2025-10-02 20:36:46.636259: Validation loss did not improve from -0.38021. Patience: 97/50
2025-10-02 20:36:46.636910: train_loss -0.8069
2025-10-02 20:36:46.637103: val_loss 0.0179
2025-10-02 20:36:46.637237: Pseudo dice [np.float32(0.4961)]
2025-10-02 20:36:46.637382: Epoch time: 45.84 s
2025-10-02 20:36:47.285268: 
2025-10-02 20:36:47.285513: Epoch 132
2025-10-02 20:36:47.285655: Current learning rate: 0.00148
2025-10-02 20:37:33.177251: Validation loss did not improve from -0.38021. Patience: 98/50
2025-10-02 20:37:33.177965: train_loss -0.8007
2025-10-02 20:37:33.178326: val_loss -0.1152
2025-10-02 20:37:33.178695: Pseudo dice [np.float32(0.5539)]
2025-10-02 20:37:33.179001: Epoch time: 45.89 s
2025-10-02 20:37:33.834524: 
2025-10-02 20:37:33.834831: Epoch 133
2025-10-02 20:37:33.835029: Current learning rate: 0.00141
2025-10-02 20:38:19.751390: Validation loss did not improve from -0.38021. Patience: 99/50
2025-10-02 20:38:19.752137: train_loss -0.8075
2025-10-02 20:38:19.752326: val_loss -0.1404
2025-10-02 20:38:19.752462: Pseudo dice [np.float32(0.5761)]
2025-10-02 20:38:19.752581: Epoch time: 45.92 s
2025-10-02 20:38:20.401446: 
2025-10-02 20:38:20.401744: Epoch 134
2025-10-02 20:38:20.401914: Current learning rate: 0.00133
2025-10-02 20:39:06.299624: Validation loss did not improve from -0.38021. Patience: 100/50
2025-10-02 20:39:06.300086: train_loss -0.8087
2025-10-02 20:39:06.300221: val_loss -0.1195
2025-10-02 20:39:06.300333: Pseudo dice [np.float32(0.5823)]
2025-10-02 20:39:06.300451: Epoch time: 45.9 s
2025-10-02 20:39:07.751321: 
2025-10-02 20:39:07.751600: Epoch 135
2025-10-02 20:39:07.751741: Current learning rate: 0.00126
2025-10-02 20:39:53.731471: Validation loss did not improve from -0.38021. Patience: 101/50
2025-10-02 20:39:53.732221: train_loss -0.8049
2025-10-02 20:39:53.732455: val_loss -0.1098
2025-10-02 20:39:53.732624: Pseudo dice [np.float32(0.551)]
2025-10-02 20:39:53.732789: Epoch time: 45.98 s
2025-10-02 20:39:54.389848: 
2025-10-02 20:39:54.390357: Epoch 136
2025-10-02 20:39:54.390718: Current learning rate: 0.00118
2025-10-02 20:40:40.270515: Validation loss did not improve from -0.38021. Patience: 102/50
2025-10-02 20:40:40.270977: train_loss -0.8038
2025-10-02 20:40:40.271202: val_loss -0.1816
2025-10-02 20:40:40.271327: Pseudo dice [np.float32(0.596)]
2025-10-02 20:40:40.271490: Epoch time: 45.88 s
2025-10-02 20:40:40.924723: 
2025-10-02 20:40:40.925032: Epoch 137
2025-10-02 20:40:40.925208: Current learning rate: 0.00111
2025-10-02 20:41:26.879646: Validation loss did not improve from -0.38021. Patience: 103/50
2025-10-02 20:41:26.880202: train_loss -0.8068
2025-10-02 20:41:26.880350: val_loss -0.1858
2025-10-02 20:41:26.880482: Pseudo dice [np.float32(0.6064)]
2025-10-02 20:41:26.880632: Epoch time: 45.96 s
2025-10-02 20:41:27.540652: 
2025-10-02 20:41:27.540910: Epoch 138
2025-10-02 20:41:27.541107: Current learning rate: 0.00103
2025-10-02 20:42:13.481869: Validation loss did not improve from -0.38021. Patience: 104/50
2025-10-02 20:42:13.482379: train_loss -0.8072
2025-10-02 20:42:13.482555: val_loss -0.0576
2025-10-02 20:42:13.482715: Pseudo dice [np.float32(0.5213)]
2025-10-02 20:42:13.482877: Epoch time: 45.94 s
2025-10-02 20:42:14.136251: 
2025-10-02 20:42:14.136557: Epoch 139
2025-10-02 20:42:14.136726: Current learning rate: 0.00095
2025-10-02 20:43:00.044242: Validation loss did not improve from -0.38021. Patience: 105/50
2025-10-02 20:43:00.044818: train_loss -0.8116
2025-10-02 20:43:00.044970: val_loss -0.1212
2025-10-02 20:43:00.045107: Pseudo dice [np.float32(0.5619)]
2025-10-02 20:43:00.045246: Epoch time: 45.91 s
2025-10-02 20:43:01.134200: 
2025-10-02 20:43:01.134511: Epoch 140
2025-10-02 20:43:01.134721: Current learning rate: 0.00087
2025-10-02 20:43:47.144123: Validation loss did not improve from -0.38021. Patience: 106/50
2025-10-02 20:43:47.144617: train_loss -0.8153
2025-10-02 20:43:47.144792: val_loss -0.1487
2025-10-02 20:43:47.144913: Pseudo dice [np.float32(0.5883)]
2025-10-02 20:43:47.145065: Epoch time: 46.01 s
2025-10-02 20:43:47.803111: 
2025-10-02 20:43:47.803432: Epoch 141
2025-10-02 20:43:47.803637: Current learning rate: 0.00079
2025-10-02 20:44:33.746519: Validation loss did not improve from -0.38021. Patience: 107/50
2025-10-02 20:44:33.746902: train_loss -0.8163
2025-10-02 20:44:33.747064: val_loss -0.1228
2025-10-02 20:44:33.747182: Pseudo dice [np.float32(0.567)]
2025-10-02 20:44:33.747323: Epoch time: 45.94 s
2025-10-02 20:44:34.408815: 
2025-10-02 20:44:34.409045: Epoch 142
2025-10-02 20:44:34.409178: Current learning rate: 0.00071
2025-10-02 20:45:20.391092: Validation loss did not improve from -0.38021. Patience: 108/50
2025-10-02 20:45:20.391688: train_loss -0.8153
2025-10-02 20:45:20.391892: val_loss -0.0938
2025-10-02 20:45:20.392039: Pseudo dice [np.float32(0.5711)]
2025-10-02 20:45:20.392165: Epoch time: 45.98 s
2025-10-02 20:45:21.053635: 
2025-10-02 20:45:21.053956: Epoch 143
2025-10-02 20:45:21.054107: Current learning rate: 0.00063
2025-10-02 20:46:07.059827: Validation loss did not improve from -0.38021. Patience: 109/50
2025-10-02 20:46:07.060280: train_loss -0.8152
2025-10-02 20:46:07.060442: val_loss -0.1345
2025-10-02 20:46:07.060580: Pseudo dice [np.float32(0.5785)]
2025-10-02 20:46:07.060734: Epoch time: 46.01 s
2025-10-02 20:46:07.724699: 
2025-10-02 20:46:07.725029: Epoch 144
2025-10-02 20:46:07.725180: Current learning rate: 0.00055
2025-10-02 20:46:53.694645: Validation loss did not improve from -0.38021. Patience: 110/50
2025-10-02 20:46:53.695110: train_loss -0.8135
2025-10-02 20:46:53.695263: val_loss -0.1018
2025-10-02 20:46:53.695383: Pseudo dice [np.float32(0.5493)]
2025-10-02 20:46:53.695518: Epoch time: 45.97 s
2025-10-02 20:46:54.777657: 
2025-10-02 20:46:54.777963: Epoch 145
2025-10-02 20:46:54.778118: Current learning rate: 0.00047
2025-10-02 20:47:40.773576: Validation loss did not improve from -0.38021. Patience: 111/50
2025-10-02 20:47:40.774689: train_loss -0.8135
2025-10-02 20:47:40.775001: val_loss -0.1396
2025-10-02 20:47:40.775364: Pseudo dice [np.float32(0.5797)]
2025-10-02 20:47:40.775779: Epoch time: 46.0 s
2025-10-02 20:47:41.431873: 
2025-10-02 20:47:41.432181: Epoch 146
2025-10-02 20:47:41.432350: Current learning rate: 0.00038
2025-10-02 20:48:27.364366: Validation loss did not improve from -0.38021. Patience: 112/50
2025-10-02 20:48:27.365056: train_loss -0.8174
2025-10-02 20:48:27.365345: val_loss -0.1392
2025-10-02 20:48:27.365598: Pseudo dice [np.float32(0.5906)]
2025-10-02 20:48:27.365871: Epoch time: 45.93 s
2025-10-02 20:48:28.025670: 
2025-10-02 20:48:28.025948: Epoch 147
2025-10-02 20:48:28.026109: Current learning rate: 0.0003
2025-10-02 20:49:14.004073: Validation loss did not improve from -0.38021. Patience: 113/50
2025-10-02 20:49:14.004464: train_loss -0.8206
2025-10-02 20:49:14.004627: val_loss -0.1679
2025-10-02 20:49:14.004773: Pseudo dice [np.float32(0.5869)]
2025-10-02 20:49:14.004933: Epoch time: 45.98 s
2025-10-02 20:49:14.662973: 
2025-10-02 20:49:14.663493: Epoch 148
2025-10-02 20:49:14.663875: Current learning rate: 0.00021
2025-10-02 20:50:00.638756: Validation loss did not improve from -0.38021. Patience: 114/50
2025-10-02 20:50:00.639582: train_loss -0.8214
2025-10-02 20:50:00.640018: val_loss -0.1504
2025-10-02 20:50:00.640372: Pseudo dice [np.float32(0.5891)]
2025-10-02 20:50:00.640843: Epoch time: 45.98 s
2025-10-02 20:50:01.307268: 
2025-10-02 20:50:01.307617: Epoch 149
2025-10-02 20:50:01.307876: Current learning rate: 0.00011
2025-10-02 20:50:47.207960: Validation loss did not improve from -0.38021. Patience: 115/50
2025-10-02 20:50:47.208351: train_loss -0.815
2025-10-02 20:50:47.208513: val_loss -0.1612
2025-10-02 20:50:47.208670: Pseudo dice [np.float32(0.5879)]
2025-10-02 20:50:47.208832: Epoch time: 45.9 s
2025-10-02 20:50:48.700544: Training done.
2025-10-02 20:50:48.731231: Using splits from existing split file: /nfs/erelab001/shared/Computational_Group/Naravich/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset310_nnInteractive_Calcium_OCT_CrossValidation/splits_final.json
2025-10-02 20:50:48.731804: The split file contains 5 splits.
2025-10-02 20:50:48.732566: Desired fold for training: 3
2025-10-02 20:50:48.733206: This split has 7 training and 1 validation cases.
2025-10-02 20:50:48.733540: predicting 701-013
2025-10-02 20:50:48.736939: 701-013, shape torch.Size([1, 375, 498, 498]), rank 0
2025-10-02 20:51:47.752347: Validation complete
2025-10-02 20:51:47.752814: Mean Validation Dice:  0.563013702446567
Finished training fold 3 saving to /nfs/erelab001/shared/Computational_Group/Naravich/datasets/nnUNet_Datasets/nnUNet_results/Dataset310_nnInteractive_Calcium_OCT_CrossValidation/nnUNetTrainer__nnUNetPlans__3d_32x160x128_b10/fold_3_No_Pretrained
