/home/gridsan/nchutisilp/.conda/envs/nnunet/bin/python
Starting training with CONFIG=3d_32x160x128_b10, DATASET_ID=307, TRAINER=nnUNetTrainerScaleAnalysis80

############################
INFO: You are using the old nnU-Net default plans. We have updated our recommendations. Please consider using those instead! Read more here: https://github.com/MIC-DKFZ/nnUNet/blob/master/documentation/resenc_presets.md
############################

Using device: cuda:0

#######################################################################
Please cite the following paper when using nnU-Net:
Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.
#######################################################################

2024-12-13 20:43:24.182250: Using torch.compile...

############################
INFO: You are using the old nnU-Net default plans. We have updated our recommendations. Please consider using those instead! Read more here: https://github.com/MIC-DKFZ/nnUNet/blob/master/documentation/resenc_presets.md
############################

Using device: cuda:0

#######################################################################
Please cite the following paper when using nnU-Net:
Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.
#######################################################################

2024-12-13 20:43:24.181850: Using torch.compile...
/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
################### Loading pretrained weights from file  /home/gridsan/nchutisilp/projects/OpenAI-CLIP/clip_preivl_poststent_pretrained_nnUNet.pt ###################
Below is the list of overlapping blocks in pretrained model and nnUNet architecture:
encoder.stages.0.0.convs.0.conv.weight shape torch.Size([32, 1, 3, 3, 3])
encoder.stages.0.0.convs.0.conv.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.norm.weight shape torch.Size([32])
encoder.stages.0.0.convs.0.norm.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.0.weight shape torch.Size([32, 1, 3, 3, 3])
encoder.stages.0.0.convs.0.all_modules.0.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.1.weight shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.1.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
encoder.stages.0.0.convs.1.conv.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.norm.weight shape torch.Size([32])
encoder.stages.0.0.convs.1.norm.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
encoder.stages.0.0.convs.1.all_modules.0.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.1.weight shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.1.bias shape torch.Size([32])
encoder.stages.1.0.convs.0.conv.weight shape torch.Size([64, 32, 3, 3, 3])
encoder.stages.1.0.convs.0.conv.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.norm.weight shape torch.Size([64])
encoder.stages.1.0.convs.0.norm.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.0.weight shape torch.Size([64, 32, 3, 3, 3])
encoder.stages.1.0.convs.0.all_modules.0.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.1.weight shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.1.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
encoder.stages.1.0.convs.1.conv.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.norm.weight shape torch.Size([64])
encoder.stages.1.0.convs.1.norm.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
encoder.stages.1.0.convs.1.all_modules.0.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.1.weight shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.1.bias shape torch.Size([64])
encoder.stages.2.0.convs.0.conv.weight shape torch.Size([128, 64, 3, 3, 3])
encoder.stages.2.0.convs.0.conv.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.norm.weight shape torch.Size([128])
encoder.stages.2.0.convs.0.norm.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.0.weight shape torch.Size([128, 64, 3, 3, 3])
encoder.stages.2.0.convs.0.all_modules.0.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.1.weight shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.1.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
encoder.stages.2.0.convs.1.conv.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.norm.weight shape torch.Size([128])
encoder.stages.2.0.convs.1.norm.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
encoder.stages.2.0.convs.1.all_modules.0.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.1.weight shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.1.bias shape torch.Size([128])
encoder.stages.3.0.convs.0.conv.weight shape torch.Size([256, 128, 3, 3, 3])
encoder.stages.3.0.convs.0.conv.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.norm.weight shape torch.Size([256])
encoder.stages.3.0.convs.0.norm.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.0.weight shape torch.Size([256, 128, 3, 3, 3])
encoder.stages.3.0.convs.0.all_modules.0.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.1.weight shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.1.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
encoder.stages.3.0.convs.1.conv.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.norm.weight shape torch.Size([256])
encoder.stages.3.0.convs.1.norm.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
encoder.stages.3.0.convs.1.all_modules.0.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.1.weight shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.1.bias shape torch.Size([256])
encoder.stages.4.0.convs.0.conv.weight shape torch.Size([320, 256, 3, 3, 3])
encoder.stages.4.0.convs.0.conv.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.norm.weight shape torch.Size([320])
encoder.stages.4.0.convs.0.norm.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.0.weight shape torch.Size([320, 256, 3, 3, 3])
encoder.stages.4.0.convs.0.all_modules.0.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.1.weight shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.1.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.4.0.convs.1.conv.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.norm.weight shape torch.Size([320])
encoder.stages.4.0.convs.1.norm.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.4.0.convs.1.all_modules.0.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.1.weight shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.1.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.0.conv.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.norm.weight shape torch.Size([320])
encoder.stages.5.0.convs.0.norm.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.0.all_modules.0.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.1.weight shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.1.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.1.conv.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.norm.weight shape torch.Size([320])
encoder.stages.5.0.convs.1.norm.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.1.all_modules.0.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.1.weight shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.0.0.convs.0.conv.weight shape torch.Size([32, 1, 3, 3, 3])
decoder.encoder.stages.0.0.convs.0.conv.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.norm.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.norm.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.0.weight shape torch.Size([32, 1, 3, 3, 3])
decoder.encoder.stages.0.0.convs.0.all_modules.0.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.1.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.1.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.encoder.stages.0.0.convs.1.conv.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.norm.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.norm.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.encoder.stages.0.0.convs.1.all_modules.0.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.1.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.1.bias shape torch.Size([32])
decoder.encoder.stages.1.0.convs.0.conv.weight shape torch.Size([64, 32, 3, 3, 3])
decoder.encoder.stages.1.0.convs.0.conv.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.norm.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.norm.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.0.weight shape torch.Size([64, 32, 3, 3, 3])
decoder.encoder.stages.1.0.convs.0.all_modules.0.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.1.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.1.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.encoder.stages.1.0.convs.1.conv.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.norm.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.norm.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.encoder.stages.1.0.convs.1.all_modules.0.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.1.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.1.bias shape torch.Size([64])
decoder.encoder.stages.2.0.convs.0.conv.weight shape torch.Size([128, 64, 3, 3, 3])
decoder.encoder.stages.2.0.convs.0.conv.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.norm.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.norm.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.0.weight shape torch.Size([128, 64, 3, 3, 3])
decoder.encoder.stages.2.0.convs.0.all_modules.0.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.1.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.1.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.encoder.stages.2.0.convs.1.conv.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.norm.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.norm.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.encoder.stages.2.0.convs.1.all_modules.0.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.1.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.1.bias shape torch.Size([128])
decoder.encoder.stages.3.0.convs.0.conv.weight shape torch.Size([256, 128, 3, 3, 3])
decoder.encoder.stages.3.0.convs.0.conv.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.norm.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.norm.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.0.weight shape torch.Size([256, 128, 3, 3, 3])
decoder.encoder.stages.3.0.convs.0.all_modules.0.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.1.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.1.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.encoder.stages.3.0.convs.1.conv.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.norm.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.norm.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.encoder.stages.3.0.convs.1.all_modules.0.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.1.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.1.bias shape torch.Size([256])
decoder.encoder.stages.4.0.convs.0.conv.weight shape torch.Size([320, 256, 3, 3, 3])
decoder.encoder.stages.4.0.convs.0.conv.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.norm.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.norm.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.0.weight shape torch.Size([320, 256, 3, 3, 3])
decoder.encoder.stages.4.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.4.0.convs.1.conv.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.norm.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.norm.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.4.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.0.conv.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.norm.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.norm.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.1.conv.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.norm.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.norm.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.stages.0.convs.0.conv.weight shape torch.Size([320, 640, 3, 3, 3])
decoder.stages.0.convs.0.conv.bias shape torch.Size([320])
decoder.stages.0.convs.0.norm.weight shape torch.Size([320])
decoder.stages.0.convs.0.norm.bias shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.0.weight shape torch.Size([320, 640, 3, 3, 3])
decoder.stages.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.stages.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.stages.0.convs.1.conv.bias shape torch.Size([320])
decoder.stages.0.convs.1.norm.weight shape torch.Size([320])
decoder.stages.0.convs.1.norm.bias shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.stages.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.stages.1.convs.0.conv.weight shape torch.Size([256, 512, 3, 3, 3])
decoder.stages.1.convs.0.conv.bias shape torch.Size([256])
decoder.stages.1.convs.0.norm.weight shape torch.Size([256])
decoder.stages.1.convs.0.norm.bias shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.0.weight shape torch.Size([256, 512, 3, 3, 3])
decoder.stages.1.convs.0.all_modules.0.bias shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.1.weight shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.1.bias shape torch.Size([256])
decoder.stages.1.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.stages.1.convs.1.conv.bias shape torch.Size([256])
decoder.stages.1.convs.1.norm.weight shape torch.Size([256])
decoder.stages.1.convs.1.norm.bias shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.stages.1.convs.1.all_modules.0.bias shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.1.weight shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.1.bias shape torch.Size([256])
decoder.stages.2.convs.0.conv.weight shape torch.Size([128, 256, 3, 3, 3])
decoder.stages.2.convs.0.conv.bias shape torch.Size([128])
decoder.stages.2.convs.0.norm.weight shape torch.Size([128])
decoder.stages.2.convs.0.norm.bias shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.0.weight shape torch.Size([128, 256, 3, 3, 3])
decoder.stages.2.convs.0.all_modules.0.bias shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.1.weight shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.1.bias shape torch.Size([128])
decoder.stages.2.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.stages.2.convs.1.conv.bias shape torch.Size([128])
decoder.stages.2.convs.1.norm.weight shape torch.Size([128])
decoder.stages.2.convs.1.norm.bias shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.stages.2.convs.1.all_modules.0.bias shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.1.weight shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.1.bias shape torch.Size([128])
decoder.stages.3.convs.0.conv.weight shape torch.Size([64, 128, 3, 3, 3])
decoder.stages.3.convs.0.conv.bias shape torch.Size([64])
decoder.stages.3.convs.0.norm.weight shape torch.Size([64])
decoder.stages.3.convs.0.norm.bias shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.0.weight shape torch.Size([64, 128, 3, 3, 3])
decoder.stages.3.convs.0.all_modules.0.bias shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.1.weight shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.1.bias shape torch.Size([64])
decoder.stages.3.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.stages.3.convs.1.conv.bias shape torch.Size([64])
decoder.stages.3.convs.1.norm.weight shape torch.Size([64])
decoder.stages.3.convs.1.norm.bias shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.stages.3.convs.1.all_modules.0.bias shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.1.weight shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.1.bias shape torch.Size([64])
decoder.stages.4.convs.0.conv.weight shape torch.Size([32, 64, 3, 3, 3])
decoder.stages.4.convs.0.conv.bias shape torch.Size([32])
decoder.stages.4.convs.0.norm.weight shape torch.Size([32])
decoder.stages.4.convs.0.norm.bias shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.0.weight shape torch.Size([32, 64, 3, 3, 3])
decoder.stages.4.convs.0.all_modules.0.bias shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.1.weight shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.1.bias shape torch.Size([32])
decoder.stages.4.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.stages.4.convs.1.conv.bias shape torch.Size([32])
decoder.stages.4.convs.1.norm.weight shape torch.Size([32])
decoder.stages.4.convs.1.norm.bias shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.stages.4.convs.1.all_modules.0.bias shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.1.weight shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.1.bias shape torch.Size([32])
decoder.transpconvs.0.weight shape torch.Size([320, 320, 1, 2, 2])
decoder.transpconvs.0.bias shape torch.Size([320])
decoder.transpconvs.1.weight shape torch.Size([320, 256, 2, 2, 2])
decoder.transpconvs.1.bias shape torch.Size([256])
decoder.transpconvs.2.weight shape torch.Size([256, 128, 2, 2, 2])
decoder.transpconvs.2.bias shape torch.Size([128])
decoder.transpconvs.3.weight shape torch.Size([128, 64, 2, 2, 2])
decoder.transpconvs.3.bias shape torch.Size([64])
decoder.transpconvs.4.weight shape torch.Size([64, 32, 2, 2, 2])
decoder.transpconvs.4.bias shape torch.Size([32])
################### Done ###################
2024-12-13 20:43:37.139131: do_dummy_2d_data_aug: True
2024-12-13 20:43:37.141234: /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset307_Sohee_Calcium_OCT_CrossValidation/nnUNetPlans_3d_fullres
2024-12-13 20:43:37.144414: Creating new 5-fold cross-validation split...
2024-12-13 20:43:37.172920: Desired fold for training: 0
2024-12-13 20:43:37.174799: This split has 6 training and 4 validation cases.
################### Loading pretrained weights from file  /home/gridsan/nchutisilp/projects/OpenAI-CLIP/clip_preivl_poststent_pretrained_nnUNet.pt ###################
Below is the list of overlapping blocks in pretrained model and nnUNet architecture:
encoder.stages.0.0.convs.0.conv.weight shape torch.Size([32, 1, 3, 3, 3])
encoder.stages.0.0.convs.0.conv.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.norm.weight shape torch.Size([32])
encoder.stages.0.0.convs.0.norm.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.0.weight shape torch.Size([32, 1, 3, 3, 3])
encoder.stages.0.0.convs.0.all_modules.0.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.1.weight shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.1.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
encoder.stages.0.0.convs.1.conv.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.norm.weight shape torch.Size([32])
encoder.stages.0.0.convs.1.norm.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
encoder.stages.0.0.convs.1.all_modules.0.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.1.weight shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.1.bias shape torch.Size([32])
encoder.stages.1.0.convs.0.conv.weight shape torch.Size([64, 32, 3, 3, 3])
encoder.stages.1.0.convs.0.conv.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.norm.weight shape torch.Size([64])
encoder.stages.1.0.convs.0.norm.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.0.weight shape torch.Size([64, 32, 3, 3, 3])
encoder.stages.1.0.convs.0.all_modules.0.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.1.weight shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.1.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
encoder.stages.1.0.convs.1.conv.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.norm.weight shape torch.Size([64])
encoder.stages.1.0.convs.1.norm.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
encoder.stages.1.0.convs.1.all_modules.0.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.1.weight shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.1.bias shape torch.Size([64])
encoder.stages.2.0.convs.0.conv.weight shape torch.Size([128, 64, 3, 3, 3])
encoder.stages.2.0.convs.0.conv.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.norm.weight shape torch.Size([128])
encoder.stages.2.0.convs.0.norm.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.0.weight shape torch.Size([128, 64, 3, 3, 3])
encoder.stages.2.0.convs.0.all_modules.0.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.1.weight shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.1.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
encoder.stages.2.0.convs.1.conv.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.norm.weight shape torch.Size([128])
encoder.stages.2.0.convs.1.norm.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
encoder.stages.2.0.convs.1.all_modules.0.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.1.weight shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.1.bias shape torch.Size([128])
encoder.stages.3.0.convs.0.conv.weight shape torch.Size([256, 128, 3, 3, 3])
encoder.stages.3.0.convs.0.conv.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.norm.weight shape torch.Size([256])
encoder.stages.3.0.convs.0.norm.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.0.weight shape torch.Size([256, 128, 3, 3, 3])
encoder.stages.3.0.convs.0.all_modules.0.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.1.weight shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.1.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
encoder.stages.3.0.convs.1.conv.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.norm.weight shape torch.Size([256])
encoder.stages.3.0.convs.1.norm.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
encoder.stages.3.0.convs.1.all_modules.0.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.1.weight shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.1.bias shape torch.Size([256])
encoder.stages.4.0.convs.0.conv.weight shape torch.Size([320, 256, 3, 3, 3])
encoder.stages.4.0.convs.0.conv.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.norm.weight shape torch.Size([320])
encoder.stages.4.0.convs.0.norm.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.0.weight shape torch.Size([320, 256, 3, 3, 3])
encoder.stages.4.0.convs.0.all_modules.0.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.1.weight shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.1.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.4.0.convs.1.conv.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.norm.weight shape torch.Size([320])
encoder.stages.4.0.convs.1.norm.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.4.0.convs.1.all_modules.0.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.1.weight shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.1.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.0.conv.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.norm.weight shape torch.Size([320])
encoder.stages.5.0.convs.0.norm.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.0.all_modules.0.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.1.weight shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.1.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.1.conv.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.norm.weight shape torch.Size([320])
encoder.stages.5.0.convs.1.norm.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.1.all_modules.0.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.1.weight shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.0.0.convs.0.conv.weight shape torch.Size([32, 1, 3, 3, 3])
decoder.encoder.stages.0.0.convs.0.conv.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.norm.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.norm.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.0.weight shape torch.Size([32, 1, 3, 3, 3])
decoder.encoder.stages.0.0.convs.0.all_modules.0.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.1.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.1.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.encoder.stages.0.0.convs.1.conv.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.norm.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.norm.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.encoder.stages.0.0.convs.1.all_modules.0.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.1.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.1.bias shape torch.Size([32])
decoder.encoder.stages.1.0.convs.0.conv.weight shape torch.Size([64, 32, 3, 3, 3])
decoder.encoder.stages.1.0.convs.0.conv.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.norm.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.norm.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.0.weight shape torch.Size([64, 32, 3, 3, 3])
decoder.encoder.stages.1.0.convs.0.all_modules.0.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.1.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.1.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.encoder.stages.1.0.convs.1.conv.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.norm.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.norm.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.encoder.stages.1.0.convs.1.all_modules.0.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.1.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.1.bias shape torch.Size([64])
decoder.encoder.stages.2.0.convs.0.conv.weight shape torch.Size([128, 64, 3, 3, 3])
decoder.encoder.stages.2.0.convs.0.conv.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.norm.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.norm.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.0.weight shape torch.Size([128, 64, 3, 3, 3])
decoder.encoder.stages.2.0.convs.0.all_modules.0.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.1.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.1.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.encoder.stages.2.0.convs.1.conv.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.norm.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.norm.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.encoder.stages.2.0.convs.1.all_modules.0.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.1.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.1.bias shape torch.Size([128])
decoder.encoder.stages.3.0.convs.0.conv.weight shape torch.Size([256, 128, 3, 3, 3])
decoder.encoder.stages.3.0.convs.0.conv.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.norm.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.norm.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.0.weight shape torch.Size([256, 128, 3, 3, 3])
decoder.encoder.stages.3.0.convs.0.all_modules.0.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.1.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.1.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.encoder.stages.3.0.convs.1.conv.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.norm.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.norm.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.encoder.stages.3.0.convs.1.all_modules.0.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.1.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.1.bias shape torch.Size([256])
decoder.encoder.stages.4.0.convs.0.conv.weight shape torch.Size([320, 256, 3, 3, 3])
decoder.encoder.stages.4.0.convs.0.conv.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.norm.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.norm.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.0.weight shape torch.Size([320, 256, 3, 3, 3])
decoder.encoder.stages.4.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.4.0.convs.1.conv.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.norm.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.norm.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.4.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.0.conv.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.norm.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.norm.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.1.conv.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.norm.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.norm.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.stages.0.convs.0.conv.weight shape torch.Size([320, 640, 3, 3, 3])
decoder.stages.0.convs.0.conv.bias shape torch.Size([320])
decoder.stages.0.convs.0.norm.weight shape torch.Size([320])
decoder.stages.0.convs.0.norm.bias shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.0.weight shape torch.Size([320, 640, 3, 3, 3])
decoder.stages.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.stages.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.stages.0.convs.1.conv.bias shape torch.Size([320])
decoder.stages.0.convs.1.norm.weight shape torch.Size([320])
decoder.stages.0.convs.1.norm.bias shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.stages.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.stages.1.convs.0.conv.weight shape torch.Size([256, 512, 3, 3, 3])
decoder.stages.1.convs.0.conv.bias shape torch.Size([256])
decoder.stages.1.convs.0.norm.weight shape torch.Size([256])
decoder.stages.1.convs.0.norm.bias shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.0.weight shape torch.Size([256, 512, 3, 3, 3])
decoder.stages.1.convs.0.all_modules.0.bias shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.1.weight shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.1.bias shape torch.Size([256])
decoder.stages.1.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.stages.1.convs.1.conv.bias shape torch.Size([256])
decoder.stages.1.convs.1.norm.weight shape torch.Size([256])
decoder.stages.1.convs.1.norm.bias shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.stages.1.convs.1.all_modules.0.bias shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.1.weight shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.1.bias shape torch.Size([256])
decoder.stages.2.convs.0.conv.weight shape torch.Size([128, 256, 3, 3, 3])
decoder.stages.2.convs.0.conv.bias shape torch.Size([128])
decoder.stages.2.convs.0.norm.weight shape torch.Size([128])
decoder.stages.2.convs.0.norm.bias shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.0.weight shape torch.Size([128, 256, 3, 3, 3])
decoder.stages.2.convs.0.all_modules.0.bias shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.1.weight shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.1.bias shape torch.Size([128])
decoder.stages.2.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.stages.2.convs.1.conv.bias shape torch.Size([128])
decoder.stages.2.convs.1.norm.weight shape torch.Size([128])
decoder.stages.2.convs.1.norm.bias shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.stages.2.convs.1.all_modules.0.bias shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.1.weight shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.1.bias shape torch.Size([128])
decoder.stages.3.convs.0.conv.weight shape torch.Size([64, 128, 3, 3, 3])
decoder.stages.3.convs.0.conv.bias shape torch.Size([64])
decoder.stages.3.convs.0.norm.weight shape torch.Size([64])
decoder.stages.3.convs.0.norm.bias shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.0.weight shape torch.Size([64, 128, 3, 3, 3])
decoder.stages.3.convs.0.all_modules.0.bias shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.1.weight shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.1.bias shape torch.Size([64])
decoder.stages.3.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.stages.3.convs.1.conv.bias shape torch.Size([64])
decoder.stages.3.convs.1.norm.weight shape torch.Size([64])
decoder.stages.3.convs.1.norm.bias shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.stages.3.convs.1.all_modules.0.bias shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.1.weight shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.1.bias shape torch.Size([64])
decoder.stages.4.convs.0.conv.weight shape torch.Size([32, 64, 3, 3, 3])
decoder.stages.4.convs.0.conv.bias shape torch.Size([32])
decoder.stages.4.convs.0.norm.weight shape torch.Size([32])
decoder.stages.4.convs.0.norm.bias shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.0.weight shape torch.Size([32, 64, 3, 3, 3])
decoder.stages.4.convs.0.all_modules.0.bias shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.1.weight shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.1.bias shape torch.Size([32])
decoder.stages.4.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.stages.4.convs.1.conv.bias shape torch.Size([32])
decoder.stages.4.convs.1.norm.weight shape torch.Size([32])
decoder.stages.4.convs.1.norm.bias shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.stages.4.convs.1.all_modules.0.bias shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.1.weight shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.1.bias shape torch.Size([32])
decoder.transpconvs.0.weight shape torch.Size([320, 320, 1, 2, 2])
decoder.transpconvs.0.bias shape torch.Size([320])
decoder.transpconvs.1.weight shape torch.Size([320, 256, 2, 2, 2])
decoder.transpconvs.1.bias shape torch.Size([256])
decoder.transpconvs.2.weight shape torch.Size([256, 128, 2, 2, 2])
decoder.transpconvs.2.bias shape torch.Size([128])
decoder.transpconvs.3.weight shape torch.Size([128, 64, 2, 2, 2])
decoder.transpconvs.3.bias shape torch.Size([64])
decoder.transpconvs.4.weight shape torch.Size([64, 32, 2, 2, 2])
decoder.transpconvs.4.bias shape torch.Size([32])
################### Done ###################
2024-12-13 20:43:37.139133: do_dummy_2d_data_aug: True
2024-12-13 20:43:37.141516: /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset307_Sohee_Calcium_OCT_CrossValidation/nnUNetPlans_3d_fullres
2024-12-13 20:43:37.144290: Creating new 5-fold cross-validation split...
2024-12-13 20:43:37.174089: Desired fold for training: 1
2024-12-13 20:43:37.175724: This split has 6 training and 4 validation cases.
using pin_memory on device 0
using pin_memory on device 0
using pin_memory on device 0

This is the configuration used by this training:
Configuration name: 3d_32x160x128_b10
 {'data_identifier': 'nnUNetPlans_3d_fullres', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 10, 'patch_size': [32, 160, 128], 'median_image_size_in_voxels': [375.0, 498.0, 498.0], 'spacing': [1.0, 1.0, 1.0], 'normalization_schemes': ['ZScoreNormalization'], 'use_mask_for_norm': [False], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'architecture': {'network_class_name': 'dynamic_network_architectures.architectures.unet.PlainConvUNet', 'arch_kwargs': {'n_stages': 6, 'features_per_stage': [32, 64, 128, 256, 320, 320], 'conv_op': 'torch.nn.modules.conv.Conv3d', 'kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]], 'strides': [[1, 1, 1], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [1, 2, 2]], 'n_conv_per_stage': [2, 2, 2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2, 2, 2], 'conv_bias': True, 'norm_op': 'torch.nn.modules.instancenorm.InstanceNorm3d', 'norm_op_kwargs': {'eps': 1e-05, 'affine': True}, 'dropout_op': None, 'dropout_op_kwargs': None, 'nonlin': 'torch.nn.LeakyReLU', 'nonlin_kwargs': {'inplace': True}, 'deep_supervision': True}, '_kw_requires_import': ['conv_op', 'norm_op', 'dropout_op', 'nonlin']}, 'batch_dice': True, 'inherits_from': '3d_fullres'} 

These are the global plan.json settings:
 {'dataset_name': 'Dataset307_Sohee_Calcium_OCT_CrossValidation', 'plans_name': 'nnUNetPlans', 'original_median_spacing_after_transp': [1.0, 1.0, 1.0], 'original_median_shape_after_transp': [375, 498, 498], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 0.4977976381778717, 'mean': 0.13507525622844696, 'median': 0.09599608182907104, 'min': 0.0, 'percentile_00_5': 0.014754901640117168, 'percentile_99_5': 0.4977976381778717, 'std': 0.12025152146816254}}} 

2024-12-13 20:44:04.632447: unpacking dataset...
using pin_memory on device 0

This is the configuration used by this training:
Configuration name: 3d_32x160x128_b10
 {'data_identifier': 'nnUNetPlans_3d_fullres', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 10, 'patch_size': [32, 160, 128], 'median_image_size_in_voxels': [375.0, 498.0, 498.0], 'spacing': [1.0, 1.0, 1.0], 'normalization_schemes': ['ZScoreNormalization'], 'use_mask_for_norm': [False], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'architecture': {'network_class_name': 'dynamic_network_architectures.architectures.unet.PlainConvUNet', 'arch_kwargs': {'n_stages': 6, 'features_per_stage': [32, 64, 128, 256, 320, 320], 'conv_op': 'torch.nn.modules.conv.Conv3d', 'kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]], 'strides': [[1, 1, 1], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [1, 2, 2]], 'n_conv_per_stage': [2, 2, 2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2, 2, 2], 'conv_bias': True, 'norm_op': 'torch.nn.modules.instancenorm.InstanceNorm3d', 'norm_op_kwargs': {'eps': 1e-05, 'affine': True}, 'dropout_op': None, 'dropout_op_kwargs': None, 'nonlin': 'torch.nn.LeakyReLU', 'nonlin_kwargs': {'inplace': True}, 'deep_supervision': True}, '_kw_requires_import': ['conv_op', 'norm_op', 'dropout_op', 'nonlin']}, 'batch_dice': True, 'inherits_from': '3d_fullres'} 

These are the global plan.json settings:
 {'dataset_name': 'Dataset307_Sohee_Calcium_OCT_CrossValidation', 'plans_name': 'nnUNetPlans', 'original_median_spacing_after_transp': [1.0, 1.0, 1.0], 'original_median_shape_after_transp': [375, 498, 498], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 0.4977976381778717, 'mean': 0.13507525622844696, 'median': 0.09599608182907104, 'min': 0.0, 'percentile_00_5': 0.014754901640117168, 'percentile_99_5': 0.4977976381778717, 'std': 0.12025152146816254}}} 

2024-12-13 20:44:04.681478: unpacking dataset...
2024-12-13 20:44:11.186079: unpacking done...
2024-12-13 20:44:11.293895: Unable to plot network architecture: nnUNet_compile is enabled!
2024-12-13 20:44:11.337575: 
2024-12-13 20:44:11.338740: Epoch 0
2024-12-13 20:44:11.339835: Current learning rate: 0.01
2024-12-13 20:51:27.298718: Validation loss improved from 1000.00000 to -0.15348! Patience: 0/50
2024-12-13 20:51:27.300343: train_loss -0.0863
2024-12-13 20:51:27.301762: val_loss -0.1535
2024-12-13 20:51:27.302642: Pseudo dice [0.5032]
2024-12-13 20:51:27.303526: Epoch time: 435.96 s
2024-12-13 20:51:27.304403: Yayy! New best EMA pseudo Dice: 0.5032
2024-12-13 20:51:29.332103: 
2024-12-13 20:51:29.333277: Epoch 1
2024-12-13 20:51:29.334242: Current learning rate: 0.00994
2024-12-13 20:57:02.503252: Validation loss improved from -0.15348 to -0.22645! Patience: 0/50
2024-12-13 20:57:02.504300: train_loss -0.2402
2024-12-13 20:57:02.505183: val_loss -0.2264
2024-12-13 20:57:02.506074: Pseudo dice [0.5517]
2024-12-13 20:57:02.507004: Epoch time: 333.17 s
2024-12-13 20:57:02.507849: Yayy! New best EMA pseudo Dice: 0.5081
2024-12-13 20:57:04.308691: 
2024-12-13 20:57:04.310022: Epoch 2
2024-12-13 20:57:04.310848: Current learning rate: 0.00988
2024-12-13 21:02:29.518004: Validation loss improved from -0.22645 to -0.25849! Patience: 0/50
2024-12-13 21:02:29.518921: train_loss -0.2871
2024-12-13 21:02:29.519588: val_loss -0.2585
2024-12-13 21:02:29.520162: Pseudo dice [0.6016]
2024-12-13 21:02:29.520813: Epoch time: 325.21 s
2024-12-13 21:02:29.521389: Yayy! New best EMA pseudo Dice: 0.5174
2024-12-13 21:02:31.340996: 
2024-12-13 21:02:31.342390: Epoch 3
2024-12-13 21:02:31.343629: Current learning rate: 0.00982
2024-12-13 21:08:17.330666: Validation loss improved from -0.25849 to -0.29103! Patience: 0/50
2024-12-13 21:08:17.331411: train_loss -0.3325
2024-12-13 21:08:17.332146: val_loss -0.291
2024-12-13 21:08:17.332747: Pseudo dice [0.6017]
2024-12-13 21:08:17.333537: Epoch time: 345.99 s
2024-12-13 21:08:17.334339: Yayy! New best EMA pseudo Dice: 0.5259
2024-12-13 21:08:19.138947: 
2024-12-13 21:08:19.140033: Epoch 4
2024-12-13 21:08:19.140943: Current learning rate: 0.00976
2024-12-13 21:13:33.642834: Validation loss improved from -0.29103 to -0.31480! Patience: 0/50
2024-12-13 21:13:33.643740: train_loss -0.3619
2024-12-13 21:13:33.644636: val_loss -0.3148
2024-12-13 21:13:33.645313: Pseudo dice [0.6188]
2024-12-13 21:13:33.646068: Epoch time: 314.51 s
2024-12-13 21:13:34.002112: Yayy! New best EMA pseudo Dice: 0.5352
2024-12-13 21:13:35.797071: 
2024-12-13 21:13:35.798346: Epoch 5
2024-12-13 21:13:35.799134: Current learning rate: 0.0097
2024-12-13 21:19:39.062012: Validation loss improved from -0.31480 to -0.34282! Patience: 0/50
2024-12-13 21:19:39.062905: train_loss -0.3746
2024-12-13 21:19:39.063831: val_loss -0.3428
2024-12-13 21:19:39.064525: Pseudo dice [0.6407]
2024-12-13 21:19:39.065192: Epoch time: 363.27 s
2024-12-13 21:19:39.065811: Yayy! New best EMA pseudo Dice: 0.5457
2024-12-13 21:19:40.763021: 
2024-12-13 21:19:40.764537: Epoch 6
2024-12-13 21:19:40.765546: Current learning rate: 0.00964
2024-12-13 21:25:59.376828: Validation loss improved from -0.34282 to -0.34897! Patience: 0/50
2024-12-13 21:25:59.377789: train_loss -0.4191
2024-12-13 21:25:59.378682: val_loss -0.349
2024-12-13 21:25:59.379353: Pseudo dice [0.6444]
2024-12-13 21:25:59.380160: Epoch time: 378.62 s
2024-12-13 21:25:59.380832: Yayy! New best EMA pseudo Dice: 0.5556
2024-12-13 21:26:01.139531: 
2024-12-13 21:26:01.140824: Epoch 7
2024-12-13 21:26:01.141622: Current learning rate: 0.00958
2024-12-13 21:31:53.706062: Validation loss improved from -0.34897 to -0.39166! Patience: 0/50
2024-12-13 21:31:53.707147: train_loss -0.4306
2024-12-13 21:31:53.708364: val_loss -0.3917
2024-12-13 21:31:53.709468: Pseudo dice [0.6679]
2024-12-13 21:31:53.710457: Epoch time: 352.57 s
2024-12-13 21:31:53.711106: Yayy! New best EMA pseudo Dice: 0.5668
2024-12-13 21:31:55.870719: 
2024-12-13 21:31:55.872343: Epoch 8
2024-12-13 21:31:55.873276: Current learning rate: 0.00952
2024-12-13 21:37:37.644693: Validation loss did not improve from -0.39166. Patience: 1/50
2024-12-13 21:37:37.645695: train_loss -0.4515
2024-12-13 21:37:37.646770: val_loss -0.3831
2024-12-13 21:37:37.647725: Pseudo dice [0.6732]
2024-12-13 21:37:37.648883: Epoch time: 341.78 s
2024-12-13 21:37:37.649936: Yayy! New best EMA pseudo Dice: 0.5775
2024-12-13 21:37:39.439925: 
2024-12-13 21:37:39.441290: Epoch 9
2024-12-13 21:37:39.442813: Current learning rate: 0.00946
2024-12-13 21:43:57.050630: Validation loss did not improve from -0.39166. Patience: 2/50
2024-12-13 21:43:57.051564: train_loss -0.4693
2024-12-13 21:43:57.052425: val_loss -0.3807
2024-12-13 21:43:57.053081: Pseudo dice [0.6721]
2024-12-13 21:43:57.053775: Epoch time: 377.61 s
2024-12-13 21:43:57.460797: Yayy! New best EMA pseudo Dice: 0.5869
2024-12-13 21:43:59.151225: 
2024-12-13 21:43:59.152371: Epoch 10
2024-12-13 21:43:59.153037: Current learning rate: 0.0094
2024-12-13 21:50:20.387862: Validation loss did not improve from -0.39166. Patience: 3/50
2024-12-13 21:50:20.393650: train_loss -0.4665
2024-12-13 21:50:20.395581: val_loss -0.3785
2024-12-13 21:50:20.397352: Pseudo dice [0.6702]
2024-12-13 21:50:20.400113: Epoch time: 381.24 s
2024-12-13 21:50:20.401917: Yayy! New best EMA pseudo Dice: 0.5952
2024-12-13 21:50:22.175710: 
2024-12-13 21:50:22.177119: Epoch 11
2024-12-13 21:50:22.177963: Current learning rate: 0.00934
2024-12-13 21:56:11.027430: Validation loss did not improve from -0.39166. Patience: 4/50
2024-12-13 21:56:11.028408: train_loss -0.4931
2024-12-13 21:56:11.029495: val_loss -0.3782
2024-12-13 21:56:11.030710: Pseudo dice [0.6742]
2024-12-13 21:56:11.031616: Epoch time: 348.85 s
2024-12-13 21:56:11.032950: Yayy! New best EMA pseudo Dice: 0.6031
2024-12-13 21:56:12.775935: 
2024-12-13 21:56:12.777397: Epoch 12
2024-12-13 21:56:12.778486: Current learning rate: 0.00928
2024-12-13 22:01:48.678461: Validation loss improved from -0.39166 to -0.41527! Patience: 4/50
2024-12-13 22:01:48.679415: train_loss -0.494
2024-12-13 22:01:48.681518: val_loss -0.4153
2024-12-13 22:01:48.682545: Pseudo dice [0.6831]
2024-12-13 22:01:48.683702: Epoch time: 335.9 s
2024-12-13 22:01:48.684507: Yayy! New best EMA pseudo Dice: 0.6111
2024-12-13 22:01:50.470865: 
2024-12-13 22:01:50.472050: Epoch 13
2024-12-13 22:01:50.472707: Current learning rate: 0.00922
2024-12-13 22:08:09.873357: Validation loss did not improve from -0.41527. Patience: 1/50
2024-12-13 22:08:09.874268: train_loss -0.4891
2024-12-13 22:08:09.875152: val_loss -0.4024
2024-12-13 22:08:09.876079: Pseudo dice [0.6665]
2024-12-13 22:08:09.877105: Epoch time: 379.4 s
2024-12-13 22:08:09.878087: Yayy! New best EMA pseudo Dice: 0.6167
2024-12-13 22:08:12.206927: 
2024-12-13 22:08:12.209016: Epoch 14
2024-12-13 22:08:12.210273: Current learning rate: 0.00916
2024-12-13 22:14:21.408322: Validation loss improved from -0.41527 to -0.45173! Patience: 1/50
2024-12-13 22:14:21.409315: train_loss -0.5089
2024-12-13 22:14:21.410260: val_loss -0.4517
2024-12-13 22:14:21.411190: Pseudo dice [0.7007]
2024-12-13 22:14:21.412314: Epoch time: 369.2 s
2024-12-13 22:14:21.746771: Yayy! New best EMA pseudo Dice: 0.6251
2024-12-13 22:14:23.525913: 
2024-12-13 22:14:23.527323: Epoch 15
2024-12-13 22:14:23.528233: Current learning rate: 0.0091
2024-12-13 22:20:06.615960: Validation loss did not improve from -0.45173. Patience: 1/50
2024-12-13 22:20:06.616903: train_loss -0.5266
2024-12-13 22:20:06.617904: val_loss -0.4218
2024-12-13 22:20:06.618885: Pseudo dice [0.6831]
2024-12-13 22:20:06.619753: Epoch time: 343.09 s
2024-12-13 22:20:06.620468: Yayy! New best EMA pseudo Dice: 0.6309
2024-12-13 22:20:08.421865: 
2024-12-13 22:20:08.422941: Epoch 16
2024-12-13 22:20:08.423692: Current learning rate: 0.00903
2024-12-13 22:26:27.764147: Validation loss did not improve from -0.45173. Patience: 2/50
2024-12-13 22:26:27.764947: train_loss -0.5281
2024-12-13 22:26:27.765765: val_loss -0.4248
2024-12-13 22:26:27.766538: Pseudo dice [0.6938]
2024-12-13 22:26:27.767183: Epoch time: 379.34 s
2024-12-13 22:26:27.767927: Yayy! New best EMA pseudo Dice: 0.6372
2024-12-13 22:26:29.601745: 
2024-12-13 22:26:29.603174: Epoch 17
2024-12-13 22:26:29.604506: Current learning rate: 0.00897
2024-12-13 22:32:30.421794: Validation loss improved from -0.45173 to -0.45652! Patience: 2/50
2024-12-13 22:32:30.422942: train_loss -0.5361
2024-12-13 22:32:30.423957: val_loss -0.4565
2024-12-13 22:32:30.424722: Pseudo dice [0.7103]
2024-12-13 22:32:30.425588: Epoch time: 360.82 s
2024-12-13 22:32:30.426362: Yayy! New best EMA pseudo Dice: 0.6445
2024-12-13 22:32:32.544579: 
2024-12-13 22:32:32.545794: Epoch 18
2024-12-13 22:32:32.546578: Current learning rate: 0.00891
2024-12-13 22:38:46.170797: Validation loss did not improve from -0.45652. Patience: 1/50
2024-12-13 22:38:46.171687: train_loss -0.5428
2024-12-13 22:38:46.172490: val_loss -0.4537
2024-12-13 22:38:46.173221: Pseudo dice [0.6969]
2024-12-13 22:38:46.173919: Epoch time: 373.63 s
2024-12-13 22:38:46.174590: Yayy! New best EMA pseudo Dice: 0.6497
2024-12-13 22:38:47.958863: 
2024-12-13 22:38:47.960324: Epoch 19
2024-12-13 22:38:47.961620: Current learning rate: 0.00885
2024-12-13 22:45:08.656724: Validation loss improved from -0.45652 to -0.46817! Patience: 1/50
2024-12-13 22:45:08.657709: train_loss -0.5419
2024-12-13 22:45:08.658633: val_loss -0.4682
2024-12-13 22:45:08.659789: Pseudo dice [0.7079]
2024-12-13 22:45:08.660563: Epoch time: 380.7 s
2024-12-13 22:45:09.085134: Yayy! New best EMA pseudo Dice: 0.6555
2024-12-13 22:45:10.905817: 
2024-12-13 22:45:10.907199: Epoch 20
2024-12-13 22:45:10.908313: Current learning rate: 0.00879
2024-12-13 22:51:38.241285: Validation loss did not improve from -0.46817. Patience: 1/50
2024-12-13 22:51:38.242299: train_loss -0.5579
2024-12-13 22:51:38.243121: val_loss -0.4628
2024-12-13 22:51:38.243797: Pseudo dice [0.7133]
2024-12-13 22:51:38.244557: Epoch time: 387.34 s
2024-12-13 22:51:38.245230: Yayy! New best EMA pseudo Dice: 0.6613
2024-12-13 22:51:40.012936: 
2024-12-13 22:51:40.014256: Epoch 21
2024-12-13 22:51:40.015140: Current learning rate: 0.00873
2024-12-13 22:57:24.356878: Validation loss improved from -0.46817 to -0.49413! Patience: 1/50
2024-12-13 22:57:24.361553: train_loss -0.5623
2024-12-13 22:57:24.363450: val_loss -0.4941
2024-12-13 22:57:24.364293: Pseudo dice [0.7222]
2024-12-13 22:57:24.365386: Epoch time: 344.35 s
2024-12-13 22:57:24.366566: Yayy! New best EMA pseudo Dice: 0.6674
2024-12-13 22:57:26.094542: 
2024-12-13 22:57:26.096220: Epoch 22
2024-12-13 22:57:26.097275: Current learning rate: 0.00867
2024-12-13 23:03:58.771606: Validation loss did not improve from -0.49413. Patience: 1/50
2024-12-13 23:03:58.772903: train_loss -0.5542
2024-12-13 23:03:58.773997: val_loss -0.4453
2024-12-13 23:03:58.774918: Pseudo dice [0.7028]
2024-12-13 23:03:58.775637: Epoch time: 392.68 s
2024-12-13 23:03:58.776361: Yayy! New best EMA pseudo Dice: 0.6709
2024-12-13 23:04:00.492299: 
2024-12-13 23:04:00.493389: Epoch 23
2024-12-13 23:04:00.494300: Current learning rate: 0.00861
2024-12-13 23:10:27.870342: Validation loss did not improve from -0.49413. Patience: 2/50
2024-12-13 23:10:27.871163: train_loss -0.5472
2024-12-13 23:10:27.871921: val_loss -0.4417
2024-12-13 23:10:27.872993: Pseudo dice [0.6993]
2024-12-13 23:10:27.873863: Epoch time: 387.38 s
2024-12-13 23:10:27.874680: Yayy! New best EMA pseudo Dice: 0.6738
2024-12-13 23:10:29.561703: 
2024-12-13 23:10:29.563171: Epoch 24
2024-12-13 23:10:29.563954: Current learning rate: 0.00855
2024-12-13 23:16:53.240993: Validation loss did not improve from -0.49413. Patience: 3/50
2024-12-13 23:16:53.241735: train_loss -0.5669
2024-12-13 23:16:53.242430: val_loss -0.4761
2024-12-13 23:16:53.243165: Pseudo dice [0.7233]
2024-12-13 23:16:53.243787: Epoch time: 383.68 s
2024-12-13 23:16:53.608071: Yayy! New best EMA pseudo Dice: 0.6787
2024-12-13 23:16:55.356383: 
2024-12-13 23:16:55.357825: Epoch 25
2024-12-13 23:16:55.359102: Current learning rate: 0.00849
2024-12-13 23:22:50.628378: Validation loss did not improve from -0.49413. Patience: 4/50
2024-12-13 23:22:50.629361: train_loss -0.5838
2024-12-13 23:22:50.630281: val_loss -0.4765
2024-12-13 23:22:50.630980: Pseudo dice [0.7192]
2024-12-13 23:22:50.631709: Epoch time: 355.27 s
2024-12-13 23:22:50.632542: Yayy! New best EMA pseudo Dice: 0.6828
2024-12-13 23:22:52.354235: 
2024-12-13 23:22:52.355379: Epoch 26
2024-12-13 23:22:52.356157: Current learning rate: 0.00843
2024-12-13 23:28:58.322691: Validation loss did not improve from -0.49413. Patience: 5/50
2024-12-13 23:28:58.323764: train_loss -0.5844
2024-12-13 23:28:58.324676: val_loss -0.4634
2024-12-13 23:28:58.325611: Pseudo dice [0.7206]
2024-12-13 23:28:58.326436: Epoch time: 365.97 s
2024-12-13 23:28:58.327358: Yayy! New best EMA pseudo Dice: 0.6866
2024-12-13 23:29:00.070579: 
2024-12-13 23:29:00.071940: Epoch 27
2024-12-13 23:29:00.072673: Current learning rate: 0.00836
2024-12-13 23:35:10.504761: Validation loss did not improve from -0.49413. Patience: 6/50
2024-12-13 23:35:10.505857: train_loss -0.593
2024-12-13 23:35:10.507081: val_loss -0.4488
2024-12-13 23:35:10.508294: Pseudo dice [0.6976]
2024-12-13 23:35:10.509444: Epoch time: 370.44 s
2024-12-13 23:35:10.510329: Yayy! New best EMA pseudo Dice: 0.6877
2024-12-13 23:35:12.231781: 
2024-12-13 23:35:12.233300: Epoch 28
2024-12-13 23:35:12.234316: Current learning rate: 0.0083
2024-12-13 23:40:57.958480: Validation loss did not improve from -0.49413. Patience: 7/50
2024-12-13 23:40:57.959401: train_loss -0.5945
2024-12-13 23:40:57.960282: val_loss -0.4398
2024-12-13 23:40:57.960918: Pseudo dice [0.7034]
2024-12-13 23:40:57.961612: Epoch time: 345.73 s
2024-12-13 23:40:57.962244: Yayy! New best EMA pseudo Dice: 0.6892
2024-12-13 23:40:59.985572: 
2024-12-13 23:40:59.986952: Epoch 29
2024-12-13 23:40:59.987838: Current learning rate: 0.00824
2024-12-13 23:47:34.730448: Validation loss did not improve from -0.49413. Patience: 8/50
2024-12-13 23:47:34.731405: train_loss -0.6049
2024-12-13 23:47:34.732140: val_loss -0.467
2024-12-13 23:47:34.732888: Pseudo dice [0.7142]
2024-12-13 23:47:34.733548: Epoch time: 394.75 s
2024-12-13 23:47:35.126593: Yayy! New best EMA pseudo Dice: 0.6917
2024-12-13 23:47:36.898566: 
2024-12-13 23:47:36.900037: Epoch 30
2024-12-13 23:47:36.900946: Current learning rate: 0.00818
2024-12-13 23:53:58.594191: Validation loss improved from -0.49413 to -0.50222! Patience: 8/50
2024-12-13 23:53:58.595273: train_loss -0.6033
2024-12-13 23:53:58.596321: val_loss -0.5022
2024-12-13 23:53:58.597383: Pseudo dice [0.7367]
2024-12-13 23:53:58.598276: Epoch time: 381.7 s
2024-12-13 23:53:58.599326: Yayy! New best EMA pseudo Dice: 0.6962
2024-12-13 23:54:00.436695: 
2024-12-13 23:54:00.438407: Epoch 31
2024-12-13 23:54:00.439755: Current learning rate: 0.00812
2024-12-14 00:00:14.376538: Validation loss did not improve from -0.50222. Patience: 1/50
2024-12-14 00:00:14.379484: train_loss -0.6068
2024-12-14 00:00:14.381405: val_loss -0.4835
2024-12-14 00:00:14.382380: Pseudo dice [0.7155]
2024-12-14 00:00:14.383424: Epoch time: 373.94 s
2024-12-14 00:00:14.384343: Yayy! New best EMA pseudo Dice: 0.6982
2024-12-14 00:00:16.106531: 
2024-12-14 00:00:16.107717: Epoch 32
2024-12-14 00:00:16.108405: Current learning rate: 0.00806
2024-12-14 00:06:26.932099: Validation loss did not improve from -0.50222. Patience: 2/50
2024-12-14 00:06:26.933391: train_loss -0.609
2024-12-14 00:06:26.934851: val_loss -0.4896
2024-12-14 00:06:26.936331: Pseudo dice [0.7313]
2024-12-14 00:06:26.937601: Epoch time: 370.83 s
2024-12-14 00:06:26.938421: Yayy! New best EMA pseudo Dice: 0.7015
2024-12-14 00:06:28.737687: 
2024-12-14 00:06:28.738808: Epoch 33
2024-12-14 00:06:28.739519: Current learning rate: 0.008
2024-12-14 00:12:43.768683: Validation loss did not improve from -0.50222. Patience: 3/50
2024-12-14 00:12:43.770398: train_loss -0.5972
2024-12-14 00:12:43.771220: val_loss -0.4729
2024-12-14 00:12:43.771990: Pseudo dice [0.7179]
2024-12-14 00:12:43.772752: Epoch time: 375.03 s
2024-12-14 00:12:43.773672: Yayy! New best EMA pseudo Dice: 0.7031
2024-12-14 00:12:45.540597: 
2024-12-14 00:12:45.541942: Epoch 34
2024-12-14 00:12:45.542847: Current learning rate: 0.00793
2024-12-14 00:18:42.614858: Validation loss did not improve from -0.50222. Patience: 4/50
2024-12-14 00:18:42.616051: train_loss -0.6105
2024-12-14 00:18:42.616941: val_loss -0.4845
2024-12-14 00:18:42.617796: Pseudo dice [0.7209]
2024-12-14 00:18:42.618626: Epoch time: 357.08 s
2024-12-14 00:18:43.032068: Yayy! New best EMA pseudo Dice: 0.7049
2024-12-14 00:18:44.815066: 
2024-12-14 00:18:44.816307: Epoch 35
2024-12-14 00:18:44.817048: Current learning rate: 0.00787
2024-12-14 00:25:12.187909: Validation loss did not improve from -0.50222. Patience: 5/50
2024-12-14 00:25:12.188951: train_loss -0.6165
2024-12-14 00:25:12.189662: val_loss -0.4807
2024-12-14 00:25:12.190299: Pseudo dice [0.7203]
2024-12-14 00:25:12.190944: Epoch time: 387.38 s
2024-12-14 00:25:12.191564: Yayy! New best EMA pseudo Dice: 0.7064
2024-12-14 00:25:13.954054: 
2024-12-14 00:25:13.955410: Epoch 36
2024-12-14 00:25:13.956412: Current learning rate: 0.00781
2024-12-14 00:31:03.955036: Validation loss did not improve from -0.50222. Patience: 6/50
2024-12-14 00:31:03.956110: train_loss -0.6218
2024-12-14 00:31:03.956995: val_loss -0.4857
2024-12-14 00:31:03.957697: Pseudo dice [0.7255]
2024-12-14 00:31:03.958514: Epoch time: 350.0 s
2024-12-14 00:31:03.959296: Yayy! New best EMA pseudo Dice: 0.7083
2024-12-14 00:31:05.758295: 
2024-12-14 00:31:05.759647: Epoch 37
2024-12-14 00:31:05.760413: Current learning rate: 0.00775
2024-12-14 00:37:03.132041: Validation loss did not improve from -0.50222. Patience: 7/50
2024-12-14 00:37:03.133009: train_loss -0.6252
2024-12-14 00:37:03.133709: val_loss -0.492
2024-12-14 00:37:03.134336: Pseudo dice [0.7287]
2024-12-14 00:37:03.135018: Epoch time: 357.38 s
2024-12-14 00:37:03.135656: Yayy! New best EMA pseudo Dice: 0.7104
2024-12-14 00:37:04.917602: 
2024-12-14 00:37:04.918951: Epoch 38
2024-12-14 00:37:04.920053: Current learning rate: 0.00769
2024-12-14 00:43:28.511222: Validation loss did not improve from -0.50222. Patience: 8/50
2024-12-14 00:43:28.512213: train_loss -0.6324
2024-12-14 00:43:28.513025: val_loss -0.4797
2024-12-14 00:43:28.513905: Pseudo dice [0.715]
2024-12-14 00:43:28.514624: Epoch time: 383.6 s
2024-12-14 00:43:28.515346: Yayy! New best EMA pseudo Dice: 0.7108
2024-12-14 00:43:30.616288: 
2024-12-14 00:43:30.617835: Epoch 39
2024-12-14 00:43:30.618935: Current learning rate: 0.00763
2024-12-14 00:49:59.666008: Validation loss did not improve from -0.50222. Patience: 9/50
2024-12-14 00:49:59.667301: train_loss -0.6355
2024-12-14 00:49:59.668169: val_loss -0.4938
2024-12-14 00:49:59.669093: Pseudo dice [0.7192]
2024-12-14 00:49:59.670327: Epoch time: 389.05 s
2024-12-14 00:50:00.069312: Yayy! New best EMA pseudo Dice: 0.7117
2024-12-14 00:50:01.840250: 
2024-12-14 00:50:01.841556: Epoch 40
2024-12-14 00:50:01.842557: Current learning rate: 0.00756
2024-12-14 00:55:46.987224: Validation loss improved from -0.50222 to -0.50338! Patience: 9/50
2024-12-14 00:55:46.988005: train_loss -0.6409
2024-12-14 00:55:46.989005: val_loss -0.5034
2024-12-14 00:55:46.989807: Pseudo dice [0.7323]
2024-12-14 00:55:46.990585: Epoch time: 345.15 s
2024-12-14 00:55:46.991487: Yayy! New best EMA pseudo Dice: 0.7137
2024-12-14 00:55:48.814714: 
2024-12-14 00:55:48.816155: Epoch 41
2024-12-14 00:55:48.817172: Current learning rate: 0.0075
2024-12-14 01:02:26.643919: Validation loss did not improve from -0.50338. Patience: 1/50
2024-12-14 01:02:26.644900: train_loss -0.6474
2024-12-14 01:02:26.645742: val_loss -0.4447
2024-12-14 01:02:26.646531: Pseudo dice [0.7057]
2024-12-14 01:02:26.647265: Epoch time: 397.83 s
2024-12-14 01:02:27.972916: 
2024-12-14 01:02:27.974469: Epoch 42
2024-12-14 01:02:27.975372: Current learning rate: 0.00744
2024-12-14 01:08:52.119125: Validation loss improved from -0.50338 to -0.51621! Patience: 1/50
2024-12-14 01:08:52.120506: train_loss -0.6468
2024-12-14 01:08:52.121451: val_loss -0.5162
2024-12-14 01:08:52.122292: Pseudo dice [0.7417]
2024-12-14 01:08:52.123266: Epoch time: 384.15 s
2024-12-14 01:08:52.124360: Yayy! New best EMA pseudo Dice: 0.7158
2024-12-14 01:08:53.820349: 
2024-12-14 01:08:53.821841: Epoch 43
2024-12-14 01:08:53.822874: Current learning rate: 0.00738
2024-12-14 01:14:56.669429: Validation loss did not improve from -0.51621. Patience: 1/50
2024-12-14 01:14:56.671571: train_loss -0.6465
2024-12-14 01:14:56.672626: val_loss -0.5078
2024-12-14 01:14:56.673470: Pseudo dice [0.7349]
2024-12-14 01:14:56.674179: Epoch time: 362.85 s
2024-12-14 01:14:56.675006: Yayy! New best EMA pseudo Dice: 0.7177
2024-12-14 01:14:58.435357: 
2024-12-14 01:14:58.436644: Epoch 44
2024-12-14 01:14:58.437354: Current learning rate: 0.00732
2024-12-14 01:21:53.080217: Validation loss did not improve from -0.51621. Patience: 2/50
2024-12-14 01:21:53.081311: train_loss -0.6552
2024-12-14 01:21:53.082155: val_loss -0.4594
2024-12-14 01:21:53.083179: Pseudo dice [0.7152]
2024-12-14 01:21:53.084143: Epoch time: 414.65 s
2024-12-14 01:21:54.768902: 
2024-12-14 01:21:54.770311: Epoch 45
2024-12-14 01:21:54.771273: Current learning rate: 0.00725
2024-12-14 01:28:05.218602: Validation loss did not improve from -0.51621. Patience: 3/50
2024-12-14 01:28:05.219429: train_loss -0.6527
2024-12-14 01:28:05.220136: val_loss -0.4442
2024-12-14 01:28:05.220807: Pseudo dice [0.7147]
2024-12-14 01:28:05.221585: Epoch time: 370.45 s
2024-12-14 01:28:06.526125: 
2024-12-14 01:28:06.527819: Epoch 46
2024-12-14 01:28:06.528855: Current learning rate: 0.00719
2024-12-14 01:34:26.723536: Validation loss did not improve from -0.51621. Patience: 4/50
2024-12-14 01:34:26.724595: train_loss -0.6524
2024-12-14 01:34:26.725441: val_loss -0.4841
2024-12-14 01:34:26.726211: Pseudo dice [0.7167]
2024-12-14 01:34:26.727034: Epoch time: 380.2 s
2024-12-14 01:34:28.040049: 
2024-12-14 01:34:28.041292: Epoch 47
2024-12-14 01:34:28.041993: Current learning rate: 0.00713
2024-12-14 01:40:28.227121: Validation loss did not improve from -0.51621. Patience: 5/50
2024-12-14 01:40:28.228251: train_loss -0.6508
2024-12-14 01:40:28.229246: val_loss -0.5127
2024-12-14 01:40:28.230151: Pseudo dice [0.7377]
2024-12-14 01:40:28.231067: Epoch time: 360.19 s
2024-12-14 01:40:28.231831: Yayy! New best EMA pseudo Dice: 0.7192
2024-12-14 01:40:29.947554: 
2024-12-14 01:40:29.948739: Epoch 48
2024-12-14 01:40:29.949572: Current learning rate: 0.00707
2024-12-14 01:46:53.769819: Validation loss did not improve from -0.51621. Patience: 6/50
2024-12-14 01:46:53.770713: train_loss -0.6637
2024-12-14 01:46:53.771506: val_loss -0.5052
2024-12-14 01:46:53.772293: Pseudo dice [0.7434]
2024-12-14 01:46:53.773228: Epoch time: 383.82 s
2024-12-14 01:46:53.774150: Yayy! New best EMA pseudo Dice: 0.7216
2024-12-14 01:46:55.560040: 
2024-12-14 01:46:55.561347: Epoch 49
2024-12-14 01:46:55.562359: Current learning rate: 0.007
2024-12-14 01:52:49.783550: Validation loss did not improve from -0.51621. Patience: 7/50
2024-12-14 01:52:49.784596: train_loss -0.6645
2024-12-14 01:52:49.785356: val_loss -0.4995
2024-12-14 01:52:49.786052: Pseudo dice [0.7317]
2024-12-14 01:52:49.786799: Epoch time: 354.23 s
2024-12-14 01:52:50.171148: Yayy! New best EMA pseudo Dice: 0.7226
2024-12-14 01:52:52.508755: 
2024-12-14 01:52:52.510262: Epoch 50
2024-12-14 01:52:52.511063: Current learning rate: 0.00694
2024-12-14 01:59:03.362094: Validation loss did not improve from -0.51621. Patience: 8/50
2024-12-14 01:59:03.363021: train_loss -0.6642
2024-12-14 01:59:03.363935: val_loss -0.5003
2024-12-14 01:59:03.364744: Pseudo dice [0.7332]
2024-12-14 01:59:03.365528: Epoch time: 370.86 s
2024-12-14 01:59:03.366356: Yayy! New best EMA pseudo Dice: 0.7237
2024-12-14 01:59:05.094662: 
2024-12-14 01:59:05.095954: Epoch 51
2024-12-14 01:59:05.096752: Current learning rate: 0.00688
2024-12-14 02:05:21.704363: Validation loss improved from -0.51621 to -0.53345! Patience: 8/50
2024-12-14 02:05:21.705419: train_loss -0.6703
2024-12-14 02:05:21.706448: val_loss -0.5335
2024-12-14 02:05:21.707123: Pseudo dice [0.7462]
2024-12-14 02:05:21.707788: Epoch time: 376.61 s
2024-12-14 02:05:21.708475: Yayy! New best EMA pseudo Dice: 0.7259
2024-12-14 02:05:23.492102: 
2024-12-14 02:05:23.493206: Epoch 52
2024-12-14 02:05:23.494040: Current learning rate: 0.00682
2024-12-14 02:11:44.758389: Validation loss did not improve from -0.53345. Patience: 1/50
2024-12-14 02:11:44.762950: train_loss -0.6723
2024-12-14 02:11:44.765088: val_loss -0.5192
2024-12-14 02:11:44.765920: Pseudo dice [0.7422]
2024-12-14 02:11:44.767381: Epoch time: 381.27 s
2024-12-14 02:11:44.768766: Yayy! New best EMA pseudo Dice: 0.7276
2024-12-14 02:11:46.556561: 
2024-12-14 02:11:46.558227: Epoch 53
2024-12-14 02:11:46.559251: Current learning rate: 0.00675
2024-12-14 02:17:46.524756: Validation loss did not improve from -0.53345. Patience: 2/50
2024-12-14 02:17:46.526498: train_loss -0.6707
2024-12-14 02:17:46.527359: val_loss -0.4899
2024-12-14 02:17:46.528114: Pseudo dice [0.7339]
2024-12-14 02:17:46.528820: Epoch time: 359.97 s
2024-12-14 02:17:46.529552: Yayy! New best EMA pseudo Dice: 0.7282
2024-12-14 02:17:48.306987: 
2024-12-14 02:17:48.308421: Epoch 54
2024-12-14 02:17:48.309252: Current learning rate: 0.00669
2024-12-14 02:24:40.489656: Validation loss did not improve from -0.53345. Patience: 3/50
2024-12-14 02:24:40.490588: train_loss -0.6736
2024-12-14 02:24:40.491462: val_loss -0.4893
2024-12-14 02:24:40.492253: Pseudo dice [0.7394]
2024-12-14 02:24:40.493092: Epoch time: 412.18 s
2024-12-14 02:24:40.868688: Yayy! New best EMA pseudo Dice: 0.7293
2024-12-14 02:24:42.639730: 
2024-12-14 02:24:42.640967: Epoch 55
2024-12-14 02:24:42.641726: Current learning rate: 0.00663
2024-12-14 02:31:15.671244: Validation loss did not improve from -0.53345. Patience: 4/50
2024-12-14 02:31:15.672190: train_loss -0.6752
2024-12-14 02:31:15.672976: val_loss -0.5087
2024-12-14 02:31:15.673621: Pseudo dice [0.7429]
2024-12-14 02:31:15.674267: Epoch time: 393.03 s
2024-12-14 02:31:15.674895: Yayy! New best EMA pseudo Dice: 0.7307
2024-12-14 02:31:17.430774: 
2024-12-14 02:31:17.432201: Epoch 56
2024-12-14 02:31:17.432951: Current learning rate: 0.00657
2024-12-14 02:37:00.459948: Validation loss did not improve from -0.53345. Patience: 5/50
2024-12-14 02:37:00.460956: train_loss -0.6793
2024-12-14 02:37:00.461731: val_loss -0.4778
2024-12-14 02:37:00.462477: Pseudo dice [0.7287]
2024-12-14 02:37:00.463094: Epoch time: 343.03 s
2024-12-14 02:37:01.818710: 
2024-12-14 02:37:01.819804: Epoch 57
2024-12-14 02:37:01.820560: Current learning rate: 0.0065
2024-12-14 02:43:48.874993: Validation loss did not improve from -0.53345. Patience: 6/50
2024-12-14 02:43:48.875732: train_loss -0.6846
2024-12-14 02:43:48.876607: val_loss -0.5134
2024-12-14 02:43:48.877752: Pseudo dice [0.7377]
2024-12-14 02:43:48.878806: Epoch time: 407.06 s
2024-12-14 02:43:48.879692: Yayy! New best EMA pseudo Dice: 0.7312
2024-12-14 02:43:50.623399: 
2024-12-14 02:43:50.624535: Epoch 58
2024-12-14 02:43:50.625542: Current learning rate: 0.00644
2024-12-14 02:50:18.433706: Validation loss did not improve from -0.53345. Patience: 7/50
2024-12-14 02:50:18.434774: train_loss -0.6916
2024-12-14 02:50:18.435563: val_loss -0.5123
2024-12-14 02:50:18.436352: Pseudo dice [0.7434]
2024-12-14 02:50:18.437043: Epoch time: 387.81 s
2024-12-14 02:50:18.437750: Yayy! New best EMA pseudo Dice: 0.7324
2024-12-14 02:50:20.269716: 
2024-12-14 02:50:20.271144: Epoch 59
2024-12-14 02:50:20.272012: Current learning rate: 0.00638
2024-12-14 02:55:57.194793: Validation loss did not improve from -0.53345. Patience: 8/50
2024-12-14 02:55:57.195925: train_loss -0.6877
2024-12-14 02:55:57.196973: val_loss -0.4588
2024-12-14 02:55:57.197985: Pseudo dice [0.7196]
2024-12-14 02:55:57.199008: Epoch time: 336.93 s
2024-12-14 02:55:58.984622: 
2024-12-14 02:55:58.986172: Epoch 60
2024-12-14 02:55:58.987353: Current learning rate: 0.00631
2024-12-14 03:02:05.765428: Validation loss did not improve from -0.53345. Patience: 9/50
2024-12-14 03:02:05.766420: train_loss -0.6915
2024-12-14 03:02:05.767300: val_loss -0.4911
2024-12-14 03:02:05.767913: Pseudo dice [0.7408]
2024-12-14 03:02:05.768614: Epoch time: 366.78 s
2024-12-14 03:02:07.670111: 
2024-12-14 03:02:07.671473: Epoch 61
2024-12-14 03:02:07.672126: Current learning rate: 0.00625
2024-12-14 03:08:58.585858: Validation loss did not improve from -0.53345. Patience: 10/50
2024-12-14 03:08:58.586776: train_loss -0.6961
2024-12-14 03:08:58.587487: val_loss -0.5024
2024-12-14 03:08:58.588154: Pseudo dice [0.7414]
2024-12-14 03:08:58.588784: Epoch time: 410.92 s
2024-12-14 03:08:58.589568: Yayy! New best EMA pseudo Dice: 0.733
2024-12-14 03:09:00.382715: 
2024-12-14 03:09:00.383801: Epoch 62
2024-12-14 03:09:00.384473: Current learning rate: 0.00619
2024-12-14 03:15:17.108035: Validation loss did not improve from -0.53345. Patience: 11/50
2024-12-14 03:15:17.111471: train_loss -0.6928
2024-12-14 03:15:17.112791: val_loss -0.513
2024-12-14 03:15:17.114041: Pseudo dice [0.7447]
2024-12-14 03:15:17.115424: Epoch time: 376.73 s
2024-12-14 03:15:17.116856: Yayy! New best EMA pseudo Dice: 0.7342
2024-12-14 03:15:18.877954: 
2024-12-14 03:15:18.879233: Epoch 63
2024-12-14 03:15:18.879945: Current learning rate: 0.00612
2024-12-14 03:21:21.633852: Validation loss did not improve from -0.53345. Patience: 12/50
2024-12-14 03:21:21.634977: train_loss -0.6952
2024-12-14 03:21:21.635778: val_loss -0.519
2024-12-14 03:21:21.636507: Pseudo dice [0.7376]
2024-12-14 03:21:21.637171: Epoch time: 362.76 s
2024-12-14 03:21:21.637864: Yayy! New best EMA pseudo Dice: 0.7345
2024-12-14 03:21:23.463890: 
2024-12-14 03:21:23.465624: Epoch 64
2024-12-14 03:21:23.466327: Current learning rate: 0.00606
2024-12-14 03:27:49.717479: Validation loss did not improve from -0.53345. Patience: 13/50
2024-12-14 03:27:49.718565: train_loss -0.6963
2024-12-14 03:27:49.719339: val_loss -0.5136
2024-12-14 03:27:49.719940: Pseudo dice [0.7439]
2024-12-14 03:27:49.720733: Epoch time: 386.26 s
2024-12-14 03:27:50.060961: Yayy! New best EMA pseudo Dice: 0.7355
2024-12-14 03:27:51.821137: 
2024-12-14 03:27:51.822552: Epoch 65
2024-12-14 03:27:51.823330: Current learning rate: 0.006
2024-12-14 03:33:59.720284: Validation loss did not improve from -0.53345. Patience: 14/50
2024-12-14 03:33:59.721271: train_loss -0.7041
2024-12-14 03:33:59.722137: val_loss -0.4914
2024-12-14 03:33:59.722924: Pseudo dice [0.7299]
2024-12-14 03:33:59.723622: Epoch time: 367.9 s
2024-12-14 03:34:01.126238: 
2024-12-14 03:34:01.127711: Epoch 66
2024-12-14 03:34:01.128808: Current learning rate: 0.00593
2024-12-14 03:40:34.739380: Validation loss did not improve from -0.53345. Patience: 15/50
2024-12-14 03:40:34.740406: train_loss -0.704
2024-12-14 03:40:34.741163: val_loss -0.4924
2024-12-14 03:40:34.741827: Pseudo dice [0.7303]
2024-12-14 03:40:34.742547: Epoch time: 393.62 s
2024-12-14 03:40:36.175642: 
2024-12-14 03:40:36.177077: Epoch 67
2024-12-14 03:40:36.177756: Current learning rate: 0.00587
2024-12-14 03:47:08.910204: Validation loss did not improve from -0.53345. Patience: 16/50
2024-12-14 03:47:08.911131: train_loss -0.7013
2024-12-14 03:47:08.912043: val_loss -0.4906
2024-12-14 03:47:08.912725: Pseudo dice [0.7317]
2024-12-14 03:47:08.913436: Epoch time: 392.74 s
2024-12-14 03:47:10.314658: 
2024-12-14 03:47:10.315867: Epoch 68
2024-12-14 03:47:10.316587: Current learning rate: 0.00581
2024-12-14 03:53:32.830257: Validation loss did not improve from -0.53345. Patience: 17/50
2024-12-14 03:53:32.831226: train_loss -0.7119
2024-12-14 03:53:32.831952: val_loss -0.5023
2024-12-14 03:53:32.832584: Pseudo dice [0.7397]
2024-12-14 03:53:32.833407: Epoch time: 382.52 s
2024-12-14 03:53:34.223785: 
2024-12-14 03:53:34.225431: Epoch 69
2024-12-14 03:53:34.226635: Current learning rate: 0.00574
2024-12-14 03:59:52.221637: Validation loss did not improve from -0.53345. Patience: 18/50
2024-12-14 03:59:52.222673: train_loss -0.7073
2024-12-14 03:59:52.223437: val_loss -0.4645
2024-12-14 03:59:52.224231: Pseudo dice [0.7198]
2024-12-14 03:59:52.225117: Epoch time: 378.0 s
2024-12-14 03:59:54.001052: 
2024-12-14 03:59:54.002316: Epoch 70
2024-12-14 03:59:54.003097: Current learning rate: 0.00568
2024-12-14 04:06:37.471933: Validation loss did not improve from -0.53345. Patience: 19/50
2024-12-14 04:06:37.473174: train_loss -0.7052
2024-12-14 04:06:37.474462: val_loss -0.4964
2024-12-14 04:06:37.475408: Pseudo dice [0.744]
2024-12-14 04:06:37.476230: Epoch time: 403.47 s
2024-12-14 04:06:39.336880: 
2024-12-14 04:06:39.338429: Epoch 71
2024-12-14 04:06:39.339287: Current learning rate: 0.00562
2024-12-14 04:13:26.744330: Validation loss did not improve from -0.53345. Patience: 20/50
2024-12-14 04:13:26.745335: train_loss -0.7115
2024-12-14 04:13:26.746605: val_loss -0.4957
2024-12-14 04:13:26.747602: Pseudo dice [0.7384]
2024-12-14 04:13:26.748711: Epoch time: 407.41 s
2024-12-14 04:13:28.137912: 
2024-12-14 04:13:28.139267: Epoch 72
2024-12-14 04:13:28.140022: Current learning rate: 0.00555
2024-12-14 04:19:21.137284: Validation loss did not improve from -0.53345. Patience: 21/50
2024-12-14 04:19:21.141494: train_loss -0.7137
2024-12-14 04:19:21.142738: val_loss -0.4592
2024-12-14 04:19:21.143635: Pseudo dice [0.7202]
2024-12-14 04:19:21.144819: Epoch time: 353.0 s
2024-12-14 04:19:22.562367: 
2024-12-14 04:19:22.563547: Epoch 73
2024-12-14 04:19:22.564270: Current learning rate: 0.00549
2024-12-14 04:25:58.349562: Validation loss did not improve from -0.53345. Patience: 22/50
2024-12-14 04:25:58.350801: train_loss -0.7108
2024-12-14 04:25:58.352153: val_loss -0.5115
2024-12-14 04:25:58.353191: Pseudo dice [0.736]
2024-12-14 04:25:58.354540: Epoch time: 395.79 s
2024-12-14 04:25:59.806043: 
2024-12-14 04:25:59.807243: Epoch 74
2024-12-14 04:25:59.808451: Current learning rate: 0.00542
2024-12-14 04:32:33.381290: Validation loss did not improve from -0.53345. Patience: 23/50
2024-12-14 04:32:33.382218: train_loss -0.7098
2024-12-14 04:32:33.383044: val_loss -0.4754
2024-12-14 04:32:33.383715: Pseudo dice [0.7262]
2024-12-14 04:32:33.384466: Epoch time: 393.58 s
2024-12-14 04:32:35.136266: 
2024-12-14 04:32:35.137097: Epoch 75
2024-12-14 04:32:35.137793: Current learning rate: 0.00536
2024-12-14 04:39:09.962004: Validation loss did not improve from -0.53345. Patience: 24/50
2024-12-14 04:39:09.962858: train_loss -0.7162
2024-12-14 04:39:09.963670: val_loss -0.4954
2024-12-14 04:39:09.964562: Pseudo dice [0.7351]
2024-12-14 04:39:09.965464: Epoch time: 394.83 s
2024-12-14 04:39:11.377957: 
2024-12-14 04:39:11.379847: Epoch 76
2024-12-14 04:39:11.380706: Current learning rate: 0.00529
2024-12-14 04:45:36.872870: Validation loss did not improve from -0.53345. Patience: 25/50
2024-12-14 04:45:36.873672: train_loss -0.7232
2024-12-14 04:45:36.874568: val_loss -0.4765
2024-12-14 04:45:36.875668: Pseudo dice [0.7273]
2024-12-14 04:45:36.876602: Epoch time: 385.5 s
2024-12-14 04:45:38.279543: 
2024-12-14 04:45:38.280788: Epoch 77
2024-12-14 04:45:38.281525: Current learning rate: 0.00523
2024-12-14 04:52:20.596184: Validation loss did not improve from -0.53345. Patience: 26/50
2024-12-14 04:52:20.597147: train_loss -0.7174
2024-12-14 04:52:20.598158: val_loss -0.5156
2024-12-14 04:52:20.599091: Pseudo dice [0.7452]
2024-12-14 04:52:20.600057: Epoch time: 402.32 s
2024-12-14 04:52:22.028052: 
2024-12-14 04:52:22.029513: Epoch 78
2024-12-14 04:52:22.030477: Current learning rate: 0.00517
2024-12-14 04:59:15.233634: Validation loss did not improve from -0.53345. Patience: 27/50
2024-12-14 04:59:15.234657: train_loss -0.7198
2024-12-14 04:59:15.235962: val_loss -0.4487
2024-12-14 04:59:15.236773: Pseudo dice [0.7244]
2024-12-14 04:59:15.237808: Epoch time: 413.21 s
2024-12-14 04:59:16.658886: 
2024-12-14 04:59:16.659704: Epoch 79
2024-12-14 04:59:16.660383: Current learning rate: 0.0051
2024-12-14 05:05:08.072503: Validation loss did not improve from -0.53345. Patience: 28/50
2024-12-14 05:05:08.073352: train_loss -0.7288
2024-12-14 05:05:08.074131: val_loss -0.512
2024-12-14 05:05:08.074815: Pseudo dice [0.7359]
2024-12-14 05:05:08.075598: Epoch time: 351.42 s
2024-12-14 05:05:09.933476: 
2024-12-14 05:05:09.934677: Epoch 80
2024-12-14 05:05:09.935564: Current learning rate: 0.00504
2024-12-14 05:11:44.431975: Validation loss did not improve from -0.53345. Patience: 29/50
2024-12-14 05:11:44.433021: train_loss -0.7269
2024-12-14 05:11:44.433821: val_loss -0.4661
2024-12-14 05:11:44.434523: Pseudo dice [0.7326]
2024-12-14 05:11:44.435214: Epoch time: 394.5 s
2024-12-14 05:11:45.889673: 
2024-12-14 05:11:45.890894: Epoch 81
2024-12-14 05:11:45.891525: Current learning rate: 0.00497
2024-12-14 05:17:51.828410: Validation loss did not improve from -0.53345. Patience: 30/50
2024-12-14 05:17:51.829375: train_loss -0.7261
2024-12-14 05:17:51.830087: val_loss -0.5169
2024-12-14 05:17:51.830732: Pseudo dice [0.7466]
2024-12-14 05:17:51.831461: Epoch time: 365.94 s
2024-12-14 05:17:53.665805: 
2024-12-14 05:17:53.666925: Epoch 82
2024-12-14 05:17:53.667590: Current learning rate: 0.00491
2024-12-14 05:24:07.253200: Validation loss did not improve from -0.53345. Patience: 31/50
2024-12-14 05:24:07.254535: train_loss -0.7369
2024-12-14 05:24:07.255387: val_loss -0.4796
2024-12-14 05:24:07.256128: Pseudo dice [0.7285]
2024-12-14 05:24:07.257004: Epoch time: 373.59 s
2024-12-14 05:24:08.611233: 
2024-12-14 05:24:08.612526: Epoch 83
2024-12-14 05:24:08.613468: Current learning rate: 0.00484
2024-12-14 05:30:53.097603: Validation loss did not improve from -0.53345. Patience: 32/50
2024-12-14 05:30:53.098874: train_loss -0.7331
2024-12-14 05:30:53.100180: val_loss -0.4691
2024-12-14 05:30:53.100940: Pseudo dice [0.7258]
2024-12-14 05:30:53.102361: Epoch time: 404.49 s
2024-12-14 05:30:54.420379: 
2024-12-14 05:30:54.421841: Epoch 84
2024-12-14 05:30:54.422733: Current learning rate: 0.00478
2024-12-14 05:37:34.233011: Validation loss did not improve from -0.53345. Patience: 33/50
2024-12-14 05:37:34.234014: train_loss -0.74
2024-12-14 05:37:34.235132: val_loss -0.5085
2024-12-14 05:37:34.236275: Pseudo dice [0.7404]
2024-12-14 05:37:34.237217: Epoch time: 399.81 s
2024-12-14 05:37:35.885026: 
2024-12-14 05:37:35.886439: Epoch 85
2024-12-14 05:37:35.887166: Current learning rate: 0.00471
2024-12-14 05:43:39.941683: Validation loss did not improve from -0.53345. Patience: 34/50
2024-12-14 05:43:39.942632: train_loss -0.7341
2024-12-14 05:43:39.943427: val_loss -0.4682
2024-12-14 05:43:39.944223: Pseudo dice [0.7337]
2024-12-14 05:43:39.945044: Epoch time: 364.06 s
2024-12-14 05:43:41.281243: 
2024-12-14 05:43:41.282656: Epoch 86
2024-12-14 05:43:41.283626: Current learning rate: 0.00465
2024-12-14 05:50:09.358590: Validation loss did not improve from -0.53345. Patience: 35/50
2024-12-14 05:50:09.359672: train_loss -0.7352
2024-12-14 05:50:09.360590: val_loss -0.4897
2024-12-14 05:50:09.361343: Pseudo dice [0.7375]
2024-12-14 05:50:09.362294: Epoch time: 388.08 s
2024-12-14 05:50:10.685527: 
2024-12-14 05:50:10.687090: Epoch 87
2024-12-14 05:50:10.688156: Current learning rate: 0.00458
2024-12-14 05:56:58.048848: Validation loss did not improve from -0.53345. Patience: 36/50
2024-12-14 05:56:58.049868: train_loss -0.7401
2024-12-14 05:56:58.050711: val_loss -0.5179
2024-12-14 05:56:58.051363: Pseudo dice [0.7549]
2024-12-14 05:56:58.052411: Epoch time: 407.37 s
2024-12-14 05:56:58.053105: Yayy! New best EMA pseudo Dice: 0.7362
2024-12-14 05:56:59.766168: 
2024-12-14 05:56:59.767356: Epoch 88
2024-12-14 05:56:59.768066: Current learning rate: 0.00452
2024-12-14 06:02:58.584589: Validation loss did not improve from -0.53345. Patience: 37/50
2024-12-14 06:02:58.585563: train_loss -0.7375
2024-12-14 06:02:58.586429: val_loss -0.482
2024-12-14 06:02:58.587229: Pseudo dice [0.7345]
2024-12-14 06:02:58.588027: Epoch time: 358.82 s
2024-12-14 06:02:59.916179: 
2024-12-14 06:02:59.917489: Epoch 89
2024-12-14 06:02:59.918167: Current learning rate: 0.00445
2024-12-14 06:09:55.131805: Validation loss did not improve from -0.53345. Patience: 38/50
2024-12-14 06:09:55.132718: train_loss -0.7431
2024-12-14 06:09:55.133587: val_loss -0.5037
2024-12-14 06:09:55.134390: Pseudo dice [0.7423]
2024-12-14 06:09:55.135175: Epoch time: 415.22 s
2024-12-14 06:09:55.571257: Yayy! New best EMA pseudo Dice: 0.7367
2024-12-14 06:09:57.283350: 
2024-12-14 06:09:57.284245: Epoch 90
2024-12-14 06:09:57.285010: Current learning rate: 0.00438
2024-12-14 06:16:28.247450: Validation loss did not improve from -0.53345. Patience: 39/50
2024-12-14 06:16:28.248418: train_loss -0.7445
2024-12-14 06:16:28.249570: val_loss -0.4855
2024-12-14 06:16:28.250488: Pseudo dice [0.7275]
2024-12-14 06:16:28.251318: Epoch time: 390.97 s
2024-12-14 06:16:29.591547: 
2024-12-14 06:16:29.593080: Epoch 91
2024-12-14 06:16:29.594311: Current learning rate: 0.00432
2024-12-14 06:23:19.475228: Validation loss did not improve from -0.53345. Patience: 40/50
2024-12-14 06:23:19.476188: train_loss -0.7477
2024-12-14 06:23:19.476891: val_loss -0.4619
2024-12-14 06:23:19.477521: Pseudo dice [0.7293]
2024-12-14 06:23:19.478180: Epoch time: 409.89 s
2024-12-14 06:23:20.788456: 
2024-12-14 06:23:20.789786: Epoch 92
2024-12-14 06:23:20.790544: Current learning rate: 0.00425
2024-12-14 06:29:51.956799: Validation loss did not improve from -0.53345. Patience: 41/50
2024-12-14 06:29:51.959589: train_loss -0.7467
2024-12-14 06:29:51.961065: val_loss -0.4947
2024-12-14 06:29:51.961998: Pseudo dice [0.74]
2024-12-14 06:29:51.963151: Epoch time: 391.17 s
2024-12-14 06:29:53.848979: 
2024-12-14 06:29:53.850449: Epoch 93
2024-12-14 06:29:53.851136: Current learning rate: 0.00419
2024-12-14 06:36:08.296787: Validation loss did not improve from -0.53345. Patience: 42/50
2024-12-14 06:36:08.297861: train_loss -0.7398
2024-12-14 06:36:08.298829: val_loss -0.4997
2024-12-14 06:36:08.299870: Pseudo dice [0.7379]
2024-12-14 06:36:08.300613: Epoch time: 374.45 s
2024-12-14 06:36:09.625443: 
2024-12-14 06:36:09.626723: Epoch 94
2024-12-14 06:36:09.627657: Current learning rate: 0.00412
2024-12-14 06:42:33.200215: Validation loss did not improve from -0.53345. Patience: 43/50
2024-12-14 06:42:33.202026: train_loss -0.746
2024-12-14 06:42:33.203220: val_loss -0.511
2024-12-14 06:42:33.204255: Pseudo dice [0.7475]
2024-12-14 06:42:33.205257: Epoch time: 383.58 s
2024-12-14 06:42:33.624014: Yayy! New best EMA pseudo Dice: 0.737
2024-12-14 06:42:35.305077: 
2024-12-14 06:42:35.306244: Epoch 95
2024-12-14 06:42:35.307092: Current learning rate: 0.00405
2024-12-14 06:49:04.995132: Validation loss did not improve from -0.53345. Patience: 44/50
2024-12-14 06:49:04.996102: train_loss -0.7456
2024-12-14 06:49:04.996897: val_loss -0.5228
2024-12-14 06:49:04.997597: Pseudo dice [0.762]
2024-12-14 06:49:04.998181: Epoch time: 389.69 s
2024-12-14 06:49:04.998946: Yayy! New best EMA pseudo Dice: 0.7395
2024-12-14 06:49:06.730926: 
2024-12-14 06:49:06.732460: Epoch 96
2024-12-14 06:49:06.733240: Current learning rate: 0.00399
2024-12-14 06:55:44.270082: Validation loss did not improve from -0.53345. Patience: 45/50
2024-12-14 06:55:44.270895: train_loss -0.7485
2024-12-14 06:55:44.271815: val_loss -0.4971
2024-12-14 06:55:44.272514: Pseudo dice [0.7358]
2024-12-14 06:55:44.273296: Epoch time: 397.54 s
2024-12-14 06:55:45.633781: 
2024-12-14 06:55:45.637727: Epoch 97
2024-12-14 06:55:45.638719: Current learning rate: 0.00392
2024-12-14 07:02:17.821079: Validation loss did not improve from -0.53345. Patience: 46/50
2024-12-14 07:02:17.821978: train_loss -0.7496
2024-12-14 07:02:17.822798: val_loss -0.5142
2024-12-14 07:02:17.823489: Pseudo dice [0.7429]
2024-12-14 07:02:17.824214: Epoch time: 392.19 s
2024-12-14 07:02:17.824905: Yayy! New best EMA pseudo Dice: 0.7395
2024-12-14 07:02:19.558707: 
2024-12-14 07:02:19.559724: Epoch 98
2024-12-14 07:02:19.560396: Current learning rate: 0.00385
2024-12-14 07:08:54.452985: Validation loss did not improve from -0.53345. Patience: 47/50
2024-12-14 07:08:54.453812: train_loss -0.7541
2024-12-14 07:08:54.455152: val_loss -0.5089
2024-12-14 07:08:54.456179: Pseudo dice [0.7428]
2024-12-14 07:08:54.457233: Epoch time: 394.9 s
2024-12-14 07:08:54.458180: Yayy! New best EMA pseudo Dice: 0.7398
2024-12-14 07:08:56.195410: 
2024-12-14 07:08:56.196827: Epoch 99
2024-12-14 07:08:56.197719: Current learning rate: 0.00379
2024-12-14 07:15:12.445142: Validation loss did not improve from -0.53345. Patience: 48/50
2024-12-14 07:15:12.446146: train_loss -0.7553
2024-12-14 07:15:12.446923: val_loss -0.5263
2024-12-14 07:15:12.447737: Pseudo dice [0.7505]
2024-12-14 07:15:12.448497: Epoch time: 376.25 s
2024-12-14 07:15:12.830086: Yayy! New best EMA pseudo Dice: 0.7409
2024-12-14 07:15:14.569788: 
2024-12-14 07:15:14.571126: Epoch 100
2024-12-14 07:15:14.572006: Current learning rate: 0.00372
2024-12-14 07:22:31.223674: Validation loss did not improve from -0.53345. Patience: 49/50
2024-12-14 07:22:31.224680: train_loss -0.756
2024-12-14 07:22:31.225539: val_loss -0.4839
2024-12-14 07:22:31.226327: Pseudo dice [0.7342]
2024-12-14 07:22:31.227148: Epoch time: 436.66 s
2024-12-14 07:22:32.581926: 
2024-12-14 07:22:32.583313: Epoch 101
2024-12-14 07:22:32.584169: Current learning rate: 0.00365
2024-12-14 07:28:38.981238: Validation loss did not improve from -0.53345. Patience: 50/50
2024-12-14 07:28:38.982136: train_loss -0.7518
2024-12-14 07:28:38.982808: val_loss -0.5106
2024-12-14 07:28:38.983384: Pseudo dice [0.7518]
2024-12-14 07:28:38.983992: Epoch time: 366.4 s
2024-12-14 07:28:38.984727: Yayy! New best EMA pseudo Dice: 0.7414
2024-12-14 07:28:40.719315: 
2024-12-14 07:28:40.720624: Epoch 102
2024-12-14 07:28:40.721400: Current learning rate: 0.00359
2024-12-14 07:34:53.287690: Validation loss did not improve from -0.53345. Patience: 51/50
2024-12-14 07:34:53.290795: train_loss -0.7519
2024-12-14 07:34:53.291878: val_loss -0.4888
2024-12-14 07:34:53.292536: Pseudo dice [0.7348]
2024-12-14 07:34:53.293586: Epoch time: 372.57 s
2024-12-14 07:34:54.654460: 
2024-12-14 07:34:54.655784: Epoch 103
2024-12-14 07:34:54.656463: Current learning rate: 0.00352
2024-12-14 07:42:15.649524: Validation loss did not improve from -0.53345. Patience: 52/50
2024-12-14 07:42:15.650747: train_loss -0.7582
2024-12-14 07:42:15.651455: val_loss -0.5131
2024-12-14 07:42:15.652507: Pseudo dice [0.743]
2024-12-14 07:42:15.653496: Epoch time: 441.0 s
2024-12-14 07:42:17.355443: 
2024-12-14 07:42:17.357376: Epoch 104
2024-12-14 07:42:17.358592: Current learning rate: 0.00345
2024-12-14 07:48:35.554322: Validation loss did not improve from -0.53345. Patience: 53/50
2024-12-14 07:48:35.556196: train_loss -0.7555
2024-12-14 07:48:35.557475: val_loss -0.4769
2024-12-14 07:48:35.558470: Pseudo dice [0.7332]
2024-12-14 07:48:35.559439: Epoch time: 378.2 s
2024-12-14 07:48:37.234507: 
2024-12-14 07:48:37.235825: Epoch 105
2024-12-14 07:48:37.236554: Current learning rate: 0.00338
2024-12-14 07:55:02.078325: Validation loss did not improve from -0.53345. Patience: 54/50
2024-12-14 07:55:02.079193: train_loss -0.7604
2024-12-14 07:55:02.079975: val_loss -0.4732
2024-12-14 07:55:02.080692: Pseudo dice [0.7295]
2024-12-14 07:55:02.081333: Epoch time: 384.85 s
2024-12-14 07:55:03.424568: 
2024-12-14 07:55:03.426298: Epoch 106
2024-12-14 07:55:03.427149: Current learning rate: 0.00332
2024-12-14 08:01:40.629093: Validation loss did not improve from -0.53345. Patience: 55/50
2024-12-14 08:01:40.630070: train_loss -0.7628
2024-12-14 08:01:40.631099: val_loss -0.511
2024-12-14 08:01:40.632000: Pseudo dice [0.748]
2024-12-14 08:01:40.632941: Epoch time: 397.21 s
2024-12-14 08:01:41.993134: 
2024-12-14 08:01:41.994373: Epoch 107
2024-12-14 08:01:41.995170: Current learning rate: 0.00325
2024-12-14 08:08:32.453850: Validation loss did not improve from -0.53345. Patience: 56/50
2024-12-14 08:08:32.454870: train_loss -0.7632
2024-12-14 08:08:32.456167: val_loss -0.5012
2024-12-14 08:08:32.457026: Pseudo dice [0.7352]
2024-12-14 08:08:32.457891: Epoch time: 410.46 s
2024-12-14 08:08:33.816376: 
2024-12-14 08:08:33.817682: Epoch 108
2024-12-14 08:08:33.818535: Current learning rate: 0.00318
2024-12-14 08:14:45.969330: Validation loss did not improve from -0.53345. Patience: 57/50
2024-12-14 08:14:45.970398: train_loss -0.7628
2024-12-14 08:14:45.971440: val_loss -0.5299
2024-12-14 08:14:45.972263: Pseudo dice [0.7516]
2024-12-14 08:14:45.973506: Epoch time: 372.16 s
2024-12-14 08:14:47.351881: 
2024-12-14 08:14:47.353918: Epoch 109
2024-12-14 08:14:47.355429: Current learning rate: 0.00311
2024-12-14 08:21:13.567897: Validation loss did not improve from -0.53345. Patience: 58/50
2024-12-14 08:21:13.568949: train_loss -0.7592
2024-12-14 08:21:13.569772: val_loss -0.5106
2024-12-14 08:21:13.570464: Pseudo dice [0.7507]
2024-12-14 08:21:13.571241: Epoch time: 386.22 s
2024-12-14 08:21:13.954471: Yayy! New best EMA pseudo Dice: 0.7417
2024-12-14 08:21:15.671520: 
2024-12-14 08:21:15.672761: Epoch 110
2024-12-14 08:21:15.673815: Current learning rate: 0.00304
2024-12-14 08:28:18.685830: Validation loss did not improve from -0.53345. Patience: 59/50
2024-12-14 08:28:18.686848: train_loss -0.7647
2024-12-14 08:28:18.687666: val_loss -0.5034
2024-12-14 08:28:18.688340: Pseudo dice [0.7457]
2024-12-14 08:28:18.688945: Epoch time: 423.02 s
2024-12-14 08:28:18.689591: Yayy! New best EMA pseudo Dice: 0.7421
2024-12-14 08:28:20.444332: 
2024-12-14 08:28:20.445444: Epoch 111
2024-12-14 08:28:20.446100: Current learning rate: 0.00297
2024-12-14 08:34:58.370434: Validation loss did not improve from -0.53345. Patience: 60/50
2024-12-14 08:34:58.371399: train_loss -0.7632
2024-12-14 08:34:58.372236: val_loss -0.4851
2024-12-14 08:34:58.373026: Pseudo dice [0.7353]
2024-12-14 08:34:58.373778: Epoch time: 397.93 s
2024-12-14 08:34:59.754160: 
2024-12-14 08:34:59.755757: Epoch 112
2024-12-14 08:34:59.757128: Current learning rate: 0.00291
2024-12-14 08:41:39.905429: Validation loss did not improve from -0.53345. Patience: 61/50
2024-12-14 08:41:39.906492: train_loss -0.7625
2024-12-14 08:41:39.907326: val_loss -0.4582
2024-12-14 08:41:39.908116: Pseudo dice [0.7173]
2024-12-14 08:41:39.908876: Epoch time: 400.15 s
2024-12-14 08:41:41.266017: 
2024-12-14 08:41:41.267335: Epoch 113
2024-12-14 08:41:41.268039: Current learning rate: 0.00284
2024-12-14 08:48:07.891468: Validation loss did not improve from -0.53345. Patience: 62/50
2024-12-14 08:48:07.892562: train_loss -0.7676
2024-12-14 08:48:07.893785: val_loss -0.5206
2024-12-14 08:48:07.894924: Pseudo dice [0.7533]
2024-12-14 08:48:07.895909: Epoch time: 386.63 s
2024-12-14 08:48:09.813207: 
2024-12-14 08:48:09.814494: Epoch 114
2024-12-14 08:48:09.815193: Current learning rate: 0.00277
2024-12-14 08:55:04.621389: Validation loss did not improve from -0.53345. Patience: 63/50
2024-12-14 08:55:04.622419: train_loss -0.7658
2024-12-14 08:55:04.623186: val_loss -0.5054
2024-12-14 08:55:04.623926: Pseudo dice [0.7493]
2024-12-14 08:55:04.624587: Epoch time: 414.81 s
2024-12-14 08:55:06.323478: 
2024-12-14 08:55:06.324763: Epoch 115
2024-12-14 08:55:06.325579: Current learning rate: 0.0027
2024-12-14 09:01:23.035479: Validation loss did not improve from -0.53345. Patience: 64/50
2024-12-14 09:01:23.036728: train_loss -0.7669
2024-12-14 09:01:23.037709: val_loss -0.4952
2024-12-14 09:01:23.038879: Pseudo dice [0.7382]
2024-12-14 09:01:23.040138: Epoch time: 376.71 s
2024-12-14 09:01:24.426062: 
2024-12-14 09:01:24.427424: Epoch 116
2024-12-14 09:01:24.428240: Current learning rate: 0.00263
2024-12-14 09:07:43.541107: Validation loss did not improve from -0.53345. Patience: 65/50
2024-12-14 09:07:43.542222: train_loss -0.7699
2024-12-14 09:07:43.543338: val_loss -0.5211
2024-12-14 09:07:43.544486: Pseudo dice [0.7526]
2024-12-14 09:07:43.545251: Epoch time: 379.12 s
2024-12-14 09:07:43.545959: Yayy! New best EMA pseudo Dice: 0.7422
2024-12-14 09:07:45.315761: 
2024-12-14 09:07:45.317058: Epoch 117
2024-12-14 09:07:45.318000: Current learning rate: 0.00256
2024-12-14 09:14:30.162903: Validation loss did not improve from -0.53345. Patience: 66/50
2024-12-14 09:14:30.163834: train_loss -0.7687
2024-12-14 09:14:30.164531: val_loss -0.5266
2024-12-14 09:14:30.165172: Pseudo dice [0.749]
2024-12-14 09:14:30.165989: Epoch time: 404.85 s
2024-12-14 09:14:30.166618: Yayy! New best EMA pseudo Dice: 0.7429
2024-12-14 09:14:31.938789: 
2024-12-14 09:14:31.940117: Epoch 118
2024-12-14 09:14:31.940916: Current learning rate: 0.00249
2024-12-14 09:21:15.068668: Validation loss did not improve from -0.53345. Patience: 67/50
2024-12-14 09:21:15.069698: train_loss -0.7701
2024-12-14 09:21:15.070572: val_loss -0.5129
2024-12-14 09:21:15.071276: Pseudo dice [0.7542]
2024-12-14 09:21:15.071959: Epoch time: 403.13 s
2024-12-14 09:21:15.072627: Yayy! New best EMA pseudo Dice: 0.744
2024-12-14 09:21:16.896568: 
2024-12-14 09:21:16.897967: Epoch 119
2024-12-14 09:21:16.898851: Current learning rate: 0.00242
2024-12-14 09:27:44.388132: Validation loss did not improve from -0.53345. Patience: 68/50
2024-12-14 09:27:44.389223: train_loss -0.77
2024-12-14 09:27:44.389945: val_loss -0.5034
2024-12-14 09:27:44.390718: Pseudo dice [0.7436]
2024-12-14 09:27:44.391469: Epoch time: 387.49 s
2024-12-14 09:27:46.202952: 
2024-12-14 09:27:46.204325: Epoch 120
2024-12-14 09:27:46.205128: Current learning rate: 0.00235
2024-12-14 09:34:37.659751: Validation loss did not improve from -0.53345. Patience: 69/50
2024-12-14 09:34:37.660736: train_loss -0.7728
2024-12-14 09:34:37.661717: val_loss -0.5048
2024-12-14 09:34:37.662478: Pseudo dice [0.7505]
2024-12-14 09:34:37.663437: Epoch time: 411.46 s
2024-12-14 09:34:37.664457: Yayy! New best EMA pseudo Dice: 0.7446
2024-12-14 09:34:39.485726: 
2024-12-14 09:34:39.486980: Epoch 121
2024-12-14 09:34:39.487703: Current learning rate: 0.00228
2024-12-14 09:41:47.868855: Validation loss did not improve from -0.53345. Patience: 70/50
2024-12-14 09:41:47.870695: train_loss -0.7695
2024-12-14 09:41:47.871655: val_loss -0.4697
2024-12-14 09:41:47.872343: Pseudo dice [0.7359]
2024-12-14 09:41:47.873129: Epoch time: 428.39 s
2024-12-14 09:41:49.286655: 
2024-12-14 09:41:49.288016: Epoch 122
2024-12-14 09:41:49.288929: Current learning rate: 0.00221
2024-12-14 09:48:05.013289: Validation loss did not improve from -0.53345. Patience: 71/50
2024-12-14 09:48:05.014709: train_loss -0.7733
2024-12-14 09:48:05.016632: val_loss -0.5115
2024-12-14 09:48:05.017650: Pseudo dice [0.7492]
2024-12-14 09:48:05.018756: Epoch time: 375.73 s
2024-12-14 09:48:06.421693: 
2024-12-14 09:48:06.422937: Epoch 123
2024-12-14 09:48:06.423711: Current learning rate: 0.00214
2024-12-14 09:55:01.163507: Validation loss did not improve from -0.53345. Patience: 72/50
2024-12-14 09:55:01.164398: train_loss -0.7774
2024-12-14 09:55:01.165275: val_loss -0.5126
2024-12-14 09:55:01.166060: Pseudo dice [0.7478]
2024-12-14 09:55:01.166732: Epoch time: 414.74 s
2024-12-14 09:55:01.167512: Yayy! New best EMA pseudo Dice: 0.7446
2024-12-14 09:55:03.155396: 
2024-12-14 09:55:03.156808: Epoch 124
2024-12-14 09:55:03.157508: Current learning rate: 0.00207
2024-12-14 10:01:58.613237: Validation loss did not improve from -0.53345. Patience: 73/50
2024-12-14 10:01:58.614722: train_loss -0.7764
2024-12-14 10:01:58.616286: val_loss -0.4818
2024-12-14 10:01:58.617699: Pseudo dice [0.7376]
2024-12-14 10:01:58.618687: Epoch time: 415.46 s
2024-12-14 10:02:00.884833: 
2024-12-14 10:02:00.886126: Epoch 125
2024-12-14 10:02:00.887360: Current learning rate: 0.00199
2024-12-14 10:08:21.799162: Validation loss did not improve from -0.53345. Patience: 74/50
2024-12-14 10:08:21.800007: train_loss -0.7727
2024-12-14 10:08:21.800713: val_loss -0.4938
2024-12-14 10:08:21.801342: Pseudo dice [0.7379]
2024-12-14 10:08:21.802071: Epoch time: 380.92 s
2024-12-14 10:08:23.179032: 
2024-12-14 10:08:23.180244: Epoch 126
2024-12-14 10:08:23.181558: Current learning rate: 0.00192
2024-12-14 10:15:04.544104: Validation loss did not improve from -0.53345. Patience: 75/50
2024-12-14 10:15:04.545135: train_loss -0.7762
2024-12-14 10:15:04.546926: val_loss -0.5006
2024-12-14 10:15:04.548242: Pseudo dice [0.7397]
2024-12-14 10:15:04.549644: Epoch time: 401.37 s
2024-12-14 10:15:05.933029: 
2024-12-14 10:15:05.934304: Epoch 127
2024-12-14 10:15:05.935147: Current learning rate: 0.00185
2024-12-14 10:21:31.939406: Validation loss did not improve from -0.53345. Patience: 76/50
2024-12-14 10:21:31.940200: train_loss -0.7794
2024-12-14 10:21:31.941042: val_loss -0.4911
2024-12-14 10:21:31.941823: Pseudo dice [0.7349]
2024-12-14 10:21:31.942709: Epoch time: 386.01 s
2024-12-14 10:21:33.363090: 
2024-12-14 10:21:33.364402: Epoch 128
2024-12-14 10:21:33.365143: Current learning rate: 0.00178
2024-12-14 10:28:07.699618: Validation loss did not improve from -0.53345. Patience: 77/50
2024-12-14 10:28:07.700516: train_loss -0.7777
2024-12-14 10:28:07.701342: val_loss -0.506
2024-12-14 10:28:07.702068: Pseudo dice [0.7471]
2024-12-14 10:28:07.702895: Epoch time: 394.34 s
2024-12-14 10:28:09.081981: 
2024-12-14 10:28:09.083796: Epoch 129
2024-12-14 10:28:09.084690: Current learning rate: 0.0017
2024-12-14 10:34:33.789949: Validation loss did not improve from -0.53345. Patience: 78/50
2024-12-14 10:34:33.790828: train_loss -0.7787
2024-12-14 10:34:33.791649: val_loss -0.4998
2024-12-14 10:34:33.792360: Pseudo dice [0.7465]
2024-12-14 10:34:33.793118: Epoch time: 384.71 s
2024-12-14 10:34:35.569694: 
2024-12-14 10:34:35.570968: Epoch 130
2024-12-14 10:34:35.571764: Current learning rate: 0.00163
2024-12-14 10:41:13.702322: Validation loss did not improve from -0.53345. Patience: 79/50
2024-12-14 10:41:13.703147: train_loss -0.7808
2024-12-14 10:41:13.704020: val_loss -0.4989
2024-12-14 10:41:13.704753: Pseudo dice [0.7393]
2024-12-14 10:41:13.705433: Epoch time: 398.13 s
2024-12-14 10:41:15.109381: 
2024-12-14 10:41:15.110607: Epoch 131
2024-12-14 10:41:15.111297: Current learning rate: 0.00156
2024-12-14 10:47:35.906184: Validation loss did not improve from -0.53345. Patience: 80/50
2024-12-14 10:47:35.907128: train_loss -0.7793
2024-12-14 10:47:35.907876: val_loss -0.4963
2024-12-14 10:47:35.908617: Pseudo dice [0.742]
2024-12-14 10:47:35.909354: Epoch time: 380.8 s
2024-12-14 10:47:37.278492: 
2024-12-14 10:47:37.279773: Epoch 132
2024-12-14 10:47:37.280552: Current learning rate: 0.00148
2024-12-14 10:53:57.803761: Validation loss did not improve from -0.53345. Patience: 81/50
2024-12-14 10:53:57.804856: train_loss -0.7804
2024-12-14 10:53:57.805763: val_loss -0.5065
2024-12-14 10:53:57.806771: Pseudo dice [0.7393]
2024-12-14 10:53:57.807683: Epoch time: 380.53 s
2024-12-14 10:53:59.211143: 
2024-12-14 10:53:59.212457: Epoch 133
2024-12-14 10:53:59.213470: Current learning rate: 0.00141
2024-12-14 11:00:37.537621: Validation loss did not improve from -0.53345. Patience: 82/50
2024-12-14 11:00:37.539037: train_loss -0.7796
2024-12-14 11:00:37.539948: val_loss -0.4953
2024-12-14 11:00:37.540809: Pseudo dice [0.7326]
2024-12-14 11:00:37.541534: Epoch time: 398.33 s
2024-12-14 11:00:38.990935: 
2024-12-14 11:00:38.992244: Epoch 134
2024-12-14 11:00:38.993136: Current learning rate: 0.00133
2024-12-14 11:06:37.172441: Validation loss did not improve from -0.53345. Patience: 83/50
2024-12-14 11:06:37.173338: train_loss -0.7835
2024-12-14 11:06:37.174279: val_loss -0.5067
2024-12-14 11:06:37.175269: Pseudo dice [0.7462]
2024-12-14 11:06:37.176069: Epoch time: 358.18 s
2024-12-14 11:06:38.980253: 
2024-12-14 11:06:38.981574: Epoch 135
2024-12-14 11:06:38.982259: Current learning rate: 0.00126
2024-12-14 11:13:49.811868: Validation loss did not improve from -0.53345. Patience: 84/50
2024-12-14 11:13:49.812624: train_loss -0.7834
2024-12-14 11:13:49.813326: val_loss -0.4978
2024-12-14 11:13:49.814145: Pseudo dice [0.7367]
2024-12-14 11:13:49.814906: Epoch time: 430.83 s
2024-12-14 11:13:51.576964: 
2024-12-14 11:13:51.578274: Epoch 136
2024-12-14 11:13:51.579065: Current learning rate: 0.00118
2024-12-14 11:20:47.566008: Validation loss did not improve from -0.53345. Patience: 85/50
2024-12-14 11:20:47.567009: train_loss -0.7822
2024-12-14 11:20:47.568040: val_loss -0.484
2024-12-14 11:20:47.569077: Pseudo dice [0.7349]
2024-12-14 11:20:47.570220: Epoch time: 415.99 s
2024-12-14 11:20:48.944807: 
2024-12-14 11:20:48.946314: Epoch 137
2024-12-14 11:20:48.947207: Current learning rate: 0.00111
2024-12-14 11:27:22.543766: Validation loss did not improve from -0.53345. Patience: 86/50
2024-12-14 11:27:22.544663: train_loss -0.7828
2024-12-14 11:27:22.545664: val_loss -0.5046
2024-12-14 11:27:22.546565: Pseudo dice [0.7468]
2024-12-14 11:27:22.547525: Epoch time: 393.6 s
2024-12-14 11:27:23.940762: 
2024-12-14 11:27:23.942018: Epoch 138
2024-12-14 11:27:23.942716: Current learning rate: 0.00103
2024-12-14 11:34:04.862506: Validation loss did not improve from -0.53345. Patience: 87/50
2024-12-14 11:34:04.863397: train_loss -0.7877
2024-12-14 11:34:04.864159: val_loss -0.5092
2024-12-14 11:34:04.864779: Pseudo dice [0.7434]
2024-12-14 11:34:04.865498: Epoch time: 400.92 s
2024-12-14 11:34:06.251129: 
2024-12-14 11:34:06.252369: Epoch 139
2024-12-14 11:34:06.253092: Current learning rate: 0.00095
2024-12-14 11:41:06.324260: Validation loss did not improve from -0.53345. Patience: 88/50
2024-12-14 11:41:06.325404: train_loss -0.7818
2024-12-14 11:41:06.326435: val_loss -0.5109
2024-12-14 11:41:06.327380: Pseudo dice [0.7479]
2024-12-14 11:41:06.328596: Epoch time: 420.08 s
2024-12-14 11:41:08.154108: 
2024-12-14 11:41:08.155347: Epoch 140
2024-12-14 11:41:08.156279: Current learning rate: 0.00087
2024-12-14 11:48:19.390028: Validation loss did not improve from -0.53345. Patience: 89/50
2024-12-14 11:48:19.391252: train_loss -0.7862
2024-12-14 11:48:19.392008: val_loss -0.5334
2024-12-14 11:48:19.392861: Pseudo dice [0.7561]
2024-12-14 11:48:19.393565: Epoch time: 431.24 s
2024-12-14 11:48:20.803585: 
2024-12-14 11:48:20.805037: Epoch 141
2024-12-14 11:48:20.805862: Current learning rate: 0.00079
2024-12-14 11:54:52.179825: Validation loss did not improve from -0.53345. Patience: 90/50
2024-12-14 11:54:52.201122: train_loss -0.7851
2024-12-14 11:54:52.202316: val_loss -0.5127
2024-12-14 11:54:52.203136: Pseudo dice [0.7452]
2024-12-14 11:54:52.204105: Epoch time: 391.4 s
2024-12-14 11:54:53.631258: 
2024-12-14 11:54:53.632837: Epoch 142
2024-12-14 11:54:53.633722: Current learning rate: 0.00071
2024-12-14 12:01:37.641975: Validation loss did not improve from -0.53345. Patience: 91/50
2024-12-14 12:01:37.643100: train_loss -0.7845
2024-12-14 12:01:37.643895: val_loss -0.5073
2024-12-14 12:01:37.644567: Pseudo dice [0.7491]
2024-12-14 12:01:37.645369: Epoch time: 404.01 s
2024-12-14 12:01:39.067514: 
2024-12-14 12:01:39.069101: Epoch 143
2024-12-14 12:01:39.070016: Current learning rate: 0.00063
2024-12-14 12:08:05.137541: Validation loss did not improve from -0.53345. Patience: 92/50
2024-12-14 12:08:05.138933: train_loss -0.7869
2024-12-14 12:08:05.139894: val_loss -0.4981
2024-12-14 12:08:05.140653: Pseudo dice [0.7342]
2024-12-14 12:08:05.141747: Epoch time: 386.07 s
2024-12-14 12:08:06.540700: 
2024-12-14 12:08:06.542205: Epoch 144
2024-12-14 12:08:06.543281: Current learning rate: 0.00055
2024-12-14 12:14:18.603737: Validation loss did not improve from -0.53345. Patience: 93/50
2024-12-14 12:14:18.605548: train_loss -0.7883
2024-12-14 12:14:18.606621: val_loss -0.5028
2024-12-14 12:14:18.607303: Pseudo dice [0.753]
2024-12-14 12:14:18.608122: Epoch time: 372.07 s
2024-12-14 12:14:20.502210: 
2024-12-14 12:14:20.503654: Epoch 145
2024-12-14 12:14:20.504423: Current learning rate: 0.00047
2024-12-14 12:21:19.877024: Validation loss did not improve from -0.53345. Patience: 94/50
2024-12-14 12:21:19.877929: train_loss -0.7871
2024-12-14 12:21:19.878787: val_loss -0.5153
2024-12-14 12:21:19.879612: Pseudo dice [0.7503]
2024-12-14 12:21:19.880348: Epoch time: 419.38 s
2024-12-14 12:21:19.881073: Yayy! New best EMA pseudo Dice: 0.7448
2024-12-14 12:21:22.943750: 
2024-12-14 12:21:22.945105: Epoch 146
2024-12-14 12:21:22.945961: Current learning rate: 0.00038
2024-12-14 12:27:49.566686: Validation loss did not improve from -0.53345. Patience: 95/50
2024-12-14 12:27:49.567562: train_loss -0.7874
2024-12-14 12:27:49.568280: val_loss -0.5092
2024-12-14 12:27:49.568928: Pseudo dice [0.7467]
2024-12-14 12:27:49.569581: Epoch time: 386.62 s
2024-12-14 12:27:49.570169: Yayy! New best EMA pseudo Dice: 0.745
2024-12-14 12:27:51.423530: 
2024-12-14 12:27:51.424814: Epoch 147
2024-12-14 12:27:51.425591: Current learning rate: 0.0003
2024-12-14 12:34:09.767139: Validation loss did not improve from -0.53345. Patience: 96/50
2024-12-14 12:34:09.768082: train_loss -0.7903
2024-12-14 12:34:09.769169: val_loss -0.5099
2024-12-14 12:34:09.770052: Pseudo dice [0.7512]
2024-12-14 12:34:09.771040: Epoch time: 378.35 s
2024-12-14 12:34:09.771853: Yayy! New best EMA pseudo Dice: 0.7456
2024-12-14 12:34:11.554800: 
2024-12-14 12:34:11.556329: Epoch 148
2024-12-14 12:34:11.557426: Current learning rate: 0.00021
2024-12-14 12:40:56.586373: Validation loss did not improve from -0.53345. Patience: 97/50
2024-12-14 12:40:56.587490: train_loss -0.7907
2024-12-14 12:40:56.588400: val_loss -0.4813
2024-12-14 12:40:56.589037: Pseudo dice [0.7346]
2024-12-14 12:40:56.589667: Epoch time: 405.03 s
2024-12-14 12:40:58.031722: 
2024-12-14 12:40:58.033000: Epoch 149
2024-12-14 12:40:58.033730: Current learning rate: 0.00011
2024-12-14 12:48:19.201557: Validation loss did not improve from -0.53345. Patience: 98/50
2024-12-14 12:48:19.202634: train_loss -0.7881
2024-12-14 12:48:19.203592: val_loss -0.5005
2024-12-14 12:48:19.204499: Pseudo dice [0.7447]
2024-12-14 12:48:19.205409: Epoch time: 441.17 s
2024-12-14 12:48:21.082613: Training done.
2024-12-14 12:48:21.525445: Using splits from existing split file: /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset307_Sohee_Calcium_OCT_CrossValidation/splits_final_80.json
2024-12-14 12:48:21.537001: The split file contains 5 splits.
2024-12-14 12:48:21.537894: Desired fold for training: 1
2024-12-14 12:48:21.538669: This split has 6 training and 4 validation cases.
2024-12-14 12:48:21.539591: predicting 106-002
2024-12-14 12:48:21.594526: 106-002, shape torch.Size([1, 539, 498, 498]), rank 0
2024-12-14 12:51:48.341417: predicting 401-004
2024-12-14 12:51:48.372305: 401-004, shape torch.Size([1, 375, 498, 498]), rank 0
2024-12-14 12:54:55.452647: predicting 704-003
2024-12-14 12:54:55.465244: 704-003, shape torch.Size([1, 375, 498, 498]), rank 0
2024-12-14 12:56:50.419307: predicting 706-005
2024-12-14 12:56:50.433211: 706-005, shape torch.Size([1, 375, 498, 498]), rank 0
2024-12-14 12:59:13.601747: Validation complete
2024-12-14 12:59:13.602303: Mean Validation Dice:  0.734500364364896
2024-12-13 20:44:11.187153: unpacking done...
2024-12-13 20:44:11.294714: Unable to plot network architecture: nnUNet_compile is enabled!
2024-12-13 20:44:11.333985: 
2024-12-13 20:44:11.335117: Epoch 0
2024-12-13 20:44:11.336003: Current learning rate: 0.01
2024-12-13 20:53:10.957407: Validation loss improved from 1000.00000 to -0.23715! Patience: 0/50
2024-12-13 20:53:10.958247: train_loss -0.0878
2024-12-13 20:53:10.959012: val_loss -0.2371
2024-12-13 20:53:10.959684: Pseudo dice [0.5747]
2024-12-13 20:53:10.960419: Epoch time: 539.63 s
2024-12-13 20:53:10.961139: Yayy! New best EMA pseudo Dice: 0.5747
2024-12-13 20:53:12.513869: 
2024-12-13 20:53:12.515051: Epoch 1
2024-12-13 20:53:12.515766: Current learning rate: 0.00994
2024-12-13 21:00:41.338868: Validation loss improved from -0.23715 to -0.28503! Patience: 0/50
2024-12-13 21:00:41.339730: train_loss -0.2376
2024-12-13 21:00:41.341023: val_loss -0.285
2024-12-13 21:00:41.342411: Pseudo dice [0.582]
2024-12-13 21:00:41.343683: Epoch time: 448.83 s
2024-12-13 21:00:41.344971: Yayy! New best EMA pseudo Dice: 0.5755
2024-12-13 21:00:43.142917: 
2024-12-13 21:00:43.144192: Epoch 2
2024-12-13 21:00:43.144951: Current learning rate: 0.00988
2024-12-13 21:08:29.486531: Validation loss improved from -0.28503 to -0.30610! Patience: 0/50
2024-12-13 21:08:29.487318: train_loss -0.3083
2024-12-13 21:08:29.488211: val_loss -0.3061
2024-12-13 21:08:29.489037: Pseudo dice [0.6062]
2024-12-13 21:08:29.489818: Epoch time: 466.35 s
2024-12-13 21:08:29.490559: Yayy! New best EMA pseudo Dice: 0.5785
2024-12-13 21:08:31.376248: 
2024-12-13 21:08:31.378023: Epoch 3
2024-12-13 21:08:31.379298: Current learning rate: 0.00982
2024-12-13 21:16:08.506341: Validation loss improved from -0.30610 to -0.33722! Patience: 0/50
2024-12-13 21:16:08.507297: train_loss -0.3603
2024-12-13 21:16:08.508284: val_loss -0.3372
2024-12-13 21:16:08.509148: Pseudo dice [0.6156]
2024-12-13 21:16:08.509877: Epoch time: 457.13 s
2024-12-13 21:16:08.510748: Yayy! New best EMA pseudo Dice: 0.5822
2024-12-13 21:16:10.283216: 
2024-12-13 21:16:10.284367: Epoch 4
2024-12-13 21:16:10.285114: Current learning rate: 0.00976
2024-12-13 21:23:58.646058: Validation loss did not improve from -0.33722. Patience: 1/50
2024-12-13 21:23:58.647010: train_loss -0.3633
2024-12-13 21:23:58.648047: val_loss -0.3359
2024-12-13 21:23:58.648969: Pseudo dice [0.6247]
2024-12-13 21:23:58.649888: Epoch time: 468.36 s
2024-12-13 21:23:59.033910: Yayy! New best EMA pseudo Dice: 0.5865
2024-12-13 21:24:00.842325: 
2024-12-13 21:24:00.843963: Epoch 5
2024-12-13 21:24:00.845254: Current learning rate: 0.0097
2024-12-13 21:31:40.241174: Validation loss improved from -0.33722 to -0.37701! Patience: 1/50
2024-12-13 21:31:40.242152: train_loss -0.413
2024-12-13 21:31:40.242893: val_loss -0.377
2024-12-13 21:31:40.243543: Pseudo dice [0.6624]
2024-12-13 21:31:40.244352: Epoch time: 459.4 s
2024-12-13 21:31:40.245060: Yayy! New best EMA pseudo Dice: 0.5941
2024-12-13 21:31:41.972210: 
2024-12-13 21:31:41.973450: Epoch 6
2024-12-13 21:31:41.974328: Current learning rate: 0.00964
2024-12-13 21:39:32.864181: Validation loss did not improve from -0.37701. Patience: 1/50
2024-12-13 21:39:32.865165: train_loss -0.4647
2024-12-13 21:39:32.865993: val_loss -0.3588
2024-12-13 21:39:32.866677: Pseudo dice [0.6293]
2024-12-13 21:39:32.867408: Epoch time: 470.89 s
2024-12-13 21:39:32.868048: Yayy! New best EMA pseudo Dice: 0.5976
2024-12-13 21:39:34.626427: 
2024-12-13 21:39:34.627716: Epoch 7
2024-12-13 21:39:34.628388: Current learning rate: 0.00958
2024-12-13 21:47:45.654954: Validation loss did not improve from -0.37701. Patience: 2/50
2024-12-13 21:47:45.655963: train_loss -0.4695
2024-12-13 21:47:45.657037: val_loss -0.3701
2024-12-13 21:47:45.658410: Pseudo dice [0.639]
2024-12-13 21:47:45.659553: Epoch time: 491.03 s
2024-12-13 21:47:45.660838: Yayy! New best EMA pseudo Dice: 0.6017
2024-12-13 21:47:47.432873: 
2024-12-13 21:47:47.434004: Epoch 8
2024-12-13 21:47:47.434961: Current learning rate: 0.00952
2024-12-13 21:55:29.737860: Validation loss improved from -0.37701 to -0.42159! Patience: 2/50
2024-12-13 21:55:29.738987: train_loss -0.4771
2024-12-13 21:55:29.739652: val_loss -0.4216
2024-12-13 21:55:29.740306: Pseudo dice [0.6826]
2024-12-13 21:55:29.741105: Epoch time: 462.31 s
2024-12-13 21:55:29.741857: Yayy! New best EMA pseudo Dice: 0.6098
2024-12-13 21:55:31.937408: 
2024-12-13 21:55:31.938690: Epoch 9
2024-12-13 21:55:31.939320: Current learning rate: 0.00946
2024-12-13 22:03:16.453110: Validation loss did not improve from -0.42159. Patience: 1/50
2024-12-13 22:03:16.454076: train_loss -0.4703
2024-12-13 22:03:16.455052: val_loss -0.3813
2024-12-13 22:03:16.455917: Pseudo dice [0.6391]
2024-12-13 22:03:16.456903: Epoch time: 464.52 s
2024-12-13 22:03:16.885480: Yayy! New best EMA pseudo Dice: 0.6127
2024-12-13 22:03:18.680798: 
2024-12-13 22:03:18.682107: Epoch 10
2024-12-13 22:03:18.683049: Current learning rate: 0.0094
2024-12-13 22:11:27.164701: Validation loss improved from -0.42159 to -0.44370! Patience: 1/50
2024-12-13 22:11:27.166313: train_loss -0.5052
2024-12-13 22:11:27.167264: val_loss -0.4437
2024-12-13 22:11:27.167880: Pseudo dice [0.6979]
2024-12-13 22:11:27.168704: Epoch time: 488.49 s
2024-12-13 22:11:27.169401: Yayy! New best EMA pseudo Dice: 0.6213
2024-12-13 22:11:29.067107: 
2024-12-13 22:11:29.068378: Epoch 11
2024-12-13 22:11:29.069166: Current learning rate: 0.00934
2024-12-13 22:19:23.995686: Validation loss improved from -0.44370 to -0.45174! Patience: 0/50
2024-12-13 22:19:23.996615: train_loss -0.5109
2024-12-13 22:19:23.997496: val_loss -0.4517
2024-12-13 22:19:23.998166: Pseudo dice [0.6879]
2024-12-13 22:19:23.998913: Epoch time: 474.93 s
2024-12-13 22:19:23.999467: Yayy! New best EMA pseudo Dice: 0.6279
2024-12-13 22:19:25.755903: 
2024-12-13 22:19:25.757162: Epoch 12
2024-12-13 22:19:25.758182: Current learning rate: 0.00928
2024-12-13 22:27:08.275424: Validation loss improved from -0.45174 to -0.45669! Patience: 0/50
2024-12-13 22:27:08.276380: train_loss -0.5317
2024-12-13 22:27:08.277496: val_loss -0.4567
2024-12-13 22:27:08.278559: Pseudo dice [0.6956]
2024-12-13 22:27:08.279587: Epoch time: 462.52 s
2024-12-13 22:27:08.280650: Yayy! New best EMA pseudo Dice: 0.6347
2024-12-13 22:27:10.084957: 
2024-12-13 22:27:10.086426: Epoch 13
2024-12-13 22:27:10.087493: Current learning rate: 0.00922
2024-12-13 22:34:43.374685: Validation loss did not improve from -0.45669. Patience: 1/50
2024-12-13 22:34:43.375697: train_loss -0.5474
2024-12-13 22:34:43.376559: val_loss -0.4503
2024-12-13 22:34:43.377418: Pseudo dice [0.6927]
2024-12-13 22:34:43.378135: Epoch time: 453.29 s
2024-12-13 22:34:43.378999: Yayy! New best EMA pseudo Dice: 0.6405
2024-12-13 22:34:45.226393: 
2024-12-13 22:34:45.227714: Epoch 14
2024-12-13 22:34:45.228539: Current learning rate: 0.00916
2024-12-13 22:42:31.563740: Validation loss did not improve from -0.45669. Patience: 2/50
2024-12-13 22:42:31.564840: train_loss -0.5484
2024-12-13 22:42:31.566194: val_loss -0.4547
2024-12-13 22:42:31.567141: Pseudo dice [0.6967]
2024-12-13 22:42:31.568207: Epoch time: 466.34 s
2024-12-13 22:42:31.999255: Yayy! New best EMA pseudo Dice: 0.6461
2024-12-13 22:42:33.755511: 
2024-12-13 22:42:33.757029: Epoch 15
2024-12-13 22:42:33.757753: Current learning rate: 0.0091
2024-12-13 22:50:30.974050: Validation loss did not improve from -0.45669. Patience: 3/50
2024-12-13 22:50:30.976611: train_loss -0.5605
2024-12-13 22:50:30.977916: val_loss -0.4064
2024-12-13 22:50:30.979049: Pseudo dice [0.6683]
2024-12-13 22:50:30.979986: Epoch time: 477.22 s
2024-12-13 22:50:30.980900: Yayy! New best EMA pseudo Dice: 0.6483
2024-12-13 22:50:32.794497: 
2024-12-13 22:50:32.795714: Epoch 16
2024-12-13 22:50:32.796653: Current learning rate: 0.00903
2024-12-13 22:58:15.352997: Validation loss did not improve from -0.45669. Patience: 4/50
2024-12-13 22:58:15.353979: train_loss -0.5555
2024-12-13 22:58:15.354758: val_loss -0.4239
2024-12-13 22:58:15.355512: Pseudo dice [0.6718]
2024-12-13 22:58:15.356221: Epoch time: 462.56 s
2024-12-13 22:58:15.356999: Yayy! New best EMA pseudo Dice: 0.6507
2024-12-13 22:58:17.183134: 
2024-12-13 22:58:17.184504: Epoch 17
2024-12-13 22:58:17.185407: Current learning rate: 0.00897
2024-12-13 23:06:19.194002: Validation loss did not improve from -0.45669. Patience: 5/50
2024-12-13 23:06:19.195067: train_loss -0.5754
2024-12-13 23:06:19.195828: val_loss -0.4161
2024-12-13 23:06:19.196500: Pseudo dice [0.6686]
2024-12-13 23:06:19.197253: Epoch time: 482.01 s
2024-12-13 23:06:19.198033: Yayy! New best EMA pseudo Dice: 0.6525
2024-12-13 23:06:20.954407: 
2024-12-13 23:06:20.955839: Epoch 18
2024-12-13 23:06:20.956915: Current learning rate: 0.00891
2024-12-13 23:14:18.260920: Validation loss did not improve from -0.45669. Patience: 6/50
2024-12-13 23:14:18.261879: train_loss -0.5708
2024-12-13 23:14:18.262820: val_loss -0.442
2024-12-13 23:14:18.263515: Pseudo dice [0.684]
2024-12-13 23:14:18.264197: Epoch time: 477.31 s
2024-12-13 23:14:18.264881: Yayy! New best EMA pseudo Dice: 0.6556
2024-12-13 23:14:20.031063: 
2024-12-13 23:14:20.032150: Epoch 19
2024-12-13 23:14:20.033061: Current learning rate: 0.00885
2024-12-13 23:22:19.905014: Validation loss did not improve from -0.45669. Patience: 7/50
2024-12-13 23:22:19.906047: train_loss -0.5791
2024-12-13 23:22:19.906832: val_loss -0.4533
2024-12-13 23:22:19.907499: Pseudo dice [0.6963]
2024-12-13 23:22:19.908239: Epoch time: 479.88 s
2024-12-13 23:22:20.284426: Yayy! New best EMA pseudo Dice: 0.6597
2024-12-13 23:22:22.569986: 
2024-12-13 23:22:22.571258: Epoch 20
2024-12-13 23:22:22.571988: Current learning rate: 0.00879
2024-12-13 23:30:35.051241: Validation loss improved from -0.45669 to -0.46189! Patience: 7/50
2024-12-13 23:30:35.052436: train_loss -0.5848
2024-12-13 23:30:35.053345: val_loss -0.4619
2024-12-13 23:30:35.054087: Pseudo dice [0.7016]
2024-12-13 23:30:35.054873: Epoch time: 492.48 s
2024-12-13 23:30:35.055559: Yayy! New best EMA pseudo Dice: 0.6639
2024-12-13 23:30:36.945616: 
2024-12-13 23:30:36.947010: Epoch 21
2024-12-13 23:30:36.947845: Current learning rate: 0.00873
2024-12-13 23:38:44.553782: Validation loss improved from -0.46189 to -0.47598! Patience: 0/50
2024-12-13 23:38:44.554754: train_loss -0.6047
2024-12-13 23:38:44.555780: val_loss -0.476
2024-12-13 23:38:44.556635: Pseudo dice [0.7124]
2024-12-13 23:38:44.557796: Epoch time: 487.61 s
2024-12-13 23:38:44.558699: Yayy! New best EMA pseudo Dice: 0.6687
2024-12-13 23:38:46.276130: 
2024-12-13 23:38:46.277578: Epoch 22
2024-12-13 23:38:46.278369: Current learning rate: 0.00867
2024-12-13 23:46:49.797053: Validation loss did not improve from -0.47598. Patience: 1/50
2024-12-13 23:46:49.798022: train_loss -0.5982
2024-12-13 23:46:49.798749: val_loss -0.4673
2024-12-13 23:46:49.799553: Pseudo dice [0.7037]
2024-12-13 23:46:49.800387: Epoch time: 483.52 s
2024-12-13 23:46:49.801245: Yayy! New best EMA pseudo Dice: 0.6722
2024-12-13 23:46:51.559832: 
2024-12-13 23:46:51.560953: Epoch 23
2024-12-13 23:46:51.561627: Current learning rate: 0.00861
2024-12-13 23:54:56.227162: Validation loss improved from -0.47598 to -0.47684! Patience: 1/50
2024-12-13 23:54:56.228357: train_loss -0.6117
2024-12-13 23:54:56.229454: val_loss -0.4768
2024-12-13 23:54:56.230270: Pseudo dice [0.7143]
2024-12-13 23:54:56.231050: Epoch time: 484.67 s
2024-12-13 23:54:56.231706: Yayy! New best EMA pseudo Dice: 0.6764
2024-12-13 23:54:58.089503: 
2024-12-13 23:54:58.091399: Epoch 24
2024-12-13 23:54:58.092443: Current learning rate: 0.00855
2024-12-14 00:03:16.753029: Validation loss improved from -0.47684 to -0.47823! Patience: 0/50
2024-12-14 00:03:16.754694: train_loss -0.6238
2024-12-14 00:03:16.755602: val_loss -0.4782
2024-12-14 00:03:16.756305: Pseudo dice [0.709]
2024-12-14 00:03:16.757189: Epoch time: 498.67 s
2024-12-14 00:03:17.110651: Yayy! New best EMA pseudo Dice: 0.6797
2024-12-14 00:03:18.942250: 
2024-12-14 00:03:18.944181: Epoch 25
2024-12-14 00:03:18.945771: Current learning rate: 0.00849
2024-12-14 00:11:38.397182: Validation loss improved from -0.47823 to -0.49537! Patience: 0/50
2024-12-14 00:11:38.398829: train_loss -0.6211
2024-12-14 00:11:38.399958: val_loss -0.4954
2024-12-14 00:11:38.401189: Pseudo dice [0.7219]
2024-12-14 00:11:38.402450: Epoch time: 499.46 s
2024-12-14 00:11:38.403574: Yayy! New best EMA pseudo Dice: 0.6839
2024-12-14 00:11:40.265220: 
2024-12-14 00:11:40.266447: Epoch 26
2024-12-14 00:11:40.267229: Current learning rate: 0.00843
2024-12-14 00:19:02.260110: Validation loss did not improve from -0.49537. Patience: 1/50
2024-12-14 00:19:02.261027: train_loss -0.6276
2024-12-14 00:19:02.261743: val_loss -0.4603
2024-12-14 00:19:02.262442: Pseudo dice [0.698]
2024-12-14 00:19:02.263345: Epoch time: 442.0 s
2024-12-14 00:19:02.264121: Yayy! New best EMA pseudo Dice: 0.6853
2024-12-14 00:19:04.118964: 
2024-12-14 00:19:04.120201: Epoch 27
2024-12-14 00:19:04.120978: Current learning rate: 0.00836
2024-12-14 00:26:57.203923: Validation loss did not improve from -0.49537. Patience: 2/50
2024-12-14 00:26:57.204936: train_loss -0.6175
2024-12-14 00:26:57.205969: val_loss -0.4323
2024-12-14 00:26:57.206794: Pseudo dice [0.6775]
2024-12-14 00:26:57.207725: Epoch time: 473.09 s
2024-12-14 00:26:58.566090: 
2024-12-14 00:26:58.567418: Epoch 28
2024-12-14 00:26:58.568129: Current learning rate: 0.0083
2024-12-14 00:35:00.405314: Validation loss did not improve from -0.49537. Patience: 3/50
2024-12-14 00:35:00.406357: train_loss -0.6358
2024-12-14 00:35:00.407236: val_loss -0.4384
2024-12-14 00:35:00.408052: Pseudo dice [0.6921]
2024-12-14 00:35:00.408827: Epoch time: 481.84 s
2024-12-14 00:35:01.756169: 
2024-12-14 00:35:01.757123: Epoch 29
2024-12-14 00:35:01.757766: Current learning rate: 0.00824
2024-12-14 00:43:09.276128: Validation loss did not improve from -0.49537. Patience: 4/50
2024-12-14 00:43:09.276988: train_loss -0.6387
2024-12-14 00:43:09.278235: val_loss -0.4484
2024-12-14 00:43:09.279372: Pseudo dice [0.6937]
2024-12-14 00:43:09.280404: Epoch time: 487.52 s
2024-12-14 00:43:09.684956: Yayy! New best EMA pseudo Dice: 0.6861
2024-12-14 00:43:11.790976: 
2024-12-14 00:43:11.792342: Epoch 30
2024-12-14 00:43:11.793349: Current learning rate: 0.00818
2024-12-14 00:50:45.449247: Validation loss did not improve from -0.49537. Patience: 5/50
2024-12-14 00:50:45.450199: train_loss -0.6332
2024-12-14 00:50:45.451111: val_loss -0.4917
2024-12-14 00:50:45.451887: Pseudo dice [0.7176]
2024-12-14 00:50:45.452719: Epoch time: 453.66 s
2024-12-14 00:50:45.453356: Yayy! New best EMA pseudo Dice: 0.6893
2024-12-14 00:50:47.272622: 
2024-12-14 00:50:47.273711: Epoch 31
2024-12-14 00:50:47.274425: Current learning rate: 0.00812
2024-12-14 00:58:52.314076: Validation loss did not improve from -0.49537. Patience: 6/50
2024-12-14 00:58:52.314986: train_loss -0.646
2024-12-14 00:58:52.315714: val_loss -0.4752
2024-12-14 00:58:52.316497: Pseudo dice [0.7096]
2024-12-14 00:58:52.317178: Epoch time: 485.04 s
2024-12-14 00:58:52.317829: Yayy! New best EMA pseudo Dice: 0.6913
2024-12-14 00:58:54.170443: 
2024-12-14 00:58:54.171806: Epoch 32
2024-12-14 00:58:54.172512: Current learning rate: 0.00806
2024-12-14 01:07:12.571235: Validation loss did not improve from -0.49537. Patience: 7/50
2024-12-14 01:07:12.574447: train_loss -0.6471
2024-12-14 01:07:12.576352: val_loss -0.4671
2024-12-14 01:07:12.577233: Pseudo dice [0.7078]
2024-12-14 01:07:12.578509: Epoch time: 498.41 s
2024-12-14 01:07:12.579385: Yayy! New best EMA pseudo Dice: 0.693
2024-12-14 01:07:14.379390: 
2024-12-14 01:07:14.380663: Epoch 33
2024-12-14 01:07:14.381410: Current learning rate: 0.008
2024-12-14 01:15:17.264380: Validation loss did not improve from -0.49537. Patience: 8/50
2024-12-14 01:15:17.265419: train_loss -0.6431
2024-12-14 01:15:17.266450: val_loss -0.4399
2024-12-14 01:15:17.267160: Pseudo dice [0.682]
2024-12-14 01:15:17.267857: Epoch time: 482.89 s
2024-12-14 01:15:18.704119: 
2024-12-14 01:15:18.713881: Epoch 34
2024-12-14 01:15:18.714631: Current learning rate: 0.00793
2024-12-14 01:23:49.108087: Validation loss did not improve from -0.49537. Patience: 9/50
2024-12-14 01:23:49.109092: train_loss -0.6488
2024-12-14 01:23:49.109916: val_loss -0.448
2024-12-14 01:23:49.110575: Pseudo dice [0.6839]
2024-12-14 01:23:49.111271: Epoch time: 510.41 s
2024-12-14 01:23:50.854826: 
2024-12-14 01:23:50.856444: Epoch 35
2024-12-14 01:23:50.857899: Current learning rate: 0.00787
2024-12-14 01:32:04.185567: Validation loss did not improve from -0.49537. Patience: 10/50
2024-12-14 01:32:04.186477: train_loss -0.6603
2024-12-14 01:32:04.187180: val_loss -0.4438
2024-12-14 01:32:04.187928: Pseudo dice [0.6926]
2024-12-14 01:32:04.188674: Epoch time: 493.33 s
2024-12-14 01:32:05.555703: 
2024-12-14 01:32:05.556986: Epoch 36
2024-12-14 01:32:05.557859: Current learning rate: 0.00781
2024-12-14 01:40:25.893628: Validation loss improved from -0.49537 to -0.49702! Patience: 10/50
2024-12-14 01:40:25.894582: train_loss -0.6646
2024-12-14 01:40:25.895345: val_loss -0.497
2024-12-14 01:40:25.896415: Pseudo dice [0.713]
2024-12-14 01:40:25.897353: Epoch time: 500.34 s
2024-12-14 01:40:25.898197: Yayy! New best EMA pseudo Dice: 0.6934
2024-12-14 01:40:27.665157: 
2024-12-14 01:40:27.666378: Epoch 37
2024-12-14 01:40:27.667163: Current learning rate: 0.00775
2024-12-14 01:48:29.307102: Validation loss did not improve from -0.49702. Patience: 1/50
2024-12-14 01:48:29.308054: train_loss -0.6574
2024-12-14 01:48:29.308911: val_loss -0.4931
2024-12-14 01:48:29.309601: Pseudo dice [0.7246]
2024-12-14 01:48:29.310273: Epoch time: 481.64 s
2024-12-14 01:48:29.310987: Yayy! New best EMA pseudo Dice: 0.6965
2024-12-14 01:48:31.159570: 
2024-12-14 01:48:31.160815: Epoch 38
2024-12-14 01:48:31.161629: Current learning rate: 0.00769
2024-12-14 01:56:24.770054: Validation loss did not improve from -0.49702. Patience: 2/50
2024-12-14 01:56:24.771203: train_loss -0.669
2024-12-14 01:56:24.772075: val_loss -0.4716
2024-12-14 01:56:24.772724: Pseudo dice [0.717]
2024-12-14 01:56:24.773383: Epoch time: 473.61 s
2024-12-14 01:56:24.774125: Yayy! New best EMA pseudo Dice: 0.6986
2024-12-14 01:56:26.579609: 
2024-12-14 01:56:26.581040: Epoch 39
2024-12-14 01:56:26.581821: Current learning rate: 0.00763
2024-12-14 02:04:42.154381: Validation loss did not improve from -0.49702. Patience: 3/50
2024-12-14 02:04:42.156800: train_loss -0.6728
2024-12-14 02:04:42.158117: val_loss -0.4453
2024-12-14 02:04:42.158880: Pseudo dice [0.6792]
2024-12-14 02:04:42.159685: Epoch time: 495.58 s
2024-12-14 02:04:44.319044: 
2024-12-14 02:04:44.320352: Epoch 40
2024-12-14 02:04:44.321044: Current learning rate: 0.00756
2024-12-14 02:12:28.832937: Validation loss did not improve from -0.49702. Patience: 4/50
2024-12-14 02:12:28.834422: train_loss -0.6709
2024-12-14 02:12:28.835151: val_loss -0.4857
2024-12-14 02:12:28.835841: Pseudo dice [0.7164]
2024-12-14 02:12:28.836643: Epoch time: 464.52 s
2024-12-14 02:12:28.837352: Yayy! New best EMA pseudo Dice: 0.6986
2024-12-14 02:12:30.585114: 
2024-12-14 02:12:30.586471: Epoch 41
2024-12-14 02:12:30.587336: Current learning rate: 0.0075
2024-12-14 02:20:54.029565: Validation loss did not improve from -0.49702. Patience: 5/50
2024-12-14 02:20:54.030637: train_loss -0.6713
2024-12-14 02:20:54.031428: val_loss -0.461
2024-12-14 02:20:54.032122: Pseudo dice [0.7097]
2024-12-14 02:20:54.032758: Epoch time: 503.45 s
2024-12-14 02:20:54.033370: Yayy! New best EMA pseudo Dice: 0.6997
2024-12-14 02:20:55.754049: 
2024-12-14 02:20:55.755321: Epoch 42
2024-12-14 02:20:55.756067: Current learning rate: 0.00744
2024-12-14 02:28:49.622327: Validation loss did not improve from -0.49702. Patience: 6/50
2024-12-14 02:28:49.623293: train_loss -0.6743
2024-12-14 02:28:49.624187: val_loss -0.3709
2024-12-14 02:28:49.625025: Pseudo dice [0.649]
2024-12-14 02:28:49.625771: Epoch time: 473.87 s
2024-12-14 02:28:50.942329: 
2024-12-14 02:28:50.943707: Epoch 43
2024-12-14 02:28:50.944552: Current learning rate: 0.00738
2024-12-14 02:36:38.199093: Validation loss improved from -0.49702 to -0.50405! Patience: 6/50
2024-12-14 02:36:38.199955: train_loss -0.6776
2024-12-14 02:36:38.200653: val_loss -0.504
2024-12-14 02:36:38.201432: Pseudo dice [0.7294]
2024-12-14 02:36:38.202144: Epoch time: 467.26 s
2024-12-14 02:36:39.520655: 
2024-12-14 02:36:39.521994: Epoch 44
2024-12-14 02:36:39.523144: Current learning rate: 0.00732
2024-12-14 02:44:26.167183: Validation loss did not improve from -0.50405. Patience: 1/50
2024-12-14 02:44:26.168120: train_loss -0.6853
2024-12-14 02:44:26.168862: val_loss -0.477
2024-12-14 02:44:26.169620: Pseudo dice [0.7184]
2024-12-14 02:44:26.170256: Epoch time: 466.65 s
2024-12-14 02:44:26.553268: Yayy! New best EMA pseudo Dice: 0.7002
2024-12-14 02:44:28.276678: 
2024-12-14 02:44:28.277657: Epoch 45
2024-12-14 02:44:28.278445: Current learning rate: 0.00725
2024-12-14 02:52:43.364909: Validation loss did not improve from -0.50405. Patience: 2/50
2024-12-14 02:52:43.365692: train_loss -0.6784
2024-12-14 02:52:43.366462: val_loss -0.3975
2024-12-14 02:52:43.367144: Pseudo dice [0.6604]
2024-12-14 02:52:43.367914: Epoch time: 495.09 s
2024-12-14 02:52:44.723785: 
2024-12-14 02:52:44.724788: Epoch 46
2024-12-14 02:52:44.725552: Current learning rate: 0.00719
2024-12-14 03:01:06.555226: Validation loss did not improve from -0.50405. Patience: 3/50
2024-12-14 03:01:06.556123: train_loss -0.6865
2024-12-14 03:01:06.557159: val_loss -0.4631
2024-12-14 03:01:06.558076: Pseudo dice [0.6981]
2024-12-14 03:01:06.558975: Epoch time: 501.83 s
2024-12-14 03:01:07.873601: 
2024-12-14 03:01:07.874793: Epoch 47
2024-12-14 03:01:07.875556: Current learning rate: 0.00713
2024-12-14 03:09:50.163943: Validation loss did not improve from -0.50405. Patience: 4/50
2024-12-14 03:09:50.166373: train_loss -0.6929
2024-12-14 03:09:50.167331: val_loss -0.4966
2024-12-14 03:09:50.168069: Pseudo dice [0.7204]
2024-12-14 03:09:50.168820: Epoch time: 522.29 s
2024-12-14 03:09:51.485242: 
2024-12-14 03:09:51.486493: Epoch 48
2024-12-14 03:09:51.487183: Current learning rate: 0.00707
2024-12-14 03:18:17.899411: Validation loss did not improve from -0.50405. Patience: 5/50
2024-12-14 03:18:17.900509: train_loss -0.694
2024-12-14 03:18:17.902186: val_loss -0.4569
2024-12-14 03:18:17.903085: Pseudo dice [0.6993]
2024-12-14 03:18:17.903973: Epoch time: 506.42 s
2024-12-14 03:18:19.273346: 
2024-12-14 03:18:19.274760: Epoch 49
2024-12-14 03:18:19.275720: Current learning rate: 0.007
2024-12-14 03:26:17.278301: Validation loss did not improve from -0.50405. Patience: 6/50
2024-12-14 03:26:17.280007: train_loss -0.6942
2024-12-14 03:26:17.281069: val_loss -0.41
2024-12-14 03:26:17.281727: Pseudo dice [0.6727]
2024-12-14 03:26:17.282482: Epoch time: 478.01 s
2024-12-14 03:26:19.052238: 
2024-12-14 03:26:19.053688: Epoch 50
2024-12-14 03:26:19.054510: Current learning rate: 0.00694
2024-12-14 03:34:04.372831: Validation loss did not improve from -0.50405. Patience: 7/50
2024-12-14 03:34:04.373702: train_loss -0.7067
2024-12-14 03:34:04.374598: val_loss -0.4675
2024-12-14 03:34:04.375386: Pseudo dice [0.7197]
2024-12-14 03:34:04.376059: Epoch time: 465.32 s
2024-12-14 03:34:05.728920: 
2024-12-14 03:34:05.730208: Epoch 51
2024-12-14 03:34:05.731016: Current learning rate: 0.00688
2024-12-14 03:41:55.319993: Validation loss did not improve from -0.50405. Patience: 8/50
2024-12-14 03:41:55.320899: train_loss -0.7007
2024-12-14 03:41:55.321727: val_loss -0.4014
2024-12-14 03:41:55.322527: Pseudo dice [0.6779]
2024-12-14 03:41:55.323296: Epoch time: 469.59 s
2024-12-14 03:41:56.728503: 
2024-12-14 03:41:56.730788: Epoch 52
2024-12-14 03:41:56.731791: Current learning rate: 0.00682
2024-12-14 03:49:53.389138: Validation loss did not improve from -0.50405. Patience: 9/50
2024-12-14 03:49:53.390135: train_loss -0.7039
2024-12-14 03:49:53.390978: val_loss -0.4955
2024-12-14 03:49:53.391785: Pseudo dice [0.7181]
2024-12-14 03:49:53.392581: Epoch time: 476.66 s
2024-12-14 03:49:54.743856: 
2024-12-14 03:49:54.745150: Epoch 53
2024-12-14 03:49:54.746128: Current learning rate: 0.00675
2024-12-14 03:58:19.659012: Validation loss did not improve from -0.50405. Patience: 10/50
2024-12-14 03:58:19.660151: train_loss -0.7003
2024-12-14 03:58:19.661365: val_loss -0.4719
2024-12-14 03:58:19.662344: Pseudo dice [0.7031]
2024-12-14 03:58:19.663136: Epoch time: 504.92 s
2024-12-14 03:58:21.029815: 
2024-12-14 03:58:21.031136: Epoch 54
2024-12-14 03:58:21.032033: Current learning rate: 0.00669
2024-12-14 04:06:25.185678: Validation loss did not improve from -0.50405. Patience: 11/50
2024-12-14 04:06:25.186751: train_loss -0.6998
2024-12-14 04:06:25.187685: val_loss -0.497
2024-12-14 04:06:25.188477: Pseudo dice [0.7273]
2024-12-14 04:06:25.189184: Epoch time: 484.16 s
2024-12-14 04:06:25.601279: Yayy! New best EMA pseudo Dice: 0.7019
2024-12-14 04:06:27.321176: 
2024-12-14 04:06:27.322502: Epoch 55
2024-12-14 04:06:27.323636: Current learning rate: 0.00663
2024-12-14 04:14:35.027175: Validation loss did not improve from -0.50405. Patience: 12/50
2024-12-14 04:14:35.028723: train_loss -0.7008
2024-12-14 04:14:35.029558: val_loss -0.4486
2024-12-14 04:14:35.030199: Pseudo dice [0.7032]
2024-12-14 04:14:35.030819: Epoch time: 487.71 s
2024-12-14 04:14:35.031489: Yayy! New best EMA pseudo Dice: 0.702
2024-12-14 04:14:36.847155: 
2024-12-14 04:14:36.848428: Epoch 56
2024-12-14 04:14:36.849195: Current learning rate: 0.00657
2024-12-14 04:22:53.244628: Validation loss did not improve from -0.50405. Patience: 13/50
2024-12-14 04:22:53.245672: train_loss -0.7109
2024-12-14 04:22:53.247080: val_loss -0.4549
2024-12-14 04:22:53.247857: Pseudo dice [0.6937]
2024-12-14 04:22:53.248652: Epoch time: 496.4 s
2024-12-14 04:22:54.626814: 
2024-12-14 04:22:54.628250: Epoch 57
2024-12-14 04:22:54.629355: Current learning rate: 0.0065
2024-12-14 04:30:56.254658: Validation loss did not improve from -0.50405. Patience: 14/50
2024-12-14 04:30:56.256207: train_loss -0.7076
2024-12-14 04:30:56.257320: val_loss -0.4385
2024-12-14 04:30:56.258171: Pseudo dice [0.6863]
2024-12-14 04:30:56.258869: Epoch time: 481.63 s
2024-12-14 04:30:57.662791: 
2024-12-14 04:30:57.664193: Epoch 58
2024-12-14 04:30:57.665169: Current learning rate: 0.00644
2024-12-14 04:39:12.498470: Validation loss did not improve from -0.50405. Patience: 15/50
2024-12-14 04:39:12.499355: train_loss -0.7175
2024-12-14 04:39:12.500182: val_loss -0.4603
2024-12-14 04:39:12.500802: Pseudo dice [0.6995]
2024-12-14 04:39:12.501421: Epoch time: 494.84 s
2024-12-14 04:39:13.900512: 
2024-12-14 04:39:13.901581: Epoch 59
2024-12-14 04:39:13.902283: Current learning rate: 0.00638
2024-12-14 04:46:55.061271: Validation loss did not improve from -0.50405. Patience: 16/50
2024-12-14 04:46:55.062196: train_loss -0.7182
2024-12-14 04:46:55.063117: val_loss -0.444
2024-12-14 04:46:55.064047: Pseudo dice [0.6913]
2024-12-14 04:46:55.065380: Epoch time: 461.16 s
2024-12-14 04:46:56.968930: 
2024-12-14 04:46:56.970239: Epoch 60
2024-12-14 04:46:56.971093: Current learning rate: 0.00631
2024-12-14 04:55:21.236941: Validation loss did not improve from -0.50405. Patience: 17/50
2024-12-14 04:55:21.237786: train_loss -0.7136
2024-12-14 04:55:21.238552: val_loss -0.4534
2024-12-14 04:55:21.239239: Pseudo dice [0.7094]
2024-12-14 04:55:21.240024: Epoch time: 504.27 s
2024-12-14 04:55:22.651002: 
2024-12-14 04:55:22.652326: Epoch 61
2024-12-14 04:55:22.653137: Current learning rate: 0.00625
2024-12-14 05:03:19.692076: Validation loss did not improve from -0.50405. Patience: 18/50
2024-12-14 05:03:19.693088: train_loss -0.7175
2024-12-14 05:03:19.693918: val_loss -0.4264
2024-12-14 05:03:19.694673: Pseudo dice [0.6905]
2024-12-14 05:03:19.695452: Epoch time: 477.04 s
2024-12-14 05:03:21.740148: 
2024-12-14 05:03:21.741534: Epoch 62
2024-12-14 05:03:21.742563: Current learning rate: 0.00619
2024-12-14 05:11:40.319719: Validation loss did not improve from -0.50405. Patience: 19/50
2024-12-14 05:11:40.320666: train_loss -0.7192
2024-12-14 05:11:40.321974: val_loss -0.4728
2024-12-14 05:11:40.322830: Pseudo dice [0.7188]
2024-12-14 05:11:40.323451: Epoch time: 498.58 s
2024-12-14 05:11:41.740786: 
2024-12-14 05:11:41.742224: Epoch 63
2024-12-14 05:11:41.742946: Current learning rate: 0.00612
2024-12-14 05:20:14.459369: Validation loss did not improve from -0.50405. Patience: 20/50
2024-12-14 05:20:14.460782: train_loss -0.7273
2024-12-14 05:20:14.462412: val_loss -0.4283
2024-12-14 05:20:14.463774: Pseudo dice [0.6897]
2024-12-14 05:20:14.464996: Epoch time: 512.72 s
2024-12-14 05:20:15.884085: 
2024-12-14 05:20:15.885290: Epoch 64
2024-12-14 05:20:15.886039: Current learning rate: 0.00606
2024-12-14 05:28:45.137067: Validation loss did not improve from -0.50405. Patience: 21/50
2024-12-14 05:28:45.155863: train_loss -0.7156
2024-12-14 05:28:45.157808: val_loss -0.449
2024-12-14 05:28:45.158874: Pseudo dice [0.7064]
2024-12-14 05:28:45.159957: Epoch time: 509.27 s
2024-12-14 05:28:46.970613: 
2024-12-14 05:28:46.972369: Epoch 65
2024-12-14 05:28:46.973431: Current learning rate: 0.006
2024-12-14 05:36:46.126943: Validation loss did not improve from -0.50405. Patience: 22/50
2024-12-14 05:36:46.128293: train_loss -0.7249
2024-12-14 05:36:46.129222: val_loss -0.452
2024-12-14 05:36:46.129936: Pseudo dice [0.7043]
2024-12-14 05:36:46.130731: Epoch time: 479.16 s
2024-12-14 05:36:47.529492: 
2024-12-14 05:36:47.530743: Epoch 66
2024-12-14 05:36:47.531417: Current learning rate: 0.00593
2024-12-14 05:44:55.336957: Validation loss did not improve from -0.50405. Patience: 23/50
2024-12-14 05:44:55.337972: train_loss -0.7299
2024-12-14 05:44:55.338828: val_loss -0.4694
2024-12-14 05:44:55.339613: Pseudo dice [0.7118]
2024-12-14 05:44:55.340377: Epoch time: 487.81 s
2024-12-14 05:44:56.723018: 
2024-12-14 05:44:56.724245: Epoch 67
2024-12-14 05:44:56.724953: Current learning rate: 0.00587
2024-12-14 05:52:51.541549: Validation loss did not improve from -0.50405. Patience: 24/50
2024-12-14 05:52:51.542467: train_loss -0.7163
2024-12-14 05:52:51.543438: val_loss -0.4571
2024-12-14 05:52:51.544451: Pseudo dice [0.7089]
2024-12-14 05:52:51.545325: Epoch time: 474.82 s
2024-12-14 05:52:51.546181: Yayy! New best EMA pseudo Dice: 0.7027
2024-12-14 05:52:53.323292: 
2024-12-14 05:52:53.324636: Epoch 68
2024-12-14 05:52:53.325392: Current learning rate: 0.00581
2024-12-14 06:01:28.174022: Validation loss did not improve from -0.50405. Patience: 25/50
2024-12-14 06:01:28.175002: train_loss -0.7234
2024-12-14 06:01:28.175769: val_loss -0.459
2024-12-14 06:01:28.176461: Pseudo dice [0.7068]
2024-12-14 06:01:28.177148: Epoch time: 514.85 s
2024-12-14 06:01:28.177906: Yayy! New best EMA pseudo Dice: 0.7031
2024-12-14 06:01:29.975384: 
2024-12-14 06:01:29.976708: Epoch 69
2024-12-14 06:01:29.977413: Current learning rate: 0.00574
2024-12-14 06:09:55.349407: Validation loss did not improve from -0.50405. Patience: 26/50
2024-12-14 06:09:55.350395: train_loss -0.7305
2024-12-14 06:09:55.351663: val_loss -0.4341
2024-12-14 06:09:55.352789: Pseudo dice [0.6901]
2024-12-14 06:09:55.353769: Epoch time: 505.38 s
2024-12-14 06:09:57.116248: 
2024-12-14 06:09:57.117513: Epoch 70
2024-12-14 06:09:57.118292: Current learning rate: 0.00568
2024-12-14 06:18:33.644403: Validation loss did not improve from -0.50405. Patience: 27/50
2024-12-14 06:18:33.645340: train_loss -0.7299
2024-12-14 06:18:33.646091: val_loss -0.4554
2024-12-14 06:18:33.646750: Pseudo dice [0.7125]
2024-12-14 06:18:33.647490: Epoch time: 516.53 s
2024-12-14 06:18:35.042801: 
2024-12-14 06:18:35.043904: Epoch 71
2024-12-14 06:18:35.044757: Current learning rate: 0.00562
2024-12-14 06:26:47.663033: Validation loss did not improve from -0.50405. Patience: 28/50
2024-12-14 06:26:47.664300: train_loss -0.732
2024-12-14 06:26:47.665194: val_loss -0.4805
2024-12-14 06:26:47.665990: Pseudo dice [0.7142]
2024-12-14 06:26:47.666759: Epoch time: 492.62 s
2024-12-14 06:26:47.667561: Yayy! New best EMA pseudo Dice: 0.704
2024-12-14 06:26:49.470439: 
2024-12-14 06:26:49.471690: Epoch 72
2024-12-14 06:26:49.472380: Current learning rate: 0.00555
2024-12-14 06:35:10.027530: Validation loss did not improve from -0.50405. Patience: 29/50
2024-12-14 06:35:10.028643: train_loss -0.7365
2024-12-14 06:35:10.030504: val_loss -0.4342
2024-12-14 06:35:10.031500: Pseudo dice [0.6983]
2024-12-14 06:35:10.032472: Epoch time: 500.56 s
2024-12-14 06:35:11.890971: 
2024-12-14 06:35:11.892196: Epoch 73
2024-12-14 06:35:11.892882: Current learning rate: 0.00549
2024-12-14 06:43:39.418339: Validation loss did not improve from -0.50405. Patience: 30/50
2024-12-14 06:43:39.419433: train_loss -0.7412
2024-12-14 06:43:39.420243: val_loss -0.4549
2024-12-14 06:43:39.420924: Pseudo dice [0.7114]
2024-12-14 06:43:39.421703: Epoch time: 507.53 s
2024-12-14 06:43:39.422386: Yayy! New best EMA pseudo Dice: 0.7042
2024-12-14 06:43:41.207429: 
2024-12-14 06:43:41.209190: Epoch 74
2024-12-14 06:43:41.210401: Current learning rate: 0.00542
2024-12-14 06:51:57.039146: Validation loss did not improve from -0.50405. Patience: 31/50
2024-12-14 06:51:57.040327: train_loss -0.7433
2024-12-14 06:51:57.041767: val_loss -0.4378
2024-12-14 06:51:57.043224: Pseudo dice [0.6925]
2024-12-14 06:51:57.044652: Epoch time: 495.83 s
2024-12-14 06:51:58.812834: 
2024-12-14 06:51:58.814342: Epoch 75
2024-12-14 06:51:58.815279: Current learning rate: 0.00536
2024-12-14 07:00:12.227390: Validation loss did not improve from -0.50405. Patience: 32/50
2024-12-14 07:00:12.228406: train_loss -0.7421
2024-12-14 07:00:12.229229: val_loss -0.4854
2024-12-14 07:00:12.229884: Pseudo dice [0.7183]
2024-12-14 07:00:12.230546: Epoch time: 493.42 s
2024-12-14 07:00:12.231141: Yayy! New best EMA pseudo Dice: 0.7046
2024-12-14 07:00:14.003489: 
2024-12-14 07:00:14.004965: Epoch 76
2024-12-14 07:00:14.005873: Current learning rate: 0.00529
2024-12-14 07:08:24.190943: Validation loss did not improve from -0.50405. Patience: 33/50
2024-12-14 07:08:24.191988: train_loss -0.7423
2024-12-14 07:08:24.193209: val_loss -0.4633
2024-12-14 07:08:24.194700: Pseudo dice [0.7096]
2024-12-14 07:08:24.195956: Epoch time: 490.19 s
2024-12-14 07:08:24.197313: Yayy! New best EMA pseudo Dice: 0.7051
2024-12-14 07:08:25.995295: 
2024-12-14 07:08:25.997198: Epoch 77
2024-12-14 07:08:25.998421: Current learning rate: 0.00523
2024-12-14 07:16:29.686496: Validation loss did not improve from -0.50405. Patience: 34/50
2024-12-14 07:16:29.687364: train_loss -0.7412
2024-12-14 07:16:29.688320: val_loss -0.4409
2024-12-14 07:16:29.689083: Pseudo dice [0.6913]
2024-12-14 07:16:29.689858: Epoch time: 483.69 s
2024-12-14 07:16:31.136664: 
2024-12-14 07:16:31.137915: Epoch 78
2024-12-14 07:16:31.138910: Current learning rate: 0.00517
2024-12-14 07:24:43.817110: Validation loss did not improve from -0.50405. Patience: 35/50
2024-12-14 07:24:43.818175: train_loss -0.7443
2024-12-14 07:24:43.819055: val_loss -0.4253
2024-12-14 07:24:43.819999: Pseudo dice [0.679]
2024-12-14 07:24:43.820815: Epoch time: 492.68 s
2024-12-14 07:24:45.282804: 
2024-12-14 07:24:45.284144: Epoch 79
2024-12-14 07:24:45.285230: Current learning rate: 0.0051
2024-12-14 07:33:03.114463: Validation loss did not improve from -0.50405. Patience: 36/50
2024-12-14 07:33:03.115935: train_loss -0.7449
2024-12-14 07:33:03.116876: val_loss -0.4455
2024-12-14 07:33:03.117625: Pseudo dice [0.7062]
2024-12-14 07:33:03.118326: Epoch time: 497.83 s
2024-12-14 07:33:04.948146: 
2024-12-14 07:33:04.949357: Epoch 80
2024-12-14 07:33:04.950068: Current learning rate: 0.00504
2024-12-14 07:41:24.660516: Validation loss did not improve from -0.50405. Patience: 37/50
2024-12-14 07:41:24.661420: train_loss -0.7492
2024-12-14 07:41:24.663364: val_loss -0.4371
2024-12-14 07:41:24.664108: Pseudo dice [0.6943]
2024-12-14 07:41:24.664817: Epoch time: 499.71 s
2024-12-14 07:41:26.080170: 
2024-12-14 07:41:26.081564: Epoch 81
2024-12-14 07:41:26.082306: Current learning rate: 0.00497
2024-12-14 07:49:27.397681: Validation loss did not improve from -0.50405. Patience: 38/50
2024-12-14 07:49:27.398643: train_loss -0.7517
2024-12-14 07:49:27.399411: val_loss -0.4759
2024-12-14 07:49:27.400151: Pseudo dice [0.7237]
2024-12-14 07:49:27.400878: Epoch time: 481.32 s
2024-12-14 07:49:28.820522: 
2024-12-14 07:49:28.821990: Epoch 82
2024-12-14 07:49:28.822922: Current learning rate: 0.00491
2024-12-14 07:57:49.717170: Validation loss did not improve from -0.50405. Patience: 39/50
2024-12-14 07:57:49.718130: train_loss -0.7546
2024-12-14 07:57:49.719164: val_loss -0.3978
2024-12-14 07:57:49.719946: Pseudo dice [0.6733]
2024-12-14 07:57:49.720757: Epoch time: 500.9 s
2024-12-14 07:57:51.453127: 
2024-12-14 07:57:51.454260: Epoch 83
2024-12-14 07:57:51.455107: Current learning rate: 0.00484
2024-12-14 08:05:56.111377: Validation loss did not improve from -0.50405. Patience: 40/50
2024-12-14 08:05:56.112405: train_loss -0.7531
2024-12-14 08:05:56.113348: val_loss -0.4531
2024-12-14 08:05:56.114103: Pseudo dice [0.7043]
2024-12-14 08:05:56.114886: Epoch time: 484.66 s
2024-12-14 08:05:57.472276: 
2024-12-14 08:05:57.473527: Epoch 84
2024-12-14 08:05:57.474307: Current learning rate: 0.00478
2024-12-14 08:14:06.292452: Validation loss did not improve from -0.50405. Patience: 41/50
2024-12-14 08:14:06.293502: train_loss -0.7539
2024-12-14 08:14:06.294263: val_loss -0.4548
2024-12-14 08:14:06.294912: Pseudo dice [0.7082]
2024-12-14 08:14:06.295722: Epoch time: 488.82 s
2024-12-14 08:14:08.004378: 
2024-12-14 08:14:08.005673: Epoch 85
2024-12-14 08:14:08.006314: Current learning rate: 0.00471
2024-12-14 08:22:38.213932: Validation loss did not improve from -0.50405. Patience: 42/50
2024-12-14 08:22:38.214989: train_loss -0.7549
2024-12-14 08:22:38.215847: val_loss -0.4608
2024-12-14 08:22:38.216586: Pseudo dice [0.7081]
2024-12-14 08:22:38.217310: Epoch time: 510.21 s
2024-12-14 08:22:39.564896: 
2024-12-14 08:22:39.566042: Epoch 86
2024-12-14 08:22:39.566730: Current learning rate: 0.00465
2024-12-14 08:31:20.182289: Validation loss did not improve from -0.50405. Patience: 43/50
2024-12-14 08:31:20.183140: train_loss -0.7564
2024-12-14 08:31:20.184184: val_loss -0.4564
2024-12-14 08:31:20.185159: Pseudo dice [0.707]
2024-12-14 08:31:20.186085: Epoch time: 520.62 s
2024-12-14 08:31:21.542088: 
2024-12-14 08:31:21.543485: Epoch 87
2024-12-14 08:31:21.544453: Current learning rate: 0.00458
2024-12-14 08:39:57.808230: Validation loss did not improve from -0.50405. Patience: 44/50
2024-12-14 08:39:57.812185: train_loss -0.7596
2024-12-14 08:39:57.814149: val_loss -0.4507
2024-12-14 08:39:57.815787: Pseudo dice [0.7113]
2024-12-14 08:39:57.817626: Epoch time: 516.27 s
2024-12-14 08:39:59.196622: 
2024-12-14 08:39:59.197907: Epoch 88
2024-12-14 08:39:59.198627: Current learning rate: 0.00452
2024-12-14 08:47:50.133464: Validation loss did not improve from -0.50405. Patience: 45/50
2024-12-14 08:47:50.134974: train_loss -0.7588
2024-12-14 08:47:50.137215: val_loss -0.5001
2024-12-14 08:47:50.138077: Pseudo dice [0.7393]
2024-12-14 08:47:50.139426: Epoch time: 470.94 s
2024-12-14 08:47:50.140237: Yayy! New best EMA pseudo Dice: 0.707
2024-12-14 08:47:51.860624: 
2024-12-14 08:47:51.861982: Epoch 89
2024-12-14 08:47:51.862740: Current learning rate: 0.00445
2024-12-14 08:56:26.619217: Validation loss did not improve from -0.50405. Patience: 46/50
2024-12-14 08:56:26.620306: train_loss -0.7617
2024-12-14 08:56:26.621495: val_loss -0.4905
2024-12-14 08:56:26.622535: Pseudo dice [0.728]
2024-12-14 08:56:26.623539: Epoch time: 514.76 s
2024-12-14 08:56:26.991480: Yayy! New best EMA pseudo Dice: 0.7091
2024-12-14 08:56:28.715550: 
2024-12-14 08:56:28.717063: Epoch 90
2024-12-14 08:56:28.718325: Current learning rate: 0.00438
2024-12-14 09:04:37.629236: Validation loss did not improve from -0.50405. Patience: 47/50
2024-12-14 09:04:37.630101: train_loss -0.7579
2024-12-14 09:04:37.631238: val_loss -0.4228
2024-12-14 09:04:37.632253: Pseudo dice [0.6822]
2024-12-14 09:04:37.633222: Epoch time: 488.92 s
2024-12-14 09:04:39.004028: 
2024-12-14 09:04:39.005544: Epoch 91
2024-12-14 09:04:39.006532: Current learning rate: 0.00432
2024-12-14 09:13:02.877122: Validation loss did not improve from -0.50405. Patience: 48/50
2024-12-14 09:13:02.878152: train_loss -0.7627
2024-12-14 09:13:02.878922: val_loss -0.4795
2024-12-14 09:13:02.879584: Pseudo dice [0.7211]
2024-12-14 09:13:02.880418: Epoch time: 503.88 s
2024-12-14 09:13:04.184265: 
2024-12-14 09:13:04.185635: Epoch 92
2024-12-14 09:13:04.186528: Current learning rate: 0.00425
2024-12-14 09:21:32.763807: Validation loss did not improve from -0.50405. Patience: 49/50
2024-12-14 09:21:32.764886: train_loss -0.7615
2024-12-14 09:21:32.765874: val_loss -0.4796
2024-12-14 09:21:32.766739: Pseudo dice [0.7184]
2024-12-14 09:21:32.767568: Epoch time: 508.58 s
2024-12-14 09:21:34.139680: 
2024-12-14 09:21:34.141103: Epoch 93
2024-12-14 09:21:34.142038: Current learning rate: 0.00419
2024-12-14 09:29:40.629493: Validation loss improved from -0.50405 to -0.51733! Patience: 49/50
2024-12-14 09:29:40.630316: train_loss -0.7583
2024-12-14 09:29:40.631377: val_loss -0.5173
2024-12-14 09:29:40.632207: Pseudo dice [0.7392]
2024-12-14 09:29:40.633117: Epoch time: 486.49 s
2024-12-14 09:29:40.634005: Yayy! New best EMA pseudo Dice: 0.712
2024-12-14 09:29:42.426980: 
2024-12-14 09:29:42.428595: Epoch 94
2024-12-14 09:29:42.429602: Current learning rate: 0.00412
2024-12-14 09:37:57.626283: Validation loss did not improve from -0.51733. Patience: 1/50
2024-12-14 09:37:57.627246: train_loss -0.7642
2024-12-14 09:37:57.628099: val_loss -0.4502
2024-12-14 09:37:57.628912: Pseudo dice [0.7088]
2024-12-14 09:37:57.629726: Epoch time: 495.2 s
2024-12-14 09:37:59.786447: 
2024-12-14 09:37:59.787837: Epoch 95
2024-12-14 09:37:59.788589: Current learning rate: 0.00405
2024-12-14 09:45:59.405498: Validation loss did not improve from -0.51733. Patience: 2/50
2024-12-14 09:45:59.409064: train_loss -0.7666
2024-12-14 09:45:59.410294: val_loss -0.4639
2024-12-14 09:45:59.411025: Pseudo dice [0.7107]
2024-12-14 09:45:59.412201: Epoch time: 479.62 s
2024-12-14 09:46:00.778017: 
2024-12-14 09:46:00.779352: Epoch 96
2024-12-14 09:46:00.780281: Current learning rate: 0.00399
2024-12-14 09:54:16.625388: Validation loss did not improve from -0.51733. Patience: 3/50
2024-12-14 09:54:16.626713: train_loss -0.765
2024-12-14 09:54:16.627635: val_loss -0.4477
2024-12-14 09:54:16.628271: Pseudo dice [0.7104]
2024-12-14 09:54:16.628879: Epoch time: 495.85 s
2024-12-14 09:54:17.989412: 
2024-12-14 09:54:17.990438: Epoch 97
2024-12-14 09:54:17.991073: Current learning rate: 0.00392
2024-12-14 10:02:33.159798: Validation loss did not improve from -0.51733. Patience: 4/50
2024-12-14 10:02:33.160682: train_loss -0.7657
2024-12-14 10:02:33.161584: val_loss -0.4679
2024-12-14 10:02:33.162289: Pseudo dice [0.7132]
2024-12-14 10:02:33.163124: Epoch time: 495.17 s
2024-12-14 10:02:34.588904: 
2024-12-14 10:02:34.590291: Epoch 98
2024-12-14 10:02:34.591115: Current learning rate: 0.00385
2024-12-14 10:10:57.125481: Validation loss did not improve from -0.51733. Patience: 5/50
2024-12-14 10:10:57.126555: train_loss -0.7698
2024-12-14 10:10:57.127517: val_loss -0.4408
2024-12-14 10:10:57.128338: Pseudo dice [0.7016]
2024-12-14 10:10:57.129189: Epoch time: 502.54 s
2024-12-14 10:10:58.510103: 
2024-12-14 10:10:58.511738: Epoch 99
2024-12-14 10:10:58.512711: Current learning rate: 0.00379
2024-12-14 10:19:19.715463: Validation loss did not improve from -0.51733. Patience: 6/50
2024-12-14 10:19:19.716405: train_loss -0.7716
2024-12-14 10:19:19.717216: val_loss -0.4723
2024-12-14 10:19:19.717881: Pseudo dice [0.7111]
2024-12-14 10:19:19.718591: Epoch time: 501.21 s
2024-12-14 10:19:21.537936: 
2024-12-14 10:19:21.539334: Epoch 100
2024-12-14 10:19:21.540147: Current learning rate: 0.00372
2024-12-14 10:27:50.648579: Validation loss did not improve from -0.51733. Patience: 7/50
2024-12-14 10:27:50.649528: train_loss -0.7683
2024-12-14 10:27:50.650359: val_loss -0.4739
2024-12-14 10:27:50.651141: Pseudo dice [0.7197]
2024-12-14 10:27:50.651953: Epoch time: 509.11 s
2024-12-14 10:27:51.994406: 
2024-12-14 10:27:51.995848: Epoch 101
2024-12-14 10:27:51.996609: Current learning rate: 0.00365
2024-12-14 10:36:09.584233: Validation loss did not improve from -0.51733. Patience: 8/50
2024-12-14 10:36:09.585166: train_loss -0.7735
2024-12-14 10:36:09.585912: val_loss -0.4423
2024-12-14 10:36:09.586557: Pseudo dice [0.6939]
2024-12-14 10:36:09.587332: Epoch time: 497.59 s
2024-12-14 10:36:10.933383: 
2024-12-14 10:36:10.934865: Epoch 102
2024-12-14 10:36:10.936089: Current learning rate: 0.00359
2024-12-14 10:44:23.068494: Validation loss did not improve from -0.51733. Patience: 9/50
2024-12-14 10:44:23.069648: train_loss -0.7726
2024-12-14 10:44:23.070443: val_loss -0.4585
2024-12-14 10:44:23.071082: Pseudo dice [0.7113]
2024-12-14 10:44:23.071717: Epoch time: 492.14 s
2024-12-14 10:44:24.551392: 
2024-12-14 10:44:24.552449: Epoch 103
2024-12-14 10:44:24.553244: Current learning rate: 0.00352
2024-12-14 10:52:47.342060: Validation loss did not improve from -0.51733. Patience: 10/50
2024-12-14 10:52:47.345362: train_loss -0.7751
2024-12-14 10:52:47.347258: val_loss -0.4732
2024-12-14 10:52:47.348123: Pseudo dice [0.7181]
2024-12-14 10:52:47.349174: Epoch time: 502.79 s
2024-12-14 10:52:48.742698: 
2024-12-14 10:52:48.743812: Epoch 104
2024-12-14 10:52:48.744540: Current learning rate: 0.00345
2024-12-14 11:00:57.041800: Validation loss did not improve from -0.51733. Patience: 11/50
2024-12-14 11:00:57.042875: train_loss -0.7739
2024-12-14 11:00:57.043647: val_loss -0.4281
2024-12-14 11:00:57.044441: Pseudo dice [0.6898]
2024-12-14 11:00:57.045151: Epoch time: 488.3 s
2024-12-14 11:00:58.851660: 
2024-12-14 11:00:58.853058: Epoch 105
2024-12-14 11:00:58.853906: Current learning rate: 0.00338
2024-12-14 11:09:11.576655: Validation loss did not improve from -0.51733. Patience: 12/50
2024-12-14 11:09:11.577790: train_loss -0.7785
2024-12-14 11:09:11.578968: val_loss -0.4306
2024-12-14 11:09:11.580034: Pseudo dice [0.6946]
2024-12-14 11:09:11.580860: Epoch time: 492.73 s
2024-12-14 11:09:13.566264: 
2024-12-14 11:09:13.567428: Epoch 106
2024-12-14 11:09:13.568177: Current learning rate: 0.00332
2024-12-14 11:17:44.398487: Validation loss did not improve from -0.51733. Patience: 13/50
2024-12-14 11:17:44.399244: train_loss -0.7761
2024-12-14 11:17:44.400110: val_loss -0.4321
2024-12-14 11:17:44.400817: Pseudo dice [0.6993]
2024-12-14 11:17:44.401542: Epoch time: 510.83 s
2024-12-14 11:17:45.838572: 
2024-12-14 11:17:45.840192: Epoch 107
2024-12-14 11:17:45.841326: Current learning rate: 0.00325
2024-12-14 11:26:17.845835: Validation loss did not improve from -0.51733. Patience: 14/50
2024-12-14 11:26:17.846741: train_loss -0.7771
2024-12-14 11:26:17.847510: val_loss -0.481
2024-12-14 11:26:17.848139: Pseudo dice [0.7272]
2024-12-14 11:26:17.848868: Epoch time: 512.01 s
2024-12-14 11:26:19.261395: 
2024-12-14 11:26:19.262748: Epoch 108
2024-12-14 11:26:19.263541: Current learning rate: 0.00318
2024-12-14 11:34:42.117071: Validation loss did not improve from -0.51733. Patience: 15/50
2024-12-14 11:34:42.118089: train_loss -0.7758
2024-12-14 11:34:42.118901: val_loss -0.4916
2024-12-14 11:34:42.119616: Pseudo dice [0.7378]
2024-12-14 11:34:42.120336: Epoch time: 502.86 s
2024-12-14 11:34:43.528288: 
2024-12-14 11:34:43.529753: Epoch 109
2024-12-14 11:34:43.530784: Current learning rate: 0.00311
2024-12-14 11:43:19.042791: Validation loss did not improve from -0.51733. Patience: 16/50
2024-12-14 11:43:19.043576: train_loss -0.7801
2024-12-14 11:43:19.044274: val_loss -0.4715
2024-12-14 11:43:19.045100: Pseudo dice [0.7141]
2024-12-14 11:43:19.045773: Epoch time: 515.52 s
2024-12-14 11:43:20.900356: 
2024-12-14 11:43:20.901790: Epoch 110
2024-12-14 11:43:20.902556: Current learning rate: 0.00304
2024-12-14 11:51:48.006402: Validation loss did not improve from -0.51733. Patience: 17/50
2024-12-14 11:51:48.007537: train_loss -0.7775
2024-12-14 11:51:48.008422: val_loss -0.4554
2024-12-14 11:51:48.009106: Pseudo dice [0.6996]
2024-12-14 11:51:48.009815: Epoch time: 507.11 s
2024-12-14 11:51:49.448173: 
2024-12-14 11:51:49.449473: Epoch 111
2024-12-14 11:51:49.450247: Current learning rate: 0.00297
2024-12-14 12:00:47.027968: Validation loss did not improve from -0.51733. Patience: 18/50
2024-12-14 12:00:47.028991: train_loss -0.781
2024-12-14 12:00:47.030371: val_loss -0.4541
2024-12-14 12:00:47.031227: Pseudo dice [0.7076]
2024-12-14 12:00:47.032158: Epoch time: 537.58 s
2024-12-14 12:00:48.393695: 
2024-12-14 12:00:48.394979: Epoch 112
2024-12-14 12:00:48.395753: Current learning rate: 0.00291
2024-12-14 12:09:53.763960: Validation loss did not improve from -0.51733. Patience: 19/50
2024-12-14 12:09:53.764958: train_loss -0.7787
2024-12-14 12:09:53.766253: val_loss -0.4349
2024-12-14 12:09:53.767170: Pseudo dice [0.6985]
2024-12-14 12:09:53.768057: Epoch time: 545.37 s
2024-12-14 12:09:55.159643: 
2024-12-14 12:09:55.160846: Epoch 113
2024-12-14 12:09:55.161689: Current learning rate: 0.00284
2024-12-14 12:18:26.578465: Validation loss did not improve from -0.51733. Patience: 20/50
2024-12-14 12:18:26.579416: train_loss -0.7801
2024-12-14 12:18:26.580470: val_loss -0.4606
2024-12-14 12:18:26.581426: Pseudo dice [0.7065]
2024-12-14 12:18:26.582346: Epoch time: 511.42 s
2024-12-14 12:18:27.971362: 
2024-12-14 12:18:27.972837: Epoch 114
2024-12-14 12:18:27.973707: Current learning rate: 0.00277
2024-12-14 12:27:08.158496: Validation loss did not improve from -0.51733. Patience: 21/50
2024-12-14 12:27:08.159386: train_loss -0.7817
2024-12-14 12:27:08.160300: val_loss -0.4665
2024-12-14 12:27:08.161217: Pseudo dice [0.7245]
2024-12-14 12:27:08.162007: Epoch time: 520.19 s
2024-12-14 12:27:09.976113: 
2024-12-14 12:27:09.977620: Epoch 115
2024-12-14 12:27:09.979086: Current learning rate: 0.0027
2024-12-14 12:35:48.400534: Validation loss did not improve from -0.51733. Patience: 22/50
2024-12-14 12:35:48.401530: train_loss -0.7757
2024-12-14 12:35:48.402751: val_loss -0.4715
2024-12-14 12:35:48.403910: Pseudo dice [0.722]
2024-12-14 12:35:48.405398: Epoch time: 518.43 s
2024-12-14 12:35:49.778465: 
2024-12-14 12:35:49.779774: Epoch 116
2024-12-14 12:35:49.781063: Current learning rate: 0.00263
2024-12-14 12:44:40.358453: Validation loss did not improve from -0.51733. Patience: 23/50
2024-12-14 12:44:40.359480: train_loss -0.7826
2024-12-14 12:44:40.360228: val_loss -0.4756
2024-12-14 12:44:40.360981: Pseudo dice [0.7255]
2024-12-14 12:44:40.361688: Epoch time: 530.58 s
2024-12-14 12:44:40.362397: Yayy! New best EMA pseudo Dice: 0.7129
2024-12-14 12:44:42.590751: 
2024-12-14 12:44:42.592082: Epoch 117
2024-12-14 12:44:42.592844: Current learning rate: 0.00256
2024-12-14 12:53:10.677026: Validation loss did not improve from -0.51733. Patience: 24/50
2024-12-14 12:53:10.677910: train_loss -0.7834
2024-12-14 12:53:10.678744: val_loss -0.466
2024-12-14 12:53:10.679485: Pseudo dice [0.7051]
2024-12-14 12:53:10.680351: Epoch time: 508.09 s
2024-12-14 12:53:12.050465: 
2024-12-14 12:53:12.051650: Epoch 118
2024-12-14 12:53:12.052325: Current learning rate: 0.00249
2024-12-14 13:01:39.722193: Validation loss did not improve from -0.51733. Patience: 25/50
2024-12-14 13:01:39.723444: train_loss -0.7831
2024-12-14 13:01:39.724268: val_loss -0.4758
2024-12-14 13:01:39.725026: Pseudo dice [0.72]
2024-12-14 13:01:39.725807: Epoch time: 507.67 s
2024-12-14 13:01:39.726704: Yayy! New best EMA pseudo Dice: 0.7129
2024-12-14 13:01:41.595753: 
2024-12-14 13:01:41.598341: Epoch 119
2024-12-14 13:01:41.599613: Current learning rate: 0.00242
2024-12-14 13:10:03.991541: Validation loss did not improve from -0.51733. Patience: 26/50
2024-12-14 13:10:03.992521: train_loss -0.786
2024-12-14 13:10:03.993407: val_loss -0.4477
2024-12-14 13:10:03.994298: Pseudo dice [0.7006]
2024-12-14 13:10:03.995150: Epoch time: 502.4 s
2024-12-14 13:10:05.730633: 
2024-12-14 13:10:05.731989: Epoch 120
2024-12-14 13:10:05.732714: Current learning rate: 0.00235
2024-12-14 13:18:51.211968: Validation loss did not improve from -0.51733. Patience: 27/50
2024-12-14 13:18:51.213563: train_loss -0.7855
2024-12-14 13:18:51.214556: val_loss -0.479
2024-12-14 13:18:51.215292: Pseudo dice [0.7218]
2024-12-14 13:18:51.216185: Epoch time: 525.48 s
2024-12-14 13:18:52.590820: 
2024-12-14 13:18:52.591964: Epoch 121
2024-12-14 13:18:52.592716: Current learning rate: 0.00228
2024-12-14 13:27:08.031489: Validation loss did not improve from -0.51733. Patience: 28/50
2024-12-14 13:27:08.032434: train_loss -0.7876
2024-12-14 13:27:08.033453: val_loss -0.5018
2024-12-14 13:27:08.034274: Pseudo dice [0.7364]
2024-12-14 13:27:08.035150: Epoch time: 495.44 s
2024-12-14 13:27:08.035919: Yayy! New best EMA pseudo Dice: 0.7151
2024-12-14 13:27:09.816744: 
2024-12-14 13:27:09.818063: Epoch 122
2024-12-14 13:27:09.818874: Current learning rate: 0.00221
2024-12-14 13:35:32.299709: Validation loss did not improve from -0.51733. Patience: 29/50
2024-12-14 13:35:32.300571: train_loss -0.7886
2024-12-14 13:35:32.301473: val_loss -0.4755
2024-12-14 13:35:32.302156: Pseudo dice [0.7182]
2024-12-14 13:35:32.302902: Epoch time: 502.48 s
2024-12-14 13:35:32.303706: Yayy! New best EMA pseudo Dice: 0.7154
2024-12-14 13:35:34.087841: 
2024-12-14 13:35:34.089221: Epoch 123
2024-12-14 13:35:34.090221: Current learning rate: 0.00214
2024-12-14 13:43:41.383301: Validation loss did not improve from -0.51733. Patience: 30/50
2024-12-14 13:43:41.384230: train_loss -0.7899
2024-12-14 13:43:41.385063: val_loss -0.4939
2024-12-14 13:43:41.385808: Pseudo dice [0.7349]
2024-12-14 13:43:41.386479: Epoch time: 487.3 s
2024-12-14 13:43:41.387141: Yayy! New best EMA pseudo Dice: 0.7173
2024-12-14 13:43:43.163594: 
2024-12-14 13:43:43.164859: Epoch 124
2024-12-14 13:43:43.165570: Current learning rate: 0.00207
2024-12-14 13:52:09.822457: Validation loss did not improve from -0.51733. Patience: 31/50
2024-12-14 13:52:09.823402: train_loss -0.7881
2024-12-14 13:52:09.824145: val_loss -0.4336
2024-12-14 13:52:09.824821: Pseudo dice [0.7054]
2024-12-14 13:52:09.825707: Epoch time: 506.66 s
2024-12-14 13:52:11.572010: 
2024-12-14 13:52:11.573313: Epoch 125
2024-12-14 13:52:11.574170: Current learning rate: 0.00199
2024-12-14 14:00:25.910856: Validation loss did not improve from -0.51733. Patience: 32/50
2024-12-14 14:00:25.913045: train_loss -0.7899
2024-12-14 14:00:25.914110: val_loss -0.4682
2024-12-14 14:00:25.914961: Pseudo dice [0.7158]
2024-12-14 14:00:25.915896: Epoch time: 494.34 s
2024-12-14 14:00:27.274917: 
2024-12-14 14:00:27.276146: Epoch 126
2024-12-14 14:00:27.277130: Current learning rate: 0.00192
2024-12-14 14:08:38.101855: Validation loss did not improve from -0.51733. Patience: 33/50
2024-12-14 14:08:38.104350: train_loss -0.7899
2024-12-14 14:08:38.106720: val_loss -0.4405
2024-12-14 14:08:38.107806: Pseudo dice [0.7029]
2024-12-14 14:08:38.109076: Epoch time: 490.83 s
2024-12-14 14:08:40.612045: 
2024-12-14 14:08:40.613538: Epoch 127
2024-12-14 14:08:40.614664: Current learning rate: 0.00185
2024-12-14 14:16:57.985502: Validation loss did not improve from -0.51733. Patience: 34/50
2024-12-14 14:16:57.986278: train_loss -0.7923
2024-12-14 14:16:57.987202: val_loss -0.4378
2024-12-14 14:16:57.987978: Pseudo dice [0.6984]
2024-12-14 14:16:57.988735: Epoch time: 497.38 s
2024-12-14 14:16:59.389350: 
2024-12-14 14:16:59.390660: Epoch 128
2024-12-14 14:16:59.391417: Current learning rate: 0.00178
2024-12-14 14:24:17.861927: Validation loss did not improve from -0.51733. Patience: 35/50
2024-12-14 14:24:17.862845: train_loss -0.7925
2024-12-14 14:24:17.863588: val_loss -0.4587
2024-12-14 14:24:17.864369: Pseudo dice [0.7211]
2024-12-14 14:24:17.865027: Epoch time: 438.47 s
2024-12-14 14:24:19.282022: 
2024-12-14 14:24:19.283284: Epoch 129
2024-12-14 14:24:19.284138: Current learning rate: 0.0017
2024-12-14 14:32:04.780779: Validation loss did not improve from -0.51733. Patience: 36/50
2024-12-14 14:32:04.781698: train_loss -0.791
2024-12-14 14:32:04.782429: val_loss -0.4992
2024-12-14 14:32:04.783020: Pseudo dice [0.7316]
2024-12-14 14:32:04.783678: Epoch time: 465.5 s
2024-12-14 14:32:06.551169: 
2024-12-14 14:32:06.552615: Epoch 130
2024-12-14 14:32:06.553504: Current learning rate: 0.00163
2024-12-14 14:39:48.439489: Validation loss did not improve from -0.51733. Patience: 37/50
2024-12-14 14:39:48.440261: train_loss -0.7934
2024-12-14 14:39:48.441174: val_loss -0.4583
2024-12-14 14:39:48.442120: Pseudo dice [0.7152]
2024-12-14 14:39:48.442773: Epoch time: 461.89 s
2024-12-14 14:39:49.804780: 
2024-12-14 14:39:49.806095: Epoch 131
2024-12-14 14:39:49.806901: Current learning rate: 0.00156
2024-12-14 14:47:40.021363: Validation loss did not improve from -0.51733. Patience: 38/50
2024-12-14 14:47:40.022314: train_loss -0.7924
2024-12-14 14:47:40.023213: val_loss -0.476
2024-12-14 14:47:40.023927: Pseudo dice [0.7177]
2024-12-14 14:47:40.024669: Epoch time: 470.22 s
2024-12-14 14:47:41.418244: 
2024-12-14 14:47:41.419879: Epoch 132
2024-12-14 14:47:41.420973: Current learning rate: 0.00148
2024-12-14 14:54:23.155827: Validation loss did not improve from -0.51733. Patience: 39/50
2024-12-14 14:54:23.156832: train_loss -0.7932
2024-12-14 14:54:23.157580: val_loss -0.4577
2024-12-14 14:54:23.158355: Pseudo dice [0.7111]
2024-12-14 14:54:23.159067: Epoch time: 401.74 s
2024-12-14 14:54:24.539310: 
2024-12-14 14:54:24.540459: Epoch 133
2024-12-14 14:54:24.541162: Current learning rate: 0.00141
2024-12-14 15:01:24.549536: Validation loss did not improve from -0.51733. Patience: 40/50
2024-12-14 15:01:24.550404: train_loss -0.7941
2024-12-14 15:01:24.551335: val_loss -0.4213
2024-12-14 15:01:24.552070: Pseudo dice [0.6951]
2024-12-14 15:01:24.553015: Epoch time: 420.01 s
2024-12-14 15:01:25.932171: 
2024-12-14 15:01:25.933974: Epoch 134
2024-12-14 15:01:25.934787: Current learning rate: 0.00133
2024-12-14 15:07:52.504792: Validation loss did not improve from -0.51733. Patience: 41/50
2024-12-14 15:07:52.507966: train_loss -0.7928
2024-12-14 15:07:52.509861: val_loss -0.4573
2024-12-14 15:07:52.510815: Pseudo dice [0.7166]
2024-12-14 15:07:52.511847: Epoch time: 386.58 s
2024-12-14 15:07:54.408616: 
2024-12-14 15:07:54.409985: Epoch 135
2024-12-14 15:07:54.410942: Current learning rate: 0.00126
2024-12-14 15:14:16.372278: Validation loss did not improve from -0.51733. Patience: 42/50
2024-12-14 15:14:16.373574: train_loss -0.7961
2024-12-14 15:14:16.374355: val_loss -0.4487
2024-12-14 15:14:16.375028: Pseudo dice [0.7073]
2024-12-14 15:14:16.375844: Epoch time: 381.97 s
2024-12-14 15:14:17.763827: 
2024-12-14 15:14:17.765115: Epoch 136
2024-12-14 15:14:17.765805: Current learning rate: 0.00118
2024-12-14 15:20:24.518271: Validation loss did not improve from -0.51733. Patience: 43/50
2024-12-14 15:20:24.519106: train_loss -0.7895
2024-12-14 15:20:24.519892: val_loss -0.4364
2024-12-14 15:20:24.520758: Pseudo dice [0.7098]
2024-12-14 15:20:24.521482: Epoch time: 366.76 s
2024-12-14 15:20:25.904053: 
2024-12-14 15:20:25.904737: Epoch 137
2024-12-14 15:20:25.905372: Current learning rate: 0.00111
2024-12-14 15:25:58.044832: Validation loss did not improve from -0.51733. Patience: 44/50
2024-12-14 15:25:58.045525: train_loss -0.7949
2024-12-14 15:25:58.046311: val_loss -0.4624
2024-12-14 15:25:58.047028: Pseudo dice [0.7174]
2024-12-14 15:25:58.047697: Epoch time: 332.14 s
2024-12-14 15:25:59.878002: 
2024-12-14 15:25:59.879297: Epoch 138
2024-12-14 15:25:59.880143: Current learning rate: 0.00103
2024-12-14 15:31:24.237745: Validation loss did not improve from -0.51733. Patience: 45/50
2024-12-14 15:31:24.238827: train_loss -0.796
2024-12-14 15:31:24.239699: val_loss -0.4574
2024-12-14 15:31:24.240437: Pseudo dice [0.7148]
2024-12-14 15:31:24.241243: Epoch time: 324.36 s
2024-12-14 15:31:25.639536: 
2024-12-14 15:31:25.640813: Epoch 139
2024-12-14 15:31:25.641428: Current learning rate: 0.00095
2024-12-14 15:36:38.113650: Validation loss did not improve from -0.51733. Patience: 46/50
2024-12-14 15:36:38.114939: train_loss -0.7955
2024-12-14 15:36:38.116140: val_loss -0.4739
2024-12-14 15:36:38.116944: Pseudo dice [0.7216]
2024-12-14 15:36:38.117751: Epoch time: 312.48 s
2024-12-14 15:36:40.006765: 
2024-12-14 15:36:40.007978: Epoch 140
2024-12-14 15:36:40.008671: Current learning rate: 0.00087
2024-12-14 15:42:12.929089: Validation loss did not improve from -0.51733. Patience: 47/50
2024-12-14 15:42:12.930072: train_loss -0.7961
2024-12-14 15:42:12.930851: val_loss -0.4744
2024-12-14 15:42:12.931434: Pseudo dice [0.725]
2024-12-14 15:42:12.932053: Epoch time: 332.92 s
2024-12-14 15:42:14.331450: 
2024-12-14 15:42:14.332702: Epoch 141
2024-12-14 15:42:14.333465: Current learning rate: 0.00079
2024-12-14 15:47:19.596701: Validation loss did not improve from -0.51733. Patience: 48/50
2024-12-14 15:47:19.597670: train_loss -0.7948
2024-12-14 15:47:19.598491: val_loss -0.4556
2024-12-14 15:47:19.599115: Pseudo dice [0.7138]
2024-12-14 15:47:19.599805: Epoch time: 305.27 s
2024-12-14 15:47:21.002776: 
2024-12-14 15:47:21.004075: Epoch 142
2024-12-14 15:47:21.004974: Current learning rate: 0.00071
2024-12-14 15:52:28.594280: Validation loss did not improve from -0.51733. Patience: 49/50
2024-12-14 15:52:28.595295: train_loss -0.7971
2024-12-14 15:52:28.596052: val_loss -0.4479
2024-12-14 15:52:28.596717: Pseudo dice [0.7073]
2024-12-14 15:52:28.597471: Epoch time: 307.59 s
2024-12-14 15:52:29.990541: 
2024-12-14 15:52:29.991881: Epoch 143
2024-12-14 15:52:29.992558: Current learning rate: 0.00063
2024-12-14 15:57:41.568512: Validation loss did not improve from -0.51733. Patience: 50/50
2024-12-14 15:57:41.569419: train_loss -0.7996
2024-12-14 15:57:41.570284: val_loss -0.4891
2024-12-14 15:57:41.570988: Pseudo dice [0.7259]
2024-12-14 15:57:41.571667: Epoch time: 311.58 s
2024-12-14 15:57:43.063911: 
2024-12-14 15:57:43.065269: Epoch 144
2024-12-14 15:57:43.066210: Current learning rate: 0.00055
2024-12-14 16:02:51.461673: Validation loss did not improve from -0.51733. Patience: 51/50
2024-12-14 16:02:51.462660: train_loss -0.7997
2024-12-14 16:02:51.463374: val_loss -0.4743
2024-12-14 16:02:51.464391: Pseudo dice [0.7206]
2024-12-14 16:02:51.465208: Epoch time: 308.4 s
2024-12-14 16:02:53.252210: 
2024-12-14 16:02:53.253175: Epoch 145
2024-12-14 16:02:53.253814: Current learning rate: 0.00047
2024-12-14 16:08:07.824728: Validation loss did not improve from -0.51733. Patience: 52/50
2024-12-14 16:08:07.827823: train_loss -0.798
2024-12-14 16:08:07.830210: val_loss -0.4485
2024-12-14 16:08:07.831007: Pseudo dice [0.7088]
2024-12-14 16:08:07.832229: Epoch time: 314.58 s
2024-12-14 16:08:09.330303: 
2024-12-14 16:08:09.331621: Epoch 146
2024-12-14 16:08:09.332324: Current learning rate: 0.00038
2024-12-14 16:13:19.111017: Validation loss did not improve from -0.51733. Patience: 53/50
2024-12-14 16:13:19.113472: train_loss -0.7996
2024-12-14 16:13:19.114252: val_loss -0.4637
2024-12-14 16:13:19.114924: Pseudo dice [0.7183]
2024-12-14 16:13:19.115602: Epoch time: 309.78 s
2024-12-14 16:13:20.547598: 
2024-12-14 16:13:20.549124: Epoch 147
2024-12-14 16:13:20.550144: Current learning rate: 0.0003
2024-12-14 16:17:41.387876: Validation loss did not improve from -0.51733. Patience: 54/50
2024-12-14 16:17:41.389324: train_loss -0.7997
2024-12-14 16:17:41.390523: val_loss -0.4411
2024-12-14 16:17:41.391586: Pseudo dice [0.7034]
2024-12-14 16:17:41.392627: Epoch time: 260.84 s
2024-12-14 16:17:43.232662: 
2024-12-14 16:17:43.234051: Epoch 148
2024-12-14 16:17:43.234870: Current learning rate: 0.00021
2024-12-14 16:21:37.984417: Validation loss did not improve from -0.51733. Patience: 55/50
2024-12-14 16:21:37.985437: train_loss -0.8012
2024-12-14 16:21:37.986329: val_loss -0.4455
2024-12-14 16:21:37.987285: Pseudo dice [0.7082]
2024-12-14 16:21:37.988219: Epoch time: 234.75 s
2024-12-14 16:21:39.399832: 
2024-12-14 16:21:39.401092: Epoch 149
2024-12-14 16:21:39.402022: Current learning rate: 0.00011
2024-12-14 16:25:51.532182: Validation loss did not improve from -0.51733. Patience: 56/50
2024-12-14 16:25:51.533165: train_loss -0.8007
2024-12-14 16:25:51.533991: val_loss -0.454
2024-12-14 16:25:51.534599: Pseudo dice [0.7125]
2024-12-14 16:25:51.535287: Epoch time: 252.13 s
2024-12-14 16:25:53.346857: Training done.
2024-12-14 16:25:53.581988: Using splits from existing split file: /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset307_Sohee_Calcium_OCT_CrossValidation/splits_final_80.json
2024-12-14 16:25:53.594107: The split file contains 5 splits.
2024-12-14 16:25:53.595013: Desired fold for training: 0
2024-12-14 16:25:53.595853: This split has 6 training and 4 validation cases.
2024-12-14 16:25:53.597120: predicting 101-045
2024-12-14 16:25:53.640836: 101-045, shape torch.Size([1, 375, 498, 498]), rank 0
2024-12-14 16:28:23.791778: predicting 701-013
2024-12-14 16:28:23.835443: 701-013, shape torch.Size([1, 375, 498, 498]), rank 0
2024-12-14 16:30:45.993082: predicting 704-003
2024-12-14 16:30:46.005574: 704-003, shape torch.Size([1, 375, 498, 498]), rank 0
2024-12-14 16:32:45.235356: predicting 706-005
2024-12-14 16:32:45.249034: 706-005, shape torch.Size([1, 375, 498, 498]), rank 0
2024-12-14 16:35:13.182735: Validation complete
2024-12-14 16:35:13.183369: Mean Validation Dice:  0.7140117950797744

############################
INFO: You are using the old nnU-Net default plans. We have updated our recommendations. Please consider using those instead! Read more here: https://github.com/MIC-DKFZ/nnUNet/blob/master/documentation/resenc_presets.md
############################

Using device: cuda:0

#######################################################################
Please cite the following paper when using nnU-Net:
Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.
#######################################################################

2024-12-14 16:35:24.229369: Using torch.compile...

############################
INFO: You are using the old nnU-Net default plans. We have updated our recommendations. Please consider using those instead! Read more here: https://github.com/MIC-DKFZ/nnUNet/blob/master/documentation/resenc_presets.md
############################

Using device: cuda:0

#######################################################################
Please cite the following paper when using nnU-Net:
Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.
#######################################################################

2024-12-14 16:35:24.229261: Using torch.compile...
/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
################### Loading pretrained weights from file  /home/gridsan/nchutisilp/projects/OpenAI-CLIP/clip_preivl_poststent_pretrained_nnUNet.pt ###################
Below is the list of overlapping blocks in pretrained model and nnUNet architecture:
encoder.stages.0.0.convs.0.conv.weight shape torch.Size([32, 1, 3, 3, 3])
encoder.stages.0.0.convs.0.conv.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.norm.weight shape torch.Size([32])
encoder.stages.0.0.convs.0.norm.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.0.weight shape torch.Size([32, 1, 3, 3, 3])
encoder.stages.0.0.convs.0.all_modules.0.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.1.weight shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.1.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
encoder.stages.0.0.convs.1.conv.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.norm.weight shape torch.Size([32])
encoder.stages.0.0.convs.1.norm.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
encoder.stages.0.0.convs.1.all_modules.0.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.1.weight shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.1.bias shape torch.Size([32])
encoder.stages.1.0.convs.0.conv.weight shape torch.Size([64, 32, 3, 3, 3])
encoder.stages.1.0.convs.0.conv.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.norm.weight shape torch.Size([64])
encoder.stages.1.0.convs.0.norm.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.0.weight shape torch.Size([64, 32, 3, 3, 3])
encoder.stages.1.0.convs.0.all_modules.0.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.1.weight shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.1.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
encoder.stages.1.0.convs.1.conv.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.norm.weight shape torch.Size([64])
encoder.stages.1.0.convs.1.norm.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
encoder.stages.1.0.convs.1.all_modules.0.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.1.weight shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.1.bias shape torch.Size([64])
encoder.stages.2.0.convs.0.conv.weight shape torch.Size([128, 64, 3, 3, 3])
encoder.stages.2.0.convs.0.conv.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.norm.weight shape torch.Size([128])
encoder.stages.2.0.convs.0.norm.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.0.weight shape torch.Size([128, 64, 3, 3, 3])
encoder.stages.2.0.convs.0.all_modules.0.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.1.weight shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.1.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
encoder.stages.2.0.convs.1.conv.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.norm.weight shape torch.Size([128])
encoder.stages.2.0.convs.1.norm.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
encoder.stages.2.0.convs.1.all_modules.0.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.1.weight shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.1.bias shape torch.Size([128])
encoder.stages.3.0.convs.0.conv.weight shape torch.Size([256, 128, 3, 3, 3])
encoder.stages.3.0.convs.0.conv.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.norm.weight shape torch.Size([256])
encoder.stages.3.0.convs.0.norm.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.0.weight shape torch.Size([256, 128, 3, 3, 3])
encoder.stages.3.0.convs.0.all_modules.0.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.1.weight shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.1.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
encoder.stages.3.0.convs.1.conv.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.norm.weight shape torch.Size([256])
encoder.stages.3.0.convs.1.norm.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
encoder.stages.3.0.convs.1.all_modules.0.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.1.weight shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.1.bias shape torch.Size([256])
encoder.stages.4.0.convs.0.conv.weight shape torch.Size([320, 256, 3, 3, 3])
encoder.stages.4.0.convs.0.conv.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.norm.weight shape torch.Size([320])
encoder.stages.4.0.convs.0.norm.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.0.weight shape torch.Size([320, 256, 3, 3, 3])
encoder.stages.4.0.convs.0.all_modules.0.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.1.weight shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.1.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.4.0.convs.1.conv.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.norm.weight shape torch.Size([320])
encoder.stages.4.0.convs.1.norm.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.4.0.convs.1.all_modules.0.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.1.weight shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.1.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.0.conv.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.norm.weight shape torch.Size([320])
encoder.stages.5.0.convs.0.norm.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.0.all_modules.0.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.1.weight shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.1.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.1.conv.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.norm.weight shape torch.Size([320])
encoder.stages.5.0.convs.1.norm.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.1.all_modules.0.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.1.weight shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.0.0.convs.0.conv.weight shape torch.Size([32, 1, 3, 3, 3])
decoder.encoder.stages.0.0.convs.0.conv.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.norm.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.norm.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.0.weight shape torch.Size([32, 1, 3, 3, 3])
decoder.encoder.stages.0.0.convs.0.all_modules.0.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.1.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.1.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.encoder.stages.0.0.convs.1.conv.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.norm.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.norm.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.encoder.stages.0.0.convs.1.all_modules.0.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.1.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.1.bias shape torch.Size([32])
decoder.encoder.stages.1.0.convs.0.conv.weight shape torch.Size([64, 32, 3, 3, 3])
decoder.encoder.stages.1.0.convs.0.conv.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.norm.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.norm.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.0.weight shape torch.Size([64, 32, 3, 3, 3])
decoder.encoder.stages.1.0.convs.0.all_modules.0.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.1.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.1.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.encoder.stages.1.0.convs.1.conv.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.norm.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.norm.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.encoder.stages.1.0.convs.1.all_modules.0.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.1.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.1.bias shape torch.Size([64])
decoder.encoder.stages.2.0.convs.0.conv.weight shape torch.Size([128, 64, 3, 3, 3])
decoder.encoder.stages.2.0.convs.0.conv.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.norm.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.norm.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.0.weight shape torch.Size([128, 64, 3, 3, 3])
decoder.encoder.stages.2.0.convs.0.all_modules.0.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.1.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.1.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.encoder.stages.2.0.convs.1.conv.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.norm.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.norm.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.encoder.stages.2.0.convs.1.all_modules.0.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.1.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.1.bias shape torch.Size([128])
decoder.encoder.stages.3.0.convs.0.conv.weight shape torch.Size([256, 128, 3, 3, 3])
decoder.encoder.stages.3.0.convs.0.conv.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.norm.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.norm.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.0.weight shape torch.Size([256, 128, 3, 3, 3])
decoder.encoder.stages.3.0.convs.0.all_modules.0.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.1.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.1.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.encoder.stages.3.0.convs.1.conv.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.norm.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.norm.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.encoder.stages.3.0.convs.1.all_modules.0.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.1.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.1.bias shape torch.Size([256])
decoder.encoder.stages.4.0.convs.0.conv.weight shape torch.Size([320, 256, 3, 3, 3])
decoder.encoder.stages.4.0.convs.0.conv.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.norm.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.norm.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.0.weight shape torch.Size([320, 256, 3, 3, 3])
decoder.encoder.stages.4.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.4.0.convs.1.conv.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.norm.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.norm.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.4.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.0.conv.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.norm.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.norm.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.1.conv.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.norm.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.norm.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.stages.0.convs.0.conv.weight shape torch.Size([320, 640, 3, 3, 3])
decoder.stages.0.convs.0.conv.bias shape torch.Size([320])
decoder.stages.0.convs.0.norm.weight shape torch.Size([320])
decoder.stages.0.convs.0.norm.bias shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.0.weight shape torch.Size([320, 640, 3, 3, 3])
decoder.stages.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.stages.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.stages.0.convs.1.conv.bias shape torch.Size([320])
decoder.stages.0.convs.1.norm.weight shape torch.Size([320])
decoder.stages.0.convs.1.norm.bias shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.stages.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.stages.1.convs.0.conv.weight shape torch.Size([256, 512, 3, 3, 3])
decoder.stages.1.convs.0.conv.bias shape torch.Size([256])
decoder.stages.1.convs.0.norm.weight shape torch.Size([256])
decoder.stages.1.convs.0.norm.bias shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.0.weight shape torch.Size([256, 512, 3, 3, 3])
decoder.stages.1.convs.0.all_modules.0.bias shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.1.weight shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.1.bias shape torch.Size([256])
decoder.stages.1.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.stages.1.convs.1.conv.bias shape torch.Size([256])
decoder.stages.1.convs.1.norm.weight shape torch.Size([256])
decoder.stages.1.convs.1.norm.bias shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.stages.1.convs.1.all_modules.0.bias shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.1.weight shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.1.bias shape torch.Size([256])
decoder.stages.2.convs.0.conv.weight shape torch.Size([128, 256, 3, 3, 3])
decoder.stages.2.convs.0.conv.bias shape torch.Size([128])
decoder.stages.2.convs.0.norm.weight shape torch.Size([128])
decoder.stages.2.convs.0.norm.bias shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.0.weight shape torch.Size([128, 256, 3, 3, 3])
decoder.stages.2.convs.0.all_modules.0.bias shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.1.weight shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.1.bias shape torch.Size([128])
decoder.stages.2.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.stages.2.convs.1.conv.bias shape torch.Size([128])
decoder.stages.2.convs.1.norm.weight shape torch.Size([128])
decoder.stages.2.convs.1.norm.bias shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.stages.2.convs.1.all_modules.0.bias shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.1.weight shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.1.bias shape torch.Size([128])
decoder.stages.3.convs.0.conv.weight shape torch.Size([64, 128, 3, 3, 3])
decoder.stages.3.convs.0.conv.bias shape torch.Size([64])
decoder.stages.3.convs.0.norm.weight shape torch.Size([64])
decoder.stages.3.convs.0.norm.bias shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.0.weight shape torch.Size([64, 128, 3, 3, 3])
decoder.stages.3.convs.0.all_modules.0.bias shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.1.weight shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.1.bias shape torch.Size([64])
decoder.stages.3.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.stages.3.convs.1.conv.bias shape torch.Size([64])
decoder.stages.3.convs.1.norm.weight shape torch.Size([64])
decoder.stages.3.convs.1.norm.bias shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.stages.3.convs.1.all_modules.0.bias shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.1.weight shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.1.bias shape torch.Size([64])
decoder.stages.4.convs.0.conv.weight shape torch.Size([32, 64, 3, 3, 3])
decoder.stages.4.convs.0.conv.bias shape torch.Size([32])
decoder.stages.4.convs.0.norm.weight shape torch.Size([32])
decoder.stages.4.convs.0.norm.bias shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.0.weight shape torch.Size([32, 64, 3, 3, 3])
decoder.stages.4.convs.0.all_modules.0.bias shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.1.weight shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.1.bias shape torch.Size([32])
decoder.stages.4.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.stages.4.convs.1.conv.bias shape torch.Size([32])
decoder.stages.4.convs.1.norm.weight shape torch.Size([32])
decoder.stages.4.convs.1.norm.bias shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.stages.4.convs.1.all_modules.0.bias shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.1.weight shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.1.bias shape torch.Size([32])
decoder.transpconvs.0.weight shape torch.Size([320, 320, 1, 2, 2])
decoder.transpconvs.0.bias shape torch.Size([320])
decoder.transpconvs.1.weight shape torch.Size([320, 256, 2, 2, 2])
decoder.transpconvs.1.bias shape torch.Size([256])
decoder.transpconvs.2.weight shape torch.Size([256, 128, 2, 2, 2])
decoder.transpconvs.2.bias shape torch.Size([128])
decoder.transpconvs.3.weight shape torch.Size([128, 64, 2, 2, 2])
decoder.transpconvs.3.bias shape torch.Size([64])
decoder.transpconvs.4.weight shape torch.Size([64, 32, 2, 2, 2])
decoder.transpconvs.4.bias shape torch.Size([32])
################### Done ###################
2024-12-14 16:35:26.002944: do_dummy_2d_data_aug: True
2024-12-14 16:35:26.003899: Using splits from existing split file: /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset307_Sohee_Calcium_OCT_CrossValidation/splits_final_80.json
2024-12-14 16:35:26.005629: The split file contains 5 splits.
2024-12-14 16:35:26.006874: Desired fold for training: 2
2024-12-14 16:35:26.007874: This split has 6 training and 3 validation cases.
################### Loading pretrained weights from file  /home/gridsan/nchutisilp/projects/OpenAI-CLIP/clip_preivl_poststent_pretrained_nnUNet.pt ###################
Below is the list of overlapping blocks in pretrained model and nnUNet architecture:
encoder.stages.0.0.convs.0.conv.weight shape torch.Size([32, 1, 3, 3, 3])
encoder.stages.0.0.convs.0.conv.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.norm.weight shape torch.Size([32])
encoder.stages.0.0.convs.0.norm.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.0.weight shape torch.Size([32, 1, 3, 3, 3])
encoder.stages.0.0.convs.0.all_modules.0.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.1.weight shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.1.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
encoder.stages.0.0.convs.1.conv.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.norm.weight shape torch.Size([32])
encoder.stages.0.0.convs.1.norm.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
encoder.stages.0.0.convs.1.all_modules.0.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.1.weight shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.1.bias shape torch.Size([32])
encoder.stages.1.0.convs.0.conv.weight shape torch.Size([64, 32, 3, 3, 3])
encoder.stages.1.0.convs.0.conv.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.norm.weight shape torch.Size([64])
encoder.stages.1.0.convs.0.norm.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.0.weight shape torch.Size([64, 32, 3, 3, 3])
encoder.stages.1.0.convs.0.all_modules.0.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.1.weight shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.1.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
encoder.stages.1.0.convs.1.conv.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.norm.weight shape torch.Size([64])
encoder.stages.1.0.convs.1.norm.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
encoder.stages.1.0.convs.1.all_modules.0.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.1.weight shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.1.bias shape torch.Size([64])
encoder.stages.2.0.convs.0.conv.weight shape torch.Size([128, 64, 3, 3, 3])
encoder.stages.2.0.convs.0.conv.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.norm.weight shape torch.Size([128])
encoder.stages.2.0.convs.0.norm.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.0.weight shape torch.Size([128, 64, 3, 3, 3])
encoder.stages.2.0.convs.0.all_modules.0.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.1.weight shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.1.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
encoder.stages.2.0.convs.1.conv.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.norm.weight shape torch.Size([128])
encoder.stages.2.0.convs.1.norm.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
encoder.stages.2.0.convs.1.all_modules.0.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.1.weight shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.1.bias shape torch.Size([128])
encoder.stages.3.0.convs.0.conv.weight shape torch.Size([256, 128, 3, 3, 3])
encoder.stages.3.0.convs.0.conv.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.norm.weight shape torch.Size([256])
encoder.stages.3.0.convs.0.norm.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.0.weight shape torch.Size([256, 128, 3, 3, 3])
encoder.stages.3.0.convs.0.all_modules.0.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.1.weight shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.1.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
encoder.stages.3.0.convs.1.conv.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.norm.weight shape torch.Size([256])
encoder.stages.3.0.convs.1.norm.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
encoder.stages.3.0.convs.1.all_modules.0.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.1.weight shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.1.bias shape torch.Size([256])
encoder.stages.4.0.convs.0.conv.weight shape torch.Size([320, 256, 3, 3, 3])
encoder.stages.4.0.convs.0.conv.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.norm.weight shape torch.Size([320])
encoder.stages.4.0.convs.0.norm.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.0.weight shape torch.Size([320, 256, 3, 3, 3])
encoder.stages.4.0.convs.0.all_modules.0.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.1.weight shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.1.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.4.0.convs.1.conv.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.norm.weight shape torch.Size([320])
encoder.stages.4.0.convs.1.norm.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.4.0.convs.1.all_modules.0.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.1.weight shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.1.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.0.conv.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.norm.weight shape torch.Size([320])
encoder.stages.5.0.convs.0.norm.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.0.all_modules.0.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.1.weight shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.1.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.1.conv.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.norm.weight shape torch.Size([320])
encoder.stages.5.0.convs.1.norm.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.1.all_modules.0.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.1.weight shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.0.0.convs.0.conv.weight shape torch.Size([32, 1, 3, 3, 3])
decoder.encoder.stages.0.0.convs.0.conv.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.norm.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.norm.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.0.weight shape torch.Size([32, 1, 3, 3, 3])
decoder.encoder.stages.0.0.convs.0.all_modules.0.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.1.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.1.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.encoder.stages.0.0.convs.1.conv.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.norm.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.norm.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.encoder.stages.0.0.convs.1.all_modules.0.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.1.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.1.bias shape torch.Size([32])
decoder.encoder.stages.1.0.convs.0.conv.weight shape torch.Size([64, 32, 3, 3, 3])
decoder.encoder.stages.1.0.convs.0.conv.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.norm.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.norm.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.0.weight shape torch.Size([64, 32, 3, 3, 3])
decoder.encoder.stages.1.0.convs.0.all_modules.0.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.1.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.1.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.encoder.stages.1.0.convs.1.conv.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.norm.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.norm.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.encoder.stages.1.0.convs.1.all_modules.0.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.1.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.1.bias shape torch.Size([64])
decoder.encoder.stages.2.0.convs.0.conv.weight shape torch.Size([128, 64, 3, 3, 3])
decoder.encoder.stages.2.0.convs.0.conv.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.norm.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.norm.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.0.weight shape torch.Size([128, 64, 3, 3, 3])
decoder.encoder.stages.2.0.convs.0.all_modules.0.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.1.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.1.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.encoder.stages.2.0.convs.1.conv.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.norm.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.norm.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.encoder.stages.2.0.convs.1.all_modules.0.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.1.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.1.bias shape torch.Size([128])
decoder.encoder.stages.3.0.convs.0.conv.weight shape torch.Size([256, 128, 3, 3, 3])
decoder.encoder.stages.3.0.convs.0.conv.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.norm.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.norm.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.0.weight shape torch.Size([256, 128, 3, 3, 3])
decoder.encoder.stages.3.0.convs.0.all_modules.0.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.1.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.1.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.encoder.stages.3.0.convs.1.conv.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.norm.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.norm.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.encoder.stages.3.0.convs.1.all_modules.0.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.1.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.1.bias shape torch.Size([256])
decoder.encoder.stages.4.0.convs.0.conv.weight shape torch.Size([320, 256, 3, 3, 3])
decoder.encoder.stages.4.0.convs.0.conv.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.norm.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.norm.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.0.weight shape torch.Size([320, 256, 3, 3, 3])
decoder.encoder.stages.4.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.4.0.convs.1.conv.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.norm.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.norm.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.4.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.0.conv.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.norm.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.norm.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.1.conv.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.norm.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.norm.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.stages.0.convs.0.conv.weight shape torch.Size([320, 640, 3, 3, 3])
decoder.stages.0.convs.0.conv.bias shape torch.Size([320])
decoder.stages.0.convs.0.norm.weight shape torch.Size([320])
decoder.stages.0.convs.0.norm.bias shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.0.weight shape torch.Size([320, 640, 3, 3, 3])
decoder.stages.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.stages.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.stages.0.convs.1.conv.bias shape torch.Size([320])
decoder.stages.0.convs.1.norm.weight shape torch.Size([320])
decoder.stages.0.convs.1.norm.bias shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.stages.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.stages.1.convs.0.conv.weight shape torch.Size([256, 512, 3, 3, 3])
decoder.stages.1.convs.0.conv.bias shape torch.Size([256])
decoder.stages.1.convs.0.norm.weight shape torch.Size([256])
decoder.stages.1.convs.0.norm.bias shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.0.weight shape torch.Size([256, 512, 3, 3, 3])
decoder.stages.1.convs.0.all_modules.0.bias shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.1.weight shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.1.bias shape torch.Size([256])
decoder.stages.1.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.stages.1.convs.1.conv.bias shape torch.Size([256])
decoder.stages.1.convs.1.norm.weight shape torch.Size([256])
decoder.stages.1.convs.1.norm.bias shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.stages.1.convs.1.all_modules.0.bias shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.1.weight shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.1.bias shape torch.Size([256])
decoder.stages.2.convs.0.conv.weight shape torch.Size([128, 256, 3, 3, 3])
decoder.stages.2.convs.0.conv.bias shape torch.Size([128])
decoder.stages.2.convs.0.norm.weight shape torch.Size([128])
decoder.stages.2.convs.0.norm.bias shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.0.weight shape torch.Size([128, 256, 3, 3, 3])
decoder.stages.2.convs.0.all_modules.0.bias shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.1.weight shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.1.bias shape torch.Size([128])
decoder.stages.2.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.stages.2.convs.1.conv.bias shape torch.Size([128])
decoder.stages.2.convs.1.norm.weight shape torch.Size([128])
decoder.stages.2.convs.1.norm.bias shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.stages.2.convs.1.all_modules.0.bias shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.1.weight shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.1.bias shape torch.Size([128])
decoder.stages.3.convs.0.conv.weight shape torch.Size([64, 128, 3, 3, 3])
decoder.stages.3.convs.0.conv.bias shape torch.Size([64])
decoder.stages.3.convs.0.norm.weight shape torch.Size([64])
decoder.stages.3.convs.0.norm.bias shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.0.weight shape torch.Size([64, 128, 3, 3, 3])
decoder.stages.3.convs.0.all_modules.0.bias shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.1.weight shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.1.bias shape torch.Size([64])
decoder.stages.3.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.stages.3.convs.1.conv.bias shape torch.Size([64])
decoder.stages.3.convs.1.norm.weight shape torch.Size([64])
decoder.stages.3.convs.1.norm.bias shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.stages.3.convs.1.all_modules.0.bias shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.1.weight shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.1.bias shape torch.Size([64])
decoder.stages.4.convs.0.conv.weight shape torch.Size([32, 64, 3, 3, 3])
decoder.stages.4.convs.0.conv.bias shape torch.Size([32])
decoder.stages.4.convs.0.norm.weight shape torch.Size([32])
decoder.stages.4.convs.0.norm.bias shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.0.weight shape torch.Size([32, 64, 3, 3, 3])
decoder.stages.4.convs.0.all_modules.0.bias shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.1.weight shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.1.bias shape torch.Size([32])
decoder.stages.4.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.stages.4.convs.1.conv.bias shape torch.Size([32])
decoder.stages.4.convs.1.norm.weight shape torch.Size([32])
decoder.stages.4.convs.1.norm.bias shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.stages.4.convs.1.all_modules.0.bias shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.1.weight shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.1.bias shape torch.Size([32])
decoder.transpconvs.0.weight shape torch.Size([320, 320, 1, 2, 2])
decoder.transpconvs.0.bias shape torch.Size([320])
decoder.transpconvs.1.weight shape torch.Size([320, 256, 2, 2, 2])
decoder.transpconvs.1.bias shape torch.Size([256])
decoder.transpconvs.2.weight shape torch.Size([256, 128, 2, 2, 2])
decoder.transpconvs.2.bias shape torch.Size([128])
decoder.transpconvs.3.weight shape torch.Size([128, 64, 2, 2, 2])
decoder.transpconvs.3.bias shape torch.Size([64])
decoder.transpconvs.4.weight shape torch.Size([64, 32, 2, 2, 2])
decoder.transpconvs.4.bias shape torch.Size([32])
################### Done ###################
2024-12-14 16:35:26.001222: do_dummy_2d_data_aug: True
2024-12-14 16:35:26.002484: Using splits from existing split file: /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset307_Sohee_Calcium_OCT_CrossValidation/splits_final_80.json
2024-12-14 16:35:26.004485: The split file contains 5 splits.
2024-12-14 16:35:26.005687: Desired fold for training: 3
2024-12-14 16:35:26.006840: This split has 6 training and 3 validation cases.
using pin_memory on device 0
using pin_memory on device 0
using pin_memory on device 0

This is the configuration used by this training:
Configuration name: 3d_32x160x128_b10
 {'data_identifier': 'nnUNetPlans_3d_fullres', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 10, 'patch_size': [32, 160, 128], 'median_image_size_in_voxels': [375.0, 498.0, 498.0], 'spacing': [1.0, 1.0, 1.0], 'normalization_schemes': ['ZScoreNormalization'], 'use_mask_for_norm': [False], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'architecture': {'network_class_name': 'dynamic_network_architectures.architectures.unet.PlainConvUNet', 'arch_kwargs': {'n_stages': 6, 'features_per_stage': [32, 64, 128, 256, 320, 320], 'conv_op': 'torch.nn.modules.conv.Conv3d', 'kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]], 'strides': [[1, 1, 1], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [1, 2, 2]], 'n_conv_per_stage': [2, 2, 2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2, 2, 2], 'conv_bias': True, 'norm_op': 'torch.nn.modules.instancenorm.InstanceNorm3d', 'norm_op_kwargs': {'eps': 1e-05, 'affine': True}, 'dropout_op': None, 'dropout_op_kwargs': None, 'nonlin': 'torch.nn.LeakyReLU', 'nonlin_kwargs': {'inplace': True}, 'deep_supervision': True}, '_kw_requires_import': ['conv_op', 'norm_op', 'dropout_op', 'nonlin']}, 'batch_dice': True, 'inherits_from': '3d_fullres'} 

These are the global plan.json settings:
 {'dataset_name': 'Dataset307_Sohee_Calcium_OCT_CrossValidation', 'plans_name': 'nnUNetPlans', 'original_median_spacing_after_transp': [1.0, 1.0, 1.0], 'original_median_shape_after_transp': [375, 498, 498], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 0.4977976381778717, 'mean': 0.13507525622844696, 'median': 0.09599608182907104, 'min': 0.0, 'percentile_00_5': 0.014754901640117168, 'percentile_99_5': 0.4977976381778717, 'std': 0.12025152146816254}}} 

2024-12-14 16:35:55.107649: unpacking dataset...
using pin_memory on device 0

This is the configuration used by this training:
Configuration name: 3d_32x160x128_b10
 {'data_identifier': 'nnUNetPlans_3d_fullres', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 10, 'patch_size': [32, 160, 128], 'median_image_size_in_voxels': [375.0, 498.0, 498.0], 'spacing': [1.0, 1.0, 1.0], 'normalization_schemes': ['ZScoreNormalization'], 'use_mask_for_norm': [False], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'architecture': {'network_class_name': 'dynamic_network_architectures.architectures.unet.PlainConvUNet', 'arch_kwargs': {'n_stages': 6, 'features_per_stage': [32, 64, 128, 256, 320, 320], 'conv_op': 'torch.nn.modules.conv.Conv3d', 'kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]], 'strides': [[1, 1, 1], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [1, 2, 2]], 'n_conv_per_stage': [2, 2, 2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2, 2, 2], 'conv_bias': True, 'norm_op': 'torch.nn.modules.instancenorm.InstanceNorm3d', 'norm_op_kwargs': {'eps': 1e-05, 'affine': True}, 'dropout_op': None, 'dropout_op_kwargs': None, 'nonlin': 'torch.nn.LeakyReLU', 'nonlin_kwargs': {'inplace': True}, 'deep_supervision': True}, '_kw_requires_import': ['conv_op', 'norm_op', 'dropout_op', 'nonlin']}, 'batch_dice': True, 'inherits_from': '3d_fullres'} 

These are the global plan.json settings:
 {'dataset_name': 'Dataset307_Sohee_Calcium_OCT_CrossValidation', 'plans_name': 'nnUNetPlans', 'original_median_spacing_after_transp': [1.0, 1.0, 1.0], 'original_median_shape_after_transp': [375, 498, 498], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 0.4977976381778717, 'mean': 0.13507525622844696, 'median': 0.09599608182907104, 'min': 0.0, 'percentile_00_5': 0.014754901640117168, 'percentile_99_5': 0.4977976381778717, 'std': 0.12025152146816254}}} 

2024-12-14 16:35:55.783516: unpacking dataset...
2024-12-14 16:36:00.298417: unpacking done...
2024-12-14 16:36:00.306518: Unable to plot network architecture: nnUNet_compile is enabled!
2024-12-14 16:36:00.360252: 
2024-12-14 16:36:00.361552: Epoch 0
2024-12-14 16:36:00.362474: Current learning rate: 0.01
2024-12-14 16:41:38.874431: Validation loss improved from 1000.00000 to -0.16746! Patience: 0/50
2024-12-14 16:41:38.875491: train_loss -0.1132
2024-12-14 16:41:38.876383: val_loss -0.1675
2024-12-14 16:41:38.877102: Pseudo dice [0.5004]
2024-12-14 16:41:38.877928: Epoch time: 338.52 s
2024-12-14 16:41:38.878647: Yayy! New best EMA pseudo Dice: 0.5004
2024-12-14 16:41:40.511993: 
2024-12-14 16:41:40.513281: Epoch 1
2024-12-14 16:41:40.514087: Current learning rate: 0.00994
2024-12-14 16:46:53.554891: Validation loss improved from -0.16746 to -0.19697! Patience: 0/50
2024-12-14 16:46:53.555943: train_loss -0.2607
2024-12-14 16:46:53.557496: val_loss -0.197
2024-12-14 16:46:53.558987: Pseudo dice [0.5316]
2024-12-14 16:46:53.560420: Epoch time: 313.05 s
2024-12-14 16:46:53.561830: Yayy! New best EMA pseudo Dice: 0.5035
2024-12-14 16:46:55.369329: 
2024-12-14 16:46:55.370943: Epoch 2
2024-12-14 16:46:55.372001: Current learning rate: 0.00988
2024-12-14 16:51:07.471807: Validation loss improved from -0.19697 to -0.25632! Patience: 0/50
2024-12-14 16:51:07.472914: train_loss -0.2894
2024-12-14 16:51:07.474012: val_loss -0.2563
2024-12-14 16:51:07.474898: Pseudo dice [0.5531]
2024-12-14 16:51:07.475953: Epoch time: 252.1 s
2024-12-14 16:51:07.476814: Yayy! New best EMA pseudo Dice: 0.5085
2024-12-14 16:51:09.377649: 
2024-12-14 16:51:09.378854: Epoch 3
2024-12-14 16:51:09.379560: Current learning rate: 0.00982
2024-12-14 16:55:47.787693: Validation loss did not improve from -0.25632. Patience: 1/50
2024-12-14 16:55:47.788769: train_loss -0.3326
2024-12-14 16:55:47.789712: val_loss -0.2429
2024-12-14 16:55:47.790735: Pseudo dice [0.5341]
2024-12-14 16:55:47.791648: Epoch time: 278.41 s
2024-12-14 16:55:47.792443: Yayy! New best EMA pseudo Dice: 0.5111
2024-12-14 16:55:49.666326: 
2024-12-14 16:55:49.667814: Epoch 4
2024-12-14 16:55:49.668648: Current learning rate: 0.00976
2024-12-14 17:00:39.229520: Validation loss improved from -0.25632 to -0.33687! Patience: 1/50
2024-12-14 17:00:39.230579: train_loss -0.3736
2024-12-14 17:00:39.231665: val_loss -0.3369
2024-12-14 17:00:39.232517: Pseudo dice [0.6139]
2024-12-14 17:00:39.233266: Epoch time: 289.57 s
2024-12-14 17:00:39.774390: Yayy! New best EMA pseudo Dice: 0.5213
2024-12-14 17:00:41.651352: 
2024-12-14 17:00:41.652789: Epoch 5
2024-12-14 17:00:41.653563: Current learning rate: 0.0097
2024-12-14 17:04:43.368539: Validation loss improved from -0.33687 to -0.39145! Patience: 0/50
2024-12-14 17:04:43.369566: train_loss -0.4
2024-12-14 17:04:43.370731: val_loss -0.3915
2024-12-14 17:04:43.372023: Pseudo dice [0.6559]
2024-12-14 17:04:43.372899: Epoch time: 241.72 s
2024-12-14 17:04:43.373585: Yayy! New best EMA pseudo Dice: 0.5348
2024-12-14 17:04:45.166508: 
2024-12-14 17:04:45.168610: Epoch 6
2024-12-14 17:04:45.170438: Current learning rate: 0.00964
2024-12-14 17:09:02.756501: Validation loss did not improve from -0.39145. Patience: 1/50
2024-12-14 17:09:02.757450: train_loss -0.4127
2024-12-14 17:09:02.758415: val_loss -0.3706
2024-12-14 17:09:02.759398: Pseudo dice [0.6339]
2024-12-14 17:09:02.760332: Epoch time: 257.6 s
2024-12-14 17:09:02.761268: Yayy! New best EMA pseudo Dice: 0.5447
2024-12-14 17:09:04.565343: 
2024-12-14 17:09:04.566792: Epoch 7
2024-12-14 17:09:04.567793: Current learning rate: 0.00958
2024-12-14 17:13:20.670854: Validation loss did not improve from -0.39145. Patience: 2/50
2024-12-14 17:13:20.671947: train_loss -0.4269
2024-12-14 17:13:20.672672: val_loss -0.3856
2024-12-14 17:13:20.673489: Pseudo dice [0.6505]
2024-12-14 17:13:20.674192: Epoch time: 256.11 s
2024-12-14 17:13:20.674833: Yayy! New best EMA pseudo Dice: 0.5553
2024-12-14 17:13:22.930770: 
2024-12-14 17:13:22.932234: Epoch 8
2024-12-14 17:13:22.933163: Current learning rate: 0.00952
2024-12-14 17:17:47.796413: Validation loss did not improve from -0.39145. Patience: 3/50
2024-12-14 17:17:47.797530: train_loss -0.4362
2024-12-14 17:17:47.798332: val_loss -0.3864
2024-12-14 17:17:47.799130: Pseudo dice [0.6337]
2024-12-14 17:17:47.799856: Epoch time: 264.87 s
2024-12-14 17:17:47.800621: Yayy! New best EMA pseudo Dice: 0.5631
2024-12-14 17:17:49.681846: 
2024-12-14 17:17:49.683196: Epoch 9
2024-12-14 17:17:49.683911: Current learning rate: 0.00946
2024-12-14 17:22:44.753437: Validation loss improved from -0.39145 to -0.39630! Patience: 3/50
2024-12-14 17:22:44.754543: train_loss -0.4597
2024-12-14 17:22:44.755527: val_loss -0.3963
2024-12-14 17:22:44.756260: Pseudo dice [0.644]
2024-12-14 17:22:44.757104: Epoch time: 295.07 s
2024-12-14 17:22:45.187361: Yayy! New best EMA pseudo Dice: 0.5712
2024-12-14 17:22:46.986690: 
2024-12-14 17:22:46.988307: Epoch 10
2024-12-14 17:22:46.989665: Current learning rate: 0.0094
2024-12-14 17:27:10.851311: Validation loss improved from -0.39630 to -0.41035! Patience: 0/50
2024-12-14 17:27:10.853148: train_loss -0.4636
2024-12-14 17:27:10.854184: val_loss -0.4104
2024-12-14 17:27:10.854862: Pseudo dice [0.6519]
2024-12-14 17:27:10.855608: Epoch time: 263.87 s
2024-12-14 17:27:10.856297: Yayy! New best EMA pseudo Dice: 0.5793
2024-12-14 17:27:12.697905: 
2024-12-14 17:27:12.699507: Epoch 11
2024-12-14 17:27:12.700462: Current learning rate: 0.00934
2024-12-14 17:31:36.290139: Validation loss improved from -0.41035 to -0.43065! Patience: 0/50
2024-12-14 17:31:36.291196: train_loss -0.4763
2024-12-14 17:31:36.293560: val_loss -0.4306
2024-12-14 17:31:36.294459: Pseudo dice [0.6694]
2024-12-14 17:31:36.295810: Epoch time: 263.59 s
2024-12-14 17:31:36.296525: Yayy! New best EMA pseudo Dice: 0.5883
2024-12-14 17:31:38.187388: 
2024-12-14 17:31:38.188924: Epoch 12
2024-12-14 17:31:38.189788: Current learning rate: 0.00928
2024-12-14 17:35:25.400985: Validation loss improved from -0.43065 to -0.44138! Patience: 0/50
2024-12-14 17:35:25.402036: train_loss -0.4924
2024-12-14 17:35:25.403104: val_loss -0.4414
2024-12-14 17:35:25.403901: Pseudo dice [0.6732]
2024-12-14 17:35:25.404619: Epoch time: 227.22 s
2024-12-14 17:35:25.405284: Yayy! New best EMA pseudo Dice: 0.5968
2024-12-14 17:35:27.219911: 
2024-12-14 17:35:27.221364: Epoch 13
2024-12-14 17:35:27.222093: Current learning rate: 0.00922
2024-12-14 17:39:17.144549: Validation loss did not improve from -0.44138. Patience: 1/50
2024-12-14 17:39:17.145797: train_loss -0.5028
2024-12-14 17:39:17.146692: val_loss -0.4343
2024-12-14 17:39:17.147633: Pseudo dice [0.6736]
2024-12-14 17:39:17.148614: Epoch time: 229.93 s
2024-12-14 17:39:17.149611: Yayy! New best EMA pseudo Dice: 0.6045
2024-12-14 17:39:19.043478: 
2024-12-14 17:39:19.044878: Epoch 14
2024-12-14 17:39:19.046363: Current learning rate: 0.00916
2024-12-14 17:45:00.742850: Validation loss improved from -0.44138 to -0.44327! Patience: 1/50
2024-12-14 17:45:00.744738: train_loss -0.5113
2024-12-14 17:45:00.745756: val_loss -0.4433
2024-12-14 17:45:00.746582: Pseudo dice [0.6805]
2024-12-14 17:45:00.747294: Epoch time: 341.7 s
2024-12-14 17:45:01.100986: Yayy! New best EMA pseudo Dice: 0.6121
2024-12-14 17:45:02.891296: 
2024-12-14 17:45:02.892986: Epoch 15
2024-12-14 17:45:02.894071: Current learning rate: 0.0091
2024-12-14 17:50:29.115515: Validation loss improved from -0.44327 to -0.45735! Patience: 0/50
2024-12-14 17:50:29.116689: train_loss -0.5112
2024-12-14 17:50:29.117956: val_loss -0.4573
2024-12-14 17:50:29.118914: Pseudo dice [0.6792]
2024-12-14 17:50:29.119918: Epoch time: 326.23 s
2024-12-14 17:50:29.120806: Yayy! New best EMA pseudo Dice: 0.6188
2024-12-14 17:50:30.908392: 
2024-12-14 17:50:30.909808: Epoch 16
2024-12-14 17:50:30.910594: Current learning rate: 0.00903
2024-12-14 17:55:39.055104: Validation loss did not improve from -0.45735. Patience: 1/50
2024-12-14 17:55:39.056084: train_loss -0.5106
2024-12-14 17:55:39.057404: val_loss -0.4423
2024-12-14 17:55:39.058292: Pseudo dice [0.6737]
2024-12-14 17:55:39.059036: Epoch time: 308.15 s
2024-12-14 17:55:39.059854: Yayy! New best EMA pseudo Dice: 0.6243
2024-12-14 17:55:40.939857: 
2024-12-14 17:55:40.941098: Epoch 17
2024-12-14 17:55:40.941773: Current learning rate: 0.00897
2024-12-14 18:01:35.174748: Validation loss did not improve from -0.45735. Patience: 2/50
2024-12-14 18:01:35.175731: train_loss -0.5335
2024-12-14 18:01:35.176644: val_loss -0.4373
2024-12-14 18:01:35.177484: Pseudo dice [0.6711]
2024-12-14 18:01:35.178137: Epoch time: 354.24 s
2024-12-14 18:01:35.178895: Yayy! New best EMA pseudo Dice: 0.629
2024-12-14 18:01:37.786779: 
2024-12-14 18:01:37.788417: Epoch 18
2024-12-14 18:01:37.789284: Current learning rate: 0.00891
2024-12-14 18:07:02.943548: Validation loss did not improve from -0.45735. Patience: 3/50
2024-12-14 18:07:02.944604: train_loss -0.5248
2024-12-14 18:07:02.945398: val_loss -0.3933
2024-12-14 18:07:02.946180: Pseudo dice [0.6422]
2024-12-14 18:07:02.946901: Epoch time: 325.16 s
2024-12-14 18:07:02.947560: Yayy! New best EMA pseudo Dice: 0.6303
2024-12-14 18:07:04.754295: 
2024-12-14 18:07:04.755694: Epoch 19
2024-12-14 18:07:04.756465: Current learning rate: 0.00885
2024-12-14 18:12:17.277759: Validation loss improved from -0.45735 to -0.47375! Patience: 3/50
2024-12-14 18:12:17.278846: train_loss -0.5334
2024-12-14 18:12:17.279685: val_loss -0.4738
2024-12-14 18:12:17.280330: Pseudo dice [0.6962]
2024-12-14 18:12:17.280996: Epoch time: 312.53 s
2024-12-14 18:12:17.700345: Yayy! New best EMA pseudo Dice: 0.6369
2024-12-14 18:12:19.521527: 
2024-12-14 18:12:19.522761: Epoch 20
2024-12-14 18:12:19.523576: Current learning rate: 0.00879
2024-12-14 18:17:45.007081: Validation loss did not improve from -0.47375. Patience: 1/50
2024-12-14 18:17:45.008026: train_loss -0.5365
2024-12-14 18:17:45.010600: val_loss -0.4573
2024-12-14 18:17:45.011344: Pseudo dice [0.688]
2024-12-14 18:17:45.012168: Epoch time: 325.49 s
2024-12-14 18:17:45.012862: Yayy! New best EMA pseudo Dice: 0.642
2024-12-14 18:17:46.900581: 
2024-12-14 18:17:46.901857: Epoch 21
2024-12-14 18:17:46.902674: Current learning rate: 0.00873
2024-12-14 18:23:42.300932: Validation loss did not improve from -0.47375. Patience: 2/50
2024-12-14 18:23:42.301858: train_loss -0.547
2024-12-14 18:23:42.302660: val_loss -0.4701
2024-12-14 18:23:42.303274: Pseudo dice [0.6898]
2024-12-14 18:23:42.304025: Epoch time: 355.4 s
2024-12-14 18:23:42.304614: Yayy! New best EMA pseudo Dice: 0.6468
2024-12-14 18:23:44.078108: 
2024-12-14 18:23:44.079596: Epoch 22
2024-12-14 18:23:44.080828: Current learning rate: 0.00867
2024-12-14 18:28:59.652618: Validation loss improved from -0.47375 to -0.49323! Patience: 2/50
2024-12-14 18:28:59.653706: train_loss -0.5488
2024-12-14 18:28:59.654762: val_loss -0.4932
2024-12-14 18:28:59.655618: Pseudo dice [0.7028]
2024-12-14 18:28:59.656502: Epoch time: 315.58 s
2024-12-14 18:28:59.657416: Yayy! New best EMA pseudo Dice: 0.6524
2024-12-14 18:29:01.395447: 
2024-12-14 18:29:01.396585: Epoch 23
2024-12-14 18:29:01.397444: Current learning rate: 0.00861
2024-12-14 18:33:49.680275: Validation loss did not improve from -0.49323. Patience: 1/50
2024-12-14 18:33:49.681282: train_loss -0.5542
2024-12-14 18:33:49.682065: val_loss -0.4885
2024-12-14 18:33:49.682812: Pseudo dice [0.7144]
2024-12-14 18:33:49.683549: Epoch time: 288.29 s
2024-12-14 18:33:49.684234: Yayy! New best EMA pseudo Dice: 0.6586
2024-12-14 18:33:51.391350: 
2024-12-14 18:33:51.392628: Epoch 24
2024-12-14 18:33:51.393390: Current learning rate: 0.00855
2024-12-14 18:39:19.151501: Validation loss did not improve from -0.49323. Patience: 2/50
2024-12-14 18:39:19.155035: train_loss -0.5538
2024-12-14 18:39:19.156245: val_loss -0.4639
2024-12-14 18:39:19.156904: Pseudo dice [0.6982]
2024-12-14 18:39:19.157966: Epoch time: 327.76 s
2024-12-14 18:39:19.575378: Yayy! New best EMA pseudo Dice: 0.6625
2024-12-14 18:39:21.312943: 
2024-12-14 18:39:21.314254: Epoch 25
2024-12-14 18:39:21.315038: Current learning rate: 0.00849
2024-12-14 18:44:51.709836: Validation loss improved from -0.49323 to -0.50159! Patience: 2/50
2024-12-14 18:44:51.710828: train_loss -0.5656
2024-12-14 18:44:51.711703: val_loss -0.5016
2024-12-14 18:44:51.712354: Pseudo dice [0.7139]
2024-12-14 18:44:51.713028: Epoch time: 330.4 s
2024-12-14 18:44:51.713836: Yayy! New best EMA pseudo Dice: 0.6677
2024-12-14 18:44:53.447851: 
2024-12-14 18:44:53.449252: Epoch 26
2024-12-14 18:44:53.449964: Current learning rate: 0.00843
2024-12-14 18:49:56.964799: Validation loss did not improve from -0.50159. Patience: 1/50
2024-12-14 18:49:56.966535: train_loss -0.5741
2024-12-14 18:49:56.967342: val_loss -0.4595
2024-12-14 18:49:56.967996: Pseudo dice [0.6853]
2024-12-14 18:49:56.968732: Epoch time: 303.52 s
2024-12-14 18:49:56.969402: Yayy! New best EMA pseudo Dice: 0.6694
2024-12-14 18:49:58.758507: 
2024-12-14 18:49:58.759842: Epoch 27
2024-12-14 18:49:58.760633: Current learning rate: 0.00836
2024-12-14 18:54:39.771077: Validation loss did not improve from -0.50159. Patience: 2/50
2024-12-14 18:54:39.772046: train_loss -0.5788
2024-12-14 18:54:39.772818: val_loss -0.5015
2024-12-14 18:54:39.773546: Pseudo dice [0.7188]
2024-12-14 18:54:39.774597: Epoch time: 281.01 s
2024-12-14 18:54:39.775443: Yayy! New best EMA pseudo Dice: 0.6744
2024-12-14 18:54:41.539824: 
2024-12-14 18:54:41.541205: Epoch 28
2024-12-14 18:54:41.541926: Current learning rate: 0.0083
2024-12-14 19:00:17.556196: Validation loss did not improve from -0.50159. Patience: 3/50
2024-12-14 19:00:17.557622: train_loss -0.5909
2024-12-14 19:00:17.558861: val_loss -0.4434
2024-12-14 19:00:17.560070: Pseudo dice [0.6638]
2024-12-14 19:00:17.561233: Epoch time: 336.02 s
2024-12-14 19:00:19.330998: 
2024-12-14 19:00:19.332421: Epoch 29
2024-12-14 19:00:19.333250: Current learning rate: 0.00824
2024-12-14 19:07:00.700139: Validation loss did not improve from -0.50159. Patience: 4/50
2024-12-14 19:07:00.701106: train_loss -0.596
2024-12-14 19:07:00.701802: val_loss -0.3989
2024-12-14 19:07:00.702431: Pseudo dice [0.651]
2024-12-14 19:07:00.703160: Epoch time: 401.37 s
2024-12-14 19:07:02.787231: 
2024-12-14 19:07:02.788542: Epoch 30
2024-12-14 19:07:02.789186: Current learning rate: 0.00818
2024-12-14 19:13:37.592841: Validation loss did not improve from -0.50159. Patience: 5/50
2024-12-14 19:13:37.593934: train_loss -0.6015
2024-12-14 19:13:37.594740: val_loss -0.4646
2024-12-14 19:13:37.595562: Pseudo dice [0.6891]
2024-12-14 19:13:37.596379: Epoch time: 394.81 s
2024-12-14 19:13:38.986096: 
2024-12-14 19:13:38.987317: Epoch 31
2024-12-14 19:13:38.988021: Current learning rate: 0.00812
2024-12-14 19:20:16.467243: Validation loss did not improve from -0.50159. Patience: 6/50
2024-12-14 19:20:16.468188: train_loss -0.5949
2024-12-14 19:20:16.469135: val_loss -0.4846
2024-12-14 19:20:16.469969: Pseudo dice [0.7015]
2024-12-14 19:20:16.470911: Epoch time: 397.48 s
2024-12-14 19:20:16.471783: Yayy! New best EMA pseudo Dice: 0.6757
2024-12-14 19:20:18.239035: 
2024-12-14 19:20:18.240358: Epoch 32
2024-12-14 19:20:18.241840: Current learning rate: 0.00806
2024-12-14 19:26:53.844225: Validation loss did not improve from -0.50159. Patience: 7/50
2024-12-14 19:26:53.845210: train_loss -0.6028
2024-12-14 19:26:53.846061: val_loss -0.4622
2024-12-14 19:26:53.846806: Pseudo dice [0.6917]
2024-12-14 19:26:53.847632: Epoch time: 395.61 s
2024-12-14 19:26:53.848386: Yayy! New best EMA pseudo Dice: 0.6773
2024-12-14 19:26:55.641978: 
2024-12-14 19:26:55.643747: Epoch 33
2024-12-14 19:26:55.644780: Current learning rate: 0.008
2024-12-14 19:33:38.973611: Validation loss improved from -0.50159 to -0.51204! Patience: 7/50
2024-12-14 19:33:38.974595: train_loss -0.6027
2024-12-14 19:33:38.975575: val_loss -0.512
2024-12-14 19:33:38.976399: Pseudo dice [0.7223]
2024-12-14 19:33:38.977189: Epoch time: 403.33 s
2024-12-14 19:33:38.977909: Yayy! New best EMA pseudo Dice: 0.6818
2024-12-14 19:33:40.806818: 
2024-12-14 19:33:40.808219: Epoch 34
2024-12-14 19:33:40.809139: Current learning rate: 0.00793
2024-12-14 19:40:14.857062: Validation loss improved from -0.51204 to -0.52856! Patience: 0/50
2024-12-14 19:40:14.858004: train_loss -0.5963
2024-12-14 19:40:14.858880: val_loss -0.5286
2024-12-14 19:40:14.859668: Pseudo dice [0.7214]
2024-12-14 19:40:14.860499: Epoch time: 394.05 s
2024-12-14 19:40:15.264413: Yayy! New best EMA pseudo Dice: 0.6858
2024-12-14 19:40:17.100871: 
2024-12-14 19:40:17.102325: Epoch 35
2024-12-14 19:40:17.103431: Current learning rate: 0.00787
2024-12-14 19:46:44.502378: Validation loss did not improve from -0.52856. Patience: 1/50
2024-12-14 19:46:44.503481: train_loss -0.5981
2024-12-14 19:46:44.504348: val_loss -0.4963
2024-12-14 19:46:44.505032: Pseudo dice [0.7042]
2024-12-14 19:46:44.505913: Epoch time: 387.4 s
2024-12-14 19:46:44.506747: Yayy! New best EMA pseudo Dice: 0.6876
2024-12-14 19:46:46.293864: 
2024-12-14 19:46:46.295364: Epoch 36
2024-12-14 19:46:46.296235: Current learning rate: 0.00781
2024-12-14 19:53:09.941014: Validation loss did not improve from -0.52856. Patience: 2/50
2024-12-14 19:53:09.942775: train_loss -0.6101
2024-12-14 19:53:09.943881: val_loss -0.4789
2024-12-14 19:53:09.944537: Pseudo dice [0.6895]
2024-12-14 19:53:09.945354: Epoch time: 383.65 s
2024-12-14 19:53:09.946043: Yayy! New best EMA pseudo Dice: 0.6878
2024-12-14 19:53:11.783801: 
2024-12-14 19:53:11.785203: Epoch 37
2024-12-14 19:53:11.786186: Current learning rate: 0.00775
2024-12-14 19:59:37.411725: Validation loss did not improve from -0.52856. Patience: 3/50
2024-12-14 19:59:37.413430: train_loss -0.6076
2024-12-14 19:59:37.414227: val_loss -0.5024
2024-12-14 19:59:37.414967: Pseudo dice [0.7183]
2024-12-14 19:59:37.415646: Epoch time: 385.63 s
2024-12-14 19:59:37.416253: Yayy! New best EMA pseudo Dice: 0.6909
2024-12-14 19:59:39.233153: 
2024-12-14 19:59:39.234653: Epoch 38
2024-12-14 19:59:39.235707: Current learning rate: 0.00769
2024-12-14 20:06:26.115511: Validation loss did not improve from -0.52856. Patience: 4/50
2024-12-14 20:06:26.116506: train_loss -0.6144
2024-12-14 20:06:26.117338: val_loss -0.4995
2024-12-14 20:06:26.118152: Pseudo dice [0.7112]
2024-12-14 20:06:26.118978: Epoch time: 406.88 s
2024-12-14 20:06:26.120070: Yayy! New best EMA pseudo Dice: 0.6929
2024-12-14 20:06:28.279587: 
2024-12-14 20:06:28.281211: Epoch 39
2024-12-14 20:06:28.282352: Current learning rate: 0.00763
2024-12-14 20:13:09.573529: Validation loss did not improve from -0.52856. Patience: 5/50
2024-12-14 20:13:09.574787: train_loss -0.6239
2024-12-14 20:13:09.575739: val_loss -0.5182
2024-12-14 20:13:09.576408: Pseudo dice [0.7246]
2024-12-14 20:13:09.577170: Epoch time: 401.3 s
2024-12-14 20:13:09.961288: Yayy! New best EMA pseudo Dice: 0.6961
2024-12-14 20:13:11.793213: 
2024-12-14 20:13:11.794383: Epoch 40
2024-12-14 20:13:11.795245: Current learning rate: 0.00756
2024-12-14 20:19:31.097405: Validation loss did not improve from -0.52856. Patience: 6/50
2024-12-14 20:19:31.098397: train_loss -0.6233
2024-12-14 20:19:31.099571: val_loss -0.4895
2024-12-14 20:19:31.100682: Pseudo dice [0.7045]
2024-12-14 20:19:31.101435: Epoch time: 379.31 s
2024-12-14 20:19:31.102336: Yayy! New best EMA pseudo Dice: 0.6969
2024-12-14 20:19:32.912770: 
2024-12-14 20:19:32.914305: Epoch 41
2024-12-14 20:19:32.915349: Current learning rate: 0.0075
2024-12-14 20:25:58.745135: Validation loss did not improve from -0.52856. Patience: 7/50
2024-12-14 20:25:58.746189: train_loss -0.6282
2024-12-14 20:25:58.747016: val_loss -0.5139
2024-12-14 20:25:58.747762: Pseudo dice [0.7252]
2024-12-14 20:25:58.748404: Epoch time: 385.83 s
2024-12-14 20:25:58.749440: Yayy! New best EMA pseudo Dice: 0.6997
2024-12-14 20:26:00.473960: 
2024-12-14 20:26:00.475494: Epoch 42
2024-12-14 20:26:00.476255: Current learning rate: 0.00744
2024-12-14 20:32:41.055395: Validation loss did not improve from -0.52856. Patience: 8/50
2024-12-14 20:32:41.056428: train_loss -0.641
2024-12-14 20:32:41.057328: val_loss -0.459
2024-12-14 20:32:41.057974: Pseudo dice [0.6807]
2024-12-14 20:32:41.058754: Epoch time: 400.58 s
2024-12-14 20:32:42.397704: 
2024-12-14 20:32:42.399081: Epoch 43
2024-12-14 20:32:42.399787: Current learning rate: 0.00738
2024-12-14 20:39:17.591787: Validation loss did not improve from -0.52856. Patience: 9/50
2024-12-14 20:39:17.592935: train_loss -0.6297
2024-12-14 20:39:17.594195: val_loss -0.5221
2024-12-14 20:39:17.595104: Pseudo dice [0.7303]
2024-12-14 20:39:17.596166: Epoch time: 395.2 s
2024-12-14 20:39:17.597442: Yayy! New best EMA pseudo Dice: 0.7011
2024-12-14 20:39:19.364919: 
2024-12-14 20:39:19.366127: Epoch 44
2024-12-14 20:39:19.366819: Current learning rate: 0.00732
2024-12-14 20:45:59.066197: Validation loss did not improve from -0.52856. Patience: 10/50
2024-12-14 20:45:59.067098: train_loss -0.6355
2024-12-14 20:45:59.067913: val_loss -0.5149
2024-12-14 20:45:59.068691: Pseudo dice [0.7217]
2024-12-14 20:45:59.069394: Epoch time: 399.7 s
2024-12-14 20:45:59.476469: Yayy! New best EMA pseudo Dice: 0.7031
2024-12-14 20:46:01.228398: 
2024-12-14 20:46:01.229661: Epoch 45
2024-12-14 20:46:01.230509: Current learning rate: 0.00725
2024-12-14 20:52:49.179495: Validation loss did not improve from -0.52856. Patience: 11/50
2024-12-14 20:52:49.180709: train_loss -0.6366
2024-12-14 20:52:49.181619: val_loss -0.5028
2024-12-14 20:52:49.182385: Pseudo dice [0.7178]
2024-12-14 20:52:49.183239: Epoch time: 407.95 s
2024-12-14 20:52:49.184119: Yayy! New best EMA pseudo Dice: 0.7046
2024-12-14 20:52:50.905683: 
2024-12-14 20:52:50.907020: Epoch 46
2024-12-14 20:52:50.907846: Current learning rate: 0.00719
2024-12-14 20:59:20.564310: Validation loss did not improve from -0.52856. Patience: 12/50
2024-12-14 20:59:20.566609: train_loss -0.6431
2024-12-14 20:59:20.567554: val_loss -0.4912
2024-12-14 20:59:20.568356: Pseudo dice [0.7115]
2024-12-14 20:59:20.569174: Epoch time: 389.66 s
2024-12-14 20:59:20.569940: Yayy! New best EMA pseudo Dice: 0.7053
2024-12-14 20:59:22.322574: 
2024-12-14 20:59:22.324037: Epoch 47
2024-12-14 20:59:22.324860: Current learning rate: 0.00713
2024-12-14 21:06:10.299425: Validation loss did not improve from -0.52856. Patience: 13/50
2024-12-14 21:06:10.301224: train_loss -0.632
2024-12-14 21:06:10.302614: val_loss -0.496
2024-12-14 21:06:10.303654: Pseudo dice [0.7099]
2024-12-14 21:06:10.304650: Epoch time: 407.98 s
2024-12-14 21:06:10.305678: Yayy! New best EMA pseudo Dice: 0.7058
2024-12-14 21:06:12.085943: 
2024-12-14 21:06:12.087600: Epoch 48
2024-12-14 21:06:12.088724: Current learning rate: 0.00707
2024-12-14 21:13:06.790658: Validation loss did not improve from -0.52856. Patience: 14/50
2024-12-14 21:13:06.791637: train_loss -0.6304
2024-12-14 21:13:06.792574: val_loss -0.5154
2024-12-14 21:13:06.793443: Pseudo dice [0.7301]
2024-12-14 21:13:06.794539: Epoch time: 414.71 s
2024-12-14 21:13:06.795376: Yayy! New best EMA pseudo Dice: 0.7082
2024-12-14 21:13:08.592371: 
2024-12-14 21:13:08.594082: Epoch 49
2024-12-14 21:13:08.595285: Current learning rate: 0.007
2024-12-14 21:20:26.706140: Validation loss did not improve from -0.52856. Patience: 15/50
2024-12-14 21:20:26.707210: train_loss -0.6474
2024-12-14 21:20:26.708053: val_loss -0.4933
2024-12-14 21:20:26.708807: Pseudo dice [0.7114]
2024-12-14 21:20:26.709451: Epoch time: 438.12 s
2024-12-14 21:20:27.088718: Yayy! New best EMA pseudo Dice: 0.7085
2024-12-14 21:20:29.926403: 
2024-12-14 21:20:29.927498: Epoch 50
2024-12-14 21:20:29.928348: Current learning rate: 0.00694
2024-12-14 21:27:17.671435: Validation loss did not improve from -0.52856. Patience: 16/50
2024-12-14 21:27:17.672460: train_loss -0.6464
2024-12-14 21:27:17.673372: val_loss -0.5273
2024-12-14 21:27:17.674138: Pseudo dice [0.7243]
2024-12-14 21:27:17.674804: Epoch time: 407.75 s
2024-12-14 21:27:17.675457: Yayy! New best EMA pseudo Dice: 0.7101
2024-12-14 21:27:19.421596: 
2024-12-14 21:27:19.423054: Epoch 51
2024-12-14 21:27:19.424127: Current learning rate: 0.00688
2024-12-14 21:34:36.035274: Validation loss did not improve from -0.52856. Patience: 17/50
2024-12-14 21:34:36.036135: train_loss -0.643
2024-12-14 21:34:36.036888: val_loss -0.4937
2024-12-14 21:34:36.037648: Pseudo dice [0.7126]
2024-12-14 21:34:36.038333: Epoch time: 436.62 s
2024-12-14 21:34:36.039116: Yayy! New best EMA pseudo Dice: 0.7103
2024-12-14 21:34:37.798150: 
2024-12-14 21:34:37.799559: Epoch 52
2024-12-14 21:34:37.800254: Current learning rate: 0.00682
2024-12-14 21:41:44.747226: Validation loss did not improve from -0.52856. Patience: 18/50
2024-12-14 21:41:44.748264: train_loss -0.6478
2024-12-14 21:41:44.749058: val_loss -0.5102
2024-12-14 21:41:44.749906: Pseudo dice [0.718]
2024-12-14 21:41:44.750731: Epoch time: 426.95 s
2024-12-14 21:41:44.751544: Yayy! New best EMA pseudo Dice: 0.7111
2024-12-14 21:41:46.500653: 
2024-12-14 21:41:46.501975: Epoch 53
2024-12-14 21:41:46.502774: Current learning rate: 0.00675
2024-12-14 21:48:54.743765: Validation loss did not improve from -0.52856. Patience: 19/50
2024-12-14 21:48:54.744998: train_loss -0.6566
2024-12-14 21:48:54.746270: val_loss -0.4985
2024-12-14 21:48:54.747328: Pseudo dice [0.7107]
2024-12-14 21:48:54.748266: Epoch time: 428.25 s
2024-12-14 21:48:56.143361: 
2024-12-14 21:48:56.144721: Epoch 54
2024-12-14 21:48:56.145542: Current learning rate: 0.00669
2024-12-14 21:55:34.252591: Validation loss did not improve from -0.52856. Patience: 20/50
2024-12-14 21:55:34.256580: train_loss -0.6581
2024-12-14 21:55:34.257771: val_loss -0.5191
2024-12-14 21:55:34.258485: Pseudo dice [0.7207]
2024-12-14 21:55:34.259674: Epoch time: 398.11 s
2024-12-14 21:55:34.679416: Yayy! New best EMA pseudo Dice: 0.712
2024-12-14 21:55:36.432945: 
2024-12-14 21:55:36.434262: Epoch 55
2024-12-14 21:55:36.435167: Current learning rate: 0.00663
2024-12-14 22:02:06.711097: Validation loss did not improve from -0.52856. Patience: 21/50
2024-12-14 22:02:06.713732: train_loss -0.6511
2024-12-14 22:02:06.715426: val_loss -0.4704
2024-12-14 22:02:06.716187: Pseudo dice [0.6998]
2024-12-14 22:02:06.717105: Epoch time: 390.28 s
2024-12-14 22:02:08.105810: 
2024-12-14 22:02:08.107208: Epoch 56
2024-12-14 22:02:08.107937: Current learning rate: 0.00657
2024-12-14 22:08:58.079839: Validation loss did not improve from -0.52856. Patience: 22/50
2024-12-14 22:08:58.080981: train_loss -0.6648
2024-12-14 22:08:58.081944: val_loss -0.5128
2024-12-14 22:08:58.082678: Pseudo dice [0.7191]
2024-12-14 22:08:58.083417: Epoch time: 409.98 s
2024-12-14 22:08:59.541769: 
2024-12-14 22:08:59.543296: Epoch 57
2024-12-14 22:08:59.544323: Current learning rate: 0.0065
2024-12-14 22:15:39.797999: Validation loss did not improve from -0.52856. Patience: 23/50
2024-12-14 22:15:39.798905: train_loss -0.6625
2024-12-14 22:15:39.799721: val_loss -0.4994
2024-12-14 22:15:39.800413: Pseudo dice [0.7117]
2024-12-14 22:15:39.801189: Epoch time: 400.26 s
2024-12-14 22:15:41.198499: 
2024-12-14 22:15:41.199785: Epoch 58
2024-12-14 22:15:41.200608: Current learning rate: 0.00644
2024-12-14 22:22:30.363819: Validation loss did not improve from -0.52856. Patience: 24/50
2024-12-14 22:22:30.365344: train_loss -0.665
2024-12-14 22:22:30.366565: val_loss -0.5226
2024-12-14 22:22:30.367734: Pseudo dice [0.7278]
2024-12-14 22:22:30.368746: Epoch time: 409.17 s
2024-12-14 22:22:30.370389: Yayy! New best EMA pseudo Dice: 0.7133
2024-12-14 22:22:32.143865: 
2024-12-14 22:22:32.145590: Epoch 59
2024-12-14 22:22:32.146594: Current learning rate: 0.00638
2024-12-14 22:29:24.626339: Validation loss improved from -0.52856 to -0.54332! Patience: 24/50
2024-12-14 22:29:24.627120: train_loss -0.6666
2024-12-14 22:29:24.627919: val_loss -0.5433
2024-12-14 22:29:24.628535: Pseudo dice [0.7353]
2024-12-14 22:29:24.629175: Epoch time: 412.48 s
2024-12-14 22:29:25.019363: Yayy! New best EMA pseudo Dice: 0.7155
2024-12-14 22:29:26.857982: 
2024-12-14 22:29:26.859388: Epoch 60
2024-12-14 22:29:26.860210: Current learning rate: 0.00631
2024-12-14 22:36:14.855651: Validation loss did not improve from -0.54332. Patience: 1/50
2024-12-14 22:36:14.856698: train_loss -0.669
2024-12-14 22:36:14.857508: val_loss -0.5147
2024-12-14 22:36:14.858243: Pseudo dice [0.7283]
2024-12-14 22:36:14.858946: Epoch time: 408.0 s
2024-12-14 22:36:14.859746: Yayy! New best EMA pseudo Dice: 0.7168
2024-12-14 22:36:16.997699: 
2024-12-14 22:36:16.999081: Epoch 61
2024-12-14 22:36:16.999887: Current learning rate: 0.00625
2024-12-14 22:43:16.544697: Validation loss did not improve from -0.54332. Patience: 2/50
2024-12-14 22:43:16.545701: train_loss -0.6729
2024-12-14 22:43:16.546620: val_loss -0.4883
2024-12-14 22:43:16.547428: Pseudo dice [0.7095]
2024-12-14 22:43:16.548157: Epoch time: 419.55 s
2024-12-14 22:43:17.942446: 
2024-12-14 22:43:17.944262: Epoch 62
2024-12-14 22:43:17.945574: Current learning rate: 0.00619
2024-12-14 22:49:59.792927: Validation loss did not improve from -0.54332. Patience: 3/50
2024-12-14 22:49:59.793972: train_loss -0.6726
2024-12-14 22:49:59.794809: val_loss -0.4953
2024-12-14 22:49:59.795406: Pseudo dice [0.7106]
2024-12-14 22:49:59.796072: Epoch time: 401.85 s
2024-12-14 22:50:01.237310: 
2024-12-14 22:50:01.238931: Epoch 63
2024-12-14 22:50:01.239848: Current learning rate: 0.00612
2024-12-14 22:56:44.003150: Validation loss did not improve from -0.54332. Patience: 4/50
2024-12-14 22:56:44.003793: train_loss -0.6768
2024-12-14 22:56:44.004441: val_loss -0.4769
2024-12-14 22:56:44.005090: Pseudo dice [0.6861]
2024-12-14 22:56:44.005762: Epoch time: 402.77 s
2024-12-14 22:56:45.430234: 
2024-12-14 22:56:45.431377: Epoch 64
2024-12-14 22:56:45.432196: Current learning rate: 0.00606
2024-12-14 23:03:38.088198: Validation loss did not improve from -0.54332. Patience: 5/50
2024-12-14 23:03:38.088916: train_loss -0.6788
2024-12-14 23:03:38.090133: val_loss -0.5196
2024-12-14 23:03:38.091261: Pseudo dice [0.7238]
2024-12-14 23:03:38.092173: Epoch time: 412.66 s
2024-12-14 23:03:39.883391: 
2024-12-14 23:03:39.884657: Epoch 65
2024-12-14 23:03:39.885761: Current learning rate: 0.006
2024-12-14 23:10:42.861980: Validation loss did not improve from -0.54332. Patience: 6/50
2024-12-14 23:10:42.863818: train_loss -0.6817
2024-12-14 23:10:42.864692: val_loss -0.5366
2024-12-14 23:10:42.865364: Pseudo dice [0.7443]
2024-12-14 23:10:42.866221: Epoch time: 422.98 s
2024-12-14 23:10:44.306440: 
2024-12-14 23:10:44.307837: Epoch 66
2024-12-14 23:10:44.308848: Current learning rate: 0.00593
2024-12-14 23:17:51.290276: Validation loss did not improve from -0.54332. Patience: 7/50
2024-12-14 23:17:51.291176: train_loss -0.6748
2024-12-14 23:17:51.292219: val_loss -0.5112
2024-12-14 23:17:51.292936: Pseudo dice [0.7111]
2024-12-14 23:17:51.293579: Epoch time: 426.99 s
2024-12-14 23:17:52.708327: 
2024-12-14 23:17:52.709321: Epoch 67
2024-12-14 23:17:52.710217: Current learning rate: 0.00587
2024-12-14 23:24:58.016413: Validation loss did not improve from -0.54332. Patience: 8/50
2024-12-14 23:24:58.017447: train_loss -0.6786
2024-12-14 23:24:58.018465: val_loss -0.5199
2024-12-14 23:24:58.019373: Pseudo dice [0.7297]
2024-12-14 23:24:58.020367: Epoch time: 425.31 s
2024-12-14 23:24:58.021321: Yayy! New best EMA pseudo Dice: 0.7175
2024-12-14 23:24:59.857441: 
2024-12-14 23:24:59.859052: Epoch 68
2024-12-14 23:24:59.860131: Current learning rate: 0.00581
2024-12-14 23:31:59.051084: Validation loss improved from -0.54332 to -0.55044! Patience: 8/50
2024-12-14 23:31:59.051780: train_loss -0.6827
2024-12-14 23:31:59.052456: val_loss -0.5504
2024-12-14 23:31:59.053035: Pseudo dice [0.7375]
2024-12-14 23:31:59.053626: Epoch time: 419.2 s
2024-12-14 23:31:59.054273: Yayy! New best EMA pseudo Dice: 0.7195
2024-12-14 23:32:00.909033: 
2024-12-14 23:32:00.909974: Epoch 69
2024-12-14 23:32:00.910629: Current learning rate: 0.00574
2024-12-14 23:39:28.876478: Validation loss did not improve from -0.55044. Patience: 1/50
2024-12-14 23:39:28.877229: train_loss -0.6778
2024-12-14 23:39:28.877944: val_loss -0.5326
2024-12-14 23:39:28.878669: Pseudo dice [0.7318]
2024-12-14 23:39:28.879415: Epoch time: 447.97 s
2024-12-14 23:39:29.309669: Yayy! New best EMA pseudo Dice: 0.7208
2024-12-14 23:39:31.133021: 
2024-12-14 23:39:31.134078: Epoch 70
2024-12-14 23:39:31.134796: Current learning rate: 0.00568
2024-12-14 23:46:41.107873: Validation loss did not improve from -0.55044. Patience: 2/50
2024-12-14 23:46:41.109191: train_loss -0.6892
2024-12-14 23:46:41.110586: val_loss -0.5324
2024-12-14 23:46:41.111619: Pseudo dice [0.7325]
2024-12-14 23:46:41.112421: Epoch time: 429.98 s
2024-12-14 23:46:41.113265: Yayy! New best EMA pseudo Dice: 0.7219
2024-12-14 23:46:43.253721: 
2024-12-14 23:46:43.254944: Epoch 71
2024-12-14 23:46:43.255675: Current learning rate: 0.00562
2024-12-14 23:53:52.146042: Validation loss did not improve from -0.55044. Patience: 3/50
2024-12-14 23:53:52.146806: train_loss -0.6871
2024-12-14 23:53:52.147642: val_loss -0.5064
2024-12-14 23:53:52.148431: Pseudo dice [0.714]
2024-12-14 23:53:52.149122: Epoch time: 428.89 s
2024-12-14 23:53:53.562615: 
2024-12-14 23:53:53.563639: Epoch 72
2024-12-14 23:53:53.564336: Current learning rate: 0.00555
2024-12-15 00:01:09.730689: Validation loss did not improve from -0.55044. Patience: 4/50
2024-12-15 00:01:09.731484: train_loss -0.7002
2024-12-15 00:01:09.732196: val_loss -0.4825
2024-12-15 00:01:09.732967: Pseudo dice [0.6927]
2024-12-15 00:01:09.733751: Epoch time: 436.17 s
2024-12-15 00:01:11.151901: 
2024-12-15 00:01:11.152691: Epoch 73
2024-12-15 00:01:11.153350: Current learning rate: 0.00549
2024-12-15 00:08:35.984902: Validation loss did not improve from -0.55044. Patience: 5/50
2024-12-15 00:08:35.985647: train_loss -0.689
2024-12-15 00:08:35.986443: val_loss -0.5188
2024-12-15 00:08:35.987045: Pseudo dice [0.7225]
2024-12-15 00:08:35.987747: Epoch time: 444.84 s
2024-12-15 00:08:37.383545: 
2024-12-15 00:08:37.384546: Epoch 74
2024-12-15 00:08:37.385264: Current learning rate: 0.00542
2024-12-15 00:15:22.509428: Validation loss did not improve from -0.55044. Patience: 6/50
2024-12-15 00:15:22.511158: train_loss -0.688
2024-12-15 00:15:22.512129: val_loss -0.526
2024-12-15 00:15:22.513067: Pseudo dice [0.7201]
2024-12-15 00:15:22.513958: Epoch time: 405.13 s
2024-12-15 00:15:24.279469: 
2024-12-15 00:15:24.280722: Epoch 75
2024-12-15 00:15:24.281492: Current learning rate: 0.00536
2024-12-15 00:22:05.763032: Validation loss improved from -0.55044 to -0.55244! Patience: 6/50
2024-12-15 00:22:05.763771: train_loss -0.6905
2024-12-15 00:22:05.764702: val_loss -0.5524
2024-12-15 00:22:05.765620: Pseudo dice [0.7427]
2024-12-15 00:22:05.766404: Epoch time: 401.49 s
2024-12-15 00:22:07.156777: 
2024-12-15 00:22:07.157600: Epoch 76
2024-12-15 00:22:07.158304: Current learning rate: 0.00529
2024-12-15 00:28:48.140275: Validation loss did not improve from -0.55244. Patience: 1/50
2024-12-15 00:28:48.141123: train_loss -0.6891
2024-12-15 00:28:48.142016: val_loss -0.5476
2024-12-15 00:28:48.142727: Pseudo dice [0.738]
2024-12-15 00:28:48.143458: Epoch time: 400.99 s
2024-12-15 00:28:48.144285: Yayy! New best EMA pseudo Dice: 0.7229
2024-12-15 00:28:49.924326: 
2024-12-15 00:28:49.925519: Epoch 77
2024-12-15 00:28:49.926624: Current learning rate: 0.00523
2024-12-15 00:35:36.409556: Validation loss did not improve from -0.55244. Patience: 2/50
2024-12-15 00:35:36.410297: train_loss -0.6928
2024-12-15 00:35:36.411237: val_loss -0.5494
2024-12-15 00:35:36.412073: Pseudo dice [0.7384]
2024-12-15 00:35:36.413045: Epoch time: 406.49 s
2024-12-15 00:35:36.413836: Yayy! New best EMA pseudo Dice: 0.7245
2024-12-15 00:35:38.254580: 
2024-12-15 00:35:38.255818: Epoch 78
2024-12-15 00:35:38.256938: Current learning rate: 0.00517
2024-12-15 00:41:55.729341: Validation loss did not improve from -0.55244. Patience: 3/50
2024-12-15 00:41:55.730332: train_loss -0.6976
2024-12-15 00:41:55.731091: val_loss -0.5456
2024-12-15 00:41:55.731738: Pseudo dice [0.7418]
2024-12-15 00:41:55.732484: Epoch time: 377.48 s
2024-12-15 00:41:55.733319: Yayy! New best EMA pseudo Dice: 0.7262
2024-12-15 00:41:57.614910: 
2024-12-15 00:41:57.615979: Epoch 79
2024-12-15 00:41:57.616709: Current learning rate: 0.0051
2024-12-15 00:48:47.573114: Validation loss did not improve from -0.55244. Patience: 4/50
2024-12-15 00:48:47.573853: train_loss -0.6994
2024-12-15 00:48:47.574612: val_loss -0.5246
2024-12-15 00:48:47.575398: Pseudo dice [0.7266]
2024-12-15 00:48:47.576166: Epoch time: 409.96 s
2024-12-15 00:48:47.995028: Yayy! New best EMA pseudo Dice: 0.7262
2024-12-15 00:48:49.846564: 
2024-12-15 00:48:49.847895: Epoch 80
2024-12-15 00:48:49.849253: Current learning rate: 0.00504
2024-12-15 00:55:29.360343: Validation loss did not improve from -0.55244. Patience: 5/50
2024-12-15 00:55:29.361574: train_loss -0.7029
2024-12-15 00:55:29.362450: val_loss -0.496
2024-12-15 00:55:29.363175: Pseudo dice [0.7093]
2024-12-15 00:55:29.363858: Epoch time: 399.52 s
2024-12-15 00:55:30.776338: 
2024-12-15 00:55:30.777350: Epoch 81
2024-12-15 00:55:30.778157: Current learning rate: 0.00497
2024-12-15 01:02:21.063314: Validation loss did not improve from -0.55244. Patience: 6/50
2024-12-15 01:02:21.063980: train_loss -0.7018
2024-12-15 01:02:21.064647: val_loss -0.5297
2024-12-15 01:02:21.065331: Pseudo dice [0.73]
2024-12-15 01:02:21.066002: Epoch time: 410.29 s
2024-12-15 01:02:22.872727: 
2024-12-15 01:02:22.873583: Epoch 82
2024-12-15 01:02:22.874220: Current learning rate: 0.00491
2024-12-15 01:09:21.312438: Validation loss did not improve from -0.55244. Patience: 7/50
2024-12-15 01:09:21.315245: train_loss -0.712
2024-12-15 01:09:21.316255: val_loss -0.5055
2024-12-15 01:09:21.316954: Pseudo dice [0.723]
2024-12-15 01:09:21.317796: Epoch time: 418.44 s
2024-12-15 01:09:22.653487: 
2024-12-15 01:09:22.654439: Epoch 83
2024-12-15 01:09:22.655454: Current learning rate: 0.00484
2024-12-15 01:16:05.560714: Validation loss did not improve from -0.55244. Patience: 8/50
2024-12-15 01:16:05.561799: train_loss -0.7022
2024-12-15 01:16:05.562936: val_loss -0.5308
2024-12-15 01:16:05.563822: Pseudo dice [0.7256]
2024-12-15 01:16:05.564842: Epoch time: 402.91 s
2024-12-15 01:16:06.904916: 
2024-12-15 01:16:06.906223: Epoch 84
2024-12-15 01:16:06.907382: Current learning rate: 0.00478
2024-12-15 01:22:22.569766: Validation loss did not improve from -0.55244. Patience: 9/50
2024-12-15 01:22:22.571673: train_loss -0.7051
2024-12-15 01:22:22.572870: val_loss -0.5324
2024-12-15 01:22:22.573740: Pseudo dice [0.7376]
2024-12-15 01:22:22.574605: Epoch time: 375.67 s
2024-12-15 01:22:24.283280: 
2024-12-15 01:22:24.284714: Epoch 85
2024-12-15 01:22:24.285393: Current learning rate: 0.00471
2024-12-15 01:29:11.062288: Validation loss did not improve from -0.55244. Patience: 10/50
2024-12-15 01:29:11.062960: train_loss -0.7111
2024-12-15 01:29:11.063617: val_loss -0.5154
2024-12-15 01:29:11.064368: Pseudo dice [0.727]
2024-12-15 01:29:11.065072: Epoch time: 406.78 s
2024-12-15 01:29:11.065695: Yayy! New best EMA pseudo Dice: 0.7263
2024-12-15 01:29:12.729696: 
2024-12-15 01:29:12.731010: Epoch 86
2024-12-15 01:29:12.732215: Current learning rate: 0.00465
2024-12-15 01:36:02.760181: Validation loss did not improve from -0.55244. Patience: 11/50
2024-12-15 01:36:02.761210: train_loss -0.7048
2024-12-15 01:36:02.762210: val_loss -0.4852
2024-12-15 01:36:02.763146: Pseudo dice [0.7]
2024-12-15 01:36:02.764101: Epoch time: 410.03 s
2024-12-15 01:36:04.093711: 
2024-12-15 01:36:04.095121: Epoch 87
2024-12-15 01:36:04.096187: Current learning rate: 0.00458
2024-12-15 01:42:45.381468: Validation loss did not improve from -0.55244. Patience: 12/50
2024-12-15 01:42:45.382186: train_loss -0.7124
2024-12-15 01:42:45.382894: val_loss -0.5512
2024-12-15 01:42:45.383454: Pseudo dice [0.7447]
2024-12-15 01:42:45.384000: Epoch time: 401.29 s
2024-12-15 01:42:46.711361: 
2024-12-15 01:42:46.712725: Epoch 88
2024-12-15 01:42:46.713837: Current learning rate: 0.00452
2024-12-15 01:49:34.885425: Validation loss did not improve from -0.55244. Patience: 13/50
2024-12-15 01:49:34.886093: train_loss -0.7152
2024-12-15 01:49:34.886827: val_loss -0.5319
2024-12-15 01:49:34.887529: Pseudo dice [0.7325]
2024-12-15 01:49:34.888167: Epoch time: 408.18 s
2024-12-15 01:49:34.888825: Yayy! New best EMA pseudo Dice: 0.7264
2024-12-15 01:49:36.598032: 
2024-12-15 01:49:36.599276: Epoch 89
2024-12-15 01:49:36.600312: Current learning rate: 0.00445
2024-12-15 01:56:29.028782: Validation loss did not improve from -0.55244. Patience: 14/50
2024-12-15 01:56:29.029559: train_loss -0.7199
2024-12-15 01:56:29.030384: val_loss -0.5471
2024-12-15 01:56:29.031013: Pseudo dice [0.7453]
2024-12-15 01:56:29.031631: Epoch time: 412.43 s
2024-12-15 01:56:29.442931: Yayy! New best EMA pseudo Dice: 0.7283
2024-12-15 01:56:31.141721: 
2024-12-15 01:56:31.142806: Epoch 90
2024-12-15 01:56:31.143625: Current learning rate: 0.00438
2024-12-15 02:03:18.146160: Validation loss did not improve from -0.55244. Patience: 15/50
2024-12-15 02:03:18.146849: train_loss -0.7189
2024-12-15 02:03:18.147602: val_loss -0.5509
2024-12-15 02:03:18.148194: Pseudo dice [0.7467]
2024-12-15 02:03:18.148871: Epoch time: 407.01 s
2024-12-15 02:03:18.149492: Yayy! New best EMA pseudo Dice: 0.7302
2024-12-15 02:03:19.870212: 
2024-12-15 02:03:19.871143: Epoch 91
2024-12-15 02:03:19.871839: Current learning rate: 0.00432
2024-12-15 02:10:09.204865: Validation loss did not improve from -0.55244. Patience: 16/50
2024-12-15 02:10:09.205541: train_loss -0.7187
2024-12-15 02:10:09.206862: val_loss -0.5392
2024-12-15 02:10:09.207859: Pseudo dice [0.7381]
2024-12-15 02:10:09.209013: Epoch time: 409.34 s
2024-12-15 02:10:09.210213: Yayy! New best EMA pseudo Dice: 0.7309
2024-12-15 02:10:10.920031: 
2024-12-15 02:10:10.921265: Epoch 92
2024-12-15 02:10:10.922168: Current learning rate: 0.00425
2024-12-15 02:17:12.122515: Validation loss improved from -0.55244 to -0.55588! Patience: 16/50
2024-12-15 02:17:12.123546: train_loss -0.7243
2024-12-15 02:17:12.124272: val_loss -0.5559
2024-12-15 02:17:12.124897: Pseudo dice [0.7427]
2024-12-15 02:17:12.125627: Epoch time: 421.2 s
2024-12-15 02:17:12.126271: Yayy! New best EMA pseudo Dice: 0.7321
2024-12-15 02:17:14.164576: 
2024-12-15 02:17:14.165823: Epoch 93
2024-12-15 02:17:14.166517: Current learning rate: 0.00419
2024-12-15 02:24:17.720970: Validation loss did not improve from -0.55588. Patience: 1/50
2024-12-15 02:24:17.722718: train_loss -0.7227
2024-12-15 02:24:17.723472: val_loss -0.5411
2024-12-15 02:24:17.724055: Pseudo dice [0.7317]
2024-12-15 02:24:17.724650: Epoch time: 423.56 s
2024-12-15 02:24:19.063344: 
2024-12-15 02:24:19.064241: Epoch 94
2024-12-15 02:24:19.065116: Current learning rate: 0.00412
2024-12-15 02:30:58.914953: Validation loss did not improve from -0.55588. Patience: 2/50
2024-12-15 02:30:58.915760: train_loss -0.7231
2024-12-15 02:30:58.916649: val_loss -0.5391
2024-12-15 02:30:58.917436: Pseudo dice [0.7379]
2024-12-15 02:30:58.918282: Epoch time: 399.85 s
2024-12-15 02:30:59.272880: Yayy! New best EMA pseudo Dice: 0.7327
2024-12-15 02:31:01.001894: 
2024-12-15 02:31:01.003012: Epoch 95
2024-12-15 02:31:01.004008: Current learning rate: 0.00405
2024-12-15 02:37:48.753230: Validation loss did not improve from -0.55588. Patience: 3/50
2024-12-15 02:37:48.753857: train_loss -0.7216
2024-12-15 02:37:48.754549: val_loss -0.5358
2024-12-15 02:37:48.755210: Pseudo dice [0.7361]
2024-12-15 02:37:48.755889: Epoch time: 407.75 s
2024-12-15 02:37:48.756537: Yayy! New best EMA pseudo Dice: 0.733
2024-12-15 02:37:50.464624: 
2024-12-15 02:37:50.465466: Epoch 96
2024-12-15 02:37:50.466260: Current learning rate: 0.00399
2024-12-15 02:44:42.063544: Validation loss did not improve from -0.55588. Patience: 4/50
2024-12-15 02:44:42.064495: train_loss -0.7249
2024-12-15 02:44:42.065674: val_loss -0.5493
2024-12-15 02:44:42.066825: Pseudo dice [0.7458]
2024-12-15 02:44:42.067868: Epoch time: 411.6 s
2024-12-15 02:44:42.069015: Yayy! New best EMA pseudo Dice: 0.7343
2024-12-15 02:44:43.881799: 
2024-12-15 02:44:43.882801: Epoch 97
2024-12-15 02:44:43.883442: Current learning rate: 0.00392
2024-12-15 02:51:36.998317: Validation loss did not improve from -0.55588. Patience: 5/50
2024-12-15 02:51:36.999125: train_loss -0.7235
2024-12-15 02:51:37.000024: val_loss -0.524
2024-12-15 02:51:37.001205: Pseudo dice [0.72]
2024-12-15 02:51:37.002127: Epoch time: 413.12 s
2024-12-15 02:51:38.357641: 
2024-12-15 02:51:38.358629: Epoch 98
2024-12-15 02:51:38.359685: Current learning rate: 0.00385
2024-12-15 02:58:27.226095: Validation loss did not improve from -0.55588. Patience: 6/50
2024-12-15 02:58:27.226919: train_loss -0.7278
2024-12-15 02:58:27.227895: val_loss -0.4853
2024-12-15 02:58:27.228926: Pseudo dice [0.7041]
2024-12-15 02:58:27.229955: Epoch time: 408.87 s
2024-12-15 02:58:28.627694: 
2024-12-15 02:58:28.628566: Epoch 99
2024-12-15 02:58:28.629209: Current learning rate: 0.00379
2024-12-15 03:05:04.911808: Validation loss improved from -0.55588 to -0.56399! Patience: 6/50
2024-12-15 03:05:04.912549: train_loss -0.7251
2024-12-15 03:05:04.913313: val_loss -0.564
2024-12-15 03:05:04.914052: Pseudo dice [0.7524]
2024-12-15 03:05:04.914722: Epoch time: 396.29 s
2024-12-15 03:05:06.683028: 
2024-12-15 03:05:06.684171: Epoch 100
2024-12-15 03:05:06.685406: Current learning rate: 0.00372
2024-12-15 03:12:03.214248: Validation loss did not improve from -0.56399. Patience: 1/50
2024-12-15 03:12:03.215246: train_loss -0.7294
2024-12-15 03:12:03.216206: val_loss -0.5051
2024-12-15 03:12:03.216955: Pseudo dice [0.7284]
2024-12-15 03:12:03.217605: Epoch time: 416.53 s
2024-12-15 03:12:04.566040: 
2024-12-15 03:12:04.567056: Epoch 101
2024-12-15 03:12:04.568035: Current learning rate: 0.00365
2024-12-15 03:19:01.142655: Validation loss did not improve from -0.56399. Patience: 2/50
2024-12-15 03:19:01.143897: train_loss -0.7323
2024-12-15 03:19:01.144864: val_loss -0.5297
2024-12-15 03:19:01.145569: Pseudo dice [0.7298]
2024-12-15 03:19:01.146315: Epoch time: 416.58 s
2024-12-15 03:19:02.560284: 
2024-12-15 03:19:02.561359: Epoch 102
2024-12-15 03:19:02.562074: Current learning rate: 0.00359
2024-12-15 03:25:49.356294: Validation loss did not improve from -0.56399. Patience: 3/50
2024-12-15 03:25:49.357020: train_loss -0.7301
2024-12-15 03:25:49.357683: val_loss -0.5034
2024-12-15 03:25:49.358256: Pseudo dice [0.7199]
2024-12-15 03:25:49.358866: Epoch time: 406.8 s
2024-12-15 03:25:50.714961: 
2024-12-15 03:25:50.715959: Epoch 103
2024-12-15 03:25:50.717251: Current learning rate: 0.00352
2024-12-15 03:33:09.046948: Validation loss did not improve from -0.56399. Patience: 4/50
2024-12-15 03:33:09.049584: train_loss -0.7319
2024-12-15 03:33:09.050586: val_loss -0.5237
2024-12-15 03:33:09.051288: Pseudo dice [0.7274]
2024-12-15 03:33:09.052030: Epoch time: 438.34 s
2024-12-15 03:33:10.899211: 
2024-12-15 03:33:10.900541: Epoch 104
2024-12-15 03:33:10.901582: Current learning rate: 0.00345
2024-12-15 03:40:18.592655: Validation loss did not improve from -0.56399. Patience: 5/50
2024-12-15 03:40:18.593730: train_loss -0.7391
2024-12-15 03:40:18.594516: val_loss -0.5197
2024-12-15 03:40:18.595112: Pseudo dice [0.7296]
2024-12-15 03:40:18.595836: Epoch time: 427.7 s
2024-12-15 03:40:20.348119: 
2024-12-15 03:40:20.349500: Epoch 105
2024-12-15 03:40:20.350507: Current learning rate: 0.00338
2024-12-15 03:47:25.610168: Validation loss did not improve from -0.56399. Patience: 6/50
2024-12-15 03:47:25.611251: train_loss -0.7362
2024-12-15 03:47:25.612576: val_loss -0.5284
2024-12-15 03:47:25.613606: Pseudo dice [0.7389]
2024-12-15 03:47:25.614549: Epoch time: 425.26 s
2024-12-15 03:47:26.968485: 
2024-12-15 03:47:26.969919: Epoch 106
2024-12-15 03:47:26.971051: Current learning rate: 0.00332
2024-12-15 03:54:20.116786: Validation loss did not improve from -0.56399. Patience: 7/50
2024-12-15 03:54:20.117793: train_loss -0.7355
2024-12-15 03:54:20.118847: val_loss -0.5332
2024-12-15 03:54:20.119893: Pseudo dice [0.738]
2024-12-15 03:54:20.120790: Epoch time: 413.15 s
2024-12-15 03:54:21.485463: 
2024-12-15 03:54:21.486754: Epoch 107
2024-12-15 03:54:21.487392: Current learning rate: 0.00325
2024-12-15 04:01:26.955528: Validation loss did not improve from -0.56399. Patience: 8/50
2024-12-15 04:01:26.956507: train_loss -0.7378
2024-12-15 04:01:26.957280: val_loss -0.514
2024-12-15 04:01:26.957936: Pseudo dice [0.7234]
2024-12-15 04:01:26.958694: Epoch time: 425.47 s
2024-12-15 04:01:28.339378: 
2024-12-15 04:01:28.340852: Epoch 108
2024-12-15 04:01:28.341795: Current learning rate: 0.00318
2024-12-15 04:08:28.302332: Validation loss did not improve from -0.56399. Patience: 9/50
2024-12-15 04:08:28.303437: train_loss -0.7368
2024-12-15 04:08:28.304363: val_loss -0.5357
2024-12-15 04:08:28.305667: Pseudo dice [0.7309]
2024-12-15 04:08:28.306632: Epoch time: 419.97 s
2024-12-15 04:08:29.691013: 
2024-12-15 04:08:29.692340: Epoch 109
2024-12-15 04:08:29.693141: Current learning rate: 0.00311
2024-12-15 04:15:24.444104: Validation loss did not improve from -0.56399. Patience: 10/50
2024-12-15 04:15:24.445184: train_loss -0.738
2024-12-15 04:15:24.445985: val_loss -0.5192
2024-12-15 04:15:24.446747: Pseudo dice [0.7285]
2024-12-15 04:15:24.447487: Epoch time: 414.76 s
2024-12-15 04:15:26.209643: 
2024-12-15 04:15:26.211050: Epoch 110
2024-12-15 04:15:26.211916: Current learning rate: 0.00304
2024-12-15 04:22:35.095397: Validation loss did not improve from -0.56399. Patience: 11/50
2024-12-15 04:22:35.096456: train_loss -0.7402
2024-12-15 04:22:35.097111: val_loss -0.4996
2024-12-15 04:22:35.097786: Pseudo dice [0.7126]
2024-12-15 04:22:35.098476: Epoch time: 428.89 s
2024-12-15 04:22:36.455730: 
2024-12-15 04:22:36.457167: Epoch 111
2024-12-15 04:22:36.457932: Current learning rate: 0.00297
2024-12-15 04:29:48.043149: Validation loss did not improve from -0.56399. Patience: 12/50
2024-12-15 04:29:48.044148: train_loss -0.7377
2024-12-15 04:29:48.044948: val_loss -0.548
2024-12-15 04:29:48.045687: Pseudo dice [0.7469]
2024-12-15 04:29:48.046319: Epoch time: 431.59 s
2024-12-15 04:29:49.410769: 
2024-12-15 04:29:49.412087: Epoch 112
2024-12-15 04:29:49.412948: Current learning rate: 0.00291
2024-12-15 04:37:02.142501: Validation loss did not improve from -0.56399. Patience: 13/50
2024-12-15 04:37:02.144619: train_loss -0.7465
2024-12-15 04:37:02.145570: val_loss -0.5338
2024-12-15 04:37:02.146510: Pseudo dice [0.7368]
2024-12-15 04:37:02.147434: Epoch time: 432.74 s
2024-12-15 04:37:03.495318: 
2024-12-15 04:37:03.496840: Epoch 113
2024-12-15 04:37:03.497563: Current learning rate: 0.00284
2024-12-15 04:44:02.444255: Validation loss did not improve from -0.56399. Patience: 14/50
2024-12-15 04:44:02.445341: train_loss -0.745
2024-12-15 04:44:02.446113: val_loss -0.536
2024-12-15 04:44:02.446999: Pseudo dice [0.7401]
2024-12-15 04:44:02.447801: Epoch time: 418.95 s
2024-12-15 04:44:03.813771: 
2024-12-15 04:44:03.815201: Epoch 114
2024-12-15 04:44:03.816094: Current learning rate: 0.00277
2024-12-15 04:50:44.388250: Validation loss did not improve from -0.56399. Patience: 15/50
2024-12-15 04:50:44.389290: train_loss -0.7476
2024-12-15 04:50:44.390180: val_loss -0.5495
2024-12-15 04:50:44.390935: Pseudo dice [0.7444]
2024-12-15 04:50:44.391752: Epoch time: 400.58 s
2024-12-15 04:50:46.799517: 
2024-12-15 04:50:46.801012: Epoch 115
2024-12-15 04:50:46.801791: Current learning rate: 0.0027
2024-12-15 04:57:37.803221: Validation loss did not improve from -0.56399. Patience: 16/50
2024-12-15 04:57:37.804275: train_loss -0.7417
2024-12-15 04:57:37.805010: val_loss -0.5321
2024-12-15 04:57:37.805658: Pseudo dice [0.7394]
2024-12-15 04:57:37.806404: Epoch time: 411.01 s
2024-12-15 04:57:39.202813: 
2024-12-15 04:57:39.204229: Epoch 116
2024-12-15 04:57:39.205010: Current learning rate: 0.00263
2024-12-15 05:04:16.057002: Validation loss did not improve from -0.56399. Patience: 17/50
2024-12-15 05:04:16.058186: train_loss -0.7458
2024-12-15 05:04:16.058952: val_loss -0.5474
2024-12-15 05:04:16.059645: Pseudo dice [0.7407]
2024-12-15 05:04:16.060360: Epoch time: 396.86 s
2024-12-15 05:04:16.061001: Yayy! New best EMA pseudo Dice: 0.7346
2024-12-15 05:04:17.820441: 
2024-12-15 05:04:17.822068: Epoch 117
2024-12-15 05:04:17.823096: Current learning rate: 0.00256
2024-12-15 05:11:14.521724: Validation loss did not improve from -0.56399. Patience: 18/50
2024-12-15 05:11:14.522692: train_loss -0.7495
2024-12-15 05:11:14.523611: val_loss -0.5482
2024-12-15 05:11:14.524417: Pseudo dice [0.7502]
2024-12-15 05:11:14.525187: Epoch time: 416.7 s
2024-12-15 05:11:14.525905: Yayy! New best EMA pseudo Dice: 0.7362
2024-12-15 05:11:16.294291: 
2024-12-15 05:11:16.295926: Epoch 118
2024-12-15 05:11:16.297096: Current learning rate: 0.00249
2024-12-15 05:18:13.594680: Validation loss did not improve from -0.56399. Patience: 19/50
2024-12-15 05:18:13.596023: train_loss -0.7512
2024-12-15 05:18:13.597345: val_loss -0.4993
2024-12-15 05:18:13.598813: Pseudo dice [0.7222]
2024-12-15 05:18:13.600392: Epoch time: 417.3 s
2024-12-15 05:18:14.981997: 
2024-12-15 05:18:14.983408: Epoch 119
2024-12-15 05:18:14.984175: Current learning rate: 0.00242
2024-12-15 05:25:16.694293: Validation loss improved from -0.56399 to -0.56477! Patience: 19/50
2024-12-15 05:25:16.695341: train_loss -0.746
2024-12-15 05:25:16.696162: val_loss -0.5648
2024-12-15 05:25:16.696934: Pseudo dice [0.7477]
2024-12-15 05:25:16.697603: Epoch time: 421.71 s
2024-12-15 05:25:18.538270: 
2024-12-15 05:25:18.539996: Epoch 120
2024-12-15 05:25:18.541183: Current learning rate: 0.00235
2024-12-15 05:32:15.917083: Validation loss did not improve from -0.56477. Patience: 1/50
2024-12-15 05:32:15.920733: train_loss -0.7529
2024-12-15 05:32:15.921784: val_loss -0.5603
2024-12-15 05:32:15.922642: Pseudo dice [0.7523]
2024-12-15 05:32:15.923560: Epoch time: 417.38 s
2024-12-15 05:32:15.924586: Yayy! New best EMA pseudo Dice: 0.7377
2024-12-15 05:32:17.772289: 
2024-12-15 05:32:17.773145: Epoch 121
2024-12-15 05:32:17.773836: Current learning rate: 0.00228
2024-12-15 05:38:51.445753: Validation loss did not improve from -0.56477. Patience: 2/50
2024-12-15 05:38:51.448673: train_loss -0.7492
2024-12-15 05:38:51.450462: val_loss -0.5332
2024-12-15 05:38:51.451081: Pseudo dice [0.7367]
2024-12-15 05:38:51.451899: Epoch time: 393.68 s
2024-12-15 05:38:52.871111: 
2024-12-15 05:38:52.872342: Epoch 122
2024-12-15 05:38:52.873003: Current learning rate: 0.00221
2024-12-15 05:45:28.828746: Validation loss did not improve from -0.56477. Patience: 3/50
2024-12-15 05:45:28.830034: train_loss -0.7526
2024-12-15 05:45:28.830832: val_loss -0.5459
2024-12-15 05:45:28.831621: Pseudo dice [0.7456]
2024-12-15 05:45:28.832357: Epoch time: 395.96 s
2024-12-15 05:45:28.833075: Yayy! New best EMA pseudo Dice: 0.7384
2024-12-15 05:45:30.580038: 
2024-12-15 05:45:30.581394: Epoch 123
2024-12-15 05:45:30.582062: Current learning rate: 0.00214
2024-12-15 05:51:57.119531: Validation loss did not improve from -0.56477. Patience: 4/50
2024-12-15 05:51:57.120567: train_loss -0.7561
2024-12-15 05:51:57.121785: val_loss -0.5286
2024-12-15 05:51:57.122794: Pseudo dice [0.7248]
2024-12-15 05:51:57.123881: Epoch time: 386.54 s
2024-12-15 05:51:58.554915: 
2024-12-15 05:51:58.556614: Epoch 124
2024-12-15 05:51:58.557606: Current learning rate: 0.00207
2024-12-15 05:58:51.499149: Validation loss did not improve from -0.56477. Patience: 5/50
2024-12-15 05:58:51.500152: train_loss -0.7551
2024-12-15 05:58:51.500927: val_loss -0.5505
2024-12-15 05:58:51.501620: Pseudo dice [0.7487]
2024-12-15 05:58:51.502459: Epoch time: 412.95 s
2024-12-15 05:58:53.315973: 
2024-12-15 05:58:53.317410: Epoch 125
2024-12-15 05:58:53.318250: Current learning rate: 0.00199
2024-12-15 06:05:35.384778: Validation loss did not improve from -0.56477. Patience: 6/50
2024-12-15 06:05:35.385682: train_loss -0.7566
2024-12-15 06:05:35.386723: val_loss -0.5197
2024-12-15 06:05:35.387654: Pseudo dice [0.7343]
2024-12-15 06:05:35.388606: Epoch time: 402.07 s
2024-12-15 06:05:37.260396: 
2024-12-15 06:05:37.261907: Epoch 126
2024-12-15 06:05:37.262909: Current learning rate: 0.00192
2024-12-15 06:12:27.136000: Validation loss did not improve from -0.56477. Patience: 7/50
2024-12-15 06:12:27.137006: train_loss -0.7605
2024-12-15 06:12:27.138081: val_loss -0.5045
2024-12-15 06:12:27.139076: Pseudo dice [0.7202]
2024-12-15 06:12:27.140255: Epoch time: 409.88 s
2024-12-15 06:12:28.534791: 
2024-12-15 06:12:28.536451: Epoch 127
2024-12-15 06:12:28.537457: Current learning rate: 0.00185
2024-12-15 06:18:55.572979: Validation loss did not improve from -0.56477. Patience: 8/50
2024-12-15 06:18:55.574114: train_loss -0.7525
2024-12-15 06:18:55.575642: val_loss -0.5463
2024-12-15 06:18:55.577006: Pseudo dice [0.7482]
2024-12-15 06:18:55.578832: Epoch time: 387.04 s
2024-12-15 06:18:56.984701: 
2024-12-15 06:18:56.986203: Epoch 128
2024-12-15 06:18:56.987195: Current learning rate: 0.00178
2024-12-15 06:25:34.665272: Validation loss did not improve from -0.56477. Patience: 9/50
2024-12-15 06:25:34.666245: train_loss -0.7565
2024-12-15 06:25:34.667434: val_loss -0.52
2024-12-15 06:25:34.668578: Pseudo dice [0.7325]
2024-12-15 06:25:34.669385: Epoch time: 397.68 s
2024-12-15 06:25:36.088565: 
2024-12-15 06:25:36.090604: Epoch 129
2024-12-15 06:25:36.091967: Current learning rate: 0.0017
2024-12-15 06:32:21.744735: Validation loss did not improve from -0.56477. Patience: 10/50
2024-12-15 06:32:21.746068: train_loss -0.7624
2024-12-15 06:32:21.747323: val_loss -0.5007
2024-12-15 06:32:21.748516: Pseudo dice [0.7174]
2024-12-15 06:32:21.749570: Epoch time: 405.66 s
2024-12-15 06:32:23.513892: 
2024-12-15 06:32:23.515046: Epoch 130
2024-12-15 06:32:23.515998: Current learning rate: 0.00163
2024-12-15 06:39:28.998047: Validation loss did not improve from -0.56477. Patience: 11/50
2024-12-15 06:39:28.998999: train_loss -0.7585
2024-12-15 06:39:28.999959: val_loss -0.552
2024-12-15 06:39:29.000801: Pseudo dice [0.7568]
2024-12-15 06:39:29.001513: Epoch time: 425.49 s
2024-12-15 06:39:30.380602: 
2024-12-15 06:39:30.381845: Epoch 131
2024-12-15 06:39:30.382532: Current learning rate: 0.00156
2024-12-15 06:46:25.154211: Validation loss did not improve from -0.56477. Patience: 12/50
2024-12-15 06:46:25.156428: train_loss -0.7609
2024-12-15 06:46:25.157218: val_loss -0.5314
2024-12-15 06:46:25.157854: Pseudo dice [0.7369]
2024-12-15 06:46:25.158578: Epoch time: 414.78 s
2024-12-15 06:46:26.555756: 
2024-12-15 06:46:26.556929: Epoch 132
2024-12-15 06:46:26.557660: Current learning rate: 0.00148
2024-12-15 06:53:30.859815: Validation loss did not improve from -0.56477. Patience: 13/50
2024-12-15 06:53:30.860740: train_loss -0.7615
2024-12-15 06:53:30.861617: val_loss -0.5474
2024-12-15 06:53:30.862506: Pseudo dice [0.739]
2024-12-15 06:53:30.863222: Epoch time: 424.31 s
2024-12-15 06:53:32.239442: 
2024-12-15 06:53:32.240945: Epoch 133
2024-12-15 06:53:32.241941: Current learning rate: 0.00141
2024-12-15 07:00:09.580634: Validation loss did not improve from -0.56477. Patience: 14/50
2024-12-15 07:00:09.581673: train_loss -0.763
2024-12-15 07:00:09.582504: val_loss -0.5308
2024-12-15 07:00:09.583185: Pseudo dice [0.742]
2024-12-15 07:00:09.583899: Epoch time: 397.34 s
2024-12-15 07:00:11.009070: 
2024-12-15 07:00:11.010532: Epoch 134
2024-12-15 07:00:11.011766: Current learning rate: 0.00133
2024-12-15 07:06:54.719617: Validation loss did not improve from -0.56477. Patience: 15/50
2024-12-15 07:06:54.720729: train_loss -0.7646
2024-12-15 07:06:54.721720: val_loss -0.5528
2024-12-15 07:06:54.722469: Pseudo dice [0.7508]
2024-12-15 07:06:54.723292: Epoch time: 403.71 s
2024-12-15 07:06:55.135145: Yayy! New best EMA pseudo Dice: 0.739
2024-12-15 07:06:56.981407: 
2024-12-15 07:06:56.982682: Epoch 135
2024-12-15 07:06:56.983490: Current learning rate: 0.00126
2024-12-15 07:13:27.634358: Validation loss did not improve from -0.56477. Patience: 16/50
2024-12-15 07:13:27.635916: train_loss -0.7621
2024-12-15 07:13:27.636875: val_loss -0.5139
2024-12-15 07:13:27.637689: Pseudo dice [0.7281]
2024-12-15 07:13:27.638542: Epoch time: 390.66 s
2024-12-15 07:13:29.063936: 
2024-12-15 07:13:29.065320: Epoch 136
2024-12-15 07:13:29.066215: Current learning rate: 0.00118
2024-12-15 07:20:30.935985: Validation loss improved from -0.56477 to -0.56815! Patience: 16/50
2024-12-15 07:20:30.936939: train_loss -0.7627
2024-12-15 07:20:30.937934: val_loss -0.5682
2024-12-15 07:20:30.938612: Pseudo dice [0.7546]
2024-12-15 07:20:30.939378: Epoch time: 421.87 s
2024-12-15 07:20:30.940127: Yayy! New best EMA pseudo Dice: 0.7396
2024-12-15 07:20:32.733995: 
2024-12-15 07:20:32.735686: Epoch 137
2024-12-15 07:20:32.737237: Current learning rate: 0.00111
2024-12-15 07:27:30.753947: Validation loss did not improve from -0.56815. Patience: 1/50
2024-12-15 07:27:30.755102: train_loss -0.7657
2024-12-15 07:27:30.756252: val_loss -0.5327
2024-12-15 07:27:30.757647: Pseudo dice [0.7366]
2024-12-15 07:27:30.758826: Epoch time: 418.02 s
2024-12-15 07:27:32.858232: 
2024-12-15 07:27:32.859835: Epoch 138
2024-12-15 07:27:32.860820: Current learning rate: 0.00103
2024-12-15 07:34:15.738670: Validation loss did not improve from -0.56815. Patience: 2/50
2024-12-15 07:34:15.739722: train_loss -0.7664
2024-12-15 07:34:15.740584: val_loss -0.5499
2024-12-15 07:34:15.741265: Pseudo dice [0.7434]
2024-12-15 07:34:15.741915: Epoch time: 402.88 s
2024-12-15 07:34:15.742693: Yayy! New best EMA pseudo Dice: 0.7397
2024-12-15 07:34:17.524128: 
2024-12-15 07:34:17.525616: Epoch 139
2024-12-15 07:34:17.526699: Current learning rate: 0.00095
2024-12-15 07:41:04.316750: Validation loss did not improve from -0.56815. Patience: 3/50
2024-12-15 07:41:04.319643: train_loss -0.767
2024-12-15 07:41:04.321241: val_loss -0.5222
2024-12-15 07:41:04.321951: Pseudo dice [0.7332]
2024-12-15 07:41:04.322713: Epoch time: 406.8 s
2024-12-15 07:41:06.128746: 
2024-12-15 07:41:06.129842: Epoch 140
2024-12-15 07:41:06.130563: Current learning rate: 0.00087
2024-12-15 07:48:03.709001: Validation loss did not improve from -0.56815. Patience: 4/50
2024-12-15 07:48:03.711192: train_loss -0.7691
2024-12-15 07:48:03.713246: val_loss -0.561
2024-12-15 07:48:03.714214: Pseudo dice [0.7529]
2024-12-15 07:48:03.715137: Epoch time: 417.58 s
2024-12-15 07:48:03.715876: Yayy! New best EMA pseudo Dice: 0.7405
2024-12-15 07:48:05.540879: 
2024-12-15 07:48:05.542259: Epoch 141
2024-12-15 07:48:05.543024: Current learning rate: 0.00079
2024-12-15 07:55:01.101741: Validation loss did not improve from -0.56815. Patience: 5/50
2024-12-15 07:55:01.102899: train_loss -0.7706
2024-12-15 07:55:01.103601: val_loss -0.5578
2024-12-15 07:55:01.104307: Pseudo dice [0.7538]
2024-12-15 07:55:01.105101: Epoch time: 415.56 s
2024-12-15 07:55:01.105927: Yayy! New best EMA pseudo Dice: 0.7418
2024-12-15 07:55:02.904937: 
2024-12-15 07:55:02.906176: Epoch 142
2024-12-15 07:55:02.907058: Current learning rate: 0.00071
2024-12-15 08:01:57.964595: Validation loss did not improve from -0.56815. Patience: 6/50
2024-12-15 08:01:57.965632: train_loss -0.7708
2024-12-15 08:01:57.966703: val_loss -0.5364
2024-12-15 08:01:57.967472: Pseudo dice [0.7375]
2024-12-15 08:01:57.968448: Epoch time: 415.06 s
2024-12-15 08:01:59.418745: 
2024-12-15 08:01:59.420017: Epoch 143
2024-12-15 08:01:59.420865: Current learning rate: 0.00063
2024-12-15 08:08:49.566275: Validation loss did not improve from -0.56815. Patience: 7/50
2024-12-15 08:08:49.567390: train_loss -0.769
2024-12-15 08:08:49.568342: val_loss -0.5517
2024-12-15 08:08:49.569146: Pseudo dice [0.7544]
2024-12-15 08:08:49.569931: Epoch time: 410.15 s
2024-12-15 08:08:49.570687: Yayy! New best EMA pseudo Dice: 0.7427
2024-12-15 08:08:51.358765: 
2024-12-15 08:08:51.359922: Epoch 144
2024-12-15 08:08:51.360773: Current learning rate: 0.00055
2024-12-15 08:16:03.101879: Validation loss did not improve from -0.56815. Patience: 8/50
2024-12-15 08:16:03.102917: train_loss -0.7692
2024-12-15 08:16:03.103660: val_loss -0.5505
2024-12-15 08:16:03.104555: Pseudo dice [0.7453]
2024-12-15 08:16:03.105495: Epoch time: 431.75 s
2024-12-15 08:16:03.462423: Yayy! New best EMA pseudo Dice: 0.7429
2024-12-15 08:16:05.246833: 
2024-12-15 08:16:05.248203: Epoch 145
2024-12-15 08:16:05.248909: Current learning rate: 0.00047
2024-12-15 08:23:07.579686: Validation loss did not improve from -0.56815. Patience: 9/50
2024-12-15 08:23:07.580790: train_loss -0.7686
2024-12-15 08:23:07.581673: val_loss -0.53
2024-12-15 08:23:07.582499: Pseudo dice [0.7373]
2024-12-15 08:23:07.583338: Epoch time: 422.34 s
2024-12-15 08:23:09.000071: 
2024-12-15 08:23:09.001915: Epoch 146
2024-12-15 08:23:09.003434: Current learning rate: 0.00038
2024-12-15 08:30:01.914090: Validation loss did not improve from -0.56815. Patience: 10/50
2024-12-15 08:30:01.915123: train_loss -0.7711
2024-12-15 08:30:01.916082: val_loss -0.5368
2024-12-15 08:30:01.917132: Pseudo dice [0.7388]
2024-12-15 08:30:01.918008: Epoch time: 412.92 s
2024-12-15 08:30:03.328424: 
2024-12-15 08:30:03.329858: Epoch 147
2024-12-15 08:30:03.330528: Current learning rate: 0.0003
2024-12-15 08:37:12.457368: Validation loss did not improve from -0.56815. Patience: 11/50
2024-12-15 08:37:12.458317: train_loss -0.7717
2024-12-15 08:37:12.459364: val_loss -0.5343
2024-12-15 08:37:12.460140: Pseudo dice [0.7344]
2024-12-15 08:37:12.460871: Epoch time: 429.13 s
2024-12-15 08:37:14.327154: 
2024-12-15 08:37:14.328389: Epoch 148
2024-12-15 08:37:14.329200: Current learning rate: 0.00021
2024-12-15 08:43:42.442060: Validation loss did not improve from -0.56815. Patience: 12/50
2024-12-15 08:43:42.442991: train_loss -0.7757
2024-12-15 08:43:42.444077: val_loss -0.5523
2024-12-15 08:43:42.445380: Pseudo dice [0.7512]
2024-12-15 08:43:42.446742: Epoch time: 388.12 s
2024-12-15 08:43:43.850866: 
2024-12-15 08:43:43.852167: Epoch 149
2024-12-15 08:43:43.853128: Current learning rate: 0.00011
2024-12-15 08:50:11.380686: Validation loss did not improve from -0.56815. Patience: 13/50
2024-12-15 08:50:11.381622: train_loss -0.7735
2024-12-15 08:50:11.382544: val_loss -0.544
2024-12-15 08:50:11.383265: Pseudo dice [0.7498]
2024-12-15 08:50:11.383914: Epoch time: 387.53 s
2024-12-15 08:50:11.384533: Yayy! New best EMA pseudo Dice: 0.743
2024-12-15 08:50:13.610853: Training done.
2024-12-15 08:50:13.844091: Using splits from existing split file: /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset307_Sohee_Calcium_OCT_CrossValidation/splits_final_80.json
2024-12-15 08:50:13.848520: The split file contains 5 splits.
2024-12-15 08:50:13.849345: Desired fold for training: 3
2024-12-15 08:50:13.850290: This split has 6 training and 3 validation cases.
2024-12-15 08:50:13.851344: predicting 101-019
2024-12-15 08:50:13.877962: 101-019, shape torch.Size([1, 375, 498, 498]), rank 0
2024-12-15 08:52:48.376822: predicting 101-044
2024-12-15 08:52:48.393585: 101-044, shape torch.Size([1, 404, 498, 498]), rank 0
2024-12-15 08:55:00.716363: predicting 401-004
2024-12-15 08:55:00.727534: 401-004, shape torch.Size([1, 375, 498, 498]), rank 0
2024-12-15 08:57:41.972852: Validation complete
2024-12-15 08:57:41.973387: Mean Validation Dice:  0.7295950505259019
2024-12-14 16:35:59.696955: unpacking done...
2024-12-14 16:35:59.893804: Unable to plot network architecture: nnUNet_compile is enabled!
2024-12-14 16:35:59.959621: 
2024-12-14 16:35:59.960682: Epoch 0
2024-12-14 16:35:59.961918: Current learning rate: 0.01
2024-12-14 16:42:31.483566: Validation loss improved from 1000.00000 to -0.08337! Patience: 0/50
2024-12-14 16:42:31.484584: train_loss -0.0911
2024-12-14 16:42:31.485453: val_loss -0.0834
2024-12-14 16:42:31.486124: Pseudo dice [0.4995]
2024-12-14 16:42:31.486789: Epoch time: 391.53 s
2024-12-14 16:42:31.487506: Yayy! New best EMA pseudo Dice: 0.4995
2024-12-14 16:42:33.062116: 
2024-12-14 16:42:33.064022: Epoch 1
2024-12-14 16:42:33.065161: Current learning rate: 0.00994
2024-12-14 16:48:25.854621: Validation loss improved from -0.08337 to -0.15896! Patience: 0/50
2024-12-14 16:48:25.855558: train_loss -0.2411
2024-12-14 16:48:25.856395: val_loss -0.159
2024-12-14 16:48:25.857140: Pseudo dice [0.5165]
2024-12-14 16:48:25.857956: Epoch time: 352.79 s
2024-12-14 16:48:25.858637: Yayy! New best EMA pseudo Dice: 0.5012
2024-12-14 16:48:27.647985: 
2024-12-14 16:48:27.649429: Epoch 2
2024-12-14 16:48:27.650274: Current learning rate: 0.00988
2024-12-14 16:53:57.880666: Validation loss improved from -0.15896 to -0.22303! Patience: 0/50
2024-12-14 16:53:57.881766: train_loss -0.3045
2024-12-14 16:53:57.882630: val_loss -0.223
2024-12-14 16:53:57.883316: Pseudo dice [0.5186]
2024-12-14 16:53:57.884112: Epoch time: 330.24 s
2024-12-14 16:53:57.884797: Yayy! New best EMA pseudo Dice: 0.5029
2024-12-14 16:53:59.727370: 
2024-12-14 16:53:59.728521: Epoch 3
2024-12-14 16:53:59.729149: Current learning rate: 0.00982
2024-12-14 16:59:25.523402: Validation loss improved from -0.22303 to -0.28624! Patience: 0/50
2024-12-14 16:59:25.524430: train_loss -0.3332
2024-12-14 16:59:25.525373: val_loss -0.2862
2024-12-14 16:59:25.526182: Pseudo dice [0.5942]
2024-12-14 16:59:25.526961: Epoch time: 325.8 s
2024-12-14 16:59:25.527732: Yayy! New best EMA pseudo Dice: 0.5121
2024-12-14 16:59:27.322689: 
2024-12-14 16:59:27.324114: Epoch 4
2024-12-14 16:59:27.324898: Current learning rate: 0.00976
2024-12-14 17:05:10.234964: Validation loss did not improve from -0.28624. Patience: 1/50
2024-12-14 17:05:10.235849: train_loss -0.3792
2024-12-14 17:05:10.236738: val_loss -0.2831
2024-12-14 17:05:10.237476: Pseudo dice [0.5924]
2024-12-14 17:05:10.238224: Epoch time: 342.91 s
2024-12-14 17:05:10.624876: Yayy! New best EMA pseudo Dice: 0.5201
2024-12-14 17:05:12.490593: 
2024-12-14 17:05:12.491862: Epoch 5
2024-12-14 17:05:12.492606: Current learning rate: 0.0097
2024-12-14 17:10:32.969912: Validation loss improved from -0.28624 to -0.29258! Patience: 1/50
2024-12-14 17:10:32.970992: train_loss -0.416
2024-12-14 17:10:32.971954: val_loss -0.2926
2024-12-14 17:10:32.972802: Pseudo dice [0.6087]
2024-12-14 17:10:32.973637: Epoch time: 320.48 s
2024-12-14 17:10:32.974573: Yayy! New best EMA pseudo Dice: 0.529
2024-12-14 17:10:34.764881: 
2024-12-14 17:10:34.766397: Epoch 6
2024-12-14 17:10:34.767437: Current learning rate: 0.00964
2024-12-14 17:15:37.467078: Validation loss improved from -0.29258 to -0.37035! Patience: 0/50
2024-12-14 17:15:37.468110: train_loss -0.4231
2024-12-14 17:15:37.468978: val_loss -0.3703
2024-12-14 17:15:37.469654: Pseudo dice [0.6368]
2024-12-14 17:15:37.470340: Epoch time: 302.7 s
2024-12-14 17:15:37.471010: Yayy! New best EMA pseudo Dice: 0.5397
2024-12-14 17:15:39.293828: 
2024-12-14 17:15:39.295181: Epoch 7
2024-12-14 17:15:39.295905: Current learning rate: 0.00958
2024-12-14 17:21:34.120424: Validation loss did not improve from -0.37035. Patience: 1/50
2024-12-14 17:21:34.121932: train_loss -0.4531
2024-12-14 17:21:34.122990: val_loss -0.3461
2024-12-14 17:21:34.124064: Pseudo dice [0.6364]
2024-12-14 17:21:34.126345: Epoch time: 354.83 s
2024-12-14 17:21:34.127641: Yayy! New best EMA pseudo Dice: 0.5494
2024-12-14 17:21:36.296398: 
2024-12-14 17:21:36.297833: Epoch 8
2024-12-14 17:21:36.298656: Current learning rate: 0.00952
2024-12-14 17:26:41.111223: Validation loss did not improve from -0.37035. Patience: 2/50
2024-12-14 17:26:41.112233: train_loss -0.4547
2024-12-14 17:26:41.113322: val_loss -0.326
2024-12-14 17:26:41.114418: Pseudo dice [0.6278]
2024-12-14 17:26:41.115448: Epoch time: 304.82 s
2024-12-14 17:26:41.116406: Yayy! New best EMA pseudo Dice: 0.5572
2024-12-14 17:26:42.962497: 
2024-12-14 17:26:42.963738: Epoch 9
2024-12-14 17:26:42.964819: Current learning rate: 0.00946
2024-12-14 17:30:43.365705: Validation loss did not improve from -0.37035. Patience: 3/50
2024-12-14 17:30:43.366882: train_loss -0.4674
2024-12-14 17:30:43.367874: val_loss -0.2965
2024-12-14 17:30:43.368715: Pseudo dice [0.6092]
2024-12-14 17:30:43.369748: Epoch time: 240.41 s
2024-12-14 17:30:43.784473: Yayy! New best EMA pseudo Dice: 0.5624
2024-12-14 17:30:45.545435: 
2024-12-14 17:30:45.546569: Epoch 10
2024-12-14 17:30:45.547434: Current learning rate: 0.0094
2024-12-14 17:34:16.125648: Validation loss improved from -0.37035 to -0.37547! Patience: 3/50
2024-12-14 17:34:16.126746: train_loss -0.4831
2024-12-14 17:34:16.127764: val_loss -0.3755
2024-12-14 17:34:16.128542: Pseudo dice [0.6458]
2024-12-14 17:34:16.129432: Epoch time: 210.58 s
2024-12-14 17:34:16.130197: Yayy! New best EMA pseudo Dice: 0.5708
2024-12-14 17:34:17.985718: 
2024-12-14 17:34:17.986990: Epoch 11
2024-12-14 17:34:17.987724: Current learning rate: 0.00934
2024-12-14 17:37:59.646207: Validation loss improved from -0.37547 to -0.39707! Patience: 0/50
2024-12-14 17:37:59.647416: train_loss -0.4916
2024-12-14 17:37:59.648356: val_loss -0.3971
2024-12-14 17:37:59.649086: Pseudo dice [0.6588]
2024-12-14 17:37:59.649820: Epoch time: 221.66 s
2024-12-14 17:37:59.650568: Yayy! New best EMA pseudo Dice: 0.5796
2024-12-14 17:38:01.529761: 
2024-12-14 17:38:01.531253: Epoch 12
2024-12-14 17:38:01.532207: Current learning rate: 0.00928
2024-12-14 17:44:14.044031: Validation loss did not improve from -0.39707. Patience: 1/50
2024-12-14 17:44:14.047047: train_loss -0.4967
2024-12-14 17:44:14.048932: val_loss -0.3589
2024-12-14 17:44:14.049986: Pseudo dice [0.635]
2024-12-14 17:44:14.050948: Epoch time: 372.52 s
2024-12-14 17:44:14.051703: Yayy! New best EMA pseudo Dice: 0.5851
2024-12-14 17:44:15.915270: 
2024-12-14 17:44:15.916709: Epoch 13
2024-12-14 17:44:15.917868: Current learning rate: 0.00922
2024-12-14 17:50:37.458251: Validation loss did not improve from -0.39707. Patience: 2/50
2024-12-14 17:50:37.459254: train_loss -0.5141
2024-12-14 17:50:37.460011: val_loss -0.3676
2024-12-14 17:50:37.460590: Pseudo dice [0.6523]
2024-12-14 17:50:37.461294: Epoch time: 381.55 s
2024-12-14 17:50:37.461977: Yayy! New best EMA pseudo Dice: 0.5918
2024-12-14 17:50:39.323208: 
2024-12-14 17:50:39.324754: Epoch 14
2024-12-14 17:50:39.325787: Current learning rate: 0.00916
2024-12-14 17:57:08.839786: Validation loss did not improve from -0.39707. Patience: 3/50
2024-12-14 17:57:08.840994: train_loss -0.5312
2024-12-14 17:57:08.842027: val_loss -0.3899
2024-12-14 17:57:08.843166: Pseudo dice [0.6696]
2024-12-14 17:57:08.844369: Epoch time: 389.52 s
2024-12-14 17:57:09.243495: Yayy! New best EMA pseudo Dice: 0.5996
2024-12-14 17:57:11.069983: 
2024-12-14 17:57:11.071545: Epoch 15
2024-12-14 17:57:11.072563: Current learning rate: 0.0091
2024-12-14 18:03:15.556946: Validation loss did not improve from -0.39707. Patience: 4/50
2024-12-14 18:03:15.557963: train_loss -0.5323
2024-12-14 18:03:15.558747: val_loss -0.3772
2024-12-14 18:03:15.559413: Pseudo dice [0.6471]
2024-12-14 18:03:15.560184: Epoch time: 364.49 s
2024-12-14 18:03:15.560841: Yayy! New best EMA pseudo Dice: 0.6044
2024-12-14 18:03:17.352452: 
2024-12-14 18:03:17.353689: Epoch 16
2024-12-14 18:03:17.354433: Current learning rate: 0.00903
2024-12-14 18:09:56.884462: Validation loss improved from -0.39707 to -0.45412! Patience: 4/50
2024-12-14 18:09:56.886207: train_loss -0.5262
2024-12-14 18:09:56.887749: val_loss -0.4541
2024-12-14 18:09:56.889307: Pseudo dice [0.68]
2024-12-14 18:09:56.890757: Epoch time: 399.53 s
2024-12-14 18:09:56.891769: Yayy! New best EMA pseudo Dice: 0.6119
2024-12-14 18:09:58.780307: 
2024-12-14 18:09:58.782067: Epoch 17
2024-12-14 18:09:58.783324: Current learning rate: 0.00897
2024-12-14 18:15:43.016014: Validation loss did not improve from -0.45412. Patience: 1/50
2024-12-14 18:15:43.017056: train_loss -0.5395
2024-12-14 18:15:43.018015: val_loss -0.4271
2024-12-14 18:15:43.018810: Pseudo dice [0.6772]
2024-12-14 18:15:43.019588: Epoch time: 344.24 s
2024-12-14 18:15:43.020391: Yayy! New best EMA pseudo Dice: 0.6185
2024-12-14 18:15:45.405799: 
2024-12-14 18:15:45.407425: Epoch 18
2024-12-14 18:15:45.408386: Current learning rate: 0.00891
2024-12-14 18:22:41.886959: Validation loss did not improve from -0.45412. Patience: 2/50
2024-12-14 18:22:41.888176: train_loss -0.5462
2024-12-14 18:22:41.889312: val_loss -0.3639
2024-12-14 18:22:41.890182: Pseudo dice [0.6413]
2024-12-14 18:22:41.891129: Epoch time: 416.48 s
2024-12-14 18:22:41.892072: Yayy! New best EMA pseudo Dice: 0.6207
2024-12-14 18:22:43.717907: 
2024-12-14 18:22:43.719265: Epoch 19
2024-12-14 18:22:43.720239: Current learning rate: 0.00885
2024-12-14 18:29:08.972006: Validation loss did not improve from -0.45412. Patience: 3/50
2024-12-14 18:29:08.972991: train_loss -0.5406
2024-12-14 18:29:08.973991: val_loss -0.4367
2024-12-14 18:29:08.974755: Pseudo dice [0.6759]
2024-12-14 18:29:08.975627: Epoch time: 385.26 s
2024-12-14 18:29:09.410346: Yayy! New best EMA pseudo Dice: 0.6263
2024-12-14 18:29:11.261329: 
2024-12-14 18:29:11.262801: Epoch 20
2024-12-14 18:29:11.263580: Current learning rate: 0.00879
2024-12-14 18:35:46.036397: Validation loss improved from -0.45412 to -0.46685! Patience: 3/50
2024-12-14 18:35:46.037321: train_loss -0.5514
2024-12-14 18:35:46.038107: val_loss -0.4668
2024-12-14 18:35:46.038818: Pseudo dice [0.6956]
2024-12-14 18:35:46.039615: Epoch time: 394.78 s
2024-12-14 18:35:46.040353: Yayy! New best EMA pseudo Dice: 0.6332
2024-12-14 18:35:47.954980: 
2024-12-14 18:35:47.956002: Epoch 21
2024-12-14 18:35:47.956799: Current learning rate: 0.00873
2024-12-14 18:41:50.267845: Validation loss did not improve from -0.46685. Patience: 1/50
2024-12-14 18:41:50.268866: train_loss -0.563
2024-12-14 18:41:50.269654: val_loss -0.4462
2024-12-14 18:41:50.270280: Pseudo dice [0.6821]
2024-12-14 18:41:50.270983: Epoch time: 362.31 s
2024-12-14 18:41:50.271734: Yayy! New best EMA pseudo Dice: 0.6381
2024-12-14 18:41:52.081193: 
2024-12-14 18:41:52.082335: Epoch 22
2024-12-14 18:41:52.083252: Current learning rate: 0.00867
2024-12-14 18:48:05.582678: Validation loss did not improve from -0.46685. Patience: 2/50
2024-12-14 18:48:05.584767: train_loss -0.5653
2024-12-14 18:48:05.586364: val_loss -0.4485
2024-12-14 18:48:05.587233: Pseudo dice [0.6821]
2024-12-14 18:48:05.588026: Epoch time: 373.5 s
2024-12-14 18:48:05.588674: Yayy! New best EMA pseudo Dice: 0.6425
2024-12-14 18:48:07.355876: 
2024-12-14 18:48:07.357045: Epoch 23
2024-12-14 18:48:07.357821: Current learning rate: 0.00861
2024-12-14 18:53:58.771438: Validation loss did not improve from -0.46685. Patience: 3/50
2024-12-14 18:53:58.772417: train_loss -0.5773
2024-12-14 18:53:58.773291: val_loss -0.4337
2024-12-14 18:53:58.774064: Pseudo dice [0.6815]
2024-12-14 18:53:58.774715: Epoch time: 351.42 s
2024-12-14 18:53:58.775481: Yayy! New best EMA pseudo Dice: 0.6464
2024-12-14 18:54:00.567553: 
2024-12-14 18:54:00.569009: Epoch 24
2024-12-14 18:54:00.569838: Current learning rate: 0.00855
2024-12-14 19:00:45.414006: Validation loss improved from -0.46685 to -0.49261! Patience: 3/50
2024-12-14 19:00:45.415061: train_loss -0.5836
2024-12-14 19:00:45.415869: val_loss -0.4926
2024-12-14 19:00:45.416557: Pseudo dice [0.7089]
2024-12-14 19:00:45.417186: Epoch time: 404.85 s
2024-12-14 19:00:45.814290: Yayy! New best EMA pseudo Dice: 0.6527
2024-12-14 19:00:47.638058: 
2024-12-14 19:00:47.639605: Epoch 25
2024-12-14 19:00:47.640427: Current learning rate: 0.00849
2024-12-14 19:08:00.964598: Validation loss did not improve from -0.49261. Patience: 1/50
2024-12-14 19:08:00.965948: train_loss -0.5834
2024-12-14 19:08:00.967139: val_loss -0.3811
2024-12-14 19:08:00.968540: Pseudo dice [0.6611]
2024-12-14 19:08:00.969932: Epoch time: 433.33 s
2024-12-14 19:08:00.971295: Yayy! New best EMA pseudo Dice: 0.6535
2024-12-14 19:08:02.758203: 
2024-12-14 19:08:02.759700: Epoch 26
2024-12-14 19:08:02.760768: Current learning rate: 0.00843
2024-12-14 19:15:17.321265: Validation loss did not improve from -0.49261. Patience: 2/50
2024-12-14 19:15:17.322347: train_loss -0.5852
2024-12-14 19:15:17.323514: val_loss -0.4297
2024-12-14 19:15:17.324656: Pseudo dice [0.6859]
2024-12-14 19:15:17.326046: Epoch time: 434.57 s
2024-12-14 19:15:17.327368: Yayy! New best EMA pseudo Dice: 0.6567
2024-12-14 19:15:19.147165: 
2024-12-14 19:15:19.148343: Epoch 27
2024-12-14 19:15:19.149070: Current learning rate: 0.00836
2024-12-14 19:22:39.330580: Validation loss did not improve from -0.49261. Patience: 3/50
2024-12-14 19:22:39.331639: train_loss -0.5997
2024-12-14 19:22:39.332925: val_loss -0.4352
2024-12-14 19:22:39.333906: Pseudo dice [0.6773]
2024-12-14 19:22:39.335006: Epoch time: 440.19 s
2024-12-14 19:22:39.336434: Yayy! New best EMA pseudo Dice: 0.6588
2024-12-14 19:22:41.121261: 
2024-12-14 19:22:41.122973: Epoch 28
2024-12-14 19:22:41.123922: Current learning rate: 0.0083
2024-12-14 19:29:58.319697: Validation loss did not improve from -0.49261. Patience: 4/50
2024-12-14 19:29:58.320791: train_loss -0.5925
2024-12-14 19:29:58.321989: val_loss -0.4728
2024-12-14 19:29:58.322941: Pseudo dice [0.7041]
2024-12-14 19:29:58.323924: Epoch time: 437.2 s
2024-12-14 19:29:58.324805: Yayy! New best EMA pseudo Dice: 0.6633
2024-12-14 19:30:00.575035: 
2024-12-14 19:30:00.576502: Epoch 29
2024-12-14 19:30:00.577550: Current learning rate: 0.00824
2024-12-14 19:37:09.890060: Validation loss did not improve from -0.49261. Patience: 5/50
2024-12-14 19:37:09.891364: train_loss -0.5993
2024-12-14 19:37:09.892648: val_loss -0.492
2024-12-14 19:37:09.893708: Pseudo dice [0.7134]
2024-12-14 19:37:09.894746: Epoch time: 429.32 s
2024-12-14 19:37:10.299783: Yayy! New best EMA pseudo Dice: 0.6683
2024-12-14 19:37:12.078259: 
2024-12-14 19:37:12.079724: Epoch 30
2024-12-14 19:37:12.080853: Current learning rate: 0.00818
2024-12-14 19:44:42.341598: Validation loss did not improve from -0.49261. Patience: 6/50
2024-12-14 19:44:42.346193: train_loss -0.6086
2024-12-14 19:44:42.347381: val_loss -0.4204
2024-12-14 19:44:42.348127: Pseudo dice [0.6746]
2024-12-14 19:44:42.349063: Epoch time: 450.27 s
2024-12-14 19:44:42.349962: Yayy! New best EMA pseudo Dice: 0.669
2024-12-14 19:44:44.129308: 
2024-12-14 19:44:44.130883: Epoch 31
2024-12-14 19:44:44.132029: Current learning rate: 0.00812
2024-12-14 19:52:35.996878: Validation loss did not improve from -0.49261. Patience: 7/50
2024-12-14 19:52:35.998554: train_loss -0.6049
2024-12-14 19:52:36.000083: val_loss -0.4434
2024-12-14 19:52:36.000785: Pseudo dice [0.6838]
2024-12-14 19:52:36.001637: Epoch time: 471.87 s
2024-12-14 19:52:36.002391: Yayy! New best EMA pseudo Dice: 0.6704
2024-12-14 19:52:37.747131: 
2024-12-14 19:52:37.748659: Epoch 32
2024-12-14 19:52:37.749926: Current learning rate: 0.00806
2024-12-14 20:00:13.561577: Validation loss improved from -0.49261 to -0.50157! Patience: 7/50
2024-12-14 20:00:13.562608: train_loss -0.6082
2024-12-14 20:00:13.563642: val_loss -0.5016
2024-12-14 20:00:13.564474: Pseudo dice [0.7088]
2024-12-14 20:00:13.565293: Epoch time: 455.82 s
2024-12-14 20:00:13.566098: Yayy! New best EMA pseudo Dice: 0.6743
2024-12-14 20:00:15.357123: 
2024-12-14 20:00:15.358466: Epoch 33
2024-12-14 20:00:15.359229: Current learning rate: 0.008
2024-12-14 20:07:37.302423: Validation loss did not improve from -0.50157. Patience: 1/50
2024-12-14 20:07:37.303436: train_loss -0.6098
2024-12-14 20:07:37.304134: val_loss -0.4824
2024-12-14 20:07:37.304817: Pseudo dice [0.7143]
2024-12-14 20:07:37.305549: Epoch time: 441.95 s
2024-12-14 20:07:37.306328: Yayy! New best EMA pseudo Dice: 0.6783
2024-12-14 20:07:39.144702: 
2024-12-14 20:07:39.145948: Epoch 34
2024-12-14 20:07:39.146791: Current learning rate: 0.00793
2024-12-14 20:15:15.178574: Validation loss improved from -0.50157 to -0.50353! Patience: 1/50
2024-12-14 20:15:15.179764: train_loss -0.6095
2024-12-14 20:15:15.180621: val_loss -0.5035
2024-12-14 20:15:15.181511: Pseudo dice [0.7218]
2024-12-14 20:15:15.182374: Epoch time: 456.04 s
2024-12-14 20:15:15.568989: Yayy! New best EMA pseudo Dice: 0.6826
2024-12-14 20:15:17.445285: 
2024-12-14 20:15:17.446640: Epoch 35
2024-12-14 20:15:17.447917: Current learning rate: 0.00787
2024-12-14 20:22:41.176471: Validation loss did not improve from -0.50353. Patience: 1/50
2024-12-14 20:22:41.177523: train_loss -0.6211
2024-12-14 20:22:41.178270: val_loss -0.4808
2024-12-14 20:22:41.179021: Pseudo dice [0.7101]
2024-12-14 20:22:41.179755: Epoch time: 443.73 s
2024-12-14 20:22:41.180434: Yayy! New best EMA pseudo Dice: 0.6854
2024-12-14 20:22:43.036977: 
2024-12-14 20:22:43.038223: Epoch 36
2024-12-14 20:22:43.038976: Current learning rate: 0.00781
2024-12-14 20:30:04.592857: Validation loss did not improve from -0.50353. Patience: 2/50
2024-12-14 20:30:04.594266: train_loss -0.6239
2024-12-14 20:30:04.595367: val_loss -0.4954
2024-12-14 20:30:04.596390: Pseudo dice [0.7163]
2024-12-14 20:30:04.597204: Epoch time: 441.56 s
2024-12-14 20:30:04.598194: Yayy! New best EMA pseudo Dice: 0.6885
2024-12-14 20:30:06.433513: 
2024-12-14 20:30:06.434987: Epoch 37
2024-12-14 20:30:06.435794: Current learning rate: 0.00775
2024-12-14 20:37:20.668499: Validation loss did not improve from -0.50353. Patience: 3/50
2024-12-14 20:37:20.669559: train_loss -0.6309
2024-12-14 20:37:20.670564: val_loss -0.4106
2024-12-14 20:37:20.671349: Pseudo dice [0.6743]
2024-12-14 20:37:20.672006: Epoch time: 434.24 s
2024-12-14 20:37:22.070375: 
2024-12-14 20:37:22.071782: Epoch 38
2024-12-14 20:37:22.072644: Current learning rate: 0.00769
2024-12-14 20:44:33.864798: Validation loss did not improve from -0.50353. Patience: 4/50
2024-12-14 20:44:33.865762: train_loss -0.6376
2024-12-14 20:44:33.866469: val_loss -0.4613
2024-12-14 20:44:33.867217: Pseudo dice [0.6963]
2024-12-14 20:44:33.868006: Epoch time: 431.8 s
2024-12-14 20:44:35.701699: 
2024-12-14 20:44:35.702997: Epoch 39
2024-12-14 20:44:35.703826: Current learning rate: 0.00763
2024-12-14 20:52:07.703068: Validation loss did not improve from -0.50353. Patience: 5/50
2024-12-14 20:52:07.706118: train_loss -0.6328
2024-12-14 20:52:07.707193: val_loss -0.4334
2024-12-14 20:52:07.707833: Pseudo dice [0.6853]
2024-12-14 20:52:07.708669: Epoch time: 452.01 s
2024-12-14 20:52:09.538104: 
2024-12-14 20:52:09.539805: Epoch 40
2024-12-14 20:52:09.540705: Current learning rate: 0.00756
2024-12-14 20:59:13.939668: Validation loss improved from -0.50353 to -0.52489! Patience: 5/50
2024-12-14 20:59:13.941892: train_loss -0.6342
2024-12-14 20:59:13.943520: val_loss -0.5249
2024-12-14 20:59:13.944367: Pseudo dice [0.7338]
2024-12-14 20:59:13.945232: Epoch time: 424.4 s
2024-12-14 20:59:13.946002: Yayy! New best EMA pseudo Dice: 0.6923
2024-12-14 20:59:15.756240: 
2024-12-14 20:59:15.757796: Epoch 41
2024-12-14 20:59:15.758908: Current learning rate: 0.0075
2024-12-14 21:06:41.057423: Validation loss did not improve from -0.52489. Patience: 1/50
2024-12-14 21:06:41.058451: train_loss -0.6306
2024-12-14 21:06:41.059343: val_loss -0.4809
2024-12-14 21:06:41.060071: Pseudo dice [0.6989]
2024-12-14 21:06:41.060769: Epoch time: 445.3 s
2024-12-14 21:06:41.061425: Yayy! New best EMA pseudo Dice: 0.693
2024-12-14 21:06:42.820409: 
2024-12-14 21:06:42.821827: Epoch 42
2024-12-14 21:06:42.822612: Current learning rate: 0.00744
2024-12-14 21:14:15.331284: Validation loss did not improve from -0.52489. Patience: 2/50
2024-12-14 21:14:15.332283: train_loss -0.6434
2024-12-14 21:14:15.333017: val_loss -0.4869
2024-12-14 21:14:15.333754: Pseudo dice [0.7159]
2024-12-14 21:14:15.334559: Epoch time: 452.51 s
2024-12-14 21:14:15.335384: Yayy! New best EMA pseudo Dice: 0.6953
2024-12-14 21:14:17.102965: 
2024-12-14 21:14:17.104406: Epoch 43
2024-12-14 21:14:17.105243: Current learning rate: 0.00738
2024-12-14 21:21:58.532500: Validation loss did not improve from -0.52489. Patience: 3/50
2024-12-14 21:21:58.533500: train_loss -0.6419
2024-12-14 21:21:58.534292: val_loss -0.52
2024-12-14 21:21:58.535046: Pseudo dice [0.73]
2024-12-14 21:21:58.535819: Epoch time: 461.43 s
2024-12-14 21:21:58.536633: Yayy! New best EMA pseudo Dice: 0.6987
2024-12-14 21:22:00.394533: 
2024-12-14 21:22:00.396089: Epoch 44
2024-12-14 21:22:00.397029: Current learning rate: 0.00732
2024-12-14 21:29:36.268494: Validation loss did not improve from -0.52489. Patience: 4/50
2024-12-14 21:29:36.269373: train_loss -0.6497
2024-12-14 21:29:36.270416: val_loss -0.4919
2024-12-14 21:29:36.271276: Pseudo dice [0.7149]
2024-12-14 21:29:36.272409: Epoch time: 455.88 s
2024-12-14 21:29:36.682142: Yayy! New best EMA pseudo Dice: 0.7004
2024-12-14 21:29:38.439865: 
2024-12-14 21:29:38.441275: Epoch 45
2024-12-14 21:29:38.442129: Current learning rate: 0.00725
2024-12-14 21:37:22.465296: Validation loss did not improve from -0.52489. Patience: 5/50
2024-12-14 21:37:22.466424: train_loss -0.6418
2024-12-14 21:37:22.467395: val_loss -0.5075
2024-12-14 21:37:22.468062: Pseudo dice [0.7294]
2024-12-14 21:37:22.468716: Epoch time: 464.03 s
2024-12-14 21:37:22.469287: Yayy! New best EMA pseudo Dice: 0.7033
2024-12-14 21:37:24.210851: 
2024-12-14 21:37:24.212202: Epoch 46
2024-12-14 21:37:24.213129: Current learning rate: 0.00719
2024-12-14 21:45:17.165337: Validation loss did not improve from -0.52489. Patience: 6/50
2024-12-14 21:45:17.166413: train_loss -0.6556
2024-12-14 21:45:17.167432: val_loss -0.5077
2024-12-14 21:45:17.168276: Pseudo dice [0.7269]
2024-12-14 21:45:17.169121: Epoch time: 472.96 s
2024-12-14 21:45:17.169924: Yayy! New best EMA pseudo Dice: 0.7056
2024-12-14 21:45:18.998809: 
2024-12-14 21:45:19.000116: Epoch 47
2024-12-14 21:45:19.000931: Current learning rate: 0.00713
2024-12-14 21:53:03.494854: Validation loss did not improve from -0.52489. Patience: 7/50
2024-12-14 21:53:03.495918: train_loss -0.6445
2024-12-14 21:53:03.496803: val_loss -0.4521
2024-12-14 21:53:03.497616: Pseudo dice [0.6927]
2024-12-14 21:53:03.498442: Epoch time: 464.5 s
2024-12-14 21:53:04.893664: 
2024-12-14 21:53:04.895135: Epoch 48
2024-12-14 21:53:04.895900: Current learning rate: 0.00707
2024-12-14 22:00:21.683162: Validation loss did not improve from -0.52489. Patience: 8/50
2024-12-14 22:00:21.684310: train_loss -0.662
2024-12-14 22:00:21.685139: val_loss -0.46
2024-12-14 22:00:21.685854: Pseudo dice [0.7012]
2024-12-14 22:00:21.686646: Epoch time: 436.79 s
2024-12-14 22:00:23.086634: 
2024-12-14 22:00:23.088084: Epoch 49
2024-12-14 22:00:23.088903: Current learning rate: 0.007
2024-12-14 22:07:51.119569: Validation loss did not improve from -0.52489. Patience: 9/50
2024-12-14 22:07:51.120751: train_loss -0.6583
2024-12-14 22:07:51.121706: val_loss -0.5125
2024-12-14 22:07:51.123034: Pseudo dice [0.7312]
2024-12-14 22:07:51.123989: Epoch time: 448.04 s
2024-12-14 22:07:51.501590: Yayy! New best EMA pseudo Dice: 0.7067
2024-12-14 22:07:53.178563: 
2024-12-14 22:07:53.179790: Epoch 50
2024-12-14 22:07:53.180538: Current learning rate: 0.00694
2024-12-14 22:15:24.673129: Validation loss did not improve from -0.52489. Patience: 10/50
2024-12-14 22:15:24.674184: train_loss -0.6578
2024-12-14 22:15:24.674927: val_loss -0.488
2024-12-14 22:15:24.675651: Pseudo dice [0.7113]
2024-12-14 22:15:24.676608: Epoch time: 451.5 s
2024-12-14 22:15:24.677436: Yayy! New best EMA pseudo Dice: 0.7072
2024-12-14 22:15:26.932045: 
2024-12-14 22:15:26.933313: Epoch 51
2024-12-14 22:15:26.934054: Current learning rate: 0.00688
2024-12-14 22:23:01.803317: Validation loss did not improve from -0.52489. Patience: 11/50
2024-12-14 22:23:01.804236: train_loss -0.657
2024-12-14 22:23:01.805033: val_loss -0.5025
2024-12-14 22:23:01.805837: Pseudo dice [0.7261]
2024-12-14 22:23:01.806518: Epoch time: 454.87 s
2024-12-14 22:23:01.807218: Yayy! New best EMA pseudo Dice: 0.7091
2024-12-14 22:23:03.615916: 
2024-12-14 22:23:03.617253: Epoch 52
2024-12-14 22:23:03.618065: Current learning rate: 0.00682
2024-12-14 22:30:28.427612: Validation loss did not improve from -0.52489. Patience: 12/50
2024-12-14 22:30:28.428799: train_loss -0.665
2024-12-14 22:30:28.429576: val_loss -0.494
2024-12-14 22:30:28.430255: Pseudo dice [0.7162]
2024-12-14 22:30:28.431048: Epoch time: 444.81 s
2024-12-14 22:30:28.431726: Yayy! New best EMA pseudo Dice: 0.7098
2024-12-14 22:30:30.200816: 
2024-12-14 22:30:30.202369: Epoch 53
2024-12-14 22:30:30.203448: Current learning rate: 0.00675
2024-12-14 22:37:45.461202: Validation loss did not improve from -0.52489. Patience: 13/50
2024-12-14 22:37:45.462267: train_loss -0.6669
2024-12-14 22:37:45.463418: val_loss -0.5236
2024-12-14 22:37:45.464670: Pseudo dice [0.7377]
2024-12-14 22:37:45.465572: Epoch time: 435.26 s
2024-12-14 22:37:45.466598: Yayy! New best EMA pseudo Dice: 0.7126
2024-12-14 22:37:47.271737: 
2024-12-14 22:37:47.273231: Epoch 54
2024-12-14 22:37:47.274072: Current learning rate: 0.00669
2024-12-14 22:45:02.894444: Validation loss did not improve from -0.52489. Patience: 14/50
2024-12-14 22:45:02.895513: train_loss -0.6573
2024-12-14 22:45:02.896787: val_loss -0.467
2024-12-14 22:45:02.897964: Pseudo dice [0.7048]
2024-12-14 22:45:02.899032: Epoch time: 435.63 s
2024-12-14 22:45:04.766032: 
2024-12-14 22:45:04.767617: Epoch 55
2024-12-14 22:45:04.768426: Current learning rate: 0.00663
2024-12-14 22:52:36.859927: Validation loss did not improve from -0.52489. Patience: 15/50
2024-12-14 22:52:36.861010: train_loss -0.6702
2024-12-14 22:52:36.861950: val_loss -0.4672
2024-12-14 22:52:36.862853: Pseudo dice [0.7]
2024-12-14 22:52:36.863710: Epoch time: 452.1 s
2024-12-14 22:52:38.322356: 
2024-12-14 22:52:38.323682: Epoch 56
2024-12-14 22:52:38.324368: Current learning rate: 0.00657
2024-12-14 23:00:13.488780: Validation loss did not improve from -0.52489. Patience: 16/50
2024-12-14 23:00:13.491463: train_loss -0.6669
2024-12-14 23:00:13.492623: val_loss -0.4354
2024-12-14 23:00:13.493463: Pseudo dice [0.6916]
2024-12-14 23:00:13.494668: Epoch time: 455.17 s
2024-12-14 23:00:15.003583: 
2024-12-14 23:00:15.006138: Epoch 57
2024-12-14 23:00:15.007770: Current learning rate: 0.0065
2024-12-14 23:08:12.006022: Validation loss did not improve from -0.52489. Patience: 17/50
2024-12-14 23:08:12.009150: train_loss -0.6771
2024-12-14 23:08:12.011062: val_loss -0.4762
2024-12-14 23:08:12.012028: Pseudo dice [0.7058]
2024-12-14 23:08:12.013231: Epoch time: 477.01 s
2024-12-14 23:08:13.662346: 
2024-12-14 23:08:13.663661: Epoch 58
2024-12-14 23:08:13.664517: Current learning rate: 0.00644
2024-12-14 23:16:11.188785: Validation loss did not improve from -0.52489. Patience: 18/50
2024-12-14 23:16:11.189580: train_loss -0.6802
2024-12-14 23:16:11.190512: val_loss -0.4926
2024-12-14 23:16:11.191293: Pseudo dice [0.7162]
2024-12-14 23:16:11.192075: Epoch time: 477.53 s
2024-12-14 23:16:12.798670: 
2024-12-14 23:16:12.799994: Epoch 59
2024-12-14 23:16:12.800884: Current learning rate: 0.00638
2024-12-14 23:24:07.459484: Validation loss did not improve from -0.52489. Patience: 19/50
2024-12-14 23:24:07.460570: train_loss -0.6708
2024-12-14 23:24:07.461479: val_loss -0.4901
2024-12-14 23:24:07.462249: Pseudo dice [0.7137]
2024-12-14 23:24:07.463260: Epoch time: 474.66 s
2024-12-14 23:24:09.459151: 
2024-12-14 23:24:09.460354: Epoch 60
2024-12-14 23:24:09.461198: Current learning rate: 0.00631
2024-12-14 23:32:00.057952: Validation loss did not improve from -0.52489. Patience: 20/50
2024-12-14 23:32:00.058694: train_loss -0.6792
2024-12-14 23:32:00.059571: val_loss -0.4817
2024-12-14 23:32:00.060274: Pseudo dice [0.722]
2024-12-14 23:32:00.060994: Epoch time: 470.6 s
2024-12-14 23:32:01.592021: 
2024-12-14 23:32:01.593092: Epoch 61
2024-12-14 23:32:01.594033: Current learning rate: 0.00625
2024-12-14 23:39:56.758262: Validation loss did not improve from -0.52489. Patience: 21/50
2024-12-14 23:39:56.758964: train_loss -0.6753
2024-12-14 23:39:56.759723: val_loss -0.4827
2024-12-14 23:39:56.760376: Pseudo dice [0.7096]
2024-12-14 23:39:56.761056: Epoch time: 475.17 s
2024-12-14 23:39:59.263365: 
2024-12-14 23:39:59.264358: Epoch 62
2024-12-14 23:39:59.265381: Current learning rate: 0.00619
2024-12-14 23:48:04.643355: Validation loss did not improve from -0.52489. Patience: 22/50
2024-12-14 23:48:04.644236: train_loss -0.6827
2024-12-14 23:48:04.645084: val_loss -0.5043
2024-12-14 23:48:04.645816: Pseudo dice [0.7197]
2024-12-14 23:48:04.646677: Epoch time: 485.38 s
2024-12-14 23:48:06.167320: 
2024-12-14 23:48:06.168868: Epoch 63
2024-12-14 23:48:06.169948: Current learning rate: 0.00612
2024-12-14 23:56:04.090641: Validation loss did not improve from -0.52489. Patience: 23/50
2024-12-14 23:56:04.091344: train_loss -0.6814
2024-12-14 23:56:04.092556: val_loss -0.5222
2024-12-14 23:56:04.093680: Pseudo dice [0.7376]
2024-12-14 23:56:04.094616: Epoch time: 477.93 s
2024-12-14 23:56:04.095614: Yayy! New best EMA pseudo Dice: 0.7142
2024-12-14 23:56:05.978821: 
2024-12-14 23:56:05.979984: Epoch 64
2024-12-14 23:56:05.981098: Current learning rate: 0.00606
2024-12-15 00:04:04.861309: Validation loss did not improve from -0.52489. Patience: 24/50
2024-12-15 00:04:04.865081: train_loss -0.6852
2024-12-15 00:04:04.866340: val_loss -0.4549
2024-12-15 00:04:04.867255: Pseudo dice [0.7025]
2024-12-15 00:04:04.868419: Epoch time: 478.89 s
2024-12-15 00:04:06.754854: 
2024-12-15 00:04:06.755745: Epoch 65
2024-12-15 00:04:06.756439: Current learning rate: 0.006
2024-12-15 00:12:01.472009: Validation loss improved from -0.52489 to -0.54039! Patience: 24/50
2024-12-15 00:12:01.478518: train_loss -0.6898
2024-12-15 00:12:01.480101: val_loss -0.5404
2024-12-15 00:12:01.483073: Pseudo dice [0.7325]
2024-12-15 00:12:01.489159: Epoch time: 474.72 s
2024-12-15 00:12:01.489975: Yayy! New best EMA pseudo Dice: 0.715
2024-12-15 00:12:03.341743: 
2024-12-15 00:12:03.342994: Epoch 66
2024-12-15 00:12:03.344615: Current learning rate: 0.00593
2024-12-15 00:19:55.464997: Validation loss did not improve from -0.54039. Patience: 1/50
2024-12-15 00:19:55.466047: train_loss -0.6889
2024-12-15 00:19:55.467081: val_loss -0.4637
2024-12-15 00:19:55.467887: Pseudo dice [0.7029]
2024-12-15 00:19:55.468814: Epoch time: 472.13 s
2024-12-15 00:19:57.011281: 
2024-12-15 00:19:57.013311: Epoch 67
2024-12-15 00:19:57.014733: Current learning rate: 0.00587
2024-12-15 00:27:40.685130: Validation loss did not improve from -0.54039. Patience: 2/50
2024-12-15 00:27:40.685843: train_loss -0.6914
2024-12-15 00:27:40.686579: val_loss -0.5293
2024-12-15 00:27:40.687364: Pseudo dice [0.7373]
2024-12-15 00:27:40.688256: Epoch time: 463.68 s
2024-12-15 00:27:40.688934: Yayy! New best EMA pseudo Dice: 0.7162
2024-12-15 00:27:42.510365: 
2024-12-15 00:27:42.511305: Epoch 68
2024-12-15 00:27:42.512061: Current learning rate: 0.00581
2024-12-15 00:35:11.954797: Validation loss did not improve from -0.54039. Patience: 3/50
2024-12-15 00:35:11.955785: train_loss -0.6997
2024-12-15 00:35:11.956615: val_loss -0.5306
2024-12-15 00:35:11.957387: Pseudo dice [0.7385]
2024-12-15 00:35:11.958176: Epoch time: 449.45 s
2024-12-15 00:35:11.959023: Yayy! New best EMA pseudo Dice: 0.7184
2024-12-15 00:35:13.935481: 
2024-12-15 00:35:13.936572: Epoch 69
2024-12-15 00:35:13.937747: Current learning rate: 0.00574
2024-12-15 00:42:43.674770: Validation loss did not improve from -0.54039. Patience: 4/50
2024-12-15 00:42:43.675776: train_loss -0.6993
2024-12-15 00:42:43.676711: val_loss -0.5382
2024-12-15 00:42:43.677531: Pseudo dice [0.7418]
2024-12-15 00:42:43.678296: Epoch time: 449.74 s
2024-12-15 00:42:44.087476: Yayy! New best EMA pseudo Dice: 0.7207
2024-12-15 00:42:45.948953: 
2024-12-15 00:42:45.950157: Epoch 70
2024-12-15 00:42:45.951349: Current learning rate: 0.00568
2024-12-15 00:50:33.676680: Validation loss did not improve from -0.54039. Patience: 5/50
2024-12-15 00:50:33.677365: train_loss -0.7049
2024-12-15 00:50:33.678140: val_loss -0.5002
2024-12-15 00:50:33.678780: Pseudo dice [0.7235]
2024-12-15 00:50:33.679429: Epoch time: 467.73 s
2024-12-15 00:50:33.680073: Yayy! New best EMA pseudo Dice: 0.721
2024-12-15 00:50:35.466355: 
2024-12-15 00:50:35.467653: Epoch 71
2024-12-15 00:50:35.468749: Current learning rate: 0.00562
2024-12-15 00:58:26.600598: Validation loss did not improve from -0.54039. Patience: 6/50
2024-12-15 00:58:26.601477: train_loss -0.7046
2024-12-15 00:58:26.602224: val_loss -0.5367
2024-12-15 00:58:26.602901: Pseudo dice [0.7419]
2024-12-15 00:58:26.603725: Epoch time: 471.14 s
2024-12-15 00:58:26.604546: Yayy! New best EMA pseudo Dice: 0.7231
2024-12-15 00:58:29.266901: 
2024-12-15 00:58:29.268085: Epoch 72
2024-12-15 00:58:29.269384: Current learning rate: 0.00555
2024-12-15 01:06:17.072775: Validation loss did not improve from -0.54039. Patience: 7/50
2024-12-15 01:06:17.073372: train_loss -0.6998
2024-12-15 01:06:17.074060: val_loss -0.5311
2024-12-15 01:06:17.074683: Pseudo dice [0.7386]
2024-12-15 01:06:17.075395: Epoch time: 467.81 s
2024-12-15 01:06:17.076025: Yayy! New best EMA pseudo Dice: 0.7246
2024-12-15 01:06:18.923575: 
2024-12-15 01:06:18.924479: Epoch 73
2024-12-15 01:06:18.925378: Current learning rate: 0.00549
2024-12-15 01:14:11.356264: Validation loss did not improve from -0.54039. Patience: 8/50
2024-12-15 01:14:11.357026: train_loss -0.7015
2024-12-15 01:14:11.358227: val_loss -0.4873
2024-12-15 01:14:11.358980: Pseudo dice [0.7128]
2024-12-15 01:14:11.359574: Epoch time: 472.43 s
2024-12-15 01:14:12.773892: 
2024-12-15 01:14:12.774809: Epoch 74
2024-12-15 01:14:12.775749: Current learning rate: 0.00542
2024-12-15 01:21:57.374578: Validation loss did not improve from -0.54039. Patience: 9/50
2024-12-15 01:21:57.375993: train_loss -0.7001
2024-12-15 01:21:57.377636: val_loss -0.4805
2024-12-15 01:21:57.378481: Pseudo dice [0.7151]
2024-12-15 01:21:57.379371: Epoch time: 464.6 s
2024-12-15 01:21:59.148510: 
2024-12-15 01:21:59.149816: Epoch 75
2024-12-15 01:21:59.150668: Current learning rate: 0.00536
2024-12-15 01:29:38.697886: Validation loss did not improve from -0.54039. Patience: 10/50
2024-12-15 01:29:38.698510: train_loss -0.7036
2024-12-15 01:29:38.699623: val_loss -0.5277
2024-12-15 01:29:38.700753: Pseudo dice [0.7376]
2024-12-15 01:29:38.701791: Epoch time: 459.55 s
2024-12-15 01:29:40.101464: 
2024-12-15 01:29:40.102395: Epoch 76
2024-12-15 01:29:40.103019: Current learning rate: 0.00529
2024-12-15 01:37:31.543608: Validation loss did not improve from -0.54039. Patience: 11/50
2024-12-15 01:37:31.544322: train_loss -0.7083
2024-12-15 01:37:31.545012: val_loss -0.5274
2024-12-15 01:37:31.545716: Pseudo dice [0.7405]
2024-12-15 01:37:31.546376: Epoch time: 471.44 s
2024-12-15 01:37:31.547164: Yayy! New best EMA pseudo Dice: 0.7258
2024-12-15 01:37:33.401451: 
2024-12-15 01:37:33.402710: Epoch 77
2024-12-15 01:37:33.403921: Current learning rate: 0.00523
2024-12-15 01:45:05.307486: Validation loss did not improve from -0.54039. Patience: 12/50
2024-12-15 01:45:05.308385: train_loss -0.7084
2024-12-15 01:45:05.309858: val_loss -0.4945
2024-12-15 01:45:05.311291: Pseudo dice [0.7208]
2024-12-15 01:45:05.312573: Epoch time: 451.91 s
2024-12-15 01:45:06.773932: 
2024-12-15 01:45:06.775182: Epoch 78
2024-12-15 01:45:06.776342: Current learning rate: 0.00517
2024-12-15 01:52:47.305764: Validation loss did not improve from -0.54039. Patience: 13/50
2024-12-15 01:52:47.306471: train_loss -0.7083
2024-12-15 01:52:47.307071: val_loss -0.5053
2024-12-15 01:52:47.307745: Pseudo dice [0.7242]
2024-12-15 01:52:47.308414: Epoch time: 460.53 s
2024-12-15 01:52:48.816920: 
2024-12-15 01:52:48.817849: Epoch 79
2024-12-15 01:52:48.818558: Current learning rate: 0.0051
2024-12-15 02:00:20.627142: Validation loss did not improve from -0.54039. Patience: 14/50
2024-12-15 02:00:20.628175: train_loss -0.7097
2024-12-15 02:00:20.629101: val_loss -0.5091
2024-12-15 02:00:20.629809: Pseudo dice [0.728]
2024-12-15 02:00:20.630579: Epoch time: 451.81 s
2024-12-15 02:00:22.500965: 
2024-12-15 02:00:22.502477: Epoch 80
2024-12-15 02:00:22.503960: Current learning rate: 0.00504
2024-12-15 02:07:43.865570: Validation loss did not improve from -0.54039. Patience: 15/50
2024-12-15 02:07:43.866280: train_loss -0.7119
2024-12-15 02:07:43.867027: val_loss -0.5062
2024-12-15 02:07:43.867608: Pseudo dice [0.7288]
2024-12-15 02:07:43.868273: Epoch time: 441.37 s
2024-12-15 02:07:43.869006: Yayy! New best EMA pseudo Dice: 0.7258
2024-12-15 02:07:45.680001: 
2024-12-15 02:07:45.680962: Epoch 81
2024-12-15 02:07:45.681765: Current learning rate: 0.00497
2024-12-15 02:15:08.558036: Validation loss did not improve from -0.54039. Patience: 16/50
2024-12-15 02:15:08.560934: train_loss -0.7188
2024-12-15 02:15:08.562457: val_loss -0.513
2024-12-15 02:15:08.563399: Pseudo dice [0.7259]
2024-12-15 02:15:08.564489: Epoch time: 442.88 s
2024-12-15 02:15:08.565894: Yayy! New best EMA pseudo Dice: 0.7258
2024-12-15 02:15:10.448156: 
2024-12-15 02:15:10.449852: Epoch 82
2024-12-15 02:15:10.450993: Current learning rate: 0.00491
2024-12-15 02:22:42.317936: Validation loss did not improve from -0.54039. Patience: 17/50
2024-12-15 02:22:42.319541: train_loss -0.7137
2024-12-15 02:22:42.321348: val_loss -0.5167
2024-12-15 02:22:42.322180: Pseudo dice [0.7335]
2024-12-15 02:22:42.323143: Epoch time: 451.87 s
2024-12-15 02:22:42.323811: Yayy! New best EMA pseudo Dice: 0.7266
2024-12-15 02:22:44.493971: 
2024-12-15 02:22:44.495047: Epoch 83
2024-12-15 02:22:44.496140: Current learning rate: 0.00484
2024-12-15 02:30:06.805572: Validation loss did not improve from -0.54039. Patience: 18/50
2024-12-15 02:30:06.806374: train_loss -0.7124
2024-12-15 02:30:06.807054: val_loss -0.5276
2024-12-15 02:30:06.807710: Pseudo dice [0.7363]
2024-12-15 02:30:06.808506: Epoch time: 442.31 s
2024-12-15 02:30:06.809242: Yayy! New best EMA pseudo Dice: 0.7275
2024-12-15 02:30:08.494507: 
2024-12-15 02:30:08.495215: Epoch 84
2024-12-15 02:30:08.495853: Current learning rate: 0.00478
2024-12-15 02:37:45.618476: Validation loss did not improve from -0.54039. Patience: 19/50
2024-12-15 02:37:45.619332: train_loss -0.7191
2024-12-15 02:37:45.620183: val_loss -0.5256
2024-12-15 02:37:45.620943: Pseudo dice [0.7374]
2024-12-15 02:37:45.621636: Epoch time: 457.13 s
2024-12-15 02:37:45.968958: Yayy! New best EMA pseudo Dice: 0.7285
2024-12-15 02:37:47.683988: 
2024-12-15 02:37:47.684885: Epoch 85
2024-12-15 02:37:47.685632: Current learning rate: 0.00471
2024-12-15 02:45:17.046445: Validation loss did not improve from -0.54039. Patience: 20/50
2024-12-15 02:45:17.047107: train_loss -0.7248
2024-12-15 02:45:17.047838: val_loss -0.5128
2024-12-15 02:45:17.048636: Pseudo dice [0.7318]
2024-12-15 02:45:17.049433: Epoch time: 449.36 s
2024-12-15 02:45:17.050269: Yayy! New best EMA pseudo Dice: 0.7288
2024-12-15 02:45:18.800965: 
2024-12-15 02:45:18.801954: Epoch 86
2024-12-15 02:45:18.802625: Current learning rate: 0.00465
2024-12-15 02:52:56.182778: Validation loss did not improve from -0.54039. Patience: 21/50
2024-12-15 02:52:56.183655: train_loss -0.7195
2024-12-15 02:52:56.184978: val_loss -0.5024
2024-12-15 02:52:56.186073: Pseudo dice [0.7264]
2024-12-15 02:52:56.187321: Epoch time: 457.38 s
2024-12-15 02:52:57.533535: 
2024-12-15 02:52:57.534683: Epoch 87
2024-12-15 02:52:57.535568: Current learning rate: 0.00458
2024-12-15 03:00:48.041488: Validation loss did not improve from -0.54039. Patience: 22/50
2024-12-15 03:00:48.042567: train_loss -0.7289
2024-12-15 03:00:48.043487: val_loss -0.5225
2024-12-15 03:00:48.044263: Pseudo dice [0.7366]
2024-12-15 03:00:48.045031: Epoch time: 470.51 s
2024-12-15 03:00:48.045845: Yayy! New best EMA pseudo Dice: 0.7294
2024-12-15 03:00:49.797473: 
2024-12-15 03:00:49.798787: Epoch 88
2024-12-15 03:00:49.799584: Current learning rate: 0.00452
2024-12-15 03:08:25.682389: Validation loss did not improve from -0.54039. Patience: 23/50
2024-12-15 03:08:25.683005: train_loss -0.7259
2024-12-15 03:08:25.685710: val_loss -0.5354
2024-12-15 03:08:25.686702: Pseudo dice [0.7369]
2024-12-15 03:08:25.687773: Epoch time: 455.89 s
2024-12-15 03:08:25.688884: Yayy! New best EMA pseudo Dice: 0.7302
2024-12-15 03:08:27.482323: 
2024-12-15 03:08:27.483408: Epoch 89
2024-12-15 03:08:27.484177: Current learning rate: 0.00445
2024-12-15 03:15:58.254117: Validation loss did not improve from -0.54039. Patience: 24/50
2024-12-15 03:15:58.254782: train_loss -0.7299
2024-12-15 03:15:58.255945: val_loss -0.5327
2024-12-15 03:15:58.256995: Pseudo dice [0.7435]
2024-12-15 03:15:58.258145: Epoch time: 450.77 s
2024-12-15 03:15:58.664456: Yayy! New best EMA pseudo Dice: 0.7315
2024-12-15 03:16:00.383008: 
2024-12-15 03:16:00.383934: Epoch 90
2024-12-15 03:16:00.384818: Current learning rate: 0.00438
2024-12-15 03:23:33.589838: Validation loss did not improve from -0.54039. Patience: 25/50
2024-12-15 03:23:33.592161: train_loss -0.7357
2024-12-15 03:23:33.593551: val_loss -0.4862
2024-12-15 03:23:33.594781: Pseudo dice [0.7154]
2024-12-15 03:23:33.596310: Epoch time: 453.21 s
2024-12-15 03:23:34.918658: 
2024-12-15 03:23:34.919914: Epoch 91
2024-12-15 03:23:34.920805: Current learning rate: 0.00432
2024-12-15 03:31:52.054480: Validation loss did not improve from -0.54039. Patience: 26/50
2024-12-15 03:31:52.056028: train_loss -0.7264
2024-12-15 03:31:52.057730: val_loss -0.5359
2024-12-15 03:31:52.058875: Pseudo dice [0.7397]
2024-12-15 03:31:52.060352: Epoch time: 497.14 s
2024-12-15 03:31:53.433263: 
2024-12-15 03:31:53.434637: Epoch 92
2024-12-15 03:31:53.436113: Current learning rate: 0.00425
2024-12-15 03:39:29.750879: Validation loss did not improve from -0.54039. Patience: 27/50
2024-12-15 03:39:29.751853: train_loss -0.7271
2024-12-15 03:39:29.752910: val_loss -0.5195
2024-12-15 03:39:29.753821: Pseudo dice [0.7335]
2024-12-15 03:39:29.754951: Epoch time: 456.32 s
2024-12-15 03:39:31.138291: 
2024-12-15 03:39:31.139837: Epoch 93
2024-12-15 03:39:31.140810: Current learning rate: 0.00419
2024-12-15 03:47:05.954320: Validation loss did not improve from -0.54039. Patience: 28/50
2024-12-15 03:47:05.955414: train_loss -0.7358
2024-12-15 03:47:05.956194: val_loss -0.4817
2024-12-15 03:47:05.956988: Pseudo dice [0.7187]
2024-12-15 03:47:05.957643: Epoch time: 454.82 s
2024-12-15 03:47:07.644089: 
2024-12-15 03:47:07.645493: Epoch 94
2024-12-15 03:47:07.646726: Current learning rate: 0.00412
2024-12-15 03:55:01.347291: Validation loss did not improve from -0.54039. Patience: 29/50
2024-12-15 03:55:01.348375: train_loss -0.7335
2024-12-15 03:55:01.349085: val_loss -0.5117
2024-12-15 03:55:01.349804: Pseudo dice [0.7379]
2024-12-15 03:55:01.350510: Epoch time: 473.71 s
2024-12-15 03:55:03.117851: 
2024-12-15 03:55:03.119229: Epoch 95
2024-12-15 03:55:03.120053: Current learning rate: 0.00405
2024-12-15 04:02:41.586420: Validation loss did not improve from -0.54039. Patience: 30/50
2024-12-15 04:02:41.587453: train_loss -0.7336
2024-12-15 04:02:41.588151: val_loss -0.5151
2024-12-15 04:02:41.589021: Pseudo dice [0.7249]
2024-12-15 04:02:41.589795: Epoch time: 458.47 s
2024-12-15 04:02:42.990243: 
2024-12-15 04:02:42.991400: Epoch 96
2024-12-15 04:02:42.992345: Current learning rate: 0.00399
2024-12-15 04:10:20.957838: Validation loss improved from -0.54039 to -0.55168! Patience: 30/50
2024-12-15 04:10:20.959083: train_loss -0.7352
2024-12-15 04:10:20.960135: val_loss -0.5517
2024-12-15 04:10:20.960990: Pseudo dice [0.7504]
2024-12-15 04:10:20.961697: Epoch time: 457.97 s
2024-12-15 04:10:20.962678: Yayy! New best EMA pseudo Dice: 0.7321
2024-12-15 04:10:22.812043: 
2024-12-15 04:10:22.814462: Epoch 97
2024-12-15 04:10:22.815591: Current learning rate: 0.00392
2024-12-15 04:18:02.430181: Validation loss did not improve from -0.55168. Patience: 1/50
2024-12-15 04:18:02.431208: train_loss -0.7401
2024-12-15 04:18:02.432052: val_loss -0.52
2024-12-15 04:18:02.432781: Pseudo dice [0.7305]
2024-12-15 04:18:02.433479: Epoch time: 459.62 s
2024-12-15 04:18:03.817121: 
2024-12-15 04:18:03.819238: Epoch 98
2024-12-15 04:18:03.820002: Current learning rate: 0.00385
2024-12-15 04:25:54.105853: Validation loss did not improve from -0.55168. Patience: 2/50
2024-12-15 04:25:54.108789: train_loss -0.7441
2024-12-15 04:25:54.109904: val_loss -0.508
2024-12-15 04:25:54.110742: Pseudo dice [0.7263]
2024-12-15 04:25:54.111704: Epoch time: 470.29 s
2024-12-15 04:25:55.507276: 
2024-12-15 04:25:55.508839: Epoch 99
2024-12-15 04:25:55.509869: Current learning rate: 0.00379
2024-12-15 04:33:46.142692: Validation loss did not improve from -0.55168. Patience: 3/50
2024-12-15 04:33:46.145591: train_loss -0.7377
2024-12-15 04:33:46.147441: val_loss -0.5494
2024-12-15 04:33:46.148542: Pseudo dice [0.7489]
2024-12-15 04:33:46.149601: Epoch time: 470.64 s
2024-12-15 04:33:46.496364: Yayy! New best EMA pseudo Dice: 0.7331
2024-12-15 04:33:48.261365: 
2024-12-15 04:33:48.262721: Epoch 100
2024-12-15 04:33:48.263493: Current learning rate: 0.00372
2024-12-15 04:41:20.971551: Validation loss improved from -0.55168 to -0.56524! Patience: 3/50
2024-12-15 04:41:20.972896: train_loss -0.7424
2024-12-15 04:41:20.974445: val_loss -0.5652
2024-12-15 04:41:20.975748: Pseudo dice [0.7579]
2024-12-15 04:41:20.976865: Epoch time: 452.71 s
2024-12-15 04:41:20.977921: Yayy! New best EMA pseudo Dice: 0.7356
2024-12-15 04:41:22.754289: 
2024-12-15 04:41:22.755694: Epoch 101
2024-12-15 04:41:22.756793: Current learning rate: 0.00365
2024-12-15 04:49:00.769853: Validation loss did not improve from -0.56524. Patience: 1/50
2024-12-15 04:49:00.770891: train_loss -0.7422
2024-12-15 04:49:00.771735: val_loss -0.5633
2024-12-15 04:49:00.772465: Pseudo dice [0.7567]
2024-12-15 04:49:00.773165: Epoch time: 458.02 s
2024-12-15 04:49:00.773906: Yayy! New best EMA pseudo Dice: 0.7377
2024-12-15 04:49:02.522523: 
2024-12-15 04:49:02.523732: Epoch 102
2024-12-15 04:49:02.524572: Current learning rate: 0.00359
2024-12-15 04:56:35.562953: Validation loss did not improve from -0.56524. Patience: 2/50
2024-12-15 04:56:35.563737: train_loss -0.7431
2024-12-15 04:56:35.564925: val_loss -0.5176
2024-12-15 04:56:35.566029: Pseudo dice [0.7298]
2024-12-15 04:56:35.566876: Epoch time: 453.04 s
2024-12-15 04:56:36.918235: 
2024-12-15 04:56:36.919592: Epoch 103
2024-12-15 04:56:36.920380: Current learning rate: 0.00352
2024-12-15 05:04:20.439897: Validation loss did not improve from -0.56524. Patience: 3/50
2024-12-15 05:04:20.440958: train_loss -0.7457
2024-12-15 05:04:20.441763: val_loss -0.5342
2024-12-15 05:04:20.442471: Pseudo dice [0.739]
2024-12-15 05:04:20.443201: Epoch time: 463.52 s
2024-12-15 05:04:21.810314: 
2024-12-15 05:04:21.811547: Epoch 104
2024-12-15 05:04:21.812240: Current learning rate: 0.00345
2024-12-15 05:11:58.926286: Validation loss did not improve from -0.56524. Patience: 4/50
2024-12-15 05:11:58.927241: train_loss -0.7445
2024-12-15 05:11:58.928115: val_loss -0.4899
2024-12-15 05:11:58.928809: Pseudo dice [0.7224]
2024-12-15 05:11:58.929460: Epoch time: 457.12 s
2024-12-15 05:12:00.725230: 
2024-12-15 05:12:00.727106: Epoch 105
2024-12-15 05:12:00.728116: Current learning rate: 0.00338
2024-12-15 05:19:41.385151: Validation loss did not improve from -0.56524. Patience: 5/50
2024-12-15 05:19:41.386126: train_loss -0.7458
2024-12-15 05:19:41.386962: val_loss -0.546
2024-12-15 05:19:41.387621: Pseudo dice [0.7465]
2024-12-15 05:19:41.388316: Epoch time: 460.66 s
2024-12-15 05:19:43.253924: 
2024-12-15 05:19:43.255213: Epoch 106
2024-12-15 05:19:43.255908: Current learning rate: 0.00332
2024-12-15 05:27:27.736863: Validation loss did not improve from -0.56524. Patience: 6/50
2024-12-15 05:27:27.737896: train_loss -0.7527
2024-12-15 05:27:27.738637: val_loss -0.5436
2024-12-15 05:27:27.739344: Pseudo dice [0.7417]
2024-12-15 05:27:27.740091: Epoch time: 464.49 s
2024-12-15 05:27:29.098387: 
2024-12-15 05:27:29.099782: Epoch 107
2024-12-15 05:27:29.100583: Current learning rate: 0.00325
2024-12-15 05:35:00.250085: Validation loss did not improve from -0.56524. Patience: 7/50
2024-12-15 05:35:00.251079: train_loss -0.7468
2024-12-15 05:35:00.252115: val_loss -0.5343
2024-12-15 05:35:00.253030: Pseudo dice [0.7463]
2024-12-15 05:35:00.253899: Epoch time: 451.15 s
2024-12-15 05:35:00.254697: Yayy! New best EMA pseudo Dice: 0.7382
2024-12-15 05:35:02.035581: 
2024-12-15 05:35:02.037210: Epoch 108
2024-12-15 05:35:02.038284: Current learning rate: 0.00318
2024-12-15 05:42:24.494065: Validation loss did not improve from -0.56524. Patience: 8/50
2024-12-15 05:42:24.495129: train_loss -0.7507
2024-12-15 05:42:24.495883: val_loss -0.5165
2024-12-15 05:42:24.496561: Pseudo dice [0.7321]
2024-12-15 05:42:24.497412: Epoch time: 442.46 s
2024-12-15 05:42:25.916615: 
2024-12-15 05:42:25.917876: Epoch 109
2024-12-15 05:42:25.918553: Current learning rate: 0.00311
2024-12-15 05:49:48.205346: Validation loss did not improve from -0.56524. Patience: 9/50
2024-12-15 05:49:48.206306: train_loss -0.7542
2024-12-15 05:49:48.207062: val_loss -0.5199
2024-12-15 05:49:48.207755: Pseudo dice [0.7311]
2024-12-15 05:49:48.208457: Epoch time: 442.29 s
2024-12-15 05:49:49.948747: 
2024-12-15 05:49:49.950161: Epoch 110
2024-12-15 05:49:49.951126: Current learning rate: 0.00304
2024-12-15 05:57:38.286952: Validation loss did not improve from -0.56524. Patience: 10/50
2024-12-15 05:57:38.287926: train_loss -0.7468
2024-12-15 05:57:38.288789: val_loss -0.5284
2024-12-15 05:57:38.289582: Pseudo dice [0.7364]
2024-12-15 05:57:38.290366: Epoch time: 468.34 s
2024-12-15 05:57:39.651635: 
2024-12-15 05:57:39.652944: Epoch 111
2024-12-15 05:57:39.653974: Current learning rate: 0.00297
2024-12-15 06:05:01.464035: Validation loss did not improve from -0.56524. Patience: 11/50
2024-12-15 06:05:01.464889: train_loss -0.7509
2024-12-15 06:05:01.465815: val_loss -0.5165
2024-12-15 06:05:01.466687: Pseudo dice [0.7335]
2024-12-15 06:05:01.467571: Epoch time: 441.81 s
2024-12-15 06:05:02.833026: 
2024-12-15 06:05:02.834370: Epoch 112
2024-12-15 06:05:02.835096: Current learning rate: 0.00291
2024-12-15 06:12:43.412546: Validation loss did not improve from -0.56524. Patience: 12/50
2024-12-15 06:12:43.413631: train_loss -0.7562
2024-12-15 06:12:43.414463: val_loss -0.5359
2024-12-15 06:12:43.415387: Pseudo dice [0.7402]
2024-12-15 06:12:43.416101: Epoch time: 460.58 s
2024-12-15 06:12:44.812827: 
2024-12-15 06:12:44.814390: Epoch 113
2024-12-15 06:12:44.815228: Current learning rate: 0.00284
2024-12-15 06:20:20.245547: Validation loss did not improve from -0.56524. Patience: 13/50
2024-12-15 06:20:20.246561: train_loss -0.7569
2024-12-15 06:20:20.247554: val_loss -0.544
2024-12-15 06:20:20.248527: Pseudo dice [0.7463]
2024-12-15 06:20:20.249345: Epoch time: 455.44 s
2024-12-15 06:20:21.611910: 
2024-12-15 06:20:21.613349: Epoch 114
2024-12-15 06:20:21.614169: Current learning rate: 0.00277
2024-12-15 06:28:00.379971: Validation loss did not improve from -0.56524. Patience: 14/50
2024-12-15 06:28:00.381042: train_loss -0.755
2024-12-15 06:28:00.382005: val_loss -0.538
2024-12-15 06:28:00.382706: Pseudo dice [0.7422]
2024-12-15 06:28:00.383512: Epoch time: 458.77 s
2024-12-15 06:28:00.794665: Yayy! New best EMA pseudo Dice: 0.7383
2024-12-15 06:28:02.551355: 
2024-12-15 06:28:02.552695: Epoch 115
2024-12-15 06:28:02.553481: Current learning rate: 0.0027
2024-12-15 06:35:45.088012: Validation loss did not improve from -0.56524. Patience: 15/50
2024-12-15 06:35:45.092011: train_loss -0.7558
2024-12-15 06:35:45.093380: val_loss -0.522
2024-12-15 06:35:45.094547: Pseudo dice [0.7398]
2024-12-15 06:35:45.095935: Epoch time: 462.54 s
2024-12-15 06:35:45.097217: Yayy! New best EMA pseudo Dice: 0.7384
2024-12-15 06:35:47.432235: 
2024-12-15 06:35:47.433842: Epoch 116
2024-12-15 06:35:47.434773: Current learning rate: 0.00263
2024-12-15 06:43:21.729954: Validation loss did not improve from -0.56524. Patience: 16/50
2024-12-15 06:43:21.731323: train_loss -0.7555
2024-12-15 06:43:21.733374: val_loss -0.5318
2024-12-15 06:43:21.734269: Pseudo dice [0.7438]
2024-12-15 06:43:21.735228: Epoch time: 454.3 s
2024-12-15 06:43:21.735873: Yayy! New best EMA pseudo Dice: 0.739
2024-12-15 06:43:23.546870: 
2024-12-15 06:43:23.548168: Epoch 117
2024-12-15 06:43:23.548859: Current learning rate: 0.00256
2024-12-15 06:51:20.732071: Validation loss did not improve from -0.56524. Patience: 17/50
2024-12-15 06:51:20.733581: train_loss -0.7581
2024-12-15 06:51:20.735109: val_loss -0.54
2024-12-15 06:51:20.736845: Pseudo dice [0.7457]
2024-12-15 06:51:20.738130: Epoch time: 477.19 s
2024-12-15 06:51:20.739390: Yayy! New best EMA pseudo Dice: 0.7396
2024-12-15 06:51:22.556382: 
2024-12-15 06:51:22.557641: Epoch 118
2024-12-15 06:51:22.558489: Current learning rate: 0.00249
2024-12-15 06:59:01.203556: Validation loss did not improve from -0.56524. Patience: 18/50
2024-12-15 06:59:01.205138: train_loss -0.762
2024-12-15 06:59:01.206452: val_loss -0.5372
2024-12-15 06:59:01.207733: Pseudo dice [0.741]
2024-12-15 06:59:01.208881: Epoch time: 458.65 s
2024-12-15 06:59:01.210519: Yayy! New best EMA pseudo Dice: 0.7398
2024-12-15 06:59:03.008295: 
2024-12-15 06:59:03.009636: Epoch 119
2024-12-15 06:59:03.010417: Current learning rate: 0.00242
2024-12-15 07:06:40.056565: Validation loss did not improve from -0.56524. Patience: 19/50
2024-12-15 07:06:40.057542: train_loss -0.7631
2024-12-15 07:06:40.058371: val_loss -0.5466
2024-12-15 07:06:40.059005: Pseudo dice [0.7474]
2024-12-15 07:06:40.059785: Epoch time: 457.05 s
2024-12-15 07:06:40.449284: Yayy! New best EMA pseudo Dice: 0.7405
2024-12-15 07:06:42.210106: 
2024-12-15 07:06:42.211494: Epoch 120
2024-12-15 07:06:42.212352: Current learning rate: 0.00235
2024-12-15 07:14:16.283173: Validation loss did not improve from -0.56524. Patience: 20/50
2024-12-15 07:14:16.284048: train_loss -0.7588
2024-12-15 07:14:16.285161: val_loss -0.5161
2024-12-15 07:14:16.285841: Pseudo dice [0.7397]
2024-12-15 07:14:16.286770: Epoch time: 454.08 s
2024-12-15 07:14:17.683300: 
2024-12-15 07:14:17.684856: Epoch 121
2024-12-15 07:14:17.685826: Current learning rate: 0.00228
2024-12-15 07:21:54.201506: Validation loss did not improve from -0.56524. Patience: 21/50
2024-12-15 07:21:54.202525: train_loss -0.7624
2024-12-15 07:21:54.203392: val_loss -0.5087
2024-12-15 07:21:54.204061: Pseudo dice [0.7225]
2024-12-15 07:21:54.204817: Epoch time: 456.52 s
2024-12-15 07:21:55.603176: 
2024-12-15 07:21:55.604377: Epoch 122
2024-12-15 07:21:55.605093: Current learning rate: 0.00221
2024-12-15 07:29:34.852656: Validation loss did not improve from -0.56524. Patience: 22/50
2024-12-15 07:29:34.853625: train_loss -0.7644
2024-12-15 07:29:34.854653: val_loss -0.5432
2024-12-15 07:29:34.855575: Pseudo dice [0.7449]
2024-12-15 07:29:34.856546: Epoch time: 459.25 s
2024-12-15 07:29:36.277770: 
2024-12-15 07:29:36.279358: Epoch 123
2024-12-15 07:29:36.280402: Current learning rate: 0.00214
2024-12-15 07:36:55.516171: Validation loss did not improve from -0.56524. Patience: 23/50
2024-12-15 07:36:55.517226: train_loss -0.7641
2024-12-15 07:36:55.518269: val_loss -0.5248
2024-12-15 07:36:55.519210: Pseudo dice [0.7396]
2024-12-15 07:36:55.520042: Epoch time: 439.24 s
2024-12-15 07:36:56.984820: 
2024-12-15 07:36:56.986182: Epoch 124
2024-12-15 07:36:56.986940: Current learning rate: 0.00207
2024-12-15 07:44:43.180146: Validation loss did not improve from -0.56524. Patience: 24/50
2024-12-15 07:44:43.181311: train_loss -0.7652
2024-12-15 07:44:43.183013: val_loss -0.5443
2024-12-15 07:44:43.184488: Pseudo dice [0.7464]
2024-12-15 07:44:43.185907: Epoch time: 466.2 s
2024-12-15 07:44:44.938312: 
2024-12-15 07:44:44.939744: Epoch 125
2024-12-15 07:44:44.940775: Current learning rate: 0.00199
2024-12-15 07:52:31.062381: Validation loss did not improve from -0.56524. Patience: 25/50
2024-12-15 07:52:31.064007: train_loss -0.7678
2024-12-15 07:52:31.064980: val_loss -0.5597
2024-12-15 07:52:31.066080: Pseudo dice [0.7593]
2024-12-15 07:52:31.067025: Epoch time: 466.13 s
2024-12-15 07:52:31.067729: Yayy! New best EMA pseudo Dice: 0.7419
2024-12-15 07:52:32.891060: 
2024-12-15 07:52:32.892388: Epoch 126
2024-12-15 07:52:32.893209: Current learning rate: 0.00192
2024-12-15 07:59:57.027789: Validation loss did not improve from -0.56524. Patience: 26/50
2024-12-15 07:59:57.028458: train_loss -0.7678
2024-12-15 07:59:57.029247: val_loss -0.5475
2024-12-15 07:59:57.029923: Pseudo dice [0.744]
2024-12-15 07:59:57.030561: Epoch time: 444.14 s
2024-12-15 07:59:57.031181: Yayy! New best EMA pseudo Dice: 0.7422
2024-12-15 07:59:59.303614: 
2024-12-15 07:59:59.304821: Epoch 127
2024-12-15 07:59:59.305619: Current learning rate: 0.00185
2024-12-15 08:07:16.791383: Validation loss did not improve from -0.56524. Patience: 27/50
2024-12-15 08:07:16.792465: train_loss -0.7629
2024-12-15 08:07:16.794445: val_loss -0.5258
2024-12-15 08:07:16.795667: Pseudo dice [0.7486]
2024-12-15 08:07:16.796572: Epoch time: 437.49 s
2024-12-15 08:07:16.797499: Yayy! New best EMA pseudo Dice: 0.7428
2024-12-15 08:07:18.657534: 
2024-12-15 08:07:18.658981: Epoch 128
2024-12-15 08:07:18.659764: Current learning rate: 0.00178
2024-12-15 08:14:49.625838: Validation loss did not improve from -0.56524. Patience: 28/50
2024-12-15 08:14:49.626803: train_loss -0.7683
2024-12-15 08:14:49.627630: val_loss -0.5339
2024-12-15 08:14:49.628236: Pseudo dice [0.7486]
2024-12-15 08:14:49.628809: Epoch time: 450.97 s
2024-12-15 08:14:49.629424: Yayy! New best EMA pseudo Dice: 0.7434
2024-12-15 08:14:51.442509: 
2024-12-15 08:14:51.443857: Epoch 129
2024-12-15 08:14:51.444851: Current learning rate: 0.0017
2024-12-15 08:22:41.717716: Validation loss did not improve from -0.56524. Patience: 29/50
2024-12-15 08:22:41.719018: train_loss -0.7693
2024-12-15 08:22:41.720408: val_loss -0.5618
2024-12-15 08:22:41.721993: Pseudo dice [0.758]
2024-12-15 08:22:41.722989: Epoch time: 470.28 s
2024-12-15 08:22:42.101504: Yayy! New best EMA pseudo Dice: 0.7448
2024-12-15 08:22:43.852994: 
2024-12-15 08:22:43.854377: Epoch 130
2024-12-15 08:22:43.855336: Current learning rate: 0.00163
2024-12-15 08:30:24.996076: Validation loss did not improve from -0.56524. Patience: 30/50
2024-12-15 08:30:24.996842: train_loss -0.7726
2024-12-15 08:30:24.998318: val_loss -0.546
2024-12-15 08:30:24.999474: Pseudo dice [0.7531]
2024-12-15 08:30:25.000521: Epoch time: 461.15 s
2024-12-15 08:30:25.001722: Yayy! New best EMA pseudo Dice: 0.7457
2024-12-15 08:30:26.810599: 
2024-12-15 08:30:26.811980: Epoch 131
2024-12-15 08:30:26.812884: Current learning rate: 0.00156
2024-12-15 08:38:27.517644: Validation loss did not improve from -0.56524. Patience: 31/50
2024-12-15 08:38:27.518548: train_loss -0.7725
2024-12-15 08:38:27.519308: val_loss -0.5318
2024-12-15 08:38:27.519997: Pseudo dice [0.7349]
2024-12-15 08:38:27.520788: Epoch time: 480.71 s
2024-12-15 08:38:28.953115: 
2024-12-15 08:38:28.954915: Epoch 132
2024-12-15 08:38:28.956610: Current learning rate: 0.00148
2024-12-15 08:45:39.296566: Validation loss did not improve from -0.56524. Patience: 32/50
2024-12-15 08:45:39.300169: train_loss -0.7729
2024-12-15 08:45:39.301262: val_loss -0.5452
2024-12-15 08:45:39.301992: Pseudo dice [0.7469]
2024-12-15 08:45:39.302935: Epoch time: 430.35 s
2024-12-15 08:45:40.735089: 
2024-12-15 08:45:40.736500: Epoch 133
2024-12-15 08:45:40.737277: Current learning rate: 0.00141
2024-12-15 08:52:38.161462: Validation loss did not improve from -0.56524. Patience: 33/50
2024-12-15 08:52:38.162518: train_loss -0.7713
2024-12-15 08:52:38.163243: val_loss -0.5346
2024-12-15 08:52:38.164008: Pseudo dice [0.7463]
2024-12-15 08:52:38.164687: Epoch time: 417.43 s
2024-12-15 08:52:39.568755: 
2024-12-15 08:52:39.569993: Epoch 134
2024-12-15 08:52:39.570758: Current learning rate: 0.00133
2024-12-15 08:59:12.154824: Validation loss did not improve from -0.56524. Patience: 34/50
2024-12-15 08:59:12.156262: train_loss -0.7705
2024-12-15 08:59:12.157616: val_loss -0.5591
2024-12-15 08:59:12.158416: Pseudo dice [0.7573]
2024-12-15 08:59:12.159266: Epoch time: 392.59 s
2024-12-15 08:59:12.525037: Yayy! New best EMA pseudo Dice: 0.7462
2024-12-15 08:59:14.294458: 
2024-12-15 08:59:14.295734: Epoch 135
2024-12-15 08:59:14.296398: Current learning rate: 0.00126
2024-12-15 09:06:07.621000: Validation loss did not improve from -0.56524. Patience: 35/50
2024-12-15 09:06:07.622118: train_loss -0.7759
2024-12-15 09:06:07.622962: val_loss -0.5338
2024-12-15 09:06:07.623682: Pseudo dice [0.7515]
2024-12-15 09:06:07.624416: Epoch time: 413.33 s
2024-12-15 09:06:07.625157: Yayy! New best EMA pseudo Dice: 0.7467
2024-12-15 09:06:09.450345: 
2024-12-15 09:06:09.451575: Epoch 136
2024-12-15 09:06:09.452691: Current learning rate: 0.00118
2024-12-15 09:13:01.930827: Validation loss did not improve from -0.56524. Patience: 36/50
2024-12-15 09:13:01.932161: train_loss -0.7731
2024-12-15 09:13:01.933288: val_loss -0.5486
2024-12-15 09:13:01.934372: Pseudo dice [0.7458]
2024-12-15 09:13:01.935404: Epoch time: 412.48 s
2024-12-15 09:13:03.323424: 
2024-12-15 09:13:03.324745: Epoch 137
2024-12-15 09:13:03.325878: Current learning rate: 0.00111
2024-12-15 09:19:47.359193: Validation loss did not improve from -0.56524. Patience: 37/50
2024-12-15 09:19:47.360047: train_loss -0.776
2024-12-15 09:19:47.361070: val_loss -0.5383
2024-12-15 09:19:47.361788: Pseudo dice [0.7442]
2024-12-15 09:19:47.362603: Epoch time: 404.04 s
2024-12-15 09:19:49.109949: 
2024-12-15 09:19:49.111345: Epoch 138
2024-12-15 09:19:49.112346: Current learning rate: 0.00103
2024-12-15 09:26:28.256315: Validation loss did not improve from -0.56524. Patience: 38/50
2024-12-15 09:26:28.257743: train_loss -0.7781
2024-12-15 09:26:28.258662: val_loss -0.5457
2024-12-15 09:26:28.259341: Pseudo dice [0.7504]
2024-12-15 09:26:28.260331: Epoch time: 399.15 s
2024-12-15 09:26:28.261219: Yayy! New best EMA pseudo Dice: 0.7468
2024-12-15 09:26:30.021018: 
2024-12-15 09:26:30.022243: Epoch 139
2024-12-15 09:26:30.023027: Current learning rate: 0.00095
2024-12-15 09:33:20.064487: Validation loss did not improve from -0.56524. Patience: 39/50
2024-12-15 09:33:20.065491: train_loss -0.7784
2024-12-15 09:33:20.066225: val_loss -0.5311
2024-12-15 09:33:20.066873: Pseudo dice [0.7481]
2024-12-15 09:33:20.067523: Epoch time: 410.05 s
2024-12-15 09:33:20.453968: Yayy! New best EMA pseudo Dice: 0.7469
2024-12-15 09:33:22.224144: 
2024-12-15 09:33:22.225344: Epoch 140
2024-12-15 09:33:22.226042: Current learning rate: 0.00087
2024-12-15 09:40:03.006311: Validation loss did not improve from -0.56524. Patience: 40/50
2024-12-15 09:40:03.007215: train_loss -0.7791
2024-12-15 09:40:03.007924: val_loss -0.5224
2024-12-15 09:40:03.008741: Pseudo dice [0.7462]
2024-12-15 09:40:03.009629: Epoch time: 400.78 s
2024-12-15 09:40:04.382898: 
2024-12-15 09:40:04.384301: Epoch 141
2024-12-15 09:40:04.385253: Current learning rate: 0.00079
2024-12-15 09:46:51.375102: Validation loss did not improve from -0.56524. Patience: 41/50
2024-12-15 09:46:51.376132: train_loss -0.7764
2024-12-15 09:46:51.376963: val_loss -0.4935
2024-12-15 09:46:51.377936: Pseudo dice [0.7318]
2024-12-15 09:46:51.378865: Epoch time: 406.99 s
2024-12-15 09:46:52.763343: 
2024-12-15 09:46:52.764676: Epoch 142
2024-12-15 09:46:52.765379: Current learning rate: 0.00071
2024-12-15 09:53:34.044603: Validation loss did not improve from -0.56524. Patience: 42/50
2024-12-15 09:53:34.045960: train_loss -0.7803
2024-12-15 09:53:34.046813: val_loss -0.5245
2024-12-15 09:53:34.047957: Pseudo dice [0.7452]
2024-12-15 09:53:34.048801: Epoch time: 401.28 s
2024-12-15 09:53:35.427095: 
2024-12-15 09:53:35.428460: Epoch 143
2024-12-15 09:53:35.429230: Current learning rate: 0.00063
2024-12-15 10:00:34.186515: Validation loss did not improve from -0.56524. Patience: 43/50
2024-12-15 10:00:34.187594: train_loss -0.7776
2024-12-15 10:00:34.188277: val_loss -0.5431
2024-12-15 10:00:34.188957: Pseudo dice [0.7548]
2024-12-15 10:00:34.189769: Epoch time: 418.76 s
2024-12-15 10:00:35.586194: 
2024-12-15 10:00:35.587564: Epoch 144
2024-12-15 10:00:35.588365: Current learning rate: 0.00055
2024-12-15 10:07:26.889394: Validation loss did not improve from -0.56524. Patience: 44/50
2024-12-15 10:07:26.892954: train_loss -0.7774
2024-12-15 10:07:26.894694: val_loss -0.5599
2024-12-15 10:07:26.895538: Pseudo dice [0.754]
2024-12-15 10:07:26.896474: Epoch time: 411.31 s
2024-12-15 10:07:27.287934: Yayy! New best EMA pseudo Dice: 0.747
2024-12-15 10:07:29.037298: 
2024-12-15 10:07:29.038251: Epoch 145
2024-12-15 10:07:29.038993: Current learning rate: 0.00047
2024-12-15 10:14:41.021310: Validation loss did not improve from -0.56524. Patience: 45/50
2024-12-15 10:14:41.022318: train_loss -0.7791
2024-12-15 10:14:41.023162: val_loss -0.5437
2024-12-15 10:14:41.024064: Pseudo dice [0.7459]
2024-12-15 10:14:41.024746: Epoch time: 431.99 s
2024-12-15 10:14:42.415189: 
2024-12-15 10:14:42.416760: Epoch 146
2024-12-15 10:14:42.417465: Current learning rate: 0.00038
2024-12-15 10:21:42.809158: Validation loss did not improve from -0.56524. Patience: 46/50
2024-12-15 10:21:42.810497: train_loss -0.7767
2024-12-15 10:21:42.811878: val_loss -0.5513
2024-12-15 10:21:42.812940: Pseudo dice [0.7515]
2024-12-15 10:21:42.814104: Epoch time: 420.4 s
2024-12-15 10:21:42.815393: Yayy! New best EMA pseudo Dice: 0.7474
2024-12-15 10:21:44.602375: 
2024-12-15 10:21:44.603971: Epoch 147
2024-12-15 10:21:44.604923: Current learning rate: 0.0003
2024-12-15 10:28:35.838356: Validation loss did not improve from -0.56524. Patience: 47/50
2024-12-15 10:28:35.839324: train_loss -0.7788
2024-12-15 10:28:35.839993: val_loss -0.5046
2024-12-15 10:28:35.840680: Pseudo dice [0.7274]
2024-12-15 10:28:35.841338: Epoch time: 411.24 s
2024-12-15 10:28:37.238659: 
2024-12-15 10:28:37.239883: Epoch 148
2024-12-15 10:28:37.240820: Current learning rate: 0.00021
2024-12-15 10:35:21.723239: Validation loss did not improve from -0.56524. Patience: 48/50
2024-12-15 10:35:21.724188: train_loss -0.7795
2024-12-15 10:35:21.724969: val_loss -0.5227
2024-12-15 10:35:21.725668: Pseudo dice [0.7395]
2024-12-15 10:35:21.726357: Epoch time: 404.49 s
2024-12-15 10:35:23.573308: 
2024-12-15 10:35:23.574771: Epoch 149
2024-12-15 10:35:23.575534: Current learning rate: 0.00011
2024-12-15 10:42:13.125421: Validation loss did not improve from -0.56524. Patience: 49/50
2024-12-15 10:42:13.126348: train_loss -0.7818
2024-12-15 10:42:13.127149: val_loss -0.5345
2024-12-15 10:42:13.128170: Pseudo dice [0.7471]
2024-12-15 10:42:13.129104: Epoch time: 409.55 s
2024-12-15 10:42:14.913373: Training done.
2024-12-15 10:42:15.164820: Using splits from existing split file: /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset307_Sohee_Calcium_OCT_CrossValidation/splits_final_80.json
2024-12-15 10:42:15.166703: The split file contains 5 splits.
2024-12-15 10:42:15.167498: Desired fold for training: 2
2024-12-15 10:42:15.168251: This split has 6 training and 3 validation cases.
2024-12-15 10:42:15.169212: predicting 101-045
2024-12-15 10:42:15.193862: 101-045, shape torch.Size([1, 375, 498, 498]), rank 0
2024-12-15 10:44:29.725624: predicting 401-004
2024-12-15 10:44:29.747905: 401-004, shape torch.Size([1, 375, 498, 498]), rank 0
2024-12-15 10:46:48.059009: predicting 704-003
2024-12-15 10:46:48.071302: 704-003, shape torch.Size([1, 375, 498, 498]), rank 0
2024-12-15 10:49:13.333253: Validation complete
2024-12-15 10:49:13.333826: Mean Validation Dice:  0.7297913196260337

############################
INFO: You are using the old nnU-Net default plans. We have updated our recommendations. Please consider using those instead! Read more here: https://github.com/MIC-DKFZ/nnUNet/blob/master/documentation/resenc_presets.md
############################

Using device: cuda:0

#######################################################################
Please cite the following paper when using nnU-Net:
Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.
#######################################################################

2024-12-15 10:49:22.302382: Using torch.compile...
/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
################### Loading pretrained weights from file  /home/gridsan/nchutisilp/projects/OpenAI-CLIP/clip_preivl_poststent_pretrained_nnUNet.pt ###################
Below is the list of overlapping blocks in pretrained model and nnUNet architecture:
encoder.stages.0.0.convs.0.conv.weight shape torch.Size([32, 1, 3, 3, 3])
encoder.stages.0.0.convs.0.conv.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.norm.weight shape torch.Size([32])
encoder.stages.0.0.convs.0.norm.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.0.weight shape torch.Size([32, 1, 3, 3, 3])
encoder.stages.0.0.convs.0.all_modules.0.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.1.weight shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.1.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
encoder.stages.0.0.convs.1.conv.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.norm.weight shape torch.Size([32])
encoder.stages.0.0.convs.1.norm.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
encoder.stages.0.0.convs.1.all_modules.0.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.1.weight shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.1.bias shape torch.Size([32])
encoder.stages.1.0.convs.0.conv.weight shape torch.Size([64, 32, 3, 3, 3])
encoder.stages.1.0.convs.0.conv.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.norm.weight shape torch.Size([64])
encoder.stages.1.0.convs.0.norm.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.0.weight shape torch.Size([64, 32, 3, 3, 3])
encoder.stages.1.0.convs.0.all_modules.0.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.1.weight shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.1.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
encoder.stages.1.0.convs.1.conv.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.norm.weight shape torch.Size([64])
encoder.stages.1.0.convs.1.norm.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
encoder.stages.1.0.convs.1.all_modules.0.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.1.weight shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.1.bias shape torch.Size([64])
encoder.stages.2.0.convs.0.conv.weight shape torch.Size([128, 64, 3, 3, 3])
encoder.stages.2.0.convs.0.conv.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.norm.weight shape torch.Size([128])
encoder.stages.2.0.convs.0.norm.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.0.weight shape torch.Size([128, 64, 3, 3, 3])
encoder.stages.2.0.convs.0.all_modules.0.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.1.weight shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.1.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
encoder.stages.2.0.convs.1.conv.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.norm.weight shape torch.Size([128])
encoder.stages.2.0.convs.1.norm.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
encoder.stages.2.0.convs.1.all_modules.0.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.1.weight shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.1.bias shape torch.Size([128])
encoder.stages.3.0.convs.0.conv.weight shape torch.Size([256, 128, 3, 3, 3])
encoder.stages.3.0.convs.0.conv.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.norm.weight shape torch.Size([256])
encoder.stages.3.0.convs.0.norm.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.0.weight shape torch.Size([256, 128, 3, 3, 3])
encoder.stages.3.0.convs.0.all_modules.0.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.1.weight shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.1.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
encoder.stages.3.0.convs.1.conv.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.norm.weight shape torch.Size([256])
encoder.stages.3.0.convs.1.norm.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
encoder.stages.3.0.convs.1.all_modules.0.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.1.weight shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.1.bias shape torch.Size([256])
encoder.stages.4.0.convs.0.conv.weight shape torch.Size([320, 256, 3, 3, 3])
encoder.stages.4.0.convs.0.conv.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.norm.weight shape torch.Size([320])
encoder.stages.4.0.convs.0.norm.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.0.weight shape torch.Size([320, 256, 3, 3, 3])
encoder.stages.4.0.convs.0.all_modules.0.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.1.weight shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.1.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.4.0.convs.1.conv.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.norm.weight shape torch.Size([320])
encoder.stages.4.0.convs.1.norm.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.4.0.convs.1.all_modules.0.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.1.weight shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.1.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.0.conv.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.norm.weight shape torch.Size([320])
encoder.stages.5.0.convs.0.norm.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.0.all_modules.0.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.1.weight shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.1.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.1.conv.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.norm.weight shape torch.Size([320])
encoder.stages.5.0.convs.1.norm.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.1.all_modules.0.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.1.weight shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.0.0.convs.0.conv.weight shape torch.Size([32, 1, 3, 3, 3])
decoder.encoder.stages.0.0.convs.0.conv.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.norm.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.norm.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.0.weight shape torch.Size([32, 1, 3, 3, 3])
decoder.encoder.stages.0.0.convs.0.all_modules.0.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.1.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.1.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.encoder.stages.0.0.convs.1.conv.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.norm.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.norm.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.encoder.stages.0.0.convs.1.all_modules.0.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.1.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.1.bias shape torch.Size([32])
decoder.encoder.stages.1.0.convs.0.conv.weight shape torch.Size([64, 32, 3, 3, 3])
decoder.encoder.stages.1.0.convs.0.conv.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.norm.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.norm.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.0.weight shape torch.Size([64, 32, 3, 3, 3])
decoder.encoder.stages.1.0.convs.0.all_modules.0.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.1.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.1.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.encoder.stages.1.0.convs.1.conv.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.norm.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.norm.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.encoder.stages.1.0.convs.1.all_modules.0.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.1.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.1.bias shape torch.Size([64])
decoder.encoder.stages.2.0.convs.0.conv.weight shape torch.Size([128, 64, 3, 3, 3])
decoder.encoder.stages.2.0.convs.0.conv.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.norm.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.norm.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.0.weight shape torch.Size([128, 64, 3, 3, 3])
decoder.encoder.stages.2.0.convs.0.all_modules.0.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.1.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.1.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.encoder.stages.2.0.convs.1.conv.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.norm.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.norm.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.encoder.stages.2.0.convs.1.all_modules.0.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.1.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.1.bias shape torch.Size([128])
decoder.encoder.stages.3.0.convs.0.conv.weight shape torch.Size([256, 128, 3, 3, 3])
decoder.encoder.stages.3.0.convs.0.conv.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.norm.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.norm.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.0.weight shape torch.Size([256, 128, 3, 3, 3])
decoder.encoder.stages.3.0.convs.0.all_modules.0.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.1.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.1.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.encoder.stages.3.0.convs.1.conv.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.norm.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.norm.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.encoder.stages.3.0.convs.1.all_modules.0.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.1.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.1.bias shape torch.Size([256])
decoder.encoder.stages.4.0.convs.0.conv.weight shape torch.Size([320, 256, 3, 3, 3])
decoder.encoder.stages.4.0.convs.0.conv.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.norm.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.norm.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.0.weight shape torch.Size([320, 256, 3, 3, 3])
decoder.encoder.stages.4.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.4.0.convs.1.conv.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.norm.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.norm.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.4.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.0.conv.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.norm.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.norm.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.1.conv.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.norm.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.norm.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.stages.0.convs.0.conv.weight shape torch.Size([320, 640, 3, 3, 3])
decoder.stages.0.convs.0.conv.bias shape torch.Size([320])
decoder.stages.0.convs.0.norm.weight shape torch.Size([320])
decoder.stages.0.convs.0.norm.bias shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.0.weight shape torch.Size([320, 640, 3, 3, 3])
decoder.stages.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.stages.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.stages.0.convs.1.conv.bias shape torch.Size([320])
decoder.stages.0.convs.1.norm.weight shape torch.Size([320])
decoder.stages.0.convs.1.norm.bias shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.stages.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.stages.1.convs.0.conv.weight shape torch.Size([256, 512, 3, 3, 3])
decoder.stages.1.convs.0.conv.bias shape torch.Size([256])
decoder.stages.1.convs.0.norm.weight shape torch.Size([256])
decoder.stages.1.convs.0.norm.bias shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.0.weight shape torch.Size([256, 512, 3, 3, 3])
decoder.stages.1.convs.0.all_modules.0.bias shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.1.weight shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.1.bias shape torch.Size([256])
decoder.stages.1.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.stages.1.convs.1.conv.bias shape torch.Size([256])
decoder.stages.1.convs.1.norm.weight shape torch.Size([256])
decoder.stages.1.convs.1.norm.bias shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.stages.1.convs.1.all_modules.0.bias shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.1.weight shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.1.bias shape torch.Size([256])
decoder.stages.2.convs.0.conv.weight shape torch.Size([128, 256, 3, 3, 3])
decoder.stages.2.convs.0.conv.bias shape torch.Size([128])
decoder.stages.2.convs.0.norm.weight shape torch.Size([128])
decoder.stages.2.convs.0.norm.bias shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.0.weight shape torch.Size([128, 256, 3, 3, 3])
decoder.stages.2.convs.0.all_modules.0.bias shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.1.weight shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.1.bias shape torch.Size([128])
decoder.stages.2.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.stages.2.convs.1.conv.bias shape torch.Size([128])
decoder.stages.2.convs.1.norm.weight shape torch.Size([128])
decoder.stages.2.convs.1.norm.bias shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.stages.2.convs.1.all_modules.0.bias shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.1.weight shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.1.bias shape torch.Size([128])
decoder.stages.3.convs.0.conv.weight shape torch.Size([64, 128, 3, 3, 3])
decoder.stages.3.convs.0.conv.bias shape torch.Size([64])
decoder.stages.3.convs.0.norm.weight shape torch.Size([64])
decoder.stages.3.convs.0.norm.bias shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.0.weight shape torch.Size([64, 128, 3, 3, 3])
decoder.stages.3.convs.0.all_modules.0.bias shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.1.weight shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.1.bias shape torch.Size([64])
decoder.stages.3.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.stages.3.convs.1.conv.bias shape torch.Size([64])
decoder.stages.3.convs.1.norm.weight shape torch.Size([64])
decoder.stages.3.convs.1.norm.bias shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.stages.3.convs.1.all_modules.0.bias shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.1.weight shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.1.bias shape torch.Size([64])
decoder.stages.4.convs.0.conv.weight shape torch.Size([32, 64, 3, 3, 3])
decoder.stages.4.convs.0.conv.bias shape torch.Size([32])
decoder.stages.4.convs.0.norm.weight shape torch.Size([32])
decoder.stages.4.convs.0.norm.bias shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.0.weight shape torch.Size([32, 64, 3, 3, 3])
decoder.stages.4.convs.0.all_modules.0.bias shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.1.weight shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.1.bias shape torch.Size([32])
decoder.stages.4.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.stages.4.convs.1.conv.bias shape torch.Size([32])
decoder.stages.4.convs.1.norm.weight shape torch.Size([32])
decoder.stages.4.convs.1.norm.bias shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.stages.4.convs.1.all_modules.0.bias shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.1.weight shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.1.bias shape torch.Size([32])
decoder.transpconvs.0.weight shape torch.Size([320, 320, 1, 2, 2])
decoder.transpconvs.0.bias shape torch.Size([320])
decoder.transpconvs.1.weight shape torch.Size([320, 256, 2, 2, 2])
decoder.transpconvs.1.bias shape torch.Size([256])
decoder.transpconvs.2.weight shape torch.Size([256, 128, 2, 2, 2])
decoder.transpconvs.2.bias shape torch.Size([128])
decoder.transpconvs.3.weight shape torch.Size([128, 64, 2, 2, 2])
decoder.transpconvs.3.bias shape torch.Size([64])
decoder.transpconvs.4.weight shape torch.Size([64, 32, 2, 2, 2])
decoder.transpconvs.4.bias shape torch.Size([32])
################### Done ###################
2024-12-15 10:49:43.573436: do_dummy_2d_data_aug: True
2024-12-15 10:49:43.575597: Using splits from existing split file: /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset307_Sohee_Calcium_OCT_CrossValidation/splits_final_80.json
2024-12-15 10:49:43.578203: The split file contains 5 splits.
2024-12-15 10:49:43.579323: Desired fold for training: 4
2024-12-15 10:49:43.580475: This split has 6 training and 5 validation cases.
using pin_memory on device 0
using pin_memory on device 0

This is the configuration used by this training:
Configuration name: 3d_32x160x128_b10
 {'data_identifier': 'nnUNetPlans_3d_fullres', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 10, 'patch_size': [32, 160, 128], 'median_image_size_in_voxels': [375.0, 498.0, 498.0], 'spacing': [1.0, 1.0, 1.0], 'normalization_schemes': ['ZScoreNormalization'], 'use_mask_for_norm': [False], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'architecture': {'network_class_name': 'dynamic_network_architectures.architectures.unet.PlainConvUNet', 'arch_kwargs': {'n_stages': 6, 'features_per_stage': [32, 64, 128, 256, 320, 320], 'conv_op': 'torch.nn.modules.conv.Conv3d', 'kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]], 'strides': [[1, 1, 1], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [1, 2, 2]], 'n_conv_per_stage': [2, 2, 2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2, 2, 2], 'conv_bias': True, 'norm_op': 'torch.nn.modules.instancenorm.InstanceNorm3d', 'norm_op_kwargs': {'eps': 1e-05, 'affine': True}, 'dropout_op': None, 'dropout_op_kwargs': None, 'nonlin': 'torch.nn.LeakyReLU', 'nonlin_kwargs': {'inplace': True}, 'deep_supervision': True}, '_kw_requires_import': ['conv_op', 'norm_op', 'dropout_op', 'nonlin']}, 'batch_dice': True, 'inherits_from': '3d_fullres'} 

These are the global plan.json settings:
 {'dataset_name': 'Dataset307_Sohee_Calcium_OCT_CrossValidation', 'plans_name': 'nnUNetPlans', 'original_median_spacing_after_transp': [1.0, 1.0, 1.0], 'original_median_shape_after_transp': [375, 498, 498], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 0.4977976381778717, 'mean': 0.13507525622844696, 'median': 0.09599608182907104, 'min': 0.0, 'percentile_00_5': 0.014754901640117168, 'percentile_99_5': 0.4977976381778717, 'std': 0.12025152146816254}}} 

2024-12-15 10:50:13.243536: unpacking dataset...
2024-12-15 10:50:17.752547: unpacking done...
2024-12-15 10:50:18.031903: Unable to plot network architecture: nnUNet_compile is enabled!
2024-12-15 10:50:18.252032: 
2024-12-15 10:50:18.253278: Epoch 0
2024-12-15 10:50:18.254144: Current learning rate: 0.01
2024-12-15 10:58:26.487471: Validation loss improved from 1000.00000 to -0.16648! Patience: 0/50
2024-12-15 10:58:26.488784: train_loss -0.0966
2024-12-15 10:58:26.489803: val_loss -0.1665
2024-12-15 10:58:26.490766: Pseudo dice [0.5294]
2024-12-15 10:58:26.491959: Epoch time: 488.24 s
2024-12-15 10:58:26.493085: Yayy! New best EMA pseudo Dice: 0.5294
2024-12-15 10:58:28.383523: 
2024-12-15 10:58:28.385099: Epoch 1
2024-12-15 10:58:28.386375: Current learning rate: 0.00994
2024-12-15 11:05:10.793755: Validation loss improved from -0.16648 to -0.23598! Patience: 0/50
2024-12-15 11:05:10.794849: train_loss -0.2396
2024-12-15 11:05:10.796074: val_loss -0.236
2024-12-15 11:05:10.797118: Pseudo dice [0.5509]
2024-12-15 11:05:10.798171: Epoch time: 402.41 s
2024-12-15 11:05:10.799596: Yayy! New best EMA pseudo Dice: 0.5316
2024-12-15 11:05:12.476548: 
2024-12-15 11:05:12.477884: Epoch 2
2024-12-15 11:05:12.479309: Current learning rate: 0.00988
2024-12-15 11:11:47.189206: Validation loss improved from -0.23598 to -0.24164! Patience: 0/50
2024-12-15 11:11:47.190248: train_loss -0.3139
2024-12-15 11:11:47.190994: val_loss -0.2416
2024-12-15 11:11:47.191996: Pseudo dice [0.5796]
2024-12-15 11:11:47.192850: Epoch time: 394.71 s
2024-12-15 11:11:47.193473: Yayy! New best EMA pseudo Dice: 0.5364
2024-12-15 11:11:48.941384: 
2024-12-15 11:11:48.942676: Epoch 3
2024-12-15 11:11:48.943691: Current learning rate: 0.00982
2024-12-15 11:18:23.705006: Validation loss improved from -0.24164 to -0.34892! Patience: 0/50
2024-12-15 11:18:23.706045: train_loss -0.3359
2024-12-15 11:18:23.706992: val_loss -0.3489
2024-12-15 11:18:23.707821: Pseudo dice [0.6277]
2024-12-15 11:18:23.708984: Epoch time: 394.77 s
2024-12-15 11:18:23.709960: Yayy! New best EMA pseudo Dice: 0.5455
2024-12-15 11:18:25.421732: 
2024-12-15 11:18:25.423144: Epoch 4
2024-12-15 11:18:25.424119: Current learning rate: 0.00976
2024-12-15 11:25:03.884189: Validation loss improved from -0.34892 to -0.36157! Patience: 0/50
2024-12-15 11:25:03.885148: train_loss -0.3736
2024-12-15 11:25:03.886263: val_loss -0.3616
2024-12-15 11:25:03.887012: Pseudo dice [0.6463]
2024-12-15 11:25:03.887701: Epoch time: 398.46 s
2024-12-15 11:25:04.233602: Yayy! New best EMA pseudo Dice: 0.5556
2024-12-15 11:25:05.977398: 
2024-12-15 11:25:05.978843: Epoch 5
2024-12-15 11:25:05.979652: Current learning rate: 0.0097
2024-12-15 11:31:33.304339: Validation loss improved from -0.36157 to -0.37293! Patience: 0/50
2024-12-15 11:31:33.305546: train_loss -0.3948
2024-12-15 11:31:33.306395: val_loss -0.3729
2024-12-15 11:31:33.307290: Pseudo dice [0.6432]
2024-12-15 11:31:33.308158: Epoch time: 387.33 s
2024-12-15 11:31:33.308748: Yayy! New best EMA pseudo Dice: 0.5643
2024-12-15 11:31:34.972970: 
2024-12-15 11:31:34.974135: Epoch 6
2024-12-15 11:31:34.974912: Current learning rate: 0.00964
2024-12-15 11:37:46.126029: Validation loss improved from -0.37293 to -0.38874! Patience: 0/50
2024-12-15 11:37:46.126925: train_loss -0.4302
2024-12-15 11:37:46.128045: val_loss -0.3887
2024-12-15 11:37:46.128835: Pseudo dice [0.6565]
2024-12-15 11:37:46.129581: Epoch time: 371.16 s
2024-12-15 11:37:46.130185: Yayy! New best EMA pseudo Dice: 0.5736
2024-12-15 11:37:47.881128: 
2024-12-15 11:37:47.882534: Epoch 7
2024-12-15 11:37:47.883235: Current learning rate: 0.00958
2024-12-15 11:43:23.179830: Validation loss improved from -0.38874 to -0.41030! Patience: 0/50
2024-12-15 11:43:23.180836: train_loss -0.4516
2024-12-15 11:43:23.181816: val_loss -0.4103
2024-12-15 11:43:23.182871: Pseudo dice [0.6746]
2024-12-15 11:43:23.183920: Epoch time: 335.3 s
2024-12-15 11:43:23.184619: Yayy! New best EMA pseudo Dice: 0.5837
2024-12-15 11:43:25.290430: 
2024-12-15 11:43:25.292406: Epoch 8
2024-12-15 11:43:25.293511: Current learning rate: 0.00952
2024-12-15 11:49:12.838234: Validation loss improved from -0.41030 to -0.41854! Patience: 0/50
2024-12-15 11:49:12.839464: train_loss -0.4709
2024-12-15 11:49:12.840502: val_loss -0.4185
2024-12-15 11:49:12.841439: Pseudo dice [0.6619]
2024-12-15 11:49:12.842216: Epoch time: 347.55 s
2024-12-15 11:49:12.843066: Yayy! New best EMA pseudo Dice: 0.5915
2024-12-15 11:49:14.592687: 
2024-12-15 11:49:14.594290: Epoch 9
2024-12-15 11:49:14.595264: Current learning rate: 0.00946
2024-12-15 11:54:49.574306: Validation loss improved from -0.41854 to -0.42192! Patience: 0/50
2024-12-15 11:54:49.575532: train_loss -0.495
2024-12-15 11:54:49.576861: val_loss -0.4219
2024-12-15 11:54:49.578151: Pseudo dice [0.6525]
2024-12-15 11:54:49.579302: Epoch time: 334.98 s
2024-12-15 11:54:49.964733: Yayy! New best EMA pseudo Dice: 0.5976
2024-12-15 11:54:51.637551: 
2024-12-15 11:54:51.639134: Epoch 10
2024-12-15 11:54:51.640399: Current learning rate: 0.0094
2024-12-15 12:01:34.090258: Validation loss improved from -0.42192 to -0.43691! Patience: 0/50
2024-12-15 12:01:34.094053: train_loss -0.5002
2024-12-15 12:01:34.096158: val_loss -0.4369
2024-12-15 12:01:34.097245: Pseudo dice [0.6728]
2024-12-15 12:01:34.098109: Epoch time: 402.46 s
2024-12-15 12:01:34.099037: Yayy! New best EMA pseudo Dice: 0.6051
2024-12-15 12:01:35.844869: 
2024-12-15 12:01:35.846449: Epoch 11
2024-12-15 12:01:35.847209: Current learning rate: 0.00934
2024-12-15 12:08:31.843971: Validation loss did not improve from -0.43691. Patience: 1/50
2024-12-15 12:08:31.845029: train_loss -0.5048
2024-12-15 12:08:31.846057: val_loss -0.4362
2024-12-15 12:08:31.846990: Pseudo dice [0.6755]
2024-12-15 12:08:31.847893: Epoch time: 416.0 s
2024-12-15 12:08:31.848657: Yayy! New best EMA pseudo Dice: 0.6121
2024-12-15 12:08:33.552989: 
2024-12-15 12:08:33.554598: Epoch 12
2024-12-15 12:08:33.555526: Current learning rate: 0.00928
2024-12-15 12:15:07.416559: Validation loss improved from -0.43691 to -0.45030! Patience: 1/50
2024-12-15 12:15:07.417939: train_loss -0.5307
2024-12-15 12:15:07.419146: val_loss -0.4503
2024-12-15 12:15:07.420109: Pseudo dice [0.6882]
2024-12-15 12:15:07.421179: Epoch time: 393.87 s
2024-12-15 12:15:07.422170: Yayy! New best EMA pseudo Dice: 0.6197
2024-12-15 12:15:09.146756: 
2024-12-15 12:15:09.148124: Epoch 13
2024-12-15 12:15:09.149063: Current learning rate: 0.00922
2024-12-15 12:22:00.531050: Validation loss improved from -0.45030 to -0.46399! Patience: 0/50
2024-12-15 12:22:00.532059: train_loss -0.5404
2024-12-15 12:22:00.533325: val_loss -0.464
2024-12-15 12:22:00.534208: Pseudo dice [0.6948]
2024-12-15 12:22:00.534991: Epoch time: 411.39 s
2024-12-15 12:22:00.535614: Yayy! New best EMA pseudo Dice: 0.6272
2024-12-15 12:22:02.253567: 
2024-12-15 12:22:02.254999: Epoch 14
2024-12-15 12:22:02.255746: Current learning rate: 0.00916
2024-12-15 12:28:40.921588: Validation loss did not improve from -0.46399. Patience: 1/50
2024-12-15 12:28:40.922762: train_loss -0.5541
2024-12-15 12:28:40.923556: val_loss -0.4494
2024-12-15 12:28:40.924237: Pseudo dice [0.6805]
2024-12-15 12:28:40.925159: Epoch time: 398.67 s
2024-12-15 12:28:41.304569: Yayy! New best EMA pseudo Dice: 0.6326
2024-12-15 12:28:43.021330: 
2024-12-15 12:28:43.022776: Epoch 15
2024-12-15 12:28:43.023752: Current learning rate: 0.0091
2024-12-15 12:35:21.828171: Validation loss did not improve from -0.46399. Patience: 2/50
2024-12-15 12:35:21.829220: train_loss -0.5415
2024-12-15 12:35:21.830156: val_loss -0.4551
2024-12-15 12:35:21.831243: Pseudo dice [0.6992]
2024-12-15 12:35:21.831998: Epoch time: 398.81 s
2024-12-15 12:35:21.832633: Yayy! New best EMA pseudo Dice: 0.6392
2024-12-15 12:35:23.566363: 
2024-12-15 12:35:23.567903: Epoch 16
2024-12-15 12:35:23.568963: Current learning rate: 0.00903
2024-12-15 12:41:36.824722: Validation loss did not improve from -0.46399. Patience: 3/50
2024-12-15 12:41:36.825892: train_loss -0.5544
2024-12-15 12:41:36.826631: val_loss -0.4609
2024-12-15 12:41:36.827283: Pseudo dice [0.6928]
2024-12-15 12:41:36.828026: Epoch time: 373.26 s
2024-12-15 12:41:36.828661: Yayy! New best EMA pseudo Dice: 0.6446
2024-12-15 12:41:38.622793: 
2024-12-15 12:41:38.624518: Epoch 17
2024-12-15 12:41:38.625559: Current learning rate: 0.00897
2024-12-15 12:47:55.393768: Validation loss did not improve from -0.46399. Patience: 4/50
2024-12-15 12:47:55.394689: train_loss -0.557
2024-12-15 12:47:55.395391: val_loss -0.4619
2024-12-15 12:47:55.396034: Pseudo dice [0.704]
2024-12-15 12:47:55.397038: Epoch time: 376.77 s
2024-12-15 12:47:55.397748: Yayy! New best EMA pseudo Dice: 0.6505
2024-12-15 12:47:57.155039: 
2024-12-15 12:47:57.156353: Epoch 18
2024-12-15 12:47:57.157555: Current learning rate: 0.00891
2024-12-15 12:54:21.441357: Validation loss improved from -0.46399 to -0.47309! Patience: 4/50
2024-12-15 12:54:21.445072: train_loss -0.5673
2024-12-15 12:54:21.447679: val_loss -0.4731
2024-12-15 12:54:21.448560: Pseudo dice [0.7055]
2024-12-15 12:54:21.449683: Epoch time: 384.29 s
2024-12-15 12:54:21.450420: Yayy! New best EMA pseudo Dice: 0.656
2024-12-15 12:54:24.061240: 
2024-12-15 12:54:24.062315: Epoch 19
2024-12-15 12:54:24.063001: Current learning rate: 0.00885
2024-12-15 13:00:34.041018: Validation loss did not improve from -0.47309. Patience: 1/50
2024-12-15 13:00:34.042222: train_loss -0.5665
2024-12-15 13:00:34.043012: val_loss -0.4367
2024-12-15 13:00:34.043648: Pseudo dice [0.6827]
2024-12-15 13:00:34.044278: Epoch time: 369.98 s
2024-12-15 13:00:34.447851: Yayy! New best EMA pseudo Dice: 0.6587
2024-12-15 13:00:36.193979: 
2024-12-15 13:00:36.195338: Epoch 20
2024-12-15 13:00:36.196043: Current learning rate: 0.00879
2024-12-15 13:06:32.659806: Validation loss did not improve from -0.47309. Patience: 2/50
2024-12-15 13:06:32.661139: train_loss -0.5786
2024-12-15 13:06:32.662117: val_loss -0.4714
2024-12-15 13:06:32.662941: Pseudo dice [0.693]
2024-12-15 13:06:32.663857: Epoch time: 356.47 s
2024-12-15 13:06:32.664834: Yayy! New best EMA pseudo Dice: 0.6621
2024-12-15 13:06:34.441231: 
2024-12-15 13:06:34.442792: Epoch 21
2024-12-15 13:06:34.444004: Current learning rate: 0.00873
2024-12-15 13:12:32.839818: Validation loss did not improve from -0.47309. Patience: 3/50
2024-12-15 13:12:32.840786: train_loss -0.5949
2024-12-15 13:12:32.841510: val_loss -0.4622
2024-12-15 13:12:32.842338: Pseudo dice [0.6944]
2024-12-15 13:12:32.843150: Epoch time: 358.4 s
2024-12-15 13:12:32.844117: Yayy! New best EMA pseudo Dice: 0.6654
2024-12-15 13:12:34.560274: 
2024-12-15 13:12:34.561685: Epoch 22
2024-12-15 13:12:34.562432: Current learning rate: 0.00867
2024-12-15 13:18:26.947730: Validation loss did not improve from -0.47309. Patience: 4/50
2024-12-15 13:18:26.949078: train_loss -0.5963
2024-12-15 13:18:26.949975: val_loss -0.4515
2024-12-15 13:18:26.950636: Pseudo dice [0.6823]
2024-12-15 13:18:26.951367: Epoch time: 352.39 s
2024-12-15 13:18:26.952000: Yayy! New best EMA pseudo Dice: 0.6671
2024-12-15 13:18:28.678811: 
2024-12-15 13:18:28.680238: Epoch 23
2024-12-15 13:18:28.680985: Current learning rate: 0.00861
2024-12-15 13:24:54.805415: Validation loss improved from -0.47309 to -0.49670! Patience: 4/50
2024-12-15 13:24:54.806529: train_loss -0.6053
2024-12-15 13:24:54.807233: val_loss -0.4967
2024-12-15 13:24:54.807927: Pseudo dice [0.7195]
2024-12-15 13:24:54.808646: Epoch time: 386.13 s
2024-12-15 13:24:54.809630: Yayy! New best EMA pseudo Dice: 0.6723
2024-12-15 13:24:56.528050: 
2024-12-15 13:24:56.529445: Epoch 24
2024-12-15 13:24:56.530157: Current learning rate: 0.00855
2024-12-15 13:31:39.395503: Validation loss did not improve from -0.49670. Patience: 1/50
2024-12-15 13:31:39.396501: train_loss -0.6164
2024-12-15 13:31:39.397873: val_loss -0.4952
2024-12-15 13:31:39.398909: Pseudo dice [0.719]
2024-12-15 13:31:39.400123: Epoch time: 402.87 s
2024-12-15 13:31:39.817202: Yayy! New best EMA pseudo Dice: 0.677
2024-12-15 13:31:41.531942: 
2024-12-15 13:31:41.532966: Epoch 25
2024-12-15 13:31:41.533786: Current learning rate: 0.00849
2024-12-15 13:38:01.762817: Validation loss did not improve from -0.49670. Patience: 2/50
2024-12-15 13:38:01.763972: train_loss -0.6191
2024-12-15 13:38:01.765145: val_loss -0.475
2024-12-15 13:38:01.766335: Pseudo dice [0.7063]
2024-12-15 13:38:01.767574: Epoch time: 380.23 s
2024-12-15 13:38:01.768677: Yayy! New best EMA pseudo Dice: 0.6799
2024-12-15 13:38:03.475502: 
2024-12-15 13:38:03.476737: Epoch 26
2024-12-15 13:38:03.477818: Current learning rate: 0.00843
2024-12-15 13:44:45.591073: Validation loss did not improve from -0.49670. Patience: 3/50
2024-12-15 13:44:45.591919: train_loss -0.611
2024-12-15 13:44:45.592787: val_loss -0.4555
2024-12-15 13:44:45.593845: Pseudo dice [0.6864]
2024-12-15 13:44:45.594856: Epoch time: 402.12 s
2024-12-15 13:44:45.595888: Yayy! New best EMA pseudo Dice: 0.6806
2024-12-15 13:44:47.298260: 
2024-12-15 13:44:47.299686: Epoch 27
2024-12-15 13:44:47.300398: Current learning rate: 0.00836
2024-12-15 13:51:23.591000: Validation loss did not improve from -0.49670. Patience: 4/50
2024-12-15 13:51:23.591892: train_loss -0.6141
2024-12-15 13:51:23.592582: val_loss -0.4828
2024-12-15 13:51:23.593203: Pseudo dice [0.7142]
2024-12-15 13:51:23.593959: Epoch time: 396.29 s
2024-12-15 13:51:23.594719: Yayy! New best EMA pseudo Dice: 0.6839
2024-12-15 13:51:25.321234: 
2024-12-15 13:51:25.322618: Epoch 28
2024-12-15 13:51:25.323502: Current learning rate: 0.0083
2024-12-15 13:58:13.506764: Validation loss did not improve from -0.49670. Patience: 5/50
2024-12-15 13:58:13.510534: train_loss -0.6246
2024-12-15 13:58:13.512407: val_loss -0.4738
2024-12-15 13:58:13.513078: Pseudo dice [0.7034]
2024-12-15 13:58:13.514370: Epoch time: 408.19 s
2024-12-15 13:58:13.515078: Yayy! New best EMA pseudo Dice: 0.6859
2024-12-15 13:58:15.862431: 
2024-12-15 13:58:15.863726: Epoch 29
2024-12-15 13:58:15.864511: Current learning rate: 0.00824
2024-12-15 14:04:52.498786: Validation loss did not improve from -0.49670. Patience: 6/50
2024-12-15 14:04:52.499761: train_loss -0.6306
2024-12-15 14:04:52.500550: val_loss -0.4569
2024-12-15 14:04:52.501411: Pseudo dice [0.695]
2024-12-15 14:04:52.502333: Epoch time: 396.64 s
2024-12-15 14:04:52.842727: Yayy! New best EMA pseudo Dice: 0.6868
2024-12-15 14:04:54.552845: 
2024-12-15 14:04:54.554059: Epoch 30
2024-12-15 14:04:54.554773: Current learning rate: 0.00818
2024-12-15 14:11:51.181696: Validation loss did not improve from -0.49670. Patience: 7/50
2024-12-15 14:11:51.183281: train_loss -0.6247
2024-12-15 14:11:51.184101: val_loss -0.4752
2024-12-15 14:11:51.184731: Pseudo dice [0.6965]
2024-12-15 14:11:51.186087: Epoch time: 416.63 s
2024-12-15 14:11:51.187237: Yayy! New best EMA pseudo Dice: 0.6878
2024-12-15 14:11:52.946504: 
2024-12-15 14:11:52.947739: Epoch 31
2024-12-15 14:11:52.948460: Current learning rate: 0.00812
2024-12-15 14:18:50.367905: Validation loss did not improve from -0.49670. Patience: 8/50
2024-12-15 14:18:50.369118: train_loss -0.6369
2024-12-15 14:18:50.370378: val_loss -0.4528
2024-12-15 14:18:50.371384: Pseudo dice [0.6804]
2024-12-15 14:18:50.372715: Epoch time: 417.42 s
2024-12-15 14:18:51.728818: 
2024-12-15 14:18:51.730403: Epoch 32
2024-12-15 14:18:51.731656: Current learning rate: 0.00806
2024-12-15 14:25:54.328600: Validation loss improved from -0.49670 to -0.49751! Patience: 8/50
2024-12-15 14:25:54.329809: train_loss -0.6371
2024-12-15 14:25:54.330521: val_loss -0.4975
2024-12-15 14:25:54.331162: Pseudo dice [0.7177]
2024-12-15 14:25:54.331816: Epoch time: 422.6 s
2024-12-15 14:25:54.332451: Yayy! New best EMA pseudo Dice: 0.6901
2024-12-15 14:25:56.085800: 
2024-12-15 14:25:56.087120: Epoch 33
2024-12-15 14:25:56.088005: Current learning rate: 0.008
2024-12-15 14:32:53.176172: Validation loss did not improve from -0.49751. Patience: 1/50
2024-12-15 14:32:53.177195: train_loss -0.6456
2024-12-15 14:32:53.178015: val_loss -0.485
2024-12-15 14:32:53.178885: Pseudo dice [0.7122]
2024-12-15 14:32:53.179685: Epoch time: 417.09 s
2024-12-15 14:32:53.180486: Yayy! New best EMA pseudo Dice: 0.6923
2024-12-15 14:32:54.923447: 
2024-12-15 14:32:54.924863: Epoch 34
2024-12-15 14:32:54.925622: Current learning rate: 0.00793
2024-12-15 14:39:58.504652: Validation loss improved from -0.49751 to -0.52312! Patience: 1/50
2024-12-15 14:39:58.506328: train_loss -0.6433
2024-12-15 14:39:58.507459: val_loss -0.5231
2024-12-15 14:39:58.508148: Pseudo dice [0.7306]
2024-12-15 14:39:58.509099: Epoch time: 423.58 s
2024-12-15 14:39:58.910653: Yayy! New best EMA pseudo Dice: 0.6961
2024-12-15 14:40:00.648743: 
2024-12-15 14:40:00.651034: Epoch 35
2024-12-15 14:40:00.652094: Current learning rate: 0.00787
2024-12-15 14:46:30.947201: Validation loss did not improve from -0.52312. Patience: 1/50
2024-12-15 14:46:30.948139: train_loss -0.644
2024-12-15 14:46:30.949186: val_loss -0.4701
2024-12-15 14:46:30.950094: Pseudo dice [0.7018]
2024-12-15 14:46:30.951213: Epoch time: 390.3 s
2024-12-15 14:46:30.952147: Yayy! New best EMA pseudo Dice: 0.6967
2024-12-15 14:46:32.681540: 
2024-12-15 14:46:32.682659: Epoch 36
2024-12-15 14:46:32.683402: Current learning rate: 0.00781
2024-12-15 14:53:00.851091: Validation loss did not improve from -0.52312. Patience: 2/50
2024-12-15 14:53:00.852054: train_loss -0.6559
2024-12-15 14:53:00.852813: val_loss -0.5114
2024-12-15 14:53:00.853546: Pseudo dice [0.7254]
2024-12-15 14:53:00.854143: Epoch time: 388.17 s
2024-12-15 14:53:00.854757: Yayy! New best EMA pseudo Dice: 0.6996
2024-12-15 14:53:02.603074: 
2024-12-15 14:53:02.604377: Epoch 37
2024-12-15 14:53:02.605091: Current learning rate: 0.00775
2024-12-15 14:59:37.270016: Validation loss did not improve from -0.52312. Patience: 3/50
2024-12-15 14:59:37.271198: train_loss -0.6426
2024-12-15 14:59:37.272438: val_loss -0.4539
2024-12-15 14:59:37.273719: Pseudo dice [0.6881]
2024-12-15 14:59:37.274805: Epoch time: 394.67 s
2024-12-15 14:59:38.630741: 
2024-12-15 14:59:38.632087: Epoch 38
2024-12-15 14:59:38.633199: Current learning rate: 0.00769
2024-12-15 15:06:00.456636: Validation loss did not improve from -0.52312. Patience: 4/50
2024-12-15 15:06:00.459252: train_loss -0.6469
2024-12-15 15:06:00.460776: val_loss -0.4454
2024-12-15 15:06:00.461434: Pseudo dice [0.6917]
2024-12-15 15:06:00.462257: Epoch time: 381.83 s
2024-12-15 15:06:01.833659: 
2024-12-15 15:06:01.835089: Epoch 39
2024-12-15 15:06:01.835971: Current learning rate: 0.00763
2024-12-15 15:12:24.722447: Validation loss did not improve from -0.52312. Patience: 5/50
2024-12-15 15:12:24.724963: train_loss -0.6604
2024-12-15 15:12:24.726418: val_loss -0.4836
2024-12-15 15:12:24.727557: Pseudo dice [0.7091]
2024-12-15 15:12:24.728496: Epoch time: 382.89 s
2024-12-15 15:12:27.323775: 
2024-12-15 15:12:27.325386: Epoch 40
2024-12-15 15:12:27.326358: Current learning rate: 0.00756
2024-12-15 15:18:55.308816: Validation loss did not improve from -0.52312. Patience: 6/50
2024-12-15 15:18:55.310247: train_loss -0.6694
2024-12-15 15:18:55.311230: val_loss -0.4962
2024-12-15 15:18:55.312446: Pseudo dice [0.7201]
2024-12-15 15:18:55.313320: Epoch time: 387.99 s
2024-12-15 15:18:55.314101: Yayy! New best EMA pseudo Dice: 0.701
2024-12-15 15:18:57.043951: 
2024-12-15 15:18:57.045675: Epoch 41
2024-12-15 15:18:57.046961: Current learning rate: 0.0075
2024-12-15 15:24:20.040034: Validation loss did not improve from -0.52312. Patience: 7/50
2024-12-15 15:24:20.041438: train_loss -0.6709
2024-12-15 15:24:20.042658: val_loss -0.504
2024-12-15 15:24:20.043469: Pseudo dice [0.7136]
2024-12-15 15:24:20.044470: Epoch time: 323.0 s
2024-12-15 15:24:20.045259: Yayy! New best EMA pseudo Dice: 0.7023
2024-12-15 15:24:21.722855: 
2024-12-15 15:24:21.724102: Epoch 42
2024-12-15 15:24:21.724867: Current learning rate: 0.00744
2024-12-15 15:29:03.556297: Validation loss did not improve from -0.52312. Patience: 8/50
2024-12-15 15:29:03.557065: train_loss -0.6764
2024-12-15 15:29:03.557953: val_loss -0.4805
2024-12-15 15:29:03.558808: Pseudo dice [0.7054]
2024-12-15 15:29:03.559510: Epoch time: 281.84 s
2024-12-15 15:29:03.560427: Yayy! New best EMA pseudo Dice: 0.7026
2024-12-15 15:29:05.264008: 
2024-12-15 15:29:05.264957: Epoch 43
2024-12-15 15:29:05.265732: Current learning rate: 0.00738
2024-12-15 15:33:36.028808: Validation loss did not improve from -0.52312. Patience: 9/50
2024-12-15 15:33:36.029631: train_loss -0.6696
2024-12-15 15:33:36.030567: val_loss -0.4808
2024-12-15 15:33:36.031701: Pseudo dice [0.7097]
2024-12-15 15:33:36.032612: Epoch time: 270.77 s
2024-12-15 15:33:36.033263: Yayy! New best EMA pseudo Dice: 0.7033
2024-12-15 15:33:37.750068: 
2024-12-15 15:33:37.751233: Epoch 44
2024-12-15 15:33:37.751904: Current learning rate: 0.00732
2024-12-15 15:37:42.701497: Validation loss did not improve from -0.52312. Patience: 10/50
2024-12-15 15:37:42.702378: train_loss -0.6649
2024-12-15 15:37:42.703193: val_loss -0.4349
2024-12-15 15:37:42.703888: Pseudo dice [0.6836]
2024-12-15 15:37:42.704587: Epoch time: 244.95 s
2024-12-15 15:37:44.422207: 
2024-12-15 15:37:44.423625: Epoch 45
2024-12-15 15:37:44.424242: Current learning rate: 0.00725
2024-12-15 15:41:23.534233: Validation loss did not improve from -0.52312. Patience: 11/50
2024-12-15 15:41:23.535491: train_loss -0.6771
2024-12-15 15:41:23.536294: val_loss -0.4948
2024-12-15 15:41:23.537086: Pseudo dice [0.7132]
2024-12-15 15:41:23.537756: Epoch time: 219.11 s
2024-12-15 15:41:24.855942: 
2024-12-15 15:41:24.857249: Epoch 46
2024-12-15 15:41:24.857893: Current learning rate: 0.00719
2024-12-15 15:45:19.063952: Validation loss did not improve from -0.52312. Patience: 12/50
2024-12-15 15:45:19.064967: train_loss -0.6872
2024-12-15 15:45:19.065799: val_loss -0.4531
2024-12-15 15:45:19.066770: Pseudo dice [0.6926]
2024-12-15 15:45:19.067608: Epoch time: 234.21 s
2024-12-15 15:45:20.370948: 
2024-12-15 15:45:20.372351: Epoch 47
2024-12-15 15:45:20.373186: Current learning rate: 0.00713
2024-12-15 15:49:58.511858: Validation loss did not improve from -0.52312. Patience: 13/50
2024-12-15 15:49:58.512816: train_loss -0.6775
2024-12-15 15:49:58.513669: val_loss -0.4977
2024-12-15 15:49:58.514359: Pseudo dice [0.7164]
2024-12-15 15:49:58.515100: Epoch time: 278.14 s
2024-12-15 15:49:59.836990: 
2024-12-15 15:49:59.838173: Epoch 48
2024-12-15 15:49:59.838804: Current learning rate: 0.00707
2024-12-15 15:54:24.467001: Validation loss did not improve from -0.52312. Patience: 14/50
2024-12-15 15:54:24.467845: train_loss -0.6853
2024-12-15 15:54:24.468990: val_loss -0.4881
2024-12-15 15:54:24.469877: Pseudo dice [0.7135]
2024-12-15 15:54:24.470748: Epoch time: 264.63 s
2024-12-15 15:54:24.471632: Yayy! New best EMA pseudo Dice: 0.7041
2024-12-15 15:54:26.190133: 
2024-12-15 15:54:26.191350: Epoch 49
2024-12-15 15:54:26.192482: Current learning rate: 0.007
2024-12-15 15:58:50.762235: Validation loss did not improve from -0.52312. Patience: 15/50
2024-12-15 15:58:50.763138: train_loss -0.6909
2024-12-15 15:58:50.764165: val_loss -0.4449
2024-12-15 15:58:50.765071: Pseudo dice [0.6826]
2024-12-15 15:58:50.765862: Epoch time: 264.57 s
2024-12-15 15:58:52.517527: 
2024-12-15 15:58:52.518999: Epoch 50
2024-12-15 15:58:52.520222: Current learning rate: 0.00694
2024-12-15 16:03:08.693337: Validation loss did not improve from -0.52312. Patience: 16/50
2024-12-15 16:03:08.694395: train_loss -0.6866
2024-12-15 16:03:08.695275: val_loss -0.4725
2024-12-15 16:03:08.696095: Pseudo dice [0.7091]
2024-12-15 16:03:08.696871: Epoch time: 256.18 s
2024-12-15 16:03:10.404316: 
2024-12-15 16:03:10.406031: Epoch 51
2024-12-15 16:03:10.406822: Current learning rate: 0.00688
2024-12-15 16:07:35.777817: Validation loss did not improve from -0.52312. Patience: 17/50
2024-12-15 16:07:35.778628: train_loss -0.6914
2024-12-15 16:07:35.780101: val_loss -0.4776
2024-12-15 16:07:35.781038: Pseudo dice [0.712]
2024-12-15 16:07:35.781892: Epoch time: 265.38 s
2024-12-15 16:07:37.180650: 
2024-12-15 16:07:37.181975: Epoch 52
2024-12-15 16:07:37.182932: Current learning rate: 0.00682
2024-12-15 16:12:08.474029: Validation loss did not improve from -0.52312. Patience: 18/50
2024-12-15 16:12:08.477272: train_loss -0.6958
2024-12-15 16:12:08.478232: val_loss -0.4454
2024-12-15 16:12:08.479029: Pseudo dice [0.6839]
2024-12-15 16:12:08.479796: Epoch time: 271.3 s
2024-12-15 16:12:09.842871: 
2024-12-15 16:12:09.843784: Epoch 53
2024-12-15 16:12:09.844635: Current learning rate: 0.00675
2024-12-15 16:16:39.714011: Validation loss did not improve from -0.52312. Patience: 19/50
2024-12-15 16:16:39.714683: train_loss -0.6976
2024-12-15 16:16:39.715768: val_loss -0.3826
2024-12-15 16:16:39.716559: Pseudo dice [0.654]
2024-12-15 16:16:39.717353: Epoch time: 269.87 s
2024-12-15 16:16:41.102965: 
2024-12-15 16:16:41.103991: Epoch 54
2024-12-15 16:16:41.104840: Current learning rate: 0.00669
2024-12-15 16:21:11.305316: Validation loss did not improve from -0.52312. Patience: 20/50
2024-12-15 16:21:11.306640: train_loss -0.6945
2024-12-15 16:21:11.307441: val_loss -0.4885
2024-12-15 16:21:11.308185: Pseudo dice [0.7104]
2024-12-15 16:21:11.309160: Epoch time: 270.2 s
2024-12-15 16:21:13.073292: 
2024-12-15 16:21:13.074221: Epoch 55
2024-12-15 16:21:13.074802: Current learning rate: 0.00663
2024-12-15 16:25:51.110000: Validation loss did not improve from -0.52312. Patience: 21/50
2024-12-15 16:25:51.110883: train_loss -0.7061
2024-12-15 16:25:51.111643: val_loss -0.4726
2024-12-15 16:25:51.112415: Pseudo dice [0.7046]
2024-12-15 16:25:51.113336: Epoch time: 278.04 s
2024-12-15 16:25:52.474761: 
2024-12-15 16:25:52.476358: Epoch 56
2024-12-15 16:25:52.477534: Current learning rate: 0.00657
2024-12-15 16:30:16.880470: Validation loss did not improve from -0.52312. Patience: 22/50
2024-12-15 16:30:16.881197: train_loss -0.6998
2024-12-15 16:30:16.881850: val_loss -0.5175
2024-12-15 16:30:16.882533: Pseudo dice [0.74]
2024-12-15 16:30:16.883110: Epoch time: 264.41 s
2024-12-15 16:30:18.239632: 
2024-12-15 16:30:18.240911: Epoch 57
2024-12-15 16:30:18.241874: Current learning rate: 0.0065
2024-12-15 16:34:47.754855: Validation loss did not improve from -0.52312. Patience: 23/50
2024-12-15 16:34:47.755480: train_loss -0.7056
2024-12-15 16:34:47.756372: val_loss -0.4594
2024-12-15 16:34:47.757340: Pseudo dice [0.6964]
2024-12-15 16:34:47.758027: Epoch time: 269.52 s
2024-12-15 16:34:49.119331: 
2024-12-15 16:34:49.120425: Epoch 58
2024-12-15 16:34:49.121865: Current learning rate: 0.00644
2024-12-15 16:39:17.578941: Validation loss did not improve from -0.52312. Patience: 24/50
2024-12-15 16:39:17.580016: train_loss -0.7066
2024-12-15 16:39:17.581047: val_loss -0.4831
2024-12-15 16:39:17.581676: Pseudo dice [0.7122]
2024-12-15 16:39:17.582510: Epoch time: 268.46 s
2024-12-15 16:39:18.948603: 
2024-12-15 16:39:18.949678: Epoch 59
2024-12-15 16:39:18.950590: Current learning rate: 0.00638
2024-12-15 16:43:55.252719: Validation loss did not improve from -0.52312. Patience: 25/50
2024-12-15 16:43:55.253368: train_loss -0.7134
2024-12-15 16:43:55.254171: val_loss -0.4945
2024-12-15 16:43:55.254758: Pseudo dice [0.7172]
2024-12-15 16:43:55.255406: Epoch time: 276.31 s
2024-12-15 16:43:55.634458: Yayy! New best EMA pseudo Dice: 0.7047
2024-12-15 16:43:57.306593: 
2024-12-15 16:43:57.307400: Epoch 60
2024-12-15 16:43:57.308523: Current learning rate: 0.00631
2024-12-15 16:48:14.585478: Validation loss did not improve from -0.52312. Patience: 26/50
2024-12-15 16:48:14.586312: train_loss -0.7141
2024-12-15 16:48:14.587284: val_loss -0.4764
2024-12-15 16:48:14.588256: Pseudo dice [0.7129]
2024-12-15 16:48:14.589026: Epoch time: 257.28 s
2024-12-15 16:48:14.589936: Yayy! New best EMA pseudo Dice: 0.7055
2024-12-15 16:48:16.323865: 
2024-12-15 16:48:16.324688: Epoch 61
2024-12-15 16:48:16.325453: Current learning rate: 0.00625
2024-12-15 16:52:29.653065: Validation loss did not improve from -0.52312. Patience: 27/50
2024-12-15 16:52:29.653733: train_loss -0.7097
2024-12-15 16:52:29.654945: val_loss -0.482
2024-12-15 16:52:29.655845: Pseudo dice [0.7145]
2024-12-15 16:52:29.656749: Epoch time: 253.33 s
2024-12-15 16:52:29.657801: Yayy! New best EMA pseudo Dice: 0.7064
2024-12-15 16:52:31.985857: 
2024-12-15 16:52:31.986950: Epoch 62
2024-12-15 16:52:31.987889: Current learning rate: 0.00619
2024-12-15 16:56:52.466840: Validation loss did not improve from -0.52312. Patience: 28/50
2024-12-15 16:56:52.467511: train_loss -0.7161
2024-12-15 16:56:52.468529: val_loss -0.459
2024-12-15 16:56:52.469281: Pseudo dice [0.6991]
2024-12-15 16:56:52.470020: Epoch time: 260.48 s
2024-12-15 16:56:53.818851: 
2024-12-15 16:56:53.819919: Epoch 63
2024-12-15 16:56:53.820689: Current learning rate: 0.00612
2024-12-15 17:01:09.646502: Validation loss improved from -0.52312 to -0.52633! Patience: 28/50
2024-12-15 17:01:09.648022: train_loss -0.7149
2024-12-15 17:01:09.649050: val_loss -0.5263
2024-12-15 17:01:09.650096: Pseudo dice [0.7441]
2024-12-15 17:01:09.650708: Epoch time: 255.83 s
2024-12-15 17:01:09.651390: Yayy! New best EMA pseudo Dice: 0.7095
2024-12-15 17:01:11.410488: 
2024-12-15 17:01:11.411844: Epoch 64
2024-12-15 17:01:11.412738: Current learning rate: 0.00606
2024-12-15 17:05:44.836857: Validation loss did not improve from -0.52633. Patience: 1/50
2024-12-15 17:05:44.837508: train_loss -0.7205
2024-12-15 17:05:44.838344: val_loss -0.4496
2024-12-15 17:05:44.839014: Pseudo dice [0.683]
2024-12-15 17:05:44.839851: Epoch time: 273.43 s
2024-12-15 17:05:46.614159: 
2024-12-15 17:05:46.615023: Epoch 65
2024-12-15 17:05:46.615877: Current learning rate: 0.006
2024-12-15 17:10:15.318270: Validation loss did not improve from -0.52633. Patience: 2/50
2024-12-15 17:10:15.319322: train_loss -0.7231
2024-12-15 17:10:15.320303: val_loss -0.4362
2024-12-15 17:10:15.321197: Pseudo dice [0.6851]
2024-12-15 17:10:15.321953: Epoch time: 268.71 s
2024-12-15 17:10:16.688745: 
2024-12-15 17:10:16.690069: Epoch 66
2024-12-15 17:10:16.690991: Current learning rate: 0.00593
2024-12-15 17:14:19.889503: Validation loss did not improve from -0.52633. Patience: 3/50
2024-12-15 17:14:19.892648: train_loss -0.7201
2024-12-15 17:14:19.894588: val_loss -0.4954
2024-12-15 17:14:19.895458: Pseudo dice [0.7245]
2024-12-15 17:14:19.896405: Epoch time: 243.2 s
2024-12-15 17:14:21.281594: 
2024-12-15 17:14:21.282829: Epoch 67
2024-12-15 17:14:21.283641: Current learning rate: 0.00587
2024-12-15 17:18:06.436450: Validation loss did not improve from -0.52633. Patience: 4/50
2024-12-15 17:18:06.437066: train_loss -0.7193
2024-12-15 17:18:06.437706: val_loss -0.4578
2024-12-15 17:18:06.438326: Pseudo dice [0.7074]
2024-12-15 17:18:06.439033: Epoch time: 225.16 s
2024-12-15 17:18:07.805366: 
2024-12-15 17:18:07.806161: Epoch 68
2024-12-15 17:18:07.806835: Current learning rate: 0.00581
2024-12-15 17:22:00.868001: Validation loss did not improve from -0.52633. Patience: 5/50
2024-12-15 17:22:00.868656: train_loss -0.7231
2024-12-15 17:22:00.869423: val_loss -0.4603
2024-12-15 17:22:00.870132: Pseudo dice [0.7099]
2024-12-15 17:22:00.870887: Epoch time: 233.06 s
2024-12-15 17:22:02.227524: 
2024-12-15 17:22:02.228394: Epoch 69
2024-12-15 17:22:02.229177: Current learning rate: 0.00574
2024-12-15 17:26:48.527434: Validation loss did not improve from -0.52633. Patience: 6/50
2024-12-15 17:26:48.528684: train_loss -0.7259
2024-12-15 17:26:48.529451: val_loss -0.4742
2024-12-15 17:26:48.530258: Pseudo dice [0.7143]
2024-12-15 17:26:48.531619: Epoch time: 286.3 s
2024-12-15 17:26:50.343777: 
2024-12-15 17:26:50.344916: Epoch 70
2024-12-15 17:26:50.345684: Current learning rate: 0.00568
2024-12-15 17:31:20.156382: Validation loss did not improve from -0.52633. Patience: 7/50
2024-12-15 17:31:20.157262: train_loss -0.729
2024-12-15 17:31:20.158475: val_loss -0.4142
2024-12-15 17:31:20.159627: Pseudo dice [0.6812]
2024-12-15 17:31:20.161096: Epoch time: 269.81 s
2024-12-15 17:31:21.597351: 
2024-12-15 17:31:21.598516: Epoch 71
2024-12-15 17:31:21.599373: Current learning rate: 0.00562
2024-12-15 17:35:38.267523: Validation loss did not improve from -0.52633. Patience: 8/50
2024-12-15 17:35:38.268366: train_loss -0.7258
2024-12-15 17:35:38.269245: val_loss -0.4749
2024-12-15 17:35:38.270062: Pseudo dice [0.7083]
2024-12-15 17:35:38.270830: Epoch time: 256.67 s
2024-12-15 17:35:39.636693: 
2024-12-15 17:35:39.637511: Epoch 72
2024-12-15 17:35:39.638207: Current learning rate: 0.00555
2024-12-15 17:39:54.930412: Validation loss did not improve from -0.52633. Patience: 9/50
2024-12-15 17:39:54.931134: train_loss -0.729
2024-12-15 17:39:54.931875: val_loss -0.4971
2024-12-15 17:39:54.932526: Pseudo dice [0.7244]
2024-12-15 17:39:54.933259: Epoch time: 255.3 s
2024-12-15 17:39:56.744354: 
2024-12-15 17:39:56.745533: Epoch 73
2024-12-15 17:39:56.746503: Current learning rate: 0.00549
2024-12-15 17:44:14.413698: Validation loss did not improve from -0.52633. Patience: 10/50
2024-12-15 17:44:14.414374: train_loss -0.7276
2024-12-15 17:44:14.415117: val_loss -0.4691
2024-12-15 17:44:14.415863: Pseudo dice [0.7041]
2024-12-15 17:44:14.416658: Epoch time: 257.67 s
2024-12-15 17:44:15.884783: 
2024-12-15 17:44:15.885987: Epoch 74
2024-12-15 17:44:15.887216: Current learning rate: 0.00542
2024-12-15 17:48:26.359015: Validation loss did not improve from -0.52633. Patience: 11/50
2024-12-15 17:48:26.359677: train_loss -0.7282
2024-12-15 17:48:26.360491: val_loss -0.5058
2024-12-15 17:48:26.361129: Pseudo dice [0.7329]
2024-12-15 17:48:26.361795: Epoch time: 250.48 s
2024-12-15 17:48:26.824212: Yayy! New best EMA pseudo Dice: 0.7096
2024-12-15 17:48:28.693391: 
2024-12-15 17:48:28.694361: Epoch 75
2024-12-15 17:48:28.695019: Current learning rate: 0.00536
2024-12-15 17:52:53.294543: Validation loss did not improve from -0.52633. Patience: 12/50
2024-12-15 17:52:53.295267: train_loss -0.729
2024-12-15 17:52:53.296454: val_loss -0.4813
2024-12-15 17:52:53.297367: Pseudo dice [0.7172]
2024-12-15 17:52:53.298208: Epoch time: 264.6 s
2024-12-15 17:52:53.299016: Yayy! New best EMA pseudo Dice: 0.7104
2024-12-15 17:52:55.059382: 
2024-12-15 17:52:55.060421: Epoch 76
2024-12-15 17:52:55.061193: Current learning rate: 0.00529
2024-12-15 17:57:27.401026: Validation loss did not improve from -0.52633. Patience: 13/50
2024-12-15 17:57:27.401677: train_loss -0.7328
2024-12-15 17:57:27.402320: val_loss -0.5019
2024-12-15 17:57:27.402934: Pseudo dice [0.7273]
2024-12-15 17:57:27.403960: Epoch time: 272.34 s
2024-12-15 17:57:27.404550: Yayy! New best EMA pseudo Dice: 0.7121
2024-12-15 17:57:29.168219: 
2024-12-15 17:57:29.169141: Epoch 77
2024-12-15 17:57:29.169795: Current learning rate: 0.00523
2024-12-15 18:02:04.490834: Validation loss did not improve from -0.52633. Patience: 14/50
2024-12-15 18:02:04.491724: train_loss -0.737
2024-12-15 18:02:04.492438: val_loss -0.4912
2024-12-15 18:02:04.493120: Pseudo dice [0.7245]
2024-12-15 18:02:04.493849: Epoch time: 275.32 s
2024-12-15 18:02:04.494446: Yayy! New best EMA pseudo Dice: 0.7133
2024-12-15 18:02:06.249293: 
2024-12-15 18:02:06.250489: Epoch 78
2024-12-15 18:02:06.251209: Current learning rate: 0.00517
2024-12-15 18:06:43.708834: Validation loss did not improve from -0.52633. Patience: 15/50
2024-12-15 18:06:43.709536: train_loss -0.7385
2024-12-15 18:06:43.710166: val_loss -0.5138
2024-12-15 18:06:43.710755: Pseudo dice [0.736]
2024-12-15 18:06:43.711349: Epoch time: 277.46 s
2024-12-15 18:06:43.711999: Yayy! New best EMA pseudo Dice: 0.7156
2024-12-15 18:06:45.516400: 
2024-12-15 18:06:45.517357: Epoch 79
2024-12-15 18:06:45.518096: Current learning rate: 0.0051
2024-12-15 18:11:17.512497: Validation loss did not improve from -0.52633. Patience: 16/50
2024-12-15 18:11:17.513355: train_loss -0.7391
2024-12-15 18:11:17.514104: val_loss -0.4903
2024-12-15 18:11:17.514773: Pseudo dice [0.7272]
2024-12-15 18:11:17.515687: Epoch time: 272.0 s
2024-12-15 18:11:17.939122: Yayy! New best EMA pseudo Dice: 0.7167
2024-12-15 18:11:19.718320: 
2024-12-15 18:11:19.719606: Epoch 80
2024-12-15 18:11:19.720551: Current learning rate: 0.00504
2024-12-15 18:15:53.001874: Validation loss did not improve from -0.52633. Patience: 17/50
2024-12-15 18:15:53.002687: train_loss -0.7388
2024-12-15 18:15:53.003690: val_loss -0.4687
2024-12-15 18:15:53.004615: Pseudo dice [0.7076]
2024-12-15 18:15:53.005617: Epoch time: 273.29 s
2024-12-15 18:15:54.403628: 
2024-12-15 18:15:54.404821: Epoch 81
2024-12-15 18:15:54.405581: Current learning rate: 0.00497
2024-12-15 18:20:21.669420: Validation loss did not improve from -0.52633. Patience: 18/50
2024-12-15 18:20:21.671764: train_loss -0.7433
2024-12-15 18:20:21.673810: val_loss -0.4683
2024-12-15 18:20:21.674786: Pseudo dice [0.7073]
2024-12-15 18:20:21.675986: Epoch time: 267.27 s
2024-12-15 18:20:23.124952: 
2024-12-15 18:20:23.125849: Epoch 82
2024-12-15 18:20:23.126638: Current learning rate: 0.00491
2024-12-15 18:24:49.452789: Validation loss did not improve from -0.52633. Patience: 19/50
2024-12-15 18:24:49.453540: train_loss -0.7399
2024-12-15 18:24:49.454175: val_loss -0.4215
2024-12-15 18:24:49.454950: Pseudo dice [0.6908]
2024-12-15 18:24:49.455665: Epoch time: 266.33 s
2024-12-15 18:24:50.780997: 
2024-12-15 18:24:50.782262: Epoch 83
2024-12-15 18:24:50.783254: Current learning rate: 0.00484
2024-12-15 18:29:07.298882: Validation loss did not improve from -0.52633. Patience: 20/50
2024-12-15 18:29:07.300463: train_loss -0.7451
2024-12-15 18:29:07.301236: val_loss -0.5034
2024-12-15 18:29:07.301957: Pseudo dice [0.7227]
2024-12-15 18:29:07.302923: Epoch time: 256.52 s
2024-12-15 18:29:09.202132: 
2024-12-15 18:29:09.203144: Epoch 84
2024-12-15 18:29:09.203933: Current learning rate: 0.00478
2024-12-15 18:33:32.863248: Validation loss did not improve from -0.52633. Patience: 21/50
2024-12-15 18:33:32.863879: train_loss -0.7463
2024-12-15 18:33:32.864704: val_loss -0.4715
2024-12-15 18:33:32.865547: Pseudo dice [0.7076]
2024-12-15 18:33:32.866343: Epoch time: 263.66 s
2024-12-15 18:33:34.668239: 
2024-12-15 18:33:34.669069: Epoch 85
2024-12-15 18:33:34.669780: Current learning rate: 0.00471
2024-12-15 18:37:58.357264: Validation loss did not improve from -0.52633. Patience: 22/50
2024-12-15 18:37:58.358010: train_loss -0.7497
2024-12-15 18:37:58.358709: val_loss -0.4659
2024-12-15 18:37:58.359370: Pseudo dice [0.7066]
2024-12-15 18:37:58.360019: Epoch time: 263.69 s
2024-12-15 18:37:59.668531: 
2024-12-15 18:37:59.669497: Epoch 86
2024-12-15 18:37:59.670176: Current learning rate: 0.00465
2024-12-15 18:42:18.293730: Validation loss improved from -0.52633 to -0.53146! Patience: 22/50
2024-12-15 18:42:18.294555: train_loss -0.7466
2024-12-15 18:42:18.295433: val_loss -0.5315
2024-12-15 18:42:18.296157: Pseudo dice [0.739]
2024-12-15 18:42:18.296978: Epoch time: 258.63 s
2024-12-15 18:42:19.621872: 
2024-12-15 18:42:19.623258: Epoch 87
2024-12-15 18:42:19.624518: Current learning rate: 0.00458
2024-12-15 18:46:52.581826: Validation loss did not improve from -0.53146. Patience: 1/50
2024-12-15 18:46:52.582445: train_loss -0.7506
2024-12-15 18:46:52.583230: val_loss -0.4901
2024-12-15 18:46:52.583989: Pseudo dice [0.7203]
2024-12-15 18:46:52.584701: Epoch time: 272.96 s
2024-12-15 18:46:53.926692: 
2024-12-15 18:46:53.927736: Epoch 88
2024-12-15 18:46:53.928627: Current learning rate: 0.00452
2024-12-15 18:51:35.300772: Validation loss did not improve from -0.53146. Patience: 2/50
2024-12-15 18:51:35.301464: train_loss -0.75
2024-12-15 18:51:35.302592: val_loss -0.4691
2024-12-15 18:51:35.303491: Pseudo dice [0.7125]
2024-12-15 18:51:35.304477: Epoch time: 281.38 s
2024-12-15 18:51:36.628081: 
2024-12-15 18:51:36.629023: Epoch 89
2024-12-15 18:51:36.629854: Current learning rate: 0.00445
2024-12-15 18:56:01.411166: Validation loss did not improve from -0.53146. Patience: 3/50
2024-12-15 18:56:01.411860: train_loss -0.7512
2024-12-15 18:56:01.412565: val_loss -0.5159
2024-12-15 18:56:01.413209: Pseudo dice [0.7365]
2024-12-15 18:56:01.413960: Epoch time: 264.79 s
2024-12-15 18:56:01.842585: Yayy! New best EMA pseudo Dice: 0.7174
2024-12-15 18:56:03.595718: 
2024-12-15 18:56:03.597226: Epoch 90
2024-12-15 18:56:03.598216: Current learning rate: 0.00438
2024-12-15 18:59:55.138549: Validation loss did not improve from -0.53146. Patience: 4/50
2024-12-15 18:59:55.139550: train_loss -0.7516
2024-12-15 18:59:55.140387: val_loss -0.5047
2024-12-15 18:59:55.140954: Pseudo dice [0.7314]
2024-12-15 18:59:55.141517: Epoch time: 231.54 s
2024-12-15 18:59:55.142185: Yayy! New best EMA pseudo Dice: 0.7188
2024-12-15 18:59:56.847317: 
2024-12-15 18:59:56.848693: Epoch 91
2024-12-15 18:59:56.849502: Current learning rate: 0.00432
2024-12-15 19:03:33.928170: Validation loss did not improve from -0.53146. Patience: 5/50
2024-12-15 19:03:33.928775: train_loss -0.7547
2024-12-15 19:03:33.929604: val_loss -0.4814
2024-12-15 19:03:33.930246: Pseudo dice [0.709]
2024-12-15 19:03:33.930888: Epoch time: 217.08 s
2024-12-15 19:03:35.274122: 
2024-12-15 19:03:35.275027: Epoch 92
2024-12-15 19:03:35.275769: Current learning rate: 0.00425
2024-12-15 19:08:10.177855: Validation loss did not improve from -0.53146. Patience: 6/50
2024-12-15 19:08:10.178479: train_loss -0.7579
2024-12-15 19:08:10.179477: val_loss -0.4825
2024-12-15 19:08:10.180383: Pseudo dice [0.7103]
2024-12-15 19:08:10.181262: Epoch time: 274.91 s
2024-12-15 19:08:11.534189: 
2024-12-15 19:08:11.534950: Epoch 93
2024-12-15 19:08:11.535620: Current learning rate: 0.00419
2024-12-15 19:12:38.938180: Validation loss did not improve from -0.53146. Patience: 7/50
2024-12-15 19:12:38.938815: train_loss -0.7565
2024-12-15 19:12:38.939741: val_loss -0.4536
2024-12-15 19:12:38.940607: Pseudo dice [0.6955]
2024-12-15 19:12:38.941302: Epoch time: 267.41 s
2024-12-15 19:12:40.260344: 
2024-12-15 19:12:40.261312: Epoch 94
2024-12-15 19:12:40.262054: Current learning rate: 0.00412
2024-12-15 19:16:59.682048: Validation loss did not improve from -0.53146. Patience: 8/50
2024-12-15 19:16:59.682758: train_loss -0.7584
2024-12-15 19:16:59.683553: val_loss -0.4744
2024-12-15 19:16:59.684214: Pseudo dice [0.7093]
2024-12-15 19:16:59.684960: Epoch time: 259.42 s
2024-12-15 19:17:02.074709: 
2024-12-15 19:17:02.076061: Epoch 95
2024-12-15 19:17:02.077461: Current learning rate: 0.00405
2024-12-15 19:21:23.662050: Validation loss did not improve from -0.53146. Patience: 9/50
2024-12-15 19:21:23.662712: train_loss -0.7575
2024-12-15 19:21:23.663534: val_loss -0.4628
2024-12-15 19:21:23.664410: Pseudo dice [0.7196]
2024-12-15 19:21:23.665319: Epoch time: 261.59 s
2024-12-15 19:21:25.026620: 
2024-12-15 19:21:25.027972: Epoch 96
2024-12-15 19:21:25.029073: Current learning rate: 0.00399
2024-12-15 19:25:37.782951: Validation loss did not improve from -0.53146. Patience: 10/50
2024-12-15 19:25:37.785950: train_loss -0.7571
2024-12-15 19:25:37.787811: val_loss -0.4818
2024-12-15 19:25:37.788625: Pseudo dice [0.7187]
2024-12-15 19:25:37.789429: Epoch time: 252.76 s
2024-12-15 19:25:39.159394: 
2024-12-15 19:25:39.160283: Epoch 97
2024-12-15 19:25:39.161245: Current learning rate: 0.00392
2024-12-15 19:30:02.381726: Validation loss did not improve from -0.53146. Patience: 11/50
2024-12-15 19:30:02.382337: train_loss -0.762
2024-12-15 19:30:02.383278: val_loss -0.4361
2024-12-15 19:30:02.383988: Pseudo dice [0.7075]
2024-12-15 19:30:02.384705: Epoch time: 263.22 s
2024-12-15 19:30:03.725820: 
2024-12-15 19:30:03.726836: Epoch 98
2024-12-15 19:30:03.727515: Current learning rate: 0.00385
2024-12-15 19:34:32.043714: Validation loss did not improve from -0.53146. Patience: 12/50
2024-12-15 19:34:32.046334: train_loss -0.7642
2024-12-15 19:34:32.047210: val_loss -0.5048
2024-12-15 19:34:32.048017: Pseudo dice [0.7296]
2024-12-15 19:34:32.048983: Epoch time: 268.32 s
2024-12-15 19:34:33.436578: 
2024-12-15 19:34:33.438230: Epoch 99
2024-12-15 19:34:33.439352: Current learning rate: 0.00379
2024-12-15 19:39:12.721299: Validation loss did not improve from -0.53146. Patience: 13/50
2024-12-15 19:39:12.722057: train_loss -0.761
2024-12-15 19:39:12.722698: val_loss -0.4851
2024-12-15 19:39:12.723378: Pseudo dice [0.7109]
2024-12-15 19:39:12.724036: Epoch time: 279.29 s
2024-12-15 19:39:14.674626: 
2024-12-15 19:39:14.675614: Epoch 100
2024-12-15 19:39:14.676564: Current learning rate: 0.00372
2024-12-15 19:43:47.085921: Validation loss did not improve from -0.53146. Patience: 14/50
2024-12-15 19:43:47.086509: train_loss -0.7617
2024-12-15 19:43:47.087185: val_loss -0.4876
2024-12-15 19:43:47.087939: Pseudo dice [0.7217]
2024-12-15 19:43:47.088596: Epoch time: 272.41 s
2024-12-15 19:43:48.442846: 
2024-12-15 19:43:48.443569: Epoch 101
2024-12-15 19:43:48.444196: Current learning rate: 0.00365
2024-12-15 19:48:30.545579: Validation loss did not improve from -0.53146. Patience: 15/50
2024-12-15 19:48:30.546662: train_loss -0.763
2024-12-15 19:48:30.547542: val_loss -0.477
2024-12-15 19:48:30.548238: Pseudo dice [0.7209]
2024-12-15 19:48:30.548970: Epoch time: 282.11 s
2024-12-15 19:48:31.951794: 
2024-12-15 19:48:31.953379: Epoch 102
2024-12-15 19:48:31.954627: Current learning rate: 0.00359
2024-12-15 19:53:00.721250: Validation loss did not improve from -0.53146. Patience: 16/50
2024-12-15 19:53:00.722089: train_loss -0.7599
2024-12-15 19:53:00.722913: val_loss -0.476
2024-12-15 19:53:00.723634: Pseudo dice [0.7156]
2024-12-15 19:53:00.724297: Epoch time: 268.77 s
2024-12-15 19:53:02.052321: 
2024-12-15 19:53:02.053067: Epoch 103
2024-12-15 19:53:02.053704: Current learning rate: 0.00352
2024-12-15 19:57:25.427837: Validation loss did not improve from -0.53146. Patience: 17/50
2024-12-15 19:57:25.428467: train_loss -0.7639
2024-12-15 19:57:25.429251: val_loss -0.4887
2024-12-15 19:57:25.430259: Pseudo dice [0.7231]
2024-12-15 19:57:25.431042: Epoch time: 263.38 s
2024-12-15 19:57:26.776117: 
2024-12-15 19:57:26.777192: Epoch 104
2024-12-15 19:57:26.778008: Current learning rate: 0.00345
2024-12-15 20:01:48.545753: Validation loss did not improve from -0.53146. Patience: 18/50
2024-12-15 20:01:48.546390: train_loss -0.7659
2024-12-15 20:01:48.547281: val_loss -0.4689
2024-12-15 20:01:48.548093: Pseudo dice [0.7085]
2024-12-15 20:01:48.549083: Epoch time: 261.77 s
2024-12-15 20:01:50.297246: 
2024-12-15 20:01:50.298031: Epoch 105
2024-12-15 20:01:50.298764: Current learning rate: 0.00338
2024-12-15 20:06:15.357596: Validation loss did not improve from -0.53146. Patience: 19/50
2024-12-15 20:06:15.358305: train_loss -0.7671
2024-12-15 20:06:15.358942: val_loss -0.4297
2024-12-15 20:06:15.359578: Pseudo dice [0.6926]
2024-12-15 20:06:15.360222: Epoch time: 265.06 s
2024-12-15 20:06:16.727062: 
2024-12-15 20:06:16.727970: Epoch 106
2024-12-15 20:06:16.728687: Current learning rate: 0.00332
2024-12-15 20:10:39.767583: Validation loss did not improve from -0.53146. Patience: 20/50
2024-12-15 20:10:39.768258: train_loss -0.7676
2024-12-15 20:10:39.769002: val_loss -0.4761
2024-12-15 20:10:39.769751: Pseudo dice [0.7151]
2024-12-15 20:10:39.770489: Epoch time: 263.04 s
2024-12-15 20:10:41.118479: 
2024-12-15 20:10:41.119314: Epoch 107
2024-12-15 20:10:41.120004: Current learning rate: 0.00325
2024-12-15 20:14:58.248291: Validation loss did not improve from -0.53146. Patience: 21/50
2024-12-15 20:14:58.248941: train_loss -0.7679
2024-12-15 20:14:58.249598: val_loss -0.4523
2024-12-15 20:14:58.250173: Pseudo dice [0.7087]
2024-12-15 20:14:58.250830: Epoch time: 257.13 s
2024-12-15 20:14:59.611761: 
2024-12-15 20:14:59.612564: Epoch 108
2024-12-15 20:14:59.613424: Current learning rate: 0.00318
2024-12-15 20:19:29.544102: Validation loss did not improve from -0.53146. Patience: 22/50
2024-12-15 20:19:29.545000: train_loss -0.772
2024-12-15 20:19:29.545805: val_loss -0.511
2024-12-15 20:19:29.546608: Pseudo dice [0.7372]
2024-12-15 20:19:29.547353: Epoch time: 269.93 s
2024-12-15 20:19:30.896662: 
2024-12-15 20:19:30.898149: Epoch 109
2024-12-15 20:19:30.898961: Current learning rate: 0.00311
2024-12-15 20:24:06.348778: Validation loss did not improve from -0.53146. Patience: 23/50
2024-12-15 20:24:06.349798: train_loss -0.7733
2024-12-15 20:24:06.350802: val_loss -0.4728
2024-12-15 20:24:06.351752: Pseudo dice [0.7173]
2024-12-15 20:24:06.352497: Epoch time: 275.45 s
2024-12-15 20:24:08.056800: 
2024-12-15 20:24:08.057865: Epoch 110
2024-12-15 20:24:08.058486: Current learning rate: 0.00304
2024-12-15 20:28:56.117761: Validation loss did not improve from -0.53146. Patience: 24/50
2024-12-15 20:28:56.120438: train_loss -0.77
2024-12-15 20:28:56.122725: val_loss -0.472
2024-12-15 20:28:56.123708: Pseudo dice [0.7094]
2024-12-15 20:28:56.124833: Epoch time: 288.06 s
2024-12-15 20:28:57.530040: 
2024-12-15 20:28:57.531332: Epoch 111
2024-12-15 20:28:57.532148: Current learning rate: 0.00297
2024-12-15 20:33:11.738989: Validation loss did not improve from -0.53146. Patience: 25/50
2024-12-15 20:33:11.739814: train_loss -0.7729
2024-12-15 20:33:11.743829: val_loss -0.492
2024-12-15 20:33:11.744674: Pseudo dice [0.7198]
2024-12-15 20:33:11.745643: Epoch time: 254.21 s
2024-12-15 20:33:13.132740: 
2024-12-15 20:33:13.134205: Epoch 112
2024-12-15 20:33:13.135475: Current learning rate: 0.00291
2024-12-15 20:37:15.589895: Validation loss did not improve from -0.53146. Patience: 26/50
2024-12-15 20:37:15.590532: train_loss -0.7712
2024-12-15 20:37:15.591356: val_loss -0.4583
2024-12-15 20:37:15.592118: Pseudo dice [0.705]
2024-12-15 20:37:15.592919: Epoch time: 242.46 s
2024-12-15 20:37:16.947491: 
2024-12-15 20:37:16.948349: Epoch 113
2024-12-15 20:37:16.949106: Current learning rate: 0.00284
2024-12-15 20:40:59.509093: Validation loss did not improve from -0.53146. Patience: 27/50
2024-12-15 20:40:59.510932: train_loss -0.7796
2024-12-15 20:40:59.511932: val_loss -0.4544
2024-12-15 20:40:59.512717: Pseudo dice [0.7013]
2024-12-15 20:40:59.513580: Epoch time: 222.56 s
2024-12-15 20:41:00.886654: 
2024-12-15 20:41:00.887825: Epoch 114
2024-12-15 20:41:00.888735: Current learning rate: 0.00277
2024-12-15 20:45:48.648951: Validation loss did not improve from -0.53146. Patience: 28/50
2024-12-15 20:45:48.649964: train_loss -0.7725
2024-12-15 20:45:48.650883: val_loss -0.4838
2024-12-15 20:45:48.651527: Pseudo dice [0.7277]
2024-12-15 20:45:48.652246: Epoch time: 287.76 s
2024-12-15 20:45:50.362161: 
2024-12-15 20:45:50.363156: Epoch 115
2024-12-15 20:45:50.363931: Current learning rate: 0.0027
2024-12-15 20:50:26.736231: Validation loss did not improve from -0.53146. Patience: 29/50
2024-12-15 20:50:26.737104: train_loss -0.7718
2024-12-15 20:50:26.738194: val_loss -0.4456
2024-12-15 20:50:26.739402: Pseudo dice [0.7034]
2024-12-15 20:50:26.740531: Epoch time: 276.38 s
2024-12-15 20:50:28.121400: 
2024-12-15 20:50:28.122669: Epoch 116
2024-12-15 20:50:28.123409: Current learning rate: 0.00263
2024-12-15 20:54:55.229779: Validation loss did not improve from -0.53146. Patience: 30/50
2024-12-15 20:54:55.230683: train_loss -0.7737
2024-12-15 20:54:55.231381: val_loss -0.4604
2024-12-15 20:54:55.232112: Pseudo dice [0.7129]
2024-12-15 20:54:55.232745: Epoch time: 267.11 s
2024-12-15 20:54:57.249984: 
2024-12-15 20:54:57.251299: Epoch 117
2024-12-15 20:54:57.252166: Current learning rate: 0.00256
2024-12-15 20:59:37.028811: Validation loss did not improve from -0.53146. Patience: 31/50
2024-12-15 20:59:37.030088: train_loss -0.7772
2024-12-15 20:59:37.030955: val_loss -0.4831
2024-12-15 20:59:37.031972: Pseudo dice [0.7156]
2024-12-15 20:59:37.032788: Epoch time: 279.78 s
2024-12-15 20:59:38.413347: 
2024-12-15 20:59:38.414851: Epoch 118
2024-12-15 20:59:38.415613: Current learning rate: 0.00249
2024-12-15 21:04:05.264152: Validation loss did not improve from -0.53146. Patience: 32/50
2024-12-15 21:04:05.265156: train_loss -0.7769
2024-12-15 21:04:05.265920: val_loss -0.4562
2024-12-15 21:04:05.266571: Pseudo dice [0.7149]
2024-12-15 21:04:05.267266: Epoch time: 266.85 s
2024-12-15 21:04:06.652168: 
2024-12-15 21:04:06.653264: Epoch 119
2024-12-15 21:04:06.654023: Current learning rate: 0.00242
2024-12-15 21:08:38.722408: Validation loss did not improve from -0.53146. Patience: 33/50
2024-12-15 21:08:38.723255: train_loss -0.7843
2024-12-15 21:08:38.724017: val_loss -0.4901
2024-12-15 21:08:38.724683: Pseudo dice [0.722]
2024-12-15 21:08:38.725374: Epoch time: 272.07 s
2024-12-15 21:08:40.519386: 
2024-12-15 21:08:40.520614: Epoch 120
2024-12-15 21:08:40.521780: Current learning rate: 0.00235
2024-12-15 21:13:08.963650: Validation loss did not improve from -0.53146. Patience: 34/50
2024-12-15 21:13:08.964547: train_loss -0.7806
2024-12-15 21:13:08.965290: val_loss -0.4616
2024-12-15 21:13:08.966052: Pseudo dice [0.7099]
2024-12-15 21:13:08.966639: Epoch time: 268.45 s
2024-12-15 21:13:10.355773: 
2024-12-15 21:13:10.357039: Epoch 121
2024-12-15 21:13:10.357703: Current learning rate: 0.00228
2024-12-15 21:17:33.314868: Validation loss did not improve from -0.53146. Patience: 35/50
2024-12-15 21:17:33.315475: train_loss -0.7829
2024-12-15 21:17:33.316437: val_loss -0.4683
2024-12-15 21:17:33.317343: Pseudo dice [0.7193]
2024-12-15 21:17:33.318328: Epoch time: 262.96 s
2024-12-15 21:17:34.716993: 
2024-12-15 21:17:34.718359: Epoch 122
2024-12-15 21:17:34.719243: Current learning rate: 0.00221
2024-12-15 21:22:15.974718: Validation loss did not improve from -0.53146. Patience: 36/50
2024-12-15 21:22:15.976033: train_loss -0.7791
2024-12-15 21:22:15.976788: val_loss -0.4933
2024-12-15 21:22:15.977508: Pseudo dice [0.7265]
2024-12-15 21:22:15.978178: Epoch time: 281.26 s
2024-12-15 21:22:17.381840: 
2024-12-15 21:22:17.383206: Epoch 123
2024-12-15 21:22:17.384317: Current learning rate: 0.00214
2024-12-15 21:26:57.252898: Validation loss did not improve from -0.53146. Patience: 37/50
2024-12-15 21:26:57.253882: train_loss -0.7819
2024-12-15 21:26:57.254692: val_loss -0.4671
2024-12-15 21:26:57.255408: Pseudo dice [0.7049]
2024-12-15 21:26:57.256120: Epoch time: 279.87 s
2024-12-15 21:26:58.632564: 
2024-12-15 21:26:58.633826: Epoch 124
2024-12-15 21:26:58.634457: Current learning rate: 0.00207
2024-12-15 21:31:28.427750: Validation loss did not improve from -0.53146. Patience: 38/50
2024-12-15 21:31:28.428814: train_loss -0.7804
2024-12-15 21:31:28.429643: val_loss -0.4653
2024-12-15 21:31:28.430271: Pseudo dice [0.7122]
2024-12-15 21:31:28.430926: Epoch time: 269.8 s
2024-12-15 21:31:30.259037: 
2024-12-15 21:31:30.260283: Epoch 125
2024-12-15 21:31:30.261622: Current learning rate: 0.00199
2024-12-15 21:35:08.065483: Validation loss did not improve from -0.53146. Patience: 39/50
2024-12-15 21:35:08.068903: train_loss -0.7806
2024-12-15 21:35:08.071276: val_loss -0.4638
2024-12-15 21:35:08.072678: Pseudo dice [0.7184]
2024-12-15 21:35:08.073845: Epoch time: 217.81 s
2024-12-15 21:35:09.538882: 
2024-12-15 21:35:09.540080: Epoch 126
2024-12-15 21:35:09.540870: Current learning rate: 0.00192
2024-12-15 21:39:40.391764: Validation loss did not improve from -0.53146. Patience: 40/50
2024-12-15 21:39:40.392761: train_loss -0.7844
2024-12-15 21:39:40.393629: val_loss -0.4836
2024-12-15 21:39:40.394361: Pseudo dice [0.7248]
2024-12-15 21:39:40.394943: Epoch time: 270.86 s
2024-12-15 21:39:41.780813: 
2024-12-15 21:39:41.781994: Epoch 127
2024-12-15 21:39:41.782804: Current learning rate: 0.00185
2024-12-15 21:44:03.379997: Validation loss did not improve from -0.53146. Patience: 41/50
2024-12-15 21:44:03.382197: train_loss -0.7819
2024-12-15 21:44:03.383444: val_loss -0.4657
2024-12-15 21:44:03.384122: Pseudo dice [0.7061]
2024-12-15 21:44:03.384953: Epoch time: 261.6 s
2024-12-15 21:44:05.230715: 
2024-12-15 21:44:05.232179: Epoch 128
2024-12-15 21:44:05.232874: Current learning rate: 0.00178
2024-12-15 21:48:36.992933: Validation loss did not improve from -0.53146. Patience: 42/50
2024-12-15 21:48:36.993918: train_loss -0.7859
2024-12-15 21:48:36.994809: val_loss -0.4724
2024-12-15 21:48:36.995888: Pseudo dice [0.7065]
2024-12-15 21:48:36.996763: Epoch time: 271.76 s
2024-12-15 21:48:38.355882: 
2024-12-15 21:48:38.357287: Epoch 129
2024-12-15 21:48:38.358368: Current learning rate: 0.0017
2024-12-15 21:53:25.103141: Validation loss did not improve from -0.53146. Patience: 43/50
2024-12-15 21:53:25.104059: train_loss -0.7885
2024-12-15 21:53:25.104808: val_loss -0.4357
2024-12-15 21:53:25.105521: Pseudo dice [0.6988]
2024-12-15 21:53:25.106165: Epoch time: 286.75 s
2024-12-15 21:53:26.836588: 
2024-12-15 21:53:26.837696: Epoch 130
2024-12-15 21:53:26.838429: Current learning rate: 0.00163
2024-12-15 21:58:03.298354: Validation loss did not improve from -0.53146. Patience: 44/50
2024-12-15 21:58:03.299174: train_loss -0.7837
2024-12-15 21:58:03.300141: val_loss -0.4886
2024-12-15 21:58:03.301159: Pseudo dice [0.7225]
2024-12-15 21:58:03.302155: Epoch time: 276.46 s
2024-12-15 21:58:04.660953: 
2024-12-15 21:58:04.661973: Epoch 131
2024-12-15 21:58:04.662861: Current learning rate: 0.00156
2024-12-15 22:01:42.733935: Validation loss did not improve from -0.53146. Patience: 45/50
2024-12-15 22:01:42.734650: train_loss -0.7857
2024-12-15 22:01:42.735343: val_loss -0.4549
2024-12-15 22:01:42.736112: Pseudo dice [0.7126]
2024-12-15 22:01:42.736965: Epoch time: 218.07 s
2024-12-15 22:01:44.094929: 
2024-12-15 22:01:44.096357: Epoch 132
2024-12-15 22:01:44.097349: Current learning rate: 0.00148
2024-12-15 22:05:42.332291: Validation loss did not improve from -0.53146. Patience: 46/50
2024-12-15 22:05:42.333174: train_loss -0.7865
2024-12-15 22:05:42.333913: val_loss -0.4745
2024-12-15 22:05:42.334654: Pseudo dice [0.7217]
2024-12-15 22:05:42.335292: Epoch time: 238.24 s
2024-12-15 22:05:43.700543: 
2024-12-15 22:05:43.701733: Epoch 133
2024-12-15 22:05:43.702662: Current learning rate: 0.00141
2024-12-15 22:10:11.586099: Validation loss did not improve from -0.53146. Patience: 47/50
2024-12-15 22:10:11.586895: train_loss -0.7869
2024-12-15 22:10:11.587727: val_loss -0.4417
2024-12-15 22:10:11.588604: Pseudo dice [0.6935]
2024-12-15 22:10:11.589345: Epoch time: 267.89 s
2024-12-15 22:10:12.965517: 
2024-12-15 22:10:12.966478: Epoch 134
2024-12-15 22:10:12.967139: Current learning rate: 0.00133
2024-12-15 22:15:02.808075: Validation loss did not improve from -0.53146. Patience: 48/50
2024-12-15 22:15:02.808967: train_loss -0.7898
2024-12-15 22:15:02.809806: val_loss -0.4596
2024-12-15 22:15:02.810449: Pseudo dice [0.7107]
2024-12-15 22:15:02.811245: Epoch time: 289.84 s
2024-12-15 22:15:04.622566: 
2024-12-15 22:15:04.623941: Epoch 135
2024-12-15 22:15:04.624809: Current learning rate: 0.00126
2024-12-15 22:19:46.022599: Validation loss did not improve from -0.53146. Patience: 49/50
2024-12-15 22:19:46.023393: train_loss -0.7881
2024-12-15 22:19:46.024415: val_loss -0.4824
2024-12-15 22:19:46.025608: Pseudo dice [0.7176]
2024-12-15 22:19:46.026571: Epoch time: 281.4 s
2024-12-15 22:19:47.408558: 
2024-12-15 22:19:47.409674: Epoch 136
2024-12-15 22:19:47.410300: Current learning rate: 0.00118
2024-12-15 22:24:17.035287: Validation loss did not improve from -0.53146. Patience: 50/50
2024-12-15 22:24:17.036244: train_loss -0.7879
2024-12-15 22:24:17.037012: val_loss -0.472
2024-12-15 22:24:17.037732: Pseudo dice [0.7085]
2024-12-15 22:24:17.038469: Epoch time: 269.63 s
2024-12-15 22:24:18.428986: 
2024-12-15 22:24:18.430440: Epoch 137
2024-12-15 22:24:18.431207: Current learning rate: 0.00111
2024-12-15 22:28:56.425894: Validation loss did not improve from -0.53146. Patience: 51/50
2024-12-15 22:28:56.426555: train_loss -0.79
2024-12-15 22:28:56.427423: val_loss -0.4435
2024-12-15 22:28:56.428055: Pseudo dice [0.7002]
2024-12-15 22:28:56.428698: Epoch time: 278.0 s
2024-12-15 22:28:57.813731: 
2024-12-15 22:28:57.814618: Epoch 138
2024-12-15 22:28:57.815298: Current learning rate: 0.00103
2024-12-15 22:33:13.293998: Validation loss did not improve from -0.53146. Patience: 52/50
2024-12-15 22:33:13.294676: train_loss -0.7921
2024-12-15 22:33:13.295382: val_loss -0.4843
2024-12-15 22:33:13.296072: Pseudo dice [0.7275]
2024-12-15 22:33:13.296778: Epoch time: 255.48 s
2024-12-15 22:33:15.083345: 
2024-12-15 22:33:15.084447: Epoch 139
2024-12-15 22:33:15.085177: Current learning rate: 0.00095
2024-12-15 22:37:05.673261: Validation loss did not improve from -0.53146. Patience: 53/50
2024-12-15 22:37:05.674169: train_loss -0.7922
2024-12-15 22:37:05.674842: val_loss -0.4447
2024-12-15 22:37:05.675564: Pseudo dice [0.6947]
2024-12-15 22:37:05.676200: Epoch time: 230.59 s
2024-12-15 22:37:07.480824: 
2024-12-15 22:37:07.481971: Epoch 140
2024-12-15 22:37:07.482753: Current learning rate: 0.00087
2024-12-15 22:41:59.961352: Validation loss did not improve from -0.53146. Patience: 54/50
2024-12-15 22:41:59.963597: train_loss -0.7916
2024-12-15 22:41:59.965592: val_loss -0.4745
2024-12-15 22:41:59.966331: Pseudo dice [0.7189]
2024-12-15 22:41:59.967145: Epoch time: 292.48 s
2024-12-15 22:42:01.418083: 
2024-12-15 22:42:01.418855: Epoch 141
2024-12-15 22:42:01.419569: Current learning rate: 0.00079
2024-12-15 22:46:41.701753: Validation loss did not improve from -0.53146. Patience: 55/50
2024-12-15 22:46:41.702580: train_loss -0.7907
2024-12-15 22:46:41.703236: val_loss -0.4549
2024-12-15 22:46:41.703893: Pseudo dice [0.7091]
2024-12-15 22:46:41.704649: Epoch time: 280.29 s
2024-12-15 22:46:43.082869: 
2024-12-15 22:46:43.084144: Epoch 142
2024-12-15 22:46:43.085013: Current learning rate: 0.00071
2024-12-15 22:51:10.036407: Validation loss did not improve from -0.53146. Patience: 56/50
2024-12-15 22:51:10.038025: train_loss -0.7926
2024-12-15 22:51:10.038897: val_loss -0.4506
2024-12-15 22:51:10.039582: Pseudo dice [0.7078]
2024-12-15 22:51:10.040522: Epoch time: 266.96 s
2024-12-15 22:51:11.432757: 
2024-12-15 22:51:11.434074: Epoch 143
2024-12-15 22:51:11.434743: Current learning rate: 0.00063
2024-12-15 22:55:51.495181: Validation loss did not improve from -0.53146. Patience: 57/50
2024-12-15 22:55:51.496025: train_loss -0.7925
2024-12-15 22:55:51.496741: val_loss -0.466
2024-12-15 22:55:51.497319: Pseudo dice [0.7175]
2024-12-15 22:55:51.497920: Epoch time: 280.06 s
2024-12-15 22:55:52.904111: 
2024-12-15 22:55:52.905570: Epoch 144
2024-12-15 22:55:52.906362: Current learning rate: 0.00055
2024-12-15 23:00:27.613153: Validation loss did not improve from -0.53146. Patience: 58/50
2024-12-15 23:00:27.613979: train_loss -0.7925
2024-12-15 23:00:27.615370: val_loss -0.4478
2024-12-15 23:00:27.616140: Pseudo dice [0.705]
2024-12-15 23:00:27.617071: Epoch time: 274.71 s
2024-12-15 23:00:29.415227: 
2024-12-15 23:00:29.416439: Epoch 145
2024-12-15 23:00:29.417182: Current learning rate: 0.00047
2024-12-15 23:04:52.904645: Validation loss did not improve from -0.53146. Patience: 59/50
2024-12-15 23:04:52.905898: train_loss -0.7959
2024-12-15 23:04:52.906690: val_loss -0.4426
2024-12-15 23:04:52.907434: Pseudo dice [0.7041]
2024-12-15 23:04:52.908180: Epoch time: 263.49 s
2024-12-15 23:04:54.320090: 
2024-12-15 23:04:54.321147: Epoch 146
2024-12-15 23:04:54.322019: Current learning rate: 0.00038
2024-12-15 23:09:23.173625: Validation loss did not improve from -0.53146. Patience: 60/50
2024-12-15 23:09:23.174881: train_loss -0.7929
2024-12-15 23:09:23.175737: val_loss -0.4628
2024-12-15 23:09:23.176971: Pseudo dice [0.7107]
2024-12-15 23:09:23.178012: Epoch time: 268.86 s
2024-12-15 23:09:24.571654: 
2024-12-15 23:09:24.572922: Epoch 147
2024-12-15 23:09:24.573601: Current learning rate: 0.0003
2024-12-15 23:14:04.564808: Validation loss did not improve from -0.53146. Patience: 61/50
2024-12-15 23:14:04.565722: train_loss -0.7931
2024-12-15 23:14:04.566450: val_loss -0.4503
2024-12-15 23:14:04.567242: Pseudo dice [0.7127]
2024-12-15 23:14:04.567948: Epoch time: 280.0 s
2024-12-15 23:14:05.990918: 
2024-12-15 23:14:05.992010: Epoch 148
2024-12-15 23:14:05.993105: Current learning rate: 0.00021
2024-12-15 23:18:46.948363: Validation loss did not improve from -0.53146. Patience: 62/50
2024-12-15 23:18:46.949281: train_loss -0.797
2024-12-15 23:18:46.950247: val_loss -0.4716
2024-12-15 23:18:46.951123: Pseudo dice [0.721]
2024-12-15 23:18:46.951885: Epoch time: 280.96 s
2024-12-15 23:18:48.360652: 
2024-12-15 23:18:48.362157: Epoch 149
2024-12-15 23:18:48.363121: Current learning rate: 0.00011
2024-12-15 23:23:26.008904: Validation loss did not improve from -0.53146. Patience: 63/50
2024-12-15 23:23:26.009788: train_loss -0.7909
2024-12-15 23:23:26.010508: val_loss -0.4223
2024-12-15 23:23:26.011279: Pseudo dice [0.6966]
2024-12-15 23:23:26.011969: Epoch time: 277.65 s
2024-12-15 23:23:28.588672: Training done.
2024-12-15 23:23:28.806842: Using splits from existing split file: /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset307_Sohee_Calcium_OCT_CrossValidation/splits_final_80.json
2024-12-15 23:23:28.831577: The split file contains 5 splits.
2024-12-15 23:23:28.832947: Desired fold for training: 4
2024-12-15 23:23:28.834225: This split has 6 training and 5 validation cases.
2024-12-15 23:23:28.835694: predicting 101-044
2024-12-15 23:23:28.876430: 101-044, shape torch.Size([1, 404, 498, 498]), rank 0
2024-12-15 23:25:40.644699: predicting 401-004
2024-12-15 23:25:40.668511: 401-004, shape torch.Size([1, 375, 498, 498]), rank 0
2024-12-15 23:27:42.880076: predicting 701-013
2024-12-15 23:27:42.901441: 701-013, shape torch.Size([1, 375, 498, 498]), rank 0
2024-12-15 23:29:56.191938: predicting 704-003
2024-12-15 23:29:56.206171: 704-003, shape torch.Size([1, 375, 498, 498]), rank 0
2024-12-15 23:31:58.648413: predicting 706-005
2024-12-15 23:31:58.662459: 706-005, shape torch.Size([1, 375, 498, 498]), rank 0
2024-12-15 23:34:24.460676: Validation complete
2024-12-15 23:34:24.461450: Mean Validation Dice:  0.7025983125693044
