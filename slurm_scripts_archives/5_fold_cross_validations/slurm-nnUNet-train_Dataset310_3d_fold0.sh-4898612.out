/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/bin/python
Starting training with CONFIG=3d_32x160x128_b10, DATASET_ID=310, TRAINER=nnUNetTrainer
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/nnunetv2/training/nnUNetTrainer/nnUNetTrainer.py:169: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  self.grad_scaler = GradScaler() if self.device.type == 'cuda' else None

############################
INFO: You are using the old nnU-Net default plans. We have updated our recommendations. Please consider using those instead! Read more here: https://github.com/MIC-DKFZ/nnUNet/blob/master/documentation/resenc_presets.md
############################

Using device: cuda:0

#######################################################################
Please cite the following paper when using nnU-Net:
Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.
#######################################################################

2025-10-02 16:49:03.551175: do_dummy_2d_data_aug: True
2025-10-02 16:49:03.552105: Using splits from existing split file: /nfs/erelab001/shared/Computational_Group/Naravich/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset310_nnInteractive_Calcium_OCT_CrossValidation/splits_final.json
2025-10-02 16:49:03.563168: The split file contains 5 splits.
2025-10-02 16:49:03.563331: Desired fold for training: 0
2025-10-02 16:49:03.563467: This split has 6 training and 2 validation cases.
using pin_memory on device 0
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/fft_conv_pytorch/fft_conv.py:139: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:306.)
  output = output[crop_slices].contiguous()
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/fft_conv_pytorch/fft_conv.py:139: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:306.)
  output = output[crop_slices].contiguous()
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/fft_conv_pytorch/fft_conv.py:139: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:306.)
  output = output[crop_slices].contiguous()
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/fft_conv_pytorch/fft_conv.py:139: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:306.)
  output = output[crop_slices].contiguous()
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/fft_conv_pytorch/fft_conv.py:139: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:306.)
  output = output[crop_slices].contiguous()
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/fft_conv_pytorch/fft_conv.py:139: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:306.)
  output = output[crop_slices].contiguous()
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/fft_conv_pytorch/fft_conv.py:139: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:306.)
  output = output[crop_slices].contiguous()
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/fft_conv_pytorch/fft_conv.py:139: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:306.)
  output = output[crop_slices].contiguous()
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/fft_conv_pytorch/fft_conv.py:139: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:306.)
  output = output[crop_slices].contiguous()
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/fft_conv_pytorch/fft_conv.py:139: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:306.)
  output = output[crop_slices].contiguous()
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/fft_conv_pytorch/fft_conv.py:139: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:306.)
  output = output[crop_slices].contiguous()
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/fft_conv_pytorch/fft_conv.py:139: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:306.)
  output = output[crop_slices].contiguous()
using pin_memory on device 0
2025-10-02 16:49:55.101938: Using torch.compile...

This is the configuration used by this training:
Configuration name: 3d_32x160x128_b10
 {'data_identifier': 'nnUNetPlans_3d_fullres', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 10, 'patch_size': [32, 160, 128], 'median_image_size_in_voxels': [375.0, 498.0, 498.0], 'spacing': [1.0, 1.0, 1.0], 'normalization_schemes': ['ZScoreNormalization'], 'use_mask_for_norm': [False], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'architecture': {'network_class_name': 'dynamic_network_architectures.architectures.unet.PlainConvUNet', 'arch_kwargs': {'n_stages': 6, 'features_per_stage': [32, 64, 128, 256, 320, 320], 'conv_op': 'torch.nn.modules.conv.Conv3d', 'kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]], 'strides': [[1, 1, 1], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [1, 2, 2]], 'n_conv_per_stage': [2, 2, 2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2, 2, 2], 'conv_bias': True, 'norm_op': 'torch.nn.modules.instancenorm.InstanceNorm3d', 'norm_op_kwargs': {'eps': 1e-05, 'affine': True}, 'dropout_op': None, 'dropout_op_kwargs': None, 'nonlin': 'torch.nn.LeakyReLU', 'nonlin_kwargs': {'inplace': True}, 'deep_supervision': True}, '_kw_requires_import': ['conv_op', 'norm_op', 'dropout_op', 'nonlin']}, 'batch_dice': True, 'inherits_from': '3d_fullres'} 

These are the global plan.json settings:
 {'dataset_name': 'Dataset310_nnInteractive_Calcium_OCT_CrossValidation', 'plans_name': 'nnUNetPlans', 'original_median_spacing_after_transp': [1.0, 1.0, 1.0], 'original_median_shape_after_transp': [375, 498, 498], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 0.4977976381778717, 'mean': 0.1394134759902954, 'median': 0.09849607944488525, 'min': 0.0, 'percentile_00_5': 0.015305490233004093, 'percentile_99_5': 0.4977976381778717, 'std': 0.121165432035923}}} 

2025-10-02 16:50:06.685570: unpacking dataset...
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/torch/_inductor/lowering.py:7095: UserWarning: 
Online softmax is disabled on the fly since Inductor decides to
split the reduction. Cut an issue to PyTorch if this is an
important use case and you want to speed it up with online
softmax.

  warnings.warn(
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/torch/_inductor/lowering.py:7095: UserWarning: 
Online softmax is disabled on the fly since Inductor decides to
split the reduction. Cut an issue to PyTorch if this is an
important use case and you want to speed it up with online
softmax.

  warnings.warn(
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/torch/_inductor/lowering.py:7095: UserWarning: 
Online softmax is disabled on the fly since Inductor decides to
split the reduction. Cut an issue to PyTorch if this is an
important use case and you want to speed it up with online
softmax.

  warnings.warn(
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/torch/_inductor/lowering.py:7095: UserWarning: 
Online softmax is disabled on the fly since Inductor decides to
split the reduction. Cut an issue to PyTorch if this is an
important use case and you want to speed it up with online
softmax.

  warnings.warn(
2025-10-02 16:50:22.881041: unpacking done...
2025-10-02 16:50:22.883238: Unable to plot network architecture: nnUNet_compile is enabled!
2025-10-02 16:50:22.963584: 
2025-10-02 16:50:22.963734: Epoch 0
2025-10-02 16:50:22.963922: Current learning rate: 0.01
2025-10-02 16:51:47.525970: Validation loss improved from 1000.00000 to -0.06078! Patience: 0/50
2025-10-02 16:51:47.526552: train_loss -0.086
2025-10-02 16:51:47.526791: val_loss -0.0608
2025-10-02 16:51:47.527015: Pseudo dice [np.float32(0.5469)]
2025-10-02 16:51:47.527251: Epoch time: 84.56 s
2025-10-02 16:51:47.527391: Yayy! New best EMA pseudo Dice: 0.5468999743461609
2025-10-02 16:51:48.578370: 
2025-10-02 16:51:48.578571: Epoch 1
2025-10-02 16:51:48.578714: Current learning rate: 0.00994
2025-10-02 16:52:34.429488: Validation loss improved from -0.06078 to -0.14776! Patience: 0/50
2025-10-02 16:52:34.429972: train_loss -0.2599
2025-10-02 16:52:34.430210: val_loss -0.1478
2025-10-02 16:52:34.430468: Pseudo dice [np.float32(0.5907)]
2025-10-02 16:52:34.430778: Epoch time: 45.85 s
2025-10-02 16:52:34.431046: Yayy! New best EMA pseudo Dice: 0.5511999726295471
2025-10-02 16:52:35.492252: 
2025-10-02 16:52:35.492573: Epoch 2
2025-10-02 16:52:35.492764: Current learning rate: 0.00988
2025-10-02 16:53:21.356021: Validation loss improved from -0.14776 to -0.21962! Patience: 0/50
2025-10-02 16:53:21.356519: train_loss -0.2775
2025-10-02 16:53:21.356680: val_loss -0.2196
2025-10-02 16:53:21.356803: Pseudo dice [np.float32(0.5839)]
2025-10-02 16:53:21.356956: Epoch time: 45.86 s
2025-10-02 16:53:21.357083: Yayy! New best EMA pseudo Dice: 0.5544999837875366
2025-10-02 16:53:22.411828: 
2025-10-02 16:53:22.412148: Epoch 3
2025-10-02 16:53:22.412323: Current learning rate: 0.00982
2025-10-02 16:54:08.345934: Validation loss improved from -0.21962 to -0.22056! Patience: 0/50
2025-10-02 16:54:08.346361: train_loss -0.3213
2025-10-02 16:54:08.346507: val_loss -0.2206
2025-10-02 16:54:08.346627: Pseudo dice [np.float32(0.6213)]
2025-10-02 16:54:08.346798: Epoch time: 45.94 s
2025-10-02 16:54:08.346921: Yayy! New best EMA pseudo Dice: 0.5612000226974487
2025-10-02 16:54:09.389766: 
2025-10-02 16:54:09.390079: Epoch 4
2025-10-02 16:54:09.390257: Current learning rate: 0.00976
2025-10-02 16:54:55.290867: Validation loss improved from -0.22056 to -0.23486! Patience: 0/50
2025-10-02 16:54:55.291532: train_loss -0.3649
2025-10-02 16:54:55.291719: val_loss -0.2349
2025-10-02 16:54:55.291914: Pseudo dice [np.float32(0.6175)]
2025-10-02 16:54:55.292175: Epoch time: 45.9 s
2025-10-02 16:54:55.692090: Yayy! New best EMA pseudo Dice: 0.5667999982833862
2025-10-02 16:54:56.787325: 
2025-10-02 16:54:56.787603: Epoch 5
2025-10-02 16:54:56.787764: Current learning rate: 0.0097
2025-10-02 16:55:42.690474: Validation loss improved from -0.23486 to -0.26123! Patience: 0/50
2025-10-02 16:55:42.690927: train_loss -0.3835
2025-10-02 16:55:42.691107: val_loss -0.2612
2025-10-02 16:55:42.691291: Pseudo dice [np.float32(0.6272)]
2025-10-02 16:55:42.691484: Epoch time: 45.9 s
2025-10-02 16:55:42.691672: Yayy! New best EMA pseudo Dice: 0.5728999972343445
2025-10-02 16:55:43.767564: 
2025-10-02 16:55:43.767887: Epoch 6
2025-10-02 16:55:43.768080: Current learning rate: 0.00964
2025-10-02 16:56:29.633198: Validation loss improved from -0.26123 to -0.28685! Patience: 0/50
2025-10-02 16:56:29.634494: train_loss -0.4051
2025-10-02 16:56:29.634892: val_loss -0.2869
2025-10-02 16:56:29.635251: Pseudo dice [np.float32(0.6662)]
2025-10-02 16:56:29.635615: Epoch time: 45.87 s
2025-10-02 16:56:29.635921: Yayy! New best EMA pseudo Dice: 0.5821999907493591
2025-10-02 16:56:30.720389: 
2025-10-02 16:56:30.721112: Epoch 7
2025-10-02 16:56:30.721688: Current learning rate: 0.00958
2025-10-02 16:57:16.596116: Validation loss did not improve from -0.28685. Patience: 1/50
2025-10-02 16:57:16.596664: train_loss -0.4322
2025-10-02 16:57:16.597037: val_loss -0.2788
2025-10-02 16:57:16.597416: Pseudo dice [np.float32(0.6664)]
2025-10-02 16:57:16.597777: Epoch time: 45.88 s
2025-10-02 16:57:16.598130: Yayy! New best EMA pseudo Dice: 0.5906000137329102
2025-10-02 16:57:17.690398: 
2025-10-02 16:57:17.690790: Epoch 8
2025-10-02 16:57:17.691043: Current learning rate: 0.00952
2025-10-02 16:58:03.640972: Validation loss improved from -0.28685 to -0.31273! Patience: 1/50
2025-10-02 16:58:03.642327: train_loss -0.437
2025-10-02 16:58:03.642698: val_loss -0.3127
2025-10-02 16:58:03.643126: Pseudo dice [np.float32(0.6669)]
2025-10-02 16:58:03.643521: Epoch time: 45.95 s
2025-10-02 16:58:03.643910: Yayy! New best EMA pseudo Dice: 0.5982999801635742
2025-10-02 16:58:04.741458: 
2025-10-02 16:58:04.741772: Epoch 9
2025-10-02 16:58:04.741959: Current learning rate: 0.00946
2025-10-02 16:58:50.659291: Validation loss did not improve from -0.31273. Patience: 1/50
2025-10-02 16:58:50.659694: train_loss -0.4239
2025-10-02 16:58:50.659835: val_loss -0.297
2025-10-02 16:58:50.659967: Pseudo dice [np.float32(0.6614)]
2025-10-02 16:58:50.660141: Epoch time: 45.92 s
2025-10-02 16:58:51.133825: Yayy! New best EMA pseudo Dice: 0.6046000123023987
2025-10-02 16:58:52.201297: 
2025-10-02 16:58:52.201607: Epoch 10
2025-10-02 16:58:52.201835: Current learning rate: 0.0094
2025-10-02 16:59:38.146943: Validation loss improved from -0.31273 to -0.32581! Patience: 1/50
2025-10-02 16:59:38.148161: train_loss -0.458
2025-10-02 16:59:38.148757: val_loss -0.3258
2025-10-02 16:59:38.149441: Pseudo dice [np.float32(0.685)]
2025-10-02 16:59:38.150156: Epoch time: 45.95 s
2025-10-02 16:59:38.150731: Yayy! New best EMA pseudo Dice: 0.6126000285148621
2025-10-02 16:59:39.245546: 
2025-10-02 16:59:39.245812: Epoch 11
2025-10-02 16:59:39.245997: Current learning rate: 0.00934
2025-10-02 17:00:25.183868: Validation loss did not improve from -0.32581. Patience: 1/50
2025-10-02 17:00:25.184410: train_loss -0.4736
2025-10-02 17:00:25.184815: val_loss -0.3149
2025-10-02 17:00:25.185195: Pseudo dice [np.float32(0.6822)]
2025-10-02 17:00:25.185531: Epoch time: 45.94 s
2025-10-02 17:00:25.185838: Yayy! New best EMA pseudo Dice: 0.6195999979972839
2025-10-02 17:00:26.603526: 
2025-10-02 17:00:26.603794: Epoch 12
2025-10-02 17:00:26.604019: Current learning rate: 0.00928
2025-10-02 17:01:12.603439: Validation loss did not improve from -0.32581. Patience: 2/50
2025-10-02 17:01:12.604507: train_loss -0.4559
2025-10-02 17:01:12.604925: val_loss -0.3106
2025-10-02 17:01:12.605325: Pseudo dice [np.float32(0.6596)]
2025-10-02 17:01:12.605723: Epoch time: 46.0 s
2025-10-02 17:01:12.606068: Yayy! New best EMA pseudo Dice: 0.6236000061035156
2025-10-02 17:01:13.698464: 
2025-10-02 17:01:13.698786: Epoch 13
2025-10-02 17:01:13.698948: Current learning rate: 0.00922
2025-10-02 17:01:59.645231: Validation loss improved from -0.32581 to -0.37774! Patience: 2/50
2025-10-02 17:01:59.645716: train_loss -0.4751
2025-10-02 17:01:59.645886: val_loss -0.3777
2025-10-02 17:01:59.646052: Pseudo dice [np.float32(0.7003)]
2025-10-02 17:01:59.646255: Epoch time: 45.95 s
2025-10-02 17:01:59.646403: Yayy! New best EMA pseudo Dice: 0.6312999725341797
2025-10-02 17:02:00.709423: 
2025-10-02 17:02:00.709706: Epoch 14
2025-10-02 17:02:00.709920: Current learning rate: 0.00916
2025-10-02 17:02:46.641076: Validation loss did not improve from -0.37774. Patience: 1/50
2025-10-02 17:02:46.642063: train_loss -0.4933
2025-10-02 17:02:46.642418: val_loss -0.2932
2025-10-02 17:02:46.642743: Pseudo dice [np.float32(0.6554)]
2025-10-02 17:02:46.643071: Epoch time: 45.93 s
2025-10-02 17:02:47.088214: Yayy! New best EMA pseudo Dice: 0.6337000131607056
2025-10-02 17:02:48.124597: 
2025-10-02 17:02:48.124926: Epoch 15
2025-10-02 17:02:48.125151: Current learning rate: 0.0091
2025-10-02 17:03:34.065836: Validation loss did not improve from -0.37774. Patience: 2/50
2025-10-02 17:03:34.066276: train_loss -0.4942
2025-10-02 17:03:34.066469: val_loss -0.335
2025-10-02 17:03:34.066689: Pseudo dice [np.float32(0.6988)]
2025-10-02 17:03:34.066892: Epoch time: 45.94 s
2025-10-02 17:03:34.067059: Yayy! New best EMA pseudo Dice: 0.6402000188827515
2025-10-02 17:03:35.141501: 
2025-10-02 17:03:35.141816: Epoch 16
2025-10-02 17:03:35.142021: Current learning rate: 0.00903
2025-10-02 17:04:21.125512: Validation loss did not improve from -0.37774. Patience: 3/50
2025-10-02 17:04:21.126664: train_loss -0.5055
2025-10-02 17:04:21.127019: val_loss -0.3672
2025-10-02 17:04:21.127359: Pseudo dice [np.float32(0.7006)]
2025-10-02 17:04:21.127721: Epoch time: 45.99 s
2025-10-02 17:04:21.128096: Yayy! New best EMA pseudo Dice: 0.6462000012397766
2025-10-02 17:04:22.180092: 
2025-10-02 17:04:22.180393: Epoch 17
2025-10-02 17:04:22.180660: Current learning rate: 0.00897
2025-10-02 17:05:08.163307: Validation loss did not improve from -0.37774. Patience: 4/50
2025-10-02 17:05:08.163751: train_loss -0.5009
2025-10-02 17:05:08.163927: val_loss -0.3555
2025-10-02 17:05:08.164105: Pseudo dice [np.float32(0.6908)]
2025-10-02 17:05:08.164334: Epoch time: 45.98 s
2025-10-02 17:05:08.164570: Yayy! New best EMA pseudo Dice: 0.6506999731063843
2025-10-02 17:05:09.327955: 
2025-10-02 17:05:09.328326: Epoch 18
2025-10-02 17:05:09.328492: Current learning rate: 0.00891
2025-10-02 17:05:55.293698: Validation loss did not improve from -0.37774. Patience: 5/50
2025-10-02 17:05:55.294812: train_loss -0.5144
2025-10-02 17:05:55.295133: val_loss -0.3267
2025-10-02 17:05:55.295444: Pseudo dice [np.float32(0.6751)]
2025-10-02 17:05:55.295765: Epoch time: 45.97 s
2025-10-02 17:05:55.296072: Yayy! New best EMA pseudo Dice: 0.6531000137329102
2025-10-02 17:05:56.399084: 
2025-10-02 17:05:56.399442: Epoch 19
2025-10-02 17:05:56.399647: Current learning rate: 0.00885
2025-10-02 17:06:42.392746: Validation loss improved from -0.37774 to -0.37971! Patience: 5/50
2025-10-02 17:06:42.393139: train_loss -0.5154
2025-10-02 17:06:42.393301: val_loss -0.3797
2025-10-02 17:06:42.393439: Pseudo dice [np.float32(0.7004)]
2025-10-02 17:06:42.393593: Epoch time: 45.99 s
2025-10-02 17:06:42.843675: Yayy! New best EMA pseudo Dice: 0.657800018787384
2025-10-02 17:06:43.919460: 
2025-10-02 17:06:43.919670: Epoch 20
2025-10-02 17:06:43.919824: Current learning rate: 0.00879
2025-10-02 17:07:29.859901: Validation loss improved from -0.37971 to -0.39400! Patience: 0/50
2025-10-02 17:07:29.860729: train_loss -0.5281
2025-10-02 17:07:29.860909: val_loss -0.394
2025-10-02 17:07:29.861125: Pseudo dice [np.float32(0.7093)]
2025-10-02 17:07:29.861405: Epoch time: 45.94 s
2025-10-02 17:07:29.861622: Yayy! New best EMA pseudo Dice: 0.6629999876022339
2025-10-02 17:07:30.931928: 
2025-10-02 17:07:30.932419: Epoch 21
2025-10-02 17:07:30.932779: Current learning rate: 0.00873
2025-10-02 17:08:16.895958: Validation loss did not improve from -0.39400. Patience: 1/50
2025-10-02 17:08:16.896639: train_loss -0.5264
2025-10-02 17:08:16.897062: val_loss -0.2933
2025-10-02 17:08:16.897501: Pseudo dice [np.float32(0.654)]
2025-10-02 17:08:16.897978: Epoch time: 45.97 s
2025-10-02 17:08:17.509645: 
2025-10-02 17:08:17.509999: Epoch 22
2025-10-02 17:08:17.510309: Current learning rate: 0.00867
2025-10-02 17:09:03.494443: Validation loss did not improve from -0.39400. Patience: 2/50
2025-10-02 17:09:03.494996: train_loss -0.5284
2025-10-02 17:09:03.495138: val_loss -0.2925
2025-10-02 17:09:03.495289: Pseudo dice [np.float32(0.6647)]
2025-10-02 17:09:03.495433: Epoch time: 45.99 s
2025-10-02 17:09:04.103582: 
2025-10-02 17:09:04.104006: Epoch 23
2025-10-02 17:09:04.104399: Current learning rate: 0.00861
2025-10-02 17:09:50.063398: Validation loss did not improve from -0.39400. Patience: 3/50
2025-10-02 17:09:50.064016: train_loss -0.5377
2025-10-02 17:09:50.064442: val_loss -0.3738
2025-10-02 17:09:50.064778: Pseudo dice [np.float32(0.6983)]
2025-10-02 17:09:50.065280: Epoch time: 45.96 s
2025-10-02 17:09:50.065676: Yayy! New best EMA pseudo Dice: 0.6658999919891357
2025-10-02 17:09:51.104174: 
2025-10-02 17:09:51.104387: Epoch 24
2025-10-02 17:09:51.104579: Current learning rate: 0.00855
2025-10-02 17:10:37.121411: Validation loss did not improve from -0.39400. Patience: 4/50
2025-10-02 17:10:37.122220: train_loss -0.5343
2025-10-02 17:10:37.122533: val_loss -0.3846
2025-10-02 17:10:37.122773: Pseudo dice [np.float32(0.7122)]
2025-10-02 17:10:37.123013: Epoch time: 46.02 s
2025-10-02 17:10:37.548908: Yayy! New best EMA pseudo Dice: 0.6705999970436096
2025-10-02 17:10:38.571265: 
2025-10-02 17:10:38.571779: Epoch 25
2025-10-02 17:10:38.572206: Current learning rate: 0.00849
2025-10-02 17:11:24.571747: Validation loss did not improve from -0.39400. Patience: 5/50
2025-10-02 17:11:24.572162: train_loss -0.5445
2025-10-02 17:11:24.572348: val_loss -0.3707
2025-10-02 17:11:24.572493: Pseudo dice [np.float32(0.7012)]
2025-10-02 17:11:24.572666: Epoch time: 46.0 s
2025-10-02 17:11:24.572790: Yayy! New best EMA pseudo Dice: 0.6736000180244446
2025-10-02 17:11:25.619741: 
2025-10-02 17:11:25.620052: Epoch 26
2025-10-02 17:11:25.620261: Current learning rate: 0.00843
2025-10-02 17:12:11.581637: Validation loss did not improve from -0.39400. Patience: 6/50
2025-10-02 17:12:11.582243: train_loss -0.5585
2025-10-02 17:12:11.582405: val_loss -0.3792
2025-10-02 17:12:11.582572: Pseudo dice [np.float32(0.7084)]
2025-10-02 17:12:11.582724: Epoch time: 45.96 s
2025-10-02 17:12:11.582888: Yayy! New best EMA pseudo Dice: 0.6771000027656555
2025-10-02 17:12:12.970733: 
2025-10-02 17:12:12.970950: Epoch 27
2025-10-02 17:12:12.971107: Current learning rate: 0.00836
2025-10-02 17:12:58.925351: Validation loss did not improve from -0.39400. Patience: 7/50
2025-10-02 17:12:58.925777: train_loss -0.5532
2025-10-02 17:12:58.925944: val_loss -0.3911
2025-10-02 17:12:58.926072: Pseudo dice [np.float32(0.7159)]
2025-10-02 17:12:58.926226: Epoch time: 45.96 s
2025-10-02 17:12:58.926352: Yayy! New best EMA pseudo Dice: 0.6809999942779541
2025-10-02 17:12:59.981565: 
2025-10-02 17:12:59.981883: Epoch 28
2025-10-02 17:12:59.982063: Current learning rate: 0.0083
2025-10-02 17:13:45.981369: Validation loss did not improve from -0.39400. Patience: 8/50
2025-10-02 17:13:45.982128: train_loss -0.552
2025-10-02 17:13:45.982379: val_loss -0.3846
2025-10-02 17:13:45.982562: Pseudo dice [np.float32(0.7277)]
2025-10-02 17:13:45.982816: Epoch time: 46.0 s
2025-10-02 17:13:45.983019: Yayy! New best EMA pseudo Dice: 0.685699999332428
2025-10-02 17:13:47.027678: 
2025-10-02 17:13:47.027973: Epoch 29
2025-10-02 17:13:47.028327: Current learning rate: 0.00824
2025-10-02 17:14:33.038007: Validation loss did not improve from -0.39400. Patience: 9/50
2025-10-02 17:14:33.038409: train_loss -0.5631
2025-10-02 17:14:33.038578: val_loss -0.3403
2025-10-02 17:14:33.038749: Pseudo dice [np.float32(0.6932)]
2025-10-02 17:14:33.038892: Epoch time: 46.01 s
2025-10-02 17:14:33.516462: Yayy! New best EMA pseudo Dice: 0.6863999962806702
2025-10-02 17:14:34.594067: 
2025-10-02 17:14:34.594552: Epoch 30
2025-10-02 17:14:34.594904: Current learning rate: 0.00818
2025-10-02 17:15:20.542913: Validation loss did not improve from -0.39400. Patience: 10/50
2025-10-02 17:15:20.543700: train_loss -0.5526
2025-10-02 17:15:20.543904: val_loss -0.3525
2025-10-02 17:15:20.544036: Pseudo dice [np.float32(0.6887)]
2025-10-02 17:15:20.544187: Epoch time: 45.95 s
2025-10-02 17:15:20.544386: Yayy! New best EMA pseudo Dice: 0.6866000294685364
2025-10-02 17:15:21.627695: 
2025-10-02 17:15:21.628068: Epoch 31
2025-10-02 17:15:21.628348: Current learning rate: 0.00812
2025-10-02 17:16:07.631458: Validation loss improved from -0.39400 to -0.39557! Patience: 10/50
2025-10-02 17:16:07.632171: train_loss -0.5639
2025-10-02 17:16:07.632648: val_loss -0.3956
2025-10-02 17:16:07.633078: Pseudo dice [np.float32(0.7188)]
2025-10-02 17:16:07.633535: Epoch time: 46.01 s
2025-10-02 17:16:07.633962: Yayy! New best EMA pseudo Dice: 0.6898999810218811
2025-10-02 17:16:08.706275: 
2025-10-02 17:16:08.706716: Epoch 32
2025-10-02 17:16:08.707103: Current learning rate: 0.00806
2025-10-02 17:16:54.660056: Validation loss did not improve from -0.39557. Patience: 1/50
2025-10-02 17:16:54.660658: train_loss -0.5759
2025-10-02 17:16:54.660821: val_loss -0.3814
2025-10-02 17:16:54.660955: Pseudo dice [np.float32(0.7109)]
2025-10-02 17:16:54.661106: Epoch time: 45.96 s
2025-10-02 17:16:54.661239: Yayy! New best EMA pseudo Dice: 0.6919999718666077
2025-10-02 17:16:55.738586: 
2025-10-02 17:16:55.739141: Epoch 33
2025-10-02 17:16:55.739578: Current learning rate: 0.008
2025-10-02 17:17:41.747113: Validation loss did not improve from -0.39557. Patience: 2/50
2025-10-02 17:17:41.747644: train_loss -0.5689
2025-10-02 17:17:41.747861: val_loss -0.3655
2025-10-02 17:17:41.748103: Pseudo dice [np.float32(0.6959)]
2025-10-02 17:17:41.748290: Epoch time: 46.01 s
2025-10-02 17:17:41.748517: Yayy! New best EMA pseudo Dice: 0.6923999786376953
2025-10-02 17:17:42.827838: 
2025-10-02 17:17:42.828362: Epoch 34
2025-10-02 17:17:42.828766: Current learning rate: 0.00793
2025-10-02 17:18:28.791963: Validation loss did not improve from -0.39557. Patience: 3/50
2025-10-02 17:18:28.793032: train_loss -0.5706
2025-10-02 17:18:28.793438: val_loss -0.3578
2025-10-02 17:18:28.793727: Pseudo dice [np.float32(0.691)]
2025-10-02 17:18:28.794141: Epoch time: 45.97 s
2025-10-02 17:18:29.859451: 
2025-10-02 17:18:29.859913: Epoch 35
2025-10-02 17:18:29.860109: Current learning rate: 0.00787
2025-10-02 17:19:15.827793: Validation loss did not improve from -0.39557. Patience: 4/50
2025-10-02 17:19:15.828321: train_loss -0.5755
2025-10-02 17:19:15.828664: val_loss -0.3422
2025-10-02 17:19:15.829004: Pseudo dice [np.float32(0.6712)]
2025-10-02 17:19:15.829334: Epoch time: 45.97 s
2025-10-02 17:19:16.446489: 
2025-10-02 17:19:16.446697: Epoch 36
2025-10-02 17:19:16.446847: Current learning rate: 0.00781
2025-10-02 17:20:02.444476: Validation loss did not improve from -0.39557. Patience: 5/50
2025-10-02 17:20:02.445555: train_loss -0.5757
2025-10-02 17:20:02.445866: val_loss -0.3445
2025-10-02 17:20:02.446205: Pseudo dice [np.float32(0.6859)]
2025-10-02 17:20:02.446545: Epoch time: 46.0 s
2025-10-02 17:20:03.065203: 
2025-10-02 17:20:03.065649: Epoch 37
2025-10-02 17:20:03.066027: Current learning rate: 0.00775
2025-10-02 17:20:49.074315: Validation loss did not improve from -0.39557. Patience: 6/50
2025-10-02 17:20:49.074733: train_loss -0.5937
2025-10-02 17:20:49.074902: val_loss -0.3643
2025-10-02 17:20:49.075073: Pseudo dice [np.float32(0.7019)]
2025-10-02 17:20:49.075321: Epoch time: 46.01 s
2025-10-02 17:20:49.692399: 
2025-10-02 17:20:49.692814: Epoch 38
2025-10-02 17:20:49.693251: Current learning rate: 0.00769
2025-10-02 17:21:35.698161: Validation loss did not improve from -0.39557. Patience: 7/50
2025-10-02 17:21:35.698749: train_loss -0.599
2025-10-02 17:21:35.698993: val_loss -0.3892
2025-10-02 17:21:35.699160: Pseudo dice [np.float32(0.7157)]
2025-10-02 17:21:35.699382: Epoch time: 46.01 s
2025-10-02 17:21:35.699643: Yayy! New best EMA pseudo Dice: 0.6934000253677368
2025-10-02 17:21:36.760529: 
2025-10-02 17:21:36.760777: Epoch 39
2025-10-02 17:21:36.760938: Current learning rate: 0.00763
2025-10-02 17:22:22.736533: Validation loss did not improve from -0.39557. Patience: 8/50
2025-10-02 17:22:22.737111: train_loss -0.5862
2025-10-02 17:22:22.737446: val_loss -0.3317
2025-10-02 17:22:22.737648: Pseudo dice [np.float32(0.7027)]
2025-10-02 17:22:22.737875: Epoch time: 45.98 s
2025-10-02 17:22:23.194355: Yayy! New best EMA pseudo Dice: 0.6942999958992004
2025-10-02 17:22:24.255325: 
2025-10-02 17:22:24.255602: Epoch 40
2025-10-02 17:22:24.255758: Current learning rate: 0.00756
2025-10-02 17:23:10.205327: Validation loss did not improve from -0.39557. Patience: 9/50
2025-10-02 17:23:10.206506: train_loss -0.6003
2025-10-02 17:23:10.206902: val_loss -0.3688
2025-10-02 17:23:10.207329: Pseudo dice [np.float32(0.722)]
2025-10-02 17:23:10.207733: Epoch time: 45.95 s
2025-10-02 17:23:10.208064: Yayy! New best EMA pseudo Dice: 0.6970999836921692
2025-10-02 17:23:11.300920: 
2025-10-02 17:23:11.301267: Epoch 41
2025-10-02 17:23:11.301441: Current learning rate: 0.0075
2025-10-02 17:23:57.258205: Validation loss did not improve from -0.39557. Patience: 10/50
2025-10-02 17:23:57.258535: train_loss -0.5906
2025-10-02 17:23:57.258706: val_loss -0.3305
2025-10-02 17:23:57.258851: Pseudo dice [np.float32(0.6927)]
2025-10-02 17:23:57.259004: Epoch time: 45.96 s
2025-10-02 17:23:57.868272: 
2025-10-02 17:23:57.868489: Epoch 42
2025-10-02 17:23:57.868700: Current learning rate: 0.00744
2025-10-02 17:24:43.862682: Validation loss did not improve from -0.39557. Patience: 11/50
2025-10-02 17:24:43.863436: train_loss -0.609
2025-10-02 17:24:43.863683: val_loss -0.3029
2025-10-02 17:24:43.863923: Pseudo dice [np.float32(0.6744)]
2025-10-02 17:24:43.864174: Epoch time: 46.0 s
2025-10-02 17:24:44.803418: 
2025-10-02 17:24:44.803902: Epoch 43
2025-10-02 17:24:44.804253: Current learning rate: 0.00738
2025-10-02 17:25:30.839770: Validation loss did not improve from -0.39557. Patience: 12/50
2025-10-02 17:25:30.840140: train_loss -0.6152
2025-10-02 17:25:30.840304: val_loss -0.3356
2025-10-02 17:25:30.840427: Pseudo dice [np.float32(0.6933)]
2025-10-02 17:25:30.840555: Epoch time: 46.04 s
2025-10-02 17:25:31.444984: 
2025-10-02 17:25:31.445257: Epoch 44
2025-10-02 17:25:31.445513: Current learning rate: 0.00732
2025-10-02 17:26:17.477854: Validation loss did not improve from -0.39557. Patience: 13/50
2025-10-02 17:26:17.478452: train_loss -0.6069
2025-10-02 17:26:17.478615: val_loss -0.3854
2025-10-02 17:26:17.478755: Pseudo dice [np.float32(0.725)]
2025-10-02 17:26:17.478964: Epoch time: 46.03 s
2025-10-02 17:26:17.921962: Yayy! New best EMA pseudo Dice: 0.6973999738693237
2025-10-02 17:26:18.985324: 
2025-10-02 17:26:18.985740: Epoch 45
2025-10-02 17:26:18.985999: Current learning rate: 0.00725
2025-10-02 17:27:05.034983: Validation loss did not improve from -0.39557. Patience: 14/50
2025-10-02 17:27:05.035446: train_loss -0.6272
2025-10-02 17:27:05.035609: val_loss -0.3372
2025-10-02 17:27:05.035747: Pseudo dice [np.float32(0.6739)]
2025-10-02 17:27:05.035931: Epoch time: 46.05 s
2025-10-02 17:27:05.645032: 
2025-10-02 17:27:05.645287: Epoch 46
2025-10-02 17:27:05.645438: Current learning rate: 0.00719
2025-10-02 17:27:51.665297: Validation loss did not improve from -0.39557. Patience: 15/50
2025-10-02 17:27:51.666466: train_loss -0.624
2025-10-02 17:27:51.666804: val_loss -0.3672
2025-10-02 17:27:51.667119: Pseudo dice [np.float32(0.7068)]
2025-10-02 17:27:51.667480: Epoch time: 46.02 s
2025-10-02 17:27:52.278732: 
2025-10-02 17:27:52.279180: Epoch 47
2025-10-02 17:27:52.279507: Current learning rate: 0.00713
2025-10-02 17:28:38.301230: Validation loss did not improve from -0.39557. Patience: 16/50
2025-10-02 17:28:38.301811: train_loss -0.6272
2025-10-02 17:28:38.302377: val_loss -0.3764
2025-10-02 17:28:38.302662: Pseudo dice [np.float32(0.7104)]
2025-10-02 17:28:38.303016: Epoch time: 46.02 s
2025-10-02 17:28:38.303393: Yayy! New best EMA pseudo Dice: 0.6976000070571899
2025-10-02 17:28:39.380041: 
2025-10-02 17:28:39.380375: Epoch 48
2025-10-02 17:28:39.380593: Current learning rate: 0.00707
2025-10-02 17:29:25.375638: Validation loss improved from -0.39557 to -0.40075! Patience: 16/50
2025-10-02 17:29:25.376290: train_loss -0.6246
2025-10-02 17:29:25.376473: val_loss -0.4007
2025-10-02 17:29:25.376627: Pseudo dice [np.float32(0.7242)]
2025-10-02 17:29:25.376806: Epoch time: 46.0 s
2025-10-02 17:29:25.376981: Yayy! New best EMA pseudo Dice: 0.7002999782562256
2025-10-02 17:29:26.497861: 
2025-10-02 17:29:26.498182: Epoch 49
2025-10-02 17:29:26.498376: Current learning rate: 0.007
2025-10-02 17:30:12.560296: Validation loss did not improve from -0.40075. Patience: 1/50
2025-10-02 17:30:12.560853: train_loss -0.6314
2025-10-02 17:30:12.561079: val_loss -0.3274
2025-10-02 17:30:12.561356: Pseudo dice [np.float32(0.7029)]
2025-10-02 17:30:12.561652: Epoch time: 46.06 s
2025-10-02 17:30:13.010977: Yayy! New best EMA pseudo Dice: 0.7005000114440918
2025-10-02 17:30:14.056938: 
2025-10-02 17:30:14.057195: Epoch 50
2025-10-02 17:30:14.057400: Current learning rate: 0.00694
2025-10-02 17:31:00.043712: Validation loss did not improve from -0.40075. Patience: 2/50
2025-10-02 17:31:00.044859: train_loss -0.6313
2025-10-02 17:31:00.045209: val_loss -0.3815
2025-10-02 17:31:00.045542: Pseudo dice [np.float32(0.714)]
2025-10-02 17:31:00.045944: Epoch time: 45.99 s
2025-10-02 17:31:00.046338: Yayy! New best EMA pseudo Dice: 0.7019000053405762
2025-10-02 17:31:01.115417: 
2025-10-02 17:31:01.115943: Epoch 51
2025-10-02 17:31:01.116245: Current learning rate: 0.00688
2025-10-02 17:31:47.164105: Validation loss did not improve from -0.40075. Patience: 3/50
2025-10-02 17:31:47.164556: train_loss -0.6345
2025-10-02 17:31:47.164821: val_loss -0.322
2025-10-02 17:31:47.164996: Pseudo dice [np.float32(0.6916)]
2025-10-02 17:31:47.165230: Epoch time: 46.05 s
2025-10-02 17:31:47.784665: 
2025-10-02 17:31:47.785140: Epoch 52
2025-10-02 17:31:47.785488: Current learning rate: 0.00682
2025-10-02 17:32:33.787210: Validation loss did not improve from -0.40075. Patience: 4/50
2025-10-02 17:32:33.787990: train_loss -0.6341
2025-10-02 17:32:33.788235: val_loss -0.3254
2025-10-02 17:32:33.788383: Pseudo dice [np.float32(0.6864)]
2025-10-02 17:32:33.788616: Epoch time: 46.0 s
2025-10-02 17:32:34.413078: 
2025-10-02 17:32:34.413316: Epoch 53
2025-10-02 17:32:34.413491: Current learning rate: 0.00675
2025-10-02 17:33:20.450707: Validation loss did not improve from -0.40075. Patience: 5/50
2025-10-02 17:33:20.451122: train_loss -0.6455
2025-10-02 17:33:20.451273: val_loss -0.3233
2025-10-02 17:33:20.451435: Pseudo dice [np.float32(0.6852)]
2025-10-02 17:33:20.451594: Epoch time: 46.04 s
2025-10-02 17:33:21.065732: 
2025-10-02 17:33:21.065973: Epoch 54
2025-10-02 17:33:21.066139: Current learning rate: 0.00669
2025-10-02 17:34:07.103520: Validation loss did not improve from -0.40075. Patience: 6/50
2025-10-02 17:34:07.104518: train_loss -0.6586
2025-10-02 17:34:07.104862: val_loss -0.3654
2025-10-02 17:34:07.105134: Pseudo dice [np.float32(0.712)]
2025-10-02 17:34:07.105372: Epoch time: 46.04 s
2025-10-02 17:34:08.175349: 
2025-10-02 17:34:08.175638: Epoch 55
2025-10-02 17:34:08.175814: Current learning rate: 0.00663
2025-10-02 17:34:54.191831: Validation loss did not improve from -0.40075. Patience: 7/50
2025-10-02 17:34:54.192404: train_loss -0.6494
2025-10-02 17:34:54.192743: val_loss -0.3711
2025-10-02 17:34:54.193077: Pseudo dice [np.float32(0.7116)]
2025-10-02 17:34:54.193217: Epoch time: 46.02 s
2025-10-02 17:34:54.809835: 
2025-10-02 17:34:54.810043: Epoch 56
2025-10-02 17:34:54.810213: Current learning rate: 0.00657
2025-10-02 17:35:40.806144: Validation loss did not improve from -0.40075. Patience: 8/50
2025-10-02 17:35:40.806725: train_loss -0.6494
2025-10-02 17:35:40.806900: val_loss -0.3758
2025-10-02 17:35:40.807087: Pseudo dice [np.float32(0.7183)]
2025-10-02 17:35:40.807270: Epoch time: 46.0 s
2025-10-02 17:35:40.807430: Yayy! New best EMA pseudo Dice: 0.7024000287055969
2025-10-02 17:35:41.875973: 
2025-10-02 17:35:41.876469: Epoch 57
2025-10-02 17:35:41.876844: Current learning rate: 0.0065
2025-10-02 17:36:27.890495: Validation loss did not improve from -0.40075. Patience: 9/50
2025-10-02 17:36:27.891020: train_loss -0.653
2025-10-02 17:36:27.891353: val_loss -0.3901
2025-10-02 17:36:27.891626: Pseudo dice [np.float32(0.7183)]
2025-10-02 17:36:27.891892: Epoch time: 46.02 s
2025-10-02 17:36:27.892148: Yayy! New best EMA pseudo Dice: 0.7039999961853027
2025-10-02 17:36:28.999775: 
2025-10-02 17:36:29.000371: Epoch 58
2025-10-02 17:36:29.000792: Current learning rate: 0.00644
2025-10-02 17:37:15.342712: Validation loss did not improve from -0.40075. Patience: 10/50
2025-10-02 17:37:15.343293: train_loss -0.6536
2025-10-02 17:37:15.343441: val_loss -0.3961
2025-10-02 17:37:15.343599: Pseudo dice [np.float32(0.7223)]
2025-10-02 17:37:15.343752: Epoch time: 46.34 s
2025-10-02 17:37:15.343899: Yayy! New best EMA pseudo Dice: 0.7057999968528748
2025-10-02 17:37:16.450881: 
2025-10-02 17:37:16.451175: Epoch 59
2025-10-02 17:37:16.451373: Current learning rate: 0.00638
2025-10-02 17:38:02.444870: Validation loss did not improve from -0.40075. Patience: 11/50
2025-10-02 17:38:02.445512: train_loss -0.6688
2025-10-02 17:38:02.445909: val_loss -0.3841
2025-10-02 17:38:02.446313: Pseudo dice [np.float32(0.7082)]
2025-10-02 17:38:02.446719: Epoch time: 46.0 s
2025-10-02 17:38:02.926289: Yayy! New best EMA pseudo Dice: 0.7059999704360962
2025-10-02 17:38:04.009262: 
2025-10-02 17:38:04.009555: Epoch 60
2025-10-02 17:38:04.009739: Current learning rate: 0.00631
2025-10-02 17:38:50.019759: Validation loss did not improve from -0.40075. Patience: 12/50
2025-10-02 17:38:50.020441: train_loss -0.6596
2025-10-02 17:38:50.020587: val_loss -0.3693
2025-10-02 17:38:50.020709: Pseudo dice [np.float32(0.7104)]
2025-10-02 17:38:50.020867: Epoch time: 46.01 s
2025-10-02 17:38:50.020997: Yayy! New best EMA pseudo Dice: 0.7064999938011169
2025-10-02 17:38:51.088868: 
2025-10-02 17:38:51.089179: Epoch 61
2025-10-02 17:38:51.089346: Current learning rate: 0.00625
2025-10-02 17:39:37.057026: Validation loss did not improve from -0.40075. Patience: 13/50
2025-10-02 17:39:37.057468: train_loss -0.6619
2025-10-02 17:39:37.057806: val_loss -0.3685
2025-10-02 17:39:37.058096: Pseudo dice [np.float32(0.7011)]
2025-10-02 17:39:37.058403: Epoch time: 45.97 s
2025-10-02 17:39:37.682163: 
2025-10-02 17:39:37.682378: Epoch 62
2025-10-02 17:39:37.682527: Current learning rate: 0.00619
2025-10-02 17:40:23.657122: Validation loss did not improve from -0.40075. Patience: 14/50
2025-10-02 17:40:23.658203: train_loss -0.6688
2025-10-02 17:40:23.658522: val_loss -0.3767
2025-10-02 17:40:23.658852: Pseudo dice [np.float32(0.708)]
2025-10-02 17:40:23.659186: Epoch time: 45.98 s
2025-10-02 17:40:24.284388: 
2025-10-02 17:40:24.284725: Epoch 63
2025-10-02 17:40:24.285012: Current learning rate: 0.00612
2025-10-02 17:41:10.272706: Validation loss did not improve from -0.40075. Patience: 15/50
2025-10-02 17:41:10.273113: train_loss -0.6702
2025-10-02 17:41:10.273254: val_loss -0.3971
2025-10-02 17:41:10.273399: Pseudo dice [np.float32(0.7256)]
2025-10-02 17:41:10.273556: Epoch time: 45.99 s
2025-10-02 17:41:10.273740: Yayy! New best EMA pseudo Dice: 0.7081000208854675
2025-10-02 17:41:11.333907: 
2025-10-02 17:41:11.334315: Epoch 64
2025-10-02 17:41:11.334566: Current learning rate: 0.00606
2025-10-02 17:41:57.301862: Validation loss did not improve from -0.40075. Patience: 16/50
2025-10-02 17:41:57.302352: train_loss -0.6763
2025-10-02 17:41:57.302496: val_loss -0.3381
2025-10-02 17:41:57.302675: Pseudo dice [np.float32(0.6843)]
2025-10-02 17:41:57.302816: Epoch time: 45.97 s
2025-10-02 17:41:58.361908: 
2025-10-02 17:41:58.362224: Epoch 65
2025-10-02 17:41:58.362420: Current learning rate: 0.006
2025-10-02 17:42:44.363853: Validation loss did not improve from -0.40075. Patience: 17/50
2025-10-02 17:42:44.364337: train_loss -0.6849
2025-10-02 17:42:44.364719: val_loss -0.3518
2025-10-02 17:42:44.365209: Pseudo dice [np.float32(0.6958)]
2025-10-02 17:42:44.365708: Epoch time: 46.0 s
2025-10-02 17:42:44.994712: 
2025-10-02 17:42:44.994974: Epoch 66
2025-10-02 17:42:44.995173: Current learning rate: 0.00593
2025-10-02 17:43:30.983079: Validation loss did not improve from -0.40075. Patience: 18/50
2025-10-02 17:43:30.983672: train_loss -0.6918
2025-10-02 17:43:30.983826: val_loss -0.3387
2025-10-02 17:43:30.983983: Pseudo dice [np.float32(0.6883)]
2025-10-02 17:43:30.984146: Epoch time: 45.99 s
2025-10-02 17:43:31.610580: 
2025-10-02 17:43:31.611019: Epoch 67
2025-10-02 17:43:31.611208: Current learning rate: 0.00587
2025-10-02 17:44:17.611088: Validation loss did not improve from -0.40075. Patience: 19/50
2025-10-02 17:44:17.611512: train_loss -0.6862
2025-10-02 17:44:17.611884: val_loss -0.3146
2025-10-02 17:44:17.612058: Pseudo dice [np.float32(0.6974)]
2025-10-02 17:44:17.612256: Epoch time: 46.0 s
2025-10-02 17:44:18.236048: 
2025-10-02 17:44:18.236294: Epoch 68
2025-10-02 17:44:18.236468: Current learning rate: 0.00581
2025-10-02 17:45:04.272541: Validation loss did not improve from -0.40075. Patience: 20/50
2025-10-02 17:45:04.273157: train_loss -0.6943
2025-10-02 17:45:04.273447: val_loss -0.3653
2025-10-02 17:45:04.273828: Pseudo dice [np.float32(0.7233)]
2025-10-02 17:45:04.274133: Epoch time: 46.04 s
2025-10-02 17:45:04.896536: 
2025-10-02 17:45:04.896911: Epoch 69
2025-10-02 17:45:04.897285: Current learning rate: 0.00574
2025-10-02 17:45:50.911132: Validation loss did not improve from -0.40075. Patience: 21/50
2025-10-02 17:45:50.911593: train_loss -0.6862
2025-10-02 17:45:50.911766: val_loss -0.386
2025-10-02 17:45:50.911986: Pseudo dice [np.float32(0.7136)]
2025-10-02 17:45:50.912148: Epoch time: 46.02 s
2025-10-02 17:45:51.982329: 
2025-10-02 17:45:51.982780: Epoch 70
2025-10-02 17:45:51.983097: Current learning rate: 0.00568
2025-10-02 17:46:37.978963: Validation loss did not improve from -0.40075. Patience: 22/50
2025-10-02 17:46:37.979672: train_loss -0.6883
2025-10-02 17:46:37.980062: val_loss -0.3312
2025-10-02 17:46:37.980376: Pseudo dice [np.float32(0.7018)]
2025-10-02 17:46:37.980652: Epoch time: 46.0 s
2025-10-02 17:46:38.610706: 
2025-10-02 17:46:38.611186: Epoch 71
2025-10-02 17:46:38.611530: Current learning rate: 0.00562
2025-10-02 17:47:24.562720: Validation loss did not improve from -0.40075. Patience: 23/50
2025-10-02 17:47:24.563279: train_loss -0.6928
2025-10-02 17:47:24.563638: val_loss -0.3607
2025-10-02 17:47:24.564000: Pseudo dice [np.float32(0.7148)]
2025-10-02 17:47:24.564389: Epoch time: 45.95 s
2025-10-02 17:47:25.187807: 
2025-10-02 17:47:25.188364: Epoch 72
2025-10-02 17:47:25.188754: Current learning rate: 0.00555
2025-10-02 17:48:11.150878: Validation loss did not improve from -0.40075. Patience: 24/50
2025-10-02 17:48:11.151985: train_loss -0.6985
2025-10-02 17:48:11.152238: val_loss -0.2966
2025-10-02 17:48:11.152594: Pseudo dice [np.float32(0.6753)]
2025-10-02 17:48:11.153011: Epoch time: 45.96 s
2025-10-02 17:48:11.788308: 
2025-10-02 17:48:11.788629: Epoch 73
2025-10-02 17:48:11.788970: Current learning rate: 0.00549
2025-10-02 17:48:57.684125: Validation loss did not improve from -0.40075. Patience: 25/50
2025-10-02 17:48:57.684501: train_loss -0.6967
2025-10-02 17:48:57.684657: val_loss -0.3601
2025-10-02 17:48:57.684792: Pseudo dice [np.float32(0.7124)]
2025-10-02 17:48:57.684968: Epoch time: 45.9 s
2025-10-02 17:48:58.645400: 
2025-10-02 17:48:58.645986: Epoch 74
2025-10-02 17:48:58.646353: Current learning rate: 0.00542
2025-10-02 17:49:44.613357: Validation loss did not improve from -0.40075. Patience: 26/50
2025-10-02 17:49:44.614515: train_loss -0.6881
2025-10-02 17:49:44.614886: val_loss -0.3222
2025-10-02 17:49:44.615243: Pseudo dice [np.float32(0.6863)]
2025-10-02 17:49:44.615638: Epoch time: 45.97 s
2025-10-02 17:49:45.708814: 
2025-10-02 17:49:45.709109: Epoch 75
2025-10-02 17:49:45.709267: Current learning rate: 0.00536
2025-10-02 17:50:31.671194: Validation loss did not improve from -0.40075. Patience: 27/50
2025-10-02 17:50:31.671546: train_loss -0.7037
2025-10-02 17:50:31.671698: val_loss -0.3219
2025-10-02 17:50:31.671842: Pseudo dice [np.float32(0.6982)]
2025-10-02 17:50:31.671972: Epoch time: 45.96 s
2025-10-02 17:50:32.294010: 
2025-10-02 17:50:32.294482: Epoch 76
2025-10-02 17:50:32.294650: Current learning rate: 0.00529
2025-10-02 17:51:18.263613: Validation loss did not improve from -0.40075. Patience: 28/50
2025-10-02 17:51:18.264670: train_loss -0.7174
2025-10-02 17:51:18.264965: val_loss -0.3162
2025-10-02 17:51:18.265290: Pseudo dice [np.float32(0.6909)]
2025-10-02 17:51:18.265588: Epoch time: 45.97 s
2025-10-02 17:51:18.891226: 
2025-10-02 17:51:18.891489: Epoch 77
2025-10-02 17:51:18.891666: Current learning rate: 0.00523
2025-10-02 17:52:04.832370: Validation loss did not improve from -0.40075. Patience: 29/50
2025-10-02 17:52:04.832770: train_loss -0.7152
2025-10-02 17:52:04.832939: val_loss -0.3328
2025-10-02 17:52:04.833078: Pseudo dice [np.float32(0.7088)]
2025-10-02 17:52:04.833246: Epoch time: 45.94 s
2025-10-02 17:52:05.465081: 
2025-10-02 17:52:05.465504: Epoch 78
2025-10-02 17:52:05.465836: Current learning rate: 0.00517
2025-10-02 17:52:51.429648: Validation loss did not improve from -0.40075. Patience: 30/50
2025-10-02 17:52:51.430239: train_loss -0.7199
2025-10-02 17:52:51.430412: val_loss -0.3486
2025-10-02 17:52:51.430569: Pseudo dice [np.float32(0.7139)]
2025-10-02 17:52:51.430738: Epoch time: 45.97 s
2025-10-02 17:52:52.062758: 
2025-10-02 17:52:52.062977: Epoch 79
2025-10-02 17:52:52.063145: Current learning rate: 0.0051
2025-10-02 17:53:38.085978: Validation loss did not improve from -0.40075. Patience: 31/50
2025-10-02 17:53:38.086577: train_loss -0.716
2025-10-02 17:53:38.086918: val_loss -0.3179
2025-10-02 17:53:38.087058: Pseudo dice [np.float32(0.6981)]
2025-10-02 17:53:38.087313: Epoch time: 46.02 s
2025-10-02 17:53:39.162509: 
2025-10-02 17:53:39.162745: Epoch 80
2025-10-02 17:53:39.162886: Current learning rate: 0.00504
2025-10-02 17:54:25.172913: Validation loss did not improve from -0.40075. Patience: 32/50
2025-10-02 17:54:25.173831: train_loss -0.7162
2025-10-02 17:54:25.174103: val_loss -0.2874
2025-10-02 17:54:25.174392: Pseudo dice [np.float32(0.674)]
2025-10-02 17:54:25.174679: Epoch time: 46.01 s
2025-10-02 17:54:25.805343: 
2025-10-02 17:54:25.805734: Epoch 81
2025-10-02 17:54:25.806117: Current learning rate: 0.00497
2025-10-02 17:55:11.830110: Validation loss did not improve from -0.40075. Patience: 33/50
2025-10-02 17:55:11.830753: train_loss -0.7126
2025-10-02 17:55:11.831121: val_loss -0.3618
2025-10-02 17:55:11.831440: Pseudo dice [np.float32(0.6968)]
2025-10-02 17:55:11.831838: Epoch time: 46.03 s
2025-10-02 17:55:12.468093: 
2025-10-02 17:55:12.468584: Epoch 82
2025-10-02 17:55:12.468942: Current learning rate: 0.00491
2025-10-02 17:55:58.483913: Validation loss did not improve from -0.40075. Patience: 34/50
2025-10-02 17:55:58.484638: train_loss -0.726
2025-10-02 17:55:58.484865: val_loss -0.3338
2025-10-02 17:55:58.485069: Pseudo dice [np.float32(0.7081)]
2025-10-02 17:55:58.485291: Epoch time: 46.02 s
2025-10-02 17:55:59.104559: 
2025-10-02 17:55:59.104814: Epoch 83
2025-10-02 17:55:59.105012: Current learning rate: 0.00484
2025-10-02 17:56:45.077597: Validation loss did not improve from -0.40075. Patience: 35/50
2025-10-02 17:56:45.077962: train_loss -0.7266
2025-10-02 17:56:45.078119: val_loss -0.3334
2025-10-02 17:56:45.078245: Pseudo dice [np.float32(0.7001)]
2025-10-02 17:56:45.078406: Epoch time: 45.97 s
2025-10-02 17:56:45.694084: 
2025-10-02 17:56:45.694475: Epoch 84
2025-10-02 17:56:45.694835: Current learning rate: 0.00478
2025-10-02 17:57:31.760656: Validation loss did not improve from -0.40075. Patience: 36/50
2025-10-02 17:57:31.761889: train_loss -0.7293
2025-10-02 17:57:31.762259: val_loss -0.3672
2025-10-02 17:57:31.762625: Pseudo dice [np.float32(0.7082)]
2025-10-02 17:57:31.763108: Epoch time: 46.07 s
2025-10-02 17:57:32.826977: 
2025-10-02 17:57:32.827477: Epoch 85
2025-10-02 17:57:32.827674: Current learning rate: 0.00471
2025-10-02 17:58:18.845876: Validation loss did not improve from -0.40075. Patience: 37/50
2025-10-02 17:58:18.846495: train_loss -0.7328
2025-10-02 17:58:18.846956: val_loss -0.3105
2025-10-02 17:58:18.847297: Pseudo dice [np.float32(0.6961)]
2025-10-02 17:58:18.847689: Epoch time: 46.02 s
2025-10-02 17:58:19.459727: 
2025-10-02 17:58:19.460056: Epoch 86
2025-10-02 17:58:19.460310: Current learning rate: 0.00465
2025-10-02 17:59:05.441048: Validation loss did not improve from -0.40075. Patience: 38/50
2025-10-02 17:59:05.441790: train_loss -0.7411
2025-10-02 17:59:05.442014: val_loss -0.3232
2025-10-02 17:59:05.442299: Pseudo dice [np.float32(0.6991)]
2025-10-02 17:59:05.442588: Epoch time: 45.98 s
2025-10-02 17:59:06.055797: 
2025-10-02 17:59:06.056123: Epoch 87
2025-10-02 17:59:06.056301: Current learning rate: 0.00458
2025-10-02 17:59:52.068390: Validation loss did not improve from -0.40075. Patience: 39/50
2025-10-02 17:59:52.069075: train_loss -0.7348
2025-10-02 17:59:52.069473: val_loss -0.3194
2025-10-02 17:59:52.069941: Pseudo dice [np.float32(0.698)]
2025-10-02 17:59:52.070358: Epoch time: 46.01 s
2025-10-02 17:59:52.692147: 
2025-10-02 17:59:52.692571: Epoch 88
2025-10-02 17:59:52.692971: Current learning rate: 0.00452
2025-10-02 18:00:38.710859: Validation loss did not improve from -0.40075. Patience: 40/50
2025-10-02 18:00:38.711414: train_loss -0.7391
2025-10-02 18:00:38.711559: val_loss -0.3651
2025-10-02 18:00:38.711728: Pseudo dice [np.float32(0.7232)]
2025-10-02 18:00:38.711876: Epoch time: 46.02 s
2025-10-02 18:00:39.657851: 
2025-10-02 18:00:39.658444: Epoch 89
2025-10-02 18:00:39.658770: Current learning rate: 0.00445
2025-10-02 18:01:25.702807: Validation loss did not improve from -0.40075. Patience: 41/50
2025-10-02 18:01:25.703437: train_loss -0.7443
2025-10-02 18:01:25.703793: val_loss -0.3181
2025-10-02 18:01:25.704131: Pseudo dice [np.float32(0.7074)]
2025-10-02 18:01:25.704489: Epoch time: 46.05 s
2025-10-02 18:01:26.760016: 
2025-10-02 18:01:26.760240: Epoch 90
2025-10-02 18:01:26.760476: Current learning rate: 0.00438
2025-10-02 18:02:12.774820: Validation loss did not improve from -0.40075. Patience: 42/50
2025-10-02 18:02:12.775964: train_loss -0.7373
2025-10-02 18:02:12.776291: val_loss -0.3265
2025-10-02 18:02:12.776586: Pseudo dice [np.float32(0.7043)]
2025-10-02 18:02:12.776807: Epoch time: 46.02 s
2025-10-02 18:02:13.393974: 
2025-10-02 18:02:13.394307: Epoch 91
2025-10-02 18:02:13.394498: Current learning rate: 0.00432
2025-10-02 18:02:59.434746: Validation loss did not improve from -0.40075. Patience: 43/50
2025-10-02 18:02:59.435253: train_loss -0.7393
2025-10-02 18:02:59.435414: val_loss -0.2931
2025-10-02 18:02:59.435633: Pseudo dice [np.float32(0.6841)]
2025-10-02 18:02:59.435777: Epoch time: 46.04 s
2025-10-02 18:03:00.051479: 
2025-10-02 18:03:00.051780: Epoch 92
2025-10-02 18:03:00.051956: Current learning rate: 0.00425
2025-10-02 18:03:46.069221: Validation loss did not improve from -0.40075. Patience: 44/50
2025-10-02 18:03:46.069879: train_loss -0.7519
2025-10-02 18:03:46.070018: val_loss -0.2802
2025-10-02 18:03:46.070155: Pseudo dice [np.float32(0.6837)]
2025-10-02 18:03:46.070329: Epoch time: 46.02 s
2025-10-02 18:03:46.687492: 
2025-10-02 18:03:46.687726: Epoch 93
2025-10-02 18:03:46.687872: Current learning rate: 0.00419
2025-10-02 18:04:32.708927: Validation loss did not improve from -0.40075. Patience: 45/50
2025-10-02 18:04:32.709353: train_loss -0.7589
2025-10-02 18:04:32.709518: val_loss -0.3004
2025-10-02 18:04:32.709645: Pseudo dice [np.float32(0.7041)]
2025-10-02 18:04:32.709795: Epoch time: 46.02 s
2025-10-02 18:04:33.326471: 
2025-10-02 18:04:33.326796: Epoch 94
2025-10-02 18:04:33.326949: Current learning rate: 0.00412
2025-10-02 18:05:19.370194: Validation loss did not improve from -0.40075. Patience: 46/50
2025-10-02 18:05:19.371281: train_loss -0.7612
2025-10-02 18:05:19.371682: val_loss -0.3334
2025-10-02 18:05:19.372063: Pseudo dice [np.float32(0.7129)]
2025-10-02 18:05:19.372455: Epoch time: 46.05 s
2025-10-02 18:05:20.426544: 
2025-10-02 18:05:20.426803: Epoch 95
2025-10-02 18:05:20.426985: Current learning rate: 0.00405
2025-10-02 18:06:06.504855: Validation loss did not improve from -0.40075. Patience: 47/50
2025-10-02 18:06:06.505239: train_loss -0.7583
2025-10-02 18:06:06.505416: val_loss -0.3281
2025-10-02 18:06:06.505703: Pseudo dice [np.float32(0.7235)]
2025-10-02 18:06:06.506068: Epoch time: 46.08 s
2025-10-02 18:06:07.122292: 
2025-10-02 18:06:07.122679: Epoch 96
2025-10-02 18:06:07.123021: Current learning rate: 0.00399
2025-10-02 18:06:53.124973: Validation loss did not improve from -0.40075. Patience: 48/50
2025-10-02 18:06:53.125797: train_loss -0.7531
2025-10-02 18:06:53.125973: val_loss -0.2755
2025-10-02 18:06:53.126114: Pseudo dice [np.float32(0.6973)]
2025-10-02 18:06:53.126248: Epoch time: 46.0 s
2025-10-02 18:06:53.752795: 
2025-10-02 18:06:53.753102: Epoch 97
2025-10-02 18:06:53.753283: Current learning rate: 0.00392
2025-10-02 18:07:39.754550: Validation loss did not improve from -0.40075. Patience: 49/50
2025-10-02 18:07:39.754994: train_loss -0.7571
2025-10-02 18:07:39.755207: val_loss -0.3051
2025-10-02 18:07:39.755521: Pseudo dice [np.float32(0.7073)]
2025-10-02 18:07:39.755700: Epoch time: 46.0 s
2025-10-02 18:07:40.378828: 
2025-10-02 18:07:40.379040: Epoch 98
2025-10-02 18:07:40.379188: Current learning rate: 0.00385
2025-10-02 18:08:26.349749: Validation loss did not improve from -0.40075. Patience: 50/50
2025-10-02 18:08:26.350704: train_loss -0.7606
2025-10-02 18:08:26.351048: val_loss -0.3043
2025-10-02 18:08:26.351420: Pseudo dice [np.float32(0.7079)]
2025-10-02 18:08:26.351755: Epoch time: 45.97 s
2025-10-02 18:08:26.976307: 
2025-10-02 18:08:26.976806: Epoch 99
2025-10-02 18:08:26.977171: Current learning rate: 0.00379
2025-10-02 18:09:12.981273: Validation loss did not improve from -0.40075. Patience: 51/50
2025-10-02 18:09:12.981761: train_loss -0.7653
2025-10-02 18:09:12.982034: val_loss -0.2484
2025-10-02 18:09:12.982256: Pseudo dice [np.float32(0.6599)]
2025-10-02 18:09:12.982440: Epoch time: 46.01 s
2025-10-02 18:09:14.038744: 
2025-10-02 18:09:14.039113: Epoch 100
2025-10-02 18:09:14.039438: Current learning rate: 0.00372
2025-10-02 18:10:00.058249: Validation loss did not improve from -0.40075. Patience: 52/50
2025-10-02 18:10:00.058967: train_loss -0.7646
2025-10-02 18:10:00.059111: val_loss -0.3331
2025-10-02 18:10:00.059301: Pseudo dice [np.float32(0.6998)]
2025-10-02 18:10:00.059525: Epoch time: 46.02 s
2025-10-02 18:10:00.684085: 
2025-10-02 18:10:00.684491: Epoch 101
2025-10-02 18:10:00.684758: Current learning rate: 0.00365
2025-10-02 18:10:46.668810: Validation loss did not improve from -0.40075. Patience: 53/50
2025-10-02 18:10:46.669282: train_loss -0.7711
2025-10-02 18:10:46.669456: val_loss -0.2751
2025-10-02 18:10:46.669627: Pseudo dice [np.float32(0.6861)]
2025-10-02 18:10:46.669815: Epoch time: 45.99 s
2025-10-02 18:10:47.297508: 
2025-10-02 18:10:47.297757: Epoch 102
2025-10-02 18:10:47.297945: Current learning rate: 0.00359
2025-10-02 18:11:33.289740: Validation loss did not improve from -0.40075. Patience: 54/50
2025-10-02 18:11:33.290210: train_loss -0.7722
2025-10-02 18:11:33.290369: val_loss -0.3409
2025-10-02 18:11:33.290489: Pseudo dice [np.float32(0.7135)]
2025-10-02 18:11:33.290679: Epoch time: 45.99 s
2025-10-02 18:11:33.916244: 
2025-10-02 18:11:33.916960: Epoch 103
2025-10-02 18:11:33.917296: Current learning rate: 0.00352
2025-10-02 18:12:19.852288: Validation loss did not improve from -0.40075. Patience: 55/50
2025-10-02 18:12:19.852810: train_loss -0.7711
2025-10-02 18:12:19.853189: val_loss -0.299
2025-10-02 18:12:19.853496: Pseudo dice [np.float32(0.6922)]
2025-10-02 18:12:19.853846: Epoch time: 45.94 s
2025-10-02 18:12:20.477886: 
2025-10-02 18:12:20.478243: Epoch 104
2025-10-02 18:12:20.478569: Current learning rate: 0.00345
2025-10-02 18:13:06.452857: Validation loss did not improve from -0.40075. Patience: 56/50
2025-10-02 18:13:06.453856: train_loss -0.7713
2025-10-02 18:13:06.454166: val_loss -0.3027
2025-10-02 18:13:06.454459: Pseudo dice [np.float32(0.7017)]
2025-10-02 18:13:06.454780: Epoch time: 45.98 s
2025-10-02 18:13:07.849625: 
2025-10-02 18:13:07.850063: Epoch 105
2025-10-02 18:13:07.850430: Current learning rate: 0.00338
2025-10-02 18:13:53.832157: Validation loss did not improve from -0.40075. Patience: 57/50
2025-10-02 18:13:53.832590: train_loss -0.7673
2025-10-02 18:13:53.832776: val_loss -0.2779
2025-10-02 18:13:53.832932: Pseudo dice [np.float32(0.7003)]
2025-10-02 18:13:53.833093: Epoch time: 45.98 s
2025-10-02 18:13:54.458084: 
2025-10-02 18:13:54.458408: Epoch 106
2025-10-02 18:13:54.458584: Current learning rate: 0.00332
2025-10-02 18:14:40.467176: Validation loss did not improve from -0.40075. Patience: 58/50
2025-10-02 18:14:40.468302: train_loss -0.7787
2025-10-02 18:14:40.468670: val_loss -0.3031
2025-10-02 18:14:40.468980: Pseudo dice [np.float32(0.7047)]
2025-10-02 18:14:40.469315: Epoch time: 46.01 s
2025-10-02 18:14:41.096697: 
2025-10-02 18:14:41.097131: Epoch 107
2025-10-02 18:14:41.097471: Current learning rate: 0.00325
2025-10-02 18:15:27.064082: Validation loss did not improve from -0.40075. Patience: 59/50
2025-10-02 18:15:27.064556: train_loss -0.7782
2025-10-02 18:15:27.064784: val_loss -0.2459
2025-10-02 18:15:27.065022: Pseudo dice [np.float32(0.692)]
2025-10-02 18:15:27.065334: Epoch time: 45.97 s
2025-10-02 18:15:27.696383: 
2025-10-02 18:15:27.696771: Epoch 108
2025-10-02 18:15:27.697147: Current learning rate: 0.00318
2025-10-02 18:16:13.643387: Validation loss did not improve from -0.40075. Patience: 60/50
2025-10-02 18:16:13.643928: train_loss -0.7773
2025-10-02 18:16:13.644120: val_loss -0.287
2025-10-02 18:16:13.644246: Pseudo dice [np.float32(0.7086)]
2025-10-02 18:16:13.644398: Epoch time: 45.95 s
2025-10-02 18:16:14.272542: 
2025-10-02 18:16:14.272901: Epoch 109
2025-10-02 18:16:14.273186: Current learning rate: 0.00311
2025-10-02 18:17:00.290424: Validation loss did not improve from -0.40075. Patience: 61/50
2025-10-02 18:17:00.290838: train_loss -0.7769
2025-10-02 18:17:00.291019: val_loss -0.3093
2025-10-02 18:17:00.291165: Pseudo dice [np.float32(0.7062)]
2025-10-02 18:17:00.291466: Epoch time: 46.02 s
2025-10-02 18:17:01.376541: 
2025-10-02 18:17:01.376940: Epoch 110
2025-10-02 18:17:01.377269: Current learning rate: 0.00304
2025-10-02 18:17:47.398619: Validation loss did not improve from -0.40075. Patience: 62/50
2025-10-02 18:17:47.399225: train_loss -0.7791
2025-10-02 18:17:47.399390: val_loss -0.2551
2025-10-02 18:17:47.399549: Pseudo dice [np.float32(0.6749)]
2025-10-02 18:17:47.399699: Epoch time: 46.02 s
2025-10-02 18:17:48.027853: 
2025-10-02 18:17:48.028116: Epoch 111
2025-10-02 18:17:48.028319: Current learning rate: 0.00297
2025-10-02 18:18:34.041052: Validation loss did not improve from -0.40075. Patience: 63/50
2025-10-02 18:18:34.041572: train_loss -0.7867
2025-10-02 18:18:34.041899: val_loss -0.2832
2025-10-02 18:18:34.042183: Pseudo dice [np.float32(0.6878)]
2025-10-02 18:18:34.042470: Epoch time: 46.01 s
2025-10-02 18:18:34.670506: 
2025-10-02 18:18:34.670853: Epoch 112
2025-10-02 18:18:34.671143: Current learning rate: 0.00291
2025-10-02 18:19:20.733322: Validation loss did not improve from -0.40075. Patience: 64/50
2025-10-02 18:19:20.734280: train_loss -0.7854
2025-10-02 18:19:20.734488: val_loss -0.3017
2025-10-02 18:19:20.734823: Pseudo dice [np.float32(0.7067)]
2025-10-02 18:19:20.734998: Epoch time: 46.06 s
2025-10-02 18:19:21.364025: 
2025-10-02 18:19:21.364355: Epoch 113
2025-10-02 18:19:21.364604: Current learning rate: 0.00284
2025-10-02 18:20:07.378112: Validation loss did not improve from -0.40075. Patience: 65/50
2025-10-02 18:20:07.378525: train_loss -0.793
2025-10-02 18:20:07.378689: val_loss -0.2896
2025-10-02 18:20:07.378813: Pseudo dice [np.float32(0.6947)]
2025-10-02 18:20:07.378964: Epoch time: 46.02 s
2025-10-02 18:20:08.008780: 
2025-10-02 18:20:08.008998: Epoch 114
2025-10-02 18:20:08.009163: Current learning rate: 0.00277
2025-10-02 18:20:54.019613: Validation loss did not improve from -0.40075. Patience: 66/50
2025-10-02 18:20:54.020120: train_loss -0.7939
2025-10-02 18:20:54.020283: val_loss -0.3155
2025-10-02 18:20:54.020437: Pseudo dice [np.float32(0.7053)]
2025-10-02 18:20:54.020579: Epoch time: 46.01 s
2025-10-02 18:20:55.100040: 
2025-10-02 18:20:55.100441: Epoch 115
2025-10-02 18:20:55.100713: Current learning rate: 0.0027
2025-10-02 18:21:41.099791: Validation loss did not improve from -0.40075. Patience: 67/50
2025-10-02 18:21:41.100214: train_loss -0.7945
2025-10-02 18:21:41.100385: val_loss -0.3303
2025-10-02 18:21:41.100528: Pseudo dice [np.float32(0.7138)]
2025-10-02 18:21:41.100667: Epoch time: 46.0 s
2025-10-02 18:21:41.732983: 
2025-10-02 18:21:41.733241: Epoch 116
2025-10-02 18:21:41.733414: Current learning rate: 0.00263
2025-10-02 18:22:27.726701: Validation loss did not improve from -0.40075. Patience: 68/50
2025-10-02 18:22:27.727495: train_loss -0.7927
2025-10-02 18:22:27.727752: val_loss -0.226
2025-10-02 18:22:27.728054: Pseudo dice [np.float32(0.6578)]
2025-10-02 18:22:27.728264: Epoch time: 46.0 s
2025-10-02 18:22:28.360494: 
2025-10-02 18:22:28.360791: Epoch 117
2025-10-02 18:22:28.361062: Current learning rate: 0.00256
2025-10-02 18:23:14.374398: Validation loss did not improve from -0.40075. Patience: 69/50
2025-10-02 18:23:14.375022: train_loss -0.8064
2025-10-02 18:23:14.375342: val_loss -0.2204
2025-10-02 18:23:14.375673: Pseudo dice [np.float32(0.6675)]
2025-10-02 18:23:14.376020: Epoch time: 46.02 s
2025-10-02 18:23:15.009048: 
2025-10-02 18:23:15.009496: Epoch 118
2025-10-02 18:23:15.009706: Current learning rate: 0.00249
2025-10-02 18:24:01.025670: Validation loss did not improve from -0.40075. Patience: 70/50
2025-10-02 18:24:01.026333: train_loss -0.8019
2025-10-02 18:24:01.026495: val_loss -0.2918
2025-10-02 18:24:01.026647: Pseudo dice [np.float32(0.7057)]
2025-10-02 18:24:01.026818: Epoch time: 46.02 s
2025-10-02 18:24:01.656044: 
2025-10-02 18:24:01.656258: Epoch 119
2025-10-02 18:24:01.656407: Current learning rate: 0.00242
2025-10-02 18:24:47.663990: Validation loss did not improve from -0.40075. Patience: 71/50
2025-10-02 18:24:47.664709: train_loss -0.7932
2025-10-02 18:24:47.665227: val_loss -0.2484
2025-10-02 18:24:47.665632: Pseudo dice [np.float32(0.6614)]
2025-10-02 18:24:47.666132: Epoch time: 46.01 s
2025-10-02 18:24:49.091018: 
2025-10-02 18:24:49.091332: Epoch 120
2025-10-02 18:24:49.091486: Current learning rate: 0.00235
2025-10-02 18:25:35.103899: Validation loss did not improve from -0.40075. Patience: 72/50
2025-10-02 18:25:35.105015: train_loss -0.8077
2025-10-02 18:25:35.105375: val_loss -0.3063
2025-10-02 18:25:35.105650: Pseudo dice [np.float32(0.7093)]
2025-10-02 18:25:35.105971: Epoch time: 46.01 s
2025-10-02 18:25:35.741043: 
2025-10-02 18:25:35.741306: Epoch 121
2025-10-02 18:25:35.741483: Current learning rate: 0.00228
2025-10-02 18:26:21.773211: Validation loss did not improve from -0.40075. Patience: 73/50
2025-10-02 18:26:21.773588: train_loss -0.8071
2025-10-02 18:26:21.773777: val_loss -0.2774
2025-10-02 18:26:21.773925: Pseudo dice [np.float32(0.6978)]
2025-10-02 18:26:21.774082: Epoch time: 46.03 s
2025-10-02 18:26:22.409452: 
2025-10-02 18:26:22.409958: Epoch 122
2025-10-02 18:26:22.410299: Current learning rate: 0.00221
2025-10-02 18:27:08.417507: Validation loss did not improve from -0.40075. Patience: 74/50
2025-10-02 18:27:08.418696: train_loss -0.8083
2025-10-02 18:27:08.419066: val_loss -0.2539
2025-10-02 18:27:08.419407: Pseudo dice [np.float32(0.6909)]
2025-10-02 18:27:08.419764: Epoch time: 46.01 s
2025-10-02 18:27:09.056978: 
2025-10-02 18:27:09.057298: Epoch 123
2025-10-02 18:27:09.057480: Current learning rate: 0.00214
2025-10-02 18:27:55.048521: Validation loss did not improve from -0.40075. Patience: 75/50
2025-10-02 18:27:55.049123: train_loss -0.8069
2025-10-02 18:27:55.049468: val_loss -0.2753
2025-10-02 18:27:55.049794: Pseudo dice [np.float32(0.6957)]
2025-10-02 18:27:55.050125: Epoch time: 45.99 s
2025-10-02 18:27:55.685611: 
2025-10-02 18:27:55.685857: Epoch 124
2025-10-02 18:27:55.686154: Current learning rate: 0.00207
2025-10-02 18:28:41.654977: Validation loss did not improve from -0.40075. Patience: 76/50
2025-10-02 18:28:41.656023: train_loss -0.8106
2025-10-02 18:28:41.656397: val_loss -0.2965
2025-10-02 18:28:41.656704: Pseudo dice [np.float32(0.7144)]
2025-10-02 18:28:41.657044: Epoch time: 45.97 s
2025-10-02 18:28:42.729802: 
2025-10-02 18:28:42.730089: Epoch 125
2025-10-02 18:28:42.730304: Current learning rate: 0.00199
2025-10-02 18:29:28.730605: Validation loss did not improve from -0.40075. Patience: 77/50
2025-10-02 18:29:28.731058: train_loss -0.8088
2025-10-02 18:29:28.731321: val_loss -0.2871
2025-10-02 18:29:28.731508: Pseudo dice [np.float32(0.7017)]
2025-10-02 18:29:28.731741: Epoch time: 46.0 s
2025-10-02 18:29:29.366543: 
2025-10-02 18:29:29.366817: Epoch 126
2025-10-02 18:29:29.366989: Current learning rate: 0.00192
2025-10-02 18:30:15.372736: Validation loss did not improve from -0.40075. Patience: 78/50
2025-10-02 18:30:15.373327: train_loss -0.8079
2025-10-02 18:30:15.373521: val_loss -0.3133
2025-10-02 18:30:15.373657: Pseudo dice [np.float32(0.7177)]
2025-10-02 18:30:15.373818: Epoch time: 46.01 s
2025-10-02 18:30:16.016488: 
2025-10-02 18:30:16.016784: Epoch 127
2025-10-02 18:30:16.016970: Current learning rate: 0.00185
2025-10-02 18:31:02.012426: Validation loss did not improve from -0.40075. Patience: 79/50
2025-10-02 18:31:02.012816: train_loss -0.8107
2025-10-02 18:31:02.013041: val_loss -0.223
2025-10-02 18:31:02.013196: Pseudo dice [np.float32(0.6708)]
2025-10-02 18:31:02.013351: Epoch time: 46.0 s
2025-10-02 18:31:02.647862: 
2025-10-02 18:31:02.648231: Epoch 128
2025-10-02 18:31:02.648522: Current learning rate: 0.00178
2025-10-02 18:31:48.642757: Validation loss did not improve from -0.40075. Patience: 80/50
2025-10-02 18:31:48.643718: train_loss -0.8207
2025-10-02 18:31:48.644101: val_loss -0.2306
2025-10-02 18:31:48.644344: Pseudo dice [np.float32(0.6729)]
2025-10-02 18:31:48.644613: Epoch time: 46.0 s
2025-10-02 18:31:49.273039: 
2025-10-02 18:31:49.273428: Epoch 129
2025-10-02 18:31:49.273588: Current learning rate: 0.0017
2025-10-02 18:32:35.297827: Validation loss did not improve from -0.40075. Patience: 81/50
2025-10-02 18:32:35.298167: train_loss -0.8157
2025-10-02 18:32:35.298338: val_loss -0.236
2025-10-02 18:32:35.298475: Pseudo dice [np.float32(0.6802)]
2025-10-02 18:32:35.298672: Epoch time: 46.03 s
2025-10-02 18:32:36.365976: 
2025-10-02 18:32:36.366220: Epoch 130
2025-10-02 18:32:36.366399: Current learning rate: 0.00163
2025-10-02 18:33:22.381861: Validation loss did not improve from -0.40075. Patience: 82/50
2025-10-02 18:33:22.383433: train_loss -0.8151
2025-10-02 18:33:22.383954: val_loss -0.2403
2025-10-02 18:33:22.384401: Pseudo dice [np.float32(0.6789)]
2025-10-02 18:33:22.384854: Epoch time: 46.02 s
2025-10-02 18:33:23.014698: 
2025-10-02 18:33:23.015232: Epoch 131
2025-10-02 18:33:23.015562: Current learning rate: 0.00156
2025-10-02 18:34:09.001421: Validation loss did not improve from -0.40075. Patience: 83/50
2025-10-02 18:34:09.001946: train_loss -0.8214
2025-10-02 18:34:09.002283: val_loss -0.2473
2025-10-02 18:34:09.002575: Pseudo dice [np.float32(0.6842)]
2025-10-02 18:34:09.002902: Epoch time: 45.99 s
2025-10-02 18:34:09.630684: 
2025-10-02 18:34:09.631162: Epoch 132
2025-10-02 18:34:09.631563: Current learning rate: 0.00148
2025-10-02 18:34:55.586848: Validation loss did not improve from -0.40075. Patience: 84/50
2025-10-02 18:34:55.587383: train_loss -0.8167
2025-10-02 18:34:55.587568: val_loss -0.2377
2025-10-02 18:34:55.587957: Pseudo dice [np.float32(0.6914)]
2025-10-02 18:34:55.588094: Epoch time: 45.96 s
2025-10-02 18:34:56.215121: 
2025-10-02 18:34:56.215433: Epoch 133
2025-10-02 18:34:56.215669: Current learning rate: 0.00141
2025-10-02 18:35:42.224139: Validation loss did not improve from -0.40075. Patience: 85/50
2025-10-02 18:35:42.224658: train_loss -0.8215
2025-10-02 18:35:42.224908: val_loss -0.2452
2025-10-02 18:35:42.225162: Pseudo dice [np.float32(0.6747)]
2025-10-02 18:35:42.225528: Epoch time: 46.01 s
2025-10-02 18:35:42.859519: 
2025-10-02 18:35:42.859735: Epoch 134
2025-10-02 18:35:42.859890: Current learning rate: 0.00133
2025-10-02 18:36:28.924459: Validation loss did not improve from -0.40075. Patience: 86/50
2025-10-02 18:36:28.924945: train_loss -0.8246
2025-10-02 18:36:28.925088: val_loss -0.2417
2025-10-02 18:36:28.925221: Pseudo dice [np.float32(0.7002)]
2025-10-02 18:36:28.925379: Epoch time: 46.07 s
2025-10-02 18:36:30.040627: 
2025-10-02 18:36:30.040947: Epoch 135
2025-10-02 18:36:30.041142: Current learning rate: 0.00126
2025-10-02 18:37:16.051462: Validation loss did not improve from -0.40075. Patience: 87/50
2025-10-02 18:37:16.052560: train_loss -0.8265
2025-10-02 18:37:16.053475: val_loss -0.3201
2025-10-02 18:37:16.054391: Pseudo dice [np.float32(0.7131)]
2025-10-02 18:37:16.055232: Epoch time: 46.01 s
2025-10-02 18:37:17.026096: 
2025-10-02 18:37:17.026396: Epoch 136
2025-10-02 18:37:17.026625: Current learning rate: 0.00118
2025-10-02 18:38:03.079978: Validation loss did not improve from -0.40075. Patience: 88/50
2025-10-02 18:38:03.080668: train_loss -0.8297
2025-10-02 18:38:03.080854: val_loss -0.2396
2025-10-02 18:38:03.080995: Pseudo dice [np.float32(0.6887)]
2025-10-02 18:38:03.081213: Epoch time: 46.06 s
2025-10-02 18:38:03.711678: 
2025-10-02 18:38:03.712006: Epoch 137
2025-10-02 18:38:03.712198: Current learning rate: 0.00111
2025-10-02 18:38:49.741473: Validation loss did not improve from -0.40075. Patience: 89/50
2025-10-02 18:38:49.741866: train_loss -0.8255
2025-10-02 18:38:49.742023: val_loss -0.2239
2025-10-02 18:38:49.742154: Pseudo dice [np.float32(0.6885)]
2025-10-02 18:38:49.742320: Epoch time: 46.03 s
2025-10-02 18:38:50.372602: 
2025-10-02 18:38:50.372992: Epoch 138
2025-10-02 18:38:50.373332: Current learning rate: 0.00103
2025-10-02 18:39:36.377894: Validation loss did not improve from -0.40075. Patience: 90/50
2025-10-02 18:39:36.378503: train_loss -0.8316
2025-10-02 18:39:36.378649: val_loss -0.1889
2025-10-02 18:39:36.378791: Pseudo dice [np.float32(0.6703)]
2025-10-02 18:39:36.378925: Epoch time: 46.01 s
2025-10-02 18:39:37.009992: 
2025-10-02 18:39:37.010319: Epoch 139
2025-10-02 18:39:37.010471: Current learning rate: 0.00095
2025-10-02 18:40:23.071629: Validation loss did not improve from -0.40075. Patience: 91/50
2025-10-02 18:40:23.072205: train_loss -0.8277
2025-10-02 18:40:23.072522: val_loss -0.3017
2025-10-02 18:40:23.072833: Pseudo dice [np.float32(0.7183)]
2025-10-02 18:40:23.073173: Epoch time: 46.06 s
2025-10-02 18:40:24.155912: 
2025-10-02 18:40:24.156396: Epoch 140
2025-10-02 18:40:24.156760: Current learning rate: 0.00087
2025-10-02 18:41:10.181751: Validation loss did not improve from -0.40075. Patience: 92/50
2025-10-02 18:41:10.182375: train_loss -0.8313
2025-10-02 18:41:10.182534: val_loss -0.2062
2025-10-02 18:41:10.182672: Pseudo dice [np.float32(0.6663)]
2025-10-02 18:41:10.182868: Epoch time: 46.03 s
2025-10-02 18:41:10.815141: 
2025-10-02 18:41:10.815449: Epoch 141
2025-10-02 18:41:10.815641: Current learning rate: 0.00079
2025-10-02 18:41:56.843764: Validation loss did not improve from -0.40075. Patience: 93/50
2025-10-02 18:41:56.844324: train_loss -0.835
2025-10-02 18:41:56.844646: val_loss -0.2315
2025-10-02 18:41:56.844967: Pseudo dice [np.float32(0.6858)]
2025-10-02 18:41:56.845305: Epoch time: 46.03 s
2025-10-02 18:41:57.480587: 
2025-10-02 18:41:57.480963: Epoch 142
2025-10-02 18:41:57.481243: Current learning rate: 0.00071
2025-10-02 18:42:43.498487: Validation loss did not improve from -0.40075. Patience: 94/50
2025-10-02 18:42:43.499586: train_loss -0.8382
2025-10-02 18:42:43.499949: val_loss -0.2677
2025-10-02 18:42:43.500228: Pseudo dice [np.float32(0.7041)]
2025-10-02 18:42:43.500510: Epoch time: 46.02 s
2025-10-02 18:42:44.134044: 
2025-10-02 18:42:44.134409: Epoch 143
2025-10-02 18:42:44.134609: Current learning rate: 0.00063
2025-10-02 18:43:30.148361: Validation loss did not improve from -0.40075. Patience: 95/50
2025-10-02 18:43:30.148875: train_loss -0.835
2025-10-02 18:43:30.149137: val_loss -0.245
2025-10-02 18:43:30.149294: Pseudo dice [np.float32(0.7032)]
2025-10-02 18:43:30.149452: Epoch time: 46.02 s
2025-10-02 18:43:30.781444: 
2025-10-02 18:43:30.781697: Epoch 144
2025-10-02 18:43:30.781940: Current learning rate: 0.00055
2025-10-02 18:44:16.806040: Validation loss did not improve from -0.40075. Patience: 96/50
2025-10-02 18:44:16.806606: train_loss -0.8365
2025-10-02 18:44:16.806832: val_loss -0.2337
2025-10-02 18:44:16.806976: Pseudo dice [np.float32(0.6821)]
2025-10-02 18:44:16.807103: Epoch time: 46.03 s
2025-10-02 18:44:17.869835: 
2025-10-02 18:44:17.870144: Epoch 145
2025-10-02 18:44:17.870366: Current learning rate: 0.00047
2025-10-02 18:45:03.845276: Validation loss did not improve from -0.40075. Patience: 97/50
2025-10-02 18:45:03.845669: train_loss -0.8369
2025-10-02 18:45:03.845828: val_loss -0.2571
2025-10-02 18:45:03.846028: Pseudo dice [np.float32(0.6849)]
2025-10-02 18:45:03.846186: Epoch time: 45.98 s
2025-10-02 18:45:04.482982: 
2025-10-02 18:45:04.483281: Epoch 146
2025-10-02 18:45:04.483559: Current learning rate: 0.00038
2025-10-02 18:45:50.503140: Validation loss did not improve from -0.40075. Patience: 98/50
2025-10-02 18:45:50.504189: train_loss -0.8372
2025-10-02 18:45:50.504504: val_loss -0.199
2025-10-02 18:45:50.504837: Pseudo dice [np.float32(0.6853)]
2025-10-02 18:45:50.505169: Epoch time: 46.02 s
2025-10-02 18:45:51.145259: 
2025-10-02 18:45:51.145591: Epoch 147
2025-10-02 18:45:51.145859: Current learning rate: 0.0003
2025-10-02 18:46:37.143516: Validation loss did not improve from -0.40075. Patience: 99/50
2025-10-02 18:46:37.144095: train_loss -0.8364
2025-10-02 18:46:37.144321: val_loss -0.2359
2025-10-02 18:46:37.144568: Pseudo dice [np.float32(0.7049)]
2025-10-02 18:46:37.144731: Epoch time: 46.0 s
2025-10-02 18:46:37.839014: 
2025-10-02 18:46:37.839328: Epoch 148
2025-10-02 18:46:37.839512: Current learning rate: 0.00021
2025-10-02 18:47:23.848264: Validation loss did not improve from -0.40075. Patience: 100/50
2025-10-02 18:47:23.848843: train_loss -0.8445
2025-10-02 18:47:23.849025: val_loss -0.2454
2025-10-02 18:47:23.849180: Pseudo dice [np.float32(0.7049)]
2025-10-02 18:47:23.849355: Epoch time: 46.01 s
2025-10-02 18:47:24.483590: 
2025-10-02 18:47:24.483801: Epoch 149
2025-10-02 18:47:24.484005: Current learning rate: 0.00011
2025-10-02 18:48:10.478990: Validation loss did not improve from -0.40075. Patience: 101/50
2025-10-02 18:48:10.479366: train_loss -0.8392
2025-10-02 18:48:10.479515: val_loss -0.2341
2025-10-02 18:48:10.479635: Pseudo dice [np.float32(0.6934)]
2025-10-02 18:48:10.479770: Epoch time: 46.0 s
2025-10-02 18:48:11.600353: Training done.
2025-10-02 18:48:11.610955: Using splits from existing split file: /nfs/erelab001/shared/Computational_Group/Naravich/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset310_nnInteractive_Calcium_OCT_CrossValidation/splits_final.json
2025-10-02 18:48:11.611339: The split file contains 5 splits.
2025-10-02 18:48:11.611514: Desired fold for training: 0
2025-10-02 18:48:11.611693: This split has 6 training and 2 validation cases.
2025-10-02 18:48:11.611867: predicting 106-002
2025-10-02 18:48:11.614085: 106-002, shape torch.Size([1, 539, 498, 498]), rank 0
2025-10-02 18:49:13.372857: predicting 706-005
2025-10-02 18:49:13.380768: 706-005, shape torch.Size([1, 375, 498, 498]), rank 0
2025-10-02 18:50:00.896281: Validation complete
2025-10-02 18:50:00.896669: Mean Validation Dice:  0.6989031190153095
Finished training fold 0 saving to /nfs/erelab001/shared/Computational_Group/Naravich/datasets/nnUNet_Datasets/nnUNet_results/Dataset310_nnInteractive_Calcium_OCT_CrossValidation/nnUNetTrainer__nnUNetPlans__3d_32x160x128_b10/fold_0_No_Pretrained
