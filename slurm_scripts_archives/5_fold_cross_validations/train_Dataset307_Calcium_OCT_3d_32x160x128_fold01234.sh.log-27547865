/home/gridsan/nchutisilp/.conda/envs/nnunet/bin/python

############################
INFO: You are using the old nnU-Net default plans. We have updated our recommendations. Please consider using those instead! Read more here: https://github.com/MIC-DKFZ/nnUNet/blob/master/documentation/resenc_presets.md
############################

Using device: cuda:0

#######################################################################
Please cite the following paper when using nnU-Net:
Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.
#######################################################################

2024-12-04 15:29:57.625319: do_dummy_2d_data_aug: True
2024-12-04 15:29:57.628617: Using splits from existing split file: /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset307_Sohee_Calcium_OCT_CrossValidation/splits_final.json
2024-12-04 15:29:57.630731: The split file contains 5 splits.
2024-12-04 15:29:57.631802: Desired fold for training: 1
2024-12-04 15:29:57.632823: This split has 6 training and 2 validation cases.

############################
INFO: You are using the old nnU-Net default plans. We have updated our recommendations. Please consider using those instead! Read more here: https://github.com/MIC-DKFZ/nnUNet/blob/master/documentation/resenc_presets.md
############################

Using device: cuda:0

#######################################################################
Please cite the following paper when using nnU-Net:
Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.
#######################################################################

2024-12-04 15:29:57.625317: do_dummy_2d_data_aug: True
2024-12-04 15:29:57.628611: Using splits from existing split file: /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset307_Sohee_Calcium_OCT_CrossValidation/splits_final.json
2024-12-04 15:29:57.630870: The split file contains 5 splits.
2024-12-04 15:29:57.632047: Desired fold for training: 0
2024-12-04 15:29:57.632958: This split has 6 training and 2 validation cases.
using pin_memory on device 0
using pin_memory on device 0
using pin_memory on device 0
2024-12-04 15:30:01.718096: Using torch.compile...
using pin_memory on device 0
2024-12-04 15:30:03.188131: Using torch.compile...
/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "

This is the configuration used by this training:
Configuration name: 3d_32x160x128_b10
 {'data_identifier': 'nnUNetPlans_3d_fullres', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 10, 'patch_size': [32, 160, 128], 'median_image_size_in_voxels': [375.0, 498.0, 498.0], 'spacing': [1.0, 1.0, 1.0], 'normalization_schemes': ['ZScoreNormalization'], 'use_mask_for_norm': [False], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'architecture': {'network_class_name': 'dynamic_network_architectures.architectures.unet.PlainConvUNet', 'arch_kwargs': {'n_stages': 6, 'features_per_stage': [32, 64, 128, 256, 320, 320], 'conv_op': 'torch.nn.modules.conv.Conv3d', 'kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]], 'strides': [[1, 1, 1], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [1, 2, 2]], 'n_conv_per_stage': [2, 2, 2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2, 2, 2], 'conv_bias': True, 'norm_op': 'torch.nn.modules.instancenorm.InstanceNorm3d', 'norm_op_kwargs': {'eps': 1e-05, 'affine': True}, 'dropout_op': None, 'dropout_op_kwargs': None, 'nonlin': 'torch.nn.LeakyReLU', 'nonlin_kwargs': {'inplace': True}, 'deep_supervision': True}, '_kw_requires_import': ['conv_op', 'norm_op', 'dropout_op', 'nonlin']}, 'batch_dice': True, 'inherits_from': '3d_fullres'} 

These are the global plan.json settings:
 {'dataset_name': 'Dataset307_Sohee_Calcium_OCT_CrossValidation', 'plans_name': 'nnUNetPlans', 'original_median_spacing_after_transp': [1.0, 1.0, 1.0], 'original_median_shape_after_transp': [375, 498, 498], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 0.4977976381778717, 'mean': 0.13507525622844696, 'median': 0.09599608182907104, 'min': 0.0, 'percentile_00_5': 0.014754901640117168, 'percentile_99_5': 0.4977976381778717, 'std': 0.12025152146816254}}} 

2024-12-04 15:30:03.741218: unpacking dataset...
/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "

This is the configuration used by this training:
Configuration name: 3d_32x160x128_b10
 {'data_identifier': 'nnUNetPlans_3d_fullres', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 10, 'patch_size': [32, 160, 128], 'median_image_size_in_voxels': [375.0, 498.0, 498.0], 'spacing': [1.0, 1.0, 1.0], 'normalization_schemes': ['ZScoreNormalization'], 'use_mask_for_norm': [False], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'architecture': {'network_class_name': 'dynamic_network_architectures.architectures.unet.PlainConvUNet', 'arch_kwargs': {'n_stages': 6, 'features_per_stage': [32, 64, 128, 256, 320, 320], 'conv_op': 'torch.nn.modules.conv.Conv3d', 'kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]], 'strides': [[1, 1, 1], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [1, 2, 2]], 'n_conv_per_stage': [2, 2, 2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2, 2, 2], 'conv_bias': True, 'norm_op': 'torch.nn.modules.instancenorm.InstanceNorm3d', 'norm_op_kwargs': {'eps': 1e-05, 'affine': True}, 'dropout_op': None, 'dropout_op_kwargs': None, 'nonlin': 'torch.nn.LeakyReLU', 'nonlin_kwargs': {'inplace': True}, 'deep_supervision': True}, '_kw_requires_import': ['conv_op', 'norm_op', 'dropout_op', 'nonlin']}, 'batch_dice': True, 'inherits_from': '3d_fullres'} 

These are the global plan.json settings:
 {'dataset_name': 'Dataset307_Sohee_Calcium_OCT_CrossValidation', 'plans_name': 'nnUNetPlans', 'original_median_spacing_after_transp': [1.0, 1.0, 1.0], 'original_median_shape_after_transp': [375, 498, 498], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 0.4977976381778717, 'mean': 0.13507525622844696, 'median': 0.09599608182907104, 'min': 0.0, 'percentile_00_5': 0.014754901640117168, 'percentile_99_5': 0.4977976381778717, 'std': 0.12025152146816254}}} 

2024-12-04 15:30:04.052849: unpacking dataset...
2024-12-04 15:30:09.174506: unpacking done...
2024-12-04 15:30:09.190457: Unable to plot network architecture: nnUNet_compile is enabled!
2024-12-04 15:30:09.266202: 
2024-12-04 15:30:09.267983: Epoch 0
2024-12-04 15:30:09.269382: Current learning rate: 0.01
2024-12-04 15:32:45.051415: Validation loss improved from 1000.00000 to -0.21527! Patience: 0/50
2024-12-04 15:32:45.052705: train_loss -0.0875
2024-12-04 15:32:45.053792: val_loss -0.2153
2024-12-04 15:32:45.054602: Pseudo dice [0.5892]
2024-12-04 15:32:45.055412: Epoch time: 155.79 s
2024-12-04 15:32:45.056167: Yayy! New best EMA pseudo Dice: 0.5892
2024-12-04 15:32:46.609572: 
2024-12-04 15:32:46.610811: Epoch 1
2024-12-04 15:32:46.611551: Current learning rate: 0.00999
2024-12-04 15:34:14.259245: Validation loss improved from -0.21527 to -0.26228! Patience: 0/50
2024-12-04 15:34:14.261510: train_loss -0.2254
2024-12-04 15:34:14.262387: val_loss -0.2623
2024-12-04 15:34:14.263249: Pseudo dice [0.601]
2024-12-04 15:34:14.264027: Epoch time: 87.65 s
2024-12-04 15:34:14.264825: Yayy! New best EMA pseudo Dice: 0.5904
2024-12-04 15:34:15.943599: 
2024-12-04 15:34:15.944892: Epoch 2
2024-12-04 15:34:15.945749: Current learning rate: 0.00998
2024-12-04 15:35:44.499368: Validation loss improved from -0.26228 to -0.26594! Patience: 0/50
2024-12-04 15:35:44.500347: train_loss -0.283
2024-12-04 15:35:44.501136: val_loss -0.2659
2024-12-04 15:35:44.501840: Pseudo dice [0.6101]
2024-12-04 15:35:44.502455: Epoch time: 88.56 s
2024-12-04 15:35:44.503168: Yayy! New best EMA pseudo Dice: 0.5924
2024-12-04 15:35:46.299084: 
2024-12-04 15:35:46.300399: Epoch 3
2024-12-04 15:35:46.301034: Current learning rate: 0.00997
2024-12-04 15:37:15.263627: Validation loss improved from -0.26594 to -0.33579! Patience: 0/50
2024-12-04 15:37:15.264518: train_loss -0.3345
2024-12-04 15:37:15.265495: val_loss -0.3358
2024-12-04 15:37:15.266324: Pseudo dice [0.641]
2024-12-04 15:37:15.267224: Epoch time: 88.97 s
2024-12-04 15:37:15.268145: Yayy! New best EMA pseudo Dice: 0.5972
2024-12-04 15:37:16.967662: 
2024-12-04 15:37:16.968823: Epoch 4
2024-12-04 15:37:16.969540: Current learning rate: 0.00996
2024-12-04 15:38:45.926035: Validation loss did not improve from -0.33579. Patience: 1/50
2024-12-04 15:38:45.928020: train_loss -0.3719
2024-12-04 15:38:45.928900: val_loss -0.2718
2024-12-04 15:38:45.929728: Pseudo dice [0.603]
2024-12-04 15:38:45.930537: Epoch time: 88.96 s
2024-12-04 15:38:46.292585: Yayy! New best EMA pseudo Dice: 0.5978
2024-12-04 15:38:48.024284: 
2024-12-04 15:38:48.025797: Epoch 5
2024-12-04 15:38:48.026644: Current learning rate: 0.00995
2024-12-04 15:40:16.979925: Validation loss did not improve from -0.33579. Patience: 2/50
2024-12-04 15:40:16.980805: train_loss -0.4041
2024-12-04 15:40:16.981810: val_loss -0.3244
2024-12-04 15:40:16.982750: Pseudo dice [0.659]
2024-12-04 15:40:16.983702: Epoch time: 88.96 s
2024-12-04 15:40:16.984613: Yayy! New best EMA pseudo Dice: 0.6039
2024-12-04 15:40:18.772154: 
2024-12-04 15:40:18.774361: Epoch 6
2024-12-04 15:40:18.775290: Current learning rate: 0.00995
2024-12-04 15:41:46.932661: Validation loss improved from -0.33579 to -0.40250! Patience: 2/50
2024-12-04 15:41:46.933681: train_loss -0.4228
2024-12-04 15:41:46.934402: val_loss -0.4025
2024-12-04 15:41:46.935133: Pseudo dice [0.6885]
2024-12-04 15:41:46.935703: Epoch time: 88.16 s
2024-12-04 15:41:46.936352: Yayy! New best EMA pseudo Dice: 0.6124
2024-12-04 15:41:48.631650: 
2024-12-04 15:41:48.632738: Epoch 7
2024-12-04 15:41:48.633472: Current learning rate: 0.00994
2024-12-04 15:43:16.145060: Validation loss did not improve from -0.40250. Patience: 1/50
2024-12-04 15:43:16.147292: train_loss -0.4401
2024-12-04 15:43:16.148389: val_loss -0.3974
2024-12-04 15:43:16.149169: Pseudo dice [0.687]
2024-12-04 15:43:16.150094: Epoch time: 87.52 s
2024-12-04 15:43:16.150955: Yayy! New best EMA pseudo Dice: 0.6198
2024-12-04 15:43:18.442251: 
2024-12-04 15:43:18.444095: Epoch 8
2024-12-04 15:43:18.445265: Current learning rate: 0.00993
2024-12-04 15:44:47.009753: Validation loss improved from -0.40250 to -0.42697! Patience: 1/50
2024-12-04 15:44:47.010870: train_loss -0.4485
2024-12-04 15:44:47.011585: val_loss -0.427
2024-12-04 15:44:47.012251: Pseudo dice [0.7026]
2024-12-04 15:44:47.012941: Epoch time: 88.57 s
2024-12-04 15:44:47.013640: Yayy! New best EMA pseudo Dice: 0.6281
2024-12-04 15:44:48.804536: 
2024-12-04 15:44:48.806061: Epoch 9
2024-12-04 15:44:48.807069: Current learning rate: 0.00992
2024-12-04 15:46:16.840095: Validation loss improved from -0.42697 to -0.46317! Patience: 0/50
2024-12-04 15:46:16.841190: train_loss -0.4709
2024-12-04 15:46:16.842073: val_loss -0.4632
2024-12-04 15:46:16.842711: Pseudo dice [0.718]
2024-12-04 15:46:16.843350: Epoch time: 88.04 s
2024-12-04 15:46:17.241035: Yayy! New best EMA pseudo Dice: 0.6371
2024-12-04 15:46:18.916208: 
2024-12-04 15:46:18.918102: Epoch 10
2024-12-04 15:46:18.919070: Current learning rate: 0.00991
2024-12-04 15:47:47.546273: Validation loss did not improve from -0.46317. Patience: 1/50
2024-12-04 15:47:47.547415: train_loss -0.4784
2024-12-04 15:47:47.548450: val_loss -0.4516
2024-12-04 15:47:47.549256: Pseudo dice [0.7129]
2024-12-04 15:47:47.550055: Epoch time: 88.63 s
2024-12-04 15:47:47.550683: Yayy! New best EMA pseudo Dice: 0.6447
2024-12-04 15:47:49.260065: 
2024-12-04 15:47:49.261646: Epoch 11
2024-12-04 15:47:49.262441: Current learning rate: 0.0099
2024-12-04 15:49:18.002015: Validation loss did not improve from -0.46317. Patience: 2/50
2024-12-04 15:49:18.003000: train_loss -0.4792
2024-12-04 15:49:18.003773: val_loss -0.4428
2024-12-04 15:49:18.004476: Pseudo dice [0.7221]
2024-12-04 15:49:18.005197: Epoch time: 88.74 s
2024-12-04 15:49:18.005803: Yayy! New best EMA pseudo Dice: 0.6524
2024-12-04 15:49:19.744237: 
2024-12-04 15:49:19.745879: Epoch 12
2024-12-04 15:49:19.747175: Current learning rate: 0.00989
2024-12-04 15:50:47.199038: Validation loss improved from -0.46317 to -0.46537! Patience: 2/50
2024-12-04 15:50:47.201519: train_loss -0.4815
2024-12-04 15:50:47.202852: val_loss -0.4654
2024-12-04 15:50:47.203807: Pseudo dice [0.7237]
2024-12-04 15:50:47.204854: Epoch time: 87.46 s
2024-12-04 15:50:47.205731: Yayy! New best EMA pseudo Dice: 0.6596
2024-12-04 15:50:48.925128: 
2024-12-04 15:50:48.926756: Epoch 13
2024-12-04 15:50:48.927847: Current learning rate: 0.00988
2024-12-04 15:52:16.266470: Validation loss did not improve from -0.46537. Patience: 1/50
2024-12-04 15:52:16.267436: train_loss -0.4939
2024-12-04 15:52:16.268181: val_loss -0.4281
2024-12-04 15:52:16.268865: Pseudo dice [0.7022]
2024-12-04 15:52:16.269491: Epoch time: 87.34 s
2024-12-04 15:52:16.270187: Yayy! New best EMA pseudo Dice: 0.6638
2024-12-04 15:52:17.897827: 
2024-12-04 15:52:17.899207: Epoch 14
2024-12-04 15:52:17.900450: Current learning rate: 0.00987
2024-12-04 15:53:45.165232: Validation loss improved from -0.46537 to -0.48375! Patience: 1/50
2024-12-04 15:53:45.166004: train_loss -0.5084
2024-12-04 15:53:45.166813: val_loss -0.4837
2024-12-04 15:53:45.167639: Pseudo dice [0.7402]
2024-12-04 15:53:45.168399: Epoch time: 87.27 s
2024-12-04 15:53:45.565786: Yayy! New best EMA pseudo Dice: 0.6715
2024-12-04 15:53:47.305122: 
2024-12-04 15:53:47.306809: Epoch 15
2024-12-04 15:53:47.308091: Current learning rate: 0.00986
2024-12-04 15:55:14.642193: Validation loss did not improve from -0.48375. Patience: 1/50
2024-12-04 15:55:14.643035: train_loss -0.5116
2024-12-04 15:55:14.643843: val_loss -0.4606
2024-12-04 15:55:14.644608: Pseudo dice [0.726]
2024-12-04 15:55:14.645435: Epoch time: 87.34 s
2024-12-04 15:55:14.646178: Yayy! New best EMA pseudo Dice: 0.6769
2024-12-04 15:55:16.282074: 
2024-12-04 15:55:16.283363: Epoch 16
2024-12-04 15:55:16.284132: Current learning rate: 0.00986
2024-12-04 15:56:44.006294: Validation loss did not improve from -0.48375. Patience: 2/50
2024-12-04 15:56:44.007024: train_loss -0.5213
2024-12-04 15:56:44.008118: val_loss -0.4542
2024-12-04 15:56:44.009128: Pseudo dice [0.7315]
2024-12-04 15:56:44.010086: Epoch time: 87.73 s
2024-12-04 15:56:44.011054: Yayy! New best EMA pseudo Dice: 0.6824
2024-12-04 15:56:45.747707: 
2024-12-04 15:56:45.749876: Epoch 17
2024-12-04 15:56:45.751189: Current learning rate: 0.00985
2024-12-04 15:58:13.369851: Validation loss improved from -0.48375 to -0.51107! Patience: 2/50
2024-12-04 15:58:13.370723: train_loss -0.5305
2024-12-04 15:58:13.371537: val_loss -0.5111
2024-12-04 15:58:13.372420: Pseudo dice [0.7536]
2024-12-04 15:58:13.373336: Epoch time: 87.62 s
2024-12-04 15:58:13.374180: Yayy! New best EMA pseudo Dice: 0.6895
2024-12-04 15:58:15.033847: 
2024-12-04 15:58:15.035813: Epoch 18
2024-12-04 15:58:15.036857: Current learning rate: 0.00984
2024-12-04 15:59:42.803076: Validation loss did not improve from -0.51107. Patience: 1/50
2024-12-04 15:59:42.804096: train_loss -0.5204
2024-12-04 15:59:42.805079: val_loss -0.4376
2024-12-04 15:59:42.805827: Pseudo dice [0.7177]
2024-12-04 15:59:42.806601: Epoch time: 87.77 s
2024-12-04 15:59:42.807294: Yayy! New best EMA pseudo Dice: 0.6923
2024-12-04 15:59:45.008158: 
2024-12-04 15:59:45.009573: Epoch 19
2024-12-04 15:59:45.010631: Current learning rate: 0.00983
2024-12-04 16:01:12.647932: Validation loss did not improve from -0.51107. Patience: 2/50
2024-12-04 16:01:12.648902: train_loss -0.5278
2024-12-04 16:01:12.649985: val_loss -0.508
2024-12-04 16:01:12.650886: Pseudo dice [0.7526]
2024-12-04 16:01:12.651639: Epoch time: 87.64 s
2024-12-04 16:01:13.036059: Yayy! New best EMA pseudo Dice: 0.6983
2024-12-04 16:01:14.675981: 
2024-12-04 16:01:14.677548: Epoch 20
2024-12-04 16:01:14.678626: Current learning rate: 0.00982
2024-12-04 16:02:42.288301: Validation loss did not improve from -0.51107. Patience: 3/50
2024-12-04 16:02:42.289317: train_loss -0.5386
2024-12-04 16:02:42.290060: val_loss -0.483
2024-12-04 16:02:42.290836: Pseudo dice [0.7479]
2024-12-04 16:02:42.291543: Epoch time: 87.61 s
2024-12-04 16:02:42.292383: Yayy! New best EMA pseudo Dice: 0.7033
2024-12-04 16:02:43.935375: 
2024-12-04 16:02:43.936230: Epoch 21
2024-12-04 16:02:43.937248: Current learning rate: 0.00981
2024-12-04 16:04:11.516630: Validation loss did not improve from -0.51107. Patience: 4/50
2024-12-04 16:04:11.517596: train_loss -0.5561
2024-12-04 16:04:11.518381: val_loss -0.5019
2024-12-04 16:04:11.519191: Pseudo dice [0.7576]
2024-12-04 16:04:11.519906: Epoch time: 87.58 s
2024-12-04 16:04:11.520571: Yayy! New best EMA pseudo Dice: 0.7087
2024-12-04 16:04:13.089296: 
2024-12-04 16:04:13.090996: Epoch 22
2024-12-04 16:04:13.091823: Current learning rate: 0.0098
2024-12-04 16:05:40.739547: Validation loss did not improve from -0.51107. Patience: 5/50
2024-12-04 16:05:40.740675: train_loss -0.5632
2024-12-04 16:05:40.741560: val_loss -0.448
2024-12-04 16:05:40.742576: Pseudo dice [0.7157]
2024-12-04 16:05:40.743312: Epoch time: 87.65 s
2024-12-04 16:05:40.743974: Yayy! New best EMA pseudo Dice: 0.7094
2024-12-04 16:05:42.322583: 
2024-12-04 16:05:42.323983: Epoch 23
2024-12-04 16:05:42.324718: Current learning rate: 0.00979
2024-12-04 16:07:09.938775: Validation loss did not improve from -0.51107. Patience: 6/50
2024-12-04 16:07:09.939775: train_loss -0.57
2024-12-04 16:07:09.940871: val_loss -0.4952
2024-12-04 16:07:09.941732: Pseudo dice [0.7514]
2024-12-04 16:07:09.942601: Epoch time: 87.62 s
2024-12-04 16:07:09.943401: Yayy! New best EMA pseudo Dice: 0.7136
2024-12-04 16:07:11.550052: 
2024-12-04 16:07:11.551507: Epoch 24
2024-12-04 16:07:11.552419: Current learning rate: 0.00978
2024-12-04 16:08:39.514585: Validation loss did not improve from -0.51107. Patience: 7/50
2024-12-04 16:08:39.515273: train_loss -0.5711
2024-12-04 16:08:39.515944: val_loss -0.5024
2024-12-04 16:08:39.516498: Pseudo dice [0.7514]
2024-12-04 16:08:39.517087: Epoch time: 87.97 s
2024-12-04 16:08:39.903632: Yayy! New best EMA pseudo Dice: 0.7174
2024-12-04 16:08:41.466975: 
2024-12-04 16:08:41.468322: Epoch 25
2024-12-04 16:08:41.469190: Current learning rate: 0.00977
2024-12-04 16:10:09.457465: Validation loss did not improve from -0.51107. Patience: 8/50
2024-12-04 16:10:09.458418: train_loss -0.5647
2024-12-04 16:10:09.459528: val_loss -0.4517
2024-12-04 16:10:09.460399: Pseudo dice [0.7343]
2024-12-04 16:10:09.461249: Epoch time: 87.99 s
2024-12-04 16:10:09.462147: Yayy! New best EMA pseudo Dice: 0.7191
2024-12-04 16:10:11.111565: 
2024-12-04 16:10:11.113524: Epoch 26
2024-12-04 16:10:11.114311: Current learning rate: 0.00977
2024-12-04 16:11:38.730993: Validation loss did not improve from -0.51107. Patience: 9/50
2024-12-04 16:11:38.732052: train_loss -0.5751
2024-12-04 16:11:38.732849: val_loss -0.511
2024-12-04 16:11:38.733549: Pseudo dice [0.7434]
2024-12-04 16:11:38.734277: Epoch time: 87.62 s
2024-12-04 16:11:38.734905: Yayy! New best EMA pseudo Dice: 0.7215
2024-12-04 16:11:40.321438: 
2024-12-04 16:11:40.323061: Epoch 27
2024-12-04 16:11:40.324175: Current learning rate: 0.00976
2024-12-04 16:13:07.956323: Validation loss did not improve from -0.51107. Patience: 10/50
2024-12-04 16:13:07.957267: train_loss -0.5803
2024-12-04 16:13:07.958127: val_loss -0.5099
2024-12-04 16:13:07.958931: Pseudo dice [0.7519]
2024-12-04 16:13:07.959755: Epoch time: 87.64 s
2024-12-04 16:13:07.960404: Yayy! New best EMA pseudo Dice: 0.7246
2024-12-04 16:13:09.545912: 
2024-12-04 16:13:09.547304: Epoch 28
2024-12-04 16:13:09.548098: Current learning rate: 0.00975
2024-12-04 16:14:37.141605: Validation loss did not improve from -0.51107. Patience: 11/50
2024-12-04 16:14:37.142727: train_loss -0.5764
2024-12-04 16:14:37.143501: val_loss -0.5044
2024-12-04 16:14:37.144149: Pseudo dice [0.7507]
2024-12-04 16:14:37.144755: Epoch time: 87.6 s
2024-12-04 16:14:37.145316: Yayy! New best EMA pseudo Dice: 0.7272
2024-12-04 16:14:39.017370: 
2024-12-04 16:14:39.018132: Epoch 29
2024-12-04 16:14:39.018899: Current learning rate: 0.00974
2024-12-04 16:16:06.615923: Validation loss did not improve from -0.51107. Patience: 12/50
2024-12-04 16:16:06.616834: train_loss -0.5883
2024-12-04 16:16:06.617904: val_loss -0.5013
2024-12-04 16:16:06.618687: Pseudo dice [0.7427]
2024-12-04 16:16:06.619565: Epoch time: 87.6 s
2024-12-04 16:16:07.029372: Yayy! New best EMA pseudo Dice: 0.7287
2024-12-04 16:16:08.712001: 
2024-12-04 16:16:08.713279: Epoch 30
2024-12-04 16:16:08.714239: Current learning rate: 0.00973
2024-12-04 16:17:36.305183: Validation loss did not improve from -0.51107. Patience: 13/50
2024-12-04 16:17:36.306317: train_loss -0.5875
2024-12-04 16:17:36.307463: val_loss -0.4751
2024-12-04 16:17:36.308271: Pseudo dice [0.7195]
2024-12-04 16:17:36.309119: Epoch time: 87.6 s
2024-12-04 16:17:37.556276: 
2024-12-04 16:17:37.557459: Epoch 31
2024-12-04 16:17:37.558317: Current learning rate: 0.00972
2024-12-04 16:19:05.098570: Validation loss improved from -0.51107 to -0.52640! Patience: 13/50
2024-12-04 16:19:05.099566: train_loss -0.6021
2024-12-04 16:19:05.100401: val_loss -0.5264
2024-12-04 16:19:05.101144: Pseudo dice [0.7675]
2024-12-04 16:19:05.101863: Epoch time: 87.54 s
2024-12-04 16:19:05.102634: Yayy! New best EMA pseudo Dice: 0.7318
2024-12-04 16:19:06.772364: 
2024-12-04 16:19:06.773822: Epoch 32
2024-12-04 16:19:06.774615: Current learning rate: 0.00971
2024-12-04 16:20:34.203321: Validation loss did not improve from -0.52640. Patience: 1/50
2024-12-04 16:20:34.204085: train_loss -0.6031
2024-12-04 16:20:34.204897: val_loss -0.3427
2024-12-04 16:20:34.205696: Pseudo dice [0.6583]
2024-12-04 16:20:34.206501: Epoch time: 87.43 s
2024-12-04 16:20:35.451561: 
2024-12-04 16:20:35.453312: Epoch 33
2024-12-04 16:20:35.454445: Current learning rate: 0.0097
2024-12-04 16:22:02.934472: Validation loss did not improve from -0.52640. Patience: 2/50
2024-12-04 16:22:02.935410: train_loss -0.5806
2024-12-04 16:22:02.936390: val_loss -0.4707
2024-12-04 16:22:02.937263: Pseudo dice [0.7385]
2024-12-04 16:22:02.938053: Epoch time: 87.48 s
2024-12-04 16:22:04.151498: 
2024-12-04 16:22:04.153002: Epoch 34
2024-12-04 16:22:04.154098: Current learning rate: 0.00969
2024-12-04 16:23:31.738609: Validation loss did not improve from -0.52640. Patience: 3/50
2024-12-04 16:23:31.739555: train_loss -0.5994
2024-12-04 16:23:31.740586: val_loss -0.5142
2024-12-04 16:23:31.741423: Pseudo dice [0.757]
2024-12-04 16:23:31.742179: Epoch time: 87.59 s
2024-12-04 16:23:33.364110: 
2024-12-04 16:23:33.365532: Epoch 35
2024-12-04 16:23:33.366451: Current learning rate: 0.00968
2024-12-04 16:25:00.653501: Validation loss did not improve from -0.52640. Patience: 4/50
2024-12-04 16:25:00.654280: train_loss -0.612
2024-12-04 16:25:00.655104: val_loss -0.4644
2024-12-04 16:25:00.655850: Pseudo dice [0.7388]
2024-12-04 16:25:00.656811: Epoch time: 87.29 s
2024-12-04 16:25:01.913508: 
2024-12-04 16:25:01.914982: Epoch 36
2024-12-04 16:25:01.916103: Current learning rate: 0.00968
2024-12-04 16:26:29.190695: Validation loss did not improve from -0.52640. Patience: 5/50
2024-12-04 16:26:29.191676: train_loss -0.6128
2024-12-04 16:26:29.192381: val_loss -0.5135
2024-12-04 16:26:29.192993: Pseudo dice [0.7531]
2024-12-04 16:26:29.193636: Epoch time: 87.28 s
2024-12-04 16:26:29.194247: Yayy! New best EMA pseudo Dice: 0.7323
2024-12-04 16:26:30.809081: 
2024-12-04 16:26:30.810990: Epoch 37
2024-12-04 16:26:30.812126: Current learning rate: 0.00967
2024-12-04 16:27:58.224936: Validation loss did not improve from -0.52640. Patience: 6/50
2024-12-04 16:27:58.225803: train_loss -0.6048
2024-12-04 16:27:58.226845: val_loss -0.5011
2024-12-04 16:27:58.227623: Pseudo dice [0.7445]
2024-12-04 16:27:58.228277: Epoch time: 87.42 s
2024-12-04 16:27:58.228944: Yayy! New best EMA pseudo Dice: 0.7335
2024-12-04 16:27:59.864657: 
2024-12-04 16:27:59.865915: Epoch 38
2024-12-04 16:27:59.866668: Current learning rate: 0.00966
2024-12-04 16:29:27.236386: Validation loss did not improve from -0.52640. Patience: 7/50
2024-12-04 16:29:27.237190: train_loss -0.6083
2024-12-04 16:29:27.238257: val_loss -0.4902
2024-12-04 16:29:27.239259: Pseudo dice [0.7441]
2024-12-04 16:29:27.240344: Epoch time: 87.37 s
2024-12-04 16:29:27.241382: Yayy! New best EMA pseudo Dice: 0.7345
2024-12-04 16:29:28.931572: 
2024-12-04 16:29:28.933161: Epoch 39
2024-12-04 16:29:28.934128: Current learning rate: 0.00965
2024-12-04 16:30:56.193364: Validation loss improved from -0.52640 to -0.53309! Patience: 7/50
2024-12-04 16:30:56.194394: train_loss -0.6096
2024-12-04 16:30:56.195364: val_loss -0.5331
2024-12-04 16:30:56.196263: Pseudo dice [0.7698]
2024-12-04 16:30:56.197008: Epoch time: 87.26 s
2024-12-04 16:30:56.606489: Yayy! New best EMA pseudo Dice: 0.7381
2024-12-04 16:30:58.578742: 
2024-12-04 16:30:58.580252: Epoch 40
2024-12-04 16:30:58.580993: Current learning rate: 0.00964
2024-12-04 16:32:25.884818: Validation loss improved from -0.53309 to -0.53456! Patience: 0/50
2024-12-04 16:32:25.885744: train_loss -0.6198
2024-12-04 16:32:25.886571: val_loss -0.5346
2024-12-04 16:32:25.887254: Pseudo dice [0.767]
2024-12-04 16:32:25.887887: Epoch time: 87.31 s
2024-12-04 16:32:25.888469: Yayy! New best EMA pseudo Dice: 0.741
2024-12-04 16:32:27.558950: 
2024-12-04 16:32:27.560597: Epoch 41
2024-12-04 16:32:27.561339: Current learning rate: 0.00963
2024-12-04 16:33:54.948490: Validation loss did not improve from -0.53456. Patience: 1/50
2024-12-04 16:33:54.949210: train_loss -0.6198
2024-12-04 16:33:54.950031: val_loss -0.5114
2024-12-04 16:33:54.950835: Pseudo dice [0.7558]
2024-12-04 16:33:54.951673: Epoch time: 87.39 s
2024-12-04 16:33:54.952299: Yayy! New best EMA pseudo Dice: 0.7424
2024-12-04 16:33:56.509149: 
2024-12-04 16:33:56.510957: Epoch 42
2024-12-04 16:33:56.511981: Current learning rate: 0.00962
2024-12-04 16:35:23.871538: Validation loss did not improve from -0.53456. Patience: 2/50
2024-12-04 16:35:23.872807: train_loss -0.6277
2024-12-04 16:35:23.873734: val_loss -0.4805
2024-12-04 16:35:23.874649: Pseudo dice [0.7397]
2024-12-04 16:35:23.875407: Epoch time: 87.36 s
2024-12-04 16:35:25.082163: 
2024-12-04 16:35:25.083974: Epoch 43
2024-12-04 16:35:25.084929: Current learning rate: 0.00961
2024-12-04 16:36:52.799127: Validation loss did not improve from -0.53456. Patience: 3/50
2024-12-04 16:36:52.799843: train_loss -0.628
2024-12-04 16:36:52.800621: val_loss -0.5149
2024-12-04 16:36:52.801225: Pseudo dice [0.7617]
2024-12-04 16:36:52.801892: Epoch time: 87.72 s
2024-12-04 16:36:52.802530: Yayy! New best EMA pseudo Dice: 0.7441
2024-12-04 16:36:54.398373: 
2024-12-04 16:36:54.399776: Epoch 44
2024-12-04 16:36:54.400590: Current learning rate: 0.0096
2024-12-04 16:38:21.965020: Validation loss did not improve from -0.53456. Patience: 4/50
2024-12-04 16:38:21.965810: train_loss -0.6141
2024-12-04 16:38:21.966707: val_loss -0.5196
2024-12-04 16:38:21.967465: Pseudo dice [0.7554]
2024-12-04 16:38:21.968316: Epoch time: 87.57 s
2024-12-04 16:38:22.320507: Yayy! New best EMA pseudo Dice: 0.7453
2024-12-04 16:38:23.863921: 
2024-12-04 16:38:23.865593: Epoch 45
2024-12-04 16:38:23.866940: Current learning rate: 0.00959
2024-12-04 16:39:51.211941: Validation loss did not improve from -0.53456. Patience: 5/50
2024-12-04 16:39:51.212770: train_loss -0.6179
2024-12-04 16:39:51.213746: val_loss -0.5121
2024-12-04 16:39:51.214451: Pseudo dice [0.7582]
2024-12-04 16:39:51.215075: Epoch time: 87.35 s
2024-12-04 16:39:51.215719: Yayy! New best EMA pseudo Dice: 0.7466
2024-12-04 16:39:52.795681: 
2024-12-04 16:39:52.797223: Epoch 46
2024-12-04 16:39:52.798116: Current learning rate: 0.00959
2024-12-04 16:41:20.121854: Validation loss did not improve from -0.53456. Patience: 6/50
2024-12-04 16:41:20.122634: train_loss -0.6072
2024-12-04 16:41:20.123756: val_loss -0.5192
2024-12-04 16:41:20.124555: Pseudo dice [0.7691]
2024-12-04 16:41:20.125474: Epoch time: 87.33 s
2024-12-04 16:41:20.126238: Yayy! New best EMA pseudo Dice: 0.7488
2024-12-04 16:41:21.661514: 
2024-12-04 16:41:21.662677: Epoch 47
2024-12-04 16:41:21.663504: Current learning rate: 0.00958
2024-12-04 16:42:49.043579: Validation loss did not improve from -0.53456. Patience: 7/50
2024-12-04 16:42:49.044473: train_loss -0.622
2024-12-04 16:42:49.045356: val_loss -0.5221
2024-12-04 16:42:49.046080: Pseudo dice [0.7599]
2024-12-04 16:42:49.046682: Epoch time: 87.38 s
2024-12-04 16:42:49.047377: Yayy! New best EMA pseudo Dice: 0.7499
2024-12-04 16:42:50.614307: 
2024-12-04 16:42:50.615927: Epoch 48
2024-12-04 16:42:50.617124: Current learning rate: 0.00957
2024-12-04 16:44:17.829690: Validation loss improved from -0.53456 to -0.55763! Patience: 7/50
2024-12-04 16:44:17.830484: train_loss -0.6265
2024-12-04 16:44:17.831331: val_loss -0.5576
2024-12-04 16:44:17.831988: Pseudo dice [0.7835]
2024-12-04 16:44:17.832607: Epoch time: 87.22 s
2024-12-04 16:44:17.833348: Yayy! New best EMA pseudo Dice: 0.7533
2024-12-04 16:44:19.414060: 
2024-12-04 16:44:19.415298: Epoch 49
2024-12-04 16:44:19.416168: Current learning rate: 0.00956
2024-12-04 16:45:46.805217: Validation loss did not improve from -0.55763. Patience: 1/50
2024-12-04 16:45:46.805940: train_loss -0.6189
2024-12-04 16:45:46.806710: val_loss -0.5203
2024-12-04 16:45:46.807465: Pseudo dice [0.7518]
2024-12-04 16:45:46.808107: Epoch time: 87.39 s
2024-12-04 16:45:48.833945: 
2024-12-04 16:45:48.835405: Epoch 50
2024-12-04 16:45:48.836158: Current learning rate: 0.00955
2024-12-04 16:47:16.086392: Validation loss did not improve from -0.55763. Patience: 2/50
2024-12-04 16:47:16.087158: train_loss -0.6397
2024-12-04 16:47:16.088044: val_loss -0.5086
2024-12-04 16:47:16.088872: Pseudo dice [0.7528]
2024-12-04 16:47:16.089851: Epoch time: 87.25 s
2024-12-04 16:47:17.293895: 
2024-12-04 16:47:17.295015: Epoch 51
2024-12-04 16:47:17.295892: Current learning rate: 0.00954
2024-12-04 16:48:44.653776: Validation loss did not improve from -0.55763. Patience: 3/50
2024-12-04 16:48:44.654641: train_loss -0.6314
2024-12-04 16:48:44.655584: val_loss -0.5406
2024-12-04 16:48:44.656865: Pseudo dice [0.7696]
2024-12-04 16:48:44.657881: Epoch time: 87.36 s
2024-12-04 16:48:44.658629: Yayy! New best EMA pseudo Dice: 0.7547
2024-12-04 16:48:46.217577: 
2024-12-04 16:48:46.219157: Epoch 52
2024-12-04 16:48:46.220253: Current learning rate: 0.00953
2024-12-04 16:50:13.737690: Validation loss did not improve from -0.55763. Patience: 4/50
2024-12-04 16:50:13.738793: train_loss -0.6332
2024-12-04 16:50:13.739460: val_loss -0.5179
2024-12-04 16:50:13.740413: Pseudo dice [0.7596]
2024-12-04 16:50:13.741413: Epoch time: 87.52 s
2024-12-04 16:50:13.742116: Yayy! New best EMA pseudo Dice: 0.7552
2024-12-04 16:50:15.405711: 
2024-12-04 16:50:15.406469: Epoch 53
2024-12-04 16:50:15.407565: Current learning rate: 0.00952
2024-12-04 16:51:42.948464: Validation loss did not improve from -0.55763. Patience: 5/50
2024-12-04 16:51:42.949561: train_loss -0.6423
2024-12-04 16:51:42.950788: val_loss -0.5008
2024-12-04 16:51:42.951669: Pseudo dice [0.7429]
2024-12-04 16:51:42.952432: Epoch time: 87.55 s
2024-12-04 16:51:44.164028: 
2024-12-04 16:51:44.165990: Epoch 54
2024-12-04 16:51:44.167085: Current learning rate: 0.00951
2024-12-04 16:53:11.695394: Validation loss did not improve from -0.55763. Patience: 6/50
2024-12-04 16:53:11.696310: train_loss -0.6388
2024-12-04 16:53:11.697256: val_loss -0.5363
2024-12-04 16:53:11.698018: Pseudo dice [0.7663]
2024-12-04 16:53:11.698755: Epoch time: 87.53 s
2024-12-04 16:53:13.268615: 
2024-12-04 16:53:13.270077: Epoch 55
2024-12-04 16:53:13.271018: Current learning rate: 0.0095
2024-12-04 16:54:40.769623: Validation loss did not improve from -0.55763. Patience: 7/50
2024-12-04 16:54:40.770741: train_loss -0.641
2024-12-04 16:54:40.771555: val_loss -0.4971
2024-12-04 16:54:40.772228: Pseudo dice [0.7482]
2024-12-04 16:54:40.772824: Epoch time: 87.5 s
2024-12-04 16:54:42.007227: 
2024-12-04 16:54:42.008279: Epoch 56
2024-12-04 16:54:42.009201: Current learning rate: 0.00949
2024-12-04 16:56:09.547761: Validation loss improved from -0.55763 to -0.56466! Patience: 7/50
2024-12-04 16:56:09.548651: train_loss -0.6564
2024-12-04 16:56:09.549540: val_loss -0.5647
2024-12-04 16:56:09.550219: Pseudo dice [0.7873]
2024-12-04 16:56:09.550920: Epoch time: 87.54 s
2024-12-04 16:56:09.551550: Yayy! New best EMA pseudo Dice: 0.7578
2024-12-04 16:56:11.118412: 
2024-12-04 16:56:11.119654: Epoch 57
2024-12-04 16:56:11.120378: Current learning rate: 0.00949
2024-12-04 16:57:38.531621: Validation loss did not improve from -0.56466. Patience: 1/50
2024-12-04 16:57:38.532540: train_loss -0.6533
2024-12-04 16:57:38.533500: val_loss -0.4813
2024-12-04 16:57:38.534240: Pseudo dice [0.7363]
2024-12-04 16:57:38.534948: Epoch time: 87.42 s
2024-12-04 16:57:39.770613: 
2024-12-04 16:57:39.772200: Epoch 58
2024-12-04 16:57:39.772832: Current learning rate: 0.00948
2024-12-04 16:59:07.369961: Validation loss did not improve from -0.56466. Patience: 2/50
2024-12-04 16:59:07.370967: train_loss -0.6458
2024-12-04 16:59:07.371838: val_loss -0.5218
2024-12-04 16:59:07.372713: Pseudo dice [0.7705]
2024-12-04 16:59:07.373429: Epoch time: 87.6 s
2024-12-04 16:59:08.622289: 
2024-12-04 16:59:08.624164: Epoch 59
2024-12-04 16:59:08.624989: Current learning rate: 0.00947
2024-12-04 17:00:36.160675: Validation loss did not improve from -0.56466. Patience: 3/50
2024-12-04 17:00:36.161647: train_loss -0.6551
2024-12-04 17:00:36.162483: val_loss -0.503
2024-12-04 17:00:36.163169: Pseudo dice [0.7461]
2024-12-04 17:00:36.163821: Epoch time: 87.54 s
2024-12-04 17:00:37.759715: 
2024-12-04 17:00:37.761019: Epoch 60
2024-12-04 17:00:37.762028: Current learning rate: 0.00946
2024-12-04 17:02:05.451002: Validation loss did not improve from -0.56466. Patience: 4/50
2024-12-04 17:02:05.451960: train_loss -0.6587
2024-12-04 17:02:05.453171: val_loss -0.5295
2024-12-04 17:02:05.454410: Pseudo dice [0.7551]
2024-12-04 17:02:05.455365: Epoch time: 87.69 s
2024-12-04 17:02:07.083417: 
2024-12-04 17:02:07.085243: Epoch 61
2024-12-04 17:02:07.086605: Current learning rate: 0.00945
2024-12-04 17:03:34.656811: Validation loss did not improve from -0.56466. Patience: 5/50
2024-12-04 17:03:34.657630: train_loss -0.6496
2024-12-04 17:03:34.658386: val_loss -0.53
2024-12-04 17:03:34.659070: Pseudo dice [0.7608]
2024-12-04 17:03:34.659690: Epoch time: 87.58 s
2024-12-04 17:03:35.963786: 
2024-12-04 17:03:35.965547: Epoch 62
2024-12-04 17:03:35.966746: Current learning rate: 0.00944
2024-12-04 17:05:03.682730: Validation loss did not improve from -0.56466. Patience: 6/50
2024-12-04 17:05:03.683628: train_loss -0.6647
2024-12-04 17:05:03.684555: val_loss -0.4883
2024-12-04 17:05:03.685328: Pseudo dice [0.7441]
2024-12-04 17:05:03.686150: Epoch time: 87.72 s
2024-12-04 17:05:04.938283: 
2024-12-04 17:05:04.939703: Epoch 63
2024-12-04 17:05:04.940499: Current learning rate: 0.00943
2024-12-04 17:06:32.515532: Validation loss did not improve from -0.56466. Patience: 7/50
2024-12-04 17:06:32.516638: train_loss -0.6613
2024-12-04 17:06:32.517409: val_loss -0.5476
2024-12-04 17:06:32.518070: Pseudo dice [0.7736]
2024-12-04 17:06:32.518690: Epoch time: 87.58 s
2024-12-04 17:06:33.753487: 
2024-12-04 17:06:33.754723: Epoch 64
2024-12-04 17:06:33.755483: Current learning rate: 0.00942
2024-12-04 17:08:01.392155: Validation loss did not improve from -0.56466. Patience: 8/50
2024-12-04 17:08:01.393359: train_loss -0.6725
2024-12-04 17:08:01.394509: val_loss -0.5373
2024-12-04 17:08:01.395327: Pseudo dice [0.7765]
2024-12-04 17:08:01.396160: Epoch time: 87.64 s
2024-12-04 17:08:01.966625: Yayy! New best EMA pseudo Dice: 0.759
2024-12-04 17:08:03.690593: 
2024-12-04 17:08:03.692282: Epoch 65
2024-12-04 17:08:03.693035: Current learning rate: 0.00941
2024-12-04 17:09:31.369679: Validation loss did not improve from -0.56466. Patience: 9/50
2024-12-04 17:09:31.370498: train_loss -0.6619
2024-12-04 17:09:31.371412: val_loss -0.5425
2024-12-04 17:09:31.372179: Pseudo dice [0.7642]
2024-12-04 17:09:31.372874: Epoch time: 87.68 s
2024-12-04 17:09:31.373549: Yayy! New best EMA pseudo Dice: 0.7595
2024-12-04 17:09:32.974439: 
2024-12-04 17:09:32.975290: Epoch 66
2024-12-04 17:09:32.976022: Current learning rate: 0.0094
2024-12-04 17:11:00.728418: Validation loss did not improve from -0.56466. Patience: 10/50
2024-12-04 17:11:00.729297: train_loss -0.665
2024-12-04 17:11:00.730276: val_loss -0.4933
2024-12-04 17:11:00.731035: Pseudo dice [0.7441]
2024-12-04 17:11:00.731833: Epoch time: 87.76 s
2024-12-04 17:11:01.968986: 
2024-12-04 17:11:01.970448: Epoch 67
2024-12-04 17:11:01.971485: Current learning rate: 0.00939
2024-12-04 17:12:29.646096: Validation loss did not improve from -0.56466. Patience: 11/50
2024-12-04 17:12:29.647161: train_loss -0.6575
2024-12-04 17:12:29.648078: val_loss -0.5171
2024-12-04 17:12:29.648737: Pseudo dice [0.7626]
2024-12-04 17:12:29.649393: Epoch time: 87.68 s
2024-12-04 17:12:30.933194: 
2024-12-04 17:12:30.934579: Epoch 68
2024-12-04 17:12:30.935362: Current learning rate: 0.00939
2024-12-04 17:13:58.617525: Validation loss did not improve from -0.56466. Patience: 12/50
2024-12-04 17:13:58.618243: train_loss -0.6736
2024-12-04 17:13:58.618933: val_loss -0.5404
2024-12-04 17:13:58.619628: Pseudo dice [0.7794]
2024-12-04 17:13:58.620365: Epoch time: 87.69 s
2024-12-04 17:13:58.620969: Yayy! New best EMA pseudo Dice: 0.7605
2024-12-04 17:14:00.269641: 
2024-12-04 17:14:00.271017: Epoch 69
2024-12-04 17:14:00.271937: Current learning rate: 0.00938
2024-12-04 17:15:28.159766: Validation loss did not improve from -0.56466. Patience: 13/50
2024-12-04 17:15:28.160467: train_loss -0.6757
2024-12-04 17:15:28.161601: val_loss -0.546
2024-12-04 17:15:28.162416: Pseudo dice [0.7823]
2024-12-04 17:15:28.163225: Epoch time: 87.89 s
2024-12-04 17:15:28.549067: Yayy! New best EMA pseudo Dice: 0.7627
2024-12-04 17:15:30.146992: 
2024-12-04 17:15:30.148291: Epoch 70
2024-12-04 17:15:30.149221: Current learning rate: 0.00937
2024-12-04 17:16:57.798753: Validation loss did not improve from -0.56466. Patience: 14/50
2024-12-04 17:16:57.799583: train_loss -0.6795
2024-12-04 17:16:57.800537: val_loss -0.4954
2024-12-04 17:16:57.801399: Pseudo dice [0.7495]
2024-12-04 17:16:57.802316: Epoch time: 87.65 s
2024-12-04 17:16:59.405427: 
2024-12-04 17:16:59.407042: Epoch 71
2024-12-04 17:16:59.407728: Current learning rate: 0.00936
2024-12-04 17:18:27.094186: Validation loss did not improve from -0.56466. Patience: 15/50
2024-12-04 17:18:27.095096: train_loss -0.6792
2024-12-04 17:18:27.095851: val_loss -0.5493
2024-12-04 17:18:27.096465: Pseudo dice [0.7753]
2024-12-04 17:18:27.097046: Epoch time: 87.69 s
2024-12-04 17:18:27.097594: Yayy! New best EMA pseudo Dice: 0.7628
2024-12-04 17:18:28.776783: 
2024-12-04 17:18:28.778451: Epoch 72
2024-12-04 17:18:28.779905: Current learning rate: 0.00935
2024-12-04 17:19:56.598021: Validation loss did not improve from -0.56466. Patience: 16/50
2024-12-04 17:19:56.598896: train_loss -0.6725
2024-12-04 17:19:56.599780: val_loss -0.5181
2024-12-04 17:19:56.600547: Pseudo dice [0.752]
2024-12-04 17:19:56.601276: Epoch time: 87.82 s
2024-12-04 17:19:57.858323: 
2024-12-04 17:19:57.859900: Epoch 73
2024-12-04 17:19:57.860662: Current learning rate: 0.00934
2024-12-04 17:21:25.306244: Validation loss did not improve from -0.56466. Patience: 17/50
2024-12-04 17:21:25.307247: train_loss -0.6778
2024-12-04 17:21:25.308424: val_loss -0.5262
2024-12-04 17:21:25.309335: Pseudo dice [0.7718]
2024-12-04 17:21:25.310227: Epoch time: 87.45 s
2024-12-04 17:21:26.636324: 
2024-12-04 17:21:26.637861: Epoch 74
2024-12-04 17:21:26.638605: Current learning rate: 0.00933
2024-12-04 17:22:54.235752: Validation loss did not improve from -0.56466. Patience: 18/50
2024-12-04 17:22:54.236825: train_loss -0.6556
2024-12-04 17:22:54.237686: val_loss -0.5284
2024-12-04 17:22:54.238299: Pseudo dice [0.7687]
2024-12-04 17:22:54.239147: Epoch time: 87.6 s
2024-12-04 17:22:54.627081: Yayy! New best EMA pseudo Dice: 0.7633
2024-12-04 17:22:56.253746: 
2024-12-04 17:22:56.255048: Epoch 75
2024-12-04 17:22:56.255826: Current learning rate: 0.00932
2024-12-04 17:24:23.771373: Validation loss did not improve from -0.56466. Patience: 19/50
2024-12-04 17:24:23.773760: train_loss -0.674
2024-12-04 17:24:23.774528: val_loss -0.5131
2024-12-04 17:24:23.775482: Pseudo dice [0.7671]
2024-12-04 17:24:23.776587: Epoch time: 87.52 s
2024-12-04 17:24:23.777468: Yayy! New best EMA pseudo Dice: 0.7637
2024-12-04 17:24:25.480843: 
2024-12-04 17:24:25.482052: Epoch 76
2024-12-04 17:24:25.482764: Current learning rate: 0.00931
2024-12-04 17:25:52.876194: Validation loss did not improve from -0.56466. Patience: 20/50
2024-12-04 17:25:52.876979: train_loss -0.6715
2024-12-04 17:25:52.877780: val_loss -0.5347
2024-12-04 17:25:52.878442: Pseudo dice [0.766]
2024-12-04 17:25:52.879086: Epoch time: 87.4 s
2024-12-04 17:25:52.879721: Yayy! New best EMA pseudo Dice: 0.7639
2024-12-04 17:25:54.584419: 
2024-12-04 17:25:54.586115: Epoch 77
2024-12-04 17:25:54.587415: Current learning rate: 0.0093
2024-12-04 17:27:22.027075: Validation loss did not improve from -0.56466. Patience: 21/50
2024-12-04 17:27:22.028056: train_loss -0.6823
2024-12-04 17:27:22.028999: val_loss -0.4941
2024-12-04 17:27:22.029846: Pseudo dice [0.7449]
2024-12-04 17:27:22.030714: Epoch time: 87.44 s
2024-12-04 17:27:23.309725: 
2024-12-04 17:27:23.312011: Epoch 78
2024-12-04 17:27:23.313151: Current learning rate: 0.0093
2024-12-04 17:28:50.694447: Validation loss did not improve from -0.56466. Patience: 22/50
2024-12-04 17:28:50.695368: train_loss -0.6906
2024-12-04 17:28:50.696198: val_loss -0.4952
2024-12-04 17:28:50.696831: Pseudo dice [0.7459]
2024-12-04 17:28:50.697551: Epoch time: 87.39 s
2024-12-04 17:28:51.957860: 
2024-12-04 17:28:51.959265: Epoch 79
2024-12-04 17:28:51.960207: Current learning rate: 0.00929
2024-12-04 17:30:19.377907: Validation loss did not improve from -0.56466. Patience: 23/50
2024-12-04 17:30:19.378741: train_loss -0.6836
2024-12-04 17:30:19.379595: val_loss -0.5432
2024-12-04 17:30:19.380377: Pseudo dice [0.767]
2024-12-04 17:30:19.381126: Epoch time: 87.42 s
2024-12-04 17:30:21.096601: 
2024-12-04 17:30:21.098017: Epoch 80
2024-12-04 17:30:21.098978: Current learning rate: 0.00928
2024-12-04 17:31:48.577175: Validation loss did not improve from -0.56466. Patience: 24/50
2024-12-04 17:31:48.578134: train_loss -0.6873
2024-12-04 17:31:48.578925: val_loss -0.5261
2024-12-04 17:31:48.579569: Pseudo dice [0.7687]
2024-12-04 17:31:48.580188: Epoch time: 87.48 s
2024-12-04 17:31:50.228975: 
2024-12-04 17:31:50.230767: Epoch 81
2024-12-04 17:31:50.231748: Current learning rate: 0.00927
2024-12-04 17:33:17.960496: Validation loss did not improve from -0.56466. Patience: 25/50
2024-12-04 17:33:17.961451: train_loss -0.6867
2024-12-04 17:33:17.962350: val_loss -0.5075
2024-12-04 17:33:17.963081: Pseudo dice [0.7489]
2024-12-04 17:33:17.963784: Epoch time: 87.73 s
2024-12-04 17:33:19.245761: 
2024-12-04 17:33:19.247536: Epoch 82
2024-12-04 17:33:19.248405: Current learning rate: 0.00926
2024-12-04 17:34:47.048041: Validation loss did not improve from -0.56466. Patience: 26/50
2024-12-04 17:34:47.048661: train_loss -0.6913
2024-12-04 17:34:47.049460: val_loss -0.5138
2024-12-04 17:34:47.050179: Pseudo dice [0.7631]
2024-12-04 17:34:47.050825: Epoch time: 87.8 s
2024-12-04 17:34:48.249457: 
2024-12-04 17:34:48.250925: Epoch 83
2024-12-04 17:34:48.251762: Current learning rate: 0.00925
2024-12-04 17:36:15.965911: Validation loss did not improve from -0.56466. Patience: 27/50
2024-12-04 17:36:15.966863: train_loss -0.6922
2024-12-04 17:36:15.967829: val_loss -0.5133
2024-12-04 17:36:15.968536: Pseudo dice [0.7584]
2024-12-04 17:36:15.969395: Epoch time: 87.72 s
2024-12-04 17:36:17.192447: 
2024-12-04 17:36:17.193711: Epoch 84
2024-12-04 17:36:17.194849: Current learning rate: 0.00924
2024-12-04 17:37:44.671343: Validation loss did not improve from -0.56466. Patience: 28/50
2024-12-04 17:37:44.672177: train_loss -0.6949
2024-12-04 17:37:44.673165: val_loss -0.5142
2024-12-04 17:37:44.674057: Pseudo dice [0.758]
2024-12-04 17:37:44.674966: Epoch time: 87.48 s
2024-12-04 17:37:46.303151: 
2024-12-04 17:37:46.304932: Epoch 85
2024-12-04 17:37:46.306018: Current learning rate: 0.00923
2024-12-04 17:39:13.710155: Validation loss did not improve from -0.56466. Patience: 29/50
2024-12-04 17:39:13.711192: train_loss -0.6869
2024-12-04 17:39:13.711978: val_loss -0.4806
2024-12-04 17:39:13.712768: Pseudo dice [0.7415]
2024-12-04 17:39:13.713488: Epoch time: 87.41 s
2024-12-04 17:39:15.177715: 
2024-12-04 17:39:15.179505: Epoch 86
2024-12-04 17:39:15.180597: Current learning rate: 0.00922
2024-12-04 17:40:44.051340: Validation loss did not improve from -0.56466. Patience: 30/50
2024-12-04 17:40:44.070181: train_loss -0.694
2024-12-04 17:40:44.072612: val_loss -0.5212
2024-12-04 17:40:44.073521: Pseudo dice [0.7696]
2024-12-04 17:40:44.074734: Epoch time: 88.88 s
2024-12-04 17:40:45.366141: 
2024-12-04 17:40:45.367631: Epoch 87
2024-12-04 17:40:45.368307: Current learning rate: 0.00921
2024-12-04 17:42:12.915906: Validation loss did not improve from -0.56466. Patience: 31/50
2024-12-04 17:42:12.916624: train_loss -0.6898
2024-12-04 17:42:12.917502: val_loss -0.5196
2024-12-04 17:42:12.918161: Pseudo dice [0.7572]
2024-12-04 17:42:12.918790: Epoch time: 87.55 s
2024-12-04 17:42:14.165900: 
2024-12-04 17:42:14.167214: Epoch 88
2024-12-04 17:42:14.168007: Current learning rate: 0.0092
2024-12-04 17:43:41.785358: Validation loss did not improve from -0.56466. Patience: 32/50
2024-12-04 17:43:41.786478: train_loss -0.6838
2024-12-04 17:43:41.787438: val_loss -0.4649
2024-12-04 17:43:41.788195: Pseudo dice [0.7212]
2024-12-04 17:43:41.788881: Epoch time: 87.62 s
2024-12-04 17:43:43.023637: 
2024-12-04 17:43:43.024878: Epoch 89
2024-12-04 17:43:43.025632: Current learning rate: 0.0092
2024-12-04 17:45:10.448083: Validation loss improved from -0.56466 to -0.57165! Patience: 32/50
2024-12-04 17:45:10.448980: train_loss -0.6987
2024-12-04 17:45:10.450083: val_loss -0.5716
2024-12-04 17:45:10.451027: Pseudo dice [0.7899]
2024-12-04 17:45:10.452046: Epoch time: 87.43 s
2024-12-04 17:45:12.048181: 
2024-12-04 17:45:12.049681: Epoch 90
2024-12-04 17:45:12.050682: Current learning rate: 0.00919
2024-12-04 17:46:39.378440: Validation loss did not improve from -0.57165. Patience: 1/50
2024-12-04 17:46:39.379480: train_loss -0.7018
2024-12-04 17:46:39.380286: val_loss -0.522
2024-12-04 17:46:39.380961: Pseudo dice [0.7617]
2024-12-04 17:46:39.381551: Epoch time: 87.33 s
2024-12-04 17:46:41.595653: 
2024-12-04 17:46:41.597531: Epoch 91
2024-12-04 17:46:41.598359: Current learning rate: 0.00918
2024-12-04 17:48:09.495469: Validation loss did not improve from -0.57165. Patience: 2/50
2024-12-04 17:48:09.496492: train_loss -0.6929
2024-12-04 17:48:09.497646: val_loss -0.5131
2024-12-04 17:48:09.498396: Pseudo dice [0.7624]
2024-12-04 17:48:09.499222: Epoch time: 87.9 s
2024-12-04 17:48:10.693568: 
2024-12-04 17:48:10.695184: Epoch 92
2024-12-04 17:48:10.696360: Current learning rate: 0.00917
2024-12-04 17:49:38.274211: Validation loss did not improve from -0.57165. Patience: 3/50
2024-12-04 17:49:38.275093: train_loss -0.6951
2024-12-04 17:49:38.276393: val_loss -0.4614
2024-12-04 17:49:38.277660: Pseudo dice [0.7346]
2024-12-04 17:49:38.279135: Epoch time: 87.58 s
2024-12-04 17:49:39.467093: 
2024-12-04 17:49:39.468584: Epoch 93
2024-12-04 17:49:39.469600: Current learning rate: 0.00916
2024-12-04 17:51:07.036205: Validation loss did not improve from -0.57165. Patience: 4/50
2024-12-04 17:51:07.036879: train_loss -0.6858
2024-12-04 17:51:07.037987: val_loss -0.561
2024-12-04 17:51:07.038669: Pseudo dice [0.7884]
2024-12-04 17:51:07.039361: Epoch time: 87.57 s
2024-12-04 17:51:08.260704: 
2024-12-04 17:51:08.262287: Epoch 94
2024-12-04 17:51:08.263049: Current learning rate: 0.00915
2024-12-04 17:52:35.545308: Validation loss did not improve from -0.57165. Patience: 5/50
2024-12-04 17:52:35.546493: train_loss -0.6915
2024-12-04 17:52:35.547334: val_loss -0.508
2024-12-04 17:52:35.548097: Pseudo dice [0.7562]
2024-12-04 17:52:35.548839: Epoch time: 87.29 s
2024-12-04 17:52:37.152884: 
2024-12-04 17:52:37.154799: Epoch 95
2024-12-04 17:52:37.155783: Current learning rate: 0.00914
2024-12-04 17:54:04.413602: Validation loss did not improve from -0.57165. Patience: 6/50
2024-12-04 17:54:04.414858: train_loss -0.7048
2024-12-04 17:54:04.415859: val_loss -0.5542
2024-12-04 17:54:04.416555: Pseudo dice [0.7797]
2024-12-04 17:54:04.417315: Epoch time: 87.26 s
2024-12-04 17:54:05.626698: 
2024-12-04 17:54:05.628280: Epoch 96
2024-12-04 17:54:05.629142: Current learning rate: 0.00913
2024-12-04 17:55:32.892898: Validation loss did not improve from -0.57165. Patience: 7/50
2024-12-04 17:55:32.893922: train_loss -0.7084
2024-12-04 17:55:32.894690: val_loss -0.5214
2024-12-04 17:55:32.895463: Pseudo dice [0.7692]
2024-12-04 17:55:32.896176: Epoch time: 87.27 s
2024-12-04 17:55:34.090458: 
2024-12-04 17:55:34.091720: Epoch 97
2024-12-04 17:55:34.092388: Current learning rate: 0.00912
2024-12-04 17:57:01.347821: Validation loss did not improve from -0.57165. Patience: 8/50
2024-12-04 17:57:01.348577: train_loss -0.7114
2024-12-04 17:57:01.349391: val_loss -0.519
2024-12-04 17:57:01.350068: Pseudo dice [0.7712]
2024-12-04 17:57:01.350758: Epoch time: 87.26 s
2024-12-04 17:57:02.578329: 
2024-12-04 17:57:02.579862: Epoch 98
2024-12-04 17:57:02.580610: Current learning rate: 0.00911
2024-12-04 17:58:29.868529: Validation loss did not improve from -0.57165. Patience: 9/50
2024-12-04 17:58:29.869169: train_loss -0.7091
2024-12-04 17:58:29.869916: val_loss -0.5068
2024-12-04 17:58:29.870585: Pseudo dice [0.7593]
2024-12-04 17:58:29.871227: Epoch time: 87.29 s
2024-12-04 17:58:31.143111: 
2024-12-04 17:58:31.144550: Epoch 99
2024-12-04 17:58:31.145332: Current learning rate: 0.0091
2024-12-04 17:59:58.541806: Validation loss did not improve from -0.57165. Patience: 10/50
2024-12-04 17:59:58.542780: train_loss -0.7102
2024-12-04 17:59:58.543773: val_loss -0.4987
2024-12-04 17:59:58.544433: Pseudo dice [0.7553]
2024-12-04 17:59:58.545059: Epoch time: 87.4 s
2024-12-04 18:00:00.191755: 
2024-12-04 18:00:00.193323: Epoch 100
2024-12-04 18:00:00.194035: Current learning rate: 0.0091
2024-12-04 18:01:27.456579: Validation loss did not improve from -0.57165. Patience: 11/50
2024-12-04 18:01:27.458564: train_loss -0.7184
2024-12-04 18:01:27.459572: val_loss -0.4834
2024-12-04 18:01:27.460406: Pseudo dice [0.7507]
2024-12-04 18:01:27.461230: Epoch time: 87.27 s
2024-12-04 18:01:28.695364: 
2024-12-04 18:01:28.697050: Epoch 101
2024-12-04 18:01:28.697976: Current learning rate: 0.00909
2024-12-04 18:02:56.219374: Validation loss did not improve from -0.57165. Patience: 12/50
2024-12-04 18:02:56.220293: train_loss -0.7202
2024-12-04 18:02:56.221166: val_loss -0.5088
2024-12-04 18:02:56.221972: Pseudo dice [0.7527]
2024-12-04 18:02:56.222768: Epoch time: 87.53 s
2024-12-04 18:02:57.455522: 
2024-12-04 18:02:57.457184: Epoch 102
2024-12-04 18:02:57.458040: Current learning rate: 0.00908
2024-12-04 18:04:25.253418: Validation loss did not improve from -0.57165. Patience: 13/50
2024-12-04 18:04:25.254098: train_loss -0.7131
2024-12-04 18:04:25.255312: val_loss -0.4869
2024-12-04 18:04:25.256325: Pseudo dice [0.7566]
2024-12-04 18:04:25.257316: Epoch time: 87.8 s
2024-12-04 18:04:26.789570: 
2024-12-04 18:04:26.791245: Epoch 103
2024-12-04 18:04:26.792531: Current learning rate: 0.00907
2024-12-04 18:05:54.705721: Validation loss did not improve from -0.57165. Patience: 14/50
2024-12-04 18:05:54.706824: train_loss -0.7105
2024-12-04 18:05:54.707789: val_loss -0.5328
2024-12-04 18:05:54.708445: Pseudo dice [0.7787]
2024-12-04 18:05:54.709075: Epoch time: 87.92 s
2024-12-04 18:05:55.926901: 
2024-12-04 18:05:55.928018: Epoch 104
2024-12-04 18:05:55.928664: Current learning rate: 0.00906
2024-12-04 18:07:23.712612: Validation loss did not improve from -0.57165. Patience: 15/50
2024-12-04 18:07:23.713695: train_loss -0.7079
2024-12-04 18:07:23.714453: val_loss -0.4662
2024-12-04 18:07:23.715214: Pseudo dice [0.7308]
2024-12-04 18:07:23.715961: Epoch time: 87.79 s
2024-12-04 18:07:25.306251: 
2024-12-04 18:07:25.307916: Epoch 105
2024-12-04 18:07:25.309167: Current learning rate: 0.00905
2024-12-04 18:08:53.161324: Validation loss did not improve from -0.57165. Patience: 16/50
2024-12-04 18:08:53.162203: train_loss -0.7211
2024-12-04 18:08:53.163126: val_loss -0.4871
2024-12-04 18:08:53.163715: Pseudo dice [0.7558]
2024-12-04 18:08:53.164287: Epoch time: 87.86 s
2024-12-04 18:08:54.360279: 
2024-12-04 18:08:54.362196: Epoch 106
2024-12-04 18:08:54.363082: Current learning rate: 0.00904
2024-12-04 18:10:22.160390: Validation loss did not improve from -0.57165. Patience: 17/50
2024-12-04 18:10:22.161345: train_loss -0.7174
2024-12-04 18:10:22.162317: val_loss -0.4867
2024-12-04 18:10:22.163231: Pseudo dice [0.7414]
2024-12-04 18:10:22.164116: Epoch time: 87.8 s
2024-12-04 18:10:23.400120: 
2024-12-04 18:10:23.401517: Epoch 107
2024-12-04 18:10:23.402897: Current learning rate: 0.00903
2024-12-04 18:11:51.041493: Validation loss did not improve from -0.57165. Patience: 18/50
2024-12-04 18:11:51.042285: train_loss -0.7161
2024-12-04 18:11:51.043162: val_loss -0.4959
2024-12-04 18:11:51.043790: Pseudo dice [0.7659]
2024-12-04 18:11:51.044509: Epoch time: 87.64 s
2024-12-04 18:11:52.300404: 
2024-12-04 18:11:52.301927: Epoch 108
2024-12-04 18:11:52.302861: Current learning rate: 0.00902
2024-12-04 18:13:19.840287: Validation loss did not improve from -0.57165. Patience: 19/50
2024-12-04 18:13:19.841376: train_loss -0.7187
2024-12-04 18:13:19.842239: val_loss -0.4863
2024-12-04 18:13:19.842935: Pseudo dice [0.7423]
2024-12-04 18:13:19.843808: Epoch time: 87.54 s
2024-12-04 18:13:21.069836: 
2024-12-04 18:13:21.071500: Epoch 109
2024-12-04 18:13:21.072478: Current learning rate: 0.00901
2024-12-04 18:14:48.759008: Validation loss did not improve from -0.57165. Patience: 20/50
2024-12-04 18:14:48.759933: train_loss -0.7159
2024-12-04 18:14:48.760996: val_loss -0.5066
2024-12-04 18:14:48.761937: Pseudo dice [0.7618]
2024-12-04 18:14:48.762806: Epoch time: 87.69 s
2024-12-04 18:14:50.337766: 
2024-12-04 18:14:50.339139: Epoch 110
2024-12-04 18:14:50.340179: Current learning rate: 0.009
2024-12-04 18:16:17.988215: Validation loss did not improve from -0.57165. Patience: 21/50
2024-12-04 18:16:17.989040: train_loss -0.7214
2024-12-04 18:16:17.989975: val_loss -0.5048
2024-12-04 18:16:17.990725: Pseudo dice [0.7605]
2024-12-04 18:16:17.991462: Epoch time: 87.65 s
2024-12-04 18:16:19.203520: 
2024-12-04 18:16:19.204725: Epoch 111
2024-12-04 18:16:19.205592: Current learning rate: 0.009
2024-12-04 18:17:46.867976: Validation loss did not improve from -0.57165. Patience: 22/50
2024-12-04 18:17:46.868945: train_loss -0.7206
2024-12-04 18:17:46.870038: val_loss -0.5428
2024-12-04 18:17:46.870863: Pseudo dice [0.7756]
2024-12-04 18:17:46.871677: Epoch time: 87.67 s
2024-12-04 18:17:48.078127: 
2024-12-04 18:17:48.079305: Epoch 112
2024-12-04 18:17:48.080599: Current learning rate: 0.00899
2024-12-04 18:19:15.639063: Validation loss did not improve from -0.57165. Patience: 23/50
2024-12-04 18:19:15.640025: train_loss -0.7257
2024-12-04 18:19:15.641137: val_loss -0.4899
2024-12-04 18:19:15.641950: Pseudo dice [0.7508]
2024-12-04 18:19:15.642583: Epoch time: 87.56 s
2024-12-04 18:19:17.208331: 
2024-12-04 18:19:17.210241: Epoch 113
2024-12-04 18:19:17.211531: Current learning rate: 0.00898
2024-12-04 18:20:44.590789: Validation loss did not improve from -0.57165. Patience: 24/50
2024-12-04 18:20:44.591654: train_loss -0.7242
2024-12-04 18:20:44.592701: val_loss -0.5195
2024-12-04 18:20:44.593620: Pseudo dice [0.7651]
2024-12-04 18:20:44.594440: Epoch time: 87.38 s
2024-12-04 18:20:45.856099: 
2024-12-04 18:20:45.857678: Epoch 114
2024-12-04 18:20:45.858612: Current learning rate: 0.00897
2024-12-04 18:22:13.181695: Validation loss did not improve from -0.57165. Patience: 25/50
2024-12-04 18:22:13.183177: train_loss -0.7206
2024-12-04 18:22:13.184354: val_loss -0.4911
2024-12-04 18:22:13.185349: Pseudo dice [0.7469]
2024-12-04 18:22:13.186397: Epoch time: 87.33 s
2024-12-04 18:22:14.784642: 
2024-12-04 18:22:14.785990: Epoch 115
2024-12-04 18:22:14.786686: Current learning rate: 0.00896
2024-12-04 18:23:42.520655: Validation loss did not improve from -0.57165. Patience: 26/50
2024-12-04 18:23:42.521648: train_loss -0.7195
2024-12-04 18:23:42.522899: val_loss -0.4816
2024-12-04 18:23:42.523709: Pseudo dice [0.744]
2024-12-04 18:23:42.524434: Epoch time: 87.74 s
2024-12-04 18:23:43.789103: 
2024-12-04 18:23:43.791369: Epoch 116
2024-12-04 18:23:43.792142: Current learning rate: 0.00895
2024-12-04 18:25:11.163954: Validation loss did not improve from -0.57165. Patience: 27/50
2024-12-04 18:25:11.164821: train_loss -0.7233
2024-12-04 18:25:11.165818: val_loss -0.5237
2024-12-04 18:25:11.166778: Pseudo dice [0.7663]
2024-12-04 18:25:11.167684: Epoch time: 87.38 s
2024-12-04 18:25:12.413081: 
2024-12-04 18:25:12.414710: Epoch 117
2024-12-04 18:25:12.415654: Current learning rate: 0.00894
2024-12-04 18:26:39.741861: Validation loss did not improve from -0.57165. Patience: 28/50
2024-12-04 18:26:39.742966: train_loss -0.729
2024-12-04 18:26:39.743736: val_loss -0.4951
2024-12-04 18:26:39.744387: Pseudo dice [0.7497]
2024-12-04 18:26:39.745066: Epoch time: 87.33 s
2024-12-04 18:26:40.967398: 
2024-12-04 18:26:40.968709: Epoch 118
2024-12-04 18:26:40.969801: Current learning rate: 0.00893
2024-12-04 18:28:08.453717: Validation loss did not improve from -0.57165. Patience: 29/50
2024-12-04 18:28:08.454867: train_loss -0.7274
2024-12-04 18:28:08.456044: val_loss -0.5198
2024-12-04 18:28:08.456978: Pseudo dice [0.7581]
2024-12-04 18:28:08.457703: Epoch time: 87.49 s
2024-12-04 18:28:09.699758: 
2024-12-04 18:28:09.701464: Epoch 119
2024-12-04 18:28:09.702502: Current learning rate: 0.00892
2024-12-04 18:29:37.128083: Validation loss did not improve from -0.57165. Patience: 30/50
2024-12-04 18:29:37.128934: train_loss -0.7261
2024-12-04 18:29:37.129817: val_loss -0.5241
2024-12-04 18:29:37.130709: Pseudo dice [0.7685]
2024-12-04 18:29:37.131493: Epoch time: 87.43 s
2024-12-04 18:29:38.779566: 
2024-12-04 18:29:38.781256: Epoch 120
2024-12-04 18:29:38.782120: Current learning rate: 0.00891
2024-12-04 18:31:06.445819: Validation loss did not improve from -0.57165. Patience: 31/50
2024-12-04 18:31:06.446822: train_loss -0.7297
2024-12-04 18:31:06.447697: val_loss -0.5614
2024-12-04 18:31:06.448400: Pseudo dice [0.7842]
2024-12-04 18:31:06.449084: Epoch time: 87.67 s
2024-12-04 18:31:07.719922: 
2024-12-04 18:31:07.721494: Epoch 121
2024-12-04 18:31:07.722659: Current learning rate: 0.0089
2024-12-04 18:32:35.495900: Validation loss did not improve from -0.57165. Patience: 32/50
2024-12-04 18:32:35.496823: train_loss -0.7315
2024-12-04 18:32:35.497622: val_loss -0.5218
2024-12-04 18:32:35.498230: Pseudo dice [0.7578]
2024-12-04 18:32:35.498818: Epoch time: 87.78 s
2024-12-04 18:32:36.808568: 
2024-12-04 18:32:36.809720: Epoch 122
2024-12-04 18:32:36.810436: Current learning rate: 0.00889
2024-12-04 18:34:04.616063: Validation loss did not improve from -0.57165. Patience: 33/50
2024-12-04 18:34:04.617082: train_loss -0.7346
2024-12-04 18:34:04.617806: val_loss -0.5485
2024-12-04 18:34:04.618422: Pseudo dice [0.7699]
2024-12-04 18:34:04.619192: Epoch time: 87.81 s
2024-12-04 18:34:05.932843: 
2024-12-04 18:34:05.934493: Epoch 123
2024-12-04 18:34:05.935891: Current learning rate: 0.00889
2024-12-04 18:35:33.747737: Validation loss did not improve from -0.57165. Patience: 34/50
2024-12-04 18:35:33.748872: train_loss -0.7332
2024-12-04 18:35:33.749732: val_loss -0.4816
2024-12-04 18:35:33.750389: Pseudo dice [0.7518]
2024-12-04 18:35:33.750998: Epoch time: 87.82 s
2024-12-04 18:35:35.315740: 
2024-12-04 18:35:35.317655: Epoch 124
2024-12-04 18:35:35.318381: Current learning rate: 0.00888
2024-12-04 18:37:03.140365: Validation loss did not improve from -0.57165. Patience: 35/50
2024-12-04 18:37:03.141389: train_loss -0.7362
2024-12-04 18:37:03.142251: val_loss -0.493
2024-12-04 18:37:03.143063: Pseudo dice [0.7528]
2024-12-04 18:37:03.143793: Epoch time: 87.83 s
2024-12-04 18:37:04.761211: 
2024-12-04 18:37:04.762854: Epoch 125
2024-12-04 18:37:04.763800: Current learning rate: 0.00887
2024-12-04 18:38:32.622069: Validation loss did not improve from -0.57165. Patience: 36/50
2024-12-04 18:38:32.623454: train_loss -0.7382
2024-12-04 18:38:32.624545: val_loss -0.4331
2024-12-04 18:38:32.625506: Pseudo dice [0.7213]
2024-12-04 18:38:32.626526: Epoch time: 87.86 s
2024-12-04 18:38:33.896939: 
2024-12-04 18:38:33.898446: Epoch 126
2024-12-04 18:38:33.899389: Current learning rate: 0.00886
2024-12-04 18:40:01.264669: Validation loss did not improve from -0.57165. Patience: 37/50
2024-12-04 18:40:01.265666: train_loss -0.7393
2024-12-04 18:40:01.266437: val_loss -0.5103
2024-12-04 18:40:01.267249: Pseudo dice [0.767]
2024-12-04 18:40:01.268059: Epoch time: 87.37 s
2024-12-04 18:40:02.520967: 
2024-12-04 18:40:02.522434: Epoch 127
2024-12-04 18:40:02.523472: Current learning rate: 0.00885
2024-12-04 18:41:29.900687: Validation loss did not improve from -0.57165. Patience: 38/50
2024-12-04 18:41:29.901757: train_loss -0.7386
2024-12-04 18:41:29.902772: val_loss -0.567
2024-12-04 18:41:29.903497: Pseudo dice [0.7886]
2024-12-04 18:41:29.904067: Epoch time: 87.38 s
2024-12-04 18:41:31.143619: 
2024-12-04 18:41:31.144706: Epoch 128
2024-12-04 18:41:31.145604: Current learning rate: 0.00884
2024-12-04 18:42:58.465446: Validation loss did not improve from -0.57165. Patience: 39/50
2024-12-04 18:42:58.466122: train_loss -0.7285
2024-12-04 18:42:58.466886: val_loss -0.5234
2024-12-04 18:42:58.467605: Pseudo dice [0.7608]
2024-12-04 18:42:58.468235: Epoch time: 87.32 s
2024-12-04 18:42:59.724873: 
2024-12-04 18:42:59.726678: Epoch 129
2024-12-04 18:42:59.727717: Current learning rate: 0.00883
2024-12-04 18:44:27.120517: Validation loss did not improve from -0.57165. Patience: 40/50
2024-12-04 18:44:27.121891: train_loss -0.7351
2024-12-04 18:44:27.122797: val_loss -0.5251
2024-12-04 18:44:27.123493: Pseudo dice [0.7643]
2024-12-04 18:44:27.124316: Epoch time: 87.4 s
2024-12-04 18:44:29.068856: 
2024-12-04 18:44:29.070278: Epoch 130
2024-12-04 18:44:29.070969: Current learning rate: 0.00882
2024-12-04 18:45:58.116669: Validation loss did not improve from -0.57165. Patience: 41/50
2024-12-04 18:45:58.184471: train_loss -0.7363
2024-12-04 18:45:58.187475: val_loss -0.5228
2024-12-04 18:45:58.188526: Pseudo dice [0.7581]
2024-12-04 18:45:58.190499: Epoch time: 89.06 s
2024-12-04 18:45:59.897609: 
2024-12-04 18:45:59.899099: Epoch 131
2024-12-04 18:45:59.899780: Current learning rate: 0.00881
2024-12-04 18:47:27.258486: Validation loss did not improve from -0.57165. Patience: 42/50
2024-12-04 18:47:27.259351: train_loss -0.7381
2024-12-04 18:47:27.260451: val_loss -0.4758
2024-12-04 18:47:27.261177: Pseudo dice [0.7384]
2024-12-04 18:47:27.261877: Epoch time: 87.36 s
2024-12-04 18:47:28.585805: 
2024-12-04 18:47:28.587427: Epoch 132
2024-12-04 18:47:28.588265: Current learning rate: 0.0088
2024-12-04 18:48:56.034945: Validation loss did not improve from -0.57165. Patience: 43/50
2024-12-04 18:48:56.035830: train_loss -0.7405
2024-12-04 18:48:56.036862: val_loss -0.4751
2024-12-04 18:48:56.037541: Pseudo dice [0.7373]
2024-12-04 18:48:56.038288: Epoch time: 87.45 s
2024-12-04 18:48:57.290501: 
2024-12-04 18:48:57.291971: Epoch 133
2024-12-04 18:48:57.292869: Current learning rate: 0.00879
2024-12-04 18:50:24.673847: Validation loss did not improve from -0.57165. Patience: 44/50
2024-12-04 18:50:24.674688: train_loss -0.74
2024-12-04 18:50:24.675556: val_loss -0.527
2024-12-04 18:50:24.676203: Pseudo dice [0.769]
2024-12-04 18:50:24.677032: Epoch time: 87.39 s
2024-12-04 18:50:25.943349: 
2024-12-04 18:50:25.944881: Epoch 134
2024-12-04 18:50:25.945708: Current learning rate: 0.00879
2024-12-04 18:51:53.551512: Validation loss did not improve from -0.57165. Patience: 45/50
2024-12-04 18:51:53.552368: train_loss -0.7352
2024-12-04 18:51:53.553222: val_loss -0.5472
2024-12-04 18:51:53.553926: Pseudo dice [0.7801]
2024-12-04 18:51:53.554651: Epoch time: 87.61 s
2024-12-04 18:51:55.628718: 
2024-12-04 18:51:55.630082: Epoch 135
2024-12-04 18:51:55.631017: Current learning rate: 0.00878
2024-12-04 18:53:23.200133: Validation loss did not improve from -0.57165. Patience: 46/50
2024-12-04 18:53:23.201220: train_loss -0.7368
2024-12-04 18:53:23.202122: val_loss -0.5109
2024-12-04 18:53:23.202876: Pseudo dice [0.7626]
2024-12-04 18:53:23.203614: Epoch time: 87.57 s
2024-12-04 18:53:24.532093: 
2024-12-04 18:53:24.534118: Epoch 136
2024-12-04 18:53:24.535336: Current learning rate: 0.00877
2024-12-04 18:54:52.066114: Validation loss did not improve from -0.57165. Patience: 47/50
2024-12-04 18:54:52.067182: train_loss -0.7376
2024-12-04 18:54:52.068227: val_loss -0.4869
2024-12-04 18:54:52.068904: Pseudo dice [0.7559]
2024-12-04 18:54:52.069525: Epoch time: 87.54 s
2024-12-04 18:54:53.335915: 
2024-12-04 18:54:53.337398: Epoch 137
2024-12-04 18:54:53.338248: Current learning rate: 0.00876
2024-12-04 18:56:21.016512: Validation loss did not improve from -0.57165. Patience: 48/50
2024-12-04 18:56:21.017410: train_loss -0.7336
2024-12-04 18:56:21.018196: val_loss -0.52
2024-12-04 18:56:21.018874: Pseudo dice [0.7614]
2024-12-04 18:56:21.019591: Epoch time: 87.68 s
2024-12-04 18:56:22.261614: 
2024-12-04 18:56:22.263141: Epoch 138
2024-12-04 18:56:22.264182: Current learning rate: 0.00875
2024-12-04 18:57:49.941786: Validation loss did not improve from -0.57165. Patience: 49/50
2024-12-04 18:57:49.942931: train_loss -0.7334
2024-12-04 18:57:49.943773: val_loss -0.5205
2024-12-04 18:57:49.944562: Pseudo dice [0.766]
2024-12-04 18:57:49.945286: Epoch time: 87.68 s
2024-12-04 18:57:51.204910: 
2024-12-04 18:57:51.206507: Epoch 139
2024-12-04 18:57:51.207588: Current learning rate: 0.00874
2024-12-04 18:59:18.784097: Validation loss did not improve from -0.57165. Patience: 50/50
2024-12-04 18:59:18.785218: train_loss -0.7389
2024-12-04 18:59:18.786206: val_loss -0.4944
2024-12-04 18:59:18.786865: Pseudo dice [0.7675]
2024-12-04 18:59:18.787580: Epoch time: 87.58 s
2024-12-04 18:59:20.461498: Patience reached. Stopping training.
2024-12-04 18:59:20.893563: Training done.
2024-12-04 18:59:21.068471: Using splits from existing split file: /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset307_Sohee_Calcium_OCT_CrossValidation/splits_final.json
2024-12-04 18:59:21.086004: The split file contains 5 splits.
2024-12-04 18:59:21.087339: Desired fold for training: 0
2024-12-04 18:59:21.088689: This split has 6 training and 2 validation cases.
2024-12-04 18:59:21.089850: predicting 106-002
2024-12-04 18:59:21.103660: 106-002, shape torch.Size([1, 539, 498, 498]), rank 0
2024-12-04 19:02:09.207304: predicting 706-005
2024-12-04 19:02:09.244137: 706-005, shape torch.Size([1, 375, 498, 498]), rank 0
2024-12-04 19:04:03.127077: Validation complete
2024-12-04 19:04:03.127564: Mean Validation Dice:  0.7700985163035465
2024-12-04 15:30:09.176016: unpacking done...
2024-12-04 15:30:09.190740: Unable to plot network architecture: nnUNet_compile is enabled!
2024-12-04 15:30:09.266546: 
2024-12-04 15:30:09.267989: Epoch 0
2024-12-04 15:30:09.269316: Current learning rate: 0.01
2024-12-04 15:32:45.236045: Validation loss improved from 1000.00000 to -0.11491! Patience: 0/50
2024-12-04 15:32:45.237009: train_loss -0.0852
2024-12-04 15:32:45.238034: val_loss -0.1149
2024-12-04 15:32:45.238849: Pseudo dice [0.4944]
2024-12-04 15:32:45.240500: Epoch time: 155.97 s
2024-12-04 15:32:45.241529: Yayy! New best EMA pseudo Dice: 0.4944
2024-12-04 15:32:46.734907: 
2024-12-04 15:32:46.736013: Epoch 1
2024-12-04 15:32:46.736678: Current learning rate: 0.00999
2024-12-04 15:34:17.804677: Validation loss improved from -0.11491 to -0.14815! Patience: 0/50
2024-12-04 15:34:17.805426: train_loss -0.2235
2024-12-04 15:34:17.806206: val_loss -0.1482
2024-12-04 15:34:17.806764: Pseudo dice [0.4938]
2024-12-04 15:34:17.807369: Epoch time: 91.07 s
2024-12-04 15:34:19.031951: 
2024-12-04 15:34:19.033227: Epoch 2
2024-12-04 15:34:19.033957: Current learning rate: 0.00998
2024-12-04 15:35:50.160689: Validation loss improved from -0.14815 to -0.21354! Patience: 0/50
2024-12-04 15:35:50.161609: train_loss -0.2842
2024-12-04 15:35:50.162374: val_loss -0.2135
2024-12-04 15:35:50.163112: Pseudo dice [0.5388]
2024-12-04 15:35:50.163690: Epoch time: 91.13 s
2024-12-04 15:35:50.164340: Yayy! New best EMA pseudo Dice: 0.4988
2024-12-04 15:35:51.806508: 
2024-12-04 15:35:51.808089: Epoch 3
2024-12-04 15:35:51.808869: Current learning rate: 0.00997
2024-12-04 15:37:23.038771: Validation loss did not improve from -0.21354. Patience: 1/50
2024-12-04 15:37:23.040323: train_loss -0.3184
2024-12-04 15:37:23.041595: val_loss -0.2052
2024-12-04 15:37:23.042755: Pseudo dice [0.54]
2024-12-04 15:37:23.043659: Epoch time: 91.23 s
2024-12-04 15:37:23.044378: Yayy! New best EMA pseudo Dice: 0.5029
2024-12-04 15:37:24.621682: 
2024-12-04 15:37:24.623007: Epoch 4
2024-12-04 15:37:24.623677: Current learning rate: 0.00996
2024-12-04 15:38:55.801218: Validation loss did not improve from -0.21354. Patience: 2/50
2024-12-04 15:38:55.802746: train_loss -0.3602
2024-12-04 15:38:55.803542: val_loss -0.2093
2024-12-04 15:38:55.804222: Pseudo dice [0.5423]
2024-12-04 15:38:55.804896: Epoch time: 91.18 s
2024-12-04 15:38:56.154992: Yayy! New best EMA pseudo Dice: 0.5069
2024-12-04 15:38:57.820298: 
2024-12-04 15:38:57.821408: Epoch 5
2024-12-04 15:38:57.822055: Current learning rate: 0.00995
2024-12-04 15:40:29.039582: Validation loss improved from -0.21354 to -0.32937! Patience: 2/50
2024-12-04 15:40:29.040508: train_loss -0.3675
2024-12-04 15:40:29.041290: val_loss -0.3294
2024-12-04 15:40:29.042031: Pseudo dice [0.6097]
2024-12-04 15:40:29.042806: Epoch time: 91.22 s
2024-12-04 15:40:29.043438: Yayy! New best EMA pseudo Dice: 0.5172
2024-12-04 15:40:30.614158: 
2024-12-04 15:40:30.615320: Epoch 6
2024-12-04 15:40:30.616006: Current learning rate: 0.00995
2024-12-04 15:42:01.824547: Validation loss did not improve from -0.32937. Patience: 1/50
2024-12-04 15:42:01.825484: train_loss -0.3936
2024-12-04 15:42:01.826445: val_loss -0.3081
2024-12-04 15:42:01.827466: Pseudo dice [0.588]
2024-12-04 15:42:01.828259: Epoch time: 91.21 s
2024-12-04 15:42:01.829097: Yayy! New best EMA pseudo Dice: 0.5242
2024-12-04 15:42:03.435728: 
2024-12-04 15:42:03.437490: Epoch 7
2024-12-04 15:42:03.438417: Current learning rate: 0.00994
2024-12-04 15:43:34.581649: Validation loss improved from -0.32937 to -0.33954! Patience: 1/50
2024-12-04 15:43:34.582841: train_loss -0.3991
2024-12-04 15:43:34.584004: val_loss -0.3395
2024-12-04 15:43:34.584837: Pseudo dice [0.6203]
2024-12-04 15:43:34.585562: Epoch time: 91.15 s
2024-12-04 15:43:34.586267: Yayy! New best EMA pseudo Dice: 0.5338
2024-12-04 15:43:36.613974: 
2024-12-04 15:43:36.615367: Epoch 8
2024-12-04 15:43:36.616105: Current learning rate: 0.00993
2024-12-04 15:45:07.904553: Validation loss improved from -0.33954 to -0.37019! Patience: 0/50
2024-12-04 15:45:07.905620: train_loss -0.4287
2024-12-04 15:45:07.906617: val_loss -0.3702
2024-12-04 15:45:07.907440: Pseudo dice [0.6421]
2024-12-04 15:45:07.908276: Epoch time: 91.29 s
2024-12-04 15:45:07.908931: Yayy! New best EMA pseudo Dice: 0.5447
2024-12-04 15:45:09.594711: 
2024-12-04 15:45:09.596132: Epoch 9
2024-12-04 15:45:09.596965: Current learning rate: 0.00992
2024-12-04 15:46:40.911088: Validation loss did not improve from -0.37019. Patience: 1/50
2024-12-04 15:46:40.911806: train_loss -0.4496
2024-12-04 15:46:40.912974: val_loss -0.3642
2024-12-04 15:46:40.913734: Pseudo dice [0.6492]
2024-12-04 15:46:40.914480: Epoch time: 91.32 s
2024-12-04 15:46:41.288015: Yayy! New best EMA pseudo Dice: 0.5551
2024-12-04 15:46:42.855417: 
2024-12-04 15:46:42.856873: Epoch 10
2024-12-04 15:46:42.857677: Current learning rate: 0.00991
2024-12-04 15:48:14.093003: Validation loss improved from -0.37019 to -0.41593! Patience: 1/50
2024-12-04 15:48:14.093899: train_loss -0.4602
2024-12-04 15:48:14.094652: val_loss -0.4159
2024-12-04 15:48:14.095437: Pseudo dice [0.6685]
2024-12-04 15:48:14.096145: Epoch time: 91.24 s
2024-12-04 15:48:14.096917: Yayy! New best EMA pseudo Dice: 0.5665
2024-12-04 15:48:15.646995: 
2024-12-04 15:48:15.648204: Epoch 11
2024-12-04 15:48:15.649014: Current learning rate: 0.0099
2024-12-04 15:49:46.905763: Validation loss did not improve from -0.41593. Patience: 1/50
2024-12-04 15:49:46.906632: train_loss -0.4572
2024-12-04 15:49:46.907393: val_loss -0.3917
2024-12-04 15:49:46.908039: Pseudo dice [0.6492]
2024-12-04 15:49:46.908682: Epoch time: 91.26 s
2024-12-04 15:49:46.909330: Yayy! New best EMA pseudo Dice: 0.5747
2024-12-04 15:49:48.473981: 
2024-12-04 15:49:48.475379: Epoch 12
2024-12-04 15:49:48.476078: Current learning rate: 0.00989
2024-12-04 15:51:19.849413: Validation loss improved from -0.41593 to -0.44706! Patience: 1/50
2024-12-04 15:51:19.850251: train_loss -0.4595
2024-12-04 15:51:19.850968: val_loss -0.4471
2024-12-04 15:51:19.851703: Pseudo dice [0.69]
2024-12-04 15:51:19.852311: Epoch time: 91.38 s
2024-12-04 15:51:19.852959: Yayy! New best EMA pseudo Dice: 0.5863
2024-12-04 15:51:21.464725: 
2024-12-04 15:51:21.465984: Epoch 13
2024-12-04 15:51:21.466603: Current learning rate: 0.00988
2024-12-04 15:52:52.789217: Validation loss did not improve from -0.44706. Patience: 1/50
2024-12-04 15:52:52.789981: train_loss -0.4742
2024-12-04 15:52:52.790734: val_loss -0.4412
2024-12-04 15:52:52.791519: Pseudo dice [0.6795]
2024-12-04 15:52:52.792165: Epoch time: 91.33 s
2024-12-04 15:52:52.792697: Yayy! New best EMA pseudo Dice: 0.5956
2024-12-04 15:52:54.420584: 
2024-12-04 15:52:54.422296: Epoch 14
2024-12-04 15:52:54.423244: Current learning rate: 0.00987
2024-12-04 15:54:25.750262: Validation loss did not improve from -0.44706. Patience: 2/50
2024-12-04 15:54:25.751530: train_loss -0.4931
2024-12-04 15:54:25.752645: val_loss -0.3527
2024-12-04 15:54:25.753538: Pseudo dice [0.6397]
2024-12-04 15:54:25.754369: Epoch time: 91.33 s
2024-12-04 15:54:26.160481: Yayy! New best EMA pseudo Dice: 0.6
2024-12-04 15:54:27.848001: 
2024-12-04 15:54:27.849124: Epoch 15
2024-12-04 15:54:27.850003: Current learning rate: 0.00986
2024-12-04 15:55:59.138772: Validation loss did not improve from -0.44706. Patience: 3/50
2024-12-04 15:55:59.139783: train_loss -0.4996
2024-12-04 15:55:59.140572: val_loss -0.3852
2024-12-04 15:55:59.141193: Pseudo dice [0.6548]
2024-12-04 15:55:59.141824: Epoch time: 91.29 s
2024-12-04 15:55:59.142369: Yayy! New best EMA pseudo Dice: 0.6055
2024-12-04 15:56:00.787540: 
2024-12-04 15:56:00.788581: Epoch 16
2024-12-04 15:56:00.789217: Current learning rate: 0.00986
2024-12-04 15:57:32.363904: Validation loss did not improve from -0.44706. Patience: 4/50
2024-12-04 15:57:32.364740: train_loss -0.4912
2024-12-04 15:57:32.365761: val_loss -0.4328
2024-12-04 15:57:32.366459: Pseudo dice [0.6685]
2024-12-04 15:57:32.367045: Epoch time: 91.58 s
2024-12-04 15:57:32.368026: Yayy! New best EMA pseudo Dice: 0.6118
2024-12-04 15:57:34.019673: 
2024-12-04 15:57:34.021051: Epoch 17
2024-12-04 15:57:34.021806: Current learning rate: 0.00985
2024-12-04 15:59:05.548862: Validation loss did not improve from -0.44706. Patience: 5/50
2024-12-04 15:59:05.549789: train_loss -0.5167
2024-12-04 15:59:05.550680: val_loss -0.4333
2024-12-04 15:59:05.551533: Pseudo dice [0.668]
2024-12-04 15:59:05.552197: Epoch time: 91.53 s
2024-12-04 15:59:05.552864: Yayy! New best EMA pseudo Dice: 0.6174
2024-12-04 15:59:07.193978: 
2024-12-04 15:59:07.195228: Epoch 18
2024-12-04 15:59:07.195989: Current learning rate: 0.00984
2024-12-04 16:00:38.478609: Validation loss did not improve from -0.44706. Patience: 6/50
2024-12-04 16:00:38.479503: train_loss -0.5096
2024-12-04 16:00:38.480438: val_loss -0.4221
2024-12-04 16:00:38.481101: Pseudo dice [0.6735]
2024-12-04 16:00:38.481771: Epoch time: 91.29 s
2024-12-04 16:00:38.482373: Yayy! New best EMA pseudo Dice: 0.623
2024-12-04 16:00:40.448416: 
2024-12-04 16:00:40.449423: Epoch 19
2024-12-04 16:00:40.450614: Current learning rate: 0.00983
2024-12-04 16:02:11.786469: Validation loss did not improve from -0.44706. Patience: 7/50
2024-12-04 16:02:11.787425: train_loss -0.5187
2024-12-04 16:02:11.788110: val_loss -0.4411
2024-12-04 16:02:11.788801: Pseudo dice [0.6902]
2024-12-04 16:02:11.789444: Epoch time: 91.34 s
2024-12-04 16:02:12.172328: Yayy! New best EMA pseudo Dice: 0.6297
2024-12-04 16:02:13.868289: 
2024-12-04 16:02:13.869366: Epoch 20
2024-12-04 16:02:13.870318: Current learning rate: 0.00982
2024-12-04 16:03:45.206370: Validation loss improved from -0.44706 to -0.44751! Patience: 7/50
2024-12-04 16:03:45.207484: train_loss -0.5237
2024-12-04 16:03:45.208322: val_loss -0.4475
2024-12-04 16:03:45.208967: Pseudo dice [0.6826]
2024-12-04 16:03:45.209676: Epoch time: 91.34 s
2024-12-04 16:03:45.210299: Yayy! New best EMA pseudo Dice: 0.635
2024-12-04 16:03:46.893467: 
2024-12-04 16:03:46.894912: Epoch 21
2024-12-04 16:03:46.895954: Current learning rate: 0.00981
2024-12-04 16:05:18.200190: Validation loss improved from -0.44751 to -0.48943! Patience: 0/50
2024-12-04 16:05:18.201033: train_loss -0.5327
2024-12-04 16:05:18.202051: val_loss -0.4894
2024-12-04 16:05:18.202687: Pseudo dice [0.7015]
2024-12-04 16:05:18.203278: Epoch time: 91.31 s
2024-12-04 16:05:18.203885: Yayy! New best EMA pseudo Dice: 0.6417
2024-12-04 16:05:19.761117: 
2024-12-04 16:05:19.762375: Epoch 22
2024-12-04 16:05:19.763031: Current learning rate: 0.0098
2024-12-04 16:06:51.077678: Validation loss did not improve from -0.48943. Patience: 1/50
2024-12-04 16:06:51.078362: train_loss -0.5291
2024-12-04 16:06:51.078985: val_loss -0.4121
2024-12-04 16:06:51.079605: Pseudo dice [0.6685]
2024-12-04 16:06:51.080164: Epoch time: 91.32 s
2024-12-04 16:06:51.080759: Yayy! New best EMA pseudo Dice: 0.6443
2024-12-04 16:06:52.687397: 
2024-12-04 16:06:52.688310: Epoch 23
2024-12-04 16:06:52.688970: Current learning rate: 0.00979
2024-12-04 16:08:23.970231: Validation loss did not improve from -0.48943. Patience: 2/50
2024-12-04 16:08:23.971098: train_loss -0.5496
2024-12-04 16:08:23.972190: val_loss -0.4589
2024-12-04 16:08:23.973100: Pseudo dice [0.6904]
2024-12-04 16:08:23.974014: Epoch time: 91.28 s
2024-12-04 16:08:23.974930: Yayy! New best EMA pseudo Dice: 0.6489
2024-12-04 16:08:25.538342: 
2024-12-04 16:08:25.539986: Epoch 24
2024-12-04 16:08:25.541190: Current learning rate: 0.00978
2024-12-04 16:09:56.847483: Validation loss did not improve from -0.48943. Patience: 3/50
2024-12-04 16:09:56.848365: train_loss -0.5583
2024-12-04 16:09:56.849281: val_loss -0.4704
2024-12-04 16:09:56.850085: Pseudo dice [0.693]
2024-12-04 16:09:56.850884: Epoch time: 91.31 s
2024-12-04 16:09:57.257648: Yayy! New best EMA pseudo Dice: 0.6534
2024-12-04 16:09:58.842716: 
2024-12-04 16:09:58.843869: Epoch 25
2024-12-04 16:09:58.844595: Current learning rate: 0.00977
2024-12-04 16:11:30.274638: Validation loss did not improve from -0.48943. Patience: 4/50
2024-12-04 16:11:30.275722: train_loss -0.5594
2024-12-04 16:11:30.276789: val_loss -0.4466
2024-12-04 16:11:30.277575: Pseudo dice [0.6763]
2024-12-04 16:11:30.278342: Epoch time: 91.43 s
2024-12-04 16:11:30.279108: Yayy! New best EMA pseudo Dice: 0.6557
2024-12-04 16:11:31.911272: 
2024-12-04 16:11:31.912344: Epoch 26
2024-12-04 16:11:31.913063: Current learning rate: 0.00977
2024-12-04 16:13:03.266609: Validation loss improved from -0.48943 to -0.49011! Patience: 4/50
2024-12-04 16:13:03.267364: train_loss -0.5583
2024-12-04 16:13:03.268233: val_loss -0.4901
2024-12-04 16:13:03.268796: Pseudo dice [0.698]
2024-12-04 16:13:03.269395: Epoch time: 91.36 s
2024-12-04 16:13:03.270153: Yayy! New best EMA pseudo Dice: 0.6599
2024-12-04 16:13:04.896937: 
2024-12-04 16:13:04.898010: Epoch 27
2024-12-04 16:13:04.899028: Current learning rate: 0.00976
2024-12-04 16:14:36.327395: Validation loss did not improve from -0.49011. Patience: 1/50
2024-12-04 16:14:36.328253: train_loss -0.5536
2024-12-04 16:14:36.329101: val_loss -0.4528
2024-12-04 16:14:36.329708: Pseudo dice [0.6852]
2024-12-04 16:14:36.330337: Epoch time: 91.43 s
2024-12-04 16:14:36.331063: Yayy! New best EMA pseudo Dice: 0.6624
2024-12-04 16:14:37.898385: 
2024-12-04 16:14:37.899724: Epoch 28
2024-12-04 16:14:37.900575: Current learning rate: 0.00975
2024-12-04 16:16:09.248722: Validation loss did not improve from -0.49011. Patience: 2/50
2024-12-04 16:16:09.249779: train_loss -0.562
2024-12-04 16:16:09.250738: val_loss -0.4339
2024-12-04 16:16:09.251466: Pseudo dice [0.6869]
2024-12-04 16:16:09.252155: Epoch time: 91.35 s
2024-12-04 16:16:09.252823: Yayy! New best EMA pseudo Dice: 0.6649
2024-12-04 16:16:11.126439: 
2024-12-04 16:16:11.127807: Epoch 29
2024-12-04 16:16:11.129037: Current learning rate: 0.00974
2024-12-04 16:17:42.469548: Validation loss did not improve from -0.49011. Patience: 3/50
2024-12-04 16:17:42.470234: train_loss -0.5506
2024-12-04 16:17:42.470991: val_loss -0.4236
2024-12-04 16:17:42.471760: Pseudo dice [0.6719]
2024-12-04 16:17:42.472615: Epoch time: 91.35 s
2024-12-04 16:17:42.893989: Yayy! New best EMA pseudo Dice: 0.6656
2024-12-04 16:17:44.576780: 
2024-12-04 16:17:44.578094: Epoch 30
2024-12-04 16:17:44.579178: Current learning rate: 0.00973
2024-12-04 16:19:15.916378: Validation loss did not improve from -0.49011. Patience: 4/50
2024-12-04 16:19:15.917642: train_loss -0.5551
2024-12-04 16:19:15.918611: val_loss -0.4323
2024-12-04 16:19:15.919555: Pseudo dice [0.6825]
2024-12-04 16:19:15.920268: Epoch time: 91.34 s
2024-12-04 16:19:15.921047: Yayy! New best EMA pseudo Dice: 0.6673
2024-12-04 16:19:17.586126: 
2024-12-04 16:19:17.587739: Epoch 31
2024-12-04 16:19:17.588527: Current learning rate: 0.00972
2024-12-04 16:20:48.886276: Validation loss did not improve from -0.49011. Patience: 5/50
2024-12-04 16:20:48.886918: train_loss -0.5756
2024-12-04 16:20:48.887751: val_loss -0.4817
2024-12-04 16:20:48.888558: Pseudo dice [0.7019]
2024-12-04 16:20:48.889333: Epoch time: 91.3 s
2024-12-04 16:20:48.890098: Yayy! New best EMA pseudo Dice: 0.6707
2024-12-04 16:20:50.520364: 
2024-12-04 16:20:50.521870: Epoch 32
2024-12-04 16:20:50.522677: Current learning rate: 0.00971
2024-12-04 16:22:21.765396: Validation loss improved from -0.49011 to -0.51359! Patience: 5/50
2024-12-04 16:22:21.766385: train_loss -0.5785
2024-12-04 16:22:21.767235: val_loss -0.5136
2024-12-04 16:22:21.768013: Pseudo dice [0.7233]
2024-12-04 16:22:21.768613: Epoch time: 91.25 s
2024-12-04 16:22:21.769176: Yayy! New best EMA pseudo Dice: 0.676
2024-12-04 16:22:23.415387: 
2024-12-04 16:22:23.416731: Epoch 33
2024-12-04 16:22:23.417420: Current learning rate: 0.0097
2024-12-04 16:23:54.659300: Validation loss did not improve from -0.51359. Patience: 1/50
2024-12-04 16:23:54.660336: train_loss -0.5984
2024-12-04 16:23:54.661116: val_loss -0.4919
2024-12-04 16:23:54.661879: Pseudo dice [0.7035]
2024-12-04 16:23:54.662603: Epoch time: 91.25 s
2024-12-04 16:23:54.663247: Yayy! New best EMA pseudo Dice: 0.6787
2024-12-04 16:23:56.306858: 
2024-12-04 16:23:56.308173: Epoch 34
2024-12-04 16:23:56.308925: Current learning rate: 0.00969
2024-12-04 16:25:27.545074: Validation loss did not improve from -0.51359. Patience: 2/50
2024-12-04 16:25:27.545990: train_loss -0.589
2024-12-04 16:25:27.546757: val_loss -0.4931
2024-12-04 16:25:27.547447: Pseudo dice [0.7112]
2024-12-04 16:25:27.548037: Epoch time: 91.24 s
2024-12-04 16:25:27.940732: Yayy! New best EMA pseudo Dice: 0.682
2024-12-04 16:25:29.593560: 
2024-12-04 16:25:29.594797: Epoch 35
2024-12-04 16:25:29.595400: Current learning rate: 0.00968
2024-12-04 16:27:01.009343: Validation loss did not improve from -0.51359. Patience: 3/50
2024-12-04 16:27:01.010441: train_loss -0.587
2024-12-04 16:27:01.011619: val_loss -0.4554
2024-12-04 16:27:01.012461: Pseudo dice [0.6883]
2024-12-04 16:27:01.013112: Epoch time: 91.42 s
2024-12-04 16:27:01.013712: Yayy! New best EMA pseudo Dice: 0.6826
2024-12-04 16:27:02.606530: 
2024-12-04 16:27:02.608356: Epoch 36
2024-12-04 16:27:02.609518: Current learning rate: 0.00968
2024-12-04 16:28:34.089280: Validation loss did not improve from -0.51359. Patience: 4/50
2024-12-04 16:28:34.090050: train_loss -0.5862
2024-12-04 16:28:34.091047: val_loss -0.499
2024-12-04 16:28:34.091767: Pseudo dice [0.711]
2024-12-04 16:28:34.092351: Epoch time: 91.48 s
2024-12-04 16:28:34.093068: Yayy! New best EMA pseudo Dice: 0.6854
2024-12-04 16:28:35.761718: 
2024-12-04 16:28:35.763121: Epoch 37
2024-12-04 16:28:35.763917: Current learning rate: 0.00967
2024-12-04 16:30:07.174535: Validation loss did not improve from -0.51359. Patience: 5/50
2024-12-04 16:30:07.175294: train_loss -0.5815
2024-12-04 16:30:07.176084: val_loss -0.504
2024-12-04 16:30:07.176807: Pseudo dice [0.712]
2024-12-04 16:30:07.177607: Epoch time: 91.41 s
2024-12-04 16:30:07.178276: Yayy! New best EMA pseudo Dice: 0.6881
2024-12-04 16:30:08.845610: 
2024-12-04 16:30:08.847202: Epoch 38
2024-12-04 16:30:08.847985: Current learning rate: 0.00966
2024-12-04 16:31:40.262084: Validation loss did not improve from -0.51359. Patience: 6/50
2024-12-04 16:31:40.262797: train_loss -0.5874
2024-12-04 16:31:40.263639: val_loss -0.4937
2024-12-04 16:31:40.264522: Pseudo dice [0.709]
2024-12-04 16:31:40.265315: Epoch time: 91.42 s
2024-12-04 16:31:40.266320: Yayy! New best EMA pseudo Dice: 0.6902
2024-12-04 16:31:41.935858: 
2024-12-04 16:31:41.937177: Epoch 39
2024-12-04 16:31:41.937917: Current learning rate: 0.00965
2024-12-04 16:33:13.160209: Validation loss did not improve from -0.51359. Patience: 7/50
2024-12-04 16:33:13.161320: train_loss -0.5973
2024-12-04 16:33:13.162116: val_loss -0.4987
2024-12-04 16:33:13.162835: Pseudo dice [0.7089]
2024-12-04 16:33:13.163642: Epoch time: 91.23 s
2024-12-04 16:33:13.551184: Yayy! New best EMA pseudo Dice: 0.6921
2024-12-04 16:33:15.720924: 
2024-12-04 16:33:15.722441: Epoch 40
2024-12-04 16:33:15.723459: Current learning rate: 0.00964
2024-12-04 16:34:46.963221: Validation loss did not improve from -0.51359. Patience: 8/50
2024-12-04 16:34:46.964035: train_loss -0.5935
2024-12-04 16:34:46.965257: val_loss -0.4912
2024-12-04 16:34:46.966110: Pseudo dice [0.698]
2024-12-04 16:34:46.966995: Epoch time: 91.24 s
2024-12-04 16:34:46.967783: Yayy! New best EMA pseudo Dice: 0.6927
2024-12-04 16:34:48.647923: 
2024-12-04 16:34:48.649789: Epoch 41
2024-12-04 16:34:48.650598: Current learning rate: 0.00963
2024-12-04 16:36:19.801909: Validation loss did not improve from -0.51359. Patience: 9/50
2024-12-04 16:36:19.803128: train_loss -0.6025
2024-12-04 16:36:19.804079: val_loss -0.4935
2024-12-04 16:36:19.805067: Pseudo dice [0.7175]
2024-12-04 16:36:19.805908: Epoch time: 91.16 s
2024-12-04 16:36:19.806779: Yayy! New best EMA pseudo Dice: 0.6951
2024-12-04 16:36:21.443782: 
2024-12-04 16:36:21.445203: Epoch 42
2024-12-04 16:36:21.446037: Current learning rate: 0.00962
2024-12-04 16:37:53.050950: Validation loss did not improve from -0.51359. Patience: 10/50
2024-12-04 16:37:53.066285: train_loss -0.5968
2024-12-04 16:37:53.068617: val_loss -0.5092
2024-12-04 16:37:53.069525: Pseudo dice [0.7145]
2024-12-04 16:37:53.070594: Epoch time: 91.62 s
2024-12-04 16:37:53.071366: Yayy! New best EMA pseudo Dice: 0.6971
2024-12-04 16:37:54.960381: 
2024-12-04 16:37:54.961990: Epoch 43
2024-12-04 16:37:54.962861: Current learning rate: 0.00961
2024-12-04 16:39:26.201775: Validation loss did not improve from -0.51359. Patience: 11/50
2024-12-04 16:39:26.202461: train_loss -0.5946
2024-12-04 16:39:26.203646: val_loss -0.4742
2024-12-04 16:39:26.204420: Pseudo dice [0.6974]
2024-12-04 16:39:26.205129: Epoch time: 91.24 s
2024-12-04 16:39:26.205888: Yayy! New best EMA pseudo Dice: 0.6971
2024-12-04 16:39:27.842729: 
2024-12-04 16:39:27.844361: Epoch 44
2024-12-04 16:39:27.845162: Current learning rate: 0.0096
2024-12-04 16:40:59.041750: Validation loss did not improve from -0.51359. Patience: 12/50
2024-12-04 16:40:59.042687: train_loss -0.609
2024-12-04 16:40:59.043478: val_loss -0.489
2024-12-04 16:40:59.044393: Pseudo dice [0.7159]
2024-12-04 16:40:59.045027: Epoch time: 91.2 s
2024-12-04 16:40:59.400670: Yayy! New best EMA pseudo Dice: 0.699
2024-12-04 16:41:01.003120: 
2024-12-04 16:41:01.004961: Epoch 45
2024-12-04 16:41:01.005808: Current learning rate: 0.00959
2024-12-04 16:42:32.288374: Validation loss did not improve from -0.51359. Patience: 13/50
2024-12-04 16:42:32.289130: train_loss -0.6091
2024-12-04 16:42:32.289931: val_loss -0.4814
2024-12-04 16:42:32.290536: Pseudo dice [0.7014]
2024-12-04 16:42:32.291103: Epoch time: 91.29 s
2024-12-04 16:42:32.291816: Yayy! New best EMA pseudo Dice: 0.6992
2024-12-04 16:42:33.884360: 
2024-12-04 16:42:33.886102: Epoch 46
2024-12-04 16:42:33.886929: Current learning rate: 0.00959
2024-12-04 16:44:05.124672: Validation loss did not improve from -0.51359. Patience: 14/50
2024-12-04 16:44:05.125632: train_loss -0.6155
2024-12-04 16:44:05.126366: val_loss -0.4754
2024-12-04 16:44:05.127117: Pseudo dice [0.6952]
2024-12-04 16:44:05.127771: Epoch time: 91.24 s
2024-12-04 16:44:06.320402: 
2024-12-04 16:44:06.322373: Epoch 47
2024-12-04 16:44:06.323045: Current learning rate: 0.00958
2024-12-04 16:45:37.621268: Validation loss did not improve from -0.51359. Patience: 15/50
2024-12-04 16:45:37.622177: train_loss -0.6105
2024-12-04 16:45:37.622854: val_loss -0.4717
2024-12-04 16:45:37.623514: Pseudo dice [0.7079]
2024-12-04 16:45:37.624140: Epoch time: 91.3 s
2024-12-04 16:45:37.624815: Yayy! New best EMA pseudo Dice: 0.6997
2024-12-04 16:45:39.267626: 
2024-12-04 16:45:39.269128: Epoch 48
2024-12-04 16:45:39.269789: Current learning rate: 0.00957
2024-12-04 16:47:10.609354: Validation loss did not improve from -0.51359. Patience: 16/50
2024-12-04 16:47:10.610050: train_loss -0.6232
2024-12-04 16:47:10.610645: val_loss -0.5081
2024-12-04 16:47:10.611239: Pseudo dice [0.7203]
2024-12-04 16:47:10.611957: Epoch time: 91.34 s
2024-12-04 16:47:10.612583: Yayy! New best EMA pseudo Dice: 0.7018
2024-12-04 16:47:12.240011: 
2024-12-04 16:47:12.241363: Epoch 49
2024-12-04 16:47:12.242687: Current learning rate: 0.00956
2024-12-04 16:48:43.563845: Validation loss did not improve from -0.51359. Patience: 17/50
2024-12-04 16:48:43.564646: train_loss -0.6137
2024-12-04 16:48:43.565649: val_loss -0.5061
2024-12-04 16:48:43.566498: Pseudo dice [0.7097]
2024-12-04 16:48:43.567332: Epoch time: 91.33 s
2024-12-04 16:48:43.968019: Yayy! New best EMA pseudo Dice: 0.7026
2024-12-04 16:48:45.914305: 
2024-12-04 16:48:45.916460: Epoch 50
2024-12-04 16:48:45.917613: Current learning rate: 0.00955
2024-12-04 16:50:17.310987: Validation loss did not improve from -0.51359. Patience: 18/50
2024-12-04 16:50:17.312078: train_loss -0.6302
2024-12-04 16:50:17.312931: val_loss -0.4935
2024-12-04 16:50:17.313614: Pseudo dice [0.7157]
2024-12-04 16:50:17.314279: Epoch time: 91.4 s
2024-12-04 16:50:17.314947: Yayy! New best EMA pseudo Dice: 0.7039
2024-12-04 16:50:18.943803: 
2024-12-04 16:50:18.945647: Epoch 51
2024-12-04 16:50:18.946478: Current learning rate: 0.00954
2024-12-04 16:51:50.338151: Validation loss did not improve from -0.51359. Patience: 19/50
2024-12-04 16:51:50.338952: train_loss -0.6331
2024-12-04 16:51:50.339698: val_loss -0.5065
2024-12-04 16:51:50.340329: Pseudo dice [0.7148]
2024-12-04 16:51:50.340996: Epoch time: 91.4 s
2024-12-04 16:51:50.341683: Yayy! New best EMA pseudo Dice: 0.705
2024-12-04 16:51:51.935099: 
2024-12-04 16:51:51.936782: Epoch 52
2024-12-04 16:51:51.937870: Current learning rate: 0.00953
2024-12-04 16:53:23.264415: Validation loss improved from -0.51359 to -0.53161! Patience: 19/50
2024-12-04 16:53:23.265295: train_loss -0.6329
2024-12-04 16:53:23.266090: val_loss -0.5316
2024-12-04 16:53:23.266863: Pseudo dice [0.7363]
2024-12-04 16:53:23.267495: Epoch time: 91.33 s
2024-12-04 16:53:23.268068: Yayy! New best EMA pseudo Dice: 0.7081
2024-12-04 16:53:24.887350: 
2024-12-04 16:53:24.888774: Epoch 53
2024-12-04 16:53:24.889456: Current learning rate: 0.00952
2024-12-04 16:54:56.241459: Validation loss did not improve from -0.53161. Patience: 1/50
2024-12-04 16:54:56.242143: train_loss -0.6314
2024-12-04 16:54:56.242875: val_loss -0.4998
2024-12-04 16:54:56.243573: Pseudo dice [0.7113]
2024-12-04 16:54:56.244216: Epoch time: 91.36 s
2024-12-04 16:54:56.244906: Yayy! New best EMA pseudo Dice: 0.7084
2024-12-04 16:54:57.871314: 
2024-12-04 16:54:57.873122: Epoch 54
2024-12-04 16:54:57.873979: Current learning rate: 0.00951
2024-12-04 16:56:29.391971: Validation loss did not improve from -0.53161. Patience: 2/50
2024-12-04 16:56:29.392831: train_loss -0.6385
2024-12-04 16:56:29.393564: val_loss -0.502
2024-12-04 16:56:29.394190: Pseudo dice [0.7118]
2024-12-04 16:56:29.394745: Epoch time: 91.52 s
2024-12-04 16:56:29.822598: Yayy! New best EMA pseudo Dice: 0.7088
2024-12-04 16:56:31.431796: 
2024-12-04 16:56:31.434001: Epoch 55
2024-12-04 16:56:31.434690: Current learning rate: 0.0095
2024-12-04 16:58:02.814145: Validation loss did not improve from -0.53161. Patience: 3/50
2024-12-04 16:58:02.815156: train_loss -0.6334
2024-12-04 16:58:02.815989: val_loss -0.5253
2024-12-04 16:58:02.816653: Pseudo dice [0.7296]
2024-12-04 16:58:02.817172: Epoch time: 91.38 s
2024-12-04 16:58:02.817753: Yayy! New best EMA pseudo Dice: 0.7109
2024-12-04 16:58:04.425724: 
2024-12-04 16:58:04.428155: Epoch 56
2024-12-04 16:58:04.428995: Current learning rate: 0.00949
2024-12-04 16:59:35.871620: Validation loss did not improve from -0.53161. Patience: 4/50
2024-12-04 16:59:35.872524: train_loss -0.6374
2024-12-04 16:59:35.873333: val_loss -0.5028
2024-12-04 16:59:35.874110: Pseudo dice [0.7127]
2024-12-04 16:59:35.874828: Epoch time: 91.45 s
2024-12-04 16:59:35.875530: Yayy! New best EMA pseudo Dice: 0.711
2024-12-04 16:59:37.508317: 
2024-12-04 16:59:37.510451: Epoch 57
2024-12-04 16:59:37.511436: Current learning rate: 0.00949
2024-12-04 17:01:08.817317: Validation loss did not improve from -0.53161. Patience: 5/50
2024-12-04 17:01:08.818115: train_loss -0.6461
2024-12-04 17:01:08.818747: val_loss -0.5087
2024-12-04 17:01:08.819759: Pseudo dice [0.7181]
2024-12-04 17:01:08.820433: Epoch time: 91.31 s
2024-12-04 17:01:08.820968: Yayy! New best EMA pseudo Dice: 0.7118
2024-12-04 17:01:10.415469: 
2024-12-04 17:01:10.416999: Epoch 58
2024-12-04 17:01:10.417761: Current learning rate: 0.00948
2024-12-04 17:02:41.900460: Validation loss did not improve from -0.53161. Patience: 6/50
2024-12-04 17:02:41.901497: train_loss -0.6438
2024-12-04 17:02:41.902314: val_loss -0.4929
2024-12-04 17:02:41.903011: Pseudo dice [0.7119]
2024-12-04 17:02:41.903728: Epoch time: 91.49 s
2024-12-04 17:02:41.904396: Yayy! New best EMA pseudo Dice: 0.7118
2024-12-04 17:02:43.571913: 
2024-12-04 17:02:43.573621: Epoch 59
2024-12-04 17:02:43.574361: Current learning rate: 0.00947
2024-12-04 17:04:15.019929: Validation loss did not improve from -0.53161. Patience: 7/50
2024-12-04 17:04:15.020741: train_loss -0.6458
2024-12-04 17:04:15.021524: val_loss -0.5147
2024-12-04 17:04:15.022214: Pseudo dice [0.7228]
2024-12-04 17:04:15.022861: Epoch time: 91.45 s
2024-12-04 17:04:15.413971: Yayy! New best EMA pseudo Dice: 0.7129
2024-12-04 17:04:17.045438: 
2024-12-04 17:04:17.047367: Epoch 60
2024-12-04 17:04:17.048404: Current learning rate: 0.00946
2024-12-04 17:05:48.314423: Validation loss did not improve from -0.53161. Patience: 8/50
2024-12-04 17:05:48.315294: train_loss -0.6511
2024-12-04 17:05:48.316335: val_loss -0.5173
2024-12-04 17:05:48.316902: Pseudo dice [0.7243]
2024-12-04 17:05:48.317466: Epoch time: 91.27 s
2024-12-04 17:05:48.318062: Yayy! New best EMA pseudo Dice: 0.714
2024-12-04 17:05:50.294720: 
2024-12-04 17:05:50.296529: Epoch 61
2024-12-04 17:05:50.297292: Current learning rate: 0.00945
2024-12-04 17:07:21.631192: Validation loss did not improve from -0.53161. Patience: 9/50
2024-12-04 17:07:21.631908: train_loss -0.6535
2024-12-04 17:07:21.632543: val_loss -0.5075
2024-12-04 17:07:21.633121: Pseudo dice [0.7223]
2024-12-04 17:07:21.633775: Epoch time: 91.34 s
2024-12-04 17:07:21.634346: Yayy! New best EMA pseudo Dice: 0.7148
2024-12-04 17:07:23.268029: 
2024-12-04 17:07:23.269565: Epoch 62
2024-12-04 17:07:23.270258: Current learning rate: 0.00944
2024-12-04 17:08:54.763238: Validation loss did not improve from -0.53161. Patience: 10/50
2024-12-04 17:08:54.764286: train_loss -0.6543
2024-12-04 17:08:54.765061: val_loss -0.5051
2024-12-04 17:08:54.765670: Pseudo dice [0.7152]
2024-12-04 17:08:54.766258: Epoch time: 91.5 s
2024-12-04 17:08:54.766859: Yayy! New best EMA pseudo Dice: 0.7149
2024-12-04 17:08:56.377085: 
2024-12-04 17:08:56.378748: Epoch 63
2024-12-04 17:08:56.379532: Current learning rate: 0.00943
2024-12-04 17:10:27.797508: Validation loss did not improve from -0.53161. Patience: 11/50
2024-12-04 17:10:27.798437: train_loss -0.6536
2024-12-04 17:10:27.799327: val_loss -0.5004
2024-12-04 17:10:27.800240: Pseudo dice [0.7189]
2024-12-04 17:10:27.800990: Epoch time: 91.42 s
2024-12-04 17:10:27.801644: Yayy! New best EMA pseudo Dice: 0.7153
2024-12-04 17:10:29.532442: 
2024-12-04 17:10:29.534151: Epoch 64
2024-12-04 17:10:29.534868: Current learning rate: 0.00942
2024-12-04 17:12:00.920952: Validation loss did not improve from -0.53161. Patience: 12/50
2024-12-04 17:12:00.921917: train_loss -0.6521
2024-12-04 17:12:00.922660: val_loss -0.503
2024-12-04 17:12:00.923378: Pseudo dice [0.7164]
2024-12-04 17:12:00.924048: Epoch time: 91.39 s
2024-12-04 17:12:01.312542: Yayy! New best EMA pseudo Dice: 0.7154
2024-12-04 17:12:02.969210: 
2024-12-04 17:12:02.971112: Epoch 65
2024-12-04 17:12:02.972138: Current learning rate: 0.00941
2024-12-04 17:13:34.210607: Validation loss did not improve from -0.53161. Patience: 13/50
2024-12-04 17:13:34.211227: train_loss -0.6495
2024-12-04 17:13:34.211822: val_loss -0.4914
2024-12-04 17:13:34.212442: Pseudo dice [0.7146]
2024-12-04 17:13:34.213065: Epoch time: 91.24 s
2024-12-04 17:13:35.480141: 
2024-12-04 17:13:35.481254: Epoch 66
2024-12-04 17:13:35.481899: Current learning rate: 0.0094
2024-12-04 17:15:06.731617: Validation loss improved from -0.53161 to -0.53886! Patience: 13/50
2024-12-04 17:15:06.732597: train_loss -0.6588
2024-12-04 17:15:06.733323: val_loss -0.5389
2024-12-04 17:15:06.734104: Pseudo dice [0.7275]
2024-12-04 17:15:06.734746: Epoch time: 91.25 s
2024-12-04 17:15:06.735448: Yayy! New best EMA pseudo Dice: 0.7165
2024-12-04 17:15:08.363658: 
2024-12-04 17:15:08.365575: Epoch 67
2024-12-04 17:15:08.366523: Current learning rate: 0.00939
2024-12-04 17:16:39.671226: Validation loss did not improve from -0.53886. Patience: 1/50
2024-12-04 17:16:39.672169: train_loss -0.6587
2024-12-04 17:16:39.673127: val_loss -0.5067
2024-12-04 17:16:39.673805: Pseudo dice [0.7116]
2024-12-04 17:16:39.674441: Epoch time: 91.31 s
2024-12-04 17:16:40.945747: 
2024-12-04 17:16:40.948049: Epoch 68
2024-12-04 17:16:40.949057: Current learning rate: 0.00939
2024-12-04 17:18:12.201951: Validation loss did not improve from -0.53886. Patience: 2/50
2024-12-04 17:18:12.203113: train_loss -0.656
2024-12-04 17:18:12.204163: val_loss -0.5071
2024-12-04 17:18:12.205026: Pseudo dice [0.7176]
2024-12-04 17:18:12.206088: Epoch time: 91.26 s
2024-12-04 17:18:13.511311: 
2024-12-04 17:18:13.513508: Epoch 69
2024-12-04 17:18:13.514507: Current learning rate: 0.00938
2024-12-04 17:19:44.800169: Validation loss did not improve from -0.53886. Patience: 3/50
2024-12-04 17:19:44.801369: train_loss -0.6529
2024-12-04 17:19:44.802190: val_loss -0.503
2024-12-04 17:19:44.802905: Pseudo dice [0.7195]
2024-12-04 17:19:44.803683: Epoch time: 91.29 s
2024-12-04 17:19:46.450634: 
2024-12-04 17:19:46.452637: Epoch 70
2024-12-04 17:19:46.453309: Current learning rate: 0.00937
2024-12-04 17:21:17.730395: Validation loss did not improve from -0.53886. Patience: 4/50
2024-12-04 17:21:17.731234: train_loss -0.6584
2024-12-04 17:21:17.732198: val_loss -0.4672
2024-12-04 17:21:17.732897: Pseudo dice [0.7072]
2024-12-04 17:21:17.733567: Epoch time: 91.28 s
2024-12-04 17:21:19.327082: 
2024-12-04 17:21:19.328878: Epoch 71
2024-12-04 17:21:19.329721: Current learning rate: 0.00936
2024-12-04 17:22:50.655031: Validation loss did not improve from -0.53886. Patience: 5/50
2024-12-04 17:22:50.656059: train_loss -0.6582
2024-12-04 17:22:50.657102: val_loss -0.5155
2024-12-04 17:22:50.657952: Pseudo dice [0.716]
2024-12-04 17:22:50.658594: Epoch time: 91.33 s
2024-12-04 17:22:51.940257: 
2024-12-04 17:22:51.942440: Epoch 72
2024-12-04 17:22:51.943942: Current learning rate: 0.00935
2024-12-04 17:24:23.621938: Validation loss did not improve from -0.53886. Patience: 6/50
2024-12-04 17:24:23.622685: train_loss -0.6587
2024-12-04 17:24:23.623534: val_loss -0.5015
2024-12-04 17:24:23.624331: Pseudo dice [0.7251]
2024-12-04 17:24:23.625250: Epoch time: 91.68 s
2024-12-04 17:24:23.626092: Yayy! New best EMA pseudo Dice: 0.7166
2024-12-04 17:24:25.266207: 
2024-12-04 17:24:25.268338: Epoch 73
2024-12-04 17:24:25.269322: Current learning rate: 0.00934
2024-12-04 17:25:56.701987: Validation loss improved from -0.53886 to -0.55533! Patience: 6/50
2024-12-04 17:25:56.702994: train_loss -0.6584
2024-12-04 17:25:56.703819: val_loss -0.5553
2024-12-04 17:25:56.704631: Pseudo dice [0.7505]
2024-12-04 17:25:56.705312: Epoch time: 91.44 s
2024-12-04 17:25:56.705909: Yayy! New best EMA pseudo Dice: 0.72
2024-12-04 17:25:58.372138: 
2024-12-04 17:25:58.374184: Epoch 74
2024-12-04 17:25:58.375003: Current learning rate: 0.00933
2024-12-04 17:27:29.837093: Validation loss did not improve from -0.55533. Patience: 1/50
2024-12-04 17:27:29.838052: train_loss -0.6635
2024-12-04 17:27:29.838899: val_loss -0.5427
2024-12-04 17:27:29.839673: Pseudo dice [0.7409]
2024-12-04 17:27:29.840245: Epoch time: 91.47 s
2024-12-04 17:27:30.249542: Yayy! New best EMA pseudo Dice: 0.7221
2024-12-04 17:27:31.902026: 
2024-12-04 17:27:31.903566: Epoch 75
2024-12-04 17:27:31.904237: Current learning rate: 0.00932
2024-12-04 17:29:03.326202: Validation loss did not improve from -0.55533. Patience: 2/50
2024-12-04 17:29:03.326987: train_loss -0.6691
2024-12-04 17:29:03.327858: val_loss -0.5192
2024-12-04 17:29:03.328539: Pseudo dice [0.7321]
2024-12-04 17:29:03.329123: Epoch time: 91.43 s
2024-12-04 17:29:03.329852: Yayy! New best EMA pseudo Dice: 0.7231
2024-12-04 17:29:05.025283: 
2024-12-04 17:29:05.026963: Epoch 76
2024-12-04 17:29:05.027687: Current learning rate: 0.00931
2024-12-04 17:30:36.338216: Validation loss did not improve from -0.55533. Patience: 3/50
2024-12-04 17:30:36.339087: train_loss -0.6781
2024-12-04 17:30:36.339935: val_loss -0.5218
2024-12-04 17:30:36.340537: Pseudo dice [0.729]
2024-12-04 17:30:36.341132: Epoch time: 91.31 s
2024-12-04 17:30:36.341782: Yayy! New best EMA pseudo Dice: 0.7237
2024-12-04 17:30:38.039231: 
2024-12-04 17:30:38.040620: Epoch 77
2024-12-04 17:30:38.041328: Current learning rate: 0.0093
2024-12-04 17:32:09.501361: Validation loss did not improve from -0.55533. Patience: 4/50
2024-12-04 17:32:09.502346: train_loss -0.67
2024-12-04 17:32:09.503043: val_loss -0.5028
2024-12-04 17:32:09.503638: Pseudo dice [0.7187]
2024-12-04 17:32:09.504333: Epoch time: 91.46 s
2024-12-04 17:32:10.847789: 
2024-12-04 17:32:10.849868: Epoch 78
2024-12-04 17:32:10.850678: Current learning rate: 0.0093
2024-12-04 17:33:42.293239: Validation loss did not improve from -0.55533. Patience: 5/50
2024-12-04 17:33:42.293975: train_loss -0.6746
2024-12-04 17:33:42.294725: val_loss -0.4973
2024-12-04 17:33:42.295745: Pseudo dice [0.7131]
2024-12-04 17:33:42.296549: Epoch time: 91.45 s
2024-12-04 17:33:43.605519: 
2024-12-04 17:33:43.607855: Epoch 79
2024-12-04 17:33:43.608713: Current learning rate: 0.00929
2024-12-04 17:35:15.146064: Validation loss did not improve from -0.55533. Patience: 6/50
2024-12-04 17:35:15.147247: train_loss -0.6828
2024-12-04 17:35:15.148021: val_loss -0.5272
2024-12-04 17:35:15.148711: Pseudo dice [0.7378]
2024-12-04 17:35:15.149315: Epoch time: 91.54 s
2024-12-04 17:35:15.624947: Yayy! New best EMA pseudo Dice: 0.7237
2024-12-04 17:35:17.581547: 
2024-12-04 17:35:17.583271: Epoch 80
2024-12-04 17:35:17.584060: Current learning rate: 0.00928
2024-12-04 17:36:49.170866: Validation loss did not improve from -0.55533. Patience: 7/50
2024-12-04 17:36:49.171779: train_loss -0.6789
2024-12-04 17:36:49.172702: val_loss -0.5365
2024-12-04 17:36:49.173521: Pseudo dice [0.7391]
2024-12-04 17:36:49.174337: Epoch time: 91.59 s
2024-12-04 17:36:49.175202: Yayy! New best EMA pseudo Dice: 0.7253
2024-12-04 17:36:51.172996: 
2024-12-04 17:36:51.174426: Epoch 81
2024-12-04 17:36:51.175291: Current learning rate: 0.00927
2024-12-04 17:38:22.788275: Validation loss did not improve from -0.55533. Patience: 8/50
2024-12-04 17:38:22.789047: train_loss -0.6784
2024-12-04 17:38:22.789831: val_loss -0.4958
2024-12-04 17:38:22.790488: Pseudo dice [0.7112]
2024-12-04 17:38:22.791113: Epoch time: 91.62 s
2024-12-04 17:38:24.094210: 
2024-12-04 17:38:24.095651: Epoch 82
2024-12-04 17:38:24.096752: Current learning rate: 0.00926
2024-12-04 17:39:56.663226: Validation loss did not improve from -0.55533. Patience: 9/50
2024-12-04 17:39:56.675454: train_loss -0.6824
2024-12-04 17:39:56.677277: val_loss -0.5184
2024-12-04 17:39:56.678026: Pseudo dice [0.7335]
2024-12-04 17:39:56.678672: Epoch time: 92.57 s
2024-12-04 17:39:58.688992: 
2024-12-04 17:39:58.690095: Epoch 83
2024-12-04 17:39:58.690807: Current learning rate: 0.00925
2024-12-04 17:41:31.021151: Validation loss did not improve from -0.55533. Patience: 10/50
2024-12-04 17:41:31.021986: train_loss -0.6792
2024-12-04 17:41:31.022635: val_loss -0.526
2024-12-04 17:41:31.023209: Pseudo dice [0.7338]
2024-12-04 17:41:31.023813: Epoch time: 92.33 s
2024-12-04 17:41:31.024447: Yayy! New best EMA pseudo Dice: 0.7257
2024-12-04 17:41:32.688636: 
2024-12-04 17:41:32.690495: Epoch 84
2024-12-04 17:41:32.691556: Current learning rate: 0.00924
2024-12-04 17:43:04.114253: Validation loss did not improve from -0.55533. Patience: 11/50
2024-12-04 17:43:04.115193: train_loss -0.6745
2024-12-04 17:43:04.116197: val_loss -0.5341
2024-12-04 17:43:04.117158: Pseudo dice [0.7369]
2024-12-04 17:43:04.118056: Epoch time: 91.43 s
2024-12-04 17:43:04.509364: Yayy! New best EMA pseudo Dice: 0.7268
2024-12-04 17:43:06.090404: 
2024-12-04 17:43:06.091692: Epoch 85
2024-12-04 17:43:06.092695: Current learning rate: 0.00923
2024-12-04 17:44:37.461207: Validation loss did not improve from -0.55533. Patience: 12/50
2024-12-04 17:44:37.462257: train_loss -0.6843
2024-12-04 17:44:37.463415: val_loss -0.5041
2024-12-04 17:44:37.464514: Pseudo dice [0.7205]
2024-12-04 17:44:37.465677: Epoch time: 91.37 s
2024-12-04 17:44:38.716678: 
2024-12-04 17:44:38.718634: Epoch 86
2024-12-04 17:44:38.719721: Current learning rate: 0.00922
2024-12-04 17:46:09.916575: Validation loss did not improve from -0.55533. Patience: 13/50
2024-12-04 17:46:09.917285: train_loss -0.6633
2024-12-04 17:46:09.918127: val_loss -0.5268
2024-12-04 17:46:09.918760: Pseudo dice [0.7324]
2024-12-04 17:46:09.919360: Epoch time: 91.2 s
2024-12-04 17:46:11.164270: 
2024-12-04 17:46:11.165912: Epoch 87
2024-12-04 17:46:11.167058: Current learning rate: 0.00921
2024-12-04 17:47:42.451822: Validation loss did not improve from -0.55533. Patience: 14/50
2024-12-04 17:47:42.452615: train_loss -0.6783
2024-12-04 17:47:42.453375: val_loss -0.4948
2024-12-04 17:47:42.454053: Pseudo dice [0.7105]
2024-12-04 17:47:42.454693: Epoch time: 91.29 s
2024-12-04 17:47:43.669734: 
2024-12-04 17:47:43.671389: Epoch 88
2024-12-04 17:47:43.672131: Current learning rate: 0.0092
2024-12-04 17:49:14.899218: Validation loss did not improve from -0.55533. Patience: 15/50
2024-12-04 17:49:14.900245: train_loss -0.6833
2024-12-04 17:49:14.901163: val_loss -0.4913
2024-12-04 17:49:14.901999: Pseudo dice [0.7167]
2024-12-04 17:49:14.902848: Epoch time: 91.23 s
2024-12-04 17:49:16.178712: 
2024-12-04 17:49:16.180164: Epoch 89
2024-12-04 17:49:16.180879: Current learning rate: 0.0092
2024-12-04 17:50:47.343431: Validation loss did not improve from -0.55533. Patience: 16/50
2024-12-04 17:50:47.344505: train_loss -0.6669
2024-12-04 17:50:47.345408: val_loss -0.5027
2024-12-04 17:50:47.346337: Pseudo dice [0.7236]
2024-12-04 17:50:47.347002: Epoch time: 91.17 s
2024-12-04 17:50:48.917092: 
2024-12-04 17:50:48.918797: Epoch 90
2024-12-04 17:50:48.919713: Current learning rate: 0.00919
2024-12-04 17:52:20.129653: Validation loss did not improve from -0.55533. Patience: 17/50
2024-12-04 17:52:20.130628: train_loss -0.6846
2024-12-04 17:52:20.131444: val_loss -0.5089
2024-12-04 17:52:20.132536: Pseudo dice [0.7265]
2024-12-04 17:52:20.133302: Epoch time: 91.21 s
2024-12-04 17:52:21.369132: 
2024-12-04 17:52:21.371147: Epoch 91
2024-12-04 17:52:21.371933: Current learning rate: 0.00918
2024-12-04 17:53:52.494919: Validation loss improved from -0.55533 to -0.56437! Patience: 17/50
2024-12-04 17:53:52.495672: train_loss -0.6863
2024-12-04 17:53:52.496446: val_loss -0.5644
2024-12-04 17:53:52.497169: Pseudo dice [0.7491]
2024-12-04 17:53:52.497892: Epoch time: 91.13 s
2024-12-04 17:53:52.498487: Yayy! New best EMA pseudo Dice: 0.727
2024-12-04 17:53:54.487506: 
2024-12-04 17:53:54.488782: Epoch 92
2024-12-04 17:53:54.489522: Current learning rate: 0.00917
2024-12-04 17:55:25.716494: Validation loss did not improve from -0.56437. Patience: 1/50
2024-12-04 17:55:25.717107: train_loss -0.6939
2024-12-04 17:55:25.717835: val_loss -0.5242
2024-12-04 17:55:25.718370: Pseudo dice [0.7243]
2024-12-04 17:55:25.718892: Epoch time: 91.23 s
2024-12-04 17:55:26.921834: 
2024-12-04 17:55:26.923196: Epoch 93
2024-12-04 17:55:26.923836: Current learning rate: 0.00916
2024-12-04 17:56:58.284640: Validation loss did not improve from -0.56437. Patience: 2/50
2024-12-04 17:56:58.285422: train_loss -0.6929
2024-12-04 17:56:58.286113: val_loss -0.4892
2024-12-04 17:56:58.286683: Pseudo dice [0.7104]
2024-12-04 17:56:58.287259: Epoch time: 91.36 s
2024-12-04 17:56:59.544824: 
2024-12-04 17:56:59.546355: Epoch 94
2024-12-04 17:56:59.547077: Current learning rate: 0.00915
2024-12-04 17:58:30.902942: Validation loss did not improve from -0.56437. Patience: 3/50
2024-12-04 17:58:30.903816: train_loss -0.6905
2024-12-04 17:58:30.904694: val_loss -0.5245
2024-12-04 17:58:30.905341: Pseudo dice [0.735]
2024-12-04 17:58:30.906024: Epoch time: 91.36 s
2024-12-04 17:58:32.624035: 
2024-12-04 17:58:32.625380: Epoch 95
2024-12-04 17:58:32.626099: Current learning rate: 0.00914
2024-12-04 18:00:04.089544: Validation loss did not improve from -0.56437. Patience: 4/50
2024-12-04 18:00:04.090449: train_loss -0.6936
2024-12-04 18:00:04.091118: val_loss -0.5426
2024-12-04 18:00:04.091698: Pseudo dice [0.7414]
2024-12-04 18:00:04.092311: Epoch time: 91.47 s
2024-12-04 18:00:04.092857: Yayy! New best EMA pseudo Dice: 0.7276
2024-12-04 18:00:05.720034: 
2024-12-04 18:00:05.721838: Epoch 96
2024-12-04 18:00:05.722929: Current learning rate: 0.00913
2024-12-04 18:01:37.225634: Validation loss did not improve from -0.56437. Patience: 5/50
2024-12-04 18:01:37.227303: train_loss -0.6897
2024-12-04 18:01:37.228218: val_loss -0.5249
2024-12-04 18:01:37.228878: Pseudo dice [0.7299]
2024-12-04 18:01:37.229621: Epoch time: 91.51 s
2024-12-04 18:01:37.230243: Yayy! New best EMA pseudo Dice: 0.7278
2024-12-04 18:01:38.778226: 
2024-12-04 18:01:38.779269: Epoch 97
2024-12-04 18:01:38.779893: Current learning rate: 0.00912
2024-12-04 18:03:10.260731: Validation loss did not improve from -0.56437. Patience: 6/50
2024-12-04 18:03:10.261457: train_loss -0.6925
2024-12-04 18:03:10.262261: val_loss -0.5428
2024-12-04 18:03:10.263016: Pseudo dice [0.7362]
2024-12-04 18:03:10.263628: Epoch time: 91.48 s
2024-12-04 18:03:10.264385: Yayy! New best EMA pseudo Dice: 0.7287
2024-12-04 18:03:11.882994: 
2024-12-04 18:03:11.884474: Epoch 98
2024-12-04 18:03:11.885219: Current learning rate: 0.00911
2024-12-04 18:04:43.365197: Validation loss did not improve from -0.56437. Patience: 7/50
2024-12-04 18:04:43.366181: train_loss -0.6986
2024-12-04 18:04:43.367598: val_loss -0.509
2024-12-04 18:04:43.368430: Pseudo dice [0.727]
2024-12-04 18:04:43.369123: Epoch time: 91.48 s
2024-12-04 18:04:44.567175: 
2024-12-04 18:04:44.569233: Epoch 99
2024-12-04 18:04:44.570299: Current learning rate: 0.0091
2024-12-04 18:06:16.097282: Validation loss did not improve from -0.56437. Patience: 8/50
2024-12-04 18:06:16.098134: train_loss -0.6933
2024-12-04 18:06:16.098833: val_loss -0.4922
2024-12-04 18:06:16.099555: Pseudo dice [0.714]
2024-12-04 18:06:16.100250: Epoch time: 91.53 s
2024-12-04 18:06:17.682660: 
2024-12-04 18:06:17.684459: Epoch 100
2024-12-04 18:06:17.685589: Current learning rate: 0.0091
2024-12-04 18:07:49.090080: Validation loss did not improve from -0.56437. Patience: 9/50
2024-12-04 18:07:49.091078: train_loss -0.7046
2024-12-04 18:07:49.091868: val_loss -0.5214
2024-12-04 18:07:49.092450: Pseudo dice [0.7369]
2024-12-04 18:07:49.093100: Epoch time: 91.41 s
2024-12-04 18:07:50.310531: 
2024-12-04 18:07:50.312281: Epoch 101
2024-12-04 18:07:50.313097: Current learning rate: 0.00909
2024-12-04 18:09:21.928567: Validation loss did not improve from -0.56437. Patience: 10/50
2024-12-04 18:09:21.929354: train_loss -0.7088
2024-12-04 18:09:21.930151: val_loss -0.5365
2024-12-04 18:09:21.930868: Pseudo dice [0.7428]
2024-12-04 18:09:21.931590: Epoch time: 91.62 s
2024-12-04 18:09:21.932373: Yayy! New best EMA pseudo Dice: 0.7295
2024-12-04 18:09:23.651728: 
2024-12-04 18:09:23.653416: Epoch 102
2024-12-04 18:09:23.654412: Current learning rate: 0.00908
2024-12-04 18:10:55.202044: Validation loss did not improve from -0.56437. Patience: 11/50
2024-12-04 18:10:55.203041: train_loss -0.7118
2024-12-04 18:10:55.203958: val_loss -0.5182
2024-12-04 18:10:55.204576: Pseudo dice [0.7362]
2024-12-04 18:10:55.205305: Epoch time: 91.55 s
2024-12-04 18:10:55.205864: Yayy! New best EMA pseudo Dice: 0.7302
2024-12-04 18:10:57.120520: 
2024-12-04 18:10:57.122187: Epoch 103
2024-12-04 18:10:57.122872: Current learning rate: 0.00907
2024-12-04 18:12:28.719367: Validation loss did not improve from -0.56437. Patience: 12/50
2024-12-04 18:12:28.720363: train_loss -0.7081
2024-12-04 18:12:28.721273: val_loss -0.5562
2024-12-04 18:12:28.721947: Pseudo dice [0.754]
2024-12-04 18:12:28.722739: Epoch time: 91.6 s
2024-12-04 18:12:28.723510: Yayy! New best EMA pseudo Dice: 0.7326
2024-12-04 18:12:30.360279: 
2024-12-04 18:12:30.361695: Epoch 104
2024-12-04 18:12:30.362563: Current learning rate: 0.00906
2024-12-04 18:14:01.853609: Validation loss did not improve from -0.56437. Patience: 13/50
2024-12-04 18:14:01.854685: train_loss -0.7026
2024-12-04 18:14:01.855533: val_loss -0.5303
2024-12-04 18:14:01.856080: Pseudo dice [0.7353]
2024-12-04 18:14:01.856649: Epoch time: 91.5 s
2024-12-04 18:14:02.286196: Yayy! New best EMA pseudo Dice: 0.7328
2024-12-04 18:14:03.951384: 
2024-12-04 18:14:03.953465: Epoch 105
2024-12-04 18:14:03.954320: Current learning rate: 0.00905
2024-12-04 18:15:35.373316: Validation loss did not improve from -0.56437. Patience: 14/50
2024-12-04 18:15:35.374163: train_loss -0.7065
2024-12-04 18:15:35.374890: val_loss -0.5223
2024-12-04 18:15:35.375629: Pseudo dice [0.7344]
2024-12-04 18:15:35.376208: Epoch time: 91.42 s
2024-12-04 18:15:35.376822: Yayy! New best EMA pseudo Dice: 0.733
2024-12-04 18:15:37.012270: 
2024-12-04 18:15:37.014460: Epoch 106
2024-12-04 18:15:37.015212: Current learning rate: 0.00904
2024-12-04 18:17:08.541672: Validation loss did not improve from -0.56437. Patience: 15/50
2024-12-04 18:17:08.542384: train_loss -0.7021
2024-12-04 18:17:08.543205: val_loss -0.5321
2024-12-04 18:17:08.543843: Pseudo dice [0.7329]
2024-12-04 18:17:08.544428: Epoch time: 91.53 s
2024-12-04 18:17:09.779141: 
2024-12-04 18:17:09.780646: Epoch 107
2024-12-04 18:17:09.781289: Current learning rate: 0.00903
2024-12-04 18:18:41.337420: Validation loss did not improve from -0.56437. Patience: 16/50
2024-12-04 18:18:41.338639: train_loss -0.695
2024-12-04 18:18:41.339663: val_loss -0.5334
2024-12-04 18:18:41.340296: Pseudo dice [0.741]
2024-12-04 18:18:41.340884: Epoch time: 91.56 s
2024-12-04 18:18:41.341478: Yayy! New best EMA pseudo Dice: 0.7338
2024-12-04 18:18:42.963528: 
2024-12-04 18:18:42.964944: Epoch 108
2024-12-04 18:18:42.965698: Current learning rate: 0.00902
2024-12-04 18:20:14.423091: Validation loss did not improve from -0.56437. Patience: 17/50
2024-12-04 18:20:14.424124: train_loss -0.6978
2024-12-04 18:20:14.425158: val_loss -0.5483
2024-12-04 18:20:14.425967: Pseudo dice [0.7438]
2024-12-04 18:20:14.426854: Epoch time: 91.46 s
2024-12-04 18:20:14.427712: Yayy! New best EMA pseudo Dice: 0.7348
2024-12-04 18:20:16.021038: 
2024-12-04 18:20:16.022568: Epoch 109
2024-12-04 18:20:16.023349: Current learning rate: 0.00901
2024-12-04 18:21:47.461438: Validation loss did not improve from -0.56437. Patience: 18/50
2024-12-04 18:21:47.462552: train_loss -0.7106
2024-12-04 18:21:47.463372: val_loss -0.5013
2024-12-04 18:21:47.464360: Pseudo dice [0.7276]
2024-12-04 18:21:47.465132: Epoch time: 91.44 s
2024-12-04 18:21:49.060150: 
2024-12-04 18:21:49.061473: Epoch 110
2024-12-04 18:21:49.062209: Current learning rate: 0.009
2024-12-04 18:23:20.483675: Validation loss did not improve from -0.56437. Patience: 19/50
2024-12-04 18:23:20.484502: train_loss -0.7139
2024-12-04 18:23:20.485194: val_loss -0.5046
2024-12-04 18:23:20.485997: Pseudo dice [0.7199]
2024-12-04 18:23:20.486792: Epoch time: 91.43 s
2024-12-04 18:23:21.692864: 
2024-12-04 18:23:21.694070: Epoch 111
2024-12-04 18:23:21.695377: Current learning rate: 0.009
2024-12-04 18:24:53.256702: Validation loss did not improve from -0.56437. Patience: 20/50
2024-12-04 18:24:53.257680: train_loss -0.7095
2024-12-04 18:24:53.258559: val_loss -0.5277
2024-12-04 18:24:53.259349: Pseudo dice [0.73]
2024-12-04 18:24:53.259978: Epoch time: 91.57 s
2024-12-04 18:24:54.510055: 
2024-12-04 18:24:54.511384: Epoch 112
2024-12-04 18:24:54.512044: Current learning rate: 0.00899
2024-12-04 18:26:25.890100: Validation loss did not improve from -0.56437. Patience: 21/50
2024-12-04 18:26:25.891062: train_loss -0.714
2024-12-04 18:26:25.892099: val_loss -0.5125
2024-12-04 18:26:25.892794: Pseudo dice [0.7252]
2024-12-04 18:26:25.893680: Epoch time: 91.38 s
2024-12-04 18:26:27.105746: 
2024-12-04 18:26:27.107063: Epoch 113
2024-12-04 18:26:27.108041: Current learning rate: 0.00898
2024-12-04 18:27:58.444416: Validation loss did not improve from -0.56437. Patience: 22/50
2024-12-04 18:27:58.445218: train_loss -0.7117
2024-12-04 18:27:58.446100: val_loss -0.5302
2024-12-04 18:27:58.446773: Pseudo dice [0.7366]
2024-12-04 18:27:58.447349: Epoch time: 91.34 s
2024-12-04 18:28:00.113689: 
2024-12-04 18:28:00.115788: Epoch 114
2024-12-04 18:28:00.116551: Current learning rate: 0.00897
2024-12-04 18:29:31.708618: Validation loss did not improve from -0.56437. Patience: 23/50
2024-12-04 18:29:31.709325: train_loss -0.7139
2024-12-04 18:29:31.710016: val_loss -0.5449
2024-12-04 18:29:31.710626: Pseudo dice [0.7398]
2024-12-04 18:29:31.711318: Epoch time: 91.6 s
2024-12-04 18:29:33.476173: 
2024-12-04 18:29:33.477908: Epoch 115
2024-12-04 18:29:33.478996: Current learning rate: 0.00896
2024-12-04 18:31:05.070169: Validation loss did not improve from -0.56437. Patience: 24/50
2024-12-04 18:31:05.070933: train_loss -0.7081
2024-12-04 18:31:05.071702: val_loss -0.509
2024-12-04 18:31:05.072367: Pseudo dice [0.7296]
2024-12-04 18:31:05.073007: Epoch time: 91.6 s
2024-12-04 18:31:06.414214: 
2024-12-04 18:31:06.416005: Epoch 116
2024-12-04 18:31:06.416804: Current learning rate: 0.00895
2024-12-04 18:32:38.032037: Validation loss did not improve from -0.56437. Patience: 25/50
2024-12-04 18:32:38.033244: train_loss -0.7145
2024-12-04 18:32:38.034081: val_loss -0.4928
2024-12-04 18:32:38.034667: Pseudo dice [0.7105]
2024-12-04 18:32:38.035271: Epoch time: 91.62 s
2024-12-04 18:32:39.322130: 
2024-12-04 18:32:39.324154: Epoch 117
2024-12-04 18:32:39.324849: Current learning rate: 0.00894
2024-12-04 18:34:10.929141: Validation loss did not improve from -0.56437. Patience: 26/50
2024-12-04 18:34:10.930316: train_loss -0.7161
2024-12-04 18:34:10.931444: val_loss -0.4968
2024-12-04 18:34:10.932146: Pseudo dice [0.7295]
2024-12-04 18:34:10.932898: Epoch time: 91.61 s
2024-12-04 18:34:12.189577: 
2024-12-04 18:34:12.191474: Epoch 118
2024-12-04 18:34:12.192215: Current learning rate: 0.00893
2024-12-04 18:35:43.894852: Validation loss did not improve from -0.56437. Patience: 27/50
2024-12-04 18:35:43.895789: train_loss -0.7246
2024-12-04 18:35:43.896600: val_loss -0.5423
2024-12-04 18:35:43.897247: Pseudo dice [0.7518]
2024-12-04 18:35:43.897880: Epoch time: 91.71 s
2024-12-04 18:35:45.111474: 
2024-12-04 18:35:45.113647: Epoch 119
2024-12-04 18:35:45.114732: Current learning rate: 0.00892
2024-12-04 18:37:16.871697: Validation loss did not improve from -0.56437. Patience: 28/50
2024-12-04 18:37:16.872628: train_loss -0.7133
2024-12-04 18:37:16.873432: val_loss -0.5412
2024-12-04 18:37:16.874097: Pseudo dice [0.7383]
2024-12-04 18:37:16.874820: Epoch time: 91.76 s
2024-12-04 18:37:18.528464: 
2024-12-04 18:37:18.530262: Epoch 120
2024-12-04 18:37:18.530999: Current learning rate: 0.00891
2024-12-04 18:38:50.121992: Validation loss did not improve from -0.56437. Patience: 29/50
2024-12-04 18:38:50.123102: train_loss -0.7195
2024-12-04 18:38:50.123975: val_loss -0.5267
2024-12-04 18:38:50.124558: Pseudo dice [0.7402]
2024-12-04 18:38:50.125289: Epoch time: 91.6 s
2024-12-04 18:38:51.382064: 
2024-12-04 18:38:51.384004: Epoch 121
2024-12-04 18:38:51.385210: Current learning rate: 0.0089
2024-12-04 18:40:22.966359: Validation loss did not improve from -0.56437. Patience: 30/50
2024-12-04 18:40:22.967097: train_loss -0.7231
2024-12-04 18:40:22.967823: val_loss -0.5233
2024-12-04 18:40:22.968384: Pseudo dice [0.7357]
2024-12-04 18:40:22.969006: Epoch time: 91.59 s
2024-12-04 18:40:24.236584: 
2024-12-04 18:40:24.238619: Epoch 122
2024-12-04 18:40:24.239366: Current learning rate: 0.00889
2024-12-04 18:41:55.893903: Validation loss did not improve from -0.56437. Patience: 31/50
2024-12-04 18:41:55.894753: train_loss -0.7156
2024-12-04 18:41:55.895506: val_loss -0.4985
2024-12-04 18:41:55.896232: Pseudo dice [0.7233]
2024-12-04 18:41:55.896838: Epoch time: 91.66 s
2024-12-04 18:41:57.151157: 
2024-12-04 18:41:57.152635: Epoch 123
2024-12-04 18:41:57.153415: Current learning rate: 0.00889
2024-12-04 18:43:28.755313: Validation loss did not improve from -0.56437. Patience: 32/50
2024-12-04 18:43:28.756460: train_loss -0.7113
2024-12-04 18:43:28.757314: val_loss -0.5287
2024-12-04 18:43:28.758065: Pseudo dice [0.7385]
2024-12-04 18:43:28.758667: Epoch time: 91.61 s
2024-12-04 18:43:30.030447: 
2024-12-04 18:43:30.032145: Epoch 124
2024-12-04 18:43:30.032815: Current learning rate: 0.00888
2024-12-04 18:45:02.105182: Validation loss did not improve from -0.56437. Patience: 33/50
2024-12-04 18:45:02.106188: train_loss -0.7156
2024-12-04 18:45:02.106927: val_loss -0.5139
2024-12-04 18:45:02.107606: Pseudo dice [0.7343]
2024-12-04 18:45:02.108292: Epoch time: 92.08 s
2024-12-04 18:45:04.503211: 
2024-12-04 18:45:04.504456: Epoch 125
2024-12-04 18:45:04.505117: Current learning rate: 0.00887
2024-12-04 18:46:37.071496: Validation loss did not improve from -0.56437. Patience: 34/50
2024-12-04 18:46:37.072640: train_loss -0.7122
2024-12-04 18:46:37.073435: val_loss -0.5057
2024-12-04 18:46:37.074164: Pseudo dice [0.7241]
2024-12-04 18:46:37.074891: Epoch time: 92.57 s
2024-12-04 18:46:38.358510: 
2024-12-04 18:46:38.359831: Epoch 126
2024-12-04 18:46:38.360592: Current learning rate: 0.00886
2024-12-04 18:48:09.907693: Validation loss did not improve from -0.56437. Patience: 35/50
2024-12-04 18:48:09.908684: train_loss -0.713
2024-12-04 18:48:09.909634: val_loss -0.5109
2024-12-04 18:48:09.910260: Pseudo dice [0.7183]
2024-12-04 18:48:09.910882: Epoch time: 91.55 s
2024-12-04 18:48:11.173702: 
2024-12-04 18:48:11.174918: Epoch 127
2024-12-04 18:48:11.175535: Current learning rate: 0.00885
2024-12-04 18:49:42.685441: Validation loss did not improve from -0.56437. Patience: 36/50
2024-12-04 18:49:42.686536: train_loss -0.7211
2024-12-04 18:49:42.687413: val_loss -0.5488
2024-12-04 18:49:42.688068: Pseudo dice [0.7567]
2024-12-04 18:49:42.688743: Epoch time: 91.51 s
2024-12-04 18:49:43.928322: 
2024-12-04 18:49:43.929985: Epoch 128
2024-12-04 18:49:43.930741: Current learning rate: 0.00884
2024-12-04 18:51:15.437525: Validation loss did not improve from -0.56437. Patience: 37/50
2024-12-04 18:51:15.438267: train_loss -0.7257
2024-12-04 18:51:15.439073: val_loss -0.5444
2024-12-04 18:51:15.439795: Pseudo dice [0.7454]
2024-12-04 18:51:15.440689: Epoch time: 91.51 s
2024-12-04 18:51:15.441461: Yayy! New best EMA pseudo Dice: 0.7349
2024-12-04 18:51:17.147817: 
2024-12-04 18:51:17.149271: Epoch 129
2024-12-04 18:51:17.150745: Current learning rate: 0.00883
2024-12-04 18:52:48.707226: Validation loss did not improve from -0.56437. Patience: 38/50
2024-12-04 18:52:48.708218: train_loss -0.7188
2024-12-04 18:52:48.709036: val_loss -0.5297
2024-12-04 18:52:48.709809: Pseudo dice [0.7399]
2024-12-04 18:52:48.710479: Epoch time: 91.56 s
2024-12-04 18:52:49.081008: Yayy! New best EMA pseudo Dice: 0.7354
2024-12-04 18:52:50.707304: 
2024-12-04 18:52:50.708476: Epoch 130
2024-12-04 18:52:50.709146: Current learning rate: 0.00882
2024-12-04 18:54:22.259235: Validation loss did not improve from -0.56437. Patience: 39/50
2024-12-04 18:54:22.260118: train_loss -0.7275
2024-12-04 18:54:22.261043: val_loss -0.5218
2024-12-04 18:54:22.261930: Pseudo dice [0.7315]
2024-12-04 18:54:22.262525: Epoch time: 91.55 s
2024-12-04 18:54:23.550041: 
2024-12-04 18:54:23.551422: Epoch 131
2024-12-04 18:54:23.552276: Current learning rate: 0.00881
2024-12-04 18:55:55.177003: Validation loss improved from -0.56437 to -0.56595! Patience: 39/50
2024-12-04 18:55:55.178103: train_loss -0.7293
2024-12-04 18:55:55.178816: val_loss -0.566
2024-12-04 18:55:55.179617: Pseudo dice [0.7547]
2024-12-04 18:55:55.180216: Epoch time: 91.63 s
2024-12-04 18:55:55.180992: Yayy! New best EMA pseudo Dice: 0.737
2024-12-04 18:55:56.794666: 
2024-12-04 18:55:56.796398: Epoch 132
2024-12-04 18:55:56.797125: Current learning rate: 0.0088
2024-12-04 18:57:28.448910: Validation loss did not improve from -0.56595. Patience: 1/50
2024-12-04 18:57:28.449658: train_loss -0.7258
2024-12-04 18:57:28.450761: val_loss -0.5175
2024-12-04 18:57:28.451669: Pseudo dice [0.733]
2024-12-04 18:57:28.452683: Epoch time: 91.66 s
2024-12-04 18:57:29.695878: 
2024-12-04 18:57:29.697416: Epoch 133
2024-12-04 18:57:29.698464: Current learning rate: 0.00879
2024-12-04 18:59:01.422031: Validation loss did not improve from -0.56595. Patience: 2/50
2024-12-04 18:59:01.422939: train_loss -0.728
2024-12-04 18:59:01.423805: val_loss -0.5187
2024-12-04 18:59:01.424821: Pseudo dice [0.7377]
2024-12-04 18:59:01.425642: Epoch time: 91.73 s
2024-12-04 18:59:02.675633: 
2024-12-04 18:59:02.677081: Epoch 134
2024-12-04 18:59:02.678083: Current learning rate: 0.00879
2024-12-04 19:00:34.607185: Validation loss did not improve from -0.56595. Patience: 3/50
2024-12-04 19:00:34.608249: train_loss -0.7342
2024-12-04 19:00:34.609210: val_loss -0.5148
2024-12-04 19:00:34.610190: Pseudo dice [0.7259]
2024-12-04 19:00:34.611124: Epoch time: 91.93 s
2024-12-04 19:00:36.637207: 
2024-12-04 19:00:36.638784: Epoch 135
2024-12-04 19:00:36.639891: Current learning rate: 0.00878
2024-12-04 19:02:08.236481: Validation loss did not improve from -0.56595. Patience: 4/50
2024-12-04 19:02:08.238281: train_loss -0.7332
2024-12-04 19:02:08.240791: val_loss -0.5328
2024-12-04 19:02:08.241757: Pseudo dice [0.7361]
2024-12-04 19:02:08.242618: Epoch time: 91.6 s
2024-12-04 19:02:09.538860: 
2024-12-04 19:02:09.540491: Epoch 136
2024-12-04 19:02:09.541414: Current learning rate: 0.00877
2024-12-04 19:03:41.109373: Validation loss did not improve from -0.56595. Patience: 5/50
2024-12-04 19:03:41.110226: train_loss -0.7328
2024-12-04 19:03:41.111021: val_loss -0.5415
2024-12-04 19:03:41.111726: Pseudo dice [0.7409]
2024-12-04 19:03:41.112405: Epoch time: 91.57 s
2024-12-04 19:03:42.331100: 
2024-12-04 19:03:42.332752: Epoch 137
2024-12-04 19:03:42.333651: Current learning rate: 0.00876
2024-12-04 19:05:13.944792: Validation loss did not improve from -0.56595. Patience: 6/50
2024-12-04 19:05:13.945662: train_loss -0.7279
2024-12-04 19:05:13.946882: val_loss -0.4815
2024-12-04 19:05:13.947959: Pseudo dice [0.7109]
2024-12-04 19:05:13.949193: Epoch time: 91.62 s
2024-12-04 19:05:15.113222: 
2024-12-04 19:05:15.115412: Epoch 138
2024-12-04 19:05:15.116468: Current learning rate: 0.00875
2024-12-04 19:06:46.570812: Validation loss did not improve from -0.56595. Patience: 7/50
2024-12-04 19:06:46.572042: train_loss -0.7252
2024-12-04 19:06:46.573183: val_loss -0.5392
2024-12-04 19:06:46.574076: Pseudo dice [0.7413]
2024-12-04 19:06:46.574857: Epoch time: 91.46 s
2024-12-04 19:06:47.749414: 
2024-12-04 19:06:47.751139: Epoch 139
2024-12-04 19:06:47.752556: Current learning rate: 0.00874
2024-12-04 19:08:19.070805: Validation loss did not improve from -0.56595. Patience: 8/50
2024-12-04 19:08:19.072009: train_loss -0.7302
2024-12-04 19:08:19.073109: val_loss -0.4894
2024-12-04 19:08:19.074377: Pseudo dice [0.7224]
2024-12-04 19:08:19.075443: Epoch time: 91.32 s
2024-12-04 19:08:20.640779: 
2024-12-04 19:08:20.642611: Epoch 140
2024-12-04 19:08:20.643390: Current learning rate: 0.00873
2024-12-04 19:09:52.042799: Validation loss did not improve from -0.56595. Patience: 9/50
2024-12-04 19:09:52.044078: train_loss -0.7307
2024-12-04 19:09:52.045084: val_loss -0.5136
2024-12-04 19:09:52.045787: Pseudo dice [0.7275]
2024-12-04 19:09:52.046408: Epoch time: 91.4 s
2024-12-04 19:09:53.226329: 
2024-12-04 19:09:53.228207: Epoch 141
2024-12-04 19:09:53.229181: Current learning rate: 0.00872
2024-12-04 19:11:24.573889: Validation loss did not improve from -0.56595. Patience: 10/50
2024-12-04 19:11:24.575264: train_loss -0.7248
2024-12-04 19:11:24.576743: val_loss -0.5379
2024-12-04 19:11:24.577960: Pseudo dice [0.7378]
2024-12-04 19:11:24.578953: Epoch time: 91.35 s
2024-12-04 19:11:25.809653: 
2024-12-04 19:11:25.811342: Epoch 142
2024-12-04 19:11:25.812446: Current learning rate: 0.00871
2024-12-04 19:12:57.152329: Validation loss did not improve from -0.56595. Patience: 11/50
2024-12-04 19:12:57.153014: train_loss -0.7406
2024-12-04 19:12:57.153991: val_loss -0.5284
2024-12-04 19:12:57.155054: Pseudo dice [0.7361]
2024-12-04 19:12:57.156055: Epoch time: 91.34 s
2024-12-04 19:12:58.370229: 
2024-12-04 19:12:58.371960: Epoch 143
2024-12-04 19:12:58.373094: Current learning rate: 0.0087
2024-12-04 19:14:29.877659: Validation loss did not improve from -0.56595. Patience: 12/50
2024-12-04 19:14:29.878482: train_loss -0.7319
2024-12-04 19:14:29.879267: val_loss -0.5305
2024-12-04 19:14:29.880095: Pseudo dice [0.734]
2024-12-04 19:14:29.880807: Epoch time: 91.51 s
2024-12-04 19:14:31.086960: 
2024-12-04 19:14:31.088718: Epoch 144
2024-12-04 19:14:31.090215: Current learning rate: 0.00869
2024-12-04 19:16:02.427961: Validation loss did not improve from -0.56595. Patience: 13/50
2024-12-04 19:16:02.429149: train_loss -0.7323
2024-12-04 19:16:02.430073: val_loss -0.5518
2024-12-04 19:16:02.430948: Pseudo dice [0.7573]
2024-12-04 19:16:02.431820: Epoch time: 91.34 s
2024-12-04 19:16:04.034204: 
2024-12-04 19:16:04.036686: Epoch 145
2024-12-04 19:16:04.037864: Current learning rate: 0.00868
2024-12-04 19:17:35.432818: Validation loss did not improve from -0.56595. Patience: 14/50
2024-12-04 19:17:35.433752: train_loss -0.7384
2024-12-04 19:17:35.434662: val_loss -0.5316
2024-12-04 19:17:35.435500: Pseudo dice [0.7357]
2024-12-04 19:17:35.436262: Epoch time: 91.4 s
2024-12-04 19:17:37.090458: 
2024-12-04 19:17:37.092202: Epoch 146
2024-12-04 19:17:37.093153: Current learning rate: 0.00868
2024-12-04 19:19:08.577868: Validation loss did not improve from -0.56595. Patience: 15/50
2024-12-04 19:19:08.578874: train_loss -0.7395
2024-12-04 19:19:08.579895: val_loss -0.5254
2024-12-04 19:19:08.580568: Pseudo dice [0.7329]
2024-12-04 19:19:08.581226: Epoch time: 91.49 s
2024-12-04 19:19:09.798904: 
2024-12-04 19:19:09.800805: Epoch 147
2024-12-04 19:19:09.801853: Current learning rate: 0.00867
2024-12-04 19:20:41.402869: Validation loss did not improve from -0.56595. Patience: 16/50
2024-12-04 19:20:41.403953: train_loss -0.7345
2024-12-04 19:20:41.405187: val_loss -0.5431
2024-12-04 19:20:41.406458: Pseudo dice [0.7461]
2024-12-04 19:20:41.407638: Epoch time: 91.61 s
2024-12-04 19:20:42.602457: 
2024-12-04 19:20:42.604561: Epoch 148
2024-12-04 19:20:42.605603: Current learning rate: 0.00866
2024-12-04 19:22:14.061277: Validation loss did not improve from -0.56595. Patience: 17/50
2024-12-04 19:22:14.062407: train_loss -0.7425
2024-12-04 19:22:14.063568: val_loss -0.5203
2024-12-04 19:22:14.064473: Pseudo dice [0.7314]
2024-12-04 19:22:14.065662: Epoch time: 91.46 s
2024-12-04 19:22:15.302323: 
2024-12-04 19:22:15.304506: Epoch 149
2024-12-04 19:22:15.305546: Current learning rate: 0.00865
2024-12-04 19:23:46.739469: Validation loss improved from -0.56595 to -0.58338! Patience: 17/50
2024-12-04 19:23:46.740288: train_loss -0.7466
2024-12-04 19:23:46.741269: val_loss -0.5834
2024-12-04 19:23:46.742017: Pseudo dice [0.7654]
2024-12-04 19:23:46.742606: Epoch time: 91.44 s
2024-12-04 19:23:47.131460: Yayy! New best EMA pseudo Dice: 0.739
2024-12-04 19:23:48.671736: 
2024-12-04 19:23:48.673365: Epoch 150
2024-12-04 19:23:48.674468: Current learning rate: 0.00864
2024-12-04 19:25:20.129487: Validation loss did not improve from -0.58338. Patience: 1/50
2024-12-04 19:25:20.130697: train_loss -0.7451
2024-12-04 19:25:20.131649: val_loss -0.5382
2024-12-04 19:25:20.132657: Pseudo dice [0.7371]
2024-12-04 19:25:20.133711: Epoch time: 91.46 s
2024-12-04 19:25:21.364040: 
2024-12-04 19:25:21.365959: Epoch 151
2024-12-04 19:25:21.367145: Current learning rate: 0.00863
2024-12-04 19:26:52.978281: Validation loss did not improve from -0.58338. Patience: 2/50
2024-12-04 19:26:52.979064: train_loss -0.7467
2024-12-04 19:26:52.979815: val_loss -0.5364
2024-12-04 19:26:52.980502: Pseudo dice [0.7375]
2024-12-04 19:26:52.981516: Epoch time: 91.62 s
2024-12-04 19:26:54.199032: 
2024-12-04 19:26:54.201028: Epoch 152
2024-12-04 19:26:54.201923: Current learning rate: 0.00862
2024-12-04 19:28:25.645293: Validation loss did not improve from -0.58338. Patience: 3/50
2024-12-04 19:28:25.646239: train_loss -0.7476
2024-12-04 19:28:25.647641: val_loss -0.5342
2024-12-04 19:28:25.648588: Pseudo dice [0.7438]
2024-12-04 19:28:25.649953: Epoch time: 91.45 s
2024-12-04 19:28:25.650938: Yayy! New best EMA pseudo Dice: 0.7392
2024-12-04 19:28:27.246057: 
2024-12-04 19:28:27.248045: Epoch 153
2024-12-04 19:28:27.249331: Current learning rate: 0.00861
2024-12-04 19:29:58.782010: Validation loss did not improve from -0.58338. Patience: 4/50
2024-12-04 19:29:58.783048: train_loss -0.7467
2024-12-04 19:29:58.783882: val_loss -0.5212
2024-12-04 19:29:58.784515: Pseudo dice [0.7385]
2024-12-04 19:29:58.785315: Epoch time: 91.54 s
2024-12-04 19:30:00.012363: 
2024-12-04 19:30:00.013802: Epoch 154
2024-12-04 19:30:00.014583: Current learning rate: 0.0086
2024-12-04 19:31:31.478634: Validation loss did not improve from -0.58338. Patience: 5/50
2024-12-04 19:31:31.479559: train_loss -0.7469
2024-12-04 19:31:31.480516: val_loss -0.5663
2024-12-04 19:31:31.481272: Pseudo dice [0.7578]
2024-12-04 19:31:31.482084: Epoch time: 91.47 s
2024-12-04 19:31:31.874585: Yayy! New best EMA pseudo Dice: 0.741
2024-12-04 19:31:33.462292: 
2024-12-04 19:31:33.464254: Epoch 155
2024-12-04 19:31:33.465412: Current learning rate: 0.00859
2024-12-04 19:33:04.874479: Validation loss did not improve from -0.58338. Patience: 6/50
2024-12-04 19:33:04.875490: train_loss -0.7348
2024-12-04 19:33:04.876165: val_loss -0.514
2024-12-04 19:33:04.877020: Pseudo dice [0.7297]
2024-12-04 19:33:04.877924: Epoch time: 91.41 s
2024-12-04 19:33:06.435805: 
2024-12-04 19:33:06.437269: Epoch 156
2024-12-04 19:33:06.438159: Current learning rate: 0.00858
2024-12-04 19:34:37.825568: Validation loss did not improve from -0.58338. Patience: 7/50
2024-12-04 19:34:37.827267: train_loss -0.7405
2024-12-04 19:34:37.828835: val_loss -0.4812
2024-12-04 19:34:37.829829: Pseudo dice [0.7137]
2024-12-04 19:34:37.830661: Epoch time: 91.39 s
2024-12-04 19:34:39.063922: 
2024-12-04 19:34:39.065380: Epoch 157
2024-12-04 19:34:39.066779: Current learning rate: 0.00858
2024-12-04 19:36:10.482034: Validation loss did not improve from -0.58338. Patience: 8/50
2024-12-04 19:36:10.483231: train_loss -0.7408
2024-12-04 19:36:10.484344: val_loss -0.5423
2024-12-04 19:36:10.485160: Pseudo dice [0.7402]
2024-12-04 19:36:10.485930: Epoch time: 91.42 s
2024-12-04 19:36:11.727850: 
2024-12-04 19:36:11.729512: Epoch 158
2024-12-04 19:36:11.730610: Current learning rate: 0.00857
2024-12-04 19:37:43.208469: Validation loss did not improve from -0.58338. Patience: 9/50
2024-12-04 19:37:43.209377: train_loss -0.7426
2024-12-04 19:37:43.210704: val_loss -0.5508
2024-12-04 19:37:43.211432: Pseudo dice [0.7511]
2024-12-04 19:37:43.212323: Epoch time: 91.48 s
2024-12-04 19:37:44.442832: 
2024-12-04 19:37:44.445122: Epoch 159
2024-12-04 19:37:44.445908: Current learning rate: 0.00856
2024-12-04 19:39:15.915725: Validation loss did not improve from -0.58338. Patience: 10/50
2024-12-04 19:39:15.916734: train_loss -0.7412
2024-12-04 19:39:15.918400: val_loss -0.5389
2024-12-04 19:39:15.919338: Pseudo dice [0.7337]
2024-12-04 19:39:15.920247: Epoch time: 91.48 s
2024-12-04 19:39:17.500304: 
2024-12-04 19:39:17.501797: Epoch 160
2024-12-04 19:39:17.502779: Current learning rate: 0.00855
2024-12-04 19:40:48.925107: Validation loss did not improve from -0.58338. Patience: 11/50
2024-12-04 19:40:48.925916: train_loss -0.7461
2024-12-04 19:40:48.927000: val_loss -0.5279
2024-12-04 19:40:48.927813: Pseudo dice [0.7385]
2024-12-04 19:40:48.928629: Epoch time: 91.43 s
2024-12-04 19:40:50.138696: 
2024-12-04 19:40:50.140255: Epoch 161
2024-12-04 19:40:50.140906: Current learning rate: 0.00854
2024-12-04 19:42:21.525254: Validation loss did not improve from -0.58338. Patience: 12/50
2024-12-04 19:42:21.526245: train_loss -0.753
2024-12-04 19:42:21.527278: val_loss -0.5219
2024-12-04 19:42:21.528033: Pseudo dice [0.7285]
2024-12-04 19:42:21.528814: Epoch time: 91.39 s
2024-12-04 19:42:22.774615: 
2024-12-04 19:42:22.776399: Epoch 162
2024-12-04 19:42:22.777680: Current learning rate: 0.00853
2024-12-04 19:43:54.416703: Validation loss did not improve from -0.58338. Patience: 13/50
2024-12-04 19:43:54.417728: train_loss -0.7471
2024-12-04 19:43:54.418474: val_loss -0.536
2024-12-04 19:43:54.419241: Pseudo dice [0.7395]
2024-12-04 19:43:54.420056: Epoch time: 91.64 s
2024-12-04 19:43:55.641802: 
2024-12-04 19:43:55.644239: Epoch 163
2024-12-04 19:43:55.645043: Current learning rate: 0.00852
2024-12-04 19:45:27.193526: Validation loss did not improve from -0.58338. Patience: 14/50
2024-12-04 19:45:27.194490: train_loss -0.7304
2024-12-04 19:45:27.195179: val_loss -0.5381
2024-12-04 19:45:27.195987: Pseudo dice [0.7395]
2024-12-04 19:45:27.196587: Epoch time: 91.55 s
2024-12-04 19:45:28.423429: 
2024-12-04 19:45:28.425443: Epoch 164
2024-12-04 19:45:28.426570: Current learning rate: 0.00851
2024-12-04 19:47:00.002228: Validation loss did not improve from -0.58338. Patience: 15/50
2024-12-04 19:47:00.003143: train_loss -0.7359
2024-12-04 19:47:00.003975: val_loss -0.5221
2024-12-04 19:47:00.004800: Pseudo dice [0.7318]
2024-12-04 19:47:00.005540: Epoch time: 91.58 s
2024-12-04 19:47:01.590075: 
2024-12-04 19:47:01.591708: Epoch 165
2024-12-04 19:47:01.592865: Current learning rate: 0.0085
2024-12-04 19:48:33.226546: Validation loss did not improve from -0.58338. Patience: 16/50
2024-12-04 19:48:33.227537: train_loss -0.7459
2024-12-04 19:48:33.228879: val_loss -0.515
2024-12-04 19:48:33.229957: Pseudo dice [0.7298]
2024-12-04 19:48:33.230954: Epoch time: 91.64 s
2024-12-04 19:48:34.463321: 
2024-12-04 19:48:34.465475: Epoch 166
2024-12-04 19:48:34.466717: Current learning rate: 0.00849
2024-12-04 19:50:06.156410: Validation loss did not improve from -0.58338. Patience: 17/50
2024-12-04 19:50:06.157534: train_loss -0.7396
2024-12-04 19:50:06.158515: val_loss -0.5185
2024-12-04 19:50:06.159441: Pseudo dice [0.7279]
2024-12-04 19:50:06.160412: Epoch time: 91.7 s
2024-12-04 19:50:07.688591: 
2024-12-04 19:50:07.690413: Epoch 167
2024-12-04 19:50:07.691417: Current learning rate: 0.00848
2024-12-04 19:51:39.242770: Validation loss did not improve from -0.58338. Patience: 18/50
2024-12-04 19:51:39.243867: train_loss -0.7487
2024-12-04 19:51:39.244892: val_loss -0.5279
2024-12-04 19:51:39.245607: Pseudo dice [0.7387]
2024-12-04 19:51:39.246267: Epoch time: 91.56 s
2024-12-04 19:51:40.479102: 
2024-12-04 19:51:40.481290: Epoch 168
2024-12-04 19:51:40.482548: Current learning rate: 0.00847
2024-12-04 19:53:15.619195: Validation loss did not improve from -0.58338. Patience: 19/50
2024-12-04 19:53:15.620665: train_loss -0.7493
2024-12-04 19:53:15.621866: val_loss -0.5241
2024-12-04 19:53:15.622901: Pseudo dice [0.7321]
2024-12-04 19:53:15.623761: Epoch time: 95.14 s
2024-12-04 19:53:16.890439: 
2024-12-04 19:53:16.892309: Epoch 169
2024-12-04 19:53:16.893528: Current learning rate: 0.00847
2024-12-04 19:54:48.231271: Validation loss did not improve from -0.58338. Patience: 20/50
2024-12-04 19:54:48.232131: train_loss -0.7452
2024-12-04 19:54:48.233301: val_loss -0.5179
2024-12-04 19:54:48.234235: Pseudo dice [0.7251]
2024-12-04 19:54:48.235169: Epoch time: 91.34 s
2024-12-04 19:54:49.855451: 
2024-12-04 19:54:49.857491: Epoch 170
2024-12-04 19:54:49.858767: Current learning rate: 0.00846
2024-12-04 19:56:21.250639: Validation loss did not improve from -0.58338. Patience: 21/50
2024-12-04 19:56:21.252204: train_loss -0.7461
2024-12-04 19:56:21.253582: val_loss -0.5421
2024-12-04 19:56:21.254704: Pseudo dice [0.746]
2024-12-04 19:56:21.255481: Epoch time: 91.4 s
2024-12-04 19:56:22.489310: 
2024-12-04 19:56:22.491703: Epoch 171
2024-12-04 19:56:22.492804: Current learning rate: 0.00845
2024-12-04 19:57:53.875817: Validation loss did not improve from -0.58338. Patience: 22/50
2024-12-04 19:57:53.876963: train_loss -0.7461
2024-12-04 19:57:53.878235: val_loss -0.5381
2024-12-04 19:57:53.879339: Pseudo dice [0.7377]
2024-12-04 19:57:53.880538: Epoch time: 91.39 s
2024-12-04 19:57:55.109482: 
2024-12-04 19:57:55.111565: Epoch 172
2024-12-04 19:57:55.112824: Current learning rate: 0.00844
2024-12-04 19:59:26.567426: Validation loss did not improve from -0.58338. Patience: 23/50
2024-12-04 19:59:26.568499: train_loss -0.7493
2024-12-04 19:59:26.569861: val_loss -0.5437
2024-12-04 19:59:26.570797: Pseudo dice [0.7526]
2024-12-04 19:59:26.571540: Epoch time: 91.46 s
2024-12-04 19:59:27.780218: 
2024-12-04 19:59:27.781986: Epoch 173
2024-12-04 19:59:27.783023: Current learning rate: 0.00843
2024-12-04 20:00:59.181908: Validation loss did not improve from -0.58338. Patience: 24/50
2024-12-04 20:00:59.183319: train_loss -0.7487
2024-12-04 20:00:59.184213: val_loss -0.5055
2024-12-04 20:00:59.185031: Pseudo dice [0.7271]
2024-12-04 20:00:59.186057: Epoch time: 91.4 s
2024-12-04 20:01:00.412840: 
2024-12-04 20:01:00.415015: Epoch 174
2024-12-04 20:01:00.415818: Current learning rate: 0.00842
2024-12-04 20:02:31.864395: Validation loss did not improve from -0.58338. Patience: 25/50
2024-12-04 20:02:31.865542: train_loss -0.7522
2024-12-04 20:02:31.866856: val_loss -0.5056
2024-12-04 20:02:31.867785: Pseudo dice [0.7331]
2024-12-04 20:02:31.868813: Epoch time: 91.45 s
2024-12-04 20:02:33.423230: 
2024-12-04 20:02:33.425367: Epoch 175
2024-12-04 20:02:33.426502: Current learning rate: 0.00841
2024-12-04 20:04:05.121497: Validation loss did not improve from -0.58338. Patience: 26/50
2024-12-04 20:04:05.122516: train_loss -0.7531
2024-12-04 20:04:05.123515: val_loss -0.5329
2024-12-04 20:04:05.124186: Pseudo dice [0.7456]
2024-12-04 20:04:05.124894: Epoch time: 91.7 s
2024-12-04 20:04:06.314248: 
2024-12-04 20:04:06.316012: Epoch 176
2024-12-04 20:04:06.317291: Current learning rate: 0.0084
2024-12-04 20:05:38.019106: Validation loss did not improve from -0.58338. Patience: 27/50
2024-12-04 20:05:38.019961: train_loss -0.7529
2024-12-04 20:05:38.022177: val_loss -0.5649
2024-12-04 20:05:38.023059: Pseudo dice [0.7598]
2024-12-04 20:05:38.024193: Epoch time: 91.71 s
2024-12-04 20:05:39.803498: 
2024-12-04 20:05:39.805718: Epoch 177
2024-12-04 20:05:39.806474: Current learning rate: 0.00839
2024-12-04 20:07:11.381929: Validation loss did not improve from -0.58338. Patience: 28/50
2024-12-04 20:07:11.383216: train_loss -0.7585
2024-12-04 20:07:11.384069: val_loss -0.4787
2024-12-04 20:07:11.384923: Pseudo dice [0.717]
2024-12-04 20:07:11.385650: Epoch time: 91.58 s
2024-12-04 20:07:12.594994: 
2024-12-04 20:07:12.596698: Epoch 178
2024-12-04 20:07:12.597849: Current learning rate: 0.00838
2024-12-04 20:08:44.081239: Validation loss did not improve from -0.58338. Patience: 29/50
2024-12-04 20:08:44.082025: train_loss -0.7601
2024-12-04 20:08:44.082963: val_loss -0.5144
2024-12-04 20:08:44.083985: Pseudo dice [0.7379]
2024-12-04 20:08:44.084927: Epoch time: 91.49 s
2024-12-04 20:08:45.321922: 
2024-12-04 20:08:45.323930: Epoch 179
2024-12-04 20:08:45.325136: Current learning rate: 0.00837
2024-12-04 20:10:18.485214: Validation loss did not improve from -0.58338. Patience: 30/50
2024-12-04 20:10:18.523066: train_loss -0.7566
2024-12-04 20:10:18.525325: val_loss -0.5309
2024-12-04 20:10:18.526266: Pseudo dice [0.7433]
2024-12-04 20:10:18.527474: Epoch time: 93.18 s
2024-12-04 20:10:20.928787: 
2024-12-04 20:10:20.931161: Epoch 180
2024-12-04 20:10:20.932356: Current learning rate: 0.00836
2024-12-04 20:11:52.410514: Validation loss did not improve from -0.58338. Patience: 31/50
2024-12-04 20:11:52.411369: train_loss -0.7645
2024-12-04 20:11:52.412565: val_loss -0.5124
2024-12-04 20:11:52.413522: Pseudo dice [0.7219]
2024-12-04 20:11:52.414649: Epoch time: 91.48 s
2024-12-04 20:11:53.620433: 
2024-12-04 20:11:53.622455: Epoch 181
2024-12-04 20:11:53.623763: Current learning rate: 0.00836
2024-12-04 20:13:25.024531: Validation loss did not improve from -0.58338. Patience: 32/50
2024-12-04 20:13:25.025694: train_loss -0.7597
2024-12-04 20:13:25.026704: val_loss -0.518
2024-12-04 20:13:25.027587: Pseudo dice [0.735]
2024-12-04 20:13:25.028325: Epoch time: 91.41 s
2024-12-04 20:13:26.274275: 
2024-12-04 20:13:26.276367: Epoch 182
2024-12-04 20:13:26.277428: Current learning rate: 0.00835
2024-12-04 20:14:57.659077: Validation loss did not improve from -0.58338. Patience: 33/50
2024-12-04 20:14:57.660083: train_loss -0.7598
2024-12-04 20:14:57.661328: val_loss -0.536
2024-12-04 20:14:57.662444: Pseudo dice [0.7444]
2024-12-04 20:14:57.663312: Epoch time: 91.39 s
2024-12-04 20:14:58.869321: 
2024-12-04 20:14:58.870991: Epoch 183
2024-12-04 20:14:58.872078: Current learning rate: 0.00834
2024-12-04 20:16:30.465022: Validation loss did not improve from -0.58338. Patience: 34/50
2024-12-04 20:16:30.466576: train_loss -0.7496
2024-12-04 20:16:30.467544: val_loss -0.5311
2024-12-04 20:16:30.468474: Pseudo dice [0.7406]
2024-12-04 20:16:30.469269: Epoch time: 91.6 s
2024-12-04 20:16:31.680785: 
2024-12-04 20:16:31.682531: Epoch 184
2024-12-04 20:16:31.683813: Current learning rate: 0.00833
2024-12-04 20:18:03.311200: Validation loss did not improve from -0.58338. Patience: 35/50
2024-12-04 20:18:03.312385: train_loss -0.7552
2024-12-04 20:18:03.313784: val_loss -0.5382
2024-12-04 20:18:03.314865: Pseudo dice [0.7431]
2024-12-04 20:18:03.315887: Epoch time: 91.63 s
2024-12-04 20:18:04.856484: 
2024-12-04 20:18:04.858398: Epoch 185
2024-12-04 20:18:04.859675: Current learning rate: 0.00832
2024-12-04 20:19:36.440623: Validation loss did not improve from -0.58338. Patience: 36/50
2024-12-04 20:19:36.442029: train_loss -0.758
2024-12-04 20:19:36.443383: val_loss -0.516
2024-12-04 20:19:36.444529: Pseudo dice [0.7373]
2024-12-04 20:19:36.445550: Epoch time: 91.59 s
2024-12-04 20:19:37.631500: 
2024-12-04 20:19:37.633236: Epoch 186
2024-12-04 20:19:37.634371: Current learning rate: 0.00831
2024-12-04 20:21:09.409272: Validation loss did not improve from -0.58338. Patience: 37/50
2024-12-04 20:21:09.410237: train_loss -0.757
2024-12-04 20:21:09.411346: val_loss -0.513
2024-12-04 20:21:09.412426: Pseudo dice [0.7314]
2024-12-04 20:21:09.413711: Epoch time: 91.78 s
2024-12-04 20:21:10.984354: 
2024-12-04 20:21:10.986040: Epoch 187
2024-12-04 20:21:10.987247: Current learning rate: 0.0083
2024-12-04 20:22:42.591713: Validation loss did not improve from -0.58338. Patience: 38/50
2024-12-04 20:22:42.593326: train_loss -0.7622
2024-12-04 20:22:42.594471: val_loss -0.5268
2024-12-04 20:22:42.595432: Pseudo dice [0.7348]
2024-12-04 20:22:42.596454: Epoch time: 91.61 s
2024-12-04 20:22:43.808213: 
2024-12-04 20:22:43.809849: Epoch 188
2024-12-04 20:22:43.810772: Current learning rate: 0.00829
2024-12-04 20:24:15.604049: Validation loss did not improve from -0.58338. Patience: 39/50
2024-12-04 20:24:15.605227: train_loss -0.7607
2024-12-04 20:24:15.606433: val_loss -0.5381
2024-12-04 20:24:15.607576: Pseudo dice [0.7448]
2024-12-04 20:24:15.608457: Epoch time: 91.8 s
2024-12-04 20:24:16.773473: 
2024-12-04 20:24:16.775708: Epoch 189
2024-12-04 20:24:16.776684: Current learning rate: 0.00828
2024-12-04 20:25:48.295520: Validation loss did not improve from -0.58338. Patience: 40/50
2024-12-04 20:25:48.296608: train_loss -0.7428
2024-12-04 20:25:48.297395: val_loss -0.5001
2024-12-04 20:25:48.298079: Pseudo dice [0.7226]
2024-12-04 20:25:48.298687: Epoch time: 91.52 s
2024-12-04 20:25:49.868181: 
2024-12-04 20:25:49.870031: Epoch 190
2024-12-04 20:25:49.871187: Current learning rate: 0.00827
2024-12-04 20:27:21.268811: Validation loss did not improve from -0.58338. Patience: 41/50
2024-12-04 20:27:21.269576: train_loss -0.7513
2024-12-04 20:27:21.270336: val_loss -0.5247
2024-12-04 20:27:21.270955: Pseudo dice [0.737]
2024-12-04 20:27:21.271806: Epoch time: 91.4 s
2024-12-04 20:27:22.508223: 
2024-12-04 20:27:22.510131: Epoch 191
2024-12-04 20:27:22.511088: Current learning rate: 0.00826
2024-12-04 20:28:53.806872: Validation loss did not improve from -0.58338. Patience: 42/50
2024-12-04 20:28:53.808057: train_loss -0.7559
2024-12-04 20:28:53.809597: val_loss -0.5128
2024-12-04 20:28:53.810849: Pseudo dice [0.7237]
2024-12-04 20:28:53.812152: Epoch time: 91.3 s
2024-12-04 20:28:55.031163: 
2024-12-04 20:28:55.032900: Epoch 192
2024-12-04 20:28:55.034166: Current learning rate: 0.00825
2024-12-04 20:30:26.366487: Validation loss did not improve from -0.58338. Patience: 43/50
2024-12-04 20:30:26.367204: train_loss -0.7511
2024-12-04 20:30:26.368392: val_loss -0.5486
2024-12-04 20:30:26.369253: Pseudo dice [0.7479]
2024-12-04 20:30:26.369928: Epoch time: 91.34 s
2024-12-04 20:30:27.595757: 
2024-12-04 20:30:27.597477: Epoch 193
2024-12-04 20:30:27.598589: Current learning rate: 0.00824
2024-12-04 20:31:59.000602: Validation loss did not improve from -0.58338. Patience: 44/50
2024-12-04 20:31:59.001434: train_loss -0.7553
2024-12-04 20:31:59.002604: val_loss -0.5367
2024-12-04 20:31:59.003748: Pseudo dice [0.7457]
2024-12-04 20:31:59.004869: Epoch time: 91.41 s
2024-12-04 20:32:00.188755: 
2024-12-04 20:32:00.190769: Epoch 194
2024-12-04 20:32:00.192089: Current learning rate: 0.00824
2024-12-04 20:33:31.411215: Validation loss did not improve from -0.58338. Patience: 45/50
2024-12-04 20:33:31.412393: train_loss -0.76
2024-12-04 20:33:31.413405: val_loss -0.5392
2024-12-04 20:33:31.414456: Pseudo dice [0.7503]
2024-12-04 20:33:31.415408: Epoch time: 91.22 s
2024-12-04 20:33:33.019938: 
2024-12-04 20:33:33.022321: Epoch 195
2024-12-04 20:33:33.023655: Current learning rate: 0.00823
2024-12-04 20:35:04.283565: Validation loss did not improve from -0.58338. Patience: 46/50
2024-12-04 20:35:04.284318: train_loss -0.7616
2024-12-04 20:35:04.285197: val_loss -0.504
2024-12-04 20:35:04.285837: Pseudo dice [0.7264]
2024-12-04 20:35:04.286487: Epoch time: 91.27 s
2024-12-04 20:35:05.513965: 
2024-12-04 20:35:05.515769: Epoch 196
2024-12-04 20:35:05.516787: Current learning rate: 0.00822
2024-12-04 20:36:36.934157: Validation loss did not improve from -0.58338. Patience: 47/50
2024-12-04 20:36:36.935956: train_loss -0.7591
2024-12-04 20:36:36.937182: val_loss -0.5571
2024-12-04 20:36:36.938266: Pseudo dice [0.7504]
2024-12-04 20:36:36.939139: Epoch time: 91.42 s
2024-12-04 20:36:38.206788: 
2024-12-04 20:36:38.298295: Epoch 197
2024-12-04 20:36:38.299757: Current learning rate: 0.00821
2024-12-04 20:38:09.686499: Validation loss did not improve from -0.58338. Patience: 48/50
2024-12-04 20:38:09.687586: train_loss -0.7635
2024-12-04 20:38:09.688887: val_loss -0.5126
2024-12-04 20:38:09.689732: Pseudo dice [0.7361]
2024-12-04 20:38:09.690436: Epoch time: 91.48 s
2024-12-04 20:38:11.279181: 
2024-12-04 20:38:11.281297: Epoch 198
2024-12-04 20:38:11.282454: Current learning rate: 0.0082
2024-12-04 20:39:42.690308: Validation loss did not improve from -0.58338. Patience: 49/50
2024-12-04 20:39:42.691235: train_loss -0.7592
2024-12-04 20:39:42.692195: val_loss -0.5349
2024-12-04 20:39:42.693290: Pseudo dice [0.7357]
2024-12-04 20:39:42.694392: Epoch time: 91.41 s
2024-12-04 20:39:43.929862: 
2024-12-04 20:39:43.932047: Epoch 199
2024-12-04 20:39:43.933435: Current learning rate: 0.00819
2024-12-04 20:41:15.220174: Validation loss did not improve from -0.58338. Patience: 50/50
2024-12-04 20:41:15.221514: train_loss -0.7464
2024-12-04 20:41:15.222863: val_loss -0.5062
2024-12-04 20:41:15.223874: Pseudo dice [0.7318]
2024-12-04 20:41:15.224673: Epoch time: 91.29 s
2024-12-04 20:41:16.873337: Patience reached. Stopping training.
2024-12-04 20:41:17.264795: Training done.
2024-12-04 20:41:17.437861: Using splits from existing split file: /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset307_Sohee_Calcium_OCT_CrossValidation/splits_final.json
2024-12-04 20:41:17.439636: The split file contains 5 splits.
2024-12-04 20:41:17.440607: Desired fold for training: 1
2024-12-04 20:41:17.441352: This split has 6 training and 2 validation cases.
2024-12-04 20:41:17.442209: predicting 101-019
2024-12-04 20:41:17.453087: 101-019, shape torch.Size([1, 375, 498, 498]), rank 0
2024-12-04 20:42:58.094254: predicting 401-004
2024-12-04 20:42:58.121303: 401-004, shape torch.Size([1, 375, 498, 498]), rank 0
2024-12-04 20:44:45.144934: Validation complete
2024-12-04 20:44:45.146026: Mean Validation Dice:  0.742783909665642

############################
INFO: You are using the old nnU-Net default plans. We have updated our recommendations. Please consider using those instead! Read more here: https://github.com/MIC-DKFZ/nnUNet/blob/master/documentation/resenc_presets.md
############################

Using device: cuda:0

#######################################################################
Please cite the following paper when using nnU-Net:
Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.
#######################################################################

2024-12-04 20:44:53.199880: do_dummy_2d_data_aug: True
2024-12-04 20:44:53.201829: Using splits from existing split file: /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset307_Sohee_Calcium_OCT_CrossValidation/splits_final.json
2024-12-04 20:44:53.202792: The split file contains 5 splits.
2024-12-04 20:44:53.203427: Desired fold for training: 3
2024-12-04 20:44:53.204270: This split has 7 training and 1 validation cases.

############################
INFO: You are using the old nnU-Net default plans. We have updated our recommendations. Please consider using those instead! Read more here: https://github.com/MIC-DKFZ/nnUNet/blob/master/documentation/resenc_presets.md
############################

Using device: cuda:0

#######################################################################
Please cite the following paper when using nnU-Net:
Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.
#######################################################################

2024-12-04 20:44:53.203589: do_dummy_2d_data_aug: True
2024-12-04 20:44:53.204963: Using splits from existing split file: /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset307_Sohee_Calcium_OCT_CrossValidation/splits_final.json
2024-12-04 20:44:53.206197: The split file contains 5 splits.
2024-12-04 20:44:53.206955: Desired fold for training: 2
2024-12-04 20:44:53.207679: This split has 6 training and 2 validation cases.
using pin_memory on device 0
using pin_memory on device 0
using pin_memory on device 0
2024-12-04 20:44:56.153346: Using torch.compile...
using pin_memory on device 0
2024-12-04 20:44:58.405528: Using torch.compile...
/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "

This is the configuration used by this training:
Configuration name: 3d_32x160x128_b10
 {'data_identifier': 'nnUNetPlans_3d_fullres', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 10, 'patch_size': [32, 160, 128], 'median_image_size_in_voxels': [375.0, 498.0, 498.0], 'spacing': [1.0, 1.0, 1.0], 'normalization_schemes': ['ZScoreNormalization'], 'use_mask_for_norm': [False], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'architecture': {'network_class_name': 'dynamic_network_architectures.architectures.unet.PlainConvUNet', 'arch_kwargs': {'n_stages': 6, 'features_per_stage': [32, 64, 128, 256, 320, 320], 'conv_op': 'torch.nn.modules.conv.Conv3d', 'kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]], 'strides': [[1, 1, 1], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [1, 2, 2]], 'n_conv_per_stage': [2, 2, 2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2, 2, 2], 'conv_bias': True, 'norm_op': 'torch.nn.modules.instancenorm.InstanceNorm3d', 'norm_op_kwargs': {'eps': 1e-05, 'affine': True}, 'dropout_op': None, 'dropout_op_kwargs': None, 'nonlin': 'torch.nn.LeakyReLU', 'nonlin_kwargs': {'inplace': True}, 'deep_supervision': True}, '_kw_requires_import': ['conv_op', 'norm_op', 'dropout_op', 'nonlin']}, 'batch_dice': True, 'inherits_from': '3d_fullres'} 

These are the global plan.json settings:
 {'dataset_name': 'Dataset307_Sohee_Calcium_OCT_CrossValidation', 'plans_name': 'nnUNetPlans', 'original_median_spacing_after_transp': [1.0, 1.0, 1.0], 'original_median_shape_after_transp': [375, 498, 498], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 0.4977976381778717, 'mean': 0.13507525622844696, 'median': 0.09599608182907104, 'min': 0.0, 'percentile_00_5': 0.014754901640117168, 'percentile_99_5': 0.4977976381778717, 'std': 0.12025152146816254}}} 

2024-12-04 20:45:10.017098: unpacking dataset...

This is the configuration used by this training:
Configuration name: 3d_32x160x128_b10
 {'data_identifier': 'nnUNetPlans_3d_fullres', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 10, 'patch_size': [32, 160, 128], 'median_image_size_in_voxels': [375.0, 498.0, 498.0], 'spacing': [1.0, 1.0, 1.0], 'normalization_schemes': ['ZScoreNormalization'], 'use_mask_for_norm': [False], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'architecture': {'network_class_name': 'dynamic_network_architectures.architectures.unet.PlainConvUNet', 'arch_kwargs': {'n_stages': 6, 'features_per_stage': [32, 64, 128, 256, 320, 320], 'conv_op': 'torch.nn.modules.conv.Conv3d', 'kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]], 'strides': [[1, 1, 1], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [1, 2, 2]], 'n_conv_per_stage': [2, 2, 2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2, 2, 2], 'conv_bias': True, 'norm_op': 'torch.nn.modules.instancenorm.InstanceNorm3d', 'norm_op_kwargs': {'eps': 1e-05, 'affine': True}, 'dropout_op': None, 'dropout_op_kwargs': None, 'nonlin': 'torch.nn.LeakyReLU', 'nonlin_kwargs': {'inplace': True}, 'deep_supervision': True}, '_kw_requires_import': ['conv_op', 'norm_op', 'dropout_op', 'nonlin']}, 'batch_dice': True, 'inherits_from': '3d_fullres'} 

These are the global plan.json settings:
 {'dataset_name': 'Dataset307_Sohee_Calcium_OCT_CrossValidation', 'plans_name': 'nnUNetPlans', 'original_median_spacing_after_transp': [1.0, 1.0, 1.0], 'original_median_shape_after_transp': [375, 498, 498], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 0.4977976381778717, 'mean': 0.13507525622844696, 'median': 0.09599608182907104, 'min': 0.0, 'percentile_00_5': 0.014754901640117168, 'percentile_99_5': 0.4977976381778717, 'std': 0.12025152146816254}}} 

2024-12-04 20:45:10.017280: unpacking dataset...
2024-12-04 20:45:14.174294: unpacking done...
2024-12-04 20:45:14.268269: Unable to plot network architecture: nnUNet_compile is enabled!
2024-12-04 20:45:14.484880: 
2024-12-04 20:45:14.486599: Epoch 0
2024-12-04 20:45:14.487446: Current learning rate: 0.01
2024-12-04 20:45:14.110031: unpacking done...
2024-12-04 20:45:14.268509: Unable to plot network architecture: nnUNet_compile is enabled!
2024-12-04 20:45:14.484877: 
2024-12-04 20:45:14.486512: Epoch 0
2024-12-04 20:45:14.487503: Current learning rate: 0.01
concurrent.futures.process._RemoteTraceback: 
"""
Traceback (most recent call last):
  File "/home/gridsan/nchutisilp/.conda/envs/nnunet/lib/python3.9/concurrent/futures/process.py", line 246, in _process_worker
    r = call_item.fn(*call_item.args, **call_item.kwargs)
  File "/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/_inductor/codecache.py", line 2492, in _worker_compile
    kernel = TritonCodeCache.load(kernel_name, source_code)
  File "/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/_inductor/codecache.py", line 2205, in load
    mod = PyCodeCache.load(source_code)
  File "/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/_inductor/codecache.py", line 2137, in load
    return cls.load_by_key_path(key, path, linemap, attrs)
  File "/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/_inductor/codecache.py", line 2160, in load_by_key_path
    exec(code, mod.__dict__, mod.__dict__)
  File "/state/partition1/slurm_tmp/27547865.4294967291.0/torchinductor_nchutisilp/sc/csc4wqp3e5k7hoocadgqqzub6rcynx3zdppt56kjlfkaxank3csb.py", line 12, in <module>
    @triton_heuristics.pointwise(
  File "/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/_inductor/triton_heuristics.py", line 1188, in pointwise
    return cached_autotune(
  File "/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/_inductor/triton_heuristics.py", line 883, in cached_autotune
    best_config = json.loads(fd.read())
  File "/home/gridsan/nchutisilp/.conda/envs/nnunet/lib/python3.9/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
  File "/home/gridsan/nchutisilp/.conda/envs/nnunet/lib/python3.9/json/decoder.py", line 340, in decode
    raise JSONDecodeError("Extra data", s, end)
json.decoder.JSONDecodeError: Extra data: line 1 column 161 (char 160)
"""

The above exception was the direct cause of the following exception:

concurrent.futures.process._RemoteTraceback: 
"""
Traceback (most recent call last):
  File "/home/gridsan/nchutisilp/.conda/envs/nnunet/lib/python3.9/concurrent/futures/process.py", line 246, in _process_worker
    r = call_item.fn(*call_item.args, **call_item.kwargs)
  File "/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/_inductor/codecache.py", line 2492, in _worker_compile
    kernel = TritonCodeCache.load(kernel_name, source_code)
  File "/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/_inductor/codecache.py", line 2205, in load
    mod = PyCodeCache.load(source_code)
  File "/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/_inductor/codecache.py", line 2137, in load
    return cls.load_by_key_path(key, path, linemap, attrs)
  File "/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/_inductor/codecache.py", line 2160, in load_by_key_path
    exec(code, mod.__dict__, mod.__dict__)
  File "/state/partition1/slurm_tmp/27547865.4294967291.0/torchinductor_nchutisilp/sc/csc4wqp3e5k7hoocadgqqzub6rcynx3zdppt56kjlfkaxank3csb.py", line 12, in <module>
    @triton_heuristics.pointwise(
  File "/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/_inductor/triton_heuristics.py", line 1188, in pointwise
    return cached_autotune(
  File "/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/_inductor/triton_heuristics.py", line 883, in cached_autotune
    best_config = json.loads(fd.read())
  File "/home/gridsan/nchutisilp/.conda/envs/nnunet/lib/python3.9/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
  File "/home/gridsan/nchutisilp/.conda/envs/nnunet/lib/python3.9/json/decoder.py", line 340, in decode
    raise JSONDecodeError("Extra data", s, end)
json.decoder.JSONDecodeError: Extra data: line 1 column 161 (char 160)
"""Traceback (most recent call last):

  File "/home/gridsan/nchutisilp/.conda/envs/nnunet/bin/nnUNetv2_train", line 8, in <module>

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/gridsan/nchutisilp/.conda/envs/nnunet/bin/nnUNetv2_train", line 8, in <module>
    sys.exit(run_training_entry())
  File "/home/gridsan/nchutisilp/projects/nnUNet/nnunetv2/run/run_training.py", line 275, in run_training_entry
    sys.exit(run_training_entry())
  File "/home/gridsan/nchutisilp/projects/nnUNet/nnunetv2/run/run_training.py", line 275, in run_training_entry
    run_training(args.dataset_name_or_id, args.configuration, args.fold, args.tr, args.p, args.pretrained_weights,
  File "/home/gridsan/nchutisilp/projects/nnUNet/nnunetv2/run/run_training.py", line 211, in run_training
    run_training(args.dataset_name_or_id, args.configuration, args.fold, args.tr, args.p, args.pretrained_weights,
  File "/home/gridsan/nchutisilp/projects/nnUNet/nnunetv2/run/run_training.py", line 211, in run_training
    nnunet_trainer.run_training()
  File "/home/gridsan/nchutisilp/projects/nnUNet/nnunetv2/training/nnUNetTrainer/nnUNetTrainer.py", line 1401, in run_training
    nnunet_trainer.run_training()
  File "/home/gridsan/nchutisilp/projects/nnUNet/nnunetv2/training/nnUNetTrainer/nnUNetTrainer.py", line 1401, in run_training
    train_outputs.append(self.train_step(next(self.dataloader_train)))
  File "/home/gridsan/nchutisilp/projects/nnUNet/nnunetv2/training/nnUNetTrainer/nnUNetTrainer.py", line 1016, in train_step
    train_outputs.append(self.train_step(next(self.dataloader_train)))
  File "/home/gridsan/nchutisilp/projects/nnUNet/nnunetv2/training/nnUNetTrainer/nnUNetTrainer.py", line 1016, in train_step
    self.grad_scaler.scale(l).backward()
  File "/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/_tensor.py", line 525, in backward
    self.grad_scaler.scale(l).backward()
  File "/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/_tensor.py", line 525, in backward
    torch.autograd.backward(
  File "/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/autograd/__init__.py", line 267, in backward
    torch.autograd.backward(
  File "/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/autograd/__init__.py", line 267, in backward
    _engine_run_backward(
  File "/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/autograd/graph.py", line 744, in _engine_run_backward
    _engine_run_backward(
  File "/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/autograd/graph.py", line 744, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
  File "/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/autograd/function.py", line 301, in apply
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
  File "/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/autograd/function.py", line 301, in apply
    return user_fn(self, *args)
  File "/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py", line 882, in backward
    return user_fn(self, *args)
  File "/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py", line 882, in backward
    out = call_compiled_backward()
  File "/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py", line 827, in call_compiled_backward
    out = call_compiled_backward()
  File "/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py", line 827, in call_compiled_backward
    CompiledFunction.compiled_bw = aot_config.bw_compiler(
    CompiledFunction.compiled_bw = aot_config.bw_compiler(
  File "/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/_dynamo/backends/common.py", line 36, in _wrapped_bw_compiler
  File "/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/_dynamo/backends/common.py", line 36, in _wrapped_bw_compiler
    return disable(disable(bw_compiler)(*args, **kwargs))
  File "/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/_dynamo/eval_frame.py", line 451, in _fn
    return disable(disable(bw_compiler)(*args, **kwargs))
  File "/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/_dynamo/eval_frame.py", line 451, in _fn
    return fn(*args, **kwargs)
  File "/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/_dynamo/external_utils.py", line 36, in inner
    return fn(*args, **kwargs)
  File "/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/_dynamo/external_utils.py", line 36, in inner
    return fn(*args, **kwargs)
  File "/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/_dynamo/utils.py", line 262, in time_wrapper
    return fn(*args, **kwargs)
  File "/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/_dynamo/utils.py", line 262, in time_wrapper
    r = func(*args, **kwargs)
  File "/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/_inductor/compile_fx.py", line 1293, in bw_compiler
    r = func(*args, **kwargs)
  File "/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/_inductor/compile_fx.py", line 1293, in bw_compiler
    return inner_compile(
  File "/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/_dynamo/repro/after_aot.py", line 83, in debug_wrapper
    return inner_compile(
  File "/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/_dynamo/repro/after_aot.py", line 83, in debug_wrapper
    inner_compiled_fn = compiler_fn(gm, example_inputs)
  File "/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/_inductor/debug.py", line 304, in inner
    inner_compiled_fn = compiler_fn(gm, example_inputs)
  File "/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/_inductor/debug.py", line 304, in inner
    return fn(*args, **kwargs)
  File "/home/gridsan/nchutisilp/.conda/envs/nnunet/lib/python3.9/contextlib.py", line 79, in inner
    return func(*args, **kwds)
  File "/home/gridsan/nchutisilp/.conda/envs/nnunet/lib/python3.9/contextlib.py", line 79, in inner
    return fn(*args, **kwargs)
    return func(*args, **kwds)
  File "/home/gridsan/nchutisilp/.conda/envs/nnunet/lib/python3.9/contextlib.py", line 79, in inner
  File "/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/_dynamo/utils.py", line 262, in time_wrapper
    return func(*args, **kwds)
  File "/home/gridsan/nchutisilp/.conda/envs/nnunet/lib/python3.9/contextlib.py", line 79, in inner
    return func(*args, **kwds)
  File "/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/_dynamo/utils.py", line 262, in time_wrapper
    r = func(*args, **kwargs)
  File "/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/_inductor/compile_fx.py", line 438, in compile_fx_inner
    r = func(*args, **kwargs)
  File "/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/_inductor/compile_fx.py", line 438, in compile_fx_inner
    compiled_graph = fx_codegen_and_compile(
  File "/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/_inductor/compile_fx.py", line 714, in fx_codegen_and_compile
    compiled_graph = fx_codegen_and_compile(
  File "/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/_inductor/compile_fx.py", line 714, in fx_codegen_and_compile
    compiled_fn = graph.compile_to_fn()
  File "/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/_inductor/graph.py", line 1307, in compile_to_fn
    compiled_fn = graph.compile_to_fn()
  File "/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/_inductor/graph.py", line 1307, in compile_to_fn
    return self.compile_to_module().call
  File "/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/_dynamo/utils.py", line 262, in time_wrapper
    r = func(*args, **kwargs)
  File "/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/_inductor/graph.py", line 1254, in compile_to_module
    return self.compile_to_module().call
  File "/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/_dynamo/utils.py", line 262, in time_wrapper
    r = func(*args, **kwargs)
  File "/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/_inductor/graph.py", line 1254, in compile_to_module
    mod = PyCodeCache.load_by_key_path(
  File "/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/_inductor/codecache.py", line 2160, in load_by_key_path
    mod = PyCodeCache.load_by_key_path(
  File "/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/_inductor/codecache.py", line 2160, in load_by_key_path
    exec(code, mod.__dict__, mod.__dict__)
  File "/state/partition1/slurm_tmp/27547865.4294967291.0/torchinductor_nchutisilp/4h/c4humqldtkp77oba6wy2535ie2boovmpficmoo2x4rklhb5lu2dw.py", line 3651, in <module>
    exec(code, mod.__dict__, mod.__dict__)
  File "/state/partition1/slurm_tmp/27547865.4294967291.0/torchinductor_nchutisilp/4h/c4humqldtkp77oba6wy2535ie2boovmpficmoo2x4rklhb5lu2dw.py", line 3651, in <module>
    async_compile.wait(globals())
  File "/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/_inductor/codecache.py", line 2715, in wait
    async_compile.wait(globals())
  File "/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/_inductor/codecache.py", line 2715, in wait
    scope[key] = result.result()
  File "/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/_inductor/codecache.py", line 2522, in result
    scope[key] = result.result()
  File "/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/_inductor/codecache.py", line 2522, in result
    self.future.result()
  File "/home/gridsan/nchutisilp/.conda/envs/nnunet/lib/python3.9/concurrent/futures/_base.py", line 439, in result
    self.future.result()
  File "/home/gridsan/nchutisilp/.conda/envs/nnunet/lib/python3.9/concurrent/futures/_base.py", line 439, in result
    return self.__get_result()
  File "/home/gridsan/nchutisilp/.conda/envs/nnunet/lib/python3.9/concurrent/futures/_base.py", line 391, in __get_result
    return self.__get_result()
  File "/home/gridsan/nchutisilp/.conda/envs/nnunet/lib/python3.9/concurrent/futures/_base.py", line 391, in __get_result
    raise self._exception
json.decoder.JSONDecodeError: Extra data: line 1 column 161 (char 160)
    raise self._exception
json.decoder.JSONDecodeError: Extra data: line 1 column 161 (char 160)
Exception in thread Thread-2:
Traceback (most recent call last):
  File "/home/gridsan/nchutisilp/.conda/envs/nnunet/lib/python3.9/threading.py", line 980, in _bootstrap_inner
Exception in thread Thread-2:
Traceback (most recent call last):
  File "/home/gridsan/nchutisilp/.conda/envs/nnunet/lib/python3.9/threading.py", line 980, in _bootstrap_inner
Exception in thread Thread-1:
Traceback (most recent call last):
  File "/home/gridsan/nchutisilp/.conda/envs/nnunet/lib/python3.9/threading.py", line 980, in _bootstrap_inner
Exception in thread Thread-1:
Traceback (most recent call last):
  File "/home/gridsan/nchutisilp/.conda/envs/nnunet/lib/python3.9/threading.py", line 980, in _bootstrap_inner
    self.run()
    self.run()
    self.run()
  File "/home/gridsan/nchutisilp/.conda/envs/nnunet/lib/python3.9/threading.py", line 917, in run
  File "/home/gridsan/nchutisilp/.conda/envs/nnunet/lib/python3.9/threading.py", line 917, in run
  File "/home/gridsan/nchutisilp/.conda/envs/nnunet/lib/python3.9/threading.py", line 917, in run
    self.run()
  File "/home/gridsan/nchutisilp/.conda/envs/nnunet/lib/python3.9/threading.py", line 917, in run
    self._target(*self._args, **self._kwargs)
  File "/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/batchgenerators/dataloading/nondet_multi_threaded_augmenter.py", line 125, in results_loop
    self._target(*self._args, **self._kwargs)
    self._target(*self._args, **self._kwargs)
  File "/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/batchgenerators/dataloading/nondet_multi_threaded_augmenter.py", line 125, in results_loop
  File "/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/batchgenerators/dataloading/nondet_multi_threaded_augmenter.py", line 125, in results_loop
    self._target(*self._args, **self._kwargs)
  File "/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/batchgenerators/dataloading/nondet_multi_threaded_augmenter.py", line 125, in results_loop
    raise e
  File "/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/batchgenerators/dataloading/nondet_multi_threaded_augmenter.py", line 103, in results_loop
    raise e
    raise e
  File "/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/batchgenerators/dataloading/nondet_multi_threaded_augmenter.py", line 103, in results_loop
  File "/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/batchgenerators/dataloading/nondet_multi_threaded_augmenter.py", line 103, in results_loop
    raise RuntimeError("One or more background workers are no longer alive. Exiting. Please check the "
RuntimeError: One or more background workers are no longer alive. Exiting. Please check the print statements above for the actual error message
/var/spool/slurmd/job27547865/slurm_script: line 21: 4194124 Aborted                 CUDA_VISIBLE_DEVICES=0 nnUNetv2_train $DATASET_ID $CONFIG 2 -tr $TRAINER -device cuda

############################
INFO: You are using the old nnU-Net default plans. We have updated our recommendations. Please consider using those instead! Read more here: https://github.com/MIC-DKFZ/nnUNet/blob/master/documentation/resenc_presets.md
############################

Using device: cuda:0

#######################################################################
Please cite the following paper when using nnU-Net:
Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.
#######################################################################

2024-12-04 20:46:08.000728: do_dummy_2d_data_aug: True
2024-12-04 20:46:08.002777: Using splits from existing split file: /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset307_Sohee_Calcium_OCT_CrossValidation/splits_final.json
2024-12-04 20:46:08.004500: The split file contains 5 splits.
2024-12-04 20:46:08.005472: Desired fold for training: 4
2024-12-04 20:46:08.006051: This split has 7 training and 1 validation cases.
using pin_memory on device 0
using pin_memory on device 0
2024-12-04 20:46:11.142709: Using torch.compile...
/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "

This is the configuration used by this training:
Configuration name: 3d_32x160x128_b10
 {'data_identifier': 'nnUNetPlans_3d_fullres', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 10, 'patch_size': [32, 160, 128], 'median_image_size_in_voxels': [375.0, 498.0, 498.0], 'spacing': [1.0, 1.0, 1.0], 'normalization_schemes': ['ZScoreNormalization'], 'use_mask_for_norm': [False], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'architecture': {'network_class_name': 'dynamic_network_architectures.architectures.unet.PlainConvUNet', 'arch_kwargs': {'n_stages': 6, 'features_per_stage': [32, 64, 128, 256, 320, 320], 'conv_op': 'torch.nn.modules.conv.Conv3d', 'kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]], 'strides': [[1, 1, 1], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [1, 2, 2]], 'n_conv_per_stage': [2, 2, 2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2, 2, 2], 'conv_bias': True, 'norm_op': 'torch.nn.modules.instancenorm.InstanceNorm3d', 'norm_op_kwargs': {'eps': 1e-05, 'affine': True}, 'dropout_op': None, 'dropout_op_kwargs': None, 'nonlin': 'torch.nn.LeakyReLU', 'nonlin_kwargs': {'inplace': True}, 'deep_supervision': True}, '_kw_requires_import': ['conv_op', 'norm_op', 'dropout_op', 'nonlin']}, 'batch_dice': True, 'inherits_from': '3d_fullres'} 

These are the global plan.json settings:
 {'dataset_name': 'Dataset307_Sohee_Calcium_OCT_CrossValidation', 'plans_name': 'nnUNetPlans', 'original_median_spacing_after_transp': [1.0, 1.0, 1.0], 'original_median_shape_after_transp': [375, 498, 498], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 0.4977976381778717, 'mean': 0.13507525622844696, 'median': 0.09599608182907104, 'min': 0.0, 'percentile_00_5': 0.014754901640117168, 'percentile_99_5': 0.4977976381778717, 'std': 0.12025152146816254}}} 

2024-12-04 20:46:11.894630: unpacking dataset...
2024-12-04 20:46:15.428381: unpacking done...
2024-12-04 20:46:15.441960: Unable to plot network architecture: nnUNet_compile is enabled!
2024-12-04 20:46:15.478613: 
2024-12-04 20:46:15.480200: Epoch 0
2024-12-04 20:46:15.481756: Current learning rate: 0.01
concurrent.futures.process._RemoteTraceback: 
"""
Traceback (most recent call last):
  File "/home/gridsan/nchutisilp/.conda/envs/nnunet/lib/python3.9/concurrent/futures/process.py", line 246, in _process_worker
    r = call_item.fn(*call_item.args, **call_item.kwargs)
  File "/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/_inductor/codecache.py", line 2492, in _worker_compile
    kernel = TritonCodeCache.load(kernel_name, source_code)
  File "/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/_inductor/codecache.py", line 2205, in load
    mod = PyCodeCache.load(source_code)
  File "/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/_inductor/codecache.py", line 2137, in load
    return cls.load_by_key_path(key, path, linemap, attrs)
  File "/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/_inductor/codecache.py", line 2160, in load_by_key_path
    exec(code, mod.__dict__, mod.__dict__)
  File "/state/partition1/slurm_tmp/27547865.4294967291.0/torchinductor_nchutisilp/sc/csc4wqp3e5k7hoocadgqqzub6rcynx3zdppt56kjlfkaxank3csb.py", line 12, in <module>
    @triton_heuristics.pointwise(
  File "/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/_inductor/triton_heuristics.py", line 1188, in pointwise
    return cached_autotune(
  File "/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/_inductor/triton_heuristics.py", line 883, in cached_autotune
    best_config = json.loads(fd.read())
  File "/home/gridsan/nchutisilp/.conda/envs/nnunet/lib/python3.9/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
  File "/home/gridsan/nchutisilp/.conda/envs/nnunet/lib/python3.9/json/decoder.py", line 340, in decode
    raise JSONDecodeError("Extra data", s, end)
json.decoder.JSONDecodeError: Extra data: line 1 column 161 (char 160)
"""

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/gridsan/nchutisilp/.conda/envs/nnunet/bin/nnUNetv2_train", line 8, in <module>
    sys.exit(run_training_entry())
  File "/home/gridsan/nchutisilp/projects/nnUNet/nnunetv2/run/run_training.py", line 275, in run_training_entry
    run_training(args.dataset_name_or_id, args.configuration, args.fold, args.tr, args.p, args.pretrained_weights,
  File "/home/gridsan/nchutisilp/projects/nnUNet/nnunetv2/run/run_training.py", line 211, in run_training
    nnunet_trainer.run_training()
  File "/home/gridsan/nchutisilp/projects/nnUNet/nnunetv2/training/nnUNetTrainer/nnUNetTrainer.py", line 1401, in run_training
    train_outputs.append(self.train_step(next(self.dataloader_train)))
  File "/home/gridsan/nchutisilp/projects/nnUNet/nnunetv2/training/nnUNetTrainer/nnUNetTrainer.py", line 1016, in train_step
    self.grad_scaler.scale(l).backward()
  File "/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/_tensor.py", line 525, in backward
    torch.autograd.backward(
  File "/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/autograd/__init__.py", line 267, in backward
    _engine_run_backward(
  File "/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/autograd/graph.py", line 744, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
  File "/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/autograd/function.py", line 301, in apply
    return user_fn(self, *args)
  File "/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py", line 882, in backward
    out = call_compiled_backward()
  File "/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py", line 827, in call_compiled_backward
    CompiledFunction.compiled_bw = aot_config.bw_compiler(
  File "/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/_dynamo/backends/common.py", line 36, in _wrapped_bw_compiler
    return disable(disable(bw_compiler)(*args, **kwargs))
  File "/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/_dynamo/eval_frame.py", line 451, in _fn
    return fn(*args, **kwargs)
  File "/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/_dynamo/external_utils.py", line 36, in inner
    return fn(*args, **kwargs)
  File "/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/_dynamo/utils.py", line 262, in time_wrapper
    r = func(*args, **kwargs)
  File "/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/_inductor/compile_fx.py", line 1293, in bw_compiler
    return inner_compile(
  File "/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/_dynamo/repro/after_aot.py", line 83, in debug_wrapper
    inner_compiled_fn = compiler_fn(gm, example_inputs)
  File "/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/_inductor/debug.py", line 304, in inner
    return fn(*args, **kwargs)
  File "/home/gridsan/nchutisilp/.conda/envs/nnunet/lib/python3.9/contextlib.py", line 79, in inner
    return func(*args, **kwds)
  File "/home/gridsan/nchutisilp/.conda/envs/nnunet/lib/python3.9/contextlib.py", line 79, in inner
    return func(*args, **kwds)
  File "/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/_dynamo/utils.py", line 262, in time_wrapper
    r = func(*args, **kwargs)
  File "/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/_inductor/compile_fx.py", line 438, in compile_fx_inner
    compiled_graph = fx_codegen_and_compile(
  File "/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/_inductor/compile_fx.py", line 714, in fx_codegen_and_compile
    compiled_fn = graph.compile_to_fn()
  File "/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/_inductor/graph.py", line 1307, in compile_to_fn
    return self.compile_to_module().call
  File "/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/_dynamo/utils.py", line 262, in time_wrapper
    r = func(*args, **kwargs)
  File "/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/_inductor/graph.py", line 1254, in compile_to_module
    mod = PyCodeCache.load_by_key_path(
  File "/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/_inductor/codecache.py", line 2160, in load_by_key_path
    exec(code, mod.__dict__, mod.__dict__)
  File "/state/partition1/slurm_tmp/27547865.4294967291.0/torchinductor_nchutisilp/4h/c4humqldtkp77oba6wy2535ie2boovmpficmoo2x4rklhb5lu2dw.py", line 3651, in <module>
    async_compile.wait(globals())
  File "/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/_inductor/codecache.py", line 2715, in wait
    scope[key] = result.result()
  File "/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/_inductor/codecache.py", line 2522, in result
    self.future.result()
  File "/home/gridsan/nchutisilp/.conda/envs/nnunet/lib/python3.9/concurrent/futures/_base.py", line 439, in result
    return self.__get_result()
  File "/home/gridsan/nchutisilp/.conda/envs/nnunet/lib/python3.9/concurrent/futures/_base.py", line 391, in __get_result
    raise self._exception
json.decoder.JSONDecodeError: Extra data: line 1 column 161 (char 160)
Exception in thread Thread-2:
Traceback (most recent call last):
  File "/home/gridsan/nchutisilp/.conda/envs/nnunet/lib/python3.9/threading.py", line 980, in _bootstrap_inner
    self.run()
  File "/home/gridsan/nchutisilp/.conda/envs/nnunet/lib/python3.9/threading.py", line 917, in run
    self._target(*self._args, **self._kwargs)
  File "/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/batchgenerators/dataloading/nondet_multi_threaded_augmenter.py", line 125, in results_loop
    raise e
  File "/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/batchgenerators/dataloading/nondet_multi_threaded_augmenter.py", line 103, in results_loop
    raise RuntimeError("One or more background workers are no longer alive. Exiting. Please check the "
RuntimeError: One or more background workers are no longer alive. Exiting. Please check the print statements above for the actual error message
Exception in thread Thread-1:
Traceback (most recent call last):
  File "/home/gridsan/nchutisilp/.conda/envs/nnunet/lib/python3.9/threading.py", line 980, in _bootstrap_inner
    self.run()
  File "/home/gridsan/nchutisilp/.conda/envs/nnunet/lib/python3.9/threading.py", line 917, in run
    self._target(*self._args, **self._kwargs)
  File "/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/batchgenerators/dataloading/nondet_multi_threaded_augmenter.py", line 125, in results_loop
    raise e
  File "/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/batchgenerators/dataloading/nondet_multi_threaded_augmenter.py", line 103, in results_loop
    raise RuntimeError("One or more background workers are no longer alive. Exiting. Please check the "
RuntimeError: One or more background workers are no longer alive. Exiting. Please check the print statements above for the actual error message
